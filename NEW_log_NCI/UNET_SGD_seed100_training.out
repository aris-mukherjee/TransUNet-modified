2021-12-11 19:26:18,671 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2021-12-11 19:26:18,672 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2021-12-11 19:26:18,672 ============================================================
2021-12-11 19:26:18,672 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2021-12-11 19:26:18,672 ============================================================
2021-12-11 19:26:18,672 Loading data...
2021-12-11 19:26:18,672 Reading NCI - RUNMC images...
2021-12-11 19:26:18,672 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2021-12-11 19:26:18,675 Already preprocessed this configuration. Loading now!
2021-12-11 19:26:18,694 Training Images: (256, 256, 286)
2021-12-11 19:26:18,694 Training Labels: (256, 256, 286)
2021-12-11 19:26:18,694 Validation Images: (256, 256, 98)
2021-12-11 19:26:18,694 Validation Labels: (256, 256, 98)
2021-12-11 19:26:18,694 ============================================================
2021-12-11 19:26:18,737 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2021-12-11 19:26:21,601 iteration 1 : loss : 0.815076, loss_ce: 0.940773
2021-12-11 19:26:22,941 iteration 2 : loss : 0.821473, loss_ce: 0.927906
2021-12-11 19:26:24,249 iteration 3 : loss : 0.823299, loss_ce: 0.917031
2021-12-11 19:26:25,612 iteration 4 : loss : 0.806712, loss_ce: 0.918540
2021-12-11 19:26:26,911 iteration 5 : loss : 0.810223, loss_ce: 0.897817
2021-12-11 19:26:28,311 iteration 6 : loss : 0.797500, loss_ce: 0.889337
2021-12-11 19:26:29,690 iteration 7 : loss : 0.791818, loss_ce: 0.872661
2021-12-11 19:26:31,065 iteration 8 : loss : 0.781344, loss_ce: 0.858080
2021-12-11 19:26:32,336 iteration 9 : loss : 0.763161, loss_ce: 0.851525
2021-12-11 19:26:33,721 iteration 10 : loss : 0.755811, loss_ce: 0.829110
2021-12-11 19:26:35,115 iteration 11 : loss : 0.747907, loss_ce: 0.806485
2021-12-11 19:26:36,541 iteration 12 : loss : 0.731782, loss_ce: 0.788922
2021-12-11 19:26:37,882 iteration 13 : loss : 0.725234, loss_ce: 0.767397
2021-12-11 19:26:39,305 iteration 14 : loss : 0.712604, loss_ce: 0.749583
2021-12-11 19:26:40,680 iteration 15 : loss : 0.701074, loss_ce: 0.727846
2021-12-11 19:26:42,018 iteration 16 : loss : 0.688566, loss_ce: 0.713470
2021-12-11 19:26:43,331 iteration 17 : loss : 0.684944, loss_ce: 0.685741
  0%|                               | 1/400 [00:24<2:44:09, 24.69s/it]2021-12-11 19:26:44,807 iteration 18 : loss : 0.664433, loss_ce: 0.674960
2021-12-11 19:26:46,191 iteration 19 : loss : 0.662045, loss_ce: 0.644254
2021-12-11 19:26:47,663 iteration 20 : loss : 0.642791, loss_ce: 0.641639
2021-12-11 19:26:49,005 iteration 21 : loss : 0.634556, loss_ce: 0.617056
2021-12-11 19:26:50,303 iteration 22 : loss : 0.624477, loss_ce: 0.595130
2021-12-11 19:26:51,715 iteration 23 : loss : 0.615706, loss_ce: 0.586474
2021-12-11 19:26:53,120 iteration 24 : loss : 0.598951, loss_ce: 0.575936
2021-12-11 19:26:54,550 iteration 25 : loss : 0.589909, loss_ce: 0.578418
2021-12-11 19:26:55,904 iteration 26 : loss : 0.577425, loss_ce: 0.541877
2021-12-11 19:26:57,218 iteration 27 : loss : 0.575877, loss_ce: 0.525212
2021-12-11 19:26:58,519 iteration 28 : loss : 0.562629, loss_ce: 0.505067
2021-12-11 19:26:59,906 iteration 29 : loss : 0.560571, loss_ce: 0.509571
2021-12-11 19:27:01,246 iteration 30 : loss : 0.554115, loss_ce: 0.492159
2021-12-11 19:27:02,559 iteration 31 : loss : 0.545670, loss_ce: 0.458958
2021-12-11 19:27:03,903 iteration 32 : loss : 0.533879, loss_ce: 0.479935
2021-12-11 19:27:05,299 iteration 33 : loss : 0.527907, loss_ce: 0.460085
2021-12-11 19:27:06,755 iteration 34 : loss : 0.522679, loss_ce: 0.441346
  0%|▏                              | 2/400 [00:48<2:38:40, 23.92s/it]2021-12-11 19:27:08,199 iteration 35 : loss : 0.511843, loss_ce: 0.439487
2021-12-11 19:27:09,591 iteration 36 : loss : 0.515758, loss_ce: 0.399140
2021-12-11 19:27:10,995 iteration 37 : loss : 0.504561, loss_ce: 0.404653
2021-12-11 19:27:12,368 iteration 38 : loss : 0.495701, loss_ce: 0.400456
2021-12-11 19:27:13,657 iteration 39 : loss : 0.498362, loss_ce: 0.391247
2021-12-11 19:27:15,009 iteration 40 : loss : 0.493612, loss_ce: 0.397538
2021-12-11 19:27:16,294 iteration 41 : loss : 0.478403, loss_ce: 0.379138
2021-12-11 19:27:17,681 iteration 42 : loss : 0.472992, loss_ce: 0.373517
2021-12-11 19:27:19,054 iteration 43 : loss : 0.473185, loss_ce: 0.363990
2021-12-11 19:27:20,341 iteration 44 : loss : 0.472446, loss_ce: 0.373449
2021-12-11 19:27:21,774 iteration 45 : loss : 0.467026, loss_ce: 0.336852
2021-12-11 19:27:23,143 iteration 46 : loss : 0.464367, loss_ce: 0.346944
2021-12-11 19:27:24,517 iteration 47 : loss : 0.461602, loss_ce: 0.330883
2021-12-11 19:27:25,887 iteration 48 : loss : 0.451077, loss_ce: 0.340981
2021-12-11 19:27:27,308 iteration 49 : loss : 0.457444, loss_ce: 0.324295
2021-12-11 19:27:28,723 iteration 50 : loss : 0.451275, loss_ce: 0.320472
2021-12-11 19:27:30,156 iteration 51 : loss : 0.443719, loss_ce: 0.320128
  1%|▏                              | 3/400 [01:11<2:36:42, 23.68s/it]2021-12-11 19:27:31,536 iteration 52 : loss : 0.439106, loss_ce: 0.301418
2021-12-11 19:27:32,831 iteration 53 : loss : 0.437874, loss_ce: 0.309091
2021-12-11 19:27:34,247 iteration 54 : loss : 0.433591, loss_ce: 0.295061
2021-12-11 19:27:35,653 iteration 55 : loss : 0.442147, loss_ce: 0.290046
2021-12-11 19:27:37,054 iteration 56 : loss : 0.437471, loss_ce: 0.316094
2021-12-11 19:27:38,395 iteration 57 : loss : 0.437297, loss_ce: 0.309325
2021-12-11 19:27:39,713 iteration 58 : loss : 0.434871, loss_ce: 0.305928
2021-12-11 19:27:41,033 iteration 59 : loss : 0.429698, loss_ce: 0.284164
2021-12-11 19:27:42,373 iteration 60 : loss : 0.435425, loss_ce: 0.297081
2021-12-11 19:27:43,724 iteration 61 : loss : 0.420534, loss_ce: 0.282286
2021-12-11 19:27:45,162 iteration 62 : loss : 0.422359, loss_ce: 0.258145
2021-12-11 19:27:46,495 iteration 63 : loss : 0.428183, loss_ce: 0.291069
2021-12-11 19:27:47,892 iteration 64 : loss : 0.418571, loss_ce: 0.267543
2021-12-11 19:27:49,269 iteration 65 : loss : 0.408948, loss_ce: 0.247119
2021-12-11 19:27:50,696 iteration 66 : loss : 0.416692, loss_ce: 0.253389
2021-12-11 19:27:52,045 iteration 67 : loss : 0.415662, loss_ce: 0.267436
2021-12-11 19:27:53,347 iteration 68 : loss : 0.413400, loss_ce: 0.272314
  1%|▎                              | 4/400 [01:34<2:35:03, 23.49s/it]2021-12-11 19:27:54,817 iteration 69 : loss : 0.411464, loss_ce: 0.253066
2021-12-11 19:27:56,158 iteration 70 : loss : 0.412479, loss_ce: 0.238133
2021-12-11 19:27:57,516 iteration 71 : loss : 0.423062, loss_ce: 0.289673
2021-12-11 19:27:58,904 iteration 72 : loss : 0.411323, loss_ce: 0.255315
2021-12-11 19:28:00,305 iteration 73 : loss : 0.406989, loss_ce: 0.241765
2021-12-11 19:28:01,703 iteration 74 : loss : 0.393594, loss_ce: 0.236135
2021-12-11 19:28:03,048 iteration 75 : loss : 0.417856, loss_ce: 0.279617
2021-12-11 19:28:04,345 iteration 76 : loss : 0.408581, loss_ce: 0.246293
2021-12-11 19:28:05,722 iteration 77 : loss : 0.403579, loss_ce: 0.238144
2021-12-11 19:28:07,097 iteration 78 : loss : 0.397111, loss_ce: 0.235430
2021-12-11 19:28:08,478 iteration 79 : loss : 0.397907, loss_ce: 0.222831
2021-12-11 19:28:09,762 iteration 80 : loss : 0.399477, loss_ce: 0.221666
2021-12-11 19:28:11,193 iteration 81 : loss : 0.392465, loss_ce: 0.247685
2021-12-11 19:28:12,544 iteration 82 : loss : 0.397468, loss_ce: 0.212221
2021-12-11 19:28:13,860 iteration 83 : loss : 0.395870, loss_ce: 0.199208
2021-12-11 19:28:15,305 iteration 84 : loss : 0.402933, loss_ce: 0.243224
2021-12-11 19:28:15,305 Training Data Eval:
2021-12-11 19:28:22,103   Average segmentation loss on training set: 0.3909
2021-12-11 19:28:22,103 Validation Data Eval:
2021-12-11 19:28:24,593   Average segmentation loss on validation set: 0.4127
2021-12-11 19:28:26,515 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:28:27,852 iteration 85 : loss : 0.405720, loss_ce: 0.249472
  1%|▍                              | 5/400 [02:09<3:00:47, 27.46s/it]2021-12-11 19:28:29,313 iteration 86 : loss : 0.397833, loss_ce: 0.243738
2021-12-11 19:28:30,663 iteration 87 : loss : 0.400814, loss_ce: 0.226148
2021-12-11 19:28:31,985 iteration 88 : loss : 0.405512, loss_ce: 0.258671
2021-12-11 19:28:33,361 iteration 89 : loss : 0.385414, loss_ce: 0.202321
2021-12-11 19:28:34,720 iteration 90 : loss : 0.391267, loss_ce: 0.221336
2021-12-11 19:28:36,133 iteration 91 : loss : 0.393959, loss_ce: 0.236101
2021-12-11 19:28:37,550 iteration 92 : loss : 0.379554, loss_ce: 0.199165
2021-12-11 19:28:38,858 iteration 93 : loss : 0.382092, loss_ce: 0.199780
2021-12-11 19:28:40,264 iteration 94 : loss : 0.380842, loss_ce: 0.220765
2021-12-11 19:28:41,622 iteration 95 : loss : 0.398474, loss_ce: 0.240031
2021-12-11 19:28:43,049 iteration 96 : loss : 0.389993, loss_ce: 0.221731
2021-12-11 19:28:44,418 iteration 97 : loss : 0.379586, loss_ce: 0.206958
2021-12-11 19:28:45,825 iteration 98 : loss : 0.388479, loss_ce: 0.202293
2021-12-11 19:28:47,186 iteration 99 : loss : 0.387439, loss_ce: 0.198111
2021-12-11 19:28:48,591 iteration 100 : loss : 0.393815, loss_ce: 0.216247
2021-12-11 19:28:49,940 iteration 101 : loss : 0.385896, loss_ce: 0.210008
2021-12-11 19:28:51,281 iteration 102 : loss : 0.394497, loss_ce: 0.233570
  2%|▍                              | 6/400 [02:32<2:51:18, 26.09s/it]2021-12-11 19:28:52,730 iteration 103 : loss : 0.370632, loss_ce: 0.190695
2021-12-11 19:28:54,154 iteration 104 : loss : 0.384111, loss_ce: 0.204674
2021-12-11 19:28:55,573 iteration 105 : loss : 0.378018, loss_ce: 0.207475
2021-12-11 19:28:57,061 iteration 106 : loss : 0.379642, loss_ce: 0.194879
2021-12-11 19:28:58,486 iteration 107 : loss : 0.389061, loss_ce: 0.189577
2021-12-11 19:28:59,857 iteration 108 : loss : 0.378488, loss_ce: 0.182351
2021-12-11 19:29:01,169 iteration 109 : loss : 0.390741, loss_ce: 0.217533
2021-12-11 19:29:02,509 iteration 110 : loss : 0.387181, loss_ce: 0.220085
2021-12-11 19:29:03,943 iteration 111 : loss : 0.394426, loss_ce: 0.224949
2021-12-11 19:29:05,312 iteration 112 : loss : 0.377882, loss_ce: 0.201468
2021-12-11 19:29:06,624 iteration 113 : loss : 0.366994, loss_ce: 0.187658
2021-12-11 19:29:08,032 iteration 114 : loss : 0.376761, loss_ce: 0.189458
2021-12-11 19:29:09,387 iteration 115 : loss : 0.377919, loss_ce: 0.223626
2021-12-11 19:29:10,788 iteration 116 : loss : 0.391152, loss_ce: 0.226838
2021-12-11 19:29:12,125 iteration 117 : loss : 0.379647, loss_ce: 0.208694
2021-12-11 19:29:13,477 iteration 118 : loss : 0.386259, loss_ce: 0.221198
2021-12-11 19:29:14,854 iteration 119 : loss : 0.363640, loss_ce: 0.181198
  2%|▌                              | 7/400 [02:56<2:45:29, 25.27s/it]2021-12-11 19:29:16,338 iteration 120 : loss : 0.379060, loss_ce: 0.207198
2021-12-11 19:29:17,740 iteration 121 : loss : 0.386901, loss_ce: 0.199302
2021-12-11 19:29:19,046 iteration 122 : loss : 0.375804, loss_ce: 0.208367
2021-12-11 19:29:20,455 iteration 123 : loss : 0.372681, loss_ce: 0.195718
2021-12-11 19:29:21,868 iteration 124 : loss : 0.379347, loss_ce: 0.191046
2021-12-11 19:29:23,231 iteration 125 : loss : 0.391629, loss_ce: 0.227683
2021-12-11 19:29:24,642 iteration 126 : loss : 0.376101, loss_ce: 0.175286
2021-12-11 19:29:26,069 iteration 127 : loss : 0.371629, loss_ce: 0.190249
2021-12-11 19:29:27,507 iteration 128 : loss : 0.371641, loss_ce: 0.206801
2021-12-11 19:29:28,852 iteration 129 : loss : 0.367293, loss_ce: 0.184885
2021-12-11 19:29:30,283 iteration 130 : loss : 0.382366, loss_ce: 0.209313
2021-12-11 19:29:31,655 iteration 131 : loss : 0.387396, loss_ce: 0.214016
2021-12-11 19:29:32,981 iteration 132 : loss : 0.362506, loss_ce: 0.169914
2021-12-11 19:29:34,408 iteration 133 : loss : 0.366651, loss_ce: 0.176052
2021-12-11 19:29:35,795 iteration 134 : loss : 0.379141, loss_ce: 0.186816
2021-12-11 19:29:37,200 iteration 135 : loss : 0.377481, loss_ce: 0.197722
2021-12-11 19:29:38,564 iteration 136 : loss : 0.383235, loss_ce: 0.207935
  2%|▌                              | 8/400 [03:19<2:41:49, 24.77s/it]2021-12-11 19:29:39,929 iteration 137 : loss : 0.354297, loss_ce: 0.169526
2021-12-11 19:29:41,259 iteration 138 : loss : 0.386536, loss_ce: 0.227967
2021-12-11 19:29:42,694 iteration 139 : loss : 0.359178, loss_ce: 0.169631
2021-12-11 19:29:44,159 iteration 140 : loss : 0.375733, loss_ce: 0.174408
2021-12-11 19:29:45,548 iteration 141 : loss : 0.370169, loss_ce: 0.172116
2021-12-11 19:29:46,871 iteration 142 : loss : 0.370329, loss_ce: 0.183858
2021-12-11 19:29:48,199 iteration 143 : loss : 0.372681, loss_ce: 0.182921
2021-12-11 19:29:49,623 iteration 144 : loss : 0.373854, loss_ce: 0.184646
2021-12-11 19:29:51,167 iteration 145 : loss : 0.382255, loss_ce: 0.211432
2021-12-11 19:29:52,525 iteration 146 : loss : 0.371431, loss_ce: 0.178405
2021-12-11 19:29:53,893 iteration 147 : loss : 0.373380, loss_ce: 0.192329
2021-12-11 19:29:55,277 iteration 148 : loss : 0.385746, loss_ce: 0.203270
2021-12-11 19:29:56,670 iteration 149 : loss : 0.366427, loss_ce: 0.177054
2021-12-11 19:29:58,042 iteration 150 : loss : 0.389783, loss_ce: 0.216655
2021-12-11 19:29:59,424 iteration 151 : loss : 0.376802, loss_ce: 0.197572
2021-12-11 19:30:00,865 iteration 152 : loss : 0.371247, loss_ce: 0.192687
2021-12-11 19:30:02,307 iteration 153 : loss : 0.356665, loss_ce: 0.191091
  2%|▋                              | 9/400 [03:43<2:39:19, 24.45s/it]2021-12-11 19:30:03,745 iteration 154 : loss : 0.363324, loss_ce: 0.177853
2021-12-11 19:30:05,142 iteration 155 : loss : 0.369453, loss_ce: 0.186587
2021-12-11 19:30:06,535 iteration 156 : loss : 0.375064, loss_ce: 0.198042
2021-12-11 19:30:07,942 iteration 157 : loss : 0.371212, loss_ce: 0.184970
2021-12-11 19:30:09,338 iteration 158 : loss : 0.374386, loss_ce: 0.192144
2021-12-11 19:30:10,715 iteration 159 : loss : 0.364066, loss_ce: 0.178311
2021-12-11 19:30:12,076 iteration 160 : loss : 0.377256, loss_ce: 0.167488
2021-12-11 19:30:13,481 iteration 161 : loss : 0.362167, loss_ce: 0.181910
2021-12-11 19:30:14,940 iteration 162 : loss : 0.371982, loss_ce: 0.193204
2021-12-11 19:30:16,237 iteration 163 : loss : 0.355271, loss_ce: 0.176088
2021-12-11 19:30:17,677 iteration 164 : loss : 0.354554, loss_ce: 0.168863
2021-12-11 19:30:19,045 iteration 165 : loss : 0.349035, loss_ce: 0.172257
2021-12-11 19:30:20,488 iteration 166 : loss : 0.377879, loss_ce: 0.189655
2021-12-11 19:30:21,922 iteration 167 : loss : 0.376086, loss_ce: 0.194525
2021-12-11 19:30:23,350 iteration 168 : loss : 0.366947, loss_ce: 0.185991
2021-12-11 19:30:24,739 iteration 169 : loss : 0.368674, loss_ce: 0.192567
2021-12-11 19:30:24,739 Training Data Eval:
2021-12-11 19:30:31,538   Average segmentation loss on training set: 0.3554
2021-12-11 19:30:31,539 Validation Data Eval:
2021-12-11 19:30:33,899   Average segmentation loss on validation set: 0.3766
2021-12-11 19:30:35,893 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:30:37,258 iteration 170 : loss : 0.369960, loss_ce: 0.190665
  2%|▊                             | 10/400 [04:18<2:59:59, 27.69s/it]2021-12-11 19:30:38,712 iteration 171 : loss : 0.367139, loss_ce: 0.198424
2021-12-11 19:30:39,987 iteration 172 : loss : 0.367284, loss_ce: 0.165809
2021-12-11 19:30:41,297 iteration 173 : loss : 0.352948, loss_ce: 0.175130
2021-12-11 19:30:42,738 iteration 174 : loss : 0.374178, loss_ce: 0.175268
2021-12-11 19:30:44,090 iteration 175 : loss : 0.380351, loss_ce: 0.193479
2021-12-11 19:30:45,556 iteration 176 : loss : 0.360450, loss_ce: 0.176525
2021-12-11 19:30:46,928 iteration 177 : loss : 0.372462, loss_ce: 0.163408
2021-12-11 19:30:48,363 iteration 178 : loss : 0.354570, loss_ce: 0.157345
2021-12-11 19:30:49,764 iteration 179 : loss : 0.361750, loss_ce: 0.185085
2021-12-11 19:30:51,172 iteration 180 : loss : 0.363648, loss_ce: 0.177674
2021-12-11 19:30:52,587 iteration 181 : loss : 0.350937, loss_ce: 0.162795
2021-12-11 19:30:53,997 iteration 182 : loss : 0.350953, loss_ce: 0.169116
2021-12-11 19:30:55,422 iteration 183 : loss : 0.364028, loss_ce: 0.167889
2021-12-11 19:30:56,805 iteration 184 : loss : 0.363773, loss_ce: 0.193586
2021-12-11 19:30:58,158 iteration 185 : loss : 0.370582, loss_ce: 0.195396
2021-12-11 19:30:59,496 iteration 186 : loss : 0.367317, loss_ce: 0.195512
2021-12-11 19:31:00,817 iteration 187 : loss : 0.381453, loss_ce: 0.201159
  3%|▊                             | 11/400 [04:42<2:51:19, 26.43s/it]2021-12-11 19:31:02,267 iteration 188 : loss : 0.375308, loss_ce: 0.172933
2021-12-11 19:31:03,683 iteration 189 : loss : 0.375707, loss_ce: 0.188422
2021-12-11 19:31:05,116 iteration 190 : loss : 0.355637, loss_ce: 0.168970
2021-12-11 19:31:06,511 iteration 191 : loss : 0.369994, loss_ce: 0.172448
2021-12-11 19:31:07,813 iteration 192 : loss : 0.348240, loss_ce: 0.174968
2021-12-11 19:31:09,142 iteration 193 : loss : 0.374769, loss_ce: 0.171387
2021-12-11 19:31:10,465 iteration 194 : loss : 0.365367, loss_ce: 0.173506
2021-12-11 19:31:11,925 iteration 195 : loss : 0.350715, loss_ce: 0.162892
2021-12-11 19:31:13,338 iteration 196 : loss : 0.355720, loss_ce: 0.154697
2021-12-11 19:31:14,733 iteration 197 : loss : 0.351864, loss_ce: 0.177452
2021-12-11 19:31:16,147 iteration 198 : loss : 0.367587, loss_ce: 0.193768
2021-12-11 19:31:17,555 iteration 199 : loss : 0.374296, loss_ce: 0.203642
2021-12-11 19:31:18,917 iteration 200 : loss : 0.358578, loss_ce: 0.166604
2021-12-11 19:31:20,288 iteration 201 : loss : 0.364822, loss_ce: 0.178205
2021-12-11 19:31:21,565 iteration 202 : loss : 0.338002, loss_ce: 0.146078
2021-12-11 19:31:22,994 iteration 203 : loss : 0.352982, loss_ce: 0.175157
2021-12-11 19:31:24,359 iteration 204 : loss : 0.369293, loss_ce: 0.202155
  3%|▉                             | 12/400 [05:05<2:45:13, 25.55s/it]2021-12-11 19:31:25,861 iteration 205 : loss : 0.352680, loss_ce: 0.165322
2021-12-11 19:31:27,257 iteration 206 : loss : 0.377145, loss_ce: 0.169910
2021-12-11 19:31:28,716 iteration 207 : loss : 0.374755, loss_ce: 0.173974
2021-12-11 19:31:30,160 iteration 208 : loss : 0.375795, loss_ce: 0.197724
2021-12-11 19:31:31,559 iteration 209 : loss : 0.351765, loss_ce: 0.162583
2021-12-11 19:31:32,941 iteration 210 : loss : 0.367458, loss_ce: 0.192753
2021-12-11 19:31:34,331 iteration 211 : loss : 0.343934, loss_ce: 0.171508
2021-12-11 19:31:35,773 iteration 212 : loss : 0.358653, loss_ce: 0.189359
2021-12-11 19:31:37,200 iteration 213 : loss : 0.363262, loss_ce: 0.197829
2021-12-11 19:31:38,570 iteration 214 : loss : 0.363268, loss_ce: 0.170248
2021-12-11 19:31:39,912 iteration 215 : loss : 0.354156, loss_ce: 0.160282
2021-12-11 19:31:41,349 iteration 216 : loss : 0.347741, loss_ce: 0.163379
2021-12-11 19:31:42,789 iteration 217 : loss : 0.363076, loss_ce: 0.167042
2021-12-11 19:31:44,162 iteration 218 : loss : 0.359019, loss_ce: 0.184809
2021-12-11 19:31:45,524 iteration 219 : loss : 0.369581, loss_ce: 0.182389
2021-12-11 19:31:46,925 iteration 220 : loss : 0.355243, loss_ce: 0.168255
2021-12-11 19:31:48,349 iteration 221 : loss : 0.349554, loss_ce: 0.157603
  3%|▉                             | 13/400 [05:29<2:41:43, 25.07s/it]2021-12-11 19:31:49,786 iteration 222 : loss : 0.358674, loss_ce: 0.172612
2021-12-11 19:31:51,151 iteration 223 : loss : 0.369494, loss_ce: 0.185372
2021-12-11 19:31:52,519 iteration 224 : loss : 0.352299, loss_ce: 0.149126
2021-12-11 19:31:54,019 iteration 225 : loss : 0.354139, loss_ce: 0.180785
2021-12-11 19:31:55,432 iteration 226 : loss : 0.350230, loss_ce: 0.160235
2021-12-11 19:31:56,809 iteration 227 : loss : 0.361156, loss_ce: 0.167500
2021-12-11 19:31:58,177 iteration 228 : loss : 0.354971, loss_ce: 0.161601
2021-12-11 19:31:59,585 iteration 229 : loss : 0.347751, loss_ce: 0.159668
2021-12-11 19:32:00,932 iteration 230 : loss : 0.352467, loss_ce: 0.176917
2021-12-11 19:32:02,309 iteration 231 : loss : 0.351487, loss_ce: 0.171533
2021-12-11 19:32:03,679 iteration 232 : loss : 0.343668, loss_ce: 0.165007
2021-12-11 19:32:04,988 iteration 233 : loss : 0.358185, loss_ce: 0.188921
2021-12-11 19:32:06,355 iteration 234 : loss : 0.371421, loss_ce: 0.170419
2021-12-11 19:32:07,692 iteration 235 : loss : 0.353059, loss_ce: 0.165447
2021-12-11 19:32:09,062 iteration 236 : loss : 0.393449, loss_ce: 0.191200
2021-12-11 19:32:10,387 iteration 237 : loss : 0.384706, loss_ce: 0.206721
2021-12-11 19:32:11,803 iteration 238 : loss : 0.340835, loss_ce: 0.138710
  4%|█                             | 14/400 [05:53<2:38:11, 24.59s/it]2021-12-11 19:32:13,242 iteration 239 : loss : 0.352661, loss_ce: 0.155327
2021-12-11 19:32:14,703 iteration 240 : loss : 0.352209, loss_ce: 0.155068
2021-12-11 19:32:16,123 iteration 241 : loss : 0.366861, loss_ce: 0.180671
2021-12-11 19:32:17,498 iteration 242 : loss : 0.349954, loss_ce: 0.154346
2021-12-11 19:32:18,857 iteration 243 : loss : 0.342596, loss_ce: 0.153452
2021-12-11 19:32:20,228 iteration 244 : loss : 0.347364, loss_ce: 0.168249
2021-12-11 19:32:21,559 iteration 245 : loss : 0.336371, loss_ce: 0.140052
2021-12-11 19:32:22,922 iteration 246 : loss : 0.350796, loss_ce: 0.159625
2021-12-11 19:32:24,325 iteration 247 : loss : 0.359554, loss_ce: 0.173508
2021-12-11 19:32:25,738 iteration 248 : loss : 0.351682, loss_ce: 0.187315
2021-12-11 19:32:27,113 iteration 249 : loss : 0.347849, loss_ce: 0.159378
2021-12-11 19:32:28,484 iteration 250 : loss : 0.340503, loss_ce: 0.160852
2021-12-11 19:32:29,812 iteration 251 : loss : 0.339595, loss_ce: 0.168991
2021-12-11 19:32:31,267 iteration 252 : loss : 0.364547, loss_ce: 0.198128
2021-12-11 19:32:32,661 iteration 253 : loss : 0.341861, loss_ce: 0.149261
2021-12-11 19:32:34,020 iteration 254 : loss : 0.371007, loss_ce: 0.185242
2021-12-11 19:32:34,020 Training Data Eval:
2021-12-11 19:32:40,840   Average segmentation loss on training set: 0.3404
2021-12-11 19:32:40,840 Validation Data Eval:
2021-12-11 19:32:43,198   Average segmentation loss on validation set: 0.3657
2021-12-11 19:32:45,262 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:32:46,611 iteration 255 : loss : 0.369461, loss_ce: 0.186023
  4%|█▏                            | 15/400 [06:27<2:57:32, 27.67s/it]2021-12-11 19:32:47,989 iteration 256 : loss : 0.352253, loss_ce: 0.155935
2021-12-11 19:32:49,321 iteration 257 : loss : 0.366291, loss_ce: 0.180706
2021-12-11 19:32:50,740 iteration 258 : loss : 0.364876, loss_ce: 0.181129
2021-12-11 19:32:52,128 iteration 259 : loss : 0.344967, loss_ce: 0.161097
2021-12-11 19:32:53,468 iteration 260 : loss : 0.379001, loss_ce: 0.216544
2021-12-11 19:32:54,865 iteration 261 : loss : 0.348505, loss_ce: 0.161628
2021-12-11 19:32:56,298 iteration 262 : loss : 0.379776, loss_ce: 0.182752
2021-12-11 19:32:57,632 iteration 263 : loss : 0.335042, loss_ce: 0.149694
2021-12-11 19:32:59,060 iteration 264 : loss : 0.351078, loss_ce: 0.147176
2021-12-11 19:33:00,540 iteration 265 : loss : 0.396707, loss_ce: 0.225772
2021-12-11 19:33:01,947 iteration 266 : loss : 0.357123, loss_ce: 0.152483
2021-12-11 19:33:03,417 iteration 267 : loss : 0.340918, loss_ce: 0.151145
2021-12-11 19:33:04,826 iteration 268 : loss : 0.356914, loss_ce: 0.179217
2021-12-11 19:33:06,222 iteration 269 : loss : 0.370288, loss_ce: 0.182113
2021-12-11 19:33:07,590 iteration 270 : loss : 0.349069, loss_ce: 0.159615
2021-12-11 19:33:08,947 iteration 271 : loss : 0.372227, loss_ce: 0.184780
2021-12-11 19:33:10,277 iteration 272 : loss : 0.352469, loss_ce: 0.163829
  4%|█▏                            | 16/400 [06:51<2:49:23, 26.47s/it]2021-12-11 19:33:11,764 iteration 273 : loss : 0.348306, loss_ce: 0.146264
2021-12-11 19:33:13,152 iteration 274 : loss : 0.370250, loss_ce: 0.179322
2021-12-11 19:33:14,526 iteration 275 : loss : 0.352703, loss_ce: 0.124771
2021-12-11 19:33:15,882 iteration 276 : loss : 0.335601, loss_ce: 0.157172
2021-12-11 19:33:17,238 iteration 277 : loss : 0.336811, loss_ce: 0.156790
2021-12-11 19:33:18,625 iteration 278 : loss : 0.368790, loss_ce: 0.174648
2021-12-11 19:33:19,997 iteration 279 : loss : 0.358919, loss_ce: 0.171563
2021-12-11 19:33:21,372 iteration 280 : loss : 0.366679, loss_ce: 0.184734
2021-12-11 19:33:22,800 iteration 281 : loss : 0.360546, loss_ce: 0.189561
2021-12-11 19:33:24,202 iteration 282 : loss : 0.337017, loss_ce: 0.144588
2021-12-11 19:33:25,556 iteration 283 : loss : 0.353172, loss_ce: 0.162827
2021-12-11 19:33:26,959 iteration 284 : loss : 0.371608, loss_ce: 0.204001
2021-12-11 19:33:28,384 iteration 285 : loss : 0.353184, loss_ce: 0.147564
2021-12-11 19:33:29,681 iteration 286 : loss : 0.344383, loss_ce: 0.162337
2021-12-11 19:33:31,056 iteration 287 : loss : 0.349109, loss_ce: 0.171550
2021-12-11 19:33:32,454 iteration 288 : loss : 0.336620, loss_ce: 0.162235
2021-12-11 19:33:33,822 iteration 289 : loss : 0.349985, loss_ce: 0.159803
  4%|█▎                            | 17/400 [07:15<2:43:19, 25.59s/it]2021-12-11 19:33:35,301 iteration 290 : loss : 0.347720, loss_ce: 0.171591
2021-12-11 19:33:36,649 iteration 291 : loss : 0.339106, loss_ce: 0.151179
2021-12-11 19:33:37,979 iteration 292 : loss : 0.350253, loss_ce: 0.170768
2021-12-11 19:33:39,411 iteration 293 : loss : 0.335412, loss_ce: 0.148054
2021-12-11 19:33:40,774 iteration 294 : loss : 0.352095, loss_ce: 0.177600
2021-12-11 19:33:42,202 iteration 295 : loss : 0.365539, loss_ce: 0.183675
2021-12-11 19:33:43,636 iteration 296 : loss : 0.360770, loss_ce: 0.182853
2021-12-11 19:33:45,034 iteration 297 : loss : 0.348771, loss_ce: 0.169464
2021-12-11 19:33:46,402 iteration 298 : loss : 0.354017, loss_ce: 0.186195
2021-12-11 19:33:47,722 iteration 299 : loss : 0.353014, loss_ce: 0.154315
2021-12-11 19:33:49,072 iteration 300 : loss : 0.338803, loss_ce: 0.157502
2021-12-11 19:33:50,364 iteration 301 : loss : 0.340551, loss_ce: 0.153623
2021-12-11 19:33:51,794 iteration 302 : loss : 0.344544, loss_ce: 0.150194
2021-12-11 19:33:53,224 iteration 303 : loss : 0.335422, loss_ce: 0.141432
2021-12-11 19:33:54,650 iteration 304 : loss : 0.355818, loss_ce: 0.173399
2021-12-11 19:33:56,003 iteration 305 : loss : 0.344377, loss_ce: 0.159761
2021-12-11 19:33:57,373 iteration 306 : loss : 0.345967, loss_ce: 0.153129
  4%|█▎                            | 18/400 [07:38<2:39:00, 24.98s/it]2021-12-11 19:33:58,748 iteration 307 : loss : 0.354463, loss_ce: 0.149034
2021-12-11 19:34:00,056 iteration 308 : loss : 0.347250, loss_ce: 0.170808
2021-12-11 19:34:01,375 iteration 309 : loss : 0.365470, loss_ce: 0.189898
2021-12-11 19:34:02,759 iteration 310 : loss : 0.355110, loss_ce: 0.168795
2021-12-11 19:34:04,170 iteration 311 : loss : 0.346537, loss_ce: 0.155906
2021-12-11 19:34:05,601 iteration 312 : loss : 0.334440, loss_ce: 0.148451
2021-12-11 19:34:06,957 iteration 313 : loss : 0.352879, loss_ce: 0.165183
2021-12-11 19:34:08,348 iteration 314 : loss : 0.353444, loss_ce: 0.185459
2021-12-11 19:34:09,788 iteration 315 : loss : 0.359517, loss_ce: 0.164524
2021-12-11 19:34:11,171 iteration 316 : loss : 0.350848, loss_ce: 0.164099
2021-12-11 19:34:12,501 iteration 317 : loss : 0.336297, loss_ce: 0.136107
2021-12-11 19:34:13,855 iteration 318 : loss : 0.361060, loss_ce: 0.166615
2021-12-11 19:34:15,296 iteration 319 : loss : 0.327050, loss_ce: 0.144224
2021-12-11 19:34:16,650 iteration 320 : loss : 0.344124, loss_ce: 0.146727
2021-12-11 19:34:17,977 iteration 321 : loss : 0.362258, loss_ce: 0.188148
2021-12-11 19:34:19,353 iteration 322 : loss : 0.350937, loss_ce: 0.166792
2021-12-11 19:34:20,771 iteration 323 : loss : 0.350877, loss_ce: 0.163627
  5%|█▍                            | 19/400 [08:02<2:35:35, 24.50s/it]2021-12-11 19:34:22,141 iteration 324 : loss : 0.342756, loss_ce: 0.134350
2021-12-11 19:34:23,515 iteration 325 : loss : 0.352702, loss_ce: 0.157548
2021-12-11 19:34:24,878 iteration 326 : loss : 0.325035, loss_ce: 0.151064
2021-12-11 19:34:26,330 iteration 327 : loss : 0.358926, loss_ce: 0.169680
2021-12-11 19:34:27,744 iteration 328 : loss : 0.347477, loss_ce: 0.170753
2021-12-11 19:34:29,131 iteration 329 : loss : 0.335656, loss_ce: 0.148428
2021-12-11 19:34:30,420 iteration 330 : loss : 0.344019, loss_ce: 0.164835
2021-12-11 19:34:31,854 iteration 331 : loss : 0.320298, loss_ce: 0.130652
2021-12-11 19:34:33,278 iteration 332 : loss : 0.330614, loss_ce: 0.162610
2021-12-11 19:34:34,610 iteration 333 : loss : 0.356941, loss_ce: 0.159258
2021-12-11 19:34:36,029 iteration 334 : loss : 0.338166, loss_ce: 0.165831
2021-12-11 19:34:37,373 iteration 335 : loss : 0.343397, loss_ce: 0.140642
2021-12-11 19:34:38,792 iteration 336 : loss : 0.331793, loss_ce: 0.146394
2021-12-11 19:34:40,137 iteration 337 : loss : 0.332501, loss_ce: 0.153968
2021-12-11 19:34:41,441 iteration 338 : loss : 0.354006, loss_ce: 0.181743
2021-12-11 19:34:42,825 iteration 339 : loss : 0.333448, loss_ce: 0.147036
2021-12-11 19:34:42,825 Training Data Eval:
2021-12-11 19:34:49,634   Average segmentation loss on training set: 0.3289
2021-12-11 19:34:49,634 Validation Data Eval:
2021-12-11 19:34:51,997   Average segmentation loss on validation set: 0.3492
2021-12-11 19:34:53,939 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:34:55,300 iteration 340 : loss : 0.341135, loss_ce: 0.156295
  5%|█▌                            | 20/400 [08:36<2:54:13, 27.51s/it]2021-12-11 19:34:56,784 iteration 341 : loss : 0.375259, loss_ce: 0.183236
2021-12-11 19:34:58,227 iteration 342 : loss : 0.345907, loss_ce: 0.149290
2021-12-11 19:34:59,724 iteration 343 : loss : 0.375011, loss_ce: 0.194022
2021-12-11 19:35:01,097 iteration 344 : loss : 0.351999, loss_ce: 0.160823
2021-12-11 19:35:02,489 iteration 345 : loss : 0.342543, loss_ce: 0.165528
2021-12-11 19:35:03,951 iteration 346 : loss : 0.354414, loss_ce: 0.154240
2021-12-11 19:35:05,308 iteration 347 : loss : 0.341263, loss_ce: 0.159368
2021-12-11 19:35:06,800 iteration 348 : loss : 0.332549, loss_ce: 0.151709
2021-12-11 19:35:08,178 iteration 349 : loss : 0.359262, loss_ce: 0.179313
2021-12-11 19:35:09,477 iteration 350 : loss : 0.355588, loss_ce: 0.150331
2021-12-11 19:35:10,928 iteration 351 : loss : 0.346072, loss_ce: 0.150135
2021-12-11 19:35:12,313 iteration 352 : loss : 0.335130, loss_ce: 0.152674
2021-12-11 19:35:13,787 iteration 353 : loss : 0.330450, loss_ce: 0.141404
2021-12-11 19:35:15,200 iteration 354 : loss : 0.344916, loss_ce: 0.166218
2021-12-11 19:35:16,650 iteration 355 : loss : 0.339061, loss_ce: 0.158724
2021-12-11 19:35:17,997 iteration 356 : loss : 0.345643, loss_ce: 0.168091
2021-12-11 19:35:19,358 iteration 357 : loss : 0.351058, loss_ce: 0.163153
  5%|█▌                            | 21/400 [09:00<2:47:14, 26.48s/it]2021-12-11 19:35:20,862 iteration 358 : loss : 0.351274, loss_ce: 0.143097
2021-12-11 19:35:22,305 iteration 359 : loss : 0.347461, loss_ce: 0.176735
2021-12-11 19:35:23,704 iteration 360 : loss : 0.314209, loss_ce: 0.111334
2021-12-11 19:35:25,121 iteration 361 : loss : 0.346391, loss_ce: 0.147798
2021-12-11 19:35:26,426 iteration 362 : loss : 0.332791, loss_ce: 0.145169
2021-12-11 19:35:27,873 iteration 363 : loss : 0.357039, loss_ce: 0.190459
2021-12-11 19:35:29,271 iteration 364 : loss : 0.378122, loss_ce: 0.201300
2021-12-11 19:35:30,638 iteration 365 : loss : 0.329082, loss_ce: 0.145892
2021-12-11 19:35:32,016 iteration 366 : loss : 0.340145, loss_ce: 0.164233
2021-12-11 19:35:33,446 iteration 367 : loss : 0.353147, loss_ce: 0.174809
2021-12-11 19:35:34,781 iteration 368 : loss : 0.332555, loss_ce: 0.134669
2021-12-11 19:35:36,188 iteration 369 : loss : 0.334885, loss_ce: 0.152246
2021-12-11 19:35:37,497 iteration 370 : loss : 0.327013, loss_ce: 0.126662
2021-12-11 19:35:38,938 iteration 371 : loss : 0.345246, loss_ce: 0.165124
2021-12-11 19:35:40,375 iteration 372 : loss : 0.352325, loss_ce: 0.158998
2021-12-11 19:35:41,784 iteration 373 : loss : 0.317091, loss_ce: 0.146060
2021-12-11 19:35:43,143 iteration 374 : loss : 0.334087, loss_ce: 0.160706
  6%|█▋                            | 22/400 [09:24<2:41:42, 25.67s/it]2021-12-11 19:35:44,628 iteration 375 : loss : 0.324206, loss_ce: 0.146877
2021-12-11 19:35:46,047 iteration 376 : loss : 0.342596, loss_ce: 0.162525
2021-12-11 19:35:47,468 iteration 377 : loss : 0.334690, loss_ce: 0.147667
2021-12-11 19:35:48,872 iteration 378 : loss : 0.343723, loss_ce: 0.149424
2021-12-11 19:35:50,207 iteration 379 : loss : 0.336945, loss_ce: 0.138565
2021-12-11 19:35:51,628 iteration 380 : loss : 0.348202, loss_ce: 0.168481
2021-12-11 19:35:52,928 iteration 381 : loss : 0.326496, loss_ce: 0.124438
2021-12-11 19:35:54,417 iteration 382 : loss : 0.359737, loss_ce: 0.171334
2021-12-11 19:35:55,858 iteration 383 : loss : 0.339819, loss_ce: 0.148016
2021-12-11 19:35:57,254 iteration 384 : loss : 0.353119, loss_ce: 0.164917
2021-12-11 19:35:58,619 iteration 385 : loss : 0.338286, loss_ce: 0.154849
2021-12-11 19:36:00,036 iteration 386 : loss : 0.349722, loss_ce: 0.161155
2021-12-11 19:36:01,446 iteration 387 : loss : 0.334088, loss_ce: 0.150476
2021-12-11 19:36:02,887 iteration 388 : loss : 0.343918, loss_ce: 0.172010
2021-12-11 19:36:04,254 iteration 389 : loss : 0.336471, loss_ce: 0.132314
2021-12-11 19:36:05,675 iteration 390 : loss : 0.344353, loss_ce: 0.165660
2021-12-11 19:36:07,030 iteration 391 : loss : 0.320744, loss_ce: 0.142326
  6%|█▋                            | 23/400 [09:48<2:37:55, 25.13s/it]2021-12-11 19:36:08,589 iteration 392 : loss : 0.331557, loss_ce: 0.155076
2021-12-11 19:36:09,941 iteration 393 : loss : 0.358227, loss_ce: 0.176048
2021-12-11 19:36:11,346 iteration 394 : loss : 0.334504, loss_ce: 0.142542
2021-12-11 19:36:12,712 iteration 395 : loss : 0.355876, loss_ce: 0.180599
2021-12-11 19:36:14,139 iteration 396 : loss : 0.330430, loss_ce: 0.152352
2021-12-11 19:36:15,559 iteration 397 : loss : 0.334148, loss_ce: 0.152238
2021-12-11 19:36:16,966 iteration 398 : loss : 0.341200, loss_ce: 0.162855
2021-12-11 19:36:18,350 iteration 399 : loss : 0.324737, loss_ce: 0.137718
2021-12-11 19:36:19,787 iteration 400 : loss : 0.338882, loss_ce: 0.166165
2021-12-11 19:36:21,113 iteration 401 : loss : 0.335315, loss_ce: 0.137265
2021-12-11 19:36:22,551 iteration 402 : loss : 0.340049, loss_ce: 0.153144
2021-12-11 19:36:24,021 iteration 403 : loss : 0.327696, loss_ce: 0.155901
2021-12-11 19:36:25,420 iteration 404 : loss : 0.309385, loss_ce: 0.113444
2021-12-11 19:36:26,853 iteration 405 : loss : 0.355750, loss_ce: 0.147517
2021-12-11 19:36:28,219 iteration 406 : loss : 0.330760, loss_ce: 0.168350
2021-12-11 19:36:29,565 iteration 407 : loss : 0.331888, loss_ce: 0.143680
2021-12-11 19:36:30,926 iteration 408 : loss : 0.333584, loss_ce: 0.144957
  6%|█▊                            | 24/400 [10:12<2:35:10, 24.76s/it]2021-12-11 19:36:32,493 iteration 409 : loss : 0.317664, loss_ce: 0.142031
2021-12-11 19:36:33,882 iteration 410 : loss : 0.334514, loss_ce: 0.144614
2021-12-11 19:36:35,295 iteration 411 : loss : 0.325437, loss_ce: 0.137673
2021-12-11 19:36:36,672 iteration 412 : loss : 0.328579, loss_ce: 0.151086
2021-12-11 19:36:38,042 iteration 413 : loss : 0.328981, loss_ce: 0.147861
2021-12-11 19:36:39,442 iteration 414 : loss : 0.329616, loss_ce: 0.158125
2021-12-11 19:36:40,879 iteration 415 : loss : 0.360737, loss_ce: 0.206628
2021-12-11 19:36:42,156 iteration 416 : loss : 0.321544, loss_ce: 0.127513
2021-12-11 19:36:43,535 iteration 417 : loss : 0.325180, loss_ce: 0.152557
2021-12-11 19:36:44,937 iteration 418 : loss : 0.318601, loss_ce: 0.133839
2021-12-11 19:36:46,305 iteration 419 : loss : 0.341468, loss_ce: 0.143281
2021-12-11 19:36:47,692 iteration 420 : loss : 0.326307, loss_ce: 0.146216
2021-12-11 19:36:48,986 iteration 421 : loss : 0.298475, loss_ce: 0.113302
2021-12-11 19:36:50,402 iteration 422 : loss : 0.323344, loss_ce: 0.129852
2021-12-11 19:36:51,758 iteration 423 : loss : 0.320625, loss_ce: 0.142173
2021-12-11 19:36:53,117 iteration 424 : loss : 0.336320, loss_ce: 0.169339
2021-12-11 19:36:53,117 Training Data Eval:
2021-12-11 19:36:59,933   Average segmentation loss on training set: 0.3124
2021-12-11 19:36:59,934 Validation Data Eval:
2021-12-11 19:37:02,294   Average segmentation loss on validation set: 0.3312
2021-12-11 19:37:04,386 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:37:05,847 iteration 425 : loss : 0.368315, loss_ce: 0.182766
  6%|█▉                            | 25/400 [10:47<2:53:48, 27.81s/it]2021-12-11 19:37:07,325 iteration 426 : loss : 0.332561, loss_ce: 0.144063
2021-12-11 19:37:08,662 iteration 427 : loss : 0.337159, loss_ce: 0.166351
2021-12-11 19:37:10,136 iteration 428 : loss : 0.338745, loss_ce: 0.152399
2021-12-11 19:37:11,488 iteration 429 : loss : 0.304790, loss_ce: 0.119906
2021-12-11 19:37:12,790 iteration 430 : loss : 0.343606, loss_ce: 0.168507
2021-12-11 19:37:14,193 iteration 431 : loss : 0.314814, loss_ce: 0.161364
2021-12-11 19:37:15,593 iteration 432 : loss : 0.317463, loss_ce: 0.127443
2021-12-11 19:37:16,913 iteration 433 : loss : 0.319497, loss_ce: 0.147043
2021-12-11 19:37:18,247 iteration 434 : loss : 0.316256, loss_ce: 0.133745
2021-12-11 19:37:19,647 iteration 435 : loss : 0.316223, loss_ce: 0.142797
2021-12-11 19:37:21,059 iteration 436 : loss : 0.307664, loss_ce: 0.118258
2021-12-11 19:37:22,410 iteration 437 : loss : 0.321699, loss_ce: 0.153888
2021-12-11 19:37:23,753 iteration 438 : loss : 0.331733, loss_ce: 0.151457
2021-12-11 19:37:25,116 iteration 439 : loss : 0.325926, loss_ce: 0.146342
2021-12-11 19:37:26,619 iteration 440 : loss : 0.328100, loss_ce: 0.156632
2021-12-11 19:37:27,988 iteration 441 : loss : 0.334130, loss_ce: 0.160092
2021-12-11 19:37:29,337 iteration 442 : loss : 0.312334, loss_ce: 0.136354
  6%|█▉                            | 26/400 [11:10<2:45:16, 26.51s/it]2021-12-11 19:37:30,810 iteration 443 : loss : 0.325988, loss_ce: 0.148016
2021-12-11 19:37:32,150 iteration 444 : loss : 0.318139, loss_ce: 0.133783
2021-12-11 19:37:33,477 iteration 445 : loss : 0.342579, loss_ce: 0.156112
2021-12-11 19:37:34,838 iteration 446 : loss : 0.323446, loss_ce: 0.126662
2021-12-11 19:37:36,263 iteration 447 : loss : 0.306865, loss_ce: 0.130689
2021-12-11 19:37:37,572 iteration 448 : loss : 0.351712, loss_ce: 0.133794
2021-12-11 19:37:38,929 iteration 449 : loss : 0.299506, loss_ce: 0.112690
2021-12-11 19:37:40,234 iteration 450 : loss : 0.320172, loss_ce: 0.160077
2021-12-11 19:37:41,600 iteration 451 : loss : 0.336615, loss_ce: 0.153239
2021-12-11 19:37:42,982 iteration 452 : loss : 0.330868, loss_ce: 0.135855
2021-12-11 19:37:44,334 iteration 453 : loss : 0.293693, loss_ce: 0.128761
2021-12-11 19:37:45,801 iteration 454 : loss : 0.326275, loss_ce: 0.158617
2021-12-11 19:37:47,226 iteration 455 : loss : 0.328860, loss_ce: 0.160785
2021-12-11 19:37:48,592 iteration 456 : loss : 0.318532, loss_ce: 0.127619
2021-12-11 19:37:49,956 iteration 457 : loss : 0.342658, loss_ce: 0.174504
2021-12-11 19:37:51,316 iteration 458 : loss : 0.333919, loss_ce: 0.157817
2021-12-11 19:37:52,699 iteration 459 : loss : 0.327707, loss_ce: 0.148605
  7%|██                            | 27/400 [11:34<2:38:56, 25.57s/it]2021-12-11 19:37:54,070 iteration 460 : loss : 0.341144, loss_ce: 0.167764
2021-12-11 19:37:55,421 iteration 461 : loss : 0.323558, loss_ce: 0.151683
2021-12-11 19:37:56,860 iteration 462 : loss : 0.329631, loss_ce: 0.156185
2021-12-11 19:37:58,242 iteration 463 : loss : 0.305804, loss_ce: 0.122915
2021-12-11 19:37:59,749 iteration 464 : loss : 0.320846, loss_ce: 0.129166
2021-12-11 19:38:01,171 iteration 465 : loss : 0.317641, loss_ce: 0.157195
2021-12-11 19:38:02,531 iteration 466 : loss : 0.302390, loss_ce: 0.146348
2021-12-11 19:38:03,903 iteration 467 : loss : 0.350586, loss_ce: 0.161734
2021-12-11 19:38:05,257 iteration 468 : loss : 0.315921, loss_ce: 0.140846
2021-12-11 19:38:06,669 iteration 469 : loss : 0.318308, loss_ce: 0.149872
2021-12-11 19:38:08,048 iteration 470 : loss : 0.314042, loss_ce: 0.144007
2021-12-11 19:38:09,431 iteration 471 : loss : 0.398909, loss_ce: 0.207745
2021-12-11 19:38:10,842 iteration 472 : loss : 0.301261, loss_ce: 0.138191
2021-12-11 19:38:12,235 iteration 473 : loss : 0.307157, loss_ce: 0.142122
2021-12-11 19:38:13,582 iteration 474 : loss : 0.343732, loss_ce: 0.146346
2021-12-11 19:38:14,951 iteration 475 : loss : 0.324022, loss_ce: 0.136082
2021-12-11 19:38:16,402 iteration 476 : loss : 0.313627, loss_ce: 0.133084
  7%|██                            | 28/400 [11:57<2:35:03, 25.01s/it]2021-12-11 19:38:17,843 iteration 477 : loss : 0.320246, loss_ce: 0.151722
2021-12-11 19:38:19,168 iteration 478 : loss : 0.336046, loss_ce: 0.163570
2021-12-11 19:38:20,580 iteration 479 : loss : 0.296925, loss_ce: 0.129220
2021-12-11 19:38:21,955 iteration 480 : loss : 0.315845, loss_ce: 0.117521
2021-12-11 19:38:23,304 iteration 481 : loss : 0.288110, loss_ce: 0.104225
2021-12-11 19:38:24,756 iteration 482 : loss : 0.288210, loss_ce: 0.120913
2021-12-11 19:38:26,078 iteration 483 : loss : 0.336733, loss_ce: 0.167245
2021-12-11 19:38:27,465 iteration 484 : loss : 0.314000, loss_ce: 0.129760
2021-12-11 19:38:28,854 iteration 485 : loss : 0.314471, loss_ce: 0.147332
2021-12-11 19:38:30,334 iteration 486 : loss : 0.320430, loss_ce: 0.135044
2021-12-11 19:38:31,723 iteration 487 : loss : 0.309436, loss_ce: 0.122338
2021-12-11 19:38:33,033 iteration 488 : loss : 0.320764, loss_ce: 0.157493
2021-12-11 19:38:34,438 iteration 489 : loss : 0.320245, loss_ce: 0.159128
2021-12-11 19:38:35,813 iteration 490 : loss : 0.340041, loss_ce: 0.162628
2021-12-11 19:38:37,247 iteration 491 : loss : 0.290663, loss_ce: 0.105344
2021-12-11 19:38:38,599 iteration 492 : loss : 0.347420, loss_ce: 0.184013
2021-12-11 19:38:39,930 iteration 493 : loss : 0.327055, loss_ce: 0.161954
  7%|██▏                           | 29/400 [12:21<2:31:54, 24.57s/it]2021-12-11 19:38:41,362 iteration 494 : loss : 0.321947, loss_ce: 0.135428
2021-12-11 19:38:42,681 iteration 495 : loss : 0.292857, loss_ce: 0.121943
2021-12-11 19:38:44,070 iteration 496 : loss : 0.327959, loss_ce: 0.117324
2021-12-11 19:38:45,524 iteration 497 : loss : 0.315506, loss_ce: 0.142315
2021-12-11 19:38:47,007 iteration 498 : loss : 0.321426, loss_ce: 0.166666
2021-12-11 19:38:48,420 iteration 499 : loss : 0.306688, loss_ce: 0.119578
2021-12-11 19:38:49,929 iteration 500 : loss : 0.301932, loss_ce: 0.130304
2021-12-11 19:38:51,324 iteration 501 : loss : 0.311720, loss_ce: 0.121087
2021-12-11 19:38:52,656 iteration 502 : loss : 0.312753, loss_ce: 0.150920
2021-12-11 19:38:54,138 iteration 503 : loss : 0.294468, loss_ce: 0.110915
2021-12-11 19:38:55,635 iteration 504 : loss : 0.297450, loss_ce: 0.132083
2021-12-11 19:38:56,996 iteration 505 : loss : 0.285401, loss_ce: 0.135209
2021-12-11 19:38:58,406 iteration 506 : loss : 0.306231, loss_ce: 0.137130
2021-12-11 19:38:59,854 iteration 507 : loss : 0.298964, loss_ce: 0.134919
2021-12-11 19:39:01,234 iteration 508 : loss : 0.317224, loss_ce: 0.152018
2021-12-11 19:39:02,641 iteration 509 : loss : 0.303097, loss_ce: 0.152923
2021-12-11 19:39:02,641 Training Data Eval:
2021-12-11 19:39:09,469   Average segmentation loss on training set: 0.2879
2021-12-11 19:39:09,469 Validation Data Eval:
2021-12-11 19:39:11,826   Average segmentation loss on validation set: 0.3004
2021-12-11 19:39:13,762 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:39:15,070 iteration 510 : loss : 0.297511, loss_ce: 0.129522
  8%|██▎                           | 30/400 [12:56<2:51:01, 27.74s/it]2021-12-11 19:39:16,376 iteration 511 : loss : 0.296676, loss_ce: 0.129464
2021-12-11 19:39:17,893 iteration 512 : loss : 0.288017, loss_ce: 0.122462
2021-12-11 19:39:19,225 iteration 513 : loss : 0.320534, loss_ce: 0.158811
2021-12-11 19:39:20,636 iteration 514 : loss : 0.313664, loss_ce: 0.122897
2021-12-11 19:39:21,985 iteration 515 : loss : 0.282160, loss_ce: 0.119596
2021-12-11 19:39:23,287 iteration 516 : loss : 0.309259, loss_ce: 0.150184
2021-12-11 19:39:24,803 iteration 517 : loss : 0.330123, loss_ce: 0.141771
2021-12-11 19:39:26,214 iteration 518 : loss : 0.306385, loss_ce: 0.166159
2021-12-11 19:39:27,559 iteration 519 : loss : 0.310838, loss_ce: 0.134654
2021-12-11 19:39:28,898 iteration 520 : loss : 0.325341, loss_ce: 0.170987
2021-12-11 19:39:30,361 iteration 521 : loss : 0.303679, loss_ce: 0.112103
2021-12-11 19:39:31,709 iteration 522 : loss : 0.325377, loss_ce: 0.152646
2021-12-11 19:39:33,125 iteration 523 : loss : 0.293882, loss_ce: 0.131557
2021-12-11 19:39:34,394 iteration 524 : loss : 0.311181, loss_ce: 0.136326
2021-12-11 19:39:35,753 iteration 525 : loss : 0.307107, loss_ce: 0.131802
2021-12-11 19:39:37,159 iteration 526 : loss : 0.301343, loss_ce: 0.130396
2021-12-11 19:39:38,531 iteration 527 : loss : 0.311980, loss_ce: 0.132462
  8%|██▎                           | 31/400 [13:19<2:42:42, 26.46s/it]2021-12-11 19:39:40,018 iteration 528 : loss : 0.266357, loss_ce: 0.107193
2021-12-11 19:39:41,402 iteration 529 : loss : 0.300886, loss_ce: 0.136855
2021-12-11 19:39:42,751 iteration 530 : loss : 0.293420, loss_ce: 0.123193
2021-12-11 19:39:44,185 iteration 531 : loss : 0.302589, loss_ce: 0.130289
2021-12-11 19:39:45,503 iteration 532 : loss : 0.295315, loss_ce: 0.112472
2021-12-11 19:39:46,907 iteration 533 : loss : 0.281401, loss_ce: 0.133571
2021-12-11 19:39:48,295 iteration 534 : loss : 0.310037, loss_ce: 0.156607
2021-12-11 19:39:49,682 iteration 535 : loss : 0.308228, loss_ce: 0.138718
2021-12-11 19:39:51,003 iteration 536 : loss : 0.312234, loss_ce: 0.154924
2021-12-11 19:39:52,374 iteration 537 : loss : 0.315932, loss_ce: 0.138829
2021-12-11 19:39:53,735 iteration 538 : loss : 0.260619, loss_ce: 0.112133
2021-12-11 19:39:55,074 iteration 539 : loss : 0.304437, loss_ce: 0.124665
2021-12-11 19:39:56,398 iteration 540 : loss : 0.323805, loss_ce: 0.154900
2021-12-11 19:39:57,766 iteration 541 : loss : 0.316158, loss_ce: 0.169049
2021-12-11 19:39:59,074 iteration 542 : loss : 0.268709, loss_ce: 0.113310
2021-12-11 19:40:00,408 iteration 543 : loss : 0.296380, loss_ce: 0.109358
2021-12-11 19:40:01,769 iteration 544 : loss : 0.284477, loss_ce: 0.123356
  8%|██▍                           | 32/400 [13:43<2:36:21, 25.49s/it]2021-12-11 19:40:03,194 iteration 545 : loss : 0.297239, loss_ce: 0.146122
2021-12-11 19:40:04,576 iteration 546 : loss : 0.301464, loss_ce: 0.135223
2021-12-11 19:40:05,947 iteration 547 : loss : 0.300789, loss_ce: 0.138631
2021-12-11 19:40:07,256 iteration 548 : loss : 0.306418, loss_ce: 0.131877
2021-12-11 19:40:08,648 iteration 549 : loss : 0.292289, loss_ce: 0.136331
2021-12-11 19:40:10,123 iteration 550 : loss : 0.305820, loss_ce: 0.134968
2021-12-11 19:40:11,531 iteration 551 : loss : 0.325821, loss_ce: 0.125728
2021-12-11 19:40:12,951 iteration 552 : loss : 0.312210, loss_ce: 0.154108
2021-12-11 19:40:14,386 iteration 553 : loss : 0.291302, loss_ce: 0.142795
2021-12-11 19:40:15,717 iteration 554 : loss : 0.286897, loss_ce: 0.122076
2021-12-11 19:40:17,127 iteration 555 : loss : 0.268004, loss_ce: 0.129661
2021-12-11 19:40:18,587 iteration 556 : loss : 0.278551, loss_ce: 0.114098
2021-12-11 19:40:19,959 iteration 557 : loss : 0.295294, loss_ce: 0.136235
2021-12-11 19:40:21,303 iteration 558 : loss : 0.282064, loss_ce: 0.114357
2021-12-11 19:40:22,705 iteration 559 : loss : 0.320466, loss_ce: 0.153428
2021-12-11 19:40:24,130 iteration 560 : loss : 0.327966, loss_ce: 0.143955
2021-12-11 19:40:25,472 iteration 561 : loss : 0.280538, loss_ce: 0.100032
  8%|██▍                           | 33/400 [14:06<2:32:37, 24.95s/it]2021-12-11 19:40:26,878 iteration 562 : loss : 0.293349, loss_ce: 0.129093
2021-12-11 19:40:28,274 iteration 563 : loss : 0.294145, loss_ce: 0.146790
2021-12-11 19:40:29,716 iteration 564 : loss : 0.290293, loss_ce: 0.135512
2021-12-11 19:40:31,121 iteration 565 : loss : 0.343387, loss_ce: 0.190974
2021-12-11 19:40:32,442 iteration 566 : loss : 0.257968, loss_ce: 0.105022
2021-12-11 19:40:33,871 iteration 567 : loss : 0.297809, loss_ce: 0.109207
2021-12-11 19:40:35,231 iteration 568 : loss : 0.257882, loss_ce: 0.110650
2021-12-11 19:40:36,609 iteration 569 : loss : 0.265397, loss_ce: 0.104335
2021-12-11 19:40:38,069 iteration 570 : loss : 0.309705, loss_ce: 0.146396
2021-12-11 19:40:39,496 iteration 571 : loss : 0.289831, loss_ce: 0.122037
2021-12-11 19:40:40,912 iteration 572 : loss : 0.273521, loss_ce: 0.121853
2021-12-11 19:40:42,269 iteration 573 : loss : 0.266506, loss_ce: 0.117502
2021-12-11 19:40:43,639 iteration 574 : loss : 0.270261, loss_ce: 0.123109
2021-12-11 19:40:45,068 iteration 575 : loss : 0.291773, loss_ce: 0.137641
2021-12-11 19:40:46,447 iteration 576 : loss : 0.274526, loss_ce: 0.130882
2021-12-11 19:40:47,844 iteration 577 : loss : 0.336210, loss_ce: 0.139450
2021-12-11 19:40:49,249 iteration 578 : loss : 0.287286, loss_ce: 0.119396
  8%|██▌                           | 34/400 [14:30<2:30:03, 24.60s/it]2021-12-11 19:40:50,670 iteration 579 : loss : 0.277024, loss_ce: 0.103382
2021-12-11 19:40:52,042 iteration 580 : loss : 0.268049, loss_ce: 0.122359
2021-12-11 19:40:53,431 iteration 581 : loss : 0.264703, loss_ce: 0.120667
2021-12-11 19:40:54,825 iteration 582 : loss : 0.265020, loss_ce: 0.111683
2021-12-11 19:40:56,182 iteration 583 : loss : 0.323032, loss_ce: 0.158142
2021-12-11 19:40:57,463 iteration 584 : loss : 0.280599, loss_ce: 0.121295
2021-12-11 19:40:58,790 iteration 585 : loss : 0.283978, loss_ce: 0.112934
2021-12-11 19:41:00,206 iteration 586 : loss : 0.281891, loss_ce: 0.138634
2021-12-11 19:41:01,569 iteration 587 : loss : 0.297033, loss_ce: 0.130799
2021-12-11 19:41:02,975 iteration 588 : loss : 0.252845, loss_ce: 0.101906
2021-12-11 19:41:04,364 iteration 589 : loss : 0.256467, loss_ce: 0.110669
2021-12-11 19:41:05,711 iteration 590 : loss : 0.285089, loss_ce: 0.155145
2021-12-11 19:41:07,076 iteration 591 : loss : 0.257492, loss_ce: 0.111034
2021-12-11 19:41:08,437 iteration 592 : loss : 0.282350, loss_ce: 0.132133
2021-12-11 19:41:09,858 iteration 593 : loss : 0.270981, loss_ce: 0.119648
2021-12-11 19:41:11,301 iteration 594 : loss : 0.282891, loss_ce: 0.130168
2021-12-11 19:41:11,301 Training Data Eval:
2021-12-11 19:41:18,139   Average segmentation loss on training set: 0.2405
2021-12-11 19:41:18,140 Validation Data Eval:
2021-12-11 19:41:20,511   Average segmentation loss on validation set: 0.2460
2021-12-11 19:41:22,463 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:41:23,882 iteration 595 : loss : 0.283246, loss_ce: 0.115424
  9%|██▋                           | 35/400 [15:05<2:47:58, 27.61s/it]2021-12-11 19:41:25,262 iteration 596 : loss : 0.285223, loss_ce: 0.128535
2021-12-11 19:41:26,641 iteration 597 : loss : 0.271155, loss_ce: 0.134968
2021-12-11 19:41:28,040 iteration 598 : loss : 0.268309, loss_ce: 0.122177
2021-12-11 19:41:29,471 iteration 599 : loss : 0.264719, loss_ce: 0.116628
2021-12-11 19:41:30,764 iteration 600 : loss : 0.312635, loss_ce: 0.164153
2021-12-11 19:41:32,139 iteration 601 : loss : 0.276826, loss_ce: 0.118780
2021-12-11 19:41:33,564 iteration 602 : loss : 0.262676, loss_ce: 0.118874
2021-12-11 19:41:34,860 iteration 603 : loss : 0.276741, loss_ce: 0.127784
2021-12-11 19:41:36,229 iteration 604 : loss : 0.300745, loss_ce: 0.129094
2021-12-11 19:41:37,602 iteration 605 : loss : 0.256601, loss_ce: 0.114695
2021-12-11 19:41:38,946 iteration 606 : loss : 0.311608, loss_ce: 0.142819
2021-12-11 19:41:40,313 iteration 607 : loss : 0.262271, loss_ce: 0.104726
2021-12-11 19:41:41,781 iteration 608 : loss : 0.244430, loss_ce: 0.100819
2021-12-11 19:41:43,139 iteration 609 : loss : 0.267111, loss_ce: 0.116096
2021-12-11 19:41:44,562 iteration 610 : loss : 0.266907, loss_ce: 0.112507
2021-12-11 19:41:45,930 iteration 611 : loss : 0.294290, loss_ce: 0.118851
2021-12-11 19:41:47,309 iteration 612 : loss : 0.263214, loss_ce: 0.115721
  9%|██▋                           | 36/400 [15:28<2:39:52, 26.35s/it]2021-12-11 19:41:48,753 iteration 613 : loss : 0.259637, loss_ce: 0.122915
2021-12-11 19:41:50,222 iteration 614 : loss : 0.262350, loss_ce: 0.127672
2021-12-11 19:41:51,598 iteration 615 : loss : 0.255904, loss_ce: 0.110713
2021-12-11 19:41:52,934 iteration 616 : loss : 0.292840, loss_ce: 0.163249
2021-12-11 19:41:54,314 iteration 617 : loss : 0.272747, loss_ce: 0.135766
2021-12-11 19:41:55,636 iteration 618 : loss : 0.337779, loss_ce: 0.154358
2021-12-11 19:41:56,994 iteration 619 : loss : 0.248930, loss_ce: 0.101030
2021-12-11 19:41:58,443 iteration 620 : loss : 0.280546, loss_ce: 0.120923
2021-12-11 19:41:59,784 iteration 621 : loss : 0.257304, loss_ce: 0.109531
2021-12-11 19:42:01,174 iteration 622 : loss : 0.278440, loss_ce: 0.124162
2021-12-11 19:42:02,610 iteration 623 : loss : 0.250332, loss_ce: 0.106201
2021-12-11 19:42:03,969 iteration 624 : loss : 0.281373, loss_ce: 0.116644
2021-12-11 19:42:05,287 iteration 625 : loss : 0.263577, loss_ce: 0.108026
2021-12-11 19:42:06,611 iteration 626 : loss : 0.225336, loss_ce: 0.092215
2021-12-11 19:42:07,962 iteration 627 : loss : 0.231497, loss_ce: 0.090076
2021-12-11 19:42:09,317 iteration 628 : loss : 0.261755, loss_ce: 0.139837
2021-12-11 19:42:10,736 iteration 629 : loss : 0.228964, loss_ce: 0.094528
  9%|██▊                           | 37/400 [15:52<2:34:08, 25.48s/it]2021-12-11 19:42:12,150 iteration 630 : loss : 0.217360, loss_ce: 0.097210
2021-12-11 19:42:13,565 iteration 631 : loss : 0.244445, loss_ce: 0.116158
2021-12-11 19:42:14,955 iteration 632 : loss : 0.302474, loss_ce: 0.132498
2021-12-11 19:42:16,361 iteration 633 : loss : 0.239313, loss_ce: 0.105532
2021-12-11 19:42:17,784 iteration 634 : loss : 0.258041, loss_ce: 0.127989
2021-12-11 19:42:19,158 iteration 635 : loss : 0.291402, loss_ce: 0.133257
2021-12-11 19:42:20,617 iteration 636 : loss : 0.265657, loss_ce: 0.130484
2021-12-11 19:42:21,962 iteration 637 : loss : 0.282726, loss_ce: 0.118029
2021-12-11 19:42:23,314 iteration 638 : loss : 0.336152, loss_ce: 0.161503
2021-12-11 19:42:24,730 iteration 639 : loss : 0.267805, loss_ce: 0.119992
2021-12-11 19:42:26,091 iteration 640 : loss : 0.249155, loss_ce: 0.119912
2021-12-11 19:42:27,486 iteration 641 : loss : 0.278132, loss_ce: 0.118813
2021-12-11 19:42:28,878 iteration 642 : loss : 0.257741, loss_ce: 0.105405
2021-12-11 19:42:30,256 iteration 643 : loss : 0.253866, loss_ce: 0.110198
2021-12-11 19:42:31,631 iteration 644 : loss : 0.236804, loss_ce: 0.096854
2021-12-11 19:42:33,026 iteration 645 : loss : 0.335928, loss_ce: 0.145554
2021-12-11 19:42:34,401 iteration 646 : loss : 0.256154, loss_ce: 0.118499
 10%|██▊                           | 38/400 [16:15<2:30:25, 24.93s/it]2021-12-11 19:42:35,824 iteration 647 : loss : 0.300264, loss_ce: 0.119291
2021-12-11 19:42:37,164 iteration 648 : loss : 0.270021, loss_ce: 0.134164
2021-12-11 19:42:38,505 iteration 649 : loss : 0.249452, loss_ce: 0.116010
2021-12-11 19:42:39,926 iteration 650 : loss : 0.276808, loss_ce: 0.091756
2021-12-11 19:42:41,336 iteration 651 : loss : 0.294631, loss_ce: 0.136032
2021-12-11 19:42:42,666 iteration 652 : loss : 0.261429, loss_ce: 0.118785
2021-12-11 19:42:44,011 iteration 653 : loss : 0.229905, loss_ce: 0.101664
2021-12-11 19:42:45,387 iteration 654 : loss : 0.244796, loss_ce: 0.095789
2021-12-11 19:42:46,692 iteration 655 : loss : 0.260844, loss_ce: 0.115267
2021-12-11 19:42:48,107 iteration 656 : loss : 0.272611, loss_ce: 0.127288
2021-12-11 19:42:49,480 iteration 657 : loss : 0.274519, loss_ce: 0.118143
2021-12-11 19:42:50,851 iteration 658 : loss : 0.269317, loss_ce: 0.117706
2021-12-11 19:42:52,270 iteration 659 : loss : 0.297476, loss_ce: 0.126067
2021-12-11 19:42:53,660 iteration 660 : loss : 0.235042, loss_ce: 0.103928
2021-12-11 19:42:55,053 iteration 661 : loss : 0.250124, loss_ce: 0.124525
2021-12-11 19:42:56,463 iteration 662 : loss : 0.267114, loss_ce: 0.118100
2021-12-11 19:42:57,846 iteration 663 : loss : 0.310295, loss_ce: 0.151415
 10%|██▉                           | 39/400 [16:39<2:27:18, 24.48s/it]2021-12-11 19:42:59,238 iteration 664 : loss : 0.305415, loss_ce: 0.157418
2021-12-11 19:43:00,658 iteration 665 : loss : 0.285450, loss_ce: 0.110597
2021-12-11 19:43:02,056 iteration 666 : loss : 0.270745, loss_ce: 0.112066
2021-12-11 19:43:03,500 iteration 667 : loss : 0.238977, loss_ce: 0.109900
2021-12-11 19:43:04,865 iteration 668 : loss : 0.306769, loss_ce: 0.137694
2021-12-11 19:43:06,307 iteration 669 : loss : 0.285281, loss_ce: 0.127982
2021-12-11 19:43:07,738 iteration 670 : loss : 0.247595, loss_ce: 0.110425
2021-12-11 19:43:09,142 iteration 671 : loss : 0.218943, loss_ce: 0.095333
2021-12-11 19:43:10,451 iteration 672 : loss : 0.248894, loss_ce: 0.110880
2021-12-11 19:43:11,820 iteration 673 : loss : 0.286750, loss_ce: 0.109807
2021-12-11 19:43:13,335 iteration 674 : loss : 0.257674, loss_ce: 0.121573
2021-12-11 19:43:14,732 iteration 675 : loss : 0.223148, loss_ce: 0.086065
2021-12-11 19:43:16,108 iteration 676 : loss : 0.270414, loss_ce: 0.120270
2021-12-11 19:43:17,600 iteration 677 : loss : 0.222568, loss_ce: 0.094905
2021-12-11 19:43:19,014 iteration 678 : loss : 0.215548, loss_ce: 0.081873
2021-12-11 19:43:20,354 iteration 679 : loss : 0.252625, loss_ce: 0.118056
2021-12-11 19:43:20,355 Training Data Eval:
2021-12-11 19:43:27,160   Average segmentation loss on training set: 0.2190
2021-12-11 19:43:27,160 Validation Data Eval:
2021-12-11 19:43:29,521   Average segmentation loss on validation set: 0.2136
2021-12-11 19:43:31,526 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:43:32,899 iteration 680 : loss : 0.232631, loss_ce: 0.104210
 10%|███                           | 40/400 [17:14<2:45:55, 27.65s/it]2021-12-11 19:43:34,433 iteration 681 : loss : 0.209989, loss_ce: 0.094099
2021-12-11 19:43:35,812 iteration 682 : loss : 0.217934, loss_ce: 0.091556
2021-12-11 19:43:37,158 iteration 683 : loss : 0.229735, loss_ce: 0.082423
2021-12-11 19:43:38,609 iteration 684 : loss : 0.224583, loss_ce: 0.101011
2021-12-11 19:43:40,058 iteration 685 : loss : 0.208756, loss_ce: 0.082905
2021-12-11 19:43:41,481 iteration 686 : loss : 0.253853, loss_ce: 0.119593
2021-12-11 19:43:42,915 iteration 687 : loss : 0.236046, loss_ce: 0.112671
2021-12-11 19:43:44,360 iteration 688 : loss : 0.242145, loss_ce: 0.111250
2021-12-11 19:43:45,789 iteration 689 : loss : 0.229170, loss_ce: 0.112348
2021-12-11 19:43:47,169 iteration 690 : loss : 0.244360, loss_ce: 0.113186
2021-12-11 19:43:48,607 iteration 691 : loss : 0.307634, loss_ce: 0.124738
2021-12-11 19:43:49,933 iteration 692 : loss : 0.200741, loss_ce: 0.089431
2021-12-11 19:43:51,293 iteration 693 : loss : 0.271059, loss_ce: 0.120504
2021-12-11 19:43:52,678 iteration 694 : loss : 0.317412, loss_ce: 0.122777
2021-12-11 19:43:54,054 iteration 695 : loss : 0.202516, loss_ce: 0.080675
2021-12-11 19:43:55,391 iteration 696 : loss : 0.252531, loss_ce: 0.089460
2021-12-11 19:43:56,685 iteration 697 : loss : 0.207885, loss_ce: 0.088863
 10%|███                           | 41/400 [17:37<2:38:31, 26.49s/it]2021-12-11 19:43:58,117 iteration 698 : loss : 0.227650, loss_ce: 0.106523
2021-12-11 19:43:59,524 iteration 699 : loss : 0.241708, loss_ce: 0.109441
2021-12-11 19:44:00,866 iteration 700 : loss : 0.272833, loss_ce: 0.127295
2021-12-11 19:44:02,223 iteration 701 : loss : 0.262163, loss_ce: 0.113129
2021-12-11 19:44:03,642 iteration 702 : loss : 0.207814, loss_ce: 0.095715
2021-12-11 19:44:05,040 iteration 703 : loss : 0.242996, loss_ce: 0.095617
2021-12-11 19:44:06,495 iteration 704 : loss : 0.235181, loss_ce: 0.100401
2021-12-11 19:44:07,924 iteration 705 : loss : 0.213361, loss_ce: 0.106971
2021-12-11 19:44:09,360 iteration 706 : loss : 0.268216, loss_ce: 0.100949
2021-12-11 19:44:10,732 iteration 707 : loss : 0.229110, loss_ce: 0.109165
2021-12-11 19:44:12,138 iteration 708 : loss : 0.242792, loss_ce: 0.106700
2021-12-11 19:44:13,551 iteration 709 : loss : 0.242559, loss_ce: 0.093838
2021-12-11 19:44:14,880 iteration 710 : loss : 0.269082, loss_ce: 0.127534
2021-12-11 19:44:16,316 iteration 711 : loss : 0.238612, loss_ce: 0.103973
2021-12-11 19:44:17,686 iteration 712 : loss : 0.282406, loss_ce: 0.113113
2021-12-11 19:44:19,005 iteration 713 : loss : 0.236430, loss_ce: 0.104559
2021-12-11 19:44:20,337 iteration 714 : loss : 0.229239, loss_ce: 0.088452
 10%|███▏                          | 42/400 [18:01<2:32:59, 25.64s/it]2021-12-11 19:44:21,819 iteration 715 : loss : 0.211514, loss_ce: 0.093277
2021-12-11 19:44:23,183 iteration 716 : loss : 0.239221, loss_ce: 0.106170
2021-12-11 19:44:24,547 iteration 717 : loss : 0.245556, loss_ce: 0.100822
2021-12-11 19:44:25,923 iteration 718 : loss : 0.245881, loss_ce: 0.118666
2021-12-11 19:44:27,376 iteration 719 : loss : 0.220815, loss_ce: 0.096638
2021-12-11 19:44:28,723 iteration 720 : loss : 0.267497, loss_ce: 0.098517
2021-12-11 19:44:30,142 iteration 721 : loss : 0.240265, loss_ce: 0.103127
2021-12-11 19:44:31,537 iteration 722 : loss : 0.220817, loss_ce: 0.106247
2021-12-11 19:44:32,884 iteration 723 : loss : 0.185452, loss_ce: 0.076761
2021-12-11 19:44:34,367 iteration 724 : loss : 0.222446, loss_ce: 0.102793
2021-12-11 19:44:35,691 iteration 725 : loss : 0.259004, loss_ce: 0.117553
2021-12-11 19:44:37,084 iteration 726 : loss : 0.275408, loss_ce: 0.097425
2021-12-11 19:44:38,521 iteration 727 : loss : 0.252553, loss_ce: 0.096668
2021-12-11 19:44:40,062 iteration 728 : loss : 0.274465, loss_ce: 0.129656
2021-12-11 19:44:41,353 iteration 729 : loss : 0.190361, loss_ce: 0.084640
2021-12-11 19:44:42,753 iteration 730 : loss : 0.240528, loss_ce: 0.105057
2021-12-11 19:44:44,149 iteration 731 : loss : 0.223775, loss_ce: 0.092778
 11%|███▏                          | 43/400 [18:25<2:29:16, 25.09s/it]2021-12-11 19:44:45,668 iteration 732 : loss : 0.230495, loss_ce: 0.105827
2021-12-11 19:44:47,066 iteration 733 : loss : 0.312670, loss_ce: 0.167694
2021-12-11 19:44:48,407 iteration 734 : loss : 0.286329, loss_ce: 0.130723
2021-12-11 19:44:49,789 iteration 735 : loss : 0.271936, loss_ce: 0.125505
2021-12-11 19:44:51,167 iteration 736 : loss : 0.247735, loss_ce: 0.108879
2021-12-11 19:44:52,595 iteration 737 : loss : 0.249884, loss_ce: 0.106047
2021-12-11 19:44:53,987 iteration 738 : loss : 0.211496, loss_ce: 0.097616
2021-12-11 19:44:55,345 iteration 739 : loss : 0.247792, loss_ce: 0.116446
2021-12-11 19:44:56,713 iteration 740 : loss : 0.214401, loss_ce: 0.104916
2021-12-11 19:44:58,104 iteration 741 : loss : 0.211804, loss_ce: 0.091774
2021-12-11 19:44:59,562 iteration 742 : loss : 0.267949, loss_ce: 0.114751
2021-12-11 19:45:00,925 iteration 743 : loss : 0.256665, loss_ce: 0.119209
2021-12-11 19:45:02,290 iteration 744 : loss : 0.253463, loss_ce: 0.106364
2021-12-11 19:45:03,636 iteration 745 : loss : 0.253904, loss_ce: 0.118813
2021-12-11 19:45:05,011 iteration 746 : loss : 0.323129, loss_ce: 0.167598
2021-12-11 19:45:06,423 iteration 747 : loss : 0.216743, loss_ce: 0.081159
2021-12-11 19:45:07,714 iteration 748 : loss : 0.252757, loss_ce: 0.114137
 11%|███▎                          | 44/400 [18:49<2:26:10, 24.64s/it]2021-12-11 19:45:09,139 iteration 749 : loss : 0.227149, loss_ce: 0.092412
2021-12-11 19:45:10,559 iteration 750 : loss : 0.221059, loss_ce: 0.098181
2021-12-11 19:45:11,876 iteration 751 : loss : 0.212133, loss_ce: 0.091885
2021-12-11 19:45:13,257 iteration 752 : loss : 0.216563, loss_ce: 0.087963
2021-12-11 19:45:14,723 iteration 753 : loss : 0.224867, loss_ce: 0.097565
2021-12-11 19:45:16,080 iteration 754 : loss : 0.236516, loss_ce: 0.108823
2021-12-11 19:45:17,466 iteration 755 : loss : 0.197562, loss_ce: 0.085166
2021-12-11 19:45:18,806 iteration 756 : loss : 0.202588, loss_ce: 0.090891
2021-12-11 19:45:20,153 iteration 757 : loss : 0.255897, loss_ce: 0.119227
2021-12-11 19:45:21,540 iteration 758 : loss : 0.254179, loss_ce: 0.119000
2021-12-11 19:45:22,853 iteration 759 : loss : 0.216380, loss_ce: 0.092881
2021-12-11 19:45:24,251 iteration 760 : loss : 0.261774, loss_ce: 0.100210
2021-12-11 19:45:25,666 iteration 761 : loss : 0.195517, loss_ce: 0.074290
2021-12-11 19:45:27,069 iteration 762 : loss : 0.239014, loss_ce: 0.113468
2021-12-11 19:45:28,441 iteration 763 : loss : 0.215971, loss_ce: 0.100012
2021-12-11 19:45:29,777 iteration 764 : loss : 0.229294, loss_ce: 0.094783
2021-12-11 19:45:29,778 Training Data Eval:
2021-12-11 19:45:36,600   Average segmentation loss on training set: 0.2053
2021-12-11 19:45:36,600 Validation Data Eval:
2021-12-11 19:45:38,947   Average segmentation loss on validation set: 0.2408
2021-12-11 19:45:40,338 iteration 765 : loss : 0.228980, loss_ce: 0.104613
 11%|███▍                          | 45/400 [19:21<2:39:56, 27.03s/it]2021-12-11 19:45:41,735 iteration 766 : loss : 0.256568, loss_ce: 0.103502
2021-12-11 19:45:43,164 iteration 767 : loss : 0.234420, loss_ce: 0.098311
2021-12-11 19:45:44,447 iteration 768 : loss : 0.250048, loss_ce: 0.098202
2021-12-11 19:45:45,826 iteration 769 : loss : 0.265729, loss_ce: 0.095991
2021-12-11 19:45:47,161 iteration 770 : loss : 0.224556, loss_ce: 0.091938
2021-12-11 19:45:48,600 iteration 771 : loss : 0.234399, loss_ce: 0.083705
2021-12-11 19:45:50,036 iteration 772 : loss : 0.188482, loss_ce: 0.082607
2021-12-11 19:45:51,435 iteration 773 : loss : 0.204445, loss_ce: 0.082910
2021-12-11 19:45:52,816 iteration 774 : loss : 0.217026, loss_ce: 0.100069
2021-12-11 19:45:54,176 iteration 775 : loss : 0.218042, loss_ce: 0.093232
2021-12-11 19:45:55,468 iteration 776 : loss : 0.191343, loss_ce: 0.083580
2021-12-11 19:45:56,888 iteration 777 : loss : 0.234449, loss_ce: 0.105201
2021-12-11 19:45:58,275 iteration 778 : loss : 0.183117, loss_ce: 0.083080
2021-12-11 19:45:59,661 iteration 779 : loss : 0.212078, loss_ce: 0.103512
2021-12-11 19:46:00,986 iteration 780 : loss : 0.209977, loss_ce: 0.093115
2021-12-11 19:46:02,410 iteration 781 : loss : 0.197870, loss_ce: 0.094209
2021-12-11 19:46:03,828 iteration 782 : loss : 0.273720, loss_ce: 0.124883
 12%|███▍                          | 46/400 [19:45<2:33:11, 25.97s/it]2021-12-11 19:46:05,284 iteration 783 : loss : 0.185155, loss_ce: 0.087282
2021-12-11 19:46:06,667 iteration 784 : loss : 0.241008, loss_ce: 0.109322
2021-12-11 19:46:08,073 iteration 785 : loss : 0.239531, loss_ce: 0.110270
2021-12-11 19:46:09,450 iteration 786 : loss : 0.196988, loss_ce: 0.072577
2021-12-11 19:46:10,832 iteration 787 : loss : 0.183851, loss_ce: 0.077216
2021-12-11 19:46:12,243 iteration 788 : loss : 0.182540, loss_ce: 0.071283
2021-12-11 19:46:13,749 iteration 789 : loss : 0.284443, loss_ce: 0.151468
2021-12-11 19:46:15,180 iteration 790 : loss : 0.195588, loss_ce: 0.079140
2021-12-11 19:46:16,542 iteration 791 : loss : 0.195717, loss_ce: 0.074492
2021-12-11 19:46:17,950 iteration 792 : loss : 0.216595, loss_ce: 0.095179
2021-12-11 19:46:19,247 iteration 793 : loss : 0.226990, loss_ce: 0.094896
2021-12-11 19:46:20,616 iteration 794 : loss : 0.206772, loss_ce: 0.087604
2021-12-11 19:46:21,995 iteration 795 : loss : 0.209804, loss_ce: 0.095877
2021-12-11 19:46:23,377 iteration 796 : loss : 0.221199, loss_ce: 0.097594
2021-12-11 19:46:24,720 iteration 797 : loss : 0.231750, loss_ce: 0.109788
2021-12-11 19:46:26,065 iteration 798 : loss : 0.244463, loss_ce: 0.093607
2021-12-11 19:46:27,470 iteration 799 : loss : 0.183802, loss_ce: 0.074703
 12%|███▌                          | 47/400 [20:08<2:28:40, 25.27s/it]2021-12-11 19:46:28,936 iteration 800 : loss : 0.217843, loss_ce: 0.076614
2021-12-11 19:46:30,277 iteration 801 : loss : 0.224753, loss_ce: 0.091505
2021-12-11 19:46:31,656 iteration 802 : loss : 0.232939, loss_ce: 0.108195
2021-12-11 19:46:33,099 iteration 803 : loss : 0.267810, loss_ce: 0.141984
2021-12-11 19:46:34,422 iteration 804 : loss : 0.199786, loss_ce: 0.086922
2021-12-11 19:46:35,742 iteration 805 : loss : 0.232000, loss_ce: 0.087654
2021-12-11 19:46:37,127 iteration 806 : loss : 0.235877, loss_ce: 0.102724
2021-12-11 19:46:38,590 iteration 807 : loss : 0.231113, loss_ce: 0.102031
2021-12-11 19:46:40,013 iteration 808 : loss : 0.210043, loss_ce: 0.085065
2021-12-11 19:46:41,384 iteration 809 : loss : 0.236552, loss_ce: 0.085957
2021-12-11 19:46:42,813 iteration 810 : loss : 0.215745, loss_ce: 0.074611
2021-12-11 19:46:44,162 iteration 811 : loss : 0.169866, loss_ce: 0.074305
2021-12-11 19:46:45,618 iteration 812 : loss : 0.237459, loss_ce: 0.104999
2021-12-11 19:46:46,991 iteration 813 : loss : 0.190959, loss_ce: 0.088512
2021-12-11 19:46:48,382 iteration 814 : loss : 0.212014, loss_ce: 0.101271
2021-12-11 19:46:49,810 iteration 815 : loss : 0.205690, loss_ce: 0.088445
2021-12-11 19:46:51,083 iteration 816 : loss : 0.199141, loss_ce: 0.077511
 12%|███▌                          | 48/400 [20:32<2:25:20, 24.77s/it]2021-12-11 19:46:52,462 iteration 817 : loss : 0.204936, loss_ce: 0.086463
2021-12-11 19:46:53,867 iteration 818 : loss : 0.286353, loss_ce: 0.139942
2021-12-11 19:46:55,242 iteration 819 : loss : 0.202626, loss_ce: 0.089171
2021-12-11 19:46:56,717 iteration 820 : loss : 0.232728, loss_ce: 0.092302
2021-12-11 19:46:58,028 iteration 821 : loss : 0.265003, loss_ce: 0.120020
2021-12-11 19:46:59,429 iteration 822 : loss : 0.160815, loss_ce: 0.067421
2021-12-11 19:47:00,793 iteration 823 : loss : 0.192682, loss_ce: 0.082022
2021-12-11 19:47:02,146 iteration 824 : loss : 0.175587, loss_ce: 0.078660
2021-12-11 19:47:03,527 iteration 825 : loss : 0.167093, loss_ce: 0.065216
2021-12-11 19:47:04,933 iteration 826 : loss : 0.161107, loss_ce: 0.072084
2021-12-11 19:47:06,297 iteration 827 : loss : 0.244596, loss_ce: 0.081763
2021-12-11 19:47:07,717 iteration 828 : loss : 0.180450, loss_ce: 0.073994
2021-12-11 19:47:09,091 iteration 829 : loss : 0.223580, loss_ce: 0.094984
2021-12-11 19:47:10,566 iteration 830 : loss : 0.214286, loss_ce: 0.084856
2021-12-11 19:47:11,953 iteration 831 : loss : 0.231050, loss_ce: 0.102341
2021-12-11 19:47:13,314 iteration 832 : loss : 0.163199, loss_ce: 0.066945
2021-12-11 19:47:14,647 iteration 833 : loss : 0.184560, loss_ce: 0.086029
 12%|███▋                          | 49/400 [20:55<2:22:47, 24.41s/it]2021-12-11 19:47:16,027 iteration 834 : loss : 0.205001, loss_ce: 0.082178
2021-12-11 19:47:17,431 iteration 835 : loss : 0.204410, loss_ce: 0.090514
2021-12-11 19:47:18,835 iteration 836 : loss : 0.197537, loss_ce: 0.087050
2021-12-11 19:47:20,191 iteration 837 : loss : 0.201541, loss_ce: 0.089051
2021-12-11 19:47:21,544 iteration 838 : loss : 0.252468, loss_ce: 0.113466
2021-12-11 19:47:22,980 iteration 839 : loss : 0.198097, loss_ce: 0.072309
2021-12-11 19:47:24,372 iteration 840 : loss : 0.246373, loss_ce: 0.112123
2021-12-11 19:47:25,763 iteration 841 : loss : 0.205935, loss_ce: 0.105902
2021-12-11 19:47:27,262 iteration 842 : loss : 0.182794, loss_ce: 0.077782
2021-12-11 19:47:28,625 iteration 843 : loss : 0.233579, loss_ce: 0.093976
2021-12-11 19:47:30,049 iteration 844 : loss : 0.211850, loss_ce: 0.090359
2021-12-11 19:47:31,395 iteration 845 : loss : 0.182559, loss_ce: 0.077388
2021-12-11 19:47:32,774 iteration 846 : loss : 0.245851, loss_ce: 0.108192
2021-12-11 19:47:34,193 iteration 847 : loss : 0.204055, loss_ce: 0.101044
2021-12-11 19:47:35,487 iteration 848 : loss : 0.128362, loss_ce: 0.058563
2021-12-11 19:47:36,888 iteration 849 : loss : 0.184780, loss_ce: 0.080566
2021-12-11 19:47:36,888 Training Data Eval:
2021-12-11 19:47:43,700   Average segmentation loss on training set: 0.1719
2021-12-11 19:47:43,700 Validation Data Eval:
2021-12-11 19:47:46,056   Average segmentation loss on validation set: 0.2109
2021-12-11 19:47:47,990 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:47:49,453 iteration 850 : loss : 0.195717, loss_ce: 0.083031
 12%|███▊                          | 50/400 [21:30<2:40:35, 27.53s/it]2021-12-11 19:47:50,912 iteration 851 : loss : 0.204376, loss_ce: 0.094677
2021-12-11 19:47:52,334 iteration 852 : loss : 0.185934, loss_ce: 0.075734
2021-12-11 19:47:53,760 iteration 853 : loss : 0.169498, loss_ce: 0.067757
2021-12-11 19:47:55,090 iteration 854 : loss : 0.179714, loss_ce: 0.074175
2021-12-11 19:47:56,370 iteration 855 : loss : 0.144332, loss_ce: 0.057699
2021-12-11 19:47:57,700 iteration 856 : loss : 0.223728, loss_ce: 0.093802
2021-12-11 19:47:59,152 iteration 857 : loss : 0.167766, loss_ce: 0.067198
2021-12-11 19:48:00,530 iteration 858 : loss : 0.193127, loss_ce: 0.079715
2021-12-11 19:48:01,988 iteration 859 : loss : 0.178454, loss_ce: 0.069539
2021-12-11 19:48:03,408 iteration 860 : loss : 0.246178, loss_ce: 0.104316
2021-12-11 19:48:04,782 iteration 861 : loss : 0.234227, loss_ce: 0.099935
2021-12-11 19:48:06,160 iteration 862 : loss : 0.210457, loss_ce: 0.066803
2021-12-11 19:48:07,466 iteration 863 : loss : 0.199805, loss_ce: 0.102131
2021-12-11 19:48:08,831 iteration 864 : loss : 0.195586, loss_ce: 0.085725
2021-12-11 19:48:10,151 iteration 865 : loss : 0.187558, loss_ce: 0.094667
2021-12-11 19:48:11,478 iteration 866 : loss : 0.188226, loss_ce: 0.072143
2021-12-11 19:48:12,824 iteration 867 : loss : 0.206378, loss_ce: 0.090224
 13%|███▊                          | 51/400 [21:54<2:32:52, 26.28s/it]2021-12-11 19:48:14,266 iteration 868 : loss : 0.198508, loss_ce: 0.085917
2021-12-11 19:48:15,608 iteration 869 : loss : 0.156511, loss_ce: 0.069172
2021-12-11 19:48:17,000 iteration 870 : loss : 0.206253, loss_ce: 0.065012
2021-12-11 19:48:18,400 iteration 871 : loss : 0.205654, loss_ce: 0.080463
2021-12-11 19:48:19,724 iteration 872 : loss : 0.149188, loss_ce: 0.062226
2021-12-11 19:48:21,056 iteration 873 : loss : 0.139335, loss_ce: 0.054370
2021-12-11 19:48:22,460 iteration 874 : loss : 0.192987, loss_ce: 0.077594
2021-12-11 19:48:23,853 iteration 875 : loss : 0.184803, loss_ce: 0.093165
2021-12-11 19:48:25,213 iteration 876 : loss : 0.187414, loss_ce: 0.087527
2021-12-11 19:48:26,536 iteration 877 : loss : 0.155286, loss_ce: 0.065863
2021-12-11 19:48:27,862 iteration 878 : loss : 0.187062, loss_ce: 0.073356
2021-12-11 19:48:29,245 iteration 879 : loss : 0.170252, loss_ce: 0.070048
2021-12-11 19:48:30,577 iteration 880 : loss : 0.154047, loss_ce: 0.065734
2021-12-11 19:48:31,996 iteration 881 : loss : 0.190128, loss_ce: 0.082469
2021-12-11 19:48:33,350 iteration 882 : loss : 0.168975, loss_ce: 0.071250
2021-12-11 19:48:34,707 iteration 883 : loss : 0.174072, loss_ce: 0.065632
2021-12-11 19:48:36,204 iteration 884 : loss : 0.162169, loss_ce: 0.057725
 13%|███▉                          | 52/400 [22:17<2:27:22, 25.41s/it]2021-12-11 19:48:37,686 iteration 885 : loss : 0.189513, loss_ce: 0.079143
2021-12-11 19:48:39,049 iteration 886 : loss : 0.163790, loss_ce: 0.060820
2021-12-11 19:48:40,416 iteration 887 : loss : 0.195797, loss_ce: 0.089149
2021-12-11 19:48:41,800 iteration 888 : loss : 0.198807, loss_ce: 0.075331
2021-12-11 19:48:43,142 iteration 889 : loss : 0.172704, loss_ce: 0.063055
2021-12-11 19:48:44,564 iteration 890 : loss : 0.211246, loss_ce: 0.090206
2021-12-11 19:48:45,963 iteration 891 : loss : 0.221356, loss_ce: 0.109951
2021-12-11 19:48:47,346 iteration 892 : loss : 0.223212, loss_ce: 0.081839
2021-12-11 19:48:48,795 iteration 893 : loss : 0.203745, loss_ce: 0.093394
2021-12-11 19:48:50,189 iteration 894 : loss : 0.187192, loss_ce: 0.077065
2021-12-11 19:48:51,585 iteration 895 : loss : 0.230674, loss_ce: 0.100894
2021-12-11 19:48:53,029 iteration 896 : loss : 0.250319, loss_ce: 0.087882
2021-12-11 19:48:54,346 iteration 897 : loss : 0.184330, loss_ce: 0.079551
2021-12-11 19:48:55,826 iteration 898 : loss : 0.199195, loss_ce: 0.086290
2021-12-11 19:48:57,210 iteration 899 : loss : 0.198890, loss_ce: 0.085984
2021-12-11 19:48:58,615 iteration 900 : loss : 0.209575, loss_ce: 0.083478
2021-12-11 19:48:59,981 iteration 901 : loss : 0.196476, loss_ce: 0.090711
 13%|███▉                          | 53/400 [22:41<2:24:08, 24.92s/it]2021-12-11 19:49:01,417 iteration 902 : loss : 0.156251, loss_ce: 0.067752
2021-12-11 19:49:02,831 iteration 903 : loss : 0.188507, loss_ce: 0.077137
2021-12-11 19:49:04,188 iteration 904 : loss : 0.157879, loss_ce: 0.071915
2021-12-11 19:49:05,524 iteration 905 : loss : 0.238053, loss_ce: 0.081932
2021-12-11 19:49:06,853 iteration 906 : loss : 0.168642, loss_ce: 0.073518
2021-12-11 19:49:08,269 iteration 907 : loss : 0.166984, loss_ce: 0.076789
2021-12-11 19:49:09,595 iteration 908 : loss : 0.179460, loss_ce: 0.085757
2021-12-11 19:49:10,993 iteration 909 : loss : 0.233528, loss_ce: 0.113669
2021-12-11 19:49:12,376 iteration 910 : loss : 0.190218, loss_ce: 0.091034
2021-12-11 19:49:13,769 iteration 911 : loss : 0.220150, loss_ce: 0.088720
2021-12-11 19:49:15,065 iteration 912 : loss : 0.199360, loss_ce: 0.083993
2021-12-11 19:49:16,481 iteration 913 : loss : 0.189128, loss_ce: 0.084149
2021-12-11 19:49:17,827 iteration 914 : loss : 0.165176, loss_ce: 0.080754
2021-12-11 19:49:19,235 iteration 915 : loss : 0.184899, loss_ce: 0.081310
2021-12-11 19:49:20,581 iteration 916 : loss : 0.177680, loss_ce: 0.080317
2021-12-11 19:49:21,955 iteration 917 : loss : 0.163870, loss_ce: 0.063140
2021-12-11 19:49:23,330 iteration 918 : loss : 0.166150, loss_ce: 0.061570
 14%|████                          | 54/400 [23:04<2:20:59, 24.45s/it]2021-12-11 19:49:24,810 iteration 919 : loss : 0.161073, loss_ce: 0.070901
2021-12-11 19:49:26,140 iteration 920 : loss : 0.229391, loss_ce: 0.142402
2021-12-11 19:49:27,601 iteration 921 : loss : 0.168278, loss_ce: 0.069623
2021-12-11 19:49:28,876 iteration 922 : loss : 0.179170, loss_ce: 0.071377
2021-12-11 19:49:30,199 iteration 923 : loss : 0.243374, loss_ce: 0.112175
2021-12-11 19:49:31,563 iteration 924 : loss : 0.152398, loss_ce: 0.056566
2021-12-11 19:49:32,880 iteration 925 : loss : 0.136304, loss_ce: 0.052240
2021-12-11 19:49:34,199 iteration 926 : loss : 0.174600, loss_ce: 0.068411
2021-12-11 19:49:35,577 iteration 927 : loss : 0.176759, loss_ce: 0.080977
2021-12-11 19:49:36,901 iteration 928 : loss : 0.150040, loss_ce: 0.059927
2021-12-11 19:49:38,410 iteration 929 : loss : 0.161899, loss_ce: 0.076140
2021-12-11 19:49:39,801 iteration 930 : loss : 0.191689, loss_ce: 0.095984
2021-12-11 19:49:41,190 iteration 931 : loss : 0.165331, loss_ce: 0.064371
2021-12-11 19:49:42,581 iteration 932 : loss : 0.231693, loss_ce: 0.080934
2021-12-11 19:49:44,097 iteration 933 : loss : 0.214658, loss_ce: 0.076506
2021-12-11 19:49:45,468 iteration 934 : loss : 0.193353, loss_ce: 0.088384
2021-12-11 19:49:45,468 Training Data Eval:
2021-12-11 19:49:52,279   Average segmentation loss on training set: 0.1512
2021-12-11 19:49:52,279 Validation Data Eval:
2021-12-11 19:49:54,634   Average segmentation loss on validation set: 0.1444
2021-12-11 19:49:56,549 Found new lowest validation loss at iteration 934! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:49:57,923 iteration 935 : loss : 0.175747, loss_ce: 0.068220
 14%|████▏                         | 55/400 [23:39<2:38:04, 27.49s/it]2021-12-11 19:49:59,323 iteration 936 : loss : 0.159575, loss_ce: 0.049777
2021-12-11 19:50:00,675 iteration 937 : loss : 0.162941, loss_ce: 0.067029
2021-12-11 19:50:02,073 iteration 938 : loss : 0.163455, loss_ce: 0.078756
2021-12-11 19:50:03,446 iteration 939 : loss : 0.182391, loss_ce: 0.078977
2021-12-11 19:50:04,791 iteration 940 : loss : 0.192858, loss_ce: 0.074449
2021-12-11 19:50:06,184 iteration 941 : loss : 0.216366, loss_ce: 0.083588
2021-12-11 19:50:07,546 iteration 942 : loss : 0.177867, loss_ce: 0.068216
2021-12-11 19:50:08,849 iteration 943 : loss : 0.217823, loss_ce: 0.084494
2021-12-11 19:50:10,187 iteration 944 : loss : 0.202774, loss_ce: 0.084840
2021-12-11 19:50:11,537 iteration 945 : loss : 0.167570, loss_ce: 0.092546
2021-12-11 19:50:12,916 iteration 946 : loss : 0.141073, loss_ce: 0.055405
2021-12-11 19:50:14,394 iteration 947 : loss : 0.147846, loss_ce: 0.065035
2021-12-11 19:50:15,744 iteration 948 : loss : 0.148735, loss_ce: 0.059703
2021-12-11 19:50:17,145 iteration 949 : loss : 0.160214, loss_ce: 0.061643
2021-12-11 19:50:18,551 iteration 950 : loss : 0.204385, loss_ce: 0.071000
2021-12-11 19:50:19,994 iteration 951 : loss : 0.154105, loss_ce: 0.068626
2021-12-11 19:50:21,324 iteration 952 : loss : 0.179472, loss_ce: 0.060154
 14%|████▏                         | 56/400 [24:02<2:30:35, 26.27s/it]2021-12-11 19:50:22,745 iteration 953 : loss : 0.150324, loss_ce: 0.061014
2021-12-11 19:50:24,224 iteration 954 : loss : 0.146891, loss_ce: 0.062631
2021-12-11 19:50:25,585 iteration 955 : loss : 0.122739, loss_ce: 0.054936
2021-12-11 19:50:26,957 iteration 956 : loss : 0.153170, loss_ce: 0.059218
2021-12-11 19:50:28,224 iteration 957 : loss : 0.187239, loss_ce: 0.068109
2021-12-11 19:50:29,571 iteration 958 : loss : 0.185643, loss_ce: 0.062955
2021-12-11 19:50:30,894 iteration 959 : loss : 0.152530, loss_ce: 0.073409
2021-12-11 19:50:32,211 iteration 960 : loss : 0.152765, loss_ce: 0.063567
2021-12-11 19:50:33,587 iteration 961 : loss : 0.173502, loss_ce: 0.062053
2021-12-11 19:50:34,937 iteration 962 : loss : 0.119589, loss_ce: 0.050716
2021-12-11 19:50:36,344 iteration 963 : loss : 0.178734, loss_ce: 0.069692
2021-12-11 19:50:37,747 iteration 964 : loss : 0.142468, loss_ce: 0.061047
2021-12-11 19:50:39,116 iteration 965 : loss : 0.134696, loss_ce: 0.054418
2021-12-11 19:50:40,575 iteration 966 : loss : 0.217824, loss_ce: 0.105919
2021-12-11 19:50:41,957 iteration 967 : loss : 0.227260, loss_ce: 0.094631
2021-12-11 19:50:43,350 iteration 968 : loss : 0.167571, loss_ce: 0.073769
2021-12-11 19:50:44,780 iteration 969 : loss : 0.151585, loss_ce: 0.061439
 14%|████▎                         | 57/400 [24:26<2:25:19, 25.42s/it]2021-12-11 19:50:46,184 iteration 970 : loss : 0.207396, loss_ce: 0.091091
2021-12-11 19:50:47,639 iteration 971 : loss : 0.211626, loss_ce: 0.076604
2021-12-11 19:50:48,969 iteration 972 : loss : 0.162736, loss_ce: 0.062503
2021-12-11 19:50:50,294 iteration 973 : loss : 0.156215, loss_ce: 0.063811
2021-12-11 19:50:51,662 iteration 974 : loss : 0.201929, loss_ce: 0.075194
2021-12-11 19:50:52,994 iteration 975 : loss : 0.189369, loss_ce: 0.062313
2021-12-11 19:50:54,340 iteration 976 : loss : 0.205757, loss_ce: 0.082212
2021-12-11 19:50:55,776 iteration 977 : loss : 0.151872, loss_ce: 0.065448
2021-12-11 19:50:57,187 iteration 978 : loss : 0.151332, loss_ce: 0.071089
2021-12-11 19:50:58,618 iteration 979 : loss : 0.137524, loss_ce: 0.062435
2021-12-11 19:50:59,941 iteration 980 : loss : 0.149566, loss_ce: 0.060239
2021-12-11 19:51:01,370 iteration 981 : loss : 0.124594, loss_ce: 0.054638
2021-12-11 19:51:02,688 iteration 982 : loss : 0.167082, loss_ce: 0.081584
2021-12-11 19:51:04,084 iteration 983 : loss : 0.167133, loss_ce: 0.073150
2021-12-11 19:51:05,461 iteration 984 : loss : 0.165523, loss_ce: 0.077636
2021-12-11 19:51:06,837 iteration 985 : loss : 0.146362, loss_ce: 0.057388
2021-12-11 19:51:08,193 iteration 986 : loss : 0.152396, loss_ce: 0.055656
 14%|████▎                         | 58/400 [24:49<2:21:28, 24.82s/it]2021-12-11 19:51:09,619 iteration 987 : loss : 0.161385, loss_ce: 0.068075
2021-12-11 19:51:11,066 iteration 988 : loss : 0.166481, loss_ce: 0.073652
2021-12-11 19:51:12,410 iteration 989 : loss : 0.145454, loss_ce: 0.055389
2021-12-11 19:51:13,851 iteration 990 : loss : 0.134790, loss_ce: 0.063389
2021-12-11 19:51:15,272 iteration 991 : loss : 0.172986, loss_ce: 0.087186
2021-12-11 19:51:16,596 iteration 992 : loss : 0.163632, loss_ce: 0.066952
2021-12-11 19:51:18,071 iteration 993 : loss : 0.176349, loss_ce: 0.064839
2021-12-11 19:51:19,497 iteration 994 : loss : 0.179962, loss_ce: 0.073522
2021-12-11 19:51:20,886 iteration 995 : loss : 0.193158, loss_ce: 0.074427
2021-12-11 19:51:22,238 iteration 996 : loss : 0.144057, loss_ce: 0.063689
2021-12-11 19:51:23,649 iteration 997 : loss : 0.180962, loss_ce: 0.084213
2021-12-11 19:51:25,034 iteration 998 : loss : 0.174885, loss_ce: 0.066916
2021-12-11 19:51:26,494 iteration 999 : loss : 0.134275, loss_ce: 0.058655
2021-12-11 19:51:27,894 iteration 1000 : loss : 0.151210, loss_ce: 0.065166
2021-12-11 19:51:29,291 iteration 1001 : loss : 0.149805, loss_ce: 0.067047
2021-12-11 19:51:30,618 iteration 1002 : loss : 0.155681, loss_ce: 0.059213
2021-12-11 19:51:31,970 iteration 1003 : loss : 0.153478, loss_ce: 0.062548
 15%|████▍                         | 59/400 [25:13<2:19:17, 24.51s/it]2021-12-11 19:51:33,365 iteration 1004 : loss : 0.166582, loss_ce: 0.072639
2021-12-11 19:51:34,832 iteration 1005 : loss : 0.224753, loss_ce: 0.086304
2021-12-11 19:51:36,229 iteration 1006 : loss : 0.170009, loss_ce: 0.080711
2021-12-11 19:51:37,641 iteration 1007 : loss : 0.148474, loss_ce: 0.061078
2021-12-11 19:51:39,037 iteration 1008 : loss : 0.150899, loss_ce: 0.055939
2021-12-11 19:51:40,435 iteration 1009 : loss : 0.172103, loss_ce: 0.067279
2021-12-11 19:51:41,785 iteration 1010 : loss : 0.154491, loss_ce: 0.063927
2021-12-11 19:51:43,144 iteration 1011 : loss : 0.150518, loss_ce: 0.058212
2021-12-11 19:51:44,595 iteration 1012 : loss : 0.175259, loss_ce: 0.059853
2021-12-11 19:51:45,951 iteration 1013 : loss : 0.171999, loss_ce: 0.066160
2021-12-11 19:51:47,372 iteration 1014 : loss : 0.198993, loss_ce: 0.083595
2021-12-11 19:51:48,744 iteration 1015 : loss : 0.144225, loss_ce: 0.069987
2021-12-11 19:51:50,049 iteration 1016 : loss : 0.162770, loss_ce: 0.087552
2021-12-11 19:51:51,529 iteration 1017 : loss : 0.142470, loss_ce: 0.052826
2021-12-11 19:51:52,962 iteration 1018 : loss : 0.140082, loss_ce: 0.061427
2021-12-11 19:51:54,417 iteration 1019 : loss : 0.189579, loss_ce: 0.078012
2021-12-11 19:51:54,417 Training Data Eval:
2021-12-11 19:52:01,244   Average segmentation loss on training set: 0.1165
2021-12-11 19:52:01,244 Validation Data Eval:
2021-12-11 19:52:03,616   Average segmentation loss on validation set: 0.1399
2021-12-11 19:52:05,542 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:52:06,970 iteration 1020 : loss : 0.141961, loss_ce: 0.052540
 15%|████▌                         | 60/400 [25:48<2:36:43, 27.66s/it]2021-12-11 19:52:08,474 iteration 1021 : loss : 0.132871, loss_ce: 0.058328
2021-12-11 19:52:09,850 iteration 1022 : loss : 0.138906, loss_ce: 0.056963
2021-12-11 19:52:11,205 iteration 1023 : loss : 0.199251, loss_ce: 0.082950
2021-12-11 19:52:12,616 iteration 1024 : loss : 0.138431, loss_ce: 0.058546
2021-12-11 19:52:13,970 iteration 1025 : loss : 0.191524, loss_ce: 0.098754
2021-12-11 19:52:15,372 iteration 1026 : loss : 0.140007, loss_ce: 0.056001
2021-12-11 19:52:16,742 iteration 1027 : loss : 0.182887, loss_ce: 0.085708
2021-12-11 19:52:18,081 iteration 1028 : loss : 0.164002, loss_ce: 0.059099
2021-12-11 19:52:19,480 iteration 1029 : loss : 0.202025, loss_ce: 0.072610
2021-12-11 19:52:20,882 iteration 1030 : loss : 0.136324, loss_ce: 0.056428
2021-12-11 19:52:22,331 iteration 1031 : loss : 0.168184, loss_ce: 0.076915
2021-12-11 19:52:23,666 iteration 1032 : loss : 0.143655, loss_ce: 0.065064
2021-12-11 19:52:25,111 iteration 1033 : loss : 0.149536, loss_ce: 0.064404
2021-12-11 19:52:26,553 iteration 1034 : loss : 0.127651, loss_ce: 0.059329
2021-12-11 19:52:27,975 iteration 1035 : loss : 0.177292, loss_ce: 0.078224
2021-12-11 19:52:29,443 iteration 1036 : loss : 0.152025, loss_ce: 0.061150
2021-12-11 19:52:30,721 iteration 1037 : loss : 0.141874, loss_ce: 0.063050
 15%|████▌                         | 61/400 [26:12<2:29:38, 26.48s/it]2021-12-11 19:52:32,085 iteration 1038 : loss : 0.137074, loss_ce: 0.061825
2021-12-11 19:52:33,460 iteration 1039 : loss : 0.176228, loss_ce: 0.071107
2021-12-11 19:52:34,769 iteration 1040 : loss : 0.158292, loss_ce: 0.053471
2021-12-11 19:52:36,214 iteration 1041 : loss : 0.116448, loss_ce: 0.047528
2021-12-11 19:52:37,662 iteration 1042 : loss : 0.166637, loss_ce: 0.073122
2021-12-11 19:52:39,014 iteration 1043 : loss : 0.099814, loss_ce: 0.042069
2021-12-11 19:52:40,388 iteration 1044 : loss : 0.163374, loss_ce: 0.069493
2021-12-11 19:52:41,741 iteration 1045 : loss : 0.138119, loss_ce: 0.070292
2021-12-11 19:52:43,128 iteration 1046 : loss : 0.183727, loss_ce: 0.070082
2021-12-11 19:52:44,628 iteration 1047 : loss : 0.170786, loss_ce: 0.076897
2021-12-11 19:52:46,027 iteration 1048 : loss : 0.169140, loss_ce: 0.073709
2021-12-11 19:52:47,345 iteration 1049 : loss : 0.145530, loss_ce: 0.056523
2021-12-11 19:52:48,691 iteration 1050 : loss : 0.121038, loss_ce: 0.048860
2021-12-11 19:52:50,076 iteration 1051 : loss : 0.148593, loss_ce: 0.058378
2021-12-11 19:52:51,460 iteration 1052 : loss : 0.156877, loss_ce: 0.066667
2021-12-11 19:52:52,934 iteration 1053 : loss : 0.149131, loss_ce: 0.077599
2021-12-11 19:52:54,285 iteration 1054 : loss : 0.132273, loss_ce: 0.052896
 16%|████▋                         | 62/400 [26:35<2:24:14, 25.61s/it]2021-12-11 19:52:55,686 iteration 1055 : loss : 0.149832, loss_ce: 0.059825
2021-12-11 19:52:57,066 iteration 1056 : loss : 0.205635, loss_ce: 0.095322
2021-12-11 19:52:58,467 iteration 1057 : loss : 0.158993, loss_ce: 0.055277
2021-12-11 19:52:59,784 iteration 1058 : loss : 0.123975, loss_ce: 0.059709
2021-12-11 19:53:01,246 iteration 1059 : loss : 0.143648, loss_ce: 0.061871
2021-12-11 19:53:02,610 iteration 1060 : loss : 0.114744, loss_ce: 0.053888
2021-12-11 19:53:04,034 iteration 1061 : loss : 0.149137, loss_ce: 0.066915
2021-12-11 19:53:05,417 iteration 1062 : loss : 0.145044, loss_ce: 0.058503
2021-12-11 19:53:06,875 iteration 1063 : loss : 0.126708, loss_ce: 0.050118
2021-12-11 19:53:08,230 iteration 1064 : loss : 0.153652, loss_ce: 0.067946
2021-12-11 19:53:09,538 iteration 1065 : loss : 0.123818, loss_ce: 0.050525
2021-12-11 19:53:10,941 iteration 1066 : loss : 0.136111, loss_ce: 0.063335
2021-12-11 19:53:12,451 iteration 1067 : loss : 0.177226, loss_ce: 0.074412
2021-12-11 19:53:13,886 iteration 1068 : loss : 0.184771, loss_ce: 0.077933
2021-12-11 19:53:15,216 iteration 1069 : loss : 0.187906, loss_ce: 0.076356
2021-12-11 19:53:16,689 iteration 1070 : loss : 0.133249, loss_ce: 0.047064
2021-12-11 19:53:18,062 iteration 1071 : loss : 0.128885, loss_ce: 0.056075
 16%|████▋                         | 63/400 [26:59<2:20:44, 25.06s/it]2021-12-11 19:53:19,456 iteration 1072 : loss : 0.132357, loss_ce: 0.052779
2021-12-11 19:53:20,932 iteration 1073 : loss : 0.152743, loss_ce: 0.060397
2021-12-11 19:53:22,286 iteration 1074 : loss : 0.108318, loss_ce: 0.049437
2021-12-11 19:53:23,676 iteration 1075 : loss : 0.135763, loss_ce: 0.053419
2021-12-11 19:53:25,030 iteration 1076 : loss : 0.135891, loss_ce: 0.053714
2021-12-11 19:53:26,447 iteration 1077 : loss : 0.220638, loss_ce: 0.071032
2021-12-11 19:53:27,766 iteration 1078 : loss : 0.147556, loss_ce: 0.064578
2021-12-11 19:53:29,151 iteration 1079 : loss : 0.112613, loss_ce: 0.056615
2021-12-11 19:53:30,554 iteration 1080 : loss : 0.134512, loss_ce: 0.058279
2021-12-11 19:53:31,962 iteration 1081 : loss : 0.140341, loss_ce: 0.056985
2021-12-11 19:53:33,349 iteration 1082 : loss : 0.174606, loss_ce: 0.076914
2021-12-11 19:53:34,732 iteration 1083 : loss : 0.154297, loss_ce: 0.056046
2021-12-11 19:53:36,169 iteration 1084 : loss : 0.220837, loss_ce: 0.075737
2021-12-11 19:53:37,534 iteration 1085 : loss : 0.124239, loss_ce: 0.049849
2021-12-11 19:53:38,953 iteration 1086 : loss : 0.129886, loss_ce: 0.052350
2021-12-11 19:53:40,274 iteration 1087 : loss : 0.097692, loss_ce: 0.046402
2021-12-11 19:53:41,629 iteration 1088 : loss : 0.135618, loss_ce: 0.067841
 16%|████▊                         | 64/400 [27:22<2:17:49, 24.61s/it]2021-12-11 19:53:42,959 iteration 1089 : loss : 0.113538, loss_ce: 0.055648
2021-12-11 19:53:44,347 iteration 1090 : loss : 0.128693, loss_ce: 0.054751
2021-12-11 19:53:45,766 iteration 1091 : loss : 0.144103, loss_ce: 0.076848
2021-12-11 19:53:47,159 iteration 1092 : loss : 0.123130, loss_ce: 0.047587
2021-12-11 19:53:48,563 iteration 1093 : loss : 0.145250, loss_ce: 0.051899
2021-12-11 19:53:50,054 iteration 1094 : loss : 0.234572, loss_ce: 0.069442
2021-12-11 19:53:51,379 iteration 1095 : loss : 0.157772, loss_ce: 0.062762
2021-12-11 19:53:52,721 iteration 1096 : loss : 0.120378, loss_ce: 0.057002
2021-12-11 19:53:54,044 iteration 1097 : loss : 0.119065, loss_ce: 0.048668
2021-12-11 19:53:55,485 iteration 1098 : loss : 0.183057, loss_ce: 0.076734
2021-12-11 19:53:56,866 iteration 1099 : loss : 0.152541, loss_ce: 0.053800
2021-12-11 19:53:58,310 iteration 1100 : loss : 0.119283, loss_ce: 0.055317
2021-12-11 19:53:59,754 iteration 1101 : loss : 0.173665, loss_ce: 0.076379
2021-12-11 19:54:01,132 iteration 1102 : loss : 0.164156, loss_ce: 0.067462
2021-12-11 19:54:02,546 iteration 1103 : loss : 0.108975, loss_ce: 0.050502
2021-12-11 19:54:03,919 iteration 1104 : loss : 0.135991, loss_ce: 0.059665
2021-12-11 19:54:03,919 Training Data Eval:
2021-12-11 19:54:10,751   Average segmentation loss on training set: 0.1073
2021-12-11 19:54:10,752 Validation Data Eval:
2021-12-11 19:54:13,108   Average segmentation loss on validation set: 0.1337
2021-12-11 19:54:15,023 Found new lowest validation loss at iteration 1104! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:54:16,401 iteration 1105 : loss : 0.131546, loss_ce: 0.054946
 16%|████▉                         | 65/400 [27:57<2:34:26, 27.66s/it]2021-12-11 19:54:17,853 iteration 1106 : loss : 0.144056, loss_ce: 0.055578
2021-12-11 19:54:19,234 iteration 1107 : loss : 0.124314, loss_ce: 0.051694
2021-12-11 19:54:20,586 iteration 1108 : loss : 0.141236, loss_ce: 0.052973
2021-12-11 19:54:22,035 iteration 1109 : loss : 0.123508, loss_ce: 0.049761
2021-12-11 19:54:23,412 iteration 1110 : loss : 0.106258, loss_ce: 0.045715
2021-12-11 19:54:24,875 iteration 1111 : loss : 0.149014, loss_ce: 0.051931
2021-12-11 19:54:26,213 iteration 1112 : loss : 0.168326, loss_ce: 0.071430
2021-12-11 19:54:27,595 iteration 1113 : loss : 0.123619, loss_ce: 0.045738
2021-12-11 19:54:28,983 iteration 1114 : loss : 0.097366, loss_ce: 0.044141
2021-12-11 19:54:30,385 iteration 1115 : loss : 0.082154, loss_ce: 0.038792
2021-12-11 19:54:31,700 iteration 1116 : loss : 0.136929, loss_ce: 0.063611
2021-12-11 19:54:33,035 iteration 1117 : loss : 0.126821, loss_ce: 0.057064
2021-12-11 19:54:34,446 iteration 1118 : loss : 0.122457, loss_ce: 0.055266
2021-12-11 19:54:35,866 iteration 1119 : loss : 0.131280, loss_ce: 0.059505
2021-12-11 19:54:37,255 iteration 1120 : loss : 0.131485, loss_ce: 0.043808
2021-12-11 19:54:38,616 iteration 1121 : loss : 0.117069, loss_ce: 0.053548
2021-12-11 19:54:39,936 iteration 1122 : loss : 0.111765, loss_ce: 0.041122
 16%|████▉                         | 66/400 [28:21<2:27:04, 26.42s/it]2021-12-11 19:54:41,443 iteration 1123 : loss : 0.083649, loss_ce: 0.039081
2021-12-11 19:54:42,825 iteration 1124 : loss : 0.091050, loss_ce: 0.044595
2021-12-11 19:54:44,173 iteration 1125 : loss : 0.142803, loss_ce: 0.042423
2021-12-11 19:54:45,529 iteration 1126 : loss : 0.117803, loss_ce: 0.049331
2021-12-11 19:54:46,882 iteration 1127 : loss : 0.106835, loss_ce: 0.045872
2021-12-11 19:54:48,329 iteration 1128 : loss : 0.119904, loss_ce: 0.049976
2021-12-11 19:54:49,800 iteration 1129 : loss : 0.143622, loss_ce: 0.061191
2021-12-11 19:54:51,127 iteration 1130 : loss : 0.149534, loss_ce: 0.067080
2021-12-11 19:54:52,546 iteration 1131 : loss : 0.144048, loss_ce: 0.063622
2021-12-11 19:54:54,009 iteration 1132 : loss : 0.123652, loss_ce: 0.052055
2021-12-11 19:54:55,456 iteration 1133 : loss : 0.182400, loss_ce: 0.081043
2021-12-11 19:54:56,891 iteration 1134 : loss : 0.125550, loss_ce: 0.051616
2021-12-11 19:54:58,229 iteration 1135 : loss : 0.150765, loss_ce: 0.060462
2021-12-11 19:54:59,589 iteration 1136 : loss : 0.100601, loss_ce: 0.047207
2021-12-11 19:55:00,952 iteration 1137 : loss : 0.168742, loss_ce: 0.082495
2021-12-11 19:55:02,363 iteration 1138 : loss : 0.147476, loss_ce: 0.066307
2021-12-11 19:55:03,695 iteration 1139 : loss : 0.157660, loss_ce: 0.064350
 17%|█████                         | 67/400 [28:44<2:22:12, 25.62s/it]2021-12-11 19:55:05,122 iteration 1140 : loss : 0.100629, loss_ce: 0.046563
2021-12-11 19:55:06,530 iteration 1141 : loss : 0.156053, loss_ce: 0.078507
2021-12-11 19:55:08,027 iteration 1142 : loss : 0.146600, loss_ce: 0.064391
2021-12-11 19:55:09,434 iteration 1143 : loss : 0.131247, loss_ce: 0.055752
2021-12-11 19:55:10,851 iteration 1144 : loss : 0.127148, loss_ce: 0.048793
2021-12-11 19:55:12,239 iteration 1145 : loss : 0.131378, loss_ce: 0.051476
2021-12-11 19:55:13,603 iteration 1146 : loss : 0.110051, loss_ce: 0.047596
2021-12-11 19:55:14,961 iteration 1147 : loss : 0.115032, loss_ce: 0.052904
2021-12-11 19:55:16,320 iteration 1148 : loss : 0.101890, loss_ce: 0.050367
2021-12-11 19:55:17,689 iteration 1149 : loss : 0.092903, loss_ce: 0.046080
2021-12-11 19:55:19,039 iteration 1150 : loss : 0.119851, loss_ce: 0.053620
2021-12-11 19:55:20,396 iteration 1151 : loss : 0.109024, loss_ce: 0.050485
2021-12-11 19:55:21,739 iteration 1152 : loss : 0.135072, loss_ce: 0.061337
2021-12-11 19:55:23,201 iteration 1153 : loss : 0.148377, loss_ce: 0.055553
2021-12-11 19:55:24,614 iteration 1154 : loss : 0.121305, loss_ce: 0.048192
2021-12-11 19:55:26,050 iteration 1155 : loss : 0.125740, loss_ce: 0.050070
2021-12-11 19:55:27,539 iteration 1156 : loss : 0.127966, loss_ce: 0.049828
 17%|█████                         | 68/400 [29:08<2:18:48, 25.09s/it]2021-12-11 19:55:28,987 iteration 1157 : loss : 0.134814, loss_ce: 0.061621
2021-12-11 19:55:30,364 iteration 1158 : loss : 0.090203, loss_ce: 0.042521
2021-12-11 19:55:31,666 iteration 1159 : loss : 0.122119, loss_ce: 0.057005
2021-12-11 19:55:33,017 iteration 1160 : loss : 0.112784, loss_ce: 0.061590
2021-12-11 19:55:34,422 iteration 1161 : loss : 0.102287, loss_ce: 0.046590
2021-12-11 19:55:35,836 iteration 1162 : loss : 0.105025, loss_ce: 0.046276
2021-12-11 19:55:37,220 iteration 1163 : loss : 0.101424, loss_ce: 0.053296
2021-12-11 19:55:38,661 iteration 1164 : loss : 0.150889, loss_ce: 0.065549
2021-12-11 19:55:39,973 iteration 1165 : loss : 0.115705, loss_ce: 0.054876
2021-12-11 19:55:41,303 iteration 1166 : loss : 0.137916, loss_ce: 0.046020
2021-12-11 19:55:42,697 iteration 1167 : loss : 0.106029, loss_ce: 0.050412
2021-12-11 19:55:44,144 iteration 1168 : loss : 0.122123, loss_ce: 0.042112
2021-12-11 19:55:45,468 iteration 1169 : loss : 0.114257, loss_ce: 0.038992
2021-12-11 19:55:46,871 iteration 1170 : loss : 0.125366, loss_ce: 0.044405
2021-12-11 19:55:48,367 iteration 1171 : loss : 0.170080, loss_ce: 0.047797
2021-12-11 19:55:49,689 iteration 1172 : loss : 0.159388, loss_ce: 0.060140
2021-12-11 19:55:51,004 iteration 1173 : loss : 0.104842, loss_ce: 0.042808
 17%|█████▏                        | 69/400 [29:32<2:15:43, 24.60s/it]2021-12-11 19:55:52,456 iteration 1174 : loss : 0.105960, loss_ce: 0.044136
2021-12-11 19:55:53,918 iteration 1175 : loss : 0.118231, loss_ce: 0.050250
2021-12-11 19:55:55,275 iteration 1176 : loss : 0.116282, loss_ce: 0.048115
2021-12-11 19:55:56,635 iteration 1177 : loss : 0.133100, loss_ce: 0.067444
2021-12-11 19:55:57,960 iteration 1178 : loss : 0.109624, loss_ce: 0.041823
2021-12-11 19:55:59,388 iteration 1179 : loss : 0.105875, loss_ce: 0.047828
2021-12-11 19:56:00,780 iteration 1180 : loss : 0.112636, loss_ce: 0.051716
2021-12-11 19:56:02,102 iteration 1181 : loss : 0.140443, loss_ce: 0.050753
2021-12-11 19:56:03,463 iteration 1182 : loss : 0.144900, loss_ce: 0.062537
2021-12-11 19:56:04,840 iteration 1183 : loss : 0.089536, loss_ce: 0.041771
2021-12-11 19:56:06,238 iteration 1184 : loss : 0.125175, loss_ce: 0.055918
2021-12-11 19:56:07,598 iteration 1185 : loss : 0.115221, loss_ce: 0.065690
2021-12-11 19:56:08,991 iteration 1186 : loss : 0.096237, loss_ce: 0.036457
2021-12-11 19:56:10,322 iteration 1187 : loss : 0.094798, loss_ce: 0.045828
2021-12-11 19:56:11,749 iteration 1188 : loss : 0.136203, loss_ce: 0.056438
2021-12-11 19:56:13,169 iteration 1189 : loss : 0.131352, loss_ce: 0.038521
2021-12-11 19:56:13,169 Training Data Eval:
2021-12-11 19:56:19,977   Average segmentation loss on training set: 0.0945
2021-12-11 19:56:19,977 Validation Data Eval:
2021-12-11 19:56:22,335   Average segmentation loss on validation set: 0.1653
2021-12-11 19:56:23,675 iteration 1190 : loss : 0.108804, loss_ce: 0.040631
 18%|█████▎                        | 70/400 [30:04<2:28:36, 27.02s/it]2021-12-11 19:56:25,094 iteration 1191 : loss : 0.096620, loss_ce: 0.043336
2021-12-11 19:56:26,450 iteration 1192 : loss : 0.149375, loss_ce: 0.073902
2021-12-11 19:56:27,762 iteration 1193 : loss : 0.098616, loss_ce: 0.045127
2021-12-11 19:56:29,149 iteration 1194 : loss : 0.144510, loss_ce: 0.047152
2021-12-11 19:56:30,533 iteration 1195 : loss : 0.124744, loss_ce: 0.049878
2021-12-11 19:56:31,940 iteration 1196 : loss : 0.096702, loss_ce: 0.040277
2021-12-11 19:56:33,358 iteration 1197 : loss : 0.148741, loss_ce: 0.054240
2021-12-11 19:56:34,675 iteration 1198 : loss : 0.091420, loss_ce: 0.042888
2021-12-11 19:56:36,042 iteration 1199 : loss : 0.109059, loss_ce: 0.061693
2021-12-11 19:56:37,361 iteration 1200 : loss : 0.107392, loss_ce: 0.044949
2021-12-11 19:56:38,706 iteration 1201 : loss : 0.115134, loss_ce: 0.047105
2021-12-11 19:56:40,142 iteration 1202 : loss : 0.101592, loss_ce: 0.049791
2021-12-11 19:56:41,567 iteration 1203 : loss : 0.108031, loss_ce: 0.053421
2021-12-11 19:56:42,879 iteration 1204 : loss : 0.120601, loss_ce: 0.038893
2021-12-11 19:56:44,294 iteration 1205 : loss : 0.110418, loss_ce: 0.058165
2021-12-11 19:56:45,638 iteration 1206 : loss : 0.096140, loss_ce: 0.041283
2021-12-11 19:56:47,068 iteration 1207 : loss : 0.110144, loss_ce: 0.049482
 18%|█████▎                        | 71/400 [30:28<2:22:11, 25.93s/it]2021-12-11 19:56:48,466 iteration 1208 : loss : 0.135324, loss_ce: 0.055080
2021-12-11 19:56:49,854 iteration 1209 : loss : 0.100859, loss_ce: 0.044036
2021-12-11 19:56:51,165 iteration 1210 : loss : 0.126511, loss_ce: 0.052562
2021-12-11 19:56:52,591 iteration 1211 : loss : 0.089790, loss_ce: 0.046186
2021-12-11 19:56:53,950 iteration 1212 : loss : 0.082406, loss_ce: 0.039411
2021-12-11 19:56:55,230 iteration 1213 : loss : 0.087387, loss_ce: 0.035768
2021-12-11 19:56:56,686 iteration 1214 : loss : 0.098799, loss_ce: 0.047818
2021-12-11 19:56:58,054 iteration 1215 : loss : 0.083718, loss_ce: 0.041805
2021-12-11 19:56:59,418 iteration 1216 : loss : 0.110864, loss_ce: 0.041281
2021-12-11 19:57:00,905 iteration 1217 : loss : 0.139305, loss_ce: 0.058254
2021-12-11 19:57:02,238 iteration 1218 : loss : 0.116174, loss_ce: 0.068785
2021-12-11 19:57:03,757 iteration 1219 : loss : 0.115331, loss_ce: 0.061580
2021-12-11 19:57:05,127 iteration 1220 : loss : 0.136558, loss_ce: 0.066062
2021-12-11 19:57:06,566 iteration 1221 : loss : 0.093233, loss_ce: 0.037178
2021-12-11 19:57:07,920 iteration 1222 : loss : 0.169486, loss_ce: 0.059108
2021-12-11 19:57:09,278 iteration 1223 : loss : 0.110840, loss_ce: 0.043467
2021-12-11 19:57:10,644 iteration 1224 : loss : 0.142076, loss_ce: 0.055660
 18%|█████▍                        | 72/400 [30:51<2:17:54, 25.23s/it]2021-12-11 19:57:12,054 iteration 1225 : loss : 0.102288, loss_ce: 0.036203
2021-12-11 19:57:13,451 iteration 1226 : loss : 0.118323, loss_ce: 0.045009
2021-12-11 19:57:14,781 iteration 1227 : loss : 0.092176, loss_ce: 0.040562
2021-12-11 19:57:16,127 iteration 1228 : loss : 0.126968, loss_ce: 0.052943
2021-12-11 19:57:17,621 iteration 1229 : loss : 0.107431, loss_ce: 0.056784
2021-12-11 19:57:19,053 iteration 1230 : loss : 0.107612, loss_ce: 0.046564
2021-12-11 19:57:20,452 iteration 1231 : loss : 0.089397, loss_ce: 0.040565
2021-12-11 19:57:21,791 iteration 1232 : loss : 0.141345, loss_ce: 0.069619
2021-12-11 19:57:23,229 iteration 1233 : loss : 0.112023, loss_ce: 0.045607
2021-12-11 19:57:24,715 iteration 1234 : loss : 0.115698, loss_ce: 0.051220
2021-12-11 19:57:26,023 iteration 1235 : loss : 0.146614, loss_ce: 0.053687
2021-12-11 19:57:27,447 iteration 1236 : loss : 0.117494, loss_ce: 0.058986
2021-12-11 19:57:28,833 iteration 1237 : loss : 0.135391, loss_ce: 0.055351
2021-12-11 19:57:30,156 iteration 1238 : loss : 0.096414, loss_ce: 0.042993
2021-12-11 19:57:31,522 iteration 1239 : loss : 0.089491, loss_ce: 0.043710
2021-12-11 19:57:32,973 iteration 1240 : loss : 0.114157, loss_ce: 0.047964
2021-12-11 19:57:34,361 iteration 1241 : loss : 0.108288, loss_ce: 0.050068
 18%|█████▍                        | 73/400 [31:15<2:15:01, 24.78s/it]2021-12-11 19:57:35,825 iteration 1242 : loss : 0.102495, loss_ce: 0.053775
2021-12-11 19:57:37,218 iteration 1243 : loss : 0.106897, loss_ce: 0.042459
2021-12-11 19:57:38,688 iteration 1244 : loss : 0.104843, loss_ce: 0.044390
2021-12-11 19:57:40,148 iteration 1245 : loss : 0.104200, loss_ce: 0.044025
2021-12-11 19:57:41,528 iteration 1246 : loss : 0.091017, loss_ce: 0.044131
2021-12-11 19:57:43,065 iteration 1247 : loss : 0.155738, loss_ce: 0.066751
2021-12-11 19:57:44,417 iteration 1248 : loss : 0.113092, loss_ce: 0.044891
2021-12-11 19:57:45,791 iteration 1249 : loss : 0.118123, loss_ce: 0.055789
2021-12-11 19:57:47,182 iteration 1250 : loss : 0.172424, loss_ce: 0.050786
2021-12-11 19:57:48,631 iteration 1251 : loss : 0.098179, loss_ce: 0.043886
2021-12-11 19:57:49,959 iteration 1252 : loss : 0.116840, loss_ce: 0.042560
2021-12-11 19:57:51,331 iteration 1253 : loss : 0.129290, loss_ce: 0.054514
2021-12-11 19:57:52,707 iteration 1254 : loss : 0.085028, loss_ce: 0.035651
2021-12-11 19:57:54,198 iteration 1255 : loss : 0.106346, loss_ce: 0.049524
2021-12-11 19:57:55,572 iteration 1256 : loss : 0.081228, loss_ce: 0.035998
2021-12-11 19:57:56,868 iteration 1257 : loss : 0.099338, loss_ce: 0.041754
2021-12-11 19:57:58,163 iteration 1258 : loss : 0.097290, loss_ce: 0.048917
 18%|█████▌                        | 74/400 [31:39<2:13:01, 24.48s/it]2021-12-11 19:57:59,545 iteration 1259 : loss : 0.101920, loss_ce: 0.041435
2021-12-11 19:58:01,022 iteration 1260 : loss : 0.077710, loss_ce: 0.035798
2021-12-11 19:58:02,377 iteration 1261 : loss : 0.084875, loss_ce: 0.032803
2021-12-11 19:58:03,697 iteration 1262 : loss : 0.074956, loss_ce: 0.034499
2021-12-11 19:58:05,065 iteration 1263 : loss : 0.094414, loss_ce: 0.039987
2021-12-11 19:58:06,455 iteration 1264 : loss : 0.104742, loss_ce: 0.047287
2021-12-11 19:58:07,827 iteration 1265 : loss : 0.082748, loss_ce: 0.035246
2021-12-11 19:58:09,166 iteration 1266 : loss : 0.084200, loss_ce: 0.039026
2021-12-11 19:58:10,496 iteration 1267 : loss : 0.114198, loss_ce: 0.051295
2021-12-11 19:58:11,837 iteration 1268 : loss : 0.078459, loss_ce: 0.037542
2021-12-11 19:58:13,188 iteration 1269 : loss : 0.103724, loss_ce: 0.053430
2021-12-11 19:58:14,558 iteration 1270 : loss : 0.075896, loss_ce: 0.037370
2021-12-11 19:58:15,999 iteration 1271 : loss : 0.115174, loss_ce: 0.048292
2021-12-11 19:58:17,276 iteration 1272 : loss : 0.074028, loss_ce: 0.034217
2021-12-11 19:58:18,601 iteration 1273 : loss : 0.119207, loss_ce: 0.041712
2021-12-11 19:58:19,997 iteration 1274 : loss : 0.135505, loss_ce: 0.064432
2021-12-11 19:58:19,997 Training Data Eval:
2021-12-11 19:58:26,831   Average segmentation loss on training set: 0.0714
2021-12-11 19:58:26,831 Validation Data Eval:
2021-12-11 19:58:29,189   Average segmentation loss on validation set: 0.1071
2021-12-11 19:58:31,119 Found new lowest validation loss at iteration 1274! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 19:58:32,452 iteration 1275 : loss : 0.096530, loss_ce: 0.050150
 19%|█████▋                        | 75/400 [32:13<2:28:34, 27.43s/it]2021-12-11 19:58:33,862 iteration 1276 : loss : 0.083118, loss_ce: 0.039303
2021-12-11 19:58:35,348 iteration 1277 : loss : 0.111229, loss_ce: 0.047652
2021-12-11 19:58:36,727 iteration 1278 : loss : 0.112447, loss_ce: 0.047077
2021-12-11 19:58:38,085 iteration 1279 : loss : 0.083745, loss_ce: 0.037535
2021-12-11 19:58:39,478 iteration 1280 : loss : 0.109917, loss_ce: 0.059184
2021-12-11 19:58:40,814 iteration 1281 : loss : 0.079520, loss_ce: 0.038966
2021-12-11 19:58:42,177 iteration 1282 : loss : 0.079207, loss_ce: 0.038374
2021-12-11 19:58:43,469 iteration 1283 : loss : 0.115430, loss_ce: 0.038784
2021-12-11 19:58:44,822 iteration 1284 : loss : 0.091636, loss_ce: 0.037378
2021-12-11 19:58:46,134 iteration 1285 : loss : 0.065399, loss_ce: 0.030760
2021-12-11 19:58:47,553 iteration 1286 : loss : 0.145602, loss_ce: 0.048310
2021-12-11 19:58:48,968 iteration 1287 : loss : 0.069000, loss_ce: 0.034133
2021-12-11 19:58:50,296 iteration 1288 : loss : 0.092521, loss_ce: 0.040820
2021-12-11 19:58:51,669 iteration 1289 : loss : 0.125897, loss_ce: 0.052641
2021-12-11 19:58:52,997 iteration 1290 : loss : 0.143422, loss_ce: 0.054805
2021-12-11 19:58:54,336 iteration 1291 : loss : 0.106292, loss_ce: 0.040403
2021-12-11 19:58:55,662 iteration 1292 : loss : 0.134562, loss_ce: 0.076478
 19%|█████▋                        | 76/400 [32:36<2:21:16, 26.16s/it]2021-12-11 19:58:57,177 iteration 1293 : loss : 0.116070, loss_ce: 0.054630
2021-12-11 19:58:58,475 iteration 1294 : loss : 0.076244, loss_ce: 0.036783
2021-12-11 19:58:59,895 iteration 1295 : loss : 0.093519, loss_ce: 0.036689
2021-12-11 19:59:01,282 iteration 1296 : loss : 0.093788, loss_ce: 0.037180
2021-12-11 19:59:02,553 iteration 1297 : loss : 0.074607, loss_ce: 0.034581
2021-12-11 19:59:03,990 iteration 1298 : loss : 0.086914, loss_ce: 0.037278
2021-12-11 19:59:05,343 iteration 1299 : loss : 0.080871, loss_ce: 0.038605
2021-12-11 19:59:06,792 iteration 1300 : loss : 0.119136, loss_ce: 0.045012
2021-12-11 19:59:08,253 iteration 1301 : loss : 0.141573, loss_ce: 0.051637
2021-12-11 19:59:09,571 iteration 1302 : loss : 0.121766, loss_ce: 0.046190
2021-12-11 19:59:10,976 iteration 1303 : loss : 0.097096, loss_ce: 0.038113
2021-12-11 19:59:12,360 iteration 1304 : loss : 0.096937, loss_ce: 0.050349
2021-12-11 19:59:13,818 iteration 1305 : loss : 0.116573, loss_ce: 0.050710
2021-12-11 19:59:15,215 iteration 1306 : loss : 0.083876, loss_ce: 0.036874
2021-12-11 19:59:16,553 iteration 1307 : loss : 0.096130, loss_ce: 0.042569
2021-12-11 19:59:17,935 iteration 1308 : loss : 0.106413, loss_ce: 0.048675
2021-12-11 19:59:19,279 iteration 1309 : loss : 0.091552, loss_ce: 0.042719
 19%|█████▊                        | 77/400 [33:00<2:16:44, 25.40s/it]2021-12-11 19:59:20,682 iteration 1310 : loss : 0.108514, loss_ce: 0.046496
2021-12-11 19:59:22,148 iteration 1311 : loss : 0.100194, loss_ce: 0.042768
2021-12-11 19:59:23,526 iteration 1312 : loss : 0.099502, loss_ce: 0.040489
2021-12-11 19:59:24,895 iteration 1313 : loss : 0.095332, loss_ce: 0.042768
2021-12-11 19:59:26,330 iteration 1314 : loss : 0.154537, loss_ce: 0.076965
2021-12-11 19:59:27,784 iteration 1315 : loss : 0.104976, loss_ce: 0.059564
2021-12-11 19:59:29,136 iteration 1316 : loss : 0.108497, loss_ce: 0.050532
2021-12-11 19:59:30,457 iteration 1317 : loss : 0.091866, loss_ce: 0.035138
2021-12-11 19:59:31,774 iteration 1318 : loss : 0.089620, loss_ce: 0.037725
2021-12-11 19:59:33,171 iteration 1319 : loss : 0.095761, loss_ce: 0.037520
2021-12-11 19:59:34,491 iteration 1320 : loss : 0.100160, loss_ce: 0.040152
2021-12-11 19:59:35,872 iteration 1321 : loss : 0.116648, loss_ce: 0.050330
2021-12-11 19:59:37,227 iteration 1322 : loss : 0.085802, loss_ce: 0.035707
2021-12-11 19:59:38,661 iteration 1323 : loss : 0.093676, loss_ce: 0.040652
2021-12-11 19:59:40,114 iteration 1324 : loss : 0.069861, loss_ce: 0.036161
2021-12-11 19:59:41,485 iteration 1325 : loss : 0.076156, loss_ce: 0.037030
2021-12-11 19:59:42,866 iteration 1326 : loss : 0.085003, loss_ce: 0.041427
 20%|█████▊                        | 78/400 [33:24<2:13:23, 24.86s/it]2021-12-11 19:59:44,279 iteration 1327 : loss : 0.067538, loss_ce: 0.030748
2021-12-11 19:59:45,665 iteration 1328 : loss : 0.075882, loss_ce: 0.035665
2021-12-11 19:59:47,055 iteration 1329 : loss : 0.098690, loss_ce: 0.045012
2021-12-11 19:59:48,505 iteration 1330 : loss : 0.091771, loss_ce: 0.037053
2021-12-11 19:59:49,880 iteration 1331 : loss : 0.100102, loss_ce: 0.045155
2021-12-11 19:59:51,239 iteration 1332 : loss : 0.108995, loss_ce: 0.044214
2021-12-11 19:59:52,591 iteration 1333 : loss : 0.097172, loss_ce: 0.040465
2021-12-11 19:59:54,015 iteration 1334 : loss : 0.065870, loss_ce: 0.034051
2021-12-11 19:59:55,419 iteration 1335 : loss : 0.079435, loss_ce: 0.038349
2021-12-11 19:59:56,756 iteration 1336 : loss : 0.084847, loss_ce: 0.041698
2021-12-11 19:59:58,183 iteration 1337 : loss : 0.111716, loss_ce: 0.050065
2021-12-11 19:59:59,532 iteration 1338 : loss : 0.075103, loss_ce: 0.034461
2021-12-11 20:00:00,868 iteration 1339 : loss : 0.092052, loss_ce: 0.038591
2021-12-11 20:00:02,196 iteration 1340 : loss : 0.073402, loss_ce: 0.037722
2021-12-11 20:00:03,592 iteration 1341 : loss : 0.126405, loss_ce: 0.046651
2021-12-11 20:00:05,050 iteration 1342 : loss : 0.106413, loss_ce: 0.045068
2021-12-11 20:00:06,346 iteration 1343 : loss : 0.074483, loss_ce: 0.033429
 20%|█████▉                        | 79/400 [33:47<2:10:45, 24.44s/it]2021-12-11 20:00:07,804 iteration 1344 : loss : 0.093985, loss_ce: 0.034400
2021-12-11 20:00:09,154 iteration 1345 : loss : 0.109410, loss_ce: 0.035237
2021-12-11 20:00:10,568 iteration 1346 : loss : 0.084511, loss_ce: 0.037284
2021-12-11 20:00:11,957 iteration 1347 : loss : 0.090992, loss_ce: 0.045389
2021-12-11 20:00:13,297 iteration 1348 : loss : 0.096859, loss_ce: 0.045312
2021-12-11 20:00:14,677 iteration 1349 : loss : 0.134624, loss_ce: 0.052019
2021-12-11 20:00:16,039 iteration 1350 : loss : 0.107708, loss_ce: 0.041700
2021-12-11 20:00:17,495 iteration 1351 : loss : 0.125044, loss_ce: 0.055412
2021-12-11 20:00:18,824 iteration 1352 : loss : 0.107529, loss_ce: 0.045069
2021-12-11 20:00:20,199 iteration 1353 : loss : 0.097803, loss_ce: 0.046753
2021-12-11 20:00:21,576 iteration 1354 : loss : 0.082474, loss_ce: 0.035765
2021-12-11 20:00:22,887 iteration 1355 : loss : 0.084544, loss_ce: 0.034922
2021-12-11 20:00:24,330 iteration 1356 : loss : 0.080174, loss_ce: 0.033726
2021-12-11 20:00:25,657 iteration 1357 : loss : 0.086806, loss_ce: 0.046095
2021-12-11 20:00:27,007 iteration 1358 : loss : 0.096407, loss_ce: 0.046877
2021-12-11 20:00:28,462 iteration 1359 : loss : 0.098438, loss_ce: 0.051122
2021-12-11 20:00:28,462 Training Data Eval:
2021-12-11 20:00:35,264   Average segmentation loss on training set: 0.0667
2021-12-11 20:00:35,264 Validation Data Eval:
2021-12-11 20:00:37,627   Average segmentation loss on validation set: 0.1358
2021-12-11 20:00:39,019 iteration 1360 : loss : 0.101330, loss_ce: 0.044363
 20%|██████                        | 80/400 [34:20<2:23:31, 26.91s/it]2021-12-11 20:00:40,509 iteration 1361 : loss : 0.114549, loss_ce: 0.040699
2021-12-11 20:00:41,873 iteration 1362 : loss : 0.109448, loss_ce: 0.035672
2021-12-11 20:00:43,275 iteration 1363 : loss : 0.080550, loss_ce: 0.033925
2021-12-11 20:00:44,574 iteration 1364 : loss : 0.101615, loss_ce: 0.048875
2021-12-11 20:00:45,910 iteration 1365 : loss : 0.116335, loss_ce: 0.037974
2021-12-11 20:00:47,358 iteration 1366 : loss : 0.111299, loss_ce: 0.049586
2021-12-11 20:00:48,747 iteration 1367 : loss : 0.096780, loss_ce: 0.054373
2021-12-11 20:00:50,287 iteration 1368 : loss : 0.068324, loss_ce: 0.033092
2021-12-11 20:00:51,659 iteration 1369 : loss : 0.095495, loss_ce: 0.040363
2021-12-11 20:00:52,993 iteration 1370 : loss : 0.101717, loss_ce: 0.045372
2021-12-11 20:00:54,365 iteration 1371 : loss : 0.081558, loss_ce: 0.037665
2021-12-11 20:00:55,871 iteration 1372 : loss : 0.106915, loss_ce: 0.051347
2021-12-11 20:00:57,239 iteration 1373 : loss : 0.083337, loss_ce: 0.039990
2021-12-11 20:00:58,575 iteration 1374 : loss : 0.099835, loss_ce: 0.041553
2021-12-11 20:00:59,930 iteration 1375 : loss : 0.080868, loss_ce: 0.045172
2021-12-11 20:01:01,295 iteration 1376 : loss : 0.066681, loss_ce: 0.029621
2021-12-11 20:01:02,735 iteration 1377 : loss : 0.080781, loss_ce: 0.043584
 20%|██████                        | 81/400 [34:44<2:17:59, 25.95s/it]2021-12-11 20:01:04,265 iteration 1378 : loss : 0.098510, loss_ce: 0.050975
2021-12-11 20:01:05,695 iteration 1379 : loss : 0.103615, loss_ce: 0.047022
2021-12-11 20:01:07,085 iteration 1380 : loss : 0.107870, loss_ce: 0.043997
2021-12-11 20:01:08,520 iteration 1381 : loss : 0.089819, loss_ce: 0.041422
2021-12-11 20:01:09,929 iteration 1382 : loss : 0.126428, loss_ce: 0.048530
2021-12-11 20:01:11,261 iteration 1383 : loss : 0.087841, loss_ce: 0.034315
2021-12-11 20:01:12,688 iteration 1384 : loss : 0.089922, loss_ce: 0.044492
2021-12-11 20:01:14,087 iteration 1385 : loss : 0.076432, loss_ce: 0.035152
2021-12-11 20:01:15,459 iteration 1386 : loss : 0.095389, loss_ce: 0.042316
2021-12-11 20:01:16,948 iteration 1387 : loss : 0.089238, loss_ce: 0.039343
2021-12-11 20:01:18,326 iteration 1388 : loss : 0.091817, loss_ce: 0.042363
2021-12-11 20:01:19,694 iteration 1389 : loss : 0.092453, loss_ce: 0.038468
2021-12-11 20:01:21,104 iteration 1390 : loss : 0.079251, loss_ce: 0.038175
2021-12-11 20:01:22,495 iteration 1391 : loss : 0.104668, loss_ce: 0.033520
2021-12-11 20:01:23,888 iteration 1392 : loss : 0.100207, loss_ce: 0.041653
2021-12-11 20:01:25,257 iteration 1393 : loss : 0.080896, loss_ce: 0.033119
2021-12-11 20:01:26,631 iteration 1394 : loss : 0.063266, loss_ce: 0.034631
 20%|██████▏                       | 82/400 [35:07<2:14:16, 25.34s/it]2021-12-11 20:01:28,068 iteration 1395 : loss : 0.079118, loss_ce: 0.042082
2021-12-11 20:01:29,424 iteration 1396 : loss : 0.074411, loss_ce: 0.034709
2021-12-11 20:01:30,821 iteration 1397 : loss : 0.132705, loss_ce: 0.051979
2021-12-11 20:01:32,284 iteration 1398 : loss : 0.067970, loss_ce: 0.033143
2021-12-11 20:01:33,704 iteration 1399 : loss : 0.140004, loss_ce: 0.070803
2021-12-11 20:01:35,062 iteration 1400 : loss : 0.113508, loss_ce: 0.062875
2021-12-11 20:01:36,417 iteration 1401 : loss : 0.093846, loss_ce: 0.035520
2021-12-11 20:01:37,730 iteration 1402 : loss : 0.083415, loss_ce: 0.039533
2021-12-11 20:01:39,219 iteration 1403 : loss : 0.090274, loss_ce: 0.036658
2021-12-11 20:01:40,640 iteration 1404 : loss : 0.107704, loss_ce: 0.042265
2021-12-11 20:01:42,062 iteration 1405 : loss : 0.077237, loss_ce: 0.037767
2021-12-11 20:01:43,463 iteration 1406 : loss : 0.087909, loss_ce: 0.040420
2021-12-11 20:01:44,795 iteration 1407 : loss : 0.094704, loss_ce: 0.049530
2021-12-11 20:01:46,154 iteration 1408 : loss : 0.080590, loss_ce: 0.035132
2021-12-11 20:01:47,612 iteration 1409 : loss : 0.142495, loss_ce: 0.056682
2021-12-11 20:01:49,025 iteration 1410 : loss : 0.095182, loss_ce: 0.040114
2021-12-11 20:01:50,389 iteration 1411 : loss : 0.085347, loss_ce: 0.044268
 21%|██████▏                       | 83/400 [35:31<2:11:20, 24.86s/it]2021-12-11 20:01:51,734 iteration 1412 : loss : 0.091220, loss_ce: 0.037130
2021-12-11 20:01:53,157 iteration 1413 : loss : 0.062008, loss_ce: 0.029759
2021-12-11 20:01:54,535 iteration 1414 : loss : 0.077777, loss_ce: 0.040311
2021-12-11 20:01:55,836 iteration 1415 : loss : 0.108585, loss_ce: 0.044865
2021-12-11 20:01:57,219 iteration 1416 : loss : 0.061155, loss_ce: 0.025400
2021-12-11 20:01:58,642 iteration 1417 : loss : 0.072652, loss_ce: 0.035276
2021-12-11 20:01:59,960 iteration 1418 : loss : 0.099397, loss_ce: 0.035339
2021-12-11 20:02:01,446 iteration 1419 : loss : 0.103124, loss_ce: 0.042655
2021-12-11 20:02:02,880 iteration 1420 : loss : 0.108778, loss_ce: 0.037262
2021-12-11 20:02:04,222 iteration 1421 : loss : 0.092254, loss_ce: 0.044164
2021-12-11 20:02:05,597 iteration 1422 : loss : 0.098539, loss_ce: 0.041874
2021-12-11 20:02:06,940 iteration 1423 : loss : 0.113532, loss_ce: 0.055899
2021-12-11 20:02:08,357 iteration 1424 : loss : 0.096950, loss_ce: 0.040696
2021-12-11 20:02:09,805 iteration 1425 : loss : 0.098267, loss_ce: 0.043660
2021-12-11 20:02:11,067 iteration 1426 : loss : 0.090926, loss_ce: 0.035844
2021-12-11 20:02:12,421 iteration 1427 : loss : 0.069821, loss_ce: 0.035238
2021-12-11 20:02:13,799 iteration 1428 : loss : 0.101278, loss_ce: 0.042206
 21%|██████▎                       | 84/400 [35:55<2:08:38, 24.43s/it]2021-12-11 20:02:15,332 iteration 1429 : loss : 0.103524, loss_ce: 0.046740
2021-12-11 20:02:16,668 iteration 1430 : loss : 0.086858, loss_ce: 0.037393
2021-12-11 20:02:18,044 iteration 1431 : loss : 0.093176, loss_ce: 0.049807
2021-12-11 20:02:19,433 iteration 1432 : loss : 0.106410, loss_ce: 0.049329
2021-12-11 20:02:20,898 iteration 1433 : loss : 0.109031, loss_ce: 0.054134
2021-12-11 20:02:22,331 iteration 1434 : loss : 0.077845, loss_ce: 0.035798
2021-12-11 20:02:23,616 iteration 1435 : loss : 0.081332, loss_ce: 0.031419
2021-12-11 20:02:25,062 iteration 1436 : loss : 0.087071, loss_ce: 0.043288
2021-12-11 20:02:26,377 iteration 1437 : loss : 0.123026, loss_ce: 0.035743
2021-12-11 20:02:27,667 iteration 1438 : loss : 0.066824, loss_ce: 0.035847
2021-12-11 20:02:29,044 iteration 1439 : loss : 0.063297, loss_ce: 0.027692
2021-12-11 20:02:30,486 iteration 1440 : loss : 0.109268, loss_ce: 0.054679
2021-12-11 20:02:31,840 iteration 1441 : loss : 0.075101, loss_ce: 0.037537
2021-12-11 20:02:33,166 iteration 1442 : loss : 0.088219, loss_ce: 0.038171
2021-12-11 20:02:34,568 iteration 1443 : loss : 0.112545, loss_ce: 0.046826
2021-12-11 20:02:36,006 iteration 1444 : loss : 0.067993, loss_ce: 0.036157
2021-12-11 20:02:36,006 Training Data Eval:
2021-12-11 20:02:42,824   Average segmentation loss on training set: 0.0578
2021-12-11 20:02:42,824 Validation Data Eval:
2021-12-11 20:02:45,177   Average segmentation loss on validation set: 0.1040
2021-12-11 20:02:47,100 Found new lowest validation loss at iteration 1444! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 20:02:48,528 iteration 1445 : loss : 0.079139, loss_ce: 0.034239
 21%|██████▍                       | 85/400 [36:29<2:24:28, 27.52s/it]2021-12-11 20:02:50,014 iteration 1446 : loss : 0.098097, loss_ce: 0.038298
2021-12-11 20:02:51,434 iteration 1447 : loss : 0.117518, loss_ce: 0.042359
2021-12-11 20:02:52,800 iteration 1448 : loss : 0.095343, loss_ce: 0.049334
2021-12-11 20:02:54,195 iteration 1449 : loss : 0.091776, loss_ce: 0.038841
2021-12-11 20:02:55,493 iteration 1450 : loss : 0.085744, loss_ce: 0.044359
2021-12-11 20:02:56,819 iteration 1451 : loss : 0.072366, loss_ce: 0.031715
2021-12-11 20:02:58,198 iteration 1452 : loss : 0.119965, loss_ce: 0.030027
2021-12-11 20:02:59,629 iteration 1453 : loss : 0.125675, loss_ce: 0.050230
2021-12-11 20:03:00,976 iteration 1454 : loss : 0.083726, loss_ce: 0.042741
2021-12-11 20:03:02,329 iteration 1455 : loss : 0.072509, loss_ce: 0.034575
2021-12-11 20:03:03,719 iteration 1456 : loss : 0.096387, loss_ce: 0.040339
2021-12-11 20:03:05,100 iteration 1457 : loss : 0.061568, loss_ce: 0.030852
2021-12-11 20:03:06,471 iteration 1458 : loss : 0.075325, loss_ce: 0.033636
2021-12-11 20:03:07,763 iteration 1459 : loss : 0.063377, loss_ce: 0.032848
2021-12-11 20:03:09,182 iteration 1460 : loss : 0.111413, loss_ce: 0.055676
2021-12-11 20:03:10,550 iteration 1461 : loss : 0.081657, loss_ce: 0.030999
2021-12-11 20:03:11,927 iteration 1462 : loss : 0.073911, loss_ce: 0.035373
 22%|██████▍                       | 86/400 [36:53<2:17:32, 26.28s/it]2021-12-11 20:03:13,352 iteration 1463 : loss : 0.085015, loss_ce: 0.038316
2021-12-11 20:03:14,765 iteration 1464 : loss : 0.075108, loss_ce: 0.028730
2021-12-11 20:03:16,194 iteration 1465 : loss : 0.126765, loss_ce: 0.058122
2021-12-11 20:03:17,571 iteration 1466 : loss : 0.101632, loss_ce: 0.037610
2021-12-11 20:03:18,939 iteration 1467 : loss : 0.087304, loss_ce: 0.038060
2021-12-11 20:03:20,330 iteration 1468 : loss : 0.076970, loss_ce: 0.033902
2021-12-11 20:03:21,738 iteration 1469 : loss : 0.057040, loss_ce: 0.029531
2021-12-11 20:03:23,128 iteration 1470 : loss : 0.077808, loss_ce: 0.035663
2021-12-11 20:03:24,483 iteration 1471 : loss : 0.076884, loss_ce: 0.031418
2021-12-11 20:03:25,840 iteration 1472 : loss : 0.069914, loss_ce: 0.030462
2021-12-11 20:03:27,279 iteration 1473 : loss : 0.078710, loss_ce: 0.045828
2021-12-11 20:03:28,606 iteration 1474 : loss : 0.097442, loss_ce: 0.040827
2021-12-11 20:03:29,972 iteration 1475 : loss : 0.074239, loss_ce: 0.031283
2021-12-11 20:03:31,391 iteration 1476 : loss : 0.110104, loss_ce: 0.061529
2021-12-11 20:03:32,762 iteration 1477 : loss : 0.066439, loss_ce: 0.028344
2021-12-11 20:03:34,194 iteration 1478 : loss : 0.078568, loss_ce: 0.032835
2021-12-11 20:03:35,595 iteration 1479 : loss : 0.110108, loss_ce: 0.046426
 22%|██████▌                       | 87/400 [37:16<2:13:00, 25.50s/it]2021-12-11 20:03:37,030 iteration 1480 : loss : 0.095722, loss_ce: 0.052649
2021-12-11 20:03:38,485 iteration 1481 : loss : 0.084208, loss_ce: 0.044136
2021-12-11 20:03:39,844 iteration 1482 : loss : 0.059834, loss_ce: 0.030725
2021-12-11 20:03:41,163 iteration 1483 : loss : 0.069927, loss_ce: 0.033565
2021-12-11 20:03:42,485 iteration 1484 : loss : 0.078782, loss_ce: 0.038171
2021-12-11 20:03:43,836 iteration 1485 : loss : 0.078159, loss_ce: 0.033646
2021-12-11 20:03:45,189 iteration 1486 : loss : 0.094380, loss_ce: 0.035454
2021-12-11 20:03:46,632 iteration 1487 : loss : 0.084784, loss_ce: 0.035865
2021-12-11 20:03:47,995 iteration 1488 : loss : 0.068408, loss_ce: 0.028362
2021-12-11 20:03:49,394 iteration 1489 : loss : 0.078458, loss_ce: 0.035053
2021-12-11 20:03:50,771 iteration 1490 : loss : 0.074448, loss_ce: 0.031087
2021-12-11 20:03:52,086 iteration 1491 : loss : 0.098068, loss_ce: 0.043194
2021-12-11 20:03:53,492 iteration 1492 : loss : 0.094614, loss_ce: 0.042382
2021-12-11 20:03:54,967 iteration 1493 : loss : 0.093656, loss_ce: 0.038717
2021-12-11 20:03:56,377 iteration 1494 : loss : 0.102185, loss_ce: 0.040123
2021-12-11 20:03:57,757 iteration 1495 : loss : 0.102329, loss_ce: 0.037004
2021-12-11 20:03:59,140 iteration 1496 : loss : 0.072620, loss_ce: 0.034767
 22%|██████▌                       | 88/400 [37:40<2:09:33, 24.91s/it]2021-12-11 20:04:00,547 iteration 1497 : loss : 0.062446, loss_ce: 0.029518
2021-12-11 20:04:01,825 iteration 1498 : loss : 0.058979, loss_ce: 0.032616
2021-12-11 20:04:03,143 iteration 1499 : loss : 0.076856, loss_ce: 0.039002
2021-12-11 20:04:04,640 iteration 1500 : loss : 0.079173, loss_ce: 0.033042
2021-12-11 20:04:05,945 iteration 1501 : loss : 0.074969, loss_ce: 0.029919
2021-12-11 20:04:07,328 iteration 1502 : loss : 0.083701, loss_ce: 0.031973
2021-12-11 20:04:08,778 iteration 1503 : loss : 0.074969, loss_ce: 0.031146
2021-12-11 20:04:10,136 iteration 1504 : loss : 0.081220, loss_ce: 0.039019
2021-12-11 20:04:11,465 iteration 1505 : loss : 0.080826, loss_ce: 0.033026
2021-12-11 20:04:12,893 iteration 1506 : loss : 0.076926, loss_ce: 0.032046
2021-12-11 20:04:14,243 iteration 1507 : loss : 0.074347, loss_ce: 0.034809
2021-12-11 20:04:15,638 iteration 1508 : loss : 0.098625, loss_ce: 0.026832
2021-12-11 20:04:17,002 iteration 1509 : loss : 0.097711, loss_ce: 0.043653
2021-12-11 20:04:18,352 iteration 1510 : loss : 0.098827, loss_ce: 0.034883
2021-12-11 20:04:19,821 iteration 1511 : loss : 0.089088, loss_ce: 0.048852
2021-12-11 20:04:21,199 iteration 1512 : loss : 0.086693, loss_ce: 0.033592
2021-12-11 20:04:22,583 iteration 1513 : loss : 0.079311, loss_ce: 0.037144
 22%|██████▋                       | 89/400 [38:03<2:06:50, 24.47s/it]2021-12-11 20:04:24,038 iteration 1514 : loss : 0.096909, loss_ce: 0.042718
2021-12-11 20:04:25,422 iteration 1515 : loss : 0.065675, loss_ce: 0.032585
2021-12-11 20:04:26,833 iteration 1516 : loss : 0.081037, loss_ce: 0.042840
2021-12-11 20:04:28,181 iteration 1517 : loss : 0.092026, loss_ce: 0.043580
2021-12-11 20:04:29,490 iteration 1518 : loss : 0.072158, loss_ce: 0.035723
2021-12-11 20:04:30,852 iteration 1519 : loss : 0.074129, loss_ce: 0.031637
2021-12-11 20:04:32,363 iteration 1520 : loss : 0.084782, loss_ce: 0.039386
2021-12-11 20:04:33,696 iteration 1521 : loss : 0.058204, loss_ce: 0.026324
2021-12-11 20:04:35,124 iteration 1522 : loss : 0.068061, loss_ce: 0.027651
2021-12-11 20:04:36,506 iteration 1523 : loss : 0.077982, loss_ce: 0.035103
2021-12-11 20:04:37,835 iteration 1524 : loss : 0.062042, loss_ce: 0.026703
2021-12-11 20:04:39,186 iteration 1525 : loss : 0.066921, loss_ce: 0.034116
2021-12-11 20:04:40,539 iteration 1526 : loss : 0.156033, loss_ce: 0.066610
2021-12-11 20:04:41,980 iteration 1527 : loss : 0.064750, loss_ce: 0.028324
2021-12-11 20:04:43,384 iteration 1528 : loss : 0.087767, loss_ce: 0.038472
2021-12-11 20:04:44,750 iteration 1529 : loss : 0.085213, loss_ce: 0.031108
2021-12-11 20:04:44,750 Training Data Eval:
2021-12-11 20:04:51,567   Average segmentation loss on training set: 0.0517
2021-12-11 20:04:51,568 Validation Data Eval:
2021-12-11 20:04:53,929   Average segmentation loss on validation set: 0.1138
2021-12-11 20:04:55,277 iteration 1530 : loss : 0.066054, loss_ce: 0.035186
 22%|██████▊                       | 90/400 [38:36<2:19:11, 26.94s/it]2021-12-11 20:04:56,738 iteration 1531 : loss : 0.079285, loss_ce: 0.039023
2021-12-11 20:04:58,152 iteration 1532 : loss : 0.106145, loss_ce: 0.042936
2021-12-11 20:04:59,480 iteration 1533 : loss : 0.077022, loss_ce: 0.032417
2021-12-11 20:05:00,844 iteration 1534 : loss : 0.055836, loss_ce: 0.029976
2021-12-11 20:05:02,178 iteration 1535 : loss : 0.096337, loss_ce: 0.043623
2021-12-11 20:05:03,566 iteration 1536 : loss : 0.087464, loss_ce: 0.031315
2021-12-11 20:05:04,909 iteration 1537 : loss : 0.120423, loss_ce: 0.061666
2021-12-11 20:05:06,180 iteration 1538 : loss : 0.100042, loss_ce: 0.033753
2021-12-11 20:05:07,496 iteration 1539 : loss : 0.083236, loss_ce: 0.038240
2021-12-11 20:05:08,878 iteration 1540 : loss : 0.071108, loss_ce: 0.037637
2021-12-11 20:05:10,304 iteration 1541 : loss : 0.072921, loss_ce: 0.034022
2021-12-11 20:05:11,767 iteration 1542 : loss : 0.085036, loss_ce: 0.037879
2021-12-11 20:05:13,152 iteration 1543 : loss : 0.087948, loss_ce: 0.028339
2021-12-11 20:05:14,553 iteration 1544 : loss : 0.059137, loss_ce: 0.029873
2021-12-11 20:05:15,867 iteration 1545 : loss : 0.093368, loss_ce: 0.038593
2021-12-11 20:05:17,225 iteration 1546 : loss : 0.075699, loss_ce: 0.029997
2021-12-11 20:05:18,627 iteration 1547 : loss : 0.067810, loss_ce: 0.034185
 23%|██████▊                       | 91/400 [38:59<2:13:11, 25.86s/it]2021-12-11 20:05:20,045 iteration 1548 : loss : 0.088157, loss_ce: 0.031578
2021-12-11 20:05:21,356 iteration 1549 : loss : 0.074485, loss_ce: 0.027384
2021-12-11 20:05:22,762 iteration 1550 : loss : 0.076773, loss_ce: 0.026600
2021-12-11 20:05:24,159 iteration 1551 : loss : 0.087575, loss_ce: 0.034329
2021-12-11 20:05:25,523 iteration 1552 : loss : 0.075258, loss_ce: 0.027627
2021-12-11 20:05:26,934 iteration 1553 : loss : 0.076514, loss_ce: 0.039517
2021-12-11 20:05:28,235 iteration 1554 : loss : 0.094128, loss_ce: 0.050145
2021-12-11 20:05:29,571 iteration 1555 : loss : 0.074036, loss_ce: 0.030915
2021-12-11 20:05:31,037 iteration 1556 : loss : 0.107418, loss_ce: 0.048643
2021-12-11 20:05:32,392 iteration 1557 : loss : 0.067809, loss_ce: 0.030463
2021-12-11 20:05:33,707 iteration 1558 : loss : 0.072693, loss_ce: 0.030786
2021-12-11 20:05:35,151 iteration 1559 : loss : 0.105852, loss_ce: 0.046134
2021-12-11 20:05:36,493 iteration 1560 : loss : 0.079379, loss_ce: 0.047280
2021-12-11 20:05:37,864 iteration 1561 : loss : 0.063029, loss_ce: 0.029830
2021-12-11 20:05:39,235 iteration 1562 : loss : 0.082877, loss_ce: 0.046561
2021-12-11 20:05:40,634 iteration 1563 : loss : 0.099140, loss_ce: 0.039263
2021-12-11 20:05:41,983 iteration 1564 : loss : 0.100168, loss_ce: 0.037893
 23%|██████▉                       | 92/400 [39:23<2:08:54, 25.11s/it]2021-12-11 20:05:43,370 iteration 1565 : loss : 0.072400, loss_ce: 0.041529
2021-12-11 20:05:44,781 iteration 1566 : loss : 0.090131, loss_ce: 0.030163
2021-12-11 20:05:46,165 iteration 1567 : loss : 0.085912, loss_ce: 0.037469
2021-12-11 20:05:47,641 iteration 1568 : loss : 0.073115, loss_ce: 0.031610
2021-12-11 20:05:49,017 iteration 1569 : loss : 0.093518, loss_ce: 0.046246
2021-12-11 20:05:50,403 iteration 1570 : loss : 0.068434, loss_ce: 0.026863
2021-12-11 20:05:51,783 iteration 1571 : loss : 0.096331, loss_ce: 0.035460
2021-12-11 20:05:53,246 iteration 1572 : loss : 0.104224, loss_ce: 0.040697
2021-12-11 20:05:54,556 iteration 1573 : loss : 0.055147, loss_ce: 0.030040
2021-12-11 20:05:55,963 iteration 1574 : loss : 0.072785, loss_ce: 0.031094
2021-12-11 20:05:57,418 iteration 1575 : loss : 0.083129, loss_ce: 0.039626
2021-12-11 20:05:58,856 iteration 1576 : loss : 0.061883, loss_ce: 0.027099
2021-12-11 20:06:00,165 iteration 1577 : loss : 0.071445, loss_ce: 0.030846
2021-12-11 20:06:01,522 iteration 1578 : loss : 0.055311, loss_ce: 0.026522
2021-12-11 20:06:02,966 iteration 1579 : loss : 0.071696, loss_ce: 0.030524
2021-12-11 20:06:04,378 iteration 1580 : loss : 0.099765, loss_ce: 0.037296
2021-12-11 20:06:05,868 iteration 1581 : loss : 0.073973, loss_ce: 0.034696
 23%|██████▉                       | 93/400 [39:47<2:06:36, 24.74s/it]2021-12-11 20:06:07,328 iteration 1582 : loss : 0.066357, loss_ce: 0.030307
2021-12-11 20:06:08,736 iteration 1583 : loss : 0.103743, loss_ce: 0.038859
2021-12-11 20:06:10,051 iteration 1584 : loss : 0.087964, loss_ce: 0.037840
2021-12-11 20:06:11,435 iteration 1585 : loss : 0.149987, loss_ce: 0.044698
2021-12-11 20:06:12,873 iteration 1586 : loss : 0.098287, loss_ce: 0.034601
2021-12-11 20:06:14,260 iteration 1587 : loss : 0.088619, loss_ce: 0.032678
2021-12-11 20:06:15,690 iteration 1588 : loss : 0.107555, loss_ce: 0.052040
2021-12-11 20:06:17,076 iteration 1589 : loss : 0.081214, loss_ce: 0.028232
2021-12-11 20:06:18,474 iteration 1590 : loss : 0.062678, loss_ce: 0.032021
2021-12-11 20:06:19,815 iteration 1591 : loss : 0.101400, loss_ce: 0.052610
2021-12-11 20:06:21,158 iteration 1592 : loss : 0.082583, loss_ce: 0.036858
2021-12-11 20:06:22,593 iteration 1593 : loss : 0.084573, loss_ce: 0.031098
2021-12-11 20:06:23,983 iteration 1594 : loss : 0.064043, loss_ce: 0.035729
2021-12-11 20:06:25,377 iteration 1595 : loss : 0.098161, loss_ce: 0.044306
2021-12-11 20:06:26,759 iteration 1596 : loss : 0.053280, loss_ce: 0.026515
2021-12-11 20:06:28,100 iteration 1597 : loss : 0.070616, loss_ce: 0.028962
2021-12-11 20:06:29,515 iteration 1598 : loss : 0.071724, loss_ce: 0.031762
 24%|███████                       | 94/400 [40:10<2:04:30, 24.41s/it]2021-12-11 20:06:30,978 iteration 1599 : loss : 0.049141, loss_ce: 0.025653
2021-12-11 20:06:32,337 iteration 1600 : loss : 0.101483, loss_ce: 0.040337
2021-12-11 20:06:33,714 iteration 1601 : loss : 0.071626, loss_ce: 0.028872
2021-12-11 20:06:35,068 iteration 1602 : loss : 0.061645, loss_ce: 0.032168
2021-12-11 20:06:36,520 iteration 1603 : loss : 0.083138, loss_ce: 0.035693
2021-12-11 20:06:37,872 iteration 1604 : loss : 0.126002, loss_ce: 0.061172
2021-12-11 20:06:39,325 iteration 1605 : loss : 0.069568, loss_ce: 0.029815
2021-12-11 20:06:40,749 iteration 1606 : loss : 0.067599, loss_ce: 0.027778
2021-12-11 20:06:42,131 iteration 1607 : loss : 0.069585, loss_ce: 0.032489
2021-12-11 20:06:43,537 iteration 1608 : loss : 0.054645, loss_ce: 0.024484
2021-12-11 20:06:44,941 iteration 1609 : loss : 0.057542, loss_ce: 0.025798
2021-12-11 20:06:46,308 iteration 1610 : loss : 0.087767, loss_ce: 0.041027
2021-12-11 20:06:47,699 iteration 1611 : loss : 0.081514, loss_ce: 0.036576
2021-12-11 20:06:49,085 iteration 1612 : loss : 0.077253, loss_ce: 0.038493
2021-12-11 20:06:50,481 iteration 1613 : loss : 0.069839, loss_ce: 0.031746
2021-12-11 20:06:51,879 iteration 1614 : loss : 0.060499, loss_ce: 0.024304
2021-12-11 20:06:51,879 Training Data Eval:
2021-12-11 20:06:58,683   Average segmentation loss on training set: 0.0488
2021-12-11 20:06:58,683 Validation Data Eval:
2021-12-11 20:07:01,037   Average segmentation loss on validation set: 0.0940
2021-12-11 20:07:02,979 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 20:07:04,410 iteration 1615 : loss : 0.094616, loss_ce: 0.045316
 24%|███████▏                      | 95/400 [40:45<2:20:05, 27.56s/it]2021-12-11 20:07:05,819 iteration 1616 : loss : 0.081532, loss_ce: 0.029035
2021-12-11 20:07:07,145 iteration 1617 : loss : 0.052461, loss_ce: 0.027438
2021-12-11 20:07:08,506 iteration 1618 : loss : 0.038693, loss_ce: 0.020256
2021-12-11 20:07:09,923 iteration 1619 : loss : 0.050037, loss_ce: 0.028689
2021-12-11 20:07:11,247 iteration 1620 : loss : 0.082090, loss_ce: 0.033450
2021-12-11 20:07:12,652 iteration 1621 : loss : 0.070508, loss_ce: 0.033328
2021-12-11 20:07:14,000 iteration 1622 : loss : 0.125988, loss_ce: 0.049538
2021-12-11 20:07:15,307 iteration 1623 : loss : 0.063384, loss_ce: 0.033660
2021-12-11 20:07:16,687 iteration 1624 : loss : 0.074366, loss_ce: 0.034268
2021-12-11 20:07:17,982 iteration 1625 : loss : 0.059809, loss_ce: 0.033407
2021-12-11 20:07:19,315 iteration 1626 : loss : 0.057568, loss_ce: 0.028050
2021-12-11 20:07:20,641 iteration 1627 : loss : 0.125466, loss_ce: 0.039616
2021-12-11 20:07:22,035 iteration 1628 : loss : 0.085438, loss_ce: 0.030885
2021-12-11 20:07:23,429 iteration 1629 : loss : 0.080828, loss_ce: 0.041533
2021-12-11 20:07:24,748 iteration 1630 : loss : 0.067710, loss_ce: 0.032504
2021-12-11 20:07:26,117 iteration 1631 : loss : 0.078850, loss_ce: 0.036893
2021-12-11 20:07:27,395 iteration 1632 : loss : 0.058018, loss_ce: 0.028999
 24%|███████▏                      | 96/400 [41:08<2:12:40, 26.19s/it]2021-12-11 20:07:28,879 iteration 1633 : loss : 0.074494, loss_ce: 0.029402
2021-12-11 20:07:30,277 iteration 1634 : loss : 0.061582, loss_ce: 0.031650
2021-12-11 20:07:31,640 iteration 1635 : loss : 0.071306, loss_ce: 0.037156
2021-12-11 20:07:32,914 iteration 1636 : loss : 0.057076, loss_ce: 0.028475
2021-12-11 20:07:34,340 iteration 1637 : loss : 0.074844, loss_ce: 0.030766
2021-12-11 20:07:35,739 iteration 1638 : loss : 0.052750, loss_ce: 0.027081
2021-12-11 20:07:37,182 iteration 1639 : loss : 0.070652, loss_ce: 0.033530
2021-12-11 20:07:38,560 iteration 1640 : loss : 0.069723, loss_ce: 0.031471
2021-12-11 20:07:39,967 iteration 1641 : loss : 0.086583, loss_ce: 0.035772
2021-12-11 20:07:41,454 iteration 1642 : loss : 0.091669, loss_ce: 0.038070
2021-12-11 20:07:42,862 iteration 1643 : loss : 0.048686, loss_ce: 0.024317
2021-12-11 20:07:44,263 iteration 1644 : loss : 0.070559, loss_ce: 0.033391
2021-12-11 20:07:45,550 iteration 1645 : loss : 0.066003, loss_ce: 0.025650
2021-12-11 20:07:46,908 iteration 1646 : loss : 0.072288, loss_ce: 0.032134
2021-12-11 20:07:48,351 iteration 1647 : loss : 0.074513, loss_ce: 0.041491
2021-12-11 20:07:49,716 iteration 1648 : loss : 0.064880, loss_ce: 0.033444
2021-12-11 20:07:51,179 iteration 1649 : loss : 0.129414, loss_ce: 0.038177
 24%|███████▎                      | 97/400 [41:32<2:08:35, 25.46s/it]2021-12-11 20:07:52,625 iteration 1650 : loss : 0.065421, loss_ce: 0.025063
2021-12-11 20:07:54,070 iteration 1651 : loss : 0.105558, loss_ce: 0.052483
2021-12-11 20:07:55,503 iteration 1652 : loss : 0.120227, loss_ce: 0.039408
2021-12-11 20:07:56,926 iteration 1653 : loss : 0.092140, loss_ce: 0.045368
2021-12-11 20:07:58,290 iteration 1654 : loss : 0.060038, loss_ce: 0.024563
2021-12-11 20:07:59,645 iteration 1655 : loss : 0.079252, loss_ce: 0.031929
2021-12-11 20:08:00,998 iteration 1656 : loss : 0.051616, loss_ce: 0.028640
2021-12-11 20:08:02,443 iteration 1657 : loss : 0.088091, loss_ce: 0.044168
2021-12-11 20:08:03,768 iteration 1658 : loss : 0.060967, loss_ce: 0.029319
2021-12-11 20:08:05,145 iteration 1659 : loss : 0.086807, loss_ce: 0.039760
2021-12-11 20:08:06,641 iteration 1660 : loss : 0.107580, loss_ce: 0.051671
2021-12-11 20:08:08,032 iteration 1661 : loss : 0.072129, loss_ce: 0.029657
2021-12-11 20:08:09,392 iteration 1662 : loss : 0.073077, loss_ce: 0.025985
2021-12-11 20:08:10,829 iteration 1663 : loss : 0.089228, loss_ce: 0.035735
2021-12-11 20:08:12,259 iteration 1664 : loss : 0.089396, loss_ce: 0.032627
2021-12-11 20:08:13,603 iteration 1665 : loss : 0.046429, loss_ce: 0.027076
2021-12-11 20:08:14,953 iteration 1666 : loss : 0.053313, loss_ce: 0.025240
 24%|███████▎                      | 98/400 [41:56<2:05:37, 24.96s/it]2021-12-11 20:08:16,382 iteration 1667 : loss : 0.074349, loss_ce: 0.033992
2021-12-11 20:08:17,826 iteration 1668 : loss : 0.074199, loss_ce: 0.036983
2021-12-11 20:08:19,189 iteration 1669 : loss : 0.058633, loss_ce: 0.028162
2021-12-11 20:08:20,564 iteration 1670 : loss : 0.072537, loss_ce: 0.028620
2021-12-11 20:08:21,970 iteration 1671 : loss : 0.064838, loss_ce: 0.030495
2021-12-11 20:08:23,332 iteration 1672 : loss : 0.056153, loss_ce: 0.023834
2021-12-11 20:08:24,666 iteration 1673 : loss : 0.056642, loss_ce: 0.027872
2021-12-11 20:08:26,106 iteration 1674 : loss : 0.061909, loss_ce: 0.030945
2021-12-11 20:08:27,467 iteration 1675 : loss : 0.059513, loss_ce: 0.027728
2021-12-11 20:08:28,899 iteration 1676 : loss : 0.071914, loss_ce: 0.035036
2021-12-11 20:08:30,334 iteration 1677 : loss : 0.066481, loss_ce: 0.031193
2021-12-11 20:08:31,751 iteration 1678 : loss : 0.083751, loss_ce: 0.028936
2021-12-11 20:08:33,176 iteration 1679 : loss : 0.066570, loss_ce: 0.026960
2021-12-11 20:08:34,585 iteration 1680 : loss : 0.055645, loss_ce: 0.032101
2021-12-11 20:08:35,961 iteration 1681 : loss : 0.111744, loss_ce: 0.039519
2021-12-11 20:08:37,304 iteration 1682 : loss : 0.075620, loss_ce: 0.041223
2021-12-11 20:08:38,725 iteration 1683 : loss : 0.088306, loss_ce: 0.041852
 25%|███████▍                      | 99/400 [42:20<2:03:25, 24.60s/it]2021-12-11 20:08:40,167 iteration 1684 : loss : 0.064914, loss_ce: 0.029899
2021-12-11 20:08:41,502 iteration 1685 : loss : 0.063166, loss_ce: 0.028715
2021-12-11 20:08:42,938 iteration 1686 : loss : 0.065941, loss_ce: 0.029611
2021-12-11 20:08:44,272 iteration 1687 : loss : 0.044092, loss_ce: 0.020620
2021-12-11 20:08:45,668 iteration 1688 : loss : 0.070025, loss_ce: 0.031066
2021-12-11 20:08:47,077 iteration 1689 : loss : 0.100473, loss_ce: 0.048170
2021-12-11 20:08:48,385 iteration 1690 : loss : 0.066601, loss_ce: 0.025703
2021-12-11 20:08:49,695 iteration 1691 : loss : 0.070956, loss_ce: 0.030547
2021-12-11 20:08:51,075 iteration 1692 : loss : 0.063246, loss_ce: 0.028692
2021-12-11 20:08:52,502 iteration 1693 : loss : 0.064769, loss_ce: 0.029548
2021-12-11 20:08:53,928 iteration 1694 : loss : 0.080629, loss_ce: 0.028357
2021-12-11 20:08:55,292 iteration 1695 : loss : 0.079856, loss_ce: 0.032658
2021-12-11 20:08:56,676 iteration 1696 : loss : 0.090035, loss_ce: 0.045752
2021-12-11 20:08:58,081 iteration 1697 : loss : 0.073135, loss_ce: 0.032767
2021-12-11 20:08:59,480 iteration 1698 : loss : 0.054111, loss_ce: 0.026500
2021-12-11 20:09:00,842 iteration 1699 : loss : 0.058307, loss_ce: 0.030884
2021-12-11 20:09:00,842 Training Data Eval:
2021-12-11 20:09:07,672   Average segmentation loss on training set: 0.0476
2021-12-11 20:09:07,673 Validation Data Eval:
2021-12-11 20:09:10,031   Average segmentation loss on validation set: 0.1107
2021-12-11 20:09:11,421 iteration 1700 : loss : 0.065764, loss_ce: 0.031828
 25%|███████▎                     | 100/400 [42:52<2:15:09, 27.03s/it]2021-12-11 20:09:12,856 iteration 1701 : loss : 0.055173, loss_ce: 0.028887
2021-12-11 20:09:14,245 iteration 1702 : loss : 0.058254, loss_ce: 0.027378
2021-12-11 20:09:15,687 iteration 1703 : loss : 0.084266, loss_ce: 0.037826
2021-12-11 20:09:17,066 iteration 1704 : loss : 0.061281, loss_ce: 0.032948
2021-12-11 20:09:18,483 iteration 1705 : loss : 0.069085, loss_ce: 0.031386
2021-12-11 20:09:19,928 iteration 1706 : loss : 0.080987, loss_ce: 0.035326
2021-12-11 20:09:21,288 iteration 1707 : loss : 0.063388, loss_ce: 0.033039
2021-12-11 20:09:22,664 iteration 1708 : loss : 0.128431, loss_ce: 0.039315
2021-12-11 20:09:24,040 iteration 1709 : loss : 0.085007, loss_ce: 0.039237
2021-12-11 20:09:25,460 iteration 1710 : loss : 0.068200, loss_ce: 0.026893
2021-12-11 20:09:26,840 iteration 1711 : loss : 0.071519, loss_ce: 0.036289
2021-12-11 20:09:28,165 iteration 1712 : loss : 0.050503, loss_ce: 0.026906
2021-12-11 20:09:29,569 iteration 1713 : loss : 0.069000, loss_ce: 0.026752
2021-12-11 20:09:31,030 iteration 1714 : loss : 0.075765, loss_ce: 0.027894
2021-12-11 20:09:32,461 iteration 1715 : loss : 0.069408, loss_ce: 0.036005
2021-12-11 20:09:33,774 iteration 1716 : loss : 0.066567, loss_ce: 0.026438
2021-12-11 20:09:35,204 iteration 1717 : loss : 0.080831, loss_ce: 0.029386
 25%|███████▎                     | 101/400 [43:16<2:09:50, 26.06s/it]2021-12-11 20:09:36,650 iteration 1718 : loss : 0.073249, loss_ce: 0.036041
2021-12-11 20:09:38,014 iteration 1719 : loss : 0.094398, loss_ce: 0.027106
2021-12-11 20:09:39,437 iteration 1720 : loss : 0.063749, loss_ce: 0.029182
2021-12-11 20:09:40,789 iteration 1721 : loss : 0.059781, loss_ce: 0.025781
2021-12-11 20:09:42,205 iteration 1722 : loss : 0.066919, loss_ce: 0.023460
2021-12-11 20:09:43,543 iteration 1723 : loss : 0.085594, loss_ce: 0.031924
2021-12-11 20:09:44,945 iteration 1724 : loss : 0.043220, loss_ce: 0.021870
2021-12-11 20:09:46,334 iteration 1725 : loss : 0.092559, loss_ce: 0.033217
2021-12-11 20:09:47,726 iteration 1726 : loss : 0.092159, loss_ce: 0.037369
2021-12-11 20:09:49,175 iteration 1727 : loss : 0.082475, loss_ce: 0.041822
2021-12-11 20:09:50,472 iteration 1728 : loss : 0.078511, loss_ce: 0.031273
2021-12-11 20:09:51,877 iteration 1729 : loss : 0.063621, loss_ce: 0.026992
2021-12-11 20:09:53,306 iteration 1730 : loss : 0.056176, loss_ce: 0.029253
2021-12-11 20:09:54,674 iteration 1731 : loss : 0.088798, loss_ce: 0.048223
2021-12-11 20:09:56,030 iteration 1732 : loss : 0.065970, loss_ce: 0.032826
2021-12-11 20:09:57,363 iteration 1733 : loss : 0.050823, loss_ce: 0.025135
2021-12-11 20:09:58,723 iteration 1734 : loss : 0.063813, loss_ce: 0.030359
 26%|███████▍                     | 102/400 [43:40<2:05:37, 25.29s/it]2021-12-11 20:10:00,274 iteration 1735 : loss : 0.055819, loss_ce: 0.028037
2021-12-11 20:10:01,633 iteration 1736 : loss : 0.072628, loss_ce: 0.034245
2021-12-11 20:10:03,126 iteration 1737 : loss : 0.100708, loss_ce: 0.046605
2021-12-11 20:10:04,507 iteration 1738 : loss : 0.056352, loss_ce: 0.031790
2021-12-11 20:10:05,826 iteration 1739 : loss : 0.056351, loss_ce: 0.025667
2021-12-11 20:10:07,255 iteration 1740 : loss : 0.051374, loss_ce: 0.026759
2021-12-11 20:10:08,668 iteration 1741 : loss : 0.050048, loss_ce: 0.023161
2021-12-11 20:10:10,089 iteration 1742 : loss : 0.065720, loss_ce: 0.028138
2021-12-11 20:10:11,406 iteration 1743 : loss : 0.072489, loss_ce: 0.028359
2021-12-11 20:10:12,851 iteration 1744 : loss : 0.093226, loss_ce: 0.040586
2021-12-11 20:10:14,289 iteration 1745 : loss : 0.073552, loss_ce: 0.031693
2021-12-11 20:10:15,677 iteration 1746 : loss : 0.079762, loss_ce: 0.040470
2021-12-11 20:10:17,037 iteration 1747 : loss : 0.065823, loss_ce: 0.031303
2021-12-11 20:10:18,442 iteration 1748 : loss : 0.050220, loss_ce: 0.022544
2021-12-11 20:10:19,732 iteration 1749 : loss : 0.045500, loss_ce: 0.020202
2021-12-11 20:10:21,115 iteration 1750 : loss : 0.105003, loss_ce: 0.031035
2021-12-11 20:10:22,585 iteration 1751 : loss : 0.087275, loss_ce: 0.033289
 26%|███████▍                     | 103/400 [44:03<2:03:05, 24.87s/it]2021-12-11 20:10:24,096 iteration 1752 : loss : 0.101138, loss_ce: 0.030181
2021-12-11 20:10:25,386 iteration 1753 : loss : 0.054571, loss_ce: 0.024888
2021-12-11 20:10:26,816 iteration 1754 : loss : 0.093786, loss_ce: 0.037146
2021-12-11 20:10:28,167 iteration 1755 : loss : 0.063578, loss_ce: 0.032083
2021-12-11 20:10:29,562 iteration 1756 : loss : 0.083048, loss_ce: 0.035441
2021-12-11 20:10:30,894 iteration 1757 : loss : 0.044345, loss_ce: 0.023544
2021-12-11 20:10:32,257 iteration 1758 : loss : 0.067103, loss_ce: 0.039256
2021-12-11 20:10:33,597 iteration 1759 : loss : 0.055121, loss_ce: 0.021390
2021-12-11 20:10:34,943 iteration 1760 : loss : 0.059076, loss_ce: 0.025518
2021-12-11 20:10:36,364 iteration 1761 : loss : 0.099860, loss_ce: 0.032090
2021-12-11 20:10:37,670 iteration 1762 : loss : 0.055726, loss_ce: 0.027675
2021-12-11 20:10:38,999 iteration 1763 : loss : 0.069497, loss_ce: 0.030494
2021-12-11 20:10:40,284 iteration 1764 : loss : 0.049708, loss_ce: 0.021657
2021-12-11 20:10:41,674 iteration 1765 : loss : 0.057340, loss_ce: 0.030482
2021-12-11 20:10:43,099 iteration 1766 : loss : 0.068758, loss_ce: 0.037430
2021-12-11 20:10:44,456 iteration 1767 : loss : 0.057912, loss_ce: 0.029838
2021-12-11 20:10:45,845 iteration 1768 : loss : 0.083405, loss_ce: 0.033783
 26%|███████▌                     | 104/400 [44:27<2:00:17, 24.38s/it]2021-12-11 20:10:47,281 iteration 1769 : loss : 0.067839, loss_ce: 0.030048
2021-12-11 20:10:48,638 iteration 1770 : loss : 0.060656, loss_ce: 0.029210
2021-12-11 20:10:49,940 iteration 1771 : loss : 0.074154, loss_ce: 0.033128
2021-12-11 20:10:51,264 iteration 1772 : loss : 0.058616, loss_ce: 0.031331
2021-12-11 20:10:52,675 iteration 1773 : loss : 0.091368, loss_ce: 0.029275
2021-12-11 20:10:54,072 iteration 1774 : loss : 0.042862, loss_ce: 0.019506
2021-12-11 20:10:55,492 iteration 1775 : loss : 0.062903, loss_ce: 0.029277
2021-12-11 20:10:56,840 iteration 1776 : loss : 0.063370, loss_ce: 0.034130
2021-12-11 20:10:58,168 iteration 1777 : loss : 0.045541, loss_ce: 0.021777
2021-12-11 20:10:59,476 iteration 1778 : loss : 0.059922, loss_ce: 0.025755
2021-12-11 20:11:00,888 iteration 1779 : loss : 0.076274, loss_ce: 0.027041
2021-12-11 20:11:02,268 iteration 1780 : loss : 0.076092, loss_ce: 0.038338
2021-12-11 20:11:03,591 iteration 1781 : loss : 0.060499, loss_ce: 0.028410
2021-12-11 20:11:04,995 iteration 1782 : loss : 0.062321, loss_ce: 0.030589
2021-12-11 20:11:06,385 iteration 1783 : loss : 0.072011, loss_ce: 0.030523
2021-12-11 20:11:07,740 iteration 1784 : loss : 0.060830, loss_ce: 0.028686
2021-12-11 20:11:07,741 Training Data Eval:
2021-12-11 20:11:14,563   Average segmentation loss on training set: 0.0438
2021-12-11 20:11:14,563 Validation Data Eval:
2021-12-11 20:11:16,926   Average segmentation loss on validation set: 0.1216
2021-12-11 20:11:18,346 iteration 1785 : loss : 0.071674, loss_ce: 0.033359
 26%|███████▌                     | 105/400 [44:59<2:11:51, 26.82s/it]2021-12-11 20:11:19,797 iteration 1786 : loss : 0.056971, loss_ce: 0.029949
2021-12-11 20:11:21,110 iteration 1787 : loss : 0.082051, loss_ce: 0.033228
2021-12-11 20:11:22,539 iteration 1788 : loss : 0.066243, loss_ce: 0.036567
2021-12-11 20:11:23,951 iteration 1789 : loss : 0.063205, loss_ce: 0.027772
2021-12-11 20:11:25,347 iteration 1790 : loss : 0.061075, loss_ce: 0.032474
2021-12-11 20:11:26,787 iteration 1791 : loss : 0.059267, loss_ce: 0.028138
2021-12-11 20:11:28,121 iteration 1792 : loss : 0.049048, loss_ce: 0.022961
2021-12-11 20:11:29,474 iteration 1793 : loss : 0.103743, loss_ce: 0.036950
2021-12-11 20:11:30,842 iteration 1794 : loss : 0.062790, loss_ce: 0.027531
2021-12-11 20:11:32,257 iteration 1795 : loss : 0.063419, loss_ce: 0.028718
2021-12-11 20:11:33,629 iteration 1796 : loss : 0.066342, loss_ce: 0.032646
2021-12-11 20:11:35,088 iteration 1797 : loss : 0.071171, loss_ce: 0.026782
2021-12-11 20:11:36,535 iteration 1798 : loss : 0.070698, loss_ce: 0.030219
2021-12-11 20:11:37,888 iteration 1799 : loss : 0.075117, loss_ce: 0.031002
2021-12-11 20:11:39,290 iteration 1800 : loss : 0.048049, loss_ce: 0.023395
2021-12-11 20:11:40,625 iteration 1801 : loss : 0.050634, loss_ce: 0.023262
2021-12-11 20:11:41,981 iteration 1802 : loss : 0.075086, loss_ce: 0.028846
 26%|███████▋                     | 106/400 [45:23<2:06:43, 25.86s/it]2021-12-11 20:11:43,483 iteration 1803 : loss : 0.079382, loss_ce: 0.033667
2021-12-11 20:11:44,932 iteration 1804 : loss : 0.055271, loss_ce: 0.021229
2021-12-11 20:11:46,283 iteration 1805 : loss : 0.071821, loss_ce: 0.025812
2021-12-11 20:11:47,636 iteration 1806 : loss : 0.067532, loss_ce: 0.037782
2021-12-11 20:11:48,987 iteration 1807 : loss : 0.080435, loss_ce: 0.034188
2021-12-11 20:11:50,357 iteration 1808 : loss : 0.058767, loss_ce: 0.030791
2021-12-11 20:11:51,698 iteration 1809 : loss : 0.048718, loss_ce: 0.023397
2021-12-11 20:11:52,987 iteration 1810 : loss : 0.053432, loss_ce: 0.024667
2021-12-11 20:11:54,398 iteration 1811 : loss : 0.060595, loss_ce: 0.030262
2021-12-11 20:11:55,832 iteration 1812 : loss : 0.059821, loss_ce: 0.026510
2021-12-11 20:11:57,327 iteration 1813 : loss : 0.067505, loss_ce: 0.029879
2021-12-11 20:11:58,735 iteration 1814 : loss : 0.053689, loss_ce: 0.028985
2021-12-11 20:12:00,057 iteration 1815 : loss : 0.054571, loss_ce: 0.022513
2021-12-11 20:12:01,366 iteration 1816 : loss : 0.053076, loss_ce: 0.026290
2021-12-11 20:12:02,802 iteration 1817 : loss : 0.064092, loss_ce: 0.028527
2021-12-11 20:12:04,174 iteration 1818 : loss : 0.072312, loss_ce: 0.033347
2021-12-11 20:12:05,592 iteration 1819 : loss : 0.076373, loss_ce: 0.039421
 27%|███████▊                     | 107/400 [45:46<2:03:00, 25.19s/it]2021-12-11 20:12:07,099 iteration 1820 : loss : 0.048625, loss_ce: 0.024457
2021-12-11 20:12:08,458 iteration 1821 : loss : 0.048201, loss_ce: 0.024088
2021-12-11 20:12:09,851 iteration 1822 : loss : 0.074158, loss_ce: 0.034937
2021-12-11 20:12:11,157 iteration 1823 : loss : 0.054133, loss_ce: 0.022634
2021-12-11 20:12:12,426 iteration 1824 : loss : 0.044480, loss_ce: 0.022820
2021-12-11 20:12:13,829 iteration 1825 : loss : 0.070578, loss_ce: 0.032862
2021-12-11 20:12:15,120 iteration 1826 : loss : 0.057420, loss_ce: 0.031351
2021-12-11 20:12:16,580 iteration 1827 : loss : 0.166545, loss_ce: 0.054098
2021-12-11 20:12:17,943 iteration 1828 : loss : 0.055657, loss_ce: 0.026915
2021-12-11 20:12:19,295 iteration 1829 : loss : 0.089808, loss_ce: 0.036061
2021-12-11 20:12:20,648 iteration 1830 : loss : 0.060655, loss_ce: 0.031161
2021-12-11 20:12:22,092 iteration 1831 : loss : 0.068036, loss_ce: 0.038300
2021-12-11 20:12:23,460 iteration 1832 : loss : 0.077209, loss_ce: 0.028214
2021-12-11 20:12:24,832 iteration 1833 : loss : 0.076483, loss_ce: 0.037083
2021-12-11 20:12:26,291 iteration 1834 : loss : 0.075735, loss_ce: 0.036850
2021-12-11 20:12:27,652 iteration 1835 : loss : 0.075890, loss_ce: 0.036918
2021-12-11 20:12:29,031 iteration 1836 : loss : 0.067776, loss_ce: 0.027437
 27%|███████▊                     | 108/400 [46:10<2:00:01, 24.66s/it]2021-12-11 20:12:30,516 iteration 1837 : loss : 0.065689, loss_ce: 0.031170
2021-12-11 20:12:31,955 iteration 1838 : loss : 0.057778, loss_ce: 0.031249
2021-12-11 20:12:33,316 iteration 1839 : loss : 0.071498, loss_ce: 0.032272
2021-12-11 20:12:34,621 iteration 1840 : loss : 0.066551, loss_ce: 0.035195
2021-12-11 20:12:36,033 iteration 1841 : loss : 0.076170, loss_ce: 0.034310
2021-12-11 20:12:37,467 iteration 1842 : loss : 0.094346, loss_ce: 0.040994
2021-12-11 20:12:38,789 iteration 1843 : loss : 0.055295, loss_ce: 0.029711
2021-12-11 20:12:40,164 iteration 1844 : loss : 0.056488, loss_ce: 0.026028
2021-12-11 20:12:41,475 iteration 1845 : loss : 0.060768, loss_ce: 0.026957
2021-12-11 20:12:42,943 iteration 1846 : loss : 0.049145, loss_ce: 0.025415
2021-12-11 20:12:44,368 iteration 1847 : loss : 0.064382, loss_ce: 0.027596
2021-12-11 20:12:45,750 iteration 1848 : loss : 0.055581, loss_ce: 0.026873
2021-12-11 20:12:47,049 iteration 1849 : loss : 0.070205, loss_ce: 0.034952
2021-12-11 20:12:48,433 iteration 1850 : loss : 0.076641, loss_ce: 0.026229
2021-12-11 20:12:49,840 iteration 1851 : loss : 0.089926, loss_ce: 0.051396
2021-12-11 20:12:51,222 iteration 1852 : loss : 0.051322, loss_ce: 0.026678
2021-12-11 20:12:52,600 iteration 1853 : loss : 0.053133, loss_ce: 0.020069
 27%|███████▉                     | 109/400 [46:33<1:58:01, 24.34s/it]2021-12-11 20:12:54,018 iteration 1854 : loss : 0.057044, loss_ce: 0.024166
2021-12-11 20:12:55,382 iteration 1855 : loss : 0.057769, loss_ce: 0.023081
2021-12-11 20:12:56,766 iteration 1856 : loss : 0.065456, loss_ce: 0.022703
2021-12-11 20:12:58,147 iteration 1857 : loss : 0.069119, loss_ce: 0.029924
2021-12-11 20:12:59,510 iteration 1858 : loss : 0.079793, loss_ce: 0.039566
2021-12-11 20:13:00,833 iteration 1859 : loss : 0.055054, loss_ce: 0.032934
2021-12-11 20:13:02,317 iteration 1860 : loss : 0.119358, loss_ce: 0.057392
2021-12-11 20:13:03,725 iteration 1861 : loss : 0.054255, loss_ce: 0.024448
2021-12-11 20:13:05,095 iteration 1862 : loss : 0.070899, loss_ce: 0.035389
2021-12-11 20:13:06,502 iteration 1863 : loss : 0.074005, loss_ce: 0.034018
2021-12-11 20:13:07,892 iteration 1864 : loss : 0.075849, loss_ce: 0.031697
2021-12-11 20:13:09,350 iteration 1865 : loss : 0.078373, loss_ce: 0.038384
2021-12-11 20:13:10,785 iteration 1866 : loss : 0.057974, loss_ce: 0.030409
2021-12-11 20:13:12,188 iteration 1867 : loss : 0.105841, loss_ce: 0.034768
2021-12-11 20:13:13,546 iteration 1868 : loss : 0.061067, loss_ce: 0.033839
2021-12-11 20:13:14,930 iteration 1869 : loss : 0.095694, loss_ce: 0.036642
2021-12-11 20:13:14,930 Training Data Eval:
2021-12-11 20:13:21,758   Average segmentation loss on training set: 0.0524
2021-12-11 20:13:21,759 Validation Data Eval:
2021-12-11 20:13:24,125   Average segmentation loss on validation set: 0.0991
2021-12-11 20:13:25,438 iteration 1870 : loss : 0.067635, loss_ce: 0.026717
 28%|███████▉                     | 110/400 [47:06<2:09:56, 26.88s/it]2021-12-11 20:13:26,863 iteration 1871 : loss : 0.065427, loss_ce: 0.033043
2021-12-11 20:13:28,195 iteration 1872 : loss : 0.069093, loss_ce: 0.025892
2021-12-11 20:13:29,578 iteration 1873 : loss : 0.058080, loss_ce: 0.027256
2021-12-11 20:13:31,006 iteration 1874 : loss : 0.070305, loss_ce: 0.031245
2021-12-11 20:13:32,457 iteration 1875 : loss : 0.060217, loss_ce: 0.034723
2021-12-11 20:13:33,903 iteration 1876 : loss : 0.068872, loss_ce: 0.026735
2021-12-11 20:13:35,336 iteration 1877 : loss : 0.090486, loss_ce: 0.031033
2021-12-11 20:13:36,670 iteration 1878 : loss : 0.056349, loss_ce: 0.025880
2021-12-11 20:13:38,011 iteration 1879 : loss : 0.058302, loss_ce: 0.029423
2021-12-11 20:13:39,344 iteration 1880 : loss : 0.062004, loss_ce: 0.026650
2021-12-11 20:13:40,674 iteration 1881 : loss : 0.050189, loss_ce: 0.022078
2021-12-11 20:13:42,057 iteration 1882 : loss : 0.090167, loss_ce: 0.025282
2021-12-11 20:13:43,374 iteration 1883 : loss : 0.049932, loss_ce: 0.023478
2021-12-11 20:13:44,805 iteration 1884 : loss : 0.059518, loss_ce: 0.026967
2021-12-11 20:13:46,134 iteration 1885 : loss : 0.040714, loss_ce: 0.020050
2021-12-11 20:13:47,531 iteration 1886 : loss : 0.061313, loss_ce: 0.030716
2021-12-11 20:13:48,869 iteration 1887 : loss : 0.061403, loss_ce: 0.028501
 28%|████████                     | 111/400 [47:30<2:04:30, 25.85s/it]2021-12-11 20:13:50,335 iteration 1888 : loss : 0.065729, loss_ce: 0.026921
2021-12-11 20:13:51,778 iteration 1889 : loss : 0.071031, loss_ce: 0.035701
2021-12-11 20:13:53,090 iteration 1890 : loss : 0.056502, loss_ce: 0.025571
2021-12-11 20:13:54,471 iteration 1891 : loss : 0.050296, loss_ce: 0.030121
2021-12-11 20:13:55,845 iteration 1892 : loss : 0.069098, loss_ce: 0.027537
2021-12-11 20:13:57,268 iteration 1893 : loss : 0.084466, loss_ce: 0.036282
2021-12-11 20:13:58,644 iteration 1894 : loss : 0.075156, loss_ce: 0.031621
2021-12-11 20:14:00,052 iteration 1895 : loss : 0.062065, loss_ce: 0.025744
2021-12-11 20:14:01,443 iteration 1896 : loss : 0.088005, loss_ce: 0.043495
2021-12-11 20:14:02,822 iteration 1897 : loss : 0.108441, loss_ce: 0.039561
2021-12-11 20:14:04,282 iteration 1898 : loss : 0.055076, loss_ce: 0.026136
2021-12-11 20:14:05,589 iteration 1899 : loss : 0.047121, loss_ce: 0.022167
2021-12-11 20:14:07,043 iteration 1900 : loss : 0.074866, loss_ce: 0.036621
2021-12-11 20:14:08,509 iteration 1901 : loss : 0.070657, loss_ce: 0.029545
2021-12-11 20:14:09,878 iteration 1902 : loss : 0.066280, loss_ce: 0.024894
2021-12-11 20:14:11,298 iteration 1903 : loss : 0.061474, loss_ce: 0.032143
2021-12-11 20:14:12,738 iteration 1904 : loss : 0.102090, loss_ce: 0.036625
 28%|████████                     | 112/400 [47:54<2:01:14, 25.26s/it]2021-12-11 20:14:14,222 iteration 1905 : loss : 0.071404, loss_ce: 0.032687
2021-12-11 20:14:15,609 iteration 1906 : loss : 0.068377, loss_ce: 0.026952
2021-12-11 20:14:16,981 iteration 1907 : loss : 0.053521, loss_ce: 0.024651
2021-12-11 20:14:18,472 iteration 1908 : loss : 0.087520, loss_ce: 0.034788
2021-12-11 20:14:19,874 iteration 1909 : loss : 0.072141, loss_ce: 0.032703
2021-12-11 20:14:21,282 iteration 1910 : loss : 0.096687, loss_ce: 0.027503
2021-12-11 20:14:22,718 iteration 1911 : loss : 0.056853, loss_ce: 0.024345
2021-12-11 20:14:24,024 iteration 1912 : loss : 0.058826, loss_ce: 0.024230
2021-12-11 20:14:25,408 iteration 1913 : loss : 0.062864, loss_ce: 0.034766
2021-12-11 20:14:26,795 iteration 1914 : loss : 0.058149, loss_ce: 0.029444
2021-12-11 20:14:28,196 iteration 1915 : loss : 0.076885, loss_ce: 0.036609
2021-12-11 20:14:29,583 iteration 1916 : loss : 0.074641, loss_ce: 0.032332
2021-12-11 20:14:30,969 iteration 1917 : loss : 0.056775, loss_ce: 0.032208
2021-12-11 20:14:32,377 iteration 1918 : loss : 0.065438, loss_ce: 0.029900
2021-12-11 20:14:33,725 iteration 1919 : loss : 0.058600, loss_ce: 0.027041
2021-12-11 20:14:35,080 iteration 1920 : loss : 0.071896, loss_ce: 0.033274
2021-12-11 20:14:36,532 iteration 1921 : loss : 0.089383, loss_ce: 0.045583
 28%|████████▏                    | 113/400 [48:17<1:58:42, 24.82s/it]2021-12-11 20:14:37,952 iteration 1922 : loss : 0.060899, loss_ce: 0.036165
2021-12-11 20:14:39,335 iteration 1923 : loss : 0.050980, loss_ce: 0.026304
2021-12-11 20:14:40,773 iteration 1924 : loss : 0.052091, loss_ce: 0.028214
2021-12-11 20:14:42,171 iteration 1925 : loss : 0.070670, loss_ce: 0.030335
2021-12-11 20:14:43,541 iteration 1926 : loss : 0.054614, loss_ce: 0.025644
2021-12-11 20:14:44,896 iteration 1927 : loss : 0.053294, loss_ce: 0.023785
2021-12-11 20:14:46,354 iteration 1928 : loss : 0.060707, loss_ce: 0.029769
2021-12-11 20:14:47,698 iteration 1929 : loss : 0.050015, loss_ce: 0.020995
2021-12-11 20:14:49,038 iteration 1930 : loss : 0.057315, loss_ce: 0.030064
2021-12-11 20:14:50,442 iteration 1931 : loss : 0.107800, loss_ce: 0.031379
2021-12-11 20:14:51,878 iteration 1932 : loss : 0.065720, loss_ce: 0.028166
2021-12-11 20:14:53,320 iteration 1933 : loss : 0.066302, loss_ce: 0.033487
2021-12-11 20:14:54,610 iteration 1934 : loss : 0.036033, loss_ce: 0.019903
2021-12-11 20:14:55,994 iteration 1935 : loss : 0.077334, loss_ce: 0.030885
2021-12-11 20:14:57,443 iteration 1936 : loss : 0.072601, loss_ce: 0.036952
2021-12-11 20:14:58,767 iteration 1937 : loss : 0.075167, loss_ce: 0.026705
2021-12-11 20:15:00,069 iteration 1938 : loss : 0.069628, loss_ce: 0.027028
 28%|████████▎                    | 114/400 [48:41<1:56:27, 24.43s/it]2021-12-11 20:15:01,479 iteration 1939 : loss : 0.038465, loss_ce: 0.018319
2021-12-11 20:15:02,958 iteration 1940 : loss : 0.053325, loss_ce: 0.028882
2021-12-11 20:15:04,441 iteration 1941 : loss : 0.063672, loss_ce: 0.027495
2021-12-11 20:15:05,852 iteration 1942 : loss : 0.082879, loss_ce: 0.033814
2021-12-11 20:15:07,213 iteration 1943 : loss : 0.067951, loss_ce: 0.030920
2021-12-11 20:15:08,597 iteration 1944 : loss : 0.044067, loss_ce: 0.022038
2021-12-11 20:15:09,946 iteration 1945 : loss : 0.067951, loss_ce: 0.026103
2021-12-11 20:15:11,336 iteration 1946 : loss : 0.048329, loss_ce: 0.023605
2021-12-11 20:15:12,748 iteration 1947 : loss : 0.077160, loss_ce: 0.037146
2021-12-11 20:15:14,136 iteration 1948 : loss : 0.090576, loss_ce: 0.030458
2021-12-11 20:15:15,492 iteration 1949 : loss : 0.044773, loss_ce: 0.021380
2021-12-11 20:15:16,808 iteration 1950 : loss : 0.034825, loss_ce: 0.019589
2021-12-11 20:15:18,242 iteration 1951 : loss : 0.080737, loss_ce: 0.031536
2021-12-11 20:15:19,662 iteration 1952 : loss : 0.071578, loss_ce: 0.026749
2021-12-11 20:15:21,106 iteration 1953 : loss : 0.072785, loss_ce: 0.038927
2021-12-11 20:15:22,497 iteration 1954 : loss : 0.037807, loss_ce: 0.021552
2021-12-11 20:15:22,497 Training Data Eval:
2021-12-11 20:15:29,301   Average segmentation loss on training set: 0.0387
2021-12-11 20:15:29,301 Validation Data Eval:
2021-12-11 20:15:31,648   Average segmentation loss on validation set: 0.1175
2021-12-11 20:15:33,054 iteration 1955 : loss : 0.060700, loss_ce: 0.024015
 29%|████████▎                    | 115/400 [49:14<2:08:14, 27.00s/it]2021-12-11 20:15:34,457 iteration 1956 : loss : 0.084845, loss_ce: 0.025600
2021-12-11 20:15:35,922 iteration 1957 : loss : 0.067233, loss_ce: 0.032586
2021-12-11 20:15:37,393 iteration 1958 : loss : 0.052523, loss_ce: 0.023604
2021-12-11 20:15:38,790 iteration 1959 : loss : 0.062847, loss_ce: 0.027061
2021-12-11 20:15:40,110 iteration 1960 : loss : 0.050217, loss_ce: 0.028548
2021-12-11 20:15:41,512 iteration 1961 : loss : 0.054669, loss_ce: 0.026397
2021-12-11 20:15:42,925 iteration 1962 : loss : 0.055143, loss_ce: 0.027706
2021-12-11 20:15:44,258 iteration 1963 : loss : 0.054080, loss_ce: 0.025123
2021-12-11 20:15:45,591 iteration 1964 : loss : 0.059995, loss_ce: 0.028075
2021-12-11 20:15:46,935 iteration 1965 : loss : 0.057413, loss_ce: 0.027598
2021-12-11 20:15:48,375 iteration 1966 : loss : 0.054979, loss_ce: 0.024178
2021-12-11 20:15:49,824 iteration 1967 : loss : 0.066910, loss_ce: 0.026607
2021-12-11 20:15:51,260 iteration 1968 : loss : 0.067238, loss_ce: 0.037262
2021-12-11 20:15:52,677 iteration 1969 : loss : 0.067683, loss_ce: 0.028930
2021-12-11 20:15:54,052 iteration 1970 : loss : 0.060718, loss_ce: 0.024497
2021-12-11 20:15:55,480 iteration 1971 : loss : 0.064730, loss_ce: 0.033099
2021-12-11 20:15:56,841 iteration 1972 : loss : 0.074357, loss_ce: 0.027846
 29%|████████▍                    | 116/400 [49:38<2:03:13, 26.03s/it]2021-12-11 20:15:58,165 iteration 1973 : loss : 0.050294, loss_ce: 0.023593
2021-12-11 20:15:59,568 iteration 1974 : loss : 0.060872, loss_ce: 0.028479
2021-12-11 20:16:00,947 iteration 1975 : loss : 0.057699, loss_ce: 0.023742
2021-12-11 20:16:02,390 iteration 1976 : loss : 0.055645, loss_ce: 0.028692
2021-12-11 20:16:03,779 iteration 1977 : loss : 0.065661, loss_ce: 0.024907
2021-12-11 20:16:05,085 iteration 1978 : loss : 0.043819, loss_ce: 0.022983
2021-12-11 20:16:06,391 iteration 1979 : loss : 0.054823, loss_ce: 0.027982
2021-12-11 20:16:07,790 iteration 1980 : loss : 0.076460, loss_ce: 0.040147
2021-12-11 20:16:09,124 iteration 1981 : loss : 0.070308, loss_ce: 0.025205
2021-12-11 20:16:10,406 iteration 1982 : loss : 0.073360, loss_ce: 0.030895
2021-12-11 20:16:11,840 iteration 1983 : loss : 0.071619, loss_ce: 0.039965
2021-12-11 20:16:13,155 iteration 1984 : loss : 0.063677, loss_ce: 0.022399
2021-12-11 20:16:14,540 iteration 1985 : loss : 0.040799, loss_ce: 0.020557
2021-12-11 20:16:15,932 iteration 1986 : loss : 0.074943, loss_ce: 0.038583
2021-12-11 20:16:17,372 iteration 1987 : loss : 0.045352, loss_ce: 0.022466
2021-12-11 20:16:18,698 iteration 1988 : loss : 0.065560, loss_ce: 0.024867
2021-12-11 20:16:20,058 iteration 1989 : loss : 0.062328, loss_ce: 0.031677
 29%|████████▍                    | 117/400 [50:01<1:58:48, 25.19s/it]2021-12-11 20:16:21,405 iteration 1990 : loss : 0.038336, loss_ce: 0.021396
2021-12-11 20:16:22,733 iteration 1991 : loss : 0.036681, loss_ce: 0.018114
2021-12-11 20:16:24,139 iteration 1992 : loss : 0.054374, loss_ce: 0.023725
2021-12-11 20:16:25,643 iteration 1993 : loss : 0.062642, loss_ce: 0.031872
2021-12-11 20:16:26,984 iteration 1994 : loss : 0.048070, loss_ce: 0.023371
2021-12-11 20:16:28,355 iteration 1995 : loss : 0.078051, loss_ce: 0.026955
2021-12-11 20:16:29,749 iteration 1996 : loss : 0.093263, loss_ce: 0.034863
2021-12-11 20:16:31,121 iteration 1997 : loss : 0.037493, loss_ce: 0.017961
2021-12-11 20:16:32,505 iteration 1998 : loss : 0.065100, loss_ce: 0.028865
2021-12-11 20:16:33,870 iteration 1999 : loss : 0.058475, loss_ce: 0.028069
2021-12-11 20:16:35,312 iteration 2000 : loss : 0.064896, loss_ce: 0.023531
2021-12-11 20:16:36,675 iteration 2001 : loss : 0.062239, loss_ce: 0.030630
2021-12-11 20:16:38,110 iteration 2002 : loss : 0.060950, loss_ce: 0.037004
2021-12-11 20:16:39,454 iteration 2003 : loss : 0.057221, loss_ce: 0.029522
2021-12-11 20:16:40,861 iteration 2004 : loss : 0.082873, loss_ce: 0.030594
2021-12-11 20:16:42,287 iteration 2005 : loss : 0.076416, loss_ce: 0.032526
2021-12-11 20:16:43,708 iteration 2006 : loss : 0.064712, loss_ce: 0.027618
 30%|████████▌                    | 118/400 [50:25<1:56:13, 24.73s/it]2021-12-11 20:16:45,176 iteration 2007 : loss : 0.054502, loss_ce: 0.027727
2021-12-11 20:16:46,511 iteration 2008 : loss : 0.061750, loss_ce: 0.028938
2021-12-11 20:16:47,871 iteration 2009 : loss : 0.075949, loss_ce: 0.029634
2021-12-11 20:16:49,193 iteration 2010 : loss : 0.051607, loss_ce: 0.024970
2021-12-11 20:16:50,557 iteration 2011 : loss : 0.065577, loss_ce: 0.032548
2021-12-11 20:16:51,946 iteration 2012 : loss : 0.077131, loss_ce: 0.028922
2021-12-11 20:16:53,374 iteration 2013 : loss : 0.049096, loss_ce: 0.021920
2021-12-11 20:16:54,789 iteration 2014 : loss : 0.074283, loss_ce: 0.028216
2021-12-11 20:16:56,108 iteration 2015 : loss : 0.064176, loss_ce: 0.026315
2021-12-11 20:16:57,482 iteration 2016 : loss : 0.062475, loss_ce: 0.023239
2021-12-11 20:16:58,778 iteration 2017 : loss : 0.048763, loss_ce: 0.022591
2021-12-11 20:17:00,157 iteration 2018 : loss : 0.069143, loss_ce: 0.034210
2021-12-11 20:17:01,486 iteration 2019 : loss : 0.052424, loss_ce: 0.026521
2021-12-11 20:17:02,867 iteration 2020 : loss : 0.061339, loss_ce: 0.029166
2021-12-11 20:17:04,275 iteration 2021 : loss : 0.056451, loss_ce: 0.025835
2021-12-11 20:17:05,717 iteration 2022 : loss : 0.066224, loss_ce: 0.035131
2021-12-11 20:17:07,085 iteration 2023 : loss : 0.084500, loss_ce: 0.027356
 30%|████████▋                    | 119/400 [50:48<1:53:54, 24.32s/it]2021-12-11 20:17:08,557 iteration 2024 : loss : 0.066126, loss_ce: 0.030998
2021-12-11 20:17:09,888 iteration 2025 : loss : 0.039810, loss_ce: 0.018994
2021-12-11 20:17:11,231 iteration 2026 : loss : 0.058716, loss_ce: 0.027744
2021-12-11 20:17:12,641 iteration 2027 : loss : 0.055180, loss_ce: 0.024716
2021-12-11 20:17:14,043 iteration 2028 : loss : 0.066636, loss_ce: 0.028944
2021-12-11 20:17:15,362 iteration 2029 : loss : 0.058851, loss_ce: 0.031635
2021-12-11 20:17:16,748 iteration 2030 : loss : 0.049793, loss_ce: 0.022314
2021-12-11 20:17:18,153 iteration 2031 : loss : 0.053885, loss_ce: 0.023934
2021-12-11 20:17:19,564 iteration 2032 : loss : 0.061110, loss_ce: 0.026888
2021-12-11 20:17:20,932 iteration 2033 : loss : 0.067659, loss_ce: 0.024320
2021-12-11 20:17:22,238 iteration 2034 : loss : 0.047034, loss_ce: 0.020355
2021-12-11 20:17:23,552 iteration 2035 : loss : 0.045075, loss_ce: 0.026154
2021-12-11 20:17:24,968 iteration 2036 : loss : 0.051859, loss_ce: 0.025457
2021-12-11 20:17:26,345 iteration 2037 : loss : 0.058068, loss_ce: 0.024388
2021-12-11 20:17:27,686 iteration 2038 : loss : 0.056871, loss_ce: 0.024256
2021-12-11 20:17:29,080 iteration 2039 : loss : 0.069430, loss_ce: 0.032762
2021-12-11 20:17:29,080 Training Data Eval:
2021-12-11 20:17:35,903   Average segmentation loss on training set: 0.0343
2021-12-11 20:17:35,904 Validation Data Eval:
2021-12-11 20:17:38,267   Average segmentation loss on validation set: 0.0917
2021-12-11 20:17:40,479 Found new lowest validation loss at iteration 2039! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 20:17:41,845 iteration 2040 : loss : 0.059951, loss_ce: 0.023026
 30%|████████▋                    | 120/400 [51:23<2:08:06, 27.45s/it]2021-12-11 20:17:43,260 iteration 2041 : loss : 0.054824, loss_ce: 0.027535
2021-12-11 20:17:44,618 iteration 2042 : loss : 0.042093, loss_ce: 0.021979
2021-12-11 20:17:45,991 iteration 2043 : loss : 0.050631, loss_ce: 0.030094
2021-12-11 20:17:47,414 iteration 2044 : loss : 0.047808, loss_ce: 0.023140
2021-12-11 20:17:48,873 iteration 2045 : loss : 0.053713, loss_ce: 0.024093
2021-12-11 20:17:50,281 iteration 2046 : loss : 0.095762, loss_ce: 0.045980
2021-12-11 20:17:51,673 iteration 2047 : loss : 0.077130, loss_ce: 0.033072
2021-12-11 20:17:52,991 iteration 2048 : loss : 0.048914, loss_ce: 0.021812
2021-12-11 20:17:54,351 iteration 2049 : loss : 0.052301, loss_ce: 0.022888
2021-12-11 20:17:55,759 iteration 2050 : loss : 0.061421, loss_ce: 0.023691
2021-12-11 20:17:57,170 iteration 2051 : loss : 0.070941, loss_ce: 0.024498
2021-12-11 20:17:58,514 iteration 2052 : loss : 0.047150, loss_ce: 0.026390
2021-12-11 20:17:59,849 iteration 2053 : loss : 0.052040, loss_ce: 0.022019
2021-12-11 20:18:01,242 iteration 2054 : loss : 0.064266, loss_ce: 0.025671
2021-12-11 20:18:02,644 iteration 2055 : loss : 0.063563, loss_ce: 0.034483
2021-12-11 20:18:04,039 iteration 2056 : loss : 0.061156, loss_ce: 0.029609
2021-12-11 20:18:05,454 iteration 2057 : loss : 0.042108, loss_ce: 0.022468
 30%|████████▊                    | 121/400 [51:46<2:02:18, 26.30s/it]2021-12-11 20:18:06,973 iteration 2058 : loss : 0.052680, loss_ce: 0.029149
2021-12-11 20:18:08,382 iteration 2059 : loss : 0.052385, loss_ce: 0.023597
2021-12-11 20:18:09,815 iteration 2060 : loss : 0.060775, loss_ce: 0.023402
2021-12-11 20:18:11,164 iteration 2061 : loss : 0.048468, loss_ce: 0.026763
2021-12-11 20:18:12,482 iteration 2062 : loss : 0.048953, loss_ce: 0.021451
2021-12-11 20:18:13,812 iteration 2063 : loss : 0.055996, loss_ce: 0.028631
2021-12-11 20:18:15,196 iteration 2064 : loss : 0.045524, loss_ce: 0.024755
2021-12-11 20:18:16,588 iteration 2065 : loss : 0.063638, loss_ce: 0.024082
2021-12-11 20:18:18,014 iteration 2066 : loss : 0.075574, loss_ce: 0.028773
2021-12-11 20:18:19,373 iteration 2067 : loss : 0.052642, loss_ce: 0.021815
2021-12-11 20:18:20,809 iteration 2068 : loss : 0.048899, loss_ce: 0.027930
2021-12-11 20:18:22,257 iteration 2069 : loss : 0.071594, loss_ce: 0.026276
2021-12-11 20:18:23,616 iteration 2070 : loss : 0.043682, loss_ce: 0.020395
2021-12-11 20:18:24,985 iteration 2071 : loss : 0.051934, loss_ce: 0.025870
2021-12-11 20:18:26,306 iteration 2072 : loss : 0.043452, loss_ce: 0.022872
2021-12-11 20:18:27,617 iteration 2073 : loss : 0.066078, loss_ce: 0.026234
2021-12-11 20:18:28,992 iteration 2074 : loss : 0.085797, loss_ce: 0.026548
 30%|████████▊                    | 122/400 [52:10<1:58:01, 25.47s/it]2021-12-11 20:18:30,435 iteration 2075 : loss : 0.048145, loss_ce: 0.021878
2021-12-11 20:18:31,867 iteration 2076 : loss : 0.081626, loss_ce: 0.033543
2021-12-11 20:18:33,263 iteration 2077 : loss : 0.050530, loss_ce: 0.024897
2021-12-11 20:18:34,597 iteration 2078 : loss : 0.054800, loss_ce: 0.025587
2021-12-11 20:18:35,950 iteration 2079 : loss : 0.053344, loss_ce: 0.024035
2021-12-11 20:18:37,380 iteration 2080 : loss : 0.046847, loss_ce: 0.021441
2021-12-11 20:18:38,780 iteration 2081 : loss : 0.057050, loss_ce: 0.028075
2021-12-11 20:18:40,168 iteration 2082 : loss : 0.068881, loss_ce: 0.027167
2021-12-11 20:18:41,538 iteration 2083 : loss : 0.046406, loss_ce: 0.022515
2021-12-11 20:18:42,913 iteration 2084 : loss : 0.062859, loss_ce: 0.025496
2021-12-11 20:18:44,256 iteration 2085 : loss : 0.056373, loss_ce: 0.030981
2021-12-11 20:18:45,623 iteration 2086 : loss : 0.052366, loss_ce: 0.028937
2021-12-11 20:18:46,954 iteration 2087 : loss : 0.052441, loss_ce: 0.020885
2021-12-11 20:18:48,445 iteration 2088 : loss : 0.072458, loss_ce: 0.037356
2021-12-11 20:18:49,806 iteration 2089 : loss : 0.048046, loss_ce: 0.021329
2021-12-11 20:18:51,226 iteration 2090 : loss : 0.067541, loss_ce: 0.027826
2021-12-11 20:18:52,598 iteration 2091 : loss : 0.051226, loss_ce: 0.024777
 31%|████████▉                    | 123/400 [52:33<1:54:59, 24.91s/it]2021-12-11 20:18:54,011 iteration 2092 : loss : 0.053551, loss_ce: 0.026414
2021-12-11 20:18:55,479 iteration 2093 : loss : 0.053404, loss_ce: 0.026219
2021-12-11 20:18:56,892 iteration 2094 : loss : 0.050561, loss_ce: 0.024888
2021-12-11 20:18:58,313 iteration 2095 : loss : 0.058186, loss_ce: 0.028255
2021-12-11 20:18:59,748 iteration 2096 : loss : 0.060167, loss_ce: 0.025327
2021-12-11 20:19:01,093 iteration 2097 : loss : 0.045998, loss_ce: 0.026003
2021-12-11 20:19:02,535 iteration 2098 : loss : 0.074809, loss_ce: 0.035447
2021-12-11 20:19:03,975 iteration 2099 : loss : 0.074266, loss_ce: 0.026927
2021-12-11 20:19:05,458 iteration 2100 : loss : 0.090029, loss_ce: 0.052048
2021-12-11 20:19:06,862 iteration 2101 : loss : 0.066100, loss_ce: 0.024029
2021-12-11 20:19:08,220 iteration 2102 : loss : 0.056107, loss_ce: 0.026459
2021-12-11 20:19:09,714 iteration 2103 : loss : 0.070120, loss_ce: 0.031536
2021-12-11 20:19:11,150 iteration 2104 : loss : 0.047107, loss_ce: 0.021667
2021-12-11 20:19:12,516 iteration 2105 : loss : 0.062466, loss_ce: 0.023436
2021-12-11 20:19:13,990 iteration 2106 : loss : 0.063562, loss_ce: 0.023279
2021-12-11 20:19:15,389 iteration 2107 : loss : 0.049894, loss_ce: 0.023695
2021-12-11 20:19:16,797 iteration 2108 : loss : 0.052051, loss_ce: 0.024574
 31%|████████▉                    | 124/400 [52:58<1:53:36, 24.70s/it]2021-12-11 20:19:18,240 iteration 2109 : loss : 0.064933, loss_ce: 0.028585
2021-12-11 20:19:19,615 iteration 2110 : loss : 0.047922, loss_ce: 0.020644
2021-12-11 20:19:21,112 iteration 2111 : loss : 0.062583, loss_ce: 0.029074
2021-12-11 20:19:22,627 iteration 2112 : loss : 0.055149, loss_ce: 0.025950
2021-12-11 20:19:23,985 iteration 2113 : loss : 0.065720, loss_ce: 0.030135
2021-12-11 20:19:25,352 iteration 2114 : loss : 0.063949, loss_ce: 0.028912
2021-12-11 20:19:26,626 iteration 2115 : loss : 0.075464, loss_ce: 0.026483
2021-12-11 20:19:28,040 iteration 2116 : loss : 0.045534, loss_ce: 0.022798
2021-12-11 20:19:29,487 iteration 2117 : loss : 0.066073, loss_ce: 0.025192
2021-12-11 20:19:30,869 iteration 2118 : loss : 0.062167, loss_ce: 0.031270
2021-12-11 20:19:32,280 iteration 2119 : loss : 0.060086, loss_ce: 0.022947
2021-12-11 20:19:33,669 iteration 2120 : loss : 0.065194, loss_ce: 0.028144
2021-12-11 20:19:35,148 iteration 2121 : loss : 0.057711, loss_ce: 0.026056
2021-12-11 20:19:36,484 iteration 2122 : loss : 0.057175, loss_ce: 0.028119
2021-12-11 20:19:37,832 iteration 2123 : loss : 0.057048, loss_ce: 0.033888
2021-12-11 20:19:39,254 iteration 2124 : loss : 0.051331, loss_ce: 0.023722
2021-12-11 20:19:39,254 Training Data Eval:
2021-12-11 20:19:46,072   Average segmentation loss on training set: 0.0326
2021-12-11 20:19:46,072 Validation Data Eval:
2021-12-11 20:19:48,447   Average segmentation loss on validation set: 0.0869
2021-12-11 20:19:50,374 Found new lowest validation loss at iteration 2124! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 20:19:51,789 iteration 2125 : loss : 0.041053, loss_ce: 0.018018
 31%|█████████                    | 125/400 [53:33<2:07:21, 27.79s/it]2021-12-11 20:19:53,227 iteration 2126 : loss : 0.035251, loss_ce: 0.018262
2021-12-11 20:19:54,673 iteration 2127 : loss : 0.043228, loss_ce: 0.020872
2021-12-11 20:19:56,004 iteration 2128 : loss : 0.058612, loss_ce: 0.031282
2021-12-11 20:19:57,401 iteration 2129 : loss : 0.046163, loss_ce: 0.021517
2021-12-11 20:19:58,730 iteration 2130 : loss : 0.063700, loss_ce: 0.024829
2021-12-11 20:20:00,185 iteration 2131 : loss : 0.098623, loss_ce: 0.045235
2021-12-11 20:20:01,649 iteration 2132 : loss : 0.064469, loss_ce: 0.030057
2021-12-11 20:20:03,083 iteration 2133 : loss : 0.066795, loss_ce: 0.029306
2021-12-11 20:20:04,512 iteration 2134 : loss : 0.082717, loss_ce: 0.030856
2021-12-11 20:20:05,923 iteration 2135 : loss : 0.046993, loss_ce: 0.020607
2021-12-11 20:20:07,306 iteration 2136 : loss : 0.055825, loss_ce: 0.026854
2021-12-11 20:20:08,690 iteration 2137 : loss : 0.078029, loss_ce: 0.025501
2021-12-11 20:20:10,045 iteration 2138 : loss : 0.078554, loss_ce: 0.027722
2021-12-11 20:20:11,350 iteration 2139 : loss : 0.053097, loss_ce: 0.025827
2021-12-11 20:20:12,690 iteration 2140 : loss : 0.046204, loss_ce: 0.020119
2021-12-11 20:20:14,071 iteration 2141 : loss : 0.060165, loss_ce: 0.028103
2021-12-11 20:20:15,456 iteration 2142 : loss : 0.093947, loss_ce: 0.048133
 32%|█████████▏                   | 126/400 [53:56<2:01:15, 26.55s/it]2021-12-11 20:20:16,923 iteration 2143 : loss : 0.064062, loss_ce: 0.022689
2021-12-11 20:20:18,294 iteration 2144 : loss : 0.062701, loss_ce: 0.021634
2021-12-11 20:20:19,591 iteration 2145 : loss : 0.045425, loss_ce: 0.026076
2021-12-11 20:20:21,102 iteration 2146 : loss : 0.070159, loss_ce: 0.027812
2021-12-11 20:20:22,512 iteration 2147 : loss : 0.049679, loss_ce: 0.023657
2021-12-11 20:20:23,898 iteration 2148 : loss : 0.063161, loss_ce: 0.027160
2021-12-11 20:20:25,308 iteration 2149 : loss : 0.074827, loss_ce: 0.029346
2021-12-11 20:20:26,744 iteration 2150 : loss : 0.056997, loss_ce: 0.022218
2021-12-11 20:20:28,184 iteration 2151 : loss : 0.060724, loss_ce: 0.025588
2021-12-11 20:20:29,515 iteration 2152 : loss : 0.039498, loss_ce: 0.019438
2021-12-11 20:20:30,873 iteration 2153 : loss : 0.047886, loss_ce: 0.019002
2021-12-11 20:20:32,189 iteration 2154 : loss : 0.042316, loss_ce: 0.023129
2021-12-11 20:20:33,534 iteration 2155 : loss : 0.051940, loss_ce: 0.024433
2021-12-11 20:20:34,952 iteration 2156 : loss : 0.059932, loss_ce: 0.026023
2021-12-11 20:20:36,365 iteration 2157 : loss : 0.058402, loss_ce: 0.028965
2021-12-11 20:20:37,710 iteration 2158 : loss : 0.047314, loss_ce: 0.022988
2021-12-11 20:20:39,143 iteration 2159 : loss : 0.083483, loss_ce: 0.030123
 32%|█████████▏                   | 127/400 [54:20<1:56:54, 25.69s/it]2021-12-11 20:20:40,519 iteration 2160 : loss : 0.035975, loss_ce: 0.017380
2021-12-11 20:20:41,915 iteration 2161 : loss : 0.054253, loss_ce: 0.025225
2021-12-11 20:20:43,299 iteration 2162 : loss : 0.046357, loss_ce: 0.023869
2021-12-11 20:20:44,593 iteration 2163 : loss : 0.055015, loss_ce: 0.020040
2021-12-11 20:20:45,991 iteration 2164 : loss : 0.052208, loss_ce: 0.025121
2021-12-11 20:20:47,377 iteration 2165 : loss : 0.045228, loss_ce: 0.019433
2021-12-11 20:20:48,809 iteration 2166 : loss : 0.076362, loss_ce: 0.038119
2021-12-11 20:20:50,190 iteration 2167 : loss : 0.058230, loss_ce: 0.031350
2021-12-11 20:20:51,574 iteration 2168 : loss : 0.043038, loss_ce: 0.019973
2021-12-11 20:20:52,968 iteration 2169 : loss : 0.049426, loss_ce: 0.023122
2021-12-11 20:20:54,324 iteration 2170 : loss : 0.056398, loss_ce: 0.022951
2021-12-11 20:20:55,688 iteration 2171 : loss : 0.033947, loss_ce: 0.016572
2021-12-11 20:20:56,973 iteration 2172 : loss : 0.052268, loss_ce: 0.029644
2021-12-11 20:20:58,347 iteration 2173 : loss : 0.059892, loss_ce: 0.023995
2021-12-11 20:20:59,763 iteration 2174 : loss : 0.055979, loss_ce: 0.026511
2021-12-11 20:21:01,094 iteration 2175 : loss : 0.052669, loss_ce: 0.021850
2021-12-11 20:21:02,446 iteration 2176 : loss : 0.048324, loss_ce: 0.027148
 32%|█████████▎                   | 128/400 [54:43<1:53:12, 24.97s/it]2021-12-11 20:21:03,942 iteration 2177 : loss : 0.054676, loss_ce: 0.024295
2021-12-11 20:21:05,268 iteration 2178 : loss : 0.049502, loss_ce: 0.025622
2021-12-11 20:21:06,641 iteration 2179 : loss : 0.045054, loss_ce: 0.022236
2021-12-11 20:21:08,061 iteration 2180 : loss : 0.052039, loss_ce: 0.022392
2021-12-11 20:21:09,423 iteration 2181 : loss : 0.054817, loss_ce: 0.022897
2021-12-11 20:21:10,783 iteration 2182 : loss : 0.051481, loss_ce: 0.022388
2021-12-11 20:21:12,120 iteration 2183 : loss : 0.051727, loss_ce: 0.021236
2021-12-11 20:21:13,438 iteration 2184 : loss : 0.052755, loss_ce: 0.026723
2021-12-11 20:21:14,731 iteration 2185 : loss : 0.051372, loss_ce: 0.024132
2021-12-11 20:21:16,209 iteration 2186 : loss : 0.086800, loss_ce: 0.041802
2021-12-11 20:21:17,508 iteration 2187 : loss : 0.043842, loss_ce: 0.025474
2021-12-11 20:21:18,939 iteration 2188 : loss : 0.044670, loss_ce: 0.020933
2021-12-11 20:21:20,277 iteration 2189 : loss : 0.051670, loss_ce: 0.024302
2021-12-11 20:21:21,697 iteration 2190 : loss : 0.051321, loss_ce: 0.023033
2021-12-11 20:21:23,004 iteration 2191 : loss : 0.039944, loss_ce: 0.020937
2021-12-11 20:21:24,376 iteration 2192 : loss : 0.048608, loss_ce: 0.021592
2021-12-11 20:21:25,813 iteration 2193 : loss : 0.054754, loss_ce: 0.026148
 32%|█████████▎                   | 129/400 [55:07<1:50:37, 24.49s/it]2021-12-11 20:21:27,213 iteration 2194 : loss : 0.063410, loss_ce: 0.028097
2021-12-11 20:21:28,555 iteration 2195 : loss : 0.058682, loss_ce: 0.023745
2021-12-11 20:21:29,949 iteration 2196 : loss : 0.053773, loss_ce: 0.031728
2021-12-11 20:21:31,359 iteration 2197 : loss : 0.072209, loss_ce: 0.027857
2021-12-11 20:21:32,711 iteration 2198 : loss : 0.042772, loss_ce: 0.023886
2021-12-11 20:21:34,231 iteration 2199 : loss : 0.052022, loss_ce: 0.027436
2021-12-11 20:21:35,606 iteration 2200 : loss : 0.050929, loss_ce: 0.023985
2021-12-11 20:21:36,988 iteration 2201 : loss : 0.067989, loss_ce: 0.025455
2021-12-11 20:21:38,361 iteration 2202 : loss : 0.081582, loss_ce: 0.027741
2021-12-11 20:21:39,776 iteration 2203 : loss : 0.036388, loss_ce: 0.016991
2021-12-11 20:21:41,198 iteration 2204 : loss : 0.053701, loss_ce: 0.023563
2021-12-11 20:21:42,482 iteration 2205 : loss : 0.048269, loss_ce: 0.022962
2021-12-11 20:21:43,788 iteration 2206 : loss : 0.039188, loss_ce: 0.022059
2021-12-11 20:21:45,158 iteration 2207 : loss : 0.069453, loss_ce: 0.031087
2021-12-11 20:21:46,522 iteration 2208 : loss : 0.045111, loss_ce: 0.022160
2021-12-11 20:21:47,962 iteration 2209 : loss : 0.046185, loss_ce: 0.019180
2021-12-11 20:21:47,962 Training Data Eval:
2021-12-11 20:21:54,781   Average segmentation loss on training set: 0.0346
2021-12-11 20:21:54,781 Validation Data Eval:
2021-12-11 20:21:57,134   Average segmentation loss on validation set: 0.1029
2021-12-11 20:21:58,523 iteration 2210 : loss : 0.052846, loss_ce: 0.025031
 32%|█████████▍                   | 130/400 [55:39<2:01:18, 26.96s/it]2021-12-11 20:21:59,969 iteration 2211 : loss : 0.049578, loss_ce: 0.027594
2021-12-11 20:22:01,267 iteration 2212 : loss : 0.039172, loss_ce: 0.018391
2021-12-11 20:22:02,584 iteration 2213 : loss : 0.040834, loss_ce: 0.019466
2021-12-11 20:22:03,971 iteration 2214 : loss : 0.048729, loss_ce: 0.024548
2021-12-11 20:22:05,334 iteration 2215 : loss : 0.070668, loss_ce: 0.027050
2021-12-11 20:22:06,670 iteration 2216 : loss : 0.063577, loss_ce: 0.031518
2021-12-11 20:22:07,970 iteration 2217 : loss : 0.062779, loss_ce: 0.032505
2021-12-11 20:22:09,355 iteration 2218 : loss : 0.074866, loss_ce: 0.033889
2021-12-11 20:22:10,706 iteration 2219 : loss : 0.054360, loss_ce: 0.024167
2021-12-11 20:22:12,094 iteration 2220 : loss : 0.041553, loss_ce: 0.023422
2021-12-11 20:22:13,539 iteration 2221 : loss : 0.057563, loss_ce: 0.029689
2021-12-11 20:22:14,886 iteration 2222 : loss : 0.082467, loss_ce: 0.029851
2021-12-11 20:22:16,369 iteration 2223 : loss : 0.090279, loss_ce: 0.039228
2021-12-11 20:22:17,719 iteration 2224 : loss : 0.081126, loss_ce: 0.026043
2021-12-11 20:22:19,084 iteration 2225 : loss : 0.051339, loss_ce: 0.025672
2021-12-11 20:22:20,580 iteration 2226 : loss : 0.057921, loss_ce: 0.024788
2021-12-11 20:22:22,016 iteration 2227 : loss : 0.066754, loss_ce: 0.028139
 33%|█████████▍                   | 131/400 [56:03<1:56:12, 25.92s/it]2021-12-11 20:22:23,489 iteration 2228 : loss : 0.093522, loss_ce: 0.037643
2021-12-11 20:22:24,884 iteration 2229 : loss : 0.078721, loss_ce: 0.039567
2021-12-11 20:22:26,240 iteration 2230 : loss : 0.079486, loss_ce: 0.036281
2021-12-11 20:22:27,566 iteration 2231 : loss : 0.042148, loss_ce: 0.023954
2021-12-11 20:22:28,892 iteration 2232 : loss : 0.042784, loss_ce: 0.019912
2021-12-11 20:22:30,217 iteration 2233 : loss : 0.067429, loss_ce: 0.026601
2021-12-11 20:22:31,620 iteration 2234 : loss : 0.064328, loss_ce: 0.026709
2021-12-11 20:22:32,942 iteration 2235 : loss : 0.060961, loss_ce: 0.031159
2021-12-11 20:22:34,274 iteration 2236 : loss : 0.046824, loss_ce: 0.023109
2021-12-11 20:22:35,646 iteration 2237 : loss : 0.055886, loss_ce: 0.026824
2021-12-11 20:22:36,991 iteration 2238 : loss : 0.091536, loss_ce: 0.032781
2021-12-11 20:22:38,393 iteration 2239 : loss : 0.058577, loss_ce: 0.025799
2021-12-11 20:22:39,698 iteration 2240 : loss : 0.053266, loss_ce: 0.023883
2021-12-11 20:22:41,098 iteration 2241 : loss : 0.044792, loss_ce: 0.022706
2021-12-11 20:22:42,418 iteration 2242 : loss : 0.062688, loss_ce: 0.033547
2021-12-11 20:22:43,744 iteration 2243 : loss : 0.041021, loss_ce: 0.016911
2021-12-11 20:22:45,101 iteration 2244 : loss : 0.040377, loss_ce: 0.019114
 33%|█████████▌                   | 132/400 [56:26<1:51:57, 25.07s/it]2021-12-11 20:22:46,495 iteration 2245 : loss : 0.038596, loss_ce: 0.019257
2021-12-11 20:22:47,893 iteration 2246 : loss : 0.051455, loss_ce: 0.023457
2021-12-11 20:22:49,270 iteration 2247 : loss : 0.070666, loss_ce: 0.033192
2021-12-11 20:22:50,554 iteration 2248 : loss : 0.030813, loss_ce: 0.016869
2021-12-11 20:22:51,983 iteration 2249 : loss : 0.056598, loss_ce: 0.027948
2021-12-11 20:22:53,476 iteration 2250 : loss : 0.061111, loss_ce: 0.026911
2021-12-11 20:22:54,787 iteration 2251 : loss : 0.049011, loss_ce: 0.023991
2021-12-11 20:22:56,186 iteration 2252 : loss : 0.046461, loss_ce: 0.019896
2021-12-11 20:22:57,515 iteration 2253 : loss : 0.069708, loss_ce: 0.026417
2021-12-11 20:22:58,951 iteration 2254 : loss : 0.059725, loss_ce: 0.031103
2021-12-11 20:23:00,375 iteration 2255 : loss : 0.040990, loss_ce: 0.022515
2021-12-11 20:23:01,842 iteration 2256 : loss : 0.075334, loss_ce: 0.032293
2021-12-11 20:23:03,208 iteration 2257 : loss : 0.063127, loss_ce: 0.024747
2021-12-11 20:23:04,579 iteration 2258 : loss : 0.055081, loss_ce: 0.023903
2021-12-11 20:23:05,988 iteration 2259 : loss : 0.059976, loss_ce: 0.025018
2021-12-11 20:23:07,376 iteration 2260 : loss : 0.061789, loss_ce: 0.026880
2021-12-11 20:23:08,708 iteration 2261 : loss : 0.053295, loss_ce: 0.018863
 33%|█████████▋                   | 133/400 [56:50<1:49:36, 24.63s/it]2021-12-11 20:23:10,259 iteration 2262 : loss : 0.052003, loss_ce: 0.023623
2021-12-11 20:23:11,657 iteration 2263 : loss : 0.060393, loss_ce: 0.029995
2021-12-11 20:23:13,062 iteration 2264 : loss : 0.062168, loss_ce: 0.023774
2021-12-11 20:23:14,478 iteration 2265 : loss : 0.051006, loss_ce: 0.022890
2021-12-11 20:23:15,858 iteration 2266 : loss : 0.042080, loss_ce: 0.020427
2021-12-11 20:23:17,243 iteration 2267 : loss : 0.059719, loss_ce: 0.023552
2021-12-11 20:23:18,550 iteration 2268 : loss : 0.047013, loss_ce: 0.023727
2021-12-11 20:23:19,941 iteration 2269 : loss : 0.065929, loss_ce: 0.030434
2021-12-11 20:23:21,275 iteration 2270 : loss : 0.052392, loss_ce: 0.023587
2021-12-11 20:23:22,614 iteration 2271 : loss : 0.078285, loss_ce: 0.048978
2021-12-11 20:23:23,940 iteration 2272 : loss : 0.042105, loss_ce: 0.016371
2021-12-11 20:23:25,363 iteration 2273 : loss : 0.052773, loss_ce: 0.021144
2021-12-11 20:23:26,686 iteration 2274 : loss : 0.050546, loss_ce: 0.020834
2021-12-11 20:23:28,056 iteration 2275 : loss : 0.046742, loss_ce: 0.024406
2021-12-11 20:23:29,441 iteration 2276 : loss : 0.069083, loss_ce: 0.024770
2021-12-11 20:23:30,789 iteration 2277 : loss : 0.054552, loss_ce: 0.026892
2021-12-11 20:23:32,158 iteration 2278 : loss : 0.044549, loss_ce: 0.024508
 34%|█████████▋                   | 134/400 [57:13<1:47:37, 24.28s/it]2021-12-11 20:23:33,675 iteration 2279 : loss : 0.069596, loss_ce: 0.034510
2021-12-11 20:23:35,086 iteration 2280 : loss : 0.045509, loss_ce: 0.022418
2021-12-11 20:23:36,391 iteration 2281 : loss : 0.049939, loss_ce: 0.023289
2021-12-11 20:23:37,873 iteration 2282 : loss : 0.064277, loss_ce: 0.026723
2021-12-11 20:23:39,208 iteration 2283 : loss : 0.040878, loss_ce: 0.018935
2021-12-11 20:23:40,671 iteration 2284 : loss : 0.086097, loss_ce: 0.035446
2021-12-11 20:23:42,011 iteration 2285 : loss : 0.044348, loss_ce: 0.020438
2021-12-11 20:23:43,369 iteration 2286 : loss : 0.037635, loss_ce: 0.017331
2021-12-11 20:23:44,784 iteration 2287 : loss : 0.049192, loss_ce: 0.020642
2021-12-11 20:23:46,093 iteration 2288 : loss : 0.053958, loss_ce: 0.024106
2021-12-11 20:23:47,479 iteration 2289 : loss : 0.042052, loss_ce: 0.019415
2021-12-11 20:23:48,864 iteration 2290 : loss : 0.042901, loss_ce: 0.021652
2021-12-11 20:23:50,222 iteration 2291 : loss : 0.045393, loss_ce: 0.020856
2021-12-11 20:23:51,652 iteration 2292 : loss : 0.053192, loss_ce: 0.021831
2021-12-11 20:23:52,961 iteration 2293 : loss : 0.076317, loss_ce: 0.035810
2021-12-11 20:23:54,313 iteration 2294 : loss : 0.045328, loss_ce: 0.023357
2021-12-11 20:23:54,313 Training Data Eval:
2021-12-11 20:24:01,110   Average segmentation loss on training set: 0.0313
2021-12-11 20:24:01,111 Validation Data Eval:
2021-12-11 20:24:03,471   Average segmentation loss on validation set: 0.1006
2021-12-11 20:24:04,849 iteration 2295 : loss : 0.046471, loss_ce: 0.021205
 34%|█████████▊                   | 135/400 [57:46<1:58:21, 26.80s/it]2021-12-11 20:24:06,278 iteration 2296 : loss : 0.051981, loss_ce: 0.025569
2021-12-11 20:24:07,656 iteration 2297 : loss : 0.058327, loss_ce: 0.021352
2021-12-11 20:24:09,152 iteration 2298 : loss : 0.067961, loss_ce: 0.027826
2021-12-11 20:24:10,515 iteration 2299 : loss : 0.068450, loss_ce: 0.022628
2021-12-11 20:24:11,972 iteration 2300 : loss : 0.061568, loss_ce: 0.025733
2021-12-11 20:24:13,346 iteration 2301 : loss : 0.049485, loss_ce: 0.025881
2021-12-11 20:24:14,703 iteration 2302 : loss : 0.072136, loss_ce: 0.038544
2021-12-11 20:24:16,042 iteration 2303 : loss : 0.061843, loss_ce: 0.023369
2021-12-11 20:24:17,337 iteration 2304 : loss : 0.082579, loss_ce: 0.037947
2021-12-11 20:24:18,744 iteration 2305 : loss : 0.057583, loss_ce: 0.023063
2021-12-11 20:24:20,073 iteration 2306 : loss : 0.053228, loss_ce: 0.021269
2021-12-11 20:24:21,383 iteration 2307 : loss : 0.038677, loss_ce: 0.017451
2021-12-11 20:24:22,724 iteration 2308 : loss : 0.050241, loss_ce: 0.028462
2021-12-11 20:24:24,112 iteration 2309 : loss : 0.050352, loss_ce: 0.023979
2021-12-11 20:24:25,509 iteration 2310 : loss : 0.060361, loss_ce: 0.030646
2021-12-11 20:24:26,967 iteration 2311 : loss : 0.060459, loss_ce: 0.026975
2021-12-11 20:24:28,354 iteration 2312 : loss : 0.051811, loss_ce: 0.025670
 34%|█████████▊                   | 136/400 [58:09<1:53:34, 25.81s/it]2021-12-11 20:24:29,784 iteration 2313 : loss : 0.045283, loss_ce: 0.026340
2021-12-11 20:24:31,116 iteration 2314 : loss : 0.056322, loss_ce: 0.027172
2021-12-11 20:24:32,490 iteration 2315 : loss : 0.041897, loss_ce: 0.019124
2021-12-11 20:24:33,896 iteration 2316 : loss : 0.054945, loss_ce: 0.029041
2021-12-11 20:24:35,199 iteration 2317 : loss : 0.048194, loss_ce: 0.022997
2021-12-11 20:24:36,642 iteration 2318 : loss : 0.052161, loss_ce: 0.026250
2021-12-11 20:24:38,098 iteration 2319 : loss : 0.061291, loss_ce: 0.022873
2021-12-11 20:24:39,420 iteration 2320 : loss : 0.049853, loss_ce: 0.026924
2021-12-11 20:24:40,792 iteration 2321 : loss : 0.053763, loss_ce: 0.023997
2021-12-11 20:24:42,176 iteration 2322 : loss : 0.055002, loss_ce: 0.025142
2021-12-11 20:24:43,615 iteration 2323 : loss : 0.062519, loss_ce: 0.025326
2021-12-11 20:24:45,017 iteration 2324 : loss : 0.040987, loss_ce: 0.017288
2021-12-11 20:24:46,393 iteration 2325 : loss : 0.048098, loss_ce: 0.026510
2021-12-11 20:24:47,811 iteration 2326 : loss : 0.056788, loss_ce: 0.035355
2021-12-11 20:24:49,173 iteration 2327 : loss : 0.049684, loss_ce: 0.019780
2021-12-11 20:24:50,639 iteration 2328 : loss : 0.057432, loss_ce: 0.023585
2021-12-11 20:24:52,003 iteration 2329 : loss : 0.105165, loss_ce: 0.040960
 34%|█████████▉                   | 137/400 [58:33<1:50:17, 25.16s/it]2021-12-11 20:24:53,409 iteration 2330 : loss : 0.049276, loss_ce: 0.026296
2021-12-11 20:24:54,741 iteration 2331 : loss : 0.056565, loss_ce: 0.021445
2021-12-11 20:24:56,196 iteration 2332 : loss : 0.042256, loss_ce: 0.023220
2021-12-11 20:24:57,513 iteration 2333 : loss : 0.045236, loss_ce: 0.026677
2021-12-11 20:24:58,878 iteration 2334 : loss : 0.045646, loss_ce: 0.016337
2021-12-11 20:25:00,361 iteration 2335 : loss : 0.060440, loss_ce: 0.030197
2021-12-11 20:25:01,703 iteration 2336 : loss : 0.052653, loss_ce: 0.027847
2021-12-11 20:25:03,080 iteration 2337 : loss : 0.055650, loss_ce: 0.021801
2021-12-11 20:25:04,487 iteration 2338 : loss : 0.048596, loss_ce: 0.021495
2021-12-11 20:25:05,859 iteration 2339 : loss : 0.086869, loss_ce: 0.041431
2021-12-11 20:25:07,270 iteration 2340 : loss : 0.056404, loss_ce: 0.028321
2021-12-11 20:25:08,654 iteration 2341 : loss : 0.049701, loss_ce: 0.023262
2021-12-11 20:25:10,060 iteration 2342 : loss : 0.036037, loss_ce: 0.016137
2021-12-11 20:25:11,444 iteration 2343 : loss : 0.043369, loss_ce: 0.020187
2021-12-11 20:25:12,747 iteration 2344 : loss : 0.044116, loss_ce: 0.019603
2021-12-11 20:25:14,080 iteration 2345 : loss : 0.040187, loss_ce: 0.019581
2021-12-11 20:25:15,368 iteration 2346 : loss : 0.061048, loss_ce: 0.024910
 34%|██████████                   | 138/400 [58:56<1:47:31, 24.62s/it]2021-12-11 20:25:16,843 iteration 2347 : loss : 0.067696, loss_ce: 0.031832
2021-12-11 20:25:18,193 iteration 2348 : loss : 0.049701, loss_ce: 0.022248
2021-12-11 20:25:19,582 iteration 2349 : loss : 0.059835, loss_ce: 0.034140
2021-12-11 20:25:20,882 iteration 2350 : loss : 0.046447, loss_ce: 0.025123
2021-12-11 20:25:22,273 iteration 2351 : loss : 0.086786, loss_ce: 0.034459
2021-12-11 20:25:23,739 iteration 2352 : loss : 0.048171, loss_ce: 0.022927
2021-12-11 20:25:25,037 iteration 2353 : loss : 0.038800, loss_ce: 0.020960
2021-12-11 20:25:26,414 iteration 2354 : loss : 0.052725, loss_ce: 0.021623
2021-12-11 20:25:27,739 iteration 2355 : loss : 0.046864, loss_ce: 0.025800
2021-12-11 20:25:29,207 iteration 2356 : loss : 0.063440, loss_ce: 0.027348
2021-12-11 20:25:30,644 iteration 2357 : loss : 0.057959, loss_ce: 0.027096
2021-12-11 20:25:32,112 iteration 2358 : loss : 0.068685, loss_ce: 0.027469
2021-12-11 20:25:33,546 iteration 2359 : loss : 0.068944, loss_ce: 0.022583
2021-12-11 20:25:34,838 iteration 2360 : loss : 0.090472, loss_ce: 0.024683
2021-12-11 20:25:36,155 iteration 2361 : loss : 0.047329, loss_ce: 0.020898
2021-12-11 20:25:37,526 iteration 2362 : loss : 0.054047, loss_ce: 0.025157
2021-12-11 20:25:38,898 iteration 2363 : loss : 0.047822, loss_ce: 0.022129
 35%|██████████                   | 139/400 [59:20<1:45:41, 24.30s/it]2021-12-11 20:25:40,322 iteration 2364 : loss : 0.079320, loss_ce: 0.038146
2021-12-11 20:25:41,643 iteration 2365 : loss : 0.048425, loss_ce: 0.020758
2021-12-11 20:25:43,029 iteration 2366 : loss : 0.042055, loss_ce: 0.021893
2021-12-11 20:25:44,375 iteration 2367 : loss : 0.078630, loss_ce: 0.028211
2021-12-11 20:25:45,830 iteration 2368 : loss : 0.049483, loss_ce: 0.018899
2021-12-11 20:25:47,193 iteration 2369 : loss : 0.075531, loss_ce: 0.026732
2021-12-11 20:25:48,527 iteration 2370 : loss : 0.041054, loss_ce: 0.018115
2021-12-11 20:25:49,909 iteration 2371 : loss : 0.047010, loss_ce: 0.022539
2021-12-11 20:25:51,197 iteration 2372 : loss : 0.039867, loss_ce: 0.022486
2021-12-11 20:25:52,575 iteration 2373 : loss : 0.050863, loss_ce: 0.022093
2021-12-11 20:25:53,993 iteration 2374 : loss : 0.041610, loss_ce: 0.019910
2021-12-11 20:25:55,316 iteration 2375 : loss : 0.048173, loss_ce: 0.018895
2021-12-11 20:25:56,739 iteration 2376 : loss : 0.052420, loss_ce: 0.025552
2021-12-11 20:25:58,111 iteration 2377 : loss : 0.043906, loss_ce: 0.018045
2021-12-11 20:25:59,593 iteration 2378 : loss : 0.062380, loss_ce: 0.035962
2021-12-11 20:26:01,003 iteration 2379 : loss : 0.063836, loss_ce: 0.024061
2021-12-11 20:26:01,003 Training Data Eval:
2021-12-11 20:26:07,811   Average segmentation loss on training set: 0.0276
2021-12-11 20:26:07,811 Validation Data Eval:
2021-12-11 20:26:10,166   Average segmentation loss on validation set: 0.0878
2021-12-11 20:26:11,663 iteration 2380 : loss : 0.048751, loss_ce: 0.021751
 35%|██████████▏                  | 140/400 [59:52<1:56:16, 26.83s/it]2021-12-11 20:26:13,060 iteration 2381 : loss : 0.044844, loss_ce: 0.018948
2021-12-11 20:26:14,492 iteration 2382 : loss : 0.055384, loss_ce: 0.029531
2021-12-11 20:26:15,916 iteration 2383 : loss : 0.072772, loss_ce: 0.038285
2021-12-11 20:26:17,296 iteration 2384 : loss : 0.051141, loss_ce: 0.020367
2021-12-11 20:26:18,681 iteration 2385 : loss : 0.056513, loss_ce: 0.030139
2021-12-11 20:26:20,140 iteration 2386 : loss : 0.058623, loss_ce: 0.021070
2021-12-11 20:26:21,487 iteration 2387 : loss : 0.064902, loss_ce: 0.025718
2021-12-11 20:26:22,887 iteration 2388 : loss : 0.054704, loss_ce: 0.026598
2021-12-11 20:26:24,217 iteration 2389 : loss : 0.047061, loss_ce: 0.022315
2021-12-11 20:26:25,596 iteration 2390 : loss : 0.035627, loss_ce: 0.018471
2021-12-11 20:26:26,930 iteration 2391 : loss : 0.042259, loss_ce: 0.019607
2021-12-11 20:26:28,253 iteration 2392 : loss : 0.042278, loss_ce: 0.019795
2021-12-11 20:26:29,580 iteration 2393 : loss : 0.056899, loss_ce: 0.029679
2021-12-11 20:26:30,968 iteration 2394 : loss : 0.080906, loss_ce: 0.024392
2021-12-11 20:26:32,469 iteration 2395 : loss : 0.054506, loss_ce: 0.021861
2021-12-11 20:26:33,925 iteration 2396 : loss : 0.038043, loss_ce: 0.020166
2021-12-11 20:26:35,310 iteration 2397 : loss : 0.045717, loss_ce: 0.018951
 35%|█████████▌                 | 141/400 [1:00:16<1:51:43, 25.88s/it]2021-12-11 20:26:36,857 iteration 2398 : loss : 0.062907, loss_ce: 0.036760
2021-12-11 20:26:38,168 iteration 2399 : loss : 0.041535, loss_ce: 0.019820
2021-12-11 20:26:39,531 iteration 2400 : loss : 0.056785, loss_ce: 0.022366
2021-12-11 20:26:40,906 iteration 2401 : loss : 0.057521, loss_ce: 0.027615
2021-12-11 20:26:42,268 iteration 2402 : loss : 0.043727, loss_ce: 0.021756
2021-12-11 20:26:43,643 iteration 2403 : loss : 0.039082, loss_ce: 0.017640
2021-12-11 20:26:44,987 iteration 2404 : loss : 0.049065, loss_ce: 0.026434
2021-12-11 20:26:46,357 iteration 2405 : loss : 0.065479, loss_ce: 0.028036
2021-12-11 20:26:47,791 iteration 2406 : loss : 0.046967, loss_ce: 0.022997
2021-12-11 20:26:49,117 iteration 2407 : loss : 0.037034, loss_ce: 0.018521
2021-12-11 20:26:50,491 iteration 2408 : loss : 0.065030, loss_ce: 0.029927
2021-12-11 20:26:51,871 iteration 2409 : loss : 0.043367, loss_ce: 0.019421
2021-12-11 20:26:53,280 iteration 2410 : loss : 0.056904, loss_ce: 0.019189
2021-12-11 20:26:54,603 iteration 2411 : loss : 0.036249, loss_ce: 0.015855
2021-12-11 20:26:56,026 iteration 2412 : loss : 0.052170, loss_ce: 0.022608
2021-12-11 20:26:57,487 iteration 2413 : loss : 0.058699, loss_ce: 0.030963
2021-12-11 20:26:58,911 iteration 2414 : loss : 0.054312, loss_ce: 0.027144
 36%|█████████▌                 | 142/400 [1:00:40<1:48:20, 25.19s/it]2021-12-11 20:27:00,361 iteration 2415 : loss : 0.060695, loss_ce: 0.021579
2021-12-11 20:27:01,737 iteration 2416 : loss : 0.047451, loss_ce: 0.023801
2021-12-11 20:27:03,126 iteration 2417 : loss : 0.052051, loss_ce: 0.020544
2021-12-11 20:27:04,535 iteration 2418 : loss : 0.048487, loss_ce: 0.018845
2021-12-11 20:27:05,915 iteration 2419 : loss : 0.048511, loss_ce: 0.027896
2021-12-11 20:27:07,244 iteration 2420 : loss : 0.043395, loss_ce: 0.021402
2021-12-11 20:27:08,728 iteration 2421 : loss : 0.056083, loss_ce: 0.021906
2021-12-11 20:27:10,141 iteration 2422 : loss : 0.053364, loss_ce: 0.021999
2021-12-11 20:27:11,543 iteration 2423 : loss : 0.053195, loss_ce: 0.028602
2021-12-11 20:27:12,869 iteration 2424 : loss : 0.048800, loss_ce: 0.021256
2021-12-11 20:27:14,319 iteration 2425 : loss : 0.052303, loss_ce: 0.027922
2021-12-11 20:27:15,695 iteration 2426 : loss : 0.042283, loss_ce: 0.019845
2021-12-11 20:27:17,175 iteration 2427 : loss : 0.077123, loss_ce: 0.026391
2021-12-11 20:27:18,637 iteration 2428 : loss : 0.065183, loss_ce: 0.029083
2021-12-11 20:27:20,110 iteration 2429 : loss : 0.052330, loss_ce: 0.025458
2021-12-11 20:27:21,509 iteration 2430 : loss : 0.045987, loss_ce: 0.024216
2021-12-11 20:27:22,860 iteration 2431 : loss : 0.048591, loss_ce: 0.023798
 36%|█████████▋                 | 143/400 [1:01:04<1:46:19, 24.82s/it]2021-12-11 20:27:24,342 iteration 2432 : loss : 0.047065, loss_ce: 0.024195
2021-12-11 20:27:25,746 iteration 2433 : loss : 0.081629, loss_ce: 0.035500
2021-12-11 20:27:27,132 iteration 2434 : loss : 0.045103, loss_ce: 0.018942
2021-12-11 20:27:28,626 iteration 2435 : loss : 0.049783, loss_ce: 0.028573
2021-12-11 20:27:29,926 iteration 2436 : loss : 0.052721, loss_ce: 0.023268
2021-12-11 20:27:31,247 iteration 2437 : loss : 0.034421, loss_ce: 0.016561
2021-12-11 20:27:32,756 iteration 2438 : loss : 0.046769, loss_ce: 0.021655
2021-12-11 20:27:34,158 iteration 2439 : loss : 0.059067, loss_ce: 0.027704
2021-12-11 20:27:35,524 iteration 2440 : loss : 0.051527, loss_ce: 0.021408
2021-12-11 20:27:36,884 iteration 2441 : loss : 0.050069, loss_ce: 0.019967
2021-12-11 20:27:38,220 iteration 2442 : loss : 0.054644, loss_ce: 0.023227
2021-12-11 20:27:39,538 iteration 2443 : loss : 0.051234, loss_ce: 0.025615
2021-12-11 20:27:40,943 iteration 2444 : loss : 0.048339, loss_ce: 0.020856
2021-12-11 20:27:42,229 iteration 2445 : loss : 0.076195, loss_ce: 0.019308
2021-12-11 20:27:43,701 iteration 2446 : loss : 0.040826, loss_ce: 0.018785
2021-12-11 20:27:45,154 iteration 2447 : loss : 0.068680, loss_ce: 0.035020
2021-12-11 20:27:46,552 iteration 2448 : loss : 0.039118, loss_ce: 0.018750
 36%|█████████▋                 | 144/400 [1:01:27<1:44:27, 24.48s/it]2021-12-11 20:27:47,968 iteration 2449 : loss : 0.045527, loss_ce: 0.022327
2021-12-11 20:27:49,416 iteration 2450 : loss : 0.062405, loss_ce: 0.023828
2021-12-11 20:27:50,863 iteration 2451 : loss : 0.050180, loss_ce: 0.021715
2021-12-11 20:27:52,172 iteration 2452 : loss : 0.037779, loss_ce: 0.020319
2021-12-11 20:27:53,543 iteration 2453 : loss : 0.050578, loss_ce: 0.017616
2021-12-11 20:27:54,956 iteration 2454 : loss : 0.045966, loss_ce: 0.025050
2021-12-11 20:27:56,377 iteration 2455 : loss : 0.053644, loss_ce: 0.029698
2021-12-11 20:27:57,884 iteration 2456 : loss : 0.093177, loss_ce: 0.036094
2021-12-11 20:27:59,267 iteration 2457 : loss : 0.054686, loss_ce: 0.022436
2021-12-11 20:28:00,733 iteration 2458 : loss : 0.044644, loss_ce: 0.018298
2021-12-11 20:28:02,075 iteration 2459 : loss : 0.050258, loss_ce: 0.019795
2021-12-11 20:28:03,459 iteration 2460 : loss : 0.061518, loss_ce: 0.023180
2021-12-11 20:28:04,863 iteration 2461 : loss : 0.069508, loss_ce: 0.027281
2021-12-11 20:28:06,227 iteration 2462 : loss : 0.040830, loss_ce: 0.021511
2021-12-11 20:28:07,631 iteration 2463 : loss : 0.049379, loss_ce: 0.020282
2021-12-11 20:28:09,088 iteration 2464 : loss : 0.044418, loss_ce: 0.021825
2021-12-11 20:28:09,088 Training Data Eval:
2021-12-11 20:28:15,897   Average segmentation loss on training set: 0.0288
2021-12-11 20:28:15,897 Validation Data Eval:
2021-12-11 20:28:18,252   Average segmentation loss on validation set: 0.0922
2021-12-11 20:28:19,596 iteration 2465 : loss : 0.048522, loss_ce: 0.020476
 36%|█████████▊                 | 145/400 [1:02:00<1:54:57, 27.05s/it]2021-12-11 20:28:21,002 iteration 2466 : loss : 0.037729, loss_ce: 0.017815
2021-12-11 20:28:22,426 iteration 2467 : loss : 0.054533, loss_ce: 0.034397
2021-12-11 20:28:23,761 iteration 2468 : loss : 0.035327, loss_ce: 0.018138
2021-12-11 20:28:25,064 iteration 2469 : loss : 0.029653, loss_ce: 0.017694
2021-12-11 20:28:26,484 iteration 2470 : loss : 0.059935, loss_ce: 0.025874
2021-12-11 20:28:27,914 iteration 2471 : loss : 0.047535, loss_ce: 0.022229
2021-12-11 20:28:29,392 iteration 2472 : loss : 0.046674, loss_ce: 0.023271
2021-12-11 20:28:30,699 iteration 2473 : loss : 0.040331, loss_ce: 0.018975
2021-12-11 20:28:32,115 iteration 2474 : loss : 0.052885, loss_ce: 0.023827
2021-12-11 20:28:33,442 iteration 2475 : loss : 0.034657, loss_ce: 0.019030
2021-12-11 20:28:34,893 iteration 2476 : loss : 0.056704, loss_ce: 0.019309
2021-12-11 20:28:36,262 iteration 2477 : loss : 0.048248, loss_ce: 0.023219
2021-12-11 20:28:37,626 iteration 2478 : loss : 0.044597, loss_ce: 0.017026
2021-12-11 20:28:39,119 iteration 2479 : loss : 0.047462, loss_ce: 0.017797
2021-12-11 20:28:40,572 iteration 2480 : loss : 0.055558, loss_ce: 0.028064
2021-12-11 20:28:41,932 iteration 2481 : loss : 0.040556, loss_ce: 0.019453
2021-12-11 20:28:43,311 iteration 2482 : loss : 0.044291, loss_ce: 0.020529
 36%|█████████▊                 | 146/400 [1:02:24<1:50:17, 26.05s/it]2021-12-11 20:28:44,761 iteration 2483 : loss : 0.039156, loss_ce: 0.020174
2021-12-11 20:28:46,194 iteration 2484 : loss : 0.043479, loss_ce: 0.018784
2021-12-11 20:28:47,667 iteration 2485 : loss : 0.048261, loss_ce: 0.022465
2021-12-11 20:28:49,003 iteration 2486 : loss : 0.047760, loss_ce: 0.026241
2021-12-11 20:28:50,365 iteration 2487 : loss : 0.044580, loss_ce: 0.020101
2021-12-11 20:28:51,796 iteration 2488 : loss : 0.051832, loss_ce: 0.020713
2021-12-11 20:28:53,232 iteration 2489 : loss : 0.052701, loss_ce: 0.024707
2021-12-11 20:28:54,592 iteration 2490 : loss : 0.045099, loss_ce: 0.021489
2021-12-11 20:28:55,984 iteration 2491 : loss : 0.047852, loss_ce: 0.022383
2021-12-11 20:28:57,331 iteration 2492 : loss : 0.049603, loss_ce: 0.022296
2021-12-11 20:28:58,684 iteration 2493 : loss : 0.044098, loss_ce: 0.018078
2021-12-11 20:29:00,011 iteration 2494 : loss : 0.041178, loss_ce: 0.021147
2021-12-11 20:29:01,378 iteration 2495 : loss : 0.048421, loss_ce: 0.019390
2021-12-11 20:29:02,802 iteration 2496 : loss : 0.068889, loss_ce: 0.026127
2021-12-11 20:29:04,177 iteration 2497 : loss : 0.047723, loss_ce: 0.023180
2021-12-11 20:29:05,489 iteration 2498 : loss : 0.048740, loss_ce: 0.025068
2021-12-11 20:29:06,935 iteration 2499 : loss : 0.054857, loss_ce: 0.029867
 37%|█████████▉                 | 147/400 [1:02:48<1:46:45, 25.32s/it]2021-12-11 20:29:08,402 iteration 2500 : loss : 0.044554, loss_ce: 0.021300
2021-12-11 20:29:09,734 iteration 2501 : loss : 0.038200, loss_ce: 0.017226
2021-12-11 20:29:11,095 iteration 2502 : loss : 0.063001, loss_ce: 0.024552
2021-12-11 20:29:12,486 iteration 2503 : loss : 0.059055, loss_ce: 0.029579
2021-12-11 20:29:13,897 iteration 2504 : loss : 0.051673, loss_ce: 0.021810
2021-12-11 20:29:15,227 iteration 2505 : loss : 0.033934, loss_ce: 0.019001
2021-12-11 20:29:16,576 iteration 2506 : loss : 0.062929, loss_ce: 0.019393
2021-12-11 20:29:17,990 iteration 2507 : loss : 0.039028, loss_ce: 0.018140
2021-12-11 20:29:19,353 iteration 2508 : loss : 0.043019, loss_ce: 0.023138
2021-12-11 20:29:20,790 iteration 2509 : loss : 0.049955, loss_ce: 0.024823
2021-12-11 20:29:22,153 iteration 2510 : loss : 0.048789, loss_ce: 0.019904
2021-12-11 20:29:23,475 iteration 2511 : loss : 0.041433, loss_ce: 0.021683
2021-12-11 20:29:24,900 iteration 2512 : loss : 0.055509, loss_ce: 0.023040
2021-12-11 20:29:26,253 iteration 2513 : loss : 0.034239, loss_ce: 0.017646
2021-12-11 20:29:27,672 iteration 2514 : loss : 0.050595, loss_ce: 0.022193
2021-12-11 20:29:29,028 iteration 2515 : loss : 0.050043, loss_ce: 0.028400
2021-12-11 20:29:30,563 iteration 2516 : loss : 0.051150, loss_ce: 0.024729
 37%|█████████▉                 | 148/400 [1:03:11<1:44:13, 24.82s/it]2021-12-11 20:29:32,017 iteration 2517 : loss : 0.044235, loss_ce: 0.019454
2021-12-11 20:29:33,438 iteration 2518 : loss : 0.079791, loss_ce: 0.025242
2021-12-11 20:29:34,757 iteration 2519 : loss : 0.029367, loss_ce: 0.014105
2021-12-11 20:29:36,174 iteration 2520 : loss : 0.057123, loss_ce: 0.029744
2021-12-11 20:29:37,571 iteration 2521 : loss : 0.039562, loss_ce: 0.020960
2021-12-11 20:29:38,976 iteration 2522 : loss : 0.054250, loss_ce: 0.024458
2021-12-11 20:29:40,369 iteration 2523 : loss : 0.054333, loss_ce: 0.021071
2021-12-11 20:29:41,797 iteration 2524 : loss : 0.054546, loss_ce: 0.026845
2021-12-11 20:29:43,150 iteration 2525 : loss : 0.044194, loss_ce: 0.018790
2021-12-11 20:29:44,476 iteration 2526 : loss : 0.044373, loss_ce: 0.019431
2021-12-11 20:29:45,873 iteration 2527 : loss : 0.057236, loss_ce: 0.027816
2021-12-11 20:29:47,243 iteration 2528 : loss : 0.041822, loss_ce: 0.023906
2021-12-11 20:29:48,655 iteration 2529 : loss : 0.053107, loss_ce: 0.023857
2021-12-11 20:29:49,971 iteration 2530 : loss : 0.035676, loss_ce: 0.016943
2021-12-11 20:29:51,374 iteration 2531 : loss : 0.047883, loss_ce: 0.019917
2021-12-11 20:29:52,830 iteration 2532 : loss : 0.054020, loss_ce: 0.027236
2021-12-11 20:29:54,167 iteration 2533 : loss : 0.072870, loss_ce: 0.022631
 37%|██████████                 | 149/400 [1:03:35<1:42:16, 24.45s/it]2021-12-11 20:29:55,624 iteration 2534 : loss : 0.042595, loss_ce: 0.020436
2021-12-11 20:29:56,910 iteration 2535 : loss : 0.050530, loss_ce: 0.026040
2021-12-11 20:29:58,242 iteration 2536 : loss : 0.030867, loss_ce: 0.017672
2021-12-11 20:29:59,697 iteration 2537 : loss : 0.061119, loss_ce: 0.030969
2021-12-11 20:30:00,961 iteration 2538 : loss : 0.038748, loss_ce: 0.017619
2021-12-11 20:30:02,258 iteration 2539 : loss : 0.039747, loss_ce: 0.019029
2021-12-11 20:30:03,624 iteration 2540 : loss : 0.055164, loss_ce: 0.025474
2021-12-11 20:30:05,061 iteration 2541 : loss : 0.054207, loss_ce: 0.023805
2021-12-11 20:30:06,441 iteration 2542 : loss : 0.049055, loss_ce: 0.019761
2021-12-11 20:30:07,899 iteration 2543 : loss : 0.060057, loss_ce: 0.024356
2021-12-11 20:30:09,363 iteration 2544 : loss : 0.064227, loss_ce: 0.028518
2021-12-11 20:30:10,705 iteration 2545 : loss : 0.035647, loss_ce: 0.019235
2021-12-11 20:30:12,270 iteration 2546 : loss : 0.064003, loss_ce: 0.026390
2021-12-11 20:30:13,707 iteration 2547 : loss : 0.090160, loss_ce: 0.034899
2021-12-11 20:30:15,169 iteration 2548 : loss : 0.046946, loss_ce: 0.022964
2021-12-11 20:30:16,542 iteration 2549 : loss : 0.059286, loss_ce: 0.024601
2021-12-11 20:30:16,542 Training Data Eval:
2021-12-11 20:30:23,369   Average segmentation loss on training set: 0.0261
2021-12-11 20:30:23,369 Validation Data Eval:
2021-12-11 20:30:25,719   Average segmentation loss on validation set: 0.0909
2021-12-11 20:30:27,094 iteration 2550 : loss : 0.052531, loss_ce: 0.016738
2021-12-11 20:30:29,141 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed100epoch_149.pth
 38%|██████████▏                | 150/400 [1:04:10<1:54:57, 27.59s/it]2021-12-11 20:30:30,574 iteration 2551 : loss : 0.064785, loss_ce: 0.024988
2021-12-11 20:30:31,975 iteration 2552 : loss : 0.036178, loss_ce: 0.018796
2021-12-11 20:30:33,368 iteration 2553 : loss : 0.039097, loss_ce: 0.017760
2021-12-11 20:30:34,729 iteration 2554 : loss : 0.039214, loss_ce: 0.018914
2021-12-11 20:30:36,081 iteration 2555 : loss : 0.045110, loss_ce: 0.021513
2021-12-11 20:30:37,455 iteration 2556 : loss : 0.045106, loss_ce: 0.022365
2021-12-11 20:30:38,817 iteration 2557 : loss : 0.047297, loss_ce: 0.022374
2021-12-11 20:30:40,173 iteration 2558 : loss : 0.038758, loss_ce: 0.018493
2021-12-11 20:30:41,489 iteration 2559 : loss : 0.055908, loss_ce: 0.032966
2021-12-11 20:30:42,956 iteration 2560 : loss : 0.050622, loss_ce: 0.021916
2021-12-11 20:30:44,337 iteration 2561 : loss : 0.095613, loss_ce: 0.045194
2021-12-11 20:30:45,733 iteration 2562 : loss : 0.040324, loss_ce: 0.020968
2021-12-11 20:30:47,054 iteration 2563 : loss : 0.053581, loss_ce: 0.025560
2021-12-11 20:30:48,386 iteration 2564 : loss : 0.030872, loss_ce: 0.017109
2021-12-11 20:30:49,714 iteration 2565 : loss : 0.030949, loss_ce: 0.016117
2021-12-11 20:30:51,016 iteration 2566 : loss : 0.036088, loss_ce: 0.017426
2021-12-11 20:30:52,371 iteration 2567 : loss : 0.075254, loss_ce: 0.020372
 38%|██████████▏                | 151/400 [1:04:33<1:49:08, 26.30s/it]2021-12-11 20:30:53,785 iteration 2568 : loss : 0.056482, loss_ce: 0.025868
2021-12-11 20:30:55,151 iteration 2569 : loss : 0.037002, loss_ce: 0.019170
2021-12-11 20:30:56,617 iteration 2570 : loss : 0.052170, loss_ce: 0.019457
2021-12-11 20:30:57,983 iteration 2571 : loss : 0.048647, loss_ce: 0.018402
2021-12-11 20:30:59,292 iteration 2572 : loss : 0.063158, loss_ce: 0.025540
2021-12-11 20:31:00,644 iteration 2573 : loss : 0.037271, loss_ce: 0.018826
2021-12-11 20:31:01,974 iteration 2574 : loss : 0.044034, loss_ce: 0.021085
2021-12-11 20:31:03,275 iteration 2575 : loss : 0.049149, loss_ce: 0.020975
2021-12-11 20:31:04,588 iteration 2576 : loss : 0.034495, loss_ce: 0.014663
2021-12-11 20:31:05,987 iteration 2577 : loss : 0.067569, loss_ce: 0.038668
2021-12-11 20:31:07,331 iteration 2578 : loss : 0.037697, loss_ce: 0.018307
2021-12-11 20:31:08,709 iteration 2579 : loss : 0.054970, loss_ce: 0.024535
2021-12-11 20:31:10,025 iteration 2580 : loss : 0.038003, loss_ce: 0.016046
2021-12-11 20:31:11,414 iteration 2581 : loss : 0.061632, loss_ce: 0.026703
2021-12-11 20:31:12,850 iteration 2582 : loss : 0.037084, loss_ce: 0.020455
2021-12-11 20:31:14,191 iteration 2583 : loss : 0.052798, loss_ce: 0.027938
2021-12-11 20:31:15,543 iteration 2584 : loss : 0.052912, loss_ce: 0.020095
 38%|██████████▎                | 152/400 [1:04:56<1:44:49, 25.36s/it]2021-12-11 20:31:16,974 iteration 2585 : loss : 0.064699, loss_ce: 0.024663
2021-12-11 20:31:18,515 iteration 2586 : loss : 0.063494, loss_ce: 0.021885
2021-12-11 20:31:19,968 iteration 2587 : loss : 0.040812, loss_ce: 0.017020
2021-12-11 20:31:21,361 iteration 2588 : loss : 0.060830, loss_ce: 0.027817
2021-12-11 20:31:22,676 iteration 2589 : loss : 0.063453, loss_ce: 0.021274
2021-12-11 20:31:24,099 iteration 2590 : loss : 0.063117, loss_ce: 0.023183
2021-12-11 20:31:25,478 iteration 2591 : loss : 0.032127, loss_ce: 0.017819
2021-12-11 20:31:26,810 iteration 2592 : loss : 0.039063, loss_ce: 0.024330
2021-12-11 20:31:28,093 iteration 2593 : loss : 0.048413, loss_ce: 0.025120
2021-12-11 20:31:29,464 iteration 2594 : loss : 0.058627, loss_ce: 0.028370
2021-12-11 20:31:30,778 iteration 2595 : loss : 0.041100, loss_ce: 0.018581
2021-12-11 20:31:32,121 iteration 2596 : loss : 0.055373, loss_ce: 0.020740
2021-12-11 20:31:33,480 iteration 2597 : loss : 0.039080, loss_ce: 0.017022
2021-12-11 20:31:34,861 iteration 2598 : loss : 0.040817, loss_ce: 0.020863
2021-12-11 20:31:36,246 iteration 2599 : loss : 0.040420, loss_ce: 0.018466
2021-12-11 20:31:37,582 iteration 2600 : loss : 0.048469, loss_ce: 0.024558
2021-12-11 20:31:38,897 iteration 2601 : loss : 0.043878, loss_ce: 0.019203
 38%|██████████▎                | 153/400 [1:05:20<1:41:55, 24.76s/it]2021-12-11 20:31:40,306 iteration 2602 : loss : 0.050843, loss_ce: 0.020459
2021-12-11 20:31:41,697 iteration 2603 : loss : 0.085954, loss_ce: 0.023084
2021-12-11 20:31:43,071 iteration 2604 : loss : 0.035060, loss_ce: 0.016665
2021-12-11 20:31:44,444 iteration 2605 : loss : 0.055567, loss_ce: 0.020708
2021-12-11 20:31:45,823 iteration 2606 : loss : 0.045804, loss_ce: 0.021971
2021-12-11 20:31:47,207 iteration 2607 : loss : 0.041128, loss_ce: 0.022229
2021-12-11 20:31:48,544 iteration 2608 : loss : 0.041974, loss_ce: 0.019099
2021-12-11 20:31:49,958 iteration 2609 : loss : 0.038980, loss_ce: 0.018840
2021-12-11 20:31:51,359 iteration 2610 : loss : 0.044762, loss_ce: 0.020640
2021-12-11 20:31:52,783 iteration 2611 : loss : 0.050382, loss_ce: 0.023962
2021-12-11 20:31:54,192 iteration 2612 : loss : 0.053002, loss_ce: 0.022976
2021-12-11 20:31:55,637 iteration 2613 : loss : 0.055874, loss_ce: 0.023340
2021-12-11 20:31:57,064 iteration 2614 : loss : 0.043725, loss_ce: 0.021348
2021-12-11 20:31:58,459 iteration 2615 : loss : 0.046530, loss_ce: 0.023681
2021-12-11 20:31:59,897 iteration 2616 : loss : 0.052197, loss_ce: 0.025289
2021-12-11 20:32:01,292 iteration 2617 : loss : 0.048678, loss_ce: 0.022095
2021-12-11 20:32:02,716 iteration 2618 : loss : 0.044235, loss_ce: 0.017340
 38%|██████████▍                | 154/400 [1:05:44<1:40:20, 24.47s/it]2021-12-11 20:32:04,269 iteration 2619 : loss : 0.051789, loss_ce: 0.024605
2021-12-11 20:32:05,688 iteration 2620 : loss : 0.058303, loss_ce: 0.021818
2021-12-11 20:32:06,977 iteration 2621 : loss : 0.038773, loss_ce: 0.020089
2021-12-11 20:32:08,373 iteration 2622 : loss : 0.077028, loss_ce: 0.026680
2021-12-11 20:32:09,716 iteration 2623 : loss : 0.031691, loss_ce: 0.016194
2021-12-11 20:32:11,142 iteration 2624 : loss : 0.072802, loss_ce: 0.029216
2021-12-11 20:32:12,566 iteration 2625 : loss : 0.039786, loss_ce: 0.021126
2021-12-11 20:32:13,968 iteration 2626 : loss : 0.043604, loss_ce: 0.020932
2021-12-11 20:32:15,281 iteration 2627 : loss : 0.043324, loss_ce: 0.023702
2021-12-11 20:32:16,604 iteration 2628 : loss : 0.049198, loss_ce: 0.021687
2021-12-11 20:32:17,894 iteration 2629 : loss : 0.050408, loss_ce: 0.019868
2021-12-11 20:32:19,296 iteration 2630 : loss : 0.110906, loss_ce: 0.025150
2021-12-11 20:32:20,609 iteration 2631 : loss : 0.055470, loss_ce: 0.030415
2021-12-11 20:32:21,931 iteration 2632 : loss : 0.040174, loss_ce: 0.018937
2021-12-11 20:32:23,317 iteration 2633 : loss : 0.036691, loss_ce: 0.016778
2021-12-11 20:32:24,670 iteration 2634 : loss : 0.055669, loss_ce: 0.024028
2021-12-11 20:32:24,670 Training Data Eval:
2021-12-11 20:32:31,490   Average segmentation loss on training set: 0.0336
2021-12-11 20:32:31,491 Validation Data Eval:
2021-12-11 20:32:33,836   Average segmentation loss on validation set: 0.1287
2021-12-11 20:32:35,179 iteration 2635 : loss : 0.056003, loss_ce: 0.026885
 39%|██████████▍                | 155/400 [1:06:16<1:49:42, 26.87s/it]2021-12-11 20:32:36,545 iteration 2636 : loss : 0.059445, loss_ce: 0.023305
2021-12-11 20:32:37,896 iteration 2637 : loss : 0.056935, loss_ce: 0.028600
2021-12-11 20:32:39,344 iteration 2638 : loss : 0.074221, loss_ce: 0.034582
2021-12-11 20:32:40,707 iteration 2639 : loss : 0.039183, loss_ce: 0.019389
2021-12-11 20:32:42,097 iteration 2640 : loss : 0.068329, loss_ce: 0.026963
2021-12-11 20:32:43,459 iteration 2641 : loss : 0.043397, loss_ce: 0.019927
2021-12-11 20:32:44,862 iteration 2642 : loss : 0.045471, loss_ce: 0.018599
2021-12-11 20:32:46,221 iteration 2643 : loss : 0.052704, loss_ce: 0.027193
2021-12-11 20:32:47,614 iteration 2644 : loss : 0.058391, loss_ce: 0.032773
2021-12-11 20:32:48,928 iteration 2645 : loss : 0.047135, loss_ce: 0.021472
2021-12-11 20:32:50,258 iteration 2646 : loss : 0.067205, loss_ce: 0.028170
2021-12-11 20:32:51,703 iteration 2647 : loss : 0.064911, loss_ce: 0.028947
2021-12-11 20:32:53,053 iteration 2648 : loss : 0.049214, loss_ce: 0.021693
2021-12-11 20:32:54,439 iteration 2649 : loss : 0.049467, loss_ce: 0.020809
2021-12-11 20:32:55,925 iteration 2650 : loss : 0.051939, loss_ce: 0.022979
2021-12-11 20:32:57,287 iteration 2651 : loss : 0.051603, loss_ce: 0.023787
2021-12-11 20:32:58,736 iteration 2652 : loss : 0.046547, loss_ce: 0.021532
 39%|██████████▌                | 156/400 [1:06:40<1:45:13, 25.88s/it]2021-12-11 20:33:00,207 iteration 2653 : loss : 0.072847, loss_ce: 0.026072
2021-12-11 20:33:01,525 iteration 2654 : loss : 0.034843, loss_ce: 0.017418
2021-12-11 20:33:02,885 iteration 2655 : loss : 0.045454, loss_ce: 0.022511
2021-12-11 20:33:04,236 iteration 2656 : loss : 0.049672, loss_ce: 0.027907
2021-12-11 20:33:05,644 iteration 2657 : loss : 0.045428, loss_ce: 0.022613
2021-12-11 20:33:07,069 iteration 2658 : loss : 0.071841, loss_ce: 0.026280
2021-12-11 20:33:08,471 iteration 2659 : loss : 0.052602, loss_ce: 0.024303
2021-12-11 20:33:09,922 iteration 2660 : loss : 0.052720, loss_ce: 0.026558
2021-12-11 20:33:11,209 iteration 2661 : loss : 0.041854, loss_ce: 0.018762
2021-12-11 20:33:12,651 iteration 2662 : loss : 0.054889, loss_ce: 0.024734
2021-12-11 20:33:14,067 iteration 2663 : loss : 0.050716, loss_ce: 0.025525
2021-12-11 20:33:15,384 iteration 2664 : loss : 0.054093, loss_ce: 0.024377
2021-12-11 20:33:16,723 iteration 2665 : loss : 0.037827, loss_ce: 0.020793
2021-12-11 20:33:18,167 iteration 2666 : loss : 0.061821, loss_ce: 0.023623
2021-12-11 20:33:19,499 iteration 2667 : loss : 0.045790, loss_ce: 0.020280
2021-12-11 20:33:20,940 iteration 2668 : loss : 0.058028, loss_ce: 0.025270
2021-12-11 20:33:22,298 iteration 2669 : loss : 0.049893, loss_ce: 0.019759
 39%|██████████▌                | 157/400 [1:07:03<1:41:59, 25.18s/it]2021-12-11 20:33:23,636 iteration 2670 : loss : 0.037003, loss_ce: 0.017159
2021-12-11 20:33:24,939 iteration 2671 : loss : 0.046776, loss_ce: 0.020620
2021-12-11 20:33:26,274 iteration 2672 : loss : 0.057156, loss_ce: 0.021664
2021-12-11 20:33:27,606 iteration 2673 : loss : 0.039010, loss_ce: 0.015558
2021-12-11 20:33:28,995 iteration 2674 : loss : 0.042095, loss_ce: 0.016882
2021-12-11 20:33:30,416 iteration 2675 : loss : 0.035022, loss_ce: 0.016847
2021-12-11 20:33:31,777 iteration 2676 : loss : 0.032286, loss_ce: 0.015651
2021-12-11 20:33:33,143 iteration 2677 : loss : 0.041054, loss_ce: 0.019238
2021-12-11 20:33:34,459 iteration 2678 : loss : 0.047929, loss_ce: 0.025920
2021-12-11 20:33:35,833 iteration 2679 : loss : 0.044773, loss_ce: 0.020610
2021-12-11 20:33:37,270 iteration 2680 : loss : 0.048954, loss_ce: 0.025507
2021-12-11 20:33:38,654 iteration 2681 : loss : 0.045722, loss_ce: 0.019724
2021-12-11 20:33:40,068 iteration 2682 : loss : 0.038926, loss_ce: 0.018478
2021-12-11 20:33:41,409 iteration 2683 : loss : 0.043290, loss_ce: 0.021449
2021-12-11 20:33:42,797 iteration 2684 : loss : 0.043694, loss_ce: 0.021985
2021-12-11 20:33:44,215 iteration 2685 : loss : 0.046195, loss_ce: 0.022665
2021-12-11 20:33:45,622 iteration 2686 : loss : 0.075283, loss_ce: 0.021466
 40%|██████████▋                | 158/400 [1:07:26<1:39:19, 24.63s/it]2021-12-11 20:33:47,002 iteration 2687 : loss : 0.045931, loss_ce: 0.022248
2021-12-11 20:33:48,358 iteration 2688 : loss : 0.039755, loss_ce: 0.017255
2021-12-11 20:33:49,686 iteration 2689 : loss : 0.039403, loss_ce: 0.017437
2021-12-11 20:33:51,094 iteration 2690 : loss : 0.079014, loss_ce: 0.021870
2021-12-11 20:33:52,425 iteration 2691 : loss : 0.037974, loss_ce: 0.020221
2021-12-11 20:33:53,758 iteration 2692 : loss : 0.050302, loss_ce: 0.020828
2021-12-11 20:33:55,081 iteration 2693 : loss : 0.037032, loss_ce: 0.017220
2021-12-11 20:33:56,473 iteration 2694 : loss : 0.038267, loss_ce: 0.018712
2021-12-11 20:33:57,837 iteration 2695 : loss : 0.045468, loss_ce: 0.025971
2021-12-11 20:33:59,204 iteration 2696 : loss : 0.039766, loss_ce: 0.019747
2021-12-11 20:34:00,534 iteration 2697 : loss : 0.053316, loss_ce: 0.027043
2021-12-11 20:34:01,870 iteration 2698 : loss : 0.049236, loss_ce: 0.018638
2021-12-11 20:34:03,228 iteration 2699 : loss : 0.050912, loss_ce: 0.027324
2021-12-11 20:34:04,560 iteration 2700 : loss : 0.044178, loss_ce: 0.018649
2021-12-11 20:34:06,038 iteration 2701 : loss : 0.058533, loss_ce: 0.019116
2021-12-11 20:34:07,453 iteration 2702 : loss : 0.052418, loss_ce: 0.029020
2021-12-11 20:34:08,889 iteration 2703 : loss : 0.048247, loss_ce: 0.021631
 40%|██████████▋                | 159/400 [1:07:50<1:37:16, 24.22s/it]2021-12-11 20:34:10,367 iteration 2704 : loss : 0.061970, loss_ce: 0.023941
2021-12-11 20:34:11,701 iteration 2705 : loss : 0.040727, loss_ce: 0.020973
2021-12-11 20:34:13,124 iteration 2706 : loss : 0.073239, loss_ce: 0.031949
2021-12-11 20:34:14,478 iteration 2707 : loss : 0.043384, loss_ce: 0.023896
2021-12-11 20:34:15,886 iteration 2708 : loss : 0.040303, loss_ce: 0.020427
2021-12-11 20:34:17,258 iteration 2709 : loss : 0.046073, loss_ce: 0.022087
2021-12-11 20:34:18,569 iteration 2710 : loss : 0.032925, loss_ce: 0.017445
2021-12-11 20:34:19,954 iteration 2711 : loss : 0.050679, loss_ce: 0.021462
2021-12-11 20:34:21,359 iteration 2712 : loss : 0.047789, loss_ce: 0.017044
2021-12-11 20:34:22,692 iteration 2713 : loss : 0.030403, loss_ce: 0.017764
2021-12-11 20:34:24,132 iteration 2714 : loss : 0.054814, loss_ce: 0.023949
2021-12-11 20:34:25,560 iteration 2715 : loss : 0.062790, loss_ce: 0.022551
2021-12-11 20:34:26,984 iteration 2716 : loss : 0.041143, loss_ce: 0.018701
2021-12-11 20:34:28,356 iteration 2717 : loss : 0.049152, loss_ce: 0.023807
2021-12-11 20:34:29,774 iteration 2718 : loss : 0.088163, loss_ce: 0.027564
2021-12-11 20:34:31,120 iteration 2719 : loss : 0.041024, loss_ce: 0.017773
2021-12-11 20:34:31,121 Training Data Eval:
2021-12-11 20:34:37,916   Average segmentation loss on training set: 0.0302
2021-12-11 20:34:37,917 Validation Data Eval:
2021-12-11 20:34:40,265   Average segmentation loss on validation set: 0.1279
2021-12-11 20:34:41,595 iteration 2720 : loss : 0.031898, loss_ce: 0.014681
 40%|██████████▊                | 160/400 [1:08:22<1:47:02, 26.76s/it]2021-12-11 20:34:43,033 iteration 2721 : loss : 0.056828, loss_ce: 0.027597
2021-12-11 20:34:44,419 iteration 2722 : loss : 0.060066, loss_ce: 0.024866
2021-12-11 20:34:45,843 iteration 2723 : loss : 0.045627, loss_ce: 0.020670
2021-12-11 20:34:47,230 iteration 2724 : loss : 0.071369, loss_ce: 0.027013
2021-12-11 20:34:48,588 iteration 2725 : loss : 0.054377, loss_ce: 0.021662
2021-12-11 20:34:50,098 iteration 2726 : loss : 0.063399, loss_ce: 0.023591
2021-12-11 20:34:51,535 iteration 2727 : loss : 0.041997, loss_ce: 0.021793
2021-12-11 20:34:52,916 iteration 2728 : loss : 0.047215, loss_ce: 0.022427
2021-12-11 20:34:54,281 iteration 2729 : loss : 0.046849, loss_ce: 0.025705
2021-12-11 20:34:55,597 iteration 2730 : loss : 0.035331, loss_ce: 0.017107
2021-12-11 20:34:56,981 iteration 2731 : loss : 0.048326, loss_ce: 0.024075
2021-12-11 20:34:58,415 iteration 2732 : loss : 0.052567, loss_ce: 0.025798
2021-12-11 20:34:59,808 iteration 2733 : loss : 0.039812, loss_ce: 0.018056
2021-12-11 20:35:01,184 iteration 2734 : loss : 0.053448, loss_ce: 0.020683
2021-12-11 20:35:02,653 iteration 2735 : loss : 0.064609, loss_ce: 0.028810
2021-12-11 20:35:04,014 iteration 2736 : loss : 0.032522, loss_ce: 0.017531
2021-12-11 20:35:05,422 iteration 2737 : loss : 0.052186, loss_ce: 0.024036
 40%|██████████▊                | 161/400 [1:08:46<1:43:05, 25.88s/it]2021-12-11 20:35:06,790 iteration 2738 : loss : 0.033791, loss_ce: 0.017161
2021-12-11 20:35:08,227 iteration 2739 : loss : 0.044881, loss_ce: 0.022497
2021-12-11 20:35:09,685 iteration 2740 : loss : 0.050336, loss_ce: 0.022860
2021-12-11 20:35:11,038 iteration 2741 : loss : 0.036474, loss_ce: 0.016722
2021-12-11 20:35:12,355 iteration 2742 : loss : 0.053064, loss_ce: 0.022021
2021-12-11 20:35:13,754 iteration 2743 : loss : 0.042713, loss_ce: 0.017718
2021-12-11 20:35:15,265 iteration 2744 : loss : 0.081950, loss_ce: 0.034545
2021-12-11 20:35:16,646 iteration 2745 : loss : 0.047684, loss_ce: 0.017145
2021-12-11 20:35:18,005 iteration 2746 : loss : 0.032901, loss_ce: 0.014987
2021-12-11 20:35:19,491 iteration 2747 : loss : 0.067461, loss_ce: 0.030529
2021-12-11 20:35:20,852 iteration 2748 : loss : 0.044496, loss_ce: 0.019873
2021-12-11 20:35:22,242 iteration 2749 : loss : 0.051012, loss_ce: 0.026213
2021-12-11 20:35:23,606 iteration 2750 : loss : 0.043125, loss_ce: 0.018371
2021-12-11 20:35:24,972 iteration 2751 : loss : 0.050699, loss_ce: 0.018425
2021-12-11 20:35:26,313 iteration 2752 : loss : 0.039862, loss_ce: 0.019905
2021-12-11 20:35:27,674 iteration 2753 : loss : 0.045341, loss_ce: 0.022769
2021-12-11 20:35:29,067 iteration 2754 : loss : 0.065544, loss_ce: 0.023844
 40%|██████████▉                | 162/400 [1:09:10<1:40:00, 25.21s/it]2021-12-11 20:35:30,491 iteration 2755 : loss : 0.049954, loss_ce: 0.023133
2021-12-11 20:35:31,884 iteration 2756 : loss : 0.047516, loss_ce: 0.022496
2021-12-11 20:35:33,217 iteration 2757 : loss : 0.036143, loss_ce: 0.017503
2021-12-11 20:35:34,583 iteration 2758 : loss : 0.045572, loss_ce: 0.021955
2021-12-11 20:35:35,948 iteration 2759 : loss : 0.036778, loss_ce: 0.017812
2021-12-11 20:35:37,363 iteration 2760 : loss : 0.063805, loss_ce: 0.021801
2021-12-11 20:35:38,742 iteration 2761 : loss : 0.034058, loss_ce: 0.015467
2021-12-11 20:35:40,104 iteration 2762 : loss : 0.047814, loss_ce: 0.023504
2021-12-11 20:35:41,465 iteration 2763 : loss : 0.046166, loss_ce: 0.021054
2021-12-11 20:35:42,781 iteration 2764 : loss : 0.042197, loss_ce: 0.021982
2021-12-11 20:35:44,149 iteration 2765 : loss : 0.060705, loss_ce: 0.029441
2021-12-11 20:35:45,570 iteration 2766 : loss : 0.063455, loss_ce: 0.024917
2021-12-11 20:35:46,882 iteration 2767 : loss : 0.033334, loss_ce: 0.014344
2021-12-11 20:35:48,367 iteration 2768 : loss : 0.037915, loss_ce: 0.017829
2021-12-11 20:35:49,724 iteration 2769 : loss : 0.044970, loss_ce: 0.017839
2021-12-11 20:35:51,043 iteration 2770 : loss : 0.046026, loss_ce: 0.024635
2021-12-11 20:35:52,404 iteration 2771 : loss : 0.039688, loss_ce: 0.022323
 41%|███████████                | 163/400 [1:09:33<1:37:22, 24.65s/it]2021-12-11 20:35:53,888 iteration 2772 : loss : 0.048411, loss_ce: 0.020056
2021-12-11 20:35:55,199 iteration 2773 : loss : 0.041864, loss_ce: 0.017688
2021-12-11 20:35:56,554 iteration 2774 : loss : 0.037840, loss_ce: 0.020335
2021-12-11 20:35:57,919 iteration 2775 : loss : 0.043538, loss_ce: 0.022103
2021-12-11 20:35:59,312 iteration 2776 : loss : 0.071139, loss_ce: 0.045253
2021-12-11 20:36:00,729 iteration 2777 : loss : 0.042930, loss_ce: 0.022287
2021-12-11 20:36:02,043 iteration 2778 : loss : 0.036306, loss_ce: 0.018382
2021-12-11 20:36:03,452 iteration 2779 : loss : 0.052263, loss_ce: 0.027161
2021-12-11 20:36:04,903 iteration 2780 : loss : 0.047756, loss_ce: 0.022306
2021-12-11 20:36:06,199 iteration 2781 : loss : 0.040364, loss_ce: 0.019336
2021-12-11 20:36:07,598 iteration 2782 : loss : 0.046210, loss_ce: 0.022183
2021-12-11 20:36:09,044 iteration 2783 : loss : 0.053681, loss_ce: 0.022737
2021-12-11 20:36:10,521 iteration 2784 : loss : 0.054324, loss_ce: 0.026616
2021-12-11 20:36:11,840 iteration 2785 : loss : 0.039098, loss_ce: 0.017103
2021-12-11 20:36:13,232 iteration 2786 : loss : 0.045750, loss_ce: 0.020044
2021-12-11 20:36:14,693 iteration 2787 : loss : 0.086748, loss_ce: 0.024701
2021-12-11 20:36:16,091 iteration 2788 : loss : 0.042680, loss_ce: 0.017887
 41%|███████████                | 164/400 [1:09:57<1:35:48, 24.36s/it]2021-12-11 20:36:17,524 iteration 2789 : loss : 0.031674, loss_ce: 0.019054
2021-12-11 20:36:18,880 iteration 2790 : loss : 0.047833, loss_ce: 0.018098
2021-12-11 20:36:20,288 iteration 2791 : loss : 0.042605, loss_ce: 0.016903
2021-12-11 20:36:21,723 iteration 2792 : loss : 0.066488, loss_ce: 0.024930
2021-12-11 20:36:23,115 iteration 2793 : loss : 0.058472, loss_ce: 0.030800
2021-12-11 20:36:24,516 iteration 2794 : loss : 0.043094, loss_ce: 0.020291
2021-12-11 20:36:25,957 iteration 2795 : loss : 0.045072, loss_ce: 0.020022
2021-12-11 20:36:27,441 iteration 2796 : loss : 0.070544, loss_ce: 0.031658
2021-12-11 20:36:28,871 iteration 2797 : loss : 0.041130, loss_ce: 0.019883
2021-12-11 20:36:30,344 iteration 2798 : loss : 0.057593, loss_ce: 0.024496
2021-12-11 20:36:31,784 iteration 2799 : loss : 0.046757, loss_ce: 0.020144
2021-12-11 20:36:33,208 iteration 2800 : loss : 0.040724, loss_ce: 0.020959
2021-12-11 20:36:34,579 iteration 2801 : loss : 0.055982, loss_ce: 0.030589
2021-12-11 20:36:35,906 iteration 2802 : loss : 0.034041, loss_ce: 0.017294
2021-12-11 20:36:37,378 iteration 2803 : loss : 0.054073, loss_ce: 0.022453
2021-12-11 20:36:38,765 iteration 2804 : loss : 0.055301, loss_ce: 0.023997
2021-12-11 20:36:38,765 Training Data Eval:
2021-12-11 20:36:45,585   Average segmentation loss on training set: 0.0259
2021-12-11 20:36:45,585 Validation Data Eval:
2021-12-11 20:36:47,952   Average segmentation loss on validation set: 0.0835
2021-12-11 20:36:49,937 Found new lowest validation loss at iteration 2804! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 20:36:51,319 iteration 2805 : loss : 0.068373, loss_ce: 0.022893
 41%|███████████▏               | 165/400 [1:10:32<1:48:10, 27.62s/it]2021-12-11 20:36:52,782 iteration 2806 : loss : 0.063843, loss_ce: 0.024907
2021-12-11 20:36:54,303 iteration 2807 : loss : 0.047804, loss_ce: 0.023376
2021-12-11 20:36:55,596 iteration 2808 : loss : 0.044830, loss_ce: 0.025672
2021-12-11 20:36:56,974 iteration 2809 : loss : 0.056221, loss_ce: 0.028871
2021-12-11 20:36:58,392 iteration 2810 : loss : 0.046706, loss_ce: 0.023716
2021-12-11 20:36:59,749 iteration 2811 : loss : 0.038280, loss_ce: 0.018645
2021-12-11 20:37:01,143 iteration 2812 : loss : 0.037560, loss_ce: 0.017999
2021-12-11 20:37:02,559 iteration 2813 : loss : 0.040664, loss_ce: 0.016079
2021-12-11 20:37:03,937 iteration 2814 : loss : 0.048527, loss_ce: 0.022285
2021-12-11 20:37:05,280 iteration 2815 : loss : 0.037560, loss_ce: 0.018762
2021-12-11 20:37:06,622 iteration 2816 : loss : 0.037562, loss_ce: 0.018928
2021-12-11 20:37:07,965 iteration 2817 : loss : 0.037320, loss_ce: 0.015608
2021-12-11 20:37:09,406 iteration 2818 : loss : 0.046258, loss_ce: 0.017791
2021-12-11 20:37:10,823 iteration 2819 : loss : 0.063334, loss_ce: 0.034180
2021-12-11 20:37:12,312 iteration 2820 : loss : 0.051902, loss_ce: 0.025259
2021-12-11 20:37:13,698 iteration 2821 : loss : 0.043373, loss_ce: 0.022499
2021-12-11 20:37:15,049 iteration 2822 : loss : 0.032109, loss_ce: 0.014422
 42%|███████████▏               | 166/400 [1:10:56<1:43:09, 26.45s/it]2021-12-11 20:37:16,551 iteration 2823 : loss : 0.062471, loss_ce: 0.025055
2021-12-11 20:37:17,887 iteration 2824 : loss : 0.033339, loss_ce: 0.015843
2021-12-11 20:37:19,293 iteration 2825 : loss : 0.051532, loss_ce: 0.024492
2021-12-11 20:37:20,629 iteration 2826 : loss : 0.040363, loss_ce: 0.020922
2021-12-11 20:37:22,032 iteration 2827 : loss : 0.054047, loss_ce: 0.029260
2021-12-11 20:37:23,422 iteration 2828 : loss : 0.058953, loss_ce: 0.031946
2021-12-11 20:37:24,917 iteration 2829 : loss : 0.041751, loss_ce: 0.019750
2021-12-11 20:37:26,243 iteration 2830 : loss : 0.037067, loss_ce: 0.020243
2021-12-11 20:37:27,620 iteration 2831 : loss : 0.047719, loss_ce: 0.023084
2021-12-11 20:37:29,051 iteration 2832 : loss : 0.038000, loss_ce: 0.016213
2021-12-11 20:37:30,495 iteration 2833 : loss : 0.052282, loss_ce: 0.023769
2021-12-11 20:37:31,879 iteration 2834 : loss : 0.061183, loss_ce: 0.025445
2021-12-11 20:37:33,328 iteration 2835 : loss : 0.076608, loss_ce: 0.024426
2021-12-11 20:37:34,802 iteration 2836 : loss : 0.063610, loss_ce: 0.028396
2021-12-11 20:37:36,212 iteration 2837 : loss : 0.056051, loss_ce: 0.019411
2021-12-11 20:37:37,548 iteration 2838 : loss : 0.038675, loss_ce: 0.016328
2021-12-11 20:37:38,987 iteration 2839 : loss : 0.041389, loss_ce: 0.019388
 42%|███████████▎               | 167/400 [1:11:20<1:39:48, 25.70s/it]2021-12-11 20:37:40,406 iteration 2840 : loss : 0.055942, loss_ce: 0.030220
2021-12-11 20:37:41,827 iteration 2841 : loss : 0.037124, loss_ce: 0.018733
2021-12-11 20:37:43,244 iteration 2842 : loss : 0.075859, loss_ce: 0.029866
2021-12-11 20:37:44,568 iteration 2843 : loss : 0.041136, loss_ce: 0.020660
2021-12-11 20:37:45,874 iteration 2844 : loss : 0.031542, loss_ce: 0.014324
2021-12-11 20:37:47,250 iteration 2845 : loss : 0.050576, loss_ce: 0.024001
2021-12-11 20:37:48,773 iteration 2846 : loss : 0.072322, loss_ce: 0.024600
2021-12-11 20:37:50,217 iteration 2847 : loss : 0.074771, loss_ce: 0.031292
2021-12-11 20:37:51,627 iteration 2848 : loss : 0.056925, loss_ce: 0.021558
2021-12-11 20:37:53,057 iteration 2849 : loss : 0.043232, loss_ce: 0.020188
2021-12-11 20:37:54,456 iteration 2850 : loss : 0.049571, loss_ce: 0.027006
2021-12-11 20:37:55,958 iteration 2851 : loss : 0.058112, loss_ce: 0.024912
2021-12-11 20:37:57,315 iteration 2852 : loss : 0.041208, loss_ce: 0.017327
2021-12-11 20:37:58,718 iteration 2853 : loss : 0.042566, loss_ce: 0.017473
2021-12-11 20:38:00,038 iteration 2854 : loss : 0.041713, loss_ce: 0.017351
2021-12-11 20:38:01,383 iteration 2855 : loss : 0.039823, loss_ce: 0.021636
2021-12-11 20:38:02,684 iteration 2856 : loss : 0.028770, loss_ce: 0.017263
 42%|███████████▎               | 168/400 [1:11:43<1:37:03, 25.10s/it]2021-12-11 20:38:04,104 iteration 2857 : loss : 0.046006, loss_ce: 0.021175
2021-12-11 20:38:05,596 iteration 2858 : loss : 0.096994, loss_ce: 0.029535
2021-12-11 20:38:07,010 iteration 2859 : loss : 0.046365, loss_ce: 0.021431
2021-12-11 20:38:08,431 iteration 2860 : loss : 0.038425, loss_ce: 0.018027
2021-12-11 20:38:09,822 iteration 2861 : loss : 0.037763, loss_ce: 0.016002
2021-12-11 20:38:11,248 iteration 2862 : loss : 0.072491, loss_ce: 0.020828
2021-12-11 20:38:12,585 iteration 2863 : loss : 0.034186, loss_ce: 0.016608
2021-12-11 20:38:13,953 iteration 2864 : loss : 0.037697, loss_ce: 0.017343
2021-12-11 20:38:15,320 iteration 2865 : loss : 0.043833, loss_ce: 0.019659
2021-12-11 20:38:16,686 iteration 2866 : loss : 0.035447, loss_ce: 0.017092
2021-12-11 20:38:18,154 iteration 2867 : loss : 0.057508, loss_ce: 0.023352
2021-12-11 20:38:19,573 iteration 2868 : loss : 0.042920, loss_ce: 0.021170
2021-12-11 20:38:21,004 iteration 2869 : loss : 0.056469, loss_ce: 0.028308
2021-12-11 20:38:22,484 iteration 2870 : loss : 0.042648, loss_ce: 0.018942
2021-12-11 20:38:23,807 iteration 2871 : loss : 0.059519, loss_ce: 0.036312
2021-12-11 20:38:25,160 iteration 2872 : loss : 0.045822, loss_ce: 0.023927
2021-12-11 20:38:26,533 iteration 2873 : loss : 0.043960, loss_ce: 0.023231
 42%|███████████▍               | 169/400 [1:12:07<1:35:11, 24.72s/it]2021-12-11 20:38:27,957 iteration 2874 : loss : 0.039427, loss_ce: 0.017379
2021-12-11 20:38:29,403 iteration 2875 : loss : 0.040087, loss_ce: 0.019229
2021-12-11 20:38:30,753 iteration 2876 : loss : 0.074038, loss_ce: 0.028356
2021-12-11 20:38:32,067 iteration 2877 : loss : 0.045111, loss_ce: 0.017998
2021-12-11 20:38:33,428 iteration 2878 : loss : 0.037640, loss_ce: 0.015423
2021-12-11 20:38:34,724 iteration 2879 : loss : 0.053457, loss_ce: 0.028571
2021-12-11 20:38:36,137 iteration 2880 : loss : 0.042247, loss_ce: 0.025319
2021-12-11 20:38:37,442 iteration 2881 : loss : 0.034895, loss_ce: 0.019933
2021-12-11 20:38:38,794 iteration 2882 : loss : 0.059669, loss_ce: 0.019990
2021-12-11 20:38:40,212 iteration 2883 : loss : 0.040875, loss_ce: 0.016705
2021-12-11 20:38:41,647 iteration 2884 : loss : 0.052318, loss_ce: 0.024764
2021-12-11 20:38:43,027 iteration 2885 : loss : 0.035295, loss_ce: 0.017426
2021-12-11 20:38:44,372 iteration 2886 : loss : 0.053764, loss_ce: 0.021530
2021-12-11 20:38:45,713 iteration 2887 : loss : 0.041179, loss_ce: 0.017033
2021-12-11 20:38:47,125 iteration 2888 : loss : 0.042990, loss_ce: 0.018845
2021-12-11 20:38:48,498 iteration 2889 : loss : 0.045047, loss_ce: 0.021777
2021-12-11 20:38:48,498 Training Data Eval:
2021-12-11 20:38:55,327   Average segmentation loss on training set: 0.0259
2021-12-11 20:38:55,328 Validation Data Eval:
2021-12-11 20:38:57,689   Average segmentation loss on validation set: 0.1023
2021-12-11 20:38:59,053 iteration 2890 : loss : 0.051486, loss_ce: 0.024155
 42%|███████████▍               | 170/400 [1:12:40<1:43:44, 27.06s/it]2021-12-11 20:39:00,460 iteration 2891 : loss : 0.044398, loss_ce: 0.022086
2021-12-11 20:39:01,866 iteration 2892 : loss : 0.041104, loss_ce: 0.019497
2021-12-11 20:39:03,182 iteration 2893 : loss : 0.033930, loss_ce: 0.018314
2021-12-11 20:39:04,517 iteration 2894 : loss : 0.038606, loss_ce: 0.019770
2021-12-11 20:39:05,867 iteration 2895 : loss : 0.038933, loss_ce: 0.017488
2021-12-11 20:39:07,228 iteration 2896 : loss : 0.042881, loss_ce: 0.019248
2021-12-11 20:39:08,737 iteration 2897 : loss : 0.092837, loss_ce: 0.029830
2021-12-11 20:39:10,149 iteration 2898 : loss : 0.061863, loss_ce: 0.023498
2021-12-11 20:39:11,532 iteration 2899 : loss : 0.053382, loss_ce: 0.027564
2021-12-11 20:39:13,053 iteration 2900 : loss : 0.047898, loss_ce: 0.024024
2021-12-11 20:39:14,481 iteration 2901 : loss : 0.043368, loss_ce: 0.019793
2021-12-11 20:39:15,840 iteration 2902 : loss : 0.076927, loss_ce: 0.046753
2021-12-11 20:39:17,229 iteration 2903 : loss : 0.057076, loss_ce: 0.019171
2021-12-11 20:39:18,596 iteration 2904 : loss : 0.046116, loss_ce: 0.025514
2021-12-11 20:39:20,008 iteration 2905 : loss : 0.051058, loss_ce: 0.027576
2021-12-11 20:39:21,345 iteration 2906 : loss : 0.041803, loss_ce: 0.019697
2021-12-11 20:39:22,725 iteration 2907 : loss : 0.030218, loss_ce: 0.014872
 43%|███████████▌               | 171/400 [1:13:04<1:39:23, 26.04s/it]2021-12-11 20:39:24,161 iteration 2908 : loss : 0.041904, loss_ce: 0.018344
2021-12-11 20:39:25,594 iteration 2909 : loss : 0.050767, loss_ce: 0.020908
2021-12-11 20:39:27,012 iteration 2910 : loss : 0.036371, loss_ce: 0.017100
2021-12-11 20:39:28,445 iteration 2911 : loss : 0.038787, loss_ce: 0.017062
2021-12-11 20:39:29,793 iteration 2912 : loss : 0.034208, loss_ce: 0.015730
2021-12-11 20:39:31,179 iteration 2913 : loss : 0.053563, loss_ce: 0.020976
2021-12-11 20:39:32,591 iteration 2914 : loss : 0.056666, loss_ce: 0.027888
2021-12-11 20:39:33,979 iteration 2915 : loss : 0.052304, loss_ce: 0.031030
2021-12-11 20:39:35,360 iteration 2916 : loss : 0.054465, loss_ce: 0.025331
2021-12-11 20:39:36,732 iteration 2917 : loss : 0.035070, loss_ce: 0.016381
2021-12-11 20:39:38,035 iteration 2918 : loss : 0.042591, loss_ce: 0.022186
2021-12-11 20:39:39,427 iteration 2919 : loss : 0.051833, loss_ce: 0.020488
2021-12-11 20:39:40,787 iteration 2920 : loss : 0.037067, loss_ce: 0.018353
2021-12-11 20:39:42,163 iteration 2921 : loss : 0.039702, loss_ce: 0.020270
2021-12-11 20:39:43,541 iteration 2922 : loss : 0.048120, loss_ce: 0.023877
2021-12-11 20:39:45,004 iteration 2923 : loss : 0.049814, loss_ce: 0.018866
2021-12-11 20:39:46,431 iteration 2924 : loss : 0.031963, loss_ce: 0.016223
 43%|███████████▌               | 172/400 [1:13:27<1:36:18, 25.34s/it]2021-12-11 20:39:47,870 iteration 2925 : loss : 0.049975, loss_ce: 0.023207
2021-12-11 20:39:49,192 iteration 2926 : loss : 0.034517, loss_ce: 0.015493
2021-12-11 20:39:50,580 iteration 2927 : loss : 0.037775, loss_ce: 0.016828
2021-12-11 20:39:51,992 iteration 2928 : loss : 0.045654, loss_ce: 0.022799
2021-12-11 20:39:53,327 iteration 2929 : loss : 0.059806, loss_ce: 0.021150
2021-12-11 20:39:54,663 iteration 2930 : loss : 0.035637, loss_ce: 0.019146
2021-12-11 20:39:56,027 iteration 2931 : loss : 0.119121, loss_ce: 0.022819
2021-12-11 20:39:57,525 iteration 2932 : loss : 0.054930, loss_ce: 0.026128
2021-12-11 20:39:58,953 iteration 2933 : loss : 0.054595, loss_ce: 0.026756
2021-12-11 20:40:00,399 iteration 2934 : loss : 0.036460, loss_ce: 0.019858
2021-12-11 20:40:01,762 iteration 2935 : loss : 0.042290, loss_ce: 0.017582
2021-12-11 20:40:03,140 iteration 2936 : loss : 0.034653, loss_ce: 0.018488
2021-12-11 20:40:04,536 iteration 2937 : loss : 0.040778, loss_ce: 0.022261
2021-12-11 20:40:05,948 iteration 2938 : loss : 0.043417, loss_ce: 0.019400
2021-12-11 20:40:07,242 iteration 2939 : loss : 0.035863, loss_ce: 0.013962
2021-12-11 20:40:08,597 iteration 2940 : loss : 0.040217, loss_ce: 0.020830
2021-12-11 20:40:09,955 iteration 2941 : loss : 0.047282, loss_ce: 0.022141
 43%|███████████▋               | 173/400 [1:13:51<1:33:48, 24.80s/it]2021-12-11 20:40:11,453 iteration 2942 : loss : 0.058840, loss_ce: 0.031326
2021-12-11 20:40:12,875 iteration 2943 : loss : 0.032343, loss_ce: 0.016538
2021-12-11 20:40:14,272 iteration 2944 : loss : 0.073523, loss_ce: 0.023995
2021-12-11 20:40:15,658 iteration 2945 : loss : 0.061356, loss_ce: 0.029117
2021-12-11 20:40:17,098 iteration 2946 : loss : 0.033362, loss_ce: 0.018004
2021-12-11 20:40:18,452 iteration 2947 : loss : 0.043325, loss_ce: 0.017567
2021-12-11 20:40:19,858 iteration 2948 : loss : 0.040190, loss_ce: 0.019351
2021-12-11 20:40:21,212 iteration 2949 : loss : 0.042682, loss_ce: 0.018879
2021-12-11 20:40:22,515 iteration 2950 : loss : 0.032928, loss_ce: 0.015429
2021-12-11 20:40:23,886 iteration 2951 : loss : 0.036744, loss_ce: 0.015004
2021-12-11 20:40:25,176 iteration 2952 : loss : 0.115831, loss_ce: 0.027615
2021-12-11 20:40:26,605 iteration 2953 : loss : 0.048262, loss_ce: 0.021517
2021-12-11 20:40:27,988 iteration 2954 : loss : 0.041018, loss_ce: 0.020108
2021-12-11 20:40:29,380 iteration 2955 : loss : 0.062521, loss_ce: 0.020054
2021-12-11 20:40:30,702 iteration 2956 : loss : 0.053499, loss_ce: 0.030797
2021-12-11 20:40:32,021 iteration 2957 : loss : 0.040510, loss_ce: 0.018448
2021-12-11 20:40:33,403 iteration 2958 : loss : 0.049390, loss_ce: 0.024275
 44%|███████████▋               | 174/400 [1:14:14<1:31:52, 24.39s/it]2021-12-11 20:40:34,855 iteration 2959 : loss : 0.046274, loss_ce: 0.023595
2021-12-11 20:40:36,229 iteration 2960 : loss : 0.044951, loss_ce: 0.022643
2021-12-11 20:40:37,652 iteration 2961 : loss : 0.051104, loss_ce: 0.022609
2021-12-11 20:40:39,068 iteration 2962 : loss : 0.039513, loss_ce: 0.017999
2021-12-11 20:40:40,407 iteration 2963 : loss : 0.042061, loss_ce: 0.021144
2021-12-11 20:40:41,748 iteration 2964 : loss : 0.043420, loss_ce: 0.019555
2021-12-11 20:40:43,052 iteration 2965 : loss : 0.042801, loss_ce: 0.019290
2021-12-11 20:40:44,349 iteration 2966 : loss : 0.034179, loss_ce: 0.015708
2021-12-11 20:40:45,733 iteration 2967 : loss : 0.038733, loss_ce: 0.020382
2021-12-11 20:40:47,193 iteration 2968 : loss : 0.092447, loss_ce: 0.026425
2021-12-11 20:40:48,609 iteration 2969 : loss : 0.032502, loss_ce: 0.016933
2021-12-11 20:40:49,969 iteration 2970 : loss : 0.038968, loss_ce: 0.015877
2021-12-11 20:40:51,283 iteration 2971 : loss : 0.039803, loss_ce: 0.019025
2021-12-11 20:40:52,689 iteration 2972 : loss : 0.062053, loss_ce: 0.023698
2021-12-11 20:40:54,028 iteration 2973 : loss : 0.045141, loss_ce: 0.020855
2021-12-11 20:40:55,465 iteration 2974 : loss : 0.030885, loss_ce: 0.014524
2021-12-11 20:40:55,465 Training Data Eval:
2021-12-11 20:41:02,288   Average segmentation loss on training set: 0.0277
2021-12-11 20:41:02,288 Validation Data Eval:
2021-12-11 20:41:04,658   Average segmentation loss on validation set: 0.1297
2021-12-11 20:41:06,060 iteration 2975 : loss : 0.047661, loss_ce: 0.017085
 44%|███████████▊               | 175/400 [1:14:47<1:40:46, 26.87s/it]2021-12-11 20:41:07,547 iteration 2976 : loss : 0.047959, loss_ce: 0.025448
2021-12-11 20:41:08,861 iteration 2977 : loss : 0.039403, loss_ce: 0.018544
2021-12-11 20:41:10,160 iteration 2978 : loss : 0.034888, loss_ce: 0.015797
2021-12-11 20:41:11,567 iteration 2979 : loss : 0.028243, loss_ce: 0.013689
2021-12-11 20:41:13,070 iteration 2980 : loss : 0.058603, loss_ce: 0.029131
2021-12-11 20:41:14,466 iteration 2981 : loss : 0.055799, loss_ce: 0.020651
2021-12-11 20:41:15,918 iteration 2982 : loss : 0.043627, loss_ce: 0.021548
2021-12-11 20:41:17,290 iteration 2983 : loss : 0.069107, loss_ce: 0.036115
2021-12-11 20:41:18,727 iteration 2984 : loss : 0.037695, loss_ce: 0.018655
2021-12-11 20:41:20,135 iteration 2985 : loss : 0.054944, loss_ce: 0.018886
2021-12-11 20:41:21,571 iteration 2986 : loss : 0.050441, loss_ce: 0.021561
2021-12-11 20:41:22,902 iteration 2987 : loss : 0.066664, loss_ce: 0.032983
2021-12-11 20:41:24,260 iteration 2988 : loss : 0.035409, loss_ce: 0.015777
2021-12-11 20:41:25,660 iteration 2989 : loss : 0.036496, loss_ce: 0.014622
2021-12-11 20:41:27,067 iteration 2990 : loss : 0.087035, loss_ce: 0.037536
2021-12-11 20:41:28,528 iteration 2991 : loss : 0.037002, loss_ce: 0.017237
2021-12-11 20:41:30,047 iteration 2992 : loss : 0.053141, loss_ce: 0.024234
 44%|███████████▉               | 176/400 [1:15:11<1:37:05, 26.00s/it]2021-12-11 20:41:31,407 iteration 2993 : loss : 0.040971, loss_ce: 0.015895
2021-12-11 20:41:32,779 iteration 2994 : loss : 0.048532, loss_ce: 0.019890
2021-12-11 20:41:34,247 iteration 2995 : loss : 0.064390, loss_ce: 0.030854
2021-12-11 20:41:35,623 iteration 2996 : loss : 0.036647, loss_ce: 0.016480
2021-12-11 20:41:36,992 iteration 2997 : loss : 0.036347, loss_ce: 0.016677
2021-12-11 20:41:38,367 iteration 2998 : loss : 0.046264, loss_ce: 0.016553
2021-12-11 20:41:39,783 iteration 2999 : loss : 0.065067, loss_ce: 0.028091
2021-12-11 20:41:41,161 iteration 3000 : loss : 0.058699, loss_ce: 0.021708
2021-12-11 20:41:42,578 iteration 3001 : loss : 0.036225, loss_ce: 0.017243
2021-12-11 20:41:43,890 iteration 3002 : loss : 0.067139, loss_ce: 0.027084
2021-12-11 20:41:45,322 iteration 3003 : loss : 0.037418, loss_ce: 0.017590
2021-12-11 20:41:46,615 iteration 3004 : loss : 0.029745, loss_ce: 0.015847
2021-12-11 20:41:48,013 iteration 3005 : loss : 0.057767, loss_ce: 0.024404
2021-12-11 20:41:49,421 iteration 3006 : loss : 0.044559, loss_ce: 0.020970
2021-12-11 20:41:50,774 iteration 3007 : loss : 0.046812, loss_ce: 0.018907
2021-12-11 20:41:52,263 iteration 3008 : loss : 0.047821, loss_ce: 0.022736
2021-12-11 20:41:53,633 iteration 3009 : loss : 0.040311, loss_ce: 0.024055
 44%|███████████▉               | 177/400 [1:15:34<1:33:57, 25.28s/it]2021-12-11 20:41:55,040 iteration 3010 : loss : 0.043544, loss_ce: 0.020793
2021-12-11 20:41:56,406 iteration 3011 : loss : 0.064707, loss_ce: 0.024878
2021-12-11 20:41:57,751 iteration 3012 : loss : 0.040675, loss_ce: 0.016970
2021-12-11 20:41:59,087 iteration 3013 : loss : 0.034482, loss_ce: 0.018043
2021-12-11 20:42:00,528 iteration 3014 : loss : 0.045672, loss_ce: 0.018394
2021-12-11 20:42:01,917 iteration 3015 : loss : 0.039683, loss_ce: 0.016936
2021-12-11 20:42:03,283 iteration 3016 : loss : 0.036454, loss_ce: 0.018307
2021-12-11 20:42:04,655 iteration 3017 : loss : 0.043714, loss_ce: 0.017341
2021-12-11 20:42:05,981 iteration 3018 : loss : 0.029589, loss_ce: 0.012159
2021-12-11 20:42:07,332 iteration 3019 : loss : 0.048286, loss_ce: 0.022235
2021-12-11 20:42:08,592 iteration 3020 : loss : 0.036453, loss_ce: 0.020490
2021-12-11 20:42:09,881 iteration 3021 : loss : 0.035808, loss_ce: 0.016726
2021-12-11 20:42:11,230 iteration 3022 : loss : 0.039518, loss_ce: 0.016093
2021-12-11 20:42:12,588 iteration 3023 : loss : 0.038578, loss_ce: 0.022292
2021-12-11 20:42:13,961 iteration 3024 : loss : 0.044466, loss_ce: 0.019185
2021-12-11 20:42:15,251 iteration 3025 : loss : 0.040097, loss_ce: 0.022432
2021-12-11 20:42:16,656 iteration 3026 : loss : 0.040668, loss_ce: 0.021575
 44%|████████████               | 178/400 [1:15:57<1:31:02, 24.60s/it]2021-12-11 20:42:18,083 iteration 3027 : loss : 0.044334, loss_ce: 0.024727
2021-12-11 20:42:19,449 iteration 3028 : loss : 0.045776, loss_ce: 0.024184
2021-12-11 20:42:20,832 iteration 3029 : loss : 0.037236, loss_ce: 0.021037
2021-12-11 20:42:22,303 iteration 3030 : loss : 0.041464, loss_ce: 0.017996
2021-12-11 20:42:23,596 iteration 3031 : loss : 0.027159, loss_ce: 0.014059
2021-12-11 20:42:24,963 iteration 3032 : loss : 0.036406, loss_ce: 0.015955
2021-12-11 20:42:26,357 iteration 3033 : loss : 0.036589, loss_ce: 0.021042
2021-12-11 20:42:27,708 iteration 3034 : loss : 0.043427, loss_ce: 0.017496
2021-12-11 20:42:29,157 iteration 3035 : loss : 0.053683, loss_ce: 0.021176
2021-12-11 20:42:30,548 iteration 3036 : loss : 0.035063, loss_ce: 0.018003
2021-12-11 20:42:31,977 iteration 3037 : loss : 0.034455, loss_ce: 0.016235
2021-12-11 20:42:33,288 iteration 3038 : loss : 0.041935, loss_ce: 0.014852
2021-12-11 20:42:34,817 iteration 3039 : loss : 0.058949, loss_ce: 0.023388
2021-12-11 20:42:36,152 iteration 3040 : loss : 0.045874, loss_ce: 0.023378
2021-12-11 20:42:37,496 iteration 3041 : loss : 0.038468, loss_ce: 0.017409
2021-12-11 20:42:38,826 iteration 3042 : loss : 0.030437, loss_ce: 0.017016
2021-12-11 20:42:40,302 iteration 3043 : loss : 0.035970, loss_ce: 0.017447
 45%|████████████               | 179/400 [1:16:21<1:29:33, 24.32s/it]2021-12-11 20:42:41,753 iteration 3044 : loss : 0.059839, loss_ce: 0.026093
2021-12-11 20:42:43,112 iteration 3045 : loss : 0.037483, loss_ce: 0.019152
2021-12-11 20:42:44,464 iteration 3046 : loss : 0.052921, loss_ce: 0.021091
2021-12-11 20:42:45,955 iteration 3047 : loss : 0.058373, loss_ce: 0.024217
2021-12-11 20:42:47,395 iteration 3048 : loss : 0.056329, loss_ce: 0.031290
2021-12-11 20:42:48,745 iteration 3049 : loss : 0.049599, loss_ce: 0.022825
2021-12-11 20:42:50,213 iteration 3050 : loss : 0.041736, loss_ce: 0.022116
2021-12-11 20:42:51,633 iteration 3051 : loss : 0.047229, loss_ce: 0.016583
2021-12-11 20:42:53,097 iteration 3052 : loss : 0.053859, loss_ce: 0.025011
2021-12-11 20:42:54,503 iteration 3053 : loss : 0.037941, loss_ce: 0.019301
2021-12-11 20:42:55,905 iteration 3054 : loss : 0.071404, loss_ce: 0.028069
2021-12-11 20:42:57,319 iteration 3055 : loss : 0.041627, loss_ce: 0.019075
2021-12-11 20:42:58,693 iteration 3056 : loss : 0.055066, loss_ce: 0.021723
2021-12-11 20:43:00,029 iteration 3057 : loss : 0.025729, loss_ce: 0.013158
2021-12-11 20:43:01,478 iteration 3058 : loss : 0.081710, loss_ce: 0.029087
2021-12-11 20:43:02,938 iteration 3059 : loss : 0.045726, loss_ce: 0.021238
2021-12-11 20:43:02,939 Training Data Eval:
2021-12-11 20:43:09,780   Average segmentation loss on training set: 0.0221
2021-12-11 20:43:09,780 Validation Data Eval:
2021-12-11 20:43:12,146   Average segmentation loss on validation set: 0.0789
2021-12-11 20:43:14,243 Found new lowest validation loss at iteration 3059! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 20:43:15,637 iteration 3060 : loss : 0.042588, loss_ce: 0.023108
 45%|████████████▏              | 180/400 [1:16:56<1:41:17, 27.62s/it]2021-12-11 20:43:17,027 iteration 3061 : loss : 0.050708, loss_ce: 0.020751
2021-12-11 20:43:18,427 iteration 3062 : loss : 0.050269, loss_ce: 0.020363
2021-12-11 20:43:19,790 iteration 3063 : loss : 0.037937, loss_ce: 0.015774
2021-12-11 20:43:21,224 iteration 3064 : loss : 0.057926, loss_ce: 0.031195
2021-12-11 20:43:22,597 iteration 3065 : loss : 0.047262, loss_ce: 0.016757
2021-12-11 20:43:23,965 iteration 3066 : loss : 0.043254, loss_ce: 0.013811
2021-12-11 20:43:25,364 iteration 3067 : loss : 0.050937, loss_ce: 0.026221
2021-12-11 20:43:26,803 iteration 3068 : loss : 0.031078, loss_ce: 0.013990
2021-12-11 20:43:28,240 iteration 3069 : loss : 0.050824, loss_ce: 0.015181
2021-12-11 20:43:29,564 iteration 3070 : loss : 0.037781, loss_ce: 0.018331
2021-12-11 20:43:31,010 iteration 3071 : loss : 0.053282, loss_ce: 0.025112
2021-12-11 20:43:32,404 iteration 3072 : loss : 0.038350, loss_ce: 0.018117
2021-12-11 20:43:33,751 iteration 3073 : loss : 0.045824, loss_ce: 0.019675
2021-12-11 20:43:35,225 iteration 3074 : loss : 0.063675, loss_ce: 0.035127
2021-12-11 20:43:36,655 iteration 3075 : loss : 0.042294, loss_ce: 0.016330
2021-12-11 20:43:38,018 iteration 3076 : loss : 0.049630, loss_ce: 0.026890
2021-12-11 20:43:39,465 iteration 3077 : loss : 0.054980, loss_ce: 0.030600
 45%|████████████▏              | 181/400 [1:17:20<1:36:40, 26.49s/it]2021-12-11 20:43:40,788 iteration 3078 : loss : 0.029609, loss_ce: 0.014953
2021-12-11 20:43:42,210 iteration 3079 : loss : 0.041324, loss_ce: 0.018352
2021-12-11 20:43:43,591 iteration 3080 : loss : 0.044124, loss_ce: 0.019754
2021-12-11 20:43:44,952 iteration 3081 : loss : 0.042221, loss_ce: 0.015974
2021-12-11 20:43:46,338 iteration 3082 : loss : 0.030700, loss_ce: 0.014954
2021-12-11 20:43:47,720 iteration 3083 : loss : 0.066685, loss_ce: 0.033988
2021-12-11 20:43:49,091 iteration 3084 : loss : 0.032179, loss_ce: 0.016565
2021-12-11 20:43:50,470 iteration 3085 : loss : 0.095275, loss_ce: 0.023347
2021-12-11 20:43:51,816 iteration 3086 : loss : 0.033051, loss_ce: 0.016150
2021-12-11 20:43:53,148 iteration 3087 : loss : 0.040994, loss_ce: 0.015778
2021-12-11 20:43:54,476 iteration 3088 : loss : 0.037713, loss_ce: 0.017125
2021-12-11 20:43:55,806 iteration 3089 : loss : 0.035621, loss_ce: 0.016908
2021-12-11 20:43:57,197 iteration 3090 : loss : 0.042272, loss_ce: 0.021044
2021-12-11 20:43:58,535 iteration 3091 : loss : 0.046353, loss_ce: 0.021368
2021-12-11 20:43:59,832 iteration 3092 : loss : 0.043307, loss_ce: 0.023529
2021-12-11 20:44:01,199 iteration 3093 : loss : 0.033883, loss_ce: 0.015313
2021-12-11 20:44:02,533 iteration 3094 : loss : 0.065375, loss_ce: 0.038675
 46%|████████████▎              | 182/400 [1:17:43<1:32:29, 25.46s/it]2021-12-11 20:44:04,006 iteration 3095 : loss : 0.046824, loss_ce: 0.020468
2021-12-11 20:44:05,359 iteration 3096 : loss : 0.039451, loss_ce: 0.018413
2021-12-11 20:44:06,716 iteration 3097 : loss : 0.044950, loss_ce: 0.020635
2021-12-11 20:44:08,042 iteration 3098 : loss : 0.034425, loss_ce: 0.016046
2021-12-11 20:44:09,475 iteration 3099 : loss : 0.047212, loss_ce: 0.018568
2021-12-11 20:44:10,807 iteration 3100 : loss : 0.032824, loss_ce: 0.015423
2021-12-11 20:44:12,190 iteration 3101 : loss : 0.033599, loss_ce: 0.018387
2021-12-11 20:44:13,606 iteration 3102 : loss : 0.038265, loss_ce: 0.016139
2021-12-11 20:44:14,965 iteration 3103 : loss : 0.038523, loss_ce: 0.015008
2021-12-11 20:44:16,348 iteration 3104 : loss : 0.035674, loss_ce: 0.015949
2021-12-11 20:44:17,702 iteration 3105 : loss : 0.033581, loss_ce: 0.014706
2021-12-11 20:44:19,214 iteration 3106 : loss : 0.045757, loss_ce: 0.020996
2021-12-11 20:44:20,587 iteration 3107 : loss : 0.043990, loss_ce: 0.021115
2021-12-11 20:44:21,897 iteration 3108 : loss : 0.045214, loss_ce: 0.021624
2021-12-11 20:44:23,327 iteration 3109 : loss : 0.048601, loss_ce: 0.027049
2021-12-11 20:44:24,758 iteration 3110 : loss : 0.066252, loss_ce: 0.031536
2021-12-11 20:44:26,096 iteration 3111 : loss : 0.052727, loss_ce: 0.018917
 46%|████████████▎              | 183/400 [1:18:07<1:30:00, 24.89s/it]2021-12-11 20:44:27,480 iteration 3112 : loss : 0.033043, loss_ce: 0.015791
2021-12-11 20:44:28,855 iteration 3113 : loss : 0.041742, loss_ce: 0.020895
2021-12-11 20:44:30,344 iteration 3114 : loss : 0.124503, loss_ce: 0.034403
2021-12-11 20:44:31,770 iteration 3115 : loss : 0.042266, loss_ce: 0.021542
2021-12-11 20:44:33,172 iteration 3116 : loss : 0.041581, loss_ce: 0.020652
2021-12-11 20:44:34,539 iteration 3117 : loss : 0.049169, loss_ce: 0.027572
2021-12-11 20:44:35,968 iteration 3118 : loss : 0.045463, loss_ce: 0.024113
2021-12-11 20:44:37,384 iteration 3119 : loss : 0.047657, loss_ce: 0.020846
2021-12-11 20:44:38,780 iteration 3120 : loss : 0.031354, loss_ce: 0.014401
2021-12-11 20:44:40,191 iteration 3121 : loss : 0.051191, loss_ce: 0.018937
2021-12-11 20:44:41,638 iteration 3122 : loss : 0.054818, loss_ce: 0.019220
2021-12-11 20:44:43,029 iteration 3123 : loss : 0.038030, loss_ce: 0.019802
2021-12-11 20:44:44,429 iteration 3124 : loss : 0.042822, loss_ce: 0.020132
2021-12-11 20:44:45,829 iteration 3125 : loss : 0.057291, loss_ce: 0.026729
2021-12-11 20:44:47,158 iteration 3126 : loss : 0.038055, loss_ce: 0.017604
2021-12-11 20:44:48,558 iteration 3127 : loss : 0.047604, loss_ce: 0.019259
2021-12-11 20:44:49,870 iteration 3128 : loss : 0.030265, loss_ce: 0.014567
 46%|████████████▍              | 184/400 [1:18:31<1:28:23, 24.55s/it]2021-12-11 20:44:51,316 iteration 3129 : loss : 0.041710, loss_ce: 0.018834
2021-12-11 20:44:52,727 iteration 3130 : loss : 0.052802, loss_ce: 0.021990
2021-12-11 20:44:54,180 iteration 3131 : loss : 0.058866, loss_ce: 0.026661
2021-12-11 20:44:55,539 iteration 3132 : loss : 0.034821, loss_ce: 0.018691
2021-12-11 20:44:56,854 iteration 3133 : loss : 0.035338, loss_ce: 0.017071
2021-12-11 20:44:58,234 iteration 3134 : loss : 0.036236, loss_ce: 0.015280
2021-12-11 20:44:59,611 iteration 3135 : loss : 0.037189, loss_ce: 0.016004
2021-12-11 20:45:01,043 iteration 3136 : loss : 0.051315, loss_ce: 0.021160
2021-12-11 20:45:02,437 iteration 3137 : loss : 0.048110, loss_ce: 0.023799
2021-12-11 20:45:03,859 iteration 3138 : loss : 0.043065, loss_ce: 0.023375
2021-12-11 20:45:05,309 iteration 3139 : loss : 0.073742, loss_ce: 0.030070
2021-12-11 20:45:06,737 iteration 3140 : loss : 0.048818, loss_ce: 0.022397
2021-12-11 20:45:08,175 iteration 3141 : loss : 0.049717, loss_ce: 0.020672
2021-12-11 20:45:09,561 iteration 3142 : loss : 0.041306, loss_ce: 0.017242
2021-12-11 20:45:11,001 iteration 3143 : loss : 0.047280, loss_ce: 0.024023
2021-12-11 20:45:12,466 iteration 3144 : loss : 0.043053, loss_ce: 0.020424
2021-12-11 20:45:12,466 Training Data Eval:
2021-12-11 20:45:19,278   Average segmentation loss on training set: 0.0229
2021-12-11 20:45:19,278 Validation Data Eval:
2021-12-11 20:45:21,643   Average segmentation loss on validation set: 0.0914
2021-12-11 20:45:22,919 iteration 3145 : loss : 0.041858, loss_ce: 0.018406
 46%|████████████▍              | 185/400 [1:19:04<1:37:07, 27.10s/it]2021-12-11 20:45:24,243 iteration 3146 : loss : 0.045500, loss_ce: 0.018624
2021-12-11 20:45:25,592 iteration 3147 : loss : 0.052409, loss_ce: 0.027581
2021-12-11 20:45:26,949 iteration 3148 : loss : 0.034498, loss_ce: 0.014144
2021-12-11 20:45:28,295 iteration 3149 : loss : 0.029156, loss_ce: 0.014397
2021-12-11 20:45:29,662 iteration 3150 : loss : 0.058843, loss_ce: 0.027079
2021-12-11 20:45:30,998 iteration 3151 : loss : 0.052321, loss_ce: 0.017394
2021-12-11 20:45:32,350 iteration 3152 : loss : 0.040756, loss_ce: 0.020058
2021-12-11 20:45:33,736 iteration 3153 : loss : 0.044308, loss_ce: 0.019327
2021-12-11 20:45:35,078 iteration 3154 : loss : 0.047623, loss_ce: 0.022943
2021-12-11 20:45:36,427 iteration 3155 : loss : 0.042784, loss_ce: 0.019491
2021-12-11 20:45:37,855 iteration 3156 : loss : 0.055796, loss_ce: 0.025136
2021-12-11 20:45:39,268 iteration 3157 : loss : 0.039402, loss_ce: 0.019194
2021-12-11 20:45:40,626 iteration 3158 : loss : 0.047580, loss_ce: 0.025217
2021-12-11 20:45:41,994 iteration 3159 : loss : 0.036998, loss_ce: 0.017169
2021-12-11 20:45:43,372 iteration 3160 : loss : 0.042325, loss_ce: 0.019842
2021-12-11 20:45:44,703 iteration 3161 : loss : 0.047362, loss_ce: 0.019140
2021-12-11 20:45:46,149 iteration 3162 : loss : 0.034377, loss_ce: 0.015796
 46%|████████████▌              | 186/400 [1:19:27<1:32:31, 25.94s/it]2021-12-11 20:45:47,627 iteration 3163 : loss : 0.056130, loss_ce: 0.026963
2021-12-11 20:45:49,042 iteration 3164 : loss : 0.040716, loss_ce: 0.018608
2021-12-11 20:45:50,407 iteration 3165 : loss : 0.030377, loss_ce: 0.013624
2021-12-11 20:45:51,849 iteration 3166 : loss : 0.031973, loss_ce: 0.014534
2021-12-11 20:45:53,221 iteration 3167 : loss : 0.027712, loss_ce: 0.014995
2021-12-11 20:45:54,721 iteration 3168 : loss : 0.064270, loss_ce: 0.032022
2021-12-11 20:45:56,197 iteration 3169 : loss : 0.049535, loss_ce: 0.025519
2021-12-11 20:45:57,565 iteration 3170 : loss : 0.036853, loss_ce: 0.020021
2021-12-11 20:45:58,877 iteration 3171 : loss : 0.033238, loss_ce: 0.018771
2021-12-11 20:46:00,275 iteration 3172 : loss : 0.044555, loss_ce: 0.018982
2021-12-11 20:46:01,704 iteration 3173 : loss : 0.033212, loss_ce: 0.015862
2021-12-11 20:46:03,090 iteration 3174 : loss : 0.044484, loss_ce: 0.021637
2021-12-11 20:46:04,566 iteration 3175 : loss : 0.064826, loss_ce: 0.022464
2021-12-11 20:46:05,948 iteration 3176 : loss : 0.044166, loss_ce: 0.017526
2021-12-11 20:46:07,380 iteration 3177 : loss : 0.061199, loss_ce: 0.021568
2021-12-11 20:46:08,704 iteration 3178 : loss : 0.035239, loss_ce: 0.015652
2021-12-11 20:46:10,066 iteration 3179 : loss : 0.051195, loss_ce: 0.020895
 47%|████████████▌              | 187/400 [1:19:51<1:29:56, 25.33s/it]2021-12-11 20:46:11,538 iteration 3180 : loss : 0.059661, loss_ce: 0.029397
2021-12-11 20:46:12,885 iteration 3181 : loss : 0.038677, loss_ce: 0.017389
2021-12-11 20:46:14,281 iteration 3182 : loss : 0.039372, loss_ce: 0.016188
2021-12-11 20:46:15,642 iteration 3183 : loss : 0.045828, loss_ce: 0.016949
2021-12-11 20:46:17,033 iteration 3184 : loss : 0.035527, loss_ce: 0.016779
2021-12-11 20:46:18,451 iteration 3185 : loss : 0.043955, loss_ce: 0.019468
2021-12-11 20:46:19,766 iteration 3186 : loss : 0.046738, loss_ce: 0.024541
2021-12-11 20:46:21,156 iteration 3187 : loss : 0.028701, loss_ce: 0.014459
2021-12-11 20:46:22,519 iteration 3188 : loss : 0.031656, loss_ce: 0.017036
2021-12-11 20:46:23,915 iteration 3189 : loss : 0.044108, loss_ce: 0.022033
2021-12-11 20:46:25,296 iteration 3190 : loss : 0.041114, loss_ce: 0.018798
2021-12-11 20:46:26,796 iteration 3191 : loss : 0.052213, loss_ce: 0.019218
2021-12-11 20:46:28,142 iteration 3192 : loss : 0.056200, loss_ce: 0.021231
2021-12-11 20:46:29,580 iteration 3193 : loss : 0.052320, loss_ce: 0.027298
2021-12-11 20:46:31,001 iteration 3194 : loss : 0.048950, loss_ce: 0.018501
2021-12-11 20:46:32,408 iteration 3195 : loss : 0.053318, loss_ce: 0.030104
2021-12-11 20:46:33,788 iteration 3196 : loss : 0.042801, loss_ce: 0.023661
 47%|████████████▋              | 188/400 [1:20:15<1:27:48, 24.85s/it]2021-12-11 20:46:35,148 iteration 3197 : loss : 0.032848, loss_ce: 0.016366
2021-12-11 20:46:36,511 iteration 3198 : loss : 0.046813, loss_ce: 0.024277
2021-12-11 20:46:37,894 iteration 3199 : loss : 0.040650, loss_ce: 0.018639
2021-12-11 20:46:39,362 iteration 3200 : loss : 0.042451, loss_ce: 0.021154
2021-12-11 20:46:40,779 iteration 3201 : loss : 0.064634, loss_ce: 0.019748
2021-12-11 20:46:42,168 iteration 3202 : loss : 0.041364, loss_ce: 0.017408
2021-12-11 20:46:43,656 iteration 3203 : loss : 0.036275, loss_ce: 0.019096
2021-12-11 20:46:45,030 iteration 3204 : loss : 0.034601, loss_ce: 0.014464
2021-12-11 20:46:46,472 iteration 3205 : loss : 0.049155, loss_ce: 0.018425
2021-12-11 20:46:47,873 iteration 3206 : loss : 0.036759, loss_ce: 0.019152
2021-12-11 20:46:49,349 iteration 3207 : loss : 0.067070, loss_ce: 0.030954
2021-12-11 20:46:50,689 iteration 3208 : loss : 0.036706, loss_ce: 0.014697
2021-12-11 20:46:52,025 iteration 3209 : loss : 0.044426, loss_ce: 0.026237
2021-12-11 20:46:53,311 iteration 3210 : loss : 0.048410, loss_ce: 0.024404
2021-12-11 20:46:54,681 iteration 3211 : loss : 0.044189, loss_ce: 0.016264
2021-12-11 20:46:56,032 iteration 3212 : loss : 0.040942, loss_ce: 0.019026
2021-12-11 20:46:57,415 iteration 3213 : loss : 0.042217, loss_ce: 0.016997
 47%|████████████▊              | 189/400 [1:20:38<1:26:06, 24.48s/it]2021-12-11 20:46:58,839 iteration 3214 : loss : 0.037377, loss_ce: 0.021336
2021-12-11 20:47:00,309 iteration 3215 : loss : 0.046023, loss_ce: 0.020665
2021-12-11 20:47:01,742 iteration 3216 : loss : 0.039043, loss_ce: 0.017462
2021-12-11 20:47:03,192 iteration 3217 : loss : 0.099468, loss_ce: 0.025300
2021-12-11 20:47:04,591 iteration 3218 : loss : 0.041610, loss_ce: 0.018988
2021-12-11 20:47:05,950 iteration 3219 : loss : 0.041773, loss_ce: 0.020029
2021-12-11 20:47:07,373 iteration 3220 : loss : 0.045811, loss_ce: 0.020422
2021-12-11 20:47:08,719 iteration 3221 : loss : 0.038909, loss_ce: 0.020524
2021-12-11 20:47:10,105 iteration 3222 : loss : 0.049240, loss_ce: 0.031141
2021-12-11 20:47:11,508 iteration 3223 : loss : 0.035949, loss_ce: 0.013829
2021-12-11 20:47:12,935 iteration 3224 : loss : 0.040339, loss_ce: 0.015561
2021-12-11 20:47:14,303 iteration 3225 : loss : 0.048732, loss_ce: 0.020457
2021-12-11 20:47:15,771 iteration 3226 : loss : 0.046119, loss_ce: 0.019821
2021-12-11 20:47:17,181 iteration 3227 : loss : 0.041320, loss_ce: 0.020424
2021-12-11 20:47:18,500 iteration 3228 : loss : 0.031320, loss_ce: 0.016994
2021-12-11 20:47:19,959 iteration 3229 : loss : 0.041400, loss_ce: 0.020796
2021-12-11 20:47:19,959 Training Data Eval:
2021-12-11 20:47:26,791   Average segmentation loss on training set: 0.0228
2021-12-11 20:47:26,791 Validation Data Eval:
2021-12-11 20:47:29,147   Average segmentation loss on validation set: 0.0908
2021-12-11 20:47:30,614 iteration 3230 : loss : 0.042240, loss_ce: 0.018551
 48%|████████████▊              | 190/400 [1:21:11<1:34:50, 27.10s/it]2021-12-11 20:47:32,014 iteration 3231 : loss : 0.039692, loss_ce: 0.017501
2021-12-11 20:47:33,397 iteration 3232 : loss : 0.045888, loss_ce: 0.022456
2021-12-11 20:47:34,769 iteration 3233 : loss : 0.046263, loss_ce: 0.018032
2021-12-11 20:47:36,103 iteration 3234 : loss : 0.036203, loss_ce: 0.016254
2021-12-11 20:47:37,444 iteration 3235 : loss : 0.035889, loss_ce: 0.014876
2021-12-11 20:47:38,857 iteration 3236 : loss : 0.038749, loss_ce: 0.018711
2021-12-11 20:47:40,280 iteration 3237 : loss : 0.031647, loss_ce: 0.016766
2021-12-11 20:47:41,673 iteration 3238 : loss : 0.042442, loss_ce: 0.016726
2021-12-11 20:47:43,041 iteration 3239 : loss : 0.031312, loss_ce: 0.015385
2021-12-11 20:47:44,432 iteration 3240 : loss : 0.046046, loss_ce: 0.019336
2021-12-11 20:47:45,853 iteration 3241 : loss : 0.037158, loss_ce: 0.018035
2021-12-11 20:47:47,320 iteration 3242 : loss : 0.044178, loss_ce: 0.022338
2021-12-11 20:47:48,666 iteration 3243 : loss : 0.033938, loss_ce: 0.018972
2021-12-11 20:47:50,055 iteration 3244 : loss : 0.059512, loss_ce: 0.018956
2021-12-11 20:47:51,408 iteration 3245 : loss : 0.035517, loss_ce: 0.017537
2021-12-11 20:47:52,845 iteration 3246 : loss : 0.040182, loss_ce: 0.019817
2021-12-11 20:47:54,261 iteration 3247 : loss : 0.042825, loss_ce: 0.018152
 48%|████████████▉              | 191/400 [1:21:35<1:30:47, 26.06s/it]2021-12-11 20:47:55,840 iteration 3248 : loss : 0.050854, loss_ce: 0.025967
2021-12-11 20:47:57,162 iteration 3249 : loss : 0.030693, loss_ce: 0.012145
2021-12-11 20:47:58,576 iteration 3250 : loss : 0.044939, loss_ce: 0.018776
2021-12-11 20:47:59,970 iteration 3251 : loss : 0.038687, loss_ce: 0.013025
2021-12-11 20:48:01,304 iteration 3252 : loss : 0.035424, loss_ce: 0.017804
2021-12-11 20:48:02,711 iteration 3253 : loss : 0.043097, loss_ce: 0.022215
2021-12-11 20:48:04,098 iteration 3254 : loss : 0.045895, loss_ce: 0.018262
2021-12-11 20:48:05,526 iteration 3255 : loss : 0.039568, loss_ce: 0.018273
2021-12-11 20:48:06,867 iteration 3256 : loss : 0.039096, loss_ce: 0.023647
2021-12-11 20:48:08,238 iteration 3257 : loss : 0.050239, loss_ce: 0.019152
2021-12-11 20:48:09,686 iteration 3258 : loss : 0.039142, loss_ce: 0.019327
2021-12-11 20:48:11,073 iteration 3259 : loss : 0.034243, loss_ce: 0.016311
2021-12-11 20:48:12,427 iteration 3260 : loss : 0.037454, loss_ce: 0.017574
2021-12-11 20:48:13,860 iteration 3261 : loss : 0.055180, loss_ce: 0.024966
2021-12-11 20:48:15,230 iteration 3262 : loss : 0.053459, loss_ce: 0.025117
2021-12-11 20:48:16,595 iteration 3263 : loss : 0.050195, loss_ce: 0.021524
2021-12-11 20:48:17,957 iteration 3264 : loss : 0.043386, loss_ce: 0.019161
 48%|████████████▉              | 192/400 [1:21:59<1:27:53, 25.35s/it]2021-12-11 20:48:19,276 iteration 3265 : loss : 0.026025, loss_ce: 0.013195
2021-12-11 20:48:20,688 iteration 3266 : loss : 0.033025, loss_ce: 0.016928
2021-12-11 20:48:22,027 iteration 3267 : loss : 0.038402, loss_ce: 0.018343
2021-12-11 20:48:23,509 iteration 3268 : loss : 0.047014, loss_ce: 0.023013
2021-12-11 20:48:24,838 iteration 3269 : loss : 0.039497, loss_ce: 0.019125
2021-12-11 20:48:26,169 iteration 3270 : loss : 0.030996, loss_ce: 0.015272
2021-12-11 20:48:27,609 iteration 3271 : loss : 0.042918, loss_ce: 0.017275
2021-12-11 20:48:28,966 iteration 3272 : loss : 0.030807, loss_ce: 0.014688
2021-12-11 20:48:30,298 iteration 3273 : loss : 0.063862, loss_ce: 0.024924
2021-12-11 20:48:31,710 iteration 3274 : loss : 0.038628, loss_ce: 0.017527
2021-12-11 20:48:33,033 iteration 3275 : loss : 0.029899, loss_ce: 0.014683
2021-12-11 20:48:34,398 iteration 3276 : loss : 0.033979, loss_ce: 0.015711
2021-12-11 20:48:35,837 iteration 3277 : loss : 0.034153, loss_ce: 0.016833
2021-12-11 20:48:37,202 iteration 3278 : loss : 0.042513, loss_ce: 0.018516
2021-12-11 20:48:38,628 iteration 3279 : loss : 0.066056, loss_ce: 0.028318
2021-12-11 20:48:40,008 iteration 3280 : loss : 0.045308, loss_ce: 0.023969
2021-12-11 20:48:41,393 iteration 3281 : loss : 0.032975, loss_ce: 0.014141
 48%|█████████████              | 193/400 [1:22:22<1:25:28, 24.78s/it]2021-12-11 20:48:42,774 iteration 3282 : loss : 0.036576, loss_ce: 0.018107
2021-12-11 20:48:44,254 iteration 3283 : loss : 0.048236, loss_ce: 0.020962
2021-12-11 20:48:45,608 iteration 3284 : loss : 0.034010, loss_ce: 0.019327
2021-12-11 20:48:46,898 iteration 3285 : loss : 0.026435, loss_ce: 0.013107
2021-12-11 20:48:48,329 iteration 3286 : loss : 0.049678, loss_ce: 0.016737
2021-12-11 20:48:49,739 iteration 3287 : loss : 0.057952, loss_ce: 0.026530
2021-12-11 20:48:51,124 iteration 3288 : loss : 0.044606, loss_ce: 0.020673
2021-12-11 20:48:52,521 iteration 3289 : loss : 0.044977, loss_ce: 0.015934
2021-12-11 20:48:53,855 iteration 3290 : loss : 0.027949, loss_ce: 0.011738
2021-12-11 20:48:55,283 iteration 3291 : loss : 0.041429, loss_ce: 0.016908
2021-12-11 20:48:56,641 iteration 3292 : loss : 0.042557, loss_ce: 0.020985
2021-12-11 20:48:57,964 iteration 3293 : loss : 0.027900, loss_ce: 0.014369
2021-12-11 20:48:59,300 iteration 3294 : loss : 0.027815, loss_ce: 0.014886
2021-12-11 20:49:00,674 iteration 3295 : loss : 0.032185, loss_ce: 0.017050
2021-12-11 20:49:02,072 iteration 3296 : loss : 0.045992, loss_ce: 0.019633
2021-12-11 20:49:03,428 iteration 3297 : loss : 0.057605, loss_ce: 0.031188
2021-12-11 20:49:04,737 iteration 3298 : loss : 0.031759, loss_ce: 0.015046
 48%|█████████████              | 194/400 [1:22:46<1:23:35, 24.35s/it]2021-12-11 20:49:06,112 iteration 3299 : loss : 0.030969, loss_ce: 0.015203
2021-12-11 20:49:07,467 iteration 3300 : loss : 0.048813, loss_ce: 0.023749
2021-12-11 20:49:08,889 iteration 3301 : loss : 0.046223, loss_ce: 0.019058
2021-12-11 20:49:10,205 iteration 3302 : loss : 0.041278, loss_ce: 0.017908
2021-12-11 20:49:11,631 iteration 3303 : loss : 0.041038, loss_ce: 0.018957
2021-12-11 20:49:13,034 iteration 3304 : loss : 0.034106, loss_ce: 0.020644
2021-12-11 20:49:14,447 iteration 3305 : loss : 0.039749, loss_ce: 0.017228
2021-12-11 20:49:15,798 iteration 3306 : loss : 0.034457, loss_ce: 0.016377
2021-12-11 20:49:17,256 iteration 3307 : loss : 0.043587, loss_ce: 0.020885
2021-12-11 20:49:18,702 iteration 3308 : loss : 0.047767, loss_ce: 0.020668
2021-12-11 20:49:20,100 iteration 3309 : loss : 0.032300, loss_ce: 0.016417
2021-12-11 20:49:21,551 iteration 3310 : loss : 0.050461, loss_ce: 0.018037
2021-12-11 20:49:22,923 iteration 3311 : loss : 0.044505, loss_ce: 0.016851
2021-12-11 20:49:24,252 iteration 3312 : loss : 0.027242, loss_ce: 0.015446
2021-12-11 20:49:25,671 iteration 3313 : loss : 0.049824, loss_ce: 0.021785
2021-12-11 20:49:26,944 iteration 3314 : loss : 0.031228, loss_ce: 0.016847
2021-12-11 20:49:26,944 Training Data Eval:
2021-12-11 20:49:33,780   Average segmentation loss on training set: 0.0228
2021-12-11 20:49:33,781 Validation Data Eval:
2021-12-11 20:49:36,147   Average segmentation loss on validation set: 0.1117
2021-12-11 20:49:37,479 iteration 3315 : loss : 0.036675, loss_ce: 0.019155
 49%|█████████████▏             | 195/400 [1:23:18<1:31:47, 26.87s/it]2021-12-11 20:49:38,882 iteration 3316 : loss : 0.030288, loss_ce: 0.012000
2021-12-11 20:49:40,281 iteration 3317 : loss : 0.050425, loss_ce: 0.020553
2021-12-11 20:49:41,674 iteration 3318 : loss : 0.039862, loss_ce: 0.020083
2021-12-11 20:49:43,001 iteration 3319 : loss : 0.031918, loss_ce: 0.016271
2021-12-11 20:49:44,473 iteration 3320 : loss : 0.057368, loss_ce: 0.026000
2021-12-11 20:49:45,787 iteration 3321 : loss : 0.031300, loss_ce: 0.017838
2021-12-11 20:49:47,080 iteration 3322 : loss : 0.030420, loss_ce: 0.014565
2021-12-11 20:49:48,466 iteration 3323 : loss : 0.041776, loss_ce: 0.020786
2021-12-11 20:49:49,844 iteration 3324 : loss : 0.037927, loss_ce: 0.016799
2021-12-11 20:49:51,284 iteration 3325 : loss : 0.044343, loss_ce: 0.018387
2021-12-11 20:49:52,675 iteration 3326 : loss : 0.033273, loss_ce: 0.015616
2021-12-11 20:49:54,197 iteration 3327 : loss : 0.056906, loss_ce: 0.022642
2021-12-11 20:49:55,540 iteration 3328 : loss : 0.039064, loss_ce: 0.022391
2021-12-11 20:49:56,890 iteration 3329 : loss : 0.043788, loss_ce: 0.017779
2021-12-11 20:49:58,279 iteration 3330 : loss : 0.041459, loss_ce: 0.025091
2021-12-11 20:49:59,571 iteration 3331 : loss : 0.030075, loss_ce: 0.014718
2021-12-11 20:50:00,856 iteration 3332 : loss : 0.040742, loss_ce: 0.016068
 49%|█████████████▏             | 196/400 [1:23:42<1:27:47, 25.82s/it]2021-12-11 20:50:02,264 iteration 3333 : loss : 0.034592, loss_ce: 0.016119
2021-12-11 20:50:03,660 iteration 3334 : loss : 0.033371, loss_ce: 0.015976
2021-12-11 20:50:05,098 iteration 3335 : loss : 0.048644, loss_ce: 0.023241
2021-12-11 20:50:06,429 iteration 3336 : loss : 0.028638, loss_ce: 0.013851
2021-12-11 20:50:07,762 iteration 3337 : loss : 0.034911, loss_ce: 0.015481
2021-12-11 20:50:09,176 iteration 3338 : loss : 0.039130, loss_ce: 0.016658
2021-12-11 20:50:10,605 iteration 3339 : loss : 0.056401, loss_ce: 0.021234
2021-12-11 20:50:11,991 iteration 3340 : loss : 0.061228, loss_ce: 0.028448
2021-12-11 20:50:13,331 iteration 3341 : loss : 0.034316, loss_ce: 0.017444
2021-12-11 20:50:14,675 iteration 3342 : loss : 0.040778, loss_ce: 0.025452
2021-12-11 20:50:16,040 iteration 3343 : loss : 0.045791, loss_ce: 0.021723
2021-12-11 20:50:17,470 iteration 3344 : loss : 0.034745, loss_ce: 0.013531
2021-12-11 20:50:18,902 iteration 3345 : loss : 0.038917, loss_ce: 0.020239
2021-12-11 20:50:20,283 iteration 3346 : loss : 0.057875, loss_ce: 0.020957
2021-12-11 20:50:21,643 iteration 3347 : loss : 0.030870, loss_ce: 0.014111
2021-12-11 20:50:23,097 iteration 3348 : loss : 0.043486, loss_ce: 0.019879
2021-12-11 20:50:24,437 iteration 3349 : loss : 0.052713, loss_ce: 0.018056
 49%|█████████████▎             | 197/400 [1:24:05<1:25:05, 25.15s/it]2021-12-11 20:50:25,808 iteration 3350 : loss : 0.035415, loss_ce: 0.019360
2021-12-11 20:50:27,231 iteration 3351 : loss : 0.030935, loss_ce: 0.014753
2021-12-11 20:50:28,577 iteration 3352 : loss : 0.038533, loss_ce: 0.016374
2021-12-11 20:50:29,956 iteration 3353 : loss : 0.042474, loss_ce: 0.016168
2021-12-11 20:50:31,303 iteration 3354 : loss : 0.036805, loss_ce: 0.014214
2021-12-11 20:50:32,688 iteration 3355 : loss : 0.030316, loss_ce: 0.015026
2021-12-11 20:50:34,019 iteration 3356 : loss : 0.043879, loss_ce: 0.021158
2021-12-11 20:50:35,347 iteration 3357 : loss : 0.025671, loss_ce: 0.013653
2021-12-11 20:50:36,714 iteration 3358 : loss : 0.034626, loss_ce: 0.014753
2021-12-11 20:50:38,074 iteration 3359 : loss : 0.037387, loss_ce: 0.019000
2021-12-11 20:50:39,492 iteration 3360 : loss : 0.031873, loss_ce: 0.014610
2021-12-11 20:50:40,875 iteration 3361 : loss : 0.036083, loss_ce: 0.017672
2021-12-11 20:50:42,214 iteration 3362 : loss : 0.040795, loss_ce: 0.016217
2021-12-11 20:50:43,495 iteration 3363 : loss : 0.037709, loss_ce: 0.015695
2021-12-11 20:50:44,923 iteration 3364 : loss : 0.034568, loss_ce: 0.016799
2021-12-11 20:50:46,285 iteration 3365 : loss : 0.029987, loss_ce: 0.015155
2021-12-11 20:50:47,657 iteration 3366 : loss : 0.033347, loss_ce: 0.016623
 50%|█████████████▎             | 198/400 [1:24:28<1:22:43, 24.57s/it]2021-12-11 20:50:49,177 iteration 3367 : loss : 0.053833, loss_ce: 0.023150
2021-12-11 20:50:50,538 iteration 3368 : loss : 0.046237, loss_ce: 0.023959
2021-12-11 20:50:51,919 iteration 3369 : loss : 0.032831, loss_ce: 0.019555
2021-12-11 20:50:53,235 iteration 3370 : loss : 0.048392, loss_ce: 0.015322
2021-12-11 20:50:54,598 iteration 3371 : loss : 0.027482, loss_ce: 0.013869
2021-12-11 20:50:56,023 iteration 3372 : loss : 0.058658, loss_ce: 0.021085
2021-12-11 20:50:57,416 iteration 3373 : loss : 0.032635, loss_ce: 0.017472
2021-12-11 20:50:58,868 iteration 3374 : loss : 0.034183, loss_ce: 0.017185
2021-12-11 20:51:00,354 iteration 3375 : loss : 0.065224, loss_ce: 0.022452
2021-12-11 20:51:01,673 iteration 3376 : loss : 0.027789, loss_ce: 0.013028
2021-12-11 20:51:03,117 iteration 3377 : loss : 0.032654, loss_ce: 0.015590
2021-12-11 20:51:04,462 iteration 3378 : loss : 0.038773, loss_ce: 0.015312
2021-12-11 20:51:05,867 iteration 3379 : loss : 0.036609, loss_ce: 0.017147
2021-12-11 20:51:07,196 iteration 3380 : loss : 0.028730, loss_ce: 0.012747
2021-12-11 20:51:08,593 iteration 3381 : loss : 0.046969, loss_ce: 0.028111
2021-12-11 20:51:10,004 iteration 3382 : loss : 0.041686, loss_ce: 0.016315
2021-12-11 20:51:11,373 iteration 3383 : loss : 0.038122, loss_ce: 0.020018
 50%|█████████████▍             | 199/400 [1:24:52<1:21:26, 24.31s/it]2021-12-11 20:51:12,794 iteration 3384 : loss : 0.030366, loss_ce: 0.014735
2021-12-11 20:51:14,143 iteration 3385 : loss : 0.044324, loss_ce: 0.020158
2021-12-11 20:51:15,479 iteration 3386 : loss : 0.037614, loss_ce: 0.015037
2021-12-11 20:51:16,811 iteration 3387 : loss : 0.042084, loss_ce: 0.017529
2021-12-11 20:51:18,231 iteration 3388 : loss : 0.042396, loss_ce: 0.019548
2021-12-11 20:51:19,560 iteration 3389 : loss : 0.037525, loss_ce: 0.015952
2021-12-11 20:51:20,934 iteration 3390 : loss : 0.040933, loss_ce: 0.020384
2021-12-11 20:51:22,398 iteration 3391 : loss : 0.059982, loss_ce: 0.033299
2021-12-11 20:51:23,821 iteration 3392 : loss : 0.044675, loss_ce: 0.019759
2021-12-11 20:51:25,130 iteration 3393 : loss : 0.032577, loss_ce: 0.016199
2021-12-11 20:51:26,487 iteration 3394 : loss : 0.048476, loss_ce: 0.019422
2021-12-11 20:51:27,877 iteration 3395 : loss : 0.041538, loss_ce: 0.019530
2021-12-11 20:51:29,259 iteration 3396 : loss : 0.034534, loss_ce: 0.016028
2021-12-11 20:51:30,577 iteration 3397 : loss : 0.036201, loss_ce: 0.016621
2021-12-11 20:51:32,077 iteration 3398 : loss : 0.045809, loss_ce: 0.018306
2021-12-11 20:51:33,472 iteration 3399 : loss : 0.043947, loss_ce: 0.021888
2021-12-11 20:51:33,472 Training Data Eval:
2021-12-11 20:51:40,293   Average segmentation loss on training set: 0.0212
2021-12-11 20:51:40,294 Validation Data Eval:
2021-12-11 20:51:42,648   Average segmentation loss on validation set: 0.0907
2021-12-11 20:51:44,033 iteration 3400 : loss : 0.044489, loss_ce: 0.016406
2021-12-11 20:51:45,995 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed100epoch_199.pth
 50%|█████████████▌             | 200/400 [1:25:27<1:31:18, 27.39s/it]2021-12-11 20:51:47,439 iteration 3401 : loss : 0.051060, loss_ce: 0.021995
2021-12-11 20:51:48,837 iteration 3402 : loss : 0.040907, loss_ce: 0.018815
2021-12-11 20:51:50,294 iteration 3403 : loss : 0.061264, loss_ce: 0.026375
2021-12-11 20:51:51,633 iteration 3404 : loss : 0.043311, loss_ce: 0.016942
2021-12-11 20:51:52,992 iteration 3405 : loss : 0.041891, loss_ce: 0.014670
2021-12-11 20:51:54,299 iteration 3406 : loss : 0.036035, loss_ce: 0.020577
2021-12-11 20:51:55,636 iteration 3407 : loss : 0.034675, loss_ce: 0.016376
2021-12-11 20:51:57,006 iteration 3408 : loss : 0.035363, loss_ce: 0.015557
2021-12-11 20:51:58,373 iteration 3409 : loss : 0.064255, loss_ce: 0.017286
2021-12-11 20:51:59,738 iteration 3410 : loss : 0.031167, loss_ce: 0.014406
2021-12-11 20:52:01,145 iteration 3411 : loss : 0.061944, loss_ce: 0.021323
2021-12-11 20:52:02,583 iteration 3412 : loss : 0.032132, loss_ce: 0.016947
2021-12-11 20:52:04,047 iteration 3413 : loss : 0.046065, loss_ce: 0.022993
2021-12-11 20:52:05,481 iteration 3414 : loss : 0.078688, loss_ce: 0.022330
2021-12-11 20:52:06,773 iteration 3415 : loss : 0.031115, loss_ce: 0.015221
2021-12-11 20:52:08,174 iteration 3416 : loss : 0.039219, loss_ce: 0.019836
2021-12-11 20:52:09,525 iteration 3417 : loss : 0.028667, loss_ce: 0.015604
 50%|█████████████▌             | 201/400 [1:25:50<1:27:03, 26.25s/it]2021-12-11 20:52:11,026 iteration 3418 : loss : 0.048535, loss_ce: 0.018186
2021-12-11 20:52:12,420 iteration 3419 : loss : 0.056133, loss_ce: 0.030554
2021-12-11 20:52:13,743 iteration 3420 : loss : 0.038479, loss_ce: 0.021663
2021-12-11 20:52:15,123 iteration 3421 : loss : 0.056284, loss_ce: 0.020299
2021-12-11 20:52:16,502 iteration 3422 : loss : 0.036067, loss_ce: 0.016354
2021-12-11 20:52:17,932 iteration 3423 : loss : 0.037477, loss_ce: 0.018730
2021-12-11 20:52:19,258 iteration 3424 : loss : 0.031795, loss_ce: 0.013600
2021-12-11 20:52:20,648 iteration 3425 : loss : 0.044592, loss_ce: 0.023144
2021-12-11 20:52:22,009 iteration 3426 : loss : 0.038893, loss_ce: 0.014435
2021-12-11 20:52:23,493 iteration 3427 : loss : 0.065201, loss_ce: 0.029868
2021-12-11 20:52:24,890 iteration 3428 : loss : 0.034786, loss_ce: 0.017458
2021-12-11 20:52:26,293 iteration 3429 : loss : 0.048693, loss_ce: 0.027995
2021-12-11 20:52:27,691 iteration 3430 : loss : 0.032147, loss_ce: 0.014447
2021-12-11 20:52:29,132 iteration 3431 : loss : 0.038687, loss_ce: 0.017876
2021-12-11 20:52:30,477 iteration 3432 : loss : 0.043906, loss_ce: 0.016815
2021-12-11 20:52:31,863 iteration 3433 : loss : 0.046672, loss_ce: 0.016357
2021-12-11 20:52:33,216 iteration 3434 : loss : 0.032913, loss_ce: 0.013658
 50%|█████████████▋             | 202/400 [1:26:14<1:24:05, 25.48s/it]2021-12-11 20:52:34,580 iteration 3435 : loss : 0.041855, loss_ce: 0.021886
2021-12-11 20:52:35,928 iteration 3436 : loss : 0.047572, loss_ce: 0.020539
2021-12-11 20:52:37,263 iteration 3437 : loss : 0.032956, loss_ce: 0.014966
2021-12-11 20:52:38,626 iteration 3438 : loss : 0.041979, loss_ce: 0.024312
2021-12-11 20:52:40,068 iteration 3439 : loss : 0.045325, loss_ce: 0.022026
2021-12-11 20:52:41,519 iteration 3440 : loss : 0.073032, loss_ce: 0.023284
2021-12-11 20:52:42,803 iteration 3441 : loss : 0.033788, loss_ce: 0.018895
2021-12-11 20:52:44,145 iteration 3442 : loss : 0.028261, loss_ce: 0.013060
2021-12-11 20:52:45,542 iteration 3443 : loss : 0.032989, loss_ce: 0.015396
2021-12-11 20:52:46,895 iteration 3444 : loss : 0.033998, loss_ce: 0.014762
2021-12-11 20:52:48,305 iteration 3445 : loss : 0.033309, loss_ce: 0.014772
2021-12-11 20:52:49,677 iteration 3446 : loss : 0.052427, loss_ce: 0.020713
2021-12-11 20:52:51,057 iteration 3447 : loss : 0.041268, loss_ce: 0.021805
2021-12-11 20:52:52,359 iteration 3448 : loss : 0.030242, loss_ce: 0.013951
2021-12-11 20:52:53,741 iteration 3449 : loss : 0.042016, loss_ce: 0.017600
2021-12-11 20:52:55,171 iteration 3450 : loss : 0.035588, loss_ce: 0.017904
2021-12-11 20:52:56,501 iteration 3451 : loss : 0.029163, loss_ce: 0.015226
 51%|█████████████▋             | 203/400 [1:26:37<1:21:29, 24.82s/it]2021-12-11 20:52:57,909 iteration 3452 : loss : 0.037133, loss_ce: 0.016508
2021-12-11 20:52:59,334 iteration 3453 : loss : 0.034052, loss_ce: 0.015856
2021-12-11 20:53:00,781 iteration 3454 : loss : 0.042131, loss_ce: 0.017723
2021-12-11 20:53:02,190 iteration 3455 : loss : 0.047489, loss_ce: 0.023809
2021-12-11 20:53:03,593 iteration 3456 : loss : 0.040734, loss_ce: 0.019106
2021-12-11 20:53:04,997 iteration 3457 : loss : 0.048928, loss_ce: 0.024060
2021-12-11 20:53:06,407 iteration 3458 : loss : 0.034826, loss_ce: 0.021148
2021-12-11 20:53:07,868 iteration 3459 : loss : 0.042256, loss_ce: 0.023960
2021-12-11 20:53:09,287 iteration 3460 : loss : 0.038986, loss_ce: 0.017086
2021-12-11 20:53:10,679 iteration 3461 : loss : 0.039066, loss_ce: 0.017862
2021-12-11 20:53:12,007 iteration 3462 : loss : 0.034324, loss_ce: 0.014834
2021-12-11 20:53:13,308 iteration 3463 : loss : 0.028633, loss_ce: 0.013126
2021-12-11 20:53:14,670 iteration 3464 : loss : 0.041050, loss_ce: 0.018809
2021-12-11 20:53:15,990 iteration 3465 : loss : 0.037677, loss_ce: 0.015172
2021-12-11 20:53:17,385 iteration 3466 : loss : 0.035481, loss_ce: 0.016615
2021-12-11 20:53:18,843 iteration 3467 : loss : 0.048478, loss_ce: 0.020905
2021-12-11 20:53:20,338 iteration 3468 : loss : 0.034506, loss_ce: 0.015579
 51%|█████████████▊             | 204/400 [1:27:01<1:20:07, 24.53s/it]2021-12-11 20:53:21,694 iteration 3469 : loss : 0.029624, loss_ce: 0.014462
2021-12-11 20:53:23,209 iteration 3470 : loss : 0.039340, loss_ce: 0.016161
2021-12-11 20:53:24,630 iteration 3471 : loss : 0.038742, loss_ce: 0.017127
2021-12-11 20:53:26,072 iteration 3472 : loss : 0.046421, loss_ce: 0.020964
2021-12-11 20:53:27,508 iteration 3473 : loss : 0.042601, loss_ce: 0.021885
2021-12-11 20:53:28,865 iteration 3474 : loss : 0.052871, loss_ce: 0.022265
2021-12-11 20:53:30,319 iteration 3475 : loss : 0.046900, loss_ce: 0.020319
2021-12-11 20:53:31,691 iteration 3476 : loss : 0.031248, loss_ce: 0.014629
2021-12-11 20:53:33,112 iteration 3477 : loss : 0.043748, loss_ce: 0.019676
2021-12-11 20:53:34,514 iteration 3478 : loss : 0.044135, loss_ce: 0.017391
2021-12-11 20:53:35,879 iteration 3479 : loss : 0.043812, loss_ce: 0.021353
2021-12-11 20:53:37,211 iteration 3480 : loss : 0.025724, loss_ce: 0.013586
2021-12-11 20:53:38,592 iteration 3481 : loss : 0.051427, loss_ce: 0.032863
2021-12-11 20:53:39,899 iteration 3482 : loss : 0.039908, loss_ce: 0.017083
2021-12-11 20:53:41,327 iteration 3483 : loss : 0.032893, loss_ce: 0.015424
2021-12-11 20:53:42,700 iteration 3484 : loss : 0.041089, loss_ce: 0.014218
2021-12-11 20:53:42,701 Training Data Eval:
2021-12-11 20:53:49,514   Average segmentation loss on training set: 0.0199
2021-12-11 20:53:49,514 Validation Data Eval:
2021-12-11 20:53:51,873   Average segmentation loss on validation set: 0.0859
2021-12-11 20:53:53,248 iteration 3485 : loss : 0.027459, loss_ce: 0.014177
 51%|█████████████▊             | 205/400 [1:27:34<1:27:53, 27.04s/it]2021-12-11 20:53:54,633 iteration 3486 : loss : 0.029448, loss_ce: 0.013172
2021-12-11 20:53:56,012 iteration 3487 : loss : 0.043310, loss_ce: 0.019971
2021-12-11 20:53:57,432 iteration 3488 : loss : 0.039297, loss_ce: 0.024511
2021-12-11 20:53:58,777 iteration 3489 : loss : 0.034754, loss_ce: 0.014974
2021-12-11 20:54:00,224 iteration 3490 : loss : 0.037360, loss_ce: 0.016766
2021-12-11 20:54:01,646 iteration 3491 : loss : 0.047910, loss_ce: 0.023863
2021-12-11 20:54:02,952 iteration 3492 : loss : 0.030041, loss_ce: 0.012192
2021-12-11 20:54:04,404 iteration 3493 : loss : 0.054213, loss_ce: 0.016403
2021-12-11 20:54:05,818 iteration 3494 : loss : 0.036722, loss_ce: 0.022531
2021-12-11 20:54:07,117 iteration 3495 : loss : 0.031459, loss_ce: 0.016078
2021-12-11 20:54:08,514 iteration 3496 : loss : 0.056530, loss_ce: 0.018366
2021-12-11 20:54:09,963 iteration 3497 : loss : 0.045539, loss_ce: 0.021295
2021-12-11 20:54:11,344 iteration 3498 : loss : 0.033039, loss_ce: 0.015448
2021-12-11 20:54:12,787 iteration 3499 : loss : 0.055324, loss_ce: 0.018279
2021-12-11 20:54:14,144 iteration 3500 : loss : 0.039852, loss_ce: 0.015889
2021-12-11 20:54:15,539 iteration 3501 : loss : 0.034072, loss_ce: 0.015805
2021-12-11 20:54:16,922 iteration 3502 : loss : 0.047663, loss_ce: 0.018658
 52%|█████████████▉             | 206/400 [1:27:58<1:24:10, 26.04s/it]2021-12-11 20:54:18,291 iteration 3503 : loss : 0.026196, loss_ce: 0.014724
2021-12-11 20:54:19,685 iteration 3504 : loss : 0.029894, loss_ce: 0.014012
2021-12-11 20:54:21,065 iteration 3505 : loss : 0.048787, loss_ce: 0.027511
2021-12-11 20:54:22,531 iteration 3506 : loss : 0.042458, loss_ce: 0.018235
2021-12-11 20:54:23,894 iteration 3507 : loss : 0.032747, loss_ce: 0.015544
2021-12-11 20:54:25,190 iteration 3508 : loss : 0.032395, loss_ce: 0.014565
2021-12-11 20:54:26,543 iteration 3509 : loss : 0.043545, loss_ce: 0.023096
2021-12-11 20:54:28,010 iteration 3510 : loss : 0.033397, loss_ce: 0.016230
2021-12-11 20:54:29,375 iteration 3511 : loss : 0.071024, loss_ce: 0.033077
2021-12-11 20:54:30,773 iteration 3512 : loss : 0.038422, loss_ce: 0.017207
2021-12-11 20:54:32,093 iteration 3513 : loss : 0.049822, loss_ce: 0.019066
2021-12-11 20:54:33,397 iteration 3514 : loss : 0.021727, loss_ce: 0.011950
2021-12-11 20:54:34,754 iteration 3515 : loss : 0.031052, loss_ce: 0.014577
2021-12-11 20:54:36,161 iteration 3516 : loss : 0.044114, loss_ce: 0.018139
2021-12-11 20:54:37,541 iteration 3517 : loss : 0.038672, loss_ce: 0.017317
2021-12-11 20:54:38,859 iteration 3518 : loss : 0.027624, loss_ce: 0.015498
2021-12-11 20:54:40,200 iteration 3519 : loss : 0.048284, loss_ce: 0.019986
 52%|█████████████▉             | 207/400 [1:28:21<1:21:05, 25.21s/it]2021-12-11 20:54:41,621 iteration 3520 : loss : 0.031911, loss_ce: 0.015150
2021-12-11 20:54:43,025 iteration 3521 : loss : 0.055780, loss_ce: 0.026638
2021-12-11 20:54:44,428 iteration 3522 : loss : 0.044203, loss_ce: 0.022229
2021-12-11 20:54:45,911 iteration 3523 : loss : 0.044909, loss_ce: 0.017657
2021-12-11 20:54:47,280 iteration 3524 : loss : 0.042291, loss_ce: 0.014543
2021-12-11 20:54:48,684 iteration 3525 : loss : 0.047274, loss_ce: 0.023849
2021-12-11 20:54:50,088 iteration 3526 : loss : 0.044781, loss_ce: 0.026055
2021-12-11 20:54:51,502 iteration 3527 : loss : 0.034109, loss_ce: 0.014683
2021-12-11 20:54:52,835 iteration 3528 : loss : 0.033687, loss_ce: 0.019774
2021-12-11 20:54:54,237 iteration 3529 : loss : 0.039829, loss_ce: 0.017462
2021-12-11 20:54:55,607 iteration 3530 : loss : 0.055060, loss_ce: 0.021854
2021-12-11 20:54:57,012 iteration 3531 : loss : 0.049003, loss_ce: 0.023626
2021-12-11 20:54:58,368 iteration 3532 : loss : 0.036551, loss_ce: 0.017233
2021-12-11 20:54:59,694 iteration 3533 : loss : 0.043213, loss_ce: 0.018648
2021-12-11 20:55:01,032 iteration 3534 : loss : 0.036681, loss_ce: 0.021280
2021-12-11 20:55:02,341 iteration 3535 : loss : 0.031067, loss_ce: 0.015656
2021-12-11 20:55:03,668 iteration 3536 : loss : 0.038390, loss_ce: 0.017076
 52%|██████████████             | 208/400 [1:28:44<1:18:59, 24.69s/it]2021-12-11 20:55:05,077 iteration 3537 : loss : 0.042419, loss_ce: 0.019825
2021-12-11 20:55:06,446 iteration 3538 : loss : 0.036733, loss_ce: 0.015498
2021-12-11 20:55:07,857 iteration 3539 : loss : 0.044655, loss_ce: 0.022805
2021-12-11 20:55:09,187 iteration 3540 : loss : 0.061299, loss_ce: 0.018339
2021-12-11 20:55:10,627 iteration 3541 : loss : 0.030751, loss_ce: 0.016618
2021-12-11 20:55:11,973 iteration 3542 : loss : 0.038057, loss_ce: 0.014322
2021-12-11 20:55:13,376 iteration 3543 : loss : 0.031364, loss_ce: 0.017537
2021-12-11 20:55:14,801 iteration 3544 : loss : 0.037380, loss_ce: 0.017388
2021-12-11 20:55:16,153 iteration 3545 : loss : 0.035384, loss_ce: 0.016463
2021-12-11 20:55:17,441 iteration 3546 : loss : 0.033729, loss_ce: 0.018916
2021-12-11 20:55:18,852 iteration 3547 : loss : 0.044232, loss_ce: 0.019782
2021-12-11 20:55:20,261 iteration 3548 : loss : 0.036060, loss_ce: 0.015256
2021-12-11 20:55:21,644 iteration 3549 : loss : 0.050299, loss_ce: 0.026356
2021-12-11 20:55:22,980 iteration 3550 : loss : 0.031140, loss_ce: 0.014392
2021-12-11 20:55:24,412 iteration 3551 : loss : 0.059065, loss_ce: 0.019499
2021-12-11 20:55:25,850 iteration 3552 : loss : 0.061136, loss_ce: 0.025894
2021-12-11 20:55:27,294 iteration 3553 : loss : 0.049976, loss_ce: 0.019771
 52%|██████████████             | 209/400 [1:29:08<1:17:33, 24.36s/it]2021-12-11 20:55:28,710 iteration 3554 : loss : 0.042563, loss_ce: 0.019069
2021-12-11 20:55:30,132 iteration 3555 : loss : 0.040725, loss_ce: 0.020349
2021-12-11 20:55:31,497 iteration 3556 : loss : 0.046615, loss_ce: 0.020196
2021-12-11 20:55:32,856 iteration 3557 : loss : 0.032326, loss_ce: 0.016720
2021-12-11 20:55:34,175 iteration 3558 : loss : 0.033104, loss_ce: 0.014578
2021-12-11 20:55:35,606 iteration 3559 : loss : 0.037823, loss_ce: 0.016878
2021-12-11 20:55:36,974 iteration 3560 : loss : 0.044103, loss_ce: 0.016939
2021-12-11 20:55:38,380 iteration 3561 : loss : 0.033531, loss_ce: 0.016780
2021-12-11 20:55:39,754 iteration 3562 : loss : 0.040108, loss_ce: 0.017715
2021-12-11 20:55:41,133 iteration 3563 : loss : 0.036632, loss_ce: 0.016831
2021-12-11 20:55:42,537 iteration 3564 : loss : 0.038257, loss_ce: 0.015651
2021-12-11 20:55:43,870 iteration 3565 : loss : 0.040346, loss_ce: 0.018942
2021-12-11 20:55:45,140 iteration 3566 : loss : 0.023692, loss_ce: 0.012012
2021-12-11 20:55:46,445 iteration 3567 : loss : 0.038253, loss_ce: 0.014596
2021-12-11 20:55:47,859 iteration 3568 : loss : 0.048119, loss_ce: 0.020719
2021-12-11 20:55:49,194 iteration 3569 : loss : 0.036145, loss_ce: 0.017986
2021-12-11 20:55:49,194 Training Data Eval:
2021-12-11 20:55:56,013   Average segmentation loss on training set: 0.0198
2021-12-11 20:55:56,013 Validation Data Eval:
2021-12-11 20:55:58,374   Average segmentation loss on validation set: 0.0892
2021-12-11 20:55:59,720 iteration 3570 : loss : 0.035877, loss_ce: 0.014690
 52%|██████████████▏            | 210/400 [1:29:41<1:24:49, 26.78s/it]2021-12-11 20:56:01,178 iteration 3571 : loss : 0.048295, loss_ce: 0.023477
2021-12-11 20:56:02,620 iteration 3572 : loss : 0.038807, loss_ce: 0.018716
2021-12-11 20:56:04,024 iteration 3573 : loss : 0.045244, loss_ce: 0.018638
2021-12-11 20:56:05,377 iteration 3574 : loss : 0.031561, loss_ce: 0.018426
2021-12-11 20:56:06,716 iteration 3575 : loss : 0.033954, loss_ce: 0.016071
2021-12-11 20:56:08,064 iteration 3576 : loss : 0.044267, loss_ce: 0.018201
2021-12-11 20:56:09,470 iteration 3577 : loss : 0.032395, loss_ce: 0.014788
2021-12-11 20:56:10,828 iteration 3578 : loss : 0.044381, loss_ce: 0.024578
2021-12-11 20:56:12,224 iteration 3579 : loss : 0.033780, loss_ce: 0.014709
2021-12-11 20:56:13,645 iteration 3580 : loss : 0.032912, loss_ce: 0.016101
2021-12-11 20:56:15,072 iteration 3581 : loss : 0.038304, loss_ce: 0.018644
2021-12-11 20:56:16,473 iteration 3582 : loss : 0.042011, loss_ce: 0.018113
2021-12-11 20:56:17,841 iteration 3583 : loss : 0.052603, loss_ce: 0.018205
2021-12-11 20:56:19,141 iteration 3584 : loss : 0.028280, loss_ce: 0.013184
2021-12-11 20:56:20,492 iteration 3585 : loss : 0.041139, loss_ce: 0.018389
2021-12-11 20:56:21,810 iteration 3586 : loss : 0.029946, loss_ce: 0.013000
2021-12-11 20:56:23,187 iteration 3587 : loss : 0.034579, loss_ce: 0.017908
 53%|██████████████▏            | 211/400 [1:30:04<1:21:14, 25.79s/it]2021-12-11 20:56:24,671 iteration 3588 : loss : 0.029993, loss_ce: 0.017132
2021-12-11 20:56:26,117 iteration 3589 : loss : 0.042971, loss_ce: 0.017543
2021-12-11 20:56:27,505 iteration 3590 : loss : 0.039174, loss_ce: 0.014598
2021-12-11 20:56:28,836 iteration 3591 : loss : 0.049635, loss_ce: 0.021590
2021-12-11 20:56:30,203 iteration 3592 : loss : 0.037505, loss_ce: 0.018070
2021-12-11 20:56:31,612 iteration 3593 : loss : 0.040613, loss_ce: 0.021590
2021-12-11 20:56:32,988 iteration 3594 : loss : 0.037952, loss_ce: 0.014260
2021-12-11 20:56:34,351 iteration 3595 : loss : 0.061729, loss_ce: 0.022113
2021-12-11 20:56:35,691 iteration 3596 : loss : 0.031488, loss_ce: 0.016028
2021-12-11 20:56:37,078 iteration 3597 : loss : 0.031260, loss_ce: 0.016318
2021-12-11 20:56:38,484 iteration 3598 : loss : 0.043460, loss_ce: 0.022659
2021-12-11 20:56:39,891 iteration 3599 : loss : 0.036054, loss_ce: 0.015641
2021-12-11 20:56:41,217 iteration 3600 : loss : 0.037389, loss_ce: 0.022992
2021-12-11 20:56:42,618 iteration 3601 : loss : 0.037781, loss_ce: 0.017138
2021-12-11 20:56:44,025 iteration 3602 : loss : 0.045938, loss_ce: 0.019008
2021-12-11 20:56:45,379 iteration 3603 : loss : 0.035381, loss_ce: 0.019157
2021-12-11 20:56:46,746 iteration 3604 : loss : 0.054494, loss_ce: 0.016931
 53%|██████████████▎            | 212/400 [1:30:28<1:18:42, 25.12s/it]2021-12-11 20:56:48,286 iteration 3605 : loss : 0.049831, loss_ce: 0.023543
2021-12-11 20:56:49,603 iteration 3606 : loss : 0.048760, loss_ce: 0.014494
2021-12-11 20:56:50,886 iteration 3607 : loss : 0.021325, loss_ce: 0.011765
2021-12-11 20:56:52,311 iteration 3608 : loss : 0.042696, loss_ce: 0.018093
2021-12-11 20:56:53,727 iteration 3609 : loss : 0.063364, loss_ce: 0.026589
2021-12-11 20:56:55,040 iteration 3610 : loss : 0.025027, loss_ce: 0.012377
2021-12-11 20:56:56,410 iteration 3611 : loss : 0.036577, loss_ce: 0.017285
2021-12-11 20:56:57,738 iteration 3612 : loss : 0.025761, loss_ce: 0.014585
2021-12-11 20:56:59,112 iteration 3613 : loss : 0.034924, loss_ce: 0.016374
2021-12-11 20:57:00,449 iteration 3614 : loss : 0.042151, loss_ce: 0.020061
2021-12-11 20:57:01,810 iteration 3615 : loss : 0.038244, loss_ce: 0.015383
2021-12-11 20:57:03,178 iteration 3616 : loss : 0.043556, loss_ce: 0.023239
2021-12-11 20:57:04,576 iteration 3617 : loss : 0.035196, loss_ce: 0.019602
2021-12-11 20:57:05,973 iteration 3618 : loss : 0.041421, loss_ce: 0.016108
2021-12-11 20:57:07,327 iteration 3619 : loss : 0.026058, loss_ce: 0.012186
2021-12-11 20:57:08,706 iteration 3620 : loss : 0.040540, loss_ce: 0.016651
2021-12-11 20:57:10,068 iteration 3621 : loss : 0.034204, loss_ce: 0.017792
 53%|██████████████▍            | 213/400 [1:30:51<1:16:36, 24.58s/it]2021-12-11 20:57:11,529 iteration 3622 : loss : 0.038156, loss_ce: 0.014887
2021-12-11 20:57:12,920 iteration 3623 : loss : 0.039772, loss_ce: 0.018466
2021-12-11 20:57:14,314 iteration 3624 : loss : 0.041974, loss_ce: 0.020051
2021-12-11 20:57:15,614 iteration 3625 : loss : 0.026194, loss_ce: 0.013084
2021-12-11 20:57:17,000 iteration 3626 : loss : 0.045603, loss_ce: 0.017860
2021-12-11 20:57:18,406 iteration 3627 : loss : 0.043911, loss_ce: 0.021514
2021-12-11 20:57:19,812 iteration 3628 : loss : 0.050168, loss_ce: 0.018703
2021-12-11 20:57:21,250 iteration 3629 : loss : 0.053311, loss_ce: 0.026052
2021-12-11 20:57:22,646 iteration 3630 : loss : 0.035907, loss_ce: 0.018399
2021-12-11 20:57:24,025 iteration 3631 : loss : 0.032613, loss_ce: 0.015372
2021-12-11 20:57:25,437 iteration 3632 : loss : 0.050758, loss_ce: 0.029647
2021-12-11 20:57:26,852 iteration 3633 : loss : 0.040278, loss_ce: 0.015862
2021-12-11 20:57:28,194 iteration 3634 : loss : 0.028643, loss_ce: 0.012050
2021-12-11 20:57:29,684 iteration 3635 : loss : 0.047603, loss_ce: 0.028235
2021-12-11 20:57:30,983 iteration 3636 : loss : 0.034625, loss_ce: 0.018192
2021-12-11 20:57:32,361 iteration 3637 : loss : 0.025084, loss_ce: 0.012184
2021-12-11 20:57:33,750 iteration 3638 : loss : 0.029510, loss_ce: 0.013575
 54%|██████████████▍            | 214/400 [1:31:15<1:15:22, 24.31s/it]2021-12-11 20:57:35,197 iteration 3639 : loss : 0.047009, loss_ce: 0.018971
2021-12-11 20:57:36,614 iteration 3640 : loss : 0.044602, loss_ce: 0.015217
2021-12-11 20:57:38,031 iteration 3641 : loss : 0.041411, loss_ce: 0.018078
2021-12-11 20:57:39,385 iteration 3642 : loss : 0.036382, loss_ce: 0.018430
2021-12-11 20:57:40,794 iteration 3643 : loss : 0.045891, loss_ce: 0.016792
2021-12-11 20:57:42,202 iteration 3644 : loss : 0.029025, loss_ce: 0.013931
2021-12-11 20:57:43,660 iteration 3645 : loss : 0.050587, loss_ce: 0.016994
2021-12-11 20:57:45,065 iteration 3646 : loss : 0.042044, loss_ce: 0.023061
2021-12-11 20:57:46,430 iteration 3647 : loss : 0.026465, loss_ce: 0.015046
2021-12-11 20:57:47,826 iteration 3648 : loss : 0.032591, loss_ce: 0.015795
2021-12-11 20:57:49,106 iteration 3649 : loss : 0.032125, loss_ce: 0.015420
2021-12-11 20:57:50,492 iteration 3650 : loss : 0.033852, loss_ce: 0.016923
2021-12-11 20:57:51,831 iteration 3651 : loss : 0.033068, loss_ce: 0.015964
2021-12-11 20:57:53,146 iteration 3652 : loss : 0.032446, loss_ce: 0.015714
2021-12-11 20:57:54,574 iteration 3653 : loss : 0.056079, loss_ce: 0.025989
2021-12-11 20:57:55,925 iteration 3654 : loss : 0.029847, loss_ce: 0.014232
2021-12-11 20:57:55,925 Training Data Eval:
2021-12-11 20:58:02,730   Average segmentation loss on training set: 0.0202
2021-12-11 20:58:02,731 Validation Data Eval:
2021-12-11 20:58:05,087   Average segmentation loss on validation set: 0.1019
2021-12-11 20:58:06,596 iteration 3655 : loss : 0.059652, loss_ce: 0.023639
 54%|██████████████▌            | 215/400 [1:31:47<1:22:50, 26.87s/it]2021-12-11 20:58:08,019 iteration 3656 : loss : 0.037271, loss_ce: 0.015331
2021-12-11 20:58:09,435 iteration 3657 : loss : 0.032877, loss_ce: 0.017118
2021-12-11 20:58:10,807 iteration 3658 : loss : 0.035838, loss_ce: 0.017621
2021-12-11 20:58:12,122 iteration 3659 : loss : 0.050925, loss_ce: 0.019378
2021-12-11 20:58:13,570 iteration 3660 : loss : 0.034881, loss_ce: 0.016227
2021-12-11 20:58:14,931 iteration 3661 : loss : 0.036699, loss_ce: 0.016830
2021-12-11 20:58:16,246 iteration 3662 : loss : 0.038889, loss_ce: 0.022321
2021-12-11 20:58:17,607 iteration 3663 : loss : 0.037672, loss_ce: 0.021881
2021-12-11 20:58:19,003 iteration 3664 : loss : 0.051046, loss_ce: 0.019736
2021-12-11 20:58:20,488 iteration 3665 : loss : 0.049202, loss_ce: 0.018332
2021-12-11 20:58:21,910 iteration 3666 : loss : 0.035136, loss_ce: 0.018637
2021-12-11 20:58:23,268 iteration 3667 : loss : 0.039556, loss_ce: 0.017618
2021-12-11 20:58:24,627 iteration 3668 : loss : 0.047445, loss_ce: 0.018200
2021-12-11 20:58:26,013 iteration 3669 : loss : 0.038121, loss_ce: 0.016073
2021-12-11 20:58:27,327 iteration 3670 : loss : 0.031823, loss_ce: 0.016476
2021-12-11 20:58:28,682 iteration 3671 : loss : 0.047859, loss_ce: 0.014959
2021-12-11 20:58:30,038 iteration 3672 : loss : 0.032816, loss_ce: 0.013576
 54%|██████████████▌            | 216/400 [1:32:11<1:19:15, 25.84s/it]2021-12-11 20:58:31,450 iteration 3673 : loss : 0.030173, loss_ce: 0.015819
2021-12-11 20:58:32,897 iteration 3674 : loss : 0.034926, loss_ce: 0.017919
2021-12-11 20:58:34,341 iteration 3675 : loss : 0.041745, loss_ce: 0.020918
2021-12-11 20:58:35,769 iteration 3676 : loss : 0.053516, loss_ce: 0.016420
2021-12-11 20:58:37,180 iteration 3677 : loss : 0.035543, loss_ce: 0.019997
2021-12-11 20:58:38,628 iteration 3678 : loss : 0.042802, loss_ce: 0.017726
2021-12-11 20:58:39,932 iteration 3679 : loss : 0.031185, loss_ce: 0.014161
2021-12-11 20:58:41,265 iteration 3680 : loss : 0.027985, loss_ce: 0.013755
2021-12-11 20:58:42,633 iteration 3681 : loss : 0.045897, loss_ce: 0.017360
2021-12-11 20:58:43,918 iteration 3682 : loss : 0.023616, loss_ce: 0.010863
2021-12-11 20:58:45,283 iteration 3683 : loss : 0.033649, loss_ce: 0.018146
2021-12-11 20:58:46,604 iteration 3684 : loss : 0.031181, loss_ce: 0.014429
2021-12-11 20:58:47,963 iteration 3685 : loss : 0.033393, loss_ce: 0.015686
2021-12-11 20:58:49,409 iteration 3686 : loss : 0.041230, loss_ce: 0.018988
2021-12-11 20:58:50,854 iteration 3687 : loss : 0.053848, loss_ce: 0.025282
2021-12-11 20:58:52,257 iteration 3688 : loss : 0.037851, loss_ce: 0.017723
2021-12-11 20:58:53,668 iteration 3689 : loss : 0.046090, loss_ce: 0.016352
 54%|██████████████▋            | 217/400 [1:32:34<1:16:47, 25.18s/it]2021-12-11 20:58:55,215 iteration 3690 : loss : 0.041982, loss_ce: 0.022501
2021-12-11 20:58:56,577 iteration 3691 : loss : 0.055760, loss_ce: 0.021574
2021-12-11 20:58:57,980 iteration 3692 : loss : 0.030768, loss_ce: 0.013526
2021-12-11 20:58:59,312 iteration 3693 : loss : 0.028582, loss_ce: 0.011588
2021-12-11 20:59:00,719 iteration 3694 : loss : 0.044255, loss_ce: 0.018035
2021-12-11 20:59:02,115 iteration 3695 : loss : 0.026244, loss_ce: 0.012503
2021-12-11 20:59:03,447 iteration 3696 : loss : 0.024021, loss_ce: 0.012399
2021-12-11 20:59:04,778 iteration 3697 : loss : 0.037780, loss_ce: 0.016308
2021-12-11 20:59:06,236 iteration 3698 : loss : 0.043938, loss_ce: 0.017875
2021-12-11 20:59:07,614 iteration 3699 : loss : 0.030014, loss_ce: 0.017215
2021-12-11 20:59:08,983 iteration 3700 : loss : 0.037233, loss_ce: 0.015780
2021-12-11 20:59:10,298 iteration 3701 : loss : 0.038859, loss_ce: 0.020927
2021-12-11 20:59:11,701 iteration 3702 : loss : 0.057590, loss_ce: 0.024154
2021-12-11 20:59:13,079 iteration 3703 : loss : 0.032090, loss_ce: 0.014052
2021-12-11 20:59:14,457 iteration 3704 : loss : 0.030113, loss_ce: 0.014489
2021-12-11 20:59:15,844 iteration 3705 : loss : 0.042076, loss_ce: 0.020615
2021-12-11 20:59:17,164 iteration 3706 : loss : 0.031949, loss_ce: 0.013867
 55%|██████████████▋            | 218/400 [1:32:58<1:14:50, 24.67s/it]2021-12-11 20:59:18,612 iteration 3707 : loss : 0.092299, loss_ce: 0.022007
2021-12-11 20:59:20,104 iteration 3708 : loss : 0.054633, loss_ce: 0.020701
2021-12-11 20:59:21,484 iteration 3709 : loss : 0.035079, loss_ce: 0.015709
2021-12-11 20:59:22,783 iteration 3710 : loss : 0.025803, loss_ce: 0.013278
2021-12-11 20:59:24,267 iteration 3711 : loss : 0.047316, loss_ce: 0.022006
2021-12-11 20:59:25,601 iteration 3712 : loss : 0.048450, loss_ce: 0.020166
2021-12-11 20:59:26,911 iteration 3713 : loss : 0.028353, loss_ce: 0.014966
2021-12-11 20:59:28,317 iteration 3714 : loss : 0.046575, loss_ce: 0.024326
2021-12-11 20:59:29,670 iteration 3715 : loss : 0.036285, loss_ce: 0.015206
2021-12-11 20:59:31,044 iteration 3716 : loss : 0.047960, loss_ce: 0.022748
2021-12-11 20:59:32,430 iteration 3717 : loss : 0.037549, loss_ce: 0.020798
2021-12-11 20:59:33,844 iteration 3718 : loss : 0.033711, loss_ce: 0.014975
2021-12-11 20:59:35,323 iteration 3719 : loss : 0.034516, loss_ce: 0.014326
2021-12-11 20:59:36,786 iteration 3720 : loss : 0.037104, loss_ce: 0.018046
2021-12-11 20:59:38,178 iteration 3721 : loss : 0.032054, loss_ce: 0.016441
2021-12-11 20:59:39,622 iteration 3722 : loss : 0.038760, loss_ce: 0.016487
2021-12-11 20:59:40,984 iteration 3723 : loss : 0.042842, loss_ce: 0.022543
 55%|██████████████▊            | 219/400 [1:33:22<1:13:39, 24.41s/it]2021-12-11 20:59:42,416 iteration 3724 : loss : 0.049164, loss_ce: 0.028974
2021-12-11 20:59:43,847 iteration 3725 : loss : 0.036512, loss_ce: 0.016727
2021-12-11 20:59:45,220 iteration 3726 : loss : 0.035428, loss_ce: 0.017908
2021-12-11 20:59:46,602 iteration 3727 : loss : 0.025491, loss_ce: 0.012240
2021-12-11 20:59:47,903 iteration 3728 : loss : 0.037047, loss_ce: 0.019648
2021-12-11 20:59:49,319 iteration 3729 : loss : 0.046911, loss_ce: 0.016256
2021-12-11 20:59:50,659 iteration 3730 : loss : 0.025433, loss_ce: 0.012389
2021-12-11 20:59:52,077 iteration 3731 : loss : 0.037651, loss_ce: 0.018349
2021-12-11 20:59:53,449 iteration 3732 : loss : 0.039975, loss_ce: 0.021186
2021-12-11 20:59:54,759 iteration 3733 : loss : 0.026228, loss_ce: 0.011067
2021-12-11 20:59:56,173 iteration 3734 : loss : 0.042244, loss_ce: 0.017380
2021-12-11 20:59:57,607 iteration 3735 : loss : 0.045748, loss_ce: 0.020193
2021-12-11 20:59:59,014 iteration 3736 : loss : 0.039100, loss_ce: 0.018123
2021-12-11 21:00:00,440 iteration 3737 : loss : 0.041648, loss_ce: 0.021142
2021-12-11 21:00:01,820 iteration 3738 : loss : 0.059502, loss_ce: 0.020654
2021-12-11 21:00:03,198 iteration 3739 : loss : 0.042327, loss_ce: 0.016072
2021-12-11 21:00:03,198 Training Data Eval:
2021-12-11 21:00:09,997   Average segmentation loss on training set: 0.0205
2021-12-11 21:00:09,998 Validation Data Eval:
2021-12-11 21:00:12,352   Average segmentation loss on validation set: 0.0996
2021-12-11 21:00:13,685 iteration 3740 : loss : 0.037948, loss_ce: 0.015407
 55%|██████████████▊            | 220/400 [1:33:55<1:20:42, 26.90s/it]2021-12-11 21:00:15,132 iteration 3741 : loss : 0.041978, loss_ce: 0.023899
2021-12-11 21:00:16,498 iteration 3742 : loss : 0.030949, loss_ce: 0.014958
2021-12-11 21:00:17,842 iteration 3743 : loss : 0.024528, loss_ce: 0.010565
2021-12-11 21:00:19,240 iteration 3744 : loss : 0.027882, loss_ce: 0.012845
2021-12-11 21:00:20,696 iteration 3745 : loss : 0.043216, loss_ce: 0.027786
2021-12-11 21:00:22,062 iteration 3746 : loss : 0.034240, loss_ce: 0.012746
2021-12-11 21:00:23,479 iteration 3747 : loss : 0.053554, loss_ce: 0.029163
2021-12-11 21:00:24,913 iteration 3748 : loss : 0.050220, loss_ce: 0.020757
2021-12-11 21:00:26,307 iteration 3749 : loss : 0.035378, loss_ce: 0.014896
2021-12-11 21:00:27,785 iteration 3750 : loss : 0.066129, loss_ce: 0.029103
2021-12-11 21:00:29,151 iteration 3751 : loss : 0.040392, loss_ce: 0.019638
2021-12-11 21:00:30,502 iteration 3752 : loss : 0.033406, loss_ce: 0.017308
2021-12-11 21:00:31,864 iteration 3753 : loss : 0.041310, loss_ce: 0.017527
2021-12-11 21:00:33,178 iteration 3754 : loss : 0.029880, loss_ce: 0.017064
2021-12-11 21:00:34,567 iteration 3755 : loss : 0.038033, loss_ce: 0.016709
2021-12-11 21:00:35,915 iteration 3756 : loss : 0.030046, loss_ce: 0.013574
2021-12-11 21:00:37,286 iteration 3757 : loss : 0.043461, loss_ce: 0.018787
 55%|██████████████▉            | 221/400 [1:34:18<1:17:18, 25.92s/it]2021-12-11 21:00:38,771 iteration 3758 : loss : 0.049576, loss_ce: 0.024396
2021-12-11 21:00:40,188 iteration 3759 : loss : 0.044741, loss_ce: 0.019851
2021-12-11 21:00:41,611 iteration 3760 : loss : 0.079720, loss_ce: 0.035565
2021-12-11 21:00:43,005 iteration 3761 : loss : 0.034249, loss_ce: 0.018361
2021-12-11 21:00:44,434 iteration 3762 : loss : 0.048739, loss_ce: 0.020563
2021-12-11 21:00:45,864 iteration 3763 : loss : 0.072674, loss_ce: 0.037575
2021-12-11 21:00:47,212 iteration 3764 : loss : 0.041421, loss_ce: 0.013963
2021-12-11 21:00:48,521 iteration 3765 : loss : 0.038599, loss_ce: 0.015838
2021-12-11 21:00:49,849 iteration 3766 : loss : 0.038645, loss_ce: 0.015614
2021-12-11 21:00:51,225 iteration 3767 : loss : 0.035938, loss_ce: 0.017375
2021-12-11 21:00:52,676 iteration 3768 : loss : 0.034678, loss_ce: 0.015700
2021-12-11 21:00:54,058 iteration 3769 : loss : 0.051519, loss_ce: 0.021092
2021-12-11 21:00:55,496 iteration 3770 : loss : 0.039552, loss_ce: 0.016915
2021-12-11 21:00:56,903 iteration 3771 : loss : 0.044240, loss_ce: 0.020765
2021-12-11 21:00:58,332 iteration 3772 : loss : 0.039451, loss_ce: 0.014055
2021-12-11 21:00:59,720 iteration 3773 : loss : 0.037859, loss_ce: 0.016039
2021-12-11 21:01:01,117 iteration 3774 : loss : 0.040772, loss_ce: 0.022120
 56%|██████████████▉            | 222/400 [1:34:42<1:15:01, 25.29s/it]2021-12-11 21:01:02,598 iteration 3775 : loss : 0.044698, loss_ce: 0.021662
2021-12-11 21:01:03,987 iteration 3776 : loss : 0.027181, loss_ce: 0.013786
2021-12-11 21:01:05,366 iteration 3777 : loss : 0.036411, loss_ce: 0.015973
2021-12-11 21:01:06,805 iteration 3778 : loss : 0.049285, loss_ce: 0.023135
2021-12-11 21:01:08,138 iteration 3779 : loss : 0.038014, loss_ce: 0.018764
2021-12-11 21:01:09,574 iteration 3780 : loss : 0.044087, loss_ce: 0.022126
2021-12-11 21:01:10,999 iteration 3781 : loss : 0.047527, loss_ce: 0.021192
2021-12-11 21:01:12,438 iteration 3782 : loss : 0.052176, loss_ce: 0.021581
2021-12-11 21:01:13,863 iteration 3783 : loss : 0.035922, loss_ce: 0.020017
2021-12-11 21:01:15,196 iteration 3784 : loss : 0.035678, loss_ce: 0.014023
2021-12-11 21:01:16,574 iteration 3785 : loss : 0.058028, loss_ce: 0.019464
2021-12-11 21:01:17,947 iteration 3786 : loss : 0.036001, loss_ce: 0.017819
2021-12-11 21:01:19,269 iteration 3787 : loss : 0.050580, loss_ce: 0.021482
2021-12-11 21:01:20,670 iteration 3788 : loss : 0.042764, loss_ce: 0.016602
2021-12-11 21:01:22,046 iteration 3789 : loss : 0.031093, loss_ce: 0.014680
2021-12-11 21:01:23,391 iteration 3790 : loss : 0.039073, loss_ce: 0.018699
2021-12-11 21:01:24,815 iteration 3791 : loss : 0.034056, loss_ce: 0.014876
 56%|███████████████            | 223/400 [1:35:06<1:13:11, 24.81s/it]2021-12-11 21:01:26,211 iteration 3792 : loss : 0.033669, loss_ce: 0.015458
2021-12-11 21:01:27,515 iteration 3793 : loss : 0.027443, loss_ce: 0.010813
2021-12-11 21:01:28,862 iteration 3794 : loss : 0.030480, loss_ce: 0.015264
2021-12-11 21:01:30,205 iteration 3795 : loss : 0.027271, loss_ce: 0.011520
2021-12-11 21:01:31,635 iteration 3796 : loss : 0.041303, loss_ce: 0.016429
2021-12-11 21:01:33,043 iteration 3797 : loss : 0.027761, loss_ce: 0.012400
2021-12-11 21:01:34,376 iteration 3798 : loss : 0.031919, loss_ce: 0.014158
2021-12-11 21:01:35,838 iteration 3799 : loss : 0.036976, loss_ce: 0.016645
2021-12-11 21:01:37,348 iteration 3800 : loss : 0.070206, loss_ce: 0.033832
2021-12-11 21:01:38,806 iteration 3801 : loss : 0.045611, loss_ce: 0.016319
2021-12-11 21:01:40,248 iteration 3802 : loss : 0.032144, loss_ce: 0.015348
2021-12-11 21:01:41,622 iteration 3803 : loss : 0.035959, loss_ce: 0.018675
2021-12-11 21:01:42,999 iteration 3804 : loss : 0.037866, loss_ce: 0.015843
2021-12-11 21:01:44,376 iteration 3805 : loss : 0.042911, loss_ce: 0.019269
2021-12-11 21:01:45,676 iteration 3806 : loss : 0.028335, loss_ce: 0.015606
2021-12-11 21:01:47,069 iteration 3807 : loss : 0.033133, loss_ce: 0.014409
2021-12-11 21:01:48,421 iteration 3808 : loss : 0.040961, loss_ce: 0.020403
 56%|███████████████            | 224/400 [1:35:29<1:11:43, 24.45s/it]2021-12-11 21:01:49,865 iteration 3809 : loss : 0.029621, loss_ce: 0.015504
2021-12-11 21:01:51,280 iteration 3810 : loss : 0.043241, loss_ce: 0.016876
2021-12-11 21:01:52,628 iteration 3811 : loss : 0.030072, loss_ce: 0.014433
2021-12-11 21:01:54,013 iteration 3812 : loss : 0.040937, loss_ce: 0.014680
2021-12-11 21:01:55,314 iteration 3813 : loss : 0.027132, loss_ce: 0.014915
2021-12-11 21:01:56,745 iteration 3814 : loss : 0.036114, loss_ce: 0.019992
2021-12-11 21:01:58,201 iteration 3815 : loss : 0.042998, loss_ce: 0.020736
2021-12-11 21:01:59,538 iteration 3816 : loss : 0.028423, loss_ce: 0.013097
2021-12-11 21:02:00,811 iteration 3817 : loss : 0.028800, loss_ce: 0.012628
2021-12-11 21:02:02,155 iteration 3818 : loss : 0.039983, loss_ce: 0.014065
2021-12-11 21:02:03,571 iteration 3819 : loss : 0.043505, loss_ce: 0.019658
2021-12-11 21:02:04,878 iteration 3820 : loss : 0.025241, loss_ce: 0.014088
2021-12-11 21:02:06,314 iteration 3821 : loss : 0.035739, loss_ce: 0.018647
2021-12-11 21:02:07,673 iteration 3822 : loss : 0.039442, loss_ce: 0.016314
2021-12-11 21:02:09,015 iteration 3823 : loss : 0.041214, loss_ce: 0.017510
2021-12-11 21:02:10,421 iteration 3824 : loss : 0.050632, loss_ce: 0.020782
2021-12-11 21:02:10,421 Training Data Eval:
2021-12-11 21:02:17,239   Average segmentation loss on training set: 0.0206
2021-12-11 21:02:17,239 Validation Data Eval:
2021-12-11 21:02:19,589   Average segmentation loss on validation set: 0.0949
2021-12-11 21:02:20,878 iteration 3825 : loss : 0.035527, loss_ce: 0.015123
 56%|███████████████▏           | 225/400 [1:36:02<1:18:19, 26.85s/it]2021-12-11 21:02:22,386 iteration 3826 : loss : 0.051214, loss_ce: 0.023910
2021-12-11 21:02:23,836 iteration 3827 : loss : 0.039968, loss_ce: 0.020757
2021-12-11 21:02:25,182 iteration 3828 : loss : 0.030181, loss_ce: 0.020650
2021-12-11 21:02:26,523 iteration 3829 : loss : 0.024963, loss_ce: 0.012368
2021-12-11 21:02:27,977 iteration 3830 : loss : 0.051009, loss_ce: 0.017145
2021-12-11 21:02:29,407 iteration 3831 : loss : 0.036846, loss_ce: 0.018965
2021-12-11 21:02:30,815 iteration 3832 : loss : 0.033709, loss_ce: 0.017973
2021-12-11 21:02:32,215 iteration 3833 : loss : 0.063769, loss_ce: 0.024284
2021-12-11 21:02:33,544 iteration 3834 : loss : 0.033928, loss_ce: 0.015846
2021-12-11 21:02:34,937 iteration 3835 : loss : 0.065862, loss_ce: 0.019158
2021-12-11 21:02:36,251 iteration 3836 : loss : 0.030417, loss_ce: 0.015963
2021-12-11 21:02:37,541 iteration 3837 : loss : 0.027652, loss_ce: 0.014078
2021-12-11 21:02:38,953 iteration 3838 : loss : 0.038979, loss_ce: 0.020255
2021-12-11 21:02:40,283 iteration 3839 : loss : 0.032864, loss_ce: 0.013470
2021-12-11 21:02:41,668 iteration 3840 : loss : 0.035854, loss_ce: 0.017888
2021-12-11 21:02:42,967 iteration 3841 : loss : 0.032844, loss_ce: 0.015067
2021-12-11 21:02:44,379 iteration 3842 : loss : 0.091463, loss_ce: 0.022764
 56%|███████████████▎           | 226/400 [1:36:25<1:14:57, 25.85s/it]2021-12-11 21:02:45,812 iteration 3843 : loss : 0.039189, loss_ce: 0.015508
2021-12-11 21:02:47,232 iteration 3844 : loss : 0.037371, loss_ce: 0.017967
2021-12-11 21:02:48,519 iteration 3845 : loss : 0.032087, loss_ce: 0.014757
2021-12-11 21:02:49,872 iteration 3846 : loss : 0.046245, loss_ce: 0.020789
2021-12-11 21:02:51,340 iteration 3847 : loss : 0.053955, loss_ce: 0.023898
2021-12-11 21:02:52,688 iteration 3848 : loss : 0.039370, loss_ce: 0.015866
2021-12-11 21:02:54,025 iteration 3849 : loss : 0.030420, loss_ce: 0.013811
2021-12-11 21:02:55,452 iteration 3850 : loss : 0.046839, loss_ce: 0.019428
2021-12-11 21:02:56,909 iteration 3851 : loss : 0.064483, loss_ce: 0.023424
2021-12-11 21:02:58,325 iteration 3852 : loss : 0.040461, loss_ce: 0.019042
2021-12-11 21:02:59,698 iteration 3853 : loss : 0.042778, loss_ce: 0.020715
2021-12-11 21:03:00,988 iteration 3854 : loss : 0.026380, loss_ce: 0.013197
2021-12-11 21:03:02,340 iteration 3855 : loss : 0.028366, loss_ce: 0.011830
2021-12-11 21:03:03,805 iteration 3856 : loss : 0.054656, loss_ce: 0.029847
2021-12-11 21:03:05,153 iteration 3857 : loss : 0.051166, loss_ce: 0.020570
2021-12-11 21:03:06,507 iteration 3858 : loss : 0.041165, loss_ce: 0.023361
2021-12-11 21:03:07,834 iteration 3859 : loss : 0.038653, loss_ce: 0.020505
 57%|███████████████▎           | 227/400 [1:36:49<1:12:27, 25.13s/it]2021-12-11 21:03:09,212 iteration 3860 : loss : 0.032061, loss_ce: 0.018298
2021-12-11 21:03:10,542 iteration 3861 : loss : 0.034236, loss_ce: 0.017480
2021-12-11 21:03:11,862 iteration 3862 : loss : 0.035647, loss_ce: 0.014077
2021-12-11 21:03:13,245 iteration 3863 : loss : 0.044469, loss_ce: 0.023028
2021-12-11 21:03:14,531 iteration 3864 : loss : 0.031384, loss_ce: 0.016571
2021-12-11 21:03:15,916 iteration 3865 : loss : 0.029620, loss_ce: 0.015911
2021-12-11 21:03:17,278 iteration 3866 : loss : 0.040978, loss_ce: 0.021371
2021-12-11 21:03:18,684 iteration 3867 : loss : 0.058698, loss_ce: 0.024896
2021-12-11 21:03:19,993 iteration 3868 : loss : 0.033846, loss_ce: 0.015585
2021-12-11 21:03:21,444 iteration 3869 : loss : 0.054343, loss_ce: 0.019797
2021-12-11 21:03:22,806 iteration 3870 : loss : 0.034060, loss_ce: 0.014426
2021-12-11 21:03:24,216 iteration 3871 : loss : 0.043334, loss_ce: 0.020495
2021-12-11 21:03:25,582 iteration 3872 : loss : 0.028299, loss_ce: 0.013020
2021-12-11 21:03:26,932 iteration 3873 : loss : 0.032019, loss_ce: 0.013631
2021-12-11 21:03:28,350 iteration 3874 : loss : 0.040330, loss_ce: 0.017789
2021-12-11 21:03:29,763 iteration 3875 : loss : 0.048568, loss_ce: 0.017706
2021-12-11 21:03:31,220 iteration 3876 : loss : 0.038557, loss_ce: 0.017665
 57%|███████████████▍           | 228/400 [1:37:12<1:10:32, 24.61s/it]2021-12-11 21:03:32,618 iteration 3877 : loss : 0.027554, loss_ce: 0.014004
2021-12-11 21:03:34,023 iteration 3878 : loss : 0.039158, loss_ce: 0.019178
2021-12-11 21:03:35,460 iteration 3879 : loss : 0.035937, loss_ce: 0.022136
2021-12-11 21:03:36,831 iteration 3880 : loss : 0.034720, loss_ce: 0.014569
2021-12-11 21:03:38,222 iteration 3881 : loss : 0.032876, loss_ce: 0.013882
2021-12-11 21:03:39,648 iteration 3882 : loss : 0.055147, loss_ce: 0.028404
2021-12-11 21:03:41,032 iteration 3883 : loss : 0.035064, loss_ce: 0.015794
2021-12-11 21:03:42,414 iteration 3884 : loss : 0.028515, loss_ce: 0.013072
2021-12-11 21:03:43,743 iteration 3885 : loss : 0.028201, loss_ce: 0.012801
2021-12-11 21:03:45,152 iteration 3886 : loss : 0.034322, loss_ce: 0.017591
2021-12-11 21:03:46,540 iteration 3887 : loss : 0.032551, loss_ce: 0.015043
2021-12-11 21:03:47,982 iteration 3888 : loss : 0.041217, loss_ce: 0.022950
2021-12-11 21:03:49,322 iteration 3889 : loss : 0.044718, loss_ce: 0.015794
2021-12-11 21:03:50,677 iteration 3890 : loss : 0.030676, loss_ce: 0.013530
2021-12-11 21:03:52,089 iteration 3891 : loss : 0.045621, loss_ce: 0.023379
2021-12-11 21:03:53,466 iteration 3892 : loss : 0.031011, loss_ce: 0.013642
2021-12-11 21:03:54,802 iteration 3893 : loss : 0.058450, loss_ce: 0.019916
 57%|███████████████▍           | 229/400 [1:37:36<1:09:14, 24.30s/it]2021-12-11 21:03:56,228 iteration 3894 : loss : 0.042032, loss_ce: 0.018114
2021-12-11 21:03:57,643 iteration 3895 : loss : 0.032274, loss_ce: 0.018365
2021-12-11 21:03:58,958 iteration 3896 : loss : 0.026420, loss_ce: 0.012955
2021-12-11 21:04:00,387 iteration 3897 : loss : 0.027000, loss_ce: 0.011820
2021-12-11 21:04:01,787 iteration 3898 : loss : 0.042017, loss_ce: 0.014132
2021-12-11 21:04:03,183 iteration 3899 : loss : 0.031635, loss_ce: 0.012420
2021-12-11 21:04:04,577 iteration 3900 : loss : 0.031859, loss_ce: 0.016701
2021-12-11 21:04:05,877 iteration 3901 : loss : 0.029614, loss_ce: 0.017057
2021-12-11 21:04:07,246 iteration 3902 : loss : 0.030247, loss_ce: 0.013201
2021-12-11 21:04:08,574 iteration 3903 : loss : 0.033191, loss_ce: 0.013951
2021-12-11 21:04:09,965 iteration 3904 : loss : 0.031861, loss_ce: 0.014757
2021-12-11 21:04:11,346 iteration 3905 : loss : 0.045545, loss_ce: 0.024988
2021-12-11 21:04:12,721 iteration 3906 : loss : 0.031992, loss_ce: 0.015108
2021-12-11 21:04:14,095 iteration 3907 : loss : 0.041239, loss_ce: 0.015018
2021-12-11 21:04:15,390 iteration 3908 : loss : 0.032527, loss_ce: 0.014346
2021-12-11 21:04:16,696 iteration 3909 : loss : 0.035462, loss_ce: 0.018198
2021-12-11 21:04:16,696 Training Data Eval:
2021-12-11 21:04:23,520   Average segmentation loss on training set: 0.0190
2021-12-11 21:04:23,520 Validation Data Eval:
2021-12-11 21:04:25,881   Average segmentation loss on validation set: 0.0928
2021-12-11 21:04:27,247 iteration 3910 : loss : 0.035693, loss_ce: 0.017346
 57%|███████████████▌           | 230/400 [1:38:08<1:15:46, 26.74s/it]2021-12-11 21:04:28,598 iteration 3911 : loss : 0.036672, loss_ce: 0.018641
2021-12-11 21:04:29,946 iteration 3912 : loss : 0.043101, loss_ce: 0.016626
2021-12-11 21:04:31,328 iteration 3913 : loss : 0.053443, loss_ce: 0.026501
2021-12-11 21:04:32,616 iteration 3914 : loss : 0.020987, loss_ce: 0.012299
2021-12-11 21:04:34,006 iteration 3915 : loss : 0.042198, loss_ce: 0.016424
2021-12-11 21:04:35,397 iteration 3916 : loss : 0.031724, loss_ce: 0.015257
2021-12-11 21:04:36,805 iteration 3917 : loss : 0.045675, loss_ce: 0.015624
2021-12-11 21:04:38,179 iteration 3918 : loss : 0.028562, loss_ce: 0.012609
2021-12-11 21:04:39,541 iteration 3919 : loss : 0.035901, loss_ce: 0.015722
2021-12-11 21:04:40,941 iteration 3920 : loss : 0.086157, loss_ce: 0.017841
2021-12-11 21:04:42,312 iteration 3921 : loss : 0.036341, loss_ce: 0.016050
2021-12-11 21:04:43,710 iteration 3922 : loss : 0.045353, loss_ce: 0.015907
2021-12-11 21:04:45,179 iteration 3923 : loss : 0.050125, loss_ce: 0.024538
2021-12-11 21:04:46,521 iteration 3924 : loss : 0.031265, loss_ce: 0.014819
2021-12-11 21:04:47,824 iteration 3925 : loss : 0.029031, loss_ce: 0.016358
2021-12-11 21:04:49,325 iteration 3926 : loss : 0.041569, loss_ce: 0.020324
2021-12-11 21:04:50,721 iteration 3927 : loss : 0.046995, loss_ce: 0.024922
 58%|███████████████▌           | 231/400 [1:38:32<1:12:33, 25.76s/it]2021-12-11 21:04:52,134 iteration 3928 : loss : 0.035615, loss_ce: 0.016771
2021-12-11 21:04:53,470 iteration 3929 : loss : 0.044088, loss_ce: 0.028146
2021-12-11 21:04:54,831 iteration 3930 : loss : 0.034734, loss_ce: 0.013999
2021-12-11 21:04:56,132 iteration 3931 : loss : 0.030783, loss_ce: 0.011808
2021-12-11 21:04:57,511 iteration 3932 : loss : 0.063344, loss_ce: 0.025551
2021-12-11 21:04:58,870 iteration 3933 : loss : 0.031777, loss_ce: 0.017365
2021-12-11 21:05:00,275 iteration 3934 : loss : 0.042027, loss_ce: 0.018436
2021-12-11 21:05:01,636 iteration 3935 : loss : 0.036032, loss_ce: 0.017747
2021-12-11 21:05:02,970 iteration 3936 : loss : 0.033543, loss_ce: 0.015046
2021-12-11 21:05:04,338 iteration 3937 : loss : 0.055906, loss_ce: 0.019263
2021-12-11 21:05:05,714 iteration 3938 : loss : 0.032254, loss_ce: 0.012752
2021-12-11 21:05:07,146 iteration 3939 : loss : 0.047296, loss_ce: 0.015888
2021-12-11 21:05:08,516 iteration 3940 : loss : 0.033466, loss_ce: 0.017406
2021-12-11 21:05:09,887 iteration 3941 : loss : 0.023312, loss_ce: 0.010326
2021-12-11 21:05:11,225 iteration 3942 : loss : 0.033277, loss_ce: 0.019246
2021-12-11 21:05:12,565 iteration 3943 : loss : 0.044598, loss_ce: 0.019404
2021-12-11 21:05:13,911 iteration 3944 : loss : 0.043448, loss_ce: 0.016757
 58%|███████████████▋           | 232/400 [1:38:55<1:09:57, 24.99s/it]2021-12-11 21:05:15,378 iteration 3945 : loss : 0.042232, loss_ce: 0.016454
2021-12-11 21:05:16,729 iteration 3946 : loss : 0.028558, loss_ce: 0.013957
2021-12-11 21:05:18,066 iteration 3947 : loss : 0.036683, loss_ce: 0.017647
2021-12-11 21:05:19,484 iteration 3948 : loss : 0.050272, loss_ce: 0.024012
2021-12-11 21:05:20,917 iteration 3949 : loss : 0.046085, loss_ce: 0.016572
2021-12-11 21:05:22,270 iteration 3950 : loss : 0.028829, loss_ce: 0.015988
2021-12-11 21:05:23,581 iteration 3951 : loss : 0.031318, loss_ce: 0.010883
2021-12-11 21:05:25,046 iteration 3952 : loss : 0.042163, loss_ce: 0.019340
2021-12-11 21:05:26,423 iteration 3953 : loss : 0.024625, loss_ce: 0.013017
2021-12-11 21:05:27,750 iteration 3954 : loss : 0.026196, loss_ce: 0.012811
2021-12-11 21:05:29,153 iteration 3955 : loss : 0.037358, loss_ce: 0.017842
2021-12-11 21:05:30,564 iteration 3956 : loss : 0.037190, loss_ce: 0.020386
2021-12-11 21:05:31,991 iteration 3957 : loss : 0.073601, loss_ce: 0.025885
2021-12-11 21:05:33,415 iteration 3958 : loss : 0.051727, loss_ce: 0.024368
2021-12-11 21:05:34,707 iteration 3959 : loss : 0.033007, loss_ce: 0.012605
2021-12-11 21:05:36,113 iteration 3960 : loss : 0.030208, loss_ce: 0.013899
2021-12-11 21:05:37,463 iteration 3961 : loss : 0.033215, loss_ce: 0.011457
 58%|███████████████▋           | 233/400 [1:39:18<1:08:20, 24.56s/it]2021-12-11 21:05:38,890 iteration 3962 : loss : 0.046217, loss_ce: 0.021350
2021-12-11 21:05:40,317 iteration 3963 : loss : 0.044672, loss_ce: 0.021723
2021-12-11 21:05:41,639 iteration 3964 : loss : 0.035907, loss_ce: 0.015812
2021-12-11 21:05:43,025 iteration 3965 : loss : 0.028550, loss_ce: 0.015111
2021-12-11 21:05:44,482 iteration 3966 : loss : 0.038756, loss_ce: 0.019770
2021-12-11 21:05:45,860 iteration 3967 : loss : 0.046156, loss_ce: 0.015937
2021-12-11 21:05:47,224 iteration 3968 : loss : 0.030677, loss_ce: 0.014867
2021-12-11 21:05:48,565 iteration 3969 : loss : 0.027707, loss_ce: 0.011745
2021-12-11 21:05:49,937 iteration 3970 : loss : 0.053158, loss_ce: 0.017689
2021-12-11 21:05:51,308 iteration 3971 : loss : 0.034318, loss_ce: 0.013780
2021-12-11 21:05:52,690 iteration 3972 : loss : 0.054352, loss_ce: 0.024725
2021-12-11 21:05:54,125 iteration 3973 : loss : 0.043833, loss_ce: 0.020502
2021-12-11 21:05:55,564 iteration 3974 : loss : 0.031056, loss_ce: 0.017247
2021-12-11 21:05:56,928 iteration 3975 : loss : 0.033870, loss_ce: 0.013561
2021-12-11 21:05:58,303 iteration 3976 : loss : 0.033694, loss_ce: 0.016429
2021-12-11 21:05:59,700 iteration 3977 : loss : 0.041318, loss_ce: 0.021888
2021-12-11 21:06:01,086 iteration 3978 : loss : 0.041165, loss_ce: 0.021210
 58%|███████████████▊           | 234/400 [1:39:42<1:07:09, 24.28s/it]2021-12-11 21:06:02,578 iteration 3979 : loss : 0.052868, loss_ce: 0.028612
2021-12-11 21:06:03,887 iteration 3980 : loss : 0.031697, loss_ce: 0.015547
2021-12-11 21:06:05,256 iteration 3981 : loss : 0.046609, loss_ce: 0.016039
2021-12-11 21:06:06,636 iteration 3982 : loss : 0.031686, loss_ce: 0.016448
2021-12-11 21:06:08,019 iteration 3983 : loss : 0.056107, loss_ce: 0.018007
2021-12-11 21:06:09,431 iteration 3984 : loss : 0.046375, loss_ce: 0.024041
2021-12-11 21:06:10,848 iteration 3985 : loss : 0.032959, loss_ce: 0.014541
2021-12-11 21:06:12,281 iteration 3986 : loss : 0.046236, loss_ce: 0.018472
2021-12-11 21:06:13,615 iteration 3987 : loss : 0.039837, loss_ce: 0.020614
2021-12-11 21:06:14,981 iteration 3988 : loss : 0.035582, loss_ce: 0.015417
2021-12-11 21:06:16,377 iteration 3989 : loss : 0.031898, loss_ce: 0.014259
2021-12-11 21:06:17,768 iteration 3990 : loss : 0.041929, loss_ce: 0.022781
2021-12-11 21:06:19,174 iteration 3991 : loss : 0.040092, loss_ce: 0.021837
2021-12-11 21:06:20,574 iteration 3992 : loss : 0.026249, loss_ce: 0.013201
2021-12-11 21:06:21,948 iteration 3993 : loss : 0.035520, loss_ce: 0.016984
2021-12-11 21:06:23,358 iteration 3994 : loss : 0.028666, loss_ce: 0.012609
2021-12-11 21:06:23,358 Training Data Eval:
2021-12-11 21:06:30,203   Average segmentation loss on training set: 0.0184
2021-12-11 21:06:30,203 Validation Data Eval:
2021-12-11 21:06:32,560   Average segmentation loss on validation set: 0.0899
2021-12-11 21:06:34,019 iteration 3995 : loss : 0.041915, loss_ce: 0.017942
 59%|███████████████▊           | 235/400 [1:40:15<1:13:54, 26.87s/it]2021-12-11 21:06:35,443 iteration 3996 : loss : 0.037029, loss_ce: 0.019451
2021-12-11 21:06:36,768 iteration 3997 : loss : 0.033284, loss_ce: 0.016842
2021-12-11 21:06:38,084 iteration 3998 : loss : 0.028675, loss_ce: 0.011413
2021-12-11 21:06:39,507 iteration 3999 : loss : 0.056028, loss_ce: 0.019748
2021-12-11 21:06:40,888 iteration 4000 : loss : 0.028346, loss_ce: 0.014095
2021-12-11 21:06:42,301 iteration 4001 : loss : 0.047416, loss_ce: 0.026045
2021-12-11 21:06:43,596 iteration 4002 : loss : 0.029759, loss_ce: 0.013517
2021-12-11 21:06:45,014 iteration 4003 : loss : 0.041774, loss_ce: 0.017636
2021-12-11 21:06:46,429 iteration 4004 : loss : 0.055345, loss_ce: 0.022425
2021-12-11 21:06:47,832 iteration 4005 : loss : 0.030652, loss_ce: 0.016042
2021-12-11 21:06:49,151 iteration 4006 : loss : 0.040373, loss_ce: 0.017309
2021-12-11 21:06:50,515 iteration 4007 : loss : 0.040852, loss_ce: 0.016747
2021-12-11 21:06:51,873 iteration 4008 : loss : 0.031420, loss_ce: 0.017045
2021-12-11 21:06:53,243 iteration 4009 : loss : 0.023719, loss_ce: 0.011600
2021-12-11 21:06:54,627 iteration 4010 : loss : 0.034766, loss_ce: 0.016428
2021-12-11 21:06:55,987 iteration 4011 : loss : 0.035748, loss_ce: 0.014836
2021-12-11 21:06:57,382 iteration 4012 : loss : 0.037899, loss_ce: 0.017547
 59%|███████████████▉           | 236/400 [1:40:38<1:10:35, 25.82s/it]2021-12-11 21:06:58,713 iteration 4013 : loss : 0.023205, loss_ce: 0.013554
2021-12-11 21:07:00,139 iteration 4014 : loss : 0.033658, loss_ce: 0.019310
2021-12-11 21:07:01,462 iteration 4015 : loss : 0.040865, loss_ce: 0.023143
2021-12-11 21:07:02,887 iteration 4016 : loss : 0.037696, loss_ce: 0.017588
2021-12-11 21:07:04,283 iteration 4017 : loss : 0.041679, loss_ce: 0.016135
2021-12-11 21:07:05,598 iteration 4018 : loss : 0.032761, loss_ce: 0.014786
2021-12-11 21:07:06,985 iteration 4019 : loss : 0.035507, loss_ce: 0.015179
2021-12-11 21:07:08,332 iteration 4020 : loss : 0.034170, loss_ce: 0.012695
2021-12-11 21:07:09,721 iteration 4021 : loss : 0.041740, loss_ce: 0.014223
2021-12-11 21:07:11,186 iteration 4022 : loss : 0.037904, loss_ce: 0.015430
2021-12-11 21:07:12,498 iteration 4023 : loss : 0.026212, loss_ce: 0.015183
2021-12-11 21:07:13,920 iteration 4024 : loss : 0.040730, loss_ce: 0.016129
2021-12-11 21:07:15,313 iteration 4025 : loss : 0.037429, loss_ce: 0.016158
2021-12-11 21:07:16,746 iteration 4026 : loss : 0.042793, loss_ce: 0.017393
2021-12-11 21:07:18,049 iteration 4027 : loss : 0.023316, loss_ce: 0.010639
2021-12-11 21:07:19,543 iteration 4028 : loss : 0.034228, loss_ce: 0.017656
2021-12-11 21:07:21,012 iteration 4029 : loss : 0.035657, loss_ce: 0.016733
 59%|███████████████▉           | 237/400 [1:41:02<1:08:22, 25.17s/it]2021-12-11 21:07:22,434 iteration 4030 : loss : 0.037115, loss_ce: 0.017259
2021-12-11 21:07:23,806 iteration 4031 : loss : 0.043303, loss_ce: 0.022091
2021-12-11 21:07:25,117 iteration 4032 : loss : 0.023613, loss_ce: 0.010791
2021-12-11 21:07:26,448 iteration 4033 : loss : 0.036870, loss_ce: 0.016438
2021-12-11 21:07:27,846 iteration 4034 : loss : 0.043847, loss_ce: 0.020370
2021-12-11 21:07:29,251 iteration 4035 : loss : 0.046874, loss_ce: 0.017082
2021-12-11 21:07:30,633 iteration 4036 : loss : 0.029821, loss_ce: 0.015991
2021-12-11 21:07:31,966 iteration 4037 : loss : 0.038021, loss_ce: 0.021532
2021-12-11 21:07:33,331 iteration 4038 : loss : 0.035213, loss_ce: 0.016576
2021-12-11 21:07:34,709 iteration 4039 : loss : 0.048170, loss_ce: 0.018373
2021-12-11 21:07:36,071 iteration 4040 : loss : 0.030985, loss_ce: 0.013010
2021-12-11 21:07:37,493 iteration 4041 : loss : 0.034617, loss_ce: 0.016431
2021-12-11 21:07:38,900 iteration 4042 : loss : 0.035113, loss_ce: 0.015978
2021-12-11 21:07:40,280 iteration 4043 : loss : 0.032636, loss_ce: 0.015864
2021-12-11 21:07:41,644 iteration 4044 : loss : 0.035466, loss_ce: 0.018303
2021-12-11 21:07:43,023 iteration 4045 : loss : 0.032093, loss_ce: 0.013418
2021-12-11 21:07:44,450 iteration 4046 : loss : 0.048655, loss_ce: 0.019706
 60%|████████████████           | 238/400 [1:41:25<1:06:32, 24.65s/it]2021-12-11 21:07:45,910 iteration 4047 : loss : 0.033725, loss_ce: 0.014953
2021-12-11 21:07:47,280 iteration 4048 : loss : 0.049563, loss_ce: 0.019120
2021-12-11 21:07:48,648 iteration 4049 : loss : 0.028786, loss_ce: 0.012422
2021-12-11 21:07:49,970 iteration 4050 : loss : 0.041075, loss_ce: 0.015870
2021-12-11 21:07:51,333 iteration 4051 : loss : 0.029543, loss_ce: 0.014685
2021-12-11 21:07:52,663 iteration 4052 : loss : 0.028784, loss_ce: 0.013947
2021-12-11 21:07:54,052 iteration 4053 : loss : 0.046779, loss_ce: 0.013793
2021-12-11 21:07:55,471 iteration 4054 : loss : 0.037534, loss_ce: 0.019535
2021-12-11 21:07:56,851 iteration 4055 : loss : 0.029719, loss_ce: 0.016495
2021-12-11 21:07:58,221 iteration 4056 : loss : 0.039661, loss_ce: 0.020393
2021-12-11 21:07:59,540 iteration 4057 : loss : 0.037521, loss_ce: 0.015820
2021-12-11 21:08:00,876 iteration 4058 : loss : 0.024068, loss_ce: 0.012616
2021-12-11 21:08:02,281 iteration 4059 : loss : 0.043775, loss_ce: 0.017358
2021-12-11 21:08:03,615 iteration 4060 : loss : 0.030152, loss_ce: 0.015185
2021-12-11 21:08:04,897 iteration 4061 : loss : 0.033434, loss_ce: 0.018182
2021-12-11 21:08:06,235 iteration 4062 : loss : 0.034403, loss_ce: 0.017097
2021-12-11 21:08:07,658 iteration 4063 : loss : 0.038183, loss_ce: 0.017304
 60%|████████████████▏          | 239/400 [1:41:48<1:04:58, 24.21s/it]2021-12-11 21:08:09,084 iteration 4064 : loss : 0.024887, loss_ce: 0.011894
2021-12-11 21:08:10,423 iteration 4065 : loss : 0.035505, loss_ce: 0.012736
2021-12-11 21:08:11,794 iteration 4066 : loss : 0.030711, loss_ce: 0.014877
2021-12-11 21:08:13,155 iteration 4067 : loss : 0.034609, loss_ce: 0.015398
2021-12-11 21:08:14,540 iteration 4068 : loss : 0.045572, loss_ce: 0.019504
2021-12-11 21:08:15,930 iteration 4069 : loss : 0.052010, loss_ce: 0.028414
2021-12-11 21:08:17,260 iteration 4070 : loss : 0.041712, loss_ce: 0.016680
2021-12-11 21:08:18,631 iteration 4071 : loss : 0.033119, loss_ce: 0.014997
2021-12-11 21:08:20,003 iteration 4072 : loss : 0.033197, loss_ce: 0.015587
2021-12-11 21:08:21,407 iteration 4073 : loss : 0.040766, loss_ce: 0.019923
2021-12-11 21:08:22,786 iteration 4074 : loss : 0.033474, loss_ce: 0.016450
2021-12-11 21:08:24,252 iteration 4075 : loss : 0.044245, loss_ce: 0.017625
2021-12-11 21:08:25,652 iteration 4076 : loss : 0.026318, loss_ce: 0.013328
2021-12-11 21:08:27,090 iteration 4077 : loss : 0.050793, loss_ce: 0.021266
2021-12-11 21:08:28,535 iteration 4078 : loss : 0.029205, loss_ce: 0.013221
2021-12-11 21:08:29,881 iteration 4079 : loss : 0.025034, loss_ce: 0.012207
2021-12-11 21:08:29,881 Training Data Eval:
2021-12-11 21:08:36,705   Average segmentation loss on training set: 0.0186
2021-12-11 21:08:36,705 Validation Data Eval:
2021-12-11 21:08:39,068   Average segmentation loss on validation set: 0.0851
2021-12-11 21:08:40,509 iteration 4080 : loss : 0.049894, loss_ce: 0.022435
 60%|████████████████▏          | 240/400 [1:42:21<1:11:29, 26.81s/it]2021-12-11 21:08:41,912 iteration 4081 : loss : 0.027799, loss_ce: 0.014517
2021-12-11 21:08:43,286 iteration 4082 : loss : 0.036505, loss_ce: 0.018271
2021-12-11 21:08:44,644 iteration 4083 : loss : 0.031853, loss_ce: 0.016461
2021-12-11 21:08:46,055 iteration 4084 : loss : 0.043588, loss_ce: 0.016943
2021-12-11 21:08:47,500 iteration 4085 : loss : 0.042023, loss_ce: 0.017680
2021-12-11 21:08:48,937 iteration 4086 : loss : 0.037143, loss_ce: 0.020458
2021-12-11 21:08:50,304 iteration 4087 : loss : 0.039625, loss_ce: 0.014374
2021-12-11 21:08:51,676 iteration 4088 : loss : 0.032425, loss_ce: 0.016926
2021-12-11 21:08:53,083 iteration 4089 : loss : 0.042129, loss_ce: 0.017349
2021-12-11 21:08:54,486 iteration 4090 : loss : 0.029420, loss_ce: 0.014626
2021-12-11 21:08:55,830 iteration 4091 : loss : 0.039491, loss_ce: 0.016885
2021-12-11 21:08:57,198 iteration 4092 : loss : 0.026013, loss_ce: 0.012097
2021-12-11 21:08:58,566 iteration 4093 : loss : 0.024521, loss_ce: 0.013072
2021-12-11 21:08:59,912 iteration 4094 : loss : 0.027749, loss_ce: 0.012538
2021-12-11 21:09:01,267 iteration 4095 : loss : 0.027925, loss_ce: 0.014180
2021-12-11 21:09:02,636 iteration 4096 : loss : 0.031302, loss_ce: 0.015321
2021-12-11 21:09:04,020 iteration 4097 : loss : 0.034539, loss_ce: 0.019555
 60%|████████████████▎          | 241/400 [1:42:45<1:08:24, 25.81s/it]2021-12-11 21:09:05,484 iteration 4098 : loss : 0.030530, loss_ce: 0.015108
2021-12-11 21:09:06,835 iteration 4099 : loss : 0.041494, loss_ce: 0.015933
2021-12-11 21:09:08,149 iteration 4100 : loss : 0.029575, loss_ce: 0.015254
2021-12-11 21:09:09,488 iteration 4101 : loss : 0.033477, loss_ce: 0.016021
2021-12-11 21:09:10,872 iteration 4102 : loss : 0.037752, loss_ce: 0.017668
2021-12-11 21:09:12,219 iteration 4103 : loss : 0.027456, loss_ce: 0.013221
2021-12-11 21:09:13,537 iteration 4104 : loss : 0.033128, loss_ce: 0.016026
2021-12-11 21:09:14,918 iteration 4105 : loss : 0.031687, loss_ce: 0.013844
2021-12-11 21:09:16,238 iteration 4106 : loss : 0.031697, loss_ce: 0.010632
2021-12-11 21:09:17,699 iteration 4107 : loss : 0.041248, loss_ce: 0.022710
2021-12-11 21:09:19,122 iteration 4108 : loss : 0.041647, loss_ce: 0.017816
2021-12-11 21:09:20,444 iteration 4109 : loss : 0.036437, loss_ce: 0.012370
2021-12-11 21:09:21,825 iteration 4110 : loss : 0.035224, loss_ce: 0.018928
2021-12-11 21:09:23,260 iteration 4111 : loss : 0.042513, loss_ce: 0.019303
2021-12-11 21:09:24,615 iteration 4112 : loss : 0.037356, loss_ce: 0.016848
2021-12-11 21:09:26,002 iteration 4113 : loss : 0.032207, loss_ce: 0.015245
2021-12-11 21:09:27,443 iteration 4114 : loss : 0.048007, loss_ce: 0.022445
 60%|████████████████▎          | 242/400 [1:43:08<1:06:05, 25.10s/it]2021-12-11 21:09:28,837 iteration 4115 : loss : 0.040514, loss_ce: 0.015147
2021-12-11 21:09:30,165 iteration 4116 : loss : 0.031355, loss_ce: 0.017036
2021-12-11 21:09:31,671 iteration 4117 : loss : 0.046013, loss_ce: 0.017340
2021-12-11 21:09:33,166 iteration 4118 : loss : 0.035068, loss_ce: 0.018016
2021-12-11 21:09:34,580 iteration 4119 : loss : 0.030934, loss_ce: 0.016650
2021-12-11 21:09:35,985 iteration 4120 : loss : 0.068446, loss_ce: 0.014606
2021-12-11 21:09:37,378 iteration 4121 : loss : 0.033523, loss_ce: 0.017761
2021-12-11 21:09:38,731 iteration 4122 : loss : 0.031053, loss_ce: 0.013873
2021-12-11 21:09:40,179 iteration 4123 : loss : 0.039938, loss_ce: 0.017768
2021-12-11 21:09:41,495 iteration 4124 : loss : 0.039096, loss_ce: 0.016675
2021-12-11 21:09:42,924 iteration 4125 : loss : 0.056402, loss_ce: 0.018168
2021-12-11 21:09:44,328 iteration 4126 : loss : 0.037991, loss_ce: 0.021442
2021-12-11 21:09:45,738 iteration 4127 : loss : 0.040198, loss_ce: 0.018980
2021-12-11 21:09:47,085 iteration 4128 : loss : 0.031388, loss_ce: 0.017361
2021-12-11 21:09:48,521 iteration 4129 : loss : 0.040991, loss_ce: 0.018034
2021-12-11 21:09:49,894 iteration 4130 : loss : 0.029893, loss_ce: 0.017718
2021-12-11 21:09:51,199 iteration 4131 : loss : 0.023140, loss_ce: 0.009407
 61%|████████████████▍          | 243/400 [1:43:32<1:04:37, 24.70s/it]2021-12-11 21:09:52,678 iteration 4132 : loss : 0.040601, loss_ce: 0.019704
2021-12-11 21:09:54,106 iteration 4133 : loss : 0.052522, loss_ce: 0.022216
2021-12-11 21:09:55,414 iteration 4134 : loss : 0.030807, loss_ce: 0.015456
2021-12-11 21:09:56,833 iteration 4135 : loss : 0.038276, loss_ce: 0.017678
2021-12-11 21:09:58,210 iteration 4136 : loss : 0.039164, loss_ce: 0.019516
2021-12-11 21:09:59,565 iteration 4137 : loss : 0.037336, loss_ce: 0.023022
2021-12-11 21:10:00,899 iteration 4138 : loss : 0.047039, loss_ce: 0.027209
2021-12-11 21:10:02,296 iteration 4139 : loss : 0.036441, loss_ce: 0.013749
2021-12-11 21:10:03,723 iteration 4140 : loss : 0.061752, loss_ce: 0.021341
2021-12-11 21:10:05,102 iteration 4141 : loss : 0.058923, loss_ce: 0.019383
2021-12-11 21:10:06,497 iteration 4142 : loss : 0.045648, loss_ce: 0.016553
2021-12-11 21:10:07,917 iteration 4143 : loss : 0.056173, loss_ce: 0.019087
2021-12-11 21:10:09,283 iteration 4144 : loss : 0.044107, loss_ce: 0.019975
2021-12-11 21:10:10,630 iteration 4145 : loss : 0.034565, loss_ce: 0.015553
2021-12-11 21:10:11,997 iteration 4146 : loss : 0.034123, loss_ce: 0.014162
2021-12-11 21:10:13,340 iteration 4147 : loss : 0.034203, loss_ce: 0.014550
2021-12-11 21:10:14,623 iteration 4148 : loss : 0.026672, loss_ce: 0.013553
 61%|████████████████▍          | 244/400 [1:43:55<1:03:13, 24.32s/it]2021-12-11 21:10:16,166 iteration 4149 : loss : 0.061222, loss_ce: 0.021999
2021-12-11 21:10:17,473 iteration 4150 : loss : 0.032551, loss_ce: 0.011643
2021-12-11 21:10:18,884 iteration 4151 : loss : 0.032990, loss_ce: 0.018216
2021-12-11 21:10:20,179 iteration 4152 : loss : 0.028370, loss_ce: 0.013377
2021-12-11 21:10:21,587 iteration 4153 : loss : 0.044088, loss_ce: 0.015346
2021-12-11 21:10:22,984 iteration 4154 : loss : 0.040207, loss_ce: 0.018733
2021-12-11 21:10:24,428 iteration 4155 : loss : 0.036824, loss_ce: 0.015530
2021-12-11 21:10:25,715 iteration 4156 : loss : 0.019845, loss_ce: 0.010280
2021-12-11 21:10:27,111 iteration 4157 : loss : 0.046915, loss_ce: 0.019160
2021-12-11 21:10:28,485 iteration 4158 : loss : 0.037806, loss_ce: 0.016753
2021-12-11 21:10:29,892 iteration 4159 : loss : 0.026464, loss_ce: 0.014112
2021-12-11 21:10:31,273 iteration 4160 : loss : 0.032253, loss_ce: 0.016965
2021-12-11 21:10:32,648 iteration 4161 : loss : 0.025588, loss_ce: 0.013658
2021-12-11 21:10:33,999 iteration 4162 : loss : 0.023894, loss_ce: 0.011633
2021-12-11 21:10:35,404 iteration 4163 : loss : 0.048577, loss_ce: 0.016320
2021-12-11 21:10:36,816 iteration 4164 : loss : 0.042449, loss_ce: 0.023797
2021-12-11 21:10:36,816 Training Data Eval:
2021-12-11 21:10:43,643   Average segmentation loss on training set: 0.0175
2021-12-11 21:10:43,643 Validation Data Eval:
2021-12-11 21:10:46,001   Average segmentation loss on validation set: 0.0748
2021-12-11 21:10:48,437 Found new lowest validation loss at iteration 4164! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_best_val_loss_seed100.pth
2021-12-11 21:10:49,874 iteration 4165 : loss : 0.049544, loss_ce: 0.022019
 61%|████████████████▌          | 245/400 [1:44:31<1:11:17, 27.60s/it]2021-12-11 21:10:51,261 iteration 4166 : loss : 0.039325, loss_ce: 0.021837
2021-12-11 21:10:52,648 iteration 4167 : loss : 0.037453, loss_ce: 0.014390
2021-12-11 21:10:54,056 iteration 4168 : loss : 0.038525, loss_ce: 0.017692
2021-12-11 21:10:55,444 iteration 4169 : loss : 0.037187, loss_ce: 0.017181
2021-12-11 21:10:56,857 iteration 4170 : loss : 0.038228, loss_ce: 0.017911
2021-12-11 21:10:58,198 iteration 4171 : loss : 0.039483, loss_ce: 0.019430
2021-12-11 21:10:59,559 iteration 4172 : loss : 0.034356, loss_ce: 0.016024
2021-12-11 21:11:00,860 iteration 4173 : loss : 0.027658, loss_ce: 0.011843
2021-12-11 21:11:02,275 iteration 4174 : loss : 0.037306, loss_ce: 0.016112
2021-12-11 21:11:03,670 iteration 4175 : loss : 0.034373, loss_ce: 0.012322
2021-12-11 21:11:05,084 iteration 4176 : loss : 0.040590, loss_ce: 0.016508
2021-12-11 21:11:06,406 iteration 4177 : loss : 0.026303, loss_ce: 0.014664
2021-12-11 21:11:07,773 iteration 4178 : loss : 0.038214, loss_ce: 0.018311
2021-12-11 21:11:09,193 iteration 4179 : loss : 0.032614, loss_ce: 0.014086
2021-12-11 21:11:10,634 iteration 4180 : loss : 0.037039, loss_ce: 0.015104
2021-12-11 21:11:12,025 iteration 4181 : loss : 0.038111, loss_ce: 0.018121
2021-12-11 21:11:13,448 iteration 4182 : loss : 0.036996, loss_ce: 0.015846
 62%|████████████████▌          | 246/400 [1:44:54<1:07:43, 26.39s/it]2021-12-11 21:11:14,880 iteration 4183 : loss : 0.063900, loss_ce: 0.020341
2021-12-11 21:11:16,232 iteration 4184 : loss : 0.032774, loss_ce: 0.013616
2021-12-11 21:11:17,632 iteration 4185 : loss : 0.041762, loss_ce: 0.016422
2021-12-11 21:11:18,984 iteration 4186 : loss : 0.032844, loss_ce: 0.015383
2021-12-11 21:11:20,311 iteration 4187 : loss : 0.033905, loss_ce: 0.017449
2021-12-11 21:11:21,670 iteration 4188 : loss : 0.036552, loss_ce: 0.016630
2021-12-11 21:11:22,970 iteration 4189 : loss : 0.033385, loss_ce: 0.013792
2021-12-11 21:11:24,435 iteration 4190 : loss : 0.048695, loss_ce: 0.023970
2021-12-11 21:11:25,848 iteration 4191 : loss : 0.031856, loss_ce: 0.011755
2021-12-11 21:11:27,169 iteration 4192 : loss : 0.025575, loss_ce: 0.015368
2021-12-11 21:11:28,475 iteration 4193 : loss : 0.029498, loss_ce: 0.013551
2021-12-11 21:11:29,840 iteration 4194 : loss : 0.030131, loss_ce: 0.012920
2021-12-11 21:11:31,177 iteration 4195 : loss : 0.026724, loss_ce: 0.014234
2021-12-11 21:11:32,553 iteration 4196 : loss : 0.029759, loss_ce: 0.015113
2021-12-11 21:11:33,883 iteration 4197 : loss : 0.030286, loss_ce: 0.014839
2021-12-11 21:11:35,195 iteration 4198 : loss : 0.043662, loss_ce: 0.015053
2021-12-11 21:11:36,604 iteration 4199 : loss : 0.034921, loss_ce: 0.016577
 62%|████████████████▋          | 247/400 [1:45:17<1:04:49, 25.42s/it]2021-12-11 21:11:38,026 iteration 4200 : loss : 0.041041, loss_ce: 0.016529
2021-12-11 21:11:39,444 iteration 4201 : loss : 0.049548, loss_ce: 0.027042
2021-12-11 21:11:40,890 iteration 4202 : loss : 0.023112, loss_ce: 0.013132
2021-12-11 21:11:42,336 iteration 4203 : loss : 0.041608, loss_ce: 0.019858
2021-12-11 21:11:43,757 iteration 4204 : loss : 0.024982, loss_ce: 0.013869
2021-12-11 21:11:44,988 iteration 4205 : loss : 0.018609, loss_ce: 0.009860
2021-12-11 21:11:46,331 iteration 4206 : loss : 0.035950, loss_ce: 0.013590
2021-12-11 21:11:47,673 iteration 4207 : loss : 0.027140, loss_ce: 0.015433
2021-12-11 21:11:49,091 iteration 4208 : loss : 0.043627, loss_ce: 0.019463
2021-12-11 21:11:50,450 iteration 4209 : loss : 0.031484, loss_ce: 0.012979
2021-12-11 21:11:51,802 iteration 4210 : loss : 0.022311, loss_ce: 0.011156
2021-12-11 21:11:53,187 iteration 4211 : loss : 0.028367, loss_ce: 0.014093
2021-12-11 21:11:54,528 iteration 4212 : loss : 0.028701, loss_ce: 0.014265
2021-12-11 21:11:55,964 iteration 4213 : loss : 0.026358, loss_ce: 0.012236
2021-12-11 21:11:57,395 iteration 4214 : loss : 0.040102, loss_ce: 0.019665
2021-12-11 21:11:58,811 iteration 4215 : loss : 0.047819, loss_ce: 0.014856
2021-12-11 21:12:00,186 iteration 4216 : loss : 0.035687, loss_ce: 0.016453
 62%|████████████████▋          | 248/400 [1:45:41<1:03:00, 24.87s/it]2021-12-11 21:12:01,698 iteration 4217 : loss : 0.051570, loss_ce: 0.020443
2021-12-11 21:12:03,157 iteration 4218 : loss : 0.043590, loss_ce: 0.024325
2021-12-11 21:12:04,552 iteration 4219 : loss : 0.028330, loss_ce: 0.012951
2021-12-11 21:12:05,927 iteration 4220 : loss : 0.030656, loss_ce: 0.013145
2021-12-11 21:12:07,334 iteration 4221 : loss : 0.041063, loss_ce: 0.017888
2021-12-11 21:12:08,764 iteration 4222 : loss : 0.027021, loss_ce: 0.013913
2021-12-11 21:12:10,268 iteration 4223 : loss : 0.046383, loss_ce: 0.017289
2021-12-11 21:12:11,644 iteration 4224 : loss : 0.031232, loss_ce: 0.014096
2021-12-11 21:12:13,070 iteration 4225 : loss : 0.034597, loss_ce: 0.017815
2021-12-11 21:12:14,534 iteration 4226 : loss : 0.035471, loss_ce: 0.016834
2021-12-11 21:12:15,892 iteration 4227 : loss : 0.036585, loss_ce: 0.014967
2021-12-11 21:12:17,304 iteration 4228 : loss : 0.050930, loss_ce: 0.026520
2021-12-11 21:12:18,640 iteration 4229 : loss : 0.029885, loss_ce: 0.014015
2021-12-11 21:12:20,063 iteration 4230 : loss : 0.041731, loss_ce: 0.019315
2021-12-11 21:12:21,475 iteration 4231 : loss : 0.028511, loss_ce: 0.013133
2021-12-11 21:12:22,906 iteration 4232 : loss : 0.055683, loss_ce: 0.024974
2021-12-11 21:12:24,233 iteration 4233 : loss : 0.033009, loss_ce: 0.014406
 62%|████████████████▊          | 249/400 [1:46:05<1:01:57, 24.62s/it]2021-12-11 21:12:25,715 iteration 4234 : loss : 0.035288, loss_ce: 0.016282
2021-12-11 21:12:27,051 iteration 4235 : loss : 0.026812, loss_ce: 0.013249
2021-12-11 21:12:28,484 iteration 4236 : loss : 0.040872, loss_ce: 0.022154
2021-12-11 21:12:29,862 iteration 4237 : loss : 0.031499, loss_ce: 0.016722
2021-12-11 21:12:31,310 iteration 4238 : loss : 0.048357, loss_ce: 0.018471
2021-12-11 21:12:32,618 iteration 4239 : loss : 0.041307, loss_ce: 0.015916
2021-12-11 21:12:34,076 iteration 4240 : loss : 0.037798, loss_ce: 0.014285
2021-12-11 21:12:35,442 iteration 4241 : loss : 0.026124, loss_ce: 0.012160
2021-12-11 21:12:36,794 iteration 4242 : loss : 0.043462, loss_ce: 0.017330
2021-12-11 21:12:38,160 iteration 4243 : loss : 0.032723, loss_ce: 0.015196
2021-12-11 21:12:39,588 iteration 4244 : loss : 0.041831, loss_ce: 0.015784
2021-12-11 21:12:40,985 iteration 4245 : loss : 0.060832, loss_ce: 0.017871
2021-12-11 21:12:42,349 iteration 4246 : loss : 0.029801, loss_ce: 0.015984
2021-12-11 21:12:43,701 iteration 4247 : loss : 0.037456, loss_ce: 0.013007
2021-12-11 21:12:45,111 iteration 4248 : loss : 0.035496, loss_ce: 0.016462
2021-12-11 21:12:46,496 iteration 4249 : loss : 0.040813, loss_ce: 0.021117
2021-12-11 21:12:46,496 Training Data Eval:
2021-12-11 21:12:53,314   Average segmentation loss on training set: 0.0185
2021-12-11 21:12:53,314 Validation Data Eval:
2021-12-11 21:12:55,674   Average segmentation loss on validation set: 0.1011
2021-12-11 21:12:57,091 iteration 4250 : loss : 0.034598, loss_ce: 0.014312
2021-12-11 21:12:59,038 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed100epoch_249.pth
 62%|████████████████▉          | 250/400 [1:46:40<1:09:08, 27.66s/it]2021-12-11 21:13:00,496 iteration 4251 : loss : 0.035531, loss_ce: 0.015413
2021-12-11 21:13:01,913 iteration 4252 : loss : 0.039991, loss_ce: 0.018417
2021-12-11 21:13:03,320 iteration 4253 : loss : 0.053421, loss_ce: 0.025934
2021-12-11 21:13:04,783 iteration 4254 : loss : 0.035192, loss_ce: 0.016978
2021-12-11 21:13:06,084 iteration 4255 : loss : 0.019623, loss_ce: 0.010474
2021-12-11 21:13:07,451 iteration 4256 : loss : 0.035298, loss_ce: 0.012762
2021-12-11 21:13:08,903 iteration 4257 : loss : 0.040286, loss_ce: 0.017007
2021-12-11 21:13:10,255 iteration 4258 : loss : 0.030078, loss_ce: 0.014544
2021-12-11 21:13:11,635 iteration 4259 : loss : 0.044138, loss_ce: 0.020588
2021-12-11 21:13:13,021 iteration 4260 : loss : 0.033331, loss_ce: 0.013122
2021-12-11 21:13:14,425 iteration 4261 : loss : 0.027001, loss_ce: 0.013655
2021-12-11 21:13:15,893 iteration 4262 : loss : 0.025835, loss_ce: 0.014314
2021-12-11 21:13:17,272 iteration 4263 : loss : 0.026449, loss_ce: 0.013252
2021-12-11 21:13:18,595 iteration 4264 : loss : 0.037392, loss_ce: 0.014723
2021-12-11 21:13:19,959 iteration 4265 : loss : 0.037221, loss_ce: 0.015322
2021-12-11 21:13:21,328 iteration 4266 : loss : 0.038285, loss_ce: 0.016983
2021-12-11 21:13:22,703 iteration 4267 : loss : 0.035593, loss_ce: 0.019232
 63%|████████████████▉          | 251/400 [1:47:04<1:05:45, 26.48s/it]2021-12-11 21:13:24,138 iteration 4268 : loss : 0.044774, loss_ce: 0.021733
2021-12-11 21:13:25,473 iteration 4269 : loss : 0.042618, loss_ce: 0.015874
2021-12-11 21:13:26,817 iteration 4270 : loss : 0.082369, loss_ce: 0.019273
2021-12-11 21:13:28,143 iteration 4271 : loss : 0.026725, loss_ce: 0.015055
2021-12-11 21:13:29,550 iteration 4272 : loss : 0.034921, loss_ce: 0.014916
2021-12-11 21:13:30,948 iteration 4273 : loss : 0.036201, loss_ce: 0.017887
2021-12-11 21:13:32,360 iteration 4274 : loss : 0.043270, loss_ce: 0.023081
2021-12-11 21:13:33,622 iteration 4275 : loss : 0.024623, loss_ce: 0.012604
2021-12-11 21:13:34,949 iteration 4276 : loss : 0.041774, loss_ce: 0.016959
2021-12-11 21:13:36,287 iteration 4277 : loss : 0.026684, loss_ce: 0.013182
2021-12-11 21:13:37,632 iteration 4278 : loss : 0.047648, loss_ce: 0.017692
2021-12-11 21:13:38,931 iteration 4279 : loss : 0.028834, loss_ce: 0.013554
2021-12-11 21:13:40,249 iteration 4280 : loss : 0.062427, loss_ce: 0.020252
2021-12-11 21:13:41,729 iteration 4281 : loss : 0.035720, loss_ce: 0.017823
2021-12-11 21:13:43,109 iteration 4282 : loss : 0.029862, loss_ce: 0.015447
2021-12-11 21:13:44,398 iteration 4283 : loss : 0.028496, loss_ce: 0.014184
2021-12-11 21:13:45,748 iteration 4284 : loss : 0.037047, loss_ce: 0.018044
 63%|█████████████████          | 252/400 [1:47:27<1:02:46, 25.45s/it]2021-12-11 21:13:47,222 iteration 4285 : loss : 0.035342, loss_ce: 0.018299
2021-12-11 21:13:48,624 iteration 4286 : loss : 0.028562, loss_ce: 0.011575
2021-12-11 21:13:50,039 iteration 4287 : loss : 0.054112, loss_ce: 0.024268
2021-12-11 21:13:51,412 iteration 4288 : loss : 0.040529, loss_ce: 0.014930
2021-12-11 21:13:52,808 iteration 4289 : loss : 0.045334, loss_ce: 0.016302
2021-12-11 21:13:54,230 iteration 4290 : loss : 0.046428, loss_ce: 0.018832
2021-12-11 21:13:55,545 iteration 4291 : loss : 0.036326, loss_ce: 0.014651
2021-12-11 21:13:56,988 iteration 4292 : loss : 0.034886, loss_ce: 0.014901
2021-12-11 21:13:58,391 iteration 4293 : loss : 0.042138, loss_ce: 0.019937
2021-12-11 21:13:59,778 iteration 4294 : loss : 0.044290, loss_ce: 0.023572
2021-12-11 21:14:01,202 iteration 4295 : loss : 0.029527, loss_ce: 0.014925
2021-12-11 21:14:02,518 iteration 4296 : loss : 0.025079, loss_ce: 0.013220
2021-12-11 21:14:03,908 iteration 4297 : loss : 0.034460, loss_ce: 0.014609
2021-12-11 21:14:05,249 iteration 4298 : loss : 0.031235, loss_ce: 0.014223
2021-12-11 21:14:06,694 iteration 4299 : loss : 0.039056, loss_ce: 0.020290
2021-12-11 21:14:08,125 iteration 4300 : loss : 0.042510, loss_ce: 0.016051
2021-12-11 21:14:09,495 iteration 4301 : loss : 0.027035, loss_ce: 0.013331
 63%|█████████████████          | 253/400 [1:47:50<1:01:05, 24.94s/it]2021-12-11 21:14:10,944 iteration 4302 : loss : 0.030357, loss_ce: 0.015403
2021-12-11 21:14:12,379 iteration 4303 : loss : 0.039244, loss_ce: 0.015741
2021-12-11 21:14:13,692 iteration 4304 : loss : 0.025616, loss_ce: 0.012192
2021-12-11 21:14:15,099 iteration 4305 : loss : 0.031241, loss_ce: 0.012157
2021-12-11 21:14:16,445 iteration 4306 : loss : 0.028492, loss_ce: 0.012694
2021-12-11 21:14:17,798 iteration 4307 : loss : 0.028338, loss_ce: 0.013627
2021-12-11 21:14:19,228 iteration 4308 : loss : 0.045583, loss_ce: 0.018628
2021-12-11 21:14:20,563 iteration 4309 : loss : 0.042779, loss_ce: 0.016382
2021-12-11 21:14:21,961 iteration 4310 : loss : 0.033676, loss_ce: 0.016686
2021-12-11 21:14:23,344 iteration 4311 : loss : 0.028910, loss_ce: 0.016168
2021-12-11 21:14:24,821 iteration 4312 : loss : 0.038412, loss_ce: 0.017475
2021-12-11 21:14:26,203 iteration 4313 : loss : 0.038248, loss_ce: 0.016577
2021-12-11 21:14:27,573 iteration 4314 : loss : 0.030810, loss_ce: 0.013297
2021-12-11 21:14:28,918 iteration 4315 : loss : 0.035206, loss_ce: 0.018676
2021-12-11 21:14:30,346 iteration 4316 : loss : 0.029713, loss_ce: 0.012926
2021-12-11 21:14:31,702 iteration 4317 : loss : 0.030930, loss_ce: 0.014611
2021-12-11 21:14:33,055 iteration 4318 : loss : 0.036294, loss_ce: 0.019728
 64%|██████████████████▍          | 254/400 [1:48:14<59:40, 24.52s/it]2021-12-11 21:14:34,536 iteration 4319 : loss : 0.041853, loss_ce: 0.015489
2021-12-11 21:14:35,968 iteration 4320 : loss : 0.041113, loss_ce: 0.019333
2021-12-11 21:14:37,416 iteration 4321 : loss : 0.038227, loss_ce: 0.019893
2021-12-11 21:14:38,814 iteration 4322 : loss : 0.035659, loss_ce: 0.014275
2021-12-11 21:14:40,161 iteration 4323 : loss : 0.022198, loss_ce: 0.010124
2021-12-11 21:14:41,568 iteration 4324 : loss : 0.034404, loss_ce: 0.016827
2021-12-11 21:14:43,051 iteration 4325 : loss : 0.059619, loss_ce: 0.018782
2021-12-11 21:14:44,451 iteration 4326 : loss : 0.034323, loss_ce: 0.021877
2021-12-11 21:14:45,803 iteration 4327 : loss : 0.039412, loss_ce: 0.017955
2021-12-11 21:14:47,153 iteration 4328 : loss : 0.039134, loss_ce: 0.017066
2021-12-11 21:14:48,566 iteration 4329 : loss : 0.041318, loss_ce: 0.019287
2021-12-11 21:14:49,905 iteration 4330 : loss : 0.024610, loss_ce: 0.012021
2021-12-11 21:14:51,405 iteration 4331 : loss : 0.049128, loss_ce: 0.017989
2021-12-11 21:14:52,833 iteration 4332 : loss : 0.049787, loss_ce: 0.023366
2021-12-11 21:14:54,151 iteration 4333 : loss : 0.035755, loss_ce: 0.014276
2021-12-11 21:14:55,486 iteration 4334 : loss : 0.029459, loss_ce: 0.014314
2021-12-11 21:14:55,486 Training Data Eval:
2021-12-11 21:15:02,307   Average segmentation loss on training set: 0.0173
2021-12-11 21:15:02,307 Validation Data Eval:
2021-12-11 21:15:04,671   Average segmentation loss on validation set: 0.0922
2021-12-11 21:15:06,106 iteration 4335 : loss : 0.055362, loss_ce: 0.032989
 64%|█████████████████▏         | 255/400 [1:48:47<1:05:26, 27.08s/it]2021-12-11 21:15:07,539 iteration 4336 : loss : 0.030623, loss_ce: 0.014016
2021-12-11 21:15:08,956 iteration 4337 : loss : 0.059326, loss_ce: 0.025604
2021-12-11 21:15:10,424 iteration 4338 : loss : 0.032156, loss_ce: 0.014491
2021-12-11 21:15:11,783 iteration 4339 : loss : 0.033173, loss_ce: 0.013273
2021-12-11 21:15:13,179 iteration 4340 : loss : 0.039207, loss_ce: 0.018622
2021-12-11 21:15:14,587 iteration 4341 : loss : 0.044226, loss_ce: 0.021709
2021-12-11 21:15:15,949 iteration 4342 : loss : 0.045066, loss_ce: 0.018649
2021-12-11 21:15:17,335 iteration 4343 : loss : 0.032074, loss_ce: 0.011974
2021-12-11 21:15:18,685 iteration 4344 : loss : 0.031738, loss_ce: 0.014239
2021-12-11 21:15:20,054 iteration 4345 : loss : 0.046051, loss_ce: 0.017937
2021-12-11 21:15:21,401 iteration 4346 : loss : 0.030623, loss_ce: 0.015893
2021-12-11 21:15:22,819 iteration 4347 : loss : 0.038714, loss_ce: 0.016630
2021-12-11 21:15:24,211 iteration 4348 : loss : 0.039622, loss_ce: 0.023418
2021-12-11 21:15:25,597 iteration 4349 : loss : 0.042898, loss_ce: 0.016529
2021-12-11 21:15:26,951 iteration 4350 : loss : 0.026189, loss_ce: 0.013109
2021-12-11 21:15:28,264 iteration 4351 : loss : 0.030147, loss_ce: 0.015572
2021-12-11 21:15:29,624 iteration 4352 : loss : 0.026658, loss_ce: 0.013537
 64%|█████████████████▎         | 256/400 [1:49:10<1:02:26, 26.02s/it]2021-12-11 21:15:31,091 iteration 4353 : loss : 0.028998, loss_ce: 0.012856
2021-12-11 21:15:32,551 iteration 4354 : loss : 0.035161, loss_ce: 0.014764
2021-12-11 21:15:33,889 iteration 4355 : loss : 0.040892, loss_ce: 0.017096
2021-12-11 21:15:35,302 iteration 4356 : loss : 0.036236, loss_ce: 0.019185
2021-12-11 21:15:36,691 iteration 4357 : loss : 0.035896, loss_ce: 0.017554
2021-12-11 21:15:38,041 iteration 4358 : loss : 0.027956, loss_ce: 0.014308
2021-12-11 21:15:39,408 iteration 4359 : loss : 0.022226, loss_ce: 0.010907
2021-12-11 21:15:40,773 iteration 4360 : loss : 0.042625, loss_ce: 0.016643
2021-12-11 21:15:42,198 iteration 4361 : loss : 0.050540, loss_ce: 0.025342
2021-12-11 21:15:43,551 iteration 4362 : loss : 0.030735, loss_ce: 0.014368
2021-12-11 21:15:44,856 iteration 4363 : loss : 0.036934, loss_ce: 0.020927
2021-12-11 21:15:46,270 iteration 4364 : loss : 0.042445, loss_ce: 0.017964
2021-12-11 21:15:47,642 iteration 4365 : loss : 0.039901, loss_ce: 0.020163
2021-12-11 21:15:48,913 iteration 4366 : loss : 0.024774, loss_ce: 0.012947
2021-12-11 21:15:50,280 iteration 4367 : loss : 0.044014, loss_ce: 0.018105
2021-12-11 21:15:51,716 iteration 4368 : loss : 0.044212, loss_ce: 0.018538
2021-12-11 21:15:53,036 iteration 4369 : loss : 0.048812, loss_ce: 0.016980
 64%|█████████████████▎         | 257/400 [1:49:34<1:00:08, 25.23s/it]2021-12-11 21:15:54,485 iteration 4370 : loss : 0.069426, loss_ce: 0.021784
2021-12-11 21:15:55,873 iteration 4371 : loss : 0.034166, loss_ce: 0.017474
2021-12-11 21:15:57,178 iteration 4372 : loss : 0.026232, loss_ce: 0.014403
2021-12-11 21:15:58,601 iteration 4373 : loss : 0.046379, loss_ce: 0.016837
2021-12-11 21:15:59,966 iteration 4374 : loss : 0.028050, loss_ce: 0.015065
2021-12-11 21:16:01,283 iteration 4375 : loss : 0.034097, loss_ce: 0.015569
2021-12-11 21:16:02,639 iteration 4376 : loss : 0.033610, loss_ce: 0.014797
2021-12-11 21:16:04,099 iteration 4377 : loss : 0.034778, loss_ce: 0.015021
2021-12-11 21:16:05,524 iteration 4378 : loss : 0.034247, loss_ce: 0.016829
2021-12-11 21:16:06,986 iteration 4379 : loss : 0.046248, loss_ce: 0.023386
2021-12-11 21:16:08,388 iteration 4380 : loss : 0.029668, loss_ce: 0.016679
2021-12-11 21:16:09,810 iteration 4381 : loss : 0.042230, loss_ce: 0.018461
2021-12-11 21:16:11,181 iteration 4382 : loss : 0.033936, loss_ce: 0.015060
2021-12-11 21:16:12,553 iteration 4383 : loss : 0.037675, loss_ce: 0.012593
2021-12-11 21:16:13,948 iteration 4384 : loss : 0.036725, loss_ce: 0.015444
2021-12-11 21:16:15,241 iteration 4385 : loss : 0.032420, loss_ce: 0.017519
2021-12-11 21:16:16,622 iteration 4386 : loss : 0.033961, loss_ce: 0.017075
 64%|██████████████████▋          | 258/400 [1:49:57<58:33, 24.74s/it]2021-12-11 21:16:18,044 iteration 4387 : loss : 0.029971, loss_ce: 0.014675
2021-12-11 21:16:19,464 iteration 4388 : loss : 0.031663, loss_ce: 0.016036
2021-12-11 21:16:20,783 iteration 4389 : loss : 0.021613, loss_ce: 0.012534
2021-12-11 21:16:22,095 iteration 4390 : loss : 0.033933, loss_ce: 0.015411
2021-12-11 21:16:23,457 iteration 4391 : loss : 0.029418, loss_ce: 0.014314
2021-12-11 21:16:24,859 iteration 4392 : loss : 0.038102, loss_ce: 0.020012
2021-12-11 21:16:26,175 iteration 4393 : loss : 0.031699, loss_ce: 0.014538
2021-12-11 21:16:27,616 iteration 4394 : loss : 0.038067, loss_ce: 0.018357
2021-12-11 21:16:29,030 iteration 4395 : loss : 0.032289, loss_ce: 0.013611
2021-12-11 21:16:30,484 iteration 4396 : loss : 0.035361, loss_ce: 0.016153
2021-12-11 21:16:31,851 iteration 4397 : loss : 0.055174, loss_ce: 0.021356
2021-12-11 21:16:33,201 iteration 4398 : loss : 0.039699, loss_ce: 0.015534
2021-12-11 21:16:34,570 iteration 4399 : loss : 0.030405, loss_ce: 0.013738
2021-12-11 21:16:35,916 iteration 4400 : loss : 0.029291, loss_ce: 0.014573
2021-12-11 21:16:37,261 iteration 4401 : loss : 0.027754, loss_ce: 0.015298
2021-12-11 21:16:38,582 iteration 4402 : loss : 0.026574, loss_ce: 0.011798
2021-12-11 21:16:39,948 iteration 4403 : loss : 0.050370, loss_ce: 0.017491
 65%|██████████████████▊          | 259/400 [1:50:21<57:08, 24.31s/it]2021-12-11 21:16:41,327 iteration 4404 : loss : 0.022059, loss_ce: 0.012951
2021-12-11 21:16:42,727 iteration 4405 : loss : 0.042244, loss_ce: 0.015070
2021-12-11 21:16:44,122 iteration 4406 : loss : 0.046150, loss_ce: 0.016551
2021-12-11 21:16:45,441 iteration 4407 : loss : 0.026537, loss_ce: 0.013335
2021-12-11 21:16:46,736 iteration 4408 : loss : 0.039774, loss_ce: 0.012526
2021-12-11 21:16:48,110 iteration 4409 : loss : 0.027638, loss_ce: 0.013631
2021-12-11 21:16:49,497 iteration 4410 : loss : 0.041575, loss_ce: 0.018149
2021-12-11 21:16:50,929 iteration 4411 : loss : 0.035121, loss_ce: 0.018608
2021-12-11 21:16:52,261 iteration 4412 : loss : 0.037277, loss_ce: 0.016939
2021-12-11 21:16:53,709 iteration 4413 : loss : 0.029824, loss_ce: 0.012047
2021-12-11 21:16:55,002 iteration 4414 : loss : 0.028348, loss_ce: 0.012709
2021-12-11 21:16:56,389 iteration 4415 : loss : 0.037906, loss_ce: 0.019271
2021-12-11 21:16:57,729 iteration 4416 : loss : 0.038778, loss_ce: 0.012917
2021-12-11 21:16:59,102 iteration 4417 : loss : 0.039494, loss_ce: 0.014824
2021-12-11 21:17:00,486 iteration 4418 : loss : 0.041959, loss_ce: 0.021581
2021-12-11 21:17:01,893 iteration 4419 : loss : 0.035058, loss_ce: 0.015651
2021-12-11 21:17:01,893 Training Data Eval:
2021-12-11 21:17:08,698   Average segmentation loss on training set: 0.0161
2021-12-11 21:17:08,698 Validation Data Eval:
2021-12-11 21:17:11,055   Average segmentation loss on validation set: 0.0804
2021-12-11 21:17:12,379 iteration 4420 : loss : 0.026083, loss_ce: 0.013572
 65%|█████████████████▌         | 260/400 [1:50:53<1:02:24, 26.75s/it]2021-12-11 21:17:13,820 iteration 4421 : loss : 0.033024, loss_ce: 0.014396
2021-12-11 21:17:15,245 iteration 4422 : loss : 0.034280, loss_ce: 0.014850
2021-12-11 21:17:16,719 iteration 4423 : loss : 0.037115, loss_ce: 0.017919
2021-12-11 21:17:18,149 iteration 4424 : loss : 0.038329, loss_ce: 0.016401
2021-12-11 21:17:19,470 iteration 4425 : loss : 0.022124, loss_ce: 0.011064
2021-12-11 21:17:20,888 iteration 4426 : loss : 0.042929, loss_ce: 0.017131
2021-12-11 21:17:22,302 iteration 4427 : loss : 0.053992, loss_ce: 0.016808
2021-12-11 21:17:23,725 iteration 4428 : loss : 0.038067, loss_ce: 0.017630
2021-12-11 21:17:25,154 iteration 4429 : loss : 0.039744, loss_ce: 0.016219
2021-12-11 21:17:26,525 iteration 4430 : loss : 0.035579, loss_ce: 0.015465
2021-12-11 21:17:27,883 iteration 4431 : loss : 0.038899, loss_ce: 0.016553
2021-12-11 21:17:29,245 iteration 4432 : loss : 0.026113, loss_ce: 0.013221
2021-12-11 21:17:30,610 iteration 4433 : loss : 0.027745, loss_ce: 0.013406
2021-12-11 21:17:31,975 iteration 4434 : loss : 0.037210, loss_ce: 0.020652
2021-12-11 21:17:33,385 iteration 4435 : loss : 0.047009, loss_ce: 0.020458
2021-12-11 21:17:34,882 iteration 4436 : loss : 0.047734, loss_ce: 0.024140
2021-12-11 21:17:36,212 iteration 4437 : loss : 0.040237, loss_ce: 0.019250
 65%|██████████████████▉          | 261/400 [1:51:17<59:56, 25.87s/it]2021-12-11 21:17:37,705 iteration 4438 : loss : 0.038288, loss_ce: 0.017014
2021-12-11 21:17:39,055 iteration 4439 : loss : 0.043711, loss_ce: 0.016453
2021-12-11 21:17:40,483 iteration 4440 : loss : 0.043729, loss_ce: 0.018407
2021-12-11 21:17:41,899 iteration 4441 : loss : 0.033763, loss_ce: 0.018134
2021-12-11 21:17:43,231 iteration 4442 : loss : 0.026338, loss_ce: 0.015431
2021-12-11 21:17:44,629 iteration 4443 : loss : 0.028625, loss_ce: 0.014752
2021-12-11 21:17:45,975 iteration 4444 : loss : 0.045555, loss_ce: 0.020102
2021-12-11 21:17:47,356 iteration 4445 : loss : 0.031785, loss_ce: 0.014506
2021-12-11 21:17:48,755 iteration 4446 : loss : 0.031689, loss_ce: 0.015755
2021-12-11 21:17:50,156 iteration 4447 : loss : 0.045451, loss_ce: 0.020471
2021-12-11 21:17:51,520 iteration 4448 : loss : 0.073742, loss_ce: 0.024380
2021-12-11 21:17:52,914 iteration 4449 : loss : 0.033908, loss_ce: 0.018623
2021-12-11 21:17:54,346 iteration 4450 : loss : 0.030632, loss_ce: 0.014463
2021-12-11 21:17:55,665 iteration 4451 : loss : 0.035283, loss_ce: 0.015365
2021-12-11 21:17:57,046 iteration 4452 : loss : 0.031755, loss_ce: 0.015619
2021-12-11 21:17:58,511 iteration 4453 : loss : 0.039032, loss_ce: 0.017266
2021-12-11 21:17:59,924 iteration 4454 : loss : 0.035305, loss_ce: 0.017204
 66%|██████████████████▉          | 262/400 [1:51:41<58:01, 25.23s/it]2021-12-11 21:18:01,437 iteration 4455 : loss : 0.034796, loss_ce: 0.015765
2021-12-11 21:18:02,830 iteration 4456 : loss : 0.045619, loss_ce: 0.022842
2021-12-11 21:18:04,132 iteration 4457 : loss : 0.024072, loss_ce: 0.014700
2021-12-11 21:18:05,530 iteration 4458 : loss : 0.042839, loss_ce: 0.023507
2021-12-11 21:18:06,856 iteration 4459 : loss : 0.025628, loss_ce: 0.012582
2021-12-11 21:18:08,231 iteration 4460 : loss : 0.026448, loss_ce: 0.012178
2021-12-11 21:18:09,554 iteration 4461 : loss : 0.033085, loss_ce: 0.017954
2021-12-11 21:18:10,888 iteration 4462 : loss : 0.030048, loss_ce: 0.013489
2021-12-11 21:18:12,226 iteration 4463 : loss : 0.025003, loss_ce: 0.011678
2021-12-11 21:18:13,545 iteration 4464 : loss : 0.025547, loss_ce: 0.012484
2021-12-11 21:18:14,941 iteration 4465 : loss : 0.034295, loss_ce: 0.017961
2021-12-11 21:18:16,355 iteration 4466 : loss : 0.042940, loss_ce: 0.017340
2021-12-11 21:18:17,730 iteration 4467 : loss : 0.026811, loss_ce: 0.012426
2021-12-11 21:18:19,097 iteration 4468 : loss : 0.028925, loss_ce: 0.012430
2021-12-11 21:18:20,419 iteration 4469 : loss : 0.037217, loss_ce: 0.013526
2021-12-11 21:18:21,819 iteration 4470 : loss : 0.055378, loss_ce: 0.020386
2021-12-11 21:18:23,234 iteration 4471 : loss : 0.031481, loss_ce: 0.014874
 66%|███████████████████          | 263/400 [1:52:04<56:17, 24.65s/it]2021-12-11 21:18:24,630 iteration 4472 : loss : 0.028407, loss_ce: 0.015038
2021-12-11 21:18:25,993 iteration 4473 : loss : 0.033350, loss_ce: 0.016358
2021-12-11 21:18:27,396 iteration 4474 : loss : 0.033357, loss_ce: 0.014228
2021-12-11 21:18:28,730 iteration 4475 : loss : 0.030430, loss_ce: 0.016226
2021-12-11 21:18:30,017 iteration 4476 : loss : 0.020167, loss_ce: 0.009765
2021-12-11 21:18:31,388 iteration 4477 : loss : 0.032760, loss_ce: 0.015909
2021-12-11 21:18:32,765 iteration 4478 : loss : 0.039368, loss_ce: 0.015700
2021-12-11 21:18:34,137 iteration 4479 : loss : 0.033307, loss_ce: 0.011278
2021-12-11 21:18:35,460 iteration 4480 : loss : 0.026053, loss_ce: 0.013047
2021-12-11 21:18:36,787 iteration 4481 : loss : 0.027135, loss_ce: 0.013524
2021-12-11 21:18:38,206 iteration 4482 : loss : 0.042601, loss_ce: 0.017012
2021-12-11 21:18:39,662 iteration 4483 : loss : 0.042012, loss_ce: 0.019969
2021-12-11 21:18:41,095 iteration 4484 : loss : 0.043680, loss_ce: 0.017768
2021-12-11 21:18:42,440 iteration 4485 : loss : 0.027236, loss_ce: 0.011249
2021-12-11 21:18:43,921 iteration 4486 : loss : 0.041870, loss_ce: 0.021086
2021-12-11 21:18:45,257 iteration 4487 : loss : 0.030843, loss_ce: 0.014792
2021-12-11 21:18:46,615 iteration 4488 : loss : 0.034317, loss_ce: 0.018252
 66%|███████████████████▏         | 264/400 [1:52:27<55:01, 24.27s/it]2021-12-11 21:18:48,088 iteration 4489 : loss : 0.035150, loss_ce: 0.017225
2021-12-11 21:18:49,474 iteration 4490 : loss : 0.026479, loss_ce: 0.011961
2021-12-11 21:18:50,930 iteration 4491 : loss : 0.046585, loss_ce: 0.018972
2021-12-11 21:18:52,336 iteration 4492 : loss : 0.038717, loss_ce: 0.016296
2021-12-11 21:18:53,780 iteration 4493 : loss : 0.060470, loss_ce: 0.021068
2021-12-11 21:18:55,074 iteration 4494 : loss : 0.033967, loss_ce: 0.014222
2021-12-11 21:18:56,479 iteration 4495 : loss : 0.039522, loss_ce: 0.019162
2021-12-11 21:18:57,889 iteration 4496 : loss : 0.039534, loss_ce: 0.014295
2021-12-11 21:18:59,216 iteration 4497 : loss : 0.027939, loss_ce: 0.011155
2021-12-11 21:19:00,671 iteration 4498 : loss : 0.040856, loss_ce: 0.020328
2021-12-11 21:19:02,177 iteration 4499 : loss : 0.046763, loss_ce: 0.021089
2021-12-11 21:19:03,546 iteration 4500 : loss : 0.045588, loss_ce: 0.022547
2021-12-11 21:19:04,880 iteration 4501 : loss : 0.038245, loss_ce: 0.016126
2021-12-11 21:19:06,212 iteration 4502 : loss : 0.025220, loss_ce: 0.013132
2021-12-11 21:19:07,649 iteration 4503 : loss : 0.032235, loss_ce: 0.016057
2021-12-11 21:19:09,029 iteration 4504 : loss : 0.029247, loss_ce: 0.011871
2021-12-11 21:19:09,030 Training Data Eval:
2021-12-11 21:19:15,846   Average segmentation loss on training set: 0.0192
2021-12-11 21:19:15,846 Validation Data Eval:
2021-12-11 21:19:18,207   Average segmentation loss on validation set: 0.0778
2021-12-11 21:19:19,503 iteration 4505 : loss : 0.026114, loss_ce: 0.012913
 66%|█████████████████▉         | 265/400 [1:53:00<1:00:25, 26.86s/it]2021-12-11 21:19:20,985 iteration 4506 : loss : 0.073480, loss_ce: 0.027132
2021-12-11 21:19:22,325 iteration 4507 : loss : 0.024262, loss_ce: 0.010556
2021-12-11 21:19:23,748 iteration 4508 : loss : 0.053380, loss_ce: 0.015817
2021-12-11 21:19:25,137 iteration 4509 : loss : 0.036559, loss_ce: 0.013579
2021-12-11 21:19:26,519 iteration 4510 : loss : 0.024133, loss_ce: 0.012278
2021-12-11 21:19:27,858 iteration 4511 : loss : 0.035067, loss_ce: 0.016243
2021-12-11 21:19:29,223 iteration 4512 : loss : 0.022216, loss_ce: 0.011853
2021-12-11 21:19:30,614 iteration 4513 : loss : 0.035026, loss_ce: 0.019600
2021-12-11 21:19:31,928 iteration 4514 : loss : 0.026125, loss_ce: 0.012619
2021-12-11 21:19:33,289 iteration 4515 : loss : 0.042117, loss_ce: 0.017397
2021-12-11 21:19:34,770 iteration 4516 : loss : 0.053642, loss_ce: 0.024808
2021-12-11 21:19:36,137 iteration 4517 : loss : 0.035144, loss_ce: 0.014292
2021-12-11 21:19:37,529 iteration 4518 : loss : 0.029219, loss_ce: 0.011984
2021-12-11 21:19:38,920 iteration 4519 : loss : 0.052017, loss_ce: 0.025913
2021-12-11 21:19:40,247 iteration 4520 : loss : 0.030112, loss_ce: 0.013311
2021-12-11 21:19:41,635 iteration 4521 : loss : 0.028712, loss_ce: 0.013018
2021-12-11 21:19:42,954 iteration 4522 : loss : 0.028124, loss_ce: 0.016175
 66%|███████████████████▎         | 266/400 [1:53:24<57:41, 25.84s/it]2021-12-11 21:19:44,446 iteration 4523 : loss : 0.026096, loss_ce: 0.012215
2021-12-11 21:19:45,757 iteration 4524 : loss : 0.020292, loss_ce: 0.010417
2021-12-11 21:19:47,198 iteration 4525 : loss : 0.057669, loss_ce: 0.020023
2021-12-11 21:19:48,588 iteration 4526 : loss : 0.025215, loss_ce: 0.012544
2021-12-11 21:19:49,963 iteration 4527 : loss : 0.040874, loss_ce: 0.020140
2021-12-11 21:19:51,398 iteration 4528 : loss : 0.033161, loss_ce: 0.014792
2021-12-11 21:19:52,756 iteration 4529 : loss : 0.036392, loss_ce: 0.018698
2021-12-11 21:19:54,152 iteration 4530 : loss : 0.036035, loss_ce: 0.017761
2021-12-11 21:19:55,486 iteration 4531 : loss : 0.031924, loss_ce: 0.017009
2021-12-11 21:19:56,912 iteration 4532 : loss : 0.029514, loss_ce: 0.015342
2021-12-11 21:19:58,362 iteration 4533 : loss : 0.040304, loss_ce: 0.018180
2021-12-11 21:19:59,861 iteration 4534 : loss : 0.041748, loss_ce: 0.017682
2021-12-11 21:20:01,165 iteration 4535 : loss : 0.040541, loss_ce: 0.016244
2021-12-11 21:20:02,576 iteration 4536 : loss : 0.028329, loss_ce: 0.015520
2021-12-11 21:20:04,032 iteration 4537 : loss : 0.048598, loss_ce: 0.018425
2021-12-11 21:20:05,392 iteration 4538 : loss : 0.033488, loss_ce: 0.015360
2021-12-11 21:20:06,800 iteration 4539 : loss : 0.046117, loss_ce: 0.020882
 67%|███████████████████▎         | 267/400 [1:53:48<55:56, 25.24s/it]2021-12-11 21:20:08,292 iteration 4540 : loss : 0.039610, loss_ce: 0.014640
2021-12-11 21:20:09,608 iteration 4541 : loss : 0.026107, loss_ce: 0.012746
2021-12-11 21:20:10,991 iteration 4542 : loss : 0.035313, loss_ce: 0.011842
2021-12-11 21:20:12,282 iteration 4543 : loss : 0.025115, loss_ce: 0.012110
2021-12-11 21:20:13,688 iteration 4544 : loss : 0.041977, loss_ce: 0.020410
2021-12-11 21:20:15,131 iteration 4545 : loss : 0.048236, loss_ce: 0.025871
2021-12-11 21:20:16,528 iteration 4546 : loss : 0.044871, loss_ce: 0.017437
2021-12-11 21:20:17,927 iteration 4547 : loss : 0.035399, loss_ce: 0.020042
2021-12-11 21:20:19,303 iteration 4548 : loss : 0.035023, loss_ce: 0.016511
2021-12-11 21:20:20,758 iteration 4549 : loss : 0.052254, loss_ce: 0.020268
2021-12-11 21:20:22,199 iteration 4550 : loss : 0.041346, loss_ce: 0.016206
2021-12-11 21:20:23,633 iteration 4551 : loss : 0.057772, loss_ce: 0.015195
2021-12-11 21:20:24,934 iteration 4552 : loss : 0.022555, loss_ce: 0.012071
2021-12-11 21:20:26,253 iteration 4553 : loss : 0.049224, loss_ce: 0.021327
2021-12-11 21:20:27,613 iteration 4554 : loss : 0.025219, loss_ce: 0.012234
2021-12-11 21:20:29,033 iteration 4555 : loss : 0.037549, loss_ce: 0.018048
2021-12-11 21:20:30,403 iteration 4556 : loss : 0.031664, loss_ce: 0.017441
 67%|███████████████████▍         | 268/400 [1:54:11<54:26, 24.75s/it]2021-12-11 21:20:31,872 iteration 4557 : loss : 0.030229, loss_ce: 0.015129
2021-12-11 21:20:33,322 iteration 4558 : loss : 0.037543, loss_ce: 0.021046
2021-12-11 21:20:34,686 iteration 4559 : loss : 0.041814, loss_ce: 0.015438
2021-12-11 21:20:36,033 iteration 4560 : loss : 0.033769, loss_ce: 0.012968
2021-12-11 21:20:37,404 iteration 4561 : loss : 0.043569, loss_ce: 0.014623
2021-12-11 21:20:38,778 iteration 4562 : loss : 0.043744, loss_ce: 0.017048
2021-12-11 21:20:40,110 iteration 4563 : loss : 0.025818, loss_ce: 0.012727
2021-12-11 21:20:41,489 iteration 4564 : loss : 0.049854, loss_ce: 0.020317
2021-12-11 21:20:42,819 iteration 4565 : loss : 0.025524, loss_ce: 0.012744
2021-12-11 21:20:44,129 iteration 4566 : loss : 0.032059, loss_ce: 0.016723
2021-12-11 21:20:45,517 iteration 4567 : loss : 0.033513, loss_ce: 0.016351
2021-12-11 21:20:46,889 iteration 4568 : loss : 0.031134, loss_ce: 0.011978
2021-12-11 21:20:48,266 iteration 4569 : loss : 0.037414, loss_ce: 0.017407
2021-12-11 21:20:49,595 iteration 4570 : loss : 0.029920, loss_ce: 0.015220
2021-12-11 21:20:50,937 iteration 4571 : loss : 0.042508, loss_ce: 0.022613
2021-12-11 21:20:52,288 iteration 4572 : loss : 0.022077, loss_ce: 0.010474
2021-12-11 21:20:53,654 iteration 4573 : loss : 0.029459, loss_ce: 0.014397
 67%|███████████████████▌         | 269/400 [1:54:34<53:03, 24.30s/it]2021-12-11 21:20:55,076 iteration 4574 : loss : 0.030230, loss_ce: 0.011503
2021-12-11 21:20:56,414 iteration 4575 : loss : 0.025903, loss_ce: 0.012912
2021-12-11 21:20:57,813 iteration 4576 : loss : 0.028156, loss_ce: 0.018405
2021-12-11 21:20:59,233 iteration 4577 : loss : 0.036432, loss_ce: 0.016366
2021-12-11 21:21:00,615 iteration 4578 : loss : 0.038055, loss_ce: 0.019166
2021-12-11 21:21:01,975 iteration 4579 : loss : 0.030271, loss_ce: 0.015811
2021-12-11 21:21:03,331 iteration 4580 : loss : 0.026026, loss_ce: 0.013478
2021-12-11 21:21:04,793 iteration 4581 : loss : 0.033115, loss_ce: 0.013450
2021-12-11 21:21:06,183 iteration 4582 : loss : 0.026659, loss_ce: 0.015057
2021-12-11 21:21:07,564 iteration 4583 : loss : 0.034531, loss_ce: 0.014652
2021-12-11 21:21:08,913 iteration 4584 : loss : 0.030221, loss_ce: 0.013657
2021-12-11 21:21:10,310 iteration 4585 : loss : 0.039198, loss_ce: 0.014588
2021-12-11 21:21:11,647 iteration 4586 : loss : 0.029782, loss_ce: 0.014643
2021-12-11 21:21:13,102 iteration 4587 : loss : 0.040488, loss_ce: 0.016996
2021-12-11 21:21:14,448 iteration 4588 : loss : 0.045043, loss_ce: 0.014967
2021-12-11 21:21:15,811 iteration 4589 : loss : 0.035609, loss_ce: 0.017497
2021-12-11 21:21:15,811 Training Data Eval:
2021-12-11 21:21:22,645   Average segmentation loss on training set: 0.0165
2021-12-11 21:21:22,645 Validation Data Eval:
2021-12-11 21:21:25,006   Average segmentation loss on validation set: 0.0896
2021-12-11 21:21:26,502 iteration 4590 : loss : 0.050454, loss_ce: 0.021404
 68%|███████████████████▌         | 270/400 [1:55:07<58:11, 26.86s/it]2021-12-11 21:21:27,874 iteration 4591 : loss : 0.023260, loss_ce: 0.010473
2021-12-11 21:21:29,248 iteration 4592 : loss : 0.040978, loss_ce: 0.016263
2021-12-11 21:21:30,537 iteration 4593 : loss : 0.033267, loss_ce: 0.013025
2021-12-11 21:21:31,892 iteration 4594 : loss : 0.032864, loss_ce: 0.016398
2021-12-11 21:21:33,290 iteration 4595 : loss : 0.044263, loss_ce: 0.013722
2021-12-11 21:21:34,745 iteration 4596 : loss : 0.061542, loss_ce: 0.025532
2021-12-11 21:21:36,118 iteration 4597 : loss : 0.030227, loss_ce: 0.015680
2021-12-11 21:21:37,422 iteration 4598 : loss : 0.035111, loss_ce: 0.020188
2021-12-11 21:21:38,759 iteration 4599 : loss : 0.037652, loss_ce: 0.015728
2021-12-11 21:21:40,191 iteration 4600 : loss : 0.036948, loss_ce: 0.019311
2021-12-11 21:21:41,668 iteration 4601 : loss : 0.045049, loss_ce: 0.025367
2021-12-11 21:21:42,974 iteration 4602 : loss : 0.030138, loss_ce: 0.013830
2021-12-11 21:21:44,288 iteration 4603 : loss : 0.032410, loss_ce: 0.016806
2021-12-11 21:21:45,686 iteration 4604 : loss : 0.043545, loss_ce: 0.016306
2021-12-11 21:21:47,081 iteration 4605 : loss : 0.047142, loss_ce: 0.017288
2021-12-11 21:21:48,512 iteration 4606 : loss : 0.043417, loss_ce: 0.016720
2021-12-11 21:21:49,821 iteration 4607 : loss : 0.025201, loss_ce: 0.015252
 68%|███████████████████▋         | 271/400 [1:55:31<55:28, 25.80s/it]2021-12-11 21:21:51,286 iteration 4608 : loss : 0.034105, loss_ce: 0.013470
2021-12-11 21:21:52,728 iteration 4609 : loss : 0.034162, loss_ce: 0.017486
2021-12-11 21:21:54,145 iteration 4610 : loss : 0.045507, loss_ce: 0.015108
2021-12-11 21:21:55,516 iteration 4611 : loss : 0.028281, loss_ce: 0.014369
2021-12-11 21:21:56,822 iteration 4612 : loss : 0.028851, loss_ce: 0.014179
2021-12-11 21:21:58,211 iteration 4613 : loss : 0.037145, loss_ce: 0.018415
2021-12-11 21:21:59,611 iteration 4614 : loss : 0.041117, loss_ce: 0.018202
2021-12-11 21:22:01,018 iteration 4615 : loss : 0.039797, loss_ce: 0.017921
2021-12-11 21:22:02,466 iteration 4616 : loss : 0.033800, loss_ce: 0.014833
2021-12-11 21:22:03,786 iteration 4617 : loss : 0.034679, loss_ce: 0.017890
2021-12-11 21:22:05,203 iteration 4618 : loss : 0.051422, loss_ce: 0.021957
2021-12-11 21:22:06,618 iteration 4619 : loss : 0.040271, loss_ce: 0.016148
2021-12-11 21:22:08,029 iteration 4620 : loss : 0.027899, loss_ce: 0.011829
2021-12-11 21:22:09,462 iteration 4621 : loss : 0.040058, loss_ce: 0.015822
2021-12-11 21:22:10,838 iteration 4622 : loss : 0.026037, loss_ce: 0.011611
2021-12-11 21:22:12,196 iteration 4623 : loss : 0.030634, loss_ce: 0.013759
2021-12-11 21:22:13,564 iteration 4624 : loss : 0.032073, loss_ce: 0.015527
 68%|███████████████████▋         | 272/400 [1:55:54<53:43, 25.18s/it]2021-12-11 21:22:15,000 iteration 4625 : loss : 0.033066, loss_ce: 0.017534
2021-12-11 21:22:16,349 iteration 4626 : loss : 0.025693, loss_ce: 0.011402
2021-12-11 21:22:17,726 iteration 4627 : loss : 0.027315, loss_ce: 0.011687
2021-12-11 21:22:19,060 iteration 4628 : loss : 0.030887, loss_ce: 0.013856
2021-12-11 21:22:20,393 iteration 4629 : loss : 0.033221, loss_ce: 0.015657
2021-12-11 21:22:21,749 iteration 4630 : loss : 0.033161, loss_ce: 0.013704
2021-12-11 21:22:23,195 iteration 4631 : loss : 0.035665, loss_ce: 0.016902
2021-12-11 21:22:24,581 iteration 4632 : loss : 0.024104, loss_ce: 0.011490
2021-12-11 21:22:25,962 iteration 4633 : loss : 0.034140, loss_ce: 0.016232
2021-12-11 21:22:27,256 iteration 4634 : loss : 0.024208, loss_ce: 0.013066
2021-12-11 21:22:28,655 iteration 4635 : loss : 0.040145, loss_ce: 0.014961
2021-12-11 21:22:30,029 iteration 4636 : loss : 0.034419, loss_ce: 0.015519
2021-12-11 21:22:31,405 iteration 4637 : loss : 0.025481, loss_ce: 0.012017
2021-12-11 21:22:32,814 iteration 4638 : loss : 0.039406, loss_ce: 0.019734
2021-12-11 21:22:34,265 iteration 4639 : loss : 0.041877, loss_ce: 0.016828
2021-12-11 21:22:35,679 iteration 4640 : loss : 0.034447, loss_ce: 0.016331
2021-12-11 21:22:37,030 iteration 4641 : loss : 0.028168, loss_ce: 0.014790
 68%|███████████████████▊         | 273/400 [1:56:18<52:12, 24.67s/it]2021-12-11 21:22:38,509 iteration 4642 : loss : 0.044202, loss_ce: 0.015980
2021-12-11 21:22:39,925 iteration 4643 : loss : 0.044051, loss_ce: 0.020513
2021-12-11 21:22:41,331 iteration 4644 : loss : 0.038531, loss_ce: 0.018067
2021-12-11 21:22:42,862 iteration 4645 : loss : 0.042767, loss_ce: 0.021074
2021-12-11 21:22:44,222 iteration 4646 : loss : 0.023385, loss_ce: 0.012286
2021-12-11 21:22:45,581 iteration 4647 : loss : 0.029220, loss_ce: 0.015962
2021-12-11 21:22:46,876 iteration 4648 : loss : 0.026165, loss_ce: 0.013205
2021-12-11 21:22:48,269 iteration 4649 : loss : 0.038365, loss_ce: 0.017815
2021-12-11 21:22:49,630 iteration 4650 : loss : 0.039602, loss_ce: 0.016389
2021-12-11 21:22:51,018 iteration 4651 : loss : 0.029845, loss_ce: 0.014818
2021-12-11 21:22:52,405 iteration 4652 : loss : 0.029449, loss_ce: 0.014945
2021-12-11 21:22:53,788 iteration 4653 : loss : 0.032280, loss_ce: 0.015469
2021-12-11 21:22:55,176 iteration 4654 : loss : 0.030117, loss_ce: 0.016817
2021-12-11 21:22:56,580 iteration 4655 : loss : 0.031219, loss_ce: 0.013490
2021-12-11 21:22:58,027 iteration 4656 : loss : 0.074948, loss_ce: 0.020218
2021-12-11 21:22:59,397 iteration 4657 : loss : 0.087381, loss_ce: 0.016658
2021-12-11 21:23:00,735 iteration 4658 : loss : 0.046477, loss_ce: 0.015163
 68%|███████████████████▊         | 274/400 [1:56:42<51:11, 24.38s/it]2021-12-11 21:23:02,170 iteration 4659 : loss : 0.039517, loss_ce: 0.019422
2021-12-11 21:23:03,593 iteration 4660 : loss : 0.032908, loss_ce: 0.012475
2021-12-11 21:23:04,983 iteration 4661 : loss : 0.036872, loss_ce: 0.019880
2021-12-11 21:23:06,467 iteration 4662 : loss : 0.041973, loss_ce: 0.023006
2021-12-11 21:23:07,793 iteration 4663 : loss : 0.030742, loss_ce: 0.012243
2021-12-11 21:23:09,182 iteration 4664 : loss : 0.031144, loss_ce: 0.013098
2021-12-11 21:23:10,706 iteration 4665 : loss : 0.058467, loss_ce: 0.024570
2021-12-11 21:23:12,038 iteration 4666 : loss : 0.029116, loss_ce: 0.013991
2021-12-11 21:23:13,405 iteration 4667 : loss : 0.040640, loss_ce: 0.019250
2021-12-11 21:23:14,765 iteration 4668 : loss : 0.042055, loss_ce: 0.012518
2021-12-11 21:23:16,202 iteration 4669 : loss : 0.027604, loss_ce: 0.014885
2021-12-11 21:23:17,650 iteration 4670 : loss : 0.044360, loss_ce: 0.019617
2021-12-11 21:23:18,985 iteration 4671 : loss : 0.025254, loss_ce: 0.011397
2021-12-11 21:23:20,496 iteration 4672 : loss : 0.047229, loss_ce: 0.022743
2021-12-11 21:23:21,912 iteration 4673 : loss : 0.039176, loss_ce: 0.015806
2021-12-11 21:23:23,367 iteration 4674 : loss : 0.046123, loss_ce: 0.026820
2021-12-11 21:23:23,367 Training Data Eval:
2021-12-11 21:23:30,203   Average segmentation loss on training set: 0.0170
2021-12-11 21:23:30,203 Validation Data Eval:
2021-12-11 21:23:32,560   Average segmentation loss on validation set: 0.0829
2021-12-11 21:23:33,905 iteration 4675 : loss : 0.025803, loss_ce: 0.013513
 69%|███████████████████▉         | 275/400 [1:57:15<56:17, 27.02s/it]2021-12-11 21:23:35,415 iteration 4676 : loss : 0.044691, loss_ce: 0.020911
2021-12-11 21:23:36,778 iteration 4677 : loss : 0.043518, loss_ce: 0.019390
2021-12-11 21:23:38,150 iteration 4678 : loss : 0.045735, loss_ce: 0.018351
2021-12-11 21:23:39,530 iteration 4679 : loss : 0.035898, loss_ce: 0.021346
2021-12-11 21:23:40,879 iteration 4680 : loss : 0.033153, loss_ce: 0.014180
2021-12-11 21:23:42,269 iteration 4681 : loss : 0.043005, loss_ce: 0.017701
2021-12-11 21:23:43,596 iteration 4682 : loss : 0.057652, loss_ce: 0.017226
2021-12-11 21:23:44,967 iteration 4683 : loss : 0.035801, loss_ce: 0.019802
2021-12-11 21:23:46,380 iteration 4684 : loss : 0.036744, loss_ce: 0.018757
2021-12-11 21:23:47,760 iteration 4685 : loss : 0.029241, loss_ce: 0.015579
2021-12-11 21:23:49,065 iteration 4686 : loss : 0.032322, loss_ce: 0.013117
2021-12-11 21:23:50,488 iteration 4687 : loss : 0.034588, loss_ce: 0.016510
2021-12-11 21:23:51,932 iteration 4688 : loss : 0.050274, loss_ce: 0.021154
2021-12-11 21:23:53,395 iteration 4689 : loss : 0.043346, loss_ce: 0.019446
2021-12-11 21:23:54,767 iteration 4690 : loss : 0.050463, loss_ce: 0.015778
2021-12-11 21:23:56,100 iteration 4691 : loss : 0.037678, loss_ce: 0.016049
2021-12-11 21:23:57,465 iteration 4692 : loss : 0.029602, loss_ce: 0.015057
 69%|████████████████████         | 276/400 [1:57:38<53:41, 25.98s/it]2021-12-11 21:23:58,932 iteration 4693 : loss : 0.030955, loss_ce: 0.014447
2021-12-11 21:24:00,280 iteration 4694 : loss : 0.031491, loss_ce: 0.012915
2021-12-11 21:24:01,723 iteration 4695 : loss : 0.034844, loss_ce: 0.015959
2021-12-11 21:24:03,120 iteration 4696 : loss : 0.043301, loss_ce: 0.019611
2021-12-11 21:24:04,564 iteration 4697 : loss : 0.047803, loss_ce: 0.023049
2021-12-11 21:24:05,846 iteration 4698 : loss : 0.038449, loss_ce: 0.018891
2021-12-11 21:24:07,217 iteration 4699 : loss : 0.046926, loss_ce: 0.020137
2021-12-11 21:24:08,615 iteration 4700 : loss : 0.055094, loss_ce: 0.019314
2021-12-11 21:24:10,002 iteration 4701 : loss : 0.037168, loss_ce: 0.018275
2021-12-11 21:24:11,309 iteration 4702 : loss : 0.025373, loss_ce: 0.012035
2021-12-11 21:24:12,722 iteration 4703 : loss : 0.036984, loss_ce: 0.015717
2021-12-11 21:24:14,113 iteration 4704 : loss : 0.044648, loss_ce: 0.014952
2021-12-11 21:24:15,429 iteration 4705 : loss : 0.024664, loss_ce: 0.011997
2021-12-11 21:24:16,760 iteration 4706 : loss : 0.026448, loss_ce: 0.014677
2021-12-11 21:24:18,200 iteration 4707 : loss : 0.032301, loss_ce: 0.015967
2021-12-11 21:24:19,565 iteration 4708 : loss : 0.046244, loss_ce: 0.013795
2021-12-11 21:24:20,913 iteration 4709 : loss : 0.030714, loss_ce: 0.015404
 69%|████████████████████         | 277/400 [1:58:02<51:41, 25.22s/it]2021-12-11 21:24:22,420 iteration 4710 : loss : 0.046750, loss_ce: 0.016596
2021-12-11 21:24:23,766 iteration 4711 : loss : 0.042471, loss_ce: 0.021818
2021-12-11 21:24:25,137 iteration 4712 : loss : 0.031630, loss_ce: 0.016163
2021-12-11 21:24:26,482 iteration 4713 : loss : 0.023229, loss_ce: 0.011452
2021-12-11 21:24:27,831 iteration 4714 : loss : 0.022628, loss_ce: 0.012163
2021-12-11 21:24:29,285 iteration 4715 : loss : 0.041174, loss_ce: 0.017294
2021-12-11 21:24:30,660 iteration 4716 : loss : 0.026000, loss_ce: 0.012992
2021-12-11 21:24:32,039 iteration 4717 : loss : 0.042226, loss_ce: 0.015818
2021-12-11 21:24:33,405 iteration 4718 : loss : 0.024051, loss_ce: 0.011434
2021-12-11 21:24:34,741 iteration 4719 : loss : 0.023419, loss_ce: 0.011376
2021-12-11 21:24:36,170 iteration 4720 : loss : 0.034915, loss_ce: 0.014619
2021-12-11 21:24:37,547 iteration 4721 : loss : 0.031268, loss_ce: 0.013798
2021-12-11 21:24:38,919 iteration 4722 : loss : 0.031741, loss_ce: 0.014959
2021-12-11 21:24:40,340 iteration 4723 : loss : 0.046866, loss_ce: 0.018415
2021-12-11 21:24:41,671 iteration 4724 : loss : 0.038427, loss_ce: 0.021921
2021-12-11 21:24:43,035 iteration 4725 : loss : 0.028747, loss_ce: 0.014443
2021-12-11 21:24:44,371 iteration 4726 : loss : 0.028936, loss_ce: 0.012611
 70%|████████████████████▏        | 278/400 [1:58:25<50:12, 24.69s/it]2021-12-11 21:24:45,812 iteration 4727 : loss : 0.035818, loss_ce: 0.014603
2021-12-11 21:24:47,218 iteration 4728 : loss : 0.038146, loss_ce: 0.018862
2021-12-11 21:24:48,632 iteration 4729 : loss : 0.025790, loss_ce: 0.011044
2021-12-11 21:24:50,021 iteration 4730 : loss : 0.044251, loss_ce: 0.017313
2021-12-11 21:24:51,393 iteration 4731 : loss : 0.036690, loss_ce: 0.014375
2021-12-11 21:24:52,717 iteration 4732 : loss : 0.034798, loss_ce: 0.017706
2021-12-11 21:24:54,187 iteration 4733 : loss : 0.046284, loss_ce: 0.023270
2021-12-11 21:24:55,565 iteration 4734 : loss : 0.032423, loss_ce: 0.014892
2021-12-11 21:24:56,886 iteration 4735 : loss : 0.026663, loss_ce: 0.012692
2021-12-11 21:24:58,316 iteration 4736 : loss : 0.043216, loss_ce: 0.019271
2021-12-11 21:24:59,723 iteration 4737 : loss : 0.030234, loss_ce: 0.015822
2021-12-11 21:25:01,227 iteration 4738 : loss : 0.046631, loss_ce: 0.018169
2021-12-11 21:25:02,637 iteration 4739 : loss : 0.026030, loss_ce: 0.012556
2021-12-11 21:25:04,027 iteration 4740 : loss : 0.030940, loss_ce: 0.011850
2021-12-11 21:25:05,417 iteration 4741 : loss : 0.033373, loss_ce: 0.017569
2021-12-11 21:25:06,818 iteration 4742 : loss : 0.034464, loss_ce: 0.018652
2021-12-11 21:25:08,353 iteration 4743 : loss : 0.053618, loss_ce: 0.025948
 70%|████████████████████▏        | 279/400 [1:58:49<49:21, 24.48s/it]2021-12-11 21:25:09,843 iteration 4744 : loss : 0.030854, loss_ce: 0.015084
2021-12-11 21:25:11,242 iteration 4745 : loss : 0.031670, loss_ce: 0.015831
2021-12-11 21:25:12,620 iteration 4746 : loss : 0.045388, loss_ce: 0.017332
2021-12-11 21:25:13,939 iteration 4747 : loss : 0.020848, loss_ce: 0.012402
2021-12-11 21:25:15,414 iteration 4748 : loss : 0.044158, loss_ce: 0.019950
2021-12-11 21:25:16,737 iteration 4749 : loss : 0.030157, loss_ce: 0.014959
2021-12-11 21:25:18,137 iteration 4750 : loss : 0.038484, loss_ce: 0.013797
2021-12-11 21:25:19,550 iteration 4751 : loss : 0.041026, loss_ce: 0.018670
2021-12-11 21:25:20,925 iteration 4752 : loss : 0.038065, loss_ce: 0.014358
2021-12-11 21:25:22,270 iteration 4753 : loss : 0.023229, loss_ce: 0.010459
2021-12-11 21:25:23,658 iteration 4754 : loss : 0.034330, loss_ce: 0.018630
2021-12-11 21:25:25,115 iteration 4755 : loss : 0.046817, loss_ce: 0.016246
2021-12-11 21:25:26,528 iteration 4756 : loss : 0.037425, loss_ce: 0.015032
2021-12-11 21:25:27,918 iteration 4757 : loss : 0.033226, loss_ce: 0.019046
2021-12-11 21:25:29,381 iteration 4758 : loss : 0.029903, loss_ce: 0.012233
2021-12-11 21:25:30,762 iteration 4759 : loss : 0.061597, loss_ce: 0.035917
2021-12-11 21:25:30,762 Training Data Eval:
2021-12-11 21:25:37,573   Average segmentation loss on training set: 0.0170
2021-12-11 21:25:37,574 Validation Data Eval:
2021-12-11 21:25:39,927   Average segmentation loss on validation set: 0.1036
2021-12-11 21:25:41,330 iteration 4760 : loss : 0.035085, loss_ce: 0.013706
 70%|████████████████████▎        | 280/400 [1:59:22<54:03, 27.03s/it]2021-12-11 21:25:42,716 iteration 4761 : loss : 0.026540, loss_ce: 0.012211
2021-12-11 21:25:44,184 iteration 4762 : loss : 0.031288, loss_ce: 0.013395
2021-12-11 21:25:45,595 iteration 4763 : loss : 0.037065, loss_ce: 0.017775
2021-12-11 21:25:46,920 iteration 4764 : loss : 0.028149, loss_ce: 0.012682
2021-12-11 21:25:48,271 iteration 4765 : loss : 0.041079, loss_ce: 0.019496
2021-12-11 21:25:49,617 iteration 4766 : loss : 0.030389, loss_ce: 0.012528
2021-12-11 21:25:51,044 iteration 4767 : loss : 0.039656, loss_ce: 0.017562
2021-12-11 21:25:52,516 iteration 4768 : loss : 0.040590, loss_ce: 0.020219
2021-12-11 21:25:53,977 iteration 4769 : loss : 0.049180, loss_ce: 0.015877
2021-12-11 21:25:55,362 iteration 4770 : loss : 0.026119, loss_ce: 0.015324
2021-12-11 21:25:56,713 iteration 4771 : loss : 0.025114, loss_ce: 0.013472
2021-12-11 21:25:58,085 iteration 4772 : loss : 0.022384, loss_ce: 0.010503
2021-12-11 21:25:59,437 iteration 4773 : loss : 0.031511, loss_ce: 0.014938
2021-12-11 21:26:00,788 iteration 4774 : loss : 0.025334, loss_ce: 0.012468
2021-12-11 21:26:02,179 iteration 4775 : loss : 0.047398, loss_ce: 0.022151
2021-12-11 21:26:03,558 iteration 4776 : loss : 0.028918, loss_ce: 0.013129
2021-12-11 21:26:04,923 iteration 4777 : loss : 0.025367, loss_ce: 0.010951
 70%|████████████████████▎        | 281/400 [1:59:46<51:33, 26.00s/it]2021-12-11 21:26:06,415 iteration 4778 : loss : 0.034528, loss_ce: 0.014458
2021-12-11 21:26:07,756 iteration 4779 : loss : 0.027910, loss_ce: 0.012084
2021-12-11 21:26:09,101 iteration 4780 : loss : 0.059863, loss_ce: 0.019016
2021-12-11 21:26:10,446 iteration 4781 : loss : 0.029954, loss_ce: 0.016786
2021-12-11 21:26:11,869 iteration 4782 : loss : 0.043390, loss_ce: 0.017476
2021-12-11 21:26:13,193 iteration 4783 : loss : 0.031656, loss_ce: 0.011207
2021-12-11 21:26:14,635 iteration 4784 : loss : 0.036301, loss_ce: 0.015971
2021-12-11 21:26:16,092 iteration 4785 : loss : 0.036587, loss_ce: 0.019312
2021-12-11 21:26:17,469 iteration 4786 : loss : 0.028940, loss_ce: 0.014196
2021-12-11 21:26:18,883 iteration 4787 : loss : 0.042818, loss_ce: 0.013974
2021-12-11 21:26:20,279 iteration 4788 : loss : 0.027840, loss_ce: 0.013904
2021-12-11 21:26:21,682 iteration 4789 : loss : 0.043133, loss_ce: 0.018915
2021-12-11 21:26:23,055 iteration 4790 : loss : 0.034605, loss_ce: 0.018411
2021-12-11 21:26:24,407 iteration 4791 : loss : 0.024676, loss_ce: 0.012265
2021-12-11 21:26:25,776 iteration 4792 : loss : 0.042080, loss_ce: 0.021304
2021-12-11 21:26:27,195 iteration 4793 : loss : 0.035017, loss_ce: 0.019235
2021-12-11 21:26:28,563 iteration 4794 : loss : 0.074512, loss_ce: 0.015985
 70%|████████████████████▍        | 282/400 [2:00:09<49:43, 25.29s/it]2021-12-11 21:26:29,933 iteration 4795 : loss : 0.028392, loss_ce: 0.012822
2021-12-11 21:26:31,251 iteration 4796 : loss : 0.032327, loss_ce: 0.015834
2021-12-11 21:26:32,590 iteration 4797 : loss : 0.026437, loss_ce: 0.012717
2021-12-11 21:26:34,030 iteration 4798 : loss : 0.054748, loss_ce: 0.027366
2021-12-11 21:26:35,420 iteration 4799 : loss : 0.036820, loss_ce: 0.013414
2021-12-11 21:26:36,812 iteration 4800 : loss : 0.057847, loss_ce: 0.025024
2021-12-11 21:26:38,200 iteration 4801 : loss : 0.027432, loss_ce: 0.014936
2021-12-11 21:26:39,500 iteration 4802 : loss : 0.023018, loss_ce: 0.010080
2021-12-11 21:26:40,879 iteration 4803 : loss : 0.034340, loss_ce: 0.017112
2021-12-11 21:26:42,228 iteration 4804 : loss : 0.028934, loss_ce: 0.012729
2021-12-11 21:26:43,636 iteration 4805 : loss : 0.041472, loss_ce: 0.017460
2021-12-11 21:26:44,979 iteration 4806 : loss : 0.037935, loss_ce: 0.015423
2021-12-11 21:26:46,435 iteration 4807 : loss : 0.043247, loss_ce: 0.019656
2021-12-11 21:26:48,016 iteration 4808 : loss : 0.044354, loss_ce: 0.021126
2021-12-11 21:26:49,306 iteration 4809 : loss : 0.026183, loss_ce: 0.013513
2021-12-11 21:26:50,728 iteration 4810 : loss : 0.046216, loss_ce: 0.018949
2021-12-11 21:26:52,090 iteration 4811 : loss : 0.028251, loss_ce: 0.014145
 71%|████████████████████▌        | 283/400 [2:00:33<48:17, 24.76s/it]2021-12-11 21:26:53,495 iteration 4812 : loss : 0.030673, loss_ce: 0.014155
2021-12-11 21:26:54,898 iteration 4813 : loss : 0.037159, loss_ce: 0.016521
2021-12-11 21:26:56,235 iteration 4814 : loss : 0.027695, loss_ce: 0.014645
2021-12-11 21:26:57,626 iteration 4815 : loss : 0.033859, loss_ce: 0.014411
2021-12-11 21:26:58,964 iteration 4816 : loss : 0.025639, loss_ce: 0.011991
2021-12-11 21:27:00,226 iteration 4817 : loss : 0.030836, loss_ce: 0.013267
2021-12-11 21:27:01,641 iteration 4818 : loss : 0.039851, loss_ce: 0.021732
2021-12-11 21:27:03,037 iteration 4819 : loss : 0.032510, loss_ce: 0.013355
2021-12-11 21:27:04,466 iteration 4820 : loss : 0.031666, loss_ce: 0.015568
2021-12-11 21:27:05,831 iteration 4821 : loss : 0.030082, loss_ce: 0.016744
2021-12-11 21:27:07,233 iteration 4822 : loss : 0.050767, loss_ce: 0.019114
2021-12-11 21:27:08,579 iteration 4823 : loss : 0.042774, loss_ce: 0.019182
2021-12-11 21:27:10,013 iteration 4824 : loss : 0.040462, loss_ce: 0.017662
2021-12-11 21:27:11,382 iteration 4825 : loss : 0.022008, loss_ce: 0.009600
2021-12-11 21:27:12,727 iteration 4826 : loss : 0.043390, loss_ce: 0.018621
2021-12-11 21:27:14,211 iteration 4827 : loss : 0.046509, loss_ce: 0.018345
2021-12-11 21:27:15,656 iteration 4828 : loss : 0.040063, loss_ce: 0.018627
 71%|████████████████████▌        | 284/400 [2:00:56<47:10, 24.41s/it]2021-12-11 21:27:17,125 iteration 4829 : loss : 0.040765, loss_ce: 0.015961
2021-12-11 21:27:18,457 iteration 4830 : loss : 0.028209, loss_ce: 0.012045
2021-12-11 21:27:19,915 iteration 4831 : loss : 0.051389, loss_ce: 0.019348
2021-12-11 21:27:21,293 iteration 4832 : loss : 0.034780, loss_ce: 0.016093
2021-12-11 21:27:22,733 iteration 4833 : loss : 0.031412, loss_ce: 0.012355
2021-12-11 21:27:24,202 iteration 4834 : loss : 0.035004, loss_ce: 0.015426
2021-12-11 21:27:25,570 iteration 4835 : loss : 0.035771, loss_ce: 0.015742
2021-12-11 21:27:26,953 iteration 4836 : loss : 0.038448, loss_ce: 0.015964
2021-12-11 21:27:28,298 iteration 4837 : loss : 0.037643, loss_ce: 0.012896
2021-12-11 21:27:29,709 iteration 4838 : loss : 0.029534, loss_ce: 0.015026
2021-12-11 21:27:31,126 iteration 4839 : loss : 0.042847, loss_ce: 0.024431
2021-12-11 21:27:32,523 iteration 4840 : loss : 0.034032, loss_ce: 0.014846
2021-12-11 21:27:33,983 iteration 4841 : loss : 0.041136, loss_ce: 0.017491
2021-12-11 21:27:35,391 iteration 4842 : loss : 0.037010, loss_ce: 0.020398
2021-12-11 21:27:36,780 iteration 4843 : loss : 0.049516, loss_ce: 0.023400
2021-12-11 21:27:38,104 iteration 4844 : loss : 0.026305, loss_ce: 0.014965
2021-12-11 21:27:38,104 Training Data Eval:
2021-12-11 21:27:44,929   Average segmentation loss on training set: 0.0162
2021-12-11 21:27:44,929 Validation Data Eval:
2021-12-11 21:27:47,289   Average segmentation loss on validation set: 0.0835
2021-12-11 21:27:48,627 iteration 4845 : loss : 0.026468, loss_ce: 0.012237
 71%|████████████████████▋        | 285/400 [2:01:29<51:42, 26.97s/it]2021-12-11 21:27:50,086 iteration 4846 : loss : 0.038467, loss_ce: 0.017413
2021-12-11 21:27:51,443 iteration 4847 : loss : 0.034430, loss_ce: 0.012759
2021-12-11 21:27:52,926 iteration 4848 : loss : 0.042423, loss_ce: 0.020616
2021-12-11 21:27:54,284 iteration 4849 : loss : 0.035539, loss_ce: 0.018291
2021-12-11 21:27:55,700 iteration 4850 : loss : 0.071857, loss_ce: 0.020771
2021-12-11 21:27:57,109 iteration 4851 : loss : 0.028504, loss_ce: 0.015148
2021-12-11 21:27:58,461 iteration 4852 : loss : 0.036138, loss_ce: 0.016643
2021-12-11 21:27:59,812 iteration 4853 : loss : 0.021261, loss_ce: 0.010712
2021-12-11 21:28:01,116 iteration 4854 : loss : 0.026781, loss_ce: 0.013559
2021-12-11 21:28:02,501 iteration 4855 : loss : 0.031191, loss_ce: 0.014656
2021-12-11 21:28:03,904 iteration 4856 : loss : 0.033844, loss_ce: 0.016625
2021-12-11 21:28:05,318 iteration 4857 : loss : 0.037423, loss_ce: 0.022311
2021-12-11 21:28:06,705 iteration 4858 : loss : 0.028513, loss_ce: 0.013614
2021-12-11 21:28:08,029 iteration 4859 : loss : 0.018498, loss_ce: 0.009908
2021-12-11 21:28:09,383 iteration 4860 : loss : 0.040649, loss_ce: 0.015766
2021-12-11 21:28:10,688 iteration 4861 : loss : 0.035362, loss_ce: 0.013146
2021-12-11 21:28:12,140 iteration 4862 : loss : 0.046699, loss_ce: 0.019951
 72%|████████████████████▋        | 286/400 [2:01:53<49:16, 25.93s/it]2021-12-11 21:28:13,604 iteration 4863 : loss : 0.042823, loss_ce: 0.016955
2021-12-11 21:28:15,014 iteration 4864 : loss : 0.040929, loss_ce: 0.018750
2021-12-11 21:28:16,402 iteration 4865 : loss : 0.032016, loss_ce: 0.014927
2021-12-11 21:28:17,788 iteration 4866 : loss : 0.033443, loss_ce: 0.015679
2021-12-11 21:28:19,202 iteration 4867 : loss : 0.043246, loss_ce: 0.017355
2021-12-11 21:28:20,598 iteration 4868 : loss : 0.042864, loss_ce: 0.022956
2021-12-11 21:28:21,947 iteration 4869 : loss : 0.027043, loss_ce: 0.012222
2021-12-11 21:28:23,238 iteration 4870 : loss : 0.025064, loss_ce: 0.012262
2021-12-11 21:28:24,582 iteration 4871 : loss : 0.029654, loss_ce: 0.013609
2021-12-11 21:28:25,948 iteration 4872 : loss : 0.028403, loss_ce: 0.015404
2021-12-11 21:28:27,321 iteration 4873 : loss : 0.029112, loss_ce: 0.012570
2021-12-11 21:28:28,701 iteration 4874 : loss : 0.036817, loss_ce: 0.015608
2021-12-11 21:28:30,045 iteration 4875 : loss : 0.029515, loss_ce: 0.012923
2021-12-11 21:28:31,328 iteration 4876 : loss : 0.026623, loss_ce: 0.013665
2021-12-11 21:28:32,684 iteration 4877 : loss : 0.020363, loss_ce: 0.010258
2021-12-11 21:28:34,060 iteration 4878 : loss : 0.049471, loss_ce: 0.021660
2021-12-11 21:28:35,476 iteration 4879 : loss : 0.029911, loss_ce: 0.011718
 72%|████████████████████▊        | 287/400 [2:02:16<47:22, 25.16s/it]2021-12-11 21:28:36,942 iteration 4880 : loss : 0.032820, loss_ce: 0.013440
2021-12-11 21:28:38,256 iteration 4881 : loss : 0.021868, loss_ce: 0.010011
2021-12-11 21:28:39,564 iteration 4882 : loss : 0.035463, loss_ce: 0.014707
2021-12-11 21:28:40,960 iteration 4883 : loss : 0.036031, loss_ce: 0.017138
2021-12-11 21:28:42,293 iteration 4884 : loss : 0.027923, loss_ce: 0.013835
2021-12-11 21:28:43,664 iteration 4885 : loss : 0.024499, loss_ce: 0.010466
2021-12-11 21:28:45,067 iteration 4886 : loss : 0.032373, loss_ce: 0.013891
2021-12-11 21:28:46,415 iteration 4887 : loss : 0.035904, loss_ce: 0.016845
2021-12-11 21:28:47,802 iteration 4888 : loss : 0.032530, loss_ce: 0.011631
2021-12-11 21:28:49,157 iteration 4889 : loss : 0.029188, loss_ce: 0.014797
2021-12-11 21:28:50,545 iteration 4890 : loss : 0.027118, loss_ce: 0.011974
2021-12-11 21:28:51,929 iteration 4891 : loss : 0.029052, loss_ce: 0.015369
2021-12-11 21:28:53,316 iteration 4892 : loss : 0.028518, loss_ce: 0.016048
2021-12-11 21:28:54,849 iteration 4893 : loss : 0.035456, loss_ce: 0.015583
2021-12-11 21:28:56,237 iteration 4894 : loss : 0.023892, loss_ce: 0.013074
2021-12-11 21:28:57,608 iteration 4895 : loss : 0.024573, loss_ce: 0.013636
2021-12-11 21:28:58,959 iteration 4896 : loss : 0.030138, loss_ce: 0.013583
 72%|████████████████████▉        | 288/400 [2:02:40<46:01, 24.65s/it]2021-12-11 21:29:00,321 iteration 4897 : loss : 0.022293, loss_ce: 0.012814
2021-12-11 21:29:01,684 iteration 4898 : loss : 0.027738, loss_ce: 0.012817
2021-12-11 21:29:03,009 iteration 4899 : loss : 0.027726, loss_ce: 0.014635
2021-12-11 21:29:04,440 iteration 4900 : loss : 0.057716, loss_ce: 0.023621
2021-12-11 21:29:05,828 iteration 4901 : loss : 0.029991, loss_ce: 0.012468
2021-12-11 21:29:07,164 iteration 4902 : loss : 0.028231, loss_ce: 0.010214
2021-12-11 21:29:08,548 iteration 4903 : loss : 0.024024, loss_ce: 0.012146
2021-12-11 21:29:09,921 iteration 4904 : loss : 0.030444, loss_ce: 0.017894
2021-12-11 21:29:11,277 iteration 4905 : loss : 0.025091, loss_ce: 0.013859
2021-12-11 21:29:12,623 iteration 4906 : loss : 0.031157, loss_ce: 0.013570
2021-12-11 21:29:13,999 iteration 4907 : loss : 0.040455, loss_ce: 0.018088
2021-12-11 21:29:15,283 iteration 4908 : loss : 0.028434, loss_ce: 0.011530
2021-12-11 21:29:16,650 iteration 4909 : loss : 0.041416, loss_ce: 0.017699
2021-12-11 21:29:18,003 iteration 4910 : loss : 0.039200, loss_ce: 0.012474
2021-12-11 21:29:19,453 iteration 4911 : loss : 0.038144, loss_ce: 0.018120
2021-12-11 21:29:20,894 iteration 4912 : loss : 0.068825, loss_ce: 0.022892
2021-12-11 21:29:22,242 iteration 4913 : loss : 0.024759, loss_ce: 0.011205
 72%|████████████████████▉        | 289/400 [2:03:03<44:50, 24.24s/it]2021-12-11 21:29:23,616 iteration 4914 : loss : 0.028764, loss_ce: 0.011915
2021-12-11 21:29:25,020 iteration 4915 : loss : 0.039060, loss_ce: 0.017203
2021-12-11 21:29:26,417 iteration 4916 : loss : 0.040031, loss_ce: 0.015655
2021-12-11 21:29:27,797 iteration 4917 : loss : 0.037264, loss_ce: 0.015412
2021-12-11 21:29:29,143 iteration 4918 : loss : 0.025285, loss_ce: 0.013928
2021-12-11 21:29:30,467 iteration 4919 : loss : 0.031002, loss_ce: 0.013851
2021-12-11 21:29:31,871 iteration 4920 : loss : 0.031052, loss_ce: 0.017461
2021-12-11 21:29:33,230 iteration 4921 : loss : 0.030326, loss_ce: 0.010858
2021-12-11 21:29:34,604 iteration 4922 : loss : 0.047562, loss_ce: 0.011913
2021-12-11 21:29:35,985 iteration 4923 : loss : 0.033737, loss_ce: 0.015243
2021-12-11 21:29:37,390 iteration 4924 : loss : 0.047653, loss_ce: 0.020248
2021-12-11 21:29:38,887 iteration 4925 : loss : 0.056515, loss_ce: 0.025806
2021-12-11 21:29:40,237 iteration 4926 : loss : 0.032080, loss_ce: 0.014704
2021-12-11 21:29:41,665 iteration 4927 : loss : 0.028194, loss_ce: 0.013880
2021-12-11 21:29:43,122 iteration 4928 : loss : 0.044597, loss_ce: 0.023762
2021-12-11 21:29:44,558 iteration 4929 : loss : 0.045049, loss_ce: 0.017497
2021-12-11 21:29:44,558 Training Data Eval:
2021-12-11 21:29:51,367   Average segmentation loss on training set: 0.0157
2021-12-11 21:29:51,368 Validation Data Eval:
2021-12-11 21:29:53,723   Average segmentation loss on validation set: 0.0854
2021-12-11 21:29:55,107 iteration 4930 : loss : 0.026896, loss_ce: 0.013496
 72%|█████████████████████        | 290/400 [2:03:36<49:11, 26.83s/it]2021-12-11 21:29:56,582 iteration 4931 : loss : 0.029981, loss_ce: 0.016019
2021-12-11 21:29:57,930 iteration 4932 : loss : 0.024683, loss_ce: 0.012356
2021-12-11 21:29:59,266 iteration 4933 : loss : 0.040442, loss_ce: 0.015425
2021-12-11 21:30:00,649 iteration 4934 : loss : 0.033340, loss_ce: 0.017529
2021-12-11 21:30:02,058 iteration 4935 : loss : 0.036318, loss_ce: 0.015025
2021-12-11 21:30:03,442 iteration 4936 : loss : 0.027990, loss_ce: 0.016761
2021-12-11 21:30:04,842 iteration 4937 : loss : 0.034680, loss_ce: 0.016578
2021-12-11 21:30:06,216 iteration 4938 : loss : 0.046658, loss_ce: 0.019514
2021-12-11 21:30:07,658 iteration 4939 : loss : 0.033245, loss_ce: 0.015306
2021-12-11 21:30:09,004 iteration 4940 : loss : 0.039179, loss_ce: 0.013763
2021-12-11 21:30:10,385 iteration 4941 : loss : 0.025854, loss_ce: 0.012344
2021-12-11 21:30:11,813 iteration 4942 : loss : 0.033959, loss_ce: 0.015500
2021-12-11 21:30:13,156 iteration 4943 : loss : 0.021161, loss_ce: 0.009751
2021-12-11 21:30:14,537 iteration 4944 : loss : 0.049159, loss_ce: 0.018554
2021-12-11 21:30:15,991 iteration 4945 : loss : 0.037576, loss_ce: 0.015917
2021-12-11 21:30:17,422 iteration 4946 : loss : 0.049365, loss_ce: 0.024800
2021-12-11 21:30:18,899 iteration 4947 : loss : 0.037033, loss_ce: 0.016364
 73%|█████████████████████        | 291/400 [2:04:00<47:05, 25.92s/it]2021-12-11 21:30:20,403 iteration 4948 : loss : 0.030383, loss_ce: 0.014420
2021-12-11 21:30:21,759 iteration 4949 : loss : 0.024145, loss_ce: 0.012053
2021-12-11 21:30:23,101 iteration 4950 : loss : 0.038573, loss_ce: 0.017352
2021-12-11 21:30:24,460 iteration 4951 : loss : 0.043122, loss_ce: 0.018327
2021-12-11 21:30:25,747 iteration 4952 : loss : 0.034525, loss_ce: 0.016442
2021-12-11 21:30:27,164 iteration 4953 : loss : 0.024746, loss_ce: 0.012546
2021-12-11 21:30:28,485 iteration 4954 : loss : 0.043480, loss_ce: 0.018885
2021-12-11 21:30:29,840 iteration 4955 : loss : 0.027694, loss_ce: 0.013334
2021-12-11 21:30:31,206 iteration 4956 : loss : 0.035596, loss_ce: 0.013924
2021-12-11 21:30:32,575 iteration 4957 : loss : 0.022268, loss_ce: 0.012346
2021-12-11 21:30:33,952 iteration 4958 : loss : 0.042017, loss_ce: 0.020626
2021-12-11 21:30:35,246 iteration 4959 : loss : 0.024432, loss_ce: 0.010310
2021-12-11 21:30:36,621 iteration 4960 : loss : 0.039909, loss_ce: 0.014074
2021-12-11 21:30:37,951 iteration 4961 : loss : 0.036401, loss_ce: 0.017242
2021-12-11 21:30:39,361 iteration 4962 : loss : 0.027657, loss_ce: 0.013152
2021-12-11 21:30:40,693 iteration 4963 : loss : 0.029761, loss_ce: 0.014839
2021-12-11 21:30:42,144 iteration 4964 : loss : 0.035409, loss_ce: 0.015928
 73%|█████████████████████▏       | 292/400 [2:04:23<45:12, 25.11s/it]2021-12-11 21:30:43,639 iteration 4965 : loss : 0.036728, loss_ce: 0.014415
2021-12-11 21:30:45,132 iteration 4966 : loss : 0.039170, loss_ce: 0.013747
2021-12-11 21:30:46,457 iteration 4967 : loss : 0.022971, loss_ce: 0.011910
2021-12-11 21:30:47,845 iteration 4968 : loss : 0.035137, loss_ce: 0.016886
2021-12-11 21:30:49,202 iteration 4969 : loss : 0.039308, loss_ce: 0.012723
2021-12-11 21:30:50,533 iteration 4970 : loss : 0.022048, loss_ce: 0.011757
2021-12-11 21:30:51,951 iteration 4971 : loss : 0.036102, loss_ce: 0.016782
2021-12-11 21:30:53,392 iteration 4972 : loss : 0.041995, loss_ce: 0.016528
2021-12-11 21:30:54,751 iteration 4973 : loss : 0.027701, loss_ce: 0.012995
2021-12-11 21:30:56,108 iteration 4974 : loss : 0.030434, loss_ce: 0.013928
2021-12-11 21:30:57,464 iteration 4975 : loss : 0.027296, loss_ce: 0.011787
2021-12-11 21:30:58,816 iteration 4976 : loss : 0.033584, loss_ce: 0.014927
2021-12-11 21:31:00,276 iteration 4977 : loss : 0.039236, loss_ce: 0.021400
2021-12-11 21:31:01,642 iteration 4978 : loss : 0.046183, loss_ce: 0.015396
2021-12-11 21:31:03,035 iteration 4979 : loss : 0.038663, loss_ce: 0.021466
2021-12-11 21:31:04,323 iteration 4980 : loss : 0.023782, loss_ce: 0.012153
2021-12-11 21:31:05,703 iteration 4981 : loss : 0.030736, loss_ce: 0.013351
 73%|█████████████████████▏       | 293/400 [2:04:47<43:57, 24.65s/it]2021-12-11 21:31:07,128 iteration 4982 : loss : 0.031359, loss_ce: 0.017288
2021-12-11 21:31:08,470 iteration 4983 : loss : 0.030013, loss_ce: 0.014329
2021-12-11 21:31:09,913 iteration 4984 : loss : 0.045433, loss_ce: 0.023039
2021-12-11 21:31:11,312 iteration 4985 : loss : 0.025821, loss_ce: 0.011453
2021-12-11 21:31:12,784 iteration 4986 : loss : 0.042709, loss_ce: 0.022424
2021-12-11 21:31:14,143 iteration 4987 : loss : 0.030236, loss_ce: 0.015773
2021-12-11 21:31:15,511 iteration 4988 : loss : 0.040266, loss_ce: 0.020175
2021-12-11 21:31:16,868 iteration 4989 : loss : 0.028304, loss_ce: 0.013419
2021-12-11 21:31:18,332 iteration 4990 : loss : 0.027754, loss_ce: 0.012232
2021-12-11 21:31:19,722 iteration 4991 : loss : 0.033355, loss_ce: 0.012828
2021-12-11 21:31:21,104 iteration 4992 : loss : 0.028399, loss_ce: 0.013983
2021-12-11 21:31:22,539 iteration 4993 : loss : 0.029530, loss_ce: 0.015195
2021-12-11 21:31:24,001 iteration 4994 : loss : 0.037747, loss_ce: 0.013838
2021-12-11 21:31:25,386 iteration 4995 : loss : 0.040596, loss_ce: 0.016993
2021-12-11 21:31:26,761 iteration 4996 : loss : 0.030437, loss_ce: 0.012922
2021-12-11 21:31:28,212 iteration 4997 : loss : 0.043546, loss_ce: 0.020014
2021-12-11 21:31:29,543 iteration 4998 : loss : 0.027659, loss_ce: 0.012503
 74%|█████████████████████▎       | 294/400 [2:05:10<43:07, 24.41s/it]2021-12-11 21:31:31,026 iteration 4999 : loss : 0.036852, loss_ce: 0.015356
2021-12-11 21:31:32,349 iteration 5000 : loss : 0.026973, loss_ce: 0.012317
2021-12-11 21:31:33,697 iteration 5001 : loss : 0.038871, loss_ce: 0.017411
2021-12-11 21:31:35,111 iteration 5002 : loss : 0.033881, loss_ce: 0.014821
2021-12-11 21:31:36,459 iteration 5003 : loss : 0.041824, loss_ce: 0.016133
2021-12-11 21:31:37,899 iteration 5004 : loss : 0.051254, loss_ce: 0.020980
2021-12-11 21:31:39,242 iteration 5005 : loss : 0.032971, loss_ce: 0.014116
2021-12-11 21:31:40,652 iteration 5006 : loss : 0.032202, loss_ce: 0.012760
2021-12-11 21:31:42,012 iteration 5007 : loss : 0.025497, loss_ce: 0.012602
2021-12-11 21:31:43,328 iteration 5008 : loss : 0.028665, loss_ce: 0.011521
2021-12-11 21:31:44,700 iteration 5009 : loss : 0.039296, loss_ce: 0.016375
2021-12-11 21:31:46,186 iteration 5010 : loss : 0.049117, loss_ce: 0.022081
2021-12-11 21:31:47,580 iteration 5011 : loss : 0.037182, loss_ce: 0.025443
2021-12-11 21:31:49,026 iteration 5012 : loss : 0.044931, loss_ce: 0.022085
2021-12-11 21:31:50,558 iteration 5013 : loss : 0.031348, loss_ce: 0.014473
2021-12-11 21:31:51,851 iteration 5014 : loss : 0.023936, loss_ce: 0.011262
2021-12-11 21:31:51,851 Training Data Eval:
2021-12-11 21:31:58,654   Average segmentation loss on training set: 0.0152
2021-12-11 21:31:58,654 Validation Data Eval:
2021-12-11 21:32:01,010   Average segmentation loss on validation set: 0.0840
2021-12-11 21:32:02,371 iteration 5015 : loss : 0.024637, loss_ce: 0.008500
 74%|█████████████████████▍       | 295/400 [2:05:43<47:07, 26.93s/it]2021-12-11 21:32:03,762 iteration 5016 : loss : 0.030687, loss_ce: 0.013869
2021-12-11 21:32:05,114 iteration 5017 : loss : 0.029238, loss_ce: 0.011956
2021-12-11 21:32:06,639 iteration 5018 : loss : 0.034391, loss_ce: 0.015709
2021-12-11 21:32:07,978 iteration 5019 : loss : 0.029019, loss_ce: 0.010703
2021-12-11 21:32:09,379 iteration 5020 : loss : 0.025614, loss_ce: 0.013431
2021-12-11 21:32:10,828 iteration 5021 : loss : 0.042642, loss_ce: 0.016507
2021-12-11 21:32:12,264 iteration 5022 : loss : 0.059491, loss_ce: 0.032887
2021-12-11 21:32:13,672 iteration 5023 : loss : 0.027030, loss_ce: 0.013366
2021-12-11 21:32:15,031 iteration 5024 : loss : 0.023198, loss_ce: 0.011544
2021-12-11 21:32:16,372 iteration 5025 : loss : 0.026055, loss_ce: 0.012067
2021-12-11 21:32:17,757 iteration 5026 : loss : 0.046146, loss_ce: 0.021057
2021-12-11 21:32:19,131 iteration 5027 : loss : 0.028067, loss_ce: 0.012614
2021-12-11 21:32:20,539 iteration 5028 : loss : 0.029316, loss_ce: 0.014061
2021-12-11 21:32:21,886 iteration 5029 : loss : 0.024880, loss_ce: 0.012454
2021-12-11 21:32:23,284 iteration 5030 : loss : 0.033982, loss_ce: 0.015138
2021-12-11 21:32:24,733 iteration 5031 : loss : 0.047841, loss_ce: 0.022364
2021-12-11 21:32:26,184 iteration 5032 : loss : 0.038468, loss_ce: 0.015757
 74%|█████████████████████▍       | 296/400 [2:06:07<45:03, 26.00s/it]2021-12-11 21:32:27,635 iteration 5033 : loss : 0.036970, loss_ce: 0.021728
2021-12-11 21:32:29,015 iteration 5034 : loss : 0.039222, loss_ce: 0.013901
2021-12-11 21:32:30,374 iteration 5035 : loss : 0.031073, loss_ce: 0.014230
2021-12-11 21:32:31,698 iteration 5036 : loss : 0.023945, loss_ce: 0.011406
2021-12-11 21:32:33,049 iteration 5037 : loss : 0.023805, loss_ce: 0.011939
2021-12-11 21:32:34,552 iteration 5038 : loss : 0.042512, loss_ce: 0.020888
2021-12-11 21:32:35,908 iteration 5039 : loss : 0.036595, loss_ce: 0.016186
2021-12-11 21:32:37,323 iteration 5040 : loss : 0.031413, loss_ce: 0.015259
2021-12-11 21:32:38,699 iteration 5041 : loss : 0.026125, loss_ce: 0.011997
2021-12-11 21:32:40,067 iteration 5042 : loss : 0.021596, loss_ce: 0.011218
2021-12-11 21:32:41,411 iteration 5043 : loss : 0.033432, loss_ce: 0.013360
2021-12-11 21:32:42,799 iteration 5044 : loss : 0.028795, loss_ce: 0.012870
2021-12-11 21:32:44,096 iteration 5045 : loss : 0.021956, loss_ce: 0.010038
2021-12-11 21:32:45,479 iteration 5046 : loss : 0.062770, loss_ce: 0.025015
2021-12-11 21:32:46,830 iteration 5047 : loss : 0.029026, loss_ce: 0.013360
2021-12-11 21:32:48,157 iteration 5048 : loss : 0.026531, loss_ce: 0.013196
2021-12-11 21:32:49,601 iteration 5049 : loss : 0.035101, loss_ce: 0.016447
 74%|█████████████████████▌       | 297/400 [2:06:30<43:17, 25.22s/it]2021-12-11 21:32:51,030 iteration 5050 : loss : 0.028083, loss_ce: 0.010794
2021-12-11 21:32:52,353 iteration 5051 : loss : 0.021930, loss_ce: 0.011811
2021-12-11 21:32:53,767 iteration 5052 : loss : 0.032602, loss_ce: 0.017693
2021-12-11 21:32:55,090 iteration 5053 : loss : 0.028716, loss_ce: 0.012861
2021-12-11 21:32:56,489 iteration 5054 : loss : 0.022668, loss_ce: 0.012226
2021-12-11 21:32:57,837 iteration 5055 : loss : 0.028293, loss_ce: 0.012173
2021-12-11 21:32:59,194 iteration 5056 : loss : 0.032832, loss_ce: 0.013056
2021-12-11 21:33:00,553 iteration 5057 : loss : 0.033266, loss_ce: 0.017577
2021-12-11 21:33:01,896 iteration 5058 : loss : 0.037834, loss_ce: 0.016601
2021-12-11 21:33:03,264 iteration 5059 : loss : 0.029944, loss_ce: 0.015517
2021-12-11 21:33:04,684 iteration 5060 : loss : 0.069307, loss_ce: 0.020602
2021-12-11 21:33:06,064 iteration 5061 : loss : 0.030106, loss_ce: 0.017066
2021-12-11 21:33:07,502 iteration 5062 : loss : 0.032779, loss_ce: 0.014874
2021-12-11 21:33:08,850 iteration 5063 : loss : 0.025378, loss_ce: 0.012115
2021-12-11 21:33:10,236 iteration 5064 : loss : 0.060052, loss_ce: 0.019724
2021-12-11 21:33:11,678 iteration 5065 : loss : 0.039776, loss_ce: 0.015978
2021-12-11 21:33:12,953 iteration 5066 : loss : 0.027621, loss_ce: 0.014697
 74%|█████████████████████▌       | 298/400 [2:06:54<41:55, 24.66s/it]2021-12-11 21:33:14,487 iteration 5067 : loss : 0.034341, loss_ce: 0.014449
2021-12-11 21:33:15,853 iteration 5068 : loss : 0.023981, loss_ce: 0.011993
2021-12-11 21:33:17,201 iteration 5069 : loss : 0.034418, loss_ce: 0.015869
2021-12-11 21:33:18,563 iteration 5070 : loss : 0.042624, loss_ce: 0.023087
2021-12-11 21:33:19,995 iteration 5071 : loss : 0.027246, loss_ce: 0.014668
2021-12-11 21:33:21,396 iteration 5072 : loss : 0.031520, loss_ce: 0.013020
2021-12-11 21:33:22,831 iteration 5073 : loss : 0.047206, loss_ce: 0.015355
2021-12-11 21:33:24,244 iteration 5074 : loss : 0.037417, loss_ce: 0.017314
2021-12-11 21:33:25,549 iteration 5075 : loss : 0.039426, loss_ce: 0.021511
2021-12-11 21:33:26,857 iteration 5076 : loss : 0.029077, loss_ce: 0.013725
2021-12-11 21:33:28,287 iteration 5077 : loss : 0.039200, loss_ce: 0.016715
2021-12-11 21:33:29,671 iteration 5078 : loss : 0.026759, loss_ce: 0.011980
2021-12-11 21:33:31,030 iteration 5079 : loss : 0.017669, loss_ce: 0.009137
2021-12-11 21:33:32,432 iteration 5080 : loss : 0.052785, loss_ce: 0.022015
2021-12-11 21:33:33,831 iteration 5081 : loss : 0.040797, loss_ce: 0.020592
2021-12-11 21:33:35,176 iteration 5082 : loss : 0.035619, loss_ce: 0.014915
2021-12-11 21:33:36,522 iteration 5083 : loss : 0.038733, loss_ce: 0.016427
 75%|█████████████████████▋       | 299/400 [2:07:17<40:57, 24.34s/it]2021-12-11 21:33:37,965 iteration 5084 : loss : 0.041501, loss_ce: 0.021612
2021-12-11 21:33:39,440 iteration 5085 : loss : 0.043263, loss_ce: 0.018953
2021-12-11 21:33:40,748 iteration 5086 : loss : 0.025014, loss_ce: 0.010909
2021-12-11 21:33:42,139 iteration 5087 : loss : 0.035194, loss_ce: 0.012403
2021-12-11 21:33:43,453 iteration 5088 : loss : 0.019977, loss_ce: 0.012151
2021-12-11 21:33:44,816 iteration 5089 : loss : 0.033035, loss_ce: 0.014926
2021-12-11 21:33:46,222 iteration 5090 : loss : 0.038284, loss_ce: 0.018461
2021-12-11 21:33:47,636 iteration 5091 : loss : 0.039535, loss_ce: 0.017237
2021-12-11 21:33:49,007 iteration 5092 : loss : 0.041308, loss_ce: 0.013832
2021-12-11 21:33:50,387 iteration 5093 : loss : 0.030804, loss_ce: 0.013851
2021-12-11 21:33:51,784 iteration 5094 : loss : 0.033439, loss_ce: 0.013470
2021-12-11 21:33:53,177 iteration 5095 : loss : 0.026392, loss_ce: 0.013472
2021-12-11 21:33:54,631 iteration 5096 : loss : 0.036480, loss_ce: 0.014220
2021-12-11 21:33:56,060 iteration 5097 : loss : 0.038610, loss_ce: 0.019618
2021-12-11 21:33:57,436 iteration 5098 : loss : 0.034942, loss_ce: 0.016519
2021-12-11 21:33:58,852 iteration 5099 : loss : 0.033830, loss_ce: 0.015533
2021-12-11 21:33:58,852 Training Data Eval:
2021-12-11 21:34:05,647   Average segmentation loss on training set: 0.0153
2021-12-11 21:34:05,647 Validation Data Eval:
2021-12-11 21:34:08,000   Average segmentation loss on validation set: 0.0872
2021-12-11 21:34:09,350 iteration 5100 : loss : 0.032328, loss_ce: 0.014060
2021-12-11 21:34:11,521 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed100epoch_299.pth
 75%|█████████████████████▊       | 300/400 [2:07:52<45:51, 27.52s/it]2021-12-11 21:34:12,954 iteration 5101 : loss : 0.041200, loss_ce: 0.014496
2021-12-11 21:34:14,404 iteration 5102 : loss : 0.032463, loss_ce: 0.017794
2021-12-11 21:34:15,743 iteration 5103 : loss : 0.030001, loss_ce: 0.013433
2021-12-11 21:34:17,120 iteration 5104 : loss : 0.030725, loss_ce: 0.013712
2021-12-11 21:34:18,475 iteration 5105 : loss : 0.032039, loss_ce: 0.013661
2021-12-11 21:34:19,849 iteration 5106 : loss : 0.037188, loss_ce: 0.017532
2021-12-11 21:34:21,239 iteration 5107 : loss : 0.033439, loss_ce: 0.015443
2021-12-11 21:34:22,636 iteration 5108 : loss : 0.023713, loss_ce: 0.012583
2021-12-11 21:34:24,015 iteration 5109 : loss : 0.027724, loss_ce: 0.013722
2021-12-11 21:34:25,303 iteration 5110 : loss : 0.026152, loss_ce: 0.012311
2021-12-11 21:34:26,699 iteration 5111 : loss : 0.033779, loss_ce: 0.013006
2021-12-11 21:34:28,030 iteration 5112 : loss : 0.022552, loss_ce: 0.011481
2021-12-11 21:34:29,484 iteration 5113 : loss : 0.033798, loss_ce: 0.014103
2021-12-11 21:34:30,821 iteration 5114 : loss : 0.024160, loss_ce: 0.011605
2021-12-11 21:34:32,164 iteration 5115 : loss : 0.031681, loss_ce: 0.020873
2021-12-11 21:34:33,550 iteration 5116 : loss : 0.040964, loss_ce: 0.017971
2021-12-11 21:34:34,934 iteration 5117 : loss : 0.031530, loss_ce: 0.014152
 75%|█████████████████████▊       | 301/400 [2:08:16<43:23, 26.30s/it]2021-12-11 21:34:36,393 iteration 5118 : loss : 0.026031, loss_ce: 0.013032
2021-12-11 21:34:37,779 iteration 5119 : loss : 0.040308, loss_ce: 0.017645
2021-12-11 21:34:39,238 iteration 5120 : loss : 0.027003, loss_ce: 0.010994
2021-12-11 21:34:40,578 iteration 5121 : loss : 0.022757, loss_ce: 0.011406
2021-12-11 21:34:41,930 iteration 5122 : loss : 0.041827, loss_ce: 0.016878
2021-12-11 21:34:43,303 iteration 5123 : loss : 0.034695, loss_ce: 0.020635
2021-12-11 21:34:44,633 iteration 5124 : loss : 0.050529, loss_ce: 0.018000
2021-12-11 21:34:46,022 iteration 5125 : loss : 0.027715, loss_ce: 0.012188
2021-12-11 21:34:47,373 iteration 5126 : loss : 0.029284, loss_ce: 0.013521
2021-12-11 21:34:48,769 iteration 5127 : loss : 0.041811, loss_ce: 0.017915
2021-12-11 21:34:50,255 iteration 5128 : loss : 0.037589, loss_ce: 0.015459
2021-12-11 21:34:51,706 iteration 5129 : loss : 0.035383, loss_ce: 0.015605
2021-12-11 21:34:53,026 iteration 5130 : loss : 0.029298, loss_ce: 0.014072
2021-12-11 21:34:54,437 iteration 5131 : loss : 0.031865, loss_ce: 0.011921
2021-12-11 21:34:55,840 iteration 5132 : loss : 0.033874, loss_ce: 0.016979
2021-12-11 21:34:57,176 iteration 5133 : loss : 0.026641, loss_ce: 0.013879
2021-12-11 21:34:58,507 iteration 5134 : loss : 0.018401, loss_ce: 0.009211
 76%|█████████████████████▉       | 302/400 [2:08:39<41:37, 25.49s/it]2021-12-11 21:34:59,935 iteration 5135 : loss : 0.028385, loss_ce: 0.013736
2021-12-11 21:35:01,397 iteration 5136 : loss : 0.032757, loss_ce: 0.013917
2021-12-11 21:35:02,850 iteration 5137 : loss : 0.030721, loss_ce: 0.013115
2021-12-11 21:35:04,316 iteration 5138 : loss : 0.059237, loss_ce: 0.029301
2021-12-11 21:35:05,636 iteration 5139 : loss : 0.020662, loss_ce: 0.009580
2021-12-11 21:35:07,072 iteration 5140 : loss : 0.043034, loss_ce: 0.023193
2021-12-11 21:35:08,398 iteration 5141 : loss : 0.030203, loss_ce: 0.017678
2021-12-11 21:35:09,871 iteration 5142 : loss : 0.039394, loss_ce: 0.016492
2021-12-11 21:35:11,199 iteration 5143 : loss : 0.021089, loss_ce: 0.010911
2021-12-11 21:35:12,516 iteration 5144 : loss : 0.023875, loss_ce: 0.013930
2021-12-11 21:35:13,916 iteration 5145 : loss : 0.036009, loss_ce: 0.017085
2021-12-11 21:35:15,429 iteration 5146 : loss : 0.054264, loss_ce: 0.019437
2021-12-11 21:35:16,822 iteration 5147 : loss : 0.030528, loss_ce: 0.018051
2021-12-11 21:35:18,234 iteration 5148 : loss : 0.037609, loss_ce: 0.016915
2021-12-11 21:35:19,594 iteration 5149 : loss : 0.031238, loss_ce: 0.013244
2021-12-11 21:35:20,948 iteration 5150 : loss : 0.025442, loss_ce: 0.012220
2021-12-11 21:35:22,262 iteration 5151 : loss : 0.027036, loss_ce: 0.011705
 76%|█████████████████████▉       | 303/400 [2:09:03<40:21, 24.96s/it]2021-12-11 21:35:23,736 iteration 5152 : loss : 0.043185, loss_ce: 0.017176
2021-12-11 21:35:25,051 iteration 5153 : loss : 0.021475, loss_ce: 0.012158
2021-12-11 21:35:26,466 iteration 5154 : loss : 0.032210, loss_ce: 0.014302
2021-12-11 21:35:27,798 iteration 5155 : loss : 0.019210, loss_ce: 0.009443
2021-12-11 21:35:29,156 iteration 5156 : loss : 0.031428, loss_ce: 0.014114
2021-12-11 21:35:30,595 iteration 5157 : loss : 0.025066, loss_ce: 0.012543
2021-12-11 21:35:31,997 iteration 5158 : loss : 0.028987, loss_ce: 0.012770
2021-12-11 21:35:33,431 iteration 5159 : loss : 0.048171, loss_ce: 0.019791
2021-12-11 21:35:34,868 iteration 5160 : loss : 0.029375, loss_ce: 0.013670
2021-12-11 21:35:36,310 iteration 5161 : loss : 0.054816, loss_ce: 0.023653
2021-12-11 21:35:37,659 iteration 5162 : loss : 0.024718, loss_ce: 0.010528
2021-12-11 21:35:39,029 iteration 5163 : loss : 0.028215, loss_ce: 0.011698
2021-12-11 21:35:40,347 iteration 5164 : loss : 0.045369, loss_ce: 0.026803
2021-12-11 21:35:41,733 iteration 5165 : loss : 0.023633, loss_ce: 0.009117
2021-12-11 21:35:43,127 iteration 5166 : loss : 0.034345, loss_ce: 0.017225
2021-12-11 21:35:44,463 iteration 5167 : loss : 0.037732, loss_ce: 0.018923
2021-12-11 21:35:45,890 iteration 5168 : loss : 0.034384, loss_ce: 0.017061
 76%|██████████████████████       | 304/400 [2:09:27<39:17, 24.56s/it]2021-12-11 21:35:47,291 iteration 5169 : loss : 0.034741, loss_ce: 0.014790
2021-12-11 21:35:48,677 iteration 5170 : loss : 0.028958, loss_ce: 0.016979
2021-12-11 21:35:50,034 iteration 5171 : loss : 0.038030, loss_ce: 0.013044
2021-12-11 21:35:51,426 iteration 5172 : loss : 0.028046, loss_ce: 0.012265
2021-12-11 21:35:52,769 iteration 5173 : loss : 0.038629, loss_ce: 0.020212
2021-12-11 21:35:54,172 iteration 5174 : loss : 0.037094, loss_ce: 0.016524
2021-12-11 21:35:55,638 iteration 5175 : loss : 0.036693, loss_ce: 0.014884
2021-12-11 21:35:57,024 iteration 5176 : loss : 0.033014, loss_ce: 0.014404
2021-12-11 21:35:58,527 iteration 5177 : loss : 0.051184, loss_ce: 0.025387
2021-12-11 21:35:59,896 iteration 5178 : loss : 0.034553, loss_ce: 0.011826
2021-12-11 21:36:01,337 iteration 5179 : loss : 0.030914, loss_ce: 0.016393
2021-12-11 21:36:02,721 iteration 5180 : loss : 0.029671, loss_ce: 0.014397
2021-12-11 21:36:04,152 iteration 5181 : loss : 0.037044, loss_ce: 0.018404
2021-12-11 21:36:05,582 iteration 5182 : loss : 0.048735, loss_ce: 0.019485
2021-12-11 21:36:07,023 iteration 5183 : loss : 0.036508, loss_ce: 0.014050
2021-12-11 21:36:08,499 iteration 5184 : loss : 0.036343, loss_ce: 0.020249
2021-12-11 21:36:08,500 Training Data Eval:
2021-12-11 21:36:15,308   Average segmentation loss on training set: 0.0149
2021-12-11 21:36:15,309 Validation Data Eval:
2021-12-11 21:36:17,665   Average segmentation loss on validation set: 0.0810
2021-12-11 21:36:19,043 iteration 5185 : loss : 0.028940, loss_ce: 0.014863
 76%|██████████████████████       | 305/400 [2:10:00<42:58, 27.14s/it]2021-12-11 21:36:20,476 iteration 5186 : loss : 0.030151, loss_ce: 0.014963
2021-12-11 21:36:21,912 iteration 5187 : loss : 0.034267, loss_ce: 0.019110
2021-12-11 21:36:23,227 iteration 5188 : loss : 0.029888, loss_ce: 0.012127
2021-12-11 21:36:24,609 iteration 5189 : loss : 0.023701, loss_ce: 0.012580
2021-12-11 21:36:25,929 iteration 5190 : loss : 0.033334, loss_ce: 0.016454
2021-12-11 21:36:27,264 iteration 5191 : loss : 0.030445, loss_ce: 0.014042
2021-12-11 21:36:28,702 iteration 5192 : loss : 0.038457, loss_ce: 0.015056
2021-12-11 21:36:30,106 iteration 5193 : loss : 0.038098, loss_ce: 0.020893
2021-12-11 21:36:31,491 iteration 5194 : loss : 0.028870, loss_ce: 0.014817
2021-12-11 21:36:32,891 iteration 5195 : loss : 0.042996, loss_ce: 0.016285
2021-12-11 21:36:34,404 iteration 5196 : loss : 0.047622, loss_ce: 0.013992
2021-12-11 21:36:35,703 iteration 5197 : loss : 0.033078, loss_ce: 0.011747
2021-12-11 21:36:37,133 iteration 5198 : loss : 0.045072, loss_ce: 0.021198
2021-12-11 21:36:38,570 iteration 5199 : loss : 0.026392, loss_ce: 0.012594
2021-12-11 21:36:39,983 iteration 5200 : loss : 0.031674, loss_ce: 0.011829
2021-12-11 21:36:41,345 iteration 5201 : loss : 0.033867, loss_ce: 0.012936
2021-12-11 21:36:42,726 iteration 5202 : loss : 0.028956, loss_ce: 0.014282
 76%|██████████████████████▏      | 306/400 [2:10:24<40:53, 26.11s/it]2021-12-11 21:36:44,068 iteration 5203 : loss : 0.022094, loss_ce: 0.009596
2021-12-11 21:36:45,478 iteration 5204 : loss : 0.030870, loss_ce: 0.014588
2021-12-11 21:36:46,874 iteration 5205 : loss : 0.037196, loss_ce: 0.018848
2021-12-11 21:36:48,199 iteration 5206 : loss : 0.021402, loss_ce: 0.011840
2021-12-11 21:36:49,560 iteration 5207 : loss : 0.025059, loss_ce: 0.012667
2021-12-11 21:36:51,011 iteration 5208 : loss : 0.033615, loss_ce: 0.015171
2021-12-11 21:36:52,418 iteration 5209 : loss : 0.034325, loss_ce: 0.016034
2021-12-11 21:36:53,790 iteration 5210 : loss : 0.025996, loss_ce: 0.011421
2021-12-11 21:36:55,133 iteration 5211 : loss : 0.022589, loss_ce: 0.009687
2021-12-11 21:36:56,525 iteration 5212 : loss : 0.026919, loss_ce: 0.011136
2021-12-11 21:36:57,941 iteration 5213 : loss : 0.063655, loss_ce: 0.022006
2021-12-11 21:36:59,316 iteration 5214 : loss : 0.039903, loss_ce: 0.024255
2021-12-11 21:37:00,671 iteration 5215 : loss : 0.063802, loss_ce: 0.027768
2021-12-11 21:37:02,036 iteration 5216 : loss : 0.022592, loss_ce: 0.011934
2021-12-11 21:37:03,363 iteration 5217 : loss : 0.023620, loss_ce: 0.011041
2021-12-11 21:37:04,761 iteration 5218 : loss : 0.031101, loss_ce: 0.012870
2021-12-11 21:37:06,153 iteration 5219 : loss : 0.039992, loss_ce: 0.017941
 77%|██████████████████████▎      | 307/400 [2:10:47<39:12, 25.30s/it]2021-12-11 21:37:07,659 iteration 5220 : loss : 0.033500, loss_ce: 0.013927
2021-12-11 21:37:09,019 iteration 5221 : loss : 0.027211, loss_ce: 0.012826
2021-12-11 21:37:10,320 iteration 5222 : loss : 0.039075, loss_ce: 0.015012
2021-12-11 21:37:11,682 iteration 5223 : loss : 0.035490, loss_ce: 0.017181
2021-12-11 21:37:13,138 iteration 5224 : loss : 0.042556, loss_ce: 0.020959
2021-12-11 21:37:14,456 iteration 5225 : loss : 0.029102, loss_ce: 0.011236
2021-12-11 21:37:15,952 iteration 5226 : loss : 0.032638, loss_ce: 0.015166
2021-12-11 21:37:17,245 iteration 5227 : loss : 0.024792, loss_ce: 0.011475
2021-12-11 21:37:18,544 iteration 5228 : loss : 0.020453, loss_ce: 0.011085
2021-12-11 21:37:19,876 iteration 5229 : loss : 0.035859, loss_ce: 0.016983
2021-12-11 21:37:21,266 iteration 5230 : loss : 0.035963, loss_ce: 0.020368
2021-12-11 21:37:22,661 iteration 5231 : loss : 0.044803, loss_ce: 0.018019
2021-12-11 21:37:24,002 iteration 5232 : loss : 0.041084, loss_ce: 0.019920
2021-12-11 21:37:25,413 iteration 5233 : loss : 0.033436, loss_ce: 0.015561
2021-12-11 21:37:26,709 iteration 5234 : loss : 0.019473, loss_ce: 0.009386
2021-12-11 21:37:28,167 iteration 5235 : loss : 0.044039, loss_ce: 0.017109
2021-12-11 21:37:29,528 iteration 5236 : loss : 0.033246, loss_ce: 0.014646
 77%|██████████████████████▎      | 308/400 [2:11:10<37:54, 24.72s/it]2021-12-11 21:37:30,993 iteration 5237 : loss : 0.032171, loss_ce: 0.013387
2021-12-11 21:37:32,368 iteration 5238 : loss : 0.027047, loss_ce: 0.014118
2021-12-11 21:37:33,748 iteration 5239 : loss : 0.028011, loss_ce: 0.011182
2021-12-11 21:37:35,223 iteration 5240 : loss : 0.032582, loss_ce: 0.013721
2021-12-11 21:37:36,638 iteration 5241 : loss : 0.035671, loss_ce: 0.016939
2021-12-11 21:37:38,048 iteration 5242 : loss : 0.039217, loss_ce: 0.019260
2021-12-11 21:37:39,444 iteration 5243 : loss : 0.028356, loss_ce: 0.012018
2021-12-11 21:37:40,861 iteration 5244 : loss : 0.035526, loss_ce: 0.015676
2021-12-11 21:37:42,249 iteration 5245 : loss : 0.031266, loss_ce: 0.013818
2021-12-11 21:37:43,532 iteration 5246 : loss : 0.021761, loss_ce: 0.012591
2021-12-11 21:37:44,905 iteration 5247 : loss : 0.032898, loss_ce: 0.014246
2021-12-11 21:37:46,317 iteration 5248 : loss : 0.028329, loss_ce: 0.013009
2021-12-11 21:37:47,675 iteration 5249 : loss : 0.035658, loss_ce: 0.016358
2021-12-11 21:37:49,046 iteration 5250 : loss : 0.035338, loss_ce: 0.016961
2021-12-11 21:37:50,478 iteration 5251 : loss : 0.032843, loss_ce: 0.017256
2021-12-11 21:37:51,850 iteration 5252 : loss : 0.030959, loss_ce: 0.012458
2021-12-11 21:37:53,255 iteration 5253 : loss : 0.023549, loss_ce: 0.012048
 77%|██████████████████████▍      | 309/400 [2:11:34<37:02, 24.42s/it]2021-12-11 21:37:54,625 iteration 5254 : loss : 0.023225, loss_ce: 0.010808
2021-12-11 21:37:56,064 iteration 5255 : loss : 0.030968, loss_ce: 0.013848
2021-12-11 21:37:57,460 iteration 5256 : loss : 0.025864, loss_ce: 0.012702
2021-12-11 21:37:58,924 iteration 5257 : loss : 0.031289, loss_ce: 0.011977
2021-12-11 21:38:00,258 iteration 5258 : loss : 0.037816, loss_ce: 0.018144
2021-12-11 21:38:01,671 iteration 5259 : loss : 0.054582, loss_ce: 0.019226
2021-12-11 21:38:03,026 iteration 5260 : loss : 0.029439, loss_ce: 0.015253
2021-12-11 21:38:04,402 iteration 5261 : loss : 0.023476, loss_ce: 0.012516
2021-12-11 21:38:05,717 iteration 5262 : loss : 0.019276, loss_ce: 0.011020
2021-12-11 21:38:07,108 iteration 5263 : loss : 0.036490, loss_ce: 0.017201
2021-12-11 21:38:08,458 iteration 5264 : loss : 0.030555, loss_ce: 0.013624
2021-12-11 21:38:09,757 iteration 5265 : loss : 0.027887, loss_ce: 0.012123
2021-12-11 21:38:11,139 iteration 5266 : loss : 0.031109, loss_ce: 0.015629
2021-12-11 21:38:12,462 iteration 5267 : loss : 0.044159, loss_ce: 0.018110
2021-12-11 21:38:13,912 iteration 5268 : loss : 0.036432, loss_ce: 0.016345
2021-12-11 21:38:15,366 iteration 5269 : loss : 0.044405, loss_ce: 0.024285
2021-12-11 21:38:15,366 Training Data Eval:
2021-12-11 21:38:22,154   Average segmentation loss on training set: 0.0154
2021-12-11 21:38:22,154 Validation Data Eval:
2021-12-11 21:38:24,510   Average segmentation loss on validation set: 0.0858
2021-12-11 21:38:25,937 iteration 5270 : loss : 0.037625, loss_ce: 0.016937
 78%|██████████████████████▍      | 310/400 [2:12:07<40:20, 26.90s/it]2021-12-11 21:38:27,415 iteration 5271 : loss : 0.032498, loss_ce: 0.016086
2021-12-11 21:38:28,803 iteration 5272 : loss : 0.030985, loss_ce: 0.013509
2021-12-11 21:38:30,175 iteration 5273 : loss : 0.029696, loss_ce: 0.014550
2021-12-11 21:38:31,518 iteration 5274 : loss : 0.026520, loss_ce: 0.012252
2021-12-11 21:38:32,917 iteration 5275 : loss : 0.047881, loss_ce: 0.016159
2021-12-11 21:38:34,320 iteration 5276 : loss : 0.038714, loss_ce: 0.015786
2021-12-11 21:38:35,634 iteration 5277 : loss : 0.035660, loss_ce: 0.014223
2021-12-11 21:38:37,138 iteration 5278 : loss : 0.034953, loss_ce: 0.017629
2021-12-11 21:38:38,511 iteration 5279 : loss : 0.034324, loss_ce: 0.012620
2021-12-11 21:38:39,959 iteration 5280 : loss : 0.038012, loss_ce: 0.018600
2021-12-11 21:38:41,342 iteration 5281 : loss : 0.029424, loss_ce: 0.016782
2021-12-11 21:38:42,754 iteration 5282 : loss : 0.025321, loss_ce: 0.012292
2021-12-11 21:38:44,129 iteration 5283 : loss : 0.031587, loss_ce: 0.012842
2021-12-11 21:38:45,538 iteration 5284 : loss : 0.044081, loss_ce: 0.022229
2021-12-11 21:38:46,939 iteration 5285 : loss : 0.032174, loss_ce: 0.018199
2021-12-11 21:38:48,368 iteration 5286 : loss : 0.026457, loss_ce: 0.012016
2021-12-11 21:38:49,852 iteration 5287 : loss : 0.045477, loss_ce: 0.024448
 78%|██████████████████████▌      | 311/400 [2:12:31<38:34, 26.01s/it]2021-12-11 21:38:51,248 iteration 5288 : loss : 0.031862, loss_ce: 0.013794
2021-12-11 21:38:52,643 iteration 5289 : loss : 0.042104, loss_ce: 0.016108
2021-12-11 21:38:54,075 iteration 5290 : loss : 0.031507, loss_ce: 0.014595
2021-12-11 21:38:55,518 iteration 5291 : loss : 0.039122, loss_ce: 0.019156
2021-12-11 21:38:56,941 iteration 5292 : loss : 0.020841, loss_ce: 0.010277
2021-12-11 21:38:58,309 iteration 5293 : loss : 0.038907, loss_ce: 0.014640
2021-12-11 21:38:59,759 iteration 5294 : loss : 0.029972, loss_ce: 0.015614
2021-12-11 21:39:01,106 iteration 5295 : loss : 0.023508, loss_ce: 0.011847
2021-12-11 21:39:02,501 iteration 5296 : loss : 0.036757, loss_ce: 0.014367
2021-12-11 21:39:03,955 iteration 5297 : loss : 0.045265, loss_ce: 0.016787
2021-12-11 21:39:05,290 iteration 5298 : loss : 0.024698, loss_ce: 0.012747
2021-12-11 21:39:06,735 iteration 5299 : loss : 0.036343, loss_ce: 0.017471
2021-12-11 21:39:08,045 iteration 5300 : loss : 0.029147, loss_ce: 0.013922
2021-12-11 21:39:09,431 iteration 5301 : loss : 0.037769, loss_ce: 0.015919
2021-12-11 21:39:10,777 iteration 5302 : loss : 0.031012, loss_ce: 0.015161
2021-12-11 21:39:12,055 iteration 5303 : loss : 0.024988, loss_ce: 0.011128
2021-12-11 21:39:13,477 iteration 5304 : loss : 0.040168, loss_ce: 0.016886
 78%|██████████████████████▌      | 312/400 [2:12:54<37:05, 25.29s/it]2021-12-11 21:39:14,919 iteration 5305 : loss : 0.034415, loss_ce: 0.014502
2021-12-11 21:39:16,323 iteration 5306 : loss : 0.051831, loss_ce: 0.019105
2021-12-11 21:39:17,709 iteration 5307 : loss : 0.026371, loss_ce: 0.012687
2021-12-11 21:39:19,123 iteration 5308 : loss : 0.035276, loss_ce: 0.016999
2021-12-11 21:39:20,504 iteration 5309 : loss : 0.031004, loss_ce: 0.015116
2021-12-11 21:39:21,885 iteration 5310 : loss : 0.027380, loss_ce: 0.014959
2021-12-11 21:39:23,268 iteration 5311 : loss : 0.031548, loss_ce: 0.013892
2021-12-11 21:39:24,644 iteration 5312 : loss : 0.034113, loss_ce: 0.013743
2021-12-11 21:39:26,063 iteration 5313 : loss : 0.037096, loss_ce: 0.013976
2021-12-11 21:39:27,410 iteration 5314 : loss : 0.025337, loss_ce: 0.013224
2021-12-11 21:39:28,821 iteration 5315 : loss : 0.045159, loss_ce: 0.020422
2021-12-11 21:39:30,088 iteration 5316 : loss : 0.022121, loss_ce: 0.012456
2021-12-11 21:39:31,476 iteration 5317 : loss : 0.029632, loss_ce: 0.013500
2021-12-11 21:39:32,894 iteration 5318 : loss : 0.039714, loss_ce: 0.015817
2021-12-11 21:39:34,319 iteration 5319 : loss : 0.039250, loss_ce: 0.016483
2021-12-11 21:39:35,749 iteration 5320 : loss : 0.053369, loss_ce: 0.026196
2021-12-11 21:39:37,076 iteration 5321 : loss : 0.029685, loss_ce: 0.010127
 78%|██████████████████████▋      | 313/400 [2:13:18<35:56, 24.79s/it]2021-12-11 21:39:38,569 iteration 5322 : loss : 0.037883, loss_ce: 0.019852
2021-12-11 21:39:40,023 iteration 5323 : loss : 0.032119, loss_ce: 0.013579
2021-12-11 21:39:41,417 iteration 5324 : loss : 0.038838, loss_ce: 0.015871
2021-12-11 21:39:42,817 iteration 5325 : loss : 0.034441, loss_ce: 0.014698
2021-12-11 21:39:44,289 iteration 5326 : loss : 0.047601, loss_ce: 0.018362
2021-12-11 21:39:45,693 iteration 5327 : loss : 0.044845, loss_ce: 0.020568
2021-12-11 21:39:47,064 iteration 5328 : loss : 0.033836, loss_ce: 0.015829
2021-12-11 21:39:48,445 iteration 5329 : loss : 0.036306, loss_ce: 0.015867
2021-12-11 21:39:49,830 iteration 5330 : loss : 0.040391, loss_ce: 0.019237
2021-12-11 21:39:51,152 iteration 5331 : loss : 0.025340, loss_ce: 0.012506
2021-12-11 21:39:52,705 iteration 5332 : loss : 0.055027, loss_ce: 0.023656
2021-12-11 21:39:54,080 iteration 5333 : loss : 0.033994, loss_ce: 0.015051
2021-12-11 21:39:55,365 iteration 5334 : loss : 0.028683, loss_ce: 0.011138
2021-12-11 21:39:56,743 iteration 5335 : loss : 0.033596, loss_ce: 0.012979
2021-12-11 21:39:58,104 iteration 5336 : loss : 0.027123, loss_ce: 0.012020
2021-12-11 21:39:59,478 iteration 5337 : loss : 0.027618, loss_ce: 0.014280
2021-12-11 21:40:00,866 iteration 5338 : loss : 0.044955, loss_ce: 0.021379
 78%|██████████████████████▊      | 314/400 [2:13:42<35:05, 24.49s/it]2021-12-11 21:40:02,305 iteration 5339 : loss : 0.025413, loss_ce: 0.012146
2021-12-11 21:40:03,660 iteration 5340 : loss : 0.025422, loss_ce: 0.012277
2021-12-11 21:40:05,109 iteration 5341 : loss : 0.027973, loss_ce: 0.010464
2021-12-11 21:40:06,531 iteration 5342 : loss : 0.032011, loss_ce: 0.012875
2021-12-11 21:40:07,933 iteration 5343 : loss : 0.031486, loss_ce: 0.014263
2021-12-11 21:40:09,271 iteration 5344 : loss : 0.025021, loss_ce: 0.013410
2021-12-11 21:40:10,631 iteration 5345 : loss : 0.029187, loss_ce: 0.015707
2021-12-11 21:40:11,976 iteration 5346 : loss : 0.031076, loss_ce: 0.014273
2021-12-11 21:40:13,372 iteration 5347 : loss : 0.025564, loss_ce: 0.013696
2021-12-11 21:40:14,708 iteration 5348 : loss : 0.034656, loss_ce: 0.014581
2021-12-11 21:40:16,054 iteration 5349 : loss : 0.027857, loss_ce: 0.012268
2021-12-11 21:40:17,416 iteration 5350 : loss : 0.027673, loss_ce: 0.011819
2021-12-11 21:40:18,739 iteration 5351 : loss : 0.025736, loss_ce: 0.011543
2021-12-11 21:40:20,190 iteration 5352 : loss : 0.035970, loss_ce: 0.016172
2021-12-11 21:40:21,593 iteration 5353 : loss : 0.029636, loss_ce: 0.014259
2021-12-11 21:40:22,931 iteration 5354 : loss : 0.028653, loss_ce: 0.014679
2021-12-11 21:40:22,931 Training Data Eval:
2021-12-11 21:40:29,751   Average segmentation loss on training set: 0.0152
2021-12-11 21:40:29,752 Validation Data Eval:
2021-12-11 21:40:32,109   Average segmentation loss on validation set: 0.0922
2021-12-11 21:40:33,498 iteration 5355 : loss : 0.035250, loss_ce: 0.018069
 79%|██████████████████████▊      | 315/400 [2:14:14<38:08, 26.93s/it]2021-12-11 21:40:34,985 iteration 5356 : loss : 0.045124, loss_ce: 0.019704
2021-12-11 21:40:36,340 iteration 5357 : loss : 0.023773, loss_ce: 0.012913
2021-12-11 21:40:37,682 iteration 5358 : loss : 0.026158, loss_ce: 0.012866
2021-12-11 21:40:39,021 iteration 5359 : loss : 0.024771, loss_ce: 0.011276
2021-12-11 21:40:40,467 iteration 5360 : loss : 0.040881, loss_ce: 0.019119
2021-12-11 21:40:41,845 iteration 5361 : loss : 0.032023, loss_ce: 0.017037
2021-12-11 21:40:43,249 iteration 5362 : loss : 0.025236, loss_ce: 0.012044
2021-12-11 21:40:44,815 iteration 5363 : loss : 0.035040, loss_ce: 0.017191
2021-12-11 21:40:46,219 iteration 5364 : loss : 0.059376, loss_ce: 0.019451
2021-12-11 21:40:47,641 iteration 5365 : loss : 0.036503, loss_ce: 0.014949
2021-12-11 21:40:49,003 iteration 5366 : loss : 0.030952, loss_ce: 0.013201
2021-12-11 21:40:50,302 iteration 5367 : loss : 0.026561, loss_ce: 0.011926
2021-12-11 21:40:51,804 iteration 5368 : loss : 0.042301, loss_ce: 0.016045
2021-12-11 21:40:53,181 iteration 5369 : loss : 0.020409, loss_ce: 0.011035
2021-12-11 21:40:54,561 iteration 5370 : loss : 0.034899, loss_ce: 0.018950
2021-12-11 21:40:55,945 iteration 5371 : loss : 0.038156, loss_ce: 0.013275
2021-12-11 21:40:57,244 iteration 5372 : loss : 0.023361, loss_ce: 0.012229
 79%|██████████████████████▉      | 316/400 [2:14:38<36:22, 25.98s/it]2021-12-11 21:40:58,667 iteration 5373 : loss : 0.029014, loss_ce: 0.014217
2021-12-11 21:41:00,029 iteration 5374 : loss : 0.039298, loss_ce: 0.014264
2021-12-11 21:41:01,437 iteration 5375 : loss : 0.023261, loss_ce: 0.011190
2021-12-11 21:41:02,842 iteration 5376 : loss : 0.035938, loss_ce: 0.013524
2021-12-11 21:41:04,265 iteration 5377 : loss : 0.038815, loss_ce: 0.013758
2021-12-11 21:41:05,680 iteration 5378 : loss : 0.036715, loss_ce: 0.014787
2021-12-11 21:41:07,061 iteration 5379 : loss : 0.028100, loss_ce: 0.013305
2021-12-11 21:41:08,374 iteration 5380 : loss : 0.026841, loss_ce: 0.013831
2021-12-11 21:41:09,825 iteration 5381 : loss : 0.038854, loss_ce: 0.017766
2021-12-11 21:41:11,217 iteration 5382 : loss : 0.027230, loss_ce: 0.012398
2021-12-11 21:41:12,595 iteration 5383 : loss : 0.038630, loss_ce: 0.021180
2021-12-11 21:41:13,960 iteration 5384 : loss : 0.021786, loss_ce: 0.012381
2021-12-11 21:41:15,371 iteration 5385 : loss : 0.046779, loss_ce: 0.020374
2021-12-11 21:41:16,726 iteration 5386 : loss : 0.034754, loss_ce: 0.015627
2021-12-11 21:41:18,221 iteration 5387 : loss : 0.038010, loss_ce: 0.018344
2021-12-11 21:41:19,654 iteration 5388 : loss : 0.024349, loss_ce: 0.010546
2021-12-11 21:41:21,146 iteration 5389 : loss : 0.032451, loss_ce: 0.015074
 79%|██████████████████████▉      | 317/400 [2:15:02<35:04, 25.35s/it]2021-12-11 21:41:22,559 iteration 5390 : loss : 0.023126, loss_ce: 0.010941
2021-12-11 21:41:24,042 iteration 5391 : loss : 0.044587, loss_ce: 0.018975
2021-12-11 21:41:25,429 iteration 5392 : loss : 0.034571, loss_ce: 0.013922
2021-12-11 21:41:26,774 iteration 5393 : loss : 0.031178, loss_ce: 0.014129
2021-12-11 21:41:28,136 iteration 5394 : loss : 0.034800, loss_ce: 0.014607
2021-12-11 21:41:29,510 iteration 5395 : loss : 0.027758, loss_ce: 0.012260
2021-12-11 21:41:30,854 iteration 5396 : loss : 0.027029, loss_ce: 0.012660
2021-12-11 21:41:32,261 iteration 5397 : loss : 0.033193, loss_ce: 0.016274
2021-12-11 21:41:33,770 iteration 5398 : loss : 0.040272, loss_ce: 0.015252
2021-12-11 21:41:35,188 iteration 5399 : loss : 0.036804, loss_ce: 0.014041
2021-12-11 21:41:36,551 iteration 5400 : loss : 0.033094, loss_ce: 0.016639
2021-12-11 21:41:37,955 iteration 5401 : loss : 0.035750, loss_ce: 0.021144
2021-12-11 21:41:39,307 iteration 5402 : loss : 0.027985, loss_ce: 0.015643
2021-12-11 21:41:40,677 iteration 5403 : loss : 0.030441, loss_ce: 0.015154
2021-12-11 21:41:42,086 iteration 5404 : loss : 0.023576, loss_ce: 0.011493
2021-12-11 21:41:43,574 iteration 5405 : loss : 0.034035, loss_ce: 0.014358
2021-12-11 21:41:44,903 iteration 5406 : loss : 0.026035, loss_ce: 0.013694
 80%|███████████████████████      | 318/400 [2:15:26<33:59, 24.87s/it]2021-12-11 21:41:46,299 iteration 5407 : loss : 0.024794, loss_ce: 0.012776
2021-12-11 21:41:47,655 iteration 5408 : loss : 0.026521, loss_ce: 0.013965
2021-12-11 21:41:48,950 iteration 5409 : loss : 0.023655, loss_ce: 0.010513
2021-12-11 21:41:50,347 iteration 5410 : loss : 0.028026, loss_ce: 0.013515
2021-12-11 21:41:51,675 iteration 5411 : loss : 0.023197, loss_ce: 0.011450
2021-12-11 21:41:53,068 iteration 5412 : loss : 0.031921, loss_ce: 0.013717
2021-12-11 21:41:54,434 iteration 5413 : loss : 0.028460, loss_ce: 0.014153
2021-12-11 21:41:55,841 iteration 5414 : loss : 0.022939, loss_ce: 0.012017
2021-12-11 21:41:57,366 iteration 5415 : loss : 0.048768, loss_ce: 0.023621
2021-12-11 21:41:58,730 iteration 5416 : loss : 0.030765, loss_ce: 0.011856
2021-12-11 21:42:00,105 iteration 5417 : loss : 0.025676, loss_ce: 0.010940
2021-12-11 21:42:01,594 iteration 5418 : loss : 0.027903, loss_ce: 0.013957
2021-12-11 21:42:02,932 iteration 5419 : loss : 0.030579, loss_ce: 0.015056
2021-12-11 21:42:04,255 iteration 5420 : loss : 0.027719, loss_ce: 0.013570
2021-12-11 21:42:05,697 iteration 5421 : loss : 0.046085, loss_ce: 0.018771
2021-12-11 21:42:07,161 iteration 5422 : loss : 0.042172, loss_ce: 0.013922
2021-12-11 21:42:08,616 iteration 5423 : loss : 0.041754, loss_ce: 0.020233
 80%|███████████████████████▏     | 319/400 [2:15:49<33:06, 24.52s/it]2021-12-11 21:42:10,062 iteration 5424 : loss : 0.040958, loss_ce: 0.016853
2021-12-11 21:42:11,421 iteration 5425 : loss : 0.038814, loss_ce: 0.012899
2021-12-11 21:42:12,913 iteration 5426 : loss : 0.050785, loss_ce: 0.025803
2021-12-11 21:42:14,227 iteration 5427 : loss : 0.026853, loss_ce: 0.011182
2021-12-11 21:42:15,596 iteration 5428 : loss : 0.023440, loss_ce: 0.011253
2021-12-11 21:42:16,964 iteration 5429 : loss : 0.041007, loss_ce: 0.017534
2021-12-11 21:42:18,503 iteration 5430 : loss : 0.038249, loss_ce: 0.014342
2021-12-11 21:42:19,893 iteration 5431 : loss : 0.039767, loss_ce: 0.017548
2021-12-11 21:42:21,260 iteration 5432 : loss : 0.034955, loss_ce: 0.016574
2021-12-11 21:42:22,733 iteration 5433 : loss : 0.032613, loss_ce: 0.017651
2021-12-11 21:42:24,111 iteration 5434 : loss : 0.023256, loss_ce: 0.011586
2021-12-11 21:42:25,636 iteration 5435 : loss : 0.052147, loss_ce: 0.025322
2021-12-11 21:42:27,037 iteration 5436 : loss : 0.022818, loss_ce: 0.011720
2021-12-11 21:42:28,310 iteration 5437 : loss : 0.022364, loss_ce: 0.011190
2021-12-11 21:42:29,822 iteration 5438 : loss : 0.034101, loss_ce: 0.015173
2021-12-11 21:42:31,173 iteration 5439 : loss : 0.027815, loss_ce: 0.012213
2021-12-11 21:42:31,173 Training Data Eval:
2021-12-11 21:42:38,028   Average segmentation loss on training set: 0.0151
2021-12-11 21:42:38,028 Validation Data Eval:
2021-12-11 21:42:40,395   Average segmentation loss on validation set: 0.0793
2021-12-11 21:42:41,844 iteration 5440 : loss : 0.056971, loss_ce: 0.020341
 80%|███████████████████████▏     | 320/400 [2:16:23<36:11, 27.14s/it]2021-12-11 21:42:43,177 iteration 5441 : loss : 0.024084, loss_ce: 0.010620
2021-12-11 21:42:44,568 iteration 5442 : loss : 0.031773, loss_ce: 0.012935
2021-12-11 21:42:46,001 iteration 5443 : loss : 0.019545, loss_ce: 0.009353
2021-12-11 21:42:47,424 iteration 5444 : loss : 0.028809, loss_ce: 0.013207
2021-12-11 21:42:48,931 iteration 5445 : loss : 0.046668, loss_ce: 0.022826
2021-12-11 21:42:50,274 iteration 5446 : loss : 0.027185, loss_ce: 0.012468
2021-12-11 21:42:51,635 iteration 5447 : loss : 0.028561, loss_ce: 0.014329
2021-12-11 21:42:53,033 iteration 5448 : loss : 0.035085, loss_ce: 0.014322
2021-12-11 21:42:54,405 iteration 5449 : loss : 0.028645, loss_ce: 0.013317
2021-12-11 21:42:55,838 iteration 5450 : loss : 0.032667, loss_ce: 0.017928
2021-12-11 21:42:57,217 iteration 5451 : loss : 0.042334, loss_ce: 0.018585
2021-12-11 21:42:58,659 iteration 5452 : loss : 0.033527, loss_ce: 0.016385
2021-12-11 21:43:00,055 iteration 5453 : loss : 0.036273, loss_ce: 0.018446
2021-12-11 21:43:01,339 iteration 5454 : loss : 0.018976, loss_ce: 0.010967
2021-12-11 21:43:02,818 iteration 5455 : loss : 0.034416, loss_ce: 0.013918
2021-12-11 21:43:04,254 iteration 5456 : loss : 0.031566, loss_ce: 0.014490
2021-12-11 21:43:05,636 iteration 5457 : loss : 0.027540, loss_ce: 0.012058
 80%|███████████████████████▎     | 321/400 [2:16:46<34:24, 26.14s/it]2021-12-11 21:43:07,053 iteration 5458 : loss : 0.027735, loss_ce: 0.013570
2021-12-11 21:43:08,403 iteration 5459 : loss : 0.028390, loss_ce: 0.013070
2021-12-11 21:43:09,824 iteration 5460 : loss : 0.044882, loss_ce: 0.019863
2021-12-11 21:43:11,245 iteration 5461 : loss : 0.028668, loss_ce: 0.014535
2021-12-11 21:43:12,665 iteration 5462 : loss : 0.026611, loss_ce: 0.011369
2021-12-11 21:43:14,046 iteration 5463 : loss : 0.028935, loss_ce: 0.013545
2021-12-11 21:43:15,430 iteration 5464 : loss : 0.032946, loss_ce: 0.014031
2021-12-11 21:43:16,767 iteration 5465 : loss : 0.025075, loss_ce: 0.010500
2021-12-11 21:43:18,256 iteration 5466 : loss : 0.032408, loss_ce: 0.015013
2021-12-11 21:43:19,656 iteration 5467 : loss : 0.024396, loss_ce: 0.014146
2021-12-11 21:43:21,079 iteration 5468 : loss : 0.051955, loss_ce: 0.027576
2021-12-11 21:43:22,423 iteration 5469 : loss : 0.023889, loss_ce: 0.010529
2021-12-11 21:43:23,869 iteration 5470 : loss : 0.041736, loss_ce: 0.017684
2021-12-11 21:43:25,272 iteration 5471 : loss : 0.023979, loss_ce: 0.010274
2021-12-11 21:43:26,747 iteration 5472 : loss : 0.030291, loss_ce: 0.013430
2021-12-11 21:43:28,195 iteration 5473 : loss : 0.033115, loss_ce: 0.013613
2021-12-11 21:43:29,644 iteration 5474 : loss : 0.032571, loss_ce: 0.015909
 80%|███████████████████████▎     | 322/400 [2:17:10<33:08, 25.50s/it]2021-12-11 21:43:31,102 iteration 5475 : loss : 0.023120, loss_ce: 0.009883
2021-12-11 21:43:32,489 iteration 5476 : loss : 0.033955, loss_ce: 0.012928
2021-12-11 21:43:33,853 iteration 5477 : loss : 0.023089, loss_ce: 0.012423
2021-12-11 21:43:35,251 iteration 5478 : loss : 0.030502, loss_ce: 0.016331
2021-12-11 21:43:36,766 iteration 5479 : loss : 0.029953, loss_ce: 0.016329
2021-12-11 21:43:38,261 iteration 5480 : loss : 0.035743, loss_ce: 0.018313
2021-12-11 21:43:39,637 iteration 5481 : loss : 0.026210, loss_ce: 0.010391
2021-12-11 21:43:41,015 iteration 5482 : loss : 0.023269, loss_ce: 0.010660
2021-12-11 21:43:42,406 iteration 5483 : loss : 0.024999, loss_ce: 0.011974
2021-12-11 21:43:43,752 iteration 5484 : loss : 0.030574, loss_ce: 0.015404
2021-12-11 21:43:45,042 iteration 5485 : loss : 0.026890, loss_ce: 0.012249
2021-12-11 21:43:46,503 iteration 5486 : loss : 0.051078, loss_ce: 0.020352
2021-12-11 21:43:48,020 iteration 5487 : loss : 0.054679, loss_ce: 0.022562
2021-12-11 21:43:49,409 iteration 5488 : loss : 0.033605, loss_ce: 0.015859
2021-12-11 21:43:50,813 iteration 5489 : loss : 0.029470, loss_ce: 0.015435
2021-12-11 21:43:52,163 iteration 5490 : loss : 0.052154, loss_ce: 0.019060
2021-12-11 21:43:53,525 iteration 5491 : loss : 0.032163, loss_ce: 0.015322
 81%|███████████████████████▍     | 323/400 [2:17:34<32:05, 25.01s/it]2021-12-11 21:43:54,954 iteration 5492 : loss : 0.030294, loss_ce: 0.014332
2021-12-11 21:43:56,313 iteration 5493 : loss : 0.025552, loss_ce: 0.014036
2021-12-11 21:43:57,632 iteration 5494 : loss : 0.027539, loss_ce: 0.010471
2021-12-11 21:43:59,065 iteration 5495 : loss : 0.081357, loss_ce: 0.012695
2021-12-11 21:44:00,405 iteration 5496 : loss : 0.031030, loss_ce: 0.011702
2021-12-11 21:44:01,789 iteration 5497 : loss : 0.022998, loss_ce: 0.010687
2021-12-11 21:44:03,190 iteration 5498 : loss : 0.044479, loss_ce: 0.028651
2021-12-11 21:44:04,562 iteration 5499 : loss : 0.034026, loss_ce: 0.015730
2021-12-11 21:44:05,962 iteration 5500 : loss : 0.040364, loss_ce: 0.017632
2021-12-11 21:44:07,445 iteration 5501 : loss : 0.036527, loss_ce: 0.017951
2021-12-11 21:44:08,869 iteration 5502 : loss : 0.037728, loss_ce: 0.018613
2021-12-11 21:44:10,227 iteration 5503 : loss : 0.039219, loss_ce: 0.015583
2021-12-11 21:44:11,628 iteration 5504 : loss : 0.035366, loss_ce: 0.019210
2021-12-11 21:44:13,039 iteration 5505 : loss : 0.028473, loss_ce: 0.014204
2021-12-11 21:44:14,418 iteration 5506 : loss : 0.029136, loss_ce: 0.012854
2021-12-11 21:44:15,847 iteration 5507 : loss : 0.033582, loss_ce: 0.013202
2021-12-11 21:44:17,264 iteration 5508 : loss : 0.035658, loss_ce: 0.018112
 81%|███████████████████████▍     | 324/400 [2:17:58<31:11, 24.63s/it]2021-12-11 21:44:18,704 iteration 5509 : loss : 0.031850, loss_ce: 0.013338
2021-12-11 21:44:20,245 iteration 5510 : loss : 0.038015, loss_ce: 0.015929
2021-12-11 21:44:21,665 iteration 5511 : loss : 0.043647, loss_ce: 0.025107
2021-12-11 21:44:23,147 iteration 5512 : loss : 0.036011, loss_ce: 0.010817
2021-12-11 21:44:24,512 iteration 5513 : loss : 0.032143, loss_ce: 0.010743
2021-12-11 21:44:25,956 iteration 5514 : loss : 0.029203, loss_ce: 0.014763
2021-12-11 21:44:27,454 iteration 5515 : loss : 0.045248, loss_ce: 0.018082
2021-12-11 21:44:28,769 iteration 5516 : loss : 0.023735, loss_ce: 0.011702
2021-12-11 21:44:30,208 iteration 5517 : loss : 0.029601, loss_ce: 0.013118
2021-12-11 21:44:31,596 iteration 5518 : loss : 0.031149, loss_ce: 0.016462
2021-12-11 21:44:32,969 iteration 5519 : loss : 0.046387, loss_ce: 0.017916
2021-12-11 21:44:34,403 iteration 5520 : loss : 0.059499, loss_ce: 0.023684
2021-12-11 21:44:35,945 iteration 5521 : loss : 0.034554, loss_ce: 0.016118
2021-12-11 21:44:37,279 iteration 5522 : loss : 0.022294, loss_ce: 0.012267
2021-12-11 21:44:38,695 iteration 5523 : loss : 0.029358, loss_ce: 0.014562
2021-12-11 21:44:40,136 iteration 5524 : loss : 0.036425, loss_ce: 0.018312
2021-12-11 21:44:40,136 Training Data Eval:
2021-12-11 21:44:46,966   Average segmentation loss on training set: 0.0150
2021-12-11 21:44:46,966 Validation Data Eval:
2021-12-11 21:44:49,318   Average segmentation loss on validation set: 0.0919
2021-12-11 21:44:50,748 iteration 5525 : loss : 0.043637, loss_ce: 0.020786
 81%|███████████████████████▌     | 325/400 [2:18:32<34:06, 27.28s/it]2021-12-11 21:44:52,254 iteration 5526 : loss : 0.037155, loss_ce: 0.017031
2021-12-11 21:44:53,606 iteration 5527 : loss : 0.037440, loss_ce: 0.016595
2021-12-11 21:44:55,012 iteration 5528 : loss : 0.034105, loss_ce: 0.014774
2021-12-11 21:44:56,366 iteration 5529 : loss : 0.025993, loss_ce: 0.010824
2021-12-11 21:44:57,692 iteration 5530 : loss : 0.034385, loss_ce: 0.013020
2021-12-11 21:44:59,078 iteration 5531 : loss : 0.021998, loss_ce: 0.010748
2021-12-11 21:45:00,379 iteration 5532 : loss : 0.032009, loss_ce: 0.013732
2021-12-11 21:45:01,794 iteration 5533 : loss : 0.027796, loss_ce: 0.013378
2021-12-11 21:45:03,191 iteration 5534 : loss : 0.044749, loss_ce: 0.017723
2021-12-11 21:45:04,599 iteration 5535 : loss : 0.037371, loss_ce: 0.018531
2021-12-11 21:45:05,967 iteration 5536 : loss : 0.035267, loss_ce: 0.013082
2021-12-11 21:45:07,343 iteration 5537 : loss : 0.026120, loss_ce: 0.011217
2021-12-11 21:45:08,705 iteration 5538 : loss : 0.028142, loss_ce: 0.016114
2021-12-11 21:45:10,107 iteration 5539 : loss : 0.026787, loss_ce: 0.013636
2021-12-11 21:45:11,526 iteration 5540 : loss : 0.025208, loss_ce: 0.011576
2021-12-11 21:45:12,834 iteration 5541 : loss : 0.027330, loss_ce: 0.013039
2021-12-11 21:45:14,185 iteration 5542 : loss : 0.029986, loss_ce: 0.014521
 82%|███████████████████████▋     | 326/400 [2:18:55<32:13, 26.13s/it]2021-12-11 21:45:15,687 iteration 5543 : loss : 0.048309, loss_ce: 0.023259
2021-12-11 21:45:17,219 iteration 5544 : loss : 0.036185, loss_ce: 0.015334
2021-12-11 21:45:18,706 iteration 5545 : loss : 0.032171, loss_ce: 0.015344
2021-12-11 21:45:20,030 iteration 5546 : loss : 0.020280, loss_ce: 0.011677
2021-12-11 21:45:21,470 iteration 5547 : loss : 0.027110, loss_ce: 0.013843
2021-12-11 21:45:22,838 iteration 5548 : loss : 0.033390, loss_ce: 0.013863
2021-12-11 21:45:24,226 iteration 5549 : loss : 0.024627, loss_ce: 0.012785
2021-12-11 21:45:25,650 iteration 5550 : loss : 0.032587, loss_ce: 0.014954
2021-12-11 21:45:27,095 iteration 5551 : loss : 0.048324, loss_ce: 0.017826
2021-12-11 21:45:28,603 iteration 5552 : loss : 0.046699, loss_ce: 0.016831
2021-12-11 21:45:30,050 iteration 5553 : loss : 0.027194, loss_ce: 0.012580
2021-12-11 21:45:31,564 iteration 5554 : loss : 0.043290, loss_ce: 0.014669
2021-12-11 21:45:33,047 iteration 5555 : loss : 0.038947, loss_ce: 0.016337
2021-12-11 21:45:34,516 iteration 5556 : loss : 0.027649, loss_ce: 0.013642
2021-12-11 21:45:35,976 iteration 5557 : loss : 0.027386, loss_ce: 0.011282
2021-12-11 21:45:37,367 iteration 5558 : loss : 0.039357, loss_ce: 0.016210
2021-12-11 21:45:38,708 iteration 5559 : loss : 0.023722, loss_ce: 0.011107
 82%|███████████████████████▋     | 327/400 [2:19:20<31:12, 25.65s/it]2021-12-11 21:45:40,173 iteration 5560 : loss : 0.026477, loss_ce: 0.013172
2021-12-11 21:45:41,673 iteration 5561 : loss : 0.033986, loss_ce: 0.012723
2021-12-11 21:45:43,097 iteration 5562 : loss : 0.029247, loss_ce: 0.012340
2021-12-11 21:45:44,544 iteration 5563 : loss : 0.042231, loss_ce: 0.015737
2021-12-11 21:45:45,855 iteration 5564 : loss : 0.033585, loss_ce: 0.012867
2021-12-11 21:45:47,258 iteration 5565 : loss : 0.041525, loss_ce: 0.014674
2021-12-11 21:45:48,599 iteration 5566 : loss : 0.033356, loss_ce: 0.013297
2021-12-11 21:45:50,029 iteration 5567 : loss : 0.034650, loss_ce: 0.019136
2021-12-11 21:45:51,467 iteration 5568 : loss : 0.046165, loss_ce: 0.023677
2021-12-11 21:45:52,920 iteration 5569 : loss : 0.042934, loss_ce: 0.021002
2021-12-11 21:45:54,314 iteration 5570 : loss : 0.027874, loss_ce: 0.015782
2021-12-11 21:45:55,698 iteration 5571 : loss : 0.030674, loss_ce: 0.012068
2021-12-11 21:45:56,995 iteration 5572 : loss : 0.018641, loss_ce: 0.009749
2021-12-11 21:45:58,415 iteration 5573 : loss : 0.029007, loss_ce: 0.013412
2021-12-11 21:45:59,765 iteration 5574 : loss : 0.024073, loss_ce: 0.012289
2021-12-11 21:46:01,084 iteration 5575 : loss : 0.023103, loss_ce: 0.011317
2021-12-11 21:46:02,387 iteration 5576 : loss : 0.026951, loss_ce: 0.011254
 82%|███████████████████████▊     | 328/400 [2:19:43<30:03, 25.05s/it]2021-12-11 21:46:03,893 iteration 5577 : loss : 0.045747, loss_ce: 0.018288
2021-12-11 21:46:05,286 iteration 5578 : loss : 0.031580, loss_ce: 0.014129
2021-12-11 21:46:06,780 iteration 5579 : loss : 0.040568, loss_ce: 0.014630
2021-12-11 21:46:08,233 iteration 5580 : loss : 0.032999, loss_ce: 0.013416
2021-12-11 21:46:09,645 iteration 5581 : loss : 0.038661, loss_ce: 0.019693
2021-12-11 21:46:11,015 iteration 5582 : loss : 0.030031, loss_ce: 0.014305
2021-12-11 21:46:12,363 iteration 5583 : loss : 0.021562, loss_ce: 0.009391
2021-12-11 21:46:13,730 iteration 5584 : loss : 0.029805, loss_ce: 0.013105
2021-12-11 21:46:15,053 iteration 5585 : loss : 0.033113, loss_ce: 0.013372
2021-12-11 21:46:16,396 iteration 5586 : loss : 0.024394, loss_ce: 0.012418
2021-12-11 21:46:17,903 iteration 5587 : loss : 0.032195, loss_ce: 0.016388
2021-12-11 21:46:19,313 iteration 5588 : loss : 0.032332, loss_ce: 0.016928
2021-12-11 21:46:20,797 iteration 5589 : loss : 0.034731, loss_ce: 0.013360
2021-12-11 21:46:22,131 iteration 5590 : loss : 0.028228, loss_ce: 0.014052
2021-12-11 21:46:23,573 iteration 5591 : loss : 0.039424, loss_ce: 0.017565
2021-12-11 21:46:25,015 iteration 5592 : loss : 0.033575, loss_ce: 0.015754
2021-12-11 21:46:26,325 iteration 5593 : loss : 0.025262, loss_ce: 0.014895
 82%|███████████████████████▊     | 329/400 [2:20:07<29:14, 24.72s/it]2021-12-11 21:46:27,770 iteration 5594 : loss : 0.029794, loss_ce: 0.011106
2021-12-11 21:46:29,117 iteration 5595 : loss : 0.022571, loss_ce: 0.011677
2021-12-11 21:46:30,591 iteration 5596 : loss : 0.041629, loss_ce: 0.016292
2021-12-11 21:46:32,024 iteration 5597 : loss : 0.031009, loss_ce: 0.014738
2021-12-11 21:46:33,565 iteration 5598 : loss : 0.050750, loss_ce: 0.026438
2021-12-11 21:46:35,072 iteration 5599 : loss : 0.042825, loss_ce: 0.016542
2021-12-11 21:46:36,425 iteration 5600 : loss : 0.026673, loss_ce: 0.011991
2021-12-11 21:46:37,858 iteration 5601 : loss : 0.031049, loss_ce: 0.013195
2021-12-11 21:46:39,275 iteration 5602 : loss : 0.025841, loss_ce: 0.012097
2021-12-11 21:46:40,693 iteration 5603 : loss : 0.042501, loss_ce: 0.024852
2021-12-11 21:46:42,084 iteration 5604 : loss : 0.050214, loss_ce: 0.018469
2021-12-11 21:46:43,442 iteration 5605 : loss : 0.030732, loss_ce: 0.016410
2021-12-11 21:46:44,861 iteration 5606 : loss : 0.043458, loss_ce: 0.016603
2021-12-11 21:46:46,204 iteration 5607 : loss : 0.029946, loss_ce: 0.015090
2021-12-11 21:46:47,615 iteration 5608 : loss : 0.028050, loss_ce: 0.016044
2021-12-11 21:46:48,958 iteration 5609 : loss : 0.027346, loss_ce: 0.012553
2021-12-11 21:46:48,958 Training Data Eval:
2021-12-11 21:46:55,757   Average segmentation loss on training set: 0.0145
2021-12-11 21:46:55,758 Validation Data Eval:
2021-12-11 21:46:58,121   Average segmentation loss on validation set: 0.0802
2021-12-11 21:46:59,531 iteration 5610 : loss : 0.044712, loss_ce: 0.019211
 82%|███████████████████████▉     | 330/400 [2:20:40<31:48, 27.27s/it]2021-12-11 21:47:00,923 iteration 5611 : loss : 0.021298, loss_ce: 0.010527
2021-12-11 21:47:02,359 iteration 5612 : loss : 0.037404, loss_ce: 0.016107
2021-12-11 21:47:03,712 iteration 5613 : loss : 0.044015, loss_ce: 0.012822
2021-12-11 21:47:05,063 iteration 5614 : loss : 0.031050, loss_ce: 0.015308
2021-12-11 21:47:06,436 iteration 5615 : loss : 0.037120, loss_ce: 0.019574
2021-12-11 21:47:07,818 iteration 5616 : loss : 0.025393, loss_ce: 0.009745
2021-12-11 21:47:09,341 iteration 5617 : loss : 0.049217, loss_ce: 0.019196
2021-12-11 21:47:10,779 iteration 5618 : loss : 0.054413, loss_ce: 0.017971
2021-12-11 21:47:12,174 iteration 5619 : loss : 0.033898, loss_ce: 0.018212
2021-12-11 21:47:13,518 iteration 5620 : loss : 0.032595, loss_ce: 0.015364
2021-12-11 21:47:14,858 iteration 5621 : loss : 0.028166, loss_ce: 0.013377
2021-12-11 21:47:16,169 iteration 5622 : loss : 0.027312, loss_ce: 0.014356
2021-12-11 21:47:17,640 iteration 5623 : loss : 0.029931, loss_ce: 0.012532
2021-12-11 21:47:19,035 iteration 5624 : loss : 0.023234, loss_ce: 0.011117
2021-12-11 21:47:20,423 iteration 5625 : loss : 0.038646, loss_ce: 0.018315
2021-12-11 21:47:21,866 iteration 5626 : loss : 0.037564, loss_ce: 0.015015
2021-12-11 21:47:23,267 iteration 5627 : loss : 0.025846, loss_ce: 0.013248
 83%|███████████████████████▉     | 331/400 [2:21:04<30:08, 26.21s/it]2021-12-11 21:47:24,740 iteration 5628 : loss : 0.024483, loss_ce: 0.011751
2021-12-11 21:47:26,151 iteration 5629 : loss : 0.040641, loss_ce: 0.017782
2021-12-11 21:47:27,596 iteration 5630 : loss : 0.024762, loss_ce: 0.011847
2021-12-11 21:47:29,064 iteration 5631 : loss : 0.033111, loss_ce: 0.018825
2021-12-11 21:47:30,562 iteration 5632 : loss : 0.031891, loss_ce: 0.015177
2021-12-11 21:47:31,890 iteration 5633 : loss : 0.039982, loss_ce: 0.014061
2021-12-11 21:47:33,375 iteration 5634 : loss : 0.045602, loss_ce: 0.015954
2021-12-11 21:47:34,770 iteration 5635 : loss : 0.027251, loss_ce: 0.013553
2021-12-11 21:47:36,104 iteration 5636 : loss : 0.045073, loss_ce: 0.014998
2021-12-11 21:47:37,597 iteration 5637 : loss : 0.033469, loss_ce: 0.016179
2021-12-11 21:47:39,012 iteration 5638 : loss : 0.027231, loss_ce: 0.014763
2021-12-11 21:47:40,438 iteration 5639 : loss : 0.030904, loss_ce: 0.012480
2021-12-11 21:47:41,850 iteration 5640 : loss : 0.049761, loss_ce: 0.019973
2021-12-11 21:47:43,180 iteration 5641 : loss : 0.021030, loss_ce: 0.009785
2021-12-11 21:47:44,668 iteration 5642 : loss : 0.033311, loss_ce: 0.016648
2021-12-11 21:47:45,964 iteration 5643 : loss : 0.027064, loss_ce: 0.015338
2021-12-11 21:47:47,328 iteration 5644 : loss : 0.036374, loss_ce: 0.019171
 83%|████████████████████████     | 332/400 [2:21:28<28:58, 25.57s/it]2021-12-11 21:47:48,716 iteration 5645 : loss : 0.023467, loss_ce: 0.010197
2021-12-11 21:47:50,089 iteration 5646 : loss : 0.029615, loss_ce: 0.011838
2021-12-11 21:47:51,411 iteration 5647 : loss : 0.027272, loss_ce: 0.011627
2021-12-11 21:47:52,785 iteration 5648 : loss : 0.031247, loss_ce: 0.016266
2021-12-11 21:47:54,123 iteration 5649 : loss : 0.030904, loss_ce: 0.012832
2021-12-11 21:47:55,559 iteration 5650 : loss : 0.036878, loss_ce: 0.015610
2021-12-11 21:47:56,932 iteration 5651 : loss : 0.036485, loss_ce: 0.017881
2021-12-11 21:47:58,319 iteration 5652 : loss : 0.028188, loss_ce: 0.011540
2021-12-11 21:47:59,692 iteration 5653 : loss : 0.032060, loss_ce: 0.012016
2021-12-11 21:48:01,058 iteration 5654 : loss : 0.029759, loss_ce: 0.014867
2021-12-11 21:48:02,354 iteration 5655 : loss : 0.027134, loss_ce: 0.014767
2021-12-11 21:48:03,824 iteration 5656 : loss : 0.040319, loss_ce: 0.018512
2021-12-11 21:48:05,265 iteration 5657 : loss : 0.040110, loss_ce: 0.018062
2021-12-11 21:48:06,696 iteration 5658 : loss : 0.043576, loss_ce: 0.021594
2021-12-11 21:48:08,036 iteration 5659 : loss : 0.027648, loss_ce: 0.013328
2021-12-11 21:48:09,440 iteration 5660 : loss : 0.036406, loss_ce: 0.015288
2021-12-11 21:48:10,820 iteration 5661 : loss : 0.031671, loss_ce: 0.015760
 83%|████████████████████████▏    | 333/400 [2:21:52<27:51, 24.94s/it]2021-12-11 21:48:12,256 iteration 5662 : loss : 0.042893, loss_ce: 0.018292
2021-12-11 21:48:13,601 iteration 5663 : loss : 0.035345, loss_ce: 0.013361
2021-12-11 21:48:14,983 iteration 5664 : loss : 0.028475, loss_ce: 0.015044
2021-12-11 21:48:16,316 iteration 5665 : loss : 0.023361, loss_ce: 0.011778
2021-12-11 21:48:17,710 iteration 5666 : loss : 0.031283, loss_ce: 0.011593
2021-12-11 21:48:19,138 iteration 5667 : loss : 0.030319, loss_ce: 0.014102
2021-12-11 21:48:20,531 iteration 5668 : loss : 0.039271, loss_ce: 0.015110
2021-12-11 21:48:21,853 iteration 5669 : loss : 0.024441, loss_ce: 0.011933
2021-12-11 21:48:23,208 iteration 5670 : loss : 0.031350, loss_ce: 0.012852
2021-12-11 21:48:24,542 iteration 5671 : loss : 0.025349, loss_ce: 0.011387
2021-12-11 21:48:26,012 iteration 5672 : loss : 0.040631, loss_ce: 0.020298
2021-12-11 21:48:27,365 iteration 5673 : loss : 0.032932, loss_ce: 0.014728
2021-12-11 21:48:28,739 iteration 5674 : loss : 0.034620, loss_ce: 0.014414
2021-12-11 21:48:30,083 iteration 5675 : loss : 0.027780, loss_ce: 0.016931
2021-12-11 21:48:31,516 iteration 5676 : loss : 0.035681, loss_ce: 0.015957
2021-12-11 21:48:32,882 iteration 5677 : loss : 0.033861, loss_ce: 0.012545
2021-12-11 21:48:34,341 iteration 5678 : loss : 0.036780, loss_ce: 0.017492
 84%|████████████████████████▏    | 334/400 [2:22:15<26:57, 24.51s/it]2021-12-11 21:48:35,819 iteration 5679 : loss : 0.025593, loss_ce: 0.012312
2021-12-11 21:48:37,151 iteration 5680 : loss : 0.034474, loss_ce: 0.017051
2021-12-11 21:48:38,548 iteration 5681 : loss : 0.030073, loss_ce: 0.015875
2021-12-11 21:48:39,916 iteration 5682 : loss : 0.032852, loss_ce: 0.014865
2021-12-11 21:48:41,335 iteration 5683 : loss : 0.035067, loss_ce: 0.016701
2021-12-11 21:48:42,773 iteration 5684 : loss : 0.030021, loss_ce: 0.013380
2021-12-11 21:48:44,195 iteration 5685 : loss : 0.029343, loss_ce: 0.011709
2021-12-11 21:48:45,577 iteration 5686 : loss : 0.027823, loss_ce: 0.013942
2021-12-11 21:48:46,925 iteration 5687 : loss : 0.024744, loss_ce: 0.009628
2021-12-11 21:48:48,301 iteration 5688 : loss : 0.030606, loss_ce: 0.011827
2021-12-11 21:48:49,662 iteration 5689 : loss : 0.026901, loss_ce: 0.014157
2021-12-11 21:48:51,065 iteration 5690 : loss : 0.038311, loss_ce: 0.021276
2021-12-11 21:48:52,415 iteration 5691 : loss : 0.026220, loss_ce: 0.010475
2021-12-11 21:48:53,762 iteration 5692 : loss : 0.022083, loss_ce: 0.010131
2021-12-11 21:48:55,173 iteration 5693 : loss : 0.044059, loss_ce: 0.021468
2021-12-11 21:48:56,586 iteration 5694 : loss : 0.035427, loss_ce: 0.017159
2021-12-11 21:48:56,587 Training Data Eval:
2021-12-11 21:49:03,429   Average segmentation loss on training set: 0.0143
2021-12-11 21:49:03,430 Validation Data Eval:
2021-12-11 21:49:05,792   Average segmentation loss on validation set: 0.0898
2021-12-11 21:49:07,138 iteration 5695 : loss : 0.033393, loss_ce: 0.014049
 84%|████████████████████████▎    | 335/400 [2:22:48<29:15, 27.00s/it]2021-12-11 21:49:08,561 iteration 5696 : loss : 0.028894, loss_ce: 0.011806
2021-12-11 21:49:10,000 iteration 5697 : loss : 0.042810, loss_ce: 0.020851
2021-12-11 21:49:11,389 iteration 5698 : loss : 0.035626, loss_ce: 0.015868
2021-12-11 21:49:12,841 iteration 5699 : loss : 0.041265, loss_ce: 0.017341
2021-12-11 21:49:14,226 iteration 5700 : loss : 0.030234, loss_ce: 0.013501
2021-12-11 21:49:15,594 iteration 5701 : loss : 0.038094, loss_ce: 0.021717
2021-12-11 21:49:16,990 iteration 5702 : loss : 0.022840, loss_ce: 0.010556
2021-12-11 21:49:18,361 iteration 5703 : loss : 0.021489, loss_ce: 0.011675
2021-12-11 21:49:19,774 iteration 5704 : loss : 0.029926, loss_ce: 0.015237
2021-12-11 21:49:21,178 iteration 5705 : loss : 0.029042, loss_ce: 0.013929
2021-12-11 21:49:22,528 iteration 5706 : loss : 0.029980, loss_ce: 0.009444
2021-12-11 21:49:24,024 iteration 5707 : loss : 0.037839, loss_ce: 0.015618
2021-12-11 21:49:25,392 iteration 5708 : loss : 0.029045, loss_ce: 0.013165
2021-12-11 21:49:26,795 iteration 5709 : loss : 0.026932, loss_ce: 0.014046
2021-12-11 21:49:28,267 iteration 5710 : loss : 0.034862, loss_ce: 0.015200
2021-12-11 21:49:29,637 iteration 5711 : loss : 0.036558, loss_ce: 0.020028
2021-12-11 21:49:31,007 iteration 5712 : loss : 0.027702, loss_ce: 0.014791
 84%|████████████████████████▎    | 336/400 [2:23:12<27:48, 26.06s/it]2021-12-11 21:49:32,399 iteration 5713 : loss : 0.030388, loss_ce: 0.011824
2021-12-11 21:49:33,748 iteration 5714 : loss : 0.025162, loss_ce: 0.011931
2021-12-11 21:49:35,096 iteration 5715 : loss : 0.024746, loss_ce: 0.012125
2021-12-11 21:49:36,486 iteration 5716 : loss : 0.022807, loss_ce: 0.012451
2021-12-11 21:49:37,907 iteration 5717 : loss : 0.026163, loss_ce: 0.012453
2021-12-11 21:49:39,227 iteration 5718 : loss : 0.028058, loss_ce: 0.011252
2021-12-11 21:49:40,633 iteration 5719 : loss : 0.024291, loss_ce: 0.012414
2021-12-11 21:49:41,977 iteration 5720 : loss : 0.026540, loss_ce: 0.013471
2021-12-11 21:49:43,260 iteration 5721 : loss : 0.026436, loss_ce: 0.010577
2021-12-11 21:49:44,619 iteration 5722 : loss : 0.030428, loss_ce: 0.013860
2021-12-11 21:49:45,982 iteration 5723 : loss : 0.029950, loss_ce: 0.013873
2021-12-11 21:49:47,275 iteration 5724 : loss : 0.019202, loss_ce: 0.010213
2021-12-11 21:49:48,673 iteration 5725 : loss : 0.036197, loss_ce: 0.020658
2021-12-11 21:49:50,008 iteration 5726 : loss : 0.030762, loss_ce: 0.015552
2021-12-11 21:49:51,367 iteration 5727 : loss : 0.031831, loss_ce: 0.013163
2021-12-11 21:49:52,737 iteration 5728 : loss : 0.031970, loss_ce: 0.016447
2021-12-11 21:49:54,142 iteration 5729 : loss : 0.026381, loss_ce: 0.012754
 84%|████████████████████████▍    | 337/400 [2:23:35<26:26, 25.18s/it]2021-12-11 21:49:55,583 iteration 5730 : loss : 0.039949, loss_ce: 0.015041
2021-12-11 21:49:57,008 iteration 5731 : loss : 0.033577, loss_ce: 0.018174
2021-12-11 21:49:58,374 iteration 5732 : loss : 0.034284, loss_ce: 0.014624
2021-12-11 21:49:59,724 iteration 5733 : loss : 0.024581, loss_ce: 0.009586
2021-12-11 21:50:01,109 iteration 5734 : loss : 0.047129, loss_ce: 0.015488
2021-12-11 21:50:02,507 iteration 5735 : loss : 0.034214, loss_ce: 0.018388
2021-12-11 21:50:03,869 iteration 5736 : loss : 0.039209, loss_ce: 0.022966
2021-12-11 21:50:05,238 iteration 5737 : loss : 0.039935, loss_ce: 0.018049
2021-12-11 21:50:06,595 iteration 5738 : loss : 0.022968, loss_ce: 0.013589
2021-12-11 21:50:07,921 iteration 5739 : loss : 0.024932, loss_ce: 0.013012
2021-12-11 21:50:09,330 iteration 5740 : loss : 0.028247, loss_ce: 0.012746
2021-12-11 21:50:10,723 iteration 5741 : loss : 0.040816, loss_ce: 0.021740
2021-12-11 21:50:12,116 iteration 5742 : loss : 0.046483, loss_ce: 0.021927
2021-12-11 21:50:13,460 iteration 5743 : loss : 0.031752, loss_ce: 0.014915
2021-12-11 21:50:14,882 iteration 5744 : loss : 0.025468, loss_ce: 0.012265
2021-12-11 21:50:16,261 iteration 5745 : loss : 0.042008, loss_ce: 0.016425
2021-12-11 21:50:17,562 iteration 5746 : loss : 0.022776, loss_ce: 0.009820
 84%|████████████████████████▌    | 338/400 [2:23:58<25:28, 24.65s/it]2021-12-11 21:50:19,030 iteration 5747 : loss : 0.033196, loss_ce: 0.013863
2021-12-11 21:50:20,447 iteration 5748 : loss : 0.036196, loss_ce: 0.014663
2021-12-11 21:50:21,881 iteration 5749 : loss : 0.029372, loss_ce: 0.014579
2021-12-11 21:50:23,337 iteration 5750 : loss : 0.037469, loss_ce: 0.015501
2021-12-11 21:50:24,654 iteration 5751 : loss : 0.034496, loss_ce: 0.015947
2021-12-11 21:50:26,057 iteration 5752 : loss : 0.037567, loss_ce: 0.015046
2021-12-11 21:50:27,421 iteration 5753 : loss : 0.031583, loss_ce: 0.014664
2021-12-11 21:50:28,733 iteration 5754 : loss : 0.024453, loss_ce: 0.011590
2021-12-11 21:50:30,103 iteration 5755 : loss : 0.034718, loss_ce: 0.019559
2021-12-11 21:50:31,477 iteration 5756 : loss : 0.023431, loss_ce: 0.009999
2021-12-11 21:50:32,833 iteration 5757 : loss : 0.027053, loss_ce: 0.015203
2021-12-11 21:50:34,210 iteration 5758 : loss : 0.040707, loss_ce: 0.012580
2021-12-11 21:50:35,604 iteration 5759 : loss : 0.036544, loss_ce: 0.018238
2021-12-11 21:50:37,067 iteration 5760 : loss : 0.036143, loss_ce: 0.016324
2021-12-11 21:50:38,366 iteration 5761 : loss : 0.022347, loss_ce: 0.011479
2021-12-11 21:50:39,917 iteration 5762 : loss : 0.034463, loss_ce: 0.015706
2021-12-11 21:50:41,287 iteration 5763 : loss : 0.028555, loss_ce: 0.015038
 85%|████████████████████████▌    | 339/400 [2:24:22<24:47, 24.38s/it]2021-12-11 21:50:42,744 iteration 5764 : loss : 0.037767, loss_ce: 0.017623
2021-12-11 21:50:44,158 iteration 5765 : loss : 0.042738, loss_ce: 0.014375
2021-12-11 21:50:45,563 iteration 5766 : loss : 0.025624, loss_ce: 0.013154
2021-12-11 21:50:46,963 iteration 5767 : loss : 0.028809, loss_ce: 0.013324
2021-12-11 21:50:48,456 iteration 5768 : loss : 0.053166, loss_ce: 0.020131
2021-12-11 21:50:49,832 iteration 5769 : loss : 0.026287, loss_ce: 0.012608
2021-12-11 21:50:51,217 iteration 5770 : loss : 0.028782, loss_ce: 0.014402
2021-12-11 21:50:52,597 iteration 5771 : loss : 0.037185, loss_ce: 0.019119
2021-12-11 21:50:53,986 iteration 5772 : loss : 0.032967, loss_ce: 0.014305
2021-12-11 21:50:55,452 iteration 5773 : loss : 0.031263, loss_ce: 0.016066
2021-12-11 21:50:56,905 iteration 5774 : loss : 0.029718, loss_ce: 0.014998
2021-12-11 21:50:58,371 iteration 5775 : loss : 0.035222, loss_ce: 0.014616
2021-12-11 21:50:59,719 iteration 5776 : loss : 0.026012, loss_ce: 0.010925
2021-12-11 21:51:01,064 iteration 5777 : loss : 0.027075, loss_ce: 0.012949
2021-12-11 21:51:02,483 iteration 5778 : loss : 0.033331, loss_ce: 0.013599
2021-12-11 21:51:03,857 iteration 5779 : loss : 0.031959, loss_ce: 0.015990
2021-12-11 21:51:03,858 Training Data Eval:
2021-12-11 21:51:10,647   Average segmentation loss on training set: 0.0143
2021-12-11 21:51:10,647 Validation Data Eval:
2021-12-11 21:51:13,002   Average segmentation loss on validation set: 0.0847
2021-12-11 21:51:14,417 iteration 5780 : loss : 0.030969, loss_ce: 0.013215
 85%|████████████████████████▋    | 340/400 [2:24:55<27:00, 27.00s/it]2021-12-11 21:51:15,971 iteration 5781 : loss : 0.059844, loss_ce: 0.024237
2021-12-11 21:51:17,260 iteration 5782 : loss : 0.021869, loss_ce: 0.012801
2021-12-11 21:51:18,583 iteration 5783 : loss : 0.038505, loss_ce: 0.018270
2021-12-11 21:51:19,925 iteration 5784 : loss : 0.029912, loss_ce: 0.012568
2021-12-11 21:51:21,233 iteration 5785 : loss : 0.027421, loss_ce: 0.013999
2021-12-11 21:51:22,625 iteration 5786 : loss : 0.051637, loss_ce: 0.014631
2021-12-11 21:51:23,979 iteration 5787 : loss : 0.028611, loss_ce: 0.013802
2021-12-11 21:51:25,254 iteration 5788 : loss : 0.028486, loss_ce: 0.014190
2021-12-11 21:51:26,561 iteration 5789 : loss : 0.018282, loss_ce: 0.009697
2021-12-11 21:51:27,949 iteration 5790 : loss : 0.021119, loss_ce: 0.011184
2021-12-11 21:51:29,314 iteration 5791 : loss : 0.021762, loss_ce: 0.012229
2021-12-11 21:51:30,639 iteration 5792 : loss : 0.033360, loss_ce: 0.017044
2021-12-11 21:51:32,056 iteration 5793 : loss : 0.036195, loss_ce: 0.014113
2021-12-11 21:51:33,506 iteration 5794 : loss : 0.033283, loss_ce: 0.016742
2021-12-11 21:51:34,839 iteration 5795 : loss : 0.020116, loss_ce: 0.010849
2021-12-11 21:51:36,226 iteration 5796 : loss : 0.031705, loss_ce: 0.016360
2021-12-11 21:51:37,611 iteration 5797 : loss : 0.031194, loss_ce: 0.013698
 85%|████████████████████████▋    | 341/400 [2:25:18<25:25, 25.86s/it]2021-12-11 21:51:39,136 iteration 5798 : loss : 0.034476, loss_ce: 0.012969
2021-12-11 21:51:40,513 iteration 5799 : loss : 0.037771, loss_ce: 0.015353
2021-12-11 21:51:41,874 iteration 5800 : loss : 0.029950, loss_ce: 0.011570
2021-12-11 21:51:43,153 iteration 5801 : loss : 0.022595, loss_ce: 0.010600
2021-12-11 21:51:44,548 iteration 5802 : loss : 0.030670, loss_ce: 0.017326
2021-12-11 21:51:45,861 iteration 5803 : loss : 0.021167, loss_ce: 0.010116
2021-12-11 21:51:47,329 iteration 5804 : loss : 0.067113, loss_ce: 0.026733
2021-12-11 21:51:48,671 iteration 5805 : loss : 0.034206, loss_ce: 0.018752
2021-12-11 21:51:49,994 iteration 5806 : loss : 0.035540, loss_ce: 0.018815
2021-12-11 21:51:51,378 iteration 5807 : loss : 0.027629, loss_ce: 0.013113
2021-12-11 21:51:52,731 iteration 5808 : loss : 0.034599, loss_ce: 0.016856
2021-12-11 21:51:54,202 iteration 5809 : loss : 0.043942, loss_ce: 0.016053
2021-12-11 21:51:55,511 iteration 5810 : loss : 0.031317, loss_ce: 0.011023
2021-12-11 21:51:56,865 iteration 5811 : loss : 0.024625, loss_ce: 0.012292
2021-12-11 21:51:58,267 iteration 5812 : loss : 0.032274, loss_ce: 0.014731
2021-12-11 21:51:59,606 iteration 5813 : loss : 0.025666, loss_ce: 0.010457
2021-12-11 21:52:00,981 iteration 5814 : loss : 0.035960, loss_ce: 0.016474
 86%|████████████████████████▊    | 342/400 [2:25:42<24:16, 25.11s/it]2021-12-11 21:52:02,522 iteration 5815 : loss : 0.042338, loss_ce: 0.018417
2021-12-11 21:52:03,828 iteration 5816 : loss : 0.021917, loss_ce: 0.009982
2021-12-11 21:52:05,224 iteration 5817 : loss : 0.029701, loss_ce: 0.011019
2021-12-11 21:52:06,585 iteration 5818 : loss : 0.029810, loss_ce: 0.011309
2021-12-11 21:52:08,061 iteration 5819 : loss : 0.029510, loss_ce: 0.014770
2021-12-11 21:52:09,438 iteration 5820 : loss : 0.030218, loss_ce: 0.016355
2021-12-11 21:52:10,801 iteration 5821 : loss : 0.036052, loss_ce: 0.015041
2021-12-11 21:52:12,244 iteration 5822 : loss : 0.032478, loss_ce: 0.011522
2021-12-11 21:52:13,641 iteration 5823 : loss : 0.024705, loss_ce: 0.010959
2021-12-11 21:52:15,022 iteration 5824 : loss : 0.035955, loss_ce: 0.016901
2021-12-11 21:52:16,397 iteration 5825 : loss : 0.034434, loss_ce: 0.016522
2021-12-11 21:52:17,733 iteration 5826 : loss : 0.038459, loss_ce: 0.013134
2021-12-11 21:52:19,059 iteration 5827 : loss : 0.045568, loss_ce: 0.019136
2021-12-11 21:52:20,349 iteration 5828 : loss : 0.017988, loss_ce: 0.010088
2021-12-11 21:52:21,720 iteration 5829 : loss : 0.029134, loss_ce: 0.014217
2021-12-11 21:52:23,109 iteration 5830 : loss : 0.033644, loss_ce: 0.016937
2021-12-11 21:52:24,536 iteration 5831 : loss : 0.029256, loss_ce: 0.014095
 86%|████████████████████████▊    | 343/400 [2:26:05<23:24, 24.64s/it]2021-12-11 21:52:26,055 iteration 5832 : loss : 0.031111, loss_ce: 0.014792
2021-12-11 21:52:27,413 iteration 5833 : loss : 0.021710, loss_ce: 0.010112
2021-12-11 21:52:28,867 iteration 5834 : loss : 0.043548, loss_ce: 0.020475
2021-12-11 21:52:30,394 iteration 5835 : loss : 0.043789, loss_ce: 0.018938
2021-12-11 21:52:31,757 iteration 5836 : loss : 0.030062, loss_ce: 0.015216
2021-12-11 21:52:33,215 iteration 5837 : loss : 0.040088, loss_ce: 0.020332
2021-12-11 21:52:34,676 iteration 5838 : loss : 0.041519, loss_ce: 0.014189
2021-12-11 21:52:35,997 iteration 5839 : loss : 0.029146, loss_ce: 0.012520
2021-12-11 21:52:37,558 iteration 5840 : loss : 0.035634, loss_ce: 0.018092
2021-12-11 21:52:38,938 iteration 5841 : loss : 0.017967, loss_ce: 0.009729
2021-12-11 21:52:40,385 iteration 5842 : loss : 0.030937, loss_ce: 0.015506
2021-12-11 21:52:41,846 iteration 5843 : loss : 0.030985, loss_ce: 0.013708
2021-12-11 21:52:43,260 iteration 5844 : loss : 0.043195, loss_ce: 0.014619
2021-12-11 21:52:44,684 iteration 5845 : loss : 0.041302, loss_ce: 0.017182
2021-12-11 21:52:46,076 iteration 5846 : loss : 0.030008, loss_ce: 0.014883
2021-12-11 21:52:47,445 iteration 5847 : loss : 0.024441, loss_ce: 0.010932
2021-12-11 21:52:48,750 iteration 5848 : loss : 0.027789, loss_ce: 0.011440
 86%|████████████████████████▉    | 344/400 [2:26:30<22:52, 24.51s/it]2021-12-11 21:52:50,127 iteration 5849 : loss : 0.028014, loss_ce: 0.013172
2021-12-11 21:52:51,568 iteration 5850 : loss : 0.041631, loss_ce: 0.014372
2021-12-11 21:52:52,912 iteration 5851 : loss : 0.023023, loss_ce: 0.011738
2021-12-11 21:52:54,264 iteration 5852 : loss : 0.032164, loss_ce: 0.013142
2021-12-11 21:52:55,576 iteration 5853 : loss : 0.041787, loss_ce: 0.011524
2021-12-11 21:52:56,946 iteration 5854 : loss : 0.036855, loss_ce: 0.017468
2021-12-11 21:52:58,304 iteration 5855 : loss : 0.031720, loss_ce: 0.011200
2021-12-11 21:52:59,625 iteration 5856 : loss : 0.026900, loss_ce: 0.013562
2021-12-11 21:53:01,027 iteration 5857 : loss : 0.049771, loss_ce: 0.024505
2021-12-11 21:53:02,375 iteration 5858 : loss : 0.038307, loss_ce: 0.014586
2021-12-11 21:53:03,776 iteration 5859 : loss : 0.036866, loss_ce: 0.021450
2021-12-11 21:53:05,183 iteration 5860 : loss : 0.023405, loss_ce: 0.011745
2021-12-11 21:53:06,579 iteration 5861 : loss : 0.033520, loss_ce: 0.016899
2021-12-11 21:53:07,943 iteration 5862 : loss : 0.024818, loss_ce: 0.011965
2021-12-11 21:53:09,306 iteration 5863 : loss : 0.030274, loss_ce: 0.013743
2021-12-11 21:53:10,661 iteration 5864 : loss : 0.034428, loss_ce: 0.016387
2021-12-11 21:53:10,661 Training Data Eval:
2021-12-11 21:53:17,459   Average segmentation loss on training set: 0.0142
2021-12-11 21:53:17,459 Validation Data Eval:
2021-12-11 21:53:19,816   Average segmentation loss on validation set: 0.0862
2021-12-11 21:53:21,157 iteration 5865 : loss : 0.029004, loss_ce: 0.013980
 86%|█████████████████████████    | 345/400 [2:27:02<24:38, 26.88s/it]2021-12-11 21:53:22,596 iteration 5866 : loss : 0.045655, loss_ce: 0.016488
2021-12-11 21:53:23,972 iteration 5867 : loss : 0.031086, loss_ce: 0.015289
2021-12-11 21:53:25,270 iteration 5868 : loss : 0.024250, loss_ce: 0.012701
2021-12-11 21:53:26,620 iteration 5869 : loss : 0.020969, loss_ce: 0.009975
2021-12-11 21:53:27,919 iteration 5870 : loss : 0.028924, loss_ce: 0.012871
2021-12-11 21:53:29,267 iteration 5871 : loss : 0.022904, loss_ce: 0.011466
2021-12-11 21:53:30,617 iteration 5872 : loss : 0.025127, loss_ce: 0.013936
2021-12-11 21:53:31,921 iteration 5873 : loss : 0.029435, loss_ce: 0.014994
2021-12-11 21:53:33,227 iteration 5874 : loss : 0.042083, loss_ce: 0.012660
2021-12-11 21:53:34,654 iteration 5875 : loss : 0.035924, loss_ce: 0.018230
2021-12-11 21:53:36,106 iteration 5876 : loss : 0.037214, loss_ce: 0.018725
2021-12-11 21:53:37,495 iteration 5877 : loss : 0.040947, loss_ce: 0.019227
2021-12-11 21:53:38,920 iteration 5878 : loss : 0.066062, loss_ce: 0.020416
2021-12-11 21:53:40,262 iteration 5879 : loss : 0.030652, loss_ce: 0.011922
2021-12-11 21:53:41,622 iteration 5880 : loss : 0.053150, loss_ce: 0.022154
2021-12-11 21:53:43,038 iteration 5881 : loss : 0.043920, loss_ce: 0.015479
2021-12-11 21:53:44,383 iteration 5882 : loss : 0.026514, loss_ce: 0.012891
 86%|█████████████████████████    | 346/400 [2:27:25<23:12, 25.79s/it]2021-12-11 21:53:45,911 iteration 5883 : loss : 0.045601, loss_ce: 0.017167
2021-12-11 21:53:47,249 iteration 5884 : loss : 0.027389, loss_ce: 0.013004
2021-12-11 21:53:48,676 iteration 5885 : loss : 0.037758, loss_ce: 0.012825
2021-12-11 21:53:50,021 iteration 5886 : loss : 0.017174, loss_ce: 0.008903
2021-12-11 21:53:51,370 iteration 5887 : loss : 0.040329, loss_ce: 0.014960
2021-12-11 21:53:52,654 iteration 5888 : loss : 0.022195, loss_ce: 0.011805
2021-12-11 21:53:54,006 iteration 5889 : loss : 0.036207, loss_ce: 0.019740
2021-12-11 21:53:55,424 iteration 5890 : loss : 0.037334, loss_ce: 0.018768
2021-12-11 21:53:56,842 iteration 5891 : loss : 0.036722, loss_ce: 0.014949
2021-12-11 21:53:58,298 iteration 5892 : loss : 0.028964, loss_ce: 0.016906
2021-12-11 21:53:59,607 iteration 5893 : loss : 0.019275, loss_ce: 0.008622
2021-12-11 21:54:00,937 iteration 5894 : loss : 0.028203, loss_ce: 0.010871
2021-12-11 21:54:02,338 iteration 5895 : loss : 0.043205, loss_ce: 0.019092
2021-12-11 21:54:03,631 iteration 5896 : loss : 0.024789, loss_ce: 0.011273
2021-12-11 21:54:05,017 iteration 5897 : loss : 0.031742, loss_ce: 0.016717
2021-12-11 21:54:06,441 iteration 5898 : loss : 0.036165, loss_ce: 0.017863
2021-12-11 21:54:07,798 iteration 5899 : loss : 0.028367, loss_ce: 0.012180
 87%|█████████████████████████▏   | 347/400 [2:27:49<22:09, 25.08s/it]2021-12-11 21:54:09,132 iteration 5900 : loss : 0.018394, loss_ce: 0.010218
2021-12-11 21:54:10,470 iteration 5901 : loss : 0.024673, loss_ce: 0.012709
2021-12-11 21:54:11,756 iteration 5902 : loss : 0.018954, loss_ce: 0.009893
2021-12-11 21:54:13,083 iteration 5903 : loss : 0.033926, loss_ce: 0.016817
2021-12-11 21:54:14,463 iteration 5904 : loss : 0.032021, loss_ce: 0.012232
2021-12-11 21:54:15,872 iteration 5905 : loss : 0.037182, loss_ce: 0.018122
2021-12-11 21:54:17,301 iteration 5906 : loss : 0.031046, loss_ce: 0.011094
2021-12-11 21:54:18,595 iteration 5907 : loss : 0.023793, loss_ce: 0.012575
2021-12-11 21:54:19,970 iteration 5908 : loss : 0.029720, loss_ce: 0.012636
2021-12-11 21:54:21,378 iteration 5909 : loss : 0.030447, loss_ce: 0.017735
2021-12-11 21:54:22,746 iteration 5910 : loss : 0.025016, loss_ce: 0.010738
2021-12-11 21:54:24,161 iteration 5911 : loss : 0.038834, loss_ce: 0.013886
2021-12-11 21:54:25,554 iteration 5912 : loss : 0.042096, loss_ce: 0.014334
2021-12-11 21:54:26,920 iteration 5913 : loss : 0.026505, loss_ce: 0.013435
2021-12-11 21:54:28,277 iteration 5914 : loss : 0.032902, loss_ce: 0.018400
2021-12-11 21:54:29,685 iteration 5915 : loss : 0.033207, loss_ce: 0.018662
2021-12-11 21:54:30,999 iteration 5916 : loss : 0.021421, loss_ce: 0.010649
 87%|█████████████████████████▏   | 348/400 [2:28:12<21:14, 24.51s/it]2021-12-11 21:54:32,375 iteration 5917 : loss : 0.024796, loss_ce: 0.013362
2021-12-11 21:54:33,822 iteration 5918 : loss : 0.039739, loss_ce: 0.021555
2021-12-11 21:54:35,265 iteration 5919 : loss : 0.026834, loss_ce: 0.012001
2021-12-11 21:54:36,718 iteration 5920 : loss : 0.038383, loss_ce: 0.014035
2021-12-11 21:54:38,086 iteration 5921 : loss : 0.029114, loss_ce: 0.012282
2021-12-11 21:54:39,506 iteration 5922 : loss : 0.030324, loss_ce: 0.016460
2021-12-11 21:54:40,899 iteration 5923 : loss : 0.032440, loss_ce: 0.016139
2021-12-11 21:54:42,187 iteration 5924 : loss : 0.019727, loss_ce: 0.009338
2021-12-11 21:54:43,541 iteration 5925 : loss : 0.029195, loss_ce: 0.011632
2021-12-11 21:54:44,978 iteration 5926 : loss : 0.043611, loss_ce: 0.015983
2021-12-11 21:54:46,318 iteration 5927 : loss : 0.018120, loss_ce: 0.009865
2021-12-11 21:54:47,752 iteration 5928 : loss : 0.028156, loss_ce: 0.014201
2021-12-11 21:54:49,156 iteration 5929 : loss : 0.035986, loss_ce: 0.019634
2021-12-11 21:54:50,613 iteration 5930 : loss : 0.027926, loss_ce: 0.013272
2021-12-11 21:54:51,947 iteration 5931 : loss : 0.027858, loss_ce: 0.013352
2021-12-11 21:54:53,339 iteration 5932 : loss : 0.031755, loss_ce: 0.014817
2021-12-11 21:54:54,754 iteration 5933 : loss : 0.039469, loss_ce: 0.015444
 87%|█████████████████████████▎   | 349/400 [2:28:36<20:38, 24.28s/it]2021-12-11 21:54:56,205 iteration 5934 : loss : 0.019241, loss_ce: 0.009173
2021-12-11 21:54:57,602 iteration 5935 : loss : 0.032341, loss_ce: 0.012259
2021-12-11 21:54:59,092 iteration 5936 : loss : 0.037528, loss_ce: 0.016025
2021-12-11 21:55:00,548 iteration 5937 : loss : 0.027029, loss_ce: 0.014170
2021-12-11 21:55:01,954 iteration 5938 : loss : 0.039224, loss_ce: 0.015391
2021-12-11 21:55:03,316 iteration 5939 : loss : 0.029717, loss_ce: 0.014283
2021-12-11 21:55:04,716 iteration 5940 : loss : 0.032997, loss_ce: 0.016480
2021-12-11 21:55:06,005 iteration 5941 : loss : 0.024724, loss_ce: 0.013551
2021-12-11 21:55:07,427 iteration 5942 : loss : 0.036446, loss_ce: 0.014850
2021-12-11 21:55:08,736 iteration 5943 : loss : 0.019250, loss_ce: 0.009657
2021-12-11 21:55:10,116 iteration 5944 : loss : 0.040910, loss_ce: 0.015228
2021-12-11 21:55:11,469 iteration 5945 : loss : 0.029013, loss_ce: 0.012839
2021-12-11 21:55:12,788 iteration 5946 : loss : 0.032475, loss_ce: 0.015166
2021-12-11 21:55:14,104 iteration 5947 : loss : 0.026587, loss_ce: 0.013073
2021-12-11 21:55:15,535 iteration 5948 : loss : 0.040228, loss_ce: 0.019955
2021-12-11 21:55:16,867 iteration 5949 : loss : 0.023151, loss_ce: 0.012191
2021-12-11 21:55:16,867 Training Data Eval:
2021-12-11 21:55:23,677   Average segmentation loss on training set: 0.0139
2021-12-11 21:55:23,677 Validation Data Eval:
2021-12-11 21:55:26,036   Average segmentation loss on validation set: 0.0849
2021-12-11 21:55:27,482 iteration 5950 : loss : 0.034473, loss_ce: 0.013243
2021-12-11 21:55:29,422 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed100epoch_349.pth
 88%|█████████████████████████▍   | 350/400 [2:29:10<22:49, 27.38s/it]2021-12-11 21:55:30,803 iteration 5951 : loss : 0.030545, loss_ce: 0.012171
2021-12-11 21:55:32,177 iteration 5952 : loss : 0.028322, loss_ce: 0.016667
2021-12-11 21:55:33,571 iteration 5953 : loss : 0.030325, loss_ce: 0.015444
2021-12-11 21:55:35,020 iteration 5954 : loss : 0.044037, loss_ce: 0.016070
2021-12-11 21:55:36,380 iteration 5955 : loss : 0.031859, loss_ce: 0.014414
2021-12-11 21:55:37,834 iteration 5956 : loss : 0.042068, loss_ce: 0.014436
2021-12-11 21:55:39,189 iteration 5957 : loss : 0.036818, loss_ce: 0.015143
2021-12-11 21:55:40,526 iteration 5958 : loss : 0.032518, loss_ce: 0.017566
2021-12-11 21:55:41,890 iteration 5959 : loss : 0.024489, loss_ce: 0.011012
2021-12-11 21:55:43,228 iteration 5960 : loss : 0.017202, loss_ce: 0.008389
2021-12-11 21:55:44,620 iteration 5961 : loss : 0.030917, loss_ce: 0.015161
2021-12-11 21:55:46,040 iteration 5962 : loss : 0.030129, loss_ce: 0.015145
2021-12-11 21:55:47,347 iteration 5963 : loss : 0.019824, loss_ce: 0.010664
2021-12-11 21:55:48,884 iteration 5964 : loss : 0.060550, loss_ce: 0.026652
2021-12-11 21:55:50,234 iteration 5965 : loss : 0.026029, loss_ce: 0.012219
2021-12-11 21:55:51,655 iteration 5966 : loss : 0.026068, loss_ce: 0.012495
2021-12-11 21:55:52,996 iteration 5967 : loss : 0.041100, loss_ce: 0.016076
 88%|█████████████████████████▍   | 351/400 [2:29:34<21:26, 26.25s/it]2021-12-11 21:55:54,408 iteration 5968 : loss : 0.026702, loss_ce: 0.014487
2021-12-11 21:55:55,824 iteration 5969 : loss : 0.034432, loss_ce: 0.011973
2021-12-11 21:55:57,218 iteration 5970 : loss : 0.028202, loss_ce: 0.012232
2021-12-11 21:55:58,520 iteration 5971 : loss : 0.026847, loss_ce: 0.012922
2021-12-11 21:55:59,851 iteration 5972 : loss : 0.023662, loss_ce: 0.011992
2021-12-11 21:56:01,238 iteration 5973 : loss : 0.018226, loss_ce: 0.010195
2021-12-11 21:56:02,568 iteration 5974 : loss : 0.029160, loss_ce: 0.013062
2021-12-11 21:56:03,976 iteration 5975 : loss : 0.042982, loss_ce: 0.017657
2021-12-11 21:56:05,374 iteration 5976 : loss : 0.039314, loss_ce: 0.013503
2021-12-11 21:56:06,767 iteration 5977 : loss : 0.024905, loss_ce: 0.011728
2021-12-11 21:56:08,148 iteration 5978 : loss : 0.024647, loss_ce: 0.012481
2021-12-11 21:56:09,486 iteration 5979 : loss : 0.027922, loss_ce: 0.012522
2021-12-11 21:56:10,907 iteration 5980 : loss : 0.020737, loss_ce: 0.009513
2021-12-11 21:56:12,315 iteration 5981 : loss : 0.030586, loss_ce: 0.014642
2021-12-11 21:56:13,753 iteration 5982 : loss : 0.036632, loss_ce: 0.015962
2021-12-11 21:56:15,068 iteration 5983 : loss : 0.036616, loss_ce: 0.018650
2021-12-11 21:56:16,396 iteration 5984 : loss : 0.034195, loss_ce: 0.013894
 88%|█████████████████████████▌   | 352/400 [2:29:57<20:19, 25.40s/it]2021-12-11 21:56:17,939 iteration 5985 : loss : 0.046424, loss_ce: 0.021904
2021-12-11 21:56:19,283 iteration 5986 : loss : 0.026991, loss_ce: 0.013847
2021-12-11 21:56:20,717 iteration 5987 : loss : 0.066041, loss_ce: 0.013752
2021-12-11 21:56:22,171 iteration 5988 : loss : 0.032658, loss_ce: 0.017477
2021-12-11 21:56:23,532 iteration 5989 : loss : 0.030328, loss_ce: 0.014459
2021-12-11 21:56:24,942 iteration 5990 : loss : 0.034316, loss_ce: 0.019800
2021-12-11 21:56:26,237 iteration 5991 : loss : 0.028119, loss_ce: 0.011664
2021-12-11 21:56:27,652 iteration 5992 : loss : 0.032491, loss_ce: 0.012268
2021-12-11 21:56:28,981 iteration 5993 : loss : 0.021214, loss_ce: 0.008735
2021-12-11 21:56:30,345 iteration 5994 : loss : 0.051043, loss_ce: 0.020070
2021-12-11 21:56:31,750 iteration 5995 : loss : 0.049334, loss_ce: 0.022293
2021-12-11 21:56:33,196 iteration 5996 : loss : 0.032014, loss_ce: 0.012829
2021-12-11 21:56:34,647 iteration 5997 : loss : 0.033272, loss_ce: 0.017005
2021-12-11 21:56:35,981 iteration 5998 : loss : 0.031844, loss_ce: 0.015281
2021-12-11 21:56:37,379 iteration 5999 : loss : 0.054527, loss_ce: 0.018444
2021-12-11 21:56:38,778 iteration 6000 : loss : 0.038010, loss_ce: 0.018461
2021-12-11 21:56:40,220 iteration 6001 : loss : 0.059618, loss_ce: 0.021551
 88%|█████████████████████████▌   | 353/400 [2:30:21<19:31, 24.93s/it]2021-12-11 21:56:41,666 iteration 6002 : loss : 0.026027, loss_ce: 0.014012
2021-12-11 21:56:43,092 iteration 6003 : loss : 0.042906, loss_ce: 0.017207
2021-12-11 21:56:44,467 iteration 6004 : loss : 0.024429, loss_ce: 0.013838
2021-12-11 21:56:45,883 iteration 6005 : loss : 0.026711, loss_ce: 0.013006
2021-12-11 21:56:47,214 iteration 6006 : loss : 0.032776, loss_ce: 0.019283
2021-12-11 21:56:48,587 iteration 6007 : loss : 0.026902, loss_ce: 0.012050
2021-12-11 21:56:49,958 iteration 6008 : loss : 0.028870, loss_ce: 0.013573
2021-12-11 21:56:51,415 iteration 6009 : loss : 0.049239, loss_ce: 0.018671
2021-12-11 21:56:52,711 iteration 6010 : loss : 0.026833, loss_ce: 0.013598
2021-12-11 21:56:54,113 iteration 6011 : loss : 0.043099, loss_ce: 0.017059
2021-12-11 21:56:55,481 iteration 6012 : loss : 0.034502, loss_ce: 0.012823
2021-12-11 21:56:56,950 iteration 6013 : loss : 0.038331, loss_ce: 0.015509
2021-12-11 21:56:58,324 iteration 6014 : loss : 0.029276, loss_ce: 0.016096
2021-12-11 21:56:59,673 iteration 6015 : loss : 0.031625, loss_ce: 0.013454
2021-12-11 21:57:01,053 iteration 6016 : loss : 0.025171, loss_ce: 0.010645
2021-12-11 21:57:02,370 iteration 6017 : loss : 0.056918, loss_ce: 0.017658
2021-12-11 21:57:03,840 iteration 6018 : loss : 0.025575, loss_ce: 0.011723
 88%|█████████████████████████▋   | 354/400 [2:30:45<18:48, 24.54s/it]2021-12-11 21:57:05,325 iteration 6019 : loss : 0.038000, loss_ce: 0.016531
2021-12-11 21:57:06,702 iteration 6020 : loss : 0.031476, loss_ce: 0.015369
2021-12-11 21:57:08,063 iteration 6021 : loss : 0.023712, loss_ce: 0.010679
2021-12-11 21:57:09,455 iteration 6022 : loss : 0.021598, loss_ce: 0.010987
2021-12-11 21:57:10,740 iteration 6023 : loss : 0.024721, loss_ce: 0.011438
2021-12-11 21:57:12,170 iteration 6024 : loss : 0.039735, loss_ce: 0.019439
2021-12-11 21:57:13,591 iteration 6025 : loss : 0.038855, loss_ce: 0.019204
2021-12-11 21:57:15,019 iteration 6026 : loss : 0.036152, loss_ce: 0.014409
2021-12-11 21:57:16,362 iteration 6027 : loss : 0.036855, loss_ce: 0.018348
2021-12-11 21:57:17,704 iteration 6028 : loss : 0.038771, loss_ce: 0.017469
2021-12-11 21:57:19,129 iteration 6029 : loss : 0.038702, loss_ce: 0.018187
2021-12-11 21:57:20,457 iteration 6030 : loss : 0.024977, loss_ce: 0.011293
2021-12-11 21:57:21,932 iteration 6031 : loss : 0.034776, loss_ce: 0.012382
2021-12-11 21:57:23,359 iteration 6032 : loss : 0.037282, loss_ce: 0.015597
2021-12-11 21:57:24,772 iteration 6033 : loss : 0.030749, loss_ce: 0.016033
2021-12-11 21:57:26,223 iteration 6034 : loss : 0.028167, loss_ce: 0.011984
2021-12-11 21:57:26,223 Training Data Eval:
2021-12-11 21:57:33,048   Average segmentation loss on training set: 0.0141
2021-12-11 21:57:33,048 Validation Data Eval:
2021-12-11 21:57:35,414   Average segmentation loss on validation set: 0.0786
2021-12-11 21:57:36,729 iteration 6035 : loss : 0.031146, loss_ce: 0.011274
 89%|█████████████████████████▋   | 355/400 [2:31:18<20:16, 27.04s/it]2021-12-11 21:57:38,195 iteration 6036 : loss : 0.034011, loss_ce: 0.014199
2021-12-11 21:57:39,640 iteration 6037 : loss : 0.036855, loss_ce: 0.015998
2021-12-11 21:57:40,985 iteration 6038 : loss : 0.033823, loss_ce: 0.017122
2021-12-11 21:57:42,374 iteration 6039 : loss : 0.045049, loss_ce: 0.022484
2021-12-11 21:57:43,745 iteration 6040 : loss : 0.026704, loss_ce: 0.013476
2021-12-11 21:57:45,236 iteration 6041 : loss : 0.054273, loss_ce: 0.023605
2021-12-11 21:57:46,673 iteration 6042 : loss : 0.028833, loss_ce: 0.013616
2021-12-11 21:57:48,056 iteration 6043 : loss : 0.024919, loss_ce: 0.013022
2021-12-11 21:57:49,424 iteration 6044 : loss : 0.029454, loss_ce: 0.012667
2021-12-11 21:57:50,852 iteration 6045 : loss : 0.045180, loss_ce: 0.018770
2021-12-11 21:57:52,236 iteration 6046 : loss : 0.038029, loss_ce: 0.014693
2021-12-11 21:57:53,575 iteration 6047 : loss : 0.038981, loss_ce: 0.014524
2021-12-11 21:57:55,053 iteration 6048 : loss : 0.044392, loss_ce: 0.012773
2021-12-11 21:57:56,482 iteration 6049 : loss : 0.025564, loss_ce: 0.013773
2021-12-11 21:57:57,836 iteration 6050 : loss : 0.032522, loss_ce: 0.016021
2021-12-11 21:57:59,278 iteration 6051 : loss : 0.033216, loss_ce: 0.015682
2021-12-11 21:58:00,572 iteration 6052 : loss : 0.029170, loss_ce: 0.011063
 89%|█████████████████████████▊   | 356/400 [2:31:41<19:07, 26.08s/it]2021-12-11 21:58:01,968 iteration 6053 : loss : 0.028720, loss_ce: 0.015948
2021-12-11 21:58:03,432 iteration 6054 : loss : 0.055153, loss_ce: 0.023400
2021-12-11 21:58:04,793 iteration 6055 : loss : 0.031251, loss_ce: 0.014127
2021-12-11 21:58:06,136 iteration 6056 : loss : 0.026136, loss_ce: 0.013510
2021-12-11 21:58:07,478 iteration 6057 : loss : 0.028527, loss_ce: 0.011563
2021-12-11 21:58:08,902 iteration 6058 : loss : 0.032407, loss_ce: 0.017151
2021-12-11 21:58:10,219 iteration 6059 : loss : 0.020059, loss_ce: 0.010171
2021-12-11 21:58:11,657 iteration 6060 : loss : 0.024109, loss_ce: 0.012874
2021-12-11 21:58:12,979 iteration 6061 : loss : 0.032319, loss_ce: 0.016122
2021-12-11 21:58:14,434 iteration 6062 : loss : 0.040045, loss_ce: 0.020282
2021-12-11 21:58:15,882 iteration 6063 : loss : 0.044879, loss_ce: 0.017307
2021-12-11 21:58:17,267 iteration 6064 : loss : 0.047319, loss_ce: 0.018187
2021-12-11 21:58:18,717 iteration 6065 : loss : 0.045648, loss_ce: 0.023842
2021-12-11 21:58:20,083 iteration 6066 : loss : 0.022143, loss_ce: 0.010638
2021-12-11 21:58:21,411 iteration 6067 : loss : 0.077245, loss_ce: 0.015177
2021-12-11 21:58:22,884 iteration 6068 : loss : 0.046166, loss_ce: 0.017855
2021-12-11 21:58:24,245 iteration 6069 : loss : 0.060791, loss_ce: 0.013935
 89%|█████████████████████████▉   | 357/400 [2:32:05<18:10, 25.36s/it]2021-12-11 21:58:25,730 iteration 6070 : loss : 0.039636, loss_ce: 0.023390
2021-12-11 21:58:27,159 iteration 6071 : loss : 0.030265, loss_ce: 0.012346
2021-12-11 21:58:28,553 iteration 6072 : loss : 0.026156, loss_ce: 0.012628
2021-12-11 21:58:29,961 iteration 6073 : loss : 0.033015, loss_ce: 0.015345
2021-12-11 21:58:31,354 iteration 6074 : loss : 0.030822, loss_ce: 0.012303
2021-12-11 21:58:32,806 iteration 6075 : loss : 0.027324, loss_ce: 0.013555
2021-12-11 21:58:34,161 iteration 6076 : loss : 0.030989, loss_ce: 0.016528
2021-12-11 21:58:35,510 iteration 6077 : loss : 0.030655, loss_ce: 0.016408
2021-12-11 21:58:36,873 iteration 6078 : loss : 0.036364, loss_ce: 0.014127
2021-12-11 21:58:38,261 iteration 6079 : loss : 0.032056, loss_ce: 0.011233
2021-12-11 21:58:39,642 iteration 6080 : loss : 0.027451, loss_ce: 0.011654
2021-12-11 21:58:40,937 iteration 6081 : loss : 0.026192, loss_ce: 0.011191
2021-12-11 21:58:42,375 iteration 6082 : loss : 0.046931, loss_ce: 0.021719
2021-12-11 21:58:43,825 iteration 6083 : loss : 0.053187, loss_ce: 0.022636
2021-12-11 21:58:45,156 iteration 6084 : loss : 0.048798, loss_ce: 0.016553
2021-12-11 21:58:46,478 iteration 6085 : loss : 0.030344, loss_ce: 0.016724
2021-12-11 21:58:47,971 iteration 6086 : loss : 0.049466, loss_ce: 0.023017
 90%|█████████████████████████▉   | 358/400 [2:32:29<17:24, 24.87s/it]2021-12-11 21:58:49,416 iteration 6087 : loss : 0.035365, loss_ce: 0.015096
2021-12-11 21:58:50,801 iteration 6088 : loss : 0.023124, loss_ce: 0.010045
2021-12-11 21:58:52,232 iteration 6089 : loss : 0.028790, loss_ce: 0.014873
2021-12-11 21:58:53,591 iteration 6090 : loss : 0.027331, loss_ce: 0.011272
2021-12-11 21:58:54,927 iteration 6091 : loss : 0.019585, loss_ce: 0.008561
2021-12-11 21:58:56,375 iteration 6092 : loss : 0.038908, loss_ce: 0.017231
2021-12-11 21:58:57,857 iteration 6093 : loss : 0.051749, loss_ce: 0.022729
2021-12-11 21:58:59,297 iteration 6094 : loss : 0.042148, loss_ce: 0.022236
2021-12-11 21:59:00,617 iteration 6095 : loss : 0.033166, loss_ce: 0.012142
2021-12-11 21:59:02,078 iteration 6096 : loss : 0.035482, loss_ce: 0.010963
2021-12-11 21:59:03,415 iteration 6097 : loss : 0.026055, loss_ce: 0.015630
2021-12-11 21:59:04,827 iteration 6098 : loss : 0.029374, loss_ce: 0.017584
2021-12-11 21:59:06,202 iteration 6099 : loss : 0.038925, loss_ce: 0.022127
2021-12-11 21:59:07,611 iteration 6100 : loss : 0.051233, loss_ce: 0.017885
2021-12-11 21:59:09,066 iteration 6101 : loss : 0.043318, loss_ce: 0.021461
2021-12-11 21:59:10,396 iteration 6102 : loss : 0.026569, loss_ce: 0.013494
2021-12-11 21:59:11,801 iteration 6103 : loss : 0.033374, loss_ce: 0.011754
 90%|██████████████████████████   | 359/400 [2:32:53<16:46, 24.56s/it]2021-12-11 21:59:13,286 iteration 6104 : loss : 0.030603, loss_ce: 0.014910
2021-12-11 21:59:14,643 iteration 6105 : loss : 0.034489, loss_ce: 0.013207
2021-12-11 21:59:15,969 iteration 6106 : loss : 0.019849, loss_ce: 0.011052
2021-12-11 21:59:17,293 iteration 6107 : loss : 0.020333, loss_ce: 0.010575
2021-12-11 21:59:18,664 iteration 6108 : loss : 0.030724, loss_ce: 0.013155
2021-12-11 21:59:20,080 iteration 6109 : loss : 0.026317, loss_ce: 0.013707
2021-12-11 21:59:21,392 iteration 6110 : loss : 0.032219, loss_ce: 0.018985
2021-12-11 21:59:22,767 iteration 6111 : loss : 0.054914, loss_ce: 0.020405
2021-12-11 21:59:24,185 iteration 6112 : loss : 0.033421, loss_ce: 0.012358
2021-12-11 21:59:25,551 iteration 6113 : loss : 0.032769, loss_ce: 0.014019
2021-12-11 21:59:26,934 iteration 6114 : loss : 0.047697, loss_ce: 0.021553
2021-12-11 21:59:28,370 iteration 6115 : loss : 0.032799, loss_ce: 0.011803
2021-12-11 21:59:29,742 iteration 6116 : loss : 0.036376, loss_ce: 0.015259
2021-12-11 21:59:31,083 iteration 6117 : loss : 0.024302, loss_ce: 0.010689
2021-12-11 21:59:32,475 iteration 6118 : loss : 0.028976, loss_ce: 0.017079
2021-12-11 21:59:33,842 iteration 6119 : loss : 0.033023, loss_ce: 0.015558
2021-12-11 21:59:33,842 Training Data Eval:
2021-12-11 21:59:40,642   Average segmentation loss on training set: 0.0141
2021-12-11 21:59:40,643 Validation Data Eval:
2021-12-11 21:59:43,000   Average segmentation loss on validation set: 0.0847
2021-12-11 21:59:44,422 iteration 6120 : loss : 0.039148, loss_ce: 0.017241
 90%|██████████████████████████   | 360/400 [2:33:25<17:59, 26.98s/it]2021-12-11 21:59:45,880 iteration 6121 : loss : 0.034991, loss_ce: 0.013653
2021-12-11 21:59:47,267 iteration 6122 : loss : 0.028039, loss_ce: 0.014265
2021-12-11 21:59:48,626 iteration 6123 : loss : 0.023980, loss_ce: 0.011047
2021-12-11 21:59:50,076 iteration 6124 : loss : 0.036816, loss_ce: 0.013840
2021-12-11 21:59:51,461 iteration 6125 : loss : 0.029586, loss_ce: 0.012724
2021-12-11 21:59:52,945 iteration 6126 : loss : 0.033029, loss_ce: 0.015280
2021-12-11 21:59:54,431 iteration 6127 : loss : 0.039138, loss_ce: 0.022294
2021-12-11 21:59:55,805 iteration 6128 : loss : 0.022371, loss_ce: 0.010845
2021-12-11 21:59:57,135 iteration 6129 : loss : 0.033844, loss_ce: 0.014816
2021-12-11 21:59:58,543 iteration 6130 : loss : 0.042452, loss_ce: 0.020843
2021-12-11 21:59:59,879 iteration 6131 : loss : 0.032407, loss_ce: 0.013084
2021-12-11 22:00:01,312 iteration 6132 : loss : 0.034057, loss_ce: 0.014675
2021-12-11 22:00:02,739 iteration 6133 : loss : 0.029309, loss_ce: 0.015897
2021-12-11 22:00:04,091 iteration 6134 : loss : 0.027157, loss_ce: 0.013719
2021-12-11 22:00:05,426 iteration 6135 : loss : 0.028873, loss_ce: 0.015145
2021-12-11 22:00:06,762 iteration 6136 : loss : 0.026789, loss_ce: 0.011365
2021-12-11 22:00:08,105 iteration 6137 : loss : 0.036194, loss_ce: 0.014867
 90%|██████████████████████████▏  | 361/400 [2:33:49<16:53, 25.99s/it]2021-12-11 22:00:09,489 iteration 6138 : loss : 0.037213, loss_ce: 0.015342
2021-12-11 22:00:10,832 iteration 6139 : loss : 0.054916, loss_ce: 0.018210
2021-12-11 22:00:12,169 iteration 6140 : loss : 0.030383, loss_ce: 0.013940
2021-12-11 22:00:13,476 iteration 6141 : loss : 0.037356, loss_ce: 0.017963
2021-12-11 22:00:14,876 iteration 6142 : loss : 0.024264, loss_ce: 0.012238
2021-12-11 22:00:16,204 iteration 6143 : loss : 0.031718, loss_ce: 0.016712
2021-12-11 22:00:17,658 iteration 6144 : loss : 0.034554, loss_ce: 0.017618
2021-12-11 22:00:18,982 iteration 6145 : loss : 0.028405, loss_ce: 0.013946
2021-12-11 22:00:20,281 iteration 6146 : loss : 0.032275, loss_ce: 0.012593
2021-12-11 22:00:21,641 iteration 6147 : loss : 0.033481, loss_ce: 0.014943
2021-12-11 22:00:22,993 iteration 6148 : loss : 0.028819, loss_ce: 0.013836
2021-12-11 22:00:24,337 iteration 6149 : loss : 0.021663, loss_ce: 0.010451
2021-12-11 22:00:25,743 iteration 6150 : loss : 0.029924, loss_ce: 0.012914
2021-12-11 22:00:27,256 iteration 6151 : loss : 0.046125, loss_ce: 0.014965
2021-12-11 22:00:28,632 iteration 6152 : loss : 0.025953, loss_ce: 0.012806
2021-12-11 22:00:30,033 iteration 6153 : loss : 0.027271, loss_ce: 0.012337
2021-12-11 22:00:31,479 iteration 6154 : loss : 0.044696, loss_ce: 0.019123
 90%|██████████████████████████▏  | 362/400 [2:34:12<15:57, 25.20s/it]2021-12-11 22:00:32,984 iteration 6155 : loss : 0.038603, loss_ce: 0.016963
2021-12-11 22:00:34,324 iteration 6156 : loss : 0.027678, loss_ce: 0.013909
2021-12-11 22:00:35,690 iteration 6157 : loss : 0.022213, loss_ce: 0.009587
2021-12-11 22:00:37,087 iteration 6158 : loss : 0.025153, loss_ce: 0.011906
2021-12-11 22:00:38,425 iteration 6159 : loss : 0.031788, loss_ce: 0.013861
2021-12-11 22:00:39,791 iteration 6160 : loss : 0.025248, loss_ce: 0.011961
2021-12-11 22:00:41,177 iteration 6161 : loss : 0.037260, loss_ce: 0.013645
2021-12-11 22:00:42,600 iteration 6162 : loss : 0.035801, loss_ce: 0.010081
2021-12-11 22:00:43,985 iteration 6163 : loss : 0.032301, loss_ce: 0.015930
2021-12-11 22:00:45,427 iteration 6164 : loss : 0.040473, loss_ce: 0.020348
2021-12-11 22:00:46,794 iteration 6165 : loss : 0.031810, loss_ce: 0.013433
2021-12-11 22:00:48,165 iteration 6166 : loss : 0.019761, loss_ce: 0.010785
2021-12-11 22:00:49,510 iteration 6167 : loss : 0.034152, loss_ce: 0.015700
2021-12-11 22:00:50,936 iteration 6168 : loss : 0.033519, loss_ce: 0.014994
2021-12-11 22:00:52,339 iteration 6169 : loss : 0.042017, loss_ce: 0.014958
2021-12-11 22:00:53,693 iteration 6170 : loss : 0.044675, loss_ce: 0.019509
2021-12-11 22:00:55,029 iteration 6171 : loss : 0.026259, loss_ce: 0.013717
 91%|██████████████████████████▎  | 363/400 [2:34:36<15:14, 24.71s/it]2021-12-11 22:00:56,436 iteration 6172 : loss : 0.031092, loss_ce: 0.015060
2021-12-11 22:00:57,891 iteration 6173 : loss : 0.028440, loss_ce: 0.012622
2021-12-11 22:00:59,262 iteration 6174 : loss : 0.034612, loss_ce: 0.018977
2021-12-11 22:01:00,683 iteration 6175 : loss : 0.038743, loss_ce: 0.017766
2021-12-11 22:01:02,073 iteration 6176 : loss : 0.028416, loss_ce: 0.012816
2021-12-11 22:01:03,538 iteration 6177 : loss : 0.042302, loss_ce: 0.020437
2021-12-11 22:01:04,924 iteration 6178 : loss : 0.032610, loss_ce: 0.015252
2021-12-11 22:01:06,303 iteration 6179 : loss : 0.023080, loss_ce: 0.012329
2021-12-11 22:01:07,683 iteration 6180 : loss : 0.029650, loss_ce: 0.013952
2021-12-11 22:01:08,973 iteration 6181 : loss : 0.020900, loss_ce: 0.009346
2021-12-11 22:01:10,355 iteration 6182 : loss : 0.032723, loss_ce: 0.012015
2021-12-11 22:01:11,810 iteration 6183 : loss : 0.031412, loss_ce: 0.014215
2021-12-11 22:01:13,217 iteration 6184 : loss : 0.042550, loss_ce: 0.016310
2021-12-11 22:01:14,577 iteration 6185 : loss : 0.032883, loss_ce: 0.013493
2021-12-11 22:01:15,963 iteration 6186 : loss : 0.029359, loss_ce: 0.013314
2021-12-11 22:01:17,399 iteration 6187 : loss : 0.026557, loss_ce: 0.013051
2021-12-11 22:01:18,704 iteration 6188 : loss : 0.023625, loss_ce: 0.010663
 91%|██████████████████████████▍  | 364/400 [2:35:00<14:38, 24.40s/it]2021-12-11 22:01:20,062 iteration 6189 : loss : 0.019503, loss_ce: 0.009174
2021-12-11 22:01:21,347 iteration 6190 : loss : 0.030104, loss_ce: 0.012984
2021-12-11 22:01:22,852 iteration 6191 : loss : 0.037163, loss_ce: 0.015946
2021-12-11 22:01:24,277 iteration 6192 : loss : 0.023921, loss_ce: 0.011452
2021-12-11 22:01:25,545 iteration 6193 : loss : 0.016554, loss_ce: 0.008686
2021-12-11 22:01:26,916 iteration 6194 : loss : 0.028729, loss_ce: 0.012705
2021-12-11 22:01:28,326 iteration 6195 : loss : 0.024750, loss_ce: 0.012145
2021-12-11 22:01:29,741 iteration 6196 : loss : 0.026052, loss_ce: 0.013130
2021-12-11 22:01:31,139 iteration 6197 : loss : 0.028843, loss_ce: 0.017363
2021-12-11 22:01:32,526 iteration 6198 : loss : 0.025893, loss_ce: 0.012091
2021-12-11 22:01:33,918 iteration 6199 : loss : 0.032991, loss_ce: 0.012639
2021-12-11 22:01:35,223 iteration 6200 : loss : 0.017243, loss_ce: 0.010121
2021-12-11 22:01:36,603 iteration 6201 : loss : 0.040735, loss_ce: 0.017768
2021-12-11 22:01:37,935 iteration 6202 : loss : 0.025080, loss_ce: 0.012645
2021-12-11 22:01:39,295 iteration 6203 : loss : 0.030654, loss_ce: 0.010999
2021-12-11 22:01:40,707 iteration 6204 : loss : 0.022790, loss_ce: 0.011891
2021-12-11 22:01:40,707 Training Data Eval:
2021-12-11 22:01:47,522   Average segmentation loss on training set: 0.0142
2021-12-11 22:01:47,523 Validation Data Eval:
2021-12-11 22:01:49,880   Average segmentation loss on validation set: 0.0913
2021-12-11 22:01:51,270 iteration 6205 : loss : 0.038217, loss_ce: 0.015629
 91%|██████████████████████████▍  | 365/400 [2:35:32<15:39, 26.85s/it]2021-12-11 22:01:52,727 iteration 6206 : loss : 0.032776, loss_ce: 0.018137
2021-12-11 22:01:54,075 iteration 6207 : loss : 0.041953, loss_ce: 0.016304
2021-12-11 22:01:55,524 iteration 6208 : loss : 0.045608, loss_ce: 0.016497
2021-12-11 22:01:56,877 iteration 6209 : loss : 0.035111, loss_ce: 0.011711
2021-12-11 22:01:58,201 iteration 6210 : loss : 0.026347, loss_ce: 0.012377
2021-12-11 22:01:59,613 iteration 6211 : loss : 0.035369, loss_ce: 0.017720
2021-12-11 22:02:00,960 iteration 6212 : loss : 0.026479, loss_ce: 0.011820
2021-12-11 22:02:02,307 iteration 6213 : loss : 0.047858, loss_ce: 0.012763
2021-12-11 22:02:03,653 iteration 6214 : loss : 0.034218, loss_ce: 0.011764
2021-12-11 22:02:05,051 iteration 6215 : loss : 0.032109, loss_ce: 0.017660
2021-12-11 22:02:06,439 iteration 6216 : loss : 0.044116, loss_ce: 0.015742
2021-12-11 22:02:07,737 iteration 6217 : loss : 0.023093, loss_ce: 0.009750
2021-12-11 22:02:09,124 iteration 6218 : loss : 0.019401, loss_ce: 0.010505
2021-12-11 22:02:10,466 iteration 6219 : loss : 0.025886, loss_ce: 0.011742
2021-12-11 22:02:11,830 iteration 6220 : loss : 0.027279, loss_ce: 0.011469
2021-12-11 22:02:13,291 iteration 6221 : loss : 0.029080, loss_ce: 0.015217
2021-12-11 22:02:14,677 iteration 6222 : loss : 0.030166, loss_ce: 0.016386
 92%|██████████████████████████▌  | 366/400 [2:35:55<14:37, 25.82s/it]2021-12-11 22:02:16,140 iteration 6223 : loss : 0.040579, loss_ce: 0.013527
2021-12-11 22:02:17,488 iteration 6224 : loss : 0.040762, loss_ce: 0.016102
2021-12-11 22:02:18,859 iteration 6225 : loss : 0.030472, loss_ce: 0.015161
2021-12-11 22:02:20,315 iteration 6226 : loss : 0.033224, loss_ce: 0.015359
2021-12-11 22:02:21,686 iteration 6227 : loss : 0.026503, loss_ce: 0.009483
2021-12-11 22:02:23,075 iteration 6228 : loss : 0.032571, loss_ce: 0.015459
2021-12-11 22:02:24,391 iteration 6229 : loss : 0.023398, loss_ce: 0.011699
2021-12-11 22:02:25,730 iteration 6230 : loss : 0.034004, loss_ce: 0.015559
2021-12-11 22:02:27,112 iteration 6231 : loss : 0.028155, loss_ce: 0.013674
2021-12-11 22:02:28,494 iteration 6232 : loss : 0.032553, loss_ce: 0.015830
2021-12-11 22:02:29,850 iteration 6233 : loss : 0.031054, loss_ce: 0.016109
2021-12-11 22:02:31,249 iteration 6234 : loss : 0.030052, loss_ce: 0.014405
2021-12-11 22:02:32,716 iteration 6235 : loss : 0.044427, loss_ce: 0.014049
2021-12-11 22:02:34,019 iteration 6236 : loss : 0.022065, loss_ce: 0.010653
2021-12-11 22:02:35,375 iteration 6237 : loss : 0.026867, loss_ce: 0.012836
2021-12-11 22:02:36,695 iteration 6238 : loss : 0.019898, loss_ce: 0.009257
2021-12-11 22:02:38,042 iteration 6239 : loss : 0.020490, loss_ce: 0.011187
 92%|██████████████████████████▌  | 367/400 [2:36:19<13:47, 25.08s/it]2021-12-11 22:02:39,522 iteration 6240 : loss : 0.029775, loss_ce: 0.013178
2021-12-11 22:02:40,947 iteration 6241 : loss : 0.039650, loss_ce: 0.019833
2021-12-11 22:02:42,368 iteration 6242 : loss : 0.035775, loss_ce: 0.016832
2021-12-11 22:02:43,683 iteration 6243 : loss : 0.025763, loss_ce: 0.012630
2021-12-11 22:02:45,019 iteration 6244 : loss : 0.032079, loss_ce: 0.012205
2021-12-11 22:02:46,371 iteration 6245 : loss : 0.030412, loss_ce: 0.013157
2021-12-11 22:02:47,713 iteration 6246 : loss : 0.034046, loss_ce: 0.015235
2021-12-11 22:02:49,048 iteration 6247 : loss : 0.041013, loss_ce: 0.012394
2021-12-11 22:02:50,462 iteration 6248 : loss : 0.029329, loss_ce: 0.014498
2021-12-11 22:02:51,815 iteration 6249 : loss : 0.028007, loss_ce: 0.012297
2021-12-11 22:02:53,220 iteration 6250 : loss : 0.033356, loss_ce: 0.017144
2021-12-11 22:02:54,682 iteration 6251 : loss : 0.027609, loss_ce: 0.012320
2021-12-11 22:02:56,095 iteration 6252 : loss : 0.056320, loss_ce: 0.015280
2021-12-11 22:02:57,436 iteration 6253 : loss : 0.028190, loss_ce: 0.011157
2021-12-11 22:02:58,746 iteration 6254 : loss : 0.021756, loss_ce: 0.011044
2021-12-11 22:03:00,169 iteration 6255 : loss : 0.030260, loss_ce: 0.012509
2021-12-11 22:03:01,544 iteration 6256 : loss : 0.026634, loss_ce: 0.013392
 92%|██████████████████████████▋  | 368/400 [2:36:42<13:07, 24.61s/it]2021-12-11 22:03:03,000 iteration 6257 : loss : 0.029045, loss_ce: 0.017158
2021-12-11 22:03:04,328 iteration 6258 : loss : 0.031425, loss_ce: 0.013901
2021-12-11 22:03:05,757 iteration 6259 : loss : 0.030305, loss_ce: 0.013240
2021-12-11 22:03:07,257 iteration 6260 : loss : 0.063314, loss_ce: 0.020446
2021-12-11 22:03:08,647 iteration 6261 : loss : 0.034130, loss_ce: 0.017276
2021-12-11 22:03:10,079 iteration 6262 : loss : 0.029674, loss_ce: 0.013323
2021-12-11 22:03:11,460 iteration 6263 : loss : 0.037864, loss_ce: 0.017668
2021-12-11 22:03:12,782 iteration 6264 : loss : 0.027305, loss_ce: 0.013181
2021-12-11 22:03:14,159 iteration 6265 : loss : 0.024899, loss_ce: 0.011824
2021-12-11 22:03:15,510 iteration 6266 : loss : 0.022770, loss_ce: 0.011771
2021-12-11 22:03:17,049 iteration 6267 : loss : 0.054584, loss_ce: 0.025505
2021-12-11 22:03:18,425 iteration 6268 : loss : 0.027164, loss_ce: 0.012046
2021-12-11 22:03:19,813 iteration 6269 : loss : 0.026424, loss_ce: 0.013095
2021-12-11 22:03:21,171 iteration 6270 : loss : 0.027787, loss_ce: 0.014154
2021-12-11 22:03:22,627 iteration 6271 : loss : 0.026577, loss_ce: 0.013439
2021-12-11 22:03:23,963 iteration 6272 : loss : 0.033920, loss_ce: 0.012799
2021-12-11 22:03:25,425 iteration 6273 : loss : 0.042714, loss_ce: 0.017368
 92%|██████████████████████████▊  | 369/400 [2:37:06<12:36, 24.39s/it]2021-12-11 22:03:26,961 iteration 6274 : loss : 0.026628, loss_ce: 0.011279
2021-12-11 22:03:28,316 iteration 6275 : loss : 0.030231, loss_ce: 0.010932
2021-12-11 22:03:29,634 iteration 6276 : loss : 0.038827, loss_ce: 0.018820
2021-12-11 22:03:31,009 iteration 6277 : loss : 0.031710, loss_ce: 0.014404
2021-12-11 22:03:32,454 iteration 6278 : loss : 0.031075, loss_ce: 0.014463
2021-12-11 22:03:33,878 iteration 6279 : loss : 0.039538, loss_ce: 0.016564
2021-12-11 22:03:35,198 iteration 6280 : loss : 0.027638, loss_ce: 0.011799
2021-12-11 22:03:36,553 iteration 6281 : loss : 0.034161, loss_ce: 0.013754
2021-12-11 22:03:37,923 iteration 6282 : loss : 0.040708, loss_ce: 0.020150
2021-12-11 22:03:39,370 iteration 6283 : loss : 0.038454, loss_ce: 0.018498
2021-12-11 22:03:40,781 iteration 6284 : loss : 0.025814, loss_ce: 0.011869
2021-12-11 22:03:42,173 iteration 6285 : loss : 0.048066, loss_ce: 0.019845
2021-12-11 22:03:43,609 iteration 6286 : loss : 0.039842, loss_ce: 0.023899
2021-12-11 22:03:44,985 iteration 6287 : loss : 0.026844, loss_ce: 0.012969
2021-12-11 22:03:46,288 iteration 6288 : loss : 0.024934, loss_ce: 0.011653
2021-12-11 22:03:47,642 iteration 6289 : loss : 0.030422, loss_ce: 0.014913
2021-12-11 22:03:47,643 Training Data Eval:
2021-12-11 22:03:54,464   Average segmentation loss on training set: 0.0140
2021-12-11 22:03:54,464 Validation Data Eval:
2021-12-11 22:03:56,823   Average segmentation loss on validation set: 0.0881
2021-12-11 22:03:58,193 iteration 6290 : loss : 0.021380, loss_ce: 0.010191
 92%|██████████████████████████▊  | 370/400 [2:37:39<13:27, 26.90s/it]2021-12-11 22:03:59,598 iteration 6291 : loss : 0.029215, loss_ce: 0.011404
2021-12-11 22:04:00,880 iteration 6292 : loss : 0.022243, loss_ce: 0.009880
2021-12-11 22:04:02,218 iteration 6293 : loss : 0.031795, loss_ce: 0.011896
2021-12-11 22:04:03,674 iteration 6294 : loss : 0.034039, loss_ce: 0.013846
2021-12-11 22:04:05,051 iteration 6295 : loss : 0.030193, loss_ce: 0.014055
2021-12-11 22:04:06,359 iteration 6296 : loss : 0.022796, loss_ce: 0.011766
2021-12-11 22:04:07,787 iteration 6297 : loss : 0.046318, loss_ce: 0.020041
2021-12-11 22:04:09,159 iteration 6298 : loss : 0.034849, loss_ce: 0.019118
2021-12-11 22:04:10,531 iteration 6299 : loss : 0.029715, loss_ce: 0.013136
2021-12-11 22:04:11,945 iteration 6300 : loss : 0.034765, loss_ce: 0.012220
2021-12-11 22:04:13,337 iteration 6301 : loss : 0.029117, loss_ce: 0.016321
2021-12-11 22:04:14,779 iteration 6302 : loss : 0.031102, loss_ce: 0.014155
2021-12-11 22:04:16,127 iteration 6303 : loss : 0.022944, loss_ce: 0.010435
2021-12-11 22:04:17,521 iteration 6304 : loss : 0.029356, loss_ce: 0.014851
2021-12-11 22:04:18,865 iteration 6305 : loss : 0.029334, loss_ce: 0.012642
2021-12-11 22:04:20,247 iteration 6306 : loss : 0.025411, loss_ce: 0.014433
2021-12-11 22:04:21,598 iteration 6307 : loss : 0.027799, loss_ce: 0.012309
 93%|██████████████████████████▉  | 371/400 [2:38:02<12:29, 25.85s/it]2021-12-11 22:04:23,053 iteration 6308 : loss : 0.034091, loss_ce: 0.016563
2021-12-11 22:04:24,407 iteration 6309 : loss : 0.032927, loss_ce: 0.012879
2021-12-11 22:04:25,774 iteration 6310 : loss : 0.021796, loss_ce: 0.011325
2021-12-11 22:04:27,212 iteration 6311 : loss : 0.044288, loss_ce: 0.016513
2021-12-11 22:04:28,563 iteration 6312 : loss : 0.028328, loss_ce: 0.014861
2021-12-11 22:04:29,959 iteration 6313 : loss : 0.043551, loss_ce: 0.015450
2021-12-11 22:04:31,375 iteration 6314 : loss : 0.022429, loss_ce: 0.011023
2021-12-11 22:04:32,731 iteration 6315 : loss : 0.040296, loss_ce: 0.019609
2021-12-11 22:04:34,041 iteration 6316 : loss : 0.028797, loss_ce: 0.011887
2021-12-11 22:04:35,334 iteration 6317 : loss : 0.029083, loss_ce: 0.012236
2021-12-11 22:04:36,744 iteration 6318 : loss : 0.044967, loss_ce: 0.020555
2021-12-11 22:04:38,092 iteration 6319 : loss : 0.029928, loss_ce: 0.015122
2021-12-11 22:04:39,471 iteration 6320 : loss : 0.023138, loss_ce: 0.013066
2021-12-11 22:04:40,842 iteration 6321 : loss : 0.031269, loss_ce: 0.013799
2021-12-11 22:04:42,296 iteration 6322 : loss : 0.048304, loss_ce: 0.019432
2021-12-11 22:04:43,757 iteration 6323 : loss : 0.067858, loss_ce: 0.017284
2021-12-11 22:04:45,123 iteration 6324 : loss : 0.024348, loss_ce: 0.012622
 93%|██████████████████████████▉  | 372/400 [2:38:26<11:44, 25.16s/it]2021-12-11 22:04:46,510 iteration 6325 : loss : 0.028121, loss_ce: 0.015394
2021-12-11 22:04:47,820 iteration 6326 : loss : 0.018823, loss_ce: 0.010508
2021-12-11 22:04:49,245 iteration 6327 : loss : 0.033526, loss_ce: 0.015254
2021-12-11 22:04:50,605 iteration 6328 : loss : 0.025777, loss_ce: 0.010068
2021-12-11 22:04:51,932 iteration 6329 : loss : 0.028045, loss_ce: 0.010568
2021-12-11 22:04:53,308 iteration 6330 : loss : 0.020036, loss_ce: 0.009201
2021-12-11 22:04:54,861 iteration 6331 : loss : 0.044758, loss_ce: 0.016494
2021-12-11 22:04:56,210 iteration 6332 : loss : 0.033853, loss_ce: 0.017427
2021-12-11 22:04:57,614 iteration 6333 : loss : 0.025911, loss_ce: 0.010334
2021-12-11 22:04:58,986 iteration 6334 : loss : 0.025590, loss_ce: 0.013947
2021-12-11 22:05:00,456 iteration 6335 : loss : 0.039039, loss_ce: 0.018184
2021-12-11 22:05:01,771 iteration 6336 : loss : 0.016903, loss_ce: 0.009077
2021-12-11 22:05:03,157 iteration 6337 : loss : 0.028259, loss_ce: 0.013051
2021-12-11 22:05:04,626 iteration 6338 : loss : 0.048958, loss_ce: 0.014789
2021-12-11 22:05:05,978 iteration 6339 : loss : 0.033867, loss_ce: 0.017312
2021-12-11 22:05:07,326 iteration 6340 : loss : 0.022137, loss_ce: 0.012658
2021-12-11 22:05:08,717 iteration 6341 : loss : 0.033115, loss_ce: 0.013444
 93%|███████████████████████████  | 373/400 [2:38:50<11:06, 24.69s/it]2021-12-11 22:05:10,154 iteration 6342 : loss : 0.042552, loss_ce: 0.017551
2021-12-11 22:05:11,517 iteration 6343 : loss : 0.028178, loss_ce: 0.010980
2021-12-11 22:05:12,968 iteration 6344 : loss : 0.055263, loss_ce: 0.025749
2021-12-11 22:05:14,366 iteration 6345 : loss : 0.031808, loss_ce: 0.013338
2021-12-11 22:05:15,703 iteration 6346 : loss : 0.027945, loss_ce: 0.013136
2021-12-11 22:05:17,115 iteration 6347 : loss : 0.040725, loss_ce: 0.024571
2021-12-11 22:05:18,460 iteration 6348 : loss : 0.029118, loss_ce: 0.014236
2021-12-11 22:05:19,830 iteration 6349 : loss : 0.034317, loss_ce: 0.014141
2021-12-11 22:05:21,191 iteration 6350 : loss : 0.025377, loss_ce: 0.010695
2021-12-11 22:05:22,581 iteration 6351 : loss : 0.024435, loss_ce: 0.012902
2021-12-11 22:05:24,012 iteration 6352 : loss : 0.038135, loss_ce: 0.015137
2021-12-11 22:05:25,497 iteration 6353 : loss : 0.056224, loss_ce: 0.022891
2021-12-11 22:05:26,850 iteration 6354 : loss : 0.033524, loss_ce: 0.010488
2021-12-11 22:05:28,302 iteration 6355 : loss : 0.033711, loss_ce: 0.013143
2021-12-11 22:05:29,769 iteration 6356 : loss : 0.037960, loss_ce: 0.015612
2021-12-11 22:05:31,185 iteration 6357 : loss : 0.043652, loss_ce: 0.021468
2021-12-11 22:05:32,630 iteration 6358 : loss : 0.025159, loss_ce: 0.010677
 94%|███████████████████████████  | 374/400 [2:39:13<10:35, 24.45s/it]2021-12-11 22:05:34,065 iteration 6359 : loss : 0.023526, loss_ce: 0.011072
2021-12-11 22:05:35,341 iteration 6360 : loss : 0.018012, loss_ce: 0.009050
2021-12-11 22:05:36,789 iteration 6361 : loss : 0.036172, loss_ce: 0.015359
2021-12-11 22:05:38,180 iteration 6362 : loss : 0.029229, loss_ce: 0.013507
2021-12-11 22:05:39,659 iteration 6363 : loss : 0.048907, loss_ce: 0.018872
2021-12-11 22:05:41,078 iteration 6364 : loss : 0.022499, loss_ce: 0.009629
2021-12-11 22:05:42,458 iteration 6365 : loss : 0.048874, loss_ce: 0.023373
2021-12-11 22:05:43,831 iteration 6366 : loss : 0.032809, loss_ce: 0.014681
2021-12-11 22:05:45,264 iteration 6367 : loss : 0.054720, loss_ce: 0.022471
2021-12-11 22:05:46,674 iteration 6368 : loss : 0.038966, loss_ce: 0.013413
2021-12-11 22:05:48,049 iteration 6369 : loss : 0.036967, loss_ce: 0.011709
2021-12-11 22:05:49,462 iteration 6370 : loss : 0.030021, loss_ce: 0.016195
2021-12-11 22:05:50,791 iteration 6371 : loss : 0.021872, loss_ce: 0.010939
2021-12-11 22:05:52,184 iteration 6372 : loss : 0.032457, loss_ce: 0.014913
2021-12-11 22:05:53,516 iteration 6373 : loss : 0.024727, loss_ce: 0.013140
2021-12-11 22:05:54,795 iteration 6374 : loss : 0.024773, loss_ce: 0.012440
2021-12-11 22:05:54,795 Training Data Eval:
2021-12-11 22:06:01,585   Average segmentation loss on training set: 0.0139
2021-12-11 22:06:01,585 Validation Data Eval:
2021-12-11 22:06:03,938   Average segmentation loss on validation set: 0.0855
2021-12-11 22:06:05,344 iteration 6375 : loss : 0.026469, loss_ce: 0.014913
 94%|███████████████████████████▏ | 375/400 [2:39:46<11:13, 26.93s/it]2021-12-11 22:06:06,850 iteration 6376 : loss : 0.031188, loss_ce: 0.013548
2021-12-11 22:06:08,245 iteration 6377 : loss : 0.035448, loss_ce: 0.011781
2021-12-11 22:06:09,597 iteration 6378 : loss : 0.026847, loss_ce: 0.014050
2021-12-11 22:06:10,975 iteration 6379 : loss : 0.028017, loss_ce: 0.015189
2021-12-11 22:06:12,415 iteration 6380 : loss : 0.046052, loss_ce: 0.018037
2021-12-11 22:06:13,860 iteration 6381 : loss : 0.035929, loss_ce: 0.019350
2021-12-11 22:06:15,258 iteration 6382 : loss : 0.029257, loss_ce: 0.015261
2021-12-11 22:06:16,674 iteration 6383 : loss : 0.029329, loss_ce: 0.012034
2021-12-11 22:06:18,119 iteration 6384 : loss : 0.028515, loss_ce: 0.013583
2021-12-11 22:06:19,376 iteration 6385 : loss : 0.027788, loss_ce: 0.010650
2021-12-11 22:06:20,794 iteration 6386 : loss : 0.028854, loss_ce: 0.012814
2021-12-11 22:06:22,214 iteration 6387 : loss : 0.043959, loss_ce: 0.023069
2021-12-11 22:06:23,527 iteration 6388 : loss : 0.030361, loss_ce: 0.016243
2021-12-11 22:06:24,954 iteration 6389 : loss : 0.027569, loss_ce: 0.014967
2021-12-11 22:06:26,358 iteration 6390 : loss : 0.045840, loss_ce: 0.020352
2021-12-11 22:06:27,784 iteration 6391 : loss : 0.029934, loss_ce: 0.016093
2021-12-11 22:06:29,221 iteration 6392 : loss : 0.050528, loss_ce: 0.017706
 94%|███████████████████████████▎ | 376/400 [2:40:10<10:24, 26.01s/it]2021-12-11 22:06:30,633 iteration 6393 : loss : 0.023288, loss_ce: 0.013636
2021-12-11 22:06:31,992 iteration 6394 : loss : 0.019631, loss_ce: 0.010777
2021-12-11 22:06:33,336 iteration 6395 : loss : 0.020137, loss_ce: 0.008640
2021-12-11 22:06:34,739 iteration 6396 : loss : 0.030516, loss_ce: 0.015046
2021-12-11 22:06:36,163 iteration 6397 : loss : 0.050239, loss_ce: 0.018647
2021-12-11 22:06:37,591 iteration 6398 : loss : 0.029261, loss_ce: 0.013230
2021-12-11 22:06:38,907 iteration 6399 : loss : 0.023766, loss_ce: 0.009770
2021-12-11 22:06:40,258 iteration 6400 : loss : 0.020205, loss_ce: 0.009724
2021-12-11 22:06:41,563 iteration 6401 : loss : 0.026105, loss_ce: 0.013034
2021-12-11 22:06:42,976 iteration 6402 : loss : 0.030663, loss_ce: 0.015636
2021-12-11 22:06:44,313 iteration 6403 : loss : 0.031975, loss_ce: 0.014579
2021-12-11 22:06:45,624 iteration 6404 : loss : 0.023243, loss_ce: 0.013623
2021-12-11 22:06:47,130 iteration 6405 : loss : 0.042910, loss_ce: 0.019878
2021-12-11 22:06:48,448 iteration 6406 : loss : 0.017976, loss_ce: 0.009262
2021-12-11 22:06:49,846 iteration 6407 : loss : 0.042127, loss_ce: 0.017776
2021-12-11 22:06:51,257 iteration 6408 : loss : 0.030531, loss_ce: 0.012744
2021-12-11 22:06:52,580 iteration 6409 : loss : 0.019578, loss_ce: 0.009060
 94%|███████████████████████████▎ | 377/400 [2:40:33<09:39, 25.22s/it]2021-12-11 22:06:53,939 iteration 6410 : loss : 0.025724, loss_ce: 0.012010
2021-12-11 22:06:55,286 iteration 6411 : loss : 0.027917, loss_ce: 0.013299
2021-12-11 22:06:56,721 iteration 6412 : loss : 0.028644, loss_ce: 0.010776
2021-12-11 22:06:58,095 iteration 6413 : loss : 0.029741, loss_ce: 0.015256
2021-12-11 22:06:59,520 iteration 6414 : loss : 0.029162, loss_ce: 0.013023
2021-12-11 22:07:00,944 iteration 6415 : loss : 0.025389, loss_ce: 0.011147
2021-12-11 22:07:02,272 iteration 6416 : loss : 0.033393, loss_ce: 0.015341
2021-12-11 22:07:03,685 iteration 6417 : loss : 0.033810, loss_ce: 0.013656
2021-12-11 22:07:05,060 iteration 6418 : loss : 0.032300, loss_ce: 0.016456
2021-12-11 22:07:06,434 iteration 6419 : loss : 0.037036, loss_ce: 0.017031
2021-12-11 22:07:07,840 iteration 6420 : loss : 0.032122, loss_ce: 0.016946
2021-12-11 22:07:09,262 iteration 6421 : loss : 0.026751, loss_ce: 0.012641
2021-12-11 22:07:10,634 iteration 6422 : loss : 0.033530, loss_ce: 0.015296
2021-12-11 22:07:12,027 iteration 6423 : loss : 0.025957, loss_ce: 0.013121
2021-12-11 22:07:13,361 iteration 6424 : loss : 0.022619, loss_ce: 0.010078
2021-12-11 22:07:14,859 iteration 6425 : loss : 0.027221, loss_ce: 0.012141
2021-12-11 22:07:16,172 iteration 6426 : loss : 0.029769, loss_ce: 0.012691
 94%|███████████████████████████▍ | 378/400 [2:40:57<09:04, 24.73s/it]2021-12-11 22:07:17,611 iteration 6427 : loss : 0.029856, loss_ce: 0.015442
2021-12-11 22:07:19,042 iteration 6428 : loss : 0.044417, loss_ce: 0.016047
2021-12-11 22:07:20,414 iteration 6429 : loss : 0.026920, loss_ce: 0.013317
2021-12-11 22:07:21,852 iteration 6430 : loss : 0.027183, loss_ce: 0.013973
2021-12-11 22:07:23,233 iteration 6431 : loss : 0.035568, loss_ce: 0.012832
2021-12-11 22:07:24,609 iteration 6432 : loss : 0.027926, loss_ce: 0.012116
2021-12-11 22:07:25,981 iteration 6433 : loss : 0.036945, loss_ce: 0.012140
2021-12-11 22:07:27,367 iteration 6434 : loss : 0.045632, loss_ce: 0.022302
2021-12-11 22:07:28,650 iteration 6435 : loss : 0.025294, loss_ce: 0.011360
2021-12-11 22:07:29,977 iteration 6436 : loss : 0.021962, loss_ce: 0.013048
2021-12-11 22:07:31,287 iteration 6437 : loss : 0.022720, loss_ce: 0.010601
2021-12-11 22:07:32,584 iteration 6438 : loss : 0.020288, loss_ce: 0.011195
2021-12-11 22:07:34,017 iteration 6439 : loss : 0.039190, loss_ce: 0.014969
2021-12-11 22:07:35,415 iteration 6440 : loss : 0.022289, loss_ce: 0.010203
2021-12-11 22:07:36,793 iteration 6441 : loss : 0.021353, loss_ce: 0.011105
2021-12-11 22:07:38,163 iteration 6442 : loss : 0.026115, loss_ce: 0.014896
2021-12-11 22:07:39,505 iteration 6443 : loss : 0.022742, loss_ce: 0.010144
 95%|███████████████████████████▍ | 379/400 [2:41:20<08:30, 24.31s/it]2021-12-11 22:07:40,886 iteration 6444 : loss : 0.026536, loss_ce: 0.014326
2021-12-11 22:07:42,215 iteration 6445 : loss : 0.019199, loss_ce: 0.010040
2021-12-11 22:07:43,578 iteration 6446 : loss : 0.035613, loss_ce: 0.014871
2021-12-11 22:07:44,952 iteration 6447 : loss : 0.025070, loss_ce: 0.012749
2021-12-11 22:07:46,330 iteration 6448 : loss : 0.045773, loss_ce: 0.014076
2021-12-11 22:07:47,726 iteration 6449 : loss : 0.039429, loss_ce: 0.014933
2021-12-11 22:07:49,160 iteration 6450 : loss : 0.045568, loss_ce: 0.016469
2021-12-11 22:07:50,521 iteration 6451 : loss : 0.029924, loss_ce: 0.013060
2021-12-11 22:07:51,942 iteration 6452 : loss : 0.046137, loss_ce: 0.012711
2021-12-11 22:07:53,298 iteration 6453 : loss : 0.026948, loss_ce: 0.014181
2021-12-11 22:07:54,673 iteration 6454 : loss : 0.032630, loss_ce: 0.012702
2021-12-11 22:07:55,996 iteration 6455 : loss : 0.028753, loss_ce: 0.013381
2021-12-11 22:07:57,382 iteration 6456 : loss : 0.025129, loss_ce: 0.014518
2021-12-11 22:07:58,735 iteration 6457 : loss : 0.022615, loss_ce: 0.011461
2021-12-11 22:08:00,062 iteration 6458 : loss : 0.032670, loss_ce: 0.020975
2021-12-11 22:08:01,496 iteration 6459 : loss : 0.031639, loss_ce: 0.013806
2021-12-11 22:08:01,496 Training Data Eval:
2021-12-11 22:08:08,314   Average segmentation loss on training set: 0.0138
2021-12-11 22:08:08,314 Validation Data Eval:
2021-12-11 22:08:10,671   Average segmentation loss on validation set: 0.0856
2021-12-11 22:08:12,130 iteration 6460 : loss : 0.023010, loss_ce: 0.010530
 95%|███████████████████████████▌ | 380/400 [2:41:53<08:56, 26.80s/it]2021-12-11 22:08:13,524 iteration 6461 : loss : 0.020082, loss_ce: 0.009445
2021-12-11 22:08:14,910 iteration 6462 : loss : 0.027686, loss_ce: 0.012862
2021-12-11 22:08:16,234 iteration 6463 : loss : 0.027527, loss_ce: 0.014650
2021-12-11 22:08:17,583 iteration 6464 : loss : 0.033834, loss_ce: 0.018236
2021-12-11 22:08:18,947 iteration 6465 : loss : 0.024516, loss_ce: 0.010366
2021-12-11 22:08:20,315 iteration 6466 : loss : 0.034850, loss_ce: 0.016442
2021-12-11 22:08:21,687 iteration 6467 : loss : 0.031737, loss_ce: 0.016365
2021-12-11 22:08:23,000 iteration 6468 : loss : 0.033884, loss_ce: 0.012620
2021-12-11 22:08:24,414 iteration 6469 : loss : 0.034658, loss_ce: 0.011783
2021-12-11 22:08:25,817 iteration 6470 : loss : 0.025187, loss_ce: 0.012240
2021-12-11 22:08:27,203 iteration 6471 : loss : 0.027733, loss_ce: 0.013826
2021-12-11 22:08:28,532 iteration 6472 : loss : 0.036287, loss_ce: 0.014536
2021-12-11 22:08:29,942 iteration 6473 : loss : 0.023561, loss_ce: 0.011595
2021-12-11 22:08:31,340 iteration 6474 : loss : 0.021617, loss_ce: 0.011717
2021-12-11 22:08:32,763 iteration 6475 : loss : 0.037325, loss_ce: 0.014796
2021-12-11 22:08:34,170 iteration 6476 : loss : 0.028289, loss_ce: 0.013580
2021-12-11 22:08:35,561 iteration 6477 : loss : 0.034200, loss_ce: 0.015873
 95%|███████████████████████████▌ | 381/400 [2:42:16<08:10, 25.79s/it]2021-12-11 22:08:36,976 iteration 6478 : loss : 0.022525, loss_ce: 0.011235
2021-12-11 22:08:38,367 iteration 6479 : loss : 0.037783, loss_ce: 0.015003
2021-12-11 22:08:39,718 iteration 6480 : loss : 0.025852, loss_ce: 0.013162
2021-12-11 22:08:41,224 iteration 6481 : loss : 0.039926, loss_ce: 0.014745
2021-12-11 22:08:42,577 iteration 6482 : loss : 0.036145, loss_ce: 0.017016
2021-12-11 22:08:44,013 iteration 6483 : loss : 0.043517, loss_ce: 0.021249
2021-12-11 22:08:45,300 iteration 6484 : loss : 0.029739, loss_ce: 0.013281
2021-12-11 22:08:46,672 iteration 6485 : loss : 0.026207, loss_ce: 0.010646
2021-12-11 22:08:48,065 iteration 6486 : loss : 0.037266, loss_ce: 0.015643
2021-12-11 22:08:49,476 iteration 6487 : loss : 0.032773, loss_ce: 0.015212
2021-12-11 22:08:50,897 iteration 6488 : loss : 0.025334, loss_ce: 0.011239
2021-12-11 22:08:52,277 iteration 6489 : loss : 0.034762, loss_ce: 0.013127
2021-12-11 22:08:53,635 iteration 6490 : loss : 0.026392, loss_ce: 0.012019
2021-12-11 22:08:54,970 iteration 6491 : loss : 0.026895, loss_ce: 0.014518
2021-12-11 22:08:56,298 iteration 6492 : loss : 0.022303, loss_ce: 0.012804
2021-12-11 22:08:57,658 iteration 6493 : loss : 0.034600, loss_ce: 0.014093
2021-12-11 22:08:59,053 iteration 6494 : loss : 0.031874, loss_ce: 0.014858
 96%|███████████████████████████▋ | 382/400 [2:42:40<07:31, 25.10s/it]2021-12-11 22:09:00,402 iteration 6495 : loss : 0.027873, loss_ce: 0.014813
2021-12-11 22:09:01,764 iteration 6496 : loss : 0.033907, loss_ce: 0.013156
2021-12-11 22:09:03,174 iteration 6497 : loss : 0.021330, loss_ce: 0.013221
2021-12-11 22:09:04,519 iteration 6498 : loss : 0.025155, loss_ce: 0.010238
2021-12-11 22:09:05,882 iteration 6499 : loss : 0.037401, loss_ce: 0.011646
2021-12-11 22:09:07,351 iteration 6500 : loss : 0.053716, loss_ce: 0.019460
2021-12-11 22:09:08,802 iteration 6501 : loss : 0.043733, loss_ce: 0.022340
2021-12-11 22:09:10,198 iteration 6502 : loss : 0.029582, loss_ce: 0.012775
2021-12-11 22:09:11,600 iteration 6503 : loss : 0.029410, loss_ce: 0.012433
2021-12-11 22:09:13,000 iteration 6504 : loss : 0.031645, loss_ce: 0.017307
2021-12-11 22:09:14,323 iteration 6505 : loss : 0.020677, loss_ce: 0.010480
2021-12-11 22:09:15,724 iteration 6506 : loss : 0.026822, loss_ce: 0.010525
2021-12-11 22:09:17,069 iteration 6507 : loss : 0.031374, loss_ce: 0.015137
2021-12-11 22:09:18,361 iteration 6508 : loss : 0.035613, loss_ce: 0.014265
2021-12-11 22:09:19,723 iteration 6509 : loss : 0.020829, loss_ce: 0.009547
2021-12-11 22:09:20,991 iteration 6510 : loss : 0.024399, loss_ce: 0.011415
2021-12-11 22:09:22,378 iteration 6511 : loss : 0.025854, loss_ce: 0.012270
 96%|███████████████████████████▊ | 383/400 [2:43:03<06:57, 24.57s/it]2021-12-11 22:09:23,827 iteration 6512 : loss : 0.030932, loss_ce: 0.012840
2021-12-11 22:09:25,262 iteration 6513 : loss : 0.031760, loss_ce: 0.014474
2021-12-11 22:09:26,651 iteration 6514 : loss : 0.036047, loss_ce: 0.014273
2021-12-11 22:09:28,081 iteration 6515 : loss : 0.047934, loss_ce: 0.017810
2021-12-11 22:09:29,432 iteration 6516 : loss : 0.027623, loss_ce: 0.013212
2021-12-11 22:09:30,759 iteration 6517 : loss : 0.026179, loss_ce: 0.014798
2021-12-11 22:09:32,146 iteration 6518 : loss : 0.034487, loss_ce: 0.017438
2021-12-11 22:09:33,581 iteration 6519 : loss : 0.035775, loss_ce: 0.016186
2021-12-11 22:09:34,895 iteration 6520 : loss : 0.030240, loss_ce: 0.015143
2021-12-11 22:09:36,313 iteration 6521 : loss : 0.038573, loss_ce: 0.021256
2021-12-11 22:09:37,695 iteration 6522 : loss : 0.027392, loss_ce: 0.012893
2021-12-11 22:09:39,097 iteration 6523 : loss : 0.023720, loss_ce: 0.011875
2021-12-11 22:09:40,500 iteration 6524 : loss : 0.045504, loss_ce: 0.017839
2021-12-11 22:09:41,924 iteration 6525 : loss : 0.036996, loss_ce: 0.014914
2021-12-11 22:09:43,305 iteration 6526 : loss : 0.025177, loss_ce: 0.012468
2021-12-11 22:09:44,644 iteration 6527 : loss : 0.032567, loss_ce: 0.013273
2021-12-11 22:09:45,983 iteration 6528 : loss : 0.021212, loss_ce: 0.010539
 96%|███████████████████████████▊ | 384/400 [2:43:27<06:28, 24.28s/it]2021-12-11 22:09:47,560 iteration 6529 : loss : 0.034262, loss_ce: 0.015933
2021-12-11 22:09:48,939 iteration 6530 : loss : 0.034901, loss_ce: 0.017290
2021-12-11 22:09:50,417 iteration 6531 : loss : 0.032141, loss_ce: 0.013146
2021-12-11 22:09:51,813 iteration 6532 : loss : 0.048972, loss_ce: 0.022960
2021-12-11 22:09:53,245 iteration 6533 : loss : 0.048054, loss_ce: 0.012736
2021-12-11 22:09:54,769 iteration 6534 : loss : 0.039264, loss_ce: 0.013341
2021-12-11 22:09:56,089 iteration 6535 : loss : 0.042629, loss_ce: 0.014519
2021-12-11 22:09:57,465 iteration 6536 : loss : 0.037818, loss_ce: 0.016856
2021-12-11 22:09:58,855 iteration 6537 : loss : 0.024160, loss_ce: 0.011462
2021-12-11 22:10:00,262 iteration 6538 : loss : 0.033008, loss_ce: 0.017436
2021-12-11 22:10:01,744 iteration 6539 : loss : 0.036789, loss_ce: 0.015644
2021-12-11 22:10:03,125 iteration 6540 : loss : 0.033173, loss_ce: 0.017471
2021-12-11 22:10:04,482 iteration 6541 : loss : 0.027639, loss_ce: 0.013196
2021-12-11 22:10:05,919 iteration 6542 : loss : 0.029711, loss_ce: 0.014041
2021-12-11 22:10:07,330 iteration 6543 : loss : 0.043056, loss_ce: 0.019962
2021-12-11 22:10:08,641 iteration 6544 : loss : 0.020110, loss_ce: 0.009571
2021-12-11 22:10:08,641 Training Data Eval:
2021-12-11 22:10:15,447   Average segmentation loss on training set: 0.0135
2021-12-11 22:10:15,447 Validation Data Eval:
2021-12-11 22:10:17,795   Average segmentation loss on validation set: 0.0824
2021-12-11 22:10:19,059 iteration 6545 : loss : 0.020286, loss_ce: 0.008383
 96%|███████████████████████████▉ | 385/400 [2:44:00<06:43, 26.92s/it]2021-12-11 22:10:20,498 iteration 6546 : loss : 0.042755, loss_ce: 0.014182
2021-12-11 22:10:22,060 iteration 6547 : loss : 0.037916, loss_ce: 0.016044
2021-12-11 22:10:23,355 iteration 6548 : loss : 0.029996, loss_ce: 0.019401
2021-12-11 22:10:24,654 iteration 6549 : loss : 0.015298, loss_ce: 0.007907
2021-12-11 22:10:25,994 iteration 6550 : loss : 0.020803, loss_ce: 0.010179
2021-12-11 22:10:27,427 iteration 6551 : loss : 0.027306, loss_ce: 0.014956
2021-12-11 22:10:28,716 iteration 6552 : loss : 0.027376, loss_ce: 0.014963
2021-12-11 22:10:30,161 iteration 6553 : loss : 0.050043, loss_ce: 0.021866
2021-12-11 22:10:31,544 iteration 6554 : loss : 0.036738, loss_ce: 0.014762
2021-12-11 22:10:32,988 iteration 6555 : loss : 0.042363, loss_ce: 0.017816
2021-12-11 22:10:34,413 iteration 6556 : loss : 0.028222, loss_ce: 0.011064
2021-12-11 22:10:35,744 iteration 6557 : loss : 0.026692, loss_ce: 0.011378
2021-12-11 22:10:37,073 iteration 6558 : loss : 0.039456, loss_ce: 0.014841
2021-12-11 22:10:38,475 iteration 6559 : loss : 0.034887, loss_ce: 0.019038
2021-12-11 22:10:39,935 iteration 6560 : loss : 0.092709, loss_ce: 0.027687
2021-12-11 22:10:41,277 iteration 6561 : loss : 0.027599, loss_ce: 0.013372
2021-12-11 22:10:42,565 iteration 6562 : loss : 0.018356, loss_ce: 0.008629
 96%|███████████████████████████▉ | 386/400 [2:44:23<06:02, 25.90s/it]2021-12-11 22:10:43,969 iteration 6563 : loss : 0.022060, loss_ce: 0.012008
2021-12-11 22:10:45,307 iteration 6564 : loss : 0.032721, loss_ce: 0.010700
2021-12-11 22:10:46,612 iteration 6565 : loss : 0.020577, loss_ce: 0.008727
2021-12-11 22:10:47,954 iteration 6566 : loss : 0.019894, loss_ce: 0.010312
2021-12-11 22:10:49,361 iteration 6567 : loss : 0.022595, loss_ce: 0.011441
2021-12-11 22:10:50,755 iteration 6568 : loss : 0.036794, loss_ce: 0.017424
2021-12-11 22:10:52,186 iteration 6569 : loss : 0.037862, loss_ce: 0.014767
2021-12-11 22:10:53,565 iteration 6570 : loss : 0.035253, loss_ce: 0.013361
2021-12-11 22:10:55,074 iteration 6571 : loss : 0.037871, loss_ce: 0.020058
2021-12-11 22:10:56,391 iteration 6572 : loss : 0.021096, loss_ce: 0.011055
2021-12-11 22:10:57,833 iteration 6573 : loss : 0.046419, loss_ce: 0.018379
2021-12-11 22:10:59,186 iteration 6574 : loss : 0.021011, loss_ce: 0.008836
2021-12-11 22:11:00,639 iteration 6575 : loss : 0.047149, loss_ce: 0.018391
2021-12-11 22:11:02,034 iteration 6576 : loss : 0.030141, loss_ce: 0.015415
2021-12-11 22:11:03,379 iteration 6577 : loss : 0.026048, loss_ce: 0.012018
2021-12-11 22:11:04,782 iteration 6578 : loss : 0.021441, loss_ce: 0.010387
2021-12-11 22:11:06,131 iteration 6579 : loss : 0.033158, loss_ce: 0.016269
 97%|████████████████████████████ | 387/400 [2:44:47<05:27, 25.19s/it]2021-12-11 22:11:07,599 iteration 6580 : loss : 0.031586, loss_ce: 0.016058
2021-12-11 22:11:08,939 iteration 6581 : loss : 0.026645, loss_ce: 0.010865
2021-12-11 22:11:10,410 iteration 6582 : loss : 0.032764, loss_ce: 0.012630
2021-12-11 22:11:11,800 iteration 6583 : loss : 0.032241, loss_ce: 0.014785
2021-12-11 22:11:13,131 iteration 6584 : loss : 0.030361, loss_ce: 0.014839
2021-12-11 22:11:14,423 iteration 6585 : loss : 0.025191, loss_ce: 0.010913
2021-12-11 22:11:15,769 iteration 6586 : loss : 0.024146, loss_ce: 0.008309
2021-12-11 22:11:17,077 iteration 6587 : loss : 0.024481, loss_ce: 0.013755
2021-12-11 22:11:18,466 iteration 6588 : loss : 0.022269, loss_ce: 0.011218
2021-12-11 22:11:19,869 iteration 6589 : loss : 0.027649, loss_ce: 0.013716
2021-12-11 22:11:21,219 iteration 6590 : loss : 0.031717, loss_ce: 0.017840
2021-12-11 22:11:22,584 iteration 6591 : loss : 0.017680, loss_ce: 0.009316
2021-12-11 22:11:24,045 iteration 6592 : loss : 0.037675, loss_ce: 0.022999
2021-12-11 22:11:25,344 iteration 6593 : loss : 0.023619, loss_ce: 0.013701
2021-12-11 22:11:26,692 iteration 6594 : loss : 0.022104, loss_ce: 0.011077
2021-12-11 22:11:28,103 iteration 6595 : loss : 0.049501, loss_ce: 0.017205
2021-12-11 22:11:29,444 iteration 6596 : loss : 0.020537, loss_ce: 0.009126
 97%|████████████████████████████▏| 388/400 [2:45:10<04:55, 24.64s/it]2021-12-11 22:11:30,835 iteration 6597 : loss : 0.020201, loss_ce: 0.011022
2021-12-11 22:11:32,263 iteration 6598 : loss : 0.044863, loss_ce: 0.011581
2021-12-11 22:11:33,719 iteration 6599 : loss : 0.037509, loss_ce: 0.015475
2021-12-11 22:11:35,043 iteration 6600 : loss : 0.022026, loss_ce: 0.010681
2021-12-11 22:11:36,520 iteration 6601 : loss : 0.034295, loss_ce: 0.016463
2021-12-11 22:11:37,945 iteration 6602 : loss : 0.030336, loss_ce: 0.014313
2021-12-11 22:11:39,297 iteration 6603 : loss : 0.027722, loss_ce: 0.014296
2021-12-11 22:11:40,731 iteration 6604 : loss : 0.032027, loss_ce: 0.018865
2021-12-11 22:11:42,100 iteration 6605 : loss : 0.035462, loss_ce: 0.019301
2021-12-11 22:11:43,550 iteration 6606 : loss : 0.042527, loss_ce: 0.018057
2021-12-11 22:11:44,999 iteration 6607 : loss : 0.041954, loss_ce: 0.019940
2021-12-11 22:11:46,335 iteration 6608 : loss : 0.028990, loss_ce: 0.015137
2021-12-11 22:11:47,772 iteration 6609 : loss : 0.035379, loss_ce: 0.017441
2021-12-11 22:11:49,140 iteration 6610 : loss : 0.024964, loss_ce: 0.012114
2021-12-11 22:11:50,490 iteration 6611 : loss : 0.042713, loss_ce: 0.018674
2021-12-11 22:11:51,866 iteration 6612 : loss : 0.039749, loss_ce: 0.015740
2021-12-11 22:11:53,302 iteration 6613 : loss : 0.036859, loss_ce: 0.014789
 97%|████████████████████████████▏| 389/400 [2:45:34<04:28, 24.40s/it]2021-12-11 22:11:54,706 iteration 6614 : loss : 0.018925, loss_ce: 0.009688
2021-12-11 22:11:56,010 iteration 6615 : loss : 0.019664, loss_ce: 0.009801
2021-12-11 22:11:57,434 iteration 6616 : loss : 0.038080, loss_ce: 0.016216
2021-12-11 22:11:58,798 iteration 6617 : loss : 0.039379, loss_ce: 0.016506
2021-12-11 22:12:00,266 iteration 6618 : loss : 0.040209, loss_ce: 0.019271
2021-12-11 22:12:01,661 iteration 6619 : loss : 0.028112, loss_ce: 0.013399
2021-12-11 22:12:03,058 iteration 6620 : loss : 0.028758, loss_ce: 0.011683
2021-12-11 22:12:04,494 iteration 6621 : loss : 0.032616, loss_ce: 0.014628
2021-12-11 22:12:05,860 iteration 6622 : loss : 0.027098, loss_ce: 0.014669
2021-12-11 22:12:07,183 iteration 6623 : loss : 0.027371, loss_ce: 0.011339
2021-12-11 22:12:08,555 iteration 6624 : loss : 0.028049, loss_ce: 0.014403
2021-12-11 22:12:09,960 iteration 6625 : loss : 0.054057, loss_ce: 0.023394
2021-12-11 22:12:11,345 iteration 6626 : loss : 0.033504, loss_ce: 0.015645
2021-12-11 22:12:12,772 iteration 6627 : loss : 0.048695, loss_ce: 0.021122
2021-12-11 22:12:14,179 iteration 6628 : loss : 0.029069, loss_ce: 0.014110
2021-12-11 22:12:15,685 iteration 6629 : loss : 0.042354, loss_ce: 0.014014
2021-12-11 22:12:15,685 Training Data Eval:
2021-12-11 22:12:22,480   Average segmentation loss on training set: 0.0139
2021-12-11 22:12:22,480 Validation Data Eval:
2021-12-11 22:12:24,831   Average segmentation loss on validation set: 0.0934
2021-12-11 22:12:26,187 iteration 6630 : loss : 0.027957, loss_ce: 0.013295
 98%|████████████████████████████▎| 390/400 [2:46:07<04:29, 26.94s/it]2021-12-11 22:12:27,622 iteration 6631 : loss : 0.033583, loss_ce: 0.014084
2021-12-11 22:12:28,965 iteration 6632 : loss : 0.025234, loss_ce: 0.014274
2021-12-11 22:12:30,383 iteration 6633 : loss : 0.035919, loss_ce: 0.013265
2021-12-11 22:12:31,749 iteration 6634 : loss : 0.030053, loss_ce: 0.013658
2021-12-11 22:12:33,132 iteration 6635 : loss : 0.028172, loss_ce: 0.014619
2021-12-11 22:12:34,520 iteration 6636 : loss : 0.020438, loss_ce: 0.012656
2021-12-11 22:12:35,862 iteration 6637 : loss : 0.032372, loss_ce: 0.013594
2021-12-11 22:12:37,289 iteration 6638 : loss : 0.054705, loss_ce: 0.013962
2021-12-11 22:12:38,729 iteration 6639 : loss : 0.027896, loss_ce: 0.013629
2021-12-11 22:12:40,092 iteration 6640 : loss : 0.031307, loss_ce: 0.011511
2021-12-11 22:12:41,491 iteration 6641 : loss : 0.025236, loss_ce: 0.010708
2021-12-11 22:12:42,880 iteration 6642 : loss : 0.038766, loss_ce: 0.019178
2021-12-11 22:12:44,274 iteration 6643 : loss : 0.030664, loss_ce: 0.012502
2021-12-11 22:12:45,777 iteration 6644 : loss : 0.048663, loss_ce: 0.021434
2021-12-11 22:12:47,129 iteration 6645 : loss : 0.029504, loss_ce: 0.014097
2021-12-11 22:12:48,629 iteration 6646 : loss : 0.040843, loss_ce: 0.015744
2021-12-11 22:12:50,002 iteration 6647 : loss : 0.028453, loss_ce: 0.012509
 98%|████████████████████████████▎| 391/400 [2:46:31<03:54, 26.01s/it]2021-12-11 22:12:51,422 iteration 6648 : loss : 0.037243, loss_ce: 0.011282
2021-12-11 22:12:52,844 iteration 6649 : loss : 0.039125, loss_ce: 0.013511
2021-12-11 22:12:54,316 iteration 6650 : loss : 0.043624, loss_ce: 0.019100
2021-12-11 22:12:55,810 iteration 6651 : loss : 0.041775, loss_ce: 0.020576
2021-12-11 22:12:57,191 iteration 6652 : loss : 0.023632, loss_ce: 0.012538
2021-12-11 22:12:58,498 iteration 6653 : loss : 0.018891, loss_ce: 0.010302
2021-12-11 22:12:59,865 iteration 6654 : loss : 0.032827, loss_ce: 0.014075
2021-12-11 22:13:01,284 iteration 6655 : loss : 0.027622, loss_ce: 0.012660
2021-12-11 22:13:02,696 iteration 6656 : loss : 0.038945, loss_ce: 0.017416
2021-12-11 22:13:04,099 iteration 6657 : loss : 0.028012, loss_ce: 0.010291
2021-12-11 22:13:05,537 iteration 6658 : loss : 0.025220, loss_ce: 0.011160
2021-12-11 22:13:06,937 iteration 6659 : loss : 0.032869, loss_ce: 0.016556
2021-12-11 22:13:08,380 iteration 6660 : loss : 0.027657, loss_ce: 0.013277
2021-12-11 22:13:09,794 iteration 6661 : loss : 0.027934, loss_ce: 0.012937
2021-12-11 22:13:11,220 iteration 6662 : loss : 0.031680, loss_ce: 0.011049
2021-12-11 22:13:12,623 iteration 6663 : loss : 0.036434, loss_ce: 0.014188
2021-12-11 22:13:14,056 iteration 6664 : loss : 0.079498, loss_ce: 0.045760
 98%|████████████████████████████▍| 392/400 [2:46:55<03:23, 25.42s/it]2021-12-11 22:13:15,530 iteration 6665 : loss : 0.043328, loss_ce: 0.016495
2021-12-11 22:13:17,041 iteration 6666 : loss : 0.052804, loss_ce: 0.016313
2021-12-11 22:13:18,467 iteration 6667 : loss : 0.042456, loss_ce: 0.012356
2021-12-11 22:13:19,779 iteration 6668 : loss : 0.029889, loss_ce: 0.017065
2021-12-11 22:13:21,161 iteration 6669 : loss : 0.026345, loss_ce: 0.013283
2021-12-11 22:13:22,540 iteration 6670 : loss : 0.026890, loss_ce: 0.015442
2021-12-11 22:13:23,955 iteration 6671 : loss : 0.028793, loss_ce: 0.012783
2021-12-11 22:13:25,379 iteration 6672 : loss : 0.032479, loss_ce: 0.012341
2021-12-11 22:13:26,761 iteration 6673 : loss : 0.039052, loss_ce: 0.012035
2021-12-11 22:13:28,137 iteration 6674 : loss : 0.032674, loss_ce: 0.015881
2021-12-11 22:13:29,605 iteration 6675 : loss : 0.031681, loss_ce: 0.017004
2021-12-11 22:13:30,962 iteration 6676 : loss : 0.027045, loss_ce: 0.012761
2021-12-11 22:13:32,307 iteration 6677 : loss : 0.033702, loss_ce: 0.012928
2021-12-11 22:13:33,677 iteration 6678 : loss : 0.033133, loss_ce: 0.018491
2021-12-11 22:13:35,102 iteration 6679 : loss : 0.040287, loss_ce: 0.020896
2021-12-11 22:13:36,428 iteration 6680 : loss : 0.019123, loss_ce: 0.009557
2021-12-11 22:13:37,742 iteration 6681 : loss : 0.021859, loss_ce: 0.009654
 98%|████████████████████████████▍| 393/400 [2:47:19<02:54, 24.90s/it]2021-12-11 22:13:39,285 iteration 6682 : loss : 0.031357, loss_ce: 0.016681
2021-12-11 22:13:40,634 iteration 6683 : loss : 0.028936, loss_ce: 0.012916
2021-12-11 22:13:42,026 iteration 6684 : loss : 0.030644, loss_ce: 0.017401
2021-12-11 22:13:43,362 iteration 6685 : loss : 0.030777, loss_ce: 0.013774
2021-12-11 22:13:44,821 iteration 6686 : loss : 0.033258, loss_ce: 0.015110
2021-12-11 22:13:46,195 iteration 6687 : loss : 0.027889, loss_ce: 0.012320
2021-12-11 22:13:47,579 iteration 6688 : loss : 0.034143, loss_ce: 0.014686
2021-12-11 22:13:48,890 iteration 6689 : loss : 0.024156, loss_ce: 0.010376
2021-12-11 22:13:50,269 iteration 6690 : loss : 0.022518, loss_ce: 0.010402
2021-12-11 22:13:51,693 iteration 6691 : loss : 0.032861, loss_ce: 0.014593
2021-12-11 22:13:53,042 iteration 6692 : loss : 0.034618, loss_ce: 0.016062
2021-12-11 22:13:54,405 iteration 6693 : loss : 0.031082, loss_ce: 0.013424
2021-12-11 22:13:55,807 iteration 6694 : loss : 0.034966, loss_ce: 0.013337
2021-12-11 22:13:57,161 iteration 6695 : loss : 0.031971, loss_ce: 0.012953
2021-12-11 22:13:58,549 iteration 6696 : loss : 0.027796, loss_ce: 0.012449
2021-12-11 22:13:59,925 iteration 6697 : loss : 0.028500, loss_ce: 0.013166
2021-12-11 22:14:01,390 iteration 6698 : loss : 0.026797, loss_ce: 0.011420
 98%|████████████████████████████▌| 394/400 [2:47:42<02:27, 24.52s/it]2021-12-11 22:14:02,806 iteration 6699 : loss : 0.024077, loss_ce: 0.011249
2021-12-11 22:14:04,255 iteration 6700 : loss : 0.033228, loss_ce: 0.012326
2021-12-11 22:14:05,631 iteration 6701 : loss : 0.024905, loss_ce: 0.011301
2021-12-11 22:14:06,947 iteration 6702 : loss : 0.021235, loss_ce: 0.011935
2021-12-11 22:14:08,261 iteration 6703 : loss : 0.021785, loss_ce: 0.010385
2021-12-11 22:14:09,684 iteration 6704 : loss : 0.037080, loss_ce: 0.019437
2021-12-11 22:14:11,008 iteration 6705 : loss : 0.035250, loss_ce: 0.017709
2021-12-11 22:14:12,465 iteration 6706 : loss : 0.031739, loss_ce: 0.014290
2021-12-11 22:14:13,788 iteration 6707 : loss : 0.021281, loss_ce: 0.010358
2021-12-11 22:14:15,173 iteration 6708 : loss : 0.027866, loss_ce: 0.012993
2021-12-11 22:14:16,569 iteration 6709 : loss : 0.037070, loss_ce: 0.017930
2021-12-11 22:14:17,992 iteration 6710 : loss : 0.045765, loss_ce: 0.015005
2021-12-11 22:14:19,407 iteration 6711 : loss : 0.025826, loss_ce: 0.010688
2021-12-11 22:14:20,882 iteration 6712 : loss : 0.030762, loss_ce: 0.016270
2021-12-11 22:14:22,213 iteration 6713 : loss : 0.029852, loss_ce: 0.012787
2021-12-11 22:14:23,598 iteration 6714 : loss : 0.025470, loss_ce: 0.011802
2021-12-11 22:14:23,599 Training Data Eval:
2021-12-11 22:14:30,441   Average segmentation loss on training set: 0.0142
2021-12-11 22:14:30,441 Validation Data Eval:
2021-12-11 22:14:32,811   Average segmentation loss on validation set: 0.0823
2021-12-11 22:14:34,145 iteration 6715 : loss : 0.030415, loss_ce: 0.013532
 99%|████████████████████████████▋| 395/400 [2:48:15<02:14, 26.99s/it]2021-12-11 22:14:35,574 iteration 6716 : loss : 0.028078, loss_ce: 0.015196
2021-12-11 22:14:36,935 iteration 6717 : loss : 0.027823, loss_ce: 0.014574
2021-12-11 22:14:38,315 iteration 6718 : loss : 0.024568, loss_ce: 0.011720
2021-12-11 22:14:39,662 iteration 6719 : loss : 0.032410, loss_ce: 0.014461
2021-12-11 22:14:41,022 iteration 6720 : loss : 0.025303, loss_ce: 0.009228
2021-12-11 22:14:42,365 iteration 6721 : loss : 0.023367, loss_ce: 0.010525
2021-12-11 22:14:43,832 iteration 6722 : loss : 0.023152, loss_ce: 0.011388
2021-12-11 22:14:45,129 iteration 6723 : loss : 0.024628, loss_ce: 0.010345
2021-12-11 22:14:46,499 iteration 6724 : loss : 0.038595, loss_ce: 0.015724
2021-12-11 22:14:47,902 iteration 6725 : loss : 0.022945, loss_ce: 0.010963
2021-12-11 22:14:49,341 iteration 6726 : loss : 0.035293, loss_ce: 0.018201
2021-12-11 22:14:50,837 iteration 6727 : loss : 0.043573, loss_ce: 0.016572
2021-12-11 22:14:52,223 iteration 6728 : loss : 0.028788, loss_ce: 0.013088
2021-12-11 22:14:53,619 iteration 6729 : loss : 0.051425, loss_ce: 0.019924
2021-12-11 22:14:54,963 iteration 6730 : loss : 0.024356, loss_ce: 0.011968
2021-12-11 22:14:56,344 iteration 6731 : loss : 0.029222, loss_ce: 0.013494
2021-12-11 22:14:57,765 iteration 6732 : loss : 0.028645, loss_ce: 0.014178
 99%|████████████████████████████▋| 396/400 [2:48:39<01:43, 25.98s/it]2021-12-11 22:14:59,202 iteration 6733 : loss : 0.019726, loss_ce: 0.009749
2021-12-11 22:15:00,697 iteration 6734 : loss : 0.038701, loss_ce: 0.015329
2021-12-11 22:15:02,047 iteration 6735 : loss : 0.022083, loss_ce: 0.011271
2021-12-11 22:15:03,491 iteration 6736 : loss : 0.027610, loss_ce: 0.014125
2021-12-11 22:15:04,935 iteration 6737 : loss : 0.036324, loss_ce: 0.015512
2021-12-11 22:15:06,235 iteration 6738 : loss : 0.026495, loss_ce: 0.010922
2021-12-11 22:15:07,585 iteration 6739 : loss : 0.031393, loss_ce: 0.015236
2021-12-11 22:15:09,006 iteration 6740 : loss : 0.033754, loss_ce: 0.021248
2021-12-11 22:15:10,374 iteration 6741 : loss : 0.020780, loss_ce: 0.010672
2021-12-11 22:15:11,720 iteration 6742 : loss : 0.029882, loss_ce: 0.011602
2021-12-11 22:15:13,130 iteration 6743 : loss : 0.031903, loss_ce: 0.016784
2021-12-11 22:15:14,536 iteration 6744 : loss : 0.039799, loss_ce: 0.018888
2021-12-11 22:15:15,872 iteration 6745 : loss : 0.026711, loss_ce: 0.012937
2021-12-11 22:15:17,265 iteration 6746 : loss : 0.029045, loss_ce: 0.014938
2021-12-11 22:15:18,708 iteration 6747 : loss : 0.036373, loss_ce: 0.012389
2021-12-11 22:15:20,139 iteration 6748 : loss : 0.031535, loss_ce: 0.013051
2021-12-11 22:15:21,543 iteration 6749 : loss : 0.029253, loss_ce: 0.014486
 99%|████████████████████████████▊| 397/400 [2:49:02<01:15, 25.32s/it]2021-12-11 22:15:23,025 iteration 6750 : loss : 0.029137, loss_ce: 0.013650
2021-12-11 22:15:24,521 iteration 6751 : loss : 0.040620, loss_ce: 0.016912
2021-12-11 22:15:25,972 iteration 6752 : loss : 0.038806, loss_ce: 0.018815
2021-12-11 22:15:27,337 iteration 6753 : loss : 0.034791, loss_ce: 0.015808
2021-12-11 22:15:28,790 iteration 6754 : loss : 0.035451, loss_ce: 0.016508
2021-12-11 22:15:30,137 iteration 6755 : loss : 0.034401, loss_ce: 0.011192
2021-12-11 22:15:31,415 iteration 6756 : loss : 0.019252, loss_ce: 0.011257
2021-12-11 22:15:32,736 iteration 6757 : loss : 0.029606, loss_ce: 0.014413
2021-12-11 22:15:34,087 iteration 6758 : loss : 0.027144, loss_ce: 0.014348
2021-12-11 22:15:35,502 iteration 6759 : loss : 0.040563, loss_ce: 0.015269
2021-12-11 22:15:36,855 iteration 6760 : loss : 0.024193, loss_ce: 0.012080
2021-12-11 22:15:38,225 iteration 6761 : loss : 0.028713, loss_ce: 0.013607
2021-12-11 22:15:39,613 iteration 6762 : loss : 0.028032, loss_ce: 0.011616
2021-12-11 22:15:40,988 iteration 6763 : loss : 0.023711, loss_ce: 0.013268
2021-12-11 22:15:42,438 iteration 6764 : loss : 0.032000, loss_ce: 0.013526
2021-12-11 22:15:43,841 iteration 6765 : loss : 0.035578, loss_ce: 0.012355
2021-12-11 22:15:45,204 iteration 6766 : loss : 0.020090, loss_ce: 0.011021
100%|████████████████████████████▊| 398/400 [2:49:26<00:49, 24.82s/it]2021-12-11 22:15:46,697 iteration 6767 : loss : 0.038257, loss_ce: 0.020342
2021-12-11 22:15:48,052 iteration 6768 : loss : 0.021731, loss_ce: 0.009752
2021-12-11 22:15:49,408 iteration 6769 : loss : 0.021735, loss_ce: 0.009934
2021-12-11 22:15:50,799 iteration 6770 : loss : 0.039406, loss_ce: 0.013470
2021-12-11 22:15:52,156 iteration 6771 : loss : 0.027180, loss_ce: 0.013884
2021-12-11 22:15:53,546 iteration 6772 : loss : 0.041444, loss_ce: 0.021374
2021-12-11 22:15:54,859 iteration 6773 : loss : 0.020105, loss_ce: 0.010445
2021-12-11 22:15:56,218 iteration 6774 : loss : 0.035879, loss_ce: 0.014313
2021-12-11 22:15:57,554 iteration 6775 : loss : 0.052617, loss_ce: 0.015217
2021-12-11 22:15:58,919 iteration 6776 : loss : 0.027885, loss_ce: 0.012788
2021-12-11 22:16:00,302 iteration 6777 : loss : 0.031113, loss_ce: 0.014731
2021-12-11 22:16:01,630 iteration 6778 : loss : 0.024422, loss_ce: 0.011601
2021-12-11 22:16:02,985 iteration 6779 : loss : 0.030454, loss_ce: 0.018524
2021-12-11 22:16:04,399 iteration 6780 : loss : 0.025715, loss_ce: 0.011192
2021-12-11 22:16:05,785 iteration 6781 : loss : 0.029396, loss_ce: 0.013169
2021-12-11 22:16:07,144 iteration 6782 : loss : 0.030135, loss_ce: 0.015194
2021-12-11 22:16:08,570 iteration 6783 : loss : 0.038428, loss_ce: 0.014623
100%|████████████████████████████▉| 399/400 [2:49:49<00:24, 24.38s/it]2021-12-11 22:16:09,945 iteration 6784 : loss : 0.030090, loss_ce: 0.018655
2021-12-11 22:16:11,308 iteration 6785 : loss : 0.037043, loss_ce: 0.013728
2021-12-11 22:16:12,724 iteration 6786 : loss : 0.033424, loss_ce: 0.014898
2021-12-11 22:16:14,090 iteration 6787 : loss : 0.028823, loss_ce: 0.011469
2021-12-11 22:16:15,465 iteration 6788 : loss : 0.024193, loss_ce: 0.013483
2021-12-11 22:16:16,917 iteration 6789 : loss : 0.037555, loss_ce: 0.013941
2021-12-11 22:16:18,303 iteration 6790 : loss : 0.031758, loss_ce: 0.015183
2021-12-11 22:16:19,755 iteration 6791 : loss : 0.037430, loss_ce: 0.014543
2021-12-11 22:16:21,122 iteration 6792 : loss : 0.031524, loss_ce: 0.015815
2021-12-11 22:16:22,425 iteration 6793 : loss : 0.017258, loss_ce: 0.009238
2021-12-11 22:16:23,842 iteration 6794 : loss : 0.036645, loss_ce: 0.016894
2021-12-11 22:16:25,177 iteration 6795 : loss : 0.024479, loss_ce: 0.013110
2021-12-11 22:16:26,545 iteration 6796 : loss : 0.023989, loss_ce: 0.009320
2021-12-11 22:16:27,900 iteration 6797 : loss : 0.025939, loss_ce: 0.012135
2021-12-11 22:16:29,322 iteration 6798 : loss : 0.036756, loss_ce: 0.014779
2021-12-11 22:16:30,688 iteration 6799 : loss : 0.029130, loss_ce: 0.011754
2021-12-11 22:16:30,689 Training Data Eval:
2021-12-11 22:16:37,500   Average segmentation loss on training set: 0.0136
2021-12-11 22:16:37,501 Validation Data Eval:
2021-12-11 22:16:39,856   Average segmentation loss on validation set: 0.0875
2021-12-11 22:16:41,228 iteration 6800 : loss : 0.028566, loss_ce: 0.013759
2021-12-11 22:16:43,178 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed100epoch_399.pth
2021-12-11 22:16:45,105 save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/UNET_SGD_seed100epoch_399.pth
100%|████████████████████████████▉| 399/400 [2:50:26<00:25, 25.63s/it]
