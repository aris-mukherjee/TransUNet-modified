2021-12-10 10:15:48,518 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2021-12-10 10:15:48,519 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2021-12-10 10:15:48,519 ============================================================
2021-12-10 10:15:48,519 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2021-12-10 10:15:48,519 ============================================================
2021-12-10 10:15:48,519 Loading data...
2021-12-10 10:15:48,519 Reading NCI - RUNMC images...
2021-12-10 10:15:48,519 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2021-12-10 10:15:48,522 Already preprocessed this configuration. Loading now!
2021-12-10 10:15:48,543 Training Images: (256, 256, 286)
2021-12-10 10:15:48,544 Training Labels: (256, 256, 286)
2021-12-10 10:15:48,544 Validation Images: (256, 256, 98)
2021-12-10 10:15:48,544 Validation Labels: (256, 256, 98)
2021-12-10 10:15:48,544 ============================================================
2021-12-10 10:15:48,575 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2021-12-10 10:15:51,428 iteration 1 : loss : 0.902121, loss_ce: 1.072191
2021-12-10 10:15:52,874 iteration 2 : loss : 0.848945, loss_ce: 0.983815
2021-12-10 10:15:54,428 iteration 3 : loss : 0.792486, loss_ce: 0.889101
2021-12-10 10:15:55,895 iteration 4 : loss : 0.739587, loss_ce: 0.799797
2021-12-10 10:15:57,364 iteration 5 : loss : 0.687877, loss_ce: 0.729071
2021-12-10 10:15:58,848 iteration 6 : loss : 0.645884, loss_ce: 0.651259
2021-12-10 10:16:00,383 iteration 7 : loss : 0.610173, loss_ce: 0.603645
2021-12-10 10:16:01,878 iteration 8 : loss : 0.586407, loss_ce: 0.540477
2021-12-10 10:16:03,393 iteration 9 : loss : 0.548571, loss_ce: 0.536546
2021-12-10 10:16:04,935 iteration 10 : loss : 0.532529, loss_ce: 0.454587
2021-12-10 10:16:06,534 iteration 11 : loss : 0.492825, loss_ce: 0.429760
2021-12-10 10:16:08,021 iteration 12 : loss : 0.471869, loss_ce: 0.390369
2021-12-10 10:16:09,490 iteration 13 : loss : 0.468143, loss_ce: 0.367342
2021-12-10 10:16:10,933 iteration 14 : loss : 0.449263, loss_ce: 0.344603
2021-12-10 10:16:12,445 iteration 15 : loss : 0.405093, loss_ce: 0.303176
2021-12-10 10:16:13,944 iteration 16 : loss : 0.417983, loss_ce: 0.294427
2021-12-10 10:16:15,437 iteration 17 : loss : 0.366497, loss_ce: 0.263559
  0%|                               | 1/400 [00:26<2:59:12, 26.95s/it]2021-12-10 10:16:17,051 iteration 18 : loss : 0.400511, loss_ce: 0.241458
2021-12-10 10:16:18,487 iteration 19 : loss : 0.337167, loss_ce: 0.213371
2021-12-10 10:16:20,050 iteration 20 : loss : 0.329841, loss_ce: 0.195432
2021-12-10 10:16:21,525 iteration 21 : loss : 0.362814, loss_ce: 0.190054
2021-12-10 10:16:23,019 iteration 22 : loss : 0.316653, loss_ce: 0.192018
2021-12-10 10:16:24,588 iteration 23 : loss : 0.325740, loss_ce: 0.166497
2021-12-10 10:16:26,083 iteration 24 : loss : 0.311728, loss_ce: 0.170779
2021-12-10 10:16:27,634 iteration 25 : loss : 0.375662, loss_ce: 0.222930
2021-12-10 10:16:29,113 iteration 26 : loss : 0.289553, loss_ce: 0.145803
2021-12-10 10:16:30,540 iteration 27 : loss : 0.286871, loss_ce: 0.150764
2021-12-10 10:16:31,981 iteration 28 : loss : 0.284702, loss_ce: 0.140838
2021-12-10 10:16:33,515 iteration 29 : loss : 0.287249, loss_ce: 0.139453
2021-12-10 10:16:35,040 iteration 30 : loss : 0.277375, loss_ce: 0.139637
2021-12-10 10:16:36,485 iteration 31 : loss : 0.267019, loss_ce: 0.133791
2021-12-10 10:16:38,038 iteration 32 : loss : 0.282932, loss_ce: 0.153578
2021-12-10 10:16:39,574 iteration 33 : loss : 0.280380, loss_ce: 0.156677
2021-12-10 10:16:41,108 iteration 34 : loss : 0.284187, loss_ce: 0.158201
  0%|▏                              | 2/400 [00:52<2:53:40, 26.18s/it]2021-12-10 10:16:42,697 iteration 35 : loss : 0.256784, loss_ce: 0.119751
2021-12-10 10:16:44,252 iteration 36 : loss : 0.283573, loss_ce: 0.146814
2021-12-10 10:16:45,812 iteration 37 : loss : 0.283014, loss_ce: 0.127699
2021-12-10 10:16:47,290 iteration 38 : loss : 0.264059, loss_ce: 0.129884
2021-12-10 10:16:48,780 iteration 39 : loss : 0.237643, loss_ce: 0.110843
2021-12-10 10:16:50,343 iteration 40 : loss : 0.263559, loss_ce: 0.126316
2021-12-10 10:16:51,896 iteration 41 : loss : 0.334088, loss_ce: 0.163152
2021-12-10 10:16:53,429 iteration 42 : loss : 0.257360, loss_ce: 0.123805
2021-12-10 10:16:54,875 iteration 43 : loss : 0.260091, loss_ce: 0.119590
2021-12-10 10:16:56,457 iteration 44 : loss : 0.222719, loss_ce: 0.106398
2021-12-10 10:16:57,969 iteration 45 : loss : 0.239822, loss_ce: 0.113367
2021-12-10 10:16:59,492 iteration 46 : loss : 0.251085, loss_ce: 0.103496
2021-12-10 10:17:01,052 iteration 47 : loss : 0.200807, loss_ce: 0.077202
2021-12-10 10:17:02,583 iteration 48 : loss : 0.221858, loss_ce: 0.095758
2021-12-10 10:17:04,157 iteration 49 : loss : 0.312911, loss_ce: 0.150204
2021-12-10 10:17:05,636 iteration 50 : loss : 0.307963, loss_ce: 0.137211
2021-12-10 10:17:07,097 iteration 51 : loss : 0.250680, loss_ce: 0.118405
  1%|▏                              | 3/400 [01:18<2:52:38, 26.09s/it]2021-12-10 10:17:08,717 iteration 52 : loss : 0.297480, loss_ce: 0.151215
2021-12-10 10:17:10,261 iteration 53 : loss : 0.258408, loss_ce: 0.123129
2021-12-10 10:17:11,777 iteration 54 : loss : 0.256382, loss_ce: 0.116450
2021-12-10 10:17:13,307 iteration 55 : loss : 0.309558, loss_ce: 0.161680
2021-12-10 10:17:14,832 iteration 56 : loss : 0.264426, loss_ce: 0.118967
2021-12-10 10:17:16,368 iteration 57 : loss : 0.233046, loss_ce: 0.098885
2021-12-10 10:17:17,906 iteration 58 : loss : 0.294551, loss_ce: 0.122154
2021-12-10 10:17:19,411 iteration 59 : loss : 0.233820, loss_ce: 0.108100
2021-12-10 10:17:20,948 iteration 60 : loss : 0.281402, loss_ce: 0.115609
2021-12-10 10:17:22,478 iteration 61 : loss : 0.255327, loss_ce: 0.124169
2021-12-10 10:17:23,995 iteration 62 : loss : 0.341580, loss_ce: 0.131313
2021-12-10 10:17:25,439 iteration 63 : loss : 0.305704, loss_ce: 0.153683
2021-12-10 10:17:26,940 iteration 64 : loss : 0.309822, loss_ce: 0.140723
2021-12-10 10:17:28,401 iteration 65 : loss : 0.262312, loss_ce: 0.101432
2021-12-10 10:17:29,899 iteration 66 : loss : 0.217110, loss_ce: 0.089386
2021-12-10 10:17:31,469 iteration 67 : loss : 0.264799, loss_ce: 0.094715
2021-12-10 10:17:32,982 iteration 68 : loss : 0.255431, loss_ce: 0.111047
  1%|▎                              | 4/400 [01:44<2:51:39, 26.01s/it]2021-12-10 10:17:34,576 iteration 69 : loss : 0.231290, loss_ce: 0.097468
2021-12-10 10:17:36,172 iteration 70 : loss : 0.229245, loss_ce: 0.093510
2021-12-10 10:17:37,662 iteration 71 : loss : 0.240962, loss_ce: 0.104578
2021-12-10 10:17:39,184 iteration 72 : loss : 0.236463, loss_ce: 0.099255
2021-12-10 10:17:40,660 iteration 73 : loss : 0.245928, loss_ce: 0.116657
2021-12-10 10:17:42,107 iteration 74 : loss : 0.226251, loss_ce: 0.095365
2021-12-10 10:17:43,583 iteration 75 : loss : 0.220683, loss_ce: 0.094465
2021-12-10 10:17:45,055 iteration 76 : loss : 0.240538, loss_ce: 0.103073
2021-12-10 10:17:46,452 iteration 77 : loss : 0.218332, loss_ce: 0.094888
2021-12-10 10:17:47,947 iteration 78 : loss : 0.259258, loss_ce: 0.111725
2021-12-10 10:17:49,414 iteration 79 : loss : 0.260843, loss_ce: 0.095245
2021-12-10 10:17:50,870 iteration 80 : loss : 0.226055, loss_ce: 0.094588
2021-12-10 10:17:52,320 iteration 81 : loss : 0.236378, loss_ce: 0.094587
2021-12-10 10:17:53,794 iteration 82 : loss : 0.230607, loss_ce: 0.088511
2021-12-10 10:17:55,261 iteration 83 : loss : 0.254692, loss_ce: 0.093689
2021-12-10 10:17:56,821 iteration 84 : loss : 0.247289, loss_ce: 0.130740
2021-12-10 10:17:56,821 Training Data Eval:
2021-12-10 10:18:04,364   Average segmentation loss on training set: 0.6490
2021-12-10 10:18:04,364 Validation Data Eval:
2021-12-10 10:18:07,119   Average segmentation loss on validation set: 0.6157
2021-12-10 10:18:12,821 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/NEW_TU_ADAM_best_val_loss_seed1234.pth
2021-12-10 10:18:14,342 iteration 85 : loss : 0.274494, loss_ce: 0.114428
  1%|▍                              | 5/400 [02:25<3:27:38, 31.54s/it]2021-12-10 10:18:15,895 iteration 86 : loss : 0.288764, loss_ce: 0.109897
2021-12-10 10:18:17,517 iteration 87 : loss : 0.233651, loss_ce: 0.101154
2021-12-10 10:18:18,974 iteration 88 : loss : 0.239889, loss_ce: 0.103165
2021-12-10 10:18:20,505 iteration 89 : loss : 0.276691, loss_ce: 0.115551
2021-12-10 10:18:22,026 iteration 90 : loss : 0.230846, loss_ce: 0.088087
2021-12-10 10:18:23,606 iteration 91 : loss : 0.225327, loss_ce: 0.101389
2021-12-10 10:18:25,052 iteration 92 : loss : 0.224823, loss_ce: 0.094767
2021-12-10 10:18:26,543 iteration 93 : loss : 0.244557, loss_ce: 0.091459
2021-12-10 10:18:28,037 iteration 94 : loss : 0.246909, loss_ce: 0.099639
2021-12-10 10:18:29,654 iteration 95 : loss : 0.229535, loss_ce: 0.098589
2021-12-10 10:18:31,137 iteration 96 : loss : 0.229271, loss_ce: 0.100507
2021-12-10 10:18:32,621 iteration 97 : loss : 0.247934, loss_ce: 0.101874
2021-12-10 10:18:34,120 iteration 98 : loss : 0.248305, loss_ce: 0.105915
2021-12-10 10:18:35,698 iteration 99 : loss : 0.233557, loss_ce: 0.098438
2021-12-10 10:18:37,223 iteration 100 : loss : 0.230241, loss_ce: 0.091484
2021-12-10 10:18:38,728 iteration 101 : loss : 0.160742, loss_ce: 0.058023
2021-12-10 10:18:40,169 iteration 102 : loss : 0.220708, loss_ce: 0.081194
  2%|▍                              | 6/400 [02:51<3:14:24, 29.60s/it]2021-12-10 10:18:41,822 iteration 103 : loss : 0.243644, loss_ce: 0.097608
2021-12-10 10:18:43,399 iteration 104 : loss : 0.215056, loss_ce: 0.079738
2021-12-10 10:18:45,033 iteration 105 : loss : 0.261632, loss_ce: 0.105214
2021-12-10 10:18:46,507 iteration 106 : loss : 0.244086, loss_ce: 0.101748
2021-12-10 10:18:48,158 iteration 107 : loss : 0.224378, loss_ce: 0.101530
2021-12-10 10:18:49,611 iteration 108 : loss : 0.291931, loss_ce: 0.128761
2021-12-10 10:18:51,094 iteration 109 : loss : 0.227638, loss_ce: 0.105524
2021-12-10 10:18:52,528 iteration 110 : loss : 0.201294, loss_ce: 0.084413
2021-12-10 10:18:54,053 iteration 111 : loss : 0.260618, loss_ce: 0.122747
2021-12-10 10:18:55,496 iteration 112 : loss : 0.208303, loss_ce: 0.080936
2021-12-10 10:18:57,004 iteration 113 : loss : 0.284650, loss_ce: 0.145335
2021-12-10 10:18:58,480 iteration 114 : loss : 0.257362, loss_ce: 0.087755
2021-12-10 10:18:59,976 iteration 115 : loss : 0.210301, loss_ce: 0.084691
2021-12-10 10:19:01,524 iteration 116 : loss : 0.294545, loss_ce: 0.127487
2021-12-10 10:19:03,072 iteration 117 : loss : 0.223320, loss_ce: 0.091685
2021-12-10 10:19:04,587 iteration 118 : loss : 0.286739, loss_ce: 0.130455
2021-12-10 10:19:06,114 iteration 119 : loss : 0.209689, loss_ce: 0.086309
  2%|▌                              | 7/400 [03:17<3:06:04, 28.41s/it]2021-12-10 10:19:07,645 iteration 120 : loss : 0.301051, loss_ce: 0.131771
2021-12-10 10:19:09,095 iteration 121 : loss : 0.236938, loss_ce: 0.103675
2021-12-10 10:19:10,651 iteration 122 : loss : 0.278086, loss_ce: 0.119131
2021-12-10 10:19:12,174 iteration 123 : loss : 0.223866, loss_ce: 0.094918
2021-12-10 10:19:13,670 iteration 124 : loss : 0.236785, loss_ce: 0.091251
2021-12-10 10:19:15,130 iteration 125 : loss : 0.227549, loss_ce: 0.104794
2021-12-10 10:19:16,652 iteration 126 : loss : 0.232580, loss_ce: 0.083826
2021-12-10 10:19:18,104 iteration 127 : loss : 0.221666, loss_ce: 0.092364
2021-12-10 10:19:19,581 iteration 128 : loss : 0.213321, loss_ce: 0.086075
2021-12-10 10:19:21,130 iteration 129 : loss : 0.226775, loss_ce: 0.083396
2021-12-10 10:19:22,622 iteration 130 : loss : 0.224079, loss_ce: 0.085922
2021-12-10 10:19:24,207 iteration 131 : loss : 0.242165, loss_ce: 0.110099
2021-12-10 10:19:25,815 iteration 132 : loss : 0.199728, loss_ce: 0.058616
2021-12-10 10:19:27,298 iteration 133 : loss : 0.217529, loss_ce: 0.079117
2021-12-10 10:19:28,873 iteration 134 : loss : 0.243002, loss_ce: 0.102879
2021-12-10 10:19:30,460 iteration 135 : loss : 0.243702, loss_ce: 0.110792
2021-12-10 10:19:31,961 iteration 136 : loss : 0.191688, loss_ce: 0.083787
  2%|▌                              | 8/400 [03:43<3:00:14, 27.59s/it]2021-12-10 10:19:33,584 iteration 137 : loss : 0.233989, loss_ce: 0.085871
2021-12-10 10:19:35,067 iteration 138 : loss : 0.245310, loss_ce: 0.126100
2021-12-10 10:19:36,606 iteration 139 : loss : 0.261532, loss_ce: 0.108123
2021-12-10 10:19:38,115 iteration 140 : loss : 0.207784, loss_ce: 0.071879
2021-12-10 10:19:39,665 iteration 141 : loss : 0.218757, loss_ce: 0.101985
2021-12-10 10:19:41,229 iteration 142 : loss : 0.228901, loss_ce: 0.088806
2021-12-10 10:19:42,820 iteration 143 : loss : 0.242528, loss_ce: 0.100235
2021-12-10 10:19:44,380 iteration 144 : loss : 0.252396, loss_ce: 0.096568
2021-12-10 10:19:45,879 iteration 145 : loss : 0.220720, loss_ce: 0.082168
2021-12-10 10:19:47,425 iteration 146 : loss : 0.212811, loss_ce: 0.095766
2021-12-10 10:19:48,984 iteration 147 : loss : 0.212179, loss_ce: 0.087495
2021-12-10 10:19:50,412 iteration 148 : loss : 0.250411, loss_ce: 0.108173
2021-12-10 10:19:51,908 iteration 149 : loss : 0.279217, loss_ce: 0.126441
2021-12-10 10:19:53,383 iteration 150 : loss : 0.273671, loss_ce: 0.102611
2021-12-10 10:19:54,862 iteration 151 : loss : 0.216619, loss_ce: 0.095121
2021-12-10 10:19:56,347 iteration 152 : loss : 0.230569, loss_ce: 0.077001
2021-12-10 10:19:57,878 iteration 153 : loss : 0.251308, loss_ce: 0.110270
  2%|▋                              | 9/400 [04:09<2:56:24, 27.07s/it]2021-12-10 10:19:59,390 iteration 154 : loss : 0.267829, loss_ce: 0.113770
2021-12-10 10:20:00,907 iteration 155 : loss : 0.218858, loss_ce: 0.076694
2021-12-10 10:20:02,499 iteration 156 : loss : 0.207253, loss_ce: 0.075060
2021-12-10 10:20:04,025 iteration 157 : loss : 0.236343, loss_ce: 0.082531
2021-12-10 10:20:05,658 iteration 158 : loss : 0.215225, loss_ce: 0.090613
2021-12-10 10:20:07,061 iteration 159 : loss : 0.205519, loss_ce: 0.086543
2021-12-10 10:20:08,649 iteration 160 : loss : 0.216030, loss_ce: 0.073648
2021-12-10 10:20:10,225 iteration 161 : loss : 0.265889, loss_ce: 0.103264
2021-12-10 10:20:11,804 iteration 162 : loss : 0.220144, loss_ce: 0.103599
2021-12-10 10:20:13,317 iteration 163 : loss : 0.230288, loss_ce: 0.089179
2021-12-10 10:20:14,857 iteration 164 : loss : 0.218117, loss_ce: 0.075531
2021-12-10 10:20:16,357 iteration 165 : loss : 0.263237, loss_ce: 0.115105
2021-12-10 10:20:17,845 iteration 166 : loss : 0.230121, loss_ce: 0.088565
2021-12-10 10:20:19,349 iteration 167 : loss : 0.205155, loss_ce: 0.077521
2021-12-10 10:20:20,897 iteration 168 : loss : 0.279659, loss_ce: 0.138263
2021-12-10 10:20:22,405 iteration 169 : loss : 0.205170, loss_ce: 0.093068
2021-12-10 10:20:22,405 Training Data Eval:
2021-12-10 10:20:29,965   Average segmentation loss on training set: 0.2608
2021-12-10 10:20:29,965 Validation Data Eval:
2021-12-10 10:20:32,585   Average segmentation loss on validation set: 0.2416
2021-12-10 10:20:38,399 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/NEW_TU/NEW_TU_ADAM_best_val_loss_seed1234.pth
2021-12-10 10:20:39,996 iteration 170 : loss : 0.198809, loss_ce: 0.080664
  2%|▊                             | 10/400 [04:51<3:26:07, 31.71s/it]2021-12-10 10:20:41,577 iteration 171 : loss : 0.221176, loss_ce: 0.087595
2021-12-10 10:20:43,111 iteration 172 : loss : 0.235748, loss_ce: 0.100690
2021-12-10 10:20:44,666 iteration 173 : loss : 0.207076, loss_ce: 0.085145
2021-12-10 10:20:46,136 iteration 174 : loss : 0.235352, loss_ce: 0.099358
2021-12-10 10:20:47,592 iteration 175 : loss : 0.277133, loss_ce: 0.116593
2021-12-10 10:20:49,121 iteration 176 : loss : 0.207879, loss_ce: 0.093906
2021-12-10 10:20:50,620 iteration 177 : loss : 0.216416, loss_ce: 0.080595
2021-12-10 10:20:52,121 iteration 178 : loss : 0.177821, loss_ce: 0.075048
2021-12-10 10:20:53,621 iteration 179 : loss : 0.231568, loss_ce: 0.093469
2021-12-10 10:20:55,107 iteration 180 : loss : 0.201396, loss_ce: 0.071206
2021-12-10 10:20:56,639 iteration 181 : loss : 0.201801, loss_ce: 0.076360
2021-12-10 10:20:58,109 iteration 182 : loss : 0.187447, loss_ce: 0.068942
2021-12-10 10:20:59,566 iteration 183 : loss : 0.206624, loss_ce: 0.070251
2021-12-10 10:21:00,991 iteration 184 : loss : 0.211932, loss_ce: 0.071145
2021-12-10 10:21:02,523 iteration 185 : loss : 0.268809, loss_ce: 0.094771
2021-12-10 10:21:04,058 iteration 186 : loss : 0.242268, loss_ce: 0.105375
2021-12-10 10:21:05,628 iteration 187 : loss : 0.221997, loss_ce: 0.092246
  3%|▊                             | 11/400 [05:17<3:13:31, 29.85s/it]2021-12-10 10:21:07,216 iteration 188 : loss : 0.300033, loss_ce: 0.130902
2021-12-10 10:21:08,744 iteration 189 : loss : 0.205387, loss_ce: 0.083789
2021-12-10 10:21:10,276 iteration 190 : loss : 0.195469, loss_ce: 0.072112
2021-12-10 10:21:11,765 iteration 191 : loss : 0.180945, loss_ce: 0.068492
2021-12-10 10:21:13,208 iteration 192 : loss : 0.203790, loss_ce: 0.088298
2021-12-10 10:21:14,774 iteration 193 : loss : 0.228565, loss_ce: 0.098183
2021-12-10 10:21:16,271 iteration 194 : loss : 0.249306, loss_ce: 0.072720
2021-12-10 10:21:17,743 iteration 195 : loss : 0.235615, loss_ce: 0.095434
2021-12-10 10:21:19,177 iteration 196 : loss : 0.230306, loss_ce: 0.074605
2021-12-10 10:21:20,693 iteration 197 : loss : 0.266530, loss_ce: 0.102021
2021-12-10 10:21:22,234 iteration 198 : loss : 0.209250, loss_ce: 0.090574
2021-12-10 10:21:23,708 iteration 199 : loss : 0.230110, loss_ce: 0.095693
2021-12-10 10:21:25,200 iteration 200 : loss : 0.229642, loss_ce: 0.096594
2021-12-10 10:21:26,725 iteration 201 : loss : 0.170498, loss_ce: 0.072160
2021-12-10 10:21:28,267 iteration 202 : loss : 0.209916, loss_ce: 0.095860
2021-12-10 10:21:29,739 iteration 203 : loss : 0.185041, loss_ce: 0.072778
2021-12-10 10:21:31,340 iteration 204 : loss : 0.218239, loss_ce: 0.085042
  3%|▉                             | 12/400 [05:42<3:04:53, 28.59s/it]2021-12-10 10:21:32,869 iteration 205 : loss : 0.183616, loss_ce: 0.066582
2021-12-10 10:21:34,370 iteration 206 : loss : 0.184406, loss_ce: 0.056227
2021-12-10 10:21:35,895 iteration 207 : loss : 0.200002, loss_ce: 0.085127
2021-12-10 10:21:37,385 iteration 208 : loss : 0.238884, loss_ce: 0.078360
2021-12-10 10:21:38,967 iteration 209 : loss : 0.284816, loss_ce: 0.145774
2021-12-10 10:21:40,395 iteration 210 : loss : 0.185165, loss_ce: 0.068901
2021-12-10 10:21:41,938 iteration 211 : loss : 0.228435, loss_ce: 0.109721
2021-12-10 10:21:43,466 iteration 212 : loss : 0.207882, loss_ce: 0.081562
2021-12-10 10:21:45,086 iteration 213 : loss : 0.195430, loss_ce: 0.090164
2021-12-10 10:21:46,600 iteration 214 : loss : 0.203047, loss_ce: 0.066567
2021-12-10 10:21:48,113 iteration 215 : loss : 0.234944, loss_ce: 0.095457
2021-12-10 10:21:49,785 iteration 216 : loss : 0.222398, loss_ce: 0.083668
2021-12-10 10:21:51,390 iteration 217 : loss : 0.207569, loss_ce: 0.080799
2021-12-10 10:21:52,920 iteration 218 : loss : 0.197624, loss_ce: 0.088706
2021-12-10 10:21:54,341 iteration 219 : loss : 0.189660, loss_ce: 0.075408
2021-12-10 10:21:55,878 iteration 220 : loss : 0.179395, loss_ce: 0.077060
2021-12-10 10:21:57,399 iteration 221 : loss : 0.254560, loss_ce: 0.094938
  3%|▉                             | 13/400 [06:08<2:59:28, 27.83s/it]2021-12-10 10:21:58,988 iteration 222 : loss : 0.199699, loss_ce: 0.089213
2021-12-10 10:22:00,550 iteration 223 : loss : 0.180998, loss_ce: 0.074681
2021-12-10 10:22:02,062 iteration 224 : loss : 0.337420, loss_ce: 0.161581
2021-12-10 10:22:03,668 iteration 225 : loss : 0.308297, loss_ce: 0.097507
2021-12-10 10:22:05,176 iteration 226 : loss : 0.218530, loss_ce: 0.077975
2021-12-10 10:22:06,733 iteration 227 : loss : 0.222027, loss_ce: 0.098819
2021-12-10 10:22:08,250 iteration 228 : loss : 0.221295, loss_ce: 0.096964
2021-12-10 10:22:09,717 iteration 229 : loss : 0.209667, loss_ce: 0.072473
2021-12-10 10:22:11,240 iteration 230 : loss : 0.188013, loss_ce: 0.075369
2021-12-10 10:22:12,799 iteration 231 : loss : 0.272724, loss_ce: 0.107962
2021-12-10 10:22:14,351 iteration 232 : loss : 0.176797, loss_ce: 0.071681
2021-12-10 10:22:15,828 iteration 233 : loss : 0.269815, loss_ce: 0.116701
2021-12-10 10:22:17,408 iteration 234 : loss : 0.234558, loss_ce: 0.087018
2021-12-10 10:22:18,907 iteration 235 : loss : 0.215790, loss_ce: 0.091063
2021-12-10 10:22:20,455 iteration 236 : loss : 0.221296, loss_ce: 0.089089
2021-12-10 10:22:21,950 iteration 237 : loss : 0.187458, loss_ce: 0.074743
2021-12-10 10:22:23,459 iteration 238 : loss : 0.209357, loss_ce: 0.097506
  4%|█                             | 14/400 [06:34<2:55:35, 27.29s/it]2021-12-10 10:22:24,979 iteration 239 : loss : 0.214000, loss_ce: 0.081262
2021-12-10 10:22:26,524 iteration 240 : loss : 0.185376, loss_ce: 0.090769
2021-12-10 10:22:28,078 iteration 241 : loss : 0.262236, loss_ce: 0.106971
2021-12-10 10:22:29,610 iteration 242 : loss : 0.218394, loss_ce: 0.117233
2021-12-10 10:22:31,265 iteration 243 : loss : 0.264163, loss_ce: 0.110999
2021-12-10 10:22:32,738 iteration 244 : loss : 0.241324, loss_ce: 0.071344
2021-12-10 10:22:34,272 iteration 245 : loss : 0.197692, loss_ce: 0.091135
2021-12-10 10:22:35,843 iteration 246 : loss : 0.184962, loss_ce: 0.072451
2021-12-10 10:22:37,349 iteration 247 : loss : 0.240126, loss_ce: 0.085448
2021-12-10 10:22:38,854 iteration 248 : loss : 0.228941, loss_ce: 0.079776
2021-12-10 10:22:40,341 iteration 249 : loss : 0.246085, loss_ce: 0.131360
2021-12-10 10:22:41,847 iteration 250 : loss : 0.232207, loss_ce: 0.094018
2021-12-10 10:22:43,300 iteration 251 : loss : 0.232157, loss_ce: 0.079759
2021-12-10 10:22:44,802 iteration 252 : loss : 0.150409, loss_ce: 0.058021
2021-12-10 10:22:46,345 iteration 253 : loss : 0.200573, loss_ce: 0.069855
2021-12-10 10:22:47,938 iteration 254 : loss : 0.304586, loss_ce: 0.104424
2021-12-10 10:22:47,938 Training Data Eval:
2021-12-10 10:22:55,503   Average segmentation loss on training set: 1.1584
2021-12-10 10:22:55,503 Validation Data Eval:
2021-12-10 10:22:58,116   Average segmentation loss on validation set: 1.1335
2021-12-10 10:22:59,598 iteration 255 : loss : 0.222316, loss_ce: 0.086116
  4%|█▏                            | 15/400 [07:11<3:12:15, 29.96s/it]2021-12-10 10:23:01,198 iteration 256 : loss : 0.198346, loss_ce: 0.078563
2021-12-10 10:23:02,770 iteration 257 : loss : 0.202717, loss_ce: 0.078746
2021-12-10 10:23:04,384 iteration 258 : loss : 0.180897, loss_ce: 0.068109
2021-12-10 10:23:05,828 iteration 259 : loss : 0.159933, loss_ce: 0.069602
2021-12-10 10:23:07,404 iteration 260 : loss : 0.181488, loss_ce: 0.076418
2021-12-10 10:23:08,943 iteration 261 : loss : 0.240077, loss_ce: 0.083392
2021-12-10 10:23:10,565 iteration 262 : loss : 0.189692, loss_ce: 0.074146
2021-12-10 10:23:12,084 iteration 263 : loss : 0.204652, loss_ce: 0.079248
2021-12-10 10:23:13,633 iteration 264 : loss : 0.253937, loss_ce: 0.124899
2021-12-10 10:23:15,121 iteration 265 : loss : 0.169725, loss_ce: 0.070021
2021-12-10 10:23:16,674 iteration 266 : loss : 0.203170, loss_ce: 0.094435
2021-12-10 10:23:18,187 iteration 267 : loss : 0.237281, loss_ce: 0.083241
2021-12-10 10:23:19,714 iteration 268 : loss : 0.229463, loss_ce: 0.106966
2021-12-10 10:23:21,428 iteration 269 : loss : 0.255172, loss_ce: 0.091845
2021-12-10 10:23:23,051 iteration 270 : loss : 0.181226, loss_ce: 0.071527
2021-12-10 10:23:24,457 iteration 271 : loss : 0.195799, loss_ce: 0.057634
2021-12-10 10:23:25,984 iteration 272 : loss : 0.208466, loss_ce: 0.099263
  4%|█▏                            | 16/400 [07:37<3:04:51, 28.88s/it]2021-12-10 10:23:27,512 iteration 273 : loss : 0.194438, loss_ce: 0.064689
2021-12-10 10:23:28,972 iteration 274 : loss : 0.146060, loss_ce: 0.055353
2021-12-10 10:23:30,437 iteration 275 : loss : 0.162752, loss_ce: 0.064443
2021-12-10 10:23:31,956 iteration 276 : loss : 0.165619, loss_ce: 0.073983
2021-12-10 10:23:33,497 iteration 277 : loss : 0.166534, loss_ce: 0.063839
2021-12-10 10:23:34,897 iteration 278 : loss : 0.217966, loss_ce: 0.080211
2021-12-10 10:23:36,360 iteration 279 : loss : 0.228476, loss_ce: 0.086575
2021-12-10 10:23:37,818 iteration 280 : loss : 0.203022, loss_ce: 0.088913
2021-12-10 10:23:39,334 iteration 281 : loss : 0.221306, loss_ce: 0.099242
2021-12-10 10:23:40,826 iteration 282 : loss : 0.253326, loss_ce: 0.098689
2021-12-10 10:23:42,301 iteration 283 : loss : 0.235167, loss_ce: 0.113410
2021-12-10 10:23:43,870 iteration 284 : loss : 0.236613, loss_ce: 0.112475
2021-12-10 10:23:45,324 iteration 285 : loss : 0.211611, loss_ce: 0.072459
2021-12-10 10:23:46,800 iteration 286 : loss : 0.184830, loss_ce: 0.075772
2021-12-10 10:23:48,385 iteration 287 : loss : 0.198027, loss_ce: 0.088850
2021-12-10 10:23:49,956 iteration 288 : loss : 0.207642, loss_ce: 0.082270
2021-12-10 10:23:51,447 iteration 289 : loss : 0.183853, loss_ce: 0.065632
  4%|█▎                            | 17/400 [08:02<2:57:48, 27.85s/it]2021-12-10 10:23:53,026 iteration 290 : loss : 0.168600, loss_ce: 0.065290
2021-12-10 10:23:54,501 iteration 291 : loss : 0.226891, loss_ce: 0.084682
2021-12-10 10:23:55,988 iteration 292 : loss : 0.161456, loss_ce: 0.066105
2021-12-10 10:23:57,480 iteration 293 : loss : 0.249427, loss_ce: 0.105151
2021-12-10 10:23:58,998 iteration 294 : loss : 0.191181, loss_ce: 0.077992
