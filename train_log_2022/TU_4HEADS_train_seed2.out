2022-01-11 22:12:29,170 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:12:29,170 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:12:29,170 ============================================================
2022-01-11 22:12:29,170 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:12:29,170 ============================================================
2022-01-11 22:12:29,171 Loading data...
2022-01-11 22:12:29,171 Reading NCI - RUNMC images...
2022-01-11 22:12:29,171 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-11 22:12:29,174 Already preprocessed this configuration. Loading now!
2022-01-11 22:12:29,196 Training Images: (256, 256, 286)
2022-01-11 22:12:29,196 Training Labels: (256, 256, 286)
2022-01-11 22:12:29,196 Validation Images: (256, 256, 98)
2022-01-11 22:12:29,196 Validation Labels: (256, 256, 98)
2022-01-11 22:12:29,196 ============================================================
2022-01-11 22:12:29,246 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-11 22:12:32,141 iteration 1 : loss : 0.925015, loss_ce: 1.121801
2022-01-11 22:12:33,545 iteration 2 : loss : 0.862125, loss_ce: 1.028954
2022-01-11 22:12:35,030 iteration 3 : loss : 0.804092, loss_ce: 0.937565
2022-01-11 22:12:36,462 iteration 4 : loss : 0.764920, loss_ce: 0.846447
2022-01-11 22:12:37,833 iteration 5 : loss : 0.725404, loss_ce: 0.768954
2022-01-11 22:12:39,251 iteration 6 : loss : 0.679852, loss_ce: 0.702703
2022-01-11 22:12:40,710 iteration 7 : loss : 0.637096, loss_ce: 0.644182
2022-01-11 22:12:42,223 iteration 8 : loss : 0.605770, loss_ce: 0.588706
2022-01-11 22:12:43,611 iteration 9 : loss : 0.587860, loss_ce: 0.540625
2022-01-11 22:12:45,002 iteration 10 : loss : 0.546740, loss_ce: 0.494640
2022-01-11 22:12:46,376 iteration 11 : loss : 0.526765, loss_ce: 0.445074
2022-01-11 22:12:47,802 iteration 12 : loss : 0.499537, loss_ce: 0.419812
2022-01-11 22:12:49,285 iteration 13 : loss : 0.463377, loss_ce: 0.394649
2022-01-11 22:12:50,678 iteration 14 : loss : 0.452786, loss_ce: 0.363707
2022-01-11 22:12:52,166 iteration 15 : loss : 0.428232, loss_ce: 0.326664
2022-01-11 22:12:53,602 iteration 16 : loss : 0.438748, loss_ce: 0.313030
2022-01-11 22:12:55,033 iteration 17 : loss : 0.413899, loss_ce: 0.300737
  0%|                               | 1/400 [00:25<2:51:54, 25.85s/it]2022-01-11 22:12:56,527 iteration 18 : loss : 0.382957, loss_ce: 0.250555
2022-01-11 22:12:58,353 iteration 19 : loss : 0.379039, loss_ce: 0.243395
2022-01-11 22:12:59,792 iteration 20 : loss : 0.353935, loss_ce: 0.224504
2022-01-11 22:13:01,316 iteration 21 : loss : 0.357215, loss_ce: 0.217162
2022-01-11 22:13:02,750 iteration 22 : loss : 0.353361, loss_ce: 0.205964
2022-01-11 22:13:04,213 iteration 23 : loss : 0.322378, loss_ce: 0.177581
2022-01-11 22:13:05,607 iteration 24 : loss : 0.332530, loss_ce: 0.193576
2022-01-11 22:13:06,964 iteration 25 : loss : 0.318320, loss_ce: 0.170211
2022-01-11 22:13:08,365 iteration 26 : loss : 0.341172, loss_ce: 0.176549
2022-01-11 22:13:09,818 iteration 27 : loss : 0.295427, loss_ce: 0.156127
2022-01-11 22:13:11,247 iteration 28 : loss : 0.313428, loss_ce: 0.151549
2022-01-11 22:13:12,825 iteration 29 : loss : 0.314901, loss_ce: 0.160345
2022-01-11 22:13:14,317 iteration 30 : loss : 0.298563, loss_ce: 0.138900
2022-01-11 22:13:15,721 iteration 31 : loss : 0.293827, loss_ce: 0.148530
2022-01-11 22:13:17,151 iteration 32 : loss : 0.299077, loss_ce: 0.145610
2022-01-11 22:13:18,680 iteration 33 : loss : 0.305438, loss_ce: 0.158199
2022-01-11 22:13:20,112 iteration 34 : loss : 0.296825, loss_ce: 0.125323
  0%|▏                              | 2/400 [00:50<2:48:21, 25.38s/it]2022-01-11 22:13:21,650 iteration 35 : loss : 0.287463, loss_ce: 0.145659
2022-01-11 22:13:23,118 iteration 36 : loss : 0.295764, loss_ce: 0.139645
2022-01-11 22:13:24,618 iteration 37 : loss : 0.287273, loss_ce: 0.149362
2022-01-11 22:13:26,183 iteration 38 : loss : 0.262099, loss_ce: 0.111902
2022-01-11 22:13:27,648 iteration 39 : loss : 0.329286, loss_ce: 0.135964
2022-01-11 22:13:29,148 iteration 40 : loss : 0.298091, loss_ce: 0.154492
2022-01-11 22:13:30,552 iteration 41 : loss : 0.283411, loss_ce: 0.111944
2022-01-11 22:13:32,133 iteration 42 : loss : 0.268934, loss_ce: 0.122288
2022-01-11 22:13:33,533 iteration 43 : loss : 0.278065, loss_ce: 0.098945
2022-01-11 22:13:35,022 iteration 44 : loss : 0.281997, loss_ce: 0.112533
2022-01-11 22:13:36,584 iteration 45 : loss : 0.296968, loss_ce: 0.116941
2022-01-11 22:13:38,070 iteration 46 : loss : 0.275833, loss_ce: 0.108025
2022-01-11 22:13:39,627 iteration 47 : loss : 0.285451, loss_ce: 0.096766
2022-01-11 22:13:41,223 iteration 48 : loss : 0.278119, loss_ce: 0.126741
2022-01-11 22:13:42,787 iteration 49 : loss : 0.282340, loss_ce: 0.105073
2022-01-11 22:13:44,389 iteration 50 : loss : 0.255542, loss_ce: 0.098686
2022-01-11 22:13:45,951 iteration 51 : loss : 0.273757, loss_ce: 0.128630
  1%|▏                              | 3/400 [01:16<2:49:19, 25.59s/it]2022-01-11 22:13:47,468 iteration 52 : loss : 0.272079, loss_ce: 0.121699
2022-01-11 22:13:48,940 iteration 53 : loss : 0.268133, loss_ce: 0.120235
2022-01-11 22:13:50,518 iteration 54 : loss : 0.225128, loss_ce: 0.097112
2022-01-11 22:13:52,085 iteration 55 : loss : 0.335026, loss_ce: 0.135983
2022-01-11 22:13:53,665 iteration 56 : loss : 0.248660, loss_ce: 0.095982
2022-01-11 22:13:55,245 iteration 57 : loss : 0.246909, loss_ce: 0.095771
2022-01-11 22:13:56,812 iteration 58 : loss : 0.263978, loss_ce: 0.114865
2022-01-11 22:13:58,365 iteration 59 : loss : 0.284147, loss_ce: 0.116368
2022-01-11 22:13:59,992 iteration 60 : loss : 0.276662, loss_ce: 0.108964
2022-01-11 22:14:01,602 iteration 61 : loss : 0.220189, loss_ce: 0.093387
2022-01-11 22:14:03,144 iteration 62 : loss : 0.256696, loss_ce: 0.107705
2022-01-11 22:14:04,642 iteration 63 : loss : 0.229011, loss_ce: 0.110817
2022-01-11 22:14:06,196 iteration 64 : loss : 0.278274, loss_ce: 0.151904
2022-01-11 22:14:07,726 iteration 65 : loss : 0.257085, loss_ce: 0.108109
2022-01-11 22:14:09,310 iteration 66 : loss : 0.246606, loss_ce: 0.108694
2022-01-11 22:14:10,817 iteration 67 : loss : 0.266003, loss_ce: 0.121525
2022-01-11 22:14:12,363 iteration 68 : loss : 0.269504, loss_ce: 0.102408
  1%|▎                              | 4/400 [01:43<2:51:02, 25.92s/it]2022-01-11 22:14:13,968 iteration 69 : loss : 0.263338, loss_ce: 0.093957
2022-01-11 22:14:15,534 iteration 70 : loss : 0.303913, loss_ce: 0.136232
2022-01-11 22:14:17,039 iteration 71 : loss : 0.238011, loss_ce: 0.095125
2022-01-11 22:14:18,641 iteration 72 : loss : 0.247073, loss_ce: 0.091470
2022-01-11 22:14:20,185 iteration 73 : loss : 0.240439, loss_ce: 0.117500
2022-01-11 22:14:21,819 iteration 74 : loss : 0.244295, loss_ce: 0.105320
2022-01-11 22:14:23,392 iteration 75 : loss : 0.265662, loss_ce: 0.136111
2022-01-11 22:14:25,000 iteration 76 : loss : 0.261245, loss_ce: 0.116155
2022-01-11 22:14:26,641 iteration 77 : loss : 0.236071, loss_ce: 0.092347
2022-01-11 22:14:28,246 iteration 78 : loss : 0.190631, loss_ce: 0.089804
2022-01-11 22:14:29,757 iteration 79 : loss : 0.194826, loss_ce: 0.072434
2022-01-11 22:14:31,374 iteration 80 : loss : 0.305491, loss_ce: 0.118131
2022-01-11 22:14:32,924 iteration 81 : loss : 0.225442, loss_ce: 0.084338
2022-01-11 22:14:34,492 iteration 82 : loss : 0.274352, loss_ce: 0.128536
2022-01-11 22:14:36,117 iteration 83 : loss : 0.220423, loss_ce: 0.084007
2022-01-11 22:14:37,662 iteration 84 : loss : 0.250401, loss_ce: 0.110667
2022-01-11 22:14:37,663 Training Data Eval:
2022-01-11 22:14:45,663   Average segmentation loss on training set: 2.3661
2022-01-11 22:14:45,664 Validation Data Eval:
2022-01-11 22:14:48,553   Average segmentation loss on validation set: 2.3087
2022-01-11 22:14:50,155 iteration 85 : loss : 0.209325, loss_ce: 0.091494
  1%|▍                              | 5/400 [02:20<3:18:47, 30.20s/it]2022-01-11 22:14:51,740 iteration 86 : loss : 0.295608, loss_ce: 0.121640
2022-01-11 22:14:53,221 iteration 87 : loss : 0.231716, loss_ce: 0.101086
2022-01-11 22:14:54,755 iteration 88 : loss : 0.222102, loss_ce: 0.087729
2022-01-11 22:14:56,371 iteration 89 : loss : 0.209025, loss_ce: 0.090310
2022-01-11 22:14:57,895 iteration 90 : loss : 0.199879, loss_ce: 0.085527
2022-01-11 22:14:59,475 iteration 91 : loss : 0.270218, loss_ce: 0.112079
2022-01-11 22:15:01,025 iteration 92 : loss : 0.204700, loss_ce: 0.082954
2022-01-11 22:15:02,650 iteration 93 : loss : 0.233151, loss_ce: 0.100322
2022-01-11 22:15:04,282 iteration 94 : loss : 0.221532, loss_ce: 0.110272
2022-01-11 22:15:05,834 iteration 95 : loss : 0.227959, loss_ce: 0.090712
2022-01-11 22:15:07,347 iteration 96 : loss : 0.207545, loss_ce: 0.075930
2022-01-11 22:15:08,858 iteration 97 : loss : 0.195714, loss_ce: 0.077572
2022-01-11 22:15:10,410 iteration 98 : loss : 0.237263, loss_ce: 0.095300
2022-01-11 22:15:12,090 iteration 99 : loss : 0.217997, loss_ce: 0.087854
2022-01-11 22:15:13,771 iteration 100 : loss : 0.236272, loss_ce: 0.093492
2022-01-11 22:15:15,303 iteration 101 : loss : 0.270230, loss_ce: 0.127822
2022-01-11 22:15:16,871 iteration 102 : loss : 0.214642, loss_ce: 0.076397
  2%|▍                              | 6/400 [02:47<3:10:32, 29.02s/it]2022-01-11 22:15:18,498 iteration 103 : loss : 0.228174, loss_ce: 0.091502
2022-01-11 22:15:20,113 iteration 104 : loss : 0.242771, loss_ce: 0.090495
2022-01-11 22:15:21,694 iteration 105 : loss : 0.213737, loss_ce: 0.088103
2022-01-11 22:15:23,282 iteration 106 : loss : 0.226407, loss_ce: 0.085540
2022-01-11 22:15:24,931 iteration 107 : loss : 0.208876, loss_ce: 0.089783
2022-01-11 22:15:26,490 iteration 108 : loss : 0.230405, loss_ce: 0.107626
2022-01-11 22:15:28,073 iteration 109 : loss : 0.173801, loss_ce: 0.068907
2022-01-11 22:15:29,680 iteration 110 : loss : 0.248156, loss_ce: 0.115968
2022-01-11 22:15:31,412 iteration 111 : loss : 0.184386, loss_ce: 0.074176
2022-01-11 22:15:32,958 iteration 112 : loss : 0.253263, loss_ce: 0.093441
2022-01-11 22:15:34,631 iteration 113 : loss : 0.177511, loss_ce: 0.066180
2022-01-11 22:15:36,219 iteration 114 : loss : 0.189130, loss_ce: 0.061757
2022-01-11 22:15:37,727 iteration 115 : loss : 0.181755, loss_ce: 0.064172
2022-01-11 22:15:39,299 iteration 116 : loss : 0.239516, loss_ce: 0.096514
2022-01-11 22:15:40,846 iteration 117 : loss : 0.184675, loss_ce: 0.067543
2022-01-11 22:15:42,480 iteration 118 : loss : 0.224374, loss_ce: 0.083395
2022-01-11 22:15:44,027 iteration 119 : loss : 0.237968, loss_ce: 0.097167
  2%|▌                              | 7/400 [03:14<3:06:03, 28.41s/it]2022-01-11 22:15:45,686 iteration 120 : loss : 0.230865, loss_ce: 0.106365
2022-01-11 22:15:47,296 iteration 121 : loss : 0.248519, loss_ce: 0.095232
2022-01-11 22:15:48,780 iteration 122 : loss : 0.188021, loss_ce: 0.071328
2022-01-11 22:15:50,357 iteration 123 : loss : 0.259414, loss_ce: 0.109939
2022-01-11 22:15:51,911 iteration 124 : loss : 0.221340, loss_ce: 0.078458
2022-01-11 22:15:53,470 iteration 125 : loss : 0.212550, loss_ce: 0.073928
2022-01-11 22:15:55,023 iteration 126 : loss : 0.171512, loss_ce: 0.065996
2022-01-11 22:15:56,595 iteration 127 : loss : 0.224206, loss_ce: 0.093309
2022-01-11 22:15:58,141 iteration 128 : loss : 0.206448, loss_ce: 0.077086
2022-01-11 22:15:59,764 iteration 129 : loss : 0.276235, loss_ce: 0.122861
2022-01-11 22:16:01,239 iteration 130 : loss : 0.201719, loss_ce: 0.079852
2022-01-11 22:16:02,788 iteration 131 : loss : 0.206608, loss_ce: 0.093091
2022-01-11 22:16:04,367 iteration 132 : loss : 0.245793, loss_ce: 0.103389
2022-01-11 22:16:05,945 iteration 133 : loss : 0.269042, loss_ce: 0.149697
2022-01-11 22:16:07,502 iteration 134 : loss : 0.181975, loss_ce: 0.066893
2022-01-11 22:16:09,049 iteration 135 : loss : 0.205547, loss_ce: 0.070304
2022-01-11 22:16:10,689 iteration 136 : loss : 0.188713, loss_ce: 0.081865
  2%|▌                              | 8/400 [03:41<3:01:57, 27.85s/it]2022-01-11 22:16:12,362 iteration 137 : loss : 0.202954, loss_ce: 0.070118
2022-01-11 22:16:13,882 iteration 138 : loss : 0.175355, loss_ce: 0.063909
2022-01-11 22:16:15,537 iteration 139 : loss : 0.210096, loss_ce: 0.058951
2022-01-11 22:16:17,105 iteration 140 : loss : 0.275086, loss_ce: 0.114449
2022-01-11 22:16:18,636 iteration 141 : loss : 0.232734, loss_ce: 0.087869
2022-01-11 22:16:20,193 iteration 142 : loss : 0.207910, loss_ce: 0.073341
2022-01-11 22:16:21,773 iteration 143 : loss : 0.232762, loss_ce: 0.083011
2022-01-11 22:16:23,325 iteration 144 : loss : 0.202057, loss_ce: 0.065105
2022-01-11 22:16:24,909 iteration 145 : loss : 0.206911, loss_ce: 0.100637
2022-01-11 22:16:26,524 iteration 146 : loss : 0.155887, loss_ce: 0.063608
2022-01-11 22:16:28,111 iteration 147 : loss : 0.247634, loss_ce: 0.114799
2022-01-11 22:16:29,769 iteration 148 : loss : 0.193095, loss_ce: 0.082847
2022-01-11 22:16:31,410 iteration 149 : loss : 0.206782, loss_ce: 0.082780
2022-01-11 22:16:33,054 iteration 150 : loss : 0.197382, loss_ce: 0.079944
2022-01-11 22:16:34,594 iteration 151 : loss : 0.187106, loss_ce: 0.095226
2022-01-11 22:16:36,082 iteration 152 : loss : 0.150654, loss_ce: 0.064301
2022-01-11 22:16:37,634 iteration 153 : loss : 0.178943, loss_ce: 0.072346
  2%|▋                              | 9/400 [04:08<2:59:38, 27.57s/it]2022-01-11 22:16:39,281 iteration 154 : loss : 0.216819, loss_ce: 0.092813
2022-01-11 22:16:40,850 iteration 155 : loss : 0.178056, loss_ce: 0.083147
2022-01-11 22:16:42,384 iteration 156 : loss : 0.190666, loss_ce: 0.074080
2022-01-11 22:16:43,924 iteration 157 : loss : 0.186519, loss_ce: 0.074038
2022-01-11 22:16:45,559 iteration 158 : loss : 0.210506, loss_ce: 0.077147
2022-01-11 22:16:47,192 iteration 159 : loss : 0.191709, loss_ce: 0.069717
2022-01-11 22:16:48,746 iteration 160 : loss : 0.202709, loss_ce: 0.075491
2022-01-11 22:16:50,396 iteration 161 : loss : 0.183653, loss_ce: 0.078202
2022-01-11 22:16:51,960 iteration 162 : loss : 0.187270, loss_ce: 0.063174
2022-01-11 22:16:53,545 iteration 163 : loss : 0.236805, loss_ce: 0.103891
2022-01-11 22:16:55,112 iteration 164 : loss : 0.189900, loss_ce: 0.065394
2022-01-11 22:16:56,623 iteration 165 : loss : 0.160605, loss_ce: 0.060604
2022-01-11 22:16:58,231 iteration 166 : loss : 0.247158, loss_ce: 0.132398
2022-01-11 22:16:59,902 iteration 167 : loss : 0.288404, loss_ce: 0.133600
2022-01-11 22:17:01,468 iteration 168 : loss : 0.232125, loss_ce: 0.083262
2022-01-11 22:17:03,056 iteration 169 : loss : 0.182696, loss_ce: 0.071088
2022-01-11 22:17:03,057 Training Data Eval:
2022-01-11 22:17:11,057   Average segmentation loss on training set: 0.4065
2022-01-11 22:17:11,058 Validation Data Eval:
2022-01-11 22:17:13,814   Average segmentation loss on validation set: 0.3660
2022-01-11 22:17:19,747 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-11 22:17:21,293 iteration 170 : loss : 0.184220, loss_ce: 0.074126
  2%|▊                             | 10/400 [04:52<3:31:28, 32.53s/it]2022-01-11 22:17:22,847 iteration 171 : loss : 0.163353, loss_ce: 0.071740
2022-01-11 22:17:24,257 iteration 172 : loss : 0.165781, loss_ce: 0.074818
2022-01-11 22:17:25,772 iteration 173 : loss : 0.194374, loss_ce: 0.067135
2022-01-11 22:17:27,153 iteration 174 : loss : 0.196378, loss_ce: 0.097159
2022-01-11 22:17:28,584 iteration 175 : loss : 0.192179, loss_ce: 0.060162
2022-01-11 22:17:30,012 iteration 176 : loss : 0.158905, loss_ce: 0.060653
2022-01-11 22:17:31,502 iteration 177 : loss : 0.152945, loss_ce: 0.057261
2022-01-11 22:17:32,987 iteration 178 : loss : 0.181569, loss_ce: 0.066122
2022-01-11 22:17:34,484 iteration 179 : loss : 0.189549, loss_ce: 0.068462
2022-01-11 22:17:36,145 iteration 180 : loss : 0.216563, loss_ce: 0.098975
2022-01-11 22:17:37,761 iteration 181 : loss : 0.184412, loss_ce: 0.079250
2022-01-11 22:17:39,359 iteration 182 : loss : 0.197007, loss_ce: 0.090586
2022-01-11 22:17:40,956 iteration 183 : loss : 0.209842, loss_ce: 0.071780
2022-01-11 22:17:42,682 iteration 184 : loss : 0.227746, loss_ce: 0.081898
2022-01-11 22:17:44,373 iteration 185 : loss : 0.214391, loss_ce: 0.092528
2022-01-11 22:17:45,938 iteration 186 : loss : 0.191725, loss_ce: 0.070345
2022-01-11 22:17:47,526 iteration 187 : loss : 0.156713, loss_ce: 0.065382
  3%|▊                             | 11/400 [05:18<3:18:25, 30.61s/it]2022-01-11 22:17:49,110 iteration 188 : loss : 0.188836, loss_ce: 0.073676
2022-01-11 22:17:50,769 iteration 189 : loss : 0.196523, loss_ce: 0.081368
2022-01-11 22:17:52,380 iteration 190 : loss : 0.230489, loss_ce: 0.109230
2022-01-11 22:17:53,988 iteration 191 : loss : 0.184747, loss_ce: 0.078668
2022-01-11 22:17:55,570 iteration 192 : loss : 0.197092, loss_ce: 0.073683
2022-01-11 22:17:57,086 iteration 193 : loss : 0.163116, loss_ce: 0.076089
2022-01-11 22:17:58,687 iteration 194 : loss : 0.159331, loss_ce: 0.068885
2022-01-11 22:18:00,308 iteration 195 : loss : 0.157517, loss_ce: 0.058825
2022-01-11 22:18:01,871 iteration 196 : loss : 0.155965, loss_ce: 0.069717
2022-01-11 22:18:03,473 iteration 197 : loss : 0.160960, loss_ce: 0.057064
2022-01-11 22:18:05,064 iteration 198 : loss : 0.163018, loss_ce: 0.056498
2022-01-11 22:18:06,557 iteration 199 : loss : 0.219650, loss_ce: 0.069776
2022-01-11 22:18:08,123 iteration 200 : loss : 0.193509, loss_ce: 0.096502
2022-01-11 22:18:09,814 iteration 201 : loss : 0.169302, loss_ce: 0.077173
2022-01-11 22:18:11,365 iteration 202 : loss : 0.135636, loss_ce: 0.058566
2022-01-11 22:18:12,896 iteration 203 : loss : 0.247221, loss_ce: 0.135914
2022-01-11 22:18:14,394 iteration 204 : loss : 0.138947, loss_ce: 0.062568
  3%|▉                             | 12/400 [05:45<3:10:34, 29.47s/it]2022-01-11 22:18:15,948 iteration 205 : loss : 0.187189, loss_ce: 0.087727
2022-01-11 22:18:17,541 iteration 206 : loss : 0.220046, loss_ce: 0.099552
2022-01-11 22:18:19,205 iteration 207 : loss : 0.186630, loss_ce: 0.093182
2022-01-11 22:18:20,865 iteration 208 : loss : 0.145228, loss_ce: 0.058056
2022-01-11 22:18:22,405 iteration 209 : loss : 0.155317, loss_ce: 0.060773
2022-01-11 22:18:23,918 iteration 210 : loss : 0.145398, loss_ce: 0.060543
2022-01-11 22:18:25,487 iteration 211 : loss : 0.164985, loss_ce: 0.066921
2022-01-11 22:18:26,978 iteration 212 : loss : 0.217590, loss_ce: 0.080849
2022-01-11 22:18:28,594 iteration 213 : loss : 0.181445, loss_ce: 0.080389
2022-01-11 22:18:30,186 iteration 214 : loss : 0.224724, loss_ce: 0.069654
2022-01-11 22:18:31,702 iteration 215 : loss : 0.184496, loss_ce: 0.087124
2022-01-11 22:18:33,324 iteration 216 : loss : 0.244530, loss_ce: 0.102536
2022-01-11 22:18:34,866 iteration 217 : loss : 0.164678, loss_ce: 0.075140
2022-01-11 22:18:36,412 iteration 218 : loss : 0.128733, loss_ce: 0.052706
2022-01-11 22:18:37,965 iteration 219 : loss : 0.178989, loss_ce: 0.065804
2022-01-11 22:18:39,489 iteration 220 : loss : 0.182215, loss_ce: 0.087871
2022-01-11 22:18:41,041 iteration 221 : loss : 0.178451, loss_ce: 0.080713
  3%|▉                             | 13/400 [06:11<3:04:34, 28.62s/it]2022-01-11 22:18:42,752 iteration 222 : loss : 0.214566, loss_ce: 0.084527
2022-01-11 22:18:44,371 iteration 223 : loss : 0.178201, loss_ce: 0.064901
2022-01-11 22:18:45,862 iteration 224 : loss : 0.190930, loss_ce: 0.081077
2022-01-11 22:18:47,467 iteration 225 : loss : 0.176307, loss_ce: 0.069980
2022-01-11 22:18:49,090 iteration 226 : loss : 0.233615, loss_ce: 0.085833
2022-01-11 22:18:50,709 iteration 227 : loss : 0.199503, loss_ce: 0.081409
2022-01-11 22:18:52,297 iteration 228 : loss : 0.207342, loss_ce: 0.081782
2022-01-11 22:18:53,874 iteration 229 : loss : 0.179681, loss_ce: 0.083880
2022-01-11 22:18:55,444 iteration 230 : loss : 0.212714, loss_ce: 0.092241
2022-01-11 22:18:56,989 iteration 231 : loss : 0.250907, loss_ce: 0.091854
2022-01-11 22:18:58,634 iteration 232 : loss : 0.158199, loss_ce: 0.085061
2022-01-11 22:19:00,268 iteration 233 : loss : 0.172889, loss_ce: 0.082624
2022-01-11 22:19:01,870 iteration 234 : loss : 0.180583, loss_ce: 0.055241
2022-01-11 22:19:03,436 iteration 235 : loss : 0.160978, loss_ce: 0.064067
2022-01-11 22:19:04,989 iteration 236 : loss : 0.214393, loss_ce: 0.092882
2022-01-11 22:19:06,526 iteration 237 : loss : 0.128614, loss_ce: 0.051437
2022-01-11 22:19:08,004 iteration 238 : loss : 0.146395, loss_ce: 0.073235
  4%|█                             | 14/400 [06:38<3:00:51, 28.11s/it]2022-01-11 22:19:09,569 iteration 239 : loss : 0.159606, loss_ce: 0.063691
2022-01-11 22:19:11,207 iteration 240 : loss : 0.190424, loss_ce: 0.094613
2022-01-11 22:19:12,738 iteration 241 : loss : 0.227883, loss_ce: 0.077866
2022-01-11 22:19:14,363 iteration 242 : loss : 0.144045, loss_ce: 0.060642
2022-01-11 22:19:15,997 iteration 243 : loss : 0.200500, loss_ce: 0.096475
2022-01-11 22:19:17,610 iteration 244 : loss : 0.174178, loss_ce: 0.077654
2022-01-11 22:19:19,108 iteration 245 : loss : 0.182163, loss_ce: 0.062835
2022-01-11 22:19:20,683 iteration 246 : loss : 0.179965, loss_ce: 0.073373
2022-01-11 22:19:22,199 iteration 247 : loss : 0.136549, loss_ce: 0.062942
2022-01-11 22:19:23,688 iteration 248 : loss : 0.151020, loss_ce: 0.057422
2022-01-11 22:19:25,348 iteration 249 : loss : 0.173954, loss_ce: 0.057756
2022-01-11 22:19:26,970 iteration 250 : loss : 0.190830, loss_ce: 0.089925
2022-01-11 22:19:28,528 iteration 251 : loss : 0.127910, loss_ce: 0.052397
2022-01-11 22:19:30,097 iteration 252 : loss : 0.169292, loss_ce: 0.085007
2022-01-11 22:19:31,620 iteration 253 : loss : 0.181807, loss_ce: 0.060526
2022-01-11 22:19:33,141 iteration 254 : loss : 0.149532, loss_ce: 0.057399
2022-01-11 22:19:33,141 Training Data Eval:
2022-01-11 22:19:41,141   Average segmentation loss on training set: 0.2305
2022-01-11 22:19:41,142 Validation Data Eval:
2022-01-11 22:19:43,890   Average segmentation loss on validation set: 0.2718
2022-01-11 22:19:49,721 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-11 22:19:51,185 iteration 255 : loss : 0.217284, loss_ce: 0.092267
  4%|█▏                            | 15/400 [07:21<3:29:32, 32.66s/it]2022-01-11 22:19:52,681 iteration 256 : loss : 0.149953, loss_ce: 0.053937
2022-01-11 22:19:54,113 iteration 257 : loss : 0.148009, loss_ce: 0.078607
2022-01-11 22:19:55,683 iteration 258 : loss : 0.183242, loss_ce: 0.067323
2022-01-11 22:19:57,082 iteration 259 : loss : 0.223024, loss_ce: 0.082430
2022-01-11 22:19:58,564 iteration 260 : loss : 0.164570, loss_ce: 0.073913
2022-01-11 22:20:00,059 iteration 261 : loss : 0.223605, loss_ce: 0.104967
2022-01-11 22:20:01,543 iteration 262 : loss : 0.194892, loss_ce: 0.097489
2022-01-11 22:20:03,008 iteration 263 : loss : 0.232112, loss_ce: 0.131922
2022-01-11 22:20:04,534 iteration 264 : loss : 0.201634, loss_ce: 0.080623
2022-01-11 22:20:06,153 iteration 265 : loss : 0.152915, loss_ce: 0.075957
2022-01-11 22:20:07,756 iteration 266 : loss : 0.193200, loss_ce: 0.067714
2022-01-11 22:20:09,329 iteration 267 : loss : 0.159561, loss_ce: 0.065182
2022-01-11 22:20:10,911 iteration 268 : loss : 0.134049, loss_ce: 0.060393
2022-01-11 22:20:12,460 iteration 269 : loss : 0.167408, loss_ce: 0.073331
2022-01-11 22:20:13,951 iteration 270 : loss : 0.162390, loss_ce: 0.062115
2022-01-11 22:20:15,544 iteration 271 : loss : 0.316601, loss_ce: 0.121620
2022-01-11 22:20:17,189 iteration 272 : loss : 0.203926, loss_ce: 0.086250
  4%|█▏                            | 16/400 [07:47<3:16:11, 30.66s/it]2022-01-11 22:20:18,792 iteration 273 : loss : 0.166523, loss_ce: 0.063336
2022-01-11 22:20:20,382 iteration 274 : loss : 0.267475, loss_ce: 0.120509
2022-01-11 22:20:21,970 iteration 275 : loss : 0.163525, loss_ce: 0.080792
2022-01-11 22:20:23,499 iteration 276 : loss : 0.189089, loss_ce: 0.094936
2022-01-11 22:20:25,154 iteration 277 : loss : 0.190925, loss_ce: 0.078975
2022-01-11 22:20:26,666 iteration 278 : loss : 0.297186, loss_ce: 0.115283
2022-01-11 22:20:28,246 iteration 279 : loss : 0.176588, loss_ce: 0.079273
2022-01-11 22:20:29,732 iteration 280 : loss : 0.186755, loss_ce: 0.092980
2022-01-11 22:20:31,317 iteration 281 : loss : 0.176552, loss_ce: 0.065856
2022-01-11 22:20:32,928 iteration 282 : loss : 0.230652, loss_ce: 0.114270
2022-01-11 22:20:34,515 iteration 283 : loss : 0.196061, loss_ce: 0.094844
2022-01-11 22:20:36,045 iteration 284 : loss : 0.158882, loss_ce: 0.065014
2022-01-11 22:20:37,634 iteration 285 : loss : 0.171112, loss_ce: 0.072905
2022-01-11 22:20:39,243 iteration 286 : loss : 0.121427, loss_ce: 0.048220
2022-01-11 22:20:40,775 iteration 287 : loss : 0.199009, loss_ce: 0.075596
2022-01-11 22:20:42,340 iteration 288 : loss : 0.118602, loss_ce: 0.056667
2022-01-11 22:20:43,911 iteration 289 : loss : 0.176007, loss_ce: 0.088256
  4%|█▎                            | 17/400 [08:14<3:08:08, 29.47s/it]2022-01-11 22:20:45,538 iteration 290 : loss : 0.160328, loss_ce: 0.069598
2022-01-11 22:20:47,112 iteration 291 : loss : 0.156301, loss_ce: 0.061866
2022-01-11 22:20:48,693 iteration 292 : loss : 0.125612, loss_ce: 0.049584
2022-01-11 22:20:50,394 iteration 293 : loss : 0.163021, loss_ce: 0.074133
2022-01-11 22:20:51,928 iteration 294 : loss : 0.147598, loss_ce: 0.062963
2022-01-11 22:20:53,468 iteration 295 : loss : 0.206771, loss_ce: 0.079211
2022-01-11 22:20:55,018 iteration 296 : loss : 0.192978, loss_ce: 0.079048
2022-01-11 22:20:56,649 iteration 297 : loss : 0.228640, loss_ce: 0.111314
2022-01-11 22:20:58,183 iteration 298 : loss : 0.179293, loss_ce: 0.068875
2022-01-11 22:20:59,775 iteration 299 : loss : 0.218426, loss_ce: 0.079935
2022-01-11 22:21:01,313 iteration 300 : loss : 0.181730, loss_ce: 0.070544
2022-01-11 22:21:02,873 iteration 301 : loss : 0.194410, loss_ce: 0.070512
2022-01-11 22:21:04,529 iteration 302 : loss : 0.254091, loss_ce: 0.136013
2022-01-11 22:21:06,133 iteration 303 : loss : 0.208982, loss_ce: 0.103989
2022-01-11 22:21:07,689 iteration 304 : loss : 0.132015, loss_ce: 0.058943
2022-01-11 22:21:09,258 iteration 305 : loss : 0.147185, loss_ce: 0.059590
2022-01-11 22:21:10,823 iteration 306 : loss : 0.184417, loss_ce: 0.071544
  4%|█▎                            | 18/400 [08:41<3:02:43, 28.70s/it]2022-01-11 22:21:12,407 iteration 307 : loss : 0.164680, loss_ce: 0.075594
2022-01-11 22:21:14,023 iteration 308 : loss : 0.179016, loss_ce: 0.063395
2022-01-11 22:21:15,592 iteration 309 : loss : 0.186025, loss_ce: 0.086784
2022-01-11 22:21:17,143 iteration 310 : loss : 0.103997, loss_ce: 0.037286
2022-01-11 22:21:18,679 iteration 311 : loss : 0.144047, loss_ce: 0.057367
2022-01-11 22:21:20,257 iteration 312 : loss : 0.193792, loss_ce: 0.052425
2022-01-11 22:21:21,874 iteration 313 : loss : 0.156558, loss_ce: 0.058040
2022-01-11 22:21:23,356 iteration 314 : loss : 0.196826, loss_ce: 0.083268
2022-01-11 22:21:24,893 iteration 315 : loss : 0.158773, loss_ce: 0.066312
2022-01-11 22:21:26,528 iteration 316 : loss : 0.164586, loss_ce: 0.071958
2022-01-11 22:21:28,173 iteration 317 : loss : 0.156405, loss_ce: 0.082026
2022-01-11 22:21:29,735 iteration 318 : loss : 0.190257, loss_ce: 0.096681
2022-01-11 22:21:31,315 iteration 319 : loss : 0.103776, loss_ce: 0.045694
2022-01-11 22:21:32,898 iteration 320 : loss : 0.134695, loss_ce: 0.051480
2022-01-11 22:21:34,434 iteration 321 : loss : 0.202778, loss_ce: 0.077057
2022-01-11 22:21:35,950 iteration 322 : loss : 0.127230, loss_ce: 0.056946
2022-01-11 22:21:37,501 iteration 323 : loss : 0.133755, loss_ce: 0.059592
  5%|█▍                            | 19/400 [09:08<2:58:24, 28.10s/it]2022-01-11 22:21:39,106 iteration 324 : loss : 0.116208, loss_ce: 0.044729
2022-01-11 22:21:40,641 iteration 325 : loss : 0.129601, loss_ce: 0.039394
2022-01-11 22:21:42,175 iteration 326 : loss : 0.253901, loss_ce: 0.121419
2022-01-11 22:21:43,684 iteration 327 : loss : 0.230518, loss_ce: 0.098445
2022-01-11 22:21:45,257 iteration 328 : loss : 0.110704, loss_ce: 0.035402
2022-01-11 22:21:46,884 iteration 329 : loss : 0.217877, loss_ce: 0.076621
2022-01-11 22:21:48,427 iteration 330 : loss : 0.161200, loss_ce: 0.074134
2022-01-11 22:21:49,984 iteration 331 : loss : 0.136583, loss_ce: 0.060295
2022-01-11 22:21:51,472 iteration 332 : loss : 0.099504, loss_ce: 0.043429
2022-01-11 22:21:53,082 iteration 333 : loss : 0.158529, loss_ce: 0.074142
2022-01-11 22:21:54,656 iteration 334 : loss : 0.130160, loss_ce: 0.058355
2022-01-11 22:21:56,286 iteration 335 : loss : 0.153488, loss_ce: 0.079072
2022-01-11 22:21:57,847 iteration 336 : loss : 0.154792, loss_ce: 0.075211
2022-01-11 22:21:59,749 iteration 337 : loss : 0.171391, loss_ce: 0.073746
2022-01-11 22:22:01,307 iteration 338 : loss : 0.139183, loss_ce: 0.063894
2022-01-11 22:22:02,850 iteration 339 : loss : 0.176512, loss_ce: 0.083783
2022-01-11 22:22:02,850 Training Data Eval:
2022-01-11 22:22:10,881   Average segmentation loss on training set: 0.1137
2022-01-11 22:22:10,882 Validation Data Eval:
2022-01-11 22:22:13,636   Average segmentation loss on validation set: 0.1575
2022-01-11 22:22:19,636 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-11 22:22:21,157 iteration 340 : loss : 0.173593, loss_ce: 0.087272
  5%|█▌                            | 20/400 [09:51<3:27:31, 32.77s/it]2022-01-11 22:22:22,640 iteration 341 : loss : 0.102514, loss_ce: 0.048558
2022-01-11 22:22:24,157 iteration 342 : loss : 0.159476, loss_ce: 0.073605
2022-01-11 22:22:25,656 iteration 343 : loss : 0.184846, loss_ce: 0.087143
2022-01-11 22:22:27,091 iteration 344 : loss : 0.131622, loss_ce: 0.056412
2022-01-11 22:22:28,526 iteration 345 : loss : 0.164003, loss_ce: 0.067376
2022-01-11 22:22:30,028 iteration 346 : loss : 0.124678, loss_ce: 0.047199
2022-01-11 22:22:31,535 iteration 347 : loss : 0.190074, loss_ce: 0.083474
2022-01-11 22:22:32,975 iteration 348 : loss : 0.124565, loss_ce: 0.051043
2022-01-11 22:22:34,488 iteration 349 : loss : 0.198957, loss_ce: 0.076920
2022-01-11 22:22:36,090 iteration 350 : loss : 0.156902, loss_ce: 0.073425
2022-01-11 22:22:37,656 iteration 351 : loss : 0.149507, loss_ce: 0.061866
2022-01-11 22:22:39,326 iteration 352 : loss : 0.166833, loss_ce: 0.084719
2022-01-11 22:22:40,988 iteration 353 : loss : 0.153740, loss_ce: 0.069034
2022-01-11 22:22:42,543 iteration 354 : loss : 0.128936, loss_ce: 0.049973
2022-01-11 22:22:44,153 iteration 355 : loss : 0.113809, loss_ce: 0.040025
2022-01-11 22:22:45,730 iteration 356 : loss : 0.114682, loss_ce: 0.036683
2022-01-11 22:22:47,317 iteration 357 : loss : 0.108387, loss_ce: 0.044575
  5%|█▌                            | 21/400 [10:18<3:14:26, 30.78s/it]2022-01-11 22:22:49,025 iteration 358 : loss : 0.177126, loss_ce: 0.087064
2022-01-11 22:22:50,672 iteration 359 : loss : 0.112030, loss_ce: 0.048773
2022-01-11 22:22:52,222 iteration 360 : loss : 0.100505, loss_ce: 0.038551
2022-01-11 22:22:53,822 iteration 361 : loss : 0.206736, loss_ce: 0.083758
2022-01-11 22:22:55,430 iteration 362 : loss : 0.126962, loss_ce: 0.074130
2022-01-11 22:22:56,943 iteration 363 : loss : 0.146100, loss_ce: 0.073825
2022-01-11 22:22:58,528 iteration 364 : loss : 0.207885, loss_ce: 0.075577
2022-01-11 22:23:00,062 iteration 365 : loss : 0.100447, loss_ce: 0.037506
2022-01-11 22:23:01,617 iteration 366 : loss : 0.139666, loss_ce: 0.041406
2022-01-11 22:23:03,201 iteration 367 : loss : 0.173571, loss_ce: 0.067057
2022-01-11 22:23:04,758 iteration 368 : loss : 0.165439, loss_ce: 0.062934
2022-01-11 22:23:06,296 iteration 369 : loss : 0.144807, loss_ce: 0.048038
2022-01-11 22:23:07,868 iteration 370 : loss : 0.217972, loss_ce: 0.108254
2022-01-11 22:23:09,438 iteration 371 : loss : 0.167417, loss_ce: 0.071965
2022-01-11 22:23:10,964 iteration 372 : loss : 0.126754, loss_ce: 0.053439
2022-01-11 22:23:12,522 iteration 373 : loss : 0.146495, loss_ce: 0.059547
2022-01-11 22:23:14,128 iteration 374 : loss : 0.135039, loss_ce: 0.060713
  6%|█▋                            | 22/400 [10:44<3:06:25, 29.59s/it]2022-01-11 22:23:15,736 iteration 375 : loss : 0.133417, loss_ce: 0.062403
2022-01-11 22:23:17,297 iteration 376 : loss : 0.169828, loss_ce: 0.055640
2022-01-11 22:23:18,872 iteration 377 : loss : 0.125514, loss_ce: 0.043813
2022-01-11 22:23:20,514 iteration 378 : loss : 0.139212, loss_ce: 0.053787
2022-01-11 22:23:22,106 iteration 379 : loss : 0.157240, loss_ce: 0.057802
2022-01-11 22:23:23,650 iteration 380 : loss : 0.196176, loss_ce: 0.096754
2022-01-11 22:23:25,231 iteration 381 : loss : 0.170457, loss_ce: 0.067438
2022-01-11 22:23:26,741 iteration 382 : loss : 0.135954, loss_ce: 0.060860
2022-01-11 22:23:28,331 iteration 383 : loss : 0.186094, loss_ce: 0.048705
2022-01-11 22:23:29,902 iteration 384 : loss : 0.123292, loss_ce: 0.049836
2022-01-11 22:23:31,447 iteration 385 : loss : 0.156130, loss_ce: 0.076739
2022-01-11 22:23:33,019 iteration 386 : loss : 0.176229, loss_ce: 0.064927
2022-01-11 22:23:34,549 iteration 387 : loss : 0.115258, loss_ce: 0.049964
2022-01-11 22:23:36,150 iteration 388 : loss : 0.156444, loss_ce: 0.069419
2022-01-11 22:23:37,706 iteration 389 : loss : 0.148837, loss_ce: 0.047786
2022-01-11 22:23:39,285 iteration 390 : loss : 0.149332, loss_ce: 0.072962
2022-01-11 22:23:40,884 iteration 391 : loss : 0.104095, loss_ce: 0.042734
  6%|█▋                            | 23/400 [11:11<3:00:35, 28.74s/it]2022-01-11 22:23:42,552 iteration 392 : loss : 0.169719, loss_ce: 0.075218
2022-01-11 22:23:44,078 iteration 393 : loss : 0.140462, loss_ce: 0.071144
2022-01-11 22:23:45,656 iteration 394 : loss : 0.140666, loss_ce: 0.050120
2022-01-11 22:23:47,179 iteration 395 : loss : 0.134085, loss_ce: 0.051121
2022-01-11 22:23:48,741 iteration 396 : loss : 0.144100, loss_ce: 0.050723
2022-01-11 22:23:50,383 iteration 397 : loss : 0.165420, loss_ce: 0.076687
2022-01-11 22:23:51,945 iteration 398 : loss : 0.151178, loss_ce: 0.051001
2022-01-11 22:23:53,524 iteration 399 : loss : 0.144305, loss_ce: 0.060874
2022-01-11 22:23:55,067 iteration 400 : loss : 0.124526, loss_ce: 0.051285
2022-01-11 22:23:56,659 iteration 401 : loss : 0.111526, loss_ce: 0.053646
2022-01-11 22:23:58,331 iteration 402 : loss : 0.129204, loss_ce: 0.060212
2022-01-11 22:23:59,948 iteration 403 : loss : 0.142639, loss_ce: 0.057349
2022-01-11 22:24:01,497 iteration 404 : loss : 0.092801, loss_ce: 0.035357
2022-01-11 22:24:03,053 iteration 405 : loss : 0.134468, loss_ce: 0.065517
2022-01-11 22:24:04,663 iteration 406 : loss : 0.143726, loss_ce: 0.054564
2022-01-11 22:24:06,271 iteration 407 : loss : 0.112728, loss_ce: 0.046137
2022-01-11 22:24:07,763 iteration 408 : loss : 0.121172, loss_ce: 0.053029
  6%|█▊                            | 24/400 [11:38<2:56:36, 28.18s/it]2022-01-11 22:24:09,433 iteration 409 : loss : 0.140404, loss_ce: 0.041592
2022-01-11 22:24:11,033 iteration 410 : loss : 0.125512, loss_ce: 0.050023
2022-01-11 22:24:12,635 iteration 411 : loss : 0.161620, loss_ce: 0.050465
2022-01-11 22:24:14,187 iteration 412 : loss : 0.127853, loss_ce: 0.052809
2022-01-11 22:24:15,751 iteration 413 : loss : 0.145465, loss_ce: 0.041306
2022-01-11 22:24:17,275 iteration 414 : loss : 0.121785, loss_ce: 0.045411
2022-01-11 22:24:18,901 iteration 415 : loss : 0.126336, loss_ce: 0.053738
2022-01-11 22:24:20,446 iteration 416 : loss : 0.115253, loss_ce: 0.056400
2022-01-11 22:24:22,052 iteration 417 : loss : 0.136659, loss_ce: 0.047893
2022-01-11 22:24:23,553 iteration 418 : loss : 0.107144, loss_ce: 0.065394
2022-01-11 22:24:25,110 iteration 419 : loss : 0.178325, loss_ce: 0.059988
2022-01-11 22:24:26,678 iteration 420 : loss : 0.099322, loss_ce: 0.045585
2022-01-11 22:24:28,306 iteration 421 : loss : 0.191251, loss_ce: 0.105682
2022-01-11 22:24:29,847 iteration 422 : loss : 0.100088, loss_ce: 0.035550
2022-01-11 22:24:31,419 iteration 423 : loss : 0.142939, loss_ce: 0.055385
2022-01-11 22:24:33,028 iteration 424 : loss : 0.110968, loss_ce: 0.042345
2022-01-11 22:24:33,028 Training Data Eval:
2022-01-11 22:24:41,037   Average segmentation loss on training set: 0.2751
2022-01-11 22:24:41,038 Validation Data Eval:
2022-01-11 22:24:43,797   Average segmentation loss on validation set: 0.3813
2022-01-11 22:24:45,440 iteration 425 : loss : 0.145203, loss_ce: 0.056361
  6%|█▉                            | 25/400 [12:16<3:13:56, 31.03s/it]2022-01-11 22:24:47,109 iteration 426 : loss : 0.121763, loss_ce: 0.049918
2022-01-11 22:24:48,699 iteration 427 : loss : 0.135168, loss_ce: 0.051176
2022-01-11 22:24:50,324 iteration 428 : loss : 0.137350, loss_ce: 0.059092
2022-01-11 22:24:51,868 iteration 429 : loss : 0.111017, loss_ce: 0.043835
2022-01-11 22:24:53,464 iteration 430 : loss : 0.153582, loss_ce: 0.057470
2022-01-11 22:24:54,978 iteration 431 : loss : 0.143068, loss_ce: 0.053511
2022-01-11 22:24:56,526 iteration 432 : loss : 0.075168, loss_ce: 0.031936
2022-01-11 22:24:58,117 iteration 433 : loss : 0.118297, loss_ce: 0.033912
2022-01-11 22:24:59,666 iteration 434 : loss : 0.096776, loss_ce: 0.033151
2022-01-11 22:25:01,229 iteration 435 : loss : 0.146078, loss_ce: 0.069867
2022-01-11 22:25:02,838 iteration 436 : loss : 0.197780, loss_ce: 0.081202
2022-01-11 22:25:04,416 iteration 437 : loss : 0.115013, loss_ce: 0.043262
2022-01-11 22:25:05,985 iteration 438 : loss : 0.153581, loss_ce: 0.093453
2022-01-11 22:25:07,554 iteration 439 : loss : 0.131075, loss_ce: 0.061043
2022-01-11 22:25:09,086 iteration 440 : loss : 0.081897, loss_ce: 0.034895
2022-01-11 22:25:10,642 iteration 441 : loss : 0.121830, loss_ce: 0.057632
2022-01-11 22:25:12,184 iteration 442 : loss : 0.168721, loss_ce: 0.063526
  6%|█▉                            | 26/400 [12:42<3:05:24, 29.74s/it]2022-01-11 22:25:13,798 iteration 443 : loss : 0.094551, loss_ce: 0.047170
2022-01-11 22:25:15,357 iteration 444 : loss : 0.154087, loss_ce: 0.056230
2022-01-11 22:25:17,099 iteration 445 : loss : 0.130913, loss_ce: 0.067733
2022-01-11 22:25:18,621 iteration 446 : loss : 0.178019, loss_ce: 0.063120
2022-01-11 22:25:20,263 iteration 447 : loss : 0.139169, loss_ce: 0.062122
2022-01-11 22:25:21,869 iteration 448 : loss : 0.101519, loss_ce: 0.045145
2022-01-11 22:25:23,415 iteration 449 : loss : 0.109797, loss_ce: 0.040469
2022-01-11 22:25:24,911 iteration 450 : loss : 0.106105, loss_ce: 0.046359
2022-01-11 22:25:26,465 iteration 451 : loss : 0.123775, loss_ce: 0.041283
2022-01-11 22:25:28,058 iteration 452 : loss : 0.108037, loss_ce: 0.046431
2022-01-11 22:25:29,636 iteration 453 : loss : 0.081851, loss_ce: 0.030397
2022-01-11 22:25:31,247 iteration 454 : loss : 0.189292, loss_ce: 0.054746
2022-01-11 22:25:32,884 iteration 455 : loss : 0.075847, loss_ce: 0.025184
2022-01-11 22:25:34,427 iteration 456 : loss : 0.151098, loss_ce: 0.049152
2022-01-11 22:25:35,970 iteration 457 : loss : 0.135438, loss_ce: 0.050150
2022-01-11 22:25:37,540 iteration 458 : loss : 0.143965, loss_ce: 0.066133
2022-01-11 22:25:39,119 iteration 459 : loss : 0.116627, loss_ce: 0.047891
  7%|██                            | 27/400 [13:09<2:59:40, 28.90s/it]2022-01-11 22:25:40,659 iteration 460 : loss : 0.118596, loss_ce: 0.042063
2022-01-11 22:25:42,248 iteration 461 : loss : 0.140936, loss_ce: 0.058478
2022-01-11 22:25:43,822 iteration 462 : loss : 0.121212, loss_ce: 0.037134
2022-01-11 22:25:45,314 iteration 463 : loss : 0.093591, loss_ce: 0.043035
2022-01-11 22:25:46,943 iteration 464 : loss : 0.126624, loss_ce: 0.060632
2022-01-11 22:25:48,498 iteration 465 : loss : 0.103194, loss_ce: 0.060102
2022-01-11 22:25:50,106 iteration 466 : loss : 0.121112, loss_ce: 0.062282
2022-01-11 22:25:51,640 iteration 467 : loss : 0.132225, loss_ce: 0.044398
2022-01-11 22:25:53,202 iteration 468 : loss : 0.126508, loss_ce: 0.043131
2022-01-11 22:25:54,732 iteration 469 : loss : 0.170104, loss_ce: 0.061968
2022-01-11 22:25:56,360 iteration 470 : loss : 0.172489, loss_ce: 0.084548
2022-01-11 22:25:57,884 iteration 471 : loss : 0.117843, loss_ce: 0.049595
2022-01-11 22:25:59,502 iteration 472 : loss : 0.164083, loss_ce: 0.065829
2022-01-11 22:26:01,100 iteration 473 : loss : 0.092343, loss_ce: 0.036718
2022-01-11 22:26:02,724 iteration 474 : loss : 0.137755, loss_ce: 0.057989
2022-01-11 22:26:04,301 iteration 475 : loss : 0.142559, loss_ce: 0.045886
2022-01-11 22:26:05,901 iteration 476 : loss : 0.136248, loss_ce: 0.072411
  7%|██                            | 28/400 [13:36<2:55:14, 28.26s/it]2022-01-11 22:26:07,500 iteration 477 : loss : 0.131019, loss_ce: 0.057425
2022-01-11 22:26:09,103 iteration 478 : loss : 0.122975, loss_ce: 0.049629
2022-01-11 22:26:10,615 iteration 479 : loss : 0.105682, loss_ce: 0.043468
2022-01-11 22:26:12,109 iteration 480 : loss : 0.110362, loss_ce: 0.051145
2022-01-11 22:26:13,664 iteration 481 : loss : 0.121938, loss_ce: 0.053117
2022-01-11 22:26:15,281 iteration 482 : loss : 0.116589, loss_ce: 0.045161
2022-01-11 22:26:16,864 iteration 483 : loss : 0.103203, loss_ce: 0.035531
2022-01-11 22:26:18,430 iteration 484 : loss : 0.121742, loss_ce: 0.048192
2022-01-11 22:26:19,938 iteration 485 : loss : 0.067924, loss_ce: 0.027434
2022-01-11 22:26:21,506 iteration 486 : loss : 0.229393, loss_ce: 0.064215
2022-01-11 22:26:23,111 iteration 487 : loss : 0.171279, loss_ce: 0.086003
2022-01-11 22:26:24,767 iteration 488 : loss : 0.173265, loss_ce: 0.068255
2022-01-11 22:26:26,364 iteration 489 : loss : 0.097802, loss_ce: 0.038979
2022-01-11 22:26:27,888 iteration 490 : loss : 0.110270, loss_ce: 0.047202
2022-01-11 22:26:29,506 iteration 491 : loss : 0.130102, loss_ce: 0.052502
2022-01-11 22:26:31,036 iteration 492 : loss : 0.123937, loss_ce: 0.051492
2022-01-11 22:26:32,586 iteration 493 : loss : 0.143800, loss_ce: 0.068238
  7%|██▏                           | 29/400 [14:03<2:51:50, 27.79s/it]2022-01-11 22:26:34,162 iteration 494 : loss : 0.126596, loss_ce: 0.052406
2022-01-11 22:26:35,724 iteration 495 : loss : 0.129739, loss_ce: 0.053457
2022-01-11 22:26:37,213 iteration 496 : loss : 0.122262, loss_ce: 0.056466
2022-01-11 22:26:38,823 iteration 497 : loss : 0.112822, loss_ce: 0.039084
2022-01-11 22:26:40,377 iteration 498 : loss : 0.133474, loss_ce: 0.053097
2022-01-11 22:26:42,010 iteration 499 : loss : 0.114648, loss_ce: 0.055161
2022-01-11 22:26:43,498 iteration 500 : loss : 0.096537, loss_ce: 0.039873
2022-01-11 22:26:45,073 iteration 501 : loss : 0.136594, loss_ce: 0.047851
2022-01-11 22:26:46,614 iteration 502 : loss : 0.134165, loss_ce: 0.048180
2022-01-11 22:26:48,176 iteration 503 : loss : 0.104542, loss_ce: 0.053767
2022-01-11 22:26:49,813 iteration 504 : loss : 0.097579, loss_ce: 0.037210
2022-01-11 22:26:51,451 iteration 505 : loss : 0.079830, loss_ce: 0.040291
2022-01-11 22:26:53,034 iteration 506 : loss : 0.137656, loss_ce: 0.060923
2022-01-11 22:26:54,598 iteration 507 : loss : 0.092068, loss_ce: 0.032324
2022-01-11 22:26:56,192 iteration 508 : loss : 0.106773, loss_ce: 0.034223
2022-01-11 22:26:57,803 iteration 509 : loss : 0.088455, loss_ce: 0.042009
2022-01-11 22:26:57,803 Training Data Eval:
2022-01-11 22:27:05,807   Average segmentation loss on training set: 0.0795
2022-01-11 22:27:05,807 Validation Data Eval:
2022-01-11 22:27:08,568   Average segmentation loss on validation set: 0.1271
2022-01-11 22:27:14,397 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-11 22:27:15,829 iteration 510 : loss : 0.083744, loss_ce: 0.036685
  8%|██▎                           | 30/400 [14:46<3:19:58, 32.43s/it]2022-01-11 22:27:17,283 iteration 511 : loss : 0.094863, loss_ce: 0.032604
2022-01-11 22:27:18,724 iteration 512 : loss : 0.147686, loss_ce: 0.075469
2022-01-11 22:27:20,187 iteration 513 : loss : 0.104036, loss_ce: 0.043228
2022-01-11 22:27:21,678 iteration 514 : loss : 0.117127, loss_ce: 0.045954
2022-01-11 22:27:23,073 iteration 515 : loss : 0.113589, loss_ce: 0.052523
2022-01-11 22:27:24,428 iteration 516 : loss : 0.070781, loss_ce: 0.026881
2022-01-11 22:27:25,961 iteration 517 : loss : 0.083789, loss_ce: 0.039688
2022-01-11 22:27:27,521 iteration 518 : loss : 0.155168, loss_ce: 0.056497
2022-01-11 22:27:29,117 iteration 519 : loss : 0.141415, loss_ce: 0.070885
2022-01-11 22:27:30,695 iteration 520 : loss : 0.087208, loss_ce: 0.037525
2022-01-11 22:27:32,370 iteration 521 : loss : 0.072267, loss_ce: 0.028285
2022-01-11 22:27:34,004 iteration 522 : loss : 0.146640, loss_ce: 0.050895
2022-01-11 22:27:35,577 iteration 523 : loss : 0.117845, loss_ce: 0.038238
2022-01-11 22:27:37,121 iteration 524 : loss : 0.100896, loss_ce: 0.051921
2022-01-11 22:27:38,681 iteration 525 : loss : 0.082572, loss_ce: 0.035272
2022-01-11 22:27:40,290 iteration 526 : loss : 0.098767, loss_ce: 0.042299
2022-01-11 22:27:41,888 iteration 527 : loss : 0.127726, loss_ce: 0.049942
  8%|██▎                           | 31/400 [15:12<3:07:40, 30.52s/it]2022-01-11 22:27:43,470 iteration 528 : loss : 0.094872, loss_ce: 0.041498
2022-01-11 22:27:45,140 iteration 529 : loss : 0.115092, loss_ce: 0.035400
2022-01-11 22:27:46,688 iteration 530 : loss : 0.093307, loss_ce: 0.036582
2022-01-11 22:27:48,214 iteration 531 : loss : 0.088406, loss_ce: 0.037479
2022-01-11 22:27:49,798 iteration 532 : loss : 0.121691, loss_ce: 0.061881
2022-01-11 22:27:51,374 iteration 533 : loss : 0.097204, loss_ce: 0.039717
2022-01-11 22:27:52,936 iteration 534 : loss : 0.098855, loss_ce: 0.037319
2022-01-11 22:27:54,541 iteration 535 : loss : 0.084371, loss_ce: 0.029121
2022-01-11 22:27:56,121 iteration 536 : loss : 0.102618, loss_ce: 0.049897
2022-01-11 22:27:57,678 iteration 537 : loss : 0.129343, loss_ce: 0.056294
2022-01-11 22:27:59,249 iteration 538 : loss : 0.120514, loss_ce: 0.057439
2022-01-11 22:28:00,752 iteration 539 : loss : 0.087954, loss_ce: 0.037486
2022-01-11 22:28:02,444 iteration 540 : loss : 0.123042, loss_ce: 0.064091
2022-01-11 22:28:04,111 iteration 541 : loss : 0.140734, loss_ce: 0.072931
2022-01-11 22:28:05,633 iteration 542 : loss : 0.163062, loss_ce: 0.080817
2022-01-11 22:28:07,207 iteration 543 : loss : 0.108725, loss_ce: 0.047697
2022-01-11 22:28:08,763 iteration 544 : loss : 0.204589, loss_ce: 0.081509
  8%|██▍                           | 32/400 [15:39<3:00:28, 29.42s/it]2022-01-11 22:28:10,421 iteration 545 : loss : 0.105740, loss_ce: 0.045961
2022-01-11 22:28:12,056 iteration 546 : loss : 0.106948, loss_ce: 0.047251
2022-01-11 22:28:13,685 iteration 547 : loss : 0.105876, loss_ce: 0.042019
2022-01-11 22:28:15,202 iteration 548 : loss : 0.166933, loss_ce: 0.061269
2022-01-11 22:28:16,729 iteration 549 : loss : 0.158080, loss_ce: 0.057490
2022-01-11 22:28:18,255 iteration 550 : loss : 0.121681, loss_ce: 0.048937
2022-01-11 22:28:19,783 iteration 551 : loss : 0.100010, loss_ce: 0.028736
2022-01-11 22:28:21,388 iteration 552 : loss : 0.138040, loss_ce: 0.057097
2022-01-11 22:28:22,973 iteration 553 : loss : 0.112169, loss_ce: 0.041908
2022-01-11 22:28:24,562 iteration 554 : loss : 0.122795, loss_ce: 0.050524
2022-01-11 22:28:26,173 iteration 555 : loss : 0.076319, loss_ce: 0.022462
2022-01-11 22:28:27,821 iteration 556 : loss : 0.111890, loss_ce: 0.050448
2022-01-11 22:28:29,348 iteration 557 : loss : 0.115423, loss_ce: 0.058162
2022-01-11 22:28:30,855 iteration 558 : loss : 0.089570, loss_ce: 0.034957
2022-01-11 22:28:32,399 iteration 559 : loss : 0.093033, loss_ce: 0.038045
2022-01-11 22:28:34,027 iteration 560 : loss : 0.089710, loss_ce: 0.044389
2022-01-11 22:28:35,636 iteration 561 : loss : 0.081293, loss_ce: 0.038693
  8%|██▍                           | 33/400 [16:06<2:55:17, 28.66s/it]2022-01-11 22:28:37,283 iteration 562 : loss : 0.136631, loss_ce: 0.058759
2022-01-11 22:28:38,823 iteration 563 : loss : 0.077603, loss_ce: 0.040995
2022-01-11 22:28:40,352 iteration 564 : loss : 0.121919, loss_ce: 0.060011
2022-01-11 22:28:41,954 iteration 565 : loss : 0.086639, loss_ce: 0.038294
2022-01-11 22:28:43,549 iteration 566 : loss : 0.127946, loss_ce: 0.039456
2022-01-11 22:28:45,153 iteration 567 : loss : 0.149973, loss_ce: 0.061438
2022-01-11 22:28:46,747 iteration 568 : loss : 0.125916, loss_ce: 0.039681
2022-01-11 22:28:48,411 iteration 569 : loss : 0.154610, loss_ce: 0.060264
2022-01-11 22:28:50,004 iteration 570 : loss : 0.107698, loss_ce: 0.047901
2022-01-11 22:28:51,628 iteration 571 : loss : 0.075654, loss_ce: 0.032431
2022-01-11 22:28:53,266 iteration 572 : loss : 0.105462, loss_ce: 0.048182
2022-01-11 22:28:54,804 iteration 573 : loss : 0.093583, loss_ce: 0.038317
2022-01-11 22:28:56,437 iteration 574 : loss : 0.087765, loss_ce: 0.033877
2022-01-11 22:28:57,985 iteration 575 : loss : 0.138125, loss_ce: 0.036909
2022-01-11 22:28:59,494 iteration 576 : loss : 0.120822, loss_ce: 0.060614
2022-01-11 22:29:01,192 iteration 577 : loss : 0.141383, loss_ce: 0.078575
2022-01-11 22:29:02,809 iteration 578 : loss : 0.097033, loss_ce: 0.045656
  8%|██▌                           | 34/400 [16:33<2:52:05, 28.21s/it]2022-01-11 22:29:04,348 iteration 579 : loss : 0.077727, loss_ce: 0.036743
2022-01-11 22:29:05,963 iteration 580 : loss : 0.087397, loss_ce: 0.035235
2022-01-11 22:29:07,664 iteration 581 : loss : 0.064253, loss_ce: 0.029777
2022-01-11 22:29:09,186 iteration 582 : loss : 0.083880, loss_ce: 0.038564
2022-01-11 22:29:10,768 iteration 583 : loss : 0.101153, loss_ce: 0.045943
2022-01-11 22:29:12,372 iteration 584 : loss : 0.118925, loss_ce: 0.050579
2022-01-11 22:29:13,911 iteration 585 : loss : 0.131916, loss_ce: 0.045344
2022-01-11 22:29:15,524 iteration 586 : loss : 0.088003, loss_ce: 0.033939
2022-01-11 22:29:17,111 iteration 587 : loss : 0.075360, loss_ce: 0.037254
2022-01-11 22:29:18,645 iteration 588 : loss : 0.121533, loss_ce: 0.040801
2022-01-11 22:29:20,280 iteration 589 : loss : 0.103289, loss_ce: 0.045545
2022-01-11 22:29:21,839 iteration 590 : loss : 0.096262, loss_ce: 0.034552
2022-01-11 22:29:23,402 iteration 591 : loss : 0.110978, loss_ce: 0.045214
2022-01-11 22:29:25,002 iteration 592 : loss : 0.072317, loss_ce: 0.027755
2022-01-11 22:29:26,516 iteration 593 : loss : 0.131632, loss_ce: 0.053579
2022-01-11 22:29:28,136 iteration 594 : loss : 0.084931, loss_ce: 0.030884
2022-01-11 22:29:28,136 Training Data Eval:
2022-01-11 22:29:36,159   Average segmentation loss on training set: 0.1304
2022-01-11 22:29:36,160 Validation Data Eval:
2022-01-11 22:29:38,920   Average segmentation loss on validation set: 0.2125
2022-01-11 22:29:40,527 iteration 595 : loss : 0.130751, loss_ce: 0.047429
  9%|██▋                           | 35/400 [17:11<3:08:58, 31.06s/it]2022-01-11 22:29:42,223 iteration 596 : loss : 0.099282, loss_ce: 0.043960
2022-01-11 22:29:43,805 iteration 597 : loss : 0.068070, loss_ce: 0.028531
2022-01-11 22:29:45,382 iteration 598 : loss : 0.084196, loss_ce: 0.030188
2022-01-11 22:29:46,989 iteration 599 : loss : 0.105467, loss_ce: 0.031677
2022-01-11 22:29:48,601 iteration 600 : loss : 0.082336, loss_ce: 0.025601
2022-01-11 22:29:50,213 iteration 601 : loss : 0.086231, loss_ce: 0.034588
2022-01-11 22:29:51,711 iteration 602 : loss : 0.095276, loss_ce: 0.041720
2022-01-11 22:29:53,277 iteration 603 : loss : 0.124336, loss_ce: 0.047775
2022-01-11 22:29:54,808 iteration 604 : loss : 0.160906, loss_ce: 0.053816
2022-01-11 22:29:56,476 iteration 605 : loss : 0.086018, loss_ce: 0.031018
2022-01-11 22:29:58,007 iteration 606 : loss : 0.067328, loss_ce: 0.026959
2022-01-11 22:29:59,539 iteration 607 : loss : 0.075003, loss_ce: 0.026477
2022-01-11 22:30:01,068 iteration 608 : loss : 0.060272, loss_ce: 0.029101
2022-01-11 22:30:02,596 iteration 609 : loss : 0.084356, loss_ce: 0.034025
2022-01-11 22:30:04,257 iteration 610 : loss : 0.132170, loss_ce: 0.067325
2022-01-11 22:30:05,812 iteration 611 : loss : 0.093911, loss_ce: 0.040588
2022-01-11 22:30:07,319 iteration 612 : loss : 0.084297, loss_ce: 0.034006
  9%|██▋                           | 36/400 [17:38<3:00:40, 29.78s/it]2022-01-11 22:30:08,917 iteration 613 : loss : 0.097071, loss_ce: 0.036627
2022-01-11 22:30:10,536 iteration 614 : loss : 0.086331, loss_ce: 0.038049
2022-01-11 22:30:12,109 iteration 615 : loss : 0.087940, loss_ce: 0.040188
2022-01-11 22:30:13,659 iteration 616 : loss : 0.116704, loss_ce: 0.042659
2022-01-11 22:30:15,264 iteration 617 : loss : 0.157239, loss_ce: 0.065041
2022-01-11 22:30:16,902 iteration 618 : loss : 0.082610, loss_ce: 0.036540
2022-01-11 22:30:18,490 iteration 619 : loss : 0.074410, loss_ce: 0.031130
2022-01-11 22:30:20,095 iteration 620 : loss : 0.075944, loss_ce: 0.032387
2022-01-11 22:30:21,582 iteration 621 : loss : 0.074603, loss_ce: 0.027036
2022-01-11 22:30:23,213 iteration 622 : loss : 0.089685, loss_ce: 0.033228
2022-01-11 22:30:24,833 iteration 623 : loss : 0.102420, loss_ce: 0.046796
2022-01-11 22:30:26,338 iteration 624 : loss : 0.117321, loss_ce: 0.049356
2022-01-11 22:30:28,027 iteration 625 : loss : 0.124080, loss_ce: 0.052763
2022-01-11 22:30:29,580 iteration 626 : loss : 0.127400, loss_ce: 0.052149
2022-01-11 22:30:31,090 iteration 627 : loss : 0.109229, loss_ce: 0.043134
2022-01-11 22:30:32,577 iteration 628 : loss : 0.161511, loss_ce: 0.066965
2022-01-11 22:30:34,173 iteration 629 : loss : 0.064246, loss_ce: 0.028312
  9%|██▊                           | 37/400 [18:04<2:54:52, 28.90s/it]2022-01-11 22:30:35,697 iteration 630 : loss : 0.101890, loss_ce: 0.047008
2022-01-11 22:30:37,260 iteration 631 : loss : 0.139373, loss_ce: 0.055954
2022-01-11 22:30:38,774 iteration 632 : loss : 0.078458, loss_ce: 0.033200
2022-01-11 22:30:40,373 iteration 633 : loss : 0.138634, loss_ce: 0.081802
2022-01-11 22:30:41,960 iteration 634 : loss : 0.069501, loss_ce: 0.030915
2022-01-11 22:30:43,554 iteration 635 : loss : 0.067866, loss_ce: 0.028526
2022-01-11 22:30:45,097 iteration 636 : loss : 0.147606, loss_ce: 0.047135
2022-01-11 22:30:46,631 iteration 637 : loss : 0.095652, loss_ce: 0.046430
2022-01-11 22:30:48,165 iteration 638 : loss : 0.075481, loss_ce: 0.036922
2022-01-11 22:30:49,780 iteration 639 : loss : 0.083895, loss_ce: 0.045989
2022-01-11 22:30:51,401 iteration 640 : loss : 0.085995, loss_ce: 0.028473
2022-01-11 22:30:52,922 iteration 641 : loss : 0.067860, loss_ce: 0.026384
2022-01-11 22:30:54,484 iteration 642 : loss : 0.064970, loss_ce: 0.023965
2022-01-11 22:30:56,037 iteration 643 : loss : 0.110531, loss_ce: 0.033732
2022-01-11 22:30:57,604 iteration 644 : loss : 0.087698, loss_ce: 0.030025
2022-01-11 22:30:59,151 iteration 645 : loss : 0.112026, loss_ce: 0.041132
2022-01-11 22:31:00,766 iteration 646 : loss : 0.096887, loss_ce: 0.042317
 10%|██▊                           | 38/400 [18:31<2:50:13, 28.21s/it]2022-01-11 22:31:02,350 iteration 647 : loss : 0.084086, loss_ce: 0.031296
2022-01-11 22:31:03,971 iteration 648 : loss : 0.116813, loss_ce: 0.054162
2022-01-11 22:31:05,554 iteration 649 : loss : 0.102818, loss_ce: 0.042737
2022-01-11 22:31:07,060 iteration 650 : loss : 0.075012, loss_ce: 0.036877
2022-01-11 22:31:08,634 iteration 651 : loss : 0.110046, loss_ce: 0.047456
2022-01-11 22:31:10,149 iteration 652 : loss : 0.112627, loss_ce: 0.036645
2022-01-11 22:31:11,726 iteration 653 : loss : 0.150847, loss_ce: 0.043877
2022-01-11 22:31:13,356 iteration 654 : loss : 0.101380, loss_ce: 0.038668
2022-01-11 22:31:14,955 iteration 655 : loss : 0.066904, loss_ce: 0.030923
2022-01-11 22:31:16,454 iteration 656 : loss : 0.067057, loss_ce: 0.028280
2022-01-11 22:31:18,027 iteration 657 : loss : 0.062297, loss_ce: 0.026685
2022-01-11 22:31:19,653 iteration 658 : loss : 0.164353, loss_ce: 0.053574
2022-01-11 22:31:21,281 iteration 659 : loss : 0.060885, loss_ce: 0.027830
2022-01-11 22:31:22,870 iteration 660 : loss : 0.074509, loss_ce: 0.026239
2022-01-11 22:31:24,442 iteration 661 : loss : 0.087982, loss_ce: 0.041853
2022-01-11 22:31:25,973 iteration 662 : loss : 0.069373, loss_ce: 0.033840
2022-01-11 22:31:27,535 iteration 663 : loss : 0.110198, loss_ce: 0.039755
 10%|██▉                           | 39/400 [18:58<2:47:08, 27.78s/it]2022-01-11 22:31:29,108 iteration 664 : loss : 0.126105, loss_ce: 0.079173
2022-01-11 22:31:30,663 iteration 665 : loss : 0.087104, loss_ce: 0.043298
2022-01-11 22:31:32,231 iteration 666 : loss : 0.095365, loss_ce: 0.035994
2022-01-11 22:31:33,754 iteration 667 : loss : 0.066863, loss_ce: 0.028574
2022-01-11 22:31:35,373 iteration 668 : loss : 0.078542, loss_ce: 0.032136
2022-01-11 22:31:36,910 iteration 669 : loss : 0.084772, loss_ce: 0.040142
2022-01-11 22:31:38,446 iteration 670 : loss : 0.089353, loss_ce: 0.038943
2022-01-11 22:31:40,056 iteration 671 : loss : 0.092533, loss_ce: 0.029806
2022-01-11 22:31:41,676 iteration 672 : loss : 0.084657, loss_ce: 0.033848
2022-01-11 22:31:43,199 iteration 673 : loss : 0.071817, loss_ce: 0.032494
2022-01-11 22:31:44,783 iteration 674 : loss : 0.096453, loss_ce: 0.041615
2022-01-11 22:31:46,389 iteration 675 : loss : 0.089990, loss_ce: 0.034756
2022-01-11 22:31:47,931 iteration 676 : loss : 0.100767, loss_ce: 0.035377
2022-01-11 22:31:49,516 iteration 677 : loss : 0.087817, loss_ce: 0.040487
2022-01-11 22:31:51,074 iteration 678 : loss : 0.069796, loss_ce: 0.025166
2022-01-11 22:31:52,604 iteration 679 : loss : 0.111301, loss_ce: 0.056610
2022-01-11 22:31:52,604 Training Data Eval:
2022-01-11 22:32:00,630   Average segmentation loss on training set: 0.1023
2022-01-11 22:32:00,630 Validation Data Eval:
2022-01-11 22:32:03,397   Average segmentation loss on validation set: 0.1958
2022-01-11 22:32:04,993 iteration 680 : loss : 0.146448, loss_ce: 0.031168
 10%|███                           | 40/400 [19:35<3:04:05, 30.68s/it]2022-01-11 22:32:06,666 iteration 681 : loss : 0.084822, loss_ce: 0.039537
2022-01-11 22:32:08,217 iteration 682 : loss : 0.065505, loss_ce: 0.023444
2022-01-11 22:32:09,814 iteration 683 : loss : 0.080214, loss_ce: 0.034549
2022-01-11 22:32:11,414 iteration 684 : loss : 0.101630, loss_ce: 0.042038
2022-01-11 22:32:13,044 iteration 685 : loss : 0.075421, loss_ce: 0.032495
2022-01-11 22:32:14,534 iteration 686 : loss : 0.098132, loss_ce: 0.033547
2022-01-11 22:32:16,067 iteration 687 : loss : 0.066906, loss_ce: 0.023979
2022-01-11 22:32:17,699 iteration 688 : loss : 0.104984, loss_ce: 0.044486
2022-01-11 22:32:19,286 iteration 689 : loss : 0.076430, loss_ce: 0.034185
2022-01-11 22:32:20,826 iteration 690 : loss : 0.098889, loss_ce: 0.047295
2022-01-11 22:32:22,478 iteration 691 : loss : 0.076335, loss_ce: 0.032608
2022-01-11 22:32:24,112 iteration 692 : loss : 0.113726, loss_ce: 0.039169
2022-01-11 22:32:25,707 iteration 693 : loss : 0.119979, loss_ce: 0.051026
2022-01-11 22:32:27,289 iteration 694 : loss : 0.092396, loss_ce: 0.040144
2022-01-11 22:32:28,861 iteration 695 : loss : 0.086253, loss_ce: 0.027317
2022-01-11 22:32:30,413 iteration 696 : loss : 0.080195, loss_ce: 0.035420
2022-01-11 22:32:31,993 iteration 697 : loss : 0.126301, loss_ce: 0.046728
 10%|███                           | 41/400 [20:02<2:56:58, 29.58s/it]2022-01-11 22:32:33,586 iteration 698 : loss : 0.092878, loss_ce: 0.040774
2022-01-11 22:32:35,164 iteration 699 : loss : 0.074810, loss_ce: 0.024073
2022-01-11 22:32:36,747 iteration 700 : loss : 0.103262, loss_ce: 0.039145
2022-01-11 22:32:38,307 iteration 701 : loss : 0.117807, loss_ce: 0.051595
2022-01-11 22:32:39,920 iteration 702 : loss : 0.126477, loss_ce: 0.052791
2022-01-11 22:32:41,480 iteration 703 : loss : 0.114280, loss_ce: 0.049396
2022-01-11 22:32:43,037 iteration 704 : loss : 0.129732, loss_ce: 0.040458
2022-01-11 22:32:44,560 iteration 705 : loss : 0.105670, loss_ce: 0.049488
2022-01-11 22:32:46,168 iteration 706 : loss : 0.063250, loss_ce: 0.027134
2022-01-11 22:32:47,758 iteration 707 : loss : 0.091148, loss_ce: 0.049804
2022-01-11 22:32:49,345 iteration 708 : loss : 0.077518, loss_ce: 0.029520
2022-01-11 22:32:50,944 iteration 709 : loss : 0.100025, loss_ce: 0.038368
2022-01-11 22:32:52,506 iteration 710 : loss : 0.095769, loss_ce: 0.031977
2022-01-11 22:32:54,110 iteration 711 : loss : 0.172916, loss_ce: 0.058131
2022-01-11 22:32:55,740 iteration 712 : loss : 0.068877, loss_ce: 0.035242
2022-01-11 22:32:57,277 iteration 713 : loss : 0.082214, loss_ce: 0.036157
2022-01-11 22:32:58,836 iteration 714 : loss : 0.062591, loss_ce: 0.028783
 10%|███▏                          | 42/400 [20:29<2:51:35, 28.76s/it]2022-01-11 22:33:00,441 iteration 715 : loss : 0.079455, loss_ce: 0.037877
2022-01-11 22:33:02,007 iteration 716 : loss : 0.136025, loss_ce: 0.045471
2022-01-11 22:33:03,560 iteration 717 : loss : 0.067927, loss_ce: 0.029283
2022-01-11 22:33:05,039 iteration 718 : loss : 0.085035, loss_ce: 0.033135
2022-01-11 22:33:06,600 iteration 719 : loss : 0.079627, loss_ce: 0.043160
2022-01-11 22:33:08,090 iteration 720 : loss : 0.067938, loss_ce: 0.031905
2022-01-11 22:33:09,778 iteration 721 : loss : 0.101625, loss_ce: 0.040175
2022-01-11 22:33:11,345 iteration 722 : loss : 0.065963, loss_ce: 0.032062
2022-01-11 22:33:12,929 iteration 723 : loss : 0.093171, loss_ce: 0.031977
2022-01-11 22:33:14,570 iteration 724 : loss : 0.084896, loss_ce: 0.033496
2022-01-11 22:33:16,095 iteration 725 : loss : 0.064178, loss_ce: 0.028135
2022-01-11 22:33:17,676 iteration 726 : loss : 0.101314, loss_ce: 0.054282
2022-01-11 22:33:19,220 iteration 727 : loss : 0.049421, loss_ce: 0.018864
2022-01-11 22:33:20,773 iteration 728 : loss : 0.078096, loss_ce: 0.026659
2022-01-11 22:33:22,321 iteration 729 : loss : 0.099263, loss_ce: 0.042833
2022-01-11 22:33:23,919 iteration 730 : loss : 0.080450, loss_ce: 0.034181
2022-01-11 22:33:25,464 iteration 731 : loss : 0.127446, loss_ce: 0.046216
 11%|███▏                          | 43/400 [20:56<2:47:18, 28.12s/it]2022-01-11 22:33:27,117 iteration 732 : loss : 0.067708, loss_ce: 0.026302
2022-01-11 22:33:28,717 iteration 733 : loss : 0.074736, loss_ce: 0.029533
2022-01-11 22:33:30,305 iteration 734 : loss : 0.143000, loss_ce: 0.060421
2022-01-11 22:33:31,900 iteration 735 : loss : 0.102317, loss_ce: 0.032797
2022-01-11 22:33:33,497 iteration 736 : loss : 0.067247, loss_ce: 0.026133
2022-01-11 22:33:35,021 iteration 737 : loss : 0.057526, loss_ce: 0.026280
2022-01-11 22:33:36,666 iteration 738 : loss : 0.083944, loss_ce: 0.036047
2022-01-11 22:33:38,272 iteration 739 : loss : 0.082304, loss_ce: 0.039722
2022-01-11 22:33:39,824 iteration 740 : loss : 0.060230, loss_ce: 0.020941
2022-01-11 22:33:41,416 iteration 741 : loss : 0.083894, loss_ce: 0.035433
2022-01-11 22:33:43,025 iteration 742 : loss : 0.081598, loss_ce: 0.031678
2022-01-11 22:33:44,585 iteration 743 : loss : 0.091792, loss_ce: 0.035714
2022-01-11 22:33:46,168 iteration 744 : loss : 0.064888, loss_ce: 0.026498
2022-01-11 22:33:47,726 iteration 745 : loss : 0.079565, loss_ce: 0.029189
2022-01-11 22:33:49,266 iteration 746 : loss : 0.129211, loss_ce: 0.041749
2022-01-11 22:33:50,824 iteration 747 : loss : 0.071217, loss_ce: 0.031030
2022-01-11 22:33:52,415 iteration 748 : loss : 0.106252, loss_ce: 0.035421
 11%|███▎                          | 44/400 [21:23<2:44:44, 27.77s/it]2022-01-11 22:33:53,986 iteration 749 : loss : 0.100381, loss_ce: 0.033576
2022-01-11 22:33:55,540 iteration 750 : loss : 0.047667, loss_ce: 0.016389
2022-01-11 22:33:57,149 iteration 751 : loss : 0.086471, loss_ce: 0.034825
2022-01-11 22:33:58,685 iteration 752 : loss : 0.078868, loss_ce: 0.032654
2022-01-11 22:34:00,282 iteration 753 : loss : 0.060456, loss_ce: 0.022610
2022-01-11 22:34:01,849 iteration 754 : loss : 0.064361, loss_ce: 0.022225
2022-01-11 22:34:03,422 iteration 755 : loss : 0.072700, loss_ce: 0.028494
2022-01-11 22:34:04,903 iteration 756 : loss : 0.077376, loss_ce: 0.030813
2022-01-11 22:34:06,420 iteration 757 : loss : 0.072968, loss_ce: 0.026142
2022-01-11 22:34:07,998 iteration 758 : loss : 0.090774, loss_ce: 0.034801
2022-01-11 22:34:09,610 iteration 759 : loss : 0.127346, loss_ce: 0.049789
2022-01-11 22:34:11,188 iteration 760 : loss : 0.077128, loss_ce: 0.024752
2022-01-11 22:34:12,783 iteration 761 : loss : 0.115244, loss_ce: 0.039321
2022-01-11 22:34:14,270 iteration 762 : loss : 0.078715, loss_ce: 0.033326
2022-01-11 22:34:15,838 iteration 763 : loss : 0.066321, loss_ce: 0.029792
2022-01-11 22:34:17,340 iteration 764 : loss : 0.079149, loss_ce: 0.029757
2022-01-11 22:34:17,341 Training Data Eval:
2022-01-11 22:34:25,371   Average segmentation loss on training set: 0.0692
2022-01-11 22:34:25,371 Validation Data Eval:
2022-01-11 22:34:28,137   Average segmentation loss on validation set: 0.0969
2022-01-11 22:34:34,029 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-11 22:34:35,396 iteration 765 : loss : 0.071715, loss_ce: 0.031489
 11%|███▍                          | 45/400 [22:06<3:11:17, 32.33s/it]2022-01-11 22:34:36,958 iteration 766 : loss : 0.084438, loss_ce: 0.031987
2022-01-11 22:34:38,552 iteration 767 : loss : 0.081149, loss_ce: 0.035740
2022-01-11 22:34:40,044 iteration 768 : loss : 0.078568, loss_ce: 0.034233
2022-01-11 22:34:41,542 iteration 769 : loss : 0.113046, loss_ce: 0.054426
2022-01-11 22:34:43,069 iteration 770 : loss : 0.100806, loss_ce: 0.048962
2022-01-11 22:34:44,551 iteration 771 : loss : 0.073545, loss_ce: 0.038918
2022-01-11 22:34:45,955 iteration 772 : loss : 0.073350, loss_ce: 0.034955
2022-01-11 22:34:47,408 iteration 773 : loss : 0.135243, loss_ce: 0.055249
2022-01-11 22:34:48,972 iteration 774 : loss : 0.089343, loss_ce: 0.041342
2022-01-11 22:34:50,478 iteration 775 : loss : 0.063114, loss_ce: 0.027886
2022-01-11 22:34:52,035 iteration 776 : loss : 0.116346, loss_ce: 0.029910
2022-01-11 22:34:53,728 iteration 777 : loss : 0.071085, loss_ce: 0.030857
2022-01-11 22:34:55,309 iteration 778 : loss : 0.082683, loss_ce: 0.028904
2022-01-11 22:34:56,881 iteration 779 : loss : 0.115300, loss_ce: 0.042319
2022-01-11 22:34:58,548 iteration 780 : loss : 0.110842, loss_ce: 0.040666
2022-01-11 22:35:00,167 iteration 781 : loss : 0.070079, loss_ce: 0.025602
2022-01-11 22:35:01,723 iteration 782 : loss : 0.062365, loss_ce: 0.025236
 12%|███▍                          | 46/400 [22:32<3:00:07, 30.53s/it]2022-01-11 22:35:03,334 iteration 783 : loss : 0.089139, loss_ce: 0.040706
2022-01-11 22:35:04,860 iteration 784 : loss : 0.050292, loss_ce: 0.017708
2022-01-11 22:35:06,491 iteration 785 : loss : 0.085275, loss_ce: 0.025976
2022-01-11 22:35:08,057 iteration 786 : loss : 0.080106, loss_ce: 0.028480
2022-01-11 22:35:09,687 iteration 787 : loss : 0.146166, loss_ce: 0.065917
2022-01-11 22:35:11,235 iteration 788 : loss : 0.097903, loss_ce: 0.039198
2022-01-11 22:35:12,858 iteration 789 : loss : 0.089783, loss_ce: 0.040326
2022-01-11 22:35:14,504 iteration 790 : loss : 0.080426, loss_ce: 0.036357
2022-01-11 22:35:16,026 iteration 791 : loss : 0.077572, loss_ce: 0.039941
2022-01-11 22:35:17,673 iteration 792 : loss : 0.095994, loss_ce: 0.039037
2022-01-11 22:35:19,291 iteration 793 : loss : 0.114118, loss_ce: 0.046768
2022-01-11 22:35:20,919 iteration 794 : loss : 0.056525, loss_ce: 0.023478
2022-01-11 22:35:22,524 iteration 795 : loss : 0.069132, loss_ce: 0.028282
2022-01-11 22:35:24,168 iteration 796 : loss : 0.083317, loss_ce: 0.036010
2022-01-11 22:35:25,727 iteration 797 : loss : 0.059506, loss_ce: 0.024922
2022-01-11 22:35:27,298 iteration 798 : loss : 0.086972, loss_ce: 0.035432
2022-01-11 22:35:28,975 iteration 799 : loss : 0.145612, loss_ce: 0.049838
 12%|███▌                          | 47/400 [22:59<2:53:49, 29.55s/it]2022-01-11 22:35:30,597 iteration 800 : loss : 0.079661, loss_ce: 0.038543
2022-01-11 22:35:32,154 iteration 801 : loss : 0.084281, loss_ce: 0.026618
2022-01-11 22:35:33,762 iteration 802 : loss : 0.100997, loss_ce: 0.048303
2022-01-11 22:35:35,271 iteration 803 : loss : 0.050395, loss_ce: 0.019893
2022-01-11 22:35:36,923 iteration 804 : loss : 0.122325, loss_ce: 0.039623
2022-01-11 22:35:38,474 iteration 805 : loss : 0.075166, loss_ce: 0.034110
2022-01-11 22:35:39,982 iteration 806 : loss : 0.083774, loss_ce: 0.031891
2022-01-11 22:35:41,551 iteration 807 : loss : 0.069666, loss_ce: 0.035526
2022-01-11 22:35:43,119 iteration 808 : loss : 0.110881, loss_ce: 0.036474
2022-01-11 22:35:44,738 iteration 809 : loss : 0.072770, loss_ce: 0.028965
2022-01-11 22:35:46,272 iteration 810 : loss : 0.059858, loss_ce: 0.022472
2022-01-11 22:35:47,872 iteration 811 : loss : 0.098132, loss_ce: 0.041423
2022-01-11 22:35:49,402 iteration 812 : loss : 0.078344, loss_ce: 0.036225
2022-01-11 22:35:51,039 iteration 813 : loss : 0.075172, loss_ce: 0.031551
2022-01-11 22:35:52,701 iteration 814 : loss : 0.185111, loss_ce: 0.050755
2022-01-11 22:35:54,155 iteration 815 : loss : 0.070397, loss_ce: 0.032469
2022-01-11 22:35:55,782 iteration 816 : loss : 0.104960, loss_ce: 0.044712
 12%|███▌                          | 48/400 [23:26<2:48:31, 28.73s/it]2022-01-11 22:35:57,359 iteration 817 : loss : 0.082092, loss_ce: 0.031925
2022-01-11 22:35:58,915 iteration 818 : loss : 0.079538, loss_ce: 0.036182
2022-01-11 22:36:00,606 iteration 819 : loss : 0.082199, loss_ce: 0.036155
2022-01-11 22:36:02,235 iteration 820 : loss : 0.105751, loss_ce: 0.039406
2022-01-11 22:36:03,839 iteration 821 : loss : 0.114714, loss_ce: 0.043418
2022-01-11 22:36:05,411 iteration 822 : loss : 0.071590, loss_ce: 0.029540
2022-01-11 22:36:07,012 iteration 823 : loss : 0.070581, loss_ce: 0.030013
2022-01-11 22:36:08,525 iteration 824 : loss : 0.086650, loss_ce: 0.030940
2022-01-11 22:36:10,094 iteration 825 : loss : 0.144621, loss_ce: 0.040367
2022-01-11 22:36:11,606 iteration 826 : loss : 0.071209, loss_ce: 0.028595
2022-01-11 22:36:13,174 iteration 827 : loss : 0.119321, loss_ce: 0.046916
2022-01-11 22:36:14,673 iteration 828 : loss : 0.099966, loss_ce: 0.036530
2022-01-11 22:36:16,308 iteration 829 : loss : 0.089699, loss_ce: 0.038914
2022-01-11 22:36:17,866 iteration 830 : loss : 0.095819, loss_ce: 0.042164
2022-01-11 22:36:19,476 iteration 831 : loss : 0.116135, loss_ce: 0.046896
2022-01-11 22:36:21,056 iteration 832 : loss : 0.085109, loss_ce: 0.025443
2022-01-11 22:36:22,639 iteration 833 : loss : 0.071592, loss_ce: 0.030472
 12%|███▋                          | 49/400 [23:53<2:44:45, 28.16s/it]2022-01-11 22:36:24,169 iteration 834 : loss : 0.070408, loss_ce: 0.025803
2022-01-11 22:36:25,659 iteration 835 : loss : 0.053232, loss_ce: 0.021275
2022-01-11 22:36:27,294 iteration 836 : loss : 0.117997, loss_ce: 0.043631
2022-01-11 22:36:28,839 iteration 837 : loss : 0.067463, loss_ce: 0.026655
2022-01-11 22:36:30,441 iteration 838 : loss : 0.111115, loss_ce: 0.046752
2022-01-11 22:36:31,936 iteration 839 : loss : 0.091487, loss_ce: 0.037554
2022-01-11 22:36:33,442 iteration 840 : loss : 0.066054, loss_ce: 0.031798
2022-01-11 22:36:34,969 iteration 841 : loss : 0.074369, loss_ce: 0.034494
2022-01-11 22:36:36,612 iteration 842 : loss : 0.131090, loss_ce: 0.054197
2022-01-11 22:36:38,143 iteration 843 : loss : 0.078300, loss_ce: 0.029834
2022-01-11 22:36:39,757 iteration 844 : loss : 0.047469, loss_ce: 0.018758
2022-01-11 22:36:41,299 iteration 845 : loss : 0.081764, loss_ce: 0.041299
2022-01-11 22:36:42,851 iteration 846 : loss : 0.067184, loss_ce: 0.034191
2022-01-11 22:36:44,485 iteration 847 : loss : 0.090551, loss_ce: 0.030617
2022-01-11 22:36:46,046 iteration 848 : loss : 0.061424, loss_ce: 0.024958
2022-01-11 22:36:47,593 iteration 849 : loss : 0.065840, loss_ce: 0.024381
2022-01-11 22:36:47,593 Training Data Eval:
2022-01-11 22:36:55,625   Average segmentation loss on training set: 0.0606
2022-01-11 22:36:55,626 Validation Data Eval:
2022-01-11 22:36:58,391   Average segmentation loss on validation set: 0.1289
2022-01-11 22:36:59,916 iteration 850 : loss : 0.090216, loss_ce: 0.045872
 12%|███▊                          | 50/400 [24:30<3:00:14, 30.90s/it]2022-01-11 22:37:01,526 iteration 851 : loss : 0.059694, loss_ce: 0.024989
2022-01-11 22:37:03,089 iteration 852 : loss : 0.098707, loss_ce: 0.036694
2022-01-11 22:37:04,667 iteration 853 : loss : 0.107430, loss_ce: 0.038256
2022-01-11 22:37:06,262 iteration 854 : loss : 0.067611, loss_ce: 0.035238
2022-01-11 22:37:07,806 iteration 855 : loss : 0.070522, loss_ce: 0.034517
2022-01-11 22:37:09,414 iteration 856 : loss : 0.113185, loss_ce: 0.052321
2022-01-11 22:37:10,968 iteration 857 : loss : 0.093357, loss_ce: 0.039376
2022-01-11 22:37:12,616 iteration 858 : loss : 0.052148, loss_ce: 0.021276
2022-01-11 22:37:14,061 iteration 859 : loss : 0.071868, loss_ce: 0.024938
2022-01-11 22:37:15,614 iteration 860 : loss : 0.073370, loss_ce: 0.028598
2022-01-11 22:37:17,177 iteration 861 : loss : 0.084057, loss_ce: 0.026385
2022-01-11 22:37:18,752 iteration 862 : loss : 0.077946, loss_ce: 0.038257
2022-01-11 22:37:20,316 iteration 863 : loss : 0.126844, loss_ce: 0.044007
2022-01-11 22:37:21,865 iteration 864 : loss : 0.068034, loss_ce: 0.030237
2022-01-11 22:37:23,443 iteration 865 : loss : 0.071998, loss_ce: 0.031662
2022-01-11 22:37:24,975 iteration 866 : loss : 0.081572, loss_ce: 0.033944
2022-01-11 22:37:26,572 iteration 867 : loss : 0.076298, loss_ce: 0.028472
 13%|███▊                          | 51/400 [24:57<2:52:19, 29.63s/it]2022-01-11 22:37:28,164 iteration 868 : loss : 0.064194, loss_ce: 0.030127
2022-01-11 22:37:29,777 iteration 869 : loss : 0.075729, loss_ce: 0.031544
2022-01-11 22:37:31,261 iteration 870 : loss : 0.066790, loss_ce: 0.030560
2022-01-11 22:37:32,811 iteration 871 : loss : 0.060880, loss_ce: 0.032498
2022-01-11 22:37:34,352 iteration 872 : loss : 0.072924, loss_ce: 0.032155
2022-01-11 22:37:35,930 iteration 873 : loss : 0.064383, loss_ce: 0.029602
2022-01-11 22:37:37,501 iteration 874 : loss : 0.075818, loss_ce: 0.031864
2022-01-11 22:37:39,072 iteration 875 : loss : 0.094538, loss_ce: 0.034857
2022-01-11 22:37:40,640 iteration 876 : loss : 0.055685, loss_ce: 0.019565
2022-01-11 22:37:42,274 iteration 877 : loss : 0.071515, loss_ce: 0.031917
2022-01-11 22:37:43,810 iteration 878 : loss : 0.174448, loss_ce: 0.052012
2022-01-11 22:37:45,365 iteration 879 : loss : 0.139062, loss_ce: 0.046250
2022-01-11 22:37:46,892 iteration 880 : loss : 0.058810, loss_ce: 0.023257
2022-01-11 22:37:48,516 iteration 881 : loss : 0.084689, loss_ce: 0.029858
2022-01-11 22:37:50,098 iteration 882 : loss : 0.111921, loss_ce: 0.036048
2022-01-11 22:37:51,624 iteration 883 : loss : 0.070319, loss_ce: 0.025679
2022-01-11 22:37:53,222 iteration 884 : loss : 0.099662, loss_ce: 0.047356
 13%|███▉                          | 52/400 [25:24<2:46:38, 28.73s/it]2022-01-11 22:37:54,838 iteration 885 : loss : 0.083138, loss_ce: 0.039339
2022-01-11 22:37:56,408 iteration 886 : loss : 0.109367, loss_ce: 0.050211
2022-01-11 22:37:57,976 iteration 887 : loss : 0.119761, loss_ce: 0.055132
2022-01-11 22:37:59,559 iteration 888 : loss : 0.113016, loss_ce: 0.045032
2022-01-11 22:38:01,134 iteration 889 : loss : 0.174125, loss_ce: 0.049103
2022-01-11 22:38:02,679 iteration 890 : loss : 0.077337, loss_ce: 0.034489
2022-01-11 22:38:04,254 iteration 891 : loss : 0.068080, loss_ce: 0.037493
2022-01-11 22:38:05,821 iteration 892 : loss : 0.078491, loss_ce: 0.035628
2022-01-11 22:38:07,387 iteration 893 : loss : 0.080565, loss_ce: 0.037137
2022-01-11 22:38:08,928 iteration 894 : loss : 0.076454, loss_ce: 0.026484
2022-01-11 22:38:10,476 iteration 895 : loss : 0.122994, loss_ce: 0.047573
2022-01-11 22:38:12,002 iteration 896 : loss : 0.081235, loss_ce: 0.036398
2022-01-11 22:38:13,698 iteration 897 : loss : 0.107201, loss_ce: 0.038692
2022-01-11 22:38:15,233 iteration 898 : loss : 0.100508, loss_ce: 0.041555
2022-01-11 22:38:16,850 iteration 899 : loss : 0.103440, loss_ce: 0.036236
2022-01-11 22:38:18,443 iteration 900 : loss : 0.103343, loss_ce: 0.035418
2022-01-11 22:38:19,974 iteration 901 : loss : 0.090997, loss_ce: 0.038661
 13%|███▉                          | 53/400 [25:50<2:42:44, 28.14s/it]2022-01-11 22:38:21,629 iteration 902 : loss : 0.082988, loss_ce: 0.042994
2022-01-11 22:38:23,220 iteration 903 : loss : 0.090749, loss_ce: 0.030427
2022-01-11 22:38:24,782 iteration 904 : loss : 0.093776, loss_ce: 0.048668
2022-01-11 22:38:26,325 iteration 905 : loss : 0.071729, loss_ce: 0.031328
2022-01-11 22:38:27,962 iteration 906 : loss : 0.079978, loss_ce: 0.033219
2022-01-11 22:38:29,479 iteration 907 : loss : 0.050290, loss_ce: 0.020417
2022-01-11 22:38:31,005 iteration 908 : loss : 0.069654, loss_ce: 0.035926
2022-01-11 22:38:32,643 iteration 909 : loss : 0.104773, loss_ce: 0.042510
2022-01-11 22:38:34,275 iteration 910 : loss : 0.063553, loss_ce: 0.027838
2022-01-11 22:38:35,831 iteration 911 : loss : 0.062324, loss_ce: 0.024975
2022-01-11 22:38:37,426 iteration 912 : loss : 0.100341, loss_ce: 0.037286
2022-01-11 22:38:38,993 iteration 913 : loss : 0.084635, loss_ce: 0.027481
2022-01-11 22:38:40,496 iteration 914 : loss : 0.085645, loss_ce: 0.035042
2022-01-11 22:38:42,076 iteration 915 : loss : 0.088351, loss_ce: 0.032077
2022-01-11 22:38:43,695 iteration 916 : loss : 0.114344, loss_ce: 0.034922
2022-01-11 22:38:45,282 iteration 917 : loss : 0.102340, loss_ce: 0.038789
2022-01-11 22:38:46,921 iteration 918 : loss : 0.134025, loss_ce: 0.048101
 14%|████                          | 54/400 [26:17<2:40:13, 27.78s/it]2022-01-11 22:38:48,489 iteration 919 : loss : 0.073774, loss_ce: 0.026578
2022-01-11 22:38:50,106 iteration 920 : loss : 0.067994, loss_ce: 0.031839
2022-01-11 22:38:51,614 iteration 921 : loss : 0.062397, loss_ce: 0.026884
2022-01-11 22:38:53,207 iteration 922 : loss : 0.102334, loss_ce: 0.038812
2022-01-11 22:38:54,721 iteration 923 : loss : 0.066688, loss_ce: 0.026548
2022-01-11 22:38:56,303 iteration 924 : loss : 0.137539, loss_ce: 0.047686
2022-01-11 22:38:57,888 iteration 925 : loss : 0.099397, loss_ce: 0.033123
2022-01-11 22:38:59,469 iteration 926 : loss : 0.078048, loss_ce: 0.028221
2022-01-11 22:39:00,971 iteration 927 : loss : 0.130468, loss_ce: 0.033367
2022-01-11 22:39:02,602 iteration 928 : loss : 0.114543, loss_ce: 0.050987
2022-01-11 22:39:04,161 iteration 929 : loss : 0.058428, loss_ce: 0.022891
2022-01-11 22:39:05,671 iteration 930 : loss : 0.071659, loss_ce: 0.028432
2022-01-11 22:39:07,196 iteration 931 : loss : 0.114873, loss_ce: 0.060742
2022-01-11 22:39:08,754 iteration 932 : loss : 0.065105, loss_ce: 0.032376
2022-01-11 22:39:10,342 iteration 933 : loss : 0.067376, loss_ce: 0.034677
2022-01-11 22:39:11,874 iteration 934 : loss : 0.077714, loss_ce: 0.031423
2022-01-11 22:39:11,875 Training Data Eval:
2022-01-11 22:39:19,892   Average segmentation loss on training set: 0.1001
2022-01-11 22:39:19,892 Validation Data Eval:
2022-01-11 22:39:22,661   Average segmentation loss on validation set: 0.1051
2022-01-11 22:39:24,255 iteration 935 : loss : 0.093364, loss_ce: 0.044368
 14%|████▏                         | 55/400 [26:55<2:56:13, 30.65s/it]2022-01-11 22:39:25,876 iteration 936 : loss : 0.094106, loss_ce: 0.044665
2022-01-11 22:39:27,430 iteration 937 : loss : 0.118985, loss_ce: 0.046396
2022-01-11 22:39:29,016 iteration 938 : loss : 0.067328, loss_ce: 0.031863
2022-01-11 22:39:30,675 iteration 939 : loss : 0.117155, loss_ce: 0.038716
2022-01-11 22:39:32,193 iteration 940 : loss : 0.085851, loss_ce: 0.035445
2022-01-11 22:39:33,735 iteration 941 : loss : 0.080520, loss_ce: 0.027854
2022-01-11 22:39:35,304 iteration 942 : loss : 0.081293, loss_ce: 0.032579
2022-01-11 22:39:36,896 iteration 943 : loss : 0.133626, loss_ce: 0.044664
2022-01-11 22:39:38,468 iteration 944 : loss : 0.095324, loss_ce: 0.036633
2022-01-11 22:39:40,050 iteration 945 : loss : 0.111725, loss_ce: 0.045181
2022-01-11 22:39:41,612 iteration 946 : loss : 0.062880, loss_ce: 0.026730
2022-01-11 22:39:43,201 iteration 947 : loss : 0.077669, loss_ce: 0.025095
2022-01-11 22:39:44,838 iteration 948 : loss : 0.100121, loss_ce: 0.043602
2022-01-11 22:39:46,468 iteration 949 : loss : 0.096911, loss_ce: 0.044087
2022-01-11 22:39:48,116 iteration 950 : loss : 0.101462, loss_ce: 0.040222
2022-01-11 22:39:49,670 iteration 951 : loss : 0.057397, loss_ce: 0.024939
2022-01-11 22:39:51,276 iteration 952 : loss : 0.094904, loss_ce: 0.034853
 14%|████▏                         | 56/400 [27:22<2:49:28, 29.56s/it]2022-01-11 22:39:52,852 iteration 953 : loss : 0.095788, loss_ce: 0.040669
2022-01-11 22:39:54,389 iteration 954 : loss : 0.056958, loss_ce: 0.021709
2022-01-11 22:39:55,995 iteration 955 : loss : 0.064932, loss_ce: 0.024302
2022-01-11 22:39:57,600 iteration 956 : loss : 0.065358, loss_ce: 0.021643
2022-01-11 22:39:59,177 iteration 957 : loss : 0.077807, loss_ce: 0.031939
2022-01-11 22:40:00,769 iteration 958 : loss : 0.064532, loss_ce: 0.029223
2022-01-11 22:40:02,388 iteration 959 : loss : 0.100464, loss_ce: 0.047733
2022-01-11 22:40:03,970 iteration 960 : loss : 0.066460, loss_ce: 0.034248
2022-01-11 22:40:05,537 iteration 961 : loss : 0.098481, loss_ce: 0.046885
2022-01-11 22:40:07,136 iteration 962 : loss : 0.092753, loss_ce: 0.028163
2022-01-11 22:40:08,644 iteration 963 : loss : 0.064044, loss_ce: 0.029983
2022-01-11 22:40:10,232 iteration 964 : loss : 0.094251, loss_ce: 0.028923
2022-01-11 22:40:11,763 iteration 965 : loss : 0.054082, loss_ce: 0.019244
2022-01-11 22:40:13,408 iteration 966 : loss : 0.096640, loss_ce: 0.036717
2022-01-11 22:40:14,989 iteration 967 : loss : 0.074875, loss_ce: 0.032567
2022-01-11 22:40:16,484 iteration 968 : loss : 0.069045, loss_ce: 0.028449
2022-01-11 22:40:18,064 iteration 969 : loss : 0.066731, loss_ce: 0.033348
 14%|████▎                         | 57/400 [27:48<2:44:12, 28.73s/it]2022-01-11 22:40:19,634 iteration 970 : loss : 0.083119, loss_ce: 0.043315
2022-01-11 22:40:21,224 iteration 971 : loss : 0.077592, loss_ce: 0.036972
2022-01-11 22:40:22,784 iteration 972 : loss : 0.066096, loss_ce: 0.025875
2022-01-11 22:40:24,367 iteration 973 : loss : 0.072523, loss_ce: 0.030384
2022-01-11 22:40:25,935 iteration 974 : loss : 0.083673, loss_ce: 0.036577
2022-01-11 22:40:27,460 iteration 975 : loss : 0.095049, loss_ce: 0.031272
2022-01-11 22:40:29,086 iteration 976 : loss : 0.072937, loss_ce: 0.034285
2022-01-11 22:40:30,642 iteration 977 : loss : 0.065407, loss_ce: 0.029040
2022-01-11 22:40:32,238 iteration 978 : loss : 0.057640, loss_ce: 0.022354
2022-01-11 22:40:33,777 iteration 979 : loss : 0.064106, loss_ce: 0.027228
2022-01-11 22:40:35,328 iteration 980 : loss : 0.068063, loss_ce: 0.024865
2022-01-11 22:40:36,862 iteration 981 : loss : 0.028648, loss_ce: 0.011134
2022-01-11 22:40:38,451 iteration 982 : loss : 0.081428, loss_ce: 0.030139
2022-01-11 22:40:40,075 iteration 983 : loss : 0.086236, loss_ce: 0.033514
2022-01-11 22:40:41,677 iteration 984 : loss : 0.074847, loss_ce: 0.037507
2022-01-11 22:40:43,275 iteration 985 : loss : 0.064775, loss_ce: 0.026998
2022-01-11 22:40:44,815 iteration 986 : loss : 0.105202, loss_ce: 0.033107
 14%|████▎                         | 58/400 [28:15<2:40:22, 28.13s/it]2022-01-11 22:40:46,345 iteration 987 : loss : 0.055386, loss_ce: 0.020081
2022-01-11 22:40:47,902 iteration 988 : loss : 0.066345, loss_ce: 0.025666
2022-01-11 22:40:49,458 iteration 989 : loss : 0.049704, loss_ce: 0.018326
2022-01-11 22:40:51,014 iteration 990 : loss : 0.061642, loss_ce: 0.022881
2022-01-11 22:40:52,574 iteration 991 : loss : 0.085480, loss_ce: 0.033149
2022-01-11 22:40:54,111 iteration 992 : loss : 0.061661, loss_ce: 0.024862
2022-01-11 22:40:55,611 iteration 993 : loss : 0.073901, loss_ce: 0.025958
2022-01-11 22:40:57,067 iteration 994 : loss : 0.064894, loss_ce: 0.022669
2022-01-11 22:40:58,697 iteration 995 : loss : 0.077253, loss_ce: 0.035836
2022-01-11 22:41:00,283 iteration 996 : loss : 0.075539, loss_ce: 0.038553
2022-01-11 22:41:01,898 iteration 997 : loss : 0.105782, loss_ce: 0.039101
2022-01-11 22:41:03,497 iteration 998 : loss : 0.065927, loss_ce: 0.024532
2022-01-11 22:41:05,027 iteration 999 : loss : 0.072178, loss_ce: 0.031259
2022-01-11 22:41:06,642 iteration 1000 : loss : 0.088340, loss_ce: 0.047413
2022-01-11 22:41:08,183 iteration 1001 : loss : 0.056573, loss_ce: 0.022472
2022-01-11 22:41:09,808 iteration 1002 : loss : 0.103135, loss_ce: 0.049783
2022-01-11 22:41:11,318 iteration 1003 : loss : 0.154677, loss_ce: 0.044689
 15%|████▍                         | 59/400 [28:42<2:37:06, 27.64s/it]2022-01-11 22:41:12,911 iteration 1004 : loss : 0.077669, loss_ce: 0.026496
2022-01-11 22:41:14,463 iteration 1005 : loss : 0.098197, loss_ce: 0.047947
2022-01-11 22:41:16,116 iteration 1006 : loss : 0.068935, loss_ce: 0.033802
2022-01-11 22:41:17,724 iteration 1007 : loss : 0.102609, loss_ce: 0.051290
2022-01-11 22:41:19,352 iteration 1008 : loss : 0.093892, loss_ce: 0.058216
2022-01-11 22:41:20,888 iteration 1009 : loss : 0.082189, loss_ce: 0.031873
2022-01-11 22:41:22,395 iteration 1010 : loss : 0.049702, loss_ce: 0.017217
2022-01-11 22:41:23,945 iteration 1011 : loss : 0.072325, loss_ce: 0.031588
2022-01-11 22:41:25,560 iteration 1012 : loss : 0.090234, loss_ce: 0.038379
2022-01-11 22:41:27,125 iteration 1013 : loss : 0.085774, loss_ce: 0.036490
2022-01-11 22:41:28,679 iteration 1014 : loss : 0.053355, loss_ce: 0.022074
2022-01-11 22:41:30,363 iteration 1015 : loss : 0.106412, loss_ce: 0.041160
2022-01-11 22:41:31,920 iteration 1016 : loss : 0.068856, loss_ce: 0.029157
2022-01-11 22:41:33,467 iteration 1017 : loss : 0.073995, loss_ce: 0.027806
2022-01-11 22:41:35,099 iteration 1018 : loss : 0.078267, loss_ce: 0.028284
2022-01-11 22:41:36,710 iteration 1019 : loss : 0.070725, loss_ce: 0.026988
2022-01-11 22:41:36,710 Training Data Eval:
2022-01-11 22:41:44,728   Average segmentation loss on training set: 0.0735
2022-01-11 22:41:44,729 Validation Data Eval:
2022-01-11 22:41:47,495   Average segmentation loss on validation set: 0.1435
2022-01-11 22:41:49,139 iteration 1020 : loss : 0.068812, loss_ce: 0.030044
 15%|████▌                         | 60/400 [29:19<2:53:57, 30.70s/it]2022-01-11 22:41:50,732 iteration 1021 : loss : 0.078064, loss_ce: 0.028544
2022-01-11 22:41:52,387 iteration 1022 : loss : 0.064631, loss_ce: 0.028892
2022-01-11 22:41:53,925 iteration 1023 : loss : 0.059294, loss_ce: 0.022972
2022-01-11 22:41:55,470 iteration 1024 : loss : 0.049304, loss_ce: 0.018555
2022-01-11 22:41:57,085 iteration 1025 : loss : 0.072363, loss_ce: 0.035980
2022-01-11 22:41:58,682 iteration 1026 : loss : 0.116578, loss_ce: 0.033476
2022-01-11 22:42:00,240 iteration 1027 : loss : 0.052660, loss_ce: 0.016446
2022-01-11 22:42:01,882 iteration 1028 : loss : 0.096756, loss_ce: 0.027544
2022-01-11 22:42:03,501 iteration 1029 : loss : 0.073698, loss_ce: 0.025197
2022-01-11 22:42:05,072 iteration 1030 : loss : 0.086764, loss_ce: 0.034948
2022-01-11 22:42:06,740 iteration 1031 : loss : 0.086841, loss_ce: 0.039654
2022-01-11 22:42:08,328 iteration 1032 : loss : 0.084929, loss_ce: 0.040262
2022-01-11 22:42:09,898 iteration 1033 : loss : 0.094866, loss_ce: 0.040639
2022-01-11 22:42:11,438 iteration 1034 : loss : 0.060323, loss_ce: 0.025232
2022-01-11 22:42:13,058 iteration 1035 : loss : 0.074033, loss_ce: 0.031303
2022-01-11 22:42:14,689 iteration 1036 : loss : 0.068699, loss_ce: 0.025806
2022-01-11 22:42:16,185 iteration 1037 : loss : 0.053276, loss_ce: 0.021482
 15%|████▌                         | 61/400 [29:46<2:47:15, 29.60s/it]2022-01-11 22:42:17,767 iteration 1038 : loss : 0.061369, loss_ce: 0.028015
2022-01-11 22:42:19,329 iteration 1039 : loss : 0.065045, loss_ce: 0.028552
2022-01-11 22:42:20,893 iteration 1040 : loss : 0.081212, loss_ce: 0.027206
2022-01-11 22:42:22,463 iteration 1041 : loss : 0.073393, loss_ce: 0.036036
2022-01-11 22:42:24,027 iteration 1042 : loss : 0.085239, loss_ce: 0.046599
2022-01-11 22:42:25,588 iteration 1043 : loss : 0.078189, loss_ce: 0.028516
2022-01-11 22:42:27,214 iteration 1044 : loss : 0.065245, loss_ce: 0.030360
2022-01-11 22:42:28,793 iteration 1045 : loss : 0.103586, loss_ce: 0.050373
2022-01-11 22:42:30,337 iteration 1046 : loss : 0.063448, loss_ce: 0.031774
2022-01-11 22:42:31,804 iteration 1047 : loss : 0.067635, loss_ce: 0.026075
2022-01-11 22:42:33,388 iteration 1048 : loss : 0.064254, loss_ce: 0.032276
2022-01-11 22:42:35,012 iteration 1049 : loss : 0.077360, loss_ce: 0.029723
2022-01-11 22:42:36,580 iteration 1050 : loss : 0.115666, loss_ce: 0.023563
2022-01-11 22:42:38,121 iteration 1051 : loss : 0.060604, loss_ce: 0.028511
2022-01-11 22:42:39,745 iteration 1052 : loss : 0.087662, loss_ce: 0.024897
2022-01-11 22:42:41,353 iteration 1053 : loss : 0.096706, loss_ce: 0.028856
2022-01-11 22:42:42,937 iteration 1054 : loss : 0.080686, loss_ce: 0.030291
 16%|████▋                         | 62/400 [30:13<2:41:56, 28.75s/it]2022-01-11 22:42:44,549 iteration 1055 : loss : 0.056679, loss_ce: 0.013660
2022-01-11 22:42:46,045 iteration 1056 : loss : 0.054150, loss_ce: 0.026887
2022-01-11 22:42:47,648 iteration 1057 : loss : 0.090933, loss_ce: 0.027237
2022-01-11 22:42:49,239 iteration 1058 : loss : 0.100880, loss_ce: 0.036778
2022-01-11 22:42:50,836 iteration 1059 : loss : 0.076412, loss_ce: 0.029533
2022-01-11 22:42:52,343 iteration 1060 : loss : 0.066084, loss_ce: 0.021878
2022-01-11 22:42:54,012 iteration 1061 : loss : 0.083568, loss_ce: 0.040624
2022-01-11 22:42:55,595 iteration 1062 : loss : 0.091590, loss_ce: 0.031401
2022-01-11 22:42:57,284 iteration 1063 : loss : 0.079212, loss_ce: 0.035765
2022-01-11 22:42:58,877 iteration 1064 : loss : 0.091572, loss_ce: 0.029782
2022-01-11 22:43:00,525 iteration 1065 : loss : 0.066565, loss_ce: 0.031453
2022-01-11 22:43:02,058 iteration 1066 : loss : 0.046477, loss_ce: 0.017903
2022-01-11 22:43:03,596 iteration 1067 : loss : 0.070107, loss_ce: 0.028410
2022-01-11 22:43:05,208 iteration 1068 : loss : 0.064088, loss_ce: 0.026172
2022-01-11 22:43:06,802 iteration 1069 : loss : 0.066346, loss_ce: 0.031507
2022-01-11 22:43:08,459 iteration 1070 : loss : 0.132750, loss_ce: 0.052765
2022-01-11 22:43:10,041 iteration 1071 : loss : 0.080074, loss_ce: 0.035012
 16%|████▋                         | 63/400 [30:40<2:38:41, 28.26s/it]2022-01-11 22:43:11,729 iteration 1072 : loss : 0.069449, loss_ce: 0.025866
2022-01-11 22:43:13,309 iteration 1073 : loss : 0.069026, loss_ce: 0.034367
2022-01-11 22:43:14,910 iteration 1074 : loss : 0.099610, loss_ce: 0.037139
2022-01-11 22:43:16,499 iteration 1075 : loss : 0.095326, loss_ce: 0.029656
2022-01-11 22:43:18,035 iteration 1076 : loss : 0.056765, loss_ce: 0.023766
2022-01-11 22:43:19,679 iteration 1077 : loss : 0.074439, loss_ce: 0.024404
2022-01-11 22:43:21,225 iteration 1078 : loss : 0.071974, loss_ce: 0.031253
2022-01-11 22:43:22,854 iteration 1079 : loss : 0.082914, loss_ce: 0.032334
2022-01-11 22:43:24,351 iteration 1080 : loss : 0.062459, loss_ce: 0.017564
2022-01-11 22:43:25,913 iteration 1081 : loss : 0.066138, loss_ce: 0.035941
2022-01-11 22:43:27,524 iteration 1082 : loss : 0.092331, loss_ce: 0.037595
2022-01-11 22:43:29,059 iteration 1083 : loss : 0.076956, loss_ce: 0.029762
2022-01-11 22:43:30,618 iteration 1084 : loss : 0.084706, loss_ce: 0.026718
2022-01-11 22:43:32,200 iteration 1085 : loss : 0.065684, loss_ce: 0.027045
2022-01-11 22:43:33,748 iteration 1086 : loss : 0.071181, loss_ce: 0.031440
2022-01-11 22:43:35,397 iteration 1087 : loss : 0.047988, loss_ce: 0.018557
2022-01-11 22:43:36,982 iteration 1088 : loss : 0.055979, loss_ce: 0.022369
 16%|████▊                         | 64/400 [31:07<2:36:00, 27.86s/it]2022-01-11 22:43:38,563 iteration 1089 : loss : 0.109800, loss_ce: 0.041972
2022-01-11 22:43:40,103 iteration 1090 : loss : 0.073441, loss_ce: 0.023550
2022-01-11 22:43:41,712 iteration 1091 : loss : 0.060056, loss_ce: 0.028823
2022-01-11 22:43:43,287 iteration 1092 : loss : 0.065248, loss_ce: 0.029106
2022-01-11 22:43:44,857 iteration 1093 : loss : 0.047907, loss_ce: 0.021346
2022-01-11 22:43:46,456 iteration 1094 : loss : 0.054763, loss_ce: 0.024775
2022-01-11 22:43:48,058 iteration 1095 : loss : 0.130004, loss_ce: 0.047094
2022-01-11 22:43:49,640 iteration 1096 : loss : 0.047399, loss_ce: 0.021404
2022-01-11 22:43:51,161 iteration 1097 : loss : 0.062953, loss_ce: 0.022192
2022-01-11 22:43:52,698 iteration 1098 : loss : 0.055226, loss_ce: 0.028292
2022-01-11 22:43:54,291 iteration 1099 : loss : 0.057723, loss_ce: 0.024215
2022-01-11 22:43:55,844 iteration 1100 : loss : 0.057397, loss_ce: 0.024421
2022-01-11 22:43:57,391 iteration 1101 : loss : 0.062992, loss_ce: 0.023211
2022-01-11 22:43:59,014 iteration 1102 : loss : 0.069875, loss_ce: 0.019452
2022-01-11 22:44:00,558 iteration 1103 : loss : 0.110668, loss_ce: 0.044447
2022-01-11 22:44:02,078 iteration 1104 : loss : 0.090624, loss_ce: 0.037394
2022-01-11 22:44:02,078 Training Data Eval:
2022-01-11 22:44:10,102   Average segmentation loss on training set: 0.1192
2022-01-11 22:44:10,103 Validation Data Eval:
2022-01-11 22:44:12,858   Average segmentation loss on validation set: 0.2549
2022-01-11 22:44:14,441 iteration 1105 : loss : 0.067900, loss_ce: 0.031453
 16%|████▉                         | 65/400 [31:45<2:51:37, 30.74s/it]2022-01-11 22:44:16,060 iteration 1106 : loss : 0.054581, loss_ce: 0.017744
2022-01-11 22:44:17,693 iteration 1107 : loss : 0.057929, loss_ce: 0.026645
2022-01-11 22:44:19,226 iteration 1108 : loss : 0.043542, loss_ce: 0.016309
2022-01-11 22:44:20,741 iteration 1109 : loss : 0.070545, loss_ce: 0.031373
2022-01-11 22:44:22,305 iteration 1110 : loss : 0.060472, loss_ce: 0.029528
2022-01-11 22:44:23,895 iteration 1111 : loss : 0.060810, loss_ce: 0.029259
2022-01-11 22:44:25,494 iteration 1112 : loss : 0.070752, loss_ce: 0.033490
2022-01-11 22:44:27,046 iteration 1113 : loss : 0.057964, loss_ce: 0.023418
2022-01-11 22:44:28,568 iteration 1114 : loss : 0.061469, loss_ce: 0.023137
2022-01-11 22:44:30,064 iteration 1115 : loss : 0.052593, loss_ce: 0.018576
2022-01-11 22:44:31,697 iteration 1116 : loss : 0.088529, loss_ce: 0.039197
2022-01-11 22:44:33,338 iteration 1117 : loss : 0.063421, loss_ce: 0.027028
2022-01-11 22:44:34,865 iteration 1118 : loss : 0.051825, loss_ce: 0.022891
2022-01-11 22:44:36,404 iteration 1119 : loss : 0.040843, loss_ce: 0.016246
2022-01-11 22:44:37,915 iteration 1120 : loss : 0.076622, loss_ce: 0.026043
2022-01-11 22:44:39,446 iteration 1121 : loss : 0.059397, loss_ce: 0.019121
2022-01-11 22:44:40,967 iteration 1122 : loss : 0.059591, loss_ce: 0.024658
 16%|████▉                         | 66/400 [32:11<2:44:05, 29.48s/it]2022-01-11 22:44:42,575 iteration 1123 : loss : 0.089769, loss_ce: 0.037041
2022-01-11 22:44:44,109 iteration 1124 : loss : 0.051897, loss_ce: 0.023898
2022-01-11 22:44:45,719 iteration 1125 : loss : 0.067814, loss_ce: 0.027410
2022-01-11 22:44:47,398 iteration 1126 : loss : 0.059375, loss_ce: 0.026460
2022-01-11 22:44:48,996 iteration 1127 : loss : 0.065360, loss_ce: 0.022828
2022-01-11 22:44:50,584 iteration 1128 : loss : 0.059084, loss_ce: 0.017855
2022-01-11 22:44:52,153 iteration 1129 : loss : 0.036406, loss_ce: 0.014002
2022-01-11 22:44:53,782 iteration 1130 : loss : 0.059689, loss_ce: 0.032039
2022-01-11 22:44:55,414 iteration 1131 : loss : 0.119793, loss_ce: 0.033354
2022-01-11 22:44:57,006 iteration 1132 : loss : 0.067672, loss_ce: 0.029350
2022-01-11 22:44:58,603 iteration 1133 : loss : 0.058597, loss_ce: 0.022801
2022-01-11 22:45:00,192 iteration 1134 : loss : 0.075214, loss_ce: 0.030837
2022-01-11 22:45:01,723 iteration 1135 : loss : 0.049943, loss_ce: 0.017847
2022-01-11 22:45:03,305 iteration 1136 : loss : 0.056090, loss_ce: 0.022621
2022-01-11 22:45:04,868 iteration 1137 : loss : 0.050297, loss_ce: 0.019947
2022-01-11 22:45:06,526 iteration 1138 : loss : 0.078680, loss_ce: 0.032928
2022-01-11 22:45:08,153 iteration 1139 : loss : 0.082939, loss_ce: 0.040750
 17%|█████                         | 67/400 [32:38<2:39:47, 28.79s/it]2022-01-11 22:45:09,744 iteration 1140 : loss : 0.069340, loss_ce: 0.031705
2022-01-11 22:45:11,330 iteration 1141 : loss : 0.069517, loss_ce: 0.033358
2022-01-11 22:45:12,856 iteration 1142 : loss : 0.040327, loss_ce: 0.016232
2022-01-11 22:45:14,453 iteration 1143 : loss : 0.107759, loss_ce: 0.035627
2022-01-11 22:45:16,084 iteration 1144 : loss : 0.056833, loss_ce: 0.025186
2022-01-11 22:45:17,664 iteration 1145 : loss : 0.057255, loss_ce: 0.020572
2022-01-11 22:45:19,266 iteration 1146 : loss : 0.058243, loss_ce: 0.024577
2022-01-11 22:45:20,786 iteration 1147 : loss : 0.039093, loss_ce: 0.014884
2022-01-11 22:45:22,361 iteration 1148 : loss : 0.054283, loss_ce: 0.023238
2022-01-11 22:45:23,967 iteration 1149 : loss : 0.064339, loss_ce: 0.030705
2022-01-11 22:45:25,530 iteration 1150 : loss : 0.056619, loss_ce: 0.025253
2022-01-11 22:45:27,100 iteration 1151 : loss : 0.055944, loss_ce: 0.016663
2022-01-11 22:45:28,698 iteration 1152 : loss : 0.104846, loss_ce: 0.018969
2022-01-11 22:45:30,270 iteration 1153 : loss : 0.052220, loss_ce: 0.018712
2022-01-11 22:45:31,789 iteration 1154 : loss : 0.068475, loss_ce: 0.033812
2022-01-11 22:45:33,449 iteration 1155 : loss : 0.075110, loss_ce: 0.029160
2022-01-11 22:45:35,056 iteration 1156 : loss : 0.104603, loss_ce: 0.036310
 17%|█████                         | 68/400 [33:05<2:36:09, 28.22s/it]2022-01-11 22:45:36,622 iteration 1157 : loss : 0.051495, loss_ce: 0.024957
2022-01-11 22:45:38,306 iteration 1158 : loss : 0.092507, loss_ce: 0.031258
2022-01-11 22:45:39,920 iteration 1159 : loss : 0.067153, loss_ce: 0.026529
2022-01-11 22:45:41,408 iteration 1160 : loss : 0.043933, loss_ce: 0.020228
2022-01-11 22:45:42,968 iteration 1161 : loss : 0.054475, loss_ce: 0.018450
2022-01-11 22:45:44,525 iteration 1162 : loss : 0.061642, loss_ce: 0.014919
2022-01-11 22:45:46,179 iteration 1163 : loss : 0.098496, loss_ce: 0.032457
2022-01-11 22:45:47,768 iteration 1164 : loss : 0.053389, loss_ce: 0.020678
2022-01-11 22:45:49,286 iteration 1165 : loss : 0.055731, loss_ce: 0.030117
2022-01-11 22:45:50,852 iteration 1166 : loss : 0.044102, loss_ce: 0.018978
2022-01-11 22:45:52,448 iteration 1167 : loss : 0.091184, loss_ce: 0.043107
2022-01-11 22:45:53,953 iteration 1168 : loss : 0.051560, loss_ce: 0.019190
2022-01-11 22:45:55,531 iteration 1169 : loss : 0.057585, loss_ce: 0.020220
2022-01-11 22:45:57,110 iteration 1170 : loss : 0.055005, loss_ce: 0.021957
2022-01-11 22:45:58,621 iteration 1171 : loss : 0.056967, loss_ce: 0.019979
2022-01-11 22:46:00,187 iteration 1172 : loss : 0.053870, loss_ce: 0.024143
2022-01-11 22:46:01,791 iteration 1173 : loss : 0.069583, loss_ce: 0.043167
 17%|█████▏                        | 69/400 [33:32<2:33:14, 27.78s/it]2022-01-11 22:46:03,437 iteration 1174 : loss : 0.059398, loss_ce: 0.021612
2022-01-11 22:46:04,996 iteration 1175 : loss : 0.052063, loss_ce: 0.024738
2022-01-11 22:46:06,633 iteration 1176 : loss : 0.095186, loss_ce: 0.044791
2022-01-11 22:46:08,298 iteration 1177 : loss : 0.086015, loss_ce: 0.034361
2022-01-11 22:46:09,894 iteration 1178 : loss : 0.078250, loss_ce: 0.034063
2022-01-11 22:46:11,389 iteration 1179 : loss : 0.051164, loss_ce: 0.023182
2022-01-11 22:46:13,010 iteration 1180 : loss : 0.063113, loss_ce: 0.026668
2022-01-11 22:46:14,518 iteration 1181 : loss : 0.047484, loss_ce: 0.021099
2022-01-11 22:46:16,169 iteration 1182 : loss : 0.074523, loss_ce: 0.031204
2022-01-11 22:46:17,734 iteration 1183 : loss : 0.052359, loss_ce: 0.023293
2022-01-11 22:46:19,257 iteration 1184 : loss : 0.058485, loss_ce: 0.025097
2022-01-11 22:46:20,855 iteration 1185 : loss : 0.086654, loss_ce: 0.033075
2022-01-11 22:46:22,400 iteration 1186 : loss : 0.057005, loss_ce: 0.023066
2022-01-11 22:46:23,984 iteration 1187 : loss : 0.054328, loss_ce: 0.019130
2022-01-11 22:46:25,503 iteration 1188 : loss : 0.045692, loss_ce: 0.017911
2022-01-11 22:46:27,075 iteration 1189 : loss : 0.076791, loss_ce: 0.032763
2022-01-11 22:46:27,075 Training Data Eval:
2022-01-11 22:46:35,095   Average segmentation loss on training set: 0.0563
2022-01-11 22:46:35,095 Validation Data Eval:
2022-01-11 22:46:37,853   Average segmentation loss on validation set: 0.1461
2022-01-11 22:46:39,448 iteration 1190 : loss : 0.054401, loss_ce: 0.021924
 18%|█████▎                        | 70/400 [34:10<2:49:03, 30.74s/it]2022-01-11 22:46:41,031 iteration 1191 : loss : 0.075232, loss_ce: 0.036064
2022-01-11 22:46:42,559 iteration 1192 : loss : 0.052163, loss_ce: 0.021686
2022-01-11 22:46:44,139 iteration 1193 : loss : 0.065511, loss_ce: 0.022008
2022-01-11 22:46:45,680 iteration 1194 : loss : 0.097460, loss_ce: 0.035485
2022-01-11 22:46:47,286 iteration 1195 : loss : 0.061969, loss_ce: 0.020483
2022-01-11 22:46:48,792 iteration 1196 : loss : 0.077222, loss_ce: 0.025234
2022-01-11 22:46:50,387 iteration 1197 : loss : 0.058018, loss_ce: 0.029803
2022-01-11 22:46:51,953 iteration 1198 : loss : 0.053477, loss_ce: 0.028065
2022-01-11 22:46:53,549 iteration 1199 : loss : 0.087152, loss_ce: 0.036995
2022-01-11 22:46:55,147 iteration 1200 : loss : 0.072486, loss_ce: 0.025638
2022-01-11 22:46:56,695 iteration 1201 : loss : 0.045122, loss_ce: 0.021762
2022-01-11 22:46:58,303 iteration 1202 : loss : 0.051548, loss_ce: 0.023654
2022-01-11 22:46:59,868 iteration 1203 : loss : 0.061752, loss_ce: 0.026327
2022-01-11 22:47:01,417 iteration 1204 : loss : 0.075998, loss_ce: 0.035541
2022-01-11 22:47:03,031 iteration 1205 : loss : 0.076676, loss_ce: 0.046933
2022-01-11 22:47:04,660 iteration 1206 : loss : 0.123301, loss_ce: 0.035918
2022-01-11 22:47:06,196 iteration 1207 : loss : 0.076754, loss_ce: 0.024275
 18%|█████▎                        | 71/400 [34:36<2:41:59, 29.54s/it]2022-01-11 22:47:07,745 iteration 1208 : loss : 0.052240, loss_ce: 0.022543
2022-01-11 22:47:09,317 iteration 1209 : loss : 0.076802, loss_ce: 0.024037
2022-01-11 22:47:10,827 iteration 1210 : loss : 0.054145, loss_ce: 0.014996
2022-01-11 22:47:12,424 iteration 1211 : loss : 0.075790, loss_ce: 0.027135
2022-01-11 22:47:13,954 iteration 1212 : loss : 0.040676, loss_ce: 0.016784
2022-01-11 22:47:15,520 iteration 1213 : loss : 0.117012, loss_ce: 0.029183
2022-01-11 22:47:17,116 iteration 1214 : loss : 0.067530, loss_ce: 0.029288
2022-01-11 22:47:18,678 iteration 1215 : loss : 0.064322, loss_ce: 0.033185
2022-01-11 22:47:20,223 iteration 1216 : loss : 0.052943, loss_ce: 0.020514
2022-01-11 22:47:21,832 iteration 1217 : loss : 0.089926, loss_ce: 0.036154
2022-01-11 22:47:23,306 iteration 1218 : loss : 0.050113, loss_ce: 0.023131
2022-01-11 22:47:24,904 iteration 1219 : loss : 0.063557, loss_ce: 0.026552
2022-01-11 22:47:26,462 iteration 1220 : loss : 0.067647, loss_ce: 0.025908
2022-01-11 22:47:28,128 iteration 1221 : loss : 0.076671, loss_ce: 0.026252
2022-01-11 22:47:29,721 iteration 1222 : loss : 0.059487, loss_ce: 0.020217
2022-01-11 22:47:31,271 iteration 1223 : loss : 0.060131, loss_ce: 0.024961
2022-01-11 22:47:32,876 iteration 1224 : loss : 0.059691, loss_ce: 0.024844
 18%|█████▍                        | 72/400 [35:03<2:36:47, 28.68s/it]2022-01-11 22:47:34,516 iteration 1225 : loss : 0.055977, loss_ce: 0.020171
2022-01-11 22:47:36,093 iteration 1226 : loss : 0.068687, loss_ce: 0.028870
2022-01-11 22:47:37,625 iteration 1227 : loss : 0.051331, loss_ce: 0.017704
2022-01-11 22:47:39,303 iteration 1228 : loss : 0.113976, loss_ce: 0.038233
2022-01-11 22:47:40,936 iteration 1229 : loss : 0.062027, loss_ce: 0.027117
2022-01-11 22:47:42,533 iteration 1230 : loss : 0.063400, loss_ce: 0.031248
2022-01-11 22:47:44,098 iteration 1231 : loss : 0.068199, loss_ce: 0.025506
2022-01-11 22:47:45,711 iteration 1232 : loss : 0.054285, loss_ce: 0.021660
2022-01-11 22:47:47,382 iteration 1233 : loss : 0.070419, loss_ce: 0.030970
2022-01-11 22:47:48,956 iteration 1234 : loss : 0.088715, loss_ce: 0.036871
2022-01-11 22:47:50,524 iteration 1235 : loss : 0.056641, loss_ce: 0.026797
2022-01-11 22:47:52,069 iteration 1236 : loss : 0.043407, loss_ce: 0.014961
2022-01-11 22:47:53,568 iteration 1237 : loss : 0.068357, loss_ce: 0.025404
2022-01-11 22:47:55,141 iteration 1238 : loss : 0.061933, loss_ce: 0.027071
2022-01-11 22:47:56,741 iteration 1239 : loss : 0.065550, loss_ce: 0.029124
2022-01-11 22:47:58,228 iteration 1240 : loss : 0.050804, loss_ce: 0.018909
2022-01-11 22:47:59,873 iteration 1241 : loss : 0.042719, loss_ce: 0.018122
 18%|█████▍                        | 73/400 [35:30<2:33:34, 28.18s/it]2022-01-11 22:48:01,409 iteration 1242 : loss : 0.047157, loss_ce: 0.020317
2022-01-11 22:48:02,920 iteration 1243 : loss : 0.036870, loss_ce: 0.018249
2022-01-11 22:48:04,434 iteration 1244 : loss : 0.070723, loss_ce: 0.024882
2022-01-11 22:48:05,988 iteration 1245 : loss : 0.068482, loss_ce: 0.029599
2022-01-11 22:48:07,549 iteration 1246 : loss : 0.055299, loss_ce: 0.022521
2022-01-11 22:48:09,160 iteration 1247 : loss : 0.051101, loss_ce: 0.018734
2022-01-11 22:48:10,697 iteration 1248 : loss : 0.036104, loss_ce: 0.014051
2022-01-11 22:48:12,274 iteration 1249 : loss : 0.049164, loss_ce: 0.019504
2022-01-11 22:48:13,803 iteration 1250 : loss : 0.052282, loss_ce: 0.018831
2022-01-11 22:48:15,389 iteration 1251 : loss : 0.050852, loss_ce: 0.020103
2022-01-11 22:48:16,970 iteration 1252 : loss : 0.095050, loss_ce: 0.028302
2022-01-11 22:48:18,554 iteration 1253 : loss : 0.058447, loss_ce: 0.027277
2022-01-11 22:48:20,108 iteration 1254 : loss : 0.072202, loss_ce: 0.031513
2022-01-11 22:48:21,717 iteration 1255 : loss : 0.052632, loss_ce: 0.022482
2022-01-11 22:48:23,337 iteration 1256 : loss : 0.061149, loss_ce: 0.017783
2022-01-11 22:48:24,900 iteration 1257 : loss : 0.052896, loss_ce: 0.027095
2022-01-11 22:48:26,528 iteration 1258 : loss : 0.067243, loss_ce: 0.028773
 18%|█████▌                        | 74/400 [35:57<2:30:37, 27.72s/it]2022-01-11 22:48:28,124 iteration 1259 : loss : 0.046113, loss_ce: 0.015739
2022-01-11 22:48:29,717 iteration 1260 : loss : 0.063182, loss_ce: 0.022627
2022-01-11 22:48:31,286 iteration 1261 : loss : 0.064596, loss_ce: 0.027391
2022-01-11 22:48:32,811 iteration 1262 : loss : 0.037102, loss_ce: 0.016827
2022-01-11 22:48:34,429 iteration 1263 : loss : 0.039872, loss_ce: 0.013416
2022-01-11 22:48:35,931 iteration 1264 : loss : 0.045142, loss_ce: 0.022493
2022-01-11 22:48:37,456 iteration 1265 : loss : 0.048982, loss_ce: 0.019477
2022-01-11 22:48:38,957 iteration 1266 : loss : 0.045628, loss_ce: 0.021003
2022-01-11 22:48:40,596 iteration 1267 : loss : 0.056630, loss_ce: 0.021079
2022-01-11 22:48:42,136 iteration 1268 : loss : 0.136709, loss_ce: 0.039145
2022-01-11 22:48:43,736 iteration 1269 : loss : 0.055249, loss_ce: 0.020285
2022-01-11 22:48:45,238 iteration 1270 : loss : 0.068448, loss_ce: 0.022733
2022-01-11 22:48:46,793 iteration 1271 : loss : 0.067539, loss_ce: 0.032236
2022-01-11 22:48:48,337 iteration 1272 : loss : 0.064950, loss_ce: 0.032557
2022-01-11 22:48:49,938 iteration 1273 : loss : 0.073307, loss_ce: 0.036311
2022-01-11 22:48:51,522 iteration 1274 : loss : 0.063034, loss_ce: 0.023393
2022-01-11 22:48:51,522 Training Data Eval:
2022-01-11 22:48:59,540   Average segmentation loss on training set: 0.0391
2022-01-11 22:48:59,540 Validation Data Eval:
2022-01-11 22:49:02,305   Average segmentation loss on validation set: 0.0863
2022-01-11 22:49:08,145 Found new lowest validation loss at iteration 1274! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-11 22:49:09,629 iteration 1275 : loss : 0.060051, loss_ce: 0.020394
 19%|█████▋                        | 75/400 [36:40<2:55:09, 32.34s/it]2022-01-11 22:49:11,208 iteration 1276 : loss : 0.121947, loss_ce: 0.035187
2022-01-11 22:49:12,618 iteration 1277 : loss : 0.055561, loss_ce: 0.022493
2022-01-11 22:49:14,100 iteration 1278 : loss : 0.054900, loss_ce: 0.024105
2022-01-11 22:49:15,457 iteration 1279 : loss : 0.046169, loss_ce: 0.024104
2022-01-11 22:49:16,818 iteration 1280 : loss : 0.055600, loss_ce: 0.027211
2022-01-11 22:49:18,323 iteration 1281 : loss : 0.063678, loss_ce: 0.028197
2022-01-11 22:49:19,732 iteration 1282 : loss : 0.076411, loss_ce: 0.042118
2022-01-11 22:49:21,166 iteration 1283 : loss : 0.070172, loss_ce: 0.027374
2022-01-11 22:49:22,698 iteration 1284 : loss : 0.053290, loss_ce: 0.024470
2022-01-11 22:49:24,412 iteration 1285 : loss : 0.085778, loss_ce: 0.025285
2022-01-11 22:49:26,028 iteration 1286 : loss : 0.063358, loss_ce: 0.024286
2022-01-11 22:49:27,588 iteration 1287 : loss : 0.064461, loss_ce: 0.022234
2022-01-11 22:49:29,184 iteration 1288 : loss : 0.044956, loss_ce: 0.019126
2022-01-11 22:49:30,834 iteration 1289 : loss : 0.053969, loss_ce: 0.021177
2022-01-11 22:49:32,376 iteration 1290 : loss : 0.071840, loss_ce: 0.022545
2022-01-11 22:49:33,954 iteration 1291 : loss : 0.051635, loss_ce: 0.021081
2022-01-11 22:49:35,474 iteration 1292 : loss : 0.039415, loss_ce: 0.016998
 19%|█████▋                        | 76/400 [37:06<2:44:05, 30.39s/it]2022-01-11 22:49:37,076 iteration 1293 : loss : 0.074991, loss_ce: 0.033296
2022-01-11 22:49:38,603 iteration 1294 : loss : 0.049744, loss_ce: 0.020010
2022-01-11 22:49:40,144 iteration 1295 : loss : 0.037433, loss_ce: 0.015073
2022-01-11 22:49:41,702 iteration 1296 : loss : 0.038423, loss_ce: 0.013959
2022-01-11 22:49:43,260 iteration 1297 : loss : 0.052281, loss_ce: 0.021508
2022-01-11 22:49:44,831 iteration 1298 : loss : 0.040179, loss_ce: 0.016430
2022-01-11 22:49:46,356 iteration 1299 : loss : 0.057703, loss_ce: 0.020454
2022-01-11 22:49:47,973 iteration 1300 : loss : 0.065597, loss_ce: 0.017796
2022-01-11 22:49:49,551 iteration 1301 : loss : 0.064245, loss_ce: 0.035218
2022-01-11 22:49:51,172 iteration 1302 : loss : 0.072084, loss_ce: 0.022644
2022-01-11 22:49:52,700 iteration 1303 : loss : 0.068732, loss_ce: 0.030613
2022-01-11 22:49:54,308 iteration 1304 : loss : 0.112941, loss_ce: 0.028688
2022-01-11 22:49:55,882 iteration 1305 : loss : 0.057280, loss_ce: 0.028314
2022-01-11 22:49:57,512 iteration 1306 : loss : 0.058937, loss_ce: 0.022842
2022-01-11 22:49:59,147 iteration 1307 : loss : 0.057591, loss_ce: 0.018748
2022-01-11 22:50:00,725 iteration 1308 : loss : 0.047885, loss_ce: 0.027456
2022-01-11 22:50:02,306 iteration 1309 : loss : 0.075545, loss_ce: 0.020886
 19%|█████▊                        | 77/400 [37:33<2:37:50, 29.32s/it]2022-01-11 22:50:03,924 iteration 1310 : loss : 0.073269, loss_ce: 0.032150
2022-01-11 22:50:05,559 iteration 1311 : loss : 0.059729, loss_ce: 0.023001
2022-01-11 22:50:07,104 iteration 1312 : loss : 0.073761, loss_ce: 0.037969
2022-01-11 22:50:08,608 iteration 1313 : loss : 0.044032, loss_ce: 0.019468
2022-01-11 22:50:10,186 iteration 1314 : loss : 0.046490, loss_ce: 0.019679
2022-01-11 22:50:11,774 iteration 1315 : loss : 0.062682, loss_ce: 0.028112
2022-01-11 22:50:13,316 iteration 1316 : loss : 0.056251, loss_ce: 0.026710
2022-01-11 22:50:14,919 iteration 1317 : loss : 0.068810, loss_ce: 0.034133
2022-01-11 22:50:16,529 iteration 1318 : loss : 0.061328, loss_ce: 0.026627
2022-01-11 22:50:18,076 iteration 1319 : loss : 0.063371, loss_ce: 0.020964
2022-01-11 22:50:19,622 iteration 1320 : loss : 0.046730, loss_ce: 0.016844
2022-01-11 22:50:21,196 iteration 1321 : loss : 0.075611, loss_ce: 0.030071
2022-01-11 22:50:22,857 iteration 1322 : loss : 0.052515, loss_ce: 0.021465
2022-01-11 22:50:24,459 iteration 1323 : loss : 0.038969, loss_ce: 0.016176
2022-01-11 22:50:26,058 iteration 1324 : loss : 0.061441, loss_ce: 0.023050
2022-01-11 22:50:27,672 iteration 1325 : loss : 0.059163, loss_ce: 0.021852
2022-01-11 22:50:29,192 iteration 1326 : loss : 0.060862, loss_ce: 0.021407
 20%|█████▊                        | 78/400 [37:59<2:33:26, 28.59s/it]2022-01-11 22:50:30,824 iteration 1327 : loss : 0.049523, loss_ce: 0.023641
2022-01-11 22:50:32,474 iteration 1328 : loss : 0.076316, loss_ce: 0.025357
2022-01-11 22:50:33,982 iteration 1329 : loss : 0.070636, loss_ce: 0.041458
2022-01-11 22:50:35,553 iteration 1330 : loss : 0.053887, loss_ce: 0.022195
2022-01-11 22:50:37,116 iteration 1331 : loss : 0.049955, loss_ce: 0.016817
2022-01-11 22:50:38,659 iteration 1332 : loss : 0.044476, loss_ce: 0.018450
2022-01-11 22:50:40,257 iteration 1333 : loss : 0.090602, loss_ce: 0.019394
2022-01-11 22:50:41,819 iteration 1334 : loss : 0.040338, loss_ce: 0.016219
2022-01-11 22:50:43,493 iteration 1335 : loss : 0.042350, loss_ce: 0.015232
2022-01-11 22:50:45,092 iteration 1336 : loss : 0.049945, loss_ce: 0.022532
2022-01-11 22:50:46,713 iteration 1337 : loss : 0.066851, loss_ce: 0.030775
2022-01-11 22:50:48,336 iteration 1338 : loss : 0.075465, loss_ce: 0.033634
2022-01-11 22:50:49,940 iteration 1339 : loss : 0.043272, loss_ce: 0.017092
2022-01-11 22:50:51,475 iteration 1340 : loss : 0.047009, loss_ce: 0.015609
2022-01-11 22:50:53,036 iteration 1341 : loss : 0.046372, loss_ce: 0.019790
2022-01-11 22:50:54,603 iteration 1342 : loss : 0.054950, loss_ce: 0.018900
2022-01-11 22:50:56,154 iteration 1343 : loss : 0.042010, loss_ce: 0.014591
 20%|█████▉                        | 79/400 [38:26<2:30:20, 28.10s/it]2022-01-11 22:50:57,745 iteration 1344 : loss : 0.039316, loss_ce: 0.014750
2022-01-11 22:50:59,290 iteration 1345 : loss : 0.044804, loss_ce: 0.019563
2022-01-11 22:51:00,824 iteration 1346 : loss : 0.042159, loss_ce: 0.015957
2022-01-11 22:51:02,472 iteration 1347 : loss : 0.139386, loss_ce: 0.034488
2022-01-11 22:51:04,121 iteration 1348 : loss : 0.052957, loss_ce: 0.020303
2022-01-11 22:51:05,710 iteration 1349 : loss : 0.076594, loss_ce: 0.033869
2022-01-11 22:51:07,325 iteration 1350 : loss : 0.078679, loss_ce: 0.028298
2022-01-11 22:51:08,850 iteration 1351 : loss : 0.052062, loss_ce: 0.018401
2022-01-11 22:51:10,474 iteration 1352 : loss : 0.052146, loss_ce: 0.020213
2022-01-11 22:51:11,973 iteration 1353 : loss : 0.053014, loss_ce: 0.025706
2022-01-11 22:51:13,568 iteration 1354 : loss : 0.041971, loss_ce: 0.016774
2022-01-11 22:51:15,090 iteration 1355 : loss : 0.089742, loss_ce: 0.033396
2022-01-11 22:51:16,704 iteration 1356 : loss : 0.064484, loss_ce: 0.024333
2022-01-11 22:51:18,219 iteration 1357 : loss : 0.057958, loss_ce: 0.032019
2022-01-11 22:51:19,791 iteration 1358 : loss : 0.070489, loss_ce: 0.027922
2022-01-11 22:51:21,307 iteration 1359 : loss : 0.058718, loss_ce: 0.035067
2022-01-11 22:51:21,307 Training Data Eval:
2022-01-11 22:51:29,327   Average segmentation loss on training set: 0.0446
2022-01-11 22:51:29,328 Validation Data Eval:
2022-01-11 22:51:32,086   Average segmentation loss on validation set: 0.0997
2022-01-11 22:51:33,646 iteration 1360 : loss : 0.059861, loss_ce: 0.021576
 20%|██████                        | 80/400 [39:04<2:44:54, 30.92s/it]2022-01-11 22:51:35,305 iteration 1361 : loss : 0.090705, loss_ce: 0.027528
2022-01-11 22:51:36,899 iteration 1362 : loss : 0.054196, loss_ce: 0.024847
2022-01-11 22:51:38,507 iteration 1363 : loss : 0.042108, loss_ce: 0.014664
2022-01-11 22:51:40,144 iteration 1364 : loss : 0.075202, loss_ce: 0.024777
2022-01-11 22:51:41,818 iteration 1365 : loss : 0.064755, loss_ce: 0.024673
2022-01-11 22:51:43,395 iteration 1366 : loss : 0.066867, loss_ce: 0.032096
2022-01-11 22:51:44,937 iteration 1367 : loss : 0.052446, loss_ce: 0.026486
2022-01-11 22:51:46,510 iteration 1368 : loss : 0.064829, loss_ce: 0.025887
2022-01-11 22:51:48,061 iteration 1369 : loss : 0.076613, loss_ce: 0.023896
2022-01-11 22:51:49,641 iteration 1370 : loss : 0.072523, loss_ce: 0.026790
2022-01-11 22:51:51,162 iteration 1371 : loss : 0.059068, loss_ce: 0.023916
2022-01-11 22:51:52,757 iteration 1372 : loss : 0.100931, loss_ce: 0.036036
2022-01-11 22:51:54,406 iteration 1373 : loss : 0.067738, loss_ce: 0.041820
2022-01-11 22:51:55,963 iteration 1374 : loss : 0.073580, loss_ce: 0.025499
2022-01-11 22:51:57,590 iteration 1375 : loss : 0.055367, loss_ce: 0.021637
2022-01-11 22:51:59,098 iteration 1376 : loss : 0.049761, loss_ce: 0.020593
2022-01-11 22:52:00,715 iteration 1377 : loss : 0.097134, loss_ce: 0.036670
 20%|██████                        | 81/400 [39:31<2:38:14, 29.76s/it]2022-01-11 22:52:02,387 iteration 1378 : loss : 0.049356, loss_ce: 0.022347
2022-01-11 22:52:04,013 iteration 1379 : loss : 0.073880, loss_ce: 0.021886
2022-01-11 22:52:05,583 iteration 1380 : loss : 0.048254, loss_ce: 0.021790
2022-01-11 22:52:07,170 iteration 1381 : loss : 0.053418, loss_ce: 0.024290
2022-01-11 22:52:08,724 iteration 1382 : loss : 0.054482, loss_ce: 0.020583
2022-01-11 22:52:10,285 iteration 1383 : loss : 0.093919, loss_ce: 0.028983
2022-01-11 22:52:11,943 iteration 1384 : loss : 0.062315, loss_ce: 0.026097
2022-01-11 22:52:13,446 iteration 1385 : loss : 0.034150, loss_ce: 0.012283
2022-01-11 22:52:15,096 iteration 1386 : loss : 0.047993, loss_ce: 0.021335
2022-01-11 22:52:16,624 iteration 1387 : loss : 0.071695, loss_ce: 0.026402
2022-01-11 22:52:18,226 iteration 1388 : loss : 0.074625, loss_ce: 0.022039
2022-01-11 22:52:19,820 iteration 1389 : loss : 0.070342, loss_ce: 0.035289
2022-01-11 22:52:21,328 iteration 1390 : loss : 0.052915, loss_ce: 0.022114
2022-01-11 22:52:22,918 iteration 1391 : loss : 0.070157, loss_ce: 0.023572
2022-01-11 22:52:24,470 iteration 1392 : loss : 0.060216, loss_ce: 0.030077
2022-01-11 22:52:26,038 iteration 1393 : loss : 0.115853, loss_ce: 0.031651
2022-01-11 22:52:27,601 iteration 1394 : loss : 0.057402, loss_ce: 0.019334
 20%|██████▏                       | 82/400 [39:58<2:33:10, 28.90s/it]2022-01-11 22:52:29,247 iteration 1395 : loss : 0.055632, loss_ce: 0.027243
2022-01-11 22:52:30,740 iteration 1396 : loss : 0.050800, loss_ce: 0.022807
2022-01-11 22:52:32,359 iteration 1397 : loss : 0.050083, loss_ce: 0.019700
2022-01-11 22:52:33,848 iteration 1398 : loss : 0.035941, loss_ce: 0.013640
2022-01-11 22:52:35,438 iteration 1399 : loss : 0.045633, loss_ce: 0.020240
2022-01-11 22:52:37,069 iteration 1400 : loss : 0.056425, loss_ce: 0.021924
2022-01-11 22:52:38,610 iteration 1401 : loss : 0.041119, loss_ce: 0.012718
2022-01-11 22:52:40,247 iteration 1402 : loss : 0.062279, loss_ce: 0.023938
2022-01-11 22:52:41,828 iteration 1403 : loss : 0.049336, loss_ce: 0.017376
2022-01-11 22:52:43,473 iteration 1404 : loss : 0.058999, loss_ce: 0.025798
2022-01-11 22:52:44,897 iteration 1405 : loss : 0.040979, loss_ce: 0.014854
2022-01-11 22:52:46,486 iteration 1406 : loss : 0.062692, loss_ce: 0.029395
2022-01-11 22:52:48,026 iteration 1407 : loss : 0.053097, loss_ce: 0.021756
2022-01-11 22:52:49,551 iteration 1408 : loss : 0.055373, loss_ce: 0.026257
2022-01-11 22:52:51,194 iteration 1409 : loss : 0.078055, loss_ce: 0.025793
2022-01-11 22:52:52,726 iteration 1410 : loss : 0.080841, loss_ce: 0.036695
2022-01-11 22:52:54,224 iteration 1411 : loss : 0.039164, loss_ce: 0.015610
 21%|██████▏                       | 83/400 [40:25<2:29:04, 28.22s/it]2022-01-11 22:52:55,860 iteration 1412 : loss : 0.045970, loss_ce: 0.020595
2022-01-11 22:52:57,365 iteration 1413 : loss : 0.034451, loss_ce: 0.013164
2022-01-11 22:52:58,956 iteration 1414 : loss : 0.043421, loss_ce: 0.018725
2022-01-11 22:53:00,462 iteration 1415 : loss : 0.053849, loss_ce: 0.021914
2022-01-11 22:53:02,025 iteration 1416 : loss : 0.034830, loss_ce: 0.014771
2022-01-11 22:53:03,611 iteration 1417 : loss : 0.067906, loss_ce: 0.027407
2022-01-11 22:53:05,188 iteration 1418 : loss : 0.043341, loss_ce: 0.014336
2022-01-11 22:53:06,686 iteration 1419 : loss : 0.044135, loss_ce: 0.017548
2022-01-11 22:53:08,235 iteration 1420 : loss : 0.047789, loss_ce: 0.014158
2022-01-11 22:53:09,795 iteration 1421 : loss : 0.076272, loss_ce: 0.043512
2022-01-11 22:53:11,380 iteration 1422 : loss : 0.050414, loss_ce: 0.022718
2022-01-11 22:53:12,904 iteration 1423 : loss : 0.048571, loss_ce: 0.016108
2022-01-11 22:53:14,397 iteration 1424 : loss : 0.029028, loss_ce: 0.011855
2022-01-11 22:53:15,980 iteration 1425 : loss : 0.124125, loss_ce: 0.034635
2022-01-11 22:53:17,575 iteration 1426 : loss : 0.055857, loss_ce: 0.030402
2022-01-11 22:53:19,143 iteration 1427 : loss : 0.087476, loss_ce: 0.018307
2022-01-11 22:53:20,799 iteration 1428 : loss : 0.074480, loss_ce: 0.027115
 21%|██████▎                       | 84/400 [40:51<2:26:01, 27.73s/it]2022-01-11 22:53:22,424 iteration 1429 : loss : 0.069429, loss_ce: 0.038969
2022-01-11 22:53:23,975 iteration 1430 : loss : 0.041778, loss_ce: 0.015198
2022-01-11 22:53:25,498 iteration 1431 : loss : 0.065047, loss_ce: 0.024031
2022-01-11 22:53:27,059 iteration 1432 : loss : 0.046050, loss_ce: 0.019599
2022-01-11 22:53:28,555 iteration 1433 : loss : 0.049249, loss_ce: 0.020954
2022-01-11 22:53:30,200 iteration 1434 : loss : 0.056793, loss_ce: 0.024933
2022-01-11 22:53:31,798 iteration 1435 : loss : 0.071055, loss_ce: 0.023283
2022-01-11 22:53:33,425 iteration 1436 : loss : 0.067549, loss_ce: 0.031829
2022-01-11 22:53:35,001 iteration 1437 : loss : 0.049041, loss_ce: 0.019842
2022-01-11 22:53:36,564 iteration 1438 : loss : 0.044824, loss_ce: 0.017766
2022-01-11 22:53:38,143 iteration 1439 : loss : 0.088228, loss_ce: 0.036765
2022-01-11 22:53:39,652 iteration 1440 : loss : 0.046713, loss_ce: 0.016183
2022-01-11 22:53:41,228 iteration 1441 : loss : 0.055896, loss_ce: 0.024954
2022-01-11 22:53:42,790 iteration 1442 : loss : 0.070900, loss_ce: 0.024593
2022-01-11 22:53:44,416 iteration 1443 : loss : 0.063560, loss_ce: 0.025393
2022-01-11 22:53:45,983 iteration 1444 : loss : 0.058969, loss_ce: 0.013814
2022-01-11 22:53:45,983 Training Data Eval:
2022-01-11 22:53:54,011   Average segmentation loss on training set: 0.1694
2022-01-11 22:53:54,011 Validation Data Eval:
2022-01-11 22:53:56,770   Average segmentation loss on validation set: 0.2977
2022-01-11 22:53:58,385 iteration 1445 : loss : 0.077002, loss_ce: 0.037469
 21%|██████▍                       | 85/400 [41:29<2:41:05, 30.68s/it]2022-01-11 22:53:59,935 iteration 1446 : loss : 0.045913, loss_ce: 0.018443
2022-01-11 22:54:01,433 iteration 1447 : loss : 0.049964, loss_ce: 0.024194
2022-01-11 22:54:03,061 iteration 1448 : loss : 0.057897, loss_ce: 0.016049
2022-01-11 22:54:04,680 iteration 1449 : loss : 0.075406, loss_ce: 0.024432
2022-01-11 22:54:06,237 iteration 1450 : loss : 0.052359, loss_ce: 0.021469
2022-01-11 22:54:07,823 iteration 1451 : loss : 0.044111, loss_ce: 0.012768
2022-01-11 22:54:09,392 iteration 1452 : loss : 0.052916, loss_ce: 0.031249
2022-01-11 22:54:10,996 iteration 1453 : loss : 0.049485, loss_ce: 0.023531
2022-01-11 22:54:12,569 iteration 1454 : loss : 0.056460, loss_ce: 0.018495
2022-01-11 22:54:14,159 iteration 1455 : loss : 0.077954, loss_ce: 0.031992
2022-01-11 22:54:15,699 iteration 1456 : loss : 0.051424, loss_ce: 0.016063
2022-01-11 22:54:17,320 iteration 1457 : loss : 0.042850, loss_ce: 0.016788
2022-01-11 22:54:18,913 iteration 1458 : loss : 0.039446, loss_ce: 0.017167
2022-01-11 22:54:20,464 iteration 1459 : loss : 0.039458, loss_ce: 0.013883
2022-01-11 22:54:22,021 iteration 1460 : loss : 0.047092, loss_ce: 0.020195
2022-01-11 22:54:23,533 iteration 1461 : loss : 0.059804, loss_ce: 0.019424
2022-01-11 22:54:25,157 iteration 1462 : loss : 0.069590, loss_ce: 0.031519
 22%|██████▍                       | 86/400 [41:55<2:34:26, 29.51s/it]2022-01-11 22:54:26,870 iteration 1463 : loss : 0.094173, loss_ce: 0.029949
2022-01-11 22:54:28,394 iteration 1464 : loss : 0.058079, loss_ce: 0.031873
2022-01-11 22:54:30,067 iteration 1465 : loss : 0.062593, loss_ce: 0.018913
2022-01-11 22:54:31,640 iteration 1466 : loss : 0.055075, loss_ce: 0.020036
2022-01-11 22:54:33,168 iteration 1467 : loss : 0.056287, loss_ce: 0.021970
2022-01-11 22:54:34,735 iteration 1468 : loss : 0.052811, loss_ce: 0.022726
2022-01-11 22:54:36,285 iteration 1469 : loss : 0.081963, loss_ce: 0.036070
2022-01-11 22:54:37,802 iteration 1470 : loss : 0.081115, loss_ce: 0.017984
2022-01-11 22:54:39,366 iteration 1471 : loss : 0.049859, loss_ce: 0.022490
2022-01-11 22:54:40,961 iteration 1472 : loss : 0.062564, loss_ce: 0.024184
2022-01-11 22:54:42,492 iteration 1473 : loss : 0.041059, loss_ce: 0.019577
2022-01-11 22:54:44,093 iteration 1474 : loss : 0.066234, loss_ce: 0.020973
2022-01-11 22:54:45,633 iteration 1475 : loss : 0.045990, loss_ce: 0.021207
2022-01-11 22:54:47,181 iteration 1476 : loss : 0.038938, loss_ce: 0.015691
2022-01-11 22:54:48,822 iteration 1477 : loss : 0.052544, loss_ce: 0.023902
2022-01-11 22:54:50,358 iteration 1478 : loss : 0.058178, loss_ce: 0.017856
2022-01-11 22:54:51,920 iteration 1479 : loss : 0.062175, loss_ce: 0.018992
 22%|██████▌                       | 87/400 [42:22<2:29:38, 28.69s/it]2022-01-11 22:54:53,411 iteration 1480 : loss : 0.037514, loss_ce: 0.012274
2022-01-11 22:54:55,060 iteration 1481 : loss : 0.055737, loss_ce: 0.022098
2022-01-11 22:54:56,618 iteration 1482 : loss : 0.051564, loss_ce: 0.021471
2022-01-11 22:54:58,238 iteration 1483 : loss : 0.050688, loss_ce: 0.018240
2022-01-11 22:54:59,831 iteration 1484 : loss : 0.045680, loss_ce: 0.013329
2022-01-11 22:55:01,430 iteration 1485 : loss : 0.059701, loss_ce: 0.028811
2022-01-11 22:55:02,993 iteration 1486 : loss : 0.049842, loss_ce: 0.016186
2022-01-11 22:55:04,547 iteration 1487 : loss : 0.044479, loss_ce: 0.018239
2022-01-11 22:55:06,057 iteration 1488 : loss : 0.043743, loss_ce: 0.019078
2022-01-11 22:55:07,591 iteration 1489 : loss : 0.048049, loss_ce: 0.020988
2022-01-11 22:55:09,179 iteration 1490 : loss : 0.057088, loss_ce: 0.019443
2022-01-11 22:55:10,669 iteration 1491 : loss : 0.065134, loss_ce: 0.020248
2022-01-11 22:55:12,187 iteration 1492 : loss : 0.035445, loss_ce: 0.015572
2022-01-11 22:55:13,780 iteration 1493 : loss : 0.039803, loss_ce: 0.017368
2022-01-11 22:55:15,336 iteration 1494 : loss : 0.053192, loss_ce: 0.017191
2022-01-11 22:55:16,836 iteration 1495 : loss : 0.043753, loss_ce: 0.011867
2022-01-11 22:55:18,448 iteration 1496 : loss : 0.044719, loss_ce: 0.020835
 22%|██████▌                       | 88/400 [42:49<2:25:48, 28.04s/it]2022-01-11 22:55:19,987 iteration 1497 : loss : 0.032648, loss_ce: 0.013006
2022-01-11 22:55:21,579 iteration 1498 : loss : 0.086641, loss_ce: 0.035598
2022-01-11 22:55:23,083 iteration 1499 : loss : 0.059100, loss_ce: 0.022391
2022-01-11 22:55:24,649 iteration 1500 : loss : 0.041465, loss_ce: 0.018285
2022-01-11 22:55:26,199 iteration 1501 : loss : 0.038297, loss_ce: 0.015734
2022-01-11 22:55:27,834 iteration 1502 : loss : 0.095503, loss_ce: 0.022817
2022-01-11 22:55:29,518 iteration 1503 : loss : 0.088349, loss_ce: 0.029426
2022-01-11 22:55:31,062 iteration 1504 : loss : 0.039674, loss_ce: 0.016991
2022-01-11 22:55:32,646 iteration 1505 : loss : 0.047621, loss_ce: 0.019003
2022-01-11 22:55:34,236 iteration 1506 : loss : 0.042363, loss_ce: 0.017050
2022-01-11 22:55:35,830 iteration 1507 : loss : 0.068253, loss_ce: 0.019556
2022-01-11 22:55:37,428 iteration 1508 : loss : 0.054397, loss_ce: 0.025876
2022-01-11 22:55:39,001 iteration 1509 : loss : 0.042691, loss_ce: 0.016381
2022-01-11 22:55:40,643 iteration 1510 : loss : 0.053939, loss_ce: 0.016198
2022-01-11 22:55:42,268 iteration 1511 : loss : 0.050498, loss_ce: 0.022016
2022-01-11 22:55:43,847 iteration 1512 : loss : 0.062965, loss_ce: 0.017250
2022-01-11 22:55:45,425 iteration 1513 : loss : 0.053746, loss_ce: 0.018126
 22%|██████▋                       | 89/400 [43:16<2:23:40, 27.72s/it]2022-01-11 22:55:47,077 iteration 1514 : loss : 0.051876, loss_ce: 0.024173
2022-01-11 22:55:48,592 iteration 1515 : loss : 0.044994, loss_ce: 0.016659
2022-01-11 22:55:50,162 iteration 1516 : loss : 0.060640, loss_ce: 0.024734
2022-01-11 22:55:51,656 iteration 1517 : loss : 0.040919, loss_ce: 0.014982
2022-01-11 22:55:53,251 iteration 1518 : loss : 0.050946, loss_ce: 0.015430
2022-01-11 22:55:54,886 iteration 1519 : loss : 0.063516, loss_ce: 0.021091
2022-01-11 22:55:56,497 iteration 1520 : loss : 0.052024, loss_ce: 0.019999
2022-01-11 22:55:58,076 iteration 1521 : loss : 0.061429, loss_ce: 0.018186
2022-01-11 22:55:59,610 iteration 1522 : loss : 0.045758, loss_ce: 0.017927
2022-01-11 22:56:01,158 iteration 1523 : loss : 0.046267, loss_ce: 0.014307
2022-01-11 22:56:02,731 iteration 1524 : loss : 0.049666, loss_ce: 0.018904
2022-01-11 22:56:04,237 iteration 1525 : loss : 0.049098, loss_ce: 0.018994
2022-01-11 22:56:05,830 iteration 1526 : loss : 0.063794, loss_ce: 0.028751
2022-01-11 22:56:07,427 iteration 1527 : loss : 0.048053, loss_ce: 0.022877
2022-01-11 22:56:08,959 iteration 1528 : loss : 0.037408, loss_ce: 0.015535
2022-01-11 22:56:10,487 iteration 1529 : loss : 0.051409, loss_ce: 0.019125
2022-01-11 22:56:10,488 Training Data Eval:
2022-01-11 22:56:18,498   Average segmentation loss on training set: 0.0352
2022-01-11 22:56:18,498 Validation Data Eval:
2022-01-11 22:56:21,263   Average segmentation loss on validation set: 0.0931
2022-01-11 22:56:22,870 iteration 1530 : loss : 0.081938, loss_ce: 0.045194
 22%|██████▊                       | 90/400 [43:53<2:38:17, 30.64s/it]2022-01-11 22:56:24,502 iteration 1531 : loss : 0.041701, loss_ce: 0.016746
2022-01-11 22:56:26,067 iteration 1532 : loss : 0.047500, loss_ce: 0.016222
2022-01-11 22:56:27,591 iteration 1533 : loss : 0.044632, loss_ce: 0.015580
2022-01-11 22:56:29,138 iteration 1534 : loss : 0.051005, loss_ce: 0.024257
2022-01-11 22:56:30,758 iteration 1535 : loss : 0.045656, loss_ce: 0.016657
2022-01-11 22:56:32,384 iteration 1536 : loss : 0.054857, loss_ce: 0.025583
2022-01-11 22:56:33,957 iteration 1537 : loss : 0.054715, loss_ce: 0.018656
2022-01-11 22:56:35,555 iteration 1538 : loss : 0.052056, loss_ce: 0.020972
2022-01-11 22:56:37,130 iteration 1539 : loss : 0.044369, loss_ce: 0.021075
2022-01-11 22:56:38,695 iteration 1540 : loss : 0.054450, loss_ce: 0.026038
2022-01-11 22:56:40,238 iteration 1541 : loss : 0.030295, loss_ce: 0.013782
2022-01-11 22:56:41,763 iteration 1542 : loss : 0.032005, loss_ce: 0.013357
2022-01-11 22:56:43,310 iteration 1543 : loss : 0.073618, loss_ce: 0.024548
2022-01-11 22:56:44,814 iteration 1544 : loss : 0.048582, loss_ce: 0.022553
2022-01-11 22:56:46,491 iteration 1545 : loss : 0.059043, loss_ce: 0.022582
2022-01-11 22:56:48,010 iteration 1546 : loss : 0.090070, loss_ce: 0.030517
2022-01-11 22:56:49,655 iteration 1547 : loss : 0.053207, loss_ce: 0.021740
 23%|██████▊                       | 91/400 [44:20<2:31:49, 29.48s/it]2022-01-11 22:56:51,282 iteration 1548 : loss : 0.063270, loss_ce: 0.031257
2022-01-11 22:56:52,764 iteration 1549 : loss : 0.040499, loss_ce: 0.016112
2022-01-11 22:56:54,298 iteration 1550 : loss : 0.053309, loss_ce: 0.021039
2022-01-11 22:56:55,763 iteration 1551 : loss : 0.036256, loss_ce: 0.012960
2022-01-11 22:56:57,345 iteration 1552 : loss : 0.039263, loss_ce: 0.013916
2022-01-11 22:56:58,963 iteration 1553 : loss : 0.057235, loss_ce: 0.023518
2022-01-11 22:57:00,559 iteration 1554 : loss : 0.042849, loss_ce: 0.017822
2022-01-11 22:57:02,107 iteration 1555 : loss : 0.064728, loss_ce: 0.027582
2022-01-11 22:57:03,700 iteration 1556 : loss : 0.043095, loss_ce: 0.012644
2022-01-11 22:57:05,240 iteration 1557 : loss : 0.052585, loss_ce: 0.021894
2022-01-11 22:57:06,802 iteration 1558 : loss : 0.040833, loss_ce: 0.019913
2022-01-11 22:57:08,383 iteration 1559 : loss : 0.056111, loss_ce: 0.019902
2022-01-11 22:57:09,899 iteration 1560 : loss : 0.040573, loss_ce: 0.018912
2022-01-11 22:57:11,498 iteration 1561 : loss : 0.037173, loss_ce: 0.013621
2022-01-11 22:57:13,026 iteration 1562 : loss : 0.064431, loss_ce: 0.016517
2022-01-11 22:57:14,638 iteration 1563 : loss : 0.041400, loss_ce: 0.019166
2022-01-11 22:57:16,248 iteration 1564 : loss : 0.040691, loss_ce: 0.018167
 23%|██████▉                       | 92/400 [44:47<2:26:52, 28.61s/it]2022-01-11 22:57:17,830 iteration 1565 : loss : 0.057027, loss_ce: 0.027168
2022-01-11 22:57:19,398 iteration 1566 : loss : 0.037986, loss_ce: 0.014140
2022-01-11 22:57:21,002 iteration 1567 : loss : 0.049522, loss_ce: 0.015266
2022-01-11 22:57:22,549 iteration 1568 : loss : 0.067207, loss_ce: 0.021890
2022-01-11 22:57:24,150 iteration 1569 : loss : 0.047548, loss_ce: 0.020337
2022-01-11 22:57:25,725 iteration 1570 : loss : 0.040267, loss_ce: 0.020084
2022-01-11 22:57:27,258 iteration 1571 : loss : 0.072009, loss_ce: 0.014552
2022-01-11 22:57:28,888 iteration 1572 : loss : 0.037583, loss_ce: 0.013935
2022-01-11 22:57:30,484 iteration 1573 : loss : 0.052003, loss_ce: 0.027234
2022-01-11 22:57:32,001 iteration 1574 : loss : 0.028130, loss_ce: 0.012944
2022-01-11 22:57:33,466 iteration 1575 : loss : 0.046161, loss_ce: 0.017885
2022-01-11 22:57:35,022 iteration 1576 : loss : 0.049783, loss_ce: 0.026184
2022-01-11 22:57:36,578 iteration 1577 : loss : 0.028101, loss_ce: 0.011123
2022-01-11 22:57:38,142 iteration 1578 : loss : 0.051646, loss_ce: 0.018067
2022-01-11 22:57:39,740 iteration 1579 : loss : 0.033358, loss_ce: 0.012317
2022-01-11 22:57:41,258 iteration 1580 : loss : 0.046675, loss_ce: 0.016904
2022-01-11 22:57:42,815 iteration 1581 : loss : 0.040743, loss_ce: 0.015443
 23%|██████▉                       | 93/400 [45:13<2:23:15, 28.00s/it]2022-01-11 22:57:44,476 iteration 1582 : loss : 0.053256, loss_ce: 0.017918
2022-01-11 22:57:46,037 iteration 1583 : loss : 0.052045, loss_ce: 0.014688
2022-01-11 22:57:47,616 iteration 1584 : loss : 0.045881, loss_ce: 0.022152
2022-01-11 22:57:49,178 iteration 1585 : loss : 0.051201, loss_ce: 0.016611
2022-01-11 22:57:50,727 iteration 1586 : loss : 0.072013, loss_ce: 0.031328
2022-01-11 22:57:52,194 iteration 1587 : loss : 0.034755, loss_ce: 0.015763
2022-01-11 22:57:53,774 iteration 1588 : loss : 0.052508, loss_ce: 0.027450
2022-01-11 22:57:55,334 iteration 1589 : loss : 0.045098, loss_ce: 0.018355
2022-01-11 22:57:57,012 iteration 1590 : loss : 0.093334, loss_ce: 0.038668
2022-01-11 22:57:58,527 iteration 1591 : loss : 0.041811, loss_ce: 0.014900
2022-01-11 22:58:00,084 iteration 1592 : loss : 0.071180, loss_ce: 0.030084
2022-01-11 22:58:01,748 iteration 1593 : loss : 0.056175, loss_ce: 0.023543
2022-01-11 22:58:03,401 iteration 1594 : loss : 0.056239, loss_ce: 0.018813
2022-01-11 22:58:04,953 iteration 1595 : loss : 0.037693, loss_ce: 0.014308
2022-01-11 22:58:06,539 iteration 1596 : loss : 0.064124, loss_ce: 0.030903
2022-01-11 22:58:08,115 iteration 1597 : loss : 0.094577, loss_ce: 0.036123
2022-01-11 22:58:09,704 iteration 1598 : loss : 0.048337, loss_ce: 0.017575
 24%|███████                       | 94/400 [45:40<2:21:06, 27.67s/it]2022-01-11 22:58:11,277 iteration 1599 : loss : 0.046940, loss_ce: 0.020653
2022-01-11 22:58:12,838 iteration 1600 : loss : 0.058812, loss_ce: 0.022924
2022-01-11 22:58:14,460 iteration 1601 : loss : 0.067812, loss_ce: 0.029209
2022-01-11 22:58:16,147 iteration 1602 : loss : 0.077692, loss_ce: 0.029000
2022-01-11 22:58:17,708 iteration 1603 : loss : 0.046730, loss_ce: 0.016311
2022-01-11 22:58:19,242 iteration 1604 : loss : 0.039371, loss_ce: 0.015916
2022-01-11 22:58:20,837 iteration 1605 : loss : 0.049685, loss_ce: 0.019072
2022-01-11 22:58:22,376 iteration 1606 : loss : 0.061856, loss_ce: 0.030107
2022-01-11 22:58:23,918 iteration 1607 : loss : 0.035056, loss_ce: 0.014220
2022-01-11 22:58:25,550 iteration 1608 : loss : 0.034212, loss_ce: 0.011402
2022-01-11 22:58:27,108 iteration 1609 : loss : 0.058023, loss_ce: 0.029112
2022-01-11 22:58:28,688 iteration 1610 : loss : 0.036850, loss_ce: 0.014016
2022-01-11 22:58:30,347 iteration 1611 : loss : 0.060125, loss_ce: 0.018649
2022-01-11 22:58:31,915 iteration 1612 : loss : 0.036417, loss_ce: 0.016821
2022-01-11 22:58:33,351 iteration 1613 : loss : 0.050942, loss_ce: 0.023499
2022-01-11 22:58:34,940 iteration 1614 : loss : 0.042900, loss_ce: 0.014415
2022-01-11 22:58:34,940 Training Data Eval:
2022-01-11 22:58:42,937   Average segmentation loss on training set: 0.0414
2022-01-11 22:58:42,937 Validation Data Eval:
2022-01-11 22:58:45,690   Average segmentation loss on validation set: 0.0636
2022-01-11 22:58:51,530 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-11 22:58:53,022 iteration 1615 : loss : 0.039515, loss_ce: 0.014503
 24%|███████▏                      | 95/400 [46:23<2:44:30, 32.36s/it]2022-01-11 22:58:54,553 iteration 1616 : loss : 0.069132, loss_ce: 0.030106
2022-01-11 22:58:56,026 iteration 1617 : loss : 0.054241, loss_ce: 0.016372
2022-01-11 22:58:57,410 iteration 1618 : loss : 0.042838, loss_ce: 0.018856
2022-01-11 22:58:58,799 iteration 1619 : loss : 0.030727, loss_ce: 0.010053
2022-01-11 22:59:00,163 iteration 1620 : loss : 0.040600, loss_ce: 0.010484
2022-01-11 22:59:01,622 iteration 1621 : loss : 0.090376, loss_ce: 0.050993
2022-01-11 22:59:03,203 iteration 1622 : loss : 0.061866, loss_ce: 0.020358
2022-01-11 22:59:04,707 iteration 1623 : loss : 0.057600, loss_ce: 0.020544
2022-01-11 22:59:06,245 iteration 1624 : loss : 0.038601, loss_ce: 0.014254
2022-01-11 22:59:07,802 iteration 1625 : loss : 0.044799, loss_ce: 0.018063
2022-01-11 22:59:09,351 iteration 1626 : loss : 0.045528, loss_ce: 0.019536
2022-01-11 22:59:10,916 iteration 1627 : loss : 0.063431, loss_ce: 0.023771
2022-01-11 22:59:12,559 iteration 1628 : loss : 0.066054, loss_ce: 0.029797
2022-01-11 22:59:14,119 iteration 1629 : loss : 0.039195, loss_ce: 0.015020
2022-01-11 22:59:15,636 iteration 1630 : loss : 0.041208, loss_ce: 0.015890
2022-01-11 22:59:17,316 iteration 1631 : loss : 0.063394, loss_ce: 0.027747
2022-01-11 22:59:18,957 iteration 1632 : loss : 0.067091, loss_ce: 0.027999
 24%|███████▏                      | 96/400 [46:49<2:34:11, 30.43s/it]2022-01-11 22:59:20,619 iteration 1633 : loss : 0.046872, loss_ce: 0.018549
2022-01-11 22:59:22,212 iteration 1634 : loss : 0.051489, loss_ce: 0.018000
2022-01-11 22:59:23,788 iteration 1635 : loss : 0.040708, loss_ce: 0.020141
2022-01-11 22:59:25,442 iteration 1636 : loss : 0.048839, loss_ce: 0.024165
2022-01-11 22:59:27,036 iteration 1637 : loss : 0.057030, loss_ce: 0.026169
2022-01-11 22:59:28,588 iteration 1638 : loss : 0.045262, loss_ce: 0.017194
2022-01-11 22:59:30,248 iteration 1639 : loss : 0.081979, loss_ce: 0.018651
2022-01-11 22:59:31,848 iteration 1640 : loss : 0.086888, loss_ce: 0.027420
2022-01-11 22:59:33,356 iteration 1641 : loss : 0.032343, loss_ce: 0.014874
2022-01-11 22:59:34,913 iteration 1642 : loss : 0.034931, loss_ce: 0.013157
2022-01-11 22:59:36,505 iteration 1643 : loss : 0.032704, loss_ce: 0.009552
2022-01-11 22:59:38,049 iteration 1644 : loss : 0.059054, loss_ce: 0.021466
2022-01-11 22:59:39,601 iteration 1645 : loss : 0.043915, loss_ce: 0.014780
2022-01-11 22:59:41,162 iteration 1646 : loss : 0.045246, loss_ce: 0.023537
2022-01-11 22:59:42,710 iteration 1647 : loss : 0.047375, loss_ce: 0.015894
2022-01-11 22:59:44,291 iteration 1648 : loss : 0.052467, loss_ce: 0.023153
2022-01-11 22:59:45,813 iteration 1649 : loss : 0.050143, loss_ce: 0.021136
 24%|███████▎                      | 97/400 [47:16<2:28:16, 29.36s/it]2022-01-11 22:59:47,448 iteration 1650 : loss : 0.045357, loss_ce: 0.020295
2022-01-11 22:59:49,129 iteration 1651 : loss : 0.074262, loss_ce: 0.027003
2022-01-11 22:59:50,640 iteration 1652 : loss : 0.076467, loss_ce: 0.024361
2022-01-11 22:59:52,162 iteration 1653 : loss : 0.049358, loss_ce: 0.016457
2022-01-11 22:59:53,761 iteration 1654 : loss : 0.103006, loss_ce: 0.024799
2022-01-11 22:59:55,379 iteration 1655 : loss : 0.053428, loss_ce: 0.018679
2022-01-11 22:59:56,937 iteration 1656 : loss : 0.049585, loss_ce: 0.020667
2022-01-11 22:59:58,606 iteration 1657 : loss : 0.062204, loss_ce: 0.020563
2022-01-11 23:00:00,233 iteration 1658 : loss : 0.057719, loss_ce: 0.031037
2022-01-11 23:00:01,764 iteration 1659 : loss : 0.034310, loss_ce: 0.011785
2022-01-11 23:00:03,447 iteration 1660 : loss : 0.105600, loss_ce: 0.049615
2022-01-11 23:00:05,110 iteration 1661 : loss : 0.039198, loss_ce: 0.013225
2022-01-11 23:00:06,686 iteration 1662 : loss : 0.039969, loss_ce: 0.014559
2022-01-11 23:00:08,152 iteration 1663 : loss : 0.037975, loss_ce: 0.019359
2022-01-11 23:00:09,807 iteration 1664 : loss : 0.057893, loss_ce: 0.022452
2022-01-11 23:00:11,426 iteration 1665 : loss : 0.042444, loss_ce: 0.015397
2022-01-11 23:00:13,058 iteration 1666 : loss : 0.057602, loss_ce: 0.023952
 24%|███████▎                      | 98/400 [47:43<2:24:35, 28.73s/it]2022-01-11 23:00:14,702 iteration 1667 : loss : 0.103083, loss_ce: 0.035392
2022-01-11 23:00:16,283 iteration 1668 : loss : 0.058620, loss_ce: 0.020361
2022-01-11 23:00:17,816 iteration 1669 : loss : 0.036995, loss_ce: 0.013715
2022-01-11 23:00:19,407 iteration 1670 : loss : 0.048889, loss_ce: 0.024750
2022-01-11 23:00:20,970 iteration 1671 : loss : 0.038331, loss_ce: 0.014422
2022-01-11 23:00:22,596 iteration 1672 : loss : 0.046985, loss_ce: 0.018476
2022-01-11 23:00:24,185 iteration 1673 : loss : 0.047793, loss_ce: 0.020130
2022-01-11 23:00:25,823 iteration 1674 : loss : 0.061324, loss_ce: 0.028435
2022-01-11 23:00:27,361 iteration 1675 : loss : 0.062344, loss_ce: 0.020379
2022-01-11 23:00:28,912 iteration 1676 : loss : 0.072101, loss_ce: 0.025845
2022-01-11 23:00:30,552 iteration 1677 : loss : 0.062572, loss_ce: 0.027669
2022-01-11 23:00:32,026 iteration 1678 : loss : 0.039321, loss_ce: 0.016485
2022-01-11 23:00:33,568 iteration 1679 : loss : 0.044700, loss_ce: 0.014745
2022-01-11 23:00:35,171 iteration 1680 : loss : 0.037940, loss_ce: 0.017896
2022-01-11 23:00:36,768 iteration 1681 : loss : 0.049143, loss_ce: 0.027540
2022-01-11 23:00:38,370 iteration 1682 : loss : 0.045787, loss_ce: 0.016146
2022-01-11 23:00:39,888 iteration 1683 : loss : 0.026819, loss_ce: 0.008924
 25%|███████▍                      | 99/400 [48:10<2:21:15, 28.16s/it]2022-01-11 23:00:41,518 iteration 1684 : loss : 0.056765, loss_ce: 0.021681
2022-01-11 23:00:43,072 iteration 1685 : loss : 0.049714, loss_ce: 0.017914
2022-01-11 23:00:44,620 iteration 1686 : loss : 0.033097, loss_ce: 0.012192
2022-01-11 23:00:46,136 iteration 1687 : loss : 0.050692, loss_ce: 0.022392
2022-01-11 23:00:47,821 iteration 1688 : loss : 0.034956, loss_ce: 0.014345
2022-01-11 23:00:49,406 iteration 1689 : loss : 0.064298, loss_ce: 0.026866
2022-01-11 23:00:51,022 iteration 1690 : loss : 0.059835, loss_ce: 0.027582
2022-01-11 23:00:52,526 iteration 1691 : loss : 0.107936, loss_ce: 0.026610
2022-01-11 23:00:54,122 iteration 1692 : loss : 0.077632, loss_ce: 0.025602
2022-01-11 23:00:55,585 iteration 1693 : loss : 0.041596, loss_ce: 0.016589
2022-01-11 23:00:57,180 iteration 1694 : loss : 0.058495, loss_ce: 0.025115
2022-01-11 23:00:58,818 iteration 1695 : loss : 0.055050, loss_ce: 0.021289
2022-01-11 23:01:00,450 iteration 1696 : loss : 0.089148, loss_ce: 0.028397
2022-01-11 23:01:02,009 iteration 1697 : loss : 0.058815, loss_ce: 0.019143
2022-01-11 23:01:03,620 iteration 1698 : loss : 0.055776, loss_ce: 0.021985
2022-01-11 23:01:05,192 iteration 1699 : loss : 0.052162, loss_ce: 0.026410
2022-01-11 23:01:05,192 Training Data Eval:
2022-01-11 23:01:13,217   Average segmentation loss on training set: 0.0400
2022-01-11 23:01:13,217 Validation Data Eval:
2022-01-11 23:01:15,975   Average segmentation loss on validation set: 0.1041
2022-01-11 23:01:17,528 iteration 1700 : loss : 0.065295, loss_ce: 0.023027
 25%|███████▎                     | 100/400 [48:48<2:35:00, 31.00s/it]2022-01-11 23:01:19,145 iteration 1701 : loss : 0.049228, loss_ce: 0.022159
2022-01-11 23:01:20,705 iteration 1702 : loss : 0.052136, loss_ce: 0.023082
2022-01-11 23:01:22,359 iteration 1703 : loss : 0.062084, loss_ce: 0.026352
2022-01-11 23:01:23,994 iteration 1704 : loss : 0.065427, loss_ce: 0.024273
2022-01-11 23:01:25,560 iteration 1705 : loss : 0.067557, loss_ce: 0.029961
2022-01-11 23:01:27,174 iteration 1706 : loss : 0.077001, loss_ce: 0.024840
2022-01-11 23:01:28,727 iteration 1707 : loss : 0.065368, loss_ce: 0.031169
2022-01-11 23:01:30,289 iteration 1708 : loss : 0.046816, loss_ce: 0.018072
2022-01-11 23:01:32,004 iteration 1709 : loss : 0.055122, loss_ce: 0.021321
2022-01-11 23:01:33,528 iteration 1710 : loss : 0.046212, loss_ce: 0.020707
2022-01-11 23:01:35,111 iteration 1711 : loss : 0.117340, loss_ce: 0.050044
2022-01-11 23:01:36,690 iteration 1712 : loss : 0.076888, loss_ce: 0.023550
2022-01-11 23:01:38,265 iteration 1713 : loss : 0.081502, loss_ce: 0.032757
2022-01-11 23:01:39,821 iteration 1714 : loss : 0.040613, loss_ce: 0.017782
2022-01-11 23:01:41,458 iteration 1715 : loss : 0.083446, loss_ce: 0.028873
2022-01-11 23:01:43,015 iteration 1716 : loss : 0.068900, loss_ce: 0.023900
2022-01-11 23:01:44,606 iteration 1717 : loss : 0.048135, loss_ce: 0.020002
 25%|███████▎                     | 101/400 [49:15<2:28:37, 29.83s/it]2022-01-11 23:01:46,248 iteration 1718 : loss : 0.049272, loss_ce: 0.017629
2022-01-11 23:01:47,883 iteration 1719 : loss : 0.047121, loss_ce: 0.014475
2022-01-11 23:01:49,490 iteration 1720 : loss : 0.051855, loss_ce: 0.019487
2022-01-11 23:01:51,052 iteration 1721 : loss : 0.105279, loss_ce: 0.045075
2022-01-11 23:01:52,577 iteration 1722 : loss : 0.035988, loss_ce: 0.015791
2022-01-11 23:01:54,123 iteration 1723 : loss : 0.043400, loss_ce: 0.018183
2022-01-11 23:01:55,628 iteration 1724 : loss : 0.040489, loss_ce: 0.019417
2022-01-11 23:01:57,157 iteration 1725 : loss : 0.041110, loss_ce: 0.017349
2022-01-11 23:01:58,839 iteration 1726 : loss : 0.051431, loss_ce: 0.020991
2022-01-11 23:02:00,460 iteration 1727 : loss : 0.044868, loss_ce: 0.015413
2022-01-11 23:02:02,120 iteration 1728 : loss : 0.050614, loss_ce: 0.022122
2022-01-11 23:02:03,650 iteration 1729 : loss : 0.040056, loss_ce: 0.015228
2022-01-11 23:02:05,218 iteration 1730 : loss : 0.047560, loss_ce: 0.018591
2022-01-11 23:02:06,734 iteration 1731 : loss : 0.039420, loss_ce: 0.017439
2022-01-11 23:02:08,322 iteration 1732 : loss : 0.060878, loss_ce: 0.025977
2022-01-11 23:02:09,888 iteration 1733 : loss : 0.052461, loss_ce: 0.017524
2022-01-11 23:02:11,432 iteration 1734 : loss : 0.050182, loss_ce: 0.014015
 26%|███████▍                     | 102/400 [49:42<2:23:38, 28.92s/it]2022-01-11 23:02:13,098 iteration 1735 : loss : 0.048446, loss_ce: 0.014889
2022-01-11 23:02:14,653 iteration 1736 : loss : 0.051985, loss_ce: 0.020369
2022-01-11 23:02:16,234 iteration 1737 : loss : 0.070483, loss_ce: 0.033764
2022-01-11 23:02:17,800 iteration 1738 : loss : 0.077800, loss_ce: 0.027862
2022-01-11 23:02:19,352 iteration 1739 : loss : 0.061990, loss_ce: 0.037217
2022-01-11 23:02:20,887 iteration 1740 : loss : 0.058647, loss_ce: 0.017234
2022-01-11 23:02:22,535 iteration 1741 : loss : 0.064719, loss_ce: 0.020010
2022-01-11 23:02:24,110 iteration 1742 : loss : 0.080857, loss_ce: 0.036850
2022-01-11 23:02:25,675 iteration 1743 : loss : 0.060038, loss_ce: 0.036347
2022-01-11 23:02:27,195 iteration 1744 : loss : 0.058154, loss_ce: 0.019550
2022-01-11 23:02:28,817 iteration 1745 : loss : 0.056326, loss_ce: 0.027266
2022-01-11 23:02:30,353 iteration 1746 : loss : 0.058960, loss_ce: 0.020921
2022-01-11 23:02:31,913 iteration 1747 : loss : 0.047659, loss_ce: 0.016996
2022-01-11 23:02:33,435 iteration 1748 : loss : 0.031176, loss_ce: 0.015719
2022-01-11 23:02:35,029 iteration 1749 : loss : 0.059347, loss_ce: 0.024178
2022-01-11 23:02:36,555 iteration 1750 : loss : 0.048485, loss_ce: 0.026471
2022-01-11 23:02:38,078 iteration 1751 : loss : 0.061124, loss_ce: 0.024856
 26%|███████▍                     | 103/400 [50:08<2:19:47, 28.24s/it]2022-01-11 23:02:39,675 iteration 1752 : loss : 0.059915, loss_ce: 0.025699
2022-01-11 23:02:41,322 iteration 1753 : loss : 0.050811, loss_ce: 0.024335
2022-01-11 23:02:42,907 iteration 1754 : loss : 0.042774, loss_ce: 0.021499
2022-01-11 23:02:44,485 iteration 1755 : loss : 0.066399, loss_ce: 0.020583
2022-01-11 23:02:46,112 iteration 1756 : loss : 0.095702, loss_ce: 0.059522
2022-01-11 23:02:47,694 iteration 1757 : loss : 0.103370, loss_ce: 0.024804
2022-01-11 23:02:49,269 iteration 1758 : loss : 0.069447, loss_ce: 0.029069
2022-01-11 23:02:50,829 iteration 1759 : loss : 0.046808, loss_ce: 0.015814
2022-01-11 23:02:52,446 iteration 1760 : loss : 0.049394, loss_ce: 0.023783
2022-01-11 23:02:53,953 iteration 1761 : loss : 0.044759, loss_ce: 0.020918
2022-01-11 23:02:55,565 iteration 1762 : loss : 0.118357, loss_ce: 0.051400
2022-01-11 23:02:57,083 iteration 1763 : loss : 0.044893, loss_ce: 0.015920
2022-01-11 23:02:58,681 iteration 1764 : loss : 0.103327, loss_ce: 0.032352
2022-01-11 23:03:00,278 iteration 1765 : loss : 0.072073, loss_ce: 0.021250
2022-01-11 23:03:01,797 iteration 1766 : loss : 0.048229, loss_ce: 0.022060
2022-01-11 23:03:03,359 iteration 1767 : loss : 0.095414, loss_ce: 0.044248
2022-01-11 23:03:04,984 iteration 1768 : loss : 0.060623, loss_ce: 0.021681
 26%|███████▌                     | 104/400 [50:35<2:17:20, 27.84s/it]2022-01-11 23:03:06,565 iteration 1769 : loss : 0.041883, loss_ce: 0.013034
2022-01-11 23:03:08,287 iteration 1770 : loss : 0.061266, loss_ce: 0.026916
2022-01-11 23:03:09,827 iteration 1771 : loss : 0.070064, loss_ce: 0.032270
2022-01-11 23:03:11,348 iteration 1772 : loss : 0.039324, loss_ce: 0.011858
2022-01-11 23:03:12,848 iteration 1773 : loss : 0.059634, loss_ce: 0.016832
2022-01-11 23:03:14,419 iteration 1774 : loss : 0.049818, loss_ce: 0.020187
2022-01-11 23:03:16,041 iteration 1775 : loss : 0.085584, loss_ce: 0.043092
2022-01-11 23:03:17,595 iteration 1776 : loss : 0.093597, loss_ce: 0.030670
2022-01-11 23:03:19,123 iteration 1777 : loss : 0.065945, loss_ce: 0.026789
2022-01-11 23:03:20,682 iteration 1778 : loss : 0.070748, loss_ce: 0.022813
2022-01-11 23:03:22,255 iteration 1779 : loss : 0.062620, loss_ce: 0.022127
2022-01-11 23:03:23,808 iteration 1780 : loss : 0.047521, loss_ce: 0.018629
2022-01-11 23:03:25,363 iteration 1781 : loss : 0.062259, loss_ce: 0.029878
2022-01-11 23:03:26,951 iteration 1782 : loss : 0.052690, loss_ce: 0.019431
2022-01-11 23:03:28,476 iteration 1783 : loss : 0.051910, loss_ce: 0.026064
2022-01-11 23:03:30,122 iteration 1784 : loss : 0.048964, loss_ce: 0.018311
2022-01-11 23:03:30,122 Training Data Eval:
2022-01-11 23:03:38,135   Average segmentation loss on training set: 0.0377
2022-01-11 23:03:38,136 Validation Data Eval:
2022-01-11 23:03:40,903   Average segmentation loss on validation set: 0.0680
2022-01-11 23:03:42,488 iteration 1785 : loss : 0.047086, loss_ce: 0.018611
 26%|███████▌                     | 105/400 [51:13<2:31:08, 30.74s/it]2022-01-11 23:03:44,204 iteration 1786 : loss : 0.066204, loss_ce: 0.032941
2022-01-11 23:03:45,874 iteration 1787 : loss : 0.060756, loss_ce: 0.026516
2022-01-11 23:03:47,467 iteration 1788 : loss : 0.050367, loss_ce: 0.022310
2022-01-11 23:03:49,040 iteration 1789 : loss : 0.048864, loss_ce: 0.016809
2022-01-11 23:03:50,585 iteration 1790 : loss : 0.035254, loss_ce: 0.014804
2022-01-11 23:03:52,091 iteration 1791 : loss : 0.032379, loss_ce: 0.015501
2022-01-11 23:03:53,693 iteration 1792 : loss : 0.053650, loss_ce: 0.017849
2022-01-11 23:03:55,327 iteration 1793 : loss : 0.035845, loss_ce: 0.016236
2022-01-11 23:03:56,870 iteration 1794 : loss : 0.044051, loss_ce: 0.019779
2022-01-11 23:03:58,458 iteration 1795 : loss : 0.057332, loss_ce: 0.028663
2022-01-11 23:04:00,025 iteration 1796 : loss : 0.046895, loss_ce: 0.016606
2022-01-11 23:04:01,681 iteration 1797 : loss : 0.033734, loss_ce: 0.013241
2022-01-11 23:04:03,255 iteration 1798 : loss : 0.057957, loss_ce: 0.020208
2022-01-11 23:04:04,861 iteration 1799 : loss : 0.083250, loss_ce: 0.035580
2022-01-11 23:04:06,465 iteration 1800 : loss : 0.072950, loss_ce: 0.036275
2022-01-11 23:04:08,068 iteration 1801 : loss : 0.043239, loss_ce: 0.016802
2022-01-11 23:04:09,562 iteration 1802 : loss : 0.077872, loss_ce: 0.022188
 26%|███████▋                     | 106/400 [51:40<2:25:14, 29.64s/it]2022-01-11 23:04:11,203 iteration 1803 : loss : 0.052086, loss_ce: 0.019756
2022-01-11 23:04:12,754 iteration 1804 : loss : 0.096314, loss_ce: 0.028449
2022-01-11 23:04:14,264 iteration 1805 : loss : 0.039088, loss_ce: 0.018908
2022-01-11 23:04:15,803 iteration 1806 : loss : 0.070505, loss_ce: 0.020191
2022-01-11 23:04:17,396 iteration 1807 : loss : 0.073862, loss_ce: 0.028269
2022-01-11 23:04:18,979 iteration 1808 : loss : 0.088019, loss_ce: 0.037866
2022-01-11 23:04:20,535 iteration 1809 : loss : 0.079837, loss_ce: 0.034523
2022-01-11 23:04:22,124 iteration 1810 : loss : 0.063082, loss_ce: 0.023370
2022-01-11 23:04:23,716 iteration 1811 : loss : 0.062443, loss_ce: 0.026347
2022-01-11 23:04:25,356 iteration 1812 : loss : 0.090821, loss_ce: 0.044155
2022-01-11 23:04:26,957 iteration 1813 : loss : 0.130606, loss_ce: 0.050204
2022-01-11 23:04:28,597 iteration 1814 : loss : 0.127459, loss_ce: 0.060671
2022-01-11 23:04:30,175 iteration 1815 : loss : 0.095058, loss_ce: 0.051638
2022-01-11 23:04:31,797 iteration 1816 : loss : 0.091487, loss_ce: 0.035104
2022-01-11 23:04:33,374 iteration 1817 : loss : 0.091761, loss_ce: 0.036047
2022-01-11 23:04:35,036 iteration 1818 : loss : 0.086591, loss_ce: 0.053146
2022-01-11 23:04:36,579 iteration 1819 : loss : 0.064341, loss_ce: 0.026745
 27%|███████▊                     | 107/400 [52:07<2:20:53, 28.85s/it]2022-01-11 23:04:38,217 iteration 1820 : loss : 0.127508, loss_ce: 0.040304
2022-01-11 23:04:39,744 iteration 1821 : loss : 0.135850, loss_ce: 0.044409
2022-01-11 23:04:41,279 iteration 1822 : loss : 0.100289, loss_ce: 0.065047
2022-01-11 23:04:42,871 iteration 1823 : loss : 0.093011, loss_ce: 0.040687
2022-01-11 23:04:44,427 iteration 1824 : loss : 0.106238, loss_ce: 0.038769
2022-01-11 23:04:46,005 iteration 1825 : loss : 0.112093, loss_ce: 0.034899
2022-01-11 23:04:47,499 iteration 1826 : loss : 0.070678, loss_ce: 0.030405
2022-01-11 23:04:49,026 iteration 1827 : loss : 0.084061, loss_ce: 0.028743
2022-01-11 23:04:50,634 iteration 1828 : loss : 0.052588, loss_ce: 0.023930
2022-01-11 23:04:52,235 iteration 1829 : loss : 0.108380, loss_ce: 0.033435
2022-01-11 23:04:53,785 iteration 1830 : loss : 0.087609, loss_ce: 0.033310
2022-01-11 23:04:55,335 iteration 1831 : loss : 0.075501, loss_ce: 0.035503
2022-01-11 23:04:56,891 iteration 1832 : loss : 0.050988, loss_ce: 0.019079
2022-01-11 23:04:58,437 iteration 1833 : loss : 0.069241, loss_ce: 0.043518
2022-01-11 23:05:00,063 iteration 1834 : loss : 0.079373, loss_ce: 0.029902
2022-01-11 23:05:01,540 iteration 1835 : loss : 0.071944, loss_ce: 0.028142
2022-01-11 23:05:03,074 iteration 1836 : loss : 0.073990, loss_ce: 0.032101
 27%|███████▊                     | 108/400 [52:33<2:16:59, 28.15s/it]2022-01-11 23:05:04,619 iteration 1837 : loss : 0.104964, loss_ce: 0.027293
2022-01-11 23:05:06,251 iteration 1838 : loss : 0.058766, loss_ce: 0.025944
2022-01-11 23:05:07,761 iteration 1839 : loss : 0.049727, loss_ce: 0.022730
2022-01-11 23:05:09,322 iteration 1840 : loss : 0.059270, loss_ce: 0.018731
2022-01-11 23:05:10,905 iteration 1841 : loss : 0.083042, loss_ce: 0.023279
2022-01-11 23:05:12,548 iteration 1842 : loss : 0.064665, loss_ce: 0.023222
2022-01-11 23:05:14,095 iteration 1843 : loss : 0.056598, loss_ce: 0.023885
2022-01-11 23:05:15,652 iteration 1844 : loss : 0.056314, loss_ce: 0.024244
2022-01-11 23:05:17,209 iteration 1845 : loss : 0.065262, loss_ce: 0.031422
2022-01-11 23:05:18,764 iteration 1846 : loss : 0.090621, loss_ce: 0.042123
2022-01-11 23:05:20,235 iteration 1847 : loss : 0.037007, loss_ce: 0.014792
2022-01-11 23:05:21,805 iteration 1848 : loss : 0.045962, loss_ce: 0.016050
2022-01-11 23:05:23,373 iteration 1849 : loss : 0.055972, loss_ce: 0.021427
2022-01-11 23:05:24,953 iteration 1850 : loss : 0.069635, loss_ce: 0.027702
2022-01-11 23:05:26,563 iteration 1851 : loss : 0.065682, loss_ce: 0.029757
2022-01-11 23:05:28,042 iteration 1852 : loss : 0.050715, loss_ce: 0.033155
2022-01-11 23:05:29,634 iteration 1853 : loss : 0.055426, loss_ce: 0.028622
 27%|███████▉                     | 109/400 [53:00<2:14:11, 27.67s/it]2022-01-11 23:05:31,246 iteration 1854 : loss : 0.067061, loss_ce: 0.022974
2022-01-11 23:05:32,847 iteration 1855 : loss : 0.062887, loss_ce: 0.025895
2022-01-11 23:05:34,326 iteration 1856 : loss : 0.061455, loss_ce: 0.031510
2022-01-11 23:05:35,815 iteration 1857 : loss : 0.046206, loss_ce: 0.021141
2022-01-11 23:05:37,334 iteration 1858 : loss : 0.049424, loss_ce: 0.025620
2022-01-11 23:05:38,920 iteration 1859 : loss : 0.047458, loss_ce: 0.020155
2022-01-11 23:05:40,518 iteration 1860 : loss : 0.088286, loss_ce: 0.040288
2022-01-11 23:05:42,088 iteration 1861 : loss : 0.038349, loss_ce: 0.019238
2022-01-11 23:05:43,675 iteration 1862 : loss : 0.042252, loss_ce: 0.021131
2022-01-11 23:05:45,276 iteration 1863 : loss : 0.061225, loss_ce: 0.022271
2022-01-11 23:05:46,874 iteration 1864 : loss : 0.043466, loss_ce: 0.018855
2022-01-11 23:05:48,437 iteration 1865 : loss : 0.079490, loss_ce: 0.028731
2022-01-11 23:05:50,011 iteration 1866 : loss : 0.036895, loss_ce: 0.014400
2022-01-11 23:05:51,493 iteration 1867 : loss : 0.069378, loss_ce: 0.024485
2022-01-11 23:05:53,030 iteration 1868 : loss : 0.039436, loss_ce: 0.013209
2022-01-11 23:05:54,569 iteration 1869 : loss : 0.054842, loss_ce: 0.025365
2022-01-11 23:05:54,569 Training Data Eval:
2022-01-11 23:06:02,590   Average segmentation loss on training set: 0.0608
2022-01-11 23:06:02,590 Validation Data Eval:
2022-01-11 23:06:05,354   Average segmentation loss on validation set: 0.1387
2022-01-11 23:06:06,954 iteration 1870 : loss : 0.074123, loss_ce: 0.029165
 28%|███████▉                     | 110/400 [53:37<2:27:44, 30.57s/it]2022-01-11 23:06:08,608 iteration 1871 : loss : 0.065032, loss_ce: 0.022321
2022-01-11 23:06:10,216 iteration 1872 : loss : 0.048939, loss_ce: 0.018671
2022-01-11 23:06:11,841 iteration 1873 : loss : 0.057334, loss_ce: 0.021625
2022-01-11 23:06:13,443 iteration 1874 : loss : 0.057979, loss_ce: 0.019275
2022-01-11 23:06:14,942 iteration 1875 : loss : 0.038018, loss_ce: 0.015131
2022-01-11 23:06:16,432 iteration 1876 : loss : 0.038651, loss_ce: 0.017585
2022-01-11 23:06:18,041 iteration 1877 : loss : 0.046105, loss_ce: 0.018529
2022-01-11 23:06:19,625 iteration 1878 : loss : 0.075979, loss_ce: 0.042231
2022-01-11 23:06:21,181 iteration 1879 : loss : 0.044945, loss_ce: 0.015079
2022-01-11 23:06:22,778 iteration 1880 : loss : 0.054730, loss_ce: 0.023936
2022-01-11 23:06:24,351 iteration 1881 : loss : 0.059149, loss_ce: 0.025728
2022-01-11 23:06:25,969 iteration 1882 : loss : 0.105804, loss_ce: 0.035463
2022-01-11 23:06:27,544 iteration 1883 : loss : 0.031478, loss_ce: 0.011183
2022-01-11 23:06:29,084 iteration 1884 : loss : 0.049317, loss_ce: 0.019072
2022-01-11 23:06:30,584 iteration 1885 : loss : 0.031695, loss_ce: 0.013389
2022-01-11 23:06:32,162 iteration 1886 : loss : 0.072255, loss_ce: 0.025941
2022-01-11 23:06:33,740 iteration 1887 : loss : 0.079079, loss_ce: 0.033950
 28%|████████                     | 111/400 [54:04<2:21:45, 29.43s/it]2022-01-11 23:06:35,443 iteration 1888 : loss : 0.055519, loss_ce: 0.017678
2022-01-11 23:06:36,954 iteration 1889 : loss : 0.088965, loss_ce: 0.027887
2022-01-11 23:06:38,464 iteration 1890 : loss : 0.048622, loss_ce: 0.022675
2022-01-11 23:06:39,982 iteration 1891 : loss : 0.051824, loss_ce: 0.017927
2022-01-11 23:06:41,540 iteration 1892 : loss : 0.041406, loss_ce: 0.018523
2022-01-11 23:06:43,125 iteration 1893 : loss : 0.044184, loss_ce: 0.015043
2022-01-11 23:06:44,681 iteration 1894 : loss : 0.056217, loss_ce: 0.024217
2022-01-11 23:06:46,185 iteration 1895 : loss : 0.033669, loss_ce: 0.013140
2022-01-11 23:06:47,811 iteration 1896 : loss : 0.047927, loss_ce: 0.021368
2022-01-11 23:06:49,413 iteration 1897 : loss : 0.067244, loss_ce: 0.026073
2022-01-11 23:06:51,047 iteration 1898 : loss : 0.042827, loss_ce: 0.021263
2022-01-11 23:06:52,600 iteration 1899 : loss : 0.045884, loss_ce: 0.016543
2022-01-11 23:06:54,122 iteration 1900 : loss : 0.063368, loss_ce: 0.026861
2022-01-11 23:06:55,632 iteration 1901 : loss : 0.046424, loss_ce: 0.015658
2022-01-11 23:06:57,199 iteration 1902 : loss : 0.047566, loss_ce: 0.019529
2022-01-11 23:06:58,816 iteration 1903 : loss : 0.045576, loss_ce: 0.018776
2022-01-11 23:07:00,449 iteration 1904 : loss : 0.060487, loss_ce: 0.025488
 28%|████████                     | 112/400 [54:31<2:17:21, 28.62s/it]2022-01-11 23:07:01,975 iteration 1905 : loss : 0.054592, loss_ce: 0.023158
2022-01-11 23:07:03,554 iteration 1906 : loss : 0.045235, loss_ce: 0.016080
2022-01-11 23:07:05,165 iteration 1907 : loss : 0.040607, loss_ce: 0.017250
2022-01-11 23:07:06,853 iteration 1908 : loss : 0.046843, loss_ce: 0.019124
2022-01-11 23:07:08,322 iteration 1909 : loss : 0.034884, loss_ce: 0.016049
2022-01-11 23:07:09,872 iteration 1910 : loss : 0.044801, loss_ce: 0.017961
2022-01-11 23:07:11,485 iteration 1911 : loss : 0.046087, loss_ce: 0.022600
2022-01-11 23:07:13,154 iteration 1912 : loss : 0.046664, loss_ce: 0.018796
2022-01-11 23:07:14,756 iteration 1913 : loss : 0.075114, loss_ce: 0.029184
2022-01-11 23:07:16,393 iteration 1914 : loss : 0.071589, loss_ce: 0.034110
2022-01-11 23:07:17,953 iteration 1915 : loss : 0.041328, loss_ce: 0.014476
2022-01-11 23:07:19,500 iteration 1916 : loss : 0.045188, loss_ce: 0.015528
2022-01-11 23:07:21,059 iteration 1917 : loss : 0.041261, loss_ce: 0.017373
2022-01-11 23:07:22,667 iteration 1918 : loss : 0.054705, loss_ce: 0.026978
2022-01-11 23:07:24,195 iteration 1919 : loss : 0.042037, loss_ce: 0.014347
2022-01-11 23:07:25,692 iteration 1920 : loss : 0.048725, loss_ce: 0.024476
2022-01-11 23:07:27,333 iteration 1921 : loss : 0.060459, loss_ce: 0.026495
 28%|████████▏                    | 113/400 [54:58<2:14:23, 28.10s/it]2022-01-11 23:07:28,984 iteration 1922 : loss : 0.057611, loss_ce: 0.026079
2022-01-11 23:07:30,510 iteration 1923 : loss : 0.036738, loss_ce: 0.014339
2022-01-11 23:07:32,134 iteration 1924 : loss : 0.091670, loss_ce: 0.031378
2022-01-11 23:07:33,661 iteration 1925 : loss : 0.093871, loss_ce: 0.026807
2022-01-11 23:07:35,286 iteration 1926 : loss : 0.064276, loss_ce: 0.025223
2022-01-11 23:07:36,908 iteration 1927 : loss : 0.040590, loss_ce: 0.019142
2022-01-11 23:07:38,428 iteration 1928 : loss : 0.061874, loss_ce: 0.018831
2022-01-11 23:07:40,118 iteration 1929 : loss : 0.066837, loss_ce: 0.032412
2022-01-11 23:07:41,676 iteration 1930 : loss : 0.055197, loss_ce: 0.026182
2022-01-11 23:07:43,292 iteration 1931 : loss : 0.070216, loss_ce: 0.027903
2022-01-11 23:07:44,892 iteration 1932 : loss : 0.079322, loss_ce: 0.021619
2022-01-11 23:07:46,489 iteration 1933 : loss : 0.046800, loss_ce: 0.016283
2022-01-11 23:07:48,037 iteration 1934 : loss : 0.078681, loss_ce: 0.024529
2022-01-11 23:07:49,605 iteration 1935 : loss : 0.059473, loss_ce: 0.024822
2022-01-11 23:07:51,164 iteration 1936 : loss : 0.050191, loss_ce: 0.021086
2022-01-11 23:07:52,689 iteration 1937 : loss : 0.039517, loss_ce: 0.014638
2022-01-11 23:07:54,288 iteration 1938 : loss : 0.069462, loss_ce: 0.029107
 28%|████████▎                    | 114/400 [55:25<2:12:17, 27.75s/it]2022-01-11 23:07:55,919 iteration 1939 : loss : 0.049585, loss_ce: 0.021642
2022-01-11 23:07:57,446 iteration 1940 : loss : 0.042293, loss_ce: 0.020276
2022-01-11 23:07:59,031 iteration 1941 : loss : 0.037771, loss_ce: 0.014730
2022-01-11 23:08:00,628 iteration 1942 : loss : 0.043072, loss_ce: 0.014429
2022-01-11 23:08:02,187 iteration 1943 : loss : 0.044747, loss_ce: 0.015864
2022-01-11 23:08:03,766 iteration 1944 : loss : 0.045239, loss_ce: 0.016958
2022-01-11 23:08:05,351 iteration 1945 : loss : 0.060712, loss_ce: 0.023802
2022-01-11 23:08:06,898 iteration 1946 : loss : 0.067093, loss_ce: 0.015353
2022-01-11 23:08:08,471 iteration 1947 : loss : 0.048400, loss_ce: 0.014471
2022-01-11 23:08:10,078 iteration 1948 : loss : 0.048150, loss_ce: 0.021364
2022-01-11 23:08:11,611 iteration 1949 : loss : 0.059750, loss_ce: 0.020051
2022-01-11 23:08:13,217 iteration 1950 : loss : 0.074358, loss_ce: 0.029009
2022-01-11 23:08:14,816 iteration 1951 : loss : 0.051417, loss_ce: 0.023525
2022-01-11 23:08:16,406 iteration 1952 : loss : 0.058234, loss_ce: 0.027872
2022-01-11 23:08:17,904 iteration 1953 : loss : 0.070311, loss_ce: 0.045855
2022-01-11 23:08:19,494 iteration 1954 : loss : 0.033165, loss_ce: 0.014392
2022-01-11 23:08:19,495 Training Data Eval:
2022-01-11 23:08:27,520   Average segmentation loss on training set: 0.0358
2022-01-11 23:08:27,521 Validation Data Eval:
2022-01-11 23:08:30,282   Average segmentation loss on validation set: 0.0765
2022-01-11 23:08:31,899 iteration 1955 : loss : 0.051643, loss_ce: 0.024957
 29%|████████▎                    | 115/400 [56:02<2:25:52, 30.71s/it]2022-01-11 23:08:33,504 iteration 1956 : loss : 0.057030, loss_ce: 0.024837
2022-01-11 23:08:35,129 iteration 1957 : loss : 0.051645, loss_ce: 0.027441
2022-01-11 23:08:36,694 iteration 1958 : loss : 0.150517, loss_ce: 0.041434
2022-01-11 23:08:38,249 iteration 1959 : loss : 0.064346, loss_ce: 0.021800
2022-01-11 23:08:39,735 iteration 1960 : loss : 0.051333, loss_ce: 0.021147
2022-01-11 23:08:41,200 iteration 1961 : loss : 0.041810, loss_ce: 0.015649
2022-01-11 23:08:42,821 iteration 1962 : loss : 0.052811, loss_ce: 0.025003
2022-01-11 23:08:44,355 iteration 1963 : loss : 0.039345, loss_ce: 0.015694
2022-01-11 23:08:45,964 iteration 1964 : loss : 0.060852, loss_ce: 0.022384
2022-01-11 23:08:47,461 iteration 1965 : loss : 0.035936, loss_ce: 0.016544
2022-01-11 23:08:49,037 iteration 1966 : loss : 0.039732, loss_ce: 0.017851
2022-01-11 23:08:50,719 iteration 1967 : loss : 0.057388, loss_ce: 0.021096
2022-01-11 23:08:52,438 iteration 1968 : loss : 0.057693, loss_ce: 0.023028
2022-01-11 23:08:54,038 iteration 1969 : loss : 0.047206, loss_ce: 0.019163
2022-01-11 23:08:55,542 iteration 1970 : loss : 0.035220, loss_ce: 0.014887
2022-01-11 23:08:57,118 iteration 1971 : loss : 0.047460, loss_ce: 0.020343
2022-01-11 23:08:58,694 iteration 1972 : loss : 0.049282, loss_ce: 0.013608
 29%|████████▍                    | 116/400 [56:29<2:19:48, 29.54s/it]2022-01-11 23:09:00,255 iteration 1973 : loss : 0.041376, loss_ce: 0.018161
2022-01-11 23:09:01,780 iteration 1974 : loss : 0.039119, loss_ce: 0.014787
2022-01-11 23:09:03,374 iteration 1975 : loss : 0.053411, loss_ce: 0.019452
2022-01-11 23:09:04,914 iteration 1976 : loss : 0.058231, loss_ce: 0.030064
2022-01-11 23:09:06,538 iteration 1977 : loss : 0.038436, loss_ce: 0.013198
2022-01-11 23:09:08,180 iteration 1978 : loss : 0.058010, loss_ce: 0.023812
2022-01-11 23:09:09,687 iteration 1979 : loss : 0.042950, loss_ce: 0.016874
2022-01-11 23:09:11,213 iteration 1980 : loss : 0.038908, loss_ce: 0.013926
2022-01-11 23:09:12,755 iteration 1981 : loss : 0.041735, loss_ce: 0.016561
2022-01-11 23:09:14,374 iteration 1982 : loss : 0.049768, loss_ce: 0.020201
2022-01-11 23:09:15,880 iteration 1983 : loss : 0.067029, loss_ce: 0.024649
2022-01-11 23:09:17,501 iteration 1984 : loss : 0.049050, loss_ce: 0.019763
2022-01-11 23:09:19,061 iteration 1985 : loss : 0.055426, loss_ce: 0.020408
2022-01-11 23:09:20,675 iteration 1986 : loss : 0.077846, loss_ce: 0.025590
2022-01-11 23:09:22,211 iteration 1987 : loss : 0.038470, loss_ce: 0.012973
2022-01-11 23:09:23,714 iteration 1988 : loss : 0.038772, loss_ce: 0.017874
2022-01-11 23:09:25,281 iteration 1989 : loss : 0.057992, loss_ce: 0.019106
 29%|████████▍                    | 117/400 [56:56<2:15:08, 28.65s/it]2022-01-11 23:09:26,898 iteration 1990 : loss : 0.070057, loss_ce: 0.018673
2022-01-11 23:09:28,412 iteration 1991 : loss : 0.034365, loss_ce: 0.014759
2022-01-11 23:09:30,004 iteration 1992 : loss : 0.035302, loss_ce: 0.012779
2022-01-11 23:09:31,598 iteration 1993 : loss : 0.041852, loss_ce: 0.015917
2022-01-11 23:09:33,233 iteration 1994 : loss : 0.067139, loss_ce: 0.029133
2022-01-11 23:09:34,768 iteration 1995 : loss : 0.037914, loss_ce: 0.015665
2022-01-11 23:09:36,358 iteration 1996 : loss : 0.052586, loss_ce: 0.023892
2022-01-11 23:09:37,885 iteration 1997 : loss : 0.033561, loss_ce: 0.014489
2022-01-11 23:09:39,456 iteration 1998 : loss : 0.043373, loss_ce: 0.016072
2022-01-11 23:09:41,046 iteration 1999 : loss : 0.048828, loss_ce: 0.026405
2022-01-11 23:09:42,617 iteration 2000 : loss : 0.042778, loss_ce: 0.020676
2022-01-11 23:09:44,132 iteration 2001 : loss : 0.051858, loss_ce: 0.017398
2022-01-11 23:09:45,754 iteration 2002 : loss : 0.040811, loss_ce: 0.015753
2022-01-11 23:09:47,269 iteration 2003 : loss : 0.034417, loss_ce: 0.011206
2022-01-11 23:09:48,863 iteration 2004 : loss : 0.041448, loss_ce: 0.018973
2022-01-11 23:09:50,394 iteration 2005 : loss : 0.049876, loss_ce: 0.024179
2022-01-11 23:09:51,954 iteration 2006 : loss : 0.059361, loss_ce: 0.022208
 30%|████████▌                    | 118/400 [57:22<2:11:51, 28.06s/it]2022-01-11 23:09:53,633 iteration 2007 : loss : 0.060758, loss_ce: 0.026154
2022-01-11 23:09:55,229 iteration 2008 : loss : 0.050448, loss_ce: 0.020357
2022-01-11 23:09:56,764 iteration 2009 : loss : 0.040224, loss_ce: 0.017345
2022-01-11 23:09:58,337 iteration 2010 : loss : 0.046017, loss_ce: 0.019490
2022-01-11 23:09:59,850 iteration 2011 : loss : 0.072731, loss_ce: 0.021421
2022-01-11 23:10:01,344 iteration 2012 : loss : 0.046432, loss_ce: 0.017137
2022-01-11 23:10:02,875 iteration 2013 : loss : 0.055183, loss_ce: 0.032422
2022-01-11 23:10:04,453 iteration 2014 : loss : 0.025697, loss_ce: 0.011420
2022-01-11 23:10:06,047 iteration 2015 : loss : 0.046259, loss_ce: 0.015295
2022-01-11 23:10:07,684 iteration 2016 : loss : 0.057261, loss_ce: 0.027519
2022-01-11 23:10:09,286 iteration 2017 : loss : 0.049213, loss_ce: 0.018174
2022-01-11 23:10:10,795 iteration 2018 : loss : 0.066538, loss_ce: 0.025061
2022-01-11 23:10:12,368 iteration 2019 : loss : 0.049114, loss_ce: 0.023525
2022-01-11 23:10:13,933 iteration 2020 : loss : 0.050825, loss_ce: 0.018976
2022-01-11 23:10:15,477 iteration 2021 : loss : 0.034115, loss_ce: 0.013536
2022-01-11 23:10:17,045 iteration 2022 : loss : 0.050360, loss_ce: 0.017619
2022-01-11 23:10:18,623 iteration 2023 : loss : 0.032165, loss_ce: 0.011034
 30%|████████▋                    | 119/400 [57:49<2:09:26, 27.64s/it]2022-01-11 23:10:20,255 iteration 2024 : loss : 0.059442, loss_ce: 0.026724
2022-01-11 23:10:21,956 iteration 2025 : loss : 0.064276, loss_ce: 0.020695
2022-01-11 23:10:23,484 iteration 2026 : loss : 0.050576, loss_ce: 0.015703
2022-01-11 23:10:25,088 iteration 2027 : loss : 0.042256, loss_ce: 0.013410
2022-01-11 23:10:26,573 iteration 2028 : loss : 0.032002, loss_ce: 0.014087
2022-01-11 23:10:28,082 iteration 2029 : loss : 0.047731, loss_ce: 0.013871
2022-01-11 23:10:29,667 iteration 2030 : loss : 0.046438, loss_ce: 0.015556
2022-01-11 23:10:31,258 iteration 2031 : loss : 0.049929, loss_ce: 0.018351
2022-01-11 23:10:32,835 iteration 2032 : loss : 0.051770, loss_ce: 0.022506
2022-01-11 23:10:34,317 iteration 2033 : loss : 0.030486, loss_ce: 0.014376
2022-01-11 23:10:35,845 iteration 2034 : loss : 0.042183, loss_ce: 0.014843
2022-01-11 23:10:37,442 iteration 2035 : loss : 0.049990, loss_ce: 0.020226
2022-01-11 23:10:38,991 iteration 2036 : loss : 0.036668, loss_ce: 0.016759
2022-01-11 23:10:40,511 iteration 2037 : loss : 0.043880, loss_ce: 0.017767
2022-01-11 23:10:42,165 iteration 2038 : loss : 0.067599, loss_ce: 0.020441
2022-01-11 23:10:43,768 iteration 2039 : loss : 0.048014, loss_ce: 0.020245
2022-01-11 23:10:43,768 Training Data Eval:
2022-01-11 23:10:51,799   Average segmentation loss on training set: 0.0316
2022-01-11 23:10:51,800 Validation Data Eval:
2022-01-11 23:10:54,555   Average segmentation loss on validation set: 0.1091
2022-01-11 23:10:56,188 iteration 2040 : loss : 0.037138, loss_ce: 0.017394
 30%|████████▋                    | 120/400 [58:26<2:22:53, 30.62s/it]2022-01-11 23:10:57,768 iteration 2041 : loss : 0.039749, loss_ce: 0.017839
2022-01-11 23:10:59,364 iteration 2042 : loss : 0.041289, loss_ce: 0.014150
2022-01-11 23:11:00,958 iteration 2043 : loss : 0.054604, loss_ce: 0.020562
2022-01-11 23:11:02,514 iteration 2044 : loss : 0.045253, loss_ce: 0.019596
2022-01-11 23:11:04,186 iteration 2045 : loss : 0.058178, loss_ce: 0.022036
2022-01-11 23:11:05,773 iteration 2046 : loss : 0.039621, loss_ce: 0.010881
2022-01-11 23:11:07,308 iteration 2047 : loss : 0.032406, loss_ce: 0.013197
2022-01-11 23:11:08,822 iteration 2048 : loss : 0.039904, loss_ce: 0.016217
2022-01-11 23:11:10,439 iteration 2049 : loss : 0.044029, loss_ce: 0.021380
2022-01-11 23:11:12,049 iteration 2050 : loss : 0.039104, loss_ce: 0.013324
2022-01-11 23:11:13,596 iteration 2051 : loss : 0.045740, loss_ce: 0.021274
2022-01-11 23:11:15,221 iteration 2052 : loss : 0.042082, loss_ce: 0.016951
2022-01-11 23:11:16,799 iteration 2053 : loss : 0.041805, loss_ce: 0.018771
2022-01-11 23:11:18,357 iteration 2054 : loss : 0.039282, loss_ce: 0.014750
2022-01-11 23:11:19,947 iteration 2055 : loss : 0.045107, loss_ce: 0.015590
2022-01-11 23:11:21,481 iteration 2056 : loss : 0.032955, loss_ce: 0.011770
2022-01-11 23:11:23,026 iteration 2057 : loss : 0.034342, loss_ce: 0.015249
 30%|████████▊                    | 121/400 [58:53<2:17:05, 29.48s/it]2022-01-11 23:11:24,625 iteration 2058 : loss : 0.036321, loss_ce: 0.017523
2022-01-11 23:11:26,237 iteration 2059 : loss : 0.044090, loss_ce: 0.014996
2022-01-11 23:11:27,869 iteration 2060 : loss : 0.041419, loss_ce: 0.016076
2022-01-11 23:11:29,424 iteration 2061 : loss : 0.049023, loss_ce: 0.026621
2022-01-11 23:11:30,948 iteration 2062 : loss : 0.052576, loss_ce: 0.020276
2022-01-11 23:11:32,660 iteration 2063 : loss : 0.052833, loss_ce: 0.021455
2022-01-11 23:11:34,237 iteration 2064 : loss : 0.047499, loss_ce: 0.019273
2022-01-11 23:11:35,878 iteration 2065 : loss : 0.066614, loss_ce: 0.026897
2022-01-11 23:11:37,432 iteration 2066 : loss : 0.038303, loss_ce: 0.015756
2022-01-11 23:11:39,032 iteration 2067 : loss : 0.057523, loss_ce: 0.021157
2022-01-11 23:11:40,553 iteration 2068 : loss : 0.048334, loss_ce: 0.023097
2022-01-11 23:11:42,116 iteration 2069 : loss : 0.057100, loss_ce: 0.028173
2022-01-11 23:11:43,732 iteration 2070 : loss : 0.050584, loss_ce: 0.022657
2022-01-11 23:11:45,269 iteration 2071 : loss : 0.031351, loss_ce: 0.012694
2022-01-11 23:11:46,804 iteration 2072 : loss : 0.045357, loss_ce: 0.022064
2022-01-11 23:11:48,424 iteration 2073 : loss : 0.053016, loss_ce: 0.022119
2022-01-11 23:11:49,929 iteration 2074 : loss : 0.060216, loss_ce: 0.019562
 30%|████████▊                    | 122/400 [59:20<2:13:01, 28.71s/it]2022-01-11 23:11:51,534 iteration 2075 : loss : 0.044696, loss_ce: 0.023823
2022-01-11 23:11:53,147 iteration 2076 : loss : 0.042633, loss_ce: 0.018386
2022-01-11 23:11:54,753 iteration 2077 : loss : 0.067345, loss_ce: 0.037684
2022-01-11 23:11:56,357 iteration 2078 : loss : 0.050996, loss_ce: 0.022564
2022-01-11 23:11:57,950 iteration 2079 : loss : 0.036977, loss_ce: 0.014472
2022-01-11 23:11:59,494 iteration 2080 : loss : 0.045682, loss_ce: 0.014822
2022-01-11 23:12:01,003 iteration 2081 : loss : 0.035841, loss_ce: 0.012942
2022-01-11 23:12:02,571 iteration 2082 : loss : 0.064297, loss_ce: 0.017830
2022-01-11 23:12:04,126 iteration 2083 : loss : 0.040448, loss_ce: 0.015695
2022-01-11 23:12:05,634 iteration 2084 : loss : 0.040581, loss_ce: 0.018232
2022-01-11 23:12:07,230 iteration 2085 : loss : 0.056037, loss_ce: 0.012854
2022-01-11 23:12:08,879 iteration 2086 : loss : 0.041861, loss_ce: 0.014736
2022-01-11 23:12:10,371 iteration 2087 : loss : 0.043939, loss_ce: 0.015347
2022-01-11 23:12:11,932 iteration 2088 : loss : 0.043343, loss_ce: 0.016832
2022-01-11 23:12:13,513 iteration 2089 : loss : 0.042938, loss_ce: 0.016185
2022-01-11 23:12:15,086 iteration 2090 : loss : 0.044580, loss_ce: 0.017980
2022-01-11 23:12:16,738 iteration 2091 : loss : 0.053483, loss_ce: 0.022307
 31%|████████▉                    | 123/400 [59:47<2:09:54, 28.14s/it]2022-01-11 23:12:18,314 iteration 2092 : loss : 0.047518, loss_ce: 0.013767
2022-01-11 23:12:19,832 iteration 2093 : loss : 0.025492, loss_ce: 0.010839
2022-01-11 23:12:21,426 iteration 2094 : loss : 0.043706, loss_ce: 0.015919
2022-01-11 23:12:23,150 iteration 2095 : loss : 0.046415, loss_ce: 0.017889
2022-01-11 23:12:24,698 iteration 2096 : loss : 0.030585, loss_ce: 0.013234
2022-01-11 23:12:26,260 iteration 2097 : loss : 0.072462, loss_ce: 0.021730
2022-01-11 23:12:27,771 iteration 2098 : loss : 0.039681, loss_ce: 0.014714
2022-01-11 23:12:29,328 iteration 2099 : loss : 0.027532, loss_ce: 0.011659
2022-01-11 23:12:30,924 iteration 2100 : loss : 0.047176, loss_ce: 0.018457
2022-01-11 23:12:32,398 iteration 2101 : loss : 0.041144, loss_ce: 0.013228
2022-01-11 23:12:34,003 iteration 2102 : loss : 0.047798, loss_ce: 0.019061
2022-01-11 23:12:35,535 iteration 2103 : loss : 0.040273, loss_ce: 0.019102
2022-01-11 23:12:37,030 iteration 2104 : loss : 0.038891, loss_ce: 0.014193
2022-01-11 23:12:38,612 iteration 2105 : loss : 0.048692, loss_ce: 0.021577
2022-01-11 23:12:40,194 iteration 2106 : loss : 0.047118, loss_ce: 0.019361
2022-01-11 23:12:41,703 iteration 2107 : loss : 0.032809, loss_ce: 0.011927
2022-01-11 23:12:43,212 iteration 2108 : loss : 0.038022, loss_ce: 0.014661
 31%|████████▎                  | 124/400 [1:00:14<2:07:09, 27.64s/it]2022-01-11 23:12:44,819 iteration 2109 : loss : 0.071756, loss_ce: 0.020369
2022-01-11 23:12:46,401 iteration 2110 : loss : 0.050945, loss_ce: 0.026817
2022-01-11 23:12:48,027 iteration 2111 : loss : 0.040144, loss_ce: 0.013728
2022-01-11 23:12:49,704 iteration 2112 : loss : 0.042304, loss_ce: 0.019239
2022-01-11 23:12:51,310 iteration 2113 : loss : 0.048473, loss_ce: 0.015950
2022-01-11 23:12:52,903 iteration 2114 : loss : 0.043297, loss_ce: 0.012955
2022-01-11 23:12:54,430 iteration 2115 : loss : 0.040749, loss_ce: 0.014153
2022-01-11 23:12:55,958 iteration 2116 : loss : 0.049906, loss_ce: 0.017136
2022-01-11 23:12:57,619 iteration 2117 : loss : 0.056514, loss_ce: 0.015962
2022-01-11 23:12:59,199 iteration 2118 : loss : 0.042711, loss_ce: 0.019906
2022-01-11 23:13:00,876 iteration 2119 : loss : 0.044980, loss_ce: 0.017583
2022-01-11 23:13:02,497 iteration 2120 : loss : 0.039138, loss_ce: 0.019954
2022-01-11 23:13:04,132 iteration 2121 : loss : 0.044327, loss_ce: 0.019411
2022-01-11 23:13:05,765 iteration 2122 : loss : 0.066995, loss_ce: 0.033870
2022-01-11 23:13:07,281 iteration 2123 : loss : 0.057703, loss_ce: 0.015192
2022-01-11 23:13:08,914 iteration 2124 : loss : 0.049335, loss_ce: 0.021094
2022-01-11 23:13:08,915 Training Data Eval:
2022-01-11 23:13:16,926   Average segmentation loss on training set: 0.0395
2022-01-11 23:13:16,926 Validation Data Eval:
2022-01-11 23:13:19,685   Average segmentation loss on validation set: 0.0714
2022-01-11 23:13:21,240 iteration 2125 : loss : 0.026885, loss_ce: 0.013729
 31%|████████▍                  | 125/400 [1:00:52<2:20:58, 30.76s/it]2022-01-11 23:13:22,902 iteration 2126 : loss : 0.077709, loss_ce: 0.024507
2022-01-11 23:13:24,416 iteration 2127 : loss : 0.033538, loss_ce: 0.014628
2022-01-11 23:13:25,925 iteration 2128 : loss : 0.030638, loss_ce: 0.012625
2022-01-11 23:13:27,569 iteration 2129 : loss : 0.062774, loss_ce: 0.023351
2022-01-11 23:13:29,132 iteration 2130 : loss : 0.053310, loss_ce: 0.020548
2022-01-11 23:13:30,661 iteration 2131 : loss : 0.029657, loss_ce: 0.011516
2022-01-11 23:13:32,269 iteration 2132 : loss : 0.058993, loss_ce: 0.018725
2022-01-11 23:13:33,840 iteration 2133 : loss : 0.035729, loss_ce: 0.014256
2022-01-11 23:13:35,478 iteration 2134 : loss : 0.054220, loss_ce: 0.018304
2022-01-11 23:13:36,985 iteration 2135 : loss : 0.044229, loss_ce: 0.013242
2022-01-11 23:13:38,620 iteration 2136 : loss : 0.059249, loss_ce: 0.022310
2022-01-11 23:13:40,182 iteration 2137 : loss : 0.034780, loss_ce: 0.015340
2022-01-11 23:13:41,837 iteration 2138 : loss : 0.044011, loss_ce: 0.012694
2022-01-11 23:13:43,388 iteration 2139 : loss : 0.032837, loss_ce: 0.015883
2022-01-11 23:13:44,990 iteration 2140 : loss : 0.049577, loss_ce: 0.031599
2022-01-11 23:13:46,577 iteration 2141 : loss : 0.043855, loss_ce: 0.016656
2022-01-11 23:13:48,175 iteration 2142 : loss : 0.083414, loss_ce: 0.029968
 32%|████████▌                  | 126/400 [1:01:18<2:15:13, 29.61s/it]2022-01-11 23:13:49,777 iteration 2143 : loss : 0.041627, loss_ce: 0.022793
2022-01-11 23:13:51,356 iteration 2144 : loss : 0.031939, loss_ce: 0.013793
2022-01-11 23:13:52,932 iteration 2145 : loss : 0.041271, loss_ce: 0.016944
2022-01-11 23:13:54,471 iteration 2146 : loss : 0.041943, loss_ce: 0.015639
2022-01-11 23:13:55,983 iteration 2147 : loss : 0.043349, loss_ce: 0.015197
2022-01-11 23:13:57,537 iteration 2148 : loss : 0.034695, loss_ce: 0.012543
2022-01-11 23:13:59,155 iteration 2149 : loss : 0.057892, loss_ce: 0.022593
2022-01-11 23:14:00,690 iteration 2150 : loss : 0.036998, loss_ce: 0.017876
2022-01-11 23:14:02,246 iteration 2151 : loss : 0.034311, loss_ce: 0.012715
2022-01-11 23:14:03,757 iteration 2152 : loss : 0.035969, loss_ce: 0.017734
2022-01-11 23:14:05,311 iteration 2153 : loss : 0.058011, loss_ce: 0.021324
2022-01-11 23:14:06,876 iteration 2154 : loss : 0.032337, loss_ce: 0.011736
2022-01-11 23:14:08,474 iteration 2155 : loss : 0.044681, loss_ce: 0.024673
2022-01-11 23:14:09,980 iteration 2156 : loss : 0.035311, loss_ce: 0.012600
2022-01-11 23:14:11,637 iteration 2157 : loss : 0.042753, loss_ce: 0.018668
2022-01-11 23:14:13,255 iteration 2158 : loss : 0.043835, loss_ce: 0.016259
2022-01-11 23:14:14,858 iteration 2159 : loss : 0.059142, loss_ce: 0.022795
 32%|████████▌                  | 127/400 [1:01:45<2:10:43, 28.73s/it]2022-01-11 23:14:16,470 iteration 2160 : loss : 0.037661, loss_ce: 0.013814
2022-01-11 23:14:18,114 iteration 2161 : loss : 0.083043, loss_ce: 0.039587
2022-01-11 23:14:19,724 iteration 2162 : loss : 0.039545, loss_ce: 0.015524
2022-01-11 23:14:21,225 iteration 2163 : loss : 0.042904, loss_ce: 0.015096
2022-01-11 23:14:22,839 iteration 2164 : loss : 0.045821, loss_ce: 0.016986
2022-01-11 23:14:24,448 iteration 2165 : loss : 0.031466, loss_ce: 0.015262
2022-01-11 23:14:26,025 iteration 2166 : loss : 0.046770, loss_ce: 0.013012
2022-01-11 23:14:27,629 iteration 2167 : loss : 0.038658, loss_ce: 0.014392
2022-01-11 23:14:29,208 iteration 2168 : loss : 0.037826, loss_ce: 0.015374
2022-01-11 23:14:30,775 iteration 2169 : loss : 0.035908, loss_ce: 0.013292
2022-01-11 23:14:32,305 iteration 2170 : loss : 0.065297, loss_ce: 0.029662
2022-01-11 23:14:33,912 iteration 2171 : loss : 0.052089, loss_ce: 0.019137
2022-01-11 23:14:35,576 iteration 2172 : loss : 0.038991, loss_ce: 0.012506
2022-01-11 23:14:37,094 iteration 2173 : loss : 0.044820, loss_ce: 0.014961
2022-01-11 23:14:38,578 iteration 2174 : loss : 0.033346, loss_ce: 0.015781
2022-01-11 23:14:40,094 iteration 2175 : loss : 0.032450, loss_ce: 0.011707
2022-01-11 23:14:41,762 iteration 2176 : loss : 0.052452, loss_ce: 0.022623
 32%|████████▋                  | 128/400 [1:02:12<2:07:46, 28.19s/it]2022-01-11 23:14:43,383 iteration 2177 : loss : 0.036982, loss_ce: 0.012917
2022-01-11 23:14:44,994 iteration 2178 : loss : 0.070843, loss_ce: 0.039379
2022-01-11 23:14:46,549 iteration 2179 : loss : 0.037039, loss_ce: 0.017582
2022-01-11 23:14:48,096 iteration 2180 : loss : 0.042757, loss_ce: 0.017331
2022-01-11 23:14:49,684 iteration 2181 : loss : 0.047327, loss_ce: 0.021330
2022-01-11 23:14:51,230 iteration 2182 : loss : 0.056403, loss_ce: 0.015314
2022-01-11 23:14:52,735 iteration 2183 : loss : 0.029297, loss_ce: 0.012819
2022-01-11 23:14:54,377 iteration 2184 : loss : 0.068821, loss_ce: 0.019716
2022-01-11 23:14:55,904 iteration 2185 : loss : 0.030498, loss_ce: 0.013035
2022-01-11 23:14:57,446 iteration 2186 : loss : 0.035686, loss_ce: 0.014713
2022-01-11 23:14:59,013 iteration 2187 : loss : 0.029569, loss_ce: 0.012200
2022-01-11 23:15:00,547 iteration 2188 : loss : 0.060679, loss_ce: 0.016051
2022-01-11 23:15:02,175 iteration 2189 : loss : 0.062112, loss_ce: 0.022312
2022-01-11 23:15:03,737 iteration 2190 : loss : 0.043620, loss_ce: 0.012948
2022-01-11 23:15:05,320 iteration 2191 : loss : 0.086203, loss_ce: 0.023785
2022-01-11 23:15:06,835 iteration 2192 : loss : 0.036572, loss_ce: 0.011706
2022-01-11 23:15:08,406 iteration 2193 : loss : 0.054802, loss_ce: 0.020861
 32%|████████▋                  | 129/400 [1:02:39<2:05:12, 27.72s/it]2022-01-11 23:15:10,056 iteration 2194 : loss : 0.046848, loss_ce: 0.023439
2022-01-11 23:15:11,593 iteration 2195 : loss : 0.042111, loss_ce: 0.015449
2022-01-11 23:15:13,154 iteration 2196 : loss : 0.033370, loss_ce: 0.011668
2022-01-11 23:15:14,737 iteration 2197 : loss : 0.039591, loss_ce: 0.019755
2022-01-11 23:15:16,308 iteration 2198 : loss : 0.040344, loss_ce: 0.018394
2022-01-11 23:15:17,886 iteration 2199 : loss : 0.040491, loss_ce: 0.018568
2022-01-11 23:15:19,423 iteration 2200 : loss : 0.054057, loss_ce: 0.020698
2022-01-11 23:15:20,959 iteration 2201 : loss : 0.029994, loss_ce: 0.011769
2022-01-11 23:15:22,568 iteration 2202 : loss : 0.119310, loss_ce: 0.034818
2022-01-11 23:15:24,183 iteration 2203 : loss : 0.037321, loss_ce: 0.019658
2022-01-11 23:15:25,723 iteration 2204 : loss : 0.035706, loss_ce: 0.015694
2022-01-11 23:15:27,212 iteration 2205 : loss : 0.028348, loss_ce: 0.012510
2022-01-11 23:15:28,694 iteration 2206 : loss : 0.044489, loss_ce: 0.012132
2022-01-11 23:15:30,225 iteration 2207 : loss : 0.052716, loss_ce: 0.019412
2022-01-11 23:15:31,815 iteration 2208 : loss : 0.041772, loss_ce: 0.012656
2022-01-11 23:15:33,357 iteration 2209 : loss : 0.059476, loss_ce: 0.019620
2022-01-11 23:15:33,358 Training Data Eval:
2022-01-11 23:15:41,378   Average segmentation loss on training set: 0.0400
2022-01-11 23:15:41,378 Validation Data Eval:
2022-01-11 23:15:44,133   Average segmentation loss on validation set: 0.1494
2022-01-11 23:15:45,718 iteration 2210 : loss : 0.057457, loss_ce: 0.022678
 32%|████████▊                  | 130/400 [1:03:16<2:17:42, 30.60s/it]2022-01-11 23:15:47,355 iteration 2211 : loss : 0.065144, loss_ce: 0.020456
2022-01-11 23:15:48,948 iteration 2212 : loss : 0.046667, loss_ce: 0.019454
2022-01-11 23:15:50,559 iteration 2213 : loss : 0.043534, loss_ce: 0.015634
2022-01-11 23:15:52,151 iteration 2214 : loss : 0.035823, loss_ce: 0.011784
2022-01-11 23:15:53,767 iteration 2215 : loss : 0.069468, loss_ce: 0.031440
2022-01-11 23:15:55,271 iteration 2216 : loss : 0.036579, loss_ce: 0.012605
2022-01-11 23:15:56,889 iteration 2217 : loss : 0.052929, loss_ce: 0.022850
2022-01-11 23:15:58,407 iteration 2218 : loss : 0.044751, loss_ce: 0.018343
2022-01-11 23:16:00,020 iteration 2219 : loss : 0.037407, loss_ce: 0.018465
2022-01-11 23:16:01,501 iteration 2220 : loss : 0.046712, loss_ce: 0.018346
2022-01-11 23:16:03,038 iteration 2221 : loss : 0.027444, loss_ce: 0.010145
2022-01-11 23:16:04,641 iteration 2222 : loss : 0.032202, loss_ce: 0.012295
2022-01-11 23:16:06,215 iteration 2223 : loss : 0.052952, loss_ce: 0.018216
2022-01-11 23:16:07,839 iteration 2224 : loss : 0.055974, loss_ce: 0.025277
2022-01-11 23:16:09,346 iteration 2225 : loss : 0.022385, loss_ce: 0.008624
2022-01-11 23:16:10,971 iteration 2226 : loss : 0.051117, loss_ce: 0.019717
2022-01-11 23:16:12,508 iteration 2227 : loss : 0.040302, loss_ce: 0.014784
 33%|████████▊                  | 131/400 [1:03:43<2:12:03, 29.46s/it]2022-01-11 23:16:14,167 iteration 2228 : loss : 0.044271, loss_ce: 0.020479
2022-01-11 23:16:15,730 iteration 2229 : loss : 0.039939, loss_ce: 0.018371
2022-01-11 23:16:17,338 iteration 2230 : loss : 0.034704, loss_ce: 0.010808
2022-01-11 23:16:18,813 iteration 2231 : loss : 0.027685, loss_ce: 0.009572
2022-01-11 23:16:20,339 iteration 2232 : loss : 0.042409, loss_ce: 0.016064
2022-01-11 23:16:21,885 iteration 2233 : loss : 0.036610, loss_ce: 0.013228
2022-01-11 23:16:23,435 iteration 2234 : loss : 0.035310, loss_ce: 0.013213
2022-01-11 23:16:24,960 iteration 2235 : loss : 0.069702, loss_ce: 0.015596
2022-01-11 23:16:26,641 iteration 2236 : loss : 0.056267, loss_ce: 0.026300
2022-01-11 23:16:28,207 iteration 2237 : loss : 0.045531, loss_ce: 0.018383
2022-01-11 23:16:29,783 iteration 2238 : loss : 0.036692, loss_ce: 0.017692
2022-01-11 23:16:31,337 iteration 2239 : loss : 0.033473, loss_ce: 0.013072
2022-01-11 23:16:32,927 iteration 2240 : loss : 0.053404, loss_ce: 0.017455
2022-01-11 23:16:34,554 iteration 2241 : loss : 0.056471, loss_ce: 0.016490
2022-01-11 23:16:36,174 iteration 2242 : loss : 0.058807, loss_ce: 0.022898
2022-01-11 23:16:37,829 iteration 2243 : loss : 0.051730, loss_ce: 0.016877
2022-01-11 23:16:39,369 iteration 2244 : loss : 0.038899, loss_ce: 0.016721
 33%|████████▉                  | 132/400 [1:04:10<2:08:05, 28.68s/it]2022-01-11 23:16:41,048 iteration 2245 : loss : 0.061170, loss_ce: 0.022329
2022-01-11 23:16:42,570 iteration 2246 : loss : 0.032646, loss_ce: 0.011283
2022-01-11 23:16:44,227 iteration 2247 : loss : 0.070404, loss_ce: 0.020133
2022-01-11 23:16:45,810 iteration 2248 : loss : 0.057044, loss_ce: 0.023228
2022-01-11 23:16:47,374 iteration 2249 : loss : 0.039515, loss_ce: 0.012133
2022-01-11 23:16:48,929 iteration 2250 : loss : 0.052300, loss_ce: 0.021486
2022-01-11 23:16:50,427 iteration 2251 : loss : 0.044797, loss_ce: 0.015682
2022-01-11 23:16:52,105 iteration 2252 : loss : 0.085683, loss_ce: 0.029442
2022-01-11 23:16:53,746 iteration 2253 : loss : 0.058483, loss_ce: 0.026887
2022-01-11 23:16:55,298 iteration 2254 : loss : 0.039507, loss_ce: 0.021375
2022-01-11 23:16:56,841 iteration 2255 : loss : 0.036392, loss_ce: 0.013366
2022-01-11 23:16:58,433 iteration 2256 : loss : 0.038204, loss_ce: 0.015468
2022-01-11 23:16:59,985 iteration 2257 : loss : 0.039552, loss_ce: 0.016305
2022-01-11 23:17:01,486 iteration 2258 : loss : 0.027136, loss_ce: 0.010909
2022-01-11 23:17:03,129 iteration 2259 : loss : 0.032718, loss_ce: 0.012307
2022-01-11 23:17:04,668 iteration 2260 : loss : 0.040837, loss_ce: 0.020335
2022-01-11 23:17:06,269 iteration 2261 : loss : 0.052651, loss_ce: 0.025252
 33%|████████▉                  | 133/400 [1:04:37<2:05:14, 28.14s/it]2022-01-11 23:17:07,908 iteration 2262 : loss : 0.041200, loss_ce: 0.016664
2022-01-11 23:17:09,490 iteration 2263 : loss : 0.044348, loss_ce: 0.018430
2022-01-11 23:17:11,093 iteration 2264 : loss : 0.046630, loss_ce: 0.017233
2022-01-11 23:17:12,643 iteration 2265 : loss : 0.031120, loss_ce: 0.012221
2022-01-11 23:17:14,197 iteration 2266 : loss : 0.033154, loss_ce: 0.009914
2022-01-11 23:17:15,702 iteration 2267 : loss : 0.029137, loss_ce: 0.012089
2022-01-11 23:17:17,258 iteration 2268 : loss : 0.035011, loss_ce: 0.014542
2022-01-11 23:17:18,786 iteration 2269 : loss : 0.046757, loss_ce: 0.021589
2022-01-11 23:17:20,286 iteration 2270 : loss : 0.027669, loss_ce: 0.009426
2022-01-11 23:17:21,843 iteration 2271 : loss : 0.073058, loss_ce: 0.030813
2022-01-11 23:17:23,467 iteration 2272 : loss : 0.051313, loss_ce: 0.018417
2022-01-11 23:17:25,090 iteration 2273 : loss : 0.043690, loss_ce: 0.015341
2022-01-11 23:17:26,665 iteration 2274 : loss : 0.036793, loss_ce: 0.015801
2022-01-11 23:17:28,227 iteration 2275 : loss : 0.056886, loss_ce: 0.018357
2022-01-11 23:17:29,746 iteration 2276 : loss : 0.071151, loss_ce: 0.038363
2022-01-11 23:17:31,304 iteration 2277 : loss : 0.030239, loss_ce: 0.009026
2022-01-11 23:17:32,898 iteration 2278 : loss : 0.030009, loss_ce: 0.010730
 34%|█████████                  | 134/400 [1:05:03<2:02:45, 27.69s/it]2022-01-11 23:17:34,448 iteration 2279 : loss : 0.032639, loss_ce: 0.012980
2022-01-11 23:17:35,961 iteration 2280 : loss : 0.037774, loss_ce: 0.019568
2022-01-11 23:17:37,534 iteration 2281 : loss : 0.053300, loss_ce: 0.017409
2022-01-11 23:17:39,043 iteration 2282 : loss : 0.041096, loss_ce: 0.014526
2022-01-11 23:17:40,630 iteration 2283 : loss : 0.045522, loss_ce: 0.013558
2022-01-11 23:17:42,125 iteration 2284 : loss : 0.041941, loss_ce: 0.019369
2022-01-11 23:17:43,736 iteration 2285 : loss : 0.037462, loss_ce: 0.017910
2022-01-11 23:17:45,263 iteration 2286 : loss : 0.062094, loss_ce: 0.038433
2022-01-11 23:17:46,815 iteration 2287 : loss : 0.026167, loss_ce: 0.007026
2022-01-11 23:17:48,384 iteration 2288 : loss : 0.050450, loss_ce: 0.015991
2022-01-11 23:17:49,971 iteration 2289 : loss : 0.034379, loss_ce: 0.011507
2022-01-11 23:17:51,535 iteration 2290 : loss : 0.039884, loss_ce: 0.013412
2022-01-11 23:17:53,197 iteration 2291 : loss : 0.044289, loss_ce: 0.018184
2022-01-11 23:17:54,797 iteration 2292 : loss : 0.049070, loss_ce: 0.018419
2022-01-11 23:17:56,384 iteration 2293 : loss : 0.080879, loss_ce: 0.034338
2022-01-11 23:17:57,937 iteration 2294 : loss : 0.031814, loss_ce: 0.015736
2022-01-11 23:17:57,937 Training Data Eval:
2022-01-11 23:18:05,965   Average segmentation loss on training set: 0.0318
2022-01-11 23:18:05,965 Validation Data Eval:
2022-01-11 23:18:08,728   Average segmentation loss on validation set: 0.0677
2022-01-11 23:18:10,300 iteration 2295 : loss : 0.050111, loss_ce: 0.018605
 34%|█████████                  | 135/400 [1:05:41<2:15:09, 30.60s/it]2022-01-11 23:18:11,858 iteration 2296 : loss : 0.028080, loss_ce: 0.011325
2022-01-11 23:18:13,476 iteration 2297 : loss : 0.033018, loss_ce: 0.013034
2022-01-11 23:18:15,052 iteration 2298 : loss : 0.047311, loss_ce: 0.017650
2022-01-11 23:18:16,659 iteration 2299 : loss : 0.040595, loss_ce: 0.012392
2022-01-11 23:18:18,204 iteration 2300 : loss : 0.031376, loss_ce: 0.013870
2022-01-11 23:18:19,776 iteration 2301 : loss : 0.037691, loss_ce: 0.015456
2022-01-11 23:18:21,390 iteration 2302 : loss : 0.032644, loss_ce: 0.012681
2022-01-11 23:18:22,973 iteration 2303 : loss : 0.040445, loss_ce: 0.012595
2022-01-11 23:18:24,555 iteration 2304 : loss : 0.040227, loss_ce: 0.017462
2022-01-11 23:18:26,109 iteration 2305 : loss : 0.033075, loss_ce: 0.015149
2022-01-11 23:18:27,643 iteration 2306 : loss : 0.038958, loss_ce: 0.016156
2022-01-11 23:18:29,264 iteration 2307 : loss : 0.030922, loss_ce: 0.011796
2022-01-11 23:18:30,788 iteration 2308 : loss : 0.037447, loss_ce: 0.014744
2022-01-11 23:18:32,404 iteration 2309 : loss : 0.041656, loss_ce: 0.016930
2022-01-11 23:18:34,001 iteration 2310 : loss : 0.035929, loss_ce: 0.017283
2022-01-11 23:18:35,529 iteration 2311 : loss : 0.062020, loss_ce: 0.026773
2022-01-11 23:18:37,044 iteration 2312 : loss : 0.028601, loss_ce: 0.010787
 34%|█████████▏                 | 136/400 [1:06:07<2:09:34, 29.45s/it]2022-01-11 23:18:38,648 iteration 2313 : loss : 0.036248, loss_ce: 0.012772
2022-01-11 23:18:40,241 iteration 2314 : loss : 0.035080, loss_ce: 0.012309
2022-01-11 23:18:41,878 iteration 2315 : loss : 0.045791, loss_ce: 0.013166
2022-01-11 23:18:43,362 iteration 2316 : loss : 0.040880, loss_ce: 0.016732
2022-01-11 23:18:44,992 iteration 2317 : loss : 0.050217, loss_ce: 0.014796
2022-01-11 23:18:46,582 iteration 2318 : loss : 0.035015, loss_ce: 0.016117
2022-01-11 23:18:48,168 iteration 2319 : loss : 0.027417, loss_ce: 0.010876
2022-01-11 23:18:49,785 iteration 2320 : loss : 0.039002, loss_ce: 0.017972
2022-01-11 23:18:51,292 iteration 2321 : loss : 0.039419, loss_ce: 0.013296
2022-01-11 23:18:52,843 iteration 2322 : loss : 0.040326, loss_ce: 0.019336
2022-01-11 23:18:54,401 iteration 2323 : loss : 0.051179, loss_ce: 0.022584
2022-01-11 23:18:55,999 iteration 2324 : loss : 0.047164, loss_ce: 0.016493
2022-01-11 23:18:57,533 iteration 2325 : loss : 0.034168, loss_ce: 0.013637
2022-01-11 23:18:59,107 iteration 2326 : loss : 0.043997, loss_ce: 0.021519
2022-01-11 23:19:00,654 iteration 2327 : loss : 0.035641, loss_ce: 0.014702
2022-01-11 23:19:02,244 iteration 2328 : loss : 0.048678, loss_ce: 0.022654
2022-01-11 23:19:03,778 iteration 2329 : loss : 0.029307, loss_ce: 0.011659
 34%|█████████▏                 | 137/400 [1:06:34<2:05:29, 28.63s/it]2022-01-11 23:19:05,345 iteration 2330 : loss : 0.037434, loss_ce: 0.014783
2022-01-11 23:19:06,906 iteration 2331 : loss : 0.042477, loss_ce: 0.019102
2022-01-11 23:19:08,526 iteration 2332 : loss : 0.061159, loss_ce: 0.024085
2022-01-11 23:19:10,140 iteration 2333 : loss : 0.036033, loss_ce: 0.017169
2022-01-11 23:19:11,670 iteration 2334 : loss : 0.031189, loss_ce: 0.015172
2022-01-11 23:19:13,236 iteration 2335 : loss : 0.044382, loss_ce: 0.015167
2022-01-11 23:19:14,848 iteration 2336 : loss : 0.082155, loss_ce: 0.018442
2022-01-11 23:19:16,437 iteration 2337 : loss : 0.037105, loss_ce: 0.016132
2022-01-11 23:19:17,987 iteration 2338 : loss : 0.030706, loss_ce: 0.012692
2022-01-11 23:19:19,588 iteration 2339 : loss : 0.065510, loss_ce: 0.021767
2022-01-11 23:19:21,221 iteration 2340 : loss : 0.049923, loss_ce: 0.022094
2022-01-11 23:19:22,709 iteration 2341 : loss : 0.040397, loss_ce: 0.011142
2022-01-11 23:19:24,286 iteration 2342 : loss : 0.033662, loss_ce: 0.013711
2022-01-11 23:19:25,919 iteration 2343 : loss : 0.052573, loss_ce: 0.019541
2022-01-11 23:19:27,509 iteration 2344 : loss : 0.073100, loss_ce: 0.027477
2022-01-11 23:19:29,140 iteration 2345 : loss : 0.048944, loss_ce: 0.019536
2022-01-11 23:19:30,716 iteration 2346 : loss : 0.039790, loss_ce: 0.016641
 34%|█████████▎                 | 138/400 [1:07:01<2:02:48, 28.12s/it]2022-01-11 23:19:32,316 iteration 2347 : loss : 0.046271, loss_ce: 0.016480
2022-01-11 23:19:33,869 iteration 2348 : loss : 0.037119, loss_ce: 0.013376
2022-01-11 23:19:35,494 iteration 2349 : loss : 0.043408, loss_ce: 0.017958
2022-01-11 23:19:37,042 iteration 2350 : loss : 0.042245, loss_ce: 0.016063
2022-01-11 23:19:38,636 iteration 2351 : loss : 0.134105, loss_ce: 0.031736
2022-01-11 23:19:40,213 iteration 2352 : loss : 0.035100, loss_ce: 0.011301
2022-01-11 23:19:41,821 iteration 2353 : loss : 0.059422, loss_ce: 0.027151
2022-01-11 23:19:43,450 iteration 2354 : loss : 0.063801, loss_ce: 0.022972
2022-01-11 23:19:44,942 iteration 2355 : loss : 0.040886, loss_ce: 0.015112
2022-01-11 23:19:46,501 iteration 2356 : loss : 0.039445, loss_ce: 0.014798
2022-01-11 23:19:48,065 iteration 2357 : loss : 0.049033, loss_ce: 0.016942
2022-01-11 23:19:49,671 iteration 2358 : loss : 0.042389, loss_ce: 0.017394
2022-01-11 23:19:51,345 iteration 2359 : loss : 0.054401, loss_ce: 0.020205
2022-01-11 23:19:52,876 iteration 2360 : loss : 0.043918, loss_ce: 0.020920
2022-01-11 23:19:54,477 iteration 2361 : loss : 0.048698, loss_ce: 0.018364
2022-01-11 23:19:56,160 iteration 2362 : loss : 0.052955, loss_ce: 0.021319
2022-01-11 23:19:57,680 iteration 2363 : loss : 0.053229, loss_ce: 0.029269
 35%|█████████▍                 | 139/400 [1:07:28<2:00:49, 27.78s/it]2022-01-11 23:19:59,202 iteration 2364 : loss : 0.092884, loss_ce: 0.018108
2022-01-11 23:20:00,742 iteration 2365 : loss : 0.070644, loss_ce: 0.017820
2022-01-11 23:20:02,341 iteration 2366 : loss : 0.048177, loss_ce: 0.019938
2022-01-11 23:20:03,880 iteration 2367 : loss : 0.050001, loss_ce: 0.025619
2022-01-11 23:20:05,417 iteration 2368 : loss : 0.039901, loss_ce: 0.014233
2022-01-11 23:20:07,002 iteration 2369 : loss : 0.071485, loss_ce: 0.032637
2022-01-11 23:20:08,493 iteration 2370 : loss : 0.043233, loss_ce: 0.016129
2022-01-11 23:20:10,101 iteration 2371 : loss : 0.039377, loss_ce: 0.014835
2022-01-11 23:20:11,661 iteration 2372 : loss : 0.053080, loss_ce: 0.020529
2022-01-11 23:20:13,193 iteration 2373 : loss : 0.041860, loss_ce: 0.016472
2022-01-11 23:20:14,733 iteration 2374 : loss : 0.041096, loss_ce: 0.016701
2022-01-11 23:20:16,334 iteration 2375 : loss : 0.054366, loss_ce: 0.018573
2022-01-11 23:20:17,949 iteration 2376 : loss : 0.036461, loss_ce: 0.013227
2022-01-11 23:20:19,498 iteration 2377 : loss : 0.035533, loss_ce: 0.013672
2022-01-11 23:20:21,047 iteration 2378 : loss : 0.034383, loss_ce: 0.013513
2022-01-11 23:20:22,631 iteration 2379 : loss : 0.044606, loss_ce: 0.023490
2022-01-11 23:20:22,632 Training Data Eval:
2022-01-11 23:20:30,637   Average segmentation loss on training set: 0.0333
2022-01-11 23:20:30,638 Validation Data Eval:
2022-01-11 23:20:33,399   Average segmentation loss on validation set: 0.0803
2022-01-11 23:20:34,926 iteration 2380 : loss : 0.032370, loss_ce: 0.015344
 35%|█████████▍                 | 140/400 [1:08:05<2:12:40, 30.62s/it]2022-01-11 23:20:36,639 iteration 2381 : loss : 0.057207, loss_ce: 0.018367
2022-01-11 23:20:38,240 iteration 2382 : loss : 0.045851, loss_ce: 0.016621
2022-01-11 23:20:39,826 iteration 2383 : loss : 0.042017, loss_ce: 0.017751
2022-01-11 23:20:41,354 iteration 2384 : loss : 0.029825, loss_ce: 0.010394
2022-01-11 23:20:42,938 iteration 2385 : loss : 0.033800, loss_ce: 0.012061
2022-01-11 23:20:44,532 iteration 2386 : loss : 0.082504, loss_ce: 0.020898
2022-01-11 23:20:46,153 iteration 2387 : loss : 0.033500, loss_ce: 0.011778
2022-01-11 23:20:47,738 iteration 2388 : loss : 0.035670, loss_ce: 0.015845
2022-01-11 23:20:49,319 iteration 2389 : loss : 0.036825, loss_ce: 0.013358
2022-01-11 23:20:50,858 iteration 2390 : loss : 0.042328, loss_ce: 0.016802
2022-01-11 23:20:52,357 iteration 2391 : loss : 0.039789, loss_ce: 0.015135
2022-01-11 23:20:54,051 iteration 2392 : loss : 0.075227, loss_ce: 0.024208
2022-01-11 23:20:55,676 iteration 2393 : loss : 0.039418, loss_ce: 0.016796
2022-01-11 23:20:57,269 iteration 2394 : loss : 0.045962, loss_ce: 0.019555
2022-01-11 23:20:58,911 iteration 2395 : loss : 0.047960, loss_ce: 0.020766
2022-01-11 23:21:00,430 iteration 2396 : loss : 0.038388, loss_ce: 0.016404
2022-01-11 23:21:02,048 iteration 2397 : loss : 0.050397, loss_ce: 0.028343
 35%|█████████▌                 | 141/400 [1:08:32<2:07:38, 29.57s/it]2022-01-11 23:21:03,629 iteration 2398 : loss : 0.036856, loss_ce: 0.018655
2022-01-11 23:21:05,282 iteration 2399 : loss : 0.039867, loss_ce: 0.015111
2022-01-11 23:21:06,831 iteration 2400 : loss : 0.047022, loss_ce: 0.019147
2022-01-11 23:21:08,517 iteration 2401 : loss : 0.076313, loss_ce: 0.030533
2022-01-11 23:21:10,061 iteration 2402 : loss : 0.047125, loss_ce: 0.018362
2022-01-11 23:21:11,606 iteration 2403 : loss : 0.050401, loss_ce: 0.022266
2022-01-11 23:21:13,137 iteration 2404 : loss : 0.034017, loss_ce: 0.013477
2022-01-11 23:21:14,744 iteration 2405 : loss : 0.047636, loss_ce: 0.016579
2022-01-11 23:21:16,257 iteration 2406 : loss : 0.049279, loss_ce: 0.021647
2022-01-11 23:21:17,809 iteration 2407 : loss : 0.029994, loss_ce: 0.010232
2022-01-11 23:21:19,522 iteration 2408 : loss : 0.057770, loss_ce: 0.018495
2022-01-11 23:21:21,048 iteration 2409 : loss : 0.034668, loss_ce: 0.014775
2022-01-11 23:21:22,582 iteration 2410 : loss : 0.044168, loss_ce: 0.015824
2022-01-11 23:21:24,113 iteration 2411 : loss : 0.034770, loss_ce: 0.013405
2022-01-11 23:21:25,657 iteration 2412 : loss : 0.037005, loss_ce: 0.015390
2022-01-11 23:21:27,180 iteration 2413 : loss : 0.045172, loss_ce: 0.018633
2022-01-11 23:21:28,691 iteration 2414 : loss : 0.029122, loss_ce: 0.011975
 36%|█████████▌                 | 142/400 [1:08:59<2:03:22, 28.69s/it]2022-01-11 23:21:30,341 iteration 2415 : loss : 0.036930, loss_ce: 0.013527
2022-01-11 23:21:31,882 iteration 2416 : loss : 0.034635, loss_ce: 0.013691
2022-01-11 23:21:33,441 iteration 2417 : loss : 0.038645, loss_ce: 0.021290
2022-01-11 23:21:34,952 iteration 2418 : loss : 0.030912, loss_ce: 0.009336
2022-01-11 23:21:36,526 iteration 2419 : loss : 0.065306, loss_ce: 0.022516
2022-01-11 23:21:38,150 iteration 2420 : loss : 0.045286, loss_ce: 0.022029
2022-01-11 23:21:39,709 iteration 2421 : loss : 0.034686, loss_ce: 0.017011
2022-01-11 23:21:41,276 iteration 2422 : loss : 0.061623, loss_ce: 0.029581
2022-01-11 23:21:42,771 iteration 2423 : loss : 0.033496, loss_ce: 0.015234
2022-01-11 23:21:44,337 iteration 2424 : loss : 0.032958, loss_ce: 0.013919
2022-01-11 23:21:45,923 iteration 2425 : loss : 0.063933, loss_ce: 0.023352
2022-01-11 23:21:47,498 iteration 2426 : loss : 0.040601, loss_ce: 0.013907
2022-01-11 23:21:49,023 iteration 2427 : loss : 0.042036, loss_ce: 0.019546
2022-01-11 23:21:50,585 iteration 2428 : loss : 0.031452, loss_ce: 0.010112
2022-01-11 23:21:52,113 iteration 2429 : loss : 0.057266, loss_ce: 0.020988
2022-01-11 23:21:53,667 iteration 2430 : loss : 0.044479, loss_ce: 0.021176
2022-01-11 23:21:55,249 iteration 2431 : loss : 0.035339, loss_ce: 0.011452
 36%|█████████▋                 | 143/400 [1:09:26<2:00:09, 28.05s/it]2022-01-11 23:21:56,787 iteration 2432 : loss : 0.054709, loss_ce: 0.021118
2022-01-11 23:21:58,327 iteration 2433 : loss : 0.029500, loss_ce: 0.013627
2022-01-11 23:21:59,926 iteration 2434 : loss : 0.058463, loss_ce: 0.021972
2022-01-11 23:22:01,486 iteration 2435 : loss : 0.028284, loss_ce: 0.012030
2022-01-11 23:22:03,118 iteration 2436 : loss : 0.034934, loss_ce: 0.013283
2022-01-11 23:22:04,704 iteration 2437 : loss : 0.032129, loss_ce: 0.012428
2022-01-11 23:22:06,217 iteration 2438 : loss : 0.028155, loss_ce: 0.011946
2022-01-11 23:22:07,782 iteration 2439 : loss : 0.035662, loss_ce: 0.013290
2022-01-11 23:22:09,358 iteration 2440 : loss : 0.035539, loss_ce: 0.013623
2022-01-11 23:22:11,024 iteration 2441 : loss : 0.052519, loss_ce: 0.016052
2022-01-11 23:22:12,633 iteration 2442 : loss : 0.033705, loss_ce: 0.011002
2022-01-11 23:22:14,248 iteration 2443 : loss : 0.033543, loss_ce: 0.014552
2022-01-11 23:22:15,852 iteration 2444 : loss : 0.036832, loss_ce: 0.013603
2022-01-11 23:22:17,527 iteration 2445 : loss : 0.040177, loss_ce: 0.014040
2022-01-11 23:22:19,065 iteration 2446 : loss : 0.031498, loss_ce: 0.013188
2022-01-11 23:22:20,689 iteration 2447 : loss : 0.049853, loss_ce: 0.019346
2022-01-11 23:22:22,209 iteration 2448 : loss : 0.051102, loss_ce: 0.028444
 36%|█████████▋                 | 144/400 [1:09:53<1:58:17, 27.72s/it]2022-01-11 23:22:23,823 iteration 2449 : loss : 0.030738, loss_ce: 0.008387
2022-01-11 23:22:25,346 iteration 2450 : loss : 0.033263, loss_ce: 0.012494
2022-01-11 23:22:26,863 iteration 2451 : loss : 0.026378, loss_ce: 0.009562
2022-01-11 23:22:28,413 iteration 2452 : loss : 0.041821, loss_ce: 0.016788
2022-01-11 23:22:29,972 iteration 2453 : loss : 0.038737, loss_ce: 0.018828
2022-01-11 23:22:31,521 iteration 2454 : loss : 0.055023, loss_ce: 0.022530
2022-01-11 23:22:33,174 iteration 2455 : loss : 0.077056, loss_ce: 0.030622
2022-01-11 23:22:34,760 iteration 2456 : loss : 0.038649, loss_ce: 0.018218
2022-01-11 23:22:36,287 iteration 2457 : loss : 0.050985, loss_ce: 0.022759
2022-01-11 23:22:37,887 iteration 2458 : loss : 0.066371, loss_ce: 0.018989
2022-01-11 23:22:39,455 iteration 2459 : loss : 0.040480, loss_ce: 0.014220
2022-01-11 23:22:41,070 iteration 2460 : loss : 0.143522, loss_ce: 0.043770
2022-01-11 23:22:42,657 iteration 2461 : loss : 0.140831, loss_ce: 0.038527
2022-01-11 23:22:44,316 iteration 2462 : loss : 0.091938, loss_ce: 0.040813
2022-01-11 23:22:45,845 iteration 2463 : loss : 0.075861, loss_ce: 0.029580
2022-01-11 23:22:47,421 iteration 2464 : loss : 0.077417, loss_ce: 0.049345
2022-01-11 23:22:47,422 Training Data Eval:
2022-01-11 23:22:55,427   Average segmentation loss on training set: 0.0567
2022-01-11 23:22:55,427 Validation Data Eval:
2022-01-11 23:22:58,185   Average segmentation loss on validation set: 0.1084
2022-01-11 23:22:59,770 iteration 2465 : loss : 0.061452, loss_ce: 0.028418
 36%|█████████▊                 | 145/400 [1:10:30<2:10:21, 30.67s/it]2022-01-11 23:23:01,412 iteration 2466 : loss : 0.110318, loss_ce: 0.045538
2022-01-11 23:23:02,967 iteration 2467 : loss : 0.055550, loss_ce: 0.024026
2022-01-11 23:23:04,605 iteration 2468 : loss : 0.086802, loss_ce: 0.029961
2022-01-11 23:23:06,135 iteration 2469 : loss : 0.070574, loss_ce: 0.036460
2022-01-11 23:23:07,746 iteration 2470 : loss : 0.072825, loss_ce: 0.032607
2022-01-11 23:23:09,340 iteration 2471 : loss : 0.058521, loss_ce: 0.026827
2022-01-11 23:23:10,934 iteration 2472 : loss : 0.052902, loss_ce: 0.018937
2022-01-11 23:23:12,499 iteration 2473 : loss : 0.090486, loss_ce: 0.039848
2022-01-11 23:23:13,977 iteration 2474 : loss : 0.091125, loss_ce: 0.076086
2022-01-11 23:23:15,580 iteration 2475 : loss : 0.087846, loss_ce: 0.053977
2022-01-11 23:23:17,170 iteration 2476 : loss : 0.087326, loss_ce: 0.031599
2022-01-11 23:23:18,748 iteration 2477 : loss : 0.073792, loss_ce: 0.023788
2022-01-11 23:23:20,303 iteration 2478 : loss : 0.059913, loss_ce: 0.022317
2022-01-11 23:23:21,918 iteration 2479 : loss : 0.059991, loss_ce: 0.018735
2022-01-11 23:23:23,460 iteration 2480 : loss : 0.039993, loss_ce: 0.014375
2022-01-11 23:23:24,991 iteration 2481 : loss : 0.051410, loss_ce: 0.022209
2022-01-11 23:23:26,648 iteration 2482 : loss : 0.088369, loss_ce: 0.038456
 36%|█████████▊                 | 146/400 [1:10:57<2:05:01, 29.53s/it]2022-01-11 23:23:28,231 iteration 2483 : loss : 0.083422, loss_ce: 0.038544
2022-01-11 23:23:29,826 iteration 2484 : loss : 0.117815, loss_ce: 0.021810
2022-01-11 23:23:31,407 iteration 2485 : loss : 0.091265, loss_ce: 0.032437
2022-01-11 23:23:32,961 iteration 2486 : loss : 0.076170, loss_ce: 0.034081
2022-01-11 23:23:34,479 iteration 2487 : loss : 0.091522, loss_ce: 0.043425
2022-01-11 23:23:36,097 iteration 2488 : loss : 0.077177, loss_ce: 0.040125
2022-01-11 23:23:37,689 iteration 2489 : loss : 0.055858, loss_ce: 0.025731
2022-01-11 23:23:39,329 iteration 2490 : loss : 0.074932, loss_ce: 0.031130
2022-01-11 23:23:40,923 iteration 2491 : loss : 0.102929, loss_ce: 0.044833
2022-01-11 23:23:42,510 iteration 2492 : loss : 0.083194, loss_ce: 0.034707
2022-01-11 23:23:44,033 iteration 2493 : loss : 0.072639, loss_ce: 0.028575
2022-01-11 23:23:45,603 iteration 2494 : loss : 0.074338, loss_ce: 0.027983
2022-01-11 23:23:47,242 iteration 2495 : loss : 0.047433, loss_ce: 0.025123
2022-01-11 23:23:48,707 iteration 2496 : loss : 0.044250, loss_ce: 0.018490
2022-01-11 23:23:50,230 iteration 2497 : loss : 0.074159, loss_ce: 0.026816
2022-01-11 23:23:51,917 iteration 2498 : loss : 0.066595, loss_ce: 0.026180
2022-01-11 23:23:53,486 iteration 2499 : loss : 0.057905, loss_ce: 0.023878
 37%|█████████▉                 | 147/400 [1:11:24<2:01:07, 28.73s/it]2022-01-11 23:23:55,105 iteration 2500 : loss : 0.072720, loss_ce: 0.026819
2022-01-11 23:23:56,691 iteration 2501 : loss : 0.046913, loss_ce: 0.020106
2022-01-11 23:23:58,281 iteration 2502 : loss : 0.062736, loss_ce: 0.031176
2022-01-11 23:23:59,839 iteration 2503 : loss : 0.080538, loss_ce: 0.035563
2022-01-11 23:24:01,442 iteration 2504 : loss : 0.060866, loss_ce: 0.023161
2022-01-11 23:24:03,026 iteration 2505 : loss : 0.091217, loss_ce: 0.035104
2022-01-11 23:24:04,609 iteration 2506 : loss : 0.050881, loss_ce: 0.022612
2022-01-11 23:24:06,135 iteration 2507 : loss : 0.056840, loss_ce: 0.025724
2022-01-11 23:24:07,628 iteration 2508 : loss : 0.052975, loss_ce: 0.023290
2022-01-11 23:24:09,177 iteration 2509 : loss : 0.069315, loss_ce: 0.030401
2022-01-11 23:24:10,694 iteration 2510 : loss : 0.086592, loss_ce: 0.028826
2022-01-11 23:24:12,252 iteration 2511 : loss : 0.065295, loss_ce: 0.027869
2022-01-11 23:24:13,798 iteration 2512 : loss : 0.045041, loss_ce: 0.017869
2022-01-11 23:24:15,350 iteration 2513 : loss : 0.081689, loss_ce: 0.035898
2022-01-11 23:24:16,929 iteration 2514 : loss : 0.067584, loss_ce: 0.026995
2022-01-11 23:24:18,470 iteration 2515 : loss : 0.071207, loss_ce: 0.024924
2022-01-11 23:24:20,000 iteration 2516 : loss : 0.069013, loss_ce: 0.031755
 37%|█████████▉                 | 148/400 [1:11:50<1:57:51, 28.06s/it]2022-01-11 23:24:21,581 iteration 2517 : loss : 0.069717, loss_ce: 0.029743
2022-01-11 23:24:23,238 iteration 2518 : loss : 0.073629, loss_ce: 0.036105
2022-01-11 23:24:24,799 iteration 2519 : loss : 0.055830, loss_ce: 0.023427
2022-01-11 23:24:26,344 iteration 2520 : loss : 0.046360, loss_ce: 0.017629
2022-01-11 23:24:27,930 iteration 2521 : loss : 0.052267, loss_ce: 0.024801
2022-01-11 23:24:29,443 iteration 2522 : loss : 0.036987, loss_ce: 0.015815
2022-01-11 23:24:30,908 iteration 2523 : loss : 0.077509, loss_ce: 0.026660
2022-01-11 23:24:32,511 iteration 2524 : loss : 0.043312, loss_ce: 0.022022
2022-01-11 23:24:34,035 iteration 2525 : loss : 0.064219, loss_ce: 0.027562
2022-01-11 23:24:35,569 iteration 2526 : loss : 0.061723, loss_ce: 0.017089
2022-01-11 23:24:37,170 iteration 2527 : loss : 0.046733, loss_ce: 0.017189
2022-01-11 23:24:38,708 iteration 2528 : loss : 0.064116, loss_ce: 0.024674
2022-01-11 23:24:40,294 iteration 2529 : loss : 0.041567, loss_ce: 0.019136
2022-01-11 23:24:41,852 iteration 2530 : loss : 0.062444, loss_ce: 0.021548
2022-01-11 23:24:43,406 iteration 2531 : loss : 0.066930, loss_ce: 0.024306
2022-01-11 23:24:44,997 iteration 2532 : loss : 0.061165, loss_ce: 0.022348
2022-01-11 23:24:46,571 iteration 2533 : loss : 0.072814, loss_ce: 0.043404
 37%|██████████                 | 149/400 [1:12:17<1:55:31, 27.62s/it]2022-01-11 23:24:48,122 iteration 2534 : loss : 0.050322, loss_ce: 0.021454
2022-01-11 23:24:49,715 iteration 2535 : loss : 0.038264, loss_ce: 0.013090
2022-01-11 23:24:51,318 iteration 2536 : loss : 0.056479, loss_ce: 0.028840
2022-01-11 23:24:52,929 iteration 2537 : loss : 0.079491, loss_ce: 0.025621
2022-01-11 23:24:54,545 iteration 2538 : loss : 0.059071, loss_ce: 0.024985
2022-01-11 23:24:56,058 iteration 2539 : loss : 0.038504, loss_ce: 0.015638
2022-01-11 23:24:57,607 iteration 2540 : loss : 0.054967, loss_ce: 0.020857
2022-01-11 23:24:59,276 iteration 2541 : loss : 0.110158, loss_ce: 0.037927
2022-01-11 23:25:00,781 iteration 2542 : loss : 0.052937, loss_ce: 0.022513
2022-01-11 23:25:02,346 iteration 2543 : loss : 0.046929, loss_ce: 0.018790
2022-01-11 23:25:03,981 iteration 2544 : loss : 0.080055, loss_ce: 0.029851
2022-01-11 23:25:05,589 iteration 2545 : loss : 0.048886, loss_ce: 0.024187
2022-01-11 23:25:07,108 iteration 2546 : loss : 0.054251, loss_ce: 0.021897
2022-01-11 23:25:08,670 iteration 2547 : loss : 0.093278, loss_ce: 0.024131
2022-01-11 23:25:10,223 iteration 2548 : loss : 0.055711, loss_ce: 0.023078
2022-01-11 23:25:11,846 iteration 2549 : loss : 0.071047, loss_ce: 0.022019
2022-01-11 23:25:11,846 Training Data Eval:
2022-01-11 23:25:19,869   Average segmentation loss on training set: 0.0389
2022-01-11 23:25:19,870 Validation Data Eval:
2022-01-11 23:25:22,630   Average segmentation loss on validation set: 0.0758
2022-01-11 23:25:24,191 iteration 2550 : loss : 0.059370, loss_ce: 0.035441
 38%|██████████▏                | 150/400 [1:12:54<2:07:33, 30.61s/it]2022-01-11 23:25:25,771 iteration 2551 : loss : 0.048564, loss_ce: 0.018207
2022-01-11 23:25:27,343 iteration 2552 : loss : 0.046872, loss_ce: 0.020640
2022-01-11 23:25:28,973 iteration 2553 : loss : 0.062541, loss_ce: 0.024510
2022-01-11 23:25:30,577 iteration 2554 : loss : 0.099176, loss_ce: 0.033715
2022-01-11 23:25:32,127 iteration 2555 : loss : 0.060204, loss_ce: 0.018140
2022-01-11 23:25:33,642 iteration 2556 : loss : 0.047648, loss_ce: 0.021182
2022-01-11 23:25:35,253 iteration 2557 : loss : 0.075120, loss_ce: 0.034717
2022-01-11 23:25:36,868 iteration 2558 : loss : 0.053864, loss_ce: 0.023892
2022-01-11 23:25:38,489 iteration 2559 : loss : 0.092517, loss_ce: 0.021563
2022-01-11 23:25:40,103 iteration 2560 : loss : 0.036994, loss_ce: 0.014379
2022-01-11 23:25:41,663 iteration 2561 : loss : 0.051817, loss_ce: 0.020011
2022-01-11 23:25:43,189 iteration 2562 : loss : 0.041910, loss_ce: 0.021934
2022-01-11 23:25:44,741 iteration 2563 : loss : 0.063832, loss_ce: 0.020037
2022-01-11 23:25:46,302 iteration 2564 : loss : 0.058976, loss_ce: 0.031822
2022-01-11 23:25:47,864 iteration 2565 : loss : 0.067235, loss_ce: 0.020657
2022-01-11 23:25:49,407 iteration 2566 : loss : 0.042171, loss_ce: 0.019292
2022-01-11 23:25:51,052 iteration 2567 : loss : 0.059033, loss_ce: 0.023490
 38%|██████████▏                | 151/400 [1:13:21<2:02:23, 29.49s/it]2022-01-11 23:25:52,628 iteration 2568 : loss : 0.040613, loss_ce: 0.019305
2022-01-11 23:25:54,190 iteration 2569 : loss : 0.063816, loss_ce: 0.024783
2022-01-11 23:25:55,729 iteration 2570 : loss : 0.072744, loss_ce: 0.022873
2022-01-11 23:25:57,303 iteration 2571 : loss : 0.084081, loss_ce: 0.035456
2022-01-11 23:25:58,816 iteration 2572 : loss : 0.062629, loss_ce: 0.019496
2022-01-11 23:26:00,344 iteration 2573 : loss : 0.043890, loss_ce: 0.018092
2022-01-11 23:26:01,893 iteration 2574 : loss : 0.083428, loss_ce: 0.032159
2022-01-11 23:26:03,485 iteration 2575 : loss : 0.057872, loss_ce: 0.019910
2022-01-11 23:26:05,143 iteration 2576 : loss : 0.040666, loss_ce: 0.018759
2022-01-11 23:26:06,693 iteration 2577 : loss : 0.046527, loss_ce: 0.017571
2022-01-11 23:26:08,270 iteration 2578 : loss : 0.056144, loss_ce: 0.030147
2022-01-11 23:26:09,812 iteration 2579 : loss : 0.053358, loss_ce: 0.018409
2022-01-11 23:26:11,422 iteration 2580 : loss : 0.040242, loss_ce: 0.012625
2022-01-11 23:26:13,092 iteration 2581 : loss : 0.041633, loss_ce: 0.014516
2022-01-11 23:26:14,562 iteration 2582 : loss : 0.050812, loss_ce: 0.022158
2022-01-11 23:26:16,167 iteration 2583 : loss : 0.057999, loss_ce: 0.024092
2022-01-11 23:26:17,701 iteration 2584 : loss : 0.030582, loss_ce: 0.012678
 38%|██████████▎                | 152/400 [1:13:48<1:58:21, 28.64s/it]2022-01-11 23:26:19,356 iteration 2585 : loss : 0.078132, loss_ce: 0.029632
2022-01-11 23:26:20,929 iteration 2586 : loss : 0.069424, loss_ce: 0.031804
2022-01-11 23:26:22,441 iteration 2587 : loss : 0.039456, loss_ce: 0.016587
2022-01-11 23:26:23,983 iteration 2588 : loss : 0.045175, loss_ce: 0.019896
2022-01-11 23:26:25,477 iteration 2589 : loss : 0.037071, loss_ce: 0.014279
2022-01-11 23:26:27,077 iteration 2590 : loss : 0.051412, loss_ce: 0.018138
2022-01-11 23:26:28,637 iteration 2591 : loss : 0.052044, loss_ce: 0.024167
2022-01-11 23:26:30,178 iteration 2592 : loss : 0.051787, loss_ce: 0.020049
2022-01-11 23:26:31,711 iteration 2593 : loss : 0.036606, loss_ce: 0.013715
2022-01-11 23:26:33,313 iteration 2594 : loss : 0.044467, loss_ce: 0.017626
2022-01-11 23:26:34,976 iteration 2595 : loss : 0.076545, loss_ce: 0.024069
2022-01-11 23:26:36,474 iteration 2596 : loss : 0.029994, loss_ce: 0.010500
2022-01-11 23:26:38,058 iteration 2597 : loss : 0.082291, loss_ce: 0.025263
2022-01-11 23:26:39,589 iteration 2598 : loss : 0.040179, loss_ce: 0.018177
2022-01-11 23:26:41,182 iteration 2599 : loss : 0.034714, loss_ce: 0.015929
2022-01-11 23:26:42,742 iteration 2600 : loss : 0.055564, loss_ce: 0.018519
2022-01-11 23:26:44,323 iteration 2601 : loss : 0.034670, loss_ce: 0.014089
 38%|██████████▎                | 153/400 [1:14:15<1:55:24, 28.03s/it]2022-01-11 23:26:45,955 iteration 2602 : loss : 0.040673, loss_ce: 0.013693
2022-01-11 23:26:47,600 iteration 2603 : loss : 0.074241, loss_ce: 0.025705
2022-01-11 23:26:49,163 iteration 2604 : loss : 0.047889, loss_ce: 0.021401
2022-01-11 23:26:50,627 iteration 2605 : loss : 0.033871, loss_ce: 0.010373
2022-01-11 23:26:52,319 iteration 2606 : loss : 0.075668, loss_ce: 0.027033
2022-01-11 23:26:53,919 iteration 2607 : loss : 0.038927, loss_ce: 0.012512
2022-01-11 23:26:55,498 iteration 2608 : loss : 0.048197, loss_ce: 0.022069
2022-01-11 23:26:57,155 iteration 2609 : loss : 0.077912, loss_ce: 0.024954
2022-01-11 23:26:58,762 iteration 2610 : loss : 0.054809, loss_ce: 0.021848
2022-01-11 23:27:00,345 iteration 2611 : loss : 0.045924, loss_ce: 0.018862
2022-01-11 23:27:01,949 iteration 2612 : loss : 0.046997, loss_ce: 0.021238
2022-01-11 23:27:03,469 iteration 2613 : loss : 0.046445, loss_ce: 0.020950
2022-01-11 23:27:05,110 iteration 2614 : loss : 0.052819, loss_ce: 0.018752
2022-01-11 23:27:06,636 iteration 2615 : loss : 0.054441, loss_ce: 0.020510
2022-01-11 23:27:08,213 iteration 2616 : loss : 0.054199, loss_ce: 0.030819
2022-01-11 23:27:09,765 iteration 2617 : loss : 0.033560, loss_ce: 0.011968
2022-01-11 23:27:11,283 iteration 2618 : loss : 0.035114, loss_ce: 0.015487
 38%|██████████▍                | 154/400 [1:14:42<1:53:37, 27.71s/it]2022-01-11 23:27:12,855 iteration 2619 : loss : 0.057744, loss_ce: 0.018807
2022-01-11 23:27:14,384 iteration 2620 : loss : 0.032905, loss_ce: 0.014594
2022-01-11 23:27:15,997 iteration 2621 : loss : 0.044060, loss_ce: 0.015847
2022-01-11 23:27:17,545 iteration 2622 : loss : 0.067085, loss_ce: 0.016963
2022-01-11 23:27:19,130 iteration 2623 : loss : 0.057325, loss_ce: 0.027584
2022-01-11 23:27:20,650 iteration 2624 : loss : 0.029741, loss_ce: 0.010162
2022-01-11 23:27:22,240 iteration 2625 : loss : 0.124852, loss_ce: 0.026100
2022-01-11 23:27:23,819 iteration 2626 : loss : 0.061252, loss_ce: 0.025309
2022-01-11 23:27:25,506 iteration 2627 : loss : 0.047638, loss_ce: 0.017600
2022-01-11 23:27:27,007 iteration 2628 : loss : 0.044105, loss_ce: 0.018794
2022-01-11 23:27:28,626 iteration 2629 : loss : 0.043041, loss_ce: 0.017421
2022-01-11 23:27:30,214 iteration 2630 : loss : 0.039154, loss_ce: 0.017501
2022-01-11 23:27:31,773 iteration 2631 : loss : 0.035344, loss_ce: 0.014580
2022-01-11 23:27:33,338 iteration 2632 : loss : 0.036851, loss_ce: 0.015415
2022-01-11 23:27:34,927 iteration 2633 : loss : 0.039649, loss_ce: 0.020067
2022-01-11 23:27:36,504 iteration 2634 : loss : 0.051592, loss_ce: 0.018499
2022-01-11 23:27:36,504 Training Data Eval:
2022-01-11 23:27:44,521   Average segmentation loss on training set: 0.0291
2022-01-11 23:27:44,521 Validation Data Eval:
2022-01-11 23:27:47,284   Average segmentation loss on validation set: 0.0842
2022-01-11 23:27:48,858 iteration 2635 : loss : 0.031761, loss_ce: 0.012626
 39%|██████████▍                | 155/400 [1:15:19<2:05:14, 30.67s/it]2022-01-11 23:27:50,461 iteration 2636 : loss : 0.063150, loss_ce: 0.030689
2022-01-11 23:27:52,094 iteration 2637 : loss : 0.039297, loss_ce: 0.014876
2022-01-11 23:27:53,639 iteration 2638 : loss : 0.059691, loss_ce: 0.023270
2022-01-11 23:27:55,163 iteration 2639 : loss : 0.038971, loss_ce: 0.018799
2022-01-11 23:27:56,707 iteration 2640 : loss : 0.030628, loss_ce: 0.010664
2022-01-11 23:27:58,259 iteration 2641 : loss : 0.047540, loss_ce: 0.021130
2022-01-11 23:27:59,785 iteration 2642 : loss : 0.039273, loss_ce: 0.014385
2022-01-11 23:28:01,451 iteration 2643 : loss : 0.044426, loss_ce: 0.015683
2022-01-11 23:28:02,992 iteration 2644 : loss : 0.033686, loss_ce: 0.012726
2022-01-11 23:28:04,515 iteration 2645 : loss : 0.029412, loss_ce: 0.010412
2022-01-11 23:28:06,239 iteration 2646 : loss : 0.043990, loss_ce: 0.015742
2022-01-11 23:28:07,841 iteration 2647 : loss : 0.055929, loss_ce: 0.021715
2022-01-11 23:28:09,382 iteration 2648 : loss : 0.041730, loss_ce: 0.016676
2022-01-11 23:28:10,946 iteration 2649 : loss : 0.049845, loss_ce: 0.021014
2022-01-11 23:28:12,536 iteration 2650 : loss : 0.033713, loss_ce: 0.011221
2022-01-11 23:28:14,110 iteration 2651 : loss : 0.052350, loss_ce: 0.020752
2022-01-11 23:28:15,625 iteration 2652 : loss : 0.029017, loss_ce: 0.011543
 39%|██████████▌                | 156/400 [1:15:46<1:59:57, 29.50s/it]2022-01-11 23:28:17,275 iteration 2653 : loss : 0.029665, loss_ce: 0.013018
2022-01-11 23:28:18,852 iteration 2654 : loss : 0.034709, loss_ce: 0.011887
2022-01-11 23:28:20,498 iteration 2655 : loss : 0.047164, loss_ce: 0.024980
2022-01-11 23:28:22,045 iteration 2656 : loss : 0.060572, loss_ce: 0.019602
2022-01-11 23:28:23,593 iteration 2657 : loss : 0.032193, loss_ce: 0.011945
2022-01-11 23:28:25,057 iteration 2658 : loss : 0.036838, loss_ce: 0.017916
2022-01-11 23:28:26,569 iteration 2659 : loss : 0.063802, loss_ce: 0.019148
2022-01-11 23:28:28,166 iteration 2660 : loss : 0.043747, loss_ce: 0.016508
2022-01-11 23:28:29,698 iteration 2661 : loss : 0.040332, loss_ce: 0.019261
2022-01-11 23:28:31,252 iteration 2662 : loss : 0.045590, loss_ce: 0.022530
2022-01-11 23:28:32,796 iteration 2663 : loss : 0.046360, loss_ce: 0.012702
2022-01-11 23:28:34,447 iteration 2664 : loss : 0.052857, loss_ce: 0.022452
2022-01-11 23:28:36,014 iteration 2665 : loss : 0.045349, loss_ce: 0.017208
2022-01-11 23:28:37,534 iteration 2666 : loss : 0.026284, loss_ce: 0.009992
2022-01-11 23:28:39,155 iteration 2667 : loss : 0.045036, loss_ce: 0.016611
2022-01-11 23:28:40,688 iteration 2668 : loss : 0.037223, loss_ce: 0.013477
2022-01-11 23:28:42,211 iteration 2669 : loss : 0.036737, loss_ce: 0.012707
 39%|██████████▌                | 157/400 [1:16:13<1:55:55, 28.62s/it]2022-01-11 23:28:43,849 iteration 2670 : loss : 0.044779, loss_ce: 0.020383
2022-01-11 23:28:45,525 iteration 2671 : loss : 0.064146, loss_ce: 0.020859
2022-01-11 23:28:47,109 iteration 2672 : loss : 0.038559, loss_ce: 0.016950
2022-01-11 23:28:48,619 iteration 2673 : loss : 0.032300, loss_ce: 0.015989
2022-01-11 23:28:50,214 iteration 2674 : loss : 0.046788, loss_ce: 0.020044
2022-01-11 23:28:51,686 iteration 2675 : loss : 0.033428, loss_ce: 0.013438
2022-01-11 23:28:53,356 iteration 2676 : loss : 0.069013, loss_ce: 0.020651
2022-01-11 23:28:54,955 iteration 2677 : loss : 0.044281, loss_ce: 0.020405
2022-01-11 23:28:56,518 iteration 2678 : loss : 0.033010, loss_ce: 0.013440
2022-01-11 23:28:58,099 iteration 2679 : loss : 0.046845, loss_ce: 0.017437
2022-01-11 23:28:59,660 iteration 2680 : loss : 0.030575, loss_ce: 0.012557
2022-01-11 23:29:01,251 iteration 2681 : loss : 0.034748, loss_ce: 0.011384
2022-01-11 23:29:02,839 iteration 2682 : loss : 0.042048, loss_ce: 0.015258
2022-01-11 23:29:04,379 iteration 2683 : loss : 0.055321, loss_ce: 0.012861
2022-01-11 23:29:05,880 iteration 2684 : loss : 0.057119, loss_ce: 0.034174
2022-01-11 23:29:07,457 iteration 2685 : loss : 0.068741, loss_ce: 0.032159
2022-01-11 23:29:08,966 iteration 2686 : loss : 0.055642, loss_ce: 0.023742
 40%|██████████▋                | 158/400 [1:16:39<1:53:11, 28.06s/it]2022-01-11 23:29:10,487 iteration 2687 : loss : 0.043720, loss_ce: 0.013436
2022-01-11 23:29:12,065 iteration 2688 : loss : 0.034726, loss_ce: 0.013582
2022-01-11 23:29:13,607 iteration 2689 : loss : 0.048863, loss_ce: 0.018638
2022-01-11 23:29:15,270 iteration 2690 : loss : 0.051427, loss_ce: 0.023968
2022-01-11 23:29:16,779 iteration 2691 : loss : 0.027308, loss_ce: 0.011417
2022-01-11 23:29:18,302 iteration 2692 : loss : 0.034681, loss_ce: 0.012493
2022-01-11 23:29:19,873 iteration 2693 : loss : 0.034781, loss_ce: 0.011602
2022-01-11 23:29:21,504 iteration 2694 : loss : 0.042684, loss_ce: 0.014917
2022-01-11 23:29:23,036 iteration 2695 : loss : 0.030853, loss_ce: 0.013210
2022-01-11 23:29:24,613 iteration 2696 : loss : 0.043328, loss_ce: 0.019701
2022-01-11 23:29:26,117 iteration 2697 : loss : 0.032085, loss_ce: 0.015163
2022-01-11 23:29:27,562 iteration 2698 : loss : 0.025543, loss_ce: 0.011723
2022-01-11 23:29:29,137 iteration 2699 : loss : 0.035522, loss_ce: 0.013350
2022-01-11 23:29:30,753 iteration 2700 : loss : 0.035083, loss_ce: 0.016946
2022-01-11 23:29:32,301 iteration 2701 : loss : 0.071827, loss_ce: 0.024422
2022-01-11 23:29:33,871 iteration 2702 : loss : 0.088329, loss_ce: 0.017981
2022-01-11 23:29:35,421 iteration 2703 : loss : 0.038522, loss_ce: 0.014336
 40%|██████████▋                | 159/400 [1:17:06<1:50:47, 27.58s/it]2022-01-11 23:29:37,110 iteration 2704 : loss : 0.044694, loss_ce: 0.018723
2022-01-11 23:29:38,624 iteration 2705 : loss : 0.051488, loss_ce: 0.013242
2022-01-11 23:29:40,148 iteration 2706 : loss : 0.050480, loss_ce: 0.025178
2022-01-11 23:29:41,786 iteration 2707 : loss : 0.062631, loss_ce: 0.017957
2022-01-11 23:29:43,275 iteration 2708 : loss : 0.034454, loss_ce: 0.013024
2022-01-11 23:29:44,829 iteration 2709 : loss : 0.040464, loss_ce: 0.013066
2022-01-11 23:29:46,338 iteration 2710 : loss : 0.033918, loss_ce: 0.013656
2022-01-11 23:29:47,989 iteration 2711 : loss : 0.061355, loss_ce: 0.022921
2022-01-11 23:29:49,485 iteration 2712 : loss : 0.029349, loss_ce: 0.012488
2022-01-11 23:29:51,137 iteration 2713 : loss : 0.067815, loss_ce: 0.024257
2022-01-11 23:29:52,643 iteration 2714 : loss : 0.043716, loss_ce: 0.015377
2022-01-11 23:29:54,216 iteration 2715 : loss : 0.054064, loss_ce: 0.029487
2022-01-11 23:29:55,749 iteration 2716 : loss : 0.046268, loss_ce: 0.015523
2022-01-11 23:29:57,244 iteration 2717 : loss : 0.033457, loss_ce: 0.014921
2022-01-11 23:29:58,756 iteration 2718 : loss : 0.052310, loss_ce: 0.019683
2022-01-11 23:30:00,356 iteration 2719 : loss : 0.069580, loss_ce: 0.028287
2022-01-11 23:30:00,356 Training Data Eval:
2022-01-11 23:30:08,358   Average segmentation loss on training set: 0.0279
2022-01-11 23:30:08,359 Validation Data Eval:
2022-01-11 23:30:11,111   Average segmentation loss on validation set: 0.0693
2022-01-11 23:30:12,625 iteration 2720 : loss : 0.032515, loss_ce: 0.015395
 40%|██████████▊                | 160/400 [1:17:43<2:01:52, 30.47s/it]2022-01-11 23:30:14,255 iteration 2721 : loss : 0.040434, loss_ce: 0.017121
2022-01-11 23:30:15,842 iteration 2722 : loss : 0.040411, loss_ce: 0.018498
2022-01-11 23:30:17,454 iteration 2723 : loss : 0.037535, loss_ce: 0.015940
2022-01-11 23:30:18,970 iteration 2724 : loss : 0.053659, loss_ce: 0.018232
2022-01-11 23:30:20,448 iteration 2725 : loss : 0.039459, loss_ce: 0.015011
2022-01-11 23:30:21,988 iteration 2726 : loss : 0.037166, loss_ce: 0.012825
2022-01-11 23:30:23,478 iteration 2727 : loss : 0.028178, loss_ce: 0.009518
2022-01-11 23:30:25,137 iteration 2728 : loss : 0.063220, loss_ce: 0.030289
2022-01-11 23:30:26,695 iteration 2729 : loss : 0.052696, loss_ce: 0.023769
2022-01-11 23:30:28,300 iteration 2730 : loss : 0.051646, loss_ce: 0.022980
2022-01-11 23:30:29,888 iteration 2731 : loss : 0.046247, loss_ce: 0.018372
2022-01-11 23:30:31,548 iteration 2732 : loss : 0.043292, loss_ce: 0.017790
2022-01-11 23:30:33,145 iteration 2733 : loss : 0.053522, loss_ce: 0.022302
2022-01-11 23:30:34,720 iteration 2734 : loss : 0.031443, loss_ce: 0.010455
2022-01-11 23:30:36,200 iteration 2735 : loss : 0.028109, loss_ce: 0.011524
2022-01-11 23:30:37,808 iteration 2736 : loss : 0.042148, loss_ce: 0.020309
2022-01-11 23:30:39,354 iteration 2737 : loss : 0.039379, loss_ce: 0.014035
 40%|██████████▊                | 161/400 [1:18:10<1:56:53, 29.34s/it]2022-01-11 23:30:40,949 iteration 2738 : loss : 0.033810, loss_ce: 0.016230
2022-01-11 23:30:42,485 iteration 2739 : loss : 0.038735, loss_ce: 0.019699
2022-01-11 23:30:44,057 iteration 2740 : loss : 0.039457, loss_ce: 0.017292
2022-01-11 23:30:45,637 iteration 2741 : loss : 0.057984, loss_ce: 0.023096
2022-01-11 23:30:47,125 iteration 2742 : loss : 0.030490, loss_ce: 0.012544
2022-01-11 23:30:48,696 iteration 2743 : loss : 0.046637, loss_ce: 0.025299
2022-01-11 23:30:50,287 iteration 2744 : loss : 0.037408, loss_ce: 0.014638
2022-01-11 23:30:51,862 iteration 2745 : loss : 0.033252, loss_ce: 0.012652
2022-01-11 23:30:53,375 iteration 2746 : loss : 0.068875, loss_ce: 0.018450
2022-01-11 23:30:54,934 iteration 2747 : loss : 0.027070, loss_ce: 0.008881
2022-01-11 23:30:56,535 iteration 2748 : loss : 0.053607, loss_ce: 0.020539
2022-01-11 23:30:58,029 iteration 2749 : loss : 0.033188, loss_ce: 0.014465
2022-01-11 23:30:59,612 iteration 2750 : loss : 0.036226, loss_ce: 0.011360
2022-01-11 23:31:01,136 iteration 2751 : loss : 0.032346, loss_ce: 0.013386
2022-01-11 23:31:02,607 iteration 2752 : loss : 0.035725, loss_ce: 0.008697
2022-01-11 23:31:04,242 iteration 2753 : loss : 0.056781, loss_ce: 0.025561
2022-01-11 23:31:05,835 iteration 2754 : loss : 0.037401, loss_ce: 0.016099
 40%|██████████▉                | 162/400 [1:18:36<1:53:00, 28.49s/it]2022-01-11 23:31:07,422 iteration 2755 : loss : 0.039980, loss_ce: 0.013702
2022-01-11 23:31:08,990 iteration 2756 : loss : 0.045442, loss_ce: 0.023866
2022-01-11 23:31:10,544 iteration 2757 : loss : 0.045239, loss_ce: 0.017645
2022-01-11 23:31:12,146 iteration 2758 : loss : 0.057165, loss_ce: 0.022580
2022-01-11 23:31:13,716 iteration 2759 : loss : 0.044002, loss_ce: 0.015318
2022-01-11 23:31:15,281 iteration 2760 : loss : 0.037051, loss_ce: 0.015949
2022-01-11 23:31:16,774 iteration 2761 : loss : 0.049837, loss_ce: 0.017168
2022-01-11 23:31:18,287 iteration 2762 : loss : 0.031823, loss_ce: 0.012180
2022-01-11 23:31:19,902 iteration 2763 : loss : 0.070341, loss_ce: 0.024125
2022-01-11 23:31:21,462 iteration 2764 : loss : 0.032440, loss_ce: 0.013548
2022-01-11 23:31:23,026 iteration 2765 : loss : 0.049470, loss_ce: 0.021295
2022-01-11 23:31:24,560 iteration 2766 : loss : 0.041210, loss_ce: 0.021144
2022-01-11 23:31:26,148 iteration 2767 : loss : 0.031479, loss_ce: 0.011526
2022-01-11 23:31:27,683 iteration 2768 : loss : 0.043808, loss_ce: 0.013827
2022-01-11 23:31:29,249 iteration 2769 : loss : 0.034497, loss_ce: 0.013922
2022-01-11 23:31:30,806 iteration 2770 : loss : 0.038265, loss_ce: 0.017925
2022-01-11 23:31:32,358 iteration 2771 : loss : 0.042355, loss_ce: 0.014405
 41%|███████████                | 163/400 [1:19:03<1:50:11, 27.90s/it]2022-01-11 23:31:34,000 iteration 2772 : loss : 0.040187, loss_ce: 0.018787
2022-01-11 23:31:35,499 iteration 2773 : loss : 0.029943, loss_ce: 0.013010
2022-01-11 23:31:37,004 iteration 2774 : loss : 0.052001, loss_ce: 0.014406
2022-01-11 23:31:38,550 iteration 2775 : loss : 0.035625, loss_ce: 0.014322
2022-01-11 23:31:40,086 iteration 2776 : loss : 0.033243, loss_ce: 0.013262
2022-01-11 23:31:41,645 iteration 2777 : loss : 0.026312, loss_ce: 0.009582
2022-01-11 23:31:43,283 iteration 2778 : loss : 0.051780, loss_ce: 0.020663
2022-01-11 23:31:44,938 iteration 2779 : loss : 0.063374, loss_ce: 0.027926
2022-01-11 23:31:46,447 iteration 2780 : loss : 0.030076, loss_ce: 0.009685
2022-01-11 23:31:48,049 iteration 2781 : loss : 0.052011, loss_ce: 0.019804
2022-01-11 23:31:49,608 iteration 2782 : loss : 0.029026, loss_ce: 0.010656
2022-01-11 23:31:51,130 iteration 2783 : loss : 0.048860, loss_ce: 0.026275
2022-01-11 23:31:52,781 iteration 2784 : loss : 0.051614, loss_ce: 0.018633
2022-01-11 23:31:54,393 iteration 2785 : loss : 0.046086, loss_ce: 0.013814
2022-01-11 23:31:55,981 iteration 2786 : loss : 0.032943, loss_ce: 0.014502
2022-01-11 23:31:57,582 iteration 2787 : loss : 0.028519, loss_ce: 0.013277
2022-01-11 23:31:59,103 iteration 2788 : loss : 0.030950, loss_ce: 0.017381
 41%|███████████                | 164/400 [1:19:29<1:48:22, 27.55s/it]2022-01-11 23:32:00,662 iteration 2789 : loss : 0.043058, loss_ce: 0.017103
2022-01-11 23:32:02,252 iteration 2790 : loss : 0.031252, loss_ce: 0.011425
2022-01-11 23:32:03,775 iteration 2791 : loss : 0.043051, loss_ce: 0.017529
2022-01-11 23:32:05,398 iteration 2792 : loss : 0.040916, loss_ce: 0.017335
2022-01-11 23:32:06,919 iteration 2793 : loss : 0.029991, loss_ce: 0.012251
2022-01-11 23:32:08,499 iteration 2794 : loss : 0.031444, loss_ce: 0.011106
2022-01-11 23:32:10,011 iteration 2795 : loss : 0.031788, loss_ce: 0.011177
2022-01-11 23:32:11,597 iteration 2796 : loss : 0.033575, loss_ce: 0.012079
2022-01-11 23:32:13,192 iteration 2797 : loss : 0.031949, loss_ce: 0.013087
2022-01-11 23:32:14,808 iteration 2798 : loss : 0.061394, loss_ce: 0.032010
2022-01-11 23:32:16,325 iteration 2799 : loss : 0.025328, loss_ce: 0.011020
2022-01-11 23:32:17,836 iteration 2800 : loss : 0.063413, loss_ce: 0.023998
2022-01-11 23:32:19,341 iteration 2801 : loss : 0.031575, loss_ce: 0.009563
2022-01-11 23:32:20,950 iteration 2802 : loss : 0.049025, loss_ce: 0.015106
2022-01-11 23:32:22,499 iteration 2803 : loss : 0.029406, loss_ce: 0.012239
2022-01-11 23:32:24,041 iteration 2804 : loss : 0.034421, loss_ce: 0.014842
2022-01-11 23:32:24,042 Training Data Eval:
2022-01-11 23:32:32,064   Average segmentation loss on training set: 0.0291
2022-01-11 23:32:32,065 Validation Data Eval:
2022-01-11 23:32:34,823   Average segmentation loss on validation set: 0.0859
2022-01-11 23:32:36,366 iteration 2805 : loss : 0.038892, loss_ce: 0.016122
 41%|███████████▏               | 165/400 [1:20:07<1:59:19, 30.46s/it]2022-01-11 23:32:38,083 iteration 2806 : loss : 0.061861, loss_ce: 0.030898
2022-01-11 23:32:39,578 iteration 2807 : loss : 0.040546, loss_ce: 0.015976
2022-01-11 23:32:41,105 iteration 2808 : loss : 0.027911, loss_ce: 0.010349
2022-01-11 23:32:42,700 iteration 2809 : loss : 0.028939, loss_ce: 0.010176
2022-01-11 23:32:44,254 iteration 2810 : loss : 0.029334, loss_ce: 0.014368
2022-01-11 23:32:45,895 iteration 2811 : loss : 0.062907, loss_ce: 0.024178
2022-01-11 23:32:47,550 iteration 2812 : loss : 0.027508, loss_ce: 0.010362
2022-01-11 23:32:49,040 iteration 2813 : loss : 0.029098, loss_ce: 0.011618
2022-01-11 23:32:50,625 iteration 2814 : loss : 0.041852, loss_ce: 0.020861
2022-01-11 23:32:52,231 iteration 2815 : loss : 0.049201, loss_ce: 0.017350
2022-01-11 23:32:53,775 iteration 2816 : loss : 0.043962, loss_ce: 0.018862
2022-01-11 23:32:55,321 iteration 2817 : loss : 0.038589, loss_ce: 0.016003
2022-01-11 23:32:56,981 iteration 2818 : loss : 0.040876, loss_ce: 0.014275
2022-01-11 23:32:58,606 iteration 2819 : loss : 0.029205, loss_ce: 0.012448
2022-01-11 23:33:00,139 iteration 2820 : loss : 0.031834, loss_ce: 0.011752
2022-01-11 23:33:01,706 iteration 2821 : loss : 0.025536, loss_ce: 0.010824
2022-01-11 23:33:03,304 iteration 2822 : loss : 0.063633, loss_ce: 0.014009
 42%|███████████▏               | 166/400 [1:20:34<1:54:40, 29.41s/it]2022-01-11 23:33:04,930 iteration 2823 : loss : 0.033929, loss_ce: 0.012576
2022-01-11 23:33:06,526 iteration 2824 : loss : 0.050490, loss_ce: 0.014267
2022-01-11 23:33:08,107 iteration 2825 : loss : 0.041091, loss_ce: 0.015445
2022-01-11 23:33:09,608 iteration 2826 : loss : 0.046067, loss_ce: 0.017556
2022-01-11 23:33:11,210 iteration 2827 : loss : 0.046172, loss_ce: 0.023846
2022-01-11 23:33:12,761 iteration 2828 : loss : 0.057864, loss_ce: 0.032945
2022-01-11 23:33:14,310 iteration 2829 : loss : 0.035584, loss_ce: 0.010636
2022-01-11 23:33:15,903 iteration 2830 : loss : 0.033989, loss_ce: 0.015248
2022-01-11 23:33:17,459 iteration 2831 : loss : 0.053564, loss_ce: 0.016028
2022-01-11 23:33:18,975 iteration 2832 : loss : 0.027916, loss_ce: 0.011657
2022-01-11 23:33:20,621 iteration 2833 : loss : 0.031927, loss_ce: 0.013178
2022-01-11 23:33:22,185 iteration 2834 : loss : 0.033412, loss_ce: 0.012968
2022-01-11 23:33:23,776 iteration 2835 : loss : 0.043143, loss_ce: 0.017902
2022-01-11 23:33:25,375 iteration 2836 : loss : 0.044712, loss_ce: 0.017399
2022-01-11 23:33:26,983 iteration 2837 : loss : 0.047146, loss_ce: 0.018311
2022-01-11 23:33:28,583 iteration 2838 : loss : 0.042188, loss_ce: 0.016151
2022-01-11 23:33:30,089 iteration 2839 : loss : 0.031901, loss_ce: 0.014717
 42%|███████████▎               | 167/400 [1:21:00<1:51:08, 28.62s/it]2022-01-11 23:33:31,685 iteration 2840 : loss : 0.036688, loss_ce: 0.013766
2022-01-11 23:33:33,219 iteration 2841 : loss : 0.030468, loss_ce: 0.014415
2022-01-11 23:33:34,722 iteration 2842 : loss : 0.029677, loss_ce: 0.011040
2022-01-11 23:33:36,271 iteration 2843 : loss : 0.037787, loss_ce: 0.012469
2022-01-11 23:33:37,874 iteration 2844 : loss : 0.041739, loss_ce: 0.017514
2022-01-11 23:33:39,494 iteration 2845 : loss : 0.032151, loss_ce: 0.011830
2022-01-11 23:33:41,036 iteration 2846 : loss : 0.041512, loss_ce: 0.016801
2022-01-11 23:33:42,619 iteration 2847 : loss : 0.040071, loss_ce: 0.015796
2022-01-11 23:33:44,274 iteration 2848 : loss : 0.043034, loss_ce: 0.022294
2022-01-11 23:33:45,812 iteration 2849 : loss : 0.033219, loss_ce: 0.011172
2022-01-11 23:33:47,334 iteration 2850 : loss : 0.033098, loss_ce: 0.012331
2022-01-11 23:33:48,906 iteration 2851 : loss : 0.042087, loss_ce: 0.019034
2022-01-11 23:33:50,478 iteration 2852 : loss : 0.088017, loss_ce: 0.038342
2022-01-11 23:33:52,010 iteration 2853 : loss : 0.035818, loss_ce: 0.018057
2022-01-11 23:33:53,610 iteration 2854 : loss : 0.042058, loss_ce: 0.019649
2022-01-11 23:33:55,182 iteration 2855 : loss : 0.056856, loss_ce: 0.027826
2022-01-11 23:33:56,816 iteration 2856 : loss : 0.031453, loss_ce: 0.011992
 42%|███████████▎               | 168/400 [1:21:27<1:48:27, 28.05s/it]2022-01-11 23:33:58,488 iteration 2857 : loss : 0.083127, loss_ce: 0.025098
2022-01-11 23:34:00,070 iteration 2858 : loss : 0.054152, loss_ce: 0.018779
2022-01-11 23:34:01,589 iteration 2859 : loss : 0.027207, loss_ce: 0.010429
2022-01-11 23:34:03,217 iteration 2860 : loss : 0.037094, loss_ce: 0.016448
2022-01-11 23:34:04,751 iteration 2861 : loss : 0.033655, loss_ce: 0.012324
2022-01-11 23:34:06,351 iteration 2862 : loss : 0.033211, loss_ce: 0.012260
2022-01-11 23:34:07,971 iteration 2863 : loss : 0.048588, loss_ce: 0.017316
2022-01-11 23:34:09,538 iteration 2864 : loss : 0.034612, loss_ce: 0.016332
2022-01-11 23:34:11,032 iteration 2865 : loss : 0.038468, loss_ce: 0.017758
2022-01-11 23:34:12,588 iteration 2866 : loss : 0.030753, loss_ce: 0.009691
2022-01-11 23:34:14,215 iteration 2867 : loss : 0.043455, loss_ce: 0.017299
2022-01-11 23:34:15,831 iteration 2868 : loss : 0.036332, loss_ce: 0.016346
2022-01-11 23:34:17,371 iteration 2869 : loss : 0.031951, loss_ce: 0.014580
2022-01-11 23:34:18,960 iteration 2870 : loss : 0.043655, loss_ce: 0.020728
2022-01-11 23:34:20,460 iteration 2871 : loss : 0.028414, loss_ce: 0.014273
2022-01-11 23:34:22,029 iteration 2872 : loss : 0.047362, loss_ce: 0.015887
2022-01-11 23:34:23,644 iteration 2873 : loss : 0.038236, loss_ce: 0.015732
 42%|███████████▍               | 169/400 [1:21:54<1:46:35, 27.69s/it]2022-01-11 23:34:25,223 iteration 2874 : loss : 0.053554, loss_ce: 0.015061
2022-01-11 23:34:26,710 iteration 2875 : loss : 0.028196, loss_ce: 0.011511
2022-01-11 23:34:28,247 iteration 2876 : loss : 0.031018, loss_ce: 0.014854
2022-01-11 23:34:29,823 iteration 2877 : loss : 0.031247, loss_ce: 0.010615
2022-01-11 23:34:31,415 iteration 2878 : loss : 0.052542, loss_ce: 0.020210
2022-01-11 23:34:32,927 iteration 2879 : loss : 0.025547, loss_ce: 0.012528
2022-01-11 23:34:34,550 iteration 2880 : loss : 0.041592, loss_ce: 0.018039
2022-01-11 23:34:36,099 iteration 2881 : loss : 0.039496, loss_ce: 0.018316
2022-01-11 23:34:37,752 iteration 2882 : loss : 0.047861, loss_ce: 0.012848
2022-01-11 23:34:39,394 iteration 2883 : loss : 0.036218, loss_ce: 0.015394
2022-01-11 23:34:41,038 iteration 2884 : loss : 0.049105, loss_ce: 0.016438
2022-01-11 23:34:42,625 iteration 2885 : loss : 0.029478, loss_ce: 0.015604
2022-01-11 23:34:44,150 iteration 2886 : loss : 0.056445, loss_ce: 0.015080
2022-01-11 23:34:45,705 iteration 2887 : loss : 0.030038, loss_ce: 0.010748
2022-01-11 23:34:47,355 iteration 2888 : loss : 0.036232, loss_ce: 0.012637
2022-01-11 23:34:48,934 iteration 2889 : loss : 0.038847, loss_ce: 0.016995
2022-01-11 23:34:48,934 Training Data Eval:
2022-01-11 23:34:56,941   Average segmentation loss on training set: 0.0661
2022-01-11 23:34:56,942 Validation Data Eval:
2022-01-11 23:34:59,694   Average segmentation loss on validation set: 0.1624
2022-01-11 23:35:01,314 iteration 2890 : loss : 0.042069, loss_ce: 0.015435
 42%|███████████▍               | 170/400 [1:22:32<1:57:36, 30.68s/it]2022-01-11 23:35:02,987 iteration 2891 : loss : 0.048835, loss_ce: 0.025789
2022-01-11 23:35:04,513 iteration 2892 : loss : 0.032342, loss_ce: 0.017348
2022-01-11 23:35:06,052 iteration 2893 : loss : 0.040728, loss_ce: 0.013729
2022-01-11 23:35:07,657 iteration 2894 : loss : 0.028323, loss_ce: 0.011884
2022-01-11 23:35:09,211 iteration 2895 : loss : 0.035553, loss_ce: 0.012191
2022-01-11 23:35:10,734 iteration 2896 : loss : 0.034673, loss_ce: 0.015426
2022-01-11 23:35:12,292 iteration 2897 : loss : 0.060035, loss_ce: 0.022705
2022-01-11 23:35:13,856 iteration 2898 : loss : 0.031431, loss_ce: 0.017716
2022-01-11 23:35:15,462 iteration 2899 : loss : 0.052048, loss_ce: 0.019969
2022-01-11 23:35:16,980 iteration 2900 : loss : 0.037785, loss_ce: 0.013803
2022-01-11 23:35:18,633 iteration 2901 : loss : 0.045142, loss_ce: 0.016215
2022-01-11 23:35:20,105 iteration 2902 : loss : 0.036069, loss_ce: 0.011911
2022-01-11 23:35:21,698 iteration 2903 : loss : 0.030806, loss_ce: 0.013197
2022-01-11 23:35:23,318 iteration 2904 : loss : 0.077727, loss_ce: 0.026182
2022-01-11 23:35:24,880 iteration 2905 : loss : 0.043806, loss_ce: 0.017102
2022-01-11 23:35:26,508 iteration 2906 : loss : 0.049962, loss_ce: 0.020043
2022-01-11 23:35:28,039 iteration 2907 : loss : 0.034281, loss_ce: 0.018732
 43%|███████████▌               | 171/400 [1:22:58<1:52:34, 29.49s/it]2022-01-11 23:35:29,639 iteration 2908 : loss : 0.037444, loss_ce: 0.017760
2022-01-11 23:35:31,135 iteration 2909 : loss : 0.048208, loss_ce: 0.012242
2022-01-11 23:35:32,734 iteration 2910 : loss : 0.043524, loss_ce: 0.015977
2022-01-11 23:35:34,303 iteration 2911 : loss : 0.039533, loss_ce: 0.016234
2022-01-11 23:35:35,868 iteration 2912 : loss : 0.036919, loss_ce: 0.014255
2022-01-11 23:35:37,474 iteration 2913 : loss : 0.034061, loss_ce: 0.013572
2022-01-11 23:35:39,048 iteration 2914 : loss : 0.039874, loss_ce: 0.017109
2022-01-11 23:35:40,660 iteration 2915 : loss : 0.032850, loss_ce: 0.011226
2022-01-11 23:35:42,175 iteration 2916 : loss : 0.033152, loss_ce: 0.016374
2022-01-11 23:35:43,795 iteration 2917 : loss : 0.050739, loss_ce: 0.022620
2022-01-11 23:35:45,384 iteration 2918 : loss : 0.095469, loss_ce: 0.039198
2022-01-11 23:35:46,975 iteration 2919 : loss : 0.084393, loss_ce: 0.044536
2022-01-11 23:35:48,579 iteration 2920 : loss : 0.078267, loss_ce: 0.022723
2022-01-11 23:35:50,213 iteration 2921 : loss : 0.098712, loss_ce: 0.029272
2022-01-11 23:35:51,772 iteration 2922 : loss : 0.062113, loss_ce: 0.027447
2022-01-11 23:35:53,385 iteration 2923 : loss : 0.082412, loss_ce: 0.036012
2022-01-11 23:35:54,945 iteration 2924 : loss : 0.079767, loss_ce: 0.024151
 43%|███████████▌               | 172/400 [1:23:25<1:49:08, 28.72s/it]2022-01-11 23:35:56,664 iteration 2925 : loss : 0.070704, loss_ce: 0.031287
2022-01-11 23:35:58,221 iteration 2926 : loss : 0.051358, loss_ce: 0.016541
2022-01-11 23:35:59,861 iteration 2927 : loss : 0.073414, loss_ce: 0.036105
2022-01-11 23:36:01,546 iteration 2928 : loss : 0.092975, loss_ce: 0.032272
2022-01-11 23:36:03,101 iteration 2929 : loss : 0.054415, loss_ce: 0.023169
2022-01-11 23:36:04,639 iteration 2930 : loss : 0.067203, loss_ce: 0.029904
2022-01-11 23:36:06,131 iteration 2931 : loss : 0.071355, loss_ce: 0.030036
2022-01-11 23:36:07,765 iteration 2932 : loss : 0.046623, loss_ce: 0.020221
2022-01-11 23:36:09,308 iteration 2933 : loss : 0.067973, loss_ce: 0.027567
2022-01-11 23:36:10,934 iteration 2934 : loss : 0.052689, loss_ce: 0.023909
2022-01-11 23:36:12,523 iteration 2935 : loss : 0.079330, loss_ce: 0.028722
2022-01-11 23:36:14,158 iteration 2936 : loss : 0.088855, loss_ce: 0.046653
2022-01-11 23:36:15,708 iteration 2937 : loss : 0.066080, loss_ce: 0.035760
2022-01-11 23:36:17,278 iteration 2938 : loss : 0.058470, loss_ce: 0.025125
2022-01-11 23:36:18,842 iteration 2939 : loss : 0.054079, loss_ce: 0.024131
2022-01-11 23:36:20,442 iteration 2940 : loss : 0.053433, loss_ce: 0.017520
2022-01-11 23:36:22,014 iteration 2941 : loss : 0.058896, loss_ce: 0.018555
 43%|███████████▋               | 173/400 [1:23:52<1:46:46, 28.22s/it]2022-01-11 23:36:23,640 iteration 2942 : loss : 0.062975, loss_ce: 0.025763
2022-01-11 23:36:25,130 iteration 2943 : loss : 0.044608, loss_ce: 0.017905
2022-01-11 23:36:26,641 iteration 2944 : loss : 0.052679, loss_ce: 0.018924
2022-01-11 23:36:28,263 iteration 2945 : loss : 0.064789, loss_ce: 0.029060
2022-01-11 23:36:29,799 iteration 2946 : loss : 0.051458, loss_ce: 0.027156
2022-01-11 23:36:31,326 iteration 2947 : loss : 0.051195, loss_ce: 0.018983
2022-01-11 23:36:32,886 iteration 2948 : loss : 0.059109, loss_ce: 0.019047
2022-01-11 23:36:34,490 iteration 2949 : loss : 0.051077, loss_ce: 0.020195
2022-01-11 23:36:36,053 iteration 2950 : loss : 0.076388, loss_ce: 0.037443
2022-01-11 23:36:37,573 iteration 2951 : loss : 0.059038, loss_ce: 0.023166
2022-01-11 23:36:39,147 iteration 2952 : loss : 0.063987, loss_ce: 0.020200
2022-01-11 23:36:40,706 iteration 2953 : loss : 0.055207, loss_ce: 0.021860
2022-01-11 23:36:42,306 iteration 2954 : loss : 0.083542, loss_ce: 0.034600
2022-01-11 23:36:43,839 iteration 2955 : loss : 0.047858, loss_ce: 0.022659
2022-01-11 23:36:45,394 iteration 2956 : loss : 0.039650, loss_ce: 0.017674
2022-01-11 23:36:46,962 iteration 2957 : loss : 0.073168, loss_ce: 0.023025
2022-01-11 23:36:48,531 iteration 2958 : loss : 0.061825, loss_ce: 0.022946
 44%|███████████▋               | 174/400 [1:24:19<1:44:22, 27.71s/it]2022-01-11 23:36:50,212 iteration 2959 : loss : 0.038657, loss_ce: 0.016952
2022-01-11 23:36:51,804 iteration 2960 : loss : 0.055699, loss_ce: 0.020311
2022-01-11 23:36:53,423 iteration 2961 : loss : 0.045504, loss_ce: 0.022222
2022-01-11 23:36:54,953 iteration 2962 : loss : 0.043800, loss_ce: 0.014110
2022-01-11 23:36:56,514 iteration 2963 : loss : 0.046883, loss_ce: 0.017841
2022-01-11 23:36:58,135 iteration 2964 : loss : 0.090308, loss_ce: 0.033799
2022-01-11 23:36:59,711 iteration 2965 : loss : 0.036180, loss_ce: 0.015841
2022-01-11 23:37:01,268 iteration 2966 : loss : 0.056967, loss_ce: 0.021090
2022-01-11 23:37:02,905 iteration 2967 : loss : 0.099492, loss_ce: 0.046833
2022-01-11 23:37:04,456 iteration 2968 : loss : 0.094694, loss_ce: 0.033131
2022-01-11 23:37:05,973 iteration 2969 : loss : 0.067585, loss_ce: 0.022650
2022-01-11 23:37:07,516 iteration 2970 : loss : 0.051732, loss_ce: 0.021551
2022-01-11 23:37:09,106 iteration 2971 : loss : 0.119863, loss_ce: 0.048950
2022-01-11 23:37:10,707 iteration 2972 : loss : 0.055013, loss_ce: 0.024334
2022-01-11 23:37:12,270 iteration 2973 : loss : 0.086671, loss_ce: 0.045163
2022-01-11 23:37:13,944 iteration 2974 : loss : 0.095396, loss_ce: 0.039762
2022-01-11 23:37:13,944 Training Data Eval:
2022-01-11 23:37:21,956   Average segmentation loss on training set: 0.3029
2022-01-11 23:37:21,956 Validation Data Eval:
2022-01-11 23:37:24,713   Average segmentation loss on validation set: 0.4590
2022-01-11 23:37:26,265 iteration 2975 : loss : 0.077299, loss_ce: 0.028706
 44%|███████████▊               | 175/400 [1:24:57<1:55:11, 30.72s/it]2022-01-11 23:37:27,921 iteration 2976 : loss : 0.076728, loss_ce: 0.031512
2022-01-11 23:37:29,621 iteration 2977 : loss : 0.046601, loss_ce: 0.020418
2022-01-11 23:37:31,213 iteration 2978 : loss : 0.059690, loss_ce: 0.024097
2022-01-11 23:37:32,865 iteration 2979 : loss : 0.077234, loss_ce: 0.025145
2022-01-11 23:37:34,583 iteration 2980 : loss : 0.064983, loss_ce: 0.028129
2022-01-11 23:37:36,132 iteration 2981 : loss : 0.045358, loss_ce: 0.024176
2022-01-11 23:37:37,706 iteration 2982 : loss : 0.067098, loss_ce: 0.030394
2022-01-11 23:37:39,320 iteration 2983 : loss : 0.053287, loss_ce: 0.015243
2022-01-11 23:37:40,903 iteration 2984 : loss : 0.067703, loss_ce: 0.035541
2022-01-11 23:37:42,486 iteration 2985 : loss : 0.054801, loss_ce: 0.024956
2022-01-11 23:37:44,049 iteration 2986 : loss : 0.070490, loss_ce: 0.027189
2022-01-11 23:37:45,590 iteration 2987 : loss : 0.044847, loss_ce: 0.017327
2022-01-11 23:37:47,187 iteration 2988 : loss : 0.040809, loss_ce: 0.017979
2022-01-11 23:37:48,728 iteration 2989 : loss : 0.044991, loss_ce: 0.017453
2022-01-11 23:37:50,299 iteration 2990 : loss : 0.079147, loss_ce: 0.037916
2022-01-11 23:37:51,837 iteration 2991 : loss : 0.049983, loss_ce: 0.018892
2022-01-11 23:37:53,421 iteration 2992 : loss : 0.049058, loss_ce: 0.022553
 44%|███████████▉               | 176/400 [1:25:24<1:50:41, 29.65s/it]2022-01-11 23:37:55,040 iteration 2993 : loss : 0.044157, loss_ce: 0.020194
2022-01-11 23:37:56,604 iteration 2994 : loss : 0.049001, loss_ce: 0.021391
2022-01-11 23:37:58,220 iteration 2995 : loss : 0.079776, loss_ce: 0.036035
2022-01-11 23:37:59,746 iteration 2996 : loss : 0.056899, loss_ce: 0.022691
2022-01-11 23:38:01,313 iteration 2997 : loss : 0.049027, loss_ce: 0.019076
2022-01-11 23:38:02,919 iteration 2998 : loss : 0.040614, loss_ce: 0.020384
2022-01-11 23:38:04,488 iteration 2999 : loss : 0.069294, loss_ce: 0.027084
2022-01-11 23:38:06,051 iteration 3000 : loss : 0.066878, loss_ce: 0.037324
2022-01-11 23:38:07,699 iteration 3001 : loss : 0.069419, loss_ce: 0.035530
2022-01-11 23:38:09,312 iteration 3002 : loss : 0.077240, loss_ce: 0.027798
2022-01-11 23:38:10,818 iteration 3003 : loss : 0.048145, loss_ce: 0.021910
2022-01-11 23:38:12,389 iteration 3004 : loss : 0.037342, loss_ce: 0.020565
2022-01-11 23:38:13,934 iteration 3005 : loss : 0.039358, loss_ce: 0.017710
2022-01-11 23:38:15,481 iteration 3006 : loss : 0.060387, loss_ce: 0.023180
2022-01-11 23:38:17,066 iteration 3007 : loss : 0.041598, loss_ce: 0.013725
2022-01-11 23:38:18,587 iteration 3008 : loss : 0.043273, loss_ce: 0.016771
2022-01-11 23:38:20,110 iteration 3009 : loss : 0.070873, loss_ce: 0.020478
 44%|███████████▉               | 177/400 [1:25:50<1:46:53, 28.76s/it]2022-01-11 23:38:21,797 iteration 3010 : loss : 0.063827, loss_ce: 0.020619
2022-01-11 23:38:23,352 iteration 3011 : loss : 0.053788, loss_ce: 0.017203
2022-01-11 23:38:24,891 iteration 3012 : loss : 0.049341, loss_ce: 0.016101
2022-01-11 23:38:26,528 iteration 3013 : loss : 0.061320, loss_ce: 0.024326
2022-01-11 23:38:28,126 iteration 3014 : loss : 0.042337, loss_ce: 0.016572
2022-01-11 23:38:29,719 iteration 3015 : loss : 0.042007, loss_ce: 0.014857
2022-01-11 23:38:31,229 iteration 3016 : loss : 0.035958, loss_ce: 0.013876
2022-01-11 23:38:32,851 iteration 3017 : loss : 0.057525, loss_ce: 0.031776
2022-01-11 23:38:34,495 iteration 3018 : loss : 0.039410, loss_ce: 0.019626
2022-01-11 23:38:36,192 iteration 3019 : loss : 0.068827, loss_ce: 0.033278
2022-01-11 23:38:37,725 iteration 3020 : loss : 0.061771, loss_ce: 0.024796
2022-01-11 23:38:39,288 iteration 3021 : loss : 0.045784, loss_ce: 0.019116
2022-01-11 23:38:40,788 iteration 3022 : loss : 0.037130, loss_ce: 0.017460
2022-01-11 23:38:42,330 iteration 3023 : loss : 0.033596, loss_ce: 0.013261
2022-01-11 23:38:43,909 iteration 3024 : loss : 0.056982, loss_ce: 0.027373
2022-01-11 23:38:45,478 iteration 3025 : loss : 0.048691, loss_ce: 0.018772
2022-01-11 23:38:47,008 iteration 3026 : loss : 0.061430, loss_ce: 0.020471
 44%|████████████               | 178/400 [1:26:17<1:44:20, 28.20s/it]2022-01-11 23:38:48,610 iteration 3027 : loss : 0.043621, loss_ce: 0.020758
2022-01-11 23:38:50,263 iteration 3028 : loss : 0.069434, loss_ce: 0.031182
2022-01-11 23:38:51,803 iteration 3029 : loss : 0.037651, loss_ce: 0.014937
2022-01-11 23:38:53,437 iteration 3030 : loss : 0.035143, loss_ce: 0.012217
2022-01-11 23:38:55,068 iteration 3031 : loss : 0.068817, loss_ce: 0.020412
2022-01-11 23:38:56,690 iteration 3032 : loss : 0.055333, loss_ce: 0.022815
2022-01-11 23:38:58,261 iteration 3033 : loss : 0.044056, loss_ce: 0.014382
2022-01-11 23:38:59,821 iteration 3034 : loss : 0.071570, loss_ce: 0.023688
2022-01-11 23:39:01,340 iteration 3035 : loss : 0.093452, loss_ce: 0.035395
2022-01-11 23:39:02,955 iteration 3036 : loss : 0.091433, loss_ce: 0.041870
2022-01-11 23:39:04,476 iteration 3037 : loss : 0.109248, loss_ce: 0.054064
2022-01-11 23:39:05,984 iteration 3038 : loss : 0.103359, loss_ce: 0.048023
2022-01-11 23:39:07,512 iteration 3039 : loss : 0.145585, loss_ce: 0.067282
2022-01-11 23:39:09,148 iteration 3040 : loss : 0.217288, loss_ce: 0.106697
2022-01-11 23:39:10,619 iteration 3041 : loss : 0.142692, loss_ce: 0.068034
2022-01-11 23:39:12,147 iteration 3042 : loss : 0.088898, loss_ce: 0.046943
2022-01-11 23:39:13,673 iteration 3043 : loss : 0.103349, loss_ce: 0.048502
 45%|████████████               | 179/400 [1:26:44<1:42:10, 27.74s/it]2022-01-11 23:39:15,306 iteration 3044 : loss : 0.139992, loss_ce: 0.047141
2022-01-11 23:39:16,795 iteration 3045 : loss : 0.162874, loss_ce: 0.063290
2022-01-11 23:39:18,447 iteration 3046 : loss : 0.149103, loss_ce: 0.073545
2022-01-11 23:39:20,093 iteration 3047 : loss : 0.120148, loss_ce: 0.042643
2022-01-11 23:39:21,686 iteration 3048 : loss : 0.102780, loss_ce: 0.044304
2022-01-11 23:39:23,286 iteration 3049 : loss : 0.075453, loss_ce: 0.031581
2022-01-11 23:39:24,904 iteration 3050 : loss : 0.134685, loss_ce: 0.046329
2022-01-11 23:39:26,550 iteration 3051 : loss : 0.127119, loss_ce: 0.051690
2022-01-11 23:39:28,165 iteration 3052 : loss : 0.151080, loss_ce: 0.087089
2022-01-11 23:39:29,664 iteration 3053 : loss : 0.077041, loss_ce: 0.028525
2022-01-11 23:39:31,208 iteration 3054 : loss : 0.120973, loss_ce: 0.058091
2022-01-11 23:39:32,713 iteration 3055 : loss : 0.112356, loss_ce: 0.054154
2022-01-11 23:39:34,207 iteration 3056 : loss : 0.099062, loss_ce: 0.034979
2022-01-11 23:39:35,882 iteration 3057 : loss : 0.176103, loss_ce: 0.073103
2022-01-11 23:39:37,400 iteration 3058 : loss : 0.104229, loss_ce: 0.042081
2022-01-11 23:39:38,945 iteration 3059 : loss : 0.065652, loss_ce: 0.034769
2022-01-11 23:39:38,945 Training Data Eval:
2022-01-11 23:39:46,990   Average segmentation loss on training set: 0.1001
2022-01-11 23:39:46,991 Validation Data Eval:
2022-01-11 23:39:49,759   Average segmentation loss on validation set: 0.1353
2022-01-11 23:39:51,373 iteration 3060 : loss : 0.096782, loss_ce: 0.038956
 45%|████████████▏              | 180/400 [1:27:22<1:52:40, 30.73s/it]2022-01-11 23:39:52,996 iteration 3061 : loss : 0.072145, loss_ce: 0.024287
2022-01-11 23:39:54,603 iteration 3062 : loss : 0.097432, loss_ce: 0.047637
2022-01-11 23:39:56,161 iteration 3063 : loss : 0.066882, loss_ce: 0.029989
2022-01-11 23:39:57,747 iteration 3064 : loss : 0.125167, loss_ce: 0.069302
2022-01-11 23:39:59,355 iteration 3065 : loss : 0.066282, loss_ce: 0.022432
2022-01-11 23:40:00,897 iteration 3066 : loss : 0.075102, loss_ce: 0.024968
2022-01-11 23:40:02,536 iteration 3067 : loss : 0.076417, loss_ce: 0.030742
2022-01-11 23:40:04,186 iteration 3068 : loss : 0.105555, loss_ce: 0.059647
2022-01-11 23:40:05,710 iteration 3069 : loss : 0.050213, loss_ce: 0.020409
2022-01-11 23:40:07,293 iteration 3070 : loss : 0.087530, loss_ce: 0.030693
2022-01-11 23:40:09,007 iteration 3071 : loss : 0.106653, loss_ce: 0.039532
2022-01-11 23:40:10,550 iteration 3072 : loss : 0.074055, loss_ce: 0.027753
2022-01-11 23:40:12,108 iteration 3073 : loss : 0.063980, loss_ce: 0.024973
2022-01-11 23:40:13,673 iteration 3074 : loss : 0.064032, loss_ce: 0.030180
2022-01-11 23:40:15,272 iteration 3075 : loss : 0.084148, loss_ce: 0.029472
2022-01-11 23:40:16,796 iteration 3076 : loss : 0.063318, loss_ce: 0.025663
2022-01-11 23:40:18,437 iteration 3077 : loss : 0.142884, loss_ce: 0.052284
 45%|████████████▏              | 181/400 [1:27:49<1:48:08, 29.63s/it]2022-01-11 23:40:20,079 iteration 3078 : loss : 0.164148, loss_ce: 0.034796
2022-01-11 23:40:21,630 iteration 3079 : loss : 0.054889, loss_ce: 0.018184
2022-01-11 23:40:23,180 iteration 3080 : loss : 0.055475, loss_ce: 0.019489
2022-01-11 23:40:24,737 iteration 3081 : loss : 0.079770, loss_ce: 0.039727
2022-01-11 23:40:26,309 iteration 3082 : loss : 0.075536, loss_ce: 0.028964
2022-01-11 23:40:27,886 iteration 3083 : loss : 0.103158, loss_ce: 0.042278
2022-01-11 23:40:29,467 iteration 3084 : loss : 0.077435, loss_ce: 0.035668
2022-01-11 23:40:30,971 iteration 3085 : loss : 0.057700, loss_ce: 0.021146
2022-01-11 23:40:32,513 iteration 3086 : loss : 0.077159, loss_ce: 0.044331
2022-01-11 23:40:34,126 iteration 3087 : loss : 0.071131, loss_ce: 0.033368
2022-01-11 23:40:35,696 iteration 3088 : loss : 0.061904, loss_ce: 0.028017
2022-01-11 23:40:37,341 iteration 3089 : loss : 0.087117, loss_ce: 0.040351
2022-01-11 23:40:38,850 iteration 3090 : loss : 0.084608, loss_ce: 0.038066
2022-01-11 23:40:40,460 iteration 3091 : loss : 0.084992, loss_ce: 0.041456
2022-01-11 23:40:42,002 iteration 3092 : loss : 0.078479, loss_ce: 0.035609
2022-01-11 23:40:43,529 iteration 3093 : loss : 0.054135, loss_ce: 0.025998
2022-01-11 23:40:45,091 iteration 3094 : loss : 0.124488, loss_ce: 0.040799
 46%|████████████▎              | 182/400 [1:28:15<1:44:24, 28.74s/it]2022-01-11 23:40:46,730 iteration 3095 : loss : 0.045827, loss_ce: 0.019351
2022-01-11 23:40:48,284 iteration 3096 : loss : 0.048266, loss_ce: 0.018798
2022-01-11 23:40:49,870 iteration 3097 : loss : 0.049169, loss_ce: 0.022754
2022-01-11 23:40:51,464 iteration 3098 : loss : 0.070798, loss_ce: 0.022379
2022-01-11 23:40:53,064 iteration 3099 : loss : 0.072651, loss_ce: 0.035659
2022-01-11 23:40:54,629 iteration 3100 : loss : 0.070114, loss_ce: 0.023904
2022-01-11 23:40:56,176 iteration 3101 : loss : 0.045072, loss_ce: 0.018294
2022-01-11 23:40:57,680 iteration 3102 : loss : 0.050674, loss_ce: 0.020230
2022-01-11 23:40:59,301 iteration 3103 : loss : 0.065401, loss_ce: 0.031021
2022-01-11 23:41:00,875 iteration 3104 : loss : 0.075232, loss_ce: 0.021316
2022-01-11 23:41:02,498 iteration 3105 : loss : 0.045577, loss_ce: 0.018570
2022-01-11 23:41:04,070 iteration 3106 : loss : 0.063443, loss_ce: 0.025101
2022-01-11 23:41:05,683 iteration 3107 : loss : 0.048282, loss_ce: 0.022622
2022-01-11 23:41:07,270 iteration 3108 : loss : 0.060728, loss_ce: 0.023794
2022-01-11 23:41:08,776 iteration 3109 : loss : 0.079287, loss_ce: 0.037697
2022-01-11 23:41:10,320 iteration 3110 : loss : 0.066821, loss_ce: 0.028327
2022-01-11 23:41:11,892 iteration 3111 : loss : 0.070787, loss_ce: 0.022094
 46%|████████████▎              | 183/400 [1:28:42<1:41:50, 28.16s/it]2022-01-11 23:41:13,458 iteration 3112 : loss : 0.043381, loss_ce: 0.015882
2022-01-11 23:41:15,054 iteration 3113 : loss : 0.062437, loss_ce: 0.023360
2022-01-11 23:41:16,588 iteration 3114 : loss : 0.049767, loss_ce: 0.024029
2022-01-11 23:41:18,165 iteration 3115 : loss : 0.050817, loss_ce: 0.018698
2022-01-11 23:41:19,663 iteration 3116 : loss : 0.071228, loss_ce: 0.030335
2022-01-11 23:41:21,298 iteration 3117 : loss : 0.112903, loss_ce: 0.033944
2022-01-11 23:41:22,885 iteration 3118 : loss : 0.047991, loss_ce: 0.024517
2022-01-11 23:41:24,471 iteration 3119 : loss : 0.044034, loss_ce: 0.020895
2022-01-11 23:41:25,978 iteration 3120 : loss : 0.046641, loss_ce: 0.016473
2022-01-11 23:41:27,560 iteration 3121 : loss : 0.111040, loss_ce: 0.032915
2022-01-11 23:41:29,123 iteration 3122 : loss : 0.048939, loss_ce: 0.024051
2022-01-11 23:41:30,798 iteration 3123 : loss : 0.079254, loss_ce: 0.030292
2022-01-11 23:41:32,330 iteration 3124 : loss : 0.049881, loss_ce: 0.018918
2022-01-11 23:41:33,859 iteration 3125 : loss : 0.037277, loss_ce: 0.018453
2022-01-11 23:41:35,396 iteration 3126 : loss : 0.066410, loss_ce: 0.029690
2022-01-11 23:41:36,947 iteration 3127 : loss : 0.042251, loss_ce: 0.017919
2022-01-11 23:41:38,505 iteration 3128 : loss : 0.075861, loss_ce: 0.021844
 46%|████████████▍              | 184/400 [1:29:09<1:39:41, 27.69s/it]2022-01-11 23:41:40,083 iteration 3129 : loss : 0.058900, loss_ce: 0.024100
2022-01-11 23:41:41,618 iteration 3130 : loss : 0.040362, loss_ce: 0.015210
2022-01-11 23:41:43,225 iteration 3131 : loss : 0.051209, loss_ce: 0.025932
2022-01-11 23:41:44,803 iteration 3132 : loss : 0.050440, loss_ce: 0.020364
2022-01-11 23:41:46,337 iteration 3133 : loss : 0.047678, loss_ce: 0.021590
2022-01-11 23:41:47,935 iteration 3134 : loss : 0.045288, loss_ce: 0.020659
2022-01-11 23:41:49,534 iteration 3135 : loss : 0.056798, loss_ce: 0.012733
2022-01-11 23:41:51,050 iteration 3136 : loss : 0.030484, loss_ce: 0.013950
2022-01-11 23:41:52,577 iteration 3137 : loss : 0.035047, loss_ce: 0.015781
2022-01-11 23:41:54,135 iteration 3138 : loss : 0.058659, loss_ce: 0.025059
2022-01-11 23:41:55,827 iteration 3139 : loss : 0.061985, loss_ce: 0.021592
2022-01-11 23:41:57,426 iteration 3140 : loss : 0.043845, loss_ce: 0.018646
2022-01-11 23:41:58,975 iteration 3141 : loss : 0.056032, loss_ce: 0.027238
2022-01-11 23:42:00,479 iteration 3142 : loss : 0.032060, loss_ce: 0.012633
2022-01-11 23:42:02,068 iteration 3143 : loss : 0.070733, loss_ce: 0.022144
2022-01-11 23:42:03,681 iteration 3144 : loss : 0.077168, loss_ce: 0.021052
2022-01-11 23:42:03,681 Training Data Eval:
2022-01-11 23:42:11,697   Average segmentation loss on training set: 0.0367
2022-01-11 23:42:11,698 Validation Data Eval:
2022-01-11 23:42:14,458   Average segmentation loss on validation set: 0.1399
2022-01-11 23:42:15,982 iteration 3145 : loss : 0.044679, loss_ce: 0.023132
 46%|████████████▍              | 185/400 [1:29:46<1:49:45, 30.63s/it]2022-01-11 23:42:17,689 iteration 3146 : loss : 0.042694, loss_ce: 0.016887
2022-01-11 23:42:19,330 iteration 3147 : loss : 0.059048, loss_ce: 0.024416
2022-01-11 23:42:20,844 iteration 3148 : loss : 0.033238, loss_ce: 0.012159
2022-01-11 23:42:22,421 iteration 3149 : loss : 0.051138, loss_ce: 0.016200
2022-01-11 23:42:23,996 iteration 3150 : loss : 0.076844, loss_ce: 0.021909
2022-01-11 23:42:25,485 iteration 3151 : loss : 0.037742, loss_ce: 0.016969
2022-01-11 23:42:27,094 iteration 3152 : loss : 0.035977, loss_ce: 0.017479
2022-01-11 23:42:28,630 iteration 3153 : loss : 0.042567, loss_ce: 0.021270
2022-01-11 23:42:30,246 iteration 3154 : loss : 0.061065, loss_ce: 0.019487
2022-01-11 23:42:31,827 iteration 3155 : loss : 0.046455, loss_ce: 0.018758
2022-01-11 23:42:33,495 iteration 3156 : loss : 0.040136, loss_ce: 0.016112
2022-01-11 23:42:34,988 iteration 3157 : loss : 0.079057, loss_ce: 0.024212
2022-01-11 23:42:36,622 iteration 3158 : loss : 0.045727, loss_ce: 0.021231
2022-01-11 23:42:38,195 iteration 3159 : loss : 0.045127, loss_ce: 0.022356
2022-01-11 23:42:39,756 iteration 3160 : loss : 0.039260, loss_ce: 0.019013
2022-01-11 23:42:41,388 iteration 3161 : loss : 0.045978, loss_ce: 0.018397
2022-01-11 23:42:43,048 iteration 3162 : loss : 0.053985, loss_ce: 0.029979
 46%|████████████▌              | 186/400 [1:30:13<1:45:25, 29.56s/it]2022-01-11 23:42:44,612 iteration 3163 : loss : 0.042227, loss_ce: 0.013192
2022-01-11 23:42:46,142 iteration 3164 : loss : 0.046734, loss_ce: 0.017151
2022-01-11 23:42:47,691 iteration 3165 : loss : 0.029131, loss_ce: 0.012315
2022-01-11 23:42:49,262 iteration 3166 : loss : 0.036972, loss_ce: 0.017713
2022-01-11 23:42:50,855 iteration 3167 : loss : 0.098762, loss_ce: 0.040101
2022-01-11 23:42:52,376 iteration 3168 : loss : 0.032097, loss_ce: 0.016870
2022-01-11 23:42:53,954 iteration 3169 : loss : 0.038847, loss_ce: 0.015327
2022-01-11 23:42:55,556 iteration 3170 : loss : 0.049382, loss_ce: 0.020227
2022-01-11 23:42:57,178 iteration 3171 : loss : 0.071419, loss_ce: 0.026492
2022-01-11 23:42:58,772 iteration 3172 : loss : 0.041502, loss_ce: 0.018273
2022-01-11 23:43:00,369 iteration 3173 : loss : 0.035477, loss_ce: 0.013750
2022-01-11 23:43:01,997 iteration 3174 : loss : 0.038488, loss_ce: 0.013797
2022-01-11 23:43:03,563 iteration 3175 : loss : 0.042301, loss_ce: 0.017002
2022-01-11 23:43:05,152 iteration 3176 : loss : 0.041767, loss_ce: 0.023883
2022-01-11 23:43:06,688 iteration 3177 : loss : 0.036197, loss_ce: 0.016554
2022-01-11 23:43:08,283 iteration 3178 : loss : 0.042001, loss_ce: 0.012670
2022-01-11 23:43:09,890 iteration 3179 : loss : 0.067863, loss_ce: 0.017759
 47%|████████████▌              | 187/400 [1:30:40<1:42:02, 28.75s/it]2022-01-11 23:43:11,559 iteration 3180 : loss : 0.053295, loss_ce: 0.025690
2022-01-11 23:43:13,149 iteration 3181 : loss : 0.039999, loss_ce: 0.018773
2022-01-11 23:43:14,756 iteration 3182 : loss : 0.051371, loss_ce: 0.022865
2022-01-11 23:43:16,353 iteration 3183 : loss : 0.035250, loss_ce: 0.015604
2022-01-11 23:43:17,836 iteration 3184 : loss : 0.030716, loss_ce: 0.012739
2022-01-11 23:43:19,362 iteration 3185 : loss : 0.050507, loss_ce: 0.021961
2022-01-11 23:43:20,902 iteration 3186 : loss : 0.048088, loss_ce: 0.024976
2022-01-11 23:43:22,419 iteration 3187 : loss : 0.036553, loss_ce: 0.013820
2022-01-11 23:43:23,982 iteration 3188 : loss : 0.040095, loss_ce: 0.014234
2022-01-11 23:43:25,502 iteration 3189 : loss : 0.030457, loss_ce: 0.015309
2022-01-11 23:43:27,130 iteration 3190 : loss : 0.090974, loss_ce: 0.038475
2022-01-11 23:43:28,734 iteration 3191 : loss : 0.035697, loss_ce: 0.012211
2022-01-11 23:43:30,296 iteration 3192 : loss : 0.034799, loss_ce: 0.014996
2022-01-11 23:43:31,828 iteration 3193 : loss : 0.045450, loss_ce: 0.018278
2022-01-11 23:43:33,344 iteration 3194 : loss : 0.048983, loss_ce: 0.016601
2022-01-11 23:43:34,904 iteration 3195 : loss : 0.051356, loss_ce: 0.015344
2022-01-11 23:43:36,507 iteration 3196 : loss : 0.037742, loss_ce: 0.011925
 47%|████████████▋              | 188/400 [1:31:07<1:39:18, 28.11s/it]2022-01-11 23:43:38,091 iteration 3197 : loss : 0.045383, loss_ce: 0.014758
2022-01-11 23:43:39,619 iteration 3198 : loss : 0.047457, loss_ce: 0.019959
2022-01-11 23:43:41,140 iteration 3199 : loss : 0.048196, loss_ce: 0.018215
2022-01-11 23:43:42,677 iteration 3200 : loss : 0.049577, loss_ce: 0.020298
2022-01-11 23:43:44,295 iteration 3201 : loss : 0.064186, loss_ce: 0.030208
2022-01-11 23:43:45,869 iteration 3202 : loss : 0.038032, loss_ce: 0.018114
2022-01-11 23:43:47,369 iteration 3203 : loss : 0.074969, loss_ce: 0.017285
2022-01-11 23:43:48,941 iteration 3204 : loss : 0.062292, loss_ce: 0.029236
2022-01-11 23:43:50,531 iteration 3205 : loss : 0.031197, loss_ce: 0.011747
2022-01-11 23:43:52,060 iteration 3206 : loss : 0.038705, loss_ce: 0.017667
2022-01-11 23:43:53,582 iteration 3207 : loss : 0.041315, loss_ce: 0.014459
2022-01-11 23:43:55,113 iteration 3208 : loss : 0.054248, loss_ce: 0.017949
2022-01-11 23:43:56,699 iteration 3209 : loss : 0.036193, loss_ce: 0.013975
2022-01-11 23:43:58,268 iteration 3210 : loss : 0.056165, loss_ce: 0.018839
2022-01-11 23:43:59,890 iteration 3211 : loss : 0.044534, loss_ce: 0.023634
2022-01-11 23:44:01,493 iteration 3212 : loss : 0.053033, loss_ce: 0.023758
2022-01-11 23:44:03,046 iteration 3213 : loss : 0.037700, loss_ce: 0.018413
 47%|████████████▊              | 189/400 [1:31:33<1:37:11, 27.64s/it]2022-01-11 23:44:04,644 iteration 3214 : loss : 0.040445, loss_ce: 0.016518
2022-01-11 23:44:06,179 iteration 3215 : loss : 0.043068, loss_ce: 0.019038
2022-01-11 23:44:07,732 iteration 3216 : loss : 0.041931, loss_ce: 0.018055
2022-01-11 23:44:09,329 iteration 3217 : loss : 0.069253, loss_ce: 0.020996
2022-01-11 23:44:10,846 iteration 3218 : loss : 0.039088, loss_ce: 0.016291
2022-01-11 23:44:12,375 iteration 3219 : loss : 0.042659, loss_ce: 0.016858
2022-01-11 23:44:13,950 iteration 3220 : loss : 0.064104, loss_ce: 0.030594
2022-01-11 23:44:15,533 iteration 3221 : loss : 0.057985, loss_ce: 0.019498
2022-01-11 23:44:17,043 iteration 3222 : loss : 0.034423, loss_ce: 0.013238
2022-01-11 23:44:18,620 iteration 3223 : loss : 0.063317, loss_ce: 0.016772
2022-01-11 23:44:20,174 iteration 3224 : loss : 0.050718, loss_ce: 0.020084
2022-01-11 23:44:21,686 iteration 3225 : loss : 0.048060, loss_ce: 0.020107
2022-01-11 23:44:23,245 iteration 3226 : loss : 0.039416, loss_ce: 0.018211
2022-01-11 23:44:24,812 iteration 3227 : loss : 0.055202, loss_ce: 0.020881
2022-01-11 23:44:26,374 iteration 3228 : loss : 0.045860, loss_ce: 0.019864
2022-01-11 23:44:27,967 iteration 3229 : loss : 0.045537, loss_ce: 0.020324
2022-01-11 23:44:27,967 Training Data Eval:
2022-01-11 23:44:35,997   Average segmentation loss on training set: 0.0319
2022-01-11 23:44:35,997 Validation Data Eval:
2022-01-11 23:44:38,757   Average segmentation loss on validation set: 0.0957
2022-01-11 23:44:40,306 iteration 3230 : loss : 0.039781, loss_ce: 0.023230
 48%|████████████▊              | 190/400 [1:32:11<1:46:49, 30.52s/it]2022-01-11 23:44:41,816 iteration 3231 : loss : 0.046549, loss_ce: 0.020027
2022-01-11 23:44:43,395 iteration 3232 : loss : 0.033483, loss_ce: 0.014068
2022-01-11 23:44:44,978 iteration 3233 : loss : 0.071611, loss_ce: 0.021914
2022-01-11 23:44:46,513 iteration 3234 : loss : 0.053262, loss_ce: 0.020537
2022-01-11 23:44:48,092 iteration 3235 : loss : 0.058856, loss_ce: 0.026202
2022-01-11 23:44:49,565 iteration 3236 : loss : 0.037594, loss_ce: 0.019279
2022-01-11 23:44:51,114 iteration 3237 : loss : 0.039533, loss_ce: 0.018877
2022-01-11 23:44:52,673 iteration 3238 : loss : 0.071290, loss_ce: 0.029845
2022-01-11 23:44:54,158 iteration 3239 : loss : 0.050817, loss_ce: 0.019817
2022-01-11 23:44:55,760 iteration 3240 : loss : 0.058880, loss_ce: 0.025286
2022-01-11 23:44:57,404 iteration 3241 : loss : 0.117845, loss_ce: 0.051273
2022-01-11 23:44:58,985 iteration 3242 : loss : 0.082375, loss_ce: 0.032700
2022-01-11 23:45:00,597 iteration 3243 : loss : 0.064882, loss_ce: 0.029796
2022-01-11 23:45:02,129 iteration 3244 : loss : 0.057186, loss_ce: 0.021268
2022-01-11 23:45:03,706 iteration 3245 : loss : 0.068578, loss_ce: 0.025095
2022-01-11 23:45:05,292 iteration 3246 : loss : 0.125786, loss_ce: 0.045324
2022-01-11 23:45:06,956 iteration 3247 : loss : 0.101491, loss_ce: 0.038848
 48%|████████████▉              | 191/400 [1:32:37<1:42:16, 29.36s/it]2022-01-11 23:45:08,604 iteration 3248 : loss : 0.087072, loss_ce: 0.027740
2022-01-11 23:45:10,211 iteration 3249 : loss : 0.060253, loss_ce: 0.021379
2022-01-11 23:45:11,780 iteration 3250 : loss : 0.069954, loss_ce: 0.026368
2022-01-11 23:45:13,363 iteration 3251 : loss : 0.078867, loss_ce: 0.030273
2022-01-11 23:45:14,844 iteration 3252 : loss : 0.050882, loss_ce: 0.023264
2022-01-11 23:45:16,384 iteration 3253 : loss : 0.109878, loss_ce: 0.036080
2022-01-11 23:45:17,975 iteration 3254 : loss : 0.080089, loss_ce: 0.036262
2022-01-11 23:45:19,549 iteration 3255 : loss : 0.079370, loss_ce: 0.031724
2022-01-11 23:45:21,150 iteration 3256 : loss : 0.074682, loss_ce: 0.041928
2022-01-11 23:45:22,721 iteration 3257 : loss : 0.084816, loss_ce: 0.034220
2022-01-11 23:45:24,232 iteration 3258 : loss : 0.077924, loss_ce: 0.026106
2022-01-11 23:45:25,802 iteration 3259 : loss : 0.073500, loss_ce: 0.032170
2022-01-11 23:45:27,398 iteration 3260 : loss : 0.077039, loss_ce: 0.036657
2022-01-11 23:45:28,958 iteration 3261 : loss : 0.072385, loss_ce: 0.028408
2022-01-11 23:45:30,598 iteration 3262 : loss : 0.061814, loss_ce: 0.023690
2022-01-11 23:45:32,133 iteration 3263 : loss : 0.057384, loss_ce: 0.026289
2022-01-11 23:45:33,770 iteration 3264 : loss : 0.083154, loss_ce: 0.028060
 48%|████████████▉              | 192/400 [1:33:04<1:39:08, 28.60s/it]2022-01-11 23:45:35,353 iteration 3265 : loss : 0.053899, loss_ce: 0.023618
2022-01-11 23:45:36,952 iteration 3266 : loss : 0.049859, loss_ce: 0.020870
2022-01-11 23:45:38,527 iteration 3267 : loss : 0.039867, loss_ce: 0.015112
2022-01-11 23:45:40,102 iteration 3268 : loss : 0.076969, loss_ce: 0.029447
2022-01-11 23:45:41,622 iteration 3269 : loss : 0.065744, loss_ce: 0.034464
2022-01-11 23:45:43,189 iteration 3270 : loss : 0.067178, loss_ce: 0.029865
2022-01-11 23:45:44,682 iteration 3271 : loss : 0.046171, loss_ce: 0.017113
2022-01-11 23:45:46,322 iteration 3272 : loss : 0.049796, loss_ce: 0.019447
2022-01-11 23:45:47,876 iteration 3273 : loss : 0.049419, loss_ce: 0.022761
2022-01-11 23:45:49,382 iteration 3274 : loss : 0.060165, loss_ce: 0.023565
2022-01-11 23:45:50,926 iteration 3275 : loss : 0.058400, loss_ce: 0.019285
2022-01-11 23:45:52,468 iteration 3276 : loss : 0.057308, loss_ce: 0.017457
2022-01-11 23:45:53,974 iteration 3277 : loss : 0.039230, loss_ce: 0.014209
2022-01-11 23:45:55,521 iteration 3278 : loss : 0.038293, loss_ce: 0.015466
2022-01-11 23:45:56,996 iteration 3279 : loss : 0.046269, loss_ce: 0.025143
2022-01-11 23:45:58,622 iteration 3280 : loss : 0.047988, loss_ce: 0.019057
2022-01-11 23:46:00,185 iteration 3281 : loss : 0.032267, loss_ce: 0.010339
 48%|█████████████              | 193/400 [1:33:30<1:36:23, 27.94s/it]2022-01-11 23:46:01,821 iteration 3282 : loss : 0.041661, loss_ce: 0.010077
2022-01-11 23:46:03,418 iteration 3283 : loss : 0.056486, loss_ce: 0.024292
2022-01-11 23:46:04,989 iteration 3284 : loss : 0.045935, loss_ce: 0.017174
2022-01-11 23:46:06,574 iteration 3285 : loss : 0.056348, loss_ce: 0.021124
2022-01-11 23:46:08,142 iteration 3286 : loss : 0.055329, loss_ce: 0.016451
2022-01-11 23:46:09,784 iteration 3287 : loss : 0.048623, loss_ce: 0.019840
2022-01-11 23:46:11,297 iteration 3288 : loss : 0.047878, loss_ce: 0.023484
2022-01-11 23:46:12,961 iteration 3289 : loss : 0.043722, loss_ce: 0.015872
2022-01-11 23:46:14,568 iteration 3290 : loss : 0.075404, loss_ce: 0.027676
2022-01-11 23:46:16,185 iteration 3291 : loss : 0.073055, loss_ce: 0.045137
2022-01-11 23:46:17,744 iteration 3292 : loss : 0.067094, loss_ce: 0.029268
2022-01-11 23:46:19,305 iteration 3293 : loss : 0.078579, loss_ce: 0.025525
2022-01-11 23:46:20,965 iteration 3294 : loss : 0.038758, loss_ce: 0.015879
2022-01-11 23:46:22,556 iteration 3295 : loss : 0.034874, loss_ce: 0.016227
2022-01-11 23:46:24,068 iteration 3296 : loss : 0.044768, loss_ce: 0.017449
2022-01-11 23:46:25,607 iteration 3297 : loss : 0.035013, loss_ce: 0.013399
2022-01-11 23:46:27,270 iteration 3298 : loss : 0.049424, loss_ce: 0.021750
 48%|█████████████              | 194/400 [1:33:58<1:35:03, 27.69s/it]2022-01-11 23:46:28,810 iteration 3299 : loss : 0.035772, loss_ce: 0.011406
2022-01-11 23:46:30,424 iteration 3300 : loss : 0.041635, loss_ce: 0.016474
2022-01-11 23:46:32,000 iteration 3301 : loss : 0.035734, loss_ce: 0.017505
2022-01-11 23:46:33,553 iteration 3302 : loss : 0.039099, loss_ce: 0.012122
2022-01-11 23:46:35,195 iteration 3303 : loss : 0.061534, loss_ce: 0.027834
2022-01-11 23:46:36,816 iteration 3304 : loss : 0.041429, loss_ce: 0.017316
2022-01-11 23:46:38,373 iteration 3305 : loss : 0.056259, loss_ce: 0.025314
2022-01-11 23:46:39,927 iteration 3306 : loss : 0.033353, loss_ce: 0.010170
2022-01-11 23:46:41,625 iteration 3307 : loss : 0.070928, loss_ce: 0.036853
2022-01-11 23:46:43,252 iteration 3308 : loss : 0.053188, loss_ce: 0.027327
2022-01-11 23:46:44,863 iteration 3309 : loss : 0.031225, loss_ce: 0.014837
2022-01-11 23:46:46,387 iteration 3310 : loss : 0.029325, loss_ce: 0.015831
2022-01-11 23:46:48,027 iteration 3311 : loss : 0.066002, loss_ce: 0.025135
2022-01-11 23:46:49,577 iteration 3312 : loss : 0.053018, loss_ce: 0.017522
2022-01-11 23:46:51,217 iteration 3313 : loss : 0.039837, loss_ce: 0.016560
2022-01-11 23:46:52,792 iteration 3314 : loss : 0.070851, loss_ce: 0.025983
2022-01-11 23:46:52,792 Training Data Eval:
2022-01-11 23:47:00,813   Average segmentation loss on training set: 0.0263
2022-01-11 23:47:00,814 Validation Data Eval:
2022-01-11 23:47:03,569   Average segmentation loss on validation set: 0.0677
2022-01-11 23:47:05,100 iteration 3315 : loss : 0.034695, loss_ce: 0.016021
 49%|█████████████▏             | 195/400 [1:34:35<1:44:59, 30.73s/it]2022-01-11 23:47:06,672 iteration 3316 : loss : 0.024263, loss_ce: 0.011839
2022-01-11 23:47:08,263 iteration 3317 : loss : 0.040824, loss_ce: 0.019004
2022-01-11 23:47:09,823 iteration 3318 : loss : 0.053008, loss_ce: 0.022977
2022-01-11 23:47:11,346 iteration 3319 : loss : 0.037772, loss_ce: 0.014373
2022-01-11 23:47:12,996 iteration 3320 : loss : 0.061734, loss_ce: 0.023763
2022-01-11 23:47:14,544 iteration 3321 : loss : 0.035219, loss_ce: 0.013737
2022-01-11 23:47:16,174 iteration 3322 : loss : 0.051316, loss_ce: 0.016223
2022-01-11 23:47:17,700 iteration 3323 : loss : 0.035100, loss_ce: 0.012732
2022-01-11 23:47:19,264 iteration 3324 : loss : 0.053452, loss_ce: 0.028803
2022-01-11 23:47:20,870 iteration 3325 : loss : 0.042983, loss_ce: 0.016821
2022-01-11 23:47:22,401 iteration 3326 : loss : 0.043452, loss_ce: 0.017593
2022-01-11 23:47:23,961 iteration 3327 : loss : 0.035562, loss_ce: 0.012110
2022-01-11 23:47:25,587 iteration 3328 : loss : 0.030501, loss_ce: 0.011373
2022-01-11 23:47:27,151 iteration 3329 : loss : 0.035271, loss_ce: 0.013646
2022-01-11 23:47:28,688 iteration 3330 : loss : 0.032398, loss_ce: 0.017445
2022-01-11 23:47:30,181 iteration 3331 : loss : 0.036529, loss_ce: 0.016604
2022-01-11 23:47:31,767 iteration 3332 : loss : 0.038398, loss_ce: 0.012178
 49%|█████████████▏             | 196/400 [1:35:02<1:40:19, 29.51s/it]2022-01-11 23:47:33,398 iteration 3333 : loss : 0.044181, loss_ce: 0.017412
2022-01-11 23:47:34,988 iteration 3334 : loss : 0.060230, loss_ce: 0.036403
2022-01-11 23:47:36,638 iteration 3335 : loss : 0.042787, loss_ce: 0.020830
2022-01-11 23:47:38,215 iteration 3336 : loss : 0.055388, loss_ce: 0.018662
2022-01-11 23:47:39,827 iteration 3337 : loss : 0.062593, loss_ce: 0.023171
2022-01-11 23:47:41,480 iteration 3338 : loss : 0.056471, loss_ce: 0.026036
2022-01-11 23:47:42,998 iteration 3339 : loss : 0.030313, loss_ce: 0.012429
2022-01-11 23:47:44,494 iteration 3340 : loss : 0.047384, loss_ce: 0.014038
2022-01-11 23:47:46,061 iteration 3341 : loss : 0.035584, loss_ce: 0.014474
2022-01-11 23:47:47,609 iteration 3342 : loss : 0.035767, loss_ce: 0.021985
2022-01-11 23:47:49,208 iteration 3343 : loss : 0.028362, loss_ce: 0.012854
2022-01-11 23:47:50,776 iteration 3344 : loss : 0.052392, loss_ce: 0.017542
2022-01-11 23:47:52,363 iteration 3345 : loss : 0.032144, loss_ce: 0.013633
2022-01-11 23:47:53,897 iteration 3346 : loss : 0.025141, loss_ce: 0.011718
2022-01-11 23:47:55,416 iteration 3347 : loss : 0.028026, loss_ce: 0.010371
2022-01-11 23:47:56,917 iteration 3348 : loss : 0.028480, loss_ce: 0.011646
2022-01-11 23:47:58,540 iteration 3349 : loss : 0.072492, loss_ce: 0.017009
 49%|█████████████▎             | 197/400 [1:35:29<1:37:03, 28.69s/it]2022-01-11 23:48:00,158 iteration 3350 : loss : 0.036988, loss_ce: 0.015624
2022-01-11 23:48:01,783 iteration 3351 : loss : 0.045464, loss_ce: 0.016377
2022-01-11 23:48:03,393 iteration 3352 : loss : 0.058409, loss_ce: 0.026826
2022-01-11 23:48:05,029 iteration 3353 : loss : 0.048704, loss_ce: 0.014372
2022-01-11 23:48:06,554 iteration 3354 : loss : 0.028833, loss_ce: 0.011867
2022-01-11 23:48:08,180 iteration 3355 : loss : 0.043814, loss_ce: 0.018831
2022-01-11 23:48:09,795 iteration 3356 : loss : 0.034515, loss_ce: 0.013004
2022-01-11 23:48:11,345 iteration 3357 : loss : 0.038725, loss_ce: 0.012783
2022-01-11 23:48:12,948 iteration 3358 : loss : 0.035012, loss_ce: 0.015894
2022-01-11 23:48:14,447 iteration 3359 : loss : 0.036440, loss_ce: 0.014276
2022-01-11 23:48:15,992 iteration 3360 : loss : 0.042494, loss_ce: 0.015370
2022-01-11 23:48:17,566 iteration 3361 : loss : 0.035013, loss_ce: 0.012982
2022-01-11 23:48:19,128 iteration 3362 : loss : 0.038424, loss_ce: 0.013928
2022-01-11 23:48:20,760 iteration 3363 : loss : 0.044497, loss_ce: 0.015733
2022-01-11 23:48:22,270 iteration 3364 : loss : 0.026497, loss_ce: 0.011430
2022-01-11 23:48:23,871 iteration 3365 : loss : 0.050221, loss_ce: 0.019602
2022-01-11 23:48:25,361 iteration 3366 : loss : 0.032450, loss_ce: 0.016350
 50%|█████████████▎             | 198/400 [1:35:56<1:34:41, 28.13s/it]2022-01-11 23:48:26,966 iteration 3367 : loss : 0.033907, loss_ce: 0.013239
2022-01-11 23:48:28,513 iteration 3368 : loss : 0.031241, loss_ce: 0.016019
2022-01-11 23:48:30,012 iteration 3369 : loss : 0.043036, loss_ce: 0.013525
2022-01-11 23:48:31,626 iteration 3370 : loss : 0.046058, loss_ce: 0.022603
2022-01-11 23:48:33,288 iteration 3371 : loss : 0.039920, loss_ce: 0.016173
2022-01-11 23:48:34,804 iteration 3372 : loss : 0.025549, loss_ce: 0.011787
2022-01-11 23:48:36,387 iteration 3373 : loss : 0.026734, loss_ce: 0.009797
2022-01-11 23:48:37,908 iteration 3374 : loss : 0.026816, loss_ce: 0.010274
2022-01-11 23:48:39,440 iteration 3375 : loss : 0.035533, loss_ce: 0.013227
2022-01-11 23:48:40,999 iteration 3376 : loss : 0.054285, loss_ce: 0.016371
2022-01-11 23:48:42,473 iteration 3377 : loss : 0.022601, loss_ce: 0.009618
2022-01-11 23:48:44,061 iteration 3378 : loss : 0.058504, loss_ce: 0.017183
2022-01-11 23:48:45,707 iteration 3379 : loss : 0.059832, loss_ce: 0.028735
2022-01-11 23:48:47,301 iteration 3380 : loss : 0.035522, loss_ce: 0.012280
2022-01-11 23:48:48,905 iteration 3381 : loss : 0.048215, loss_ce: 0.019182
2022-01-11 23:48:50,497 iteration 3382 : loss : 0.039221, loss_ce: 0.017313
2022-01-11 23:48:52,037 iteration 3383 : loss : 0.030822, loss_ce: 0.012101
 50%|█████████████▍             | 199/400 [1:36:22<1:32:46, 27.69s/it]2022-01-11 23:48:53,706 iteration 3384 : loss : 0.049514, loss_ce: 0.016627
2022-01-11 23:48:55,309 iteration 3385 : loss : 0.040387, loss_ce: 0.014479
2022-01-11 23:48:56,836 iteration 3386 : loss : 0.027219, loss_ce: 0.009031
2022-01-11 23:48:58,465 iteration 3387 : loss : 0.040845, loss_ce: 0.017236
2022-01-11 23:49:00,050 iteration 3388 : loss : 0.039891, loss_ce: 0.017530
2022-01-11 23:49:01,597 iteration 3389 : loss : 0.023402, loss_ce: 0.009432
2022-01-11 23:49:03,066 iteration 3390 : loss : 0.022563, loss_ce: 0.009702
2022-01-11 23:49:04,677 iteration 3391 : loss : 0.045479, loss_ce: 0.017067
2022-01-11 23:49:06,279 iteration 3392 : loss : 0.043549, loss_ce: 0.013943
2022-01-11 23:49:07,859 iteration 3393 : loss : 0.033948, loss_ce: 0.014294
2022-01-11 23:49:09,458 iteration 3394 : loss : 0.026692, loss_ce: 0.012069
2022-01-11 23:49:10,959 iteration 3395 : loss : 0.023376, loss_ce: 0.009714
2022-01-11 23:49:12,577 iteration 3396 : loss : 0.039891, loss_ce: 0.012946
2022-01-11 23:49:14,168 iteration 3397 : loss : 0.030199, loss_ce: 0.012487
2022-01-11 23:49:15,764 iteration 3398 : loss : 0.038076, loss_ce: 0.017380
2022-01-11 23:49:17,323 iteration 3399 : loss : 0.041139, loss_ce: 0.015336
2022-01-11 23:49:17,323 Training Data Eval:
2022-01-11 23:49:25,342   Average segmentation loss on training set: 0.0226
2022-01-11 23:49:25,343 Validation Data Eval:
2022-01-11 23:49:28,100   Average segmentation loss on validation set: 0.0718
2022-01-11 23:49:29,603 iteration 3400 : loss : 0.030351, loss_ce: 0.010558
 50%|█████████████▌             | 200/400 [1:37:00<1:42:11, 30.66s/it]2022-01-11 23:49:31,230 iteration 3401 : loss : 0.033106, loss_ce: 0.016758
2022-01-11 23:49:32,741 iteration 3402 : loss : 0.033567, loss_ce: 0.014195
2022-01-11 23:49:34,398 iteration 3403 : loss : 0.039587, loss_ce: 0.013436
2022-01-11 23:49:36,032 iteration 3404 : loss : 0.035162, loss_ce: 0.012773
2022-01-11 23:49:37,552 iteration 3405 : loss : 0.051019, loss_ce: 0.014294
2022-01-11 23:49:39,182 iteration 3406 : loss : 0.035554, loss_ce: 0.014135
2022-01-11 23:49:40,756 iteration 3407 : loss : 0.036226, loss_ce: 0.015387
2022-01-11 23:49:42,255 iteration 3408 : loss : 0.032630, loss_ce: 0.011850
2022-01-11 23:49:43,823 iteration 3409 : loss : 0.025315, loss_ce: 0.008927
2022-01-11 23:49:45,433 iteration 3410 : loss : 0.028788, loss_ce: 0.013230
2022-01-11 23:49:47,053 iteration 3411 : loss : 0.039649, loss_ce: 0.016455
2022-01-11 23:49:48,652 iteration 3412 : loss : 0.027708, loss_ce: 0.011416
2022-01-11 23:49:50,211 iteration 3413 : loss : 0.050917, loss_ce: 0.011159
2022-01-11 23:49:51,834 iteration 3414 : loss : 0.036627, loss_ce: 0.016993
2022-01-11 23:49:53,411 iteration 3415 : loss : 0.027517, loss_ce: 0.012869
2022-01-11 23:49:55,043 iteration 3416 : loss : 0.030014, loss_ce: 0.010172
2022-01-11 23:49:56,676 iteration 3417 : loss : 0.048047, loss_ce: 0.015413
 50%|█████████████▌             | 201/400 [1:37:27<1:38:06, 29.58s/it]2022-01-11 23:49:58,234 iteration 3418 : loss : 0.023071, loss_ce: 0.008241
2022-01-11 23:49:59,714 iteration 3419 : loss : 0.025857, loss_ce: 0.011781
2022-01-11 23:50:01,240 iteration 3420 : loss : 0.032442, loss_ce: 0.012545
2022-01-11 23:50:02,754 iteration 3421 : loss : 0.028309, loss_ce: 0.011574
2022-01-11 23:50:04,256 iteration 3422 : loss : 0.025806, loss_ce: 0.009763
2022-01-11 23:50:05,842 iteration 3423 : loss : 0.044053, loss_ce: 0.024081
2022-01-11 23:50:07,415 iteration 3424 : loss : 0.047382, loss_ce: 0.019340
2022-01-11 23:50:08,995 iteration 3425 : loss : 0.034724, loss_ce: 0.012506
2022-01-11 23:50:10,476 iteration 3426 : loss : 0.040056, loss_ce: 0.016917
2022-01-11 23:50:12,050 iteration 3427 : loss : 0.038925, loss_ce: 0.012358
2022-01-11 23:50:13,648 iteration 3428 : loss : 0.065520, loss_ce: 0.027545
2022-01-11 23:50:15,178 iteration 3429 : loss : 0.028202, loss_ce: 0.009899
2022-01-11 23:50:16,772 iteration 3430 : loss : 0.037312, loss_ce: 0.015174
2022-01-11 23:50:18,345 iteration 3431 : loss : 0.034469, loss_ce: 0.012973
2022-01-11 23:50:19,972 iteration 3432 : loss : 0.037666, loss_ce: 0.014992
2022-01-11 23:50:21,580 iteration 3433 : loss : 0.054455, loss_ce: 0.019473
2022-01-11 23:50:23,158 iteration 3434 : loss : 0.044677, loss_ce: 0.013414
 50%|█████████████▋             | 202/400 [1:37:53<1:34:32, 28.65s/it]2022-01-11 23:50:24,753 iteration 3435 : loss : 0.041745, loss_ce: 0.015112
2022-01-11 23:50:26,375 iteration 3436 : loss : 0.043871, loss_ce: 0.018972
2022-01-11 23:50:28,004 iteration 3437 : loss : 0.028540, loss_ce: 0.010446
2022-01-11 23:50:29,644 iteration 3438 : loss : 0.042668, loss_ce: 0.017291
2022-01-11 23:50:31,163 iteration 3439 : loss : 0.034832, loss_ce: 0.013820
2022-01-11 23:50:32,750 iteration 3440 : loss : 0.027779, loss_ce: 0.009994
2022-01-11 23:50:34,298 iteration 3441 : loss : 0.034328, loss_ce: 0.011887
2022-01-11 23:50:35,879 iteration 3442 : loss : 0.041799, loss_ce: 0.016948
2022-01-11 23:50:37,532 iteration 3443 : loss : 0.028201, loss_ce: 0.010677
2022-01-11 23:50:39,079 iteration 3444 : loss : 0.038423, loss_ce: 0.013721
2022-01-11 23:50:40,676 iteration 3445 : loss : 0.042476, loss_ce: 0.020147
2022-01-11 23:50:42,294 iteration 3446 : loss : 0.037219, loss_ce: 0.018019
2022-01-11 23:50:43,913 iteration 3447 : loss : 0.057462, loss_ce: 0.025251
2022-01-11 23:50:45,434 iteration 3448 : loss : 0.033170, loss_ce: 0.012340
2022-01-11 23:50:46,967 iteration 3449 : loss : 0.044868, loss_ce: 0.019222
2022-01-11 23:50:48,516 iteration 3450 : loss : 0.038720, loss_ce: 0.013794
2022-01-11 23:50:50,068 iteration 3451 : loss : 0.028673, loss_ce: 0.017992
 51%|█████████████▋             | 203/400 [1:38:20<1:32:21, 28.13s/it]2022-01-11 23:50:51,596 iteration 3452 : loss : 0.023945, loss_ce: 0.011190
2022-01-11 23:50:53,210 iteration 3453 : loss : 0.034251, loss_ce: 0.012516
2022-01-11 23:50:54,904 iteration 3454 : loss : 0.047222, loss_ce: 0.020642
2022-01-11 23:50:56,429 iteration 3455 : loss : 0.044617, loss_ce: 0.021462
2022-01-11 23:50:58,007 iteration 3456 : loss : 0.035174, loss_ce: 0.009127
2022-01-11 23:50:59,525 iteration 3457 : loss : 0.025962, loss_ce: 0.010194
2022-01-11 23:51:01,034 iteration 3458 : loss : 0.044137, loss_ce: 0.013825
2022-01-11 23:51:02,656 iteration 3459 : loss : 0.037238, loss_ce: 0.016491
2022-01-11 23:51:04,165 iteration 3460 : loss : 0.026041, loss_ce: 0.010723
2022-01-11 23:51:05,686 iteration 3461 : loss : 0.027164, loss_ce: 0.009867
2022-01-11 23:51:07,154 iteration 3462 : loss : 0.023907, loss_ce: 0.011024
2022-01-11 23:51:08,755 iteration 3463 : loss : 0.038337, loss_ce: 0.018258
2022-01-11 23:51:10,338 iteration 3464 : loss : 0.037306, loss_ce: 0.014376
2022-01-11 23:51:11,870 iteration 3465 : loss : 0.028328, loss_ce: 0.011483
2022-01-11 23:51:13,507 iteration 3466 : loss : 0.043693, loss_ce: 0.021609
2022-01-11 23:51:14,994 iteration 3467 : loss : 0.025044, loss_ce: 0.010368
2022-01-11 23:51:16,555 iteration 3468 : loss : 0.032774, loss_ce: 0.011330
 51%|█████████████▊             | 204/400 [1:38:47<1:30:16, 27.63s/it]2022-01-11 23:51:18,143 iteration 3469 : loss : 0.028253, loss_ce: 0.012034
2022-01-11 23:51:19,696 iteration 3470 : loss : 0.033209, loss_ce: 0.014271
2022-01-11 23:51:21,323 iteration 3471 : loss : 0.029412, loss_ce: 0.011754
2022-01-11 23:51:22,958 iteration 3472 : loss : 0.045988, loss_ce: 0.017052
2022-01-11 23:51:24,481 iteration 3473 : loss : 0.022637, loss_ce: 0.011249
2022-01-11 23:51:26,100 iteration 3474 : loss : 0.037560, loss_ce: 0.015081
2022-01-11 23:51:27,666 iteration 3475 : loss : 0.036250, loss_ce: 0.015820
2022-01-11 23:51:29,187 iteration 3476 : loss : 0.038704, loss_ce: 0.015011
2022-01-11 23:51:30,740 iteration 3477 : loss : 0.028367, loss_ce: 0.010438
2022-01-11 23:51:32,319 iteration 3478 : loss : 0.032514, loss_ce: 0.014569
2022-01-11 23:51:33,879 iteration 3479 : loss : 0.028375, loss_ce: 0.012804
2022-01-11 23:51:35,402 iteration 3480 : loss : 0.025537, loss_ce: 0.012853
2022-01-11 23:51:36,940 iteration 3481 : loss : 0.057119, loss_ce: 0.017376
2022-01-11 23:51:38,500 iteration 3482 : loss : 0.029533, loss_ce: 0.011944
2022-01-11 23:51:40,129 iteration 3483 : loss : 0.036888, loss_ce: 0.016806
2022-01-11 23:51:41,715 iteration 3484 : loss : 0.032963, loss_ce: 0.010863
2022-01-11 23:51:41,715 Training Data Eval:
2022-01-11 23:51:49,750   Average segmentation loss on training set: 0.0227
2022-01-11 23:51:49,750 Validation Data Eval:
2022-01-11 23:51:52,513   Average segmentation loss on validation set: 0.0649
2022-01-11 23:51:54,073 iteration 3485 : loss : 0.024932, loss_ce: 0.009116
 51%|█████████████▊             | 205/400 [1:39:24<1:39:27, 30.60s/it]2022-01-11 23:51:55,708 iteration 3486 : loss : 0.033521, loss_ce: 0.009568
2022-01-11 23:51:57,276 iteration 3487 : loss : 0.032703, loss_ce: 0.017518
2022-01-11 23:51:58,830 iteration 3488 : loss : 0.046622, loss_ce: 0.013794
2022-01-11 23:52:00,287 iteration 3489 : loss : 0.033135, loss_ce: 0.016060
2022-01-11 23:52:01,885 iteration 3490 : loss : 0.044577, loss_ce: 0.018422
2022-01-11 23:52:03,547 iteration 3491 : loss : 0.054018, loss_ce: 0.026124
2022-01-11 23:52:05,128 iteration 3492 : loss : 0.029894, loss_ce: 0.008341
2022-01-11 23:52:06,670 iteration 3493 : loss : 0.040648, loss_ce: 0.014055
2022-01-11 23:52:08,255 iteration 3494 : loss : 0.027845, loss_ce: 0.010291
2022-01-11 23:52:09,859 iteration 3495 : loss : 0.027602, loss_ce: 0.008750
2022-01-11 23:52:11,529 iteration 3496 : loss : 0.039052, loss_ce: 0.015331
2022-01-11 23:52:13,090 iteration 3497 : loss : 0.025819, loss_ce: 0.010292
2022-01-11 23:52:14,698 iteration 3498 : loss : 0.036446, loss_ce: 0.015270
2022-01-11 23:52:16,233 iteration 3499 : loss : 0.029986, loss_ce: 0.014740
2022-01-11 23:52:17,799 iteration 3500 : loss : 0.036941, loss_ce: 0.016702
2022-01-11 23:52:19,448 iteration 3501 : loss : 0.038284, loss_ce: 0.015494
2022-01-11 23:52:21,013 iteration 3502 : loss : 0.026040, loss_ce: 0.013600
 52%|█████████████▉             | 206/400 [1:39:51<1:35:23, 29.50s/it]2022-01-11 23:52:22,653 iteration 3503 : loss : 0.043635, loss_ce: 0.013264
2022-01-11 23:52:24,224 iteration 3504 : loss : 0.029401, loss_ce: 0.010664
2022-01-11 23:52:25,801 iteration 3505 : loss : 0.025204, loss_ce: 0.012143
2022-01-11 23:52:27,438 iteration 3506 : loss : 0.056606, loss_ce: 0.020867
2022-01-11 23:52:28,960 iteration 3507 : loss : 0.022915, loss_ce: 0.009787
2022-01-11 23:52:30,522 iteration 3508 : loss : 0.039348, loss_ce: 0.008759
2022-01-11 23:52:32,123 iteration 3509 : loss : 0.034738, loss_ce: 0.014557
2022-01-11 23:52:33,685 iteration 3510 : loss : 0.028059, loss_ce: 0.012451
2022-01-11 23:52:35,443 iteration 3511 : loss : 0.059372, loss_ce: 0.029959
2022-01-11 23:52:36,955 iteration 3512 : loss : 0.024310, loss_ce: 0.008813
2022-01-11 23:52:38,449 iteration 3513 : loss : 0.031407, loss_ce: 0.013885
2022-01-11 23:52:40,011 iteration 3514 : loss : 0.034437, loss_ce: 0.016270
2022-01-11 23:52:41,561 iteration 3515 : loss : 0.040924, loss_ce: 0.019445
2022-01-11 23:52:43,102 iteration 3516 : loss : 0.023801, loss_ce: 0.011521
2022-01-11 23:52:44,664 iteration 3517 : loss : 0.033892, loss_ce: 0.015958
2022-01-11 23:52:46,309 iteration 3518 : loss : 0.068387, loss_ce: 0.026854
2022-01-11 23:52:47,916 iteration 3519 : loss : 0.039118, loss_ce: 0.018074
 52%|█████████████▉             | 207/400 [1:40:18<1:32:23, 28.72s/it]2022-01-11 23:52:49,461 iteration 3520 : loss : 0.028196, loss_ce: 0.009624
2022-01-11 23:52:51,010 iteration 3521 : loss : 0.030757, loss_ce: 0.008599
2022-01-11 23:52:52,520 iteration 3522 : loss : 0.034911, loss_ce: 0.010876
2022-01-11 23:52:54,119 iteration 3523 : loss : 0.036761, loss_ce: 0.021388
2022-01-11 23:52:55,754 iteration 3524 : loss : 0.036473, loss_ce: 0.015551
2022-01-11 23:52:57,288 iteration 3525 : loss : 0.043090, loss_ce: 0.013760
2022-01-11 23:52:58,885 iteration 3526 : loss : 0.031560, loss_ce: 0.017017
2022-01-11 23:53:00,538 iteration 3527 : loss : 0.031967, loss_ce: 0.012279
2022-01-11 23:53:02,068 iteration 3528 : loss : 0.060686, loss_ce: 0.013063
2022-01-11 23:53:03,608 iteration 3529 : loss : 0.036637, loss_ce: 0.013277
2022-01-11 23:53:05,160 iteration 3530 : loss : 0.031121, loss_ce: 0.011317
2022-01-11 23:53:06,734 iteration 3531 : loss : 0.038839, loss_ce: 0.014863
2022-01-11 23:53:08,321 iteration 3532 : loss : 0.030831, loss_ce: 0.013480
2022-01-11 23:53:09,907 iteration 3533 : loss : 0.043101, loss_ce: 0.015972
2022-01-11 23:53:11,420 iteration 3534 : loss : 0.051016, loss_ce: 0.017387
2022-01-11 23:53:12,999 iteration 3535 : loss : 0.038550, loss_ce: 0.015009
2022-01-11 23:53:14,535 iteration 3536 : loss : 0.037191, loss_ce: 0.015939
 52%|██████████████             | 208/400 [1:40:45<1:29:53, 28.09s/it]2022-01-11 23:53:16,211 iteration 3537 : loss : 0.052236, loss_ce: 0.027762
2022-01-11 23:53:17,741 iteration 3538 : loss : 0.032492, loss_ce: 0.012053
2022-01-11 23:53:19,288 iteration 3539 : loss : 0.034460, loss_ce: 0.011320
2022-01-11 23:53:20,891 iteration 3540 : loss : 0.037606, loss_ce: 0.017285
2022-01-11 23:53:22,514 iteration 3541 : loss : 0.041389, loss_ce: 0.018517
2022-01-11 23:53:24,115 iteration 3542 : loss : 0.039072, loss_ce: 0.017131
2022-01-11 23:53:25,754 iteration 3543 : loss : 0.038804, loss_ce: 0.016630
2022-01-11 23:53:27,350 iteration 3544 : loss : 0.081361, loss_ce: 0.024108
2022-01-11 23:53:28,960 iteration 3545 : loss : 0.049838, loss_ce: 0.016390
2022-01-11 23:53:30,574 iteration 3546 : loss : 0.052180, loss_ce: 0.019958
2022-01-11 23:53:32,215 iteration 3547 : loss : 0.035312, loss_ce: 0.015463
2022-01-11 23:53:33,744 iteration 3548 : loss : 0.018615, loss_ce: 0.006750
2022-01-11 23:53:35,393 iteration 3549 : loss : 0.061065, loss_ce: 0.035362
2022-01-11 23:53:36,925 iteration 3550 : loss : 0.025506, loss_ce: 0.009025
2022-01-11 23:53:38,462 iteration 3551 : loss : 0.031618, loss_ce: 0.013819
2022-01-11 23:53:39,987 iteration 3552 : loss : 0.039482, loss_ce: 0.018016
2022-01-11 23:53:41,519 iteration 3553 : loss : 0.025158, loss_ce: 0.008986
 52%|██████████████             | 209/400 [1:41:12<1:28:22, 27.76s/it]2022-01-11 23:53:43,111 iteration 3554 : loss : 0.031929, loss_ce: 0.013789
2022-01-11 23:53:44,665 iteration 3555 : loss : 0.048571, loss_ce: 0.021424
2022-01-11 23:53:46,296 iteration 3556 : loss : 0.037311, loss_ce: 0.016824
2022-01-11 23:53:47,935 iteration 3557 : loss : 0.037230, loss_ce: 0.020565
2022-01-11 23:53:49,492 iteration 3558 : loss : 0.039254, loss_ce: 0.013100
2022-01-11 23:53:51,066 iteration 3559 : loss : 0.055055, loss_ce: 0.015551
2022-01-11 23:53:52,632 iteration 3560 : loss : 0.030542, loss_ce: 0.012630
2022-01-11 23:53:54,119 iteration 3561 : loss : 0.021994, loss_ce: 0.010335
2022-01-11 23:53:55,693 iteration 3562 : loss : 0.035827, loss_ce: 0.014706
2022-01-11 23:53:57,262 iteration 3563 : loss : 0.033857, loss_ce: 0.012302
2022-01-11 23:53:58,874 iteration 3564 : loss : 0.062263, loss_ce: 0.015479
2022-01-11 23:54:00,396 iteration 3565 : loss : 0.042137, loss_ce: 0.011056
2022-01-11 23:54:01,918 iteration 3566 : loss : 0.029754, loss_ce: 0.011835
2022-01-11 23:54:03,549 iteration 3567 : loss : 0.040463, loss_ce: 0.021541
2022-01-11 23:54:05,113 iteration 3568 : loss : 0.039452, loss_ce: 0.013611
2022-01-11 23:54:06,634 iteration 3569 : loss : 0.032800, loss_ce: 0.013660
2022-01-11 23:54:06,635 Training Data Eval:
2022-01-11 23:54:14,672   Average segmentation loss on training set: 0.0224
2022-01-11 23:54:14,672 Validation Data Eval:
2022-01-11 23:54:17,433   Average segmentation loss on validation set: 0.0929
2022-01-11 23:54:19,040 iteration 3570 : loss : 0.031524, loss_ce: 0.009860
 52%|██████████████▏            | 210/400 [1:41:49<1:37:10, 30.69s/it]2022-01-11 23:54:20,682 iteration 3571 : loss : 0.031492, loss_ce: 0.016252
2022-01-11 23:54:22,193 iteration 3572 : loss : 0.024825, loss_ce: 0.009299
2022-01-11 23:54:23,762 iteration 3573 : loss : 0.039962, loss_ce: 0.015099
2022-01-11 23:54:25,334 iteration 3574 : loss : 0.033397, loss_ce: 0.010570
2022-01-11 23:54:26,861 iteration 3575 : loss : 0.032921, loss_ce: 0.011730
2022-01-11 23:54:28,483 iteration 3576 : loss : 0.030877, loss_ce: 0.008596
2022-01-11 23:54:30,075 iteration 3577 : loss : 0.030370, loss_ce: 0.011245
2022-01-11 23:54:31,560 iteration 3578 : loss : 0.026412, loss_ce: 0.010112
2022-01-11 23:54:33,095 iteration 3579 : loss : 0.031273, loss_ce: 0.012592
2022-01-11 23:54:34,623 iteration 3580 : loss : 0.038302, loss_ce: 0.016216
2022-01-11 23:54:36,138 iteration 3581 : loss : 0.042379, loss_ce: 0.023922
2022-01-11 23:54:37,693 iteration 3582 : loss : 0.029466, loss_ce: 0.010641
2022-01-11 23:54:39,294 iteration 3583 : loss : 0.039719, loss_ce: 0.013190
2022-01-11 23:54:40,878 iteration 3584 : loss : 0.040650, loss_ce: 0.014939
2022-01-11 23:54:42,395 iteration 3585 : loss : 0.041753, loss_ce: 0.017841
2022-01-11 23:54:44,015 iteration 3586 : loss : 0.027870, loss_ce: 0.012499
2022-01-11 23:54:45,566 iteration 3587 : loss : 0.028184, loss_ce: 0.011863
 53%|██████████████▏            | 211/400 [1:42:16<1:32:44, 29.44s/it]2022-01-11 23:54:47,231 iteration 3588 : loss : 0.021240, loss_ce: 0.008452
2022-01-11 23:54:48,720 iteration 3589 : loss : 0.030849, loss_ce: 0.011533
2022-01-11 23:54:50,255 iteration 3590 : loss : 0.031637, loss_ce: 0.012697
2022-01-11 23:54:51,822 iteration 3591 : loss : 0.029391, loss_ce: 0.010282
2022-01-11 23:54:53,538 iteration 3592 : loss : 0.038376, loss_ce: 0.015593
2022-01-11 23:54:55,077 iteration 3593 : loss : 0.053624, loss_ce: 0.022127
2022-01-11 23:54:56,629 iteration 3594 : loss : 0.033314, loss_ce: 0.012281
2022-01-11 23:54:58,215 iteration 3595 : loss : 0.052092, loss_ce: 0.020858
2022-01-11 23:54:59,762 iteration 3596 : loss : 0.027823, loss_ce: 0.010902
2022-01-11 23:55:01,352 iteration 3597 : loss : 0.045604, loss_ce: 0.020976
2022-01-11 23:55:02,860 iteration 3598 : loss : 0.022119, loss_ce: 0.009112
2022-01-11 23:55:04,469 iteration 3599 : loss : 0.027980, loss_ce: 0.009008
2022-01-11 23:55:05,933 iteration 3600 : loss : 0.027436, loss_ce: 0.010469
2022-01-11 23:55:07,499 iteration 3601 : loss : 0.035219, loss_ce: 0.017739
2022-01-11 23:55:09,011 iteration 3602 : loss : 0.024996, loss_ce: 0.008445
2022-01-11 23:55:10,593 iteration 3603 : loss : 0.048691, loss_ce: 0.024005
2022-01-11 23:55:12,080 iteration 3604 : loss : 0.026389, loss_ce: 0.011590
 53%|██████████████▎            | 212/400 [1:42:42<1:29:29, 28.56s/it]2022-01-11 23:55:13,687 iteration 3605 : loss : 0.027539, loss_ce: 0.010940
2022-01-11 23:55:15,222 iteration 3606 : loss : 0.029573, loss_ce: 0.014584
2022-01-11 23:55:16,807 iteration 3607 : loss : 0.050127, loss_ce: 0.018526
2022-01-11 23:55:18,359 iteration 3608 : loss : 0.023446, loss_ce: 0.008820
2022-01-11 23:55:19,891 iteration 3609 : loss : 0.027889, loss_ce: 0.013891
2022-01-11 23:55:21,453 iteration 3610 : loss : 0.032347, loss_ce: 0.012767
2022-01-11 23:55:22,954 iteration 3611 : loss : 0.039508, loss_ce: 0.010970
2022-01-11 23:55:24,462 iteration 3612 : loss : 0.029637, loss_ce: 0.012546
2022-01-11 23:55:26,118 iteration 3613 : loss : 0.030804, loss_ce: 0.013197
2022-01-11 23:55:27,627 iteration 3614 : loss : 0.024241, loss_ce: 0.010386
2022-01-11 23:55:29,093 iteration 3615 : loss : 0.027637, loss_ce: 0.010373
2022-01-11 23:55:30,675 iteration 3616 : loss : 0.042806, loss_ce: 0.015643
2022-01-11 23:55:32,270 iteration 3617 : loss : 0.043251, loss_ce: 0.015051
2022-01-11 23:55:33,841 iteration 3618 : loss : 0.027456, loss_ce: 0.011794
2022-01-11 23:55:35,397 iteration 3619 : loss : 0.052664, loss_ce: 0.015389
2022-01-11 23:55:36,905 iteration 3620 : loss : 0.033898, loss_ce: 0.010542
2022-01-11 23:55:38,483 iteration 3621 : loss : 0.031725, loss_ce: 0.011397
 53%|██████████████▍            | 213/400 [1:43:09<1:26:59, 27.91s/it]2022-01-11 23:55:40,095 iteration 3622 : loss : 0.044270, loss_ce: 0.024204
2022-01-11 23:55:41,709 iteration 3623 : loss : 0.033579, loss_ce: 0.007721
2022-01-11 23:55:43,261 iteration 3624 : loss : 0.022195, loss_ce: 0.009530
2022-01-11 23:55:44,805 iteration 3625 : loss : 0.031010, loss_ce: 0.013386
2022-01-11 23:55:46,325 iteration 3626 : loss : 0.030351, loss_ce: 0.015266
2022-01-11 23:55:47,833 iteration 3627 : loss : 0.023753, loss_ce: 0.009080
2022-01-11 23:55:49,442 iteration 3628 : loss : 0.060773, loss_ce: 0.016736
2022-01-11 23:55:51,137 iteration 3629 : loss : 0.031057, loss_ce: 0.013806
2022-01-11 23:55:52,602 iteration 3630 : loss : 0.031834, loss_ce: 0.012799
2022-01-11 23:55:54,170 iteration 3631 : loss : 0.033736, loss_ce: 0.011299
2022-01-11 23:55:55,849 iteration 3632 : loss : 0.044076, loss_ce: 0.014261
2022-01-11 23:55:57,397 iteration 3633 : loss : 0.027640, loss_ce: 0.010529
2022-01-11 23:55:58,936 iteration 3634 : loss : 0.027950, loss_ce: 0.011841
2022-01-11 23:56:00,584 iteration 3635 : loss : 0.056682, loss_ce: 0.030776
2022-01-11 23:56:02,190 iteration 3636 : loss : 0.035693, loss_ce: 0.009495
2022-01-11 23:56:03,732 iteration 3637 : loss : 0.039560, loss_ce: 0.013116
2022-01-11 23:56:05,285 iteration 3638 : loss : 0.028218, loss_ce: 0.012418
 54%|██████████████▍            | 214/400 [1:43:36<1:25:29, 27.58s/it]2022-01-11 23:56:06,838 iteration 3639 : loss : 0.026439, loss_ce: 0.010520
2022-01-11 23:56:08,347 iteration 3640 : loss : 0.027428, loss_ce: 0.010468
2022-01-11 23:56:09,854 iteration 3641 : loss : 0.025129, loss_ce: 0.008914
2022-01-11 23:56:11,425 iteration 3642 : loss : 0.036303, loss_ce: 0.010496
2022-01-11 23:56:12,933 iteration 3643 : loss : 0.023140, loss_ce: 0.009791
2022-01-11 23:56:14,462 iteration 3644 : loss : 0.018686, loss_ce: 0.007163
2022-01-11 23:56:16,022 iteration 3645 : loss : 0.030691, loss_ce: 0.010583
2022-01-11 23:56:17,591 iteration 3646 : loss : 0.032482, loss_ce: 0.012682
2022-01-11 23:56:19,149 iteration 3647 : loss : 0.035851, loss_ce: 0.009667
2022-01-11 23:56:20,739 iteration 3648 : loss : 0.048349, loss_ce: 0.023361
2022-01-11 23:56:22,287 iteration 3649 : loss : 0.023862, loss_ce: 0.006973
2022-01-11 23:56:23,853 iteration 3650 : loss : 0.031091, loss_ce: 0.016697
2022-01-11 23:56:25,430 iteration 3651 : loss : 0.040645, loss_ce: 0.013130
2022-01-11 23:56:27,107 iteration 3652 : loss : 0.045006, loss_ce: 0.012547
2022-01-11 23:56:28,631 iteration 3653 : loss : 0.035757, loss_ce: 0.021638
2022-01-11 23:56:30,133 iteration 3654 : loss : 0.024588, loss_ce: 0.012716
2022-01-11 23:56:30,133 Training Data Eval:
2022-01-11 23:56:38,165   Average segmentation loss on training set: 0.0202
2022-01-11 23:56:38,165 Validation Data Eval:
2022-01-11 23:56:40,935   Average segmentation loss on validation set: 0.0643
2022-01-11 23:56:42,524 iteration 3655 : loss : 0.023782, loss_ce: 0.010096
 54%|██████████████▌            | 215/400 [1:44:13<1:33:58, 30.48s/it]2022-01-11 23:56:44,167 iteration 3656 : loss : 0.041635, loss_ce: 0.013814
2022-01-11 23:56:45,693 iteration 3657 : loss : 0.022985, loss_ce: 0.011915
2022-01-11 23:56:47,313 iteration 3658 : loss : 0.023235, loss_ce: 0.010523
2022-01-11 23:56:48,979 iteration 3659 : loss : 0.027759, loss_ce: 0.009693
2022-01-11 23:56:50,548 iteration 3660 : loss : 0.034197, loss_ce: 0.008993
2022-01-11 23:56:52,062 iteration 3661 : loss : 0.024596, loss_ce: 0.011221
2022-01-11 23:56:53,636 iteration 3662 : loss : 0.025655, loss_ce: 0.009821
2022-01-11 23:56:55,175 iteration 3663 : loss : 0.033501, loss_ce: 0.011762
2022-01-11 23:56:56,767 iteration 3664 : loss : 0.023095, loss_ce: 0.009996
2022-01-11 23:56:58,330 iteration 3665 : loss : 0.032371, loss_ce: 0.017164
2022-01-11 23:56:59,879 iteration 3666 : loss : 0.032069, loss_ce: 0.012949
2022-01-11 23:57:01,416 iteration 3667 : loss : 0.044442, loss_ce: 0.021495
2022-01-11 23:57:02,907 iteration 3668 : loss : 0.023695, loss_ce: 0.005249
2022-01-11 23:57:04,522 iteration 3669 : loss : 0.030327, loss_ce: 0.013225
2022-01-11 23:57:06,033 iteration 3670 : loss : 0.022048, loss_ce: 0.008811
2022-01-11 23:57:07,577 iteration 3671 : loss : 0.023741, loss_ce: 0.012052
2022-01-11 23:57:09,272 iteration 3672 : loss : 0.053137, loss_ce: 0.015407
 54%|██████████████▌            | 216/400 [1:44:40<1:30:02, 29.36s/it]2022-01-11 23:57:10,883 iteration 3673 : loss : 0.024215, loss_ce: 0.009979
2022-01-11 23:57:12,498 iteration 3674 : loss : 0.036739, loss_ce: 0.011530
2022-01-11 23:57:14,136 iteration 3675 : loss : 0.041332, loss_ce: 0.014102
2022-01-11 23:57:15,619 iteration 3676 : loss : 0.019282, loss_ce: 0.007919
2022-01-11 23:57:17,199 iteration 3677 : loss : 0.033022, loss_ce: 0.010377
2022-01-11 23:57:18,825 iteration 3678 : loss : 0.048993, loss_ce: 0.018197
2022-01-11 23:57:20,361 iteration 3679 : loss : 0.027565, loss_ce: 0.010662
2022-01-11 23:57:21,924 iteration 3680 : loss : 0.041157, loss_ce: 0.013672
2022-01-11 23:57:23,479 iteration 3681 : loss : 0.023830, loss_ce: 0.010273
2022-01-11 23:57:24,963 iteration 3682 : loss : 0.025240, loss_ce: 0.009533
2022-01-11 23:57:26,441 iteration 3683 : loss : 0.021490, loss_ce: 0.007126
2022-01-11 23:57:28,061 iteration 3684 : loss : 0.032706, loss_ce: 0.014378
2022-01-11 23:57:29,590 iteration 3685 : loss : 0.026739, loss_ce: 0.009073
2022-01-11 23:57:31,172 iteration 3686 : loss : 0.034449, loss_ce: 0.017689
2022-01-11 23:57:32,696 iteration 3687 : loss : 0.025196, loss_ce: 0.009882
2022-01-11 23:57:34,243 iteration 3688 : loss : 0.023366, loss_ce: 0.011866
2022-01-11 23:57:35,811 iteration 3689 : loss : 0.027676, loss_ce: 0.008932
 54%|██████████████▋            | 217/400 [1:45:06<1:26:57, 28.51s/it]2022-01-11 23:57:37,402 iteration 3690 : loss : 0.033930, loss_ce: 0.009649
2022-01-11 23:57:38,997 iteration 3691 : loss : 0.053767, loss_ce: 0.013817
2022-01-11 23:57:40,573 iteration 3692 : loss : 0.023768, loss_ce: 0.011566
2022-01-11 23:57:42,075 iteration 3693 : loss : 0.023183, loss_ce: 0.010312
2022-01-11 23:57:43,644 iteration 3694 : loss : 0.045184, loss_ce: 0.021406
2022-01-11 23:57:45,220 iteration 3695 : loss : 0.029491, loss_ce: 0.015814
2022-01-11 23:57:46,768 iteration 3696 : loss : 0.039232, loss_ce: 0.012452
2022-01-11 23:57:48,368 iteration 3697 : loss : 0.039750, loss_ce: 0.012750
2022-01-11 23:57:50,012 iteration 3698 : loss : 0.044798, loss_ce: 0.017194
2022-01-11 23:57:51,522 iteration 3699 : loss : 0.030367, loss_ce: 0.015039
2022-01-11 23:57:53,093 iteration 3700 : loss : 0.029805, loss_ce: 0.009977
2022-01-11 23:57:54,570 iteration 3701 : loss : 0.022776, loss_ce: 0.010420
2022-01-11 23:57:56,150 iteration 3702 : loss : 0.028571, loss_ce: 0.013234
2022-01-11 23:57:57,722 iteration 3703 : loss : 0.027910, loss_ce: 0.013448
2022-01-11 23:57:59,188 iteration 3704 : loss : 0.027440, loss_ce: 0.010875
2022-01-11 23:58:00,809 iteration 3705 : loss : 0.039650, loss_ce: 0.014333
2022-01-11 23:58:02,292 iteration 3706 : loss : 0.021404, loss_ce: 0.006683
 55%|██████████████▋            | 218/400 [1:45:33<1:24:38, 27.90s/it]2022-01-11 23:58:03,919 iteration 3707 : loss : 0.047684, loss_ce: 0.022486
2022-01-11 23:58:05,416 iteration 3708 : loss : 0.027833, loss_ce: 0.009859
2022-01-11 23:58:07,070 iteration 3709 : loss : 0.046273, loss_ce: 0.014185
2022-01-11 23:58:08,623 iteration 3710 : loss : 0.049271, loss_ce: 0.017307
2022-01-11 23:58:10,144 iteration 3711 : loss : 0.023005, loss_ce: 0.007968
2022-01-11 23:58:11,655 iteration 3712 : loss : 0.022647, loss_ce: 0.010775
2022-01-11 23:58:13,296 iteration 3713 : loss : 0.036144, loss_ce: 0.013060
2022-01-11 23:58:14,851 iteration 3714 : loss : 0.027115, loss_ce: 0.009982
2022-01-11 23:58:16,392 iteration 3715 : loss : 0.035548, loss_ce: 0.017125
2022-01-11 23:58:17,954 iteration 3716 : loss : 0.037303, loss_ce: 0.018026
2022-01-11 23:58:19,549 iteration 3717 : loss : 0.020927, loss_ce: 0.008565
2022-01-11 23:58:21,085 iteration 3718 : loss : 0.023026, loss_ce: 0.008717
2022-01-11 23:58:22,659 iteration 3719 : loss : 0.032909, loss_ce: 0.012461
2022-01-11 23:58:24,295 iteration 3720 : loss : 0.037193, loss_ce: 0.011042
2022-01-11 23:58:25,830 iteration 3721 : loss : 0.030997, loss_ce: 0.012419
2022-01-11 23:58:27,371 iteration 3722 : loss : 0.039783, loss_ce: 0.018500
2022-01-11 23:58:28,979 iteration 3723 : loss : 0.084129, loss_ce: 0.021829
 55%|██████████████▊            | 219/400 [1:45:59<1:23:04, 27.54s/it]2022-01-11 23:58:30,611 iteration 3724 : loss : 0.031260, loss_ce: 0.012729
2022-01-11 23:58:32,153 iteration 3725 : loss : 0.072795, loss_ce: 0.038368
2022-01-11 23:58:33,828 iteration 3726 : loss : 0.022148, loss_ce: 0.008006
2022-01-11 23:58:35,347 iteration 3727 : loss : 0.021359, loss_ce: 0.007142
2022-01-11 23:58:36,907 iteration 3728 : loss : 0.035343, loss_ce: 0.018875
2022-01-11 23:58:38,525 iteration 3729 : loss : 0.035585, loss_ce: 0.012416
2022-01-11 23:58:40,068 iteration 3730 : loss : 0.038294, loss_ce: 0.012006
2022-01-11 23:58:41,578 iteration 3731 : loss : 0.037613, loss_ce: 0.012830
2022-01-11 23:58:43,143 iteration 3732 : loss : 0.032306, loss_ce: 0.013502
2022-01-11 23:58:44,676 iteration 3733 : loss : 0.038677, loss_ce: 0.012141
2022-01-11 23:58:46,256 iteration 3734 : loss : 0.039964, loss_ce: 0.014787
2022-01-11 23:58:47,852 iteration 3735 : loss : 0.039684, loss_ce: 0.021843
2022-01-11 23:58:49,462 iteration 3736 : loss : 0.047497, loss_ce: 0.013354
2022-01-11 23:58:51,027 iteration 3737 : loss : 0.032303, loss_ce: 0.012768
2022-01-11 23:58:52,590 iteration 3738 : loss : 0.030609, loss_ce: 0.012809
2022-01-11 23:58:54,152 iteration 3739 : loss : 0.047151, loss_ce: 0.018927
2022-01-11 23:58:54,152 Training Data Eval:
2022-01-11 23:59:02,170   Average segmentation loss on training set: 0.0192
2022-01-11 23:59:02,170 Validation Data Eval:
2022-01-11 23:59:04,934   Average segmentation loss on validation set: 0.0903
2022-01-11 23:59:06,555 iteration 3740 : loss : 0.028699, loss_ce: 0.009984
 55%|██████████████▊            | 220/400 [1:46:37<1:31:39, 30.55s/it]2022-01-11 23:59:08,189 iteration 3741 : loss : 0.024742, loss_ce: 0.010207
2022-01-11 23:59:09,727 iteration 3742 : loss : 0.028889, loss_ce: 0.012883
2022-01-11 23:59:11,293 iteration 3743 : loss : 0.039420, loss_ce: 0.014730
2022-01-11 23:59:12,870 iteration 3744 : loss : 0.024108, loss_ce: 0.010995
2022-01-11 23:59:14,397 iteration 3745 : loss : 0.023049, loss_ce: 0.008972
2022-01-11 23:59:16,011 iteration 3746 : loss : 0.021295, loss_ce: 0.007908
2022-01-11 23:59:17,618 iteration 3747 : loss : 0.061077, loss_ce: 0.007957
2022-01-11 23:59:19,191 iteration 3748 : loss : 0.046635, loss_ce: 0.023526
2022-01-11 23:59:20,752 iteration 3749 : loss : 0.039633, loss_ce: 0.019651
2022-01-11 23:59:22,363 iteration 3750 : loss : 0.038088, loss_ce: 0.014824
2022-01-11 23:59:23,950 iteration 3751 : loss : 0.050618, loss_ce: 0.013536
2022-01-11 23:59:25,513 iteration 3752 : loss : 0.024150, loss_ce: 0.009913
2022-01-11 23:59:27,028 iteration 3753 : loss : 0.034870, loss_ce: 0.009132
2022-01-11 23:59:28,536 iteration 3754 : loss : 0.020353, loss_ce: 0.007159
2022-01-11 23:59:30,072 iteration 3755 : loss : 0.028664, loss_ce: 0.012829
2022-01-11 23:59:31,632 iteration 3756 : loss : 0.029777, loss_ce: 0.010869
2022-01-11 23:59:33,200 iteration 3757 : loss : 0.032042, loss_ce: 0.011344
 55%|██████████████▉            | 221/400 [1:47:03<1:27:38, 29.38s/it]2022-01-11 23:59:34,838 iteration 3758 : loss : 0.035320, loss_ce: 0.016549
2022-01-11 23:59:36,401 iteration 3759 : loss : 0.033964, loss_ce: 0.012474
2022-01-11 23:59:37,880 iteration 3760 : loss : 0.024566, loss_ce: 0.008955
2022-01-11 23:59:39,491 iteration 3761 : loss : 0.035698, loss_ce: 0.016150
2022-01-11 23:59:41,050 iteration 3762 : loss : 0.041341, loss_ce: 0.012906
2022-01-11 23:59:42,661 iteration 3763 : loss : 0.047809, loss_ce: 0.019101
2022-01-11 23:59:44,271 iteration 3764 : loss : 0.032065, loss_ce: 0.012743
2022-01-11 23:59:45,839 iteration 3765 : loss : 0.028473, loss_ce: 0.012556
2022-01-11 23:59:47,376 iteration 3766 : loss : 0.030453, loss_ce: 0.011514
2022-01-11 23:59:48,999 iteration 3767 : loss : 0.027451, loss_ce: 0.008483
2022-01-11 23:59:50,584 iteration 3768 : loss : 0.026969, loss_ce: 0.013377
2022-01-11 23:59:52,114 iteration 3769 : loss : 0.023142, loss_ce: 0.009583
2022-01-11 23:59:53,631 iteration 3770 : loss : 0.025860, loss_ce: 0.010285
2022-01-11 23:59:55,157 iteration 3771 : loss : 0.024483, loss_ce: 0.009555
2022-01-11 23:59:56,711 iteration 3772 : loss : 0.025129, loss_ce: 0.007841
2022-01-11 23:59:58,209 iteration 3773 : loss : 0.022229, loss_ce: 0.008586
2022-01-11 23:59:59,718 iteration 3774 : loss : 0.020378, loss_ce: 0.008664
 56%|██████████████▉            | 222/400 [1:47:30<1:24:36, 28.52s/it]2022-01-12 00:00:01,290 iteration 3775 : loss : 0.032888, loss_ce: 0.008645
2022-01-12 00:00:02,898 iteration 3776 : loss : 0.028415, loss_ce: 0.012508
2022-01-12 00:00:04,575 iteration 3777 : loss : 0.027705, loss_ce: 0.009442
2022-01-12 00:00:06,109 iteration 3778 : loss : 0.028918, loss_ce: 0.014873
2022-01-12 00:00:07,700 iteration 3779 : loss : 0.023942, loss_ce: 0.009202
2022-01-12 00:00:09,242 iteration 3780 : loss : 0.024157, loss_ce: 0.011169
2022-01-12 00:00:10,801 iteration 3781 : loss : 0.030550, loss_ce: 0.011808
2022-01-12 00:00:12,383 iteration 3782 : loss : 0.038380, loss_ce: 0.012812
2022-01-12 00:00:13,992 iteration 3783 : loss : 0.023076, loss_ce: 0.007615
2022-01-12 00:00:15,616 iteration 3784 : loss : 0.034977, loss_ce: 0.015898
2022-01-12 00:00:17,151 iteration 3785 : loss : 0.025677, loss_ce: 0.009797
2022-01-12 00:00:18,725 iteration 3786 : loss : 0.027621, loss_ce: 0.008765
2022-01-12 00:00:20,346 iteration 3787 : loss : 0.034475, loss_ce: 0.014969
2022-01-12 00:00:21,965 iteration 3788 : loss : 0.053097, loss_ce: 0.022898
2022-01-12 00:00:23,489 iteration 3789 : loss : 0.021801, loss_ce: 0.006285
2022-01-12 00:00:25,063 iteration 3790 : loss : 0.023629, loss_ce: 0.009258
2022-01-12 00:00:26,541 iteration 3791 : loss : 0.023116, loss_ce: 0.009394
 56%|███████████████            | 223/400 [1:47:57<1:22:37, 28.01s/it]2022-01-12 00:00:28,137 iteration 3792 : loss : 0.022938, loss_ce: 0.009968
2022-01-12 00:00:29,709 iteration 3793 : loss : 0.021526, loss_ce: 0.008985
2022-01-12 00:00:31,231 iteration 3794 : loss : 0.022257, loss_ce: 0.008693
2022-01-12 00:00:32,900 iteration 3795 : loss : 0.043545, loss_ce: 0.019569
2022-01-12 00:00:34,476 iteration 3796 : loss : 0.039140, loss_ce: 0.009555
2022-01-12 00:00:36,015 iteration 3797 : loss : 0.027851, loss_ce: 0.009619
2022-01-12 00:00:37,667 iteration 3798 : loss : 0.033391, loss_ce: 0.014542
2022-01-12 00:00:39,191 iteration 3799 : loss : 0.023557, loss_ce: 0.013143
2022-01-12 00:00:40,801 iteration 3800 : loss : 0.021612, loss_ce: 0.007551
2022-01-12 00:00:42,320 iteration 3801 : loss : 0.018386, loss_ce: 0.007096
2022-01-12 00:00:43,892 iteration 3802 : loss : 0.029343, loss_ce: 0.009515
2022-01-12 00:00:45,449 iteration 3803 : loss : 0.023948, loss_ce: 0.008809
2022-01-12 00:00:46,921 iteration 3804 : loss : 0.023560, loss_ce: 0.009332
2022-01-12 00:00:48,522 iteration 3805 : loss : 0.031202, loss_ce: 0.015425
2022-01-12 00:00:50,054 iteration 3806 : loss : 0.030369, loss_ce: 0.008686
2022-01-12 00:00:51,600 iteration 3807 : loss : 0.028396, loss_ce: 0.012199
2022-01-12 00:00:53,187 iteration 3808 : loss : 0.023559, loss_ce: 0.009230
 56%|███████████████            | 224/400 [1:48:23<1:20:58, 27.60s/it]2022-01-12 00:00:54,793 iteration 3809 : loss : 0.022111, loss_ce: 0.009647
2022-01-12 00:00:56,345 iteration 3810 : loss : 0.024133, loss_ce: 0.009801
2022-01-12 00:00:58,021 iteration 3811 : loss : 0.028506, loss_ce: 0.009686
2022-01-12 00:00:59,564 iteration 3812 : loss : 0.032520, loss_ce: 0.012330
2022-01-12 00:01:01,140 iteration 3813 : loss : 0.036366, loss_ce: 0.011443
2022-01-12 00:01:02,628 iteration 3814 : loss : 0.021836, loss_ce: 0.007723
2022-01-12 00:01:04,246 iteration 3815 : loss : 0.048739, loss_ce: 0.018582
2022-01-12 00:01:05,917 iteration 3816 : loss : 0.033653, loss_ce: 0.012925
2022-01-12 00:01:07,494 iteration 3817 : loss : 0.021921, loss_ce: 0.009706
2022-01-12 00:01:09,167 iteration 3818 : loss : 0.032265, loss_ce: 0.014089
2022-01-12 00:01:10,687 iteration 3819 : loss : 0.023675, loss_ce: 0.010803
2022-01-12 00:01:12,298 iteration 3820 : loss : 0.038101, loss_ce: 0.015241
2022-01-12 00:01:13,860 iteration 3821 : loss : 0.023531, loss_ce: 0.011332
2022-01-12 00:01:15,331 iteration 3822 : loss : 0.020144, loss_ce: 0.008880
2022-01-12 00:01:16,851 iteration 3823 : loss : 0.025949, loss_ce: 0.009381
2022-01-12 00:01:18,402 iteration 3824 : loss : 0.037972, loss_ce: 0.020146
2022-01-12 00:01:18,402 Training Data Eval:
2022-01-12 00:01:26,437   Average segmentation loss on training set: 0.0182
2022-01-12 00:01:26,437 Validation Data Eval:
2022-01-12 00:01:29,208   Average segmentation loss on validation set: 0.0637
2022-01-12 00:01:30,738 iteration 3825 : loss : 0.027387, loss_ce: 0.010134
 56%|███████████████▏           | 225/400 [1:49:01<1:29:12, 30.59s/it]2022-01-12 00:01:32,263 iteration 3826 : loss : 0.026401, loss_ce: 0.011020
2022-01-12 00:01:33,842 iteration 3827 : loss : 0.021708, loss_ce: 0.008761
2022-01-12 00:01:35,445 iteration 3828 : loss : 0.029719, loss_ce: 0.012874
2022-01-12 00:01:37,012 iteration 3829 : loss : 0.036601, loss_ce: 0.016073
2022-01-12 00:01:38,587 iteration 3830 : loss : 0.037687, loss_ce: 0.015866
2022-01-12 00:01:40,218 iteration 3831 : loss : 0.036320, loss_ce: 0.009545
2022-01-12 00:01:41,709 iteration 3832 : loss : 0.019005, loss_ce: 0.007289
2022-01-12 00:01:43,298 iteration 3833 : loss : 0.026601, loss_ce: 0.009481
2022-01-12 00:01:44,844 iteration 3834 : loss : 0.021820, loss_ce: 0.010403
2022-01-12 00:01:46,459 iteration 3835 : loss : 0.025639, loss_ce: 0.012512
2022-01-12 00:01:48,088 iteration 3836 : loss : 0.024218, loss_ce: 0.008555
2022-01-12 00:01:49,589 iteration 3837 : loss : 0.028143, loss_ce: 0.010424
2022-01-12 00:01:51,106 iteration 3838 : loss : 0.023192, loss_ce: 0.007639
2022-01-12 00:01:52,635 iteration 3839 : loss : 0.026569, loss_ce: 0.010248
2022-01-12 00:01:54,117 iteration 3840 : loss : 0.027505, loss_ce: 0.007854
2022-01-12 00:01:55,649 iteration 3841 : loss : 0.022172, loss_ce: 0.007186
2022-01-12 00:01:57,193 iteration 3842 : loss : 0.025563, loss_ce: 0.005886
 56%|███████████████▎           | 226/400 [1:49:27<1:25:06, 29.35s/it]2022-01-12 00:01:58,828 iteration 3843 : loss : 0.023145, loss_ce: 0.009984
2022-01-12 00:02:00,401 iteration 3844 : loss : 0.033676, loss_ce: 0.016698
2022-01-12 00:02:01,935 iteration 3845 : loss : 0.032557, loss_ce: 0.011707
2022-01-12 00:02:03,488 iteration 3846 : loss : 0.019262, loss_ce: 0.005400
2022-01-12 00:02:05,091 iteration 3847 : loss : 0.024802, loss_ce: 0.009246
2022-01-12 00:02:06,637 iteration 3848 : loss : 0.019210, loss_ce: 0.007983
2022-01-12 00:02:08,130 iteration 3849 : loss : 0.020290, loss_ce: 0.008288
2022-01-12 00:02:09,724 iteration 3850 : loss : 0.051631, loss_ce: 0.012518
2022-01-12 00:02:11,336 iteration 3851 : loss : 0.028088, loss_ce: 0.015749
2022-01-12 00:02:12,879 iteration 3852 : loss : 0.042254, loss_ce: 0.015958
2022-01-12 00:02:14,406 iteration 3853 : loss : 0.022918, loss_ce: 0.011945
2022-01-12 00:02:16,010 iteration 3854 : loss : 0.037617, loss_ce: 0.016234
2022-01-12 00:02:17,574 iteration 3855 : loss : 0.024914, loss_ce: 0.010015
2022-01-12 00:02:19,149 iteration 3856 : loss : 0.024883, loss_ce: 0.008770
2022-01-12 00:02:20,682 iteration 3857 : loss : 0.018510, loss_ce: 0.007737
2022-01-12 00:02:22,266 iteration 3858 : loss : 0.025098, loss_ce: 0.009236
2022-01-12 00:02:23,837 iteration 3859 : loss : 0.035979, loss_ce: 0.008452
 57%|███████████████▎           | 227/400 [1:49:54<1:22:16, 28.54s/it]2022-01-12 00:02:25,452 iteration 3860 : loss : 0.022432, loss_ce: 0.009626
2022-01-12 00:02:27,086 iteration 3861 : loss : 0.030725, loss_ce: 0.014687
2022-01-12 00:02:28,597 iteration 3862 : loss : 0.026486, loss_ce: 0.013512
2022-01-12 00:02:30,186 iteration 3863 : loss : 0.028910, loss_ce: 0.013820
2022-01-12 00:02:31,798 iteration 3864 : loss : 0.022942, loss_ce: 0.007898
2022-01-12 00:02:33,491 iteration 3865 : loss : 0.040268, loss_ce: 0.018034
2022-01-12 00:02:35,150 iteration 3866 : loss : 0.040946, loss_ce: 0.016472
2022-01-12 00:02:36,696 iteration 3867 : loss : 0.017476, loss_ce: 0.006731
2022-01-12 00:02:38,299 iteration 3868 : loss : 0.024334, loss_ce: 0.009907
2022-01-12 00:02:39,892 iteration 3869 : loss : 0.028139, loss_ce: 0.010847
2022-01-12 00:02:41,503 iteration 3870 : loss : 0.025472, loss_ce: 0.008691
2022-01-12 00:02:43,056 iteration 3871 : loss : 0.033419, loss_ce: 0.010926
2022-01-12 00:02:44,598 iteration 3872 : loss : 0.047970, loss_ce: 0.025024
2022-01-12 00:02:46,151 iteration 3873 : loss : 0.037831, loss_ce: 0.014723
2022-01-12 00:02:47,773 iteration 3874 : loss : 0.033926, loss_ce: 0.015577
2022-01-12 00:02:49,383 iteration 3875 : loss : 0.022125, loss_ce: 0.008490
2022-01-12 00:02:50,938 iteration 3876 : loss : 0.039125, loss_ce: 0.008979
 57%|███████████████▍           | 228/400 [1:50:21<1:20:34, 28.11s/it]2022-01-12 00:02:52,514 iteration 3877 : loss : 0.027509, loss_ce: 0.011665
2022-01-12 00:02:54,133 iteration 3878 : loss : 0.032184, loss_ce: 0.014836
2022-01-12 00:02:55,721 iteration 3879 : loss : 0.035005, loss_ce: 0.015536
2022-01-12 00:02:57,276 iteration 3880 : loss : 0.024954, loss_ce: 0.011386
2022-01-12 00:02:58,806 iteration 3881 : loss : 0.050543, loss_ce: 0.032235
2022-01-12 00:03:00,429 iteration 3882 : loss : 0.037311, loss_ce: 0.015025
2022-01-12 00:03:02,018 iteration 3883 : loss : 0.037581, loss_ce: 0.017748
2022-01-12 00:03:03,549 iteration 3884 : loss : 0.040689, loss_ce: 0.008855
2022-01-12 00:03:05,067 iteration 3885 : loss : 0.023177, loss_ce: 0.007840
2022-01-12 00:03:06,593 iteration 3886 : loss : 0.038986, loss_ce: 0.013329
2022-01-12 00:03:08,221 iteration 3887 : loss : 0.035937, loss_ce: 0.016088
2022-01-12 00:03:09,789 iteration 3888 : loss : 0.048324, loss_ce: 0.019769
2022-01-12 00:03:11,368 iteration 3889 : loss : 0.029484, loss_ce: 0.014796
2022-01-12 00:03:12,850 iteration 3890 : loss : 0.026488, loss_ce: 0.009403
2022-01-12 00:03:14,438 iteration 3891 : loss : 0.029466, loss_ce: 0.012787
2022-01-12 00:03:16,019 iteration 3892 : loss : 0.050279, loss_ce: 0.020361
2022-01-12 00:03:17,657 iteration 3893 : loss : 0.027233, loss_ce: 0.014370
 57%|███████████████▍           | 229/400 [1:50:48<1:18:54, 27.69s/it]2022-01-12 00:03:19,192 iteration 3894 : loss : 0.028014, loss_ce: 0.012301
2022-01-12 00:03:20,812 iteration 3895 : loss : 0.045656, loss_ce: 0.016424
2022-01-12 00:03:22,365 iteration 3896 : loss : 0.029707, loss_ce: 0.008547
2022-01-12 00:03:23,953 iteration 3897 : loss : 0.038599, loss_ce: 0.019247
2022-01-12 00:03:25,489 iteration 3898 : loss : 0.023099, loss_ce: 0.008564
2022-01-12 00:03:27,093 iteration 3899 : loss : 0.028273, loss_ce: 0.012533
2022-01-12 00:03:28,726 iteration 3900 : loss : 0.032863, loss_ce: 0.010581
2022-01-12 00:03:30,228 iteration 3901 : loss : 0.027711, loss_ce: 0.010009
2022-01-12 00:03:31,755 iteration 3902 : loss : 0.021716, loss_ce: 0.008186
2022-01-12 00:03:33,314 iteration 3903 : loss : 0.025100, loss_ce: 0.009678
2022-01-12 00:03:34,952 iteration 3904 : loss : 0.024223, loss_ce: 0.008849
2022-01-12 00:03:36,523 iteration 3905 : loss : 0.025904, loss_ce: 0.012394
2022-01-12 00:03:38,059 iteration 3906 : loss : 0.030405, loss_ce: 0.013795
2022-01-12 00:03:39,635 iteration 3907 : loss : 0.024197, loss_ce: 0.008545
2022-01-12 00:03:41,269 iteration 3908 : loss : 0.042091, loss_ce: 0.014962
2022-01-12 00:03:42,866 iteration 3909 : loss : 0.023370, loss_ce: 0.009968
2022-01-12 00:03:42,867 Training Data Eval:
2022-01-12 00:03:50,896   Average segmentation loss on training set: 0.0179
2022-01-12 00:03:50,896 Validation Data Eval:
2022-01-12 00:03:53,663   Average segmentation loss on validation set: 0.0708
2022-01-12 00:03:55,168 iteration 3910 : loss : 0.020706, loss_ce: 0.007615
 57%|███████████████▌           | 230/400 [1:51:25<1:26:48, 30.64s/it]2022-01-12 00:03:56,730 iteration 3911 : loss : 0.019514, loss_ce: 0.006951
2022-01-12 00:03:58,353 iteration 3912 : loss : 0.028316, loss_ce: 0.012211
2022-01-12 00:03:59,857 iteration 3913 : loss : 0.027525, loss_ce: 0.008757
2022-01-12 00:04:01,351 iteration 3914 : loss : 0.024774, loss_ce: 0.009580
2022-01-12 00:04:02,843 iteration 3915 : loss : 0.021633, loss_ce: 0.007718
2022-01-12 00:04:04,459 iteration 3916 : loss : 0.042369, loss_ce: 0.015321
2022-01-12 00:04:06,079 iteration 3917 : loss : 0.051531, loss_ce: 0.013334
2022-01-12 00:04:07,700 iteration 3918 : loss : 0.036922, loss_ce: 0.016956
2022-01-12 00:04:09,279 iteration 3919 : loss : 0.026938, loss_ce: 0.010083
2022-01-12 00:04:10,870 iteration 3920 : loss : 0.026744, loss_ce: 0.011355
2022-01-12 00:04:12,455 iteration 3921 : loss : 0.037937, loss_ce: 0.010317
2022-01-12 00:04:13,982 iteration 3922 : loss : 0.017382, loss_ce: 0.007169
2022-01-12 00:04:15,549 iteration 3923 : loss : 0.024106, loss_ce: 0.009946
2022-01-12 00:04:17,102 iteration 3924 : loss : 0.029295, loss_ce: 0.011400
2022-01-12 00:04:18,797 iteration 3925 : loss : 0.031225, loss_ce: 0.012901
2022-01-12 00:04:20,370 iteration 3926 : loss : 0.030079, loss_ce: 0.010294
2022-01-12 00:04:21,963 iteration 3927 : loss : 0.036682, loss_ce: 0.016952
 58%|███████████████▌           | 231/400 [1:51:52<1:23:02, 29.48s/it]2022-01-12 00:04:23,586 iteration 3928 : loss : 0.025315, loss_ce: 0.011036
2022-01-12 00:04:25,214 iteration 3929 : loss : 0.036580, loss_ce: 0.014062
2022-01-12 00:04:26,885 iteration 3930 : loss : 0.053827, loss_ce: 0.023362
2022-01-12 00:04:28,462 iteration 3931 : loss : 0.042236, loss_ce: 0.021602
2022-01-12 00:04:30,076 iteration 3932 : loss : 0.030445, loss_ce: 0.008540
2022-01-12 00:04:31,619 iteration 3933 : loss : 0.034381, loss_ce: 0.009166
2022-01-12 00:04:33,185 iteration 3934 : loss : 0.028929, loss_ce: 0.009468
2022-01-12 00:04:34,803 iteration 3935 : loss : 0.026964, loss_ce: 0.010236
2022-01-12 00:04:36,353 iteration 3936 : loss : 0.032214, loss_ce: 0.013946
2022-01-12 00:04:37,922 iteration 3937 : loss : 0.023319, loss_ce: 0.007831
2022-01-12 00:04:39,461 iteration 3938 : loss : 0.023694, loss_ce: 0.010717
2022-01-12 00:04:41,093 iteration 3939 : loss : 0.032538, loss_ce: 0.013455
2022-01-12 00:04:42,632 iteration 3940 : loss : 0.022239, loss_ce: 0.006382
2022-01-12 00:04:44,192 iteration 3941 : loss : 0.026156, loss_ce: 0.011953
2022-01-12 00:04:45,746 iteration 3942 : loss : 0.026745, loss_ce: 0.012119
2022-01-12 00:04:47,358 iteration 3943 : loss : 0.031512, loss_ce: 0.016099
2022-01-12 00:04:48,942 iteration 3944 : loss : 0.026366, loss_ce: 0.008850
 58%|███████████████▋           | 232/400 [1:52:19<1:20:26, 28.73s/it]2022-01-12 00:04:50,527 iteration 3945 : loss : 0.024188, loss_ce: 0.011931
2022-01-12 00:04:52,008 iteration 3946 : loss : 0.020512, loss_ce: 0.008738
2022-01-12 00:04:53,569 iteration 3947 : loss : 0.021476, loss_ce: 0.008035
2022-01-12 00:04:55,119 iteration 3948 : loss : 0.022479, loss_ce: 0.009704
2022-01-12 00:04:56,698 iteration 3949 : loss : 0.036224, loss_ce: 0.018489
2022-01-12 00:04:58,265 iteration 3950 : loss : 0.024661, loss_ce: 0.009790
2022-01-12 00:04:59,870 iteration 3951 : loss : 0.029961, loss_ce: 0.011893
2022-01-12 00:05:01,433 iteration 3952 : loss : 0.031068, loss_ce: 0.014572
2022-01-12 00:05:02,988 iteration 3953 : loss : 0.023444, loss_ce: 0.008010
2022-01-12 00:05:04,590 iteration 3954 : loss : 0.026688, loss_ce: 0.010785
2022-01-12 00:05:06,171 iteration 3955 : loss : 0.025007, loss_ce: 0.009257
2022-01-12 00:05:07,704 iteration 3956 : loss : 0.026913, loss_ce: 0.011002
2022-01-12 00:05:09,256 iteration 3957 : loss : 0.024650, loss_ce: 0.011527
2022-01-12 00:05:10,902 iteration 3958 : loss : 0.032781, loss_ce: 0.015489
2022-01-12 00:05:12,437 iteration 3959 : loss : 0.021358, loss_ce: 0.006816
2022-01-12 00:05:14,013 iteration 3960 : loss : 0.023998, loss_ce: 0.005925
2022-01-12 00:05:15,621 iteration 3961 : loss : 0.022249, loss_ce: 0.009143
 58%|███████████████▋           | 233/400 [1:52:46<1:18:15, 28.12s/it]2022-01-12 00:05:17,356 iteration 3962 : loss : 0.056918, loss_ce: 0.022097
2022-01-12 00:05:18,853 iteration 3963 : loss : 0.023645, loss_ce: 0.009928
2022-01-12 00:05:20,456 iteration 3964 : loss : 0.050259, loss_ce: 0.021939
2022-01-12 00:05:22,108 iteration 3965 : loss : 0.029438, loss_ce: 0.008331
2022-01-12 00:05:23,645 iteration 3966 : loss : 0.022366, loss_ce: 0.008107
2022-01-12 00:05:25,178 iteration 3967 : loss : 0.024466, loss_ce: 0.007369
2022-01-12 00:05:26,821 iteration 3968 : loss : 0.029924, loss_ce: 0.012332
2022-01-12 00:05:28,433 iteration 3969 : loss : 0.022432, loss_ce: 0.008893
2022-01-12 00:05:30,074 iteration 3970 : loss : 0.024562, loss_ce: 0.012280
2022-01-12 00:05:31,610 iteration 3971 : loss : 0.027535, loss_ce: 0.008796
2022-01-12 00:05:33,153 iteration 3972 : loss : 0.023770, loss_ce: 0.008767
2022-01-12 00:05:34,731 iteration 3973 : loss : 0.031323, loss_ce: 0.012242
2022-01-12 00:05:36,363 iteration 3974 : loss : 0.031318, loss_ce: 0.014378
2022-01-12 00:05:37,992 iteration 3975 : loss : 0.021799, loss_ce: 0.008105
2022-01-12 00:05:39,642 iteration 3976 : loss : 0.025221, loss_ce: 0.011598
2022-01-12 00:05:41,208 iteration 3977 : loss : 0.031747, loss_ce: 0.012029
2022-01-12 00:05:42,761 iteration 3978 : loss : 0.028959, loss_ce: 0.014175
 58%|███████████████▊           | 234/400 [1:53:13<1:16:58, 27.82s/it]2022-01-12 00:05:44,368 iteration 3979 : loss : 0.033656, loss_ce: 0.011736
2022-01-12 00:05:45,949 iteration 3980 : loss : 0.026231, loss_ce: 0.012694
2022-01-12 00:05:47,487 iteration 3981 : loss : 0.027454, loss_ce: 0.010428
2022-01-12 00:05:49,194 iteration 3982 : loss : 0.030347, loss_ce: 0.011655
2022-01-12 00:05:50,767 iteration 3983 : loss : 0.033715, loss_ce: 0.011718
2022-01-12 00:05:52,383 iteration 3984 : loss : 0.024942, loss_ce: 0.009165
2022-01-12 00:05:53,965 iteration 3985 : loss : 0.035034, loss_ce: 0.012849
2022-01-12 00:05:55,494 iteration 3986 : loss : 0.021146, loss_ce: 0.008469
2022-01-12 00:05:57,042 iteration 3987 : loss : 0.018149, loss_ce: 0.008173
2022-01-12 00:05:58,665 iteration 3988 : loss : 0.030122, loss_ce: 0.014950
2022-01-12 00:06:00,248 iteration 3989 : loss : 0.035386, loss_ce: 0.013699
2022-01-12 00:06:01,785 iteration 3990 : loss : 0.021125, loss_ce: 0.008540
2022-01-12 00:06:03,339 iteration 3991 : loss : 0.042741, loss_ce: 0.014845
2022-01-12 00:06:05,022 iteration 3992 : loss : 0.042784, loss_ce: 0.013373
2022-01-12 00:06:06,580 iteration 3993 : loss : 0.055066, loss_ce: 0.028262
2022-01-12 00:06:08,130 iteration 3994 : loss : 0.027560, loss_ce: 0.013101
2022-01-12 00:06:08,131 Training Data Eval:
2022-01-12 00:06:16,155   Average segmentation loss on training set: 0.0171
2022-01-12 00:06:16,155 Validation Data Eval:
2022-01-12 00:06:18,913   Average segmentation loss on validation set: 0.0695
2022-01-12 00:06:20,477 iteration 3995 : loss : 0.027553, loss_ce: 0.008329
 59%|███████████████▊           | 235/400 [1:53:51<1:24:40, 30.79s/it]2022-01-12 00:06:22,157 iteration 3996 : loss : 0.034348, loss_ce: 0.019133
2022-01-12 00:06:23,763 iteration 3997 : loss : 0.028153, loss_ce: 0.009617
2022-01-12 00:06:25,331 iteration 3998 : loss : 0.028820, loss_ce: 0.011442
2022-01-12 00:06:26,951 iteration 3999 : loss : 0.032584, loss_ce: 0.012505
2022-01-12 00:06:28,540 iteration 4000 : loss : 0.030187, loss_ce: 0.013392
2022-01-12 00:06:30,225 iteration 4001 : loss : 0.034338, loss_ce: 0.010883
2022-01-12 00:06:31,795 iteration 4002 : loss : 0.022072, loss_ce: 0.010541
2022-01-12 00:06:33,369 iteration 4003 : loss : 0.036383, loss_ce: 0.015185
2022-01-12 00:06:34,943 iteration 4004 : loss : 0.032718, loss_ce: 0.011592
2022-01-12 00:06:36,525 iteration 4005 : loss : 0.030527, loss_ce: 0.016153
2022-01-12 00:06:38,175 iteration 4006 : loss : 0.039911, loss_ce: 0.012033
2022-01-12 00:06:39,706 iteration 4007 : loss : 0.021661, loss_ce: 0.008629
2022-01-12 00:06:41,273 iteration 4008 : loss : 0.032221, loss_ce: 0.010414
2022-01-12 00:06:42,832 iteration 4009 : loss : 0.021038, loss_ce: 0.008352
2022-01-12 00:06:44,338 iteration 4010 : loss : 0.015182, loss_ce: 0.005875
2022-01-12 00:06:45,958 iteration 4011 : loss : 0.034855, loss_ce: 0.010391
2022-01-12 00:06:47,580 iteration 4012 : loss : 0.026460, loss_ce: 0.012237
 59%|███████████████▉           | 236/400 [1:54:18<1:21:08, 29.68s/it]2022-01-12 00:06:49,168 iteration 4013 : loss : 0.027887, loss_ce: 0.008461
2022-01-12 00:06:50,771 iteration 4014 : loss : 0.030108, loss_ce: 0.009149
2022-01-12 00:06:52,314 iteration 4015 : loss : 0.022179, loss_ce: 0.011294
2022-01-12 00:06:53,922 iteration 4016 : loss : 0.038669, loss_ce: 0.014780
2022-01-12 00:06:55,433 iteration 4017 : loss : 0.030098, loss_ce: 0.010957
2022-01-12 00:06:57,056 iteration 4018 : loss : 0.031842, loss_ce: 0.011043
2022-01-12 00:06:58,682 iteration 4019 : loss : 0.031622, loss_ce: 0.014775
2022-01-12 00:07:00,234 iteration 4020 : loss : 0.022676, loss_ce: 0.007845
2022-01-12 00:07:01,812 iteration 4021 : loss : 0.026667, loss_ce: 0.009658
2022-01-12 00:07:03,377 iteration 4022 : loss : 0.028609, loss_ce: 0.010525
2022-01-12 00:07:04,957 iteration 4023 : loss : 0.040070, loss_ce: 0.016480
2022-01-12 00:07:06,531 iteration 4024 : loss : 0.042137, loss_ce: 0.013637
2022-01-12 00:07:08,078 iteration 4025 : loss : 0.026595, loss_ce: 0.010580
2022-01-12 00:07:09,631 iteration 4026 : loss : 0.018048, loss_ce: 0.008023
2022-01-12 00:07:11,175 iteration 4027 : loss : 0.031443, loss_ce: 0.011785
2022-01-12 00:07:12,735 iteration 4028 : loss : 0.024141, loss_ce: 0.006861
2022-01-12 00:07:14,307 iteration 4029 : loss : 0.041074, loss_ce: 0.016087
 59%|███████████████▉           | 237/400 [1:54:45<1:18:13, 28.80s/it]2022-01-12 00:07:15,917 iteration 4030 : loss : 0.030722, loss_ce: 0.012155
2022-01-12 00:07:17,463 iteration 4031 : loss : 0.019561, loss_ce: 0.007453
2022-01-12 00:07:19,073 iteration 4032 : loss : 0.021082, loss_ce: 0.007524
2022-01-12 00:07:20,720 iteration 4033 : loss : 0.039743, loss_ce: 0.011773
2022-01-12 00:07:22,251 iteration 4034 : loss : 0.039553, loss_ce: 0.015915
2022-01-12 00:07:23,830 iteration 4035 : loss : 0.028703, loss_ce: 0.010977
2022-01-12 00:07:25,399 iteration 4036 : loss : 0.018117, loss_ce: 0.006590
2022-01-12 00:07:26,998 iteration 4037 : loss : 0.029798, loss_ce: 0.009246
2022-01-12 00:07:28,570 iteration 4038 : loss : 0.030990, loss_ce: 0.020938
2022-01-12 00:07:30,048 iteration 4039 : loss : 0.027853, loss_ce: 0.016685
2022-01-12 00:07:31,616 iteration 4040 : loss : 0.039998, loss_ce: 0.010150
2022-01-12 00:07:33,280 iteration 4041 : loss : 0.037373, loss_ce: 0.020676
2022-01-12 00:07:34,768 iteration 4042 : loss : 0.021557, loss_ce: 0.009526
2022-01-12 00:07:36,355 iteration 4043 : loss : 0.027547, loss_ce: 0.012590
2022-01-12 00:07:37,947 iteration 4044 : loss : 0.025544, loss_ce: 0.010399
2022-01-12 00:07:39,485 iteration 4045 : loss : 0.038045, loss_ce: 0.017404
2022-01-12 00:07:41,076 iteration 4046 : loss : 0.023530, loss_ce: 0.008229
 60%|████████████████           | 238/400 [1:55:11<1:16:06, 28.19s/it]2022-01-12 00:07:42,616 iteration 4047 : loss : 0.020030, loss_ce: 0.008953
2022-01-12 00:07:44,176 iteration 4048 : loss : 0.016497, loss_ce: 0.005269
2022-01-12 00:07:45,809 iteration 4049 : loss : 0.037010, loss_ce: 0.012921
2022-01-12 00:07:47,386 iteration 4050 : loss : 0.024587, loss_ce: 0.006706
2022-01-12 00:07:48,978 iteration 4051 : loss : 0.016751, loss_ce: 0.005734
2022-01-12 00:07:50,535 iteration 4052 : loss : 0.025805, loss_ce: 0.007687
2022-01-12 00:07:52,081 iteration 4053 : loss : 0.026942, loss_ce: 0.012897
2022-01-12 00:07:53,607 iteration 4054 : loss : 0.040157, loss_ce: 0.019315
2022-01-12 00:07:55,257 iteration 4055 : loss : 0.027394, loss_ce: 0.007805
2022-01-12 00:07:56,837 iteration 4056 : loss : 0.024753, loss_ce: 0.008443
2022-01-12 00:07:58,456 iteration 4057 : loss : 0.026914, loss_ce: 0.011653
2022-01-12 00:07:59,960 iteration 4058 : loss : 0.025224, loss_ce: 0.011270
2022-01-12 00:08:01,596 iteration 4059 : loss : 0.028954, loss_ce: 0.013470
2022-01-12 00:08:03,188 iteration 4060 : loss : 0.026184, loss_ce: 0.009443
2022-01-12 00:08:04,747 iteration 4061 : loss : 0.082787, loss_ce: 0.019079
2022-01-12 00:08:06,293 iteration 4062 : loss : 0.027275, loss_ce: 0.011988
2022-01-12 00:08:07,966 iteration 4063 : loss : 0.076246, loss_ce: 0.027029
 60%|████████████████▏          | 239/400 [1:55:38<1:14:35, 27.80s/it]2022-01-12 00:08:09,555 iteration 4064 : loss : 0.023925, loss_ce: 0.009373
2022-01-12 00:08:11,127 iteration 4065 : loss : 0.025212, loss_ce: 0.009272
2022-01-12 00:08:12,628 iteration 4066 : loss : 0.025711, loss_ce: 0.008680
2022-01-12 00:08:14,160 iteration 4067 : loss : 0.033557, loss_ce: 0.009217
2022-01-12 00:08:15,736 iteration 4068 : loss : 0.028645, loss_ce: 0.010188
2022-01-12 00:08:17,340 iteration 4069 : loss : 0.022866, loss_ce: 0.007808
2022-01-12 00:08:18,945 iteration 4070 : loss : 0.038324, loss_ce: 0.020768
2022-01-12 00:08:20,463 iteration 4071 : loss : 0.028318, loss_ce: 0.012428
2022-01-12 00:08:22,039 iteration 4072 : loss : 0.025195, loss_ce: 0.010697
2022-01-12 00:08:23,638 iteration 4073 : loss : 0.041972, loss_ce: 0.023475
2022-01-12 00:08:25,180 iteration 4074 : loss : 0.033467, loss_ce: 0.011175
2022-01-12 00:08:26,798 iteration 4075 : loss : 0.027743, loss_ce: 0.012748
2022-01-12 00:08:28,383 iteration 4076 : loss : 0.021620, loss_ce: 0.007442
2022-01-12 00:08:29,994 iteration 4077 : loss : 0.026810, loss_ce: 0.012907
2022-01-12 00:08:31,649 iteration 4078 : loss : 0.040290, loss_ce: 0.010652
2022-01-12 00:08:33,235 iteration 4079 : loss : 0.036432, loss_ce: 0.012404
2022-01-12 00:08:33,235 Training Data Eval:
2022-01-12 00:08:41,250   Average segmentation loss on training set: 0.0169
2022-01-12 00:08:41,250 Validation Data Eval:
2022-01-12 00:08:44,020   Average segmentation loss on validation set: 0.0660
2022-01-12 00:08:45,613 iteration 4080 : loss : 0.025167, loss_ce: 0.008869
 60%|████████████████▏          | 240/400 [1:56:16<1:22:00, 30.75s/it]2022-01-12 00:08:47,232 iteration 4081 : loss : 0.024151, loss_ce: 0.012259
2022-01-12 00:08:48,888 iteration 4082 : loss : 0.031867, loss_ce: 0.011223
2022-01-12 00:08:50,396 iteration 4083 : loss : 0.025604, loss_ce: 0.011096
2022-01-12 00:08:51,942 iteration 4084 : loss : 0.037217, loss_ce: 0.012071
2022-01-12 00:08:53,515 iteration 4085 : loss : 0.036257, loss_ce: 0.009682
2022-01-12 00:08:55,052 iteration 4086 : loss : 0.021315, loss_ce: 0.007791
2022-01-12 00:08:56,624 iteration 4087 : loss : 0.032989, loss_ce: 0.009346
2022-01-12 00:08:58,212 iteration 4088 : loss : 0.024602, loss_ce: 0.011843
2022-01-12 00:08:59,798 iteration 4089 : loss : 0.032105, loss_ce: 0.012601
2022-01-12 00:09:01,374 iteration 4090 : loss : 0.034026, loss_ce: 0.019055
2022-01-12 00:09:02,987 iteration 4091 : loss : 0.030208, loss_ce: 0.010025
2022-01-12 00:09:04,501 iteration 4092 : loss : 0.048089, loss_ce: 0.012859
2022-01-12 00:09:06,047 iteration 4093 : loss : 0.025823, loss_ce: 0.011584
2022-01-12 00:09:07,678 iteration 4094 : loss : 0.035361, loss_ce: 0.009605
2022-01-12 00:09:09,248 iteration 4095 : loss : 0.021924, loss_ce: 0.006454
2022-01-12 00:09:10,839 iteration 4096 : loss : 0.028540, loss_ce: 0.012302
2022-01-12 00:09:12,447 iteration 4097 : loss : 0.030478, loss_ce: 0.013352
 60%|████████████████▎          | 241/400 [1:56:43<1:18:22, 29.58s/it]2022-01-12 00:09:14,061 iteration 4098 : loss : 0.019856, loss_ce: 0.007353
2022-01-12 00:09:15,632 iteration 4099 : loss : 0.022813, loss_ce: 0.007680
2022-01-12 00:09:17,161 iteration 4100 : loss : 0.029408, loss_ce: 0.013018
2022-01-12 00:09:18,745 iteration 4101 : loss : 0.021712, loss_ce: 0.009755
2022-01-12 00:09:20,379 iteration 4102 : loss : 0.025612, loss_ce: 0.009904
2022-01-12 00:09:21,864 iteration 4103 : loss : 0.018934, loss_ce: 0.008182
2022-01-12 00:09:23,464 iteration 4104 : loss : 0.032894, loss_ce: 0.012249
2022-01-12 00:09:25,088 iteration 4105 : loss : 0.025467, loss_ce: 0.009363
2022-01-12 00:09:26,728 iteration 4106 : loss : 0.037272, loss_ce: 0.012303
2022-01-12 00:09:28,314 iteration 4107 : loss : 0.028954, loss_ce: 0.010512
2022-01-12 00:09:29,903 iteration 4108 : loss : 0.044360, loss_ce: 0.017906
2022-01-12 00:09:31,492 iteration 4109 : loss : 0.030854, loss_ce: 0.008977
2022-01-12 00:09:33,116 iteration 4110 : loss : 0.021615, loss_ce: 0.007286
2022-01-12 00:09:34,695 iteration 4111 : loss : 0.026663, loss_ce: 0.008204
2022-01-12 00:09:36,170 iteration 4112 : loss : 0.018215, loss_ce: 0.007491
2022-01-12 00:09:37,804 iteration 4113 : loss : 0.034229, loss_ce: 0.012988
2022-01-12 00:09:39,377 iteration 4114 : loss : 0.033046, loss_ce: 0.015143
 60%|████████████████▎          | 242/400 [1:57:10<1:15:47, 28.78s/it]2022-01-12 00:09:40,969 iteration 4115 : loss : 0.026978, loss_ce: 0.007438
2022-01-12 00:09:42,501 iteration 4116 : loss : 0.019420, loss_ce: 0.008689
2022-01-12 00:09:44,016 iteration 4117 : loss : 0.024989, loss_ce: 0.012375
2022-01-12 00:09:45,565 iteration 4118 : loss : 0.029539, loss_ce: 0.008436
2022-01-12 00:09:47,039 iteration 4119 : loss : 0.020932, loss_ce: 0.008342
2022-01-12 00:09:48,716 iteration 4120 : loss : 0.033036, loss_ce: 0.011847
2022-01-12 00:09:50,277 iteration 4121 : loss : 0.023130, loss_ce: 0.008586
2022-01-12 00:09:51,870 iteration 4122 : loss : 0.028899, loss_ce: 0.010048
2022-01-12 00:09:53,458 iteration 4123 : loss : 0.028389, loss_ce: 0.011676
2022-01-12 00:09:54,962 iteration 4124 : loss : 0.022788, loss_ce: 0.008449
2022-01-12 00:09:56,556 iteration 4125 : loss : 0.034775, loss_ce: 0.013392
2022-01-12 00:09:58,227 iteration 4126 : loss : 0.026926, loss_ce: 0.012466
2022-01-12 00:09:59,837 iteration 4127 : loss : 0.040172, loss_ce: 0.020431
2022-01-12 00:10:01,366 iteration 4128 : loss : 0.022130, loss_ce: 0.009442
2022-01-12 00:10:03,049 iteration 4129 : loss : 0.039243, loss_ce: 0.010357
2022-01-12 00:10:04,665 iteration 4130 : loss : 0.035281, loss_ce: 0.014492
2022-01-12 00:10:06,233 iteration 4131 : loss : 0.026791, loss_ce: 0.012752
 61%|████████████████▍          | 243/400 [1:57:37<1:13:47, 28.20s/it]2022-01-12 00:10:07,884 iteration 4132 : loss : 0.029367, loss_ce: 0.011665
2022-01-12 00:10:09,400 iteration 4133 : loss : 0.031120, loss_ce: 0.016008
2022-01-12 00:10:11,014 iteration 4134 : loss : 0.017286, loss_ce: 0.007816
2022-01-12 00:10:12,593 iteration 4135 : loss : 0.036905, loss_ce: 0.013931
2022-01-12 00:10:14,162 iteration 4136 : loss : 0.024592, loss_ce: 0.007801
2022-01-12 00:10:15,745 iteration 4137 : loss : 0.023979, loss_ce: 0.011212
2022-01-12 00:10:17,327 iteration 4138 : loss : 0.023308, loss_ce: 0.011207
2022-01-12 00:10:18,919 iteration 4139 : loss : 0.059031, loss_ce: 0.020779
2022-01-12 00:10:20,521 iteration 4140 : loss : 0.032959, loss_ce: 0.014491
2022-01-12 00:10:22,070 iteration 4141 : loss : 0.028723, loss_ce: 0.008084
2022-01-12 00:10:23,617 iteration 4142 : loss : 0.025224, loss_ce: 0.012202
2022-01-12 00:10:25,152 iteration 4143 : loss : 0.029373, loss_ce: 0.011045
2022-01-12 00:10:26,741 iteration 4144 : loss : 0.026747, loss_ce: 0.010771
2022-01-12 00:10:28,300 iteration 4145 : loss : 0.033014, loss_ce: 0.012179
2022-01-12 00:10:29,772 iteration 4146 : loss : 0.028721, loss_ce: 0.007596
2022-01-12 00:10:31,328 iteration 4147 : loss : 0.028782, loss_ce: 0.011238
2022-01-12 00:10:32,900 iteration 4148 : loss : 0.040472, loss_ce: 0.014060
 61%|████████████████▍          | 244/400 [1:58:03<1:12:08, 27.74s/it]2022-01-12 00:10:34,526 iteration 4149 : loss : 0.031903, loss_ce: 0.012974
2022-01-12 00:10:36,084 iteration 4150 : loss : 0.018935, loss_ce: 0.007077
2022-01-12 00:10:37,640 iteration 4151 : loss : 0.021099, loss_ce: 0.009365
2022-01-12 00:10:39,278 iteration 4152 : loss : 0.022916, loss_ce: 0.009814
2022-01-12 00:10:40,881 iteration 4153 : loss : 0.026544, loss_ce: 0.011891
2022-01-12 00:10:42,389 iteration 4154 : loss : 0.022270, loss_ce: 0.008762
2022-01-12 00:10:43,932 iteration 4155 : loss : 0.021337, loss_ce: 0.009747
2022-01-12 00:10:45,515 iteration 4156 : loss : 0.029765, loss_ce: 0.009856
2022-01-12 00:10:47,095 iteration 4157 : loss : 0.024498, loss_ce: 0.010483
2022-01-12 00:10:48,642 iteration 4158 : loss : 0.028393, loss_ce: 0.009022
2022-01-12 00:10:50,276 iteration 4159 : loss : 0.021527, loss_ce: 0.009575
2022-01-12 00:10:51,881 iteration 4160 : loss : 0.028932, loss_ce: 0.010162
2022-01-12 00:10:53,434 iteration 4161 : loss : 0.031134, loss_ce: 0.010694
2022-01-12 00:10:55,033 iteration 4162 : loss : 0.032252, loss_ce: 0.012009
2022-01-12 00:10:56,626 iteration 4163 : loss : 0.029175, loss_ce: 0.010389
2022-01-12 00:10:58,173 iteration 4164 : loss : 0.022520, loss_ce: 0.006771
2022-01-12 00:10:58,173 Training Data Eval:
2022-01-12 00:11:06,188   Average segmentation loss on training set: 0.0169
2022-01-12 00:11:06,231 Validation Data Eval:
2022-01-12 00:11:08,991   Average segmentation loss on validation set: 0.0754
2022-01-12 00:11:10,602 iteration 4165 : loss : 0.024672, loss_ce: 0.007797
 61%|████████████████▌          | 245/400 [1:58:41<1:19:23, 30.73s/it]2022-01-12 00:11:12,206 iteration 4166 : loss : 0.032345, loss_ce: 0.008038
2022-01-12 00:11:13,761 iteration 4167 : loss : 0.020473, loss_ce: 0.008367
2022-01-12 00:11:15,260 iteration 4168 : loss : 0.020182, loss_ce: 0.006456
2022-01-12 00:11:16,792 iteration 4169 : loss : 0.036419, loss_ce: 0.016286
2022-01-12 00:11:18,440 iteration 4170 : loss : 0.030480, loss_ce: 0.010060
2022-01-12 00:11:20,108 iteration 4171 : loss : 0.042016, loss_ce: 0.013945
2022-01-12 00:11:21,637 iteration 4172 : loss : 0.018144, loss_ce: 0.004293
2022-01-12 00:11:23,214 iteration 4173 : loss : 0.025540, loss_ce: 0.008713
2022-01-12 00:11:24,792 iteration 4174 : loss : 0.044863, loss_ce: 0.022057
2022-01-12 00:11:26,399 iteration 4175 : loss : 0.027604, loss_ce: 0.011138
2022-01-12 00:11:27,977 iteration 4176 : loss : 0.027684, loss_ce: 0.011153
2022-01-12 00:11:29,572 iteration 4177 : loss : 0.030510, loss_ce: 0.010812
2022-01-12 00:11:31,137 iteration 4178 : loss : 0.034447, loss_ce: 0.013796
2022-01-12 00:11:32,759 iteration 4179 : loss : 0.024102, loss_ce: 0.008743
2022-01-12 00:11:34,314 iteration 4180 : loss : 0.035789, loss_ce: 0.012401
2022-01-12 00:11:35,803 iteration 4181 : loss : 0.036799, loss_ce: 0.011548
2022-01-12 00:11:37,410 iteration 4182 : loss : 0.025629, loss_ce: 0.010062
 62%|████████████████▌          | 246/400 [1:59:08<1:15:51, 29.55s/it]2022-01-12 00:11:39,028 iteration 4183 : loss : 0.028214, loss_ce: 0.010466
2022-01-12 00:11:40,620 iteration 4184 : loss : 0.028979, loss_ce: 0.011199
2022-01-12 00:11:42,187 iteration 4185 : loss : 0.036803, loss_ce: 0.013367
2022-01-12 00:11:43,716 iteration 4186 : loss : 0.020546, loss_ce: 0.007262
2022-01-12 00:11:45,331 iteration 4187 : loss : 0.025154, loss_ce: 0.008183
2022-01-12 00:11:46,936 iteration 4188 : loss : 0.027387, loss_ce: 0.013421
2022-01-12 00:11:48,525 iteration 4189 : loss : 0.020188, loss_ce: 0.006274
2022-01-12 00:11:50,150 iteration 4190 : loss : 0.039658, loss_ce: 0.014332
2022-01-12 00:11:51,701 iteration 4191 : loss : 0.022096, loss_ce: 0.007047
2022-01-12 00:11:53,279 iteration 4192 : loss : 0.028854, loss_ce: 0.008674
2022-01-12 00:11:54,849 iteration 4193 : loss : 0.020275, loss_ce: 0.006739
2022-01-12 00:11:56,437 iteration 4194 : loss : 0.023965, loss_ce: 0.009766
2022-01-12 00:11:58,149 iteration 4195 : loss : 0.040259, loss_ce: 0.012688
2022-01-12 00:11:59,692 iteration 4196 : loss : 0.021495, loss_ce: 0.010814
2022-01-12 00:12:01,324 iteration 4197 : loss : 0.025843, loss_ce: 0.007427
2022-01-12 00:12:02,897 iteration 4198 : loss : 0.028190, loss_ce: 0.011390
2022-01-12 00:12:04,398 iteration 4199 : loss : 0.023919, loss_ce: 0.008674
 62%|████████████████▋          | 247/400 [1:59:35<1:13:24, 28.79s/it]2022-01-12 00:12:05,983 iteration 4200 : loss : 0.024487, loss_ce: 0.007985
2022-01-12 00:12:07,583 iteration 4201 : loss : 0.032854, loss_ce: 0.015253
2022-01-12 00:12:09,094 iteration 4202 : loss : 0.017512, loss_ce: 0.007936
2022-01-12 00:12:10,623 iteration 4203 : loss : 0.032116, loss_ce: 0.013416
2022-01-12 00:12:12,188 iteration 4204 : loss : 0.036627, loss_ce: 0.014585
2022-01-12 00:12:13,731 iteration 4205 : loss : 0.018805, loss_ce: 0.006549
2022-01-12 00:12:15,217 iteration 4206 : loss : 0.022722, loss_ce: 0.007092
2022-01-12 00:12:16,750 iteration 4207 : loss : 0.021979, loss_ce: 0.010666
2022-01-12 00:12:18,294 iteration 4208 : loss : 0.026005, loss_ce: 0.009247
2022-01-12 00:12:19,951 iteration 4209 : loss : 0.054868, loss_ce: 0.013456
2022-01-12 00:12:21,580 iteration 4210 : loss : 0.030410, loss_ce: 0.012223
2022-01-12 00:12:23,009 iteration 4211 : loss : 0.018857, loss_ce: 0.007616
2022-01-12 00:12:24,570 iteration 4212 : loss : 0.029233, loss_ce: 0.014816
2022-01-12 00:12:26,135 iteration 4213 : loss : 0.030416, loss_ce: 0.014069
2022-01-12 00:12:27,760 iteration 4214 : loss : 0.033596, loss_ce: 0.011232
2022-01-12 00:12:29,312 iteration 4215 : loss : 0.021457, loss_ce: 0.007819
2022-01-12 00:12:30,966 iteration 4216 : loss : 0.057316, loss_ce: 0.020180
 62%|████████████████▋          | 248/400 [2:00:01<1:11:14, 28.12s/it]2022-01-12 00:12:32,653 iteration 4217 : loss : 0.023943, loss_ce: 0.010410
2022-01-12 00:12:34,151 iteration 4218 : loss : 0.018829, loss_ce: 0.009957
2022-01-12 00:12:35,672 iteration 4219 : loss : 0.027854, loss_ce: 0.011561
2022-01-12 00:12:37,353 iteration 4220 : loss : 0.033433, loss_ce: 0.013192
2022-01-12 00:12:38,952 iteration 4221 : loss : 0.028512, loss_ce: 0.009251
2022-01-12 00:12:40,552 iteration 4222 : loss : 0.032683, loss_ce: 0.015577
2022-01-12 00:12:42,123 iteration 4223 : loss : 0.033660, loss_ce: 0.011660
2022-01-12 00:12:43,695 iteration 4224 : loss : 0.033110, loss_ce: 0.010217
2022-01-12 00:12:45,231 iteration 4225 : loss : 0.029410, loss_ce: 0.010683
2022-01-12 00:12:46,775 iteration 4226 : loss : 0.027508, loss_ce: 0.009934
2022-01-12 00:12:48,441 iteration 4227 : loss : 0.034453, loss_ce: 0.010862
2022-01-12 00:12:50,013 iteration 4228 : loss : 0.031294, loss_ce: 0.012367
2022-01-12 00:12:51,618 iteration 4229 : loss : 0.019870, loss_ce: 0.007567
2022-01-12 00:12:53,192 iteration 4230 : loss : 0.024423, loss_ce: 0.011911
2022-01-12 00:12:54,732 iteration 4231 : loss : 0.026245, loss_ce: 0.009107
2022-01-12 00:12:56,273 iteration 4232 : loss : 0.027585, loss_ce: 0.011560
2022-01-12 00:12:57,851 iteration 4233 : loss : 0.026126, loss_ce: 0.012317
 62%|████████████████▊          | 249/400 [2:00:28<1:09:50, 27.75s/it]2022-01-12 00:12:59,502 iteration 4234 : loss : 0.065320, loss_ce: 0.033571
2022-01-12 00:13:01,057 iteration 4235 : loss : 0.037062, loss_ce: 0.011108
2022-01-12 00:13:02,575 iteration 4236 : loss : 0.020286, loss_ce: 0.008705
2022-01-12 00:13:04,097 iteration 4237 : loss : 0.038329, loss_ce: 0.023298
2022-01-12 00:13:05,652 iteration 4238 : loss : 0.023134, loss_ce: 0.007484
2022-01-12 00:13:07,267 iteration 4239 : loss : 0.023285, loss_ce: 0.009161
2022-01-12 00:13:08,854 iteration 4240 : loss : 0.042754, loss_ce: 0.016771
2022-01-12 00:13:10,424 iteration 4241 : loss : 0.021470, loss_ce: 0.009461
2022-01-12 00:13:11,882 iteration 4242 : loss : 0.017481, loss_ce: 0.007431
2022-01-12 00:13:13,468 iteration 4243 : loss : 0.034145, loss_ce: 0.013280
2022-01-12 00:13:14,994 iteration 4244 : loss : 0.027297, loss_ce: 0.009805
2022-01-12 00:13:16,547 iteration 4245 : loss : 0.027081, loss_ce: 0.012105
2022-01-12 00:13:18,088 iteration 4246 : loss : 0.026679, loss_ce: 0.009902
2022-01-12 00:13:19,641 iteration 4247 : loss : 0.025478, loss_ce: 0.007360
2022-01-12 00:13:21,134 iteration 4248 : loss : 0.020147, loss_ce: 0.008591
2022-01-12 00:13:22,753 iteration 4249 : loss : 0.029247, loss_ce: 0.013745
2022-01-12 00:13:22,753 Training Data Eval:
2022-01-12 00:13:30,771   Average segmentation loss on training set: 0.0159
2022-01-12 00:13:30,772 Validation Data Eval:
2022-01-12 00:13:33,531   Average segmentation loss on validation set: 0.0668
2022-01-12 00:13:35,115 iteration 4250 : loss : 0.024228, loss_ce: 0.009376
 62%|████████████████▉          | 250/400 [2:01:05<1:16:30, 30.60s/it]2022-01-12 00:13:36,745 iteration 4251 : loss : 0.021096, loss_ce: 0.006905
2022-01-12 00:13:38,323 iteration 4252 : loss : 0.022809, loss_ce: 0.006104
2022-01-12 00:13:39,930 iteration 4253 : loss : 0.020653, loss_ce: 0.009964
2022-01-12 00:13:41,535 iteration 4254 : loss : 0.023080, loss_ce: 0.009075
2022-01-12 00:13:43,139 iteration 4255 : loss : 0.036301, loss_ce: 0.011454
2022-01-12 00:13:44,685 iteration 4256 : loss : 0.033007, loss_ce: 0.012905
2022-01-12 00:13:46,267 iteration 4257 : loss : 0.025604, loss_ce: 0.012248
2022-01-12 00:13:47,906 iteration 4258 : loss : 0.058704, loss_ce: 0.029080
2022-01-12 00:13:49,393 iteration 4259 : loss : 0.020274, loss_ce: 0.007325
2022-01-12 00:13:50,981 iteration 4260 : loss : 0.021721, loss_ce: 0.007210
2022-01-12 00:13:52,493 iteration 4261 : loss : 0.019062, loss_ce: 0.006550
2022-01-12 00:13:54,042 iteration 4262 : loss : 0.022556, loss_ce: 0.009646
2022-01-12 00:13:55,657 iteration 4263 : loss : 0.030930, loss_ce: 0.012939
2022-01-12 00:13:57,213 iteration 4264 : loss : 0.026280, loss_ce: 0.012690
2022-01-12 00:13:58,668 iteration 4265 : loss : 0.021958, loss_ce: 0.009436
2022-01-12 00:14:00,252 iteration 4266 : loss : 0.028198, loss_ce: 0.011216
2022-01-12 00:14:01,847 iteration 4267 : loss : 0.035767, loss_ce: 0.014698
 63%|████████████████▉          | 251/400 [2:01:32<1:13:06, 29.44s/it]2022-01-12 00:14:03,395 iteration 4268 : loss : 0.028568, loss_ce: 0.011570
2022-01-12 00:14:04,911 iteration 4269 : loss : 0.036892, loss_ce: 0.014575
2022-01-12 00:14:06,465 iteration 4270 : loss : 0.035851, loss_ce: 0.010389
2022-01-12 00:14:08,005 iteration 4271 : loss : 0.033130, loss_ce: 0.012939
2022-01-12 00:14:09,567 iteration 4272 : loss : 0.033100, loss_ce: 0.013918
2022-01-12 00:14:11,105 iteration 4273 : loss : 0.036551, loss_ce: 0.017260
2022-01-12 00:14:12,671 iteration 4274 : loss : 0.028686, loss_ce: 0.013545
2022-01-12 00:14:14,226 iteration 4275 : loss : 0.021386, loss_ce: 0.008693
2022-01-12 00:14:15,765 iteration 4276 : loss : 0.038439, loss_ce: 0.016606
2022-01-12 00:14:17,344 iteration 4277 : loss : 0.029856, loss_ce: 0.012905
2022-01-12 00:14:18,908 iteration 4278 : loss : 0.020996, loss_ce: 0.005859
2022-01-12 00:14:20,428 iteration 4279 : loss : 0.023498, loss_ce: 0.010574
2022-01-12 00:14:22,004 iteration 4280 : loss : 0.032279, loss_ce: 0.010041
2022-01-12 00:14:23,574 iteration 4281 : loss : 0.057702, loss_ce: 0.021980
2022-01-12 00:14:25,166 iteration 4282 : loss : 0.036060, loss_ce: 0.013600
2022-01-12 00:14:26,803 iteration 4283 : loss : 0.023796, loss_ce: 0.012248
2022-01-12 00:14:28,437 iteration 4284 : loss : 0.030404, loss_ce: 0.014577
 63%|█████████████████          | 252/400 [2:01:59<1:10:30, 28.59s/it]2022-01-12 00:14:29,984 iteration 4285 : loss : 0.024455, loss_ce: 0.012563
2022-01-12 00:14:31,516 iteration 4286 : loss : 0.032180, loss_ce: 0.013250
2022-01-12 00:14:33,091 iteration 4287 : loss : 0.030963, loss_ce: 0.012940
2022-01-12 00:14:34,613 iteration 4288 : loss : 0.018868, loss_ce: 0.005805
2022-01-12 00:14:36,135 iteration 4289 : loss : 0.020753, loss_ce: 0.009062
2022-01-12 00:14:37,695 iteration 4290 : loss : 0.021979, loss_ce: 0.008860
2022-01-12 00:14:39,315 iteration 4291 : loss : 0.021931, loss_ce: 0.009322
2022-01-12 00:14:40,908 iteration 4292 : loss : 0.028441, loss_ce: 0.016208
2022-01-12 00:14:42,392 iteration 4293 : loss : 0.022427, loss_ce: 0.008931
2022-01-12 00:14:43,920 iteration 4294 : loss : 0.022336, loss_ce: 0.010789
2022-01-12 00:14:45,608 iteration 4295 : loss : 0.034739, loss_ce: 0.012763
2022-01-12 00:14:47,146 iteration 4296 : loss : 0.022326, loss_ce: 0.008144
2022-01-12 00:14:48,680 iteration 4297 : loss : 0.019936, loss_ce: 0.006282
2022-01-12 00:14:50,311 iteration 4298 : loss : 0.026449, loss_ce: 0.009011
2022-01-12 00:14:51,911 iteration 4299 : loss : 0.029111, loss_ce: 0.009005
2022-01-12 00:14:53,553 iteration 4300 : loss : 0.031480, loss_ce: 0.012042
2022-01-12 00:14:55,116 iteration 4301 : loss : 0.041102, loss_ce: 0.014502
 63%|█████████████████          | 253/400 [2:02:25<1:08:37, 28.01s/it]2022-01-12 00:14:56,784 iteration 4302 : loss : 0.032518, loss_ce: 0.012597
2022-01-12 00:14:58,284 iteration 4303 : loss : 0.029024, loss_ce: 0.015428
2022-01-12 00:14:59,851 iteration 4304 : loss : 0.025412, loss_ce: 0.010504
2022-01-12 00:15:01,429 iteration 4305 : loss : 0.031585, loss_ce: 0.010451
2022-01-12 00:15:03,004 iteration 4306 : loss : 0.023358, loss_ce: 0.007479
2022-01-12 00:15:04,651 iteration 4307 : loss : 0.029134, loss_ce: 0.008934
2022-01-12 00:15:06,205 iteration 4308 : loss : 0.022646, loss_ce: 0.008590
2022-01-12 00:15:07,810 iteration 4309 : loss : 0.022821, loss_ce: 0.007712
2022-01-12 00:15:09,310 iteration 4310 : loss : 0.019951, loss_ce: 0.007174
2022-01-12 00:15:10,944 iteration 4311 : loss : 0.048124, loss_ce: 0.020812
2022-01-12 00:15:12,476 iteration 4312 : loss : 0.038030, loss_ce: 0.016590
2022-01-12 00:15:14,104 iteration 4313 : loss : 0.022236, loss_ce: 0.009630
2022-01-12 00:15:15,693 iteration 4314 : loss : 0.020536, loss_ce: 0.007501
2022-01-12 00:15:17,225 iteration 4315 : loss : 0.022661, loss_ce: 0.007828
2022-01-12 00:15:18,774 iteration 4316 : loss : 0.030495, loss_ce: 0.011787
2022-01-12 00:15:20,352 iteration 4317 : loss : 0.025464, loss_ce: 0.013388
2022-01-12 00:15:22,012 iteration 4318 : loss : 0.021766, loss_ce: 0.008510
 64%|█████████████████▏         | 254/400 [2:02:52<1:07:21, 27.68s/it]2022-01-12 00:15:23,581 iteration 4319 : loss : 0.022545, loss_ce: 0.008509
2022-01-12 00:15:25,053 iteration 4320 : loss : 0.021287, loss_ce: 0.008981
2022-01-12 00:15:26,754 iteration 4321 : loss : 0.039785, loss_ce: 0.023007
2022-01-12 00:15:28,324 iteration 4322 : loss : 0.030704, loss_ce: 0.012367
2022-01-12 00:15:29,891 iteration 4323 : loss : 0.028405, loss_ce: 0.010143
2022-01-12 00:15:31,441 iteration 4324 : loss : 0.021351, loss_ce: 0.008685
2022-01-12 00:15:32,986 iteration 4325 : loss : 0.025818, loss_ce: 0.008055
2022-01-12 00:15:34,601 iteration 4326 : loss : 0.032988, loss_ce: 0.011475
2022-01-12 00:15:36,260 iteration 4327 : loss : 0.020808, loss_ce: 0.009650
2022-01-12 00:15:37,864 iteration 4328 : loss : 0.022121, loss_ce: 0.010345
2022-01-12 00:15:39,422 iteration 4329 : loss : 0.021829, loss_ce: 0.008864
2022-01-12 00:15:40,947 iteration 4330 : loss : 0.025387, loss_ce: 0.012483
2022-01-12 00:15:42,499 iteration 4331 : loss : 0.022871, loss_ce: 0.011324
2022-01-12 00:15:44,017 iteration 4332 : loss : 0.020516, loss_ce: 0.006973
2022-01-12 00:15:45,496 iteration 4333 : loss : 0.016821, loss_ce: 0.006877
2022-01-12 00:15:47,084 iteration 4334 : loss : 0.024963, loss_ce: 0.008057
2022-01-12 00:15:47,085 Training Data Eval:
2022-01-12 00:15:55,107   Average segmentation loss on training set: 0.0149
2022-01-12 00:15:55,107 Validation Data Eval:
2022-01-12 00:15:57,867   Average segmentation loss on validation set: 0.0613
2022-01-12 00:16:03,674 Found new lowest validation loss at iteration 4334! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed2.pth
2022-01-12 00:16:05,167 iteration 4335 : loss : 0.039016, loss_ce: 0.010729
 64%|█████████████████▏         | 255/400 [2:03:35<1:18:06, 32.32s/it]2022-01-12 00:16:06,574 iteration 4336 : loss : 0.019446, loss_ce: 0.007900
2022-01-12 00:16:08,046 iteration 4337 : loss : 0.020691, loss_ce: 0.008763
2022-01-12 00:16:09,524 iteration 4338 : loss : 0.028965, loss_ce: 0.011012
2022-01-12 00:16:10,903 iteration 4339 : loss : 0.023655, loss_ce: 0.005858
2022-01-12 00:16:12,373 iteration 4340 : loss : 0.022732, loss_ce: 0.004490
2022-01-12 00:16:13,818 iteration 4341 : loss : 0.021555, loss_ce: 0.009151
2022-01-12 00:16:15,265 iteration 4342 : loss : 0.021986, loss_ce: 0.008759
2022-01-12 00:16:16,848 iteration 4343 : loss : 0.039913, loss_ce: 0.011716
2022-01-12 00:16:18,386 iteration 4344 : loss : 0.024584, loss_ce: 0.009882
2022-01-12 00:16:19,924 iteration 4345 : loss : 0.018015, loss_ce: 0.007552
2022-01-12 00:16:21,393 iteration 4346 : loss : 0.015211, loss_ce: 0.004932
2022-01-12 00:16:23,021 iteration 4347 : loss : 0.026209, loss_ce: 0.008527
2022-01-12 00:16:24,639 iteration 4348 : loss : 0.025681, loss_ce: 0.014257
2022-01-12 00:16:26,157 iteration 4349 : loss : 0.023798, loss_ce: 0.009819
2022-01-12 00:16:27,668 iteration 4350 : loss : 0.022596, loss_ce: 0.007534
2022-01-12 00:16:29,292 iteration 4351 : loss : 0.032331, loss_ce: 0.012788
2022-01-12 00:16:30,898 iteration 4352 : loss : 0.029642, loss_ce: 0.011919
 64%|█████████████████▎         | 256/400 [2:04:01<1:12:49, 30.35s/it]2022-01-12 00:16:32,476 iteration 4353 : loss : 0.026405, loss_ce: 0.010414
2022-01-12 00:16:34,111 iteration 4354 : loss : 0.021028, loss_ce: 0.006715
2022-01-12 00:16:35,715 iteration 4355 : loss : 0.031909, loss_ce: 0.011069
2022-01-12 00:16:37,262 iteration 4356 : loss : 0.025086, loss_ce: 0.009295
2022-01-12 00:16:38,905 iteration 4357 : loss : 0.028072, loss_ce: 0.011312
2022-01-12 00:16:40,449 iteration 4358 : loss : 0.024397, loss_ce: 0.009248
2022-01-12 00:16:42,053 iteration 4359 : loss : 0.023415, loss_ce: 0.007088
2022-01-12 00:16:43,570 iteration 4360 : loss : 0.018437, loss_ce: 0.007866
2022-01-12 00:16:45,203 iteration 4361 : loss : 0.034128, loss_ce: 0.013097
2022-01-12 00:16:46,725 iteration 4362 : loss : 0.018287, loss_ce: 0.006510
2022-01-12 00:16:48,333 iteration 4363 : loss : 0.022306, loss_ce: 0.009449
2022-01-12 00:16:49,913 iteration 4364 : loss : 0.023255, loss_ce: 0.010612
2022-01-12 00:16:51,544 iteration 4365 : loss : 0.029870, loss_ce: 0.008819
2022-01-12 00:16:53,080 iteration 4366 : loss : 0.032692, loss_ce: 0.011485
2022-01-12 00:16:54,595 iteration 4367 : loss : 0.022613, loss_ce: 0.009556
2022-01-12 00:16:56,139 iteration 4368 : loss : 0.018219, loss_ce: 0.006458
2022-01-12 00:16:57,762 iteration 4369 : loss : 0.035628, loss_ce: 0.016069
 64%|█████████████████▎         | 257/400 [2:04:28<1:09:49, 29.30s/it]2022-01-12 00:16:59,340 iteration 4370 : loss : 0.027630, loss_ce: 0.010082
2022-01-12 00:17:01,084 iteration 4371 : loss : 0.026554, loss_ce: 0.010178
2022-01-12 00:17:02,707 iteration 4372 : loss : 0.035363, loss_ce: 0.011402
2022-01-12 00:17:04,320 iteration 4373 : loss : 0.045716, loss_ce: 0.018264
2022-01-12 00:17:05,863 iteration 4374 : loss : 0.030145, loss_ce: 0.012165
2022-01-12 00:17:07,439 iteration 4375 : loss : 0.023699, loss_ce: 0.009666
2022-01-12 00:17:09,032 iteration 4376 : loss : 0.024133, loss_ce: 0.008919
2022-01-12 00:17:10,569 iteration 4377 : loss : 0.025386, loss_ce: 0.009692
2022-01-12 00:17:12,174 iteration 4378 : loss : 0.023459, loss_ce: 0.011240
2022-01-12 00:17:13,712 iteration 4379 : loss : 0.020257, loss_ce: 0.008416
2022-01-12 00:17:15,291 iteration 4380 : loss : 0.031273, loss_ce: 0.012045
2022-01-12 00:17:16,891 iteration 4381 : loss : 0.035761, loss_ce: 0.009805
2022-01-12 00:17:18,555 iteration 4382 : loss : 0.037727, loss_ce: 0.014579
2022-01-12 00:17:20,124 iteration 4383 : loss : 0.024785, loss_ce: 0.008687
2022-01-12 00:17:21,740 iteration 4384 : loss : 0.027970, loss_ce: 0.011215
2022-01-12 00:17:23,356 iteration 4385 : loss : 0.035774, loss_ce: 0.017107
2022-01-12 00:17:24,906 iteration 4386 : loss : 0.021570, loss_ce: 0.006712
 64%|█████████████████▍         | 258/400 [2:04:55<1:07:48, 28.65s/it]2022-01-12 00:17:26,585 iteration 4387 : loss : 0.036032, loss_ce: 0.009658
2022-01-12 00:17:28,178 iteration 4388 : loss : 0.029117, loss_ce: 0.014817
2022-01-12 00:17:29,765 iteration 4389 : loss : 0.024851, loss_ce: 0.011146
2022-01-12 00:17:31,366 iteration 4390 : loss : 0.030183, loss_ce: 0.012006
2022-01-12 00:17:32,997 iteration 4391 : loss : 0.032538, loss_ce: 0.014794
2022-01-12 00:17:34,602 iteration 4392 : loss : 0.027652, loss_ce: 0.011680
2022-01-12 00:17:36,264 iteration 4393 : loss : 0.025090, loss_ce: 0.010128
2022-01-12 00:17:37,758 iteration 4394 : loss : 0.018397, loss_ce: 0.007203
2022-01-12 00:17:39,333 iteration 4395 : loss : 0.022188, loss_ce: 0.010494
2022-01-12 00:17:40,921 iteration 4396 : loss : 0.025213, loss_ce: 0.010012
2022-01-12 00:17:42,439 iteration 4397 : loss : 0.017296, loss_ce: 0.005541
2022-01-12 00:17:44,007 iteration 4398 : loss : 0.017935, loss_ce: 0.007990
2022-01-12 00:17:45,582 iteration 4399 : loss : 0.039993, loss_ce: 0.011617
2022-01-12 00:17:47,101 iteration 4400 : loss : 0.021412, loss_ce: 0.006430
2022-01-12 00:17:48,711 iteration 4401 : loss : 0.033052, loss_ce: 0.018001
2022-01-12 00:17:50,212 iteration 4402 : loss : 0.021118, loss_ce: 0.006847
2022-01-12 00:17:51,798 iteration 4403 : loss : 0.025144, loss_ce: 0.008118
 65%|█████████████████▍         | 259/400 [2:05:22<1:06:05, 28.13s/it]2022-01-12 00:17:53,470 iteration 4404 : loss : 0.034883, loss_ce: 0.015671
2022-01-12 00:17:55,036 iteration 4405 : loss : 0.042282, loss_ce: 0.015307
2022-01-12 00:17:56,640 iteration 4406 : loss : 0.026259, loss_ce: 0.010711
2022-01-12 00:17:58,226 iteration 4407 : loss : 0.025490, loss_ce: 0.007141
2022-01-12 00:17:59,763 iteration 4408 : loss : 0.022546, loss_ce: 0.011602
2022-01-12 00:18:01,285 iteration 4409 : loss : 0.024649, loss_ce: 0.008797
2022-01-12 00:18:02,942 iteration 4410 : loss : 0.033934, loss_ce: 0.012984
2022-01-12 00:18:04,522 iteration 4411 : loss : 0.031053, loss_ce: 0.011907
2022-01-12 00:18:06,107 iteration 4412 : loss : 0.028089, loss_ce: 0.011893
2022-01-12 00:18:07,704 iteration 4413 : loss : 0.020734, loss_ce: 0.008599
2022-01-12 00:18:09,236 iteration 4414 : loss : 0.033305, loss_ce: 0.010604
2022-01-12 00:18:10,745 iteration 4415 : loss : 0.019691, loss_ce: 0.008970
2022-01-12 00:18:12,332 iteration 4416 : loss : 0.027127, loss_ce: 0.010706
2022-01-12 00:18:13,910 iteration 4417 : loss : 0.022284, loss_ce: 0.010826
2022-01-12 00:18:15,500 iteration 4418 : loss : 0.021312, loss_ce: 0.008826
2022-01-12 00:18:17,064 iteration 4419 : loss : 0.020430, loss_ce: 0.007761
2022-01-12 00:18:17,064 Training Data Eval:
2022-01-12 00:18:25,100   Average segmentation loss on training set: 0.0150
2022-01-12 00:18:25,101 Validation Data Eval:
2022-01-12 00:18:27,869   Average segmentation loss on validation set: 0.0668
2022-01-12 00:18:29,422 iteration 4420 : loss : 0.018411, loss_ce: 0.005382
 65%|█████████████████▌         | 260/400 [2:06:00<1:12:16, 30.97s/it]2022-01-12 00:18:31,043 iteration 4421 : loss : 0.024120, loss_ce: 0.007414
2022-01-12 00:18:32,603 iteration 4422 : loss : 0.018849, loss_ce: 0.006167
2022-01-12 00:18:34,159 iteration 4423 : loss : 0.019449, loss_ce: 0.007099
2022-01-12 00:18:35,777 iteration 4424 : loss : 0.035048, loss_ce: 0.015577
2022-01-12 00:18:37,344 iteration 4425 : loss : 0.035979, loss_ce: 0.015393
2022-01-12 00:18:38,903 iteration 4426 : loss : 0.024211, loss_ce: 0.007553
2022-01-12 00:18:40,482 iteration 4427 : loss : 0.027481, loss_ce: 0.012164
2022-01-12 00:18:42,081 iteration 4428 : loss : 0.029570, loss_ce: 0.011925
2022-01-12 00:18:43,561 iteration 4429 : loss : 0.030332, loss_ce: 0.013405
2022-01-12 00:18:45,141 iteration 4430 : loss : 0.020127, loss_ce: 0.009657
2022-01-12 00:18:46,741 iteration 4431 : loss : 0.032601, loss_ce: 0.011944
2022-01-12 00:18:48,358 iteration 4432 : loss : 0.028114, loss_ce: 0.012834
2022-01-12 00:18:49,962 iteration 4433 : loss : 0.029820, loss_ce: 0.008955
2022-01-12 00:18:51,574 iteration 4434 : loss : 0.030518, loss_ce: 0.012699
2022-01-12 00:18:53,103 iteration 4435 : loss : 0.021202, loss_ce: 0.006673
2022-01-12 00:18:54,653 iteration 4436 : loss : 0.033026, loss_ce: 0.017122
2022-01-12 00:18:56,206 iteration 4437 : loss : 0.022736, loss_ce: 0.007868
 65%|█████████████████▌         | 261/400 [2:06:27<1:08:50, 29.72s/it]2022-01-12 00:18:57,887 iteration 4438 : loss : 0.032571, loss_ce: 0.013568
2022-01-12 00:18:59,382 iteration 4439 : loss : 0.015188, loss_ce: 0.006984
2022-01-12 00:19:01,014 iteration 4440 : loss : 0.032515, loss_ce: 0.015241
2022-01-12 00:19:02,592 iteration 4441 : loss : 0.021209, loss_ce: 0.008736
2022-01-12 00:19:04,251 iteration 4442 : loss : 0.028222, loss_ce: 0.012136
2022-01-12 00:19:05,895 iteration 4443 : loss : 0.024426, loss_ce: 0.009659
2022-01-12 00:19:07,492 iteration 4444 : loss : 0.028721, loss_ce: 0.010264
2022-01-12 00:19:09,031 iteration 4445 : loss : 0.028835, loss_ce: 0.009496
2022-01-12 00:19:10,667 iteration 4446 : loss : 0.058534, loss_ce: 0.011053
2022-01-12 00:19:12,326 iteration 4447 : loss : 0.031871, loss_ce: 0.010794
2022-01-12 00:19:13,882 iteration 4448 : loss : 0.036293, loss_ce: 0.012626
2022-01-12 00:19:15,557 iteration 4449 : loss : 0.037956, loss_ce: 0.016635
2022-01-12 00:19:17,127 iteration 4450 : loss : 0.024990, loss_ce: 0.012186
2022-01-12 00:19:18,727 iteration 4451 : loss : 0.021939, loss_ce: 0.007797
2022-01-12 00:19:20,254 iteration 4452 : loss : 0.032913, loss_ce: 0.010273
2022-01-12 00:19:21,776 iteration 4453 : loss : 0.017767, loss_ce: 0.007038
2022-01-12 00:19:23,332 iteration 4454 : loss : 0.032116, loss_ce: 0.014785
 66%|█████████████████▋         | 262/400 [2:06:54<1:06:33, 28.94s/it]2022-01-12 00:19:24,912 iteration 4455 : loss : 0.033758, loss_ce: 0.010691
2022-01-12 00:19:26,411 iteration 4456 : loss : 0.023235, loss_ce: 0.008434
2022-01-12 00:19:27,982 iteration 4457 : loss : 0.023742, loss_ce: 0.010258
2022-01-12 00:19:29,525 iteration 4458 : loss : 0.023017, loss_ce: 0.009978
2022-01-12 00:19:31,131 iteration 4459 : loss : 0.067488, loss_ce: 0.018209
2022-01-12 00:19:32,665 iteration 4460 : loss : 0.019444, loss_ce: 0.007686
2022-01-12 00:19:34,229 iteration 4461 : loss : 0.018806, loss_ce: 0.008320
2022-01-12 00:19:35,878 iteration 4462 : loss : 0.045271, loss_ce: 0.014548
2022-01-12 00:19:37,386 iteration 4463 : loss : 0.020611, loss_ce: 0.007644
2022-01-12 00:19:38,927 iteration 4464 : loss : 0.018967, loss_ce: 0.008438
2022-01-12 00:19:40,609 iteration 4465 : loss : 0.065566, loss_ce: 0.013010
2022-01-12 00:19:42,108 iteration 4466 : loss : 0.021117, loss_ce: 0.008639
2022-01-12 00:19:43,694 iteration 4467 : loss : 0.025881, loss_ce: 0.011696
2022-01-12 00:19:45,261 iteration 4468 : loss : 0.030890, loss_ce: 0.011530
2022-01-12 00:19:46,833 iteration 4469 : loss : 0.020458, loss_ce: 0.006255
2022-01-12 00:19:48,387 iteration 4470 : loss : 0.032148, loss_ce: 0.014459
2022-01-12 00:19:50,004 iteration 4471 : loss : 0.052884, loss_ce: 0.013052
 66%|█████████████████▊         | 263/400 [2:07:20<1:04:31, 28.26s/it]2022-01-12 00:19:51,580 iteration 4472 : loss : 0.029573, loss_ce: 0.012762
2022-01-12 00:19:53,195 iteration 4473 : loss : 0.033473, loss_ce: 0.013350
2022-01-12 00:19:54,849 iteration 4474 : loss : 0.026268, loss_ce: 0.011639
2022-01-12 00:19:56,443 iteration 4475 : loss : 0.022512, loss_ce: 0.009261
2022-01-12 00:19:57,977 iteration 4476 : loss : 0.026022, loss_ce: 0.008977
2022-01-12 00:19:59,527 iteration 4477 : loss : 0.026586, loss_ce: 0.010785
2022-01-12 00:20:01,187 iteration 4478 : loss : 0.023371, loss_ce: 0.011147
2022-01-12 00:20:02,743 iteration 4479 : loss : 0.022149, loss_ce: 0.010196
2022-01-12 00:20:04,249 iteration 4480 : loss : 0.018584, loss_ce: 0.008138
2022-01-12 00:20:05,869 iteration 4481 : loss : 0.032557, loss_ce: 0.008544
2022-01-12 00:20:07,428 iteration 4482 : loss : 0.024829, loss_ce: 0.009579
2022-01-12 00:20:08,953 iteration 4483 : loss : 0.025240, loss_ce: 0.006776
2022-01-12 00:20:10,593 iteration 4484 : loss : 0.051434, loss_ce: 0.020221
2022-01-12 00:20:12,204 iteration 4485 : loss : 0.030848, loss_ce: 0.013703
2022-01-12 00:20:13,817 iteration 4486 : loss : 0.025676, loss_ce: 0.007880
2022-01-12 00:20:15,342 iteration 4487 : loss : 0.024000, loss_ce: 0.007075
2022-01-12 00:20:16,889 iteration 4488 : loss : 0.048922, loss_ce: 0.021181
 66%|█████████████████▊         | 264/400 [2:07:47<1:03:07, 27.85s/it]2022-01-12 00:20:18,528 iteration 4489 : loss : 0.029559, loss_ce: 0.014510
2022-01-12 00:20:20,045 iteration 4490 : loss : 0.021672, loss_ce: 0.012533
2022-01-12 00:20:21,655 iteration 4491 : loss : 0.023910, loss_ce: 0.007799
2022-01-12 00:20:23,301 iteration 4492 : loss : 0.022758, loss_ce: 0.009558
2022-01-12 00:20:24,831 iteration 4493 : loss : 0.030355, loss_ce: 0.011789
2022-01-12 00:20:26,385 iteration 4494 : loss : 0.024578, loss_ce: 0.007996
2022-01-12 00:20:27,972 iteration 4495 : loss : 0.032533, loss_ce: 0.011061
2022-01-12 00:20:29,502 iteration 4496 : loss : 0.022274, loss_ce: 0.011394
2022-01-12 00:20:31,135 iteration 4497 : loss : 0.032088, loss_ce: 0.013600
2022-01-12 00:20:32,703 iteration 4498 : loss : 0.038504, loss_ce: 0.010162
2022-01-12 00:20:34,292 iteration 4499 : loss : 0.027915, loss_ce: 0.009065
2022-01-12 00:20:35,792 iteration 4500 : loss : 0.017354, loss_ce: 0.006101
2022-01-12 00:20:37,320 iteration 4501 : loss : 0.022179, loss_ce: 0.007470
2022-01-12 00:20:38,857 iteration 4502 : loss : 0.030731, loss_ce: 0.010604
2022-01-12 00:20:40,496 iteration 4503 : loss : 0.029749, loss_ce: 0.013173
2022-01-12 00:20:42,050 iteration 4504 : loss : 0.019970, loss_ce: 0.007895
2022-01-12 00:20:42,051 Training Data Eval:
2022-01-12 00:20:50,067   Average segmentation loss on training set: 0.0157
2022-01-12 00:20:50,067 Validation Data Eval:
2022-01-12 00:20:52,837   Average segmentation loss on validation set: 0.0825
2022-01-12 00:20:54,443 iteration 4505 : loss : 0.021845, loss_ce: 0.007965
 66%|█████████████████▉         | 265/400 [2:08:25<1:09:12, 30.76s/it]2022-01-12 00:20:56,028 iteration 4506 : loss : 0.027593, loss_ce: 0.011552
2022-01-12 00:20:57,608 iteration 4507 : loss : 0.017221, loss_ce: 0.007173
2022-01-12 00:20:59,153 iteration 4508 : loss : 0.018716, loss_ce: 0.006360
2022-01-12 00:21:00,696 iteration 4509 : loss : 0.035262, loss_ce: 0.007517
2022-01-12 00:21:02,327 iteration 4510 : loss : 0.040544, loss_ce: 0.013126
2022-01-12 00:21:03,888 iteration 4511 : loss : 0.025631, loss_ce: 0.007515
2022-01-12 00:21:05,526 iteration 4512 : loss : 0.026536, loss_ce: 0.013350
2022-01-12 00:21:07,168 iteration 4513 : loss : 0.032730, loss_ce: 0.011081
2022-01-12 00:21:08,646 iteration 4514 : loss : 0.022054, loss_ce: 0.006493
2022-01-12 00:21:10,207 iteration 4515 : loss : 0.027949, loss_ce: 0.009246
2022-01-12 00:21:11,710 iteration 4516 : loss : 0.018125, loss_ce: 0.007828
2022-01-12 00:21:13,229 iteration 4517 : loss : 0.019253, loss_ce: 0.007712
2022-01-12 00:21:14,804 iteration 4518 : loss : 0.031255, loss_ce: 0.010766
2022-01-12 00:21:16,287 iteration 4519 : loss : 0.022488, loss_ce: 0.008273
2022-01-12 00:21:17,876 iteration 4520 : loss : 0.026243, loss_ce: 0.010383
2022-01-12 00:21:19,406 iteration 4521 : loss : 0.020592, loss_ce: 0.008056
2022-01-12 00:21:20,974 iteration 4522 : loss : 0.024674, loss_ce: 0.009221
 66%|█████████████████▉         | 266/400 [2:08:51<1:05:52, 29.49s/it]2022-01-12 00:21:22,651 iteration 4523 : loss : 0.022766, loss_ce: 0.008629
2022-01-12 00:21:24,187 iteration 4524 : loss : 0.030101, loss_ce: 0.011474
2022-01-12 00:21:25,684 iteration 4525 : loss : 0.025063, loss_ce: 0.011446
2022-01-12 00:21:27,223 iteration 4526 : loss : 0.021696, loss_ce: 0.007184
2022-01-12 00:21:28,758 iteration 4527 : loss : 0.020158, loss_ce: 0.009447
2022-01-12 00:21:30,335 iteration 4528 : loss : 0.018801, loss_ce: 0.007359
2022-01-12 00:21:31,846 iteration 4529 : loss : 0.025822, loss_ce: 0.008858
2022-01-12 00:21:33,407 iteration 4530 : loss : 0.028220, loss_ce: 0.010680
2022-01-12 00:21:35,063 iteration 4531 : loss : 0.036558, loss_ce: 0.012071
2022-01-12 00:21:36,546 iteration 4532 : loss : 0.016665, loss_ce: 0.005789
2022-01-12 00:21:38,240 iteration 4533 : loss : 0.044800, loss_ce: 0.019546
2022-01-12 00:21:39,758 iteration 4534 : loss : 0.018919, loss_ce: 0.008910
2022-01-12 00:21:41,359 iteration 4535 : loss : 0.025250, loss_ce: 0.011933
2022-01-12 00:21:42,947 iteration 4536 : loss : 0.026664, loss_ce: 0.007929
2022-01-12 00:21:44,473 iteration 4537 : loss : 0.031539, loss_ce: 0.008581
2022-01-12 00:21:46,025 iteration 4538 : loss : 0.020074, loss_ce: 0.008778
2022-01-12 00:21:47,501 iteration 4539 : loss : 0.017310, loss_ce: 0.006668
 67%|██████████████████         | 267/400 [2:09:18<1:03:24, 28.60s/it]2022-01-12 00:21:49,058 iteration 4540 : loss : 0.020686, loss_ce: 0.010375
2022-01-12 00:21:50,623 iteration 4541 : loss : 0.026884, loss_ce: 0.010814
2022-01-12 00:21:52,192 iteration 4542 : loss : 0.031794, loss_ce: 0.009021
2022-01-12 00:21:53,907 iteration 4543 : loss : 0.036744, loss_ce: 0.018947
2022-01-12 00:21:55,407 iteration 4544 : loss : 0.023172, loss_ce: 0.009874
2022-01-12 00:21:56,920 iteration 4545 : loss : 0.016236, loss_ce: 0.006287
2022-01-12 00:21:58,472 iteration 4546 : loss : 0.031328, loss_ce: 0.013671
2022-01-12 00:22:00,044 iteration 4547 : loss : 0.030349, loss_ce: 0.012220
2022-01-12 00:22:01,583 iteration 4548 : loss : 0.021397, loss_ce: 0.010114
2022-01-12 00:22:03,085 iteration 4549 : loss : 0.029514, loss_ce: 0.009877
2022-01-12 00:22:04,723 iteration 4550 : loss : 0.036418, loss_ce: 0.011159
2022-01-12 00:22:06,355 iteration 4551 : loss : 0.034182, loss_ce: 0.012783
2022-01-12 00:22:07,916 iteration 4552 : loss : 0.028348, loss_ce: 0.008988
2022-01-12 00:22:09,459 iteration 4553 : loss : 0.017792, loss_ce: 0.006098
2022-01-12 00:22:11,058 iteration 4554 : loss : 0.029448, loss_ce: 0.008128
2022-01-12 00:22:12,577 iteration 4555 : loss : 0.038187, loss_ce: 0.012547
2022-01-12 00:22:14,143 iteration 4556 : loss : 0.025513, loss_ce: 0.011654
 67%|██████████████████         | 268/400 [2:09:44<1:01:37, 28.01s/it]2022-01-12 00:22:15,738 iteration 4557 : loss : 0.017011, loss_ce: 0.008539
2022-01-12 00:22:17,302 iteration 4558 : loss : 0.031426, loss_ce: 0.014167
2022-01-12 00:22:18,905 iteration 4559 : loss : 0.021759, loss_ce: 0.007147
2022-01-12 00:22:20,476 iteration 4560 : loss : 0.031041, loss_ce: 0.010952
2022-01-12 00:22:22,076 iteration 4561 : loss : 0.029868, loss_ce: 0.007930
2022-01-12 00:22:23,596 iteration 4562 : loss : 0.042828, loss_ce: 0.011525
2022-01-12 00:22:25,191 iteration 4563 : loss : 0.025188, loss_ce: 0.013452
2022-01-12 00:22:26,804 iteration 4564 : loss : 0.025826, loss_ce: 0.010265
2022-01-12 00:22:28,444 iteration 4565 : loss : 0.040017, loss_ce: 0.014229
2022-01-12 00:22:30,045 iteration 4566 : loss : 0.039167, loss_ce: 0.018586
2022-01-12 00:22:31,584 iteration 4567 : loss : 0.020284, loss_ce: 0.007632
2022-01-12 00:22:33,146 iteration 4568 : loss : 0.023351, loss_ce: 0.007553
2022-01-12 00:22:34,778 iteration 4569 : loss : 0.062114, loss_ce: 0.010715
2022-01-12 00:22:36,430 iteration 4570 : loss : 0.044946, loss_ce: 0.020078
2022-01-12 00:22:37,992 iteration 4571 : loss : 0.032480, loss_ce: 0.014847
2022-01-12 00:22:39,501 iteration 4572 : loss : 0.019538, loss_ce: 0.007984
2022-01-12 00:22:41,101 iteration 4573 : loss : 0.041804, loss_ce: 0.019819
 67%|██████████████████▏        | 269/400 [2:10:11<1:00:28, 27.70s/it]2022-01-12 00:22:42,632 iteration 4574 : loss : 0.023053, loss_ce: 0.011727
2022-01-12 00:22:44,254 iteration 4575 : loss : 0.035864, loss_ce: 0.011007
2022-01-12 00:22:45,936 iteration 4576 : loss : 0.038381, loss_ce: 0.016246
2022-01-12 00:22:47,513 iteration 4577 : loss : 0.023930, loss_ce: 0.008786
2022-01-12 00:22:49,008 iteration 4578 : loss : 0.020264, loss_ce: 0.008290
2022-01-12 00:22:50,642 iteration 4579 : loss : 0.032586, loss_ce: 0.013878
2022-01-12 00:22:52,218 iteration 4580 : loss : 0.019695, loss_ce: 0.008410
2022-01-12 00:22:53,796 iteration 4581 : loss : 0.025620, loss_ce: 0.010160
2022-01-12 00:22:55,344 iteration 4582 : loss : 0.024621, loss_ce: 0.010734
2022-01-12 00:22:56,984 iteration 4583 : loss : 0.046616, loss_ce: 0.009331
2022-01-12 00:22:58,548 iteration 4584 : loss : 0.034643, loss_ce: 0.008557
2022-01-12 00:23:00,070 iteration 4585 : loss : 0.020169, loss_ce: 0.006442
2022-01-12 00:23:01,625 iteration 4586 : loss : 0.019381, loss_ce: 0.009668
2022-01-12 00:23:03,194 iteration 4587 : loss : 0.021951, loss_ce: 0.008638
2022-01-12 00:23:04,826 iteration 4588 : loss : 0.040894, loss_ce: 0.016606
2022-01-12 00:23:06,391 iteration 4589 : loss : 0.028576, loss_ce: 0.012930
2022-01-12 00:23:06,391 Training Data Eval:
2022-01-12 00:23:14,425   Average segmentation loss on training set: 0.0166
2022-01-12 00:23:14,426 Validation Data Eval:
2022-01-12 00:23:17,191   Average segmentation loss on validation set: 0.0886
2022-01-12 00:23:18,820 iteration 4590 : loss : 0.037166, loss_ce: 0.021032
 68%|██████████████████▏        | 270/400 [2:10:49<1:06:31, 30.70s/it]2022-01-12 00:23:20,562 iteration 4591 : loss : 0.033146, loss_ce: 0.012550
2022-01-12 00:23:22,144 iteration 4592 : loss : 0.027085, loss_ce: 0.012460
2022-01-12 00:23:23,723 iteration 4593 : loss : 0.034892, loss_ce: 0.012724
2022-01-12 00:23:25,306 iteration 4594 : loss : 0.027643, loss_ce: 0.011403
2022-01-12 00:23:26,875 iteration 4595 : loss : 0.019224, loss_ce: 0.008082
2022-01-12 00:23:28,541 iteration 4596 : loss : 0.023050, loss_ce: 0.009530
2022-01-12 00:23:30,053 iteration 4597 : loss : 0.028089, loss_ce: 0.008683
2022-01-12 00:23:31,542 iteration 4598 : loss : 0.026008, loss_ce: 0.008308
2022-01-12 00:23:33,085 iteration 4599 : loss : 0.024695, loss_ce: 0.010306
2022-01-12 00:23:34,661 iteration 4600 : loss : 0.021839, loss_ce: 0.008478
2022-01-12 00:23:36,253 iteration 4601 : loss : 0.024674, loss_ce: 0.013104
2022-01-12 00:23:37,906 iteration 4602 : loss : 0.027321, loss_ce: 0.009894
2022-01-12 00:23:39,554 iteration 4603 : loss : 0.027024, loss_ce: 0.009903
2022-01-12 00:23:41,077 iteration 4604 : loss : 0.024420, loss_ce: 0.008665
2022-01-12 00:23:42,643 iteration 4605 : loss : 0.017362, loss_ce: 0.005290
2022-01-12 00:23:44,204 iteration 4606 : loss : 0.031614, loss_ce: 0.009904
2022-01-12 00:23:45,758 iteration 4607 : loss : 0.027023, loss_ce: 0.011349
 68%|██████████████████▎        | 271/400 [2:11:16<1:03:34, 29.57s/it]2022-01-12 00:23:47,391 iteration 4608 : loss : 0.019391, loss_ce: 0.006474
2022-01-12 00:23:48,973 iteration 4609 : loss : 0.022525, loss_ce: 0.006997
2022-01-12 00:23:50,558 iteration 4610 : loss : 0.032940, loss_ce: 0.009341
2022-01-12 00:23:52,175 iteration 4611 : loss : 0.031765, loss_ce: 0.013193
2022-01-12 00:23:53,747 iteration 4612 : loss : 0.030969, loss_ce: 0.013249
2022-01-12 00:23:55,333 iteration 4613 : loss : 0.036961, loss_ce: 0.012828
2022-01-12 00:23:56,917 iteration 4614 : loss : 0.018869, loss_ce: 0.006202
2022-01-12 00:23:58,593 iteration 4615 : loss : 0.032653, loss_ce: 0.014813
2022-01-12 00:24:00,098 iteration 4616 : loss : 0.022844, loss_ce: 0.010343
2022-01-12 00:24:01,685 iteration 4617 : loss : 0.036114, loss_ce: 0.011983
2022-01-12 00:24:03,224 iteration 4618 : loss : 0.015524, loss_ce: 0.006444
2022-01-12 00:24:04,807 iteration 4619 : loss : 0.026644, loss_ce: 0.009938
2022-01-12 00:24:06,358 iteration 4620 : loss : 0.019970, loss_ce: 0.007622
2022-01-12 00:24:07,906 iteration 4621 : loss : 0.022121, loss_ce: 0.007997
2022-01-12 00:24:09,480 iteration 4622 : loss : 0.027518, loss_ce: 0.011667
2022-01-12 00:24:11,056 iteration 4623 : loss : 0.020674, loss_ce: 0.009413
2022-01-12 00:24:12,597 iteration 4624 : loss : 0.025072, loss_ce: 0.007903
 68%|██████████████████▎        | 272/400 [2:11:43<1:01:20, 28.75s/it]2022-01-12 00:24:14,273 iteration 4625 : loss : 0.026827, loss_ce: 0.008572
2022-01-12 00:24:15,868 iteration 4626 : loss : 0.035160, loss_ce: 0.011376
2022-01-12 00:24:17,425 iteration 4627 : loss : 0.019552, loss_ce: 0.008651
2022-01-12 00:24:18,999 iteration 4628 : loss : 0.032622, loss_ce: 0.012466
2022-01-12 00:24:20,511 iteration 4629 : loss : 0.037191, loss_ce: 0.007458
2022-01-12 00:24:22,099 iteration 4630 : loss : 0.028880, loss_ce: 0.015863
2022-01-12 00:24:23,643 iteration 4631 : loss : 0.038180, loss_ce: 0.014535
2022-01-12 00:24:25,224 iteration 4632 : loss : 0.023631, loss_ce: 0.006972
2022-01-12 00:24:26,810 iteration 4633 : loss : 0.026386, loss_ce: 0.009523
2022-01-12 00:24:28,436 iteration 4634 : loss : 0.029652, loss_ce: 0.012697
2022-01-12 00:24:29,893 iteration 4635 : loss : 0.016471, loss_ce: 0.005455
2022-01-12 00:24:31,405 iteration 4636 : loss : 0.015522, loss_ce: 0.005814
2022-01-12 00:24:32,968 iteration 4637 : loss : 0.031761, loss_ce: 0.011039
2022-01-12 00:24:34,534 iteration 4638 : loss : 0.049992, loss_ce: 0.031082
2022-01-12 00:24:36,131 iteration 4639 : loss : 0.045983, loss_ce: 0.014970
2022-01-12 00:24:37,740 iteration 4640 : loss : 0.030278, loss_ce: 0.013383
2022-01-12 00:24:39,321 iteration 4641 : loss : 0.029373, loss_ce: 0.010758
 68%|███████████████████▊         | 273/400 [2:12:10<59:34, 28.14s/it]2022-01-12 00:24:40,941 iteration 4642 : loss : 0.026453, loss_ce: 0.010666
2022-01-12 00:24:42,513 iteration 4643 : loss : 0.026681, loss_ce: 0.010320
2022-01-12 00:24:44,098 iteration 4644 : loss : 0.037767, loss_ce: 0.016059
2022-01-12 00:24:45,641 iteration 4645 : loss : 0.031783, loss_ce: 0.011161
2022-01-12 00:24:47,200 iteration 4646 : loss : 0.021648, loss_ce: 0.009056
2022-01-12 00:24:48,758 iteration 4647 : loss : 0.020855, loss_ce: 0.008695
2022-01-12 00:24:50,288 iteration 4648 : loss : 0.020245, loss_ce: 0.008719
2022-01-12 00:24:51,982 iteration 4649 : loss : 0.027767, loss_ce: 0.010779
2022-01-12 00:24:53,438 iteration 4650 : loss : 0.018521, loss_ce: 0.007789
2022-01-12 00:24:55,013 iteration 4651 : loss : 0.022995, loss_ce: 0.008840
2022-01-12 00:24:56,571 iteration 4652 : loss : 0.023419, loss_ce: 0.008931
2022-01-12 00:24:58,070 iteration 4653 : loss : 0.017626, loss_ce: 0.007133
2022-01-12 00:24:59,581 iteration 4654 : loss : 0.021992, loss_ce: 0.006863
2022-01-12 00:25:01,207 iteration 4655 : loss : 0.027895, loss_ce: 0.009068
2022-01-12 00:25:02,769 iteration 4656 : loss : 0.025599, loss_ce: 0.008414
2022-01-12 00:25:04,342 iteration 4657 : loss : 0.034810, loss_ce: 0.015754
2022-01-12 00:25:05,895 iteration 4658 : loss : 0.034565, loss_ce: 0.016581
 68%|███████████████████▊         | 274/400 [2:12:36<58:07, 27.68s/it]2022-01-12 00:25:07,512 iteration 4659 : loss : 0.022157, loss_ce: 0.008830
2022-01-12 00:25:09,047 iteration 4660 : loss : 0.017478, loss_ce: 0.006700
2022-01-12 00:25:10,661 iteration 4661 : loss : 0.021072, loss_ce: 0.009186
2022-01-12 00:25:12,139 iteration 4662 : loss : 0.015471, loss_ce: 0.007737
2022-01-12 00:25:13,790 iteration 4663 : loss : 0.021607, loss_ce: 0.008185
2022-01-12 00:25:15,359 iteration 4664 : loss : 0.023648, loss_ce: 0.008946
2022-01-12 00:25:16,926 iteration 4665 : loss : 0.025500, loss_ce: 0.012223
2022-01-12 00:25:18,547 iteration 4666 : loss : 0.022000, loss_ce: 0.009407
2022-01-12 00:25:20,173 iteration 4667 : loss : 0.033973, loss_ce: 0.010289
2022-01-12 00:25:21,733 iteration 4668 : loss : 0.016556, loss_ce: 0.006739
2022-01-12 00:25:23,356 iteration 4669 : loss : 0.027159, loss_ce: 0.010722
2022-01-12 00:25:24,934 iteration 4670 : loss : 0.048488, loss_ce: 0.012202
2022-01-12 00:25:26,469 iteration 4671 : loss : 0.018632, loss_ce: 0.007385
2022-01-12 00:25:28,000 iteration 4672 : loss : 0.016006, loss_ce: 0.005823
2022-01-12 00:25:29,494 iteration 4673 : loss : 0.017595, loss_ce: 0.005952
2022-01-12 00:25:31,092 iteration 4674 : loss : 0.023732, loss_ce: 0.006727
2022-01-12 00:25:31,092 Training Data Eval:
2022-01-12 00:25:39,122   Average segmentation loss on training set: 0.0171
2022-01-12 00:25:39,122 Validation Data Eval:
2022-01-12 00:25:41,885   Average segmentation loss on validation set: 0.0860
2022-01-12 00:25:43,387 iteration 4675 : loss : 0.017905, loss_ce: 0.007167
 69%|██████████████████▌        | 275/400 [2:13:14<1:03:47, 30.62s/it]2022-01-12 00:25:45,010 iteration 4676 : loss : 0.039100, loss_ce: 0.016843
2022-01-12 00:25:46,542 iteration 4677 : loss : 0.015084, loss_ce: 0.006753
2022-01-12 00:25:48,160 iteration 4678 : loss : 0.027631, loss_ce: 0.008969
2022-01-12 00:25:49,811 iteration 4679 : loss : 0.027250, loss_ce: 0.011672
2022-01-12 00:25:51,319 iteration 4680 : loss : 0.021966, loss_ce: 0.008981
2022-01-12 00:25:52,892 iteration 4681 : loss : 0.026660, loss_ce: 0.013631
2022-01-12 00:25:54,445 iteration 4682 : loss : 0.020216, loss_ce: 0.007455
2022-01-12 00:25:55,978 iteration 4683 : loss : 0.019423, loss_ce: 0.006184
2022-01-12 00:25:57,548 iteration 4684 : loss : 0.034135, loss_ce: 0.012605
2022-01-12 00:25:59,163 iteration 4685 : loss : 0.022123, loss_ce: 0.009840
2022-01-12 00:26:00,678 iteration 4686 : loss : 0.032226, loss_ce: 0.013297
2022-01-12 00:26:02,253 iteration 4687 : loss : 0.033806, loss_ce: 0.009212
2022-01-12 00:26:03,736 iteration 4688 : loss : 0.014318, loss_ce: 0.006414
2022-01-12 00:26:05,328 iteration 4689 : loss : 0.028659, loss_ce: 0.010849
2022-01-12 00:26:06,809 iteration 4690 : loss : 0.017910, loss_ce: 0.006642
2022-01-12 00:26:08,377 iteration 4691 : loss : 0.025345, loss_ce: 0.007073
2022-01-12 00:26:10,037 iteration 4692 : loss : 0.026815, loss_ce: 0.010698
 69%|██████████████████▋        | 276/400 [2:13:40<1:00:49, 29.43s/it]2022-01-12 00:26:11,615 iteration 4693 : loss : 0.022682, loss_ce: 0.009419
2022-01-12 00:26:13,169 iteration 4694 : loss : 0.022768, loss_ce: 0.007410
2022-01-12 00:26:14,758 iteration 4695 : loss : 0.026664, loss_ce: 0.009960
2022-01-12 00:26:16,349 iteration 4696 : loss : 0.034917, loss_ce: 0.012733
2022-01-12 00:26:17,871 iteration 4697 : loss : 0.020733, loss_ce: 0.008628
2022-01-12 00:26:19,403 iteration 4698 : loss : 0.023842, loss_ce: 0.009446
2022-01-12 00:26:20,959 iteration 4699 : loss : 0.023684, loss_ce: 0.006765
2022-01-12 00:26:22,520 iteration 4700 : loss : 0.028985, loss_ce: 0.012585
2022-01-12 00:26:24,066 iteration 4701 : loss : 0.024999, loss_ce: 0.007423
2022-01-12 00:26:25,678 iteration 4702 : loss : 0.037448, loss_ce: 0.013526
2022-01-12 00:26:27,256 iteration 4703 : loss : 0.030232, loss_ce: 0.014146
2022-01-12 00:26:28,850 iteration 4704 : loss : 0.022363, loss_ce: 0.006153
2022-01-12 00:26:30,434 iteration 4705 : loss : 0.037297, loss_ce: 0.010435
2022-01-12 00:26:31,929 iteration 4706 : loss : 0.016824, loss_ce: 0.006628
2022-01-12 00:26:33,461 iteration 4707 : loss : 0.023776, loss_ce: 0.007242
2022-01-12 00:26:35,021 iteration 4708 : loss : 0.028549, loss_ce: 0.012807
2022-01-12 00:26:36,566 iteration 4709 : loss : 0.024987, loss_ce: 0.009830
 69%|████████████████████         | 277/400 [2:14:07<58:32, 28.56s/it]2022-01-12 00:26:38,132 iteration 4710 : loss : 0.019729, loss_ce: 0.007139
2022-01-12 00:26:39,643 iteration 4711 : loss : 0.023146, loss_ce: 0.007271
2022-01-12 00:26:41,132 iteration 4712 : loss : 0.016037, loss_ce: 0.005818
2022-01-12 00:26:42,697 iteration 4713 : loss : 0.026187, loss_ce: 0.008028
2022-01-12 00:26:44,196 iteration 4714 : loss : 0.018549, loss_ce: 0.006189
2022-01-12 00:26:45,773 iteration 4715 : loss : 0.023689, loss_ce: 0.012646
2022-01-12 00:26:47,380 iteration 4716 : loss : 0.022770, loss_ce: 0.009154
2022-01-12 00:26:48,934 iteration 4717 : loss : 0.019579, loss_ce: 0.007240
2022-01-12 00:26:50,507 iteration 4718 : loss : 0.021860, loss_ce: 0.008464
2022-01-12 00:26:52,053 iteration 4719 : loss : 0.014220, loss_ce: 0.004910
2022-01-12 00:26:53,611 iteration 4720 : loss : 0.026440, loss_ce: 0.013281
2022-01-12 00:26:55,157 iteration 4721 : loss : 0.023347, loss_ce: 0.008614
2022-01-12 00:26:56,776 iteration 4722 : loss : 0.032421, loss_ce: 0.015339
2022-01-12 00:26:58,423 iteration 4723 : loss : 0.027782, loss_ce: 0.008210
2022-01-12 00:27:00,079 iteration 4724 : loss : 0.024957, loss_ce: 0.012552
2022-01-12 00:27:01,610 iteration 4725 : loss : 0.023871, loss_ce: 0.008029
2022-01-12 00:27:03,206 iteration 4726 : loss : 0.024260, loss_ce: 0.007306
 70%|████████████████████▏        | 278/400 [2:14:34<56:54, 27.98s/it]2022-01-12 00:27:04,822 iteration 4727 : loss : 0.025764, loss_ce: 0.008623
2022-01-12 00:27:06,410 iteration 4728 : loss : 0.021420, loss_ce: 0.008786
2022-01-12 00:27:07,943 iteration 4729 : loss : 0.020885, loss_ce: 0.006538
2022-01-12 00:27:09,511 iteration 4730 : loss : 0.027740, loss_ce: 0.011814
2022-01-12 00:27:11,079 iteration 4731 : loss : 0.020184, loss_ce: 0.005676
2022-01-12 00:27:12,667 iteration 4732 : loss : 0.022316, loss_ce: 0.008087
2022-01-12 00:27:14,184 iteration 4733 : loss : 0.019291, loss_ce: 0.007106
2022-01-12 00:27:15,698 iteration 4734 : loss : 0.015792, loss_ce: 0.005256
2022-01-12 00:27:17,317 iteration 4735 : loss : 0.027801, loss_ce: 0.014459
2022-01-12 00:27:18,892 iteration 4736 : loss : 0.025486, loss_ce: 0.010033
2022-01-12 00:27:20,535 iteration 4737 : loss : 0.048221, loss_ce: 0.015566
2022-01-12 00:27:22,166 iteration 4738 : loss : 0.045787, loss_ce: 0.008028
2022-01-12 00:27:23,718 iteration 4739 : loss : 0.018621, loss_ce: 0.007581
2022-01-12 00:27:25,219 iteration 4740 : loss : 0.016946, loss_ce: 0.007508
2022-01-12 00:27:26,776 iteration 4741 : loss : 0.033927, loss_ce: 0.021386
2022-01-12 00:27:28,265 iteration 4742 : loss : 0.016583, loss_ce: 0.006191
2022-01-12 00:27:29,837 iteration 4743 : loss : 0.017249, loss_ce: 0.005710
 70%|████████████████████▏        | 279/400 [2:15:00<55:37, 27.58s/it]2022-01-12 00:27:31,456 iteration 4744 : loss : 0.020232, loss_ce: 0.007891
2022-01-12 00:27:33,036 iteration 4745 : loss : 0.021950, loss_ce: 0.010051
2022-01-12 00:27:34,595 iteration 4746 : loss : 0.023995, loss_ce: 0.010205
2022-01-12 00:27:36,197 iteration 4747 : loss : 0.021532, loss_ce: 0.008312
2022-01-12 00:27:37,848 iteration 4748 : loss : 0.034361, loss_ce: 0.014755
2022-01-12 00:27:39,466 iteration 4749 : loss : 0.022084, loss_ce: 0.006586
2022-01-12 00:27:41,054 iteration 4750 : loss : 0.018598, loss_ce: 0.007339
2022-01-12 00:27:42,559 iteration 4751 : loss : 0.021257, loss_ce: 0.007672
2022-01-12 00:27:44,126 iteration 4752 : loss : 0.030562, loss_ce: 0.007415
2022-01-12 00:27:45,727 iteration 4753 : loss : 0.021904, loss_ce: 0.010085
2022-01-12 00:27:47,311 iteration 4754 : loss : 0.023511, loss_ce: 0.006823
2022-01-12 00:27:48,893 iteration 4755 : loss : 0.020483, loss_ce: 0.007317
2022-01-12 00:27:50,473 iteration 4756 : loss : 0.018369, loss_ce: 0.007926
2022-01-12 00:27:52,053 iteration 4757 : loss : 0.018711, loss_ce: 0.007718
2022-01-12 00:27:53,712 iteration 4758 : loss : 0.023237, loss_ce: 0.007304
2022-01-12 00:27:55,223 iteration 4759 : loss : 0.018121, loss_ce: 0.007320
2022-01-12 00:27:55,223 Training Data Eval:
2022-01-12 00:28:03,252   Average segmentation loss on training set: 0.0135
2022-01-12 00:28:03,252 Validation Data Eval:
2022-01-12 00:28:06,017   Average segmentation loss on validation set: 0.0615
2022-01-12 00:28:07,595 iteration 4760 : loss : 0.033141, loss_ce: 0.013013
 70%|██████████████████▉        | 280/400 [2:15:38<1:01:15, 30.63s/it]2022-01-12 00:28:09,222 iteration 4761 : loss : 0.019868, loss_ce: 0.008261
2022-01-12 00:28:10,845 iteration 4762 : loss : 0.021115, loss_ce: 0.006144
2022-01-12 00:28:12,436 iteration 4763 : loss : 0.028849, loss_ce: 0.010466
2022-01-12 00:28:14,077 iteration 4764 : loss : 0.018907, loss_ce: 0.006310
2022-01-12 00:28:15,698 iteration 4765 : loss : 0.018036, loss_ce: 0.006613
2022-01-12 00:28:17,395 iteration 4766 : loss : 0.027532, loss_ce: 0.007779
2022-01-12 00:28:19,071 iteration 4767 : loss : 0.024415, loss_ce: 0.012210
2022-01-12 00:28:20,662 iteration 4768 : loss : 0.023539, loss_ce: 0.009298
2022-01-12 00:28:22,176 iteration 4769 : loss : 0.017889, loss_ce: 0.006507
2022-01-12 00:28:23,808 iteration 4770 : loss : 0.021323, loss_ce: 0.010061
2022-01-12 00:28:25,383 iteration 4771 : loss : 0.038950, loss_ce: 0.015478
2022-01-12 00:28:26,917 iteration 4772 : loss : 0.023447, loss_ce: 0.008943
2022-01-12 00:28:28,423 iteration 4773 : loss : 0.015536, loss_ce: 0.006762
2022-01-12 00:28:30,036 iteration 4774 : loss : 0.033664, loss_ce: 0.010985
2022-01-12 00:28:31,549 iteration 4775 : loss : 0.017860, loss_ce: 0.006672
2022-01-12 00:28:33,117 iteration 4776 : loss : 0.028421, loss_ce: 0.009726
2022-01-12 00:28:34,676 iteration 4777 : loss : 0.016115, loss_ce: 0.005301
 70%|████████████████████▎        | 281/400 [2:16:05<58:38, 29.56s/it]2022-01-12 00:28:36,312 iteration 4778 : loss : 0.021886, loss_ce: 0.010947
2022-01-12 00:28:37,846 iteration 4779 : loss : 0.019810, loss_ce: 0.009660
2022-01-12 00:28:39,382 iteration 4780 : loss : 0.022769, loss_ce: 0.010745
2022-01-12 00:28:40,990 iteration 4781 : loss : 0.024626, loss_ce: 0.008319
2022-01-12 00:28:42,601 iteration 4782 : loss : 0.029408, loss_ce: 0.011499
2022-01-12 00:28:44,158 iteration 4783 : loss : 0.022633, loss_ce: 0.010619
2022-01-12 00:28:45,778 iteration 4784 : loss : 0.035051, loss_ce: 0.011787
2022-01-12 00:28:47,350 iteration 4785 : loss : 0.022102, loss_ce: 0.008843
2022-01-12 00:28:48,917 iteration 4786 : loss : 0.017012, loss_ce: 0.007160
2022-01-12 00:28:50,477 iteration 4787 : loss : 0.042606, loss_ce: 0.013695
2022-01-12 00:28:52,062 iteration 4788 : loss : 0.023284, loss_ce: 0.006362
2022-01-12 00:28:53,545 iteration 4789 : loss : 0.017006, loss_ce: 0.006471
2022-01-12 00:28:55,020 iteration 4790 : loss : 0.017925, loss_ce: 0.006594
2022-01-12 00:28:56,605 iteration 4791 : loss : 0.029775, loss_ce: 0.013004
2022-01-12 00:28:58,186 iteration 4792 : loss : 0.021651, loss_ce: 0.008142
2022-01-12 00:28:59,825 iteration 4793 : loss : 0.022027, loss_ce: 0.007738
2022-01-12 00:29:01,394 iteration 4794 : loss : 0.017695, loss_ce: 0.005356
 70%|████████████████████▍        | 282/400 [2:16:32<56:28, 28.71s/it]2022-01-12 00:29:03,036 iteration 4795 : loss : 0.038685, loss_ce: 0.014081
2022-01-12 00:29:04,631 iteration 4796 : loss : 0.030859, loss_ce: 0.009547
2022-01-12 00:29:06,207 iteration 4797 : loss : 0.019728, loss_ce: 0.007456
2022-01-12 00:29:07,821 iteration 4798 : loss : 0.024484, loss_ce: 0.009317
2022-01-12 00:29:09,375 iteration 4799 : loss : 0.028290, loss_ce: 0.006796
2022-01-12 00:29:10,970 iteration 4800 : loss : 0.024512, loss_ce: 0.009657
2022-01-12 00:29:12,527 iteration 4801 : loss : 0.022829, loss_ce: 0.010683
2022-01-12 00:29:14,156 iteration 4802 : loss : 0.018859, loss_ce: 0.007549
2022-01-12 00:29:15,719 iteration 4803 : loss : 0.024568, loss_ce: 0.009104
2022-01-12 00:29:17,312 iteration 4804 : loss : 0.021135, loss_ce: 0.008843
2022-01-12 00:29:18,961 iteration 4805 : loss : 0.028165, loss_ce: 0.012683
2022-01-12 00:29:20,562 iteration 4806 : loss : 0.040903, loss_ce: 0.010617
2022-01-12 00:29:22,085 iteration 4807 : loss : 0.021059, loss_ce: 0.006401
2022-01-12 00:29:23,716 iteration 4808 : loss : 0.023310, loss_ce: 0.008812
2022-01-12 00:29:25,254 iteration 4809 : loss : 0.019793, loss_ce: 0.010008
2022-01-12 00:29:26,839 iteration 4810 : loss : 0.022048, loss_ce: 0.007826
2022-01-12 00:29:28,446 iteration 4811 : loss : 0.024540, loss_ce: 0.010183
 71%|████████████████████▌        | 283/400 [2:16:59<55:01, 28.21s/it]2022-01-12 00:29:30,031 iteration 4812 : loss : 0.022405, loss_ce: 0.007747
2022-01-12 00:29:31,570 iteration 4813 : loss : 0.015759, loss_ce: 0.006253
2022-01-12 00:29:33,272 iteration 4814 : loss : 0.040086, loss_ce: 0.021300
2022-01-12 00:29:34,832 iteration 4815 : loss : 0.016899, loss_ce: 0.006236
2022-01-12 00:29:36,356 iteration 4816 : loss : 0.031955, loss_ce: 0.015230
2022-01-12 00:29:37,866 iteration 4817 : loss : 0.017042, loss_ce: 0.007331
2022-01-12 00:29:39,406 iteration 4818 : loss : 0.020222, loss_ce: 0.007125
2022-01-12 00:29:41,002 iteration 4819 : loss : 0.019882, loss_ce: 0.006508
2022-01-12 00:29:42,541 iteration 4820 : loss : 0.020520, loss_ce: 0.009525
2022-01-12 00:29:44,115 iteration 4821 : loss : 0.019428, loss_ce: 0.006456
2022-01-12 00:29:45,739 iteration 4822 : loss : 0.036085, loss_ce: 0.015831
2022-01-12 00:29:47,312 iteration 4823 : loss : 0.028371, loss_ce: 0.006707
2022-01-12 00:29:48,842 iteration 4824 : loss : 0.023721, loss_ce: 0.007672
2022-01-12 00:29:50,468 iteration 4825 : loss : 0.018793, loss_ce: 0.008948
2022-01-12 00:29:52,193 iteration 4826 : loss : 0.029124, loss_ce: 0.013505
2022-01-12 00:29:53,715 iteration 4827 : loss : 0.020395, loss_ce: 0.010732
2022-01-12 00:29:55,233 iteration 4828 : loss : 0.018159, loss_ce: 0.007010
 71%|████████████████████▌        | 284/400 [2:17:26<53:42, 27.78s/it]2022-01-12 00:29:56,815 iteration 4829 : loss : 0.023746, loss_ce: 0.014018
2022-01-12 00:29:58,438 iteration 4830 : loss : 0.023732, loss_ce: 0.009941
2022-01-12 00:30:00,039 iteration 4831 : loss : 0.026704, loss_ce: 0.014795
2022-01-12 00:30:01,647 iteration 4832 : loss : 0.021012, loss_ce: 0.008092
2022-01-12 00:30:03,206 iteration 4833 : loss : 0.018661, loss_ce: 0.009013
2022-01-12 00:30:04,871 iteration 4834 : loss : 0.016914, loss_ce: 0.006183
2022-01-12 00:30:06,417 iteration 4835 : loss : 0.031073, loss_ce: 0.012441
2022-01-12 00:30:07,963 iteration 4836 : loss : 0.020486, loss_ce: 0.005885
2022-01-12 00:30:09,545 iteration 4837 : loss : 0.021616, loss_ce: 0.006325
2022-01-12 00:30:11,127 iteration 4838 : loss : 0.033949, loss_ce: 0.014493
2022-01-12 00:30:12,602 iteration 4839 : loss : 0.023882, loss_ce: 0.009141
2022-01-12 00:30:14,147 iteration 4840 : loss : 0.021607, loss_ce: 0.007832
2022-01-12 00:30:15,749 iteration 4841 : loss : 0.017898, loss_ce: 0.006475
2022-01-12 00:30:17,326 iteration 4842 : loss : 0.033942, loss_ce: 0.014066
2022-01-12 00:30:18,862 iteration 4843 : loss : 0.022332, loss_ce: 0.007576
2022-01-12 00:30:20,485 iteration 4844 : loss : 0.017872, loss_ce: 0.005280
2022-01-12 00:30:20,485 Training Data Eval:
2022-01-12 00:30:28,533   Average segmentation loss on training set: 0.0128
2022-01-12 00:30:28,534 Validation Data Eval:
2022-01-12 00:30:31,305   Average segmentation loss on validation set: 0.0717
2022-01-12 00:30:32,841 iteration 4845 : loss : 0.020493, loss_ce: 0.006866
 71%|████████████████████▋        | 285/400 [2:18:03<58:54, 30.73s/it]2022-01-12 00:30:34,458 iteration 4846 : loss : 0.018310, loss_ce: 0.007885
2022-01-12 00:30:35,983 iteration 4847 : loss : 0.015125, loss_ce: 0.005216
2022-01-12 00:30:37,645 iteration 4848 : loss : 0.027620, loss_ce: 0.011989
2022-01-12 00:30:39,200 iteration 4849 : loss : 0.021409, loss_ce: 0.007015
2022-01-12 00:30:40,703 iteration 4850 : loss : 0.015621, loss_ce: 0.007276
2022-01-12 00:30:42,365 iteration 4851 : loss : 0.029454, loss_ce: 0.009674
2022-01-12 00:30:43,929 iteration 4852 : loss : 0.020516, loss_ce: 0.009922
2022-01-12 00:30:45,534 iteration 4853 : loss : 0.034679, loss_ce: 0.007285
2022-01-12 00:30:47,240 iteration 4854 : loss : 0.029228, loss_ce: 0.009146
2022-01-12 00:30:48,817 iteration 4855 : loss : 0.025798, loss_ce: 0.008680
2022-01-12 00:30:50,325 iteration 4856 : loss : 0.018837, loss_ce: 0.006373
2022-01-12 00:30:51,941 iteration 4857 : loss : 0.019083, loss_ce: 0.009165
2022-01-12 00:30:53,542 iteration 4858 : loss : 0.030708, loss_ce: 0.014180
2022-01-12 00:30:55,118 iteration 4859 : loss : 0.020501, loss_ce: 0.008414
2022-01-12 00:30:56,714 iteration 4860 : loss : 0.032770, loss_ce: 0.012823
2022-01-12 00:30:58,284 iteration 4861 : loss : 0.017484, loss_ce: 0.006937
2022-01-12 00:30:59,805 iteration 4862 : loss : 0.016803, loss_ce: 0.006789
 72%|████████████████████▋        | 286/400 [2:18:30<56:14, 29.60s/it]2022-01-12 00:31:01,363 iteration 4863 : loss : 0.022101, loss_ce: 0.008863
2022-01-12 00:31:02,975 iteration 4864 : loss : 0.029669, loss_ce: 0.012246
2022-01-12 00:31:04,632 iteration 4865 : loss : 0.036405, loss_ce: 0.011688
2022-01-12 00:31:06,181 iteration 4866 : loss : 0.017429, loss_ce: 0.006827
2022-01-12 00:31:07,762 iteration 4867 : loss : 0.024609, loss_ce: 0.010815
2022-01-12 00:31:09,298 iteration 4868 : loss : 0.019797, loss_ce: 0.009162
2022-01-12 00:31:10,838 iteration 4869 : loss : 0.020262, loss_ce: 0.008498
2022-01-12 00:31:12,472 iteration 4870 : loss : 0.019975, loss_ce: 0.005990
2022-01-12 00:31:14,055 iteration 4871 : loss : 0.015283, loss_ce: 0.005981
2022-01-12 00:31:15,560 iteration 4872 : loss : 0.018403, loss_ce: 0.007040
2022-01-12 00:31:17,056 iteration 4873 : loss : 0.020194, loss_ce: 0.006649
2022-01-12 00:31:18,545 iteration 4874 : loss : 0.014705, loss_ce: 0.006591
2022-01-12 00:31:20,106 iteration 4875 : loss : 0.021103, loss_ce: 0.007792
2022-01-12 00:31:21,609 iteration 4876 : loss : 0.019704, loss_ce: 0.007292
2022-01-12 00:31:23,261 iteration 4877 : loss : 0.027261, loss_ce: 0.010636
2022-01-12 00:31:24,816 iteration 4878 : loss : 0.020732, loss_ce: 0.007418
2022-01-12 00:31:26,377 iteration 4879 : loss : 0.017864, loss_ce: 0.007066
 72%|████████████████████▊        | 287/400 [2:18:57<54:02, 28.69s/it]2022-01-12 00:31:28,043 iteration 4880 : loss : 0.045450, loss_ce: 0.024484
2022-01-12 00:31:29,574 iteration 4881 : loss : 0.025820, loss_ce: 0.007964
2022-01-12 00:31:31,135 iteration 4882 : loss : 0.018825, loss_ce: 0.005361
2022-01-12 00:31:32,701 iteration 4883 : loss : 0.022535, loss_ce: 0.006708
2022-01-12 00:31:34,275 iteration 4884 : loss : 0.014829, loss_ce: 0.008318
2022-01-12 00:31:35,954 iteration 4885 : loss : 0.031457, loss_ce: 0.010342
2022-01-12 00:31:37,557 iteration 4886 : loss : 0.022360, loss_ce: 0.007774
2022-01-12 00:31:39,191 iteration 4887 : loss : 0.021466, loss_ce: 0.008432
2022-01-12 00:31:40,743 iteration 4888 : loss : 0.026651, loss_ce: 0.007842
2022-01-12 00:31:42,295 iteration 4889 : loss : 0.019896, loss_ce: 0.007574
2022-01-12 00:31:43,922 iteration 4890 : loss : 0.022351, loss_ce: 0.008078
2022-01-12 00:31:45,546 iteration 4891 : loss : 0.022140, loss_ce: 0.012598
2022-01-12 00:31:47,063 iteration 4892 : loss : 0.020602, loss_ce: 0.008753
2022-01-12 00:31:48,603 iteration 4893 : loss : 0.016688, loss_ce: 0.007440
2022-01-12 00:31:50,165 iteration 4894 : loss : 0.024857, loss_ce: 0.006689
2022-01-12 00:31:51,694 iteration 4895 : loss : 0.021900, loss_ce: 0.008662
2022-01-12 00:31:53,274 iteration 4896 : loss : 0.025962, loss_ce: 0.009784
 72%|████████████████████▉        | 288/400 [2:19:24<52:33, 28.15s/it]2022-01-12 00:31:54,856 iteration 4897 : loss : 0.026652, loss_ce: 0.010024
2022-01-12 00:31:56,443 iteration 4898 : loss : 0.030585, loss_ce: 0.012741
2022-01-12 00:31:57,883 iteration 4899 : loss : 0.017413, loss_ce: 0.007268
2022-01-12 00:31:59,530 iteration 4900 : loss : 0.042262, loss_ce: 0.012709
2022-01-12 00:32:01,059 iteration 4901 : loss : 0.016372, loss_ce: 0.004205
2022-01-12 00:32:02,691 iteration 4902 : loss : 0.035835, loss_ce: 0.013088
2022-01-12 00:32:04,331 iteration 4903 : loss : 0.029772, loss_ce: 0.012752
2022-01-12 00:32:05,950 iteration 4904 : loss : 0.031705, loss_ce: 0.016816
2022-01-12 00:32:07,543 iteration 4905 : loss : 0.032180, loss_ce: 0.010289
2022-01-12 00:32:09,152 iteration 4906 : loss : 0.029232, loss_ce: 0.012535
2022-01-12 00:32:10,633 iteration 4907 : loss : 0.016495, loss_ce: 0.006435
2022-01-12 00:32:12,172 iteration 4908 : loss : 0.019855, loss_ce: 0.008438
2022-01-12 00:32:13,748 iteration 4909 : loss : 0.017690, loss_ce: 0.006693
2022-01-12 00:32:15,306 iteration 4910 : loss : 0.016816, loss_ce: 0.005899
2022-01-12 00:32:16,841 iteration 4911 : loss : 0.019425, loss_ce: 0.006443
2022-01-12 00:32:18,456 iteration 4912 : loss : 0.032230, loss_ce: 0.012749
2022-01-12 00:32:20,014 iteration 4913 : loss : 0.023797, loss_ce: 0.007606
 72%|████████████████████▉        | 289/400 [2:19:50<51:18, 27.73s/it]2022-01-12 00:32:21,558 iteration 4914 : loss : 0.019715, loss_ce: 0.007093
2022-01-12 00:32:23,147 iteration 4915 : loss : 0.021300, loss_ce: 0.009767
2022-01-12 00:32:24,760 iteration 4916 : loss : 0.024646, loss_ce: 0.009263
2022-01-12 00:32:26,322 iteration 4917 : loss : 0.019783, loss_ce: 0.006372
2022-01-12 00:32:27,910 iteration 4918 : loss : 0.020720, loss_ce: 0.009230
2022-01-12 00:32:29,473 iteration 4919 : loss : 0.026503, loss_ce: 0.014461
2022-01-12 00:32:31,096 iteration 4920 : loss : 0.039171, loss_ce: 0.014309
2022-01-12 00:32:32,602 iteration 4921 : loss : 0.022577, loss_ce: 0.009167
2022-01-12 00:32:34,218 iteration 4922 : loss : 0.021156, loss_ce: 0.007321
2022-01-12 00:32:35,792 iteration 4923 : loss : 0.019747, loss_ce: 0.008044
2022-01-12 00:32:37,376 iteration 4924 : loss : 0.021709, loss_ce: 0.009954
2022-01-12 00:32:38,914 iteration 4925 : loss : 0.019879, loss_ce: 0.007793
2022-01-12 00:32:40,385 iteration 4926 : loss : 0.014777, loss_ce: 0.006953
2022-01-12 00:32:41,965 iteration 4927 : loss : 0.016395, loss_ce: 0.005829
2022-01-12 00:32:43,559 iteration 4928 : loss : 0.024223, loss_ce: 0.012252
2022-01-12 00:32:45,078 iteration 4929 : loss : 0.032659, loss_ce: 0.011440
2022-01-12 00:32:45,078 Training Data Eval:
2022-01-12 00:32:53,119   Average segmentation loss on training set: 0.0128
2022-01-12 00:32:53,119 Validation Data Eval:
2022-01-12 00:32:55,890   Average segmentation loss on validation set: 0.0621
2022-01-12 00:32:57,478 iteration 4930 : loss : 0.022452, loss_ce: 0.007051
 72%|█████████████████████        | 290/400 [2:20:28<56:11, 30.65s/it]2022-01-12 00:32:59,128 iteration 4931 : loss : 0.020235, loss_ce: 0.009411
2022-01-12 00:33:00,734 iteration 4932 : loss : 0.025398, loss_ce: 0.007253
2022-01-12 00:33:02,252 iteration 4933 : loss : 0.029183, loss_ce: 0.011285
2022-01-12 00:33:03,829 iteration 4934 : loss : 0.017689, loss_ce: 0.006099
2022-01-12 00:33:05,380 iteration 4935 : loss : 0.014532, loss_ce: 0.003647
2022-01-12 00:33:06,975 iteration 4936 : loss : 0.021788, loss_ce: 0.008312
2022-01-12 00:33:08,518 iteration 4937 : loss : 0.019826, loss_ce: 0.007652
2022-01-12 00:33:10,047 iteration 4938 : loss : 0.022736, loss_ce: 0.010420
2022-01-12 00:33:11,720 iteration 4939 : loss : 0.026921, loss_ce: 0.012289
2022-01-12 00:33:13,249 iteration 4940 : loss : 0.020704, loss_ce: 0.008412
2022-01-12 00:33:14,831 iteration 4941 : loss : 0.018272, loss_ce: 0.007448
2022-01-12 00:33:16,578 iteration 4942 : loss : 0.025981, loss_ce: 0.012131
2022-01-12 00:33:18,109 iteration 4943 : loss : 0.033288, loss_ce: 0.012360
2022-01-12 00:33:19,640 iteration 4944 : loss : 0.025667, loss_ce: 0.015930
2022-01-12 00:33:21,196 iteration 4945 : loss : 0.016935, loss_ce: 0.006920
2022-01-12 00:33:22,709 iteration 4946 : loss : 0.017922, loss_ce: 0.006602
2022-01-12 00:33:24,344 iteration 4947 : loss : 0.032352, loss_ce: 0.013280
 73%|█████████████████████        | 291/400 [2:20:55<53:37, 29.51s/it]2022-01-12 00:33:25,968 iteration 4948 : loss : 0.026696, loss_ce: 0.009607
2022-01-12 00:33:27,640 iteration 4949 : loss : 0.030804, loss_ce: 0.014802
2022-01-12 00:33:29,177 iteration 4950 : loss : 0.020783, loss_ce: 0.010535
2022-01-12 00:33:30,737 iteration 4951 : loss : 0.023882, loss_ce: 0.009331
2022-01-12 00:33:32,302 iteration 4952 : loss : 0.021579, loss_ce: 0.008399
2022-01-12 00:33:33,952 iteration 4953 : loss : 0.023319, loss_ce: 0.006860
2022-01-12 00:33:35,441 iteration 4954 : loss : 0.024111, loss_ce: 0.008134
2022-01-12 00:33:36,970 iteration 4955 : loss : 0.019910, loss_ce: 0.009695
2022-01-12 00:33:38,567 iteration 4956 : loss : 0.032051, loss_ce: 0.010350
2022-01-12 00:33:40,153 iteration 4957 : loss : 0.020062, loss_ce: 0.008050
2022-01-12 00:33:41,860 iteration 4958 : loss : 0.025716, loss_ce: 0.011299
2022-01-12 00:33:43,486 iteration 4959 : loss : 0.027275, loss_ce: 0.011578
2022-01-12 00:33:45,154 iteration 4960 : loss : 0.028621, loss_ce: 0.012541
2022-01-12 00:33:46,704 iteration 4961 : loss : 0.024616, loss_ce: 0.005893
2022-01-12 00:33:48,279 iteration 4962 : loss : 0.018566, loss_ce: 0.008645
2022-01-12 00:33:49,877 iteration 4963 : loss : 0.036480, loss_ce: 0.015330
2022-01-12 00:33:51,473 iteration 4964 : loss : 0.020835, loss_ce: 0.007395
 73%|█████████████████████▏       | 292/400 [2:21:22<51:50, 28.80s/it]2022-01-12 00:33:53,048 iteration 4965 : loss : 0.022509, loss_ce: 0.007334
2022-01-12 00:33:54,610 iteration 4966 : loss : 0.022745, loss_ce: 0.008973
2022-01-12 00:33:56,176 iteration 4967 : loss : 0.022408, loss_ce: 0.006696
2022-01-12 00:33:57,730 iteration 4968 : loss : 0.029002, loss_ce: 0.008847
2022-01-12 00:33:59,314 iteration 4969 : loss : 0.030734, loss_ce: 0.012197
2022-01-12 00:34:00,905 iteration 4970 : loss : 0.026208, loss_ce: 0.015960
2022-01-12 00:34:02,451 iteration 4971 : loss : 0.021145, loss_ce: 0.008729
2022-01-12 00:34:03,976 iteration 4972 : loss : 0.021822, loss_ce: 0.007127
2022-01-12 00:34:05,528 iteration 4973 : loss : 0.018617, loss_ce: 0.006750
2022-01-12 00:34:07,116 iteration 4974 : loss : 0.018810, loss_ce: 0.009754
2022-01-12 00:34:08,752 iteration 4975 : loss : 0.015043, loss_ce: 0.005851
2022-01-12 00:34:10,276 iteration 4976 : loss : 0.023110, loss_ce: 0.007997
2022-01-12 00:34:11,819 iteration 4977 : loss : 0.019091, loss_ce: 0.008667
2022-01-12 00:34:13,389 iteration 4978 : loss : 0.019006, loss_ce: 0.008551
2022-01-12 00:34:14,916 iteration 4979 : loss : 0.019590, loss_ce: 0.007861
2022-01-12 00:34:16,417 iteration 4980 : loss : 0.015061, loss_ce: 0.005319
2022-01-12 00:34:17,967 iteration 4981 : loss : 0.027685, loss_ce: 0.009719
 73%|█████████████████████▏       | 293/400 [2:21:48<50:07, 28.11s/it]2022-01-12 00:34:19,590 iteration 4982 : loss : 0.037181, loss_ce: 0.010140
2022-01-12 00:34:21,079 iteration 4983 : loss : 0.019580, loss_ce: 0.006492
2022-01-12 00:34:22,651 iteration 4984 : loss : 0.029275, loss_ce: 0.008702
2022-01-12 00:34:24,152 iteration 4985 : loss : 0.017224, loss_ce: 0.005574
2022-01-12 00:34:25,678 iteration 4986 : loss : 0.017049, loss_ce: 0.005932
2022-01-12 00:34:27,253 iteration 4987 : loss : 0.017616, loss_ce: 0.008553
2022-01-12 00:34:28,836 iteration 4988 : loss : 0.018844, loss_ce: 0.009200
2022-01-12 00:34:30,388 iteration 4989 : loss : 0.020825, loss_ce: 0.008724
2022-01-12 00:34:31,986 iteration 4990 : loss : 0.019739, loss_ce: 0.009470
2022-01-12 00:34:33,527 iteration 4991 : loss : 0.019255, loss_ce: 0.005999
2022-01-12 00:34:35,102 iteration 4992 : loss : 0.021114, loss_ce: 0.006033
2022-01-12 00:34:36,672 iteration 4993 : loss : 0.033605, loss_ce: 0.013878
2022-01-12 00:34:38,338 iteration 4994 : loss : 0.030774, loss_ce: 0.013129
2022-01-12 00:34:39,939 iteration 4995 : loss : 0.034609, loss_ce: 0.016615
2022-01-12 00:34:41,465 iteration 4996 : loss : 0.018543, loss_ce: 0.005113
2022-01-12 00:34:43,056 iteration 4997 : loss : 0.021378, loss_ce: 0.007817
2022-01-12 00:34:44,630 iteration 4998 : loss : 0.019872, loss_ce: 0.008009
 74%|█████████████████████▎       | 294/400 [2:22:15<48:53, 27.68s/it]2022-01-12 00:34:46,226 iteration 4999 : loss : 0.019115, loss_ce: 0.008391
2022-01-12 00:34:47,724 iteration 5000 : loss : 0.020154, loss_ce: 0.006562
2022-01-12 00:34:49,325 iteration 5001 : loss : 0.021663, loss_ce: 0.006271
2022-01-12 00:34:50,967 iteration 5002 : loss : 0.024574, loss_ce: 0.008564
2022-01-12 00:34:52,612 iteration 5003 : loss : 0.026691, loss_ce: 0.011760
2022-01-12 00:34:54,154 iteration 5004 : loss : 0.015156, loss_ce: 0.006589
2022-01-12 00:34:55,747 iteration 5005 : loss : 0.022915, loss_ce: 0.009402
2022-01-12 00:34:57,258 iteration 5006 : loss : 0.016776, loss_ce: 0.006613
2022-01-12 00:34:58,848 iteration 5007 : loss : 0.026790, loss_ce: 0.006354
2022-01-12 00:35:00,349 iteration 5008 : loss : 0.025446, loss_ce: 0.009145
2022-01-12 00:35:01,864 iteration 5009 : loss : 0.015809, loss_ce: 0.007749
2022-01-12 00:35:03,465 iteration 5010 : loss : 0.019852, loss_ce: 0.009057
2022-01-12 00:35:04,988 iteration 5011 : loss : 0.019318, loss_ce: 0.008560
2022-01-12 00:35:06,523 iteration 5012 : loss : 0.035576, loss_ce: 0.011825
2022-01-12 00:35:08,161 iteration 5013 : loss : 0.020664, loss_ce: 0.009019
2022-01-12 00:35:09,663 iteration 5014 : loss : 0.020431, loss_ce: 0.007146
2022-01-12 00:35:09,664 Training Data Eval:
2022-01-12 00:35:17,703   Average segmentation loss on training set: 0.0123
2022-01-12 00:35:17,703 Validation Data Eval:
2022-01-12 00:35:20,470   Average segmentation loss on validation set: 0.0714
2022-01-12 00:35:22,035 iteration 5015 : loss : 0.031667, loss_ce: 0.011840
 74%|█████████████████████▍       | 295/400 [2:22:52<53:32, 30.59s/it]2022-01-12 00:35:23,668 iteration 5016 : loss : 0.032236, loss_ce: 0.016232
2022-01-12 00:35:25,155 iteration 5017 : loss : 0.022855, loss_ce: 0.009371
2022-01-12 00:35:26,820 iteration 5018 : loss : 0.040378, loss_ce: 0.021634
2022-01-12 00:35:28,323 iteration 5019 : loss : 0.017718, loss_ce: 0.006734
2022-01-12 00:35:29,946 iteration 5020 : loss : 0.031532, loss_ce: 0.009510
2022-01-12 00:35:31,503 iteration 5021 : loss : 0.032326, loss_ce: 0.012446
2022-01-12 00:35:33,058 iteration 5022 : loss : 0.018862, loss_ce: 0.006248
2022-01-12 00:35:34,622 iteration 5023 : loss : 0.026987, loss_ce: 0.012764
2022-01-12 00:35:36,261 iteration 5024 : loss : 0.025315, loss_ce: 0.009373
2022-01-12 00:35:37,836 iteration 5025 : loss : 0.022267, loss_ce: 0.008360
2022-01-12 00:35:39,454 iteration 5026 : loss : 0.021346, loss_ce: 0.009354
2022-01-12 00:35:40,974 iteration 5027 : loss : 0.019199, loss_ce: 0.007646
2022-01-12 00:35:42,478 iteration 5028 : loss : 0.020532, loss_ce: 0.008928
2022-01-12 00:35:44,107 iteration 5029 : loss : 0.041381, loss_ce: 0.019585
2022-01-12 00:35:45,641 iteration 5030 : loss : 0.019378, loss_ce: 0.007858
2022-01-12 00:35:47,246 iteration 5031 : loss : 0.031868, loss_ce: 0.009071
2022-01-12 00:35:48,743 iteration 5032 : loss : 0.017504, loss_ce: 0.006860
 74%|█████████████████████▍       | 296/400 [2:23:19<51:00, 29.43s/it]2022-01-12 00:35:50,290 iteration 5033 : loss : 0.022402, loss_ce: 0.008368
2022-01-12 00:35:51,812 iteration 5034 : loss : 0.022114, loss_ce: 0.006922
2022-01-12 00:35:53,367 iteration 5035 : loss : 0.022904, loss_ce: 0.007804
2022-01-12 00:35:54,894 iteration 5036 : loss : 0.021044, loss_ce: 0.008563
2022-01-12 00:35:56,444 iteration 5037 : loss : 0.020725, loss_ce: 0.007027
2022-01-12 00:35:57,979 iteration 5038 : loss : 0.020907, loss_ce: 0.006614
2022-01-12 00:35:59,545 iteration 5039 : loss : 0.029999, loss_ce: 0.012357
2022-01-12 00:36:01,138 iteration 5040 : loss : 0.016154, loss_ce: 0.006572
2022-01-12 00:36:02,695 iteration 5041 : loss : 0.021494, loss_ce: 0.010008
2022-01-12 00:36:04,341 iteration 5042 : loss : 0.028236, loss_ce: 0.010007
2022-01-12 00:36:05,813 iteration 5043 : loss : 0.015894, loss_ce: 0.005815
2022-01-12 00:36:07,420 iteration 5044 : loss : 0.022995, loss_ce: 0.008871
2022-01-12 00:36:09,003 iteration 5045 : loss : 0.031105, loss_ce: 0.020427
2022-01-12 00:36:10,569 iteration 5046 : loss : 0.031446, loss_ce: 0.008419
2022-01-12 00:36:12,072 iteration 5047 : loss : 0.012563, loss_ce: 0.003878
2022-01-12 00:36:13,567 iteration 5048 : loss : 0.013957, loss_ce: 0.006796
2022-01-12 00:36:15,052 iteration 5049 : loss : 0.014352, loss_ce: 0.006595
 74%|█████████████████████▌       | 297/400 [2:23:45<48:54, 28.49s/it]2022-01-12 00:36:16,679 iteration 5050 : loss : 0.021635, loss_ce: 0.007056
2022-01-12 00:36:18,313 iteration 5051 : loss : 0.027240, loss_ce: 0.009152
2022-01-12 00:36:19,942 iteration 5052 : loss : 0.032078, loss_ce: 0.014498
2022-01-12 00:36:21,579 iteration 5053 : loss : 0.034172, loss_ce: 0.011445
2022-01-12 00:36:23,123 iteration 5054 : loss : 0.022697, loss_ce: 0.006219
2022-01-12 00:36:24,702 iteration 5055 : loss : 0.020896, loss_ce: 0.007737
2022-01-12 00:36:26,292 iteration 5056 : loss : 0.020271, loss_ce: 0.007912
2022-01-12 00:36:27,942 iteration 5057 : loss : 0.048376, loss_ce: 0.026268
2022-01-12 00:36:29,588 iteration 5058 : loss : 0.031467, loss_ce: 0.011696
2022-01-12 00:36:31,147 iteration 5059 : loss : 0.019011, loss_ce: 0.007833
2022-01-12 00:36:32,744 iteration 5060 : loss : 0.021431, loss_ce: 0.009644
2022-01-12 00:36:34,353 iteration 5061 : loss : 0.021642, loss_ce: 0.008363
2022-01-12 00:36:35,998 iteration 5062 : loss : 0.033444, loss_ce: 0.012824
2022-01-12 00:36:37,650 iteration 5063 : loss : 0.034051, loss_ce: 0.012744
2022-01-12 00:36:39,206 iteration 5064 : loss : 0.022030, loss_ce: 0.007879
2022-01-12 00:36:40,844 iteration 5065 : loss : 0.023460, loss_ce: 0.012601
2022-01-12 00:36:42,357 iteration 5066 : loss : 0.019721, loss_ce: 0.007884
 74%|█████████████████████▌       | 298/400 [2:24:13<47:49, 28.13s/it]2022-01-12 00:36:43,855 iteration 5067 : loss : 0.017776, loss_ce: 0.006600
2022-01-12 00:36:45,470 iteration 5068 : loss : 0.028167, loss_ce: 0.010183
2022-01-12 00:36:47,054 iteration 5069 : loss : 0.024267, loss_ce: 0.012998
2022-01-12 00:36:48,558 iteration 5070 : loss : 0.025705, loss_ce: 0.008753
2022-01-12 00:36:50,158 iteration 5071 : loss : 0.031165, loss_ce: 0.013236
2022-01-12 00:36:51,652 iteration 5072 : loss : 0.017245, loss_ce: 0.006483
2022-01-12 00:36:53,287 iteration 5073 : loss : 0.021977, loss_ce: 0.009570
2022-01-12 00:36:54,830 iteration 5074 : loss : 0.016853, loss_ce: 0.006705
2022-01-12 00:36:56,387 iteration 5075 : loss : 0.027146, loss_ce: 0.010578
2022-01-12 00:36:57,991 iteration 5076 : loss : 0.029004, loss_ce: 0.005091
2022-01-12 00:36:59,563 iteration 5077 : loss : 0.021818, loss_ce: 0.007200
2022-01-12 00:37:01,091 iteration 5078 : loss : 0.021149, loss_ce: 0.010352
2022-01-12 00:37:02,600 iteration 5079 : loss : 0.016511, loss_ce: 0.007076
2022-01-12 00:37:04,137 iteration 5080 : loss : 0.019152, loss_ce: 0.007651
2022-01-12 00:37:05,726 iteration 5081 : loss : 0.024592, loss_ce: 0.006997
2022-01-12 00:37:07,272 iteration 5082 : loss : 0.020991, loss_ce: 0.007791
2022-01-12 00:37:08,860 iteration 5083 : loss : 0.029102, loss_ce: 0.005953
 75%|█████████████████████▋       | 299/400 [2:24:39<46:32, 27.65s/it]2022-01-12 00:37:10,495 iteration 5084 : loss : 0.023509, loss_ce: 0.012047
2022-01-12 00:37:12,146 iteration 5085 : loss : 0.024741, loss_ce: 0.008143
2022-01-12 00:37:13,754 iteration 5086 : loss : 0.016495, loss_ce: 0.005989
2022-01-12 00:37:15,274 iteration 5087 : loss : 0.022033, loss_ce: 0.010019
2022-01-12 00:37:16,803 iteration 5088 : loss : 0.020918, loss_ce: 0.008300
2022-01-12 00:37:18,400 iteration 5089 : loss : 0.019785, loss_ce: 0.006104
2022-01-12 00:37:19,978 iteration 5090 : loss : 0.019403, loss_ce: 0.004160
2022-01-12 00:37:21,621 iteration 5091 : loss : 0.019839, loss_ce: 0.006208
2022-01-12 00:37:23,216 iteration 5092 : loss : 0.031473, loss_ce: 0.010802
2022-01-12 00:37:24,725 iteration 5093 : loss : 0.017261, loss_ce: 0.005270
2022-01-12 00:37:26,330 iteration 5094 : loss : 0.022975, loss_ce: 0.008115
2022-01-12 00:37:27,851 iteration 5095 : loss : 0.017434, loss_ce: 0.006541
2022-01-12 00:37:29,421 iteration 5096 : loss : 0.022446, loss_ce: 0.010689
2022-01-12 00:37:30,927 iteration 5097 : loss : 0.024577, loss_ce: 0.012207
2022-01-12 00:37:32,573 iteration 5098 : loss : 0.022856, loss_ce: 0.007782
2022-01-12 00:37:34,165 iteration 5099 : loss : 0.030090, loss_ce: 0.010506
2022-01-12 00:37:34,165 Training Data Eval:
2022-01-12 00:37:42,182   Average segmentation loss on training set: 0.0124
2022-01-12 00:37:42,183 Validation Data Eval:
2022-01-12 00:37:44,943   Average segmentation loss on validation set: 0.0698
2022-01-12 00:37:46,587 iteration 5100 : loss : 0.025166, loss_ce: 0.006463
 75%|█████████████████████▊       | 300/400 [2:25:17<51:07, 30.67s/it]2022-01-12 00:37:48,294 iteration 5101 : loss : 0.021347, loss_ce: 0.008715
2022-01-12 00:37:49,886 iteration 5102 : loss : 0.018940, loss_ce: 0.006674
2022-01-12 00:37:51,466 iteration 5103 : loss : 0.019601, loss_ce: 0.010045
2022-01-12 00:37:53,109 iteration 5104 : loss : 0.021311, loss_ce: 0.006483
2022-01-12 00:37:54,634 iteration 5105 : loss : 0.019781, loss_ce: 0.006809
2022-01-12 00:37:56,180 iteration 5106 : loss : 0.020667, loss_ce: 0.007415
2022-01-12 00:37:57,810 iteration 5107 : loss : 0.023552, loss_ce: 0.006926
2022-01-12 00:37:59,395 iteration 5108 : loss : 0.020146, loss_ce: 0.010671
2022-01-12 00:38:00,959 iteration 5109 : loss : 0.029004, loss_ce: 0.010421
2022-01-12 00:38:02,484 iteration 5110 : loss : 0.015395, loss_ce: 0.005563
2022-01-12 00:38:04,081 iteration 5111 : loss : 0.024947, loss_ce: 0.008158
2022-01-12 00:38:05,737 iteration 5112 : loss : 0.036460, loss_ce: 0.014511
2022-01-12 00:38:07,329 iteration 5113 : loss : 0.022064, loss_ce: 0.007582
2022-01-12 00:38:08,955 iteration 5114 : loss : 0.019264, loss_ce: 0.006607
2022-01-12 00:38:10,512 iteration 5115 : loss : 0.023182, loss_ce: 0.007642
2022-01-12 00:38:12,048 iteration 5116 : loss : 0.020465, loss_ce: 0.007982
2022-01-12 00:38:13,648 iteration 5117 : loss : 0.026530, loss_ce: 0.013290
 75%|█████████████████████▊       | 301/400 [2:25:44<48:49, 29.59s/it]2022-01-12 00:38:15,294 iteration 5118 : loss : 0.024574, loss_ce: 0.007295
2022-01-12 00:38:16,853 iteration 5119 : loss : 0.018238, loss_ce: 0.008089
2022-01-12 00:38:18,364 iteration 5120 : loss : 0.013649, loss_ce: 0.005578
2022-01-12 00:38:19,991 iteration 5121 : loss : 0.019981, loss_ce: 0.005223
2022-01-12 00:38:21,643 iteration 5122 : loss : 0.019980, loss_ce: 0.007555
2022-01-12 00:38:23,280 iteration 5123 : loss : 0.029166, loss_ce: 0.009463
2022-01-12 00:38:24,796 iteration 5124 : loss : 0.015688, loss_ce: 0.007349
2022-01-12 00:38:26,316 iteration 5125 : loss : 0.018553, loss_ce: 0.006376
2022-01-12 00:38:27,900 iteration 5126 : loss : 0.016101, loss_ce: 0.005619
2022-01-12 00:38:29,452 iteration 5127 : loss : 0.019859, loss_ce: 0.008731
2022-01-12 00:38:30,980 iteration 5128 : loss : 0.028333, loss_ce: 0.008691
2022-01-12 00:38:32,558 iteration 5129 : loss : 0.029853, loss_ce: 0.017438
2022-01-12 00:38:34,122 iteration 5130 : loss : 0.024845, loss_ce: 0.012443
2022-01-12 00:38:35,717 iteration 5131 : loss : 0.016956, loss_ce: 0.006706
2022-01-12 00:38:37,278 iteration 5132 : loss : 0.017582, loss_ce: 0.006467
2022-01-12 00:38:38,897 iteration 5133 : loss : 0.042021, loss_ce: 0.016300
2022-01-12 00:38:40,451 iteration 5134 : loss : 0.023674, loss_ce: 0.006201
 76%|█████████████████████▉       | 302/400 [2:26:11<46:57, 28.75s/it]2022-01-12 00:38:42,037 iteration 5135 : loss : 0.029908, loss_ce: 0.016743
2022-01-12 00:38:43,601 iteration 5136 : loss : 0.013545, loss_ce: 0.004663
2022-01-12 00:38:45,117 iteration 5137 : loss : 0.023721, loss_ce: 0.007955
2022-01-12 00:38:46,713 iteration 5138 : loss : 0.019920, loss_ce: 0.009919
2022-01-12 00:38:48,360 iteration 5139 : loss : 0.026531, loss_ce: 0.010594
2022-01-12 00:38:49,990 iteration 5140 : loss : 0.032022, loss_ce: 0.011397
2022-01-12 00:38:51,515 iteration 5141 : loss : 0.026747, loss_ce: 0.008662
2022-01-12 00:38:53,105 iteration 5142 : loss : 0.017147, loss_ce: 0.006039
2022-01-12 00:38:54,660 iteration 5143 : loss : 0.016872, loss_ce: 0.008587
2022-01-12 00:38:56,304 iteration 5144 : loss : 0.021054, loss_ce: 0.008592
2022-01-12 00:38:57,993 iteration 5145 : loss : 0.034824, loss_ce: 0.015390
2022-01-12 00:38:59,590 iteration 5146 : loss : 0.021321, loss_ce: 0.006866
2022-01-12 00:39:01,097 iteration 5147 : loss : 0.020120, loss_ce: 0.004584
2022-01-12 00:39:02,661 iteration 5148 : loss : 0.020915, loss_ce: 0.008314
2022-01-12 00:39:04,216 iteration 5149 : loss : 0.020675, loss_ce: 0.007352
2022-01-12 00:39:05,772 iteration 5150 : loss : 0.032172, loss_ce: 0.017665
2022-01-12 00:39:07,380 iteration 5151 : loss : 0.020471, loss_ce: 0.010410
 76%|█████████████████████▉       | 303/400 [2:26:38<45:36, 28.21s/it]2022-01-12 00:39:08,960 iteration 5152 : loss : 0.034668, loss_ce: 0.015813
2022-01-12 00:39:10,539 iteration 5153 : loss : 0.023549, loss_ce: 0.009930
2022-01-12 00:39:12,046 iteration 5154 : loss : 0.019136, loss_ce: 0.006727
2022-01-12 00:39:13,570 iteration 5155 : loss : 0.019099, loss_ce: 0.007470
2022-01-12 00:39:15,127 iteration 5156 : loss : 0.031713, loss_ce: 0.009263
2022-01-12 00:39:16,643 iteration 5157 : loss : 0.024148, loss_ce: 0.008861
2022-01-12 00:39:18,177 iteration 5158 : loss : 0.018988, loss_ce: 0.007464
2022-01-12 00:39:19,770 iteration 5159 : loss : 0.025025, loss_ce: 0.009548
2022-01-12 00:39:21,356 iteration 5160 : loss : 0.018450, loss_ce: 0.008207
2022-01-12 00:39:22,942 iteration 5161 : loss : 0.023487, loss_ce: 0.009867
2022-01-12 00:39:24,566 iteration 5162 : loss : 0.021091, loss_ce: 0.006752
2022-01-12 00:39:26,157 iteration 5163 : loss : 0.018723, loss_ce: 0.009703
2022-01-12 00:39:27,680 iteration 5164 : loss : 0.016421, loss_ce: 0.006118
2022-01-12 00:39:29,316 iteration 5165 : loss : 0.028703, loss_ce: 0.009022
2022-01-12 00:39:30,904 iteration 5166 : loss : 0.020896, loss_ce: 0.007063
2022-01-12 00:39:32,512 iteration 5167 : loss : 0.026018, loss_ce: 0.007630
2022-01-12 00:39:34,002 iteration 5168 : loss : 0.016001, loss_ce: 0.007757
 76%|██████████████████████       | 304/400 [2:27:04<44:22, 27.73s/it]2022-01-12 00:39:35,661 iteration 5169 : loss : 0.022112, loss_ce: 0.007157
2022-01-12 00:39:37,219 iteration 5170 : loss : 0.019705, loss_ce: 0.010180
2022-01-12 00:39:38,791 iteration 5171 : loss : 0.025366, loss_ce: 0.008450
2022-01-12 00:39:40,382 iteration 5172 : loss : 0.023439, loss_ce: 0.008159
2022-01-12 00:39:42,023 iteration 5173 : loss : 0.020116, loss_ce: 0.008518
2022-01-12 00:39:43,593 iteration 5174 : loss : 0.013379, loss_ce: 0.003720
2022-01-12 00:39:45,204 iteration 5175 : loss : 0.019949, loss_ce: 0.006618
2022-01-12 00:39:46,857 iteration 5176 : loss : 0.018580, loss_ce: 0.005684
2022-01-12 00:39:48,450 iteration 5177 : loss : 0.017812, loss_ce: 0.007068
2022-01-12 00:39:50,057 iteration 5178 : loss : 0.015558, loss_ce: 0.007426
2022-01-12 00:39:51,602 iteration 5179 : loss : 0.023313, loss_ce: 0.011753
2022-01-12 00:39:53,105 iteration 5180 : loss : 0.016917, loss_ce: 0.007319
2022-01-12 00:39:54,645 iteration 5181 : loss : 0.018290, loss_ce: 0.006377
2022-01-12 00:39:56,207 iteration 5182 : loss : 0.018082, loss_ce: 0.008010
2022-01-12 00:39:57,758 iteration 5183 : loss : 0.015307, loss_ce: 0.005040
2022-01-12 00:39:59,285 iteration 5184 : loss : 0.019927, loss_ce: 0.009682
2022-01-12 00:39:59,285 Training Data Eval:
2022-01-12 00:40:07,312   Average segmentation loss on training set: 0.0118
2022-01-12 00:40:07,312 Validation Data Eval:
2022-01-12 00:40:10,078   Average segmentation loss on validation set: 0.0760
2022-01-12 00:40:11,630 iteration 5185 : loss : 0.015070, loss_ce: 0.006310
 76%|██████████████████████       | 305/400 [2:27:42<48:36, 30.70s/it]2022-01-12 00:40:13,290 iteration 5186 : loss : 0.025353, loss_ce: 0.006955
2022-01-12 00:40:14,814 iteration 5187 : loss : 0.019040, loss_ce: 0.007891
2022-01-12 00:40:16,385 iteration 5188 : loss : 0.024052, loss_ce: 0.006387
2022-01-12 00:40:17,987 iteration 5189 : loss : 0.024954, loss_ce: 0.007364
2022-01-12 00:40:19,668 iteration 5190 : loss : 0.029806, loss_ce: 0.010002
2022-01-12 00:40:21,224 iteration 5191 : loss : 0.019528, loss_ce: 0.007163
2022-01-12 00:40:22,795 iteration 5192 : loss : 0.019913, loss_ce: 0.009202
2022-01-12 00:40:24,409 iteration 5193 : loss : 0.019168, loss_ce: 0.009019
2022-01-12 00:40:26,005 iteration 5194 : loss : 0.021030, loss_ce: 0.006123
2022-01-12 00:40:27,565 iteration 5195 : loss : 0.023179, loss_ce: 0.005866
2022-01-12 00:40:29,228 iteration 5196 : loss : 0.025928, loss_ce: 0.007755
2022-01-12 00:40:30,781 iteration 5197 : loss : 0.016922, loss_ce: 0.005051
2022-01-12 00:40:32,305 iteration 5198 : loss : 0.021821, loss_ce: 0.013404
2022-01-12 00:40:33,979 iteration 5199 : loss : 0.017571, loss_ce: 0.006430
2022-01-12 00:40:35,482 iteration 5200 : loss : 0.034060, loss_ce: 0.015074
2022-01-12 00:40:36,993 iteration 5201 : loss : 0.015284, loss_ce: 0.006518
2022-01-12 00:40:38,552 iteration 5202 : loss : 0.015667, loss_ce: 0.006552
 76%|██████████████████████▏      | 306/400 [2:28:09<46:19, 29.57s/it]2022-01-12 00:40:40,143 iteration 5203 : loss : 0.018718, loss_ce: 0.008200
2022-01-12 00:40:41,766 iteration 5204 : loss : 0.018287, loss_ce: 0.007526
2022-01-12 00:40:43,379 iteration 5205 : loss : 0.030628, loss_ce: 0.008013
2022-01-12 00:40:45,079 iteration 5206 : loss : 0.034260, loss_ce: 0.012624
2022-01-12 00:40:46,709 iteration 5207 : loss : 0.057593, loss_ce: 0.010748
2022-01-12 00:40:48,286 iteration 5208 : loss : 0.015073, loss_ce: 0.006931
2022-01-12 00:40:49,916 iteration 5209 : loss : 0.028317, loss_ce: 0.011444
2022-01-12 00:40:51,545 iteration 5210 : loss : 0.022749, loss_ce: 0.010931
2022-01-12 00:40:53,106 iteration 5211 : loss : 0.023315, loss_ce: 0.008463
2022-01-12 00:40:54,719 iteration 5212 : loss : 0.026024, loss_ce: 0.007278
2022-01-12 00:40:56,284 iteration 5213 : loss : 0.030292, loss_ce: 0.010374
2022-01-12 00:40:57,854 iteration 5214 : loss : 0.015733, loss_ce: 0.007070
2022-01-12 00:40:59,353 iteration 5215 : loss : 0.015165, loss_ce: 0.005308
2022-01-12 00:41:00,893 iteration 5216 : loss : 0.016154, loss_ce: 0.005596
2022-01-12 00:41:02,523 iteration 5217 : loss : 0.029367, loss_ce: 0.012578
2022-01-12 00:41:04,038 iteration 5218 : loss : 0.019555, loss_ce: 0.006818
2022-01-12 00:41:05,616 iteration 5219 : loss : 0.032574, loss_ce: 0.013443
 77%|██████████████████████▎      | 307/400 [2:28:36<44:40, 28.82s/it]2022-01-12 00:41:07,149 iteration 5220 : loss : 0.014894, loss_ce: 0.006261
2022-01-12 00:41:08,661 iteration 5221 : loss : 0.020875, loss_ce: 0.005125
2022-01-12 00:41:10,267 iteration 5222 : loss : 0.019668, loss_ce: 0.006036
2022-01-12 00:41:11,829 iteration 5223 : loss : 0.018749, loss_ce: 0.007320
2022-01-12 00:41:13,468 iteration 5224 : loss : 0.023417, loss_ce: 0.006247
2022-01-12 00:41:15,013 iteration 5225 : loss : 0.016160, loss_ce: 0.006778
2022-01-12 00:41:16,605 iteration 5226 : loss : 0.023824, loss_ce: 0.010714
2022-01-12 00:41:18,195 iteration 5227 : loss : 0.027751, loss_ce: 0.007497
2022-01-12 00:41:19,698 iteration 5228 : loss : 0.013194, loss_ce: 0.004463
2022-01-12 00:41:21,296 iteration 5229 : loss : 0.027832, loss_ce: 0.010708
2022-01-12 00:41:22,810 iteration 5230 : loss : 0.015710, loss_ce: 0.005708
2022-01-12 00:41:24,386 iteration 5231 : loss : 0.017226, loss_ce: 0.005293
2022-01-12 00:41:26,035 iteration 5232 : loss : 0.037215, loss_ce: 0.011490
2022-01-12 00:41:27,495 iteration 5233 : loss : 0.015679, loss_ce: 0.006854
2022-01-12 00:41:29,063 iteration 5234 : loss : 0.020228, loss_ce: 0.008302
2022-01-12 00:41:30,677 iteration 5235 : loss : 0.020418, loss_ce: 0.010702
2022-01-12 00:41:32,332 iteration 5236 : loss : 0.037533, loss_ce: 0.016580
 77%|██████████████████████▎      | 308/400 [2:29:03<43:12, 28.18s/it]2022-01-12 00:41:33,869 iteration 5237 : loss : 0.014871, loss_ce: 0.006746
2022-01-12 00:41:35,429 iteration 5238 : loss : 0.015712, loss_ce: 0.006325
2022-01-12 00:41:36,989 iteration 5239 : loss : 0.016386, loss_ce: 0.005722
2022-01-12 00:41:38,625 iteration 5240 : loss : 0.046658, loss_ce: 0.006477
2022-01-12 00:41:40,210 iteration 5241 : loss : 0.026227, loss_ce: 0.009457
2022-01-12 00:41:41,873 iteration 5242 : loss : 0.026161, loss_ce: 0.012192
2022-01-12 00:41:43,469 iteration 5243 : loss : 0.037179, loss_ce: 0.020249
2022-01-12 00:41:45,023 iteration 5244 : loss : 0.019826, loss_ce: 0.006701
2022-01-12 00:41:46,654 iteration 5245 : loss : 0.021531, loss_ce: 0.006757
2022-01-12 00:41:48,255 iteration 5246 : loss : 0.022178, loss_ce: 0.008004
2022-01-12 00:41:49,818 iteration 5247 : loss : 0.025818, loss_ce: 0.009796
2022-01-12 00:41:51,450 iteration 5248 : loss : 0.023722, loss_ce: 0.011070
2022-01-12 00:41:52,997 iteration 5249 : loss : 0.014655, loss_ce: 0.005452
2022-01-12 00:41:54,576 iteration 5250 : loss : 0.018127, loss_ce: 0.006475
2022-01-12 00:41:56,200 iteration 5251 : loss : 0.022096, loss_ce: 0.008924
2022-01-12 00:41:57,744 iteration 5252 : loss : 0.016461, loss_ce: 0.008050
2022-01-12 00:41:59,300 iteration 5253 : loss : 0.016741, loss_ce: 0.006332
 77%|██████████████████████▍      | 309/400 [2:29:30<42:11, 27.82s/it]2022-01-12 00:42:00,900 iteration 5254 : loss : 0.017837, loss_ce: 0.008290
2022-01-12 00:42:02,518 iteration 5255 : loss : 0.019597, loss_ce: 0.005897
2022-01-12 00:42:04,093 iteration 5256 : loss : 0.022434, loss_ce: 0.010005
2022-01-12 00:42:05,696 iteration 5257 : loss : 0.019704, loss_ce: 0.007327
2022-01-12 00:42:07,201 iteration 5258 : loss : 0.022413, loss_ce: 0.009799
2022-01-12 00:42:08,750 iteration 5259 : loss : 0.026556, loss_ce: 0.011849
2022-01-12 00:42:10,411 iteration 5260 : loss : 0.028754, loss_ce: 0.010114
2022-01-12 00:42:11,976 iteration 5261 : loss : 0.018777, loss_ce: 0.008200
2022-01-12 00:42:13,502 iteration 5262 : loss : 0.022978, loss_ce: 0.009865
2022-01-12 00:42:15,100 iteration 5263 : loss : 0.020934, loss_ce: 0.008521
2022-01-12 00:42:16,710 iteration 5264 : loss : 0.025207, loss_ce: 0.009076
2022-01-12 00:42:18,335 iteration 5265 : loss : 0.021941, loss_ce: 0.010018
2022-01-12 00:42:19,876 iteration 5266 : loss : 0.018547, loss_ce: 0.006492
2022-01-12 00:42:21,512 iteration 5267 : loss : 0.024579, loss_ce: 0.011696
2022-01-12 00:42:22,958 iteration 5268 : loss : 0.013896, loss_ce: 0.003311
2022-01-12 00:42:24,436 iteration 5269 : loss : 0.016783, loss_ce: 0.006391
2022-01-12 00:42:24,436 Training Data Eval:
2022-01-12 00:42:32,477   Average segmentation loss on training set: 0.0121
2022-01-12 00:42:32,478 Validation Data Eval:
2022-01-12 00:42:35,244   Average segmentation loss on validation set: 0.0644
2022-01-12 00:42:36,800 iteration 5270 : loss : 0.025909, loss_ce: 0.003973
 78%|██████████████████████▍      | 310/400 [2:30:07<46:05, 30.72s/it]2022-01-12 00:42:38,522 iteration 5271 : loss : 0.037993, loss_ce: 0.011663
2022-01-12 00:42:40,177 iteration 5272 : loss : 0.021859, loss_ce: 0.007575
2022-01-12 00:42:41,764 iteration 5273 : loss : 0.023690, loss_ce: 0.010173
2022-01-12 00:42:43,349 iteration 5274 : loss : 0.021924, loss_ce: 0.008807
2022-01-12 00:42:44,892 iteration 5275 : loss : 0.018490, loss_ce: 0.007964
2022-01-12 00:42:46,440 iteration 5276 : loss : 0.022968, loss_ce: 0.008160
2022-01-12 00:42:48,115 iteration 5277 : loss : 0.028726, loss_ce: 0.013212
2022-01-12 00:42:49,684 iteration 5278 : loss : 0.027247, loss_ce: 0.005929
2022-01-12 00:42:51,300 iteration 5279 : loss : 0.023266, loss_ce: 0.007919
2022-01-12 00:42:52,823 iteration 5280 : loss : 0.015278, loss_ce: 0.004312
2022-01-12 00:42:54,366 iteration 5281 : loss : 0.016119, loss_ce: 0.007904
2022-01-12 00:42:55,901 iteration 5282 : loss : 0.016107, loss_ce: 0.006485
2022-01-12 00:42:57,424 iteration 5283 : loss : 0.021405, loss_ce: 0.009848
2022-01-12 00:42:58,994 iteration 5284 : loss : 0.016915, loss_ce: 0.006201
2022-01-12 00:43:00,540 iteration 5285 : loss : 0.018123, loss_ce: 0.006808
2022-01-12 00:43:02,057 iteration 5286 : loss : 0.029712, loss_ce: 0.014124
2022-01-12 00:43:03,649 iteration 5287 : loss : 0.017794, loss_ce: 0.006541
 78%|██████████████████████▌      | 311/400 [2:30:34<43:51, 29.56s/it]2022-01-12 00:43:05,269 iteration 5288 : loss : 0.021460, loss_ce: 0.009311
2022-01-12 00:43:06,796 iteration 5289 : loss : 0.030865, loss_ce: 0.012867
2022-01-12 00:43:08,485 iteration 5290 : loss : 0.024449, loss_ce: 0.006216
2022-01-12 00:43:10,067 iteration 5291 : loss : 0.021790, loss_ce: 0.009846
2022-01-12 00:43:11,662 iteration 5292 : loss : 0.021506, loss_ce: 0.006231
2022-01-12 00:43:13,176 iteration 5293 : loss : 0.021220, loss_ce: 0.007227
2022-01-12 00:43:14,778 iteration 5294 : loss : 0.055939, loss_ce: 0.020075
2022-01-12 00:43:16,361 iteration 5295 : loss : 0.020552, loss_ce: 0.006323
2022-01-12 00:43:17,860 iteration 5296 : loss : 0.025671, loss_ce: 0.006453
2022-01-12 00:43:19,408 iteration 5297 : loss : 0.024624, loss_ce: 0.008009
2022-01-12 00:43:20,942 iteration 5298 : loss : 0.035410, loss_ce: 0.017976
2022-01-12 00:43:22,482 iteration 5299 : loss : 0.023067, loss_ce: 0.004764
2022-01-12 00:43:24,013 iteration 5300 : loss : 0.016050, loss_ce: 0.006353
2022-01-12 00:43:25,564 iteration 5301 : loss : 0.023120, loss_ce: 0.006013
2022-01-12 00:43:27,198 iteration 5302 : loss : 0.021879, loss_ce: 0.009450
2022-01-12 00:43:28,806 iteration 5303 : loss : 0.018970, loss_ce: 0.008325
2022-01-12 00:43:30,370 iteration 5304 : loss : 0.020653, loss_ce: 0.010450
 78%|██████████████████████▌      | 312/400 [2:31:01<42:06, 28.71s/it]2022-01-12 00:43:31,910 iteration 5305 : loss : 0.013618, loss_ce: 0.004970
2022-01-12 00:43:33,526 iteration 5306 : loss : 0.026207, loss_ce: 0.011851
2022-01-12 00:43:35,119 iteration 5307 : loss : 0.027868, loss_ce: 0.011973
2022-01-12 00:43:36,655 iteration 5308 : loss : 0.016861, loss_ce: 0.005950
2022-01-12 00:43:38,234 iteration 5309 : loss : 0.017822, loss_ce: 0.008667
2022-01-12 00:43:39,901 iteration 5310 : loss : 0.018978, loss_ce: 0.007024
2022-01-12 00:43:41,463 iteration 5311 : loss : 0.021024, loss_ce: 0.008172
2022-01-12 00:43:43,097 iteration 5312 : loss : 0.026424, loss_ce: 0.010171
2022-01-12 00:43:44,642 iteration 5313 : loss : 0.017213, loss_ce: 0.008300
2022-01-12 00:43:46,202 iteration 5314 : loss : 0.018621, loss_ce: 0.006911
2022-01-12 00:43:47,736 iteration 5315 : loss : 0.017779, loss_ce: 0.006339
2022-01-12 00:43:49,297 iteration 5316 : loss : 0.016076, loss_ce: 0.004674
2022-01-12 00:43:50,848 iteration 5317 : loss : 0.015944, loss_ce: 0.005115
2022-01-12 00:43:52,461 iteration 5318 : loss : 0.029312, loss_ce: 0.014741
2022-01-12 00:43:54,013 iteration 5319 : loss : 0.033498, loss_ce: 0.011631
2022-01-12 00:43:55,536 iteration 5320 : loss : 0.020213, loss_ce: 0.007532
2022-01-12 00:43:57,071 iteration 5321 : loss : 0.023504, loss_ce: 0.009642
 78%|██████████████████████▋      | 313/400 [2:31:27<40:45, 28.11s/it]2022-01-12 00:43:58,625 iteration 5322 : loss : 0.021930, loss_ce: 0.007892
2022-01-12 00:44:00,217 iteration 5323 : loss : 0.033234, loss_ce: 0.011126
2022-01-12 00:44:01,757 iteration 5324 : loss : 0.020382, loss_ce: 0.006622
2022-01-12 00:44:03,380 iteration 5325 : loss : 0.018295, loss_ce: 0.007286
2022-01-12 00:44:04,991 iteration 5326 : loss : 0.021301, loss_ce: 0.007464
2022-01-12 00:44:06,556 iteration 5327 : loss : 0.029902, loss_ce: 0.010853
2022-01-12 00:44:08,155 iteration 5328 : loss : 0.013805, loss_ce: 0.005015
2022-01-12 00:44:09,762 iteration 5329 : loss : 0.021852, loss_ce: 0.011470
2022-01-12 00:44:11,375 iteration 5330 : loss : 0.021458, loss_ce: 0.007616
2022-01-12 00:44:12,886 iteration 5331 : loss : 0.018882, loss_ce: 0.006805
2022-01-12 00:44:14,422 iteration 5332 : loss : 0.023532, loss_ce: 0.009627
2022-01-12 00:44:15,969 iteration 5333 : loss : 0.029079, loss_ce: 0.010499
2022-01-12 00:44:17,501 iteration 5334 : loss : 0.014003, loss_ce: 0.004574
2022-01-12 00:44:19,165 iteration 5335 : loss : 0.033990, loss_ce: 0.012631
2022-01-12 00:44:20,740 iteration 5336 : loss : 0.022360, loss_ce: 0.007712
2022-01-12 00:44:22,308 iteration 5337 : loss : 0.016837, loss_ce: 0.006308
2022-01-12 00:44:23,791 iteration 5338 : loss : 0.015907, loss_ce: 0.005890
 78%|██████████████████████▊      | 314/400 [2:31:54<39:41, 27.69s/it]2022-01-12 00:44:25,382 iteration 5339 : loss : 0.018698, loss_ce: 0.006676
2022-01-12 00:44:26,952 iteration 5340 : loss : 0.016707, loss_ce: 0.009805
2022-01-12 00:44:28,537 iteration 5341 : loss : 0.021360, loss_ce: 0.008889
2022-01-12 00:44:30,142 iteration 5342 : loss : 0.032914, loss_ce: 0.009693
2022-01-12 00:44:31,676 iteration 5343 : loss : 0.018809, loss_ce: 0.006772
2022-01-12 00:44:33,222 iteration 5344 : loss : 0.017740, loss_ce: 0.008104
2022-01-12 00:44:34,796 iteration 5345 : loss : 0.019896, loss_ce: 0.008025
2022-01-12 00:44:36,345 iteration 5346 : loss : 0.021710, loss_ce: 0.008120
2022-01-12 00:44:37,997 iteration 5347 : loss : 0.025345, loss_ce: 0.010601
2022-01-12 00:44:39,585 iteration 5348 : loss : 0.022231, loss_ce: 0.009654
2022-01-12 00:44:41,137 iteration 5349 : loss : 0.030525, loss_ce: 0.016364
2022-01-12 00:44:42,641 iteration 5350 : loss : 0.013296, loss_ce: 0.004848
2022-01-12 00:44:44,251 iteration 5351 : loss : 0.019097, loss_ce: 0.003407
2022-01-12 00:44:45,816 iteration 5352 : loss : 0.017732, loss_ce: 0.006664
2022-01-12 00:44:47,436 iteration 5353 : loss : 0.021770, loss_ce: 0.009939
2022-01-12 00:44:48,945 iteration 5354 : loss : 0.015942, loss_ce: 0.006250
2022-01-12 00:44:48,945 Training Data Eval:
2022-01-12 00:44:56,970   Average segmentation loss on training set: 0.0121
2022-01-12 00:44:56,971 Validation Data Eval:
2022-01-12 00:44:59,725   Average segmentation loss on validation set: 0.0751
2022-01-12 00:45:01,244 iteration 5355 : loss : 0.020398, loss_ce: 0.008938
 79%|██████████████████████▊      | 315/400 [2:32:32<43:22, 30.62s/it]2022-01-12 00:45:02,911 iteration 5356 : loss : 0.033397, loss_ce: 0.014671
2022-01-12 00:45:04,413 iteration 5357 : loss : 0.011933, loss_ce: 0.003666
2022-01-12 00:45:05,983 iteration 5358 : loss : 0.019590, loss_ce: 0.004794
2022-01-12 00:45:07,527 iteration 5359 : loss : 0.020805, loss_ce: 0.012895
2022-01-12 00:45:09,085 iteration 5360 : loss : 0.017174, loss_ce: 0.006150
2022-01-12 00:45:10,712 iteration 5361 : loss : 0.020186, loss_ce: 0.006280
2022-01-12 00:45:12,323 iteration 5362 : loss : 0.020334, loss_ce: 0.012435
2022-01-12 00:45:13,901 iteration 5363 : loss : 0.015507, loss_ce: 0.006491
2022-01-12 00:45:15,384 iteration 5364 : loss : 0.016502, loss_ce: 0.002563
2022-01-12 00:45:16,993 iteration 5365 : loss : 0.017733, loss_ce: 0.006218
2022-01-12 00:45:18,500 iteration 5366 : loss : 0.026398, loss_ce: 0.009206
2022-01-12 00:45:20,091 iteration 5367 : loss : 0.017591, loss_ce: 0.004932
2022-01-12 00:45:21,730 iteration 5368 : loss : 0.019722, loss_ce: 0.007342
2022-01-12 00:45:23,308 iteration 5369 : loss : 0.025026, loss_ce: 0.012297
2022-01-12 00:45:24,887 iteration 5370 : loss : 0.023811, loss_ce: 0.007227
2022-01-12 00:45:26,523 iteration 5371 : loss : 0.031097, loss_ce: 0.011998
2022-01-12 00:45:28,056 iteration 5372 : loss : 0.020886, loss_ce: 0.008784
 79%|██████████████████████▉      | 316/400 [2:32:58<41:16, 29.48s/it]2022-01-12 00:45:29,585 iteration 5373 : loss : 0.015461, loss_ce: 0.007131
2022-01-12 00:45:31,169 iteration 5374 : loss : 0.028354, loss_ce: 0.010173
2022-01-12 00:45:32,696 iteration 5375 : loss : 0.021995, loss_ce: 0.008372
2022-01-12 00:45:34,205 iteration 5376 : loss : 0.015422, loss_ce: 0.004873
2022-01-12 00:45:35,676 iteration 5377 : loss : 0.026964, loss_ce: 0.003450
2022-01-12 00:45:37,144 iteration 5378 : loss : 0.014069, loss_ce: 0.005040
2022-01-12 00:45:38,719 iteration 5379 : loss : 0.021054, loss_ce: 0.006601
2022-01-12 00:45:40,255 iteration 5380 : loss : 0.014932, loss_ce: 0.006206
2022-01-12 00:45:41,822 iteration 5381 : loss : 0.023509, loss_ce: 0.010234
2022-01-12 00:45:43,422 iteration 5382 : loss : 0.028326, loss_ce: 0.009779
2022-01-12 00:45:45,034 iteration 5383 : loss : 0.026711, loss_ce: 0.010810
2022-01-12 00:45:46,591 iteration 5384 : loss : 0.021002, loss_ce: 0.008334
2022-01-12 00:45:48,136 iteration 5385 : loss : 0.021648, loss_ce: 0.008742
2022-01-12 00:45:49,736 iteration 5386 : loss : 0.014831, loss_ce: 0.005429
2022-01-12 00:45:51,315 iteration 5387 : loss : 0.023460, loss_ce: 0.009366
2022-01-12 00:45:52,797 iteration 5388 : loss : 0.016935, loss_ce: 0.006590
2022-01-12 00:45:54,345 iteration 5389 : loss : 0.018160, loss_ce: 0.006254
 79%|██████████████████████▉      | 317/400 [2:33:25<39:27, 28.52s/it]2022-01-12 00:45:56,010 iteration 5390 : loss : 0.024462, loss_ce: 0.007108
2022-01-12 00:45:57,484 iteration 5391 : loss : 0.015414, loss_ce: 0.005360
2022-01-12 00:45:59,080 iteration 5392 : loss : 0.021064, loss_ce: 0.009219
2022-01-12 00:46:00,665 iteration 5393 : loss : 0.025859, loss_ce: 0.007613
2022-01-12 00:46:02,210 iteration 5394 : loss : 0.016001, loss_ce: 0.006391
2022-01-12 00:46:03,721 iteration 5395 : loss : 0.025880, loss_ce: 0.013055
2022-01-12 00:46:05,328 iteration 5396 : loss : 0.023490, loss_ce: 0.007327
2022-01-12 00:46:06,900 iteration 5397 : loss : 0.021567, loss_ce: 0.008109
2022-01-12 00:46:08,366 iteration 5398 : loss : 0.012510, loss_ce: 0.006295
2022-01-12 00:46:09,901 iteration 5399 : loss : 0.014965, loss_ce: 0.006795
2022-01-12 00:46:11,539 iteration 5400 : loss : 0.024262, loss_ce: 0.007927
2022-01-12 00:46:13,140 iteration 5401 : loss : 0.020363, loss_ce: 0.006121
2022-01-12 00:46:14,695 iteration 5402 : loss : 0.016248, loss_ce: 0.005781
2022-01-12 00:46:16,300 iteration 5403 : loss : 0.021364, loss_ce: 0.006942
2022-01-12 00:46:17,823 iteration 5404 : loss : 0.017988, loss_ce: 0.008880
2022-01-12 00:46:19,320 iteration 5405 : loss : 0.015162, loss_ce: 0.005010
2022-01-12 00:46:20,788 iteration 5406 : loss : 0.017429, loss_ce: 0.004406
 80%|███████████████████████      | 318/400 [2:33:51<38:07, 27.90s/it]2022-01-12 00:46:22,448 iteration 5407 : loss : 0.021374, loss_ce: 0.006903
2022-01-12 00:46:23,986 iteration 5408 : loss : 0.019565, loss_ce: 0.005312
2022-01-12 00:46:25,496 iteration 5409 : loss : 0.017250, loss_ce: 0.009186
2022-01-12 00:46:27,022 iteration 5410 : loss : 0.016541, loss_ce: 0.005370
2022-01-12 00:46:28,620 iteration 5411 : loss : 0.017590, loss_ce: 0.006015
2022-01-12 00:46:30,222 iteration 5412 : loss : 0.028964, loss_ce: 0.014827
2022-01-12 00:46:31,721 iteration 5413 : loss : 0.016696, loss_ce: 0.006354
2022-01-12 00:46:33,329 iteration 5414 : loss : 0.024859, loss_ce: 0.009395
2022-01-12 00:46:34,891 iteration 5415 : loss : 0.021959, loss_ce: 0.008153
2022-01-12 00:46:36,497 iteration 5416 : loss : 0.019896, loss_ce: 0.009150
2022-01-12 00:46:38,021 iteration 5417 : loss : 0.024771, loss_ce: 0.008769
2022-01-12 00:46:39,569 iteration 5418 : loss : 0.017745, loss_ce: 0.005995
2022-01-12 00:46:41,105 iteration 5419 : loss : 0.015475, loss_ce: 0.006974
2022-01-12 00:46:42,659 iteration 5420 : loss : 0.017142, loss_ce: 0.007655
2022-01-12 00:46:44,242 iteration 5421 : loss : 0.025400, loss_ce: 0.007338
2022-01-12 00:46:45,826 iteration 5422 : loss : 0.030955, loss_ce: 0.009130
2022-01-12 00:46:47,446 iteration 5423 : loss : 0.027267, loss_ce: 0.007892
 80%|███████████████████████▏     | 319/400 [2:34:18<37:09, 27.52s/it]2022-01-12 00:46:49,076 iteration 5424 : loss : 0.024001, loss_ce: 0.009339
2022-01-12 00:46:50,642 iteration 5425 : loss : 0.038899, loss_ce: 0.018887
2022-01-12 00:46:52,222 iteration 5426 : loss : 0.015442, loss_ce: 0.004351
2022-01-12 00:46:53,884 iteration 5427 : loss : 0.029745, loss_ce: 0.014022
2022-01-12 00:46:55,494 iteration 5428 : loss : 0.040219, loss_ce: 0.017020
2022-01-12 00:46:57,016 iteration 5429 : loss : 0.017901, loss_ce: 0.006345
2022-01-12 00:46:58,503 iteration 5430 : loss : 0.014876, loss_ce: 0.006348
2022-01-12 00:47:00,129 iteration 5431 : loss : 0.022993, loss_ce: 0.007702
2022-01-12 00:47:01,659 iteration 5432 : loss : 0.030355, loss_ce: 0.012563
2022-01-12 00:47:03,346 iteration 5433 : loss : 0.026253, loss_ce: 0.010103
2022-01-12 00:47:04,978 iteration 5434 : loss : 0.025953, loss_ce: 0.009370
2022-01-12 00:47:06,553 iteration 5435 : loss : 0.018269, loss_ce: 0.007936
2022-01-12 00:47:08,168 iteration 5436 : loss : 0.036091, loss_ce: 0.019829
2022-01-12 00:47:09,701 iteration 5437 : loss : 0.016426, loss_ce: 0.004389
2022-01-12 00:47:11,264 iteration 5438 : loss : 0.018608, loss_ce: 0.007936
2022-01-12 00:47:12,855 iteration 5439 : loss : 0.024413, loss_ce: 0.008121
2022-01-12 00:47:12,855 Training Data Eval:
2022-01-12 00:47:20,862   Average segmentation loss on training set: 0.0118
2022-01-12 00:47:20,862 Validation Data Eval:
2022-01-12 00:47:23,626   Average segmentation loss on validation set: 0.0888
2022-01-12 00:47:25,209 iteration 5440 : loss : 0.028615, loss_ce: 0.013601
 80%|███████████████████████▏     | 320/400 [2:34:56<40:47, 30.60s/it]2022-01-12 00:47:26,763 iteration 5441 : loss : 0.016694, loss_ce: 0.007014
2022-01-12 00:47:28,453 iteration 5442 : loss : 0.031789, loss_ce: 0.013331
2022-01-12 00:47:30,059 iteration 5443 : loss : 0.029687, loss_ce: 0.010987
2022-01-12 00:47:31,618 iteration 5444 : loss : 0.018045, loss_ce: 0.008734
2022-01-12 00:47:33,167 iteration 5445 : loss : 0.032770, loss_ce: 0.013508
2022-01-12 00:47:34,726 iteration 5446 : loss : 0.024924, loss_ce: 0.010222
2022-01-12 00:47:36,229 iteration 5447 : loss : 0.021919, loss_ce: 0.005148
2022-01-12 00:47:37,819 iteration 5448 : loss : 0.024524, loss_ce: 0.008437
2022-01-12 00:47:39,365 iteration 5449 : loss : 0.018111, loss_ce: 0.005004
2022-01-12 00:47:40,899 iteration 5450 : loss : 0.019527, loss_ce: 0.006140
2022-01-12 00:47:42,468 iteration 5451 : loss : 0.013232, loss_ce: 0.003856
2022-01-12 00:47:44,029 iteration 5452 : loss : 0.014585, loss_ce: 0.005595
2022-01-12 00:47:45,638 iteration 5453 : loss : 0.030470, loss_ce: 0.016570
2022-01-12 00:47:47,228 iteration 5454 : loss : 0.029269, loss_ce: 0.014373
2022-01-12 00:47:48,796 iteration 5455 : loss : 0.020855, loss_ce: 0.010587
2022-01-12 00:47:50,392 iteration 5456 : loss : 0.017299, loss_ce: 0.005130
2022-01-12 00:47:51,973 iteration 5457 : loss : 0.022326, loss_ce: 0.009320
 80%|███████████████████████▎     | 321/400 [2:35:22<38:46, 29.45s/it]2022-01-12 00:47:53,627 iteration 5458 : loss : 0.025390, loss_ce: 0.006720
2022-01-12 00:47:55,140 iteration 5459 : loss : 0.016604, loss_ce: 0.006037
2022-01-12 00:47:56,712 iteration 5460 : loss : 0.026209, loss_ce: 0.010483
2022-01-12 00:47:58,311 iteration 5461 : loss : 0.023581, loss_ce: 0.009309
2022-01-12 00:47:59,925 iteration 5462 : loss : 0.020818, loss_ce: 0.009614
2022-01-12 00:48:01,459 iteration 5463 : loss : 0.015272, loss_ce: 0.004959
2022-01-12 00:48:03,141 iteration 5464 : loss : 0.033162, loss_ce: 0.011979
2022-01-12 00:48:04,673 iteration 5465 : loss : 0.019457, loss_ce: 0.007167
2022-01-12 00:48:06,220 iteration 5466 : loss : 0.016370, loss_ce: 0.006507
2022-01-12 00:48:07,849 iteration 5467 : loss : 0.021563, loss_ce: 0.007710
2022-01-12 00:48:09,469 iteration 5468 : loss : 0.027141, loss_ce: 0.015101
2022-01-12 00:48:11,089 iteration 5469 : loss : 0.042747, loss_ce: 0.024352
2022-01-12 00:48:12,642 iteration 5470 : loss : 0.016857, loss_ce: 0.007272
2022-01-12 00:48:14,187 iteration 5471 : loss : 0.020972, loss_ce: 0.010521
2022-01-12 00:48:15,813 iteration 5472 : loss : 0.021949, loss_ce: 0.010152
2022-01-12 00:48:17,350 iteration 5473 : loss : 0.016998, loss_ce: 0.004839
2022-01-12 00:48:18,976 iteration 5474 : loss : 0.017224, loss_ce: 0.005582
 80%|███████████████████████▎     | 322/400 [2:35:49<37:19, 28.71s/it]2022-01-12 00:48:20,542 iteration 5475 : loss : 0.022586, loss_ce: 0.010828
2022-01-12 00:48:22,095 iteration 5476 : loss : 0.013812, loss_ce: 0.006313
2022-01-12 00:48:23,712 iteration 5477 : loss : 0.014200, loss_ce: 0.005353
2022-01-12 00:48:25,345 iteration 5478 : loss : 0.023825, loss_ce: 0.010406
2022-01-12 00:48:26,867 iteration 5479 : loss : 0.027219, loss_ce: 0.007601
2022-01-12 00:48:28,440 iteration 5480 : loss : 0.031358, loss_ce: 0.007148
2022-01-12 00:48:30,028 iteration 5481 : loss : 0.017952, loss_ce: 0.007172
2022-01-12 00:48:31,592 iteration 5482 : loss : 0.021070, loss_ce: 0.007973
2022-01-12 00:48:33,090 iteration 5483 : loss : 0.019287, loss_ce: 0.006922
2022-01-12 00:48:34,713 iteration 5484 : loss : 0.030111, loss_ce: 0.012451
2022-01-12 00:48:36,215 iteration 5485 : loss : 0.015438, loss_ce: 0.007358
2022-01-12 00:48:37,898 iteration 5486 : loss : 0.019953, loss_ce: 0.005872
2022-01-12 00:48:39,406 iteration 5487 : loss : 0.017588, loss_ce: 0.009847
2022-01-12 00:48:40,996 iteration 5488 : loss : 0.047849, loss_ce: 0.013253
2022-01-12 00:48:42,511 iteration 5489 : loss : 0.020925, loss_ce: 0.005726
2022-01-12 00:48:44,107 iteration 5490 : loss : 0.018792, loss_ce: 0.007210
2022-01-12 00:48:45,658 iteration 5491 : loss : 0.017599, loss_ce: 0.006035
 81%|███████████████████████▍     | 323/400 [2:36:16<36:03, 28.10s/it]2022-01-12 00:48:47,284 iteration 5492 : loss : 0.023336, loss_ce: 0.010275
2022-01-12 00:48:48,788 iteration 5493 : loss : 0.014424, loss_ce: 0.005084
2022-01-12 00:48:50,325 iteration 5494 : loss : 0.023882, loss_ce: 0.010318
2022-01-12 00:48:51,947 iteration 5495 : loss : 0.033388, loss_ce: 0.007199
2022-01-12 00:48:53,464 iteration 5496 : loss : 0.015731, loss_ce: 0.005278
2022-01-12 00:48:55,034 iteration 5497 : loss : 0.017951, loss_ce: 0.008684
2022-01-12 00:48:56,642 iteration 5498 : loss : 0.018123, loss_ce: 0.007480
2022-01-12 00:48:58,203 iteration 5499 : loss : 0.019041, loss_ce: 0.006145
2022-01-12 00:48:59,765 iteration 5500 : loss : 0.015635, loss_ce: 0.005341
2022-01-12 00:49:01,301 iteration 5501 : loss : 0.017312, loss_ce: 0.006232
2022-01-12 00:49:02,842 iteration 5502 : loss : 0.021112, loss_ce: 0.011197
2022-01-12 00:49:04,532 iteration 5503 : loss : 0.033478, loss_ce: 0.008811
2022-01-12 00:49:06,101 iteration 5504 : loss : 0.025235, loss_ce: 0.011994
2022-01-12 00:49:07,593 iteration 5505 : loss : 0.015202, loss_ce: 0.005500
2022-01-12 00:49:09,121 iteration 5506 : loss : 0.014787, loss_ce: 0.005743
2022-01-12 00:49:10,766 iteration 5507 : loss : 0.022792, loss_ce: 0.007048
2022-01-12 00:49:12,371 iteration 5508 : loss : 0.028410, loss_ce: 0.013467
 81%|███████████████████████▍     | 324/400 [2:36:43<35:04, 27.69s/it]2022-01-12 00:49:14,007 iteration 5509 : loss : 0.028746, loss_ce: 0.013380
2022-01-12 00:49:15,535 iteration 5510 : loss : 0.015147, loss_ce: 0.005759
2022-01-12 00:49:17,115 iteration 5511 : loss : 0.023102, loss_ce: 0.008009
2022-01-12 00:49:18,668 iteration 5512 : loss : 0.020760, loss_ce: 0.009339
2022-01-12 00:49:20,235 iteration 5513 : loss : 0.018549, loss_ce: 0.010832
2022-01-12 00:49:21,858 iteration 5514 : loss : 0.036068, loss_ce: 0.015691
2022-01-12 00:49:23,410 iteration 5515 : loss : 0.017339, loss_ce: 0.006932
2022-01-12 00:49:24,961 iteration 5516 : loss : 0.015497, loss_ce: 0.007468
2022-01-12 00:49:26,547 iteration 5517 : loss : 0.027197, loss_ce: 0.008925
2022-01-12 00:49:28,051 iteration 5518 : loss : 0.015520, loss_ce: 0.005652
2022-01-12 00:49:29,697 iteration 5519 : loss : 0.031839, loss_ce: 0.009475
2022-01-12 00:49:31,262 iteration 5520 : loss : 0.015144, loss_ce: 0.002625
2022-01-12 00:49:32,899 iteration 5521 : loss : 0.023209, loss_ce: 0.009009
2022-01-12 00:49:34,455 iteration 5522 : loss : 0.018601, loss_ce: 0.007009
2022-01-12 00:49:36,064 iteration 5523 : loss : 0.026310, loss_ce: 0.010615
2022-01-12 00:49:37,693 iteration 5524 : loss : 0.017735, loss_ce: 0.006406
2022-01-12 00:49:37,693 Training Data Eval:
2022-01-12 00:49:45,700   Average segmentation loss on training set: 0.0114
2022-01-12 00:49:45,701 Validation Data Eval:
2022-01-12 00:49:48,458   Average segmentation loss on validation set: 0.0773
2022-01-12 00:49:50,117 iteration 5525 : loss : 0.029324, loss_ce: 0.013163
 81%|███████████████████████▌     | 325/400 [2:37:20<38:22, 30.71s/it]2022-01-12 00:49:51,722 iteration 5526 : loss : 0.026281, loss_ce: 0.007884
2022-01-12 00:49:53,233 iteration 5527 : loss : 0.014311, loss_ce: 0.005776
2022-01-12 00:49:54,888 iteration 5528 : loss : 0.029786, loss_ce: 0.013886
2022-01-12 00:49:56,430 iteration 5529 : loss : 0.019705, loss_ce: 0.007568
2022-01-12 00:49:57,947 iteration 5530 : loss : 0.014153, loss_ce: 0.005713
2022-01-12 00:49:59,438 iteration 5531 : loss : 0.012203, loss_ce: 0.003792
2022-01-12 00:50:00,956 iteration 5532 : loss : 0.021035, loss_ce: 0.007955
2022-01-12 00:50:02,561 iteration 5533 : loss : 0.025232, loss_ce: 0.010687
2022-01-12 00:50:04,061 iteration 5534 : loss : 0.014532, loss_ce: 0.005408
2022-01-12 00:50:05,595 iteration 5535 : loss : 0.015171, loss_ce: 0.006206
2022-01-12 00:50:07,198 iteration 5536 : loss : 0.025741, loss_ce: 0.008572
2022-01-12 00:50:08,807 iteration 5537 : loss : 0.022514, loss_ce: 0.009660
2022-01-12 00:50:10,460 iteration 5538 : loss : 0.028628, loss_ce: 0.007920
2022-01-12 00:50:12,028 iteration 5539 : loss : 0.016309, loss_ce: 0.005686
2022-01-12 00:50:13,513 iteration 5540 : loss : 0.019846, loss_ce: 0.007386
2022-01-12 00:50:15,172 iteration 5541 : loss : 0.028122, loss_ce: 0.013673
2022-01-12 00:50:16,752 iteration 5542 : loss : 0.026477, loss_ce: 0.014839
 82%|███████████████████████▋     | 326/400 [2:37:47<36:21, 29.48s/it]2022-01-12 00:50:18,409 iteration 5543 : loss : 0.018675, loss_ce: 0.008758
2022-01-12 00:50:19,941 iteration 5544 : loss : 0.020898, loss_ce: 0.007072
2022-01-12 00:50:21,543 iteration 5545 : loss : 0.018470, loss_ce: 0.008738
2022-01-12 00:50:23,185 iteration 5546 : loss : 0.038574, loss_ce: 0.009344
2022-01-12 00:50:24,786 iteration 5547 : loss : 0.022362, loss_ce: 0.009211
2022-01-12 00:50:26,331 iteration 5548 : loss : 0.027163, loss_ce: 0.006479
2022-01-12 00:50:27,885 iteration 5549 : loss : 0.020988, loss_ce: 0.006069
2022-01-12 00:50:29,483 iteration 5550 : loss : 0.019694, loss_ce: 0.006974
2022-01-12 00:50:31,028 iteration 5551 : loss : 0.025561, loss_ce: 0.012956
2022-01-12 00:50:32,484 iteration 5552 : loss : 0.014975, loss_ce: 0.005145
2022-01-12 00:50:34,050 iteration 5553 : loss : 0.020581, loss_ce: 0.009251
2022-01-12 00:50:35,588 iteration 5554 : loss : 0.017569, loss_ce: 0.007347
2022-01-12 00:50:37,084 iteration 5555 : loss : 0.015012, loss_ce: 0.006478
2022-01-12 00:50:38,711 iteration 5556 : loss : 0.024663, loss_ce: 0.010324
2022-01-12 00:50:40,298 iteration 5557 : loss : 0.017592, loss_ce: 0.008640
2022-01-12 00:50:41,856 iteration 5558 : loss : 0.024798, loss_ce: 0.008865
2022-01-12 00:50:43,418 iteration 5559 : loss : 0.023677, loss_ce: 0.006778
 82%|███████████████████████▋     | 327/400 [2:38:14<34:50, 28.64s/it]2022-01-12 00:50:45,034 iteration 5560 : loss : 0.020306, loss_ce: 0.007462
2022-01-12 00:50:46,569 iteration 5561 : loss : 0.025097, loss_ce: 0.008915
2022-01-12 00:50:48,135 iteration 5562 : loss : 0.014262, loss_ce: 0.006346
2022-01-12 00:50:49,685 iteration 5563 : loss : 0.020330, loss_ce: 0.009040
2022-01-12 00:50:51,289 iteration 5564 : loss : 0.022928, loss_ce: 0.010023
2022-01-12 00:50:52,892 iteration 5565 : loss : 0.026245, loss_ce: 0.013178
2022-01-12 00:50:54,468 iteration 5566 : loss : 0.017498, loss_ce: 0.007516
2022-01-12 00:50:56,078 iteration 5567 : loss : 0.019059, loss_ce: 0.009172
2022-01-12 00:50:57,624 iteration 5568 : loss : 0.027317, loss_ce: 0.009231
2022-01-12 00:50:59,204 iteration 5569 : loss : 0.023624, loss_ce: 0.007086
2022-01-12 00:51:00,718 iteration 5570 : loss : 0.012432, loss_ce: 0.003738
2022-01-12 00:51:02,189 iteration 5571 : loss : 0.013433, loss_ce: 0.005172
2022-01-12 00:51:03,705 iteration 5572 : loss : 0.020855, loss_ce: 0.007065
2022-01-12 00:51:05,208 iteration 5573 : loss : 0.010945, loss_ce: 0.003490
2022-01-12 00:51:06,768 iteration 5574 : loss : 0.016018, loss_ce: 0.005087
2022-01-12 00:51:08,425 iteration 5575 : loss : 0.026104, loss_ce: 0.007249
2022-01-12 00:51:10,024 iteration 5576 : loss : 0.024954, loss_ce: 0.008919
 82%|███████████████████████▊     | 328/400 [2:38:40<33:38, 28.03s/it]2022-01-12 00:51:11,654 iteration 5577 : loss : 0.033164, loss_ce: 0.015940
2022-01-12 00:51:13,260 iteration 5578 : loss : 0.024032, loss_ce: 0.010552
2022-01-12 00:51:14,872 iteration 5579 : loss : 0.024823, loss_ce: 0.011992
2022-01-12 00:51:16,402 iteration 5580 : loss : 0.021177, loss_ce: 0.007679
2022-01-12 00:51:17,917 iteration 5581 : loss : 0.015345, loss_ce: 0.005985
2022-01-12 00:51:19,490 iteration 5582 : loss : 0.015772, loss_ce: 0.004947
2022-01-12 00:51:21,046 iteration 5583 : loss : 0.011572, loss_ce: 0.004333
2022-01-12 00:51:22,649 iteration 5584 : loss : 0.022866, loss_ce: 0.010131
2022-01-12 00:51:24,194 iteration 5585 : loss : 0.016466, loss_ce: 0.004759
2022-01-12 00:51:25,818 iteration 5586 : loss : 0.022149, loss_ce: 0.008131
2022-01-12 00:51:27,334 iteration 5587 : loss : 0.030904, loss_ce: 0.013005
2022-01-12 00:51:28,887 iteration 5588 : loss : 0.017501, loss_ce: 0.006242
2022-01-12 00:51:30,392 iteration 5589 : loss : 0.012816, loss_ce: 0.003640
2022-01-12 00:51:31,929 iteration 5590 : loss : 0.016688, loss_ce: 0.003926
2022-01-12 00:51:33,467 iteration 5591 : loss : 0.020280, loss_ce: 0.010347
2022-01-12 00:51:35,084 iteration 5592 : loss : 0.018446, loss_ce: 0.005906
2022-01-12 00:51:36,663 iteration 5593 : loss : 0.019837, loss_ce: 0.007912
 82%|███████████████████████▊     | 329/400 [2:39:07<32:40, 27.61s/it]2022-01-12 00:51:38,223 iteration 5594 : loss : 0.019558, loss_ce: 0.005717
2022-01-12 00:51:39,822 iteration 5595 : loss : 0.024711, loss_ce: 0.010923
2022-01-12 00:51:41,365 iteration 5596 : loss : 0.016560, loss_ce: 0.005811
2022-01-12 00:51:42,992 iteration 5597 : loss : 0.013050, loss_ce: 0.005197
2022-01-12 00:51:44,503 iteration 5598 : loss : 0.012908, loss_ce: 0.005560
2022-01-12 00:51:46,068 iteration 5599 : loss : 0.024625, loss_ce: 0.010376
2022-01-12 00:51:47,623 iteration 5600 : loss : 0.013446, loss_ce: 0.005717
2022-01-12 00:51:49,164 iteration 5601 : loss : 0.014361, loss_ce: 0.006099
2022-01-12 00:51:50,751 iteration 5602 : loss : 0.026815, loss_ce: 0.011488
2022-01-12 00:51:52,263 iteration 5603 : loss : 0.012412, loss_ce: 0.004977
2022-01-12 00:51:53,859 iteration 5604 : loss : 0.033448, loss_ce: 0.012155
2022-01-12 00:51:55,469 iteration 5605 : loss : 0.029835, loss_ce: 0.010826
2022-01-12 00:51:57,057 iteration 5606 : loss : 0.016885, loss_ce: 0.005403
2022-01-12 00:51:58,625 iteration 5607 : loss : 0.017628, loss_ce: 0.005388
2022-01-12 00:52:00,196 iteration 5608 : loss : 0.016836, loss_ce: 0.005661
2022-01-12 00:52:01,806 iteration 5609 : loss : 0.016855, loss_ce: 0.007249
2022-01-12 00:52:01,807 Training Data Eval:
2022-01-12 00:52:09,830   Average segmentation loss on training set: 0.0111
2022-01-12 00:52:09,831 Validation Data Eval:
2022-01-12 00:52:12,594   Average segmentation loss on validation set: 0.0775
2022-01-12 00:52:14,132 iteration 5610 : loss : 0.015416, loss_ce: 0.005618
 82%|███████████████████████▉     | 330/400 [2:39:44<35:39, 30.57s/it]2022-01-12 00:52:15,729 iteration 5611 : loss : 0.020318, loss_ce: 0.007305
2022-01-12 00:52:17,312 iteration 5612 : loss : 0.037695, loss_ce: 0.006990
2022-01-12 00:52:18,889 iteration 5613 : loss : 0.016108, loss_ce: 0.006499
2022-01-12 00:52:20,464 iteration 5614 : loss : 0.024316, loss_ce: 0.009077
2022-01-12 00:52:21,957 iteration 5615 : loss : 0.012567, loss_ce: 0.005548
2022-01-12 00:52:23,569 iteration 5616 : loss : 0.025816, loss_ce: 0.011367
2022-01-12 00:52:25,210 iteration 5617 : loss : 0.018034, loss_ce: 0.007338
2022-01-12 00:52:26,819 iteration 5618 : loss : 0.022837, loss_ce: 0.008895
2022-01-12 00:52:28,368 iteration 5619 : loss : 0.021158, loss_ce: 0.007023
2022-01-12 00:52:30,004 iteration 5620 : loss : 0.021175, loss_ce: 0.007841
2022-01-12 00:52:31,512 iteration 5621 : loss : 0.020028, loss_ce: 0.007436
2022-01-12 00:52:33,092 iteration 5622 : loss : 0.015789, loss_ce: 0.005081
2022-01-12 00:52:34,751 iteration 5623 : loss : 0.027301, loss_ce: 0.010822
2022-01-12 00:52:36,274 iteration 5624 : loss : 0.020281, loss_ce: 0.005816
2022-01-12 00:52:37,789 iteration 5625 : loss : 0.015227, loss_ce: 0.006617
2022-01-12 00:52:39,343 iteration 5626 : loss : 0.027149, loss_ce: 0.007899
2022-01-12 00:52:40,951 iteration 5627 : loss : 0.019687, loss_ce: 0.007908
 83%|███████████████████████▉     | 331/400 [2:40:11<33:51, 29.44s/it]2022-01-12 00:52:42,664 iteration 5628 : loss : 0.026090, loss_ce: 0.012895
2022-01-12 00:52:44,188 iteration 5629 : loss : 0.015848, loss_ce: 0.007011
2022-01-12 00:52:45,775 iteration 5630 : loss : 0.020659, loss_ce: 0.009383
2022-01-12 00:52:47,350 iteration 5631 : loss : 0.019677, loss_ce: 0.007610
2022-01-12 00:52:48,972 iteration 5632 : loss : 0.017864, loss_ce: 0.007233
2022-01-12 00:52:50,602 iteration 5633 : loss : 0.029036, loss_ce: 0.009408
2022-01-12 00:52:52,185 iteration 5634 : loss : 0.012990, loss_ce: 0.005680
2022-01-12 00:52:53,739 iteration 5635 : loss : 0.015440, loss_ce: 0.006093
2022-01-12 00:52:55,335 iteration 5636 : loss : 0.027429, loss_ce: 0.007884
2022-01-12 00:52:56,907 iteration 5637 : loss : 0.016181, loss_ce: 0.005065
2022-01-12 00:52:58,489 iteration 5638 : loss : 0.019907, loss_ce: 0.008666
2022-01-12 00:53:00,074 iteration 5639 : loss : 0.026262, loss_ce: 0.012088
2022-01-12 00:53:01,660 iteration 5640 : loss : 0.017850, loss_ce: 0.008163
2022-01-12 00:53:03,165 iteration 5641 : loss : 0.016579, loss_ce: 0.004668
2022-01-12 00:53:04,688 iteration 5642 : loss : 0.015739, loss_ce: 0.005552
2022-01-12 00:53:06,243 iteration 5643 : loss : 0.019738, loss_ce: 0.007247
2022-01-12 00:53:07,789 iteration 5644 : loss : 0.021675, loss_ce: 0.010529
 83%|████████████████████████     | 332/400 [2:40:38<32:29, 28.66s/it]2022-01-12 00:53:09,384 iteration 5645 : loss : 0.021193, loss_ce: 0.006755
2022-01-12 00:53:10,949 iteration 5646 : loss : 0.013408, loss_ce: 0.004178
2022-01-12 00:53:12,555 iteration 5647 : loss : 0.021336, loss_ce: 0.006963
2022-01-12 00:53:14,157 iteration 5648 : loss : 0.024881, loss_ce: 0.007068
2022-01-12 00:53:15,737 iteration 5649 : loss : 0.015487, loss_ce: 0.005748
2022-01-12 00:53:17,365 iteration 5650 : loss : 0.023312, loss_ce: 0.006602
2022-01-12 00:53:18,925 iteration 5651 : loss : 0.015869, loss_ce: 0.007480
2022-01-12 00:53:20,460 iteration 5652 : loss : 0.018374, loss_ce: 0.007591
2022-01-12 00:53:22,007 iteration 5653 : loss : 0.015635, loss_ce: 0.006173
2022-01-12 00:53:23,650 iteration 5654 : loss : 0.016377, loss_ce: 0.006557
2022-01-12 00:53:25,209 iteration 5655 : loss : 0.026818, loss_ce: 0.008680
2022-01-12 00:53:26,808 iteration 5656 : loss : 0.029191, loss_ce: 0.014318
2022-01-12 00:53:28,352 iteration 5657 : loss : 0.026314, loss_ce: 0.014057
2022-01-12 00:53:29,976 iteration 5658 : loss : 0.031099, loss_ce: 0.008779
2022-01-12 00:53:31,641 iteration 5659 : loss : 0.017400, loss_ce: 0.007490
2022-01-12 00:53:33,121 iteration 5660 : loss : 0.013224, loss_ce: 0.004967
2022-01-12 00:53:34,724 iteration 5661 : loss : 0.029084, loss_ce: 0.010892
 83%|████████████████████████▏    | 333/400 [2:41:05<31:25, 28.14s/it]2022-01-12 00:53:36,296 iteration 5662 : loss : 0.015892, loss_ce: 0.006404
2022-01-12 00:53:37,917 iteration 5663 : loss : 0.020804, loss_ce: 0.010213
2022-01-12 00:53:39,559 iteration 5664 : loss : 0.024080, loss_ce: 0.009049
2022-01-12 00:53:41,118 iteration 5665 : loss : 0.013996, loss_ce: 0.003377
2022-01-12 00:53:42,751 iteration 5666 : loss : 0.018561, loss_ce: 0.006350
2022-01-12 00:53:44,341 iteration 5667 : loss : 0.018974, loss_ce: 0.005884
2022-01-12 00:53:45,934 iteration 5668 : loss : 0.021143, loss_ce: 0.006845
2022-01-12 00:53:47,567 iteration 5669 : loss : 0.025545, loss_ce: 0.011440
2022-01-12 00:53:49,159 iteration 5670 : loss : 0.019656, loss_ce: 0.006240
2022-01-12 00:53:50,800 iteration 5671 : loss : 0.051837, loss_ce: 0.014820
2022-01-12 00:53:52,285 iteration 5672 : loss : 0.020680, loss_ce: 0.005487
2022-01-12 00:53:53,834 iteration 5673 : loss : 0.017793, loss_ce: 0.009142
2022-01-12 00:53:55,354 iteration 5674 : loss : 0.032123, loss_ce: 0.011629
2022-01-12 00:53:57,031 iteration 5675 : loss : 0.031501, loss_ce: 0.015132
2022-01-12 00:53:58,601 iteration 5676 : loss : 0.029605, loss_ce: 0.015909
2022-01-12 00:54:00,142 iteration 5677 : loss : 0.020968, loss_ce: 0.007487
2022-01-12 00:54:01,697 iteration 5678 : loss : 0.019265, loss_ce: 0.007247
 84%|████████████████████████▏    | 334/400 [2:41:32<30:34, 27.79s/it]2022-01-12 00:54:03,334 iteration 5679 : loss : 0.019319, loss_ce: 0.006031
2022-01-12 00:54:04,898 iteration 5680 : loss : 0.019647, loss_ce: 0.008878
2022-01-12 00:54:06,493 iteration 5681 : loss : 0.021968, loss_ce: 0.006669
2022-01-12 00:54:07,987 iteration 5682 : loss : 0.014408, loss_ce: 0.004800
2022-01-12 00:54:09,487 iteration 5683 : loss : 0.011701, loss_ce: 0.005289
2022-01-12 00:54:11,119 iteration 5684 : loss : 0.034209, loss_ce: 0.015144
2022-01-12 00:54:12,724 iteration 5685 : loss : 0.026135, loss_ce: 0.009369
2022-01-12 00:54:14,296 iteration 5686 : loss : 0.029352, loss_ce: 0.012075
2022-01-12 00:54:15,893 iteration 5687 : loss : 0.015701, loss_ce: 0.004887
2022-01-12 00:54:17,525 iteration 5688 : loss : 0.029220, loss_ce: 0.010714
2022-01-12 00:54:19,072 iteration 5689 : loss : 0.018040, loss_ce: 0.007167
2022-01-12 00:54:20,588 iteration 5690 : loss : 0.016505, loss_ce: 0.005429
2022-01-12 00:54:22,181 iteration 5691 : loss : 0.016382, loss_ce: 0.007267
2022-01-12 00:54:23,750 iteration 5692 : loss : 0.017634, loss_ce: 0.006094
2022-01-12 00:54:25,208 iteration 5693 : loss : 0.016706, loss_ce: 0.007081
2022-01-12 00:54:26,807 iteration 5694 : loss : 0.019163, loss_ce: 0.007417
2022-01-12 00:54:26,807 Training Data Eval:
2022-01-12 00:54:34,842   Average segmentation loss on training set: 0.0108
2022-01-12 00:54:34,843 Validation Data Eval:
2022-01-12 00:54:37,600   Average segmentation loss on validation set: 0.0682
2022-01-12 00:54:39,170 iteration 5695 : loss : 0.020802, loss_ce: 0.008806
 84%|████████████████████████▎    | 335/400 [2:42:09<33:15, 30.70s/it]2022-01-12 00:54:40,805 iteration 5696 : loss : 0.027669, loss_ce: 0.007727
2022-01-12 00:54:42,337 iteration 5697 : loss : 0.015065, loss_ce: 0.004464
2022-01-12 00:54:43,972 iteration 5698 : loss : 0.022038, loss_ce: 0.008062
2022-01-12 00:54:45,617 iteration 5699 : loss : 0.020140, loss_ce: 0.005888
2022-01-12 00:54:47,113 iteration 5700 : loss : 0.014135, loss_ce: 0.006064
2022-01-12 00:54:48,787 iteration 5701 : loss : 0.022618, loss_ce: 0.004268
2022-01-12 00:54:50,390 iteration 5702 : loss : 0.017940, loss_ce: 0.006547
2022-01-12 00:54:52,040 iteration 5703 : loss : 0.019910, loss_ce: 0.009139
2022-01-12 00:54:53,583 iteration 5704 : loss : 0.030459, loss_ce: 0.010715
2022-01-12 00:54:55,129 iteration 5705 : loss : 0.018149, loss_ce: 0.007942
2022-01-12 00:54:56,651 iteration 5706 : loss : 0.014697, loss_ce: 0.005623
2022-01-12 00:54:58,252 iteration 5707 : loss : 0.029461, loss_ce: 0.007024
2022-01-12 00:54:59,824 iteration 5708 : loss : 0.020413, loss_ce: 0.010154
2022-01-12 00:55:01,350 iteration 5709 : loss : 0.018261, loss_ce: 0.006620
2022-01-12 00:55:02,902 iteration 5710 : loss : 0.018566, loss_ce: 0.009443
2022-01-12 00:55:04,483 iteration 5711 : loss : 0.014954, loss_ce: 0.007203
2022-01-12 00:55:06,049 iteration 5712 : loss : 0.018874, loss_ce: 0.008303
 84%|████████████████████████▎    | 336/400 [2:42:36<31:31, 29.55s/it]2022-01-12 00:55:07,669 iteration 5713 : loss : 0.013913, loss_ce: 0.006522
2022-01-12 00:55:09,262 iteration 5714 : loss : 0.026269, loss_ce: 0.007275
2022-01-12 00:55:10,781 iteration 5715 : loss : 0.015992, loss_ce: 0.006649
2022-01-12 00:55:12,391 iteration 5716 : loss : 0.019539, loss_ce: 0.007352
2022-01-12 00:55:13,899 iteration 5717 : loss : 0.012934, loss_ce: 0.004896
2022-01-12 00:55:15,419 iteration 5718 : loss : 0.020688, loss_ce: 0.009836
2022-01-12 00:55:17,026 iteration 5719 : loss : 0.018633, loss_ce: 0.005885
2022-01-12 00:55:18,652 iteration 5720 : loss : 0.034701, loss_ce: 0.013106
2022-01-12 00:55:20,172 iteration 5721 : loss : 0.018235, loss_ce: 0.005442
2022-01-12 00:55:21,773 iteration 5722 : loss : 0.019417, loss_ce: 0.007230
2022-01-12 00:55:23,411 iteration 5723 : loss : 0.029635, loss_ce: 0.010331
2022-01-12 00:55:25,003 iteration 5724 : loss : 0.017184, loss_ce: 0.007626
2022-01-12 00:55:26,543 iteration 5725 : loss : 0.015749, loss_ce: 0.006311
2022-01-12 00:55:28,153 iteration 5726 : loss : 0.012053, loss_ce: 0.004777
2022-01-12 00:55:29,744 iteration 5727 : loss : 0.023917, loss_ce: 0.010840
2022-01-12 00:55:31,275 iteration 5728 : loss : 0.014698, loss_ce: 0.005067
2022-01-12 00:55:32,771 iteration 5729 : loss : 0.016008, loss_ce: 0.004510
 84%|████████████████████████▍    | 337/400 [2:43:03<30:08, 28.70s/it]2022-01-12 00:55:34,463 iteration 5730 : loss : 0.031836, loss_ce: 0.014406
2022-01-12 00:55:36,089 iteration 5731 : loss : 0.031637, loss_ce: 0.010908
2022-01-12 00:55:37,704 iteration 5732 : loss : 0.026881, loss_ce: 0.006478
2022-01-12 00:55:39,282 iteration 5733 : loss : 0.023663, loss_ce: 0.008378
2022-01-12 00:55:40,825 iteration 5734 : loss : 0.012670, loss_ce: 0.005091
2022-01-12 00:55:42,493 iteration 5735 : loss : 0.023116, loss_ce: 0.007339
2022-01-12 00:55:44,107 iteration 5736 : loss : 0.024012, loss_ce: 0.010581
2022-01-12 00:55:45,667 iteration 5737 : loss : 0.017899, loss_ce: 0.006751
2022-01-12 00:55:47,206 iteration 5738 : loss : 0.015696, loss_ce: 0.006887
2022-01-12 00:55:48,794 iteration 5739 : loss : 0.016122, loss_ce: 0.004889
2022-01-12 00:55:50,429 iteration 5740 : loss : 0.021108, loss_ce: 0.009114
2022-01-12 00:55:52,013 iteration 5741 : loss : 0.013378, loss_ce: 0.005360
2022-01-12 00:55:53,555 iteration 5742 : loss : 0.038732, loss_ce: 0.006513
2022-01-12 00:55:55,142 iteration 5743 : loss : 0.021585, loss_ce: 0.007720
2022-01-12 00:55:56,680 iteration 5744 : loss : 0.018516, loss_ce: 0.008275
2022-01-12 00:55:58,263 iteration 5745 : loss : 0.014243, loss_ce: 0.004735
2022-01-12 00:55:59,795 iteration 5746 : loss : 0.019375, loss_ce: 0.009170
 84%|████████████████████████▌    | 338/400 [2:43:30<29:08, 28.20s/it]2022-01-12 00:56:01,434 iteration 5747 : loss : 0.029638, loss_ce: 0.011464
2022-01-12 00:56:02,976 iteration 5748 : loss : 0.020167, loss_ce: 0.008207
2022-01-12 00:56:04,590 iteration 5749 : loss : 0.029385, loss_ce: 0.015714
2022-01-12 00:56:06,178 iteration 5750 : loss : 0.025163, loss_ce: 0.006529
2022-01-12 00:56:07,812 iteration 5751 : loss : 0.036474, loss_ce: 0.014040
2022-01-12 00:56:09,396 iteration 5752 : loss : 0.017054, loss_ce: 0.004817
2022-01-12 00:56:10,991 iteration 5753 : loss : 0.021994, loss_ce: 0.011642
2022-01-12 00:56:12,560 iteration 5754 : loss : 0.016507, loss_ce: 0.005890
2022-01-12 00:56:14,110 iteration 5755 : loss : 0.027967, loss_ce: 0.007515
2022-01-12 00:56:15,647 iteration 5756 : loss : 0.019811, loss_ce: 0.009044
2022-01-12 00:56:17,237 iteration 5757 : loss : 0.025320, loss_ce: 0.008412
2022-01-12 00:56:18,747 iteration 5758 : loss : 0.012943, loss_ce: 0.004172
2022-01-12 00:56:20,363 iteration 5759 : loss : 0.027089, loss_ce: 0.006342
2022-01-12 00:56:21,930 iteration 5760 : loss : 0.017949, loss_ce: 0.007582
2022-01-12 00:56:23,531 iteration 5761 : loss : 0.026835, loss_ce: 0.009461
2022-01-12 00:56:25,154 iteration 5762 : loss : 0.024599, loss_ce: 0.011464
2022-01-12 00:56:26,746 iteration 5763 : loss : 0.020397, loss_ce: 0.008191
 85%|████████████████████████▌    | 339/400 [2:43:57<28:17, 27.82s/it]2022-01-12 00:56:28,416 iteration 5764 : loss : 0.033415, loss_ce: 0.013416
2022-01-12 00:56:30,002 iteration 5765 : loss : 0.023128, loss_ce: 0.006952
2022-01-12 00:56:31,544 iteration 5766 : loss : 0.020844, loss_ce: 0.007537
2022-01-12 00:56:33,157 iteration 5767 : loss : 0.019088, loss_ce: 0.008448
2022-01-12 00:56:34,773 iteration 5768 : loss : 0.033983, loss_ce: 0.010165
2022-01-12 00:56:36,368 iteration 5769 : loss : 0.022641, loss_ce: 0.006723
2022-01-12 00:56:37,958 iteration 5770 : loss : 0.038036, loss_ce: 0.015349
2022-01-12 00:56:39,452 iteration 5771 : loss : 0.012630, loss_ce: 0.005249
2022-01-12 00:56:41,036 iteration 5772 : loss : 0.015318, loss_ce: 0.005687
2022-01-12 00:56:42,539 iteration 5773 : loss : 0.013902, loss_ce: 0.004856
2022-01-12 00:56:44,072 iteration 5774 : loss : 0.016135, loss_ce: 0.006552
2022-01-12 00:56:45,666 iteration 5775 : loss : 0.027343, loss_ce: 0.010377
2022-01-12 00:56:47,148 iteration 5776 : loss : 0.015844, loss_ce: 0.006634
2022-01-12 00:56:48,698 iteration 5777 : loss : 0.015793, loss_ce: 0.005863
2022-01-12 00:56:50,286 iteration 5778 : loss : 0.015092, loss_ce: 0.005324
2022-01-12 00:56:51,862 iteration 5779 : loss : 0.013206, loss_ce: 0.005683
2022-01-12 00:56:51,862 Training Data Eval:
2022-01-12 00:56:59,892   Average segmentation loss on training set: 0.0105
2022-01-12 00:56:59,892 Validation Data Eval:
2022-01-12 00:57:02,654   Average segmentation loss on validation set: 0.0723
2022-01-12 00:57:04,220 iteration 5780 : loss : 0.015667, loss_ce: 0.006105
 85%|████████████████████████▋    | 340/400 [2:44:35<30:43, 30.72s/it]2022-01-12 00:57:05,834 iteration 5781 : loss : 0.017731, loss_ce: 0.006521
2022-01-12 00:57:07,328 iteration 5782 : loss : 0.012372, loss_ce: 0.004469
2022-01-12 00:57:08,917 iteration 5783 : loss : 0.017983, loss_ce: 0.006163
2022-01-12 00:57:10,525 iteration 5784 : loss : 0.026418, loss_ce: 0.010715
2022-01-12 00:57:12,117 iteration 5785 : loss : 0.022535, loss_ce: 0.010115
2022-01-12 00:57:13,754 iteration 5786 : loss : 0.019634, loss_ce: 0.007315
2022-01-12 00:57:15,324 iteration 5787 : loss : 0.022739, loss_ce: 0.006850
2022-01-12 00:57:16,903 iteration 5788 : loss : 0.017520, loss_ce: 0.007091
2022-01-12 00:57:18,480 iteration 5789 : loss : 0.015341, loss_ce: 0.004592
2022-01-12 00:57:20,085 iteration 5790 : loss : 0.027700, loss_ce: 0.013162
2022-01-12 00:57:21,693 iteration 5791 : loss : 0.012932, loss_ce: 0.005102
2022-01-12 00:57:23,328 iteration 5792 : loss : 0.020137, loss_ce: 0.005766
2022-01-12 00:57:24,857 iteration 5793 : loss : 0.018327, loss_ce: 0.007452
2022-01-12 00:57:26,385 iteration 5794 : loss : 0.017478, loss_ce: 0.005869
2022-01-12 00:57:27,938 iteration 5795 : loss : 0.015398, loss_ce: 0.007162
2022-01-12 00:57:29,492 iteration 5796 : loss : 0.014728, loss_ce: 0.005606
2022-01-12 00:57:31,029 iteration 5797 : loss : 0.018242, loss_ce: 0.006279
 85%|████████████████████████▋    | 341/400 [2:45:01<29:03, 29.55s/it]2022-01-12 00:57:32,594 iteration 5798 : loss : 0.016531, loss_ce: 0.005100
2022-01-12 00:57:34,125 iteration 5799 : loss : 0.015143, loss_ce: 0.005577
2022-01-12 00:57:35,591 iteration 5800 : loss : 0.010644, loss_ce: 0.004265
2022-01-12 00:57:37,159 iteration 5801 : loss : 0.018001, loss_ce: 0.008876
2022-01-12 00:57:38,675 iteration 5802 : loss : 0.013880, loss_ce: 0.006040
2022-01-12 00:57:40,269 iteration 5803 : loss : 0.017145, loss_ce: 0.005594
2022-01-12 00:57:41,790 iteration 5804 : loss : 0.017714, loss_ce: 0.006113
2022-01-12 00:57:43,308 iteration 5805 : loss : 0.014871, loss_ce: 0.005256
2022-01-12 00:57:44,813 iteration 5806 : loss : 0.012229, loss_ce: 0.005661
2022-01-12 00:57:46,397 iteration 5807 : loss : 0.020928, loss_ce: 0.007173
2022-01-12 00:57:48,005 iteration 5808 : loss : 0.023629, loss_ce: 0.008021
2022-01-12 00:57:49,615 iteration 5809 : loss : 0.019094, loss_ce: 0.007114
2022-01-12 00:57:51,252 iteration 5810 : loss : 0.019148, loss_ce: 0.005278
2022-01-12 00:57:52,861 iteration 5811 : loss : 0.019849, loss_ce: 0.007946
2022-01-12 00:57:54,412 iteration 5812 : loss : 0.016635, loss_ce: 0.007510
2022-01-12 00:57:55,939 iteration 5813 : loss : 0.016445, loss_ce: 0.005557
2022-01-12 00:57:57,474 iteration 5814 : loss : 0.026913, loss_ce: 0.008005
 86%|████████████████████████▊    | 342/400 [2:45:28<27:39, 28.62s/it]2022-01-12 00:57:59,132 iteration 5815 : loss : 0.023293, loss_ce: 0.009653
2022-01-12 00:58:00,686 iteration 5816 : loss : 0.013860, loss_ce: 0.005112
2022-01-12 00:58:02,331 iteration 5817 : loss : 0.025747, loss_ce: 0.012953
2022-01-12 00:58:03,930 iteration 5818 : loss : 0.029077, loss_ce: 0.012166
2022-01-12 00:58:05,470 iteration 5819 : loss : 0.022680, loss_ce: 0.008140
2022-01-12 00:58:07,001 iteration 5820 : loss : 0.024287, loss_ce: 0.009049
2022-01-12 00:58:08,585 iteration 5821 : loss : 0.015248, loss_ce: 0.006276
2022-01-12 00:58:10,222 iteration 5822 : loss : 0.039234, loss_ce: 0.012388
2022-01-12 00:58:11,729 iteration 5823 : loss : 0.014794, loss_ce: 0.007078
2022-01-12 00:58:13,381 iteration 5824 : loss : 0.027358, loss_ce: 0.006299
2022-01-12 00:58:14,974 iteration 5825 : loss : 0.024455, loss_ce: 0.008459
2022-01-12 00:58:16,548 iteration 5826 : loss : 0.016291, loss_ce: 0.007580
2022-01-12 00:58:18,063 iteration 5827 : loss : 0.017379, loss_ce: 0.007449
2022-01-12 00:58:19,720 iteration 5828 : loss : 0.025182, loss_ce: 0.005968
2022-01-12 00:58:21,279 iteration 5829 : loss : 0.017140, loss_ce: 0.006183
2022-01-12 00:58:22,795 iteration 5830 : loss : 0.019764, loss_ce: 0.006544
2022-01-12 00:58:24,424 iteration 5831 : loss : 0.022522, loss_ce: 0.008366
 86%|████████████████████████▊    | 343/400 [2:45:55<26:42, 28.12s/it]2022-01-12 00:58:26,056 iteration 5832 : loss : 0.021803, loss_ce: 0.009626
2022-01-12 00:58:27,680 iteration 5833 : loss : 0.019981, loss_ce: 0.006658
2022-01-12 00:58:29,248 iteration 5834 : loss : 0.015929, loss_ce: 0.007591
2022-01-12 00:58:30,813 iteration 5835 : loss : 0.023854, loss_ce: 0.011776
2022-01-12 00:58:32,364 iteration 5836 : loss : 0.023975, loss_ce: 0.008941
2022-01-12 00:58:34,011 iteration 5837 : loss : 0.020378, loss_ce: 0.008148
2022-01-12 00:58:35,593 iteration 5838 : loss : 0.020058, loss_ce: 0.006562
2022-01-12 00:58:37,165 iteration 5839 : loss : 0.018839, loss_ce: 0.006495
2022-01-12 00:58:38,726 iteration 5840 : loss : 0.014141, loss_ce: 0.004911
2022-01-12 00:58:40,335 iteration 5841 : loss : 0.027617, loss_ce: 0.008971
2022-01-12 00:58:41,849 iteration 5842 : loss : 0.015045, loss_ce: 0.005122
2022-01-12 00:58:43,364 iteration 5843 : loss : 0.014948, loss_ce: 0.005755
2022-01-12 00:58:44,939 iteration 5844 : loss : 0.025692, loss_ce: 0.009855
2022-01-12 00:58:46,462 iteration 5845 : loss : 0.017639, loss_ce: 0.006702
2022-01-12 00:58:47,985 iteration 5846 : loss : 0.014167, loss_ce: 0.005919
2022-01-12 00:58:49,530 iteration 5847 : loss : 0.017535, loss_ce: 0.008189
2022-01-12 00:58:51,078 iteration 5848 : loss : 0.014947, loss_ce: 0.005107
 86%|████████████████████████▉    | 344/400 [2:46:21<25:50, 27.68s/it]2022-01-12 00:58:52,693 iteration 5849 : loss : 0.020016, loss_ce: 0.006235
2022-01-12 00:58:54,322 iteration 5850 : loss : 0.021430, loss_ce: 0.011658
2022-01-12 00:58:55,937 iteration 5851 : loss : 0.014980, loss_ce: 0.003725
2022-01-12 00:58:57,525 iteration 5852 : loss : 0.018361, loss_ce: 0.008982
2022-01-12 00:58:59,061 iteration 5853 : loss : 0.022538, loss_ce: 0.007545
2022-01-12 00:59:00,673 iteration 5854 : loss : 0.019401, loss_ce: 0.007109
2022-01-12 00:59:02,231 iteration 5855 : loss : 0.015949, loss_ce: 0.007303
2022-01-12 00:59:03,784 iteration 5856 : loss : 0.021311, loss_ce: 0.008418
2022-01-12 00:59:05,373 iteration 5857 : loss : 0.024687, loss_ce: 0.008663
2022-01-12 00:59:06,970 iteration 5858 : loss : 0.015806, loss_ce: 0.004721
2022-01-12 00:59:08,609 iteration 5859 : loss : 0.025673, loss_ce: 0.008941
2022-01-12 00:59:10,171 iteration 5860 : loss : 0.014832, loss_ce: 0.005271
2022-01-12 00:59:11,697 iteration 5861 : loss : 0.018734, loss_ce: 0.008387
2022-01-12 00:59:13,265 iteration 5862 : loss : 0.015679, loss_ce: 0.006987
2022-01-12 00:59:14,841 iteration 5863 : loss : 0.030777, loss_ce: 0.010118
2022-01-12 00:59:16,490 iteration 5864 : loss : 0.017774, loss_ce: 0.008616
2022-01-12 00:59:16,490 Training Data Eval:
2022-01-12 00:59:24,502   Average segmentation loss on training set: 0.0102
2022-01-12 00:59:24,502 Validation Data Eval:
2022-01-12 00:59:27,260   Average segmentation loss on validation set: 0.0700
2022-01-12 00:59:28,928 iteration 5865 : loss : 0.030581, loss_ce: 0.009109
 86%|█████████████████████████    | 345/400 [2:46:59<28:10, 30.73s/it]2022-01-12 00:59:30,486 iteration 5866 : loss : 0.014369, loss_ce: 0.003597
2022-01-12 00:59:32,036 iteration 5867 : loss : 0.018633, loss_ce: 0.007279
2022-01-12 00:59:33,624 iteration 5868 : loss : 0.015052, loss_ce: 0.005959
2022-01-12 00:59:35,171 iteration 5869 : loss : 0.015575, loss_ce: 0.005076
2022-01-12 00:59:36,837 iteration 5870 : loss : 0.038199, loss_ce: 0.015354
2022-01-12 00:59:38,394 iteration 5871 : loss : 0.014796, loss_ce: 0.005089
2022-01-12 00:59:39,933 iteration 5872 : loss : 0.017010, loss_ce: 0.005333
2022-01-12 00:59:41,519 iteration 5873 : loss : 0.025038, loss_ce: 0.014497
2022-01-12 00:59:43,100 iteration 5874 : loss : 0.026261, loss_ce: 0.008363
2022-01-12 00:59:44,690 iteration 5875 : loss : 0.015116, loss_ce: 0.007124
2022-01-12 00:59:46,339 iteration 5876 : loss : 0.026999, loss_ce: 0.012766
2022-01-12 00:59:47,951 iteration 5877 : loss : 0.021092, loss_ce: 0.008409
2022-01-12 00:59:49,455 iteration 5878 : loss : 0.023451, loss_ce: 0.007824
2022-01-12 00:59:51,016 iteration 5879 : loss : 0.020010, loss_ce: 0.007728
2022-01-12 00:59:52,593 iteration 5880 : loss : 0.024071, loss_ce: 0.005663
2022-01-12 00:59:54,129 iteration 5881 : loss : 0.019751, loss_ce: 0.007771
2022-01-12 00:59:55,641 iteration 5882 : loss : 0.018955, loss_ce: 0.007403
 86%|█████████████████████████    | 346/400 [2:47:26<26:34, 29.52s/it]2022-01-12 00:59:57,205 iteration 5883 : loss : 0.017152, loss_ce: 0.004514
2022-01-12 00:59:58,686 iteration 5884 : loss : 0.012742, loss_ce: 0.004954
2022-01-12 01:00:00,304 iteration 5885 : loss : 0.016951, loss_ce: 0.005008
2022-01-12 01:00:01,862 iteration 5886 : loss : 0.024694, loss_ce: 0.007540
2022-01-12 01:00:03,462 iteration 5887 : loss : 0.015427, loss_ce: 0.006154
2022-01-12 01:00:05,026 iteration 5888 : loss : 0.010403, loss_ce: 0.004206
2022-01-12 01:00:06,644 iteration 5889 : loss : 0.016603, loss_ce: 0.005882
2022-01-12 01:00:08,188 iteration 5890 : loss : 0.017876, loss_ce: 0.006923
2022-01-12 01:00:09,749 iteration 5891 : loss : 0.026488, loss_ce: 0.007644
2022-01-12 01:00:11,337 iteration 5892 : loss : 0.024352, loss_ce: 0.007022
2022-01-12 01:00:12,873 iteration 5893 : loss : 0.020382, loss_ce: 0.009980
2022-01-12 01:00:14,381 iteration 5894 : loss : 0.018096, loss_ce: 0.007845
2022-01-12 01:00:15,901 iteration 5895 : loss : 0.013817, loss_ce: 0.005495
2022-01-12 01:00:17,419 iteration 5896 : loss : 0.024915, loss_ce: 0.010592
2022-01-12 01:00:18,975 iteration 5897 : loss : 0.017448, loss_ce: 0.007989
2022-01-12 01:00:20,494 iteration 5898 : loss : 0.057313, loss_ce: 0.025543
2022-01-12 01:00:22,094 iteration 5899 : loss : 0.023586, loss_ce: 0.010514
 87%|█████████████████████████▏   | 347/400 [2:47:52<25:15, 28.60s/it]2022-01-12 01:00:23,628 iteration 5900 : loss : 0.018740, loss_ce: 0.006716
2022-01-12 01:00:25,285 iteration 5901 : loss : 0.017686, loss_ce: 0.007113
2022-01-12 01:00:26,916 iteration 5902 : loss : 0.018414, loss_ce: 0.008530
2022-01-12 01:00:28,453 iteration 5903 : loss : 0.020551, loss_ce: 0.006470
2022-01-12 01:00:30,018 iteration 5904 : loss : 0.016179, loss_ce: 0.007127
2022-01-12 01:00:31,530 iteration 5905 : loss : 0.014120, loss_ce: 0.005181
2022-01-12 01:00:33,075 iteration 5906 : loss : 0.021094, loss_ce: 0.008963
2022-01-12 01:00:34,654 iteration 5907 : loss : 0.019545, loss_ce: 0.007890
2022-01-12 01:00:36,218 iteration 5908 : loss : 0.022889, loss_ce: 0.008868
2022-01-12 01:00:37,741 iteration 5909 : loss : 0.013797, loss_ce: 0.005693
2022-01-12 01:00:39,288 iteration 5910 : loss : 0.021313, loss_ce: 0.010193
2022-01-12 01:00:40,837 iteration 5911 : loss : 0.015787, loss_ce: 0.006286
2022-01-12 01:00:42,507 iteration 5912 : loss : 0.043188, loss_ce: 0.008665
2022-01-12 01:00:43,996 iteration 5913 : loss : 0.018482, loss_ce: 0.006792
2022-01-12 01:00:45,627 iteration 5914 : loss : 0.028650, loss_ce: 0.011985
2022-01-12 01:00:47,222 iteration 5915 : loss : 0.023868, loss_ce: 0.010373
2022-01-12 01:00:48,785 iteration 5916 : loss : 0.019938, loss_ce: 0.007729
 87%|█████████████████████████▏   | 348/400 [2:48:19<24:17, 28.03s/it]2022-01-12 01:00:50,368 iteration 5917 : loss : 0.025774, loss_ce: 0.008757
2022-01-12 01:00:52,009 iteration 5918 : loss : 0.026415, loss_ce: 0.011280
2022-01-12 01:00:53,532 iteration 5919 : loss : 0.016222, loss_ce: 0.009571
2022-01-12 01:00:55,193 iteration 5920 : loss : 0.022188, loss_ce: 0.009703
2022-01-12 01:00:56,737 iteration 5921 : loss : 0.027316, loss_ce: 0.008899
2022-01-12 01:00:58,332 iteration 5922 : loss : 0.021422, loss_ce: 0.009076
2022-01-12 01:00:59,866 iteration 5923 : loss : 0.019099, loss_ce: 0.005522
2022-01-12 01:01:01,391 iteration 5924 : loss : 0.014010, loss_ce: 0.005630
2022-01-12 01:01:02,937 iteration 5925 : loss : 0.016412, loss_ce: 0.004727
2022-01-12 01:01:04,496 iteration 5926 : loss : 0.016892, loss_ce: 0.006871
2022-01-12 01:01:06,121 iteration 5927 : loss : 0.021384, loss_ce: 0.008506
2022-01-12 01:01:07,586 iteration 5928 : loss : 0.013659, loss_ce: 0.006028
2022-01-12 01:01:09,101 iteration 5929 : loss : 0.025463, loss_ce: 0.007692
2022-01-12 01:01:10,684 iteration 5930 : loss : 0.015168, loss_ce: 0.005395
2022-01-12 01:01:12,259 iteration 5931 : loss : 0.016035, loss_ce: 0.007352
2022-01-12 01:01:13,810 iteration 5932 : loss : 0.021825, loss_ce: 0.009808
2022-01-12 01:01:15,331 iteration 5933 : loss : 0.012015, loss_ce: 0.004774
 87%|█████████████████████████▎   | 349/400 [2:48:46<23:26, 27.59s/it]2022-01-12 01:01:16,875 iteration 5934 : loss : 0.014999, loss_ce: 0.005408
2022-01-12 01:01:18,376 iteration 5935 : loss : 0.018044, loss_ce: 0.008905
2022-01-12 01:01:19,875 iteration 5936 : loss : 0.022679, loss_ce: 0.010855
2022-01-12 01:01:21,467 iteration 5937 : loss : 0.028998, loss_ce: 0.012833
2022-01-12 01:01:23,044 iteration 5938 : loss : 0.022361, loss_ce: 0.011296
2022-01-12 01:01:24,667 iteration 5939 : loss : 0.031106, loss_ce: 0.008921
2022-01-12 01:01:26,203 iteration 5940 : loss : 0.011626, loss_ce: 0.004601
2022-01-12 01:01:27,815 iteration 5941 : loss : 0.019449, loss_ce: 0.007143
2022-01-12 01:01:29,391 iteration 5942 : loss : 0.018179, loss_ce: 0.007237
2022-01-12 01:01:30,992 iteration 5943 : loss : 0.022164, loss_ce: 0.009492
2022-01-12 01:01:32,469 iteration 5944 : loss : 0.016530, loss_ce: 0.006860
2022-01-12 01:01:34,136 iteration 5945 : loss : 0.026719, loss_ce: 0.007814
2022-01-12 01:01:35,716 iteration 5946 : loss : 0.013316, loss_ce: 0.005499
2022-01-12 01:01:37,380 iteration 5947 : loss : 0.030996, loss_ce: 0.009925
2022-01-12 01:01:39,001 iteration 5948 : loss : 0.019174, loss_ce: 0.006224
2022-01-12 01:01:40,564 iteration 5949 : loss : 0.016835, loss_ce: 0.004145
2022-01-12 01:01:40,564 Training Data Eval:
2022-01-12 01:01:48,577   Average segmentation loss on training set: 0.0101
2022-01-12 01:01:48,577 Validation Data Eval:
2022-01-12 01:01:51,344   Average segmentation loss on validation set: 0.0764
2022-01-12 01:01:52,860 iteration 5950 : loss : 0.013750, loss_ce: 0.004700
 88%|█████████████████████████▍   | 350/400 [2:49:23<25:28, 30.57s/it]2022-01-12 01:01:54,402 iteration 5951 : loss : 0.017892, loss_ce: 0.005749
2022-01-12 01:01:55,990 iteration 5952 : loss : 0.020773, loss_ce: 0.010081
2022-01-12 01:01:57,656 iteration 5953 : loss : 0.028341, loss_ce: 0.013631
2022-01-12 01:01:59,201 iteration 5954 : loss : 0.017723, loss_ce: 0.004861
2022-01-12 01:02:00,866 iteration 5955 : loss : 0.029034, loss_ce: 0.010272
2022-01-12 01:02:02,406 iteration 5956 : loss : 0.020628, loss_ce: 0.005164
2022-01-12 01:02:04,009 iteration 5957 : loss : 0.025433, loss_ce: 0.011683
2022-01-12 01:02:05,614 iteration 5958 : loss : 0.026805, loss_ce: 0.011610
2022-01-12 01:02:07,213 iteration 5959 : loss : 0.016044, loss_ce: 0.003978
2022-01-12 01:02:08,738 iteration 5960 : loss : 0.024596, loss_ce: 0.006097
2022-01-12 01:02:10,333 iteration 5961 : loss : 0.025884, loss_ce: 0.012785
2022-01-12 01:02:11,922 iteration 5962 : loss : 0.017498, loss_ce: 0.007192
2022-01-12 01:02:13,441 iteration 5963 : loss : 0.019330, loss_ce: 0.007042
2022-01-12 01:02:15,049 iteration 5964 : loss : 0.019659, loss_ce: 0.007910
2022-01-12 01:02:16,630 iteration 5965 : loss : 0.014283, loss_ce: 0.007443
2022-01-12 01:02:18,158 iteration 5966 : loss : 0.012979, loss_ce: 0.004381
2022-01-12 01:02:19,795 iteration 5967 : loss : 0.019214, loss_ce: 0.007617
 88%|█████████████████████████▍   | 351/400 [2:49:50<24:04, 29.48s/it]2022-01-12 01:02:21,389 iteration 5968 : loss : 0.018889, loss_ce: 0.008026
2022-01-12 01:02:22,939 iteration 5969 : loss : 0.015734, loss_ce: 0.007686
2022-01-12 01:02:24,515 iteration 5970 : loss : 0.021913, loss_ce: 0.009395
2022-01-12 01:02:26,033 iteration 5971 : loss : 0.013824, loss_ce: 0.006214
2022-01-12 01:02:27,543 iteration 5972 : loss : 0.019880, loss_ce: 0.005381
2022-01-12 01:02:29,062 iteration 5973 : loss : 0.017335, loss_ce: 0.007710
2022-01-12 01:02:30,621 iteration 5974 : loss : 0.026445, loss_ce: 0.007682
2022-01-12 01:02:32,198 iteration 5975 : loss : 0.045746, loss_ce: 0.034871
2022-01-12 01:02:33,792 iteration 5976 : loss : 0.019930, loss_ce: 0.005997
2022-01-12 01:02:35,302 iteration 5977 : loss : 0.018244, loss_ce: 0.006951
2022-01-12 01:02:36,844 iteration 5978 : loss : 0.019481, loss_ce: 0.006219
2022-01-12 01:02:38,410 iteration 5979 : loss : 0.017346, loss_ce: 0.004777
2022-01-12 01:02:40,032 iteration 5980 : loss : 0.020892, loss_ce: 0.004895
2022-01-12 01:02:41,654 iteration 5981 : loss : 0.021356, loss_ce: 0.010644
2022-01-12 01:02:43,209 iteration 5982 : loss : 0.034705, loss_ce: 0.017862
2022-01-12 01:02:44,764 iteration 5983 : loss : 0.030500, loss_ce: 0.007658
2022-01-12 01:02:46,384 iteration 5984 : loss : 0.017894, loss_ce: 0.005399
 88%|█████████████████████████▌   | 352/400 [2:50:17<22:53, 28.61s/it]2022-01-12 01:02:48,016 iteration 5985 : loss : 0.028996, loss_ce: 0.010223
2022-01-12 01:02:49,490 iteration 5986 : loss : 0.011884, loss_ce: 0.005744
2022-01-12 01:02:51,098 iteration 5987 : loss : 0.023257, loss_ce: 0.006279
2022-01-12 01:02:52,589 iteration 5988 : loss : 0.016283, loss_ce: 0.006581
2022-01-12 01:02:54,169 iteration 5989 : loss : 0.015307, loss_ce: 0.005903
2022-01-12 01:02:55,719 iteration 5990 : loss : 0.024216, loss_ce: 0.008335
2022-01-12 01:02:57,337 iteration 5991 : loss : 0.031009, loss_ce: 0.010110
2022-01-12 01:02:58,951 iteration 5992 : loss : 0.032403, loss_ce: 0.009580
2022-01-12 01:03:00,505 iteration 5993 : loss : 0.017316, loss_ce: 0.006672
2022-01-12 01:03:01,993 iteration 5994 : loss : 0.012755, loss_ce: 0.005903
2022-01-12 01:03:03,628 iteration 5995 : loss : 0.021083, loss_ce: 0.008599
2022-01-12 01:03:05,233 iteration 5996 : loss : 0.016515, loss_ce: 0.008827
2022-01-12 01:03:06,757 iteration 5997 : loss : 0.017720, loss_ce: 0.005448
2022-01-12 01:03:08,436 iteration 5998 : loss : 0.018347, loss_ce: 0.006394
2022-01-12 01:03:09,963 iteration 5999 : loss : 0.024209, loss_ce: 0.008668
2022-01-12 01:03:11,625 iteration 6000 : loss : 0.028547, loss_ce: 0.012911
2022-01-12 01:03:13,137 iteration 6001 : loss : 0.012324, loss_ce: 0.004733
 88%|█████████████████████████▌   | 353/400 [2:50:43<21:58, 28.05s/it]2022-01-12 01:03:14,680 iteration 6002 : loss : 0.015256, loss_ce: 0.005831
2022-01-12 01:03:16,277 iteration 6003 : loss : 0.013624, loss_ce: 0.006328
2022-01-12 01:03:17,817 iteration 6004 : loss : 0.017294, loss_ce: 0.007035
2022-01-12 01:03:19,356 iteration 6005 : loss : 0.021137, loss_ce: 0.007338
2022-01-12 01:03:20,982 iteration 6006 : loss : 0.018765, loss_ce: 0.005749
2022-01-12 01:03:22,506 iteration 6007 : loss : 0.014623, loss_ce: 0.005555
2022-01-12 01:03:24,062 iteration 6008 : loss : 0.019627, loss_ce: 0.008765
2022-01-12 01:03:25,673 iteration 6009 : loss : 0.026562, loss_ce: 0.007405
2022-01-12 01:03:27,191 iteration 6010 : loss : 0.020011, loss_ce: 0.004676
2022-01-12 01:03:28,784 iteration 6011 : loss : 0.031579, loss_ce: 0.011013
2022-01-12 01:03:30,395 iteration 6012 : loss : 0.018598, loss_ce: 0.008974
2022-01-12 01:03:31,945 iteration 6013 : loss : 0.016796, loss_ce: 0.006355
2022-01-12 01:03:33,470 iteration 6014 : loss : 0.014779, loss_ce: 0.005397
2022-01-12 01:03:34,966 iteration 6015 : loss : 0.014557, loss_ce: 0.007638
2022-01-12 01:03:36,503 iteration 6016 : loss : 0.013558, loss_ce: 0.004030
2022-01-12 01:03:38,060 iteration 6017 : loss : 0.019074, loss_ce: 0.008238
2022-01-12 01:03:39,541 iteration 6018 : loss : 0.017990, loss_ce: 0.006874
 88%|█████████████████████████▋   | 354/400 [2:51:10<21:07, 27.56s/it]2022-01-12 01:03:41,188 iteration 6019 : loss : 0.015669, loss_ce: 0.005909
2022-01-12 01:03:42,769 iteration 6020 : loss : 0.017662, loss_ce: 0.007004
2022-01-12 01:03:44,293 iteration 6021 : loss : 0.012106, loss_ce: 0.004359
2022-01-12 01:03:45,892 iteration 6022 : loss : 0.015842, loss_ce: 0.006371
2022-01-12 01:03:47,485 iteration 6023 : loss : 0.019545, loss_ce: 0.008426
2022-01-12 01:03:49,063 iteration 6024 : loss : 0.016216, loss_ce: 0.007062
2022-01-12 01:03:50,580 iteration 6025 : loss : 0.017397, loss_ce: 0.003934
2022-01-12 01:03:52,123 iteration 6026 : loss : 0.039741, loss_ce: 0.018182
2022-01-12 01:03:53,723 iteration 6027 : loss : 0.013906, loss_ce: 0.006127
2022-01-12 01:03:55,234 iteration 6028 : loss : 0.012912, loss_ce: 0.005839
2022-01-12 01:03:56,809 iteration 6029 : loss : 0.017254, loss_ce: 0.007643
2022-01-12 01:03:58,434 iteration 6030 : loss : 0.022693, loss_ce: 0.009486
2022-01-12 01:04:00,045 iteration 6031 : loss : 0.029080, loss_ce: 0.008887
2022-01-12 01:04:01,602 iteration 6032 : loss : 0.027313, loss_ce: 0.007971
2022-01-12 01:04:03,149 iteration 6033 : loss : 0.030802, loss_ce: 0.010789
2022-01-12 01:04:04,668 iteration 6034 : loss : 0.011163, loss_ce: 0.004214
2022-01-12 01:04:04,669 Training Data Eval:
2022-01-12 01:04:12,688   Average segmentation loss on training set: 0.0098
2022-01-12 01:04:12,689 Validation Data Eval:
2022-01-12 01:04:15,452   Average segmentation loss on validation set: 0.0705
2022-01-12 01:04:16,976 iteration 6035 : loss : 0.016726, loss_ce: 0.007064
 89%|█████████████████████████▋   | 355/400 [2:51:47<22:53, 30.52s/it]2022-01-12 01:04:18,592 iteration 6036 : loss : 0.017389, loss_ce: 0.008285
2022-01-12 01:04:20,115 iteration 6037 : loss : 0.012948, loss_ce: 0.004904
2022-01-12 01:04:21,650 iteration 6038 : loss : 0.016571, loss_ce: 0.006692
2022-01-12 01:04:23,129 iteration 6039 : loss : 0.012468, loss_ce: 0.004451
2022-01-12 01:04:24,660 iteration 6040 : loss : 0.018514, loss_ce: 0.006373
2022-01-12 01:04:26,274 iteration 6041 : loss : 0.021373, loss_ce: 0.008733
2022-01-12 01:04:27,794 iteration 6042 : loss : 0.015476, loss_ce: 0.004077
2022-01-12 01:04:29,283 iteration 6043 : loss : 0.014457, loss_ce: 0.004231
2022-01-12 01:04:30,871 iteration 6044 : loss : 0.014474, loss_ce: 0.004506
2022-01-12 01:04:32,405 iteration 6045 : loss : 0.015284, loss_ce: 0.006484
2022-01-12 01:04:33,969 iteration 6046 : loss : 0.019525, loss_ce: 0.008456
2022-01-12 01:04:35,509 iteration 6047 : loss : 0.013032, loss_ce: 0.005149
2022-01-12 01:04:37,109 iteration 6048 : loss : 0.029630, loss_ce: 0.011523
2022-01-12 01:04:38,659 iteration 6049 : loss : 0.013585, loss_ce: 0.006653
2022-01-12 01:04:40,228 iteration 6050 : loss : 0.021765, loss_ce: 0.008924
2022-01-12 01:04:41,895 iteration 6051 : loss : 0.016688, loss_ce: 0.005770
2022-01-12 01:04:43,437 iteration 6052 : loss : 0.022237, loss_ce: 0.012403
 89%|█████████████████████████▊   | 356/400 [2:52:14<21:29, 29.30s/it]2022-01-12 01:04:44,983 iteration 6053 : loss : 0.025702, loss_ce: 0.009517
2022-01-12 01:04:46,481 iteration 6054 : loss : 0.014568, loss_ce: 0.007237
2022-01-12 01:04:48,089 iteration 6055 : loss : 0.018309, loss_ce: 0.005978
2022-01-12 01:04:49,650 iteration 6056 : loss : 0.018153, loss_ce: 0.008114
2022-01-12 01:04:51,176 iteration 6057 : loss : 0.019376, loss_ce: 0.006307
2022-01-12 01:04:52,757 iteration 6058 : loss : 0.020084, loss_ce: 0.006605
2022-01-12 01:04:54,385 iteration 6059 : loss : 0.019666, loss_ce: 0.010989
2022-01-12 01:04:56,095 iteration 6060 : loss : 0.033497, loss_ce: 0.013391
2022-01-12 01:04:57,642 iteration 6061 : loss : 0.014057, loss_ce: 0.006101
2022-01-12 01:04:59,278 iteration 6062 : loss : 0.017060, loss_ce: 0.006018
2022-01-12 01:05:00,804 iteration 6063 : loss : 0.023585, loss_ce: 0.008418
2022-01-12 01:05:02,429 iteration 6064 : loss : 0.018456, loss_ce: 0.008009
2022-01-12 01:05:03,934 iteration 6065 : loss : 0.012505, loss_ce: 0.004584
2022-01-12 01:05:05,455 iteration 6066 : loss : 0.011093, loss_ce: 0.003518
2022-01-12 01:05:07,001 iteration 6067 : loss : 0.015469, loss_ce: 0.006549
2022-01-12 01:05:08,525 iteration 6068 : loss : 0.011732, loss_ce: 0.003385
2022-01-12 01:05:10,073 iteration 6069 : loss : 0.041812, loss_ce: 0.017825
 89%|█████████████████████████▉   | 357/400 [2:52:40<20:25, 28.51s/it]2022-01-12 01:05:11,674 iteration 6070 : loss : 0.011034, loss_ce: 0.005763
2022-01-12 01:05:13,192 iteration 6071 : loss : 0.014834, loss_ce: 0.004694
2022-01-12 01:05:14,723 iteration 6072 : loss : 0.018044, loss_ce: 0.011288
2022-01-12 01:05:16,289 iteration 6073 : loss : 0.020570, loss_ce: 0.007361
2022-01-12 01:05:17,881 iteration 6074 : loss : 0.015636, loss_ce: 0.006511
2022-01-12 01:05:19,369 iteration 6075 : loss : 0.011903, loss_ce: 0.005429
2022-01-12 01:05:20,919 iteration 6076 : loss : 0.014134, loss_ce: 0.003093
2022-01-12 01:05:22,500 iteration 6077 : loss : 0.021199, loss_ce: 0.008728
2022-01-12 01:05:24,160 iteration 6078 : loss : 0.025181, loss_ce: 0.010131
2022-01-12 01:05:25,821 iteration 6079 : loss : 0.023482, loss_ce: 0.008530
2022-01-12 01:05:27,383 iteration 6080 : loss : 0.022642, loss_ce: 0.007966
2022-01-12 01:05:28,973 iteration 6081 : loss : 0.021758, loss_ce: 0.008642
2022-01-12 01:05:30,463 iteration 6082 : loss : 0.016906, loss_ce: 0.005126
2022-01-12 01:05:31,983 iteration 6083 : loss : 0.014672, loss_ce: 0.004538
2022-01-12 01:05:33,541 iteration 6084 : loss : 0.020829, loss_ce: 0.005956
2022-01-12 01:05:35,082 iteration 6085 : loss : 0.031789, loss_ce: 0.015720
2022-01-12 01:05:36,625 iteration 6086 : loss : 0.014032, loss_ce: 0.004828
 90%|█████████████████████████▉   | 358/400 [2:53:07<19:32, 27.92s/it]2022-01-12 01:05:38,175 iteration 6087 : loss : 0.015680, loss_ce: 0.005128
2022-01-12 01:05:39,773 iteration 6088 : loss : 0.019493, loss_ce: 0.008000
2022-01-12 01:05:41,307 iteration 6089 : loss : 0.016869, loss_ce: 0.005176
2022-01-12 01:05:42,774 iteration 6090 : loss : 0.011689, loss_ce: 0.005089
2022-01-12 01:05:44,356 iteration 6091 : loss : 0.014141, loss_ce: 0.005170
2022-01-12 01:05:45,941 iteration 6092 : loss : 0.016008, loss_ce: 0.004637
2022-01-12 01:05:47,488 iteration 6093 : loss : 0.022137, loss_ce: 0.011264
2022-01-12 01:05:48,997 iteration 6094 : loss : 0.019576, loss_ce: 0.007340
2022-01-12 01:05:50,534 iteration 6095 : loss : 0.020627, loss_ce: 0.006084
2022-01-12 01:05:52,119 iteration 6096 : loss : 0.016537, loss_ce: 0.005571
2022-01-12 01:05:53,703 iteration 6097 : loss : 0.012671, loss_ce: 0.005052
2022-01-12 01:05:55,347 iteration 6098 : loss : 0.023236, loss_ce: 0.012485
2022-01-12 01:05:56,943 iteration 6099 : loss : 0.014245, loss_ce: 0.006084
2022-01-12 01:05:58,527 iteration 6100 : loss : 0.028350, loss_ce: 0.019393
2022-01-12 01:06:00,106 iteration 6101 : loss : 0.020305, loss_ce: 0.008129
2022-01-12 01:06:01,713 iteration 6102 : loss : 0.019009, loss_ce: 0.010493
2022-01-12 01:06:03,321 iteration 6103 : loss : 0.023755, loss_ce: 0.008705
 90%|██████████████████████████   | 359/400 [2:53:34<18:49, 27.55s/it]2022-01-12 01:06:04,965 iteration 6104 : loss : 0.027542, loss_ce: 0.012492
2022-01-12 01:06:06,565 iteration 6105 : loss : 0.014861, loss_ce: 0.007518
2022-01-12 01:06:08,146 iteration 6106 : loss : 0.020288, loss_ce: 0.009480
2022-01-12 01:06:09,719 iteration 6107 : loss : 0.019961, loss_ce: 0.008067
2022-01-12 01:06:11,254 iteration 6108 : loss : 0.016784, loss_ce: 0.007242
2022-01-12 01:06:12,796 iteration 6109 : loss : 0.017939, loss_ce: 0.004293
2022-01-12 01:06:14,324 iteration 6110 : loss : 0.017884, loss_ce: 0.007082
2022-01-12 01:06:15,872 iteration 6111 : loss : 0.014876, loss_ce: 0.006108
2022-01-12 01:06:17,429 iteration 6112 : loss : 0.026515, loss_ce: 0.008533
2022-01-12 01:06:18,989 iteration 6113 : loss : 0.015272, loss_ce: 0.006756
2022-01-12 01:06:20,594 iteration 6114 : loss : 0.015515, loss_ce: 0.004751
2022-01-12 01:06:22,141 iteration 6115 : loss : 0.015468, loss_ce: 0.005812
2022-01-12 01:06:23,710 iteration 6116 : loss : 0.020276, loss_ce: 0.006291
2022-01-12 01:06:25,237 iteration 6117 : loss : 0.015654, loss_ce: 0.006177
2022-01-12 01:06:26,751 iteration 6118 : loss : 0.012087, loss_ce: 0.003766
2022-01-12 01:06:28,401 iteration 6119 : loss : 0.057527, loss_ce: 0.027722
2022-01-12 01:06:28,401 Training Data Eval:
2022-01-12 01:06:36,412   Average segmentation loss on training set: 0.0097
2022-01-12 01:06:36,413 Validation Data Eval:
2022-01-12 01:06:39,171   Average segmentation loss on validation set: 0.0712
2022-01-12 01:06:40,732 iteration 6120 : loss : 0.030997, loss_ce: 0.009839
 90%|██████████████████████████   | 360/400 [2:54:11<20:20, 30.51s/it]2022-01-12 01:06:42,330 iteration 6121 : loss : 0.023246, loss_ce: 0.007499
2022-01-12 01:06:43,899 iteration 6122 : loss : 0.018667, loss_ce: 0.008394
2022-01-12 01:06:45,455 iteration 6123 : loss : 0.034492, loss_ce: 0.013283
2022-01-12 01:06:46,971 iteration 6124 : loss : 0.010793, loss_ce: 0.004050
2022-01-12 01:06:48,529 iteration 6125 : loss : 0.024235, loss_ce: 0.010040
2022-01-12 01:06:50,058 iteration 6126 : loss : 0.015393, loss_ce: 0.008384
2022-01-12 01:06:51,663 iteration 6127 : loss : 0.013708, loss_ce: 0.004756
2022-01-12 01:06:53,203 iteration 6128 : loss : 0.016928, loss_ce: 0.006336
2022-01-12 01:06:54,862 iteration 6129 : loss : 0.016549, loss_ce: 0.006745
2022-01-12 01:06:56,401 iteration 6130 : loss : 0.018212, loss_ce: 0.004882
2022-01-12 01:06:58,054 iteration 6131 : loss : 0.023088, loss_ce: 0.012960
2022-01-12 01:06:59,605 iteration 6132 : loss : 0.017487, loss_ce: 0.009045
2022-01-12 01:07:01,192 iteration 6133 : loss : 0.024331, loss_ce: 0.007341
2022-01-12 01:07:02,771 iteration 6134 : loss : 0.021243, loss_ce: 0.007175
2022-01-12 01:07:04,429 iteration 6135 : loss : 0.024575, loss_ce: 0.009035
2022-01-12 01:07:05,987 iteration 6136 : loss : 0.017746, loss_ce: 0.005428
2022-01-12 01:07:07,520 iteration 6137 : loss : 0.017160, loss_ce: 0.006711
 90%|██████████████████████████▏  | 361/400 [2:54:38<19:06, 29.39s/it]2022-01-12 01:07:09,092 iteration 6138 : loss : 0.020521, loss_ce: 0.007579
2022-01-12 01:07:10,639 iteration 6139 : loss : 0.014289, loss_ce: 0.005436
2022-01-12 01:07:12,171 iteration 6140 : loss : 0.020749, loss_ce: 0.010222
2022-01-12 01:07:13,668 iteration 6141 : loss : 0.012367, loss_ce: 0.004767
2022-01-12 01:07:15,145 iteration 6142 : loss : 0.011331, loss_ce: 0.005207
2022-01-12 01:07:16,750 iteration 6143 : loss : 0.019966, loss_ce: 0.005650
2022-01-12 01:07:18,301 iteration 6144 : loss : 0.019023, loss_ce: 0.007708
2022-01-12 01:07:19,833 iteration 6145 : loss : 0.015769, loss_ce: 0.007231
2022-01-12 01:07:21,345 iteration 6146 : loss : 0.024371, loss_ce: 0.007303
2022-01-12 01:07:22,899 iteration 6147 : loss : 0.013318, loss_ce: 0.005405
2022-01-12 01:07:24,500 iteration 6148 : loss : 0.023187, loss_ce: 0.008871
2022-01-12 01:07:26,098 iteration 6149 : loss : 0.020438, loss_ce: 0.009417
2022-01-12 01:07:27,733 iteration 6150 : loss : 0.018238, loss_ce: 0.006335
2022-01-12 01:07:29,372 iteration 6151 : loss : 0.016710, loss_ce: 0.004144
2022-01-12 01:07:30,893 iteration 6152 : loss : 0.013985, loss_ce: 0.005628
2022-01-12 01:07:32,475 iteration 6153 : loss : 0.114920, loss_ce: 0.021132
2022-01-12 01:07:34,057 iteration 6154 : loss : 0.019803, loss_ce: 0.007751
 90%|██████████████████████████▏  | 362/400 [2:55:04<18:04, 28.53s/it]2022-01-12 01:07:35,693 iteration 6155 : loss : 0.019859, loss_ce: 0.006756
2022-01-12 01:07:37,285 iteration 6156 : loss : 0.026545, loss_ce: 0.010725
2022-01-12 01:07:38,862 iteration 6157 : loss : 0.025459, loss_ce: 0.009320
2022-01-12 01:07:40,488 iteration 6158 : loss : 0.020646, loss_ce: 0.006565
2022-01-12 01:07:42,018 iteration 6159 : loss : 0.014319, loss_ce: 0.005095
2022-01-12 01:07:43,565 iteration 6160 : loss : 0.013895, loss_ce: 0.005136
2022-01-12 01:07:45,169 iteration 6161 : loss : 0.022487, loss_ce: 0.006163
2022-01-12 01:07:46,796 iteration 6162 : loss : 0.025441, loss_ce: 0.006124
2022-01-12 01:07:48,366 iteration 6163 : loss : 0.021854, loss_ce: 0.008872
2022-01-12 01:07:49,863 iteration 6164 : loss : 0.013808, loss_ce: 0.003969
2022-01-12 01:07:51,487 iteration 6165 : loss : 0.018605, loss_ce: 0.006841
2022-01-12 01:07:53,024 iteration 6166 : loss : 0.013088, loss_ce: 0.004862
2022-01-12 01:07:54,636 iteration 6167 : loss : 0.019809, loss_ce: 0.010398
2022-01-12 01:07:56,216 iteration 6168 : loss : 0.025539, loss_ce: 0.010273
2022-01-12 01:07:57,772 iteration 6169 : loss : 0.036016, loss_ce: 0.009855
2022-01-12 01:07:59,294 iteration 6170 : loss : 0.013864, loss_ce: 0.006626
2022-01-12 01:08:00,832 iteration 6171 : loss : 0.022310, loss_ce: 0.011900
 91%|██████████████████████████▎  | 363/400 [2:55:31<17:16, 28.01s/it]2022-01-12 01:08:02,409 iteration 6172 : loss : 0.028208, loss_ce: 0.011092
2022-01-12 01:08:04,043 iteration 6173 : loss : 0.027469, loss_ce: 0.007826
2022-01-12 01:08:05,628 iteration 6174 : loss : 0.022957, loss_ce: 0.009908
2022-01-12 01:08:07,294 iteration 6175 : loss : 0.022775, loss_ce: 0.008063
2022-01-12 01:08:08,877 iteration 6176 : loss : 0.013743, loss_ce: 0.005175
2022-01-12 01:08:10,433 iteration 6177 : loss : 0.013082, loss_ce: 0.006073
2022-01-12 01:08:12,012 iteration 6178 : loss : 0.017440, loss_ce: 0.006821
2022-01-12 01:08:13,541 iteration 6179 : loss : 0.018824, loss_ce: 0.005988
2022-01-12 01:08:15,169 iteration 6180 : loss : 0.023967, loss_ce: 0.011021
2022-01-12 01:08:16,702 iteration 6181 : loss : 0.013880, loss_ce: 0.004758
2022-01-12 01:08:18,239 iteration 6182 : loss : 0.017315, loss_ce: 0.004467
2022-01-12 01:08:19,846 iteration 6183 : loss : 0.016387, loss_ce: 0.006510
2022-01-12 01:08:21,427 iteration 6184 : loss : 0.013078, loss_ce: 0.005695
2022-01-12 01:08:23,058 iteration 6185 : loss : 0.036826, loss_ce: 0.008916
2022-01-12 01:08:24,618 iteration 6186 : loss : 0.013374, loss_ce: 0.004861
2022-01-12 01:08:26,207 iteration 6187 : loss : 0.018646, loss_ce: 0.010626
2022-01-12 01:08:27,852 iteration 6188 : loss : 0.023081, loss_ce: 0.010751
 91%|██████████████████████████▍  | 364/400 [2:55:58<16:37, 27.71s/it]2022-01-12 01:08:29,511 iteration 6189 : loss : 0.018523, loss_ce: 0.003840
2022-01-12 01:08:31,121 iteration 6190 : loss : 0.023362, loss_ce: 0.006132
2022-01-12 01:08:32,660 iteration 6191 : loss : 0.015179, loss_ce: 0.007484
2022-01-12 01:08:34,173 iteration 6192 : loss : 0.015490, loss_ce: 0.005663
2022-01-12 01:08:35,739 iteration 6193 : loss : 0.019030, loss_ce: 0.005883
2022-01-12 01:08:37,253 iteration 6194 : loss : 0.018389, loss_ce: 0.007768
2022-01-12 01:08:38,905 iteration 6195 : loss : 0.017551, loss_ce: 0.006003
2022-01-12 01:08:40,465 iteration 6196 : loss : 0.024407, loss_ce: 0.007508
2022-01-12 01:08:41,985 iteration 6197 : loss : 0.015906, loss_ce: 0.007970
2022-01-12 01:08:43,675 iteration 6198 : loss : 0.029409, loss_ce: 0.008761
2022-01-12 01:08:45,228 iteration 6199 : loss : 0.023201, loss_ce: 0.009744
2022-01-12 01:08:46,783 iteration 6200 : loss : 0.013858, loss_ce: 0.006013
2022-01-12 01:08:48,373 iteration 6201 : loss : 0.017201, loss_ce: 0.007042
2022-01-12 01:08:49,892 iteration 6202 : loss : 0.014383, loss_ce: 0.005194
2022-01-12 01:08:51,622 iteration 6203 : loss : 0.028724, loss_ce: 0.012303
2022-01-12 01:08:53,133 iteration 6204 : loss : 0.014179, loss_ce: 0.006635
2022-01-12 01:08:53,134 Training Data Eval:
2022-01-12 01:09:01,160   Average segmentation loss on training set: 0.0097
2022-01-12 01:09:01,161 Validation Data Eval:
2022-01-12 01:09:03,924   Average segmentation loss on validation set: 0.0717
2022-01-12 01:09:05,551 iteration 6205 : loss : 0.018724, loss_ce: 0.006212
 91%|██████████████████████████▍  | 365/400 [2:56:36<17:54, 30.71s/it]2022-01-12 01:09:07,185 iteration 6206 : loss : 0.020758, loss_ce: 0.009236
2022-01-12 01:09:08,785 iteration 6207 : loss : 0.028287, loss_ce: 0.013814
2022-01-12 01:09:10,407 iteration 6208 : loss : 0.032050, loss_ce: 0.009422
2022-01-12 01:09:11,993 iteration 6209 : loss : 0.017188, loss_ce: 0.006189
2022-01-12 01:09:13,586 iteration 6210 : loss : 0.012292, loss_ce: 0.004741
2022-01-12 01:09:15,138 iteration 6211 : loss : 0.023376, loss_ce: 0.013250
2022-01-12 01:09:16,681 iteration 6212 : loss : 0.014463, loss_ce: 0.005861
2022-01-12 01:09:18,170 iteration 6213 : loss : 0.013759, loss_ce: 0.005066
2022-01-12 01:09:19,815 iteration 6214 : loss : 0.018486, loss_ce: 0.007086
2022-01-12 01:09:21,379 iteration 6215 : loss : 0.015462, loss_ce: 0.004814
2022-01-12 01:09:22,954 iteration 6216 : loss : 0.015236, loss_ce: 0.005665
2022-01-12 01:09:24,550 iteration 6217 : loss : 0.024709, loss_ce: 0.012355
2022-01-12 01:09:26,071 iteration 6218 : loss : 0.016577, loss_ce: 0.007387
2022-01-12 01:09:27,655 iteration 6219 : loss : 0.022360, loss_ce: 0.008038
2022-01-12 01:09:29,149 iteration 6220 : loss : 0.014112, loss_ce: 0.003894
2022-01-12 01:09:30,787 iteration 6221 : loss : 0.017664, loss_ce: 0.005204
2022-01-12 01:09:32,362 iteration 6222 : loss : 0.015566, loss_ce: 0.008590
 92%|██████████████████████████▌  | 366/400 [2:57:03<16:44, 29.54s/it]2022-01-12 01:09:33,985 iteration 6223 : loss : 0.016927, loss_ce: 0.007829
2022-01-12 01:09:35,576 iteration 6224 : loss : 0.014184, loss_ce: 0.006236
2022-01-12 01:09:37,171 iteration 6225 : loss : 0.013890, loss_ce: 0.006651
2022-01-12 01:09:38,726 iteration 6226 : loss : 0.019888, loss_ce: 0.007076
2022-01-12 01:09:40,378 iteration 6227 : loss : 0.015733, loss_ce: 0.006104
2022-01-12 01:09:41,939 iteration 6228 : loss : 0.019343, loss_ce: 0.006754
2022-01-12 01:09:43,542 iteration 6229 : loss : 0.019180, loss_ce: 0.006450
2022-01-12 01:09:45,166 iteration 6230 : loss : 0.036649, loss_ce: 0.007653
2022-01-12 01:09:46,686 iteration 6231 : loss : 0.017294, loss_ce: 0.008051
2022-01-12 01:09:48,226 iteration 6232 : loss : 0.015981, loss_ce: 0.005638
2022-01-12 01:09:49,859 iteration 6233 : loss : 0.025782, loss_ce: 0.009562
2022-01-12 01:09:51,401 iteration 6234 : loss : 0.024286, loss_ce: 0.010147
2022-01-12 01:09:52,965 iteration 6235 : loss : 0.017823, loss_ce: 0.005469
2022-01-12 01:09:54,581 iteration 6236 : loss : 0.026356, loss_ce: 0.010188
2022-01-12 01:09:56,162 iteration 6237 : loss : 0.028015, loss_ce: 0.010400
2022-01-12 01:09:57,765 iteration 6238 : loss : 0.016711, loss_ce: 0.006747
2022-01-12 01:09:59,395 iteration 6239 : loss : 0.021444, loss_ce: 0.009486
 92%|██████████████████████████▌  | 367/400 [2:57:30<15:49, 28.79s/it]2022-01-12 01:10:00,954 iteration 6240 : loss : 0.018593, loss_ce: 0.006208
2022-01-12 01:10:02,473 iteration 6241 : loss : 0.018339, loss_ce: 0.007919
2022-01-12 01:10:04,065 iteration 6242 : loss : 0.014456, loss_ce: 0.006280
2022-01-12 01:10:05,632 iteration 6243 : loss : 0.019988, loss_ce: 0.008942
2022-01-12 01:10:07,206 iteration 6244 : loss : 0.030487, loss_ce: 0.010242
2022-01-12 01:10:08,794 iteration 6245 : loss : 0.013390, loss_ce: 0.005923
2022-01-12 01:10:10,347 iteration 6246 : loss : 0.015401, loss_ce: 0.006260
2022-01-12 01:10:11,887 iteration 6247 : loss : 0.013873, loss_ce: 0.004741
2022-01-12 01:10:13,487 iteration 6248 : loss : 0.026773, loss_ce: 0.006745
2022-01-12 01:10:15,095 iteration 6249 : loss : 0.034208, loss_ce: 0.011814
2022-01-12 01:10:16,573 iteration 6250 : loss : 0.015173, loss_ce: 0.003721
2022-01-12 01:10:18,183 iteration 6251 : loss : 0.038869, loss_ce: 0.025533
2022-01-12 01:10:19,684 iteration 6252 : loss : 0.013211, loss_ce: 0.004579
2022-01-12 01:10:21,333 iteration 6253 : loss : 0.017603, loss_ce: 0.007346
2022-01-12 01:10:22,820 iteration 6254 : loss : 0.011174, loss_ce: 0.005023
2022-01-12 01:10:24,512 iteration 6255 : loss : 0.032181, loss_ce: 0.011927
2022-01-12 01:10:26,048 iteration 6256 : loss : 0.021920, loss_ce: 0.009151
 92%|██████████████████████████▋  | 368/400 [2:57:56<15:00, 28.15s/it]2022-01-12 01:10:27,697 iteration 6257 : loss : 0.024193, loss_ce: 0.010670
2022-01-12 01:10:29,247 iteration 6258 : loss : 0.013296, loss_ce: 0.006766
2022-01-12 01:10:30,828 iteration 6259 : loss : 0.018805, loss_ce: 0.008651
2022-01-12 01:10:32,379 iteration 6260 : loss : 0.018596, loss_ce: 0.005194
2022-01-12 01:10:33,910 iteration 6261 : loss : 0.018422, loss_ce: 0.008487
2022-01-12 01:10:35,499 iteration 6262 : loss : 0.018932, loss_ce: 0.008051
2022-01-12 01:10:37,100 iteration 6263 : loss : 0.021181, loss_ce: 0.006145
2022-01-12 01:10:38,663 iteration 6264 : loss : 0.014631, loss_ce: 0.007069
2022-01-12 01:10:40,195 iteration 6265 : loss : 0.019664, loss_ce: 0.008893
2022-01-12 01:10:41,759 iteration 6266 : loss : 0.015002, loss_ce: 0.006354
2022-01-12 01:10:43,327 iteration 6267 : loss : 0.034282, loss_ce: 0.013302
2022-01-12 01:10:44,885 iteration 6268 : loss : 0.016903, loss_ce: 0.006861
2022-01-12 01:10:46,428 iteration 6269 : loss : 0.021535, loss_ce: 0.007120
2022-01-12 01:10:48,019 iteration 6270 : loss : 0.027211, loss_ce: 0.008818
2022-01-12 01:10:49,577 iteration 6271 : loss : 0.018211, loss_ce: 0.004401
2022-01-12 01:10:51,132 iteration 6272 : loss : 0.011908, loss_ce: 0.003448
2022-01-12 01:10:52,630 iteration 6273 : loss : 0.012750, loss_ce: 0.005325
 92%|██████████████████████████▊  | 369/400 [2:58:23<14:17, 27.68s/it]2022-01-12 01:10:54,216 iteration 6274 : loss : 0.014345, loss_ce: 0.004630
2022-01-12 01:10:55,780 iteration 6275 : loss : 0.019774, loss_ce: 0.008408
2022-01-12 01:10:57,325 iteration 6276 : loss : 0.018109, loss_ce: 0.006447
2022-01-12 01:10:58,940 iteration 6277 : loss : 0.026340, loss_ce: 0.008761
2022-01-12 01:11:00,572 iteration 6278 : loss : 0.026725, loss_ce: 0.011829
2022-01-12 01:11:02,251 iteration 6279 : loss : 0.032441, loss_ce: 0.009432
2022-01-12 01:11:03,828 iteration 6280 : loss : 0.018444, loss_ce: 0.008960
2022-01-12 01:11:05,386 iteration 6281 : loss : 0.014088, loss_ce: 0.005050
2022-01-12 01:11:06,924 iteration 6282 : loss : 0.017367, loss_ce: 0.008149
2022-01-12 01:11:08,538 iteration 6283 : loss : 0.021046, loss_ce: 0.006607
2022-01-12 01:11:10,170 iteration 6284 : loss : 0.018485, loss_ce: 0.009442
2022-01-12 01:11:11,742 iteration 6285 : loss : 0.028039, loss_ce: 0.009949
2022-01-12 01:11:13,307 iteration 6286 : loss : 0.017678, loss_ce: 0.010515
2022-01-12 01:11:14,916 iteration 6287 : loss : 0.015181, loss_ce: 0.005816
2022-01-12 01:11:16,551 iteration 6288 : loss : 0.026353, loss_ce: 0.009378
2022-01-12 01:11:18,101 iteration 6289 : loss : 0.015717, loss_ce: 0.005418
2022-01-12 01:11:18,101 Training Data Eval:
2022-01-12 01:11:26,132   Average segmentation loss on training set: 0.0095
2022-01-12 01:11:26,132 Validation Data Eval:
2022-01-12 01:11:28,904   Average segmentation loss on validation set: 0.0736
2022-01-12 01:11:30,448 iteration 6290 : loss : 0.013839, loss_ce: 0.005046
 92%|██████████████████████████▊  | 370/400 [2:59:01<15:21, 30.72s/it]2022-01-12 01:11:32,057 iteration 6291 : loss : 0.016254, loss_ce: 0.004245
2022-01-12 01:11:33,730 iteration 6292 : loss : 0.018492, loss_ce: 0.008514
2022-01-12 01:11:35,293 iteration 6293 : loss : 0.021576, loss_ce: 0.007559
2022-01-12 01:11:36,887 iteration 6294 : loss : 0.015806, loss_ce: 0.005812
2022-01-12 01:11:38,491 iteration 6295 : loss : 0.032474, loss_ce: 0.013754
2022-01-12 01:11:40,047 iteration 6296 : loss : 0.013063, loss_ce: 0.005394
2022-01-12 01:11:41,620 iteration 6297 : loss : 0.017281, loss_ce: 0.004550
2022-01-12 01:11:43,168 iteration 6298 : loss : 0.015843, loss_ce: 0.006085
2022-01-12 01:11:44,784 iteration 6299 : loss : 0.018808, loss_ce: 0.007141
2022-01-12 01:11:46,355 iteration 6300 : loss : 0.020485, loss_ce: 0.008190
2022-01-12 01:11:47,889 iteration 6301 : loss : 0.018525, loss_ce: 0.006809
2022-01-12 01:11:49,415 iteration 6302 : loss : 0.014958, loss_ce: 0.006184
2022-01-12 01:11:50,994 iteration 6303 : loss : 0.017844, loss_ce: 0.007784
2022-01-12 01:11:52,528 iteration 6304 : loss : 0.012510, loss_ce: 0.005037
2022-01-12 01:11:54,032 iteration 6305 : loss : 0.013712, loss_ce: 0.006338
2022-01-12 01:11:55,544 iteration 6306 : loss : 0.018680, loss_ce: 0.006604
2022-01-12 01:11:57,184 iteration 6307 : loss : 0.017894, loss_ce: 0.006120
 93%|██████████████████████████▉  | 371/400 [2:59:27<14:16, 29.53s/it]2022-01-12 01:11:58,763 iteration 6308 : loss : 0.013026, loss_ce: 0.005379
2022-01-12 01:12:00,369 iteration 6309 : loss : 0.022660, loss_ce: 0.009638
2022-01-12 01:12:01,966 iteration 6310 : loss : 0.014576, loss_ce: 0.006104
2022-01-12 01:12:03,504 iteration 6311 : loss : 0.030677, loss_ce: 0.007328
2022-01-12 01:12:05,118 iteration 6312 : loss : 0.025487, loss_ce: 0.011828
2022-01-12 01:12:06,680 iteration 6313 : loss : 0.011814, loss_ce: 0.003151
2022-01-12 01:12:08,299 iteration 6314 : loss : 0.031271, loss_ce: 0.010803
2022-01-12 01:12:09,939 iteration 6315 : loss : 0.020457, loss_ce: 0.008748
2022-01-12 01:12:11,500 iteration 6316 : loss : 0.016505, loss_ce: 0.008304
2022-01-12 01:12:13,095 iteration 6317 : loss : 0.013550, loss_ce: 0.005289
2022-01-12 01:12:14,729 iteration 6318 : loss : 0.017577, loss_ce: 0.007823
2022-01-12 01:12:16,394 iteration 6319 : loss : 0.018377, loss_ce: 0.005862
2022-01-12 01:12:18,033 iteration 6320 : loss : 0.022069, loss_ce: 0.010144
2022-01-12 01:12:19,614 iteration 6321 : loss : 0.018196, loss_ce: 0.008296
2022-01-12 01:12:21,244 iteration 6322 : loss : 0.015455, loss_ce: 0.006335
2022-01-12 01:12:22,693 iteration 6323 : loss : 0.012352, loss_ce: 0.005515
2022-01-12 01:12:24,272 iteration 6324 : loss : 0.018957, loss_ce: 0.005848
 93%|██████████████████████████▉  | 372/400 [2:59:55<13:26, 28.79s/it]2022-01-12 01:12:25,820 iteration 6325 : loss : 0.017974, loss_ce: 0.006075
2022-01-12 01:12:27,318 iteration 6326 : loss : 0.012355, loss_ce: 0.005743
2022-01-12 01:12:28,816 iteration 6327 : loss : 0.011711, loss_ce: 0.004425
2022-01-12 01:12:30,463 iteration 6328 : loss : 0.022459, loss_ce: 0.010718
2022-01-12 01:12:32,144 iteration 6329 : loss : 0.020504, loss_ce: 0.008333
2022-01-12 01:12:33,749 iteration 6330 : loss : 0.012818, loss_ce: 0.005139
2022-01-12 01:12:35,369 iteration 6331 : loss : 0.015896, loss_ce: 0.007246
2022-01-12 01:12:36,922 iteration 6332 : loss : 0.013538, loss_ce: 0.004970
2022-01-12 01:12:38,480 iteration 6333 : loss : 0.014474, loss_ce: 0.006036
2022-01-12 01:12:40,083 iteration 6334 : loss : 0.012689, loss_ce: 0.005220
2022-01-12 01:12:41,796 iteration 6335 : loss : 0.026524, loss_ce: 0.011520
2022-01-12 01:12:43,372 iteration 6336 : loss : 0.012618, loss_ce: 0.004519
2022-01-12 01:12:44,866 iteration 6337 : loss : 0.014798, loss_ce: 0.006191
2022-01-12 01:12:46,505 iteration 6338 : loss : 0.017044, loss_ce: 0.005277
2022-01-12 01:12:48,052 iteration 6339 : loss : 0.012702, loss_ce: 0.004422
2022-01-12 01:12:49,633 iteration 6340 : loss : 0.016988, loss_ce: 0.008444
2022-01-12 01:12:51,223 iteration 6341 : loss : 0.020740, loss_ce: 0.005861
 93%|███████████████████████████  | 373/400 [3:00:22<12:42, 28.24s/it]2022-01-12 01:12:52,919 iteration 6342 : loss : 0.027703, loss_ce: 0.011539
2022-01-12 01:12:54,461 iteration 6343 : loss : 0.019745, loss_ce: 0.007078
2022-01-12 01:12:56,035 iteration 6344 : loss : 0.016295, loss_ce: 0.007236
2022-01-12 01:12:57,621 iteration 6345 : loss : 0.023685, loss_ce: 0.012217
2022-01-12 01:12:59,153 iteration 6346 : loss : 0.017350, loss_ce: 0.005530
2022-01-12 01:13:00,719 iteration 6347 : loss : 0.018138, loss_ce: 0.007201
2022-01-12 01:13:02,375 iteration 6348 : loss : 0.043165, loss_ce: 0.012965
2022-01-12 01:13:03,894 iteration 6349 : loss : 0.012070, loss_ce: 0.005202
2022-01-12 01:13:05,441 iteration 6350 : loss : 0.015896, loss_ce: 0.004799
2022-01-12 01:13:06,968 iteration 6351 : loss : 0.015570, loss_ce: 0.005683
2022-01-12 01:13:08,589 iteration 6352 : loss : 0.015279, loss_ce: 0.006297
2022-01-12 01:13:10,213 iteration 6353 : loss : 0.015079, loss_ce: 0.006268
2022-01-12 01:13:11,858 iteration 6354 : loss : 0.015207, loss_ce: 0.005309
2022-01-12 01:13:13,380 iteration 6355 : loss : 0.012822, loss_ce: 0.005908
2022-01-12 01:13:14,970 iteration 6356 : loss : 0.016379, loss_ce: 0.007612
2022-01-12 01:13:16,512 iteration 6357 : loss : 0.018791, loss_ce: 0.006568
2022-01-12 01:13:18,146 iteration 6358 : loss : 0.024363, loss_ce: 0.009645
 94%|███████████████████████████  | 374/400 [3:00:48<12:04, 27.85s/it]2022-01-12 01:13:19,756 iteration 6359 : loss : 0.022358, loss_ce: 0.009489
2022-01-12 01:13:21,267 iteration 6360 : loss : 0.017884, loss_ce: 0.005231
2022-01-12 01:13:22,857 iteration 6361 : loss : 0.020206, loss_ce: 0.004938
2022-01-12 01:13:24,458 iteration 6362 : loss : 0.019202, loss_ce: 0.009678
2022-01-12 01:13:26,102 iteration 6363 : loss : 0.021536, loss_ce: 0.009668
2022-01-12 01:13:27,616 iteration 6364 : loss : 0.011282, loss_ce: 0.004607
2022-01-12 01:13:29,126 iteration 6365 : loss : 0.015725, loss_ce: 0.005938
2022-01-12 01:13:30,730 iteration 6366 : loss : 0.016216, loss_ce: 0.006342
2022-01-12 01:13:32,295 iteration 6367 : loss : 0.017106, loss_ce: 0.008228
2022-01-12 01:13:33,836 iteration 6368 : loss : 0.014946, loss_ce: 0.005596
2022-01-12 01:13:35,415 iteration 6369 : loss : 0.017266, loss_ce: 0.007887
2022-01-12 01:13:37,018 iteration 6370 : loss : 0.027256, loss_ce: 0.007786
2022-01-12 01:13:38,625 iteration 6371 : loss : 0.031358, loss_ce: 0.010152
2022-01-12 01:13:40,187 iteration 6372 : loss : 0.015136, loss_ce: 0.007879
2022-01-12 01:13:41,642 iteration 6373 : loss : 0.010252, loss_ce: 0.002841
2022-01-12 01:13:43,169 iteration 6374 : loss : 0.014864, loss_ce: 0.006233
2022-01-12 01:13:43,169 Training Data Eval:
2022-01-12 01:13:51,197   Average segmentation loss on training set: 0.0092
2022-01-12 01:13:51,197 Validation Data Eval:
2022-01-12 01:13:53,957   Average segmentation loss on validation set: 0.0665
2022-01-12 01:13:55,525 iteration 6375 : loss : 0.017984, loss_ce: 0.006831
 94%|███████████████████████████▏ | 375/400 [3:01:26<12:47, 30.71s/it]2022-01-12 01:13:57,317 iteration 6376 : loss : 0.022418, loss_ce: 0.009211
2022-01-12 01:13:58,895 iteration 6377 : loss : 0.013484, loss_ce: 0.004719
2022-01-12 01:14:00,430 iteration 6378 : loss : 0.017914, loss_ce: 0.006461
2022-01-12 01:14:02,039 iteration 6379 : loss : 0.017603, loss_ce: 0.003863
2022-01-12 01:14:03,538 iteration 6380 : loss : 0.013733, loss_ce: 0.006591
2022-01-12 01:14:05,043 iteration 6381 : loss : 0.016422, loss_ce: 0.006140
2022-01-12 01:14:06,628 iteration 6382 : loss : 0.020957, loss_ce: 0.009883
2022-01-12 01:14:08,228 iteration 6383 : loss : 0.015526, loss_ce: 0.006486
2022-01-12 01:14:09,785 iteration 6384 : loss : 0.013746, loss_ce: 0.005746
2022-01-12 01:14:11,430 iteration 6385 : loss : 0.025481, loss_ce: 0.010408
2022-01-12 01:14:13,000 iteration 6386 : loss : 0.015533, loss_ce: 0.006019
2022-01-12 01:14:14,679 iteration 6387 : loss : 0.034498, loss_ce: 0.016675
2022-01-12 01:14:16,213 iteration 6388 : loss : 0.011097, loss_ce: 0.003863
2022-01-12 01:14:17,692 iteration 6389 : loss : 0.013298, loss_ce: 0.004275
2022-01-12 01:14:19,248 iteration 6390 : loss : 0.012217, loss_ce: 0.004643
2022-01-12 01:14:20,909 iteration 6391 : loss : 0.022767, loss_ce: 0.005280
2022-01-12 01:14:22,409 iteration 6392 : loss : 0.010643, loss_ce: 0.004662
 94%|███████████████████████████▎ | 376/400 [3:01:53<11:49, 29.56s/it]2022-01-12 01:14:24,013 iteration 6393 : loss : 0.012968, loss_ce: 0.004446
2022-01-12 01:14:25,597 iteration 6394 : loss : 0.017122, loss_ce: 0.005764
2022-01-12 01:14:27,135 iteration 6395 : loss : 0.013333, loss_ce: 0.006434
2022-01-12 01:14:28,786 iteration 6396 : loss : 0.019179, loss_ce: 0.008363
2022-01-12 01:14:30,315 iteration 6397 : loss : 0.017284, loss_ce: 0.006872
2022-01-12 01:14:31,819 iteration 6398 : loss : 0.011788, loss_ce: 0.003736
2022-01-12 01:14:33,384 iteration 6399 : loss : 0.014267, loss_ce: 0.004483
2022-01-12 01:14:34,954 iteration 6400 : loss : 0.015016, loss_ce: 0.007418
2022-01-12 01:14:36,465 iteration 6401 : loss : 0.012621, loss_ce: 0.004965
2022-01-12 01:14:38,033 iteration 6402 : loss : 0.024024, loss_ce: 0.008634
2022-01-12 01:14:39,715 iteration 6403 : loss : 0.016396, loss_ce: 0.006229
2022-01-12 01:14:41,270 iteration 6404 : loss : 0.018856, loss_ce: 0.005282
2022-01-12 01:14:42,815 iteration 6405 : loss : 0.017906, loss_ce: 0.006569
2022-01-12 01:14:44,532 iteration 6406 : loss : 0.029554, loss_ce: 0.013631
2022-01-12 01:14:46,036 iteration 6407 : loss : 0.016166, loss_ce: 0.004590
2022-01-12 01:14:47,596 iteration 6408 : loss : 0.015052, loss_ce: 0.003981
2022-01-12 01:14:49,129 iteration 6409 : loss : 0.015897, loss_ce: 0.006330
 94%|███████████████████████████▎ | 377/400 [3:02:19<11:00, 28.71s/it]2022-01-12 01:14:50,856 iteration 6410 : loss : 0.025656, loss_ce: 0.009739
2022-01-12 01:14:52,489 iteration 6411 : loss : 0.020238, loss_ce: 0.008256
2022-01-12 01:14:54,054 iteration 6412 : loss : 0.018913, loss_ce: 0.008474
2022-01-12 01:14:55,620 iteration 6413 : loss : 0.011248, loss_ce: 0.003385
2022-01-12 01:14:57,227 iteration 6414 : loss : 0.018989, loss_ce: 0.006920
2022-01-12 01:14:58,766 iteration 6415 : loss : 0.015083, loss_ce: 0.005058
2022-01-12 01:15:00,318 iteration 6416 : loss : 0.020281, loss_ce: 0.006437
2022-01-12 01:15:01,918 iteration 6417 : loss : 0.018601, loss_ce: 0.009870
2022-01-12 01:15:03,512 iteration 6418 : loss : 0.013380, loss_ce: 0.004899
2022-01-12 01:15:05,078 iteration 6419 : loss : 0.014126, loss_ce: 0.004817
2022-01-12 01:15:06,690 iteration 6420 : loss : 0.022921, loss_ce: 0.007056
2022-01-12 01:15:08,255 iteration 6421 : loss : 0.013878, loss_ce: 0.005735
2022-01-12 01:15:09,754 iteration 6422 : loss : 0.016946, loss_ce: 0.006886
2022-01-12 01:15:11,359 iteration 6423 : loss : 0.017892, loss_ce: 0.006644
2022-01-12 01:15:12,972 iteration 6424 : loss : 0.017979, loss_ce: 0.009192
2022-01-12 01:15:14,462 iteration 6425 : loss : 0.018613, loss_ce: 0.006488
2022-01-12 01:15:16,060 iteration 6426 : loss : 0.016943, loss_ce: 0.007365
 94%|███████████████████████████▍ | 378/400 [3:02:46<10:19, 28.18s/it]2022-01-12 01:15:17,673 iteration 6427 : loss : 0.017496, loss_ce: 0.008428
2022-01-12 01:15:19,165 iteration 6428 : loss : 0.012236, loss_ce: 0.005794
2022-01-12 01:15:20,761 iteration 6429 : loss : 0.019664, loss_ce: 0.006938
2022-01-12 01:15:22,268 iteration 6430 : loss : 0.013199, loss_ce: 0.003947
2022-01-12 01:15:23,838 iteration 6431 : loss : 0.019496, loss_ce: 0.007265
2022-01-12 01:15:25,409 iteration 6432 : loss : 0.017765, loss_ce: 0.007348
2022-01-12 01:15:27,041 iteration 6433 : loss : 0.041349, loss_ce: 0.015326
2022-01-12 01:15:28,526 iteration 6434 : loss : 0.010280, loss_ce: 0.003788
2022-01-12 01:15:30,100 iteration 6435 : loss : 0.024487, loss_ce: 0.011333
2022-01-12 01:15:31,656 iteration 6436 : loss : 0.016402, loss_ce: 0.006742
2022-01-12 01:15:33,248 iteration 6437 : loss : 0.024873, loss_ce: 0.005978
2022-01-12 01:15:34,840 iteration 6438 : loss : 0.018293, loss_ce: 0.007687
2022-01-12 01:15:36,436 iteration 6439 : loss : 0.018116, loss_ce: 0.008064
2022-01-12 01:15:38,079 iteration 6440 : loss : 0.023011, loss_ce: 0.008556
2022-01-12 01:15:39,670 iteration 6441 : loss : 0.014526, loss_ce: 0.004035
2022-01-12 01:15:41,228 iteration 6442 : loss : 0.016083, loss_ce: 0.006972
2022-01-12 01:15:42,836 iteration 6443 : loss : 0.024136, loss_ce: 0.009471
 95%|███████████████████████████▍ | 379/400 [3:03:13<09:42, 27.75s/it]2022-01-12 01:15:44,494 iteration 6444 : loss : 0.017919, loss_ce: 0.007859
2022-01-12 01:15:46,133 iteration 6445 : loss : 0.019157, loss_ce: 0.007434
2022-01-12 01:15:47,701 iteration 6446 : loss : 0.025199, loss_ce: 0.007669
2022-01-12 01:15:49,272 iteration 6447 : loss : 0.019441, loss_ce: 0.011658
2022-01-12 01:15:50,845 iteration 6448 : loss : 0.020893, loss_ce: 0.012078
2022-01-12 01:15:52,466 iteration 6449 : loss : 0.014305, loss_ce: 0.004719
2022-01-12 01:15:53,991 iteration 6450 : loss : 0.012783, loss_ce: 0.005521
2022-01-12 01:15:55,503 iteration 6451 : loss : 0.014812, loss_ce: 0.005333
2022-01-12 01:15:57,113 iteration 6452 : loss : 0.018538, loss_ce: 0.009194
2022-01-12 01:15:58,702 iteration 6453 : loss : 0.017539, loss_ce: 0.007252
2022-01-12 01:16:00,253 iteration 6454 : loss : 0.016397, loss_ce: 0.006017
2022-01-12 01:16:01,799 iteration 6455 : loss : 0.013314, loss_ce: 0.004150
2022-01-12 01:16:03,454 iteration 6456 : loss : 0.026903, loss_ce: 0.009920
2022-01-12 01:16:05,018 iteration 6457 : loss : 0.017932, loss_ce: 0.005068
2022-01-12 01:16:06,558 iteration 6458 : loss : 0.019294, loss_ce: 0.008619
2022-01-12 01:16:08,072 iteration 6459 : loss : 0.014221, loss_ce: 0.004661
2022-01-12 01:16:08,072 Training Data Eval:
2022-01-12 01:16:16,104   Average segmentation loss on training set: 0.0090
2022-01-12 01:16:16,104 Validation Data Eval:
2022-01-12 01:16:18,863   Average segmentation loss on validation set: 0.0812
2022-01-12 01:16:20,473 iteration 6460 : loss : 0.018408, loss_ce: 0.005201
 95%|███████████████████████████▌ | 380/400 [3:03:51<10:14, 30.72s/it]2022-01-12 01:16:22,098 iteration 6461 : loss : 0.025527, loss_ce: 0.006818
2022-01-12 01:16:23,690 iteration 6462 : loss : 0.015500, loss_ce: 0.005224
2022-01-12 01:16:25,211 iteration 6463 : loss : 0.010721, loss_ce: 0.003431
2022-01-12 01:16:26,736 iteration 6464 : loss : 0.014577, loss_ce: 0.005126
2022-01-12 01:16:28,289 iteration 6465 : loss : 0.021290, loss_ce: 0.006414
2022-01-12 01:16:29,889 iteration 6466 : loss : 0.016100, loss_ce: 0.006025
2022-01-12 01:16:31,516 iteration 6467 : loss : 0.015534, loss_ce: 0.006885
2022-01-12 01:16:33,160 iteration 6468 : loss : 0.022587, loss_ce: 0.010619
2022-01-12 01:16:34,730 iteration 6469 : loss : 0.018041, loss_ce: 0.008847
2022-01-12 01:16:36,273 iteration 6470 : loss : 0.019562, loss_ce: 0.005832
2022-01-12 01:16:37,814 iteration 6471 : loss : 0.017698, loss_ce: 0.006074
2022-01-12 01:16:39,365 iteration 6472 : loss : 0.018321, loss_ce: 0.005003
2022-01-12 01:16:40,877 iteration 6473 : loss : 0.014720, loss_ce: 0.007236
2022-01-12 01:16:42,501 iteration 6474 : loss : 0.020143, loss_ce: 0.010969
2022-01-12 01:16:44,004 iteration 6475 : loss : 0.013461, loss_ce: 0.004787
2022-01-12 01:16:45,539 iteration 6476 : loss : 0.013576, loss_ce: 0.005333
2022-01-12 01:16:47,042 iteration 6477 : loss : 0.017471, loss_ce: 0.006183
 95%|███████████████████████████▌ | 381/400 [3:04:17<09:20, 29.48s/it]2022-01-12 01:16:48,683 iteration 6478 : loss : 0.014972, loss_ce: 0.006251
2022-01-12 01:16:50,387 iteration 6479 : loss : 0.024571, loss_ce: 0.015353
2022-01-12 01:16:51,975 iteration 6480 : loss : 0.028553, loss_ce: 0.006995
2022-01-12 01:16:53,557 iteration 6481 : loss : 0.019975, loss_ce: 0.008424
2022-01-12 01:16:55,148 iteration 6482 : loss : 0.020709, loss_ce: 0.008267
2022-01-12 01:16:56,699 iteration 6483 : loss : 0.017808, loss_ce: 0.006897
2022-01-12 01:16:58,265 iteration 6484 : loss : 0.015769, loss_ce: 0.005039
2022-01-12 01:16:59,848 iteration 6485 : loss : 0.021594, loss_ce: 0.009619
2022-01-12 01:17:01,482 iteration 6486 : loss : 0.014588, loss_ce: 0.004980
2022-01-12 01:17:03,067 iteration 6487 : loss : 0.016035, loss_ce: 0.005294
2022-01-12 01:17:04,550 iteration 6488 : loss : 0.011419, loss_ce: 0.004887
2022-01-12 01:17:06,169 iteration 6489 : loss : 0.027634, loss_ce: 0.005699
2022-01-12 01:17:07,649 iteration 6490 : loss : 0.020920, loss_ce: 0.008399
2022-01-12 01:17:09,249 iteration 6491 : loss : 0.016506, loss_ce: 0.005108
2022-01-12 01:17:10,799 iteration 6492 : loss : 0.013061, loss_ce: 0.004831
2022-01-12 01:17:12,399 iteration 6493 : loss : 0.021465, loss_ce: 0.005586
2022-01-12 01:17:13,960 iteration 6494 : loss : 0.016387, loss_ce: 0.005540
 96%|███████████████████████████▋ | 382/400 [3:04:44<08:36, 28.71s/it]2022-01-12 01:17:15,592 iteration 6495 : loss : 0.011680, loss_ce: 0.003458
2022-01-12 01:17:17,132 iteration 6496 : loss : 0.024681, loss_ce: 0.010301
2022-01-12 01:17:18,732 iteration 6497 : loss : 0.013477, loss_ce: 0.004598
2022-01-12 01:17:20,316 iteration 6498 : loss : 0.021578, loss_ce: 0.011229
2022-01-12 01:17:21,975 iteration 6499 : loss : 0.025620, loss_ce: 0.009627
2022-01-12 01:17:23,582 iteration 6500 : loss : 0.021014, loss_ce: 0.009231
2022-01-12 01:17:25,199 iteration 6501 : loss : 0.015083, loss_ce: 0.004069
2022-01-12 01:17:26,812 iteration 6502 : loss : 0.023140, loss_ce: 0.008765
2022-01-12 01:17:28,478 iteration 6503 : loss : 0.021762, loss_ce: 0.008188
2022-01-12 01:17:29,996 iteration 6504 : loss : 0.014164, loss_ce: 0.007558
2022-01-12 01:17:31,530 iteration 6505 : loss : 0.016882, loss_ce: 0.004707
2022-01-12 01:17:33,085 iteration 6506 : loss : 0.015233, loss_ce: 0.004800
2022-01-12 01:17:34,646 iteration 6507 : loss : 0.014506, loss_ce: 0.005646
2022-01-12 01:17:36,225 iteration 6508 : loss : 0.014878, loss_ce: 0.004589
2022-01-12 01:17:37,795 iteration 6509 : loss : 0.012396, loss_ce: 0.005219
2022-01-12 01:17:39,337 iteration 6510 : loss : 0.021481, loss_ce: 0.007276
2022-01-12 01:17:40,901 iteration 6511 : loss : 0.013899, loss_ce: 0.005807
 96%|███████████████████████████▊ | 383/400 [3:05:11<07:59, 28.18s/it]2022-01-12 01:17:42,501 iteration 6512 : loss : 0.017506, loss_ce: 0.007424
2022-01-12 01:17:44,006 iteration 6513 : loss : 0.013539, loss_ce: 0.005208
2022-01-12 01:17:45,556 iteration 6514 : loss : 0.014071, loss_ce: 0.003559
2022-01-12 01:17:47,123 iteration 6515 : loss : 0.016410, loss_ce: 0.004946
2022-01-12 01:17:48,693 iteration 6516 : loss : 0.015634, loss_ce: 0.004750
2022-01-12 01:17:50,329 iteration 6517 : loss : 0.017885, loss_ce: 0.008258
2022-01-12 01:17:51,832 iteration 6518 : loss : 0.018543, loss_ce: 0.007593
2022-01-12 01:17:53,472 iteration 6519 : loss : 0.016880, loss_ce: 0.005895
2022-01-12 01:17:54,970 iteration 6520 : loss : 0.012458, loss_ce: 0.005298
2022-01-12 01:17:56,577 iteration 6521 : loss : 0.014305, loss_ce: 0.006280
2022-01-12 01:17:58,129 iteration 6522 : loss : 0.014356, loss_ce: 0.006469
2022-01-12 01:17:59,644 iteration 6523 : loss : 0.017527, loss_ce: 0.005844
2022-01-12 01:18:01,213 iteration 6524 : loss : 0.015645, loss_ce: 0.005416
2022-01-12 01:18:02,770 iteration 6525 : loss : 0.013657, loss_ce: 0.004456
2022-01-12 01:18:04,360 iteration 6526 : loss : 0.016773, loss_ce: 0.006747
2022-01-12 01:18:05,977 iteration 6527 : loss : 0.016168, loss_ce: 0.005057
2022-01-12 01:18:07,560 iteration 6528 : loss : 0.017851, loss_ce: 0.006633
 96%|███████████████████████████▊ | 384/400 [3:05:38<07:23, 27.72s/it]2022-01-12 01:18:09,126 iteration 6529 : loss : 0.013920, loss_ce: 0.005009
2022-01-12 01:18:10,623 iteration 6530 : loss : 0.014733, loss_ce: 0.007445
2022-01-12 01:18:12,202 iteration 6531 : loss : 0.015422, loss_ce: 0.003319
2022-01-12 01:18:13,810 iteration 6532 : loss : 0.013874, loss_ce: 0.004153
2022-01-12 01:18:15,347 iteration 6533 : loss : 0.019671, loss_ce: 0.009801
2022-01-12 01:18:16,948 iteration 6534 : loss : 0.024979, loss_ce: 0.007495
2022-01-12 01:18:18,528 iteration 6535 : loss : 0.017591, loss_ce: 0.006978
2022-01-12 01:18:20,144 iteration 6536 : loss : 0.018853, loss_ce: 0.007290
2022-01-12 01:18:21,827 iteration 6537 : loss : 0.035500, loss_ce: 0.011033
2022-01-12 01:18:23,350 iteration 6538 : loss : 0.016165, loss_ce: 0.005641
2022-01-12 01:18:25,058 iteration 6539 : loss : 0.020107, loss_ce: 0.008786
2022-01-12 01:18:26,588 iteration 6540 : loss : 0.015071, loss_ce: 0.006404
2022-01-12 01:18:28,088 iteration 6541 : loss : 0.014611, loss_ce: 0.006544
2022-01-12 01:18:29,660 iteration 6542 : loss : 0.017665, loss_ce: 0.006586
2022-01-12 01:18:31,312 iteration 6543 : loss : 0.017494, loss_ce: 0.006986
2022-01-12 01:18:32,882 iteration 6544 : loss : 0.019607, loss_ce: 0.007549
2022-01-12 01:18:32,882 Training Data Eval:
2022-01-12 01:18:40,912   Average segmentation loss on training set: 0.0089
2022-01-12 01:18:40,912 Validation Data Eval:
2022-01-12 01:18:43,675   Average segmentation loss on validation set: 0.0748
2022-01-12 01:18:45,255 iteration 6545 : loss : 0.010952, loss_ce: 0.002610
 96%|███████████████████████████▉ | 385/400 [3:06:16<07:40, 30.71s/it]2022-01-12 01:18:46,941 iteration 6546 : loss : 0.020090, loss_ce: 0.007446
2022-01-12 01:18:48,462 iteration 6547 : loss : 0.016081, loss_ce: 0.003875
2022-01-12 01:18:50,056 iteration 6548 : loss : 0.023805, loss_ce: 0.005865
2022-01-12 01:18:51,722 iteration 6549 : loss : 0.025179, loss_ce: 0.008755
2022-01-12 01:18:53,298 iteration 6550 : loss : 0.015577, loss_ce: 0.007394
2022-01-12 01:18:54,937 iteration 6551 : loss : 0.019535, loss_ce: 0.008272
2022-01-12 01:18:56,610 iteration 6552 : loss : 0.024461, loss_ce: 0.010033
2022-01-12 01:18:58,176 iteration 6553 : loss : 0.018115, loss_ce: 0.006739
2022-01-12 01:18:59,885 iteration 6554 : loss : 0.029355, loss_ce: 0.009596
2022-01-12 01:19:01,422 iteration 6555 : loss : 0.019906, loss_ce: 0.007624
2022-01-12 01:19:03,017 iteration 6556 : loss : 0.021458, loss_ce: 0.008397
2022-01-12 01:19:04,516 iteration 6557 : loss : 0.018521, loss_ce: 0.008326
2022-01-12 01:19:06,139 iteration 6558 : loss : 0.022107, loss_ce: 0.008284
2022-01-12 01:19:07,641 iteration 6559 : loss : 0.010420, loss_ce: 0.003588
2022-01-12 01:19:09,164 iteration 6560 : loss : 0.011941, loss_ce: 0.005601
2022-01-12 01:19:10,753 iteration 6561 : loss : 0.017675, loss_ce: 0.006802
2022-01-12 01:19:12,320 iteration 6562 : loss : 0.017292, loss_ce: 0.007539
 96%|███████████████████████████▉ | 386/400 [3:06:43<06:54, 29.62s/it]2022-01-12 01:19:13,946 iteration 6563 : loss : 0.020576, loss_ce: 0.009738
2022-01-12 01:19:15,571 iteration 6564 : loss : 0.019146, loss_ce: 0.006410
2022-01-12 01:19:17,094 iteration 6565 : loss : 0.024050, loss_ce: 0.005760
2022-01-12 01:19:18,731 iteration 6566 : loss : 0.018323, loss_ce: 0.007006
2022-01-12 01:19:20,316 iteration 6567 : loss : 0.032633, loss_ce: 0.008047
2022-01-12 01:19:21,889 iteration 6568 : loss : 0.011653, loss_ce: 0.004212
2022-01-12 01:19:23,426 iteration 6569 : loss : 0.017001, loss_ce: 0.005321
2022-01-12 01:19:25,074 iteration 6570 : loss : 0.018328, loss_ce: 0.007720
2022-01-12 01:19:26,567 iteration 6571 : loss : 0.012312, loss_ce: 0.005424
2022-01-12 01:19:28,099 iteration 6572 : loss : 0.015015, loss_ce: 0.005136
2022-01-12 01:19:29,612 iteration 6573 : loss : 0.013921, loss_ce: 0.004731
2022-01-12 01:19:31,191 iteration 6574 : loss : 0.018107, loss_ce: 0.006965
2022-01-12 01:19:32,674 iteration 6575 : loss : 0.010536, loss_ce: 0.004330
2022-01-12 01:19:34,243 iteration 6576 : loss : 0.021311, loss_ce: 0.008358
2022-01-12 01:19:35,843 iteration 6577 : loss : 0.017445, loss_ce: 0.006102
2022-01-12 01:19:37,453 iteration 6578 : loss : 0.013676, loss_ce: 0.005984
2022-01-12 01:19:39,015 iteration 6579 : loss : 0.014924, loss_ce: 0.004463
 97%|████████████████████████████ | 387/400 [3:07:09<06:13, 28.74s/it]2022-01-12 01:19:40,670 iteration 6580 : loss : 0.019735, loss_ce: 0.007038
2022-01-12 01:19:42,284 iteration 6581 : loss : 0.025365, loss_ce: 0.007322
2022-01-12 01:19:43,910 iteration 6582 : loss : 0.029060, loss_ce: 0.015893
2022-01-12 01:19:45,474 iteration 6583 : loss : 0.012163, loss_ce: 0.005007
2022-01-12 01:19:47,065 iteration 6584 : loss : 0.022117, loss_ce: 0.007402
2022-01-12 01:19:48,593 iteration 6585 : loss : 0.013972, loss_ce: 0.004467
2022-01-12 01:19:50,175 iteration 6586 : loss : 0.017193, loss_ce: 0.006911
2022-01-12 01:19:51,681 iteration 6587 : loss : 0.018683, loss_ce: 0.006987
2022-01-12 01:19:53,303 iteration 6588 : loss : 0.019346, loss_ce: 0.006859
2022-01-12 01:19:54,869 iteration 6589 : loss : 0.019511, loss_ce: 0.006457
2022-01-12 01:19:56,424 iteration 6590 : loss : 0.016787, loss_ce: 0.007272
2022-01-12 01:19:57,960 iteration 6591 : loss : 0.014463, loss_ce: 0.006278
2022-01-12 01:19:59,535 iteration 6592 : loss : 0.014344, loss_ce: 0.004738
2022-01-12 01:20:01,141 iteration 6593 : loss : 0.021636, loss_ce: 0.010123
2022-01-12 01:20:02,694 iteration 6594 : loss : 0.013559, loss_ce: 0.005123
2022-01-12 01:20:04,207 iteration 6595 : loss : 0.014089, loss_ce: 0.004828
2022-01-12 01:20:05,735 iteration 6596 : loss : 0.009870, loss_ce: 0.003907
 97%|████████████████████████████▏| 388/400 [3:07:36<05:37, 28.14s/it]2022-01-12 01:20:07,402 iteration 6597 : loss : 0.022422, loss_ce: 0.012142
2022-01-12 01:20:09,008 iteration 6598 : loss : 0.019012, loss_ce: 0.005190
2022-01-12 01:20:10,518 iteration 6599 : loss : 0.016853, loss_ce: 0.006094
2022-01-12 01:20:12,122 iteration 6600 : loss : 0.023920, loss_ce: 0.009639
2022-01-12 01:20:13,633 iteration 6601 : loss : 0.018241, loss_ce: 0.007517
2022-01-12 01:20:15,105 iteration 6602 : loss : 0.009848, loss_ce: 0.003538
2022-01-12 01:20:16,695 iteration 6603 : loss : 0.040925, loss_ce: 0.012845
2022-01-12 01:20:18,349 iteration 6604 : loss : 0.021370, loss_ce: 0.008616
2022-01-12 01:20:19,945 iteration 6605 : loss : 0.020109, loss_ce: 0.008078
2022-01-12 01:20:21,533 iteration 6606 : loss : 0.015662, loss_ce: 0.008271
2022-01-12 01:20:23,075 iteration 6607 : loss : 0.016546, loss_ce: 0.005356
2022-01-12 01:20:24,674 iteration 6608 : loss : 0.018864, loss_ce: 0.005915
2022-01-12 01:20:26,307 iteration 6609 : loss : 0.014854, loss_ce: 0.006574
2022-01-12 01:20:27,859 iteration 6610 : loss : 0.022762, loss_ce: 0.008231
2022-01-12 01:20:29,342 iteration 6611 : loss : 0.012271, loss_ce: 0.005363
2022-01-12 01:20:30,922 iteration 6612 : loss : 0.016782, loss_ce: 0.006084
2022-01-12 01:20:32,537 iteration 6613 : loss : 0.028754, loss_ce: 0.007667
 97%|████████████████████████████▏| 389/400 [3:08:03<05:05, 27.74s/it]2022-01-12 01:20:34,043 iteration 6614 : loss : 0.011176, loss_ce: 0.004373
2022-01-12 01:20:35,642 iteration 6615 : loss : 0.019729, loss_ce: 0.008501
2022-01-12 01:20:37,245 iteration 6616 : loss : 0.016571, loss_ce: 0.007015
2022-01-12 01:20:38,774 iteration 6617 : loss : 0.011730, loss_ce: 0.002280
2022-01-12 01:20:40,336 iteration 6618 : loss : 0.017530, loss_ce: 0.006000
2022-01-12 01:20:41,939 iteration 6619 : loss : 0.017121, loss_ce: 0.005393
2022-01-12 01:20:43,503 iteration 6620 : loss : 0.017782, loss_ce: 0.005755
2022-01-12 01:20:45,013 iteration 6621 : loss : 0.013839, loss_ce: 0.006093
2022-01-12 01:20:46,478 iteration 6622 : loss : 0.010674, loss_ce: 0.003605
2022-01-12 01:20:48,052 iteration 6623 : loss : 0.021208, loss_ce: 0.010189
2022-01-12 01:20:49,520 iteration 6624 : loss : 0.013018, loss_ce: 0.004986
2022-01-12 01:20:50,993 iteration 6625 : loss : 0.010626, loss_ce: 0.004279
2022-01-12 01:20:52,559 iteration 6626 : loss : 0.017789, loss_ce: 0.006780
2022-01-12 01:20:54,074 iteration 6627 : loss : 0.011685, loss_ce: 0.004308
2022-01-12 01:20:55,671 iteration 6628 : loss : 0.018251, loss_ce: 0.006432
2022-01-12 01:20:57,194 iteration 6629 : loss : 0.013440, loss_ce: 0.005892
2022-01-12 01:20:57,194 Training Data Eval:
2022-01-12 01:21:05,225   Average segmentation loss on training set: 0.0087
2022-01-12 01:21:05,226 Validation Data Eval:
2022-01-12 01:21:07,988   Average segmentation loss on validation set: 0.0711
2022-01-12 01:21:09,572 iteration 6630 : loss : 0.032200, loss_ce: 0.017559
 98%|████████████████████████████▎| 390/400 [3:08:40<05:05, 30.52s/it]2022-01-12 01:21:11,095 iteration 6631 : loss : 0.011535, loss_ce: 0.004617
2022-01-12 01:21:12,694 iteration 6632 : loss : 0.027648, loss_ce: 0.006600
2022-01-12 01:21:14,251 iteration 6633 : loss : 0.014285, loss_ce: 0.006077
2022-01-12 01:21:15,779 iteration 6634 : loss : 0.014944, loss_ce: 0.005591
2022-01-12 01:21:17,388 iteration 6635 : loss : 0.014435, loss_ce: 0.005874
2022-01-12 01:21:18,925 iteration 6636 : loss : 0.014258, loss_ce: 0.007818
2022-01-12 01:21:20,520 iteration 6637 : loss : 0.020219, loss_ce: 0.006569
2022-01-12 01:21:22,110 iteration 6638 : loss : 0.011178, loss_ce: 0.003715
2022-01-12 01:21:23,698 iteration 6639 : loss : 0.018324, loss_ce: 0.006233
2022-01-12 01:21:25,288 iteration 6640 : loss : 0.026911, loss_ce: 0.012842
2022-01-12 01:21:26,843 iteration 6641 : loss : 0.012808, loss_ce: 0.005250
2022-01-12 01:21:28,400 iteration 6642 : loss : 0.015333, loss_ce: 0.004755
2022-01-12 01:21:30,031 iteration 6643 : loss : 0.019770, loss_ce: 0.009541
2022-01-12 01:21:31,676 iteration 6644 : loss : 0.019770, loss_ce: 0.009300
2022-01-12 01:21:33,237 iteration 6645 : loss : 0.020015, loss_ce: 0.008468
2022-01-12 01:21:34,795 iteration 6646 : loss : 0.016583, loss_ce: 0.007820
2022-01-12 01:21:36,349 iteration 6647 : loss : 0.019361, loss_ce: 0.004129
 98%|████████████████████████████▎| 391/400 [3:09:07<04:24, 29.40s/it]2022-01-12 01:21:37,990 iteration 6648 : loss : 0.015540, loss_ce: 0.006374
2022-01-12 01:21:39,599 iteration 6649 : loss : 0.020107, loss_ce: 0.006078
2022-01-12 01:21:41,109 iteration 6650 : loss : 0.013172, loss_ce: 0.005022
2022-01-12 01:21:42,642 iteration 6651 : loss : 0.013562, loss_ce: 0.005751
2022-01-12 01:21:44,236 iteration 6652 : loss : 0.014618, loss_ce: 0.005816
2022-01-12 01:21:45,871 iteration 6653 : loss : 0.019467, loss_ce: 0.005878
2022-01-12 01:21:47,552 iteration 6654 : loss : 0.020868, loss_ce: 0.006631
2022-01-12 01:21:49,106 iteration 6655 : loss : 0.013359, loss_ce: 0.005538
2022-01-12 01:21:50,752 iteration 6656 : loss : 0.027021, loss_ce: 0.008757
2022-01-12 01:21:52,321 iteration 6657 : loss : 0.016135, loss_ce: 0.005182
2022-01-12 01:21:53,935 iteration 6658 : loss : 0.019762, loss_ce: 0.007671
2022-01-12 01:21:55,516 iteration 6659 : loss : 0.026283, loss_ce: 0.007139
2022-01-12 01:21:57,081 iteration 6660 : loss : 0.014231, loss_ce: 0.005861
2022-01-12 01:21:58,610 iteration 6661 : loss : 0.012739, loss_ce: 0.004707
2022-01-12 01:22:00,222 iteration 6662 : loss : 0.021835, loss_ce: 0.008055
2022-01-12 01:22:01,757 iteration 6663 : loss : 0.018933, loss_ce: 0.009277
2022-01-12 01:22:03,363 iteration 6664 : loss : 0.013021, loss_ce: 0.005688
 98%|████████████████████████████▍| 392/400 [3:09:34<03:49, 28.69s/it]2022-01-12 01:22:04,983 iteration 6665 : loss : 0.034154, loss_ce: 0.012158
2022-01-12 01:22:06,582 iteration 6666 : loss : 0.012188, loss_ce: 0.003960
2022-01-12 01:22:08,159 iteration 6667 : loss : 0.015647, loss_ce: 0.006411
2022-01-12 01:22:09,855 iteration 6668 : loss : 0.020565, loss_ce: 0.010104
2022-01-12 01:22:11,363 iteration 6669 : loss : 0.016797, loss_ce: 0.005943
2022-01-12 01:22:12,892 iteration 6670 : loss : 0.014942, loss_ce: 0.007451
2022-01-12 01:22:14,440 iteration 6671 : loss : 0.016550, loss_ce: 0.007172
2022-01-12 01:22:15,945 iteration 6672 : loss : 0.010488, loss_ce: 0.002933
2022-01-12 01:22:17,547 iteration 6673 : loss : 0.032150, loss_ce: 0.011455
2022-01-12 01:22:19,103 iteration 6674 : loss : 0.022519, loss_ce: 0.008950
2022-01-12 01:22:20,725 iteration 6675 : loss : 0.016183, loss_ce: 0.005082
2022-01-12 01:22:22,249 iteration 6676 : loss : 0.011877, loss_ce: 0.003679
2022-01-12 01:22:23,812 iteration 6677 : loss : 0.013894, loss_ce: 0.003805
2022-01-12 01:22:25,276 iteration 6678 : loss : 0.010709, loss_ce: 0.004587
2022-01-12 01:22:26,956 iteration 6679 : loss : 0.017781, loss_ce: 0.008363
2022-01-12 01:22:28,520 iteration 6680 : loss : 0.019934, loss_ce: 0.007382
2022-01-12 01:22:30,135 iteration 6681 : loss : 0.018335, loss_ce: 0.007183
 98%|████████████████████████████▍| 393/400 [3:10:00<03:16, 28.11s/it]2022-01-12 01:22:31,755 iteration 6682 : loss : 0.016004, loss_ce: 0.007738
2022-01-12 01:22:33,432 iteration 6683 : loss : 0.017868, loss_ce: 0.006724
2022-01-12 01:22:34,965 iteration 6684 : loss : 0.012827, loss_ce: 0.004025
2022-01-12 01:22:36,596 iteration 6685 : loss : 0.020207, loss_ce: 0.009260
2022-01-12 01:22:38,121 iteration 6686 : loss : 0.015545, loss_ce: 0.007414
2022-01-12 01:22:39,699 iteration 6687 : loss : 0.020952, loss_ce: 0.004593
2022-01-12 01:22:41,319 iteration 6688 : loss : 0.017790, loss_ce: 0.006602
2022-01-12 01:22:42,858 iteration 6689 : loss : 0.012164, loss_ce: 0.004472
2022-01-12 01:22:44,410 iteration 6690 : loss : 0.016644, loss_ce: 0.007105
2022-01-12 01:22:46,037 iteration 6691 : loss : 0.022083, loss_ce: 0.009609
2022-01-12 01:22:47,617 iteration 6692 : loss : 0.017105, loss_ce: 0.004768
2022-01-12 01:22:49,228 iteration 6693 : loss : 0.013471, loss_ce: 0.004280
2022-01-12 01:22:50,814 iteration 6694 : loss : 0.016342, loss_ce: 0.006692
2022-01-12 01:22:52,433 iteration 6695 : loss : 0.018855, loss_ce: 0.007569
2022-01-12 01:22:53,953 iteration 6696 : loss : 0.013273, loss_ce: 0.006414
2022-01-12 01:22:55,574 iteration 6697 : loss : 0.015295, loss_ce: 0.005877
2022-01-12 01:22:57,148 iteration 6698 : loss : 0.016351, loss_ce: 0.006555
 98%|████████████████████████████▌| 394/400 [3:10:27<02:46, 27.78s/it]2022-01-12 01:22:58,732 iteration 6699 : loss : 0.014718, loss_ce: 0.007716
2022-01-12 01:23:00,366 iteration 6700 : loss : 0.021280, loss_ce: 0.009438
2022-01-12 01:23:01,914 iteration 6701 : loss : 0.015136, loss_ce: 0.007557
2022-01-12 01:23:03,451 iteration 6702 : loss : 0.018762, loss_ce: 0.006138
2022-01-12 01:23:05,007 iteration 6703 : loss : 0.021022, loss_ce: 0.007999
2022-01-12 01:23:06,617 iteration 6704 : loss : 0.026095, loss_ce: 0.010431
2022-01-12 01:23:08,156 iteration 6705 : loss : 0.022773, loss_ce: 0.008556
2022-01-12 01:23:09,684 iteration 6706 : loss : 0.010907, loss_ce: 0.005357
2022-01-12 01:23:11,225 iteration 6707 : loss : 0.015159, loss_ce: 0.004518
2022-01-12 01:23:12,722 iteration 6708 : loss : 0.012560, loss_ce: 0.004570
2022-01-12 01:23:14,249 iteration 6709 : loss : 0.012700, loss_ce: 0.004967
2022-01-12 01:23:15,784 iteration 6710 : loss : 0.015211, loss_ce: 0.005079
2022-01-12 01:23:17,317 iteration 6711 : loss : 0.016006, loss_ce: 0.002841
2022-01-12 01:23:18,906 iteration 6712 : loss : 0.017664, loss_ce: 0.006990
2022-01-12 01:23:20,530 iteration 6713 : loss : 0.021082, loss_ce: 0.009293
2022-01-12 01:23:22,116 iteration 6714 : loss : 0.016589, loss_ce: 0.007724
2022-01-12 01:23:22,117 Training Data Eval:
2022-01-12 01:23:30,144   Average segmentation loss on training set: 0.0084
2022-01-12 01:23:30,144 Validation Data Eval:
2022-01-12 01:23:32,907   Average segmentation loss on validation set: 0.0726
2022-01-12 01:23:34,524 iteration 6715 : loss : 0.025036, loss_ce: 0.011655
 99%|████████████████████████████▋| 395/400 [3:11:05<02:33, 30.66s/it]2022-01-12 01:23:36,149 iteration 6716 : loss : 0.008907, loss_ce: 0.003156
2022-01-12 01:23:37,723 iteration 6717 : loss : 0.030181, loss_ce: 0.013314
2022-01-12 01:23:39,239 iteration 6718 : loss : 0.012767, loss_ce: 0.004241
2022-01-12 01:23:40,796 iteration 6719 : loss : 0.021943, loss_ce: 0.010135
2022-01-12 01:23:42,366 iteration 6720 : loss : 0.017022, loss_ce: 0.005894
2022-01-12 01:23:43,976 iteration 6721 : loss : 0.017271, loss_ce: 0.005866
2022-01-12 01:23:45,589 iteration 6722 : loss : 0.023877, loss_ce: 0.007684
2022-01-12 01:23:47,154 iteration 6723 : loss : 0.016438, loss_ce: 0.005312
2022-01-12 01:23:48,696 iteration 6724 : loss : 0.016796, loss_ce: 0.007929
2022-01-12 01:23:50,295 iteration 6725 : loss : 0.013672, loss_ce: 0.006368
2022-01-12 01:23:51,883 iteration 6726 : loss : 0.023588, loss_ce: 0.006728
2022-01-12 01:23:53,489 iteration 6727 : loss : 0.017533, loss_ce: 0.007607
2022-01-12 01:23:55,095 iteration 6728 : loss : 0.020227, loss_ce: 0.010572
2022-01-12 01:23:56,612 iteration 6729 : loss : 0.016117, loss_ce: 0.005635
2022-01-12 01:23:58,153 iteration 6730 : loss : 0.028734, loss_ce: 0.009418
2022-01-12 01:23:59,687 iteration 6731 : loss : 0.012991, loss_ce: 0.005634
2022-01-12 01:24:01,261 iteration 6732 : loss : 0.017771, loss_ce: 0.009093
 99%|████████████████████████████▋| 396/400 [3:11:32<01:57, 29.48s/it]2022-01-12 01:24:02,913 iteration 6733 : loss : 0.020156, loss_ce: 0.008975
2022-01-12 01:24:04,440 iteration 6734 : loss : 0.014293, loss_ce: 0.006062
2022-01-12 01:24:06,018 iteration 6735 : loss : 0.015284, loss_ce: 0.007505
2022-01-12 01:24:07,588 iteration 6736 : loss : 0.014012, loss_ce: 0.004437
2022-01-12 01:24:09,128 iteration 6737 : loss : 0.013291, loss_ce: 0.005933
2022-01-12 01:24:10,688 iteration 6738 : loss : 0.011854, loss_ce: 0.004292
2022-01-12 01:24:12,293 iteration 6739 : loss : 0.020130, loss_ce: 0.007543
2022-01-12 01:24:13,928 iteration 6740 : loss : 0.019918, loss_ce: 0.007547
2022-01-12 01:24:15,426 iteration 6741 : loss : 0.010424, loss_ce: 0.004401
2022-01-12 01:24:17,028 iteration 6742 : loss : 0.019698, loss_ce: 0.008474
2022-01-12 01:24:18,616 iteration 6743 : loss : 0.020029, loss_ce: 0.006587
2022-01-12 01:24:20,254 iteration 6744 : loss : 0.015690, loss_ce: 0.006069
2022-01-12 01:24:21,785 iteration 6745 : loss : 0.022538, loss_ce: 0.006469
2022-01-12 01:24:23,333 iteration 6746 : loss : 0.022729, loss_ce: 0.010499
2022-01-12 01:24:24,885 iteration 6747 : loss : 0.017301, loss_ce: 0.006048
2022-01-12 01:24:26,456 iteration 6748 : loss : 0.016127, loss_ce: 0.006316
2022-01-12 01:24:28,067 iteration 6749 : loss : 0.022792, loss_ce: 0.008673
 99%|████████████████████████████▊| 397/400 [3:11:58<01:26, 28.68s/it]2022-01-12 01:24:29,658 iteration 6750 : loss : 0.016335, loss_ce: 0.006028
2022-01-12 01:24:31,265 iteration 6751 : loss : 0.017354, loss_ce: 0.008436
2022-01-12 01:24:32,876 iteration 6752 : loss : 0.016719, loss_ce: 0.008160
2022-01-12 01:24:34,467 iteration 6753 : loss : 0.014402, loss_ce: 0.005720
2022-01-12 01:24:36,099 iteration 6754 : loss : 0.017401, loss_ce: 0.005195
2022-01-12 01:24:37,675 iteration 6755 : loss : 0.009356, loss_ce: 0.003005
2022-01-12 01:24:39,255 iteration 6756 : loss : 0.012440, loss_ce: 0.004809
2022-01-12 01:24:40,842 iteration 6757 : loss : 0.017165, loss_ce: 0.007579
2022-01-12 01:24:42,369 iteration 6758 : loss : 0.010352, loss_ce: 0.004508
2022-01-12 01:24:43,981 iteration 6759 : loss : 0.020306, loss_ce: 0.009123
2022-01-12 01:24:45,536 iteration 6760 : loss : 0.016951, loss_ce: 0.006538
2022-01-12 01:24:47,143 iteration 6761 : loss : 0.015253, loss_ce: 0.006356
2022-01-12 01:24:48,626 iteration 6762 : loss : 0.010842, loss_ce: 0.004081
2022-01-12 01:24:50,211 iteration 6763 : loss : 0.018229, loss_ce: 0.005012
2022-01-12 01:24:51,765 iteration 6764 : loss : 0.012747, loss_ce: 0.004541
2022-01-12 01:24:53,299 iteration 6765 : loss : 0.017470, loss_ce: 0.005212
2022-01-12 01:24:54,856 iteration 6766 : loss : 0.015443, loss_ce: 0.005384
100%|████████████████████████████▊| 398/400 [3:12:25<00:56, 28.11s/it]2022-01-12 01:24:56,579 iteration 6767 : loss : 0.020873, loss_ce: 0.006496
2022-01-12 01:24:58,093 iteration 6768 : loss : 0.012651, loss_ce: 0.005991
2022-01-12 01:24:59,663 iteration 6769 : loss : 0.015643, loss_ce: 0.005336
2022-01-12 01:25:01,173 iteration 6770 : loss : 0.011425, loss_ce: 0.004774
2022-01-12 01:25:02,734 iteration 6771 : loss : 0.014867, loss_ce: 0.004778
2022-01-12 01:25:04,363 iteration 6772 : loss : 0.019648, loss_ce: 0.005544
2022-01-12 01:25:05,950 iteration 6773 : loss : 0.014267, loss_ce: 0.006148
2022-01-12 01:25:07,514 iteration 6774 : loss : 0.017568, loss_ce: 0.008955
2022-01-12 01:25:09,079 iteration 6775 : loss : 0.019039, loss_ce: 0.007658
2022-01-12 01:25:10,705 iteration 6776 : loss : 0.025171, loss_ce: 0.010579
2022-01-12 01:25:12,288 iteration 6777 : loss : 0.012647, loss_ce: 0.004856
2022-01-12 01:25:13,815 iteration 6778 : loss : 0.013540, loss_ce: 0.004731
2022-01-12 01:25:15,365 iteration 6779 : loss : 0.013815, loss_ce: 0.004649
2022-01-12 01:25:16,859 iteration 6780 : loss : 0.011594, loss_ce: 0.003979
2022-01-12 01:25:18,389 iteration 6781 : loss : 0.013105, loss_ce: 0.006018
2022-01-12 01:25:19,953 iteration 6782 : loss : 0.010265, loss_ce: 0.004382
2022-01-12 01:25:21,517 iteration 6783 : loss : 0.012565, loss_ce: 0.005010
100%|████████████████████████████▉| 399/400 [3:12:52<00:27, 27.68s/it]2022-01-12 01:25:23,205 iteration 6784 : loss : 0.022363, loss_ce: 0.007535
2022-01-12 01:25:24,707 iteration 6785 : loss : 0.017282, loss_ce: 0.007484
2022-01-12 01:25:26,188 iteration 6786 : loss : 0.012293, loss_ce: 0.005208
2022-01-12 01:25:27,780 iteration 6787 : loss : 0.016172, loss_ce: 0.005410
2022-01-12 01:25:29,279 iteration 6788 : loss : 0.038901, loss_ce: 0.008289
2022-01-12 01:25:30,847 iteration 6789 : loss : 0.010421, loss_ce: 0.004272
2022-01-12 01:25:32,447 iteration 6790 : loss : 0.012839, loss_ce: 0.004233
2022-01-12 01:25:34,077 iteration 6791 : loss : 0.025265, loss_ce: 0.009594
2022-01-12 01:25:35,699 iteration 6792 : loss : 0.017428, loss_ce: 0.007580
2022-01-12 01:25:37,286 iteration 6793 : loss : 0.019046, loss_ce: 0.007103
2022-01-12 01:25:38,894 iteration 6794 : loss : 0.016136, loss_ce: 0.006350
2022-01-12 01:25:40,456 iteration 6795 : loss : 0.012211, loss_ce: 0.004475
2022-01-12 01:25:42,031 iteration 6796 : loss : 0.015665, loss_ce: 0.005901
2022-01-12 01:25:43,564 iteration 6797 : loss : 0.013974, loss_ce: 0.005074
2022-01-12 01:25:45,057 iteration 6798 : loss : 0.014382, loss_ce: 0.005311
2022-01-12 01:25:46,611 iteration 6799 : loss : 0.020666, loss_ce: 0.009128
2022-01-12 01:25:46,611 Training Data Eval:
2022-01-12 01:25:54,638   Average segmentation loss on training set: 0.0084
2022-01-12 01:25:54,638 Validation Data Eval:
2022-01-12 01:25:57,402   Average segmentation loss on validation set: 0.0723
2022-01-12 01:25:58,957 iteration 6800 : loss : 0.023870, loss_ce: 0.010454
100%|█████████████████████████████| 400/400 [3:13:29<00:00, 30.61s/it]100%|█████████████████████████████| 400/400 [3:13:29<00:00, 29.02s/it]
