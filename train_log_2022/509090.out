2022-01-20 19:32:58,753 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-20 19:32:58,753 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-20 19:32:58,753 ============================================================
2022-01-20 19:32:58,753 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-20 19:32:58,754 ============================================================
2022-01-20 19:32:58,754 Loading data...
2022-01-20 19:32:58,754 Reading NCI - RUNMC images...
2022-01-20 19:32:58,754 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-20 19:32:58,754 Already preprocessed this configuration. Loading now!
2022-01-20 19:32:58,774 Training Images: (256, 256, 286)
2022-01-20 19:32:58,774 Training Labels: (256, 256, 286)
2022-01-20 19:32:58,774 Validation Images: (256, 256, 98)
2022-01-20 19:32:58,774 Validation Labels: (256, 256, 98)
2022-01-20 19:32:58,774 ============================================================
2022-01-20 19:32:59,329 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-20 19:33:01,762 iteration 1 : loss : 0.925569, loss_ce: 1.121795
2022-01-20 19:33:02,960 iteration 2 : loss : 0.862832, loss_ce: 1.031697
2022-01-20 19:33:04,243 iteration 3 : loss : 0.801136, loss_ce: 0.933532
2022-01-20 19:33:05,470 iteration 4 : loss : 0.762388, loss_ce: 0.844726
2022-01-20 19:33:06,625 iteration 5 : loss : 0.724757, loss_ce: 0.768498
2022-01-20 19:33:07,836 iteration 6 : loss : 0.678595, loss_ce: 0.702200
2022-01-20 19:33:09,086 iteration 7 : loss : 0.633984, loss_ce: 0.645631
2022-01-20 19:33:10,390 iteration 8 : loss : 0.605733, loss_ce: 0.596087
2022-01-20 19:33:11,557 iteration 9 : loss : 0.601680, loss_ce: 0.551505
2022-01-20 19:33:12,725 iteration 10 : loss : 0.565998, loss_ce: 0.509108
2022-01-20 19:33:13,875 iteration 11 : loss : 0.548477, loss_ce: 0.464739
2022-01-20 19:33:15,076 iteration 12 : loss : 0.526619, loss_ce: 0.445268
2022-01-20 19:33:16,334 iteration 13 : loss : 0.501637, loss_ce: 0.428861
2022-01-20 19:33:17,491 iteration 14 : loss : 0.471823, loss_ce: 0.377161
2022-01-20 19:33:18,746 iteration 15 : loss : 0.457543, loss_ce: 0.346544
2022-01-20 19:33:19,946 iteration 16 : loss : 0.445682, loss_ce: 0.319973
2022-01-20 19:33:21,139 iteration 17 : loss : 0.424883, loss_ce: 0.311750
  0%|                               | 1/400 [00:21<2:21:50, 21.33s/it]2022-01-20 19:33:22,373 iteration 18 : loss : 0.434741, loss_ce: 0.287648
2022-01-20 19:33:23,602 iteration 19 : loss : 0.390864, loss_ce: 0.256823
2022-01-20 19:33:24,808 iteration 20 : loss : 0.383339, loss_ce: 0.246056
2022-01-20 19:33:26,089 iteration 21 : loss : 0.370594, loss_ce: 0.233021
2022-01-20 19:33:27,288 iteration 22 : loss : 0.377477, loss_ce: 0.224969
2022-01-20 19:33:28,511 iteration 23 : loss : 0.334414, loss_ce: 0.192915
2022-01-20 19:33:29,674 iteration 24 : loss : 0.352235, loss_ce: 0.212382
2022-01-20 19:33:30,815 iteration 25 : loss : 0.329796, loss_ce: 0.176304
2022-01-20 19:33:32,042 iteration 26 : loss : 0.365960, loss_ce: 0.182564
2022-01-20 19:33:33,340 iteration 27 : loss : 0.315183, loss_ce: 0.170546
2022-01-20 19:33:34,624 iteration 28 : loss : 0.315619, loss_ce: 0.157986
2022-01-20 19:33:36,041 iteration 29 : loss : 0.322734, loss_ce: 0.171613
2022-01-20 19:33:37,369 iteration 30 : loss : 0.313099, loss_ce: 0.151432
2022-01-20 19:33:38,621 iteration 31 : loss : 0.295832, loss_ce: 0.150735
2022-01-20 19:33:39,913 iteration 32 : loss : 0.295521, loss_ce: 0.146338
2022-01-20 19:33:41,295 iteration 33 : loss : 0.307464, loss_ce: 0.160265
2022-01-20 19:33:42,593 iteration 34 : loss : 0.294106, loss_ce: 0.125374
  0%|▏                              | 2/400 [00:42<2:21:54, 21.39s/it]2022-01-20 19:33:44,001 iteration 35 : loss : 0.290800, loss_ce: 0.144396
2022-01-20 19:33:45,330 iteration 36 : loss : 0.292927, loss_ce: 0.139408
2022-01-20 19:33:46,708 iteration 37 : loss : 0.296756, loss_ce: 0.151702
2022-01-20 19:33:48,136 iteration 38 : loss : 0.270907, loss_ce: 0.118055
2022-01-20 19:33:49,478 iteration 39 : loss : 0.340495, loss_ce: 0.139622
2022-01-20 19:33:50,859 iteration 40 : loss : 0.286372, loss_ce: 0.139485
2022-01-20 19:33:52,145 iteration 41 : loss : 0.277822, loss_ce: 0.113661
2022-01-20 19:33:53,602 iteration 42 : loss : 0.269493, loss_ce: 0.123193
2022-01-20 19:33:54,885 iteration 43 : loss : 0.286380, loss_ce: 0.107379
2022-01-20 19:33:56,246 iteration 44 : loss : 0.267045, loss_ce: 0.103227
2022-01-20 19:33:57,625 iteration 45 : loss : 0.343121, loss_ce: 0.150897
2022-01-20 19:33:58,878 iteration 46 : loss : 0.322350, loss_ce: 0.152748
2022-01-20 19:34:00,204 iteration 47 : loss : 0.320360, loss_ce: 0.131958
2022-01-20 19:34:01,576 iteration 48 : loss : 0.260346, loss_ce: 0.129220
2022-01-20 19:34:02,933 iteration 49 : loss : 0.300944, loss_ce: 0.117472
2022-01-20 19:34:04,324 iteration 50 : loss : 0.280663, loss_ce: 0.105341
2022-01-20 19:34:05,681 iteration 51 : loss : 0.292896, loss_ce: 0.139789
  1%|▏                              | 3/400 [01:05<2:26:40, 22.17s/it]2022-01-20 19:34:06,981 iteration 52 : loss : 0.287937, loss_ce: 0.127322
2022-01-20 19:34:08,235 iteration 53 : loss : 0.273729, loss_ce: 0.124442
2022-01-20 19:34:09,597 iteration 54 : loss : 0.233503, loss_ce: 0.104427
2022-01-20 19:34:10,947 iteration 55 : loss : 0.354525, loss_ce: 0.152821
2022-01-20 19:34:12,321 iteration 56 : loss : 0.273015, loss_ce: 0.118688
2022-01-20 19:34:13,688 iteration 57 : loss : 0.267434, loss_ce: 0.109648
2022-01-20 19:34:15,036 iteration 58 : loss : 0.293230, loss_ce: 0.143950
2022-01-20 19:34:16,364 iteration 59 : loss : 0.270743, loss_ce: 0.114815
2022-01-20 19:34:17,776 iteration 60 : loss : 0.267216, loss_ce: 0.110528
2022-01-20 19:34:19,199 iteration 61 : loss : 0.235873, loss_ce: 0.103937
2022-01-20 19:34:20,554 iteration 62 : loss : 0.254859, loss_ce: 0.104725
2022-01-20 19:34:21,829 iteration 63 : loss : 0.236495, loss_ce: 0.112144
2022-01-20 19:34:23,156 iteration 64 : loss : 0.279770, loss_ce: 0.149410
2022-01-20 19:34:24,480 iteration 65 : loss : 0.281438, loss_ce: 0.113478
2022-01-20 19:34:25,868 iteration 66 : loss : 0.252405, loss_ce: 0.111130
2022-01-20 19:34:27,166 iteration 67 : loss : 0.265821, loss_ce: 0.122684
2022-01-20 19:34:28,501 iteration 68 : loss : 0.293248, loss_ce: 0.112530
  1%|▎                              | 4/400 [01:28<2:28:00, 22.43s/it]2022-01-20 19:34:29,885 iteration 69 : loss : 0.262149, loss_ce: 0.099448
2022-01-20 19:34:31,248 iteration 70 : loss : 0.286677, loss_ce: 0.131590
2022-01-20 19:34:32,543 iteration 71 : loss : 0.232399, loss_ce: 0.097566
2022-01-20 19:34:33,936 iteration 72 : loss : 0.256432, loss_ce: 0.095386
2022-01-20 19:34:35,269 iteration 73 : loss : 0.249503, loss_ce: 0.118683
2022-01-20 19:34:36,684 iteration 74 : loss : 0.251345, loss_ce: 0.110983
2022-01-20 19:34:38,041 iteration 75 : loss : 0.265049, loss_ce: 0.136435
2022-01-20 19:34:39,436 iteration 76 : loss : 0.246093, loss_ce: 0.104779
2022-01-20 19:34:40,860 iteration 77 : loss : 0.239343, loss_ce: 0.089448
2022-01-20 19:34:42,262 iteration 78 : loss : 0.208667, loss_ce: 0.098241
2022-01-20 19:34:43,554 iteration 79 : loss : 0.196567, loss_ce: 0.073790
2022-01-20 19:34:44,947 iteration 80 : loss : 0.296386, loss_ce: 0.111019
2022-01-20 19:34:46,277 iteration 81 : loss : 0.204954, loss_ce: 0.078668
2022-01-20 19:34:47,620 iteration 82 : loss : 0.269555, loss_ce: 0.128536
2022-01-20 19:34:49,017 iteration 83 : loss : 0.231353, loss_ce: 0.086388
2022-01-20 19:34:50,351 iteration 84 : loss : 0.271784, loss_ce: 0.124382
2022-01-20 19:34:50,351 Training Data Eval:
2022-01-20 19:34:56,954   Average segmentation loss on training set: 0.2503
2022-01-20 19:34:56,955 Validation Data Eval:
2022-01-20 19:34:59,220   Average segmentation loss on validation set: 0.3099
2022-01-20 19:35:05,095 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 19:35:06,367 iteration 85 : loss : 0.239846, loss_ce: 0.109231
  1%|▍                              | 5/400 [02:06<3:04:16, 27.99s/it]2022-01-20 19:35:07,599 iteration 86 : loss : 0.295604, loss_ce: 0.113109
2022-01-20 19:35:08,762 iteration 87 : loss : 0.248349, loss_ce: 0.106557
2022-01-20 19:35:10,058 iteration 88 : loss : 0.225034, loss_ce: 0.085016
2022-01-20 19:35:11,445 iteration 89 : loss : 0.209148, loss_ce: 0.091211
2022-01-20 19:35:12,740 iteration 90 : loss : 0.171231, loss_ce: 0.071592
2022-01-20 19:35:14,088 iteration 91 : loss : 0.227843, loss_ce: 0.090447
2022-01-20 19:35:15,405 iteration 92 : loss : 0.205969, loss_ce: 0.081067
2022-01-20 19:35:16,805 iteration 93 : loss : 0.229069, loss_ce: 0.088093
2022-01-20 19:35:18,204 iteration 94 : loss : 0.212346, loss_ce: 0.102708
2022-01-20 19:35:19,520 iteration 95 : loss : 0.224658, loss_ce: 0.084426
2022-01-20 19:35:20,806 iteration 96 : loss : 0.184529, loss_ce: 0.068454
2022-01-20 19:35:22,085 iteration 97 : loss : 0.183747, loss_ce: 0.074515
2022-01-20 19:35:23,401 iteration 98 : loss : 0.250180, loss_ce: 0.097113
2022-01-20 19:35:24,851 iteration 99 : loss : 0.183953, loss_ce: 0.074834
2022-01-20 19:35:26,309 iteration 100 : loss : 0.226202, loss_ce: 0.095616
2022-01-20 19:35:27,616 iteration 101 : loss : 0.279549, loss_ce: 0.133720
2022-01-20 19:35:28,958 iteration 102 : loss : 0.243612, loss_ce: 0.093757
  2%|▍                              | 6/400 [02:29<2:51:46, 26.16s/it]2022-01-20 19:35:30,358 iteration 103 : loss : 0.198030, loss_ce: 0.077257
2022-01-20 19:35:31,753 iteration 104 : loss : 0.225893, loss_ce: 0.084542
2022-01-20 19:35:33,110 iteration 105 : loss : 0.191311, loss_ce: 0.078254
2022-01-20 19:35:34,468 iteration 106 : loss : 0.209097, loss_ce: 0.072058
2022-01-20 19:35:35,890 iteration 107 : loss : 0.175142, loss_ce: 0.068298
2022-01-20 19:35:37,219 iteration 108 : loss : 0.230013, loss_ce: 0.104082
2022-01-20 19:35:38,570 iteration 109 : loss : 0.162327, loss_ce: 0.059757
2022-01-20 19:35:39,952 iteration 110 : loss : 0.207918, loss_ce: 0.088203
2022-01-20 19:35:41,465 iteration 111 : loss : 0.187169, loss_ce: 0.072518
2022-01-20 19:35:42,781 iteration 112 : loss : 0.242288, loss_ce: 0.094673
2022-01-20 19:35:44,228 iteration 113 : loss : 0.171551, loss_ce: 0.060145
2022-01-20 19:35:45,588 iteration 114 : loss : 0.193621, loss_ce: 0.064442
2022-01-20 19:35:46,865 iteration 115 : loss : 0.178783, loss_ce: 0.063937
2022-01-20 19:35:48,211 iteration 116 : loss : 0.247246, loss_ce: 0.103402
2022-01-20 19:35:49,524 iteration 117 : loss : 0.149591, loss_ce: 0.053472
2022-01-20 19:35:50,923 iteration 118 : loss : 0.237195, loss_ce: 0.085776
2022-01-20 19:35:52,238 iteration 119 : loss : 0.245252, loss_ce: 0.097498
  2%|▌                              | 7/400 [02:52<2:45:10, 25.22s/it]2022-01-20 19:35:53,682 iteration 120 : loss : 0.226516, loss_ce: 0.102849
2022-01-20 19:35:55,077 iteration 121 : loss : 0.234684, loss_ce: 0.094284
2022-01-20 19:35:56,329 iteration 122 : loss : 0.166970, loss_ce: 0.062768
2022-01-20 19:35:57,680 iteration 123 : loss : 0.221073, loss_ce: 0.089636
2022-01-20 19:35:59,004 iteration 124 : loss : 0.185138, loss_ce: 0.067252
2022-01-20 19:36:00,335 iteration 125 : loss : 0.204043, loss_ce: 0.078347
2022-01-20 19:36:01,652 iteration 126 : loss : 0.168152, loss_ce: 0.066289
2022-01-20 19:36:02,994 iteration 127 : loss : 0.237447, loss_ce: 0.107326
2022-01-20 19:36:04,303 iteration 128 : loss : 0.173167, loss_ce: 0.060080
2022-01-20 19:36:05,713 iteration 129 : loss : 0.228080, loss_ce: 0.083135
2022-01-20 19:36:06,954 iteration 130 : loss : 0.167606, loss_ce: 0.060142
2022-01-20 19:36:08,270 iteration 131 : loss : 0.205462, loss_ce: 0.090779
2022-01-20 19:36:09,616 iteration 132 : loss : 0.190208, loss_ce: 0.073510
2022-01-20 19:36:10,962 iteration 133 : loss : 0.273149, loss_ce: 0.137301
2022-01-20 19:36:12,286 iteration 134 : loss : 0.192765, loss_ce: 0.074006
2022-01-20 19:36:13,599 iteration 135 : loss : 0.201996, loss_ce: 0.067447
2022-01-20 19:36:15,017 iteration 136 : loss : 0.180033, loss_ce: 0.073596
  2%|▌                              | 8/400 [03:15<2:39:40, 24.44s/it]2022-01-20 19:36:16,480 iteration 137 : loss : 0.162786, loss_ce: 0.052002
2022-01-20 19:36:17,773 iteration 138 : loss : 0.159176, loss_ce: 0.057346
2022-01-20 19:36:19,205 iteration 139 : loss : 0.186505, loss_ce: 0.049447
2022-01-20 19:36:20,537 iteration 140 : loss : 0.229039, loss_ce: 0.091501
2022-01-20 19:36:21,829 iteration 141 : loss : 0.228390, loss_ce: 0.084190
2022-01-20 19:36:23,151 iteration 142 : loss : 0.183522, loss_ce: 0.067991
2022-01-20 19:36:24,497 iteration 143 : loss : 0.176435, loss_ce: 0.063650
2022-01-20 19:36:25,815 iteration 144 : loss : 0.189420, loss_ce: 0.059597
2022-01-20 19:36:27,159 iteration 145 : loss : 0.185551, loss_ce: 0.087991
2022-01-20 19:36:28,538 iteration 146 : loss : 0.153656, loss_ce: 0.059184
2022-01-20 19:36:29,886 iteration 147 : loss : 0.242997, loss_ce: 0.101257
2022-01-20 19:36:31,320 iteration 148 : loss : 0.161369, loss_ce: 0.061174
2022-01-20 19:36:32,739 iteration 149 : loss : 0.165281, loss_ce: 0.060331
2022-01-20 19:36:34,156 iteration 150 : loss : 0.182445, loss_ce: 0.070864
2022-01-20 19:36:35,460 iteration 151 : loss : 0.166965, loss_ce: 0.084094
2022-01-20 19:36:36,713 iteration 152 : loss : 0.141007, loss_ce: 0.063095
2022-01-20 19:36:38,036 iteration 153 : loss : 0.164437, loss_ce: 0.062540
  2%|▋                              | 9/400 [03:38<2:36:21, 23.99s/it]2022-01-20 19:36:39,458 iteration 154 : loss : 0.210251, loss_ce: 0.101917
2022-01-20 19:36:40,791 iteration 155 : loss : 0.182880, loss_ce: 0.091776
2022-01-20 19:36:42,091 iteration 156 : loss : 0.164093, loss_ce: 0.070695
2022-01-20 19:36:43,392 iteration 157 : loss : 0.173401, loss_ce: 0.069907
2022-01-20 19:36:44,793 iteration 158 : loss : 0.152680, loss_ce: 0.053320
2022-01-20 19:36:46,191 iteration 159 : loss : 0.148921, loss_ce: 0.056665
2022-01-20 19:36:47,510 iteration 160 : loss : 0.165933, loss_ce: 0.058857
2022-01-20 19:36:48,938 iteration 161 : loss : 0.133395, loss_ce: 0.055955
2022-01-20 19:36:50,293 iteration 162 : loss : 0.198112, loss_ce: 0.074710
2022-01-20 19:36:51,644 iteration 163 : loss : 0.215564, loss_ce: 0.101209
2022-01-20 19:36:52,980 iteration 164 : loss : 0.148915, loss_ce: 0.050172
2022-01-20 19:36:54,254 iteration 165 : loss : 0.150460, loss_ce: 0.051821
2022-01-20 19:36:55,656 iteration 166 : loss : 0.192374, loss_ce: 0.102636
2022-01-20 19:36:57,124 iteration 167 : loss : 0.226568, loss_ce: 0.099005
2022-01-20 19:36:58,464 iteration 168 : loss : 0.211173, loss_ce: 0.061222
2022-01-20 19:36:59,818 iteration 169 : loss : 0.189574, loss_ce: 0.073164
2022-01-20 19:36:59,818 Training Data Eval:
2022-01-20 19:37:06,408   Average segmentation loss on training set: 0.2437
2022-01-20 19:37:06,409 Validation Data Eval:
2022-01-20 19:37:08,669   Average segmentation loss on validation set: 0.3053
2022-01-20 19:37:14,431 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 19:37:15,757 iteration 170 : loss : 0.169809, loss_ce: 0.073136
  2%|▊                             | 10/400 [04:15<3:03:30, 28.23s/it]2022-01-20 19:37:17,082 iteration 171 : loss : 0.162250, loss_ce: 0.075578
2022-01-20 19:37:18,306 iteration 172 : loss : 0.175838, loss_ce: 0.083518
2022-01-20 19:37:19,714 iteration 173 : loss : 0.173109, loss_ce: 0.067094
2022-01-20 19:37:20,981 iteration 174 : loss : 0.186252, loss_ce: 0.102810
2022-01-20 19:37:22,292 iteration 175 : loss : 0.200776, loss_ce: 0.069715
2022-01-20 19:37:23,599 iteration 176 : loss : 0.150925, loss_ce: 0.060196
2022-01-20 19:37:24,966 iteration 177 : loss : 0.125688, loss_ce: 0.050853
2022-01-20 19:37:26,299 iteration 178 : loss : 0.178381, loss_ce: 0.068284
2022-01-20 19:37:27,568 iteration 179 : loss : 0.178973, loss_ce: 0.057958
2022-01-20 19:37:28,989 iteration 180 : loss : 0.201581, loss_ce: 0.091241
2022-01-20 19:37:30,365 iteration 181 : loss : 0.167764, loss_ce: 0.070308
2022-01-20 19:37:31,701 iteration 182 : loss : 0.153988, loss_ce: 0.066946
2022-01-20 19:37:33,065 iteration 183 : loss : 0.170768, loss_ce: 0.055234
2022-01-20 19:37:34,561 iteration 184 : loss : 0.177040, loss_ce: 0.063853
2022-01-20 19:37:36,039 iteration 185 : loss : 0.164355, loss_ce: 0.071766
2022-01-20 19:37:37,369 iteration 186 : loss : 0.161334, loss_ce: 0.057721
2022-01-20 19:37:38,728 iteration 187 : loss : 0.107093, loss_ce: 0.046011
  3%|▊                             | 11/400 [04:38<2:52:35, 26.62s/it]2022-01-20 19:37:40,070 iteration 188 : loss : 0.162327, loss_ce: 0.061703
2022-01-20 19:37:41,502 iteration 189 : loss : 0.157837, loss_ce: 0.063437
2022-01-20 19:37:42,878 iteration 190 : loss : 0.213128, loss_ce: 0.095802
2022-01-20 19:37:44,243 iteration 191 : loss : 0.134766, loss_ce: 0.052767
2022-01-20 19:37:45,586 iteration 192 : loss : 0.143204, loss_ce: 0.050005
2022-01-20 19:37:46,861 iteration 193 : loss : 0.141331, loss_ce: 0.062773
2022-01-20 19:37:48,237 iteration 194 : loss : 0.141341, loss_ce: 0.057064
2022-01-20 19:37:49,623 iteration 195 : loss : 0.128358, loss_ce: 0.049960
2022-01-20 19:37:50,950 iteration 196 : loss : 0.132498, loss_ce: 0.059562
2022-01-20 19:37:52,326 iteration 197 : loss : 0.136975, loss_ce: 0.050482
2022-01-20 19:37:53,685 iteration 198 : loss : 0.141660, loss_ce: 0.047003
2022-01-20 19:37:54,936 iteration 199 : loss : 0.186348, loss_ce: 0.063377
2022-01-20 19:37:56,263 iteration 200 : loss : 0.141935, loss_ce: 0.070019
2022-01-20 19:37:57,724 iteration 201 : loss : 0.156633, loss_ce: 0.065143
2022-01-20 19:37:59,040 iteration 202 : loss : 0.141378, loss_ce: 0.052206
2022-01-20 19:38:00,336 iteration 203 : loss : 0.189056, loss_ce: 0.102413
2022-01-20 19:38:01,599 iteration 204 : loss : 0.111908, loss_ce: 0.046336
  3%|▉                             | 12/400 [05:01<2:44:47, 25.48s/it]2022-01-20 19:38:02,919 iteration 205 : loss : 0.199310, loss_ce: 0.095006
2022-01-20 19:38:04,279 iteration 206 : loss : 0.187538, loss_ce: 0.080769
2022-01-20 19:38:05,715 iteration 207 : loss : 0.155120, loss_ce: 0.070327
2022-01-20 19:38:07,150 iteration 208 : loss : 0.140417, loss_ce: 0.049439
2022-01-20 19:38:08,450 iteration 209 : loss : 0.128119, loss_ce: 0.049502
2022-01-20 19:38:09,727 iteration 210 : loss : 0.121497, loss_ce: 0.045035
2022-01-20 19:38:11,061 iteration 211 : loss : 0.143055, loss_ce: 0.055897
2022-01-20 19:38:12,313 iteration 212 : loss : 0.166693, loss_ce: 0.058624
2022-01-20 19:38:13,694 iteration 213 : loss : 0.145199, loss_ce: 0.069520
2022-01-20 19:38:15,048 iteration 214 : loss : 0.196121, loss_ce: 0.063104
2022-01-20 19:38:16,323 iteration 215 : loss : 0.156299, loss_ce: 0.065137
2022-01-20 19:38:17,713 iteration 216 : loss : 0.165347, loss_ce: 0.067274
2022-01-20 19:38:19,020 iteration 217 : loss : 0.138760, loss_ce: 0.057122
2022-01-20 19:38:20,336 iteration 218 : loss : 0.112582, loss_ce: 0.044400
2022-01-20 19:38:21,654 iteration 219 : loss : 0.129829, loss_ce: 0.044814
2022-01-20 19:38:22,934 iteration 220 : loss : 0.189366, loss_ce: 0.097485
2022-01-20 19:38:24,266 iteration 221 : loss : 0.120811, loss_ce: 0.062774
  3%|▉                             | 13/400 [05:24<2:38:52, 24.63s/it]2022-01-20 19:38:25,763 iteration 222 : loss : 0.207556, loss_ce: 0.094658
2022-01-20 19:38:27,151 iteration 223 : loss : 0.120356, loss_ce: 0.052061
2022-01-20 19:38:28,400 iteration 224 : loss : 0.156801, loss_ce: 0.073796
2022-01-20 19:38:29,781 iteration 225 : loss : 0.148715, loss_ce: 0.055209
2022-01-20 19:38:31,168 iteration 226 : loss : 0.180273, loss_ce: 0.057704
2022-01-20 19:38:32,559 iteration 227 : loss : 0.163589, loss_ce: 0.061453
2022-01-20 19:38:33,927 iteration 228 : loss : 0.164477, loss_ce: 0.058503
2022-01-20 19:38:35,292 iteration 229 : loss : 0.206598, loss_ce: 0.088157
2022-01-20 19:38:36,634 iteration 230 : loss : 0.215490, loss_ce: 0.102127
2022-01-20 19:38:37,946 iteration 231 : loss : 0.184763, loss_ce: 0.057636
2022-01-20 19:38:39,379 iteration 232 : loss : 0.160613, loss_ce: 0.086342
2022-01-20 19:38:40,799 iteration 233 : loss : 0.169257, loss_ce: 0.092796
2022-01-20 19:38:42,182 iteration 234 : loss : 0.158582, loss_ce: 0.042439
2022-01-20 19:38:43,522 iteration 235 : loss : 0.135200, loss_ce: 0.050456
2022-01-20 19:38:44,843 iteration 236 : loss : 0.159398, loss_ce: 0.061847
2022-01-20 19:38:46,152 iteration 237 : loss : 0.147509, loss_ce: 0.054503
2022-01-20 19:38:47,389 iteration 238 : loss : 0.131276, loss_ce: 0.063688
  4%|█                             | 14/400 [05:47<2:35:30, 24.17s/it]2022-01-20 19:38:48,720 iteration 239 : loss : 0.124953, loss_ce: 0.048848
2022-01-20 19:38:50,124 iteration 240 : loss : 0.185339, loss_ce: 0.094260
2022-01-20 19:38:51,422 iteration 241 : loss : 0.190020, loss_ce: 0.061881
2022-01-20 19:38:52,819 iteration 242 : loss : 0.152306, loss_ce: 0.063876
2022-01-20 19:38:54,229 iteration 243 : loss : 0.153752, loss_ce: 0.067010
2022-01-20 19:38:55,627 iteration 244 : loss : 0.153578, loss_ce: 0.065759
2022-01-20 19:38:56,895 iteration 245 : loss : 0.165218, loss_ce: 0.057166
2022-01-20 19:38:58,248 iteration 246 : loss : 0.176257, loss_ce: 0.065153
2022-01-20 19:38:59,533 iteration 247 : loss : 0.126752, loss_ce: 0.057642
2022-01-20 19:39:00,785 iteration 248 : loss : 0.124895, loss_ce: 0.048630
2022-01-20 19:39:02,232 iteration 249 : loss : 0.156715, loss_ce: 0.050131
2022-01-20 19:39:03,637 iteration 250 : loss : 0.158149, loss_ce: 0.069050
2022-01-20 19:39:04,972 iteration 251 : loss : 0.138849, loss_ce: 0.058214
2022-01-20 19:39:06,307 iteration 252 : loss : 0.147521, loss_ce: 0.075181
2022-01-20 19:39:07,597 iteration 253 : loss : 0.177051, loss_ce: 0.056529
2022-01-20 19:39:08,887 iteration 254 : loss : 0.142256, loss_ce: 0.054091
2022-01-20 19:39:08,888 Training Data Eval:
2022-01-20 19:39:15,490   Average segmentation loss on training set: 0.1534
2022-01-20 19:39:15,491 Validation Data Eval:
2022-01-20 19:39:17,746   Average segmentation loss on validation set: 0.2750
2022-01-20 19:39:23,567 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 19:39:24,793 iteration 255 : loss : 0.157622, loss_ce: 0.057810
  4%|█▏                            | 15/400 [06:24<3:00:41, 28.16s/it]2022-01-20 19:39:26,061 iteration 256 : loss : 0.125042, loss_ce: 0.047539
2022-01-20 19:39:27,357 iteration 257 : loss : 0.177560, loss_ce: 0.106721
2022-01-20 19:39:28,815 iteration 258 : loss : 0.157227, loss_ce: 0.054295
2022-01-20 19:39:30,095 iteration 259 : loss : 0.153005, loss_ce: 0.059369
2022-01-20 19:39:31,452 iteration 260 : loss : 0.135338, loss_ce: 0.062870
2022-01-20 19:39:32,835 iteration 261 : loss : 0.152107, loss_ce: 0.067133
2022-01-20 19:39:34,204 iteration 262 : loss : 0.099096, loss_ce: 0.051927
2022-01-20 19:39:35,505 iteration 263 : loss : 0.162738, loss_ce: 0.088997
2022-01-20 19:39:36,800 iteration 264 : loss : 0.156242, loss_ce: 0.059543
2022-01-20 19:39:38,180 iteration 265 : loss : 0.147920, loss_ce: 0.059206
2022-01-20 19:39:39,539 iteration 266 : loss : 0.186500, loss_ce: 0.056393
2022-01-20 19:39:40,864 iteration 267 : loss : 0.153911, loss_ce: 0.064277
2022-01-20 19:39:42,198 iteration 268 : loss : 0.125287, loss_ce: 0.054268
2022-01-20 19:39:43,511 iteration 269 : loss : 0.107208, loss_ce: 0.042908
2022-01-20 19:39:44,766 iteration 270 : loss : 0.141124, loss_ce: 0.051666
2022-01-20 19:39:46,117 iteration 271 : loss : 0.211302, loss_ce: 0.082042
2022-01-20 19:39:47,526 iteration 272 : loss : 0.156698, loss_ce: 0.065028
  4%|█▏                            | 16/400 [06:47<2:49:47, 26.53s/it]2022-01-20 19:39:48,893 iteration 273 : loss : 0.129134, loss_ce: 0.047306
2022-01-20 19:39:50,251 iteration 274 : loss : 0.176932, loss_ce: 0.061547
2022-01-20 19:39:51,586 iteration 275 : loss : 0.109166, loss_ce: 0.049838
2022-01-20 19:39:52,868 iteration 276 : loss : 0.135137, loss_ce: 0.062784
2022-01-20 19:39:54,276 iteration 277 : loss : 0.150888, loss_ce: 0.056953
2022-01-20 19:39:55,544 iteration 278 : loss : 0.255240, loss_ce: 0.094579
2022-01-20 19:39:56,880 iteration 279 : loss : 0.130447, loss_ce: 0.052533
2022-01-20 19:39:58,130 iteration 280 : loss : 0.112617, loss_ce: 0.047148
2022-01-20 19:39:59,474 iteration 281 : loss : 0.137496, loss_ce: 0.047864
2022-01-20 19:40:00,843 iteration 282 : loss : 0.125836, loss_ce: 0.050569
2022-01-20 19:40:02,190 iteration 283 : loss : 0.113228, loss_ce: 0.048813
2022-01-20 19:40:03,474 iteration 284 : loss : 0.088699, loss_ce: 0.031799
2022-01-20 19:40:04,815 iteration 285 : loss : 0.101632, loss_ce: 0.034170
2022-01-20 19:40:06,177 iteration 286 : loss : 0.104745, loss_ce: 0.037639
2022-01-20 19:40:07,463 iteration 287 : loss : 0.136573, loss_ce: 0.046959
2022-01-20 19:40:08,783 iteration 288 : loss : 0.113513, loss_ce: 0.049959
2022-01-20 19:40:10,108 iteration 289 : loss : 0.140220, loss_ce: 0.068570
  4%|█▎                            | 17/400 [07:10<2:41:46, 25.34s/it]2022-01-20 19:40:11,493 iteration 290 : loss : 0.092510, loss_ce: 0.042263
2022-01-20 19:40:12,829 iteration 291 : loss : 0.113056, loss_ce: 0.044761
2022-01-20 19:40:14,164 iteration 292 : loss : 0.097978, loss_ce: 0.039142
2022-01-20 19:40:15,635 iteration 293 : loss : 0.118325, loss_ce: 0.052323
2022-01-20 19:40:16,936 iteration 294 : loss : 0.090293, loss_ce: 0.039231
2022-01-20 19:40:18,238 iteration 295 : loss : 0.184088, loss_ce: 0.067040
2022-01-20 19:40:19,555 iteration 296 : loss : 0.186314, loss_ce: 0.076383
2022-01-20 19:40:20,953 iteration 297 : loss : 0.137098, loss_ce: 0.062011
2022-01-20 19:40:22,252 iteration 298 : loss : 0.120219, loss_ce: 0.038015
2022-01-20 19:40:23,617 iteration 299 : loss : 0.194998, loss_ce: 0.057415
2022-01-20 19:40:24,922 iteration 300 : loss : 0.127135, loss_ce: 0.044014
2022-01-20 19:40:26,256 iteration 301 : loss : 0.147545, loss_ce: 0.050773
2022-01-20 19:40:27,691 iteration 302 : loss : 0.190410, loss_ce: 0.092308
2022-01-20 19:40:29,071 iteration 303 : loss : 0.150273, loss_ce: 0.065359
2022-01-20 19:40:30,399 iteration 304 : loss : 0.105992, loss_ce: 0.042797
2022-01-20 19:40:31,749 iteration 305 : loss : 0.116367, loss_ce: 0.044845
2022-01-20 19:40:33,089 iteration 306 : loss : 0.137627, loss_ce: 0.050433
  4%|█▎                            | 18/400 [07:33<2:36:48, 24.63s/it]2022-01-20 19:40:34,442 iteration 307 : loss : 0.132241, loss_ce: 0.053841
2022-01-20 19:40:35,831 iteration 308 : loss : 0.162104, loss_ce: 0.053929
2022-01-20 19:40:37,182 iteration 309 : loss : 0.141046, loss_ce: 0.064915
2022-01-20 19:40:38,503 iteration 310 : loss : 0.102197, loss_ce: 0.041033
2022-01-20 19:40:39,810 iteration 311 : loss : 0.112688, loss_ce: 0.042649
2022-01-20 19:40:41,166 iteration 312 : loss : 0.184381, loss_ce: 0.048321
2022-01-20 19:40:42,557 iteration 313 : loss : 0.131456, loss_ce: 0.049342
2022-01-20 19:40:43,804 iteration 314 : loss : 0.138567, loss_ce: 0.049377
2022-01-20 19:40:45,119 iteration 315 : loss : 0.125639, loss_ce: 0.056617
2022-01-20 19:40:46,526 iteration 316 : loss : 0.136356, loss_ce: 0.052624
2022-01-20 19:40:47,953 iteration 317 : loss : 0.110662, loss_ce: 0.050231
2022-01-20 19:40:49,290 iteration 318 : loss : 0.114462, loss_ce: 0.053784
2022-01-20 19:40:50,645 iteration 319 : loss : 0.088446, loss_ce: 0.041202
2022-01-20 19:40:51,995 iteration 320 : loss : 0.129113, loss_ce: 0.050887
2022-01-20 19:40:53,304 iteration 321 : loss : 0.139311, loss_ce: 0.058521
2022-01-20 19:40:54,589 iteration 322 : loss : 0.094928, loss_ce: 0.046848
2022-01-20 19:40:55,907 iteration 323 : loss : 0.130813, loss_ce: 0.059173
  5%|█▍                            | 19/400 [07:56<2:32:57, 24.09s/it]2022-01-20 19:40:57,291 iteration 324 : loss : 0.115708, loss_ce: 0.043931
2022-01-20 19:40:58,597 iteration 325 : loss : 0.145165, loss_ce: 0.048811
2022-01-20 19:40:59,895 iteration 326 : loss : 0.143876, loss_ce: 0.061090
2022-01-20 19:41:01,169 iteration 327 : loss : 0.148460, loss_ce: 0.061429
2022-01-20 19:41:02,515 iteration 328 : loss : 0.078013, loss_ce: 0.024084
2022-01-20 19:41:03,912 iteration 329 : loss : 0.129116, loss_ce: 0.048521
2022-01-20 19:41:05,220 iteration 330 : loss : 0.100506, loss_ce: 0.050468
2022-01-20 19:41:06,547 iteration 331 : loss : 0.137885, loss_ce: 0.061183
2022-01-20 19:41:07,795 iteration 332 : loss : 0.108931, loss_ce: 0.051604
2022-01-20 19:41:09,175 iteration 333 : loss : 0.137216, loss_ce: 0.064847
2022-01-20 19:41:10,508 iteration 334 : loss : 0.084349, loss_ce: 0.035264
2022-01-20 19:41:11,905 iteration 335 : loss : 0.132434, loss_ce: 0.064157
2022-01-20 19:41:13,228 iteration 336 : loss : 0.146063, loss_ce: 0.067709
2022-01-20 19:41:14,534 iteration 337 : loss : 0.162871, loss_ce: 0.055347
2022-01-20 19:41:15,865 iteration 338 : loss : 0.095084, loss_ce: 0.036172
2022-01-20 19:41:17,168 iteration 339 : loss : 0.131351, loss_ce: 0.056096
2022-01-20 19:41:17,168 Training Data Eval:
2022-01-20 19:41:23,770   Average segmentation loss on training set: 0.1167
2022-01-20 19:41:23,771 Validation Data Eval:
2022-01-20 19:41:26,027   Average segmentation loss on validation set: 0.1157
2022-01-20 19:41:31,893 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 19:41:33,172 iteration 340 : loss : 0.143218, loss_ce: 0.063186
  5%|█▌                            | 20/400 [08:33<2:57:36, 28.04s/it]2022-01-20 19:41:34,422 iteration 341 : loss : 0.085935, loss_ce: 0.038720
2022-01-20 19:41:35,781 iteration 342 : loss : 0.152285, loss_ce: 0.077062
2022-01-20 19:41:37,154 iteration 343 : loss : 0.130707, loss_ce: 0.054205
2022-01-20 19:41:38,470 iteration 344 : loss : 0.089262, loss_ce: 0.036312
2022-01-20 19:41:39,784 iteration 345 : loss : 0.098804, loss_ce: 0.043175
2022-01-20 19:41:41,164 iteration 346 : loss : 0.094445, loss_ce: 0.040577
2022-01-20 19:41:42,548 iteration 347 : loss : 0.128172, loss_ce: 0.055005
2022-01-20 19:41:43,868 iteration 348 : loss : 0.086222, loss_ce: 0.034493
2022-01-20 19:41:45,196 iteration 349 : loss : 0.112984, loss_ce: 0.040528
2022-01-20 19:41:46,560 iteration 350 : loss : 0.143359, loss_ce: 0.068867
2022-01-20 19:41:47,883 iteration 351 : loss : 0.117342, loss_ce: 0.047578
2022-01-20 19:41:49,312 iteration 352 : loss : 0.109862, loss_ce: 0.059117
2022-01-20 19:41:50,733 iteration 353 : loss : 0.126963, loss_ce: 0.054110
2022-01-20 19:41:52,054 iteration 354 : loss : 0.100831, loss_ce: 0.038685
2022-01-20 19:41:53,426 iteration 355 : loss : 0.079352, loss_ce: 0.025953
2022-01-20 19:41:54,770 iteration 356 : loss : 0.080030, loss_ce: 0.028880
2022-01-20 19:41:56,129 iteration 357 : loss : 0.103399, loss_ce: 0.036944
  5%|█▌                            | 21/400 [08:56<2:47:29, 26.52s/it]2022-01-20 19:41:57,614 iteration 358 : loss : 0.139624, loss_ce: 0.065126
2022-01-20 19:41:59,038 iteration 359 : loss : 0.100466, loss_ce: 0.041658
2022-01-20 19:42:00,365 iteration 360 : loss : 0.081005, loss_ce: 0.027736
2022-01-20 19:42:01,744 iteration 361 : loss : 0.161509, loss_ce: 0.057804
2022-01-20 19:42:03,123 iteration 362 : loss : 0.116238, loss_ce: 0.062062
2022-01-20 19:42:04,400 iteration 363 : loss : 0.093198, loss_ce: 0.042001
2022-01-20 19:42:05,754 iteration 364 : loss : 0.199917, loss_ce: 0.070804
2022-01-20 19:42:07,051 iteration 365 : loss : 0.092607, loss_ce: 0.038242
2022-01-20 19:42:08,375 iteration 366 : loss : 0.128784, loss_ce: 0.037505
2022-01-20 19:42:09,728 iteration 367 : loss : 0.135329, loss_ce: 0.052265
2022-01-20 19:42:11,055 iteration 368 : loss : 0.134751, loss_ce: 0.064589
2022-01-20 19:42:12,358 iteration 369 : loss : 0.082926, loss_ce: 0.033329
2022-01-20 19:42:13,697 iteration 370 : loss : 0.146293, loss_ce: 0.072860
2022-01-20 19:42:15,036 iteration 371 : loss : 0.122116, loss_ce: 0.043764
2022-01-20 19:42:16,332 iteration 372 : loss : 0.122911, loss_ce: 0.048726
2022-01-20 19:42:17,659 iteration 373 : loss : 0.071273, loss_ce: 0.026788
2022-01-20 19:42:19,032 iteration 374 : loss : 0.078010, loss_ce: 0.033223
  6%|█▋                            | 22/400 [09:19<2:40:13, 25.43s/it]2022-01-20 19:42:20,406 iteration 375 : loss : 0.089541, loss_ce: 0.039553
2022-01-20 19:42:21,729 iteration 376 : loss : 0.117752, loss_ce: 0.039391
2022-01-20 19:42:23,071 iteration 377 : loss : 0.095809, loss_ce: 0.030546
2022-01-20 19:42:24,479 iteration 378 : loss : 0.112440, loss_ce: 0.038665
2022-01-20 19:42:25,836 iteration 379 : loss : 0.112874, loss_ce: 0.052038
2022-01-20 19:42:27,146 iteration 380 : loss : 0.116232, loss_ce: 0.053128
2022-01-20 19:42:28,499 iteration 381 : loss : 0.119197, loss_ce: 0.054134
2022-01-20 19:42:29,773 iteration 382 : loss : 0.087043, loss_ce: 0.038482
2022-01-20 19:42:31,126 iteration 383 : loss : 0.159509, loss_ce: 0.033667
2022-01-20 19:42:32,462 iteration 384 : loss : 0.105805, loss_ce: 0.044297
2022-01-20 19:42:33,775 iteration 385 : loss : 0.095500, loss_ce: 0.039366
2022-01-20 19:42:35,119 iteration 386 : loss : 0.110904, loss_ce: 0.039801
2022-01-20 19:42:36,414 iteration 387 : loss : 0.066813, loss_ce: 0.027472
2022-01-20 19:42:37,780 iteration 388 : loss : 0.082713, loss_ce: 0.038899
2022-01-20 19:42:39,105 iteration 389 : loss : 0.105878, loss_ce: 0.037703
2022-01-20 19:42:40,451 iteration 390 : loss : 0.136048, loss_ce: 0.074147
2022-01-20 19:42:41,817 iteration 391 : loss : 0.076522, loss_ce: 0.031776
  6%|█▋                            | 23/400 [09:41<2:34:49, 24.64s/it]2022-01-20 19:42:43,253 iteration 392 : loss : 0.104894, loss_ce: 0.046669
2022-01-20 19:42:44,547 iteration 393 : loss : 0.093250, loss_ce: 0.043991
2022-01-20 19:42:45,893 iteration 394 : loss : 0.126460, loss_ce: 0.043089
2022-01-20 19:42:47,187 iteration 395 : loss : 0.083839, loss_ce: 0.028743
2022-01-20 19:42:48,515 iteration 396 : loss : 0.114055, loss_ce: 0.038814
2022-01-20 19:42:49,939 iteration 397 : loss : 0.109647, loss_ce: 0.053053
2022-01-20 19:42:51,270 iteration 398 : loss : 0.102447, loss_ce: 0.036307
2022-01-20 19:42:52,620 iteration 399 : loss : 0.122013, loss_ce: 0.047265
2022-01-20 19:42:53,939 iteration 400 : loss : 0.086647, loss_ce: 0.034460
2022-01-20 19:42:55,299 iteration 401 : loss : 0.084010, loss_ce: 0.041108
2022-01-20 19:42:56,736 iteration 402 : loss : 0.083438, loss_ce: 0.036128
2022-01-20 19:42:58,118 iteration 403 : loss : 0.111071, loss_ce: 0.043053
2022-01-20 19:42:59,437 iteration 404 : loss : 0.065805, loss_ce: 0.024734
2022-01-20 19:43:00,761 iteration 405 : loss : 0.121721, loss_ce: 0.058296
2022-01-20 19:43:02,140 iteration 406 : loss : 0.085452, loss_ce: 0.034523
2022-01-20 19:43:03,512 iteration 407 : loss : 0.085017, loss_ce: 0.034187
2022-01-20 19:43:04,774 iteration 408 : loss : 0.084424, loss_ce: 0.039192
  6%|█▊                            | 24/400 [10:04<2:31:14, 24.13s/it]2022-01-20 19:43:06,218 iteration 409 : loss : 0.132810, loss_ce: 0.035950
2022-01-20 19:43:07,585 iteration 410 : loss : 0.100051, loss_ce: 0.039984
2022-01-20 19:43:08,959 iteration 411 : loss : 0.130623, loss_ce: 0.038943
2022-01-20 19:43:10,283 iteration 412 : loss : 0.092415, loss_ce: 0.039714
2022-01-20 19:43:11,622 iteration 413 : loss : 0.107198, loss_ce: 0.029133
2022-01-20 19:43:12,906 iteration 414 : loss : 0.111458, loss_ce: 0.044816
2022-01-20 19:43:14,307 iteration 415 : loss : 0.104612, loss_ce: 0.043492
2022-01-20 19:43:15,614 iteration 416 : loss : 0.092229, loss_ce: 0.039624
2022-01-20 19:43:16,984 iteration 417 : loss : 0.089132, loss_ce: 0.032994
2022-01-20 19:43:18,254 iteration 418 : loss : 0.102239, loss_ce: 0.060277
2022-01-20 19:43:19,575 iteration 419 : loss : 0.132756, loss_ce: 0.041397
2022-01-20 19:43:20,916 iteration 420 : loss : 0.083185, loss_ce: 0.035504
2022-01-20 19:43:22,334 iteration 421 : loss : 0.156304, loss_ce: 0.071544
2022-01-20 19:43:23,653 iteration 422 : loss : 0.159832, loss_ce: 0.050514
2022-01-20 19:43:24,997 iteration 423 : loss : 0.104706, loss_ce: 0.039738
2022-01-20 19:43:26,464 iteration 424 : loss : 0.089747, loss_ce: 0.042946
2022-01-20 19:43:26,464 Training Data Eval:
2022-01-20 19:43:33,056   Average segmentation loss on training set: 0.1103
2022-01-20 19:43:33,057 Validation Data Eval:
2022-01-20 19:43:35,317   Average segmentation loss on validation set: 0.1593
2022-01-20 19:43:36,711 iteration 425 : loss : 0.109146, loss_ce: 0.044185
  6%|█▉                            | 25/400 [10:36<2:45:27, 26.47s/it]2022-01-20 19:43:38,143 iteration 426 : loss : 0.116595, loss_ce: 0.053412
2022-01-20 19:43:39,490 iteration 427 : loss : 0.131699, loss_ce: 0.054411
2022-01-20 19:43:40,876 iteration 428 : loss : 0.120614, loss_ce: 0.050301
2022-01-20 19:43:42,186 iteration 429 : loss : 0.100636, loss_ce: 0.039491
2022-01-20 19:43:43,544 iteration 430 : loss : 0.106542, loss_ce: 0.039254
2022-01-20 19:43:44,826 iteration 431 : loss : 0.115188, loss_ce: 0.041480
2022-01-20 19:43:46,137 iteration 432 : loss : 0.057005, loss_ce: 0.021943
2022-01-20 19:43:47,487 iteration 433 : loss : 0.106569, loss_ce: 0.031937
2022-01-20 19:43:48,805 iteration 434 : loss : 0.097009, loss_ce: 0.035340
2022-01-20 19:43:50,130 iteration 435 : loss : 0.114456, loss_ce: 0.056385
2022-01-20 19:43:51,509 iteration 436 : loss : 0.105366, loss_ce: 0.044603
2022-01-20 19:43:52,853 iteration 437 : loss : 0.091004, loss_ce: 0.037566
2022-01-20 19:43:54,193 iteration 438 : loss : 0.124767, loss_ce: 0.064471
2022-01-20 19:43:55,527 iteration 439 : loss : 0.103732, loss_ce: 0.045295
2022-01-20 19:43:56,821 iteration 440 : loss : 0.085759, loss_ce: 0.037502
2022-01-20 19:43:58,142 iteration 441 : loss : 0.091113, loss_ce: 0.044358
2022-01-20 19:43:59,451 iteration 442 : loss : 0.205040, loss_ce: 0.083593
  6%|█▉                            | 26/400 [10:59<2:38:02, 25.35s/it]2022-01-20 19:44:00,839 iteration 443 : loss : 0.091668, loss_ce: 0.044305
2022-01-20 19:44:02,164 iteration 444 : loss : 0.126115, loss_ce: 0.041508
2022-01-20 19:44:03,678 iteration 445 : loss : 0.106821, loss_ce: 0.045776
2022-01-20 19:44:04,964 iteration 446 : loss : 0.138899, loss_ce: 0.050196
2022-01-20 19:44:06,380 iteration 447 : loss : 0.123667, loss_ce: 0.053097
2022-01-20 19:44:07,757 iteration 448 : loss : 0.076600, loss_ce: 0.034579
2022-01-20 19:44:09,081 iteration 449 : loss : 0.113719, loss_ce: 0.043318
2022-01-20 19:44:10,342 iteration 450 : loss : 0.081712, loss_ce: 0.036944
2022-01-20 19:44:11,669 iteration 451 : loss : 0.101344, loss_ce: 0.035824
2022-01-20 19:44:13,027 iteration 452 : loss : 0.077571, loss_ce: 0.033909
2022-01-20 19:44:14,371 iteration 453 : loss : 0.063941, loss_ce: 0.024013
2022-01-20 19:44:15,748 iteration 454 : loss : 0.192322, loss_ce: 0.070929
2022-01-20 19:44:17,161 iteration 455 : loss : 0.063789, loss_ce: 0.020698
2022-01-20 19:44:18,475 iteration 456 : loss : 0.104146, loss_ce: 0.033836
2022-01-20 19:44:19,786 iteration 457 : loss : 0.123450, loss_ce: 0.057015
2022-01-20 19:44:21,118 iteration 458 : loss : 0.125822, loss_ce: 0.061446
2022-01-20 19:44:22,468 iteration 459 : loss : 0.077499, loss_ce: 0.030095
  7%|██                            | 27/400 [11:22<2:33:16, 24.65s/it]2022-01-20 19:44:23,775 iteration 460 : loss : 0.085392, loss_ce: 0.031014
2022-01-20 19:44:25,135 iteration 461 : loss : 0.132699, loss_ce: 0.053724
2022-01-20 19:44:26,478 iteration 462 : loss : 0.139851, loss_ce: 0.039101
2022-01-20 19:44:27,734 iteration 463 : loss : 0.073639, loss_ce: 0.034664
2022-01-20 19:44:29,138 iteration 464 : loss : 0.073616, loss_ce: 0.029569
2022-01-20 19:44:30,460 iteration 465 : loss : 0.085277, loss_ce: 0.047932
2022-01-20 19:44:31,838 iteration 466 : loss : 0.090403, loss_ce: 0.039102
2022-01-20 19:44:33,145 iteration 467 : loss : 0.098525, loss_ce: 0.031554
2022-01-20 19:44:34,470 iteration 468 : loss : 0.090510, loss_ce: 0.033477
2022-01-20 19:44:35,765 iteration 469 : loss : 0.098154, loss_ce: 0.035156
2022-01-20 19:44:37,177 iteration 470 : loss : 0.120138, loss_ce: 0.064518
2022-01-20 19:44:38,479 iteration 471 : loss : 0.077000, loss_ce: 0.033946
2022-01-20 19:44:39,881 iteration 472 : loss : 0.115269, loss_ce: 0.043963
2022-01-20 19:44:41,268 iteration 473 : loss : 0.084140, loss_ce: 0.032384
2022-01-20 19:44:42,672 iteration 474 : loss : 0.093706, loss_ce: 0.034860
2022-01-20 19:44:44,024 iteration 475 : loss : 0.113795, loss_ce: 0.034257
2022-01-20 19:44:45,396 iteration 476 : loss : 0.088828, loss_ce: 0.041543
  7%|██                            | 28/400 [11:45<2:29:37, 24.13s/it]2022-01-20 19:44:46,762 iteration 477 : loss : 0.082958, loss_ce: 0.033103
2022-01-20 19:44:48,145 iteration 478 : loss : 0.111998, loss_ce: 0.044741
2022-01-20 19:44:49,427 iteration 479 : loss : 0.076005, loss_ce: 0.031862
2022-01-20 19:44:50,678 iteration 480 : loss : 0.088247, loss_ce: 0.039447
2022-01-20 19:44:52,001 iteration 481 : loss : 0.076651, loss_ce: 0.033268
2022-01-20 19:44:53,390 iteration 482 : loss : 0.074528, loss_ce: 0.030356
2022-01-20 19:44:54,743 iteration 483 : loss : 0.079114, loss_ce: 0.026365
2022-01-20 19:44:56,077 iteration 484 : loss : 0.080920, loss_ce: 0.033574
2022-01-20 19:44:57,352 iteration 485 : loss : 0.060452, loss_ce: 0.023411
2022-01-20 19:44:58,685 iteration 486 : loss : 0.205558, loss_ce: 0.057368
2022-01-20 19:45:00,057 iteration 487 : loss : 0.112131, loss_ce: 0.067354
2022-01-20 19:45:01,494 iteration 488 : loss : 0.155490, loss_ce: 0.067113
2022-01-20 19:45:02,862 iteration 489 : loss : 0.060985, loss_ce: 0.026006
2022-01-20 19:45:04,160 iteration 490 : loss : 0.082755, loss_ce: 0.035732
2022-01-20 19:45:05,556 iteration 491 : loss : 0.079095, loss_ce: 0.032176
2022-01-20 19:45:06,854 iteration 492 : loss : 0.088005, loss_ce: 0.035124
2022-01-20 19:45:08,174 iteration 493 : loss : 0.112380, loss_ce: 0.050053
  7%|██▏                           | 29/400 [12:08<2:26:42, 23.73s/it]2022-01-20 19:45:09,524 iteration 494 : loss : 0.080228, loss_ce: 0.031119
2022-01-20 19:45:10,856 iteration 495 : loss : 0.068225, loss_ce: 0.028028
2022-01-20 19:45:12,115 iteration 496 : loss : 0.080304, loss_ce: 0.035095
2022-01-20 19:45:13,492 iteration 497 : loss : 0.072908, loss_ce: 0.027570
2022-01-20 19:45:14,818 iteration 498 : loss : 0.118205, loss_ce: 0.042142
2022-01-20 19:45:16,228 iteration 499 : loss : 0.085697, loss_ce: 0.040775
2022-01-20 19:45:17,483 iteration 500 : loss : 0.074635, loss_ce: 0.029059
2022-01-20 19:45:18,830 iteration 501 : loss : 0.076444, loss_ce: 0.025552
2022-01-20 19:45:20,135 iteration 502 : loss : 0.111691, loss_ce: 0.039831
2022-01-20 19:45:21,471 iteration 503 : loss : 0.120456, loss_ce: 0.054248
2022-01-20 19:45:22,886 iteration 504 : loss : 0.077226, loss_ce: 0.031117
2022-01-20 19:45:24,308 iteration 505 : loss : 0.063105, loss_ce: 0.028280
2022-01-20 19:45:25,680 iteration 506 : loss : 0.100547, loss_ce: 0.046871
2022-01-20 19:45:27,014 iteration 507 : loss : 0.086749, loss_ce: 0.031160
2022-01-20 19:45:28,388 iteration 508 : loss : 0.099538, loss_ce: 0.029690
2022-01-20 19:45:29,779 iteration 509 : loss : 0.071127, loss_ce: 0.033445
2022-01-20 19:45:29,780 Training Data Eval:
2022-01-20 19:45:36,372   Average segmentation loss on training set: 0.0784
2022-01-20 19:45:36,373 Validation Data Eval:
2022-01-20 19:45:38,636   Average segmentation loss on validation set: 0.1321
2022-01-20 19:45:39,948 iteration 510 : loss : 0.057151, loss_ce: 0.023066
  8%|██▎                           | 30/400 [12:40<2:41:13, 26.14s/it]2022-01-20 19:45:41,289 iteration 511 : loss : 0.067756, loss_ce: 0.024402
2022-01-20 19:45:42,618 iteration 512 : loss : 0.092317, loss_ce: 0.041230
2022-01-20 19:45:43,964 iteration 513 : loss : 0.102977, loss_ce: 0.041929
2022-01-20 19:45:45,336 iteration 514 : loss : 0.114387, loss_ce: 0.046317
2022-01-20 19:45:46,609 iteration 515 : loss : 0.077057, loss_ce: 0.030201
2022-01-20 19:45:47,843 iteration 516 : loss : 0.055356, loss_ce: 0.020723
2022-01-20 19:45:49,252 iteration 517 : loss : 0.088638, loss_ce: 0.049238
2022-01-20 19:45:50,678 iteration 518 : loss : 0.147422, loss_ce: 0.055472
2022-01-20 19:45:52,101 iteration 519 : loss : 0.078397, loss_ce: 0.035838
2022-01-20 19:45:53,467 iteration 520 : loss : 0.087846, loss_ce: 0.038663
2022-01-20 19:45:54,753 iteration 521 : loss : 0.075186, loss_ce: 0.027050
2022-01-20 19:45:56,148 iteration 522 : loss : 0.105540, loss_ce: 0.040359
2022-01-20 19:45:57,483 iteration 523 : loss : 0.085443, loss_ce: 0.029259
2022-01-20 19:45:58,801 iteration 524 : loss : 0.075913, loss_ce: 0.036701
2022-01-20 19:46:00,131 iteration 525 : loss : 0.069189, loss_ce: 0.027133
2022-01-20 19:46:01,515 iteration 526 : loss : 0.081655, loss_ce: 0.029420
2022-01-20 19:46:02,884 iteration 527 : loss : 0.105046, loss_ce: 0.036016
  8%|██▎                           | 31/400 [13:03<2:34:51, 25.18s/it]2022-01-20 19:46:04,242 iteration 528 : loss : 0.058735, loss_ce: 0.023216
2022-01-20 19:46:05,687 iteration 529 : loss : 0.134656, loss_ce: 0.036824
2022-01-20 19:46:07,003 iteration 530 : loss : 0.064588, loss_ce: 0.024768
2022-01-20 19:46:08,297 iteration 531 : loss : 0.074899, loss_ce: 0.027208
2022-01-20 19:46:09,654 iteration 532 : loss : 0.084500, loss_ce: 0.034167
2022-01-20 19:46:10,994 iteration 533 : loss : 0.061224, loss_ce: 0.024101
2022-01-20 19:46:12,321 iteration 534 : loss : 0.077729, loss_ce: 0.029959
2022-01-20 19:46:13,697 iteration 535 : loss : 0.071025, loss_ce: 0.026688
2022-01-20 19:46:15,046 iteration 536 : loss : 0.065553, loss_ce: 0.030172
2022-01-20 19:46:16,369 iteration 537 : loss : 0.064735, loss_ce: 0.026049
2022-01-20 19:46:17,715 iteration 538 : loss : 0.089710, loss_ce: 0.047265
2022-01-20 19:46:18,986 iteration 539 : loss : 0.072478, loss_ce: 0.027708
2022-01-20 19:46:20,447 iteration 540 : loss : 0.106074, loss_ce: 0.055711
2022-01-20 19:46:21,883 iteration 541 : loss : 0.120496, loss_ce: 0.059531
2022-01-20 19:46:23,174 iteration 542 : loss : 0.113142, loss_ce: 0.049223
2022-01-20 19:46:24,519 iteration 543 : loss : 0.082193, loss_ce: 0.034170
2022-01-20 19:46:25,842 iteration 544 : loss : 0.140398, loss_ce: 0.043928
  8%|██▍                           | 32/400 [13:26<2:30:21, 24.51s/it]2022-01-20 19:46:27,286 iteration 545 : loss : 0.081873, loss_ce: 0.034162
2022-01-20 19:46:28,697 iteration 546 : loss : 0.067274, loss_ce: 0.030644
2022-01-20 19:46:30,101 iteration 547 : loss : 0.073914, loss_ce: 0.032162
2022-01-20 19:46:31,391 iteration 548 : loss : 0.108096, loss_ce: 0.045282
2022-01-20 19:46:32,684 iteration 549 : loss : 0.131815, loss_ce: 0.052124
2022-01-20 19:46:33,979 iteration 550 : loss : 0.107917, loss_ce: 0.042303
2022-01-20 19:46:35,272 iteration 551 : loss : 0.080816, loss_ce: 0.022512
2022-01-20 19:46:36,654 iteration 552 : loss : 0.130153, loss_ce: 0.057264
2022-01-20 19:46:38,024 iteration 553 : loss : 0.069625, loss_ce: 0.026481
2022-01-20 19:46:39,387 iteration 554 : loss : 0.079097, loss_ce: 0.027825
2022-01-20 19:46:40,779 iteration 555 : loss : 0.075587, loss_ce: 0.021944
2022-01-20 19:46:42,205 iteration 556 : loss : 0.091354, loss_ce: 0.040950
2022-01-20 19:46:43,511 iteration 557 : loss : 0.075145, loss_ce: 0.035918
2022-01-20 19:46:44,797 iteration 558 : loss : 0.081340, loss_ce: 0.033771
2022-01-20 19:46:46,120 iteration 559 : loss : 0.094575, loss_ce: 0.041317
2022-01-20 19:46:47,530 iteration 560 : loss : 0.061392, loss_ce: 0.028667
2022-01-20 19:46:48,923 iteration 561 : loss : 0.061693, loss_ce: 0.026800
  8%|██▍                           | 33/400 [13:49<2:27:18, 24.08s/it]2022-01-20 19:46:50,346 iteration 562 : loss : 0.099151, loss_ce: 0.035142
2022-01-20 19:46:51,638 iteration 563 : loss : 0.054570, loss_ce: 0.027253
2022-01-20 19:46:52,933 iteration 564 : loss : 0.078052, loss_ce: 0.040925
2022-01-20 19:46:54,306 iteration 565 : loss : 0.066283, loss_ce: 0.030221
2022-01-20 19:46:55,657 iteration 566 : loss : 0.098811, loss_ce: 0.031011
2022-01-20 19:46:57,024 iteration 567 : loss : 0.090923, loss_ce: 0.034390
2022-01-20 19:46:58,387 iteration 568 : loss : 0.084140, loss_ce: 0.023601
2022-01-20 19:46:59,807 iteration 569 : loss : 0.161902, loss_ce: 0.052684
2022-01-20 19:47:01,164 iteration 570 : loss : 0.075717, loss_ce: 0.032159
2022-01-20 19:47:02,544 iteration 571 : loss : 0.078174, loss_ce: 0.033388
2022-01-20 19:47:03,944 iteration 572 : loss : 0.088310, loss_ce: 0.038722
2022-01-20 19:47:05,253 iteration 573 : loss : 0.095088, loss_ce: 0.040522
2022-01-20 19:47:06,672 iteration 574 : loss : 0.076891, loss_ce: 0.029589
2022-01-20 19:47:08,004 iteration 575 : loss : 0.129545, loss_ce: 0.041686
2022-01-20 19:47:09,285 iteration 576 : loss : 0.090126, loss_ce: 0.042690
2022-01-20 19:47:10,745 iteration 577 : loss : 0.126115, loss_ce: 0.061091
2022-01-20 19:47:12,124 iteration 578 : loss : 0.075758, loss_ce: 0.037289
  8%|██▌                           | 34/400 [14:12<2:25:16, 23.82s/it]2022-01-20 19:47:13,443 iteration 579 : loss : 0.070912, loss_ce: 0.035094
2022-01-20 19:47:14,835 iteration 580 : loss : 0.098276, loss_ce: 0.038229
2022-01-20 19:47:16,320 iteration 581 : loss : 0.052763, loss_ce: 0.022614
2022-01-20 19:47:17,611 iteration 582 : loss : 0.069624, loss_ce: 0.033515
2022-01-20 19:47:18,967 iteration 583 : loss : 0.097882, loss_ce: 0.047245
2022-01-20 19:47:20,349 iteration 584 : loss : 0.103271, loss_ce: 0.049136
2022-01-20 19:47:21,663 iteration 585 : loss : 0.120913, loss_ce: 0.043170
2022-01-20 19:47:23,040 iteration 586 : loss : 0.083468, loss_ce: 0.033557
2022-01-20 19:47:24,400 iteration 587 : loss : 0.067695, loss_ce: 0.033513
2022-01-20 19:47:25,700 iteration 588 : loss : 0.094719, loss_ce: 0.035945
2022-01-20 19:47:27,101 iteration 589 : loss : 0.099539, loss_ce: 0.051172
2022-01-20 19:47:28,428 iteration 590 : loss : 0.076652, loss_ce: 0.029098
2022-01-20 19:47:29,756 iteration 591 : loss : 0.090659, loss_ce: 0.031174
2022-01-20 19:47:31,130 iteration 592 : loss : 0.073224, loss_ce: 0.030388
2022-01-20 19:47:32,416 iteration 593 : loss : 0.092339, loss_ce: 0.036797
2022-01-20 19:47:33,804 iteration 594 : loss : 0.092648, loss_ce: 0.035210
2022-01-20 19:47:33,804 Training Data Eval:
2022-01-20 19:47:40,407   Average segmentation loss on training set: 0.0653
2022-01-20 19:47:40,408 Validation Data Eval:
2022-01-20 19:47:42,671   Average segmentation loss on validation set: 0.1036
2022-01-20 19:47:48,527 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 19:47:49,776 iteration 595 : loss : 0.101932, loss_ce: 0.040721
  9%|██▋                           | 35/400 [14:49<2:50:08, 27.97s/it]2022-01-20 19:47:51,115 iteration 596 : loss : 0.112168, loss_ce: 0.045660
2022-01-20 19:47:52,389 iteration 597 : loss : 0.048552, loss_ce: 0.020391
2022-01-20 19:47:53,726 iteration 598 : loss : 0.071642, loss_ce: 0.025132
2022-01-20 19:47:55,097 iteration 599 : loss : 0.160926, loss_ce: 0.053643
2022-01-20 19:47:56,483 iteration 600 : loss : 0.059483, loss_ce: 0.019775
2022-01-20 19:47:57,863 iteration 601 : loss : 0.061359, loss_ce: 0.024505
2022-01-20 19:47:59,122 iteration 602 : loss : 0.088891, loss_ce: 0.045413
2022-01-20 19:48:00,455 iteration 603 : loss : 0.097720, loss_ce: 0.045217
2022-01-20 19:48:01,751 iteration 604 : loss : 0.137578, loss_ce: 0.039407
2022-01-20 19:48:03,190 iteration 605 : loss : 0.080778, loss_ce: 0.034461
2022-01-20 19:48:04,493 iteration 606 : loss : 0.055272, loss_ce: 0.022932
2022-01-20 19:48:05,793 iteration 607 : loss : 0.079306, loss_ce: 0.026682
2022-01-20 19:48:07,090 iteration 608 : loss : 0.047111, loss_ce: 0.023705
2022-01-20 19:48:08,385 iteration 609 : loss : 0.077182, loss_ce: 0.028854
2022-01-20 19:48:09,813 iteration 610 : loss : 0.091933, loss_ce: 0.042565
2022-01-20 19:48:11,142 iteration 611 : loss : 0.079362, loss_ce: 0.034997
2022-01-20 19:48:12,421 iteration 612 : loss : 0.057075, loss_ce: 0.024541
  9%|██▋                           | 36/400 [15:12<2:39:58, 26.37s/it]2022-01-20 19:48:13,795 iteration 613 : loss : 0.084056, loss_ce: 0.032581
2022-01-20 19:48:15,186 iteration 614 : loss : 0.068086, loss_ce: 0.029634
2022-01-20 19:48:16,534 iteration 615 : loss : 0.070520, loss_ce: 0.034043
2022-01-20 19:48:17,850 iteration 616 : loss : 0.126713, loss_ce: 0.037471
2022-01-20 19:48:19,234 iteration 617 : loss : 0.122913, loss_ce: 0.046892
2022-01-20 19:48:20,657 iteration 618 : loss : 0.050550, loss_ce: 0.025006
2022-01-20 19:48:22,035 iteration 619 : loss : 0.072743, loss_ce: 0.030077
2022-01-20 19:48:23,438 iteration 620 : loss : 0.063546, loss_ce: 0.026126
2022-01-20 19:48:24,701 iteration 621 : loss : 0.054473, loss_ce: 0.017413
2022-01-20 19:48:26,114 iteration 622 : loss : 0.087657, loss_ce: 0.034270
2022-01-20 19:48:27,529 iteration 623 : loss : 0.083769, loss_ce: 0.032400
2022-01-20 19:48:28,803 iteration 624 : loss : 0.086069, loss_ce: 0.039255
2022-01-20 19:48:30,264 iteration 625 : loss : 0.078191, loss_ce: 0.035751
2022-01-20 19:48:31,596 iteration 626 : loss : 0.059404, loss_ce: 0.025194
2022-01-20 19:48:32,877 iteration 627 : loss : 0.100520, loss_ce: 0.029702
2022-01-20 19:48:34,132 iteration 628 : loss : 0.088687, loss_ce: 0.031075
2022-01-20 19:48:35,491 iteration 629 : loss : 0.047242, loss_ce: 0.021652
  9%|██▊                           | 37/400 [15:35<2:33:33, 25.38s/it]2022-01-20 19:48:36,778 iteration 630 : loss : 0.088594, loss_ce: 0.037744
2022-01-20 19:48:38,112 iteration 631 : loss : 0.084376, loss_ce: 0.029217
2022-01-20 19:48:39,387 iteration 632 : loss : 0.057274, loss_ce: 0.021145
2022-01-20 19:48:40,758 iteration 633 : loss : 0.105393, loss_ce: 0.058556
2022-01-20 19:48:42,116 iteration 634 : loss : 0.052150, loss_ce: 0.023844
2022-01-20 19:48:43,489 iteration 635 : loss : 0.090492, loss_ce: 0.037560
2022-01-20 19:48:44,807 iteration 636 : loss : 0.122464, loss_ce: 0.036287
2022-01-20 19:48:46,112 iteration 637 : loss : 0.076876, loss_ce: 0.033803
2022-01-20 19:48:47,414 iteration 638 : loss : 0.058971, loss_ce: 0.023923
2022-01-20 19:48:48,798 iteration 639 : loss : 0.066402, loss_ce: 0.029675
2022-01-20 19:48:50,185 iteration 640 : loss : 0.084462, loss_ce: 0.030459
2022-01-20 19:48:51,479 iteration 641 : loss : 0.056889, loss_ce: 0.022673
2022-01-20 19:48:52,829 iteration 642 : loss : 0.070327, loss_ce: 0.026360
2022-01-20 19:48:54,151 iteration 643 : loss : 0.069613, loss_ce: 0.023079
2022-01-20 19:48:55,485 iteration 644 : loss : 0.086220, loss_ce: 0.026712
2022-01-20 19:48:56,792 iteration 645 : loss : 0.089598, loss_ce: 0.034500
2022-01-20 19:48:58,169 iteration 646 : loss : 0.063892, loss_ce: 0.027238
 10%|██▊                           | 38/400 [15:58<2:28:15, 24.57s/it]2022-01-20 19:48:59,520 iteration 647 : loss : 0.073099, loss_ce: 0.028711
2022-01-20 19:49:00,915 iteration 648 : loss : 0.096243, loss_ce: 0.037560
2022-01-20 19:49:02,278 iteration 649 : loss : 0.067713, loss_ce: 0.026905
2022-01-20 19:49:03,553 iteration 650 : loss : 0.066464, loss_ce: 0.032666
2022-01-20 19:49:04,898 iteration 651 : loss : 0.076366, loss_ce: 0.032462
2022-01-20 19:49:06,182 iteration 652 : loss : 0.088229, loss_ce: 0.029708
2022-01-20 19:49:07,534 iteration 653 : loss : 0.099980, loss_ce: 0.028564
2022-01-20 19:49:08,941 iteration 654 : loss : 0.107261, loss_ce: 0.033562
2022-01-20 19:49:10,309 iteration 655 : loss : 0.055322, loss_ce: 0.020771
2022-01-20 19:49:11,593 iteration 656 : loss : 0.072052, loss_ce: 0.025721
2022-01-20 19:49:12,935 iteration 657 : loss : 0.068498, loss_ce: 0.027910
2022-01-20 19:49:14,330 iteration 658 : loss : 0.129498, loss_ce: 0.040345
2022-01-20 19:49:15,728 iteration 659 : loss : 0.130251, loss_ce: 0.062630
2022-01-20 19:49:17,100 iteration 660 : loss : 0.097092, loss_ce: 0.037616
2022-01-20 19:49:18,456 iteration 661 : loss : 0.079733, loss_ce: 0.032741
2022-01-20 19:49:19,758 iteration 662 : loss : 0.067797, loss_ce: 0.033101
2022-01-20 19:49:21,091 iteration 663 : loss : 0.072415, loss_ce: 0.023714
 10%|██▉                           | 39/400 [16:21<2:24:51, 24.08s/it]2022-01-20 19:49:22,432 iteration 664 : loss : 0.113700, loss_ce: 0.072641
2022-01-20 19:49:23,758 iteration 665 : loss : 0.083634, loss_ce: 0.038997
2022-01-20 19:49:25,112 iteration 666 : loss : 0.104691, loss_ce: 0.037148
2022-01-20 19:49:26,410 iteration 667 : loss : 0.061040, loss_ce: 0.024873
2022-01-20 19:49:27,819 iteration 668 : loss : 0.075090, loss_ce: 0.028663
2022-01-20 19:49:29,129 iteration 669 : loss : 0.076734, loss_ce: 0.032300
2022-01-20 19:49:30,440 iteration 670 : loss : 0.055260, loss_ce: 0.023910
2022-01-20 19:49:31,824 iteration 671 : loss : 0.079111, loss_ce: 0.025331
2022-01-20 19:49:33,216 iteration 672 : loss : 0.089680, loss_ce: 0.031857
2022-01-20 19:49:34,510 iteration 673 : loss : 0.073826, loss_ce: 0.033859
2022-01-20 19:49:35,876 iteration 674 : loss : 0.092156, loss_ce: 0.039968
2022-01-20 19:49:37,258 iteration 675 : loss : 0.064488, loss_ce: 0.024107
2022-01-20 19:49:38,577 iteration 676 : loss : 0.071832, loss_ce: 0.020905
2022-01-20 19:49:39,930 iteration 677 : loss : 0.077102, loss_ce: 0.033157
2022-01-20 19:49:41,263 iteration 678 : loss : 0.054815, loss_ce: 0.019675
2022-01-20 19:49:42,556 iteration 679 : loss : 0.100971, loss_ce: 0.050859
2022-01-20 19:49:42,557 Training Data Eval:
2022-01-20 19:49:49,161   Average segmentation loss on training set: 0.0555
2022-01-20 19:49:49,162 Validation Data Eval:
2022-01-20 19:49:51,427   Average segmentation loss on validation set: 0.1147
2022-01-20 19:49:52,784 iteration 680 : loss : 0.136623, loss_ce: 0.031233
 10%|███                           | 40/400 [16:52<2:38:09, 26.36s/it]2022-01-20 19:49:54,220 iteration 681 : loss : 0.088136, loss_ce: 0.035326
2022-01-20 19:49:55,525 iteration 682 : loss : 0.041483, loss_ce: 0.015933
2022-01-20 19:49:56,888 iteration 683 : loss : 0.077979, loss_ce: 0.033490
2022-01-20 19:49:58,260 iteration 684 : loss : 0.064488, loss_ce: 0.025592
2022-01-20 19:49:59,656 iteration 685 : loss : 0.062153, loss_ce: 0.025207
2022-01-20 19:50:00,910 iteration 686 : loss : 0.070819, loss_ce: 0.023987
2022-01-20 19:50:02,210 iteration 687 : loss : 0.053074, loss_ce: 0.019479
2022-01-20 19:50:03,614 iteration 688 : loss : 0.095243, loss_ce: 0.040937
2022-01-20 19:50:04,964 iteration 689 : loss : 0.059528, loss_ce: 0.025866
2022-01-20 19:50:06,256 iteration 690 : loss : 0.103725, loss_ce: 0.063714
2022-01-20 19:50:07,677 iteration 691 : loss : 0.064303, loss_ce: 0.028108
2022-01-20 19:50:09,079 iteration 692 : loss : 0.082684, loss_ce: 0.030115
2022-01-20 19:50:10,466 iteration 693 : loss : 0.082614, loss_ce: 0.032486
2022-01-20 19:50:11,817 iteration 694 : loss : 0.076151, loss_ce: 0.030111
2022-01-20 19:50:13,159 iteration 695 : loss : 0.078778, loss_ce: 0.024599
2022-01-20 19:50:14,488 iteration 696 : loss : 0.073223, loss_ce: 0.030574
2022-01-20 19:50:15,837 iteration 697 : loss : 0.110370, loss_ce: 0.048584
 10%|███                           | 41/400 [17:16<2:31:47, 25.37s/it]2022-01-20 19:50:17,198 iteration 698 : loss : 0.074404, loss_ce: 0.028682
2022-01-20 19:50:18,540 iteration 699 : loss : 0.067735, loss_ce: 0.020113
2022-01-20 19:50:19,895 iteration 700 : loss : 0.083583, loss_ce: 0.027760
2022-01-20 19:50:21,224 iteration 701 : loss : 0.088692, loss_ce: 0.031470
2022-01-20 19:50:22,608 iteration 702 : loss : 0.084769, loss_ce: 0.026017
2022-01-20 19:50:23,953 iteration 703 : loss : 0.072550, loss_ce: 0.026858
2022-01-20 19:50:25,291 iteration 704 : loss : 0.087887, loss_ce: 0.025481
2022-01-20 19:50:26,600 iteration 705 : loss : 0.078161, loss_ce: 0.037273
2022-01-20 19:50:27,999 iteration 706 : loss : 0.052957, loss_ce: 0.023474
2022-01-20 19:50:29,372 iteration 707 : loss : 0.081324, loss_ce: 0.039767
2022-01-20 19:50:30,727 iteration 708 : loss : 0.084569, loss_ce: 0.033269
2022-01-20 19:50:32,100 iteration 709 : loss : 0.064489, loss_ce: 0.029071
2022-01-20 19:50:33,455 iteration 710 : loss : 0.073258, loss_ce: 0.026541
2022-01-20 19:50:34,843 iteration 711 : loss : 0.126602, loss_ce: 0.041022
2022-01-20 19:50:36,246 iteration 712 : loss : 0.057332, loss_ce: 0.030804
2022-01-20 19:50:37,551 iteration 713 : loss : 0.068943, loss_ce: 0.028037
2022-01-20 19:50:38,886 iteration 714 : loss : 0.058881, loss_ce: 0.025957
 10%|███▏                          | 42/400 [17:39<2:27:13, 24.67s/it]2022-01-20 19:50:40,260 iteration 715 : loss : 0.050418, loss_ce: 0.023193
2022-01-20 19:50:41,589 iteration 716 : loss : 0.102376, loss_ce: 0.028637
2022-01-20 19:50:42,910 iteration 717 : loss : 0.061639, loss_ce: 0.025946
2022-01-20 19:50:44,156 iteration 718 : loss : 0.065272, loss_ce: 0.024544
2022-01-20 19:50:45,489 iteration 719 : loss : 0.073635, loss_ce: 0.039358
2022-01-20 19:50:46,747 iteration 720 : loss : 0.067089, loss_ce: 0.028993
2022-01-20 19:50:48,206 iteration 721 : loss : 0.101737, loss_ce: 0.043140
2022-01-20 19:50:49,550 iteration 722 : loss : 0.054178, loss_ce: 0.025421
2022-01-20 19:50:50,906 iteration 723 : loss : 0.075274, loss_ce: 0.027384
2022-01-20 19:50:52,321 iteration 724 : loss : 0.070224, loss_ce: 0.026223
2022-01-20 19:50:53,618 iteration 725 : loss : 0.056548, loss_ce: 0.023842
2022-01-20 19:50:54,971 iteration 726 : loss : 0.105937, loss_ce: 0.063740
2022-01-20 19:50:56,279 iteration 727 : loss : 0.049813, loss_ce: 0.019483
2022-01-20 19:50:57,607 iteration 728 : loss : 0.076595, loss_ce: 0.022464
2022-01-20 19:50:58,928 iteration 729 : loss : 0.101881, loss_ce: 0.043905
2022-01-20 19:51:00,290 iteration 730 : loss : 0.067489, loss_ce: 0.023578
2022-01-20 19:51:01,598 iteration 731 : loss : 0.099942, loss_ce: 0.036403
 11%|███▏                          | 43/400 [18:01<2:23:18, 24.08s/it]2022-01-20 19:51:03,021 iteration 732 : loss : 0.058806, loss_ce: 0.024087
2022-01-20 19:51:04,392 iteration 733 : loss : 0.064294, loss_ce: 0.030950
2022-01-20 19:51:05,754 iteration 734 : loss : 0.112611, loss_ce: 0.047957
2022-01-20 19:51:07,132 iteration 735 : loss : 0.076261, loss_ce: 0.026846
2022-01-20 19:51:08,504 iteration 736 : loss : 0.060017, loss_ce: 0.023513
2022-01-20 19:51:09,789 iteration 737 : loss : 0.060787, loss_ce: 0.026302
2022-01-20 19:51:11,190 iteration 738 : loss : 0.072025, loss_ce: 0.030737
2022-01-20 19:51:12,558 iteration 739 : loss : 0.082924, loss_ce: 0.041037
2022-01-20 19:51:13,899 iteration 740 : loss : 0.076443, loss_ce: 0.023332
2022-01-20 19:51:15,272 iteration 741 : loss : 0.083315, loss_ce: 0.029332
2022-01-20 19:51:16,642 iteration 742 : loss : 0.082792, loss_ce: 0.030296
2022-01-20 19:51:17,971 iteration 743 : loss : 0.083038, loss_ce: 0.030133
2022-01-20 19:51:19,328 iteration 744 : loss : 0.059673, loss_ce: 0.024287
2022-01-20 19:51:20,670 iteration 745 : loss : 0.085058, loss_ce: 0.029950
2022-01-20 19:51:21,987 iteration 746 : loss : 0.144316, loss_ce: 0.049970
2022-01-20 19:51:23,309 iteration 747 : loss : 0.074028, loss_ce: 0.035300
2022-01-20 19:51:24,670 iteration 748 : loss : 0.088361, loss_ce: 0.035384
 11%|███▎                          | 44/400 [18:24<2:21:04, 23.78s/it]2022-01-20 19:51:26,022 iteration 749 : loss : 0.092586, loss_ce: 0.030661
2022-01-20 19:51:27,340 iteration 750 : loss : 0.052675, loss_ce: 0.016508
2022-01-20 19:51:28,719 iteration 751 : loss : 0.071403, loss_ce: 0.027123
2022-01-20 19:51:30,032 iteration 752 : loss : 0.067537, loss_ce: 0.027334
2022-01-20 19:51:31,403 iteration 753 : loss : 0.058532, loss_ce: 0.022042
2022-01-20 19:51:32,741 iteration 754 : loss : 0.046570, loss_ce: 0.015505
2022-01-20 19:51:34,097 iteration 755 : loss : 0.054866, loss_ce: 0.023348
2022-01-20 19:51:35,355 iteration 756 : loss : 0.063264, loss_ce: 0.026396
2022-01-20 19:51:36,631 iteration 757 : loss : 0.065296, loss_ce: 0.023180
2022-01-20 19:51:37,975 iteration 758 : loss : 0.072757, loss_ce: 0.026649
2022-01-20 19:51:39,357 iteration 759 : loss : 0.132505, loss_ce: 0.044329
2022-01-20 19:51:40,709 iteration 760 : loss : 0.062038, loss_ce: 0.018614
2022-01-20 19:51:42,084 iteration 761 : loss : 0.077009, loss_ce: 0.026182
2022-01-20 19:51:43,347 iteration 762 : loss : 0.065498, loss_ce: 0.029808
2022-01-20 19:51:44,681 iteration 763 : loss : 0.048428, loss_ce: 0.021887
2022-01-20 19:51:45,945 iteration 764 : loss : 0.074741, loss_ce: 0.024597
2022-01-20 19:51:45,945 Training Data Eval:
2022-01-20 19:51:52,539   Average segmentation loss on training set: 0.0589
2022-01-20 19:51:52,539 Validation Data Eval:
2022-01-20 19:51:54,801   Average segmentation loss on validation set: 0.1241
2022-01-20 19:51:56,053 iteration 765 : loss : 0.057796, loss_ce: 0.024674
 11%|███▍                          | 45/400 [18:56<2:34:11, 26.06s/it]2022-01-20 19:51:57,502 iteration 766 : loss : 0.062866, loss_ce: 0.023822
2022-01-20 19:51:58,978 iteration 767 : loss : 0.078823, loss_ce: 0.031896
2022-01-20 19:52:00,352 iteration 768 : loss : 0.073675, loss_ce: 0.028124
2022-01-20 19:52:01,741 iteration 769 : loss : 0.075528, loss_ce: 0.035130
2022-01-20 19:52:03,173 iteration 770 : loss : 0.076237, loss_ce: 0.040474
2022-01-20 19:52:04,548 iteration 771 : loss : 0.056782, loss_ce: 0.028998
2022-01-20 19:52:05,832 iteration 772 : loss : 0.054523, loss_ce: 0.023328
2022-01-20 19:52:07,176 iteration 773 : loss : 0.124760, loss_ce: 0.049337
2022-01-20 19:52:08,595 iteration 774 : loss : 0.051792, loss_ce: 0.021770
2022-01-20 19:52:09,879 iteration 775 : loss : 0.064897, loss_ce: 0.025207
2022-01-20 19:52:11,200 iteration 776 : loss : 0.120595, loss_ce: 0.037934
2022-01-20 19:52:12,659 iteration 777 : loss : 0.059506, loss_ce: 0.028233
2022-01-20 19:52:14,020 iteration 778 : loss : 0.094663, loss_ce: 0.043650
2022-01-20 19:52:15,366 iteration 779 : loss : 0.129210, loss_ce: 0.055727
2022-01-20 19:52:16,806 iteration 780 : loss : 0.114994, loss_ce: 0.048821
2022-01-20 19:52:18,193 iteration 781 : loss : 0.049224, loss_ce: 0.018585
2022-01-20 19:52:19,512 iteration 782 : loss : 0.060677, loss_ce: 0.024333
 12%|███▍                          | 46/400 [19:19<2:29:09, 25.28s/it]2022-01-20 19:52:20,885 iteration 783 : loss : 0.081630, loss_ce: 0.044994
2022-01-20 19:52:22,175 iteration 784 : loss : 0.055242, loss_ce: 0.021326
2022-01-20 19:52:23,576 iteration 785 : loss : 0.095546, loss_ce: 0.032589
2022-01-20 19:52:24,901 iteration 786 : loss : 0.068543, loss_ce: 0.026635
2022-01-20 19:52:26,286 iteration 787 : loss : 0.115502, loss_ce: 0.049418
2022-01-20 19:52:27,592 iteration 788 : loss : 0.079910, loss_ce: 0.035347
2022-01-20 19:52:28,976 iteration 789 : loss : 0.089742, loss_ce: 0.034449
2022-01-20 19:52:30,376 iteration 790 : loss : 0.083554, loss_ce: 0.043078
2022-01-20 19:52:31,659 iteration 791 : loss : 0.067441, loss_ce: 0.034788
2022-01-20 19:52:33,063 iteration 792 : loss : 0.099606, loss_ce: 0.041447
2022-01-20 19:52:34,440 iteration 793 : loss : 0.081934, loss_ce: 0.034029
2022-01-20 19:52:35,830 iteration 794 : loss : 0.051838, loss_ce: 0.022195
2022-01-20 19:52:37,205 iteration 795 : loss : 0.055479, loss_ce: 0.022050
2022-01-20 19:52:38,646 iteration 796 : loss : 0.071275, loss_ce: 0.027124
2022-01-20 19:52:39,998 iteration 797 : loss : 0.060257, loss_ce: 0.027021
2022-01-20 19:52:41,338 iteration 798 : loss : 0.068156, loss_ce: 0.029885
2022-01-20 19:52:42,798 iteration 799 : loss : 0.137666, loss_ce: 0.044051
 12%|███▌                          | 47/400 [19:42<2:25:12, 24.68s/it]2022-01-20 19:52:44,206 iteration 800 : loss : 0.075361, loss_ce: 0.037063
2022-01-20 19:52:45,533 iteration 801 : loss : 0.072277, loss_ce: 0.020358
2022-01-20 19:52:46,913 iteration 802 : loss : 0.086350, loss_ce: 0.043867
2022-01-20 19:52:48,184 iteration 803 : loss : 0.052017, loss_ce: 0.019851
2022-01-20 19:52:49,611 iteration 804 : loss : 0.076865, loss_ce: 0.026481
2022-01-20 19:52:50,933 iteration 805 : loss : 0.064805, loss_ce: 0.027775
2022-01-20 19:52:52,202 iteration 806 : loss : 0.068088, loss_ce: 0.025358
2022-01-20 19:52:53,536 iteration 807 : loss : 0.052849, loss_ce: 0.027398
2022-01-20 19:52:54,871 iteration 808 : loss : 0.074983, loss_ce: 0.026478
2022-01-20 19:52:56,264 iteration 809 : loss : 0.072955, loss_ce: 0.028794
2022-01-20 19:52:57,564 iteration 810 : loss : 0.048628, loss_ce: 0.017545
2022-01-20 19:52:58,933 iteration 811 : loss : 0.084382, loss_ce: 0.039113
2022-01-20 19:53:00,226 iteration 812 : loss : 0.070347, loss_ce: 0.033504
2022-01-20 19:53:01,631 iteration 813 : loss : 0.065255, loss_ce: 0.027147
2022-01-20 19:53:03,070 iteration 814 : loss : 0.161304, loss_ce: 0.046929
2022-01-20 19:53:04,286 iteration 815 : loss : 0.064268, loss_ce: 0.029945
2022-01-20 19:53:05,688 iteration 816 : loss : 0.092818, loss_ce: 0.034583
 12%|███▌                          | 48/400 [20:05<2:21:38, 24.14s/it]2022-01-20 19:53:07,029 iteration 817 : loss : 0.058321, loss_ce: 0.023576
2022-01-20 19:53:08,352 iteration 818 : loss : 0.072003, loss_ce: 0.031381
2022-01-20 19:53:09,830 iteration 819 : loss : 0.088557, loss_ce: 0.036957
2022-01-20 19:53:11,246 iteration 820 : loss : 0.081405, loss_ce: 0.031142
2022-01-20 19:53:12,624 iteration 821 : loss : 0.077033, loss_ce: 0.032937
2022-01-20 19:53:13,974 iteration 822 : loss : 0.060088, loss_ce: 0.028272
2022-01-20 19:53:15,347 iteration 823 : loss : 0.065682, loss_ce: 0.026255
2022-01-20 19:53:16,638 iteration 824 : loss : 0.066929, loss_ce: 0.025455
2022-01-20 19:53:17,972 iteration 825 : loss : 0.132347, loss_ce: 0.037858
2022-01-20 19:53:19,252 iteration 826 : loss : 0.056188, loss_ce: 0.020315
2022-01-20 19:53:20,586 iteration 827 : loss : 0.082580, loss_ce: 0.030084
2022-01-20 19:53:21,857 iteration 828 : loss : 0.057383, loss_ce: 0.021279
2022-01-20 19:53:23,279 iteration 829 : loss : 0.087242, loss_ce: 0.040384
2022-01-20 19:53:24,614 iteration 830 : loss : 0.081769, loss_ce: 0.036889
2022-01-20 19:53:25,985 iteration 831 : loss : 0.072721, loss_ce: 0.031108
2022-01-20 19:53:27,347 iteration 832 : loss : 0.067727, loss_ce: 0.020246
2022-01-20 19:53:28,706 iteration 833 : loss : 0.061799, loss_ce: 0.026392
 12%|███▋                          | 49/400 [20:28<2:19:15, 23.81s/it]2022-01-20 19:53:30,011 iteration 834 : loss : 0.074562, loss_ce: 0.026737
2022-01-20 19:53:31,266 iteration 835 : loss : 0.041869, loss_ce: 0.016168
2022-01-20 19:53:32,665 iteration 836 : loss : 0.071010, loss_ce: 0.027726
2022-01-20 19:53:33,978 iteration 837 : loss : 0.053798, loss_ce: 0.022686
2022-01-20 19:53:35,349 iteration 838 : loss : 0.062562, loss_ce: 0.031627
2022-01-20 19:53:36,612 iteration 839 : loss : 0.054590, loss_ce: 0.024089
2022-01-20 19:53:37,886 iteration 840 : loss : 0.042127, loss_ce: 0.019251
2022-01-20 19:53:39,176 iteration 841 : loss : 0.070566, loss_ce: 0.027037
2022-01-20 19:53:40,593 iteration 842 : loss : 0.095165, loss_ce: 0.035001
2022-01-20 19:53:41,894 iteration 843 : loss : 0.060067, loss_ce: 0.018134
2022-01-20 19:53:43,289 iteration 844 : loss : 0.070184, loss_ce: 0.026164
2022-01-20 19:53:44,616 iteration 845 : loss : 0.064401, loss_ce: 0.031286
2022-01-20 19:53:45,939 iteration 846 : loss : 0.051908, loss_ce: 0.025160
2022-01-20 19:53:47,355 iteration 847 : loss : 0.075160, loss_ce: 0.031179
2022-01-20 19:53:48,684 iteration 848 : loss : 0.044418, loss_ce: 0.016625
2022-01-20 19:53:49,997 iteration 849 : loss : 0.053943, loss_ce: 0.018391
2022-01-20 19:53:49,997 Training Data Eval:
2022-01-20 19:53:56,599   Average segmentation loss on training set: 0.0543
2022-01-20 19:53:56,600 Validation Data Eval:
2022-01-20 19:53:58,861   Average segmentation loss on validation set: 0.1626
2022-01-20 19:54:00,149 iteration 850 : loss : 0.075691, loss_ce: 0.037269
 12%|███▊                          | 50/400 [21:00<2:32:13, 26.10s/it]2022-01-20 19:54:01,529 iteration 851 : loss : 0.069324, loss_ce: 0.028878
2022-01-20 19:54:02,867 iteration 852 : loss : 0.058603, loss_ce: 0.018740
2022-01-20 19:54:04,222 iteration 853 : loss : 0.059052, loss_ce: 0.020100
2022-01-20 19:54:05,589 iteration 854 : loss : 0.045220, loss_ce: 0.020511
2022-01-20 19:54:06,900 iteration 855 : loss : 0.057032, loss_ce: 0.026785
2022-01-20 19:54:08,280 iteration 856 : loss : 0.061780, loss_ce: 0.028028
2022-01-20 19:54:09,600 iteration 857 : loss : 0.096370, loss_ce: 0.045154
2022-01-20 19:54:11,032 iteration 858 : loss : 0.035569, loss_ce: 0.014691
2022-01-20 19:54:12,250 iteration 859 : loss : 0.051854, loss_ce: 0.017400
2022-01-20 19:54:13,585 iteration 860 : loss : 0.050908, loss_ce: 0.021718
2022-01-20 19:54:14,924 iteration 861 : loss : 0.067948, loss_ce: 0.018884
2022-01-20 19:54:16,282 iteration 862 : loss : 0.068963, loss_ce: 0.032333
2022-01-20 19:54:17,620 iteration 863 : loss : 0.090165, loss_ce: 0.034931
2022-01-20 19:54:18,946 iteration 864 : loss : 0.051338, loss_ce: 0.024742
2022-01-20 19:54:20,304 iteration 865 : loss : 0.057038, loss_ce: 0.020797
2022-01-20 19:54:21,611 iteration 866 : loss : 0.058337, loss_ce: 0.023408
2022-01-20 19:54:22,996 iteration 867 : loss : 0.067055, loss_ce: 0.024895
 13%|███▊                          | 51/400 [21:23<2:26:07, 25.12s/it]2022-01-20 19:54:24,370 iteration 868 : loss : 0.054153, loss_ce: 0.023741
2022-01-20 19:54:25,757 iteration 869 : loss : 0.067955, loss_ce: 0.031581
2022-01-20 19:54:27,015 iteration 870 : loss : 0.070193, loss_ce: 0.031593
2022-01-20 19:54:28,338 iteration 871 : loss : 0.066317, loss_ce: 0.036387
2022-01-20 19:54:29,661 iteration 872 : loss : 0.069361, loss_ce: 0.030319
2022-01-20 19:54:31,023 iteration 873 : loss : 0.059336, loss_ce: 0.025696
2022-01-20 19:54:32,370 iteration 874 : loss : 0.060974, loss_ce: 0.025032
2022-01-20 19:54:33,717 iteration 875 : loss : 0.073572, loss_ce: 0.026424
2022-01-20 19:54:35,063 iteration 876 : loss : 0.060925, loss_ce: 0.023155
2022-01-20 19:54:36,477 iteration 877 : loss : 0.044664, loss_ce: 0.017308
2022-01-20 19:54:37,784 iteration 878 : loss : 0.091237, loss_ce: 0.021884
2022-01-20 19:54:39,116 iteration 879 : loss : 0.121605, loss_ce: 0.030851
2022-01-20 19:54:40,413 iteration 880 : loss : 0.055302, loss_ce: 0.023850
2022-01-20 19:54:41,824 iteration 881 : loss : 0.081674, loss_ce: 0.027807
2022-01-20 19:54:43,188 iteration 882 : loss : 0.090026, loss_ce: 0.038120
2022-01-20 19:54:44,492 iteration 883 : loss : 0.053763, loss_ce: 0.017294
2022-01-20 19:54:45,869 iteration 884 : loss : 0.060221, loss_ce: 0.025478
 13%|███▉                          | 52/400 [21:46<2:21:47, 24.45s/it]2022-01-20 19:54:47,263 iteration 885 : loss : 0.049121, loss_ce: 0.022730
2022-01-20 19:54:48,606 iteration 886 : loss : 0.068907, loss_ce: 0.035255
2022-01-20 19:54:49,945 iteration 887 : loss : 0.077769, loss_ce: 0.030885
2022-01-20 19:54:51,305 iteration 888 : loss : 0.069659, loss_ce: 0.027366
2022-01-20 19:54:52,653 iteration 889 : loss : 0.139283, loss_ce: 0.038814
2022-01-20 19:54:53,972 iteration 890 : loss : 0.056459, loss_ce: 0.024391
2022-01-20 19:54:55,317 iteration 891 : loss : 0.065491, loss_ce: 0.040219
2022-01-20 19:54:56,658 iteration 892 : loss : 0.056970, loss_ce: 0.025978
2022-01-20 19:54:57,988 iteration 893 : loss : 0.055354, loss_ce: 0.025565
2022-01-20 19:54:59,311 iteration 894 : loss : 0.061821, loss_ce: 0.022146
2022-01-20 19:55:00,630 iteration 895 : loss : 0.065201, loss_ce: 0.021894
2022-01-20 19:55:01,931 iteration 896 : loss : 0.068227, loss_ce: 0.035513
2022-01-20 19:55:03,406 iteration 897 : loss : 0.086050, loss_ce: 0.030515
2022-01-20 19:55:04,725 iteration 898 : loss : 0.058134, loss_ce: 0.023623
2022-01-20 19:55:06,111 iteration 899 : loss : 0.078774, loss_ce: 0.027029
2022-01-20 19:55:07,482 iteration 900 : loss : 0.066835, loss_ce: 0.021301
2022-01-20 19:55:08,791 iteration 901 : loss : 0.058466, loss_ce: 0.022104
 13%|███▉                          | 53/400 [22:08<2:18:45, 23.99s/it]2022-01-20 19:55:10,249 iteration 902 : loss : 0.050557, loss_ce: 0.024203
2022-01-20 19:55:11,626 iteration 903 : loss : 0.110002, loss_ce: 0.033751
2022-01-20 19:55:12,966 iteration 904 : loss : 0.082275, loss_ce: 0.038008
2022-01-20 19:55:14,293 iteration 905 : loss : 0.053777, loss_ce: 0.022206
2022-01-20 19:55:15,737 iteration 906 : loss : 0.057699, loss_ce: 0.024649
2022-01-20 19:55:17,041 iteration 907 : loss : 0.056931, loss_ce: 0.024772
2022-01-20 19:55:18,350 iteration 908 : loss : 0.070186, loss_ce: 0.035888
2022-01-20 19:55:19,722 iteration 909 : loss : 0.069256, loss_ce: 0.031036
2022-01-20 19:55:21,144 iteration 910 : loss : 0.047751, loss_ce: 0.018459
2022-01-20 19:55:22,482 iteration 911 : loss : 0.062197, loss_ce: 0.025794
2022-01-20 19:55:23,857 iteration 912 : loss : 0.068652, loss_ce: 0.029807
2022-01-20 19:55:25,196 iteration 913 : loss : 0.058917, loss_ce: 0.020811
2022-01-20 19:55:26,473 iteration 914 : loss : 0.059009, loss_ce: 0.024381
2022-01-20 19:55:27,824 iteration 915 : loss : 0.065182, loss_ce: 0.023304
2022-01-20 19:55:29,228 iteration 916 : loss : 0.097654, loss_ce: 0.032158
2022-01-20 19:55:30,587 iteration 917 : loss : 0.059049, loss_ce: 0.021085
2022-01-20 19:55:32,002 iteration 918 : loss : 0.118290, loss_ce: 0.030505
 14%|████                          | 54/400 [22:32<2:17:00, 23.76s/it]2022-01-20 19:55:33,406 iteration 919 : loss : 0.056807, loss_ce: 0.018586
2022-01-20 19:55:34,808 iteration 920 : loss : 0.046939, loss_ce: 0.021858
2022-01-20 19:55:36,094 iteration 921 : loss : 0.062181, loss_ce: 0.023797
2022-01-20 19:55:37,460 iteration 922 : loss : 0.057750, loss_ce: 0.018327
2022-01-20 19:55:38,737 iteration 923 : loss : 0.041980, loss_ce: 0.016919
2022-01-20 19:55:40,100 iteration 924 : loss : 0.070311, loss_ce: 0.023318
2022-01-20 19:55:41,477 iteration 925 : loss : 0.073780, loss_ce: 0.020639
2022-01-20 19:55:42,849 iteration 926 : loss : 0.059720, loss_ce: 0.021470
2022-01-20 19:55:44,129 iteration 927 : loss : 0.075333, loss_ce: 0.021541
2022-01-20 19:55:45,540 iteration 928 : loss : 0.066457, loss_ce: 0.036120
2022-01-20 19:55:46,864 iteration 929 : loss : 0.048628, loss_ce: 0.017639
2022-01-20 19:55:48,150 iteration 930 : loss : 0.055074, loss_ce: 0.022204
2022-01-20 19:55:49,454 iteration 931 : loss : 0.102923, loss_ce: 0.057728
2022-01-20 19:55:50,802 iteration 932 : loss : 0.047483, loss_ce: 0.022963
2022-01-20 19:55:52,185 iteration 933 : loss : 0.062210, loss_ce: 0.030012
2022-01-20 19:55:53,488 iteration 934 : loss : 0.070116, loss_ce: 0.027036
2022-01-20 19:55:53,488 Training Data Eval:
2022-01-20 19:56:00,096   Average segmentation loss on training set: 0.0504
2022-01-20 19:56:00,097 Validation Data Eval:
2022-01-20 19:56:02,370   Average segmentation loss on validation set: 0.0713
2022-01-20 19:56:07,067 Found new lowest validation loss at iteration 934! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 19:56:08,336 iteration 935 : loss : 0.066525, loss_ce: 0.031675
 14%|████▏                         | 55/400 [23:08<2:38:17, 27.53s/it]2022-01-20 19:56:09,614 iteration 936 : loss : 0.057536, loss_ce: 0.027822
2022-01-20 19:56:10,925 iteration 937 : loss : 0.096790, loss_ce: 0.037565
2022-01-20 19:56:12,283 iteration 938 : loss : 0.051877, loss_ce: 0.026331
2022-01-20 19:56:13,719 iteration 939 : loss : 0.118252, loss_ce: 0.038214
2022-01-20 19:56:15,001 iteration 940 : loss : 0.064866, loss_ce: 0.026141
2022-01-20 19:56:16,312 iteration 941 : loss : 0.069369, loss_ce: 0.023838
2022-01-20 19:56:17,645 iteration 942 : loss : 0.069716, loss_ce: 0.029820
2022-01-20 19:56:19,008 iteration 943 : loss : 0.099254, loss_ce: 0.032689
2022-01-20 19:56:20,350 iteration 944 : loss : 0.092400, loss_ce: 0.035209
2022-01-20 19:56:21,715 iteration 945 : loss : 0.064968, loss_ce: 0.026618
2022-01-20 19:56:23,050 iteration 946 : loss : 0.058936, loss_ce: 0.022020
2022-01-20 19:56:24,391 iteration 947 : loss : 0.060805, loss_ce: 0.018877
2022-01-20 19:56:25,801 iteration 948 : loss : 0.077146, loss_ce: 0.033021
2022-01-20 19:56:27,207 iteration 949 : loss : 0.118280, loss_ce: 0.050549
2022-01-20 19:56:28,625 iteration 950 : loss : 0.099709, loss_ce: 0.034233
2022-01-20 19:56:29,949 iteration 951 : loss : 0.040235, loss_ce: 0.015288
2022-01-20 19:56:31,328 iteration 952 : loss : 0.077023, loss_ce: 0.028161
 14%|████▏                         | 56/400 [23:31<2:30:02, 26.17s/it]2022-01-20 19:56:32,689 iteration 953 : loss : 0.071781, loss_ce: 0.030936
2022-01-20 19:56:34,014 iteration 954 : loss : 0.058827, loss_ce: 0.018603
2022-01-20 19:56:35,391 iteration 955 : loss : 0.070244, loss_ce: 0.024443
2022-01-20 19:56:36,764 iteration 956 : loss : 0.052660, loss_ce: 0.017502
2022-01-20 19:56:38,119 iteration 957 : loss : 0.077942, loss_ce: 0.032096
2022-01-20 19:56:39,502 iteration 958 : loss : 0.051721, loss_ce: 0.020195
2022-01-20 19:56:40,917 iteration 959 : loss : 0.070997, loss_ce: 0.032098
2022-01-20 19:56:42,304 iteration 960 : loss : 0.058135, loss_ce: 0.024770
2022-01-20 19:56:43,667 iteration 961 : loss : 0.074534, loss_ce: 0.029984
2022-01-20 19:56:45,055 iteration 962 : loss : 0.083281, loss_ce: 0.025747
2022-01-20 19:56:46,353 iteration 963 : loss : 0.056883, loss_ce: 0.023743
2022-01-20 19:56:47,731 iteration 964 : loss : 0.078656, loss_ce: 0.021636
2022-01-20 19:56:49,050 iteration 965 : loss : 0.066713, loss_ce: 0.021217
2022-01-20 19:56:50,487 iteration 966 : loss : 0.069860, loss_ce: 0.026069
2022-01-20 19:56:51,850 iteration 967 : loss : 0.066348, loss_ce: 0.031329
2022-01-20 19:56:53,119 iteration 968 : loss : 0.057624, loss_ce: 0.021550
2022-01-20 19:56:54,466 iteration 969 : loss : 0.061786, loss_ce: 0.030728
 14%|████▎                         | 57/400 [23:54<2:24:23, 25.26s/it]2022-01-20 19:56:55,805 iteration 970 : loss : 0.092429, loss_ce: 0.057765
2022-01-20 19:56:57,171 iteration 971 : loss : 0.055379, loss_ce: 0.024920
2022-01-20 19:56:58,507 iteration 972 : loss : 0.057937, loss_ce: 0.022088
2022-01-20 19:56:59,870 iteration 973 : loss : 0.056565, loss_ce: 0.021219
2022-01-20 19:57:01,213 iteration 974 : loss : 0.083972, loss_ce: 0.040921
2022-01-20 19:57:02,517 iteration 975 : loss : 0.069451, loss_ce: 0.024321
2022-01-20 19:57:03,920 iteration 976 : loss : 0.041848, loss_ce: 0.019516
2022-01-20 19:57:05,253 iteration 977 : loss : 0.054028, loss_ce: 0.023383
2022-01-20 19:57:06,618 iteration 978 : loss : 0.049195, loss_ce: 0.019316
2022-01-20 19:57:07,933 iteration 979 : loss : 0.039299, loss_ce: 0.016085
2022-01-20 19:57:09,252 iteration 980 : loss : 0.067143, loss_ce: 0.024035
2022-01-20 19:57:10,559 iteration 981 : loss : 0.032224, loss_ce: 0.011441
2022-01-20 19:57:11,929 iteration 982 : loss : 0.071502, loss_ce: 0.024055
2022-01-20 19:57:13,335 iteration 983 : loss : 0.087012, loss_ce: 0.038618
2022-01-20 19:57:14,742 iteration 984 : loss : 0.053919, loss_ce: 0.024223
2022-01-20 19:57:16,119 iteration 985 : loss : 0.066265, loss_ce: 0.028142
2022-01-20 19:57:17,429 iteration 986 : loss : 0.081018, loss_ce: 0.036265
 14%|████▎                         | 58/400 [24:17<2:20:03, 24.57s/it]2022-01-20 19:57:18,732 iteration 987 : loss : 0.054716, loss_ce: 0.019372
2022-01-20 19:57:20,067 iteration 988 : loss : 0.049250, loss_ce: 0.018078
2022-01-20 19:57:21,396 iteration 989 : loss : 0.044187, loss_ce: 0.016268
2022-01-20 19:57:22,730 iteration 990 : loss : 0.065914, loss_ce: 0.023744
2022-01-20 19:57:24,068 iteration 991 : loss : 0.066891, loss_ce: 0.028174
2022-01-20 19:57:25,379 iteration 992 : loss : 0.052647, loss_ce: 0.020548
2022-01-20 19:57:26,647 iteration 993 : loss : 0.053542, loss_ce: 0.018309
2022-01-20 19:57:27,862 iteration 994 : loss : 0.055192, loss_ce: 0.019163
2022-01-20 19:57:29,276 iteration 995 : loss : 0.056210, loss_ce: 0.024376
2022-01-20 19:57:30,640 iteration 996 : loss : 0.053558, loss_ce: 0.024088
2022-01-20 19:57:32,023 iteration 997 : loss : 0.066845, loss_ce: 0.027284
2022-01-20 19:57:33,396 iteration 998 : loss : 0.056086, loss_ce: 0.020847
2022-01-20 19:57:34,698 iteration 999 : loss : 0.049396, loss_ce: 0.021520
2022-01-20 19:57:36,094 iteration 1000 : loss : 0.060083, loss_ce: 0.030383
2022-01-20 19:57:37,412 iteration 1001 : loss : 0.048885, loss_ce: 0.017822
2022-01-20 19:57:38,811 iteration 1002 : loss : 0.077181, loss_ce: 0.028794
2022-01-20 19:57:40,093 iteration 1003 : loss : 0.103257, loss_ce: 0.033848
 15%|████▍                         | 59/400 [24:40<2:16:22, 24.00s/it]2022-01-20 19:57:41,459 iteration 1004 : loss : 0.067316, loss_ce: 0.021963
2022-01-20 19:57:42,799 iteration 1005 : loss : 0.082449, loss_ce: 0.041564
2022-01-20 19:57:44,228 iteration 1006 : loss : 0.065688, loss_ce: 0.030930
2022-01-20 19:57:45,620 iteration 1007 : loss : 0.075009, loss_ce: 0.032086
2022-01-20 19:57:47,022 iteration 1008 : loss : 0.083389, loss_ce: 0.051632
2022-01-20 19:57:48,340 iteration 1009 : loss : 0.047830, loss_ce: 0.016963
2022-01-20 19:57:49,615 iteration 1010 : loss : 0.035453, loss_ce: 0.010267
2022-01-20 19:57:50,935 iteration 1011 : loss : 0.064088, loss_ce: 0.024312
2022-01-20 19:57:52,326 iteration 1012 : loss : 0.075366, loss_ce: 0.031612
2022-01-20 19:57:53,664 iteration 1013 : loss : 0.049962, loss_ce: 0.018140
2022-01-20 19:57:54,991 iteration 1014 : loss : 0.036544, loss_ce: 0.015086
2022-01-20 19:57:56,455 iteration 1015 : loss : 0.103837, loss_ce: 0.036784
2022-01-20 19:57:57,791 iteration 1016 : loss : 0.056660, loss_ce: 0.025521
2022-01-20 19:57:59,101 iteration 1017 : loss : 0.060675, loss_ce: 0.023805
2022-01-20 19:58:00,504 iteration 1018 : loss : 0.073841, loss_ce: 0.026851
2022-01-20 19:58:01,891 iteration 1019 : loss : 0.064648, loss_ce: 0.025187
2022-01-20 19:58:01,891 Training Data Eval:
2022-01-20 19:58:08,499   Average segmentation loss on training set: 0.0426
2022-01-20 19:58:08,500 Validation Data Eval:
2022-01-20 19:58:10,768   Average segmentation loss on validation set: 0.1128
2022-01-20 19:58:12,182 iteration 1020 : loss : 0.060436, loss_ce: 0.026184
 15%|████▌                         | 60/400 [25:12<2:29:44, 26.43s/it]2022-01-20 19:58:13,547 iteration 1021 : loss : 0.058564, loss_ce: 0.022347
2022-01-20 19:58:14,976 iteration 1022 : loss : 0.082085, loss_ce: 0.038478
2022-01-20 19:58:16,278 iteration 1023 : loss : 0.050594, loss_ce: 0.017757
2022-01-20 19:58:17,597 iteration 1024 : loss : 0.042831, loss_ce: 0.016503
2022-01-20 19:58:18,993 iteration 1025 : loss : 0.056884, loss_ce: 0.025030
2022-01-20 19:58:20,364 iteration 1026 : loss : 0.096088, loss_ce: 0.025344
2022-01-20 19:58:21,701 iteration 1027 : loss : 0.060148, loss_ce: 0.020238
2022-01-20 19:58:23,130 iteration 1028 : loss : 0.069449, loss_ce: 0.022796
2022-01-20 19:58:24,526 iteration 1029 : loss : 0.078446, loss_ce: 0.024073
2022-01-20 19:58:25,876 iteration 1030 : loss : 0.070114, loss_ce: 0.031863
2022-01-20 19:58:27,323 iteration 1031 : loss : 0.057749, loss_ce: 0.025163
2022-01-20 19:58:28,688 iteration 1032 : loss : 0.063108, loss_ce: 0.030192
2022-01-20 19:58:30,012 iteration 1033 : loss : 0.054512, loss_ce: 0.024321
2022-01-20 19:58:31,309 iteration 1034 : loss : 0.044557, loss_ce: 0.016690
2022-01-20 19:58:32,694 iteration 1035 : loss : 0.051464, loss_ce: 0.022768
2022-01-20 19:58:34,103 iteration 1036 : loss : 0.070086, loss_ce: 0.029747
2022-01-20 19:58:35,374 iteration 1037 : loss : 0.053777, loss_ce: 0.021655
 15%|████▌                         | 61/400 [25:35<2:23:49, 25.46s/it]2022-01-20 19:58:36,723 iteration 1038 : loss : 0.052419, loss_ce: 0.023820
2022-01-20 19:58:38,041 iteration 1039 : loss : 0.051039, loss_ce: 0.021558
2022-01-20 19:58:39,362 iteration 1040 : loss : 0.070792, loss_ce: 0.023411
2022-01-20 19:58:40,700 iteration 1041 : loss : 0.064458, loss_ce: 0.028954
2022-01-20 19:58:42,016 iteration 1042 : loss : 0.043902, loss_ce: 0.022462
2022-01-20 19:58:43,344 iteration 1043 : loss : 0.065752, loss_ce: 0.024021
2022-01-20 19:58:44,739 iteration 1044 : loss : 0.055746, loss_ce: 0.024619
2022-01-20 19:58:46,096 iteration 1045 : loss : 0.059767, loss_ce: 0.022457
2022-01-20 19:58:47,405 iteration 1046 : loss : 0.049606, loss_ce: 0.022843
2022-01-20 19:58:48,637 iteration 1047 : loss : 0.045140, loss_ce: 0.017809
2022-01-20 19:58:49,989 iteration 1048 : loss : 0.043082, loss_ce: 0.017934
2022-01-20 19:58:51,381 iteration 1049 : loss : 0.046976, loss_ce: 0.017626
2022-01-20 19:58:52,715 iteration 1050 : loss : 0.102057, loss_ce: 0.017544
2022-01-20 19:58:54,032 iteration 1051 : loss : 0.046360, loss_ce: 0.021602
2022-01-20 19:58:55,422 iteration 1052 : loss : 0.072640, loss_ce: 0.019537
2022-01-20 19:58:56,797 iteration 1053 : loss : 0.067739, loss_ce: 0.016947
2022-01-20 19:58:58,147 iteration 1054 : loss : 0.045430, loss_ce: 0.016146
 16%|████▋                         | 62/400 [25:58<2:18:51, 24.65s/it]2022-01-20 19:58:59,529 iteration 1055 : loss : 0.039793, loss_ce: 0.011882
2022-01-20 19:59:00,798 iteration 1056 : loss : 0.048451, loss_ce: 0.022611
2022-01-20 19:59:02,174 iteration 1057 : loss : 0.065715, loss_ce: 0.025872
2022-01-20 19:59:03,529 iteration 1058 : loss : 0.071782, loss_ce: 0.023374
2022-01-20 19:59:04,895 iteration 1059 : loss : 0.058754, loss_ce: 0.022791
2022-01-20 19:59:06,180 iteration 1060 : loss : 0.041690, loss_ce: 0.012877
2022-01-20 19:59:07,628 iteration 1061 : loss : 0.100271, loss_ce: 0.060118
2022-01-20 19:59:09,005 iteration 1062 : loss : 0.077789, loss_ce: 0.030148
2022-01-20 19:59:10,491 iteration 1063 : loss : 0.053968, loss_ce: 0.024114
2022-01-20 19:59:11,863 iteration 1064 : loss : 0.077747, loss_ce: 0.026293
2022-01-20 19:59:13,298 iteration 1065 : loss : 0.054514, loss_ce: 0.025657
2022-01-20 19:59:14,612 iteration 1066 : loss : 0.040377, loss_ce: 0.015547
2022-01-20 19:59:15,927 iteration 1067 : loss : 0.046902, loss_ce: 0.019476
2022-01-20 19:59:17,309 iteration 1068 : loss : 0.052516, loss_ce: 0.022053
2022-01-20 19:59:18,676 iteration 1069 : loss : 0.051124, loss_ce: 0.022772
2022-01-20 19:59:20,132 iteration 1070 : loss : 0.103508, loss_ce: 0.028164
2022-01-20 19:59:21,485 iteration 1071 : loss : 0.074536, loss_ce: 0.035104
 16%|████▋                         | 63/400 [26:21<2:16:14, 24.26s/it]2022-01-20 19:59:22,966 iteration 1072 : loss : 0.063902, loss_ce: 0.018218
2022-01-20 19:59:24,341 iteration 1073 : loss : 0.082249, loss_ce: 0.038967
2022-01-20 19:59:25,717 iteration 1074 : loss : 0.079843, loss_ce: 0.031642
2022-01-20 19:59:27,093 iteration 1075 : loss : 0.091360, loss_ce: 0.033299
2022-01-20 19:59:28,415 iteration 1076 : loss : 0.063420, loss_ce: 0.024544
2022-01-20 19:59:29,845 iteration 1077 : loss : 0.078307, loss_ce: 0.021530
2022-01-20 19:59:31,166 iteration 1078 : loss : 0.053069, loss_ce: 0.024968
2022-01-20 19:59:32,575 iteration 1079 : loss : 0.065770, loss_ce: 0.026294
2022-01-20 19:59:33,850 iteration 1080 : loss : 0.032873, loss_ce: 0.011835
2022-01-20 19:59:35,180 iteration 1081 : loss : 0.055796, loss_ce: 0.029850
2022-01-20 19:59:36,563 iteration 1082 : loss : 0.073158, loss_ce: 0.031615
2022-01-20 19:59:37,872 iteration 1083 : loss : 0.061514, loss_ce: 0.023261
2022-01-20 19:59:39,200 iteration 1084 : loss : 0.060670, loss_ce: 0.018825
2022-01-20 19:59:40,558 iteration 1085 : loss : 0.051027, loss_ce: 0.020433
2022-01-20 19:59:41,894 iteration 1086 : loss : 0.057781, loss_ce: 0.026912
2022-01-20 19:59:43,316 iteration 1087 : loss : 0.047474, loss_ce: 0.019830
2022-01-20 19:59:44,663 iteration 1088 : loss : 0.057474, loss_ce: 0.023263
 16%|████▊                         | 64/400 [26:44<2:14:00, 23.93s/it]2022-01-20 19:59:46,011 iteration 1089 : loss : 0.083426, loss_ce: 0.033278
2022-01-20 19:59:47,328 iteration 1090 : loss : 0.065146, loss_ce: 0.021314
2022-01-20 19:59:48,728 iteration 1091 : loss : 0.042964, loss_ce: 0.020596
2022-01-20 19:59:50,083 iteration 1092 : loss : 0.050664, loss_ce: 0.021605
2022-01-20 19:59:51,435 iteration 1093 : loss : 0.044441, loss_ce: 0.020298
2022-01-20 19:59:52,806 iteration 1094 : loss : 0.067521, loss_ce: 0.034086
2022-01-20 19:59:54,187 iteration 1095 : loss : 0.101949, loss_ce: 0.029257
2022-01-20 19:59:55,539 iteration 1096 : loss : 0.041684, loss_ce: 0.019214
2022-01-20 19:59:56,842 iteration 1097 : loss : 0.070068, loss_ce: 0.019506
2022-01-20 19:59:58,154 iteration 1098 : loss : 0.050873, loss_ce: 0.023285
2022-01-20 19:59:59,518 iteration 1099 : loss : 0.052779, loss_ce: 0.021557
2022-01-20 20:00:00,844 iteration 1100 : loss : 0.057201, loss_ce: 0.023907
2022-01-20 20:00:02,165 iteration 1101 : loss : 0.051133, loss_ce: 0.021452
2022-01-20 20:00:03,580 iteration 1102 : loss : 0.065568, loss_ce: 0.017096
2022-01-20 20:00:04,911 iteration 1103 : loss : 0.095984, loss_ce: 0.042380
2022-01-20 20:00:06,203 iteration 1104 : loss : 0.053204, loss_ce: 0.022036
2022-01-20 20:00:06,204 Training Data Eval:
2022-01-20 20:00:12,810   Average segmentation loss on training set: 0.0421
2022-01-20 20:00:12,811 Validation Data Eval:
2022-01-20 20:00:15,070   Average segmentation loss on validation set: 0.0723
2022-01-20 20:00:16,415 iteration 1105 : loss : 0.046078, loss_ce: 0.021720
 16%|████▉                         | 65/400 [27:16<2:26:43, 26.28s/it]2022-01-20 20:00:17,799 iteration 1106 : loss : 0.057017, loss_ce: 0.018746
2022-01-20 20:00:19,201 iteration 1107 : loss : 0.065588, loss_ce: 0.031098
2022-01-20 20:00:20,500 iteration 1108 : loss : 0.038070, loss_ce: 0.013016
2022-01-20 20:00:21,783 iteration 1109 : loss : 0.070051, loss_ce: 0.030840
2022-01-20 20:00:23,114 iteration 1110 : loss : 0.050512, loss_ce: 0.021104
2022-01-20 20:00:24,463 iteration 1111 : loss : 0.045166, loss_ce: 0.021162
2022-01-20 20:00:25,824 iteration 1112 : loss : 0.062399, loss_ce: 0.028214
2022-01-20 20:00:27,144 iteration 1113 : loss : 0.052276, loss_ce: 0.017606
2022-01-20 20:00:28,436 iteration 1114 : loss : 0.069490, loss_ce: 0.024734
2022-01-20 20:00:29,692 iteration 1115 : loss : 0.045446, loss_ce: 0.014461
2022-01-20 20:00:31,103 iteration 1116 : loss : 0.059498, loss_ce: 0.023762
2022-01-20 20:00:32,537 iteration 1117 : loss : 0.049834, loss_ce: 0.018222
2022-01-20 20:00:33,852 iteration 1118 : loss : 0.042658, loss_ce: 0.017338
2022-01-20 20:00:35,162 iteration 1119 : loss : 0.047893, loss_ce: 0.018328
2022-01-20 20:00:36,455 iteration 1120 : loss : 0.051018, loss_ce: 0.016746
2022-01-20 20:00:37,758 iteration 1121 : loss : 0.059143, loss_ce: 0.015553
2022-01-20 20:00:39,048 iteration 1122 : loss : 0.045205, loss_ce: 0.016927
 16%|████▉                         | 66/400 [27:39<2:20:12, 25.19s/it]2022-01-20 20:00:40,434 iteration 1123 : loss : 0.047617, loss_ce: 0.020277
2022-01-20 20:00:41,740 iteration 1124 : loss : 0.064409, loss_ce: 0.026080
2022-01-20 20:00:43,130 iteration 1125 : loss : 0.057968, loss_ce: 0.022383
2022-01-20 20:00:44,597 iteration 1126 : loss : 0.078728, loss_ce: 0.039129
2022-01-20 20:00:45,983 iteration 1127 : loss : 0.051694, loss_ce: 0.018137
2022-01-20 20:00:47,360 iteration 1128 : loss : 0.062984, loss_ce: 0.019806
2022-01-20 20:00:48,708 iteration 1129 : loss : 0.041771, loss_ce: 0.015113
2022-01-20 20:00:50,110 iteration 1130 : loss : 0.057766, loss_ce: 0.029201
2022-01-20 20:00:51,526 iteration 1131 : loss : 0.104020, loss_ce: 0.031300
2022-01-20 20:00:52,898 iteration 1132 : loss : 0.071664, loss_ce: 0.024258
2022-01-20 20:00:54,262 iteration 1133 : loss : 0.054672, loss_ce: 0.020734
2022-01-20 20:00:55,616 iteration 1134 : loss : 0.061940, loss_ce: 0.024823
2022-01-20 20:00:56,918 iteration 1135 : loss : 0.046902, loss_ce: 0.014767
2022-01-20 20:00:58,266 iteration 1136 : loss : 0.083287, loss_ce: 0.032450
2022-01-20 20:00:59,587 iteration 1137 : loss : 0.055450, loss_ce: 0.019734
2022-01-20 20:01:01,005 iteration 1138 : loss : 0.064827, loss_ce: 0.029736
2022-01-20 20:01:02,417 iteration 1139 : loss : 0.052883, loss_ce: 0.024197
 17%|█████                         | 67/400 [28:02<2:16:45, 24.64s/it]2022-01-20 20:01:03,790 iteration 1140 : loss : 0.052889, loss_ce: 0.020544
2022-01-20 20:01:05,139 iteration 1141 : loss : 0.044867, loss_ce: 0.019962
2022-01-20 20:01:06,439 iteration 1142 : loss : 0.043774, loss_ce: 0.016673
2022-01-20 20:01:07,801 iteration 1143 : loss : 0.092759, loss_ce: 0.031547
2022-01-20 20:01:09,196 iteration 1144 : loss : 0.067118, loss_ce: 0.030095
2022-01-20 20:01:10,533 iteration 1145 : loss : 0.044326, loss_ce: 0.014030
2022-01-20 20:01:11,906 iteration 1146 : loss : 0.056169, loss_ce: 0.024226
2022-01-20 20:01:13,190 iteration 1147 : loss : 0.036664, loss_ce: 0.012385
2022-01-20 20:01:14,528 iteration 1148 : loss : 0.046300, loss_ce: 0.020220
2022-01-20 20:01:15,925 iteration 1149 : loss : 0.047159, loss_ce: 0.022697
2022-01-20 20:01:17,277 iteration 1150 : loss : 0.051429, loss_ce: 0.021473
2022-01-20 20:01:18,635 iteration 1151 : loss : 0.054302, loss_ce: 0.016157
2022-01-20 20:01:20,017 iteration 1152 : loss : 0.093633, loss_ce: 0.018961
2022-01-20 20:01:21,374 iteration 1153 : loss : 0.055298, loss_ce: 0.019831
2022-01-20 20:01:22,658 iteration 1154 : loss : 0.058170, loss_ce: 0.023423
2022-01-20 20:01:24,086 iteration 1155 : loss : 0.058645, loss_ce: 0.020474
2022-01-20 20:01:25,466 iteration 1156 : loss : 0.088698, loss_ce: 0.026092
 17%|█████                         | 68/400 [28:25<2:13:41, 24.16s/it]2022-01-20 20:01:26,807 iteration 1157 : loss : 0.046734, loss_ce: 0.018277
2022-01-20 20:01:28,268 iteration 1158 : loss : 0.073997, loss_ce: 0.022515
2022-01-20 20:01:29,686 iteration 1159 : loss : 0.057527, loss_ce: 0.023392
2022-01-20 20:01:30,964 iteration 1160 : loss : 0.050693, loss_ce: 0.024677
2022-01-20 20:01:32,313 iteration 1161 : loss : 0.049120, loss_ce: 0.016784
2022-01-20 20:01:33,643 iteration 1162 : loss : 0.056439, loss_ce: 0.016497
2022-01-20 20:01:35,079 iteration 1163 : loss : 0.075778, loss_ce: 0.023238
2022-01-20 20:01:36,448 iteration 1164 : loss : 0.050636, loss_ce: 0.018657
2022-01-20 20:01:37,743 iteration 1165 : loss : 0.047909, loss_ce: 0.023882
2022-01-20 20:01:39,078 iteration 1166 : loss : 0.057912, loss_ce: 0.029753
2022-01-20 20:01:40,446 iteration 1167 : loss : 0.056241, loss_ce: 0.026227
2022-01-20 20:01:41,717 iteration 1168 : loss : 0.047803, loss_ce: 0.016976
2022-01-20 20:01:43,064 iteration 1169 : loss : 0.066823, loss_ce: 0.023097
2022-01-20 20:01:44,427 iteration 1170 : loss : 0.042895, loss_ce: 0.016143
2022-01-20 20:01:45,708 iteration 1171 : loss : 0.040395, loss_ce: 0.016633
2022-01-20 20:01:47,045 iteration 1172 : loss : 0.053189, loss_ce: 0.022882
2022-01-20 20:01:48,410 iteration 1173 : loss : 0.073033, loss_ce: 0.048902
 17%|█████▏                        | 69/400 [28:48<2:11:17, 23.80s/it]2022-01-20 20:01:49,826 iteration 1174 : loss : 0.056954, loss_ce: 0.018000
2022-01-20 20:01:51,154 iteration 1175 : loss : 0.046765, loss_ce: 0.020023
2022-01-20 20:01:52,548 iteration 1176 : loss : 0.067636, loss_ce: 0.031584
2022-01-20 20:01:53,986 iteration 1177 : loss : 0.072813, loss_ce: 0.029174
2022-01-20 20:01:55,353 iteration 1178 : loss : 0.049828, loss_ce: 0.022883
2022-01-20 20:01:56,621 iteration 1179 : loss : 0.045158, loss_ce: 0.018124
2022-01-20 20:01:58,010 iteration 1180 : loss : 0.044417, loss_ce: 0.017154
2022-01-20 20:01:59,284 iteration 1181 : loss : 0.047075, loss_ce: 0.019607
2022-01-20 20:02:00,719 iteration 1182 : loss : 0.064203, loss_ce: 0.023128
2022-01-20 20:02:02,065 iteration 1183 : loss : 0.060080, loss_ce: 0.024815
2022-01-20 20:02:03,343 iteration 1184 : loss : 0.040474, loss_ce: 0.016859
2022-01-20 20:02:04,704 iteration 1185 : loss : 0.064518, loss_ce: 0.023942
2022-01-20 20:02:06,008 iteration 1186 : loss : 0.058343, loss_ce: 0.021177
2022-01-20 20:02:07,367 iteration 1187 : loss : 0.051558, loss_ce: 0.018309
2022-01-20 20:02:08,656 iteration 1188 : loss : 0.045258, loss_ce: 0.015337
2022-01-20 20:02:09,994 iteration 1189 : loss : 0.044100, loss_ce: 0.015911
2022-01-20 20:02:09,994 Training Data Eval:
2022-01-20 20:02:16,600   Average segmentation loss on training set: 0.0434
2022-01-20 20:02:16,600 Validation Data Eval:
2022-01-20 20:02:18,859   Average segmentation loss on validation set: 0.0933
2022-01-20 20:02:20,219 iteration 1190 : loss : 0.048496, loss_ce: 0.019533
 18%|█████▎                        | 70/400 [29:20<2:24:05, 26.20s/it]2022-01-20 20:02:21,571 iteration 1191 : loss : 0.046769, loss_ce: 0.021119
2022-01-20 20:02:22,870 iteration 1192 : loss : 0.054447, loss_ce: 0.024246
2022-01-20 20:02:24,211 iteration 1193 : loss : 0.060012, loss_ce: 0.019033
2022-01-20 20:02:25,515 iteration 1194 : loss : 0.066635, loss_ce: 0.022577
2022-01-20 20:02:26,893 iteration 1195 : loss : 0.053369, loss_ce: 0.015134
2022-01-20 20:02:28,165 iteration 1196 : loss : 0.050618, loss_ce: 0.015726
2022-01-20 20:02:29,533 iteration 1197 : loss : 0.046524, loss_ce: 0.022775
2022-01-20 20:02:30,878 iteration 1198 : loss : 0.041729, loss_ce: 0.020257
2022-01-20 20:02:32,238 iteration 1199 : loss : 0.052273, loss_ce: 0.017430
2022-01-20 20:02:33,622 iteration 1200 : loss : 0.061377, loss_ce: 0.022753
2022-01-20 20:02:34,962 iteration 1201 : loss : 0.036705, loss_ce: 0.018567
2022-01-20 20:02:36,356 iteration 1202 : loss : 0.045558, loss_ce: 0.021625
2022-01-20 20:02:37,693 iteration 1203 : loss : 0.048869, loss_ce: 0.016616
2022-01-20 20:02:39,026 iteration 1204 : loss : 0.058740, loss_ce: 0.030560
2022-01-20 20:02:40,403 iteration 1205 : loss : 0.071049, loss_ce: 0.039628
2022-01-20 20:02:41,798 iteration 1206 : loss : 0.106999, loss_ce: 0.028057
2022-01-20 20:02:43,106 iteration 1207 : loss : 0.061284, loss_ce: 0.022724
 18%|█████▎                        | 71/400 [29:43<2:18:13, 25.21s/it]2022-01-20 20:02:44,433 iteration 1208 : loss : 0.040274, loss_ce: 0.019544
2022-01-20 20:02:45,775 iteration 1209 : loss : 0.056210, loss_ce: 0.015845
2022-01-20 20:02:47,046 iteration 1210 : loss : 0.038362, loss_ce: 0.012281
2022-01-20 20:02:48,420 iteration 1211 : loss : 0.062358, loss_ce: 0.021019
2022-01-20 20:02:49,719 iteration 1212 : loss : 0.034269, loss_ce: 0.013713
2022-01-20 20:02:51,063 iteration 1213 : loss : 0.087914, loss_ce: 0.022332
2022-01-20 20:02:52,429 iteration 1214 : loss : 0.039874, loss_ce: 0.016715
2022-01-20 20:02:53,767 iteration 1215 : loss : 0.048111, loss_ce: 0.021107
2022-01-20 20:02:55,087 iteration 1216 : loss : 0.041148, loss_ce: 0.016179
2022-01-20 20:02:56,469 iteration 1217 : loss : 0.044649, loss_ce: 0.019005
2022-01-20 20:02:57,716 iteration 1218 : loss : 0.039777, loss_ce: 0.018279
2022-01-20 20:02:59,085 iteration 1219 : loss : 0.051323, loss_ce: 0.017902
2022-01-20 20:03:00,407 iteration 1220 : loss : 0.042829, loss_ce: 0.014230
2022-01-20 20:03:01,859 iteration 1221 : loss : 0.054121, loss_ce: 0.021113
2022-01-20 20:03:03,245 iteration 1222 : loss : 0.050156, loss_ce: 0.017192
2022-01-20 20:03:04,569 iteration 1223 : loss : 0.039282, loss_ce: 0.014869
2022-01-20 20:03:05,941 iteration 1224 : loss : 0.055910, loss_ce: 0.025230
 18%|█████▍                        | 72/400 [30:06<2:13:53, 24.49s/it]2022-01-20 20:03:07,352 iteration 1225 : loss : 0.055856, loss_ce: 0.023298
2022-01-20 20:03:08,707 iteration 1226 : loss : 0.052847, loss_ce: 0.020592
2022-01-20 20:03:10,015 iteration 1227 : loss : 0.040961, loss_ce: 0.012282
2022-01-20 20:03:11,456 iteration 1228 : loss : 0.075200, loss_ce: 0.022180
2022-01-20 20:03:12,868 iteration 1229 : loss : 0.045642, loss_ce: 0.018047
2022-01-20 20:03:14,262 iteration 1230 : loss : 0.053200, loss_ce: 0.021333
2022-01-20 20:03:15,603 iteration 1231 : loss : 0.070962, loss_ce: 0.028824
2022-01-20 20:03:17,013 iteration 1232 : loss : 0.040594, loss_ce: 0.016761
2022-01-20 20:03:18,484 iteration 1233 : loss : 0.060052, loss_ce: 0.025404
2022-01-20 20:03:19,853 iteration 1234 : loss : 0.066579, loss_ce: 0.030294
2022-01-20 20:03:21,182 iteration 1235 : loss : 0.052520, loss_ce: 0.022662
2022-01-20 20:03:22,489 iteration 1236 : loss : 0.032498, loss_ce: 0.012530
2022-01-20 20:03:23,757 iteration 1237 : loss : 0.057466, loss_ce: 0.017957
2022-01-20 20:03:25,093 iteration 1238 : loss : 0.051184, loss_ce: 0.021985
2022-01-20 20:03:26,475 iteration 1239 : loss : 0.052754, loss_ce: 0.017896
2022-01-20 20:03:27,747 iteration 1240 : loss : 0.036946, loss_ce: 0.013861
2022-01-20 20:03:29,167 iteration 1241 : loss : 0.036397, loss_ce: 0.014385
 18%|█████▍                        | 73/400 [30:29<2:11:26, 24.12s/it]2022-01-20 20:03:30,475 iteration 1242 : loss : 0.045274, loss_ce: 0.018524
2022-01-20 20:03:31,746 iteration 1243 : loss : 0.041264, loss_ce: 0.021513
2022-01-20 20:03:33,032 iteration 1244 : loss : 0.058323, loss_ce: 0.019865
2022-01-20 20:03:34,354 iteration 1245 : loss : 0.042638, loss_ce: 0.015932
2022-01-20 20:03:35,699 iteration 1246 : loss : 0.060261, loss_ce: 0.025458
2022-01-20 20:03:37,071 iteration 1247 : loss : 0.079487, loss_ce: 0.034972
2022-01-20 20:03:38,370 iteration 1248 : loss : 0.031539, loss_ce: 0.012003
2022-01-20 20:03:39,722 iteration 1249 : loss : 0.052211, loss_ce: 0.016965
2022-01-20 20:03:41,028 iteration 1250 : loss : 0.049127, loss_ce: 0.017909
2022-01-20 20:03:42,387 iteration 1251 : loss : 0.039705, loss_ce: 0.015523
2022-01-20 20:03:43,742 iteration 1252 : loss : 0.080424, loss_ce: 0.023212
2022-01-20 20:03:45,098 iteration 1253 : loss : 0.053774, loss_ce: 0.020989
2022-01-20 20:03:46,420 iteration 1254 : loss : 0.053746, loss_ce: 0.024268
2022-01-20 20:03:47,801 iteration 1255 : loss : 0.037197, loss_ce: 0.016035
2022-01-20 20:03:49,208 iteration 1256 : loss : 0.045167, loss_ce: 0.016382
2022-01-20 20:03:50,554 iteration 1257 : loss : 0.038427, loss_ce: 0.016902
2022-01-20 20:03:51,986 iteration 1258 : loss : 0.046531, loss_ce: 0.020220
 18%|█████▌                        | 74/400 [30:52<2:08:54, 23.73s/it]2022-01-20 20:03:53,377 iteration 1259 : loss : 0.053281, loss_ce: 0.019956
2022-01-20 20:03:54,775 iteration 1260 : loss : 0.043896, loss_ce: 0.013352
2022-01-20 20:03:56,148 iteration 1261 : loss : 0.048364, loss_ce: 0.018986
2022-01-20 20:03:57,462 iteration 1262 : loss : 0.040036, loss_ce: 0.020322
2022-01-20 20:03:58,874 iteration 1263 : loss : 0.042033, loss_ce: 0.014740
2022-01-20 20:04:00,162 iteration 1264 : loss : 0.043623, loss_ce: 0.021091
2022-01-20 20:04:01,464 iteration 1265 : loss : 0.043999, loss_ce: 0.016930
2022-01-20 20:04:02,741 iteration 1266 : loss : 0.040270, loss_ce: 0.017359
2022-01-20 20:04:04,172 iteration 1267 : loss : 0.041446, loss_ce: 0.013938
2022-01-20 20:04:05,509 iteration 1268 : loss : 0.118441, loss_ce: 0.034733
2022-01-20 20:04:06,888 iteration 1269 : loss : 0.060441, loss_ce: 0.021448
2022-01-20 20:04:08,183 iteration 1270 : loss : 0.046362, loss_ce: 0.016090
2022-01-20 20:04:09,515 iteration 1271 : loss : 0.058549, loss_ce: 0.025923
2022-01-20 20:04:10,838 iteration 1272 : loss : 0.065716, loss_ce: 0.026119
2022-01-20 20:04:12,215 iteration 1273 : loss : 0.061633, loss_ce: 0.032113
2022-01-20 20:04:13,570 iteration 1274 : loss : 0.069785, loss_ce: 0.031158
2022-01-20 20:04:13,570 Training Data Eval:
2022-01-20 20:04:20,179   Average segmentation loss on training set: 0.0399
2022-01-20 20:04:20,179 Validation Data Eval:
2022-01-20 20:04:22,445   Average segmentation loss on validation set: 0.1311
2022-01-20 20:04:23,810 iteration 1275 : loss : 0.058430, loss_ce: 0.020057
 19%|█████▋                        | 75/400 [31:23<2:21:40, 26.16s/it]2022-01-20 20:04:25,283 iteration 1276 : loss : 0.091066, loss_ce: 0.026604
2022-01-20 20:04:26,580 iteration 1277 : loss : 0.040373, loss_ce: 0.016632
2022-01-20 20:04:27,953 iteration 1278 : loss : 0.041964, loss_ce: 0.019064
2022-01-20 20:04:29,196 iteration 1279 : loss : 0.041326, loss_ce: 0.020672
2022-01-20 20:04:30,447 iteration 1280 : loss : 0.046639, loss_ce: 0.022804
2022-01-20 20:04:31,849 iteration 1281 : loss : 0.051842, loss_ce: 0.023835
2022-01-20 20:04:33,147 iteration 1282 : loss : 0.053383, loss_ce: 0.031148
2022-01-20 20:04:34,469 iteration 1283 : loss : 0.038740, loss_ce: 0.015181
2022-01-20 20:04:35,809 iteration 1284 : loss : 0.039984, loss_ce: 0.017307
2022-01-20 20:04:37,291 iteration 1285 : loss : 0.060093, loss_ce: 0.021521
2022-01-20 20:04:38,676 iteration 1286 : loss : 0.055632, loss_ce: 0.020392
2022-01-20 20:04:40,001 iteration 1287 : loss : 0.050794, loss_ce: 0.018555
2022-01-20 20:04:41,364 iteration 1288 : loss : 0.061143, loss_ce: 0.022178
2022-01-20 20:04:42,783 iteration 1289 : loss : 0.043995, loss_ce: 0.016533
2022-01-20 20:04:44,095 iteration 1290 : loss : 0.052872, loss_ce: 0.016389
2022-01-20 20:04:45,444 iteration 1291 : loss : 0.052271, loss_ce: 0.017400
2022-01-20 20:04:46,733 iteration 1292 : loss : 0.034962, loss_ce: 0.013063
 19%|█████▋                        | 76/400 [31:46<2:15:59, 25.18s/it]2022-01-20 20:04:48,109 iteration 1293 : loss : 0.063088, loss_ce: 0.031016
2022-01-20 20:04:49,409 iteration 1294 : loss : 0.057676, loss_ce: 0.018642
2022-01-20 20:04:50,729 iteration 1295 : loss : 0.044836, loss_ce: 0.018169
2022-01-20 20:04:52,058 iteration 1296 : loss : 0.049844, loss_ce: 0.023736
2022-01-20 20:04:53,385 iteration 1297 : loss : 0.052332, loss_ce: 0.023279
2022-01-20 20:04:54,730 iteration 1298 : loss : 0.039760, loss_ce: 0.017390
2022-01-20 20:04:56,024 iteration 1299 : loss : 0.050838, loss_ce: 0.017511
2022-01-20 20:04:57,418 iteration 1300 : loss : 0.087017, loss_ce: 0.026060
2022-01-20 20:04:58,772 iteration 1301 : loss : 0.065148, loss_ce: 0.027363
2022-01-20 20:05:00,167 iteration 1302 : loss : 0.061004, loss_ce: 0.020215
2022-01-20 20:05:01,469 iteration 1303 : loss : 0.056051, loss_ce: 0.024858
2022-01-20 20:05:02,854 iteration 1304 : loss : 0.074917, loss_ce: 0.021682
2022-01-20 20:05:04,208 iteration 1305 : loss : 0.049767, loss_ce: 0.022416
2022-01-20 20:05:05,617 iteration 1306 : loss : 0.052384, loss_ce: 0.017981
2022-01-20 20:05:07,037 iteration 1307 : loss : 0.053445, loss_ce: 0.018689
2022-01-20 20:05:08,402 iteration 1308 : loss : 0.083089, loss_ce: 0.045235
2022-01-20 20:05:09,759 iteration 1309 : loss : 0.065814, loss_ce: 0.025951
 19%|█████▊                        | 77/400 [32:09<2:12:05, 24.54s/it]2022-01-20 20:05:11,163 iteration 1310 : loss : 0.054130, loss_ce: 0.025147
2022-01-20 20:05:12,586 iteration 1311 : loss : 0.049213, loss_ce: 0.019531
2022-01-20 20:05:13,923 iteration 1312 : loss : 0.049530, loss_ce: 0.024567
2022-01-20 20:05:15,205 iteration 1313 : loss : 0.041693, loss_ce: 0.019442
2022-01-20 20:05:16,560 iteration 1314 : loss : 0.042357, loss_ce: 0.019983
2022-01-20 20:05:17,925 iteration 1315 : loss : 0.052955, loss_ce: 0.024092
2022-01-20 20:05:19,251 iteration 1316 : loss : 0.072190, loss_ce: 0.036064
2022-01-20 20:05:20,633 iteration 1317 : loss : 0.051233, loss_ce: 0.027327
2022-01-20 20:05:22,025 iteration 1318 : loss : 0.063387, loss_ce: 0.025744
2022-01-20 20:05:23,347 iteration 1319 : loss : 0.054634, loss_ce: 0.016682
2022-01-20 20:05:24,684 iteration 1320 : loss : 0.044386, loss_ce: 0.016092
2022-01-20 20:05:26,032 iteration 1321 : loss : 0.055447, loss_ce: 0.021613
2022-01-20 20:05:27,475 iteration 1322 : loss : 0.049281, loss_ce: 0.017555
2022-01-20 20:05:28,859 iteration 1323 : loss : 0.039490, loss_ce: 0.015985
2022-01-20 20:05:30,236 iteration 1324 : loss : 0.043410, loss_ce: 0.015705
2022-01-20 20:05:31,626 iteration 1325 : loss : 0.039556, loss_ce: 0.012963
2022-01-20 20:05:32,914 iteration 1326 : loss : 0.044616, loss_ce: 0.015509
 20%|█████▊                        | 78/400 [32:33<2:09:28, 24.12s/it]2022-01-20 20:05:34,321 iteration 1327 : loss : 0.053270, loss_ce: 0.026483
2022-01-20 20:05:35,764 iteration 1328 : loss : 0.057900, loss_ce: 0.020161
2022-01-20 20:05:37,045 iteration 1329 : loss : 0.061871, loss_ce: 0.030006
2022-01-20 20:05:38,387 iteration 1330 : loss : 0.056868, loss_ce: 0.026594
2022-01-20 20:05:39,724 iteration 1331 : loss : 0.040044, loss_ce: 0.013796
2022-01-20 20:05:41,052 iteration 1332 : loss : 0.040299, loss_ce: 0.017430
2022-01-20 20:05:42,419 iteration 1333 : loss : 0.069251, loss_ce: 0.014740
2022-01-20 20:05:43,749 iteration 1334 : loss : 0.034546, loss_ce: 0.013444
2022-01-20 20:05:45,202 iteration 1335 : loss : 0.044772, loss_ce: 0.014878
2022-01-20 20:05:46,587 iteration 1336 : loss : 0.038242, loss_ce: 0.015430
2022-01-20 20:05:48,011 iteration 1337 : loss : 0.046601, loss_ce: 0.021066
2022-01-20 20:05:49,435 iteration 1338 : loss : 0.049155, loss_ce: 0.020522
2022-01-20 20:05:50,823 iteration 1339 : loss : 0.043378, loss_ce: 0.017766
2022-01-20 20:05:52,126 iteration 1340 : loss : 0.055823, loss_ce: 0.015623
2022-01-20 20:05:53,461 iteration 1341 : loss : 0.041999, loss_ce: 0.019084
2022-01-20 20:05:54,798 iteration 1342 : loss : 0.052801, loss_ce: 0.017490
2022-01-20 20:05:56,117 iteration 1343 : loss : 0.049283, loss_ce: 0.019232
 20%|█████▉                        | 79/400 [32:56<2:07:34, 23.85s/it]2022-01-20 20:05:57,474 iteration 1344 : loss : 0.036504, loss_ce: 0.013758
2022-01-20 20:05:58,782 iteration 1345 : loss : 0.042573, loss_ce: 0.017943
2022-01-20 20:06:00,078 iteration 1346 : loss : 0.033796, loss_ce: 0.012122
2022-01-20 20:06:01,489 iteration 1347 : loss : 0.106558, loss_ce: 0.026283
2022-01-20 20:06:02,908 iteration 1348 : loss : 0.050119, loss_ce: 0.020960
2022-01-20 20:06:04,272 iteration 1349 : loss : 0.058289, loss_ce: 0.025115
2022-01-20 20:06:05,659 iteration 1350 : loss : 0.060002, loss_ce: 0.024531
2022-01-20 20:06:06,957 iteration 1351 : loss : 0.059121, loss_ce: 0.018617
2022-01-20 20:06:08,380 iteration 1352 : loss : 0.041734, loss_ce: 0.014777
2022-01-20 20:06:09,657 iteration 1353 : loss : 0.045577, loss_ce: 0.020282
2022-01-20 20:06:11,028 iteration 1354 : loss : 0.041294, loss_ce: 0.015945
2022-01-20 20:06:12,316 iteration 1355 : loss : 0.061722, loss_ce: 0.018870
2022-01-20 20:06:13,696 iteration 1356 : loss : 0.047595, loss_ce: 0.017292
2022-01-20 20:06:14,987 iteration 1357 : loss : 0.045746, loss_ce: 0.025543
2022-01-20 20:06:16,326 iteration 1358 : loss : 0.047617, loss_ce: 0.015927
2022-01-20 20:06:17,611 iteration 1359 : loss : 0.046727, loss_ce: 0.024118
2022-01-20 20:06:17,611 Training Data Eval:
2022-01-20 20:06:24,216   Average segmentation loss on training set: 0.0406
2022-01-20 20:06:24,216 Validation Data Eval:
2022-01-20 20:06:26,472   Average segmentation loss on validation set: 0.0740
2022-01-20 20:06:27,793 iteration 1360 : loss : 0.059699, loss_ce: 0.022305
 20%|██████                        | 80/400 [33:27<2:19:42, 26.20s/it]2022-01-20 20:06:29,211 iteration 1361 : loss : 0.081380, loss_ce: 0.024447
2022-01-20 20:06:30,572 iteration 1362 : loss : 0.037112, loss_ce: 0.016291
2022-01-20 20:06:31,940 iteration 1363 : loss : 0.033754, loss_ce: 0.011988
2022-01-20 20:06:33,340 iteration 1364 : loss : 0.048464, loss_ce: 0.016653
2022-01-20 20:06:34,781 iteration 1365 : loss : 0.062956, loss_ce: 0.024806
2022-01-20 20:06:36,133 iteration 1366 : loss : 0.059728, loss_ce: 0.026998
2022-01-20 20:06:37,437 iteration 1367 : loss : 0.038043, loss_ce: 0.016649
2022-01-20 20:06:38,788 iteration 1368 : loss : 0.071677, loss_ce: 0.036045
2022-01-20 20:06:40,111 iteration 1369 : loss : 0.082737, loss_ce: 0.024593
2022-01-20 20:06:41,458 iteration 1370 : loss : 0.060091, loss_ce: 0.019229
2022-01-20 20:06:42,741 iteration 1371 : loss : 0.037008, loss_ce: 0.014132
2022-01-20 20:06:44,105 iteration 1372 : loss : 0.082884, loss_ce: 0.024608
2022-01-20 20:06:45,543 iteration 1373 : loss : 0.056215, loss_ce: 0.029804
2022-01-20 20:06:46,882 iteration 1374 : loss : 0.048255, loss_ce: 0.016018
2022-01-20 20:06:48,284 iteration 1375 : loss : 0.043973, loss_ce: 0.015133
2022-01-20 20:06:49,558 iteration 1376 : loss : 0.042341, loss_ce: 0.017579
2022-01-20 20:06:50,951 iteration 1377 : loss : 0.079972, loss_ce: 0.028390
 20%|██████                        | 81/400 [33:51<2:14:25, 25.28s/it]2022-01-20 20:06:52,410 iteration 1378 : loss : 0.045078, loss_ce: 0.020942
2022-01-20 20:06:53,827 iteration 1379 : loss : 0.064773, loss_ce: 0.017880
2022-01-20 20:06:55,182 iteration 1380 : loss : 0.037928, loss_ce: 0.016424
2022-01-20 20:06:56,547 iteration 1381 : loss : 0.039420, loss_ce: 0.017631
2022-01-20 20:06:57,880 iteration 1382 : loss : 0.038989, loss_ce: 0.015012
2022-01-20 20:06:59,226 iteration 1383 : loss : 0.061993, loss_ce: 0.020112
2022-01-20 20:07:00,662 iteration 1384 : loss : 0.073665, loss_ce: 0.036674
2022-01-20 20:07:01,941 iteration 1385 : loss : 0.032509, loss_ce: 0.012466
2022-01-20 20:07:03,374 iteration 1386 : loss : 0.054368, loss_ce: 0.023937
2022-01-20 20:07:04,688 iteration 1387 : loss : 0.065632, loss_ce: 0.020046
2022-01-20 20:07:06,077 iteration 1388 : loss : 0.043203, loss_ce: 0.013947
2022-01-20 20:07:07,453 iteration 1389 : loss : 0.058711, loss_ce: 0.031827
2022-01-20 20:07:08,737 iteration 1390 : loss : 0.062704, loss_ce: 0.031208
2022-01-20 20:07:10,102 iteration 1391 : loss : 0.053411, loss_ce: 0.018682
2022-01-20 20:07:11,439 iteration 1392 : loss : 0.052645, loss_ce: 0.027864
2022-01-20 20:07:12,793 iteration 1393 : loss : 0.048305, loss_ce: 0.016547
2022-01-20 20:07:14,140 iteration 1394 : loss : 0.047005, loss_ce: 0.013797
 20%|██████▏                       | 82/400 [34:14<2:10:40, 24.65s/it]2022-01-20 20:07:15,573 iteration 1395 : loss : 0.059846, loss_ce: 0.027986
2022-01-20 20:07:16,856 iteration 1396 : loss : 0.041854, loss_ce: 0.018968
2022-01-20 20:07:18,251 iteration 1397 : loss : 0.042415, loss_ce: 0.016771
2022-01-20 20:07:19,522 iteration 1398 : loss : 0.037503, loss_ce: 0.013459
2022-01-20 20:07:20,889 iteration 1399 : loss : 0.038230, loss_ce: 0.015894
2022-01-20 20:07:22,300 iteration 1400 : loss : 0.057998, loss_ce: 0.023021
2022-01-20 20:07:23,620 iteration 1401 : loss : 0.036106, loss_ce: 0.012925
2022-01-20 20:07:25,035 iteration 1402 : loss : 0.047923, loss_ce: 0.018023
2022-01-20 20:07:26,398 iteration 1403 : loss : 0.050944, loss_ce: 0.019518
2022-01-20 20:07:27,825 iteration 1404 : loss : 0.053046, loss_ce: 0.022492
2022-01-20 20:07:29,026 iteration 1405 : loss : 0.034269, loss_ce: 0.011184
2022-01-20 20:07:30,400 iteration 1406 : loss : 0.039200, loss_ce: 0.013737
2022-01-20 20:07:31,716 iteration 1407 : loss : 0.042259, loss_ce: 0.017346
2022-01-20 20:07:33,007 iteration 1408 : loss : 0.048245, loss_ce: 0.021101
2022-01-20 20:07:34,429 iteration 1409 : loss : 0.069177, loss_ce: 0.017326
2022-01-20 20:07:35,731 iteration 1410 : loss : 0.045235, loss_ce: 0.016421
2022-01-20 20:07:37,001 iteration 1411 : loss : 0.033312, loss_ce: 0.012981
 21%|██████▏                       | 83/400 [34:37<2:07:24, 24.12s/it]2022-01-20 20:07:38,408 iteration 1412 : loss : 0.067079, loss_ce: 0.032146
2022-01-20 20:07:39,684 iteration 1413 : loss : 0.028922, loss_ce: 0.010456
2022-01-20 20:07:41,054 iteration 1414 : loss : 0.061487, loss_ce: 0.029134
2022-01-20 20:07:42,339 iteration 1415 : loss : 0.037738, loss_ce: 0.015092
2022-01-20 20:07:43,674 iteration 1416 : loss : 0.033376, loss_ce: 0.013087
2022-01-20 20:07:45,033 iteration 1417 : loss : 0.050751, loss_ce: 0.019396
2022-01-20 20:07:46,381 iteration 1418 : loss : 0.034910, loss_ce: 0.011482
2022-01-20 20:07:47,650 iteration 1419 : loss : 0.034621, loss_ce: 0.013566
2022-01-20 20:07:48,971 iteration 1420 : loss : 0.044568, loss_ce: 0.011791
2022-01-20 20:07:50,302 iteration 1421 : loss : 0.053048, loss_ce: 0.029104
2022-01-20 20:07:51,656 iteration 1422 : loss : 0.047235, loss_ce: 0.022468
2022-01-20 20:07:52,958 iteration 1423 : loss : 0.038524, loss_ce: 0.011621
2022-01-20 20:07:54,218 iteration 1424 : loss : 0.028427, loss_ce: 0.011322
2022-01-20 20:07:55,575 iteration 1425 : loss : 0.051427, loss_ce: 0.015355
2022-01-20 20:07:56,944 iteration 1426 : loss : 0.044387, loss_ce: 0.024914
2022-01-20 20:07:58,287 iteration 1427 : loss : 0.084260, loss_ce: 0.014514
2022-01-20 20:07:59,733 iteration 1428 : loss : 0.058673, loss_ce: 0.021929
 21%|██████▎                       | 84/400 [34:59<2:04:50, 23.70s/it]2022-01-20 20:08:01,145 iteration 1429 : loss : 0.046706, loss_ce: 0.023025
2022-01-20 20:08:02,476 iteration 1430 : loss : 0.034727, loss_ce: 0.013029
2022-01-20 20:08:03,767 iteration 1431 : loss : 0.055093, loss_ce: 0.019524
2022-01-20 20:08:05,113 iteration 1432 : loss : 0.044538, loss_ce: 0.017863
2022-01-20 20:08:06,388 iteration 1433 : loss : 0.042325, loss_ce: 0.017321
2022-01-20 20:08:07,814 iteration 1434 : loss : 0.059233, loss_ce: 0.027984
2022-01-20 20:08:09,194 iteration 1435 : loss : 0.042023, loss_ce: 0.017364
2022-01-20 20:08:10,593 iteration 1436 : loss : 0.044382, loss_ce: 0.021419
2022-01-20 20:08:11,940 iteration 1437 : loss : 0.039335, loss_ce: 0.017252
2022-01-20 20:08:13,273 iteration 1438 : loss : 0.043883, loss_ce: 0.016949
2022-01-20 20:08:14,631 iteration 1439 : loss : 0.100538, loss_ce: 0.029791
2022-01-20 20:08:15,923 iteration 1440 : loss : 0.045170, loss_ce: 0.014340
2022-01-20 20:08:17,281 iteration 1441 : loss : 0.050158, loss_ce: 0.019546
2022-01-20 20:08:18,619 iteration 1442 : loss : 0.050729, loss_ce: 0.013532
2022-01-20 20:08:20,020 iteration 1443 : loss : 0.054212, loss_ce: 0.019126
2022-01-20 20:08:21,359 iteration 1444 : loss : 0.039226, loss_ce: 0.009556
2022-01-20 20:08:21,359 Training Data Eval:
2022-01-20 20:08:27,970   Average segmentation loss on training set: 0.0386
2022-01-20 20:08:27,971 Validation Data Eval:
2022-01-20 20:08:30,234   Average segmentation loss on validation set: 0.0924
2022-01-20 20:08:31,614 iteration 1445 : loss : 0.074485, loss_ce: 0.036785
 21%|██████▍                       | 85/400 [35:31<2:17:19, 26.16s/it]2022-01-20 20:08:32,928 iteration 1446 : loss : 0.038512, loss_ce: 0.013716
2022-01-20 20:08:34,189 iteration 1447 : loss : 0.034809, loss_ce: 0.016716
2022-01-20 20:08:35,579 iteration 1448 : loss : 0.070460, loss_ce: 0.016678
2022-01-20 20:08:36,966 iteration 1449 : loss : 0.050458, loss_ce: 0.015866
2022-01-20 20:08:38,291 iteration 1450 : loss : 0.047302, loss_ce: 0.017613
2022-01-20 20:08:39,655 iteration 1451 : loss : 0.044363, loss_ce: 0.014510
2022-01-20 20:08:40,991 iteration 1452 : loss : 0.050316, loss_ce: 0.028513
2022-01-20 20:08:42,362 iteration 1453 : loss : 0.047399, loss_ce: 0.020184
2022-01-20 20:08:43,709 iteration 1454 : loss : 0.065439, loss_ce: 0.026177
2022-01-20 20:08:45,071 iteration 1455 : loss : 0.042784, loss_ce: 0.015467
2022-01-20 20:08:46,375 iteration 1456 : loss : 0.042206, loss_ce: 0.012424
2022-01-20 20:08:47,773 iteration 1457 : loss : 0.042889, loss_ce: 0.018238
2022-01-20 20:08:49,141 iteration 1458 : loss : 0.038969, loss_ce: 0.016948
2022-01-20 20:08:50,460 iteration 1459 : loss : 0.037987, loss_ce: 0.012890
2022-01-20 20:08:51,788 iteration 1460 : loss : 0.043216, loss_ce: 0.019057
2022-01-20 20:08:53,076 iteration 1461 : loss : 0.045529, loss_ce: 0.014882
2022-01-20 20:08:54,476 iteration 1462 : loss : 0.052918, loss_ce: 0.024125
 22%|██████▍                       | 86/400 [35:54<2:11:42, 25.17s/it]2022-01-20 20:08:55,971 iteration 1463 : loss : 0.106767, loss_ce: 0.036246
2022-01-20 20:08:57,277 iteration 1464 : loss : 0.043067, loss_ce: 0.021023
2022-01-20 20:08:58,753 iteration 1465 : loss : 0.055920, loss_ce: 0.018688
2022-01-20 20:09:00,116 iteration 1466 : loss : 0.036883, loss_ce: 0.013116
2022-01-20 20:09:01,430 iteration 1467 : loss : 0.047501, loss_ce: 0.016777
2022-01-20 20:09:02,790 iteration 1468 : loss : 0.036663, loss_ce: 0.016793
2022-01-20 20:09:04,127 iteration 1469 : loss : 0.052515, loss_ce: 0.020459
2022-01-20 20:09:05,420 iteration 1470 : loss : 0.063179, loss_ce: 0.014206
2022-01-20 20:09:06,759 iteration 1471 : loss : 0.039973, loss_ce: 0.015841
2022-01-20 20:09:08,129 iteration 1472 : loss : 0.061999, loss_ce: 0.021942
2022-01-20 20:09:09,433 iteration 1473 : loss : 0.036246, loss_ce: 0.017037
2022-01-20 20:09:10,813 iteration 1474 : loss : 0.068837, loss_ce: 0.024702
2022-01-20 20:09:12,131 iteration 1475 : loss : 0.047711, loss_ce: 0.020399
2022-01-20 20:09:13,447 iteration 1476 : loss : 0.036953, loss_ce: 0.015109
2022-01-20 20:09:14,867 iteration 1477 : loss : 0.052176, loss_ce: 0.025385
2022-01-20 20:09:16,172 iteration 1478 : loss : 0.064100, loss_ce: 0.018327
2022-01-20 20:09:17,512 iteration 1479 : loss : 0.051753, loss_ce: 0.015248
 22%|██████▌                       | 87/400 [36:17<2:07:57, 24.53s/it]2022-01-20 20:09:18,776 iteration 1480 : loss : 0.032771, loss_ce: 0.011396
2022-01-20 20:09:20,203 iteration 1481 : loss : 0.045550, loss_ce: 0.018990
2022-01-20 20:09:21,540 iteration 1482 : loss : 0.054979, loss_ce: 0.020160
2022-01-20 20:09:22,938 iteration 1483 : loss : 0.039937, loss_ce: 0.016000
2022-01-20 20:09:24,311 iteration 1484 : loss : 0.042470, loss_ce: 0.012485
2022-01-20 20:09:25,698 iteration 1485 : loss : 0.032823, loss_ce: 0.015868
2022-01-20 20:09:27,032 iteration 1486 : loss : 0.049519, loss_ce: 0.015729
2022-01-20 20:09:28,371 iteration 1487 : loss : 0.037377, loss_ce: 0.014282
2022-01-20 20:09:29,655 iteration 1488 : loss : 0.048078, loss_ce: 0.022120
2022-01-20 20:09:30,973 iteration 1489 : loss : 0.058078, loss_ce: 0.025355
2022-01-20 20:09:32,339 iteration 1490 : loss : 0.059993, loss_ce: 0.020824
2022-01-20 20:09:33,598 iteration 1491 : loss : 0.053299, loss_ce: 0.016103
2022-01-20 20:09:34,882 iteration 1492 : loss : 0.040788, loss_ce: 0.016035
2022-01-20 20:09:36,257 iteration 1493 : loss : 0.045325, loss_ce: 0.018677
2022-01-20 20:09:37,641 iteration 1494 : loss : 0.044424, loss_ce: 0.014823
2022-01-20 20:09:38,924 iteration 1495 : loss : 0.047049, loss_ce: 0.012435
2022-01-20 20:09:40,297 iteration 1496 : loss : 0.052369, loss_ce: 0.023991
 22%|██████▌                       | 88/400 [36:40<2:04:49, 24.01s/it]2022-01-20 20:09:41,610 iteration 1497 : loss : 0.034604, loss_ce: 0.014165
2022-01-20 20:09:42,985 iteration 1498 : loss : 0.053060, loss_ce: 0.020291
2022-01-20 20:09:44,263 iteration 1499 : loss : 0.055396, loss_ce: 0.018539
2022-01-20 20:09:45,609 iteration 1500 : loss : 0.042580, loss_ce: 0.019164
2022-01-20 20:09:46,928 iteration 1501 : loss : 0.044736, loss_ce: 0.019129
2022-01-20 20:09:48,341 iteration 1502 : loss : 0.064036, loss_ce: 0.014463
2022-01-20 20:09:49,816 iteration 1503 : loss : 0.072031, loss_ce: 0.020064
2022-01-20 20:09:51,140 iteration 1504 : loss : 0.038137, loss_ce: 0.016383
2022-01-20 20:09:52,511 iteration 1505 : loss : 0.041489, loss_ce: 0.016180
2022-01-20 20:09:53,878 iteration 1506 : loss : 0.035745, loss_ce: 0.013877
2022-01-20 20:09:55,250 iteration 1507 : loss : 0.054160, loss_ce: 0.016581
2022-01-20 20:09:56,617 iteration 1508 : loss : 0.051639, loss_ce: 0.026349
2022-01-20 20:09:57,961 iteration 1509 : loss : 0.038512, loss_ce: 0.013701
2022-01-20 20:09:59,394 iteration 1510 : loss : 0.044896, loss_ce: 0.013321
2022-01-20 20:10:00,811 iteration 1511 : loss : 0.057808, loss_ce: 0.036369
2022-01-20 20:10:02,168 iteration 1512 : loss : 0.047469, loss_ce: 0.012591
2022-01-20 20:10:03,522 iteration 1513 : loss : 0.044507, loss_ce: 0.014287
 22%|██████▋                       | 89/400 [37:03<2:03:12, 23.77s/it]2022-01-20 20:10:04,946 iteration 1514 : loss : 0.037629, loss_ce: 0.011904
2022-01-20 20:10:06,242 iteration 1515 : loss : 0.038678, loss_ce: 0.014568
2022-01-20 20:10:07,589 iteration 1516 : loss : 0.054014, loss_ce: 0.020411
2022-01-20 20:10:08,875 iteration 1517 : loss : 0.031312, loss_ce: 0.011941
2022-01-20 20:10:10,245 iteration 1518 : loss : 0.042814, loss_ce: 0.016464
2022-01-20 20:10:11,655 iteration 1519 : loss : 0.044004, loss_ce: 0.017040
2022-01-20 20:10:13,051 iteration 1520 : loss : 0.047976, loss_ce: 0.019376
2022-01-20 20:10:14,404 iteration 1521 : loss : 0.057694, loss_ce: 0.018344
2022-01-20 20:10:15,713 iteration 1522 : loss : 0.047091, loss_ce: 0.020525
2022-01-20 20:10:17,027 iteration 1523 : loss : 0.037478, loss_ce: 0.010982
2022-01-20 20:10:18,379 iteration 1524 : loss : 0.048405, loss_ce: 0.016855
2022-01-20 20:10:19,654 iteration 1525 : loss : 0.034489, loss_ce: 0.014083
2022-01-20 20:10:21,021 iteration 1526 : loss : 0.053138, loss_ce: 0.022633
2022-01-20 20:10:22,401 iteration 1527 : loss : 0.051371, loss_ce: 0.026231
2022-01-20 20:10:23,702 iteration 1528 : loss : 0.039487, loss_ce: 0.017427
2022-01-20 20:10:24,997 iteration 1529 : loss : 0.040861, loss_ce: 0.014750
2022-01-20 20:10:24,998 Training Data Eval:
2022-01-20 20:10:31,597   Average segmentation loss on training set: 0.0311
2022-01-20 20:10:31,597 Validation Data Eval:
2022-01-20 20:10:33,864   Average segmentation loss on validation set: 0.0962
2022-01-20 20:10:35,245 iteration 1530 : loss : 0.056550, loss_ce: 0.023222
 22%|██████▊                       | 90/400 [37:35<2:15:08, 26.16s/it]2022-01-20 20:10:36,646 iteration 1531 : loss : 0.035526, loss_ce: 0.014100
2022-01-20 20:10:37,982 iteration 1532 : loss : 0.038896, loss_ce: 0.013901
2022-01-20 20:10:39,270 iteration 1533 : loss : 0.029487, loss_ce: 0.010666
2022-01-20 20:10:40,588 iteration 1534 : loss : 0.039977, loss_ce: 0.020017
2022-01-20 20:10:41,984 iteration 1535 : loss : 0.044802, loss_ce: 0.015428
2022-01-20 20:10:43,378 iteration 1536 : loss : 0.055144, loss_ce: 0.028224
2022-01-20 20:10:44,711 iteration 1537 : loss : 0.047098, loss_ce: 0.014405
2022-01-20 20:10:46,090 iteration 1538 : loss : 0.030866, loss_ce: 0.011885
2022-01-20 20:10:47,437 iteration 1539 : loss : 0.046794, loss_ce: 0.021441
2022-01-20 20:10:48,772 iteration 1540 : loss : 0.049810, loss_ce: 0.020844
2022-01-20 20:10:50,081 iteration 1541 : loss : 0.029460, loss_ce: 0.012061
2022-01-20 20:10:51,369 iteration 1542 : loss : 0.033782, loss_ce: 0.013625
2022-01-20 20:10:52,684 iteration 1543 : loss : 0.070192, loss_ce: 0.023725
2022-01-20 20:10:53,956 iteration 1544 : loss : 0.031268, loss_ce: 0.011680
2022-01-20 20:10:55,401 iteration 1545 : loss : 0.044567, loss_ce: 0.014946
2022-01-20 20:10:56,691 iteration 1546 : loss : 0.080122, loss_ce: 0.020356
2022-01-20 20:10:58,112 iteration 1547 : loss : 0.030624, loss_ce: 0.010869
 23%|██████▊                       | 91/400 [37:58<2:09:37, 25.17s/it]2022-01-20 20:10:59,514 iteration 1548 : loss : 0.048008, loss_ce: 0.020812
2022-01-20 20:11:00,768 iteration 1549 : loss : 0.037231, loss_ce: 0.014636
2022-01-20 20:11:02,069 iteration 1550 : loss : 0.053576, loss_ce: 0.022963
2022-01-20 20:11:03,303 iteration 1551 : loss : 0.037486, loss_ce: 0.012597
2022-01-20 20:11:04,657 iteration 1552 : loss : 0.035565, loss_ce: 0.011963
2022-01-20 20:11:06,042 iteration 1553 : loss : 0.060370, loss_ce: 0.025111
2022-01-20 20:11:07,409 iteration 1554 : loss : 0.042367, loss_ce: 0.016887
2022-01-20 20:11:08,736 iteration 1555 : loss : 0.048496, loss_ce: 0.020712
2022-01-20 20:11:10,112 iteration 1556 : loss : 0.035677, loss_ce: 0.012847
2022-01-20 20:11:11,431 iteration 1557 : loss : 0.054497, loss_ce: 0.022813
2022-01-20 20:11:12,770 iteration 1558 : loss : 0.047502, loss_ce: 0.023993
2022-01-20 20:11:14,130 iteration 1559 : loss : 0.040984, loss_ce: 0.014162
2022-01-20 20:11:15,427 iteration 1560 : loss : 0.030559, loss_ce: 0.014654
2022-01-20 20:11:16,803 iteration 1561 : loss : 0.042217, loss_ce: 0.014399
2022-01-20 20:11:18,108 iteration 1562 : loss : 0.085262, loss_ce: 0.021371
2022-01-20 20:11:19,506 iteration 1563 : loss : 0.053325, loss_ce: 0.022400
2022-01-20 20:11:20,886 iteration 1564 : loss : 0.047694, loss_ce: 0.018323
 23%|██████▉                       | 92/400 [38:21<2:05:30, 24.45s/it]2022-01-20 20:11:22,244 iteration 1565 : loss : 0.039660, loss_ce: 0.018018
2022-01-20 20:11:23,583 iteration 1566 : loss : 0.036571, loss_ce: 0.014224
2022-01-20 20:11:24,960 iteration 1567 : loss : 0.044595, loss_ce: 0.013278
2022-01-20 20:11:26,271 iteration 1568 : loss : 0.061099, loss_ce: 0.018568
2022-01-20 20:11:27,638 iteration 1569 : loss : 0.042942, loss_ce: 0.016837
2022-01-20 20:11:28,987 iteration 1570 : loss : 0.030831, loss_ce: 0.013349
2022-01-20 20:11:30,297 iteration 1571 : loss : 0.081192, loss_ce: 0.016072
2022-01-20 20:11:31,705 iteration 1572 : loss : 0.051819, loss_ce: 0.020206
2022-01-20 20:11:33,062 iteration 1573 : loss : 0.056247, loss_ce: 0.030644
2022-01-20 20:11:34,359 iteration 1574 : loss : 0.026598, loss_ce: 0.011553
2022-01-20 20:11:35,599 iteration 1575 : loss : 0.034044, loss_ce: 0.013375
2022-01-20 20:11:36,921 iteration 1576 : loss : 0.039701, loss_ce: 0.018166
2022-01-20 20:11:38,246 iteration 1577 : loss : 0.028365, loss_ce: 0.011226
2022-01-20 20:11:39,579 iteration 1578 : loss : 0.046840, loss_ce: 0.015850
2022-01-20 20:11:40,947 iteration 1579 : loss : 0.033418, loss_ce: 0.011621
2022-01-20 20:11:42,250 iteration 1580 : loss : 0.036649, loss_ce: 0.013625
2022-01-20 20:11:43,577 iteration 1581 : loss : 0.041967, loss_ce: 0.015858
 23%|██████▉                       | 93/400 [38:43<2:02:24, 23.92s/it]2022-01-20 20:11:45,029 iteration 1582 : loss : 0.042548, loss_ce: 0.014628
2022-01-20 20:11:46,349 iteration 1583 : loss : 0.051047, loss_ce: 0.013853
2022-01-20 20:11:47,711 iteration 1584 : loss : 0.032185, loss_ce: 0.017141
2022-01-20 20:11:49,044 iteration 1585 : loss : 0.036197, loss_ce: 0.011320
2022-01-20 20:11:50,366 iteration 1586 : loss : 0.048907, loss_ce: 0.024219
2022-01-20 20:11:51,602 iteration 1587 : loss : 0.030736, loss_ce: 0.012405
2022-01-20 20:11:52,962 iteration 1588 : loss : 0.035077, loss_ce: 0.015509
2022-01-20 20:11:54,299 iteration 1589 : loss : 0.028892, loss_ce: 0.011524
2022-01-20 20:11:55,761 iteration 1590 : loss : 0.039313, loss_ce: 0.017372
2022-01-20 20:11:57,058 iteration 1591 : loss : 0.030375, loss_ce: 0.010314
2022-01-20 20:11:58,393 iteration 1592 : loss : 0.040382, loss_ce: 0.014639
2022-01-20 20:11:59,824 iteration 1593 : loss : 0.052154, loss_ce: 0.020384
2022-01-20 20:12:01,253 iteration 1594 : loss : 0.047525, loss_ce: 0.016016
2022-01-20 20:12:02,591 iteration 1595 : loss : 0.028988, loss_ce: 0.009776
2022-01-20 20:12:03,952 iteration 1596 : loss : 0.044452, loss_ce: 0.019913
2022-01-20 20:12:05,292 iteration 1597 : loss : 0.050256, loss_ce: 0.021485
2022-01-20 20:12:06,650 iteration 1598 : loss : 0.034883, loss_ce: 0.012550
 24%|███████                       | 94/400 [39:06<2:00:42, 23.67s/it]2022-01-20 20:12:08,002 iteration 1599 : loss : 0.032106, loss_ce: 0.012415
2022-01-20 20:12:09,341 iteration 1600 : loss : 0.032856, loss_ce: 0.011443
2022-01-20 20:12:10,741 iteration 1601 : loss : 0.033776, loss_ce: 0.013514
2022-01-20 20:12:12,205 iteration 1602 : loss : 0.051789, loss_ce: 0.019391
2022-01-20 20:12:13,541 iteration 1603 : loss : 0.033205, loss_ce: 0.011347
2022-01-20 20:12:14,845 iteration 1604 : loss : 0.036353, loss_ce: 0.013401
2022-01-20 20:12:16,225 iteration 1605 : loss : 0.050499, loss_ce: 0.016982
2022-01-20 20:12:17,549 iteration 1606 : loss : 0.045316, loss_ce: 0.024923
2022-01-20 20:12:18,858 iteration 1607 : loss : 0.031550, loss_ce: 0.011982
2022-01-20 20:12:20,267 iteration 1608 : loss : 0.036849, loss_ce: 0.010776
2022-01-20 20:12:21,600 iteration 1609 : loss : 0.058617, loss_ce: 0.027764
2022-01-20 20:12:22,954 iteration 1610 : loss : 0.036157, loss_ce: 0.014473
2022-01-20 20:12:24,386 iteration 1611 : loss : 0.046029, loss_ce: 0.015037
2022-01-20 20:12:25,725 iteration 1612 : loss : 0.028375, loss_ce: 0.012164
2022-01-20 20:12:26,932 iteration 1613 : loss : 0.046070, loss_ce: 0.019866
2022-01-20 20:12:28,297 iteration 1614 : loss : 0.055901, loss_ce: 0.016091
2022-01-20 20:12:28,297 Training Data Eval:
2022-01-20 20:12:34,898   Average segmentation loss on training set: 0.0286
2022-01-20 20:12:34,898 Validation Data Eval:
2022-01-20 20:12:37,159   Average segmentation loss on validation set: 0.0947
2022-01-20 20:12:38,521 iteration 1615 : loss : 0.037254, loss_ce: 0.014255
 24%|███████▏                      | 95/400 [39:38<2:12:49, 26.13s/it]2022-01-20 20:12:39,939 iteration 1616 : loss : 0.039863, loss_ce: 0.014882
2022-01-20 20:12:41,303 iteration 1617 : loss : 0.045009, loss_ce: 0.012717
2022-01-20 20:12:42,571 iteration 1618 : loss : 0.028750, loss_ce: 0.013253
2022-01-20 20:12:43,844 iteration 1619 : loss : 0.026616, loss_ce: 0.008855
2022-01-20 20:12:45,094 iteration 1620 : loss : 0.041633, loss_ce: 0.012039
2022-01-20 20:12:46,442 iteration 1621 : loss : 0.053641, loss_ce: 0.032451
2022-01-20 20:12:47,909 iteration 1622 : loss : 0.050981, loss_ce: 0.013838
2022-01-20 20:12:49,244 iteration 1623 : loss : 0.053973, loss_ce: 0.030420
2022-01-20 20:12:50,544 iteration 1624 : loss : 0.039681, loss_ce: 0.014719
2022-01-20 20:12:51,866 iteration 1625 : loss : 0.035237, loss_ce: 0.014363
2022-01-20 20:12:53,177 iteration 1626 : loss : 0.031852, loss_ce: 0.013201
2022-01-20 20:12:54,501 iteration 1627 : loss : 0.045941, loss_ce: 0.018778
2022-01-20 20:12:55,911 iteration 1628 : loss : 0.040790, loss_ce: 0.017106
2022-01-20 20:12:57,238 iteration 1629 : loss : 0.030240, loss_ce: 0.011080
2022-01-20 20:12:58,518 iteration 1630 : loss : 0.040219, loss_ce: 0.014731
2022-01-20 20:12:59,971 iteration 1631 : loss : 0.048939, loss_ce: 0.022656
2022-01-20 20:13:01,387 iteration 1632 : loss : 0.056502, loss_ce: 0.019949
 24%|███████▏                      | 96/400 [40:01<2:07:24, 25.15s/it]2022-01-20 20:13:02,831 iteration 1633 : loss : 0.027859, loss_ce: 0.010306
2022-01-20 20:13:04,203 iteration 1634 : loss : 0.042880, loss_ce: 0.014490
2022-01-20 20:13:05,542 iteration 1635 : loss : 0.032025, loss_ce: 0.015539
2022-01-20 20:13:06,967 iteration 1636 : loss : 0.049241, loss_ce: 0.024901
2022-01-20 20:13:08,334 iteration 1637 : loss : 0.049284, loss_ce: 0.019262
2022-01-20 20:13:09,657 iteration 1638 : loss : 0.037035, loss_ce: 0.015463
2022-01-20 20:13:11,105 iteration 1639 : loss : 0.074671, loss_ce: 0.017531
2022-01-20 20:13:12,493 iteration 1640 : loss : 0.045987, loss_ce: 0.012382
2022-01-20 20:13:13,773 iteration 1641 : loss : 0.030836, loss_ce: 0.013686
2022-01-20 20:13:15,106 iteration 1642 : loss : 0.022609, loss_ce: 0.008053
2022-01-20 20:13:16,484 iteration 1643 : loss : 0.036203, loss_ce: 0.009770
2022-01-20 20:13:17,802 iteration 1644 : loss : 0.039916, loss_ce: 0.014417
2022-01-20 20:13:19,134 iteration 1645 : loss : 0.037525, loss_ce: 0.014094
2022-01-20 20:13:20,461 iteration 1646 : loss : 0.037541, loss_ce: 0.019302
2022-01-20 20:13:21,774 iteration 1647 : loss : 0.040202, loss_ce: 0.013593
2022-01-20 20:13:23,122 iteration 1648 : loss : 0.078424, loss_ce: 0.038461
2022-01-20 20:13:24,411 iteration 1649 : loss : 0.040827, loss_ce: 0.018206
 24%|███████▎                      | 97/400 [40:24<2:03:47, 24.51s/it]2022-01-20 20:13:25,820 iteration 1650 : loss : 0.034057, loss_ce: 0.016090
2022-01-20 20:13:27,284 iteration 1651 : loss : 0.087113, loss_ce: 0.028987
2022-01-20 20:13:28,574 iteration 1652 : loss : 0.045693, loss_ce: 0.013958
2022-01-20 20:13:29,858 iteration 1653 : loss : 0.035070, loss_ce: 0.012506
2022-01-20 20:13:31,232 iteration 1654 : loss : 0.076527, loss_ce: 0.020579
2022-01-20 20:13:32,627 iteration 1655 : loss : 0.042496, loss_ce: 0.016341
2022-01-20 20:13:33,957 iteration 1656 : loss : 0.034842, loss_ce: 0.011855
2022-01-20 20:13:35,408 iteration 1657 : loss : 0.055332, loss_ce: 0.019821
2022-01-20 20:13:36,797 iteration 1658 : loss : 0.049180, loss_ce: 0.027801
2022-01-20 20:13:38,089 iteration 1659 : loss : 0.025408, loss_ce: 0.008388
2022-01-20 20:13:39,539 iteration 1660 : loss : 0.041692, loss_ce: 0.014182
2022-01-20 20:13:40,969 iteration 1661 : loss : 0.039339, loss_ce: 0.012145
2022-01-20 20:13:42,317 iteration 1662 : loss : 0.037347, loss_ce: 0.014952
2022-01-20 20:13:43,546 iteration 1663 : loss : 0.027650, loss_ce: 0.014180
2022-01-20 20:13:44,972 iteration 1664 : loss : 0.044003, loss_ce: 0.019005
2022-01-20 20:13:46,355 iteration 1665 : loss : 0.037116, loss_ce: 0.012841
2022-01-20 20:13:47,764 iteration 1666 : loss : 0.038805, loss_ce: 0.015578
 24%|███████▎                      | 98/400 [40:47<2:01:38, 24.17s/it]2022-01-20 20:13:49,180 iteration 1667 : loss : 0.058546, loss_ce: 0.019800
2022-01-20 20:13:50,535 iteration 1668 : loss : 0.054849, loss_ce: 0.018131
2022-01-20 20:13:51,840 iteration 1669 : loss : 0.031376, loss_ce: 0.011856
2022-01-20 20:13:53,203 iteration 1670 : loss : 0.045919, loss_ce: 0.020337
2022-01-20 20:13:54,537 iteration 1671 : loss : 0.037440, loss_ce: 0.014713
2022-01-20 20:13:55,942 iteration 1672 : loss : 0.038593, loss_ce: 0.015290
2022-01-20 20:13:57,316 iteration 1673 : loss : 0.042351, loss_ce: 0.016835
2022-01-20 20:13:58,734 iteration 1674 : loss : 0.043502, loss_ce: 0.020423
2022-01-20 20:14:00,051 iteration 1675 : loss : 0.051400, loss_ce: 0.017193
2022-01-20 20:14:01,383 iteration 1676 : loss : 0.057527, loss_ce: 0.018844
2022-01-20 20:14:02,807 iteration 1677 : loss : 0.083805, loss_ce: 0.048040
2022-01-20 20:14:04,040 iteration 1678 : loss : 0.042742, loss_ce: 0.019542
2022-01-20 20:14:05,350 iteration 1679 : loss : 0.033850, loss_ce: 0.009960
2022-01-20 20:14:06,728 iteration 1680 : loss : 0.042582, loss_ce: 0.018903
2022-01-20 20:14:08,101 iteration 1681 : loss : 0.050609, loss_ce: 0.027918
2022-01-20 20:14:09,479 iteration 1682 : loss : 0.053247, loss_ce: 0.020257
2022-01-20 20:14:10,771 iteration 1683 : loss : 0.033776, loss_ce: 0.010709
 25%|███████▍                      | 99/400 [41:10<1:59:29, 23.82s/it]2022-01-20 20:14:12,179 iteration 1684 : loss : 0.046457, loss_ce: 0.018327
2022-01-20 20:14:13,521 iteration 1685 : loss : 0.051551, loss_ce: 0.016452
2022-01-20 20:14:14,851 iteration 1686 : loss : 0.035981, loss_ce: 0.014222
2022-01-20 20:14:16,135 iteration 1687 : loss : 0.045049, loss_ce: 0.019271
2022-01-20 20:14:17,607 iteration 1688 : loss : 0.031853, loss_ce: 0.013490
2022-01-20 20:14:18,983 iteration 1689 : loss : 0.044219, loss_ce: 0.018931
2022-01-20 20:14:20,389 iteration 1690 : loss : 0.045038, loss_ce: 0.023746
2022-01-20 20:14:21,667 iteration 1691 : loss : 0.097969, loss_ce: 0.029119
2022-01-20 20:14:23,068 iteration 1692 : loss : 0.072271, loss_ce: 0.024984
2022-01-20 20:14:24,294 iteration 1693 : loss : 0.033103, loss_ce: 0.012641
2022-01-20 20:14:25,675 iteration 1694 : loss : 0.029018, loss_ce: 0.011701
2022-01-20 20:14:27,101 iteration 1695 : loss : 0.066248, loss_ce: 0.029227
2022-01-20 20:14:28,530 iteration 1696 : loss : 0.055734, loss_ce: 0.016714
2022-01-20 20:14:29,877 iteration 1697 : loss : 0.041253, loss_ce: 0.014057
2022-01-20 20:14:31,281 iteration 1698 : loss : 0.045398, loss_ce: 0.017612
2022-01-20 20:14:32,634 iteration 1699 : loss : 0.041065, loss_ce: 0.020396
2022-01-20 20:14:32,634 Training Data Eval:
2022-01-20 20:14:39,251   Average segmentation loss on training set: 0.0565
2022-01-20 20:14:39,251 Validation Data Eval:
2022-01-20 20:14:41,515   Average segmentation loss on validation set: 0.1843
2022-01-20 20:14:42,834 iteration 1700 : loss : 0.032918, loss_ce: 0.010387
 25%|███████▎                     | 100/400 [41:43<2:11:27, 26.29s/it]2022-01-20 20:14:44,231 iteration 1701 : loss : 0.039074, loss_ce: 0.016656
2022-01-20 20:14:45,556 iteration 1702 : loss : 0.063123, loss_ce: 0.035265
2022-01-20 20:14:46,986 iteration 1703 : loss : 0.033452, loss_ce: 0.014475
2022-01-20 20:14:48,391 iteration 1704 : loss : 0.053653, loss_ce: 0.021360
2022-01-20 20:14:49,734 iteration 1705 : loss : 0.034248, loss_ce: 0.011640
2022-01-20 20:14:51,134 iteration 1706 : loss : 0.067137, loss_ce: 0.019683
2022-01-20 20:14:52,463 iteration 1707 : loss : 0.039228, loss_ce: 0.016167
2022-01-20 20:14:53,794 iteration 1708 : loss : 0.041129, loss_ce: 0.018073
2022-01-20 20:14:55,297 iteration 1709 : loss : 0.045141, loss_ce: 0.019169
2022-01-20 20:14:56,600 iteration 1710 : loss : 0.041358, loss_ce: 0.021317
2022-01-20 20:14:57,961 iteration 1711 : loss : 0.044968, loss_ce: 0.019446
2022-01-20 20:14:59,317 iteration 1712 : loss : 0.041404, loss_ce: 0.012861
2022-01-20 20:15:00,665 iteration 1713 : loss : 0.056869, loss_ce: 0.019164
2022-01-20 20:15:01,993 iteration 1714 : loss : 0.038208, loss_ce: 0.017028
2022-01-20 20:15:03,400 iteration 1715 : loss : 0.053341, loss_ce: 0.019106
2022-01-20 20:15:04,742 iteration 1716 : loss : 0.046871, loss_ce: 0.014547
2022-01-20 20:15:06,105 iteration 1717 : loss : 0.036779, loss_ce: 0.016030
 25%|███████▎                     | 101/400 [42:06<2:06:30, 25.39s/it]2022-01-20 20:15:07,530 iteration 1718 : loss : 0.042186, loss_ce: 0.014060
2022-01-20 20:15:08,950 iteration 1719 : loss : 0.038762, loss_ce: 0.010944
2022-01-20 20:15:10,339 iteration 1720 : loss : 0.042810, loss_ce: 0.014483
2022-01-20 20:15:11,703 iteration 1721 : loss : 0.048073, loss_ce: 0.025389
2022-01-20 20:15:13,007 iteration 1722 : loss : 0.028843, loss_ce: 0.009486
2022-01-20 20:15:14,324 iteration 1723 : loss : 0.041578, loss_ce: 0.015973
2022-01-20 20:15:15,595 iteration 1724 : loss : 0.034608, loss_ce: 0.015042
2022-01-20 20:15:16,884 iteration 1725 : loss : 0.036990, loss_ce: 0.015997
2022-01-20 20:15:18,333 iteration 1726 : loss : 0.041377, loss_ce: 0.013921
2022-01-20 20:15:19,725 iteration 1727 : loss : 0.036986, loss_ce: 0.013470
2022-01-20 20:15:21,172 iteration 1728 : loss : 0.046724, loss_ce: 0.022313
2022-01-20 20:15:22,493 iteration 1729 : loss : 0.047333, loss_ce: 0.015733
2022-01-20 20:15:23,862 iteration 1730 : loss : 0.045866, loss_ce: 0.020267
2022-01-20 20:15:25,174 iteration 1731 : loss : 0.045734, loss_ce: 0.020706
2022-01-20 20:15:26,540 iteration 1732 : loss : 0.047779, loss_ce: 0.019895
2022-01-20 20:15:27,899 iteration 1733 : loss : 0.046291, loss_ce: 0.014322
2022-01-20 20:15:29,228 iteration 1734 : loss : 0.043746, loss_ce: 0.011468
 26%|███████▍                     | 102/400 [42:29<2:02:41, 24.70s/it]2022-01-20 20:15:30,683 iteration 1735 : loss : 0.037494, loss_ce: 0.012680
2022-01-20 20:15:32,033 iteration 1736 : loss : 0.042574, loss_ce: 0.017405
2022-01-20 20:15:33,400 iteration 1737 : loss : 0.051216, loss_ce: 0.020857
2022-01-20 20:15:34,741 iteration 1738 : loss : 0.066008, loss_ce: 0.022950
2022-01-20 20:15:36,073 iteration 1739 : loss : 0.041876, loss_ce: 0.021185
2022-01-20 20:15:37,397 iteration 1740 : loss : 0.051076, loss_ce: 0.013825
2022-01-20 20:15:38,823 iteration 1741 : loss : 0.027451, loss_ce: 0.008618
2022-01-20 20:15:40,176 iteration 1742 : loss : 0.044634, loss_ce: 0.015499
2022-01-20 20:15:41,525 iteration 1743 : loss : 0.046304, loss_ce: 0.024861
2022-01-20 20:15:42,817 iteration 1744 : loss : 0.039190, loss_ce: 0.014311
2022-01-20 20:15:44,225 iteration 1745 : loss : 0.038624, loss_ce: 0.015732
2022-01-20 20:15:45,545 iteration 1746 : loss : 0.052416, loss_ce: 0.019389
2022-01-20 20:15:46,876 iteration 1747 : loss : 0.048057, loss_ce: 0.017160
2022-01-20 20:15:48,168 iteration 1748 : loss : 0.026366, loss_ce: 0.012146
2022-01-20 20:15:49,534 iteration 1749 : loss : 0.049063, loss_ce: 0.020150
2022-01-20 20:15:50,834 iteration 1750 : loss : 0.031549, loss_ce: 0.015540
2022-01-20 20:15:52,136 iteration 1751 : loss : 0.041778, loss_ce: 0.015671
 26%|███████▍                     | 103/400 [42:52<1:59:37, 24.17s/it]2022-01-20 20:15:53,518 iteration 1752 : loss : 0.039030, loss_ce: 0.016094
2022-01-20 20:15:54,945 iteration 1753 : loss : 0.046512, loss_ce: 0.021651
2022-01-20 20:15:56,315 iteration 1754 : loss : 0.039918, loss_ce: 0.017769
2022-01-20 20:15:57,683 iteration 1755 : loss : 0.052045, loss_ce: 0.016768
2022-01-20 20:15:59,092 iteration 1756 : loss : 0.079079, loss_ce: 0.049285
2022-01-20 20:16:00,457 iteration 1757 : loss : 0.057361, loss_ce: 0.011743
2022-01-20 20:16:01,826 iteration 1758 : loss : 0.043519, loss_ce: 0.017359
2022-01-20 20:16:03,167 iteration 1759 : loss : 0.037918, loss_ce: 0.012577
2022-01-20 20:16:04,568 iteration 1760 : loss : 0.089483, loss_ce: 0.063931
2022-01-20 20:16:05,861 iteration 1761 : loss : 0.031576, loss_ce: 0.014378
2022-01-20 20:16:07,269 iteration 1762 : loss : 0.107654, loss_ce: 0.047445
2022-01-20 20:16:08,576 iteration 1763 : loss : 0.072349, loss_ce: 0.027957
2022-01-20 20:16:09,953 iteration 1764 : loss : 0.152428, loss_ce: 0.054841
2022-01-20 20:16:11,336 iteration 1765 : loss : 0.091639, loss_ce: 0.034385
2022-01-20 20:16:12,631 iteration 1766 : loss : 0.048668, loss_ce: 0.021260
2022-01-20 20:16:13,969 iteration 1767 : loss : 0.073736, loss_ce: 0.033637
2022-01-20 20:16:15,379 iteration 1768 : loss : 0.051602, loss_ce: 0.016546
 26%|███████▌                     | 104/400 [43:15<1:57:51, 23.89s/it]2022-01-20 20:16:16,752 iteration 1769 : loss : 0.056808, loss_ce: 0.020000
2022-01-20 20:16:18,276 iteration 1770 : loss : 0.065788, loss_ce: 0.029449
2022-01-20 20:16:19,618 iteration 1771 : loss : 0.055694, loss_ce: 0.024308
2022-01-20 20:16:20,912 iteration 1772 : loss : 0.053027, loss_ce: 0.016651
2022-01-20 20:16:22,185 iteration 1773 : loss : 0.051634, loss_ce: 0.016298
2022-01-20 20:16:23,530 iteration 1774 : loss : 0.041635, loss_ce: 0.017940
2022-01-20 20:16:24,929 iteration 1775 : loss : 0.072311, loss_ce: 0.035716
2022-01-20 20:16:26,266 iteration 1776 : loss : 0.067982, loss_ce: 0.019647
2022-01-20 20:16:27,583 iteration 1777 : loss : 0.050529, loss_ce: 0.018741
2022-01-20 20:16:28,910 iteration 1778 : loss : 0.071823, loss_ce: 0.029270
2022-01-20 20:16:30,260 iteration 1779 : loss : 0.045212, loss_ce: 0.016461
2022-01-20 20:16:31,580 iteration 1780 : loss : 0.054296, loss_ce: 0.020770
2022-01-20 20:16:32,906 iteration 1781 : loss : 0.051103, loss_ce: 0.024231
2022-01-20 20:16:34,273 iteration 1782 : loss : 0.067870, loss_ce: 0.023867
2022-01-20 20:16:35,572 iteration 1783 : loss : 0.063694, loss_ce: 0.029232
2022-01-20 20:16:37,003 iteration 1784 : loss : 0.032223, loss_ce: 0.012987
2022-01-20 20:16:37,003 Training Data Eval:
2022-01-20 20:16:43,614   Average segmentation loss on training set: 0.0529
2022-01-20 20:16:43,614 Validation Data Eval:
2022-01-20 20:16:45,880   Average segmentation loss on validation set: 0.0828
2022-01-20 20:16:47,231 iteration 1785 : loss : 0.054001, loss_ce: 0.019574
 26%|███████▌                     | 105/400 [43:47<2:09:12, 26.28s/it]2022-01-20 20:16:48,732 iteration 1786 : loss : 0.069255, loss_ce: 0.035982
2022-01-20 20:16:50,173 iteration 1787 : loss : 0.055114, loss_ce: 0.020071
2022-01-20 20:16:51,548 iteration 1788 : loss : 0.049624, loss_ce: 0.020890
2022-01-20 20:16:52,900 iteration 1789 : loss : 0.050649, loss_ce: 0.017837
2022-01-20 20:16:54,237 iteration 1790 : loss : 0.046183, loss_ce: 0.017727
2022-01-20 20:16:55,518 iteration 1791 : loss : 0.028796, loss_ce: 0.013779
2022-01-20 20:16:56,890 iteration 1792 : loss : 0.042402, loss_ce: 0.014090
2022-01-20 20:16:58,281 iteration 1793 : loss : 0.047822, loss_ce: 0.023856
2022-01-20 20:16:59,596 iteration 1794 : loss : 0.037799, loss_ce: 0.017246
2022-01-20 20:17:00,953 iteration 1795 : loss : 0.069475, loss_ce: 0.032280
2022-01-20 20:17:02,300 iteration 1796 : loss : 0.035811, loss_ce: 0.012516
2022-01-20 20:17:03,746 iteration 1797 : loss : 0.036358, loss_ce: 0.013735
2022-01-20 20:17:05,109 iteration 1798 : loss : 0.051449, loss_ce: 0.017463
2022-01-20 20:17:06,499 iteration 1799 : loss : 0.046354, loss_ce: 0.017221
2022-01-20 20:17:07,875 iteration 1800 : loss : 0.048410, loss_ce: 0.022875
2022-01-20 20:17:09,265 iteration 1801 : loss : 0.035303, loss_ce: 0.015243
2022-01-20 20:17:10,533 iteration 1802 : loss : 0.067409, loss_ce: 0.018694
 26%|███████▋                     | 106/400 [44:10<2:04:23, 25.39s/it]2022-01-20 20:17:11,945 iteration 1803 : loss : 0.050735, loss_ce: 0.018777
2022-01-20 20:17:13,272 iteration 1804 : loss : 0.071036, loss_ce: 0.013727
2022-01-20 20:17:14,556 iteration 1805 : loss : 0.027688, loss_ce: 0.012610
2022-01-20 20:17:15,863 iteration 1806 : loss : 0.059954, loss_ce: 0.015621
2022-01-20 20:17:17,219 iteration 1807 : loss : 0.039371, loss_ce: 0.012392
2022-01-20 20:17:18,574 iteration 1808 : loss : 0.065157, loss_ce: 0.027728
2022-01-20 20:17:19,900 iteration 1809 : loss : 0.045712, loss_ce: 0.020604
2022-01-20 20:17:21,255 iteration 1810 : loss : 0.042613, loss_ce: 0.016285
2022-01-20 20:17:22,632 iteration 1811 : loss : 0.049544, loss_ce: 0.015660
2022-01-20 20:17:24,050 iteration 1812 : loss : 0.046609, loss_ce: 0.022467
2022-01-20 20:17:25,432 iteration 1813 : loss : 0.064055, loss_ce: 0.020852
2022-01-20 20:17:26,867 iteration 1814 : loss : 0.053509, loss_ce: 0.020161
2022-01-20 20:17:28,239 iteration 1815 : loss : 0.031491, loss_ce: 0.014583
2022-01-20 20:17:29,645 iteration 1816 : loss : 0.068214, loss_ce: 0.023556
2022-01-20 20:17:31,000 iteration 1817 : loss : 0.038723, loss_ce: 0.011842
2022-01-20 20:17:32,443 iteration 1818 : loss : 0.036548, loss_ce: 0.021122
2022-01-20 20:17:33,771 iteration 1819 : loss : 0.040249, loss_ce: 0.014979
 27%|███████▊                     | 107/400 [44:33<2:00:48, 24.74s/it]2022-01-20 20:17:35,208 iteration 1820 : loss : 0.076497, loss_ce: 0.020531
2022-01-20 20:17:36,525 iteration 1821 : loss : 0.037314, loss_ce: 0.012229
2022-01-20 20:17:37,838 iteration 1822 : loss : 0.043387, loss_ce: 0.022689
2022-01-20 20:17:39,224 iteration 1823 : loss : 0.046129, loss_ce: 0.020549
2022-01-20 20:17:40,558 iteration 1824 : loss : 0.029544, loss_ce: 0.010621
2022-01-20 20:17:41,921 iteration 1825 : loss : 0.046422, loss_ce: 0.016173
2022-01-20 20:17:43,191 iteration 1826 : loss : 0.075540, loss_ce: 0.037230
2022-01-20 20:17:44,499 iteration 1827 : loss : 0.057281, loss_ce: 0.015607
2022-01-20 20:17:45,879 iteration 1828 : loss : 0.038851, loss_ce: 0.017412
2022-01-20 20:17:47,257 iteration 1829 : loss : 0.057759, loss_ce: 0.015887
2022-01-20 20:17:48,601 iteration 1830 : loss : 0.046271, loss_ce: 0.018156
2022-01-20 20:17:49,927 iteration 1831 : loss : 0.039928, loss_ce: 0.017302
2022-01-20 20:17:51,254 iteration 1832 : loss : 0.035501, loss_ce: 0.012080
2022-01-20 20:17:52,567 iteration 1833 : loss : 0.044365, loss_ce: 0.023899
2022-01-20 20:17:53,974 iteration 1834 : loss : 0.040091, loss_ce: 0.013914
2022-01-20 20:17:55,238 iteration 1835 : loss : 0.047763, loss_ce: 0.015920
2022-01-20 20:17:56,546 iteration 1836 : loss : 0.046112, loss_ce: 0.016511
 27%|███████▊                     | 108/400 [44:56<1:57:32, 24.15s/it]2022-01-20 20:17:57,872 iteration 1837 : loss : 0.052090, loss_ce: 0.009949
2022-01-20 20:17:59,281 iteration 1838 : loss : 0.046691, loss_ce: 0.019366
2022-01-20 20:18:00,563 iteration 1839 : loss : 0.031659, loss_ce: 0.013182
2022-01-20 20:18:01,912 iteration 1840 : loss : 0.045681, loss_ce: 0.013287
2022-01-20 20:18:03,264 iteration 1841 : loss : 0.046834, loss_ce: 0.018019
2022-01-20 20:18:04,689 iteration 1842 : loss : 0.038525, loss_ce: 0.015844
2022-01-20 20:18:06,021 iteration 1843 : loss : 0.047658, loss_ce: 0.022184
2022-01-20 20:18:07,366 iteration 1844 : loss : 0.044347, loss_ce: 0.017924
2022-01-20 20:18:08,706 iteration 1845 : loss : 0.036593, loss_ce: 0.015411
2022-01-20 20:18:10,060 iteration 1846 : loss : 0.052314, loss_ce: 0.015364
2022-01-20 20:18:11,314 iteration 1847 : loss : 0.030310, loss_ce: 0.010871
2022-01-20 20:18:12,665 iteration 1848 : loss : 0.049407, loss_ce: 0.017923
2022-01-20 20:18:14,011 iteration 1849 : loss : 0.036853, loss_ce: 0.012677
2022-01-20 20:18:15,367 iteration 1850 : loss : 0.048241, loss_ce: 0.016418
2022-01-20 20:18:16,753 iteration 1851 : loss : 0.048694, loss_ce: 0.017708
2022-01-20 20:18:18,010 iteration 1852 : loss : 0.032344, loss_ce: 0.017195
2022-01-20 20:18:19,384 iteration 1853 : loss : 0.041884, loss_ce: 0.018881
 27%|███████▉                     | 109/400 [45:19<1:55:13, 23.76s/it]2022-01-20 20:18:20,791 iteration 1854 : loss : 0.051406, loss_ce: 0.016530
2022-01-20 20:18:22,189 iteration 1855 : loss : 0.038171, loss_ce: 0.015043
2022-01-20 20:18:23,445 iteration 1856 : loss : 0.046532, loss_ce: 0.017435
2022-01-20 20:18:24,702 iteration 1857 : loss : 0.033644, loss_ce: 0.014317
2022-01-20 20:18:25,989 iteration 1858 : loss : 0.031250, loss_ce: 0.015144
2022-01-20 20:18:27,344 iteration 1859 : loss : 0.048799, loss_ce: 0.018660
2022-01-20 20:18:28,725 iteration 1860 : loss : 0.061193, loss_ce: 0.025209
2022-01-20 20:18:30,079 iteration 1861 : loss : 0.040263, loss_ce: 0.018765
2022-01-20 20:18:31,452 iteration 1862 : loss : 0.038276, loss_ce: 0.016757
2022-01-20 20:18:32,831 iteration 1863 : loss : 0.059872, loss_ce: 0.023382
2022-01-20 20:18:34,203 iteration 1864 : loss : 0.040349, loss_ce: 0.013059
2022-01-20 20:18:35,555 iteration 1865 : loss : 0.047357, loss_ce: 0.014379
2022-01-20 20:18:36,906 iteration 1866 : loss : 0.032814, loss_ce: 0.010704
2022-01-20 20:18:38,150 iteration 1867 : loss : 0.031670, loss_ce: 0.010572
2022-01-20 20:18:39,461 iteration 1868 : loss : 0.033141, loss_ce: 0.010643
2022-01-20 20:18:40,767 iteration 1869 : loss : 0.044023, loss_ce: 0.021104
2022-01-20 20:18:40,767 Training Data Eval:
2022-01-20 20:18:47,373   Average segmentation loss on training set: 0.0306
2022-01-20 20:18:47,373 Validation Data Eval:
2022-01-20 20:18:49,637   Average segmentation loss on validation set: 0.0895
2022-01-20 20:18:50,994 iteration 1870 : loss : 0.051687, loss_ce: 0.019528
 28%|███████▉                     | 110/400 [45:51<2:06:13, 26.11s/it]2022-01-20 20:18:52,428 iteration 1871 : loss : 0.051557, loss_ce: 0.013782
2022-01-20 20:18:53,811 iteration 1872 : loss : 0.027442, loss_ce: 0.009518
2022-01-20 20:18:55,210 iteration 1873 : loss : 0.035098, loss_ce: 0.011796
2022-01-20 20:18:56,584 iteration 1874 : loss : 0.036700, loss_ce: 0.011360
2022-01-20 20:18:57,860 iteration 1875 : loss : 0.029885, loss_ce: 0.012145
2022-01-20 20:18:59,117 iteration 1876 : loss : 0.035466, loss_ce: 0.016302
2022-01-20 20:19:00,500 iteration 1877 : loss : 0.040137, loss_ce: 0.012502
2022-01-20 20:19:01,867 iteration 1878 : loss : 0.048692, loss_ce: 0.024729
2022-01-20 20:19:03,217 iteration 1879 : loss : 0.041503, loss_ce: 0.014035
2022-01-20 20:19:04,592 iteration 1880 : loss : 0.058052, loss_ce: 0.024841
2022-01-20 20:19:05,934 iteration 1881 : loss : 0.051534, loss_ce: 0.022803
2022-01-20 20:19:07,345 iteration 1882 : loss : 0.051704, loss_ce: 0.015661
2022-01-20 20:19:08,709 iteration 1883 : loss : 0.022660, loss_ce: 0.007528
2022-01-20 20:19:10,022 iteration 1884 : loss : 0.031909, loss_ce: 0.011843
2022-01-20 20:19:11,295 iteration 1885 : loss : 0.031059, loss_ce: 0.013586
2022-01-20 20:19:12,652 iteration 1886 : loss : 0.053980, loss_ce: 0.019738
2022-01-20 20:19:14,016 iteration 1887 : loss : 0.064267, loss_ce: 0.029052
 28%|████████                     | 111/400 [46:14<2:01:18, 25.18s/it]2022-01-20 20:19:15,523 iteration 1888 : loss : 0.046461, loss_ce: 0.017794
2022-01-20 20:19:16,824 iteration 1889 : loss : 0.046458, loss_ce: 0.016586
2022-01-20 20:19:18,106 iteration 1890 : loss : 0.038101, loss_ce: 0.016587
2022-01-20 20:19:19,406 iteration 1891 : loss : 0.031075, loss_ce: 0.011136
2022-01-20 20:19:20,746 iteration 1892 : loss : 0.028601, loss_ce: 0.012767
2022-01-20 20:19:22,116 iteration 1893 : loss : 0.033945, loss_ce: 0.010966
2022-01-20 20:19:23,459 iteration 1894 : loss : 0.038594, loss_ce: 0.016133
2022-01-20 20:19:24,743 iteration 1895 : loss : 0.031554, loss_ce: 0.012814
2022-01-20 20:19:26,156 iteration 1896 : loss : 0.042694, loss_ce: 0.018767
2022-01-20 20:19:27,526 iteration 1897 : loss : 0.045852, loss_ce: 0.015294
2022-01-20 20:19:28,951 iteration 1898 : loss : 0.061352, loss_ce: 0.033360
2022-01-20 20:19:30,298 iteration 1899 : loss : 0.034339, loss_ce: 0.010708
2022-01-20 20:19:31,597 iteration 1900 : loss : 0.047222, loss_ce: 0.018276
2022-01-20 20:19:32,872 iteration 1901 : loss : 0.033129, loss_ce: 0.011492
2022-01-20 20:19:34,207 iteration 1902 : loss : 0.040781, loss_ce: 0.015912
2022-01-20 20:19:35,592 iteration 1903 : loss : 0.036484, loss_ce: 0.015571
2022-01-20 20:19:36,993 iteration 1904 : loss : 0.026967, loss_ce: 0.010311
 28%|████████                     | 112/400 [46:37<1:57:42, 24.52s/it]2022-01-20 20:19:38,314 iteration 1905 : loss : 0.034870, loss_ce: 0.016015
2022-01-20 20:19:39,693 iteration 1906 : loss : 0.031329, loss_ce: 0.009418
2022-01-20 20:19:41,086 iteration 1907 : loss : 0.029375, loss_ce: 0.012664
2022-01-20 20:19:42,573 iteration 1908 : loss : 0.046921, loss_ce: 0.018496
2022-01-20 20:19:43,843 iteration 1909 : loss : 0.024599, loss_ce: 0.010264
2022-01-20 20:19:45,175 iteration 1910 : loss : 0.037633, loss_ce: 0.014153
2022-01-20 20:19:46,563 iteration 1911 : loss : 0.035259, loss_ce: 0.016049
2022-01-20 20:19:48,019 iteration 1912 : loss : 0.043811, loss_ce: 0.017486
2022-01-20 20:19:49,408 iteration 1913 : loss : 0.043746, loss_ce: 0.014568
2022-01-20 20:19:50,843 iteration 1914 : loss : 0.049385, loss_ce: 0.020023
2022-01-20 20:19:52,197 iteration 1915 : loss : 0.029892, loss_ce: 0.009695
2022-01-20 20:19:53,523 iteration 1916 : loss : 0.061289, loss_ce: 0.019773
2022-01-20 20:19:54,850 iteration 1917 : loss : 0.028392, loss_ce: 0.011217
2022-01-20 20:19:56,242 iteration 1918 : loss : 0.042782, loss_ce: 0.020057
2022-01-20 20:19:57,550 iteration 1919 : loss : 0.032054, loss_ce: 0.011124
2022-01-20 20:19:58,817 iteration 1920 : loss : 0.028415, loss_ce: 0.013544
2022-01-20 20:20:00,233 iteration 1921 : loss : 0.041340, loss_ce: 0.014338
 28%|████████▏                    | 113/400 [47:00<1:55:27, 24.14s/it]2022-01-20 20:20:01,677 iteration 1922 : loss : 0.061178, loss_ce: 0.026597
2022-01-20 20:20:02,985 iteration 1923 : loss : 0.026762, loss_ce: 0.010177
2022-01-20 20:20:04,393 iteration 1924 : loss : 0.068004, loss_ce: 0.019542
2022-01-20 20:20:05,688 iteration 1925 : loss : 0.044898, loss_ce: 0.012933
2022-01-20 20:20:07,100 iteration 1926 : loss : 0.030458, loss_ce: 0.011650
2022-01-20 20:20:08,538 iteration 1927 : loss : 0.026609, loss_ce: 0.012696
2022-01-20 20:20:09,854 iteration 1928 : loss : 0.059274, loss_ce: 0.018435
2022-01-20 20:20:11,353 iteration 1929 : loss : 0.069861, loss_ce: 0.036771
2022-01-20 20:20:12,714 iteration 1930 : loss : 0.039043, loss_ce: 0.017897
2022-01-20 20:20:14,122 iteration 1931 : loss : 0.050661, loss_ce: 0.015093
2022-01-20 20:20:15,496 iteration 1932 : loss : 0.049146, loss_ce: 0.012898
2022-01-20 20:20:16,875 iteration 1933 : loss : 0.042793, loss_ce: 0.017801
2022-01-20 20:20:18,215 iteration 1934 : loss : 0.039458, loss_ce: 0.013170
2022-01-20 20:20:19,576 iteration 1935 : loss : 0.035319, loss_ce: 0.013703
2022-01-20 20:20:20,905 iteration 1936 : loss : 0.039764, loss_ce: 0.015079
2022-01-20 20:20:22,198 iteration 1937 : loss : 0.035985, loss_ce: 0.013765
2022-01-20 20:20:23,570 iteration 1938 : loss : 0.042182, loss_ce: 0.016451
 28%|████████▎                    | 114/400 [47:23<1:53:55, 23.90s/it]2022-01-20 20:20:24,988 iteration 1939 : loss : 0.059656, loss_ce: 0.023762
2022-01-20 20:20:26,285 iteration 1940 : loss : 0.042974, loss_ce: 0.021112
2022-01-20 20:20:27,631 iteration 1941 : loss : 0.029962, loss_ce: 0.010883
2022-01-20 20:20:28,995 iteration 1942 : loss : 0.035551, loss_ce: 0.011261
2022-01-20 20:20:30,333 iteration 1943 : loss : 0.032746, loss_ce: 0.011607
2022-01-20 20:20:31,686 iteration 1944 : loss : 0.045683, loss_ce: 0.019919
2022-01-20 20:20:33,045 iteration 1945 : loss : 0.043392, loss_ce: 0.016407
2022-01-20 20:20:34,362 iteration 1946 : loss : 0.044096, loss_ce: 0.013255
2022-01-20 20:20:35,695 iteration 1947 : loss : 0.034069, loss_ce: 0.008578
2022-01-20 20:20:37,064 iteration 1948 : loss : 0.043966, loss_ce: 0.018204
2022-01-20 20:20:38,370 iteration 1949 : loss : 0.040516, loss_ce: 0.013999
2022-01-20 20:20:39,756 iteration 1950 : loss : 0.053549, loss_ce: 0.017866
2022-01-20 20:20:41,137 iteration 1951 : loss : 0.037638, loss_ce: 0.015785
2022-01-20 20:20:42,497 iteration 1952 : loss : 0.032350, loss_ce: 0.013805
2022-01-20 20:20:43,774 iteration 1953 : loss : 0.042489, loss_ce: 0.025412
2022-01-20 20:20:45,148 iteration 1954 : loss : 0.028813, loss_ce: 0.012495
2022-01-20 20:20:45,149 Training Data Eval:
2022-01-20 20:20:51,766   Average segmentation loss on training set: 0.0303
2022-01-20 20:20:51,767 Validation Data Eval:
2022-01-20 20:20:54,030   Average segmentation loss on validation set: 0.1007
2022-01-20 20:20:55,415 iteration 1955 : loss : 0.045396, loss_ce: 0.020860
 29%|████████▎                    | 115/400 [47:55<2:04:50, 26.28s/it]2022-01-20 20:20:56,789 iteration 1956 : loss : 0.051518, loss_ce: 0.022283
2022-01-20 20:20:58,184 iteration 1957 : loss : 0.053350, loss_ce: 0.027390
2022-01-20 20:20:59,521 iteration 1958 : loss : 0.085997, loss_ce: 0.022108
2022-01-20 20:21:00,846 iteration 1959 : loss : 0.046180, loss_ce: 0.013391
2022-01-20 20:21:02,099 iteration 1960 : loss : 0.031136, loss_ce: 0.010972
2022-01-20 20:21:03,327 iteration 1961 : loss : 0.031978, loss_ce: 0.012082
2022-01-20 20:21:04,713 iteration 1962 : loss : 0.034235, loss_ce: 0.013396
2022-01-20 20:21:06,006 iteration 1963 : loss : 0.033825, loss_ce: 0.012554
2022-01-20 20:21:07,384 iteration 1964 : loss : 0.052776, loss_ce: 0.015241
2022-01-20 20:21:08,642 iteration 1965 : loss : 0.033496, loss_ce: 0.015030
2022-01-20 20:21:09,975 iteration 1966 : loss : 0.027781, loss_ce: 0.011518
2022-01-20 20:21:11,435 iteration 1967 : loss : 0.037428, loss_ce: 0.014141
2022-01-20 20:21:12,956 iteration 1968 : loss : 0.045497, loss_ce: 0.018155
2022-01-20 20:21:14,361 iteration 1969 : loss : 0.054029, loss_ce: 0.018657
2022-01-20 20:21:15,647 iteration 1970 : loss : 0.028523, loss_ce: 0.010833
2022-01-20 20:21:17,010 iteration 1971 : loss : 0.031346, loss_ce: 0.011973
2022-01-20 20:21:18,373 iteration 1972 : loss : 0.046813, loss_ce: 0.013029
 29%|████████▍                    | 116/400 [48:18<1:59:41, 25.29s/it]2022-01-20 20:21:19,715 iteration 1973 : loss : 0.031738, loss_ce: 0.013028
2022-01-20 20:21:21,027 iteration 1974 : loss : 0.035584, loss_ce: 0.013802
2022-01-20 20:21:22,402 iteration 1975 : loss : 0.040193, loss_ce: 0.013894
2022-01-20 20:21:23,719 iteration 1976 : loss : 0.040745, loss_ce: 0.016610
2022-01-20 20:21:25,128 iteration 1977 : loss : 0.037550, loss_ce: 0.013725
2022-01-20 20:21:26,552 iteration 1978 : loss : 0.051461, loss_ce: 0.017821
2022-01-20 20:21:27,840 iteration 1979 : loss : 0.032693, loss_ce: 0.013333
2022-01-20 20:21:29,131 iteration 1980 : loss : 0.030454, loss_ce: 0.011080
2022-01-20 20:21:30,444 iteration 1981 : loss : 0.029681, loss_ce: 0.012227
2022-01-20 20:21:31,834 iteration 1982 : loss : 0.032186, loss_ce: 0.011812
2022-01-20 20:21:33,113 iteration 1983 : loss : 0.047755, loss_ce: 0.015545
2022-01-20 20:21:34,507 iteration 1984 : loss : 0.084052, loss_ce: 0.040202
2022-01-20 20:21:35,841 iteration 1985 : loss : 0.043000, loss_ce: 0.015796
2022-01-20 20:21:37,236 iteration 1986 : loss : 0.037815, loss_ce: 0.011961
2022-01-20 20:21:38,555 iteration 1987 : loss : 0.030550, loss_ce: 0.010415
2022-01-20 20:21:39,822 iteration 1988 : loss : 0.029195, loss_ce: 0.011854
2022-01-20 20:21:41,157 iteration 1989 : loss : 0.038900, loss_ce: 0.013702
 29%|████████▍                    | 117/400 [48:41<1:55:43, 24.53s/it]2022-01-20 20:21:42,545 iteration 1990 : loss : 0.042831, loss_ce: 0.011573
2022-01-20 20:21:43,819 iteration 1991 : loss : 0.029500, loss_ce: 0.012379
2022-01-20 20:21:45,189 iteration 1992 : loss : 0.026371, loss_ce: 0.008679
2022-01-20 20:21:46,560 iteration 1993 : loss : 0.037222, loss_ce: 0.013150
2022-01-20 20:21:47,964 iteration 1994 : loss : 0.030804, loss_ce: 0.012661
2022-01-20 20:21:49,287 iteration 1995 : loss : 0.073567, loss_ce: 0.044281
2022-01-20 20:21:50,669 iteration 1996 : loss : 0.026605, loss_ce: 0.010915
2022-01-20 20:21:51,966 iteration 1997 : loss : 0.030083, loss_ce: 0.011966
2022-01-20 20:21:53,314 iteration 1998 : loss : 0.031491, loss_ce: 0.012061
2022-01-20 20:21:54,681 iteration 1999 : loss : 0.039155, loss_ce: 0.014083
2022-01-20 20:21:56,024 iteration 2000 : loss : 0.040400, loss_ce: 0.018799
2022-01-20 20:21:57,318 iteration 2001 : loss : 0.063228, loss_ce: 0.020708
2022-01-20 20:21:58,717 iteration 2002 : loss : 0.030535, loss_ce: 0.011664
2022-01-20 20:22:00,013 iteration 2003 : loss : 0.032283, loss_ce: 0.009030
2022-01-20 20:22:01,394 iteration 2004 : loss : 0.035409, loss_ce: 0.018509
2022-01-20 20:22:02,704 iteration 2005 : loss : 0.035671, loss_ce: 0.014826
2022-01-20 20:22:04,046 iteration 2006 : loss : 0.036690, loss_ce: 0.012131
 30%|████████▌                    | 118/400 [49:04<1:52:58, 24.04s/it]2022-01-20 20:22:05,514 iteration 2007 : loss : 0.049318, loss_ce: 0.015974
2022-01-20 20:22:06,885 iteration 2008 : loss : 0.038084, loss_ce: 0.015587
2022-01-20 20:22:08,185 iteration 2009 : loss : 0.029940, loss_ce: 0.012882
2022-01-20 20:22:09,535 iteration 2010 : loss : 0.034986, loss_ce: 0.014098
2022-01-20 20:22:10,814 iteration 2011 : loss : 0.054611, loss_ce: 0.022088
2022-01-20 20:22:12,074 iteration 2012 : loss : 0.025922, loss_ce: 0.009832
2022-01-20 20:22:13,374 iteration 2013 : loss : 0.034428, loss_ce: 0.017443
2022-01-20 20:22:14,732 iteration 2014 : loss : 0.030182, loss_ce: 0.013786
2022-01-20 20:22:16,110 iteration 2015 : loss : 0.046113, loss_ce: 0.015916
2022-01-20 20:22:17,515 iteration 2016 : loss : 0.048454, loss_ce: 0.022291
2022-01-20 20:22:18,882 iteration 2017 : loss : 0.038126, loss_ce: 0.014839
2022-01-20 20:22:20,154 iteration 2018 : loss : 0.039157, loss_ce: 0.013795
2022-01-20 20:22:21,499 iteration 2019 : loss : 0.039354, loss_ce: 0.014479
2022-01-20 20:22:22,832 iteration 2020 : loss : 0.038067, loss_ce: 0.015460
2022-01-20 20:22:24,156 iteration 2021 : loss : 0.027107, loss_ce: 0.010753
2022-01-20 20:22:25,498 iteration 2022 : loss : 0.032331, loss_ce: 0.009856
2022-01-20 20:22:26,856 iteration 2023 : loss : 0.030982, loss_ce: 0.009815
 30%|████████▋                    | 119/400 [49:27<1:50:51, 23.67s/it]2022-01-20 20:22:28,268 iteration 2024 : loss : 0.036789, loss_ce: 0.015488
2022-01-20 20:22:29,764 iteration 2025 : loss : 0.055840, loss_ce: 0.021501
2022-01-20 20:22:31,078 iteration 2026 : loss : 0.031150, loss_ce: 0.009303
2022-01-20 20:22:32,453 iteration 2027 : loss : 0.040005, loss_ce: 0.011250
2022-01-20 20:22:33,700 iteration 2028 : loss : 0.034164, loss_ce: 0.015104
2022-01-20 20:22:34,975 iteration 2029 : loss : 0.033248, loss_ce: 0.008484
2022-01-20 20:22:36,336 iteration 2030 : loss : 0.046619, loss_ce: 0.015938
2022-01-20 20:22:37,701 iteration 2031 : loss : 0.036930, loss_ce: 0.012952
2022-01-20 20:22:39,052 iteration 2032 : loss : 0.041635, loss_ce: 0.021645
2022-01-20 20:22:40,293 iteration 2033 : loss : 0.023383, loss_ce: 0.009970
2022-01-20 20:22:41,593 iteration 2034 : loss : 0.035005, loss_ce: 0.012117
2022-01-20 20:22:42,963 iteration 2035 : loss : 0.039526, loss_ce: 0.015076
2022-01-20 20:22:44,277 iteration 2036 : loss : 0.031729, loss_ce: 0.013614
2022-01-20 20:22:45,566 iteration 2037 : loss : 0.044993, loss_ce: 0.017570
2022-01-20 20:22:46,998 iteration 2038 : loss : 0.091796, loss_ce: 0.029663
2022-01-20 20:22:48,374 iteration 2039 : loss : 0.033528, loss_ce: 0.010737
2022-01-20 20:22:48,375 Training Data Eval:
2022-01-20 20:22:54,990   Average segmentation loss on training set: 0.0287
2022-01-20 20:22:54,991 Validation Data Eval:
2022-01-20 20:22:57,250   Average segmentation loss on validation set: 0.0698
2022-01-20 20:23:03,012 Found new lowest validation loss at iteration 2039! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 20:23:04,321 iteration 2040 : loss : 0.035827, loss_ce: 0.016546
 30%|████████▋                    | 120/400 [50:04<2:09:46, 27.81s/it]2022-01-20 20:23:05,557 iteration 2041 : loss : 0.039277, loss_ce: 0.017401
2022-01-20 20:23:06,874 iteration 2042 : loss : 0.032169, loss_ce: 0.012923
2022-01-20 20:23:08,240 iteration 2043 : loss : 0.034632, loss_ce: 0.012147
2022-01-20 20:23:09,568 iteration 2044 : loss : 0.041569, loss_ce: 0.020239
2022-01-20 20:23:11,020 iteration 2045 : loss : 0.046970, loss_ce: 0.018204
2022-01-20 20:23:12,380 iteration 2046 : loss : 0.059576, loss_ce: 0.013662
2022-01-20 20:23:13,696 iteration 2047 : loss : 0.035729, loss_ce: 0.013683
2022-01-20 20:23:14,973 iteration 2048 : loss : 0.033097, loss_ce: 0.012480
2022-01-20 20:23:16,375 iteration 2049 : loss : 0.057019, loss_ce: 0.026873
2022-01-20 20:23:17,759 iteration 2050 : loss : 0.031907, loss_ce: 0.009284
2022-01-20 20:23:19,074 iteration 2051 : loss : 0.039132, loss_ce: 0.017611
2022-01-20 20:23:20,475 iteration 2052 : loss : 0.043027, loss_ce: 0.015154
2022-01-20 20:23:21,830 iteration 2053 : loss : 0.034290, loss_ce: 0.014462
2022-01-20 20:23:23,164 iteration 2054 : loss : 0.032606, loss_ce: 0.011810
2022-01-20 20:23:24,529 iteration 2055 : loss : 0.038332, loss_ce: 0.012485
2022-01-20 20:23:25,838 iteration 2056 : loss : 0.031268, loss_ce: 0.011078
2022-01-20 20:23:27,150 iteration 2057 : loss : 0.027708, loss_ce: 0.011268
 30%|████████▊                    | 121/400 [50:27<2:02:21, 26.31s/it]2022-01-20 20:23:28,523 iteration 2058 : loss : 0.049950, loss_ce: 0.034139
2022-01-20 20:23:29,912 iteration 2059 : loss : 0.039953, loss_ce: 0.012567
2022-01-20 20:23:31,330 iteration 2060 : loss : 0.041742, loss_ce: 0.014332
2022-01-20 20:23:32,676 iteration 2061 : loss : 0.030806, loss_ce: 0.014326
2022-01-20 20:23:33,968 iteration 2062 : loss : 0.035137, loss_ce: 0.013485
2022-01-20 20:23:35,469 iteration 2063 : loss : 0.040122, loss_ce: 0.015798
2022-01-20 20:23:36,823 iteration 2064 : loss : 0.045733, loss_ce: 0.017358
2022-01-20 20:23:38,240 iteration 2065 : loss : 0.071402, loss_ce: 0.024155
2022-01-20 20:23:39,574 iteration 2066 : loss : 0.035126, loss_ce: 0.016001
2022-01-20 20:23:40,958 iteration 2067 : loss : 0.041946, loss_ce: 0.013305
2022-01-20 20:23:42,253 iteration 2068 : loss : 0.022952, loss_ce: 0.008133
2022-01-20 20:23:43,602 iteration 2069 : loss : 0.042477, loss_ce: 0.020061
2022-01-20 20:23:45,001 iteration 2070 : loss : 0.041334, loss_ce: 0.019922
2022-01-20 20:23:46,300 iteration 2071 : loss : 0.026004, loss_ce: 0.010025
2022-01-20 20:23:47,603 iteration 2072 : loss : 0.042761, loss_ce: 0.023221
2022-01-20 20:23:48,989 iteration 2073 : loss : 0.035732, loss_ce: 0.014657
2022-01-20 20:23:50,268 iteration 2074 : loss : 0.039095, loss_ce: 0.010612
 30%|████████▊                    | 122/400 [50:50<1:57:28, 25.36s/it]2022-01-20 20:23:51,646 iteration 2075 : loss : 0.044475, loss_ce: 0.020632
2022-01-20 20:23:53,036 iteration 2076 : loss : 0.040260, loss_ce: 0.018788
2022-01-20 20:23:54,421 iteration 2077 : loss : 0.073907, loss_ce: 0.036814
2022-01-20 20:23:55,833 iteration 2078 : loss : 0.039050, loss_ce: 0.012953
2022-01-20 20:23:57,235 iteration 2079 : loss : 0.033084, loss_ce: 0.012512
2022-01-20 20:23:58,544 iteration 2080 : loss : 0.044606, loss_ce: 0.016019
2022-01-20 20:23:59,823 iteration 2081 : loss : 0.034416, loss_ce: 0.012318
2022-01-20 20:24:01,160 iteration 2082 : loss : 0.045414, loss_ce: 0.013213
2022-01-20 20:24:02,485 iteration 2083 : loss : 0.035319, loss_ce: 0.012795
2022-01-20 20:24:03,765 iteration 2084 : loss : 0.037045, loss_ce: 0.017021
2022-01-20 20:24:05,127 iteration 2085 : loss : 0.088871, loss_ce: 0.016439
2022-01-20 20:24:06,567 iteration 2086 : loss : 0.035354, loss_ce: 0.010450
2022-01-20 20:24:07,839 iteration 2087 : loss : 0.037939, loss_ce: 0.012071
2022-01-20 20:24:09,164 iteration 2088 : loss : 0.037118, loss_ce: 0.013546
2022-01-20 20:24:10,521 iteration 2089 : loss : 0.032636, loss_ce: 0.013017
2022-01-20 20:24:11,868 iteration 2090 : loss : 0.037892, loss_ce: 0.018008
2022-01-20 20:24:13,302 iteration 2091 : loss : 0.055301, loss_ce: 0.022256
 31%|████████▉                    | 123/400 [51:13<1:53:50, 24.66s/it]2022-01-20 20:24:14,665 iteration 2092 : loss : 0.030671, loss_ce: 0.010420
2022-01-20 20:24:15,945 iteration 2093 : loss : 0.023022, loss_ce: 0.009206
2022-01-20 20:24:17,322 iteration 2094 : loss : 0.045905, loss_ce: 0.017835
2022-01-20 20:24:18,832 iteration 2095 : loss : 0.034052, loss_ce: 0.012693
2022-01-20 20:24:20,159 iteration 2096 : loss : 0.037577, loss_ce: 0.015053
2022-01-20 20:24:21,485 iteration 2097 : loss : 0.056672, loss_ce: 0.015615
2022-01-20 20:24:22,764 iteration 2098 : loss : 0.031623, loss_ce: 0.011904
2022-01-20 20:24:24,088 iteration 2099 : loss : 0.025910, loss_ce: 0.010663
2022-01-20 20:24:25,470 iteration 2100 : loss : 0.036201, loss_ce: 0.013957
2022-01-20 20:24:26,719 iteration 2101 : loss : 0.037557, loss_ce: 0.009858
2022-01-20 20:24:28,092 iteration 2102 : loss : 0.041434, loss_ce: 0.017040
2022-01-20 20:24:29,387 iteration 2103 : loss : 0.033298, loss_ce: 0.016969
2022-01-20 20:24:30,646 iteration 2104 : loss : 0.030322, loss_ce: 0.010250
2022-01-20 20:24:31,995 iteration 2105 : loss : 0.036374, loss_ce: 0.015488
2022-01-20 20:24:33,353 iteration 2106 : loss : 0.047049, loss_ce: 0.018680
2022-01-20 20:24:34,650 iteration 2107 : loss : 0.039581, loss_ce: 0.012538
2022-01-20 20:24:35,925 iteration 2108 : loss : 0.033776, loss_ce: 0.013361
 31%|████████▉                    | 124/400 [51:36<1:50:37, 24.05s/it]2022-01-20 20:24:37,311 iteration 2109 : loss : 0.056987, loss_ce: 0.017735
2022-01-20 20:24:38,672 iteration 2110 : loss : 0.036990, loss_ce: 0.016811
2022-01-20 20:24:40,070 iteration 2111 : loss : 0.027073, loss_ce: 0.009187
2022-01-20 20:24:41,519 iteration 2112 : loss : 0.037591, loss_ce: 0.015973
2022-01-20 20:24:42,929 iteration 2113 : loss : 0.035165, loss_ce: 0.010744
2022-01-20 20:24:44,319 iteration 2114 : loss : 0.039267, loss_ce: 0.013110
2022-01-20 20:24:45,632 iteration 2115 : loss : 0.028866, loss_ce: 0.009024
2022-01-20 20:24:46,936 iteration 2116 : loss : 0.039749, loss_ce: 0.012729
2022-01-20 20:24:48,377 iteration 2117 : loss : 0.040577, loss_ce: 0.012499
2022-01-20 20:24:49,736 iteration 2118 : loss : 0.037215, loss_ce: 0.014392
2022-01-20 20:24:51,199 iteration 2119 : loss : 0.035499, loss_ce: 0.015529
2022-01-20 20:24:52,606 iteration 2120 : loss : 0.040925, loss_ce: 0.022687
2022-01-20 20:24:54,014 iteration 2121 : loss : 0.053006, loss_ce: 0.022704
2022-01-20 20:24:55,413 iteration 2122 : loss : 0.063462, loss_ce: 0.035011
2022-01-20 20:24:56,693 iteration 2123 : loss : 0.036130, loss_ce: 0.012030
2022-01-20 20:24:58,104 iteration 2124 : loss : 0.037178, loss_ce: 0.014343
2022-01-20 20:24:58,105 Training Data Eval:
2022-01-20 20:25:04,714   Average segmentation loss on training set: 0.0311
2022-01-20 20:25:04,715 Validation Data Eval:
2022-01-20 20:25:06,977   Average segmentation loss on validation set: 0.0629
2022-01-20 20:25:12,798 Found new lowest validation loss at iteration 2124! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed2.pth
2022-01-20 20:25:14,021 iteration 2125 : loss : 0.033543, loss_ce: 0.016044
 31%|█████████                    | 125/400 [52:14<2:09:32, 28.26s/it]2022-01-20 20:25:15,334 iteration 2126 : loss : 0.087260, loss_ce: 0.024026
2022-01-20 20:25:16,596 iteration 2127 : loss : 0.029184, loss_ce: 0.012863
2022-01-20 20:25:17,867 iteration 2128 : loss : 0.030947, loss_ce: 0.012689
2022-01-20 20:25:19,277 iteration 2129 : loss : 0.043797, loss_ce: 0.019646
2022-01-20 20:25:20,604 iteration 2130 : loss : 0.036794, loss_ce: 0.016178
2022-01-20 20:25:21,891 iteration 2131 : loss : 0.025626, loss_ce: 0.009486
2022-01-20 20:25:23,257 iteration 2132 : loss : 0.039496, loss_ce: 0.010243
2022-01-20 20:25:24,592 iteration 2133 : loss : 0.024843, loss_ce: 0.010824
2022-01-20 20:25:26,005 iteration 2134 : loss : 0.049798, loss_ce: 0.018623
2022-01-20 20:25:27,278 iteration 2135 : loss : 0.031938, loss_ce: 0.008964
2022-01-20 20:25:28,676 iteration 2136 : loss : 0.050567, loss_ce: 0.022903
2022-01-20 20:25:30,009 iteration 2137 : loss : 0.031947, loss_ce: 0.014692
2022-01-20 20:25:31,427 iteration 2138 : loss : 0.028657, loss_ce: 0.008550
2022-01-20 20:25:32,752 iteration 2139 : loss : 0.040780, loss_ce: 0.022366
2022-01-20 20:25:34,142 iteration 2140 : loss : 0.039467, loss_ce: 0.022413
2022-01-20 20:25:35,511 iteration 2141 : loss : 0.047257, loss_ce: 0.014645
2022-01-20 20:25:36,892 iteration 2142 : loss : 0.049785, loss_ce: 0.015578
 32%|█████████▏                   | 126/400 [52:37<2:01:40, 26.65s/it]2022-01-20 20:25:38,294 iteration 2143 : loss : 0.038389, loss_ce: 0.022445
2022-01-20 20:25:39,665 iteration 2144 : loss : 0.039282, loss_ce: 0.016347
2022-01-20 20:25:41,019 iteration 2145 : loss : 0.031038, loss_ce: 0.013645
2022-01-20 20:25:42,337 iteration 2146 : loss : 0.031689, loss_ce: 0.011980
2022-01-20 20:25:43,630 iteration 2147 : loss : 0.026929, loss_ce: 0.009857
2022-01-20 20:25:44,954 iteration 2148 : loss : 0.030537, loss_ce: 0.011429
2022-01-20 20:25:46,357 iteration 2149 : loss : 0.043466, loss_ce: 0.013723
2022-01-20 20:25:47,663 iteration 2150 : loss : 0.028309, loss_ce: 0.011681
2022-01-20 20:25:49,004 iteration 2151 : loss : 0.031696, loss_ce: 0.012024
2022-01-20 20:25:50,290 iteration 2152 : loss : 0.029756, loss_ce: 0.014071
2022-01-20 20:25:51,625 iteration 2153 : loss : 0.037307, loss_ce: 0.011487
2022-01-20 20:25:52,961 iteration 2154 : loss : 0.026859, loss_ce: 0.009593
2022-01-20 20:25:54,331 iteration 2155 : loss : 0.041385, loss_ce: 0.019355
2022-01-20 20:25:55,615 iteration 2156 : loss : 0.032639, loss_ce: 0.011696
2022-01-20 20:25:57,051 iteration 2157 : loss : 0.034101, loss_ce: 0.013480
2022-01-20 20:25:58,459 iteration 2158 : loss : 0.035247, loss_ce: 0.011747
2022-01-20 20:25:59,837 iteration 2159 : loss : 0.041996, loss_ce: 0.013911
 32%|█████████▏                   | 127/400 [53:00<1:56:10, 25.53s/it]2022-01-20 20:26:01,233 iteration 2160 : loss : 0.033789, loss_ce: 0.011506
2022-01-20 20:26:02,672 iteration 2161 : loss : 0.050124, loss_ce: 0.021689
2022-01-20 20:26:04,090 iteration 2162 : loss : 0.029189, loss_ce: 0.010615
2022-01-20 20:26:05,386 iteration 2163 : loss : 0.033721, loss_ce: 0.011354
2022-01-20 20:26:06,787 iteration 2164 : loss : 0.038235, loss_ce: 0.014465
2022-01-20 20:26:08,178 iteration 2165 : loss : 0.023920, loss_ce: 0.009605
2022-01-20 20:26:09,559 iteration 2166 : loss : 0.036805, loss_ce: 0.009495
2022-01-20 20:26:10,969 iteration 2167 : loss : 0.031448, loss_ce: 0.011552
2022-01-20 20:26:12,327 iteration 2168 : loss : 0.032253, loss_ce: 0.012084
2022-01-20 20:26:13,676 iteration 2169 : loss : 0.031920, loss_ce: 0.011361
2022-01-20 20:26:14,977 iteration 2170 : loss : 0.059055, loss_ce: 0.026788
2022-01-20 20:26:16,375 iteration 2171 : loss : 0.067051, loss_ce: 0.021001
2022-01-20 20:26:17,834 iteration 2172 : loss : 0.029606, loss_ce: 0.008383
2022-01-20 20:26:19,140 iteration 2173 : loss : 0.033580, loss_ce: 0.010324
2022-01-20 20:26:20,405 iteration 2174 : loss : 0.027305, loss_ce: 0.013350
2022-01-20 20:26:21,702 iteration 2175 : loss : 0.027982, loss_ce: 0.010019
2022-01-20 20:26:23,170 iteration 2176 : loss : 0.039108, loss_ce: 0.015465
 32%|█████████▎                   | 128/400 [53:23<1:52:46, 24.88s/it]2022-01-20 20:26:24,595 iteration 2177 : loss : 0.039378, loss_ce: 0.012992
2022-01-20 20:26:26,004 iteration 2178 : loss : 0.041256, loss_ce: 0.017769
2022-01-20 20:26:27,332 iteration 2179 : loss : 0.037345, loss_ce: 0.018443
2022-01-20 20:26:28,663 iteration 2180 : loss : 0.029767, loss_ce: 0.012936
2022-01-20 20:26:30,049 iteration 2181 : loss : 0.031401, loss_ce: 0.015097
2022-01-20 20:26:31,377 iteration 2182 : loss : 0.049650, loss_ce: 0.012910
2022-01-20 20:26:32,659 iteration 2183 : loss : 0.029794, loss_ce: 0.012520
2022-01-20 20:26:34,069 iteration 2184 : loss : 0.048994, loss_ce: 0.014365
2022-01-20 20:26:35,369 iteration 2185 : loss : 0.027718, loss_ce: 0.008971
2022-01-20 20:26:36,690 iteration 2186 : loss : 0.035520, loss_ce: 0.011353
2022-01-20 20:26:38,022 iteration 2187 : loss : 0.026916, loss_ce: 0.009849
2022-01-20 20:26:39,328 iteration 2188 : loss : 0.044122, loss_ce: 0.012509
2022-01-20 20:26:40,730 iteration 2189 : loss : 0.043620, loss_ce: 0.015310
2022-01-20 20:26:42,070 iteration 2190 : loss : 0.035422, loss_ce: 0.012593
2022-01-20 20:26:43,436 iteration 2191 : loss : 0.071891, loss_ce: 0.020498
2022-01-20 20:26:44,741 iteration 2192 : loss : 0.031388, loss_ce: 0.010448
2022-01-20 20:26:46,092 iteration 2193 : loss : 0.035063, loss_ce: 0.012979
 32%|█████████▎                   | 129/400 [53:46<1:49:42, 24.29s/it]2022-01-20 20:26:47,524 iteration 2194 : loss : 0.043912, loss_ce: 0.024921
2022-01-20 20:26:48,849 iteration 2195 : loss : 0.034104, loss_ce: 0.011227
2022-01-20 20:26:50,186 iteration 2196 : loss : 0.033373, loss_ce: 0.012317
2022-01-20 20:26:51,558 iteration 2197 : loss : 0.050499, loss_ce: 0.029212
2022-01-20 20:26:52,929 iteration 2198 : loss : 0.041142, loss_ce: 0.019130
2022-01-20 20:26:54,278 iteration 2199 : loss : 0.038429, loss_ce: 0.017683
2022-01-20 20:26:55,593 iteration 2200 : loss : 0.039525, loss_ce: 0.013227
2022-01-20 20:26:56,894 iteration 2201 : loss : 0.026426, loss_ce: 0.010292
2022-01-20 20:26:58,288 iteration 2202 : loss : 0.098133, loss_ce: 0.026564
2022-01-20 20:26:59,691 iteration 2203 : loss : 0.033869, loss_ce: 0.019335
2022-01-20 20:27:01,002 iteration 2204 : loss : 0.033192, loss_ce: 0.014850
2022-01-20 20:27:02,266 iteration 2205 : loss : 0.024723, loss_ce: 0.010554
2022-01-20 20:27:03,528 iteration 2206 : loss : 0.036094, loss_ce: 0.012748
2022-01-20 20:27:04,828 iteration 2207 : loss : 0.043354, loss_ce: 0.016966
2022-01-20 20:27:06,188 iteration 2208 : loss : 0.028481, loss_ce: 0.008469
2022-01-20 20:27:07,510 iteration 2209 : loss : 0.040074, loss_ce: 0.011588
2022-01-20 20:27:07,510 Training Data Eval:
2022-01-20 20:27:14,116   Average segmentation loss on training set: 0.0238
2022-01-20 20:27:14,116 Validation Data Eval:
2022-01-20 20:27:16,375   Average segmentation loss on validation set: 0.0834
2022-01-20 20:27:17,723 iteration 2210 : loss : 0.037419, loss_ce: 0.012454
 32%|█████████▍                   | 130/400 [54:17<1:59:12, 26.49s/it]2022-01-20 20:27:19,137 iteration 2211 : loss : 0.046086, loss_ce: 0.014516
2022-01-20 20:27:20,508 iteration 2212 : loss : 0.035076, loss_ce: 0.013644
2022-01-20 20:27:21,884 iteration 2213 : loss : 0.037303, loss_ce: 0.013844
2022-01-20 20:27:23,257 iteration 2214 : loss : 0.033425, loss_ce: 0.011236
2022-01-20 20:27:24,656 iteration 2215 : loss : 0.050149, loss_ce: 0.019366
2022-01-20 20:27:25,936 iteration 2216 : loss : 0.029621, loss_ce: 0.009334
2022-01-20 20:27:27,348 iteration 2217 : loss : 0.033836, loss_ce: 0.015206
2022-01-20 20:27:28,653 iteration 2218 : loss : 0.038137, loss_ce: 0.013796
2022-01-20 20:27:30,056 iteration 2219 : loss : 0.033921, loss_ce: 0.014418
2022-01-20 20:27:31,313 iteration 2220 : loss : 0.037993, loss_ce: 0.014513
2022-01-20 20:27:32,625 iteration 2221 : loss : 0.026806, loss_ce: 0.009875
2022-01-20 20:27:34,023 iteration 2222 : loss : 0.034658, loss_ce: 0.013420
2022-01-20 20:27:35,396 iteration 2223 : loss : 0.045346, loss_ce: 0.014044
2022-01-20 20:27:36,818 iteration 2224 : loss : 0.030881, loss_ce: 0.014764
2022-01-20 20:27:38,107 iteration 2225 : loss : 0.024002, loss_ce: 0.009341
2022-01-20 20:27:39,534 iteration 2226 : loss : 0.042684, loss_ce: 0.019020
2022-01-20 20:27:40,862 iteration 2227 : loss : 0.027042, loss_ce: 0.010603
 33%|█████████▍                   | 131/400 [54:41<1:54:15, 25.49s/it]2022-01-20 20:27:42,305 iteration 2228 : loss : 0.026977, loss_ce: 0.012020
2022-01-20 20:27:43,660 iteration 2229 : loss : 0.037449, loss_ce: 0.017189
2022-01-20 20:27:45,057 iteration 2230 : loss : 0.028261, loss_ce: 0.009128
2022-01-20 20:27:46,309 iteration 2231 : loss : 0.021766, loss_ce: 0.008593
2022-01-20 20:27:47,608 iteration 2232 : loss : 0.045063, loss_ce: 0.018154
2022-01-20 20:27:48,924 iteration 2233 : loss : 0.031429, loss_ce: 0.012489
2022-01-20 20:27:50,247 iteration 2234 : loss : 0.034158, loss_ce: 0.011899
2022-01-20 20:27:51,566 iteration 2235 : loss : 0.067783, loss_ce: 0.013662
2022-01-20 20:27:53,038 iteration 2236 : loss : 0.036144, loss_ce: 0.015095
2022-01-20 20:27:54,386 iteration 2237 : loss : 0.026081, loss_ce: 0.011189
2022-01-20 20:27:55,741 iteration 2238 : loss : 0.044207, loss_ce: 0.019258
2022-01-20 20:27:57,092 iteration 2239 : loss : 0.030396, loss_ce: 0.010715
2022-01-20 20:27:58,492 iteration 2240 : loss : 0.040389, loss_ce: 0.012450
2022-01-20 20:27:59,914 iteration 2241 : loss : 0.033839, loss_ce: 0.010833
2022-01-20 20:28:01,329 iteration 2242 : loss : 0.034545, loss_ce: 0.013119
2022-01-20 20:28:02,793 iteration 2243 : loss : 0.069313, loss_ce: 0.017833
2022-01-20 20:28:04,144 iteration 2244 : loss : 0.032885, loss_ce: 0.014374
 33%|█████████▌                   | 132/400 [55:04<1:50:52, 24.82s/it]2022-01-20 20:28:05,626 iteration 2245 : loss : 0.067807, loss_ce: 0.030333
2022-01-20 20:28:06,944 iteration 2246 : loss : 0.033704, loss_ce: 0.012138
2022-01-20 20:28:08,391 iteration 2247 : loss : 0.049857, loss_ce: 0.012770
2022-01-20 20:28:09,769 iteration 2248 : loss : 0.044670, loss_ce: 0.018478
2022-01-20 20:28:11,132 iteration 2249 : loss : 0.032629, loss_ce: 0.010206
2022-01-20 20:28:12,473 iteration 2250 : loss : 0.045438, loss_ce: 0.018618
2022-01-20 20:28:13,748 iteration 2251 : loss : 0.036637, loss_ce: 0.013021
2022-01-20 20:28:15,197 iteration 2252 : loss : 0.053911, loss_ce: 0.019836
2022-01-20 20:28:16,626 iteration 2253 : loss : 0.039531, loss_ce: 0.016941
2022-01-20 20:28:17,969 iteration 2254 : loss : 0.030469, loss_ce: 0.012553
2022-01-20 20:28:19,285 iteration 2255 : loss : 0.035366, loss_ce: 0.013627
2022-01-20 20:28:20,654 iteration 2256 : loss : 0.033484, loss_ce: 0.012645
2022-01-20 20:28:21,990 iteration 2257 : loss : 0.028366, loss_ce: 0.010602
2022-01-20 20:28:23,270 iteration 2258 : loss : 0.023338, loss_ce: 0.009922
2022-01-20 20:28:24,711 iteration 2259 : loss : 0.039723, loss_ce: 0.012739
2022-01-20 20:28:26,026 iteration 2260 : loss : 0.028958, loss_ce: 0.010913
2022-01-20 20:28:27,396 iteration 2261 : loss : 0.028322, loss_ce: 0.010519
 33%|█████████▋                   | 133/400 [55:27<1:48:21, 24.35s/it]2022-01-20 20:28:28,813 iteration 2262 : loss : 0.034941, loss_ce: 0.013691
2022-01-20 20:28:30,175 iteration 2263 : loss : 0.035144, loss_ce: 0.015612
2022-01-20 20:28:31,582 iteration 2264 : loss : 0.044614, loss_ce: 0.017924
2022-01-20 20:28:32,924 iteration 2265 : loss : 0.025990, loss_ce: 0.009607
2022-01-20 20:28:34,259 iteration 2266 : loss : 0.037315, loss_ce: 0.010628
2022-01-20 20:28:35,527 iteration 2267 : loss : 0.022186, loss_ce: 0.008892
2022-01-20 20:28:36,851 iteration 2268 : loss : 0.027984, loss_ce: 0.013776
2022-01-20 20:28:38,153 iteration 2269 : loss : 0.035732, loss_ce: 0.016315
2022-01-20 20:28:39,434 iteration 2270 : loss : 0.024478, loss_ce: 0.008213
2022-01-20 20:28:40,766 iteration 2271 : loss : 0.043927, loss_ce: 0.022307
2022-01-20 20:28:42,165 iteration 2272 : loss : 0.047358, loss_ce: 0.020296
2022-01-20 20:28:43,565 iteration 2273 : loss : 0.034517, loss_ce: 0.011468
2022-01-20 20:28:44,912 iteration 2274 : loss : 0.030358, loss_ce: 0.012764
2022-01-20 20:28:46,246 iteration 2275 : loss : 0.060451, loss_ce: 0.019995
2022-01-20 20:28:47,539 iteration 2276 : loss : 0.031494, loss_ce: 0.013079
2022-01-20 20:28:48,865 iteration 2277 : loss : 0.025981, loss_ce: 0.007444
2022-01-20 20:28:50,229 iteration 2278 : loss : 0.046182, loss_ce: 0.017307
 34%|█████████▋                   | 134/400 [55:50<1:45:56, 23.90s/it]2022-01-20 20:28:51,548 iteration 2279 : loss : 0.025839, loss_ce: 0.008959
2022-01-20 20:28:52,827 iteration 2280 : loss : 0.036231, loss_ce: 0.017834
2022-01-20 20:28:54,164 iteration 2281 : loss : 0.051376, loss_ce: 0.017540
2022-01-20 20:28:55,435 iteration 2282 : loss : 0.040020, loss_ce: 0.015878
2022-01-20 20:28:56,789 iteration 2283 : loss : 0.054872, loss_ce: 0.016231
2022-01-20 20:28:58,057 iteration 2284 : loss : 0.031308, loss_ce: 0.013975
2022-01-20 20:28:59,432 iteration 2285 : loss : 0.040281, loss_ce: 0.016819
2022-01-20 20:29:00,748 iteration 2286 : loss : 0.054208, loss_ce: 0.023205
2022-01-20 20:29:02,082 iteration 2287 : loss : 0.036334, loss_ce: 0.013514
2022-01-20 20:29:03,421 iteration 2288 : loss : 0.043682, loss_ce: 0.012816
2022-01-20 20:29:04,779 iteration 2289 : loss : 0.028833, loss_ce: 0.009209
2022-01-20 20:29:06,127 iteration 2290 : loss : 0.036033, loss_ce: 0.014253
2022-01-20 20:29:07,566 iteration 2291 : loss : 0.062564, loss_ce: 0.023250
2022-01-20 20:29:08,947 iteration 2292 : loss : 0.030570, loss_ce: 0.011282
2022-01-20 20:29:10,300 iteration 2293 : loss : 0.046698, loss_ce: 0.019191
2022-01-20 20:29:11,627 iteration 2294 : loss : 0.031585, loss_ce: 0.014861
2022-01-20 20:29:11,627 Training Data Eval:
2022-01-20 20:29:18,240   Average segmentation loss on training set: 0.0363
2022-01-20 20:29:18,240 Validation Data Eval:
2022-01-20 20:29:20,503   Average segmentation loss on validation set: 0.0805
2022-01-20 20:29:21,842 iteration 2295 : loss : 0.046430, loss_ce: 0.014080
 34%|█████████▊                   | 135/400 [56:22<1:55:45, 26.21s/it]2022-01-20 20:29:23,164 iteration 2296 : loss : 0.027553, loss_ce: 0.011143
2022-01-20 20:29:24,561 iteration 2297 : loss : 0.033431, loss_ce: 0.011519
2022-01-20 20:29:25,915 iteration 2298 : loss : 0.038831, loss_ce: 0.012492
2022-01-20 20:29:27,299 iteration 2299 : loss : 0.049992, loss_ce: 0.016159
2022-01-20 20:29:28,617 iteration 2300 : loss : 0.024428, loss_ce: 0.010249
2022-01-20 20:29:29,968 iteration 2301 : loss : 0.045509, loss_ce: 0.023122
2022-01-20 20:29:31,357 iteration 2302 : loss : 0.038105, loss_ce: 0.013358
2022-01-20 20:29:32,735 iteration 2303 : loss : 0.045740, loss_ce: 0.014445
2022-01-20 20:29:34,111 iteration 2304 : loss : 0.031403, loss_ce: 0.011637
2022-01-20 20:29:35,442 iteration 2305 : loss : 0.041464, loss_ce: 0.017087
2022-01-20 20:29:36,757 iteration 2306 : loss : 0.029806, loss_ce: 0.011365
2022-01-20 20:29:38,173 iteration 2307 : loss : 0.033289, loss_ce: 0.012570
2022-01-20 20:29:39,479 iteration 2308 : loss : 0.030464, loss_ce: 0.011659
2022-01-20 20:29:40,873 iteration 2309 : loss : 0.049292, loss_ce: 0.019885
2022-01-20 20:29:42,244 iteration 2310 : loss : 0.038500, loss_ce: 0.017935
2022-01-20 20:29:43,553 iteration 2311 : loss : 0.041045, loss_ce: 0.018226
2022-01-20 20:29:44,835 iteration 2312 : loss : 0.026646, loss_ce: 0.009599
 34%|█████████▊                   | 136/400 [56:45<1:51:05, 25.25s/it]2022-01-20 20:29:46,207 iteration 2313 : loss : 0.037072, loss_ce: 0.011911
2022-01-20 20:29:47,574 iteration 2314 : loss : 0.024464, loss_ce: 0.007736
2022-01-20 20:29:49,015 iteration 2315 : loss : 0.053725, loss_ce: 0.013352
2022-01-20 20:29:50,288 iteration 2316 : loss : 0.029156, loss_ce: 0.010280
2022-01-20 20:29:51,699 iteration 2317 : loss : 0.038870, loss_ce: 0.009840
2022-01-20 20:29:53,067 iteration 2318 : loss : 0.035764, loss_ce: 0.015220
2022-01-20 20:29:54,429 iteration 2319 : loss : 0.063102, loss_ce: 0.035869
2022-01-20 20:29:55,831 iteration 2320 : loss : 0.027273, loss_ce: 0.011870
2022-01-20 20:29:57,128 iteration 2321 : loss : 0.033836, loss_ce: 0.010695
2022-01-20 20:29:58,466 iteration 2322 : loss : 0.039133, loss_ce: 0.022476
2022-01-20 20:29:59,800 iteration 2323 : loss : 0.032807, loss_ce: 0.014651
2022-01-20 20:30:01,174 iteration 2324 : loss : 0.035312, loss_ce: 0.012604
2022-01-20 20:30:02,497 iteration 2325 : loss : 0.027106, loss_ce: 0.011542
2022-01-20 20:30:03,854 iteration 2326 : loss : 0.037461, loss_ce: 0.019076
2022-01-20 20:30:05,182 iteration 2327 : loss : 0.031748, loss_ce: 0.014177
2022-01-20 20:30:06,559 iteration 2328 : loss : 0.043832, loss_ce: 0.020900
2022-01-20 20:30:07,872 iteration 2329 : loss : 0.025858, loss_ce: 0.010750
 34%|█████████▉                   | 137/400 [57:08<1:47:45, 24.58s/it]2022-01-20 20:30:09,231 iteration 2330 : loss : 0.028712, loss_ce: 0.009877
2022-01-20 20:30:10,570 iteration 2331 : loss : 0.041919, loss_ce: 0.017019
2022-01-20 20:30:11,963 iteration 2332 : loss : 0.046258, loss_ce: 0.019056
2022-01-20 20:30:13,373 iteration 2333 : loss : 0.038909, loss_ce: 0.019002
2022-01-20 20:30:14,700 iteration 2334 : loss : 0.030850, loss_ce: 0.013619
2022-01-20 20:30:16,033 iteration 2335 : loss : 0.035496, loss_ce: 0.010023
2022-01-20 20:30:17,429 iteration 2336 : loss : 0.050353, loss_ce: 0.012410
2022-01-20 20:30:18,819 iteration 2337 : loss : 0.043955, loss_ce: 0.024330
2022-01-20 20:30:20,155 iteration 2338 : loss : 0.027543, loss_ce: 0.009612
2022-01-20 20:30:21,531 iteration 2339 : loss : 0.044716, loss_ce: 0.014787
2022-01-20 20:30:22,952 iteration 2340 : loss : 0.042538, loss_ce: 0.017906
2022-01-20 20:30:24,235 iteration 2341 : loss : 0.025089, loss_ce: 0.006431
2022-01-20 20:30:25,586 iteration 2342 : loss : 0.033255, loss_ce: 0.013434
2022-01-20 20:30:26,997 iteration 2343 : loss : 0.044509, loss_ce: 0.016816
2022-01-20 20:30:28,380 iteration 2344 : loss : 0.038037, loss_ce: 0.013520
2022-01-20 20:30:29,785 iteration 2345 : loss : 0.034316, loss_ce: 0.010426
2022-01-20 20:30:31,146 iteration 2346 : loss : 0.032934, loss_ce: 0.013055
 34%|██████████                   | 138/400 [57:31<1:45:37, 24.19s/it]2022-01-20 20:30:32,544 iteration 2347 : loss : 0.033305, loss_ce: 0.010650
2022-01-20 20:30:33,877 iteration 2348 : loss : 0.035858, loss_ce: 0.012646
2022-01-20 20:30:35,282 iteration 2349 : loss : 0.032230, loss_ce: 0.012981
2022-01-20 20:30:36,615 iteration 2350 : loss : 0.038148, loss_ce: 0.014046
2022-01-20 20:30:37,978 iteration 2351 : loss : 0.087255, loss_ce: 0.020933
2022-01-20 20:30:39,337 iteration 2352 : loss : 0.031740, loss_ce: 0.010057
2022-01-20 20:30:40,737 iteration 2353 : loss : 0.042252, loss_ce: 0.016553
2022-01-20 20:30:42,148 iteration 2354 : loss : 0.036938, loss_ce: 0.013048
2022-01-20 20:30:43,425 iteration 2355 : loss : 0.039855, loss_ce: 0.015970
2022-01-20 20:30:44,769 iteration 2356 : loss : 0.036499, loss_ce: 0.013066
2022-01-20 20:30:46,111 iteration 2357 : loss : 0.046971, loss_ce: 0.014370
2022-01-20 20:30:47,486 iteration 2358 : loss : 0.034067, loss_ce: 0.014952
2022-01-20 20:30:48,951 iteration 2359 : loss : 0.042845, loss_ce: 0.017511
2022-01-20 20:30:50,269 iteration 2360 : loss : 0.026066, loss_ce: 0.011440
2022-01-20 20:30:51,647 iteration 2361 : loss : 0.036589, loss_ce: 0.011750
2022-01-20 20:30:53,098 iteration 2362 : loss : 0.046160, loss_ce: 0.021948
2022-01-20 20:30:54,390 iteration 2363 : loss : 0.030786, loss_ce: 0.013142
 35%|██████████                   | 139/400 [57:54<1:43:59, 23.91s/it]2022-01-20 20:30:55,701 iteration 2364 : loss : 0.081103, loss_ce: 0.014599
2022-01-20 20:30:57,004 iteration 2365 : loss : 0.050118, loss_ce: 0.013100
2022-01-20 20:30:58,376 iteration 2366 : loss : 0.049087, loss_ce: 0.019593
2022-01-20 20:30:59,710 iteration 2367 : loss : 0.054883, loss_ce: 0.027317
2022-01-20 20:31:01,033 iteration 2368 : loss : 0.041070, loss_ce: 0.016575
2022-01-20 20:31:02,408 iteration 2369 : loss : 0.039576, loss_ce: 0.016476
2022-01-20 20:31:03,679 iteration 2370 : loss : 0.033211, loss_ce: 0.010934
2022-01-20 20:31:05,085 iteration 2371 : loss : 0.034315, loss_ce: 0.014004
2022-01-20 20:31:06,432 iteration 2372 : loss : 0.036991, loss_ce: 0.013275
2022-01-20 20:31:07,734 iteration 2373 : loss : 0.030512, loss_ce: 0.011529
2022-01-20 20:31:09,055 iteration 2374 : loss : 0.037965, loss_ce: 0.013207
2022-01-20 20:31:10,439 iteration 2375 : loss : 0.051423, loss_ce: 0.013691
2022-01-20 20:31:11,834 iteration 2376 : loss : 0.045775, loss_ce: 0.020174
2022-01-20 20:31:13,172 iteration 2377 : loss : 0.032634, loss_ce: 0.011007
2022-01-20 20:31:14,511 iteration 2378 : loss : 0.025955, loss_ce: 0.010472
2022-01-20 20:31:15,864 iteration 2379 : loss : 0.039457, loss_ce: 0.020886
2022-01-20 20:31:15,864 Training Data Eval:
2022-01-20 20:31:22,473   Average segmentation loss on training set: 0.0398
2022-01-20 20:31:22,476 Validation Data Eval:
2022-01-20 20:31:24,740   Average segmentation loss on validation set: 0.1815
2022-01-20 20:31:26,042 iteration 2380 : loss : 0.026265, loss_ce: 0.012640
 35%|██████████▏                  | 140/400 [58:26<1:53:40, 26.23s/it]2022-01-20 20:31:27,534 iteration 2381 : loss : 0.051305, loss_ce: 0.016065
2022-01-20 20:31:28,902 iteration 2382 : loss : 0.035749, loss_ce: 0.012530
2022-01-20 20:31:30,238 iteration 2383 : loss : 0.041587, loss_ce: 0.015935
2022-01-20 20:31:31,530 iteration 2384 : loss : 0.030853, loss_ce: 0.011002
2022-01-20 20:31:32,873 iteration 2385 : loss : 0.029133, loss_ce: 0.009738
2022-01-20 20:31:34,261 iteration 2386 : loss : 0.035911, loss_ce: 0.009736
2022-01-20 20:31:35,657 iteration 2387 : loss : 0.029564, loss_ce: 0.010643
2022-01-20 20:31:37,014 iteration 2388 : loss : 0.034897, loss_ce: 0.016604
2022-01-20 20:31:38,368 iteration 2389 : loss : 0.032165, loss_ce: 0.011408
2022-01-20 20:31:39,675 iteration 2390 : loss : 0.035537, loss_ce: 0.013913
2022-01-20 20:31:40,948 iteration 2391 : loss : 0.029617, loss_ce: 0.011102
2022-01-20 20:31:42,422 iteration 2392 : loss : 0.033821, loss_ce: 0.008544
2022-01-20 20:31:43,835 iteration 2393 : loss : 0.039425, loss_ce: 0.013977
2022-01-20 20:31:45,191 iteration 2394 : loss : 0.051707, loss_ce: 0.023135
2022-01-20 20:31:46,605 iteration 2395 : loss : 0.028239, loss_ce: 0.012479
2022-01-20 20:31:47,903 iteration 2396 : loss : 0.030399, loss_ce: 0.013004
2022-01-20 20:31:49,319 iteration 2397 : loss : 0.025268, loss_ce: 0.012393
 35%|██████████▏                  | 141/400 [58:49<1:49:24, 25.35s/it]2022-01-20 20:31:50,684 iteration 2398 : loss : 0.025994, loss_ce: 0.012001
2022-01-20 20:31:52,116 iteration 2399 : loss : 0.033660, loss_ce: 0.014875
2022-01-20 20:31:53,450 iteration 2400 : loss : 0.040838, loss_ce: 0.015803
2022-01-20 20:31:54,927 iteration 2401 : loss : 0.053781, loss_ce: 0.019571
2022-01-20 20:31:56,270 iteration 2402 : loss : 0.045809, loss_ce: 0.019480
2022-01-20 20:31:57,596 iteration 2403 : loss : 0.033126, loss_ce: 0.012958
2022-01-20 20:31:58,903 iteration 2404 : loss : 0.023014, loss_ce: 0.007628
2022-01-20 20:32:00,298 iteration 2405 : loss : 0.048110, loss_ce: 0.015216
2022-01-20 20:32:01,585 iteration 2406 : loss : 0.033000, loss_ce: 0.013534
2022-01-20 20:32:02,915 iteration 2407 : loss : 0.024173, loss_ce: 0.007992
2022-01-20 20:32:04,408 iteration 2408 : loss : 0.041301, loss_ce: 0.010612
2022-01-20 20:32:05,717 iteration 2409 : loss : 0.027988, loss_ce: 0.011746
2022-01-20 20:32:07,030 iteration 2410 : loss : 0.025613, loss_ce: 0.009019
2022-01-20 20:32:08,336 iteration 2411 : loss : 0.027651, loss_ce: 0.010663
2022-01-20 20:32:09,650 iteration 2412 : loss : 0.033544, loss_ce: 0.013415
2022-01-20 20:32:10,942 iteration 2413 : loss : 0.042837, loss_ce: 0.016801
2022-01-20 20:32:12,230 iteration 2414 : loss : 0.024709, loss_ce: 0.009821
 36%|██████████▎                  | 142/400 [59:12<1:45:50, 24.61s/it]2022-01-20 20:32:13,663 iteration 2415 : loss : 0.035467, loss_ce: 0.012513
2022-01-20 20:32:14,979 iteration 2416 : loss : 0.028877, loss_ce: 0.011310
2022-01-20 20:32:16,320 iteration 2417 : loss : 0.024771, loss_ce: 0.011700
2022-01-20 20:32:17,608 iteration 2418 : loss : 0.029910, loss_ce: 0.010674
2022-01-20 20:32:18,949 iteration 2419 : loss : 0.050502, loss_ce: 0.016981
2022-01-20 20:32:20,360 iteration 2420 : loss : 0.038473, loss_ce: 0.017712
2022-01-20 20:32:21,698 iteration 2421 : loss : 0.025362, loss_ce: 0.010405
2022-01-20 20:32:23,040 iteration 2422 : loss : 0.031171, loss_ce: 0.012729
2022-01-20 20:32:24,312 iteration 2423 : loss : 0.040926, loss_ce: 0.016971
2022-01-20 20:32:25,659 iteration 2424 : loss : 0.027929, loss_ce: 0.010597
2022-01-20 20:32:27,038 iteration 2425 : loss : 0.032059, loss_ce: 0.010664
2022-01-20 20:32:28,377 iteration 2426 : loss : 0.036937, loss_ce: 0.012418
2022-01-20 20:32:29,663 iteration 2427 : loss : 0.026286, loss_ce: 0.011343
2022-01-20 20:32:30,993 iteration 2428 : loss : 0.028606, loss_ce: 0.008714
2022-01-20 20:32:32,298 iteration 2429 : loss : 0.051473, loss_ce: 0.015329
2022-01-20 20:32:33,639 iteration 2430 : loss : 0.034245, loss_ce: 0.013664
2022-01-20 20:32:35,012 iteration 2431 : loss : 0.022190, loss_ce: 0.006940
 36%|██████████▎                  | 143/400 [59:35<1:43:04, 24.06s/it]2022-01-20 20:32:36,334 iteration 2432 : loss : 0.023247, loss_ce: 0.007776
2022-01-20 20:32:37,652 iteration 2433 : loss : 0.026327, loss_ce: 0.012265
2022-01-20 20:32:39,021 iteration 2434 : loss : 0.029606, loss_ce: 0.010746
2022-01-20 20:32:40,365 iteration 2435 : loss : 0.024654, loss_ce: 0.010035
2022-01-20 20:32:41,785 iteration 2436 : loss : 0.042099, loss_ce: 0.014019
2022-01-20 20:32:43,138 iteration 2437 : loss : 0.028465, loss_ce: 0.011626
2022-01-20 20:32:44,424 iteration 2438 : loss : 0.020417, loss_ce: 0.008087
2022-01-20 20:32:45,758 iteration 2439 : loss : 0.028215, loss_ce: 0.010855
2022-01-20 20:32:47,099 iteration 2440 : loss : 0.028555, loss_ce: 0.010496
2022-01-20 20:32:48,549 iteration 2441 : loss : 0.040703, loss_ce: 0.013250
2022-01-20 20:32:49,926 iteration 2442 : loss : 0.031191, loss_ce: 0.009939
2022-01-20 20:32:51,307 iteration 2443 : loss : 0.024463, loss_ce: 0.009669
2022-01-20 20:32:52,681 iteration 2444 : loss : 0.034150, loss_ce: 0.012842
2022-01-20 20:32:54,123 iteration 2445 : loss : 0.028497, loss_ce: 0.009382
2022-01-20 20:32:55,440 iteration 2446 : loss : 0.031380, loss_ce: 0.013145
2022-01-20 20:32:56,846 iteration 2447 : loss : 0.029792, loss_ce: 0.010702
2022-01-20 20:32:58,143 iteration 2448 : loss : 0.031268, loss_ce: 0.018690
 36%|██████████▍                  | 144/400 [59:58<1:41:29, 23.79s/it]2022-01-20 20:32:59,527 iteration 2449 : loss : 0.032620, loss_ce: 0.008246
2022-01-20 20:33:00,812 iteration 2450 : loss : 0.032217, loss_ce: 0.013754
2022-01-20 20:33:02,097 iteration 2451 : loss : 0.033686, loss_ce: 0.010120
2022-01-20 20:33:03,410 iteration 2452 : loss : 0.033845, loss_ce: 0.014698
2022-01-20 20:33:04,736 iteration 2453 : loss : 0.026555, loss_ce: 0.012581
2022-01-20 20:33:06,049 iteration 2454 : loss : 0.032420, loss_ce: 0.012463
2022-01-20 20:33:07,471 iteration 2455 : loss : 0.041256, loss_ce: 0.013951
2022-01-20 20:33:08,829 iteration 2456 : loss : 0.027352, loss_ce: 0.012417
2022-01-20 20:33:10,135 iteration 2457 : loss : 0.026731, loss_ce: 0.012371
2022-01-20 20:33:11,506 iteration 2458 : loss : 0.034467, loss_ce: 0.011492
2022-01-20 20:33:12,840 iteration 2459 : loss : 0.024487, loss_ce: 0.008875
2022-01-20 20:33:14,238 iteration 2460 : loss : 0.062502, loss_ce: 0.017579
2022-01-20 20:33:15,620 iteration 2461 : loss : 0.035793, loss_ce: 0.010007
2022-01-20 20:33:17,077 iteration 2462 : loss : 0.040588, loss_ce: 0.019940
2022-01-20 20:33:18,402 iteration 2463 : loss : 0.025129, loss_ce: 0.009646
2022-01-20 20:33:19,764 iteration 2464 : loss : 0.037017, loss_ce: 0.020873
2022-01-20 20:33:19,764 Training Data Eval:
2022-01-20 20:33:26,372   Average segmentation loss on training set: 0.0227
2022-01-20 20:33:26,372 Validation Data Eval:
2022-01-20 20:33:28,635   Average segmentation loss on validation set: 0.0828
2022-01-20 20:33:29,977 iteration 2465 : loss : 0.026688, loss_ce: 0.010642
 36%|█████████▊                 | 145/400 [1:00:30<1:51:20, 26.20s/it]2022-01-20 20:33:31,393 iteration 2466 : loss : 0.032065, loss_ce: 0.011702
2022-01-20 20:33:32,722 iteration 2467 : loss : 0.029807, loss_ce: 0.010169
2022-01-20 20:33:34,148 iteration 2468 : loss : 0.033071, loss_ce: 0.011176
2022-01-20 20:33:35,441 iteration 2469 : loss : 0.031564, loss_ce: 0.013966
2022-01-20 20:33:36,813 iteration 2470 : loss : 0.032489, loss_ce: 0.012533
2022-01-20 20:33:38,179 iteration 2471 : loss : 0.039091, loss_ce: 0.020630
2022-01-20 20:33:39,547 iteration 2472 : loss : 0.026416, loss_ce: 0.009807
2022-01-20 20:33:40,886 iteration 2473 : loss : 0.027087, loss_ce: 0.009749
2022-01-20 20:33:42,129 iteration 2474 : loss : 0.024295, loss_ce: 0.009464
2022-01-20 20:33:43,499 iteration 2475 : loss : 0.033129, loss_ce: 0.019277
2022-01-20 20:33:44,862 iteration 2476 : loss : 0.034253, loss_ce: 0.010468
2022-01-20 20:33:46,212 iteration 2477 : loss : 0.029192, loss_ce: 0.010089
2022-01-20 20:33:47,534 iteration 2478 : loss : 0.029789, loss_ce: 0.008974
2022-01-20 20:33:48,923 iteration 2479 : loss : 0.030442, loss_ce: 0.009406
2022-01-20 20:33:50,249 iteration 2480 : loss : 0.023098, loss_ce: 0.007953
2022-01-20 20:33:51,561 iteration 2481 : loss : 0.027479, loss_ce: 0.011061
2022-01-20 20:33:53,004 iteration 2482 : loss : 0.054884, loss_ce: 0.018679
 36%|█████████▊                 | 146/400 [1:00:53<1:46:52, 25.25s/it]2022-01-20 20:33:54,373 iteration 2483 : loss : 0.038959, loss_ce: 0.018969
2022-01-20 20:33:55,748 iteration 2484 : loss : 0.052595, loss_ce: 0.009552
2022-01-20 20:33:57,119 iteration 2485 : loss : 0.040216, loss_ce: 0.012481
2022-01-20 20:33:58,474 iteration 2486 : loss : 0.040851, loss_ce: 0.017659
2022-01-20 20:33:59,760 iteration 2487 : loss : 0.026433, loss_ce: 0.011999
2022-01-20 20:34:01,158 iteration 2488 : loss : 0.026998, loss_ce: 0.013472
2022-01-20 20:34:02,540 iteration 2489 : loss : 0.030730, loss_ce: 0.012910
2022-01-20 20:34:03,952 iteration 2490 : loss : 0.037121, loss_ce: 0.015344
2022-01-20 20:34:05,324 iteration 2491 : loss : 0.031678, loss_ce: 0.010286
2022-01-20 20:34:06,702 iteration 2492 : loss : 0.041579, loss_ce: 0.013693
2022-01-20 20:34:08,004 iteration 2493 : loss : 0.026417, loss_ce: 0.009173
2022-01-20 20:34:09,343 iteration 2494 : loss : 0.026050, loss_ce: 0.009785
2022-01-20 20:34:10,761 iteration 2495 : loss : 0.024312, loss_ce: 0.010232
2022-01-20 20:34:11,996 iteration 2496 : loss : 0.017962, loss_ce: 0.006773
2022-01-20 20:34:13,287 iteration 2497 : loss : 0.029982, loss_ce: 0.010674
2022-01-20 20:34:14,752 iteration 2498 : loss : 0.030763, loss_ce: 0.011453
2022-01-20 20:34:16,096 iteration 2499 : loss : 0.033651, loss_ce: 0.015668
 37%|█████████▉                 | 147/400 [1:01:16<1:43:44, 24.60s/it]2022-01-20 20:34:17,495 iteration 2500 : loss : 0.026304, loss_ce: 0.010020
2022-01-20 20:34:18,865 iteration 2501 : loss : 0.028669, loss_ce: 0.010615
2022-01-20 20:34:20,235 iteration 2502 : loss : 0.031300, loss_ce: 0.015355
2022-01-20 20:34:21,567 iteration 2503 : loss : 0.028916, loss_ce: 0.013669
2022-01-20 20:34:22,951 iteration 2504 : loss : 0.035635, loss_ce: 0.012320
2022-01-20 20:34:24,315 iteration 2505 : loss : 0.026935, loss_ce: 0.009581
2022-01-20 20:34:25,685 iteration 2506 : loss : 0.019636, loss_ce: 0.008263
2022-01-20 20:34:26,989 iteration 2507 : loss : 0.022071, loss_ce: 0.007338
2022-01-20 20:34:28,256 iteration 2508 : loss : 0.024757, loss_ce: 0.010650
2022-01-20 20:34:29,579 iteration 2509 : loss : 0.022402, loss_ce: 0.008893
2022-01-20 20:34:30,863 iteration 2510 : loss : 0.037260, loss_ce: 0.011169
2022-01-20 20:34:32,197 iteration 2511 : loss : 0.033499, loss_ce: 0.017622
2022-01-20 20:34:33,523 iteration 2512 : loss : 0.023772, loss_ce: 0.008477
2022-01-20 20:34:34,861 iteration 2513 : loss : 0.034284, loss_ce: 0.012585
2022-01-20 20:34:36,219 iteration 2514 : loss : 0.034354, loss_ce: 0.014224
2022-01-20 20:34:37,544 iteration 2515 : loss : 0.025675, loss_ce: 0.007357
2022-01-20 20:34:38,844 iteration 2516 : loss : 0.031182, loss_ce: 0.012106
 37%|█████████▉                 | 148/400 [1:01:39<1:40:59, 24.05s/it]2022-01-20 20:34:40,210 iteration 2517 : loss : 0.025288, loss_ce: 0.009033
2022-01-20 20:34:41,659 iteration 2518 : loss : 0.031194, loss_ce: 0.013383
2022-01-20 20:34:43,008 iteration 2519 : loss : 0.036997, loss_ce: 0.016801
2022-01-20 20:34:44,323 iteration 2520 : loss : 0.020228, loss_ce: 0.006220
2022-01-20 20:34:45,690 iteration 2521 : loss : 0.026137, loss_ce: 0.011895
2022-01-20 20:34:46,972 iteration 2522 : loss : 0.024335, loss_ce: 0.009910
2022-01-20 20:34:48,199 iteration 2523 : loss : 0.028239, loss_ce: 0.006668
2022-01-20 20:34:49,576 iteration 2524 : loss : 0.027357, loss_ce: 0.012068
2022-01-20 20:34:50,878 iteration 2525 : loss : 0.025264, loss_ce: 0.009900
2022-01-20 20:34:52,181 iteration 2526 : loss : 0.023549, loss_ce: 0.008911
2022-01-20 20:34:53,562 iteration 2527 : loss : 0.029301, loss_ce: 0.010758
2022-01-20 20:34:54,867 iteration 2528 : loss : 0.029149, loss_ce: 0.011664
2022-01-20 20:34:56,226 iteration 2529 : loss : 0.033910, loss_ce: 0.013093
2022-01-20 20:34:57,566 iteration 2530 : loss : 0.042635, loss_ce: 0.013361
2022-01-20 20:34:58,892 iteration 2531 : loss : 0.028571, loss_ce: 0.009239
2022-01-20 20:35:00,256 iteration 2532 : loss : 0.044445, loss_ce: 0.018235
2022-01-20 20:35:01,602 iteration 2533 : loss : 0.033519, loss_ce: 0.020305
 37%|██████████                 | 149/400 [1:02:01<1:38:58, 23.66s/it]2022-01-20 20:35:02,929 iteration 2534 : loss : 0.021474, loss_ce: 0.009420
2022-01-20 20:35:04,293 iteration 2535 : loss : 0.022813, loss_ce: 0.006501
2022-01-20 20:35:05,675 iteration 2536 : loss : 0.030683, loss_ce: 0.014496
2022-01-20 20:35:07,066 iteration 2537 : loss : 0.039818, loss_ce: 0.012307
2022-01-20 20:35:08,470 iteration 2538 : loss : 0.036634, loss_ce: 0.012829
2022-01-20 20:35:09,755 iteration 2539 : loss : 0.023101, loss_ce: 0.008952
2022-01-20 20:35:11,086 iteration 2540 : loss : 0.027807, loss_ce: 0.009909
2022-01-20 20:35:12,545 iteration 2541 : loss : 0.040156, loss_ce: 0.012235
2022-01-20 20:35:13,826 iteration 2542 : loss : 0.025350, loss_ce: 0.009228
2022-01-20 20:35:15,161 iteration 2543 : loss : 0.033603, loss_ce: 0.014028
2022-01-20 20:35:16,567 iteration 2544 : loss : 0.033908, loss_ce: 0.012041
2022-01-20 20:35:17,951 iteration 2545 : loss : 0.029161, loss_ce: 0.014261
2022-01-20 20:35:19,240 iteration 2546 : loss : 0.026444, loss_ce: 0.009595
2022-01-20 20:35:20,578 iteration 2547 : loss : 0.041992, loss_ce: 0.008924
2022-01-20 20:35:21,908 iteration 2548 : loss : 0.031055, loss_ce: 0.011695
2022-01-20 20:35:23,307 iteration 2549 : loss : 0.057546, loss_ce: 0.029384
2022-01-20 20:35:23,307 Training Data Eval:
2022-01-20 20:35:29,920   Average segmentation loss on training set: 0.0204
2022-01-20 20:35:29,921 Validation Data Eval:
2022-01-20 20:35:32,183   Average segmentation loss on validation set: 0.0795
2022-01-20 20:35:33,506 iteration 2550 : loss : 0.036903, loss_ce: 0.021967
 38%|██████████▏                | 150/400 [1:02:33<1:48:52, 26.13s/it]2022-01-20 20:35:34,852 iteration 2551 : loss : 0.023918, loss_ce: 0.008470
2022-01-20 20:35:36,191 iteration 2552 : loss : 0.026682, loss_ce: 0.010098
2022-01-20 20:35:37,595 iteration 2553 : loss : 0.041891, loss_ce: 0.017135
2022-01-20 20:35:38,974 iteration 2554 : loss : 0.047848, loss_ce: 0.014296
2022-01-20 20:35:40,298 iteration 2555 : loss : 0.036401, loss_ce: 0.011470
2022-01-20 20:35:41,568 iteration 2556 : loss : 0.024126, loss_ce: 0.010394
2022-01-20 20:35:42,952 iteration 2557 : loss : 0.040716, loss_ce: 0.017188
2022-01-20 20:35:44,331 iteration 2558 : loss : 0.026243, loss_ce: 0.011214
2022-01-20 20:35:45,738 iteration 2559 : loss : 0.045574, loss_ce: 0.011377
2022-01-20 20:35:47,139 iteration 2560 : loss : 0.029742, loss_ce: 0.012407
2022-01-20 20:35:48,477 iteration 2561 : loss : 0.037769, loss_ce: 0.014823
2022-01-20 20:35:49,780 iteration 2562 : loss : 0.028404, loss_ce: 0.014482
2022-01-20 20:35:51,103 iteration 2563 : loss : 0.029899, loss_ce: 0.010889
2022-01-20 20:35:52,441 iteration 2564 : loss : 0.037533, loss_ce: 0.019715
2022-01-20 20:35:53,773 iteration 2565 : loss : 0.039504, loss_ce: 0.012921
2022-01-20 20:35:55,086 iteration 2566 : loss : 0.033001, loss_ce: 0.015294
2022-01-20 20:35:56,514 iteration 2567 : loss : 0.041386, loss_ce: 0.019409
 38%|██████████▏                | 151/400 [1:02:56<1:44:33, 25.20s/it]2022-01-20 20:35:57,896 iteration 2568 : loss : 0.039810, loss_ce: 0.022390
2022-01-20 20:35:59,236 iteration 2569 : loss : 0.043629, loss_ce: 0.015712
2022-01-20 20:36:00,551 iteration 2570 : loss : 0.039444, loss_ce: 0.010260
2022-01-20 20:36:01,901 iteration 2571 : loss : 0.057680, loss_ce: 0.032474
2022-01-20 20:36:03,178 iteration 2572 : loss : 0.026993, loss_ce: 0.007978
2022-01-20 20:36:04,484 iteration 2573 : loss : 0.025025, loss_ce: 0.009870
2022-01-20 20:36:05,820 iteration 2574 : loss : 0.035961, loss_ce: 0.011275
2022-01-20 20:36:07,196 iteration 2575 : loss : 0.033888, loss_ce: 0.011677
2022-01-20 20:36:08,644 iteration 2576 : loss : 0.032501, loss_ce: 0.015171
2022-01-20 20:36:09,963 iteration 2577 : loss : 0.037302, loss_ce: 0.012133
2022-01-20 20:36:11,327 iteration 2578 : loss : 0.034902, loss_ce: 0.017772
2022-01-20 20:36:12,643 iteration 2579 : loss : 0.032969, loss_ce: 0.011026
2022-01-20 20:36:14,014 iteration 2580 : loss : 0.028874, loss_ce: 0.009679
2022-01-20 20:36:15,462 iteration 2581 : loss : 0.033577, loss_ce: 0.011378
2022-01-20 20:36:16,706 iteration 2582 : loss : 0.030746, loss_ce: 0.012229
2022-01-20 20:36:18,079 iteration 2583 : loss : 0.021419, loss_ce: 0.007024
2022-01-20 20:36:19,399 iteration 2584 : loss : 0.023228, loss_ce: 0.008181
 38%|██████████▎                | 152/400 [1:03:19<1:41:16, 24.50s/it]2022-01-20 20:36:20,832 iteration 2585 : loss : 0.061759, loss_ce: 0.030130
2022-01-20 20:36:22,182 iteration 2586 : loss : 0.038651, loss_ce: 0.017685
2022-01-20 20:36:23,470 iteration 2587 : loss : 0.023064, loss_ce: 0.007643
2022-01-20 20:36:24,784 iteration 2588 : loss : 0.030250, loss_ce: 0.013204
2022-01-20 20:36:26,055 iteration 2589 : loss : 0.026666, loss_ce: 0.009594
2022-01-20 20:36:27,440 iteration 2590 : loss : 0.034156, loss_ce: 0.012777
2022-01-20 20:36:28,786 iteration 2591 : loss : 0.033903, loss_ce: 0.015508
2022-01-20 20:36:30,115 iteration 2592 : loss : 0.041939, loss_ce: 0.018331
2022-01-20 20:36:31,420 iteration 2593 : loss : 0.037667, loss_ce: 0.016039
2022-01-20 20:36:32,790 iteration 2594 : loss : 0.034377, loss_ce: 0.012057
2022-01-20 20:36:34,228 iteration 2595 : loss : 0.050161, loss_ce: 0.015754
2022-01-20 20:36:35,506 iteration 2596 : loss : 0.025674, loss_ce: 0.008087
2022-01-20 20:36:36,852 iteration 2597 : loss : 0.067478, loss_ce: 0.021809
2022-01-20 20:36:38,161 iteration 2598 : loss : 0.026002, loss_ce: 0.011398
2022-01-20 20:36:39,537 iteration 2599 : loss : 0.024855, loss_ce: 0.010916
2022-01-20 20:36:40,869 iteration 2600 : loss : 0.038564, loss_ce: 0.012839
2022-01-20 20:36:42,224 iteration 2601 : loss : 0.041592, loss_ce: 0.016267
 38%|██████████▎                | 153/400 [1:03:42<1:38:48, 24.00s/it]2022-01-20 20:36:43,623 iteration 2602 : loss : 0.030625, loss_ce: 0.008626
2022-01-20 20:36:45,049 iteration 2603 : loss : 0.050182, loss_ce: 0.016331
2022-01-20 20:36:46,391 iteration 2604 : loss : 0.044905, loss_ce: 0.017957
2022-01-20 20:36:47,614 iteration 2605 : loss : 0.024458, loss_ce: 0.006778
2022-01-20 20:36:49,076 iteration 2606 : loss : 0.060290, loss_ce: 0.021858
2022-01-20 20:36:50,457 iteration 2607 : loss : 0.040349, loss_ce: 0.017639
2022-01-20 20:36:51,798 iteration 2608 : loss : 0.026091, loss_ce: 0.012114
2022-01-20 20:36:53,230 iteration 2609 : loss : 0.043429, loss_ce: 0.015524
2022-01-20 20:36:54,618 iteration 2610 : loss : 0.037920, loss_ce: 0.014574
2022-01-20 20:36:55,984 iteration 2611 : loss : 0.046691, loss_ce: 0.017412
2022-01-20 20:36:57,378 iteration 2612 : loss : 0.042940, loss_ce: 0.017283
2022-01-20 20:36:58,674 iteration 2613 : loss : 0.032479, loss_ce: 0.012225
2022-01-20 20:37:00,084 iteration 2614 : loss : 0.037348, loss_ce: 0.011790
2022-01-20 20:37:01,400 iteration 2615 : loss : 0.041018, loss_ce: 0.015374
2022-01-20 20:37:02,784 iteration 2616 : loss : 0.032993, loss_ce: 0.015634
2022-01-20 20:37:04,121 iteration 2617 : loss : 0.028743, loss_ce: 0.009838
2022-01-20 20:37:05,422 iteration 2618 : loss : 0.037687, loss_ce: 0.022535
 38%|██████████▍                | 154/400 [1:04:05<1:37:24, 23.76s/it]2022-01-20 20:37:06,784 iteration 2619 : loss : 0.032352, loss_ce: 0.009452
2022-01-20 20:37:08,105 iteration 2620 : loss : 0.026924, loss_ce: 0.012066
2022-01-20 20:37:09,509 iteration 2621 : loss : 0.032523, loss_ce: 0.012246
2022-01-20 20:37:10,848 iteration 2622 : loss : 0.034263, loss_ce: 0.009764
2022-01-20 20:37:12,222 iteration 2623 : loss : 0.030547, loss_ce: 0.014178
2022-01-20 20:37:13,523 iteration 2624 : loss : 0.022075, loss_ce: 0.007022
2022-01-20 20:37:14,886 iteration 2625 : loss : 0.045789, loss_ce: 0.008169
2022-01-20 20:37:16,245 iteration 2626 : loss : 0.036548, loss_ce: 0.015129
2022-01-20 20:37:17,730 iteration 2627 : loss : 0.032130, loss_ce: 0.013116
2022-01-20 20:37:19,014 iteration 2628 : loss : 0.031540, loss_ce: 0.013788
2022-01-20 20:37:20,411 iteration 2629 : loss : 0.032424, loss_ce: 0.015007
2022-01-20 20:37:21,775 iteration 2630 : loss : 0.027425, loss_ce: 0.010295
2022-01-20 20:37:23,117 iteration 2631 : loss : 0.024115, loss_ce: 0.009572
2022-01-20 20:37:24,452 iteration 2632 : loss : 0.024917, loss_ce: 0.008981
2022-01-20 20:37:25,823 iteration 2633 : loss : 0.038408, loss_ce: 0.020441
2022-01-20 20:37:27,184 iteration 2634 : loss : 0.038134, loss_ce: 0.013447
2022-01-20 20:37:27,185 Training Data Eval:
2022-01-20 20:37:33,798   Average segmentation loss on training set: 0.0206
2022-01-20 20:37:33,798 Validation Data Eval:
2022-01-20 20:37:36,065   Average segmentation loss on validation set: 0.0674
2022-01-20 20:37:37,407 iteration 2635 : loss : 0.032997, loss_ce: 0.012139
 39%|██████████▍                | 155/400 [1:04:37<1:47:05, 26.23s/it]2022-01-20 20:37:38,789 iteration 2636 : loss : 0.034849, loss_ce: 0.016627
2022-01-20 20:37:40,206 iteration 2637 : loss : 0.036495, loss_ce: 0.013336
2022-01-20 20:37:41,523 iteration 2638 : loss : 0.035934, loss_ce: 0.013774
2022-01-20 20:37:42,817 iteration 2639 : loss : 0.039783, loss_ce: 0.021256
2022-01-20 20:37:44,132 iteration 2640 : loss : 0.025172, loss_ce: 0.008368
2022-01-20 20:37:45,461 iteration 2641 : loss : 0.040858, loss_ce: 0.021924
2022-01-20 20:37:46,757 iteration 2642 : loss : 0.033040, loss_ce: 0.009846
2022-01-20 20:37:48,205 iteration 2643 : loss : 0.028105, loss_ce: 0.009170
2022-01-20 20:37:49,527 iteration 2644 : loss : 0.024341, loss_ce: 0.008593
2022-01-20 20:37:50,823 iteration 2645 : loss : 0.023963, loss_ce: 0.008422
2022-01-20 20:37:52,331 iteration 2646 : loss : 0.033600, loss_ce: 0.011984
2022-01-20 20:37:53,744 iteration 2647 : loss : 0.037830, loss_ce: 0.015111
2022-01-20 20:37:55,075 iteration 2648 : loss : 0.042414, loss_ce: 0.019646
2022-01-20 20:37:56,436 iteration 2649 : loss : 0.034834, loss_ce: 0.013744
2022-01-20 20:37:57,830 iteration 2650 : loss : 0.032000, loss_ce: 0.010791
2022-01-20 20:37:59,199 iteration 2651 : loss : 0.035026, loss_ce: 0.013724
2022-01-20 20:38:00,485 iteration 2652 : loss : 0.031630, loss_ce: 0.010736
 39%|██████████▌                | 156/400 [1:05:00<1:42:48, 25.28s/it]2022-01-20 20:38:01,922 iteration 2653 : loss : 0.033357, loss_ce: 0.015363
2022-01-20 20:38:03,289 iteration 2654 : loss : 0.026762, loss_ce: 0.008355
2022-01-20 20:38:04,718 iteration 2655 : loss : 0.045591, loss_ce: 0.025410
2022-01-20 20:38:06,047 iteration 2656 : loss : 0.028430, loss_ce: 0.009269
2022-01-20 20:38:07,382 iteration 2657 : loss : 0.053261, loss_ce: 0.023748
2022-01-20 20:38:08,612 iteration 2658 : loss : 0.021036, loss_ce: 0.010536
2022-01-20 20:38:09,894 iteration 2659 : loss : 0.079115, loss_ce: 0.023697
2022-01-20 20:38:11,271 iteration 2660 : loss : 0.034719, loss_ce: 0.011588
2022-01-20 20:38:12,580 iteration 2661 : loss : 0.026040, loss_ce: 0.012057
2022-01-20 20:38:13,909 iteration 2662 : loss : 0.043476, loss_ce: 0.019791
2022-01-20 20:38:15,219 iteration 2663 : loss : 0.029079, loss_ce: 0.008344
2022-01-20 20:38:16,651 iteration 2664 : loss : 0.039907, loss_ce: 0.016472
2022-01-20 20:38:18,001 iteration 2665 : loss : 0.033308, loss_ce: 0.012475
2022-01-20 20:38:19,307 iteration 2666 : loss : 0.021013, loss_ce: 0.007370
2022-01-20 20:38:20,704 iteration 2667 : loss : 0.034749, loss_ce: 0.012641
2022-01-20 20:38:22,014 iteration 2668 : loss : 0.038108, loss_ce: 0.014496
2022-01-20 20:38:23,323 iteration 2669 : loss : 0.030490, loss_ce: 0.011956
 39%|██████████▌                | 157/400 [1:05:23<1:39:25, 24.55s/it]2022-01-20 20:38:24,740 iteration 2670 : loss : 0.029207, loss_ce: 0.013801
2022-01-20 20:38:26,208 iteration 2671 : loss : 0.060666, loss_ce: 0.021441
2022-01-20 20:38:27,595 iteration 2672 : loss : 0.032063, loss_ce: 0.010439
2022-01-20 20:38:28,891 iteration 2673 : loss : 0.026400, loss_ce: 0.013441
2022-01-20 20:38:30,275 iteration 2674 : loss : 0.038273, loss_ce: 0.014935
2022-01-20 20:38:31,509 iteration 2675 : loss : 0.023200, loss_ce: 0.009893
2022-01-20 20:38:32,970 iteration 2676 : loss : 0.039195, loss_ce: 0.007782
2022-01-20 20:38:34,344 iteration 2677 : loss : 0.031223, loss_ce: 0.012724
2022-01-20 20:38:35,680 iteration 2678 : loss : 0.028265, loss_ce: 0.011926
2022-01-20 20:38:37,034 iteration 2679 : loss : 0.028831, loss_ce: 0.009201
2022-01-20 20:38:38,366 iteration 2680 : loss : 0.024200, loss_ce: 0.009363
2022-01-20 20:38:39,717 iteration 2681 : loss : 0.028364, loss_ce: 0.009898
2022-01-20 20:38:41,071 iteration 2682 : loss : 0.034707, loss_ce: 0.011641
2022-01-20 20:38:42,404 iteration 2683 : loss : 0.038747, loss_ce: 0.009452
2022-01-20 20:38:43,698 iteration 2684 : loss : 0.028257, loss_ce: 0.013441
2022-01-20 20:38:45,053 iteration 2685 : loss : 0.024846, loss_ce: 0.010445
2022-01-20 20:38:46,342 iteration 2686 : loss : 0.032042, loss_ce: 0.014188
 40%|██████████▋                | 158/400 [1:05:46<1:37:09, 24.09s/it]2022-01-20 20:38:47,634 iteration 2687 : loss : 0.031283, loss_ce: 0.008295
2022-01-20 20:38:48,975 iteration 2688 : loss : 0.036076, loss_ce: 0.010337
2022-01-20 20:38:50,289 iteration 2689 : loss : 0.034040, loss_ce: 0.013757
2022-01-20 20:38:51,728 iteration 2690 : loss : 0.035334, loss_ce: 0.013891
2022-01-20 20:38:53,009 iteration 2691 : loss : 0.025320, loss_ce: 0.009537
2022-01-20 20:38:54,309 iteration 2692 : loss : 0.025390, loss_ce: 0.007678
2022-01-20 20:38:55,654 iteration 2693 : loss : 0.026851, loss_ce: 0.007983
2022-01-20 20:38:57,065 iteration 2694 : loss : 0.036643, loss_ce: 0.011711
2022-01-20 20:38:58,378 iteration 2695 : loss : 0.025342, loss_ce: 0.010836
2022-01-20 20:38:59,730 iteration 2696 : loss : 0.029952, loss_ce: 0.013061
2022-01-20 20:39:01,008 iteration 2697 : loss : 0.024056, loss_ce: 0.010322
2022-01-20 20:39:02,216 iteration 2698 : loss : 0.021507, loss_ce: 0.009773
2022-01-20 20:39:03,557 iteration 2699 : loss : 0.041033, loss_ce: 0.014715
2022-01-20 20:39:04,950 iteration 2700 : loss : 0.031908, loss_ce: 0.015841
2022-01-20 20:39:06,290 iteration 2701 : loss : 0.039153, loss_ce: 0.013926
2022-01-20 20:39:07,662 iteration 2702 : loss : 0.062631, loss_ce: 0.010029
2022-01-20 20:39:08,992 iteration 2703 : loss : 0.025894, loss_ce: 0.009506
 40%|██████████▋                | 159/400 [1:06:09<1:35:01, 23.66s/it]2022-01-20 20:39:10,472 iteration 2704 : loss : 0.032692, loss_ce: 0.013433
2022-01-20 20:39:11,771 iteration 2705 : loss : 0.036211, loss_ce: 0.009073
2022-01-20 20:39:13,067 iteration 2706 : loss : 0.031481, loss_ce: 0.016338
2022-01-20 20:39:14,477 iteration 2707 : loss : 0.038398, loss_ce: 0.013746
2022-01-20 20:39:15,742 iteration 2708 : loss : 0.018372, loss_ce: 0.006951
2022-01-20 20:39:17,080 iteration 2709 : loss : 0.027383, loss_ce: 0.008382
2022-01-20 20:39:18,368 iteration 2710 : loss : 0.024770, loss_ce: 0.008812
2022-01-20 20:39:19,804 iteration 2711 : loss : 0.049648, loss_ce: 0.016748
2022-01-20 20:39:21,093 iteration 2712 : loss : 0.025418, loss_ce: 0.010794
2022-01-20 20:39:22,550 iteration 2713 : loss : 0.037149, loss_ce: 0.011909
2022-01-20 20:39:23,839 iteration 2714 : loss : 0.035425, loss_ce: 0.011448
2022-01-20 20:39:25,208 iteration 2715 : loss : 0.042054, loss_ce: 0.021454
2022-01-20 20:39:26,525 iteration 2716 : loss : 0.027985, loss_ce: 0.009793
2022-01-20 20:39:27,808 iteration 2717 : loss : 0.023657, loss_ce: 0.010440
2022-01-20 20:39:29,100 iteration 2718 : loss : 0.026541, loss_ce: 0.009738
2022-01-20 20:39:30,500 iteration 2719 : loss : 0.045372, loss_ce: 0.015507
2022-01-20 20:39:30,500 Training Data Eval:
2022-01-20 20:39:37,113   Average segmentation loss on training set: 0.0225
2022-01-20 20:39:37,114 Validation Data Eval:
2022-01-20 20:39:39,368   Average segmentation loss on validation set: 0.0685
2022-01-20 20:39:40,643 iteration 2720 : loss : 0.025900, loss_ce: 0.011615
 40%|██████████▊                | 160/400 [1:06:40<1:44:13, 26.06s/it]2022-01-20 20:39:42,037 iteration 2721 : loss : 0.032809, loss_ce: 0.012481
2022-01-20 20:39:43,413 iteration 2722 : loss : 0.030894, loss_ce: 0.011614
2022-01-20 20:39:44,799 iteration 2723 : loss : 0.024250, loss_ce: 0.009110
2022-01-20 20:39:46,085 iteration 2724 : loss : 0.032186, loss_ce: 0.010251
2022-01-20 20:39:47,335 iteration 2725 : loss : 0.027440, loss_ce: 0.010773
2022-01-20 20:39:48,635 iteration 2726 : loss : 0.029019, loss_ce: 0.010592
2022-01-20 20:39:49,895 iteration 2727 : loss : 0.021335, loss_ce: 0.006663
2022-01-20 20:39:51,350 iteration 2728 : loss : 0.062729, loss_ce: 0.045930
2022-01-20 20:39:52,705 iteration 2729 : loss : 0.032672, loss_ce: 0.013386
2022-01-20 20:39:54,088 iteration 2730 : loss : 0.026921, loss_ce: 0.011602
2022-01-20 20:39:55,464 iteration 2731 : loss : 0.040681, loss_ce: 0.015794
2022-01-20 20:39:56,897 iteration 2732 : loss : 0.037741, loss_ce: 0.014410
2022-01-20 20:39:58,269 iteration 2733 : loss : 0.032467, loss_ce: 0.012670
2022-01-20 20:39:59,634 iteration 2734 : loss : 0.026818, loss_ce: 0.008422
2022-01-20 20:40:00,894 iteration 2735 : loss : 0.025519, loss_ce: 0.010393
2022-01-20 20:40:02,285 iteration 2736 : loss : 0.035010, loss_ce: 0.017075
2022-01-20 20:40:03,607 iteration 2737 : loss : 0.026861, loss_ce: 0.009184
 40%|██████████▊                | 161/400 [1:07:03<1:40:05, 25.13s/it]2022-01-20 20:40:04,975 iteration 2738 : loss : 0.027278, loss_ce: 0.012839
2022-01-20 20:40:06,292 iteration 2739 : loss : 0.034030, loss_ce: 0.015695
2022-01-20 20:40:07,650 iteration 2740 : loss : 0.030381, loss_ce: 0.011166
2022-01-20 20:40:09,013 iteration 2741 : loss : 0.020536, loss_ce: 0.006198
2022-01-20 20:40:10,283 iteration 2742 : loss : 0.021457, loss_ce: 0.008032
2022-01-20 20:40:11,623 iteration 2743 : loss : 0.034436, loss_ce: 0.017130
2022-01-20 20:40:12,992 iteration 2744 : loss : 0.032792, loss_ce: 0.012000
2022-01-20 20:40:14,340 iteration 2745 : loss : 0.026021, loss_ce: 0.009009
2022-01-20 20:40:15,624 iteration 2746 : loss : 0.037426, loss_ce: 0.014700
2022-01-20 20:40:16,967 iteration 2747 : loss : 0.028688, loss_ce: 0.009468
2022-01-20 20:40:18,343 iteration 2748 : loss : 0.030472, loss_ce: 0.011386
2022-01-20 20:40:19,615 iteration 2749 : loss : 0.032150, loss_ce: 0.013812
2022-01-20 20:40:20,979 iteration 2750 : loss : 0.041818, loss_ce: 0.012150
2022-01-20 20:40:22,284 iteration 2751 : loss : 0.020518, loss_ce: 0.009032
2022-01-20 20:40:23,541 iteration 2752 : loss : 0.032462, loss_ce: 0.007267
2022-01-20 20:40:24,960 iteration 2753 : loss : 0.044825, loss_ce: 0.021541
2022-01-20 20:40:26,343 iteration 2754 : loss : 0.026970, loss_ce: 0.011669
 40%|██████████▉                | 162/400 [1:07:26<1:36:49, 24.41s/it]2022-01-20 20:40:27,723 iteration 2755 : loss : 0.021262, loss_ce: 0.008753
2022-01-20 20:40:29,073 iteration 2756 : loss : 0.033959, loss_ce: 0.017001
2022-01-20 20:40:30,411 iteration 2757 : loss : 0.037237, loss_ce: 0.017560
2022-01-20 20:40:31,817 iteration 2758 : loss : 0.037078, loss_ce: 0.012355
2022-01-20 20:40:33,169 iteration 2759 : loss : 0.031831, loss_ce: 0.010118
2022-01-20 20:40:34,511 iteration 2760 : loss : 0.033814, loss_ce: 0.012530
2022-01-20 20:40:35,773 iteration 2761 : loss : 0.030278, loss_ce: 0.011144
2022-01-20 20:40:37,052 iteration 2762 : loss : 0.021019, loss_ce: 0.008219
2022-01-20 20:40:38,434 iteration 2763 : loss : 0.054580, loss_ce: 0.016383
2022-01-20 20:40:39,768 iteration 2764 : loss : 0.024703, loss_ce: 0.009252
2022-01-20 20:40:41,111 iteration 2765 : loss : 0.033860, loss_ce: 0.013245
2022-01-20 20:40:42,418 iteration 2766 : loss : 0.024706, loss_ce: 0.010712
2022-01-20 20:40:43,776 iteration 2767 : loss : 0.023092, loss_ce: 0.008311
2022-01-20 20:40:45,073 iteration 2768 : loss : 0.024899, loss_ce: 0.006819
2022-01-20 20:40:46,409 iteration 2769 : loss : 0.022514, loss_ce: 0.008911
2022-01-20 20:40:47,748 iteration 2770 : loss : 0.039438, loss_ce: 0.024890
2022-01-20 20:40:49,063 iteration 2771 : loss : 0.024528, loss_ce: 0.007238
 41%|███████████                | 163/400 [1:07:49<1:34:25, 23.90s/it]2022-01-20 20:40:50,491 iteration 2772 : loss : 0.023782, loss_ce: 0.010860
2022-01-20 20:40:51,773 iteration 2773 : loss : 0.022921, loss_ce: 0.008905
2022-01-20 20:40:53,043 iteration 2774 : loss : 0.033175, loss_ce: 0.007951
2022-01-20 20:40:54,349 iteration 2775 : loss : 0.026939, loss_ce: 0.009645
2022-01-20 20:40:55,650 iteration 2776 : loss : 0.021088, loss_ce: 0.007467
2022-01-20 20:40:56,980 iteration 2777 : loss : 0.020442, loss_ce: 0.007403
2022-01-20 20:40:58,388 iteration 2778 : loss : 0.037800, loss_ce: 0.013932
2022-01-20 20:40:59,836 iteration 2779 : loss : 0.025383, loss_ce: 0.008657
2022-01-20 20:41:01,133 iteration 2780 : loss : 0.023131, loss_ce: 0.007410
2022-01-20 20:41:02,526 iteration 2781 : loss : 0.036545, loss_ce: 0.010949
2022-01-20 20:41:03,859 iteration 2782 : loss : 0.024259, loss_ce: 0.008320
2022-01-20 20:41:05,162 iteration 2783 : loss : 0.029376, loss_ce: 0.012975
2022-01-20 20:41:06,613 iteration 2784 : loss : 0.030826, loss_ce: 0.010648
2022-01-20 20:41:08,025 iteration 2785 : loss : 0.028397, loss_ce: 0.007917
2022-01-20 20:41:09,398 iteration 2786 : loss : 0.033701, loss_ce: 0.015401
2022-01-20 20:41:10,781 iteration 2787 : loss : 0.021324, loss_ce: 0.009642
2022-01-20 20:41:12,083 iteration 2788 : loss : 0.025146, loss_ce: 0.013396
 41%|███████████                | 164/400 [1:08:12<1:32:58, 23.64s/it]2022-01-20 20:41:13,428 iteration 2789 : loss : 0.026533, loss_ce: 0.010113
2022-01-20 20:41:14,794 iteration 2790 : loss : 0.025811, loss_ce: 0.009490
2022-01-20 20:41:16,102 iteration 2791 : loss : 0.025054, loss_ce: 0.009728
2022-01-20 20:41:17,497 iteration 2792 : loss : 0.049301, loss_ce: 0.017676
2022-01-20 20:41:18,789 iteration 2793 : loss : 0.024784, loss_ce: 0.009852
2022-01-20 20:41:20,161 iteration 2794 : loss : 0.032809, loss_ce: 0.010728
2022-01-20 20:41:21,452 iteration 2795 : loss : 0.019954, loss_ce: 0.006599
2022-01-20 20:41:22,815 iteration 2796 : loss : 0.023316, loss_ce: 0.008861
2022-01-20 20:41:24,180 iteration 2797 : loss : 0.022209, loss_ce: 0.008294
2022-01-20 20:41:25,575 iteration 2798 : loss : 0.033047, loss_ce: 0.017069
2022-01-20 20:41:26,871 iteration 2799 : loss : 0.021521, loss_ce: 0.009367
2022-01-20 20:41:28,152 iteration 2800 : loss : 0.037177, loss_ce: 0.012480
2022-01-20 20:41:29,423 iteration 2801 : loss : 0.019078, loss_ce: 0.005980
2022-01-20 20:41:30,810 iteration 2802 : loss : 0.041682, loss_ce: 0.011564
2022-01-20 20:41:32,137 iteration 2803 : loss : 0.028742, loss_ce: 0.012743
2022-01-20 20:41:33,445 iteration 2804 : loss : 0.024881, loss_ce: 0.007918
2022-01-20 20:41:33,446 Training Data Eval:
2022-01-20 20:41:40,060   Average segmentation loss on training set: 0.0180
2022-01-20 20:41:40,061 Validation Data Eval:
2022-01-20 20:41:42,324   Average segmentation loss on validation set: 0.0848
2022-01-20 20:41:43,630 iteration 2805 : loss : 0.022275, loss_ce: 0.009237
 41%|███████████▏               | 165/400 [1:08:43<1:41:52, 26.01s/it]2022-01-20 20:41:45,121 iteration 2806 : loss : 0.043939, loss_ce: 0.018997
2022-01-20 20:41:46,388 iteration 2807 : loss : 0.028157, loss_ce: 0.010492
2022-01-20 20:41:47,701 iteration 2808 : loss : 0.022166, loss_ce: 0.008450
2022-01-20 20:41:49,058 iteration 2809 : loss : 0.021301, loss_ce: 0.007122
2022-01-20 20:41:50,379 iteration 2810 : loss : 0.030241, loss_ce: 0.014031
2022-01-20 20:41:51,796 iteration 2811 : loss : 0.057283, loss_ce: 0.020948
2022-01-20 20:41:53,231 iteration 2812 : loss : 0.026013, loss_ce: 0.008748
2022-01-20 20:41:54,487 iteration 2813 : loss : 0.028811, loss_ce: 0.010228
2022-01-20 20:41:55,843 iteration 2814 : loss : 0.028991, loss_ce: 0.013944
2022-01-20 20:41:57,218 iteration 2815 : loss : 0.038433, loss_ce: 0.015236
2022-01-20 20:41:58,539 iteration 2816 : loss : 0.026728, loss_ce: 0.009868
2022-01-20 20:41:59,857 iteration 2817 : loss : 0.041339, loss_ce: 0.013749
2022-01-20 20:42:01,290 iteration 2818 : loss : 0.034166, loss_ce: 0.012640
2022-01-20 20:42:02,698 iteration 2819 : loss : 0.028396, loss_ce: 0.012532
2022-01-20 20:42:04,017 iteration 2820 : loss : 0.027875, loss_ce: 0.009779
2022-01-20 20:42:05,377 iteration 2821 : loss : 0.019577, loss_ce: 0.008057
2022-01-20 20:42:06,775 iteration 2822 : loss : 0.046822, loss_ce: 0.007394
 42%|███████████▏               | 166/400 [1:09:06<1:38:05, 25.15s/it]2022-01-20 20:42:08,199 iteration 2823 : loss : 0.025314, loss_ce: 0.008716
2022-01-20 20:42:09,586 iteration 2824 : loss : 0.035191, loss_ce: 0.009612
2022-01-20 20:42:10,963 iteration 2825 : loss : 0.045176, loss_ce: 0.015160
2022-01-20 20:42:12,269 iteration 2826 : loss : 0.033943, loss_ce: 0.013307
2022-01-20 20:42:13,664 iteration 2827 : loss : 0.035393, loss_ce: 0.018255
2022-01-20 20:42:14,990 iteration 2828 : loss : 0.038370, loss_ce: 0.019731
2022-01-20 20:42:16,308 iteration 2829 : loss : 0.040000, loss_ce: 0.013171
2022-01-20 20:42:17,676 iteration 2830 : loss : 0.027786, loss_ce: 0.010294
2022-01-20 20:42:19,004 iteration 2831 : loss : 0.053124, loss_ce: 0.027003
2022-01-20 20:42:20,288 iteration 2832 : loss : 0.025380, loss_ce: 0.010382
2022-01-20 20:42:21,696 iteration 2833 : loss : 0.021980, loss_ce: 0.009322
2022-01-20 20:42:23,049 iteration 2834 : loss : 0.026543, loss_ce: 0.011119
2022-01-20 20:42:24,410 iteration 2835 : loss : 0.057512, loss_ce: 0.028063
2022-01-20 20:42:25,776 iteration 2836 : loss : 0.041250, loss_ce: 0.019401
2022-01-20 20:42:27,161 iteration 2837 : loss : 0.030440, loss_ce: 0.010803
2022-01-20 20:42:28,536 iteration 2838 : loss : 0.034992, loss_ce: 0.012097
2022-01-20 20:42:29,815 iteration 2839 : loss : 0.029914, loss_ce: 0.012237
 42%|███████████▎               | 167/400 [1:09:29<1:35:12, 24.52s/it]2022-01-20 20:42:31,187 iteration 2840 : loss : 0.032334, loss_ce: 0.011427
2022-01-20 20:42:32,488 iteration 2841 : loss : 0.024179, loss_ce: 0.010161
2022-01-20 20:42:33,749 iteration 2842 : loss : 0.018308, loss_ce: 0.006659
2022-01-20 20:42:35,073 iteration 2843 : loss : 0.029458, loss_ce: 0.008381
2022-01-20 20:42:36,452 iteration 2844 : loss : 0.032047, loss_ce: 0.013414
slurmstepd: error: *** JOB 509090 ON biwirender06 CANCELLED AT 2022-01-20T20:42:37 ***
