2022-01-09 11:24:20,796 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-09 11:24:20,797 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-09 11:24:20,797 ============================================================
2022-01-09 11:24:20,797 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-09 11:24:20,797 ============================================================
2022-01-09 11:24:20,797 Loading data...
2022-01-09 11:24:20,797 Reading NCI - RUNMC images...
2022-01-09 11:24:20,797 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-09 11:24:20,799 Already preprocessed this configuration. Loading now!
2022-01-09 11:24:20,817 Training Images: (256, 256, 286)
2022-01-09 11:24:20,817 Training Labels: (256, 256, 286)
2022-01-09 11:24:20,817 Validation Images: (256, 256, 98)
2022-01-09 11:24:20,817 Validation Labels: (256, 256, 98)
2022-01-09 11:24:20,817 ============================================================
2022-01-09 11:24:20,845 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-09 11:24:23,455 iteration 1 : loss : 0.767216, loss_ce: 0.862417
2022-01-09 11:24:24,842 iteration 2 : loss : 0.726907, loss_ce: 0.777426
2022-01-09 11:24:26,204 iteration 3 : loss : 0.684242, loss_ce: 0.688064
2022-01-09 11:24:27,615 iteration 4 : loss : 0.631409, loss_ce: 0.640934
2022-01-09 11:24:28,960 iteration 5 : loss : 0.605140, loss_ce: 0.564661
2022-01-09 11:24:30,415 iteration 6 : loss : 0.562328, loss_ce: 0.513226
2022-01-09 11:24:31,837 iteration 7 : loss : 0.548059, loss_ce: 0.474642
2022-01-09 11:24:33,268 iteration 8 : loss : 0.512022, loss_ce: 0.423386
2022-01-09 11:24:34,589 iteration 9 : loss : 0.473756, loss_ce: 0.400419
2022-01-09 11:24:36,025 iteration 10 : loss : 0.465380, loss_ce: 0.367269
2022-01-09 11:24:37,465 iteration 11 : loss : 0.441495, loss_ce: 0.326426
2022-01-09 11:24:38,940 iteration 12 : loss : 0.425358, loss_ce: 0.309941
2022-01-09 11:24:40,323 iteration 13 : loss : 0.400759, loss_ce: 0.272504
2022-01-09 11:24:41,809 iteration 14 : loss : 0.396017, loss_ce: 0.262669
2022-01-09 11:24:43,223 iteration 15 : loss : 0.368836, loss_ce: 0.231898
2022-01-09 11:24:44,617 iteration 16 : loss : 0.372216, loss_ce: 0.231088
2022-01-09 11:24:45,983 iteration 17 : loss : 0.363322, loss_ce: 0.191989
  0%|                               | 1/400 [00:25<2:47:34, 25.20s/it]2022-01-09 11:24:47,482 iteration 18 : loss : 0.344310, loss_ce: 0.195387
2022-01-09 11:24:48,915 iteration 19 : loss : 0.343150, loss_ce: 0.172416
2022-01-09 11:24:50,440 iteration 20 : loss : 0.316709, loss_ce: 0.176624
2022-01-09 11:24:51,836 iteration 21 : loss : 0.345063, loss_ce: 0.170329
2022-01-09 11:24:53,186 iteration 22 : loss : 0.312941, loss_ce: 0.153149
2022-01-09 11:24:54,675 iteration 23 : loss : 0.327974, loss_ce: 0.153219
2022-01-09 11:24:56,159 iteration 24 : loss : 0.291819, loss_ce: 0.146753
2022-01-09 11:24:57,675 iteration 25 : loss : 0.326252, loss_ce: 0.183894
2022-01-09 11:24:59,237 iteration 26 : loss : 0.305268, loss_ce: 0.159533
2022-01-09 11:25:00,608 iteration 27 : loss : 0.317444, loss_ce: 0.153957
2022-01-09 11:25:01,953 iteration 28 : loss : 0.266756, loss_ce: 0.119825
2022-01-09 11:25:03,392 iteration 29 : loss : 0.326131, loss_ce: 0.156068
2022-01-09 11:25:04,767 iteration 30 : loss : 0.273309, loss_ce: 0.126102
2022-01-09 11:25:06,133 iteration 31 : loss : 0.268335, loss_ce: 0.106963
2022-01-09 11:25:07,526 iteration 32 : loss : 0.292647, loss_ce: 0.156022
2022-01-09 11:25:08,976 iteration 33 : loss : 0.300202, loss_ce: 0.142856
2022-01-09 11:25:10,496 iteration 34 : loss : 0.232952, loss_ce: 0.100621
  0%|▏                              | 2/400 [00:49<2:44:24, 24.79s/it]2022-01-09 11:25:11,994 iteration 35 : loss : 0.243869, loss_ce: 0.112606
2022-01-09 11:25:13,422 iteration 36 : loss : 0.272930, loss_ce: 0.094541
2022-01-09 11:25:14,882 iteration 37 : loss : 0.252716, loss_ce: 0.097918
2022-01-09 11:25:16,311 iteration 38 : loss : 0.268445, loss_ce: 0.108882
2022-01-09 11:25:17,643 iteration 39 : loss : 0.285199, loss_ce: 0.125271
2022-01-09 11:25:19,047 iteration 40 : loss : 0.314035, loss_ce: 0.148195
2022-01-09 11:25:20,375 iteration 41 : loss : 0.199902, loss_ce: 0.091513
2022-01-09 11:25:21,826 iteration 42 : loss : 0.269151, loss_ce: 0.132189
2022-01-09 11:25:23,244 iteration 43 : loss : 0.245735, loss_ce: 0.108524
2022-01-09 11:25:24,583 iteration 44 : loss : 0.248513, loss_ce: 0.129567
2022-01-09 11:25:26,081 iteration 45 : loss : 0.269856, loss_ce: 0.109147
2022-01-09 11:25:27,508 iteration 46 : loss : 0.237137, loss_ce: 0.102987
2022-01-09 11:25:28,942 iteration 47 : loss : 0.240658, loss_ce: 0.088935
2022-01-09 11:25:30,362 iteration 48 : loss : 0.237119, loss_ce: 0.103957
2022-01-09 11:25:31,839 iteration 49 : loss : 0.292716, loss_ce: 0.130147
2022-01-09 11:25:33,302 iteration 50 : loss : 0.226996, loss_ce: 0.091690
2022-01-09 11:25:34,800 iteration 51 : loss : 0.223438, loss_ce: 0.093339
  1%|▏                              | 3/400 [01:14<2:42:33, 24.57s/it]2022-01-09 11:25:36,253 iteration 52 : loss : 0.279365, loss_ce: 0.106258
2022-01-09 11:25:37,705 iteration 53 : loss : 0.232209, loss_ce: 0.106408
2022-01-09 11:25:39,305 iteration 54 : loss : 0.232217, loss_ce: 0.092399
2022-01-09 11:25:40,890 iteration 55 : loss : 0.290666, loss_ce: 0.110156
2022-01-09 11:25:42,480 iteration 56 : loss : 0.257336, loss_ce: 0.125281
2022-01-09 11:25:44,011 iteration 57 : loss : 0.261179, loss_ce: 0.125129
2022-01-09 11:25:45,516 iteration 58 : loss : 0.264336, loss_ce: 0.127535
2022-01-09 11:25:47,022 iteration 59 : loss : 0.220798, loss_ce: 0.097075
2022-01-09 11:25:48,551 iteration 60 : loss : 0.204110, loss_ce: 0.094221
2022-01-09 11:25:50,100 iteration 61 : loss : 0.254604, loss_ce: 0.113167
2022-01-09 11:25:51,720 iteration 62 : loss : 0.297622, loss_ce: 0.114791
2022-01-09 11:25:53,241 iteration 63 : loss : 0.268154, loss_ce: 0.118690
2022-01-09 11:25:54,812 iteration 64 : loss : 0.312986, loss_ce: 0.124808
2022-01-09 11:25:56,381 iteration 65 : loss : 0.222715, loss_ce: 0.081395
2022-01-09 11:25:58,002 iteration 66 : loss : 0.271306, loss_ce: 0.112693
2022-01-09 11:25:59,548 iteration 67 : loss : 0.241628, loss_ce: 0.109904
2022-01-09 11:26:01,036 iteration 68 : loss : 0.232403, loss_ce: 0.108371
  1%|▎                              | 4/400 [01:40<2:46:28, 25.22s/it]2022-01-09 11:26:02,668 iteration 69 : loss : 0.282678, loss_ce: 0.130152
2022-01-09 11:26:04,206 iteration 70 : loss : 0.268130, loss_ce: 0.111820
2022-01-09 11:26:05,765 iteration 71 : loss : 0.263637, loss_ce: 0.135057
2022-01-09 11:26:07,335 iteration 72 : loss : 0.248664, loss_ce: 0.110825
2022-01-09 11:26:08,920 iteration 73 : loss : 0.237353, loss_ce: 0.093606
2022-01-09 11:26:10,500 iteration 74 : loss : 0.202887, loss_ce: 0.081064
2022-01-09 11:26:12,031 iteration 75 : loss : 0.208252, loss_ce: 0.101972
2022-01-09 11:26:13,514 iteration 76 : loss : 0.224306, loss_ce: 0.104254
2022-01-09 11:26:15,073 iteration 77 : loss : 0.249657, loss_ce: 0.110432
2022-01-09 11:26:16,637 iteration 78 : loss : 0.220855, loss_ce: 0.081228
2022-01-09 11:26:18,213 iteration 79 : loss : 0.267981, loss_ce: 0.108095
2022-01-09 11:26:19,677 iteration 80 : loss : 0.224171, loss_ce: 0.091006
2022-01-09 11:26:21,311 iteration 81 : loss : 0.268940, loss_ce: 0.122488
2022-01-09 11:26:22,863 iteration 82 : loss : 0.274378, loss_ce: 0.090828
2022-01-09 11:26:24,363 iteration 83 : loss : 0.297348, loss_ce: 0.089673
2022-01-09 11:26:26,016 iteration 84 : loss : 0.305291, loss_ce: 0.130802
2022-01-09 11:26:26,016 Training Data Eval:
