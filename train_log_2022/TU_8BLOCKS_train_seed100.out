2022-01-07 23:24:35,752 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-07 23:24:35,753 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-07 23:24:35,753 ============================================================
2022-01-07 23:24:35,753 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-07 23:24:35,753 ============================================================
2022-01-07 23:24:35,753 Loading data...
2022-01-07 23:24:35,753 Reading NCI - RUNMC images...
2022-01-07 23:24:35,753 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-07 23:24:35,756 Already preprocessed this configuration. Loading now!
2022-01-07 23:24:35,779 Training Images: (256, 256, 286)
2022-01-07 23:24:35,779 Training Labels: (256, 256, 286)
2022-01-07 23:24:35,779 Validation Images: (256, 256, 98)
2022-01-07 23:24:35,779 Validation Labels: (256, 256, 98)
2022-01-07 23:24:35,779 ============================================================
2022-01-07 23:24:35,825 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-07 23:24:38,558 iteration 1 : loss : 0.817821, loss_ce: 0.947751
2022-01-07 23:24:39,812 iteration 2 : loss : 0.762268, loss_ce: 0.846804
2022-01-07 23:24:41,059 iteration 3 : loss : 0.738966, loss_ce: 0.779807
2022-01-07 23:24:42,352 iteration 4 : loss : 0.674371, loss_ce: 0.711501
2022-01-07 23:24:43,562 iteration 5 : loss : 0.643165, loss_ce: 0.633728
2022-01-07 23:24:44,890 iteration 6 : loss : 0.603041, loss_ce: 0.576240
2022-01-07 23:24:46,188 iteration 7 : loss : 0.582020, loss_ce: 0.530831
2022-01-07 23:24:47,519 iteration 8 : loss : 0.544480, loss_ce: 0.480908
2022-01-07 23:24:48,709 iteration 9 : loss : 0.510370, loss_ce: 0.460116
2022-01-07 23:24:50,018 iteration 10 : loss : 0.492254, loss_ce: 0.417705
2022-01-07 23:24:51,332 iteration 11 : loss : 0.461787, loss_ce: 0.377982
2022-01-07 23:24:52,678 iteration 12 : loss : 0.450419, loss_ce: 0.352710
2022-01-07 23:24:53,959 iteration 13 : loss : 0.423233, loss_ce: 0.310349
2022-01-07 23:24:55,304 iteration 14 : loss : 0.414807, loss_ce: 0.294851
2022-01-07 23:24:56,601 iteration 15 : loss : 0.389387, loss_ce: 0.262804
2022-01-07 23:24:57,954 iteration 16 : loss : 0.387954, loss_ce: 0.258726
2022-01-07 23:24:59,316 iteration 17 : loss : 0.386562, loss_ce: 0.223006
  0%|                               | 1/400 [00:23<2:36:38, 23.56s/it]2022-01-07 23:25:00,801 iteration 18 : loss : 0.351271, loss_ce: 0.220515
2022-01-07 23:25:02,217 iteration 19 : loss : 0.348928, loss_ce: 0.186011
2022-01-07 23:25:03,716 iteration 20 : loss : 0.350956, loss_ce: 0.199603
2022-01-07 23:25:05,087 iteration 21 : loss : 0.367660, loss_ce: 0.191323
2022-01-07 23:25:06,419 iteration 22 : loss : 0.342023, loss_ce: 0.176421
2022-01-07 23:25:07,857 iteration 23 : loss : 0.331111, loss_ce: 0.172774
2022-01-07 23:25:09,292 iteration 24 : loss : 0.288908, loss_ce: 0.161194
2022-01-07 23:25:10,756 iteration 25 : loss : 0.341972, loss_ce: 0.209416
2022-01-07 23:25:12,160 iteration 26 : loss : 0.295282, loss_ce: 0.160288
2022-01-07 23:25:13,496 iteration 27 : loss : 0.304992, loss_ce: 0.158315
2022-01-07 23:25:14,820 iteration 28 : loss : 0.272588, loss_ce: 0.139530
2022-01-07 23:25:16,231 iteration 29 : loss : 0.324585, loss_ce: 0.170227
2022-01-07 23:25:17,599 iteration 30 : loss : 0.289943, loss_ce: 0.144085
2022-01-07 23:25:18,935 iteration 31 : loss : 0.262031, loss_ce: 0.108539
2022-01-07 23:25:20,305 iteration 32 : loss : 0.282170, loss_ce: 0.151813
2022-01-07 23:25:21,724 iteration 33 : loss : 0.288865, loss_ce: 0.137556
2022-01-07 23:25:23,206 iteration 34 : loss : 0.241616, loss_ce: 0.104425
  0%|▏                              | 2/400 [00:47<2:37:28, 23.74s/it]2022-01-07 23:25:24,685 iteration 35 : loss : 0.266931, loss_ce: 0.129085
2022-01-07 23:25:26,083 iteration 36 : loss : 0.292835, loss_ce: 0.102627
2022-01-07 23:25:27,506 iteration 37 : loss : 0.249445, loss_ce: 0.098352
2022-01-07 23:25:28,896 iteration 38 : loss : 0.255045, loss_ce: 0.102031
2022-01-07 23:25:30,211 iteration 39 : loss : 0.311670, loss_ce: 0.139123
2022-01-07 23:25:31,569 iteration 40 : loss : 0.314630, loss_ce: 0.146326
2022-01-07 23:25:32,852 iteration 41 : loss : 0.262234, loss_ce: 0.115667
2022-01-07 23:25:34,258 iteration 42 : loss : 0.291327, loss_ce: 0.133809
2022-01-07 23:25:35,649 iteration 43 : loss : 0.216168, loss_ce: 0.089905
2022-01-07 23:25:36,960 iteration 44 : loss : 0.297447, loss_ce: 0.152620
2022-01-07 23:25:38,404 iteration 45 : loss : 0.262518, loss_ce: 0.109547
2022-01-07 23:25:39,794 iteration 46 : loss : 0.254177, loss_ce: 0.114568
2022-01-07 23:25:41,181 iteration 47 : loss : 0.266035, loss_ce: 0.111957
2022-01-07 23:25:42,590 iteration 48 : loss : 0.231361, loss_ce: 0.110283
2022-01-07 23:25:44,024 iteration 49 : loss : 0.311823, loss_ce: 0.141109
2022-01-07 23:25:45,458 iteration 50 : loss : 0.222285, loss_ce: 0.089763
2022-01-07 23:25:46,908 iteration 51 : loss : 0.250649, loss_ce: 0.105308
  1%|▏                              | 3/400 [01:11<2:36:57, 23.72s/it]2022-01-07 23:25:48,297 iteration 52 : loss : 0.246368, loss_ce: 0.097442
2022-01-07 23:25:49,617 iteration 53 : loss : 0.231817, loss_ce: 0.105013
2022-01-07 23:25:51,059 iteration 54 : loss : 0.258999, loss_ce: 0.101893
2022-01-07 23:25:52,475 iteration 55 : loss : 0.340868, loss_ce: 0.130876
2022-01-07 23:25:53,914 iteration 56 : loss : 0.262270, loss_ce: 0.118964
2022-01-07 23:25:55,285 iteration 57 : loss : 0.285899, loss_ce: 0.134078
2022-01-07 23:25:56,615 iteration 58 : loss : 0.287385, loss_ce: 0.151480
2022-01-07 23:25:57,944 iteration 59 : loss : 0.251847, loss_ce: 0.119198
2022-01-07 23:25:59,302 iteration 60 : loss : 0.220944, loss_ce: 0.111396
2022-01-07 23:26:00,688 iteration 61 : loss : 0.260601, loss_ce: 0.119226
2022-01-07 23:26:02,135 iteration 62 : loss : 0.281685, loss_ce: 0.109168
2022-01-07 23:26:03,478 iteration 63 : loss : 0.256097, loss_ce: 0.112844
2022-01-07 23:26:04,882 iteration 64 : loss : 0.373223, loss_ce: 0.157215
2022-01-07 23:26:06,297 iteration 65 : loss : 0.269534, loss_ce: 0.101351
2022-01-07 23:26:07,740 iteration 66 : loss : 0.268519, loss_ce: 0.109749
2022-01-07 23:26:09,110 iteration 67 : loss : 0.259829, loss_ce: 0.112138
2022-01-07 23:26:10,426 iteration 68 : loss : 0.251068, loss_ce: 0.103866
  1%|▎                              | 4/400 [01:34<2:36:01, 23.64s/it]2022-01-07 23:26:11,888 iteration 69 : loss : 0.297868, loss_ce: 0.133950
2022-01-07 23:26:13,260 iteration 70 : loss : 0.290106, loss_ce: 0.120191
2022-01-07 23:26:14,634 iteration 71 : loss : 0.279756, loss_ce: 0.145203
2022-01-07 23:26:16,027 iteration 72 : loss : 0.264245, loss_ce: 0.117793
2022-01-07 23:26:17,442 iteration 73 : loss : 0.237321, loss_ce: 0.092912
2022-01-07 23:26:18,870 iteration 74 : loss : 0.227030, loss_ce: 0.091475
2022-01-07 23:26:20,227 iteration 75 : loss : 0.240365, loss_ce: 0.114419
2022-01-07 23:26:21,529 iteration 76 : loss : 0.246494, loss_ce: 0.108099
2022-01-07 23:26:22,922 iteration 77 : loss : 0.268202, loss_ce: 0.114971
2022-01-07 23:26:24,322 iteration 78 : loss : 0.229277, loss_ce: 0.089010
2022-01-07 23:26:25,721 iteration 79 : loss : 0.265495, loss_ce: 0.106907
2022-01-07 23:26:27,013 iteration 80 : loss : 0.230286, loss_ce: 0.085962
2022-01-07 23:26:28,454 iteration 81 : loss : 0.304737, loss_ce: 0.143093
2022-01-07 23:26:29,815 iteration 82 : loss : 0.303694, loss_ce: 0.108235
2022-01-07 23:26:31,160 iteration 83 : loss : 0.314784, loss_ce: 0.099391
2022-01-07 23:26:32,625 iteration 84 : loss : 0.319080, loss_ce: 0.143115
2022-01-07 23:26:32,625 Training Data Eval:
2022-01-07 23:26:39,516   Average segmentation loss on training set: 0.3979
2022-01-07 23:26:39,516 Validation Data Eval:
2022-01-07 23:26:42,040   Average segmentation loss on validation set: 0.3566
2022-01-07 23:26:46,300 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:26:47,527 iteration 85 : loss : 0.297751, loss_ce: 0.136968
  1%|▍                              | 5/400 [02:11<3:07:36, 28.50s/it]2022-01-07 23:26:48,909 iteration 86 : loss : 0.251567, loss_ce: 0.114055
2022-01-07 23:26:50,246 iteration 87 : loss : 0.241050, loss_ce: 0.097149
2022-01-07 23:26:51,575 iteration 88 : loss : 0.229320, loss_ce: 0.113777
2022-01-07 23:26:52,965 iteration 89 : loss : 0.283572, loss_ce: 0.113888
2022-01-07 23:26:54,354 iteration 90 : loss : 0.224960, loss_ce: 0.099277
2022-01-07 23:26:55,806 iteration 91 : loss : 0.237616, loss_ce: 0.110162
2022-01-07 23:26:57,244 iteration 92 : loss : 0.224414, loss_ce: 0.082281
2022-01-07 23:26:58,567 iteration 93 : loss : 0.223668, loss_ce: 0.078434
2022-01-07 23:26:59,982 iteration 94 : loss : 0.216400, loss_ce: 0.091884
2022-01-07 23:27:01,350 iteration 95 : loss : 0.306875, loss_ce: 0.153598
2022-01-07 23:27:02,794 iteration 96 : loss : 0.229690, loss_ce: 0.095273
2022-01-07 23:27:04,177 iteration 97 : loss : 0.236949, loss_ce: 0.097268
2022-01-07 23:27:05,596 iteration 98 : loss : 0.295692, loss_ce: 0.133094
2022-01-07 23:27:06,994 iteration 99 : loss : 0.289290, loss_ce: 0.118883
2022-01-07 23:27:08,405 iteration 100 : loss : 0.213707, loss_ce: 0.088882
2022-01-07 23:27:09,767 iteration 101 : loss : 0.235572, loss_ce: 0.095143
2022-01-07 23:27:11,121 iteration 102 : loss : 0.238482, loss_ce: 0.102187
  2%|▍                              | 6/400 [02:35<2:56:10, 26.83s/it]2022-01-07 23:27:12,588 iteration 103 : loss : 0.246193, loss_ce: 0.083645
2022-01-07 23:27:14,020 iteration 104 : loss : 0.254822, loss_ce: 0.098410
2022-01-07 23:27:15,449 iteration 105 : loss : 0.190780, loss_ce: 0.072169
2022-01-07 23:27:16,949 iteration 106 : loss : 0.250360, loss_ce: 0.103750
2022-01-07 23:27:18,397 iteration 107 : loss : 0.295503, loss_ce: 0.115383
2022-01-07 23:27:19,779 iteration 108 : loss : 0.237038, loss_ce: 0.083429
2022-01-07 23:27:21,092 iteration 109 : loss : 0.268612, loss_ce: 0.114008
2022-01-07 23:27:22,438 iteration 110 : loss : 0.254345, loss_ce: 0.110803
2022-01-07 23:27:23,873 iteration 111 : loss : 0.243237, loss_ce: 0.110360
2022-01-07 23:27:25,273 iteration 112 : loss : 0.244913, loss_ce: 0.102199
2022-01-07 23:27:26,595 iteration 113 : loss : 0.222947, loss_ce: 0.086976
2022-01-07 23:27:28,009 iteration 114 : loss : 0.228407, loss_ce: 0.084106
2022-01-07 23:27:29,374 iteration 115 : loss : 0.214221, loss_ce: 0.101890
2022-01-07 23:27:30,812 iteration 116 : loss : 0.268309, loss_ce: 0.134040
2022-01-07 23:27:32,161 iteration 117 : loss : 0.215972, loss_ce: 0.092280
2022-01-07 23:27:33,526 iteration 118 : loss : 0.276965, loss_ce: 0.131487
2022-01-07 23:27:34,910 iteration 119 : loss : 0.211692, loss_ce: 0.080121
  2%|▌                              | 7/400 [02:59<2:49:12, 25.83s/it]2022-01-07 23:27:36,399 iteration 120 : loss : 0.226419, loss_ce: 0.093606
2022-01-07 23:27:37,844 iteration 121 : loss : 0.309682, loss_ce: 0.131217
2022-01-07 23:27:39,158 iteration 122 : loss : 0.251154, loss_ce: 0.112778
2022-01-07 23:27:40,575 iteration 123 : loss : 0.230786, loss_ce: 0.093753
2022-01-07 23:27:42,001 iteration 124 : loss : 0.306907, loss_ce: 0.124435
2022-01-07 23:27:43,394 iteration 125 : loss : 0.246612, loss_ce: 0.111942
2022-01-07 23:27:44,817 iteration 126 : loss : 0.256809, loss_ce: 0.091841
2022-01-07 23:27:46,244 iteration 127 : loss : 0.220175, loss_ce: 0.087653
2022-01-07 23:27:47,687 iteration 128 : loss : 0.203414, loss_ce: 0.090680
2022-01-07 23:27:49,066 iteration 129 : loss : 0.224769, loss_ce: 0.082373
2022-01-07 23:27:50,507 iteration 130 : loss : 0.247382, loss_ce: 0.120949
2022-01-07 23:27:51,891 iteration 131 : loss : 0.230708, loss_ce: 0.107337
2022-01-07 23:27:53,231 iteration 132 : loss : 0.210101, loss_ce: 0.074341
2022-01-07 23:27:54,687 iteration 133 : loss : 0.252864, loss_ce: 0.089294
2022-01-07 23:27:56,104 iteration 134 : loss : 0.243339, loss_ce: 0.098043
2022-01-07 23:27:57,507 iteration 135 : loss : 0.198680, loss_ce: 0.082777
2022-01-07 23:27:58,878 iteration 136 : loss : 0.259655, loss_ce: 0.122548
  2%|▌                              | 8/400 [03:23<2:44:53, 25.24s/it]2022-01-07 23:28:00,235 iteration 137 : loss : 0.186132, loss_ce: 0.058379
2022-01-07 23:28:01,595 iteration 138 : loss : 0.237351, loss_ce: 0.118526
2022-01-07 23:28:03,048 iteration 139 : loss : 0.229676, loss_ce: 0.089200
2022-01-07 23:28:04,531 iteration 140 : loss : 0.228573, loss_ce: 0.088171
2022-01-07 23:28:05,933 iteration 141 : loss : 0.279948, loss_ce: 0.112968
2022-01-07 23:28:07,292 iteration 142 : loss : 0.241473, loss_ce: 0.099980
2022-01-07 23:28:08,624 iteration 143 : loss : 0.221807, loss_ce: 0.096305
2022-01-07 23:28:10,051 iteration 144 : loss : 0.263876, loss_ce: 0.107611
2022-01-07 23:28:11,606 iteration 145 : loss : 0.188764, loss_ce: 0.087648
2022-01-07 23:28:12,987 iteration 146 : loss : 0.238496, loss_ce: 0.094747
2022-01-07 23:28:14,366 iteration 147 : loss : 0.277624, loss_ce: 0.118535
2022-01-07 23:28:15,757 iteration 148 : loss : 0.240513, loss_ce: 0.100146
2022-01-07 23:28:17,159 iteration 149 : loss : 0.218393, loss_ce: 0.084334
2022-01-07 23:28:18,542 iteration 150 : loss : 0.281492, loss_ce: 0.147930
2022-01-07 23:28:19,950 iteration 151 : loss : 0.238824, loss_ce: 0.110044
2022-01-07 23:28:21,397 iteration 152 : loss : 0.213178, loss_ce: 0.094515
2022-01-07 23:28:22,846 iteration 153 : loss : 0.190102, loss_ce: 0.081566
  2%|▋                              | 9/400 [03:47<2:41:53, 24.84s/it]2022-01-07 23:28:24,274 iteration 154 : loss : 0.192582, loss_ce: 0.078568
2022-01-07 23:28:25,721 iteration 155 : loss : 0.242639, loss_ce: 0.097271
2022-01-07 23:28:27,147 iteration 156 : loss : 0.192747, loss_ce: 0.073865
2022-01-07 23:28:28,558 iteration 157 : loss : 0.247054, loss_ce: 0.094852
2022-01-07 23:28:29,964 iteration 158 : loss : 0.260169, loss_ce: 0.112458
2022-01-07 23:28:31,381 iteration 159 : loss : 0.232768, loss_ce: 0.093670
2022-01-07 23:28:32,756 iteration 160 : loss : 0.321802, loss_ce: 0.120161
2022-01-07 23:28:34,173 iteration 161 : loss : 0.220587, loss_ce: 0.098856
2022-01-07 23:28:35,647 iteration 162 : loss : 0.237975, loss_ce: 0.105304
2022-01-07 23:28:36,953 iteration 163 : loss : 0.182404, loss_ce: 0.078522
2022-01-07 23:28:38,423 iteration 164 : loss : 0.182635, loss_ce: 0.076572
2022-01-07 23:28:39,793 iteration 165 : loss : 0.199563, loss_ce: 0.075088
2022-01-07 23:28:41,241 iteration 166 : loss : 0.246982, loss_ce: 0.101086
2022-01-07 23:28:42,681 iteration 167 : loss : 0.245685, loss_ce: 0.106799
2022-01-07 23:28:44,112 iteration 168 : loss : 0.223817, loss_ce: 0.090448
2022-01-07 23:28:45,500 iteration 169 : loss : 0.215443, loss_ce: 0.087544
2022-01-07 23:28:45,500 Training Data Eval:
2022-01-07 23:28:52,384   Average segmentation loss on training set: 0.2301
2022-01-07 23:28:52,384 Validation Data Eval:
2022-01-07 23:28:54,757   Average segmentation loss on validation set: 0.2079
2022-01-07 23:28:58,901 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:29:00,144 iteration 170 : loss : 0.205018, loss_ce: 0.093307
  2%|▊                             | 10/400 [04:24<3:06:28, 28.69s/it]2022-01-07 23:29:01,507 iteration 171 : loss : 0.265151, loss_ce: 0.128684
2022-01-07 23:29:02,768 iteration 172 : loss : 0.297238, loss_ce: 0.117612
2022-01-07 23:29:04,082 iteration 173 : loss : 0.182066, loss_ce: 0.079392
2022-01-07 23:29:05,528 iteration 174 : loss : 0.245150, loss_ce: 0.104960
2022-01-07 23:29:06,896 iteration 175 : loss : 0.237676, loss_ce: 0.102895
2022-01-07 23:29:08,406 iteration 176 : loss : 0.216154, loss_ce: 0.082816
2022-01-07 23:29:09,780 iteration 177 : loss : 0.259092, loss_ce: 0.097176
2022-01-07 23:29:11,214 iteration 178 : loss : 0.219561, loss_ce: 0.075514
2022-01-07 23:29:12,633 iteration 179 : loss : 0.219949, loss_ce: 0.090383
2022-01-07 23:29:14,083 iteration 180 : loss : 0.201168, loss_ce: 0.061889
2022-01-07 23:29:15,509 iteration 181 : loss : 0.186838, loss_ce: 0.067555
2022-01-07 23:29:16,931 iteration 182 : loss : 0.187448, loss_ce: 0.073234
2022-01-07 23:29:18,362 iteration 183 : loss : 0.218239, loss_ce: 0.091621
2022-01-07 23:29:19,777 iteration 184 : loss : 0.246194, loss_ce: 0.107635
2022-01-07 23:29:21,146 iteration 185 : loss : 0.227676, loss_ce: 0.108112
2022-01-07 23:29:22,492 iteration 186 : loss : 0.206384, loss_ce: 0.095581
2022-01-07 23:29:23,818 iteration 187 : loss : 0.261098, loss_ce: 0.135454
  3%|▊                             | 11/400 [04:48<2:56:02, 27.15s/it]2022-01-07 23:29:25,285 iteration 188 : loss : 0.303201, loss_ce: 0.138557
2022-01-07 23:29:26,725 iteration 189 : loss : 0.229840, loss_ce: 0.104671
2022-01-07 23:29:28,174 iteration 190 : loss : 0.214120, loss_ce: 0.079958
2022-01-07 23:29:29,569 iteration 191 : loss : 0.292533, loss_ce: 0.132681
2022-01-07 23:29:30,865 iteration 192 : loss : 0.215108, loss_ce: 0.087635
2022-01-07 23:29:32,217 iteration 193 : loss : 0.331627, loss_ce: 0.144054
2022-01-07 23:29:33,542 iteration 194 : loss : 0.272143, loss_ce: 0.120322
2022-01-07 23:29:35,002 iteration 195 : loss : 0.172770, loss_ce: 0.068801
2022-01-07 23:29:36,419 iteration 196 : loss : 0.224765, loss_ce: 0.081607
2022-01-07 23:29:37,841 iteration 197 : loss : 0.219559, loss_ce: 0.096353
2022-01-07 23:29:39,265 iteration 198 : loss : 0.311985, loss_ce: 0.152284
2022-01-07 23:29:40,681 iteration 199 : loss : 0.217346, loss_ce: 0.102661
2022-01-07 23:29:42,050 iteration 200 : loss : 0.259824, loss_ce: 0.104606
2022-01-07 23:29:43,451 iteration 201 : loss : 0.254335, loss_ce: 0.113583
2022-01-07 23:29:44,734 iteration 202 : loss : 0.232389, loss_ce: 0.073407
2022-01-07 23:29:46,171 iteration 203 : loss : 0.186349, loss_ce: 0.073373
2022-01-07 23:29:47,541 iteration 204 : loss : 0.222354, loss_ce: 0.107982
  3%|▉                             | 12/400 [05:11<2:48:51, 26.11s/it]2022-01-07 23:29:49,048 iteration 205 : loss : 0.209083, loss_ce: 0.080562
2022-01-07 23:29:50,475 iteration 206 : loss : 0.361798, loss_ce: 0.159154
2022-01-07 23:29:51,938 iteration 207 : loss : 0.283325, loss_ce: 0.121517
2022-01-07 23:29:53,383 iteration 208 : loss : 0.212501, loss_ce: 0.101685
2022-01-07 23:29:54,801 iteration 209 : loss : 0.192623, loss_ce: 0.080392
2022-01-07 23:29:56,208 iteration 210 : loss : 0.217644, loss_ce: 0.102080
2022-01-07 23:29:57,604 iteration 211 : loss : 0.213002, loss_ce: 0.097151
2022-01-07 23:29:59,050 iteration 212 : loss : 0.190973, loss_ce: 0.085388
2022-01-07 23:30:00,490 iteration 213 : loss : 0.225054, loss_ce: 0.099315
2022-01-07 23:30:01,888 iteration 214 : loss : 0.231766, loss_ce: 0.091774
2022-01-07 23:30:03,242 iteration 215 : loss : 0.274804, loss_ce: 0.112723
2022-01-07 23:30:04,678 iteration 216 : loss : 0.186538, loss_ce: 0.068053
2022-01-07 23:30:06,129 iteration 217 : loss : 0.244121, loss_ce: 0.104271
2022-01-07 23:30:07,511 iteration 218 : loss : 0.198667, loss_ce: 0.084668
2022-01-07 23:30:08,885 iteration 219 : loss : 0.231978, loss_ce: 0.108335
2022-01-07 23:30:10,289 iteration 220 : loss : 0.195031, loss_ce: 0.079256
2022-01-07 23:30:11,720 iteration 221 : loss : 0.221890, loss_ce: 0.094976
  3%|▉                             | 13/400 [05:35<2:44:37, 25.52s/it]2022-01-07 23:30:13,162 iteration 222 : loss : 0.186194, loss_ce: 0.075170
2022-01-07 23:30:14,566 iteration 223 : loss : 0.327797, loss_ce: 0.157020
2022-01-07 23:30:15,940 iteration 224 : loss : 0.233777, loss_ce: 0.085168
2022-01-07 23:30:17,444 iteration 225 : loss : 0.201223, loss_ce: 0.096848
2022-01-07 23:30:18,868 iteration 226 : loss : 0.252432, loss_ce: 0.100843
2022-01-07 23:30:20,273 iteration 227 : loss : 0.221375, loss_ce: 0.091140
2022-01-07 23:30:21,650 iteration 228 : loss : 0.169431, loss_ce: 0.058958
2022-01-07 23:30:23,062 iteration 229 : loss : 0.231593, loss_ce: 0.085941
2022-01-07 23:30:24,404 iteration 230 : loss : 0.168552, loss_ce: 0.066248
2022-01-07 23:30:25,813 iteration 231 : loss : 0.186822, loss_ce: 0.073129
2022-01-07 23:30:27,192 iteration 232 : loss : 0.195335, loss_ce: 0.078885
2022-01-07 23:30:28,509 iteration 233 : loss : 0.179941, loss_ce: 0.084076
2022-01-07 23:30:29,880 iteration 234 : loss : 0.228491, loss_ce: 0.094599
2022-01-07 23:30:31,225 iteration 235 : loss : 0.207852, loss_ce: 0.088679
2022-01-07 23:30:32,628 iteration 236 : loss : 0.328605, loss_ce: 0.145140
2022-01-07 23:30:33,960 iteration 237 : loss : 0.267713, loss_ce: 0.137116
2022-01-07 23:30:35,382 iteration 238 : loss : 0.223749, loss_ce: 0.074121
  4%|█                             | 14/400 [05:59<2:40:36, 24.96s/it]2022-01-07 23:30:36,805 iteration 239 : loss : 0.213372, loss_ce: 0.079884
2022-01-07 23:30:38,314 iteration 240 : loss : 0.196676, loss_ce: 0.067659
2022-01-07 23:30:39,742 iteration 241 : loss : 0.144218, loss_ce: 0.062584
2022-01-07 23:30:41,115 iteration 242 : loss : 0.195261, loss_ce: 0.071992
2022-01-07 23:30:42,482 iteration 243 : loss : 0.188394, loss_ce: 0.072861
2022-01-07 23:30:43,873 iteration 244 : loss : 0.207126, loss_ce: 0.081057
2022-01-07 23:30:45,226 iteration 245 : loss : 0.181223, loss_ce: 0.057446
2022-01-07 23:30:46,592 iteration 246 : loss : 0.236046, loss_ce: 0.082440
2022-01-07 23:30:47,998 iteration 247 : loss : 0.214148, loss_ce: 0.093688
2022-01-07 23:30:49,419 iteration 248 : loss : 0.146010, loss_ce: 0.067884
2022-01-07 23:30:50,814 iteration 249 : loss : 0.185327, loss_ce: 0.078417
2022-01-07 23:30:52,193 iteration 250 : loss : 0.235295, loss_ce: 0.087185
2022-01-07 23:30:53,522 iteration 251 : loss : 0.153920, loss_ce: 0.072370
2022-01-07 23:30:54,982 iteration 252 : loss : 0.204293, loss_ce: 0.099046
2022-01-07 23:30:56,403 iteration 253 : loss : 0.165207, loss_ce: 0.063601
2022-01-07 23:30:57,789 iteration 254 : loss : 0.162259, loss_ce: 0.073610
2022-01-07 23:30:57,790 Training Data Eval:
2022-01-07 23:31:04,675   Average segmentation loss on training set: 0.1859
2022-01-07 23:31:04,675 Validation Data Eval:
2022-01-07 23:31:07,046   Average segmentation loss on validation set: 0.2039
2022-01-07 23:31:11,336 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:31:12,563 iteration 255 : loss : 0.219093, loss_ce: 0.101413
  4%|█▏                            | 15/400 [06:36<3:03:48, 28.64s/it]2022-01-07 23:31:13,838 iteration 256 : loss : 0.161583, loss_ce: 0.062709
2022-01-07 23:31:15,166 iteration 257 : loss : 0.204580, loss_ce: 0.079612
2022-01-07 23:31:16,599 iteration 258 : loss : 0.189494, loss_ce: 0.087598
2022-01-07 23:31:17,990 iteration 259 : loss : 0.229217, loss_ce: 0.100155
2022-01-07 23:31:19,333 iteration 260 : loss : 0.143912, loss_ce: 0.076773
2022-01-07 23:31:20,775 iteration 261 : loss : 0.193144, loss_ce: 0.076395
2022-01-07 23:31:22,211 iteration 262 : loss : 0.297181, loss_ce: 0.142729
2022-01-07 23:31:23,551 iteration 263 : loss : 0.150717, loss_ce: 0.062090
2022-01-07 23:31:24,984 iteration 264 : loss : 0.233543, loss_ce: 0.078881
2022-01-07 23:31:26,511 iteration 265 : loss : 0.248833, loss_ce: 0.135748
2022-01-07 23:31:27,926 iteration 266 : loss : 0.199519, loss_ce: 0.084712
2022-01-07 23:31:29,404 iteration 267 : loss : 0.125321, loss_ce: 0.044958
2022-01-07 23:31:30,815 iteration 268 : loss : 0.220352, loss_ce: 0.091096
2022-01-07 23:31:32,247 iteration 269 : loss : 0.256849, loss_ce: 0.111004
2022-01-07 23:31:33,622 iteration 270 : loss : 0.219132, loss_ce: 0.081419
2022-01-07 23:31:34,985 iteration 271 : loss : 0.206974, loss_ce: 0.091476
2022-01-07 23:31:36,326 iteration 272 : loss : 0.203842, loss_ce: 0.084256
  4%|█▏                            | 16/400 [07:00<2:53:56, 27.18s/it]2022-01-07 23:31:37,833 iteration 273 : loss : 0.218598, loss_ce: 0.071681
2022-01-07 23:31:39,228 iteration 274 : loss : 0.330980, loss_ce: 0.145358
2022-01-07 23:31:40,599 iteration 275 : loss : 0.286711, loss_ce: 0.083532
2022-01-07 23:31:41,963 iteration 276 : loss : 0.162763, loss_ce: 0.065543
2022-01-07 23:31:43,325 iteration 277 : loss : 0.171405, loss_ce: 0.065587
2022-01-07 23:31:44,739 iteration 278 : loss : 0.216686, loss_ce: 0.097618
2022-01-07 23:31:46,123 iteration 279 : loss : 0.217162, loss_ce: 0.087029
2022-01-07 23:31:47,507 iteration 280 : loss : 0.220811, loss_ce: 0.106697
2022-01-07 23:31:48,941 iteration 281 : loss : 0.182249, loss_ce: 0.089801
2022-01-07 23:31:50,374 iteration 282 : loss : 0.152799, loss_ce: 0.059858
2022-01-07 23:31:51,737 iteration 283 : loss : 0.208739, loss_ce: 0.082833
2022-01-07 23:31:53,143 iteration 284 : loss : 0.195168, loss_ce: 0.109097
2022-01-07 23:31:54,572 iteration 285 : loss : 0.199560, loss_ce: 0.068341
2022-01-07 23:31:55,882 iteration 286 : loss : 0.179375, loss_ce: 0.071622
2022-01-07 23:31:57,283 iteration 287 : loss : 0.178038, loss_ce: 0.077453
2022-01-07 23:31:58,684 iteration 288 : loss : 0.178208, loss_ce: 0.068070
2022-01-07 23:32:00,053 iteration 289 : loss : 0.236245, loss_ce: 0.090756
  4%|█▎                            | 17/400 [07:24<2:46:50, 26.14s/it]2022-01-07 23:32:01,516 iteration 290 : loss : 0.203050, loss_ce: 0.082043
2022-01-07 23:32:02,881 iteration 291 : loss : 0.205841, loss_ce: 0.074480
2022-01-07 23:32:04,219 iteration 292 : loss : 0.193614, loss_ce: 0.086127
2022-01-07 23:32:05,653 iteration 293 : loss : 0.188974, loss_ce: 0.072201
2022-01-07 23:32:07,026 iteration 294 : loss : 0.142137, loss_ce: 0.069891
2022-01-07 23:32:08,489 iteration 295 : loss : 0.160863, loss_ce: 0.076494
2022-01-07 23:32:09,958 iteration 296 : loss : 0.219386, loss_ce: 0.095251
2022-01-07 23:32:11,370 iteration 297 : loss : 0.162933, loss_ce: 0.079225
2022-01-07 23:32:12,745 iteration 298 : loss : 0.168736, loss_ce: 0.087052
2022-01-07 23:32:14,087 iteration 299 : loss : 0.188918, loss_ce: 0.074560
2022-01-07 23:32:15,455 iteration 300 : loss : 0.162007, loss_ce: 0.064578
2022-01-07 23:32:16,755 iteration 301 : loss : 0.217878, loss_ce: 0.074511
2022-01-07 23:32:18,199 iteration 302 : loss : 0.185555, loss_ce: 0.071545
2022-01-07 23:32:19,630 iteration 303 : loss : 0.128916, loss_ce: 0.041229
2022-01-07 23:32:21,091 iteration 304 : loss : 0.234830, loss_ce: 0.102271
2022-01-07 23:32:22,453 iteration 305 : loss : 0.189170, loss_ce: 0.081132
2022-01-07 23:32:23,829 iteration 306 : loss : 0.178392, loss_ce: 0.070595
  4%|█▎                            | 18/400 [07:48<2:41:54, 25.43s/it]2022-01-07 23:32:25,193 iteration 307 : loss : 0.246899, loss_ce: 0.093869
2022-01-07 23:32:26,518 iteration 308 : loss : 0.162913, loss_ce: 0.065678
2022-01-07 23:32:27,847 iteration 309 : loss : 0.184985, loss_ce: 0.087771
2022-01-07 23:32:29,239 iteration 310 : loss : 0.210278, loss_ce: 0.095811
2022-01-07 23:32:30,654 iteration 311 : loss : 0.200572, loss_ce: 0.087327
2022-01-07 23:32:32,102 iteration 312 : loss : 0.152241, loss_ce: 0.062931
2022-01-07 23:32:33,462 iteration 313 : loss : 0.202806, loss_ce: 0.089232
2022-01-07 23:32:34,858 iteration 314 : loss : 0.164227, loss_ce: 0.076111
2022-01-07 23:32:36,296 iteration 315 : loss : 0.204610, loss_ce: 0.087197
2022-01-07 23:32:37,692 iteration 316 : loss : 0.229698, loss_ce: 0.093506
2022-01-07 23:32:39,046 iteration 317 : loss : 0.179173, loss_ce: 0.062350
2022-01-07 23:32:40,401 iteration 318 : loss : 0.183930, loss_ce: 0.072015
2022-01-07 23:32:41,843 iteration 319 : loss : 0.126757, loss_ce: 0.045600
2022-01-07 23:32:43,202 iteration 320 : loss : 0.213780, loss_ce: 0.081591
2022-01-07 23:32:44,547 iteration 321 : loss : 0.176495, loss_ce: 0.079635
2022-01-07 23:32:45,939 iteration 322 : loss : 0.225708, loss_ce: 0.101621
2022-01-07 23:32:47,373 iteration 323 : loss : 0.162359, loss_ce: 0.066873
  5%|█▍                            | 19/400 [08:11<2:37:53, 24.86s/it]2022-01-07 23:32:48,730 iteration 324 : loss : 0.207121, loss_ce: 0.081412
2022-01-07 23:32:50,105 iteration 325 : loss : 0.164456, loss_ce: 0.058436
2022-01-07 23:32:51,498 iteration 326 : loss : 0.182093, loss_ce: 0.064530
2022-01-07 23:32:52,958 iteration 327 : loss : 0.205080, loss_ce: 0.070914
2022-01-07 23:32:54,387 iteration 328 : loss : 0.194868, loss_ce: 0.079184
2022-01-07 23:32:55,783 iteration 329 : loss : 0.160798, loss_ce: 0.063462
2022-01-07 23:32:57,096 iteration 330 : loss : 0.166904, loss_ce: 0.074646
2022-01-07 23:32:58,532 iteration 331 : loss : 0.123871, loss_ce: 0.048079
2022-01-07 23:32:59,972 iteration 332 : loss : 0.130895, loss_ce: 0.055703
2022-01-07 23:33:01,309 iteration 333 : loss : 0.205074, loss_ce: 0.083753
2022-01-07 23:33:02,763 iteration 334 : loss : 0.175615, loss_ce: 0.082668
2022-01-07 23:33:04,136 iteration 335 : loss : 0.216517, loss_ce: 0.086728
2022-01-07 23:33:05,565 iteration 336 : loss : 0.154920, loss_ce: 0.065467
2022-01-07 23:33:06,922 iteration 337 : loss : 0.171942, loss_ce: 0.071017
2022-01-07 23:33:08,239 iteration 338 : loss : 0.183323, loss_ce: 0.086781
2022-01-07 23:33:09,642 iteration 339 : loss : 0.156707, loss_ce: 0.056981
2022-01-07 23:33:09,643 Training Data Eval:
2022-01-07 23:33:16,534   Average segmentation loss on training set: 0.2653
2022-01-07 23:33:16,534 Validation Data Eval:
2022-01-07 23:33:18,912   Average segmentation loss on validation set: 0.2345
2022-01-07 23:33:20,272 iteration 340 : loss : 0.161624, loss_ce: 0.062212
  5%|█▌                            | 20/400 [08:44<2:52:44, 27.27s/it]2022-01-07 23:33:21,771 iteration 341 : loss : 0.224339, loss_ce: 0.103941
2022-01-07 23:33:23,221 iteration 342 : loss : 0.142309, loss_ce: 0.059873
2022-01-07 23:33:24,731 iteration 343 : loss : 0.220113, loss_ce: 0.125145
2022-01-07 23:33:26,116 iteration 344 : loss : 0.230159, loss_ce: 0.089474
2022-01-07 23:33:27,554 iteration 345 : loss : 0.266260, loss_ce: 0.115152
2022-01-07 23:33:29,016 iteration 346 : loss : 0.162591, loss_ce: 0.059355
2022-01-07 23:33:30,379 iteration 347 : loss : 0.177225, loss_ce: 0.079624
2022-01-07 23:33:31,868 iteration 348 : loss : 0.209427, loss_ce: 0.088962
2022-01-07 23:33:33,275 iteration 349 : loss : 0.204211, loss_ce: 0.096870
2022-01-07 23:33:34,582 iteration 350 : loss : 0.165171, loss_ce: 0.063158
2022-01-07 23:33:36,030 iteration 351 : loss : 0.171475, loss_ce: 0.068158
2022-01-07 23:33:37,418 iteration 352 : loss : 0.171615, loss_ce: 0.059287
2022-01-07 23:33:38,920 iteration 353 : loss : 0.132415, loss_ce: 0.050108
2022-01-07 23:33:40,345 iteration 354 : loss : 0.209457, loss_ce: 0.087623
2022-01-07 23:33:41,804 iteration 355 : loss : 0.187158, loss_ce: 0.071479
2022-01-07 23:33:43,154 iteration 356 : loss : 0.184059, loss_ce: 0.074796
2022-01-07 23:33:44,522 iteration 357 : loss : 0.174918, loss_ce: 0.070850
  5%|█▌                            | 21/400 [09:08<2:46:33, 26.37s/it]2022-01-07 23:33:46,041 iteration 358 : loss : 0.185672, loss_ce: 0.069858
2022-01-07 23:33:47,489 iteration 359 : loss : 0.141097, loss_ce: 0.066018
2022-01-07 23:33:48,886 iteration 360 : loss : 0.223417, loss_ce: 0.064982
2022-01-07 23:33:50,314 iteration 361 : loss : 0.221512, loss_ce: 0.084565
2022-01-07 23:33:51,638 iteration 362 : loss : 0.140633, loss_ce: 0.058547
2022-01-07 23:33:53,127 iteration 363 : loss : 0.210614, loss_ce: 0.107351
2022-01-07 23:33:54,534 iteration 364 : loss : 0.211431, loss_ce: 0.112831
2022-01-07 23:33:55,911 iteration 365 : loss : 0.170463, loss_ce: 0.066610
2022-01-07 23:33:57,314 iteration 366 : loss : 0.163745, loss_ce: 0.070870
2022-01-07 23:33:58,770 iteration 367 : loss : 0.197280, loss_ce: 0.080668
2022-01-07 23:34:00,109 iteration 368 : loss : 0.208097, loss_ce: 0.075398
2022-01-07 23:34:01,529 iteration 369 : loss : 0.210447, loss_ce: 0.095554
2022-01-07 23:34:02,843 iteration 370 : loss : 0.228996, loss_ce: 0.066437
2022-01-07 23:34:04,322 iteration 371 : loss : 0.190981, loss_ce: 0.099735
2022-01-07 23:34:05,774 iteration 372 : loss : 0.212210, loss_ce: 0.088196
2022-01-07 23:34:07,188 iteration 373 : loss : 0.198741, loss_ce: 0.072947
2022-01-07 23:34:08,554 iteration 374 : loss : 0.225030, loss_ce: 0.096434
  6%|█▋                            | 22/400 [09:32<2:41:41, 25.67s/it]2022-01-07 23:34:10,058 iteration 375 : loss : 0.157652, loss_ce: 0.064029
2022-01-07 23:34:11,484 iteration 376 : loss : 0.211643, loss_ce: 0.089542
2022-01-07 23:34:12,920 iteration 377 : loss : 0.202655, loss_ce: 0.065014
2022-01-07 23:34:14,338 iteration 378 : loss : 0.204100, loss_ce: 0.088387
2022-01-07 23:34:15,696 iteration 379 : loss : 0.210723, loss_ce: 0.080672
2022-01-07 23:34:17,120 iteration 380 : loss : 0.207035, loss_ce: 0.095780
2022-01-07 23:34:18,421 iteration 381 : loss : 0.144392, loss_ce: 0.054743
2022-01-07 23:34:19,919 iteration 382 : loss : 0.183676, loss_ce: 0.084466
2022-01-07 23:34:21,389 iteration 383 : loss : 0.188602, loss_ce: 0.080980
2022-01-07 23:34:22,788 iteration 384 : loss : 0.176444, loss_ce: 0.079208
2022-01-07 23:34:24,148 iteration 385 : loss : 0.166196, loss_ce: 0.072932
2022-01-07 23:34:25,580 iteration 386 : loss : 0.231571, loss_ce: 0.095438
2022-01-07 23:34:27,000 iteration 387 : loss : 0.214312, loss_ce: 0.089656
2022-01-07 23:34:28,470 iteration 388 : loss : 0.162492, loss_ce: 0.070697
2022-01-07 23:34:29,843 iteration 389 : loss : 0.225423, loss_ce: 0.086636
2022-01-07 23:34:31,274 iteration 390 : loss : 0.186800, loss_ce: 0.092494
2022-01-07 23:34:32,641 iteration 391 : loss : 0.183729, loss_ce: 0.066628
  6%|█▋                            | 23/400 [09:56<2:38:17, 25.19s/it]2022-01-07 23:34:34,233 iteration 392 : loss : 0.163423, loss_ce: 0.067193
2022-01-07 23:34:35,592 iteration 393 : loss : 0.220953, loss_ce: 0.111555
2022-01-07 23:34:37,008 iteration 394 : loss : 0.253136, loss_ce: 0.094756
2022-01-07 23:34:38,385 iteration 395 : loss : 0.215301, loss_ce: 0.093419
2022-01-07 23:34:39,859 iteration 396 : loss : 0.183909, loss_ce: 0.069407
2022-01-07 23:34:41,292 iteration 397 : loss : 0.208564, loss_ce: 0.086501
2022-01-07 23:34:42,710 iteration 398 : loss : 0.189286, loss_ce: 0.088767
2022-01-07 23:34:44,099 iteration 399 : loss : 0.141419, loss_ce: 0.053960
2022-01-07 23:34:45,576 iteration 400 : loss : 0.154622, loss_ce: 0.072792
2022-01-07 23:34:46,902 iteration 401 : loss : 0.206355, loss_ce: 0.075095
2022-01-07 23:34:48,347 iteration 402 : loss : 0.243643, loss_ce: 0.104922
2022-01-07 23:34:49,826 iteration 403 : loss : 0.190887, loss_ce: 0.069673
2022-01-07 23:34:51,243 iteration 404 : loss : 0.153846, loss_ce: 0.044603
2022-01-07 23:34:52,692 iteration 405 : loss : 0.241186, loss_ce: 0.090614
2022-01-07 23:34:54,054 iteration 406 : loss : 0.227003, loss_ce: 0.098326
2022-01-07 23:34:55,415 iteration 407 : loss : 0.216732, loss_ce: 0.076663
2022-01-07 23:34:56,770 iteration 408 : loss : 0.160919, loss_ce: 0.056556
  6%|█▊                            | 24/400 [10:20<2:35:51, 24.87s/it]2022-01-07 23:34:58,347 iteration 409 : loss : 0.127776, loss_ce: 0.047335
2022-01-07 23:34:59,739 iteration 410 : loss : 0.195659, loss_ce: 0.078163
2022-01-07 23:35:01,158 iteration 411 : loss : 0.182035, loss_ce: 0.077184
2022-01-07 23:35:02,544 iteration 412 : loss : 0.184480, loss_ce: 0.071595
2022-01-07 23:35:03,947 iteration 413 : loss : 0.172944, loss_ce: 0.068305
2022-01-07 23:35:05,356 iteration 414 : loss : 0.162755, loss_ce: 0.074586
2022-01-07 23:35:06,801 iteration 415 : loss : 0.202280, loss_ce: 0.111722
2022-01-07 23:35:08,078 iteration 416 : loss : 0.209824, loss_ce: 0.074580
2022-01-07 23:35:09,484 iteration 417 : loss : 0.235047, loss_ce: 0.102237
2022-01-07 23:35:10,893 iteration 418 : loss : 0.237410, loss_ce: 0.090401
2022-01-07 23:35:12,264 iteration 419 : loss : 0.276953, loss_ce: 0.113146
2022-01-07 23:35:13,664 iteration 420 : loss : 0.137794, loss_ce: 0.061335
2022-01-07 23:35:14,969 iteration 421 : loss : 0.169383, loss_ce: 0.052963
2022-01-07 23:35:16,422 iteration 422 : loss : 0.199812, loss_ce: 0.079504
2022-01-07 23:35:17,790 iteration 423 : loss : 0.170832, loss_ce: 0.067839
2022-01-07 23:35:19,152 iteration 424 : loss : 0.165416, loss_ce: 0.086267
2022-01-07 23:35:19,152 Training Data Eval:
2022-01-07 23:35:26,061   Average segmentation loss on training set: 0.2264
2022-01-07 23:35:26,061 Validation Data Eval:
2022-01-07 23:35:28,438   Average segmentation loss on validation set: 0.2265
2022-01-07 23:35:29,877 iteration 425 : loss : 0.268425, loss_ce: 0.127715
  6%|█▉                            | 25/400 [10:54<2:50:53, 27.34s/it]2022-01-07 23:35:31,351 iteration 426 : loss : 0.200137, loss_ce: 0.085794
2022-01-07 23:35:32,705 iteration 427 : loss : 0.161170, loss_ce: 0.069417
2022-01-07 23:35:34,235 iteration 428 : loss : 0.161163, loss_ce: 0.074194
2022-01-07 23:35:35,608 iteration 429 : loss : 0.140051, loss_ce: 0.046817
2022-01-07 23:35:36,923 iteration 430 : loss : 0.186041, loss_ce: 0.081808
2022-01-07 23:35:38,350 iteration 431 : loss : 0.159359, loss_ce: 0.072161
2022-01-07 23:35:39,787 iteration 432 : loss : 0.170905, loss_ce: 0.057512
2022-01-07 23:35:41,118 iteration 433 : loss : 0.179009, loss_ce: 0.069500
2022-01-07 23:35:42,460 iteration 434 : loss : 0.151656, loss_ce: 0.056284
2022-01-07 23:35:43,865 iteration 435 : loss : 0.148723, loss_ce: 0.059667
2022-01-07 23:35:45,293 iteration 436 : loss : 0.204273, loss_ce: 0.060133
2022-01-07 23:35:46,668 iteration 437 : loss : 0.167980, loss_ce: 0.065370
2022-01-07 23:35:48,022 iteration 438 : loss : 0.159186, loss_ce: 0.064453
2022-01-07 23:35:49,388 iteration 439 : loss : 0.181056, loss_ce: 0.078713
2022-01-07 23:35:50,906 iteration 440 : loss : 0.195952, loss_ce: 0.080209
2022-01-07 23:35:52,304 iteration 441 : loss : 0.143960, loss_ce: 0.066725
2022-01-07 23:35:53,662 iteration 442 : loss : 0.157963, loss_ce: 0.059228
  6%|█▉                            | 26/400 [11:17<2:43:47, 26.28s/it]2022-01-07 23:35:55,125 iteration 443 : loss : 0.199169, loss_ce: 0.091915
2022-01-07 23:35:56,475 iteration 444 : loss : 0.163231, loss_ce: 0.061975
2022-01-07 23:35:57,823 iteration 445 : loss : 0.195379, loss_ce: 0.081157
2022-01-07 23:35:59,187 iteration 446 : loss : 0.183065, loss_ce: 0.067496
2022-01-07 23:36:00,614 iteration 447 : loss : 0.162337, loss_ce: 0.060953
2022-01-07 23:36:01,921 iteration 448 : loss : 0.270160, loss_ce: 0.097255
2022-01-07 23:36:03,275 iteration 449 : loss : 0.183690, loss_ce: 0.052814
2022-01-07 23:36:04,595 iteration 450 : loss : 0.172005, loss_ce: 0.073790
2022-01-07 23:36:05,964 iteration 451 : loss : 0.202029, loss_ce: 0.093613
2022-01-07 23:36:07,352 iteration 452 : loss : 0.221065, loss_ce: 0.075674
2022-01-07 23:36:08,706 iteration 453 : loss : 0.110420, loss_ce: 0.039598
2022-01-07 23:36:10,220 iteration 454 : loss : 0.178343, loss_ce: 0.075612
2022-01-07 23:36:11,643 iteration 455 : loss : 0.172056, loss_ce: 0.083818
2022-01-07 23:36:13,022 iteration 456 : loss : 0.176956, loss_ce: 0.065601
2022-01-07 23:36:14,395 iteration 457 : loss : 0.163868, loss_ce: 0.083448
2022-01-07 23:36:15,779 iteration 458 : loss : 0.177595, loss_ce: 0.088111
2022-01-07 23:36:17,173 iteration 459 : loss : 0.181234, loss_ce: 0.084099
  7%|██                            | 27/400 [11:41<2:38:11, 25.45s/it]2022-01-07 23:36:18,541 iteration 460 : loss : 0.173694, loss_ce: 0.087630
2022-01-07 23:36:19,898 iteration 461 : loss : 0.130020, loss_ce: 0.054856
2022-01-07 23:36:21,356 iteration 462 : loss : 0.194529, loss_ce: 0.075793
2022-01-07 23:36:22,776 iteration 463 : loss : 0.194955, loss_ce: 0.070510
2022-01-07 23:36:24,302 iteration 464 : loss : 0.235505, loss_ce: 0.089822
2022-01-07 23:36:25,739 iteration 465 : loss : 0.203305, loss_ce: 0.085732
2022-01-07 23:36:27,104 iteration 466 : loss : 0.155200, loss_ce: 0.062959
2022-01-07 23:36:28,503 iteration 467 : loss : 0.173215, loss_ce: 0.069665
2022-01-07 23:36:29,865 iteration 468 : loss : 0.121938, loss_ce: 0.053977
2022-01-07 23:36:31,293 iteration 469 : loss : 0.148567, loss_ce: 0.068106
2022-01-07 23:36:32,687 iteration 470 : loss : 0.167274, loss_ce: 0.068514
2022-01-07 23:36:34,102 iteration 471 : loss : 0.311105, loss_ce: 0.179784
2022-01-07 23:36:35,523 iteration 472 : loss : 0.111803, loss_ce: 0.048867
2022-01-07 23:36:36,920 iteration 473 : loss : 0.188713, loss_ce: 0.084363
2022-01-07 23:36:38,270 iteration 474 : loss : 0.254133, loss_ce: 0.106539
2022-01-07 23:36:39,670 iteration 475 : loss : 0.211418, loss_ce: 0.079946
2022-01-07 23:36:41,158 iteration 476 : loss : 0.164119, loss_ce: 0.073156
  7%|██                            | 28/400 [12:05<2:35:02, 25.01s/it]2022-01-07 23:36:42,587 iteration 477 : loss : 0.123516, loss_ce: 0.055286
2022-01-07 23:36:43,915 iteration 478 : loss : 0.132459, loss_ce: 0.060326
2022-01-07 23:36:45,340 iteration 479 : loss : 0.152015, loss_ce: 0.052742
2022-01-07 23:36:46,752 iteration 480 : loss : 0.218548, loss_ce: 0.065959
2022-01-07 23:36:48,100 iteration 481 : loss : 0.102737, loss_ce: 0.031007
2022-01-07 23:36:49,563 iteration 482 : loss : 0.154783, loss_ce: 0.053106
2022-01-07 23:36:50,896 iteration 483 : loss : 0.229453, loss_ce: 0.104949
2022-01-07 23:36:52,324 iteration 484 : loss : 0.191648, loss_ce: 0.062386
2022-01-07 23:36:53,726 iteration 485 : loss : 0.181479, loss_ce: 0.082720
2022-01-07 23:36:55,218 iteration 486 : loss : 0.152530, loss_ce: 0.062685
2022-01-07 23:36:56,610 iteration 487 : loss : 0.186242, loss_ce: 0.062335
2022-01-07 23:36:57,939 iteration 488 : loss : 0.149971, loss_ce: 0.072238
2022-01-07 23:36:59,376 iteration 489 : loss : 0.178822, loss_ce: 0.084103
2022-01-07 23:37:00,760 iteration 490 : loss : 0.174404, loss_ce: 0.083488
2022-01-07 23:37:02,206 iteration 491 : loss : 0.186428, loss_ce: 0.061996
2022-01-07 23:37:03,566 iteration 492 : loss : 0.182439, loss_ce: 0.110782
2022-01-07 23:37:04,931 iteration 493 : loss : 0.176724, loss_ce: 0.080774
  7%|██▏                           | 29/400 [12:29<2:32:21, 24.64s/it]2022-01-07 23:37:06,352 iteration 494 : loss : 0.210297, loss_ce: 0.094466
2022-01-07 23:37:07,677 iteration 495 : loss : 0.159870, loss_ce: 0.053993
2022-01-07 23:37:09,074 iteration 496 : loss : 0.182711, loss_ce: 0.066442
2022-01-07 23:37:10,576 iteration 497 : loss : 0.206269, loss_ce: 0.085845
2022-01-07 23:37:12,068 iteration 498 : loss : 0.184006, loss_ce: 0.098436
2022-01-07 23:37:13,491 iteration 499 : loss : 0.176343, loss_ce: 0.061819
2022-01-07 23:37:15,013 iteration 500 : loss : 0.196775, loss_ce: 0.071238
2022-01-07 23:37:16,434 iteration 501 : loss : 0.180110, loss_ce: 0.066670
2022-01-07 23:37:17,776 iteration 502 : loss : 0.163629, loss_ce: 0.072108
2022-01-07 23:37:19,268 iteration 503 : loss : 0.160189, loss_ce: 0.050200
2022-01-07 23:37:20,775 iteration 504 : loss : 0.148768, loss_ce: 0.054667
2022-01-07 23:37:22,164 iteration 505 : loss : 0.125028, loss_ce: 0.050229
2022-01-07 23:37:23,588 iteration 506 : loss : 0.163287, loss_ce: 0.068314
2022-01-07 23:37:25,037 iteration 507 : loss : 0.129585, loss_ce: 0.052749
2022-01-07 23:37:26,428 iteration 508 : loss : 0.156109, loss_ce: 0.064457
2022-01-07 23:37:27,866 iteration 509 : loss : 0.138114, loss_ce: 0.058885
2022-01-07 23:37:27,866 Training Data Eval:
2022-01-07 23:37:34,765   Average segmentation loss on training set: 0.1705
2022-01-07 23:37:34,765 Validation Data Eval:
2022-01-07 23:37:37,133   Average segmentation loss on validation set: 0.1825
2022-01-07 23:37:41,280 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:37:42,478 iteration 510 : loss : 0.157484, loss_ce: 0.058515
  8%|██▎                           | 30/400 [13:06<2:55:48, 28.51s/it]2022-01-07 23:37:43,666 iteration 511 : loss : 0.130210, loss_ce: 0.056563
2022-01-07 23:37:45,161 iteration 512 : loss : 0.193275, loss_ce: 0.077531
2022-01-07 23:37:46,515 iteration 513 : loss : 0.181508, loss_ce: 0.089794
2022-01-07 23:37:47,933 iteration 514 : loss : 0.168125, loss_ce: 0.063164
2022-01-07 23:37:49,292 iteration 515 : loss : 0.127539, loss_ce: 0.051895
2022-01-07 23:37:50,598 iteration 516 : loss : 0.112799, loss_ce: 0.051504
2022-01-07 23:37:52,164 iteration 517 : loss : 0.129685, loss_ce: 0.048683
2022-01-07 23:37:53,591 iteration 518 : loss : 0.198301, loss_ce: 0.103785
2022-01-07 23:37:54,943 iteration 519 : loss : 0.125215, loss_ce: 0.050271
2022-01-07 23:37:56,292 iteration 520 : loss : 0.163262, loss_ce: 0.080285
2022-01-07 23:37:57,770 iteration 521 : loss : 0.218581, loss_ce: 0.062622
2022-01-07 23:37:59,123 iteration 522 : loss : 0.144183, loss_ce: 0.060097
2022-01-07 23:38:00,539 iteration 523 : loss : 0.171955, loss_ce: 0.074287
2022-01-07 23:38:01,814 iteration 524 : loss : 0.135323, loss_ce: 0.062464
2022-01-07 23:38:03,190 iteration 525 : loss : 0.152243, loss_ce: 0.068971
2022-01-07 23:38:04,625 iteration 526 : loss : 0.133313, loss_ce: 0.054063
2022-01-07 23:38:06,002 iteration 527 : loss : 0.194686, loss_ce: 0.087609
  8%|██▎                           | 31/400 [13:30<2:46:09, 27.02s/it]2022-01-07 23:38:07,472 iteration 528 : loss : 0.093984, loss_ce: 0.038479
2022-01-07 23:38:08,860 iteration 529 : loss : 0.127572, loss_ce: 0.063677
2022-01-07 23:38:10,238 iteration 530 : loss : 0.093433, loss_ce: 0.036780
2022-01-07 23:38:11,701 iteration 531 : loss : 0.204529, loss_ce: 0.081336
2022-01-07 23:38:13,017 iteration 532 : loss : 0.209750, loss_ce: 0.061495
2022-01-07 23:38:14,426 iteration 533 : loss : 0.119132, loss_ce: 0.050922
2022-01-07 23:38:15,825 iteration 534 : loss : 0.162650, loss_ce: 0.085862
2022-01-07 23:38:17,246 iteration 535 : loss : 0.170220, loss_ce: 0.069343
2022-01-07 23:38:18,565 iteration 536 : loss : 0.159337, loss_ce: 0.076419
2022-01-07 23:38:19,944 iteration 537 : loss : 0.192996, loss_ce: 0.077152
2022-01-07 23:38:21,312 iteration 538 : loss : 0.116171, loss_ce: 0.043573
2022-01-07 23:38:22,675 iteration 539 : loss : 0.178924, loss_ce: 0.067885
2022-01-07 23:38:24,004 iteration 540 : loss : 0.152945, loss_ce: 0.069500
2022-01-07 23:38:25,372 iteration 541 : loss : 0.162124, loss_ce: 0.084309
2022-01-07 23:38:26,688 iteration 542 : loss : 0.124750, loss_ce: 0.045726
2022-01-07 23:38:28,023 iteration 543 : loss : 0.184916, loss_ce: 0.061657
2022-01-07 23:38:29,407 iteration 544 : loss : 0.138776, loss_ce: 0.052114
  8%|██▍                           | 32/400 [13:53<2:39:03, 25.93s/it]2022-01-07 23:38:30,812 iteration 545 : loss : 0.196850, loss_ce: 0.081204
2022-01-07 23:38:32,206 iteration 546 : loss : 0.143514, loss_ce: 0.055202
2022-01-07 23:38:33,588 iteration 547 : loss : 0.186466, loss_ce: 0.088073
2022-01-07 23:38:34,919 iteration 548 : loss : 0.178439, loss_ce: 0.066381
2022-01-07 23:38:36,308 iteration 549 : loss : 0.144039, loss_ce: 0.060671
2022-01-07 23:38:37,788 iteration 550 : loss : 0.174631, loss_ce: 0.089091
2022-01-07 23:38:39,200 iteration 551 : loss : 0.178307, loss_ce: 0.072558
2022-01-07 23:38:40,659 iteration 552 : loss : 0.181483, loss_ce: 0.086914
2022-01-07 23:38:42,128 iteration 553 : loss : 0.129309, loss_ce: 0.058764
2022-01-07 23:38:43,455 iteration 554 : loss : 0.142369, loss_ce: 0.059952
2022-01-07 23:38:44,862 iteration 555 : loss : 0.120631, loss_ce: 0.055188
2022-01-07 23:38:46,358 iteration 556 : loss : 0.120634, loss_ce: 0.047041
2022-01-07 23:38:47,759 iteration 557 : loss : 0.154822, loss_ce: 0.082050
2022-01-07 23:38:49,105 iteration 558 : loss : 0.156935, loss_ce: 0.055561
2022-01-07 23:38:50,512 iteration 559 : loss : 0.185055, loss_ce: 0.086166
2022-01-07 23:38:51,937 iteration 560 : loss : 0.239891, loss_ce: 0.099086
2022-01-07 23:38:53,297 iteration 561 : loss : 0.148354, loss_ce: 0.045694
  8%|██▍                           | 33/400 [14:17<2:34:52, 25.32s/it]2022-01-07 23:38:54,705 iteration 562 : loss : 0.153812, loss_ce: 0.057883
2022-01-07 23:38:56,110 iteration 563 : loss : 0.144560, loss_ce: 0.065105
2022-01-07 23:38:57,553 iteration 564 : loss : 0.102445, loss_ce: 0.052941
2022-01-07 23:38:58,988 iteration 565 : loss : 0.200380, loss_ce: 0.105308
2022-01-07 23:39:00,299 iteration 566 : loss : 0.113002, loss_ce: 0.043289
2022-01-07 23:39:01,730 iteration 567 : loss : 0.151821, loss_ce: 0.051087
2022-01-07 23:39:03,089 iteration 568 : loss : 0.142615, loss_ce: 0.058495
2022-01-07 23:39:04,480 iteration 569 : loss : 0.136462, loss_ce: 0.047944
2022-01-07 23:39:05,951 iteration 570 : loss : 0.151061, loss_ce: 0.067399
2022-01-07 23:39:07,390 iteration 571 : loss : 0.139385, loss_ce: 0.058960
2022-01-07 23:39:08,816 iteration 572 : loss : 0.126938, loss_ce: 0.048733
2022-01-07 23:39:10,186 iteration 573 : loss : 0.122156, loss_ce: 0.046657
2022-01-07 23:39:11,587 iteration 574 : loss : 0.122390, loss_ce: 0.055796
2022-01-07 23:39:13,018 iteration 575 : loss : 0.156794, loss_ce: 0.064460
2022-01-07 23:39:14,412 iteration 576 : loss : 0.156116, loss_ce: 0.072217
2022-01-07 23:39:15,829 iteration 577 : loss : 0.207325, loss_ce: 0.070308
2022-01-07 23:39:17,267 iteration 578 : loss : 0.138191, loss_ce: 0.049268
  8%|██▌                           | 34/400 [14:41<2:31:58, 24.91s/it]2022-01-07 23:39:18,691 iteration 579 : loss : 0.138932, loss_ce: 0.044853
2022-01-07 23:39:20,073 iteration 580 : loss : 0.109183, loss_ce: 0.046897
2022-01-07 23:39:21,476 iteration 581 : loss : 0.113538, loss_ce: 0.048653
2022-01-07 23:39:22,909 iteration 582 : loss : 0.114282, loss_ce: 0.045997
2022-01-07 23:39:24,282 iteration 583 : loss : 0.182831, loss_ce: 0.097036
2022-01-07 23:39:25,568 iteration 584 : loss : 0.106932, loss_ce: 0.041789
2022-01-07 23:39:26,904 iteration 585 : loss : 0.107579, loss_ce: 0.042799
2022-01-07 23:39:28,324 iteration 586 : loss : 0.164893, loss_ce: 0.088755
2022-01-07 23:39:29,721 iteration 587 : loss : 0.096221, loss_ce: 0.037891
2022-01-07 23:39:31,141 iteration 588 : loss : 0.123443, loss_ce: 0.043437
2022-01-07 23:39:32,531 iteration 589 : loss : 0.109200, loss_ce: 0.045889
2022-01-07 23:39:33,884 iteration 590 : loss : 0.173189, loss_ce: 0.093898
2022-01-07 23:39:35,276 iteration 591 : loss : 0.105255, loss_ce: 0.043451
2022-01-07 23:39:36,643 iteration 592 : loss : 0.169440, loss_ce: 0.062365
2022-01-07 23:39:38,069 iteration 593 : loss : 0.142450, loss_ce: 0.059475
2022-01-07 23:39:39,514 iteration 594 : loss : 0.133433, loss_ce: 0.062752
2022-01-07 23:39:39,514 Training Data Eval:
2022-01-07 23:39:46,434   Average segmentation loss on training set: 0.1030
2022-01-07 23:39:46,434 Validation Data Eval:
2022-01-07 23:39:48,820   Average segmentation loss on validation set: 0.1193
2022-01-07 23:39:53,027 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:39:54,354 iteration 595 : loss : 0.157332, loss_ce: 0.059685
  9%|██▋                           | 35/400 [15:18<2:53:47, 28.57s/it]2022-01-07 23:39:55,630 iteration 596 : loss : 0.095057, loss_ce: 0.045201
2022-01-07 23:39:57,000 iteration 597 : loss : 0.138098, loss_ce: 0.062472
2022-01-07 23:39:58,408 iteration 598 : loss : 0.133074, loss_ce: 0.056162
2022-01-07 23:39:59,864 iteration 599 : loss : 0.097155, loss_ce: 0.040736
2022-01-07 23:40:01,169 iteration 600 : loss : 0.144761, loss_ce: 0.071719
2022-01-07 23:40:02,551 iteration 601 : loss : 0.129588, loss_ce: 0.056170
2022-01-07 23:40:03,977 iteration 602 : loss : 0.099755, loss_ce: 0.046518
2022-01-07 23:40:05,283 iteration 603 : loss : 0.142908, loss_ce: 0.061137
2022-01-07 23:40:06,650 iteration 604 : loss : 0.148692, loss_ce: 0.061891
2022-01-07 23:40:08,030 iteration 605 : loss : 0.121014, loss_ce: 0.050556
2022-01-07 23:40:09,377 iteration 606 : loss : 0.151170, loss_ce: 0.058774
2022-01-07 23:40:10,753 iteration 607 : loss : 0.129272, loss_ce: 0.050284
2022-01-07 23:40:12,242 iteration 608 : loss : 0.126772, loss_ce: 0.048623
2022-01-07 23:40:13,609 iteration 609 : loss : 0.111394, loss_ce: 0.045069
2022-01-07 23:40:15,044 iteration 610 : loss : 0.112193, loss_ce: 0.041658
2022-01-07 23:40:16,425 iteration 611 : loss : 0.183060, loss_ce: 0.072804
2022-01-07 23:40:17,832 iteration 612 : loss : 0.112705, loss_ce: 0.054488
  9%|██▋                           | 36/400 [15:42<2:44:01, 27.04s/it]2022-01-07 23:40:19,299 iteration 613 : loss : 0.115468, loss_ce: 0.052962
2022-01-07 23:40:20,791 iteration 614 : loss : 0.106071, loss_ce: 0.047559
2022-01-07 23:40:22,178 iteration 615 : loss : 0.098803, loss_ce: 0.037897
2022-01-07 23:40:23,534 iteration 616 : loss : 0.155544, loss_ce: 0.090781
2022-01-07 23:40:24,916 iteration 617 : loss : 0.165276, loss_ce: 0.077898
2022-01-07 23:40:26,251 iteration 618 : loss : 0.164945, loss_ce: 0.068888
2022-01-07 23:40:27,620 iteration 619 : loss : 0.138164, loss_ce: 0.047483
2022-01-07 23:40:29,112 iteration 620 : loss : 0.155644, loss_ce: 0.068977
2022-01-07 23:40:30,466 iteration 621 : loss : 0.123972, loss_ce: 0.048674
2022-01-07 23:40:31,869 iteration 622 : loss : 0.093714, loss_ce: 0.040409
2022-01-07 23:40:33,316 iteration 623 : loss : 0.143675, loss_ce: 0.049933
2022-01-07 23:40:34,688 iteration 624 : loss : 0.161205, loss_ce: 0.060056
2022-01-07 23:40:36,027 iteration 625 : loss : 0.191724, loss_ce: 0.076745
2022-01-07 23:40:37,353 iteration 626 : loss : 0.097350, loss_ce: 0.036425
2022-01-07 23:40:38,708 iteration 627 : loss : 0.078381, loss_ce: 0.026889
2022-01-07 23:40:40,071 iteration 628 : loss : 0.120075, loss_ce: 0.066253
2022-01-07 23:40:41,505 iteration 629 : loss : 0.099091, loss_ce: 0.038061
  9%|██▊                           | 37/400 [16:05<2:37:28, 26.03s/it]2022-01-07 23:40:42,914 iteration 630 : loss : 0.086181, loss_ce: 0.039104
2022-01-07 23:40:44,334 iteration 631 : loss : 0.143916, loss_ce: 0.070605
2022-01-07 23:40:45,733 iteration 632 : loss : 0.177917, loss_ce: 0.073980
2022-01-07 23:40:47,162 iteration 633 : loss : 0.112998, loss_ce: 0.044971
2022-01-07 23:40:48,607 iteration 634 : loss : 0.108887, loss_ce: 0.052777
2022-01-07 23:40:49,983 iteration 635 : loss : 0.200406, loss_ce: 0.090987
2022-01-07 23:40:51,443 iteration 636 : loss : 0.163843, loss_ce: 0.085156
2022-01-07 23:40:52,791 iteration 637 : loss : 0.143175, loss_ce: 0.052267
2022-01-07 23:40:54,166 iteration 638 : loss : 0.174636, loss_ce: 0.081025
2022-01-07 23:40:55,592 iteration 639 : loss : 0.134570, loss_ce: 0.057595
2022-01-07 23:40:56,960 iteration 640 : loss : 0.145423, loss_ce: 0.053020
2022-01-07 23:40:58,364 iteration 641 : loss : 0.138134, loss_ce: 0.054182
2022-01-07 23:40:59,793 iteration 642 : loss : 0.106606, loss_ce: 0.039176
2022-01-07 23:41:01,193 iteration 643 : loss : 0.132831, loss_ce: 0.053169
2022-01-07 23:41:02,576 iteration 644 : loss : 0.105123, loss_ce: 0.041752
2022-01-07 23:41:03,979 iteration 645 : loss : 0.197970, loss_ce: 0.091849
2022-01-07 23:41:05,375 iteration 646 : loss : 0.124565, loss_ce: 0.053310
 10%|██▊                           | 38/400 [16:29<2:33:08, 25.38s/it]2022-01-07 23:41:06,806 iteration 647 : loss : 0.141285, loss_ce: 0.044646
2022-01-07 23:41:08,153 iteration 648 : loss : 0.114585, loss_ce: 0.056251
2022-01-07 23:41:09,494 iteration 649 : loss : 0.103808, loss_ce: 0.050076
2022-01-07 23:41:10,922 iteration 650 : loss : 0.160690, loss_ce: 0.048922
2022-01-07 23:41:12,368 iteration 651 : loss : 0.189221, loss_ce: 0.084388
2022-01-07 23:41:13,706 iteration 652 : loss : 0.107318, loss_ce: 0.048431
2022-01-07 23:41:15,062 iteration 653 : loss : 0.107109, loss_ce: 0.049636
2022-01-07 23:41:16,452 iteration 654 : loss : 0.136807, loss_ce: 0.047772
2022-01-07 23:41:17,782 iteration 655 : loss : 0.152474, loss_ce: 0.056709
2022-01-07 23:41:19,231 iteration 656 : loss : 0.141770, loss_ce: 0.055721
2022-01-07 23:41:20,609 iteration 657 : loss : 0.123788, loss_ce: 0.048461
2022-01-07 23:41:21,994 iteration 658 : loss : 0.131623, loss_ce: 0.053258
2022-01-07 23:41:23,426 iteration 659 : loss : 0.134837, loss_ce: 0.048218
2022-01-07 23:41:24,833 iteration 660 : loss : 0.114671, loss_ce: 0.047439
2022-01-07 23:41:26,239 iteration 661 : loss : 0.129898, loss_ce: 0.056602
2022-01-07 23:41:27,653 iteration 662 : loss : 0.144424, loss_ce: 0.064426
2022-01-07 23:41:29,040 iteration 663 : loss : 0.212612, loss_ce: 0.115158
 10%|██▉                           | 39/400 [16:53<2:29:37, 24.87s/it]2022-01-07 23:41:30,454 iteration 664 : loss : 0.207301, loss_ce: 0.108777
2022-01-07 23:41:31,879 iteration 665 : loss : 0.155191, loss_ce: 0.054986
2022-01-07 23:41:33,285 iteration 666 : loss : 0.117745, loss_ce: 0.037312
2022-01-07 23:41:34,733 iteration 667 : loss : 0.152323, loss_ce: 0.060191
2022-01-07 23:41:36,134 iteration 668 : loss : 0.087564, loss_ce: 0.036732
2022-01-07 23:41:37,584 iteration 669 : loss : 0.126073, loss_ce: 0.053015
2022-01-07 23:41:39,025 iteration 670 : loss : 0.148649, loss_ce: 0.063554
2022-01-07 23:41:40,434 iteration 671 : loss : 0.121073, loss_ce: 0.043669
2022-01-07 23:41:41,758 iteration 672 : loss : 0.117954, loss_ce: 0.050522
2022-01-07 23:41:43,147 iteration 673 : loss : 0.158959, loss_ce: 0.058503
2022-01-07 23:41:44,675 iteration 674 : loss : 0.137060, loss_ce: 0.064879
2022-01-07 23:41:46,079 iteration 675 : loss : 0.150169, loss_ce: 0.056859
2022-01-07 23:41:47,466 iteration 676 : loss : 0.116970, loss_ce: 0.044143
2022-01-07 23:41:49,000 iteration 677 : loss : 0.115950, loss_ce: 0.056986
2022-01-07 23:41:50,421 iteration 678 : loss : 0.152096, loss_ce: 0.045065
2022-01-07 23:41:51,769 iteration 679 : loss : 0.116523, loss_ce: 0.055355
2022-01-07 23:41:51,769 Training Data Eval:
2022-01-07 23:41:58,661   Average segmentation loss on training set: 0.1411
2022-01-07 23:41:58,661 Validation Data Eval:
2022-01-07 23:42:01,045   Average segmentation loss on validation set: 0.1609
2022-01-07 23:42:02,390 iteration 680 : loss : 0.114427, loss_ce: 0.055733
 10%|███                           | 40/400 [17:26<2:44:28, 27.41s/it]2022-01-07 23:42:03,933 iteration 681 : loss : 0.120486, loss_ce: 0.047111
2022-01-07 23:42:05,319 iteration 682 : loss : 0.149408, loss_ce: 0.056761
2022-01-07 23:42:06,689 iteration 683 : loss : 0.111922, loss_ce: 0.032214
2022-01-07 23:42:08,136 iteration 684 : loss : 0.111783, loss_ce: 0.044226
2022-01-07 23:42:09,600 iteration 685 : loss : 0.120787, loss_ce: 0.041540
2022-01-07 23:42:11,032 iteration 686 : loss : 0.114061, loss_ce: 0.051185
2022-01-07 23:42:12,490 iteration 687 : loss : 0.146809, loss_ce: 0.072488
2022-01-07 23:42:13,970 iteration 688 : loss : 0.124840, loss_ce: 0.047276
2022-01-07 23:42:15,402 iteration 689 : loss : 0.125246, loss_ce: 0.066796
2022-01-07 23:42:16,793 iteration 690 : loss : 0.121443, loss_ce: 0.050090
2022-01-07 23:42:18,254 iteration 691 : loss : 0.110186, loss_ce: 0.042729
2022-01-07 23:42:19,605 iteration 692 : loss : 0.078175, loss_ce: 0.034442
2022-01-07 23:42:20,974 iteration 693 : loss : 0.108560, loss_ce: 0.045983
2022-01-07 23:42:22,368 iteration 694 : loss : 0.175434, loss_ce: 0.057721
2022-01-07 23:42:23,758 iteration 695 : loss : 0.134029, loss_ce: 0.049085
2022-01-07 23:42:25,132 iteration 696 : loss : 0.173786, loss_ce: 0.049364
2022-01-07 23:42:26,436 iteration 697 : loss : 0.096855, loss_ce: 0.041304
 10%|███                           | 41/400 [17:50<2:37:58, 26.40s/it]2022-01-07 23:42:27,878 iteration 698 : loss : 0.112215, loss_ce: 0.049673
2022-01-07 23:42:29,295 iteration 699 : loss : 0.092279, loss_ce: 0.036158
2022-01-07 23:42:30,674 iteration 700 : loss : 0.137734, loss_ce: 0.052430
2022-01-07 23:42:32,037 iteration 701 : loss : 0.169783, loss_ce: 0.060041
2022-01-07 23:42:33,466 iteration 702 : loss : 0.119023, loss_ce: 0.052080
2022-01-07 23:42:34,868 iteration 703 : loss : 0.138965, loss_ce: 0.048331
2022-01-07 23:42:36,373 iteration 704 : loss : 0.158816, loss_ce: 0.054444
2022-01-07 23:42:37,817 iteration 705 : loss : 0.102742, loss_ce: 0.047688
2022-01-07 23:42:39,268 iteration 706 : loss : 0.143121, loss_ce: 0.049137
2022-01-07 23:42:40,645 iteration 707 : loss : 0.134882, loss_ce: 0.059262
2022-01-07 23:42:42,065 iteration 708 : loss : 0.125528, loss_ce: 0.041587
2022-01-07 23:42:43,502 iteration 709 : loss : 0.099411, loss_ce: 0.040481
2022-01-07 23:42:44,846 iteration 710 : loss : 0.110688, loss_ce: 0.048892
2022-01-07 23:42:46,284 iteration 711 : loss : 0.093767, loss_ce: 0.038617
2022-01-07 23:42:47,667 iteration 712 : loss : 0.160871, loss_ce: 0.058347
2022-01-07 23:42:49,007 iteration 713 : loss : 0.090752, loss_ce: 0.038782
2022-01-07 23:42:50,345 iteration 714 : loss : 0.106211, loss_ce: 0.039317
 10%|███▏                          | 42/400 [18:14<2:33:04, 25.65s/it]2022-01-07 23:42:51,836 iteration 715 : loss : 0.111189, loss_ce: 0.056336
2022-01-07 23:42:53,210 iteration 716 : loss : 0.126286, loss_ce: 0.047985
2022-01-07 23:42:54,601 iteration 717 : loss : 0.152997, loss_ce: 0.066292
2022-01-07 23:42:56,002 iteration 718 : loss : 0.174514, loss_ce: 0.084261
2022-01-07 23:42:57,467 iteration 719 : loss : 0.093172, loss_ce: 0.036190
2022-01-07 23:42:58,819 iteration 720 : loss : 0.148839, loss_ce: 0.050923
2022-01-07 23:43:00,280 iteration 721 : loss : 0.117854, loss_ce: 0.043567
2022-01-07 23:43:01,701 iteration 722 : loss : 0.149278, loss_ce: 0.065959
2022-01-07 23:43:03,054 iteration 723 : loss : 0.082606, loss_ce: 0.034018
2022-01-07 23:43:04,542 iteration 724 : loss : 0.108707, loss_ce: 0.045418
2022-01-07 23:43:05,876 iteration 725 : loss : 0.109116, loss_ce: 0.042826
2022-01-07 23:43:07,299 iteration 726 : loss : 0.197116, loss_ce: 0.067961
2022-01-07 23:43:08,735 iteration 727 : loss : 0.160495, loss_ce: 0.055116
2022-01-07 23:43:10,286 iteration 728 : loss : 0.149729, loss_ce: 0.069065
2022-01-07 23:43:11,578 iteration 729 : loss : 0.085688, loss_ce: 0.038809
2022-01-07 23:43:13,006 iteration 730 : loss : 0.113474, loss_ce: 0.050867
2022-01-07 23:43:14,404 iteration 731 : loss : 0.122064, loss_ce: 0.044674
 11%|███▏                          | 43/400 [18:38<2:29:46, 25.17s/it]2022-01-07 23:43:15,939 iteration 732 : loss : 0.113550, loss_ce: 0.048291
2022-01-07 23:43:17,354 iteration 733 : loss : 0.184672, loss_ce: 0.086858
2022-01-07 23:43:18,713 iteration 734 : loss : 0.187343, loss_ce: 0.075900
2022-01-07 23:43:20,098 iteration 735 : loss : 0.121848, loss_ce: 0.048467
2022-01-07 23:43:21,500 iteration 736 : loss : 0.139224, loss_ce: 0.052846
2022-01-07 23:43:22,945 iteration 737 : loss : 0.103397, loss_ce: 0.038188
2022-01-07 23:43:24,371 iteration 738 : loss : 0.076870, loss_ce: 0.035456
2022-01-07 23:43:25,758 iteration 739 : loss : 0.131259, loss_ce: 0.056678
2022-01-07 23:43:27,134 iteration 740 : loss : 0.127723, loss_ce: 0.058866
2022-01-07 23:43:28,536 iteration 741 : loss : 0.101186, loss_ce: 0.041121
2022-01-07 23:43:29,999 iteration 742 : loss : 0.150898, loss_ce: 0.054334
2022-01-07 23:43:31,365 iteration 743 : loss : 0.131903, loss_ce: 0.059260
2022-01-07 23:43:32,735 iteration 744 : loss : 0.131599, loss_ce: 0.050717
2022-01-07 23:43:34,090 iteration 745 : loss : 0.097584, loss_ce: 0.044826
2022-01-07 23:43:35,473 iteration 746 : loss : 0.218525, loss_ce: 0.112102
2022-01-07 23:43:36,913 iteration 747 : loss : 0.168179, loss_ce: 0.054530
2022-01-07 23:43:38,228 iteration 748 : loss : 0.090251, loss_ce: 0.038250
 11%|███▎                          | 44/400 [19:02<2:26:58, 24.77s/it]2022-01-07 23:43:39,658 iteration 749 : loss : 0.086831, loss_ce: 0.037582
2022-01-07 23:43:41,086 iteration 750 : loss : 0.101203, loss_ce: 0.044202
2022-01-07 23:43:42,425 iteration 751 : loss : 0.083074, loss_ce: 0.037572
2022-01-07 23:43:43,824 iteration 752 : loss : 0.102340, loss_ce: 0.035701
2022-01-07 23:43:45,292 iteration 753 : loss : 0.096457, loss_ce: 0.038877
2022-01-07 23:43:46,657 iteration 754 : loss : 0.130078, loss_ce: 0.050782
2022-01-07 23:43:48,052 iteration 755 : loss : 0.095271, loss_ce: 0.035445
2022-01-07 23:43:49,406 iteration 756 : loss : 0.098167, loss_ce: 0.042304
2022-01-07 23:43:50,757 iteration 757 : loss : 0.112063, loss_ce: 0.040432
2022-01-07 23:43:52,151 iteration 758 : loss : 0.106328, loss_ce: 0.043893
2022-01-07 23:43:53,459 iteration 759 : loss : 0.077892, loss_ce: 0.030429
2022-01-07 23:43:54,896 iteration 760 : loss : 0.147084, loss_ce: 0.052100
2022-01-07 23:43:56,322 iteration 761 : loss : 0.091701, loss_ce: 0.029293
2022-01-07 23:43:57,738 iteration 762 : loss : 0.131761, loss_ce: 0.061617
2022-01-07 23:43:59,113 iteration 763 : loss : 0.102746, loss_ce: 0.044998
2022-01-07 23:44:00,469 iteration 764 : loss : 0.117671, loss_ce: 0.052587
2022-01-07 23:44:00,469 Training Data Eval:
2022-01-07 23:44:07,370   Average segmentation loss on training set: 0.0978
2022-01-07 23:44:07,371 Validation Data Eval:
2022-01-07 23:44:09,742   Average segmentation loss on validation set: 0.1140
2022-01-07 23:44:13,824 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:44:15,133 iteration 765 : loss : 0.107081, loss_ce: 0.046873
 11%|███▍                          | 45/400 [19:39<2:48:05, 28.41s/it]2022-01-07 23:44:16,422 iteration 766 : loss : 0.136958, loss_ce: 0.056665
2022-01-07 23:44:17,814 iteration 767 : loss : 0.135103, loss_ce: 0.064135
2022-01-07 23:44:19,123 iteration 768 : loss : 0.138906, loss_ce: 0.049438
2022-01-07 23:44:20,502 iteration 769 : loss : 0.188366, loss_ce: 0.038936
2022-01-07 23:44:21,833 iteration 770 : loss : 0.086478, loss_ce: 0.036072
2022-01-07 23:44:23,279 iteration 771 : loss : 0.084217, loss_ce: 0.029752
2022-01-07 23:44:24,752 iteration 772 : loss : 0.086982, loss_ce: 0.040497
2022-01-07 23:44:26,174 iteration 773 : loss : 0.087178, loss_ce: 0.030570
2022-01-07 23:44:27,561 iteration 774 : loss : 0.079849, loss_ce: 0.037831
2022-01-07 23:44:28,935 iteration 775 : loss : 0.091297, loss_ce: 0.031926
2022-01-07 23:44:30,235 iteration 776 : loss : 0.101549, loss_ce: 0.045838
2022-01-07 23:44:31,705 iteration 777 : loss : 0.111537, loss_ce: 0.043848
2022-01-07 23:44:33,098 iteration 778 : loss : 0.074032, loss_ce: 0.032618
2022-01-07 23:44:34,490 iteration 779 : loss : 0.086152, loss_ce: 0.043515
2022-01-07 23:44:35,819 iteration 780 : loss : 0.097909, loss_ce: 0.038620
2022-01-07 23:44:37,280 iteration 781 : loss : 0.078716, loss_ce: 0.033245
2022-01-07 23:44:38,704 iteration 782 : loss : 0.117814, loss_ce: 0.044731
 12%|███▍                          | 46/400 [20:02<2:39:02, 26.96s/it]2022-01-07 23:44:40,172 iteration 783 : loss : 0.056130, loss_ce: 0.028322
2022-01-07 23:44:41,563 iteration 784 : loss : 0.112805, loss_ce: 0.037635
2022-01-07 23:44:43,013 iteration 785 : loss : 0.076306, loss_ce: 0.032248
2022-01-07 23:44:44,396 iteration 786 : loss : 0.103075, loss_ce: 0.034010
2022-01-07 23:44:45,788 iteration 787 : loss : 0.101577, loss_ce: 0.044075
2022-01-07 23:44:47,202 iteration 788 : loss : 0.080076, loss_ce: 0.027016
2022-01-07 23:44:48,722 iteration 789 : loss : 0.128413, loss_ce: 0.071546
2022-01-07 23:44:50,172 iteration 790 : loss : 0.078848, loss_ce: 0.030218
2022-01-07 23:44:51,540 iteration 791 : loss : 0.096376, loss_ce: 0.032131
2022-01-07 23:44:52,959 iteration 792 : loss : 0.097363, loss_ce: 0.044742
2022-01-07 23:44:54,264 iteration 793 : loss : 0.144289, loss_ce: 0.063420
2022-01-07 23:44:55,658 iteration 794 : loss : 0.086795, loss_ce: 0.035972
2022-01-07 23:44:57,048 iteration 795 : loss : 0.113920, loss_ce: 0.047625
2022-01-07 23:44:58,438 iteration 796 : loss : 0.098067, loss_ce: 0.045232
2022-01-07 23:44:59,789 iteration 797 : loss : 0.111159, loss_ce: 0.050838
2022-01-07 23:45:01,159 iteration 798 : loss : 0.143548, loss_ce: 0.045902
2022-01-07 23:45:02,592 iteration 799 : loss : 0.065292, loss_ce: 0.027220
 12%|███▌                          | 47/400 [20:26<2:33:11, 26.04s/it]2022-01-07 23:45:04,089 iteration 800 : loss : 0.099899, loss_ce: 0.028224
2022-01-07 23:45:05,442 iteration 801 : loss : 0.120428, loss_ce: 0.044985
2022-01-07 23:45:06,825 iteration 802 : loss : 0.123838, loss_ce: 0.060385
2022-01-07 23:45:08,315 iteration 803 : loss : 0.146616, loss_ce: 0.084372
2022-01-07 23:45:09,646 iteration 804 : loss : 0.123050, loss_ce: 0.059783
2022-01-07 23:45:10,967 iteration 805 : loss : 0.185939, loss_ce: 0.053233
2022-01-07 23:45:12,360 iteration 806 : loss : 0.136799, loss_ce: 0.056876
2022-01-07 23:45:13,866 iteration 807 : loss : 0.141727, loss_ce: 0.058035
2022-01-07 23:45:15,297 iteration 808 : loss : 0.133780, loss_ce: 0.056611
2022-01-07 23:45:16,678 iteration 809 : loss : 0.193254, loss_ce: 0.065069
2022-01-07 23:45:18,112 iteration 810 : loss : 0.141030, loss_ce: 0.049510
2022-01-07 23:45:19,488 iteration 811 : loss : 0.062762, loss_ce: 0.027269
2022-01-07 23:45:20,938 iteration 812 : loss : 0.155010, loss_ce: 0.057637
2022-01-07 23:45:22,315 iteration 813 : loss : 0.102746, loss_ce: 0.045886
2022-01-07 23:45:23,714 iteration 814 : loss : 0.105510, loss_ce: 0.047706
2022-01-07 23:45:25,173 iteration 815 : loss : 0.095377, loss_ce: 0.046859
2022-01-07 23:45:26,446 iteration 816 : loss : 0.085808, loss_ce: 0.032183
 12%|███▌                          | 48/400 [20:50<2:28:54, 25.38s/it]2022-01-07 23:45:27,832 iteration 817 : loss : 0.096009, loss_ce: 0.036687
2022-01-07 23:45:29,246 iteration 818 : loss : 0.134230, loss_ce: 0.057517
2022-01-07 23:45:30,628 iteration 819 : loss : 0.084985, loss_ce: 0.033669
2022-01-07 23:45:32,145 iteration 820 : loss : 0.119275, loss_ce: 0.047770
2022-01-07 23:45:33,447 iteration 821 : loss : 0.158654, loss_ce: 0.060011
2022-01-07 23:45:34,859 iteration 822 : loss : 0.083903, loss_ce: 0.033540
2022-01-07 23:45:36,237 iteration 823 : loss : 0.114867, loss_ce: 0.041419
2022-01-07 23:45:37,620 iteration 824 : loss : 0.101026, loss_ce: 0.046086
2022-01-07 23:45:39,000 iteration 825 : loss : 0.097639, loss_ce: 0.028609
2022-01-07 23:45:40,410 iteration 826 : loss : 0.085506, loss_ce: 0.038676
2022-01-07 23:45:41,781 iteration 827 : loss : 0.142468, loss_ce: 0.040152
2022-01-07 23:45:43,237 iteration 828 : loss : 0.123020, loss_ce: 0.054718
2022-01-07 23:45:44,636 iteration 829 : loss : 0.128204, loss_ce: 0.046071
2022-01-07 23:45:46,115 iteration 830 : loss : 0.107395, loss_ce: 0.037731
2022-01-07 23:45:47,512 iteration 831 : loss : 0.109508, loss_ce: 0.043520
2022-01-07 23:45:48,884 iteration 832 : loss : 0.069950, loss_ce: 0.028218
2022-01-07 23:45:50,242 iteration 833 : loss : 0.082296, loss_ce: 0.040107
 12%|███▋                          | 49/400 [21:14<2:25:42, 24.91s/it]2022-01-07 23:45:51,627 iteration 834 : loss : 0.120120, loss_ce: 0.046493
2022-01-07 23:45:53,037 iteration 835 : loss : 0.120939, loss_ce: 0.050651
2022-01-07 23:45:54,445 iteration 836 : loss : 0.094217, loss_ce: 0.038571
2022-01-07 23:45:55,835 iteration 837 : loss : 0.091461, loss_ce: 0.038912
2022-01-07 23:45:57,197 iteration 838 : loss : 0.147450, loss_ce: 0.064776
2022-01-07 23:45:58,648 iteration 839 : loss : 0.084440, loss_ce: 0.031994
2022-01-07 23:46:00,059 iteration 840 : loss : 0.157854, loss_ce: 0.066739
2022-01-07 23:46:01,479 iteration 841 : loss : 0.122349, loss_ce: 0.067151
2022-01-07 23:46:02,990 iteration 842 : loss : 0.134441, loss_ce: 0.058075
2022-01-07 23:46:04,359 iteration 843 : loss : 0.115822, loss_ce: 0.044508
2022-01-07 23:46:05,784 iteration 844 : loss : 0.133893, loss_ce: 0.062218
2022-01-07 23:46:07,153 iteration 845 : loss : 0.090125, loss_ce: 0.038922
2022-01-07 23:46:08,551 iteration 846 : loss : 0.137391, loss_ce: 0.065640
2022-01-07 23:46:09,980 iteration 847 : loss : 0.126671, loss_ce: 0.061878
2022-01-07 23:46:11,288 iteration 848 : loss : 0.108799, loss_ce: 0.042712
2022-01-07 23:46:12,703 iteration 849 : loss : 0.102113, loss_ce: 0.050771
2022-01-07 23:46:12,703 Training Data Eval:
2022-01-07 23:46:19,620   Average segmentation loss on training set: 0.0841
2022-01-07 23:46:19,621 Validation Data Eval:
2022-01-07 23:46:22,000   Average segmentation loss on validation set: 0.0994
2022-01-07 23:46:26,118 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:46:27,465 iteration 850 : loss : 0.120085, loss_ce: 0.041700
 12%|███▊                          | 50/400 [21:51<2:46:50, 28.60s/it]2022-01-07 23:46:28,811 iteration 851 : loss : 0.089829, loss_ce: 0.045120
2022-01-07 23:46:30,205 iteration 852 : loss : 0.110742, loss_ce: 0.041977
2022-01-07 23:46:31,669 iteration 853 : loss : 0.130534, loss_ce: 0.051879
2022-01-07 23:46:33,009 iteration 854 : loss : 0.102890, loss_ce: 0.040295
2022-01-07 23:46:34,305 iteration 855 : loss : 0.071623, loss_ce: 0.029204
2022-01-07 23:46:35,646 iteration 856 : loss : 0.110890, loss_ce: 0.044777
2022-01-07 23:46:37,114 iteration 857 : loss : 0.074760, loss_ce: 0.033617
2022-01-07 23:46:38,513 iteration 858 : loss : 0.107392, loss_ce: 0.037805
2022-01-07 23:46:39,983 iteration 859 : loss : 0.094310, loss_ce: 0.036089
2022-01-07 23:46:41,411 iteration 860 : loss : 0.123144, loss_ce: 0.052941
2022-01-07 23:46:42,794 iteration 861 : loss : 0.132157, loss_ce: 0.048500
2022-01-07 23:46:44,206 iteration 862 : loss : 0.095714, loss_ce: 0.027008
2022-01-07 23:46:45,519 iteration 863 : loss : 0.121689, loss_ce: 0.056106
2022-01-07 23:46:46,895 iteration 864 : loss : 0.093497, loss_ce: 0.039409
2022-01-07 23:46:48,215 iteration 865 : loss : 0.085903, loss_ce: 0.042926
2022-01-07 23:46:49,566 iteration 866 : loss : 0.082905, loss_ce: 0.034995
2022-01-07 23:46:50,911 iteration 867 : loss : 0.128560, loss_ce: 0.060279
 13%|███▊                          | 51/400 [22:15<2:37:22, 27.06s/it]2022-01-07 23:46:52,362 iteration 868 : loss : 0.091768, loss_ce: 0.034381
2022-01-07 23:46:53,717 iteration 869 : loss : 0.083790, loss_ce: 0.037685
2022-01-07 23:46:55,124 iteration 870 : loss : 0.132830, loss_ce: 0.034671
2022-01-07 23:46:56,527 iteration 871 : loss : 0.113673, loss_ce: 0.033829
2022-01-07 23:46:57,852 iteration 872 : loss : 0.075106, loss_ce: 0.029823
2022-01-07 23:46:59,184 iteration 873 : loss : 0.067173, loss_ce: 0.024015
2022-01-07 23:47:00,591 iteration 874 : loss : 0.090336, loss_ce: 0.040728
2022-01-07 23:47:02,019 iteration 875 : loss : 0.134376, loss_ce: 0.058929
2022-01-07 23:47:03,397 iteration 876 : loss : 0.100208, loss_ce: 0.049194
2022-01-07 23:47:04,726 iteration 877 : loss : 0.070362, loss_ce: 0.027678
2022-01-07 23:47:06,064 iteration 878 : loss : 0.094688, loss_ce: 0.033251
2022-01-07 23:47:07,480 iteration 879 : loss : 0.102356, loss_ce: 0.040966
2022-01-07 23:47:08,835 iteration 880 : loss : 0.075914, loss_ce: 0.030863
2022-01-07 23:47:10,258 iteration 881 : loss : 0.111784, loss_ce: 0.045758
2022-01-07 23:47:11,620 iteration 882 : loss : 0.083402, loss_ce: 0.035942
2022-01-07 23:47:12,984 iteration 883 : loss : 0.101923, loss_ce: 0.035363
2022-01-07 23:47:14,507 iteration 884 : loss : 0.071257, loss_ce: 0.025857
 13%|███▉                          | 52/400 [22:38<2:30:53, 26.02s/it]2022-01-07 23:47:15,999 iteration 885 : loss : 0.105101, loss_ce: 0.044078
2022-01-07 23:47:17,374 iteration 886 : loss : 0.116821, loss_ce: 0.043652
2022-01-07 23:47:18,746 iteration 887 : loss : 0.122902, loss_ce: 0.062009
2022-01-07 23:47:20,174 iteration 888 : loss : 0.104680, loss_ce: 0.041089
2022-01-07 23:47:21,544 iteration 889 : loss : 0.090648, loss_ce: 0.031465
2022-01-07 23:47:22,980 iteration 890 : loss : 0.108942, loss_ce: 0.044302
2022-01-07 23:47:24,392 iteration 891 : loss : 0.144793, loss_ce: 0.079836
2022-01-07 23:47:25,804 iteration 892 : loss : 0.155010, loss_ce: 0.046351
2022-01-07 23:47:27,294 iteration 893 : loss : 0.113628, loss_ce: 0.056854
2022-01-07 23:47:28,687 iteration 894 : loss : 0.107334, loss_ce: 0.047628
2022-01-07 23:47:30,096 iteration 895 : loss : 0.087674, loss_ce: 0.034219
2022-01-07 23:47:31,547 iteration 896 : loss : 0.115432, loss_ce: 0.037637
2022-01-07 23:47:32,880 iteration 897 : loss : 0.064474, loss_ce: 0.027199
2022-01-07 23:47:34,372 iteration 898 : loss : 0.146869, loss_ce: 0.058328
2022-01-07 23:47:35,760 iteration 899 : loss : 0.104002, loss_ce: 0.041987
2022-01-07 23:47:37,177 iteration 900 : loss : 0.120240, loss_ce: 0.054841
2022-01-07 23:47:38,577 iteration 901 : loss : 0.109176, loss_ce: 0.061119
 13%|███▉                          | 53/400 [23:02<2:27:05, 25.43s/it]2022-01-07 23:47:40,009 iteration 902 : loss : 0.061989, loss_ce: 0.026664
2022-01-07 23:47:41,426 iteration 903 : loss : 0.089984, loss_ce: 0.032761
2022-01-07 23:47:42,790 iteration 904 : loss : 0.087166, loss_ce: 0.040266
2022-01-07 23:47:44,144 iteration 905 : loss : 0.144969, loss_ce: 0.040307
2022-01-07 23:47:45,482 iteration 906 : loss : 0.083014, loss_ce: 0.032318
2022-01-07 23:47:46,908 iteration 907 : loss : 0.063282, loss_ce: 0.032145
2022-01-07 23:47:48,238 iteration 908 : loss : 0.077701, loss_ce: 0.040914
2022-01-07 23:47:49,663 iteration 909 : loss : 0.105499, loss_ce: 0.050915
2022-01-07 23:47:51,056 iteration 910 : loss : 0.114216, loss_ce: 0.044908
2022-01-07 23:47:52,462 iteration 911 : loss : 0.110651, loss_ce: 0.039227
2022-01-07 23:47:53,763 iteration 912 : loss : 0.098939, loss_ce: 0.039688
2022-01-07 23:47:55,190 iteration 913 : loss : 0.106940, loss_ce: 0.041417
2022-01-07 23:47:56,568 iteration 914 : loss : 0.079418, loss_ce: 0.035465
2022-01-07 23:47:57,983 iteration 915 : loss : 0.096593, loss_ce: 0.049527
2022-01-07 23:47:59,345 iteration 916 : loss : 0.127627, loss_ce: 0.059522
2022-01-07 23:48:00,728 iteration 917 : loss : 0.096165, loss_ce: 0.040388
2022-01-07 23:48:02,129 iteration 918 : loss : 0.097619, loss_ce: 0.034144
 14%|████                          | 54/400 [23:26<2:23:24, 24.87s/it]2022-01-07 23:48:03,621 iteration 919 : loss : 0.075137, loss_ce: 0.030624
2022-01-07 23:48:04,955 iteration 920 : loss : 0.128752, loss_ce: 0.072629
2022-01-07 23:48:06,424 iteration 921 : loss : 0.069393, loss_ce: 0.031548
2022-01-07 23:48:07,724 iteration 922 : loss : 0.080265, loss_ce: 0.036188
2022-01-07 23:48:09,075 iteration 923 : loss : 0.115040, loss_ce: 0.051557
2022-01-07 23:48:10,448 iteration 924 : loss : 0.071070, loss_ce: 0.024307
2022-01-07 23:48:11,780 iteration 925 : loss : 0.086121, loss_ce: 0.030937
2022-01-07 23:48:13,108 iteration 926 : loss : 0.109639, loss_ce: 0.041555
2022-01-07 23:48:14,523 iteration 927 : loss : 0.095521, loss_ce: 0.044148
2022-01-07 23:48:15,859 iteration 928 : loss : 0.082524, loss_ce: 0.031872
2022-01-07 23:48:17,384 iteration 929 : loss : 0.079546, loss_ce: 0.038720
2022-01-07 23:48:18,782 iteration 930 : loss : 0.079906, loss_ce: 0.045907
2022-01-07 23:48:20,214 iteration 931 : loss : 0.077161, loss_ce: 0.029031
2022-01-07 23:48:21,617 iteration 932 : loss : 0.117269, loss_ce: 0.034518
2022-01-07 23:48:23,148 iteration 933 : loss : 0.128596, loss_ce: 0.040400
2022-01-07 23:48:24,531 iteration 934 : loss : 0.083599, loss_ce: 0.034424
2022-01-07 23:48:24,532 Training Data Eval:
2022-01-07 23:48:31,462   Average segmentation loss on training set: 0.1476
2022-01-07 23:48:31,462 Validation Data Eval:
2022-01-07 23:48:33,850   Average segmentation loss on validation set: 0.2648
2022-01-07 23:48:35,201 iteration 935 : loss : 0.065186, loss_ce: 0.022557
 14%|████▏                         | 55/400 [23:59<2:37:08, 27.33s/it]2022-01-07 23:48:36,605 iteration 936 : loss : 0.095416, loss_ce: 0.029137
2022-01-07 23:48:37,983 iteration 937 : loss : 0.084981, loss_ce: 0.035655
2022-01-07 23:48:39,401 iteration 938 : loss : 0.090814, loss_ce: 0.046450
2022-01-07 23:48:40,783 iteration 939 : loss : 0.095695, loss_ce: 0.041256
2022-01-07 23:48:42,139 iteration 940 : loss : 0.094791, loss_ce: 0.040226
2022-01-07 23:48:43,545 iteration 941 : loss : 0.115420, loss_ce: 0.042655
2022-01-07 23:48:44,927 iteration 942 : loss : 0.083721, loss_ce: 0.025913
2022-01-07 23:48:46,236 iteration 943 : loss : 0.122513, loss_ce: 0.045934
2022-01-07 23:48:47,582 iteration 944 : loss : 0.071425, loss_ce: 0.027565
2022-01-07 23:48:48,938 iteration 945 : loss : 0.104637, loss_ce: 0.059370
2022-01-07 23:48:50,349 iteration 946 : loss : 0.067056, loss_ce: 0.024371
2022-01-07 23:48:51,836 iteration 947 : loss : 0.069371, loss_ce: 0.032468
2022-01-07 23:48:53,187 iteration 948 : loss : 0.083269, loss_ce: 0.036432
2022-01-07 23:48:54,598 iteration 949 : loss : 0.060629, loss_ce: 0.023335
2022-01-07 23:48:56,046 iteration 950 : loss : 0.144039, loss_ce: 0.045220
2022-01-07 23:48:57,537 iteration 951 : loss : 0.095357, loss_ce: 0.045544
2022-01-07 23:48:58,875 iteration 952 : loss : 0.100464, loss_ce: 0.029722
 14%|████▏                         | 56/400 [24:23<2:30:24, 26.23s/it]2022-01-07 23:49:00,303 iteration 953 : loss : 0.076283, loss_ce: 0.029951
2022-01-07 23:49:01,789 iteration 954 : loss : 0.060585, loss_ce: 0.022427
2022-01-07 23:49:03,163 iteration 955 : loss : 0.061210, loss_ce: 0.023100
2022-01-07 23:49:04,536 iteration 956 : loss : 0.082813, loss_ce: 0.034072
2022-01-07 23:49:05,804 iteration 957 : loss : 0.106798, loss_ce: 0.037556
2022-01-07 23:49:07,162 iteration 958 : loss : 0.071859, loss_ce: 0.022381
2022-01-07 23:49:08,506 iteration 959 : loss : 0.091959, loss_ce: 0.043598
2022-01-07 23:49:09,836 iteration 960 : loss : 0.076467, loss_ce: 0.031038
2022-01-07 23:49:11,221 iteration 961 : loss : 0.094085, loss_ce: 0.032781
2022-01-07 23:49:12,576 iteration 962 : loss : 0.075658, loss_ce: 0.023048
2022-01-07 23:49:14,016 iteration 963 : loss : 0.079137, loss_ce: 0.027289
2022-01-07 23:49:15,445 iteration 964 : loss : 0.082920, loss_ce: 0.033234
2022-01-07 23:49:16,820 iteration 965 : loss : 0.071873, loss_ce: 0.027480
2022-01-07 23:49:18,287 iteration 966 : loss : 0.112181, loss_ce: 0.054817
2022-01-07 23:49:19,677 iteration 967 : loss : 0.204703, loss_ce: 0.076633
2022-01-07 23:49:21,103 iteration 968 : loss : 0.080457, loss_ce: 0.030870
2022-01-07 23:49:22,569 iteration 969 : loss : 0.062209, loss_ce: 0.025177
 14%|████▎                         | 57/400 [24:46<2:25:36, 25.47s/it]2022-01-07 23:49:23,989 iteration 970 : loss : 0.071267, loss_ce: 0.031486
2022-01-07 23:49:25,455 iteration 971 : loss : 0.109318, loss_ce: 0.038924
2022-01-07 23:49:26,804 iteration 972 : loss : 0.070041, loss_ce: 0.028241
2022-01-07 23:49:28,140 iteration 973 : loss : 0.077207, loss_ce: 0.029296
2022-01-07 23:49:29,517 iteration 974 : loss : 0.082465, loss_ce: 0.035424
2022-01-07 23:49:30,855 iteration 975 : loss : 0.110821, loss_ce: 0.040942
2022-01-07 23:49:32,225 iteration 976 : loss : 0.141391, loss_ce: 0.052802
2022-01-07 23:49:33,692 iteration 977 : loss : 0.096955, loss_ce: 0.034028
2022-01-07 23:49:35,120 iteration 978 : loss : 0.081852, loss_ce: 0.034965
2022-01-07 23:49:36,544 iteration 979 : loss : 0.062826, loss_ce: 0.029907
2022-01-07 23:49:37,870 iteration 980 : loss : 0.086388, loss_ce: 0.028171
2022-01-07 23:49:39,338 iteration 981 : loss : 0.075234, loss_ce: 0.031747
2022-01-07 23:49:40,672 iteration 982 : loss : 0.098726, loss_ce: 0.053221
2022-01-07 23:49:42,080 iteration 983 : loss : 0.080882, loss_ce: 0.037009
2022-01-07 23:49:43,467 iteration 984 : loss : 0.082964, loss_ce: 0.038534
2022-01-07 23:49:44,875 iteration 985 : loss : 0.085238, loss_ce: 0.033009
2022-01-07 23:49:46,251 iteration 986 : loss : 0.082646, loss_ce: 0.028770
 14%|████▎                         | 58/400 [25:10<2:22:07, 24.93s/it]2022-01-07 23:49:47,681 iteration 987 : loss : 0.076927, loss_ce: 0.031703
2022-01-07 23:49:49,140 iteration 988 : loss : 0.108083, loss_ce: 0.053035
2022-01-07 23:49:50,511 iteration 989 : loss : 0.081020, loss_ce: 0.028277
2022-01-07 23:49:51,964 iteration 990 : loss : 0.073485, loss_ce: 0.036003
2022-01-07 23:49:53,401 iteration 991 : loss : 0.107325, loss_ce: 0.057707
2022-01-07 23:49:54,740 iteration 992 : loss : 0.089957, loss_ce: 0.037801
2022-01-07 23:49:56,262 iteration 993 : loss : 0.062832, loss_ce: 0.026237
2022-01-07 23:49:57,701 iteration 994 : loss : 0.091786, loss_ce: 0.032584
2022-01-07 23:49:59,110 iteration 995 : loss : 0.083995, loss_ce: 0.033331
2022-01-07 23:50:00,469 iteration 996 : loss : 0.055619, loss_ce: 0.023527
2022-01-07 23:50:01,887 iteration 997 : loss : 0.083939, loss_ce: 0.038859
2022-01-07 23:50:03,303 iteration 998 : loss : 0.084101, loss_ce: 0.033437
2022-01-07 23:50:04,777 iteration 999 : loss : 0.066596, loss_ce: 0.026333
2022-01-07 23:50:06,190 iteration 1000 : loss : 0.106329, loss_ce: 0.057779
2022-01-07 23:50:07,603 iteration 1001 : loss : 0.069094, loss_ce: 0.025925
2022-01-07 23:50:08,956 iteration 1002 : loss : 0.071562, loss_ce: 0.023691
2022-01-07 23:50:10,313 iteration 1003 : loss : 0.090891, loss_ce: 0.036297
 15%|████▍                         | 59/400 [25:34<2:20:13, 24.67s/it]2022-01-07 23:50:11,723 iteration 1004 : loss : 0.096558, loss_ce: 0.041608
2022-01-07 23:50:13,200 iteration 1005 : loss : 0.142492, loss_ce: 0.051285
2022-01-07 23:50:14,623 iteration 1006 : loss : 0.095118, loss_ce: 0.050702
2022-01-07 23:50:16,041 iteration 1007 : loss : 0.075872, loss_ce: 0.027452
2022-01-07 23:50:17,446 iteration 1008 : loss : 0.122964, loss_ce: 0.034197
2022-01-07 23:50:18,858 iteration 1009 : loss : 0.096290, loss_ce: 0.042761
2022-01-07 23:50:20,222 iteration 1010 : loss : 0.076946, loss_ce: 0.029543
2022-01-07 23:50:21,597 iteration 1011 : loss : 0.062253, loss_ce: 0.025691
2022-01-07 23:50:23,063 iteration 1012 : loss : 0.124565, loss_ce: 0.041805
2022-01-07 23:50:24,428 iteration 1013 : loss : 0.108282, loss_ce: 0.038702
2022-01-07 23:50:25,859 iteration 1014 : loss : 0.096600, loss_ce: 0.035653
2022-01-07 23:50:27,245 iteration 1015 : loss : 0.077391, loss_ce: 0.034522
2022-01-07 23:50:28,556 iteration 1016 : loss : 0.066907, loss_ce: 0.036795
2022-01-07 23:50:30,042 iteration 1017 : loss : 0.072234, loss_ce: 0.022539
2022-01-07 23:50:31,488 iteration 1018 : loss : 0.063305, loss_ce: 0.027964
2022-01-07 23:50:32,980 iteration 1019 : loss : 0.082817, loss_ce: 0.032178
2022-01-07 23:50:32,980 Training Data Eval:
2022-01-07 23:50:39,898   Average segmentation loss on training set: 0.0538
2022-01-07 23:50:39,899 Validation Data Eval:
2022-01-07 23:50:42,285   Average segmentation loss on validation set: 0.0868
2022-01-07 23:50:46,368 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-07 23:50:47,710 iteration 1020 : loss : 0.077993, loss_ce: 0.029368
 15%|████▌                         | 60/400 [26:11<2:41:27, 28.49s/it]2022-01-07 23:50:49,114 iteration 1021 : loss : 0.104586, loss_ce: 0.044939
2022-01-07 23:50:50,457 iteration 1022 : loss : 0.124000, loss_ce: 0.054264
2022-01-07 23:50:51,821 iteration 1023 : loss : 0.111304, loss_ce: 0.041374
2022-01-07 23:50:53,247 iteration 1024 : loss : 0.055131, loss_ce: 0.027033
2022-01-07 23:50:54,611 iteration 1025 : loss : 0.112618, loss_ce: 0.056430
2022-01-07 23:50:56,033 iteration 1026 : loss : 0.092384, loss_ce: 0.036133
2022-01-07 23:50:57,440 iteration 1027 : loss : 0.102013, loss_ce: 0.057715
2022-01-07 23:50:58,789 iteration 1028 : loss : 0.067288, loss_ce: 0.021037
2022-01-07 23:51:00,203 iteration 1029 : loss : 0.117313, loss_ce: 0.033685
2022-01-07 23:51:01,614 iteration 1030 : loss : 0.075721, loss_ce: 0.028847
2022-01-07 23:51:03,111 iteration 1031 : loss : 0.074663, loss_ce: 0.029222
2022-01-07 23:51:04,451 iteration 1032 : loss : 0.064323, loss_ce: 0.026125
2022-01-07 23:51:05,897 iteration 1033 : loss : 0.072277, loss_ce: 0.029562
2022-01-07 23:51:07,360 iteration 1034 : loss : 0.052158, loss_ce: 0.024412
2022-01-07 23:51:08,810 iteration 1035 : loss : 0.133942, loss_ce: 0.051588
2022-01-07 23:51:10,299 iteration 1036 : loss : 0.071819, loss_ce: 0.030710
2022-01-07 23:51:11,584 iteration 1037 : loss : 0.066211, loss_ce: 0.029336
 15%|████▌                         | 61/400 [26:35<2:33:09, 27.11s/it]2022-01-07 23:51:12,961 iteration 1038 : loss : 0.086993, loss_ce: 0.040820
2022-01-07 23:51:14,353 iteration 1039 : loss : 0.099047, loss_ce: 0.042509
2022-01-07 23:51:15,697 iteration 1040 : loss : 0.079022, loss_ce: 0.029317
2022-01-07 23:51:17,158 iteration 1041 : loss : 0.073606, loss_ce: 0.026443
2022-01-07 23:51:18,616 iteration 1042 : loss : 0.078158, loss_ce: 0.036366
2022-01-07 23:51:19,973 iteration 1043 : loss : 0.049984, loss_ce: 0.018796
2022-01-07 23:51:21,379 iteration 1044 : loss : 0.068966, loss_ce: 0.026949
2022-01-07 23:51:22,744 iteration 1045 : loss : 0.072852, loss_ce: 0.039755
2022-01-07 23:51:24,141 iteration 1046 : loss : 0.078699, loss_ce: 0.027075
2022-01-07 23:51:25,613 iteration 1047 : loss : 0.074379, loss_ce: 0.030313
2022-01-07 23:51:27,046 iteration 1048 : loss : 0.104108, loss_ce: 0.043848
2022-01-07 23:51:28,379 iteration 1049 : loss : 0.080651, loss_ce: 0.032683
2022-01-07 23:51:29,728 iteration 1050 : loss : 0.068054, loss_ce: 0.026234
2022-01-07 23:51:31,128 iteration 1051 : loss : 0.082229, loss_ce: 0.031213
2022-01-07 23:51:32,519 iteration 1052 : loss : 0.081969, loss_ce: 0.039197
2022-01-07 23:51:34,052 iteration 1053 : loss : 0.081717, loss_ce: 0.039460
2022-01-07 23:51:35,421 iteration 1054 : loss : 0.057781, loss_ce: 0.022968
 16%|████▋                         | 62/400 [26:59<2:27:09, 26.12s/it]2022-01-07 23:51:36,842 iteration 1055 : loss : 0.069942, loss_ce: 0.028591
2022-01-07 23:51:38,234 iteration 1056 : loss : 0.089305, loss_ce: 0.039215
2022-01-07 23:51:39,684 iteration 1057 : loss : 0.122270, loss_ce: 0.032761
2022-01-07 23:51:41,014 iteration 1058 : loss : 0.071976, loss_ce: 0.031523
2022-01-07 23:51:42,481 iteration 1059 : loss : 0.089107, loss_ce: 0.036473
2022-01-07 23:51:43,854 iteration 1060 : loss : 0.068939, loss_ce: 0.030551
2022-01-07 23:51:45,313 iteration 1061 : loss : 0.075787, loss_ce: 0.030435
2022-01-07 23:51:46,707 iteration 1062 : loss : 0.061996, loss_ce: 0.023876
2022-01-07 23:51:48,174 iteration 1063 : loss : 0.079825, loss_ce: 0.028011
2022-01-07 23:51:49,538 iteration 1064 : loss : 0.067896, loss_ce: 0.030417
2022-01-07 23:51:50,867 iteration 1065 : loss : 0.091797, loss_ce: 0.034432
2022-01-07 23:51:52,281 iteration 1066 : loss : 0.084559, loss_ce: 0.038284
2022-01-07 23:51:53,798 iteration 1067 : loss : 0.092185, loss_ce: 0.037306
2022-01-07 23:51:55,246 iteration 1068 : loss : 0.123091, loss_ce: 0.047380
2022-01-07 23:51:56,589 iteration 1069 : loss : 0.117748, loss_ce: 0.048926
2022-01-07 23:51:58,076 iteration 1070 : loss : 0.086169, loss_ce: 0.034237
2022-01-07 23:51:59,462 iteration 1071 : loss : 0.078632, loss_ce: 0.030960
 16%|████▋                         | 63/400 [27:23<2:23:13, 25.50s/it]2022-01-07 23:52:00,867 iteration 1072 : loss : 0.074366, loss_ce: 0.032024
2022-01-07 23:52:02,360 iteration 1073 : loss : 0.076162, loss_ce: 0.028507
2022-01-07 23:52:03,757 iteration 1074 : loss : 0.055256, loss_ce: 0.023101
2022-01-07 23:52:05,160 iteration 1075 : loss : 0.083567, loss_ce: 0.028785
2022-01-07 23:52:06,517 iteration 1076 : loss : 0.072947, loss_ce: 0.027671
2022-01-07 23:52:07,949 iteration 1077 : loss : 0.153416, loss_ce: 0.051176
2022-01-07 23:52:09,293 iteration 1078 : loss : 0.079959, loss_ce: 0.029626
2022-01-07 23:52:10,682 iteration 1079 : loss : 0.066555, loss_ce: 0.028012
2022-01-07 23:52:12,101 iteration 1080 : loss : 0.082749, loss_ce: 0.032853
2022-01-07 23:52:13,522 iteration 1081 : loss : 0.060038, loss_ce: 0.025010
2022-01-07 23:52:14,942 iteration 1082 : loss : 0.103841, loss_ce: 0.045156
2022-01-07 23:52:16,361 iteration 1083 : loss : 0.080053, loss_ce: 0.028814
2022-01-07 23:52:17,810 iteration 1084 : loss : 0.140112, loss_ce: 0.037211
2022-01-07 23:52:19,189 iteration 1085 : loss : 0.062985, loss_ce: 0.022960
2022-01-07 23:52:20,619 iteration 1086 : loss : 0.073210, loss_ce: 0.028186
2022-01-07 23:52:21,963 iteration 1087 : loss : 0.065984, loss_ce: 0.024961
2022-01-07 23:52:23,330 iteration 1088 : loss : 0.082808, loss_ce: 0.039194
 16%|████▊                         | 64/400 [27:47<2:20:03, 25.01s/it]2022-01-07 23:52:24,662 iteration 1089 : loss : 0.072989, loss_ce: 0.034451
2022-01-07 23:52:26,050 iteration 1090 : loss : 0.071486, loss_ce: 0.022610
2022-01-07 23:52:27,509 iteration 1091 : loss : 0.104879, loss_ce: 0.055130
2022-01-07 23:52:28,910 iteration 1092 : loss : 0.103087, loss_ce: 0.027624
2022-01-07 23:52:30,322 iteration 1093 : loss : 0.109177, loss_ce: 0.034819
2022-01-07 23:52:31,829 iteration 1094 : loss : 0.151596, loss_ce: 0.029466
2022-01-07 23:52:33,178 iteration 1095 : loss : 0.061068, loss_ce: 0.022246
2022-01-07 23:52:34,530 iteration 1096 : loss : 0.059800, loss_ce: 0.026091
2022-01-07 23:52:35,856 iteration 1097 : loss : 0.078295, loss_ce: 0.028318
2022-01-07 23:52:37,304 iteration 1098 : loss : 0.079566, loss_ce: 0.041348
2022-01-07 23:52:38,700 iteration 1099 : loss : 0.063822, loss_ce: 0.023402
2022-01-07 23:52:40,181 iteration 1100 : loss : 0.090852, loss_ce: 0.042546
2022-01-07 23:52:41,636 iteration 1101 : loss : 0.104895, loss_ce: 0.046990
2022-01-07 23:52:43,028 iteration 1102 : loss : 0.096457, loss_ce: 0.038498
2022-01-07 23:52:44,443 iteration 1103 : loss : 0.064679, loss_ce: 0.031934
2022-01-07 23:52:45,849 iteration 1104 : loss : 0.074574, loss_ce: 0.031629
2022-01-07 23:52:45,849 Training Data Eval:
2022-01-07 23:52:52,779   Average segmentation loss on training set: 0.1133
2022-01-07 23:52:52,780 Validation Data Eval:
2022-01-07 23:52:55,163   Average segmentation loss on validation set: 0.2354
2022-01-07 23:52:56,545 iteration 1105 : loss : 0.062729, loss_ce: 0.025721
 16%|████▉                         | 65/400 [28:20<2:33:23, 27.47s/it]2022-01-07 23:52:58,037 iteration 1106 : loss : 0.106522, loss_ce: 0.042181
2022-01-07 23:52:59,427 iteration 1107 : loss : 0.076685, loss_ce: 0.033354
2022-01-07 23:53:00,794 iteration 1108 : loss : 0.077484, loss_ce: 0.026309
2022-01-07 23:53:02,256 iteration 1109 : loss : 0.087921, loss_ce: 0.032998
2022-01-07 23:53:03,671 iteration 1110 : loss : 0.071116, loss_ce: 0.028857
2022-01-07 23:53:05,146 iteration 1111 : loss : 0.142337, loss_ce: 0.045402
2022-01-07 23:53:06,494 iteration 1112 : loss : 0.093869, loss_ce: 0.042108
2022-01-07 23:53:07,889 iteration 1113 : loss : 0.077809, loss_ce: 0.023920
2022-01-07 23:53:09,313 iteration 1114 : loss : 0.104594, loss_ce: 0.038815
2022-01-07 23:53:10,731 iteration 1115 : loss : 0.077257, loss_ce: 0.029061
2022-01-07 23:53:12,049 iteration 1116 : loss : 0.068304, loss_ce: 0.031669
2022-01-07 23:53:13,391 iteration 1117 : loss : 0.092721, loss_ce: 0.043510
2022-01-07 23:53:14,812 iteration 1118 : loss : 0.081347, loss_ce: 0.036265
2022-01-07 23:53:16,261 iteration 1119 : loss : 0.087481, loss_ce: 0.042739
2022-01-07 23:53:17,659 iteration 1120 : loss : 0.096064, loss_ce: 0.033035
2022-01-07 23:53:19,025 iteration 1121 : loss : 0.081672, loss_ce: 0.042430
2022-01-07 23:53:20,347 iteration 1122 : loss : 0.075806, loss_ce: 0.025446
 16%|████▉                         | 66/400 [28:44<2:26:47, 26.37s/it]2022-01-07 23:53:21,899 iteration 1123 : loss : 0.049657, loss_ce: 0.019239
2022-01-07 23:53:23,288 iteration 1124 : loss : 0.051510, loss_ce: 0.022809
2022-01-07 23:53:24,641 iteration 1125 : loss : 0.071961, loss_ce: 0.019585
2022-01-07 23:53:26,004 iteration 1126 : loss : 0.071479, loss_ce: 0.026441
2022-01-07 23:53:27,387 iteration 1127 : loss : 0.074187, loss_ce: 0.028737
2022-01-07 23:53:28,853 iteration 1128 : loss : 0.071371, loss_ce: 0.032757
2022-01-07 23:53:30,331 iteration 1129 : loss : 0.107687, loss_ce: 0.049872
2022-01-07 23:53:31,666 iteration 1130 : loss : 0.088877, loss_ce: 0.038443
2022-01-07 23:53:33,106 iteration 1131 : loss : 0.095079, loss_ce: 0.041147
2022-01-07 23:53:34,611 iteration 1132 : loss : 0.081061, loss_ce: 0.027506
2022-01-07 23:53:36,068 iteration 1133 : loss : 0.093709, loss_ce: 0.038676
2022-01-07 23:53:37,522 iteration 1134 : loss : 0.092219, loss_ce: 0.028793
2022-01-07 23:53:38,880 iteration 1135 : loss : 0.078188, loss_ce: 0.028532
2022-01-07 23:53:40,280 iteration 1136 : loss : 0.045047, loss_ce: 0.019957
2022-01-07 23:53:41,650 iteration 1137 : loss : 0.082505, loss_ce: 0.037167
2022-01-07 23:53:43,070 iteration 1138 : loss : 0.080597, loss_ce: 0.033081
2022-01-07 23:53:44,407 iteration 1139 : loss : 0.067814, loss_ce: 0.027822
 17%|█████                         | 67/400 [29:08<2:22:30, 25.68s/it]2022-01-07 23:53:45,859 iteration 1140 : loss : 0.066563, loss_ce: 0.028392
2022-01-07 23:53:47,274 iteration 1141 : loss : 0.076837, loss_ce: 0.036047
2022-01-07 23:53:48,784 iteration 1142 : loss : 0.105772, loss_ce: 0.043206
2022-01-07 23:53:50,199 iteration 1143 : loss : 0.066816, loss_ce: 0.027291
2022-01-07 23:53:51,644 iteration 1144 : loss : 0.076240, loss_ce: 0.027786
2022-01-07 23:53:53,051 iteration 1145 : loss : 0.079398, loss_ce: 0.030514
2022-01-07 23:53:54,426 iteration 1146 : loss : 0.052076, loss_ce: 0.020791
2022-01-07 23:53:55,798 iteration 1147 : loss : 0.054583, loss_ce: 0.020315
2022-01-07 23:53:57,161 iteration 1148 : loss : 0.067090, loss_ce: 0.026838
2022-01-07 23:53:58,571 iteration 1149 : loss : 0.047753, loss_ce: 0.024038
2022-01-07 23:53:59,932 iteration 1150 : loss : 0.064984, loss_ce: 0.022302
2022-01-07 23:54:01,305 iteration 1151 : loss : 0.049739, loss_ce: 0.023968
2022-01-07 23:54:02,654 iteration 1152 : loss : 0.076190, loss_ce: 0.031675
2022-01-07 23:54:04,165 iteration 1153 : loss : 0.088755, loss_ce: 0.023782
2022-01-07 23:54:05,596 iteration 1154 : loss : 0.067465, loss_ce: 0.024379
2022-01-07 23:54:07,039 iteration 1155 : loss : 0.074529, loss_ce: 0.024413
2022-01-07 23:54:08,539 iteration 1156 : loss : 0.085330, loss_ce: 0.028227
 17%|█████                         | 68/400 [29:32<2:19:30, 25.21s/it]2022-01-07 23:54:10,024 iteration 1157 : loss : 0.084171, loss_ce: 0.030021
2022-01-07 23:54:11,418 iteration 1158 : loss : 0.069645, loss_ce: 0.024368
2022-01-07 23:54:12,720 iteration 1159 : loss : 0.079772, loss_ce: 0.032798
2022-01-07 23:54:14,077 iteration 1160 : loss : 0.069233, loss_ce: 0.035696
2022-01-07 23:54:15,490 iteration 1161 : loss : 0.058562, loss_ce: 0.021508
2022-01-07 23:54:16,922 iteration 1162 : loss : 0.072320, loss_ce: 0.028472
2022-01-07 23:54:18,317 iteration 1163 : loss : 0.052435, loss_ce: 0.025567
2022-01-07 23:54:19,763 iteration 1164 : loss : 0.066509, loss_ce: 0.028112
2022-01-07 23:54:21,083 iteration 1165 : loss : 0.060953, loss_ce: 0.028898
2022-01-07 23:54:22,450 iteration 1166 : loss : 0.106222, loss_ce: 0.036476
2022-01-07 23:54:23,852 iteration 1167 : loss : 0.072904, loss_ce: 0.036038
2022-01-07 23:54:25,307 iteration 1168 : loss : 0.134079, loss_ce: 0.041754
2022-01-07 23:54:26,641 iteration 1169 : loss : 0.088263, loss_ce: 0.027433
2022-01-07 23:54:28,077 iteration 1170 : loss : 0.087950, loss_ce: 0.027004
2022-01-07 23:54:29,591 iteration 1171 : loss : 0.141341, loss_ce: 0.039009
2022-01-07 23:54:30,921 iteration 1172 : loss : 0.060943, loss_ce: 0.023053
2022-01-07 23:54:32,247 iteration 1173 : loss : 0.048197, loss_ce: 0.016604
 17%|█████▏                        | 69/400 [29:56<2:16:36, 24.76s/it]2022-01-07 23:54:33,710 iteration 1174 : loss : 0.068510, loss_ce: 0.023160
2022-01-07 23:54:35,180 iteration 1175 : loss : 0.073525, loss_ce: 0.029558
2022-01-07 23:54:36,571 iteration 1176 : loss : 0.078324, loss_ce: 0.029560
2022-01-07 23:54:37,934 iteration 1177 : loss : 0.098764, loss_ce: 0.046364
2022-01-07 23:54:39,267 iteration 1178 : loss : 0.075820, loss_ce: 0.026619
2022-01-07 23:54:40,738 iteration 1179 : loss : 0.065548, loss_ce: 0.025703
2022-01-07 23:54:42,154 iteration 1180 : loss : 0.064961, loss_ce: 0.025048
2022-01-07 23:54:43,486 iteration 1181 : loss : 0.076090, loss_ce: 0.024370
2022-01-07 23:54:44,854 iteration 1182 : loss : 0.068063, loss_ce: 0.028859
2022-01-07 23:54:46,254 iteration 1183 : loss : 0.056064, loss_ce: 0.024808
2022-01-07 23:54:47,659 iteration 1184 : loss : 0.106099, loss_ce: 0.042739
2022-01-07 23:54:49,033 iteration 1185 : loss : 0.075374, loss_ce: 0.040474
2022-01-07 23:54:50,437 iteration 1186 : loss : 0.055688, loss_ce: 0.017668
2022-01-07 23:54:51,774 iteration 1187 : loss : 0.056885, loss_ce: 0.024066
2022-01-07 23:54:53,224 iteration 1188 : loss : 0.096852, loss_ce: 0.036289
2022-01-07 23:54:54,660 iteration 1189 : loss : 0.123817, loss_ce: 0.033065
2022-01-07 23:54:54,660 Training Data Eval:
2022-01-07 23:55:01,577   Average segmentation loss on training set: 0.1220
2022-01-07 23:55:01,578 Validation Data Eval:
2022-01-07 23:55:03,960   Average segmentation loss on validation set: 0.2083
2022-01-07 23:55:05,324 iteration 1190 : loss : 0.089782, loss_ce: 0.036562
 18%|█████▎                        | 70/400 [30:29<2:29:54, 27.25s/it]2022-01-07 23:55:06,750 iteration 1191 : loss : 0.085735, loss_ce: 0.037187
2022-01-07 23:55:08,112 iteration 1192 : loss : 0.098838, loss_ce: 0.051325
2022-01-07 23:55:09,417 iteration 1193 : loss : 0.078179, loss_ce: 0.030415
2022-01-07 23:55:10,830 iteration 1194 : loss : 0.100019, loss_ce: 0.032597
2022-01-07 23:55:12,212 iteration 1195 : loss : 0.089850, loss_ce: 0.036618
2022-01-07 23:55:13,618 iteration 1196 : loss : 0.062363, loss_ce: 0.023131
2022-01-07 23:55:15,048 iteration 1197 : loss : 0.105430, loss_ce: 0.036963
2022-01-07 23:55:16,378 iteration 1198 : loss : 0.072749, loss_ce: 0.030228
2022-01-07 23:55:17,770 iteration 1199 : loss : 0.081732, loss_ce: 0.038852
2022-01-07 23:55:19,084 iteration 1200 : loss : 0.095392, loss_ce: 0.033195
2022-01-07 23:55:20,436 iteration 1201 : loss : 0.080630, loss_ce: 0.025179
2022-01-07 23:55:21,879 iteration 1202 : loss : 0.073093, loss_ce: 0.029563
2022-01-07 23:55:23,346 iteration 1203 : loss : 0.081698, loss_ce: 0.038601
2022-01-07 23:55:24,667 iteration 1204 : loss : 0.103550, loss_ce: 0.031047
2022-01-07 23:55:26,086 iteration 1205 : loss : 0.075219, loss_ce: 0.029737
2022-01-07 23:55:27,439 iteration 1206 : loss : 0.079761, loss_ce: 0.026043
2022-01-07 23:55:28,915 iteration 1207 : loss : 0.071216, loss_ce: 0.030470
 18%|█████▎                        | 71/400 [30:53<2:23:25, 26.16s/it]2022-01-07 23:55:30,318 iteration 1208 : loss : 0.093684, loss_ce: 0.043961
2022-01-07 23:55:31,717 iteration 1209 : loss : 0.090363, loss_ce: 0.028214
2022-01-07 23:55:33,039 iteration 1210 : loss : 0.078259, loss_ce: 0.033199
2022-01-07 23:55:34,504 iteration 1211 : loss : 0.075397, loss_ce: 0.033476
2022-01-07 23:55:35,881 iteration 1212 : loss : 0.055623, loss_ce: 0.022643
2022-01-07 23:55:37,171 iteration 1213 : loss : 0.053125, loss_ce: 0.019047
2022-01-07 23:55:38,637 iteration 1214 : loss : 0.070455, loss_ce: 0.029564
2022-01-07 23:55:40,013 iteration 1215 : loss : 0.061659, loss_ce: 0.025575
2022-01-07 23:55:41,411 iteration 1216 : loss : 0.099587, loss_ce: 0.036675
2022-01-07 23:55:42,914 iteration 1217 : loss : 0.110733, loss_ce: 0.043161
2022-01-07 23:55:44,242 iteration 1218 : loss : 0.089118, loss_ce: 0.048144
2022-01-07 23:55:45,770 iteration 1219 : loss : 0.068859, loss_ce: 0.030730
2022-01-07 23:55:47,182 iteration 1220 : loss : 0.075475, loss_ce: 0.031056
2022-01-07 23:55:48,635 iteration 1221 : loss : 0.073883, loss_ce: 0.023792
2022-01-07 23:55:49,996 iteration 1222 : loss : 0.094242, loss_ce: 0.028509
2022-01-07 23:55:51,358 iteration 1223 : loss : 0.066810, loss_ce: 0.024036
2022-01-07 23:55:52,752 iteration 1224 : loss : 0.094954, loss_ce: 0.034087
 18%|█████▍                        | 72/400 [31:16<2:19:11, 25.46s/it]2022-01-07 23:55:54,192 iteration 1225 : loss : 0.067747, loss_ce: 0.019966
2022-01-07 23:55:55,599 iteration 1226 : loss : 0.066668, loss_ce: 0.022970
2022-01-07 23:55:56,937 iteration 1227 : loss : 0.059572, loss_ce: 0.021684
2022-01-07 23:55:58,287 iteration 1228 : loss : 0.135741, loss_ce: 0.048488
2022-01-07 23:55:59,822 iteration 1229 : loss : 0.071274, loss_ce: 0.035998
2022-01-07 23:56:01,253 iteration 1230 : loss : 0.063879, loss_ce: 0.024281
2022-01-07 23:56:02,659 iteration 1231 : loss : 0.066200, loss_ce: 0.025190
2022-01-07 23:56:04,014 iteration 1232 : loss : 0.080459, loss_ce: 0.030850
2022-01-07 23:56:05,486 iteration 1233 : loss : 0.056261, loss_ce: 0.021044
2022-01-07 23:56:06,982 iteration 1234 : loss : 0.073747, loss_ce: 0.030633
2022-01-07 23:56:08,296 iteration 1235 : loss : 0.110253, loss_ce: 0.040305
2022-01-07 23:56:09,728 iteration 1236 : loss : 0.108948, loss_ce: 0.062680
2022-01-07 23:56:11,138 iteration 1237 : loss : 0.086648, loss_ce: 0.032262
2022-01-07 23:56:12,499 iteration 1238 : loss : 0.074284, loss_ce: 0.028596
2022-01-07 23:56:13,872 iteration 1239 : loss : 0.055973, loss_ce: 0.025378
2022-01-07 23:56:15,340 iteration 1240 : loss : 0.068614, loss_ce: 0.027852
2022-01-07 23:56:16,737 iteration 1241 : loss : 0.069713, loss_ce: 0.031702
 18%|█████▍                        | 73/400 [31:40<2:16:21, 25.02s/it]2022-01-07 23:56:18,235 iteration 1242 : loss : 0.081411, loss_ce: 0.039719
2022-01-07 23:56:19,632 iteration 1243 : loss : 0.067595, loss_ce: 0.025363
2022-01-07 23:56:21,122 iteration 1244 : loss : 0.075369, loss_ce: 0.030825
2022-01-07 23:56:22,588 iteration 1245 : loss : 0.077780, loss_ce: 0.030952
2022-01-07 23:56:23,976 iteration 1246 : loss : 0.073155, loss_ce: 0.031473
2022-01-07 23:56:25,537 iteration 1247 : loss : 0.087503, loss_ce: 0.027611
2022-01-07 23:56:26,898 iteration 1248 : loss : 0.079389, loss_ce: 0.029631
2022-01-07 23:56:28,284 iteration 1249 : loss : 0.074511, loss_ce: 0.033307
2022-01-07 23:56:29,713 iteration 1250 : loss : 0.082970, loss_ce: 0.021090
2022-01-07 23:56:31,170 iteration 1251 : loss : 0.068038, loss_ce: 0.028730
2022-01-07 23:56:32,508 iteration 1252 : loss : 0.100757, loss_ce: 0.031858
2022-01-07 23:56:33,886 iteration 1253 : loss : 0.077571, loss_ce: 0.029370
2022-01-07 23:56:35,300 iteration 1254 : loss : 0.050741, loss_ce: 0.017721
2022-01-07 23:56:36,833 iteration 1255 : loss : 0.099550, loss_ce: 0.048290
2022-01-07 23:56:38,221 iteration 1256 : loss : 0.060602, loss_ce: 0.022488
2022-01-07 23:56:39,521 iteration 1257 : loss : 0.045561, loss_ce: 0.016632
2022-01-07 23:56:40,825 iteration 1258 : loss : 0.063491, loss_ce: 0.032450
 18%|█████▌                        | 74/400 [32:05<2:14:24, 24.74s/it]2022-01-07 23:56:42,230 iteration 1259 : loss : 0.069720, loss_ce: 0.030424
2022-01-07 23:56:43,722 iteration 1260 : loss : 0.054342, loss_ce: 0.023966
2022-01-07 23:56:45,097 iteration 1261 : loss : 0.084462, loss_ce: 0.029986
2022-01-07 23:56:46,424 iteration 1262 : loss : 0.065384, loss_ce: 0.025676
2022-01-07 23:56:47,830 iteration 1263 : loss : 0.081252, loss_ce: 0.029772
2022-01-07 23:56:49,235 iteration 1264 : loss : 0.058865, loss_ce: 0.024872
2022-01-07 23:56:50,628 iteration 1265 : loss : 0.066671, loss_ce: 0.023856
2022-01-07 23:56:51,978 iteration 1266 : loss : 0.071728, loss_ce: 0.032294
2022-01-07 23:56:53,332 iteration 1267 : loss : 0.092596, loss_ce: 0.047562
2022-01-07 23:56:54,684 iteration 1268 : loss : 0.063029, loss_ce: 0.027750
2022-01-07 23:56:56,050 iteration 1269 : loss : 0.071280, loss_ce: 0.033476
2022-01-07 23:56:57,437 iteration 1270 : loss : 0.053242, loss_ce: 0.022684
2022-01-07 23:56:58,892 iteration 1271 : loss : 0.093861, loss_ce: 0.045917
2022-01-07 23:57:00,194 iteration 1272 : loss : 0.058434, loss_ce: 0.020072
2022-01-07 23:57:01,537 iteration 1273 : loss : 0.074185, loss_ce: 0.024388
2022-01-07 23:57:02,949 iteration 1274 : loss : 0.068688, loss_ce: 0.026974
2022-01-07 23:57:02,950 Training Data Eval:
2022-01-07 23:57:09,880   Average segmentation loss on training set: 0.0767
2022-01-07 23:57:09,880 Validation Data Eval:
2022-01-07 23:57:12,266   Average segmentation loss on validation set: 0.1085
2022-01-07 23:57:13,608 iteration 1275 : loss : 0.074199, loss_ce: 0.032744
 19%|█████▋                        | 75/400 [32:37<2:27:05, 27.15s/it]2022-01-07 23:57:15,015 iteration 1276 : loss : 0.051173, loss_ce: 0.022227
2022-01-07 23:57:16,512 iteration 1277 : loss : 0.089557, loss_ce: 0.032872
2022-01-07 23:57:17,926 iteration 1278 : loss : 0.087882, loss_ce: 0.042755
2022-01-07 23:57:19,301 iteration 1279 : loss : 0.081155, loss_ce: 0.033108
2022-01-07 23:57:20,710 iteration 1280 : loss : 0.063040, loss_ce: 0.028526
2022-01-07 23:57:22,054 iteration 1281 : loss : 0.067085, loss_ce: 0.026897
2022-01-07 23:57:23,452 iteration 1282 : loss : 0.072473, loss_ce: 0.028599
2022-01-07 23:57:24,761 iteration 1283 : loss : 0.082930, loss_ce: 0.024777
2022-01-07 23:57:26,126 iteration 1284 : loss : 0.047459, loss_ce: 0.016710
2022-01-07 23:57:27,446 iteration 1285 : loss : 0.052360, loss_ce: 0.017019
2022-01-07 23:57:28,880 iteration 1286 : loss : 0.088831, loss_ce: 0.029702
2022-01-07 23:57:30,331 iteration 1287 : loss : 0.058668, loss_ce: 0.024688
2022-01-07 23:57:31,674 iteration 1288 : loss : 0.075938, loss_ce: 0.032030
2022-01-07 23:57:33,054 iteration 1289 : loss : 0.093184, loss_ce: 0.035936
2022-01-07 23:57:34,401 iteration 1290 : loss : 0.116400, loss_ce: 0.041325
2022-01-07 23:57:35,758 iteration 1291 : loss : 0.063401, loss_ce: 0.018957
2022-01-07 23:57:37,109 iteration 1292 : loss : 0.102575, loss_ce: 0.062446
 19%|█████▋                        | 76/400 [33:01<2:20:42, 26.06s/it]2022-01-07 23:57:38,622 iteration 1293 : loss : 0.072840, loss_ce: 0.033529
2022-01-07 23:57:39,933 iteration 1294 : loss : 0.052960, loss_ce: 0.022543
2022-01-07 23:57:41,371 iteration 1295 : loss : 0.067213, loss_ce: 0.024741
2022-01-07 23:57:42,802 iteration 1296 : loss : 0.081503, loss_ce: 0.029742
2022-01-07 23:57:44,079 iteration 1297 : loss : 0.047829, loss_ce: 0.019416
2022-01-07 23:57:45,528 iteration 1298 : loss : 0.067969, loss_ce: 0.026121
2022-01-07 23:57:46,888 iteration 1299 : loss : 0.062252, loss_ce: 0.026376
2022-01-07 23:57:48,385 iteration 1300 : loss : 0.089624, loss_ce: 0.033115
2022-01-07 23:57:49,857 iteration 1301 : loss : 0.102593, loss_ce: 0.033695
2022-01-07 23:57:51,184 iteration 1302 : loss : 0.091422, loss_ce: 0.034537
2022-01-07 23:57:52,609 iteration 1303 : loss : 0.064816, loss_ce: 0.020315
2022-01-07 23:57:54,028 iteration 1304 : loss : 0.055044, loss_ce: 0.024820
2022-01-07 23:57:55,499 iteration 1305 : loss : 0.063221, loss_ce: 0.021992
2022-01-07 23:57:56,908 iteration 1306 : loss : 0.051880, loss_ce: 0.021120
2022-01-07 23:57:58,265 iteration 1307 : loss : 0.067638, loss_ce: 0.026333
2022-01-07 23:57:59,683 iteration 1308 : loss : 0.070312, loss_ce: 0.033781
2022-01-07 23:58:01,039 iteration 1309 : loss : 0.061137, loss_ce: 0.023476
 19%|█████▊                        | 77/400 [33:25<2:16:50, 25.42s/it]2022-01-07 23:58:02,447 iteration 1310 : loss : 0.073241, loss_ce: 0.030977
2022-01-07 23:58:03,935 iteration 1311 : loss : 0.074928, loss_ce: 0.026506
2022-01-07 23:58:05,327 iteration 1312 : loss : 0.069832, loss_ce: 0.023123
2022-01-07 23:58:06,726 iteration 1313 : loss : 0.067258, loss_ce: 0.030389
2022-01-07 23:58:08,172 iteration 1314 : loss : 0.079931, loss_ce: 0.037063
2022-01-07 23:58:09,638 iteration 1315 : loss : 0.061692, loss_ce: 0.032085
2022-01-07 23:58:11,002 iteration 1316 : loss : 0.064829, loss_ce: 0.026836
2022-01-07 23:58:12,352 iteration 1317 : loss : 0.088012, loss_ce: 0.034296
2022-01-07 23:58:13,683 iteration 1318 : loss : 0.047502, loss_ce: 0.018059
2022-01-07 23:58:15,094 iteration 1319 : loss : 0.072642, loss_ce: 0.024587
2022-01-07 23:58:16,415 iteration 1320 : loss : 0.066369, loss_ce: 0.024662
2022-01-07 23:58:17,816 iteration 1321 : loss : 0.082181, loss_ce: 0.043354
2022-01-07 23:58:19,186 iteration 1322 : loss : 0.053416, loss_ce: 0.019734
2022-01-07 23:58:20,624 iteration 1323 : loss : 0.067285, loss_ce: 0.022306
2022-01-07 23:58:22,090 iteration 1324 : loss : 0.067422, loss_ce: 0.028272
2022-01-07 23:58:23,473 iteration 1325 : loss : 0.077099, loss_ce: 0.033835
2022-01-07 23:58:24,887 iteration 1326 : loss : 0.058048, loss_ce: 0.022687
 20%|█████▊                        | 78/400 [33:49<2:13:52, 24.95s/it]2022-01-07 23:58:26,297 iteration 1327 : loss : 0.057271, loss_ce: 0.020846
2022-01-07 23:58:27,698 iteration 1328 : loss : 0.061046, loss_ce: 0.024908
2022-01-07 23:58:29,090 iteration 1329 : loss : 0.069228, loss_ce: 0.027284
2022-01-07 23:58:30,592 iteration 1330 : loss : 0.079367, loss_ce: 0.027859
2022-01-07 23:58:31,987 iteration 1331 : loss : 0.069735, loss_ce: 0.027481
2022-01-07 23:58:33,352 iteration 1332 : loss : 0.060975, loss_ce: 0.023518
2022-01-07 23:58:34,715 iteration 1333 : loss : 0.073144, loss_ce: 0.031666
2022-01-07 23:58:36,168 iteration 1334 : loss : 0.045320, loss_ce: 0.018322
2022-01-07 23:58:37,584 iteration 1335 : loss : 0.057145, loss_ce: 0.022854
2022-01-07 23:58:38,931 iteration 1336 : loss : 0.053805, loss_ce: 0.024529
2022-01-07 23:58:40,373 iteration 1337 : loss : 0.090676, loss_ce: 0.033717
2022-01-07 23:58:41,737 iteration 1338 : loss : 0.055893, loss_ce: 0.023894
2022-01-07 23:58:43,102 iteration 1339 : loss : 0.088332, loss_ce: 0.036447
2022-01-07 23:58:44,459 iteration 1340 : loss : 0.056913, loss_ce: 0.023768
2022-01-07 23:58:45,869 iteration 1341 : loss : 0.075502, loss_ce: 0.031418
2022-01-07 23:58:47,335 iteration 1342 : loss : 0.075702, loss_ce: 0.029790
2022-01-07 23:58:48,658 iteration 1343 : loss : 0.059666, loss_ce: 0.021449
 20%|█████▉                        | 79/400 [34:12<2:11:34, 24.59s/it]2022-01-07 23:58:50,118 iteration 1344 : loss : 0.056708, loss_ce: 0.019718
2022-01-07 23:58:51,475 iteration 1345 : loss : 0.063941, loss_ce: 0.017987
2022-01-07 23:58:52,903 iteration 1346 : loss : 0.053859, loss_ce: 0.018872
2022-01-07 23:58:54,318 iteration 1347 : loss : 0.087503, loss_ce: 0.043043
2022-01-07 23:58:55,688 iteration 1348 : loss : 0.092465, loss_ce: 0.044061
2022-01-07 23:58:57,076 iteration 1349 : loss : 0.105709, loss_ce: 0.038169
2022-01-07 23:58:58,450 iteration 1350 : loss : 0.081690, loss_ce: 0.024852
2022-01-07 23:58:59,942 iteration 1351 : loss : 0.090916, loss_ce: 0.041685
2022-01-07 23:59:01,280 iteration 1352 : loss : 0.074067, loss_ce: 0.030767
2022-01-07 23:59:02,670 iteration 1353 : loss : 0.056528, loss_ce: 0.024867
2022-01-07 23:59:04,063 iteration 1354 : loss : 0.065591, loss_ce: 0.023989
2022-01-07 23:59:05,384 iteration 1355 : loss : 0.078692, loss_ce: 0.026912
2022-01-07 23:59:06,874 iteration 1356 : loss : 0.071568, loss_ce: 0.026404
2022-01-07 23:59:08,201 iteration 1357 : loss : 0.056924, loss_ce: 0.027716
2022-01-07 23:59:09,554 iteration 1358 : loss : 0.065220, loss_ce: 0.030384
2022-01-07 23:59:11,019 iteration 1359 : loss : 0.074291, loss_ce: 0.039413
2022-01-07 23:59:11,019 Training Data Eval:
2022-01-07 23:59:17,941   Average segmentation loss on training set: 0.0565
2022-01-07 23:59:17,941 Validation Data Eval:
2022-01-07 23:59:20,325   Average segmentation loss on validation set: 0.1438
2022-01-07 23:59:21,722 iteration 1360 : loss : 0.046256, loss_ce: 0.017477
 20%|██████                        | 80/400 [34:45<2:24:42, 27.13s/it]2022-01-07 23:59:23,210 iteration 1361 : loss : 0.078445, loss_ce: 0.023147
2022-01-07 23:59:24,603 iteration 1362 : loss : 0.076306, loss_ce: 0.023599
2022-01-07 23:59:26,025 iteration 1363 : loss : 0.048483, loss_ce: 0.018035
2022-01-07 23:59:27,336 iteration 1364 : loss : 0.075517, loss_ce: 0.034313
2022-01-07 23:59:28,686 iteration 1365 : loss : 0.055323, loss_ce: 0.018075
2022-01-07 23:59:30,167 iteration 1366 : loss : 0.078269, loss_ce: 0.031633
2022-01-07 23:59:31,600 iteration 1367 : loss : 0.083122, loss_ce: 0.048286
2022-01-07 23:59:33,161 iteration 1368 : loss : 0.054938, loss_ce: 0.020525
2022-01-07 23:59:34,543 iteration 1369 : loss : 0.057787, loss_ce: 0.021488
2022-01-07 23:59:35,896 iteration 1370 : loss : 0.066892, loss_ce: 0.035192
2022-01-07 23:59:37,289 iteration 1371 : loss : 0.071912, loss_ce: 0.026591
2022-01-07 23:59:38,813 iteration 1372 : loss : 0.097000, loss_ce: 0.039587
2022-01-07 23:59:40,188 iteration 1373 : loss : 0.063259, loss_ce: 0.026377
2022-01-07 23:59:41,541 iteration 1374 : loss : 0.060193, loss_ce: 0.021607
2022-01-07 23:59:42,931 iteration 1375 : loss : 0.052967, loss_ce: 0.021330
2022-01-07 23:59:44,315 iteration 1376 : loss : 0.053997, loss_ce: 0.017144
2022-01-07 23:59:45,761 iteration 1377 : loss : 0.066395, loss_ce: 0.032073
 20%|██████                        | 81/400 [35:09<2:19:20, 26.21s/it]2022-01-07 23:59:47,289 iteration 1378 : loss : 0.066577, loss_ce: 0.033644
2022-01-07 23:59:48,762 iteration 1379 : loss : 0.078313, loss_ce: 0.033863
2022-01-07 23:59:50,158 iteration 1380 : loss : 0.100874, loss_ce: 0.032804
2022-01-07 23:59:51,597 iteration 1381 : loss : 0.066299, loss_ce: 0.024250
2022-01-07 23:59:53,013 iteration 1382 : loss : 0.115425, loss_ce: 0.048421
2022-01-07 23:59:54,368 iteration 1383 : loss : 0.093787, loss_ce: 0.028302
2022-01-07 23:59:55,803 iteration 1384 : loss : 0.066872, loss_ce: 0.031048
2022-01-07 23:59:57,216 iteration 1385 : loss : 0.054889, loss_ce: 0.021689
2022-01-07 23:59:58,598 iteration 1386 : loss : 0.081452, loss_ce: 0.034846
2022-01-08 00:00:00,101 iteration 1387 : loss : 0.065248, loss_ce: 0.024173
2022-01-08 00:00:01,497 iteration 1388 : loss : 0.065587, loss_ce: 0.025667
2022-01-08 00:00:02,876 iteration 1389 : loss : 0.057230, loss_ce: 0.022733
2022-01-08 00:00:04,309 iteration 1390 : loss : 0.066393, loss_ce: 0.026600
2022-01-08 00:00:05,721 iteration 1391 : loss : 0.064314, loss_ce: 0.017409
2022-01-08 00:00:07,159 iteration 1392 : loss : 0.061884, loss_ce: 0.021384
2022-01-08 00:00:08,530 iteration 1393 : loss : 0.042214, loss_ce: 0.013836
2022-01-08 00:00:09,909 iteration 1394 : loss : 0.038974, loss_ce: 0.017199
 20%|██████▏                       | 82/400 [35:34<2:15:37, 25.59s/it]2022-01-08 00:00:11,340 iteration 1395 : loss : 0.047588, loss_ce: 0.019033
2022-01-08 00:00:12,719 iteration 1396 : loss : 0.053152, loss_ce: 0.022278
2022-01-08 00:00:14,123 iteration 1397 : loss : 0.082022, loss_ce: 0.026792
2022-01-08 00:00:15,608 iteration 1398 : loss : 0.049716, loss_ce: 0.021301
2022-01-08 00:00:17,031 iteration 1399 : loss : 0.078551, loss_ce: 0.038699
2022-01-08 00:00:18,420 iteration 1400 : loss : 0.084605, loss_ce: 0.045292
2022-01-08 00:00:19,804 iteration 1401 : loss : 0.077193, loss_ce: 0.030007
2022-01-08 00:00:21,123 iteration 1402 : loss : 0.055528, loss_ce: 0.022448
2022-01-08 00:00:22,614 iteration 1403 : loss : 0.056502, loss_ce: 0.022018
2022-01-08 00:00:24,056 iteration 1404 : loss : 0.062337, loss_ce: 0.022301
2022-01-08 00:00:25,509 iteration 1405 : loss : 0.053629, loss_ce: 0.022353
2022-01-08 00:00:26,917 iteration 1406 : loss : 0.067559, loss_ce: 0.024379
2022-01-08 00:00:28,261 iteration 1407 : loss : 0.052393, loss_ce: 0.024418
2022-01-08 00:00:29,626 iteration 1408 : loss : 0.066681, loss_ce: 0.022407
2022-01-08 00:00:31,133 iteration 1409 : loss : 0.099405, loss_ce: 0.031369
2022-01-08 00:00:32,558 iteration 1410 : loss : 0.056951, loss_ce: 0.019973
2022-01-08 00:00:33,934 iteration 1411 : loss : 0.071704, loss_ce: 0.027845
 21%|██████▏                       | 83/400 [35:58<2:12:42, 25.12s/it]2022-01-08 00:00:35,279 iteration 1412 : loss : 0.067372, loss_ce: 0.021544
2022-01-08 00:00:36,743 iteration 1413 : loss : 0.045688, loss_ce: 0.016606
2022-01-08 00:00:38,153 iteration 1414 : loss : 0.046613, loss_ce: 0.022559
2022-01-08 00:00:39,482 iteration 1415 : loss : 0.063264, loss_ce: 0.022514
2022-01-08 00:00:40,877 iteration 1416 : loss : 0.046852, loss_ce: 0.013973
2022-01-08 00:00:42,313 iteration 1417 : loss : 0.049794, loss_ce: 0.019662
2022-01-08 00:00:43,639 iteration 1418 : loss : 0.073175, loss_ce: 0.021556
2022-01-08 00:00:45,147 iteration 1419 : loss : 0.089013, loss_ce: 0.035603
2022-01-08 00:00:46,594 iteration 1420 : loss : 0.064898, loss_ce: 0.019191
2022-01-08 00:00:47,954 iteration 1421 : loss : 0.058002, loss_ce: 0.026587
2022-01-08 00:00:49,368 iteration 1422 : loss : 0.048933, loss_ce: 0.017301
2022-01-08 00:00:50,721 iteration 1423 : loss : 0.072908, loss_ce: 0.029714
2022-01-08 00:00:52,151 iteration 1424 : loss : 0.068158, loss_ce: 0.026786
2022-01-08 00:00:53,613 iteration 1425 : loss : 0.081367, loss_ce: 0.034895
2022-01-08 00:00:54,896 iteration 1426 : loss : 0.049405, loss_ce: 0.019246
2022-01-08 00:00:56,261 iteration 1427 : loss : 0.058121, loss_ce: 0.025263
2022-01-08 00:00:57,653 iteration 1428 : loss : 0.056268, loss_ce: 0.020929
 21%|██████▎                       | 84/400 [36:21<2:10:04, 24.70s/it]2022-01-08 00:00:59,184 iteration 1429 : loss : 0.070525, loss_ce: 0.026710
2022-01-08 00:01:00,541 iteration 1430 : loss : 0.079700, loss_ce: 0.030349
2022-01-08 00:01:01,922 iteration 1431 : loss : 0.050776, loss_ce: 0.023163
2022-01-08 00:01:03,325 iteration 1432 : loss : 0.060241, loss_ce: 0.023800
2022-01-08 00:01:04,807 iteration 1433 : loss : 0.080221, loss_ce: 0.031183
2022-01-08 00:01:06,262 iteration 1434 : loss : 0.062847, loss_ce: 0.023649
2022-01-08 00:01:07,571 iteration 1435 : loss : 0.060486, loss_ce: 0.020671
2022-01-08 00:01:09,025 iteration 1436 : loss : 0.056492, loss_ce: 0.024430
2022-01-08 00:01:10,348 iteration 1437 : loss : 0.116310, loss_ce: 0.028922
2022-01-08 00:01:11,647 iteration 1438 : loss : 0.052246, loss_ce: 0.021582
2022-01-08 00:01:13,051 iteration 1439 : loss : 0.043285, loss_ce: 0.013844
2022-01-08 00:01:14,523 iteration 1440 : loss : 0.073795, loss_ce: 0.031329
2022-01-08 00:01:15,882 iteration 1441 : loss : 0.042091, loss_ce: 0.015521
2022-01-08 00:01:17,219 iteration 1442 : loss : 0.066862, loss_ce: 0.030925
2022-01-08 00:01:18,635 iteration 1443 : loss : 0.065530, loss_ce: 0.023934
2022-01-08 00:01:20,094 iteration 1444 : loss : 0.044174, loss_ce: 0.021219
2022-01-08 00:01:20,094 Training Data Eval:
2022-01-08 00:01:27,002   Average segmentation loss on training set: 0.0462
2022-01-08 00:01:27,003 Validation Data Eval:
2022-01-08 00:01:29,378   Average segmentation loss on validation set: 0.0810
2022-01-08 00:01:33,522 Found new lowest validation loss at iteration 1444! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-08 00:01:34,836 iteration 1445 : loss : 0.057102, loss_ce: 0.018739
 21%|██████▍                       | 85/400 [36:59<2:29:20, 28.44s/it]2022-01-08 00:01:36,207 iteration 1446 : loss : 0.056293, loss_ce: 0.018469
2022-01-08 00:01:37,615 iteration 1447 : loss : 0.068125, loss_ce: 0.026558
2022-01-08 00:01:38,986 iteration 1448 : loss : 0.067181, loss_ce: 0.031801
2022-01-08 00:01:40,379 iteration 1449 : loss : 0.071899, loss_ce: 0.029348
2022-01-08 00:01:41,680 iteration 1450 : loss : 0.058506, loss_ce: 0.022191
2022-01-08 00:01:43,031 iteration 1451 : loss : 0.046337, loss_ce: 0.016608
2022-01-08 00:01:44,428 iteration 1452 : loss : 0.085171, loss_ce: 0.018289
2022-01-08 00:01:45,873 iteration 1453 : loss : 0.074325, loss_ce: 0.022303
2022-01-08 00:01:47,222 iteration 1454 : loss : 0.055440, loss_ce: 0.023397
2022-01-08 00:01:48,585 iteration 1455 : loss : 0.055920, loss_ce: 0.026177
2022-01-08 00:01:50,004 iteration 1456 : loss : 0.063812, loss_ce: 0.022757
2022-01-08 00:01:51,414 iteration 1457 : loss : 0.042007, loss_ce: 0.017864
2022-01-08 00:01:52,792 iteration 1458 : loss : 0.050020, loss_ce: 0.017413
2022-01-08 00:01:54,083 iteration 1459 : loss : 0.045514, loss_ce: 0.019369
2022-01-08 00:01:55,549 iteration 1460 : loss : 0.080056, loss_ce: 0.034380
2022-01-08 00:01:56,928 iteration 1461 : loss : 0.067253, loss_ce: 0.022320
2022-01-08 00:01:58,310 iteration 1462 : loss : 0.059715, loss_ce: 0.026489
 22%|██████▍                       | 86/400 [37:22<2:21:03, 26.95s/it]2022-01-08 00:01:59,719 iteration 1463 : loss : 0.054012, loss_ce: 0.021914
2022-01-08 00:02:01,167 iteration 1464 : loss : 0.057421, loss_ce: 0.019379
2022-01-08 00:02:02,637 iteration 1465 : loss : 0.084468, loss_ce: 0.037210
2022-01-08 00:02:04,029 iteration 1466 : loss : 0.070052, loss_ce: 0.022545
2022-01-08 00:02:05,404 iteration 1467 : loss : 0.066208, loss_ce: 0.023219
2022-01-08 00:02:06,816 iteration 1468 : loss : 0.056553, loss_ce: 0.019945
2022-01-08 00:02:08,258 iteration 1469 : loss : 0.050378, loss_ce: 0.019024
2022-01-08 00:02:09,657 iteration 1470 : loss : 0.056225, loss_ce: 0.020464
2022-01-08 00:02:10,980 iteration 1471 : loss : 0.060576, loss_ce: 0.016796
2022-01-08 00:02:12,357 iteration 1472 : loss : 0.057907, loss_ce: 0.018448
2022-01-08 00:02:13,838 iteration 1473 : loss : 0.057004, loss_ce: 0.031380
2022-01-08 00:02:15,172 iteration 1474 : loss : 0.070067, loss_ce: 0.028245
2022-01-08 00:02:16,553 iteration 1475 : loss : 0.066304, loss_ce: 0.023215
2022-01-08 00:02:17,988 iteration 1476 : loss : 0.075794, loss_ce: 0.042809
2022-01-08 00:02:19,394 iteration 1477 : loss : 0.065694, loss_ce: 0.021974
2022-01-08 00:02:20,839 iteration 1478 : loss : 0.056029, loss_ce: 0.019224
2022-01-08 00:02:22,249 iteration 1479 : loss : 0.088402, loss_ce: 0.036034
 22%|██████▌                       | 87/400 [37:46<2:15:53, 26.05s/it]2022-01-08 00:02:23,677 iteration 1480 : loss : 0.060028, loss_ce: 0.031520
2022-01-08 00:02:25,156 iteration 1481 : loss : 0.056867, loss_ce: 0.023376
2022-01-08 00:02:26,527 iteration 1482 : loss : 0.051466, loss_ce: 0.020923
2022-01-08 00:02:27,851 iteration 1483 : loss : 0.050182, loss_ce: 0.019873
2022-01-08 00:02:29,182 iteration 1484 : loss : 0.060289, loss_ce: 0.026715
2022-01-08 00:02:30,526 iteration 1485 : loss : 0.067563, loss_ce: 0.026188
2022-01-08 00:02:31,916 iteration 1486 : loss : 0.073174, loss_ce: 0.024550
2022-01-08 00:02:33,364 iteration 1487 : loss : 0.067914, loss_ce: 0.028554
2022-01-08 00:02:34,729 iteration 1488 : loss : 0.054044, loss_ce: 0.022987
2022-01-08 00:02:36,136 iteration 1489 : loss : 0.067127, loss_ce: 0.022425
2022-01-08 00:02:37,543 iteration 1490 : loss : 0.061606, loss_ce: 0.025339
2022-01-08 00:02:38,877 iteration 1491 : loss : 0.071722, loss_ce: 0.027393
2022-01-08 00:02:40,293 iteration 1492 : loss : 0.058498, loss_ce: 0.022321
2022-01-08 00:02:41,786 iteration 1493 : loss : 0.070537, loss_ce: 0.028046
2022-01-08 00:02:43,210 iteration 1494 : loss : 0.064862, loss_ce: 0.020636
2022-01-08 00:02:44,620 iteration 1495 : loss : 0.065047, loss_ce: 0.022458
2022-01-08 00:02:46,013 iteration 1496 : loss : 0.060528, loss_ce: 0.023847
 22%|██████▌                       | 88/400 [38:10<2:11:53, 25.36s/it]2022-01-08 00:02:47,407 iteration 1497 : loss : 0.045354, loss_ce: 0.018748
2022-01-08 00:02:48,689 iteration 1498 : loss : 0.041125, loss_ce: 0.017489
2022-01-08 00:02:50,028 iteration 1499 : loss : 0.062936, loss_ce: 0.025002
2022-01-08 00:02:51,538 iteration 1500 : loss : 0.081040, loss_ce: 0.027654
2022-01-08 00:02:52,851 iteration 1501 : loss : 0.053141, loss_ce: 0.017251
2022-01-08 00:02:54,243 iteration 1502 : loss : 0.064424, loss_ce: 0.024005
2022-01-08 00:02:55,727 iteration 1503 : loss : 0.060658, loss_ce: 0.022563
2022-01-08 00:02:57,106 iteration 1504 : loss : 0.063761, loss_ce: 0.029515
2022-01-08 00:02:58,449 iteration 1505 : loss : 0.054101, loss_ce: 0.020899
2022-01-08 00:02:59,881 iteration 1506 : loss : 0.059328, loss_ce: 0.020249
2022-01-08 00:03:01,242 iteration 1507 : loss : 0.044398, loss_ce: 0.020757
2022-01-08 00:03:02,646 iteration 1508 : loss : 0.113169, loss_ce: 0.021577
2022-01-08 00:03:04,017 iteration 1509 : loss : 0.056362, loss_ce: 0.024760
2022-01-08 00:03:05,377 iteration 1510 : loss : 0.064396, loss_ce: 0.020501
2022-01-08 00:03:06,855 iteration 1511 : loss : 0.087610, loss_ce: 0.051856
2022-01-08 00:03:08,278 iteration 1512 : loss : 0.065020, loss_ce: 0.022597
2022-01-08 00:03:09,682 iteration 1513 : loss : 0.073265, loss_ce: 0.028655
 22%|██████▋                       | 89/400 [38:33<2:08:49, 24.85s/it]2022-01-08 00:03:11,130 iteration 1514 : loss : 0.086749, loss_ce: 0.037620
2022-01-08 00:03:12,524 iteration 1515 : loss : 0.045977, loss_ce: 0.021069
2022-01-08 00:03:13,970 iteration 1516 : loss : 0.075774, loss_ce: 0.039604
2022-01-08 00:03:15,327 iteration 1517 : loss : 0.051487, loss_ce: 0.020873
2022-01-08 00:03:16,646 iteration 1518 : loss : 0.048892, loss_ce: 0.020509
2022-01-08 00:03:18,013 iteration 1519 : loss : 0.055163, loss_ce: 0.020912
2022-01-08 00:03:19,550 iteration 1520 : loss : 0.085043, loss_ce: 0.034796
2022-01-08 00:03:20,896 iteration 1521 : loss : 0.062006, loss_ce: 0.024809
2022-01-08 00:03:22,337 iteration 1522 : loss : 0.068516, loss_ce: 0.026802
2022-01-08 00:03:23,729 iteration 1523 : loss : 0.043868, loss_ce: 0.018008
2022-01-08 00:03:25,059 iteration 1524 : loss : 0.045397, loss_ce: 0.018002
2022-01-08 00:03:26,450 iteration 1525 : loss : 0.053770, loss_ce: 0.023686
2022-01-08 00:03:27,812 iteration 1526 : loss : 0.065622, loss_ce: 0.023533
2022-01-08 00:03:29,268 iteration 1527 : loss : 0.042685, loss_ce: 0.016513
2022-01-08 00:03:30,689 iteration 1528 : loss : 0.064597, loss_ce: 0.022219
2022-01-08 00:03:32,091 iteration 1529 : loss : 0.069581, loss_ce: 0.021346
2022-01-08 00:03:32,091 Training Data Eval:
2022-01-08 00:03:39,003   Average segmentation loss on training set: 0.0481
2022-01-08 00:03:39,003 Validation Data Eval:
2022-01-08 00:03:41,383   Average segmentation loss on validation set: 0.0855
2022-01-08 00:03:42,731 iteration 1530 : loss : 0.065479, loss_ce: 0.029163
 22%|██████▊                       | 90/400 [39:06<2:21:07, 27.32s/it]2022-01-08 00:03:44,215 iteration 1531 : loss : 0.068723, loss_ce: 0.029988
2022-01-08 00:03:45,639 iteration 1532 : loss : 0.074182, loss_ce: 0.022157
2022-01-08 00:03:46,980 iteration 1533 : loss : 0.051297, loss_ce: 0.020217
2022-01-08 00:03:48,349 iteration 1534 : loss : 0.043164, loss_ce: 0.020545
2022-01-08 00:03:49,707 iteration 1535 : loss : 0.062066, loss_ce: 0.025453
2022-01-08 00:03:51,112 iteration 1536 : loss : 0.059432, loss_ce: 0.019113
2022-01-08 00:03:52,466 iteration 1537 : loss : 0.088733, loss_ce: 0.049753
2022-01-08 00:03:53,742 iteration 1538 : loss : 0.075410, loss_ce: 0.019143
2022-01-08 00:03:55,068 iteration 1539 : loss : 0.057739, loss_ce: 0.026490
2022-01-08 00:03:56,483 iteration 1540 : loss : 0.056215, loss_ce: 0.026065
2022-01-08 00:03:57,916 iteration 1541 : loss : 0.058928, loss_ce: 0.023004
2022-01-08 00:03:59,403 iteration 1542 : loss : 0.060052, loss_ce: 0.026009
2022-01-08 00:04:00,800 iteration 1543 : loss : 0.093573, loss_ce: 0.024304
2022-01-08 00:04:02,226 iteration 1544 : loss : 0.051380, loss_ce: 0.022573
2022-01-08 00:04:03,547 iteration 1545 : loss : 0.076861, loss_ce: 0.034254
2022-01-08 00:04:04,908 iteration 1546 : loss : 0.064105, loss_ce: 0.023007
2022-01-08 00:04:06,316 iteration 1547 : loss : 0.048484, loss_ce: 0.020507
 23%|██████▊                       | 91/400 [39:30<2:14:54, 26.20s/it]2022-01-08 00:04:07,729 iteration 1548 : loss : 0.075929, loss_ce: 0.026202
2022-01-08 00:04:09,060 iteration 1549 : loss : 0.080602, loss_ce: 0.029392
2022-01-08 00:04:10,480 iteration 1550 : loss : 0.065061, loss_ce: 0.021346
2022-01-08 00:04:11,881 iteration 1551 : loss : 0.059301, loss_ce: 0.021859
2022-01-08 00:04:13,248 iteration 1552 : loss : 0.053746, loss_ce: 0.015785
2022-01-08 00:04:14,697 iteration 1553 : loss : 0.073370, loss_ce: 0.033959
2022-01-08 00:04:15,994 iteration 1554 : loss : 0.088186, loss_ce: 0.042903
2022-01-08 00:04:17,339 iteration 1555 : loss : 0.062089, loss_ce: 0.019298
2022-01-08 00:04:18,814 iteration 1556 : loss : 0.069742, loss_ce: 0.025496
2022-01-08 00:04:20,202 iteration 1557 : loss : 0.066054, loss_ce: 0.023879
2022-01-08 00:04:21,532 iteration 1558 : loss : 0.074234, loss_ce: 0.027092
2022-01-08 00:04:22,989 iteration 1559 : loss : 0.093159, loss_ce: 0.033949
2022-01-08 00:04:24,345 iteration 1560 : loss : 0.079493, loss_ce: 0.043124
2022-01-08 00:04:25,730 iteration 1561 : loss : 0.058087, loss_ce: 0.025046
2022-01-08 00:04:27,134 iteration 1562 : loss : 0.055441, loss_ce: 0.026632
2022-01-08 00:04:28,544 iteration 1563 : loss : 0.087131, loss_ce: 0.030840
2022-01-08 00:04:29,909 iteration 1564 : loss : 0.054398, loss_ce: 0.019930
 23%|██████▉                       | 92/400 [39:54<2:10:27, 25.41s/it]2022-01-08 00:04:31,291 iteration 1565 : loss : 0.052539, loss_ce: 0.026677
2022-01-08 00:04:32,743 iteration 1566 : loss : 0.073836, loss_ce: 0.022369
2022-01-08 00:04:34,135 iteration 1567 : loss : 0.070161, loss_ce: 0.032085
2022-01-08 00:04:35,625 iteration 1568 : loss : 0.066238, loss_ce: 0.024482
2022-01-08 00:04:37,014 iteration 1569 : loss : 0.074939, loss_ce: 0.033042
2022-01-08 00:04:38,436 iteration 1570 : loss : 0.055624, loss_ce: 0.017167
2022-01-08 00:04:39,847 iteration 1571 : loss : 0.083025, loss_ce: 0.028537
2022-01-08 00:04:41,323 iteration 1572 : loss : 0.077983, loss_ce: 0.025519
2022-01-08 00:04:42,630 iteration 1573 : loss : 0.039183, loss_ce: 0.017552
2022-01-08 00:04:44,048 iteration 1574 : loss : 0.055728, loss_ce: 0.022430
2022-01-08 00:04:45,532 iteration 1575 : loss : 0.047235, loss_ce: 0.018706
2022-01-08 00:04:46,987 iteration 1576 : loss : 0.038679, loss_ce: 0.015305
2022-01-08 00:04:48,302 iteration 1577 : loss : 0.055628, loss_ce: 0.021186
2022-01-08 00:04:49,664 iteration 1578 : loss : 0.048129, loss_ce: 0.016232
2022-01-08 00:04:51,139 iteration 1579 : loss : 0.068166, loss_ce: 0.022681
2022-01-08 00:04:52,562 iteration 1580 : loss : 0.081090, loss_ce: 0.027129
2022-01-08 00:04:54,074 iteration 1581 : loss : 0.054183, loss_ce: 0.021725
 23%|██████▉                       | 93/400 [40:18<2:08:07, 25.04s/it]2022-01-08 00:04:55,531 iteration 1582 : loss : 0.042135, loss_ce: 0.015272
2022-01-08 00:04:56,978 iteration 1583 : loss : 0.058550, loss_ce: 0.022333
2022-01-08 00:04:58,293 iteration 1584 : loss : 0.058992, loss_ce: 0.019170
2022-01-08 00:04:59,684 iteration 1585 : loss : 0.132759, loss_ce: 0.028731
2022-01-08 00:05:01,136 iteration 1586 : loss : 0.056654, loss_ce: 0.016506
2022-01-08 00:05:02,561 iteration 1587 : loss : 0.052366, loss_ce: 0.016159
2022-01-08 00:05:04,026 iteration 1588 : loss : 0.064430, loss_ce: 0.028757
2022-01-08 00:05:05,414 iteration 1589 : loss : 0.061084, loss_ce: 0.018990
2022-01-08 00:05:06,828 iteration 1590 : loss : 0.068294, loss_ce: 0.032577
2022-01-08 00:05:08,188 iteration 1591 : loss : 0.078827, loss_ce: 0.038781
2022-01-08 00:05:09,539 iteration 1592 : loss : 0.086573, loss_ce: 0.038577
2022-01-08 00:05:10,991 iteration 1593 : loss : 0.063434, loss_ce: 0.017786
2022-01-08 00:05:12,400 iteration 1594 : loss : 0.057694, loss_ce: 0.030716
2022-01-08 00:05:13,814 iteration 1595 : loss : 0.073303, loss_ce: 0.028935
2022-01-08 00:05:15,230 iteration 1596 : loss : 0.041815, loss_ce: 0.016273
2022-01-08 00:05:16,581 iteration 1597 : loss : 0.056551, loss_ce: 0.022343
2022-01-08 00:05:18,007 iteration 1598 : loss : 0.066560, loss_ce: 0.032370
 24%|███████                       | 94/400 [40:42<2:06:00, 24.71s/it]2022-01-08 00:05:19,465 iteration 1599 : loss : 0.053283, loss_ce: 0.023850
2022-01-08 00:05:20,854 iteration 1600 : loss : 0.069873, loss_ce: 0.028691
2022-01-08 00:05:22,239 iteration 1601 : loss : 0.087210, loss_ce: 0.029496
2022-01-08 00:05:23,598 iteration 1602 : loss : 0.075803, loss_ce: 0.037441
2022-01-08 00:05:25,061 iteration 1603 : loss : 0.068737, loss_ce: 0.028873
2022-01-08 00:05:26,451 iteration 1604 : loss : 0.058506, loss_ce: 0.025532
2022-01-08 00:05:27,941 iteration 1605 : loss : 0.055174, loss_ce: 0.020014
2022-01-08 00:05:29,382 iteration 1606 : loss : 0.053522, loss_ce: 0.014708
2022-01-08 00:05:30,774 iteration 1607 : loss : 0.062617, loss_ce: 0.026432
2022-01-08 00:05:32,199 iteration 1608 : loss : 0.040979, loss_ce: 0.015382
2022-01-08 00:05:33,647 iteration 1609 : loss : 0.053376, loss_ce: 0.019380
2022-01-08 00:05:35,032 iteration 1610 : loss : 0.074449, loss_ce: 0.030901
2022-01-08 00:05:36,444 iteration 1611 : loss : 0.076237, loss_ce: 0.032098
2022-01-08 00:05:37,843 iteration 1612 : loss : 0.066172, loss_ce: 0.032982
2022-01-08 00:05:39,275 iteration 1613 : loss : 0.063728, loss_ce: 0.025002
2022-01-08 00:05:40,686 iteration 1614 : loss : 0.064934, loss_ce: 0.024851
2022-01-08 00:05:40,686 Training Data Eval:
2022-01-08 00:05:47,608   Average segmentation loss on training set: 0.0459
2022-01-08 00:05:47,608 Validation Data Eval:
2022-01-08 00:05:49,992   Average segmentation loss on validation set: 0.1028
2022-01-08 00:05:51,435 iteration 1615 : loss : 0.062781, loss_ce: 0.031558
 24%|███████▏                      | 95/400 [41:15<2:18:53, 27.32s/it]2022-01-08 00:05:52,841 iteration 1616 : loss : 0.057957, loss_ce: 0.018528
2022-01-08 00:05:54,173 iteration 1617 : loss : 0.048416, loss_ce: 0.022400
2022-01-08 00:05:55,555 iteration 1618 : loss : 0.035543, loss_ce: 0.014822
2022-01-08 00:05:57,011 iteration 1619 : loss : 0.040796, loss_ce: 0.020494
2022-01-08 00:05:58,351 iteration 1620 : loss : 0.059956, loss_ce: 0.025353
2022-01-08 00:05:59,766 iteration 1621 : loss : 0.065692, loss_ce: 0.025532
2022-01-08 00:06:01,129 iteration 1622 : loss : 0.069678, loss_ce: 0.026006
2022-01-08 00:06:02,451 iteration 1623 : loss : 0.045877, loss_ce: 0.020614
2022-01-08 00:06:03,844 iteration 1624 : loss : 0.066479, loss_ce: 0.025710
2022-01-08 00:06:05,147 iteration 1625 : loss : 0.053269, loss_ce: 0.025880
2022-01-08 00:06:06,493 iteration 1626 : loss : 0.059849, loss_ce: 0.022047
2022-01-08 00:06:07,827 iteration 1627 : loss : 0.077498, loss_ce: 0.021217
2022-01-08 00:06:09,252 iteration 1628 : loss : 0.067585, loss_ce: 0.017899
2022-01-08 00:06:10,651 iteration 1629 : loss : 0.076593, loss_ce: 0.033390
2022-01-08 00:06:11,969 iteration 1630 : loss : 0.063731, loss_ce: 0.023867
2022-01-08 00:06:13,347 iteration 1631 : loss : 0.065788, loss_ce: 0.030810
2022-01-08 00:06:14,632 iteration 1632 : loss : 0.048778, loss_ce: 0.019434
 24%|███████▏                      | 96/400 [41:38<2:12:10, 26.09s/it]2022-01-08 00:06:16,141 iteration 1633 : loss : 0.055637, loss_ce: 0.022783
2022-01-08 00:06:17,553 iteration 1634 : loss : 0.045266, loss_ce: 0.017424
2022-01-08 00:06:18,920 iteration 1635 : loss : 0.060579, loss_ce: 0.028294
2022-01-08 00:06:20,206 iteration 1636 : loss : 0.051472, loss_ce: 0.019582
2022-01-08 00:06:21,676 iteration 1637 : loss : 0.069141, loss_ce: 0.025161
2022-01-08 00:06:23,079 iteration 1638 : loss : 0.044000, loss_ce: 0.016855
2022-01-08 00:06:24,526 iteration 1639 : loss : 0.055627, loss_ce: 0.021995
2022-01-08 00:06:25,911 iteration 1640 : loss : 0.053756, loss_ce: 0.024250
2022-01-08 00:06:27,346 iteration 1641 : loss : 0.059069, loss_ce: 0.023987
2022-01-08 00:06:28,837 iteration 1642 : loss : 0.099737, loss_ce: 0.037618
2022-01-08 00:06:30,252 iteration 1643 : loss : 0.038985, loss_ce: 0.015887
2022-01-08 00:06:31,660 iteration 1644 : loss : 0.066227, loss_ce: 0.028882
2022-01-08 00:06:32,973 iteration 1645 : loss : 0.062951, loss_ce: 0.020775
2022-01-08 00:06:34,343 iteration 1646 : loss : 0.051519, loss_ce: 0.020613
2022-01-08 00:06:35,800 iteration 1647 : loss : 0.068816, loss_ce: 0.041946
2022-01-08 00:06:37,180 iteration 1648 : loss : 0.054981, loss_ce: 0.023725
2022-01-08 00:06:38,679 iteration 1649 : loss : 0.085256, loss_ce: 0.019395
 24%|███████▎                      | 97/400 [42:02<2:08:38, 25.47s/it]2022-01-08 00:06:40,120 iteration 1650 : loss : 0.043718, loss_ce: 0.017180
2022-01-08 00:06:41,577 iteration 1651 : loss : 0.104820, loss_ce: 0.047972
2022-01-08 00:06:43,032 iteration 1652 : loss : 0.109598, loss_ce: 0.033114
2022-01-08 00:06:44,471 iteration 1653 : loss : 0.088034, loss_ce: 0.044283
2022-01-08 00:06:45,852 iteration 1654 : loss : 0.049912, loss_ce: 0.016883
2022-01-08 00:06:47,221 iteration 1655 : loss : 0.077217, loss_ce: 0.025393
2022-01-08 00:06:48,588 iteration 1656 : loss : 0.035609, loss_ce: 0.016289
2022-01-08 00:06:50,049 iteration 1657 : loss : 0.051639, loss_ce: 0.022801
2022-01-08 00:06:51,406 iteration 1658 : loss : 0.038353, loss_ce: 0.015573
2022-01-08 00:06:52,799 iteration 1659 : loss : 0.068960, loss_ce: 0.030687
2022-01-08 00:06:54,307 iteration 1660 : loss : 0.104592, loss_ce: 0.056279
2022-01-08 00:06:55,708 iteration 1661 : loss : 0.050932, loss_ce: 0.019101
2022-01-08 00:06:57,104 iteration 1662 : loss : 0.059829, loss_ce: 0.023252
2022-01-08 00:06:58,551 iteration 1663 : loss : 0.064101, loss_ce: 0.026733
2022-01-08 00:06:59,998 iteration 1664 : loss : 0.055422, loss_ce: 0.019256
2022-01-08 00:07:01,353 iteration 1665 : loss : 0.045759, loss_ce: 0.019348
2022-01-08 00:07:02,727 iteration 1666 : loss : 0.043748, loss_ce: 0.017914
 24%|███████▎                      | 98/400 [42:26<2:06:03, 25.05s/it]2022-01-08 00:07:04,155 iteration 1667 : loss : 0.053521, loss_ce: 0.025268
2022-01-08 00:07:05,605 iteration 1668 : loss : 0.065139, loss_ce: 0.028786
2022-01-08 00:07:06,984 iteration 1669 : loss : 0.040848, loss_ce: 0.017998
2022-01-08 00:07:08,369 iteration 1670 : loss : 0.054875, loss_ce: 0.016953
2022-01-08 00:07:09,812 iteration 1671 : loss : 0.050562, loss_ce: 0.019177
2022-01-08 00:07:11,178 iteration 1672 : loss : 0.047900, loss_ce: 0.015527
2022-01-08 00:07:12,522 iteration 1673 : loss : 0.037583, loss_ce: 0.014782
2022-01-08 00:07:13,971 iteration 1674 : loss : 0.041860, loss_ce: 0.016902
2022-01-08 00:07:15,355 iteration 1675 : loss : 0.049724, loss_ce: 0.019867
2022-01-08 00:07:16,793 iteration 1676 : loss : 0.046768, loss_ce: 0.020009
2022-01-08 00:07:18,241 iteration 1677 : loss : 0.065255, loss_ce: 0.027157
2022-01-08 00:07:19,666 iteration 1678 : loss : 0.079924, loss_ce: 0.024393
2022-01-08 00:07:21,133 iteration 1679 : loss : 0.059787, loss_ce: 0.018363
2022-01-08 00:07:22,549 iteration 1680 : loss : 0.040172, loss_ce: 0.019878
2022-01-08 00:07:23,934 iteration 1681 : loss : 0.077002, loss_ce: 0.019854
2022-01-08 00:07:25,292 iteration 1682 : loss : 0.057115, loss_ce: 0.026061
2022-01-08 00:07:26,717 iteration 1683 : loss : 0.073446, loss_ce: 0.032040
 25%|███████▍                      | 99/400 [42:50<2:04:03, 24.73s/it]2022-01-08 00:07:28,164 iteration 1684 : loss : 0.050382, loss_ce: 0.018835
2022-01-08 00:07:29,512 iteration 1685 : loss : 0.059703, loss_ce: 0.024428
2022-01-08 00:07:30,954 iteration 1686 : loss : 0.051546, loss_ce: 0.020101
2022-01-08 00:07:32,299 iteration 1687 : loss : 0.044956, loss_ce: 0.013739
2022-01-08 00:07:33,735 iteration 1688 : loss : 0.063561, loss_ce: 0.025942
2022-01-08 00:07:35,153 iteration 1689 : loss : 0.078383, loss_ce: 0.034620
2022-01-08 00:07:36,480 iteration 1690 : loss : 0.047607, loss_ce: 0.015228
2022-01-08 00:07:37,809 iteration 1691 : loss : 0.085401, loss_ce: 0.033541
2022-01-08 00:07:39,234 iteration 1692 : loss : 0.046979, loss_ce: 0.020329
2022-01-08 00:07:40,688 iteration 1693 : loss : 0.050315, loss_ce: 0.018670
2022-01-08 00:07:42,123 iteration 1694 : loss : 0.053924, loss_ce: 0.016838
2022-01-08 00:07:43,491 iteration 1695 : loss : 0.063869, loss_ce: 0.021020
2022-01-08 00:07:44,885 iteration 1696 : loss : 0.078865, loss_ce: 0.040225
2022-01-08 00:07:46,322 iteration 1697 : loss : 0.065922, loss_ce: 0.027355
2022-01-08 00:07:47,734 iteration 1698 : loss : 0.041193, loss_ce: 0.015269
2022-01-08 00:07:49,102 iteration 1699 : loss : 0.048721, loss_ce: 0.022903
2022-01-08 00:07:49,102 Training Data Eval:
2022-01-08 00:07:56,021   Average segmentation loss on training set: 0.0409
2022-01-08 00:07:56,022 Validation Data Eval:
2022-01-08 00:07:58,403   Average segmentation loss on validation set: 0.0958
2022-01-08 00:07:59,817 iteration 1700 : loss : 0.046471, loss_ce: 0.019187
 25%|███████▎                     | 100/400 [43:24<2:16:12, 27.24s/it]2022-01-08 00:08:01,237 iteration 1701 : loss : 0.042991, loss_ce: 0.018121
2022-01-08 00:08:02,633 iteration 1702 : loss : 0.051808, loss_ce: 0.020744
2022-01-08 00:08:04,122 iteration 1703 : loss : 0.078294, loss_ce: 0.033156
2022-01-08 00:08:05,503 iteration 1704 : loss : 0.045925, loss_ce: 0.020349
2022-01-08 00:08:06,923 iteration 1705 : loss : 0.057402, loss_ce: 0.022862
2022-01-08 00:08:08,382 iteration 1706 : loss : 0.057200, loss_ce: 0.025859
2022-01-08 00:08:09,772 iteration 1707 : loss : 0.045294, loss_ce: 0.022715
2022-01-08 00:08:11,161 iteration 1708 : loss : 0.108111, loss_ce: 0.033307
2022-01-08 00:08:12,541 iteration 1709 : loss : 0.058203, loss_ce: 0.026166
2022-01-08 00:08:13,970 iteration 1710 : loss : 0.072613, loss_ce: 0.026803
2022-01-08 00:08:15,362 iteration 1711 : loss : 0.066946, loss_ce: 0.031196
2022-01-08 00:08:16,701 iteration 1712 : loss : 0.041646, loss_ce: 0.018947
2022-01-08 00:08:18,112 iteration 1713 : loss : 0.056412, loss_ce: 0.015973
2022-01-08 00:08:19,576 iteration 1714 : loss : 0.059447, loss_ce: 0.015665
2022-01-08 00:08:21,024 iteration 1715 : loss : 0.061469, loss_ce: 0.027185
2022-01-08 00:08:22,369 iteration 1716 : loss : 0.051636, loss_ce: 0.015552
2022-01-08 00:08:23,807 iteration 1717 : loss : 0.066269, loss_ce: 0.023747
 25%|███████▎                     | 101/400 [43:48<2:10:52, 26.26s/it]2022-01-08 00:08:25,247 iteration 1718 : loss : 0.063794, loss_ce: 0.029580
2022-01-08 00:08:26,626 iteration 1719 : loss : 0.092599, loss_ce: 0.026771
2022-01-08 00:08:28,085 iteration 1720 : loss : 0.053508, loss_ce: 0.020326
2022-01-08 00:08:29,446 iteration 1721 : loss : 0.049872, loss_ce: 0.018364
2022-01-08 00:08:30,866 iteration 1722 : loss : 0.060177, loss_ce: 0.018366
2022-01-08 00:08:32,208 iteration 1723 : loss : 0.105169, loss_ce: 0.039371
2022-01-08 00:08:33,643 iteration 1724 : loss : 0.046755, loss_ce: 0.017845
2022-01-08 00:08:35,049 iteration 1725 : loss : 0.070857, loss_ce: 0.022548
2022-01-08 00:08:36,451 iteration 1726 : loss : 0.043389, loss_ce: 0.018510
2022-01-08 00:08:37,918 iteration 1727 : loss : 0.052502, loss_ce: 0.024983
2022-01-08 00:08:39,236 iteration 1728 : loss : 0.043353, loss_ce: 0.016832
2022-01-08 00:08:40,646 iteration 1729 : loss : 0.050440, loss_ce: 0.019415
2022-01-08 00:08:42,094 iteration 1730 : loss : 0.040285, loss_ce: 0.017384
2022-01-08 00:08:43,472 iteration 1731 : loss : 0.058623, loss_ce: 0.032395
2022-01-08 00:08:44,837 iteration 1732 : loss : 0.053655, loss_ce: 0.022986
2022-01-08 00:08:46,203 iteration 1733 : loss : 0.044465, loss_ce: 0.018533
2022-01-08 00:08:47,569 iteration 1734 : loss : 0.053501, loss_ce: 0.022627
 26%|███████▍                     | 102/400 [44:11<2:06:42, 25.51s/it]2022-01-08 00:08:49,117 iteration 1735 : loss : 0.042970, loss_ce: 0.015623
2022-01-08 00:08:50,480 iteration 1736 : loss : 0.066347, loss_ce: 0.025919
2022-01-08 00:08:52,021 iteration 1737 : loss : 0.067190, loss_ce: 0.028767
2022-01-08 00:08:53,419 iteration 1738 : loss : 0.047424, loss_ce: 0.024131
2022-01-08 00:08:54,749 iteration 1739 : loss : 0.046128, loss_ce: 0.018410
2022-01-08 00:08:56,184 iteration 1740 : loss : 0.053259, loss_ce: 0.021952
2022-01-08 00:08:57,608 iteration 1741 : loss : 0.037316, loss_ce: 0.012461
2022-01-08 00:08:59,065 iteration 1742 : loss : 0.057818, loss_ce: 0.022609
2022-01-08 00:09:00,387 iteration 1743 : loss : 0.059257, loss_ce: 0.019247
2022-01-08 00:09:01,849 iteration 1744 : loss : 0.073772, loss_ce: 0.026380
2022-01-08 00:09:03,297 iteration 1745 : loss : 0.078882, loss_ce: 0.031157
2022-01-08 00:09:04,727 iteration 1746 : loss : 0.061924, loss_ce: 0.029104
2022-01-08 00:09:06,090 iteration 1747 : loss : 0.050812, loss_ce: 0.023343
2022-01-08 00:09:07,503 iteration 1748 : loss : 0.051907, loss_ce: 0.023038
2022-01-08 00:09:08,797 iteration 1749 : loss : 0.056133, loss_ce: 0.017831
2022-01-08 00:09:10,225 iteration 1750 : loss : 0.137153, loss_ce: 0.035906
2022-01-08 00:09:11,711 iteration 1751 : loss : 0.067597, loss_ce: 0.024207
 26%|███████▍                     | 103/400 [44:35<2:04:15, 25.10s/it]2022-01-08 00:09:13,216 iteration 1752 : loss : 0.091160, loss_ce: 0.021457
2022-01-08 00:09:14,519 iteration 1753 : loss : 0.064973, loss_ce: 0.025061
2022-01-08 00:09:15,997 iteration 1754 : loss : 0.048159, loss_ce: 0.018063
2022-01-08 00:09:17,362 iteration 1755 : loss : 0.044683, loss_ce: 0.018601
2022-01-08 00:09:18,763 iteration 1756 : loss : 0.078376, loss_ce: 0.026005
2022-01-08 00:09:20,106 iteration 1757 : loss : 0.037327, loss_ce: 0.014294
2022-01-08 00:09:21,481 iteration 1758 : loss : 0.053366, loss_ce: 0.030459
2022-01-08 00:09:22,847 iteration 1759 : loss : 0.054323, loss_ce: 0.016288
2022-01-08 00:09:24,197 iteration 1760 : loss : 0.054967, loss_ce: 0.018891
2022-01-08 00:09:25,631 iteration 1761 : loss : 0.080010, loss_ce: 0.025919
2022-01-08 00:09:26,940 iteration 1762 : loss : 0.044283, loss_ce: 0.017600
2022-01-08 00:09:28,297 iteration 1763 : loss : 0.059961, loss_ce: 0.021415
2022-01-08 00:09:29,587 iteration 1764 : loss : 0.036875, loss_ce: 0.011839
2022-01-08 00:09:30,986 iteration 1765 : loss : 0.060131, loss_ce: 0.030843
2022-01-08 00:09:32,432 iteration 1766 : loss : 0.073134, loss_ce: 0.035789
2022-01-08 00:09:33,808 iteration 1767 : loss : 0.042329, loss_ce: 0.018019
2022-01-08 00:09:35,213 iteration 1768 : loss : 0.059831, loss_ce: 0.023799
 26%|███████▌                     | 104/400 [44:59<2:01:28, 24.62s/it]2022-01-08 00:09:36,645 iteration 1769 : loss : 0.058045, loss_ce: 0.023870
2022-01-08 00:09:38,002 iteration 1770 : loss : 0.047659, loss_ce: 0.019464
2022-01-08 00:09:39,311 iteration 1771 : loss : 0.057775, loss_ce: 0.022100
2022-01-08 00:09:40,661 iteration 1772 : loss : 0.046346, loss_ce: 0.022945
2022-01-08 00:09:42,087 iteration 1773 : loss : 0.092879, loss_ce: 0.027268
2022-01-08 00:09:43,491 iteration 1774 : loss : 0.048903, loss_ce: 0.017380
2022-01-08 00:09:44,914 iteration 1775 : loss : 0.052531, loss_ce: 0.020326
2022-01-08 00:09:46,284 iteration 1776 : loss : 0.044919, loss_ce: 0.022620
2022-01-08 00:09:47,625 iteration 1777 : loss : 0.037042, loss_ce: 0.014783
2022-01-08 00:09:48,942 iteration 1778 : loss : 0.065264, loss_ce: 0.025076
2022-01-08 00:09:50,364 iteration 1779 : loss : 0.045066, loss_ce: 0.015973
2022-01-08 00:09:51,755 iteration 1780 : loss : 0.070018, loss_ce: 0.031988
2022-01-08 00:09:53,097 iteration 1781 : loss : 0.051533, loss_ce: 0.019776
2022-01-08 00:09:54,512 iteration 1782 : loss : 0.065303, loss_ce: 0.025859
2022-01-08 00:09:55,913 iteration 1783 : loss : 0.056768, loss_ce: 0.022434
2022-01-08 00:09:57,266 iteration 1784 : loss : 0.060082, loss_ce: 0.021591
2022-01-08 00:09:57,266 Training Data Eval:
2022-01-08 00:10:04,173   Average segmentation loss on training set: 0.0383
2022-01-08 00:10:04,173 Validation Data Eval:
2022-01-08 00:10:06,556   Average segmentation loss on validation set: 0.1062
2022-01-08 00:10:07,983 iteration 1785 : loss : 0.055442, loss_ce: 0.020957
 26%|███████▌                     | 105/400 [45:32<2:13:04, 27.07s/it]2022-01-08 00:10:09,426 iteration 1786 : loss : 0.038340, loss_ce: 0.017646
2022-01-08 00:10:10,761 iteration 1787 : loss : 0.055519, loss_ce: 0.021006
2022-01-08 00:10:12,191 iteration 1788 : loss : 0.053323, loss_ce: 0.030037
2022-01-08 00:10:13,606 iteration 1789 : loss : 0.051781, loss_ce: 0.019386
2022-01-08 00:10:15,004 iteration 1790 : loss : 0.043650, loss_ce: 0.020332
2022-01-08 00:10:16,479 iteration 1791 : loss : 0.060722, loss_ce: 0.023706
2022-01-08 00:10:17,824 iteration 1792 : loss : 0.047327, loss_ce: 0.021098
2022-01-08 00:10:19,183 iteration 1793 : loss : 0.077937, loss_ce: 0.029178
2022-01-08 00:10:20,559 iteration 1794 : loss : 0.039307, loss_ce: 0.014130
2022-01-08 00:10:21,984 iteration 1795 : loss : 0.045190, loss_ce: 0.015922
2022-01-08 00:10:23,366 iteration 1796 : loss : 0.055504, loss_ce: 0.024279
2022-01-08 00:10:24,831 iteration 1797 : loss : 0.053380, loss_ce: 0.017765
2022-01-08 00:10:26,288 iteration 1798 : loss : 0.051243, loss_ce: 0.017183
2022-01-08 00:10:27,654 iteration 1799 : loss : 0.051235, loss_ce: 0.021940
2022-01-08 00:10:29,086 iteration 1800 : loss : 0.040983, loss_ce: 0.013188
2022-01-08 00:10:30,424 iteration 1801 : loss : 0.070126, loss_ce: 0.025421
2022-01-08 00:10:31,788 iteration 1802 : loss : 0.064769, loss_ce: 0.021038
 26%|███████▋                     | 106/400 [45:55<2:07:49, 26.09s/it]2022-01-08 00:10:33,288 iteration 1803 : loss : 0.048926, loss_ce: 0.018164
2022-01-08 00:10:34,775 iteration 1804 : loss : 0.044632, loss_ce: 0.013530
2022-01-08 00:10:36,138 iteration 1805 : loss : 0.040713, loss_ce: 0.011566
2022-01-08 00:10:37,495 iteration 1806 : loss : 0.048442, loss_ce: 0.021957
2022-01-08 00:10:38,852 iteration 1807 : loss : 0.078170, loss_ce: 0.028822
2022-01-08 00:10:40,243 iteration 1808 : loss : 0.042067, loss_ce: 0.018200
2022-01-08 00:10:41,596 iteration 1809 : loss : 0.039074, loss_ce: 0.015456
2022-01-08 00:10:42,885 iteration 1810 : loss : 0.037791, loss_ce: 0.014525
2022-01-08 00:10:44,312 iteration 1811 : loss : 0.044440, loss_ce: 0.019178
2022-01-08 00:10:45,759 iteration 1812 : loss : 0.057291, loss_ce: 0.020752
2022-01-08 00:10:47,290 iteration 1813 : loss : 0.049405, loss_ce: 0.019405
2022-01-08 00:10:48,710 iteration 1814 : loss : 0.046663, loss_ce: 0.021913
2022-01-08 00:10:50,048 iteration 1815 : loss : 0.059389, loss_ce: 0.019857
2022-01-08 00:10:51,352 iteration 1816 : loss : 0.044258, loss_ce: 0.018017
2022-01-08 00:10:52,821 iteration 1817 : loss : 0.075310, loss_ce: 0.028944
2022-01-08 00:10:54,206 iteration 1818 : loss : 0.070487, loss_ce: 0.033583
2022-01-08 00:10:55,633 iteration 1819 : loss : 0.049268, loss_ce: 0.021950
 27%|███████▊                     | 107/400 [46:19<2:04:06, 25.42s/it]2022-01-08 00:10:57,133 iteration 1820 : loss : 0.042391, loss_ce: 0.021997
2022-01-08 00:10:58,514 iteration 1821 : loss : 0.036033, loss_ce: 0.016486
2022-01-08 00:10:59,931 iteration 1822 : loss : 0.046978, loss_ce: 0.017219
2022-01-08 00:11:01,240 iteration 1823 : loss : 0.045685, loss_ce: 0.014795
2022-01-08 00:11:02,512 iteration 1824 : loss : 0.038242, loss_ce: 0.015372
2022-01-08 00:11:03,931 iteration 1825 : loss : 0.078948, loss_ce: 0.030385
2022-01-08 00:11:05,246 iteration 1826 : loss : 0.050962, loss_ce: 0.023402
2022-01-08 00:11:06,721 iteration 1827 : loss : 0.138617, loss_ce: 0.037650
2022-01-08 00:11:08,092 iteration 1828 : loss : 0.045271, loss_ce: 0.016759
2022-01-08 00:11:09,459 iteration 1829 : loss : 0.056529, loss_ce: 0.018353
2022-01-08 00:11:10,851 iteration 1830 : loss : 0.043555, loss_ce: 0.019310
2022-01-08 00:11:12,303 iteration 1831 : loss : 0.064490, loss_ce: 0.034010
2022-01-08 00:11:13,672 iteration 1832 : loss : 0.053075, loss_ce: 0.016315
2022-01-08 00:11:15,051 iteration 1833 : loss : 0.052753, loss_ce: 0.024613
2022-01-08 00:11:16,558 iteration 1834 : loss : 0.059914, loss_ce: 0.023651
2022-01-08 00:11:17,932 iteration 1835 : loss : 0.059901, loss_ce: 0.028245
2022-01-08 00:11:19,314 iteration 1836 : loss : 0.066264, loss_ce: 0.021367
 27%|███████▊                     | 108/400 [46:43<2:01:09, 24.90s/it]2022-01-08 00:11:20,790 iteration 1837 : loss : 0.055153, loss_ce: 0.025262
2022-01-08 00:11:22,240 iteration 1838 : loss : 0.047493, loss_ce: 0.022162
2022-01-08 00:11:23,653 iteration 1839 : loss : 0.060965, loss_ce: 0.024872
2022-01-08 00:11:25,031 iteration 1840 : loss : 0.044249, loss_ce: 0.019998
2022-01-08 00:11:26,508 iteration 1841 : loss : 0.062104, loss_ce: 0.026172
2022-01-08 00:11:28,017 iteration 1842 : loss : 0.072669, loss_ce: 0.027365
2022-01-08 00:11:29,405 iteration 1843 : loss : 0.044676, loss_ce: 0.020350
2022-01-08 00:11:30,856 iteration 1844 : loss : 0.066679, loss_ce: 0.027639
2022-01-08 00:11:32,211 iteration 1845 : loss : 0.046260, loss_ce: 0.018642
2022-01-08 00:11:33,735 iteration 1846 : loss : 0.046430, loss_ce: 0.018452
2022-01-08 00:11:35,202 iteration 1847 : loss : 0.074687, loss_ce: 0.028270
2022-01-08 00:11:36,635 iteration 1848 : loss : 0.050891, loss_ce: 0.018741
2022-01-08 00:11:37,961 iteration 1849 : loss : 0.049676, loss_ce: 0.022455
2022-01-08 00:11:39,394 iteration 1850 : loss : 0.067522, loss_ce: 0.019882
2022-01-08 00:11:40,850 iteration 1851 : loss : 0.064221, loss_ce: 0.027445
2022-01-08 00:11:42,279 iteration 1852 : loss : 0.039466, loss_ce: 0.016402
2022-01-08 00:11:43,717 iteration 1853 : loss : 0.066083, loss_ce: 0.015332
 27%|███████▉                     | 109/400 [47:07<2:00:01, 24.75s/it]2022-01-08 00:11:45,170 iteration 1854 : loss : 0.046916, loss_ce: 0.017263
2022-01-08 00:11:46,581 iteration 1855 : loss : 0.046151, loss_ce: 0.015528
2022-01-08 00:11:48,002 iteration 1856 : loss : 0.075841, loss_ce: 0.018377
2022-01-08 00:11:49,424 iteration 1857 : loss : 0.079359, loss_ce: 0.029148
2022-01-08 00:11:50,825 iteration 1858 : loss : 0.059422, loss_ce: 0.030547
2022-01-08 00:11:52,178 iteration 1859 : loss : 0.049195, loss_ce: 0.024223
2022-01-08 00:11:53,711 iteration 1860 : loss : 0.071941, loss_ce: 0.030034
2022-01-08 00:11:55,156 iteration 1861 : loss : 0.069678, loss_ce: 0.027776
2022-01-08 00:11:56,572 iteration 1862 : loss : 0.047233, loss_ce: 0.018755
2022-01-08 00:11:58,050 iteration 1863 : loss : 0.059525, loss_ce: 0.024866
2022-01-08 00:11:59,492 iteration 1864 : loss : 0.051467, loss_ce: 0.019348
2022-01-08 00:12:01,024 iteration 1865 : loss : 0.057338, loss_ce: 0.024922
2022-01-08 00:12:02,517 iteration 1866 : loss : 0.046629, loss_ce: 0.022960
2022-01-08 00:12:03,971 iteration 1867 : loss : 0.111063, loss_ce: 0.042769
2022-01-08 00:12:05,364 iteration 1868 : loss : 0.049152, loss_ce: 0.023142
2022-01-08 00:12:06,804 iteration 1869 : loss : 0.059478, loss_ce: 0.020313
2022-01-08 00:12:06,804 Training Data Eval:
2022-01-08 00:12:13,759   Average segmentation loss on training set: 0.0548
2022-01-08 00:12:13,759 Validation Data Eval:
2022-01-08 00:12:16,150   Average segmentation loss on validation set: 0.0835
2022-01-08 00:12:17,484 iteration 1870 : loss : 0.052546, loss_ce: 0.016726
 28%|███████▉                     | 110/400 [47:41<2:12:41, 27.45s/it]2022-01-08 00:12:18,915 iteration 1871 : loss : 0.045371, loss_ce: 0.021280
2022-01-08 00:12:20,293 iteration 1872 : loss : 0.050446, loss_ce: 0.016841
2022-01-08 00:12:21,717 iteration 1873 : loss : 0.058776, loss_ce: 0.024563
2022-01-08 00:12:23,191 iteration 1874 : loss : 0.066940, loss_ce: 0.029142
2022-01-08 00:12:24,702 iteration 1875 : loss : 0.046583, loss_ce: 0.024968
2022-01-08 00:12:26,205 iteration 1876 : loss : 0.075822, loss_ce: 0.023139
2022-01-08 00:12:27,692 iteration 1877 : loss : 0.054812, loss_ce: 0.021969
2022-01-08 00:12:29,066 iteration 1878 : loss : 0.052469, loss_ce: 0.021650
2022-01-08 00:12:30,446 iteration 1879 : loss : 0.054710, loss_ce: 0.024166
2022-01-08 00:12:31,805 iteration 1880 : loss : 0.044184, loss_ce: 0.013715
2022-01-08 00:12:33,175 iteration 1881 : loss : 0.040347, loss_ce: 0.013007
2022-01-08 00:12:34,595 iteration 1882 : loss : 0.060560, loss_ce: 0.016661
2022-01-08 00:12:35,953 iteration 1883 : loss : 0.040205, loss_ce: 0.016576
2022-01-08 00:12:37,460 iteration 1884 : loss : 0.057651, loss_ce: 0.023921
2022-01-08 00:12:38,836 iteration 1885 : loss : 0.045879, loss_ce: 0.015995
2022-01-08 00:12:40,287 iteration 1886 : loss : 0.049136, loss_ce: 0.021156
2022-01-08 00:12:41,669 iteration 1887 : loss : 0.058728, loss_ce: 0.025592
 28%|████████                     | 111/400 [48:05<2:07:30, 26.47s/it]2022-01-08 00:12:43,177 iteration 1888 : loss : 0.047694, loss_ce: 0.015729
2022-01-08 00:12:44,700 iteration 1889 : loss : 0.061180, loss_ce: 0.026267
2022-01-08 00:12:46,039 iteration 1890 : loss : 0.039427, loss_ce: 0.015069
2022-01-08 00:12:47,451 iteration 1891 : loss : 0.046695, loss_ce: 0.022431
2022-01-08 00:12:48,863 iteration 1892 : loss : 0.056457, loss_ce: 0.019159
2022-01-08 00:12:50,316 iteration 1893 : loss : 0.073014, loss_ce: 0.025825
2022-01-08 00:12:51,721 iteration 1894 : loss : 0.059717, loss_ce: 0.024811
2022-01-08 00:12:53,171 iteration 1895 : loss : 0.050690, loss_ce: 0.016516
2022-01-08 00:12:54,599 iteration 1896 : loss : 0.055905, loss_ce: 0.026317
2022-01-08 00:12:56,038 iteration 1897 : loss : 0.081930, loss_ce: 0.032205
2022-01-08 00:12:57,566 iteration 1898 : loss : 0.046384, loss_ce: 0.020021
2022-01-08 00:12:58,901 iteration 1899 : loss : 0.038764, loss_ce: 0.017103
2022-01-08 00:13:00,398 iteration 1900 : loss : 0.058719, loss_ce: 0.026939
2022-01-08 00:13:01,939 iteration 1901 : loss : 0.066909, loss_ce: 0.025104
2022-01-08 00:13:03,362 iteration 1902 : loss : 0.062428, loss_ce: 0.023067
2022-01-08 00:13:04,836 iteration 1903 : loss : 0.061491, loss_ce: 0.027039
2022-01-08 00:13:06,316 iteration 1904 : loss : 0.067769, loss_ce: 0.022877
 28%|████████                     | 112/400 [48:30<2:04:27, 25.93s/it]2022-01-08 00:13:07,843 iteration 1905 : loss : 0.053791, loss_ce: 0.022592
2022-01-08 00:13:09,292 iteration 1906 : loss : 0.049278, loss_ce: 0.015271
2022-01-08 00:13:10,706 iteration 1907 : loss : 0.049507, loss_ce: 0.019723
2022-01-08 00:13:12,277 iteration 1908 : loss : 0.055630, loss_ce: 0.018795
2022-01-08 00:13:13,751 iteration 1909 : loss : 0.047836, loss_ce: 0.018372
2022-01-08 00:13:15,224 iteration 1910 : loss : 0.128369, loss_ce: 0.027742
2022-01-08 00:13:16,723 iteration 1911 : loss : 0.044123, loss_ce: 0.015598
2022-01-08 00:13:18,074 iteration 1912 : loss : 0.041408, loss_ce: 0.014297
2022-01-08 00:13:19,496 iteration 1913 : loss : 0.048859, loss_ce: 0.022084
2022-01-08 00:13:20,949 iteration 1914 : loss : 0.044500, loss_ce: 0.020438
2022-01-08 00:13:22,412 iteration 1915 : loss : 0.039487, loss_ce: 0.015292
2022-01-08 00:13:23,836 iteration 1916 : loss : 0.056044, loss_ce: 0.022724
2022-01-08 00:13:25,263 iteration 1917 : loss : 0.050687, loss_ce: 0.021569
2022-01-08 00:13:26,728 iteration 1918 : loss : 0.053013, loss_ce: 0.018363
2022-01-08 00:13:28,122 iteration 1919 : loss : 0.046885, loss_ce: 0.020198
2022-01-08 00:13:29,506 iteration 1920 : loss : 0.050808, loss_ce: 0.019916
2022-01-08 00:13:31,015 iteration 1921 : loss : 0.054753, loss_ce: 0.024220
 28%|████████▏                    | 113/400 [48:55<2:02:15, 25.56s/it]2022-01-08 00:13:32,457 iteration 1922 : loss : 0.057897, loss_ce: 0.028337
2022-01-08 00:13:33,890 iteration 1923 : loss : 0.045532, loss_ce: 0.020459
2022-01-08 00:13:35,375 iteration 1924 : loss : 0.036147, loss_ce: 0.017136
2022-01-08 00:13:36,820 iteration 1925 : loss : 0.081869, loss_ce: 0.034452
2022-01-08 00:13:38,222 iteration 1926 : loss : 0.033209, loss_ce: 0.013536
2022-01-08 00:13:39,606 iteration 1927 : loss : 0.066256, loss_ce: 0.030994
2022-01-08 00:13:41,141 iteration 1928 : loss : 0.074372, loss_ce: 0.035933
2022-01-08 00:13:42,520 iteration 1929 : loss : 0.047220, loss_ce: 0.016491
2022-01-08 00:13:43,896 iteration 1930 : loss : 0.047201, loss_ce: 0.019071
2022-01-08 00:13:45,339 iteration 1931 : loss : 0.088424, loss_ce: 0.017877
2022-01-08 00:13:46,826 iteration 1932 : loss : 0.059812, loss_ce: 0.020388
2022-01-08 00:13:48,336 iteration 1933 : loss : 0.066818, loss_ce: 0.038723
2022-01-08 00:13:49,670 iteration 1934 : loss : 0.035469, loss_ce: 0.013883
2022-01-08 00:13:51,099 iteration 1935 : loss : 0.061653, loss_ce: 0.020691
2022-01-08 00:13:52,597 iteration 1936 : loss : 0.045594, loss_ce: 0.020387
2022-01-08 00:13:53,957 iteration 1937 : loss : 0.045842, loss_ce: 0.012191
2022-01-08 00:13:55,298 iteration 1938 : loss : 0.058942, loss_ce: 0.025045
 28%|████████▎                    | 114/400 [49:19<1:59:59, 25.17s/it]2022-01-08 00:13:56,727 iteration 1939 : loss : 0.038493, loss_ce: 0.012624
2022-01-08 00:13:58,284 iteration 1940 : loss : 0.063773, loss_ce: 0.028021
2022-01-08 00:13:59,813 iteration 1941 : loss : 0.055678, loss_ce: 0.019832
2022-01-08 00:14:01,278 iteration 1942 : loss : 0.066190, loss_ce: 0.024991
2022-01-08 00:14:02,684 iteration 1943 : loss : 0.043728, loss_ce: 0.017493
2022-01-08 00:14:04,123 iteration 1944 : loss : 0.042137, loss_ce: 0.015208
2022-01-08 00:14:05,504 iteration 1945 : loss : 0.048803, loss_ce: 0.017544
2022-01-08 00:14:06,918 iteration 1946 : loss : 0.048820, loss_ce: 0.018976
2022-01-08 00:14:08,378 iteration 1947 : loss : 0.055130, loss_ce: 0.025603
2022-01-08 00:14:09,804 iteration 1948 : loss : 0.090170, loss_ce: 0.027323
2022-01-08 00:14:11,203 iteration 1949 : loss : 0.040190, loss_ce: 0.015841
2022-01-08 00:14:12,556 iteration 1950 : loss : 0.029737, loss_ce: 0.012640
2022-01-08 00:14:14,040 iteration 1951 : loss : 0.062667, loss_ce: 0.022118
2022-01-08 00:14:15,504 iteration 1952 : loss : 0.058128, loss_ce: 0.018943
2022-01-08 00:14:16,990 iteration 1953 : loss : 0.047833, loss_ce: 0.022344
2022-01-08 00:14:18,423 iteration 1954 : loss : 0.024388, loss_ce: 0.011257
2022-01-08 00:14:18,423 Training Data Eval:
2022-01-08 00:14:25,362   Average segmentation loss on training set: 0.0350
2022-01-08 00:14:25,362 Validation Data Eval:
2022-01-08 00:14:27,742   Average segmentation loss on validation set: 0.0948
2022-01-08 00:14:29,188 iteration 1955 : loss : 0.070720, loss_ce: 0.024065
 29%|████████▎                    | 115/400 [49:53<2:12:00, 27.79s/it]2022-01-08 00:14:30,604 iteration 1956 : loss : 0.072262, loss_ce: 0.022554
2022-01-08 00:14:32,122 iteration 1957 : loss : 0.066710, loss_ce: 0.027074
2022-01-08 00:14:33,637 iteration 1958 : loss : 0.051544, loss_ce: 0.018400
2022-01-08 00:14:35,074 iteration 1959 : loss : 0.069590, loss_ce: 0.026376
2022-01-08 00:14:36,407 iteration 1960 : loss : 0.039564, loss_ce: 0.017201
2022-01-08 00:14:37,843 iteration 1961 : loss : 0.052455, loss_ce: 0.022470
2022-01-08 00:14:39,308 iteration 1962 : loss : 0.051429, loss_ce: 0.028996
2022-01-08 00:14:40,680 iteration 1963 : loss : 0.043180, loss_ce: 0.016657
2022-01-08 00:14:42,060 iteration 1964 : loss : 0.037017, loss_ce: 0.015006
2022-01-08 00:14:43,442 iteration 1965 : loss : 0.046742, loss_ce: 0.020155
2022-01-08 00:14:44,956 iteration 1966 : loss : 0.072355, loss_ce: 0.032804
2022-01-08 00:14:46,470 iteration 1967 : loss : 0.047989, loss_ce: 0.019143
2022-01-08 00:14:47,963 iteration 1968 : loss : 0.057014, loss_ce: 0.033836
2022-01-08 00:14:49,442 iteration 1969 : loss : 0.052687, loss_ce: 0.020476
2022-01-08 00:14:50,877 iteration 1970 : loss : 0.051597, loss_ce: 0.015653
2022-01-08 00:14:52,375 iteration 1971 : loss : 0.053354, loss_ce: 0.024689
2022-01-08 00:14:53,783 iteration 1972 : loss : 0.074966, loss_ce: 0.024751
 29%|████████▍                    | 116/400 [50:17<2:06:59, 26.83s/it]2022-01-08 00:14:55,116 iteration 1973 : loss : 0.038051, loss_ce: 0.015277
2022-01-08 00:14:56,562 iteration 1974 : loss : 0.047481, loss_ce: 0.017905
2022-01-08 00:14:57,995 iteration 1975 : loss : 0.045669, loss_ce: 0.012884
2022-01-08 00:14:59,514 iteration 1976 : loss : 0.067652, loss_ce: 0.034418
2022-01-08 00:15:00,954 iteration 1977 : loss : 0.038547, loss_ce: 0.012160
2022-01-08 00:15:02,305 iteration 1978 : loss : 0.034546, loss_ce: 0.014568
2022-01-08 00:15:03,648 iteration 1979 : loss : 0.042041, loss_ce: 0.017958
2022-01-08 00:15:05,101 iteration 1980 : loss : 0.050884, loss_ce: 0.022697
2022-01-08 00:15:06,488 iteration 1981 : loss : 0.067120, loss_ce: 0.026541
2022-01-08 00:15:07,823 iteration 1982 : loss : 0.059347, loss_ce: 0.022014
2022-01-08 00:15:09,338 iteration 1983 : loss : 0.051514, loss_ce: 0.024407
2022-01-08 00:15:10,689 iteration 1984 : loss : 0.040625, loss_ce: 0.010995
2022-01-08 00:15:12,116 iteration 1985 : loss : 0.032662, loss_ce: 0.012636
2022-01-08 00:15:13,563 iteration 1986 : loss : 0.057228, loss_ce: 0.027568
2022-01-08 00:15:15,075 iteration 1987 : loss : 0.033234, loss_ce: 0.012518
2022-01-08 00:15:16,453 iteration 1988 : loss : 0.054546, loss_ce: 0.015735
2022-01-08 00:15:17,878 iteration 1989 : loss : 0.049947, loss_ce: 0.026877
 29%|████████▍                    | 117/400 [50:42<2:02:41, 26.01s/it]2022-01-08 00:15:19,259 iteration 1990 : loss : 0.029777, loss_ce: 0.013654
2022-01-08 00:15:20,636 iteration 1991 : loss : 0.031018, loss_ce: 0.011471
2022-01-08 00:15:22,087 iteration 1992 : loss : 0.037458, loss_ce: 0.013445
2022-01-08 00:15:23,678 iteration 1993 : loss : 0.049237, loss_ce: 0.024026
2022-01-08 00:15:25,066 iteration 1994 : loss : 0.035816, loss_ce: 0.015184
2022-01-08 00:15:26,505 iteration 1995 : loss : 0.060647, loss_ce: 0.018056
2022-01-08 00:15:27,956 iteration 1996 : loss : 0.048288, loss_ce: 0.013758
2022-01-08 00:15:29,395 iteration 1997 : loss : 0.038371, loss_ce: 0.011567
2022-01-08 00:15:30,836 iteration 1998 : loss : 0.046211, loss_ce: 0.016057
2022-01-08 00:15:32,255 iteration 1999 : loss : 0.046992, loss_ce: 0.021904
2022-01-08 00:15:33,768 iteration 2000 : loss : 0.053457, loss_ce: 0.012900
2022-01-08 00:15:35,184 iteration 2001 : loss : 0.047028, loss_ce: 0.020419
2022-01-08 00:15:36,698 iteration 2002 : loss : 0.041336, loss_ce: 0.020225
2022-01-08 00:15:38,081 iteration 2003 : loss : 0.048149, loss_ce: 0.019137
2022-01-08 00:15:39,554 iteration 2004 : loss : 0.074851, loss_ce: 0.031514
2022-01-08 00:15:41,049 iteration 2005 : loss : 0.061967, loss_ce: 0.026505
2022-01-08 00:15:42,526 iteration 2006 : loss : 0.051872, loss_ce: 0.018568
 30%|████████▌                    | 118/400 [51:06<2:00:19, 25.60s/it]2022-01-08 00:15:44,038 iteration 2007 : loss : 0.047943, loss_ce: 0.021454
2022-01-08 00:15:45,430 iteration 2008 : loss : 0.045415, loss_ce: 0.017292
2022-01-08 00:15:46,859 iteration 2009 : loss : 0.068920, loss_ce: 0.022533
2022-01-08 00:15:48,222 iteration 2010 : loss : 0.045799, loss_ce: 0.019182
2022-01-08 00:15:49,622 iteration 2011 : loss : 0.052072, loss_ce: 0.022197
2022-01-08 00:15:51,064 iteration 2012 : loss : 0.058245, loss_ce: 0.021414
2022-01-08 00:15:52,558 iteration 2013 : loss : 0.043908, loss_ce: 0.016262
2022-01-08 00:15:54,043 iteration 2014 : loss : 0.038507, loss_ce: 0.014138
2022-01-08 00:15:55,388 iteration 2015 : loss : 0.044931, loss_ce: 0.016460
2022-01-08 00:15:56,813 iteration 2016 : loss : 0.055710, loss_ce: 0.019741
2022-01-08 00:15:58,130 iteration 2017 : loss : 0.031246, loss_ce: 0.012861
2022-01-08 00:15:59,554 iteration 2018 : loss : 0.064360, loss_ce: 0.025053
2022-01-08 00:16:00,920 iteration 2019 : loss : 0.037887, loss_ce: 0.017527
2022-01-08 00:16:02,361 iteration 2020 : loss : 0.054008, loss_ce: 0.023141
2022-01-08 00:16:03,822 iteration 2021 : loss : 0.041147, loss_ce: 0.015753
2022-01-08 00:16:05,315 iteration 2022 : loss : 0.048097, loss_ce: 0.023328
2022-01-08 00:16:06,724 iteration 2023 : loss : 0.061790, loss_ce: 0.019735
 30%|████████▋                    | 119/400 [51:30<1:57:55, 25.18s/it]2022-01-08 00:16:08,234 iteration 2024 : loss : 0.057612, loss_ce: 0.025895
2022-01-08 00:16:09,606 iteration 2025 : loss : 0.034030, loss_ce: 0.012274
2022-01-08 00:16:10,992 iteration 2026 : loss : 0.053932, loss_ce: 0.023287
2022-01-08 00:16:12,456 iteration 2027 : loss : 0.051853, loss_ce: 0.020049
2022-01-08 00:16:13,898 iteration 2028 : loss : 0.057687, loss_ce: 0.025284
2022-01-08 00:16:15,251 iteration 2029 : loss : 0.036427, loss_ce: 0.017073
2022-01-08 00:16:16,697 iteration 2030 : loss : 0.047308, loss_ce: 0.019022
2022-01-08 00:16:18,163 iteration 2031 : loss : 0.040558, loss_ce: 0.015677
2022-01-08 00:16:19,650 iteration 2032 : loss : 0.057286, loss_ce: 0.023395
2022-01-08 00:16:21,082 iteration 2033 : loss : 0.063802, loss_ce: 0.017910
2022-01-08 00:16:22,434 iteration 2034 : loss : 0.030189, loss_ce: 0.010936
2022-01-08 00:16:23,793 iteration 2035 : loss : 0.032154, loss_ce: 0.015719
2022-01-08 00:16:25,272 iteration 2036 : loss : 0.034320, loss_ce: 0.013129
2022-01-08 00:16:26,698 iteration 2037 : loss : 0.049139, loss_ce: 0.017267
2022-01-08 00:16:28,076 iteration 2038 : loss : 0.041184, loss_ce: 0.014601
2022-01-08 00:16:29,534 iteration 2039 : loss : 0.055351, loss_ce: 0.026010
2022-01-08 00:16:29,534 Training Data Eval:
2022-01-08 00:16:36,629   Average segmentation loss on training set: 0.0326
2022-01-08 00:16:36,630 Validation Data Eval:
2022-01-08 00:16:39,173   Average segmentation loss on validation set: 0.0719
2022-01-08 00:16:43,301 Found new lowest validation loss at iteration 2039! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-08 00:16:44,593 iteration 2040 : loss : 0.041908, loss_ce: 0.012913
 30%|████████▋                    | 120/400 [52:08<2:15:15, 28.99s/it]2022-01-08 00:16:45,925 iteration 2041 : loss : 0.046235, loss_ce: 0.019123
2022-01-08 00:16:47,289 iteration 2042 : loss : 0.033295, loss_ce: 0.013835
2022-01-08 00:16:48,693 iteration 2043 : loss : 0.043055, loss_ce: 0.020022
2022-01-08 00:16:50,147 iteration 2044 : loss : 0.043797, loss_ce: 0.018077
2022-01-08 00:16:51,629 iteration 2045 : loss : 0.039513, loss_ce: 0.017331
2022-01-08 00:16:53,088 iteration 2046 : loss : 0.062103, loss_ce: 0.023289
2022-01-08 00:16:54,511 iteration 2047 : loss : 0.089493, loss_ce: 0.035346
2022-01-08 00:16:55,842 iteration 2048 : loss : 0.046096, loss_ce: 0.016115
2022-01-08 00:16:57,219 iteration 2049 : loss : 0.044617, loss_ce: 0.018978
2022-01-08 00:16:58,666 iteration 2050 : loss : 0.065966, loss_ce: 0.023876
2022-01-08 00:17:00,123 iteration 2051 : loss : 0.081532, loss_ce: 0.032867
2022-01-08 00:17:01,482 iteration 2052 : loss : 0.035298, loss_ce: 0.018135
2022-01-08 00:17:02,838 iteration 2053 : loss : 0.057369, loss_ce: 0.026093
2022-01-08 00:17:04,244 iteration 2054 : loss : 0.089390, loss_ce: 0.032604
2022-01-08 00:17:05,685 iteration 2055 : loss : 0.058909, loss_ce: 0.026165
2022-01-08 00:17:07,092 iteration 2056 : loss : 0.060637, loss_ce: 0.025202
2022-01-08 00:17:08,525 iteration 2057 : loss : 0.048802, loss_ce: 0.019369
 30%|████████▊                    | 121/400 [52:32<2:07:44, 27.47s/it]2022-01-08 00:17:10,043 iteration 2058 : loss : 0.041595, loss_ce: 0.021525
2022-01-08 00:17:11,509 iteration 2059 : loss : 0.055846, loss_ce: 0.021257
2022-01-08 00:17:12,958 iteration 2060 : loss : 0.067510, loss_ce: 0.024440
2022-01-08 00:17:14,328 iteration 2061 : loss : 0.035361, loss_ce: 0.016626
2022-01-08 00:17:15,666 iteration 2062 : loss : 0.050483, loss_ce: 0.016552
2022-01-08 00:17:17,034 iteration 2063 : loss : 0.037631, loss_ce: 0.016722
2022-01-08 00:17:18,445 iteration 2064 : loss : 0.041674, loss_ce: 0.019831
2022-01-08 00:17:19,856 iteration 2065 : loss : 0.061462, loss_ce: 0.022333
2022-01-08 00:17:21,306 iteration 2066 : loss : 0.049687, loss_ce: 0.018027
2022-01-08 00:17:22,684 iteration 2067 : loss : 0.039997, loss_ce: 0.014421
2022-01-08 00:17:24,143 iteration 2068 : loss : 0.047532, loss_ce: 0.022344
2022-01-08 00:17:25,608 iteration 2069 : loss : 0.056023, loss_ce: 0.018498
2022-01-08 00:17:26,986 iteration 2070 : loss : 0.044168, loss_ce: 0.014925
2022-01-08 00:17:28,373 iteration 2071 : loss : 0.053289, loss_ce: 0.021945
2022-01-08 00:17:29,730 iteration 2072 : loss : 0.043416, loss_ce: 0.020480
2022-01-08 00:17:31,060 iteration 2073 : loss : 0.047259, loss_ce: 0.015828
2022-01-08 00:17:32,458 iteration 2074 : loss : 0.055345, loss_ce: 0.017044
 30%|████████▊                    | 122/400 [52:56<2:02:24, 26.42s/it]2022-01-08 00:17:33,941 iteration 2075 : loss : 0.038902, loss_ce: 0.013570
2022-01-08 00:17:35,438 iteration 2076 : loss : 0.063929, loss_ce: 0.020503
2022-01-08 00:17:36,856 iteration 2077 : loss : 0.046567, loss_ce: 0.016122
2022-01-08 00:17:38,210 iteration 2078 : loss : 0.041500, loss_ce: 0.017713
2022-01-08 00:17:39,582 iteration 2079 : loss : 0.043873, loss_ce: 0.016233
2022-01-08 00:17:41,062 iteration 2080 : loss : 0.048073, loss_ce: 0.015668
2022-01-08 00:17:42,489 iteration 2081 : loss : 0.045083, loss_ce: 0.018126
2022-01-08 00:17:43,890 iteration 2082 : loss : 0.052937, loss_ce: 0.020205
2022-01-08 00:17:45,272 iteration 2083 : loss : 0.038572, loss_ce: 0.016233
2022-01-08 00:17:46,664 iteration 2084 : loss : 0.053045, loss_ce: 0.020220
2022-01-08 00:17:48,045 iteration 2085 : loss : 0.061424, loss_ce: 0.024453
2022-01-08 00:17:49,429 iteration 2086 : loss : 0.032390, loss_ce: 0.014615
2022-01-08 00:17:50,777 iteration 2087 : loss : 0.047795, loss_ce: 0.017876
2022-01-08 00:17:52,285 iteration 2088 : loss : 0.070373, loss_ce: 0.033589
2022-01-08 00:17:53,689 iteration 2089 : loss : 0.042048, loss_ce: 0.015696
2022-01-08 00:17:55,132 iteration 2090 : loss : 0.042898, loss_ce: 0.014701
2022-01-08 00:17:56,525 iteration 2091 : loss : 0.038109, loss_ce: 0.015075
 31%|████████▉                    | 123/400 [53:20<1:58:39, 25.70s/it]2022-01-08 00:17:57,951 iteration 2092 : loss : 0.036240, loss_ce: 0.013684
2022-01-08 00:17:59,473 iteration 2093 : loss : 0.045553, loss_ce: 0.022797
2022-01-08 00:18:00,914 iteration 2094 : loss : 0.051500, loss_ce: 0.022244
2022-01-08 00:18:02,345 iteration 2095 : loss : 0.039215, loss_ce: 0.016148
2022-01-08 00:18:03,799 iteration 2096 : loss : 0.049333, loss_ce: 0.017948
2022-01-08 00:18:05,169 iteration 2097 : loss : 0.035862, loss_ce: 0.017677
2022-01-08 00:18:06,631 iteration 2098 : loss : 0.053524, loss_ce: 0.018935
2022-01-08 00:18:08,079 iteration 2099 : loss : 0.055264, loss_ce: 0.020455
2022-01-08 00:18:09,581 iteration 2100 : loss : 0.067312, loss_ce: 0.029768
2022-01-08 00:18:11,003 iteration 2101 : loss : 0.062456, loss_ce: 0.016991
2022-01-08 00:18:12,382 iteration 2102 : loss : 0.045413, loss_ce: 0.016361
2022-01-08 00:18:13,889 iteration 2103 : loss : 0.044884, loss_ce: 0.018311
2022-01-08 00:18:15,341 iteration 2104 : loss : 0.028609, loss_ce: 0.010938
2022-01-08 00:18:16,723 iteration 2105 : loss : 0.049777, loss_ce: 0.016244
2022-01-08 00:18:18,248 iteration 2106 : loss : 0.087133, loss_ce: 0.031333
2022-01-08 00:18:19,688 iteration 2107 : loss : 0.043020, loss_ce: 0.016681
2022-01-08 00:18:21,115 iteration 2108 : loss : 0.052001, loss_ce: 0.017403
 31%|████████▉                    | 124/400 [53:45<1:56:41, 25.37s/it]2022-01-08 00:18:22,562 iteration 2109 : loss : 0.055604, loss_ce: 0.018803
2022-01-08 00:18:23,984 iteration 2110 : loss : 0.039892, loss_ce: 0.012150
2022-01-08 00:18:25,511 iteration 2111 : loss : 0.086304, loss_ce: 0.032867
2022-01-08 00:18:27,053 iteration 2112 : loss : 0.046463, loss_ce: 0.019816
2022-01-08 00:18:28,440 iteration 2113 : loss : 0.092987, loss_ce: 0.046375
2022-01-08 00:18:29,851 iteration 2114 : loss : 0.052625, loss_ce: 0.018401
2022-01-08 00:18:31,150 iteration 2115 : loss : 0.061375, loss_ce: 0.020484
2022-01-08 00:18:32,576 iteration 2116 : loss : 0.050201, loss_ce: 0.023359
2022-01-08 00:18:34,051 iteration 2117 : loss : 0.050737, loss_ce: 0.017653
2022-01-08 00:18:35,454 iteration 2118 : loss : 0.062200, loss_ce: 0.027213
2022-01-08 00:18:36,913 iteration 2119 : loss : 0.065666, loss_ce: 0.019210
2022-01-08 00:18:38,323 iteration 2120 : loss : 0.059955, loss_ce: 0.024965
2022-01-08 00:18:39,829 iteration 2121 : loss : 0.052101, loss_ce: 0.023067
2022-01-08 00:18:41,187 iteration 2122 : loss : 0.044041, loss_ce: 0.017221
2022-01-08 00:18:42,581 iteration 2123 : loss : 0.071553, loss_ce: 0.038601
2022-01-08 00:18:44,017 iteration 2124 : loss : 0.067546, loss_ce: 0.026465
2022-01-08 00:18:44,017 Training Data Eval:
2022-01-08 00:18:51,065   Average segmentation loss on training set: 0.0521
2022-01-08 00:18:51,065 Validation Data Eval:
2022-01-08 00:18:53,461   Average segmentation loss on validation set: 0.0790
2022-01-08 00:18:54,879 iteration 2125 : loss : 0.030140, loss_ce: 0.010837
 31%|█████████                    | 125/400 [54:19<2:07:49, 27.89s/it]2022-01-08 00:18:56,313 iteration 2126 : loss : 0.027906, loss_ce: 0.010330
2022-01-08 00:18:57,782 iteration 2127 : loss : 0.038267, loss_ce: 0.014688
2022-01-08 00:18:59,137 iteration 2128 : loss : 0.045498, loss_ce: 0.023688
2022-01-08 00:19:00,574 iteration 2129 : loss : 0.047665, loss_ce: 0.020830
2022-01-08 00:19:01,922 iteration 2130 : loss : 0.058523, loss_ce: 0.018507
2022-01-08 00:19:03,401 iteration 2131 : loss : 0.100068, loss_ce: 0.043738
2022-01-08 00:19:04,879 iteration 2132 : loss : 0.058466, loss_ce: 0.022719
2022-01-08 00:19:06,356 iteration 2133 : loss : 0.071389, loss_ce: 0.028698
2022-01-08 00:19:07,809 iteration 2134 : loss : 0.090941, loss_ce: 0.033813
2022-01-08 00:19:09,233 iteration 2135 : loss : 0.049687, loss_ce: 0.017538
2022-01-08 00:19:10,653 iteration 2136 : loss : 0.055101, loss_ce: 0.019915
2022-01-08 00:19:12,088 iteration 2137 : loss : 0.085507, loss_ce: 0.026063
2022-01-08 00:19:13,463 iteration 2138 : loss : 0.073528, loss_ce: 0.024241
2022-01-08 00:19:14,792 iteration 2139 : loss : 0.044911, loss_ce: 0.020919
2022-01-08 00:19:16,165 iteration 2140 : loss : 0.039014, loss_ce: 0.011711
2022-01-08 00:19:17,575 iteration 2141 : loss : 0.054648, loss_ce: 0.023229
2022-01-08 00:19:19,002 iteration 2142 : loss : 0.062157, loss_ce: 0.031258
 32%|█████████▏                   | 126/400 [54:43<2:02:12, 26.76s/it]2022-01-08 00:19:20,473 iteration 2143 : loss : 0.047950, loss_ce: 0.014716
2022-01-08 00:19:21,869 iteration 2144 : loss : 0.052437, loss_ce: 0.016978
2022-01-08 00:19:23,188 iteration 2145 : loss : 0.036140, loss_ce: 0.016060
2022-01-08 00:19:24,779 iteration 2146 : loss : 0.051405, loss_ce: 0.018117
2022-01-08 00:19:26,204 iteration 2147 : loss : 0.040494, loss_ce: 0.017808
2022-01-08 00:19:27,614 iteration 2148 : loss : 0.045281, loss_ce: 0.018924
2022-01-08 00:19:29,038 iteration 2149 : loss : 0.055600, loss_ce: 0.019580
2022-01-08 00:19:30,533 iteration 2150 : loss : 0.055386, loss_ce: 0.019924
2022-01-08 00:19:31,996 iteration 2151 : loss : 0.046535, loss_ce: 0.017239
2022-01-08 00:19:33,345 iteration 2152 : loss : 0.035346, loss_ce: 0.013394
2022-01-08 00:19:34,718 iteration 2153 : loss : 0.045842, loss_ce: 0.012465
2022-01-08 00:19:36,065 iteration 2154 : loss : 0.044317, loss_ce: 0.021194
2022-01-08 00:19:37,425 iteration 2155 : loss : 0.048849, loss_ce: 0.017120
2022-01-08 00:19:38,861 iteration 2156 : loss : 0.057457, loss_ce: 0.021986
2022-01-08 00:19:40,279 iteration 2157 : loss : 0.060965, loss_ce: 0.032695
2022-01-08 00:19:41,657 iteration 2158 : loss : 0.042179, loss_ce: 0.017588
2022-01-08 00:19:43,140 iteration 2159 : loss : 0.067738, loss_ce: 0.021533
 32%|█████████▏                   | 127/400 [55:07<1:58:10, 25.97s/it]2022-01-08 00:19:44,531 iteration 2160 : loss : 0.030811, loss_ce: 0.010414
2022-01-08 00:19:45,948 iteration 2161 : loss : 0.037370, loss_ce: 0.014559
2022-01-08 00:19:47,352 iteration 2162 : loss : 0.036264, loss_ce: 0.013563
2022-01-08 00:19:48,672 iteration 2163 : loss : 0.069679, loss_ce: 0.024138
2022-01-08 00:19:50,085 iteration 2164 : loss : 0.044963, loss_ce: 0.020705
2022-01-08 00:19:51,485 iteration 2165 : loss : 0.032404, loss_ce: 0.010716
2022-01-08 00:19:52,940 iteration 2166 : loss : 0.053120, loss_ce: 0.026135
2022-01-08 00:19:54,373 iteration 2167 : loss : 0.041181, loss_ce: 0.018460
2022-01-08 00:19:55,778 iteration 2168 : loss : 0.048250, loss_ce: 0.019421
2022-01-08 00:19:57,200 iteration 2169 : loss : 0.050922, loss_ce: 0.018683
2022-01-08 00:19:58,576 iteration 2170 : loss : 0.063078, loss_ce: 0.025645
2022-01-08 00:19:59,968 iteration 2171 : loss : 0.025132, loss_ce: 0.007847
2022-01-08 00:20:01,269 iteration 2172 : loss : 0.050971, loss_ce: 0.028808
2022-01-08 00:20:02,667 iteration 2173 : loss : 0.049532, loss_ce: 0.017001
2022-01-08 00:20:04,103 iteration 2174 : loss : 0.065857, loss_ce: 0.030295
2022-01-08 00:20:05,456 iteration 2175 : loss : 0.054013, loss_ce: 0.018688
2022-01-08 00:20:06,850 iteration 2176 : loss : 0.046032, loss_ce: 0.022237
 32%|█████████▎                   | 128/400 [55:31<1:54:39, 25.29s/it]2022-01-08 00:20:08,346 iteration 2177 : loss : 0.051588, loss_ce: 0.016014
2022-01-08 00:20:09,695 iteration 2178 : loss : 0.035667, loss_ce: 0.017210
2022-01-08 00:20:11,081 iteration 2179 : loss : 0.039368, loss_ce: 0.017015
2022-01-08 00:20:12,536 iteration 2180 : loss : 0.046552, loss_ce: 0.015991
2022-01-08 00:20:13,918 iteration 2181 : loss : 0.040562, loss_ce: 0.014164
2022-01-08 00:20:15,297 iteration 2182 : loss : 0.043695, loss_ce: 0.015888
2022-01-08 00:20:16,652 iteration 2183 : loss : 0.058470, loss_ce: 0.021530
2022-01-08 00:20:17,988 iteration 2184 : loss : 0.044561, loss_ce: 0.020575
2022-01-08 00:20:19,307 iteration 2185 : loss : 0.036648, loss_ce: 0.013958
2022-01-08 00:20:20,809 iteration 2186 : loss : 0.069772, loss_ce: 0.032350
2022-01-08 00:20:22,140 iteration 2187 : loss : 0.039330, loss_ce: 0.019500
2022-01-08 00:20:23,573 iteration 2188 : loss : 0.043386, loss_ce: 0.017148
2022-01-08 00:20:24,942 iteration 2189 : loss : 0.055812, loss_ce: 0.020741
2022-01-08 00:20:26,389 iteration 2190 : loss : 0.041257, loss_ce: 0.015233
2022-01-08 00:20:27,714 iteration 2191 : loss : 0.033062, loss_ce: 0.014154
2022-01-08 00:20:29,105 iteration 2192 : loss : 0.057291, loss_ce: 0.021671
2022-01-08 00:20:30,598 iteration 2193 : loss : 0.047092, loss_ce: 0.018472
 32%|█████████▎                   | 129/400 [55:54<1:52:09, 24.83s/it]2022-01-08 00:20:32,008 iteration 2194 : loss : 0.055928, loss_ce: 0.021599
2022-01-08 00:20:33,368 iteration 2195 : loss : 0.039462, loss_ce: 0.012560
2022-01-08 00:20:34,774 iteration 2196 : loss : 0.040679, loss_ce: 0.019206
2022-01-08 00:20:36,208 iteration 2197 : loss : 0.061451, loss_ce: 0.020710
2022-01-08 00:20:37,596 iteration 2198 : loss : 0.040832, loss_ce: 0.018861
2022-01-08 00:20:39,144 iteration 2199 : loss : 0.037495, loss_ce: 0.016786
2022-01-08 00:20:40,541 iteration 2200 : loss : 0.058239, loss_ce: 0.022343
2022-01-08 00:20:41,949 iteration 2201 : loss : 0.054086, loss_ce: 0.016514
2022-01-08 00:20:43,364 iteration 2202 : loss : 0.118886, loss_ce: 0.036292
2022-01-08 00:20:44,799 iteration 2203 : loss : 0.033591, loss_ce: 0.011149
2022-01-08 00:20:46,249 iteration 2204 : loss : 0.040425, loss_ce: 0.015072
2022-01-08 00:20:47,553 iteration 2205 : loss : 0.034503, loss_ce: 0.013699
2022-01-08 00:20:48,892 iteration 2206 : loss : 0.031797, loss_ce: 0.014696
2022-01-08 00:20:50,290 iteration 2207 : loss : 0.078283, loss_ce: 0.035848
2022-01-08 00:20:51,670 iteration 2208 : loss : 0.043277, loss_ce: 0.017793
2022-01-08 00:20:53,126 iteration 2209 : loss : 0.074480, loss_ce: 0.023485
2022-01-08 00:20:53,127 Training Data Eval:
2022-01-08 00:21:00,183   Average segmentation loss on training set: 0.0439
2022-01-08 00:21:00,184 Validation Data Eval:
2022-01-08 00:21:02,571   Average segmentation loss on validation set: 0.1403
2022-01-08 00:21:03,977 iteration 2210 : loss : 0.055332, loss_ce: 0.020429
 32%|█████████▍                   | 130/400 [56:28<2:03:16, 27.39s/it]2022-01-08 00:21:05,415 iteration 2211 : loss : 0.057276, loss_ce: 0.030348
2022-01-08 00:21:06,749 iteration 2212 : loss : 0.032588, loss_ce: 0.011406
2022-01-08 00:21:08,088 iteration 2213 : loss : 0.032718, loss_ce: 0.011771
2022-01-08 00:21:09,491 iteration 2214 : loss : 0.039921, loss_ce: 0.016748
2022-01-08 00:21:10,869 iteration 2215 : loss : 0.038190, loss_ce: 0.014202
2022-01-08 00:21:12,225 iteration 2216 : loss : 0.041196, loss_ce: 0.019291
2022-01-08 00:21:13,553 iteration 2217 : loss : 0.040929, loss_ce: 0.018043
2022-01-08 00:21:14,977 iteration 2218 : loss : 0.056025, loss_ce: 0.027264
2022-01-08 00:21:16,351 iteration 2219 : loss : 0.044575, loss_ce: 0.018080
2022-01-08 00:21:17,757 iteration 2220 : loss : 0.030059, loss_ce: 0.015490
2022-01-08 00:21:19,249 iteration 2221 : loss : 0.038438, loss_ce: 0.017809
2022-01-08 00:21:20,622 iteration 2222 : loss : 0.060742, loss_ce: 0.019910
2022-01-08 00:21:22,132 iteration 2223 : loss : 0.066632, loss_ce: 0.023957
2022-01-08 00:21:23,501 iteration 2224 : loss : 0.077807, loss_ce: 0.022078
2022-01-08 00:21:24,900 iteration 2225 : loss : 0.042178, loss_ce: 0.015877
2022-01-08 00:21:26,430 iteration 2226 : loss : 0.042911, loss_ce: 0.016660
2022-01-08 00:21:27,877 iteration 2227 : loss : 0.049297, loss_ce: 0.014469
 33%|█████████▍                   | 131/400 [56:52<1:58:07, 26.35s/it]2022-01-08 00:21:29,361 iteration 2228 : loss : 0.049740, loss_ce: 0.016372
2022-01-08 00:21:30,802 iteration 2229 : loss : 0.053999, loss_ce: 0.021561
2022-01-08 00:21:32,193 iteration 2230 : loss : 0.053574, loss_ce: 0.020234
2022-01-08 00:21:33,539 iteration 2231 : loss : 0.032554, loss_ce: 0.014222
2022-01-08 00:21:34,886 iteration 2232 : loss : 0.035118, loss_ce: 0.012827
2022-01-08 00:21:36,241 iteration 2233 : loss : 0.035385, loss_ce: 0.010475
2022-01-08 00:21:37,676 iteration 2234 : loss : 0.060177, loss_ce: 0.021507
2022-01-08 00:21:39,007 iteration 2235 : loss : 0.055777, loss_ce: 0.026282
2022-01-08 00:21:40,359 iteration 2236 : loss : 0.034235, loss_ce: 0.014526
2022-01-08 00:21:41,746 iteration 2237 : loss : 0.040243, loss_ce: 0.016000
2022-01-08 00:21:43,129 iteration 2238 : loss : 0.064505, loss_ce: 0.020783
2022-01-08 00:21:44,547 iteration 2239 : loss : 0.037252, loss_ce: 0.013292
2022-01-08 00:21:45,870 iteration 2240 : loss : 0.041264, loss_ce: 0.014846
2022-01-08 00:21:47,295 iteration 2241 : loss : 0.037216, loss_ce: 0.014984
2022-01-08 00:21:48,636 iteration 2242 : loss : 0.040386, loss_ce: 0.017380
2022-01-08 00:21:49,994 iteration 2243 : loss : 0.040666, loss_ce: 0.012705
2022-01-08 00:21:51,369 iteration 2244 : loss : 0.028302, loss_ce: 0.011302
 33%|█████████▌                   | 132/400 [57:15<1:53:51, 25.49s/it]2022-01-08 00:21:52,763 iteration 2245 : loss : 0.031985, loss_ce: 0.012809
2022-01-08 00:21:54,177 iteration 2246 : loss : 0.040989, loss_ce: 0.016558
2022-01-08 00:21:55,599 iteration 2247 : loss : 0.044803, loss_ce: 0.017915
2022-01-08 00:21:56,894 iteration 2248 : loss : 0.026122, loss_ce: 0.010444
2022-01-08 00:21:58,341 iteration 2249 : loss : 0.052344, loss_ce: 0.021489
2022-01-08 00:21:59,857 iteration 2250 : loss : 0.070175, loss_ce: 0.037626
2022-01-08 00:22:01,196 iteration 2251 : loss : 0.060155, loss_ce: 0.028581
2022-01-08 00:22:02,618 iteration 2252 : loss : 0.042894, loss_ce: 0.014780
2022-01-08 00:22:03,965 iteration 2253 : loss : 0.054701, loss_ce: 0.017464
2022-01-08 00:22:05,410 iteration 2254 : loss : 0.059847, loss_ce: 0.024814
2022-01-08 00:22:06,856 iteration 2255 : loss : 0.029967, loss_ce: 0.014370
2022-01-08 00:22:08,365 iteration 2256 : loss : 0.046407, loss_ce: 0.018536
2022-01-08 00:22:09,743 iteration 2257 : loss : 0.044380, loss_ce: 0.016289
2022-01-08 00:22:11,134 iteration 2258 : loss : 0.067527, loss_ce: 0.027224
2022-01-08 00:22:12,574 iteration 2259 : loss : 0.047173, loss_ce: 0.016154
2022-01-08 00:22:14,008 iteration 2260 : loss : 0.050123, loss_ce: 0.017714
2022-01-08 00:22:15,357 iteration 2261 : loss : 0.048808, loss_ce: 0.012395
 33%|█████████▋                   | 133/400 [57:39<1:51:25, 25.04s/it]2022-01-08 00:22:16,915 iteration 2262 : loss : 0.045695, loss_ce: 0.020282
2022-01-08 00:22:18,331 iteration 2263 : loss : 0.034388, loss_ce: 0.013353
2022-01-08 00:22:19,790 iteration 2264 : loss : 0.042233, loss_ce: 0.012351
2022-01-08 00:22:21,216 iteration 2265 : loss : 0.037962, loss_ce: 0.014991
2022-01-08 00:22:22,607 iteration 2266 : loss : 0.042227, loss_ce: 0.016689
2022-01-08 00:22:24,006 iteration 2267 : loss : 0.053911, loss_ce: 0.015878
2022-01-08 00:22:25,336 iteration 2268 : loss : 0.035668, loss_ce: 0.014607
2022-01-08 00:22:26,759 iteration 2269 : loss : 0.046112, loss_ce: 0.020892
2022-01-08 00:22:28,106 iteration 2270 : loss : 0.034980, loss_ce: 0.013266
2022-01-08 00:22:29,464 iteration 2271 : loss : 0.081126, loss_ce: 0.050007
2022-01-08 00:22:30,807 iteration 2272 : loss : 0.042130, loss_ce: 0.012757
2022-01-08 00:22:32,265 iteration 2273 : loss : 0.052653, loss_ce: 0.015792
2022-01-08 00:22:33,595 iteration 2274 : loss : 0.043945, loss_ce: 0.013711
2022-01-08 00:22:34,963 iteration 2275 : loss : 0.033836, loss_ce: 0.014491
2022-01-08 00:22:36,355 iteration 2276 : loss : 0.059898, loss_ce: 0.018068
2022-01-08 00:22:37,742 iteration 2277 : loss : 0.039582, loss_ce: 0.017653
2022-01-08 00:22:39,126 iteration 2278 : loss : 0.041861, loss_ce: 0.017032
 34%|█████████▋                   | 134/400 [58:03<1:49:19, 24.66s/it]2022-01-08 00:22:40,644 iteration 2279 : loss : 0.053787, loss_ce: 0.025012
2022-01-08 00:22:42,071 iteration 2280 : loss : 0.033186, loss_ce: 0.015029
2022-01-08 00:22:43,405 iteration 2281 : loss : 0.034828, loss_ce: 0.013496
2022-01-08 00:22:44,906 iteration 2282 : loss : 0.058784, loss_ce: 0.023857
2022-01-08 00:22:46,261 iteration 2283 : loss : 0.036243, loss_ce: 0.013940
2022-01-08 00:22:47,750 iteration 2284 : loss : 0.051688, loss_ce: 0.017542
2022-01-08 00:22:49,108 iteration 2285 : loss : 0.036514, loss_ce: 0.013961
2022-01-08 00:22:50,500 iteration 2286 : loss : 0.032545, loss_ce: 0.012156
2022-01-08 00:22:51,931 iteration 2287 : loss : 0.055400, loss_ce: 0.016586
2022-01-08 00:22:53,251 iteration 2288 : loss : 0.042266, loss_ce: 0.016151
2022-01-08 00:22:54,658 iteration 2289 : loss : 0.051271, loss_ce: 0.018405
2022-01-08 00:22:56,084 iteration 2290 : loss : 0.034191, loss_ce: 0.012863
2022-01-08 00:22:57,464 iteration 2291 : loss : 0.038531, loss_ce: 0.013215
2022-01-08 00:22:58,917 iteration 2292 : loss : 0.046017, loss_ce: 0.015512
2022-01-08 00:23:00,241 iteration 2293 : loss : 0.056150, loss_ce: 0.024454
2022-01-08 00:23:01,637 iteration 2294 : loss : 0.037222, loss_ce: 0.016153
2022-01-08 00:23:01,638 Training Data Eval:
2022-01-08 00:23:08,697   Average segmentation loss on training set: 0.0315
2022-01-08 00:23:08,697 Validation Data Eval:
2022-01-08 00:23:11,087   Average segmentation loss on validation set: 0.0636
2022-01-08 00:23:16,752 Found new lowest validation loss at iteration 2294! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-08 00:23:18,045 iteration 2295 : loss : 0.028746, loss_ce: 0.010342
 34%|█████████▊                   | 135/400 [58:42<2:07:47, 28.94s/it]2022-01-08 00:23:19,368 iteration 2296 : loss : 0.041840, loss_ce: 0.015599
2022-01-08 00:23:20,678 iteration 2297 : loss : 0.056229, loss_ce: 0.017875
2022-01-08 00:23:22,137 iteration 2298 : loss : 0.047649, loss_ce: 0.017820
2022-01-08 00:23:23,518 iteration 2299 : loss : 0.039627, loss_ce: 0.011515
2022-01-08 00:23:24,989 iteration 2300 : loss : 0.042969, loss_ce: 0.016185
2022-01-08 00:23:26,396 iteration 2301 : loss : 0.040184, loss_ce: 0.017908
2022-01-08 00:23:27,772 iteration 2302 : loss : 0.040717, loss_ce: 0.019309
2022-01-08 00:23:29,120 iteration 2303 : loss : 0.047981, loss_ce: 0.014150
2022-01-08 00:23:30,428 iteration 2304 : loss : 0.062053, loss_ce: 0.026039
2022-01-08 00:23:31,872 iteration 2305 : loss : 0.061336, loss_ce: 0.020140
2022-01-08 00:23:33,227 iteration 2306 : loss : 0.063111, loss_ce: 0.024412
2022-01-08 00:23:34,555 iteration 2307 : loss : 0.035053, loss_ce: 0.013993
2022-01-08 00:23:35,915 iteration 2308 : loss : 0.047719, loss_ce: 0.024312
2022-01-08 00:23:37,328 iteration 2309 : loss : 0.036465, loss_ce: 0.014066
2022-01-08 00:23:38,767 iteration 2310 : loss : 0.042661, loss_ce: 0.019256
2022-01-08 00:23:40,244 iteration 2311 : loss : 0.061331, loss_ce: 0.022748
2022-01-08 00:23:41,652 iteration 2312 : loss : 0.054120, loss_ce: 0.024549
 34%|█████████▊                   | 136/400 [59:05<2:00:17, 27.34s/it]2022-01-08 00:23:43,081 iteration 2313 : loss : 0.043754, loss_ce: 0.020553
2022-01-08 00:23:44,443 iteration 2314 : loss : 0.059665, loss_ce: 0.027842
2022-01-08 00:23:45,837 iteration 2315 : loss : 0.044382, loss_ce: 0.018336
2022-01-08 00:23:47,261 iteration 2316 : loss : 0.046258, loss_ce: 0.023032
2022-01-08 00:23:48,574 iteration 2317 : loss : 0.061046, loss_ce: 0.024867
2022-01-08 00:23:50,056 iteration 2318 : loss : 0.030847, loss_ce: 0.014144
2022-01-08 00:23:51,536 iteration 2319 : loss : 0.077897, loss_ce: 0.025530
2022-01-08 00:23:52,881 iteration 2320 : loss : 0.043752, loss_ce: 0.021110
2022-01-08 00:23:54,265 iteration 2321 : loss : 0.055828, loss_ce: 0.017081
2022-01-08 00:23:55,667 iteration 2322 : loss : 0.041378, loss_ce: 0.016184
2022-01-08 00:23:57,159 iteration 2323 : loss : 0.061532, loss_ce: 0.019739
2022-01-08 00:23:58,572 iteration 2324 : loss : 0.044741, loss_ce: 0.013892
2022-01-08 00:23:59,961 iteration 2325 : loss : 0.060840, loss_ce: 0.030226
2022-01-08 00:24:01,396 iteration 2326 : loss : 0.038618, loss_ce: 0.022581
2022-01-08 00:24:02,796 iteration 2327 : loss : 0.031337, loss_ce: 0.011996
2022-01-08 00:24:04,280 iteration 2328 : loss : 0.054294, loss_ce: 0.021673
2022-01-08 00:24:05,668 iteration 2329 : loss : 0.054086, loss_ce: 0.020777
 34%|█████████▉                   | 137/400 [59:29<1:55:27, 26.34s/it]2022-01-08 00:24:07,085 iteration 2330 : loss : 0.039069, loss_ce: 0.015393
2022-01-08 00:24:08,444 iteration 2331 : loss : 0.045476, loss_ce: 0.014642
2022-01-08 00:24:09,936 iteration 2332 : loss : 0.046686, loss_ce: 0.020584
2022-01-08 00:24:11,267 iteration 2333 : loss : 0.044640, loss_ce: 0.022958
2022-01-08 00:24:12,646 iteration 2334 : loss : 0.066081, loss_ce: 0.013498
2022-01-08 00:24:14,168 iteration 2335 : loss : 0.050311, loss_ce: 0.022293
2022-01-08 00:24:15,527 iteration 2336 : loss : 0.042164, loss_ce: 0.020067
2022-01-08 00:24:16,917 iteration 2337 : loss : 0.040882, loss_ce: 0.014748
2022-01-08 00:24:18,345 iteration 2338 : loss : 0.031599, loss_ce: 0.012244
2022-01-08 00:24:19,737 iteration 2339 : loss : 0.105330, loss_ce: 0.037020
2022-01-08 00:24:21,177 iteration 2340 : loss : 0.040526, loss_ce: 0.017030
2022-01-08 00:24:22,574 iteration 2341 : loss : 0.042521, loss_ce: 0.015192
2022-01-08 00:24:24,000 iteration 2342 : loss : 0.030509, loss_ce: 0.010525
2022-01-08 00:24:25,402 iteration 2343 : loss : 0.032910, loss_ce: 0.011909
2022-01-08 00:24:26,732 iteration 2344 : loss : 0.034872, loss_ce: 0.011589
2022-01-08 00:24:28,082 iteration 2345 : loss : 0.035008, loss_ce: 0.013747
2022-01-08 00:24:29,400 iteration 2346 : loss : 0.049433, loss_ce: 0.016838
 34%|██████████                   | 138/400 [59:53<1:51:36, 25.56s/it]2022-01-08 00:24:30,875 iteration 2347 : loss : 0.031666, loss_ce: 0.010328
2022-01-08 00:24:32,265 iteration 2348 : loss : 0.043824, loss_ce: 0.016995
2022-01-08 00:24:33,700 iteration 2349 : loss : 0.050954, loss_ce: 0.024945
2022-01-08 00:24:35,016 iteration 2350 : loss : 0.037855, loss_ce: 0.018905
2022-01-08 00:24:36,423 iteration 2351 : loss : 0.046824, loss_ce: 0.014281
2022-01-08 00:24:37,908 iteration 2352 : loss : 0.043761, loss_ce: 0.021668
2022-01-08 00:24:39,243 iteration 2353 : loss : 0.033300, loss_ce: 0.014324
2022-01-08 00:24:40,644 iteration 2354 : loss : 0.040422, loss_ce: 0.013075
2022-01-08 00:24:41,981 iteration 2355 : loss : 0.040836, loss_ce: 0.018140
2022-01-08 00:24:43,466 iteration 2356 : loss : 0.052937, loss_ce: 0.019760
2022-01-08 00:24:44,958 iteration 2357 : loss : 0.039886, loss_ce: 0.016991
2022-01-08 00:24:46,440 iteration 2358 : loss : 0.057783, loss_ce: 0.021401
2022-01-08 00:24:47,887 iteration 2359 : loss : 0.066621, loss_ce: 0.016512
2022-01-08 00:24:49,188 iteration 2360 : loss : 0.072101, loss_ce: 0.020172
2022-01-08 00:24:50,530 iteration 2361 : loss : 0.028731, loss_ce: 0.010633
2022-01-08 00:24:51,921 iteration 2362 : loss : 0.061063, loss_ce: 0.023362
2022-01-08 00:24:53,310 iteration 2363 : loss : 0.043857, loss_ce: 0.015514
 35%|█████████▍                 | 139/400 [1:00:17<1:49:01, 25.06s/it]2022-01-08 00:24:54,743 iteration 2364 : loss : 0.075696, loss_ce: 0.033381
2022-01-08 00:24:56,072 iteration 2365 : loss : 0.040871, loss_ce: 0.014208
2022-01-08 00:24:57,494 iteration 2366 : loss : 0.034858, loss_ce: 0.014637
2022-01-08 00:24:58,853 iteration 2367 : loss : 0.057638, loss_ce: 0.016849
2022-01-08 00:25:00,330 iteration 2368 : loss : 0.047538, loss_ce: 0.012500
2022-01-08 00:25:01,707 iteration 2369 : loss : 0.056286, loss_ce: 0.018387
2022-01-08 00:25:03,077 iteration 2370 : loss : 0.032630, loss_ce: 0.010497
2022-01-08 00:25:04,473 iteration 2371 : loss : 0.050888, loss_ce: 0.022280
2022-01-08 00:25:05,794 iteration 2372 : loss : 0.034061, loss_ce: 0.017109
2022-01-08 00:25:07,188 iteration 2373 : loss : 0.033591, loss_ce: 0.013489
2022-01-08 00:25:08,657 iteration 2374 : loss : 0.036358, loss_ce: 0.013344
2022-01-08 00:25:10,004 iteration 2375 : loss : 0.043771, loss_ce: 0.013899
2022-01-08 00:25:11,444 iteration 2376 : loss : 0.039158, loss_ce: 0.017056
2022-01-08 00:25:12,828 iteration 2377 : loss : 0.036989, loss_ce: 0.010760
2022-01-08 00:25:14,325 iteration 2378 : loss : 0.048170, loss_ce: 0.023344
2022-01-08 00:25:15,752 iteration 2379 : loss : 0.049323, loss_ce: 0.016633
2022-01-08 00:25:15,753 Training Data Eval:
2022-01-08 00:25:22,802   Average segmentation loss on training set: 0.0266
2022-01-08 00:25:22,802 Validation Data Eval:
2022-01-08 00:25:25,192   Average segmentation loss on validation set: 0.0686
2022-01-08 00:25:26,737 iteration 2380 : loss : 0.034412, loss_ce: 0.011780
 35%|█████████▍                 | 140/400 [1:00:50<1:59:28, 27.57s/it]2022-01-08 00:25:28,127 iteration 2381 : loss : 0.040728, loss_ce: 0.013508
2022-01-08 00:25:29,574 iteration 2382 : loss : 0.041556, loss_ce: 0.019825
2022-01-08 00:25:31,023 iteration 2383 : loss : 0.038727, loss_ce: 0.015850
2022-01-08 00:25:32,431 iteration 2384 : loss : 0.045884, loss_ce: 0.014725
2022-01-08 00:25:33,868 iteration 2385 : loss : 0.037591, loss_ce: 0.016468
2022-01-08 00:25:35,350 iteration 2386 : loss : 0.042569, loss_ce: 0.014158
2022-01-08 00:25:36,717 iteration 2387 : loss : 0.047562, loss_ce: 0.016424
2022-01-08 00:25:38,142 iteration 2388 : loss : 0.025630, loss_ce: 0.010298
2022-01-08 00:25:39,516 iteration 2389 : loss : 0.036211, loss_ce: 0.015457
2022-01-08 00:25:40,919 iteration 2390 : loss : 0.027772, loss_ce: 0.012351
2022-01-08 00:25:42,272 iteration 2391 : loss : 0.042491, loss_ce: 0.014104
2022-01-08 00:25:43,606 iteration 2392 : loss : 0.032170, loss_ce: 0.014071
2022-01-08 00:25:44,958 iteration 2393 : loss : 0.039346, loss_ce: 0.018561
2022-01-08 00:25:46,369 iteration 2394 : loss : 0.054998, loss_ce: 0.012698
2022-01-08 00:25:47,893 iteration 2395 : loss : 0.050566, loss_ce: 0.015878
2022-01-08 00:25:49,367 iteration 2396 : loss : 0.026268, loss_ce: 0.010353
2022-01-08 00:25:50,794 iteration 2397 : loss : 0.036885, loss_ce: 0.012812
 35%|█████████▌                 | 141/400 [1:01:15<1:54:28, 26.52s/it]2022-01-08 00:25:52,351 iteration 2398 : loss : 0.063852, loss_ce: 0.034761
2022-01-08 00:25:53,683 iteration 2399 : loss : 0.028139, loss_ce: 0.011643
2022-01-08 00:25:55,060 iteration 2400 : loss : 0.032276, loss_ce: 0.008942
2022-01-08 00:25:56,462 iteration 2401 : loss : 0.042716, loss_ce: 0.015539
2022-01-08 00:25:57,850 iteration 2402 : loss : 0.040945, loss_ce: 0.017710
2022-01-08 00:25:59,238 iteration 2403 : loss : 0.031159, loss_ce: 0.011210
2022-01-08 00:26:00,600 iteration 2404 : loss : 0.044017, loss_ce: 0.024489
2022-01-08 00:26:01,992 iteration 2405 : loss : 0.037087, loss_ce: 0.012789
2022-01-08 00:26:03,479 iteration 2406 : loss : 0.035357, loss_ce: 0.014285
2022-01-08 00:26:04,821 iteration 2407 : loss : 0.031587, loss_ce: 0.011738
2022-01-08 00:26:06,205 iteration 2408 : loss : 0.059628, loss_ce: 0.028749
2022-01-08 00:26:07,602 iteration 2409 : loss : 0.056584, loss_ce: 0.022558
2022-01-08 00:26:09,060 iteration 2410 : loss : 0.058434, loss_ce: 0.016701
2022-01-08 00:26:10,403 iteration 2411 : loss : 0.044477, loss_ce: 0.017315
2022-01-08 00:26:11,851 iteration 2412 : loss : 0.049370, loss_ce: 0.018518
2022-01-08 00:26:13,331 iteration 2413 : loss : 0.053556, loss_ce: 0.026776
2022-01-08 00:26:14,782 iteration 2414 : loss : 0.053946, loss_ce: 0.024902
 36%|█████████▌                 | 142/400 [1:01:38<1:50:45, 25.76s/it]2022-01-08 00:26:16,238 iteration 2415 : loss : 0.067228, loss_ce: 0.018593
2022-01-08 00:26:17,631 iteration 2416 : loss : 0.049185, loss_ce: 0.023037
2022-01-08 00:26:19,045 iteration 2417 : loss : 0.046017, loss_ce: 0.016300
2022-01-08 00:26:20,475 iteration 2418 : loss : 0.053006, loss_ce: 0.016982
2022-01-08 00:26:21,899 iteration 2419 : loss : 0.046476, loss_ce: 0.022760
2022-01-08 00:26:23,241 iteration 2420 : loss : 0.044753, loss_ce: 0.017914
2022-01-08 00:26:24,749 iteration 2421 : loss : 0.057857, loss_ce: 0.017372
2022-01-08 00:26:26,186 iteration 2422 : loss : 0.052271, loss_ce: 0.016277
2022-01-08 00:26:27,634 iteration 2423 : loss : 0.049183, loss_ce: 0.023307
2022-01-08 00:26:28,976 iteration 2424 : loss : 0.049911, loss_ce: 0.017694
2022-01-08 00:26:30,446 iteration 2425 : loss : 0.041188, loss_ce: 0.020075
2022-01-08 00:26:31,839 iteration 2426 : loss : 0.037130, loss_ce: 0.013780
2022-01-08 00:26:33,377 iteration 2427 : loss : 0.101610, loss_ce: 0.034328
2022-01-08 00:26:34,859 iteration 2428 : loss : 0.080031, loss_ce: 0.035711
2022-01-08 00:26:36,349 iteration 2429 : loss : 0.054769, loss_ce: 0.025296
2022-01-08 00:26:37,766 iteration 2430 : loss : 0.039541, loss_ce: 0.015974
2022-01-08 00:26:39,149 iteration 2431 : loss : 0.053696, loss_ce: 0.029438
 36%|█████████▋                 | 143/400 [1:02:03<1:48:33, 25.34s/it]2022-01-08 00:26:40,652 iteration 2432 : loss : 0.043618, loss_ce: 0.018763
2022-01-08 00:26:42,066 iteration 2433 : loss : 0.107162, loss_ce: 0.049371
2022-01-08 00:26:43,474 iteration 2434 : loss : 0.043028, loss_ce: 0.014476
2022-01-08 00:26:45,015 iteration 2435 : loss : 0.060267, loss_ce: 0.033769
2022-01-08 00:26:46,330 iteration 2436 : loss : 0.057919, loss_ce: 0.024217
2022-01-08 00:26:47,664 iteration 2437 : loss : 0.037038, loss_ce: 0.016552
2022-01-08 00:26:49,193 iteration 2438 : loss : 0.059068, loss_ce: 0.023310
2022-01-08 00:26:50,623 iteration 2439 : loss : 0.050467, loss_ce: 0.020724
2022-01-08 00:26:52,027 iteration 2440 : loss : 0.031717, loss_ce: 0.010516
2022-01-08 00:26:53,398 iteration 2441 : loss : 0.069884, loss_ce: 0.022256
2022-01-08 00:26:54,752 iteration 2442 : loss : 0.053439, loss_ce: 0.020350
2022-01-08 00:26:56,082 iteration 2443 : loss : 0.050298, loss_ce: 0.023057
2022-01-08 00:26:57,536 iteration 2444 : loss : 0.038529, loss_ce: 0.014819
2022-01-08 00:26:58,851 iteration 2445 : loss : 0.082061, loss_ce: 0.020024
2022-01-08 00:27:00,338 iteration 2446 : loss : 0.062316, loss_ce: 0.022749
2022-01-08 00:27:01,811 iteration 2447 : loss : 0.046773, loss_ce: 0.021902
2022-01-08 00:27:03,244 iteration 2448 : loss : 0.041783, loss_ce: 0.014857
 36%|█████████▋                 | 144/400 [1:02:27<1:46:31, 24.97s/it]2022-01-08 00:27:04,678 iteration 2449 : loss : 0.054086, loss_ce: 0.025818
2022-01-08 00:27:06,144 iteration 2450 : loss : 0.056691, loss_ce: 0.020140
2022-01-08 00:27:07,605 iteration 2451 : loss : 0.046911, loss_ce: 0.017172
2022-01-08 00:27:08,947 iteration 2452 : loss : 0.052219, loss_ce: 0.023526
2022-01-08 00:27:10,354 iteration 2453 : loss : 0.076495, loss_ce: 0.021651
2022-01-08 00:27:11,784 iteration 2454 : loss : 0.057155, loss_ce: 0.032279
2022-01-08 00:27:13,212 iteration 2455 : loss : 0.051830, loss_ce: 0.025812
2022-01-08 00:27:14,749 iteration 2456 : loss : 0.079701, loss_ce: 0.029898
2022-01-08 00:27:16,180 iteration 2457 : loss : 0.044952, loss_ce: 0.017867
2022-01-08 00:27:17,662 iteration 2458 : loss : 0.033747, loss_ce: 0.012069
2022-01-08 00:27:19,021 iteration 2459 : loss : 0.070520, loss_ce: 0.019940
2022-01-08 00:27:20,413 iteration 2460 : loss : 0.055871, loss_ce: 0.020593
2022-01-08 00:27:21,858 iteration 2461 : loss : 0.062826, loss_ce: 0.023213
2022-01-08 00:27:23,237 iteration 2462 : loss : 0.026935, loss_ce: 0.011215
2022-01-08 00:27:24,657 iteration 2463 : loss : 0.052172, loss_ce: 0.019922
2022-01-08 00:27:26,136 iteration 2464 : loss : 0.047065, loss_ce: 0.021342
2022-01-08 00:27:26,137 Training Data Eval:
2022-01-08 00:27:33,169   Average segmentation loss on training set: 0.0404
2022-01-08 00:27:33,170 Validation Data Eval:
2022-01-08 00:27:35,554   Average segmentation loss on validation set: 0.0724
2022-01-08 00:27:36,914 iteration 2465 : loss : 0.052470, loss_ce: 0.018840
 36%|█████████▊                 | 145/400 [1:03:01<1:57:12, 27.58s/it]2022-01-08 00:27:38,323 iteration 2466 : loss : 0.043937, loss_ce: 0.015419
2022-01-08 00:27:39,797 iteration 2467 : loss : 0.050118, loss_ce: 0.030868
2022-01-08 00:27:41,152 iteration 2468 : loss : 0.035431, loss_ce: 0.015658
2022-01-08 00:27:42,476 iteration 2469 : loss : 0.025759, loss_ce: 0.012591
2022-01-08 00:27:43,910 iteration 2470 : loss : 0.061065, loss_ce: 0.026749
2022-01-08 00:27:45,385 iteration 2471 : loss : 0.043255, loss_ce: 0.017194
2022-01-08 00:27:46,879 iteration 2472 : loss : 0.052626, loss_ce: 0.019989
2022-01-08 00:27:48,209 iteration 2473 : loss : 0.057429, loss_ce: 0.024438
2022-01-08 00:27:49,635 iteration 2474 : loss : 0.047670, loss_ce: 0.017021
2022-01-08 00:27:50,977 iteration 2475 : loss : 0.028828, loss_ce: 0.012525
2022-01-08 00:27:52,477 iteration 2476 : loss : 0.076909, loss_ce: 0.020316
2022-01-08 00:27:53,867 iteration 2477 : loss : 0.064911, loss_ce: 0.026357
2022-01-08 00:27:55,252 iteration 2478 : loss : 0.047612, loss_ce: 0.015828
2022-01-08 00:27:56,764 iteration 2479 : loss : 0.039756, loss_ce: 0.011438
2022-01-08 00:27:58,268 iteration 2480 : loss : 0.067146, loss_ce: 0.024429
2022-01-08 00:27:59,647 iteration 2481 : loss : 0.042944, loss_ce: 0.016687
2022-01-08 00:28:01,051 iteration 2482 : loss : 0.039637, loss_ce: 0.014883
 36%|█████████▊                 | 146/400 [1:03:25<1:52:23, 26.55s/it]2022-01-08 00:28:02,499 iteration 2483 : loss : 0.032117, loss_ce: 0.012357
2022-01-08 00:28:03,987 iteration 2484 : loss : 0.059053, loss_ce: 0.018869
2022-01-08 00:28:05,480 iteration 2485 : loss : 0.065173, loss_ce: 0.021167
2022-01-08 00:28:06,840 iteration 2486 : loss : 0.039798, loss_ce: 0.018870
2022-01-08 00:28:08,218 iteration 2487 : loss : 0.041624, loss_ce: 0.014854
2022-01-08 00:28:09,706 iteration 2488 : loss : 0.038444, loss_ce: 0.012220
2022-01-08 00:28:11,165 iteration 2489 : loss : 0.055421, loss_ce: 0.021980
2022-01-08 00:28:12,543 iteration 2490 : loss : 0.040475, loss_ce: 0.016148
2022-01-08 00:28:13,956 iteration 2491 : loss : 0.038608, loss_ce: 0.016132
2022-01-08 00:28:15,336 iteration 2492 : loss : 0.052151, loss_ce: 0.019990
2022-01-08 00:28:16,715 iteration 2493 : loss : 0.039912, loss_ce: 0.012021
2022-01-08 00:28:18,062 iteration 2494 : loss : 0.039310, loss_ce: 0.016828
2022-01-08 00:28:19,440 iteration 2495 : loss : 0.042996, loss_ce: 0.013752
2022-01-08 00:28:20,880 iteration 2496 : loss : 0.048462, loss_ce: 0.015044
2022-01-08 00:28:22,293 iteration 2497 : loss : 0.051498, loss_ce: 0.021926
2022-01-08 00:28:23,609 iteration 2498 : loss : 0.049691, loss_ce: 0.024503
2022-01-08 00:28:25,063 iteration 2499 : loss : 0.042579, loss_ce: 0.021137
 37%|█████████▉                 | 147/400 [1:03:49<1:48:42, 25.78s/it]2022-01-08 00:28:26,528 iteration 2500 : loss : 0.043828, loss_ce: 0.014376
2022-01-08 00:28:27,898 iteration 2501 : loss : 0.041152, loss_ce: 0.015826
2022-01-08 00:28:29,302 iteration 2502 : loss : 0.043863, loss_ce: 0.015314
2022-01-08 00:28:30,706 iteration 2503 : loss : 0.043682, loss_ce: 0.020360
2022-01-08 00:28:32,132 iteration 2504 : loss : 0.050575, loss_ce: 0.019523
2022-01-08 00:28:33,486 iteration 2505 : loss : 0.039281, loss_ce: 0.018109
2022-01-08 00:28:34,845 iteration 2506 : loss : 0.064360, loss_ce: 0.016493
2022-01-08 00:28:36,275 iteration 2507 : loss : 0.030051, loss_ce: 0.011401
2022-01-08 00:28:37,651 iteration 2508 : loss : 0.040643, loss_ce: 0.020633
2022-01-08 00:28:39,108 iteration 2509 : loss : 0.042910, loss_ce: 0.017316
2022-01-08 00:28:40,495 iteration 2510 : loss : 0.041442, loss_ce: 0.014655
2022-01-08 00:28:41,831 iteration 2511 : loss : 0.040271, loss_ce: 0.019795
2022-01-08 00:28:43,278 iteration 2512 : loss : 0.041016, loss_ce: 0.013273
2022-01-08 00:28:44,639 iteration 2513 : loss : 0.038345, loss_ce: 0.015645
2022-01-08 00:28:46,102 iteration 2514 : loss : 0.054172, loss_ce: 0.016273
2022-01-08 00:28:47,474 iteration 2515 : loss : 0.042954, loss_ce: 0.020313
2022-01-08 00:28:49,028 iteration 2516 : loss : 0.036985, loss_ce: 0.015022
 37%|█████████▉                 | 148/400 [1:04:13<1:46:00, 25.24s/it]2022-01-08 00:28:50,492 iteration 2517 : loss : 0.043957, loss_ce: 0.016117
2022-01-08 00:28:51,962 iteration 2518 : loss : 0.066539, loss_ce: 0.022784
2022-01-08 00:28:53,299 iteration 2519 : loss : 0.029045, loss_ce: 0.010585
2022-01-08 00:28:54,733 iteration 2520 : loss : 0.051404, loss_ce: 0.023606
2022-01-08 00:28:56,144 iteration 2521 : loss : 0.037053, loss_ce: 0.016615
2022-01-08 00:28:57,593 iteration 2522 : loss : 0.044383, loss_ce: 0.018810
2022-01-08 00:28:59,001 iteration 2523 : loss : 0.049904, loss_ce: 0.014635
2022-01-08 00:29:00,450 iteration 2524 : loss : 0.034828, loss_ce: 0.015432
2022-01-08 00:29:01,818 iteration 2525 : loss : 0.034548, loss_ce: 0.010865
2022-01-08 00:29:03,160 iteration 2526 : loss : 0.035361, loss_ce: 0.013229
2022-01-08 00:29:04,603 iteration 2527 : loss : 0.053437, loss_ce: 0.027689
2022-01-08 00:29:05,993 iteration 2528 : loss : 0.042353, loss_ce: 0.020679
2022-01-08 00:29:07,421 iteration 2529 : loss : 0.049209, loss_ce: 0.018840
2022-01-08 00:29:08,751 iteration 2530 : loss : 0.030617, loss_ce: 0.010398
2022-01-08 00:29:10,196 iteration 2531 : loss : 0.034250, loss_ce: 0.010262
2022-01-08 00:29:11,680 iteration 2532 : loss : 0.034078, loss_ce: 0.016541
2022-01-08 00:29:13,022 iteration 2533 : loss : 0.043660, loss_ce: 0.011970
 37%|██████████                 | 149/400 [1:04:37<1:44:00, 24.86s/it]2022-01-08 00:29:14,467 iteration 2534 : loss : 0.030823, loss_ce: 0.010646
2022-01-08 00:29:15,781 iteration 2535 : loss : 0.043954, loss_ce: 0.017317
2022-01-08 00:29:17,131 iteration 2536 : loss : 0.023655, loss_ce: 0.010689
2022-01-08 00:29:18,618 iteration 2537 : loss : 0.046204, loss_ce: 0.025087
2022-01-08 00:29:19,893 iteration 2538 : loss : 0.036585, loss_ce: 0.012473
2022-01-08 00:29:21,210 iteration 2539 : loss : 0.030140, loss_ce: 0.012155
2022-01-08 00:29:22,617 iteration 2540 : loss : 0.052083, loss_ce: 0.024067
2022-01-08 00:29:24,079 iteration 2541 : loss : 0.042549, loss_ce: 0.017849
2022-01-08 00:29:25,476 iteration 2542 : loss : 0.055152, loss_ce: 0.021379
2022-01-08 00:29:26,957 iteration 2543 : loss : 0.057573, loss_ce: 0.016738
2022-01-08 00:29:28,474 iteration 2544 : loss : 0.074077, loss_ce: 0.024144
2022-01-08 00:29:29,830 iteration 2545 : loss : 0.043173, loss_ce: 0.019538
2022-01-08 00:29:31,413 iteration 2546 : loss : 0.045662, loss_ce: 0.017955
2022-01-08 00:29:32,872 iteration 2547 : loss : 0.047668, loss_ce: 0.015596
2022-01-08 00:29:34,390 iteration 2548 : loss : 0.036949, loss_ce: 0.016177
2022-01-08 00:29:35,807 iteration 2549 : loss : 0.041026, loss_ce: 0.014273
2022-01-08 00:29:35,807 Training Data Eval:
2022-01-08 00:29:42,860   Average segmentation loss on training set: 0.0303
2022-01-08 00:29:42,860 Validation Data Eval:
2022-01-08 00:29:45,244   Average segmentation loss on validation set: 0.0663
2022-01-08 00:29:46,661 iteration 2550 : loss : 0.050790, loss_ce: 0.010554
 38%|██████████▏                | 150/400 [1:05:10<1:54:34, 27.50s/it]2022-01-08 00:29:48,101 iteration 2551 : loss : 0.057373, loss_ce: 0.017996
2022-01-08 00:29:49,513 iteration 2552 : loss : 0.029355, loss_ce: 0.012622
2022-01-08 00:29:50,926 iteration 2553 : loss : 0.038999, loss_ce: 0.012685
2022-01-08 00:29:52,332 iteration 2554 : loss : 0.039746, loss_ce: 0.015077
2022-01-08 00:29:53,700 iteration 2555 : loss : 0.031993, loss_ce: 0.012826
2022-01-08 00:29:55,094 iteration 2556 : loss : 0.039358, loss_ce: 0.017288
2022-01-08 00:29:56,480 iteration 2557 : loss : 0.033608, loss_ce: 0.012488
2022-01-08 00:29:57,874 iteration 2558 : loss : 0.029748, loss_ce: 0.009774
2022-01-08 00:29:59,218 iteration 2559 : loss : 0.037247, loss_ce: 0.017723
2022-01-08 00:30:00,702 iteration 2560 : loss : 0.038141, loss_ce: 0.012009
2022-01-08 00:30:02,093 iteration 2561 : loss : 0.072913, loss_ce: 0.033456
2022-01-08 00:30:03,514 iteration 2562 : loss : 0.037908, loss_ce: 0.015380
2022-01-08 00:30:04,857 iteration 2563 : loss : 0.044619, loss_ce: 0.019150
2022-01-08 00:30:06,206 iteration 2564 : loss : 0.027060, loss_ce: 0.011814
2022-01-08 00:30:07,551 iteration 2565 : loss : 0.024573, loss_ce: 0.009298
2022-01-08 00:30:08,864 iteration 2566 : loss : 0.028091, loss_ce: 0.010615
2022-01-08 00:30:10,252 iteration 2567 : loss : 0.053195, loss_ce: 0.012235
 38%|██████████▏                | 151/400 [1:05:34<1:49:15, 26.33s/it]2022-01-08 00:30:11,684 iteration 2568 : loss : 0.040534, loss_ce: 0.018774
2022-01-08 00:30:13,063 iteration 2569 : loss : 0.037412, loss_ce: 0.012162
2022-01-08 00:30:14,541 iteration 2570 : loss : 0.035212, loss_ce: 0.011415
2022-01-08 00:30:15,957 iteration 2571 : loss : 0.039194, loss_ce: 0.012196
2022-01-08 00:30:17,289 iteration 2572 : loss : 0.046758, loss_ce: 0.015057
2022-01-08 00:30:18,652 iteration 2573 : loss : 0.038264, loss_ce: 0.015285
2022-01-08 00:30:19,996 iteration 2574 : loss : 0.045219, loss_ce: 0.020512
2022-01-08 00:30:21,313 iteration 2575 : loss : 0.034224, loss_ce: 0.011488
2022-01-08 00:30:22,651 iteration 2576 : loss : 0.030631, loss_ce: 0.009219
2022-01-08 00:30:24,067 iteration 2577 : loss : 0.052561, loss_ce: 0.033085
2022-01-08 00:30:25,429 iteration 2578 : loss : 0.039069, loss_ce: 0.014377
2022-01-08 00:30:26,830 iteration 2579 : loss : 0.039583, loss_ce: 0.014657
2022-01-08 00:30:28,181 iteration 2580 : loss : 0.033890, loss_ce: 0.011754
2022-01-08 00:30:29,589 iteration 2581 : loss : 0.038923, loss_ce: 0.016084
2022-01-08 00:30:31,039 iteration 2582 : loss : 0.027372, loss_ce: 0.012938
2022-01-08 00:30:32,400 iteration 2583 : loss : 0.037402, loss_ce: 0.017277
2022-01-08 00:30:33,785 iteration 2584 : loss : 0.053582, loss_ce: 0.016068
 38%|██████████▎                | 152/400 [1:05:58<1:45:21, 25.49s/it]2022-01-08 00:30:35,224 iteration 2585 : loss : 0.057430, loss_ce: 0.019793
2022-01-08 00:30:36,792 iteration 2586 : loss : 0.058827, loss_ce: 0.018439
2022-01-08 00:30:38,267 iteration 2587 : loss : 0.037656, loss_ce: 0.012489
2022-01-08 00:30:39,676 iteration 2588 : loss : 0.038934, loss_ce: 0.013888
2022-01-08 00:30:41,018 iteration 2589 : loss : 0.057003, loss_ce: 0.015978
2022-01-08 00:30:42,466 iteration 2590 : loss : 0.058686, loss_ce: 0.025120
2022-01-08 00:30:43,859 iteration 2591 : loss : 0.028624, loss_ce: 0.012312
2022-01-08 00:30:45,212 iteration 2592 : loss : 0.034657, loss_ce: 0.018734
2022-01-08 00:30:46,524 iteration 2593 : loss : 0.040656, loss_ce: 0.020115
2022-01-08 00:30:47,913 iteration 2594 : loss : 0.042854, loss_ce: 0.018543
2022-01-08 00:30:49,248 iteration 2595 : loss : 0.031769, loss_ce: 0.011177
2022-01-08 00:30:50,610 iteration 2596 : loss : 0.050943, loss_ce: 0.012997
2022-01-08 00:30:51,998 iteration 2597 : loss : 0.047529, loss_ce: 0.016331
2022-01-08 00:30:53,400 iteration 2598 : loss : 0.039749, loss_ce: 0.015708
2022-01-08 00:30:54,804 iteration 2599 : loss : 0.033840, loss_ce: 0.013561
2022-01-08 00:30:56,161 iteration 2600 : loss : 0.042401, loss_ce: 0.018127
2022-01-08 00:30:57,494 iteration 2601 : loss : 0.037840, loss_ce: 0.014715
 38%|██████████▎                | 153/400 [1:06:21<1:42:44, 24.96s/it]2022-01-08 00:30:58,920 iteration 2602 : loss : 0.067927, loss_ce: 0.025623
2022-01-08 00:31:00,328 iteration 2603 : loss : 0.053956, loss_ce: 0.014678
2022-01-08 00:31:01,718 iteration 2604 : loss : 0.030941, loss_ce: 0.012098
2022-01-08 00:31:03,106 iteration 2605 : loss : 0.042030, loss_ce: 0.015348
2022-01-08 00:31:04,530 iteration 2606 : loss : 0.027646, loss_ce: 0.010912
2022-01-08 00:31:05,935 iteration 2607 : loss : 0.029469, loss_ce: 0.013244
2022-01-08 00:31:07,279 iteration 2608 : loss : 0.035582, loss_ce: 0.014503
2022-01-08 00:31:08,707 iteration 2609 : loss : 0.041659, loss_ce: 0.017374
2022-01-08 00:31:10,148 iteration 2610 : loss : 0.038937, loss_ce: 0.014609
2022-01-08 00:31:11,603 iteration 2611 : loss : 0.045768, loss_ce: 0.018598
2022-01-08 00:31:13,031 iteration 2612 : loss : 0.057421, loss_ce: 0.019242
2022-01-08 00:31:14,494 iteration 2613 : loss : 0.043064, loss_ce: 0.016920
2022-01-08 00:31:15,949 iteration 2614 : loss : 0.032719, loss_ce: 0.013441
2022-01-08 00:31:17,364 iteration 2615 : loss : 0.040165, loss_ce: 0.018446
2022-01-08 00:31:18,822 iteration 2616 : loss : 0.041621, loss_ce: 0.017686
2022-01-08 00:31:20,234 iteration 2617 : loss : 0.045927, loss_ce: 0.016731
2022-01-08 00:31:21,682 iteration 2618 : loss : 0.035605, loss_ce: 0.012698
 38%|██████████▍                | 154/400 [1:06:45<1:41:22, 24.73s/it]2022-01-08 00:31:23,260 iteration 2619 : loss : 0.045354, loss_ce: 0.021847
2022-01-08 00:31:24,693 iteration 2620 : loss : 0.047337, loss_ce: 0.015409
2022-01-08 00:31:25,995 iteration 2621 : loss : 0.038586, loss_ce: 0.015895
2022-01-08 00:31:27,413 iteration 2622 : loss : 0.061005, loss_ce: 0.019461
2022-01-08 00:31:28,793 iteration 2623 : loss : 0.026946, loss_ce: 0.010909
2022-01-08 00:31:30,236 iteration 2624 : loss : 0.051620, loss_ce: 0.021648
2022-01-08 00:31:31,687 iteration 2625 : loss : 0.035996, loss_ce: 0.015284
2022-01-08 00:31:33,112 iteration 2626 : loss : 0.033845, loss_ce: 0.012791
2022-01-08 00:31:34,450 iteration 2627 : loss : 0.043814, loss_ce: 0.020205
2022-01-08 00:31:35,795 iteration 2628 : loss : 0.038761, loss_ce: 0.015936
2022-01-08 00:31:37,102 iteration 2629 : loss : 0.039636, loss_ce: 0.011616
2022-01-08 00:31:38,525 iteration 2630 : loss : 0.066495, loss_ce: 0.011591
2022-01-08 00:31:39,851 iteration 2631 : loss : 0.044298, loss_ce: 0.022380
2022-01-08 00:31:41,208 iteration 2632 : loss : 0.032823, loss_ce: 0.011500
2022-01-08 00:31:42,658 iteration 2633 : loss : 0.020308, loss_ce: 0.007006
2022-01-08 00:31:44,027 iteration 2634 : loss : 0.046623, loss_ce: 0.015676
2022-01-08 00:31:44,027 Training Data Eval:
2022-01-08 00:31:51,085   Average segmentation loss on training set: 0.0417
2022-01-08 00:31:51,086 Validation Data Eval:
2022-01-08 00:31:53,477   Average segmentation loss on validation set: 0.1375
2022-01-08 00:31:54,829 iteration 2635 : loss : 0.061081, loss_ce: 0.022189
 39%|██████████▍                | 155/400 [1:07:19<1:51:15, 27.25s/it]2022-01-08 00:31:56,206 iteration 2636 : loss : 0.035939, loss_ce: 0.011713
2022-01-08 00:31:57,575 iteration 2637 : loss : 0.039118, loss_ce: 0.017274
2022-01-08 00:31:59,082 iteration 2638 : loss : 0.062154, loss_ce: 0.022825
2022-01-08 00:32:00,460 iteration 2639 : loss : 0.033012, loss_ce: 0.012606
2022-01-08 00:32:01,868 iteration 2640 : loss : 0.054072, loss_ce: 0.021437
2022-01-08 00:32:03,250 iteration 2641 : loss : 0.037776, loss_ce: 0.014664
2022-01-08 00:32:04,687 iteration 2642 : loss : 0.040743, loss_ce: 0.014139
2022-01-08 00:32:06,063 iteration 2643 : loss : 0.041867, loss_ce: 0.018593
2022-01-08 00:32:07,472 iteration 2644 : loss : 0.054599, loss_ce: 0.028961
2022-01-08 00:32:08,804 iteration 2645 : loss : 0.035550, loss_ce: 0.014830
2022-01-08 00:32:10,149 iteration 2646 : loss : 0.064519, loss_ce: 0.025596
2022-01-08 00:32:11,628 iteration 2647 : loss : 0.043961, loss_ce: 0.019711
2022-01-08 00:32:12,985 iteration 2648 : loss : 0.051195, loss_ce: 0.020140
2022-01-08 00:32:14,381 iteration 2649 : loss : 0.040520, loss_ce: 0.013761
2022-01-08 00:32:15,888 iteration 2650 : loss : 0.052464, loss_ce: 0.020224
2022-01-08 00:32:17,283 iteration 2651 : loss : 0.037997, loss_ce: 0.014441
2022-01-08 00:32:18,743 iteration 2652 : loss : 0.040707, loss_ce: 0.016466
 39%|██████████▌                | 156/400 [1:07:42<1:46:44, 26.25s/it]2022-01-08 00:32:20,234 iteration 2653 : loss : 0.057823, loss_ce: 0.019213
2022-01-08 00:32:21,567 iteration 2654 : loss : 0.030175, loss_ce: 0.011710
2022-01-08 00:32:22,955 iteration 2655 : loss : 0.037161, loss_ce: 0.016384
2022-01-08 00:32:24,321 iteration 2656 : loss : 0.044084, loss_ce: 0.022687
2022-01-08 00:32:25,731 iteration 2657 : loss : 0.039889, loss_ce: 0.015981
2022-01-08 00:32:27,180 iteration 2658 : loss : 0.074579, loss_ce: 0.020154
2022-01-08 00:32:28,619 iteration 2659 : loss : 0.042823, loss_ce: 0.016446
2022-01-08 00:32:30,123 iteration 2660 : loss : 0.042629, loss_ce: 0.017021
2022-01-08 00:32:31,429 iteration 2661 : loss : 0.036119, loss_ce: 0.012487
2022-01-08 00:32:32,882 iteration 2662 : loss : 0.031401, loss_ce: 0.012780
2022-01-08 00:32:34,322 iteration 2663 : loss : 0.053184, loss_ce: 0.022842
2022-01-08 00:32:35,651 iteration 2664 : loss : 0.049118, loss_ce: 0.017342
2022-01-08 00:32:37,004 iteration 2665 : loss : 0.035715, loss_ce: 0.017198
2022-01-08 00:32:38,472 iteration 2666 : loss : 0.039191, loss_ce: 0.014316
2022-01-08 00:32:39,819 iteration 2667 : loss : 0.037631, loss_ce: 0.015338
2022-01-08 00:32:41,319 iteration 2668 : loss : 0.045915, loss_ce: 0.019177
2022-01-08 00:32:42,694 iteration 2669 : loss : 0.046662, loss_ce: 0.013464
 39%|██████████▌                | 157/400 [1:08:06<1:43:31, 25.56s/it]2022-01-08 00:32:44,052 iteration 2670 : loss : 0.027264, loss_ce: 0.010733
2022-01-08 00:32:45,367 iteration 2671 : loss : 0.072363, loss_ce: 0.032773
2022-01-08 00:32:46,739 iteration 2672 : loss : 0.044568, loss_ce: 0.016641
2022-01-08 00:32:48,093 iteration 2673 : loss : 0.038899, loss_ce: 0.012368
2022-01-08 00:32:49,505 iteration 2674 : loss : 0.039403, loss_ce: 0.012796
2022-01-08 00:32:50,946 iteration 2675 : loss : 0.029409, loss_ce: 0.011119
2022-01-08 00:32:52,320 iteration 2676 : loss : 0.027181, loss_ce: 0.009139
2022-01-08 00:32:53,723 iteration 2677 : loss : 0.033764, loss_ce: 0.015330
2022-01-08 00:32:55,049 iteration 2678 : loss : 0.038231, loss_ce: 0.017805
2022-01-08 00:32:56,445 iteration 2679 : loss : 0.039554, loss_ce: 0.016489
2022-01-08 00:32:57,903 iteration 2680 : loss : 0.033065, loss_ce: 0.013465
2022-01-08 00:32:59,330 iteration 2681 : loss : 0.044629, loss_ce: 0.013408
2022-01-08 00:33:00,757 iteration 2682 : loss : 0.035866, loss_ce: 0.014728
2022-01-08 00:33:02,117 iteration 2683 : loss : 0.043656, loss_ce: 0.020551
2022-01-08 00:33:03,524 iteration 2684 : loss : 0.037515, loss_ce: 0.016587
2022-01-08 00:33:04,985 iteration 2685 : loss : 0.036850, loss_ce: 0.016363
2022-01-08 00:33:06,437 iteration 2686 : loss : 0.079727, loss_ce: 0.019480
 40%|██████████▋                | 158/400 [1:08:30<1:40:53, 25.02s/it]2022-01-08 00:33:07,836 iteration 2687 : loss : 0.042398, loss_ce: 0.017802
2022-01-08 00:33:09,205 iteration 2688 : loss : 0.036392, loss_ce: 0.014356
2022-01-08 00:33:10,559 iteration 2689 : loss : 0.045620, loss_ce: 0.015483
2022-01-08 00:33:11,998 iteration 2690 : loss : 0.083037, loss_ce: 0.018823
2022-01-08 00:33:13,337 iteration 2691 : loss : 0.027732, loss_ce: 0.010854
2022-01-08 00:33:14,686 iteration 2692 : loss : 0.079624, loss_ce: 0.033497
2022-01-08 00:33:16,021 iteration 2693 : loss : 0.034302, loss_ce: 0.012916
2022-01-08 00:33:17,451 iteration 2694 : loss : 0.036571, loss_ce: 0.015878
2022-01-08 00:33:18,831 iteration 2695 : loss : 0.045663, loss_ce: 0.023079
2022-01-08 00:33:20,210 iteration 2696 : loss : 0.035204, loss_ce: 0.014221
2022-01-08 00:33:21,559 iteration 2697 : loss : 0.047695, loss_ce: 0.022646
2022-01-08 00:33:22,920 iteration 2698 : loss : 0.049841, loss_ce: 0.014946
2022-01-08 00:33:24,294 iteration 2699 : loss : 0.038731, loss_ce: 0.017719
2022-01-08 00:33:25,640 iteration 2700 : loss : 0.042410, loss_ce: 0.015226
2022-01-08 00:33:27,140 iteration 2701 : loss : 0.045336, loss_ce: 0.017239
2022-01-08 00:33:28,579 iteration 2702 : loss : 0.042898, loss_ce: 0.022187
2022-01-08 00:33:30,064 iteration 2703 : loss : 0.043292, loss_ce: 0.016387
 40%|██████████▋                | 159/400 [1:08:54<1:38:48, 24.60s/it]2022-01-08 00:33:31,568 iteration 2704 : loss : 0.056496, loss_ce: 0.019297
2022-01-08 00:33:32,916 iteration 2705 : loss : 0.028243, loss_ce: 0.012210
2022-01-08 00:33:34,356 iteration 2706 : loss : 0.059322, loss_ce: 0.025547
2022-01-08 00:33:35,734 iteration 2707 : loss : 0.036640, loss_ce: 0.018917
2022-01-08 00:33:37,163 iteration 2708 : loss : 0.034374, loss_ce: 0.013813
2022-01-08 00:33:38,547 iteration 2709 : loss : 0.033046, loss_ce: 0.014258
2022-01-08 00:33:39,871 iteration 2710 : loss : 0.030439, loss_ce: 0.012774
2022-01-08 00:33:41,305 iteration 2711 : loss : 0.034504, loss_ce: 0.012663
2022-01-08 00:33:42,730 iteration 2712 : loss : 0.032176, loss_ce: 0.009089
2022-01-08 00:33:44,075 iteration 2713 : loss : 0.034509, loss_ce: 0.018256
2022-01-08 00:33:45,531 iteration 2714 : loss : 0.043373, loss_ce: 0.015423
2022-01-08 00:33:46,989 iteration 2715 : loss : 0.042545, loss_ce: 0.014620
2022-01-08 00:33:48,451 iteration 2716 : loss : 0.034062, loss_ce: 0.011288
2022-01-08 00:33:49,836 iteration 2717 : loss : 0.042266, loss_ce: 0.017082
2022-01-08 00:33:51,275 iteration 2718 : loss : 0.065697, loss_ce: 0.017543
2022-01-08 00:33:52,650 iteration 2719 : loss : 0.030952, loss_ce: 0.012305
2022-01-08 00:33:52,650 Training Data Eval:
2022-01-08 00:33:59,715   Average segmentation loss on training set: 0.0247
2022-01-08 00:33:59,716 Validation Data Eval:
2022-01-08 00:34:02,103   Average segmentation loss on validation set: 0.0649
2022-01-08 00:34:03,445 iteration 2720 : loss : 0.031722, loss_ce: 0.010562
 40%|██████████▊                | 160/400 [1:09:27<1:48:55, 27.23s/it]2022-01-08 00:34:04,905 iteration 2721 : loss : 0.033777, loss_ce: 0.013630
2022-01-08 00:34:06,333 iteration 2722 : loss : 0.040868, loss_ce: 0.015123
2022-01-08 00:34:07,760 iteration 2723 : loss : 0.037811, loss_ce: 0.013436
2022-01-08 00:34:09,166 iteration 2724 : loss : 0.050734, loss_ce: 0.020395
2022-01-08 00:34:10,543 iteration 2725 : loss : 0.046230, loss_ce: 0.016223
2022-01-08 00:34:12,115 iteration 2726 : loss : 0.068003, loss_ce: 0.020445
2022-01-08 00:34:13,569 iteration 2727 : loss : 0.035525, loss_ce: 0.015364
2022-01-08 00:34:14,968 iteration 2728 : loss : 0.029055, loss_ce: 0.010388
2022-01-08 00:34:16,356 iteration 2729 : loss : 0.042298, loss_ce: 0.018259
2022-01-08 00:34:17,699 iteration 2730 : loss : 0.027250, loss_ce: 0.010774
2022-01-08 00:34:19,102 iteration 2731 : loss : 0.032373, loss_ce: 0.013988
2022-01-08 00:34:20,565 iteration 2732 : loss : 0.039129, loss_ce: 0.016340
2022-01-08 00:34:21,988 iteration 2733 : loss : 0.028033, loss_ce: 0.009994
2022-01-08 00:34:23,411 iteration 2734 : loss : 0.039430, loss_ce: 0.013143
2022-01-08 00:34:24,905 iteration 2735 : loss : 0.037090, loss_ce: 0.011819
2022-01-08 00:34:26,293 iteration 2736 : loss : 0.027552, loss_ce: 0.012316
2022-01-08 00:34:27,722 iteration 2737 : loss : 0.045171, loss_ce: 0.017656
 40%|██████████▊                | 161/400 [1:09:51<1:44:56, 26.35s/it]2022-01-08 00:34:29,103 iteration 2738 : loss : 0.023831, loss_ce: 0.009460
2022-01-08 00:34:30,588 iteration 2739 : loss : 0.048594, loss_ce: 0.020169
2022-01-08 00:34:32,067 iteration 2740 : loss : 0.044384, loss_ce: 0.016369
2022-01-08 00:34:33,437 iteration 2741 : loss : 0.029295, loss_ce: 0.010903
2022-01-08 00:34:34,765 iteration 2742 : loss : 0.046900, loss_ce: 0.015585
2022-01-08 00:34:36,202 iteration 2743 : loss : 0.030949, loss_ce: 0.009782
2022-01-08 00:34:37,737 iteration 2744 : loss : 0.042813, loss_ce: 0.016133
2022-01-08 00:34:39,127 iteration 2745 : loss : 0.041891, loss_ce: 0.018281
2022-01-08 00:34:40,503 iteration 2746 : loss : 0.027969, loss_ce: 0.010195
2022-01-08 00:34:42,042 iteration 2747 : loss : 0.045327, loss_ce: 0.019528
2022-01-08 00:34:43,434 iteration 2748 : loss : 0.033310, loss_ce: 0.012523
2022-01-08 00:34:44,839 iteration 2749 : loss : 0.036651, loss_ce: 0.017816
2022-01-08 00:34:46,220 iteration 2750 : loss : 0.037545, loss_ce: 0.011910
2022-01-08 00:34:47,612 iteration 2751 : loss : 0.036064, loss_ce: 0.010914
2022-01-08 00:34:48,969 iteration 2752 : loss : 0.031348, loss_ce: 0.012385
2022-01-08 00:34:50,343 iteration 2753 : loss : 0.040877, loss_ce: 0.019554
2022-01-08 00:34:51,748 iteration 2754 : loss : 0.048466, loss_ce: 0.016208
 40%|██████████▉                | 162/400 [1:10:15<1:41:44, 25.65s/it]2022-01-08 00:34:53,192 iteration 2755 : loss : 0.049984, loss_ce: 0.021036
2022-01-08 00:34:54,630 iteration 2756 : loss : 0.036711, loss_ce: 0.015110
2022-01-08 00:34:55,984 iteration 2757 : loss : 0.039949, loss_ce: 0.017125
2022-01-08 00:34:57,367 iteration 2758 : loss : 0.031777, loss_ce: 0.012713
2022-01-08 00:34:58,748 iteration 2759 : loss : 0.028718, loss_ce: 0.010555
2022-01-08 00:35:00,216 iteration 2760 : loss : 0.054442, loss_ce: 0.014409
2022-01-08 00:35:01,611 iteration 2761 : loss : 0.033421, loss_ce: 0.013112
2022-01-08 00:35:02,995 iteration 2762 : loss : 0.037380, loss_ce: 0.015223
2022-01-08 00:35:04,376 iteration 2763 : loss : 0.032399, loss_ce: 0.013963
2022-01-08 00:35:05,721 iteration 2764 : loss : 0.027932, loss_ce: 0.012088
2022-01-08 00:35:07,106 iteration 2765 : loss : 0.047348, loss_ce: 0.022951
2022-01-08 00:35:08,548 iteration 2766 : loss : 0.052268, loss_ce: 0.021760
2022-01-08 00:35:09,874 iteration 2767 : loss : 0.022623, loss_ce: 0.006171
2022-01-08 00:35:11,389 iteration 2768 : loss : 0.032466, loss_ce: 0.013077
2022-01-08 00:35:12,789 iteration 2769 : loss : 0.030278, loss_ce: 0.010681
2022-01-08 00:35:14,125 iteration 2770 : loss : 0.040994, loss_ce: 0.022967
2022-01-08 00:35:15,503 iteration 2771 : loss : 0.037341, loss_ce: 0.018574
 41%|███████████                | 163/400 [1:10:39<1:39:04, 25.08s/it]2022-01-08 00:35:17,008 iteration 2772 : loss : 0.038566, loss_ce: 0.014456
2022-01-08 00:35:18,356 iteration 2773 : loss : 0.039057, loss_ce: 0.013579
2022-01-08 00:35:19,720 iteration 2774 : loss : 0.030612, loss_ce: 0.012675
2022-01-08 00:35:21,111 iteration 2775 : loss : 0.033060, loss_ce: 0.013941
2022-01-08 00:35:22,505 iteration 2776 : loss : 0.042256, loss_ce: 0.020514
2022-01-08 00:35:23,961 iteration 2777 : loss : 0.031545, loss_ce: 0.013227
2022-01-08 00:35:25,284 iteration 2778 : loss : 0.028294, loss_ce: 0.013032
2022-01-08 00:35:26,710 iteration 2779 : loss : 0.036556, loss_ce: 0.016669
2022-01-08 00:35:28,181 iteration 2780 : loss : 0.033372, loss_ce: 0.013730
2022-01-08 00:35:29,501 iteration 2781 : loss : 0.031856, loss_ce: 0.013590
2022-01-08 00:35:30,948 iteration 2782 : loss : 0.029770, loss_ce: 0.011562
2022-01-08 00:35:32,412 iteration 2783 : loss : 0.037325, loss_ce: 0.012800
2022-01-08 00:35:33,913 iteration 2784 : loss : 0.030490, loss_ce: 0.014083
2022-01-08 00:35:35,247 iteration 2785 : loss : 0.025049, loss_ce: 0.009622
2022-01-08 00:35:36,688 iteration 2786 : loss : 0.035895, loss_ce: 0.012252
2022-01-08 00:35:38,162 iteration 2787 : loss : 0.060604, loss_ce: 0.016302
2022-01-08 00:35:39,578 iteration 2788 : loss : 0.034562, loss_ce: 0.010737
 41%|███████████                | 164/400 [1:11:03<1:37:27, 24.78s/it]2022-01-08 00:35:41,026 iteration 2789 : loss : 0.028176, loss_ce: 0.013420
2022-01-08 00:35:42,417 iteration 2790 : loss : 0.046004, loss_ce: 0.014713
2022-01-08 00:35:43,834 iteration 2791 : loss : 0.030604, loss_ce: 0.010271
2022-01-08 00:35:45,285 iteration 2792 : loss : 0.059122, loss_ce: 0.014726
2022-01-08 00:35:46,689 iteration 2793 : loss : 0.045845, loss_ce: 0.022322
2022-01-08 00:35:48,137 iteration 2794 : loss : 0.033525, loss_ce: 0.013689
2022-01-08 00:35:49,600 iteration 2795 : loss : 0.037697, loss_ce: 0.015458
2022-01-08 00:35:51,094 iteration 2796 : loss : 0.039587, loss_ce: 0.014189
2022-01-08 00:35:52,536 iteration 2797 : loss : 0.035955, loss_ce: 0.013930
2022-01-08 00:35:54,056 iteration 2798 : loss : 0.057795, loss_ce: 0.023528
2022-01-08 00:35:55,510 iteration 2799 : loss : 0.070691, loss_ce: 0.031351
2022-01-08 00:35:56,943 iteration 2800 : loss : 0.030414, loss_ce: 0.014335
2022-01-08 00:35:58,326 iteration 2801 : loss : 0.039599, loss_ce: 0.019064
2022-01-08 00:35:59,679 iteration 2802 : loss : 0.030931, loss_ce: 0.013579
2022-01-08 00:36:01,199 iteration 2803 : loss : 0.039955, loss_ce: 0.013737
2022-01-08 00:36:02,607 iteration 2804 : loss : 0.039006, loss_ce: 0.015506
2022-01-08 00:36:02,607 Training Data Eval:
2022-01-08 00:36:09,645   Average segmentation loss on training set: 0.0266
2022-01-08 00:36:09,646 Validation Data Eval:
2022-01-08 00:36:12,038   Average segmentation loss on validation set: 0.0650
2022-01-08 00:36:13,405 iteration 2805 : loss : 0.057606, loss_ce: 0.018417
 41%|███████████▏               | 165/400 [1:11:37<1:47:40, 27.49s/it]2022-01-08 00:36:14,873 iteration 2806 : loss : 0.077187, loss_ce: 0.027445
2022-01-08 00:36:16,414 iteration 2807 : loss : 0.058982, loss_ce: 0.024232
2022-01-08 00:36:17,734 iteration 2808 : loss : 0.030813, loss_ce: 0.016230
2022-01-08 00:36:19,129 iteration 2809 : loss : 0.043077, loss_ce: 0.017395
2022-01-08 00:36:20,564 iteration 2810 : loss : 0.036474, loss_ce: 0.015135
2022-01-08 00:36:21,931 iteration 2811 : loss : 0.030221, loss_ce: 0.011363
2022-01-08 00:36:23,328 iteration 2812 : loss : 0.037544, loss_ce: 0.014797
2022-01-08 00:36:24,758 iteration 2813 : loss : 0.035032, loss_ce: 0.010287
2022-01-08 00:36:26,161 iteration 2814 : loss : 0.050063, loss_ce: 0.020757
2022-01-08 00:36:27,518 iteration 2815 : loss : 0.029663, loss_ce: 0.012851
2022-01-08 00:36:28,869 iteration 2816 : loss : 0.036548, loss_ce: 0.016624
2022-01-08 00:36:30,249 iteration 2817 : loss : 0.024453, loss_ce: 0.008039
2022-01-08 00:36:31,717 iteration 2818 : loss : 0.043535, loss_ce: 0.014855
2022-01-08 00:36:33,159 iteration 2819 : loss : 0.040649, loss_ce: 0.017692
2022-01-08 00:36:34,666 iteration 2820 : loss : 0.033412, loss_ce: 0.013936
2022-01-08 00:36:36,096 iteration 2821 : loss : 0.033551, loss_ce: 0.015914
2022-01-08 00:36:37,463 iteration 2822 : loss : 0.032547, loss_ce: 0.010999
 42%|███████████▏               | 166/400 [1:12:01<1:43:11, 26.46s/it]2022-01-08 00:36:38,979 iteration 2823 : loss : 0.055976, loss_ce: 0.018949
2022-01-08 00:36:40,331 iteration 2824 : loss : 0.024192, loss_ce: 0.010260
2022-01-08 00:36:41,782 iteration 2825 : loss : 0.034142, loss_ce: 0.014389
2022-01-08 00:36:43,140 iteration 2826 : loss : 0.026273, loss_ce: 0.010928
2022-01-08 00:36:44,568 iteration 2827 : loss : 0.049545, loss_ce: 0.022902
2022-01-08 00:36:45,983 iteration 2828 : loss : 0.043595, loss_ce: 0.018319
2022-01-08 00:36:47,496 iteration 2829 : loss : 0.038637, loss_ce: 0.016031
2022-01-08 00:36:48,850 iteration 2830 : loss : 0.027937, loss_ce: 0.013183
2022-01-08 00:36:50,248 iteration 2831 : loss : 0.036122, loss_ce: 0.014789
2022-01-08 00:36:51,684 iteration 2832 : loss : 0.038022, loss_ce: 0.012235
2022-01-08 00:36:53,140 iteration 2833 : loss : 0.040321, loss_ce: 0.015276
2022-01-08 00:36:54,553 iteration 2834 : loss : 0.057646, loss_ce: 0.016322
2022-01-08 00:36:55,985 iteration 2835 : loss : 0.073728, loss_ce: 0.018694
2022-01-08 00:36:57,440 iteration 2836 : loss : 0.047808, loss_ce: 0.021767
2022-01-08 00:36:58,837 iteration 2837 : loss : 0.034487, loss_ce: 0.009543
2022-01-08 00:37:00,191 iteration 2838 : loss : 0.031879, loss_ce: 0.009712
2022-01-08 00:37:01,623 iteration 2839 : loss : 0.048901, loss_ce: 0.017978
 42%|███████████▎               | 167/400 [1:12:25<1:40:04, 25.77s/it]2022-01-08 00:37:03,043 iteration 2840 : loss : 0.056926, loss_ce: 0.028254
2022-01-08 00:37:04,455 iteration 2841 : loss : 0.028040, loss_ce: 0.011555
2022-01-08 00:37:05,874 iteration 2842 : loss : 0.053165, loss_ce: 0.017698
2022-01-08 00:37:07,197 iteration 2843 : loss : 0.031620, loss_ce: 0.014167
2022-01-08 00:37:08,498 iteration 2844 : loss : 0.027492, loss_ce: 0.010036
2022-01-08 00:37:09,863 iteration 2845 : loss : 0.044592, loss_ce: 0.016966
2022-01-08 00:37:11,357 iteration 2846 : loss : 0.045768, loss_ce: 0.012929
2022-01-08 00:37:12,832 iteration 2847 : loss : 0.040541, loss_ce: 0.014744
2022-01-08 00:37:14,232 iteration 2848 : loss : 0.070827, loss_ce: 0.025951
2022-01-08 00:37:15,661 iteration 2849 : loss : 0.034977, loss_ce: 0.014241
2022-01-08 00:37:17,057 iteration 2850 : loss : 0.072208, loss_ce: 0.042103
2022-01-08 00:37:18,570 iteration 2851 : loss : 0.057707, loss_ce: 0.024729
2022-01-08 00:37:19,930 iteration 2852 : loss : 0.041401, loss_ce: 0.014345
2022-01-08 00:37:21,347 iteration 2853 : loss : 0.055089, loss_ce: 0.016138
2022-01-08 00:37:22,683 iteration 2854 : loss : 0.045488, loss_ce: 0.014384
2022-01-08 00:37:24,043 iteration 2855 : loss : 0.040310, loss_ce: 0.017386
2022-01-08 00:37:25,365 iteration 2856 : loss : 0.028436, loss_ce: 0.013889
 42%|███████████▎               | 168/400 [1:12:49<1:37:18, 25.17s/it]2022-01-08 00:37:26,806 iteration 2857 : loss : 0.040380, loss_ce: 0.016427
2022-01-08 00:37:28,321 iteration 2858 : loss : 0.053034, loss_ce: 0.017347
2022-01-08 00:37:29,763 iteration 2859 : loss : 0.034844, loss_ce: 0.013138
2022-01-08 00:37:31,229 iteration 2860 : loss : 0.030215, loss_ce: 0.011736
2022-01-08 00:37:32,642 iteration 2861 : loss : 0.034994, loss_ce: 0.010030
2022-01-08 00:37:34,081 iteration 2862 : loss : 0.053290, loss_ce: 0.013526
2022-01-08 00:37:35,437 iteration 2863 : loss : 0.029187, loss_ce: 0.010723
2022-01-08 00:37:36,847 iteration 2864 : loss : 0.038085, loss_ce: 0.014510
2022-01-08 00:37:38,230 iteration 2865 : loss : 0.030289, loss_ce: 0.012063
2022-01-08 00:37:39,612 iteration 2866 : loss : 0.025869, loss_ce: 0.008575
2022-01-08 00:37:41,097 iteration 2867 : loss : 0.048767, loss_ce: 0.015665
2022-01-08 00:37:42,561 iteration 2868 : loss : 0.042584, loss_ce: 0.016901
2022-01-08 00:37:44,008 iteration 2869 : loss : 0.043914, loss_ce: 0.016209
2022-01-08 00:37:45,498 iteration 2870 : loss : 0.043360, loss_ce: 0.014588
2022-01-08 00:37:46,837 iteration 2871 : loss : 0.044636, loss_ce: 0.025893
2022-01-08 00:37:48,220 iteration 2872 : loss : 0.034330, loss_ce: 0.016012
2022-01-08 00:37:49,622 iteration 2873 : loss : 0.031484, loss_ce: 0.013052
 42%|███████████▍               | 169/400 [1:13:13<1:35:50, 24.89s/it]2022-01-08 00:37:51,053 iteration 2874 : loss : 0.032660, loss_ce: 0.012351
2022-01-08 00:37:52,519 iteration 2875 : loss : 0.030983, loss_ce: 0.012641
2022-01-08 00:37:53,884 iteration 2876 : loss : 0.045933, loss_ce: 0.017228
2022-01-08 00:37:55,225 iteration 2877 : loss : 0.055873, loss_ce: 0.023581
2022-01-08 00:37:56,606 iteration 2878 : loss : 0.037644, loss_ce: 0.011005
2022-01-08 00:37:57,919 iteration 2879 : loss : 0.043141, loss_ce: 0.024109
2022-01-08 00:37:59,337 iteration 2880 : loss : 0.039207, loss_ce: 0.019336
2022-01-08 00:38:00,662 iteration 2881 : loss : 0.028543, loss_ce: 0.013423
2022-01-08 00:38:02,023 iteration 2882 : loss : 0.047028, loss_ce: 0.014861
2022-01-08 00:38:03,446 iteration 2883 : loss : 0.047544, loss_ce: 0.014015
2022-01-08 00:38:04,884 iteration 2884 : loss : 0.051337, loss_ce: 0.022622
2022-01-08 00:38:06,289 iteration 2885 : loss : 0.049416, loss_ce: 0.020468
2022-01-08 00:38:07,665 iteration 2886 : loss : 0.034253, loss_ce: 0.011485
2022-01-08 00:38:09,015 iteration 2887 : loss : 0.049192, loss_ce: 0.015555
2022-01-08 00:38:10,443 iteration 2888 : loss : 0.035753, loss_ce: 0.015264
2022-01-08 00:38:11,838 iteration 2889 : loss : 0.038655, loss_ce: 0.014594
2022-01-08 00:38:11,838 Training Data Eval:
2022-01-08 00:38:18,907   Average segmentation loss on training set: 0.0257
2022-01-08 00:38:18,907 Validation Data Eval:
2022-01-08 00:38:21,300   Average segmentation loss on validation set: 0.0834
2022-01-08 00:38:22,665 iteration 2890 : loss : 0.043066, loss_ce: 0.018300
 42%|███████████▍               | 170/400 [1:13:46<1:44:47, 27.34s/it]2022-01-08 00:38:24,082 iteration 2891 : loss : 0.050457, loss_ce: 0.021181
2022-01-08 00:38:25,516 iteration 2892 : loss : 0.038779, loss_ce: 0.015465
2022-01-08 00:38:26,832 iteration 2893 : loss : 0.026930, loss_ce: 0.011472
2022-01-08 00:38:28,177 iteration 2894 : loss : 0.033457, loss_ce: 0.013635
2022-01-08 00:38:29,521 iteration 2895 : loss : 0.037368, loss_ce: 0.012463
2022-01-08 00:38:30,916 iteration 2896 : loss : 0.056095, loss_ce: 0.026510
2022-01-08 00:38:32,423 iteration 2897 : loss : 0.079779, loss_ce: 0.024106
2022-01-08 00:38:33,838 iteration 2898 : loss : 0.037599, loss_ce: 0.013537
2022-01-08 00:38:35,235 iteration 2899 : loss : 0.038887, loss_ce: 0.017603
2022-01-08 00:38:36,779 iteration 2900 : loss : 0.071484, loss_ce: 0.034653
2022-01-08 00:38:38,225 iteration 2901 : loss : 0.047510, loss_ce: 0.020682
2022-01-08 00:38:39,599 iteration 2902 : loss : 0.038269, loss_ce: 0.016961
2022-01-08 00:38:40,998 iteration 2903 : loss : 0.056937, loss_ce: 0.013079
2022-01-08 00:38:42,383 iteration 2904 : loss : 0.038375, loss_ce: 0.019488
2022-01-08 00:38:43,841 iteration 2905 : loss : 0.030032, loss_ce: 0.012960
2022-01-08 00:38:45,191 iteration 2906 : loss : 0.046532, loss_ce: 0.017703
2022-01-08 00:38:46,582 iteration 2907 : loss : 0.034530, loss_ce: 0.012956
 43%|███████████▌               | 171/400 [1:14:10<1:40:24, 26.31s/it]2022-01-08 00:38:48,036 iteration 2908 : loss : 0.043298, loss_ce: 0.016135
2022-01-08 00:38:49,520 iteration 2909 : loss : 0.039330, loss_ce: 0.012999
2022-01-08 00:38:50,986 iteration 2910 : loss : 0.024717, loss_ce: 0.008516
2022-01-08 00:38:52,436 iteration 2911 : loss : 0.041861, loss_ce: 0.016876
2022-01-08 00:38:53,799 iteration 2912 : loss : 0.025241, loss_ce: 0.009531
2022-01-08 00:38:55,228 iteration 2913 : loss : 0.032902, loss_ce: 0.011237
2022-01-08 00:38:56,681 iteration 2914 : loss : 0.046860, loss_ce: 0.020370
2022-01-08 00:38:58,086 iteration 2915 : loss : 0.046975, loss_ce: 0.026798
2022-01-08 00:38:59,482 iteration 2916 : loss : 0.033768, loss_ce: 0.013655
2022-01-08 00:39:00,883 iteration 2917 : loss : 0.047706, loss_ce: 0.018545
2022-01-08 00:39:02,220 iteration 2918 : loss : 0.034784, loss_ce: 0.016955
2022-01-08 00:39:03,622 iteration 2919 : loss : 0.048967, loss_ce: 0.016736
2022-01-08 00:39:04,995 iteration 2920 : loss : 0.026362, loss_ce: 0.010770
2022-01-08 00:39:06,383 iteration 2921 : loss : 0.031457, loss_ce: 0.014086
2022-01-08 00:39:07,777 iteration 2922 : loss : 0.074415, loss_ce: 0.030870
2022-01-08 00:39:09,247 iteration 2923 : loss : 0.038474, loss_ce: 0.011617
2022-01-08 00:39:10,683 iteration 2924 : loss : 0.026831, loss_ce: 0.011341
 43%|███████████▌               | 172/400 [1:14:34<1:37:28, 25.65s/it]2022-01-08 00:39:12,134 iteration 2925 : loss : 0.033233, loss_ce: 0.015320
2022-01-08 00:39:13,487 iteration 2926 : loss : 0.028798, loss_ce: 0.010723
2022-01-08 00:39:14,913 iteration 2927 : loss : 0.033128, loss_ce: 0.011049
2022-01-08 00:39:16,344 iteration 2928 : loss : 0.072543, loss_ce: 0.038254
2022-01-08 00:39:17,697 iteration 2929 : loss : 0.036197, loss_ce: 0.011068
2022-01-08 00:39:19,058 iteration 2930 : loss : 0.028658, loss_ce: 0.012858
2022-01-08 00:39:20,469 iteration 2931 : loss : 0.103525, loss_ce: 0.013528
2022-01-08 00:39:21,940 iteration 2932 : loss : 0.049487, loss_ce: 0.018603
2022-01-08 00:39:23,375 iteration 2933 : loss : 0.038878, loss_ce: 0.016222
2022-01-08 00:39:24,843 iteration 2934 : loss : 0.038358, loss_ce: 0.020598
2022-01-08 00:39:26,232 iteration 2935 : loss : 0.036761, loss_ce: 0.011755
2022-01-08 00:39:27,625 iteration 2936 : loss : 0.062896, loss_ce: 0.031094
2022-01-08 00:39:29,038 iteration 2937 : loss : 0.040979, loss_ce: 0.021103
2022-01-08 00:39:30,467 iteration 2938 : loss : 0.053650, loss_ce: 0.019699
2022-01-08 00:39:31,786 iteration 2939 : loss : 0.050142, loss_ce: 0.013607
2022-01-08 00:39:33,177 iteration 2940 : loss : 0.044858, loss_ce: 0.016853
2022-01-08 00:39:34,548 iteration 2941 : loss : 0.052692, loss_ce: 0.021635
 43%|███████████▋               | 173/400 [1:14:58<1:35:00, 25.11s/it]2022-01-08 00:39:36,068 iteration 2942 : loss : 0.037274, loss_ce: 0.015648
2022-01-08 00:39:37,541 iteration 2943 : loss : 0.041735, loss_ce: 0.018175
2022-01-08 00:39:38,957 iteration 2944 : loss : 0.088556, loss_ce: 0.030633
2022-01-08 00:39:40,354 iteration 2945 : loss : 0.061777, loss_ce: 0.027130
2022-01-08 00:39:41,815 iteration 2946 : loss : 0.039461, loss_ce: 0.015802
2022-01-08 00:39:43,197 iteration 2947 : loss : 0.042237, loss_ce: 0.015907
2022-01-08 00:39:44,618 iteration 2948 : loss : 0.036123, loss_ce: 0.014394
2022-01-08 00:39:45,986 iteration 2949 : loss : 0.039831, loss_ce: 0.014678
2022-01-08 00:39:47,298 iteration 2950 : loss : 0.032082, loss_ce: 0.012200
2022-01-08 00:39:48,683 iteration 2951 : loss : 0.033822, loss_ce: 0.012518
2022-01-08 00:39:50,005 iteration 2952 : loss : 0.117759, loss_ce: 0.024709
2022-01-08 00:39:51,459 iteration 2953 : loss : 0.038460, loss_ce: 0.014164
2022-01-08 00:39:52,855 iteration 2954 : loss : 0.033525, loss_ce: 0.013216
2022-01-08 00:39:54,255 iteration 2955 : loss : 0.058827, loss_ce: 0.015383
2022-01-08 00:39:55,607 iteration 2956 : loss : 0.061721, loss_ce: 0.031733
2022-01-08 00:39:56,939 iteration 2957 : loss : 0.039664, loss_ce: 0.013243
2022-01-08 00:39:58,333 iteration 2958 : loss : 0.058780, loss_ce: 0.022852
 44%|███████████▋               | 174/400 [1:15:22<1:33:05, 24.72s/it]2022-01-08 00:39:59,796 iteration 2959 : loss : 0.047058, loss_ce: 0.021324
2022-01-08 00:40:01,208 iteration 2960 : loss : 0.042278, loss_ce: 0.019831
2022-01-08 00:40:02,652 iteration 2961 : loss : 0.034373, loss_ce: 0.016359
2022-01-08 00:40:04,086 iteration 2962 : loss : 0.057845, loss_ce: 0.020490
2022-01-08 00:40:05,449 iteration 2963 : loss : 0.046483, loss_ce: 0.022358
2022-01-08 00:40:06,805 iteration 2964 : loss : 0.040158, loss_ce: 0.014964
2022-01-08 00:40:08,139 iteration 2965 : loss : 0.064873, loss_ce: 0.021368
2022-01-08 00:40:09,446 iteration 2966 : loss : 0.039310, loss_ce: 0.014892
2022-01-08 00:40:10,855 iteration 2967 : loss : 0.042572, loss_ce: 0.020783
2022-01-08 00:40:12,345 iteration 2968 : loss : 0.101115, loss_ce: 0.024678
2022-01-08 00:40:13,814 iteration 2969 : loss : 0.027397, loss_ce: 0.011728
2022-01-08 00:40:15,192 iteration 2970 : loss : 0.037127, loss_ce: 0.012084
2022-01-08 00:40:16,523 iteration 2971 : loss : 0.045811, loss_ce: 0.020855
2022-01-08 00:40:17,951 iteration 2972 : loss : 0.050427, loss_ce: 0.019557
2022-01-08 00:40:19,325 iteration 2973 : loss : 0.035310, loss_ce: 0.014493
2022-01-08 00:40:20,807 iteration 2974 : loss : 0.030081, loss_ce: 0.010739
2022-01-08 00:40:20,807 Training Data Eval:
2022-01-08 00:40:27,858   Average segmentation loss on training set: 0.0285
2022-01-08 00:40:27,859 Validation Data Eval:
2022-01-08 00:40:30,250   Average segmentation loss on validation set: 0.0986
2022-01-08 00:40:31,690 iteration 2975 : loss : 0.051248, loss_ce: 0.014171
 44%|███████████▊               | 175/400 [1:15:55<1:42:24, 27.31s/it]2022-01-08 00:40:33,205 iteration 2976 : loss : 0.056024, loss_ce: 0.029493
2022-01-08 00:40:34,530 iteration 2977 : loss : 0.040611, loss_ce: 0.017396
2022-01-08 00:40:35,834 iteration 2978 : loss : 0.031738, loss_ce: 0.011287
2022-01-08 00:40:37,280 iteration 2979 : loss : 0.024466, loss_ce: 0.008876
2022-01-08 00:40:38,836 iteration 2980 : loss : 0.057900, loss_ce: 0.025352
2022-01-08 00:40:40,253 iteration 2981 : loss : 0.038200, loss_ce: 0.013323
2022-01-08 00:40:41,732 iteration 2982 : loss : 0.041985, loss_ce: 0.018610
2022-01-08 00:40:43,119 iteration 2983 : loss : 0.061230, loss_ce: 0.023903
2022-01-08 00:40:44,574 iteration 2984 : loss : 0.030946, loss_ce: 0.012795
2022-01-08 00:40:45,993 iteration 2985 : loss : 0.055165, loss_ce: 0.019060
2022-01-08 00:40:47,445 iteration 2986 : loss : 0.060290, loss_ce: 0.023640
2022-01-08 00:40:48,793 iteration 2987 : loss : 0.038239, loss_ce: 0.016304
2022-01-08 00:40:50,162 iteration 2988 : loss : 0.028574, loss_ce: 0.010204
2022-01-08 00:40:51,574 iteration 2989 : loss : 0.040019, loss_ce: 0.015016
2022-01-08 00:40:52,998 iteration 2990 : loss : 0.062895, loss_ce: 0.026059
2022-01-08 00:40:54,474 iteration 2991 : loss : 0.031862, loss_ce: 0.011281
2022-01-08 00:40:56,053 iteration 2992 : loss : 0.033988, loss_ce: 0.014505
 44%|███████████▉               | 176/400 [1:16:20<1:38:38, 26.42s/it]2022-01-08 00:40:57,426 iteration 2993 : loss : 0.043547, loss_ce: 0.012106
2022-01-08 00:40:58,811 iteration 2994 : loss : 0.033593, loss_ce: 0.012959
2022-01-08 00:41:00,298 iteration 2995 : loss : 0.037586, loss_ce: 0.015906
2022-01-08 00:41:01,719 iteration 2996 : loss : 0.031712, loss_ce: 0.010831
2022-01-08 00:41:03,133 iteration 2997 : loss : 0.031014, loss_ce: 0.011712
2022-01-08 00:41:04,514 iteration 2998 : loss : 0.027875, loss_ce: 0.008206
2022-01-08 00:41:05,952 iteration 2999 : loss : 0.047225, loss_ce: 0.016794
2022-01-08 00:41:07,341 iteration 3000 : loss : 0.045215, loss_ce: 0.014547
2022-01-08 00:41:08,800 iteration 3001 : loss : 0.025349, loss_ce: 0.010227
2022-01-08 00:41:10,116 iteration 3002 : loss : 0.040006, loss_ce: 0.017331
2022-01-08 00:41:11,564 iteration 3003 : loss : 0.035793, loss_ce: 0.014295
2022-01-08 00:41:12,876 iteration 3004 : loss : 0.030829, loss_ce: 0.012820
2022-01-08 00:41:14,318 iteration 3005 : loss : 0.068267, loss_ce: 0.024291
2022-01-08 00:41:15,743 iteration 3006 : loss : 0.039338, loss_ce: 0.014352
2022-01-08 00:41:17,113 iteration 3007 : loss : 0.026447, loss_ce: 0.010429
2022-01-08 00:41:18,620 iteration 3008 : loss : 0.043143, loss_ce: 0.017698
2022-01-08 00:41:20,024 iteration 3009 : loss : 0.036479, loss_ce: 0.019411
 44%|███████████▉               | 177/400 [1:16:44<1:35:28, 25.69s/it]2022-01-08 00:41:21,449 iteration 3010 : loss : 0.029519, loss_ce: 0.011211
2022-01-08 00:41:22,833 iteration 3011 : loss : 0.048946, loss_ce: 0.017745
2022-01-08 00:41:24,192 iteration 3012 : loss : 0.034532, loss_ce: 0.011731
2022-01-08 00:41:25,559 iteration 3013 : loss : 0.027444, loss_ce: 0.011777
2022-01-08 00:41:27,050 iteration 3014 : loss : 0.035779, loss_ce: 0.017394
2022-01-08 00:41:28,458 iteration 3015 : loss : 0.036129, loss_ce: 0.012132
2022-01-08 00:41:29,842 iteration 3016 : loss : 0.034866, loss_ce: 0.016009
2022-01-08 00:41:31,223 iteration 3017 : loss : 0.043547, loss_ce: 0.010587
2022-01-08 00:41:32,566 iteration 3018 : loss : 0.026146, loss_ce: 0.007842
2022-01-08 00:41:33,936 iteration 3019 : loss : 0.041691, loss_ce: 0.015955
2022-01-08 00:41:35,208 iteration 3020 : loss : 0.027198, loss_ce: 0.013952
2022-01-08 00:41:36,507 iteration 3021 : loss : 0.040442, loss_ce: 0.015150
2022-01-08 00:41:37,889 iteration 3022 : loss : 0.034630, loss_ce: 0.010294
2022-01-08 00:41:39,268 iteration 3023 : loss : 0.031228, loss_ce: 0.015711
2022-01-08 00:41:40,659 iteration 3024 : loss : 0.038473, loss_ce: 0.015643
2022-01-08 00:41:41,950 iteration 3025 : loss : 0.029388, loss_ce: 0.013867
2022-01-08 00:41:43,370 iteration 3026 : loss : 0.032242, loss_ce: 0.014648
 44%|████████████               | 178/400 [1:17:07<1:32:27, 24.99s/it]2022-01-08 00:41:44,828 iteration 3027 : loss : 0.030357, loss_ce: 0.014035
2022-01-08 00:41:46,209 iteration 3028 : loss : 0.036508, loss_ce: 0.013661
2022-01-08 00:41:47,608 iteration 3029 : loss : 0.026055, loss_ce: 0.011476
2022-01-08 00:41:49,094 iteration 3030 : loss : 0.034399, loss_ce: 0.012520
2022-01-08 00:41:50,411 iteration 3031 : loss : 0.027811, loss_ce: 0.011221
2022-01-08 00:41:51,789 iteration 3032 : loss : 0.029997, loss_ce: 0.010459
2022-01-08 00:41:53,202 iteration 3033 : loss : 0.033416, loss_ce: 0.017978
2022-01-08 00:41:54,573 iteration 3034 : loss : 0.037611, loss_ce: 0.013052
2022-01-08 00:41:56,076 iteration 3035 : loss : 0.046545, loss_ce: 0.013843
2022-01-08 00:41:57,486 iteration 3036 : loss : 0.028478, loss_ce: 0.011939
2022-01-08 00:41:58,929 iteration 3037 : loss : 0.037725, loss_ce: 0.014649
2022-01-08 00:42:00,250 iteration 3038 : loss : 0.025311, loss_ce: 0.006020
2022-01-08 00:42:01,828 iteration 3039 : loss : 0.061014, loss_ce: 0.022428
2022-01-08 00:42:03,181 iteration 3040 : loss : 0.034778, loss_ce: 0.015006
2022-01-08 00:42:04,536 iteration 3041 : loss : 0.037461, loss_ce: 0.015580
2022-01-08 00:42:05,882 iteration 3042 : loss : 0.032520, loss_ce: 0.014539
2022-01-08 00:42:07,378 iteration 3043 : loss : 0.034905, loss_ce: 0.013973
 45%|████████████               | 179/400 [1:17:31<1:30:57, 24.69s/it]2022-01-08 00:42:08,871 iteration 3044 : loss : 0.042020, loss_ce: 0.016577
2022-01-08 00:42:10,268 iteration 3045 : loss : 0.030406, loss_ce: 0.012574
2022-01-08 00:42:11,630 iteration 3046 : loss : 0.040798, loss_ce: 0.014182
2022-01-08 00:42:13,139 iteration 3047 : loss : 0.029482, loss_ce: 0.011764
2022-01-08 00:42:14,630 iteration 3048 : loss : 0.037200, loss_ce: 0.020132
2022-01-08 00:42:15,993 iteration 3049 : loss : 0.034263, loss_ce: 0.014077
2022-01-08 00:42:17,484 iteration 3050 : loss : 0.041911, loss_ce: 0.023958
2022-01-08 00:42:18,923 iteration 3051 : loss : 0.048831, loss_ce: 0.015232
2022-01-08 00:42:20,444 iteration 3052 : loss : 0.047547, loss_ce: 0.017175
2022-01-08 00:42:21,868 iteration 3053 : loss : 0.038898, loss_ce: 0.015822
2022-01-08 00:42:23,286 iteration 3054 : loss : 0.040343, loss_ce: 0.013689
2022-01-08 00:42:24,711 iteration 3055 : loss : 0.033577, loss_ce: 0.012568
2022-01-08 00:42:26,127 iteration 3056 : loss : 0.038688, loss_ce: 0.013072
2022-01-08 00:42:27,472 iteration 3057 : loss : 0.031888, loss_ce: 0.012383
2022-01-08 00:42:28,938 iteration 3058 : loss : 0.055474, loss_ce: 0.017921
2022-01-08 00:42:30,413 iteration 3059 : loss : 0.049134, loss_ce: 0.021975
2022-01-08 00:42:30,413 Training Data Eval:
2022-01-08 00:42:37,452   Average segmentation loss on training set: 0.0304
2022-01-08 00:42:37,452 Validation Data Eval:
2022-01-08 00:42:39,843   Average segmentation loss on validation set: 0.1054
2022-01-08 00:42:41,227 iteration 3060 : loss : 0.049138, loss_ce: 0.019668
 45%|████████████▏              | 180/400 [1:18:05<1:40:36, 27.44s/it]2022-01-08 00:42:42,630 iteration 3061 : loss : 0.036277, loss_ce: 0.012403
2022-01-08 00:42:44,078 iteration 3062 : loss : 0.038861, loss_ce: 0.013416
2022-01-08 00:42:45,449 iteration 3063 : loss : 0.043261, loss_ce: 0.016062
2022-01-08 00:42:46,899 iteration 3064 : loss : 0.048169, loss_ce: 0.026290
2022-01-08 00:42:48,286 iteration 3065 : loss : 0.038054, loss_ce: 0.011222
2022-01-08 00:42:49,684 iteration 3066 : loss : 0.061626, loss_ce: 0.017021
2022-01-08 00:42:51,110 iteration 3067 : loss : 0.036166, loss_ce: 0.017145
2022-01-08 00:42:52,575 iteration 3068 : loss : 0.025398, loss_ce: 0.008979
2022-01-08 00:42:54,035 iteration 3069 : loss : 0.068836, loss_ce: 0.021017
2022-01-08 00:42:55,370 iteration 3070 : loss : 0.033556, loss_ce: 0.012480
2022-01-08 00:42:56,865 iteration 3071 : loss : 0.039543, loss_ce: 0.017572
2022-01-08 00:42:58,288 iteration 3072 : loss : 0.030525, loss_ce: 0.012325
2022-01-08 00:42:59,646 iteration 3073 : loss : 0.051782, loss_ce: 0.022295
2022-01-08 00:43:01,148 iteration 3074 : loss : 0.054575, loss_ce: 0.025501
2022-01-08 00:43:02,628 iteration 3075 : loss : 0.032062, loss_ce: 0.013171
2022-01-08 00:43:04,010 iteration 3076 : loss : 0.030983, loss_ce: 0.014835
2022-01-08 00:43:05,486 iteration 3077 : loss : 0.055452, loss_ce: 0.028815
 45%|████████████▏              | 181/400 [1:18:29<1:36:40, 26.49s/it]2022-01-08 00:43:06,821 iteration 3078 : loss : 0.023211, loss_ce: 0.009673
2022-01-08 00:43:08,299 iteration 3079 : loss : 0.044302, loss_ce: 0.015075
2022-01-08 00:43:09,699 iteration 3080 : loss : 0.029484, loss_ce: 0.011322
2022-01-08 00:43:11,072 iteration 3081 : loss : 0.038222, loss_ce: 0.011331
2022-01-08 00:43:12,480 iteration 3082 : loss : 0.029435, loss_ce: 0.011597
2022-01-08 00:43:13,907 iteration 3083 : loss : 0.034640, loss_ce: 0.015020
2022-01-08 00:43:15,293 iteration 3084 : loss : 0.025832, loss_ce: 0.010207
2022-01-08 00:43:16,694 iteration 3085 : loss : 0.070592, loss_ce: 0.011951
2022-01-08 00:43:18,052 iteration 3086 : loss : 0.029723, loss_ce: 0.010928
2022-01-08 00:43:19,415 iteration 3087 : loss : 0.033008, loss_ce: 0.010016
2022-01-08 00:43:20,772 iteration 3088 : loss : 0.035492, loss_ce: 0.012412
2022-01-08 00:43:22,112 iteration 3089 : loss : 0.039016, loss_ce: 0.015589
2022-01-08 00:43:23,524 iteration 3090 : loss : 0.072803, loss_ce: 0.026815
2022-01-08 00:43:24,878 iteration 3091 : loss : 0.044284, loss_ce: 0.016297
2022-01-08 00:43:26,205 iteration 3092 : loss : 0.034905, loss_ce: 0.015439
2022-01-08 00:43:27,583 iteration 3093 : loss : 0.028948, loss_ce: 0.009249
2022-01-08 00:43:28,934 iteration 3094 : loss : 0.045427, loss_ce: 0.021906
 46%|████████████▎              | 182/400 [1:18:53<1:32:54, 25.57s/it]2022-01-08 00:43:30,425 iteration 3095 : loss : 0.028874, loss_ce: 0.009945
2022-01-08 00:43:31,817 iteration 3096 : loss : 0.027049, loss_ce: 0.011071
2022-01-08 00:43:33,194 iteration 3097 : loss : 0.036619, loss_ce: 0.014709
2022-01-08 00:43:34,539 iteration 3098 : loss : 0.030553, loss_ce: 0.011476
2022-01-08 00:43:35,992 iteration 3099 : loss : 0.048371, loss_ce: 0.015681
2022-01-08 00:43:37,338 iteration 3100 : loss : 0.033767, loss_ce: 0.011467
2022-01-08 00:43:38,728 iteration 3101 : loss : 0.029705, loss_ce: 0.014246
2022-01-08 00:43:40,162 iteration 3102 : loss : 0.033419, loss_ce: 0.012882
2022-01-08 00:43:41,535 iteration 3103 : loss : 0.033235, loss_ce: 0.009897
2022-01-08 00:43:42,927 iteration 3104 : loss : 0.028890, loss_ce: 0.011294
2022-01-08 00:43:44,302 iteration 3105 : loss : 0.024735, loss_ce: 0.007828
2022-01-08 00:43:45,835 iteration 3106 : loss : 0.027833, loss_ce: 0.010841
2022-01-08 00:43:47,221 iteration 3107 : loss : 0.036104, loss_ce: 0.015075
2022-01-08 00:43:48,545 iteration 3108 : loss : 0.030805, loss_ce: 0.010969
2022-01-08 00:43:50,019 iteration 3109 : loss : 0.041596, loss_ce: 0.020520
2022-01-08 00:43:51,472 iteration 3110 : loss : 0.053517, loss_ce: 0.023322
2022-01-08 00:43:52,826 iteration 3111 : loss : 0.031389, loss_ce: 0.009223
 46%|████████████▎              | 183/400 [1:19:17<1:30:39, 25.07s/it]2022-01-08 00:43:54,216 iteration 3112 : loss : 0.022724, loss_ce: 0.008401
2022-01-08 00:43:55,598 iteration 3113 : loss : 0.034756, loss_ce: 0.015273
2022-01-08 00:43:57,139 iteration 3114 : loss : 0.070776, loss_ce: 0.019757
2022-01-08 00:43:58,587 iteration 3115 : loss : 0.031376, loss_ce: 0.013322
2022-01-08 00:44:00,004 iteration 3116 : loss : 0.033514, loss_ce: 0.015598
2022-01-08 00:44:01,390 iteration 3117 : loss : 0.034831, loss_ce: 0.012968
2022-01-08 00:44:02,877 iteration 3118 : loss : 0.059988, loss_ce: 0.024377
2022-01-08 00:44:04,309 iteration 3119 : loss : 0.042941, loss_ce: 0.017707
2022-01-08 00:44:05,721 iteration 3120 : loss : 0.028230, loss_ce: 0.009421
2022-01-08 00:44:07,143 iteration 3121 : loss : 0.041265, loss_ce: 0.014813
2022-01-08 00:44:08,651 iteration 3122 : loss : 0.042192, loss_ce: 0.012088
2022-01-08 00:44:10,061 iteration 3123 : loss : 0.029737, loss_ce: 0.012343
2022-01-08 00:44:11,484 iteration 3124 : loss : 0.031420, loss_ce: 0.012654
2022-01-08 00:44:12,902 iteration 3125 : loss : 0.033587, loss_ce: 0.015879
2022-01-08 00:44:14,254 iteration 3126 : loss : 0.031286, loss_ce: 0.011904
2022-01-08 00:44:15,670 iteration 3127 : loss : 0.032155, loss_ce: 0.012231
2022-01-08 00:44:16,990 iteration 3128 : loss : 0.027714, loss_ce: 0.009490
 46%|████████████▍              | 184/400 [1:19:41<1:29:15, 24.80s/it]2022-01-08 00:44:18,463 iteration 3129 : loss : 0.035864, loss_ce: 0.014423
2022-01-08 00:44:19,907 iteration 3130 : loss : 0.030406, loss_ce: 0.010543
2022-01-08 00:44:21,373 iteration 3131 : loss : 0.033614, loss_ce: 0.012975
2022-01-08 00:44:22,753 iteration 3132 : loss : 0.030654, loss_ce: 0.012096
2022-01-08 00:44:24,085 iteration 3133 : loss : 0.028789, loss_ce: 0.010330
2022-01-08 00:44:25,484 iteration 3134 : loss : 0.023934, loss_ce: 0.007180
2022-01-08 00:44:26,903 iteration 3135 : loss : 0.034341, loss_ce: 0.015192
2022-01-08 00:44:28,359 iteration 3136 : loss : 0.051745, loss_ce: 0.019605
2022-01-08 00:44:29,776 iteration 3137 : loss : 0.046588, loss_ce: 0.020064
2022-01-08 00:44:31,213 iteration 3138 : loss : 0.040463, loss_ce: 0.018165
2022-01-08 00:44:32,724 iteration 3139 : loss : 0.039615, loss_ce: 0.013858
2022-01-08 00:44:34,168 iteration 3140 : loss : 0.041065, loss_ce: 0.015760
2022-01-08 00:44:35,616 iteration 3141 : loss : 0.030527, loss_ce: 0.011618
2022-01-08 00:44:37,017 iteration 3142 : loss : 0.035298, loss_ce: 0.012401
2022-01-08 00:44:38,514 iteration 3143 : loss : 0.032712, loss_ce: 0.014154
2022-01-08 00:44:40,007 iteration 3144 : loss : 0.037043, loss_ce: 0.019755
2022-01-08 00:44:40,007 Training Data Eval:
2022-01-08 00:44:47,051   Average segmentation loss on training set: 0.0267
2022-01-08 00:44:47,051 Validation Data Eval:
2022-01-08 00:44:49,439   Average segmentation loss on validation set: 0.0633
2022-01-08 00:44:53,570 Found new lowest validation loss at iteration 3144! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-08 00:44:54,757 iteration 3145 : loss : 0.036903, loss_ce: 0.012978
 46%|████████████▍              | 185/400 [1:20:18<1:42:48, 28.69s/it]2022-01-08 00:44:56,000 iteration 3146 : loss : 0.031187, loss_ce: 0.011105
2022-01-08 00:44:57,364 iteration 3147 : loss : 0.029365, loss_ce: 0.013347
2022-01-08 00:44:58,742 iteration 3148 : loss : 0.032014, loss_ce: 0.010296
2022-01-08 00:45:00,101 iteration 3149 : loss : 0.030410, loss_ce: 0.012145
2022-01-08 00:45:01,473 iteration 3150 : loss : 0.031419, loss_ce: 0.014407
2022-01-08 00:45:02,841 iteration 3151 : loss : 0.037704, loss_ce: 0.008924
2022-01-08 00:45:04,228 iteration 3152 : loss : 0.032990, loss_ce: 0.014221
2022-01-08 00:45:05,631 iteration 3153 : loss : 0.037999, loss_ce: 0.014979
2022-01-08 00:45:06,987 iteration 3154 : loss : 0.032725, loss_ce: 0.015342
2022-01-08 00:45:08,373 iteration 3155 : loss : 0.028586, loss_ce: 0.010098
2022-01-08 00:45:09,823 iteration 3156 : loss : 0.041823, loss_ce: 0.014307
2022-01-08 00:45:11,249 iteration 3157 : loss : 0.031538, loss_ce: 0.013650
2022-01-08 00:45:12,626 iteration 3158 : loss : 0.036189, loss_ce: 0.015796
2022-01-08 00:45:14,019 iteration 3159 : loss : 0.027562, loss_ce: 0.010517
2022-01-08 00:45:15,425 iteration 3160 : loss : 0.029360, loss_ce: 0.011606
2022-01-08 00:45:16,766 iteration 3161 : loss : 0.035555, loss_ce: 0.012203
2022-01-08 00:45:18,225 iteration 3162 : loss : 0.033413, loss_ce: 0.012124
 46%|████████████▌              | 186/400 [1:20:42<1:36:44, 27.12s/it]2022-01-08 00:45:19,727 iteration 3163 : loss : 0.045926, loss_ce: 0.020724
2022-01-08 00:45:21,187 iteration 3164 : loss : 0.023476, loss_ce: 0.008373
2022-01-08 00:45:22,567 iteration 3165 : loss : 0.021520, loss_ce: 0.007536
2022-01-08 00:45:24,022 iteration 3166 : loss : 0.032889, loss_ce: 0.009712
2022-01-08 00:45:25,419 iteration 3167 : loss : 0.024129, loss_ce: 0.009965
2022-01-08 00:45:26,987 iteration 3168 : loss : 0.033785, loss_ce: 0.015773
2022-01-08 00:45:28,478 iteration 3169 : loss : 0.032137, loss_ce: 0.015083
2022-01-08 00:45:29,855 iteration 3170 : loss : 0.026633, loss_ce: 0.011678
2022-01-08 00:45:31,182 iteration 3171 : loss : 0.024730, loss_ce: 0.010746
2022-01-08 00:45:32,627 iteration 3172 : loss : 0.031754, loss_ce: 0.010985
2022-01-08 00:45:34,073 iteration 3173 : loss : 0.027298, loss_ce: 0.010107
2022-01-08 00:45:35,470 iteration 3174 : loss : 0.036280, loss_ce: 0.014749
2022-01-08 00:45:36,962 iteration 3175 : loss : 0.048456, loss_ce: 0.013992
2022-01-08 00:45:38,381 iteration 3176 : loss : 0.035214, loss_ce: 0.012017
2022-01-08 00:45:39,853 iteration 3177 : loss : 0.048323, loss_ce: 0.016981
2022-01-08 00:45:41,195 iteration 3178 : loss : 0.032259, loss_ce: 0.011415
2022-01-08 00:45:42,566 iteration 3179 : loss : 0.041270, loss_ce: 0.012693
 47%|████████████▌              | 187/400 [1:21:06<1:33:19, 26.29s/it]2022-01-08 00:45:44,066 iteration 3180 : loss : 0.044421, loss_ce: 0.018755
2022-01-08 00:45:45,454 iteration 3181 : loss : 0.027194, loss_ce: 0.010237
2022-01-08 00:45:46,868 iteration 3182 : loss : 0.040465, loss_ce: 0.013401
2022-01-08 00:45:48,242 iteration 3183 : loss : 0.031562, loss_ce: 0.008452
2022-01-08 00:45:49,650 iteration 3184 : loss : 0.033488, loss_ce: 0.011276
2022-01-08 00:45:51,121 iteration 3185 : loss : 0.045954, loss_ce: 0.016888
2022-01-08 00:45:52,444 iteration 3186 : loss : 0.033030, loss_ce: 0.015612
2022-01-08 00:45:53,853 iteration 3187 : loss : 0.028187, loss_ce: 0.010959
2022-01-08 00:45:55,233 iteration 3188 : loss : 0.030468, loss_ce: 0.013745
2022-01-08 00:45:56,676 iteration 3189 : loss : 0.039056, loss_ce: 0.016675
2022-01-08 00:45:58,069 iteration 3190 : loss : 0.031230, loss_ce: 0.012939
2022-01-08 00:45:59,579 iteration 3191 : loss : 0.038744, loss_ce: 0.011356
2022-01-08 00:46:00,940 iteration 3192 : loss : 0.040041, loss_ce: 0.012325
2022-01-08 00:46:02,414 iteration 3193 : loss : 0.033670, loss_ce: 0.014722
2022-01-08 00:46:03,859 iteration 3194 : loss : 0.056988, loss_ce: 0.019281
2022-01-08 00:46:05,274 iteration 3195 : loss : 0.029324, loss_ce: 0.013662
2022-01-08 00:46:06,671 iteration 3196 : loss : 0.037647, loss_ce: 0.015963
 47%|████████████▋              | 188/400 [1:21:30<1:30:34, 25.63s/it]2022-01-08 00:46:08,042 iteration 3197 : loss : 0.027145, loss_ce: 0.010944
2022-01-08 00:46:09,447 iteration 3198 : loss : 0.031470, loss_ce: 0.012457
2022-01-08 00:46:10,845 iteration 3199 : loss : 0.031194, loss_ce: 0.013685
2022-01-08 00:46:12,341 iteration 3200 : loss : 0.025481, loss_ce: 0.011441
2022-01-08 00:46:13,767 iteration 3201 : loss : 0.055117, loss_ce: 0.013783
2022-01-08 00:46:15,201 iteration 3202 : loss : 0.039551, loss_ce: 0.013441
2022-01-08 00:46:16,703 iteration 3203 : loss : 0.026114, loss_ce: 0.012790
2022-01-08 00:46:18,096 iteration 3204 : loss : 0.025937, loss_ce: 0.007306
2022-01-08 00:46:19,554 iteration 3205 : loss : 0.032737, loss_ce: 0.009740
2022-01-08 00:46:20,995 iteration 3206 : loss : 0.028437, loss_ce: 0.011463
2022-01-08 00:46:22,486 iteration 3207 : loss : 0.043910, loss_ce: 0.019090
2022-01-08 00:46:23,832 iteration 3208 : loss : 0.031608, loss_ce: 0.008372
2022-01-08 00:46:25,183 iteration 3209 : loss : 0.035507, loss_ce: 0.019010
2022-01-08 00:46:26,495 iteration 3210 : loss : 0.025965, loss_ce: 0.012015
2022-01-08 00:46:27,877 iteration 3211 : loss : 0.035342, loss_ce: 0.009718
2022-01-08 00:46:29,247 iteration 3212 : loss : 0.043085, loss_ce: 0.016826
2022-01-08 00:46:30,642 iteration 3213 : loss : 0.034198, loss_ce: 0.009859
 47%|████████████▊              | 189/400 [1:21:54<1:28:23, 25.14s/it]2022-01-08 00:46:32,085 iteration 3214 : loss : 0.033813, loss_ce: 0.014826
2022-01-08 00:46:33,582 iteration 3215 : loss : 0.030011, loss_ce: 0.012615
2022-01-08 00:46:35,038 iteration 3216 : loss : 0.027187, loss_ce: 0.009831
2022-01-08 00:46:36,500 iteration 3217 : loss : 0.073487, loss_ce: 0.015519
2022-01-08 00:46:37,921 iteration 3218 : loss : 0.051320, loss_ce: 0.021402
2022-01-08 00:46:39,321 iteration 3219 : loss : 0.025822, loss_ce: 0.009913
2022-01-08 00:46:40,789 iteration 3220 : loss : 0.035410, loss_ce: 0.014729
2022-01-08 00:46:42,157 iteration 3221 : loss : 0.034086, loss_ce: 0.012873
2022-01-08 00:46:43,565 iteration 3222 : loss : 0.042413, loss_ce: 0.025091
2022-01-08 00:46:45,017 iteration 3223 : loss : 0.027679, loss_ce: 0.008230
2022-01-08 00:46:46,476 iteration 3224 : loss : 0.027131, loss_ce: 0.008677
2022-01-08 00:46:47,866 iteration 3225 : loss : 0.034077, loss_ce: 0.012318
2022-01-08 00:46:49,355 iteration 3226 : loss : 0.048806, loss_ce: 0.014055
2022-01-08 00:46:50,803 iteration 3227 : loss : 0.027115, loss_ce: 0.011354
2022-01-08 00:46:52,134 iteration 3228 : loss : 0.023342, loss_ce: 0.010497
2022-01-08 00:46:53,603 iteration 3229 : loss : 0.034508, loss_ce: 0.015761
2022-01-08 00:46:53,604 Training Data Eval:
2022-01-08 00:47:00,648   Average segmentation loss on training set: 0.0204
2022-01-08 00:47:00,648 Validation Data Eval:
2022-01-08 00:47:03,045   Average segmentation loss on validation set: 0.0745
2022-01-08 00:47:04,563 iteration 3230 : loss : 0.042090, loss_ce: 0.016134
 48%|████████████▊              | 190/400 [1:22:28<1:37:11, 27.77s/it]2022-01-08 00:47:05,981 iteration 3231 : loss : 0.038752, loss_ce: 0.014622
2022-01-08 00:47:07,378 iteration 3232 : loss : 0.026722, loss_ce: 0.012189
2022-01-08 00:47:08,782 iteration 3233 : loss : 0.028258, loss_ce: 0.010423
2022-01-08 00:47:10,129 iteration 3234 : loss : 0.032612, loss_ce: 0.011633
2022-01-08 00:47:11,485 iteration 3235 : loss : 0.025203, loss_ce: 0.006780
2022-01-08 00:47:12,907 iteration 3236 : loss : 0.028663, loss_ce: 0.012069
2022-01-08 00:47:14,341 iteration 3237 : loss : 0.024327, loss_ce: 0.009805
2022-01-08 00:47:15,779 iteration 3238 : loss : 0.028234, loss_ce: 0.006978
2022-01-08 00:47:17,168 iteration 3239 : loss : 0.025040, loss_ce: 0.010118
2022-01-08 00:47:18,583 iteration 3240 : loss : 0.029904, loss_ce: 0.010186
2022-01-08 00:47:20,026 iteration 3241 : loss : 0.028600, loss_ce: 0.011111
2022-01-08 00:47:21,559 iteration 3242 : loss : 0.045448, loss_ce: 0.023074
2022-01-08 00:47:22,922 iteration 3243 : loss : 0.028194, loss_ce: 0.013730
2022-01-08 00:47:24,318 iteration 3244 : loss : 0.039702, loss_ce: 0.010439
2022-01-08 00:47:25,691 iteration 3245 : loss : 0.029171, loss_ce: 0.012272
2022-01-08 00:47:27,175 iteration 3246 : loss : 0.035591, loss_ce: 0.013997
2022-01-08 00:47:28,599 iteration 3247 : loss : 0.034325, loss_ce: 0.013453
 48%|████████████▉              | 191/400 [1:22:52<1:32:50, 26.65s/it]2022-01-08 00:47:30,196 iteration 3248 : loss : 0.055437, loss_ce: 0.030040
2022-01-08 00:47:31,524 iteration 3249 : loss : 0.026432, loss_ce: 0.007000
2022-01-08 00:47:32,967 iteration 3250 : loss : 0.041587, loss_ce: 0.015112
2022-01-08 00:47:34,371 iteration 3251 : loss : 0.048096, loss_ce: 0.015919
2022-01-08 00:47:35,723 iteration 3252 : loss : 0.030795, loss_ce: 0.013304
2022-01-08 00:47:37,148 iteration 3253 : loss : 0.037068, loss_ce: 0.016422
2022-01-08 00:47:38,553 iteration 3254 : loss : 0.039277, loss_ce: 0.011845
2022-01-08 00:47:40,028 iteration 3255 : loss : 0.029573, loss_ce: 0.011197
2022-01-08 00:47:41,374 iteration 3256 : loss : 0.025077, loss_ce: 0.013211
2022-01-08 00:47:42,758 iteration 3257 : loss : 0.039903, loss_ce: 0.013618
2022-01-08 00:47:44,226 iteration 3258 : loss : 0.033840, loss_ce: 0.014487
2022-01-08 00:47:45,643 iteration 3259 : loss : 0.031542, loss_ce: 0.011381
2022-01-08 00:47:47,011 iteration 3260 : loss : 0.026706, loss_ce: 0.010565
2022-01-08 00:47:48,467 iteration 3261 : loss : 0.035062, loss_ce: 0.012370
2022-01-08 00:47:49,861 iteration 3262 : loss : 0.031151, loss_ce: 0.013724
2022-01-08 00:47:51,267 iteration 3263 : loss : 0.047094, loss_ce: 0.014978
2022-01-08 00:47:52,663 iteration 3264 : loss : 0.034401, loss_ce: 0.012322
 48%|████████████▉              | 192/400 [1:23:16<1:29:42, 25.88s/it]2022-01-08 00:47:53,991 iteration 3265 : loss : 0.022819, loss_ce: 0.008719
2022-01-08 00:47:55,416 iteration 3266 : loss : 0.026205, loss_ce: 0.010937
2022-01-08 00:47:56,772 iteration 3267 : loss : 0.024537, loss_ce: 0.010195
2022-01-08 00:47:58,310 iteration 3268 : loss : 0.044070, loss_ce: 0.018973
2022-01-08 00:47:59,654 iteration 3269 : loss : 0.037807, loss_ce: 0.015365
2022-01-08 00:48:01,000 iteration 3270 : loss : 0.033596, loss_ce: 0.013011
2022-01-08 00:48:02,464 iteration 3271 : loss : 0.059843, loss_ce: 0.019906
2022-01-08 00:48:03,853 iteration 3272 : loss : 0.027416, loss_ce: 0.010221
2022-01-08 00:48:05,212 iteration 3273 : loss : 0.034438, loss_ce: 0.012577
2022-01-08 00:48:06,641 iteration 3274 : loss : 0.034704, loss_ce: 0.011961
2022-01-08 00:48:07,991 iteration 3275 : loss : 0.022760, loss_ce: 0.009428
2022-01-08 00:48:09,386 iteration 3276 : loss : 0.030439, loss_ce: 0.010471
2022-01-08 00:48:10,852 iteration 3277 : loss : 0.039845, loss_ce: 0.016781
2022-01-08 00:48:12,239 iteration 3278 : loss : 0.033813, loss_ce: 0.013136
2022-01-08 00:48:13,684 iteration 3279 : loss : 0.053429, loss_ce: 0.023405
2022-01-08 00:48:15,102 iteration 3280 : loss : 0.043060, loss_ce: 0.018908
2022-01-08 00:48:16,527 iteration 3281 : loss : 0.025317, loss_ce: 0.007382
 48%|█████████████              | 193/400 [1:23:40<1:27:11, 25.27s/it]2022-01-08 00:48:17,932 iteration 3282 : loss : 0.030514, loss_ce: 0.014817
2022-01-08 00:48:19,425 iteration 3283 : loss : 0.048009, loss_ce: 0.020107
2022-01-08 00:48:20,795 iteration 3284 : loss : 0.025600, loss_ce: 0.011979
2022-01-08 00:48:22,120 iteration 3285 : loss : 0.022414, loss_ce: 0.008691
2022-01-08 00:48:23,570 iteration 3286 : loss : 0.038845, loss_ce: 0.011655
2022-01-08 00:48:25,002 iteration 3287 : loss : 0.030679, loss_ce: 0.013063
2022-01-08 00:48:26,404 iteration 3288 : loss : 0.024887, loss_ce: 0.009284
2022-01-08 00:48:27,835 iteration 3289 : loss : 0.042426, loss_ce: 0.012053
2022-01-08 00:48:29,196 iteration 3290 : loss : 0.025628, loss_ce: 0.007366
2022-01-08 00:48:30,641 iteration 3291 : loss : 0.032587, loss_ce: 0.011559
2022-01-08 00:48:32,015 iteration 3292 : loss : 0.026213, loss_ce: 0.010892
2022-01-08 00:48:33,346 iteration 3293 : loss : 0.021591, loss_ce: 0.008100
2022-01-08 00:48:34,709 iteration 3294 : loss : 0.023151, loss_ce: 0.009713
2022-01-08 00:48:36,094 iteration 3295 : loss : 0.028771, loss_ce: 0.011670
2022-01-08 00:48:37,506 iteration 3296 : loss : 0.033415, loss_ce: 0.012784
2022-01-08 00:48:38,875 iteration 3297 : loss : 0.042178, loss_ce: 0.017507
2022-01-08 00:48:40,211 iteration 3298 : loss : 0.028084, loss_ce: 0.010780
 48%|█████████████              | 194/400 [1:24:04<1:25:08, 24.80s/it]2022-01-08 00:48:41,613 iteration 3299 : loss : 0.023123, loss_ce: 0.009030
2022-01-08 00:48:42,982 iteration 3300 : loss : 0.031899, loss_ce: 0.013966
2022-01-08 00:48:44,415 iteration 3301 : loss : 0.036981, loss_ce: 0.010554
2022-01-08 00:48:45,753 iteration 3302 : loss : 0.027849, loss_ce: 0.009248
2022-01-08 00:48:47,215 iteration 3303 : loss : 0.029719, loss_ce: 0.010499
2022-01-08 00:48:48,635 iteration 3304 : loss : 0.034643, loss_ce: 0.019941
2022-01-08 00:48:50,068 iteration 3305 : loss : 0.035839, loss_ce: 0.013437
2022-01-08 00:48:51,431 iteration 3306 : loss : 0.028695, loss_ce: 0.013635
2022-01-08 00:48:52,918 iteration 3307 : loss : 0.036852, loss_ce: 0.016482
2022-01-08 00:48:54,391 iteration 3308 : loss : 0.037545, loss_ce: 0.013564
2022-01-08 00:48:55,808 iteration 3309 : loss : 0.024843, loss_ce: 0.010472
2022-01-08 00:48:57,276 iteration 3310 : loss : 0.036598, loss_ce: 0.010545
2022-01-08 00:48:58,690 iteration 3311 : loss : 0.034111, loss_ce: 0.010188
2022-01-08 00:49:00,035 iteration 3312 : loss : 0.022593, loss_ce: 0.009778
2022-01-08 00:49:01,480 iteration 3313 : loss : 0.056918, loss_ce: 0.023147
2022-01-08 00:49:02,766 iteration 3314 : loss : 0.027885, loss_ce: 0.011983
2022-01-08 00:49:02,766 Training Data Eval:
2022-01-08 00:49:09,833   Average segmentation loss on training set: 0.0243
2022-01-08 00:49:09,833 Validation Data Eval:
2022-01-08 00:49:12,222   Average segmentation loss on validation set: 0.0964
2022-01-08 00:49:13,568 iteration 3315 : loss : 0.037438, loss_ce: 0.014358
 49%|█████████████▏             | 195/400 [1:24:37<1:33:29, 27.36s/it]2022-01-08 00:49:14,989 iteration 3316 : loss : 0.033060, loss_ce: 0.009133
2022-01-08 00:49:16,425 iteration 3317 : loss : 0.037385, loss_ce: 0.015933
2022-01-08 00:49:17,843 iteration 3318 : loss : 0.034891, loss_ce: 0.013853
2022-01-08 00:49:19,187 iteration 3319 : loss : 0.024498, loss_ce: 0.010230
2022-01-08 00:49:20,684 iteration 3320 : loss : 0.030895, loss_ce: 0.010973
2022-01-08 00:49:22,016 iteration 3321 : loss : 0.024404, loss_ce: 0.012041
2022-01-08 00:49:23,334 iteration 3322 : loss : 0.029372, loss_ce: 0.009180
2022-01-08 00:49:24,742 iteration 3323 : loss : 0.047622, loss_ce: 0.019876
2022-01-08 00:49:26,143 iteration 3324 : loss : 0.036144, loss_ce: 0.014196
2022-01-08 00:49:27,600 iteration 3325 : loss : 0.029673, loss_ce: 0.010408
2022-01-08 00:49:29,040 iteration 3326 : loss : 0.023765, loss_ce: 0.009172
2022-01-08 00:49:30,580 iteration 3327 : loss : 0.044446, loss_ce: 0.013685
2022-01-08 00:49:31,948 iteration 3328 : loss : 0.026142, loss_ce: 0.013536
2022-01-08 00:49:33,313 iteration 3329 : loss : 0.028019, loss_ce: 0.009697
2022-01-08 00:49:34,753 iteration 3330 : loss : 0.032347, loss_ce: 0.018255
2022-01-08 00:49:36,071 iteration 3331 : loss : 0.024338, loss_ce: 0.010148
2022-01-08 00:49:37,364 iteration 3332 : loss : 0.021665, loss_ce: 0.007922
 49%|█████████████▏             | 196/400 [1:25:01<1:29:23, 26.29s/it]2022-01-08 00:49:38,789 iteration 3333 : loss : 0.041380, loss_ce: 0.015330
2022-01-08 00:49:40,193 iteration 3334 : loss : 0.023026, loss_ce: 0.009759
2022-01-08 00:49:41,677 iteration 3335 : loss : 0.032592, loss_ce: 0.013286
2022-01-08 00:49:43,025 iteration 3336 : loss : 0.025664, loss_ce: 0.009179
2022-01-08 00:49:44,363 iteration 3337 : loss : 0.027682, loss_ce: 0.010573
2022-01-08 00:49:45,787 iteration 3338 : loss : 0.037701, loss_ce: 0.013367
2022-01-08 00:49:47,254 iteration 3339 : loss : 0.036761, loss_ce: 0.011365
2022-01-08 00:49:48,657 iteration 3340 : loss : 0.029983, loss_ce: 0.012429
2022-01-08 00:49:50,018 iteration 3341 : loss : 0.034247, loss_ce: 0.014743
2022-01-08 00:49:51,373 iteration 3342 : loss : 0.036807, loss_ce: 0.018396
2022-01-08 00:49:52,754 iteration 3343 : loss : 0.033445, loss_ce: 0.014080
2022-01-08 00:49:54,232 iteration 3344 : loss : 0.026481, loss_ce: 0.008203
2022-01-08 00:49:55,686 iteration 3345 : loss : 0.033093, loss_ce: 0.015360
2022-01-08 00:49:57,088 iteration 3346 : loss : 0.031878, loss_ce: 0.010205
2022-01-08 00:49:58,465 iteration 3347 : loss : 0.024943, loss_ce: 0.008829
2022-01-08 00:49:59,934 iteration 3348 : loss : 0.026490, loss_ce: 0.010499
2022-01-08 00:50:01,283 iteration 3349 : loss : 0.041729, loss_ce: 0.010139
 49%|█████████████▎             | 197/400 [1:25:25<1:26:33, 25.58s/it]2022-01-08 00:50:02,674 iteration 3350 : loss : 0.023526, loss_ce: 0.010475
2022-01-08 00:50:04,103 iteration 3351 : loss : 0.021377, loss_ce: 0.007565
2022-01-08 00:50:05,476 iteration 3352 : loss : 0.034189, loss_ce: 0.011225
2022-01-08 00:50:06,891 iteration 3353 : loss : 0.027762, loss_ce: 0.008314
2022-01-08 00:50:08,253 iteration 3354 : loss : 0.024678, loss_ce: 0.007521
2022-01-08 00:50:09,656 iteration 3355 : loss : 0.025091, loss_ce: 0.010231
2022-01-08 00:50:11,024 iteration 3356 : loss : 0.044942, loss_ce: 0.018169
2022-01-08 00:50:12,374 iteration 3357 : loss : 0.018744, loss_ce: 0.007536
2022-01-08 00:50:13,759 iteration 3358 : loss : 0.022325, loss_ce: 0.007303
2022-01-08 00:50:15,132 iteration 3359 : loss : 0.029950, loss_ce: 0.013217
2022-01-08 00:50:16,569 iteration 3360 : loss : 0.025584, loss_ce: 0.008375
2022-01-08 00:50:17,999 iteration 3361 : loss : 0.041015, loss_ce: 0.017790
2022-01-08 00:50:19,358 iteration 3362 : loss : 0.039518, loss_ce: 0.014183
2022-01-08 00:50:20,658 iteration 3363 : loss : 0.031500, loss_ce: 0.010878
2022-01-08 00:50:22,100 iteration 3364 : loss : 0.026965, loss_ce: 0.010297
2022-01-08 00:50:23,504 iteration 3365 : loss : 0.031668, loss_ce: 0.014624
2022-01-08 00:50:24,880 iteration 3366 : loss : 0.026790, loss_ce: 0.011500
 50%|█████████████▎             | 198/400 [1:25:49<1:24:07, 24.99s/it]2022-01-08 00:50:26,415 iteration 3367 : loss : 0.030555, loss_ce: 0.012027
2022-01-08 00:50:27,807 iteration 3368 : loss : 0.027600, loss_ce: 0.013976
2022-01-08 00:50:29,241 iteration 3369 : loss : 0.024900, loss_ce: 0.011839
2022-01-08 00:50:30,584 iteration 3370 : loss : 0.028689, loss_ce: 0.008108
2022-01-08 00:50:31,964 iteration 3371 : loss : 0.023646, loss_ce: 0.008571
2022-01-08 00:50:33,409 iteration 3372 : loss : 0.039696, loss_ce: 0.012186
2022-01-08 00:50:34,834 iteration 3373 : loss : 0.037036, loss_ce: 0.015775
2022-01-08 00:50:36,338 iteration 3374 : loss : 0.030129, loss_ce: 0.014043
2022-01-08 00:50:37,836 iteration 3375 : loss : 0.041571, loss_ce: 0.011766
2022-01-08 00:50:39,160 iteration 3376 : loss : 0.020426, loss_ce: 0.007179
2022-01-08 00:50:40,602 iteration 3377 : loss : 0.044695, loss_ce: 0.020609
2022-01-08 00:50:41,961 iteration 3378 : loss : 0.026063, loss_ce: 0.007990
2022-01-08 00:50:43,385 iteration 3379 : loss : 0.027181, loss_ce: 0.009789
2022-01-08 00:50:44,732 iteration 3380 : loss : 0.027852, loss_ce: 0.008952
2022-01-08 00:50:46,140 iteration 3381 : loss : 0.027634, loss_ce: 0.014726
2022-01-08 00:50:47,595 iteration 3382 : loss : 0.037560, loss_ce: 0.012510
2022-01-08 00:50:48,978 iteration 3383 : loss : 0.031794, loss_ce: 0.014499
 50%|█████████████▍             | 199/400 [1:26:13<1:22:48, 24.72s/it]2022-01-08 00:50:50,413 iteration 3384 : loss : 0.024408, loss_ce: 0.008894
2022-01-08 00:50:51,779 iteration 3385 : loss : 0.032691, loss_ce: 0.012997
2022-01-08 00:50:53,153 iteration 3386 : loss : 0.026382, loss_ce: 0.009037
2022-01-08 00:50:54,505 iteration 3387 : loss : 0.050724, loss_ce: 0.015777
2022-01-08 00:50:55,943 iteration 3388 : loss : 0.040318, loss_ce: 0.017083
2022-01-08 00:50:57,286 iteration 3389 : loss : 0.027887, loss_ce: 0.010090
2022-01-08 00:50:58,689 iteration 3390 : loss : 0.054097, loss_ce: 0.024457
2022-01-08 00:51:00,205 iteration 3391 : loss : 0.029396, loss_ce: 0.014987
2022-01-08 00:51:01,643 iteration 3392 : loss : 0.034262, loss_ce: 0.011950
2022-01-08 00:51:02,978 iteration 3393 : loss : 0.026638, loss_ce: 0.011281
2022-01-08 00:51:04,355 iteration 3394 : loss : 0.029077, loss_ce: 0.009380
2022-01-08 00:51:05,777 iteration 3395 : loss : 0.025563, loss_ce: 0.011445
2022-01-08 00:51:07,175 iteration 3396 : loss : 0.051281, loss_ce: 0.019406
2022-01-08 00:51:08,506 iteration 3397 : loss : 0.024528, loss_ce: 0.009769
2022-01-08 00:51:10,031 iteration 3398 : loss : 0.045891, loss_ce: 0.020835
2022-01-08 00:51:11,472 iteration 3399 : loss : 0.022278, loss_ce: 0.010183
2022-01-08 00:51:11,472 Training Data Eval:
2022-01-08 00:51:18,531   Average segmentation loss on training set: 0.0206
2022-01-08 00:51:18,531 Validation Data Eval:
2022-01-08 00:51:20,921   Average segmentation loss on validation set: 0.0786
2022-01-08 00:51:22,318 iteration 3400 : loss : 0.030369, loss_ce: 0.009142
 50%|█████████████▌             | 200/400 [1:26:46<1:31:00, 27.30s/it]2022-01-08 00:51:23,819 iteration 3401 : loss : 0.034261, loss_ce: 0.013635
2022-01-08 00:51:25,241 iteration 3402 : loss : 0.031002, loss_ce: 0.012403
2022-01-08 00:51:26,725 iteration 3403 : loss : 0.038946, loss_ce: 0.014840
2022-01-08 00:51:28,081 iteration 3404 : loss : 0.029492, loss_ce: 0.009462
2022-01-08 00:51:29,482 iteration 3405 : loss : 0.025687, loss_ce: 0.007213
2022-01-08 00:51:30,809 iteration 3406 : loss : 0.033830, loss_ce: 0.015422
2022-01-08 00:51:32,163 iteration 3407 : loss : 0.026473, loss_ce: 0.009462
2022-01-08 00:51:33,558 iteration 3408 : loss : 0.023099, loss_ce: 0.009018
2022-01-08 00:51:34,946 iteration 3409 : loss : 0.030091, loss_ce: 0.006282
2022-01-08 00:51:36,354 iteration 3410 : loss : 0.040972, loss_ce: 0.014838
2022-01-08 00:51:37,811 iteration 3411 : loss : 0.035204, loss_ce: 0.010277
2022-01-08 00:51:39,266 iteration 3412 : loss : 0.032300, loss_ce: 0.014189
2022-01-08 00:51:40,756 iteration 3413 : loss : 0.033974, loss_ce: 0.018102
2022-01-08 00:51:42,242 iteration 3414 : loss : 0.052923, loss_ce: 0.019900
2022-01-08 00:51:43,554 iteration 3415 : loss : 0.024994, loss_ce: 0.010083
2022-01-08 00:51:44,971 iteration 3416 : loss : 0.033456, loss_ce: 0.016957
2022-01-08 00:51:46,347 iteration 3417 : loss : 0.025837, loss_ce: 0.010871
 50%|█████████████▌             | 201/400 [1:27:10<1:27:18, 26.32s/it]2022-01-08 00:51:47,904 iteration 3418 : loss : 0.045631, loss_ce: 0.012434
2022-01-08 00:51:49,318 iteration 3419 : loss : 0.031777, loss_ce: 0.015721
2022-01-08 00:51:50,659 iteration 3420 : loss : 0.029044, loss_ce: 0.015670
2022-01-08 00:51:52,058 iteration 3421 : loss : 0.039233, loss_ce: 0.011759
2022-01-08 00:51:53,467 iteration 3422 : loss : 0.058475, loss_ce: 0.021779
2022-01-08 00:51:54,919 iteration 3423 : loss : 0.034824, loss_ce: 0.014792
2022-01-08 00:51:56,253 iteration 3424 : loss : 0.028217, loss_ce: 0.009330
2022-01-08 00:51:57,658 iteration 3425 : loss : 0.037719, loss_ce: 0.018115
2022-01-08 00:51:59,032 iteration 3426 : loss : 0.032707, loss_ce: 0.009923
2022-01-08 00:52:00,571 iteration 3427 : loss : 0.059254, loss_ce: 0.025757
2022-01-08 00:52:01,982 iteration 3428 : loss : 0.028244, loss_ce: 0.011450
2022-01-08 00:52:03,398 iteration 3429 : loss : 0.036195, loss_ce: 0.017394
2022-01-08 00:52:04,817 iteration 3430 : loss : 0.025910, loss_ce: 0.008652
2022-01-08 00:52:06,292 iteration 3431 : loss : 0.035554, loss_ce: 0.012394
2022-01-08 00:52:07,651 iteration 3432 : loss : 0.038234, loss_ce: 0.011635
2022-01-08 00:52:09,049 iteration 3433 : loss : 0.030765, loss_ce: 0.009140
2022-01-08 00:52:10,423 iteration 3434 : loss : 0.028085, loss_ce: 0.008376
 50%|█████████████▋             | 202/400 [1:27:34<1:24:38, 25.65s/it]2022-01-08 00:52:11,816 iteration 3435 : loss : 0.026949, loss_ce: 0.013293
2022-01-08 00:52:13,186 iteration 3436 : loss : 0.038338, loss_ce: 0.019216
2022-01-08 00:52:14,533 iteration 3437 : loss : 0.023864, loss_ce: 0.007567
2022-01-08 00:52:15,913 iteration 3438 : loss : 0.037043, loss_ce: 0.017031
2022-01-08 00:52:17,392 iteration 3439 : loss : 0.037269, loss_ce: 0.015672
2022-01-08 00:52:18,869 iteration 3440 : loss : 0.049683, loss_ce: 0.018067
2022-01-08 00:52:20,161 iteration 3441 : loss : 0.025802, loss_ce: 0.012785
2022-01-08 00:52:21,516 iteration 3442 : loss : 0.023135, loss_ce: 0.007731
2022-01-08 00:52:22,943 iteration 3443 : loss : 0.024305, loss_ce: 0.008797
2022-01-08 00:52:24,331 iteration 3444 : loss : 0.030340, loss_ce: 0.010762
2022-01-08 00:52:25,759 iteration 3445 : loss : 0.025553, loss_ce: 0.009538
2022-01-08 00:52:27,148 iteration 3446 : loss : 0.043113, loss_ce: 0.015919
2022-01-08 00:52:28,546 iteration 3447 : loss : 0.031751, loss_ce: 0.015518
2022-01-08 00:52:29,874 iteration 3448 : loss : 0.027140, loss_ce: 0.009512
2022-01-08 00:52:31,284 iteration 3449 : loss : 0.049496, loss_ce: 0.016443
2022-01-08 00:52:32,726 iteration 3450 : loss : 0.028227, loss_ce: 0.009043
2022-01-08 00:52:34,072 iteration 3451 : loss : 0.021094, loss_ce: 0.008879
 51%|█████████████▋             | 203/400 [1:27:58<1:22:14, 25.05s/it]2022-01-08 00:52:35,495 iteration 3452 : loss : 0.030640, loss_ce: 0.010503
2022-01-08 00:52:36,972 iteration 3453 : loss : 0.028561, loss_ce: 0.010229
2022-01-08 00:52:38,428 iteration 3454 : loss : 0.047320, loss_ce: 0.015805
2022-01-08 00:52:39,852 iteration 3455 : loss : 0.033158, loss_ce: 0.013661
2022-01-08 00:52:41,272 iteration 3456 : loss : 0.046395, loss_ce: 0.015558
2022-01-08 00:52:42,726 iteration 3457 : loss : 0.035753, loss_ce: 0.013923
2022-01-08 00:52:44,159 iteration 3458 : loss : 0.022035, loss_ce: 0.011355
2022-01-08 00:52:45,644 iteration 3459 : loss : 0.032979, loss_ce: 0.014489
2022-01-08 00:52:47,085 iteration 3460 : loss : 0.026820, loss_ce: 0.009797
2022-01-08 00:52:48,509 iteration 3461 : loss : 0.040219, loss_ce: 0.016141
2022-01-08 00:52:49,858 iteration 3462 : loss : 0.031879, loss_ce: 0.012510
2022-01-08 00:52:51,174 iteration 3463 : loss : 0.022576, loss_ce: 0.007240
2022-01-08 00:52:52,554 iteration 3464 : loss : 0.031594, loss_ce: 0.012381
2022-01-08 00:52:53,914 iteration 3465 : loss : 0.030796, loss_ce: 0.011823
2022-01-08 00:52:55,361 iteration 3466 : loss : 0.025211, loss_ce: 0.010186
2022-01-08 00:52:56,894 iteration 3467 : loss : 0.046641, loss_ce: 0.021448
2022-01-08 00:52:58,462 iteration 3468 : loss : 0.032510, loss_ce: 0.011492
 51%|█████████████▊             | 204/400 [1:28:22<1:21:10, 24.85s/it]2022-01-08 00:52:59,829 iteration 3469 : loss : 0.025933, loss_ce: 0.009977
2022-01-08 00:53:01,366 iteration 3470 : loss : 0.040534, loss_ce: 0.012879
2022-01-08 00:53:02,815 iteration 3471 : loss : 0.031141, loss_ce: 0.010690
2022-01-08 00:53:04,275 iteration 3472 : loss : 0.042097, loss_ce: 0.015196
2022-01-08 00:53:05,736 iteration 3473 : loss : 0.033473, loss_ce: 0.016319
2022-01-08 00:53:07,106 iteration 3474 : loss : 0.032365, loss_ce: 0.012263
2022-01-08 00:53:08,597 iteration 3475 : loss : 0.055563, loss_ce: 0.020915
2022-01-08 00:53:09,991 iteration 3476 : loss : 0.024620, loss_ce: 0.008993
2022-01-08 00:53:11,437 iteration 3477 : loss : 0.042068, loss_ce: 0.015417
2022-01-08 00:53:12,851 iteration 3478 : loss : 0.032960, loss_ce: 0.010348
2022-01-08 00:53:14,236 iteration 3479 : loss : 0.042045, loss_ce: 0.020737
2022-01-08 00:53:15,608 iteration 3480 : loss : 0.026982, loss_ce: 0.012213
2022-01-08 00:53:17,009 iteration 3481 : loss : 0.028614, loss_ce: 0.015250
2022-01-08 00:53:18,337 iteration 3482 : loss : 0.034847, loss_ce: 0.011923
2022-01-08 00:53:19,790 iteration 3483 : loss : 0.035818, loss_ce: 0.013866
2022-01-08 00:53:21,217 iteration 3484 : loss : 0.043815, loss_ce: 0.014274
2022-01-08 00:53:21,217 Training Data Eval:
2022-01-08 00:53:28,263   Average segmentation loss on training set: 0.0250
2022-01-08 00:53:28,264 Validation Data Eval:
2022-01-08 00:53:30,654   Average segmentation loss on validation set: 0.0960
2022-01-08 00:53:32,049 iteration 3485 : loss : 0.026048, loss_ce: 0.010741
 51%|█████████████▊             | 205/400 [1:28:56<1:29:17, 27.47s/it]2022-01-08 00:53:33,471 iteration 3486 : loss : 0.026826, loss_ce: 0.009374
2022-01-08 00:53:34,867 iteration 3487 : loss : 0.025510, loss_ce: 0.010179
2022-01-08 00:53:36,313 iteration 3488 : loss : 0.037497, loss_ce: 0.023797
2022-01-08 00:53:37,671 iteration 3489 : loss : 0.038836, loss_ce: 0.013410
2022-01-08 00:53:39,173 iteration 3490 : loss : 0.032499, loss_ce: 0.013722
2022-01-08 00:53:40,606 iteration 3491 : loss : 0.030159, loss_ce: 0.013355
2022-01-08 00:53:41,921 iteration 3492 : loss : 0.026070, loss_ce: 0.007362
2022-01-08 00:53:43,399 iteration 3493 : loss : 0.037465, loss_ce: 0.010485
2022-01-08 00:53:44,846 iteration 3494 : loss : 0.029836, loss_ce: 0.016503
2022-01-08 00:53:46,158 iteration 3495 : loss : 0.019693, loss_ce: 0.008171
2022-01-08 00:53:47,576 iteration 3496 : loss : 0.062112, loss_ce: 0.018309
2022-01-08 00:53:49,039 iteration 3497 : loss : 0.033794, loss_ce: 0.017299
2022-01-08 00:53:50,451 iteration 3498 : loss : 0.025002, loss_ce: 0.010910
2022-01-08 00:53:51,935 iteration 3499 : loss : 0.047955, loss_ce: 0.014244
2022-01-08 00:53:53,313 iteration 3500 : loss : 0.039314, loss_ce: 0.012621
2022-01-08 00:53:54,731 iteration 3501 : loss : 0.038071, loss_ce: 0.014801
2022-01-08 00:53:56,141 iteration 3502 : loss : 0.033343, loss_ce: 0.012556
 52%|█████████████▉             | 206/400 [1:29:20<1:25:32, 26.46s/it]2022-01-08 00:53:57,529 iteration 3503 : loss : 0.026476, loss_ce: 0.012894
2022-01-08 00:53:58,937 iteration 3504 : loss : 0.021400, loss_ce: 0.007424
2022-01-08 00:54:00,334 iteration 3505 : loss : 0.035852, loss_ce: 0.018352
2022-01-08 00:54:01,822 iteration 3506 : loss : 0.055011, loss_ce: 0.023838
2022-01-08 00:54:03,216 iteration 3507 : loss : 0.028669, loss_ce: 0.012170
2022-01-08 00:54:04,552 iteration 3508 : loss : 0.025989, loss_ce: 0.007968
2022-01-08 00:54:05,929 iteration 3509 : loss : 0.031179, loss_ce: 0.013285
2022-01-08 00:54:07,423 iteration 3510 : loss : 0.036348, loss_ce: 0.016812
2022-01-08 00:54:08,821 iteration 3511 : loss : 0.032991, loss_ce: 0.015059
2022-01-08 00:54:10,235 iteration 3512 : loss : 0.033630, loss_ce: 0.012143
2022-01-08 00:54:11,568 iteration 3513 : loss : 0.032149, loss_ce: 0.010586
2022-01-08 00:54:12,883 iteration 3514 : loss : 0.021316, loss_ce: 0.010143
2022-01-08 00:54:14,248 iteration 3515 : loss : 0.026998, loss_ce: 0.009740
2022-01-08 00:54:15,694 iteration 3516 : loss : 0.038162, loss_ce: 0.015216
2022-01-08 00:54:17,090 iteration 3517 : loss : 0.052107, loss_ce: 0.022034
2022-01-08 00:54:18,429 iteration 3518 : loss : 0.022191, loss_ce: 0.009028
2022-01-08 00:54:19,781 iteration 3519 : loss : 0.033960, loss_ce: 0.011956
 52%|█████████████▉             | 207/400 [1:29:44<1:22:23, 25.61s/it]2022-01-08 00:54:21,223 iteration 3520 : loss : 0.028205, loss_ce: 0.010567
2022-01-08 00:54:22,646 iteration 3521 : loss : 0.040168, loss_ce: 0.017028
2022-01-08 00:54:24,068 iteration 3522 : loss : 0.031560, loss_ce: 0.013735
2022-01-08 00:54:25,569 iteration 3523 : loss : 0.035895, loss_ce: 0.012979
2022-01-08 00:54:26,970 iteration 3524 : loss : 0.045301, loss_ce: 0.012714
2022-01-08 00:54:28,397 iteration 3525 : loss : 0.055282, loss_ce: 0.023235
2022-01-08 00:54:29,815 iteration 3526 : loss : 0.032310, loss_ce: 0.015206
2022-01-08 00:54:31,235 iteration 3527 : loss : 0.028746, loss_ce: 0.009524
2022-01-08 00:54:32,586 iteration 3528 : loss : 0.022616, loss_ce: 0.010463
2022-01-08 00:54:34,016 iteration 3529 : loss : 0.039479, loss_ce: 0.015043
2022-01-08 00:54:35,400 iteration 3530 : loss : 0.046164, loss_ce: 0.014195
2022-01-08 00:54:36,820 iteration 3531 : loss : 0.048914, loss_ce: 0.019250
2022-01-08 00:54:38,184 iteration 3532 : loss : 0.033416, loss_ce: 0.014897
2022-01-08 00:54:39,549 iteration 3533 : loss : 0.029952, loss_ce: 0.013067
2022-01-08 00:54:40,907 iteration 3534 : loss : 0.032618, loss_ce: 0.017536
2022-01-08 00:54:42,237 iteration 3535 : loss : 0.022264, loss_ce: 0.008306
2022-01-08 00:54:43,574 iteration 3536 : loss : 0.034070, loss_ce: 0.011557
 52%|██████████████             | 208/400 [1:30:07<1:20:12, 25.07s/it]2022-01-08 00:54:44,999 iteration 3537 : loss : 0.028334, loss_ce: 0.011587
2022-01-08 00:54:46,413 iteration 3538 : loss : 0.036424, loss_ce: 0.010559
2022-01-08 00:54:47,842 iteration 3539 : loss : 0.034210, loss_ce: 0.015817
2022-01-08 00:54:49,186 iteration 3540 : loss : 0.045687, loss_ce: 0.009475
2022-01-08 00:54:50,642 iteration 3541 : loss : 0.031758, loss_ce: 0.015466
2022-01-08 00:54:52,012 iteration 3542 : loss : 0.026943, loss_ce: 0.008093
2022-01-08 00:54:53,430 iteration 3543 : loss : 0.033147, loss_ce: 0.017471
2022-01-08 00:54:54,871 iteration 3544 : loss : 0.029081, loss_ce: 0.011155
2022-01-08 00:54:56,234 iteration 3545 : loss : 0.044982, loss_ce: 0.017914
2022-01-08 00:54:57,560 iteration 3546 : loss : 0.032832, loss_ce: 0.016155
2022-01-08 00:54:58,991 iteration 3547 : loss : 0.039424, loss_ce: 0.014565
2022-01-08 00:55:00,424 iteration 3548 : loss : 0.034329, loss_ce: 0.012346
2022-01-08 00:55:01,826 iteration 3549 : loss : 0.046435, loss_ce: 0.023819
2022-01-08 00:55:03,195 iteration 3550 : loss : 0.033270, loss_ce: 0.011781
2022-01-08 00:55:04,656 iteration 3551 : loss : 0.066336, loss_ce: 0.019125
2022-01-08 00:55:06,111 iteration 3552 : loss : 0.066419, loss_ce: 0.032729
2022-01-08 00:55:07,570 iteration 3553 : loss : 0.032387, loss_ce: 0.011292
 52%|██████████████             | 209/400 [1:30:31<1:18:45, 24.74s/it]2022-01-08 00:55:09,006 iteration 3554 : loss : 0.034498, loss_ce: 0.014046
2022-01-08 00:55:10,451 iteration 3555 : loss : 0.038952, loss_ce: 0.018398
2022-01-08 00:55:11,833 iteration 3556 : loss : 0.050455, loss_ce: 0.016340
2022-01-08 00:55:13,202 iteration 3557 : loss : 0.024187, loss_ce: 0.010703
2022-01-08 00:55:14,530 iteration 3558 : loss : 0.025876, loss_ce: 0.009157
2022-01-08 00:55:16,010 iteration 3559 : loss : 0.031572, loss_ce: 0.012487
2022-01-08 00:55:17,391 iteration 3560 : loss : 0.041651, loss_ce: 0.012734
2022-01-08 00:55:18,814 iteration 3561 : loss : 0.029357, loss_ce: 0.013144
2022-01-08 00:55:20,203 iteration 3562 : loss : 0.035355, loss_ce: 0.013529
2022-01-08 00:55:21,629 iteration 3563 : loss : 0.027908, loss_ce: 0.011465
2022-01-08 00:55:23,066 iteration 3564 : loss : 0.033033, loss_ce: 0.011904
2022-01-08 00:55:24,407 iteration 3565 : loss : 0.030309, loss_ce: 0.011727
2022-01-08 00:55:25,692 iteration 3566 : loss : 0.019916, loss_ce: 0.007795
2022-01-08 00:55:27,025 iteration 3567 : loss : 0.026354, loss_ce: 0.008743
2022-01-08 00:55:28,452 iteration 3568 : loss : 0.035886, loss_ce: 0.014185
2022-01-08 00:55:29,801 iteration 3569 : loss : 0.046770, loss_ce: 0.017411
2022-01-08 00:55:29,802 Training Data Eval:
2022-01-08 00:55:36,852   Average segmentation loss on training set: 0.0214
2022-01-08 00:55:36,853 Validation Data Eval:
2022-01-08 00:55:39,249   Average segmentation loss on validation set: 0.0803
2022-01-08 00:55:40,608 iteration 3570 : loss : 0.025351, loss_ce: 0.009081
 52%|██████████████▏            | 210/400 [1:31:04<1:26:14, 27.23s/it]2022-01-08 00:55:42,064 iteration 3571 : loss : 0.052782, loss_ce: 0.023436
2022-01-08 00:55:43,535 iteration 3572 : loss : 0.029873, loss_ce: 0.012368
2022-01-08 00:55:44,980 iteration 3573 : loss : 0.035733, loss_ce: 0.014376
2022-01-08 00:55:46,345 iteration 3574 : loss : 0.021914, loss_ce: 0.010702
2022-01-08 00:55:47,695 iteration 3575 : loss : 0.027182, loss_ce: 0.011284
2022-01-08 00:55:49,060 iteration 3576 : loss : 0.035580, loss_ce: 0.011471
2022-01-08 00:55:50,485 iteration 3577 : loss : 0.024909, loss_ce: 0.010921
2022-01-08 00:55:51,878 iteration 3578 : loss : 0.028050, loss_ce: 0.011682
2022-01-08 00:55:53,283 iteration 3579 : loss : 0.025054, loss_ce: 0.010484
2022-01-08 00:55:54,718 iteration 3580 : loss : 0.028166, loss_ce: 0.013341
2022-01-08 00:55:56,155 iteration 3581 : loss : 0.030913, loss_ce: 0.010627
2022-01-08 00:55:57,596 iteration 3582 : loss : 0.043378, loss_ce: 0.017596
2022-01-08 00:55:58,980 iteration 3583 : loss : 0.059716, loss_ce: 0.016807
2022-01-08 00:56:00,300 iteration 3584 : loss : 0.023383, loss_ce: 0.008567
2022-01-08 00:56:01,664 iteration 3585 : loss : 0.021043, loss_ce: 0.007994
2022-01-08 00:56:03,014 iteration 3586 : loss : 0.022837, loss_ce: 0.006749
2022-01-08 00:56:04,427 iteration 3587 : loss : 0.029571, loss_ce: 0.014194
 53%|██████████████▏            | 211/400 [1:31:28<1:22:33, 26.21s/it]2022-01-08 00:56:05,913 iteration 3588 : loss : 0.028356, loss_ce: 0.012624
2022-01-08 00:56:07,378 iteration 3589 : loss : 0.050890, loss_ce: 0.015246
2022-01-08 00:56:08,794 iteration 3590 : loss : 0.037650, loss_ce: 0.011667
2022-01-08 00:56:10,156 iteration 3591 : loss : 0.029664, loss_ce: 0.010865
2022-01-08 00:56:11,539 iteration 3592 : loss : 0.031985, loss_ce: 0.018135
2022-01-08 00:56:12,962 iteration 3593 : loss : 0.025231, loss_ce: 0.013072
2022-01-08 00:56:14,357 iteration 3594 : loss : 0.026724, loss_ce: 0.008308
2022-01-08 00:56:15,761 iteration 3595 : loss : 0.050613, loss_ce: 0.023766
2022-01-08 00:56:17,117 iteration 3596 : loss : 0.029116, loss_ce: 0.012656
2022-01-08 00:56:18,514 iteration 3597 : loss : 0.025694, loss_ce: 0.010845
2022-01-08 00:56:19,940 iteration 3598 : loss : 0.027047, loss_ce: 0.012531
2022-01-08 00:56:21,387 iteration 3599 : loss : 0.045026, loss_ce: 0.016524
2022-01-08 00:56:22,743 iteration 3600 : loss : 0.027262, loss_ce: 0.015350
2022-01-08 00:56:24,154 iteration 3601 : loss : 0.036034, loss_ce: 0.017087
2022-01-08 00:56:25,583 iteration 3602 : loss : 0.035369, loss_ce: 0.012935
2022-01-08 00:56:26,972 iteration 3603 : loss : 0.032803, loss_ce: 0.016580
2022-01-08 00:56:28,360 iteration 3604 : loss : 0.039222, loss_ce: 0.008767
 53%|██████████████▎            | 212/400 [1:31:52<1:19:59, 25.53s/it]2022-01-08 00:56:29,899 iteration 3605 : loss : 0.053185, loss_ce: 0.019529
2022-01-08 00:56:31,234 iteration 3606 : loss : 0.039021, loss_ce: 0.010918
2022-01-08 00:56:32,535 iteration 3607 : loss : 0.017935, loss_ce: 0.006972
2022-01-08 00:56:33,976 iteration 3608 : loss : 0.036970, loss_ce: 0.012674
2022-01-08 00:56:35,413 iteration 3609 : loss : 0.058582, loss_ce: 0.021726
2022-01-08 00:56:36,744 iteration 3610 : loss : 0.018145, loss_ce: 0.007479
2022-01-08 00:56:38,131 iteration 3611 : loss : 0.036968, loss_ce: 0.014904
2022-01-08 00:56:39,491 iteration 3612 : loss : 0.021137, loss_ce: 0.008858
2022-01-08 00:56:40,884 iteration 3613 : loss : 0.025780, loss_ce: 0.009263
2022-01-08 00:56:42,237 iteration 3614 : loss : 0.034654, loss_ce: 0.016517
2022-01-08 00:56:43,612 iteration 3615 : loss : 0.050381, loss_ce: 0.017033
2022-01-08 00:56:45,004 iteration 3616 : loss : 0.027945, loss_ce: 0.013326
2022-01-08 00:56:46,429 iteration 3617 : loss : 0.029368, loss_ce: 0.014851
2022-01-08 00:56:47,849 iteration 3618 : loss : 0.035507, loss_ce: 0.011425
2022-01-08 00:56:49,221 iteration 3619 : loss : 0.030575, loss_ce: 0.010514
2022-01-08 00:56:50,622 iteration 3620 : loss : 0.026586, loss_ce: 0.008327
2022-01-08 00:56:51,997 iteration 3621 : loss : 0.031488, loss_ce: 0.015109
 53%|██████████████▍            | 213/400 [1:32:16<1:17:47, 24.96s/it]2022-01-08 00:56:53,447 iteration 3622 : loss : 0.030048, loss_ce: 0.009619
2022-01-08 00:56:54,851 iteration 3623 : loss : 0.034272, loss_ce: 0.014663
2022-01-08 00:56:56,261 iteration 3624 : loss : 0.062110, loss_ce: 0.023247
2022-01-08 00:56:57,590 iteration 3625 : loss : 0.023851, loss_ce: 0.008635
2022-01-08 00:56:59,008 iteration 3626 : loss : 0.029027, loss_ce: 0.009492
2022-01-08 00:57:00,433 iteration 3627 : loss : 0.038936, loss_ce: 0.016629
2022-01-08 00:57:01,862 iteration 3628 : loss : 0.041774, loss_ce: 0.014326
2022-01-08 00:57:03,360 iteration 3629 : loss : 0.028936, loss_ce: 0.013663
2022-01-08 00:57:04,794 iteration 3630 : loss : 0.037270, loss_ce: 0.016903
2022-01-08 00:57:06,192 iteration 3631 : loss : 0.026500, loss_ce: 0.009529
2022-01-08 00:57:07,622 iteration 3632 : loss : 0.034812, loss_ce: 0.020016
2022-01-08 00:57:09,079 iteration 3633 : loss : 0.041714, loss_ce: 0.013060
2022-01-08 00:57:10,439 iteration 3634 : loss : 0.033360, loss_ce: 0.011417
2022-01-08 00:57:11,943 iteration 3635 : loss : 0.039595, loss_ce: 0.019280
2022-01-08 00:57:13,263 iteration 3636 : loss : 0.023070, loss_ce: 0.010619
2022-01-08 00:57:14,658 iteration 3637 : loss : 0.028480, loss_ce: 0.010624
2022-01-08 00:57:16,081 iteration 3638 : loss : 0.027627, loss_ce: 0.009289
 54%|██████████████▍            | 214/400 [1:32:40<1:16:33, 24.70s/it]2022-01-08 00:57:17,530 iteration 3639 : loss : 0.040575, loss_ce: 0.015688
2022-01-08 00:57:18,968 iteration 3640 : loss : 0.037989, loss_ce: 0.011837
2022-01-08 00:57:20,402 iteration 3641 : loss : 0.022307, loss_ce: 0.008304
2022-01-08 00:57:21,792 iteration 3642 : loss : 0.028756, loss_ce: 0.011942
2022-01-08 00:57:23,211 iteration 3643 : loss : 0.036106, loss_ce: 0.011872
2022-01-08 00:57:24,635 iteration 3644 : loss : 0.031870, loss_ce: 0.011766
2022-01-08 00:57:26,111 iteration 3645 : loss : 0.055467, loss_ce: 0.013434
2022-01-08 00:57:27,561 iteration 3646 : loss : 0.050202, loss_ce: 0.028443
2022-01-08 00:57:28,956 iteration 3647 : loss : 0.029063, loss_ce: 0.011504
2022-01-08 00:57:30,374 iteration 3648 : loss : 0.036820, loss_ce: 0.013157
2022-01-08 00:57:31,666 iteration 3649 : loss : 0.028082, loss_ce: 0.011622
2022-01-08 00:57:33,072 iteration 3650 : loss : 0.025578, loss_ce: 0.010596
2022-01-08 00:57:34,448 iteration 3651 : loss : 0.021847, loss_ce: 0.008259
2022-01-08 00:57:35,777 iteration 3652 : loss : 0.023545, loss_ce: 0.009981
2022-01-08 00:57:37,218 iteration 3653 : loss : 0.039392, loss_ce: 0.015831
2022-01-08 00:57:38,586 iteration 3654 : loss : 0.021679, loss_ce: 0.008773
2022-01-08 00:57:38,586 Training Data Eval:
2022-01-08 00:57:45,628   Average segmentation loss on training set: 0.0194
2022-01-08 00:57:45,628 Validation Data Eval:
2022-01-08 00:57:48,016   Average segmentation loss on validation set: 0.0643
2022-01-08 00:57:49,540 iteration 3655 : loss : 0.036067, loss_ce: 0.013361
 54%|██████████████▌            | 215/400 [1:33:13<1:24:14, 27.32s/it]2022-01-08 00:57:50,967 iteration 3656 : loss : 0.027490, loss_ce: 0.008583
2022-01-08 00:57:52,438 iteration 3657 : loss : 0.025798, loss_ce: 0.011563
2022-01-08 00:57:53,831 iteration 3658 : loss : 0.035866, loss_ce: 0.015265
2022-01-08 00:57:55,159 iteration 3659 : loss : 0.024710, loss_ce: 0.010216
2022-01-08 00:57:56,622 iteration 3660 : loss : 0.040208, loss_ce: 0.019389
2022-01-08 00:57:58,024 iteration 3661 : loss : 0.030784, loss_ce: 0.012577
2022-01-08 00:57:59,366 iteration 3662 : loss : 0.027962, loss_ce: 0.013267
2022-01-08 00:58:00,749 iteration 3663 : loss : 0.028206, loss_ce: 0.011753
2022-01-08 00:58:02,166 iteration 3664 : loss : 0.035716, loss_ce: 0.011330
2022-01-08 00:58:03,673 iteration 3665 : loss : 0.048115, loss_ce: 0.019605
2022-01-08 00:58:05,149 iteration 3666 : loss : 0.027180, loss_ce: 0.012573
2022-01-08 00:58:06,522 iteration 3667 : loss : 0.025246, loss_ce: 0.009832
2022-01-08 00:58:07,898 iteration 3668 : loss : 0.036242, loss_ce: 0.013507
2022-01-08 00:58:09,317 iteration 3669 : loss : 0.039963, loss_ce: 0.014146
2022-01-08 00:58:10,662 iteration 3670 : loss : 0.026295, loss_ce: 0.012357
2022-01-08 00:58:12,038 iteration 3671 : loss : 0.043609, loss_ce: 0.011560
2022-01-08 00:58:13,409 iteration 3672 : loss : 0.033699, loss_ce: 0.009583
 54%|██████████████▌            | 216/400 [1:33:37<1:20:37, 26.29s/it]2022-01-08 00:58:14,822 iteration 3673 : loss : 0.023404, loss_ce: 0.010376
2022-01-08 00:58:16,320 iteration 3674 : loss : 0.024331, loss_ce: 0.012285
2022-01-08 00:58:17,791 iteration 3675 : loss : 0.034553, loss_ce: 0.013642
2022-01-08 00:58:19,243 iteration 3676 : loss : 0.046253, loss_ce: 0.018262
2022-01-08 00:58:20,669 iteration 3677 : loss : 0.032998, loss_ce: 0.016091
2022-01-08 00:58:22,172 iteration 3678 : loss : 0.038842, loss_ce: 0.013626
2022-01-08 00:58:23,506 iteration 3679 : loss : 0.024303, loss_ce: 0.008120
2022-01-08 00:58:24,850 iteration 3680 : loss : 0.025075, loss_ce: 0.009934
2022-01-08 00:58:26,239 iteration 3681 : loss : 0.035935, loss_ce: 0.010652
2022-01-08 00:58:27,550 iteration 3682 : loss : 0.022214, loss_ce: 0.007246
2022-01-08 00:58:28,958 iteration 3683 : loss : 0.026912, loss_ce: 0.011633
2022-01-08 00:58:30,284 iteration 3684 : loss : 0.023556, loss_ce: 0.007637
2022-01-08 00:58:31,651 iteration 3685 : loss : 0.029723, loss_ce: 0.010534
2022-01-08 00:58:33,106 iteration 3686 : loss : 0.029443, loss_ce: 0.012186
2022-01-08 00:58:34,609 iteration 3687 : loss : 0.056219, loss_ce: 0.025595
2022-01-08 00:58:36,036 iteration 3688 : loss : 0.054171, loss_ce: 0.022776
2022-01-08 00:58:37,459 iteration 3689 : loss : 0.040517, loss_ce: 0.012388
 54%|██████████████▋            | 217/400 [1:34:01<1:18:07, 25.62s/it]2022-01-08 00:58:39,009 iteration 3690 : loss : 0.028870, loss_ce: 0.012763
2022-01-08 00:58:40,406 iteration 3691 : loss : 0.046589, loss_ce: 0.012831
2022-01-08 00:58:41,830 iteration 3692 : loss : 0.045542, loss_ce: 0.015103
2022-01-08 00:58:43,173 iteration 3693 : loss : 0.025349, loss_ce: 0.006469
2022-01-08 00:58:44,590 iteration 3694 : loss : 0.026566, loss_ce: 0.008638
2022-01-08 00:58:45,998 iteration 3695 : loss : 0.026617, loss_ce: 0.008932
2022-01-08 00:58:47,359 iteration 3696 : loss : 0.020435, loss_ce: 0.007868
2022-01-08 00:58:48,705 iteration 3697 : loss : 0.028770, loss_ce: 0.009414
2022-01-08 00:58:50,179 iteration 3698 : loss : 0.028080, loss_ce: 0.009675
2022-01-08 00:58:51,577 iteration 3699 : loss : 0.026347, loss_ce: 0.013241
2022-01-08 00:58:52,993 iteration 3700 : loss : 0.026132, loss_ce: 0.008425
2022-01-08 00:58:54,319 iteration 3701 : loss : 0.037398, loss_ce: 0.019942
2022-01-08 00:58:55,744 iteration 3702 : loss : 0.029916, loss_ce: 0.010689
2022-01-08 00:58:57,142 iteration 3703 : loss : 0.022155, loss_ce: 0.007992
2022-01-08 00:58:58,555 iteration 3704 : loss : 0.027552, loss_ce: 0.009828
2022-01-08 00:58:59,963 iteration 3705 : loss : 0.028231, loss_ce: 0.013511
2022-01-08 00:59:01,297 iteration 3706 : loss : 0.026000, loss_ce: 0.008926
 55%|██████████████▋            | 218/400 [1:34:25<1:16:05, 25.08s/it]2022-01-08 00:59:02,751 iteration 3707 : loss : 0.075712, loss_ce: 0.015785
2022-01-08 00:59:04,276 iteration 3708 : loss : 0.043733, loss_ce: 0.018197
2022-01-08 00:59:05,698 iteration 3709 : loss : 0.026404, loss_ce: 0.009497
2022-01-08 00:59:07,008 iteration 3710 : loss : 0.024343, loss_ce: 0.009036
2022-01-08 00:59:08,506 iteration 3711 : loss : 0.043282, loss_ce: 0.018652
2022-01-08 00:59:09,858 iteration 3712 : loss : 0.040981, loss_ce: 0.014501
2022-01-08 00:59:11,197 iteration 3713 : loss : 0.019114, loss_ce: 0.007579
2022-01-08 00:59:12,621 iteration 3714 : loss : 0.036891, loss_ce: 0.016834
2022-01-08 00:59:13,982 iteration 3715 : loss : 0.028260, loss_ce: 0.010797
2022-01-08 00:59:15,373 iteration 3716 : loss : 0.035524, loss_ce: 0.017698
2022-01-08 00:59:16,805 iteration 3717 : loss : 0.024304, loss_ce: 0.011239
2022-01-08 00:59:18,240 iteration 3718 : loss : 0.031761, loss_ce: 0.012479
2022-01-08 00:59:19,732 iteration 3719 : loss : 0.039694, loss_ce: 0.014469
2022-01-08 00:59:21,209 iteration 3720 : loss : 0.026161, loss_ce: 0.011852
2022-01-08 00:59:22,640 iteration 3721 : loss : 0.035070, loss_ce: 0.016134
2022-01-08 00:59:24,108 iteration 3722 : loss : 0.038774, loss_ce: 0.012494
2022-01-08 00:59:25,484 iteration 3723 : loss : 0.032386, loss_ce: 0.015228
 55%|██████████████▊            | 219/400 [1:34:49<1:14:50, 24.81s/it]2022-01-08 00:59:26,916 iteration 3724 : loss : 0.023860, loss_ce: 0.012068
2022-01-08 00:59:28,364 iteration 3725 : loss : 0.037055, loss_ce: 0.014565
2022-01-08 00:59:29,770 iteration 3726 : loss : 0.024818, loss_ce: 0.010623
2022-01-08 00:59:31,170 iteration 3727 : loss : 0.021718, loss_ce: 0.007517
2022-01-08 00:59:32,481 iteration 3728 : loss : 0.023917, loss_ce: 0.011175
2022-01-08 00:59:33,909 iteration 3729 : loss : 0.038664, loss_ce: 0.011217
2022-01-08 00:59:35,286 iteration 3730 : loss : 0.023771, loss_ce: 0.009018
2022-01-08 00:59:36,731 iteration 3731 : loss : 0.033348, loss_ce: 0.014309
2022-01-08 00:59:38,122 iteration 3732 : loss : 0.026184, loss_ce: 0.012327
2022-01-08 00:59:39,450 iteration 3733 : loss : 0.019700, loss_ce: 0.006044
2022-01-08 00:59:40,900 iteration 3734 : loss : 0.029159, loss_ce: 0.010950
2022-01-08 00:59:42,359 iteration 3735 : loss : 0.030412, loss_ce: 0.009902
2022-01-08 00:59:43,775 iteration 3736 : loss : 0.033680, loss_ce: 0.012510
2022-01-08 00:59:45,228 iteration 3737 : loss : 0.040269, loss_ce: 0.020506
2022-01-08 00:59:46,632 iteration 3738 : loss : 0.037685, loss_ce: 0.012228
2022-01-08 00:59:48,021 iteration 3739 : loss : 0.031310, loss_ce: 0.008901
2022-01-08 00:59:48,021 Training Data Eval:
2022-01-08 00:59:55,047   Average segmentation loss on training set: 0.0185
2022-01-08 00:59:55,048 Validation Data Eval:
2022-01-08 00:59:57,430   Average segmentation loss on validation set: 0.0709
2022-01-08 00:59:58,789 iteration 3740 : loss : 0.031043, loss_ce: 0.009752
 55%|██████████████▊            | 220/400 [1:35:23<1:22:05, 27.36s/it]2022-01-08 01:00:00,243 iteration 3741 : loss : 0.033299, loss_ce: 0.015690
2022-01-08 01:00:01,621 iteration 3742 : loss : 0.026127, loss_ce: 0.010071
2022-01-08 01:00:02,985 iteration 3743 : loss : 0.026454, loss_ce: 0.007884
2022-01-08 01:00:04,401 iteration 3744 : loss : 0.019005, loss_ce: 0.006857
2022-01-08 01:00:05,910 iteration 3745 : loss : 0.028514, loss_ce: 0.014758
2022-01-08 01:00:07,285 iteration 3746 : loss : 0.026798, loss_ce: 0.007436
2022-01-08 01:00:08,717 iteration 3747 : loss : 0.034964, loss_ce: 0.018437
2022-01-08 01:00:10,168 iteration 3748 : loss : 0.046719, loss_ce: 0.014943
2022-01-08 01:00:11,604 iteration 3749 : loss : 0.027632, loss_ce: 0.007792
2022-01-08 01:00:13,101 iteration 3750 : loss : 0.035593, loss_ce: 0.014280
2022-01-08 01:00:14,486 iteration 3751 : loss : 0.031189, loss_ce: 0.013873
2022-01-08 01:00:15,850 iteration 3752 : loss : 0.023132, loss_ce: 0.009363
2022-01-08 01:00:17,248 iteration 3753 : loss : 0.028917, loss_ce: 0.009549
2022-01-08 01:00:18,571 iteration 3754 : loss : 0.021302, loss_ce: 0.010573
2022-01-08 01:00:19,972 iteration 3755 : loss : 0.026092, loss_ce: 0.009243
2022-01-08 01:00:21,333 iteration 3756 : loss : 0.021880, loss_ce: 0.008336
2022-01-08 01:00:22,726 iteration 3757 : loss : 0.035120, loss_ce: 0.013665
 55%|██████████████▉            | 221/400 [1:35:46<1:18:34, 26.34s/it]2022-01-08 01:00:24,232 iteration 3758 : loss : 0.028116, loss_ce: 0.010727
2022-01-08 01:00:25,690 iteration 3759 : loss : 0.037973, loss_ce: 0.015746
2022-01-08 01:00:27,126 iteration 3760 : loss : 0.038960, loss_ce: 0.016443
2022-01-08 01:00:28,531 iteration 3761 : loss : 0.021389, loss_ce: 0.010294
2022-01-08 01:00:29,977 iteration 3762 : loss : 0.031673, loss_ce: 0.013793
2022-01-08 01:00:31,425 iteration 3763 : loss : 0.028752, loss_ce: 0.013094
2022-01-08 01:00:32,788 iteration 3764 : loss : 0.032041, loss_ce: 0.007490
2022-01-08 01:00:34,104 iteration 3765 : loss : 0.032266, loss_ce: 0.014411
2022-01-08 01:00:35,476 iteration 3766 : loss : 0.021798, loss_ce: 0.007209
2022-01-08 01:00:36,871 iteration 3767 : loss : 0.029397, loss_ce: 0.011460
2022-01-08 01:00:38,335 iteration 3768 : loss : 0.031132, loss_ce: 0.012090
2022-01-08 01:00:39,733 iteration 3769 : loss : 0.027655, loss_ce: 0.010432
2022-01-08 01:00:41,213 iteration 3770 : loss : 0.023709, loss_ce: 0.008472
2022-01-08 01:00:42,640 iteration 3771 : loss : 0.030270, loss_ce: 0.011827
2022-01-08 01:00:44,080 iteration 3772 : loss : 0.032491, loss_ce: 0.008645
2022-01-08 01:00:45,484 iteration 3773 : loss : 0.027625, loss_ce: 0.009427
2022-01-08 01:00:46,923 iteration 3774 : loss : 0.042001, loss_ce: 0.021023
 56%|██████████████▉            | 222/400 [1:36:11<1:16:13, 25.69s/it]2022-01-08 01:00:48,411 iteration 3775 : loss : 0.028516, loss_ce: 0.012525
2022-01-08 01:00:49,814 iteration 3776 : loss : 0.021160, loss_ce: 0.007968
2022-01-08 01:00:51,206 iteration 3777 : loss : 0.044745, loss_ce: 0.013632
2022-01-08 01:00:52,658 iteration 3778 : loss : 0.039103, loss_ce: 0.019599
2022-01-08 01:00:54,026 iteration 3779 : loss : 0.028891, loss_ce: 0.010605
2022-01-08 01:00:55,478 iteration 3780 : loss : 0.036285, loss_ce: 0.019167
2022-01-08 01:00:56,913 iteration 3781 : loss : 0.040700, loss_ce: 0.018497
2022-01-08 01:00:58,373 iteration 3782 : loss : 0.030313, loss_ce: 0.012018
2022-01-08 01:00:59,854 iteration 3783 : loss : 0.024799, loss_ce: 0.011565
2022-01-08 01:01:01,199 iteration 3784 : loss : 0.032337, loss_ce: 0.008754
2022-01-08 01:01:02,599 iteration 3785 : loss : 0.049871, loss_ce: 0.014216
2022-01-08 01:01:03,991 iteration 3786 : loss : 0.028258, loss_ce: 0.011456
2022-01-08 01:01:05,348 iteration 3787 : loss : 0.033887, loss_ce: 0.012969
2022-01-08 01:01:06,778 iteration 3788 : loss : 0.029304, loss_ce: 0.010354
2022-01-08 01:01:08,175 iteration 3789 : loss : 0.033642, loss_ce: 0.015367
2022-01-08 01:01:09,538 iteration 3790 : loss : 0.025054, loss_ce: 0.009630
2022-01-08 01:01:10,986 iteration 3791 : loss : 0.028275, loss_ce: 0.010031
 56%|███████████████            | 223/400 [1:36:35<1:14:21, 25.20s/it]2022-01-08 01:01:12,396 iteration 3792 : loss : 0.025685, loss_ce: 0.009542
2022-01-08 01:01:13,712 iteration 3793 : loss : 0.024194, loss_ce: 0.005726
2022-01-08 01:01:15,079 iteration 3794 : loss : 0.024227, loss_ce: 0.010301
2022-01-08 01:01:16,438 iteration 3795 : loss : 0.026068, loss_ce: 0.007611
2022-01-08 01:01:17,929 iteration 3796 : loss : 0.022230, loss_ce: 0.007217
2022-01-08 01:01:19,375 iteration 3797 : loss : 0.023647, loss_ce: 0.007077
2022-01-08 01:01:20,719 iteration 3798 : loss : 0.021636, loss_ce: 0.007709
2022-01-08 01:01:22,193 iteration 3799 : loss : 0.031008, loss_ce: 0.011129
2022-01-08 01:01:23,770 iteration 3800 : loss : 0.041460, loss_ce: 0.019328
2022-01-08 01:01:25,252 iteration 3801 : loss : 0.050788, loss_ce: 0.017538
2022-01-08 01:01:26,708 iteration 3802 : loss : 0.035084, loss_ce: 0.014998
2022-01-08 01:01:28,098 iteration 3803 : loss : 0.024903, loss_ce: 0.009854
2022-01-08 01:01:29,518 iteration 3804 : loss : 0.032549, loss_ce: 0.011978
2022-01-08 01:01:30,920 iteration 3805 : loss : 0.027378, loss_ce: 0.009877
2022-01-08 01:01:32,232 iteration 3806 : loss : 0.024122, loss_ce: 0.011427
2022-01-08 01:01:33,643 iteration 3807 : loss : 0.025476, loss_ce: 0.008843
2022-01-08 01:01:35,019 iteration 3808 : loss : 0.032856, loss_ce: 0.014059
 56%|███████████████            | 224/400 [1:36:59<1:12:54, 24.85s/it]2022-01-08 01:01:36,479 iteration 3809 : loss : 0.023889, loss_ce: 0.008937
2022-01-08 01:01:37,908 iteration 3810 : loss : 0.029765, loss_ce: 0.010492
2022-01-08 01:01:39,274 iteration 3811 : loss : 0.023708, loss_ce: 0.008619
2022-01-08 01:01:40,682 iteration 3812 : loss : 0.036701, loss_ce: 0.013678
2022-01-08 01:01:42,007 iteration 3813 : loss : 0.021487, loss_ce: 0.009633
2022-01-08 01:01:43,479 iteration 3814 : loss : 0.028830, loss_ce: 0.013929
2022-01-08 01:01:44,965 iteration 3815 : loss : 0.025473, loss_ce: 0.009446
2022-01-08 01:01:46,319 iteration 3816 : loss : 0.023107, loss_ce: 0.009059
2022-01-08 01:01:47,617 iteration 3817 : loss : 0.024108, loss_ce: 0.008660
2022-01-08 01:01:48,983 iteration 3818 : loss : 0.028262, loss_ce: 0.007562
2022-01-08 01:01:50,412 iteration 3819 : loss : 0.032139, loss_ce: 0.012849
2022-01-08 01:01:51,733 iteration 3820 : loss : 0.022466, loss_ce: 0.008740
2022-01-08 01:01:53,188 iteration 3821 : loss : 0.033345, loss_ce: 0.017105
2022-01-08 01:01:54,567 iteration 3822 : loss : 0.032270, loss_ce: 0.010270
2022-01-08 01:01:55,921 iteration 3823 : loss : 0.033720, loss_ce: 0.011099
2022-01-08 01:01:57,344 iteration 3824 : loss : 0.034698, loss_ce: 0.014109
2022-01-08 01:01:57,344 Training Data Eval:
2022-01-08 01:02:04,384   Average segmentation loss on training set: 0.0175
2022-01-08 01:02:04,385 Validation Data Eval:
2022-01-08 01:02:06,768   Average segmentation loss on validation set: 0.0727
2022-01-08 01:02:08,072 iteration 3825 : loss : 0.027022, loss_ce: 0.009138
 56%|███████████████▏           | 225/400 [1:37:32<1:19:39, 27.31s/it]2022-01-08 01:02:09,571 iteration 3826 : loss : 0.029215, loss_ce: 0.011762
2022-01-08 01:02:11,030 iteration 3827 : loss : 0.031028, loss_ce: 0.013387
2022-01-08 01:02:12,398 iteration 3828 : loss : 0.020903, loss_ce: 0.010589
2022-01-08 01:02:13,749 iteration 3829 : loss : 0.018336, loss_ce: 0.007198
2022-01-08 01:02:15,219 iteration 3830 : loss : 0.035084, loss_ce: 0.010320
2022-01-08 01:02:16,665 iteration 3831 : loss : 0.040540, loss_ce: 0.017157
2022-01-08 01:02:18,110 iteration 3832 : loss : 0.030579, loss_ce: 0.014875
2022-01-08 01:02:19,526 iteration 3833 : loss : 0.067716, loss_ce: 0.022521
2022-01-08 01:02:20,870 iteration 3834 : loss : 0.024209, loss_ce: 0.007022
2022-01-08 01:02:22,285 iteration 3835 : loss : 0.051434, loss_ce: 0.013326
2022-01-08 01:02:23,628 iteration 3836 : loss : 0.021465, loss_ce: 0.008884
2022-01-08 01:02:24,932 iteration 3837 : loss : 0.023121, loss_ce: 0.009902
2022-01-08 01:02:26,362 iteration 3838 : loss : 0.028850, loss_ce: 0.010952
2022-01-08 01:02:27,708 iteration 3839 : loss : 0.035876, loss_ce: 0.009937
2022-01-08 01:02:29,113 iteration 3840 : loss : 0.023940, loss_ce: 0.010026
2022-01-08 01:02:30,436 iteration 3841 : loss : 0.027180, loss_ce: 0.010406
2022-01-08 01:02:31,859 iteration 3842 : loss : 0.075063, loss_ce: 0.012614
 56%|███████████████▎           | 226/400 [1:37:56<1:16:08, 26.26s/it]2022-01-08 01:02:33,285 iteration 3843 : loss : 0.031532, loss_ce: 0.009423
2022-01-08 01:02:34,730 iteration 3844 : loss : 0.040064, loss_ce: 0.017060
2022-01-08 01:02:36,043 iteration 3845 : loss : 0.027086, loss_ce: 0.010060
2022-01-08 01:02:37,413 iteration 3846 : loss : 0.028806, loss_ce: 0.011474
2022-01-08 01:02:38,906 iteration 3847 : loss : 0.029736, loss_ce: 0.011720
2022-01-08 01:02:40,273 iteration 3848 : loss : 0.028613, loss_ce: 0.010427
2022-01-08 01:02:41,646 iteration 3849 : loss : 0.022442, loss_ce: 0.008368
2022-01-08 01:02:43,128 iteration 3850 : loss : 0.032462, loss_ce: 0.011511
2022-01-08 01:02:44,608 iteration 3851 : loss : 0.061047, loss_ce: 0.020565
2022-01-08 01:02:46,048 iteration 3852 : loss : 0.028275, loss_ce: 0.010353
2022-01-08 01:02:47,437 iteration 3853 : loss : 0.028597, loss_ce: 0.012328
2022-01-08 01:02:48,758 iteration 3854 : loss : 0.020728, loss_ce: 0.008282
2022-01-08 01:02:50,125 iteration 3855 : loss : 0.029914, loss_ce: 0.009384
2022-01-08 01:02:51,616 iteration 3856 : loss : 0.035143, loss_ce: 0.017894
2022-01-08 01:02:52,987 iteration 3857 : loss : 0.051228, loss_ce: 0.014413
2022-01-08 01:02:54,387 iteration 3858 : loss : 0.031613, loss_ce: 0.015561
2022-01-08 01:02:55,731 iteration 3859 : loss : 0.028259, loss_ce: 0.012973
 57%|███████████████▎           | 227/400 [1:38:19<1:13:38, 25.54s/it]2022-01-08 01:02:57,112 iteration 3860 : loss : 0.022973, loss_ce: 0.011012
2022-01-08 01:02:58,469 iteration 3861 : loss : 0.023673, loss_ce: 0.010754
2022-01-08 01:02:59,819 iteration 3862 : loss : 0.024885, loss_ce: 0.008009
2022-01-08 01:03:01,222 iteration 3863 : loss : 0.030678, loss_ce: 0.016391
2022-01-08 01:03:02,533 iteration 3864 : loss : 0.024220, loss_ce: 0.010024
2022-01-08 01:03:03,943 iteration 3865 : loss : 0.027557, loss_ce: 0.012444
2022-01-08 01:03:05,321 iteration 3866 : loss : 0.026964, loss_ce: 0.012441
2022-01-08 01:03:06,778 iteration 3867 : loss : 0.040653, loss_ce: 0.015438
2022-01-08 01:03:08,097 iteration 3868 : loss : 0.030043, loss_ce: 0.013090
2022-01-08 01:03:09,556 iteration 3869 : loss : 0.038953, loss_ce: 0.011469
2022-01-08 01:03:10,920 iteration 3870 : loss : 0.036521, loss_ce: 0.012956
2022-01-08 01:03:12,381 iteration 3871 : loss : 0.032212, loss_ce: 0.012242
2022-01-08 01:03:13,764 iteration 3872 : loss : 0.041211, loss_ce: 0.013684
2022-01-08 01:03:15,130 iteration 3873 : loss : 0.021911, loss_ce: 0.006752
2022-01-08 01:03:16,570 iteration 3874 : loss : 0.024845, loss_ce: 0.007992
2022-01-08 01:03:18,015 iteration 3875 : loss : 0.034580, loss_ce: 0.010105
2022-01-08 01:03:19,499 iteration 3876 : loss : 0.033889, loss_ce: 0.012647
 57%|███████████████▍           | 228/400 [1:38:43<1:11:41, 25.01s/it]2022-01-08 01:03:20,892 iteration 3877 : loss : 0.027031, loss_ce: 0.012338
2022-01-08 01:03:22,315 iteration 3878 : loss : 0.033746, loss_ce: 0.011725
2022-01-08 01:03:23,784 iteration 3879 : loss : 0.024380, loss_ce: 0.013217
2022-01-08 01:03:25,202 iteration 3880 : loss : 0.024336, loss_ce: 0.008190
2022-01-08 01:03:26,613 iteration 3881 : loss : 0.036279, loss_ce: 0.010556
2022-01-08 01:03:28,054 iteration 3882 : loss : 0.026046, loss_ce: 0.012802
2022-01-08 01:03:29,455 iteration 3883 : loss : 0.025112, loss_ce: 0.009277
2022-01-08 01:03:30,882 iteration 3884 : loss : 0.024045, loss_ce: 0.009010
2022-01-08 01:03:32,235 iteration 3885 : loss : 0.026933, loss_ce: 0.008886
2022-01-08 01:03:33,667 iteration 3886 : loss : 0.029086, loss_ce: 0.012802
2022-01-08 01:03:35,079 iteration 3887 : loss : 0.027942, loss_ce: 0.009874
2022-01-08 01:03:36,569 iteration 3888 : loss : 0.034606, loss_ce: 0.020546
2022-01-08 01:03:37,932 iteration 3889 : loss : 0.037638, loss_ce: 0.012361
2022-01-08 01:03:39,307 iteration 3890 : loss : 0.027219, loss_ce: 0.009308
2022-01-08 01:03:40,739 iteration 3891 : loss : 0.030158, loss_ce: 0.014360
2022-01-08 01:03:42,159 iteration 3892 : loss : 0.033201, loss_ce: 0.012032
2022-01-08 01:03:43,538 iteration 3893 : loss : 0.042908, loss_ce: 0.012693
 57%|███████████████▍           | 229/400 [1:39:07<1:10:26, 24.72s/it]2022-01-08 01:03:44,969 iteration 3894 : loss : 0.040738, loss_ce: 0.013223
2022-01-08 01:03:46,399 iteration 3895 : loss : 0.027794, loss_ce: 0.011892
2022-01-08 01:03:47,733 iteration 3896 : loss : 0.024112, loss_ce: 0.010445
2022-01-08 01:03:49,204 iteration 3897 : loss : 0.027779, loss_ce: 0.008037
2022-01-08 01:03:50,625 iteration 3898 : loss : 0.030064, loss_ce: 0.008749
2022-01-08 01:03:52,031 iteration 3899 : loss : 0.022640, loss_ce: 0.007755
2022-01-08 01:03:53,448 iteration 3900 : loss : 0.024228, loss_ce: 0.010862
2022-01-08 01:03:54,767 iteration 3901 : loss : 0.023784, loss_ce: 0.011193
2022-01-08 01:03:56,159 iteration 3902 : loss : 0.025616, loss_ce: 0.008598
2022-01-08 01:03:57,500 iteration 3903 : loss : 0.032281, loss_ce: 0.011036
2022-01-08 01:03:58,904 iteration 3904 : loss : 0.022400, loss_ce: 0.009179
2022-01-08 01:04:00,322 iteration 3905 : loss : 0.027870, loss_ce: 0.017275
2022-01-08 01:04:01,719 iteration 3906 : loss : 0.021435, loss_ce: 0.007846
2022-01-08 01:04:03,104 iteration 3907 : loss : 0.032299, loss_ce: 0.009462
2022-01-08 01:04:04,407 iteration 3908 : loss : 0.028426, loss_ce: 0.008621
2022-01-08 01:04:05,722 iteration 3909 : loss : 0.027395, loss_ce: 0.011282
2022-01-08 01:04:05,722 Training Data Eval:
2022-01-08 01:04:12,774   Average segmentation loss on training set: 0.0221
2022-01-08 01:04:12,775 Validation Data Eval:
2022-01-08 01:04:15,163   Average segmentation loss on validation set: 0.0717
2022-01-08 01:04:16,547 iteration 3910 : loss : 0.027763, loss_ce: 0.011594
 57%|███████████████▌           | 230/400 [1:39:40<1:17:04, 27.20s/it]2022-01-08 01:04:17,891 iteration 3911 : loss : 0.022401, loss_ce: 0.010563
2022-01-08 01:04:19,272 iteration 3912 : loss : 0.030038, loss_ce: 0.010803
2022-01-08 01:04:20,670 iteration 3913 : loss : 0.028614, loss_ce: 0.013479
2022-01-08 01:04:21,964 iteration 3914 : loss : 0.018073, loss_ce: 0.008243
2022-01-08 01:04:23,372 iteration 3915 : loss : 0.045849, loss_ce: 0.016939
2022-01-08 01:04:24,796 iteration 3916 : loss : 0.034220, loss_ce: 0.013700
2022-01-08 01:04:26,218 iteration 3917 : loss : 0.027907, loss_ce: 0.010353
2022-01-08 01:04:27,600 iteration 3918 : loss : 0.025328, loss_ce: 0.008002
2022-01-08 01:04:28,983 iteration 3919 : loss : 0.029124, loss_ce: 0.010775
2022-01-08 01:04:30,429 iteration 3920 : loss : 0.044365, loss_ce: 0.006841
2022-01-08 01:04:31,820 iteration 3921 : loss : 0.023919, loss_ce: 0.007672
2022-01-08 01:04:33,237 iteration 3922 : loss : 0.038978, loss_ce: 0.009329
2022-01-08 01:04:34,718 iteration 3923 : loss : 0.052031, loss_ce: 0.022999
2022-01-08 01:04:36,084 iteration 3924 : loss : 0.041605, loss_ce: 0.016994
2022-01-08 01:04:37,397 iteration 3925 : loss : 0.035325, loss_ce: 0.016369
2022-01-08 01:04:38,919 iteration 3926 : loss : 0.032170, loss_ce: 0.014410
2022-01-08 01:04:40,334 iteration 3927 : loss : 0.033602, loss_ce: 0.015956
 58%|███████████████▌           | 231/400 [1:40:04<1:13:44, 26.18s/it]2022-01-08 01:04:41,748 iteration 3928 : loss : 0.027231, loss_ce: 0.009628
2022-01-08 01:04:43,114 iteration 3929 : loss : 0.027384, loss_ce: 0.015847
2022-01-08 01:04:44,498 iteration 3930 : loss : 0.031988, loss_ce: 0.010188
2022-01-08 01:04:45,812 iteration 3931 : loss : 0.030463, loss_ce: 0.008164
2022-01-08 01:04:47,208 iteration 3932 : loss : 0.038844, loss_ce: 0.015077
2022-01-08 01:04:48,597 iteration 3933 : loss : 0.023677, loss_ce: 0.011516
2022-01-08 01:04:50,008 iteration 3934 : loss : 0.034137, loss_ce: 0.012973
2022-01-08 01:04:51,385 iteration 3935 : loss : 0.030752, loss_ce: 0.010883
2022-01-08 01:04:52,726 iteration 3936 : loss : 0.027291, loss_ce: 0.011025
2022-01-08 01:04:54,111 iteration 3937 : loss : 0.031334, loss_ce: 0.009577
2022-01-08 01:04:55,520 iteration 3938 : loss : 0.033226, loss_ce: 0.009034
2022-01-08 01:04:56,966 iteration 3939 : loss : 0.034590, loss_ce: 0.010544
2022-01-08 01:04:58,358 iteration 3940 : loss : 0.030585, loss_ce: 0.013594
2022-01-08 01:04:59,746 iteration 3941 : loss : 0.019966, loss_ce: 0.006216
2022-01-08 01:05:01,121 iteration 3942 : loss : 0.023128, loss_ce: 0.011800
2022-01-08 01:05:02,470 iteration 3943 : loss : 0.036100, loss_ce: 0.016058
2022-01-08 01:05:03,833 iteration 3944 : loss : 0.041080, loss_ce: 0.011974
 58%|███████████████▋           | 232/400 [1:40:28<1:11:03, 25.38s/it]2022-01-08 01:05:05,309 iteration 3945 : loss : 0.030962, loss_ce: 0.009706
2022-01-08 01:05:06,698 iteration 3946 : loss : 0.027841, loss_ce: 0.011085
2022-01-08 01:05:08,063 iteration 3947 : loss : 0.027565, loss_ce: 0.010877
2022-01-08 01:05:09,490 iteration 3948 : loss : 0.041504, loss_ce: 0.018685
2022-01-08 01:05:10,937 iteration 3949 : loss : 0.032121, loss_ce: 0.010262
2022-01-08 01:05:12,316 iteration 3950 : loss : 0.024124, loss_ce: 0.012388
2022-01-08 01:05:13,645 iteration 3951 : loss : 0.030089, loss_ce: 0.007010
2022-01-08 01:05:15,128 iteration 3952 : loss : 0.027540, loss_ce: 0.013179
2022-01-08 01:05:16,519 iteration 3953 : loss : 0.022160, loss_ce: 0.009290
2022-01-08 01:05:17,861 iteration 3954 : loss : 0.020079, loss_ce: 0.007023
2022-01-08 01:05:19,306 iteration 3955 : loss : 0.026239, loss_ce: 0.012013
2022-01-08 01:05:20,767 iteration 3956 : loss : 0.044742, loss_ce: 0.024415
2022-01-08 01:05:22,209 iteration 3957 : loss : 0.050045, loss_ce: 0.014224
2022-01-08 01:05:23,657 iteration 3958 : loss : 0.027055, loss_ce: 0.013605
2022-01-08 01:05:24,975 iteration 3959 : loss : 0.021805, loss_ce: 0.005932
2022-01-08 01:05:26,398 iteration 3960 : loss : 0.032707, loss_ce: 0.012309
2022-01-08 01:05:27,758 iteration 3961 : loss : 0.024306, loss_ce: 0.007197
 58%|███████████████▋           | 233/400 [1:40:51<1:09:24, 24.94s/it]2022-01-08 01:05:29,198 iteration 3962 : loss : 0.029390, loss_ce: 0.012141
2022-01-08 01:05:30,676 iteration 3963 : loss : 0.037403, loss_ce: 0.015661
2022-01-08 01:05:32,027 iteration 3964 : loss : 0.021918, loss_ce: 0.006666
2022-01-08 01:05:33,433 iteration 3965 : loss : 0.019091, loss_ce: 0.008310
2022-01-08 01:05:34,911 iteration 3966 : loss : 0.030149, loss_ce: 0.014463
2022-01-08 01:05:36,321 iteration 3967 : loss : 0.031622, loss_ce: 0.007625
2022-01-08 01:05:37,723 iteration 3968 : loss : 0.023825, loss_ce: 0.008689
2022-01-08 01:05:39,075 iteration 3969 : loss : 0.022730, loss_ce: 0.007402
2022-01-08 01:05:40,466 iteration 3970 : loss : 0.039053, loss_ce: 0.012347
2022-01-08 01:05:41,848 iteration 3971 : loss : 0.021256, loss_ce: 0.006580
2022-01-08 01:05:43,263 iteration 3972 : loss : 0.033054, loss_ce: 0.012865
2022-01-08 01:05:44,718 iteration 3973 : loss : 0.034572, loss_ce: 0.013941
2022-01-08 01:05:46,169 iteration 3974 : loss : 0.022710, loss_ce: 0.010551
2022-01-08 01:05:47,555 iteration 3975 : loss : 0.027666, loss_ce: 0.007895
2022-01-08 01:05:48,956 iteration 3976 : loss : 0.022580, loss_ce: 0.007924
2022-01-08 01:05:50,375 iteration 3977 : loss : 0.024185, loss_ce: 0.010864
2022-01-08 01:05:51,776 iteration 3978 : loss : 0.027937, loss_ce: 0.011948
 58%|███████████████▊           | 234/400 [1:41:15<1:08:14, 24.66s/it]2022-01-08 01:05:53,278 iteration 3979 : loss : 0.037960, loss_ce: 0.019222
2022-01-08 01:05:54,605 iteration 3980 : loss : 0.022485, loss_ce: 0.009151
2022-01-08 01:05:55,996 iteration 3981 : loss : 0.036706, loss_ce: 0.010643
2022-01-08 01:05:57,397 iteration 3982 : loss : 0.033532, loss_ce: 0.015081
2022-01-08 01:05:58,804 iteration 3983 : loss : 0.043394, loss_ce: 0.011794
2022-01-08 01:06:00,229 iteration 3984 : loss : 0.023788, loss_ce: 0.010435
2022-01-08 01:06:01,663 iteration 3985 : loss : 0.025705, loss_ce: 0.008906
2022-01-08 01:06:03,110 iteration 3986 : loss : 0.042129, loss_ce: 0.016293
2022-01-08 01:06:04,461 iteration 3987 : loss : 0.034388, loss_ce: 0.014863
2022-01-08 01:06:05,841 iteration 3988 : loss : 0.028545, loss_ce: 0.009189
2022-01-08 01:06:07,276 iteration 3989 : loss : 0.025644, loss_ce: 0.009702
2022-01-08 01:06:08,683 iteration 3990 : loss : 0.032031, loss_ce: 0.015868
2022-01-08 01:06:10,110 iteration 3991 : loss : 0.035020, loss_ce: 0.016928
2022-01-08 01:06:11,537 iteration 3992 : loss : 0.025128, loss_ce: 0.009341
2022-01-08 01:06:12,961 iteration 3993 : loss : 0.025938, loss_ce: 0.009882
2022-01-08 01:06:14,398 iteration 3994 : loss : 0.019054, loss_ce: 0.006537
2022-01-08 01:06:14,398 Training Data Eval:
2022-01-08 01:06:21,448   Average segmentation loss on training set: 0.0166
2022-01-08 01:06:21,449 Validation Data Eval:
2022-01-08 01:06:23,839   Average segmentation loss on validation set: 0.0911
2022-01-08 01:06:25,371 iteration 3995 : loss : 0.029951, loss_ce: 0.010584
 59%|███████████████▊           | 235/400 [1:41:49<1:15:11, 27.34s/it]2022-01-08 01:06:26,799 iteration 3996 : loss : 0.031687, loss_ce: 0.014463
2022-01-08 01:06:28,140 iteration 3997 : loss : 0.035636, loss_ce: 0.014228
2022-01-08 01:06:29,464 iteration 3998 : loss : 0.028904, loss_ce: 0.008375
2022-01-08 01:06:30,935 iteration 3999 : loss : 0.032332, loss_ce: 0.009515
2022-01-08 01:06:32,333 iteration 4000 : loss : 0.019350, loss_ce: 0.007721
2022-01-08 01:06:33,759 iteration 4001 : loss : 0.022430, loss_ce: 0.011628
2022-01-08 01:06:35,067 iteration 4002 : loss : 0.023039, loss_ce: 0.008591
2022-01-08 01:06:36,507 iteration 4003 : loss : 0.038178, loss_ce: 0.012419
2022-01-08 01:06:37,949 iteration 4004 : loss : 0.034568, loss_ce: 0.012601
2022-01-08 01:06:39,361 iteration 4005 : loss : 0.022133, loss_ce: 0.008679
2022-01-08 01:06:40,699 iteration 4006 : loss : 0.040037, loss_ce: 0.013380
2022-01-08 01:06:42,085 iteration 4007 : loss : 0.025060, loss_ce: 0.008161
2022-01-08 01:06:43,475 iteration 4008 : loss : 0.022248, loss_ce: 0.010166
2022-01-08 01:06:44,864 iteration 4009 : loss : 0.021575, loss_ce: 0.007679
2022-01-08 01:06:46,263 iteration 4010 : loss : 0.022637, loss_ce: 0.009132
2022-01-08 01:06:47,634 iteration 4011 : loss : 0.024992, loss_ce: 0.008442
2022-01-08 01:06:49,051 iteration 4012 : loss : 0.024097, loss_ce: 0.008862
 59%|███████████████▉           | 236/400 [1:42:13<1:11:44, 26.24s/it]2022-01-08 01:06:50,382 iteration 4013 : loss : 0.019198, loss_ce: 0.008976
2022-01-08 01:06:51,821 iteration 4014 : loss : 0.027012, loss_ce: 0.012113
2022-01-08 01:06:53,154 iteration 4015 : loss : 0.023367, loss_ce: 0.011504
2022-01-08 01:06:54,601 iteration 4016 : loss : 0.022285, loss_ce: 0.008603
2022-01-08 01:06:56,036 iteration 4017 : loss : 0.027870, loss_ce: 0.007610
2022-01-08 01:06:57,353 iteration 4018 : loss : 0.024722, loss_ce: 0.008921
2022-01-08 01:06:58,752 iteration 4019 : loss : 0.033692, loss_ce: 0.010617
2022-01-08 01:07:00,117 iteration 4020 : loss : 0.023136, loss_ce: 0.008586
2022-01-08 01:07:01,553 iteration 4021 : loss : 0.029412, loss_ce: 0.008954
2022-01-08 01:07:03,028 iteration 4022 : loss : 0.030381, loss_ce: 0.010785
2022-01-08 01:07:04,356 iteration 4023 : loss : 0.018655, loss_ce: 0.008725
2022-01-08 01:07:05,790 iteration 4024 : loss : 0.026175, loss_ce: 0.006690
2022-01-08 01:07:07,214 iteration 4025 : loss : 0.029392, loss_ce: 0.010746
2022-01-08 01:07:08,708 iteration 4026 : loss : 0.025236, loss_ce: 0.007443
2022-01-08 01:07:10,019 iteration 4027 : loss : 0.019886, loss_ce: 0.006089
2022-01-08 01:07:11,530 iteration 4028 : loss : 0.025558, loss_ce: 0.010623
2022-01-08 01:07:13,047 iteration 4029 : loss : 0.053968, loss_ce: 0.026087
 59%|███████████████▉           | 237/400 [1:42:37<1:09:28, 25.57s/it]2022-01-08 01:07:14,468 iteration 4030 : loss : 0.022624, loss_ce: 0.008281
2022-01-08 01:07:15,851 iteration 4031 : loss : 0.028720, loss_ce: 0.013860
2022-01-08 01:07:17,171 iteration 4032 : loss : 0.018134, loss_ce: 0.005471
2022-01-08 01:07:18,517 iteration 4033 : loss : 0.030504, loss_ce: 0.011987
2022-01-08 01:07:19,970 iteration 4034 : loss : 0.025957, loss_ce: 0.011526
2022-01-08 01:07:21,399 iteration 4035 : loss : 0.031124, loss_ce: 0.009792
2022-01-08 01:07:22,807 iteration 4036 : loss : 0.022872, loss_ce: 0.010190
2022-01-08 01:07:24,154 iteration 4037 : loss : 0.022922, loss_ce: 0.011436
2022-01-08 01:07:25,558 iteration 4038 : loss : 0.030296, loss_ce: 0.012682
2022-01-08 01:07:26,962 iteration 4039 : loss : 0.036634, loss_ce: 0.012562
2022-01-08 01:07:28,337 iteration 4040 : loss : 0.025469, loss_ce: 0.008688
2022-01-08 01:07:29,776 iteration 4041 : loss : 0.036524, loss_ce: 0.014195
2022-01-08 01:07:31,214 iteration 4042 : loss : 0.032448, loss_ce: 0.011208
2022-01-08 01:07:32,636 iteration 4043 : loss : 0.024286, loss_ce: 0.009614
2022-01-08 01:07:34,023 iteration 4044 : loss : 0.024116, loss_ce: 0.009206
2022-01-08 01:07:35,427 iteration 4045 : loss : 0.027807, loss_ce: 0.008331
2022-01-08 01:07:36,869 iteration 4046 : loss : 0.034728, loss_ce: 0.011399
 60%|████████████████           | 238/400 [1:43:01<1:07:37, 25.04s/it]2022-01-08 01:07:38,352 iteration 4047 : loss : 0.026608, loss_ce: 0.012631
2022-01-08 01:07:39,737 iteration 4048 : loss : 0.024893, loss_ce: 0.007370
2022-01-08 01:07:41,124 iteration 4049 : loss : 0.031567, loss_ce: 0.008747
2022-01-08 01:07:42,458 iteration 4050 : loss : 0.055853, loss_ce: 0.023438
2022-01-08 01:07:43,866 iteration 4051 : loss : 0.021158, loss_ce: 0.008239
2022-01-08 01:07:45,202 iteration 4052 : loss : 0.021419, loss_ce: 0.008707
2022-01-08 01:07:46,609 iteration 4053 : loss : 0.055458, loss_ce: 0.013537
2022-01-08 01:07:48,039 iteration 4054 : loss : 0.022715, loss_ce: 0.009154
2022-01-08 01:07:49,467 iteration 4055 : loss : 0.025988, loss_ce: 0.010736
2022-01-08 01:07:50,876 iteration 4056 : loss : 0.029285, loss_ce: 0.013988
2022-01-08 01:07:52,203 iteration 4057 : loss : 0.046680, loss_ce: 0.014238
2022-01-08 01:07:53,551 iteration 4058 : loss : 0.018888, loss_ce: 0.006680
2022-01-08 01:07:54,976 iteration 4059 : loss : 0.029148, loss_ce: 0.009647
2022-01-08 01:07:56,350 iteration 4060 : loss : 0.027040, loss_ce: 0.011618
2022-01-08 01:07:57,647 iteration 4061 : loss : 0.022234, loss_ce: 0.009725
2022-01-08 01:07:59,004 iteration 4062 : loss : 0.021672, loss_ce: 0.010057
2022-01-08 01:08:00,434 iteration 4063 : loss : 0.025966, loss_ce: 0.009360
 60%|████████████████▏          | 239/400 [1:43:24<1:06:00, 24.60s/it]2022-01-08 01:08:01,876 iteration 4064 : loss : 0.022568, loss_ce: 0.008592
2022-01-08 01:08:03,224 iteration 4065 : loss : 0.027195, loss_ce: 0.007680
2022-01-08 01:08:04,610 iteration 4066 : loss : 0.030848, loss_ce: 0.013037
2022-01-08 01:08:05,987 iteration 4067 : loss : 0.026838, loss_ce: 0.008969
2022-01-08 01:08:07,420 iteration 4068 : loss : 0.038645, loss_ce: 0.016500
2022-01-08 01:08:08,862 iteration 4069 : loss : 0.056581, loss_ce: 0.029013
2022-01-08 01:08:10,206 iteration 4070 : loss : 0.026322, loss_ce: 0.007486
2022-01-08 01:08:11,595 iteration 4071 : loss : 0.029144, loss_ce: 0.009703
2022-01-08 01:08:12,991 iteration 4072 : loss : 0.027829, loss_ce: 0.012192
2022-01-08 01:08:14,434 iteration 4073 : loss : 0.024636, loss_ce: 0.009257
2022-01-08 01:08:15,829 iteration 4074 : loss : 0.024902, loss_ce: 0.009924
2022-01-08 01:08:17,310 iteration 4075 : loss : 0.043035, loss_ce: 0.013692
2022-01-08 01:08:18,727 iteration 4076 : loss : 0.025706, loss_ce: 0.012332
2022-01-08 01:08:20,208 iteration 4077 : loss : 0.028139, loss_ce: 0.011137
2022-01-08 01:08:21,664 iteration 4078 : loss : 0.026555, loss_ce: 0.010123
2022-01-08 01:08:23,026 iteration 4079 : loss : 0.021754, loss_ce: 0.007703
2022-01-08 01:08:23,026 Training Data Eval:
2022-01-08 01:08:30,079   Average segmentation loss on training set: 0.0178
2022-01-08 01:08:30,079 Validation Data Eval:
2022-01-08 01:08:32,472   Average segmentation loss on validation set: 0.0749
2022-01-08 01:08:33,936 iteration 4080 : loss : 0.032936, loss_ce: 0.013140
 60%|████████████████▏          | 240/400 [1:43:58<1:12:43, 27.27s/it]2022-01-08 01:08:35,335 iteration 4081 : loss : 0.021462, loss_ce: 0.008689
2022-01-08 01:08:36,717 iteration 4082 : loss : 0.024319, loss_ce: 0.009513
2022-01-08 01:08:38,120 iteration 4083 : loss : 0.020132, loss_ce: 0.008866
2022-01-08 01:08:39,566 iteration 4084 : loss : 0.033021, loss_ce: 0.010974
2022-01-08 01:08:41,034 iteration 4085 : loss : 0.030938, loss_ce: 0.010794
2022-01-08 01:08:42,484 iteration 4086 : loss : 0.024804, loss_ce: 0.011925
2022-01-08 01:08:43,881 iteration 4087 : loss : 0.024056, loss_ce: 0.006997
2022-01-08 01:08:45,264 iteration 4088 : loss : 0.028979, loss_ce: 0.011180
2022-01-08 01:08:46,681 iteration 4089 : loss : 0.028868, loss_ce: 0.011496
2022-01-08 01:08:48,103 iteration 4090 : loss : 0.017690, loss_ce: 0.007086
2022-01-08 01:08:49,470 iteration 4091 : loss : 0.031372, loss_ce: 0.009588
2022-01-08 01:08:50,856 iteration 4092 : loss : 0.021977, loss_ce: 0.008601
2022-01-08 01:08:52,235 iteration 4093 : loss : 0.020310, loss_ce: 0.008634
2022-01-08 01:08:53,585 iteration 4094 : loss : 0.019715, loss_ce: 0.006452
2022-01-08 01:08:54,958 iteration 4095 : loss : 0.019927, loss_ce: 0.007138
2022-01-08 01:08:56,362 iteration 4096 : loss : 0.022852, loss_ce: 0.009208
2022-01-08 01:08:57,760 iteration 4097 : loss : 0.028248, loss_ce: 0.012840
 60%|████████████████▎          | 241/400 [1:44:21<1:09:31, 26.23s/it]2022-01-08 01:08:59,226 iteration 4098 : loss : 0.026834, loss_ce: 0.010547
2022-01-08 01:09:00,593 iteration 4099 : loss : 0.041847, loss_ce: 0.012808
2022-01-08 01:09:01,946 iteration 4100 : loss : 0.026472, loss_ce: 0.011535
2022-01-08 01:09:03,301 iteration 4101 : loss : 0.019062, loss_ce: 0.007011
2022-01-08 01:09:04,696 iteration 4102 : loss : 0.030305, loss_ce: 0.011434
2022-01-08 01:09:06,055 iteration 4103 : loss : 0.019950, loss_ce: 0.008215
2022-01-08 01:09:07,390 iteration 4104 : loss : 0.022932, loss_ce: 0.009557
2022-01-08 01:09:08,810 iteration 4105 : loss : 0.025183, loss_ce: 0.009722
2022-01-08 01:09:10,147 iteration 4106 : loss : 0.026891, loss_ce: 0.005351
2022-01-08 01:09:11,628 iteration 4107 : loss : 0.036994, loss_ce: 0.020655
2022-01-08 01:09:13,064 iteration 4108 : loss : 0.025430, loss_ce: 0.009763
2022-01-08 01:09:14,424 iteration 4109 : loss : 0.017764, loss_ce: 0.004270
2022-01-08 01:09:15,827 iteration 4110 : loss : 0.027798, loss_ce: 0.011104
2022-01-08 01:09:17,276 iteration 4111 : loss : 0.024570, loss_ce: 0.009326
2022-01-08 01:09:18,642 iteration 4112 : loss : 0.027587, loss_ce: 0.010575
2022-01-08 01:09:20,075 iteration 4113 : loss : 0.022807, loss_ce: 0.009415
2022-01-08 01:09:21,545 iteration 4114 : loss : 0.043473, loss_ce: 0.019078
 60%|████████████████▎          | 242/400 [1:44:45<1:07:09, 25.50s/it]2022-01-08 01:09:22,945 iteration 4115 : loss : 0.022389, loss_ce: 0.006015
2022-01-08 01:09:24,275 iteration 4116 : loss : 0.020864, loss_ce: 0.008909
2022-01-08 01:09:25,822 iteration 4117 : loss : 0.040728, loss_ce: 0.013358
2022-01-08 01:09:27,336 iteration 4118 : loss : 0.024312, loss_ce: 0.010978
2022-01-08 01:09:28,765 iteration 4119 : loss : 0.026508, loss_ce: 0.011894
2022-01-08 01:09:30,179 iteration 4120 : loss : 0.041126, loss_ce: 0.008819
2022-01-08 01:09:31,589 iteration 4121 : loss : 0.022374, loss_ce: 0.010411
2022-01-08 01:09:32,980 iteration 4122 : loss : 0.028670, loss_ce: 0.012095
2022-01-08 01:09:34,440 iteration 4123 : loss : 0.041871, loss_ce: 0.015649
2022-01-08 01:09:35,778 iteration 4124 : loss : 0.029654, loss_ce: 0.011149
2022-01-08 01:09:37,233 iteration 4125 : loss : 0.027770, loss_ce: 0.008459
2022-01-08 01:09:38,667 iteration 4126 : loss : 0.022983, loss_ce: 0.011262
2022-01-08 01:09:40,095 iteration 4127 : loss : 0.024836, loss_ce: 0.009602
2022-01-08 01:09:41,456 iteration 4128 : loss : 0.023673, loss_ce: 0.012501
2022-01-08 01:09:42,903 iteration 4129 : loss : 0.029618, loss_ce: 0.012667
2022-01-08 01:09:44,312 iteration 4130 : loss : 0.021089, loss_ce: 0.010753
2022-01-08 01:09:45,626 iteration 4131 : loss : 0.016489, loss_ce: 0.004510
 61%|████████████████▍          | 243/400 [1:45:09<1:05:36, 25.08s/it]2022-01-08 01:09:47,103 iteration 4132 : loss : 0.028209, loss_ce: 0.011525
2022-01-08 01:09:48,549 iteration 4133 : loss : 0.032985, loss_ce: 0.015837
2022-01-08 01:09:49,872 iteration 4134 : loss : 0.021911, loss_ce: 0.009362
2022-01-08 01:09:51,345 iteration 4135 : loss : 0.025897, loss_ce: 0.010630
2022-01-08 01:09:52,739 iteration 4136 : loss : 0.018532, loss_ce: 0.009380
2022-01-08 01:09:54,104 iteration 4137 : loss : 0.021356, loss_ce: 0.011375
2022-01-08 01:09:55,458 iteration 4138 : loss : 0.020004, loss_ce: 0.008781
2022-01-08 01:09:56,906 iteration 4139 : loss : 0.028079, loss_ce: 0.009521
2022-01-08 01:09:58,349 iteration 4140 : loss : 0.028801, loss_ce: 0.009904
2022-01-08 01:09:59,735 iteration 4141 : loss : 0.050634, loss_ce: 0.014098
2022-01-08 01:10:01,146 iteration 4142 : loss : 0.034724, loss_ce: 0.011015
2022-01-08 01:10:02,617 iteration 4143 : loss : 0.027285, loss_ce: 0.006601
2022-01-08 01:10:03,986 iteration 4144 : loss : 0.023917, loss_ce: 0.009061
2022-01-08 01:10:05,351 iteration 4145 : loss : 0.024554, loss_ce: 0.007473
2022-01-08 01:10:06,735 iteration 4146 : loss : 0.031419, loss_ce: 0.009740
2022-01-08 01:10:08,104 iteration 4147 : loss : 0.033540, loss_ce: 0.010539
2022-01-08 01:10:09,403 iteration 4148 : loss : 0.020356, loss_ce: 0.007882
 61%|████████████████▍          | 244/400 [1:45:33<1:04:11, 24.69s/it]2022-01-08 01:10:10,938 iteration 4149 : loss : 0.032186, loss_ce: 0.012912
2022-01-08 01:10:12,252 iteration 4150 : loss : 0.023934, loss_ce: 0.006428
2022-01-08 01:10:13,672 iteration 4151 : loss : 0.021105, loss_ce: 0.009799
2022-01-08 01:10:14,984 iteration 4152 : loss : 0.020007, loss_ce: 0.007598
2022-01-08 01:10:16,399 iteration 4153 : loss : 0.041276, loss_ce: 0.010587
2022-01-08 01:10:17,806 iteration 4154 : loss : 0.030370, loss_ce: 0.012919
2022-01-08 01:10:19,256 iteration 4155 : loss : 0.024593, loss_ce: 0.008315
2022-01-08 01:10:20,569 iteration 4156 : loss : 0.015582, loss_ce: 0.006262
2022-01-08 01:10:21,985 iteration 4157 : loss : 0.023991, loss_ce: 0.008247
2022-01-08 01:10:23,372 iteration 4158 : loss : 0.037069, loss_ce: 0.017366
2022-01-08 01:10:24,788 iteration 4159 : loss : 0.023804, loss_ce: 0.010248
2022-01-08 01:10:26,182 iteration 4160 : loss : 0.024946, loss_ce: 0.010914
2022-01-08 01:10:27,597 iteration 4161 : loss : 0.023414, loss_ce: 0.009716
2022-01-08 01:10:28,959 iteration 4162 : loss : 0.020176, loss_ce: 0.007250
2022-01-08 01:10:30,379 iteration 4163 : loss : 0.031738, loss_ce: 0.008839
2022-01-08 01:10:31,811 iteration 4164 : loss : 0.026865, loss_ce: 0.011636
2022-01-08 01:10:31,811 Training Data Eval:
2022-01-08 01:10:38,858   Average segmentation loss on training set: 0.0165
2022-01-08 01:10:38,859 Validation Data Eval:
2022-01-08 01:10:41,251   Average segmentation loss on validation set: 0.0592
2022-01-08 01:10:45,438 Found new lowest validation loss at iteration 4164! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed100.pth
2022-01-08 01:10:46,737 iteration 4165 : loss : 0.030173, loss_ce: 0.012328
 61%|████████████████▌          | 245/400 [1:46:10<1:13:34, 28.48s/it]2022-01-08 01:10:48,008 iteration 4166 : loss : 0.019881, loss_ce: 0.008510
2022-01-08 01:10:49,386 iteration 4167 : loss : 0.032061, loss_ce: 0.008388
2022-01-08 01:10:50,847 iteration 4168 : loss : 0.027733, loss_ce: 0.011221
2022-01-08 01:10:52,257 iteration 4169 : loss : 0.036958, loss_ce: 0.016375
2022-01-08 01:10:53,686 iteration 4170 : loss : 0.028958, loss_ce: 0.011840
2022-01-08 01:10:55,041 iteration 4171 : loss : 0.021014, loss_ce: 0.008695
2022-01-08 01:10:56,436 iteration 4172 : loss : 0.027230, loss_ce: 0.010235
2022-01-08 01:10:57,761 iteration 4173 : loss : 0.022233, loss_ce: 0.007550
2022-01-08 01:10:59,188 iteration 4174 : loss : 0.036045, loss_ce: 0.011594
2022-01-08 01:11:00,593 iteration 4175 : loss : 0.023534, loss_ce: 0.006249
2022-01-08 01:11:02,025 iteration 4176 : loss : 0.036169, loss_ce: 0.011377
2022-01-08 01:11:03,357 iteration 4177 : loss : 0.020441, loss_ce: 0.009586
2022-01-08 01:11:04,732 iteration 4178 : loss : 0.019155, loss_ce: 0.007833
2022-01-08 01:11:06,158 iteration 4179 : loss : 0.026125, loss_ce: 0.010093
2022-01-08 01:11:07,621 iteration 4180 : loss : 0.026619, loss_ce: 0.009187
2022-01-08 01:11:09,060 iteration 4181 : loss : 0.034816, loss_ce: 0.014397
2022-01-08 01:11:10,501 iteration 4182 : loss : 0.038494, loss_ce: 0.015197
 62%|████████████████▌          | 246/400 [1:46:34<1:09:27, 27.06s/it]2022-01-08 01:11:11,919 iteration 4183 : loss : 0.042629, loss_ce: 0.010516
2022-01-08 01:11:13,289 iteration 4184 : loss : 0.024849, loss_ce: 0.008342
2022-01-08 01:11:14,737 iteration 4185 : loss : 0.029576, loss_ce: 0.009485
2022-01-08 01:11:16,106 iteration 4186 : loss : 0.022784, loss_ce: 0.008156
2022-01-08 01:11:17,448 iteration 4187 : loss : 0.023478, loss_ce: 0.009423
2022-01-08 01:11:18,820 iteration 4188 : loss : 0.023751, loss_ce: 0.009035
2022-01-08 01:11:20,131 iteration 4189 : loss : 0.023976, loss_ce: 0.008036
2022-01-08 01:11:21,647 iteration 4190 : loss : 0.035906, loss_ce: 0.014197
2022-01-08 01:11:23,083 iteration 4191 : loss : 0.023887, loss_ce: 0.005876
2022-01-08 01:11:24,413 iteration 4192 : loss : 0.020150, loss_ce: 0.009998
2022-01-08 01:11:25,724 iteration 4193 : loss : 0.018062, loss_ce: 0.005672
2022-01-08 01:11:27,123 iteration 4194 : loss : 0.025873, loss_ce: 0.008989
2022-01-08 01:11:28,473 iteration 4195 : loss : 0.020328, loss_ce: 0.008572
2022-01-08 01:11:29,814 iteration 4196 : loss : 0.019608, loss_ce: 0.007814
2022-01-08 01:11:31,153 iteration 4197 : loss : 0.022245, loss_ce: 0.008656
2022-01-08 01:11:32,480 iteration 4198 : loss : 0.026043, loss_ce: 0.006866
2022-01-08 01:11:33,931 iteration 4199 : loss : 0.025832, loss_ce: 0.010807
 62%|████████████████▋          | 247/400 [1:46:58<1:06:13, 25.97s/it]2022-01-08 01:11:35,351 iteration 4200 : loss : 0.023115, loss_ce: 0.007923
2022-01-08 01:11:36,777 iteration 4201 : loss : 0.036559, loss_ce: 0.017605
2022-01-08 01:11:38,240 iteration 4202 : loss : 0.020885, loss_ce: 0.009259
2022-01-08 01:11:39,746 iteration 4203 : loss : 0.032208, loss_ce: 0.014665
2022-01-08 01:11:41,177 iteration 4204 : loss : 0.020048, loss_ce: 0.007377
2022-01-08 01:11:42,423 iteration 4205 : loss : 0.014473, loss_ce: 0.005166
2022-01-08 01:11:43,780 iteration 4206 : loss : 0.031032, loss_ce: 0.008175
2022-01-08 01:11:45,148 iteration 4207 : loss : 0.016257, loss_ce: 0.007711
2022-01-08 01:11:46,582 iteration 4208 : loss : 0.021477, loss_ce: 0.006811
2022-01-08 01:11:47,953 iteration 4209 : loss : 0.020780, loss_ce: 0.006184
2022-01-08 01:11:49,319 iteration 4210 : loss : 0.018050, loss_ce: 0.006448
2022-01-08 01:11:50,721 iteration 4211 : loss : 0.022580, loss_ce: 0.008494
2022-01-08 01:11:52,094 iteration 4212 : loss : 0.017541, loss_ce: 0.007320
2022-01-08 01:11:53,545 iteration 4213 : loss : 0.025097, loss_ce: 0.008703
2022-01-08 01:11:54,995 iteration 4214 : loss : 0.025867, loss_ce: 0.010318
2022-01-08 01:11:56,425 iteration 4215 : loss : 0.032990, loss_ce: 0.008710
2022-01-08 01:11:57,844 iteration 4216 : loss : 0.033432, loss_ce: 0.011783
 62%|████████████████▋          | 248/400 [1:47:22<1:04:14, 25.36s/it]2022-01-08 01:11:59,353 iteration 4217 : loss : 0.054864, loss_ce: 0.020027
2022-01-08 01:12:00,830 iteration 4218 : loss : 0.027426, loss_ce: 0.013171
2022-01-08 01:12:02,247 iteration 4219 : loss : 0.022837, loss_ce: 0.008683
2022-01-08 01:12:03,669 iteration 4220 : loss : 0.037900, loss_ce: 0.013031
2022-01-08 01:12:05,092 iteration 4221 : loss : 0.033055, loss_ce: 0.011629
2022-01-08 01:12:06,533 iteration 4222 : loss : 0.021471, loss_ce: 0.008968
2022-01-08 01:12:08,055 iteration 4223 : loss : 0.040087, loss_ce: 0.009367
2022-01-08 01:12:09,473 iteration 4224 : loss : 0.025122, loss_ce: 0.009384
2022-01-08 01:12:10,917 iteration 4225 : loss : 0.030482, loss_ce: 0.012569
2022-01-08 01:12:12,401 iteration 4226 : loss : 0.026003, loss_ce: 0.009696
2022-01-08 01:12:13,775 iteration 4227 : loss : 0.037962, loss_ce: 0.014886
2022-01-08 01:12:15,218 iteration 4228 : loss : 0.027584, loss_ce: 0.012695
2022-01-08 01:12:16,567 iteration 4229 : loss : 0.027474, loss_ce: 0.011918
2022-01-08 01:12:18,007 iteration 4230 : loss : 0.029837, loss_ce: 0.010813
2022-01-08 01:12:19,431 iteration 4231 : loss : 0.039983, loss_ce: 0.014249
2022-01-08 01:12:20,880 iteration 4232 : loss : 0.039299, loss_ce: 0.015688
2022-01-08 01:12:22,237 iteration 4233 : loss : 0.020090, loss_ce: 0.007901
 62%|████████████████▊          | 249/400 [1:47:46<1:03:05, 25.07s/it]2022-01-08 01:12:23,716 iteration 4234 : loss : 0.025562, loss_ce: 0.010298
2022-01-08 01:12:25,067 iteration 4235 : loss : 0.022915, loss_ce: 0.008715
2022-01-08 01:12:26,511 iteration 4236 : loss : 0.027541, loss_ce: 0.013501
2022-01-08 01:12:27,924 iteration 4237 : loss : 0.022467, loss_ce: 0.010711
2022-01-08 01:12:29,392 iteration 4238 : loss : 0.035242, loss_ce: 0.010860
2022-01-08 01:12:30,717 iteration 4239 : loss : 0.024215, loss_ce: 0.007808
2022-01-08 01:12:32,189 iteration 4240 : loss : 0.032668, loss_ce: 0.010827
2022-01-08 01:12:33,591 iteration 4241 : loss : 0.056219, loss_ce: 0.031204
2022-01-08 01:12:34,965 iteration 4242 : loss : 0.031774, loss_ce: 0.009469
2022-01-08 01:12:36,344 iteration 4243 : loss : 0.028767, loss_ce: 0.009449
2022-01-08 01:12:37,784 iteration 4244 : loss : 0.033767, loss_ce: 0.012188
2022-01-08 01:12:39,197 iteration 4245 : loss : 0.051261, loss_ce: 0.014829
2022-01-08 01:12:40,584 iteration 4246 : loss : 0.027729, loss_ce: 0.013278
2022-01-08 01:12:41,953 iteration 4247 : loss : 0.029297, loss_ce: 0.006255
2022-01-08 01:12:43,379 iteration 4248 : loss : 0.040495, loss_ce: 0.016607
2022-01-08 01:12:44,779 iteration 4249 : loss : 0.033529, loss_ce: 0.016376
2022-01-08 01:12:44,780 Training Data Eval:
2022-01-08 01:12:51,827   Average segmentation loss on training set: 0.0173
2022-01-08 01:12:51,827 Validation Data Eval:
2022-01-08 01:12:54,216   Average segmentation loss on validation set: 0.0700
2022-01-08 01:12:55,645 iteration 4250 : loss : 0.028768, loss_ce: 0.011061
 62%|████████████████▉          | 250/400 [1:48:19<1:08:55, 27.57s/it]2022-01-08 01:12:57,136 iteration 4251 : loss : 0.033224, loss_ce: 0.012765
2022-01-08 01:12:58,596 iteration 4252 : loss : 0.029251, loss_ce: 0.011818
2022-01-08 01:13:00,020 iteration 4253 : loss : 0.025855, loss_ce: 0.009690
2022-01-08 01:13:01,504 iteration 4254 : loss : 0.026558, loss_ce: 0.011242
2022-01-08 01:13:02,818 iteration 4255 : loss : 0.019251, loss_ce: 0.007808
2022-01-08 01:13:04,231 iteration 4256 : loss : 0.023924, loss_ce: 0.007095
2022-01-08 01:13:05,697 iteration 4257 : loss : 0.026116, loss_ce: 0.010130
2022-01-08 01:13:07,065 iteration 4258 : loss : 0.019646, loss_ce: 0.007346
2022-01-08 01:13:08,467 iteration 4259 : loss : 0.023298, loss_ce: 0.008709
2022-01-08 01:13:09,893 iteration 4260 : loss : 0.027948, loss_ce: 0.011245
2022-01-08 01:13:11,307 iteration 4261 : loss : 0.020584, loss_ce: 0.008083
2022-01-08 01:13:12,796 iteration 4262 : loss : 0.027114, loss_ce: 0.012882
2022-01-08 01:13:14,200 iteration 4263 : loss : 0.025982, loss_ce: 0.012019
2022-01-08 01:13:15,545 iteration 4264 : loss : 0.038018, loss_ce: 0.011621
2022-01-08 01:13:16,928 iteration 4265 : loss : 0.034259, loss_ce: 0.012712
2022-01-08 01:13:18,321 iteration 4266 : loss : 0.034473, loss_ce: 0.014718
2022-01-08 01:13:19,718 iteration 4267 : loss : 0.029515, loss_ce: 0.015360
 63%|████████████████▉          | 251/400 [1:48:43<1:05:51, 26.52s/it]2022-01-08 01:13:21,153 iteration 4268 : loss : 0.027683, loss_ce: 0.011140
2022-01-08 01:13:22,518 iteration 4269 : loss : 0.020479, loss_ce: 0.007273
2022-01-08 01:13:23,875 iteration 4270 : loss : 0.029826, loss_ce: 0.004613
2022-01-08 01:13:25,218 iteration 4271 : loss : 0.022919, loss_ce: 0.010923
2022-01-08 01:13:26,651 iteration 4272 : loss : 0.023763, loss_ce: 0.008054
2022-01-08 01:13:28,086 iteration 4273 : loss : 0.025437, loss_ce: 0.010923
2022-01-08 01:13:29,537 iteration 4274 : loss : 0.032921, loss_ce: 0.016276
2022-01-08 01:13:30,809 iteration 4275 : loss : 0.019738, loss_ce: 0.007948
2022-01-08 01:13:32,153 iteration 4276 : loss : 0.026188, loss_ce: 0.008728
2022-01-08 01:13:33,504 iteration 4277 : loss : 0.017476, loss_ce: 0.007019
2022-01-08 01:13:34,884 iteration 4278 : loss : 0.035030, loss_ce: 0.010528
2022-01-08 01:13:36,203 iteration 4279 : loss : 0.018503, loss_ce: 0.006042
2022-01-08 01:13:37,536 iteration 4280 : loss : 0.037334, loss_ce: 0.013121
2022-01-08 01:13:39,034 iteration 4281 : loss : 0.028944, loss_ce: 0.013948
2022-01-08 01:13:40,461 iteration 4282 : loss : 0.022080, loss_ce: 0.010011
2022-01-08 01:13:41,763 iteration 4283 : loss : 0.021787, loss_ce: 0.009580
2022-01-08 01:13:43,131 iteration 4284 : loss : 0.025025, loss_ce: 0.011648
 63%|█████████████████          | 252/400 [1:49:07<1:03:07, 25.59s/it]2022-01-08 01:13:44,603 iteration 4285 : loss : 0.022900, loss_ce: 0.010285
2022-01-08 01:13:46,058 iteration 4286 : loss : 0.021890, loss_ce: 0.006757
2022-01-08 01:13:47,507 iteration 4287 : loss : 0.031358, loss_ce: 0.015587
2022-01-08 01:13:48,896 iteration 4288 : loss : 0.038634, loss_ce: 0.015161
2022-01-08 01:13:50,304 iteration 4289 : loss : 0.024926, loss_ce: 0.007387
2022-01-08 01:13:51,749 iteration 4290 : loss : 0.028606, loss_ce: 0.009885
2022-01-08 01:13:53,083 iteration 4291 : loss : 0.023890, loss_ce: 0.008043
2022-01-08 01:13:54,537 iteration 4292 : loss : 0.020309, loss_ce: 0.007793
2022-01-08 01:13:55,949 iteration 4293 : loss : 0.031453, loss_ce: 0.014027
2022-01-08 01:13:57,350 iteration 4294 : loss : 0.027876, loss_ce: 0.014317
2022-01-08 01:13:58,821 iteration 4295 : loss : 0.025115, loss_ce: 0.011400
2022-01-08 01:14:00,149 iteration 4296 : loss : 0.018583, loss_ce: 0.007720
2022-01-08 01:14:01,550 iteration 4297 : loss : 0.022083, loss_ce: 0.007219
2022-01-08 01:14:02,904 iteration 4298 : loss : 0.020775, loss_ce: 0.006839
2022-01-08 01:14:04,410 iteration 4299 : loss : 0.031446, loss_ce: 0.013195
2022-01-08 01:14:05,850 iteration 4300 : loss : 0.035738, loss_ce: 0.012220
2022-01-08 01:14:07,233 iteration 4301 : loss : 0.022080, loss_ce: 0.008231
 63%|█████████████████          | 253/400 [1:49:31<1:01:35, 25.14s/it]2022-01-08 01:14:08,692 iteration 4302 : loss : 0.020899, loss_ce: 0.008121
2022-01-08 01:14:10,163 iteration 4303 : loss : 0.024573, loss_ce: 0.008062
2022-01-08 01:14:11,509 iteration 4304 : loss : 0.022067, loss_ce: 0.008159
2022-01-08 01:14:12,928 iteration 4305 : loss : 0.018581, loss_ce: 0.005495
2022-01-08 01:14:14,292 iteration 4306 : loss : 0.024695, loss_ce: 0.008529
2022-01-08 01:14:15,667 iteration 4307 : loss : 0.021800, loss_ce: 0.007592
2022-01-08 01:14:17,143 iteration 4308 : loss : 0.035128, loss_ce: 0.012053
2022-01-08 01:14:18,502 iteration 4309 : loss : 0.021919, loss_ce: 0.006523
2022-01-08 01:14:19,919 iteration 4310 : loss : 0.028605, loss_ce: 0.013390
2022-01-08 01:14:21,328 iteration 4311 : loss : 0.022566, loss_ce: 0.010626
2022-01-08 01:14:22,869 iteration 4312 : loss : 0.028261, loss_ce: 0.011234
2022-01-08 01:14:24,268 iteration 4313 : loss : 0.025499, loss_ce: 0.009472
2022-01-08 01:14:25,657 iteration 4314 : loss : 0.022970, loss_ce: 0.007312
2022-01-08 01:14:27,017 iteration 4315 : loss : 0.021949, loss_ce: 0.008770
2022-01-08 01:14:28,494 iteration 4316 : loss : 0.026732, loss_ce: 0.010931
2022-01-08 01:14:29,870 iteration 4317 : loss : 0.035438, loss_ce: 0.011604
2022-01-08 01:14:31,241 iteration 4318 : loss : 0.025010, loss_ce: 0.014349
 64%|█████████████████▏         | 254/400 [1:49:55<1:00:21, 24.80s/it]2022-01-08 01:14:32,729 iteration 4319 : loss : 0.026267, loss_ce: 0.008564
2022-01-08 01:14:34,210 iteration 4320 : loss : 0.032719, loss_ce: 0.013987
2022-01-08 01:14:35,673 iteration 4321 : loss : 0.022694, loss_ce: 0.009491
2022-01-08 01:14:37,089 iteration 4322 : loss : 0.025498, loss_ce: 0.007203
2022-01-08 01:14:38,452 iteration 4323 : loss : 0.017838, loss_ce: 0.005478
2022-01-08 01:14:39,870 iteration 4324 : loss : 0.029185, loss_ce: 0.011281
2022-01-08 01:14:41,419 iteration 4325 : loss : 0.031230, loss_ce: 0.008230
2022-01-08 01:14:42,825 iteration 4326 : loss : 0.039615, loss_ce: 0.030609
2022-01-08 01:14:44,184 iteration 4327 : loss : 0.023620, loss_ce: 0.009206
2022-01-08 01:14:45,558 iteration 4328 : loss : 0.035950, loss_ce: 0.012531
2022-01-08 01:14:47,020 iteration 4329 : loss : 0.024630, loss_ce: 0.009983
2022-01-08 01:14:48,373 iteration 4330 : loss : 0.017277, loss_ce: 0.006445
2022-01-08 01:14:49,895 iteration 4331 : loss : 0.029203, loss_ce: 0.008998
2022-01-08 01:14:51,343 iteration 4332 : loss : 0.034129, loss_ce: 0.018035
2022-01-08 01:14:52,686 iteration 4333 : loss : 0.023466, loss_ce: 0.008224
2022-01-08 01:14:54,041 iteration 4334 : loss : 0.021155, loss_ce: 0.008658
2022-01-08 01:14:54,041 Training Data Eval:
2022-01-08 01:15:01,068   Average segmentation loss on training set: 0.0156
2022-01-08 01:15:01,068 Validation Data Eval:
2022-01-08 01:15:03,454   Average segmentation loss on validation set: 0.0635
2022-01-08 01:15:04,934 iteration 4335 : loss : 0.021737, loss_ce: 0.011289
 64%|█████████████████▏         | 255/400 [1:50:29<1:06:23, 27.47s/it]2022-01-08 01:15:06,362 iteration 4336 : loss : 0.022446, loss_ce: 0.008746
2022-01-08 01:15:07,777 iteration 4337 : loss : 0.032030, loss_ce: 0.013497
2022-01-08 01:15:09,261 iteration 4338 : loss : 0.025059, loss_ce: 0.009147
2022-01-08 01:15:10,648 iteration 4339 : loss : 0.027207, loss_ce: 0.008567
2022-01-08 01:15:12,081 iteration 4340 : loss : 0.019780, loss_ce: 0.007534
2022-01-08 01:15:13,505 iteration 4341 : loss : 0.025854, loss_ce: 0.010631
2022-01-08 01:15:14,889 iteration 4342 : loss : 0.040069, loss_ce: 0.012991
2022-01-08 01:15:16,300 iteration 4343 : loss : 0.018482, loss_ce: 0.005701
2022-01-08 01:15:17,679 iteration 4344 : loss : 0.023814, loss_ce: 0.008279
2022-01-08 01:15:19,067 iteration 4345 : loss : 0.025379, loss_ce: 0.007346
2022-01-08 01:15:20,424 iteration 4346 : loss : 0.017466, loss_ce: 0.008087
2022-01-08 01:15:21,862 iteration 4347 : loss : 0.026368, loss_ce: 0.008726
2022-01-08 01:15:23,298 iteration 4348 : loss : 0.022411, loss_ce: 0.011547
2022-01-08 01:15:24,697 iteration 4349 : loss : 0.023482, loss_ce: 0.008005
2022-01-08 01:15:26,062 iteration 4350 : loss : 0.020602, loss_ce: 0.008959
2022-01-08 01:15:27,388 iteration 4351 : loss : 0.016958, loss_ce: 0.007480
2022-01-08 01:15:28,771 iteration 4352 : loss : 0.016616, loss_ce: 0.006768
 64%|█████████████████▎         | 256/400 [1:50:52<1:03:18, 26.38s/it]2022-01-08 01:15:30,235 iteration 4353 : loss : 0.021625, loss_ce: 0.008570
2022-01-08 01:15:31,710 iteration 4354 : loss : 0.033416, loss_ce: 0.010376
2022-01-08 01:15:33,057 iteration 4355 : loss : 0.021004, loss_ce: 0.007231
2022-01-08 01:15:34,489 iteration 4356 : loss : 0.027130, loss_ce: 0.013625
2022-01-08 01:15:35,914 iteration 4357 : loss : 0.026694, loss_ce: 0.011291
2022-01-08 01:15:37,280 iteration 4358 : loss : 0.017057, loss_ce: 0.007204
2022-01-08 01:15:38,656 iteration 4359 : loss : 0.018553, loss_ce: 0.006490
2022-01-08 01:15:40,039 iteration 4360 : loss : 0.028141, loss_ce: 0.010555
2022-01-08 01:15:41,517 iteration 4361 : loss : 0.029762, loss_ce: 0.014956
2022-01-08 01:15:42,884 iteration 4362 : loss : 0.020764, loss_ce: 0.007110
2022-01-08 01:15:44,202 iteration 4363 : loss : 0.022641, loss_ce: 0.010888
2022-01-08 01:15:45,641 iteration 4364 : loss : 0.025663, loss_ce: 0.009108
2022-01-08 01:15:47,057 iteration 4365 : loss : 0.021889, loss_ce: 0.008696
2022-01-08 01:15:48,341 iteration 4366 : loss : 0.019728, loss_ce: 0.008681
2022-01-08 01:15:49,721 iteration 4367 : loss : 0.036312, loss_ce: 0.011332
2022-01-08 01:15:51,176 iteration 4368 : loss : 0.061358, loss_ce: 0.024916
2022-01-08 01:15:52,525 iteration 4369 : loss : 0.024341, loss_ce: 0.007031
 64%|█████████████████▎         | 257/400 [1:51:16<1:00:59, 25.59s/it]2022-01-08 01:15:54,001 iteration 4370 : loss : 0.028760, loss_ce: 0.006857
2022-01-08 01:15:55,398 iteration 4371 : loss : 0.025267, loss_ce: 0.011552
2022-01-08 01:15:56,716 iteration 4372 : loss : 0.020118, loss_ce: 0.008850
2022-01-08 01:15:58,144 iteration 4373 : loss : 0.027898, loss_ce: 0.008633
2022-01-08 01:15:59,539 iteration 4374 : loss : 0.022478, loss_ce: 0.011392
2022-01-08 01:16:00,863 iteration 4375 : loss : 0.021795, loss_ce: 0.009838
2022-01-08 01:16:02,226 iteration 4376 : loss : 0.021770, loss_ce: 0.007907
2022-01-08 01:16:03,704 iteration 4377 : loss : 0.026907, loss_ce: 0.008988
2022-01-08 01:16:05,166 iteration 4378 : loss : 0.035424, loss_ce: 0.014740
2022-01-08 01:16:06,653 iteration 4379 : loss : 0.042278, loss_ce: 0.018680
2022-01-08 01:16:08,072 iteration 4380 : loss : 0.021720, loss_ce: 0.010650
2022-01-08 01:16:09,505 iteration 4381 : loss : 0.034632, loss_ce: 0.013936
2022-01-08 01:16:10,911 iteration 4382 : loss : 0.026188, loss_ce: 0.010160
2022-01-08 01:16:12,302 iteration 4383 : loss : 0.030997, loss_ce: 0.007980
2022-01-08 01:16:13,703 iteration 4384 : loss : 0.025041, loss_ce: 0.008652
2022-01-08 01:16:15,010 iteration 4385 : loss : 0.017678, loss_ce: 0.008034
2022-01-08 01:16:16,416 iteration 4386 : loss : 0.025418, loss_ce: 0.010844
 64%|██████████████████▋          | 258/400 [1:51:40<59:21, 25.08s/it]2022-01-08 01:16:17,844 iteration 4387 : loss : 0.022304, loss_ce: 0.007904
2022-01-08 01:16:19,272 iteration 4388 : loss : 0.020112, loss_ce: 0.008565
2022-01-08 01:16:20,606 iteration 4389 : loss : 0.018710, loss_ce: 0.008474
2022-01-08 01:16:21,936 iteration 4390 : loss : 0.020832, loss_ce: 0.007763
2022-01-08 01:16:23,341 iteration 4391 : loss : 0.025487, loss_ce: 0.010843
2022-01-08 01:16:24,768 iteration 4392 : loss : 0.030712, loss_ce: 0.014676
2022-01-08 01:16:26,082 iteration 4393 : loss : 0.022158, loss_ce: 0.007552
2022-01-08 01:16:27,545 iteration 4394 : loss : 0.028686, loss_ce: 0.017923
2022-01-08 01:16:28,999 iteration 4395 : loss : 0.021977, loss_ce: 0.008557
2022-01-08 01:16:30,469 iteration 4396 : loss : 0.026180, loss_ce: 0.010825
2022-01-08 01:16:31,856 iteration 4397 : loss : 0.025990, loss_ce: 0.008397
2022-01-08 01:16:33,220 iteration 4398 : loss : 0.026967, loss_ce: 0.007456
2022-01-08 01:16:34,607 iteration 4399 : loss : 0.021477, loss_ce: 0.007815
2022-01-08 01:16:35,983 iteration 4400 : loss : 0.019507, loss_ce: 0.008282
2022-01-08 01:16:37,349 iteration 4401 : loss : 0.018949, loss_ce: 0.008187
2022-01-08 01:16:38,679 iteration 4402 : loss : 0.018213, loss_ce: 0.006049
2022-01-08 01:16:40,067 iteration 4403 : loss : 0.024570, loss_ce: 0.006817
 65%|██████████████████▊          | 259/400 [1:52:04<57:55, 24.65s/it]2022-01-08 01:16:41,467 iteration 4404 : loss : 0.016324, loss_ce: 0.008188
2022-01-08 01:16:42,913 iteration 4405 : loss : 0.024271, loss_ce: 0.007313
2022-01-08 01:16:44,320 iteration 4406 : loss : 0.027191, loss_ce: 0.009671
2022-01-08 01:16:45,656 iteration 4407 : loss : 0.024064, loss_ce: 0.008888
2022-01-08 01:16:46,960 iteration 4408 : loss : 0.024478, loss_ce: 0.007161
2022-01-08 01:16:48,379 iteration 4409 : loss : 0.020222, loss_ce: 0.007677
2022-01-08 01:16:49,775 iteration 4410 : loss : 0.026882, loss_ce: 0.008721
2022-01-08 01:16:51,227 iteration 4411 : loss : 0.018570, loss_ce: 0.008033
2022-01-08 01:16:52,563 iteration 4412 : loss : 0.023534, loss_ce: 0.009684
2022-01-08 01:16:54,052 iteration 4413 : loss : 0.023375, loss_ce: 0.007580
2022-01-08 01:16:55,354 iteration 4414 : loss : 0.017482, loss_ce: 0.006792
2022-01-08 01:16:56,755 iteration 4415 : loss : 0.023468, loss_ce: 0.010339
2022-01-08 01:16:58,110 iteration 4416 : loss : 0.032133, loss_ce: 0.006833
2022-01-08 01:16:59,520 iteration 4417 : loss : 0.023842, loss_ce: 0.008282
2022-01-08 01:17:00,922 iteration 4418 : loss : 0.024761, loss_ce: 0.011549
2022-01-08 01:17:02,332 iteration 4419 : loss : 0.024954, loss_ce: 0.008271
2022-01-08 01:17:02,332 Training Data Eval:
2022-01-08 01:17:09,371   Average segmentation loss on training set: 0.0145
2022-01-08 01:17:09,372 Validation Data Eval:
2022-01-08 01:17:11,760   Average segmentation loss on validation set: 0.0631
2022-01-08 01:17:13,094 iteration 4420 : loss : 0.015528, loss_ce: 0.006991
 65%|█████████████████▌         | 260/400 [1:52:37<1:03:22, 27.16s/it]2022-01-08 01:17:14,541 iteration 4421 : loss : 0.021861, loss_ce: 0.008052
2022-01-08 01:17:15,979 iteration 4422 : loss : 0.023154, loss_ce: 0.008700
2022-01-08 01:17:17,511 iteration 4423 : loss : 0.030347, loss_ce: 0.013269
2022-01-08 01:17:18,971 iteration 4424 : loss : 0.028827, loss_ce: 0.010684
2022-01-08 01:17:20,304 iteration 4425 : loss : 0.018063, loss_ce: 0.006346
2022-01-08 01:17:21,742 iteration 4426 : loss : 0.023932, loss_ce: 0.008532
2022-01-08 01:17:23,166 iteration 4427 : loss : 0.035166, loss_ce: 0.010587
2022-01-08 01:17:24,635 iteration 4428 : loss : 0.019971, loss_ce: 0.008261
2022-01-08 01:17:26,111 iteration 4429 : loss : 0.029498, loss_ce: 0.011099
2022-01-08 01:17:27,491 iteration 4430 : loss : 0.024463, loss_ce: 0.008750
2022-01-08 01:17:28,861 iteration 4431 : loss : 0.020511, loss_ce: 0.008678
2022-01-08 01:17:30,258 iteration 4432 : loss : 0.017980, loss_ce: 0.007481
2022-01-08 01:17:31,637 iteration 4433 : loss : 0.020617, loss_ce: 0.008115
2022-01-08 01:17:33,012 iteration 4434 : loss : 0.018991, loss_ce: 0.006907
2022-01-08 01:17:34,436 iteration 4435 : loss : 0.039366, loss_ce: 0.012414
2022-01-08 01:17:35,992 iteration 4436 : loss : 0.040398, loss_ce: 0.018728
2022-01-08 01:17:37,333 iteration 4437 : loss : 0.019302, loss_ce: 0.007292
 65%|█████████████████▌         | 261/400 [1:53:01<1:00:53, 26.29s/it]2022-01-08 01:17:38,824 iteration 4438 : loss : 0.051440, loss_ce: 0.015706
2022-01-08 01:17:40,181 iteration 4439 : loss : 0.029066, loss_ce: 0.009570
2022-01-08 01:17:41,648 iteration 4440 : loss : 0.025627, loss_ce: 0.008995
2022-01-08 01:17:43,087 iteration 4441 : loss : 0.028056, loss_ce: 0.012634
2022-01-08 01:17:44,434 iteration 4442 : loss : 0.018578, loss_ce: 0.008314
2022-01-08 01:17:45,837 iteration 4443 : loss : 0.019885, loss_ce: 0.008439
2022-01-08 01:17:47,196 iteration 4444 : loss : 0.033230, loss_ce: 0.011305
2022-01-08 01:17:48,610 iteration 4445 : loss : 0.031376, loss_ce: 0.012600
2022-01-08 01:17:50,033 iteration 4446 : loss : 0.020696, loss_ce: 0.008228
2022-01-08 01:17:51,453 iteration 4447 : loss : 0.025772, loss_ce: 0.010299
2022-01-08 01:17:52,838 iteration 4448 : loss : 0.052067, loss_ce: 0.015515
2022-01-08 01:17:54,272 iteration 4449 : loss : 0.028791, loss_ce: 0.013657
2022-01-08 01:17:55,723 iteration 4450 : loss : 0.018887, loss_ce: 0.006824
2022-01-08 01:17:57,048 iteration 4451 : loss : 0.022883, loss_ce: 0.007339
2022-01-08 01:17:58,451 iteration 4452 : loss : 0.021975, loss_ce: 0.008642
2022-01-08 01:17:59,967 iteration 4453 : loss : 0.039028, loss_ce: 0.017889
2022-01-08 01:18:01,393 iteration 4454 : loss : 0.029655, loss_ce: 0.013255
 66%|██████████████████▉          | 262/400 [1:53:25<58:55, 25.62s/it]2022-01-08 01:18:02,908 iteration 4455 : loss : 0.028882, loss_ce: 0.011404
2022-01-08 01:18:04,313 iteration 4456 : loss : 0.034294, loss_ce: 0.015992
2022-01-08 01:18:05,638 iteration 4457 : loss : 0.017574, loss_ce: 0.008365
2022-01-08 01:18:07,069 iteration 4458 : loss : 0.024298, loss_ce: 0.009665
2022-01-08 01:18:08,403 iteration 4459 : loss : 0.018293, loss_ce: 0.007580
2022-01-08 01:18:09,794 iteration 4460 : loss : 0.018754, loss_ce: 0.006796
2022-01-08 01:18:11,133 iteration 4461 : loss : 0.022211, loss_ce: 0.010207
2022-01-08 01:18:12,499 iteration 4462 : loss : 0.016702, loss_ce: 0.006719
2022-01-08 01:18:13,844 iteration 4463 : loss : 0.021574, loss_ce: 0.008043
2022-01-08 01:18:15,180 iteration 4464 : loss : 0.027885, loss_ce: 0.011481
2022-01-08 01:18:16,595 iteration 4465 : loss : 0.032025, loss_ce: 0.014093
2022-01-08 01:18:18,041 iteration 4466 : loss : 0.028700, loss_ce: 0.009860
2022-01-08 01:18:19,432 iteration 4467 : loss : 0.019287, loss_ce: 0.007090
2022-01-08 01:18:20,809 iteration 4468 : loss : 0.020853, loss_ce: 0.006894
2022-01-08 01:18:22,138 iteration 4469 : loss : 0.031671, loss_ce: 0.010466
2022-01-08 01:18:23,573 iteration 4470 : loss : 0.026490, loss_ce: 0.009177
2022-01-08 01:18:25,010 iteration 4471 : loss : 0.024434, loss_ce: 0.008704
 66%|███████████████████          | 263/400 [1:53:49<57:07, 25.02s/it]2022-01-08 01:18:26,408 iteration 4472 : loss : 0.019968, loss_ce: 0.008646
2022-01-08 01:18:27,780 iteration 4473 : loss : 0.028087, loss_ce: 0.010955
2022-01-08 01:18:29,194 iteration 4474 : loss : 0.023491, loss_ce: 0.008039
2022-01-08 01:18:30,562 iteration 4475 : loss : 0.022223, loss_ce: 0.009335
2022-01-08 01:18:31,859 iteration 4476 : loss : 0.016467, loss_ce: 0.005414
2022-01-08 01:18:33,243 iteration 4477 : loss : 0.021536, loss_ce: 0.008597
2022-01-08 01:18:34,637 iteration 4478 : loss : 0.023732, loss_ce: 0.008192
2022-01-08 01:18:36,052 iteration 4479 : loss : 0.027650, loss_ce: 0.006367
2022-01-08 01:18:37,390 iteration 4480 : loss : 0.019461, loss_ce: 0.008125
2022-01-08 01:18:38,721 iteration 4481 : loss : 0.015677, loss_ce: 0.006396
2022-01-08 01:18:40,161 iteration 4482 : loss : 0.035106, loss_ce: 0.015515
2022-01-08 01:18:41,658 iteration 4483 : loss : 0.034166, loss_ce: 0.017496
2022-01-08 01:18:43,127 iteration 4484 : loss : 0.024674, loss_ce: 0.007717
2022-01-08 01:18:44,486 iteration 4485 : loss : 0.017908, loss_ce: 0.005797
2022-01-08 01:18:45,984 iteration 4486 : loss : 0.029369, loss_ce: 0.013204
2022-01-08 01:18:47,337 iteration 4487 : loss : 0.028369, loss_ce: 0.015670
2022-01-08 01:18:48,735 iteration 4488 : loss : 0.021040, loss_ce: 0.008403
 66%|███████████████████▏         | 264/400 [1:54:12<55:50, 24.63s/it]2022-01-08 01:18:50,199 iteration 4489 : loss : 0.025101, loss_ce: 0.010033
2022-01-08 01:18:51,598 iteration 4490 : loss : 0.017137, loss_ce: 0.006777
2022-01-08 01:18:53,067 iteration 4491 : loss : 0.040074, loss_ce: 0.013830
2022-01-08 01:18:54,517 iteration 4492 : loss : 0.026097, loss_ce: 0.010599
2022-01-08 01:18:55,977 iteration 4493 : loss : 0.061540, loss_ce: 0.026774
2022-01-08 01:18:57,286 iteration 4494 : loss : 0.023189, loss_ce: 0.009030
2022-01-08 01:18:58,708 iteration 4495 : loss : 0.025541, loss_ce: 0.011233
2022-01-08 01:19:00,166 iteration 4496 : loss : 0.040951, loss_ce: 0.010782
2022-01-08 01:19:01,504 iteration 4497 : loss : 0.026676, loss_ce: 0.010439
2022-01-08 01:19:02,978 iteration 4498 : loss : 0.034553, loss_ce: 0.019841
2022-01-08 01:19:04,501 iteration 4499 : loss : 0.038904, loss_ce: 0.017163
2022-01-08 01:19:05,914 iteration 4500 : loss : 0.023291, loss_ce: 0.010721
2022-01-08 01:19:07,275 iteration 4501 : loss : 0.027493, loss_ce: 0.009720
2022-01-08 01:19:08,615 iteration 4502 : loss : 0.021104, loss_ce: 0.008124
2022-01-08 01:19:10,070 iteration 4503 : loss : 0.024407, loss_ce: 0.010154
2022-01-08 01:19:11,463 iteration 4504 : loss : 0.023069, loss_ce: 0.007556
2022-01-08 01:19:11,463 Training Data Eval:
2022-01-08 01:19:18,517   Average segmentation loss on training set: 0.0146
2022-01-08 01:19:18,517 Validation Data Eval:
2022-01-08 01:19:20,907   Average segmentation loss on validation set: 0.0700
2022-01-08 01:19:22,220 iteration 4505 : loss : 0.019952, loss_ce: 0.008344
 66%|█████████████████▉         | 265/400 [1:54:46<1:01:23, 27.29s/it]2022-01-08 01:19:23,716 iteration 4506 : loss : 0.030086, loss_ce: 0.009928
2022-01-08 01:19:25,094 iteration 4507 : loss : 0.016544, loss_ce: 0.005433
2022-01-08 01:19:26,536 iteration 4508 : loss : 0.039916, loss_ce: 0.013849
2022-01-08 01:19:27,943 iteration 4509 : loss : 0.031408, loss_ce: 0.009176
2022-01-08 01:19:29,342 iteration 4510 : loss : 0.019540, loss_ce: 0.007401
2022-01-08 01:19:30,720 iteration 4511 : loss : 0.027908, loss_ce: 0.010990
2022-01-08 01:19:32,101 iteration 4512 : loss : 0.017001, loss_ce: 0.006930
2022-01-08 01:19:33,508 iteration 4513 : loss : 0.019822, loss_ce: 0.009883
2022-01-08 01:19:34,835 iteration 4514 : loss : 0.021363, loss_ce: 0.007931
2022-01-08 01:19:36,231 iteration 4515 : loss : 0.033902, loss_ce: 0.011717
2022-01-08 01:19:37,726 iteration 4516 : loss : 0.027685, loss_ce: 0.011739
2022-01-08 01:19:39,107 iteration 4517 : loss : 0.022048, loss_ce: 0.007534
2022-01-08 01:19:40,524 iteration 4518 : loss : 0.020448, loss_ce: 0.006600
2022-01-08 01:19:41,943 iteration 4519 : loss : 0.021611, loss_ce: 0.010173
2022-01-08 01:19:43,292 iteration 4520 : loss : 0.022700, loss_ce: 0.007660
2022-01-08 01:19:44,699 iteration 4521 : loss : 0.020834, loss_ce: 0.006998
2022-01-08 01:19:46,034 iteration 4522 : loss : 0.022823, loss_ce: 0.012241
 66%|███████████████████▎         | 266/400 [1:55:10<58:36, 26.25s/it]2022-01-08 01:19:47,525 iteration 4523 : loss : 0.020455, loss_ce: 0.007266
2022-01-08 01:19:48,847 iteration 4524 : loss : 0.014628, loss_ce: 0.005934
2022-01-08 01:19:50,301 iteration 4525 : loss : 0.031821, loss_ce: 0.010512
2022-01-08 01:19:51,709 iteration 4526 : loss : 0.017958, loss_ce: 0.006660
2022-01-08 01:19:53,107 iteration 4527 : loss : 0.023258, loss_ce: 0.010383
2022-01-08 01:19:54,580 iteration 4528 : loss : 0.037701, loss_ce: 0.014116
2022-01-08 01:19:55,961 iteration 4529 : loss : 0.023373, loss_ce: 0.010000
2022-01-08 01:19:57,379 iteration 4530 : loss : 0.028021, loss_ce: 0.012170
2022-01-08 01:19:58,726 iteration 4531 : loss : 0.021195, loss_ce: 0.009490
2022-01-08 01:20:00,187 iteration 4532 : loss : 0.041489, loss_ce: 0.020375
2022-01-08 01:20:01,660 iteration 4533 : loss : 0.031597, loss_ce: 0.012230
2022-01-08 01:20:03,180 iteration 4534 : loss : 0.024368, loss_ce: 0.008784
2022-01-08 01:20:04,503 iteration 4535 : loss : 0.025050, loss_ce: 0.009015
2022-01-08 01:20:05,931 iteration 4536 : loss : 0.025024, loss_ce: 0.009713
2022-01-08 01:20:07,429 iteration 4537 : loss : 0.025395, loss_ce: 0.008972
2022-01-08 01:20:08,807 iteration 4538 : loss : 0.021046, loss_ce: 0.008348
2022-01-08 01:20:10,230 iteration 4539 : loss : 0.024252, loss_ce: 0.010043
 67%|███████████████████▎         | 267/400 [1:55:34<56:48, 25.63s/it]2022-01-08 01:20:11,717 iteration 4540 : loss : 0.028170, loss_ce: 0.008240
2022-01-08 01:20:13,047 iteration 4541 : loss : 0.019302, loss_ce: 0.007286
2022-01-08 01:20:14,443 iteration 4542 : loss : 0.035517, loss_ce: 0.007774
2022-01-08 01:20:15,748 iteration 4543 : loss : 0.022513, loss_ce: 0.008540
2022-01-08 01:20:17,168 iteration 4544 : loss : 0.030651, loss_ce: 0.013606
2022-01-08 01:20:18,660 iteration 4545 : loss : 0.032945, loss_ce: 0.013851
2022-01-08 01:20:20,081 iteration 4546 : loss : 0.033616, loss_ce: 0.011087
2022-01-08 01:20:21,494 iteration 4547 : loss : 0.021505, loss_ce: 0.010715
2022-01-08 01:20:22,888 iteration 4548 : loss : 0.034652, loss_ce: 0.013813
2022-01-08 01:20:24,390 iteration 4549 : loss : 0.029781, loss_ce: 0.010178
2022-01-08 01:20:25,860 iteration 4550 : loss : 0.031982, loss_ce: 0.009224
2022-01-08 01:20:27,310 iteration 4551 : loss : 0.027087, loss_ce: 0.006001
2022-01-08 01:20:28,627 iteration 4552 : loss : 0.020838, loss_ce: 0.008078
2022-01-08 01:20:29,974 iteration 4553 : loss : 0.029201, loss_ce: 0.011215
2022-01-08 01:20:31,382 iteration 4554 : loss : 0.020912, loss_ce: 0.008032
2022-01-08 01:20:32,821 iteration 4555 : loss : 0.044226, loss_ce: 0.018962
2022-01-08 01:20:34,208 iteration 4556 : loss : 0.019173, loss_ce: 0.008988
 67%|███████████████████▍         | 268/400 [1:55:58<55:17, 25.13s/it]2022-01-08 01:20:35,672 iteration 4557 : loss : 0.019912, loss_ce: 0.009081
2022-01-08 01:20:37,171 iteration 4558 : loss : 0.032512, loss_ce: 0.017451
2022-01-08 01:20:38,576 iteration 4559 : loss : 0.033367, loss_ce: 0.009949
2022-01-08 01:20:39,944 iteration 4560 : loss : 0.017371, loss_ce: 0.004927
2022-01-08 01:20:41,333 iteration 4561 : loss : 0.029143, loss_ce: 0.007708
2022-01-08 01:20:42,744 iteration 4562 : loss : 0.025991, loss_ce: 0.009295
2022-01-08 01:20:44,111 iteration 4563 : loss : 0.021849, loss_ce: 0.007418
2022-01-08 01:20:45,505 iteration 4564 : loss : 0.037259, loss_ce: 0.010796
2022-01-08 01:20:46,859 iteration 4565 : loss : 0.022461, loss_ce: 0.008798
2022-01-08 01:20:48,191 iteration 4566 : loss : 0.023157, loss_ce: 0.010309
2022-01-08 01:20:49,614 iteration 4567 : loss : 0.024013, loss_ce: 0.009919
2022-01-08 01:20:50,992 iteration 4568 : loss : 0.021659, loss_ce: 0.006686
2022-01-08 01:20:52,386 iteration 4569 : loss : 0.022194, loss_ce: 0.009367
2022-01-08 01:20:53,729 iteration 4570 : loss : 0.022234, loss_ce: 0.009858
2022-01-08 01:20:55,098 iteration 4571 : loss : 0.020371, loss_ce: 0.009786
2022-01-08 01:20:56,462 iteration 4572 : loss : 0.020257, loss_ce: 0.006830
2022-01-08 01:20:57,843 iteration 4573 : loss : 0.029263, loss_ce: 0.013893
 67%|███████████████████▌         | 269/400 [1:56:22<53:53, 24.69s/it]2022-01-08 01:20:59,259 iteration 4574 : loss : 0.030231, loss_ce: 0.009703
2022-01-08 01:21:00,625 iteration 4575 : loss : 0.019121, loss_ce: 0.007033
2022-01-08 01:21:02,045 iteration 4576 : loss : 0.024621, loss_ce: 0.014123
2022-01-08 01:21:03,484 iteration 4577 : loss : 0.025473, loss_ce: 0.008809
2022-01-08 01:21:04,883 iteration 4578 : loss : 0.036130, loss_ce: 0.015889
2022-01-08 01:21:06,259 iteration 4579 : loss : 0.024451, loss_ce: 0.012009
2022-01-08 01:21:07,635 iteration 4580 : loss : 0.019403, loss_ce: 0.008414
2022-01-08 01:21:09,115 iteration 4581 : loss : 0.026898, loss_ce: 0.008562
2022-01-08 01:21:10,515 iteration 4582 : loss : 0.025138, loss_ce: 0.011489
2022-01-08 01:21:11,912 iteration 4583 : loss : 0.025062, loss_ce: 0.008085
2022-01-08 01:21:13,297 iteration 4584 : loss : 0.022429, loss_ce: 0.007955
2022-01-08 01:21:14,713 iteration 4585 : loss : 0.023940, loss_ce: 0.007189
2022-01-08 01:21:16,073 iteration 4586 : loss : 0.020132, loss_ce: 0.008763
2022-01-08 01:21:17,547 iteration 4587 : loss : 0.023643, loss_ce: 0.007416
2022-01-08 01:21:18,932 iteration 4588 : loss : 0.021911, loss_ce: 0.005337
2022-01-08 01:21:20,334 iteration 4589 : loss : 0.022114, loss_ce: 0.009832
2022-01-08 01:21:20,334 Training Data Eval:
2022-01-08 01:21:27,381   Average segmentation loss on training set: 0.0144
2022-01-08 01:21:27,382 Validation Data Eval:
2022-01-08 01:21:29,773   Average segmentation loss on validation set: 0.0675
2022-01-08 01:21:31,325 iteration 4590 : loss : 0.030414, loss_ce: 0.011837
 68%|███████████████████▌         | 270/400 [1:56:55<59:11, 27.32s/it]2022-01-08 01:21:32,693 iteration 4591 : loss : 0.018836, loss_ce: 0.005546
2022-01-08 01:21:34,083 iteration 4592 : loss : 0.023364, loss_ce: 0.008038
2022-01-08 01:21:35,392 iteration 4593 : loss : 0.019004, loss_ce: 0.005849
2022-01-08 01:21:36,771 iteration 4594 : loss : 0.020413, loss_ce: 0.008086
2022-01-08 01:21:38,201 iteration 4595 : loss : 0.027054, loss_ce: 0.005313
2022-01-08 01:21:39,670 iteration 4596 : loss : 0.030098, loss_ce: 0.010434
2022-01-08 01:21:41,057 iteration 4597 : loss : 0.019196, loss_ce: 0.008077
2022-01-08 01:21:42,367 iteration 4598 : loss : 0.020727, loss_ce: 0.009247
2022-01-08 01:21:43,736 iteration 4599 : loss : 0.022593, loss_ce: 0.006286
2022-01-08 01:21:45,182 iteration 4600 : loss : 0.021489, loss_ce: 0.010203
2022-01-08 01:21:46,671 iteration 4601 : loss : 0.029074, loss_ce: 0.016152
2022-01-08 01:21:47,975 iteration 4602 : loss : 0.017744, loss_ce: 0.006642
2022-01-08 01:21:49,315 iteration 4603 : loss : 0.018070, loss_ce: 0.007849
2022-01-08 01:21:50,727 iteration 4604 : loss : 0.039049, loss_ce: 0.013747
2022-01-08 01:21:52,134 iteration 4605 : loss : 0.029437, loss_ce: 0.008193
2022-01-08 01:21:53,582 iteration 4606 : loss : 0.028072, loss_ce: 0.009525
2022-01-08 01:21:54,916 iteration 4607 : loss : 0.016207, loss_ce: 0.008093
 68%|███████████████████▋         | 271/400 [1:57:19<56:20, 26.21s/it]2022-01-08 01:21:56,384 iteration 4608 : loss : 0.025972, loss_ce: 0.008189
2022-01-08 01:21:57,840 iteration 4609 : loss : 0.027346, loss_ce: 0.012641
2022-01-08 01:21:59,269 iteration 4610 : loss : 0.025020, loss_ce: 0.006384
2022-01-08 01:22:00,652 iteration 4611 : loss : 0.022718, loss_ce: 0.008629
2022-01-08 01:22:01,980 iteration 4612 : loss : 0.017395, loss_ce: 0.006470
2022-01-08 01:22:03,389 iteration 4613 : loss : 0.019415, loss_ce: 0.008424
2022-01-08 01:22:04,808 iteration 4614 : loss : 0.021640, loss_ce: 0.008432
2022-01-08 01:22:06,231 iteration 4615 : loss : 0.034009, loss_ce: 0.013089
2022-01-08 01:22:07,727 iteration 4616 : loss : 0.026957, loss_ce: 0.009393
2022-01-08 01:22:09,062 iteration 4617 : loss : 0.021226, loss_ce: 0.009253
2022-01-08 01:22:10,493 iteration 4618 : loss : 0.028470, loss_ce: 0.010565
2022-01-08 01:22:11,928 iteration 4619 : loss : 0.028552, loss_ce: 0.010654
2022-01-08 01:22:13,402 iteration 4620 : loss : 0.021383, loss_ce: 0.007686
2022-01-08 01:22:14,857 iteration 4621 : loss : 0.028558, loss_ce: 0.011074
2022-01-08 01:22:16,249 iteration 4622 : loss : 0.023602, loss_ce: 0.008489
2022-01-08 01:22:17,617 iteration 4623 : loss : 0.034782, loss_ce: 0.015863
2022-01-08 01:22:19,026 iteration 4624 : loss : 0.025131, loss_ce: 0.010973
 68%|███████████████████▋         | 272/400 [1:57:43<54:33, 25.58s/it]2022-01-08 01:22:20,457 iteration 4625 : loss : 0.017132, loss_ce: 0.007320
2022-01-08 01:22:21,817 iteration 4626 : loss : 0.019965, loss_ce: 0.007273
2022-01-08 01:22:23,206 iteration 4627 : loss : 0.019382, loss_ce: 0.006809
2022-01-08 01:22:24,551 iteration 4628 : loss : 0.029473, loss_ce: 0.009856
2022-01-08 01:22:25,912 iteration 4629 : loss : 0.023336, loss_ce: 0.009247
2022-01-08 01:22:27,278 iteration 4630 : loss : 0.020065, loss_ce: 0.006358
2022-01-08 01:22:28,743 iteration 4631 : loss : 0.022451, loss_ce: 0.008477
2022-01-08 01:22:30,143 iteration 4632 : loss : 0.018507, loss_ce: 0.006359
2022-01-08 01:22:31,570 iteration 4633 : loss : 0.020833, loss_ce: 0.008277
2022-01-08 01:22:32,887 iteration 4634 : loss : 0.016931, loss_ce: 0.007581
2022-01-08 01:22:34,304 iteration 4635 : loss : 0.033205, loss_ce: 0.009378
2022-01-08 01:22:35,695 iteration 4636 : loss : 0.022304, loss_ce: 0.007766
2022-01-08 01:22:37,098 iteration 4637 : loss : 0.019702, loss_ce: 0.007262
2022-01-08 01:22:38,528 iteration 4638 : loss : 0.032061, loss_ce: 0.014324
2022-01-08 01:22:40,005 iteration 4639 : loss : 0.027837, loss_ce: 0.009951
2022-01-08 01:22:41,435 iteration 4640 : loss : 0.022351, loss_ce: 0.008635
2022-01-08 01:22:42,798 iteration 4641 : loss : 0.015869, loss_ce: 0.007199
 68%|███████████████████▊         | 273/400 [1:58:07<52:59, 25.03s/it]2022-01-08 01:22:44,284 iteration 4642 : loss : 0.026828, loss_ce: 0.008393
2022-01-08 01:22:45,717 iteration 4643 : loss : 0.029815, loss_ce: 0.012863
2022-01-08 01:22:47,139 iteration 4644 : loss : 0.022393, loss_ce: 0.009080
2022-01-08 01:22:48,694 iteration 4645 : loss : 0.026190, loss_ce: 0.010923
2022-01-08 01:22:50,087 iteration 4646 : loss : 0.018396, loss_ce: 0.008150
2022-01-08 01:22:51,454 iteration 4647 : loss : 0.020130, loss_ce: 0.008408
2022-01-08 01:22:52,755 iteration 4648 : loss : 0.016644, loss_ce: 0.006733
2022-01-08 01:22:54,166 iteration 4649 : loss : 0.024680, loss_ce: 0.009794
2022-01-08 01:22:55,565 iteration 4650 : loss : 0.020951, loss_ce: 0.007402
2022-01-08 01:22:56,967 iteration 4651 : loss : 0.020482, loss_ce: 0.008045
2022-01-08 01:22:58,372 iteration 4652 : loss : 0.025378, loss_ce: 0.010646
2022-01-08 01:22:59,762 iteration 4653 : loss : 0.028054, loss_ce: 0.011114
2022-01-08 01:23:01,172 iteration 4654 : loss : 0.021918, loss_ce: 0.010160
2022-01-08 01:23:02,598 iteration 4655 : loss : 0.019720, loss_ce: 0.006896
2022-01-08 01:23:04,068 iteration 4656 : loss : 0.026295, loss_ce: 0.007565
2022-01-08 01:23:05,454 iteration 4657 : loss : 0.052859, loss_ce: 0.007092
2022-01-08 01:23:06,809 iteration 4658 : loss : 0.021520, loss_ce: 0.005344
 68%|███████████████████▊         | 274/400 [1:58:31<51:55, 24.73s/it]2022-01-08 01:23:08,260 iteration 4659 : loss : 0.021593, loss_ce: 0.007995
2022-01-08 01:23:09,686 iteration 4660 : loss : 0.022005, loss_ce: 0.006449
2022-01-08 01:23:11,085 iteration 4661 : loss : 0.022149, loss_ce: 0.010708
2022-01-08 01:23:12,591 iteration 4662 : loss : 0.028112, loss_ce: 0.012962
2022-01-08 01:23:13,953 iteration 4663 : loss : 0.023308, loss_ce: 0.006418
2022-01-08 01:23:15,358 iteration 4664 : loss : 0.025647, loss_ce: 0.007433
2022-01-08 01:23:16,897 iteration 4665 : loss : 0.048469, loss_ce: 0.013672
2022-01-08 01:23:18,239 iteration 4666 : loss : 0.021189, loss_ce: 0.008247
2022-01-08 01:23:19,646 iteration 4667 : loss : 0.025708, loss_ce: 0.009812
2022-01-08 01:23:21,031 iteration 4668 : loss : 0.026613, loss_ce: 0.006082
2022-01-08 01:23:22,491 iteration 4669 : loss : 0.017377, loss_ce: 0.006468
2022-01-08 01:23:23,953 iteration 4670 : loss : 0.034050, loss_ce: 0.014589
2022-01-08 01:23:25,282 iteration 4671 : loss : 0.019958, loss_ce: 0.006358
2022-01-08 01:23:26,815 iteration 4672 : loss : 0.037897, loss_ce: 0.014676
2022-01-08 01:23:28,245 iteration 4673 : loss : 0.022030, loss_ce: 0.006667
2022-01-08 01:23:29,715 iteration 4674 : loss : 0.024856, loss_ce: 0.012093
2022-01-08 01:23:29,716 Training Data Eval:
2022-01-08 01:23:36,736   Average segmentation loss on training set: 0.0153
2022-01-08 01:23:36,737 Validation Data Eval:
2022-01-08 01:23:39,121   Average segmentation loss on validation set: 0.0758
2022-01-08 01:23:40,483 iteration 4675 : loss : 0.018881, loss_ce: 0.007531
 69%|███████████████████▉         | 275/400 [1:59:04<57:07, 27.42s/it]2022-01-08 01:23:42,004 iteration 4676 : loss : 0.040380, loss_ce: 0.017506
2022-01-08 01:23:43,379 iteration 4677 : loss : 0.025683, loss_ce: 0.009342
2022-01-08 01:23:44,787 iteration 4678 : loss : 0.029516, loss_ce: 0.009333
2022-01-08 01:23:46,206 iteration 4679 : loss : 0.024487, loss_ce: 0.012707
2022-01-08 01:23:47,559 iteration 4680 : loss : 0.020026, loss_ce: 0.008434
2022-01-08 01:23:48,963 iteration 4681 : loss : 0.018156, loss_ce: 0.005732
2022-01-08 01:23:50,326 iteration 4682 : loss : 0.024165, loss_ce: 0.005331
2022-01-08 01:23:51,712 iteration 4683 : loss : 0.024307, loss_ce: 0.010048
2022-01-08 01:23:53,140 iteration 4684 : loss : 0.026436, loss_ce: 0.010514
2022-01-08 01:23:54,544 iteration 4685 : loss : 0.024131, loss_ce: 0.011794
2022-01-08 01:23:55,875 iteration 4686 : loss : 0.017449, loss_ce: 0.005730
2022-01-08 01:23:57,337 iteration 4687 : loss : 0.021949, loss_ce: 0.008943
2022-01-08 01:23:58,800 iteration 4688 : loss : 0.025213, loss_ce: 0.009006
2022-01-08 01:24:00,275 iteration 4689 : loss : 0.027794, loss_ce: 0.011730
2022-01-08 01:24:01,661 iteration 4690 : loss : 0.033085, loss_ce: 0.010443
2022-01-08 01:24:03,030 iteration 4691 : loss : 0.019851, loss_ce: 0.006587
2022-01-08 01:24:04,412 iteration 4692 : loss : 0.019718, loss_ce: 0.008123
 69%|████████████████████         | 276/400 [1:59:28<54:29, 26.37s/it]2022-01-08 01:24:05,874 iteration 4693 : loss : 0.027318, loss_ce: 0.012106
2022-01-08 01:24:07,237 iteration 4694 : loss : 0.024176, loss_ce: 0.008076
2022-01-08 01:24:08,720 iteration 4695 : loss : 0.031459, loss_ce: 0.011711
2022-01-08 01:24:10,138 iteration 4696 : loss : 0.024054, loss_ce: 0.010581
2022-01-08 01:24:11,592 iteration 4697 : loss : 0.027655, loss_ce: 0.012296
2022-01-08 01:24:12,891 iteration 4698 : loss : 0.021212, loss_ce: 0.007431
2022-01-08 01:24:14,301 iteration 4699 : loss : 0.022605, loss_ce: 0.007717
2022-01-08 01:24:15,708 iteration 4700 : loss : 0.047769, loss_ce: 0.014744
2022-01-08 01:24:17,112 iteration 4701 : loss : 0.027448, loss_ce: 0.011601
2022-01-08 01:24:18,422 iteration 4702 : loss : 0.017057, loss_ce: 0.006365
2022-01-08 01:24:19,849 iteration 4703 : loss : 0.019269, loss_ce: 0.006863
2022-01-08 01:24:21,270 iteration 4704 : loss : 0.037346, loss_ce: 0.010027
2022-01-08 01:24:22,606 iteration 4705 : loss : 0.016707, loss_ce: 0.006185
2022-01-08 01:24:23,959 iteration 4706 : loss : 0.021631, loss_ce: 0.008979
2022-01-08 01:24:25,413 iteration 4707 : loss : 0.024832, loss_ce: 0.010600
2022-01-08 01:24:26,826 iteration 4708 : loss : 0.032066, loss_ce: 0.009006
2022-01-08 01:24:28,216 iteration 4709 : loss : 0.020759, loss_ce: 0.008880
 69%|████████████████████         | 277/400 [1:59:52<52:28, 25.59s/it]2022-01-08 01:24:29,715 iteration 4710 : loss : 0.025350, loss_ce: 0.008820
2022-01-08 01:24:31,077 iteration 4711 : loss : 0.024898, loss_ce: 0.012109
2022-01-08 01:24:32,495 iteration 4712 : loss : 0.021116, loss_ce: 0.008633
2022-01-08 01:24:33,869 iteration 4713 : loss : 0.017624, loss_ce: 0.006448
2022-01-08 01:24:35,225 iteration 4714 : loss : 0.018977, loss_ce: 0.009144
2022-01-08 01:24:36,695 iteration 4715 : loss : 0.037622, loss_ce: 0.012175
2022-01-08 01:24:38,093 iteration 4716 : loss : 0.016696, loss_ce: 0.006727
2022-01-08 01:24:39,507 iteration 4717 : loss : 0.030433, loss_ce: 0.010748
2022-01-08 01:24:40,882 iteration 4718 : loss : 0.020019, loss_ce: 0.006979
2022-01-08 01:24:42,241 iteration 4719 : loss : 0.014657, loss_ce: 0.005211
2022-01-08 01:24:43,688 iteration 4720 : loss : 0.022898, loss_ce: 0.009426
2022-01-08 01:24:45,098 iteration 4721 : loss : 0.020000, loss_ce: 0.006204
2022-01-08 01:24:46,483 iteration 4722 : loss : 0.022386, loss_ce: 0.009051
2022-01-08 01:24:47,910 iteration 4723 : loss : 0.028229, loss_ce: 0.007560
2022-01-08 01:24:49,249 iteration 4724 : loss : 0.020521, loss_ce: 0.010511
2022-01-08 01:24:50,638 iteration 4725 : loss : 0.021606, loss_ce: 0.008017
2022-01-08 01:24:52,000 iteration 4726 : loss : 0.021895, loss_ce: 0.007136
 70%|████████████████████▏        | 278/400 [2:00:16<50:56, 25.05s/it]2022-01-08 01:24:53,429 iteration 4727 : loss : 0.026337, loss_ce: 0.009728
2022-01-08 01:24:54,846 iteration 4728 : loss : 0.023966, loss_ce: 0.009108
2022-01-08 01:24:56,274 iteration 4729 : loss : 0.024851, loss_ce: 0.007597
2022-01-08 01:24:57,700 iteration 4730 : loss : 0.025986, loss_ce: 0.009463
2022-01-08 01:24:59,089 iteration 4731 : loss : 0.029248, loss_ce: 0.009133
2022-01-08 01:25:00,419 iteration 4732 : loss : 0.017350, loss_ce: 0.007472
2022-01-08 01:25:01,904 iteration 4733 : loss : 0.049911, loss_ce: 0.022742
2022-01-08 01:25:03,334 iteration 4734 : loss : 0.023910, loss_ce: 0.009879
2022-01-08 01:25:04,664 iteration 4735 : loss : 0.017641, loss_ce: 0.007176
2022-01-08 01:25:06,104 iteration 4736 : loss : 0.024056, loss_ce: 0.009806
2022-01-08 01:25:07,520 iteration 4737 : loss : 0.023244, loss_ce: 0.011374
2022-01-08 01:25:09,081 iteration 4738 : loss : 0.037253, loss_ce: 0.010643
2022-01-08 01:25:10,515 iteration 4739 : loss : 0.025198, loss_ce: 0.010825
2022-01-08 01:25:11,928 iteration 4740 : loss : 0.017225, loss_ce: 0.004898
2022-01-08 01:25:13,324 iteration 4741 : loss : 0.026382, loss_ce: 0.013684
2022-01-08 01:25:14,760 iteration 4742 : loss : 0.028250, loss_ce: 0.013126
2022-01-08 01:25:16,337 iteration 4743 : loss : 0.038213, loss_ce: 0.016963
 70%|████████████████████▏        | 279/400 [2:00:40<50:05, 24.84s/it]2022-01-08 01:25:17,833 iteration 4744 : loss : 0.021473, loss_ce: 0.007780
2022-01-08 01:25:19,253 iteration 4745 : loss : 0.018813, loss_ce: 0.007660
2022-01-08 01:25:20,664 iteration 4746 : loss : 0.027339, loss_ce: 0.008900
2022-01-08 01:25:21,996 iteration 4747 : loss : 0.033249, loss_ce: 0.021790
2022-01-08 01:25:23,483 iteration 4748 : loss : 0.027453, loss_ce: 0.011407
2022-01-08 01:25:24,821 iteration 4749 : loss : 0.020700, loss_ce: 0.008495
2022-01-08 01:25:26,234 iteration 4750 : loss : 0.026555, loss_ce: 0.009017
2022-01-08 01:25:27,688 iteration 4751 : loss : 0.024529, loss_ce: 0.011250
2022-01-08 01:25:29,079 iteration 4752 : loss : 0.030254, loss_ce: 0.009313
2022-01-08 01:25:30,446 iteration 4753 : loss : 0.015196, loss_ce: 0.005306
2022-01-08 01:25:31,857 iteration 4754 : loss : 0.023807, loss_ce: 0.011364
2022-01-08 01:25:33,373 iteration 4755 : loss : 0.031243, loss_ce: 0.009586
2022-01-08 01:25:34,803 iteration 4756 : loss : 0.030322, loss_ce: 0.009284
2022-01-08 01:25:36,212 iteration 4757 : loss : 0.021481, loss_ce: 0.011168
2022-01-08 01:25:37,689 iteration 4758 : loss : 0.028558, loss_ce: 0.007909
2022-01-08 01:25:39,113 iteration 4759 : loss : 0.032846, loss_ce: 0.019296
2022-01-08 01:25:39,113 Training Data Eval:
2022-01-08 01:25:46,150   Average segmentation loss on training set: 0.0161
2022-01-08 01:25:46,150 Validation Data Eval:
2022-01-08 01:25:48,534   Average segmentation loss on validation set: 0.1013
2022-01-08 01:25:49,944 iteration 4760 : loss : 0.020608, loss_ce: 0.006786
 70%|████████████████████▎        | 280/400 [2:01:14<54:56, 27.47s/it]2022-01-08 01:25:51,341 iteration 4761 : loss : 0.017386, loss_ce: 0.006214
2022-01-08 01:25:52,821 iteration 4762 : loss : 0.038286, loss_ce: 0.012255
2022-01-08 01:25:54,241 iteration 4763 : loss : 0.022413, loss_ce: 0.010027
2022-01-08 01:25:55,580 iteration 4764 : loss : 0.020093, loss_ce: 0.007830
2022-01-08 01:25:56,965 iteration 4765 : loss : 0.025399, loss_ce: 0.010353
2022-01-08 01:25:58,319 iteration 4766 : loss : 0.019932, loss_ce: 0.005937
2022-01-08 01:25:59,758 iteration 4767 : loss : 0.030666, loss_ce: 0.014050
2022-01-08 01:26:01,247 iteration 4768 : loss : 0.032117, loss_ce: 0.014140
2022-01-08 01:26:02,723 iteration 4769 : loss : 0.049943, loss_ce: 0.013438
2022-01-08 01:26:04,153 iteration 4770 : loss : 0.020847, loss_ce: 0.010689
2022-01-08 01:26:05,518 iteration 4771 : loss : 0.020086, loss_ce: 0.008779
2022-01-08 01:26:06,900 iteration 4772 : loss : 0.018935, loss_ce: 0.006280
2022-01-08 01:26:08,259 iteration 4773 : loss : 0.018722, loss_ce: 0.007527
2022-01-08 01:26:09,644 iteration 4774 : loss : 0.027352, loss_ce: 0.011886
2022-01-08 01:26:11,050 iteration 4775 : loss : 0.028988, loss_ce: 0.011603
2022-01-08 01:26:12,440 iteration 4776 : loss : 0.025063, loss_ce: 0.010944
2022-01-08 01:26:13,820 iteration 4777 : loss : 0.020162, loss_ce: 0.007291
 70%|████████████████████▎        | 281/400 [2:01:38<52:20, 26.39s/it]2022-01-08 01:26:15,353 iteration 4778 : loss : 0.023580, loss_ce: 0.009085
2022-01-08 01:26:16,728 iteration 4779 : loss : 0.017580, loss_ce: 0.005941
2022-01-08 01:26:18,089 iteration 4780 : loss : 0.027087, loss_ce: 0.007156
2022-01-08 01:26:19,453 iteration 4781 : loss : 0.016151, loss_ce: 0.007504
2022-01-08 01:26:20,901 iteration 4782 : loss : 0.022757, loss_ce: 0.008905
2022-01-08 01:26:22,247 iteration 4783 : loss : 0.021964, loss_ce: 0.006270
2022-01-08 01:26:23,703 iteration 4784 : loss : 0.031898, loss_ce: 0.014402
2022-01-08 01:26:25,180 iteration 4785 : loss : 0.028673, loss_ce: 0.013646
2022-01-08 01:26:26,575 iteration 4786 : loss : 0.018948, loss_ce: 0.007003
2022-01-08 01:26:28,027 iteration 4787 : loss : 0.025110, loss_ce: 0.005724
2022-01-08 01:26:29,440 iteration 4788 : loss : 0.020961, loss_ce: 0.008710
2022-01-08 01:26:30,854 iteration 4789 : loss : 0.018855, loss_ce: 0.007416
2022-01-08 01:26:32,248 iteration 4790 : loss : 0.023157, loss_ce: 0.011438
2022-01-08 01:26:33,634 iteration 4791 : loss : 0.016480, loss_ce: 0.005847
2022-01-08 01:26:35,018 iteration 4792 : loss : 0.029561, loss_ce: 0.013855
2022-01-08 01:26:36,446 iteration 4793 : loss : 0.021676, loss_ce: 0.011128
2022-01-08 01:26:37,828 iteration 4794 : loss : 0.045991, loss_ce: 0.008051
 70%|████████████████████▍        | 282/400 [2:02:02<50:29, 25.67s/it]2022-01-08 01:26:39,210 iteration 4795 : loss : 0.018644, loss_ce: 0.005497
2022-01-08 01:26:40,547 iteration 4796 : loss : 0.024882, loss_ce: 0.010260
2022-01-08 01:26:41,894 iteration 4797 : loss : 0.023245, loss_ce: 0.009366
2022-01-08 01:26:43,346 iteration 4798 : loss : 0.037234, loss_ce: 0.017209
2022-01-08 01:26:44,745 iteration 4799 : loss : 0.036955, loss_ce: 0.008234
2022-01-08 01:26:46,179 iteration 4800 : loss : 0.053695, loss_ce: 0.029601
2022-01-08 01:26:47,584 iteration 4801 : loss : 0.022533, loss_ce: 0.009993
2022-01-08 01:26:48,889 iteration 4802 : loss : 0.016022, loss_ce: 0.005400
2022-01-08 01:26:50,286 iteration 4803 : loss : 0.021932, loss_ce: 0.009481
2022-01-08 01:26:51,669 iteration 4804 : loss : 0.025106, loss_ce: 0.009731
2022-01-08 01:26:53,111 iteration 4805 : loss : 0.029288, loss_ce: 0.010953
2022-01-08 01:26:54,463 iteration 4806 : loss : 0.031536, loss_ce: 0.010079
2022-01-08 01:26:55,932 iteration 4807 : loss : 0.029401, loss_ce: 0.012172
2022-01-08 01:26:57,581 iteration 4808 : loss : 0.024570, loss_ce: 0.009950
2022-01-08 01:26:58,888 iteration 4809 : loss : 0.017372, loss_ce: 0.007786
2022-01-08 01:27:00,326 iteration 4810 : loss : 0.022928, loss_ce: 0.009192
2022-01-08 01:27:01,702 iteration 4811 : loss : 0.018174, loss_ce: 0.007175
 71%|████████████████████▌        | 283/400 [2:02:25<49:00, 25.14s/it]2022-01-08 01:27:03,111 iteration 4812 : loss : 0.022140, loss_ce: 0.007882
2022-01-08 01:27:04,543 iteration 4813 : loss : 0.032053, loss_ce: 0.012686
2022-01-08 01:27:05,893 iteration 4814 : loss : 0.025341, loss_ce: 0.014578
2022-01-08 01:27:07,295 iteration 4815 : loss : 0.022201, loss_ce: 0.008040
2022-01-08 01:27:08,657 iteration 4816 : loss : 0.027912, loss_ce: 0.009252
2022-01-08 01:27:09,944 iteration 4817 : loss : 0.021325, loss_ce: 0.007313
2022-01-08 01:27:11,372 iteration 4818 : loss : 0.034184, loss_ce: 0.015561
2022-01-08 01:27:12,783 iteration 4819 : loss : 0.024322, loss_ce: 0.007787
2022-01-08 01:27:14,233 iteration 4820 : loss : 0.021787, loss_ce: 0.010561
2022-01-08 01:27:15,629 iteration 4821 : loss : 0.017566, loss_ce: 0.007122
2022-01-08 01:27:17,047 iteration 4822 : loss : 0.027354, loss_ce: 0.009798
2022-01-08 01:27:18,410 iteration 4823 : loss : 0.020824, loss_ce: 0.008591
2022-01-08 01:27:19,862 iteration 4824 : loss : 0.018382, loss_ce: 0.006975
2022-01-08 01:27:21,264 iteration 4825 : loss : 0.015013, loss_ce: 0.004766
2022-01-08 01:27:22,633 iteration 4826 : loss : 0.025089, loss_ce: 0.007862
2022-01-08 01:27:24,136 iteration 4827 : loss : 0.031943, loss_ce: 0.010237
2022-01-08 01:27:25,597 iteration 4828 : loss : 0.028962, loss_ce: 0.013064
 71%|████████████████████▌        | 284/400 [2:02:49<47:52, 24.77s/it]2022-01-08 01:27:27,078 iteration 4829 : loss : 0.035413, loss_ce: 0.010947
2022-01-08 01:27:28,422 iteration 4830 : loss : 0.021851, loss_ce: 0.007635
2022-01-08 01:27:29,902 iteration 4831 : loss : 0.040669, loss_ce: 0.013741
2022-01-08 01:27:31,294 iteration 4832 : loss : 0.032266, loss_ce: 0.013039
2022-01-08 01:27:32,756 iteration 4833 : loss : 0.022890, loss_ce: 0.006704
2022-01-08 01:27:34,248 iteration 4834 : loss : 0.018982, loss_ce: 0.006896
2022-01-08 01:27:35,635 iteration 4835 : loss : 0.023945, loss_ce: 0.008860
2022-01-08 01:27:37,032 iteration 4836 : loss : 0.033012, loss_ce: 0.009942
2022-01-08 01:27:38,384 iteration 4837 : loss : 0.025040, loss_ce: 0.005122
2022-01-08 01:27:39,845 iteration 4838 : loss : 0.021788, loss_ce: 0.008565
2022-01-08 01:27:41,274 iteration 4839 : loss : 0.021508, loss_ce: 0.010473
2022-01-08 01:27:42,686 iteration 4840 : loss : 0.022562, loss_ce: 0.009235
2022-01-08 01:27:44,161 iteration 4841 : loss : 0.022299, loss_ce: 0.007426
2022-01-08 01:27:45,619 iteration 4842 : loss : 0.024230, loss_ce: 0.010743
2022-01-08 01:27:47,028 iteration 4843 : loss : 0.021048, loss_ce: 0.007948
2022-01-08 01:27:48,364 iteration 4844 : loss : 0.020783, loss_ce: 0.010756
2022-01-08 01:27:48,365 Training Data Eval:
2022-01-08 01:27:55,399   Average segmentation loss on training set: 0.0137
2022-01-08 01:27:55,399 Validation Data Eval:
2022-01-08 01:27:57,788   Average segmentation loss on validation set: 0.0751
2022-01-08 01:27:59,140 iteration 4845 : loss : 0.027362, loss_ce: 0.009556
 71%|████████████████████▋        | 285/400 [2:03:23<52:30, 27.40s/it]2022-01-08 01:28:00,595 iteration 4846 : loss : 0.021337, loss_ce: 0.007398
2022-01-08 01:28:01,968 iteration 4847 : loss : 0.026933, loss_ce: 0.009336
2022-01-08 01:28:03,498 iteration 4848 : loss : 0.034502, loss_ce: 0.016257
2022-01-08 01:28:04,885 iteration 4849 : loss : 0.018024, loss_ce: 0.007815
2022-01-08 01:28:06,313 iteration 4850 : loss : 0.051930, loss_ce: 0.011790
2022-01-08 01:28:07,735 iteration 4851 : loss : 0.021566, loss_ce: 0.010606
2022-01-08 01:28:09,103 iteration 4852 : loss : 0.019945, loss_ce: 0.007971
2022-01-08 01:28:10,466 iteration 4853 : loss : 0.020316, loss_ce: 0.007712
2022-01-08 01:28:11,787 iteration 4854 : loss : 0.018824, loss_ce: 0.006556
2022-01-08 01:28:13,181 iteration 4855 : loss : 0.016819, loss_ce: 0.006137
2022-01-08 01:28:14,595 iteration 4856 : loss : 0.025531, loss_ce: 0.009830
2022-01-08 01:28:16,054 iteration 4857 : loss : 0.022374, loss_ce: 0.011719
2022-01-08 01:28:17,454 iteration 4858 : loss : 0.021368, loss_ce: 0.008383
2022-01-08 01:28:18,790 iteration 4859 : loss : 0.014623, loss_ce: 0.005985
2022-01-08 01:28:20,155 iteration 4860 : loss : 0.021478, loss_ce: 0.006300
2022-01-08 01:28:21,484 iteration 4861 : loss : 0.023254, loss_ce: 0.007323
2022-01-08 01:28:22,987 iteration 4862 : loss : 0.028230, loss_ce: 0.009960
 72%|████████████████████▋        | 286/400 [2:03:47<50:01, 26.33s/it]2022-01-08 01:28:24,451 iteration 4863 : loss : 0.023520, loss_ce: 0.008101
2022-01-08 01:28:25,876 iteration 4864 : loss : 0.021970, loss_ce: 0.008843
2022-01-08 01:28:27,286 iteration 4865 : loss : 0.018229, loss_ce: 0.006380
2022-01-08 01:28:28,689 iteration 4866 : loss : 0.026317, loss_ce: 0.010976
2022-01-08 01:28:30,117 iteration 4867 : loss : 0.027201, loss_ce: 0.009767
2022-01-08 01:28:31,525 iteration 4868 : loss : 0.020587, loss_ce: 0.009721
2022-01-08 01:28:32,888 iteration 4869 : loss : 0.019327, loss_ce: 0.006437
2022-01-08 01:28:34,200 iteration 4870 : loss : 0.019340, loss_ce: 0.007281
2022-01-08 01:28:35,556 iteration 4871 : loss : 0.020680, loss_ce: 0.006629
2022-01-08 01:28:36,944 iteration 4872 : loss : 0.020212, loss_ce: 0.009677
2022-01-08 01:28:38,335 iteration 4873 : loss : 0.019770, loss_ce: 0.006692
2022-01-08 01:28:39,755 iteration 4874 : loss : 0.017394, loss_ce: 0.006120
2022-01-08 01:28:41,135 iteration 4875 : loss : 0.020691, loss_ce: 0.006379
2022-01-08 01:28:42,441 iteration 4876 : loss : 0.017437, loss_ce: 0.007627
2022-01-08 01:28:43,805 iteration 4877 : loss : 0.015361, loss_ce: 0.005737
2022-01-08 01:28:45,200 iteration 4878 : loss : 0.030444, loss_ce: 0.010784
2022-01-08 01:28:46,669 iteration 4879 : loss : 0.019284, loss_ce: 0.005523
 72%|████████████████████▊        | 287/400 [2:04:10<48:05, 25.54s/it]2022-01-08 01:28:48,126 iteration 4880 : loss : 0.019045, loss_ce: 0.006542
2022-01-08 01:28:49,458 iteration 4881 : loss : 0.033024, loss_ce: 0.012498
2022-01-08 01:28:50,788 iteration 4882 : loss : 0.016365, loss_ce: 0.004252
2022-01-08 01:28:52,229 iteration 4883 : loss : 0.022657, loss_ce: 0.010355
2022-01-08 01:28:53,575 iteration 4884 : loss : 0.022976, loss_ce: 0.009642
2022-01-08 01:28:54,966 iteration 4885 : loss : 0.022309, loss_ce: 0.007666
2022-01-08 01:28:56,389 iteration 4886 : loss : 0.020925, loss_ce: 0.007099
2022-01-08 01:28:57,770 iteration 4887 : loss : 0.022597, loss_ce: 0.010349
2022-01-08 01:28:59,194 iteration 4888 : loss : 0.017916, loss_ce: 0.004753
2022-01-08 01:29:00,567 iteration 4889 : loss : 0.018943, loss_ce: 0.007891
2022-01-08 01:29:01,972 iteration 4890 : loss : 0.021182, loss_ce: 0.007714
2022-01-08 01:29:03,374 iteration 4891 : loss : 0.027896, loss_ce: 0.013359
2022-01-08 01:29:04,799 iteration 4892 : loss : 0.024964, loss_ce: 0.012660
2022-01-08 01:29:06,355 iteration 4893 : loss : 0.031940, loss_ce: 0.012382
2022-01-08 01:29:07,756 iteration 4894 : loss : 0.016105, loss_ce: 0.007113
2022-01-08 01:29:09,143 iteration 4895 : loss : 0.021552, loss_ce: 0.010415
2022-01-08 01:29:10,539 iteration 4896 : loss : 0.020739, loss_ce: 0.008135
 72%|████████████████████▉        | 288/400 [2:04:34<46:44, 25.04s/it]2022-01-08 01:29:11,904 iteration 4897 : loss : 0.027914, loss_ce: 0.012088
2022-01-08 01:29:13,280 iteration 4898 : loss : 0.015541, loss_ce: 0.005886
2022-01-08 01:29:14,620 iteration 4899 : loss : 0.024196, loss_ce: 0.011369
2022-01-08 01:29:16,105 iteration 4900 : loss : 0.036978, loss_ce: 0.014495
2022-01-08 01:29:17,512 iteration 4901 : loss : 0.018825, loss_ce: 0.005670
2022-01-08 01:29:18,863 iteration 4902 : loss : 0.020927, loss_ce: 0.004499
2022-01-08 01:29:20,265 iteration 4903 : loss : 0.017636, loss_ce: 0.007004
2022-01-08 01:29:21,649 iteration 4904 : loss : 0.017723, loss_ce: 0.008251
2022-01-08 01:29:23,038 iteration 4905 : loss : 0.015356, loss_ce: 0.006924
2022-01-08 01:29:24,406 iteration 4906 : loss : 0.017276, loss_ce: 0.005634
2022-01-08 01:29:25,798 iteration 4907 : loss : 0.025495, loss_ce: 0.008843
2022-01-08 01:29:27,091 iteration 4908 : loss : 0.032301, loss_ce: 0.008506
2022-01-08 01:29:28,497 iteration 4909 : loss : 0.027556, loss_ce: 0.008178
2022-01-08 01:29:29,860 iteration 4910 : loss : 0.022095, loss_ce: 0.005735
2022-01-08 01:29:31,329 iteration 4911 : loss : 0.020286, loss_ce: 0.008665
2022-01-08 01:29:32,777 iteration 4912 : loss : 0.035361, loss_ce: 0.010935
2022-01-08 01:29:34,156 iteration 4913 : loss : 0.018553, loss_ce: 0.006523
 72%|████████████████████▉        | 289/400 [2:04:58<45:31, 24.61s/it]2022-01-08 01:29:35,520 iteration 4914 : loss : 0.019246, loss_ce: 0.006558
2022-01-08 01:29:36,937 iteration 4915 : loss : 0.023511, loss_ce: 0.009572
2022-01-08 01:29:38,347 iteration 4916 : loss : 0.018466, loss_ce: 0.005385
2022-01-08 01:29:39,738 iteration 4917 : loss : 0.021967, loss_ce: 0.009859
2022-01-08 01:29:41,089 iteration 4918 : loss : 0.025532, loss_ce: 0.011361
2022-01-08 01:29:42,431 iteration 4919 : loss : 0.027694, loss_ce: 0.010976
2022-01-08 01:29:43,840 iteration 4920 : loss : 0.023350, loss_ce: 0.009892
2022-01-08 01:29:45,208 iteration 4921 : loss : 0.017095, loss_ce: 0.003724
2022-01-08 01:29:46,614 iteration 4922 : loss : 0.035227, loss_ce: 0.006172
2022-01-08 01:29:48,027 iteration 4923 : loss : 0.031316, loss_ce: 0.012292
2022-01-08 01:29:49,442 iteration 4924 : loss : 0.021988, loss_ce: 0.007727
2022-01-08 01:29:50,958 iteration 4925 : loss : 0.020073, loss_ce: 0.008453
2022-01-08 01:29:52,353 iteration 4926 : loss : 0.018524, loss_ce: 0.006665
2022-01-08 01:29:53,807 iteration 4927 : loss : 0.018085, loss_ce: 0.007159
2022-01-08 01:29:55,273 iteration 4928 : loss : 0.033626, loss_ce: 0.018295
2022-01-08 01:29:56,724 iteration 4929 : loss : 0.032177, loss_ce: 0.009648
2022-01-08 01:29:56,725 Training Data Eval:
2022-01-08 01:30:03,787   Average segmentation loss on training set: 0.0135
2022-01-08 01:30:03,788 Validation Data Eval:
2022-01-08 01:30:06,186   Average segmentation loss on validation set: 0.0709
2022-01-08 01:30:07,587 iteration 4930 : loss : 0.017437, loss_ce: 0.006972
 72%|█████████████████████        | 290/400 [2:05:31<49:58, 27.26s/it]2022-01-08 01:30:09,060 iteration 4931 : loss : 0.025612, loss_ce: 0.012424
2022-01-08 01:30:10,443 iteration 4932 : loss : 0.019562, loss_ce: 0.007687
2022-01-08 01:30:11,802 iteration 4933 : loss : 0.021656, loss_ce: 0.007579
2022-01-08 01:30:13,206 iteration 4934 : loss : 0.028581, loss_ce: 0.012246
2022-01-08 01:30:14,634 iteration 4935 : loss : 0.029517, loss_ce: 0.009782
2022-01-08 01:30:16,039 iteration 4936 : loss : 0.018708, loss_ce: 0.008107
2022-01-08 01:30:17,476 iteration 4937 : loss : 0.019603, loss_ce: 0.007210
2022-01-08 01:30:18,869 iteration 4938 : loss : 0.027354, loss_ce: 0.011680
2022-01-08 01:30:20,322 iteration 4939 : loss : 0.020534, loss_ce: 0.008273
2022-01-08 01:30:21,677 iteration 4940 : loss : 0.023518, loss_ce: 0.006534
2022-01-08 01:30:23,095 iteration 4941 : loss : 0.016932, loss_ce: 0.006066
2022-01-08 01:30:24,530 iteration 4942 : loss : 0.027399, loss_ce: 0.010748
2022-01-08 01:30:25,889 iteration 4943 : loss : 0.015831, loss_ce: 0.004938
2022-01-08 01:30:27,282 iteration 4944 : loss : 0.047688, loss_ce: 0.020161
2022-01-08 01:30:28,796 iteration 4945 : loss : 0.025277, loss_ce: 0.006507
2022-01-08 01:30:30,247 iteration 4946 : loss : 0.028740, loss_ce: 0.014424
2022-01-08 01:30:31,744 iteration 4947 : loss : 0.023265, loss_ce: 0.008802
 73%|█████████████████████        | 291/400 [2:05:55<47:49, 26.33s/it]2022-01-08 01:30:33,243 iteration 4948 : loss : 0.022307, loss_ce: 0.008322
2022-01-08 01:30:34,639 iteration 4949 : loss : 0.018250, loss_ce: 0.008202
2022-01-08 01:30:36,004 iteration 4950 : loss : 0.020204, loss_ce: 0.007436
2022-01-08 01:30:37,383 iteration 4951 : loss : 0.044893, loss_ce: 0.019071
2022-01-08 01:30:38,692 iteration 4952 : loss : 0.024703, loss_ce: 0.010289
2022-01-08 01:30:40,123 iteration 4953 : loss : 0.017933, loss_ce: 0.007765
2022-01-08 01:30:41,477 iteration 4954 : loss : 0.025475, loss_ce: 0.009767
2022-01-08 01:30:42,848 iteration 4955 : loss : 0.013874, loss_ce: 0.005394
2022-01-08 01:30:44,222 iteration 4956 : loss : 0.023718, loss_ce: 0.008075
2022-01-08 01:30:45,604 iteration 4957 : loss : 0.018530, loss_ce: 0.007733
2022-01-08 01:30:47,028 iteration 4958 : loss : 0.020731, loss_ce: 0.009688
2022-01-08 01:30:48,336 iteration 4959 : loss : 0.019965, loss_ce: 0.007053
2022-01-08 01:30:49,719 iteration 4960 : loss : 0.025183, loss_ce: 0.008697
2022-01-08 01:30:51,062 iteration 4961 : loss : 0.023617, loss_ce: 0.009581
2022-01-08 01:30:52,497 iteration 4962 : loss : 0.024884, loss_ce: 0.010233
2022-01-08 01:30:53,852 iteration 4963 : loss : 0.019563, loss_ce: 0.007772
2022-01-08 01:30:55,320 iteration 4964 : loss : 0.029024, loss_ce: 0.010265
 73%|█████████████████████▏       | 292/400 [2:06:19<45:54, 25.50s/it]2022-01-08 01:30:56,821 iteration 4965 : loss : 0.024495, loss_ce: 0.007724
2022-01-08 01:30:58,336 iteration 4966 : loss : 0.029927, loss_ce: 0.008906
2022-01-08 01:30:59,694 iteration 4967 : loss : 0.018043, loss_ce: 0.007468
2022-01-08 01:31:01,093 iteration 4968 : loss : 0.024509, loss_ce: 0.010352
2022-01-08 01:31:02,463 iteration 4969 : loss : 0.039005, loss_ce: 0.009611
2022-01-08 01:31:03,814 iteration 4970 : loss : 0.014672, loss_ce: 0.005732
2022-01-08 01:31:05,281 iteration 4971 : loss : 0.022038, loss_ce: 0.008851
2022-01-08 01:31:06,739 iteration 4972 : loss : 0.023501, loss_ce: 0.007571
2022-01-08 01:31:08,111 iteration 4973 : loss : 0.032630, loss_ce: 0.012529
2022-01-08 01:31:09,479 iteration 4974 : loss : 0.018348, loss_ce: 0.007072
2022-01-08 01:31:10,861 iteration 4975 : loss : 0.034343, loss_ce: 0.011899
2022-01-08 01:31:12,231 iteration 4976 : loss : 0.051580, loss_ce: 0.020412
2022-01-08 01:31:13,712 iteration 4977 : loss : 0.027462, loss_ce: 0.013879
2022-01-08 01:31:15,102 iteration 4978 : loss : 0.072749, loss_ce: 0.026110
2022-01-08 01:31:16,508 iteration 4979 : loss : 0.025073, loss_ce: 0.014101
2022-01-08 01:31:17,826 iteration 4980 : loss : 0.021073, loss_ce: 0.008407
2022-01-08 01:31:19,218 iteration 4981 : loss : 0.024532, loss_ce: 0.008516
 73%|█████████████████████▏       | 293/400 [2:06:43<44:36, 25.02s/it]2022-01-08 01:31:20,640 iteration 4982 : loss : 0.026661, loss_ce: 0.009452
2022-01-08 01:31:22,002 iteration 4983 : loss : 0.032264, loss_ce: 0.010742
2022-01-08 01:31:23,489 iteration 4984 : loss : 0.029674, loss_ce: 0.010889
2022-01-08 01:31:24,915 iteration 4985 : loss : 0.026148, loss_ce: 0.009556
2022-01-08 01:31:26,398 iteration 4986 : loss : 0.033871, loss_ce: 0.016956
2022-01-08 01:31:27,773 iteration 4987 : loss : 0.019537, loss_ce: 0.008438
2022-01-08 01:31:29,176 iteration 4988 : loss : 0.022431, loss_ce: 0.009338
2022-01-08 01:31:30,572 iteration 4989 : loss : 0.022936, loss_ce: 0.007869
2022-01-08 01:31:32,055 iteration 4990 : loss : 0.024395, loss_ce: 0.010027
2022-01-08 01:31:33,462 iteration 4991 : loss : 0.020554, loss_ce: 0.005851
2022-01-08 01:31:34,882 iteration 4992 : loss : 0.019213, loss_ce: 0.007911
2022-01-08 01:31:36,344 iteration 4993 : loss : 0.022007, loss_ce: 0.010060
2022-01-08 01:31:37,828 iteration 4994 : loss : 0.039635, loss_ce: 0.009956
2022-01-08 01:31:39,227 iteration 4995 : loss : 0.023182, loss_ce: 0.008417
2022-01-08 01:31:40,622 iteration 4996 : loss : 0.032514, loss_ce: 0.011781
2022-01-08 01:31:42,125 iteration 4997 : loss : 0.030170, loss_ce: 0.012880
2022-01-08 01:31:43,474 iteration 4998 : loss : 0.019505, loss_ce: 0.006973
 74%|█████████████████████▎       | 294/400 [2:07:07<43:48, 24.79s/it]2022-01-08 01:31:44,947 iteration 4999 : loss : 0.027115, loss_ce: 0.010941
2022-01-08 01:31:46,284 iteration 5000 : loss : 0.020216, loss_ce: 0.008076
2022-01-08 01:31:47,671 iteration 5001 : loss : 0.028287, loss_ce: 0.009282
2022-01-08 01:31:49,093 iteration 5002 : loss : 0.023612, loss_ce: 0.010490
2022-01-08 01:31:50,460 iteration 5003 : loss : 0.025025, loss_ce: 0.007477
2022-01-08 01:31:51,923 iteration 5004 : loss : 0.037084, loss_ce: 0.012943
2022-01-08 01:31:53,304 iteration 5005 : loss : 0.025943, loss_ce: 0.009822
2022-01-08 01:31:54,735 iteration 5006 : loss : 0.018947, loss_ce: 0.005664
2022-01-08 01:31:56,101 iteration 5007 : loss : 0.020046, loss_ce: 0.007298
2022-01-08 01:31:57,429 iteration 5008 : loss : 0.020647, loss_ce: 0.007035
2022-01-08 01:31:58,821 iteration 5009 : loss : 0.030122, loss_ce: 0.010560
2022-01-08 01:32:00,318 iteration 5010 : loss : 0.039824, loss_ce: 0.015063
2022-01-08 01:32:01,726 iteration 5011 : loss : 0.023129, loss_ce: 0.013534
2022-01-08 01:32:03,183 iteration 5012 : loss : 0.031640, loss_ce: 0.014328
2022-01-08 01:32:04,744 iteration 5013 : loss : 0.026690, loss_ce: 0.012062
2022-01-08 01:32:06,068 iteration 5014 : loss : 0.017796, loss_ce: 0.006023
2022-01-08 01:32:06,068 Training Data Eval:
2022-01-08 01:32:13,098   Average segmentation loss on training set: 0.0142
2022-01-08 01:32:13,099 Validation Data Eval:
2022-01-08 01:32:15,489   Average segmentation loss on validation set: 0.0698
2022-01-08 01:32:16,863 iteration 5015 : loss : 0.016305, loss_ce: 0.003287
 74%|█████████████████████▍       | 295/400 [2:07:41<47:53, 27.37s/it]2022-01-08 01:32:18,264 iteration 5016 : loss : 0.021566, loss_ce: 0.008476
2022-01-08 01:32:19,628 iteration 5017 : loss : 0.020041, loss_ce: 0.006011
2022-01-08 01:32:21,171 iteration 5018 : loss : 0.025172, loss_ce: 0.008991
2022-01-08 01:32:22,527 iteration 5019 : loss : 0.024122, loss_ce: 0.006298
2022-01-08 01:32:23,981 iteration 5020 : loss : 0.018931, loss_ce: 0.007960
2022-01-08 01:32:25,450 iteration 5021 : loss : 0.023515, loss_ce: 0.008089
2022-01-08 01:32:26,904 iteration 5022 : loss : 0.024331, loss_ce: 0.010805
2022-01-08 01:32:28,331 iteration 5023 : loss : 0.020793, loss_ce: 0.009138
2022-01-08 01:32:29,726 iteration 5024 : loss : 0.014502, loss_ce: 0.005339
2022-01-08 01:32:31,082 iteration 5025 : loss : 0.018124, loss_ce: 0.006957
2022-01-08 01:32:32,484 iteration 5026 : loss : 0.024503, loss_ce: 0.009835
2022-01-08 01:32:33,879 iteration 5027 : loss : 0.019320, loss_ce: 0.007268
2022-01-08 01:32:35,313 iteration 5028 : loss : 0.021731, loss_ce: 0.007661
2022-01-08 01:32:36,685 iteration 5029 : loss : 0.019879, loss_ce: 0.008068
2022-01-08 01:32:38,099 iteration 5030 : loss : 0.031309, loss_ce: 0.012722
2022-01-08 01:32:39,593 iteration 5031 : loss : 0.025085, loss_ce: 0.010195
2022-01-08 01:32:41,086 iteration 5032 : loss : 0.026596, loss_ce: 0.009201
 74%|█████████████████████▍       | 296/400 [2:08:05<45:48, 26.43s/it]2022-01-08 01:32:42,561 iteration 5033 : loss : 0.024328, loss_ce: 0.012521
2022-01-08 01:32:43,958 iteration 5034 : loss : 0.025187, loss_ce: 0.006215
2022-01-08 01:32:45,336 iteration 5035 : loss : 0.020040, loss_ce: 0.006775
2022-01-08 01:32:46,666 iteration 5036 : loss : 0.016223, loss_ce: 0.005496
2022-01-08 01:32:48,055 iteration 5037 : loss : 0.016197, loss_ce: 0.005924
2022-01-08 01:32:49,581 iteration 5038 : loss : 0.025339, loss_ce: 0.011192
2022-01-08 01:32:50,955 iteration 5039 : loss : 0.019287, loss_ce: 0.006070
2022-01-08 01:32:52,389 iteration 5040 : loss : 0.023218, loss_ce: 0.010036
2022-01-08 01:32:53,811 iteration 5041 : loss : 0.018756, loss_ce: 0.006916
2022-01-08 01:32:55,201 iteration 5042 : loss : 0.017695, loss_ce: 0.006811
2022-01-08 01:32:56,559 iteration 5043 : loss : 0.024257, loss_ce: 0.008237
2022-01-08 01:32:57,958 iteration 5044 : loss : 0.016914, loss_ce: 0.005903
2022-01-08 01:32:59,273 iteration 5045 : loss : 0.020881, loss_ce: 0.006963
2022-01-08 01:33:00,677 iteration 5046 : loss : 0.019785, loss_ce: 0.006424
2022-01-08 01:33:02,036 iteration 5047 : loss : 0.021073, loss_ce: 0.008153
2022-01-08 01:33:03,373 iteration 5048 : loss : 0.021109, loss_ce: 0.008145
2022-01-08 01:33:04,835 iteration 5049 : loss : 0.042072, loss_ce: 0.022053
 74%|█████████████████████▌       | 297/400 [2:08:29<43:59, 25.62s/it]2022-01-08 01:33:06,263 iteration 5050 : loss : 0.025525, loss_ce: 0.006463
2022-01-08 01:33:07,598 iteration 5051 : loss : 0.018018, loss_ce: 0.007957
2022-01-08 01:33:09,025 iteration 5052 : loss : 0.018490, loss_ce: 0.008866
2022-01-08 01:33:10,365 iteration 5053 : loss : 0.018696, loss_ce: 0.007145
2022-01-08 01:33:11,803 iteration 5054 : loss : 0.020626, loss_ce: 0.009370
2022-01-08 01:33:13,161 iteration 5055 : loss : 0.016366, loss_ce: 0.005759
2022-01-08 01:33:14,527 iteration 5056 : loss : 0.018803, loss_ce: 0.005435
2022-01-08 01:33:15,905 iteration 5057 : loss : 0.021371, loss_ce: 0.010134
2022-01-08 01:33:17,281 iteration 5058 : loss : 0.031250, loss_ce: 0.013035
2022-01-08 01:33:18,692 iteration 5059 : loss : 0.018959, loss_ce: 0.008427
2022-01-08 01:33:20,134 iteration 5060 : loss : 0.030871, loss_ce: 0.008160
2022-01-08 01:33:21,531 iteration 5061 : loss : 0.017616, loss_ce: 0.008403
2022-01-08 01:33:23,006 iteration 5062 : loss : 0.018276, loss_ce: 0.006304
2022-01-08 01:33:24,385 iteration 5063 : loss : 0.018705, loss_ce: 0.007105
2022-01-08 01:33:25,787 iteration 5064 : loss : 0.031938, loss_ce: 0.009085
2022-01-08 01:33:27,245 iteration 5065 : loss : 0.032723, loss_ce: 0.010680
2022-01-08 01:33:28,531 iteration 5066 : loss : 0.016227, loss_ce: 0.007123
 74%|█████████████████████▌       | 298/400 [2:08:52<42:34, 25.04s/it]2022-01-08 01:33:30,106 iteration 5067 : loss : 0.040405, loss_ce: 0.014218
2022-01-08 01:33:31,488 iteration 5068 : loss : 0.018088, loss_ce: 0.006982
2022-01-08 01:33:32,844 iteration 5069 : loss : 0.032286, loss_ce: 0.012262
2022-01-08 01:33:34,219 iteration 5070 : loss : 0.020964, loss_ce: 0.010424
2022-01-08 01:33:35,695 iteration 5071 : loss : 0.022387, loss_ce: 0.009393
2022-01-08 01:33:37,112 iteration 5072 : loss : 0.019310, loss_ce: 0.007367
2022-01-08 01:33:38,564 iteration 5073 : loss : 0.039405, loss_ce: 0.011877
2022-01-08 01:33:40,000 iteration 5074 : loss : 0.020484, loss_ce: 0.008207
2022-01-08 01:33:41,333 iteration 5075 : loss : 0.021581, loss_ce: 0.009512
2022-01-08 01:33:42,672 iteration 5076 : loss : 0.018436, loss_ce: 0.006934
2022-01-08 01:33:44,116 iteration 5077 : loss : 0.032871, loss_ce: 0.012021
2022-01-08 01:33:45,521 iteration 5078 : loss : 0.024330, loss_ce: 0.007862
2022-01-08 01:33:46,898 iteration 5079 : loss : 0.013551, loss_ce: 0.004871
2022-01-08 01:33:48,339 iteration 5080 : loss : 0.020257, loss_ce: 0.007480
2022-01-08 01:33:49,756 iteration 5081 : loss : 0.022956, loss_ce: 0.009703
2022-01-08 01:33:51,126 iteration 5082 : loss : 0.026461, loss_ce: 0.006593
2022-01-08 01:33:52,490 iteration 5083 : loss : 0.023676, loss_ce: 0.008690
 75%|█████████████████████▋       | 299/400 [2:09:16<41:36, 24.72s/it]2022-01-08 01:33:53,962 iteration 5084 : loss : 0.025363, loss_ce: 0.012021
2022-01-08 01:33:55,453 iteration 5085 : loss : 0.021252, loss_ce: 0.009202
2022-01-08 01:33:56,774 iteration 5086 : loss : 0.016624, loss_ce: 0.005010
2022-01-08 01:33:58,180 iteration 5087 : loss : 0.031165, loss_ce: 0.009552
2022-01-08 01:33:59,526 iteration 5088 : loss : 0.019887, loss_ce: 0.009833
2022-01-08 01:34:00,906 iteration 5089 : loss : 0.019663, loss_ce: 0.007513
2022-01-08 01:34:02,324 iteration 5090 : loss : 0.029569, loss_ce: 0.012286
2022-01-08 01:34:03,751 iteration 5091 : loss : 0.029220, loss_ce: 0.010715
2022-01-08 01:34:05,144 iteration 5092 : loss : 0.024073, loss_ce: 0.005823
2022-01-08 01:34:06,570 iteration 5093 : loss : 0.019045, loss_ce: 0.006715
2022-01-08 01:34:07,986 iteration 5094 : loss : 0.017857, loss_ce: 0.005480
2022-01-08 01:34:09,397 iteration 5095 : loss : 0.018806, loss_ce: 0.007596
2022-01-08 01:34:10,870 iteration 5096 : loss : 0.020871, loss_ce: 0.005595
2022-01-08 01:34:12,342 iteration 5097 : loss : 0.037749, loss_ce: 0.022142
2022-01-08 01:34:13,738 iteration 5098 : loss : 0.017604, loss_ce: 0.007359
2022-01-08 01:34:15,173 iteration 5099 : loss : 0.022752, loss_ce: 0.007912
2022-01-08 01:34:15,173 Training Data Eval:
2022-01-08 01:34:22,214   Average segmentation loss on training set: 0.0127
2022-01-08 01:34:22,215 Validation Data Eval:
2022-01-08 01:34:24,605   Average segmentation loss on validation set: 0.0810
2022-01-08 01:34:25,963 iteration 5100 : loss : 0.021536, loss_ce: 0.007435
 75%|█████████████████████▊       | 300/400 [2:09:50<45:34, 27.35s/it]2022-01-08 01:34:27,420 iteration 5101 : loss : 0.043978, loss_ce: 0.010621
2022-01-08 01:34:28,885 iteration 5102 : loss : 0.020481, loss_ce: 0.009953
2022-01-08 01:34:30,257 iteration 5103 : loss : 0.020588, loss_ce: 0.007789
2022-01-08 01:34:31,647 iteration 5104 : loss : 0.027533, loss_ce: 0.007649
2022-01-08 01:34:33,014 iteration 5105 : loss : 0.027803, loss_ce: 0.009302
2022-01-08 01:34:34,406 iteration 5106 : loss : 0.029516, loss_ce: 0.011414
2022-01-08 01:34:35,845 iteration 5107 : loss : 0.024015, loss_ce: 0.009113
2022-01-08 01:34:37,269 iteration 5108 : loss : 0.024232, loss_ce: 0.009441
2022-01-08 01:34:38,660 iteration 5109 : loss : 0.022219, loss_ce: 0.010689
2022-01-08 01:34:39,958 iteration 5110 : loss : 0.018467, loss_ce: 0.007168
2022-01-08 01:34:41,372 iteration 5111 : loss : 0.021937, loss_ce: 0.006081
2022-01-08 01:34:42,711 iteration 5112 : loss : 0.015426, loss_ce: 0.006255
2022-01-08 01:34:44,175 iteration 5113 : loss : 0.024850, loss_ce: 0.008374
2022-01-08 01:34:45,520 iteration 5114 : loss : 0.014120, loss_ce: 0.005419
2022-01-08 01:34:46,876 iteration 5115 : loss : 0.016659, loss_ce: 0.008028
2022-01-08 01:34:48,305 iteration 5116 : loss : 0.021611, loss_ce: 0.008262
2022-01-08 01:34:49,712 iteration 5117 : loss : 0.021877, loss_ce: 0.007916
 75%|█████████████████████▊       | 301/400 [2:10:13<43:20, 26.26s/it]2022-01-08 01:34:51,158 iteration 5118 : loss : 0.022425, loss_ce: 0.009151
2022-01-08 01:34:52,558 iteration 5119 : loss : 0.025809, loss_ce: 0.009422
2022-01-08 01:34:54,074 iteration 5120 : loss : 0.017405, loss_ce: 0.004723
2022-01-08 01:34:55,434 iteration 5121 : loss : 0.019406, loss_ce: 0.007352
2022-01-08 01:34:56,803 iteration 5122 : loss : 0.019571, loss_ce: 0.006142
2022-01-08 01:34:58,193 iteration 5123 : loss : 0.017669, loss_ce: 0.009054
2022-01-08 01:34:59,548 iteration 5124 : loss : 0.034266, loss_ce: 0.012005
2022-01-08 01:35:00,975 iteration 5125 : loss : 0.021228, loss_ce: 0.009622
2022-01-08 01:35:02,347 iteration 5126 : loss : 0.020557, loss_ce: 0.007049
2022-01-08 01:35:03,763 iteration 5127 : loss : 0.020910, loss_ce: 0.007392
2022-01-08 01:35:05,267 iteration 5128 : loss : 0.022980, loss_ce: 0.007784
2022-01-08 01:35:06,772 iteration 5129 : loss : 0.025062, loss_ce: 0.009425
2022-01-08 01:35:08,108 iteration 5130 : loss : 0.024643, loss_ce: 0.011878
2022-01-08 01:35:09,529 iteration 5131 : loss : 0.017553, loss_ce: 0.005020
2022-01-08 01:35:10,943 iteration 5132 : loss : 0.019658, loss_ce: 0.008628
2022-01-08 01:35:12,319 iteration 5133 : loss : 0.026611, loss_ce: 0.010502
2022-01-08 01:35:13,677 iteration 5134 : loss : 0.019182, loss_ce: 0.007357
 76%|█████████████████████▉       | 302/400 [2:10:37<41:46, 25.58s/it]2022-01-08 01:35:15,100 iteration 5135 : loss : 0.018602, loss_ce: 0.006769
2022-01-08 01:35:16,578 iteration 5136 : loss : 0.017898, loss_ce: 0.006477
2022-01-08 01:35:18,082 iteration 5137 : loss : 0.031489, loss_ce: 0.009192
2022-01-08 01:35:19,581 iteration 5138 : loss : 0.038052, loss_ce: 0.019272
2022-01-08 01:35:20,912 iteration 5139 : loss : 0.015599, loss_ce: 0.004918
2022-01-08 01:35:22,362 iteration 5140 : loss : 0.024897, loss_ce: 0.012235
2022-01-08 01:35:23,705 iteration 5141 : loss : 0.018841, loss_ce: 0.009649
2022-01-08 01:35:25,224 iteration 5142 : loss : 0.030105, loss_ce: 0.011374
2022-01-08 01:35:26,565 iteration 5143 : loss : 0.014864, loss_ce: 0.005303
2022-01-08 01:35:27,898 iteration 5144 : loss : 0.015952, loss_ce: 0.007102
2022-01-08 01:35:29,316 iteration 5145 : loss : 0.027484, loss_ce: 0.011675
2022-01-08 01:35:30,889 iteration 5146 : loss : 0.029802, loss_ce: 0.008047
2022-01-08 01:35:32,302 iteration 5147 : loss : 0.016575, loss_ce: 0.007760
2022-01-08 01:35:33,728 iteration 5148 : loss : 0.019356, loss_ce: 0.006624
2022-01-08 01:35:35,095 iteration 5149 : loss : 0.020288, loss_ce: 0.007584
2022-01-08 01:35:36,486 iteration 5150 : loss : 0.022191, loss_ce: 0.009859
2022-01-08 01:35:37,835 iteration 5151 : loss : 0.020346, loss_ce: 0.006282
 76%|█████████████████████▉       | 303/400 [2:11:02<40:39, 25.15s/it]2022-01-08 01:35:39,309 iteration 5152 : loss : 0.027241, loss_ce: 0.006679
2022-01-08 01:35:40,641 iteration 5153 : loss : 0.014780, loss_ce: 0.007171
2022-01-08 01:35:42,110 iteration 5154 : loss : 0.021586, loss_ce: 0.007684
2022-01-08 01:35:43,445 iteration 5155 : loss : 0.012425, loss_ce: 0.004250
2022-01-08 01:35:44,818 iteration 5156 : loss : 0.021498, loss_ce: 0.008962
2022-01-08 01:35:46,268 iteration 5157 : loss : 0.019704, loss_ce: 0.007489
2022-01-08 01:35:47,688 iteration 5158 : loss : 0.024232, loss_ce: 0.008017
2022-01-08 01:35:49,134 iteration 5159 : loss : 0.030055, loss_ce: 0.009583
2022-01-08 01:35:50,593 iteration 5160 : loss : 0.021372, loss_ce: 0.008715
2022-01-08 01:35:52,042 iteration 5161 : loss : 0.020772, loss_ce: 0.007648
2022-01-08 01:35:53,406 iteration 5162 : loss : 0.016374, loss_ce: 0.005140
2022-01-08 01:35:54,810 iteration 5163 : loss : 0.023008, loss_ce: 0.007634
2022-01-08 01:35:56,143 iteration 5164 : loss : 0.018453, loss_ce: 0.009670
2022-01-08 01:35:57,538 iteration 5165 : loss : 0.019191, loss_ce: 0.005215
2022-01-08 01:35:58,942 iteration 5166 : loss : 0.016926, loss_ce: 0.007802
2022-01-08 01:36:00,307 iteration 5167 : loss : 0.028102, loss_ce: 0.012900
2022-01-08 01:36:01,774 iteration 5168 : loss : 0.027742, loss_ce: 0.011083
 76%|██████████████████████       | 304/400 [2:11:25<39:39, 24.79s/it]2022-01-08 01:36:03,169 iteration 5169 : loss : 0.018088, loss_ce: 0.006971
2022-01-08 01:36:04,571 iteration 5170 : loss : 0.023666, loss_ce: 0.011040
2022-01-08 01:36:05,939 iteration 5171 : loss : 0.022306, loss_ce: 0.005827
2022-01-08 01:36:07,345 iteration 5172 : loss : 0.020575, loss_ce: 0.006969
2022-01-08 01:36:08,708 iteration 5173 : loss : 0.027794, loss_ce: 0.012807
2022-01-08 01:36:10,131 iteration 5174 : loss : 0.023502, loss_ce: 0.008350
2022-01-08 01:36:11,611 iteration 5175 : loss : 0.031973, loss_ce: 0.011652
2022-01-08 01:36:13,035 iteration 5176 : loss : 0.031182, loss_ce: 0.008858
2022-01-08 01:36:14,559 iteration 5177 : loss : 0.041956, loss_ce: 0.023078
2022-01-08 01:36:15,944 iteration 5178 : loss : 0.026810, loss_ce: 0.005199
2022-01-08 01:36:17,399 iteration 5179 : loss : 0.016554, loss_ce: 0.006963
2022-01-08 01:36:18,835 iteration 5180 : loss : 0.019750, loss_ce: 0.008126
2022-01-08 01:36:20,280 iteration 5181 : loss : 0.030031, loss_ce: 0.012310
2022-01-08 01:36:21,734 iteration 5182 : loss : 0.026111, loss_ce: 0.008302
2022-01-08 01:36:23,190 iteration 5183 : loss : 0.025475, loss_ce: 0.008406
2022-01-08 01:36:24,723 iteration 5184 : loss : 0.020065, loss_ce: 0.010140
2022-01-08 01:36:24,723 Training Data Eval:
2022-01-08 01:36:31,763   Average segmentation loss on training set: 0.0130
2022-01-08 01:36:31,764 Validation Data Eval:
2022-01-08 01:36:34,149   Average segmentation loss on validation set: 0.0755
2022-01-08 01:36:35,559 iteration 5185 : loss : 0.021246, loss_ce: 0.009269
 76%|██████████████████████       | 305/400 [2:11:59<43:31, 27.49s/it]2022-01-08 01:36:37,033 iteration 5186 : loss : 0.020630, loss_ce: 0.009696
2022-01-08 01:36:38,482 iteration 5187 : loss : 0.018278, loss_ce: 0.007519
2022-01-08 01:36:39,814 iteration 5188 : loss : 0.021225, loss_ce: 0.006808
2022-01-08 01:36:41,216 iteration 5189 : loss : 0.022558, loss_ce: 0.010135
2022-01-08 01:36:42,556 iteration 5190 : loss : 0.018690, loss_ce: 0.008424
2022-01-08 01:36:43,921 iteration 5191 : loss : 0.021459, loss_ce: 0.007966
2022-01-08 01:36:45,363 iteration 5192 : loss : 0.058176, loss_ce: 0.015300
2022-01-08 01:36:46,779 iteration 5193 : loss : 0.022424, loss_ce: 0.010061
2022-01-08 01:36:48,191 iteration 5194 : loss : 0.027610, loss_ce: 0.010435
2022-01-08 01:36:49,632 iteration 5195 : loss : 0.023341, loss_ce: 0.007361
2022-01-08 01:36:51,156 iteration 5196 : loss : 0.026888, loss_ce: 0.006570
2022-01-08 01:36:52,466 iteration 5197 : loss : 0.022543, loss_ce: 0.006264
2022-01-08 01:36:53,912 iteration 5198 : loss : 0.031645, loss_ce: 0.011384
2022-01-08 01:36:55,362 iteration 5199 : loss : 0.020090, loss_ce: 0.007026
2022-01-08 01:36:56,788 iteration 5200 : loss : 0.018040, loss_ce: 0.005462
2022-01-08 01:36:58,163 iteration 5201 : loss : 0.017626, loss_ce: 0.005309
2022-01-08 01:36:59,561 iteration 5202 : loss : 0.021919, loss_ce: 0.008767
 76%|██████████████████████▏      | 306/400 [2:12:23<41:25, 26.44s/it]2022-01-08 01:37:00,922 iteration 5203 : loss : 0.015853, loss_ce: 0.004644
2022-01-08 01:37:02,352 iteration 5204 : loss : 0.021817, loss_ce: 0.007418
2022-01-08 01:37:03,765 iteration 5205 : loss : 0.018375, loss_ce: 0.006892
2022-01-08 01:37:05,100 iteration 5206 : loss : 0.015626, loss_ce: 0.006370
2022-01-08 01:37:06,501 iteration 5207 : loss : 0.016054, loss_ce: 0.005920
2022-01-08 01:37:07,969 iteration 5208 : loss : 0.025915, loss_ce: 0.010590
2022-01-08 01:37:09,384 iteration 5209 : loss : 0.022766, loss_ce: 0.008765
2022-01-08 01:37:10,764 iteration 5210 : loss : 0.019348, loss_ce: 0.006659
2022-01-08 01:37:12,126 iteration 5211 : loss : 0.017002, loss_ce: 0.004834
2022-01-08 01:37:13,549 iteration 5212 : loss : 0.016676, loss_ce: 0.005089
2022-01-08 01:37:14,975 iteration 5213 : loss : 0.033965, loss_ce: 0.012478
2022-01-08 01:37:16,362 iteration 5214 : loss : 0.026483, loss_ce: 0.016657
2022-01-08 01:37:17,732 iteration 5215 : loss : 0.023326, loss_ce: 0.008397
2022-01-08 01:37:19,138 iteration 5216 : loss : 0.017452, loss_ce: 0.006760
2022-01-08 01:37:20,480 iteration 5217 : loss : 0.019638, loss_ce: 0.007779
2022-01-08 01:37:21,856 iteration 5218 : loss : 0.018597, loss_ce: 0.005470
2022-01-08 01:37:23,268 iteration 5219 : loss : 0.021879, loss_ce: 0.009836
 77%|██████████████████████▎      | 307/400 [2:12:47<39:42, 25.62s/it]2022-01-08 01:37:24,806 iteration 5220 : loss : 0.038283, loss_ce: 0.017709
2022-01-08 01:37:26,201 iteration 5221 : loss : 0.015164, loss_ce: 0.005943
2022-01-08 01:37:27,515 iteration 5222 : loss : 0.015998, loss_ce: 0.004437
2022-01-08 01:37:28,888 iteration 5223 : loss : 0.023566, loss_ce: 0.010783
2022-01-08 01:37:30,366 iteration 5224 : loss : 0.024308, loss_ce: 0.009646
2022-01-08 01:37:31,695 iteration 5225 : loss : 0.016307, loss_ce: 0.004460
2022-01-08 01:37:33,211 iteration 5226 : loss : 0.031958, loss_ce: 0.012953
2022-01-08 01:37:34,519 iteration 5227 : loss : 0.015484, loss_ce: 0.005619
2022-01-08 01:37:35,833 iteration 5228 : loss : 0.014561, loss_ce: 0.006083
2022-01-08 01:37:37,199 iteration 5229 : loss : 0.030445, loss_ce: 0.012678
2022-01-08 01:37:38,613 iteration 5230 : loss : 0.020637, loss_ce: 0.009797
2022-01-08 01:37:40,032 iteration 5231 : loss : 0.029696, loss_ce: 0.009792
2022-01-08 01:37:41,387 iteration 5232 : loss : 0.017924, loss_ce: 0.007842
2022-01-08 01:37:42,841 iteration 5233 : loss : 0.026021, loss_ce: 0.010621
2022-01-08 01:37:44,159 iteration 5234 : loss : 0.013923, loss_ce: 0.004853
2022-01-08 01:37:45,637 iteration 5235 : loss : 0.025547, loss_ce: 0.010393
2022-01-08 01:37:47,019 iteration 5236 : loss : 0.016898, loss_ce: 0.006373
 77%|██████████████████████▎      | 308/400 [2:13:11<38:25, 25.06s/it]2022-01-08 01:37:48,508 iteration 5237 : loss : 0.025018, loss_ce: 0.006922
2022-01-08 01:37:49,914 iteration 5238 : loss : 0.018714, loss_ce: 0.008309
2022-01-08 01:37:51,313 iteration 5239 : loss : 0.021657, loss_ce: 0.005409
2022-01-08 01:37:52,804 iteration 5240 : loss : 0.020531, loss_ce: 0.007510
2022-01-08 01:37:54,251 iteration 5241 : loss : 0.019154, loss_ce: 0.008480
2022-01-08 01:37:55,690 iteration 5242 : loss : 0.022506, loss_ce: 0.008560
2022-01-08 01:37:57,102 iteration 5243 : loss : 0.018972, loss_ce: 0.006065
2022-01-08 01:37:58,531 iteration 5244 : loss : 0.033722, loss_ce: 0.011977
2022-01-08 01:37:59,936 iteration 5245 : loss : 0.022163, loss_ce: 0.007939
2022-01-08 01:38:01,236 iteration 5246 : loss : 0.013682, loss_ce: 0.006033
2022-01-08 01:38:02,628 iteration 5247 : loss : 0.017778, loss_ce: 0.006072
2022-01-08 01:38:04,058 iteration 5248 : loss : 0.017960, loss_ce: 0.006707
2022-01-08 01:38:05,435 iteration 5249 : loss : 0.021255, loss_ce: 0.008978
2022-01-08 01:38:06,854 iteration 5250 : loss : 0.023467, loss_ce: 0.009698
2022-01-08 01:38:08,303 iteration 5251 : loss : 0.015746, loss_ce: 0.005803
2022-01-08 01:38:09,688 iteration 5252 : loss : 0.021355, loss_ce: 0.006994
2022-01-08 01:38:11,115 iteration 5253 : loss : 0.017418, loss_ce: 0.007024
 77%|██████████████████████▍      | 309/400 [2:13:35<37:34, 24.77s/it]2022-01-08 01:38:12,495 iteration 5254 : loss : 0.015792, loss_ce: 0.005490
2022-01-08 01:38:13,964 iteration 5255 : loss : 0.020899, loss_ce: 0.006992
2022-01-08 01:38:15,380 iteration 5256 : loss : 0.016355, loss_ce: 0.007165
2022-01-08 01:38:16,864 iteration 5257 : loss : 0.024445, loss_ce: 0.007263
2022-01-08 01:38:18,218 iteration 5258 : loss : 0.020421, loss_ce: 0.008317
2022-01-08 01:38:19,671 iteration 5259 : loss : 0.020472, loss_ce: 0.005666
2022-01-08 01:38:21,057 iteration 5260 : loss : 0.017175, loss_ce: 0.006942
2022-01-08 01:38:22,445 iteration 5261 : loss : 0.015905, loss_ce: 0.006687
2022-01-08 01:38:23,777 iteration 5262 : loss : 0.012799, loss_ce: 0.005916
2022-01-08 01:38:25,202 iteration 5263 : loss : 0.031865, loss_ce: 0.015756
2022-01-08 01:38:26,568 iteration 5264 : loss : 0.017438, loss_ce: 0.006463
2022-01-08 01:38:27,883 iteration 5265 : loss : 0.015194, loss_ce: 0.005228
2022-01-08 01:38:29,280 iteration 5266 : loss : 0.018363, loss_ce: 0.007569
2022-01-08 01:38:30,632 iteration 5267 : loss : 0.030239, loss_ce: 0.011797
2022-01-08 01:38:32,120 iteration 5268 : loss : 0.025354, loss_ce: 0.009814
2022-01-08 01:38:33,589 iteration 5269 : loss : 0.024955, loss_ce: 0.012516
2022-01-08 01:38:33,589 Training Data Eval:
2022-01-08 01:38:40,626   Average segmentation loss on training set: 0.0122
2022-01-08 01:38:40,627 Validation Data Eval:
2022-01-08 01:38:43,011   Average segmentation loss on validation set: 0.0715
2022-01-08 01:38:44,451 iteration 5270 : loss : 0.024814, loss_ce: 0.009087
 78%|██████████████████████▍      | 310/400 [2:14:08<41:00, 27.34s/it]2022-01-08 01:38:45,923 iteration 5271 : loss : 0.022564, loss_ce: 0.009812
2022-01-08 01:38:47,321 iteration 5272 : loss : 0.023566, loss_ce: 0.008947
2022-01-08 01:38:48,720 iteration 5273 : loss : 0.020870, loss_ce: 0.008667
2022-01-08 01:38:50,069 iteration 5274 : loss : 0.017512, loss_ce: 0.006497
2022-01-08 01:38:51,488 iteration 5275 : loss : 0.024314, loss_ce: 0.007392
2022-01-08 01:38:52,906 iteration 5276 : loss : 0.021235, loss_ce: 0.006352
2022-01-08 01:38:54,234 iteration 5277 : loss : 0.021160, loss_ce: 0.005780
2022-01-08 01:38:55,801 iteration 5278 : loss : 0.019228, loss_ce: 0.008154
2022-01-08 01:38:57,187 iteration 5279 : loss : 0.024447, loss_ce: 0.008930
2022-01-08 01:38:58,652 iteration 5280 : loss : 0.022006, loss_ce: 0.008545
2022-01-08 01:39:00,059 iteration 5281 : loss : 0.023253, loss_ce: 0.012609
2022-01-08 01:39:01,517 iteration 5282 : loss : 0.020310, loss_ce: 0.007394
2022-01-08 01:39:02,931 iteration 5283 : loss : 0.022081, loss_ce: 0.009095
2022-01-08 01:39:04,354 iteration 5284 : loss : 0.023403, loss_ce: 0.010432
2022-01-08 01:39:05,773 iteration 5285 : loss : 0.023017, loss_ce: 0.011634
2022-01-08 01:39:07,256 iteration 5286 : loss : 0.018641, loss_ce: 0.007371
2022-01-08 01:39:08,759 iteration 5287 : loss : 0.020631, loss_ce: 0.008515
 78%|██████████████████████▌      | 311/400 [2:14:32<39:12, 26.43s/it]2022-01-08 01:39:10,151 iteration 5288 : loss : 0.046106, loss_ce: 0.018292
2022-01-08 01:39:11,558 iteration 5289 : loss : 0.017172, loss_ce: 0.005666
2022-01-08 01:39:13,036 iteration 5290 : loss : 0.039238, loss_ce: 0.014033
2022-01-08 01:39:14,514 iteration 5291 : loss : 0.020216, loss_ce: 0.008496
2022-01-08 01:39:15,952 iteration 5292 : loss : 0.013604, loss_ce: 0.004719
2022-01-08 01:39:17,330 iteration 5293 : loss : 0.025600, loss_ce: 0.008250
2022-01-08 01:39:18,817 iteration 5294 : loss : 0.020391, loss_ce: 0.008561
2022-01-08 01:39:20,180 iteration 5295 : loss : 0.016293, loss_ce: 0.006338
2022-01-08 01:39:21,581 iteration 5296 : loss : 0.023324, loss_ce: 0.006872
2022-01-08 01:39:23,059 iteration 5297 : loss : 0.023509, loss_ce: 0.006529
2022-01-08 01:39:24,410 iteration 5298 : loss : 0.016240, loss_ce: 0.006817
2022-01-08 01:39:25,903 iteration 5299 : loss : 0.019889, loss_ce: 0.008596
2022-01-08 01:39:27,227 iteration 5300 : loss : 0.019511, loss_ce: 0.007925
2022-01-08 01:39:28,630 iteration 5301 : loss : 0.022638, loss_ce: 0.008420
2022-01-08 01:39:29,992 iteration 5302 : loss : 0.015305, loss_ce: 0.006965
2022-01-08 01:39:31,295 iteration 5303 : loss : 0.015571, loss_ce: 0.005926
2022-01-08 01:39:32,734 iteration 5304 : loss : 0.021382, loss_ce: 0.008221
 78%|██████████████████████▌      | 312/400 [2:14:56<37:41, 25.69s/it]2022-01-08 01:39:34,171 iteration 5305 : loss : 0.027754, loss_ce: 0.009114
2022-01-08 01:39:35,595 iteration 5306 : loss : 0.024067, loss_ce: 0.007997
2022-01-08 01:39:37,015 iteration 5307 : loss : 0.017909, loss_ce: 0.007341
2022-01-08 01:39:38,453 iteration 5308 : loss : 0.021716, loss_ce: 0.009300
2022-01-08 01:39:39,841 iteration 5309 : loss : 0.017527, loss_ce: 0.006478
2022-01-08 01:39:41,233 iteration 5310 : loss : 0.016857, loss_ce: 0.007523
2022-01-08 01:39:42,641 iteration 5311 : loss : 0.027010, loss_ce: 0.009301
2022-01-08 01:39:44,057 iteration 5312 : loss : 0.017424, loss_ce: 0.005435
2022-01-08 01:39:45,487 iteration 5313 : loss : 0.022950, loss_ce: 0.005960
2022-01-08 01:39:46,841 iteration 5314 : loss : 0.018540, loss_ce: 0.007997
2022-01-08 01:39:48,271 iteration 5315 : loss : 0.027638, loss_ce: 0.009688
2022-01-08 01:39:49,568 iteration 5316 : loss : 0.013721, loss_ce: 0.006279
2022-01-08 01:39:50,972 iteration 5317 : loss : 0.019054, loss_ce: 0.007090
2022-01-08 01:39:52,403 iteration 5318 : loss : 0.028868, loss_ce: 0.009245
2022-01-08 01:39:53,845 iteration 5319 : loss : 0.026250, loss_ce: 0.008944
2022-01-08 01:39:55,315 iteration 5320 : loss : 0.021117, loss_ce: 0.008582
2022-01-08 01:39:56,652 iteration 5321 : loss : 0.023242, loss_ce: 0.004780
 78%|██████████████████████▋      | 313/400 [2:15:20<36:29, 25.16s/it]2022-01-08 01:39:58,135 iteration 5322 : loss : 0.024476, loss_ce: 0.011313
2022-01-08 01:39:59,607 iteration 5323 : loss : 0.029505, loss_ce: 0.009973
2022-01-08 01:40:01,044 iteration 5324 : loss : 0.020340, loss_ce: 0.008062
2022-01-08 01:40:02,460 iteration 5325 : loss : 0.018661, loss_ce: 0.007072
2022-01-08 01:40:03,949 iteration 5326 : loss : 0.028574, loss_ce: 0.009004
2022-01-08 01:40:05,372 iteration 5327 : loss : 0.021821, loss_ce: 0.008763
2022-01-08 01:40:06,766 iteration 5328 : loss : 0.021663, loss_ce: 0.008886
2022-01-08 01:40:08,190 iteration 5329 : loss : 0.017908, loss_ce: 0.006374
2022-01-08 01:40:09,589 iteration 5330 : loss : 0.020822, loss_ce: 0.008278
2022-01-08 01:40:10,925 iteration 5331 : loss : 0.015424, loss_ce: 0.006030
2022-01-08 01:40:12,507 iteration 5332 : loss : 0.039658, loss_ce: 0.017524
2022-01-08 01:40:13,928 iteration 5333 : loss : 0.017917, loss_ce: 0.007269
2022-01-08 01:40:15,225 iteration 5334 : loss : 0.022614, loss_ce: 0.006207
2022-01-08 01:40:16,626 iteration 5335 : loss : 0.017880, loss_ce: 0.004865
2022-01-08 01:40:18,005 iteration 5336 : loss : 0.022266, loss_ce: 0.008571
2022-01-08 01:40:19,419 iteration 5337 : loss : 0.022377, loss_ce: 0.008939
2022-01-08 01:40:20,832 iteration 5338 : loss : 0.022693, loss_ce: 0.008124
 78%|██████████████████████▊      | 314/400 [2:15:45<35:38, 24.87s/it]2022-01-08 01:40:22,255 iteration 5339 : loss : 0.015124, loss_ce: 0.005666
2022-01-08 01:40:23,626 iteration 5340 : loss : 0.018105, loss_ce: 0.007283
2022-01-08 01:40:25,112 iteration 5341 : loss : 0.021133, loss_ce: 0.004890
2022-01-08 01:40:26,580 iteration 5342 : loss : 0.024685, loss_ce: 0.011812
2022-01-08 01:40:28,000 iteration 5343 : loss : 0.019317, loss_ce: 0.008021
2022-01-08 01:40:29,357 iteration 5344 : loss : 0.017040, loss_ce: 0.007564
2022-01-08 01:40:30,734 iteration 5345 : loss : 0.021101, loss_ce: 0.008216
2022-01-08 01:40:32,114 iteration 5346 : loss : 0.018086, loss_ce: 0.007057
2022-01-08 01:40:33,528 iteration 5347 : loss : 0.015424, loss_ce: 0.006093
2022-01-08 01:40:34,874 iteration 5348 : loss : 0.016597, loss_ce: 0.006515
2022-01-08 01:40:36,234 iteration 5349 : loss : 0.019308, loss_ce: 0.006271
2022-01-08 01:40:37,638 iteration 5350 : loss : 0.018407, loss_ce: 0.005422
2022-01-08 01:40:38,983 iteration 5351 : loss : 0.012784, loss_ce: 0.004718
2022-01-08 01:40:40,438 iteration 5352 : loss : 0.026711, loss_ce: 0.010359
2022-01-08 01:40:41,846 iteration 5353 : loss : 0.019149, loss_ce: 0.008069
2022-01-08 01:40:43,207 iteration 5354 : loss : 0.017358, loss_ce: 0.007647
2022-01-08 01:40:43,207 Training Data Eval:
2022-01-08 01:40:50,233   Average segmentation loss on training set: 0.0120
2022-01-08 01:40:50,233 Validation Data Eval:
2022-01-08 01:40:52,612   Average segmentation loss on validation set: 0.0701
2022-01-08 01:40:54,000 iteration 5355 : loss : 0.020824, loss_ce: 0.009370
 79%|██████████████████████▊      | 315/400 [2:16:18<38:45, 27.36s/it]2022-01-08 01:40:55,503 iteration 5356 : loss : 0.030571, loss_ce: 0.013320
2022-01-08 01:40:56,863 iteration 5357 : loss : 0.011536, loss_ce: 0.003952
2022-01-08 01:40:58,210 iteration 5358 : loss : 0.019916, loss_ce: 0.008232
2022-01-08 01:40:59,558 iteration 5359 : loss : 0.017848, loss_ce: 0.006649
2022-01-08 01:41:00,993 iteration 5360 : loss : 0.029365, loss_ce: 0.013683
2022-01-08 01:41:02,385 iteration 5361 : loss : 0.023664, loss_ce: 0.011225
2022-01-08 01:41:03,794 iteration 5362 : loss : 0.019776, loss_ce: 0.008187
2022-01-08 01:41:05,332 iteration 5363 : loss : 0.032543, loss_ce: 0.014008
2022-01-08 01:41:06,730 iteration 5364 : loss : 0.021617, loss_ce: 0.006455
2022-01-08 01:41:08,171 iteration 5365 : loss : 0.031939, loss_ce: 0.009463
2022-01-08 01:41:09,542 iteration 5366 : loss : 0.021202, loss_ce: 0.007453
2022-01-08 01:41:10,840 iteration 5367 : loss : 0.016548, loss_ce: 0.005390
2022-01-08 01:41:12,331 iteration 5368 : loss : 0.024650, loss_ce: 0.008603
2022-01-08 01:41:13,720 iteration 5369 : loss : 0.014604, loss_ce: 0.005278
2022-01-08 01:41:15,081 iteration 5370 : loss : 0.026360, loss_ce: 0.014811
2022-01-08 01:41:16,452 iteration 5371 : loss : 0.015144, loss_ce: 0.004119
2022-01-08 01:41:17,756 iteration 5372 : loss : 0.013850, loss_ce: 0.005656
 79%|██████████████████████▉      | 316/400 [2:16:41<36:47, 26.28s/it]2022-01-08 01:41:19,172 iteration 5373 : loss : 0.028692, loss_ce: 0.010133
2022-01-08 01:41:20,546 iteration 5374 : loss : 0.017615, loss_ce: 0.004782
2022-01-08 01:41:21,940 iteration 5375 : loss : 0.018541, loss_ce: 0.006745
2022-01-08 01:41:23,338 iteration 5376 : loss : 0.024268, loss_ce: 0.008925
2022-01-08 01:41:24,748 iteration 5377 : loss : 0.023406, loss_ce: 0.005619
2022-01-08 01:41:26,183 iteration 5378 : loss : 0.020320, loss_ce: 0.006408
2022-01-08 01:41:27,558 iteration 5379 : loss : 0.033871, loss_ce: 0.014207
2022-01-08 01:41:28,877 iteration 5380 : loss : 0.015108, loss_ce: 0.006690
2022-01-08 01:41:30,334 iteration 5381 : loss : 0.017294, loss_ce: 0.007207
2022-01-08 01:41:31,747 iteration 5382 : loss : 0.019656, loss_ce: 0.007247
2022-01-08 01:41:33,128 iteration 5383 : loss : 0.023832, loss_ce: 0.011953
2022-01-08 01:41:34,496 iteration 5384 : loss : 0.022439, loss_ce: 0.012851
2022-01-08 01:41:35,901 iteration 5385 : loss : 0.041932, loss_ce: 0.016530
2022-01-08 01:41:37,276 iteration 5386 : loss : 0.018903, loss_ce: 0.006731
2022-01-08 01:41:38,767 iteration 5387 : loss : 0.022371, loss_ce: 0.009015
2022-01-08 01:41:40,196 iteration 5388 : loss : 0.016762, loss_ce: 0.005112
2022-01-08 01:41:41,665 iteration 5389 : loss : 0.021514, loss_ce: 0.010046
 79%|██████████████████████▉      | 317/400 [2:17:05<35:22, 25.57s/it]2022-01-08 01:41:43,069 iteration 5390 : loss : 0.019705, loss_ce: 0.006789
2022-01-08 01:41:44,567 iteration 5391 : loss : 0.027497, loss_ce: 0.010111
2022-01-08 01:41:45,947 iteration 5392 : loss : 0.028553, loss_ce: 0.009909
2022-01-08 01:41:47,295 iteration 5393 : loss : 0.015674, loss_ce: 0.005354
2022-01-08 01:41:48,678 iteration 5394 : loss : 0.022098, loss_ce: 0.006436
2022-01-08 01:41:50,094 iteration 5395 : loss : 0.030416, loss_ce: 0.009341
2022-01-08 01:41:51,454 iteration 5396 : loss : 0.018181, loss_ce: 0.006595
2022-01-08 01:41:52,886 iteration 5397 : loss : 0.021822, loss_ce: 0.009533
2022-01-08 01:41:54,422 iteration 5398 : loss : 0.024220, loss_ce: 0.008457
2022-01-08 01:41:55,897 iteration 5399 : loss : 0.023738, loss_ce: 0.007327
2022-01-08 01:41:57,267 iteration 5400 : loss : 0.018523, loss_ce: 0.007303
2022-01-08 01:41:58,690 iteration 5401 : loss : 0.019663, loss_ce: 0.011171
2022-01-08 01:42:00,058 iteration 5402 : loss : 0.016502, loss_ce: 0.008501
2022-01-08 01:42:01,475 iteration 5403 : loss : 0.016951, loss_ce: 0.007533
2022-01-08 01:42:02,924 iteration 5404 : loss : 0.014438, loss_ce: 0.006047
2022-01-08 01:42:04,429 iteration 5405 : loss : 0.022035, loss_ce: 0.008093
2022-01-08 01:42:05,776 iteration 5406 : loss : 0.015822, loss_ce: 0.007083
 80%|███████████████████████      | 318/400 [2:17:29<34:20, 25.13s/it]2022-01-08 01:42:07,205 iteration 5407 : loss : 0.015406, loss_ce: 0.006242
2022-01-08 01:42:08,600 iteration 5408 : loss : 0.016979, loss_ce: 0.007253
2022-01-08 01:42:09,912 iteration 5409 : loss : 0.014067, loss_ce: 0.004687
2022-01-08 01:42:11,315 iteration 5410 : loss : 0.023512, loss_ce: 0.010440
2022-01-08 01:42:12,658 iteration 5411 : loss : 0.014156, loss_ce: 0.005507
2022-01-08 01:42:14,093 iteration 5412 : loss : 0.029457, loss_ce: 0.011606
2022-01-08 01:42:15,458 iteration 5413 : loss : 0.016174, loss_ce: 0.006414
2022-01-08 01:42:16,857 iteration 5414 : loss : 0.018119, loss_ce: 0.007463
2022-01-08 01:42:18,349 iteration 5415 : loss : 0.022593, loss_ce: 0.008542
2022-01-08 01:42:19,724 iteration 5416 : loss : 0.018600, loss_ce: 0.006628
2022-01-08 01:42:21,131 iteration 5417 : loss : 0.018403, loss_ce: 0.006450
2022-01-08 01:42:22,604 iteration 5418 : loss : 0.020319, loss_ce: 0.008686
2022-01-08 01:42:23,935 iteration 5419 : loss : 0.022074, loss_ce: 0.008296
2022-01-08 01:42:25,280 iteration 5420 : loss : 0.018027, loss_ce: 0.007164
2022-01-08 01:42:26,736 iteration 5421 : loss : 0.025641, loss_ce: 0.009184
2022-01-08 01:42:28,191 iteration 5422 : loss : 0.026578, loss_ce: 0.006250
2022-01-08 01:42:29,638 iteration 5423 : loss : 0.020739, loss_ce: 0.008897
 80%|███████████████████████▏     | 319/400 [2:17:53<33:24, 24.75s/it]2022-01-08 01:42:31,079 iteration 5424 : loss : 0.023298, loss_ce: 0.006976
2022-01-08 01:42:32,440 iteration 5425 : loss : 0.028747, loss_ce: 0.006222
2022-01-08 01:42:33,886 iteration 5426 : loss : 0.053392, loss_ce: 0.027482
2022-01-08 01:42:35,202 iteration 5427 : loss : 0.018944, loss_ce: 0.005778
2022-01-08 01:42:36,557 iteration 5428 : loss : 0.017049, loss_ce: 0.006094
2022-01-08 01:42:37,941 iteration 5429 : loss : 0.027611, loss_ce: 0.009167
2022-01-08 01:42:39,439 iteration 5430 : loss : 0.021706, loss_ce: 0.006732
2022-01-08 01:42:40,813 iteration 5431 : loss : 0.021894, loss_ce: 0.008961
2022-01-08 01:42:42,177 iteration 5432 : loss : 0.018249, loss_ce: 0.008182
2022-01-08 01:42:43,650 iteration 5433 : loss : 0.017722, loss_ce: 0.007713
2022-01-08 01:42:45,038 iteration 5434 : loss : 0.023038, loss_ce: 0.009071
2022-01-08 01:42:46,529 iteration 5435 : loss : 0.023428, loss_ce: 0.010304
2022-01-08 01:42:47,915 iteration 5436 : loss : 0.016026, loss_ce: 0.006371
2022-01-08 01:42:49,192 iteration 5437 : loss : 0.015514, loss_ce: 0.005578
2022-01-08 01:42:50,679 iteration 5438 : loss : 0.025591, loss_ce: 0.009839
2022-01-08 01:42:52,032 iteration 5439 : loss : 0.017419, loss_ce: 0.006746
2022-01-08 01:42:52,032 Training Data Eval:
2022-01-08 01:42:59,060   Average segmentation loss on training set: 0.0131
2022-01-08 01:42:59,061 Validation Data Eval:
2022-01-08 01:43:01,451   Average segmentation loss on validation set: 0.0678
2022-01-08 01:43:02,885 iteration 5440 : loss : 0.072873, loss_ce: 0.022747
 80%|███████████████████████▏     | 320/400 [2:18:27<36:23, 27.30s/it]2022-01-08 01:43:04,210 iteration 5441 : loss : 0.016143, loss_ce: 0.005353
2022-01-08 01:43:05,599 iteration 5442 : loss : 0.024568, loss_ce: 0.009174
2022-01-08 01:43:07,021 iteration 5443 : loss : 0.019870, loss_ce: 0.007308
2022-01-08 01:43:08,457 iteration 5444 : loss : 0.014339, loss_ce: 0.005190
2022-01-08 01:43:09,940 iteration 5445 : loss : 0.030294, loss_ce: 0.012650
2022-01-08 01:43:11,298 iteration 5446 : loss : 0.024511, loss_ce: 0.009205
2022-01-08 01:43:12,677 iteration 5447 : loss : 0.020385, loss_ce: 0.009358
2022-01-08 01:43:14,111 iteration 5448 : loss : 0.022355, loss_ce: 0.008476
2022-01-08 01:43:15,494 iteration 5449 : loss : 0.027188, loss_ce: 0.009707
2022-01-08 01:43:16,945 iteration 5450 : loss : 0.026400, loss_ce: 0.012236
2022-01-08 01:43:18,333 iteration 5451 : loss : 0.020055, loss_ce: 0.007816
2022-01-08 01:43:19,813 iteration 5452 : loss : 0.020718, loss_ce: 0.008552
2022-01-08 01:43:21,228 iteration 5453 : loss : 0.025939, loss_ce: 0.010533
2022-01-08 01:43:22,522 iteration 5454 : loss : 0.013396, loss_ce: 0.006027
2022-01-08 01:43:24,016 iteration 5455 : loss : 0.038967, loss_ce: 0.013465
2022-01-08 01:43:25,504 iteration 5456 : loss : 0.019549, loss_ce: 0.007895
2022-01-08 01:43:26,906 iteration 5457 : loss : 0.023164, loss_ce: 0.008365
 80%|███████████████████████▎     | 321/400 [2:18:51<34:39, 26.32s/it]2022-01-08 01:43:28,312 iteration 5458 : loss : 0.019669, loss_ce: 0.007955
2022-01-08 01:43:29,668 iteration 5459 : loss : 0.016138, loss_ce: 0.005893
2022-01-08 01:43:31,089 iteration 5460 : loss : 0.024566, loss_ce: 0.009335
2022-01-08 01:43:32,542 iteration 5461 : loss : 0.027916, loss_ce: 0.012540
2022-01-08 01:43:33,943 iteration 5462 : loss : 0.015402, loss_ce: 0.004145
2022-01-08 01:43:35,312 iteration 5463 : loss : 0.018174, loss_ce: 0.007653
2022-01-08 01:43:36,689 iteration 5464 : loss : 0.018689, loss_ce: 0.006658
2022-01-08 01:43:38,046 iteration 5465 : loss : 0.016700, loss_ce: 0.005317
2022-01-08 01:43:39,516 iteration 5466 : loss : 0.022112, loss_ce: 0.009180
2022-01-08 01:43:40,896 iteration 5467 : loss : 0.019341, loss_ce: 0.009551
2022-01-08 01:43:42,304 iteration 5468 : loss : 0.027827, loss_ce: 0.012993
2022-01-08 01:43:43,656 iteration 5469 : loss : 0.013755, loss_ce: 0.004662
2022-01-08 01:43:45,092 iteration 5470 : loss : 0.030564, loss_ce: 0.012253
2022-01-08 01:43:46,489 iteration 5471 : loss : 0.018217, loss_ce: 0.005618
2022-01-08 01:43:47,935 iteration 5472 : loss : 0.020986, loss_ce: 0.007854
2022-01-08 01:43:49,377 iteration 5473 : loss : 0.020851, loss_ce: 0.007132
2022-01-08 01:43:50,838 iteration 5474 : loss : 0.022744, loss_ce: 0.009181
 80%|███████████████████████▎     | 322/400 [2:19:15<33:16, 25.60s/it]2022-01-08 01:43:52,258 iteration 5475 : loss : 0.017722, loss_ce: 0.004998
2022-01-08 01:43:53,625 iteration 5476 : loss : 0.027191, loss_ce: 0.008627
2022-01-08 01:43:54,982 iteration 5477 : loss : 0.014299, loss_ce: 0.005810
2022-01-08 01:43:56,395 iteration 5478 : loss : 0.025034, loss_ce: 0.011272
2022-01-08 01:43:57,881 iteration 5479 : loss : 0.023394, loss_ce: 0.010596
2022-01-08 01:43:59,344 iteration 5480 : loss : 0.024910, loss_ce: 0.010761
2022-01-08 01:44:00,737 iteration 5481 : loss : 0.016902, loss_ce: 0.005492
2022-01-08 01:44:02,150 iteration 5482 : loss : 0.016303, loss_ce: 0.005173
2022-01-08 01:44:03,560 iteration 5483 : loss : 0.017530, loss_ce: 0.006958
2022-01-08 01:44:04,924 iteration 5484 : loss : 0.017756, loss_ce: 0.007329
2022-01-08 01:44:06,221 iteration 5485 : loss : 0.014386, loss_ce: 0.004954
2022-01-08 01:44:07,725 iteration 5486 : loss : 0.022376, loss_ce: 0.009069
2022-01-08 01:44:09,299 iteration 5487 : loss : 0.020794, loss_ce: 0.008223
2022-01-08 01:44:10,700 iteration 5488 : loss : 0.026560, loss_ce: 0.012589
2022-01-08 01:44:12,093 iteration 5489 : loss : 0.017439, loss_ce: 0.006833
2022-01-08 01:44:13,462 iteration 5490 : loss : 0.033399, loss_ce: 0.012022
2022-01-08 01:44:14,841 iteration 5491 : loss : 0.015450, loss_ce: 0.005933
 81%|███████████████████████▍     | 323/400 [2:19:39<32:14, 25.12s/it]2022-01-08 01:44:16,276 iteration 5492 : loss : 0.017790, loss_ce: 0.006752
2022-01-08 01:44:17,642 iteration 5493 : loss : 0.025293, loss_ce: 0.011062
2022-01-08 01:44:18,983 iteration 5494 : loss : 0.025788, loss_ce: 0.008116
2022-01-08 01:44:20,467 iteration 5495 : loss : 0.033296, loss_ce: 0.004923
2022-01-08 01:44:21,837 iteration 5496 : loss : 0.014675, loss_ce: 0.004656
2022-01-08 01:44:23,228 iteration 5497 : loss : 0.013660, loss_ce: 0.004207
2022-01-08 01:44:24,645 iteration 5498 : loss : 0.020089, loss_ce: 0.011368
2022-01-08 01:44:26,056 iteration 5499 : loss : 0.027463, loss_ce: 0.010316
2022-01-08 01:44:27,480 iteration 5500 : loss : 0.025104, loss_ce: 0.009611
2022-01-08 01:44:28,964 iteration 5501 : loss : 0.024242, loss_ce: 0.010941
2022-01-08 01:44:30,404 iteration 5502 : loss : 0.022438, loss_ce: 0.008987
2022-01-08 01:44:31,794 iteration 5503 : loss : 0.020981, loss_ce: 0.006834
2022-01-08 01:44:33,239 iteration 5504 : loss : 0.017868, loss_ce: 0.007678
2022-01-08 01:44:34,644 iteration 5505 : loss : 0.015882, loss_ce: 0.006618
2022-01-08 01:44:35,997 iteration 5506 : loss : 0.020377, loss_ce: 0.007958
2022-01-08 01:44:37,418 iteration 5507 : loss : 0.025524, loss_ce: 0.008227
2022-01-08 01:44:38,841 iteration 5508 : loss : 0.016490, loss_ce: 0.007586
 81%|███████████████████████▍     | 324/400 [2:20:03<31:23, 24.78s/it]2022-01-08 01:44:40,266 iteration 5509 : loss : 0.018711, loss_ce: 0.005964
2022-01-08 01:44:41,788 iteration 5510 : loss : 0.024619, loss_ce: 0.007696
2022-01-08 01:44:43,195 iteration 5511 : loss : 0.031476, loss_ce: 0.017248
2022-01-08 01:44:44,688 iteration 5512 : loss : 0.019404, loss_ce: 0.003538
2022-01-08 01:44:46,052 iteration 5513 : loss : 0.018740, loss_ce: 0.005674
2022-01-08 01:44:47,495 iteration 5514 : loss : 0.019386, loss_ce: 0.007574
2022-01-08 01:44:48,973 iteration 5515 : loss : 0.044628, loss_ce: 0.015451
2022-01-08 01:44:50,300 iteration 5516 : loss : 0.014891, loss_ce: 0.006058
2022-01-08 01:44:51,726 iteration 5517 : loss : 0.024148, loss_ce: 0.008783
2022-01-08 01:44:53,105 iteration 5518 : loss : 0.021075, loss_ce: 0.008805
2022-01-08 01:44:54,473 iteration 5519 : loss : 0.022693, loss_ce: 0.007441
2022-01-08 01:44:55,911 iteration 5520 : loss : 0.026889, loss_ce: 0.009578
2022-01-08 01:44:57,466 iteration 5521 : loss : 0.030729, loss_ce: 0.011245
2022-01-08 01:44:58,798 iteration 5522 : loss : 0.016289, loss_ce: 0.007311
2022-01-08 01:45:00,203 iteration 5523 : loss : 0.020262, loss_ce: 0.008751
2022-01-08 01:45:01,625 iteration 5524 : loss : 0.029465, loss_ce: 0.013974
2022-01-08 01:45:01,625 Training Data Eval:
2022-01-08 01:45:08,663   Average segmentation loss on training set: 0.0122
2022-01-08 01:45:08,664 Validation Data Eval:
2022-01-08 01:45:11,043   Average segmentation loss on validation set: 0.0810
2022-01-08 01:45:12,485 iteration 5525 : loss : 0.030572, loss_ce: 0.012405
 81%|███████████████████████▌     | 325/400 [2:20:36<34:18, 27.44s/it]2022-01-08 01:45:14,036 iteration 5526 : loss : 0.028325, loss_ce: 0.012421
2022-01-08 01:45:15,416 iteration 5527 : loss : 0.020920, loss_ce: 0.007502
2022-01-08 01:45:16,835 iteration 5528 : loss : 0.020661, loss_ce: 0.006632
2022-01-08 01:45:18,203 iteration 5529 : loss : 0.014367, loss_ce: 0.005119
2022-01-08 01:45:19,538 iteration 5530 : loss : 0.016247, loss_ce: 0.004179
2022-01-08 01:45:20,963 iteration 5531 : loss : 0.016133, loss_ce: 0.005823
2022-01-08 01:45:22,273 iteration 5532 : loss : 0.015213, loss_ce: 0.005055
2022-01-08 01:45:23,696 iteration 5533 : loss : 0.015840, loss_ce: 0.006861
2022-01-08 01:45:25,111 iteration 5534 : loss : 0.020179, loss_ce: 0.007316
2022-01-08 01:45:26,556 iteration 5535 : loss : 0.019597, loss_ce: 0.008061
2022-01-08 01:45:27,942 iteration 5536 : loss : 0.020184, loss_ce: 0.006119
2022-01-08 01:45:29,332 iteration 5537 : loss : 0.015636, loss_ce: 0.005044
2022-01-08 01:45:30,711 iteration 5538 : loss : 0.020195, loss_ce: 0.010368
2022-01-08 01:45:32,143 iteration 5539 : loss : 0.016544, loss_ce: 0.007074
2022-01-08 01:45:33,567 iteration 5540 : loss : 0.014002, loss_ce: 0.004637
2022-01-08 01:45:34,891 iteration 5541 : loss : 0.023522, loss_ce: 0.009896
2022-01-08 01:45:36,256 iteration 5542 : loss : 0.021667, loss_ce: 0.009557
 82%|███████████████████████▋     | 326/400 [2:21:00<32:29, 26.34s/it]2022-01-08 01:45:37,747 iteration 5543 : loss : 0.021021, loss_ce: 0.009678
2022-01-08 01:45:39,280 iteration 5544 : loss : 0.023077, loss_ce: 0.006915
2022-01-08 01:45:40,737 iteration 5545 : loss : 0.023765, loss_ce: 0.008533
2022-01-08 01:45:42,062 iteration 5546 : loss : 0.012386, loss_ce: 0.005890
2022-01-08 01:45:43,489 iteration 5547 : loss : 0.015420, loss_ce: 0.005915
2022-01-08 01:45:44,866 iteration 5548 : loss : 0.020598, loss_ce: 0.007134
2022-01-08 01:45:46,247 iteration 5549 : loss : 0.017193, loss_ce: 0.006784
2022-01-08 01:45:47,664 iteration 5550 : loss : 0.019359, loss_ce: 0.007933
2022-01-08 01:45:49,081 iteration 5551 : loss : 0.017346, loss_ce: 0.004358
2022-01-08 01:45:50,607 iteration 5552 : loss : 0.031761, loss_ce: 0.011813
2022-01-08 01:45:52,034 iteration 5553 : loss : 0.024427, loss_ce: 0.009438
2022-01-08 01:45:53,523 iteration 5554 : loss : 0.025418, loss_ce: 0.006404
2022-01-08 01:45:54,988 iteration 5555 : loss : 0.020018, loss_ce: 0.008666
2022-01-08 01:45:56,461 iteration 5556 : loss : 0.016146, loss_ce: 0.006543
2022-01-08 01:45:57,902 iteration 5557 : loss : 0.019092, loss_ce: 0.005832
2022-01-08 01:45:59,282 iteration 5558 : loss : 0.027375, loss_ce: 0.009918
2022-01-08 01:46:00,610 iteration 5559 : loss : 0.014789, loss_ce: 0.004884
 82%|███████████████████████▋     | 327/400 [2:21:24<31:19, 25.75s/it]2022-01-08 01:46:02,053 iteration 5560 : loss : 0.023570, loss_ce: 0.009491
2022-01-08 01:46:03,564 iteration 5561 : loss : 0.022236, loss_ce: 0.006212
2022-01-08 01:46:04,980 iteration 5562 : loss : 0.020250, loss_ce: 0.006567
2022-01-08 01:46:06,441 iteration 5563 : loss : 0.036529, loss_ce: 0.013437
2022-01-08 01:46:07,765 iteration 5564 : loss : 0.021719, loss_ce: 0.006255
2022-01-08 01:46:09,207 iteration 5565 : loss : 0.024582, loss_ce: 0.006187
2022-01-08 01:46:10,566 iteration 5566 : loss : 0.015940, loss_ce: 0.005046
2022-01-08 01:46:12,014 iteration 5567 : loss : 0.019538, loss_ce: 0.010972
2022-01-08 01:46:13,465 iteration 5568 : loss : 0.023215, loss_ce: 0.010436
2022-01-08 01:46:14,950 iteration 5569 : loss : 0.037175, loss_ce: 0.015563
2022-01-08 01:46:16,355 iteration 5570 : loss : 0.016414, loss_ce: 0.006999
2022-01-08 01:46:17,748 iteration 5571 : loss : 0.021506, loss_ce: 0.005587
2022-01-08 01:46:19,050 iteration 5572 : loss : 0.014733, loss_ce: 0.005036
2022-01-08 01:46:20,509 iteration 5573 : loss : 0.019818, loss_ce: 0.007941
2022-01-08 01:46:21,868 iteration 5574 : loss : 0.021478, loss_ce: 0.008727
2022-01-08 01:46:23,199 iteration 5575 : loss : 0.015579, loss_ce: 0.006065
2022-01-08 01:46:24,502 iteration 5576 : loss : 0.021660, loss_ce: 0.006092
 82%|███████████████████████▊     | 328/400 [2:21:48<30:13, 25.19s/it]2022-01-08 01:46:25,988 iteration 5577 : loss : 0.029275, loss_ce: 0.009808
2022-01-08 01:46:27,416 iteration 5578 : loss : 0.015594, loss_ce: 0.005335
2022-01-08 01:46:28,913 iteration 5579 : loss : 0.026799, loss_ce: 0.006523
2022-01-08 01:46:30,369 iteration 5580 : loss : 0.023188, loss_ce: 0.007228
2022-01-08 01:46:31,792 iteration 5581 : loss : 0.018779, loss_ce: 0.008328
2022-01-08 01:46:33,206 iteration 5582 : loss : 0.017496, loss_ce: 0.006814
2022-01-08 01:46:34,554 iteration 5583 : loss : 0.012603, loss_ce: 0.003788
2022-01-08 01:46:35,926 iteration 5584 : loss : 0.022246, loss_ce: 0.008002
2022-01-08 01:46:37,250 iteration 5585 : loss : 0.018088, loss_ce: 0.005608
2022-01-08 01:46:38,612 iteration 5586 : loss : 0.013642, loss_ce: 0.005564
2022-01-08 01:46:40,086 iteration 5587 : loss : 0.022407, loss_ce: 0.008851
2022-01-08 01:46:41,477 iteration 5588 : loss : 0.021687, loss_ce: 0.008725
2022-01-08 01:46:42,934 iteration 5589 : loss : 0.022299, loss_ce: 0.006934
2022-01-08 01:46:44,269 iteration 5590 : loss : 0.014849, loss_ce: 0.006048
2022-01-08 01:46:45,714 iteration 5591 : loss : 0.023498, loss_ce: 0.007241
2022-01-08 01:46:47,128 iteration 5592 : loss : 0.021053, loss_ce: 0.008185
2022-01-08 01:46:48,438 iteration 5593 : loss : 0.015393, loss_ce: 0.007774
 82%|███████████████████████▊     | 329/400 [2:22:12<29:21, 24.81s/it]2022-01-08 01:46:49,870 iteration 5594 : loss : 0.014852, loss_ce: 0.004705
2022-01-08 01:46:51,234 iteration 5595 : loss : 0.012496, loss_ce: 0.004697
2022-01-08 01:46:52,730 iteration 5596 : loss : 0.028929, loss_ce: 0.008507
2022-01-08 01:46:54,146 iteration 5597 : loss : 0.027637, loss_ce: 0.013718
2022-01-08 01:46:55,660 iteration 5598 : loss : 0.027644, loss_ce: 0.015007
2022-01-08 01:46:57,173 iteration 5599 : loss : 0.018066, loss_ce: 0.006288
2022-01-08 01:46:58,519 iteration 5600 : loss : 0.015623, loss_ce: 0.006082
2022-01-08 01:46:59,932 iteration 5601 : loss : 0.019224, loss_ce: 0.006995
2022-01-08 01:47:01,340 iteration 5602 : loss : 0.025193, loss_ce: 0.010440
2022-01-08 01:47:02,751 iteration 5603 : loss : 0.021770, loss_ce: 0.010635
2022-01-08 01:47:04,179 iteration 5604 : loss : 0.021651, loss_ce: 0.007117
2022-01-08 01:47:05,535 iteration 5605 : loss : 0.026199, loss_ce: 0.012643
2022-01-08 01:47:06,964 iteration 5606 : loss : 0.021498, loss_ce: 0.007862
2022-01-08 01:47:08,314 iteration 5607 : loss : 0.017859, loss_ce: 0.007595
2022-01-08 01:47:09,763 iteration 5608 : loss : 0.017583, loss_ce: 0.009296
2022-01-08 01:47:11,117 iteration 5609 : loss : 0.015948, loss_ce: 0.005668
2022-01-08 01:47:11,118 Training Data Eval:
2022-01-08 01:47:18,164   Average segmentation loss on training set: 0.0116
2022-01-08 01:47:18,165 Validation Data Eval:
2022-01-08 01:47:20,563   Average segmentation loss on validation set: 0.0666
2022-01-08 01:47:22,010 iteration 5610 : loss : 0.022440, loss_ce: 0.009492
 82%|███████████████████████▉     | 330/400 [2:22:46<32:00, 27.44s/it]2022-01-08 01:47:23,392 iteration 5611 : loss : 0.012004, loss_ce: 0.004729
2022-01-08 01:47:24,838 iteration 5612 : loss : 0.026892, loss_ce: 0.008665
2022-01-08 01:47:26,202 iteration 5613 : loss : 0.017645, loss_ce: 0.003550
2022-01-08 01:47:27,597 iteration 5614 : loss : 0.020173, loss_ce: 0.007474
2022-01-08 01:47:29,008 iteration 5615 : loss : 0.026382, loss_ce: 0.014039
2022-01-08 01:47:30,403 iteration 5616 : loss : 0.016274, loss_ce: 0.004936
2022-01-08 01:47:31,943 iteration 5617 : loss : 0.033960, loss_ce: 0.013790
2022-01-08 01:47:33,419 iteration 5618 : loss : 0.017686, loss_ce: 0.005856
2022-01-08 01:47:34,825 iteration 5619 : loss : 0.017099, loss_ce: 0.008704
2022-01-08 01:47:36,171 iteration 5620 : loss : 0.016802, loss_ce: 0.006874
2022-01-08 01:47:37,505 iteration 5621 : loss : 0.018353, loss_ce: 0.006395
2022-01-08 01:47:38,809 iteration 5622 : loss : 0.013913, loss_ce: 0.005963
2022-01-08 01:47:40,252 iteration 5623 : loss : 0.014924, loss_ce: 0.005848
2022-01-08 01:47:41,627 iteration 5624 : loss : 0.014865, loss_ce: 0.005249
2022-01-08 01:47:43,007 iteration 5625 : loss : 0.028634, loss_ce: 0.013528
2022-01-08 01:47:44,421 iteration 5626 : loss : 0.018887, loss_ce: 0.008335
2022-01-08 01:47:45,827 iteration 5627 : loss : 0.015983, loss_ce: 0.006495
 83%|███████████████████████▉     | 331/400 [2:23:10<30:18, 26.35s/it]2022-01-08 01:47:47,259 iteration 5628 : loss : 0.018531, loss_ce: 0.008301
2022-01-08 01:47:48,666 iteration 5629 : loss : 0.023124, loss_ce: 0.008909
2022-01-08 01:47:50,085 iteration 5630 : loss : 0.022098, loss_ce: 0.008363
2022-01-08 01:47:51,537 iteration 5631 : loss : 0.017486, loss_ce: 0.008944
2022-01-08 01:47:53,012 iteration 5632 : loss : 0.021054, loss_ce: 0.008073
2022-01-08 01:47:54,325 iteration 5633 : loss : 0.022707, loss_ce: 0.005714
2022-01-08 01:47:55,780 iteration 5634 : loss : 0.054001, loss_ce: 0.012945
2022-01-08 01:47:57,179 iteration 5635 : loss : 0.015068, loss_ce: 0.005991
2022-01-08 01:47:58,515 iteration 5636 : loss : 0.022678, loss_ce: 0.006513
2022-01-08 01:47:59,986 iteration 5637 : loss : 0.022843, loss_ce: 0.009507
2022-01-08 01:48:01,379 iteration 5638 : loss : 0.019276, loss_ce: 0.008911
2022-01-08 01:48:02,793 iteration 5639 : loss : 0.019925, loss_ce: 0.006367
2022-01-08 01:48:04,214 iteration 5640 : loss : 0.026643, loss_ce: 0.008710
2022-01-08 01:48:05,561 iteration 5641 : loss : 0.014006, loss_ce: 0.004951
2022-01-08 01:48:07,063 iteration 5642 : loss : 0.020100, loss_ce: 0.008915
2022-01-08 01:48:08,377 iteration 5643 : loss : 0.018602, loss_ce: 0.009060
2022-01-08 01:48:09,783 iteration 5644 : loss : 0.021634, loss_ce: 0.010319
 83%|████████████████████████     | 332/400 [2:23:34<29:03, 25.64s/it]2022-01-08 01:48:11,163 iteration 5645 : loss : 0.017549, loss_ce: 0.005559
2022-01-08 01:48:12,535 iteration 5646 : loss : 0.014608, loss_ce: 0.004068
2022-01-08 01:48:13,865 iteration 5647 : loss : 0.012612, loss_ce: 0.004309
2022-01-08 01:48:15,278 iteration 5648 : loss : 0.020534, loss_ce: 0.010349
2022-01-08 01:48:16,645 iteration 5649 : loss : 0.017358, loss_ce: 0.005272
2022-01-08 01:48:18,098 iteration 5650 : loss : 0.031818, loss_ce: 0.012537
2022-01-08 01:48:19,485 iteration 5651 : loss : 0.019092, loss_ce: 0.009276
2022-01-08 01:48:20,893 iteration 5652 : loss : 0.022503, loss_ce: 0.006383
2022-01-08 01:48:22,283 iteration 5653 : loss : 0.018889, loss_ce: 0.005664
2022-01-08 01:48:23,671 iteration 5654 : loss : 0.018587, loss_ce: 0.006996
2022-01-08 01:48:24,975 iteration 5655 : loss : 0.016531, loss_ce: 0.008064
2022-01-08 01:48:26,454 iteration 5656 : loss : 0.022603, loss_ce: 0.009128
2022-01-08 01:48:27,937 iteration 5657 : loss : 0.025382, loss_ce: 0.008941
2022-01-08 01:48:29,384 iteration 5658 : loss : 0.023606, loss_ce: 0.010466
2022-01-08 01:48:30,742 iteration 5659 : loss : 0.016295, loss_ce: 0.006288
2022-01-08 01:48:32,162 iteration 5660 : loss : 0.018392, loss_ce: 0.006342
2022-01-08 01:48:33,577 iteration 5661 : loss : 0.016887, loss_ce: 0.007544
 83%|████████████████████████▏    | 333/400 [2:23:57<28:00, 25.08s/it]2022-01-08 01:48:35,014 iteration 5662 : loss : 0.022034, loss_ce: 0.010680
2022-01-08 01:48:36,369 iteration 5663 : loss : 0.017722, loss_ce: 0.006090
2022-01-08 01:48:37,769 iteration 5664 : loss : 0.022625, loss_ce: 0.009931
2022-01-08 01:48:39,114 iteration 5665 : loss : 0.014537, loss_ce: 0.005824
2022-01-08 01:48:40,545 iteration 5666 : loss : 0.024254, loss_ce: 0.005573
2022-01-08 01:48:41,983 iteration 5667 : loss : 0.019643, loss_ce: 0.007874
2022-01-08 01:48:43,393 iteration 5668 : loss : 0.027188, loss_ce: 0.010018
2022-01-08 01:48:44,732 iteration 5669 : loss : 0.015415, loss_ce: 0.005950
2022-01-08 01:48:46,128 iteration 5670 : loss : 0.017077, loss_ce: 0.005656
2022-01-08 01:48:47,480 iteration 5671 : loss : 0.019498, loss_ce: 0.006946
2022-01-08 01:48:48,965 iteration 5672 : loss : 0.032330, loss_ce: 0.013727
2022-01-08 01:48:50,338 iteration 5673 : loss : 0.019263, loss_ce: 0.007607
2022-01-08 01:48:51,750 iteration 5674 : loss : 0.018467, loss_ce: 0.005706
2022-01-08 01:48:53,115 iteration 5675 : loss : 0.013814, loss_ce: 0.006570
2022-01-08 01:48:54,561 iteration 5676 : loss : 0.027509, loss_ce: 0.010433
2022-01-08 01:48:55,943 iteration 5677 : loss : 0.027814, loss_ce: 0.006306
2022-01-08 01:48:57,436 iteration 5678 : loss : 0.040167, loss_ce: 0.018984
 84%|████████████████████████▏    | 334/400 [2:24:21<27:11, 24.71s/it]2022-01-08 01:48:58,924 iteration 5679 : loss : 0.020327, loss_ce: 0.007580
2022-01-08 01:49:00,276 iteration 5680 : loss : 0.020227, loss_ce: 0.008462
2022-01-08 01:49:01,685 iteration 5681 : loss : 0.020989, loss_ce: 0.008566
2022-01-08 01:49:03,064 iteration 5682 : loss : 0.018432, loss_ce: 0.006739
2022-01-08 01:49:04,516 iteration 5683 : loss : 0.023779, loss_ce: 0.009127
2022-01-08 01:49:05,978 iteration 5684 : loss : 0.022893, loss_ce: 0.008083
2022-01-08 01:49:07,414 iteration 5685 : loss : 0.017162, loss_ce: 0.005150
2022-01-08 01:49:08,808 iteration 5686 : loss : 0.029609, loss_ce: 0.011319
2022-01-08 01:49:10,189 iteration 5687 : loss : 0.018772, loss_ce: 0.005486
2022-01-08 01:49:11,574 iteration 5688 : loss : 0.022671, loss_ce: 0.006967
2022-01-08 01:49:12,950 iteration 5689 : loss : 0.017016, loss_ce: 0.007381
2022-01-08 01:49:14,369 iteration 5690 : loss : 0.021761, loss_ce: 0.011735
2022-01-08 01:49:15,756 iteration 5691 : loss : 0.013267, loss_ce: 0.003714
2022-01-08 01:49:17,114 iteration 5692 : loss : 0.017731, loss_ce: 0.006168
2022-01-08 01:49:18,533 iteration 5693 : loss : 0.023490, loss_ce: 0.010741
2022-01-08 01:49:19,966 iteration 5694 : loss : 0.031057, loss_ce: 0.011779
2022-01-08 01:49:19,966 Training Data Eval:
2022-01-08 01:49:27,004   Average segmentation loss on training set: 0.0112
2022-01-08 01:49:27,004 Validation Data Eval:
2022-01-08 01:49:29,393   Average segmentation loss on validation set: 0.0655
2022-01-08 01:49:30,753 iteration 5695 : loss : 0.019807, loss_ce: 0.006694
 84%|████████████████████████▎    | 335/400 [2:24:54<29:34, 27.30s/it]2022-01-08 01:49:32,165 iteration 5696 : loss : 0.023308, loss_ce: 0.007507
2022-01-08 01:49:33,651 iteration 5697 : loss : 0.019907, loss_ce: 0.007755
2022-01-08 01:49:35,080 iteration 5698 : loss : 0.014816, loss_ce: 0.006028
2022-01-08 01:49:36,550 iteration 5699 : loss : 0.035212, loss_ce: 0.013533
2022-01-08 01:49:37,953 iteration 5700 : loss : 0.016386, loss_ce: 0.005929
2022-01-08 01:49:39,332 iteration 5701 : loss : 0.016865, loss_ce: 0.008376
2022-01-08 01:49:40,761 iteration 5702 : loss : 0.013457, loss_ce: 0.004645
2022-01-08 01:49:42,145 iteration 5703 : loss : 0.013835, loss_ce: 0.006450
2022-01-08 01:49:43,576 iteration 5704 : loss : 0.020422, loss_ce: 0.009094
2022-01-08 01:49:44,995 iteration 5705 : loss : 0.033841, loss_ce: 0.009813
2022-01-08 01:49:46,380 iteration 5706 : loss : 0.026223, loss_ce: 0.005179
2022-01-08 01:49:47,894 iteration 5707 : loss : 0.029847, loss_ce: 0.010373
2022-01-08 01:49:49,277 iteration 5708 : loss : 0.019071, loss_ce: 0.007614
2022-01-08 01:49:50,693 iteration 5709 : loss : 0.017429, loss_ce: 0.007378
2022-01-08 01:49:52,204 iteration 5710 : loss : 0.021556, loss_ce: 0.009654
2022-01-08 01:49:53,580 iteration 5711 : loss : 0.027988, loss_ce: 0.015517
2022-01-08 01:49:54,958 iteration 5712 : loss : 0.022821, loss_ce: 0.010614
 84%|████████████████████████▎    | 336/400 [2:25:19<28:07, 26.37s/it]2022-01-08 01:49:56,351 iteration 5713 : loss : 0.018324, loss_ce: 0.005428
2022-01-08 01:49:57,715 iteration 5714 : loss : 0.016313, loss_ce: 0.006218
2022-01-08 01:49:59,077 iteration 5715 : loss : 0.013821, loss_ce: 0.005893
2022-01-08 01:50:00,479 iteration 5716 : loss : 0.017265, loss_ce: 0.007842
2022-01-08 01:50:01,914 iteration 5717 : loss : 0.031125, loss_ce: 0.009698
2022-01-08 01:50:03,247 iteration 5718 : loss : 0.017883, loss_ce: 0.005445
2022-01-08 01:50:04,706 iteration 5719 : loss : 0.014310, loss_ce: 0.005470
2022-01-08 01:50:06,066 iteration 5720 : loss : 0.022383, loss_ce: 0.009526
2022-01-08 01:50:07,356 iteration 5721 : loss : 0.015723, loss_ce: 0.004319
2022-01-08 01:50:08,729 iteration 5722 : loss : 0.016784, loss_ce: 0.005931
2022-01-08 01:50:10,124 iteration 5723 : loss : 0.016206, loss_ce: 0.006518
2022-01-08 01:50:11,421 iteration 5724 : loss : 0.013877, loss_ce: 0.004705
2022-01-08 01:50:12,836 iteration 5725 : loss : 0.016995, loss_ce: 0.007672
2022-01-08 01:50:14,172 iteration 5726 : loss : 0.016093, loss_ce: 0.006951
2022-01-08 01:50:15,547 iteration 5727 : loss : 0.019443, loss_ce: 0.006764
2022-01-08 01:50:16,928 iteration 5728 : loss : 0.016111, loss_ce: 0.007491
2022-01-08 01:50:18,348 iteration 5729 : loss : 0.016827, loss_ce: 0.006210
 84%|████████████████████████▍    | 337/400 [2:25:42<26:44, 25.47s/it]2022-01-08 01:50:19,790 iteration 5730 : loss : 0.030530, loss_ce: 0.009416
2022-01-08 01:50:21,228 iteration 5731 : loss : 0.015210, loss_ce: 0.006218
2022-01-08 01:50:22,631 iteration 5732 : loss : 0.019077, loss_ce: 0.005527
2022-01-08 01:50:23,981 iteration 5733 : loss : 0.042901, loss_ce: 0.012217
2022-01-08 01:50:25,376 iteration 5734 : loss : 0.025336, loss_ce: 0.008190
2022-01-08 01:50:26,780 iteration 5735 : loss : 0.016963, loss_ce: 0.007965
2022-01-08 01:50:28,184 iteration 5736 : loss : 0.021146, loss_ce: 0.011095
2022-01-08 01:50:29,559 iteration 5737 : loss : 0.030185, loss_ce: 0.015534
2022-01-08 01:50:30,924 iteration 5738 : loss : 0.019001, loss_ce: 0.010298
2022-01-08 01:50:32,255 iteration 5739 : loss : 0.018188, loss_ce: 0.006816
2022-01-08 01:50:33,671 iteration 5740 : loss : 0.019581, loss_ce: 0.006826
2022-01-08 01:50:35,098 iteration 5741 : loss : 0.021267, loss_ce: 0.011371
2022-01-08 01:50:36,504 iteration 5742 : loss : 0.023398, loss_ce: 0.009323
2022-01-08 01:50:37,864 iteration 5743 : loss : 0.017137, loss_ce: 0.006764
2022-01-08 01:50:39,309 iteration 5744 : loss : 0.017712, loss_ce: 0.006967
2022-01-08 01:50:40,728 iteration 5745 : loss : 0.023334, loss_ce: 0.007668
2022-01-08 01:50:42,046 iteration 5746 : loss : 0.015857, loss_ce: 0.004443
 84%|████████████████████████▌    | 338/400 [2:26:06<25:46, 24.94s/it]2022-01-08 01:50:43,506 iteration 5747 : loss : 0.015960, loss_ce: 0.005378
2022-01-08 01:50:44,932 iteration 5748 : loss : 0.026252, loss_ce: 0.009049
2022-01-08 01:50:46,401 iteration 5749 : loss : 0.017707, loss_ce: 0.007307
2022-01-08 01:50:47,909 iteration 5750 : loss : 0.016411, loss_ce: 0.004273
2022-01-08 01:50:49,244 iteration 5751 : loss : 0.016117, loss_ce: 0.006248
2022-01-08 01:50:50,660 iteration 5752 : loss : 0.014779, loss_ce: 0.005185
2022-01-08 01:50:52,039 iteration 5753 : loss : 0.029769, loss_ce: 0.010455
2022-01-08 01:50:53,379 iteration 5754 : loss : 0.014021, loss_ce: 0.005227
2022-01-08 01:50:54,765 iteration 5755 : loss : 0.020888, loss_ce: 0.010158
2022-01-08 01:50:56,155 iteration 5756 : loss : 0.029076, loss_ce: 0.012202
2022-01-08 01:50:57,525 iteration 5757 : loss : 0.016826, loss_ce: 0.008248
2022-01-08 01:50:58,939 iteration 5758 : loss : 0.023839, loss_ce: 0.005529
2022-01-08 01:51:00,343 iteration 5759 : loss : 0.018466, loss_ce: 0.008299
2022-01-08 01:51:01,814 iteration 5760 : loss : 0.017999, loss_ce: 0.006870
2022-01-08 01:51:03,135 iteration 5761 : loss : 0.012784, loss_ce: 0.005079
2022-01-08 01:51:04,739 iteration 5762 : loss : 0.025162, loss_ce: 0.009956
2022-01-08 01:51:06,132 iteration 5763 : loss : 0.018638, loss_ce: 0.007518
 85%|████████████████████████▌    | 339/400 [2:26:30<25:05, 24.69s/it]2022-01-08 01:51:07,584 iteration 5764 : loss : 0.021716, loss_ce: 0.009325
2022-01-08 01:51:09,014 iteration 5765 : loss : 0.020591, loss_ce: 0.005700
2022-01-08 01:51:10,439 iteration 5766 : loss : 0.017084, loss_ce: 0.007296
2022-01-08 01:51:11,878 iteration 5767 : loss : 0.018104, loss_ce: 0.007177
2022-01-08 01:51:13,385 iteration 5768 : loss : 0.027244, loss_ce: 0.008151
2022-01-08 01:51:14,776 iteration 5769 : loss : 0.015536, loss_ce: 0.005735
2022-01-08 01:51:16,170 iteration 5770 : loss : 0.015359, loss_ce: 0.006248
2022-01-08 01:51:17,582 iteration 5771 : loss : 0.026441, loss_ce: 0.011141
2022-01-08 01:51:18,988 iteration 5772 : loss : 0.027877, loss_ce: 0.009957
2022-01-08 01:51:20,465 iteration 5773 : loss : 0.015940, loss_ce: 0.006339
2022-01-08 01:51:21,935 iteration 5774 : loss : 0.021395, loss_ce: 0.008435
2022-01-08 01:51:23,462 iteration 5775 : loss : 0.025358, loss_ce: 0.008889
2022-01-08 01:51:24,841 iteration 5776 : loss : 0.014670, loss_ce: 0.004591
2022-01-08 01:51:26,202 iteration 5777 : loss : 0.017560, loss_ce: 0.006683
2022-01-08 01:51:27,637 iteration 5778 : loss : 0.016127, loss_ce: 0.005279
2022-01-08 01:51:29,054 iteration 5779 : loss : 0.019130, loss_ce: 0.008225
2022-01-08 01:51:29,055 Training Data Eval:
2022-01-08 01:51:36,088   Average segmentation loss on training set: 0.0106
2022-01-08 01:51:36,088 Validation Data Eval:
2022-01-08 01:51:38,466   Average segmentation loss on validation set: 0.0688
2022-01-08 01:51:39,898 iteration 5780 : loss : 0.017262, loss_ce: 0.006814
 85%|████████████████████████▋    | 340/400 [2:27:04<27:24, 27.41s/it]2022-01-08 01:51:41,502 iteration 5781 : loss : 0.025321, loss_ce: 0.008276
2022-01-08 01:51:42,806 iteration 5782 : loss : 0.012612, loss_ce: 0.005902
2022-01-08 01:51:44,147 iteration 5783 : loss : 0.020445, loss_ce: 0.008693
2022-01-08 01:51:45,498 iteration 5784 : loss : 0.017269, loss_ce: 0.005742
2022-01-08 01:51:46,830 iteration 5785 : loss : 0.015647, loss_ce: 0.006770
2022-01-08 01:51:48,231 iteration 5786 : loss : 0.039094, loss_ce: 0.011594
2022-01-08 01:51:49,605 iteration 5787 : loss : 0.020860, loss_ce: 0.008226
2022-01-08 01:51:50,898 iteration 5788 : loss : 0.016967, loss_ce: 0.006792
2022-01-08 01:51:52,214 iteration 5789 : loss : 0.011906, loss_ce: 0.004811
2022-01-08 01:51:53,649 iteration 5790 : loss : 0.014146, loss_ce: 0.005497
2022-01-08 01:51:55,026 iteration 5791 : loss : 0.013849, loss_ce: 0.006246
2022-01-08 01:51:56,364 iteration 5792 : loss : 0.014651, loss_ce: 0.005916
2022-01-08 01:51:57,796 iteration 5793 : loss : 0.017464, loss_ce: 0.005445
2022-01-08 01:51:59,286 iteration 5794 : loss : 0.021848, loss_ce: 0.010462
2022-01-08 01:52:00,632 iteration 5795 : loss : 0.013185, loss_ce: 0.006400
2022-01-08 01:52:02,035 iteration 5796 : loss : 0.014710, loss_ce: 0.006911
2022-01-08 01:52:03,431 iteration 5797 : loss : 0.023767, loss_ce: 0.009184
 85%|████████████████████████▋    | 341/400 [2:27:27<25:48, 26.25s/it]2022-01-08 01:52:04,962 iteration 5798 : loss : 0.020388, loss_ce: 0.005490
2022-01-08 01:52:06,348 iteration 5799 : loss : 0.014825, loss_ce: 0.004195
2022-01-08 01:52:07,727 iteration 5800 : loss : 0.017009, loss_ce: 0.004525
2022-01-08 01:52:09,028 iteration 5801 : loss : 0.011873, loss_ce: 0.004208
2022-01-08 01:52:10,444 iteration 5802 : loss : 0.015991, loss_ce: 0.008073
2022-01-08 01:52:11,785 iteration 5803 : loss : 0.012287, loss_ce: 0.004270
2022-01-08 01:52:13,271 iteration 5804 : loss : 0.023956, loss_ce: 0.008219
2022-01-08 01:52:14,629 iteration 5805 : loss : 0.018010, loss_ce: 0.008139
2022-01-08 01:52:15,967 iteration 5806 : loss : 0.017627, loss_ce: 0.006939
2022-01-08 01:52:17,393 iteration 5807 : loss : 0.014882, loss_ce: 0.005590
2022-01-08 01:52:18,758 iteration 5808 : loss : 0.037824, loss_ce: 0.017569
2022-01-08 01:52:20,239 iteration 5809 : loss : 0.029250, loss_ce: 0.007946
2022-01-08 01:52:21,565 iteration 5810 : loss : 0.016571, loss_ce: 0.005605
2022-01-08 01:52:22,955 iteration 5811 : loss : 0.015675, loss_ce: 0.006935
2022-01-08 01:52:24,382 iteration 5812 : loss : 0.018991, loss_ce: 0.008552
2022-01-08 01:52:25,731 iteration 5813 : loss : 0.026425, loss_ce: 0.008559
2022-01-08 01:52:27,118 iteration 5814 : loss : 0.020856, loss_ce: 0.008418
 86%|████████████████████████▊    | 342/400 [2:27:51<24:37, 25.48s/it]2022-01-08 01:52:28,657 iteration 5815 : loss : 0.020037, loss_ce: 0.009017
2022-01-08 01:52:29,986 iteration 5816 : loss : 0.016779, loss_ce: 0.005237
2022-01-08 01:52:31,387 iteration 5817 : loss : 0.017169, loss_ce: 0.004905
2022-01-08 01:52:32,759 iteration 5818 : loss : 0.018220, loss_ce: 0.005698
2022-01-08 01:52:34,243 iteration 5819 : loss : 0.018744, loss_ce: 0.007560
2022-01-08 01:52:35,655 iteration 5820 : loss : 0.014872, loss_ce: 0.006333
2022-01-08 01:52:37,035 iteration 5821 : loss : 0.023885, loss_ce: 0.008112
2022-01-08 01:52:38,486 iteration 5822 : loss : 0.030057, loss_ce: 0.007803
2022-01-08 01:52:39,896 iteration 5823 : loss : 0.011824, loss_ce: 0.003775
2022-01-08 01:52:41,315 iteration 5824 : loss : 0.025615, loss_ce: 0.012429
2022-01-08 01:52:42,703 iteration 5825 : loss : 0.021240, loss_ce: 0.010159
2022-01-08 01:52:44,059 iteration 5826 : loss : 0.018294, loss_ce: 0.005828
2022-01-08 01:52:45,397 iteration 5827 : loss : 0.024110, loss_ce: 0.005593
2022-01-08 01:52:46,698 iteration 5828 : loss : 0.012453, loss_ce: 0.005294
2022-01-08 01:52:48,081 iteration 5829 : loss : 0.016851, loss_ce: 0.006578
2022-01-08 01:52:49,482 iteration 5830 : loss : 0.022671, loss_ce: 0.008248
2022-01-08 01:52:50,922 iteration 5831 : loss : 0.018973, loss_ce: 0.007283
 86%|████████████████████████▊    | 343/400 [2:28:15<23:43, 24.98s/it]2022-01-08 01:52:52,443 iteration 5832 : loss : 0.016715, loss_ce: 0.006937
2022-01-08 01:52:53,835 iteration 5833 : loss : 0.012646, loss_ce: 0.004291
2022-01-08 01:52:55,309 iteration 5834 : loss : 0.018206, loss_ce: 0.007715
2022-01-08 01:52:56,856 iteration 5835 : loss : 0.031327, loss_ce: 0.013562
2022-01-08 01:52:58,236 iteration 5836 : loss : 0.017032, loss_ce: 0.006705
2022-01-08 01:52:59,746 iteration 5837 : loss : 0.028684, loss_ce: 0.012453
2022-01-08 01:53:01,221 iteration 5838 : loss : 0.020853, loss_ce: 0.006045
2022-01-08 01:53:02,548 iteration 5839 : loss : 0.017697, loss_ce: 0.005828
2022-01-08 01:53:04,135 iteration 5840 : loss : 0.025808, loss_ce: 0.011489
2022-01-08 01:53:05,551 iteration 5841 : loss : 0.013860, loss_ce: 0.005798
2022-01-08 01:53:07,021 iteration 5842 : loss : 0.036787, loss_ce: 0.017159
2022-01-08 01:53:08,497 iteration 5843 : loss : 0.028143, loss_ce: 0.008069
2022-01-08 01:53:09,926 iteration 5844 : loss : 0.028446, loss_ce: 0.008152
2022-01-08 01:53:11,405 iteration 5845 : loss : 0.017400, loss_ce: 0.005778
2022-01-08 01:53:12,835 iteration 5846 : loss : 0.016812, loss_ce: 0.006376
2022-01-08 01:53:14,223 iteration 5847 : loss : 0.016791, loss_ce: 0.005469
2022-01-08 01:53:15,543 iteration 5848 : loss : 0.013821, loss_ce: 0.004313
 86%|████████████████████████▉    | 344/400 [2:28:39<23:12, 24.87s/it]2022-01-08 01:53:16,921 iteration 5849 : loss : 0.017605, loss_ce: 0.005769
2022-01-08 01:53:18,415 iteration 5850 : loss : 0.025344, loss_ce: 0.007037
2022-01-08 01:53:19,767 iteration 5851 : loss : 0.015076, loss_ce: 0.006003
2022-01-08 01:53:21,127 iteration 5852 : loss : 0.017966, loss_ce: 0.006082
2022-01-08 01:53:22,457 iteration 5853 : loss : 0.022545, loss_ce: 0.005465
2022-01-08 01:53:23,870 iteration 5854 : loss : 0.022744, loss_ce: 0.008575
2022-01-08 01:53:25,246 iteration 5855 : loss : 0.025301, loss_ce: 0.005811
2022-01-08 01:53:26,575 iteration 5856 : loss : 0.014466, loss_ce: 0.006036
2022-01-08 01:53:27,997 iteration 5857 : loss : 0.022329, loss_ce: 0.010214
2022-01-08 01:53:29,384 iteration 5858 : loss : 0.030306, loss_ce: 0.009807
2022-01-08 01:53:30,805 iteration 5859 : loss : 0.025489, loss_ce: 0.012067
2022-01-08 01:53:32,224 iteration 5860 : loss : 0.014115, loss_ce: 0.005389
2022-01-08 01:53:33,644 iteration 5861 : loss : 0.019464, loss_ce: 0.008321
2022-01-08 01:53:35,027 iteration 5862 : loss : 0.014952, loss_ce: 0.005635
2022-01-08 01:53:36,422 iteration 5863 : loss : 0.018428, loss_ce: 0.007100
2022-01-08 01:53:37,798 iteration 5864 : loss : 0.020811, loss_ce: 0.007713
2022-01-08 01:53:37,798 Training Data Eval:
2022-01-08 01:53:44,827   Average segmentation loss on training set: 0.0109
2022-01-08 01:53:44,828 Validation Data Eval:
2022-01-08 01:53:47,218   Average segmentation loss on validation set: 0.0703
2022-01-08 01:53:48,575 iteration 5865 : loss : 0.014463, loss_ce: 0.005200
 86%|█████████████████████████    | 345/400 [2:29:12<25:02, 27.32s/it]2022-01-08 01:53:50,015 iteration 5866 : loss : 0.032744, loss_ce: 0.011469
2022-01-08 01:53:51,416 iteration 5867 : loss : 0.017850, loss_ce: 0.007341
2022-01-08 01:53:52,733 iteration 5868 : loss : 0.012904, loss_ce: 0.005467
2022-01-08 01:53:54,108 iteration 5869 : loss : 0.012279, loss_ce: 0.004427
2022-01-08 01:53:55,413 iteration 5870 : loss : 0.016162, loss_ce: 0.006125
2022-01-08 01:53:56,780 iteration 5871 : loss : 0.015872, loss_ce: 0.006746
2022-01-08 01:53:58,148 iteration 5872 : loss : 0.013308, loss_ce: 0.005902
2022-01-08 01:53:59,479 iteration 5873 : loss : 0.015483, loss_ce: 0.006935
2022-01-08 01:54:00,805 iteration 5874 : loss : 0.024054, loss_ce: 0.005568
2022-01-08 01:54:02,259 iteration 5875 : loss : 0.018955, loss_ce: 0.008718
2022-01-08 01:54:03,725 iteration 5876 : loss : 0.018505, loss_ce: 0.008658
2022-01-08 01:54:05,139 iteration 5877 : loss : 0.028078, loss_ce: 0.010736
2022-01-08 01:54:06,608 iteration 5878 : loss : 0.030124, loss_ce: 0.010861
2022-01-08 01:54:07,966 iteration 5879 : loss : 0.021918, loss_ce: 0.005994
2022-01-08 01:54:09,338 iteration 5880 : loss : 0.023396, loss_ce: 0.007831
2022-01-08 01:54:10,766 iteration 5881 : loss : 0.041316, loss_ce: 0.011156
2022-01-08 01:54:12,155 iteration 5882 : loss : 0.015596, loss_ce: 0.005548
 86%|█████████████████████████    | 346/400 [2:29:36<23:34, 26.20s/it]2022-01-08 01:54:13,722 iteration 5883 : loss : 0.020260, loss_ce: 0.005829
2022-01-08 01:54:15,073 iteration 5884 : loss : 0.018586, loss_ce: 0.007262
2022-01-08 01:54:16,526 iteration 5885 : loss : 0.019531, loss_ce: 0.005130
2022-01-08 01:54:17,899 iteration 5886 : loss : 0.013879, loss_ce: 0.005709
2022-01-08 01:54:19,262 iteration 5887 : loss : 0.016279, loss_ce: 0.004947
2022-01-08 01:54:20,554 iteration 5888 : loss : 0.013128, loss_ce: 0.005636
2022-01-08 01:54:21,917 iteration 5889 : loss : 0.016469, loss_ce: 0.007081
2022-01-08 01:54:23,358 iteration 5890 : loss : 0.015665, loss_ce: 0.006921
2022-01-08 01:54:24,798 iteration 5891 : loss : 0.043132, loss_ce: 0.015079
2022-01-08 01:54:26,262 iteration 5892 : loss : 0.019359, loss_ce: 0.008860
2022-01-08 01:54:27,578 iteration 5893 : loss : 0.012577, loss_ce: 0.003363
2022-01-08 01:54:28,927 iteration 5894 : loss : 0.013253, loss_ce: 0.003690
2022-01-08 01:54:30,374 iteration 5895 : loss : 0.018331, loss_ce: 0.007944
2022-01-08 01:54:31,678 iteration 5896 : loss : 0.015551, loss_ce: 0.005095
2022-01-08 01:54:33,080 iteration 5897 : loss : 0.029098, loss_ce: 0.011396
2022-01-08 01:54:34,528 iteration 5898 : loss : 0.026412, loss_ce: 0.011041
2022-01-08 01:54:35,925 iteration 5899 : loss : 0.019034, loss_ce: 0.005990
 87%|█████████████████████████▏   | 347/400 [2:30:00<22:29, 25.47s/it]2022-01-08 01:54:37,255 iteration 5900 : loss : 0.013203, loss_ce: 0.005318
2022-01-08 01:54:38,603 iteration 5901 : loss : 0.013624, loss_ce: 0.006003
2022-01-08 01:54:39,909 iteration 5902 : loss : 0.012561, loss_ce: 0.004900
2022-01-08 01:54:41,261 iteration 5903 : loss : 0.018145, loss_ce: 0.007319
2022-01-08 01:54:42,685 iteration 5904 : loss : 0.021577, loss_ce: 0.006346
2022-01-08 01:54:44,115 iteration 5905 : loss : 0.022230, loss_ce: 0.011923
2022-01-08 01:54:45,564 iteration 5906 : loss : 0.020449, loss_ce: 0.007329
2022-01-08 01:54:46,886 iteration 5907 : loss : 0.013624, loss_ce: 0.005918
2022-01-08 01:54:48,297 iteration 5908 : loss : 0.018282, loss_ce: 0.006123
2022-01-08 01:54:49,729 iteration 5909 : loss : 0.022221, loss_ce: 0.010432
2022-01-08 01:54:51,114 iteration 5910 : loss : 0.017981, loss_ce: 0.005746
2022-01-08 01:54:52,540 iteration 5911 : loss : 0.014994, loss_ce: 0.003764
2022-01-08 01:54:53,970 iteration 5912 : loss : 0.028272, loss_ce: 0.006968
2022-01-08 01:54:55,354 iteration 5913 : loss : 0.017231, loss_ce: 0.007101
2022-01-08 01:54:56,722 iteration 5914 : loss : 0.029632, loss_ce: 0.018306
2022-01-08 01:54:58,137 iteration 5915 : loss : 0.019717, loss_ce: 0.010105
2022-01-08 01:54:59,488 iteration 5916 : loss : 0.012878, loss_ce: 0.004554
 87%|█████████████████████████▏   | 348/400 [2:30:23<21:34, 24.90s/it]2022-01-08 01:55:00,866 iteration 5917 : loss : 0.016940, loss_ce: 0.007229
2022-01-08 01:55:02,330 iteration 5918 : loss : 0.027659, loss_ce: 0.013744
2022-01-08 01:55:03,794 iteration 5919 : loss : 0.024654, loss_ce: 0.010175
2022-01-08 01:55:05,266 iteration 5920 : loss : 0.017327, loss_ce: 0.005666
2022-01-08 01:55:06,668 iteration 5921 : loss : 0.026183, loss_ce: 0.009121
2022-01-08 01:55:08,101 iteration 5922 : loss : 0.022139, loss_ce: 0.011239
2022-01-08 01:55:09,507 iteration 5923 : loss : 0.022778, loss_ce: 0.010396
2022-01-08 01:55:10,817 iteration 5924 : loss : 0.012085, loss_ce: 0.004165
2022-01-08 01:55:12,216 iteration 5925 : loss : 0.015606, loss_ce: 0.005101
2022-01-08 01:55:13,670 iteration 5926 : loss : 0.038545, loss_ce: 0.012177
2022-01-08 01:55:15,024 iteration 5927 : loss : 0.011357, loss_ce: 0.004621
2022-01-08 01:55:16,474 iteration 5928 : loss : 0.018063, loss_ce: 0.008384
2022-01-08 01:55:17,922 iteration 5929 : loss : 0.018721, loss_ce: 0.007274
2022-01-08 01:55:19,393 iteration 5930 : loss : 0.021059, loss_ce: 0.008397
2022-01-08 01:55:20,741 iteration 5931 : loss : 0.017068, loss_ce: 0.007009
2022-01-08 01:55:22,149 iteration 5932 : loss : 0.017410, loss_ce: 0.005585
2022-01-08 01:55:23,608 iteration 5933 : loss : 0.028516, loss_ce: 0.007269
 87%|█████████████████████████▎   | 349/400 [2:30:47<20:57, 24.66s/it]2022-01-08 01:55:25,071 iteration 5934 : loss : 0.012124, loss_ce: 0.004065
2022-01-08 01:55:26,482 iteration 5935 : loss : 0.019679, loss_ce: 0.005659
2022-01-08 01:55:27,993 iteration 5936 : loss : 0.027612, loss_ce: 0.008920
2022-01-08 01:55:29,479 iteration 5937 : loss : 0.016326, loss_ce: 0.007138
2022-01-08 01:55:30,926 iteration 5938 : loss : 0.022523, loss_ce: 0.007486
2022-01-08 01:55:32,302 iteration 5939 : loss : 0.021660, loss_ce: 0.008308
2022-01-08 01:55:33,724 iteration 5940 : loss : 0.015388, loss_ce: 0.006814
2022-01-08 01:55:35,030 iteration 5941 : loss : 0.014979, loss_ce: 0.006716
2022-01-08 01:55:36,492 iteration 5942 : loss : 0.021811, loss_ce: 0.006869
2022-01-08 01:55:37,820 iteration 5943 : loss : 0.011492, loss_ce: 0.004438
2022-01-08 01:55:39,211 iteration 5944 : loss : 0.018077, loss_ce: 0.006444
2022-01-08 01:55:40,583 iteration 5945 : loss : 0.017665, loss_ce: 0.006390
2022-01-08 01:55:41,938 iteration 5946 : loss : 0.017010, loss_ce: 0.007049
2022-01-08 01:55:43,269 iteration 5947 : loss : 0.014490, loss_ce: 0.005800
2022-01-08 01:55:44,718 iteration 5948 : loss : 0.018949, loss_ce: 0.007825
2022-01-08 01:55:46,068 iteration 5949 : loss : 0.013343, loss_ce: 0.005597
2022-01-08 01:55:46,068 Training Data Eval:
2022-01-08 01:55:53,117   Average segmentation loss on training set: 0.0101
2022-01-08 01:55:53,118 Validation Data Eval:
2022-01-08 01:55:55,505   Average segmentation loss on validation set: 0.0664
2022-01-08 01:55:56,968 iteration 5950 : loss : 0.035478, loss_ce: 0.011281
 88%|█████████████████████████▍   | 350/400 [2:31:21<22:43, 27.27s/it]2022-01-08 01:55:58,375 iteration 5951 : loss : 0.013011, loss_ce: 0.003567
2022-01-08 01:55:59,803 iteration 5952 : loss : 0.018000, loss_ce: 0.009170
2022-01-08 01:56:01,239 iteration 5953 : loss : 0.016491, loss_ce: 0.007221
2022-01-08 01:56:02,711 iteration 5954 : loss : 0.025180, loss_ce: 0.008237
2022-01-08 01:56:04,091 iteration 5955 : loss : 0.019828, loss_ce: 0.007322
2022-01-08 01:56:05,558 iteration 5956 : loss : 0.018483, loss_ce: 0.006576
2022-01-08 01:56:06,961 iteration 5957 : loss : 0.021013, loss_ce: 0.007028
2022-01-08 01:56:08,317 iteration 5958 : loss : 0.015931, loss_ce: 0.006881
2022-01-08 01:56:09,697 iteration 5959 : loss : 0.014005, loss_ce: 0.004598
2022-01-08 01:56:11,045 iteration 5960 : loss : 0.019688, loss_ce: 0.009419
2022-01-08 01:56:12,483 iteration 5961 : loss : 0.019238, loss_ce: 0.007340
2022-01-08 01:56:13,928 iteration 5962 : loss : 0.016050, loss_ce: 0.007312
2022-01-08 01:56:15,243 iteration 5963 : loss : 0.013332, loss_ce: 0.005608
2022-01-08 01:56:16,805 iteration 5964 : loss : 0.028074, loss_ce: 0.010132
2022-01-08 01:56:18,192 iteration 5965 : loss : 0.016684, loss_ce: 0.006377
2022-01-08 01:56:19,636 iteration 5966 : loss : 0.020114, loss_ce: 0.007918
2022-01-08 01:56:20,997 iteration 5967 : loss : 0.019665, loss_ce: 0.006998
 88%|█████████████████████████▍   | 351/400 [2:31:45<21:28, 26.30s/it]2022-01-08 01:56:22,410 iteration 5968 : loss : 0.017583, loss_ce: 0.007168
2022-01-08 01:56:23,852 iteration 5969 : loss : 0.025447, loss_ce: 0.006369
2022-01-08 01:56:25,275 iteration 5970 : loss : 0.018446, loss_ce: 0.006301
2022-01-08 01:56:26,582 iteration 5971 : loss : 0.012492, loss_ce: 0.004980
2022-01-08 01:56:27,932 iteration 5972 : loss : 0.016165, loss_ce: 0.006205
2022-01-08 01:56:29,331 iteration 5973 : loss : 0.012754, loss_ce: 0.005253
2022-01-08 01:56:30,688 iteration 5974 : loss : 0.020303, loss_ce: 0.007019
2022-01-08 01:56:32,110 iteration 5975 : loss : 0.020383, loss_ce: 0.006829
2022-01-08 01:56:33,512 iteration 5976 : loss : 0.018576, loss_ce: 0.004303
2022-01-08 01:56:34,930 iteration 5977 : loss : 0.014583, loss_ce: 0.005179
2022-01-08 01:56:36,357 iteration 5978 : loss : 0.014522, loss_ce: 0.005809
2022-01-08 01:56:37,726 iteration 5979 : loss : 0.012706, loss_ce: 0.004751
2022-01-08 01:56:39,158 iteration 5980 : loss : 0.011454, loss_ce: 0.004050
2022-01-08 01:56:40,580 iteration 5981 : loss : 0.018677, loss_ce: 0.007336
2022-01-08 01:56:42,044 iteration 5982 : loss : 0.026882, loss_ce: 0.010923
2022-01-08 01:56:43,384 iteration 5983 : loss : 0.020093, loss_ce: 0.009177
2022-01-08 01:56:44,727 iteration 5984 : loss : 0.020155, loss_ce: 0.006218
 88%|█████████████████████████▌   | 352/400 [2:32:08<20:25, 25.53s/it]2022-01-08 01:56:46,266 iteration 5985 : loss : 0.026504, loss_ce: 0.012010
2022-01-08 01:56:47,631 iteration 5986 : loss : 0.014078, loss_ce: 0.005950
2022-01-08 01:56:49,112 iteration 5987 : loss : 0.054427, loss_ce: 0.008850
2022-01-08 01:56:50,593 iteration 5988 : loss : 0.015769, loss_ce: 0.007036
2022-01-08 01:56:51,978 iteration 5989 : loss : 0.016037, loss_ce: 0.006639
2022-01-08 01:56:53,408 iteration 5990 : loss : 0.018814, loss_ce: 0.011668
2022-01-08 01:56:54,734 iteration 5991 : loss : 0.013531, loss_ce: 0.004750
2022-01-08 01:56:56,167 iteration 5992 : loss : 0.032550, loss_ce: 0.010384
2022-01-08 01:56:57,509 iteration 5993 : loss : 0.010331, loss_ce: 0.003478
2022-01-08 01:56:58,887 iteration 5994 : loss : 0.021835, loss_ce: 0.007415
2022-01-08 01:57:00,326 iteration 5995 : loss : 0.019720, loss_ce: 0.007491
2022-01-08 01:57:01,810 iteration 5996 : loss : 0.015676, loss_ce: 0.005288
2022-01-08 01:57:03,275 iteration 5997 : loss : 0.019041, loss_ce: 0.008542
2022-01-08 01:57:04,628 iteration 5998 : loss : 0.013064, loss_ce: 0.004386
2022-01-08 01:57:06,062 iteration 5999 : loss : 0.028454, loss_ce: 0.007204
2022-01-08 01:57:07,508 iteration 6000 : loss : 0.022630, loss_ce: 0.010481
2022-01-08 01:57:08,966 iteration 6001 : loss : 0.026477, loss_ce: 0.009507
 88%|█████████████████████████▌   | 353/400 [2:32:33<19:41, 25.14s/it]2022-01-08 01:57:10,417 iteration 6002 : loss : 0.013103, loss_ce: 0.005622
2022-01-08 01:57:11,869 iteration 6003 : loss : 0.021820, loss_ce: 0.007571
2022-01-08 01:57:13,293 iteration 6004 : loss : 0.015148, loss_ce: 0.006338
2022-01-08 01:57:14,725 iteration 6005 : loss : 0.018101, loss_ce: 0.007722
2022-01-08 01:57:16,063 iteration 6006 : loss : 0.016163, loss_ce: 0.008476
2022-01-08 01:57:17,457 iteration 6007 : loss : 0.018684, loss_ce: 0.008182
2022-01-08 01:57:18,881 iteration 6008 : loss : 0.016065, loss_ce: 0.005916
2022-01-08 01:57:20,388 iteration 6009 : loss : 0.022076, loss_ce: 0.007905
2022-01-08 01:57:21,693 iteration 6010 : loss : 0.013247, loss_ce: 0.005541
2022-01-08 01:57:23,108 iteration 6011 : loss : 0.017969, loss_ce: 0.006118
2022-01-08 01:57:24,520 iteration 6012 : loss : 0.020087, loss_ce: 0.006505
2022-01-08 01:57:26,038 iteration 6013 : loss : 0.023636, loss_ce: 0.009163
2022-01-08 01:57:27,430 iteration 6014 : loss : 0.016597, loss_ce: 0.006669
2022-01-08 01:57:28,790 iteration 6015 : loss : 0.051872, loss_ce: 0.021380
2022-01-08 01:57:30,189 iteration 6016 : loss : 0.017800, loss_ce: 0.005511
2022-01-08 01:57:31,540 iteration 6017 : loss : 0.026667, loss_ce: 0.006590
2022-01-08 01:57:33,023 iteration 6018 : loss : 0.017928, loss_ce: 0.006499
 88%|█████████████████████████▋   | 354/400 [2:32:57<19:01, 24.82s/it]2022-01-08 01:57:34,502 iteration 6019 : loss : 0.027035, loss_ce: 0.010521
2022-01-08 01:57:35,891 iteration 6020 : loss : 0.016743, loss_ce: 0.006762
2022-01-08 01:57:37,296 iteration 6021 : loss : 0.016925, loss_ce: 0.005696
2022-01-08 01:57:38,786 iteration 6022 : loss : 0.012676, loss_ce: 0.004956
2022-01-08 01:57:40,144 iteration 6023 : loss : 0.012260, loss_ce: 0.004152
2022-01-08 01:57:41,665 iteration 6024 : loss : 0.029632, loss_ce: 0.011229
2022-01-08 01:57:43,179 iteration 6025 : loss : 0.018836, loss_ce: 0.008299
2022-01-08 01:57:44,697 iteration 6026 : loss : 0.025689, loss_ce: 0.008728
2022-01-08 01:57:46,097 iteration 6027 : loss : 0.023907, loss_ce: 0.010550
2022-01-08 01:57:47,476 iteration 6028 : loss : 0.018890, loss_ce: 0.007262
2022-01-08 01:57:48,954 iteration 6029 : loss : 0.019272, loss_ce: 0.009349
2022-01-08 01:57:50,327 iteration 6030 : loss : 0.016796, loss_ce: 0.005596
2022-01-08 01:57:51,873 iteration 6031 : loss : 0.032837, loss_ce: 0.009047
2022-01-08 01:57:53,367 iteration 6032 : loss : 0.020035, loss_ce: 0.006894
2022-01-08 01:57:54,833 iteration 6033 : loss : 0.018759, loss_ce: 0.008367
2022-01-08 01:57:56,359 iteration 6034 : loss : 0.014339, loss_ce: 0.004349
2022-01-08 01:57:56,360 Training Data Eval:
2022-01-08 01:58:03,427   Average segmentation loss on training set: 0.0105
2022-01-08 01:58:03,428 Validation Data Eval:
2022-01-08 01:58:05,824   Average segmentation loss on validation set: 0.0640
2022-01-08 01:58:07,176 iteration 6035 : loss : 0.019169, loss_ce: 0.005427
 89%|█████████████████████████▋   | 355/400 [2:33:31<20:42, 27.62s/it]2022-01-08 01:58:08,690 iteration 6036 : loss : 0.018308, loss_ce: 0.006342
2022-01-08 01:58:10,195 iteration 6037 : loss : 0.025585, loss_ce: 0.009912
2022-01-08 01:58:11,586 iteration 6038 : loss : 0.017869, loss_ce: 0.007359
2022-01-08 01:58:13,032 iteration 6039 : loss : 0.021819, loss_ce: 0.010458
2022-01-08 01:58:14,459 iteration 6040 : loss : 0.015928, loss_ce: 0.006373
2022-01-08 01:58:16,029 iteration 6041 : loss : 0.044459, loss_ce: 0.013782
2022-01-08 01:58:17,536 iteration 6042 : loss : 0.023342, loss_ce: 0.008726
2022-01-08 01:58:18,990 iteration 6043 : loss : 0.016038, loss_ce: 0.007871
2022-01-08 01:58:20,427 iteration 6044 : loss : 0.014166, loss_ce: 0.004648
2022-01-08 01:58:21,941 iteration 6045 : loss : 0.021878, loss_ce: 0.006978
2022-01-08 01:58:23,399 iteration 6046 : loss : 0.026008, loss_ce: 0.008450
2022-01-08 01:58:24,796 iteration 6047 : loss : 0.016477, loss_ce: 0.004376
2022-01-08 01:58:26,362 iteration 6048 : loss : 0.021180, loss_ce: 0.003603
2022-01-08 01:58:27,865 iteration 6049 : loss : 0.017832, loss_ce: 0.007891
2022-01-08 01:58:29,268 iteration 6050 : loss : 0.014892, loss_ce: 0.006042
2022-01-08 01:58:30,785 iteration 6051 : loss : 0.018010, loss_ce: 0.007241
2022-01-08 01:58:32,110 iteration 6052 : loss : 0.015944, loss_ce: 0.003823
 89%|█████████████████████████▊   | 356/400 [2:33:56<19:39, 26.81s/it]2022-01-08 01:58:33,543 iteration 6053 : loss : 0.016659, loss_ce: 0.007597
2022-01-08 01:58:35,090 iteration 6054 : loss : 0.020075, loss_ce: 0.007462
2022-01-08 01:58:36,510 iteration 6055 : loss : 0.015068, loss_ce: 0.005438
2022-01-08 01:58:37,885 iteration 6056 : loss : 0.016841, loss_ce: 0.006270
2022-01-08 01:58:39,261 iteration 6057 : loss : 0.016199, loss_ce: 0.005221
2022-01-08 01:58:40,747 iteration 6058 : loss : 0.016205, loss_ce: 0.006491
2022-01-08 01:58:42,105 iteration 6059 : loss : 0.011803, loss_ce: 0.004457
2022-01-08 01:58:43,621 iteration 6060 : loss : 0.014465, loss_ce: 0.005767
2022-01-08 01:58:44,986 iteration 6061 : loss : 0.022239, loss_ce: 0.008581
2022-01-08 01:58:46,513 iteration 6062 : loss : 0.031061, loss_ce: 0.018362
2022-01-08 01:58:48,045 iteration 6063 : loss : 0.041949, loss_ce: 0.013336
2022-01-08 01:58:49,492 iteration 6064 : loss : 0.031979, loss_ce: 0.009433
2022-01-08 01:58:51,032 iteration 6065 : loss : 0.033561, loss_ce: 0.018037
2022-01-08 01:58:52,458 iteration 6066 : loss : 0.018486, loss_ce: 0.006314
2022-01-08 01:58:53,846 iteration 6067 : loss : 0.029037, loss_ce: 0.005172
2022-01-08 01:58:55,403 iteration 6068 : loss : 0.031848, loss_ce: 0.010893
2022-01-08 01:58:56,809 iteration 6069 : loss : 0.017905, loss_ce: 0.002645
 89%|█████████████████████████▉   | 357/400 [2:34:21<18:45, 26.18s/it]2022-01-08 01:58:58,339 iteration 6070 : loss : 0.021285, loss_ce: 0.011319
2022-01-08 01:58:59,819 iteration 6071 : loss : 0.023466, loss_ce: 0.006504
2022-01-08 01:59:01,264 iteration 6072 : loss : 0.014273, loss_ce: 0.004737
2022-01-08 01:59:02,725 iteration 6073 : loss : 0.022331, loss_ce: 0.008339
2022-01-08 01:59:04,168 iteration 6074 : loss : 0.016218, loss_ce: 0.005663
2022-01-08 01:59:05,679 iteration 6075 : loss : 0.021581, loss_ce: 0.009334
2022-01-08 01:59:07,096 iteration 6076 : loss : 0.018404, loss_ce: 0.009454
2022-01-08 01:59:08,490 iteration 6077 : loss : 0.022950, loss_ce: 0.010179
2022-01-08 01:59:09,910 iteration 6078 : loss : 0.019050, loss_ce: 0.005895
2022-01-08 01:59:11,360 iteration 6079 : loss : 0.028409, loss_ce: 0.005757
2022-01-08 01:59:12,798 iteration 6080 : loss : 0.016248, loss_ce: 0.005171
2022-01-08 01:59:14,135 iteration 6081 : loss : 0.018038, loss_ce: 0.005503
2022-01-08 01:59:15,629 iteration 6082 : loss : 0.021055, loss_ce: 0.007852
2022-01-08 01:59:17,164 iteration 6083 : loss : 0.036928, loss_ce: 0.014885
2022-01-08 01:59:18,543 iteration 6084 : loss : 0.029650, loss_ce: 0.008174
2022-01-08 01:59:19,915 iteration 6085 : loss : 0.015819, loss_ce: 0.007968
2022-01-08 01:59:21,480 iteration 6086 : loss : 0.040571, loss_ce: 0.012896
 90%|█████████████████████████▉   | 358/400 [2:34:45<18:00, 25.72s/it]2022-01-08 01:59:22,969 iteration 6087 : loss : 0.026729, loss_ce: 0.008766
2022-01-08 01:59:24,427 iteration 6088 : loss : 0.019529, loss_ce: 0.006504
2022-01-08 01:59:25,928 iteration 6089 : loss : 0.021017, loss_ce: 0.009569
2022-01-08 01:59:27,344 iteration 6090 : loss : 0.014077, loss_ce: 0.004056
2022-01-08 01:59:28,729 iteration 6091 : loss : 0.012186, loss_ce: 0.003188
2022-01-08 01:59:30,248 iteration 6092 : loss : 0.019202, loss_ce: 0.007215
2022-01-08 01:59:31,808 iteration 6093 : loss : 0.025695, loss_ce: 0.011780
2022-01-08 01:59:33,333 iteration 6094 : loss : 0.022976, loss_ce: 0.011380
2022-01-08 01:59:34,690 iteration 6095 : loss : 0.019821, loss_ce: 0.005372
2022-01-08 01:59:36,227 iteration 6096 : loss : 0.018748, loss_ce: 0.004795
2022-01-08 01:59:37,612 iteration 6097 : loss : 0.015597, loss_ce: 0.007968
2022-01-08 01:59:39,082 iteration 6098 : loss : 0.020156, loss_ce: 0.010952
2022-01-08 01:59:40,519 iteration 6099 : loss : 0.019553, loss_ce: 0.009018
2022-01-08 01:59:41,987 iteration 6100 : loss : 0.024268, loss_ce: 0.007302
2022-01-08 01:59:43,506 iteration 6101 : loss : 0.017386, loss_ce: 0.007589
2022-01-08 01:59:44,883 iteration 6102 : loss : 0.015136, loss_ce: 0.006285
2022-01-08 01:59:46,348 iteration 6103 : loss : 0.028096, loss_ce: 0.009174
 90%|██████████████████████████   | 359/400 [2:35:10<17:24, 25.47s/it]2022-01-08 01:59:47,893 iteration 6104 : loss : 0.018181, loss_ce: 0.007159
2022-01-08 01:59:49,295 iteration 6105 : loss : 0.028457, loss_ce: 0.008993
2022-01-08 01:59:50,665 iteration 6106 : loss : 0.012751, loss_ce: 0.005158
2022-01-08 01:59:52,043 iteration 6107 : loss : 0.017498, loss_ce: 0.008697
2022-01-08 01:59:53,459 iteration 6108 : loss : 0.017540, loss_ce: 0.006017
2022-01-08 01:59:54,928 iteration 6109 : loss : 0.018481, loss_ce: 0.008968
2022-01-08 01:59:56,297 iteration 6110 : loss : 0.015233, loss_ce: 0.006881
2022-01-08 01:59:57,737 iteration 6111 : loss : 0.022948, loss_ce: 0.007566
2022-01-08 01:59:59,227 iteration 6112 : loss : 0.017139, loss_ce: 0.005473
2022-01-08 02:00:00,651 iteration 6113 : loss : 0.019587, loss_ce: 0.007538
2022-01-08 02:00:02,087 iteration 6114 : loss : 0.019560, loss_ce: 0.007151
2022-01-08 02:00:03,612 iteration 6115 : loss : 0.034050, loss_ce: 0.013369
2022-01-08 02:00:05,032 iteration 6116 : loss : 0.012783, loss_ce: 0.003930
2022-01-08 02:00:06,415 iteration 6117 : loss : 0.015243, loss_ce: 0.005969
2022-01-08 02:00:07,879 iteration 6118 : loss : 0.018360, loss_ce: 0.009945
2022-01-08 02:00:09,304 iteration 6119 : loss : 0.019080, loss_ce: 0.007973
2022-01-08 02:00:09,305 Training Data Eval:
2022-01-08 02:00:16,386   Average segmentation loss on training set: 0.0100
2022-01-08 02:00:16,387 Validation Data Eval:
2022-01-08 02:00:18,785   Average segmentation loss on validation set: 0.0680
2022-01-08 02:00:20,258 iteration 6120 : loss : 0.018230, loss_ce: 0.006534
 90%|██████████████████████████   | 360/400 [2:35:44<18:40, 28.00s/it]2022-01-08 02:00:21,770 iteration 6121 : loss : 0.018990, loss_ce: 0.006262
2022-01-08 02:00:23,221 iteration 6122 : loss : 0.014069, loss_ce: 0.005650
2022-01-08 02:00:24,639 iteration 6123 : loss : 0.016871, loss_ce: 0.005917
2022-01-08 02:00:26,148 iteration 6124 : loss : 0.023413, loss_ce: 0.007875
2022-01-08 02:00:27,594 iteration 6125 : loss : 0.014317, loss_ce: 0.004412
2022-01-08 02:00:29,151 iteration 6126 : loss : 0.035983, loss_ce: 0.017376
2022-01-08 02:00:30,702 iteration 6127 : loss : 0.021617, loss_ce: 0.010951
2022-01-08 02:00:32,119 iteration 6128 : loss : 0.019340, loss_ce: 0.006539
2022-01-08 02:00:33,480 iteration 6129 : loss : 0.015433, loss_ce: 0.005198
2022-01-08 02:00:34,932 iteration 6130 : loss : 0.028413, loss_ce: 0.010850
2022-01-08 02:00:36,312 iteration 6131 : loss : 0.027374, loss_ce: 0.007655
2022-01-08 02:00:37,803 iteration 6132 : loss : 0.022826, loss_ce: 0.008884
2022-01-08 02:00:39,281 iteration 6133 : loss : 0.020312, loss_ce: 0.009890
2022-01-08 02:00:40,664 iteration 6134 : loss : 0.015651, loss_ce: 0.005710
2022-01-08 02:00:42,023 iteration 6135 : loss : 0.019188, loss_ce: 0.008740
2022-01-08 02:00:43,398 iteration 6136 : loss : 0.014814, loss_ce: 0.004189
2022-01-08 02:00:44,781 iteration 6137 : loss : 0.016010, loss_ce: 0.005641
 90%|██████████████████████████▏  | 361/400 [2:36:08<17:31, 26.96s/it]2022-01-08 02:00:46,192 iteration 6138 : loss : 0.025806, loss_ce: 0.009270
2022-01-08 02:00:47,579 iteration 6139 : loss : 0.023472, loss_ce: 0.004301
2022-01-08 02:00:48,958 iteration 6140 : loss : 0.016512, loss_ce: 0.005917
2022-01-08 02:00:50,300 iteration 6141 : loss : 0.015552, loss_ce: 0.007237
2022-01-08 02:00:51,739 iteration 6142 : loss : 0.016459, loss_ce: 0.006989
2022-01-08 02:00:53,105 iteration 6143 : loss : 0.019413, loss_ce: 0.008967
2022-01-08 02:00:54,614 iteration 6144 : loss : 0.019655, loss_ce: 0.007961
2022-01-08 02:00:55,989 iteration 6145 : loss : 0.016991, loss_ce: 0.007172
2022-01-08 02:00:57,327 iteration 6146 : loss : 0.014699, loss_ce: 0.004732
2022-01-08 02:00:58,735 iteration 6147 : loss : 0.015947, loss_ce: 0.006121
2022-01-08 02:01:00,136 iteration 6148 : loss : 0.015193, loss_ce: 0.006225
2022-01-08 02:01:01,510 iteration 6149 : loss : 0.015967, loss_ce: 0.005620
2022-01-08 02:01:02,975 iteration 6150 : loss : 0.021793, loss_ce: 0.008247
2022-01-08 02:01:04,560 iteration 6151 : loss : 0.033514, loss_ce: 0.008665
2022-01-08 02:01:05,996 iteration 6152 : loss : 0.017609, loss_ce: 0.007832
2022-01-08 02:01:07,466 iteration 6153 : loss : 0.016022, loss_ce: 0.006318
2022-01-08 02:01:08,999 iteration 6154 : loss : 0.028238, loss_ce: 0.010273
 90%|██████████████████████████▏  | 362/400 [2:36:33<16:33, 26.14s/it]2022-01-08 02:01:10,553 iteration 6155 : loss : 0.019689, loss_ce: 0.008573
2022-01-08 02:01:11,935 iteration 6156 : loss : 0.013267, loss_ce: 0.005697
2022-01-08 02:01:13,360 iteration 6157 : loss : 0.017556, loss_ce: 0.007122
2022-01-08 02:01:14,814 iteration 6158 : loss : 0.017772, loss_ce: 0.006798
2022-01-08 02:01:16,186 iteration 6159 : loss : 0.022807, loss_ce: 0.007412
2022-01-08 02:01:17,606 iteration 6160 : loss : 0.015652, loss_ce: 0.006166
2022-01-08 02:01:19,045 iteration 6161 : loss : 0.018996, loss_ce: 0.005392
2022-01-08 02:01:20,538 iteration 6162 : loss : 0.028224, loss_ce: 0.005458
2022-01-08 02:01:21,984 iteration 6163 : loss : 0.021259, loss_ce: 0.008188
2022-01-08 02:01:23,480 iteration 6164 : loss : 0.021262, loss_ce: 0.009152
2022-01-08 02:01:24,907 iteration 6165 : loss : 0.018006, loss_ce: 0.006395
2022-01-08 02:01:26,337 iteration 6166 : loss : 0.024971, loss_ce: 0.010858
2022-01-08 02:01:27,730 iteration 6167 : loss : 0.021831, loss_ce: 0.008719
2022-01-08 02:01:29,225 iteration 6168 : loss : 0.024908, loss_ce: 0.010807
2022-01-08 02:01:30,682 iteration 6169 : loss : 0.023254, loss_ce: 0.005985
2022-01-08 02:01:32,086 iteration 6170 : loss : 0.020186, loss_ce: 0.007642
2022-01-08 02:01:33,468 iteration 6171 : loss : 0.013828, loss_ce: 0.005430
 91%|██████████████████████████▎  | 363/400 [2:36:57<15:48, 25.63s/it]2022-01-08 02:01:34,904 iteration 6172 : loss : 0.014899, loss_ce: 0.005371
2022-01-08 02:01:36,450 iteration 6173 : loss : 0.018069, loss_ce: 0.006984
2022-01-08 02:01:37,873 iteration 6174 : loss : 0.018433, loss_ce: 0.007325
2022-01-08 02:01:39,364 iteration 6175 : loss : 0.024656, loss_ce: 0.012523
2022-01-08 02:01:40,814 iteration 6176 : loss : 0.017065, loss_ce: 0.006638
2022-01-08 02:01:42,368 iteration 6177 : loss : 0.023655, loss_ce: 0.009976
2022-01-08 02:01:43,810 iteration 6178 : loss : 0.019767, loss_ce: 0.008402
2022-01-08 02:01:45,249 iteration 6179 : loss : 0.015600, loss_ce: 0.007041
2022-01-08 02:01:46,691 iteration 6180 : loss : 0.018346, loss_ce: 0.006883
2022-01-08 02:01:48,006 iteration 6181 : loss : 0.012308, loss_ce: 0.004111
2022-01-08 02:01:49,450 iteration 6182 : loss : 0.023367, loss_ce: 0.006031
2022-01-08 02:01:50,975 iteration 6183 : loss : 0.023185, loss_ce: 0.009592
2022-01-08 02:01:52,428 iteration 6184 : loss : 0.056836, loss_ce: 0.017974
2022-01-08 02:01:53,842 iteration 6185 : loss : 0.017870, loss_ce: 0.006468
2022-01-08 02:01:55,287 iteration 6186 : loss : 0.015031, loss_ce: 0.004893
2022-01-08 02:01:56,802 iteration 6187 : loss : 0.022282, loss_ce: 0.008633
2022-01-08 02:01:58,153 iteration 6188 : loss : 0.015812, loss_ce: 0.005228
 91%|██████████████████████████▍  | 364/400 [2:37:22<15:12, 25.35s/it]2022-01-08 02:01:59,550 iteration 6189 : loss : 0.015949, loss_ce: 0.004080
2022-01-08 02:02:00,886 iteration 6190 : loss : 0.017283, loss_ce: 0.006150
2022-01-08 02:02:02,473 iteration 6191 : loss : 0.021507, loss_ce: 0.007878
2022-01-08 02:02:03,960 iteration 6192 : loss : 0.016826, loss_ce: 0.006097
2022-01-08 02:02:05,263 iteration 6193 : loss : 0.011803, loss_ce: 0.004554
2022-01-08 02:02:06,685 iteration 6194 : loss : 0.015508, loss_ce: 0.005881
2022-01-08 02:02:08,159 iteration 6195 : loss : 0.012796, loss_ce: 0.005088
2022-01-08 02:02:09,626 iteration 6196 : loss : 0.014763, loss_ce: 0.006627
2022-01-08 02:02:11,082 iteration 6197 : loss : 0.016061, loss_ce: 0.008028
2022-01-08 02:02:12,514 iteration 6198 : loss : 0.015793, loss_ce: 0.005548
2022-01-08 02:02:13,952 iteration 6199 : loss : 0.024832, loss_ce: 0.007534
2022-01-08 02:02:15,280 iteration 6200 : loss : 0.016590, loss_ce: 0.009080
2022-01-08 02:02:16,705 iteration 6201 : loss : 0.032791, loss_ce: 0.011200
2022-01-08 02:02:18,068 iteration 6202 : loss : 0.014455, loss_ce: 0.005819
2022-01-08 02:02:19,440 iteration 6203 : loss : 0.014927, loss_ce: 0.003699
2022-01-08 02:02:20,893 iteration 6204 : loss : 0.015044, loss_ce: 0.005928
2022-01-08 02:02:20,894 Training Data Eval:
2022-01-08 02:02:27,972   Average segmentation loss on training set: 0.0100
2022-01-08 02:02:27,972 Validation Data Eval:
2022-01-08 02:02:30,376   Average segmentation loss on validation set: 0.0733
2022-01-08 02:02:31,832 iteration 6205 : loss : 0.018881, loss_ce: 0.004653
 91%|██████████████████████████▍  | 365/400 [2:37:56<16:14, 27.85s/it]2022-01-08 02:02:33,335 iteration 6206 : loss : 0.019628, loss_ce: 0.009502
2022-01-08 02:02:34,735 iteration 6207 : loss : 0.015352, loss_ce: 0.004365
2022-01-08 02:02:36,266 iteration 6208 : loss : 0.023490, loss_ce: 0.007896
2022-01-08 02:02:37,679 iteration 6209 : loss : 0.018197, loss_ce: 0.005024
2022-01-08 02:02:39,050 iteration 6210 : loss : 0.017077, loss_ce: 0.006543
2022-01-08 02:02:40,523 iteration 6211 : loss : 0.019297, loss_ce: 0.008334
2022-01-08 02:02:41,928 iteration 6212 : loss : 0.014887, loss_ce: 0.004976
2022-01-08 02:02:43,316 iteration 6213 : loss : 0.024436, loss_ce: 0.006751
2022-01-08 02:02:44,697 iteration 6214 : loss : 0.024100, loss_ce: 0.004626
2022-01-08 02:02:46,152 iteration 6215 : loss : 0.018695, loss_ce: 0.009498
2022-01-08 02:02:47,588 iteration 6216 : loss : 0.022805, loss_ce: 0.007670
2022-01-08 02:02:48,918 iteration 6217 : loss : 0.028844, loss_ce: 0.009847
2022-01-08 02:02:50,352 iteration 6218 : loss : 0.012751, loss_ce: 0.006050
2022-01-08 02:02:51,757 iteration 6219 : loss : 0.018079, loss_ce: 0.006647
2022-01-08 02:02:53,151 iteration 6220 : loss : 0.016517, loss_ce: 0.005077
2022-01-08 02:02:54,630 iteration 6221 : loss : 0.015009, loss_ce: 0.006718
2022-01-08 02:02:56,042 iteration 6222 : loss : 0.020843, loss_ce: 0.011145
 92%|██████████████████████████▌  | 366/400 [2:38:20<15:09, 26.76s/it]2022-01-08 02:02:57,521 iteration 6223 : loss : 0.027104, loss_ce: 0.006827
2022-01-08 02:02:58,911 iteration 6224 : loss : 0.017403, loss_ce: 0.005403
2022-01-08 02:03:00,299 iteration 6225 : loss : 0.018403, loss_ce: 0.007083
2022-01-08 02:03:01,770 iteration 6226 : loss : 0.020250, loss_ce: 0.008046
2022-01-08 02:03:03,166 iteration 6227 : loss : 0.017534, loss_ce: 0.004131
2022-01-08 02:03:04,596 iteration 6228 : loss : 0.020582, loss_ce: 0.007668
2022-01-08 02:03:05,946 iteration 6229 : loss : 0.014004, loss_ce: 0.005916
2022-01-08 02:03:07,309 iteration 6230 : loss : 0.019417, loss_ce: 0.008217
2022-01-08 02:03:08,714 iteration 6231 : loss : 0.015510, loss_ce: 0.006414
2022-01-08 02:03:10,125 iteration 6232 : loss : 0.018581, loss_ce: 0.007486
2022-01-08 02:03:11,504 iteration 6233 : loss : 0.017575, loss_ce: 0.007486
2022-01-08 02:03:12,929 iteration 6234 : loss : 0.015127, loss_ce: 0.005057
2022-01-08 02:03:14,445 iteration 6235 : loss : 0.024265, loss_ce: 0.005962
2022-01-08 02:03:15,758 iteration 6236 : loss : 0.012694, loss_ce: 0.005141
2022-01-08 02:03:17,135 iteration 6237 : loss : 0.018291, loss_ce: 0.007021
2022-01-08 02:03:18,460 iteration 6238 : loss : 0.010181, loss_ce: 0.003904
2022-01-08 02:03:19,828 iteration 6239 : loss : 0.013075, loss_ce: 0.005461
 92%|██████████████████████████▌  | 367/400 [2:38:44<14:13, 25.87s/it]2022-01-08 02:03:21,308 iteration 6240 : loss : 0.018090, loss_ce: 0.007354
2022-01-08 02:03:22,740 iteration 6241 : loss : 0.035976, loss_ce: 0.019233
2022-01-08 02:03:24,179 iteration 6242 : loss : 0.018462, loss_ce: 0.006753
2022-01-08 02:03:25,501 iteration 6243 : loss : 0.013226, loss_ce: 0.005040
2022-01-08 02:03:26,848 iteration 6244 : loss : 0.015694, loss_ce: 0.004643
2022-01-08 02:03:28,213 iteration 6245 : loss : 0.015394, loss_ce: 0.005732
2022-01-08 02:03:29,566 iteration 6246 : loss : 0.017546, loss_ce: 0.006557
2022-01-08 02:03:30,916 iteration 6247 : loss : 0.017169, loss_ce: 0.004544
2022-01-08 02:03:32,389 iteration 6248 : loss : 0.019689, loss_ce: 0.008138
2022-01-08 02:03:33,755 iteration 6249 : loss : 0.024413, loss_ce: 0.009155
2022-01-08 02:03:35,171 iteration 6250 : loss : 0.021309, loss_ce: 0.008984
2022-01-08 02:03:36,654 iteration 6251 : loss : 0.016985, loss_ce: 0.005750
2022-01-08 02:03:38,118 iteration 6252 : loss : 0.026514, loss_ce: 0.006544
2022-01-08 02:03:39,481 iteration 6253 : loss : 0.019855, loss_ce: 0.007087
2022-01-08 02:03:40,801 iteration 6254 : loss : 0.012531, loss_ce: 0.004985
2022-01-08 02:03:42,244 iteration 6255 : loss : 0.016235, loss_ce: 0.005963
2022-01-08 02:03:43,638 iteration 6256 : loss : 0.016343, loss_ce: 0.005861
 92%|██████████████████████████▋  | 368/400 [2:39:07<13:27, 25.25s/it]2022-01-08 02:03:45,097 iteration 6257 : loss : 0.013340, loss_ce: 0.004188
2022-01-08 02:03:46,436 iteration 6258 : loss : 0.013563, loss_ce: 0.004696
2022-01-08 02:03:47,878 iteration 6259 : loss : 0.020070, loss_ce: 0.007469
2022-01-08 02:03:49,397 iteration 6260 : loss : 0.037788, loss_ce: 0.008397
2022-01-08 02:03:50,839 iteration 6261 : loss : 0.021833, loss_ce: 0.009338
2022-01-08 02:03:52,286 iteration 6262 : loss : 0.015571, loss_ce: 0.005529
2022-01-08 02:03:53,648 iteration 6263 : loss : 0.015031, loss_ce: 0.005061
2022-01-08 02:03:54,987 iteration 6264 : loss : 0.016886, loss_ce: 0.007120
2022-01-08 02:03:56,403 iteration 6265 : loss : 0.017953, loss_ce: 0.006592
2022-01-08 02:03:57,787 iteration 6266 : loss : 0.011478, loss_ce: 0.004265
2022-01-08 02:03:59,352 iteration 6267 : loss : 0.025742, loss_ce: 0.011495
2022-01-08 02:04:00,742 iteration 6268 : loss : 0.015653, loss_ce: 0.005019
2022-01-08 02:04:02,146 iteration 6269 : loss : 0.016356, loss_ce: 0.006607
2022-01-08 02:04:03,539 iteration 6270 : loss : 0.013038, loss_ce: 0.005385
2022-01-08 02:04:05,008 iteration 6271 : loss : 0.022544, loss_ce: 0.009565
2022-01-08 02:04:06,360 iteration 6272 : loss : 0.033831, loss_ce: 0.009270
2022-01-08 02:04:07,844 iteration 6273 : loss : 0.026890, loss_ce: 0.010374
 92%|██████████████████████████▊  | 369/400 [2:39:32<12:53, 24.94s/it]2022-01-08 02:04:09,422 iteration 6274 : loss : 0.022663, loss_ce: 0.008431
2022-01-08 02:04:10,791 iteration 6275 : loss : 0.016881, loss_ce: 0.004873
2022-01-08 02:04:12,119 iteration 6276 : loss : 0.023646, loss_ce: 0.010788
2022-01-08 02:04:13,512 iteration 6277 : loss : 0.014424, loss_ce: 0.005340
2022-01-08 02:04:15,003 iteration 6278 : loss : 0.023934, loss_ce: 0.011278
2022-01-08 02:04:16,476 iteration 6279 : loss : 0.022031, loss_ce: 0.007504
2022-01-08 02:04:17,815 iteration 6280 : loss : 0.016627, loss_ce: 0.005742
2022-01-08 02:04:19,179 iteration 6281 : loss : 0.019166, loss_ce: 0.005204
2022-01-08 02:04:20,586 iteration 6282 : loss : 0.034899, loss_ce: 0.018580
2022-01-08 02:04:22,077 iteration 6283 : loss : 0.016226, loss_ce: 0.006737
2022-01-08 02:04:23,496 iteration 6284 : loss : 0.016832, loss_ce: 0.005315
2022-01-08 02:04:24,911 iteration 6285 : loss : 0.020602, loss_ce: 0.009422
2022-01-08 02:04:26,361 iteration 6286 : loss : 0.020562, loss_ce: 0.009996
2022-01-08 02:04:27,758 iteration 6287 : loss : 0.015289, loss_ce: 0.005776
2022-01-08 02:04:29,070 iteration 6288 : loss : 0.016972, loss_ce: 0.005795
2022-01-08 02:04:30,440 iteration 6289 : loss : 0.018789, loss_ce: 0.007528
2022-01-08 02:04:30,440 Training Data Eval:
2022-01-08 02:04:37,478   Average segmentation loss on training set: 0.0097
2022-01-08 02:04:37,479 Validation Data Eval:
2022-01-08 02:04:39,869   Average segmentation loss on validation set: 0.0711
2022-01-08 02:04:41,252 iteration 6290 : loss : 0.015288, loss_ce: 0.005303
 92%|██████████████████████████▊  | 370/400 [2:40:05<13:44, 27.48s/it]2022-01-08 02:04:42,655 iteration 6291 : loss : 0.016334, loss_ce: 0.005057
2022-01-08 02:04:43,952 iteration 6292 : loss : 0.019750, loss_ce: 0.006658
2022-01-08 02:04:45,320 iteration 6293 : loss : 0.014278, loss_ce: 0.003722
2022-01-08 02:04:46,789 iteration 6294 : loss : 0.020930, loss_ce: 0.006324
2022-01-08 02:04:48,176 iteration 6295 : loss : 0.027450, loss_ce: 0.010307
2022-01-08 02:04:49,490 iteration 6296 : loss : 0.013728, loss_ce: 0.005311
2022-01-08 02:04:50,969 iteration 6297 : loss : 0.018037, loss_ce: 0.007282
2022-01-08 02:04:52,358 iteration 6298 : loss : 0.016168, loss_ce: 0.007514
2022-01-08 02:04:53,740 iteration 6299 : loss : 0.017958, loss_ce: 0.007613
2022-01-08 02:04:55,174 iteration 6300 : loss : 0.025314, loss_ce: 0.005953
2022-01-08 02:04:56,586 iteration 6301 : loss : 0.014047, loss_ce: 0.006701
2022-01-08 02:04:58,053 iteration 6302 : loss : 0.016176, loss_ce: 0.006252
2022-01-08 02:04:59,414 iteration 6303 : loss : 0.013526, loss_ce: 0.005117
2022-01-08 02:05:00,827 iteration 6304 : loss : 0.022951, loss_ce: 0.010899
2022-01-08 02:05:02,190 iteration 6305 : loss : 0.019046, loss_ce: 0.005738
2022-01-08 02:05:03,609 iteration 6306 : loss : 0.019290, loss_ce: 0.010163
2022-01-08 02:05:04,976 iteration 6307 : loss : 0.013913, loss_ce: 0.004611
 93%|██████████████████████████▉  | 371/400 [2:40:29<12:44, 26.35s/it]2022-01-08 02:05:06,443 iteration 6308 : loss : 0.015916, loss_ce: 0.005784
2022-01-08 02:05:07,813 iteration 6309 : loss : 0.020197, loss_ce: 0.005530
2022-01-08 02:05:09,227 iteration 6310 : loss : 0.011995, loss_ce: 0.004821
2022-01-08 02:05:10,683 iteration 6311 : loss : 0.021127, loss_ce: 0.006252
2022-01-08 02:05:12,054 iteration 6312 : loss : 0.015000, loss_ce: 0.006776
2022-01-08 02:05:13,464 iteration 6313 : loss : 0.024913, loss_ce: 0.007196
2022-01-08 02:05:14,899 iteration 6314 : loss : 0.013338, loss_ce: 0.004887
2022-01-08 02:05:16,295 iteration 6315 : loss : 0.018174, loss_ce: 0.007471
2022-01-08 02:05:17,619 iteration 6316 : loss : 0.013857, loss_ce: 0.004140
2022-01-08 02:05:18,930 iteration 6317 : loss : 0.016468, loss_ce: 0.005709
2022-01-08 02:05:20,359 iteration 6318 : loss : 0.039611, loss_ce: 0.015798
2022-01-08 02:05:21,750 iteration 6319 : loss : 0.019794, loss_ce: 0.008216
2022-01-08 02:05:23,140 iteration 6320 : loss : 0.013849, loss_ce: 0.005969
2022-01-08 02:05:24,519 iteration 6321 : loss : 0.016759, loss_ce: 0.006201
2022-01-08 02:05:25,990 iteration 6322 : loss : 0.029134, loss_ce: 0.010146
2022-01-08 02:05:27,508 iteration 6323 : loss : 0.030868, loss_ce: 0.007655
2022-01-08 02:05:28,886 iteration 6324 : loss : 0.017528, loss_ce: 0.007260
 93%|██████████████████████████▉  | 372/400 [2:40:53<11:57, 25.62s/it]2022-01-08 02:05:30,267 iteration 6325 : loss : 0.014259, loss_ce: 0.006775
2022-01-08 02:05:31,588 iteration 6326 : loss : 0.012270, loss_ce: 0.004786
2022-01-08 02:05:33,028 iteration 6327 : loss : 0.017271, loss_ce: 0.006599
2022-01-08 02:05:34,420 iteration 6328 : loss : 0.020747, loss_ce: 0.005932
2022-01-08 02:05:35,755 iteration 6329 : loss : 0.015553, loss_ce: 0.004153
2022-01-08 02:05:37,139 iteration 6330 : loss : 0.010972, loss_ce: 0.003512
2022-01-08 02:05:38,717 iteration 6331 : loss : 0.026975, loss_ce: 0.009274
2022-01-08 02:05:40,094 iteration 6332 : loss : 0.016204, loss_ce: 0.006820
2022-01-08 02:05:41,514 iteration 6333 : loss : 0.013367, loss_ce: 0.003756
2022-01-08 02:05:42,903 iteration 6334 : loss : 0.015810, loss_ce: 0.007186
2022-01-08 02:05:44,390 iteration 6335 : loss : 0.027997, loss_ce: 0.012240
2022-01-08 02:05:45,730 iteration 6336 : loss : 0.009709, loss_ce: 0.003675
2022-01-08 02:05:47,165 iteration 6337 : loss : 0.023970, loss_ce: 0.009197
2022-01-08 02:05:48,654 iteration 6338 : loss : 0.057069, loss_ce: 0.008914
2022-01-08 02:05:50,024 iteration 6339 : loss : 0.016943, loss_ce: 0.007394
2022-01-08 02:05:51,410 iteration 6340 : loss : 0.011691, loss_ce: 0.005765
2022-01-08 02:05:52,821 iteration 6341 : loss : 0.019188, loss_ce: 0.006842
 93%|███████████████████████████  | 373/400 [2:41:17<11:18, 25.11s/it]2022-01-08 02:05:54,260 iteration 6342 : loss : 0.019310, loss_ce: 0.007284
2022-01-08 02:05:55,636 iteration 6343 : loss : 0.016893, loss_ce: 0.004695
2022-01-08 02:05:57,106 iteration 6344 : loss : 0.023480, loss_ce: 0.009434
2022-01-08 02:05:58,523 iteration 6345 : loss : 0.015095, loss_ce: 0.004550
2022-01-08 02:05:59,877 iteration 6346 : loss : 0.014652, loss_ce: 0.005715
2022-01-08 02:06:01,306 iteration 6347 : loss : 0.021637, loss_ce: 0.011651
2022-01-08 02:06:02,670 iteration 6348 : loss : 0.015875, loss_ce: 0.007210
2022-01-08 02:06:04,077 iteration 6349 : loss : 0.019643, loss_ce: 0.006370
2022-01-08 02:06:05,490 iteration 6350 : loss : 0.015511, loss_ce: 0.005250
2022-01-08 02:06:06,903 iteration 6351 : loss : 0.015097, loss_ce: 0.006191
2022-01-08 02:06:08,347 iteration 6352 : loss : 0.021174, loss_ce: 0.007655
2022-01-08 02:06:09,892 iteration 6353 : loss : 0.033479, loss_ce: 0.010900
2022-01-08 02:06:11,262 iteration 6354 : loss : 0.017613, loss_ce: 0.004845
2022-01-08 02:06:12,733 iteration 6355 : loss : 0.015467, loss_ce: 0.005591
2022-01-08 02:06:14,224 iteration 6356 : loss : 0.022317, loss_ce: 0.007624
2022-01-08 02:06:15,681 iteration 6357 : loss : 0.014269, loss_ce: 0.006888
2022-01-08 02:06:17,166 iteration 6358 : loss : 0.020902, loss_ce: 0.006233
 94%|███████████████████████████  | 374/400 [2:41:41<10:46, 24.88s/it]2022-01-08 02:06:18,603 iteration 6359 : loss : 0.014781, loss_ce: 0.005080
2022-01-08 02:06:19,891 iteration 6360 : loss : 0.009803, loss_ce: 0.003613
2022-01-08 02:06:21,365 iteration 6361 : loss : 0.018924, loss_ce: 0.005896
2022-01-08 02:06:22,791 iteration 6362 : loss : 0.015798, loss_ce: 0.006429
2022-01-08 02:06:24,285 iteration 6363 : loss : 0.028681, loss_ce: 0.009687
2022-01-08 02:06:25,729 iteration 6364 : loss : 0.012459, loss_ce: 0.003475
2022-01-08 02:06:27,130 iteration 6365 : loss : 0.026072, loss_ce: 0.011172
2022-01-08 02:06:28,542 iteration 6366 : loss : 0.016604, loss_ce: 0.006477
2022-01-08 02:06:29,999 iteration 6367 : loss : 0.022084, loss_ce: 0.011158
2022-01-08 02:06:31,429 iteration 6368 : loss : 0.022167, loss_ce: 0.005872
2022-01-08 02:06:32,823 iteration 6369 : loss : 0.018040, loss_ce: 0.004270
2022-01-08 02:06:34,271 iteration 6370 : loss : 0.022346, loss_ce: 0.009915
2022-01-08 02:06:35,617 iteration 6371 : loss : 0.012677, loss_ce: 0.005091
2022-01-08 02:06:37,030 iteration 6372 : loss : 0.017458, loss_ce: 0.006841
2022-01-08 02:06:38,376 iteration 6373 : loss : 0.012627, loss_ce: 0.005596
2022-01-08 02:06:39,676 iteration 6374 : loss : 0.012907, loss_ce: 0.004910
2022-01-08 02:06:39,677 Training Data Eval:
2022-01-08 02:06:46,721   Average segmentation loss on training set: 0.0096
2022-01-08 02:06:46,722 Validation Data Eval:
2022-01-08 02:06:49,107   Average segmentation loss on validation set: 0.0680
2022-01-08 02:06:50,533 iteration 6375 : loss : 0.015412, loss_ce: 0.007433
 94%|███████████████████████████▏ | 375/400 [2:42:14<11:25, 27.43s/it]2022-01-08 02:06:52,067 iteration 6376 : loss : 0.019517, loss_ce: 0.007681
2022-01-08 02:06:53,501 iteration 6377 : loss : 0.019835, loss_ce: 0.004970
2022-01-08 02:06:54,870 iteration 6378 : loss : 0.015921, loss_ce: 0.007505
2022-01-08 02:06:56,266 iteration 6379 : loss : 0.018588, loss_ce: 0.008721
2022-01-08 02:06:57,715 iteration 6380 : loss : 0.019886, loss_ce: 0.006223
2022-01-08 02:06:59,209 iteration 6381 : loss : 0.025555, loss_ce: 0.010403
2022-01-08 02:07:00,607 iteration 6382 : loss : 0.015540, loss_ce: 0.008146
2022-01-08 02:07:02,044 iteration 6383 : loss : 0.016723, loss_ce: 0.005483
2022-01-08 02:07:03,504 iteration 6384 : loss : 0.022644, loss_ce: 0.008354
2022-01-08 02:07:04,781 iteration 6385 : loss : 0.013118, loss_ce: 0.004959
2022-01-08 02:07:06,244 iteration 6386 : loss : 0.030155, loss_ce: 0.010478
2022-01-08 02:07:07,675 iteration 6387 : loss : 0.019088, loss_ce: 0.009412
2022-01-08 02:07:09,005 iteration 6388 : loss : 0.012105, loss_ce: 0.005885
2022-01-08 02:07:10,476 iteration 6389 : loss : 0.014600, loss_ce: 0.006182
2022-01-08 02:07:11,897 iteration 6390 : loss : 0.026525, loss_ce: 0.007254
2022-01-08 02:07:13,338 iteration 6391 : loss : 0.015518, loss_ce: 0.006018
2022-01-08 02:07:14,796 iteration 6392 : loss : 0.026024, loss_ce: 0.006370
 94%|███████████████████████████▎ | 376/400 [2:42:39<10:35, 26.48s/it]2022-01-08 02:07:16,221 iteration 6393 : loss : 0.013082, loss_ce: 0.006325
2022-01-08 02:07:17,618 iteration 6394 : loss : 0.014408, loss_ce: 0.006966
2022-01-08 02:07:18,972 iteration 6395 : loss : 0.010250, loss_ce: 0.003122
2022-01-08 02:07:20,392 iteration 6396 : loss : 0.015934, loss_ce: 0.006747
2022-01-08 02:07:21,835 iteration 6397 : loss : 0.022175, loss_ce: 0.006754
2022-01-08 02:07:23,319 iteration 6398 : loss : 0.015927, loss_ce: 0.005621
2022-01-08 02:07:24,651 iteration 6399 : loss : 0.018735, loss_ce: 0.005814
2022-01-08 02:07:26,020 iteration 6400 : loss : 0.014159, loss_ce: 0.005419
2022-01-08 02:07:27,339 iteration 6401 : loss : 0.010725, loss_ce: 0.003913
2022-01-08 02:07:28,796 iteration 6402 : loss : 0.020365, loss_ce: 0.008468
2022-01-08 02:07:30,153 iteration 6403 : loss : 0.015675, loss_ce: 0.006781
2022-01-08 02:07:31,484 iteration 6404 : loss : 0.011930, loss_ce: 0.005677
2022-01-08 02:07:33,011 iteration 6405 : loss : 0.022870, loss_ce: 0.009286
2022-01-08 02:07:34,368 iteration 6406 : loss : 0.011199, loss_ce: 0.004239
2022-01-08 02:07:35,791 iteration 6407 : loss : 0.025254, loss_ce: 0.009489
2022-01-08 02:07:37,222 iteration 6408 : loss : 0.019478, loss_ce: 0.006508
2022-01-08 02:07:38,566 iteration 6409 : loss : 0.011155, loss_ce: 0.003810
 94%|███████████████████████████▎ | 377/400 [2:43:02<09:50, 25.67s/it]2022-01-08 02:07:39,935 iteration 6410 : loss : 0.014725, loss_ce: 0.005721
2022-01-08 02:07:41,323 iteration 6411 : loss : 0.013032, loss_ce: 0.005023
2022-01-08 02:07:42,779 iteration 6412 : loss : 0.015208, loss_ce: 0.004373
2022-01-08 02:07:44,179 iteration 6413 : loss : 0.017355, loss_ce: 0.008243
2022-01-08 02:07:45,619 iteration 6414 : loss : 0.017709, loss_ce: 0.006928
2022-01-08 02:07:47,088 iteration 6415 : loss : 0.013813, loss_ce: 0.004862
2022-01-08 02:07:48,430 iteration 6416 : loss : 0.016477, loss_ce: 0.006268
2022-01-08 02:07:49,861 iteration 6417 : loss : 0.022745, loss_ce: 0.009005
2022-01-08 02:07:51,253 iteration 6418 : loss : 0.018526, loss_ce: 0.007513
2022-01-08 02:07:52,667 iteration 6419 : loss : 0.014201, loss_ce: 0.005526
2022-01-08 02:07:54,084 iteration 6420 : loss : 0.021108, loss_ce: 0.008280
2022-01-08 02:07:55,523 iteration 6421 : loss : 0.014596, loss_ce: 0.005564
2022-01-08 02:07:56,902 iteration 6422 : loss : 0.017742, loss_ce: 0.006799
2022-01-08 02:07:58,324 iteration 6423 : loss : 0.016779, loss_ce: 0.006613
2022-01-08 02:07:59,665 iteration 6424 : loss : 0.014945, loss_ce: 0.004663
2022-01-08 02:08:01,187 iteration 6425 : loss : 0.016823, loss_ce: 0.005850
2022-01-08 02:08:02,527 iteration 6426 : loss : 0.016780, loss_ce: 0.005226
 94%|███████████████████████████▍ | 378/400 [2:43:26<09:13, 25.16s/it]2022-01-08 02:08:03,984 iteration 6427 : loss : 0.018439, loss_ce: 0.007780
2022-01-08 02:08:05,447 iteration 6428 : loss : 0.022225, loss_ce: 0.007187
2022-01-08 02:08:06,829 iteration 6429 : loss : 0.013153, loss_ce: 0.005071
2022-01-08 02:08:08,285 iteration 6430 : loss : 0.024133, loss_ce: 0.010081
2022-01-08 02:08:09,674 iteration 6431 : loss : 0.017215, loss_ce: 0.004534
2022-01-08 02:08:11,091 iteration 6432 : loss : 0.018371, loss_ce: 0.006419
2022-01-08 02:08:12,486 iteration 6433 : loss : 0.015878, loss_ce: 0.004212
2022-01-08 02:08:13,888 iteration 6434 : loss : 0.027960, loss_ce: 0.011508
2022-01-08 02:08:15,186 iteration 6435 : loss : 0.012633, loss_ce: 0.004118
2022-01-08 02:08:16,536 iteration 6436 : loss : 0.012913, loss_ce: 0.006561
2022-01-08 02:08:17,863 iteration 6437 : loss : 0.013529, loss_ce: 0.005078
2022-01-08 02:08:19,171 iteration 6438 : loss : 0.011967, loss_ce: 0.005787
2022-01-08 02:08:20,619 iteration 6439 : loss : 0.023765, loss_ce: 0.007491
2022-01-08 02:08:22,017 iteration 6440 : loss : 0.020153, loss_ce: 0.008383
2022-01-08 02:08:23,444 iteration 6441 : loss : 0.014379, loss_ce: 0.006146
2022-01-08 02:08:24,830 iteration 6442 : loss : 0.012221, loss_ce: 0.005308
2022-01-08 02:08:26,190 iteration 6443 : loss : 0.014479, loss_ce: 0.004923
 95%|███████████████████████████▍ | 379/400 [2:43:50<08:38, 24.71s/it]2022-01-08 02:08:27,589 iteration 6444 : loss : 0.016720, loss_ce: 0.007321
2022-01-08 02:08:28,958 iteration 6445 : loss : 0.010326, loss_ce: 0.003868
2022-01-08 02:08:30,346 iteration 6446 : loss : 0.014131, loss_ce: 0.004968
2022-01-08 02:08:31,733 iteration 6447 : loss : 0.012653, loss_ce: 0.005376
2022-01-08 02:08:33,117 iteration 6448 : loss : 0.020477, loss_ce: 0.004291
2022-01-08 02:08:34,553 iteration 6449 : loss : 0.019718, loss_ce: 0.006908
2022-01-08 02:08:36,032 iteration 6450 : loss : 0.026827, loss_ce: 0.007370
2022-01-08 02:08:37,408 iteration 6451 : loss : 0.023265, loss_ce: 0.009576
2022-01-08 02:08:38,842 iteration 6452 : loss : 0.040797, loss_ce: 0.007689
2022-01-08 02:08:40,214 iteration 6453 : loss : 0.013791, loss_ce: 0.005882
2022-01-08 02:08:41,620 iteration 6454 : loss : 0.015714, loss_ce: 0.005522
2022-01-08 02:08:42,959 iteration 6455 : loss : 0.014847, loss_ce: 0.005829
2022-01-08 02:08:44,364 iteration 6456 : loss : 0.022981, loss_ce: 0.011794
2022-01-08 02:08:45,731 iteration 6457 : loss : 0.013622, loss_ce: 0.005585
2022-01-08 02:08:47,095 iteration 6458 : loss : 0.015267, loss_ce: 0.008293
2022-01-08 02:08:48,543 iteration 6459 : loss : 0.017077, loss_ce: 0.006702
2022-01-08 02:08:48,543 Training Data Eval:
2022-01-08 02:08:55,587   Average segmentation loss on training set: 0.0094
2022-01-08 02:08:55,587 Validation Data Eval:
2022-01-08 02:08:57,984   Average segmentation loss on validation set: 0.0640
2022-01-08 02:08:59,492 iteration 6460 : loss : 0.015171, loss_ce: 0.005115
 95%|███████████████████████████▌ | 380/400 [2:44:23<09:05, 27.28s/it]2022-01-08 02:09:00,901 iteration 6461 : loss : 0.013247, loss_ce: 0.004348
2022-01-08 02:09:02,301 iteration 6462 : loss : 0.014974, loss_ce: 0.006287
2022-01-08 02:09:03,637 iteration 6463 : loss : 0.014500, loss_ce: 0.006510
2022-01-08 02:09:05,023 iteration 6464 : loss : 0.013894, loss_ce: 0.005264
2022-01-08 02:09:06,410 iteration 6465 : loss : 0.022407, loss_ce: 0.009507
2022-01-08 02:09:07,790 iteration 6466 : loss : 0.015114, loss_ce: 0.005796
2022-01-08 02:09:09,183 iteration 6467 : loss : 0.016344, loss_ce: 0.007243
2022-01-08 02:09:10,522 iteration 6468 : loss : 0.016314, loss_ce: 0.004828
2022-01-08 02:09:11,986 iteration 6469 : loss : 0.022590, loss_ce: 0.005499
2022-01-08 02:09:13,402 iteration 6470 : loss : 0.013171, loss_ce: 0.005496
2022-01-08 02:09:14,809 iteration 6471 : loss : 0.017485, loss_ce: 0.006578
2022-01-08 02:09:16,151 iteration 6472 : loss : 0.016630, loss_ce: 0.006102
2022-01-08 02:09:17,601 iteration 6473 : loss : 0.015986, loss_ce: 0.005998
2022-01-08 02:09:19,019 iteration 6474 : loss : 0.012319, loss_ce: 0.004575
2022-01-08 02:09:20,458 iteration 6475 : loss : 0.016075, loss_ce: 0.005440
2022-01-08 02:09:21,879 iteration 6476 : loss : 0.015833, loss_ce: 0.005715
2022-01-08 02:09:23,313 iteration 6477 : loss : 0.018075, loss_ce: 0.007527
 95%|███████████████████████████▌ | 381/400 [2:44:47<08:18, 26.25s/it]2022-01-08 02:09:24,747 iteration 6478 : loss : 0.015099, loss_ce: 0.006152
2022-01-08 02:09:26,142 iteration 6479 : loss : 0.019714, loss_ce: 0.006277
2022-01-08 02:09:27,505 iteration 6480 : loss : 0.012457, loss_ce: 0.004573
2022-01-08 02:09:29,070 iteration 6481 : loss : 0.027049, loss_ce: 0.009763
2022-01-08 02:09:30,441 iteration 6482 : loss : 0.017559, loss_ce: 0.007528
2022-01-08 02:09:31,890 iteration 6483 : loss : 0.017711, loss_ce: 0.007434
2022-01-08 02:09:33,192 iteration 6484 : loss : 0.013853, loss_ce: 0.004558
2022-01-08 02:09:34,591 iteration 6485 : loss : 0.017448, loss_ce: 0.005584
2022-01-08 02:09:36,023 iteration 6486 : loss : 0.015660, loss_ce: 0.005487
2022-01-08 02:09:37,454 iteration 6487 : loss : 0.016955, loss_ce: 0.007271
2022-01-08 02:09:38,896 iteration 6488 : loss : 0.012643, loss_ce: 0.004534
2022-01-08 02:09:40,296 iteration 6489 : loss : 0.012787, loss_ce: 0.003918
2022-01-08 02:09:41,683 iteration 6490 : loss : 0.013175, loss_ce: 0.004607
2022-01-08 02:09:43,030 iteration 6491 : loss : 0.019171, loss_ce: 0.008930
2022-01-08 02:09:44,375 iteration 6492 : loss : 0.012751, loss_ce: 0.006391
2022-01-08 02:09:45,749 iteration 6493 : loss : 0.020338, loss_ce: 0.007462
2022-01-08 02:09:47,187 iteration 6494 : loss : 0.018524, loss_ce: 0.007993
 96%|███████████████████████████▋ | 382/400 [2:45:11<07:39, 25.53s/it]2022-01-08 02:09:48,555 iteration 6495 : loss : 0.013413, loss_ce: 0.005850
2022-01-08 02:09:49,938 iteration 6496 : loss : 0.019334, loss_ce: 0.006883
2022-01-08 02:09:51,362 iteration 6497 : loss : 0.012435, loss_ce: 0.006485
2022-01-08 02:09:52,735 iteration 6498 : loss : 0.016346, loss_ce: 0.004381
2022-01-08 02:09:54,113 iteration 6499 : loss : 0.022961, loss_ce: 0.005590
2022-01-08 02:09:55,607 iteration 6500 : loss : 0.035465, loss_ce: 0.010716
2022-01-08 02:09:57,073 iteration 6501 : loss : 0.027416, loss_ce: 0.014576
2022-01-08 02:09:58,488 iteration 6502 : loss : 0.017574, loss_ce: 0.007816
2022-01-08 02:09:59,917 iteration 6503 : loss : 0.015335, loss_ce: 0.005615
2022-01-08 02:10:01,340 iteration 6504 : loss : 0.016333, loss_ce: 0.006813
2022-01-08 02:10:02,672 iteration 6505 : loss : 0.012896, loss_ce: 0.004785
2022-01-08 02:10:04,087 iteration 6506 : loss : 0.024491, loss_ce: 0.006323
2022-01-08 02:10:05,477 iteration 6507 : loss : 0.015017, loss_ce: 0.006237
2022-01-08 02:10:06,783 iteration 6508 : loss : 0.013785, loss_ce: 0.005318
2022-01-08 02:10:08,159 iteration 6509 : loss : 0.017008, loss_ce: 0.005788
2022-01-08 02:10:09,442 iteration 6510 : loss : 0.010473, loss_ce: 0.003989
2022-01-08 02:10:10,876 iteration 6511 : loss : 0.018521, loss_ce: 0.007067
 96%|███████████████████████████▊ | 383/400 [2:45:35<07:04, 24.98s/it]2022-01-08 02:10:12,357 iteration 6512 : loss : 0.025640, loss_ce: 0.007298
2022-01-08 02:10:13,815 iteration 6513 : loss : 0.016884, loss_ce: 0.006283
2022-01-08 02:10:15,226 iteration 6514 : loss : 0.016900, loss_ce: 0.004619
2022-01-08 02:10:16,702 iteration 6515 : loss : 0.020313, loss_ce: 0.007267
2022-01-08 02:10:18,069 iteration 6516 : loss : 0.014755, loss_ce: 0.006218
2022-01-08 02:10:19,414 iteration 6517 : loss : 0.015486, loss_ce: 0.008595
2022-01-08 02:10:20,828 iteration 6518 : loss : 0.020353, loss_ce: 0.007389
2022-01-08 02:10:22,292 iteration 6519 : loss : 0.033482, loss_ce: 0.015554
2022-01-08 02:10:23,633 iteration 6520 : loss : 0.019965, loss_ce: 0.009813
2022-01-08 02:10:25,060 iteration 6521 : loss : 0.018751, loss_ce: 0.008982
2022-01-08 02:10:26,467 iteration 6522 : loss : 0.016860, loss_ce: 0.005413
2022-01-08 02:10:27,892 iteration 6523 : loss : 0.015162, loss_ce: 0.005852
2022-01-08 02:10:29,338 iteration 6524 : loss : 0.017589, loss_ce: 0.006491
2022-01-08 02:10:30,813 iteration 6525 : loss : 0.016313, loss_ce: 0.005486
2022-01-08 02:10:32,209 iteration 6526 : loss : 0.013661, loss_ce: 0.005210
2022-01-08 02:10:33,562 iteration 6527 : loss : 0.017070, loss_ce: 0.005900
2022-01-08 02:10:34,933 iteration 6528 : loss : 0.013344, loss_ce: 0.005344
 96%|███████████████████████████▊ | 384/400 [2:45:59<06:35, 24.70s/it]2022-01-08 02:10:36,569 iteration 6529 : loss : 0.023534, loss_ce: 0.009943
2022-01-08 02:10:37,961 iteration 6530 : loss : 0.016873, loss_ce: 0.007326
2022-01-08 02:10:39,453 iteration 6531 : loss : 0.023152, loss_ce: 0.008680
2022-01-08 02:10:40,864 iteration 6532 : loss : 0.018080, loss_ce: 0.007470
2022-01-08 02:10:42,343 iteration 6533 : loss : 0.054630, loss_ce: 0.009595
2022-01-08 02:10:43,888 iteration 6534 : loss : 0.023096, loss_ce: 0.006067
2022-01-08 02:10:45,223 iteration 6535 : loss : 0.015093, loss_ce: 0.002587
2022-01-08 02:10:46,620 iteration 6536 : loss : 0.024228, loss_ce: 0.009192
2022-01-08 02:10:48,051 iteration 6537 : loss : 0.013485, loss_ce: 0.004834
2022-01-08 02:10:49,472 iteration 6538 : loss : 0.016499, loss_ce: 0.008011
2022-01-08 02:10:50,968 iteration 6539 : loss : 0.015976, loss_ce: 0.006047
2022-01-08 02:10:52,370 iteration 6540 : loss : 0.019432, loss_ce: 0.009558
2022-01-08 02:10:53,774 iteration 6541 : loss : 0.012782, loss_ce: 0.005069
2022-01-08 02:10:55,245 iteration 6542 : loss : 0.026926, loss_ce: 0.012511
2022-01-08 02:10:56,674 iteration 6543 : loss : 0.028997, loss_ce: 0.010615
2022-01-08 02:10:57,995 iteration 6544 : loss : 0.010739, loss_ce: 0.003518
2022-01-08 02:10:57,995 Training Data Eval:
2022-01-08 02:11:05,033   Average segmentation loss on training set: 0.0094
2022-01-08 02:11:05,033 Validation Data Eval:
2022-01-08 02:11:07,410   Average segmentation loss on validation set: 0.0678
2022-01-08 02:11:08,689 iteration 6545 : loss : 0.010426, loss_ce: 0.002496
 96%|███████████████████████████▉ | 385/400 [2:46:32<06:51, 27.42s/it]2022-01-08 02:11:10,149 iteration 6546 : loss : 0.029122, loss_ce: 0.009244
2022-01-08 02:11:11,777 iteration 6547 : loss : 0.032061, loss_ce: 0.011170
2022-01-08 02:11:13,103 iteration 6548 : loss : 0.016058, loss_ce: 0.010668
2022-01-08 02:11:14,402 iteration 6549 : loss : 0.010083, loss_ce: 0.003498
2022-01-08 02:11:15,765 iteration 6550 : loss : 0.012686, loss_ce: 0.005211
2022-01-08 02:11:17,219 iteration 6551 : loss : 0.017253, loss_ce: 0.008492
2022-01-08 02:11:18,522 iteration 6552 : loss : 0.013243, loss_ce: 0.006288
2022-01-08 02:11:19,986 iteration 6553 : loss : 0.020832, loss_ce: 0.007757
2022-01-08 02:11:21,381 iteration 6554 : loss : 0.015675, loss_ce: 0.004914
2022-01-08 02:11:22,849 iteration 6555 : loss : 0.019417, loss_ce: 0.007503
2022-01-08 02:11:24,319 iteration 6556 : loss : 0.016586, loss_ce: 0.004375
2022-01-08 02:11:25,665 iteration 6557 : loss : 0.017673, loss_ce: 0.006005
2022-01-08 02:11:27,010 iteration 6558 : loss : 0.018421, loss_ce: 0.005239
2022-01-08 02:11:28,435 iteration 6559 : loss : 0.020269, loss_ce: 0.010281
2022-01-08 02:11:29,963 iteration 6560 : loss : 0.049552, loss_ce: 0.012841
2022-01-08 02:11:31,317 iteration 6561 : loss : 0.016736, loss_ce: 0.006718
2022-01-08 02:11:32,620 iteration 6562 : loss : 0.010031, loss_ce: 0.003165
 96%|███████████████████████████▉ | 386/400 [2:46:56<06:09, 26.37s/it]2022-01-08 02:11:34,037 iteration 6563 : loss : 0.012008, loss_ce: 0.005321
2022-01-08 02:11:35,406 iteration 6564 : loss : 0.020871, loss_ce: 0.006223
2022-01-08 02:11:36,737 iteration 6565 : loss : 0.012699, loss_ce: 0.003354
2022-01-08 02:11:38,095 iteration 6566 : loss : 0.012795, loss_ce: 0.005603
2022-01-08 02:11:39,513 iteration 6567 : loss : 0.019883, loss_ce: 0.007328
2022-01-08 02:11:40,925 iteration 6568 : loss : 0.017664, loss_ce: 0.007084
2022-01-08 02:11:42,410 iteration 6569 : loss : 0.027548, loss_ce: 0.011651
2022-01-08 02:11:43,802 iteration 6570 : loss : 0.018826, loss_ce: 0.004765
2022-01-08 02:11:45,326 iteration 6571 : loss : 0.026108, loss_ce: 0.012555
2022-01-08 02:11:46,662 iteration 6572 : loss : 0.011085, loss_ce: 0.004790
2022-01-08 02:11:48,140 iteration 6573 : loss : 0.023534, loss_ce: 0.007348
2022-01-08 02:11:49,514 iteration 6574 : loss : 0.012311, loss_ce: 0.003683
2022-01-08 02:11:50,984 iteration 6575 : loss : 0.029074, loss_ce: 0.009815
2022-01-08 02:11:52,386 iteration 6576 : loss : 0.017284, loss_ce: 0.007285
2022-01-08 02:11:53,768 iteration 6577 : loss : 0.012558, loss_ce: 0.004086
2022-01-08 02:11:55,195 iteration 6578 : loss : 0.013423, loss_ce: 0.005203
2022-01-08 02:11:56,561 iteration 6579 : loss : 0.017040, loss_ce: 0.007379
 97%|████████████████████████████ | 387/400 [2:47:20<05:33, 25.64s/it]2022-01-08 02:11:58,049 iteration 6580 : loss : 0.020863, loss_ce: 0.008432
2022-01-08 02:11:59,410 iteration 6581 : loss : 0.012625, loss_ce: 0.003679
2022-01-08 02:12:00,906 iteration 6582 : loss : 0.015955, loss_ce: 0.005213
2022-01-08 02:12:02,312 iteration 6583 : loss : 0.026231, loss_ce: 0.009668
2022-01-08 02:12:03,662 iteration 6584 : loss : 0.027715, loss_ce: 0.011364
2022-01-08 02:12:04,973 iteration 6585 : loss : 0.014251, loss_ce: 0.004346
2022-01-08 02:12:06,334 iteration 6586 : loss : 0.013040, loss_ce: 0.002842
2022-01-08 02:12:07,653 iteration 6587 : loss : 0.014372, loss_ce: 0.006541
2022-01-08 02:12:09,055 iteration 6588 : loss : 0.011545, loss_ce: 0.005484
2022-01-08 02:12:10,466 iteration 6589 : loss : 0.021029, loss_ce: 0.008709
2022-01-08 02:12:11,859 iteration 6590 : loss : 0.013248, loss_ce: 0.005693
2022-01-08 02:12:13,235 iteration 6591 : loss : 0.010833, loss_ce: 0.004105
2022-01-08 02:12:14,707 iteration 6592 : loss : 0.017459, loss_ce: 0.009686
2022-01-08 02:12:16,013 iteration 6593 : loss : 0.011514, loss_ce: 0.005978
2022-01-08 02:12:17,391 iteration 6594 : loss : 0.014281, loss_ce: 0.005521
2022-01-08 02:12:18,843 iteration 6595 : loss : 0.022589, loss_ce: 0.007334
2022-01-08 02:12:20,179 iteration 6596 : loss : 0.012595, loss_ce: 0.003488
 97%|████████████████████████████▏| 388/400 [2:47:44<05:00, 25.04s/it]2022-01-08 02:12:21,570 iteration 6597 : loss : 0.011500, loss_ce: 0.005027
2022-01-08 02:12:23,013 iteration 6598 : loss : 0.019587, loss_ce: 0.003249
2022-01-08 02:12:24,497 iteration 6599 : loss : 0.016765, loss_ce: 0.006495
2022-01-08 02:12:25,837 iteration 6600 : loss : 0.012708, loss_ce: 0.004861
2022-01-08 02:12:27,331 iteration 6601 : loss : 0.017032, loss_ce: 0.006684
2022-01-08 02:12:28,781 iteration 6602 : loss : 0.018729, loss_ce: 0.007069
2022-01-08 02:12:30,161 iteration 6603 : loss : 0.013296, loss_ce: 0.005869
2022-01-08 02:12:31,596 iteration 6604 : loss : 0.018995, loss_ce: 0.009695
2022-01-08 02:12:32,972 iteration 6605 : loss : 0.018562, loss_ce: 0.008401
2022-01-08 02:12:34,439 iteration 6606 : loss : 0.017652, loss_ce: 0.006036
2022-01-08 02:12:35,941 iteration 6607 : loss : 0.021089, loss_ce: 0.008208
2022-01-08 02:12:37,279 iteration 6608 : loss : 0.014548, loss_ce: 0.006343
2022-01-08 02:12:38,729 iteration 6609 : loss : 0.018423, loss_ce: 0.008221
2022-01-08 02:12:40,109 iteration 6610 : loss : 0.015417, loss_ce: 0.005881
2022-01-08 02:12:41,497 iteration 6611 : loss : 0.011127, loss_ce: 0.002849
2022-01-08 02:12:42,889 iteration 6612 : loss : 0.014563, loss_ce: 0.004077
2022-01-08 02:12:44,331 iteration 6613 : loss : 0.015156, loss_ce: 0.004912
 97%|████████████████████████████▏| 389/400 [2:48:08<04:32, 24.77s/it]2022-01-08 02:12:45,727 iteration 6614 : loss : 0.008527, loss_ce: 0.003209
2022-01-08 02:12:47,046 iteration 6615 : loss : 0.010050, loss_ce: 0.003878
2022-01-08 02:12:48,472 iteration 6616 : loss : 0.016802, loss_ce: 0.005700
2022-01-08 02:12:49,846 iteration 6617 : loss : 0.021008, loss_ce: 0.007569
2022-01-08 02:12:51,323 iteration 6618 : loss : 0.019192, loss_ce: 0.007215
2022-01-08 02:12:52,728 iteration 6619 : loss : 0.014879, loss_ce: 0.005027
2022-01-08 02:12:54,160 iteration 6620 : loss : 0.018885, loss_ce: 0.005762
2022-01-08 02:12:55,615 iteration 6621 : loss : 0.022680, loss_ce: 0.008285
2022-01-08 02:12:56,989 iteration 6622 : loss : 0.017058, loss_ce: 0.008862
2022-01-08 02:12:58,325 iteration 6623 : loss : 0.018082, loss_ce: 0.006901
2022-01-08 02:12:59,733 iteration 6624 : loss : 0.013740, loss_ce: 0.005452
2022-01-08 02:13:01,144 iteration 6625 : loss : 0.025512, loss_ce: 0.008581
2022-01-08 02:13:02,536 iteration 6626 : loss : 0.017478, loss_ce: 0.006683
2022-01-08 02:13:03,968 iteration 6627 : loss : 0.019399, loss_ce: 0.007073
2022-01-08 02:13:05,419 iteration 6628 : loss : 0.017958, loss_ce: 0.008410
2022-01-08 02:13:06,963 iteration 6629 : loss : 0.022469, loss_ce: 0.006295
2022-01-08 02:13:06,963 Training Data Eval:
2022-01-08 02:13:13,990   Average segmentation loss on training set: 0.0092
2022-01-08 02:13:13,990 Validation Data Eval:
2022-01-08 02:13:16,369   Average segmentation loss on validation set: 0.0726
2022-01-08 02:13:17,751 iteration 6630 : loss : 0.014755, loss_ce: 0.005746
 98%|████████████████████████████▎| 390/400 [2:48:41<04:33, 27.36s/it]2022-01-08 02:13:19,178 iteration 6631 : loss : 0.017295, loss_ce: 0.005802
2022-01-08 02:13:20,535 iteration 6632 : loss : 0.012137, loss_ce: 0.005615
2022-01-08 02:13:21,963 iteration 6633 : loss : 0.020577, loss_ce: 0.005476
2022-01-08 02:13:23,350 iteration 6634 : loss : 0.026625, loss_ce: 0.015561
2022-01-08 02:13:24,764 iteration 6635 : loss : 0.015516, loss_ce: 0.006051
2022-01-08 02:13:26,165 iteration 6636 : loss : 0.014420, loss_ce: 0.008096
2022-01-08 02:13:27,517 iteration 6637 : loss : 0.016196, loss_ce: 0.005482
2022-01-08 02:13:28,967 iteration 6638 : loss : 0.031299, loss_ce: 0.006637
2022-01-08 02:13:30,450 iteration 6639 : loss : 0.015511, loss_ce: 0.006014
2022-01-08 02:13:31,833 iteration 6640 : loss : 0.017688, loss_ce: 0.005111
2022-01-08 02:13:33,246 iteration 6641 : loss : 0.018058, loss_ce: 0.006797
2022-01-08 02:13:34,655 iteration 6642 : loss : 0.017149, loss_ce: 0.007917
2022-01-08 02:13:36,089 iteration 6643 : loss : 0.019242, loss_ce: 0.006921
2022-01-08 02:13:37,606 iteration 6644 : loss : 0.032950, loss_ce: 0.011731
2022-01-08 02:13:38,978 iteration 6645 : loss : 0.014037, loss_ce: 0.005997
2022-01-08 02:13:40,483 iteration 6646 : loss : 0.024732, loss_ce: 0.007045
2022-01-08 02:13:41,895 iteration 6647 : loss : 0.020919, loss_ce: 0.007000
 98%|████████████████████████████▎| 391/400 [2:49:06<03:57, 26.40s/it]2022-01-08 02:13:43,310 iteration 6648 : loss : 0.018967, loss_ce: 0.003871
2022-01-08 02:13:44,744 iteration 6649 : loss : 0.018928, loss_ce: 0.005035
2022-01-08 02:13:46,226 iteration 6650 : loss : 0.036839, loss_ce: 0.010784
2022-01-08 02:13:47,773 iteration 6651 : loss : 0.018762, loss_ce: 0.007915
2022-01-08 02:13:49,169 iteration 6652 : loss : 0.013868, loss_ce: 0.006095
2022-01-08 02:13:50,497 iteration 6653 : loss : 0.011158, loss_ce: 0.004506
2022-01-08 02:13:51,879 iteration 6654 : loss : 0.015685, loss_ce: 0.005331
2022-01-08 02:13:53,319 iteration 6655 : loss : 0.019007, loss_ce: 0.006876
2022-01-08 02:13:54,773 iteration 6656 : loss : 0.023255, loss_ce: 0.009017
2022-01-08 02:13:56,188 iteration 6657 : loss : 0.019252, loss_ce: 0.004898
2022-01-08 02:13:57,630 iteration 6658 : loss : 0.018902, loss_ce: 0.006506
2022-01-08 02:13:59,037 iteration 6659 : loss : 0.018930, loss_ce: 0.008135
2022-01-08 02:14:00,527 iteration 6660 : loss : 0.023989, loss_ce: 0.010439
2022-01-08 02:14:01,952 iteration 6661 : loss : 0.017710, loss_ce: 0.007088
2022-01-08 02:14:03,389 iteration 6662 : loss : 0.024378, loss_ce: 0.005865
2022-01-08 02:14:04,803 iteration 6663 : loss : 0.019880, loss_ce: 0.006907
2022-01-08 02:14:06,281 iteration 6664 : loss : 0.020021, loss_ce: 0.010977
 98%|████████████████████████████▍| 392/400 [2:49:30<03:26, 25.80s/it]2022-01-08 02:14:07,748 iteration 6665 : loss : 0.021904, loss_ce: 0.006818
2022-01-08 02:14:09,261 iteration 6666 : loss : 0.029705, loss_ce: 0.008558
2022-01-08 02:14:10,694 iteration 6667 : loss : 0.026240, loss_ce: 0.004394
2022-01-08 02:14:12,034 iteration 6668 : loss : 0.014752, loss_ce: 0.006686
2022-01-08 02:14:13,418 iteration 6669 : loss : 0.015174, loss_ce: 0.005893
2022-01-08 02:14:14,804 iteration 6670 : loss : 0.018213, loss_ce: 0.008577
2022-01-08 02:14:16,227 iteration 6671 : loss : 0.018884, loss_ce: 0.006888
2022-01-08 02:14:17,699 iteration 6672 : loss : 0.015761, loss_ce: 0.004203
2022-01-08 02:14:19,113 iteration 6673 : loss : 0.019553, loss_ce: 0.005416
2022-01-08 02:14:20,507 iteration 6674 : loss : 0.020505, loss_ce: 0.009685
2022-01-08 02:14:21,983 iteration 6675 : loss : 0.022259, loss_ce: 0.010885
2022-01-08 02:14:23,369 iteration 6676 : loss : 0.017606, loss_ce: 0.008179
2022-01-08 02:14:24,721 iteration 6677 : loss : 0.014743, loss_ce: 0.005116
2022-01-08 02:14:26,113 iteration 6678 : loss : 0.012533, loss_ce: 0.005796
2022-01-08 02:14:27,565 iteration 6679 : loss : 0.019043, loss_ce: 0.008458
2022-01-08 02:14:28,910 iteration 6680 : loss : 0.011005, loss_ce: 0.003974
2022-01-08 02:14:30,251 iteration 6681 : loss : 0.012343, loss_ce: 0.004199
 98%|████████████████████████████▍| 393/400 [2:49:54<02:56, 25.25s/it]2022-01-08 02:14:31,781 iteration 6682 : loss : 0.015064, loss_ce: 0.006923
2022-01-08 02:14:33,135 iteration 6683 : loss : 0.012872, loss_ce: 0.005063
2022-01-08 02:14:34,533 iteration 6684 : loss : 0.015574, loss_ce: 0.008042
2022-01-08 02:14:35,898 iteration 6685 : loss : 0.019330, loss_ce: 0.006999
2022-01-08 02:14:37,369 iteration 6686 : loss : 0.022769, loss_ce: 0.009834
2022-01-08 02:14:38,757 iteration 6687 : loss : 0.020659, loss_ce: 0.009005
2022-01-08 02:14:40,158 iteration 6688 : loss : 0.016817, loss_ce: 0.006467
2022-01-08 02:14:41,488 iteration 6689 : loss : 0.011460, loss_ce: 0.003105
2022-01-08 02:14:42,902 iteration 6690 : loss : 0.009882, loss_ce: 0.002868
2022-01-08 02:14:44,338 iteration 6691 : loss : 0.019433, loss_ce: 0.006984
2022-01-08 02:14:45,701 iteration 6692 : loss : 0.024984, loss_ce: 0.009482
2022-01-08 02:14:47,088 iteration 6693 : loss : 0.020301, loss_ce: 0.008538
2022-01-08 02:14:48,514 iteration 6694 : loss : 0.017988, loss_ce: 0.006030
2022-01-08 02:14:49,878 iteration 6695 : loss : 0.015830, loss_ce: 0.005135
2022-01-08 02:14:51,276 iteration 6696 : loss : 0.012714, loss_ce: 0.004963
2022-01-08 02:14:52,663 iteration 6697 : loss : 0.017022, loss_ce: 0.005940
2022-01-08 02:14:54,171 iteration 6698 : loss : 0.015019, loss_ce: 0.004999
 98%|████████████████████████████▌| 394/400 [2:50:18<02:29, 24.85s/it]2022-01-08 02:14:55,583 iteration 6699 : loss : 0.014117, loss_ce: 0.004999
2022-01-08 02:14:57,043 iteration 6700 : loss : 0.017984, loss_ce: 0.005657
2022-01-08 02:14:58,432 iteration 6701 : loss : 0.018799, loss_ce: 0.005926
2022-01-08 02:14:59,770 iteration 6702 : loss : 0.017268, loss_ce: 0.008696
2022-01-08 02:15:01,103 iteration 6703 : loss : 0.013570, loss_ce: 0.004391
2022-01-08 02:15:02,541 iteration 6704 : loss : 0.019998, loss_ce: 0.010796
2022-01-08 02:15:03,888 iteration 6705 : loss : 0.016330, loss_ce: 0.007775
2022-01-08 02:15:05,351 iteration 6706 : loss : 0.016149, loss_ce: 0.005288
2022-01-08 02:15:06,701 iteration 6707 : loss : 0.011569, loss_ce: 0.004080
2022-01-08 02:15:08,095 iteration 6708 : loss : 0.016695, loss_ce: 0.006397
2022-01-08 02:15:09,500 iteration 6709 : loss : 0.022947, loss_ce: 0.010847
2022-01-08 02:15:10,935 iteration 6710 : loss : 0.020059, loss_ce: 0.004985
2022-01-08 02:15:12,398 iteration 6711 : loss : 0.019079, loss_ce: 0.005446
2022-01-08 02:15:13,885 iteration 6712 : loss : 0.019574, loss_ce: 0.006903
2022-01-08 02:15:15,223 iteration 6713 : loss : 0.015673, loss_ce: 0.005957
2022-01-08 02:15:16,626 iteration 6714 : loss : 0.017662, loss_ce: 0.006664
2022-01-08 02:15:16,627 Training Data Eval:
2022-01-08 02:15:23,668   Average segmentation loss on training set: 0.0093
2022-01-08 02:15:23,668 Validation Data Eval:
2022-01-08 02:15:26,057   Average segmentation loss on validation set: 0.0648
2022-01-08 02:15:27,400 iteration 6715 : loss : 0.015180, loss_ce: 0.005442
 99%|████████████████████████████▋| 395/400 [2:50:51<02:16, 27.36s/it]2022-01-08 02:15:28,818 iteration 6716 : loss : 0.014679, loss_ce: 0.006504
2022-01-08 02:15:30,202 iteration 6717 : loss : 0.012453, loss_ce: 0.005283
2022-01-08 02:15:31,608 iteration 6718 : loss : 0.018200, loss_ce: 0.006297
2022-01-08 02:15:32,960 iteration 6719 : loss : 0.018337, loss_ce: 0.006818
2022-01-08 02:15:34,324 iteration 6720 : loss : 0.012540, loss_ce: 0.003593
2022-01-08 02:15:35,677 iteration 6721 : loss : 0.013178, loss_ce: 0.004195
2022-01-08 02:15:37,148 iteration 6722 : loss : 0.013160, loss_ce: 0.004882
2022-01-08 02:15:38,448 iteration 6723 : loss : 0.015363, loss_ce: 0.005687
2022-01-08 02:15:39,837 iteration 6724 : loss : 0.020374, loss_ce: 0.006707
2022-01-08 02:15:41,255 iteration 6725 : loss : 0.010666, loss_ce: 0.003361
2022-01-08 02:15:42,730 iteration 6726 : loss : 0.018047, loss_ce: 0.008522
2022-01-08 02:15:44,253 iteration 6727 : loss : 0.020941, loss_ce: 0.007844
2022-01-08 02:15:45,644 iteration 6728 : loss : 0.019320, loss_ce: 0.007187
2022-01-08 02:15:47,031 iteration 6729 : loss : 0.029254, loss_ce: 0.008934
2022-01-08 02:15:48,395 iteration 6730 : loss : 0.012314, loss_ce: 0.005033
2022-01-08 02:15:49,781 iteration 6731 : loss : 0.015930, loss_ce: 0.005835
2022-01-08 02:15:51,212 iteration 6732 : loss : 0.021405, loss_ce: 0.008375
 99%|████████████████████████████▋| 396/400 [2:51:15<01:45, 26.30s/it]2022-01-08 02:15:52,636 iteration 6733 : loss : 0.012220, loss_ce: 0.004460
2022-01-08 02:15:54,174 iteration 6734 : loss : 0.024633, loss_ce: 0.007877
2022-01-08 02:15:55,530 iteration 6735 : loss : 0.011692, loss_ce: 0.004594
2022-01-08 02:15:56,973 iteration 6736 : loss : 0.019964, loss_ce: 0.008506
2022-01-08 02:15:58,414 iteration 6737 : loss : 0.020904, loss_ce: 0.008663
2022-01-08 02:15:59,736 iteration 6738 : loss : 0.011292, loss_ce: 0.003267
2022-01-08 02:16:01,101 iteration 6739 : loss : 0.015513, loss_ce: 0.006409
2022-01-08 02:16:02,532 iteration 6740 : loss : 0.012857, loss_ce: 0.005837
2022-01-08 02:16:03,910 iteration 6741 : loss : 0.010961, loss_ce: 0.003977
2022-01-08 02:16:05,268 iteration 6742 : loss : 0.015345, loss_ce: 0.004695
2022-01-08 02:16:06,716 iteration 6743 : loss : 0.015114, loss_ce: 0.006416
2022-01-08 02:16:08,140 iteration 6744 : loss : 0.021978, loss_ce: 0.009255
2022-01-08 02:16:09,490 iteration 6745 : loss : 0.017673, loss_ce: 0.006648
2022-01-08 02:16:10,893 iteration 6746 : loss : 0.014492, loss_ce: 0.005849
2022-01-08 02:16:12,382 iteration 6747 : loss : 0.021366, loss_ce: 0.005162
2022-01-08 02:16:13,823 iteration 6748 : loss : 0.021700, loss_ce: 0.006420
2022-01-08 02:16:15,230 iteration 6749 : loss : 0.014737, loss_ce: 0.006589
 99%|████████████████████████████▊| 397/400 [2:51:39<01:16, 25.61s/it]2022-01-08 02:16:16,703 iteration 6750 : loss : 0.014563, loss_ce: 0.005336
2022-01-08 02:16:18,244 iteration 6751 : loss : 0.022505, loss_ce: 0.007132
2022-01-08 02:16:19,712 iteration 6752 : loss : 0.017246, loss_ce: 0.007118
2022-01-08 02:16:21,102 iteration 6753 : loss : 0.024464, loss_ce: 0.010343
2022-01-08 02:16:22,577 iteration 6754 : loss : 0.016874, loss_ce: 0.007004
2022-01-08 02:16:23,951 iteration 6755 : loss : 0.013024, loss_ce: 0.003223
2022-01-08 02:16:25,248 iteration 6756 : loss : 0.009210, loss_ce: 0.004358
2022-01-08 02:16:26,574 iteration 6757 : loss : 0.015621, loss_ce: 0.006366
2022-01-08 02:16:27,939 iteration 6758 : loss : 0.012220, loss_ce: 0.005704
2022-01-08 02:16:29,370 iteration 6759 : loss : 0.028818, loss_ce: 0.009077
2022-01-08 02:16:30,753 iteration 6760 : loss : 0.012404, loss_ce: 0.005059
2022-01-08 02:16:32,118 iteration 6761 : loss : 0.015470, loss_ce: 0.005481
2022-01-08 02:16:33,521 iteration 6762 : loss : 0.017022, loss_ce: 0.005088
2022-01-08 02:16:34,907 iteration 6763 : loss : 0.016577, loss_ce: 0.008165
2022-01-08 02:16:36,402 iteration 6764 : loss : 0.016663, loss_ce: 0.006290
2022-01-08 02:16:37,823 iteration 6765 : loss : 0.018593, loss_ce: 0.004341
2022-01-08 02:16:39,204 iteration 6766 : loss : 0.012534, loss_ce: 0.005639
100%|████████████████████████████▊| 398/400 [2:52:03<00:50, 25.12s/it]2022-01-08 02:16:40,697 iteration 6767 : loss : 0.023084, loss_ce: 0.010243
2022-01-08 02:16:42,083 iteration 6768 : loss : 0.015745, loss_ce: 0.004970
2022-01-08 02:16:43,473 iteration 6769 : loss : 0.012409, loss_ce: 0.003974
2022-01-08 02:16:44,879 iteration 6770 : loss : 0.015396, loss_ce: 0.004920
2022-01-08 02:16:46,247 iteration 6771 : loss : 0.015268, loss_ce: 0.006366
2022-01-08 02:16:47,650 iteration 6772 : loss : 0.015267, loss_ce: 0.005918
2022-01-08 02:16:48,987 iteration 6773 : loss : 0.014349, loss_ce: 0.005738
2022-01-08 02:16:50,361 iteration 6774 : loss : 0.020793, loss_ce: 0.006077
2022-01-08 02:16:51,706 iteration 6775 : loss : 0.016058, loss_ce: 0.003387
2022-01-08 02:16:53,086 iteration 6776 : loss : 0.013138, loss_ce: 0.005340
2022-01-08 02:16:54,506 iteration 6777 : loss : 0.023385, loss_ce: 0.008645
2022-01-08 02:16:55,858 iteration 6778 : loss : 0.013383, loss_ce: 0.004930
2022-01-08 02:16:57,228 iteration 6779 : loss : 0.016298, loss_ce: 0.008781
2022-01-08 02:16:58,660 iteration 6780 : loss : 0.020359, loss_ce: 0.008129
2022-01-08 02:17:00,082 iteration 6781 : loss : 0.012964, loss_ce: 0.004731
2022-01-08 02:17:01,463 iteration 6782 : loss : 0.015833, loss_ce: 0.006622
2022-01-08 02:17:02,910 iteration 6783 : loss : 0.017379, loss_ce: 0.005825
100%|████████████████████████████▉| 399/400 [2:52:27<00:24, 24.70s/it]2022-01-08 02:17:04,276 iteration 6784 : loss : 0.016113, loss_ce: 0.007850
2022-01-08 02:17:05,655 iteration 6785 : loss : 0.014706, loss_ce: 0.004607
2022-01-08 02:17:07,113 iteration 6786 : loss : 0.019007, loss_ce: 0.007392
2022-01-08 02:17:08,496 iteration 6787 : loss : 0.016206, loss_ce: 0.004864
2022-01-08 02:17:09,892 iteration 6788 : loss : 0.014200, loss_ce: 0.006533
2022-01-08 02:17:11,356 iteration 6789 : loss : 0.020379, loss_ce: 0.006030
2022-01-08 02:17:12,780 iteration 6790 : loss : 0.016889, loss_ce: 0.007269
2022-01-08 02:17:14,243 iteration 6791 : loss : 0.019281, loss_ce: 0.006183
2022-01-08 02:17:15,624 iteration 6792 : loss : 0.017943, loss_ce: 0.007744
2022-01-08 02:17:16,935 iteration 6793 : loss : 0.011524, loss_ce: 0.004105
2022-01-08 02:17:18,385 iteration 6794 : loss : 0.025714, loss_ce: 0.009839
2022-01-08 02:17:19,743 iteration 6795 : loss : 0.012217, loss_ce: 0.005271
2022-01-08 02:17:21,122 iteration 6796 : loss : 0.015007, loss_ce: 0.003810
2022-01-08 02:17:22,484 iteration 6797 : loss : 0.015678, loss_ce: 0.005272
2022-01-08 02:17:23,929 iteration 6798 : loss : 0.021561, loss_ce: 0.007281
2022-01-08 02:17:25,332 iteration 6799 : loss : 0.014401, loss_ce: 0.004651
2022-01-08 02:17:25,333 Training Data Eval:
2022-01-08 02:17:32,369   Average segmentation loss on training set: 0.0089
2022-01-08 02:17:32,370 Validation Data Eval:
2022-01-08 02:17:34,759   Average segmentation loss on validation set: 0.0668
2022-01-08 02:17:36,161 iteration 6800 : loss : 0.019411, loss_ce: 0.009008
100%|█████████████████████████████| 400/400 [2:53:00<00:00, 27.26s/it]100%|█████████████████████████████| 400/400 [2:53:00<00:00, 25.95s/it]
