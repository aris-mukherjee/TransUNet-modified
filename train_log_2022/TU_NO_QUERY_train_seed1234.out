2022-01-08 09:28:09,707 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:28:09,708 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:28:09,708 ============================================================
2022-01-08 09:28:09,708 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:28:09,708 ============================================================
2022-01-08 09:28:09,709 Loading data...
2022-01-08 09:28:09,709 Reading NCI - RUNMC images...
2022-01-08 09:28:09,709 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-08 09:28:09,712 Already preprocessed this configuration. Loading now!
2022-01-08 09:28:09,731 Training Images: (256, 256, 286)
2022-01-08 09:28:09,731 Training Labels: (256, 256, 286)
2022-01-08 09:28:09,731 Validation Images: (256, 256, 98)
2022-01-08 09:28:09,731 Validation Labels: (256, 256, 98)
2022-01-08 09:28:09,731 ============================================================
2022-01-08 09:28:09,770 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-08 09:28:12,574 iteration 1 : loss : 0.985145, loss_ce: 1.214311
2022-01-08 09:28:13,969 iteration 2 : loss : 0.919431, loss_ce: 1.111775
2022-01-08 09:28:15,482 iteration 3 : loss : 0.868279, loss_ce: 1.022440
2022-01-08 09:28:16,893 iteration 4 : loss : 0.829186, loss_ce: 0.958801
2022-01-08 09:28:18,306 iteration 5 : loss : 0.764100, loss_ce: 0.859355
2022-01-08 09:28:19,739 iteration 6 : loss : 0.732162, loss_ce: 0.797623
2022-01-08 09:28:21,230 iteration 7 : loss : 0.686193, loss_ce: 0.733650
2022-01-08 09:28:22,670 iteration 8 : loss : 0.671997, loss_ce: 0.678257
2022-01-08 09:28:24,131 iteration 9 : loss : 0.606183, loss_ce: 0.637198
2022-01-08 09:28:25,630 iteration 10 : loss : 0.610254, loss_ce: 0.578078
2022-01-08 09:28:27,181 iteration 11 : loss : 0.566190, loss_ce: 0.542643
2022-01-08 09:28:28,614 iteration 12 : loss : 0.537938, loss_ce: 0.492108
2022-01-08 09:28:30,028 iteration 13 : loss : 0.527791, loss_ce: 0.463551
2022-01-08 09:28:31,409 iteration 14 : loss : 0.494704, loss_ce: 0.422961
2022-01-08 09:28:32,873 iteration 15 : loss : 0.465759, loss_ce: 0.383474
2022-01-08 09:28:34,373 iteration 16 : loss : 0.473546, loss_ce: 0.372516
2022-01-08 09:28:35,898 iteration 17 : loss : 0.412648, loss_ce: 0.331226
  0%|                               | 1/400 [00:26<2:54:12, 26.20s/it]2022-01-08 09:28:37,547 iteration 18 : loss : 0.440981, loss_ce: 0.299542
2022-01-08 09:28:39,042 iteration 19 : loss : 0.378990, loss_ce: 0.268862
2022-01-08 09:28:40,687 iteration 20 : loss : 0.370501, loss_ce: 0.250713
2022-01-08 09:28:42,229 iteration 21 : loss : 0.388142, loss_ce: 0.232161
2022-01-08 09:28:43,800 iteration 22 : loss : 0.339200, loss_ce: 0.224684
2022-01-08 09:28:45,450 iteration 23 : loss : 0.336332, loss_ce: 0.196292
2022-01-08 09:28:47,020 iteration 24 : loss : 0.317954, loss_ce: 0.188883
2022-01-08 09:28:48,655 iteration 25 : loss : 0.379550, loss_ce: 0.234614
2022-01-08 09:28:50,210 iteration 26 : loss : 0.300724, loss_ce: 0.170498
2022-01-08 09:28:51,707 iteration 27 : loss : 0.305395, loss_ce: 0.176990
2022-01-08 09:28:53,220 iteration 28 : loss : 0.288470, loss_ce: 0.157942
2022-01-08 09:28:54,841 iteration 29 : loss : 0.292557, loss_ce: 0.151315
2022-01-08 09:28:56,449 iteration 30 : loss : 0.305700, loss_ce: 0.159756
2022-01-08 09:28:57,959 iteration 31 : loss : 0.280486, loss_ce: 0.147893
2022-01-08 09:28:59,603 iteration 32 : loss : 0.296438, loss_ce: 0.167568
2022-01-08 09:29:01,230 iteration 33 : loss : 0.281530, loss_ce: 0.153844
2022-01-08 09:29:02,856 iteration 34 : loss : 0.282566, loss_ce: 0.162889
  0%|▏                              | 2/400 [00:53<2:56:40, 26.63s/it]2022-01-08 09:29:04,519 iteration 35 : loss : 0.272841, loss_ce: 0.137885
2022-01-08 09:29:06,158 iteration 36 : loss : 0.281274, loss_ce: 0.149729
2022-01-08 09:29:07,804 iteration 37 : loss : 0.274637, loss_ce: 0.118586
2022-01-08 09:29:09,355 iteration 38 : loss : 0.243648, loss_ce: 0.116532
2022-01-08 09:29:10,918 iteration 39 : loss : 0.234065, loss_ce: 0.109291
2022-01-08 09:29:12,563 iteration 40 : loss : 0.259244, loss_ce: 0.124372
2022-01-08 09:29:14,198 iteration 41 : loss : 0.320714, loss_ce: 0.157694
2022-01-08 09:29:15,808 iteration 42 : loss : 0.247936, loss_ce: 0.120230
2022-01-08 09:29:17,314 iteration 43 : loss : 0.262473, loss_ce: 0.120416
2022-01-08 09:29:18,978 iteration 44 : loss : 0.263054, loss_ce: 0.127016
2022-01-08 09:29:20,566 iteration 45 : loss : 0.245402, loss_ce: 0.119135
2022-01-08 09:29:22,165 iteration 46 : loss : 0.260373, loss_ce: 0.110049
2022-01-08 09:29:23,802 iteration 47 : loss : 0.219899, loss_ce: 0.090204
2022-01-08 09:29:25,411 iteration 48 : loss : 0.229936, loss_ce: 0.098777
2022-01-08 09:29:27,067 iteration 49 : loss : 0.304987, loss_ce: 0.142846
2022-01-08 09:29:28,616 iteration 50 : loss : 0.332983, loss_ce: 0.148053
2022-01-08 09:29:30,142 iteration 51 : loss : 0.265461, loss_ce: 0.130270
  1%|▏                              | 3/400 [01:20<2:58:11, 26.93s/it]2022-01-08 09:29:31,826 iteration 52 : loss : 0.280331, loss_ce: 0.142750
2022-01-08 09:29:33,453 iteration 53 : loss : 0.273248, loss_ce: 0.134111
2022-01-08 09:29:35,048 iteration 54 : loss : 0.252634, loss_ce: 0.111663
2022-01-08 09:29:36,654 iteration 55 : loss : 0.287257, loss_ce: 0.142444
2022-01-08 09:29:38,240 iteration 56 : loss : 0.268919, loss_ce: 0.116783
2022-01-08 09:29:39,850 iteration 57 : loss : 0.228297, loss_ce: 0.096491
2022-01-08 09:29:41,461 iteration 58 : loss : 0.264407, loss_ce: 0.107015
2022-01-08 09:29:43,059 iteration 59 : loss : 0.231942, loss_ce: 0.101667
2022-01-08 09:29:44,670 iteration 60 : loss : 0.245385, loss_ce: 0.095691
2022-01-08 09:29:46,270 iteration 61 : loss : 0.264498, loss_ce: 0.120200
2022-01-08 09:29:47,854 iteration 62 : loss : 0.325387, loss_ce: 0.125647
2022-01-08 09:29:49,359 iteration 63 : loss : 0.315943, loss_ce: 0.153873
2022-01-08 09:29:50,927 iteration 64 : loss : 0.327183, loss_ce: 0.152077
2022-01-08 09:29:52,443 iteration 65 : loss : 0.262366, loss_ce: 0.103446
2022-01-08 09:29:54,004 iteration 66 : loss : 0.248143, loss_ce: 0.112697
2022-01-08 09:29:55,647 iteration 67 : loss : 0.258437, loss_ce: 0.087612
2022-01-08 09:29:57,227 iteration 68 : loss : 0.249796, loss_ce: 0.108456
  1%|▎                              | 4/400 [01:47<2:58:07, 26.99s/it]2022-01-08 09:29:58,878 iteration 69 : loss : 0.227467, loss_ce: 0.093109
2022-01-08 09:30:00,551 iteration 70 : loss : 0.242425, loss_ce: 0.099194
2022-01-08 09:30:02,103 iteration 71 : loss : 0.233539, loss_ce: 0.094197
2022-01-08 09:30:03,704 iteration 72 : loss : 0.232814, loss_ce: 0.096585
2022-01-08 09:30:05,243 iteration 73 : loss : 0.244854, loss_ce: 0.118050
2022-01-08 09:30:06,748 iteration 74 : loss : 0.222982, loss_ce: 0.092749
2022-01-08 09:30:08,296 iteration 75 : loss : 0.234117, loss_ce: 0.101266
2022-01-08 09:30:09,835 iteration 76 : loss : 0.240086, loss_ce: 0.103128
2022-01-08 09:30:11,285 iteration 77 : loss : 0.221785, loss_ce: 0.098633
2022-01-08 09:30:12,849 iteration 78 : loss : 0.269593, loss_ce: 0.121424
2022-01-08 09:30:14,380 iteration 79 : loss : 0.275631, loss_ce: 0.107613
2022-01-08 09:30:15,900 iteration 80 : loss : 0.232056, loss_ce: 0.108825
2022-01-08 09:30:17,407 iteration 81 : loss : 0.247864, loss_ce: 0.111753
2022-01-08 09:30:18,944 iteration 82 : loss : 0.234719, loss_ce: 0.094907
2022-01-08 09:30:20,472 iteration 83 : loss : 0.263470, loss_ce: 0.101905
2022-01-08 09:30:22,103 iteration 84 : loss : 0.256714, loss_ce: 0.132507
2022-01-08 09:30:22,103 Training Data Eval:
2022-01-08 09:30:30,002   Average segmentation loss on training set: 0.4359
2022-01-08 09:30:30,002 Validation Data Eval:
2022-01-08 09:30:32,872   Average segmentation loss on validation set: 0.5042
2022-01-08 09:30:38,858 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:30:40,272 iteration 85 : loss : 0.254536, loss_ce: 0.103470
  1%|▍                              | 5/400 [02:30<3:35:46, 32.78s/it]2022-01-08 09:30:41,775 iteration 86 : loss : 0.242985, loss_ce: 0.082529
2022-01-08 09:30:43,474 iteration 87 : loss : 0.266036, loss_ce: 0.103947
2022-01-08 09:30:44,998 iteration 88 : loss : 0.246093, loss_ce: 0.103505
2022-01-08 09:30:46,602 iteration 89 : loss : 0.246032, loss_ce: 0.101723
2022-01-08 09:30:48,197 iteration 90 : loss : 0.235284, loss_ce: 0.093964
2022-01-08 09:30:49,861 iteration 91 : loss : 0.241137, loss_ce: 0.123062
2022-01-08 09:30:51,364 iteration 92 : loss : 0.240386, loss_ce: 0.108188
2022-01-08 09:30:52,919 iteration 93 : loss : 0.256458, loss_ce: 0.097443
2022-01-08 09:30:54,482 iteration 94 : loss : 0.235073, loss_ce: 0.095423
2022-01-08 09:30:56,184 iteration 95 : loss : 0.229095, loss_ce: 0.099520
2022-01-08 09:30:57,729 iteration 96 : loss : 0.250254, loss_ce: 0.110911
2022-01-08 09:30:59,279 iteration 97 : loss : 0.268172, loss_ce: 0.112842
2022-01-08 09:31:00,845 iteration 98 : loss : 0.239464, loss_ce: 0.095167
2022-01-08 09:31:02,500 iteration 99 : loss : 0.221270, loss_ce: 0.094730
2022-01-08 09:31:04,101 iteration 100 : loss : 0.237591, loss_ce: 0.102877
2022-01-08 09:31:05,676 iteration 101 : loss : 0.206767, loss_ce: 0.086588
2022-01-08 09:31:07,175 iteration 102 : loss : 0.219317, loss_ce: 0.085118
  2%|▍                              | 6/400 [02:57<3:22:08, 30.78s/it]2022-01-08 09:31:08,895 iteration 103 : loss : 0.210902, loss_ce: 0.087204
2022-01-08 09:31:10,560 iteration 104 : loss : 0.233187, loss_ce: 0.091876
2022-01-08 09:31:12,277 iteration 105 : loss : 0.237434, loss_ce: 0.090726
2022-01-08 09:31:13,816 iteration 106 : loss : 0.246261, loss_ce: 0.100414
2022-01-08 09:31:15,561 iteration 107 : loss : 0.216460, loss_ce: 0.091551
2022-01-08 09:31:17,076 iteration 108 : loss : 0.269779, loss_ce: 0.114220
2022-01-08 09:31:18,625 iteration 109 : loss : 0.208226, loss_ce: 0.090086
2022-01-08 09:31:20,115 iteration 110 : loss : 0.196698, loss_ce: 0.080013
2022-01-08 09:31:21,711 iteration 111 : loss : 0.235311, loss_ce: 0.104261
2022-01-08 09:31:23,213 iteration 112 : loss : 0.219243, loss_ce: 0.090310
2022-01-08 09:31:24,787 iteration 113 : loss : 0.241384, loss_ce: 0.119498
2022-01-08 09:31:26,323 iteration 114 : loss : 0.252173, loss_ce: 0.092072
2022-01-08 09:31:27,879 iteration 115 : loss : 0.234103, loss_ce: 0.101524
2022-01-08 09:31:29,499 iteration 116 : loss : 0.240626, loss_ce: 0.102859
2022-01-08 09:31:31,115 iteration 117 : loss : 0.223110, loss_ce: 0.102560
2022-01-08 09:31:32,694 iteration 118 : loss : 0.238097, loss_ce: 0.099900
2022-01-08 09:31:34,289 iteration 119 : loss : 0.202560, loss_ce: 0.075274
  2%|▌                              | 7/400 [03:24<3:13:46, 29.58s/it]2022-01-08 09:31:35,862 iteration 120 : loss : 0.327587, loss_ce: 0.166329
2022-01-08 09:31:37,368 iteration 121 : loss : 0.215234, loss_ce: 0.091314
2022-01-08 09:31:38,997 iteration 122 : loss : 0.242336, loss_ce: 0.095579
2022-01-08 09:31:40,586 iteration 123 : loss : 0.200451, loss_ce: 0.077034
2022-01-08 09:31:42,146 iteration 124 : loss : 0.224920, loss_ce: 0.079615
2022-01-08 09:31:43,667 iteration 125 : loss : 0.212047, loss_ce: 0.090411
2022-01-08 09:31:45,255 iteration 126 : loss : 0.219497, loss_ce: 0.075415
2022-01-08 09:31:46,768 iteration 127 : loss : 0.204527, loss_ce: 0.080795
2022-01-08 09:31:48,306 iteration 128 : loss : 0.180552, loss_ce: 0.064755
2022-01-08 09:31:49,927 iteration 129 : loss : 0.208400, loss_ce: 0.076848
2022-01-08 09:31:51,483 iteration 130 : loss : 0.210295, loss_ce: 0.083400
2022-01-08 09:31:53,145 iteration 131 : loss : 0.241467, loss_ce: 0.117242
2022-01-08 09:31:54,830 iteration 132 : loss : 0.201074, loss_ce: 0.060247
2022-01-08 09:31:56,371 iteration 133 : loss : 0.196938, loss_ce: 0.070933
2022-01-08 09:31:58,020 iteration 134 : loss : 0.216798, loss_ce: 0.083987
2022-01-08 09:31:59,683 iteration 135 : loss : 0.239555, loss_ce: 0.109874
2022-01-08 09:32:01,248 iteration 136 : loss : 0.162978, loss_ce: 0.066949
  2%|▌                              | 8/400 [03:51<3:07:48, 28.75s/it]2022-01-08 09:32:02,940 iteration 137 : loss : 0.224985, loss_ce: 0.079525
2022-01-08 09:32:04,479 iteration 138 : loss : 0.230374, loss_ce: 0.111067
2022-01-08 09:32:06,088 iteration 139 : loss : 0.221439, loss_ce: 0.084784
2022-01-08 09:32:07,665 iteration 140 : loss : 0.202846, loss_ce: 0.070064
2022-01-08 09:32:09,291 iteration 141 : loss : 0.213670, loss_ce: 0.094380
2022-01-08 09:32:10,928 iteration 142 : loss : 0.212657, loss_ce: 0.087318
2022-01-08 09:32:12,594 iteration 143 : loss : 0.249221, loss_ce: 0.113880
2022-01-08 09:32:14,225 iteration 144 : loss : 0.207118, loss_ce: 0.078198
2022-01-08 09:32:15,796 iteration 145 : loss : 0.212712, loss_ce: 0.075731
2022-01-08 09:32:17,408 iteration 146 : loss : 0.196639, loss_ce: 0.087319
2022-01-08 09:32:19,035 iteration 147 : loss : 0.201912, loss_ce: 0.084423
2022-01-08 09:32:20,524 iteration 148 : loss : 0.196216, loss_ce: 0.076583
2022-01-08 09:32:22,085 iteration 149 : loss : 0.255532, loss_ce: 0.110930
2022-01-08 09:32:23,623 iteration 150 : loss : 0.255963, loss_ce: 0.093034
2022-01-08 09:32:25,167 iteration 151 : loss : 0.197783, loss_ce: 0.082026
2022-01-08 09:32:26,716 iteration 152 : loss : 0.229688, loss_ce: 0.080046
2022-01-08 09:32:28,317 iteration 153 : loss : 0.235595, loss_ce: 0.103557
  2%|▋                              | 9/400 [04:18<3:03:55, 28.22s/it]2022-01-08 09:32:29,870 iteration 154 : loss : 0.262267, loss_ce: 0.112402
2022-01-08 09:32:31,454 iteration 155 : loss : 0.213762, loss_ce: 0.074434
2022-01-08 09:32:33,128 iteration 156 : loss : 0.206551, loss_ce: 0.078538
2022-01-08 09:32:34,725 iteration 157 : loss : 0.222907, loss_ce: 0.071266
2022-01-08 09:32:36,450 iteration 158 : loss : 0.286194, loss_ce: 0.127706
2022-01-08 09:32:37,904 iteration 159 : loss : 0.198697, loss_ce: 0.082318
2022-01-08 09:32:39,569 iteration 160 : loss : 0.187682, loss_ce: 0.060937
2022-01-08 09:32:41,219 iteration 161 : loss : 0.257755, loss_ce: 0.098539
2022-01-08 09:32:42,879 iteration 162 : loss : 0.204862, loss_ce: 0.095856
2022-01-08 09:32:44,462 iteration 163 : loss : 0.216792, loss_ce: 0.087117
2022-01-08 09:32:46,067 iteration 164 : loss : 0.223675, loss_ce: 0.078685
2022-01-08 09:32:47,639 iteration 165 : loss : 0.233545, loss_ce: 0.103393
2022-01-08 09:32:49,193 iteration 166 : loss : 0.209429, loss_ce: 0.077340
2022-01-08 09:32:50,764 iteration 167 : loss : 0.202352, loss_ce: 0.074621
2022-01-08 09:32:52,379 iteration 168 : loss : 0.264791, loss_ce: 0.129530
2022-01-08 09:32:53,960 iteration 169 : loss : 0.199962, loss_ce: 0.090604
2022-01-08 09:32:53,960 Training Data Eval:
2022-01-08 09:33:01,861   Average segmentation loss on training set: 1.3449
2022-01-08 09:33:01,861 Validation Data Eval:
2022-01-08 09:33:04,589   Average segmentation loss on validation set: 1.2781
2022-01-08 09:33:06,211 iteration 170 : loss : 0.208560, loss_ce: 0.084384
  2%|▊                             | 10/400 [04:56<3:22:50, 31.21s/it]2022-01-08 09:33:07,856 iteration 171 : loss : 0.203878, loss_ce: 0.082053
2022-01-08 09:33:09,468 iteration 172 : loss : 0.215322, loss_ce: 0.091653
2022-01-08 09:33:11,101 iteration 173 : loss : 0.175013, loss_ce: 0.074305
2022-01-08 09:33:12,637 iteration 174 : loss : 0.232571, loss_ce: 0.103393
2022-01-08 09:33:14,158 iteration 175 : loss : 0.266904, loss_ce: 0.108717
2022-01-08 09:33:15,761 iteration 176 : loss : 0.204446, loss_ce: 0.094132
2022-01-08 09:33:17,334 iteration 177 : loss : 0.204300, loss_ce: 0.071749
2022-01-08 09:33:18,909 iteration 178 : loss : 0.174803, loss_ce: 0.071091
2022-01-08 09:33:20,483 iteration 179 : loss : 0.225727, loss_ce: 0.089085
2022-01-08 09:33:22,033 iteration 180 : loss : 0.180710, loss_ce: 0.066016
2022-01-08 09:33:23,640 iteration 181 : loss : 0.185791, loss_ce: 0.075001
2022-01-08 09:33:25,183 iteration 182 : loss : 0.183575, loss_ce: 0.072964
2022-01-08 09:33:26,706 iteration 183 : loss : 0.190691, loss_ce: 0.065611
2022-01-08 09:33:28,184 iteration 184 : loss : 0.193120, loss_ce: 0.072000
2022-01-08 09:33:29,793 iteration 185 : loss : 0.234798, loss_ce: 0.082255
2022-01-08 09:33:31,399 iteration 186 : loss : 0.254734, loss_ce: 0.114936
2022-01-08 09:33:33,046 iteration 187 : loss : 0.211193, loss_ce: 0.079225
  3%|▊                             | 11/400 [05:23<3:13:38, 29.87s/it]2022-01-08 09:33:34,698 iteration 188 : loss : 0.231396, loss_ce: 0.093740
2022-01-08 09:33:36,296 iteration 189 : loss : 0.182239, loss_ce: 0.070641
2022-01-08 09:33:37,902 iteration 190 : loss : 0.201157, loss_ce: 0.076894
2022-01-08 09:33:39,454 iteration 191 : loss : 0.193174, loss_ce: 0.075899
2022-01-08 09:33:40,955 iteration 192 : loss : 0.200953, loss_ce: 0.089006
2022-01-08 09:33:42,593 iteration 193 : loss : 0.202297, loss_ce: 0.087806
2022-01-08 09:33:44,152 iteration 194 : loss : 0.236939, loss_ce: 0.069152
2022-01-08 09:33:45,689 iteration 195 : loss : 0.185365, loss_ce: 0.083375
2022-01-08 09:33:47,172 iteration 196 : loss : 0.222746, loss_ce: 0.076321
2022-01-08 09:33:48,752 iteration 197 : loss : 0.256041, loss_ce: 0.095781
2022-01-08 09:33:50,365 iteration 198 : loss : 0.211822, loss_ce: 0.092644
2022-01-08 09:33:51,900 iteration 199 : loss : 0.196601, loss_ce: 0.083921
2022-01-08 09:33:53,456 iteration 200 : loss : 0.209616, loss_ce: 0.086754
2022-01-08 09:33:55,045 iteration 201 : loss : 0.161120, loss_ce: 0.066935
2022-01-08 09:33:56,663 iteration 202 : loss : 0.196036, loss_ce: 0.086270
2022-01-08 09:33:58,190 iteration 203 : loss : 0.161586, loss_ce: 0.065636
2022-01-08 09:33:59,863 iteration 204 : loss : 0.222450, loss_ce: 0.087475
  3%|▉                             | 12/400 [05:50<3:07:08, 28.94s/it]2022-01-08 09:34:01,444 iteration 205 : loss : 0.174382, loss_ce: 0.062715
2022-01-08 09:34:03,014 iteration 206 : loss : 0.172749, loss_ce: 0.057821
2022-01-08 09:34:04,612 iteration 207 : loss : 0.169520, loss_ce: 0.067905
2022-01-08 09:34:06,165 iteration 208 : loss : 0.213950, loss_ce: 0.065776
2022-01-08 09:34:07,822 iteration 209 : loss : 0.242692, loss_ce: 0.118246
2022-01-08 09:34:09,300 iteration 210 : loss : 0.183423, loss_ce: 0.068392
2022-01-08 09:34:10,916 iteration 211 : loss : 0.206412, loss_ce: 0.089679
2022-01-08 09:34:12,512 iteration 212 : loss : 0.214520, loss_ce: 0.078063
2022-01-08 09:34:14,205 iteration 213 : loss : 0.160355, loss_ce: 0.075649
2022-01-08 09:34:15,788 iteration 214 : loss : 0.195809, loss_ce: 0.064816
2022-01-08 09:34:17,367 iteration 215 : loss : 0.235676, loss_ce: 0.098838
2022-01-08 09:34:19,124 iteration 216 : loss : 0.169982, loss_ce: 0.058072
2022-01-08 09:34:20,811 iteration 217 : loss : 0.161086, loss_ce: 0.066186
2022-01-08 09:34:22,411 iteration 218 : loss : 0.177104, loss_ce: 0.084206
2022-01-08 09:34:23,882 iteration 219 : loss : 0.187117, loss_ce: 0.076728
2022-01-08 09:34:25,486 iteration 220 : loss : 0.162060, loss_ce: 0.070746
2022-01-08 09:34:27,070 iteration 221 : loss : 0.220863, loss_ce: 0.084331
  3%|▉                             | 13/400 [06:17<3:03:16, 28.42s/it]2022-01-08 09:34:28,713 iteration 222 : loss : 0.195590, loss_ce: 0.098229
2022-01-08 09:34:30,348 iteration 223 : loss : 0.184183, loss_ce: 0.075569
2022-01-08 09:34:31,928 iteration 224 : loss : 0.243840, loss_ce: 0.125768
2022-01-08 09:34:33,623 iteration 225 : loss : 0.275154, loss_ce: 0.085005
2022-01-08 09:34:35,200 iteration 226 : loss : 0.174848, loss_ce: 0.061707
2022-01-08 09:34:36,840 iteration 227 : loss : 0.202971, loss_ce: 0.086159
2022-01-08 09:34:38,418 iteration 228 : loss : 0.206340, loss_ce: 0.081980
2022-01-08 09:34:39,944 iteration 229 : loss : 0.181599, loss_ce: 0.064759
2022-01-08 09:34:41,533 iteration 230 : loss : 0.161747, loss_ce: 0.060892
2022-01-08 09:34:43,164 iteration 231 : loss : 0.237921, loss_ce: 0.091946
2022-01-08 09:34:44,793 iteration 232 : loss : 0.168828, loss_ce: 0.064395
2022-01-08 09:34:46,326 iteration 233 : loss : 0.229364, loss_ce: 0.101692
2022-01-08 09:34:47,981 iteration 234 : loss : 0.214344, loss_ce: 0.091431
2022-01-08 09:34:49,542 iteration 235 : loss : 0.171520, loss_ce: 0.074318
2022-01-08 09:34:51,152 iteration 236 : loss : 0.193265, loss_ce: 0.081861
2022-01-08 09:34:52,710 iteration 237 : loss : 0.160650, loss_ce: 0.064902
2022-01-08 09:34:54,290 iteration 238 : loss : 0.159107, loss_ce: 0.076071
  4%|█                             | 14/400 [06:44<3:00:29, 28.06s/it]2022-01-08 09:34:55,854 iteration 239 : loss : 0.198322, loss_ce: 0.065666
2022-01-08 09:34:57,470 iteration 240 : loss : 0.159413, loss_ce: 0.074522
2022-01-08 09:34:59,096 iteration 241 : loss : 0.196851, loss_ce: 0.084032
2022-01-08 09:35:00,700 iteration 242 : loss : 0.228270, loss_ce: 0.117861
2022-01-08 09:35:02,444 iteration 243 : loss : 0.191754, loss_ce: 0.076572
2022-01-08 09:35:03,973 iteration 244 : loss : 0.215381, loss_ce: 0.054655
2022-01-08 09:35:05,573 iteration 245 : loss : 0.187936, loss_ce: 0.082453
2022-01-08 09:35:07,214 iteration 246 : loss : 0.176745, loss_ce: 0.071595
2022-01-08 09:35:08,788 iteration 247 : loss : 0.191776, loss_ce: 0.065216
2022-01-08 09:35:10,355 iteration 248 : loss : 0.172585, loss_ce: 0.059093
2022-01-08 09:35:11,905 iteration 249 : loss : 0.169099, loss_ce: 0.089424
2022-01-08 09:35:13,473 iteration 250 : loss : 0.229909, loss_ce: 0.103946
2022-01-08 09:35:14,982 iteration 251 : loss : 0.202391, loss_ce: 0.074499
2022-01-08 09:35:16,547 iteration 252 : loss : 0.126865, loss_ce: 0.051815
2022-01-08 09:35:18,152 iteration 253 : loss : 0.167881, loss_ce: 0.064902
2022-01-08 09:35:19,824 iteration 254 : loss : 0.285718, loss_ce: 0.091360
2022-01-08 09:35:19,825 Training Data Eval:
2022-01-08 09:35:27,709   Average segmentation loss on training set: 0.3141
2022-01-08 09:35:27,710 Validation Data Eval:
2022-01-08 09:35:30,434   Average segmentation loss on validation set: 0.2781
2022-01-08 09:35:36,262 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:35:37,683 iteration 255 : loss : 0.170171, loss_ce: 0.073470
  4%|█▏                            | 15/400 [07:27<3:29:41, 32.68s/it]2022-01-08 09:35:39,222 iteration 256 : loss : 0.169611, loss_ce: 0.064767
2022-01-08 09:35:40,829 iteration 257 : loss : 0.173599, loss_ce: 0.073321
2022-01-08 09:35:42,533 iteration 258 : loss : 0.179131, loss_ce: 0.070172
2022-01-08 09:35:44,030 iteration 259 : loss : 0.138598, loss_ce: 0.061709
2022-01-08 09:35:45,683 iteration 260 : loss : 0.174980, loss_ce: 0.079330
2022-01-08 09:35:47,288 iteration 261 : loss : 0.225300, loss_ce: 0.082425
2022-01-08 09:35:48,989 iteration 262 : loss : 0.146756, loss_ce: 0.053101
2022-01-08 09:35:50,581 iteration 263 : loss : 0.123814, loss_ce: 0.051720
2022-01-08 09:35:52,207 iteration 264 : loss : 0.217047, loss_ce: 0.092665
2022-01-08 09:35:53,754 iteration 265 : loss : 0.136294, loss_ce: 0.055875
2022-01-08 09:35:55,376 iteration 266 : loss : 0.143180, loss_ce: 0.060644
2022-01-08 09:35:56,955 iteration 267 : loss : 0.240959, loss_ce: 0.084670
2022-01-08 09:35:58,551 iteration 268 : loss : 0.189361, loss_ce: 0.079323
2022-01-08 09:36:00,361 iteration 269 : loss : 0.181347, loss_ce: 0.063742
2022-01-08 09:36:02,068 iteration 270 : loss : 0.219811, loss_ce: 0.091837
2022-01-08 09:36:03,524 iteration 271 : loss : 0.163010, loss_ce: 0.048735
2022-01-08 09:36:05,116 iteration 272 : loss : 0.171578, loss_ce: 0.081662
  4%|█▏                            | 16/400 [07:55<3:19:02, 31.10s/it]2022-01-08 09:36:06,692 iteration 273 : loss : 0.178998, loss_ce: 0.068657
2022-01-08 09:36:08,210 iteration 274 : loss : 0.140861, loss_ce: 0.055298
2022-01-08 09:36:09,736 iteration 275 : loss : 0.156928, loss_ce: 0.061267
2022-01-08 09:36:11,326 iteration 276 : loss : 0.153843, loss_ce: 0.066688
2022-01-08 09:36:12,940 iteration 277 : loss : 0.145957, loss_ce: 0.053132
2022-01-08 09:36:14,382 iteration 278 : loss : 0.176346, loss_ce: 0.057727
2022-01-08 09:36:15,904 iteration 279 : loss : 0.190158, loss_ce: 0.067253
2022-01-08 09:36:17,417 iteration 280 : loss : 0.193708, loss_ce: 0.074767
2022-01-08 09:36:19,001 iteration 281 : loss : 0.208186, loss_ce: 0.087087
2022-01-08 09:36:20,556 iteration 282 : loss : 0.241327, loss_ce: 0.098052
2022-01-08 09:36:22,090 iteration 283 : loss : 0.162667, loss_ce: 0.078127
2022-01-08 09:36:23,735 iteration 284 : loss : 0.188431, loss_ce: 0.086176
2022-01-08 09:36:25,249 iteration 285 : loss : 0.185408, loss_ce: 0.059190
2022-01-08 09:36:26,784 iteration 286 : loss : 0.151165, loss_ce: 0.063145
2022-01-08 09:36:28,457 iteration 287 : loss : 0.190904, loss_ce: 0.077196
2022-01-08 09:36:30,103 iteration 288 : loss : 0.183013, loss_ce: 0.069053
2022-01-08 09:36:31,660 iteration 289 : loss : 0.150173, loss_ce: 0.052785
  4%|█▎                            | 17/400 [08:21<3:09:46, 29.73s/it]2022-01-08 09:36:33,298 iteration 290 : loss : 0.122278, loss_ce: 0.041846
2022-01-08 09:36:34,836 iteration 291 : loss : 0.183288, loss_ce: 0.065960
2022-01-08 09:36:36,386 iteration 292 : loss : 0.131433, loss_ce: 0.049178
2022-01-08 09:36:37,943 iteration 293 : loss : 0.189160, loss_ce: 0.073211
2022-01-08 09:36:39,529 iteration 294 : loss : 0.169624, loss_ce: 0.071699
2022-01-08 09:36:41,220 iteration 295 : loss : 0.183056, loss_ce: 0.059539
2022-01-08 09:36:42,773 iteration 296 : loss : 0.150967, loss_ce: 0.053570
2022-01-08 09:36:44,400 iteration 297 : loss : 0.149601, loss_ce: 0.052469
2022-01-08 09:36:45,946 iteration 298 : loss : 0.130953, loss_ce: 0.044514
2022-01-08 09:36:47,518 iteration 299 : loss : 0.165073, loss_ce: 0.056247
2022-01-08 09:36:49,097 iteration 300 : loss : 0.177170, loss_ce: 0.057250
2022-01-08 09:36:50,686 iteration 301 : loss : 0.201850, loss_ce: 0.097640
2022-01-08 09:36:52,340 iteration 302 : loss : 0.131057, loss_ce: 0.055968
2022-01-08 09:36:53,968 iteration 303 : loss : 0.190714, loss_ce: 0.075184
2022-01-08 09:36:55,580 iteration 304 : loss : 0.177477, loss_ce: 0.077791
2022-01-08 09:36:57,225 iteration 305 : loss : 0.149283, loss_ce: 0.063105
2022-01-08 09:36:58,865 iteration 306 : loss : 0.140544, loss_ce: 0.066172
  4%|█▎                            | 18/400 [08:49<3:04:27, 28.97s/it]2022-01-08 09:37:00,512 iteration 307 : loss : 0.151145, loss_ce: 0.052212
2022-01-08 09:37:02,060 iteration 308 : loss : 0.145989, loss_ce: 0.067007
2022-01-08 09:37:03,669 iteration 309 : loss : 0.225658, loss_ce: 0.089474
2022-01-08 09:37:05,305 iteration 310 : loss : 0.153966, loss_ce: 0.063845
2022-01-08 09:37:06,845 iteration 311 : loss : 0.140749, loss_ce: 0.042342
2022-01-08 09:37:08,423 iteration 312 : loss : 0.131585, loss_ce: 0.062716
2022-01-08 09:37:10,014 iteration 313 : loss : 0.180273, loss_ce: 0.091261
2022-01-08 09:37:11,516 iteration 314 : loss : 0.134327, loss_ce: 0.063307
2022-01-08 09:37:13,083 iteration 315 : loss : 0.181051, loss_ce: 0.075637
2022-01-08 09:37:14,687 iteration 316 : loss : 0.172567, loss_ce: 0.068334
2022-01-08 09:37:16,272 iteration 317 : loss : 0.193494, loss_ce: 0.078571
2022-01-08 09:37:17,873 iteration 318 : loss : 0.127383, loss_ce: 0.055179
2022-01-08 09:37:19,388 iteration 319 : loss : 0.144946, loss_ce: 0.061807
2022-01-08 09:37:21,060 iteration 320 : loss : 0.141008, loss_ce: 0.052492
2022-01-08 09:37:22,688 iteration 321 : loss : 0.120220, loss_ce: 0.040692
2022-01-08 09:37:24,267 iteration 322 : loss : 0.243663, loss_ce: 0.107057
2022-01-08 09:37:25,916 iteration 323 : loss : 0.175528, loss_ce: 0.049228
  5%|█▍                            | 19/400 [09:16<3:00:20, 28.40s/it]2022-01-08 09:37:27,561 iteration 324 : loss : 0.171304, loss_ce: 0.062119
2022-01-08 09:37:29,151 iteration 325 : loss : 0.231184, loss_ce: 0.088318
2022-01-08 09:37:30,772 iteration 326 : loss : 0.150725, loss_ce: 0.066679
2022-01-08 09:37:32,476 iteration 327 : loss : 0.217468, loss_ce: 0.077520
2022-01-08 09:37:34,057 iteration 328 : loss : 0.191566, loss_ce: 0.088752
2022-01-08 09:37:35,685 iteration 329 : loss : 0.169637, loss_ce: 0.080542
2022-01-08 09:37:37,327 iteration 330 : loss : 0.154219, loss_ce: 0.061093
2022-01-08 09:37:38,864 iteration 331 : loss : 0.155904, loss_ce: 0.076636
2022-01-08 09:37:40,434 iteration 332 : loss : 0.126907, loss_ce: 0.045920
2022-01-08 09:37:42,046 iteration 333 : loss : 0.129919, loss_ce: 0.058189
2022-01-08 09:37:43,642 iteration 334 : loss : 0.150887, loss_ce: 0.061919
2022-01-08 09:37:45,203 iteration 335 : loss : 0.188977, loss_ce: 0.069112
2022-01-08 09:37:46,849 iteration 336 : loss : 0.127236, loss_ce: 0.047660
2022-01-08 09:37:48,431 iteration 337 : loss : 0.133186, loss_ce: 0.046577
2022-01-08 09:37:49,971 iteration 338 : loss : 0.187702, loss_ce: 0.086201
2022-01-08 09:37:51,514 iteration 339 : loss : 0.181940, loss_ce: 0.060289
2022-01-08 09:37:51,514 Training Data Eval:
2022-01-08 09:37:59,404   Average segmentation loss on training set: 0.1547
2022-01-08 09:37:59,405 Validation Data Eval:
2022-01-08 09:38:02,129   Average segmentation loss on validation set: 0.1736
2022-01-08 09:38:09,002 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:38:10,522 iteration 340 : loss : 0.185383, loss_ce: 0.070244
  5%|█▌                            | 20/400 [10:00<3:30:38, 33.26s/it]2022-01-08 09:38:12,069 iteration 341 : loss : 0.131239, loss_ce: 0.055174
2022-01-08 09:38:13,625 iteration 342 : loss : 0.126913, loss_ce: 0.040732
2022-01-08 09:38:15,204 iteration 343 : loss : 0.179897, loss_ce: 0.068747
2022-01-08 09:38:16,828 iteration 344 : loss : 0.174704, loss_ce: 0.066628
2022-01-08 09:38:18,403 iteration 345 : loss : 0.112813, loss_ce: 0.040132
2022-01-08 09:38:20,053 iteration 346 : loss : 0.174257, loss_ce: 0.067500
2022-01-08 09:38:21,579 iteration 347 : loss : 0.131640, loss_ce: 0.058903
2022-01-08 09:38:23,189 iteration 348 : loss : 0.165664, loss_ce: 0.058668
2022-01-08 09:38:24,913 iteration 349 : loss : 0.146553, loss_ce: 0.071898
2022-01-08 09:38:26,490 iteration 350 : loss : 0.137735, loss_ce: 0.054324
2022-01-08 09:38:28,131 iteration 351 : loss : 0.155139, loss_ce: 0.063374
2022-01-08 09:38:29,754 iteration 352 : loss : 0.131312, loss_ce: 0.050781
2022-01-08 09:38:31,283 iteration 353 : loss : 0.126891, loss_ce: 0.049090
2022-01-08 09:38:32,875 iteration 354 : loss : 0.130562, loss_ce: 0.047248
2022-01-08 09:38:34,518 iteration 355 : loss : 0.162870, loss_ce: 0.069411
2022-01-08 09:38:36,246 iteration 356 : loss : 0.157861, loss_ce: 0.069300
2022-01-08 09:38:37,757 iteration 357 : loss : 0.151640, loss_ce: 0.058829
  5%|█▌                            | 21/400 [10:28<3:18:39, 31.45s/it]2022-01-08 09:38:39,419 iteration 358 : loss : 0.163010, loss_ce: 0.071932
2022-01-08 09:38:41,085 iteration 359 : loss : 0.153717, loss_ce: 0.055805
2022-01-08 09:38:42,613 iteration 360 : loss : 0.120023, loss_ce: 0.042147
2022-01-08 09:38:44,240 iteration 361 : loss : 0.170350, loss_ce: 0.064104
2022-01-08 09:38:45,920 iteration 362 : loss : 0.147037, loss_ce: 0.047197
2022-01-08 09:38:47,597 iteration 363 : loss : 0.156746, loss_ce: 0.061233
2022-01-08 09:38:49,215 iteration 364 : loss : 0.143354, loss_ce: 0.053155
2022-01-08 09:38:50,702 iteration 365 : loss : 0.176732, loss_ce: 0.083817
2022-01-08 09:38:52,331 iteration 366 : loss : 0.184177, loss_ce: 0.079397
2022-01-08 09:38:53,840 iteration 367 : loss : 0.160791, loss_ce: 0.062676
2022-01-08 09:38:55,472 iteration 368 : loss : 0.144050, loss_ce: 0.051050
2022-01-08 09:38:57,137 iteration 369 : loss : 0.192574, loss_ce: 0.064035
2022-01-08 09:38:58,795 iteration 370 : loss : 0.158324, loss_ce: 0.078356
2022-01-08 09:39:00,411 iteration 371 : loss : 0.242396, loss_ce: 0.088634
2022-01-08 09:39:02,031 iteration 372 : loss : 0.211974, loss_ce: 0.106164
2022-01-08 09:39:03,564 iteration 373 : loss : 0.172560, loss_ce: 0.060075
2022-01-08 09:39:05,127 iteration 374 : loss : 0.147885, loss_ce: 0.056828
  6%|█▋                            | 22/400 [10:55<3:10:26, 30.23s/it]2022-01-08 09:39:06,825 iteration 375 : loss : 0.213427, loss_ce: 0.092239
2022-01-08 09:39:08,361 iteration 376 : loss : 0.120303, loss_ce: 0.043805
2022-01-08 09:39:09,929 iteration 377 : loss : 0.144513, loss_ce: 0.046038
2022-01-08 09:39:11,594 iteration 378 : loss : 0.148113, loss_ce: 0.053192
2022-01-08 09:39:13,091 iteration 379 : loss : 0.129369, loss_ce: 0.051051
2022-01-08 09:39:14,711 iteration 380 : loss : 0.148663, loss_ce: 0.056550
2022-01-08 09:39:16,289 iteration 381 : loss : 0.119116, loss_ce: 0.046700
2022-01-08 09:39:17,780 iteration 382 : loss : 0.153725, loss_ce: 0.070490
2022-01-08 09:39:19,359 iteration 383 : loss : 0.193931, loss_ce: 0.081418
2022-01-08 09:39:20,965 iteration 384 : loss : 0.117071, loss_ce: 0.050338
2022-01-08 09:39:22,546 iteration 385 : loss : 0.114020, loss_ce: 0.052157
2022-01-08 09:39:24,138 iteration 386 : loss : 0.132046, loss_ce: 0.046415
2022-01-08 09:39:25,749 iteration 387 : loss : 0.192743, loss_ce: 0.057466
2022-01-08 09:39:27,374 iteration 388 : loss : 0.160224, loss_ce: 0.065152
2022-01-08 09:39:28,992 iteration 389 : loss : 0.226324, loss_ce: 0.105279
2022-01-08 09:39:30,558 iteration 390 : loss : 0.126244, loss_ce: 0.046284
2022-01-08 09:39:32,122 iteration 391 : loss : 0.181569, loss_ce: 0.104273
  6%|█▋                            | 23/400 [11:22<3:03:48, 29.25s/it]2022-01-08 09:39:33,780 iteration 392 : loss : 0.140950, loss_ce: 0.080978
2022-01-08 09:39:35,324 iteration 393 : loss : 0.118279, loss_ce: 0.041858
2022-01-08 09:39:36,970 iteration 394 : loss : 0.126544, loss_ce: 0.044254
2022-01-08 09:39:38,580 iteration 395 : loss : 0.170909, loss_ce: 0.077987
2022-01-08 09:39:40,137 iteration 396 : loss : 0.134432, loss_ce: 0.051337
2022-01-08 09:39:41,723 iteration 397 : loss : 0.166654, loss_ce: 0.071971
2022-01-08 09:39:43,332 iteration 398 : loss : 0.096654, loss_ce: 0.040065
2022-01-08 09:39:44,804 iteration 399 : loss : 0.142610, loss_ce: 0.059505
2022-01-08 09:39:46,394 iteration 400 : loss : 0.151565, loss_ce: 0.053187
2022-01-08 09:39:48,049 iteration 401 : loss : 0.161409, loss_ce: 0.076199
2022-01-08 09:39:49,609 iteration 402 : loss : 0.088783, loss_ce: 0.041225
2022-01-08 09:39:51,165 iteration 403 : loss : 0.100965, loss_ce: 0.037803
2022-01-08 09:39:52,782 iteration 404 : loss : 0.139261, loss_ce: 0.058487
2022-01-08 09:39:54,446 iteration 405 : loss : 0.143762, loss_ce: 0.059342
2022-01-08 09:39:56,007 iteration 406 : loss : 0.132034, loss_ce: 0.048607
2022-01-08 09:39:57,526 iteration 407 : loss : 0.138142, loss_ce: 0.048183
2022-01-08 09:39:59,093 iteration 408 : loss : 0.168168, loss_ce: 0.059325
  6%|█▊                            | 24/400 [11:49<2:59:03, 28.57s/it]2022-01-08 09:40:00,751 iteration 409 : loss : 0.131994, loss_ce: 0.055975
2022-01-08 09:40:02,365 iteration 410 : loss : 0.117962, loss_ce: 0.043875
2022-01-08 09:40:03,948 iteration 411 : loss : 0.220488, loss_ce: 0.060941
2022-01-08 09:40:05,429 iteration 412 : loss : 0.120910, loss_ce: 0.045390
2022-01-08 09:40:07,018 iteration 413 : loss : 0.121075, loss_ce: 0.043117
2022-01-08 09:40:08,536 iteration 414 : loss : 0.163301, loss_ce: 0.075182
2022-01-08 09:40:09,999 iteration 415 : loss : 0.098668, loss_ce: 0.039373
2022-01-08 09:40:11,561 iteration 416 : loss : 0.157134, loss_ce: 0.075009
2022-01-08 09:40:13,092 iteration 417 : loss : 0.193608, loss_ce: 0.065903
2022-01-08 09:40:14,734 iteration 418 : loss : 0.121776, loss_ce: 0.051705
2022-01-08 09:40:16,283 iteration 419 : loss : 0.144813, loss_ce: 0.073308
2022-01-08 09:40:17,875 iteration 420 : loss : 0.112359, loss_ce: 0.043134
2022-01-08 09:40:19,400 iteration 421 : loss : 0.150255, loss_ce: 0.056994
2022-01-08 09:40:20,920 iteration 422 : loss : 0.143895, loss_ce: 0.059777
2022-01-08 09:40:22,449 iteration 423 : loss : 0.134339, loss_ce: 0.047291
2022-01-08 09:40:24,038 iteration 424 : loss : 0.141040, loss_ce: 0.056883
2022-01-08 09:40:24,038 Training Data Eval:
2022-01-08 09:40:31,926   Average segmentation loss on training set: 0.1175
2022-01-08 09:40:31,926 Validation Data Eval:
2022-01-08 09:40:34,649   Average segmentation loss on validation set: 0.1811
2022-01-08 09:40:36,197 iteration 425 : loss : 0.124103, loss_ce: 0.050005
  6%|█▉                            | 25/400 [12:26<3:14:34, 31.13s/it]2022-01-08 09:40:37,762 iteration 426 : loss : 0.128676, loss_ce: 0.041049
2022-01-08 09:40:39,383 iteration 427 : loss : 0.102448, loss_ce: 0.037588
2022-01-08 09:40:40,965 iteration 428 : loss : 0.134739, loss_ce: 0.052314
2022-01-08 09:40:42,487 iteration 429 : loss : 0.146612, loss_ce: 0.078601
2022-01-08 09:40:43,996 iteration 430 : loss : 0.155314, loss_ce: 0.050448
2022-01-08 09:40:45,536 iteration 431 : loss : 0.177902, loss_ce: 0.066303
2022-01-08 09:40:47,143 iteration 432 : loss : 0.097334, loss_ce: 0.041310
2022-01-08 09:40:48,791 iteration 433 : loss : 0.107208, loss_ce: 0.042499
2022-01-08 09:40:50,381 iteration 434 : loss : 0.096117, loss_ce: 0.041446
2022-01-08 09:40:51,897 iteration 435 : loss : 0.119735, loss_ce: 0.047867
2022-01-08 09:40:53,451 iteration 436 : loss : 0.122888, loss_ce: 0.050716
2022-01-08 09:40:55,014 iteration 437 : loss : 0.180682, loss_ce: 0.085331
2022-01-08 09:40:56,519 iteration 438 : loss : 0.111930, loss_ce: 0.036607
2022-01-08 09:40:58,101 iteration 439 : loss : 0.126273, loss_ce: 0.039843
2022-01-08 09:40:59,655 iteration 440 : loss : 0.142113, loss_ce: 0.047297
2022-01-08 09:41:01,277 iteration 441 : loss : 0.119431, loss_ce: 0.044537
2022-01-08 09:41:02,884 iteration 442 : loss : 0.137019, loss_ce: 0.062808
  6%|█▉                            | 26/400 [12:53<3:05:44, 29.80s/it]2022-01-08 09:41:04,480 iteration 443 : loss : 0.132936, loss_ce: 0.060262
2022-01-08 09:41:05,997 iteration 444 : loss : 0.128301, loss_ce: 0.050110
2022-01-08 09:41:07,560 iteration 445 : loss : 0.137597, loss_ce: 0.044601
2022-01-08 09:41:09,280 iteration 446 : loss : 0.134960, loss_ce: 0.050369
2022-01-08 09:41:10,934 iteration 447 : loss : 0.101344, loss_ce: 0.056218
2022-01-08 09:41:12,568 iteration 448 : loss : 0.222762, loss_ce: 0.061356
2022-01-08 09:41:14,154 iteration 449 : loss : 0.115068, loss_ce: 0.052667
2022-01-08 09:41:15,715 iteration 450 : loss : 0.125830, loss_ce: 0.042200
2022-01-08 09:41:17,380 iteration 451 : loss : 0.098599, loss_ce: 0.039664
2022-01-08 09:41:18,999 iteration 452 : loss : 0.099844, loss_ce: 0.037878
2022-01-08 09:41:20,606 iteration 453 : loss : 0.121960, loss_ce: 0.051139
2022-01-08 09:41:22,218 iteration 454 : loss : 0.102166, loss_ce: 0.033164
2022-01-08 09:41:23,864 iteration 455 : loss : 0.111872, loss_ce: 0.041735
2022-01-08 09:41:25,465 iteration 456 : loss : 0.095735, loss_ce: 0.031069
2022-01-08 09:41:27,052 iteration 457 : loss : 0.135247, loss_ce: 0.054486
2022-01-08 09:41:28,720 iteration 458 : loss : 0.134373, loss_ce: 0.064787
2022-01-08 09:41:30,235 iteration 459 : loss : 0.080372, loss_ce: 0.031115
  7%|██                            | 27/400 [13:20<3:00:40, 29.06s/it]2022-01-08 09:41:31,882 iteration 460 : loss : 0.126435, loss_ce: 0.063151
2022-01-08 09:41:33,490 iteration 461 : loss : 0.108223, loss_ce: 0.049582
2022-01-08 09:41:35,059 iteration 462 : loss : 0.120998, loss_ce: 0.052171
2022-01-08 09:41:36,713 iteration 463 : loss : 0.126805, loss_ce: 0.062269
2022-01-08 09:41:38,307 iteration 464 : loss : 0.088212, loss_ce: 0.034527
2022-01-08 09:41:39,840 iteration 465 : loss : 0.111970, loss_ce: 0.033424
2022-01-08 09:41:41,411 iteration 466 : loss : 0.098615, loss_ce: 0.031176
2022-01-08 09:41:42,966 iteration 467 : loss : 0.130274, loss_ce: 0.044302
2022-01-08 09:41:44,654 iteration 468 : loss : 0.128905, loss_ce: 0.049925
2022-01-08 09:41:46,215 iteration 469 : loss : 0.113817, loss_ce: 0.034671
2022-01-08 09:41:47,788 iteration 470 : loss : 0.120564, loss_ce: 0.042791
2022-01-08 09:41:49,357 iteration 471 : loss : 0.127670, loss_ce: 0.039832
2022-01-08 09:41:50,992 iteration 472 : loss : 0.141149, loss_ce: 0.053262
2022-01-08 09:41:52,732 iteration 473 : loss : 0.129232, loss_ce: 0.050222
2022-01-08 09:41:54,418 iteration 474 : loss : 0.173917, loss_ce: 0.076654
2022-01-08 09:41:55,979 iteration 475 : loss : 0.134957, loss_ce: 0.057707
2022-01-08 09:41:57,628 iteration 476 : loss : 0.162686, loss_ce: 0.087475
  7%|██                            | 28/400 [13:47<2:57:05, 28.56s/it]2022-01-08 09:41:59,256 iteration 477 : loss : 0.168360, loss_ce: 0.059908
2022-01-08 09:42:00,892 iteration 478 : loss : 0.105852, loss_ce: 0.038558
2022-01-08 09:42:02,474 iteration 479 : loss : 0.151887, loss_ce: 0.060598
2022-01-08 09:42:04,082 iteration 480 : loss : 0.107550, loss_ce: 0.051128
2022-01-08 09:42:05,577 iteration 481 : loss : 0.126699, loss_ce: 0.050338
2022-01-08 09:42:07,176 iteration 482 : loss : 0.168938, loss_ce: 0.047399
2022-01-08 09:42:08,786 iteration 483 : loss : 0.145481, loss_ce: 0.048156
2022-01-08 09:42:10,416 iteration 484 : loss : 0.196727, loss_ce: 0.074258
2022-01-08 09:42:11,979 iteration 485 : loss : 0.128785, loss_ce: 0.052172
2022-01-08 09:42:13,534 iteration 486 : loss : 0.139524, loss_ce: 0.038266
2022-01-08 09:42:15,172 iteration 487 : loss : 0.126867, loss_ce: 0.050661
2022-01-08 09:42:16,698 iteration 488 : loss : 0.122410, loss_ce: 0.049363
2022-01-08 09:42:18,285 iteration 489 : loss : 0.093888, loss_ce: 0.039615
2022-01-08 09:42:19,752 iteration 490 : loss : 0.135518, loss_ce: 0.052472
2022-01-08 09:42:21,340 iteration 491 : loss : 0.119495, loss_ce: 0.051752
2022-01-08 09:42:22,900 iteration 492 : loss : 0.099106, loss_ce: 0.047764
2022-01-08 09:42:24,508 iteration 493 : loss : 0.115848, loss_ce: 0.056548
  7%|██▏                           | 29/400 [14:14<2:53:29, 28.06s/it]2022-01-08 09:42:26,126 iteration 494 : loss : 0.103775, loss_ce: 0.045344
2022-01-08 09:42:27,749 iteration 495 : loss : 0.136656, loss_ce: 0.047560
2022-01-08 09:42:29,322 iteration 496 : loss : 0.132736, loss_ce: 0.058230
2022-01-08 09:42:30,931 iteration 497 : loss : 0.115482, loss_ce: 0.045420
2022-01-08 09:42:32,578 iteration 498 : loss : 0.119267, loss_ce: 0.054385
2022-01-08 09:42:34,089 iteration 499 : loss : 0.096219, loss_ce: 0.039411
2022-01-08 09:42:35,615 iteration 500 : loss : 0.104822, loss_ce: 0.037697
2022-01-08 09:42:37,143 iteration 501 : loss : 0.113616, loss_ce: 0.049717
2022-01-08 09:42:38,792 iteration 502 : loss : 0.082772, loss_ce: 0.032033
2022-01-08 09:42:40,334 iteration 503 : loss : 0.143835, loss_ce: 0.046941
2022-01-08 09:42:41,945 iteration 504 : loss : 0.109406, loss_ce: 0.036356
2022-01-08 09:42:43,479 iteration 505 : loss : 0.089218, loss_ce: 0.034020
2022-01-08 09:42:45,163 iteration 506 : loss : 0.112117, loss_ce: 0.043037
2022-01-08 09:42:46,758 iteration 507 : loss : 0.151946, loss_ce: 0.061280
2022-01-08 09:42:48,350 iteration 508 : loss : 0.168348, loss_ce: 0.073720
2022-01-08 09:42:49,987 iteration 509 : loss : 0.112564, loss_ce: 0.037907
2022-01-08 09:42:49,987 Training Data Eval:
2022-01-08 09:42:57,860   Average segmentation loss on training set: 0.0979
2022-01-08 09:42:57,860 Validation Data Eval:
2022-01-08 09:43:00,582   Average segmentation loss on validation set: 0.1307
2022-01-08 09:43:06,398 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:43:07,918 iteration 510 : loss : 0.089979, loss_ce: 0.036139
  8%|██▎                           | 30/400 [14:58<3:21:24, 32.66s/it]2022-01-08 09:43:09,432 iteration 511 : loss : 0.143735, loss_ce: 0.057551
2022-01-08 09:43:10,936 iteration 512 : loss : 0.137228, loss_ce: 0.060250
2022-01-08 09:43:12,487 iteration 513 : loss : 0.111066, loss_ce: 0.033481
2022-01-08 09:43:14,061 iteration 514 : loss : 0.096308, loss_ce: 0.039372
2022-01-08 09:43:15,612 iteration 515 : loss : 0.189292, loss_ce: 0.090002
2022-01-08 09:43:17,216 iteration 516 : loss : 0.124146, loss_ce: 0.053235
2022-01-08 09:43:18,863 iteration 517 : loss : 0.125055, loss_ce: 0.054259
2022-01-08 09:43:20,390 iteration 518 : loss : 0.143266, loss_ce: 0.051166
2022-01-08 09:43:21,980 iteration 519 : loss : 0.134618, loss_ce: 0.057787
2022-01-08 09:43:23,505 iteration 520 : loss : 0.100400, loss_ce: 0.038772
2022-01-08 09:43:25,097 iteration 521 : loss : 0.117333, loss_ce: 0.044930
2022-01-08 09:43:26,720 iteration 522 : loss : 0.103738, loss_ce: 0.036981
2022-01-08 09:43:28,382 iteration 523 : loss : 0.099470, loss_ce: 0.040799
2022-01-08 09:43:30,079 iteration 524 : loss : 0.116523, loss_ce: 0.044464
2022-01-08 09:43:31,663 iteration 525 : loss : 0.157943, loss_ce: 0.074568
2022-01-08 09:43:33,242 iteration 526 : loss : 0.141094, loss_ce: 0.062011
2022-01-08 09:43:34,819 iteration 527 : loss : 0.123875, loss_ce: 0.041427
  8%|██▎                           | 31/400 [15:25<3:10:14, 30.93s/it]2022-01-08 09:43:36,414 iteration 528 : loss : 0.101252, loss_ce: 0.039830
2022-01-08 09:43:38,006 iteration 529 : loss : 0.101595, loss_ce: 0.028296
2022-01-08 09:43:39,545 iteration 530 : loss : 0.134259, loss_ce: 0.044642
2022-01-08 09:43:41,064 iteration 531 : loss : 0.094293, loss_ce: 0.030443
2022-01-08 09:43:42,602 iteration 532 : loss : 0.107111, loss_ce: 0.042691
2022-01-08 09:43:44,287 iteration 533 : loss : 0.069104, loss_ce: 0.028117
2022-01-08 09:43:45,810 iteration 534 : loss : 0.086681, loss_ce: 0.036111
2022-01-08 09:43:47,435 iteration 535 : loss : 0.080531, loss_ce: 0.029220
2022-01-08 09:43:49,097 iteration 536 : loss : 0.132657, loss_ce: 0.056748
2022-01-08 09:43:50,778 iteration 537 : loss : 0.132196, loss_ce: 0.045010
2022-01-08 09:43:52,387 iteration 538 : loss : 0.096292, loss_ce: 0.042669
2022-01-08 09:43:53,949 iteration 539 : loss : 0.108454, loss_ce: 0.043805
2022-01-08 09:43:55,556 iteration 540 : loss : 0.107491, loss_ce: 0.038147
2022-01-08 09:43:57,116 iteration 541 : loss : 0.099957, loss_ce: 0.040296
2022-01-08 09:43:58,695 iteration 542 : loss : 0.108836, loss_ce: 0.049757
2022-01-08 09:44:00,221 iteration 543 : loss : 0.068492, loss_ce: 0.025274
2022-01-08 09:44:01,783 iteration 544 : loss : 0.074969, loss_ce: 0.027430
  8%|██▍                           | 32/400 [15:52<3:02:25, 29.74s/it]2022-01-08 09:44:03,443 iteration 545 : loss : 0.083174, loss_ce: 0.040821
2022-01-08 09:44:05,026 iteration 546 : loss : 0.109032, loss_ce: 0.040743
2022-01-08 09:44:06,607 iteration 547 : loss : 0.112396, loss_ce: 0.049574
2022-01-08 09:44:08,175 iteration 548 : loss : 0.144299, loss_ce: 0.048130
2022-01-08 09:44:09,783 iteration 549 : loss : 0.074363, loss_ce: 0.031384
2022-01-08 09:44:11,328 iteration 550 : loss : 0.091545, loss_ce: 0.038895
2022-01-08 09:44:12,902 iteration 551 : loss : 0.083591, loss_ce: 0.030713
2022-01-08 09:44:14,450 iteration 552 : loss : 0.086962, loss_ce: 0.030723
2022-01-08 09:44:16,063 iteration 553 : loss : 0.110556, loss_ce: 0.044406
2022-01-08 09:44:17,575 iteration 554 : loss : 0.079620, loss_ce: 0.037478
2022-01-08 09:44:19,142 iteration 555 : loss : 0.124035, loss_ce: 0.037730
2022-01-08 09:44:20,764 iteration 556 : loss : 0.104739, loss_ce: 0.047682
2022-01-08 09:44:22,259 iteration 557 : loss : 0.118908, loss_ce: 0.046680
2022-01-08 09:44:23,873 iteration 558 : loss : 0.101067, loss_ce: 0.036894
2022-01-08 09:44:25,448 iteration 559 : loss : 0.112181, loss_ce: 0.045781
2022-01-08 09:44:27,050 iteration 560 : loss : 0.103487, loss_ce: 0.026545
2022-01-08 09:44:28,679 iteration 561 : loss : 0.157639, loss_ce: 0.082062
  8%|██▍                           | 33/400 [16:18<2:56:43, 28.89s/it]2022-01-08 09:44:30,442 iteration 562 : loss : 0.145842, loss_ce: 0.058775
2022-01-08 09:44:31,926 iteration 563 : loss : 0.097829, loss_ce: 0.042314
2022-01-08 09:44:33,533 iteration 564 : loss : 0.102216, loss_ce: 0.052812
2022-01-08 09:44:35,142 iteration 565 : loss : 0.085655, loss_ce: 0.034453
2022-01-08 09:44:36,782 iteration 566 : loss : 0.086972, loss_ce: 0.038137
2022-01-08 09:44:38,338 iteration 567 : loss : 0.088813, loss_ce: 0.028691
2022-01-08 09:44:39,883 iteration 568 : loss : 0.150640, loss_ce: 0.050597
2022-01-08 09:44:41,460 iteration 569 : loss : 0.101389, loss_ce: 0.040291
2022-01-08 09:44:43,078 iteration 570 : loss : 0.083237, loss_ce: 0.038878
2022-01-08 09:44:44,637 iteration 571 : loss : 0.192211, loss_ce: 0.085600
2022-01-08 09:44:46,238 iteration 572 : loss : 0.116275, loss_ce: 0.036386
2022-01-08 09:44:47,781 iteration 573 : loss : 0.123395, loss_ce: 0.050732
2022-01-08 09:44:49,361 iteration 574 : loss : 0.125371, loss_ce: 0.045388
2022-01-08 09:44:51,009 iteration 575 : loss : 0.106302, loss_ce: 0.031084
2022-01-08 09:44:52,640 iteration 576 : loss : 0.135371, loss_ce: 0.071834
2022-01-08 09:44:54,220 iteration 577 : loss : 0.067281, loss_ce: 0.026169
2022-01-08 09:44:55,753 iteration 578 : loss : 0.089059, loss_ce: 0.036665
  8%|██▌                           | 34/400 [16:46<2:52:53, 28.34s/it]2022-01-08 09:44:57,558 iteration 579 : loss : 0.130567, loss_ce: 0.070724
2022-01-08 09:44:59,163 iteration 580 : loss : 0.108626, loss_ce: 0.044241
2022-01-08 09:45:00,801 iteration 581 : loss : 0.135707, loss_ce: 0.041127
2022-01-08 09:45:02,383 iteration 582 : loss : 0.106584, loss_ce: 0.041626
2022-01-08 09:45:03,975 iteration 583 : loss : 0.195751, loss_ce: 0.065664
2022-01-08 09:45:05,524 iteration 584 : loss : 0.087652, loss_ce: 0.040816
2022-01-08 09:45:07,164 iteration 585 : loss : 0.103614, loss_ce: 0.040020
2022-01-08 09:45:08,705 iteration 586 : loss : 0.149897, loss_ce: 0.056652
2022-01-08 09:45:10,238 iteration 587 : loss : 0.080486, loss_ce: 0.035394
2022-01-08 09:45:11,849 iteration 588 : loss : 0.105898, loss_ce: 0.035560
2022-01-08 09:45:13,413 iteration 589 : loss : 0.084416, loss_ce: 0.032151
2022-01-08 09:45:14,993 iteration 590 : loss : 0.102714, loss_ce: 0.034923
2022-01-08 09:45:16,585 iteration 591 : loss : 0.104430, loss_ce: 0.036643
2022-01-08 09:45:18,175 iteration 592 : loss : 0.096884, loss_ce: 0.038568
2022-01-08 09:45:19,792 iteration 593 : loss : 0.079304, loss_ce: 0.028287
2022-01-08 09:45:21,389 iteration 594 : loss : 0.108244, loss_ce: 0.046827
2022-01-08 09:45:21,389 Training Data Eval:
2022-01-08 09:45:29,262   Average segmentation loss on training set: 0.1520
2022-01-08 09:45:29,262 Validation Data Eval:
2022-01-08 09:45:31,987   Average segmentation loss on validation set: 0.1500
2022-01-08 09:45:33,647 iteration 595 : loss : 0.095206, loss_ce: 0.049930
  9%|██▋                           | 35/400 [17:23<3:09:51, 31.21s/it]2022-01-08 09:45:35,283 iteration 596 : loss : 0.085313, loss_ce: 0.034893
2022-01-08 09:45:36,847 iteration 597 : loss : 0.120597, loss_ce: 0.038470
2022-01-08 09:45:38,386 iteration 598 : loss : 0.077051, loss_ce: 0.028621
2022-01-08 09:45:39,951 iteration 599 : loss : 0.114674, loss_ce: 0.046361
2022-01-08 09:45:41,512 iteration 600 : loss : 0.104116, loss_ce: 0.045010
2022-01-08 09:45:43,108 iteration 601 : loss : 0.089980, loss_ce: 0.032329
2022-01-08 09:45:44,717 iteration 602 : loss : 0.079194, loss_ce: 0.032092
2022-01-08 09:45:46,223 iteration 603 : loss : 0.085457, loss_ce: 0.029926
2022-01-08 09:45:47,748 iteration 604 : loss : 0.072208, loss_ce: 0.024839
2022-01-08 09:45:49,256 iteration 605 : loss : 0.095059, loss_ce: 0.033983
2022-01-08 09:45:50,979 iteration 606 : loss : 0.089992, loss_ce: 0.037851
2022-01-08 09:45:52,551 iteration 607 : loss : 0.102600, loss_ce: 0.043554
2022-01-08 09:45:54,127 iteration 608 : loss : 0.091640, loss_ce: 0.043124
2022-01-08 09:45:55,615 iteration 609 : loss : 0.080173, loss_ce: 0.029875
2022-01-08 09:45:57,243 iteration 610 : loss : 0.071795, loss_ce: 0.030846
2022-01-08 09:45:58,930 iteration 611 : loss : 0.099681, loss_ce: 0.048129
2022-01-08 09:46:00,532 iteration 612 : loss : 0.138481, loss_ce: 0.047105
  9%|██▋                           | 36/400 [17:50<3:01:28, 29.91s/it]2022-01-08 09:46:02,187 iteration 613 : loss : 0.079477, loss_ce: 0.032799
2022-01-08 09:46:03,725 iteration 614 : loss : 0.094585, loss_ce: 0.030340
2022-01-08 09:46:05,258 iteration 615 : loss : 0.119182, loss_ce: 0.050295
2022-01-08 09:46:06,915 iteration 616 : loss : 0.092062, loss_ce: 0.040261
2022-01-08 09:46:08,454 iteration 617 : loss : 0.108739, loss_ce: 0.051157
2022-01-08 09:46:10,044 iteration 618 : loss : 0.157202, loss_ce: 0.041590
2022-01-08 09:46:11,638 iteration 619 : loss : 0.149829, loss_ce: 0.055956
2022-01-08 09:46:13,333 iteration 620 : loss : 0.176980, loss_ce: 0.060758
2022-01-08 09:46:14,924 iteration 621 : loss : 0.072042, loss_ce: 0.028144
2022-01-08 09:46:16,647 iteration 622 : loss : 0.088662, loss_ce: 0.031338
2022-01-08 09:46:18,340 iteration 623 : loss : 0.138169, loss_ce: 0.068009
2022-01-08 09:46:19,916 iteration 624 : loss : 0.097860, loss_ce: 0.032172
2022-01-08 09:46:21,450 iteration 625 : loss : 0.086096, loss_ce: 0.038139
2022-01-08 09:46:23,086 iteration 626 : loss : 0.105096, loss_ce: 0.053156
2022-01-08 09:46:24,727 iteration 627 : loss : 0.102766, loss_ce: 0.034294
2022-01-08 09:46:26,419 iteration 628 : loss : 0.092116, loss_ce: 0.034104
2022-01-08 09:46:27,990 iteration 629 : loss : 0.064714, loss_ce: 0.025824
  9%|██▊                           | 37/400 [18:18<2:56:30, 29.18s/it]2022-01-08 09:46:29,655 iteration 630 : loss : 0.068892, loss_ce: 0.030648
2022-01-08 09:46:31,213 iteration 631 : loss : 0.058831, loss_ce: 0.025724
2022-01-08 09:46:32,773 iteration 632 : loss : 0.152008, loss_ce: 0.076467
2022-01-08 09:46:34,438 iteration 633 : loss : 0.080553, loss_ce: 0.032106
2022-01-08 09:46:36,030 iteration 634 : loss : 0.145148, loss_ce: 0.056982
2022-01-08 09:46:37,726 iteration 635 : loss : 0.092284, loss_ce: 0.039310
2022-01-08 09:46:39,342 iteration 636 : loss : 0.103107, loss_ce: 0.040775
2022-01-08 09:46:40,857 iteration 637 : loss : 0.113880, loss_ce: 0.051580
2022-01-08 09:46:42,559 iteration 638 : loss : 0.107361, loss_ce: 0.035448
2022-01-08 09:46:44,105 iteration 639 : loss : 0.070395, loss_ce: 0.027535
2022-01-08 09:46:45,622 iteration 640 : loss : 0.126557, loss_ce: 0.037389
2022-01-08 09:46:47,142 iteration 641 : loss : 0.093583, loss_ce: 0.038995
2022-01-08 09:46:48,671 iteration 642 : loss : 0.100442, loss_ce: 0.027400
2022-01-08 09:46:50,235 iteration 643 : loss : 0.107637, loss_ce: 0.040966
2022-01-08 09:46:51,807 iteration 644 : loss : 0.097234, loss_ce: 0.041504
2022-01-08 09:46:53,343 iteration 645 : loss : 0.085601, loss_ce: 0.026994
2022-01-08 09:46:54,994 iteration 646 : loss : 0.092738, loss_ce: 0.040725
 10%|██▊                           | 38/400 [18:45<2:52:05, 28.52s/it]2022-01-08 09:46:56,593 iteration 647 : loss : 0.089443, loss_ce: 0.035870
2022-01-08 09:46:58,178 iteration 648 : loss : 0.071986, loss_ce: 0.029719
2022-01-08 09:46:59,792 iteration 649 : loss : 0.086481, loss_ce: 0.033417
2022-01-08 09:47:01,455 iteration 650 : loss : 0.100350, loss_ce: 0.040377
2022-01-08 09:47:03,038 iteration 651 : loss : 0.068172, loss_ce: 0.025581
2022-01-08 09:47:04,510 iteration 652 : loss : 0.071554, loss_ce: 0.027210
2022-01-08 09:47:06,169 iteration 653 : loss : 0.093767, loss_ce: 0.041220
2022-01-08 09:47:07,697 iteration 654 : loss : 0.083627, loss_ce: 0.029905
2022-01-08 09:47:09,277 iteration 655 : loss : 0.112682, loss_ce: 0.036800
2022-01-08 09:47:10,870 iteration 656 : loss : 0.085624, loss_ce: 0.030176
2022-01-08 09:47:12,533 iteration 657 : loss : 0.054240, loss_ce: 0.019263
2022-01-08 09:47:14,146 iteration 658 : loss : 0.097600, loss_ce: 0.031151
2022-01-08 09:47:15,765 iteration 659 : loss : 0.109525, loss_ce: 0.041356
2022-01-08 09:47:17,318 iteration 660 : loss : 0.071618, loss_ce: 0.032357
2022-01-08 09:47:18,930 iteration 661 : loss : 0.086155, loss_ce: 0.036692
2022-01-08 09:47:20,625 iteration 662 : loss : 0.162369, loss_ce: 0.045974
2022-01-08 09:47:22,218 iteration 663 : loss : 0.095341, loss_ce: 0.040334
 10%|██▉                           | 39/400 [19:12<2:49:15, 28.13s/it]2022-01-08 09:47:23,879 iteration 664 : loss : 0.084042, loss_ce: 0.035330
2022-01-08 09:47:25,385 iteration 665 : loss : 0.081508, loss_ce: 0.036306
2022-01-08 09:47:26,957 iteration 666 : loss : 0.069286, loss_ce: 0.026758
2022-01-08 09:47:28,581 iteration 667 : loss : 0.054422, loss_ce: 0.022635
2022-01-08 09:47:30,177 iteration 668 : loss : 0.114836, loss_ce: 0.046810
2022-01-08 09:47:31,739 iteration 669 : loss : 0.079820, loss_ce: 0.022623
2022-01-08 09:47:33,460 iteration 670 : loss : 0.081541, loss_ce: 0.029047
2022-01-08 09:47:35,022 iteration 671 : loss : 0.086840, loss_ce: 0.028298
2022-01-08 09:47:36,682 iteration 672 : loss : 0.089748, loss_ce: 0.029301
2022-01-08 09:47:38,312 iteration 673 : loss : 0.085254, loss_ce: 0.035053
2022-01-08 09:47:39,928 iteration 674 : loss : 0.084027, loss_ce: 0.037572
2022-01-08 09:47:41,573 iteration 675 : loss : 0.128317, loss_ce: 0.052142
2022-01-08 09:47:43,153 iteration 676 : loss : 0.081279, loss_ce: 0.031010
2022-01-08 09:47:44,749 iteration 677 : loss : 0.065934, loss_ce: 0.029410
2022-01-08 09:47:46,374 iteration 678 : loss : 0.096750, loss_ce: 0.032421
2022-01-08 09:47:47,986 iteration 679 : loss : 0.090134, loss_ce: 0.038950
2022-01-08 09:47:47,987 Training Data Eval:
2022-01-08 09:47:55,873   Average segmentation loss on training set: 0.1166
2022-01-08 09:47:55,874 Validation Data Eval:
2022-01-08 09:47:58,601   Average segmentation loss on validation set: 0.1269
2022-01-08 09:48:04,420 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:48:05,895 iteration 680 : loss : 0.100850, loss_ce: 0.033162
 10%|███                           | 40/400 [19:56<3:16:47, 32.80s/it]2022-01-08 09:48:07,419 iteration 681 : loss : 0.053900, loss_ce: 0.018920
2022-01-08 09:48:08,881 iteration 682 : loss : 0.125271, loss_ce: 0.036146
2022-01-08 09:48:10,460 iteration 683 : loss : 0.093354, loss_ce: 0.042415
2022-01-08 09:48:12,001 iteration 684 : loss : 0.100415, loss_ce: 0.034015
2022-01-08 09:48:13,653 iteration 685 : loss : 0.090029, loss_ce: 0.035433
2022-01-08 09:48:15,240 iteration 686 : loss : 0.107092, loss_ce: 0.041633
2022-01-08 09:48:16,825 iteration 687 : loss : 0.081643, loss_ce: 0.036058
2022-01-08 09:48:18,384 iteration 688 : loss : 0.080138, loss_ce: 0.028855
2022-01-08 09:48:19,994 iteration 689 : loss : 0.063085, loss_ce: 0.022503
2022-01-08 09:48:21,568 iteration 690 : loss : 0.090468, loss_ce: 0.034225
2022-01-08 09:48:23,165 iteration 691 : loss : 0.092422, loss_ce: 0.032500
2022-01-08 09:48:24,812 iteration 692 : loss : 0.068952, loss_ce: 0.025216
2022-01-08 09:48:26,332 iteration 693 : loss : 0.075107, loss_ce: 0.033937
2022-01-08 09:48:27,929 iteration 694 : loss : 0.088596, loss_ce: 0.037927
2022-01-08 09:48:29,470 iteration 695 : loss : 0.085600, loss_ce: 0.034279
2022-01-08 09:48:31,125 iteration 696 : loss : 0.081779, loss_ce: 0.034425
2022-01-08 09:48:32,715 iteration 697 : loss : 0.083614, loss_ce: 0.034303
 10%|███                           | 41/400 [20:22<3:05:31, 31.01s/it]2022-01-08 09:48:34,309 iteration 698 : loss : 0.085626, loss_ce: 0.026955
2022-01-08 09:48:35,942 iteration 699 : loss : 0.089358, loss_ce: 0.037872
2022-01-08 09:48:37,507 iteration 700 : loss : 0.089900, loss_ce: 0.038057
2022-01-08 09:48:39,065 iteration 701 : loss : 0.118656, loss_ce: 0.063529
2022-01-08 09:48:40,694 iteration 702 : loss : 0.103398, loss_ce: 0.038515
2022-01-08 09:48:42,379 iteration 703 : loss : 0.061279, loss_ce: 0.027314
2022-01-08 09:48:43,899 iteration 704 : loss : 0.060242, loss_ce: 0.023338
2022-01-08 09:48:45,484 iteration 705 : loss : 0.102539, loss_ce: 0.036232
2022-01-08 09:48:47,084 iteration 706 : loss : 0.068094, loss_ce: 0.026006
2022-01-08 09:48:48,598 iteration 707 : loss : 0.049966, loss_ce: 0.020812
2022-01-08 09:48:50,178 iteration 708 : loss : 0.060739, loss_ce: 0.023848
2022-01-08 09:48:51,762 iteration 709 : loss : 0.095858, loss_ce: 0.038941
2022-01-08 09:48:53,387 iteration 710 : loss : 0.078720, loss_ce: 0.029001
2022-01-08 09:48:55,002 iteration 711 : loss : 0.091679, loss_ce: 0.037205
2022-01-08 09:48:56,684 iteration 712 : loss : 0.100556, loss_ce: 0.038401
2022-01-08 09:48:58,261 iteration 713 : loss : 0.057354, loss_ce: 0.024151
2022-01-08 09:48:59,922 iteration 714 : loss : 0.099428, loss_ce: 0.033762
 10%|███▏                          | 42/400 [20:50<2:58:12, 29.87s/it]2022-01-08 09:49:01,606 iteration 715 : loss : 0.067916, loss_ce: 0.023695
2022-01-08 09:49:03,250 iteration 716 : loss : 0.068506, loss_ce: 0.030265
2022-01-08 09:49:04,840 iteration 717 : loss : 0.076510, loss_ce: 0.027375
2022-01-08 09:49:06,444 iteration 718 : loss : 0.078615, loss_ce: 0.032523
2022-01-08 09:49:08,106 iteration 719 : loss : 0.082459, loss_ce: 0.030647
2022-01-08 09:49:09,699 iteration 720 : loss : 0.090873, loss_ce: 0.033365
2022-01-08 09:49:11,283 iteration 721 : loss : 0.086729, loss_ce: 0.030665
2022-01-08 09:49:12,863 iteration 722 : loss : 0.087967, loss_ce: 0.028328
2022-01-08 09:49:14,503 iteration 723 : loss : 0.096350, loss_ce: 0.051156
2022-01-08 09:49:16,068 iteration 724 : loss : 0.123353, loss_ce: 0.051504
2022-01-08 09:49:17,718 iteration 725 : loss : 0.077892, loss_ce: 0.026812
2022-01-08 09:49:19,299 iteration 726 : loss : 0.080599, loss_ce: 0.028720
2022-01-08 09:49:20,812 iteration 727 : loss : 0.065647, loss_ce: 0.024060
2022-01-08 09:49:22,436 iteration 728 : loss : 0.064099, loss_ce: 0.024867
2022-01-08 09:49:24,075 iteration 729 : loss : 0.065942, loss_ce: 0.025703
2022-01-08 09:49:25,689 iteration 730 : loss : 0.078999, loss_ce: 0.027767
2022-01-08 09:49:27,183 iteration 731 : loss : 0.065636, loss_ce: 0.023194
 11%|███▏                          | 43/400 [21:17<2:53:03, 29.08s/it]2022-01-08 09:49:28,748 iteration 732 : loss : 0.082377, loss_ce: 0.035477
2022-01-08 09:49:30,249 iteration 733 : loss : 0.093276, loss_ce: 0.042233
2022-01-08 09:49:31,882 iteration 734 : loss : 0.071751, loss_ce: 0.026627
2022-01-08 09:49:33,453 iteration 735 : loss : 0.086460, loss_ce: 0.035264
2022-01-08 09:49:35,001 iteration 736 : loss : 0.088772, loss_ce: 0.038051
2022-01-08 09:49:36,710 iteration 737 : loss : 0.093228, loss_ce: 0.036868
2022-01-08 09:49:38,204 iteration 738 : loss : 0.065175, loss_ce: 0.028307
2022-01-08 09:49:39,823 iteration 739 : loss : 0.084936, loss_ce: 0.031509
2022-01-08 09:49:41,386 iteration 740 : loss : 0.073265, loss_ce: 0.029036
2022-01-08 09:49:42,933 iteration 741 : loss : 0.055407, loss_ce: 0.021933
2022-01-08 09:49:44,433 iteration 742 : loss : 0.092825, loss_ce: 0.041242
2022-01-08 09:49:46,021 iteration 743 : loss : 0.068364, loss_ce: 0.025487
2022-01-08 09:49:47,640 iteration 744 : loss : 0.086866, loss_ce: 0.035927
2022-01-08 09:49:49,183 iteration 745 : loss : 0.106080, loss_ce: 0.020848
2022-01-08 09:49:50,734 iteration 746 : loss : 0.072021, loss_ce: 0.024765
2022-01-08 09:49:52,282 iteration 747 : loss : 0.096810, loss_ce: 0.038070
2022-01-08 09:49:53,813 iteration 748 : loss : 0.062809, loss_ce: 0.025927
 11%|███▎                          | 44/400 [21:44<2:48:12, 28.35s/it]2022-01-08 09:49:55,371 iteration 749 : loss : 0.177599, loss_ce: 0.034827
2022-01-08 09:49:56,931 iteration 750 : loss : 0.050433, loss_ce: 0.020098
2022-01-08 09:49:58,531 iteration 751 : loss : 0.060256, loss_ce: 0.018959
2022-01-08 09:50:00,091 iteration 752 : loss : 0.117389, loss_ce: 0.051516
2022-01-08 09:50:01,614 iteration 753 : loss : 0.081969, loss_ce: 0.036102
2022-01-08 09:50:03,151 iteration 754 : loss : 0.120614, loss_ce: 0.041758
2022-01-08 09:50:04,628 iteration 755 : loss : 0.088556, loss_ce: 0.043001
2022-01-08 09:50:06,116 iteration 756 : loss : 0.059656, loss_ce: 0.022521
2022-01-08 09:50:07,678 iteration 757 : loss : 0.108146, loss_ce: 0.048086
2022-01-08 09:50:09,346 iteration 758 : loss : 0.069624, loss_ce: 0.032856
2022-01-08 09:50:11,069 iteration 759 : loss : 0.086491, loss_ce: 0.034121
2022-01-08 09:50:12,634 iteration 760 : loss : 0.080714, loss_ce: 0.040855
2022-01-08 09:50:14,247 iteration 761 : loss : 0.087371, loss_ce: 0.035883
2022-01-08 09:50:15,879 iteration 762 : loss : 0.083726, loss_ce: 0.032829
2022-01-08 09:50:17,514 iteration 763 : loss : 0.093332, loss_ce: 0.039826
2022-01-08 09:50:19,065 iteration 764 : loss : 0.075316, loss_ce: 0.033014
2022-01-08 09:50:19,065 Training Data Eval:
2022-01-08 09:50:26,969   Average segmentation loss on training set: 0.0833
2022-01-08 09:50:26,969 Validation Data Eval:
2022-01-08 09:50:29,698   Average segmentation loss on validation set: 0.1190
2022-01-08 09:50:35,501 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:50:37,134 iteration 765 : loss : 0.122009, loss_ce: 0.041290
 11%|███▍                          | 45/400 [22:27<3:14:18, 32.84s/it]2022-01-08 09:50:38,711 iteration 766 : loss : 0.088178, loss_ce: 0.034396
2022-01-08 09:50:40,327 iteration 767 : loss : 0.072698, loss_ce: 0.025999
2022-01-08 09:50:41,933 iteration 768 : loss : 0.107254, loss_ce: 0.040462
2022-01-08 09:50:43,560 iteration 769 : loss : 0.098864, loss_ce: 0.045549
2022-01-08 09:50:45,060 iteration 770 : loss : 0.067756, loss_ce: 0.028164
2022-01-08 09:50:46,648 iteration 771 : loss : 0.113856, loss_ce: 0.031202
2022-01-08 09:50:48,245 iteration 772 : loss : 0.064516, loss_ce: 0.025415
2022-01-08 09:50:49,862 iteration 773 : loss : 0.085773, loss_ce: 0.023842
2022-01-08 09:50:51,562 iteration 774 : loss : 0.093447, loss_ce: 0.041385
2022-01-08 09:50:53,126 iteration 775 : loss : 0.095155, loss_ce: 0.049374
2022-01-08 09:50:54,659 iteration 776 : loss : 0.083035, loss_ce: 0.031014
2022-01-08 09:50:56,434 iteration 777 : loss : 0.099310, loss_ce: 0.042105
2022-01-08 09:50:57,938 iteration 778 : loss : 0.114229, loss_ce: 0.036987
2022-01-08 09:50:59,583 iteration 779 : loss : 0.088401, loss_ce: 0.035198
2022-01-08 09:51:01,230 iteration 780 : loss : 0.085135, loss_ce: 0.031041
2022-01-08 09:51:02,776 iteration 781 : loss : 0.072871, loss_ce: 0.038910
2022-01-08 09:51:04,270 iteration 782 : loss : 0.064019, loss_ce: 0.032099
 12%|███▍                          | 46/400 [22:54<3:03:39, 31.13s/it]2022-01-08 09:51:05,879 iteration 783 : loss : 0.086697, loss_ce: 0.043609
2022-01-08 09:51:07,419 iteration 784 : loss : 0.088533, loss_ce: 0.030623
2022-01-08 09:51:09,007 iteration 785 : loss : 0.084489, loss_ce: 0.036393
2022-01-08 09:51:10,562 iteration 786 : loss : 0.096123, loss_ce: 0.036712
2022-01-08 09:51:12,131 iteration 787 : loss : 0.092754, loss_ce: 0.033450
2022-01-08 09:51:13,775 iteration 788 : loss : 0.100435, loss_ce: 0.038356
2022-01-08 09:51:15,265 iteration 789 : loss : 0.088413, loss_ce: 0.032924
2022-01-08 09:51:16,896 iteration 790 : loss : 0.080550, loss_ce: 0.027719
2022-01-08 09:51:18,472 iteration 791 : loss : 0.057936, loss_ce: 0.022856
2022-01-08 09:51:19,987 iteration 792 : loss : 0.070701, loss_ce: 0.029633
2022-01-08 09:51:21,634 iteration 793 : loss : 0.080272, loss_ce: 0.040295
2022-01-08 09:51:23,117 iteration 794 : loss : 0.062757, loss_ce: 0.026497
2022-01-08 09:51:24,627 iteration 795 : loss : 0.080900, loss_ce: 0.030352
2022-01-08 09:51:26,139 iteration 796 : loss : 0.080590, loss_ce: 0.037360
2022-01-08 09:51:27,760 iteration 797 : loss : 0.138626, loss_ce: 0.052160
2022-01-08 09:51:29,316 iteration 798 : loss : 0.075664, loss_ce: 0.029199
2022-01-08 09:51:30,816 iteration 799 : loss : 0.063852, loss_ce: 0.030810
 12%|███▌                          | 47/400 [23:21<2:55:03, 29.75s/it]2022-01-08 09:51:32,546 iteration 800 : loss : 0.067768, loss_ce: 0.024994
2022-01-08 09:51:34,150 iteration 801 : loss : 0.073949, loss_ce: 0.028915
2022-01-08 09:51:35,732 iteration 802 : loss : 0.097916, loss_ce: 0.035197
2022-01-08 09:51:37,332 iteration 803 : loss : 0.074974, loss_ce: 0.038924
2022-01-08 09:51:39,006 iteration 804 : loss : 0.129596, loss_ce: 0.064555
2022-01-08 09:51:40,753 iteration 805 : loss : 0.081225, loss_ce: 0.030998
2022-01-08 09:51:42,312 iteration 806 : loss : 0.069504, loss_ce: 0.028467
2022-01-08 09:51:43,833 iteration 807 : loss : 0.056353, loss_ce: 0.021803
2022-01-08 09:51:45,455 iteration 808 : loss : 0.088530, loss_ce: 0.041432
2022-01-08 09:51:47,045 iteration 809 : loss : 0.070870, loss_ce: 0.023885
2022-01-08 09:51:48,588 iteration 810 : loss : 0.106958, loss_ce: 0.040941
2022-01-08 09:51:50,097 iteration 811 : loss : 0.108583, loss_ce: 0.058225
2022-01-08 09:51:51,640 iteration 812 : loss : 0.100261, loss_ce: 0.048799
2022-01-08 09:51:53,214 iteration 813 : loss : 0.100637, loss_ce: 0.036078
2022-01-08 09:51:54,913 iteration 814 : loss : 0.076771, loss_ce: 0.030555
2022-01-08 09:51:56,567 iteration 815 : loss : 0.106934, loss_ce: 0.042262
2022-01-08 09:51:58,167 iteration 816 : loss : 0.080456, loss_ce: 0.034545
 12%|███▌                          | 48/400 [23:48<2:50:19, 29.03s/it]2022-01-08 09:51:59,782 iteration 817 : loss : 0.079096, loss_ce: 0.033202
2022-01-08 09:52:01,302 iteration 818 : loss : 0.063760, loss_ce: 0.024818
2022-01-08 09:52:02,883 iteration 819 : loss : 0.052644, loss_ce: 0.020694
2022-01-08 09:52:04,388 iteration 820 : loss : 0.073912, loss_ce: 0.036046
2022-01-08 09:52:06,033 iteration 821 : loss : 0.075843, loss_ce: 0.027922
2022-01-08 09:52:07,662 iteration 822 : loss : 0.094971, loss_ce: 0.034925
2022-01-08 09:52:09,188 iteration 823 : loss : 0.057813, loss_ce: 0.023035
2022-01-08 09:52:10,705 iteration 824 : loss : 0.077379, loss_ce: 0.035633
2022-01-08 09:52:12,271 iteration 825 : loss : 0.064649, loss_ce: 0.024658
2022-01-08 09:52:13,826 iteration 826 : loss : 0.110300, loss_ce: 0.030893
2022-01-08 09:52:15,391 iteration 827 : loss : 0.107907, loss_ce: 0.032140
2022-01-08 09:52:16,958 iteration 828 : loss : 0.065060, loss_ce: 0.026214
2022-01-08 09:52:18,555 iteration 829 : loss : 0.098336, loss_ce: 0.040623
2022-01-08 09:52:20,194 iteration 830 : loss : 0.080384, loss_ce: 0.029112
2022-01-08 09:52:21,767 iteration 831 : loss : 0.084958, loss_ce: 0.035225
2022-01-08 09:52:23,320 iteration 832 : loss : 0.056079, loss_ce: 0.018922
2022-01-08 09:52:24,955 iteration 833 : loss : 0.076887, loss_ce: 0.037553
 12%|███▋                          | 49/400 [24:15<2:45:54, 28.36s/it]2022-01-08 09:52:26,573 iteration 834 : loss : 0.059005, loss_ce: 0.024108
2022-01-08 09:52:28,050 iteration 835 : loss : 0.069600, loss_ce: 0.019605
2022-01-08 09:52:29,632 iteration 836 : loss : 0.061947, loss_ce: 0.027646
2022-01-08 09:52:31,150 iteration 837 : loss : 0.054809, loss_ce: 0.029335
2022-01-08 09:52:32,641 iteration 838 : loss : 0.063888, loss_ce: 0.023634
2022-01-08 09:52:34,236 iteration 839 : loss : 0.068572, loss_ce: 0.029881
2022-01-08 09:52:35,796 iteration 840 : loss : 0.076790, loss_ce: 0.026861
2022-01-08 09:52:37,317 iteration 841 : loss : 0.076140, loss_ce: 0.035189
2022-01-08 09:52:38,873 iteration 842 : loss : 0.068095, loss_ce: 0.029809
2022-01-08 09:52:40,544 iteration 843 : loss : 0.070188, loss_ce: 0.026223
2022-01-08 09:52:42,104 iteration 844 : loss : 0.067342, loss_ce: 0.022062
2022-01-08 09:52:43,711 iteration 845 : loss : 0.070414, loss_ce: 0.026696
2022-01-08 09:52:45,421 iteration 846 : loss : 0.062924, loss_ce: 0.031418
2022-01-08 09:52:47,017 iteration 847 : loss : 0.061166, loss_ce: 0.026437
2022-01-08 09:52:48,492 iteration 848 : loss : 0.057175, loss_ce: 0.021764
2022-01-08 09:52:50,084 iteration 849 : loss : 0.068408, loss_ce: 0.024583
2022-01-08 09:52:50,084 Training Data Eval:
2022-01-08 09:52:57,986   Average segmentation loss on training set: 0.0647
2022-01-08 09:52:57,986 Validation Data Eval:
2022-01-08 09:53:00,711   Average segmentation loss on validation set: 0.1077
2022-01-08 09:53:07,474 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:53:08,968 iteration 850 : loss : 0.097977, loss_ce: 0.038953
 12%|███▊                          | 50/400 [24:59<3:12:49, 33.06s/it]2022-01-08 09:53:10,478 iteration 851 : loss : 0.050109, loss_ce: 0.020239
2022-01-08 09:53:11,990 iteration 852 : loss : 0.054340, loss_ce: 0.021443
2022-01-08 09:53:13,645 iteration 853 : loss : 0.065018, loss_ce: 0.024727
2022-01-08 09:53:15,256 iteration 854 : loss : 0.046458, loss_ce: 0.012833
2022-01-08 09:53:16,858 iteration 855 : loss : 0.076193, loss_ce: 0.030864
2022-01-08 09:53:18,480 iteration 856 : loss : 0.057887, loss_ce: 0.019035
2022-01-08 09:53:20,024 iteration 857 : loss : 0.067389, loss_ce: 0.029902
2022-01-08 09:53:21,498 iteration 858 : loss : 0.060885, loss_ce: 0.028420
2022-01-08 09:53:23,056 iteration 859 : loss : 0.075769, loss_ce: 0.024788
2022-01-08 09:53:24,569 iteration 860 : loss : 0.072185, loss_ce: 0.021575
2022-01-08 09:53:26,213 iteration 861 : loss : 0.063652, loss_ce: 0.026831
2022-01-08 09:53:27,709 iteration 862 : loss : 0.077940, loss_ce: 0.026235
2022-01-08 09:53:29,368 iteration 863 : loss : 0.066762, loss_ce: 0.025666
2022-01-08 09:53:30,983 iteration 864 : loss : 0.070050, loss_ce: 0.033744
2022-01-08 09:53:32,473 iteration 865 : loss : 0.054919, loss_ce: 0.022761
2022-01-08 09:53:34,114 iteration 866 : loss : 0.075886, loss_ce: 0.036273
2022-01-08 09:53:35,619 iteration 867 : loss : 0.041195, loss_ce: 0.017397
 13%|███▊                          | 51/400 [25:25<3:01:05, 31.13s/it]2022-01-08 09:53:37,277 iteration 868 : loss : 0.114024, loss_ce: 0.033281
2022-01-08 09:53:38,853 iteration 869 : loss : 0.052682, loss_ce: 0.021266
2022-01-08 09:53:40,329 iteration 870 : loss : 0.048445, loss_ce: 0.018896
2022-01-08 09:53:41,845 iteration 871 : loss : 0.061569, loss_ce: 0.024135
2022-01-08 09:53:43,382 iteration 872 : loss : 0.055522, loss_ce: 0.026286
2022-01-08 09:53:45,031 iteration 873 : loss : 0.067674, loss_ce: 0.022841
2022-01-08 09:53:46,516 iteration 874 : loss : 0.044084, loss_ce: 0.018858
2022-01-08 09:53:48,246 iteration 875 : loss : 0.091807, loss_ce: 0.029723
2022-01-08 09:53:49,823 iteration 876 : loss : 0.037152, loss_ce: 0.016278
2022-01-08 09:53:51,287 iteration 877 : loss : 0.043484, loss_ce: 0.017273
2022-01-08 09:53:52,906 iteration 878 : loss : 0.062770, loss_ce: 0.026169
2022-01-08 09:53:54,450 iteration 879 : loss : 0.064959, loss_ce: 0.026698
2022-01-08 09:53:56,050 iteration 880 : loss : 0.048571, loss_ce: 0.017214
2022-01-08 09:53:57,588 iteration 881 : loss : 0.109595, loss_ce: 0.053801
2022-01-08 09:53:59,208 iteration 882 : loss : 0.071580, loss_ce: 0.026658
2022-01-08 09:54:00,778 iteration 883 : loss : 0.063639, loss_ce: 0.029450
2022-01-08 09:54:02,459 iteration 884 : loss : 0.073595, loss_ce: 0.037980
 13%|███▉                          | 52/400 [25:52<2:53:06, 29.85s/it]2022-01-08 09:54:04,116 iteration 885 : loss : 0.077262, loss_ce: 0.031401
2022-01-08 09:54:05,732 iteration 886 : loss : 0.053325, loss_ce: 0.019491
2022-01-08 09:54:07,355 iteration 887 : loss : 0.091834, loss_ce: 0.039071
2022-01-08 09:54:08,941 iteration 888 : loss : 0.064103, loss_ce: 0.022819
2022-01-08 09:54:10,593 iteration 889 : loss : 0.064451, loss_ce: 0.023347
2022-01-08 09:54:12,297 iteration 890 : loss : 0.065868, loss_ce: 0.024475
2022-01-08 09:54:13,892 iteration 891 : loss : 0.045974, loss_ce: 0.016080
2022-01-08 09:54:15,476 iteration 892 : loss : 0.058866, loss_ce: 0.028714
2022-01-08 09:54:17,039 iteration 893 : loss : 0.076768, loss_ce: 0.030689
2022-01-08 09:54:18,735 iteration 894 : loss : 0.067720, loss_ce: 0.023546
2022-01-08 09:54:20,284 iteration 895 : loss : 0.080086, loss_ce: 0.025411
2022-01-08 09:54:21,942 iteration 896 : loss : 0.086112, loss_ce: 0.030639
2022-01-08 09:54:23,630 iteration 897 : loss : 0.090193, loss_ce: 0.035794
2022-01-08 09:54:25,191 iteration 898 : loss : 0.073837, loss_ce: 0.037646
2022-01-08 09:54:26,755 iteration 899 : loss : 0.068262, loss_ce: 0.030827
2022-01-08 09:54:28,276 iteration 900 : loss : 0.055698, loss_ce: 0.024601
2022-01-08 09:54:29,821 iteration 901 : loss : 0.080864, loss_ce: 0.028522
 13%|███▉                          | 53/400 [26:20<2:48:17, 29.10s/it]2022-01-08 09:54:31,475 iteration 902 : loss : 0.061115, loss_ce: 0.025450
2022-01-08 09:54:33,158 iteration 903 : loss : 0.067774, loss_ce: 0.024011
2022-01-08 09:54:34,781 iteration 904 : loss : 0.072592, loss_ce: 0.025515
2022-01-08 09:54:36,384 iteration 905 : loss : 0.083628, loss_ce: 0.037282
2022-01-08 09:54:37,965 iteration 906 : loss : 0.066814, loss_ce: 0.032866
2022-01-08 09:54:39,612 iteration 907 : loss : 0.063736, loss_ce: 0.022759
2022-01-08 09:54:41,090 iteration 908 : loss : 0.048984, loss_ce: 0.018197
2022-01-08 09:54:42,697 iteration 909 : loss : 0.061454, loss_ce: 0.028696
2022-01-08 09:54:44,201 iteration 910 : loss : 0.078195, loss_ce: 0.032227
2022-01-08 09:54:45,772 iteration 911 : loss : 0.056573, loss_ce: 0.023029
2022-01-08 09:54:47,335 iteration 912 : loss : 0.053526, loss_ce: 0.019522
2022-01-08 09:54:48,886 iteration 913 : loss : 0.082537, loss_ce: 0.027245
2022-01-08 09:54:50,449 iteration 914 : loss : 0.087947, loss_ce: 0.042492
2022-01-08 09:54:51,982 iteration 915 : loss : 0.132920, loss_ce: 0.034569
2022-01-08 09:54:53,520 iteration 916 : loss : 0.054220, loss_ce: 0.019008
2022-01-08 09:54:55,120 iteration 917 : loss : 0.077854, loss_ce: 0.028452
2022-01-08 09:54:56,743 iteration 918 : loss : 0.079467, loss_ce: 0.030929
 14%|████                          | 54/400 [26:47<2:44:01, 28.44s/it]2022-01-08 09:54:58,411 iteration 919 : loss : 0.056596, loss_ce: 0.025259
2022-01-08 09:55:00,013 iteration 920 : loss : 0.065013, loss_ce: 0.027389
2022-01-08 09:55:01,647 iteration 921 : loss : 0.067065, loss_ce: 0.025825
2022-01-08 09:55:03,299 iteration 922 : loss : 0.091172, loss_ce: 0.036408
2022-01-08 09:55:04,854 iteration 923 : loss : 0.047799, loss_ce: 0.021368
2022-01-08 09:55:06,490 iteration 924 : loss : 0.098274, loss_ce: 0.029093
2022-01-08 09:55:08,028 iteration 925 : loss : 0.046457, loss_ce: 0.016693
2022-01-08 09:55:09,666 iteration 926 : loss : 0.062161, loss_ce: 0.027061
2022-01-08 09:55:11,172 iteration 927 : loss : 0.079326, loss_ce: 0.026831
2022-01-08 09:55:12,744 iteration 928 : loss : 0.059700, loss_ce: 0.025328
2022-01-08 09:55:14,386 iteration 929 : loss : 0.077264, loss_ce: 0.030034
2022-01-08 09:55:15,989 iteration 930 : loss : 0.064820, loss_ce: 0.024308
2022-01-08 09:55:17,594 iteration 931 : loss : 0.088323, loss_ce: 0.019483
2022-01-08 09:55:19,181 iteration 932 : loss : 0.065787, loss_ce: 0.021302
2022-01-08 09:55:20,729 iteration 933 : loss : 0.089562, loss_ce: 0.033591
2022-01-08 09:55:22,308 iteration 934 : loss : 0.055688, loss_ce: 0.022731
2022-01-08 09:55:22,309 Training Data Eval:
2022-01-08 09:55:30,210   Average segmentation loss on training set: 0.1521
2022-01-08 09:55:30,211 Validation Data Eval:
2022-01-08 09:55:32,938   Average segmentation loss on validation set: 0.2610
2022-01-08 09:55:34,621 iteration 935 : loss : 0.117969, loss_ce: 0.034761
 14%|████▏                         | 55/400 [27:24<2:59:49, 31.27s/it]2022-01-08 09:55:36,221 iteration 936 : loss : 0.047351, loss_ce: 0.016347
2022-01-08 09:55:37,824 iteration 937 : loss : 0.065347, loss_ce: 0.020787
2022-01-08 09:55:39,459 iteration 938 : loss : 0.090524, loss_ce: 0.033910
2022-01-08 09:55:40,992 iteration 939 : loss : 0.062216, loss_ce: 0.021995
2022-01-08 09:55:42,594 iteration 940 : loss : 0.075672, loss_ce: 0.032421
2022-01-08 09:55:44,290 iteration 941 : loss : 0.060704, loss_ce: 0.021790
2022-01-08 09:55:45,829 iteration 942 : loss : 0.055947, loss_ce: 0.019727
2022-01-08 09:55:47,390 iteration 943 : loss : 0.060446, loss_ce: 0.026679
2022-01-08 09:55:48,924 iteration 944 : loss : 0.060354, loss_ce: 0.023232
2022-01-08 09:55:50,470 iteration 945 : loss : 0.063854, loss_ce: 0.024138
2022-01-08 09:55:52,015 iteration 946 : loss : 0.048759, loss_ce: 0.022377
2022-01-08 09:55:53,547 iteration 947 : loss : 0.069514, loss_ce: 0.022013
2022-01-08 09:55:55,074 iteration 948 : loss : 0.087816, loss_ce: 0.028864
2022-01-08 09:55:56,636 iteration 949 : loss : 0.058624, loss_ce: 0.021214
2022-01-08 09:55:58,191 iteration 950 : loss : 0.078437, loss_ce: 0.033279
2022-01-08 09:55:59,806 iteration 951 : loss : 0.059769, loss_ce: 0.030821
2022-01-08 09:56:01,328 iteration 952 : loss : 0.062694, loss_ce: 0.025701
 14%|████▏                         | 56/400 [27:51<2:51:27, 29.91s/it]2022-01-08 09:56:02,993 iteration 953 : loss : 0.073526, loss_ce: 0.028908
2022-01-08 09:56:04,621 iteration 954 : loss : 0.056721, loss_ce: 0.021628
2022-01-08 09:56:06,117 iteration 955 : loss : 0.038780, loss_ce: 0.016177
2022-01-08 09:56:07,732 iteration 956 : loss : 0.054869, loss_ce: 0.021016
2022-01-08 09:56:09,326 iteration 957 : loss : 0.061151, loss_ce: 0.022158
2022-01-08 09:56:10,938 iteration 958 : loss : 0.060815, loss_ce: 0.018615
2022-01-08 09:56:12,628 iteration 959 : loss : 0.073718, loss_ce: 0.035625
2022-01-08 09:56:14,385 iteration 960 : loss : 0.068732, loss_ce: 0.029923
2022-01-08 09:56:15,933 iteration 961 : loss : 0.077560, loss_ce: 0.023800
2022-01-08 09:56:17,442 iteration 962 : loss : 0.061699, loss_ce: 0.018524
2022-01-08 09:56:19,107 iteration 963 : loss : 0.077591, loss_ce: 0.034192
2022-01-08 09:56:20,729 iteration 964 : loss : 0.065595, loss_ce: 0.034842
2022-01-08 09:56:22,287 iteration 965 : loss : 0.050798, loss_ce: 0.020583
2022-01-08 09:56:23,870 iteration 966 : loss : 0.057396, loss_ce: 0.022011
2022-01-08 09:56:25,471 iteration 967 : loss : 0.057894, loss_ce: 0.022797
2022-01-08 09:56:27,003 iteration 968 : loss : 0.113502, loss_ce: 0.037403
2022-01-08 09:56:28,610 iteration 969 : loss : 0.052428, loss_ce: 0.020824
 14%|████▎                         | 57/400 [28:18<2:46:27, 29.12s/it]2022-01-08 09:56:30,213 iteration 970 : loss : 0.063303, loss_ce: 0.030941
2022-01-08 09:56:31,830 iteration 971 : loss : 0.061533, loss_ce: 0.017135
2022-01-08 09:56:33,345 iteration 972 : loss : 0.057103, loss_ce: 0.024904
2022-01-08 09:56:34,949 iteration 973 : loss : 0.051336, loss_ce: 0.015038
2022-01-08 09:56:36,468 iteration 974 : loss : 0.058954, loss_ce: 0.024738
2022-01-08 09:56:38,101 iteration 975 : loss : 0.055686, loss_ce: 0.018452
2022-01-08 09:56:39,658 iteration 976 : loss : 0.051666, loss_ce: 0.022818
2022-01-08 09:56:41,226 iteration 977 : loss : 0.053387, loss_ce: 0.017846
2022-01-08 09:56:42,832 iteration 978 : loss : 0.040542, loss_ce: 0.014373
2022-01-08 09:56:44,387 iteration 979 : loss : 0.085586, loss_ce: 0.042878
2022-01-08 09:56:45,941 iteration 980 : loss : 0.056364, loss_ce: 0.017850
2022-01-08 09:56:47,625 iteration 981 : loss : 0.054569, loss_ce: 0.024027
2022-01-08 09:56:49,180 iteration 982 : loss : 0.053837, loss_ce: 0.024518
2022-01-08 09:56:50,819 iteration 983 : loss : 0.104459, loss_ce: 0.036118
2022-01-08 09:56:52,318 iteration 984 : loss : 0.051075, loss_ce: 0.022566
2022-01-08 09:56:53,897 iteration 985 : loss : 0.056322, loss_ce: 0.018147
2022-01-08 09:56:55,538 iteration 986 : loss : 0.060483, loss_ce: 0.023233
 14%|████▎                         | 58/400 [28:45<2:42:14, 28.46s/it]2022-01-08 09:56:57,239 iteration 987 : loss : 0.051380, loss_ce: 0.023998
2022-01-08 09:56:58,855 iteration 988 : loss : 0.053928, loss_ce: 0.023181
2022-01-08 09:57:00,452 iteration 989 : loss : 0.072919, loss_ce: 0.026116
2022-01-08 09:57:02,035 iteration 990 : loss : 0.051938, loss_ce: 0.022509
2022-01-08 09:57:03,607 iteration 991 : loss : 0.048407, loss_ce: 0.021901
2022-01-08 09:57:05,276 iteration 992 : loss : 0.055718, loss_ce: 0.026343
2022-01-08 09:57:06,893 iteration 993 : loss : 0.043251, loss_ce: 0.016385
2022-01-08 09:57:08,601 iteration 994 : loss : 0.058864, loss_ce: 0.022593
2022-01-08 09:57:10,203 iteration 995 : loss : 0.102641, loss_ce: 0.035625
2022-01-08 09:57:11,861 iteration 996 : loss : 0.049035, loss_ce: 0.021992
2022-01-08 09:57:13,513 iteration 997 : loss : 0.083597, loss_ce: 0.030089
2022-01-08 09:57:15,047 iteration 998 : loss : 0.098026, loss_ce: 0.034296
2022-01-08 09:57:16,688 iteration 999 : loss : 0.061651, loss_ce: 0.025205
2022-01-08 09:57:18,235 iteration 1000 : loss : 0.070218, loss_ce: 0.022781
2022-01-08 09:57:19,786 iteration 1001 : loss : 0.104465, loss_ce: 0.027715
2022-01-08 09:57:21,474 iteration 1002 : loss : 0.059878, loss_ce: 0.024778
2022-01-08 09:57:23,151 iteration 1003 : loss : 0.062157, loss_ce: 0.024129
 15%|████▍                         | 59/400 [29:13<2:40:18, 28.21s/it]2022-01-08 09:57:24,785 iteration 1004 : loss : 0.110088, loss_ce: 0.045386
2022-01-08 09:57:26,357 iteration 1005 : loss : 0.069210, loss_ce: 0.024770
2022-01-08 09:57:28,044 iteration 1006 : loss : 0.083479, loss_ce: 0.032288
2022-01-08 09:57:29,583 iteration 1007 : loss : 0.056461, loss_ce: 0.028886
2022-01-08 09:57:31,112 iteration 1008 : loss : 0.130486, loss_ce: 0.053911
2022-01-08 09:57:32,652 iteration 1009 : loss : 0.059560, loss_ce: 0.023476
2022-01-08 09:57:34,190 iteration 1010 : loss : 0.065765, loss_ce: 0.025907
2022-01-08 09:57:35,830 iteration 1011 : loss : 0.058186, loss_ce: 0.025098
2022-01-08 09:57:37,396 iteration 1012 : loss : 0.041788, loss_ce: 0.015774
2022-01-08 09:57:38,916 iteration 1013 : loss : 0.048292, loss_ce: 0.015856
2022-01-08 09:57:40,512 iteration 1014 : loss : 0.096114, loss_ce: 0.036560
2022-01-08 09:57:42,019 iteration 1015 : loss : 0.060473, loss_ce: 0.025890
2022-01-08 09:57:43,612 iteration 1016 : loss : 0.055907, loss_ce: 0.023556
2022-01-08 09:57:45,090 iteration 1017 : loss : 0.046889, loss_ce: 0.016798
2022-01-08 09:57:46,565 iteration 1018 : loss : 0.065080, loss_ce: 0.028396
2022-01-08 09:57:48,129 iteration 1019 : loss : 0.066307, loss_ce: 0.027279
2022-01-08 09:57:48,129 Training Data Eval:
2022-01-08 09:57:56,039   Average segmentation loss on training set: 0.0479
2022-01-08 09:57:56,040 Validation Data Eval:
2022-01-08 09:57:58,769   Average segmentation loss on validation set: 0.1069
2022-01-08 09:58:04,600 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 09:58:05,998 iteration 1020 : loss : 0.046167, loss_ce: 0.015231
 15%|████▌                         | 60/400 [29:56<3:04:44, 32.60s/it]2022-01-08 09:58:07,581 iteration 1021 : loss : 0.074345, loss_ce: 0.016626
2022-01-08 09:58:09,137 iteration 1022 : loss : 0.059421, loss_ce: 0.024108
2022-01-08 09:58:10,722 iteration 1023 : loss : 0.114986, loss_ce: 0.030156
2022-01-08 09:58:12,365 iteration 1024 : loss : 0.044923, loss_ce: 0.017131
2022-01-08 09:58:13,997 iteration 1025 : loss : 0.042252, loss_ce: 0.019040
2022-01-08 09:58:15,480 iteration 1026 : loss : 0.048570, loss_ce: 0.016643
2022-01-08 09:58:17,110 iteration 1027 : loss : 0.055283, loss_ce: 0.020324
2022-01-08 09:58:18,728 iteration 1028 : loss : 0.069654, loss_ce: 0.026556
2022-01-08 09:58:20,294 iteration 1029 : loss : 0.043278, loss_ce: 0.019480
2022-01-08 09:58:21,873 iteration 1030 : loss : 0.067785, loss_ce: 0.025100
2022-01-08 09:58:23,513 iteration 1031 : loss : 0.099940, loss_ce: 0.045268
2022-01-08 09:58:25,122 iteration 1032 : loss : 0.064770, loss_ce: 0.028716
2022-01-08 09:58:26,672 iteration 1033 : loss : 0.071665, loss_ce: 0.027592
2022-01-08 09:58:28,327 iteration 1034 : loss : 0.093194, loss_ce: 0.031180
2022-01-08 09:58:29,930 iteration 1035 : loss : 0.054124, loss_ce: 0.023281
2022-01-08 09:58:31,534 iteration 1036 : loss : 0.053406, loss_ce: 0.022482
2022-01-08 09:58:33,082 iteration 1037 : loss : 0.062468, loss_ce: 0.023089
 15%|████▌                         | 61/400 [30:23<2:54:49, 30.94s/it]2022-01-08 09:58:34,726 iteration 1038 : loss : 0.051019, loss_ce: 0.021002
2022-01-08 09:58:36,305 iteration 1039 : loss : 0.042134, loss_ce: 0.020456
2022-01-08 09:58:37,930 iteration 1040 : loss : 0.097643, loss_ce: 0.047948
2022-01-08 09:58:39,538 iteration 1041 : loss : 0.056641, loss_ce: 0.022591
2022-01-08 09:58:41,153 iteration 1042 : loss : 0.059742, loss_ce: 0.024017
2022-01-08 09:58:42,748 iteration 1043 : loss : 0.071982, loss_ce: 0.023847
2022-01-08 09:58:44,378 iteration 1044 : loss : 0.044115, loss_ce: 0.015765
2022-01-08 09:58:45,871 iteration 1045 : loss : 0.044735, loss_ce: 0.018675
2022-01-08 09:58:47,397 iteration 1046 : loss : 0.073507, loss_ce: 0.019572
2022-01-08 09:58:49,030 iteration 1047 : loss : 0.064005, loss_ce: 0.021187
2022-01-08 09:58:50,618 iteration 1048 : loss : 0.060064, loss_ce: 0.020018
2022-01-08 09:58:52,206 iteration 1049 : loss : 0.095932, loss_ce: 0.037022
2022-01-08 09:58:53,852 iteration 1050 : loss : 0.086124, loss_ce: 0.026847
2022-01-08 09:58:55,431 iteration 1051 : loss : 0.062410, loss_ce: 0.023160
2022-01-08 09:58:57,018 iteration 1052 : loss : 0.116936, loss_ce: 0.043935
2022-01-08 09:58:58,518 iteration 1053 : loss : 0.062315, loss_ce: 0.017581
2022-01-08 09:59:00,183 iteration 1054 : loss : 0.062041, loss_ce: 0.031269
 16%|████▋                         | 62/400 [30:50<2:47:49, 29.79s/it]2022-01-08 09:59:01,813 iteration 1055 : loss : 0.059329, loss_ce: 0.018494
2022-01-08 09:59:03,367 iteration 1056 : loss : 0.052757, loss_ce: 0.026503
2022-01-08 09:59:04,924 iteration 1057 : loss : 0.076908, loss_ce: 0.038067
2022-01-08 09:59:06,498 iteration 1058 : loss : 0.063286, loss_ce: 0.030367
2022-01-08 09:59:07,985 iteration 1059 : loss : 0.071648, loss_ce: 0.026087
2022-01-08 09:59:09,553 iteration 1060 : loss : 0.048584, loss_ce: 0.018922
2022-01-08 09:59:11,163 iteration 1061 : loss : 0.054363, loss_ce: 0.024058
2022-01-08 09:59:12,767 iteration 1062 : loss : 0.072707, loss_ce: 0.028070
2022-01-08 09:59:14,427 iteration 1063 : loss : 0.115316, loss_ce: 0.042840
2022-01-08 09:59:16,006 iteration 1064 : loss : 0.084597, loss_ce: 0.027699
2022-01-08 09:59:17,519 iteration 1065 : loss : 0.050212, loss_ce: 0.020051
2022-01-08 09:59:19,141 iteration 1066 : loss : 0.055128, loss_ce: 0.021499
2022-01-08 09:59:20,703 iteration 1067 : loss : 0.135423, loss_ce: 0.025345
2022-01-08 09:59:22,339 iteration 1068 : loss : 0.059717, loss_ce: 0.026850
2022-01-08 09:59:23,951 iteration 1069 : loss : 0.084217, loss_ce: 0.036340
2022-01-08 09:59:25,529 iteration 1070 : loss : 0.051621, loss_ce: 0.019873
2022-01-08 09:59:27,101 iteration 1071 : loss : 0.059119, loss_ce: 0.026046
 16%|████▋                         | 63/400 [31:17<2:42:28, 28.93s/it]2022-01-08 09:59:28,837 iteration 1072 : loss : 0.059524, loss_ce: 0.018771
2022-01-08 09:59:30,399 iteration 1073 : loss : 0.103124, loss_ce: 0.028065
2022-01-08 09:59:31,965 iteration 1074 : loss : 0.064032, loss_ce: 0.025305
2022-01-08 09:59:33,582 iteration 1075 : loss : 0.073649, loss_ce: 0.022743
2022-01-08 09:59:35,103 iteration 1076 : loss : 0.059735, loss_ce: 0.021811
2022-01-08 09:59:36,669 iteration 1077 : loss : 0.049034, loss_ce: 0.017818
2022-01-08 09:59:38,321 iteration 1078 : loss : 0.083979, loss_ce: 0.047764
2022-01-08 09:59:39,860 iteration 1079 : loss : 0.052413, loss_ce: 0.022893
2022-01-08 09:59:41,387 iteration 1080 : loss : 0.044314, loss_ce: 0.018890
2022-01-08 09:59:42,997 iteration 1081 : loss : 0.071213, loss_ce: 0.022662
2022-01-08 09:59:44,604 iteration 1082 : loss : 0.064620, loss_ce: 0.034889
2022-01-08 09:59:46,245 iteration 1083 : loss : 0.076294, loss_ce: 0.037908
2022-01-08 09:59:47,805 iteration 1084 : loss : 0.065316, loss_ce: 0.034859
2022-01-08 09:59:49,490 iteration 1085 : loss : 0.075257, loss_ce: 0.032904
2022-01-08 09:59:51,051 iteration 1086 : loss : 0.062449, loss_ce: 0.023682
2022-01-08 09:59:52,634 iteration 1087 : loss : 0.047113, loss_ce: 0.019911
2022-01-08 09:59:54,282 iteration 1088 : loss : 0.052195, loss_ce: 0.021410
 16%|████▊                         | 64/400 [31:44<2:39:04, 28.41s/it]2022-01-08 09:59:55,877 iteration 1089 : loss : 0.061671, loss_ce: 0.025975
2022-01-08 09:59:57,435 iteration 1090 : loss : 0.059924, loss_ce: 0.029103
2022-01-08 09:59:59,019 iteration 1091 : loss : 0.053575, loss_ce: 0.026266
2022-01-08 10:00:00,601 iteration 1092 : loss : 0.045458, loss_ce: 0.017968
2022-01-08 10:00:02,202 iteration 1093 : loss : 0.081809, loss_ce: 0.031655
2022-01-08 10:00:03,862 iteration 1094 : loss : 0.046449, loss_ce: 0.018794
2022-01-08 10:00:05,393 iteration 1095 : loss : 0.048211, loss_ce: 0.020829
2022-01-08 10:00:07,085 iteration 1096 : loss : 0.061762, loss_ce: 0.025676
2022-01-08 10:00:08,692 iteration 1097 : loss : 0.068256, loss_ce: 0.027216
2022-01-08 10:00:10,235 iteration 1098 : loss : 0.043999, loss_ce: 0.019554
2022-01-08 10:00:11,846 iteration 1099 : loss : 0.058181, loss_ce: 0.017304
2022-01-08 10:00:13,520 iteration 1100 : loss : 0.051310, loss_ce: 0.019865
2022-01-08 10:00:15,106 iteration 1101 : loss : 0.049411, loss_ce: 0.019304
2022-01-08 10:00:16,776 iteration 1102 : loss : 0.087086, loss_ce: 0.025336
2022-01-08 10:00:18,344 iteration 1103 : loss : 0.063151, loss_ce: 0.021549
2022-01-08 10:00:19,905 iteration 1104 : loss : 0.070389, loss_ce: 0.030451
2022-01-08 10:00:19,905 Training Data Eval:
2022-01-08 10:00:27,810   Average segmentation loss on training set: 0.0777
2022-01-08 10:00:27,810 Validation Data Eval:
2022-01-08 10:00:30,545   Average segmentation loss on validation set: 0.1757
2022-01-08 10:00:32,190 iteration 1105 : loss : 0.068806, loss_ce: 0.021505
 16%|████▉                         | 65/400 [32:22<2:54:30, 31.26s/it]2022-01-08 10:00:33,744 iteration 1106 : loss : 0.057413, loss_ce: 0.019827
2022-01-08 10:00:35,304 iteration 1107 : loss : 0.053527, loss_ce: 0.027534
2022-01-08 10:00:36,893 iteration 1108 : loss : 0.097520, loss_ce: 0.029705
2022-01-08 10:00:38,409 iteration 1109 : loss : 0.043395, loss_ce: 0.017985
2022-01-08 10:00:39,999 iteration 1110 : loss : 0.055674, loss_ce: 0.025779
2022-01-08 10:00:41,551 iteration 1111 : loss : 0.044308, loss_ce: 0.017868
2022-01-08 10:00:43,113 iteration 1112 : loss : 0.062593, loss_ce: 0.023730
2022-01-08 10:00:44,655 iteration 1113 : loss : 0.064773, loss_ce: 0.020616
2022-01-08 10:00:46,298 iteration 1114 : loss : 0.063693, loss_ce: 0.028262
2022-01-08 10:00:48,018 iteration 1115 : loss : 0.047778, loss_ce: 0.023225
2022-01-08 10:00:49,655 iteration 1116 : loss : 0.068080, loss_ce: 0.020753
2022-01-08 10:00:51,373 iteration 1117 : loss : 0.042986, loss_ce: 0.015499
2022-01-08 10:00:53,016 iteration 1118 : loss : 0.074839, loss_ce: 0.027199
2022-01-08 10:00:54,573 iteration 1119 : loss : 0.043415, loss_ce: 0.017335
2022-01-08 10:00:56,125 iteration 1120 : loss : 0.065244, loss_ce: 0.020732
2022-01-08 10:00:57,731 iteration 1121 : loss : 0.073388, loss_ce: 0.021004
2022-01-08 10:00:59,333 iteration 1122 : loss : 0.060962, loss_ce: 0.028110
 16%|████▉                         | 66/400 [32:49<2:47:07, 30.02s/it]2022-01-08 10:01:00,910 iteration 1123 : loss : 0.112077, loss_ce: 0.050957
2022-01-08 10:01:02,533 iteration 1124 : loss : 0.049416, loss_ce: 0.022130
2022-01-08 10:01:04,114 iteration 1125 : loss : 0.040684, loss_ce: 0.011436
2022-01-08 10:01:05,680 iteration 1126 : loss : 0.059763, loss_ce: 0.022571
2022-01-08 10:01:07,233 iteration 1127 : loss : 0.084945, loss_ce: 0.028818
2022-01-08 10:01:08,874 iteration 1128 : loss : 0.084687, loss_ce: 0.041616
2022-01-08 10:01:10,526 iteration 1129 : loss : 0.055178, loss_ce: 0.017686
2022-01-08 10:01:12,089 iteration 1130 : loss : 0.051382, loss_ce: 0.024165
2022-01-08 10:01:13,652 iteration 1131 : loss : 0.052558, loss_ce: 0.022715
2022-01-08 10:01:15,275 iteration 1132 : loss : 0.057379, loss_ce: 0.023724
2022-01-08 10:01:16,788 iteration 1133 : loss : 0.046582, loss_ce: 0.021275
2022-01-08 10:01:18,367 iteration 1134 : loss : 0.069255, loss_ce: 0.021812
2022-01-08 10:01:19,906 iteration 1135 : loss : 0.060139, loss_ce: 0.023999
2022-01-08 10:01:21,475 iteration 1136 : loss : 0.033553, loss_ce: 0.012656
2022-01-08 10:01:23,064 iteration 1137 : loss : 0.086130, loss_ce: 0.034503
2022-01-08 10:01:24,689 iteration 1138 : loss : 0.050037, loss_ce: 0.021911
2022-01-08 10:01:26,286 iteration 1139 : loss : 0.076969, loss_ce: 0.029440
 17%|█████                         | 67/400 [33:16<2:41:30, 29.10s/it]2022-01-08 10:01:27,914 iteration 1140 : loss : 0.075322, loss_ce: 0.030552
2022-01-08 10:01:29,452 iteration 1141 : loss : 0.061440, loss_ce: 0.021601
2022-01-08 10:01:31,100 iteration 1142 : loss : 0.093633, loss_ce: 0.041932
2022-01-08 10:01:32,680 iteration 1143 : loss : 0.073305, loss_ce: 0.031743
2022-01-08 10:01:34,299 iteration 1144 : loss : 0.098416, loss_ce: 0.049854
2022-01-08 10:01:35,828 iteration 1145 : loss : 0.050317, loss_ce: 0.019031
2022-01-08 10:01:37,425 iteration 1146 : loss : 0.071632, loss_ce: 0.032272
2022-01-08 10:01:38,912 iteration 1147 : loss : 0.055368, loss_ce: 0.019278
2022-01-08 10:01:40,545 iteration 1148 : loss : 0.079791, loss_ce: 0.030982
2022-01-08 10:01:42,068 iteration 1149 : loss : 0.049258, loss_ce: 0.022259
2022-01-08 10:01:43,651 iteration 1150 : loss : 0.062254, loss_ce: 0.025513
2022-01-08 10:01:45,270 iteration 1151 : loss : 0.067607, loss_ce: 0.026781
2022-01-08 10:01:46,989 iteration 1152 : loss : 0.066373, loss_ce: 0.026702
2022-01-08 10:01:48,478 iteration 1153 : loss : 0.049454, loss_ce: 0.019500
2022-01-08 10:01:50,095 iteration 1154 : loss : 0.074571, loss_ce: 0.024927
2022-01-08 10:01:51,632 iteration 1155 : loss : 0.061884, loss_ce: 0.020755
2022-01-08 10:01:53,273 iteration 1156 : loss : 0.094734, loss_ce: 0.050137
 17%|█████                         | 68/400 [33:43<2:37:30, 28.47s/it]2022-01-08 10:01:54,878 iteration 1157 : loss : 0.114767, loss_ce: 0.029632
2022-01-08 10:01:56,478 iteration 1158 : loss : 0.058510, loss_ce: 0.021217
2022-01-08 10:01:58,143 iteration 1159 : loss : 0.040521, loss_ce: 0.014148
2022-01-08 10:01:59,860 iteration 1160 : loss : 0.075934, loss_ce: 0.031347
2022-01-08 10:02:01,519 iteration 1161 : loss : 0.056755, loss_ce: 0.024536
2022-01-08 10:02:03,141 iteration 1162 : loss : 0.047827, loss_ce: 0.015970
2022-01-08 10:02:04,712 iteration 1163 : loss : 0.068897, loss_ce: 0.030128
2022-01-08 10:02:06,287 iteration 1164 : loss : 0.059115, loss_ce: 0.023058
2022-01-08 10:02:07,868 iteration 1165 : loss : 0.049470, loss_ce: 0.021719
2022-01-08 10:02:09,533 iteration 1166 : loss : 0.039936, loss_ce: 0.014970
2022-01-08 10:02:11,079 iteration 1167 : loss : 0.052306, loss_ce: 0.023738
2022-01-08 10:02:12,644 iteration 1168 : loss : 0.051552, loss_ce: 0.019766
2022-01-08 10:02:14,186 iteration 1169 : loss : 0.055720, loss_ce: 0.023092
2022-01-08 10:02:15,733 iteration 1170 : loss : 0.059374, loss_ce: 0.023353
2022-01-08 10:02:17,329 iteration 1171 : loss : 0.056599, loss_ce: 0.019267
2022-01-08 10:02:18,921 iteration 1172 : loss : 0.066357, loss_ce: 0.033237
2022-01-08 10:02:20,538 iteration 1173 : loss : 0.052008, loss_ce: 0.018427
 17%|█████▏                        | 69/400 [34:10<2:35:02, 28.10s/it]2022-01-08 10:02:22,139 iteration 1174 : loss : 0.061401, loss_ce: 0.021601
2022-01-08 10:02:23,723 iteration 1175 : loss : 0.044875, loss_ce: 0.017978
2022-01-08 10:02:25,295 iteration 1176 : loss : 0.065147, loss_ce: 0.031738
2022-01-08 10:02:26,833 iteration 1177 : loss : 0.050562, loss_ce: 0.021294
2022-01-08 10:02:28,450 iteration 1178 : loss : 0.040103, loss_ce: 0.013502
2022-01-08 10:02:30,123 iteration 1179 : loss : 0.072075, loss_ce: 0.030565
2022-01-08 10:02:31,674 iteration 1180 : loss : 0.050779, loss_ce: 0.021514
2022-01-08 10:02:33,248 iteration 1181 : loss : 0.054730, loss_ce: 0.020706
2022-01-08 10:02:34,839 iteration 1182 : loss : 0.048794, loss_ce: 0.017939
2022-01-08 10:02:36,370 iteration 1183 : loss : 0.046106, loss_ce: 0.018887
2022-01-08 10:02:37,911 iteration 1184 : loss : 0.042733, loss_ce: 0.019586
2022-01-08 10:02:39,535 iteration 1185 : loss : 0.052973, loss_ce: 0.020465
2022-01-08 10:02:41,081 iteration 1186 : loss : 0.045825, loss_ce: 0.021295
2022-01-08 10:02:42,731 iteration 1187 : loss : 0.046204, loss_ce: 0.014986
2022-01-08 10:02:44,270 iteration 1188 : loss : 0.057792, loss_ce: 0.021588
2022-01-08 10:02:45,935 iteration 1189 : loss : 0.055764, loss_ce: 0.021820
2022-01-08 10:02:45,935 Training Data Eval:
2022-01-08 10:02:53,836   Average segmentation loss on training set: 0.0389
2022-01-08 10:02:53,836 Validation Data Eval:
2022-01-08 10:02:56,567   Average segmentation loss on validation set: 0.0654
2022-01-08 10:03:02,502 Found new lowest validation loss at iteration 1189! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 10:03:03,960 iteration 1190 : loss : 0.053045, loss_ce: 0.025845
 18%|█████▎                        | 70/400 [34:54<2:59:52, 32.70s/it]2022-01-08 10:03:05,529 iteration 1191 : loss : 0.051993, loss_ce: 0.020976
2022-01-08 10:03:07,043 iteration 1192 : loss : 0.039560, loss_ce: 0.018521
2022-01-08 10:03:08,710 iteration 1193 : loss : 0.078036, loss_ce: 0.022067
2022-01-08 10:03:10,273 iteration 1194 : loss : 0.064472, loss_ce: 0.020922
2022-01-08 10:03:11,791 iteration 1195 : loss : 0.039527, loss_ce: 0.017541
2022-01-08 10:03:13,311 iteration 1196 : loss : 0.047803, loss_ce: 0.020325
2022-01-08 10:03:14,792 iteration 1197 : loss : 0.045616, loss_ce: 0.016699
2022-01-08 10:03:16,409 iteration 1198 : loss : 0.051014, loss_ce: 0.021200
2022-01-08 10:03:17,987 iteration 1199 : loss : 0.041030, loss_ce: 0.016758
2022-01-08 10:03:19,534 iteration 1200 : loss : 0.060582, loss_ce: 0.019385
2022-01-08 10:03:21,047 iteration 1201 : loss : 0.041277, loss_ce: 0.018155
2022-01-08 10:03:22,646 iteration 1202 : loss : 0.043085, loss_ce: 0.016291
2022-01-08 10:03:24,236 iteration 1203 : loss : 0.059537, loss_ce: 0.016941
2022-01-08 10:03:25,813 iteration 1204 : loss : 0.051175, loss_ce: 0.016933
2022-01-08 10:03:27,402 iteration 1205 : loss : 0.076110, loss_ce: 0.030824
2022-01-08 10:03:28,994 iteration 1206 : loss : 0.036965, loss_ce: 0.014424
2022-01-08 10:03:30,630 iteration 1207 : loss : 0.054830, loss_ce: 0.018929
 18%|█████▎                        | 71/400 [35:20<2:49:23, 30.89s/it]2022-01-08 10:03:32,293 iteration 1208 : loss : 0.055222, loss_ce: 0.026753
2022-01-08 10:03:33,885 iteration 1209 : loss : 0.041216, loss_ce: 0.017286
2022-01-08 10:03:35,471 iteration 1210 : loss : 0.043195, loss_ce: 0.016408
2022-01-08 10:03:36,999 iteration 1211 : loss : 0.039834, loss_ce: 0.016513
2022-01-08 10:03:38,507 iteration 1212 : loss : 0.080618, loss_ce: 0.025094
2022-01-08 10:03:40,214 iteration 1213 : loss : 0.057208, loss_ce: 0.023002
2022-01-08 10:03:41,795 iteration 1214 : loss : 0.041134, loss_ce: 0.016005
2022-01-08 10:03:43,386 iteration 1215 : loss : 0.085135, loss_ce: 0.035180
2022-01-08 10:03:45,094 iteration 1216 : loss : 0.043003, loss_ce: 0.014966
2022-01-08 10:03:46,686 iteration 1217 : loss : 0.045222, loss_ce: 0.015952
2022-01-08 10:03:48,224 iteration 1218 : loss : 0.042960, loss_ce: 0.015894
2022-01-08 10:03:49,722 iteration 1219 : loss : 0.058726, loss_ce: 0.022084
2022-01-08 10:03:51,290 iteration 1220 : loss : 0.055596, loss_ce: 0.020888
2022-01-08 10:03:52,824 iteration 1221 : loss : 0.065208, loss_ce: 0.025580
2022-01-08 10:03:54,471 iteration 1222 : loss : 0.085927, loss_ce: 0.031362
2022-01-08 10:03:56,021 iteration 1223 : loss : 0.057122, loss_ce: 0.024858
2022-01-08 10:03:57,638 iteration 1224 : loss : 0.081574, loss_ce: 0.031756
 18%|█████▍                        | 72/400 [35:47<2:42:30, 29.73s/it]2022-01-08 10:03:59,232 iteration 1225 : loss : 0.052819, loss_ce: 0.021493
2022-01-08 10:04:00,850 iteration 1226 : loss : 0.062091, loss_ce: 0.025475
2022-01-08 10:04:02,371 iteration 1227 : loss : 0.072438, loss_ce: 0.026791
2022-01-08 10:04:03,936 iteration 1228 : loss : 0.061515, loss_ce: 0.025357
2022-01-08 10:04:05,435 iteration 1229 : loss : 0.068701, loss_ce: 0.025317
2022-01-08 10:04:06,963 iteration 1230 : loss : 0.056630, loss_ce: 0.022169
2022-01-08 10:04:08,574 iteration 1231 : loss : 0.064824, loss_ce: 0.028053
2022-01-08 10:04:10,100 iteration 1232 : loss : 0.052643, loss_ce: 0.019251
2022-01-08 10:04:11,641 iteration 1233 : loss : 0.041649, loss_ce: 0.016387
2022-01-08 10:04:13,268 iteration 1234 : loss : 0.058486, loss_ce: 0.024870
2022-01-08 10:04:14,883 iteration 1235 : loss : 0.095738, loss_ce: 0.029203
2022-01-08 10:04:16,416 iteration 1236 : loss : 0.066863, loss_ce: 0.024978
2022-01-08 10:04:17,894 iteration 1237 : loss : 0.062766, loss_ce: 0.026080
2022-01-08 10:04:19,517 iteration 1238 : loss : 0.064543, loss_ce: 0.025015
2022-01-08 10:04:21,032 iteration 1239 : loss : 0.047113, loss_ce: 0.017700
2022-01-08 10:04:22,554 iteration 1240 : loss : 0.046798, loss_ce: 0.019596
2022-01-08 10:04:24,076 iteration 1241 : loss : 0.057457, loss_ce: 0.026481
 18%|█████▍                        | 73/400 [36:14<2:36:37, 28.74s/it]2022-01-08 10:04:25,718 iteration 1242 : loss : 0.067278, loss_ce: 0.039092
2022-01-08 10:04:27,251 iteration 1243 : loss : 0.091388, loss_ce: 0.029528
2022-01-08 10:04:28,789 iteration 1244 : loss : 0.081207, loss_ce: 0.031734
2022-01-08 10:04:30,375 iteration 1245 : loss : 0.091982, loss_ce: 0.038666
2022-01-08 10:04:31,960 iteration 1246 : loss : 0.044502, loss_ce: 0.016263
2022-01-08 10:04:33,496 iteration 1247 : loss : 0.045348, loss_ce: 0.019511
2022-01-08 10:04:35,091 iteration 1248 : loss : 0.076067, loss_ce: 0.023541
2022-01-08 10:04:36,620 iteration 1249 : loss : 0.043142, loss_ce: 0.014521
2022-01-08 10:04:38,298 iteration 1250 : loss : 0.060300, loss_ce: 0.023919
2022-01-08 10:04:40,011 iteration 1251 : loss : 0.057493, loss_ce: 0.028282
2022-01-08 10:04:41,623 iteration 1252 : loss : 0.047531, loss_ce: 0.019697
2022-01-08 10:04:43,118 iteration 1253 : loss : 0.041790, loss_ce: 0.014992
2022-01-08 10:04:44,696 iteration 1254 : loss : 0.050892, loss_ce: 0.025121
2022-01-08 10:04:46,286 iteration 1255 : loss : 0.060631, loss_ce: 0.022863
2022-01-08 10:04:47,845 iteration 1256 : loss : 0.054574, loss_ce: 0.023404
2022-01-08 10:04:49,416 iteration 1257 : loss : 0.071830, loss_ce: 0.020987
2022-01-08 10:04:51,009 iteration 1258 : loss : 0.040379, loss_ce: 0.018161
 18%|█████▌                        | 74/400 [36:41<2:33:11, 28.20s/it]2022-01-08 10:04:52,578 iteration 1259 : loss : 0.042316, loss_ce: 0.019769
2022-01-08 10:04:54,212 iteration 1260 : loss : 0.036537, loss_ce: 0.013936
2022-01-08 10:04:55,768 iteration 1261 : loss : 0.032431, loss_ce: 0.010270
2022-01-08 10:04:57,432 iteration 1262 : loss : 0.046556, loss_ce: 0.016485
2022-01-08 10:04:59,119 iteration 1263 : loss : 0.090744, loss_ce: 0.034390
2022-01-08 10:05:00,676 iteration 1264 : loss : 0.041864, loss_ce: 0.018949
2022-01-08 10:05:02,315 iteration 1265 : loss : 0.058469, loss_ce: 0.028249
2022-01-08 10:05:03,946 iteration 1266 : loss : 0.047270, loss_ce: 0.017214
2022-01-08 10:05:05,553 iteration 1267 : loss : 0.058963, loss_ce: 0.021563
2022-01-08 10:05:07,093 iteration 1268 : loss : 0.083075, loss_ce: 0.027862
2022-01-08 10:05:08,755 iteration 1269 : loss : 0.097559, loss_ce: 0.026541
2022-01-08 10:05:10,376 iteration 1270 : loss : 0.040892, loss_ce: 0.016732
2022-01-08 10:05:11,950 iteration 1271 : loss : 0.050787, loss_ce: 0.024070
2022-01-08 10:05:13,440 iteration 1272 : loss : 0.048358, loss_ce: 0.017849
2022-01-08 10:05:14,988 iteration 1273 : loss : 0.053541, loss_ce: 0.018958
2022-01-08 10:05:16,563 iteration 1274 : loss : 0.064714, loss_ce: 0.019376
2022-01-08 10:05:16,563 Training Data Eval:
2022-01-08 10:05:24,472   Average segmentation loss on training set: 0.0454
2022-01-08 10:05:24,472 Validation Data Eval:
2022-01-08 10:05:27,210   Average segmentation loss on validation set: 0.1029
2022-01-08 10:05:28,775 iteration 1275 : loss : 0.047574, loss_ce: 0.017319
 19%|█████▋                        | 75/400 [37:19<2:48:16, 31.07s/it]2022-01-08 10:05:30,301 iteration 1276 : loss : 0.044375, loss_ce: 0.018655
2022-01-08 10:05:31,996 iteration 1277 : loss : 0.043580, loss_ce: 0.016174
2022-01-08 10:05:33,534 iteration 1278 : loss : 0.040804, loss_ce: 0.015723
2022-01-08 10:05:35,215 iteration 1279 : loss : 0.065192, loss_ce: 0.024877
2022-01-08 10:05:36,820 iteration 1280 : loss : 0.056010, loss_ce: 0.025621
2022-01-08 10:05:38,416 iteration 1281 : loss : 0.058246, loss_ce: 0.023799
2022-01-08 10:05:39,936 iteration 1282 : loss : 0.030971, loss_ce: 0.014086
2022-01-08 10:05:41,569 iteration 1283 : loss : 0.062266, loss_ce: 0.019484
2022-01-08 10:05:43,100 iteration 1284 : loss : 0.048330, loss_ce: 0.018259
2022-01-08 10:05:44,745 iteration 1285 : loss : 0.043782, loss_ce: 0.012514
2022-01-08 10:05:46,424 iteration 1286 : loss : 0.133216, loss_ce: 0.039121
2022-01-08 10:05:47,983 iteration 1287 : loss : 0.041577, loss_ce: 0.022250
2022-01-08 10:05:49,516 iteration 1288 : loss : 0.057119, loss_ce: 0.025698
2022-01-08 10:05:51,094 iteration 1289 : loss : 0.074549, loss_ce: 0.033669
2022-01-08 10:05:52,710 iteration 1290 : loss : 0.072079, loss_ce: 0.023311
2022-01-08 10:05:54,341 iteration 1291 : loss : 0.079990, loss_ce: 0.039064
2022-01-08 10:05:55,985 iteration 1292 : loss : 0.052711, loss_ce: 0.021174
 19%|█████▋                        | 76/400 [37:46<2:41:31, 29.91s/it]2022-01-08 10:05:57,641 iteration 1293 : loss : 0.046131, loss_ce: 0.015406
2022-01-08 10:05:59,260 iteration 1294 : loss : 0.097136, loss_ce: 0.033459
2022-01-08 10:06:00,925 iteration 1295 : loss : 0.065239, loss_ce: 0.020714
2022-01-08 10:06:02,537 iteration 1296 : loss : 0.067504, loss_ce: 0.024043
2022-01-08 10:06:04,078 iteration 1297 : loss : 0.042522, loss_ce: 0.020921
2022-01-08 10:06:05,584 iteration 1298 : loss : 0.035344, loss_ce: 0.013754
2022-01-08 10:06:07,174 iteration 1299 : loss : 0.056028, loss_ce: 0.028929
2022-01-08 10:06:08,749 iteration 1300 : loss : 0.040899, loss_ce: 0.012482
2022-01-08 10:06:10,362 iteration 1301 : loss : 0.029700, loss_ce: 0.011355
2022-01-08 10:06:11,914 iteration 1302 : loss : 0.072905, loss_ce: 0.029707
2022-01-08 10:06:13,470 iteration 1303 : loss : 0.057381, loss_ce: 0.020484
2022-01-08 10:06:15,061 iteration 1304 : loss : 0.047045, loss_ce: 0.017401
2022-01-08 10:06:16,635 iteration 1305 : loss : 0.062045, loss_ce: 0.024854
2022-01-08 10:06:18,259 iteration 1306 : loss : 0.063327, loss_ce: 0.020019
2022-01-08 10:06:19,879 iteration 1307 : loss : 0.047493, loss_ce: 0.022818
2022-01-08 10:06:21,473 iteration 1308 : loss : 0.102046, loss_ce: 0.022484
2022-01-08 10:06:22,990 iteration 1309 : loss : 0.048271, loss_ce: 0.018511
 19%|█████▊                        | 77/400 [38:13<2:36:20, 29.04s/it]2022-01-08 10:06:24,699 iteration 1310 : loss : 0.048475, loss_ce: 0.018270
2022-01-08 10:06:26,256 iteration 1311 : loss : 0.048471, loss_ce: 0.021367
2022-01-08 10:06:27,799 iteration 1312 : loss : 0.038277, loss_ce: 0.015346
2022-01-08 10:06:29,344 iteration 1313 : loss : 0.061924, loss_ce: 0.017417
2022-01-08 10:06:30,931 iteration 1314 : loss : 0.060566, loss_ce: 0.028633
2022-01-08 10:06:32,541 iteration 1315 : loss : 0.067557, loss_ce: 0.024401
2022-01-08 10:06:34,245 iteration 1316 : loss : 0.057014, loss_ce: 0.022737
2022-01-08 10:06:35,819 iteration 1317 : loss : 0.066382, loss_ce: 0.036382
2022-01-08 10:06:37,493 iteration 1318 : loss : 0.181412, loss_ce: 0.037786
2022-01-08 10:06:39,123 iteration 1319 : loss : 0.048368, loss_ce: 0.020729
2022-01-08 10:06:40,685 iteration 1320 : loss : 0.047985, loss_ce: 0.021541
2022-01-08 10:06:42,289 iteration 1321 : loss : 0.078990, loss_ce: 0.026461
2022-01-08 10:06:43,811 iteration 1322 : loss : 0.051519, loss_ce: 0.023690
2022-01-08 10:06:45,380 iteration 1323 : loss : 0.053197, loss_ce: 0.019458
2022-01-08 10:06:46,987 iteration 1324 : loss : 0.034573, loss_ce: 0.014131
2022-01-08 10:06:48,631 iteration 1325 : loss : 0.060202, loss_ce: 0.019598
2022-01-08 10:06:50,112 iteration 1326 : loss : 0.051082, loss_ce: 0.018365
 20%|█████▊                        | 78/400 [38:40<2:32:45, 28.46s/it]2022-01-08 10:06:51,696 iteration 1327 : loss : 0.052872, loss_ce: 0.017345
2022-01-08 10:06:53,306 iteration 1328 : loss : 0.093757, loss_ce: 0.046133
2022-01-08 10:06:54,882 iteration 1329 : loss : 0.044361, loss_ce: 0.018663
2022-01-08 10:06:56,427 iteration 1330 : loss : 0.089143, loss_ce: 0.026286
2022-01-08 10:06:57,937 iteration 1331 : loss : 0.050224, loss_ce: 0.025537
2022-01-08 10:06:59,463 iteration 1332 : loss : 0.054045, loss_ce: 0.017203
2022-01-08 10:07:01,018 iteration 1333 : loss : 0.050686, loss_ce: 0.016047
2022-01-08 10:07:02,597 iteration 1334 : loss : 0.066395, loss_ce: 0.023996
2022-01-08 10:07:04,141 iteration 1335 : loss : 0.040879, loss_ce: 0.013967
2022-01-08 10:07:05,694 iteration 1336 : loss : 0.043808, loss_ce: 0.017569
2022-01-08 10:07:07,333 iteration 1337 : loss : 0.051621, loss_ce: 0.020590
2022-01-08 10:07:08,945 iteration 1338 : loss : 0.056989, loss_ce: 0.016514
2022-01-08 10:07:10,554 iteration 1339 : loss : 0.044373, loss_ce: 0.020517
2022-01-08 10:07:12,160 iteration 1340 : loss : 0.048760, loss_ce: 0.020970
2022-01-08 10:07:13,755 iteration 1341 : loss : 0.065583, loss_ce: 0.024579
2022-01-08 10:07:15,326 iteration 1342 : loss : 0.066707, loss_ce: 0.027990
2022-01-08 10:07:16,845 iteration 1343 : loss : 0.037452, loss_ce: 0.013494
 20%|█████▉                        | 79/400 [39:07<2:29:30, 27.94s/it]2022-01-08 10:07:18,465 iteration 1344 : loss : 0.049095, loss_ce: 0.018766
2022-01-08 10:07:20,126 iteration 1345 : loss : 0.072108, loss_ce: 0.027951
2022-01-08 10:07:21,689 iteration 1346 : loss : 0.066110, loss_ce: 0.028355
2022-01-08 10:07:23,347 iteration 1347 : loss : 0.041702, loss_ce: 0.016339
2022-01-08 10:07:25,043 iteration 1348 : loss : 0.056952, loss_ce: 0.023655
2022-01-08 10:07:26,632 iteration 1349 : loss : 0.051721, loss_ce: 0.018922
2022-01-08 10:07:28,215 iteration 1350 : loss : 0.049963, loss_ce: 0.023234
2022-01-08 10:07:29,797 iteration 1351 : loss : 0.054362, loss_ce: 0.026035
2022-01-08 10:07:31,441 iteration 1352 : loss : 0.039606, loss_ce: 0.018408
2022-01-08 10:07:33,060 iteration 1353 : loss : 0.072456, loss_ce: 0.030220
2022-01-08 10:07:34,704 iteration 1354 : loss : 0.080930, loss_ce: 0.025967
2022-01-08 10:07:36,404 iteration 1355 : loss : 0.194004, loss_ce: 0.050186
2022-01-08 10:07:38,019 iteration 1356 : loss : 0.049401, loss_ce: 0.021549
2022-01-08 10:07:39,645 iteration 1357 : loss : 0.038935, loss_ce: 0.015958
2022-01-08 10:07:41,291 iteration 1358 : loss : 0.052191, loss_ce: 0.021386
2022-01-08 10:07:42,878 iteration 1359 : loss : 0.054518, loss_ce: 0.024289
2022-01-08 10:07:42,878 Training Data Eval:
2022-01-08 10:07:50,776   Average segmentation loss on training set: 0.0616
2022-01-08 10:07:50,776 Validation Data Eval:
2022-01-08 10:07:53,502   Average segmentation loss on validation set: 0.1672
2022-01-08 10:07:55,151 iteration 1360 : loss : 0.050089, loss_ce: 0.014354
 20%|██████                        | 80/400 [39:45<2:45:36, 31.05s/it]2022-01-08 10:07:56,882 iteration 1361 : loss : 0.065995, loss_ce: 0.027046
2022-01-08 10:07:58,481 iteration 1362 : loss : 0.046893, loss_ce: 0.017726
2022-01-08 10:08:00,245 iteration 1363 : loss : 0.071374, loss_ce: 0.031265
2022-01-08 10:08:01,697 iteration 1364 : loss : 0.074633, loss_ce: 0.017653
2022-01-08 10:08:03,358 iteration 1365 : loss : 0.052910, loss_ce: 0.021933
2022-01-08 10:08:04,966 iteration 1366 : loss : 0.080447, loss_ce: 0.028151
2022-01-08 10:08:06,542 iteration 1367 : loss : 0.051102, loss_ce: 0.017970
2022-01-08 10:08:08,138 iteration 1368 : loss : 0.051013, loss_ce: 0.021992
2022-01-08 10:08:09,690 iteration 1369 : loss : 0.056473, loss_ce: 0.020068
2022-01-08 10:08:11,319 iteration 1370 : loss : 0.051220, loss_ce: 0.025330
2022-01-08 10:08:12,876 iteration 1371 : loss : 0.041570, loss_ce: 0.016752
2022-01-08 10:08:14,422 iteration 1372 : loss : 0.057722, loss_ce: 0.023225
2022-01-08 10:08:15,959 iteration 1373 : loss : 0.053824, loss_ce: 0.018551
2022-01-08 10:08:17,554 iteration 1374 : loss : 0.046621, loss_ce: 0.015478
2022-01-08 10:08:19,165 iteration 1375 : loss : 0.066504, loss_ce: 0.031092
2022-01-08 10:08:20,761 iteration 1376 : loss : 0.082859, loss_ce: 0.031941
2022-01-08 10:08:22,332 iteration 1377 : loss : 0.031871, loss_ce: 0.012168
 20%|██████                        | 81/400 [40:12<2:38:55, 29.89s/it]2022-01-08 10:08:23,885 iteration 1378 : loss : 0.050479, loss_ce: 0.018961
2022-01-08 10:08:25,514 iteration 1379 : loss : 0.045041, loss_ce: 0.016665
2022-01-08 10:08:27,108 iteration 1380 : loss : 0.052027, loss_ce: 0.023462
2022-01-08 10:08:28,785 iteration 1381 : loss : 0.054981, loss_ce: 0.023492
2022-01-08 10:08:30,287 iteration 1382 : loss : 0.044397, loss_ce: 0.016365
2022-01-08 10:08:31,946 iteration 1383 : loss : 0.053425, loss_ce: 0.020777
2022-01-08 10:08:33,541 iteration 1384 : loss : 0.057950, loss_ce: 0.021182
2022-01-08 10:08:35,059 iteration 1385 : loss : 0.066802, loss_ce: 0.021379
2022-01-08 10:08:36,612 iteration 1386 : loss : 0.039591, loss_ce: 0.014775
2022-01-08 10:08:38,182 iteration 1387 : loss : 0.062319, loss_ce: 0.024783
2022-01-08 10:08:39,774 iteration 1388 : loss : 0.067086, loss_ce: 0.019706
2022-01-08 10:08:41,351 iteration 1389 : loss : 0.054456, loss_ce: 0.020990
2022-01-08 10:08:42,985 iteration 1390 : loss : 0.043974, loss_ce: 0.017717
2022-01-08 10:08:44,645 iteration 1391 : loss : 0.056825, loss_ce: 0.018766
2022-01-08 10:08:46,276 iteration 1392 : loss : 0.040898, loss_ce: 0.022185
2022-01-08 10:08:47,833 iteration 1393 : loss : 0.045784, loss_ce: 0.015853
2022-01-08 10:08:49,481 iteration 1394 : loss : 0.041522, loss_ce: 0.018639
 20%|██████▏                       | 82/400 [40:39<2:34:03, 29.07s/it]2022-01-08 10:08:51,092 iteration 1395 : loss : 0.056831, loss_ce: 0.023713
2022-01-08 10:08:52,655 iteration 1396 : loss : 0.044349, loss_ce: 0.020509
2022-01-08 10:08:54,301 iteration 1397 : loss : 0.044638, loss_ce: 0.017438
2022-01-08 10:08:55,961 iteration 1398 : loss : 0.054878, loss_ce: 0.025202
2022-01-08 10:08:57,463 iteration 1399 : loss : 0.071196, loss_ce: 0.037457
2022-01-08 10:08:58,962 iteration 1400 : loss : 0.042481, loss_ce: 0.014026
2022-01-08 10:09:00,606 iteration 1401 : loss : 0.048833, loss_ce: 0.017825
2022-01-08 10:09:02,208 iteration 1402 : loss : 0.043518, loss_ce: 0.016761
2022-01-08 10:09:03,735 iteration 1403 : loss : 0.046858, loss_ce: 0.015449
2022-01-08 10:09:05,258 iteration 1404 : loss : 0.033112, loss_ce: 0.013808
2022-01-08 10:09:06,832 iteration 1405 : loss : 0.044912, loss_ce: 0.016416
2022-01-08 10:09:08,420 iteration 1406 : loss : 0.052127, loss_ce: 0.018193
2022-01-08 10:09:10,048 iteration 1407 : loss : 0.032916, loss_ce: 0.013203
2022-01-08 10:09:11,629 iteration 1408 : loss : 0.050053, loss_ce: 0.017332
2022-01-08 10:09:13,223 iteration 1409 : loss : 0.081377, loss_ce: 0.030287
2022-01-08 10:09:14,882 iteration 1410 : loss : 0.039055, loss_ce: 0.013898
2022-01-08 10:09:16,438 iteration 1411 : loss : 0.051446, loss_ce: 0.016546
 21%|██████▏                       | 83/400 [41:06<2:30:13, 28.44s/it]2022-01-08 10:09:18,032 iteration 1412 : loss : 0.048436, loss_ce: 0.018829
2022-01-08 10:09:19,690 iteration 1413 : loss : 0.072413, loss_ce: 0.025814
2022-01-08 10:09:21,255 iteration 1414 : loss : 0.045472, loss_ce: 0.016966
2022-01-08 10:09:22,894 iteration 1415 : loss : 0.058342, loss_ce: 0.015100
2022-01-08 10:09:24,462 iteration 1416 : loss : 0.046600, loss_ce: 0.018741
2022-01-08 10:09:26,016 iteration 1417 : loss : 0.057570, loss_ce: 0.020896
2022-01-08 10:09:27,568 iteration 1418 : loss : 0.063428, loss_ce: 0.027354
2022-01-08 10:09:29,084 iteration 1419 : loss : 0.039851, loss_ce: 0.016529
2022-01-08 10:09:30,714 iteration 1420 : loss : 0.048417, loss_ce: 0.017084
2022-01-08 10:09:32,388 iteration 1421 : loss : 0.067991, loss_ce: 0.026920
2022-01-08 10:09:33,999 iteration 1422 : loss : 0.040508, loss_ce: 0.012319
2022-01-08 10:09:35,513 iteration 1423 : loss : 0.055117, loss_ce: 0.023556
2022-01-08 10:09:37,126 iteration 1424 : loss : 0.044640, loss_ce: 0.018074
2022-01-08 10:09:38,734 iteration 1425 : loss : 0.031450, loss_ce: 0.013788
2022-01-08 10:09:40,268 iteration 1426 : loss : 0.055754, loss_ce: 0.026114
2022-01-08 10:09:41,843 iteration 1427 : loss : 0.062541, loss_ce: 0.023969
2022-01-08 10:09:43,321 iteration 1428 : loss : 0.071246, loss_ce: 0.028106
 21%|██████▎                       | 84/400 [41:33<2:27:18, 27.97s/it]2022-01-08 10:09:45,027 iteration 1429 : loss : 0.057777, loss_ce: 0.026410
2022-01-08 10:09:46,564 iteration 1430 : loss : 0.145948, loss_ce: 0.032180
2022-01-08 10:09:48,141 iteration 1431 : loss : 0.058344, loss_ce: 0.028499
2022-01-08 10:09:49,737 iteration 1432 : loss : 0.050662, loss_ce: 0.022997
2022-01-08 10:09:51,377 iteration 1433 : loss : 0.057969, loss_ce: 0.019722
2022-01-08 10:09:52,939 iteration 1434 : loss : 0.058155, loss_ce: 0.022694
2022-01-08 10:09:54,551 iteration 1435 : loss : 0.059339, loss_ce: 0.024477
2022-01-08 10:09:56,175 iteration 1436 : loss : 0.064084, loss_ce: 0.025439
2022-01-08 10:09:57,810 iteration 1437 : loss : 0.062598, loss_ce: 0.024053
2022-01-08 10:09:59,407 iteration 1438 : loss : 0.064324, loss_ce: 0.030013
2022-01-08 10:10:01,052 iteration 1439 : loss : 0.045749, loss_ce: 0.017069
2022-01-08 10:10:02,685 iteration 1440 : loss : 0.040367, loss_ce: 0.017577
2022-01-08 10:10:04,259 iteration 1441 : loss : 0.044667, loss_ce: 0.021417
2022-01-08 10:10:05,856 iteration 1442 : loss : 0.047366, loss_ce: 0.020096
2022-01-08 10:10:07,477 iteration 1443 : loss : 0.058712, loss_ce: 0.021589
2022-01-08 10:10:09,069 iteration 1444 : loss : 0.083035, loss_ce: 0.021906
2022-01-08 10:10:09,069 Training Data Eval:
2022-01-08 10:10:16,960   Average segmentation loss on training set: 0.0440
2022-01-08 10:10:16,960 Validation Data Eval:
2022-01-08 10:10:19,676   Average segmentation loss on validation set: 0.0820
2022-01-08 10:10:21,259 iteration 1445 : loss : 0.044090, loss_ce: 0.013151
 21%|██████▍                       | 85/400 [42:11<2:42:31, 30.96s/it]2022-01-08 10:10:22,985 iteration 1446 : loss : 0.049185, loss_ce: 0.017750
2022-01-08 10:10:24,554 iteration 1447 : loss : 0.059165, loss_ce: 0.019303
2022-01-08 10:10:26,070 iteration 1448 : loss : 0.069159, loss_ce: 0.039043
2022-01-08 10:10:27,682 iteration 1449 : loss : 0.047911, loss_ce: 0.016477
2022-01-08 10:10:29,377 iteration 1450 : loss : 0.056589, loss_ce: 0.030660
2022-01-08 10:10:31,084 iteration 1451 : loss : 0.057302, loss_ce: 0.023022
2022-01-08 10:10:32,638 iteration 1452 : loss : 0.049786, loss_ce: 0.013484
2022-01-08 10:10:34,197 iteration 1453 : loss : 0.063236, loss_ce: 0.028767
2022-01-08 10:10:35,810 iteration 1454 : loss : 0.058474, loss_ce: 0.020450
2022-01-08 10:10:37,357 iteration 1455 : loss : 0.036963, loss_ce: 0.015766
2022-01-08 10:10:39,030 iteration 1456 : loss : 0.091507, loss_ce: 0.025567
2022-01-08 10:10:40,618 iteration 1457 : loss : 0.043136, loss_ce: 0.018507
2022-01-08 10:10:42,190 iteration 1458 : loss : 0.052079, loss_ce: 0.016148
2022-01-08 10:10:43,727 iteration 1459 : loss : 0.060163, loss_ce: 0.018550
2022-01-08 10:10:45,380 iteration 1460 : loss : 0.074881, loss_ce: 0.030712
2022-01-08 10:10:46,973 iteration 1461 : loss : 0.059196, loss_ce: 0.021644
2022-01-08 10:10:48,620 iteration 1462 : loss : 0.046160, loss_ce: 0.020423
 22%|██████▍                       | 86/400 [42:38<2:36:22, 29.88s/it]2022-01-08 10:10:50,242 iteration 1463 : loss : 0.044169, loss_ce: 0.018128
2022-01-08 10:10:51,800 iteration 1464 : loss : 0.042896, loss_ce: 0.016291
2022-01-08 10:10:53,422 iteration 1465 : loss : 0.066217, loss_ce: 0.022817
2022-01-08 10:10:55,046 iteration 1466 : loss : 0.040485, loss_ce: 0.016018
2022-01-08 10:10:56,643 iteration 1467 : loss : 0.044952, loss_ce: 0.018405
2022-01-08 10:10:58,206 iteration 1468 : loss : 0.041056, loss_ce: 0.019609
2022-01-08 10:10:59,789 iteration 1469 : loss : 0.044617, loss_ce: 0.016642
2022-01-08 10:11:01,409 iteration 1470 : loss : 0.058239, loss_ce: 0.022158
2022-01-08 10:11:03,030 iteration 1471 : loss : 0.052286, loss_ce: 0.023575
2022-01-08 10:11:04,507 iteration 1472 : loss : 0.038377, loss_ce: 0.012917
2022-01-08 10:11:06,185 iteration 1473 : loss : 0.065943, loss_ce: 0.028890
2022-01-08 10:11:07,823 iteration 1474 : loss : 0.045888, loss_ce: 0.013674
2022-01-08 10:11:09,404 iteration 1475 : loss : 0.041033, loss_ce: 0.021104
2022-01-08 10:11:11,085 iteration 1476 : loss : 0.062420, loss_ce: 0.021372
2022-01-08 10:11:12,724 iteration 1477 : loss : 0.047070, loss_ce: 0.015520
2022-01-08 10:11:14,399 iteration 1478 : loss : 0.048182, loss_ce: 0.019434
2022-01-08 10:11:15,942 iteration 1479 : loss : 0.040715, loss_ce: 0.014041
 22%|██████▌                       | 87/400 [43:06<2:31:52, 29.11s/it]2022-01-08 10:11:17,578 iteration 1480 : loss : 0.061276, loss_ce: 0.021202
2022-01-08 10:11:19,139 iteration 1481 : loss : 0.048946, loss_ce: 0.018726
2022-01-08 10:11:20,718 iteration 1482 : loss : 0.041941, loss_ce: 0.015755
2022-01-08 10:11:22,389 iteration 1483 : loss : 0.048255, loss_ce: 0.016674
2022-01-08 10:11:24,123 iteration 1484 : loss : 0.058388, loss_ce: 0.021336
2022-01-08 10:11:25,787 iteration 1485 : loss : 0.035868, loss_ce: 0.016246
2022-01-08 10:11:27,358 iteration 1486 : loss : 0.042690, loss_ce: 0.018075
2022-01-08 10:11:28,944 iteration 1487 : loss : 0.059802, loss_ce: 0.026588
2022-01-08 10:11:30,554 iteration 1488 : loss : 0.041787, loss_ce: 0.018102
2022-01-08 10:11:32,165 iteration 1489 : loss : 0.038220, loss_ce: 0.016297
2022-01-08 10:11:33,798 iteration 1490 : loss : 0.056664, loss_ce: 0.021680
2022-01-08 10:11:35,465 iteration 1491 : loss : 0.042859, loss_ce: 0.018439
2022-01-08 10:11:37,014 iteration 1492 : loss : 0.038294, loss_ce: 0.014757
2022-01-08 10:11:38,649 iteration 1493 : loss : 0.049774, loss_ce: 0.020120
2022-01-08 10:11:40,209 iteration 1494 : loss : 0.040290, loss_ce: 0.016870
2022-01-08 10:11:41,772 iteration 1495 : loss : 0.052855, loss_ce: 0.020769
2022-01-08 10:11:43,365 iteration 1496 : loss : 0.067353, loss_ce: 0.017887
 22%|██████▌                       | 88/400 [43:33<2:28:44, 28.60s/it]2022-01-08 10:11:44,943 iteration 1497 : loss : 0.044642, loss_ce: 0.020396
2022-01-08 10:11:46,498 iteration 1498 : loss : 0.046064, loss_ce: 0.013521
2022-01-08 10:11:48,123 iteration 1499 : loss : 0.044818, loss_ce: 0.022395
2022-01-08 10:11:49,637 iteration 1500 : loss : 0.072189, loss_ce: 0.024249
2022-01-08 10:11:51,206 iteration 1501 : loss : 0.039583, loss_ce: 0.012235
2022-01-08 10:11:52,825 iteration 1502 : loss : 0.045480, loss_ce: 0.015448
2022-01-08 10:11:54,394 iteration 1503 : loss : 0.029177, loss_ce: 0.012899
2022-01-08 10:11:55,961 iteration 1504 : loss : 0.038858, loss_ce: 0.011705
2022-01-08 10:11:57,491 iteration 1505 : loss : 0.035716, loss_ce: 0.013931
2022-01-08 10:11:59,063 iteration 1506 : loss : 0.047535, loss_ce: 0.019438
2022-01-08 10:12:00,647 iteration 1507 : loss : 0.065726, loss_ce: 0.023382
2022-01-08 10:12:02,192 iteration 1508 : loss : 0.062825, loss_ce: 0.029691
2022-01-08 10:12:03,761 iteration 1509 : loss : 0.059254, loss_ce: 0.019499
2022-01-08 10:12:05,296 iteration 1510 : loss : 0.035035, loss_ce: 0.012716
2022-01-08 10:12:06,898 iteration 1511 : loss : 0.043716, loss_ce: 0.021150
2022-01-08 10:12:08,432 iteration 1512 : loss : 0.041090, loss_ce: 0.016999
2022-01-08 10:12:10,029 iteration 1513 : loss : 0.050948, loss_ce: 0.021698
 22%|██████▋                       | 89/400 [44:00<2:25:14, 28.02s/it]2022-01-08 10:12:11,622 iteration 1514 : loss : 0.047598, loss_ce: 0.018238
2022-01-08 10:12:13,207 iteration 1515 : loss : 0.048329, loss_ce: 0.019953
2022-01-08 10:12:14,810 iteration 1516 : loss : 0.047510, loss_ce: 0.018017
2022-01-08 10:12:16,384 iteration 1517 : loss : 0.037624, loss_ce: 0.017095
2022-01-08 10:12:17,998 iteration 1518 : loss : 0.037257, loss_ce: 0.011135
2022-01-08 10:12:19,557 iteration 1519 : loss : 0.046016, loss_ce: 0.017226
2022-01-08 10:12:21,107 iteration 1520 : loss : 0.034825, loss_ce: 0.012422
2022-01-08 10:12:22,665 iteration 1521 : loss : 0.051789, loss_ce: 0.019303
2022-01-08 10:12:24,356 iteration 1522 : loss : 0.037143, loss_ce: 0.012532
2022-01-08 10:12:25,878 iteration 1523 : loss : 0.040789, loss_ce: 0.018705
2022-01-08 10:12:27,481 iteration 1524 : loss : 0.046853, loss_ce: 0.017867
2022-01-08 10:12:29,033 iteration 1525 : loss : 0.037234, loss_ce: 0.014990
2022-01-08 10:12:30,681 iteration 1526 : loss : 0.033087, loss_ce: 0.013415
2022-01-08 10:12:32,336 iteration 1527 : loss : 0.049511, loss_ce: 0.021567
2022-01-08 10:12:33,964 iteration 1528 : loss : 0.041515, loss_ce: 0.015661
2022-01-08 10:12:35,512 iteration 1529 : loss : 0.036839, loss_ce: 0.014688
2022-01-08 10:12:35,512 Training Data Eval:
2022-01-08 10:12:43,409   Average segmentation loss on training set: 0.0304
2022-01-08 10:12:43,409 Validation Data Eval:
2022-01-08 10:12:46,131   Average segmentation loss on validation set: 0.0721
2022-01-08 10:12:47,697 iteration 1530 : loss : 0.041902, loss_ce: 0.018520
 22%|██████▊                       | 90/400 [44:37<2:39:44, 30.92s/it]2022-01-08 10:12:49,456 iteration 1531 : loss : 0.040005, loss_ce: 0.016374
2022-01-08 10:12:51,092 iteration 1532 : loss : 0.040595, loss_ce: 0.014778
2022-01-08 10:12:52,638 iteration 1533 : loss : 0.034278, loss_ce: 0.014301
2022-01-08 10:12:54,280 iteration 1534 : loss : 0.043330, loss_ce: 0.015024
2022-01-08 10:12:55,849 iteration 1535 : loss : 0.034032, loss_ce: 0.012733
2022-01-08 10:12:57,397 iteration 1536 : loss : 0.052428, loss_ce: 0.027851
2022-01-08 10:12:58,936 iteration 1537 : loss : 0.052804, loss_ce: 0.016988
2022-01-08 10:13:00,460 iteration 1538 : loss : 0.094641, loss_ce: 0.021746
2022-01-08 10:13:02,033 iteration 1539 : loss : 0.046820, loss_ce: 0.016490
2022-01-08 10:13:03,616 iteration 1540 : loss : 0.050385, loss_ce: 0.023677
2022-01-08 10:13:05,106 iteration 1541 : loss : 0.037671, loss_ce: 0.012728
2022-01-08 10:13:06,689 iteration 1542 : loss : 0.057898, loss_ce: 0.027426
2022-01-08 10:13:08,245 iteration 1543 : loss : 0.045802, loss_ce: 0.012652
2022-01-08 10:13:09,754 iteration 1544 : loss : 0.055964, loss_ce: 0.021030
2022-01-08 10:13:11,320 iteration 1545 : loss : 0.054531, loss_ce: 0.022744
2022-01-08 10:13:12,880 iteration 1546 : loss : 0.046999, loss_ce: 0.019152
2022-01-08 10:13:14,437 iteration 1547 : loss : 0.061905, loss_ce: 0.022456
 23%|██████▊                       | 91/400 [45:04<2:32:46, 29.66s/it]2022-01-08 10:13:16,055 iteration 1548 : loss : 0.051149, loss_ce: 0.018335
2022-01-08 10:13:17,571 iteration 1549 : loss : 0.039400, loss_ce: 0.016158
2022-01-08 10:13:19,252 iteration 1550 : loss : 0.043800, loss_ce: 0.019326
2022-01-08 10:13:20,832 iteration 1551 : loss : 0.051788, loss_ce: 0.020207
2022-01-08 10:13:22,415 iteration 1552 : loss : 0.050020, loss_ce: 0.020938
2022-01-08 10:13:23,902 iteration 1553 : loss : 0.033098, loss_ce: 0.017258
2022-01-08 10:13:25,431 iteration 1554 : loss : 0.063534, loss_ce: 0.025689
2022-01-08 10:13:27,073 iteration 1555 : loss : 0.040062, loss_ce: 0.013098
2022-01-08 10:13:28,790 iteration 1556 : loss : 0.057464, loss_ce: 0.019299
2022-01-08 10:13:30,447 iteration 1557 : loss : 0.075796, loss_ce: 0.018539
2022-01-08 10:13:32,040 iteration 1558 : loss : 0.040368, loss_ce: 0.012258
2022-01-08 10:13:33,733 iteration 1559 : loss : 0.050800, loss_ce: 0.020974
2022-01-08 10:13:35,296 iteration 1560 : loss : 0.046823, loss_ce: 0.013440
2022-01-08 10:13:36,861 iteration 1561 : loss : 0.058704, loss_ce: 0.023046
2022-01-08 10:13:38,451 iteration 1562 : loss : 0.048799, loss_ce: 0.022656
2022-01-08 10:13:40,020 iteration 1563 : loss : 0.047257, loss_ce: 0.022300
2022-01-08 10:13:41,602 iteration 1564 : loss : 0.048684, loss_ce: 0.015721
 23%|██████▉                       | 92/400 [45:31<2:28:25, 28.91s/it]2022-01-08 10:13:43,251 iteration 1565 : loss : 0.036621, loss_ce: 0.013989
2022-01-08 10:13:44,812 iteration 1566 : loss : 0.065018, loss_ce: 0.028743
2022-01-08 10:13:46,432 iteration 1567 : loss : 0.068944, loss_ce: 0.019044
2022-01-08 10:13:48,128 iteration 1568 : loss : 0.045879, loss_ce: 0.019145
2022-01-08 10:13:49,687 iteration 1569 : loss : 0.041336, loss_ce: 0.017691
2022-01-08 10:13:51,285 iteration 1570 : loss : 0.042980, loss_ce: 0.019620
2022-01-08 10:13:52,850 iteration 1571 : loss : 0.045389, loss_ce: 0.018476
2022-01-08 10:13:54,427 iteration 1572 : loss : 0.060366, loss_ce: 0.026975
2022-01-08 10:13:56,019 iteration 1573 : loss : 0.061876, loss_ce: 0.016213
2022-01-08 10:13:57,612 iteration 1574 : loss : 0.056667, loss_ce: 0.026788
2022-01-08 10:13:59,223 iteration 1575 : loss : 0.048302, loss_ce: 0.020239
2022-01-08 10:14:00,788 iteration 1576 : loss : 0.041756, loss_ce: 0.012811
2022-01-08 10:14:02,320 iteration 1577 : loss : 0.061534, loss_ce: 0.023122
2022-01-08 10:14:03,796 iteration 1578 : loss : 0.037894, loss_ce: 0.014479
2022-01-08 10:14:05,371 iteration 1579 : loss : 0.037254, loss_ce: 0.012865
2022-01-08 10:14:06,974 iteration 1580 : loss : 0.036891, loss_ce: 0.011560
2022-01-08 10:14:08,580 iteration 1581 : loss : 0.039061, loss_ce: 0.015151
 23%|██████▉                       | 93/400 [45:58<2:24:58, 28.33s/it]2022-01-08 10:14:10,246 iteration 1582 : loss : 0.040049, loss_ce: 0.017353
2022-01-08 10:14:11,833 iteration 1583 : loss : 0.061288, loss_ce: 0.019393
2022-01-08 10:14:13,364 iteration 1584 : loss : 0.041651, loss_ce: 0.013883
2022-01-08 10:14:14,927 iteration 1585 : loss : 0.035944, loss_ce: 0.014641
2022-01-08 10:14:16,421 iteration 1586 : loss : 0.035848, loss_ce: 0.014390
2022-01-08 10:14:17,970 iteration 1587 : loss : 0.048516, loss_ce: 0.017503
2022-01-08 10:14:19,524 iteration 1588 : loss : 0.038046, loss_ce: 0.010458
2022-01-08 10:14:21,100 iteration 1589 : loss : 0.038003, loss_ce: 0.014919
2022-01-08 10:14:22,771 iteration 1590 : loss : 0.049646, loss_ce: 0.015777
2022-01-08 10:14:24,329 iteration 1591 : loss : 0.040226, loss_ce: 0.020482
2022-01-08 10:14:25,816 iteration 1592 : loss : 0.034404, loss_ce: 0.015028
2022-01-08 10:14:27,400 iteration 1593 : loss : 0.044401, loss_ce: 0.014094
2022-01-08 10:14:28,964 iteration 1594 : loss : 0.052972, loss_ce: 0.018123
2022-01-08 10:14:30,483 iteration 1595 : loss : 0.038193, loss_ce: 0.019292
2022-01-08 10:14:31,970 iteration 1596 : loss : 0.053988, loss_ce: 0.020472
2022-01-08 10:14:33,559 iteration 1597 : loss : 0.072341, loss_ce: 0.016767
2022-01-08 10:14:35,171 iteration 1598 : loss : 0.052035, loss_ce: 0.017081
 24%|███████                       | 94/400 [46:25<2:21:50, 27.81s/it]2022-01-08 10:14:36,834 iteration 1599 : loss : 0.042306, loss_ce: 0.019374
2022-01-08 10:14:38,395 iteration 1600 : loss : 0.032875, loss_ce: 0.015250
2022-01-08 10:14:40,013 iteration 1601 : loss : 0.039942, loss_ce: 0.015322
2022-01-08 10:14:41,610 iteration 1602 : loss : 0.066633, loss_ce: 0.020990
2022-01-08 10:14:43,177 iteration 1603 : loss : 0.040897, loss_ce: 0.016494
2022-01-08 10:14:44,728 iteration 1604 : loss : 0.057858, loss_ce: 0.015891
2022-01-08 10:14:46,289 iteration 1605 : loss : 0.055710, loss_ce: 0.020285
2022-01-08 10:14:47,875 iteration 1606 : loss : 0.037282, loss_ce: 0.015043
2022-01-08 10:14:49,448 iteration 1607 : loss : 0.066939, loss_ce: 0.026910
2022-01-08 10:14:51,114 iteration 1608 : loss : 0.081693, loss_ce: 0.034358
2022-01-08 10:14:52,674 iteration 1609 : loss : 0.037585, loss_ce: 0.012876
2022-01-08 10:14:54,348 iteration 1610 : loss : 0.047197, loss_ce: 0.016944
2022-01-08 10:14:56,017 iteration 1611 : loss : 0.049612, loss_ce: 0.027009
2022-01-08 10:14:57,708 iteration 1612 : loss : 0.033168, loss_ce: 0.012699
2022-01-08 10:14:59,301 iteration 1613 : loss : 0.045743, loss_ce: 0.016345
2022-01-08 10:15:00,948 iteration 1614 : loss : 0.044481, loss_ce: 0.019626
2022-01-08 10:15:00,948 Training Data Eval:
2022-01-08 10:15:08,832   Average segmentation loss on training set: 0.0307
2022-01-08 10:15:08,833 Validation Data Eval:
2022-01-08 10:15:11,558   Average segmentation loss on validation set: 0.0687
2022-01-08 10:15:13,194 iteration 1615 : loss : 0.077244, loss_ce: 0.029156
 24%|███████▏                      | 95/400 [47:03<2:36:57, 30.88s/it]2022-01-08 10:15:14,830 iteration 1616 : loss : 0.052626, loss_ce: 0.019657
2022-01-08 10:15:16,394 iteration 1617 : loss : 0.046949, loss_ce: 0.019648
2022-01-08 10:15:17,916 iteration 1618 : loss : 0.042759, loss_ce: 0.014894
2022-01-08 10:15:19,488 iteration 1619 : loss : 0.044666, loss_ce: 0.026362
2022-01-08 10:15:21,191 iteration 1620 : loss : 0.047825, loss_ce: 0.015917
2022-01-08 10:15:22,902 iteration 1621 : loss : 0.044681, loss_ce: 0.016724
2022-01-08 10:15:24,481 iteration 1622 : loss : 0.055044, loss_ce: 0.021166
2022-01-08 10:15:26,013 iteration 1623 : loss : 0.034412, loss_ce: 0.011543
2022-01-08 10:15:27,616 iteration 1624 : loss : 0.043016, loss_ce: 0.016127
2022-01-08 10:15:29,124 iteration 1625 : loss : 0.033946, loss_ce: 0.012223
2022-01-08 10:15:30,676 iteration 1626 : loss : 0.033285, loss_ce: 0.013183
2022-01-08 10:15:32,384 iteration 1627 : loss : 0.071619, loss_ce: 0.024732
2022-01-08 10:15:34,068 iteration 1628 : loss : 0.063691, loss_ce: 0.027208
2022-01-08 10:15:35,626 iteration 1629 : loss : 0.040410, loss_ce: 0.014169
2022-01-08 10:15:37,158 iteration 1630 : loss : 0.029915, loss_ce: 0.012116
2022-01-08 10:15:38,691 iteration 1631 : loss : 0.052446, loss_ce: 0.020428
2022-01-08 10:15:40,240 iteration 1632 : loss : 0.034949, loss_ce: 0.015058
 24%|███████▏                      | 96/400 [47:30<2:30:36, 29.72s/it]2022-01-08 10:15:41,867 iteration 1633 : loss : 0.039548, loss_ce: 0.013385
2022-01-08 10:15:43,472 iteration 1634 : loss : 0.071137, loss_ce: 0.029930
2022-01-08 10:15:45,026 iteration 1635 : loss : 0.062486, loss_ce: 0.021768
2022-01-08 10:15:46,528 iteration 1636 : loss : 0.043896, loss_ce: 0.021322
2022-01-08 10:15:48,129 iteration 1637 : loss : 0.046637, loss_ce: 0.013724
2022-01-08 10:15:49,704 iteration 1638 : loss : 0.046238, loss_ce: 0.020909
2022-01-08 10:15:51,344 iteration 1639 : loss : 0.040382, loss_ce: 0.018764
2022-01-08 10:15:52,892 iteration 1640 : loss : 0.046759, loss_ce: 0.016145
2022-01-08 10:15:54,560 iteration 1641 : loss : 0.061660, loss_ce: 0.020463
2022-01-08 10:15:56,093 iteration 1642 : loss : 0.035924, loss_ce: 0.013058
2022-01-08 10:15:57,602 iteration 1643 : loss : 0.042046, loss_ce: 0.014133
2022-01-08 10:15:59,244 iteration 1644 : loss : 0.042863, loss_ce: 0.017911
2022-01-08 10:16:00,926 iteration 1645 : loss : 0.048397, loss_ce: 0.018620
2022-01-08 10:16:02,472 iteration 1646 : loss : 0.063411, loss_ce: 0.021604
2022-01-08 10:16:04,030 iteration 1647 : loss : 0.053978, loss_ce: 0.019104
2022-01-08 10:16:05,521 iteration 1648 : loss : 0.037722, loss_ce: 0.014966
2022-01-08 10:16:07,028 iteration 1649 : loss : 0.032625, loss_ce: 0.013284
 24%|███████▎                      | 97/400 [47:57<2:25:40, 28.85s/it]2022-01-08 10:16:08,772 iteration 1650 : loss : 0.056824, loss_ce: 0.026062
2022-01-08 10:16:10,334 iteration 1651 : loss : 0.069547, loss_ce: 0.021298
2022-01-08 10:16:11,928 iteration 1652 : loss : 0.051691, loss_ce: 0.018097
2022-01-08 10:16:13,489 iteration 1653 : loss : 0.040982, loss_ce: 0.013330
2022-01-08 10:16:15,051 iteration 1654 : loss : 0.044264, loss_ce: 0.014361
2022-01-08 10:16:16,575 iteration 1655 : loss : 0.042789, loss_ce: 0.015437
2022-01-08 10:16:18,207 iteration 1656 : loss : 0.060215, loss_ce: 0.023836
2022-01-08 10:16:19,776 iteration 1657 : loss : 0.039967, loss_ce: 0.016679
2022-01-08 10:16:21,416 iteration 1658 : loss : 0.096946, loss_ce: 0.071476
2022-01-08 10:16:23,085 iteration 1659 : loss : 0.043080, loss_ce: 0.018878
2022-01-08 10:16:24,678 iteration 1660 : loss : 0.032499, loss_ce: 0.013233
2022-01-08 10:16:26,202 iteration 1661 : loss : 0.043460, loss_ce: 0.015970
2022-01-08 10:16:27,826 iteration 1662 : loss : 0.047626, loss_ce: 0.018614
2022-01-08 10:16:29,480 iteration 1663 : loss : 0.045455, loss_ce: 0.021194
2022-01-08 10:16:31,064 iteration 1664 : loss : 0.069195, loss_ce: 0.023873
2022-01-08 10:16:32,626 iteration 1665 : loss : 0.047682, loss_ce: 0.020457
2022-01-08 10:16:34,223 iteration 1666 : loss : 0.053070, loss_ce: 0.018595
 24%|███████▎                      | 98/400 [48:24<2:22:40, 28.35s/it]2022-01-08 10:16:35,790 iteration 1667 : loss : 0.044883, loss_ce: 0.021189
2022-01-08 10:16:37,450 iteration 1668 : loss : 0.059759, loss_ce: 0.021654
2022-01-08 10:16:39,071 iteration 1669 : loss : 0.043994, loss_ce: 0.015059
2022-01-08 10:16:40,572 iteration 1670 : loss : 0.028073, loss_ce: 0.009753
2022-01-08 10:16:42,092 iteration 1671 : loss : 0.046250, loss_ce: 0.020358
2022-01-08 10:16:43,742 iteration 1672 : loss : 0.067857, loss_ce: 0.028128
2022-01-08 10:16:45,367 iteration 1673 : loss : 0.033776, loss_ce: 0.011397
2022-01-08 10:16:47,055 iteration 1674 : loss : 0.063657, loss_ce: 0.024046
2022-01-08 10:16:48,614 iteration 1675 : loss : 0.066301, loss_ce: 0.025268
2022-01-08 10:16:50,262 iteration 1676 : loss : 0.047794, loss_ce: 0.021609
2022-01-08 10:16:51,870 iteration 1677 : loss : 0.074027, loss_ce: 0.021821
2022-01-08 10:16:53,504 iteration 1678 : loss : 0.041276, loss_ce: 0.017202
2022-01-08 10:16:55,067 iteration 1679 : loss : 0.078979, loss_ce: 0.039988
2022-01-08 10:16:56,684 iteration 1680 : loss : 0.044479, loss_ce: 0.020356
2022-01-08 10:16:58,256 iteration 1681 : loss : 0.036859, loss_ce: 0.015147
2022-01-08 10:16:59,841 iteration 1682 : loss : 0.045794, loss_ce: 0.018859
2022-01-08 10:17:01,388 iteration 1683 : loss : 0.042908, loss_ce: 0.020518
 25%|███████▍                      | 99/400 [48:51<2:20:26, 28.00s/it]2022-01-08 10:17:03,051 iteration 1684 : loss : 0.065057, loss_ce: 0.025524
2022-01-08 10:17:04,591 iteration 1685 : loss : 0.040286, loss_ce: 0.020094
2022-01-08 10:17:06,096 iteration 1686 : loss : 0.031209, loss_ce: 0.011287
2022-01-08 10:17:07,641 iteration 1687 : loss : 0.034781, loss_ce: 0.013970
2022-01-08 10:17:09,229 iteration 1688 : loss : 0.045905, loss_ce: 0.021182
2022-01-08 10:17:10,766 iteration 1689 : loss : 0.034198, loss_ce: 0.012081
2022-01-08 10:17:12,375 iteration 1690 : loss : 0.072027, loss_ce: 0.028130
2022-01-08 10:17:13,938 iteration 1691 : loss : 0.031137, loss_ce: 0.012492
2022-01-08 10:17:15,572 iteration 1692 : loss : 0.058349, loss_ce: 0.021287
2022-01-08 10:17:17,225 iteration 1693 : loss : 0.041735, loss_ce: 0.019024
2022-01-08 10:17:18,846 iteration 1694 : loss : 0.039945, loss_ce: 0.014476
2022-01-08 10:17:20,346 iteration 1695 : loss : 0.033736, loss_ce: 0.011193
2022-01-08 10:17:21,909 iteration 1696 : loss : 0.046604, loss_ce: 0.016787
2022-01-08 10:17:23,540 iteration 1697 : loss : 0.058513, loss_ce: 0.018329
2022-01-08 10:17:25,107 iteration 1698 : loss : 0.031303, loss_ce: 0.010131
2022-01-08 10:17:26,579 iteration 1699 : loss : 0.036880, loss_ce: 0.017490
2022-01-08 10:17:26,579 Training Data Eval:
2022-01-08 10:17:34,481   Average segmentation loss on training set: 0.0304
2022-01-08 10:17:34,482 Validation Data Eval:
2022-01-08 10:17:37,214   Average segmentation loss on validation set: 0.1053
2022-01-08 10:17:38,813 iteration 1700 : loss : 0.041136, loss_ce: 0.012266
 25%|███████▎                     | 100/400 [49:29<2:34:06, 30.82s/it]2022-01-08 10:17:40,479 iteration 1701 : loss : 0.054241, loss_ce: 0.026284
2022-01-08 10:17:42,073 iteration 1702 : loss : 0.040058, loss_ce: 0.017617
2022-01-08 10:17:43,596 iteration 1703 : loss : 0.036815, loss_ce: 0.015354
2022-01-08 10:17:45,255 iteration 1704 : loss : 0.034760, loss_ce: 0.012536
2022-01-08 10:17:46,836 iteration 1705 : loss : 0.039316, loss_ce: 0.019902
2022-01-08 10:17:48,436 iteration 1706 : loss : 0.036352, loss_ce: 0.016859
2022-01-08 10:17:50,010 iteration 1707 : loss : 0.048933, loss_ce: 0.017445
2022-01-08 10:17:51,626 iteration 1708 : loss : 0.046661, loss_ce: 0.013063
2022-01-08 10:17:53,136 iteration 1709 : loss : 0.040412, loss_ce: 0.018254
2022-01-08 10:17:54,626 iteration 1710 : loss : 0.033912, loss_ce: 0.013553
2022-01-08 10:17:56,212 iteration 1711 : loss : 0.041267, loss_ce: 0.015182
2022-01-08 10:17:57,735 iteration 1712 : loss : 0.051227, loss_ce: 0.019728
2022-01-08 10:17:59,248 iteration 1713 : loss : 0.048751, loss_ce: 0.017450
2022-01-08 10:18:00,917 iteration 1714 : loss : 0.059828, loss_ce: 0.019034
2022-01-08 10:18:02,538 iteration 1715 : loss : 0.067686, loss_ce: 0.018967
2022-01-08 10:18:04,204 iteration 1716 : loss : 0.055430, loss_ce: 0.020929
2022-01-08 10:18:05,785 iteration 1717 : loss : 0.044907, loss_ce: 0.016542
 25%|███████▎                     | 101/400 [49:56<2:27:51, 29.67s/it]2022-01-08 10:18:07,375 iteration 1718 : loss : 0.042812, loss_ce: 0.014355
2022-01-08 10:18:08,979 iteration 1719 : loss : 0.055255, loss_ce: 0.025330
2022-01-08 10:18:10,458 iteration 1720 : loss : 0.035944, loss_ce: 0.012326
2022-01-08 10:18:12,028 iteration 1721 : loss : 0.047300, loss_ce: 0.020875
2022-01-08 10:18:13,664 iteration 1722 : loss : 0.041089, loss_ce: 0.013340
2022-01-08 10:18:15,241 iteration 1723 : loss : 0.062083, loss_ce: 0.025510
2022-01-08 10:18:16,862 iteration 1724 : loss : 0.039468, loss_ce: 0.013600
2022-01-08 10:18:18,574 iteration 1725 : loss : 0.081545, loss_ce: 0.027744
2022-01-08 10:18:20,242 iteration 1726 : loss : 0.069967, loss_ce: 0.022292
2022-01-08 10:18:21,838 iteration 1727 : loss : 0.043301, loss_ce: 0.021093
2022-01-08 10:18:23,404 iteration 1728 : loss : 0.046626, loss_ce: 0.014883
2022-01-08 10:18:24,980 iteration 1729 : loss : 0.048207, loss_ce: 0.023610
2022-01-08 10:18:26,583 iteration 1730 : loss : 0.049064, loss_ce: 0.018487
2022-01-08 10:18:28,234 iteration 1731 : loss : 0.048403, loss_ce: 0.018912
2022-01-08 10:18:29,827 iteration 1732 : loss : 0.033018, loss_ce: 0.016108
2022-01-08 10:18:31,503 iteration 1733 : loss : 0.068276, loss_ce: 0.029462
2022-01-08 10:18:33,042 iteration 1734 : loss : 0.036214, loss_ce: 0.013010
 26%|███████▍                     | 102/400 [50:23<2:23:45, 28.94s/it]2022-01-08 10:18:34,625 iteration 1735 : loss : 0.053272, loss_ce: 0.019765
2022-01-08 10:18:36,195 iteration 1736 : loss : 0.032917, loss_ce: 0.012683
2022-01-08 10:18:37,777 iteration 1737 : loss : 0.032624, loss_ce: 0.013220
2022-01-08 10:18:39,328 iteration 1738 : loss : 0.040059, loss_ce: 0.015006
2022-01-08 10:18:41,012 iteration 1739 : loss : 0.071025, loss_ce: 0.029986
2022-01-08 10:18:42,637 iteration 1740 : loss : 0.055253, loss_ce: 0.019035
2022-01-08 10:18:44,212 iteration 1741 : loss : 0.045583, loss_ce: 0.016861
2022-01-08 10:18:45,907 iteration 1742 : loss : 0.042247, loss_ce: 0.017816
2022-01-08 10:18:47,477 iteration 1743 : loss : 0.043613, loss_ce: 0.012966
2022-01-08 10:18:49,091 iteration 1744 : loss : 0.037463, loss_ce: 0.014572
2022-01-08 10:18:50,839 iteration 1745 : loss : 0.042162, loss_ce: 0.022811
2022-01-08 10:18:52,415 iteration 1746 : loss : 0.033639, loss_ce: 0.014354
2022-01-08 10:18:53,978 iteration 1747 : loss : 0.039256, loss_ce: 0.016516
2022-01-08 10:18:55,569 iteration 1748 : loss : 0.035986, loss_ce: 0.013270
2022-01-08 10:18:57,152 iteration 1749 : loss : 0.037339, loss_ce: 0.012705
2022-01-08 10:18:58,680 iteration 1750 : loss : 0.041078, loss_ce: 0.020706
2022-01-08 10:19:00,365 iteration 1751 : loss : 0.051226, loss_ce: 0.019025
 26%|███████▍                     | 103/400 [50:50<2:20:51, 28.46s/it]2022-01-08 10:19:01,961 iteration 1752 : loss : 0.027784, loss_ce: 0.010191
2022-01-08 10:19:03,505 iteration 1753 : loss : 0.039463, loss_ce: 0.017903
2022-01-08 10:19:05,065 iteration 1754 : loss : 0.035926, loss_ce: 0.013957
2022-01-08 10:19:06,640 iteration 1755 : loss : 0.038828, loss_ce: 0.015569
2022-01-08 10:19:08,240 iteration 1756 : loss : 0.053094, loss_ce: 0.024066
2022-01-08 10:19:09,896 iteration 1757 : loss : 0.046676, loss_ce: 0.022433
2022-01-08 10:19:11,524 iteration 1758 : loss : 0.072141, loss_ce: 0.027509
2022-01-08 10:19:13,030 iteration 1759 : loss : 0.053601, loss_ce: 0.015470
2022-01-08 10:19:14,601 iteration 1760 : loss : 0.038766, loss_ce: 0.015157
2022-01-08 10:19:16,135 iteration 1761 : loss : 0.031718, loss_ce: 0.012139
2022-01-08 10:19:17,709 iteration 1762 : loss : 0.035920, loss_ce: 0.015886
2022-01-08 10:19:19,202 iteration 1763 : loss : 0.048676, loss_ce: 0.022968
2022-01-08 10:19:20,887 iteration 1764 : loss : 0.033685, loss_ce: 0.010359
2022-01-08 10:19:22,481 iteration 1765 : loss : 0.043745, loss_ce: 0.014708
2022-01-08 10:19:24,129 iteration 1766 : loss : 0.048619, loss_ce: 0.017535
2022-01-08 10:19:25,738 iteration 1767 : loss : 0.044416, loss_ce: 0.016079
2022-01-08 10:19:27,314 iteration 1768 : loss : 0.044630, loss_ce: 0.017573
 26%|███████▌                     | 104/400 [51:17<2:18:10, 28.01s/it]2022-01-08 10:19:28,978 iteration 1769 : loss : 0.040701, loss_ce: 0.015449
2022-01-08 10:19:30,548 iteration 1770 : loss : 0.034726, loss_ce: 0.010703
2022-01-08 10:19:32,085 iteration 1771 : loss : 0.023880, loss_ce: 0.008144
2022-01-08 10:19:33,730 iteration 1772 : loss : 0.042845, loss_ce: 0.019011
2022-01-08 10:19:35,345 iteration 1773 : loss : 0.041194, loss_ce: 0.016578
2022-01-08 10:19:36,900 iteration 1774 : loss : 0.038793, loss_ce: 0.012953
2022-01-08 10:19:38,478 iteration 1775 : loss : 0.037334, loss_ce: 0.012495
2022-01-08 10:19:40,028 iteration 1776 : loss : 0.042646, loss_ce: 0.013592
2022-01-08 10:19:41,555 iteration 1777 : loss : 0.033311, loss_ce: 0.013013
2022-01-08 10:19:43,188 iteration 1778 : loss : 0.039150, loss_ce: 0.016085
2022-01-08 10:19:44,776 iteration 1779 : loss : 0.050147, loss_ce: 0.020236
2022-01-08 10:19:46,388 iteration 1780 : loss : 0.059056, loss_ce: 0.015603
2022-01-08 10:19:48,001 iteration 1781 : loss : 0.048049, loss_ce: 0.015507
2022-01-08 10:19:49,510 iteration 1782 : loss : 0.064935, loss_ce: 0.017438
2022-01-08 10:19:51,053 iteration 1783 : loss : 0.037448, loss_ce: 0.017356
2022-01-08 10:19:52,667 iteration 1784 : loss : 0.037439, loss_ce: 0.018444
2022-01-08 10:19:52,667 Training Data Eval:
2022-01-08 10:20:00,561   Average segmentation loss on training set: 0.0346
2022-01-08 10:20:00,562 Validation Data Eval:
2022-01-08 10:20:03,284   Average segmentation loss on validation set: 0.1245
2022-01-08 10:20:04,864 iteration 1785 : loss : 0.038528, loss_ce: 0.020746
 26%|███████▌                     | 105/400 [51:55<2:31:46, 30.87s/it]2022-01-08 10:20:06,539 iteration 1786 : loss : 0.042441, loss_ce: 0.014358
2022-01-08 10:20:08,064 iteration 1787 : loss : 0.036908, loss_ce: 0.009502
2022-01-08 10:20:09,658 iteration 1788 : loss : 0.036397, loss_ce: 0.013981
2022-01-08 10:20:11,256 iteration 1789 : loss : 0.048479, loss_ce: 0.027767
2022-01-08 10:20:12,994 iteration 1790 : loss : 0.032801, loss_ce: 0.009942
2022-01-08 10:20:14,602 iteration 1791 : loss : 0.041333, loss_ce: 0.015556
2022-01-08 10:20:16,214 iteration 1792 : loss : 0.040921, loss_ce: 0.016247
2022-01-08 10:20:17,837 iteration 1793 : loss : 0.070628, loss_ce: 0.032410
2022-01-08 10:20:19,412 iteration 1794 : loss : 0.045159, loss_ce: 0.025099
2022-01-08 10:20:20,892 iteration 1795 : loss : 0.030919, loss_ce: 0.011746
2022-01-08 10:20:22,554 iteration 1796 : loss : 0.046871, loss_ce: 0.020417
2022-01-08 10:20:24,089 iteration 1797 : loss : 0.041825, loss_ce: 0.019336
2022-01-08 10:20:25,680 iteration 1798 : loss : 0.044710, loss_ce: 0.017026
2022-01-08 10:20:27,334 iteration 1799 : loss : 0.036679, loss_ce: 0.015064
2022-01-08 10:20:28,857 iteration 1800 : loss : 0.034008, loss_ce: 0.011254
2022-01-08 10:20:30,443 iteration 1801 : loss : 0.068376, loss_ce: 0.025543
2022-01-08 10:20:32,077 iteration 1802 : loss : 0.045544, loss_ce: 0.015819
 26%|███████▋                     | 106/400 [52:22<2:25:52, 29.77s/it]2022-01-08 10:20:33,754 iteration 1803 : loss : 0.042064, loss_ce: 0.016433
2022-01-08 10:20:35,272 iteration 1804 : loss : 0.046719, loss_ce: 0.015857
2022-01-08 10:20:36,989 iteration 1805 : loss : 0.049304, loss_ce: 0.017600
2022-01-08 10:20:38,668 iteration 1806 : loss : 0.052886, loss_ce: 0.025271
2022-01-08 10:20:40,253 iteration 1807 : loss : 0.038764, loss_ce: 0.016551
2022-01-08 10:20:41,887 iteration 1808 : loss : 0.035329, loss_ce: 0.017017
2022-01-08 10:20:43,455 iteration 1809 : loss : 0.036428, loss_ce: 0.017216
2022-01-08 10:20:45,075 iteration 1810 : loss : 0.059889, loss_ce: 0.032107
2022-01-08 10:20:46,644 iteration 1811 : loss : 0.044867, loss_ce: 0.015064
2022-01-08 10:20:48,203 iteration 1812 : loss : 0.040852, loss_ce: 0.013448
2022-01-08 10:20:49,790 iteration 1813 : loss : 0.042524, loss_ce: 0.019310
2022-01-08 10:20:51,350 iteration 1814 : loss : 0.050746, loss_ce: 0.020205
2022-01-08 10:20:53,017 iteration 1815 : loss : 0.046910, loss_ce: 0.015292
2022-01-08 10:20:54,622 iteration 1816 : loss : 0.035763, loss_ce: 0.014157
2022-01-08 10:20:56,290 iteration 1817 : loss : 0.051809, loss_ce: 0.019861
2022-01-08 10:20:57,818 iteration 1818 : loss : 0.030869, loss_ce: 0.009888
2022-01-08 10:20:59,458 iteration 1819 : loss : 0.056970, loss_ce: 0.023628
 27%|███████▊                     | 107/400 [52:49<2:21:53, 29.06s/it]2022-01-08 10:21:00,974 iteration 1820 : loss : 0.039026, loss_ce: 0.015678
2022-01-08 10:21:02,684 iteration 1821 : loss : 0.054789, loss_ce: 0.019035
2022-01-08 10:21:04,330 iteration 1822 : loss : 0.052053, loss_ce: 0.024905
2022-01-08 10:21:06,001 iteration 1823 : loss : 0.048356, loss_ce: 0.018074
2022-01-08 10:21:07,625 iteration 1824 : loss : 0.056731, loss_ce: 0.017392
2022-01-08 10:21:09,167 iteration 1825 : loss : 0.034373, loss_ce: 0.013882
2022-01-08 10:21:10,828 iteration 1826 : loss : 0.050167, loss_ce: 0.014330
2022-01-08 10:21:12,366 iteration 1827 : loss : 0.037613, loss_ce: 0.013964
2022-01-08 10:21:13,964 iteration 1828 : loss : 0.035602, loss_ce: 0.012535
2022-01-08 10:21:15,516 iteration 1829 : loss : 0.052873, loss_ce: 0.025942
2022-01-08 10:21:17,197 iteration 1830 : loss : 0.050364, loss_ce: 0.018883
2022-01-08 10:21:18,801 iteration 1831 : loss : 0.036256, loss_ce: 0.014090
2022-01-08 10:21:20,363 iteration 1832 : loss : 0.043591, loss_ce: 0.016628
2022-01-08 10:21:21,887 iteration 1833 : loss : 0.039170, loss_ce: 0.016563
2022-01-08 10:21:23,466 iteration 1834 : loss : 0.052047, loss_ce: 0.014612
2022-01-08 10:21:25,054 iteration 1835 : loss : 0.034504, loss_ce: 0.014291
2022-01-08 10:21:26,603 iteration 1836 : loss : 0.045789, loss_ce: 0.014958
 27%|███████▊                     | 108/400 [53:16<2:18:36, 28.48s/it]2022-01-08 10:21:28,278 iteration 1837 : loss : 0.031243, loss_ce: 0.010825
2022-01-08 10:21:29,783 iteration 1838 : loss : 0.038425, loss_ce: 0.016300
2022-01-08 10:21:31,323 iteration 1839 : loss : 0.047655, loss_ce: 0.017895
2022-01-08 10:21:32,927 iteration 1840 : loss : 0.038515, loss_ce: 0.018658
2022-01-08 10:21:34,445 iteration 1841 : loss : 0.050836, loss_ce: 0.024508
2022-01-08 10:21:36,015 iteration 1842 : loss : 0.028446, loss_ce: 0.009200
2022-01-08 10:21:37,622 iteration 1843 : loss : 0.030799, loss_ce: 0.014626
2022-01-08 10:21:39,190 iteration 1844 : loss : 0.036290, loss_ce: 0.010565
2022-01-08 10:21:40,747 iteration 1845 : loss : 0.041892, loss_ce: 0.009852
2022-01-08 10:21:42,408 iteration 1846 : loss : 0.039420, loss_ce: 0.017912
2022-01-08 10:21:43,957 iteration 1847 : loss : 0.036128, loss_ce: 0.012837
2022-01-08 10:21:45,527 iteration 1848 : loss : 0.040859, loss_ce: 0.016680
2022-01-08 10:21:47,076 iteration 1849 : loss : 0.049274, loss_ce: 0.020880
2022-01-08 10:21:48,799 iteration 1850 : loss : 0.045816, loss_ce: 0.016576
2022-01-08 10:21:50,388 iteration 1851 : loss : 0.068211, loss_ce: 0.021632
2022-01-08 10:21:51,992 iteration 1852 : loss : 0.033978, loss_ce: 0.014501
2022-01-08 10:21:53,544 iteration 1853 : loss : 0.044026, loss_ce: 0.018078
 27%|███████▉                     | 109/400 [53:43<2:15:54, 28.02s/it]2022-01-08 10:21:55,145 iteration 1854 : loss : 0.035509, loss_ce: 0.012476
2022-01-08 10:21:56,784 iteration 1855 : loss : 0.046958, loss_ce: 0.021654
2022-01-08 10:21:58,428 iteration 1856 : loss : 0.052808, loss_ce: 0.016940
2022-01-08 10:22:00,083 iteration 1857 : loss : 0.032555, loss_ce: 0.012184
2022-01-08 10:22:01,692 iteration 1858 : loss : 0.039579, loss_ce: 0.015459
2022-01-08 10:22:03,303 iteration 1859 : loss : 0.042860, loss_ce: 0.016449
2022-01-08 10:22:04,851 iteration 1860 : loss : 0.044174, loss_ce: 0.016861
2022-01-08 10:22:06,502 iteration 1861 : loss : 0.039571, loss_ce: 0.013685
2022-01-08 10:22:08,037 iteration 1862 : loss : 0.032431, loss_ce: 0.014193
2022-01-08 10:22:09,622 iteration 1863 : loss : 0.041148, loss_ce: 0.018272
2022-01-08 10:22:11,247 iteration 1864 : loss : 0.066822, loss_ce: 0.017573
2022-01-08 10:22:12,733 iteration 1865 : loss : 0.031190, loss_ce: 0.014550
2022-01-08 10:22:14,396 iteration 1866 : loss : 0.036526, loss_ce: 0.017342
2022-01-08 10:22:15,985 iteration 1867 : loss : 0.033055, loss_ce: 0.013269
2022-01-08 10:22:17,537 iteration 1868 : loss : 0.046016, loss_ce: 0.018075
2022-01-08 10:22:19,077 iteration 1869 : loss : 0.074403, loss_ce: 0.024712
2022-01-08 10:22:19,078 Training Data Eval:
2022-01-08 10:22:26,971   Average segmentation loss on training set: 0.0310
2022-01-08 10:22:26,972 Validation Data Eval:
2022-01-08 10:22:29,703   Average segmentation loss on validation set: 0.0669
2022-01-08 10:22:31,330 iteration 1870 : loss : 0.046820, loss_ce: 0.016729
 28%|███████▉                     | 110/400 [54:21<2:29:35, 30.95s/it]2022-01-08 10:22:33,019 iteration 1871 : loss : 0.032445, loss_ce: 0.011265
2022-01-08 10:22:34,723 iteration 1872 : loss : 0.070328, loss_ce: 0.020791
2022-01-08 10:22:36,256 iteration 1873 : loss : 0.042239, loss_ce: 0.017321
2022-01-08 10:22:37,920 iteration 1874 : loss : 0.043807, loss_ce: 0.021161
2022-01-08 10:22:39,415 iteration 1875 : loss : 0.039433, loss_ce: 0.013269
2022-01-08 10:22:41,084 iteration 1876 : loss : 0.047330, loss_ce: 0.016030
2022-01-08 10:22:42,723 iteration 1877 : loss : 0.056011, loss_ce: 0.023167
2022-01-08 10:22:44,283 iteration 1878 : loss : 0.037402, loss_ce: 0.016144
2022-01-08 10:22:45,830 iteration 1879 : loss : 0.037765, loss_ce: 0.013074
2022-01-08 10:22:47,371 iteration 1880 : loss : 0.039717, loss_ce: 0.017878
2022-01-08 10:22:48,952 iteration 1881 : loss : 0.045079, loss_ce: 0.016630
2022-01-08 10:22:50,485 iteration 1882 : loss : 0.046431, loss_ce: 0.014388
2022-01-08 10:22:51,980 iteration 1883 : loss : 0.039694, loss_ce: 0.013191
2022-01-08 10:22:53,600 iteration 1884 : loss : 0.034916, loss_ce: 0.013083
2022-01-08 10:22:55,087 iteration 1885 : loss : 0.032335, loss_ce: 0.012893
2022-01-08 10:22:56,754 iteration 1886 : loss : 0.035656, loss_ce: 0.014491
2022-01-08 10:22:58,416 iteration 1887 : loss : 0.054422, loss_ce: 0.025180
 28%|████████                     | 111/400 [54:48<2:23:29, 29.79s/it]2022-01-08 10:23:00,030 iteration 1888 : loss : 0.037488, loss_ce: 0.015985
2022-01-08 10:23:01,679 iteration 1889 : loss : 0.032657, loss_ce: 0.013158
2022-01-08 10:23:03,308 iteration 1890 : loss : 0.051255, loss_ce: 0.019380
2022-01-08 10:23:04,948 iteration 1891 : loss : 0.046329, loss_ce: 0.017464
2022-01-08 10:23:06,550 iteration 1892 : loss : 0.033301, loss_ce: 0.014196
2022-01-08 10:23:08,158 iteration 1893 : loss : 0.043608, loss_ce: 0.021907
2022-01-08 10:23:09,739 iteration 1894 : loss : 0.035202, loss_ce: 0.016195
2022-01-08 10:23:11,312 iteration 1895 : loss : 0.035393, loss_ce: 0.012712
2022-01-08 10:23:13,040 iteration 1896 : loss : 0.058360, loss_ce: 0.027306
2022-01-08 10:23:14,615 iteration 1897 : loss : 0.044926, loss_ce: 0.013339
2022-01-08 10:23:16,268 iteration 1898 : loss : 0.037366, loss_ce: 0.012392
2022-01-08 10:23:17,851 iteration 1899 : loss : 0.039217, loss_ce: 0.019748
2022-01-08 10:23:19,353 iteration 1900 : loss : 0.031407, loss_ce: 0.013165
2022-01-08 10:23:20,936 iteration 1901 : loss : 0.034086, loss_ce: 0.013196
2022-01-08 10:23:22,547 iteration 1902 : loss : 0.029387, loss_ce: 0.012097
2022-01-08 10:23:24,189 iteration 1903 : loss : 0.035367, loss_ce: 0.011884
2022-01-08 10:23:25,891 iteration 1904 : loss : 0.053514, loss_ce: 0.018221
 28%|████████                     | 112/400 [55:16<2:19:39, 29.09s/it]2022-01-08 10:23:27,465 iteration 1905 : loss : 0.030430, loss_ce: 0.011988
2022-01-08 10:23:29,120 iteration 1906 : loss : 0.032675, loss_ce: 0.010701
2022-01-08 10:23:30,831 iteration 1907 : loss : 0.033838, loss_ce: 0.012991
2022-01-08 10:23:32,463 iteration 1908 : loss : 0.033882, loss_ce: 0.014206
2022-01-08 10:23:34,125 iteration 1909 : loss : 0.039929, loss_ce: 0.019258
2022-01-08 10:23:35,773 iteration 1910 : loss : 0.027980, loss_ce: 0.012766
2022-01-08 10:23:37,498 iteration 1911 : loss : 0.025184, loss_ce: 0.009509
2022-01-08 10:23:39,140 iteration 1912 : loss : 0.031970, loss_ce: 0.009143
2022-01-08 10:23:40,723 iteration 1913 : loss : 0.032255, loss_ce: 0.011575
2022-01-08 10:23:42,270 iteration 1914 : loss : 0.028783, loss_ce: 0.012790
2022-01-08 10:23:43,869 iteration 1915 : loss : 0.050862, loss_ce: 0.017577
2022-01-08 10:23:45,460 iteration 1916 : loss : 0.035005, loss_ce: 0.012271
2022-01-08 10:23:47,080 iteration 1917 : loss : 0.041831, loss_ce: 0.015436
2022-01-08 10:23:48,625 iteration 1918 : loss : 0.039333, loss_ce: 0.015690
2022-01-08 10:23:50,195 iteration 1919 : loss : 0.030110, loss_ce: 0.014155
2022-01-08 10:23:51,771 iteration 1920 : loss : 0.030734, loss_ce: 0.011032
2022-01-08 10:23:53,402 iteration 1921 : loss : 0.038426, loss_ce: 0.013040
 28%|████████▏                    | 113/400 [55:43<2:16:53, 28.62s/it]2022-01-08 10:23:54,978 iteration 1922 : loss : 0.039970, loss_ce: 0.015081
2022-01-08 10:23:56,484 iteration 1923 : loss : 0.023109, loss_ce: 0.009651
2022-01-08 10:23:58,096 iteration 1924 : loss : 0.055870, loss_ce: 0.029149
2022-01-08 10:23:59,629 iteration 1925 : loss : 0.040605, loss_ce: 0.017334
2022-01-08 10:24:01,181 iteration 1926 : loss : 0.031425, loss_ce: 0.013188
2022-01-08 10:24:02,814 iteration 1927 : loss : 0.049459, loss_ce: 0.019739
2022-01-08 10:24:04,454 iteration 1928 : loss : 0.025556, loss_ce: 0.009940
2022-01-08 10:24:06,013 iteration 1929 : loss : 0.027877, loss_ce: 0.011708
2022-01-08 10:24:07,598 iteration 1930 : loss : 0.046634, loss_ce: 0.015588
2022-01-08 10:24:09,152 iteration 1931 : loss : 0.055137, loss_ce: 0.016130
2022-01-08 10:24:10,732 iteration 1932 : loss : 0.039247, loss_ce: 0.011446
2022-01-08 10:24:12,321 iteration 1933 : loss : 0.051977, loss_ce: 0.017293
2022-01-08 10:24:13,774 iteration 1934 : loss : 0.025259, loss_ce: 0.011366
2022-01-08 10:24:15,321 iteration 1935 : loss : 0.043212, loss_ce: 0.019422
2022-01-08 10:24:16,952 iteration 1936 : loss : 0.029292, loss_ce: 0.010757
2022-01-08 10:24:18,570 iteration 1937 : loss : 0.031156, loss_ce: 0.012985
2022-01-08 10:24:20,194 iteration 1938 : loss : 0.032026, loss_ce: 0.009831
 28%|████████▎                    | 114/400 [56:10<2:13:49, 28.07s/it]2022-01-08 10:24:21,754 iteration 1939 : loss : 0.031639, loss_ce: 0.014888
2022-01-08 10:24:23,309 iteration 1940 : loss : 0.023374, loss_ce: 0.008651
2022-01-08 10:24:24,949 iteration 1941 : loss : 0.039333, loss_ce: 0.020044
2022-01-08 10:24:26,525 iteration 1942 : loss : 0.049998, loss_ce: 0.016079
2022-01-08 10:24:28,024 iteration 1943 : loss : 0.036160, loss_ce: 0.011763
2022-01-08 10:24:29,571 iteration 1944 : loss : 0.030853, loss_ce: 0.011655
2022-01-08 10:24:31,133 iteration 1945 : loss : 0.032700, loss_ce: 0.016313
2022-01-08 10:24:32,699 iteration 1946 : loss : 0.042819, loss_ce: 0.017549
2022-01-08 10:24:34,253 iteration 1947 : loss : 0.041108, loss_ce: 0.015562
2022-01-08 10:24:35,801 iteration 1948 : loss : 0.033859, loss_ce: 0.014183
2022-01-08 10:24:37,298 iteration 1949 : loss : 0.030894, loss_ce: 0.016254
2022-01-08 10:24:38,916 iteration 1950 : loss : 0.038465, loss_ce: 0.015371
2022-01-08 10:24:40,610 iteration 1951 : loss : 0.050474, loss_ce: 0.021589
2022-01-08 10:24:42,133 iteration 1952 : loss : 0.037934, loss_ce: 0.010842
2022-01-08 10:24:43,734 iteration 1953 : loss : 0.032812, loss_ce: 0.016582
2022-01-08 10:24:45,398 iteration 1954 : loss : 0.051145, loss_ce: 0.019354
2022-01-08 10:24:45,398 Training Data Eval:
2022-01-08 10:24:53,297   Average segmentation loss on training set: 0.0258
2022-01-08 10:24:53,297 Validation Data Eval:
2022-01-08 10:24:56,025   Average segmentation loss on validation set: 0.0695
2022-01-08 10:24:57,584 iteration 1955 : loss : 0.029804, loss_ce: 0.011166
 29%|████████▎                    | 115/400 [56:47<2:26:37, 30.87s/it]2022-01-08 10:24:59,259 iteration 1956 : loss : 0.045191, loss_ce: 0.018672
2022-01-08 10:25:00,780 iteration 1957 : loss : 0.028863, loss_ce: 0.012066
2022-01-08 10:25:02,334 iteration 1958 : loss : 0.038407, loss_ce: 0.017419
2022-01-08 10:25:04,005 iteration 1959 : loss : 0.054904, loss_ce: 0.014625
2022-01-08 10:25:05,563 iteration 1960 : loss : 0.044410, loss_ce: 0.014236
2022-01-08 10:25:07,156 iteration 1961 : loss : 0.039450, loss_ce: 0.018096
2022-01-08 10:25:08,724 iteration 1962 : loss : 0.025619, loss_ce: 0.009941
2022-01-08 10:25:10,325 iteration 1963 : loss : 0.037568, loss_ce: 0.017959
2022-01-08 10:25:11,885 iteration 1964 : loss : 0.035112, loss_ce: 0.009593
2022-01-08 10:25:13,423 iteration 1965 : loss : 0.039289, loss_ce: 0.018055
2022-01-08 10:25:14,955 iteration 1966 : loss : 0.033097, loss_ce: 0.015654
2022-01-08 10:25:16,473 iteration 1967 : loss : 0.035812, loss_ce: 0.012866
2022-01-08 10:25:17,975 iteration 1968 : loss : 0.036225, loss_ce: 0.019578
2022-01-08 10:25:19,581 iteration 1969 : loss : 0.032016, loss_ce: 0.012760
2022-01-08 10:25:21,308 iteration 1970 : loss : 0.039462, loss_ce: 0.013533
2022-01-08 10:25:22,791 iteration 1971 : loss : 0.030932, loss_ce: 0.009717
2022-01-08 10:25:24,494 iteration 1972 : loss : 0.041764, loss_ce: 0.014592
 29%|████████▍                    | 116/400 [57:14<2:20:29, 29.68s/it]2022-01-08 10:25:26,025 iteration 1973 : loss : 0.029363, loss_ce: 0.010916
2022-01-08 10:25:27,531 iteration 1974 : loss : 0.041045, loss_ce: 0.016677
2022-01-08 10:25:29,192 iteration 1975 : loss : 0.038850, loss_ce: 0.013910
2022-01-08 10:25:30,685 iteration 1976 : loss : 0.037239, loss_ce: 0.015086
2022-01-08 10:25:32,354 iteration 1977 : loss : 0.042380, loss_ce: 0.018055
2022-01-08 10:25:33,894 iteration 1978 : loss : 0.044938, loss_ce: 0.019579
2022-01-08 10:25:35,449 iteration 1979 : loss : 0.039515, loss_ce: 0.019818
2022-01-08 10:25:37,072 iteration 1980 : loss : 0.047524, loss_ce: 0.017920
2022-01-08 10:25:38,606 iteration 1981 : loss : 0.033089, loss_ce: 0.013270
2022-01-08 10:25:40,251 iteration 1982 : loss : 0.027418, loss_ce: 0.009163
2022-01-08 10:25:41,929 iteration 1983 : loss : 0.039194, loss_ce: 0.016082
2022-01-08 10:25:43,471 iteration 1984 : loss : 0.030011, loss_ce: 0.009737
2022-01-08 10:25:45,021 iteration 1985 : loss : 0.032890, loss_ce: 0.011567
2022-01-08 10:25:46,555 iteration 1986 : loss : 0.028980, loss_ce: 0.009907
2022-01-08 10:25:48,085 iteration 1987 : loss : 0.022562, loss_ce: 0.009134
2022-01-08 10:25:49,758 iteration 1988 : loss : 0.036336, loss_ce: 0.013290
2022-01-08 10:25:51,339 iteration 1989 : loss : 0.029760, loss_ce: 0.012806
 29%|████████▍                    | 117/400 [57:41<2:15:57, 28.83s/it]2022-01-08 10:25:52,954 iteration 1990 : loss : 0.040917, loss_ce: 0.015029
2022-01-08 10:25:54,441 iteration 1991 : loss : 0.028060, loss_ce: 0.012420
2022-01-08 10:25:56,056 iteration 1992 : loss : 0.029100, loss_ce: 0.011590
2022-01-08 10:25:57,620 iteration 1993 : loss : 0.047204, loss_ce: 0.011408
2022-01-08 10:25:59,234 iteration 1994 : loss : 0.029635, loss_ce: 0.011469
2022-01-08 10:26:00,814 iteration 1995 : loss : 0.044319, loss_ce: 0.016753
2022-01-08 10:26:02,408 iteration 1996 : loss : 0.035871, loss_ce: 0.015080
2022-01-08 10:26:03,997 iteration 1997 : loss : 0.034977, loss_ce: 0.013432
2022-01-08 10:26:05,616 iteration 1998 : loss : 0.040353, loss_ce: 0.018624
2022-01-08 10:26:07,224 iteration 1999 : loss : 0.048793, loss_ce: 0.020922
2022-01-08 10:26:08,949 iteration 2000 : loss : 0.062212, loss_ce: 0.016002
2022-01-08 10:26:10,545 iteration 2001 : loss : 0.041910, loss_ce: 0.014418
2022-01-08 10:26:12,052 iteration 2002 : loss : 0.033480, loss_ce: 0.013687
2022-01-08 10:26:13,495 iteration 2003 : loss : 0.022339, loss_ce: 0.010650
2022-01-08 10:26:15,025 iteration 2004 : loss : 0.038869, loss_ce: 0.011492
2022-01-08 10:26:16,644 iteration 2005 : loss : 0.029869, loss_ce: 0.010993
2022-01-08 10:26:18,253 iteration 2006 : loss : 0.035422, loss_ce: 0.011802
 30%|████████▌                    | 118/400 [58:08<2:12:47, 28.25s/it]2022-01-08 10:26:19,938 iteration 2007 : loss : 0.031574, loss_ce: 0.011592
2022-01-08 10:26:21,655 iteration 2008 : loss : 0.027112, loss_ce: 0.008385
2022-01-08 10:26:23,243 iteration 2009 : loss : 0.031938, loss_ce: 0.011774
2022-01-08 10:26:24,821 iteration 2010 : loss : 0.045603, loss_ce: 0.019415
2022-01-08 10:26:26,500 iteration 2011 : loss : 0.043758, loss_ce: 0.015212
2022-01-08 10:26:28,098 iteration 2012 : loss : 0.039050, loss_ce: 0.020393
2022-01-08 10:26:29,604 iteration 2013 : loss : 0.034305, loss_ce: 0.013141
2022-01-08 10:26:31,158 iteration 2014 : loss : 0.044896, loss_ce: 0.016901
2022-01-08 10:26:32,781 iteration 2015 : loss : 0.028284, loss_ce: 0.011524
2022-01-08 10:26:34,351 iteration 2016 : loss : 0.051454, loss_ce: 0.017236
2022-01-08 10:26:35,987 iteration 2017 : loss : 0.026674, loss_ce: 0.007690
2022-01-08 10:26:37,687 iteration 2018 : loss : 0.036507, loss_ce: 0.015880
2022-01-08 10:26:39,256 iteration 2019 : loss : 0.046225, loss_ce: 0.014812
2022-01-08 10:26:40,805 iteration 2020 : loss : 0.023191, loss_ce: 0.007690
2022-01-08 10:26:42,417 iteration 2021 : loss : 0.036170, loss_ce: 0.012680
2022-01-08 10:26:44,031 iteration 2022 : loss : 0.037519, loss_ce: 0.016050
2022-01-08 10:26:45,690 iteration 2023 : loss : 0.039357, loss_ce: 0.014668
 30%|████████▋                    | 119/400 [58:35<2:11:10, 28.01s/it]2022-01-08 10:26:47,262 iteration 2024 : loss : 0.043869, loss_ce: 0.014981
2022-01-08 10:26:48,865 iteration 2025 : loss : 0.032406, loss_ce: 0.015233
2022-01-08 10:26:50,510 iteration 2026 : loss : 0.039613, loss_ce: 0.015980
2022-01-08 10:26:52,078 iteration 2027 : loss : 0.033286, loss_ce: 0.009246
2022-01-08 10:26:53,604 iteration 2028 : loss : 0.029205, loss_ce: 0.013617
2022-01-08 10:26:55,162 iteration 2029 : loss : 0.030244, loss_ce: 0.011980
2022-01-08 10:26:56,705 iteration 2030 : loss : 0.042614, loss_ce: 0.015345
2022-01-08 10:26:58,285 iteration 2031 : loss : 0.033806, loss_ce: 0.013154
2022-01-08 10:26:59,965 iteration 2032 : loss : 0.052625, loss_ce: 0.021682
2022-01-08 10:27:01,676 iteration 2033 : loss : 0.053343, loss_ce: 0.019344
2022-01-08 10:27:03,230 iteration 2034 : loss : 0.035225, loss_ce: 0.014272
2022-01-08 10:27:04,822 iteration 2035 : loss : 0.033066, loss_ce: 0.013281
2022-01-08 10:27:06,462 iteration 2036 : loss : 0.034250, loss_ce: 0.017157
2022-01-08 10:27:08,047 iteration 2037 : loss : 0.077430, loss_ce: 0.020621
2022-01-08 10:27:09,684 iteration 2038 : loss : 0.053803, loss_ce: 0.018911
2022-01-08 10:27:11,373 iteration 2039 : loss : 0.045908, loss_ce: 0.015284
2022-01-08 10:27:11,373 Training Data Eval:
2022-01-08 10:27:19,265   Average segmentation loss on training set: 0.0295
2022-01-08 10:27:19,265 Validation Data Eval:
2022-01-08 10:27:21,985   Average segmentation loss on validation set: 0.0911
2022-01-08 10:27:23,649 iteration 2040 : loss : 0.057132, loss_ce: 0.023047
 30%|████████▋                    | 120/400 [59:13<2:24:39, 31.00s/it]2022-01-08 10:27:25,329 iteration 2041 : loss : 0.044504, loss_ce: 0.011475
2022-01-08 10:27:26,889 iteration 2042 : loss : 0.046568, loss_ce: 0.018420
2022-01-08 10:27:28,447 iteration 2043 : loss : 0.033583, loss_ce: 0.011602
2022-01-08 10:27:30,025 iteration 2044 : loss : 0.062130, loss_ce: 0.025683
2022-01-08 10:27:31,572 iteration 2045 : loss : 0.038071, loss_ce: 0.012370
2022-01-08 10:27:33,166 iteration 2046 : loss : 0.039704, loss_ce: 0.016106
2022-01-08 10:27:34,723 iteration 2047 : loss : 0.038174, loss_ce: 0.016526
2022-01-08 10:27:36,315 iteration 2048 : loss : 0.049645, loss_ce: 0.018185
2022-01-08 10:27:37,871 iteration 2049 : loss : 0.038784, loss_ce: 0.010049
2022-01-08 10:27:39,475 iteration 2050 : loss : 0.047396, loss_ce: 0.019338
2022-01-08 10:27:41,075 iteration 2051 : loss : 0.040630, loss_ce: 0.019097
2022-01-08 10:27:42,596 iteration 2052 : loss : 0.047460, loss_ce: 0.017940
2022-01-08 10:27:44,189 iteration 2053 : loss : 0.056737, loss_ce: 0.017250
2022-01-08 10:27:45,745 iteration 2054 : loss : 0.037264, loss_ce: 0.015561
2022-01-08 10:27:47,316 iteration 2055 : loss : 0.040949, loss_ce: 0.012995
2022-01-08 10:27:48,946 iteration 2056 : loss : 0.044647, loss_ce: 0.021128
2022-01-08 10:27:50,478 iteration 2057 : loss : 0.041474, loss_ce: 0.016597
 30%|████████▊                    | 121/400 [59:40<2:18:18, 29.74s/it]2022-01-08 10:27:52,105 iteration 2058 : loss : 0.038902, loss_ce: 0.018452
2022-01-08 10:27:53,752 iteration 2059 : loss : 0.051598, loss_ce: 0.019537
2022-01-08 10:27:55,243 iteration 2060 : loss : 0.039474, loss_ce: 0.014244
2022-01-08 10:27:56,857 iteration 2061 : loss : 0.071218, loss_ce: 0.023287
2022-01-08 10:27:58,418 iteration 2062 : loss : 0.043345, loss_ce: 0.017431
2022-01-08 10:27:59,986 iteration 2063 : loss : 0.094090, loss_ce: 0.019441
2022-01-08 10:28:01,508 iteration 2064 : loss : 0.029572, loss_ce: 0.010899
2022-01-08 10:28:03,064 iteration 2065 : loss : 0.035868, loss_ce: 0.010924
2022-01-08 10:28:04,648 iteration 2066 : loss : 0.039827, loss_ce: 0.012949
2022-01-08 10:28:06,260 iteration 2067 : loss : 0.032780, loss_ce: 0.012325
2022-01-08 10:28:07,912 iteration 2068 : loss : 0.034921, loss_ce: 0.015485
2022-01-08 10:28:09,470 iteration 2069 : loss : 0.032913, loss_ce: 0.009415
2022-01-08 10:28:11,103 iteration 2070 : loss : 0.040944, loss_ce: 0.016988
2022-01-08 10:28:12,758 iteration 2071 : loss : 0.047188, loss_ce: 0.017876
2022-01-08 10:28:14,357 iteration 2072 : loss : 0.038677, loss_ce: 0.014331
2022-01-08 10:28:15,999 iteration 2073 : loss : 0.067984, loss_ce: 0.018743
2022-01-08 10:28:17,625 iteration 2074 : loss : 0.033165, loss_ce: 0.016646
 30%|████████▏                  | 122/400 [1:00:07<2:14:12, 28.97s/it]2022-01-08 10:28:19,221 iteration 2075 : loss : 0.026034, loss_ce: 0.008510
2022-01-08 10:28:20,820 iteration 2076 : loss : 0.051182, loss_ce: 0.015949
2022-01-08 10:28:22,440 iteration 2077 : loss : 0.036580, loss_ce: 0.016656
2022-01-08 10:28:24,181 iteration 2078 : loss : 0.043452, loss_ce: 0.015149
2022-01-08 10:28:25,747 iteration 2079 : loss : 0.039699, loss_ce: 0.011801
2022-01-08 10:28:27,399 iteration 2080 : loss : 0.047400, loss_ce: 0.016707
2022-01-08 10:28:28,887 iteration 2081 : loss : 0.036525, loss_ce: 0.012997
2022-01-08 10:28:30,359 iteration 2082 : loss : 0.024791, loss_ce: 0.009054
2022-01-08 10:28:31,924 iteration 2083 : loss : 0.052131, loss_ce: 0.032583
2022-01-08 10:28:33,488 iteration 2084 : loss : 0.069154, loss_ce: 0.037554
2022-01-08 10:28:35,016 iteration 2085 : loss : 0.045227, loss_ce: 0.015763
2022-01-08 10:28:36,588 iteration 2086 : loss : 0.051786, loss_ce: 0.019752
2022-01-08 10:28:38,124 iteration 2087 : loss : 0.039494, loss_ce: 0.014998
2022-01-08 10:28:39,775 iteration 2088 : loss : 0.030352, loss_ce: 0.012701
2022-01-08 10:28:41,406 iteration 2089 : loss : 0.036394, loss_ce: 0.016947
2022-01-08 10:28:42,915 iteration 2090 : loss : 0.032816, loss_ce: 0.014988
2022-01-08 10:28:44,538 iteration 2091 : loss : 0.038476, loss_ce: 0.016327
 31%|████████▎                  | 123/400 [1:00:34<2:10:52, 28.35s/it]2022-01-08 10:28:46,086 iteration 2092 : loss : 0.026005, loss_ce: 0.011889
2022-01-08 10:28:47,643 iteration 2093 : loss : 0.047395, loss_ce: 0.017713
2022-01-08 10:28:49,226 iteration 2094 : loss : 0.051501, loss_ce: 0.013544
2022-01-08 10:28:50,743 iteration 2095 : loss : 0.026335, loss_ce: 0.011444
2022-01-08 10:28:52,414 iteration 2096 : loss : 0.036916, loss_ce: 0.018620
2022-01-08 10:28:54,021 iteration 2097 : loss : 0.076011, loss_ce: 0.028352
2022-01-08 10:28:55,611 iteration 2098 : loss : 0.045983, loss_ce: 0.016647
2022-01-08 10:28:57,214 iteration 2099 : loss : 0.028700, loss_ce: 0.009033
2022-01-08 10:28:58,844 iteration 2100 : loss : 0.042835, loss_ce: 0.022167
2022-01-08 10:29:00,454 iteration 2101 : loss : 0.037582, loss_ce: 0.012893
2022-01-08 10:29:02,091 iteration 2102 : loss : 0.044825, loss_ce: 0.021338
2022-01-08 10:29:03,586 iteration 2103 : loss : 0.038737, loss_ce: 0.017814
2022-01-08 10:29:05,103 iteration 2104 : loss : 0.062201, loss_ce: 0.019670
2022-01-08 10:29:06,787 iteration 2105 : loss : 0.063006, loss_ce: 0.020129
2022-01-08 10:29:08,335 iteration 2106 : loss : 0.027826, loss_ce: 0.012457
2022-01-08 10:29:10,009 iteration 2107 : loss : 0.031420, loss_ce: 0.012960
2022-01-08 10:29:11,615 iteration 2108 : loss : 0.070052, loss_ce: 0.030668
 31%|████████▎                  | 124/400 [1:01:01<2:08:39, 27.97s/it]2022-01-08 10:29:13,238 iteration 2109 : loss : 0.036581, loss_ce: 0.013838
2022-01-08 10:29:14,866 iteration 2110 : loss : 0.034334, loss_ce: 0.011727
2022-01-08 10:29:16,550 iteration 2111 : loss : 0.054072, loss_ce: 0.014336
2022-01-08 10:29:18,154 iteration 2112 : loss : 0.043254, loss_ce: 0.018293
2022-01-08 10:29:19,703 iteration 2113 : loss : 0.043399, loss_ce: 0.023293
2022-01-08 10:29:21,326 iteration 2114 : loss : 0.036827, loss_ce: 0.017795
2022-01-08 10:29:22,945 iteration 2115 : loss : 0.036243, loss_ce: 0.011028
2022-01-08 10:29:24,556 iteration 2116 : loss : 0.032190, loss_ce: 0.010725
2022-01-08 10:29:26,157 iteration 2117 : loss : 0.034264, loss_ce: 0.013588
2022-01-08 10:29:27,795 iteration 2118 : loss : 0.052108, loss_ce: 0.018552
2022-01-08 10:29:29,438 iteration 2119 : loss : 0.047473, loss_ce: 0.017826
2022-01-08 10:29:31,037 iteration 2120 : loss : 0.038218, loss_ce: 0.011734
2022-01-08 10:29:32,521 iteration 2121 : loss : 0.073743, loss_ce: 0.033720
2022-01-08 10:29:34,086 iteration 2122 : loss : 0.031104, loss_ce: 0.010680
2022-01-08 10:29:35,735 iteration 2123 : loss : 0.044476, loss_ce: 0.019081
2022-01-08 10:29:37,411 iteration 2124 : loss : 0.036626, loss_ce: 0.014915
2022-01-08 10:29:37,412 Training Data Eval:
2022-01-08 10:29:45,316   Average segmentation loss on training set: 0.0585
2022-01-08 10:29:45,316 Validation Data Eval:
2022-01-08 10:29:48,043   Average segmentation loss on validation set: 0.1056
2022-01-08 10:29:49,648 iteration 2125 : loss : 0.043960, loss_ce: 0.027122
 31%|████████▍                  | 125/400 [1:01:39<2:22:02, 30.99s/it]2022-01-08 10:29:51,334 iteration 2126 : loss : 0.029560, loss_ce: 0.011468
2022-01-08 10:29:52,952 iteration 2127 : loss : 0.070547, loss_ce: 0.036492
2022-01-08 10:29:54,566 iteration 2128 : loss : 0.053197, loss_ce: 0.019034
2022-01-08 10:29:56,117 iteration 2129 : loss : 0.042674, loss_ce: 0.016268
2022-01-08 10:29:57,749 iteration 2130 : loss : 0.065217, loss_ce: 0.025402
2022-01-08 10:29:59,317 iteration 2131 : loss : 0.058833, loss_ce: 0.021481
2022-01-08 10:30:00,901 iteration 2132 : loss : 0.039518, loss_ce: 0.016349
2022-01-08 10:30:02,567 iteration 2133 : loss : 0.048485, loss_ce: 0.015434
2022-01-08 10:30:04,081 iteration 2134 : loss : 0.039858, loss_ce: 0.019259
2022-01-08 10:30:05,666 iteration 2135 : loss : 0.064288, loss_ce: 0.021367
2022-01-08 10:30:07,238 iteration 2136 : loss : 0.036944, loss_ce: 0.015477
2022-01-08 10:30:08,824 iteration 2137 : loss : 0.053417, loss_ce: 0.020923
2022-01-08 10:30:10,367 iteration 2138 : loss : 0.037639, loss_ce: 0.015395
2022-01-08 10:30:11,996 iteration 2139 : loss : 0.036226, loss_ce: 0.017205
2022-01-08 10:30:13,581 iteration 2140 : loss : 0.043177, loss_ce: 0.021130
2022-01-08 10:30:15,254 iteration 2141 : loss : 0.078351, loss_ce: 0.027161
2022-01-08 10:30:16,799 iteration 2142 : loss : 0.038183, loss_ce: 0.013304
 32%|████████▌                  | 126/400 [1:02:07<2:16:15, 29.84s/it]2022-01-08 10:30:18,349 iteration 2143 : loss : 0.040700, loss_ce: 0.016860
2022-01-08 10:30:19,956 iteration 2144 : loss : 0.033065, loss_ce: 0.011037
2022-01-08 10:30:21,577 iteration 2145 : loss : 0.037179, loss_ce: 0.011401
2022-01-08 10:30:23,104 iteration 2146 : loss : 0.021803, loss_ce: 0.009249
2022-01-08 10:30:24,704 iteration 2147 : loss : 0.037098, loss_ce: 0.012443
2022-01-08 10:30:26,263 iteration 2148 : loss : 0.045775, loss_ce: 0.016007
2022-01-08 10:30:27,850 iteration 2149 : loss : 0.038556, loss_ce: 0.014282
2022-01-08 10:30:29,406 iteration 2150 : loss : 0.024750, loss_ce: 0.009789
2022-01-08 10:30:31,060 iteration 2151 : loss : 0.039789, loss_ce: 0.015837
2022-01-08 10:30:32,599 iteration 2152 : loss : 0.029505, loss_ce: 0.011339
2022-01-08 10:30:34,129 iteration 2153 : loss : 0.027160, loss_ce: 0.010573
2022-01-08 10:30:35,863 iteration 2154 : loss : 0.055119, loss_ce: 0.017381
2022-01-08 10:30:37,452 iteration 2155 : loss : 0.044716, loss_ce: 0.015214
2022-01-08 10:30:38,981 iteration 2156 : loss : 0.033842, loss_ce: 0.013024
2022-01-08 10:30:40,720 iteration 2157 : loss : 0.047296, loss_ce: 0.015781
2022-01-08 10:30:42,414 iteration 2158 : loss : 0.046757, loss_ce: 0.022332
2022-01-08 10:30:44,034 iteration 2159 : loss : 0.053115, loss_ce: 0.017256
 32%|████████▌                  | 127/400 [1:02:34<2:12:11, 29.05s/it]2022-01-08 10:30:45,617 iteration 2160 : loss : 0.029618, loss_ce: 0.010928
2022-01-08 10:30:47,141 iteration 2161 : loss : 0.038175, loss_ce: 0.014518
2022-01-08 10:30:48,702 iteration 2162 : loss : 0.041817, loss_ce: 0.015672
2022-01-08 10:30:50,249 iteration 2163 : loss : 0.033695, loss_ce: 0.013263
2022-01-08 10:30:51,874 iteration 2164 : loss : 0.029812, loss_ce: 0.015907
2022-01-08 10:30:53,479 iteration 2165 : loss : 0.054331, loss_ce: 0.016360
2022-01-08 10:30:55,013 iteration 2166 : loss : 0.035010, loss_ce: 0.015430
2022-01-08 10:30:56,513 iteration 2167 : loss : 0.036447, loss_ce: 0.014324
2022-01-08 10:30:58,063 iteration 2168 : loss : 0.034521, loss_ce: 0.015440
2022-01-08 10:30:59,669 iteration 2169 : loss : 0.042615, loss_ce: 0.016044
2022-01-08 10:31:01,132 iteration 2170 : loss : 0.025714, loss_ce: 0.008805
2022-01-08 10:31:02,691 iteration 2171 : loss : 0.036546, loss_ce: 0.014682
2022-01-08 10:31:04,233 iteration 2172 : loss : 0.050093, loss_ce: 0.019599
2022-01-08 10:31:05,768 iteration 2173 : loss : 0.042000, loss_ce: 0.012266
2022-01-08 10:31:07,346 iteration 2174 : loss : 0.032343, loss_ce: 0.014046
2022-01-08 10:31:09,016 iteration 2175 : loss : 0.028170, loss_ce: 0.011802
2022-01-08 10:31:10,480 iteration 2176 : loss : 0.027611, loss_ce: 0.013217
 32%|████████▋                  | 128/400 [1:03:00<2:08:10, 28.27s/it]2022-01-08 10:31:12,024 iteration 2177 : loss : 0.049936, loss_ce: 0.027212
2022-01-08 10:31:13,602 iteration 2178 : loss : 0.033228, loss_ce: 0.010964
2022-01-08 10:31:15,265 iteration 2179 : loss : 0.037411, loss_ce: 0.014985
2022-01-08 10:31:16,896 iteration 2180 : loss : 0.040983, loss_ce: 0.016274
2022-01-08 10:31:18,403 iteration 2181 : loss : 0.027697, loss_ce: 0.010636
2022-01-08 10:31:19,983 iteration 2182 : loss : 0.037583, loss_ce: 0.013655
2022-01-08 10:31:21,581 iteration 2183 : loss : 0.059908, loss_ce: 0.017293
2022-01-08 10:31:23,105 iteration 2184 : loss : 0.034831, loss_ce: 0.017269
2022-01-08 10:31:24,632 iteration 2185 : loss : 0.031040, loss_ce: 0.010546
2022-01-08 10:31:26,297 iteration 2186 : loss : 0.032773, loss_ce: 0.012619
2022-01-08 10:31:27,976 iteration 2187 : loss : 0.032650, loss_ce: 0.009615
2022-01-08 10:31:29,731 iteration 2188 : loss : 0.052790, loss_ce: 0.025406
2022-01-08 10:31:31,466 iteration 2189 : loss : 0.052317, loss_ce: 0.016922
2022-01-08 10:31:33,067 iteration 2190 : loss : 0.043985, loss_ce: 0.018428
2022-01-08 10:31:34,680 iteration 2191 : loss : 0.027879, loss_ce: 0.009104
2022-01-08 10:31:36,229 iteration 2192 : loss : 0.034395, loss_ce: 0.013612
2022-01-08 10:31:37,832 iteration 2193 : loss : 0.030203, loss_ce: 0.010396
 32%|████████▋                  | 129/400 [1:03:28<2:06:27, 28.00s/it]2022-01-08 10:31:39,500 iteration 2194 : loss : 0.032115, loss_ce: 0.012945
2022-01-08 10:31:41,116 iteration 2195 : loss : 0.038625, loss_ce: 0.020585
2022-01-08 10:31:42,764 iteration 2196 : loss : 0.038457, loss_ce: 0.017019
2022-01-08 10:31:44,449 iteration 2197 : loss : 0.027049, loss_ce: 0.009639
2022-01-08 10:31:46,042 iteration 2198 : loss : 0.036866, loss_ce: 0.011640
2022-01-08 10:31:47,641 iteration 2199 : loss : 0.033040, loss_ce: 0.014412
2022-01-08 10:31:49,320 iteration 2200 : loss : 0.105300, loss_ce: 0.026091
2022-01-08 10:31:50,849 iteration 2201 : loss : 0.027399, loss_ce: 0.010404
2022-01-08 10:31:52,555 iteration 2202 : loss : 0.055569, loss_ce: 0.025314
2022-01-08 10:31:54,180 iteration 2203 : loss : 0.038671, loss_ce: 0.013411
2022-01-08 10:31:55,701 iteration 2204 : loss : 0.029653, loss_ce: 0.009039
2022-01-08 10:31:57,304 iteration 2205 : loss : 0.038007, loss_ce: 0.019605
2022-01-08 10:31:58,992 iteration 2206 : loss : 0.054278, loss_ce: 0.019211
2022-01-08 10:32:00,617 iteration 2207 : loss : 0.046275, loss_ce: 0.013515
2022-01-08 10:32:02,278 iteration 2208 : loss : 0.054062, loss_ce: 0.016652
2022-01-08 10:32:03,873 iteration 2209 : loss : 0.041788, loss_ce: 0.017297
2022-01-08 10:32:03,873 Training Data Eval:
2022-01-08 10:32:11,767   Average segmentation loss on training set: 0.0263
2022-01-08 10:32:11,768 Validation Data Eval:
2022-01-08 10:32:14,499   Average segmentation loss on validation set: 0.0895
2022-01-08 10:32:16,116 iteration 2210 : loss : 0.039001, loss_ce: 0.019831
 32%|████████▊                  | 130/400 [1:04:06<2:19:52, 31.08s/it]2022-01-08 10:32:17,744 iteration 2211 : loss : 0.060648, loss_ce: 0.024667
2022-01-08 10:32:19,277 iteration 2212 : loss : 0.035068, loss_ce: 0.016419
2022-01-08 10:32:20,864 iteration 2213 : loss : 0.048702, loss_ce: 0.011951
2022-01-08 10:32:22,364 iteration 2214 : loss : 0.035892, loss_ce: 0.013603
2022-01-08 10:32:23,859 iteration 2215 : loss : 0.023686, loss_ce: 0.009438
2022-01-08 10:32:25,404 iteration 2216 : loss : 0.032395, loss_ce: 0.014112
2022-01-08 10:32:26,923 iteration 2217 : loss : 0.039008, loss_ce: 0.018237
2022-01-08 10:32:28,476 iteration 2218 : loss : 0.030719, loss_ce: 0.011432
2022-01-08 10:32:30,085 iteration 2219 : loss : 0.030278, loss_ce: 0.013755
2022-01-08 10:32:31,755 iteration 2220 : loss : 0.046029, loss_ce: 0.023860
2022-01-08 10:32:33,416 iteration 2221 : loss : 0.042031, loss_ce: 0.013091
2022-01-08 10:32:34,911 iteration 2222 : loss : 0.039129, loss_ce: 0.015992
2022-01-08 10:32:36,434 iteration 2223 : loss : 0.038864, loss_ce: 0.014021
2022-01-08 10:32:38,110 iteration 2224 : loss : 0.041925, loss_ce: 0.017789
2022-01-08 10:32:39,716 iteration 2225 : loss : 0.035832, loss_ce: 0.012853
2022-01-08 10:32:41,379 iteration 2226 : loss : 0.042574, loss_ce: 0.018191
2022-01-08 10:32:43,017 iteration 2227 : loss : 0.052441, loss_ce: 0.017914
 33%|████████▊                  | 131/400 [1:04:33<2:13:43, 29.83s/it]2022-01-08 10:32:44,739 iteration 2228 : loss : 0.034406, loss_ce: 0.015184
2022-01-08 10:32:46,344 iteration 2229 : loss : 0.030509, loss_ce: 0.012384
2022-01-08 10:32:48,094 iteration 2230 : loss : 0.053369, loss_ce: 0.021557
2022-01-08 10:32:49,675 iteration 2231 : loss : 0.041562, loss_ce: 0.019822
2022-01-08 10:32:51,253 iteration 2232 : loss : 0.043403, loss_ce: 0.019339
2022-01-08 10:32:53,056 iteration 2233 : loss : 0.071295, loss_ce: 0.017512
2022-01-08 10:32:54,683 iteration 2234 : loss : 0.038077, loss_ce: 0.009742
2022-01-08 10:32:56,278 iteration 2235 : loss : 0.043039, loss_ce: 0.020786
2022-01-08 10:32:57,855 iteration 2236 : loss : 0.038276, loss_ce: 0.018328
2022-01-08 10:32:59,464 iteration 2237 : loss : 0.037342, loss_ce: 0.017862
2022-01-08 10:33:01,001 iteration 2238 : loss : 0.035577, loss_ce: 0.010885
2022-01-08 10:33:02,568 iteration 2239 : loss : 0.036075, loss_ce: 0.009878
2022-01-08 10:33:04,065 iteration 2240 : loss : 0.041975, loss_ce: 0.011155
2022-01-08 10:33:05,641 iteration 2241 : loss : 0.029727, loss_ce: 0.010886
2022-01-08 10:33:07,294 iteration 2242 : loss : 0.043977, loss_ce: 0.014596
2022-01-08 10:33:08,918 iteration 2243 : loss : 0.051082, loss_ce: 0.025515
2022-01-08 10:33:10,484 iteration 2244 : loss : 0.026638, loss_ce: 0.008921
 33%|████████▉                  | 132/400 [1:05:00<2:10:04, 29.12s/it]2022-01-08 10:33:12,100 iteration 2245 : loss : 0.031386, loss_ce: 0.010530
2022-01-08 10:33:13,584 iteration 2246 : loss : 0.032513, loss_ce: 0.010082
2022-01-08 10:33:15,174 iteration 2247 : loss : 0.032084, loss_ce: 0.015830
2022-01-08 10:33:16,834 iteration 2248 : loss : 0.039627, loss_ce: 0.018098
2022-01-08 10:33:18,420 iteration 2249 : loss : 0.026062, loss_ce: 0.011927
2022-01-08 10:33:20,011 iteration 2250 : loss : 0.031043, loss_ce: 0.010306
2022-01-08 10:33:21,633 iteration 2251 : loss : 0.032550, loss_ce: 0.015494
2022-01-08 10:33:23,225 iteration 2252 : loss : 0.034298, loss_ce: 0.012170
2022-01-08 10:33:24,920 iteration 2253 : loss : 0.043889, loss_ce: 0.015718
2022-01-08 10:33:26,564 iteration 2254 : loss : 0.032726, loss_ce: 0.012330
2022-01-08 10:33:28,155 iteration 2255 : loss : 0.025920, loss_ce: 0.009502
2022-01-08 10:33:29,728 iteration 2256 : loss : 0.027179, loss_ce: 0.011435
2022-01-08 10:33:31,262 iteration 2257 : loss : 0.043243, loss_ce: 0.015429
2022-01-08 10:33:32,790 iteration 2258 : loss : 0.044422, loss_ce: 0.015712
2022-01-08 10:33:34,441 iteration 2259 : loss : 0.031898, loss_ce: 0.012584
2022-01-08 10:33:35,953 iteration 2260 : loss : 0.034149, loss_ce: 0.013567
2022-01-08 10:33:37,556 iteration 2261 : loss : 0.029148, loss_ce: 0.012592
 33%|████████▉                  | 133/400 [1:05:27<2:06:51, 28.51s/it]2022-01-08 10:33:39,128 iteration 2262 : loss : 0.029415, loss_ce: 0.014838
2022-01-08 10:33:40,787 iteration 2263 : loss : 0.034847, loss_ce: 0.012445
2022-01-08 10:33:42,476 iteration 2264 : loss : 0.037481, loss_ce: 0.010696
2022-01-08 10:33:44,097 iteration 2265 : loss : 0.034578, loss_ce: 0.012113
2022-01-08 10:33:45,652 iteration 2266 : loss : 0.036794, loss_ce: 0.014512
2022-01-08 10:33:47,213 iteration 2267 : loss : 0.025863, loss_ce: 0.008479
2022-01-08 10:33:48,799 iteration 2268 : loss : 0.034515, loss_ce: 0.013166
2022-01-08 10:33:50,348 iteration 2269 : loss : 0.028368, loss_ce: 0.015089
2022-01-08 10:33:51,934 iteration 2270 : loss : 0.048089, loss_ce: 0.019078
2022-01-08 10:33:53,489 iteration 2271 : loss : 0.042329, loss_ce: 0.017006
2022-01-08 10:33:55,061 iteration 2272 : loss : 0.045087, loss_ce: 0.022002
2022-01-08 10:33:56,670 iteration 2273 : loss : 0.034146, loss_ce: 0.010822
2022-01-08 10:33:58,281 iteration 2274 : loss : 0.034118, loss_ce: 0.011606
2022-01-08 10:33:59,935 iteration 2275 : loss : 0.040534, loss_ce: 0.013107
2022-01-08 10:34:01,459 iteration 2276 : loss : 0.025940, loss_ce: 0.010238
2022-01-08 10:34:03,055 iteration 2277 : loss : 0.034594, loss_ce: 0.011176
2022-01-08 10:34:04,588 iteration 2278 : loss : 0.025634, loss_ce: 0.009697
 34%|█████████                  | 134/400 [1:05:54<2:04:24, 28.06s/it]2022-01-08 10:34:06,141 iteration 2279 : loss : 0.057663, loss_ce: 0.021321
2022-01-08 10:34:07,731 iteration 2280 : loss : 0.032445, loss_ce: 0.012947
2022-01-08 10:34:09,312 iteration 2281 : loss : 0.040287, loss_ce: 0.009428
2022-01-08 10:34:10,927 iteration 2282 : loss : 0.036758, loss_ce: 0.013739
2022-01-08 10:34:12,596 iteration 2283 : loss : 0.035381, loss_ce: 0.016503
2022-01-08 10:34:14,156 iteration 2284 : loss : 0.046419, loss_ce: 0.022563
2022-01-08 10:34:15,722 iteration 2285 : loss : 0.028602, loss_ce: 0.010198
2022-01-08 10:34:17,288 iteration 2286 : loss : 0.043157, loss_ce: 0.016580
2022-01-08 10:34:18,857 iteration 2287 : loss : 0.028889, loss_ce: 0.011723
2022-01-08 10:34:20,483 iteration 2288 : loss : 0.033563, loss_ce: 0.011754
2022-01-08 10:34:22,018 iteration 2289 : loss : 0.038055, loss_ce: 0.011970
2022-01-08 10:34:23,663 iteration 2290 : loss : 0.034777, loss_ce: 0.015842
2022-01-08 10:34:25,241 iteration 2291 : loss : 0.026925, loss_ce: 0.010627
2022-01-08 10:34:26,803 iteration 2292 : loss : 0.036334, loss_ce: 0.015543
2022-01-08 10:34:28,300 iteration 2293 : loss : 0.034800, loss_ce: 0.018193
2022-01-08 10:34:29,919 iteration 2294 : loss : 0.054098, loss_ce: 0.017958
2022-01-08 10:34:29,919 Training Data Eval:
2022-01-08 10:34:37,834   Average segmentation loss on training set: 0.0225
2022-01-08 10:34:37,834 Validation Data Eval:
2022-01-08 10:34:40,564   Average segmentation loss on validation set: 0.0797
2022-01-08 10:34:42,230 iteration 2295 : loss : 0.057316, loss_ce: 0.020360
 34%|█████████                  | 135/400 [1:06:32<2:16:38, 30.94s/it]2022-01-08 10:34:43,773 iteration 2296 : loss : 0.027000, loss_ce: 0.010795
2022-01-08 10:34:45,341 iteration 2297 : loss : 0.032064, loss_ce: 0.013737
2022-01-08 10:34:46,911 iteration 2298 : loss : 0.024981, loss_ce: 0.010534
2022-01-08 10:34:48,568 iteration 2299 : loss : 0.052986, loss_ce: 0.025304
2022-01-08 10:34:50,118 iteration 2300 : loss : 0.028546, loss_ce: 0.012056
2022-01-08 10:34:51,780 iteration 2301 : loss : 0.043908, loss_ce: 0.012939
2022-01-08 10:34:53,410 iteration 2302 : loss : 0.055162, loss_ce: 0.018128
2022-01-08 10:34:55,013 iteration 2303 : loss : 0.032440, loss_ce: 0.011519
2022-01-08 10:34:56,695 iteration 2304 : loss : 0.055446, loss_ce: 0.022191
2022-01-08 10:34:58,330 iteration 2305 : loss : 0.035382, loss_ce: 0.014081
2022-01-08 10:35:00,042 iteration 2306 : loss : 0.035661, loss_ce: 0.017273
2022-01-08 10:35:01,613 iteration 2307 : loss : 0.052490, loss_ce: 0.016344
2022-01-08 10:35:03,153 iteration 2308 : loss : 0.039370, loss_ce: 0.013066
2022-01-08 10:35:04,709 iteration 2309 : loss : 0.035721, loss_ce: 0.017369
2022-01-08 10:35:06,366 iteration 2310 : loss : 0.038178, loss_ce: 0.015699
2022-01-08 10:35:07,868 iteration 2311 : loss : 0.039673, loss_ce: 0.013994
2022-01-08 10:35:09,464 iteration 2312 : loss : 0.042235, loss_ce: 0.014115
 34%|█████████▏                 | 136/400 [1:06:59<2:11:13, 29.82s/it]2022-01-08 10:35:11,135 iteration 2313 : loss : 0.035755, loss_ce: 0.013460
2022-01-08 10:35:12,611 iteration 2314 : loss : 0.028852, loss_ce: 0.008670
2022-01-08 10:35:14,141 iteration 2315 : loss : 0.026236, loss_ce: 0.009354
2022-01-08 10:35:15,814 iteration 2316 : loss : 0.042476, loss_ce: 0.015800
2022-01-08 10:35:17,328 iteration 2317 : loss : 0.038462, loss_ce: 0.012037
2022-01-08 10:35:18,881 iteration 2318 : loss : 0.031225, loss_ce: 0.015843
2022-01-08 10:35:20,463 iteration 2319 : loss : 0.035297, loss_ce: 0.014356
2022-01-08 10:35:22,127 iteration 2320 : loss : 0.049173, loss_ce: 0.018634
2022-01-08 10:35:23,707 iteration 2321 : loss : 0.041011, loss_ce: 0.017227
2022-01-08 10:35:25,303 iteration 2322 : loss : 0.029853, loss_ce: 0.012793
2022-01-08 10:35:26,788 iteration 2323 : loss : 0.024176, loss_ce: 0.009349
2022-01-08 10:35:28,349 iteration 2324 : loss : 0.027445, loss_ce: 0.011544
2022-01-08 10:35:29,799 iteration 2325 : loss : 0.026652, loss_ce: 0.011715
2022-01-08 10:35:31,353 iteration 2326 : loss : 0.078434, loss_ce: 0.014353
2022-01-08 10:35:32,888 iteration 2327 : loss : 0.029369, loss_ce: 0.010736
2022-01-08 10:35:34,459 iteration 2328 : loss : 0.039030, loss_ce: 0.018529
2022-01-08 10:35:35,979 iteration 2329 : loss : 0.036727, loss_ce: 0.013814
 34%|█████████▏                 | 137/400 [1:07:26<2:06:23, 28.83s/it]2022-01-08 10:35:37,617 iteration 2330 : loss : 0.036384, loss_ce: 0.014126
2022-01-08 10:35:39,145 iteration 2331 : loss : 0.027580, loss_ce: 0.009745
2022-01-08 10:35:40,811 iteration 2332 : loss : 0.031480, loss_ce: 0.011499
2022-01-08 10:35:42,391 iteration 2333 : loss : 0.061180, loss_ce: 0.030354
2022-01-08 10:35:44,109 iteration 2334 : loss : 0.033664, loss_ce: 0.012577
2022-01-08 10:35:45,816 iteration 2335 : loss : 0.029714, loss_ce: 0.011216
2022-01-08 10:35:47,482 iteration 2336 : loss : 0.038480, loss_ce: 0.015060
2022-01-08 10:35:49,075 iteration 2337 : loss : 0.049495, loss_ce: 0.017046
2022-01-08 10:35:50,702 iteration 2338 : loss : 0.040332, loss_ce: 0.015130
2022-01-08 10:35:52,317 iteration 2339 : loss : 0.055443, loss_ce: 0.018373
2022-01-08 10:35:53,837 iteration 2340 : loss : 0.031839, loss_ce: 0.012440
2022-01-08 10:35:55,440 iteration 2341 : loss : 0.040539, loss_ce: 0.016969
2022-01-08 10:35:57,051 iteration 2342 : loss : 0.029062, loss_ce: 0.010702
2022-01-08 10:35:58,776 iteration 2343 : loss : 0.038624, loss_ce: 0.014527
2022-01-08 10:36:00,319 iteration 2344 : loss : 0.040802, loss_ce: 0.011413
2022-01-08 10:36:01,872 iteration 2345 : loss : 0.039711, loss_ce: 0.020005
2022-01-08 10:36:03,447 iteration 2346 : loss : 0.049513, loss_ce: 0.020344
 34%|█████████▎                 | 138/400 [1:07:53<2:04:07, 28.42s/it]2022-01-08 10:36:05,175 iteration 2347 : loss : 0.058428, loss_ce: 0.026814
2022-01-08 10:36:06,783 iteration 2348 : loss : 0.028945, loss_ce: 0.010290
2022-01-08 10:36:08,341 iteration 2349 : loss : 0.033138, loss_ce: 0.011510
2022-01-08 10:36:09,925 iteration 2350 : loss : 0.043967, loss_ce: 0.013844
2022-01-08 10:36:11,591 iteration 2351 : loss : 0.040347, loss_ce: 0.018564
2022-01-08 10:36:13,134 iteration 2352 : loss : 0.024022, loss_ce: 0.012093
2022-01-08 10:36:14,683 iteration 2353 : loss : 0.026909, loss_ce: 0.011853
2022-01-08 10:36:16,211 iteration 2354 : loss : 0.027189, loss_ce: 0.013889
2022-01-08 10:36:17,754 iteration 2355 : loss : 0.036932, loss_ce: 0.013300
2022-01-08 10:36:19,392 iteration 2356 : loss : 0.032308, loss_ce: 0.012367
2022-01-08 10:36:20,987 iteration 2357 : loss : 0.035394, loss_ce: 0.011640
2022-01-08 10:36:22,586 iteration 2358 : loss : 0.029215, loss_ce: 0.011795
2022-01-08 10:36:24,142 iteration 2359 : loss : 0.041029, loss_ce: 0.015345
2022-01-08 10:36:25,674 iteration 2360 : loss : 0.031769, loss_ce: 0.009010
2022-01-08 10:36:27,250 iteration 2361 : loss : 0.025940, loss_ce: 0.009527
2022-01-08 10:36:28,963 iteration 2362 : loss : 0.060603, loss_ce: 0.023967
2022-01-08 10:36:30,558 iteration 2363 : loss : 0.027339, loss_ce: 0.013056
 35%|█████████▍                 | 139/400 [1:08:20<2:01:56, 28.03s/it]2022-01-08 10:36:32,273 iteration 2364 : loss : 0.026671, loss_ce: 0.009068
2022-01-08 10:36:33,959 iteration 2365 : loss : 0.038858, loss_ce: 0.014405
2022-01-08 10:36:35,601 iteration 2366 : loss : 0.042903, loss_ce: 0.015802
2022-01-08 10:36:37,131 iteration 2367 : loss : 0.028518, loss_ce: 0.006230
2022-01-08 10:36:38,661 iteration 2368 : loss : 0.027274, loss_ce: 0.011112
2022-01-08 10:36:40,251 iteration 2369 : loss : 0.033353, loss_ce: 0.010283
2022-01-08 10:36:41,820 iteration 2370 : loss : 0.044787, loss_ce: 0.016600
2022-01-08 10:36:43,442 iteration 2371 : loss : 0.025331, loss_ce: 0.009970
2022-01-08 10:36:44,982 iteration 2372 : loss : 0.029545, loss_ce: 0.008426
2022-01-08 10:36:46,544 iteration 2373 : loss : 0.038861, loss_ce: 0.013461
2022-01-08 10:36:48,065 iteration 2374 : loss : 0.028060, loss_ce: 0.011638
2022-01-08 10:36:49,608 iteration 2375 : loss : 0.024420, loss_ce: 0.008638
2022-01-08 10:36:51,208 iteration 2376 : loss : 0.041976, loss_ce: 0.019700
2022-01-08 10:36:52,805 iteration 2377 : loss : 0.032962, loss_ce: 0.014197
2022-01-08 10:36:54,399 iteration 2378 : loss : 0.023853, loss_ce: 0.009599
2022-01-08 10:36:56,024 iteration 2379 : loss : 0.044370, loss_ce: 0.019422
2022-01-08 10:36:56,024 Training Data Eval:
2022-01-08 10:37:03,931   Average segmentation loss on training set: 0.0235
2022-01-08 10:37:03,931 Validation Data Eval:
2022-01-08 10:37:06,660   Average segmentation loss on validation set: 0.0662
2022-01-08 10:37:08,206 iteration 2380 : loss : 0.022715, loss_ce: 0.010680
 35%|█████████▍                 | 140/400 [1:08:58<2:13:57, 30.91s/it]2022-01-08 10:37:09,777 iteration 2381 : loss : 0.029309, loss_ce: 0.010496
2022-01-08 10:37:11,408 iteration 2382 : loss : 0.036831, loss_ce: 0.011671
2022-01-08 10:37:12,949 iteration 2383 : loss : 0.022752, loss_ce: 0.007603
2022-01-08 10:37:14,583 iteration 2384 : loss : 0.036842, loss_ce: 0.014426
2022-01-08 10:37:16,233 iteration 2385 : loss : 0.038377, loss_ce: 0.013539
2022-01-08 10:37:17,809 iteration 2386 : loss : 0.032030, loss_ce: 0.009498
2022-01-08 10:37:19,354 iteration 2387 : loss : 0.034297, loss_ce: 0.010465
2022-01-08 10:37:20,938 iteration 2388 : loss : 0.033058, loss_ce: 0.013030
2022-01-08 10:37:22,450 iteration 2389 : loss : 0.017585, loss_ce: 0.005648
2022-01-08 10:37:24,047 iteration 2390 : loss : 0.033813, loss_ce: 0.015996
2022-01-08 10:37:25,611 iteration 2391 : loss : 0.032407, loss_ce: 0.018979
2022-01-08 10:37:27,222 iteration 2392 : loss : 0.033478, loss_ce: 0.010578
2022-01-08 10:37:28,830 iteration 2393 : loss : 0.037333, loss_ce: 0.011817
2022-01-08 10:37:30,310 iteration 2394 : loss : 0.027903, loss_ce: 0.009357
2022-01-08 10:37:31,952 iteration 2395 : loss : 0.028760, loss_ce: 0.012484
2022-01-08 10:37:33,485 iteration 2396 : loss : 0.026649, loss_ce: 0.012674
2022-01-08 10:37:35,101 iteration 2397 : loss : 0.050682, loss_ce: 0.020105
 35%|█████████▌                 | 141/400 [1:09:25<2:08:15, 29.71s/it]2022-01-08 10:37:36,715 iteration 2398 : loss : 0.040067, loss_ce: 0.014639
2022-01-08 10:37:38,265 iteration 2399 : loss : 0.030996, loss_ce: 0.009204
2022-01-08 10:37:39,911 iteration 2400 : loss : 0.029001, loss_ce: 0.012770
2022-01-08 10:37:41,497 iteration 2401 : loss : 0.025629, loss_ce: 0.008052
2022-01-08 10:37:43,136 iteration 2402 : loss : 0.042571, loss_ce: 0.023119
2022-01-08 10:37:44,684 iteration 2403 : loss : 0.048627, loss_ce: 0.023651
2022-01-08 10:37:46,209 iteration 2404 : loss : 0.029894, loss_ce: 0.013262
2022-01-08 10:37:47,698 iteration 2405 : loss : 0.036306, loss_ce: 0.014029
2022-01-08 10:37:49,420 iteration 2406 : loss : 0.035966, loss_ce: 0.012337
2022-01-08 10:37:50,936 iteration 2407 : loss : 0.030586, loss_ce: 0.014955
2022-01-08 10:37:52,454 iteration 2408 : loss : 0.024844, loss_ce: 0.009644
2022-01-08 10:37:54,084 iteration 2409 : loss : 0.037546, loss_ce: 0.011547
2022-01-08 10:37:55,728 iteration 2410 : loss : 0.032611, loss_ce: 0.013752
2022-01-08 10:37:57,348 iteration 2411 : loss : 0.048984, loss_ce: 0.015870
2022-01-08 10:37:58,885 iteration 2412 : loss : 0.026277, loss_ce: 0.010228
2022-01-08 10:38:00,545 iteration 2413 : loss : 0.087104, loss_ce: 0.050196
2022-01-08 10:38:02,140 iteration 2414 : loss : 0.028353, loss_ce: 0.010612
 36%|█████████▌                 | 142/400 [1:09:52<2:04:17, 28.91s/it]2022-01-08 10:38:03,788 iteration 2415 : loss : 0.045675, loss_ce: 0.015847
2022-01-08 10:38:05,349 iteration 2416 : loss : 0.027889, loss_ce: 0.010039
2022-01-08 10:38:06,931 iteration 2417 : loss : 0.025767, loss_ce: 0.008596
2022-01-08 10:38:08,611 iteration 2418 : loss : 0.030922, loss_ce: 0.012369
2022-01-08 10:38:10,210 iteration 2419 : loss : 0.042789, loss_ce: 0.022510
2022-01-08 10:38:11,911 iteration 2420 : loss : 0.049256, loss_ce: 0.021135
2022-01-08 10:38:13,587 iteration 2421 : loss : 0.040681, loss_ce: 0.013219
2022-01-08 10:38:15,246 iteration 2422 : loss : 0.033625, loss_ce: 0.012302
2022-01-08 10:38:16,835 iteration 2423 : loss : 0.037865, loss_ce: 0.013984
2022-01-08 10:38:18,404 iteration 2424 : loss : 0.025110, loss_ce: 0.010384
2022-01-08 10:38:20,105 iteration 2425 : loss : 0.031029, loss_ce: 0.009912
2022-01-08 10:38:21,676 iteration 2426 : loss : 0.038588, loss_ce: 0.013300
2022-01-08 10:38:23,160 iteration 2427 : loss : 0.024709, loss_ce: 0.011331
2022-01-08 10:38:24,697 iteration 2428 : loss : 0.030760, loss_ce: 0.009111
2022-01-08 10:38:26,241 iteration 2429 : loss : 0.065738, loss_ce: 0.021684
2022-01-08 10:38:27,783 iteration 2430 : loss : 0.019987, loss_ce: 0.007865
2022-01-08 10:38:29,363 iteration 2431 : loss : 0.033957, loss_ce: 0.016984
 36%|█████████▋                 | 143/400 [1:10:19<2:01:39, 28.40s/it]2022-01-08 10:38:30,941 iteration 2432 : loss : 0.028642, loss_ce: 0.009017
2022-01-08 10:38:32,509 iteration 2433 : loss : 0.029888, loss_ce: 0.012730
2022-01-08 10:38:34,095 iteration 2434 : loss : 0.032493, loss_ce: 0.010835
2022-01-08 10:38:35,772 iteration 2435 : loss : 0.037192, loss_ce: 0.020343
2022-01-08 10:38:37,266 iteration 2436 : loss : 0.024369, loss_ce: 0.010386
2022-01-08 10:38:38,898 iteration 2437 : loss : 0.030632, loss_ce: 0.010318
2022-01-08 10:38:40,540 iteration 2438 : loss : 0.042974, loss_ce: 0.013541
2022-01-08 10:38:42,168 iteration 2439 : loss : 0.031370, loss_ce: 0.011131
2022-01-08 10:38:43,809 iteration 2440 : loss : 0.034195, loss_ce: 0.013043
2022-01-08 10:38:45,382 iteration 2441 : loss : 0.031176, loss_ce: 0.010759
2022-01-08 10:38:46,943 iteration 2442 : loss : 0.029480, loss_ce: 0.012064
2022-01-08 10:38:48,447 iteration 2443 : loss : 0.033993, loss_ce: 0.011659
2022-01-08 10:38:50,027 iteration 2444 : loss : 0.036204, loss_ce: 0.015772
2022-01-08 10:38:51,620 iteration 2445 : loss : 0.037765, loss_ce: 0.014215
2022-01-08 10:38:53,301 iteration 2446 : loss : 0.027576, loss_ce: 0.012263
2022-01-08 10:38:54,818 iteration 2447 : loss : 0.026702, loss_ce: 0.008429
2022-01-08 10:38:56,450 iteration 2448 : loss : 0.043858, loss_ce: 0.015690
 36%|█████████▋                 | 144/400 [1:10:46<1:59:29, 28.01s/it]2022-01-08 10:38:58,002 iteration 2449 : loss : 0.053459, loss_ce: 0.021511
2022-01-08 10:38:59,516 iteration 2450 : loss : 0.023042, loss_ce: 0.012172
2022-01-08 10:39:01,070 iteration 2451 : loss : 0.028134, loss_ce: 0.008511
2022-01-08 10:39:02,602 iteration 2452 : loss : 0.042806, loss_ce: 0.012874
2022-01-08 10:39:04,188 iteration 2453 : loss : 0.032134, loss_ce: 0.011674
2022-01-08 10:39:05,746 iteration 2454 : loss : 0.030281, loss_ce: 0.010615
2022-01-08 10:39:07,274 iteration 2455 : loss : 0.047178, loss_ce: 0.023778
2022-01-08 10:39:08,787 iteration 2456 : loss : 0.029728, loss_ce: 0.012549
2022-01-08 10:39:10,342 iteration 2457 : loss : 0.035165, loss_ce: 0.013133
2022-01-08 10:39:11,865 iteration 2458 : loss : 0.029590, loss_ce: 0.011867
2022-01-08 10:39:13,338 iteration 2459 : loss : 0.023671, loss_ce: 0.008846
2022-01-08 10:39:14,879 iteration 2460 : loss : 0.041818, loss_ce: 0.013873
2022-01-08 10:39:16,526 iteration 2461 : loss : 0.038735, loss_ce: 0.014921
2022-01-08 10:39:18,133 iteration 2462 : loss : 0.052100, loss_ce: 0.018279
2022-01-08 10:39:19,684 iteration 2463 : loss : 0.027405, loss_ce: 0.011869
2022-01-08 10:39:21,295 iteration 2464 : loss : 0.024949, loss_ce: 0.008279
2022-01-08 10:39:21,296 Training Data Eval:
2022-01-08 10:39:29,196   Average segmentation loss on training set: 0.0211
2022-01-08 10:39:29,196 Validation Data Eval:
2022-01-08 10:39:31,923   Average segmentation loss on validation set: 0.0704
2022-01-08 10:39:33,481 iteration 2465 : loss : 0.029200, loss_ce: 0.011987
 36%|█████████▊                 | 145/400 [1:11:23<2:10:32, 30.71s/it]2022-01-08 10:39:35,170 iteration 2466 : loss : 0.041952, loss_ce: 0.015024
2022-01-08 10:39:36,833 iteration 2467 : loss : 0.031289, loss_ce: 0.013043
2022-01-08 10:39:38,389 iteration 2468 : loss : 0.031250, loss_ce: 0.012605
2022-01-08 10:39:39,908 iteration 2469 : loss : 0.026761, loss_ce: 0.012122
2022-01-08 10:39:41,472 iteration 2470 : loss : 0.038174, loss_ce: 0.011875
2022-01-08 10:39:43,145 iteration 2471 : loss : 0.043371, loss_ce: 0.016488
2022-01-08 10:39:44,686 iteration 2472 : loss : 0.038428, loss_ce: 0.013750
2022-01-08 10:39:46,319 iteration 2473 : loss : 0.034487, loss_ce: 0.013641
2022-01-08 10:39:47,863 iteration 2474 : loss : 0.026217, loss_ce: 0.008767
2022-01-08 10:39:49,442 iteration 2475 : loss : 0.032409, loss_ce: 0.011194
2022-01-08 10:39:50,962 iteration 2476 : loss : 0.044719, loss_ce: 0.020538
2022-01-08 10:39:52,632 iteration 2477 : loss : 0.041988, loss_ce: 0.016512
2022-01-08 10:39:54,247 iteration 2478 : loss : 0.028625, loss_ce: 0.010474
2022-01-08 10:39:55,738 iteration 2479 : loss : 0.027065, loss_ce: 0.009425
2022-01-08 10:39:57,321 iteration 2480 : loss : 0.032426, loss_ce: 0.011265
2022-01-08 10:39:58,966 iteration 2481 : loss : 0.031483, loss_ce: 0.010439
2022-01-08 10:40:00,515 iteration 2482 : loss : 0.022822, loss_ce: 0.009781
 36%|█████████▊                 | 146/400 [1:11:50<2:05:20, 29.61s/it]2022-01-08 10:40:02,226 iteration 2483 : loss : 0.048978, loss_ce: 0.024519
2022-01-08 10:40:03,842 iteration 2484 : loss : 0.036482, loss_ce: 0.009464
2022-01-08 10:40:05,382 iteration 2485 : loss : 0.043712, loss_ce: 0.016864
2022-01-08 10:40:06,948 iteration 2486 : loss : 0.025345, loss_ce: 0.009954
2022-01-08 10:40:08,507 iteration 2487 : loss : 0.034351, loss_ce: 0.011710
2022-01-08 10:40:10,151 iteration 2488 : loss : 0.038334, loss_ce: 0.014162
2022-01-08 10:40:11,713 iteration 2489 : loss : 0.031271, loss_ce: 0.013917
2022-01-08 10:40:13,343 iteration 2490 : loss : 0.039433, loss_ce: 0.015797
2022-01-08 10:40:15,015 iteration 2491 : loss : 0.034200, loss_ce: 0.015014
2022-01-08 10:40:16,500 iteration 2492 : loss : 0.029657, loss_ce: 0.010856
2022-01-08 10:40:18,100 iteration 2493 : loss : 0.038486, loss_ce: 0.015001
2022-01-08 10:40:19,662 iteration 2494 : loss : 0.055128, loss_ce: 0.014803
2022-01-08 10:40:21,215 iteration 2495 : loss : 0.035240, loss_ce: 0.017677
2022-01-08 10:40:22,994 iteration 2496 : loss : 0.044614, loss_ce: 0.013231
2022-01-08 10:40:24,627 iteration 2497 : loss : 0.042302, loss_ce: 0.015449
2022-01-08 10:40:26,282 iteration 2498 : loss : 0.049637, loss_ce: 0.020150
2022-01-08 10:40:27,798 iteration 2499 : loss : 0.027834, loss_ce: 0.007690
 37%|█████████▉                 | 147/400 [1:12:18<2:01:54, 28.91s/it]2022-01-08 10:40:29,403 iteration 2500 : loss : 0.040713, loss_ce: 0.013350
2022-01-08 10:40:30,973 iteration 2501 : loss : 0.031300, loss_ce: 0.009343
2022-01-08 10:40:32,631 iteration 2502 : loss : 0.046920, loss_ce: 0.015357
2022-01-08 10:40:34,150 iteration 2503 : loss : 0.035322, loss_ce: 0.011238
2022-01-08 10:40:35,690 iteration 2504 : loss : 0.024165, loss_ce: 0.008209
2022-01-08 10:40:37,312 iteration 2505 : loss : 0.033450, loss_ce: 0.013241
2022-01-08 10:40:38,887 iteration 2506 : loss : 0.044471, loss_ce: 0.013857
2022-01-08 10:40:40,412 iteration 2507 : loss : 0.030365, loss_ce: 0.013278
2022-01-08 10:40:41,962 iteration 2508 : loss : 0.026081, loss_ce: 0.008522
2022-01-08 10:40:43,519 iteration 2509 : loss : 0.026287, loss_ce: 0.008927
2022-01-08 10:40:45,058 iteration 2510 : loss : 0.026052, loss_ce: 0.010540
2022-01-08 10:40:46,646 iteration 2511 : loss : 0.026663, loss_ce: 0.011605
2022-01-08 10:40:48,249 iteration 2512 : loss : 0.030628, loss_ce: 0.010947
2022-01-08 10:40:49,891 iteration 2513 : loss : 0.037806, loss_ce: 0.012844
2022-01-08 10:40:51,481 iteration 2514 : loss : 0.032066, loss_ce: 0.009739
2022-01-08 10:40:53,050 iteration 2515 : loss : 0.025304, loss_ce: 0.014445
2022-01-08 10:40:54,608 iteration 2516 : loss : 0.023623, loss_ce: 0.010891
 37%|█████████▉                 | 148/400 [1:12:44<1:58:47, 28.28s/it]2022-01-08 10:40:56,203 iteration 2517 : loss : 0.025523, loss_ce: 0.010143
2022-01-08 10:40:57,782 iteration 2518 : loss : 0.035617, loss_ce: 0.010354
2022-01-08 10:40:59,388 iteration 2519 : loss : 0.029834, loss_ce: 0.009605
2022-01-08 10:41:00,964 iteration 2520 : loss : 0.036455, loss_ce: 0.015331
2022-01-08 10:41:02,523 iteration 2521 : loss : 0.025420, loss_ce: 0.007789
2022-01-08 10:41:04,112 iteration 2522 : loss : 0.031784, loss_ce: 0.016760
2022-01-08 10:41:05,749 iteration 2523 : loss : 0.029400, loss_ce: 0.008619
2022-01-08 10:41:07,311 iteration 2524 : loss : 0.025891, loss_ce: 0.011834
2022-01-08 10:41:08,901 iteration 2525 : loss : 0.033396, loss_ce: 0.012525
2022-01-08 10:41:10,505 iteration 2526 : loss : 0.038070, loss_ce: 0.014177
2022-01-08 10:41:12,090 iteration 2527 : loss : 0.025927, loss_ce: 0.009489
2022-01-08 10:41:13,697 iteration 2528 : loss : 0.035246, loss_ce: 0.012045
2022-01-08 10:41:15,289 iteration 2529 : loss : 0.039613, loss_ce: 0.012301
2022-01-08 10:41:16,889 iteration 2530 : loss : 0.029118, loss_ce: 0.014198
2022-01-08 10:41:18,557 iteration 2531 : loss : 0.036597, loss_ce: 0.012600
2022-01-08 10:41:20,136 iteration 2532 : loss : 0.053295, loss_ce: 0.017533
2022-01-08 10:41:21,729 iteration 2533 : loss : 0.036029, loss_ce: 0.009978
 37%|██████████                 | 149/400 [1:13:11<1:56:50, 27.93s/it]2022-01-08 10:41:23,276 iteration 2534 : loss : 0.031339, loss_ce: 0.012950
2022-01-08 10:41:24,937 iteration 2535 : loss : 0.039752, loss_ce: 0.013990
2022-01-08 10:41:26,481 iteration 2536 : loss : 0.027634, loss_ce: 0.012461
2022-01-08 10:41:28,028 iteration 2537 : loss : 0.025602, loss_ce: 0.009678
2022-01-08 10:41:29,498 iteration 2538 : loss : 0.021179, loss_ce: 0.009314
2022-01-08 10:41:31,100 iteration 2539 : loss : 0.027760, loss_ce: 0.011183
2022-01-08 10:41:32,633 iteration 2540 : loss : 0.021705, loss_ce: 0.006750
2022-01-08 10:41:34,188 iteration 2541 : loss : 0.027687, loss_ce: 0.011331
2022-01-08 10:41:35,707 iteration 2542 : loss : 0.020569, loss_ce: 0.007146
2022-01-08 10:41:37,223 iteration 2543 : loss : 0.030609, loss_ce: 0.010507
2022-01-08 10:41:38,805 iteration 2544 : loss : 0.032684, loss_ce: 0.012809
2022-01-08 10:41:40,494 iteration 2545 : loss : 0.027921, loss_ce: 0.010485
2022-01-08 10:41:42,171 iteration 2546 : loss : 0.038801, loss_ce: 0.018858
2022-01-08 10:41:43,678 iteration 2547 : loss : 0.024085, loss_ce: 0.010681
2022-01-08 10:41:45,260 iteration 2548 : loss : 0.026001, loss_ce: 0.011896
2022-01-08 10:41:46,842 iteration 2549 : loss : 0.054687, loss_ce: 0.020345
2022-01-08 10:41:46,842 Training Data Eval:
2022-01-08 10:41:54,699   Average segmentation loss on training set: 0.0206
2022-01-08 10:41:54,700 Validation Data Eval:
2022-01-08 10:41:58,352   Average segmentation loss on validation set: 0.0644
2022-01-08 10:42:04,165 Found new lowest validation loss at iteration 2549! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 10:42:05,577 iteration 2550 : loss : 0.030041, loss_ce: 0.009621
 38%|██████████▏                | 150/400 [1:13:55<2:16:17, 32.71s/it]2022-01-08 10:42:07,028 iteration 2551 : loss : 0.023283, loss_ce: 0.010964
2022-01-08 10:42:08,608 iteration 2552 : loss : 0.025607, loss_ce: 0.010646
2022-01-08 10:42:10,097 iteration 2553 : loss : 0.021635, loss_ce: 0.007057
2022-01-08 10:42:11,657 iteration 2554 : loss : 0.027474, loss_ce: 0.010283
2022-01-08 10:42:13,199 iteration 2555 : loss : 0.027928, loss_ce: 0.007582
2022-01-08 10:42:14,807 iteration 2556 : loss : 0.044718, loss_ce: 0.023566
2022-01-08 10:42:16,354 iteration 2557 : loss : 0.027151, loss_ce: 0.012757
2022-01-08 10:42:17,925 iteration 2558 : loss : 0.036058, loss_ce: 0.023643
2022-01-08 10:42:19,570 iteration 2559 : loss : 0.029888, loss_ce: 0.011351
2022-01-08 10:42:21,135 iteration 2560 : loss : 0.028448, loss_ce: 0.013920
2022-01-08 10:42:22,796 iteration 2561 : loss : 0.036310, loss_ce: 0.011606
2022-01-08 10:42:24,382 iteration 2562 : loss : 0.030905, loss_ce: 0.011938
2022-01-08 10:42:25,983 iteration 2563 : loss : 0.033302, loss_ce: 0.014254
2022-01-08 10:42:27,508 iteration 2564 : loss : 0.027792, loss_ce: 0.012412
2022-01-08 10:42:29,036 iteration 2565 : loss : 0.028940, loss_ce: 0.010973
2022-01-08 10:42:30,581 iteration 2566 : loss : 0.036267, loss_ce: 0.017285
2022-01-08 10:42:32,159 iteration 2567 : loss : 0.062231, loss_ce: 0.021193
 38%|██████████▏                | 151/400 [1:14:22<2:08:06, 30.87s/it]2022-01-08 10:42:33,793 iteration 2568 : loss : 0.036596, loss_ce: 0.018020
2022-01-08 10:42:35,342 iteration 2569 : loss : 0.023651, loss_ce: 0.011942
2022-01-08 10:42:36,896 iteration 2570 : loss : 0.031927, loss_ce: 0.012978
2022-01-08 10:42:38,454 iteration 2571 : loss : 0.019517, loss_ce: 0.007498
2022-01-08 10:42:39,959 iteration 2572 : loss : 0.025439, loss_ce: 0.007247
2022-01-08 10:42:41,470 iteration 2573 : loss : 0.027217, loss_ce: 0.010618
2022-01-08 10:42:43,013 iteration 2574 : loss : 0.027727, loss_ce: 0.009181
2022-01-08 10:42:44,689 iteration 2575 : loss : 0.040263, loss_ce: 0.018914
2022-01-08 10:42:46,274 iteration 2576 : loss : 0.021890, loss_ce: 0.008143
2022-01-08 10:42:47,854 iteration 2577 : loss : 0.044182, loss_ce: 0.015232
2022-01-08 10:42:49,442 iteration 2578 : loss : 0.039006, loss_ce: 0.014909
2022-01-08 10:42:50,954 iteration 2579 : loss : 0.033946, loss_ce: 0.008769
2022-01-08 10:42:52,584 iteration 2580 : loss : 0.030075, loss_ce: 0.007205
2022-01-08 10:42:54,201 iteration 2581 : loss : 0.033337, loss_ce: 0.014776
2022-01-08 10:42:55,844 iteration 2582 : loss : 0.025022, loss_ce: 0.008626
2022-01-08 10:42:57,399 iteration 2583 : loss : 0.025180, loss_ce: 0.009834
2022-01-08 10:42:58,872 iteration 2584 : loss : 0.020682, loss_ce: 0.008308
 38%|██████████▎                | 152/400 [1:14:49<2:02:26, 29.62s/it]2022-01-08 10:43:00,474 iteration 2585 : loss : 0.017938, loss_ce: 0.004756
2022-01-08 10:43:02,042 iteration 2586 : loss : 0.043387, loss_ce: 0.016909
2022-01-08 10:43:03,541 iteration 2587 : loss : 0.028239, loss_ce: 0.010739
2022-01-08 10:43:05,075 iteration 2588 : loss : 0.024025, loss_ce: 0.011573
2022-01-08 10:43:06,673 iteration 2589 : loss : 0.027744, loss_ce: 0.008943
2022-01-08 10:43:08,280 iteration 2590 : loss : 0.026914, loss_ce: 0.012295
2022-01-08 10:43:09,806 iteration 2591 : loss : 0.021710, loss_ce: 0.007186
2022-01-08 10:43:11,337 iteration 2592 : loss : 0.029898, loss_ce: 0.009565
2022-01-08 10:43:12,878 iteration 2593 : loss : 0.032581, loss_ce: 0.016006
2022-01-08 10:43:14,573 iteration 2594 : loss : 0.029601, loss_ce: 0.013855
2022-01-08 10:43:16,173 iteration 2595 : loss : 0.030198, loss_ce: 0.011064
2022-01-08 10:43:17,700 iteration 2596 : loss : 0.022866, loss_ce: 0.010375
2022-01-08 10:43:19,209 iteration 2597 : loss : 0.029025, loss_ce: 0.010486
2022-01-08 10:43:20,746 iteration 2598 : loss : 0.024950, loss_ce: 0.009784
2022-01-08 10:43:22,296 iteration 2599 : loss : 0.025987, loss_ce: 0.006511
2022-01-08 10:43:23,847 iteration 2600 : loss : 0.030376, loss_ce: 0.010143
2022-01-08 10:43:25,350 iteration 2601 : loss : 0.024670, loss_ce: 0.010171
 38%|██████████▎                | 153/400 [1:15:15<1:58:03, 28.68s/it]2022-01-08 10:43:26,974 iteration 2602 : loss : 0.024679, loss_ce: 0.011567
2022-01-08 10:43:28,463 iteration 2603 : loss : 0.022709, loss_ce: 0.009099
2022-01-08 10:43:29,999 iteration 2604 : loss : 0.022747, loss_ce: 0.008285
2022-01-08 10:43:31,677 iteration 2605 : loss : 0.032949, loss_ce: 0.010515
2022-01-08 10:43:33,231 iteration 2606 : loss : 0.026062, loss_ce: 0.010820
2022-01-08 10:43:34,795 iteration 2607 : loss : 0.032618, loss_ce: 0.012371
2022-01-08 10:43:36,297 iteration 2608 : loss : 0.027572, loss_ce: 0.012648
2022-01-08 10:43:37,818 iteration 2609 : loss : 0.029159, loss_ce: 0.008479
2022-01-08 10:43:39,494 iteration 2610 : loss : 0.031140, loss_ce: 0.012915
2022-01-08 10:43:41,025 iteration 2611 : loss : 0.029183, loss_ce: 0.009831
2022-01-08 10:43:42,487 iteration 2612 : loss : 0.021189, loss_ce: 0.010703
2022-01-08 10:43:44,014 iteration 2613 : loss : 0.039453, loss_ce: 0.011914
2022-01-08 10:43:45,622 iteration 2614 : loss : 0.041169, loss_ce: 0.014001
2022-01-08 10:43:47,142 iteration 2615 : loss : 0.027418, loss_ce: 0.010858
2022-01-08 10:43:48,670 iteration 2616 : loss : 0.026657, loss_ce: 0.010408
2022-01-08 10:43:50,302 iteration 2617 : loss : 0.031690, loss_ce: 0.009238
2022-01-08 10:43:51,876 iteration 2618 : loss : 0.034961, loss_ce: 0.016284
 38%|██████████▍                | 154/400 [1:15:42<1:54:56, 28.03s/it]2022-01-08 10:43:53,525 iteration 2619 : loss : 0.031457, loss_ce: 0.008980
2022-01-08 10:43:55,057 iteration 2620 : loss : 0.026855, loss_ce: 0.009973
2022-01-08 10:43:56,643 iteration 2621 : loss : 0.038902, loss_ce: 0.013631
2022-01-08 10:43:58,228 iteration 2622 : loss : 0.034753, loss_ce: 0.019084
2022-01-08 10:43:59,824 iteration 2623 : loss : 0.041287, loss_ce: 0.017544
2022-01-08 10:44:01,344 iteration 2624 : loss : 0.028035, loss_ce: 0.009347
2022-01-08 10:44:02,881 iteration 2625 : loss : 0.022357, loss_ce: 0.009134
2022-01-08 10:44:04,498 iteration 2626 : loss : 0.033740, loss_ce: 0.014161
2022-01-08 10:44:06,084 iteration 2627 : loss : 0.032099, loss_ce: 0.009464
2022-01-08 10:44:07,601 iteration 2628 : loss : 0.024352, loss_ce: 0.008525
2022-01-08 10:44:09,221 iteration 2629 : loss : 0.035403, loss_ce: 0.010940
2022-01-08 10:44:10,725 iteration 2630 : loss : 0.021559, loss_ce: 0.006381
2022-01-08 10:44:12,302 iteration 2631 : loss : 0.026934, loss_ce: 0.012548
2022-01-08 10:44:13,889 iteration 2632 : loss : 0.023632, loss_ce: 0.008673
2022-01-08 10:44:15,412 iteration 2633 : loss : 0.027222, loss_ce: 0.013065
2022-01-08 10:44:16,990 iteration 2634 : loss : 0.024282, loss_ce: 0.007737
2022-01-08 10:44:16,990 Training Data Eval:
2022-01-08 10:44:24,875   Average segmentation loss on training set: 0.0188
2022-01-08 10:44:24,875 Validation Data Eval:
2022-01-08 10:44:27,586   Average segmentation loss on validation set: 0.0712
2022-01-08 10:44:29,216 iteration 2635 : loss : 0.034082, loss_ce: 0.014490
 39%|██████████▍                | 155/400 [1:16:19<2:05:51, 30.82s/it]2022-01-08 10:44:30,779 iteration 2636 : loss : 0.022108, loss_ce: 0.006482
2022-01-08 10:44:32,468 iteration 2637 : loss : 0.035686, loss_ce: 0.012012
2022-01-08 10:44:34,040 iteration 2638 : loss : 0.021137, loss_ce: 0.007729
2022-01-08 10:44:35,492 iteration 2639 : loss : 0.021362, loss_ce: 0.007692
2022-01-08 10:44:37,001 iteration 2640 : loss : 0.029447, loss_ce: 0.010955
2022-01-08 10:44:38,642 iteration 2641 : loss : 0.043290, loss_ce: 0.016640
2022-01-08 10:44:40,249 iteration 2642 : loss : 0.040716, loss_ce: 0.011961
2022-01-08 10:44:41,849 iteration 2643 : loss : 0.042225, loss_ce: 0.016960
2022-01-08 10:44:43,379 iteration 2644 : loss : 0.029796, loss_ce: 0.008794
2022-01-08 10:44:44,905 iteration 2645 : loss : 0.027230, loss_ce: 0.013571
2022-01-08 10:44:46,441 iteration 2646 : loss : 0.027732, loss_ce: 0.008076
2022-01-08 10:44:48,047 iteration 2647 : loss : 0.044412, loss_ce: 0.020791
2022-01-08 10:44:49,526 iteration 2648 : loss : 0.018708, loss_ce: 0.008817
2022-01-08 10:44:51,118 iteration 2649 : loss : 0.027950, loss_ce: 0.012652
2022-01-08 10:44:52,709 iteration 2650 : loss : 0.026546, loss_ce: 0.010574
2022-01-08 10:44:54,241 iteration 2651 : loss : 0.037784, loss_ce: 0.012082
2022-01-08 10:44:55,800 iteration 2652 : loss : 0.026267, loss_ce: 0.012063
 39%|██████████▌                | 156/400 [1:16:46<2:00:11, 29.55s/it]2022-01-08 10:44:57,493 iteration 2653 : loss : 0.039613, loss_ce: 0.013686
2022-01-08 10:44:59,052 iteration 2654 : loss : 0.023279, loss_ce: 0.008883
2022-01-08 10:45:00,553 iteration 2655 : loss : 0.022098, loss_ce: 0.007788
2022-01-08 10:45:02,036 iteration 2656 : loss : 0.021451, loss_ce: 0.009738
2022-01-08 10:45:03,536 iteration 2657 : loss : 0.031530, loss_ce: 0.013043
2022-01-08 10:45:05,054 iteration 2658 : loss : 0.026058, loss_ce: 0.010772
2022-01-08 10:45:06,725 iteration 2659 : loss : 0.035268, loss_ce: 0.016421
2022-01-08 10:45:08,278 iteration 2660 : loss : 0.023800, loss_ce: 0.009008
2022-01-08 10:45:09,751 iteration 2661 : loss : 0.027175, loss_ce: 0.010190
2022-01-08 10:45:11,350 iteration 2662 : loss : 0.034712, loss_ce: 0.014832
2022-01-08 10:45:12,859 iteration 2663 : loss : 0.030715, loss_ce: 0.012069
2022-01-08 10:45:14,477 iteration 2664 : loss : 0.025360, loss_ce: 0.009159
2022-01-08 10:45:16,016 iteration 2665 : loss : 0.024126, loss_ce: 0.009363
2022-01-08 10:45:17,581 iteration 2666 : loss : 0.026119, loss_ce: 0.007802
2022-01-08 10:45:19,174 iteration 2667 : loss : 0.021148, loss_ce: 0.007703
2022-01-08 10:45:20,757 iteration 2668 : loss : 0.035679, loss_ce: 0.011818
2022-01-08 10:45:22,362 iteration 2669 : loss : 0.026735, loss_ce: 0.010151
 39%|██████████▌                | 157/400 [1:17:12<1:56:03, 28.66s/it]2022-01-08 10:45:23,984 iteration 2670 : loss : 0.030182, loss_ce: 0.006822
2022-01-08 10:45:25,561 iteration 2671 : loss : 0.028860, loss_ce: 0.008125
2022-01-08 10:45:27,151 iteration 2672 : loss : 0.023806, loss_ce: 0.010931
2022-01-08 10:45:28,764 iteration 2673 : loss : 0.051722, loss_ce: 0.018890
2022-01-08 10:45:30,291 iteration 2674 : loss : 0.034883, loss_ce: 0.018009
2022-01-08 10:45:31,907 iteration 2675 : loss : 0.034435, loss_ce: 0.011549
2022-01-08 10:45:33,487 iteration 2676 : loss : 0.030669, loss_ce: 0.008124
2022-01-08 10:45:34,991 iteration 2677 : loss : 0.021741, loss_ce: 0.009078
2022-01-08 10:45:36,479 iteration 2678 : loss : 0.023971, loss_ce: 0.007320
2022-01-08 10:45:38,060 iteration 2679 : loss : 0.025755, loss_ce: 0.010897
2022-01-08 10:45:39,676 iteration 2680 : loss : 0.032074, loss_ce: 0.013628
2022-01-08 10:45:41,226 iteration 2681 : loss : 0.023332, loss_ce: 0.010752
2022-01-08 10:45:42,724 iteration 2682 : loss : 0.020995, loss_ce: 0.008046
2022-01-08 10:45:44,293 iteration 2683 : loss : 0.027717, loss_ce: 0.013393
2022-01-08 10:45:45,849 iteration 2684 : loss : 0.060796, loss_ce: 0.015689
2022-01-08 10:45:47,409 iteration 2685 : loss : 0.028916, loss_ce: 0.013394
2022-01-08 10:45:48,957 iteration 2686 : loss : 0.027913, loss_ce: 0.009733
 40%|██████████▋                | 158/400 [1:17:39<1:53:05, 28.04s/it]2022-01-08 10:45:50,530 iteration 2687 : loss : 0.017995, loss_ce: 0.007964
2022-01-08 10:45:52,069 iteration 2688 : loss : 0.023119, loss_ce: 0.009203
2022-01-08 10:45:53,518 iteration 2689 : loss : 0.021097, loss_ce: 0.008072
2022-01-08 10:45:55,054 iteration 2690 : loss : 0.032886, loss_ce: 0.012995
2022-01-08 10:45:56,623 iteration 2691 : loss : 0.020033, loss_ce: 0.006554
2022-01-08 10:45:58,171 iteration 2692 : loss : 0.026804, loss_ce: 0.008548
2022-01-08 10:45:59,689 iteration 2693 : loss : 0.028061, loss_ce: 0.011017
2022-01-08 10:46:01,156 iteration 2694 : loss : 0.027085, loss_ce: 0.009264
2022-01-08 10:46:02,755 iteration 2695 : loss : 0.040513, loss_ce: 0.015111
2022-01-08 10:46:04,411 iteration 2696 : loss : 0.038114, loss_ce: 0.014266
2022-01-08 10:46:06,027 iteration 2697 : loss : 0.027589, loss_ce: 0.015294
2022-01-08 10:46:07,528 iteration 2698 : loss : 0.019153, loss_ce: 0.007145
2022-01-08 10:46:09,045 iteration 2699 : loss : 0.022109, loss_ce: 0.007353
2022-01-08 10:46:10,561 iteration 2700 : loss : 0.027104, loss_ce: 0.008077
2022-01-08 10:46:12,119 iteration 2701 : loss : 0.028995, loss_ce: 0.013771
2022-01-08 10:46:13,708 iteration 2702 : loss : 0.037407, loss_ce: 0.007849
2022-01-08 10:46:15,302 iteration 2703 : loss : 0.037094, loss_ce: 0.011768
 40%|██████████▋                | 159/400 [1:18:05<1:50:34, 27.53s/it]2022-01-08 10:46:16,846 iteration 2704 : loss : 0.021166, loss_ce: 0.006602
2022-01-08 10:46:18,349 iteration 2705 : loss : 0.032050, loss_ce: 0.009322
2022-01-08 10:46:19,949 iteration 2706 : loss : 0.036042, loss_ce: 0.017779
2022-01-08 10:46:21,500 iteration 2707 : loss : 0.026186, loss_ce: 0.009751
2022-01-08 10:46:23,033 iteration 2708 : loss : 0.031419, loss_ce: 0.012767
2022-01-08 10:46:24,655 iteration 2709 : loss : 0.030193, loss_ce: 0.009711
2022-01-08 10:46:26,260 iteration 2710 : loss : 0.030685, loss_ce: 0.012502
2022-01-08 10:46:27,791 iteration 2711 : loss : 0.020812, loss_ce: 0.007130
2022-01-08 10:46:29,335 iteration 2712 : loss : 0.029321, loss_ce: 0.011478
2022-01-08 10:46:30,902 iteration 2713 : loss : 0.031079, loss_ce: 0.011170
2022-01-08 10:46:32,461 iteration 2714 : loss : 0.028995, loss_ce: 0.012092
2022-01-08 10:46:34,008 iteration 2715 : loss : 0.025680, loss_ce: 0.014987
2022-01-08 10:46:35,548 iteration 2716 : loss : 0.033589, loss_ce: 0.008473
2022-01-08 10:46:37,041 iteration 2717 : loss : 0.024386, loss_ce: 0.008600
2022-01-08 10:46:38,626 iteration 2718 : loss : 0.028575, loss_ce: 0.010325
2022-01-08 10:46:40,150 iteration 2719 : loss : 0.024675, loss_ce: 0.011524
2022-01-08 10:46:40,150 Training Data Eval:
2022-01-08 10:46:48,034   Average segmentation loss on training set: 0.0191
2022-01-08 10:46:48,034 Validation Data Eval:
2022-01-08 10:46:50,749   Average segmentation loss on validation set: 0.0657
2022-01-08 10:46:52,289 iteration 2720 : loss : 0.027015, loss_ce: 0.012264
 40%|██████████▊                | 160/400 [1:18:42<2:01:27, 30.36s/it]2022-01-08 10:46:53,890 iteration 2721 : loss : 0.025429, loss_ce: 0.009264
2022-01-08 10:46:55,473 iteration 2722 : loss : 0.024299, loss_ce: 0.013048
2022-01-08 10:46:57,104 iteration 2723 : loss : 0.027952, loss_ce: 0.012158
2022-01-08 10:46:58,605 iteration 2724 : loss : 0.034218, loss_ce: 0.011779
2022-01-08 10:47:00,139 iteration 2725 : loss : 0.033529, loss_ce: 0.011110
2022-01-08 10:47:01,759 iteration 2726 : loss : 0.019972, loss_ce: 0.007328
2022-01-08 10:47:03,310 iteration 2727 : loss : 0.028990, loss_ce: 0.009972
2022-01-08 10:47:04,907 iteration 2728 : loss : 0.029432, loss_ce: 0.010489
2022-01-08 10:47:06,495 iteration 2729 : loss : 0.046956, loss_ce: 0.021629
2022-01-08 10:47:08,121 iteration 2730 : loss : 0.039955, loss_ce: 0.021286
2022-01-08 10:47:09,669 iteration 2731 : loss : 0.039860, loss_ce: 0.014391
2022-01-08 10:47:11,136 iteration 2732 : loss : 0.022583, loss_ce: 0.008390
2022-01-08 10:47:12,682 iteration 2733 : loss : 0.018863, loss_ce: 0.008718
2022-01-08 10:47:14,247 iteration 2734 : loss : 0.055039, loss_ce: 0.013325
2022-01-08 10:47:15,812 iteration 2735 : loss : 0.026775, loss_ce: 0.013592
2022-01-08 10:47:17,350 iteration 2736 : loss : 0.026966, loss_ce: 0.008353
2022-01-08 10:47:18,891 iteration 2737 : loss : 0.029357, loss_ce: 0.007052
 40%|██████████▊                | 161/400 [1:19:09<1:56:27, 29.24s/it]2022-01-08 10:47:20,530 iteration 2738 : loss : 0.031173, loss_ce: 0.015278
2022-01-08 10:47:22,036 iteration 2739 : loss : 0.026851, loss_ce: 0.007844
2022-01-08 10:47:23,678 iteration 2740 : loss : 0.039167, loss_ce: 0.014093
2022-01-08 10:47:25,232 iteration 2741 : loss : 0.033405, loss_ce: 0.014862
2022-01-08 10:47:26,798 iteration 2742 : loss : 0.026974, loss_ce: 0.010738
2022-01-08 10:47:28,299 iteration 2743 : loss : 0.024188, loss_ce: 0.008541
2022-01-08 10:47:29,849 iteration 2744 : loss : 0.024647, loss_ce: 0.008457
2022-01-08 10:47:31,362 iteration 2745 : loss : 0.019777, loss_ce: 0.006091
2022-01-08 10:47:33,015 iteration 2746 : loss : 0.050252, loss_ce: 0.014117
2022-01-08 10:47:34,545 iteration 2747 : loss : 0.025366, loss_ce: 0.010547
2022-01-08 10:47:36,144 iteration 2748 : loss : 0.024638, loss_ce: 0.006557
2022-01-08 10:47:37,775 iteration 2749 : loss : 0.035206, loss_ce: 0.012791
2022-01-08 10:47:39,321 iteration 2750 : loss : 0.036614, loss_ce: 0.015137
2022-01-08 10:47:40,876 iteration 2751 : loss : 0.050441, loss_ce: 0.015075
2022-01-08 10:47:42,532 iteration 2752 : loss : 0.035995, loss_ce: 0.013865
2022-01-08 10:47:44,195 iteration 2753 : loss : 0.035109, loss_ce: 0.014842
2022-01-08 10:47:45,771 iteration 2754 : loss : 0.020374, loss_ce: 0.010513
 40%|██████████▉                | 162/400 [1:19:36<1:53:09, 28.53s/it]2022-01-08 10:47:47,244 iteration 2755 : loss : 0.023348, loss_ce: 0.007132
2022-01-08 10:47:48,813 iteration 2756 : loss : 0.030137, loss_ce: 0.011580
2022-01-08 10:47:50,368 iteration 2757 : loss : 0.029461, loss_ce: 0.011822
2022-01-08 10:47:51,920 iteration 2758 : loss : 0.033014, loss_ce: 0.012052
2022-01-08 10:47:53,472 iteration 2759 : loss : 0.036140, loss_ce: 0.014923
2022-01-08 10:47:55,028 iteration 2760 : loss : 0.026862, loss_ce: 0.012637
2022-01-08 10:47:56,624 iteration 2761 : loss : 0.034165, loss_ce: 0.013379
2022-01-08 10:47:58,175 iteration 2762 : loss : 0.020001, loss_ce: 0.008370
2022-01-08 10:47:59,761 iteration 2763 : loss : 0.045369, loss_ce: 0.012655
2022-01-08 10:48:01,258 iteration 2764 : loss : 0.018807, loss_ce: 0.006735
2022-01-08 10:48:02,916 iteration 2765 : loss : 0.032529, loss_ce: 0.013824
2022-01-08 10:48:04,403 iteration 2766 : loss : 0.028106, loss_ce: 0.009387
2022-01-08 10:48:05,972 iteration 2767 : loss : 0.036659, loss_ce: 0.013576
2022-01-08 10:48:07,504 iteration 2768 : loss : 0.029032, loss_ce: 0.009134
2022-01-08 10:48:08,991 iteration 2769 : loss : 0.024800, loss_ce: 0.010927
2022-01-08 10:48:10,513 iteration 2770 : loss : 0.031289, loss_ce: 0.014275
2022-01-08 10:48:12,063 iteration 2771 : loss : 0.022089, loss_ce: 0.009714
 41%|███████████                | 163/400 [1:20:02<1:50:02, 27.86s/it]2022-01-08 10:48:13,589 iteration 2772 : loss : 0.033429, loss_ce: 0.010509
2022-01-08 10:48:15,155 iteration 2773 : loss : 0.026094, loss_ce: 0.010159
2022-01-08 10:48:16,718 iteration 2774 : loss : 0.033820, loss_ce: 0.007542
2022-01-08 10:48:18,286 iteration 2775 : loss : 0.062679, loss_ce: 0.034188
2022-01-08 10:48:19,822 iteration 2776 : loss : 0.026328, loss_ce: 0.009964
2022-01-08 10:48:21,434 iteration 2777 : loss : 0.019187, loss_ce: 0.007012
2022-01-08 10:48:22,947 iteration 2778 : loss : 0.024523, loss_ce: 0.008977
2022-01-08 10:48:24,467 iteration 2779 : loss : 0.023251, loss_ce: 0.009643
2022-01-08 10:48:26,009 iteration 2780 : loss : 0.023007, loss_ce: 0.011877
2022-01-08 10:48:27,677 iteration 2781 : loss : 0.029393, loss_ce: 0.011239
2022-01-08 10:48:29,308 iteration 2782 : loss : 0.029101, loss_ce: 0.012020
2022-01-08 10:48:30,856 iteration 2783 : loss : 0.030957, loss_ce: 0.009643
2022-01-08 10:48:32,308 iteration 2784 : loss : 0.026383, loss_ce: 0.007418
2022-01-08 10:48:33,812 iteration 2785 : loss : 0.021858, loss_ce: 0.009533
2022-01-08 10:48:35,305 iteration 2786 : loss : 0.023813, loss_ce: 0.009845
2022-01-08 10:48:36,817 iteration 2787 : loss : 0.024314, loss_ce: 0.009113
2022-01-08 10:48:38,499 iteration 2788 : loss : 0.031610, loss_ce: 0.011995
 41%|███████████                | 164/400 [1:20:28<1:47:54, 27.43s/it]2022-01-08 10:48:40,116 iteration 2789 : loss : 0.021565, loss_ce: 0.007050
2022-01-08 10:48:41,756 iteration 2790 : loss : 0.036352, loss_ce: 0.016818
2022-01-08 10:48:43,253 iteration 2791 : loss : 0.022012, loss_ce: 0.007641
2022-01-08 10:48:44,793 iteration 2792 : loss : 0.023291, loss_ce: 0.008886
2022-01-08 10:48:46,346 iteration 2793 : loss : 0.024673, loss_ce: 0.008755
2022-01-08 10:48:47,955 iteration 2794 : loss : 0.027857, loss_ce: 0.013287
2022-01-08 10:48:49,503 iteration 2795 : loss : 0.027875, loss_ce: 0.010293
2022-01-08 10:48:51,028 iteration 2796 : loss : 0.022669, loss_ce: 0.008540
2022-01-08 10:48:52,624 iteration 2797 : loss : 0.034208, loss_ce: 0.015170
2022-01-08 10:48:54,202 iteration 2798 : loss : 0.029009, loss_ce: 0.008509
2022-01-08 10:48:55,849 iteration 2799 : loss : 0.035449, loss_ce: 0.012363
2022-01-08 10:48:57,425 iteration 2800 : loss : 0.023492, loss_ce: 0.010932
2022-01-08 10:48:58,899 iteration 2801 : loss : 0.027174, loss_ce: 0.007553
2022-01-08 10:49:00,415 iteration 2802 : loss : 0.018102, loss_ce: 0.005792
2022-01-08 10:49:01,989 iteration 2803 : loss : 0.031733, loss_ce: 0.011700
2022-01-08 10:49:03,461 iteration 2804 : loss : 0.033379, loss_ce: 0.012288
2022-01-08 10:49:03,461 Training Data Eval:
2022-01-08 10:49:11,353   Average segmentation loss on training set: 0.0177
2022-01-08 10:49:11,354 Validation Data Eval:
2022-01-08 10:49:14,060   Average segmentation loss on validation set: 0.0642
2022-01-08 10:49:20,033 Found new lowest validation loss at iteration 2804! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 10:49:21,588 iteration 2805 : loss : 0.026096, loss_ce: 0.009777
 41%|███████████▏               | 165/400 [1:21:11<2:05:50, 32.13s/it]2022-01-08 10:49:23,134 iteration 2806 : loss : 0.028283, loss_ce: 0.012731
2022-01-08 10:49:24,743 iteration 2807 : loss : 0.033969, loss_ce: 0.010113
2022-01-08 10:49:26,349 iteration 2808 : loss : 0.031886, loss_ce: 0.009767
2022-01-08 10:49:27,931 iteration 2809 : loss : 0.029766, loss_ce: 0.010333
2022-01-08 10:49:29,421 iteration 2810 : loss : 0.033823, loss_ce: 0.011583
2022-01-08 10:49:31,023 iteration 2811 : loss : 0.024335, loss_ce: 0.011521
2022-01-08 10:49:32,516 iteration 2812 : loss : 0.016574, loss_ce: 0.006635
2022-01-08 10:49:34,138 iteration 2813 : loss : 0.033197, loss_ce: 0.015137
2022-01-08 10:49:35,713 iteration 2814 : loss : 0.026915, loss_ce: 0.010462
2022-01-08 10:49:37,269 iteration 2815 : loss : 0.023444, loss_ce: 0.011150
2022-01-08 10:49:38,824 iteration 2816 : loss : 0.033564, loss_ce: 0.011787
2022-01-08 10:49:40,459 iteration 2817 : loss : 0.038163, loss_ce: 0.008404
2022-01-08 10:49:42,081 iteration 2818 : loss : 0.034031, loss_ce: 0.013102
2022-01-08 10:49:43,623 iteration 2819 : loss : 0.024726, loss_ce: 0.010651
2022-01-08 10:49:45,283 iteration 2820 : loss : 0.040111, loss_ce: 0.010480
2022-01-08 10:49:46,917 iteration 2821 : loss : 0.038500, loss_ce: 0.013834
2022-01-08 10:49:48,517 iteration 2822 : loss : 0.029554, loss_ce: 0.011511
 42%|███████████▏               | 166/400 [1:21:38<1:59:13, 30.57s/it]2022-01-08 10:49:50,221 iteration 2823 : loss : 0.037001, loss_ce: 0.015330
2022-01-08 10:49:51,755 iteration 2824 : loss : 0.024850, loss_ce: 0.012820
2022-01-08 10:49:53,328 iteration 2825 : loss : 0.024587, loss_ce: 0.009746
2022-01-08 10:49:54,993 iteration 2826 : loss : 0.052060, loss_ce: 0.021280
2022-01-08 10:49:56,684 iteration 2827 : loss : 0.036050, loss_ce: 0.013161
2022-01-08 10:49:58,284 iteration 2828 : loss : 0.033301, loss_ce: 0.010778
2022-01-08 10:49:59,853 iteration 2829 : loss : 0.033905, loss_ce: 0.016178
2022-01-08 10:50:01,411 iteration 2830 : loss : 0.027168, loss_ce: 0.009520
2022-01-08 10:50:02,880 iteration 2831 : loss : 0.023287, loss_ce: 0.010330
2022-01-08 10:50:04,578 iteration 2832 : loss : 0.048840, loss_ce: 0.019281
2022-01-08 10:50:06,138 iteration 2833 : loss : 0.033314, loss_ce: 0.013333
2022-01-08 10:50:07,779 iteration 2834 : loss : 0.045730, loss_ce: 0.019122
2022-01-08 10:50:09,387 iteration 2835 : loss : 0.040760, loss_ce: 0.012016
2022-01-08 10:50:10,957 iteration 2836 : loss : 0.037506, loss_ce: 0.014219
2022-01-08 10:50:12,584 iteration 2837 : loss : 0.034962, loss_ce: 0.013767
2022-01-08 10:50:14,091 iteration 2838 : loss : 0.022518, loss_ce: 0.009600
2022-01-08 10:50:15,691 iteration 2839 : loss : 0.024029, loss_ce: 0.010124
 42%|███████████▎               | 167/400 [1:22:05<1:54:45, 29.55s/it]2022-01-08 10:50:17,359 iteration 2840 : loss : 0.031049, loss_ce: 0.010709
2022-01-08 10:50:18,916 iteration 2841 : loss : 0.025018, loss_ce: 0.007432
2022-01-08 10:50:20,475 iteration 2842 : loss : 0.026595, loss_ce: 0.010737
2022-01-08 10:50:22,089 iteration 2843 : loss : 0.024991, loss_ce: 0.007320
2022-01-08 10:50:23,671 iteration 2844 : loss : 0.029255, loss_ce: 0.011283
2022-01-08 10:50:25,231 iteration 2845 : loss : 0.030957, loss_ce: 0.011585
2022-01-08 10:50:26,801 iteration 2846 : loss : 0.022236, loss_ce: 0.007934
2022-01-08 10:50:28,244 iteration 2847 : loss : 0.017280, loss_ce: 0.008849
2022-01-08 10:50:29,818 iteration 2848 : loss : 0.032703, loss_ce: 0.010738
2022-01-08 10:50:31,435 iteration 2849 : loss : 0.031510, loss_ce: 0.011175
2022-01-08 10:50:33,037 iteration 2850 : loss : 0.027687, loss_ce: 0.010795
2022-01-08 10:50:34,653 iteration 2851 : loss : 0.030808, loss_ce: 0.011656
2022-01-08 10:50:36,237 iteration 2852 : loss : 0.051283, loss_ce: 0.012420
2022-01-08 10:50:37,864 iteration 2853 : loss : 0.023965, loss_ce: 0.010551
2022-01-08 10:50:39,457 iteration 2854 : loss : 0.032486, loss_ce: 0.013942
2022-01-08 10:50:40,992 iteration 2855 : loss : 0.023453, loss_ce: 0.011215
2022-01-08 10:50:42,593 iteration 2856 : loss : 0.040208, loss_ce: 0.020877
 42%|███████████▎               | 168/400 [1:22:32<1:51:11, 28.76s/it]2022-01-08 10:50:44,113 iteration 2857 : loss : 0.023869, loss_ce: 0.007623
2022-01-08 10:50:45,683 iteration 2858 : loss : 0.021445, loss_ce: 0.009007
2022-01-08 10:50:47,213 iteration 2859 : loss : 0.018634, loss_ce: 0.007664
2022-01-08 10:50:48,814 iteration 2860 : loss : 0.037432, loss_ce: 0.011494
2022-01-08 10:50:50,295 iteration 2861 : loss : 0.025415, loss_ce: 0.010992
2022-01-08 10:50:51,944 iteration 2862 : loss : 0.049015, loss_ce: 0.015548
2022-01-08 10:50:53,462 iteration 2863 : loss : 0.043265, loss_ce: 0.011525
2022-01-08 10:50:55,054 iteration 2864 : loss : 0.028401, loss_ce: 0.011246
2022-01-08 10:50:56,657 iteration 2865 : loss : 0.038279, loss_ce: 0.014478
2022-01-08 10:50:58,202 iteration 2866 : loss : 0.029312, loss_ce: 0.011394
2022-01-08 10:50:59,869 iteration 2867 : loss : 0.036219, loss_ce: 0.013292
2022-01-08 10:51:01,398 iteration 2868 : loss : 0.028591, loss_ce: 0.010818
2022-01-08 10:51:03,046 iteration 2869 : loss : 0.035688, loss_ce: 0.015082
2022-01-08 10:51:04,643 iteration 2870 : loss : 0.024581, loss_ce: 0.008659
2022-01-08 10:51:06,174 iteration 2871 : loss : 0.021767, loss_ce: 0.008943
2022-01-08 10:51:07,666 iteration 2872 : loss : 0.027823, loss_ce: 0.010707
2022-01-08 10:51:09,278 iteration 2873 : loss : 0.034322, loss_ce: 0.009901
 42%|███████████▍               | 169/400 [1:22:59<1:48:19, 28.14s/it]2022-01-08 10:51:10,919 iteration 2874 : loss : 0.030398, loss_ce: 0.008840
2022-01-08 10:51:12,502 iteration 2875 : loss : 0.027168, loss_ce: 0.011967
2022-01-08 10:51:14,073 iteration 2876 : loss : 0.033335, loss_ce: 0.013683
2022-01-08 10:51:15,693 iteration 2877 : loss : 0.040647, loss_ce: 0.015427
2022-01-08 10:51:17,262 iteration 2878 : loss : 0.027770, loss_ce: 0.012815
2022-01-08 10:51:18,877 iteration 2879 : loss : 0.027029, loss_ce: 0.008411
2022-01-08 10:51:20,505 iteration 2880 : loss : 0.030853, loss_ce: 0.010503
2022-01-08 10:51:22,082 iteration 2881 : loss : 0.024669, loss_ce: 0.009427
2022-01-08 10:51:23,719 iteration 2882 : loss : 0.031537, loss_ce: 0.014837
2022-01-08 10:51:25,266 iteration 2883 : loss : 0.053567, loss_ce: 0.026159
2022-01-08 10:51:26,818 iteration 2884 : loss : 0.028765, loss_ce: 0.011972
2022-01-08 10:51:28,405 iteration 2885 : loss : 0.029346, loss_ce: 0.012195
2022-01-08 10:51:29,895 iteration 2886 : loss : 0.036796, loss_ce: 0.013306
2022-01-08 10:51:31,533 iteration 2887 : loss : 0.032308, loss_ce: 0.016524
2022-01-08 10:51:33,118 iteration 2888 : loss : 0.056506, loss_ce: 0.018197
2022-01-08 10:51:34,792 iteration 2889 : loss : 0.033764, loss_ce: 0.012448
2022-01-08 10:51:34,792 Training Data Eval:
2022-01-08 10:51:42,681   Average segmentation loss on training set: 0.0320
2022-01-08 10:51:42,681 Validation Data Eval:
2022-01-08 10:51:45,393   Average segmentation loss on validation set: 0.1600
2022-01-08 10:51:47,025 iteration 2890 : loss : 0.027889, loss_ce: 0.010005
 42%|███████████▍               | 170/400 [1:23:37<1:58:53, 31.02s/it]2022-01-08 10:51:48,676 iteration 2891 : loss : 0.036016, loss_ce: 0.016508
2022-01-08 10:51:50,337 iteration 2892 : loss : 0.028875, loss_ce: 0.007647
2022-01-08 10:51:51,869 iteration 2893 : loss : 0.032471, loss_ce: 0.010223
2022-01-08 10:51:53,513 iteration 2894 : loss : 0.034766, loss_ce: 0.010161
2022-01-08 10:51:55,144 iteration 2895 : loss : 0.036283, loss_ce: 0.016911
2022-01-08 10:51:56,683 iteration 2896 : loss : 0.020937, loss_ce: 0.007007
2022-01-08 10:51:58,256 iteration 2897 : loss : 0.042289, loss_ce: 0.018645
2022-01-08 10:51:59,767 iteration 2898 : loss : 0.026912, loss_ce: 0.011581
2022-01-08 10:52:01,373 iteration 2899 : loss : 0.033303, loss_ce: 0.011215
2022-01-08 10:52:02,936 iteration 2900 : loss : 0.021156, loss_ce: 0.006905
2022-01-08 10:52:04,513 iteration 2901 : loss : 0.027866, loss_ce: 0.008280
2022-01-08 10:52:06,015 iteration 2902 : loss : 0.032841, loss_ce: 0.011802
2022-01-08 10:52:07,545 iteration 2903 : loss : 0.025614, loss_ce: 0.012833
2022-01-08 10:52:09,070 iteration 2904 : loss : 0.019953, loss_ce: 0.008115
2022-01-08 10:52:10,565 iteration 2905 : loss : 0.028568, loss_ce: 0.014666
2022-01-08 10:52:12,166 iteration 2906 : loss : 0.033294, loss_ce: 0.017013
2022-01-08 10:52:13,700 iteration 2907 : loss : 0.029008, loss_ce: 0.011303
 43%|███████████▌               | 171/400 [1:24:03<1:53:25, 29.72s/it]2022-01-08 10:52:15,347 iteration 2908 : loss : 0.035025, loss_ce: 0.017172
2022-01-08 10:52:16,982 iteration 2909 : loss : 0.039102, loss_ce: 0.022465
2022-01-08 10:52:18,569 iteration 2910 : loss : 0.032050, loss_ce: 0.011921
2022-01-08 10:52:20,195 iteration 2911 : loss : 0.027065, loss_ce: 0.009437
2022-01-08 10:52:21,739 iteration 2912 : loss : 0.027481, loss_ce: 0.010365
2022-01-08 10:52:23,382 iteration 2913 : loss : 0.038695, loss_ce: 0.018904
2022-01-08 10:52:24,928 iteration 2914 : loss : 0.031243, loss_ce: 0.011955
2022-01-08 10:52:26,479 iteration 2915 : loss : 0.022302, loss_ce: 0.009252
2022-01-08 10:52:28,066 iteration 2916 : loss : 0.021092, loss_ce: 0.008202
2022-01-08 10:52:29,568 iteration 2917 : loss : 0.021625, loss_ce: 0.009118
2022-01-08 10:52:31,101 iteration 2918 : loss : 0.022827, loss_ce: 0.005748
2022-01-08 10:52:32,673 iteration 2919 : loss : 0.029450, loss_ce: 0.008121
2022-01-08 10:52:34,272 iteration 2920 : loss : 0.027490, loss_ce: 0.010882
2022-01-08 10:52:35,834 iteration 2921 : loss : 0.028702, loss_ce: 0.012481
2022-01-08 10:52:37,331 iteration 2922 : loss : 0.035937, loss_ce: 0.012671
2022-01-08 10:52:38,875 iteration 2923 : loss : 0.033056, loss_ce: 0.010834
2022-01-08 10:52:40,452 iteration 2924 : loss : 0.042403, loss_ce: 0.013793
 43%|███████████▌               | 172/400 [1:24:30<1:49:32, 28.83s/it]2022-01-08 10:52:41,962 iteration 2925 : loss : 0.020176, loss_ce: 0.010285
2022-01-08 10:52:43,623 iteration 2926 : loss : 0.027997, loss_ce: 0.012541
2022-01-08 10:52:45,188 iteration 2927 : loss : 0.023561, loss_ce: 0.008981
2022-01-08 10:52:46,639 iteration 2928 : loss : 0.023435, loss_ce: 0.007460
2022-01-08 10:52:48,178 iteration 2929 : loss : 0.022716, loss_ce: 0.009953
2022-01-08 10:52:49,830 iteration 2930 : loss : 0.041618, loss_ce: 0.017450
2022-01-08 10:52:51,376 iteration 2931 : loss : 0.027864, loss_ce: 0.010699
2022-01-08 10:52:52,893 iteration 2932 : loss : 0.025117, loss_ce: 0.009023
2022-01-08 10:52:54,535 iteration 2933 : loss : 0.032273, loss_ce: 0.009253
2022-01-08 10:52:56,103 iteration 2934 : loss : 0.027881, loss_ce: 0.009083
2022-01-08 10:52:57,685 iteration 2935 : loss : 0.022479, loss_ce: 0.007574
2022-01-08 10:52:59,247 iteration 2936 : loss : 0.021667, loss_ce: 0.007863
2022-01-08 10:53:00,837 iteration 2937 : loss : 0.024573, loss_ce: 0.011705
2022-01-08 10:53:02,460 iteration 2938 : loss : 0.025964, loss_ce: 0.007513
2022-01-08 10:53:04,082 iteration 2939 : loss : 0.027185, loss_ce: 0.008472
2022-01-08 10:53:05,758 iteration 2940 : loss : 0.042519, loss_ce: 0.020804
2022-01-08 10:53:07,280 iteration 2941 : loss : 0.024598, loss_ce: 0.007853
 43%|███████████▋               | 173/400 [1:24:57<1:46:47, 28.23s/it]2022-01-08 10:53:08,835 iteration 2942 : loss : 0.019648, loss_ce: 0.006117
2022-01-08 10:53:10,438 iteration 2943 : loss : 0.031269, loss_ce: 0.008384
2022-01-08 10:53:11,970 iteration 2944 : loss : 0.023398, loss_ce: 0.011838
2022-01-08 10:53:13,547 iteration 2945 : loss : 0.022603, loss_ce: 0.006500
2022-01-08 10:53:15,014 iteration 2946 : loss : 0.022695, loss_ce: 0.009326
2022-01-08 10:53:16,550 iteration 2947 : loss : 0.021531, loss_ce: 0.008504
2022-01-08 10:53:18,075 iteration 2948 : loss : 0.022365, loss_ce: 0.006342
2022-01-08 10:53:19,703 iteration 2949 : loss : 0.028794, loss_ce: 0.010484
2022-01-08 10:53:21,291 iteration 2950 : loss : 0.022087, loss_ce: 0.009268
2022-01-08 10:53:22,904 iteration 2951 : loss : 0.022521, loss_ce: 0.009772
2022-01-08 10:53:24,507 iteration 2952 : loss : 0.019713, loss_ce: 0.006120
2022-01-08 10:53:26,147 iteration 2953 : loss : 0.028364, loss_ce: 0.014341
2022-01-08 10:53:27,825 iteration 2954 : loss : 0.038683, loss_ce: 0.010236
2022-01-08 10:53:29,362 iteration 2955 : loss : 0.022723, loss_ce: 0.010089
2022-01-08 10:53:30,906 iteration 2956 : loss : 0.018628, loss_ce: 0.007406
2022-01-08 10:53:32,421 iteration 2957 : loss : 0.027875, loss_ce: 0.009611
2022-01-08 10:53:34,002 iteration 2958 : loss : 0.023952, loss_ce: 0.011954
 44%|███████████▋               | 174/400 [1:25:24<1:44:36, 27.77s/it]2022-01-08 10:53:35,587 iteration 2959 : loss : 0.023524, loss_ce: 0.007683
2022-01-08 10:53:37,131 iteration 2960 : loss : 0.030004, loss_ce: 0.014425
2022-01-08 10:53:38,703 iteration 2961 : loss : 0.028633, loss_ce: 0.011112
2022-01-08 10:53:40,417 iteration 2962 : loss : 0.023322, loss_ce: 0.007732
2022-01-08 10:53:42,004 iteration 2963 : loss : 0.046376, loss_ce: 0.015979
2022-01-08 10:53:43,478 iteration 2964 : loss : 0.027939, loss_ce: 0.009014
2022-01-08 10:53:45,050 iteration 2965 : loss : 0.023781, loss_ce: 0.011897
2022-01-08 10:53:46,656 iteration 2966 : loss : 0.025693, loss_ce: 0.008380
2022-01-08 10:53:48,209 iteration 2967 : loss : 0.027144, loss_ce: 0.010410
2022-01-08 10:53:49,780 iteration 2968 : loss : 0.036626, loss_ce: 0.019342
2022-01-08 10:53:51,320 iteration 2969 : loss : 0.030119, loss_ce: 0.011711
2022-01-08 10:53:52,900 iteration 2970 : loss : 0.041184, loss_ce: 0.016705
2022-01-08 10:53:54,448 iteration 2971 : loss : 0.021250, loss_ce: 0.007756
2022-01-08 10:53:56,000 iteration 2972 : loss : 0.031265, loss_ce: 0.014800
2022-01-08 10:53:57,513 iteration 2973 : loss : 0.026333, loss_ce: 0.006583
2022-01-08 10:53:59,125 iteration 2974 : loss : 0.028248, loss_ce: 0.008444
2022-01-08 10:53:59,125 Training Data Eval:
2022-01-08 10:54:07,010   Average segmentation loss on training set: 0.0180
2022-01-08 10:54:07,010 Validation Data Eval:
2022-01-08 10:54:09,729   Average segmentation loss on validation set: 0.0856
2022-01-08 10:54:11,264 iteration 2975 : loss : 0.019156, loss_ce: 0.007295
 44%|███████████▊               | 175/400 [1:26:01<1:54:49, 30.62s/it]2022-01-08 10:54:12,781 iteration 2976 : loss : 0.018927, loss_ce: 0.008067
2022-01-08 10:54:14,374 iteration 2977 : loss : 0.024935, loss_ce: 0.011319
2022-01-08 10:54:15,950 iteration 2978 : loss : 0.020228, loss_ce: 0.006427
2022-01-08 10:54:17,562 iteration 2979 : loss : 0.021771, loss_ce: 0.007422
2022-01-08 10:54:19,163 iteration 2980 : loss : 0.027871, loss_ce: 0.008252
2022-01-08 10:54:20,825 iteration 2981 : loss : 0.034754, loss_ce: 0.015019
2022-01-08 10:54:22,396 iteration 2982 : loss : 0.017778, loss_ce: 0.006624
2022-01-08 10:54:23,973 iteration 2983 : loss : 0.024956, loss_ce: 0.009165
2022-01-08 10:54:25,548 iteration 2984 : loss : 0.028687, loss_ce: 0.009626
2022-01-08 10:54:27,147 iteration 2985 : loss : 0.044742, loss_ce: 0.015342
2022-01-08 10:54:28,652 iteration 2986 : loss : 0.025177, loss_ce: 0.011072
2022-01-08 10:54:30,219 iteration 2987 : loss : 0.031532, loss_ce: 0.010022
2022-01-08 10:54:31,829 iteration 2988 : loss : 0.024186, loss_ce: 0.009511
2022-01-08 10:54:33,431 iteration 2989 : loss : 0.027234, loss_ce: 0.009652
2022-01-08 10:54:34,936 iteration 2990 : loss : 0.016758, loss_ce: 0.007045
2022-01-08 10:54:36,495 iteration 2991 : loss : 0.019040, loss_ce: 0.007229
2022-01-08 10:54:38,124 iteration 2992 : loss : 0.025127, loss_ce: 0.011035
 44%|███████████▉               | 176/400 [1:26:28<1:50:06, 29.49s/it]2022-01-08 10:54:39,672 iteration 2993 : loss : 0.022236, loss_ce: 0.011014
2022-01-08 10:54:41,250 iteration 2994 : loss : 0.046319, loss_ce: 0.011268
2022-01-08 10:54:42,791 iteration 2995 : loss : 0.027563, loss_ce: 0.009112
2022-01-08 10:54:44,348 iteration 2996 : loss : 0.020814, loss_ce: 0.010150
2022-01-08 10:54:45,906 iteration 2997 : loss : 0.036772, loss_ce: 0.011349
2022-01-08 10:54:47,479 iteration 2998 : loss : 0.021905, loss_ce: 0.008106
2022-01-08 10:54:49,102 iteration 2999 : loss : 0.052080, loss_ce: 0.009548
2022-01-08 10:54:50,755 iteration 3000 : loss : 0.033773, loss_ce: 0.014859
2022-01-08 10:54:52,409 iteration 3001 : loss : 0.031887, loss_ce: 0.009290
2022-01-08 10:54:53,931 iteration 3002 : loss : 0.027613, loss_ce: 0.011107
2022-01-08 10:54:55,473 iteration 3003 : loss : 0.023645, loss_ce: 0.008061
2022-01-08 10:54:57,027 iteration 3004 : loss : 0.023108, loss_ce: 0.008944
2022-01-08 10:54:58,605 iteration 3005 : loss : 0.036035, loss_ce: 0.015624
2022-01-08 10:55:00,213 iteration 3006 : loss : 0.022449, loss_ce: 0.008216
2022-01-08 10:55:01,816 iteration 3007 : loss : 0.034287, loss_ce: 0.015925
2022-01-08 10:55:03,425 iteration 3008 : loss : 0.039789, loss_ce: 0.014972
2022-01-08 10:55:04,925 iteration 3009 : loss : 0.034354, loss_ce: 0.014615
 44%|███████████▉               | 177/400 [1:26:55<1:46:36, 28.69s/it]2022-01-08 10:55:06,650 iteration 3010 : loss : 0.039105, loss_ce: 0.014566
2022-01-08 10:55:08,251 iteration 3011 : loss : 0.022469, loss_ce: 0.010337
2022-01-08 10:55:09,766 iteration 3012 : loss : 0.020423, loss_ce: 0.008923
2022-01-08 10:55:11,263 iteration 3013 : loss : 0.033802, loss_ce: 0.010146
2022-01-08 10:55:12,831 iteration 3014 : loss : 0.029810, loss_ce: 0.012585
2022-01-08 10:55:14,386 iteration 3015 : loss : 0.035750, loss_ce: 0.009007
2022-01-08 10:55:15,937 iteration 3016 : loss : 0.035548, loss_ce: 0.014162
2022-01-08 10:55:17,470 iteration 3017 : loss : 0.025660, loss_ce: 0.013772
2022-01-08 10:55:19,000 iteration 3018 : loss : 0.023054, loss_ce: 0.009586
2022-01-08 10:55:20,499 iteration 3019 : loss : 0.021947, loss_ce: 0.006897
2022-01-08 10:55:22,091 iteration 3020 : loss : 0.039505, loss_ce: 0.013950
2022-01-08 10:55:23,666 iteration 3021 : loss : 0.016301, loss_ce: 0.005809
2022-01-08 10:55:25,210 iteration 3022 : loss : 0.019928, loss_ce: 0.007239
2022-01-08 10:55:26,783 iteration 3023 : loss : 0.036082, loss_ce: 0.011604
2022-01-08 10:55:28,431 iteration 3024 : loss : 0.031456, loss_ce: 0.012308
2022-01-08 10:55:30,050 iteration 3025 : loss : 0.033920, loss_ce: 0.014382
2022-01-08 10:55:31,570 iteration 3026 : loss : 0.024687, loss_ce: 0.010092
 44%|████████████               | 178/400 [1:27:21<1:43:51, 28.07s/it]2022-01-08 10:55:33,224 iteration 3027 : loss : 0.042293, loss_ce: 0.015840
2022-01-08 10:55:34,745 iteration 3028 : loss : 0.022202, loss_ce: 0.008717
2022-01-08 10:55:36,323 iteration 3029 : loss : 0.024293, loss_ce: 0.010101
2022-01-08 10:55:37,853 iteration 3030 : loss : 0.027086, loss_ce: 0.012121
2022-01-08 10:55:39,414 iteration 3031 : loss : 0.033312, loss_ce: 0.010856
2022-01-08 10:55:40,938 iteration 3032 : loss : 0.029269, loss_ce: 0.011251
2022-01-08 10:55:42,530 iteration 3033 : loss : 0.034851, loss_ce: 0.011568
2022-01-08 10:55:44,141 iteration 3034 : loss : 0.030490, loss_ce: 0.011514
2022-01-08 10:55:45,723 iteration 3035 : loss : 0.031669, loss_ce: 0.014281
2022-01-08 10:55:47,267 iteration 3036 : loss : 0.030207, loss_ce: 0.016600
2022-01-08 10:55:48,828 iteration 3037 : loss : 0.023676, loss_ce: 0.009418
2022-01-08 10:55:50,422 iteration 3038 : loss : 0.026538, loss_ce: 0.008530
2022-01-08 10:55:51,925 iteration 3039 : loss : 0.027221, loss_ce: 0.008186
2022-01-08 10:55:53,496 iteration 3040 : loss : 0.026798, loss_ce: 0.008067
2022-01-08 10:55:55,102 iteration 3041 : loss : 0.024140, loss_ce: 0.009178
2022-01-08 10:55:56,665 iteration 3042 : loss : 0.026315, loss_ce: 0.013075
2022-01-08 10:55:58,406 iteration 3043 : loss : 0.028632, loss_ce: 0.008282
 45%|████████████               | 179/400 [1:27:48<1:42:02, 27.70s/it]2022-01-08 10:56:00,028 iteration 3044 : loss : 0.029862, loss_ce: 0.013441
2022-01-08 10:56:01,541 iteration 3045 : loss : 0.025317, loss_ce: 0.008911
2022-01-08 10:56:03,080 iteration 3046 : loss : 0.021895, loss_ce: 0.007713
2022-01-08 10:56:04,749 iteration 3047 : loss : 0.031944, loss_ce: 0.017465
2022-01-08 10:56:06,276 iteration 3048 : loss : 0.024730, loss_ce: 0.008212
2022-01-08 10:56:07,844 iteration 3049 : loss : 0.032459, loss_ce: 0.012757
2022-01-08 10:56:09,440 iteration 3050 : loss : 0.033012, loss_ce: 0.016098
2022-01-08 10:56:11,015 iteration 3051 : loss : 0.038790, loss_ce: 0.012494
2022-01-08 10:56:12,568 iteration 3052 : loss : 0.023916, loss_ce: 0.008704
2022-01-08 10:56:14,217 iteration 3053 : loss : 0.020190, loss_ce: 0.008406
2022-01-08 10:56:15,739 iteration 3054 : loss : 0.027655, loss_ce: 0.008268
2022-01-08 10:56:17,325 iteration 3055 : loss : 0.027556, loss_ce: 0.010379
2022-01-08 10:56:18,864 iteration 3056 : loss : 0.018865, loss_ce: 0.008206
2022-01-08 10:56:20,367 iteration 3057 : loss : 0.027432, loss_ce: 0.011576
2022-01-08 10:56:21,944 iteration 3058 : loss : 0.022472, loss_ce: 0.008820
2022-01-08 10:56:23,546 iteration 3059 : loss : 0.021837, loss_ce: 0.006407
2022-01-08 10:56:23,547 Training Data Eval:
2022-01-08 10:56:31,435   Average segmentation loss on training set: 0.0182
2022-01-08 10:56:31,435 Validation Data Eval:
2022-01-08 10:56:34,159   Average segmentation loss on validation set: 0.0690
2022-01-08 10:56:35,690 iteration 3060 : loss : 0.023833, loss_ce: 0.011441
 45%|████████████▏              | 180/400 [1:28:25<1:52:07, 30.58s/it]2022-01-08 10:56:37,214 iteration 3061 : loss : 0.020717, loss_ce: 0.007450
2022-01-08 10:56:38,773 iteration 3062 : loss : 0.043255, loss_ce: 0.017211
2022-01-08 10:56:40,368 iteration 3063 : loss : 0.022655, loss_ce: 0.008479
2022-01-08 10:56:41,972 iteration 3064 : loss : 0.024373, loss_ce: 0.011323
2022-01-08 10:56:43,441 iteration 3065 : loss : 0.017845, loss_ce: 0.006022
2022-01-08 10:56:44,969 iteration 3066 : loss : 0.022782, loss_ce: 0.010296
2022-01-08 10:56:46,554 iteration 3067 : loss : 0.028201, loss_ce: 0.013045
2022-01-08 10:56:48,130 iteration 3068 : loss : 0.035759, loss_ce: 0.012153
2022-01-08 10:56:49,696 iteration 3069 : loss : 0.033891, loss_ce: 0.010747
2022-01-08 10:56:51,269 iteration 3070 : loss : 0.030993, loss_ce: 0.012089
2022-01-08 10:56:52,779 iteration 3071 : loss : 0.021965, loss_ce: 0.007956
2022-01-08 10:56:54,285 iteration 3072 : loss : 0.024772, loss_ce: 0.009269
2022-01-08 10:56:55,828 iteration 3073 : loss : 0.030251, loss_ce: 0.014011
2022-01-08 10:56:57,457 iteration 3074 : loss : 0.027647, loss_ce: 0.012253
2022-01-08 10:56:59,051 iteration 3075 : loss : 0.027493, loss_ce: 0.009374
2022-01-08 10:57:00,680 iteration 3076 : loss : 0.045526, loss_ce: 0.022057
2022-01-08 10:57:02,197 iteration 3077 : loss : 0.024001, loss_ce: 0.008784
 45%|████████████▏              | 181/400 [1:28:52<1:47:08, 29.35s/it]2022-01-08 10:57:03,743 iteration 3078 : loss : 0.021871, loss_ce: 0.008762
2022-01-08 10:57:05,411 iteration 3079 : loss : 0.023868, loss_ce: 0.008453
2022-01-08 10:57:06,900 iteration 3080 : loss : 0.024638, loss_ce: 0.008147
2022-01-08 10:57:08,488 iteration 3081 : loss : 0.031681, loss_ce: 0.011387
2022-01-08 10:57:10,032 iteration 3082 : loss : 0.035538, loss_ce: 0.009445
2022-01-08 10:57:11,640 iteration 3083 : loss : 0.030188, loss_ce: 0.014512
2022-01-08 10:57:13,089 iteration 3084 : loss : 0.023143, loss_ce: 0.009902
2022-01-08 10:57:14,667 iteration 3085 : loss : 0.041064, loss_ce: 0.017152
2022-01-08 10:57:16,322 iteration 3086 : loss : 0.022983, loss_ce: 0.012378
2022-01-08 10:57:17,864 iteration 3087 : loss : 0.024383, loss_ce: 0.009431
2022-01-08 10:57:19,440 iteration 3088 : loss : 0.027126, loss_ce: 0.008488
2022-01-08 10:57:21,105 iteration 3089 : loss : 0.036734, loss_ce: 0.014249
2022-01-08 10:57:22,597 iteration 3090 : loss : 0.024105, loss_ce: 0.010475
2022-01-08 10:57:24,168 iteration 3091 : loss : 0.030672, loss_ce: 0.013807
2022-01-08 10:57:25,885 iteration 3092 : loss : 0.033811, loss_ce: 0.013194
2022-01-08 10:57:27,421 iteration 3093 : loss : 0.021807, loss_ce: 0.009285
2022-01-08 10:57:29,002 iteration 3094 : loss : 0.034904, loss_ce: 0.015073
 46%|████████████▎              | 182/400 [1:29:19<1:43:53, 28.59s/it]2022-01-08 10:57:30,610 iteration 3095 : loss : 0.022101, loss_ce: 0.008484
2022-01-08 10:57:32,200 iteration 3096 : loss : 0.019243, loss_ce: 0.007755
2022-01-08 10:57:33,753 iteration 3097 : loss : 0.021314, loss_ce: 0.007180
2022-01-08 10:57:35,279 iteration 3098 : loss : 0.017762, loss_ce: 0.007129
2022-01-08 10:57:36,851 iteration 3099 : loss : 0.018810, loss_ce: 0.006107
2022-01-08 10:57:38,368 iteration 3100 : loss : 0.023519, loss_ce: 0.007421
2022-01-08 10:57:39,989 iteration 3101 : loss : 0.023244, loss_ce: 0.008793
2022-01-08 10:57:41,565 iteration 3102 : loss : 0.028936, loss_ce: 0.010269
2022-01-08 10:57:43,154 iteration 3103 : loss : 0.028955, loss_ce: 0.012234
2022-01-08 10:57:44,681 iteration 3104 : loss : 0.026515, loss_ce: 0.011479
2022-01-08 10:57:46,186 iteration 3105 : loss : 0.019464, loss_ce: 0.008875
2022-01-08 10:57:47,737 iteration 3106 : loss : 0.024228, loss_ce: 0.011512
2022-01-08 10:57:49,282 iteration 3107 : loss : 0.022976, loss_ce: 0.006957
2022-01-08 10:57:50,870 iteration 3108 : loss : 0.030264, loss_ce: 0.010161
2022-01-08 10:57:52,475 iteration 3109 : loss : 0.028381, loss_ce: 0.009152
2022-01-08 10:57:54,088 iteration 3110 : loss : 0.040171, loss_ce: 0.017023
2022-01-08 10:57:55,673 iteration 3111 : loss : 0.022427, loss_ce: 0.008318
 46%|████████████▎              | 183/400 [1:29:45<1:41:18, 28.01s/it]2022-01-08 10:57:57,301 iteration 3112 : loss : 0.027248, loss_ce: 0.009833
2022-01-08 10:57:58,830 iteration 3113 : loss : 0.019639, loss_ce: 0.008124
2022-01-08 10:58:00,442 iteration 3114 : loss : 0.027364, loss_ce: 0.009137
2022-01-08 10:58:02,020 iteration 3115 : loss : 0.022314, loss_ce: 0.010755
2022-01-08 10:58:03,582 iteration 3116 : loss : 0.022028, loss_ce: 0.010076
2022-01-08 10:58:05,219 iteration 3117 : loss : 0.020854, loss_ce: 0.007558
2022-01-08 10:58:06,764 iteration 3118 : loss : 0.027539, loss_ce: 0.012234
2022-01-08 10:58:08,429 iteration 3119 : loss : 0.034493, loss_ce: 0.011171
2022-01-08 10:58:09,945 iteration 3120 : loss : 0.023421, loss_ce: 0.008550
2022-01-08 10:58:11,574 iteration 3121 : loss : 0.024393, loss_ce: 0.011643
2022-01-08 10:58:13,118 iteration 3122 : loss : 0.030923, loss_ce: 0.008785
2022-01-08 10:58:14,733 iteration 3123 : loss : 0.021034, loss_ce: 0.008954
2022-01-08 10:58:16,221 iteration 3124 : loss : 0.026574, loss_ce: 0.009093
2022-01-08 10:58:17,773 iteration 3125 : loss : 0.030157, loss_ce: 0.008463
2022-01-08 10:58:19,387 iteration 3126 : loss : 0.023330, loss_ce: 0.011166
2022-01-08 10:58:21,016 iteration 3127 : loss : 0.024198, loss_ce: 0.008227
2022-01-08 10:58:22,637 iteration 3128 : loss : 0.024175, loss_ce: 0.008934
 46%|████████████▍              | 184/400 [1:30:12<1:39:43, 27.70s/it]2022-01-08 10:58:24,212 iteration 3129 : loss : 0.025165, loss_ce: 0.009134
2022-01-08 10:58:25,868 iteration 3130 : loss : 0.027292, loss_ce: 0.011286
2022-01-08 10:58:27,420 iteration 3131 : loss : 0.023550, loss_ce: 0.007261
2022-01-08 10:58:28,980 iteration 3132 : loss : 0.021504, loss_ce: 0.006312
2022-01-08 10:58:30,545 iteration 3133 : loss : 0.025461, loss_ce: 0.009706
2022-01-08 10:58:32,080 iteration 3134 : loss : 0.020682, loss_ce: 0.009477
2022-01-08 10:58:33,642 iteration 3135 : loss : 0.023330, loss_ce: 0.010571
2022-01-08 10:58:35,182 iteration 3136 : loss : 0.024382, loss_ce: 0.011018
2022-01-08 10:58:36,857 iteration 3137 : loss : 0.025083, loss_ce: 0.010144
2022-01-08 10:58:38,486 iteration 3138 : loss : 0.037894, loss_ce: 0.016506
2022-01-08 10:58:39,990 iteration 3139 : loss : 0.021611, loss_ce: 0.007311
2022-01-08 10:58:41,501 iteration 3140 : loss : 0.020883, loss_ce: 0.008635
2022-01-08 10:58:43,079 iteration 3141 : loss : 0.043971, loss_ce: 0.023424
2022-01-08 10:58:44,623 iteration 3142 : loss : 0.025471, loss_ce: 0.008938
2022-01-08 10:58:46,234 iteration 3143 : loss : 0.031972, loss_ce: 0.013197
2022-01-08 10:58:47,786 iteration 3144 : loss : 0.024332, loss_ce: 0.008625
2022-01-08 10:58:47,787 Training Data Eval:
2022-01-08 10:58:55,677   Average segmentation loss on training set: 0.0162
2022-01-08 10:58:55,678 Validation Data Eval:
2022-01-08 10:58:58,394   Average segmentation loss on validation set: 0.0635
2022-01-08 10:59:04,256 Found new lowest validation loss at iteration 3144! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 10:59:05,666 iteration 3145 : loss : 0.017842, loss_ce: 0.007133
 46%|████████████▍              | 185/400 [1:30:55<1:55:44, 32.30s/it]2022-01-08 10:59:07,285 iteration 3146 : loss : 0.041677, loss_ce: 0.012607
2022-01-08 10:59:08,873 iteration 3147 : loss : 0.032605, loss_ce: 0.016641
2022-01-08 10:59:10,377 iteration 3148 : loss : 0.018858, loss_ce: 0.006646
2022-01-08 10:59:11,980 iteration 3149 : loss : 0.040842, loss_ce: 0.012436
2022-01-08 10:59:13,581 iteration 3150 : loss : 0.022583, loss_ce: 0.006441
2022-01-08 10:59:15,179 iteration 3151 : loss : 0.053468, loss_ce: 0.019112
2022-01-08 10:59:16,769 iteration 3152 : loss : 0.039390, loss_ce: 0.012217
2022-01-08 10:59:18,345 iteration 3153 : loss : 0.033311, loss_ce: 0.015758
2022-01-08 10:59:19,988 iteration 3154 : loss : 0.024631, loss_ce: 0.008455
2022-01-08 10:59:21,590 iteration 3155 : loss : 0.021199, loss_ce: 0.007718
2022-01-08 10:59:23,099 iteration 3156 : loss : 0.024789, loss_ce: 0.008450
2022-01-08 10:59:24,680 iteration 3157 : loss : 0.025293, loss_ce: 0.010921
2022-01-08 10:59:26,202 iteration 3158 : loss : 0.047595, loss_ce: 0.010143
2022-01-08 10:59:27,795 iteration 3159 : loss : 0.027233, loss_ce: 0.014735
2022-01-08 10:59:29,391 iteration 3160 : loss : 0.024717, loss_ce: 0.009887
2022-01-08 10:59:31,034 iteration 3161 : loss : 0.026756, loss_ce: 0.009865
2022-01-08 10:59:32,611 iteration 3162 : loss : 0.025452, loss_ce: 0.008251
 46%|████████████▌              | 186/400 [1:31:22<1:49:28, 30.69s/it]2022-01-08 10:59:34,172 iteration 3163 : loss : 0.029850, loss_ce: 0.009451
2022-01-08 10:59:35,839 iteration 3164 : loss : 0.046124, loss_ce: 0.019422
2022-01-08 10:59:37,456 iteration 3165 : loss : 0.030496, loss_ce: 0.011946
2022-01-08 10:59:38,989 iteration 3166 : loss : 0.025983, loss_ce: 0.010090
2022-01-08 10:59:40,607 iteration 3167 : loss : 0.025959, loss_ce: 0.008707
2022-01-08 10:59:42,180 iteration 3168 : loss : 0.037060, loss_ce: 0.014062
2022-01-08 10:59:43,788 iteration 3169 : loss : 0.028047, loss_ce: 0.012832
2022-01-08 10:59:45,368 iteration 3170 : loss : 0.022182, loss_ce: 0.009692
2022-01-08 10:59:46,829 iteration 3171 : loss : 0.019777, loss_ce: 0.006156
2022-01-08 10:59:48,358 iteration 3172 : loss : 0.024740, loss_ce: 0.009382
2022-01-08 10:59:49,925 iteration 3173 : loss : 0.031032, loss_ce: 0.010622
2022-01-08 10:59:51,463 iteration 3174 : loss : 0.023330, loss_ce: 0.010109
2022-01-08 10:59:52,967 iteration 3175 : loss : 0.043873, loss_ce: 0.015474
2022-01-08 10:59:54,609 iteration 3176 : loss : 0.056698, loss_ce: 0.013165
2022-01-08 10:59:56,180 iteration 3177 : loss : 0.024382, loss_ce: 0.008866
2022-01-08 10:59:57,849 iteration 3178 : loss : 0.027888, loss_ce: 0.008781
2022-01-08 10:59:59,436 iteration 3179 : loss : 0.048587, loss_ce: 0.024193
 47%|████████████▌              | 187/400 [1:31:49<1:44:50, 29.53s/it]2022-01-08 11:00:01,071 iteration 3180 : loss : 0.076110, loss_ce: 0.036157
2022-01-08 11:00:02,624 iteration 3181 : loss : 0.022441, loss_ce: 0.010673
2022-01-08 11:00:04,145 iteration 3182 : loss : 0.031697, loss_ce: 0.014515
2022-01-08 11:00:05,782 iteration 3183 : loss : 0.041232, loss_ce: 0.013049
2022-01-08 11:00:07,362 iteration 3184 : loss : 0.028533, loss_ce: 0.010253
2022-01-08 11:00:08,950 iteration 3185 : loss : 0.028737, loss_ce: 0.009477
2022-01-08 11:00:10,564 iteration 3186 : loss : 0.036574, loss_ce: 0.014911
2022-01-08 11:00:12,183 iteration 3187 : loss : 0.027127, loss_ce: 0.010527
2022-01-08 11:00:13,805 iteration 3188 : loss : 0.027772, loss_ce: 0.011234
2022-01-08 11:00:15,368 iteration 3189 : loss : 0.023869, loss_ce: 0.007949
2022-01-08 11:00:17,035 iteration 3190 : loss : 0.036223, loss_ce: 0.014781
2022-01-08 11:00:18,655 iteration 3191 : loss : 0.033426, loss_ce: 0.011388
2022-01-08 11:00:20,165 iteration 3192 : loss : 0.032792, loss_ce: 0.007544
2022-01-08 11:00:21,733 iteration 3193 : loss : 0.025570, loss_ce: 0.010697
2022-01-08 11:00:23,426 iteration 3194 : loss : 0.035115, loss_ce: 0.014117
2022-01-08 11:00:25,009 iteration 3195 : loss : 0.030213, loss_ce: 0.014310
2022-01-08 11:00:26,583 iteration 3196 : loss : 0.024712, loss_ce: 0.011394
 47%|████████████▋              | 188/400 [1:32:16<1:41:48, 28.82s/it]2022-01-08 11:00:28,177 iteration 3197 : loss : 0.020540, loss_ce: 0.008477
2022-01-08 11:00:29,784 iteration 3198 : loss : 0.030897, loss_ce: 0.015054
2022-01-08 11:00:31,386 iteration 3199 : loss : 0.031128, loss_ce: 0.010580
2022-01-08 11:00:33,005 iteration 3200 : loss : 0.027063, loss_ce: 0.008985
2022-01-08 11:00:34,620 iteration 3201 : loss : 0.025064, loss_ce: 0.009487
2022-01-08 11:00:36,241 iteration 3202 : loss : 0.034100, loss_ce: 0.010603
2022-01-08 11:00:37,741 iteration 3203 : loss : 0.020798, loss_ce: 0.009895
2022-01-08 11:00:39,226 iteration 3204 : loss : 0.020724, loss_ce: 0.008486
2022-01-08 11:00:40,711 iteration 3205 : loss : 0.035720, loss_ce: 0.013709
2022-01-08 11:00:42,161 iteration 3206 : loss : 0.020544, loss_ce: 0.006981
2022-01-08 11:00:43,753 iteration 3207 : loss : 0.034391, loss_ce: 0.012950
2022-01-08 11:00:45,313 iteration 3208 : loss : 0.022905, loss_ce: 0.008989
2022-01-08 11:00:46,849 iteration 3209 : loss : 0.024013, loss_ce: 0.008040
2022-01-08 11:00:48,440 iteration 3210 : loss : 0.021512, loss_ce: 0.009685
2022-01-08 11:00:49,949 iteration 3211 : loss : 0.024096, loss_ce: 0.007646
2022-01-08 11:00:51,586 iteration 3212 : loss : 0.041070, loss_ce: 0.023099
2022-01-08 11:00:53,236 iteration 3213 : loss : 0.024356, loss_ce: 0.007614
 47%|████████████▊              | 189/400 [1:32:43<1:39:03, 28.17s/it]2022-01-08 11:00:54,791 iteration 3214 : loss : 0.016820, loss_ce: 0.007080
2022-01-08 11:00:56,354 iteration 3215 : loss : 0.020078, loss_ce: 0.009105
2022-01-08 11:00:57,917 iteration 3216 : loss : 0.026092, loss_ce: 0.009648
2022-01-08 11:00:59,515 iteration 3217 : loss : 0.027606, loss_ce: 0.011698
2022-01-08 11:01:01,088 iteration 3218 : loss : 0.048080, loss_ce: 0.017727
2022-01-08 11:01:02,655 iteration 3219 : loss : 0.035884, loss_ce: 0.013420
2022-01-08 11:01:04,176 iteration 3220 : loss : 0.027541, loss_ce: 0.010275
2022-01-08 11:01:05,834 iteration 3221 : loss : 0.039322, loss_ce: 0.016153
2022-01-08 11:01:07,378 iteration 3222 : loss : 0.022189, loss_ce: 0.005745
2022-01-08 11:01:08,970 iteration 3223 : loss : 0.022084, loss_ce: 0.007613
2022-01-08 11:01:10,629 iteration 3224 : loss : 0.040205, loss_ce: 0.011958
2022-01-08 11:01:12,199 iteration 3225 : loss : 0.028871, loss_ce: 0.012938
2022-01-08 11:01:13,770 iteration 3226 : loss : 0.027669, loss_ce: 0.012371
2022-01-08 11:01:15,357 iteration 3227 : loss : 0.029456, loss_ce: 0.011894
2022-01-08 11:01:16,903 iteration 3228 : loss : 0.038391, loss_ce: 0.010415
2022-01-08 11:01:18,409 iteration 3229 : loss : 0.029287, loss_ce: 0.008038
2022-01-08 11:01:18,409 Training Data Eval:
2022-01-08 11:01:26,308   Average segmentation loss on training set: 0.0188
2022-01-08 11:01:26,309 Validation Data Eval:
2022-01-08 11:01:29,026   Average segmentation loss on validation set: 0.0812
2022-01-08 11:01:30,522 iteration 3230 : loss : 0.026227, loss_ce: 0.009827
 48%|████████████▊              | 190/400 [1:33:20<1:48:09, 30.90s/it]2022-01-08 11:01:32,085 iteration 3231 : loss : 0.032339, loss_ce: 0.006440
2022-01-08 11:01:33,628 iteration 3232 : loss : 0.020134, loss_ce: 0.005840
2022-01-08 11:01:35,199 iteration 3233 : loss : 0.025241, loss_ce: 0.011462
2022-01-08 11:01:36,789 iteration 3234 : loss : 0.024001, loss_ce: 0.011397
2022-01-08 11:01:38,374 iteration 3235 : loss : 0.045644, loss_ce: 0.017854
2022-01-08 11:01:39,882 iteration 3236 : loss : 0.018273, loss_ce: 0.009021
2022-01-08 11:01:41,542 iteration 3237 : loss : 0.030337, loss_ce: 0.016300
2022-01-08 11:01:43,118 iteration 3238 : loss : 0.022140, loss_ce: 0.009931
2022-01-08 11:01:44,689 iteration 3239 : loss : 0.027755, loss_ce: 0.011125
2022-01-08 11:01:46,203 iteration 3240 : loss : 0.037103, loss_ce: 0.010425
2022-01-08 11:01:47,696 iteration 3241 : loss : 0.031368, loss_ce: 0.008147
2022-01-08 11:01:49,267 iteration 3242 : loss : 0.031540, loss_ce: 0.012105
2022-01-08 11:01:50,765 iteration 3243 : loss : 0.037497, loss_ce: 0.012177
2022-01-08 11:01:52,353 iteration 3244 : loss : 0.023052, loss_ce: 0.009006
2022-01-08 11:01:53,877 iteration 3245 : loss : 0.020282, loss_ce: 0.009714
2022-01-08 11:01:55,505 iteration 3246 : loss : 0.034261, loss_ce: 0.020098
2022-01-08 11:01:57,037 iteration 3247 : loss : 0.026230, loss_ce: 0.010179
 48%|████████████▉              | 191/400 [1:33:47<1:43:03, 29.59s/it]2022-01-08 11:01:58,594 iteration 3248 : loss : 0.022311, loss_ce: 0.008418
2022-01-08 11:02:00,109 iteration 3249 : loss : 0.018323, loss_ce: 0.006771
2022-01-08 11:02:01,653 iteration 3250 : loss : 0.032112, loss_ce: 0.010837
2022-01-08 11:02:03,200 iteration 3251 : loss : 0.022182, loss_ce: 0.008806
2022-01-08 11:02:04,769 iteration 3252 : loss : 0.027169, loss_ce: 0.008570
2022-01-08 11:02:06,314 iteration 3253 : loss : 0.019533, loss_ce: 0.008790
2022-01-08 11:02:07,853 iteration 3254 : loss : 0.026997, loss_ce: 0.009719
2022-01-08 11:02:09,330 iteration 3255 : loss : 0.029083, loss_ce: 0.012069
2022-01-08 11:02:10,872 iteration 3256 : loss : 0.018664, loss_ce: 0.006301
2022-01-08 11:02:12,390 iteration 3257 : loss : 0.019473, loss_ce: 0.008129
2022-01-08 11:02:13,961 iteration 3258 : loss : 0.027167, loss_ce: 0.010416
2022-01-08 11:02:15,464 iteration 3259 : loss : 0.025819, loss_ce: 0.010054
2022-01-08 11:02:17,033 iteration 3260 : loss : 0.017736, loss_ce: 0.006792
2022-01-08 11:02:18,565 iteration 3261 : loss : 0.040240, loss_ce: 0.010598
2022-01-08 11:02:20,164 iteration 3262 : loss : 0.023027, loss_ce: 0.008169
2022-01-08 11:02:21,828 iteration 3263 : loss : 0.032192, loss_ce: 0.010634
2022-01-08 11:02:23,402 iteration 3264 : loss : 0.034247, loss_ce: 0.014956
 48%|████████████▉              | 192/400 [1:34:13<1:39:13, 28.62s/it]2022-01-08 11:02:24,949 iteration 3265 : loss : 0.019068, loss_ce: 0.007461
2022-01-08 11:02:26,558 iteration 3266 : loss : 0.029478, loss_ce: 0.013648
2022-01-08 11:02:28,140 iteration 3267 : loss : 0.022530, loss_ce: 0.008915
2022-01-08 11:02:29,661 iteration 3268 : loss : 0.021932, loss_ce: 0.007949
2022-01-08 11:02:31,279 iteration 3269 : loss : 0.042542, loss_ce: 0.018963
2022-01-08 11:02:32,923 iteration 3270 : loss : 0.033546, loss_ce: 0.009817
2022-01-08 11:02:34,538 iteration 3271 : loss : 0.025207, loss_ce: 0.008300
2022-01-08 11:02:36,158 iteration 3272 : loss : 0.023835, loss_ce: 0.009300
2022-01-08 11:02:37,665 iteration 3273 : loss : 0.021745, loss_ce: 0.007342
2022-01-08 11:02:39,216 iteration 3274 : loss : 0.019847, loss_ce: 0.010099
2022-01-08 11:02:40,837 iteration 3275 : loss : 0.036273, loss_ce: 0.010695
2022-01-08 11:02:42,374 iteration 3276 : loss : 0.023174, loss_ce: 0.010247
2022-01-08 11:02:43,967 iteration 3277 : loss : 0.027662, loss_ce: 0.014708
2022-01-08 11:02:45,538 iteration 3278 : loss : 0.025129, loss_ce: 0.009355
2022-01-08 11:02:47,063 iteration 3279 : loss : 0.021392, loss_ce: 0.008128
2022-01-08 11:02:48,609 iteration 3280 : loss : 0.022609, loss_ce: 0.007127
2022-01-08 11:02:50,112 iteration 3281 : loss : 0.023157, loss_ce: 0.005940
 48%|█████████████              | 193/400 [1:34:40<1:36:45, 28.05s/it]2022-01-08 11:02:51,739 iteration 3282 : loss : 0.023352, loss_ce: 0.008585
2022-01-08 11:02:53,299 iteration 3283 : loss : 0.019053, loss_ce: 0.006137
2022-01-08 11:02:54,858 iteration 3284 : loss : 0.023918, loss_ce: 0.010339
2022-01-08 11:02:56,403 iteration 3285 : loss : 0.028529, loss_ce: 0.009958
2022-01-08 11:02:57,949 iteration 3286 : loss : 0.018944, loss_ce: 0.007055
2022-01-08 11:02:59,520 iteration 3287 : loss : 0.025573, loss_ce: 0.009905
2022-01-08 11:03:01,029 iteration 3288 : loss : 0.024221, loss_ce: 0.012824
2022-01-08 11:03:02,548 iteration 3289 : loss : 0.022791, loss_ce: 0.006369
2022-01-08 11:03:04,156 iteration 3290 : loss : 0.030835, loss_ce: 0.008982
2022-01-08 11:03:05,776 iteration 3291 : loss : 0.033619, loss_ce: 0.011217
2022-01-08 11:03:07,352 iteration 3292 : loss : 0.026469, loss_ce: 0.007223
2022-01-08 11:03:08,908 iteration 3293 : loss : 0.024197, loss_ce: 0.010946
2022-01-08 11:03:10,438 iteration 3294 : loss : 0.024552, loss_ce: 0.010891
2022-01-08 11:03:12,083 iteration 3295 : loss : 0.033593, loss_ce: 0.012100
2022-01-08 11:03:13,670 iteration 3296 : loss : 0.028088, loss_ce: 0.008012
2022-01-08 11:03:15,165 iteration 3297 : loss : 0.014981, loss_ce: 0.005696
2022-01-08 11:03:16,765 iteration 3298 : loss : 0.025742, loss_ce: 0.010248
 48%|█████████████              | 194/400 [1:35:07<1:34:51, 27.63s/it]2022-01-08 11:03:18,298 iteration 3299 : loss : 0.016481, loss_ce: 0.007258
2022-01-08 11:03:19,823 iteration 3300 : loss : 0.033876, loss_ce: 0.010383
2022-01-08 11:03:21,437 iteration 3301 : loss : 0.019992, loss_ce: 0.009397
2022-01-08 11:03:22,999 iteration 3302 : loss : 0.035199, loss_ce: 0.016587
2022-01-08 11:03:24,540 iteration 3303 : loss : 0.023256, loss_ce: 0.009136
2022-01-08 11:03:26,107 iteration 3304 : loss : 0.022608, loss_ce: 0.007859
2022-01-08 11:03:27,692 iteration 3305 : loss : 0.021863, loss_ce: 0.007296
2022-01-08 11:03:29,313 iteration 3306 : loss : 0.023481, loss_ce: 0.009399
2022-01-08 11:03:30,850 iteration 3307 : loss : 0.031198, loss_ce: 0.010910
2022-01-08 11:03:32,419 iteration 3308 : loss : 0.028667, loss_ce: 0.012521
2022-01-08 11:03:33,975 iteration 3309 : loss : 0.046637, loss_ce: 0.010150
2022-01-08 11:03:35,606 iteration 3310 : loss : 0.051845, loss_ce: 0.017208
2022-01-08 11:03:37,263 iteration 3311 : loss : 0.019100, loss_ce: 0.007084
2022-01-08 11:03:38,776 iteration 3312 : loss : 0.022916, loss_ce: 0.007606
2022-01-08 11:03:40,301 iteration 3313 : loss : 0.020994, loss_ce: 0.007514
2022-01-08 11:03:41,926 iteration 3314 : loss : 0.028143, loss_ce: 0.009684
2022-01-08 11:03:41,926 Training Data Eval:
2022-01-08 11:03:49,825   Average segmentation loss on training set: 0.0187
2022-01-08 11:03:49,826 Validation Data Eval:
2022-01-08 11:03:52,557   Average segmentation loss on validation set: 0.0713
2022-01-08 11:03:54,073 iteration 3315 : loss : 0.019502, loss_ce: 0.008649
 49%|█████████████▏             | 195/400 [1:35:44<1:44:19, 30.53s/it]2022-01-08 11:03:55,628 iteration 3316 : loss : 0.016210, loss_ce: 0.005815
2022-01-08 11:03:57,162 iteration 3317 : loss : 0.022885, loss_ce: 0.009140
2022-01-08 11:03:58,818 iteration 3318 : loss : 0.032448, loss_ce: 0.014096
2022-01-08 11:04:00,352 iteration 3319 : loss : 0.021993, loss_ce: 0.008084
2022-01-08 11:04:01,892 iteration 3320 : loss : 0.016621, loss_ce: 0.005793
2022-01-08 11:04:03,461 iteration 3321 : loss : 0.020818, loss_ce: 0.006375
2022-01-08 11:04:05,027 iteration 3322 : loss : 0.030325, loss_ce: 0.012453
2022-01-08 11:04:06,541 iteration 3323 : loss : 0.027405, loss_ce: 0.008688
2022-01-08 11:04:08,179 iteration 3324 : loss : 0.032284, loss_ce: 0.014774
2022-01-08 11:04:09,693 iteration 3325 : loss : 0.028416, loss_ce: 0.008585
2022-01-08 11:04:11,282 iteration 3326 : loss : 0.024553, loss_ce: 0.008734
2022-01-08 11:04:12,940 iteration 3327 : loss : 0.028057, loss_ce: 0.008745
2022-01-08 11:04:14,502 iteration 3328 : loss : 0.024486, loss_ce: 0.008902
2022-01-08 11:04:16,029 iteration 3329 : loss : 0.024004, loss_ce: 0.012328
2022-01-08 11:04:17,575 iteration 3330 : loss : 0.034790, loss_ce: 0.010655
2022-01-08 11:04:19,151 iteration 3331 : loss : 0.020885, loss_ce: 0.007573
2022-01-08 11:04:20,628 iteration 3332 : loss : 0.022073, loss_ce: 0.011187
 49%|█████████████▏             | 196/400 [1:36:10<1:39:45, 29.34s/it]2022-01-08 11:04:22,206 iteration 3333 : loss : 0.022272, loss_ce: 0.008636
2022-01-08 11:04:23,783 iteration 3334 : loss : 0.017568, loss_ce: 0.007471
2022-01-08 11:04:25,266 iteration 3335 : loss : 0.018255, loss_ce: 0.007928
2022-01-08 11:04:26,826 iteration 3336 : loss : 0.034266, loss_ce: 0.012720
2022-01-08 11:04:28,433 iteration 3337 : loss : 0.028038, loss_ce: 0.011788
2022-01-08 11:04:30,086 iteration 3338 : loss : 0.034980, loss_ce: 0.015387
2022-01-08 11:04:31,641 iteration 3339 : loss : 0.035654, loss_ce: 0.010360
2022-01-08 11:04:33,177 iteration 3340 : loss : 0.017067, loss_ce: 0.005929
2022-01-08 11:04:34,731 iteration 3341 : loss : 0.024182, loss_ce: 0.009067
2022-01-08 11:04:36,339 iteration 3342 : loss : 0.047457, loss_ce: 0.023014
2022-01-08 11:04:37,924 iteration 3343 : loss : 0.023591, loss_ce: 0.007614
2022-01-08 11:04:39,499 iteration 3344 : loss : 0.028022, loss_ce: 0.010216
2022-01-08 11:04:41,039 iteration 3345 : loss : 0.018147, loss_ce: 0.006091
2022-01-08 11:04:42,608 iteration 3346 : loss : 0.026983, loss_ce: 0.011952
2022-01-08 11:04:44,201 iteration 3347 : loss : 0.043551, loss_ce: 0.010923
2022-01-08 11:04:45,852 iteration 3348 : loss : 0.031641, loss_ce: 0.011016
2022-01-08 11:04:47,436 iteration 3349 : loss : 0.026996, loss_ce: 0.011350
 49%|█████████████▎             | 197/400 [1:36:37<1:36:41, 28.58s/it]2022-01-08 11:04:48,996 iteration 3350 : loss : 0.020541, loss_ce: 0.008962
2022-01-08 11:04:50,511 iteration 3351 : loss : 0.021430, loss_ce: 0.008015
2022-01-08 11:04:52,049 iteration 3352 : loss : 0.027394, loss_ce: 0.009641
2022-01-08 11:04:53,630 iteration 3353 : loss : 0.024886, loss_ce: 0.009395
2022-01-08 11:04:55,208 iteration 3354 : loss : 0.020511, loss_ce: 0.007154
2022-01-08 11:04:56,843 iteration 3355 : loss : 0.026433, loss_ce: 0.014121
2022-01-08 11:04:58,450 iteration 3356 : loss : 0.051748, loss_ce: 0.014110
2022-01-08 11:05:00,068 iteration 3357 : loss : 0.030796, loss_ce: 0.011379
2022-01-08 11:05:01,704 iteration 3358 : loss : 0.034366, loss_ce: 0.010932
2022-01-08 11:05:03,303 iteration 3359 : loss : 0.021013, loss_ce: 0.008241
2022-01-08 11:05:04,961 iteration 3360 : loss : 0.026367, loss_ce: 0.011158
2022-01-08 11:05:06,477 iteration 3361 : loss : 0.027264, loss_ce: 0.007447
2022-01-08 11:05:08,006 iteration 3362 : loss : 0.026402, loss_ce: 0.010498
2022-01-08 11:05:09,522 iteration 3363 : loss : 0.035666, loss_ce: 0.021883
2022-01-08 11:05:11,105 iteration 3364 : loss : 0.023863, loss_ce: 0.008005
2022-01-08 11:05:12,622 iteration 3365 : loss : 0.025498, loss_ce: 0.014577
2022-01-08 11:05:14,172 iteration 3366 : loss : 0.020589, loss_ce: 0.007579
 50%|█████████████▎             | 198/400 [1:37:04<1:34:21, 28.03s/it]2022-01-08 11:05:15,768 iteration 3367 : loss : 0.029014, loss_ce: 0.009585
2022-01-08 11:05:17,319 iteration 3368 : loss : 0.023014, loss_ce: 0.009916
2022-01-08 11:05:18,901 iteration 3369 : loss : 0.021657, loss_ce: 0.007419
2022-01-08 11:05:20,488 iteration 3370 : loss : 0.027799, loss_ce: 0.012986
2022-01-08 11:05:21,964 iteration 3371 : loss : 0.015174, loss_ce: 0.006193
2022-01-08 11:05:23,505 iteration 3372 : loss : 0.017597, loss_ce: 0.008044
2022-01-08 11:05:25,057 iteration 3373 : loss : 0.018992, loss_ce: 0.008725
2022-01-08 11:05:26,563 iteration 3374 : loss : 0.017528, loss_ce: 0.007281
2022-01-08 11:05:28,075 iteration 3375 : loss : 0.019471, loss_ce: 0.008372
2022-01-08 11:05:29,601 iteration 3376 : loss : 0.022732, loss_ce: 0.008426
2022-01-08 11:05:31,199 iteration 3377 : loss : 0.038840, loss_ce: 0.013424
2022-01-08 11:05:32,752 iteration 3378 : loss : 0.037957, loss_ce: 0.012591
2022-01-08 11:05:34,227 iteration 3379 : loss : 0.019479, loss_ce: 0.008539
2022-01-08 11:05:35,840 iteration 3380 : loss : 0.036344, loss_ce: 0.015981
2022-01-08 11:05:37,335 iteration 3381 : loss : 0.025878, loss_ce: 0.005350
2022-01-08 11:05:38,897 iteration 3382 : loss : 0.027255, loss_ce: 0.009921
2022-01-08 11:05:40,440 iteration 3383 : loss : 0.020065, loss_ce: 0.007303
 50%|█████████████▍             | 199/400 [1:37:30<1:32:07, 27.50s/it]2022-01-08 11:05:41,988 iteration 3384 : loss : 0.020104, loss_ce: 0.007727
2022-01-08 11:05:43,595 iteration 3385 : loss : 0.025747, loss_ce: 0.010466
2022-01-08 11:05:45,208 iteration 3386 : loss : 0.039059, loss_ce: 0.021253
2022-01-08 11:05:46,806 iteration 3387 : loss : 0.028674, loss_ce: 0.010701
2022-01-08 11:05:48,309 iteration 3388 : loss : 0.021431, loss_ce: 0.008243
2022-01-08 11:05:49,893 iteration 3389 : loss : 0.032751, loss_ce: 0.015391
2022-01-08 11:05:51,384 iteration 3390 : loss : 0.030721, loss_ce: 0.007672
2022-01-08 11:05:53,042 iteration 3391 : loss : 0.035558, loss_ce: 0.011099
2022-01-08 11:05:54,641 iteration 3392 : loss : 0.027344, loss_ce: 0.010221
2022-01-08 11:05:56,230 iteration 3393 : loss : 0.028907, loss_ce: 0.010237
2022-01-08 11:05:57,705 iteration 3394 : loss : 0.024244, loss_ce: 0.008686
2022-01-08 11:05:59,367 iteration 3395 : loss : 0.027507, loss_ce: 0.010908
2022-01-08 11:06:00,949 iteration 3396 : loss : 0.026353, loss_ce: 0.011453
2022-01-08 11:06:02,486 iteration 3397 : loss : 0.022872, loss_ce: 0.009399
2022-01-08 11:06:04,083 iteration 3398 : loss : 0.024755, loss_ce: 0.007932
2022-01-08 11:06:05,661 iteration 3399 : loss : 0.018469, loss_ce: 0.007892
2022-01-08 11:06:05,662 Training Data Eval:
2022-01-08 11:06:13,543   Average segmentation loss on training set: 0.0161
2022-01-08 11:06:13,543 Validation Data Eval:
2022-01-08 11:06:16,258   Average segmentation loss on validation set: 0.0614
2022-01-08 11:06:22,051 Found new lowest validation loss at iteration 3399! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 11:06:23,481 iteration 3400 : loss : 0.023172, loss_ce: 0.006327
 50%|█████████████▌             | 200/400 [1:38:13<1:47:12, 32.16s/it]2022-01-08 11:06:25,099 iteration 3401 : loss : 0.025465, loss_ce: 0.012217
2022-01-08 11:06:26,713 iteration 3402 : loss : 0.020986, loss_ce: 0.007161
2022-01-08 11:06:28,250 iteration 3403 : loss : 0.022484, loss_ce: 0.010994
2022-01-08 11:06:29,790 iteration 3404 : loss : 0.018452, loss_ce: 0.006302
2022-01-08 11:06:31,334 iteration 3405 : loss : 0.021813, loss_ce: 0.007853
2022-01-08 11:06:32,946 iteration 3406 : loss : 0.027358, loss_ce: 0.010021
2022-01-08 11:06:34,518 iteration 3407 : loss : 0.028132, loss_ce: 0.011036
2022-01-08 11:06:36,103 iteration 3408 : loss : 0.019643, loss_ce: 0.006852
2022-01-08 11:06:37,704 iteration 3409 : loss : 0.026879, loss_ce: 0.011333
2022-01-08 11:06:39,299 iteration 3410 : loss : 0.026738, loss_ce: 0.008978
2022-01-08 11:06:40,937 iteration 3411 : loss : 0.024662, loss_ce: 0.009438
2022-01-08 11:06:42,456 iteration 3412 : loss : 0.027468, loss_ce: 0.009970
2022-01-08 11:06:43,957 iteration 3413 : loss : 0.016326, loss_ce: 0.007229
2022-01-08 11:06:45,554 iteration 3414 : loss : 0.023764, loss_ce: 0.005800
2022-01-08 11:06:47,168 iteration 3415 : loss : 0.024530, loss_ce: 0.009328
2022-01-08 11:06:48,780 iteration 3416 : loss : 0.018314, loss_ce: 0.005664
2022-01-08 11:06:50,433 iteration 3417 : loss : 0.029773, loss_ce: 0.011349
 50%|█████████████▌             | 201/400 [1:38:40<1:41:29, 30.60s/it]2022-01-08 11:06:52,047 iteration 3418 : loss : 0.021781, loss_ce: 0.010336
2022-01-08 11:06:53,649 iteration 3419 : loss : 0.022972, loss_ce: 0.008569
2022-01-08 11:06:55,314 iteration 3420 : loss : 0.034314, loss_ce: 0.011662
2022-01-08 11:06:56,909 iteration 3421 : loss : 0.032412, loss_ce: 0.010975
2022-01-08 11:06:58,425 iteration 3422 : loss : 0.021006, loss_ce: 0.007008
2022-01-08 11:06:59,997 iteration 3423 : loss : 0.018169, loss_ce: 0.007156
2022-01-08 11:07:01,565 iteration 3424 : loss : 0.024264, loss_ce: 0.011401
2022-01-08 11:07:03,084 iteration 3425 : loss : 0.030119, loss_ce: 0.014490
2022-01-08 11:07:04,661 iteration 3426 : loss : 0.022940, loss_ce: 0.008207
2022-01-08 11:07:06,252 iteration 3427 : loss : 0.030460, loss_ce: 0.009353
2022-01-08 11:07:07,844 iteration 3428 : loss : 0.028974, loss_ce: 0.011307
2022-01-08 11:07:09,462 iteration 3429 : loss : 0.028412, loss_ce: 0.010273
2022-01-08 11:07:11,085 iteration 3430 : loss : 0.032563, loss_ce: 0.013028
2022-01-08 11:07:12,581 iteration 3431 : loss : 0.022969, loss_ce: 0.009483
2022-01-08 11:07:14,108 iteration 3432 : loss : 0.025494, loss_ce: 0.008277
2022-01-08 11:07:15,635 iteration 3433 : loss : 0.023462, loss_ce: 0.008148
2022-01-08 11:07:17,244 iteration 3434 : loss : 0.030670, loss_ce: 0.010076
 50%|█████████████▋             | 202/400 [1:39:07<1:37:13, 29.46s/it]2022-01-08 11:07:18,769 iteration 3435 : loss : 0.030572, loss_ce: 0.011306
2022-01-08 11:07:20,300 iteration 3436 : loss : 0.030674, loss_ce: 0.015238
2022-01-08 11:07:21,971 iteration 3437 : loss : 0.028028, loss_ce: 0.013803
2022-01-08 11:07:23,609 iteration 3438 : loss : 0.032474, loss_ce: 0.016903
2022-01-08 11:07:25,229 iteration 3439 : loss : 0.024353, loss_ce: 0.008315
2022-01-08 11:07:26,765 iteration 3440 : loss : 0.026410, loss_ce: 0.009737
2022-01-08 11:07:28,326 iteration 3441 : loss : 0.022416, loss_ce: 0.009308
2022-01-08 11:07:30,000 iteration 3442 : loss : 0.030877, loss_ce: 0.011054
2022-01-08 11:07:31,613 iteration 3443 : loss : 0.026892, loss_ce: 0.010322
2022-01-08 11:07:33,227 iteration 3444 : loss : 0.032877, loss_ce: 0.013824
2022-01-08 11:07:34,807 iteration 3445 : loss : 0.044422, loss_ce: 0.011111
2022-01-08 11:07:36,369 iteration 3446 : loss : 0.022236, loss_ce: 0.009425
2022-01-08 11:07:37,925 iteration 3447 : loss : 0.024115, loss_ce: 0.008553
2022-01-08 11:07:39,551 iteration 3448 : loss : 0.030982, loss_ce: 0.007295
2022-01-08 11:07:41,161 iteration 3449 : loss : 0.034089, loss_ce: 0.013422
2022-01-08 11:07:42,745 iteration 3450 : loss : 0.037181, loss_ce: 0.018655
2022-01-08 11:07:44,409 iteration 3451 : loss : 0.025175, loss_ce: 0.009871
 51%|█████████████▋             | 203/400 [1:39:34<1:34:28, 28.77s/it]2022-01-08 11:07:46,015 iteration 3452 : loss : 0.022292, loss_ce: 0.007666
2022-01-08 11:07:47,562 iteration 3453 : loss : 0.028206, loss_ce: 0.012550
2022-01-08 11:07:49,092 iteration 3454 : loss : 0.023712, loss_ce: 0.008366
2022-01-08 11:07:50,734 iteration 3455 : loss : 0.023984, loss_ce: 0.011164
2022-01-08 11:07:52,262 iteration 3456 : loss : 0.019617, loss_ce: 0.006421
2022-01-08 11:07:53,929 iteration 3457 : loss : 0.023173, loss_ce: 0.008146
2022-01-08 11:07:55,539 iteration 3458 : loss : 0.028708, loss_ce: 0.010267
2022-01-08 11:07:57,106 iteration 3459 : loss : 0.026984, loss_ce: 0.010389
2022-01-08 11:07:58,665 iteration 3460 : loss : 0.030240, loss_ce: 0.011925
2022-01-08 11:08:00,226 iteration 3461 : loss : 0.024868, loss_ce: 0.008990
2022-01-08 11:08:01,776 iteration 3462 : loss : 0.020626, loss_ce: 0.009564
2022-01-08 11:08:03,380 iteration 3463 : loss : 0.038983, loss_ce: 0.014804
2022-01-08 11:08:04,894 iteration 3464 : loss : 0.018364, loss_ce: 0.006420
2022-01-08 11:08:06,507 iteration 3465 : loss : 0.025730, loss_ce: 0.007905
2022-01-08 11:08:08,092 iteration 3466 : loss : 0.026460, loss_ce: 0.010445
2022-01-08 11:08:09,661 iteration 3467 : loss : 0.025870, loss_ce: 0.014778
2022-01-08 11:08:11,219 iteration 3468 : loss : 0.021661, loss_ce: 0.009577
 51%|█████████████▊             | 204/400 [1:40:01<1:32:03, 28.18s/it]2022-01-08 11:08:12,793 iteration 3469 : loss : 0.021693, loss_ce: 0.006282
2022-01-08 11:08:14,412 iteration 3470 : loss : 0.037140, loss_ce: 0.011822
2022-01-08 11:08:15,910 iteration 3471 : loss : 0.019578, loss_ce: 0.006557
2022-01-08 11:08:17,502 iteration 3472 : loss : 0.029400, loss_ce: 0.013016
2022-01-08 11:08:19,189 iteration 3473 : loss : 0.035788, loss_ce: 0.014843
2022-01-08 11:08:20,758 iteration 3474 : loss : 0.023545, loss_ce: 0.006870
2022-01-08 11:08:22,399 iteration 3475 : loss : 0.039219, loss_ce: 0.011482
2022-01-08 11:08:23,934 iteration 3476 : loss : 0.023215, loss_ce: 0.006928
2022-01-08 11:08:25,532 iteration 3477 : loss : 0.024285, loss_ce: 0.007114
2022-01-08 11:08:27,107 iteration 3478 : loss : 0.025250, loss_ce: 0.009600
2022-01-08 11:08:28,644 iteration 3479 : loss : 0.020505, loss_ce: 0.008487
2022-01-08 11:08:30,124 iteration 3480 : loss : 0.020984, loss_ce: 0.008658
2022-01-08 11:08:31,681 iteration 3481 : loss : 0.029343, loss_ce: 0.010185
2022-01-08 11:08:33,240 iteration 3482 : loss : 0.031410, loss_ce: 0.012679
2022-01-08 11:08:34,865 iteration 3483 : loss : 0.031910, loss_ce: 0.014950
2022-01-08 11:08:36,347 iteration 3484 : loss : 0.020653, loss_ce: 0.008464
2022-01-08 11:08:36,347 Training Data Eval:
2022-01-08 11:08:44,242   Average segmentation loss on training set: 0.0163
2022-01-08 11:08:44,243 Validation Data Eval:
2022-01-08 11:08:46,969   Average segmentation loss on validation set: 0.0620
2022-01-08 11:08:48,623 iteration 3485 : loss : 0.022810, loss_ce: 0.010189
 51%|█████████████▊             | 205/400 [1:40:38<1:40:35, 30.95s/it]2022-01-08 11:08:50,223 iteration 3486 : loss : 0.023583, loss_ce: 0.010054
2022-01-08 11:08:51,736 iteration 3487 : loss : 0.019870, loss_ce: 0.008678
2022-01-08 11:08:53,276 iteration 3488 : loss : 0.028501, loss_ce: 0.011544
2022-01-08 11:08:54,829 iteration 3489 : loss : 0.031210, loss_ce: 0.011331
2022-01-08 11:08:56,415 iteration 3490 : loss : 0.022714, loss_ce: 0.008570
2022-01-08 11:08:58,038 iteration 3491 : loss : 0.027644, loss_ce: 0.011386
2022-01-08 11:08:59,543 iteration 3492 : loss : 0.022212, loss_ce: 0.009097
2022-01-08 11:09:01,209 iteration 3493 : loss : 0.028637, loss_ce: 0.011255
2022-01-08 11:09:02,813 iteration 3494 : loss : 0.025943, loss_ce: 0.007939
2022-01-08 11:09:04,370 iteration 3495 : loss : 0.022583, loss_ce: 0.007198
2022-01-08 11:09:05,978 iteration 3496 : loss : 0.020684, loss_ce: 0.008203
2022-01-08 11:09:07,583 iteration 3497 : loss : 0.026456, loss_ce: 0.010143
2022-01-08 11:09:09,267 iteration 3498 : loss : 0.024830, loss_ce: 0.011855
2022-01-08 11:09:10,805 iteration 3499 : loss : 0.018399, loss_ce: 0.006988
2022-01-08 11:09:12,289 iteration 3500 : loss : 0.017188, loss_ce: 0.005415
2022-01-08 11:09:13,813 iteration 3501 : loss : 0.023831, loss_ce: 0.009801
2022-01-08 11:09:15,397 iteration 3502 : loss : 0.024698, loss_ce: 0.010594
 52%|█████████████▉             | 206/400 [1:41:05<1:36:01, 29.70s/it]2022-01-08 11:09:17,017 iteration 3503 : loss : 0.024287, loss_ce: 0.010382
2022-01-08 11:09:18,559 iteration 3504 : loss : 0.017992, loss_ce: 0.005212
2022-01-08 11:09:20,024 iteration 3505 : loss : 0.017003, loss_ce: 0.007753
2022-01-08 11:09:21,609 iteration 3506 : loss : 0.025790, loss_ce: 0.007790
2022-01-08 11:09:23,164 iteration 3507 : loss : 0.026556, loss_ce: 0.008271
2022-01-08 11:09:24,850 iteration 3508 : loss : 0.030547, loss_ce: 0.014586
2022-01-08 11:09:26,442 iteration 3509 : loss : 0.017508, loss_ce: 0.006509
2022-01-08 11:09:27,958 iteration 3510 : loss : 0.020781, loss_ce: 0.008137
2022-01-08 11:09:29,464 iteration 3511 : loss : 0.016896, loss_ce: 0.007684
2022-01-08 11:09:31,095 iteration 3512 : loss : 0.034651, loss_ce: 0.010231
2022-01-08 11:09:32,665 iteration 3513 : loss : 0.019898, loss_ce: 0.006318
2022-01-08 11:09:34,351 iteration 3514 : loss : 0.034685, loss_ce: 0.019243
2022-01-08 11:09:35,900 iteration 3515 : loss : 0.023018, loss_ce: 0.007325
2022-01-08 11:09:37,438 iteration 3516 : loss : 0.017091, loss_ce: 0.006556
2022-01-08 11:09:38,979 iteration 3517 : loss : 0.021609, loss_ce: 0.008381
2022-01-08 11:09:40,528 iteration 3518 : loss : 0.019707, loss_ce: 0.009321
2022-01-08 11:09:42,108 iteration 3519 : loss : 0.026913, loss_ce: 0.010323
 52%|█████████████▉             | 207/400 [1:41:32<1:32:38, 28.80s/it]2022-01-08 11:09:43,725 iteration 3520 : loss : 0.022821, loss_ce: 0.008708
2022-01-08 11:09:45,258 iteration 3521 : loss : 0.028129, loss_ce: 0.012680
2022-01-08 11:09:46,759 iteration 3522 : loss : 0.017565, loss_ce: 0.007504
2022-01-08 11:09:48,309 iteration 3523 : loss : 0.017469, loss_ce: 0.006202
2022-01-08 11:09:49,905 iteration 3524 : loss : 0.020476, loss_ce: 0.008999
2022-01-08 11:09:51,566 iteration 3525 : loss : 0.021745, loss_ce: 0.007971
2022-01-08 11:09:53,151 iteration 3526 : loss : 0.019863, loss_ce: 0.007277
2022-01-08 11:09:54,762 iteration 3527 : loss : 0.033407, loss_ce: 0.009831
2022-01-08 11:09:56,451 iteration 3528 : loss : 0.041453, loss_ce: 0.021856
2022-01-08 11:09:58,054 iteration 3529 : loss : 0.025030, loss_ce: 0.007044
2022-01-08 11:09:59,649 iteration 3530 : loss : 0.016573, loss_ce: 0.006440
2022-01-08 11:10:01,167 iteration 3531 : loss : 0.027261, loss_ce: 0.008553
2022-01-08 11:10:02,812 iteration 3532 : loss : 0.022330, loss_ce: 0.009012
2022-01-08 11:10:04,362 iteration 3533 : loss : 0.023146, loss_ce: 0.006321
2022-01-08 11:10:05,954 iteration 3534 : loss : 0.022739, loss_ce: 0.009016
2022-01-08 11:10:07,596 iteration 3535 : loss : 0.028047, loss_ce: 0.012559
2022-01-08 11:10:09,199 iteration 3536 : loss : 0.018501, loss_ce: 0.007659
 52%|██████████████             | 208/400 [1:41:59<1:30:31, 28.29s/it]2022-01-08 11:10:10,724 iteration 3537 : loss : 0.016595, loss_ce: 0.006523
2022-01-08 11:10:12,217 iteration 3538 : loss : 0.016184, loss_ce: 0.007025
2022-01-08 11:10:13,729 iteration 3539 : loss : 0.017567, loss_ce: 0.007993
2022-01-08 11:10:15,182 iteration 3540 : loss : 0.017941, loss_ce: 0.004943
2022-01-08 11:10:16,815 iteration 3541 : loss : 0.027119, loss_ce: 0.010452
2022-01-08 11:10:18,378 iteration 3542 : loss : 0.022715, loss_ce: 0.009008
2022-01-08 11:10:20,053 iteration 3543 : loss : 0.029229, loss_ce: 0.009324
2022-01-08 11:10:21,546 iteration 3544 : loss : 0.014204, loss_ce: 0.005627
2022-01-08 11:10:23,099 iteration 3545 : loss : 0.017978, loss_ce: 0.007648
2022-01-08 11:10:24,594 iteration 3546 : loss : 0.021014, loss_ce: 0.007880
2022-01-08 11:10:26,157 iteration 3547 : loss : 0.017789, loss_ce: 0.007607
2022-01-08 11:10:27,667 iteration 3548 : loss : 0.022975, loss_ce: 0.007543
2022-01-08 11:10:29,235 iteration 3549 : loss : 0.018538, loss_ce: 0.007587
2022-01-08 11:10:30,838 iteration 3550 : loss : 0.019252, loss_ce: 0.008340
2022-01-08 11:10:32,464 iteration 3551 : loss : 0.030039, loss_ce: 0.010281
2022-01-08 11:10:34,091 iteration 3552 : loss : 0.056328, loss_ce: 0.014827
2022-01-08 11:10:35,722 iteration 3553 : loss : 0.025547, loss_ce: 0.011565
 52%|██████████████             | 209/400 [1:42:25<1:28:22, 27.76s/it]2022-01-08 11:10:37,340 iteration 3554 : loss : 0.025507, loss_ce: 0.011905
2022-01-08 11:10:38,880 iteration 3555 : loss : 0.021775, loss_ce: 0.007395
2022-01-08 11:10:40,423 iteration 3556 : loss : 0.019456, loss_ce: 0.008241
2022-01-08 11:10:42,072 iteration 3557 : loss : 0.026546, loss_ce: 0.007618
2022-01-08 11:10:43,590 iteration 3558 : loss : 0.035326, loss_ce: 0.009337
2022-01-08 11:10:45,141 iteration 3559 : loss : 0.019217, loss_ce: 0.005959
2022-01-08 11:10:46,598 iteration 3560 : loss : 0.020616, loss_ce: 0.010966
2022-01-08 11:10:48,235 iteration 3561 : loss : 0.024560, loss_ce: 0.010089
2022-01-08 11:10:49,769 iteration 3562 : loss : 0.020539, loss_ce: 0.006985
2022-01-08 11:10:51,361 iteration 3563 : loss : 0.020408, loss_ce: 0.006718
2022-01-08 11:10:52,937 iteration 3564 : loss : 0.024968, loss_ce: 0.009228
2022-01-08 11:10:54,622 iteration 3565 : loss : 0.027613, loss_ce: 0.009779
2022-01-08 11:10:56,208 iteration 3566 : loss : 0.019749, loss_ce: 0.007534
2022-01-08 11:10:57,875 iteration 3567 : loss : 0.025473, loss_ce: 0.011433
2022-01-08 11:10:59,522 iteration 3568 : loss : 0.027870, loss_ce: 0.010304
2022-01-08 11:11:01,148 iteration 3569 : loss : 0.042238, loss_ce: 0.009351
2022-01-08 11:11:01,148 Training Data Eval:
2022-01-08 11:11:09,033   Average segmentation loss on training set: 0.0158
2022-01-08 11:11:09,033 Validation Data Eval:
2022-01-08 11:11:11,755   Average segmentation loss on validation set: 0.0645
2022-01-08 11:11:13,350 iteration 3570 : loss : 0.032054, loss_ce: 0.014009
 52%|██████████████▏            | 210/400 [1:43:03<1:37:16, 30.72s/it]2022-01-08 11:11:14,960 iteration 3571 : loss : 0.019667, loss_ce: 0.009018
2022-01-08 11:11:16,543 iteration 3572 : loss : 0.022134, loss_ce: 0.006281
2022-01-08 11:11:18,118 iteration 3573 : loss : 0.027829, loss_ce: 0.011558
2022-01-08 11:11:19,656 iteration 3574 : loss : 0.016928, loss_ce: 0.007154
2022-01-08 11:11:21,206 iteration 3575 : loss : 0.019672, loss_ce: 0.007832
2022-01-08 11:11:22,795 iteration 3576 : loss : 0.031925, loss_ce: 0.011159
2022-01-08 11:11:24,421 iteration 3577 : loss : 0.019833, loss_ce: 0.008407
2022-01-08 11:11:26,043 iteration 3578 : loss : 0.031896, loss_ce: 0.011718
2022-01-08 11:11:27,593 iteration 3579 : loss : 0.022852, loss_ce: 0.008175
2022-01-08 11:11:29,196 iteration 3580 : loss : 0.023000, loss_ce: 0.007039
2022-01-08 11:11:30,725 iteration 3581 : loss : 0.030132, loss_ce: 0.006156
2022-01-08 11:11:32,282 iteration 3582 : loss : 0.023041, loss_ce: 0.008198
2022-01-08 11:11:33,885 iteration 3583 : loss : 0.025956, loss_ce: 0.011703
2022-01-08 11:11:35,418 iteration 3584 : loss : 0.020862, loss_ce: 0.007978
2022-01-08 11:11:36,976 iteration 3585 : loss : 0.021748, loss_ce: 0.008521
2022-01-08 11:11:38,486 iteration 3586 : loss : 0.022485, loss_ce: 0.009419
2022-01-08 11:11:40,044 iteration 3587 : loss : 0.023073, loss_ce: 0.010058
 53%|██████████████▏            | 211/400 [1:43:30<1:32:57, 29.51s/it]2022-01-08 11:11:41,565 iteration 3588 : loss : 0.017051, loss_ce: 0.006286
2022-01-08 11:11:43,169 iteration 3589 : loss : 0.029807, loss_ce: 0.011303
2022-01-08 11:11:44,744 iteration 3590 : loss : 0.022765, loss_ce: 0.009810
2022-01-08 11:11:46,299 iteration 3591 : loss : 0.019573, loss_ce: 0.006321
2022-01-08 11:11:47,930 iteration 3592 : loss : 0.019526, loss_ce: 0.008293
2022-01-08 11:11:49,514 iteration 3593 : loss : 0.024958, loss_ce: 0.007473
2022-01-08 11:11:51,098 iteration 3594 : loss : 0.018889, loss_ce: 0.005802
2022-01-08 11:11:52,791 iteration 3595 : loss : 0.055060, loss_ce: 0.011769
2022-01-08 11:11:54,287 iteration 3596 : loss : 0.021605, loss_ce: 0.009771
2022-01-08 11:11:55,929 iteration 3597 : loss : 0.027170, loss_ce: 0.009674
2022-01-08 11:11:57,490 iteration 3598 : loss : 0.023340, loss_ce: 0.011761
2022-01-08 11:11:59,068 iteration 3599 : loss : 0.024142, loss_ce: 0.009939
2022-01-08 11:12:00,617 iteration 3600 : loss : 0.025067, loss_ce: 0.011555
2022-01-08 11:12:02,182 iteration 3601 : loss : 0.022970, loss_ce: 0.010783
2022-01-08 11:12:03,720 iteration 3602 : loss : 0.028911, loss_ce: 0.014585
2022-01-08 11:12:05,236 iteration 3603 : loss : 0.018951, loss_ce: 0.007367
2022-01-08 11:12:06,744 iteration 3604 : loss : 0.024686, loss_ce: 0.010015
 53%|██████████████▎            | 212/400 [1:43:57<1:29:49, 28.67s/it]2022-01-08 11:12:08,331 iteration 3605 : loss : 0.026194, loss_ce: 0.010130
2022-01-08 11:12:09,890 iteration 3606 : loss : 0.025149, loss_ce: 0.008731
2022-01-08 11:12:11,391 iteration 3607 : loss : 0.020924, loss_ce: 0.007394
2022-01-08 11:12:12,894 iteration 3608 : loss : 0.021236, loss_ce: 0.009783
2022-01-08 11:12:14,489 iteration 3609 : loss : 0.027650, loss_ce: 0.009994
2022-01-08 11:12:16,019 iteration 3610 : loss : 0.024295, loss_ce: 0.010860
2022-01-08 11:12:17,626 iteration 3611 : loss : 0.026860, loss_ce: 0.011084
2022-01-08 11:12:19,248 iteration 3612 : loss : 0.028317, loss_ce: 0.010074
2022-01-08 11:12:20,819 iteration 3613 : loss : 0.025745, loss_ce: 0.008706
2022-01-08 11:12:22,422 iteration 3614 : loss : 0.019737, loss_ce: 0.005820
2022-01-08 11:12:23,957 iteration 3615 : loss : 0.023816, loss_ce: 0.007861
2022-01-08 11:12:25,588 iteration 3616 : loss : 0.022608, loss_ce: 0.007020
2022-01-08 11:12:27,133 iteration 3617 : loss : 0.025050, loss_ce: 0.010102
2022-01-08 11:12:28,686 iteration 3618 : loss : 0.025718, loss_ce: 0.012005
2022-01-08 11:12:30,235 iteration 3619 : loss : 0.019242, loss_ce: 0.007468
2022-01-08 11:12:31,828 iteration 3620 : loss : 0.026730, loss_ce: 0.011969
2022-01-08 11:12:33,374 iteration 3621 : loss : 0.028914, loss_ce: 0.012562
 53%|██████████████▍            | 213/400 [1:44:23<1:27:26, 28.06s/it]2022-01-08 11:12:35,034 iteration 3622 : loss : 0.025260, loss_ce: 0.009872
2022-01-08 11:12:36,597 iteration 3623 : loss : 0.018893, loss_ce: 0.007686
2022-01-08 11:12:38,235 iteration 3624 : loss : 0.023530, loss_ce: 0.007845
2022-01-08 11:12:39,867 iteration 3625 : loss : 0.037439, loss_ce: 0.018226
2022-01-08 11:12:41,428 iteration 3626 : loss : 0.023002, loss_ce: 0.008914
2022-01-08 11:12:43,029 iteration 3627 : loss : 0.021797, loss_ce: 0.010830
2022-01-08 11:12:44,609 iteration 3628 : loss : 0.020280, loss_ce: 0.006107
2022-01-08 11:12:46,248 iteration 3629 : loss : 0.020076, loss_ce: 0.008505
2022-01-08 11:12:47,815 iteration 3630 : loss : 0.021823, loss_ce: 0.008771
2022-01-08 11:12:49,311 iteration 3631 : loss : 0.018661, loss_ce: 0.008607
2022-01-08 11:12:50,808 iteration 3632 : loss : 0.016405, loss_ce: 0.007005
2022-01-08 11:12:52,428 iteration 3633 : loss : 0.029809, loss_ce: 0.012639
2022-01-08 11:12:53,984 iteration 3634 : loss : 0.020850, loss_ce: 0.007944
2022-01-08 11:12:55,501 iteration 3635 : loss : 0.019749, loss_ce: 0.006736
2022-01-08 11:12:57,019 iteration 3636 : loss : 0.029903, loss_ce: 0.010696
2022-01-08 11:12:58,639 iteration 3637 : loss : 0.029392, loss_ce: 0.008141
2022-01-08 11:13:00,145 iteration 3638 : loss : 0.019925, loss_ce: 0.006847
 54%|██████████████▍            | 214/400 [1:44:50<1:25:46, 27.67s/it]2022-01-08 11:13:01,720 iteration 3639 : loss : 0.017594, loss_ce: 0.007357
2022-01-08 11:13:03,328 iteration 3640 : loss : 0.021237, loss_ce: 0.007852
2022-01-08 11:13:04,907 iteration 3641 : loss : 0.027057, loss_ce: 0.009575
2022-01-08 11:13:06,516 iteration 3642 : loss : 0.026905, loss_ce: 0.011236
2022-01-08 11:13:08,076 iteration 3643 : loss : 0.022744, loss_ce: 0.010446
2022-01-08 11:13:09,627 iteration 3644 : loss : 0.019603, loss_ce: 0.008901
2022-01-08 11:13:11,152 iteration 3645 : loss : 0.019678, loss_ce: 0.007142
2022-01-08 11:13:12,726 iteration 3646 : loss : 0.017575, loss_ce: 0.006126
2022-01-08 11:13:14,332 iteration 3647 : loss : 0.032405, loss_ce: 0.012662
2022-01-08 11:13:16,007 iteration 3648 : loss : 0.033853, loss_ce: 0.012970
2022-01-08 11:13:17,600 iteration 3649 : loss : 0.028661, loss_ce: 0.010257
2022-01-08 11:13:19,171 iteration 3650 : loss : 0.026007, loss_ce: 0.008584
2022-01-08 11:13:20,764 iteration 3651 : loss : 0.020315, loss_ce: 0.007728
2022-01-08 11:13:22,306 iteration 3652 : loss : 0.018814, loss_ce: 0.008229
2022-01-08 11:13:23,938 iteration 3653 : loss : 0.032079, loss_ce: 0.006592
2022-01-08 11:13:25,520 iteration 3654 : loss : 0.019965, loss_ce: 0.005825
2022-01-08 11:13:25,520 Training Data Eval:
2022-01-08 11:13:33,400   Average segmentation loss on training set: 0.0148
2022-01-08 11:13:33,400 Validation Data Eval:
2022-01-08 11:13:36,114   Average segmentation loss on validation set: 0.0922
2022-01-08 11:13:37,582 iteration 3655 : loss : 0.021956, loss_ce: 0.009274
 54%|██████████████▌            | 215/400 [1:45:27<1:34:21, 30.60s/it]2022-01-08 11:13:39,238 iteration 3656 : loss : 0.038212, loss_ce: 0.008244
2022-01-08 11:13:40,773 iteration 3657 : loss : 0.022153, loss_ce: 0.010956
2022-01-08 11:13:42,338 iteration 3658 : loss : 0.021261, loss_ce: 0.008082
2022-01-08 11:13:43,939 iteration 3659 : loss : 0.015646, loss_ce: 0.004847
2022-01-08 11:13:45,601 iteration 3660 : loss : 0.040344, loss_ce: 0.015212
2022-01-08 11:13:47,284 iteration 3661 : loss : 0.027453, loss_ce: 0.011213
2022-01-08 11:13:48,776 iteration 3662 : loss : 0.022764, loss_ce: 0.007986
2022-01-08 11:13:50,316 iteration 3663 : loss : 0.017940, loss_ce: 0.005533
2022-01-08 11:13:51,870 iteration 3664 : loss : 0.041896, loss_ce: 0.011226
2022-01-08 11:13:53,432 iteration 3665 : loss : 0.026287, loss_ce: 0.010630
2022-01-08 11:13:55,114 iteration 3666 : loss : 0.028486, loss_ce: 0.012303
2022-01-08 11:13:56,739 iteration 3667 : loss : 0.029287, loss_ce: 0.011132
2022-01-08 11:13:58,366 iteration 3668 : loss : 0.025601, loss_ce: 0.009845
2022-01-08 11:13:59,955 iteration 3669 : loss : 0.035663, loss_ce: 0.023253
2022-01-08 11:14:01,505 iteration 3670 : loss : 0.028260, loss_ce: 0.008261
2022-01-08 11:14:03,125 iteration 3671 : loss : 0.027077, loss_ce: 0.009692
2022-01-08 11:14:04,650 iteration 3672 : loss : 0.019337, loss_ce: 0.007750
 54%|██████████████▌            | 216/400 [1:45:54<1:30:35, 29.54s/it]2022-01-08 11:14:06,246 iteration 3673 : loss : 0.024579, loss_ce: 0.010880
2022-01-08 11:14:07,823 iteration 3674 : loss : 0.031084, loss_ce: 0.013836
2022-01-08 11:14:09,377 iteration 3675 : loss : 0.023165, loss_ce: 0.011785
2022-01-08 11:14:10,928 iteration 3676 : loss : 0.019344, loss_ce: 0.006042
2022-01-08 11:14:12,481 iteration 3677 : loss : 0.023546, loss_ce: 0.012180
2022-01-08 11:14:14,138 iteration 3678 : loss : 0.032409, loss_ce: 0.011700
2022-01-08 11:14:15,698 iteration 3679 : loss : 0.023080, loss_ce: 0.008658
2022-01-08 11:14:17,240 iteration 3680 : loss : 0.032316, loss_ce: 0.011789
2022-01-08 11:14:18,862 iteration 3681 : loss : 0.022714, loss_ce: 0.008230
2022-01-08 11:14:20,414 iteration 3682 : loss : 0.021429, loss_ce: 0.008768
2022-01-08 11:14:22,030 iteration 3683 : loss : 0.049438, loss_ce: 0.005563
2022-01-08 11:14:23,566 iteration 3684 : loss : 0.012535, loss_ce: 0.004005
2022-01-08 11:14:25,117 iteration 3685 : loss : 0.025287, loss_ce: 0.010965
2022-01-08 11:14:26,700 iteration 3686 : loss : 0.022319, loss_ce: 0.008611
2022-01-08 11:14:28,266 iteration 3687 : loss : 0.025616, loss_ce: 0.008155
2022-01-08 11:14:29,822 iteration 3688 : loss : 0.038784, loss_ce: 0.018614
2022-01-08 11:14:31,322 iteration 3689 : loss : 0.021602, loss_ce: 0.008227
 54%|██████████████▋            | 217/400 [1:46:21<1:27:28, 28.68s/it]2022-01-08 11:14:32,881 iteration 3690 : loss : 0.023629, loss_ce: 0.005884
2022-01-08 11:14:34,520 iteration 3691 : loss : 0.062149, loss_ce: 0.014323
2022-01-08 11:14:36,094 iteration 3692 : loss : 0.020758, loss_ce: 0.009147
2022-01-08 11:14:37,738 iteration 3693 : loss : 0.057710, loss_ce: 0.022264
2022-01-08 11:14:39,312 iteration 3694 : loss : 0.030511, loss_ce: 0.014660
2022-01-08 11:14:40,831 iteration 3695 : loss : 0.023892, loss_ce: 0.010654
2022-01-08 11:14:42,436 iteration 3696 : loss : 0.030170, loss_ce: 0.013913
2022-01-08 11:14:43,989 iteration 3697 : loss : 0.025965, loss_ce: 0.008219
2022-01-08 11:14:45,555 iteration 3698 : loss : 0.027314, loss_ce: 0.008829
2022-01-08 11:14:47,185 iteration 3699 : loss : 0.029807, loss_ce: 0.010476
2022-01-08 11:14:48,754 iteration 3700 : loss : 0.025683, loss_ce: 0.011291
2022-01-08 11:14:50,294 iteration 3701 : loss : 0.025695, loss_ce: 0.012347
2022-01-08 11:14:51,825 iteration 3702 : loss : 0.032206, loss_ce: 0.014072
2022-01-08 11:14:53,498 iteration 3703 : loss : 0.030761, loss_ce: 0.013790
2022-01-08 11:14:55,028 iteration 3704 : loss : 0.020975, loss_ce: 0.006338
2022-01-08 11:14:56,549 iteration 3705 : loss : 0.018277, loss_ce: 0.005871
2022-01-08 11:14:58,059 iteration 3706 : loss : 0.016862, loss_ce: 0.004501
 55%|██████████████▋            | 218/400 [1:46:48<1:25:13, 28.10s/it]2022-01-08 11:14:59,650 iteration 3707 : loss : 0.033894, loss_ce: 0.012928
2022-01-08 11:15:01,219 iteration 3708 : loss : 0.042326, loss_ce: 0.020038
2022-01-08 11:15:02,795 iteration 3709 : loss : 0.017809, loss_ce: 0.005638
2022-01-08 11:15:04,414 iteration 3710 : loss : 0.049216, loss_ce: 0.019027
2022-01-08 11:15:05,957 iteration 3711 : loss : 0.017647, loss_ce: 0.006488
2022-01-08 11:15:07,543 iteration 3712 : loss : 0.028446, loss_ce: 0.009056
2022-01-08 11:15:09,063 iteration 3713 : loss : 0.020204, loss_ce: 0.007896
2022-01-08 11:15:10,594 iteration 3714 : loss : 0.025434, loss_ce: 0.008674
2022-01-08 11:15:12,167 iteration 3715 : loss : 0.025535, loss_ce: 0.009574
2022-01-08 11:15:13,695 iteration 3716 : loss : 0.026408, loss_ce: 0.009166
2022-01-08 11:15:15,272 iteration 3717 : loss : 0.038866, loss_ce: 0.014865
2022-01-08 11:15:16,889 iteration 3718 : loss : 0.022974, loss_ce: 0.009888
2022-01-08 11:15:18,406 iteration 3719 : loss : 0.020904, loss_ce: 0.006136
2022-01-08 11:15:19,987 iteration 3720 : loss : 0.022098, loss_ce: 0.011773
2022-01-08 11:15:21,556 iteration 3721 : loss : 0.023944, loss_ce: 0.009079
2022-01-08 11:15:23,106 iteration 3722 : loss : 0.025314, loss_ce: 0.008956
2022-01-08 11:15:24,632 iteration 3723 : loss : 0.023621, loss_ce: 0.007046
 55%|██████████████▊            | 219/400 [1:47:14<1:23:22, 27.64s/it]2022-01-08 11:15:26,161 iteration 3724 : loss : 0.017985, loss_ce: 0.007808
2022-01-08 11:15:27,679 iteration 3725 : loss : 0.024503, loss_ce: 0.010612
2022-01-08 11:15:29,300 iteration 3726 : loss : 0.030685, loss_ce: 0.017080
2022-01-08 11:15:30,871 iteration 3727 : loss : 0.018026, loss_ce: 0.007446
2022-01-08 11:15:32,451 iteration 3728 : loss : 0.023243, loss_ce: 0.006148
2022-01-08 11:15:33,976 iteration 3729 : loss : 0.017759, loss_ce: 0.005246
2022-01-08 11:15:35,523 iteration 3730 : loss : 0.018228, loss_ce: 0.007467
2022-01-08 11:15:37,062 iteration 3731 : loss : 0.019667, loss_ce: 0.007005
2022-01-08 11:15:38,652 iteration 3732 : loss : 0.022917, loss_ce: 0.007491
2022-01-08 11:15:40,168 iteration 3733 : loss : 0.037219, loss_ce: 0.016462
2022-01-08 11:15:41,744 iteration 3734 : loss : 0.021434, loss_ce: 0.009386
2022-01-08 11:15:43,310 iteration 3735 : loss : 0.024136, loss_ce: 0.007588
2022-01-08 11:15:44,927 iteration 3736 : loss : 0.029719, loss_ce: 0.012827
2022-01-08 11:15:46,454 iteration 3737 : loss : 0.024593, loss_ce: 0.007983
2022-01-08 11:15:48,006 iteration 3738 : loss : 0.022457, loss_ce: 0.007119
2022-01-08 11:15:49,520 iteration 3739 : loss : 0.019667, loss_ce: 0.007051
2022-01-08 11:15:49,521 Training Data Eval:
2022-01-08 11:15:57,417   Average segmentation loss on training set: 0.0176
2022-01-08 11:15:57,417 Validation Data Eval:
2022-01-08 11:16:00,140   Average segmentation loss on validation set: 0.0611
2022-01-08 11:16:06,001 Found new lowest validation loss at iteration 3739! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 11:16:07,592 iteration 3740 : loss : 0.025629, loss_ce: 0.011792
 55%|██████████████▊            | 220/400 [1:47:57<1:36:42, 32.23s/it]2022-01-08 11:16:09,188 iteration 3741 : loss : 0.025305, loss_ce: 0.008035
2022-01-08 11:16:10,720 iteration 3742 : loss : 0.021761, loss_ce: 0.010763
2022-01-08 11:16:12,276 iteration 3743 : loss : 0.026070, loss_ce: 0.008826
2022-01-08 11:16:13,883 iteration 3744 : loss : 0.028524, loss_ce: 0.009886
2022-01-08 11:16:15,512 iteration 3745 : loss : 0.023996, loss_ce: 0.006405
2022-01-08 11:16:17,145 iteration 3746 : loss : 0.030144, loss_ce: 0.011722
2022-01-08 11:16:18,700 iteration 3747 : loss : 0.023044, loss_ce: 0.007875
2022-01-08 11:16:20,242 iteration 3748 : loss : 0.019146, loss_ce: 0.009411
2022-01-08 11:16:21,851 iteration 3749 : loss : 0.029451, loss_ce: 0.012194
2022-01-08 11:16:23,494 iteration 3750 : loss : 0.025822, loss_ce: 0.011398
2022-01-08 11:16:25,088 iteration 3751 : loss : 0.039605, loss_ce: 0.011558
2022-01-08 11:16:26,637 iteration 3752 : loss : 0.027809, loss_ce: 0.007206
2022-01-08 11:16:28,158 iteration 3753 : loss : 0.036945, loss_ce: 0.011227
2022-01-08 11:16:29,729 iteration 3754 : loss : 0.017547, loss_ce: 0.005990
2022-01-08 11:16:31,342 iteration 3755 : loss : 0.019533, loss_ce: 0.008871
2022-01-08 11:16:32,837 iteration 3756 : loss : 0.026308, loss_ce: 0.012302
2022-01-08 11:16:34,351 iteration 3757 : loss : 0.023461, loss_ce: 0.010178
 55%|██████████████▉            | 221/400 [1:48:24<1:31:16, 30.59s/it]2022-01-08 11:16:36,027 iteration 3758 : loss : 0.023351, loss_ce: 0.007549
2022-01-08 11:16:37,551 iteration 3759 : loss : 0.020190, loss_ce: 0.006356
2022-01-08 11:16:39,097 iteration 3760 : loss : 0.019625, loss_ce: 0.008146
2022-01-08 11:16:40,702 iteration 3761 : loss : 0.028988, loss_ce: 0.010543
2022-01-08 11:16:42,333 iteration 3762 : loss : 0.024852, loss_ce: 0.012833
2022-01-08 11:16:44,008 iteration 3763 : loss : 0.028427, loss_ce: 0.010493
2022-01-08 11:16:45,577 iteration 3764 : loss : 0.031262, loss_ce: 0.011023
2022-01-08 11:16:47,105 iteration 3765 : loss : 0.022107, loss_ce: 0.008923
2022-01-08 11:16:48,729 iteration 3766 : loss : 0.040917, loss_ce: 0.010539
2022-01-08 11:16:50,302 iteration 3767 : loss : 0.016365, loss_ce: 0.005781
2022-01-08 11:16:51,840 iteration 3768 : loss : 0.023938, loss_ce: 0.006988
2022-01-08 11:16:53,382 iteration 3769 : loss : 0.021076, loss_ce: 0.009048
2022-01-08 11:16:54,989 iteration 3770 : loss : 0.031003, loss_ce: 0.006314
2022-01-08 11:16:56,694 iteration 3771 : loss : 0.026425, loss_ce: 0.008622
2022-01-08 11:16:58,331 iteration 3772 : loss : 0.036501, loss_ce: 0.008395
2022-01-08 11:16:59,927 iteration 3773 : loss : 0.019227, loss_ce: 0.006615
2022-01-08 11:17:01,563 iteration 3774 : loss : 0.036081, loss_ce: 0.017733
 56%|██████████████▉            | 222/400 [1:48:51<1:27:44, 29.58s/it]2022-01-08 11:17:03,151 iteration 3775 : loss : 0.021232, loss_ce: 0.008361
2022-01-08 11:17:04,644 iteration 3776 : loss : 0.025337, loss_ce: 0.011064
2022-01-08 11:17:06,197 iteration 3777 : loss : 0.025074, loss_ce: 0.006959
2022-01-08 11:17:07,732 iteration 3778 : loss : 0.024644, loss_ce: 0.009475
2022-01-08 11:17:09,268 iteration 3779 : loss : 0.018113, loss_ce: 0.008134
2022-01-08 11:17:10,857 iteration 3780 : loss : 0.029830, loss_ce: 0.009693
2022-01-08 11:17:12,439 iteration 3781 : loss : 0.024466, loss_ce: 0.010904
2022-01-08 11:17:14,035 iteration 3782 : loss : 0.058293, loss_ce: 0.011769
2022-01-08 11:17:15,629 iteration 3783 : loss : 0.021250, loss_ce: 0.008543
2022-01-08 11:17:17,223 iteration 3784 : loss : 0.024984, loss_ce: 0.011645
2022-01-08 11:17:18,760 iteration 3785 : loss : 0.021403, loss_ce: 0.008967
2022-01-08 11:17:20,297 iteration 3786 : loss : 0.019819, loss_ce: 0.007158
2022-01-08 11:17:21,824 iteration 3787 : loss : 0.023695, loss_ce: 0.008012
2022-01-08 11:17:23,451 iteration 3788 : loss : 0.038105, loss_ce: 0.010027
2022-01-08 11:17:25,013 iteration 3789 : loss : 0.023856, loss_ce: 0.006305
2022-01-08 11:17:26,610 iteration 3790 : loss : 0.025157, loss_ce: 0.008743
2022-01-08 11:17:28,201 iteration 3791 : loss : 0.023915, loss_ce: 0.009352
 56%|███████████████            | 223/400 [1:49:18<1:24:39, 28.70s/it]2022-01-08 11:17:29,768 iteration 3792 : loss : 0.016912, loss_ce: 0.006413
2022-01-08 11:17:31,380 iteration 3793 : loss : 0.024758, loss_ce: 0.007213
2022-01-08 11:17:32,968 iteration 3794 : loss : 0.024680, loss_ce: 0.009053
2022-01-08 11:17:34,534 iteration 3795 : loss : 0.019621, loss_ce: 0.006083
2022-01-08 11:17:36,009 iteration 3796 : loss : 0.016552, loss_ce: 0.005275
2022-01-08 11:17:37,551 iteration 3797 : loss : 0.022771, loss_ce: 0.007257
2022-01-08 11:17:39,048 iteration 3798 : loss : 0.018267, loss_ce: 0.006670
2022-01-08 11:17:40,590 iteration 3799 : loss : 0.029392, loss_ce: 0.012936
2022-01-08 11:17:42,135 iteration 3800 : loss : 0.014504, loss_ce: 0.004613
2022-01-08 11:17:43,683 iteration 3801 : loss : 0.040775, loss_ce: 0.011905
2022-01-08 11:17:45,216 iteration 3802 : loss : 0.023440, loss_ce: 0.012282
2022-01-08 11:17:46,770 iteration 3803 : loss : 0.015816, loss_ce: 0.004947
2022-01-08 11:17:48,324 iteration 3804 : loss : 0.024742, loss_ce: 0.009624
2022-01-08 11:17:49,855 iteration 3805 : loss : 0.021738, loss_ce: 0.008261
2022-01-08 11:17:51,384 iteration 3806 : loss : 0.025517, loss_ce: 0.012309
2022-01-08 11:17:52,982 iteration 3807 : loss : 0.022542, loss_ce: 0.008720
2022-01-08 11:17:54,551 iteration 3808 : loss : 0.020781, loss_ce: 0.008328
 56%|███████████████            | 224/400 [1:49:44<1:22:06, 27.99s/it]2022-01-08 11:17:56,095 iteration 3809 : loss : 0.017926, loss_ce: 0.006077
2022-01-08 11:17:57,630 iteration 3810 : loss : 0.017906, loss_ce: 0.006716
2022-01-08 11:17:59,186 iteration 3811 : loss : 0.019272, loss_ce: 0.005042
2022-01-08 11:18:00,761 iteration 3812 : loss : 0.023685, loss_ce: 0.011992
2022-01-08 11:18:02,309 iteration 3813 : loss : 0.019095, loss_ce: 0.008878
2022-01-08 11:18:03,895 iteration 3814 : loss : 0.025859, loss_ce: 0.009333
2022-01-08 11:18:05,457 iteration 3815 : loss : 0.026104, loss_ce: 0.008719
2022-01-08 11:18:07,029 iteration 3816 : loss : 0.027175, loss_ce: 0.011878
2022-01-08 11:18:08,544 iteration 3817 : loss : 0.016472, loss_ce: 0.006091
2022-01-08 11:18:10,136 iteration 3818 : loss : 0.023225, loss_ce: 0.009824
2022-01-08 11:18:11,744 iteration 3819 : loss : 0.018524, loss_ce: 0.007686
2022-01-08 11:18:13,332 iteration 3820 : loss : 0.022311, loss_ce: 0.008179
2022-01-08 11:18:14,924 iteration 3821 : loss : 0.024008, loss_ce: 0.009564
2022-01-08 11:18:16,453 iteration 3822 : loss : 0.024039, loss_ce: 0.007530
2022-01-08 11:18:18,108 iteration 3823 : loss : 0.031091, loss_ce: 0.007147
2022-01-08 11:18:19,690 iteration 3824 : loss : 0.027922, loss_ce: 0.010217
2022-01-08 11:18:19,691 Training Data Eval:
2022-01-08 11:18:27,592   Average segmentation loss on training set: 0.0164
2022-01-08 11:18:27,592 Validation Data Eval:
2022-01-08 11:18:30,318   Average segmentation loss on validation set: 0.1200
2022-01-08 11:18:31,951 iteration 3825 : loss : 0.021942, loss_ce: 0.007593
 56%|███████████████▏           | 225/400 [1:50:22<1:29:52, 30.81s/it]2022-01-08 11:18:33,574 iteration 3826 : loss : 0.025309, loss_ce: 0.013052
2022-01-08 11:18:35,093 iteration 3827 : loss : 0.020656, loss_ce: 0.007137
2022-01-08 11:18:36,670 iteration 3828 : loss : 0.020739, loss_ce: 0.007797
2022-01-08 11:18:38,238 iteration 3829 : loss : 0.022300, loss_ce: 0.006668
2022-01-08 11:18:39,767 iteration 3830 : loss : 0.017531, loss_ce: 0.007367
2022-01-08 11:18:41,283 iteration 3831 : loss : 0.018862, loss_ce: 0.005932
2022-01-08 11:18:42,794 iteration 3832 : loss : 0.034931, loss_ce: 0.008865
2022-01-08 11:18:44,351 iteration 3833 : loss : 0.016219, loss_ce: 0.007268
2022-01-08 11:18:45,945 iteration 3834 : loss : 0.024146, loss_ce: 0.010449
2022-01-08 11:18:47,523 iteration 3835 : loss : 0.029645, loss_ce: 0.010828
2022-01-08 11:18:49,095 iteration 3836 : loss : 0.020554, loss_ce: 0.008060
2022-01-08 11:18:50,685 iteration 3837 : loss : 0.021643, loss_ce: 0.007796
2022-01-08 11:18:52,340 iteration 3838 : loss : 0.021686, loss_ce: 0.007576
2022-01-08 11:18:53,902 iteration 3839 : loss : 0.019650, loss_ce: 0.008084
2022-01-08 11:18:55,478 iteration 3840 : loss : 0.034482, loss_ce: 0.008572
2022-01-08 11:18:57,011 iteration 3841 : loss : 0.020092, loss_ce: 0.007481
2022-01-08 11:18:58,599 iteration 3842 : loss : 0.033223, loss_ce: 0.016065
 56%|███████████████▎           | 226/400 [1:50:48<1:25:44, 29.57s/it]2022-01-08 11:19:00,171 iteration 3843 : loss : 0.026467, loss_ce: 0.012894
2022-01-08 11:19:01,776 iteration 3844 : loss : 0.032433, loss_ce: 0.008968
2022-01-08 11:19:03,371 iteration 3845 : loss : 0.020845, loss_ce: 0.009478
2022-01-08 11:19:04,897 iteration 3846 : loss : 0.016878, loss_ce: 0.006130
2022-01-08 11:19:06,445 iteration 3847 : loss : 0.020043, loss_ce: 0.007256
2022-01-08 11:19:08,055 iteration 3848 : loss : 0.032651, loss_ce: 0.013311
2022-01-08 11:19:09,717 iteration 3849 : loss : 0.029188, loss_ce: 0.012509
2022-01-08 11:19:11,364 iteration 3850 : loss : 0.046320, loss_ce: 0.012112
2022-01-08 11:19:12,942 iteration 3851 : loss : 0.015974, loss_ce: 0.006805
2022-01-08 11:19:14,531 iteration 3852 : loss : 0.023585, loss_ce: 0.007386
2022-01-08 11:19:16,053 iteration 3853 : loss : 0.020271, loss_ce: 0.006092
2022-01-08 11:19:17,642 iteration 3854 : loss : 0.022189, loss_ce: 0.006636
2022-01-08 11:19:19,342 iteration 3855 : loss : 0.024152, loss_ce: 0.010724
2022-01-08 11:19:20,787 iteration 3856 : loss : 0.019598, loss_ce: 0.008563
2022-01-08 11:19:22,332 iteration 3857 : loss : 0.025308, loss_ce: 0.011714
2022-01-08 11:19:23,804 iteration 3858 : loss : 0.022618, loss_ce: 0.009159
2022-01-08 11:19:25,355 iteration 3859 : loss : 0.020162, loss_ce: 0.008228
 57%|███████████████▎           | 227/400 [1:51:15<1:22:48, 28.72s/it]2022-01-08 11:19:26,954 iteration 3860 : loss : 0.018368, loss_ce: 0.006976
2022-01-08 11:19:28,590 iteration 3861 : loss : 0.034714, loss_ce: 0.016580
2022-01-08 11:19:30,186 iteration 3862 : loss : 0.029444, loss_ce: 0.010689
2022-01-08 11:19:31,768 iteration 3863 : loss : 0.021171, loss_ce: 0.007536
2022-01-08 11:19:33,372 iteration 3864 : loss : 0.020629, loss_ce: 0.007595
2022-01-08 11:19:34,935 iteration 3865 : loss : 0.024752, loss_ce: 0.008737
2022-01-08 11:19:36,617 iteration 3866 : loss : 0.023831, loss_ce: 0.007842
2022-01-08 11:19:38,184 iteration 3867 : loss : 0.018739, loss_ce: 0.006397
2022-01-08 11:19:39,816 iteration 3868 : loss : 0.059069, loss_ce: 0.015703
2022-01-08 11:19:41,429 iteration 3869 : loss : 0.021532, loss_ce: 0.009306
2022-01-08 11:19:43,061 iteration 3870 : loss : 0.018724, loss_ce: 0.007178
2022-01-08 11:19:44,666 iteration 3871 : loss : 0.031211, loss_ce: 0.011054
2022-01-08 11:19:46,199 iteration 3872 : loss : 0.023812, loss_ce: 0.009043
2022-01-08 11:19:47,727 iteration 3873 : loss : 0.018941, loss_ce: 0.009825
2022-01-08 11:19:49,318 iteration 3874 : loss : 0.024137, loss_ce: 0.010410
2022-01-08 11:19:50,900 iteration 3875 : loss : 0.030450, loss_ce: 0.011933
2022-01-08 11:19:52,488 iteration 3876 : loss : 0.030311, loss_ce: 0.010513
 57%|███████████████▍           | 228/400 [1:51:42<1:20:58, 28.25s/it]2022-01-08 11:19:54,066 iteration 3877 : loss : 0.023439, loss_ce: 0.008958
2022-01-08 11:19:55,662 iteration 3878 : loss : 0.023060, loss_ce: 0.008360
2022-01-08 11:19:57,202 iteration 3879 : loss : 0.020212, loss_ce: 0.007648
2022-01-08 11:19:58,801 iteration 3880 : loss : 0.024254, loss_ce: 0.010218
2022-01-08 11:20:00,341 iteration 3881 : loss : 0.018338, loss_ce: 0.008563
2022-01-08 11:20:02,040 iteration 3882 : loss : 0.031729, loss_ce: 0.008642
2022-01-08 11:20:03,666 iteration 3883 : loss : 0.032398, loss_ce: 0.009940
2022-01-08 11:20:05,259 iteration 3884 : loss : 0.021846, loss_ce: 0.009388
2022-01-08 11:20:06,872 iteration 3885 : loss : 0.047825, loss_ce: 0.010785
2022-01-08 11:20:08,490 iteration 3886 : loss : 0.027740, loss_ce: 0.009649
2022-01-08 11:20:10,012 iteration 3887 : loss : 0.018882, loss_ce: 0.007435
2022-01-08 11:20:11,620 iteration 3888 : loss : 0.026638, loss_ce: 0.013825
2022-01-08 11:20:13,199 iteration 3889 : loss : 0.018874, loss_ce: 0.006860
2022-01-08 11:20:14,876 iteration 3890 : loss : 0.031888, loss_ce: 0.010781
2022-01-08 11:20:16,423 iteration 3891 : loss : 0.030124, loss_ce: 0.011653
2022-01-08 11:20:17,997 iteration 3892 : loss : 0.024484, loss_ce: 0.010273
2022-01-08 11:20:19,598 iteration 3893 : loss : 0.037518, loss_ce: 0.012364
 57%|███████████████▍           | 229/400 [1:52:09<1:19:31, 27.90s/it]2022-01-08 11:20:21,195 iteration 3894 : loss : 0.023582, loss_ce: 0.007455
2022-01-08 11:20:22,722 iteration 3895 : loss : 0.022186, loss_ce: 0.008287
2022-01-08 11:20:24,291 iteration 3896 : loss : 0.025844, loss_ce: 0.011799
2022-01-08 11:20:25,899 iteration 3897 : loss : 0.022357, loss_ce: 0.009525
2022-01-08 11:20:27,495 iteration 3898 : loss : 0.031202, loss_ce: 0.009215
2022-01-08 11:20:29,005 iteration 3899 : loss : 0.024694, loss_ce: 0.008975
2022-01-08 11:20:30,546 iteration 3900 : loss : 0.024332, loss_ce: 0.009179
2022-01-08 11:20:32,061 iteration 3901 : loss : 0.021557, loss_ce: 0.008581
2022-01-08 11:20:33,585 iteration 3902 : loss : 0.025769, loss_ce: 0.008900
2022-01-08 11:20:35,074 iteration 3903 : loss : 0.024669, loss_ce: 0.007225
2022-01-08 11:20:36,618 iteration 3904 : loss : 0.023784, loss_ce: 0.007291
2022-01-08 11:20:38,170 iteration 3905 : loss : 0.019081, loss_ce: 0.006348
2022-01-08 11:20:39,650 iteration 3906 : loss : 0.017533, loss_ce: 0.007222
2022-01-08 11:20:41,113 iteration 3907 : loss : 0.018613, loss_ce: 0.006668
2022-01-08 11:20:42,600 iteration 3908 : loss : 0.019319, loss_ce: 0.007277
2022-01-08 11:20:44,148 iteration 3909 : loss : 0.027671, loss_ce: 0.015213
2022-01-08 11:20:44,148 Training Data Eval:
2022-01-08 11:20:52,044   Average segmentation loss on training set: 0.0152
2022-01-08 11:20:52,045 Validation Data Eval:
2022-01-08 11:20:54,762   Average segmentation loss on validation set: 0.0772
2022-01-08 11:20:56,386 iteration 3910 : loss : 0.021642, loss_ce: 0.007313
 57%|███████████████▌           | 230/400 [1:52:46<1:26:37, 30.57s/it]2022-01-08 11:20:57,943 iteration 3911 : loss : 0.022337, loss_ce: 0.008813
2022-01-08 11:20:59,533 iteration 3912 : loss : 0.023391, loss_ce: 0.009045
2022-01-08 11:21:01,089 iteration 3913 : loss : 0.017387, loss_ce: 0.006827
2022-01-08 11:21:02,639 iteration 3914 : loss : 0.016658, loss_ce: 0.007547
2022-01-08 11:21:04,186 iteration 3915 : loss : 0.019457, loss_ce: 0.008219
2022-01-08 11:21:05,703 iteration 3916 : loss : 0.023319, loss_ce: 0.007661
2022-01-08 11:21:07,295 iteration 3917 : loss : 0.026417, loss_ce: 0.009466
2022-01-08 11:21:08,882 iteration 3918 : loss : 0.027392, loss_ce: 0.008445
2022-01-08 11:21:10,564 iteration 3919 : loss : 0.037066, loss_ce: 0.009739
2022-01-08 11:21:12,087 iteration 3920 : loss : 0.020839, loss_ce: 0.008904
2022-01-08 11:21:13,639 iteration 3921 : loss : 0.025190, loss_ce: 0.010030
2022-01-08 11:21:15,238 iteration 3922 : loss : 0.031476, loss_ce: 0.011760
2022-01-08 11:21:16,776 iteration 3923 : loss : 0.019312, loss_ce: 0.010933
2022-01-08 11:21:18,288 iteration 3924 : loss : 0.021693, loss_ce: 0.005469
2022-01-08 11:21:19,922 iteration 3925 : loss : 0.025261, loss_ce: 0.008421
2022-01-08 11:21:21,532 iteration 3926 : loss : 0.017020, loss_ce: 0.005834
2022-01-08 11:21:23,090 iteration 3927 : loss : 0.031616, loss_ce: 0.013038
 58%|███████████████▌           | 231/400 [1:53:13<1:22:50, 29.41s/it]2022-01-08 11:21:24,740 iteration 3928 : loss : 0.034852, loss_ce: 0.015814
2022-01-08 11:21:26,309 iteration 3929 : loss : 0.035515, loss_ce: 0.019780
2022-01-08 11:21:27,924 iteration 3930 : loss : 0.022117, loss_ce: 0.009773
2022-01-08 11:21:29,532 iteration 3931 : loss : 0.018050, loss_ce: 0.007462
2022-01-08 11:21:31,139 iteration 3932 : loss : 0.047367, loss_ce: 0.014965
2022-01-08 11:21:32,784 iteration 3933 : loss : 0.023250, loss_ce: 0.009595
2022-01-08 11:21:34,384 iteration 3934 : loss : 0.017386, loss_ce: 0.006720
2022-01-08 11:21:35,879 iteration 3935 : loss : 0.020228, loss_ce: 0.009098
2022-01-08 11:21:37,571 iteration 3936 : loss : 0.026131, loss_ce: 0.009105
2022-01-08 11:21:39,094 iteration 3937 : loss : 0.023391, loss_ce: 0.011106
2022-01-08 11:21:40,707 iteration 3938 : loss : 0.068080, loss_ce: 0.013556
2022-01-08 11:21:42,199 iteration 3939 : loss : 0.023970, loss_ce: 0.007199
2022-01-08 11:21:43,736 iteration 3940 : loss : 0.020665, loss_ce: 0.007654
2022-01-08 11:21:45,295 iteration 3941 : loss : 0.020888, loss_ce: 0.008426
2022-01-08 11:21:46,910 iteration 3942 : loss : 0.070529, loss_ce: 0.041449
2022-01-08 11:21:48,438 iteration 3943 : loss : 0.027688, loss_ce: 0.007374
2022-01-08 11:21:50,060 iteration 3944 : loss : 0.039560, loss_ce: 0.016542
 58%|███████████████▋           | 232/400 [1:53:40<1:20:17, 28.68s/it]2022-01-08 11:21:51,689 iteration 3945 : loss : 0.033400, loss_ce: 0.017387
2022-01-08 11:21:53,165 iteration 3946 : loss : 0.019822, loss_ce: 0.010382
2022-01-08 11:21:54,717 iteration 3947 : loss : 0.029994, loss_ce: 0.009412
2022-01-08 11:21:56,421 iteration 3948 : loss : 0.064379, loss_ce: 0.018078
2022-01-08 11:21:58,046 iteration 3949 : loss : 0.038806, loss_ce: 0.013683
2022-01-08 11:21:59,690 iteration 3950 : loss : 0.028958, loss_ce: 0.008950
2022-01-08 11:22:01,204 iteration 3951 : loss : 0.022628, loss_ce: 0.009759
2022-01-08 11:22:02,827 iteration 3952 : loss : 0.025236, loss_ce: 0.010368
2022-01-08 11:22:04,462 iteration 3953 : loss : 0.044481, loss_ce: 0.014877
2022-01-08 11:22:06,019 iteration 3954 : loss : 0.029245, loss_ce: 0.010273
2022-01-08 11:22:07,571 iteration 3955 : loss : 0.021938, loss_ce: 0.008837
2022-01-08 11:22:09,255 iteration 3956 : loss : 0.039785, loss_ce: 0.014845
2022-01-08 11:22:10,784 iteration 3957 : loss : 0.022840, loss_ce: 0.009057
2022-01-08 11:22:12,324 iteration 3958 : loss : 0.023009, loss_ce: 0.008317
2022-01-08 11:22:13,968 iteration 3959 : loss : 0.034251, loss_ce: 0.009508
2022-01-08 11:22:15,509 iteration 3960 : loss : 0.022452, loss_ce: 0.011171
2022-01-08 11:22:17,068 iteration 3961 : loss : 0.026920, loss_ce: 0.010945
 58%|███████████████▋           | 233/400 [1:54:07<1:18:25, 28.18s/it]2022-01-08 11:22:18,701 iteration 3962 : loss : 0.024894, loss_ce: 0.008455
2022-01-08 11:22:20,269 iteration 3963 : loss : 0.023776, loss_ce: 0.008585
2022-01-08 11:22:21,925 iteration 3964 : loss : 0.031096, loss_ce: 0.012566
2022-01-08 11:22:23,445 iteration 3965 : loss : 0.022972, loss_ce: 0.005917
2022-01-08 11:22:25,056 iteration 3966 : loss : 0.046578, loss_ce: 0.015216
2022-01-08 11:22:26,630 iteration 3967 : loss : 0.018416, loss_ce: 0.006269
2022-01-08 11:22:28,101 iteration 3968 : loss : 0.022804, loss_ce: 0.007317
2022-01-08 11:22:29,679 iteration 3969 : loss : 0.022400, loss_ce: 0.006265
2022-01-08 11:22:31,231 iteration 3970 : loss : 0.022200, loss_ce: 0.006904
2022-01-08 11:22:32,692 iteration 3971 : loss : 0.019247, loss_ce: 0.007242
2022-01-08 11:22:34,283 iteration 3972 : loss : 0.026636, loss_ce: 0.006923
2022-01-08 11:22:35,798 iteration 3973 : loss : 0.037579, loss_ce: 0.019622
2022-01-08 11:22:37,273 iteration 3974 : loss : 0.018747, loss_ce: 0.005914
2022-01-08 11:22:38,849 iteration 3975 : loss : 0.021805, loss_ce: 0.008193
2022-01-08 11:22:40,476 iteration 3976 : loss : 0.024125, loss_ce: 0.009351
2022-01-08 11:22:42,099 iteration 3977 : loss : 0.023293, loss_ce: 0.008882
2022-01-08 11:22:43,748 iteration 3978 : loss : 0.030490, loss_ce: 0.010870
 58%|███████████████▊           | 234/400 [1:54:34<1:16:42, 27.73s/it]2022-01-08 11:22:45,353 iteration 3979 : loss : 0.025912, loss_ce: 0.008855
2022-01-08 11:22:46,958 iteration 3980 : loss : 0.022139, loss_ce: 0.007115
2022-01-08 11:22:48,504 iteration 3981 : loss : 0.027052, loss_ce: 0.011405
2022-01-08 11:22:49,967 iteration 3982 : loss : 0.016055, loss_ce: 0.005683
2022-01-08 11:22:51,449 iteration 3983 : loss : 0.018850, loss_ce: 0.008456
2022-01-08 11:22:52,973 iteration 3984 : loss : 0.015571, loss_ce: 0.006197
2022-01-08 11:22:54,554 iteration 3985 : loss : 0.043387, loss_ce: 0.032046
2022-01-08 11:22:56,194 iteration 3986 : loss : 0.025549, loss_ce: 0.011121
2022-01-08 11:22:57,783 iteration 3987 : loss : 0.037765, loss_ce: 0.016224
2022-01-08 11:22:59,346 iteration 3988 : loss : 0.022085, loss_ce: 0.009587
2022-01-08 11:23:00,884 iteration 3989 : loss : 0.088987, loss_ce: 0.038947
2022-01-08 11:23:02,423 iteration 3990 : loss : 0.032249, loss_ce: 0.010570
2022-01-08 11:23:04,055 iteration 3991 : loss : 0.026464, loss_ce: 0.005955
2022-01-08 11:23:05,635 iteration 3992 : loss : 0.026342, loss_ce: 0.008481
2022-01-08 11:23:07,250 iteration 3993 : loss : 0.029828, loss_ce: 0.009175
2022-01-08 11:23:08,795 iteration 3994 : loss : 0.021092, loss_ce: 0.009663
2022-01-08 11:23:08,795 Training Data Eval:
2022-01-08 11:23:16,684   Average segmentation loss on training set: 0.0197
2022-01-08 11:23:16,685 Validation Data Eval:
2022-01-08 11:23:19,404   Average segmentation loss on validation set: 0.0812
2022-01-08 11:23:20,909 iteration 3995 : loss : 0.026719, loss_ce: 0.010735
 59%|███████████████▊           | 235/400 [1:55:11<1:24:02, 30.56s/it]2022-01-08 11:23:22,572 iteration 3996 : loss : 0.025580, loss_ce: 0.007543
2022-01-08 11:23:24,080 iteration 3997 : loss : 0.024570, loss_ce: 0.007813
2022-01-08 11:23:25,659 iteration 3998 : loss : 0.032007, loss_ce: 0.015661
2022-01-08 11:23:27,205 iteration 3999 : loss : 0.026830, loss_ce: 0.006456
2022-01-08 11:23:28,819 iteration 4000 : loss : 0.039817, loss_ce: 0.023605
2022-01-08 11:23:30,317 iteration 4001 : loss : 0.019246, loss_ce: 0.007063
2022-01-08 11:23:31,878 iteration 4002 : loss : 0.019032, loss_ce: 0.007674
2022-01-08 11:23:33,449 iteration 4003 : loss : 0.024099, loss_ce: 0.010842
2022-01-08 11:23:35,045 iteration 4004 : loss : 0.041695, loss_ce: 0.012054
2022-01-08 11:23:36,612 iteration 4005 : loss : 0.024090, loss_ce: 0.012926
2022-01-08 11:23:38,123 iteration 4006 : loss : 0.018121, loss_ce: 0.007356
2022-01-08 11:23:39,731 iteration 4007 : loss : 0.025987, loss_ce: 0.008610
2022-01-08 11:23:41,256 iteration 4008 : loss : 0.036086, loss_ce: 0.013587
2022-01-08 11:23:42,813 iteration 4009 : loss : 0.030551, loss_ce: 0.014997
2022-01-08 11:23:44,438 iteration 4010 : loss : 0.029974, loss_ce: 0.011761
2022-01-08 11:23:45,954 iteration 4011 : loss : 0.022011, loss_ce: 0.006026
2022-01-08 11:23:47,592 iteration 4012 : loss : 0.034936, loss_ce: 0.015978
 59%|███████████████▉           | 236/400 [1:55:37<1:20:21, 29.40s/it]2022-01-08 11:23:49,275 iteration 4013 : loss : 0.041550, loss_ce: 0.011444
2022-01-08 11:23:50,790 iteration 4014 : loss : 0.020707, loss_ce: 0.006600
2022-01-08 11:23:52,401 iteration 4015 : loss : 0.035921, loss_ce: 0.015908
2022-01-08 11:23:54,004 iteration 4016 : loss : 0.024014, loss_ce: 0.008755
2022-01-08 11:23:55,625 iteration 4017 : loss : 0.024977, loss_ce: 0.007851
2022-01-08 11:23:57,151 iteration 4018 : loss : 0.022891, loss_ce: 0.007685
2022-01-08 11:23:58,706 iteration 4019 : loss : 0.024940, loss_ce: 0.013277
2022-01-08 11:24:00,218 iteration 4020 : loss : 0.022741, loss_ce: 0.013122
2022-01-08 11:24:01,751 iteration 4021 : loss : 0.019837, loss_ce: 0.008083
2022-01-08 11:24:03,338 iteration 4022 : loss : 0.022343, loss_ce: 0.010730
2022-01-08 11:24:04,899 iteration 4023 : loss : 0.017897, loss_ce: 0.006574
2022-01-08 11:24:06,398 iteration 4024 : loss : 0.021028, loss_ce: 0.007762
2022-01-08 11:24:07,917 iteration 4025 : loss : 0.020425, loss_ce: 0.007490
2022-01-08 11:24:09,506 iteration 4026 : loss : 0.020456, loss_ce: 0.008728
2022-01-08 11:24:11,093 iteration 4027 : loss : 0.025492, loss_ce: 0.008314
2022-01-08 11:24:12,709 iteration 4028 : loss : 0.027527, loss_ce: 0.008771
2022-01-08 11:24:14,230 iteration 4029 : loss : 0.027255, loss_ce: 0.013056
 59%|███████████████▉           | 237/400 [1:56:04<1:17:36, 28.57s/it]2022-01-08 11:24:15,895 iteration 4030 : loss : 0.026920, loss_ce: 0.011205
2022-01-08 11:24:17,482 iteration 4031 : loss : 0.027552, loss_ce: 0.010022
2022-01-08 11:24:19,038 iteration 4032 : loss : 0.023609, loss_ce: 0.010062
2022-01-08 11:24:20,605 iteration 4033 : loss : 0.036228, loss_ce: 0.011094
2022-01-08 11:24:22,111 iteration 4034 : loss : 0.029954, loss_ce: 0.010488
2022-01-08 11:24:23,651 iteration 4035 : loss : 0.017799, loss_ce: 0.006317
2022-01-08 11:24:25,251 iteration 4036 : loss : 0.033214, loss_ce: 0.012310
2022-01-08 11:24:26,767 iteration 4037 : loss : 0.016785, loss_ce: 0.008031
2022-01-08 11:24:28,395 iteration 4038 : loss : 0.025021, loss_ce: 0.008141
2022-01-08 11:24:30,055 iteration 4039 : loss : 0.027647, loss_ce: 0.011081
2022-01-08 11:24:31,717 iteration 4040 : loss : 0.030926, loss_ce: 0.014043
2022-01-08 11:24:33,351 iteration 4041 : loss : 0.022775, loss_ce: 0.006439
2022-01-08 11:24:34,887 iteration 4042 : loss : 0.020253, loss_ce: 0.007138
2022-01-08 11:24:36,466 iteration 4043 : loss : 0.177680, loss_ce: 0.006278
2022-01-08 11:24:37,999 iteration 4044 : loss : 0.021318, loss_ce: 0.008332
2022-01-08 11:24:39,557 iteration 4045 : loss : 0.018828, loss_ce: 0.008857
2022-01-08 11:24:41,177 iteration 4046 : loss : 0.024618, loss_ce: 0.011729
 60%|████████████████           | 238/400 [1:56:31<1:15:49, 28.08s/it]2022-01-08 11:24:42,867 iteration 4047 : loss : 0.021499, loss_ce: 0.009331
2022-01-08 11:24:44,475 iteration 4048 : loss : 0.041079, loss_ce: 0.019598
2022-01-08 11:24:46,009 iteration 4049 : loss : 0.019493, loss_ce: 0.007214
2022-01-08 11:24:47,603 iteration 4050 : loss : 0.021498, loss_ce: 0.008286
2022-01-08 11:24:49,278 iteration 4051 : loss : 0.091113, loss_ce: 0.010810
2022-01-08 11:24:50,787 iteration 4052 : loss : 0.021488, loss_ce: 0.007553
2022-01-08 11:24:52,267 iteration 4053 : loss : 0.017139, loss_ce: 0.005726
2022-01-08 11:24:53,784 iteration 4054 : loss : 0.027944, loss_ce: 0.008295
2022-01-08 11:24:55,331 iteration 4055 : loss : 0.019989, loss_ce: 0.009365
2022-01-08 11:24:56,909 iteration 4056 : loss : 0.021363, loss_ce: 0.010251
2022-01-08 11:24:58,426 iteration 4057 : loss : 0.017563, loss_ce: 0.004538
2022-01-08 11:24:59,974 iteration 4058 : loss : 0.017879, loss_ce: 0.006824
2022-01-08 11:25:01,635 iteration 4059 : loss : 0.032283, loss_ce: 0.015337
2022-01-08 11:25:03,213 iteration 4060 : loss : 0.025148, loss_ce: 0.010035
2022-01-08 11:25:04,778 iteration 4061 : loss : 0.028827, loss_ce: 0.013745
2022-01-08 11:25:06,374 iteration 4062 : loss : 0.026933, loss_ce: 0.010300
2022-01-08 11:25:07,955 iteration 4063 : loss : 0.030620, loss_ce: 0.008860
 60%|████████████████▏          | 239/400 [1:56:58<1:14:17, 27.69s/it]2022-01-08 11:25:09,538 iteration 4064 : loss : 0.028827, loss_ce: 0.007599
2022-01-08 11:25:11,072 iteration 4065 : loss : 0.019786, loss_ce: 0.006679
2022-01-08 11:25:12,701 iteration 4066 : loss : 0.029144, loss_ce: 0.011187
2022-01-08 11:25:14,231 iteration 4067 : loss : 0.020535, loss_ce: 0.009180
2022-01-08 11:25:15,794 iteration 4068 : loss : 0.022206, loss_ce: 0.008961
2022-01-08 11:25:17,384 iteration 4069 : loss : 0.019279, loss_ce: 0.008073
2022-01-08 11:25:18,970 iteration 4070 : loss : 0.023597, loss_ce: 0.010549
2022-01-08 11:25:20,596 iteration 4071 : loss : 0.026769, loss_ce: 0.009195
2022-01-08 11:25:22,157 iteration 4072 : loss : 0.018891, loss_ce: 0.005591
2022-01-08 11:25:23,685 iteration 4073 : loss : 0.022199, loss_ce: 0.008749
2022-01-08 11:25:25,244 iteration 4074 : loss : 0.018208, loss_ce: 0.008041
2022-01-08 11:25:26,850 iteration 4075 : loss : 0.019054, loss_ce: 0.006497
2022-01-08 11:25:28,371 iteration 4076 : loss : 0.024043, loss_ce: 0.009582
2022-01-08 11:25:29,923 iteration 4077 : loss : 0.017591, loss_ce: 0.006083
2022-01-08 11:25:31,539 iteration 4078 : loss : 0.042436, loss_ce: 0.015277
2022-01-08 11:25:33,137 iteration 4079 : loss : 0.021487, loss_ce: 0.009259
2022-01-08 11:25:33,138 Training Data Eval:
2022-01-08 11:25:41,031   Average segmentation loss on training set: 0.0130
2022-01-08 11:25:41,031 Validation Data Eval:
2022-01-08 11:25:43,749   Average segmentation loss on validation set: 0.0643
2022-01-08 11:25:45,361 iteration 4080 : loss : 0.020801, loss_ce: 0.008429
 60%|████████████████▏          | 240/400 [1:57:35<1:21:37, 30.61s/it]2022-01-08 11:25:46,992 iteration 4081 : loss : 0.031226, loss_ce: 0.012147
2022-01-08 11:25:48,568 iteration 4082 : loss : 0.015260, loss_ce: 0.005377
2022-01-08 11:25:50,084 iteration 4083 : loss : 0.015280, loss_ce: 0.006109
2022-01-08 11:25:51,647 iteration 4084 : loss : 0.015636, loss_ce: 0.004623
2022-01-08 11:25:53,165 iteration 4085 : loss : 0.021307, loss_ce: 0.006428
2022-01-08 11:25:54,848 iteration 4086 : loss : 0.036804, loss_ce: 0.012564
2022-01-08 11:25:56,436 iteration 4087 : loss : 0.033446, loss_ce: 0.006560
2022-01-08 11:25:57,939 iteration 4088 : loss : 0.017485, loss_ce: 0.008005
2022-01-08 11:25:59,504 iteration 4089 : loss : 0.036588, loss_ce: 0.013895
2022-01-08 11:26:00,975 iteration 4090 : loss : 0.026764, loss_ce: 0.007419
2022-01-08 11:26:02,526 iteration 4091 : loss : 0.029984, loss_ce: 0.011234
2022-01-08 11:26:04,102 iteration 4092 : loss : 0.020864, loss_ce: 0.007435
2022-01-08 11:26:05,677 iteration 4093 : loss : 0.036779, loss_ce: 0.011596
2022-01-08 11:26:07,216 iteration 4094 : loss : 0.024739, loss_ce: 0.009891
2022-01-08 11:26:08,800 iteration 4095 : loss : 0.032852, loss_ce: 0.013362
2022-01-08 11:26:10,336 iteration 4096 : loss : 0.020404, loss_ce: 0.010740
2022-01-08 11:26:11,884 iteration 4097 : loss : 0.025050, loss_ce: 0.007901
 60%|████████████████▎          | 241/400 [1:58:02<1:17:51, 29.38s/it]2022-01-08 11:26:13,471 iteration 4098 : loss : 0.023082, loss_ce: 0.008213
2022-01-08 11:26:15,007 iteration 4099 : loss : 0.022815, loss_ce: 0.005694
2022-01-08 11:26:16,584 iteration 4100 : loss : 0.026047, loss_ce: 0.008988
2022-01-08 11:26:18,202 iteration 4101 : loss : 0.028939, loss_ce: 0.009759
2022-01-08 11:26:19,813 iteration 4102 : loss : 0.025983, loss_ce: 0.011317
2022-01-08 11:26:21,410 iteration 4103 : loss : 0.031321, loss_ce: 0.013135
2022-01-08 11:26:22,990 iteration 4104 : loss : 0.040172, loss_ce: 0.016397
2022-01-08 11:26:24,585 iteration 4105 : loss : 0.025345, loss_ce: 0.010048
2022-01-08 11:26:26,142 iteration 4106 : loss : 0.024729, loss_ce: 0.010029
2022-01-08 11:26:27,697 iteration 4107 : loss : 0.018234, loss_ce: 0.007730
2022-01-08 11:26:29,270 iteration 4108 : loss : 0.020361, loss_ce: 0.008655
2022-01-08 11:26:30,781 iteration 4109 : loss : 0.016994, loss_ce: 0.005904
2022-01-08 11:26:32,384 iteration 4110 : loss : 0.025024, loss_ce: 0.011849
2022-01-08 11:26:34,107 iteration 4111 : loss : 0.027312, loss_ce: 0.014327
2022-01-08 11:26:35,645 iteration 4112 : loss : 0.018230, loss_ce: 0.006368
2022-01-08 11:26:37,255 iteration 4113 : loss : 0.026333, loss_ce: 0.008857
2022-01-08 11:26:38,819 iteration 4114 : loss : 0.016474, loss_ce: 0.007143
 60%|████████████████▎          | 242/400 [1:58:29<1:15:26, 28.65s/it]2022-01-08 11:26:40,386 iteration 4115 : loss : 0.019461, loss_ce: 0.009572
2022-01-08 11:26:42,017 iteration 4116 : loss : 0.017345, loss_ce: 0.005949
2022-01-08 11:26:43,530 iteration 4117 : loss : 0.018035, loss_ce: 0.007360
2022-01-08 11:26:45,163 iteration 4118 : loss : 0.026298, loss_ce: 0.010946
2022-01-08 11:26:46,857 iteration 4119 : loss : 0.038052, loss_ce: 0.013835
2022-01-08 11:26:48,448 iteration 4120 : loss : 0.026717, loss_ce: 0.007008
2022-01-08 11:26:49,979 iteration 4121 : loss : 0.022646, loss_ce: 0.006348
2022-01-08 11:26:51,643 iteration 4122 : loss : 0.034737, loss_ce: 0.010468
2022-01-08 11:26:53,152 iteration 4123 : loss : 0.011991, loss_ce: 0.005075
2022-01-08 11:26:54,749 iteration 4124 : loss : 0.025004, loss_ce: 0.012034
2022-01-08 11:26:56,339 iteration 4125 : loss : 0.019514, loss_ce: 0.009161
2022-01-08 11:26:57,913 iteration 4126 : loss : 0.020137, loss_ce: 0.008521
2022-01-08 11:26:59,436 iteration 4127 : loss : 0.015514, loss_ce: 0.007412
2022-01-08 11:27:01,102 iteration 4128 : loss : 0.049766, loss_ce: 0.013507
2022-01-08 11:27:02,610 iteration 4129 : loss : 0.029067, loss_ce: 0.006785
2022-01-08 11:27:04,106 iteration 4130 : loss : 0.017092, loss_ce: 0.005063
2022-01-08 11:27:05,647 iteration 4131 : loss : 0.016389, loss_ce: 0.007320
 61%|████████████████▍          | 243/400 [1:58:55<1:13:31, 28.10s/it]2022-01-08 11:27:07,296 iteration 4132 : loss : 0.027970, loss_ce: 0.007313
2022-01-08 11:27:08,876 iteration 4133 : loss : 0.023144, loss_ce: 0.008284
2022-01-08 11:27:10,476 iteration 4134 : loss : 0.033575, loss_ce: 0.013341
2022-01-08 11:27:11,964 iteration 4135 : loss : 0.021457, loss_ce: 0.008633
2022-01-08 11:27:13,518 iteration 4136 : loss : 0.020879, loss_ce: 0.009442
2022-01-08 11:27:15,008 iteration 4137 : loss : 0.015840, loss_ce: 0.005499
2022-01-08 11:27:16,591 iteration 4138 : loss : 0.026110, loss_ce: 0.011182
2022-01-08 11:27:18,179 iteration 4139 : loss : 0.016701, loss_ce: 0.007337
2022-01-08 11:27:19,757 iteration 4140 : loss : 0.017725, loss_ce: 0.007134
2022-01-08 11:27:21,340 iteration 4141 : loss : 0.026824, loss_ce: 0.009625
2022-01-08 11:27:22,974 iteration 4142 : loss : 0.033234, loss_ce: 0.011760
2022-01-08 11:27:24,449 iteration 4143 : loss : 0.018322, loss_ce: 0.005874
2022-01-08 11:27:25,946 iteration 4144 : loss : 0.019272, loss_ce: 0.004531
2022-01-08 11:27:27,556 iteration 4145 : loss : 0.017331, loss_ce: 0.007087
2022-01-08 11:27:29,089 iteration 4146 : loss : 0.015732, loss_ce: 0.006239
2022-01-08 11:27:30,665 iteration 4147 : loss : 0.020821, loss_ce: 0.007194
2022-01-08 11:27:32,202 iteration 4148 : loss : 0.031505, loss_ce: 0.013326
 61%|████████████████▍          | 244/400 [1:59:22<1:11:51, 27.64s/it]2022-01-08 11:27:33,722 iteration 4149 : loss : 0.021847, loss_ce: 0.008990
2022-01-08 11:27:35,297 iteration 4150 : loss : 0.021909, loss_ce: 0.006533
2022-01-08 11:27:36,911 iteration 4151 : loss : 0.028463, loss_ce: 0.009990
2022-01-08 11:27:38,481 iteration 4152 : loss : 0.020597, loss_ce: 0.009233
2022-01-08 11:27:39,975 iteration 4153 : loss : 0.015456, loss_ce: 0.006175
2022-01-08 11:27:41,550 iteration 4154 : loss : 0.015307, loss_ce: 0.005160
2022-01-08 11:27:43,098 iteration 4155 : loss : 0.027626, loss_ce: 0.008449
2022-01-08 11:27:44,601 iteration 4156 : loss : 0.014483, loss_ce: 0.005082
2022-01-08 11:27:46,213 iteration 4157 : loss : 0.019595, loss_ce: 0.009044
2022-01-08 11:27:47,742 iteration 4158 : loss : 0.017877, loss_ce: 0.006587
2022-01-08 11:27:49,280 iteration 4159 : loss : 0.016927, loss_ce: 0.007011
2022-01-08 11:27:50,944 iteration 4160 : loss : 0.036586, loss_ce: 0.014034
2022-01-08 11:27:52,585 iteration 4161 : loss : 0.021605, loss_ce: 0.006434
2022-01-08 11:27:54,227 iteration 4162 : loss : 0.036839, loss_ce: 0.012548
2022-01-08 11:27:55,846 iteration 4163 : loss : 0.027068, loss_ce: 0.014501
2022-01-08 11:27:57,419 iteration 4164 : loss : 0.018268, loss_ce: 0.006868
2022-01-08 11:27:57,419 Training Data Eval:
2022-01-08 11:28:05,302   Average segmentation loss on training set: 0.0142
2022-01-08 11:28:05,303 Validation Data Eval:
2022-01-08 11:28:08,018   Average segmentation loss on validation set: 0.0665
2022-01-08 11:28:09,608 iteration 4165 : loss : 0.026442, loss_ce: 0.009986
 61%|████████████████▌          | 245/400 [1:59:59<1:18:57, 30.57s/it]2022-01-08 11:28:11,200 iteration 4166 : loss : 0.016122, loss_ce: 0.006390
2022-01-08 11:28:12,687 iteration 4167 : loss : 0.020043, loss_ce: 0.005785
2022-01-08 11:28:14,203 iteration 4168 : loss : 0.014712, loss_ce: 0.005867
2022-01-08 11:28:15,718 iteration 4169 : loss : 0.016693, loss_ce: 0.007967
2022-01-08 11:28:17,382 iteration 4170 : loss : 0.028420, loss_ce: 0.012209
2022-01-08 11:28:18,875 iteration 4171 : loss : 0.015766, loss_ce: 0.004947
2022-01-08 11:28:20,426 iteration 4172 : loss : 0.022489, loss_ce: 0.010459
2022-01-08 11:28:22,017 iteration 4173 : loss : 0.019994, loss_ce: 0.005990
2022-01-08 11:28:23,558 iteration 4174 : loss : 0.017710, loss_ce: 0.005719
2022-01-08 11:28:25,069 iteration 4175 : loss : 0.015890, loss_ce: 0.006286
2022-01-08 11:28:26,612 iteration 4176 : loss : 0.019364, loss_ce: 0.008525
2022-01-08 11:28:28,213 iteration 4177 : loss : 0.023893, loss_ce: 0.005966
2022-01-08 11:28:29,726 iteration 4178 : loss : 0.021586, loss_ce: 0.008004
2022-01-08 11:28:31,262 iteration 4179 : loss : 0.028499, loss_ce: 0.012170
2022-01-08 11:28:32,790 iteration 4180 : loss : 0.015803, loss_ce: 0.005089
2022-01-08 11:28:34,334 iteration 4181 : loss : 0.022018, loss_ce: 0.009126
2022-01-08 11:28:35,890 iteration 4182 : loss : 0.023052, loss_ce: 0.006009
 62%|████████████████▌          | 246/400 [2:00:26<1:15:09, 29.28s/it]2022-01-08 11:28:37,506 iteration 4183 : loss : 0.021400, loss_ce: 0.008023
2022-01-08 11:28:39,101 iteration 4184 : loss : 0.018530, loss_ce: 0.007821
2022-01-08 11:28:40,663 iteration 4185 : loss : 0.018785, loss_ce: 0.007772
2022-01-08 11:28:42,204 iteration 4186 : loss : 0.019597, loss_ce: 0.006015
2022-01-08 11:28:43,808 iteration 4187 : loss : 0.026539, loss_ce: 0.009137
2022-01-08 11:28:45,330 iteration 4188 : loss : 0.027770, loss_ce: 0.008461
2022-01-08 11:28:46,992 iteration 4189 : loss : 0.025909, loss_ce: 0.009699
2022-01-08 11:28:48,612 iteration 4190 : loss : 0.022834, loss_ce: 0.009927
2022-01-08 11:28:50,243 iteration 4191 : loss : 0.025846, loss_ce: 0.009985
2022-01-08 11:28:51,716 iteration 4192 : loss : 0.013155, loss_ce: 0.003939
2022-01-08 11:28:53,293 iteration 4193 : loss : 0.023325, loss_ce: 0.010732
2022-01-08 11:28:54,901 iteration 4194 : loss : 0.019106, loss_ce: 0.006481
2022-01-08 11:28:56,449 iteration 4195 : loss : 0.020122, loss_ce: 0.005603
2022-01-08 11:28:58,046 iteration 4196 : loss : 0.015714, loss_ce: 0.006796
2022-01-08 11:28:59,593 iteration 4197 : loss : 0.021803, loss_ce: 0.005548
2022-01-08 11:29:01,145 iteration 4198 : loss : 0.016982, loss_ce: 0.006617
2022-01-08 11:29:02,650 iteration 4199 : loss : 0.016485, loss_ce: 0.006347
 62%|████████████████▋          | 247/400 [2:00:52<1:12:44, 28.53s/it]2022-01-08 11:29:04,248 iteration 4200 : loss : 0.017076, loss_ce: 0.005666
2022-01-08 11:29:05,838 iteration 4201 : loss : 0.026610, loss_ce: 0.011932
2022-01-08 11:29:07,426 iteration 4202 : loss : 0.018024, loss_ce: 0.007474
2022-01-08 11:29:09,091 iteration 4203 : loss : 0.022446, loss_ce: 0.009598
2022-01-08 11:29:10,642 iteration 4204 : loss : 0.027058, loss_ce: 0.010824
2022-01-08 11:29:12,203 iteration 4205 : loss : 0.023836, loss_ce: 0.009333
2022-01-08 11:29:13,900 iteration 4206 : loss : 0.024650, loss_ce: 0.007843
2022-01-08 11:29:15,515 iteration 4207 : loss : 0.027587, loss_ce: 0.009626
2022-01-08 11:29:16,996 iteration 4208 : loss : 0.013509, loss_ce: 0.005055
2022-01-08 11:29:18,538 iteration 4209 : loss : 0.023842, loss_ce: 0.010470
2022-01-08 11:29:20,076 iteration 4210 : loss : 0.021397, loss_ce: 0.009084
2022-01-08 11:29:21,664 iteration 4211 : loss : 0.019957, loss_ce: 0.008001
2022-01-08 11:29:23,288 iteration 4212 : loss : 0.019334, loss_ce: 0.007303
2022-01-08 11:29:24,826 iteration 4213 : loss : 0.024338, loss_ce: 0.005771
2022-01-08 11:29:26,440 iteration 4214 : loss : 0.021607, loss_ce: 0.007360
2022-01-08 11:29:28,006 iteration 4215 : loss : 0.023454, loss_ce: 0.012637
2022-01-08 11:29:29,568 iteration 4216 : loss : 0.017805, loss_ce: 0.005482
 62%|████████████████▋          | 248/400 [2:01:19<1:11:02, 28.04s/it]2022-01-08 11:29:31,218 iteration 4217 : loss : 0.027330, loss_ce: 0.010822
2022-01-08 11:29:32,773 iteration 4218 : loss : 0.020464, loss_ce: 0.006220
2022-01-08 11:29:34,307 iteration 4219 : loss : 0.019706, loss_ce: 0.007115
2022-01-08 11:29:35,781 iteration 4220 : loss : 0.022024, loss_ce: 0.004926
2022-01-08 11:29:37,446 iteration 4221 : loss : 0.022453, loss_ce: 0.010309
2022-01-08 11:29:39,100 iteration 4222 : loss : 0.030730, loss_ce: 0.012420
2022-01-08 11:29:40,638 iteration 4223 : loss : 0.028288, loss_ce: 0.008775
2022-01-08 11:29:42,256 iteration 4224 : loss : 0.031652, loss_ce: 0.008509
2022-01-08 11:29:43,842 iteration 4225 : loss : 0.030117, loss_ce: 0.008138
2022-01-08 11:29:45,452 iteration 4226 : loss : 0.020495, loss_ce: 0.009170
2022-01-08 11:29:47,037 iteration 4227 : loss : 0.016766, loss_ce: 0.006723
2022-01-08 11:29:48,555 iteration 4228 : loss : 0.025744, loss_ce: 0.015067
2022-01-08 11:29:50,109 iteration 4229 : loss : 0.026863, loss_ce: 0.011831
2022-01-08 11:29:51,653 iteration 4230 : loss : 0.034985, loss_ce: 0.018065
2022-01-08 11:29:53,257 iteration 4231 : loss : 0.025297, loss_ce: 0.010210
2022-01-08 11:29:54,862 iteration 4232 : loss : 0.023996, loss_ce: 0.007188
2022-01-08 11:29:56,420 iteration 4233 : loss : 0.018459, loss_ce: 0.009378
 62%|████████████████▊          | 249/400 [2:01:46<1:09:40, 27.69s/it]2022-01-08 11:29:57,941 iteration 4234 : loss : 0.023276, loss_ce: 0.005578
2022-01-08 11:29:59,437 iteration 4235 : loss : 0.015627, loss_ce: 0.005647
2022-01-08 11:30:01,024 iteration 4236 : loss : 0.022626, loss_ce: 0.009436
2022-01-08 11:30:02,543 iteration 4237 : loss : 0.021555, loss_ce: 0.009040
2022-01-08 11:30:04,095 iteration 4238 : loss : 0.024543, loss_ce: 0.012128
2022-01-08 11:30:05,788 iteration 4239 : loss : 0.035300, loss_ce: 0.015744
2022-01-08 11:30:07,365 iteration 4240 : loss : 0.019680, loss_ce: 0.006600
2022-01-08 11:30:08,846 iteration 4241 : loss : 0.018654, loss_ce: 0.008355
2022-01-08 11:30:10,412 iteration 4242 : loss : 0.018067, loss_ce: 0.005295
2022-01-08 11:30:11,950 iteration 4243 : loss : 0.026304, loss_ce: 0.010857
2022-01-08 11:30:13,434 iteration 4244 : loss : 0.016486, loss_ce: 0.008305
2022-01-08 11:30:15,047 iteration 4245 : loss : 0.026848, loss_ce: 0.012979
2022-01-08 11:30:16,688 iteration 4246 : loss : 0.025025, loss_ce: 0.006989
2022-01-08 11:30:18,284 iteration 4247 : loss : 0.019955, loss_ce: 0.009353
2022-01-08 11:30:19,790 iteration 4248 : loss : 0.020434, loss_ce: 0.008100
2022-01-08 11:30:21,412 iteration 4249 : loss : 0.020849, loss_ce: 0.006555
2022-01-08 11:30:21,413 Training Data Eval:
2022-01-08 11:30:29,299   Average segmentation loss on training set: 0.0140
2022-01-08 11:30:29,299 Validation Data Eval:
2022-01-08 11:30:32,012   Average segmentation loss on validation set: 0.0791
2022-01-08 11:30:33,561 iteration 4250 : loss : 0.026879, loss_ce: 0.010998
 62%|████████████████▉          | 250/400 [2:02:23<1:16:18, 30.52s/it]2022-01-08 11:30:35,223 iteration 4251 : loss : 0.030330, loss_ce: 0.007763
2022-01-08 11:30:36,893 iteration 4252 : loss : 0.021959, loss_ce: 0.010196
2022-01-08 11:30:38,457 iteration 4253 : loss : 0.025480, loss_ce: 0.008084
2022-01-08 11:30:40,059 iteration 4254 : loss : 0.016314, loss_ce: 0.005984
2022-01-08 11:30:41,674 iteration 4255 : loss : 0.021058, loss_ce: 0.008407
2022-01-08 11:30:43,148 iteration 4256 : loss : 0.016469, loss_ce: 0.007511
2022-01-08 11:30:44,704 iteration 4257 : loss : 0.020937, loss_ce: 0.008360
2022-01-08 11:30:46,197 iteration 4258 : loss : 0.028120, loss_ce: 0.008944
2022-01-08 11:30:47,778 iteration 4259 : loss : 0.020817, loss_ce: 0.008160
2022-01-08 11:30:49,361 iteration 4260 : loss : 0.024608, loss_ce: 0.007722
2022-01-08 11:30:50,948 iteration 4261 : loss : 0.022461, loss_ce: 0.009382
2022-01-08 11:30:52,471 iteration 4262 : loss : 0.017322, loss_ce: 0.007430
2022-01-08 11:30:54,052 iteration 4263 : loss : 0.020846, loss_ce: 0.008874
2022-01-08 11:30:55,665 iteration 4264 : loss : 0.023990, loss_ce: 0.009642
2022-01-08 11:30:57,224 iteration 4265 : loss : 0.028451, loss_ce: 0.008324
2022-01-08 11:30:58,814 iteration 4266 : loss : 0.026045, loss_ce: 0.008248
2022-01-08 11:31:00,424 iteration 4267 : loss : 0.021678, loss_ce: 0.008209
 63%|████████████████▉          | 251/400 [2:02:50<1:13:04, 29.42s/it]2022-01-08 11:31:02,036 iteration 4268 : loss : 0.020216, loss_ce: 0.004836
2022-01-08 11:31:03,588 iteration 4269 : loss : 0.026033, loss_ce: 0.007819
2022-01-08 11:31:05,116 iteration 4270 : loss : 0.031196, loss_ce: 0.017415
2022-01-08 11:31:06,603 iteration 4271 : loss : 0.014964, loss_ce: 0.005828
2022-01-08 11:31:08,151 iteration 4272 : loss : 0.016039, loss_ce: 0.008017
2022-01-08 11:31:09,643 iteration 4273 : loss : 0.014432, loss_ce: 0.006014
2022-01-08 11:31:11,283 iteration 4274 : loss : 0.021120, loss_ce: 0.009330
2022-01-08 11:31:12,821 iteration 4275 : loss : 0.017475, loss_ce: 0.005927
2022-01-08 11:31:14,429 iteration 4276 : loss : 0.025232, loss_ce: 0.007253
2022-01-08 11:31:16,038 iteration 4277 : loss : 0.021295, loss_ce: 0.005914
2022-01-08 11:31:17,646 iteration 4278 : loss : 0.025192, loss_ce: 0.008311
2022-01-08 11:31:19,193 iteration 4279 : loss : 0.019302, loss_ce: 0.009703
2022-01-08 11:31:20,767 iteration 4280 : loss : 0.022942, loss_ce: 0.007916
2022-01-08 11:31:22,339 iteration 4281 : loss : 0.025447, loss_ce: 0.007185
2022-01-08 11:31:23,839 iteration 4282 : loss : 0.018032, loss_ce: 0.007135
2022-01-08 11:31:25,388 iteration 4283 : loss : 0.023219, loss_ce: 0.006985
2022-01-08 11:31:26,914 iteration 4284 : loss : 0.021933, loss_ce: 0.009582
 63%|█████████████████          | 252/400 [2:03:17<1:10:24, 28.54s/it]2022-01-08 11:31:28,454 iteration 4285 : loss : 0.022526, loss_ce: 0.007607
2022-01-08 11:31:30,058 iteration 4286 : loss : 0.021352, loss_ce: 0.008350
2022-01-08 11:31:31,574 iteration 4287 : loss : 0.021382, loss_ce: 0.010126
2022-01-08 11:31:33,091 iteration 4288 : loss : 0.017900, loss_ce: 0.008051
2022-01-08 11:31:34,657 iteration 4289 : loss : 0.019035, loss_ce: 0.007815
2022-01-08 11:31:36,205 iteration 4290 : loss : 0.021594, loss_ce: 0.005449
2022-01-08 11:31:37,787 iteration 4291 : loss : 0.023415, loss_ce: 0.009353
2022-01-08 11:31:39,368 iteration 4292 : loss : 0.021516, loss_ce: 0.011908
2022-01-08 11:31:40,935 iteration 4293 : loss : 0.021906, loss_ce: 0.006756
2022-01-08 11:31:42,471 iteration 4294 : loss : 0.018071, loss_ce: 0.004867
2022-01-08 11:31:44,141 iteration 4295 : loss : 0.026387, loss_ce: 0.010929
2022-01-08 11:31:45,718 iteration 4296 : loss : 0.027816, loss_ce: 0.011202
2022-01-08 11:31:47,312 iteration 4297 : loss : 0.020990, loss_ce: 0.007642
2022-01-08 11:31:48,851 iteration 4298 : loss : 0.014958, loss_ce: 0.007356
2022-01-08 11:31:50,468 iteration 4299 : loss : 0.019897, loss_ce: 0.009811
2022-01-08 11:31:52,119 iteration 4300 : loss : 0.020698, loss_ce: 0.008548
2022-01-08 11:31:53,750 iteration 4301 : loss : 0.025774, loss_ce: 0.015645
 63%|█████████████████          | 253/400 [2:03:44<1:08:40, 28.03s/it]2022-01-08 11:31:55,397 iteration 4302 : loss : 0.024303, loss_ce: 0.010181
2022-01-08 11:31:57,022 iteration 4303 : loss : 0.035299, loss_ce: 0.006976
2022-01-08 11:31:58,551 iteration 4304 : loss : 0.016583, loss_ce: 0.006379
2022-01-08 11:32:00,088 iteration 4305 : loss : 0.019291, loss_ce: 0.007583
2022-01-08 11:32:01,630 iteration 4306 : loss : 0.014938, loss_ce: 0.004564
2022-01-08 11:32:03,132 iteration 4307 : loss : 0.017356, loss_ce: 0.008701
2022-01-08 11:32:04,720 iteration 4308 : loss : 0.019703, loss_ce: 0.006710
2022-01-08 11:32:06,346 iteration 4309 : loss : 0.022370, loss_ce: 0.007267
2022-01-08 11:32:07,914 iteration 4310 : loss : 0.014325, loss_ce: 0.006155
2022-01-08 11:32:09,438 iteration 4311 : loss : 0.021644, loss_ce: 0.007566
2022-01-08 11:32:10,950 iteration 4312 : loss : 0.018185, loss_ce: 0.006789
2022-01-08 11:32:12,471 iteration 4313 : loss : 0.016388, loss_ce: 0.008453
2022-01-08 11:32:14,001 iteration 4314 : loss : 0.017462, loss_ce: 0.005986
2022-01-08 11:32:15,593 iteration 4315 : loss : 0.027507, loss_ce: 0.009460
2022-01-08 11:32:17,119 iteration 4316 : loss : 0.025008, loss_ce: 0.008204
2022-01-08 11:32:18,736 iteration 4317 : loss : 0.026825, loss_ce: 0.011438
2022-01-08 11:32:20,246 iteration 4318 : loss : 0.018527, loss_ce: 0.007136
 64%|█████████████████▏         | 254/400 [2:04:10<1:07:05, 27.57s/it]2022-01-08 11:32:21,895 iteration 4319 : loss : 0.023088, loss_ce: 0.007727
2022-01-08 11:32:23,495 iteration 4320 : loss : 0.027886, loss_ce: 0.011470
2022-01-08 11:32:24,962 iteration 4321 : loss : 0.020405, loss_ce: 0.005858
2022-01-08 11:32:26,521 iteration 4322 : loss : 0.016883, loss_ce: 0.005372
2022-01-08 11:32:28,029 iteration 4323 : loss : 0.016856, loss_ce: 0.008565
2022-01-08 11:32:29,697 iteration 4324 : loss : 0.026143, loss_ce: 0.010868
2022-01-08 11:32:31,263 iteration 4325 : loss : 0.021915, loss_ce: 0.008909
2022-01-08 11:32:32,801 iteration 4326 : loss : 0.016343, loss_ce: 0.007474
2022-01-08 11:32:34,392 iteration 4327 : loss : 0.047651, loss_ce: 0.010257
2022-01-08 11:32:35,916 iteration 4328 : loss : 0.021234, loss_ce: 0.007186
2022-01-08 11:32:37,535 iteration 4329 : loss : 0.033272, loss_ce: 0.009126
2022-01-08 11:32:39,133 iteration 4330 : loss : 0.019299, loss_ce: 0.008378
2022-01-08 11:32:40,781 iteration 4331 : loss : 0.050199, loss_ce: 0.013162
2022-01-08 11:32:42,394 iteration 4332 : loss : 0.020851, loss_ce: 0.006927
2022-01-08 11:32:44,035 iteration 4333 : loss : 0.037554, loss_ce: 0.018532
2022-01-08 11:32:45,617 iteration 4334 : loss : 0.023334, loss_ce: 0.011920
2022-01-08 11:32:45,617 Training Data Eval:
2022-01-08 11:32:53,495   Average segmentation loss on training set: 0.0130
2022-01-08 11:32:53,495 Validation Data Eval:
2022-01-08 11:32:56,205   Average segmentation loss on validation set: 0.0704
2022-01-08 11:32:57,804 iteration 4335 : loss : 0.030113, loss_ce: 0.007263
 64%|█████████████████▏         | 255/400 [2:04:48<1:13:52, 30.57s/it]2022-01-08 11:32:59,299 iteration 4336 : loss : 0.014791, loss_ce: 0.007110
2022-01-08 11:33:00,937 iteration 4337 : loss : 0.023851, loss_ce: 0.010297
2022-01-08 11:33:02,498 iteration 4338 : loss : 0.020755, loss_ce: 0.007060
2022-01-08 11:33:04,015 iteration 4339 : loss : 0.016839, loss_ce: 0.005805
2022-01-08 11:33:05,616 iteration 4340 : loss : 0.026764, loss_ce: 0.010706
2022-01-08 11:33:07,133 iteration 4341 : loss : 0.016434, loss_ce: 0.005169
2022-01-08 11:33:08,744 iteration 4342 : loss : 0.031150, loss_ce: 0.008486
2022-01-08 11:33:10,353 iteration 4343 : loss : 0.018932, loss_ce: 0.005439
2022-01-08 11:33:11,954 iteration 4344 : loss : 0.026381, loss_ce: 0.010942
2022-01-08 11:33:13,622 iteration 4345 : loss : 0.024845, loss_ce: 0.010139
2022-01-08 11:33:15,126 iteration 4346 : loss : 0.023466, loss_ce: 0.006890
2022-01-08 11:33:16,701 iteration 4347 : loss : 0.018267, loss_ce: 0.008302
2022-01-08 11:33:18,223 iteration 4348 : loss : 0.016817, loss_ce: 0.008305
2022-01-08 11:33:19,699 iteration 4349 : loss : 0.015527, loss_ce: 0.004114
2022-01-08 11:33:21,170 iteration 4350 : loss : 0.026955, loss_ce: 0.008637
2022-01-08 11:33:22,747 iteration 4351 : loss : 0.018339, loss_ce: 0.006012
2022-01-08 11:33:24,313 iteration 4352 : loss : 0.021333, loss_ce: 0.008967
 64%|█████████████████▎         | 256/400 [2:05:14<1:10:26, 29.35s/it]2022-01-08 11:33:25,871 iteration 4353 : loss : 0.020127, loss_ce: 0.006377
2022-01-08 11:33:27,521 iteration 4354 : loss : 0.025301, loss_ce: 0.011551
2022-01-08 11:33:29,134 iteration 4355 : loss : 0.018999, loss_ce: 0.008615
2022-01-08 11:33:30,784 iteration 4356 : loss : 0.020117, loss_ce: 0.005301
2022-01-08 11:33:32,390 iteration 4357 : loss : 0.016782, loss_ce: 0.007718
2022-01-08 11:33:34,015 iteration 4358 : loss : 0.029723, loss_ce: 0.011317
2022-01-08 11:33:35,607 iteration 4359 : loss : 0.019347, loss_ce: 0.006322
2022-01-08 11:33:37,161 iteration 4360 : loss : 0.021479, loss_ce: 0.009310
2022-01-08 11:33:38,779 iteration 4361 : loss : 0.027626, loss_ce: 0.014313
2022-01-08 11:33:40,344 iteration 4362 : loss : 0.015822, loss_ce: 0.006924
2022-01-08 11:33:41,910 iteration 4363 : loss : 0.016726, loss_ce: 0.007425
2022-01-08 11:33:43,503 iteration 4364 : loss : 0.019556, loss_ce: 0.007299
2022-01-08 11:33:45,164 iteration 4365 : loss : 0.034823, loss_ce: 0.016478
2022-01-08 11:33:46,681 iteration 4366 : loss : 0.015925, loss_ce: 0.004299
2022-01-08 11:33:48,143 iteration 4367 : loss : 0.017411, loss_ce: 0.007101
2022-01-08 11:33:49,635 iteration 4368 : loss : 0.017138, loss_ce: 0.005318
2022-01-08 11:33:51,209 iteration 4369 : loss : 0.021482, loss_ce: 0.008143
 64%|█████████████████▎         | 257/400 [2:05:41<1:08:11, 28.61s/it]2022-01-08 11:33:52,818 iteration 4370 : loss : 0.018558, loss_ce: 0.006005
2022-01-08 11:33:54,404 iteration 4371 : loss : 0.021854, loss_ce: 0.012366
2022-01-08 11:33:56,000 iteration 4372 : loss : 0.018931, loss_ce: 0.008268
2022-01-08 11:33:57,590 iteration 4373 : loss : 0.019248, loss_ce: 0.009920
2022-01-08 11:33:59,190 iteration 4374 : loss : 0.018713, loss_ce: 0.006340
2022-01-08 11:34:00,787 iteration 4375 : loss : 0.025737, loss_ce: 0.011033
2022-01-08 11:34:02,327 iteration 4376 : loss : 0.024409, loss_ce: 0.008290
2022-01-08 11:34:03,868 iteration 4377 : loss : 0.017287, loss_ce: 0.004924
2022-01-08 11:34:05,366 iteration 4378 : loss : 0.016011, loss_ce: 0.005951
2022-01-08 11:34:06,872 iteration 4379 : loss : 0.014345, loss_ce: 0.004807
2022-01-08 11:34:08,351 iteration 4380 : loss : 0.017831, loss_ce: 0.006137
2022-01-08 11:34:09,935 iteration 4381 : loss : 0.027743, loss_ce: 0.010319
2022-01-08 11:34:11,497 iteration 4382 : loss : 0.035135, loss_ce: 0.005722
2022-01-08 11:34:13,029 iteration 4383 : loss : 0.017210, loss_ce: 0.005250
2022-01-08 11:34:14,649 iteration 4384 : loss : 0.017514, loss_ce: 0.007575
2022-01-08 11:34:16,265 iteration 4385 : loss : 0.024957, loss_ce: 0.007889
2022-01-08 11:34:17,791 iteration 4386 : loss : 0.018587, loss_ce: 0.008588
 64%|█████████████████▍         | 258/400 [2:06:08<1:06:16, 28.01s/it]2022-01-08 11:34:19,331 iteration 4387 : loss : 0.018692, loss_ce: 0.005657
2022-01-08 11:34:20,895 iteration 4388 : loss : 0.022856, loss_ce: 0.010704
2022-01-08 11:34:22,461 iteration 4389 : loss : 0.027914, loss_ce: 0.010410
2022-01-08 11:34:23,966 iteration 4390 : loss : 0.020008, loss_ce: 0.006492
2022-01-08 11:34:25,544 iteration 4391 : loss : 0.024333, loss_ce: 0.007710
2022-01-08 11:34:27,099 iteration 4392 : loss : 0.016392, loss_ce: 0.004948
2022-01-08 11:34:28,729 iteration 4393 : loss : 0.026351, loss_ce: 0.012201
2022-01-08 11:34:30,330 iteration 4394 : loss : 0.025685, loss_ce: 0.009854
2022-01-08 11:34:31,996 iteration 4395 : loss : 0.038847, loss_ce: 0.013621
2022-01-08 11:34:33,531 iteration 4396 : loss : 0.018789, loss_ce: 0.006832
2022-01-08 11:34:35,202 iteration 4397 : loss : 0.021266, loss_ce: 0.010941
2022-01-08 11:34:36,720 iteration 4398 : loss : 0.017452, loss_ce: 0.007350
2022-01-08 11:34:38,333 iteration 4399 : loss : 0.045863, loss_ce: 0.028308
2022-01-08 11:34:39,915 iteration 4400 : loss : 0.024082, loss_ce: 0.007765
2022-01-08 11:34:41,618 iteration 4401 : loss : 0.028634, loss_ce: 0.011439
2022-01-08 11:34:43,207 iteration 4402 : loss : 0.029740, loss_ce: 0.010255
2022-01-08 11:34:44,794 iteration 4403 : loss : 0.016760, loss_ce: 0.005933
 65%|█████████████████▍         | 259/400 [2:06:35<1:05:06, 27.70s/it]2022-01-08 11:34:46,370 iteration 4404 : loss : 0.020917, loss_ce: 0.009626
2022-01-08 11:34:47,976 iteration 4405 : loss : 0.021945, loss_ce: 0.010226
2022-01-08 11:34:49,568 iteration 4406 : loss : 0.023457, loss_ce: 0.010737
2022-01-08 11:34:51,199 iteration 4407 : loss : 0.026514, loss_ce: 0.010297
2022-01-08 11:34:52,747 iteration 4408 : loss : 0.018265, loss_ce: 0.007707
2022-01-08 11:34:54,344 iteration 4409 : loss : 0.025872, loss_ce: 0.011890
2022-01-08 11:34:56,058 iteration 4410 : loss : 0.040889, loss_ce: 0.016936
2022-01-08 11:34:57,684 iteration 4411 : loss : 0.020925, loss_ce: 0.008918
2022-01-08 11:34:59,254 iteration 4412 : loss : 0.021907, loss_ce: 0.008483
2022-01-08 11:35:00,848 iteration 4413 : loss : 0.016946, loss_ce: 0.005865
2022-01-08 11:35:02,357 iteration 4414 : loss : 0.016289, loss_ce: 0.005376
2022-01-08 11:35:03,956 iteration 4415 : loss : 0.019571, loss_ce: 0.006149
2022-01-08 11:35:05,589 iteration 4416 : loss : 0.029096, loss_ce: 0.010693
2022-01-08 11:35:07,159 iteration 4417 : loss : 0.023253, loss_ce: 0.008454
2022-01-08 11:35:08,743 iteration 4418 : loss : 0.019890, loss_ce: 0.008821
2022-01-08 11:35:10,364 iteration 4419 : loss : 0.022613, loss_ce: 0.007607
2022-01-08 11:35:10,365 Training Data Eval:
2022-01-08 11:35:18,249   Average segmentation loss on training set: 0.0131
2022-01-08 11:35:18,250 Validation Data Eval:
2022-01-08 11:35:20,962   Average segmentation loss on validation set: 0.0705
2022-01-08 11:35:22,469 iteration 4420 : loss : 0.014920, loss_ce: 0.007104
 65%|█████████████████▌         | 260/400 [2:07:12<1:11:37, 30.70s/it]2022-01-08 11:35:24,040 iteration 4421 : loss : 0.017682, loss_ce: 0.006051
2022-01-08 11:35:25,622 iteration 4422 : loss : 0.020778, loss_ce: 0.008877
2022-01-08 11:35:27,247 iteration 4423 : loss : 0.021076, loss_ce: 0.009209
2022-01-08 11:35:28,755 iteration 4424 : loss : 0.016300, loss_ce: 0.005246
2022-01-08 11:35:30,257 iteration 4425 : loss : 0.018418, loss_ce: 0.006424
2022-01-08 11:35:31,877 iteration 4426 : loss : 0.016945, loss_ce: 0.007221
2022-01-08 11:35:33,386 iteration 4427 : loss : 0.016077, loss_ce: 0.008016
2022-01-08 11:35:34,850 iteration 4428 : loss : 0.016270, loss_ce: 0.009240
2022-01-08 11:35:36,338 iteration 4429 : loss : 0.018215, loss_ce: 0.006081
2022-01-08 11:35:37,829 iteration 4430 : loss : 0.016036, loss_ce: 0.007580
2022-01-08 11:35:39,404 iteration 4431 : loss : 0.026115, loss_ce: 0.011226
2022-01-08 11:35:41,009 iteration 4432 : loss : 0.018439, loss_ce: 0.006932
2022-01-08 11:35:42,549 iteration 4433 : loss : 0.025396, loss_ce: 0.006523
2022-01-08 11:35:44,047 iteration 4434 : loss : 0.019663, loss_ce: 0.007841
2022-01-08 11:35:45,606 iteration 4435 : loss : 0.019534, loss_ce: 0.004594
2022-01-08 11:35:47,159 iteration 4436 : loss : 0.017074, loss_ce: 0.007080
2022-01-08 11:35:48,777 iteration 4437 : loss : 0.019064, loss_ce: 0.006750
 65%|█████████████████▌         | 261/400 [2:07:39<1:08:03, 29.38s/it]2022-01-08 11:35:50,441 iteration 4438 : loss : 0.040580, loss_ce: 0.009866
2022-01-08 11:35:52,036 iteration 4439 : loss : 0.017581, loss_ce: 0.006145
2022-01-08 11:35:53,612 iteration 4440 : loss : 0.021446, loss_ce: 0.007897
2022-01-08 11:35:55,291 iteration 4441 : loss : 0.026386, loss_ce: 0.010140
2022-01-08 11:35:56,919 iteration 4442 : loss : 0.019544, loss_ce: 0.009016
2022-01-08 11:35:58,508 iteration 4443 : loss : 0.028333, loss_ce: 0.010359
2022-01-08 11:36:00,006 iteration 4444 : loss : 0.020911, loss_ce: 0.006568
2022-01-08 11:36:01,553 iteration 4445 : loss : 0.023957, loss_ce: 0.010178
2022-01-08 11:36:03,152 iteration 4446 : loss : 0.026139, loss_ce: 0.010964
2022-01-08 11:36:04,692 iteration 4447 : loss : 0.018032, loss_ce: 0.006472
2022-01-08 11:36:06,281 iteration 4448 : loss : 0.022315, loss_ce: 0.008964
2022-01-08 11:36:07,853 iteration 4449 : loss : 0.019668, loss_ce: 0.009328
2022-01-08 11:36:09,484 iteration 4450 : loss : 0.025314, loss_ce: 0.008342
2022-01-08 11:36:11,018 iteration 4451 : loss : 0.024977, loss_ce: 0.009176
2022-01-08 11:36:12,603 iteration 4452 : loss : 0.017960, loss_ce: 0.007778
2022-01-08 11:36:14,145 iteration 4453 : loss : 0.019108, loss_ce: 0.006180
2022-01-08 11:36:15,681 iteration 4454 : loss : 0.019727, loss_ce: 0.008462
 66%|█████████████████▋         | 262/400 [2:08:05<1:05:51, 28.64s/it]2022-01-08 11:36:17,304 iteration 4455 : loss : 0.016457, loss_ce: 0.005220
2022-01-08 11:36:18,786 iteration 4456 : loss : 0.014922, loss_ce: 0.005710
2022-01-08 11:36:20,369 iteration 4457 : loss : 0.023217, loss_ce: 0.012173
2022-01-08 11:36:22,034 iteration 4458 : loss : 0.030752, loss_ce: 0.013732
2022-01-08 11:36:23,765 iteration 4459 : loss : 0.021801, loss_ce: 0.009641
2022-01-08 11:36:25,301 iteration 4460 : loss : 0.025493, loss_ce: 0.009072
2022-01-08 11:36:26,877 iteration 4461 : loss : 0.021254, loss_ce: 0.007008
2022-01-08 11:36:28,488 iteration 4462 : loss : 0.033030, loss_ce: 0.011583
2022-01-08 11:36:30,048 iteration 4463 : loss : 0.029127, loss_ce: 0.012451
2022-01-08 11:36:31,593 iteration 4464 : loss : 0.017301, loss_ce: 0.005954
2022-01-08 11:36:33,082 iteration 4465 : loss : 0.020877, loss_ce: 0.005050
2022-01-08 11:36:34,595 iteration 4466 : loss : 0.018990, loss_ce: 0.005721
2022-01-08 11:36:36,158 iteration 4467 : loss : 0.037767, loss_ce: 0.016224
2022-01-08 11:36:37,764 iteration 4468 : loss : 0.022309, loss_ce: 0.009286
2022-01-08 11:36:39,409 iteration 4469 : loss : 0.022330, loss_ce: 0.010855
2022-01-08 11:36:41,035 iteration 4470 : loss : 0.017137, loss_ce: 0.007214
2022-01-08 11:36:42,700 iteration 4471 : loss : 0.030129, loss_ce: 0.008092
 66%|█████████████████▊         | 263/400 [2:08:32<1:04:16, 28.15s/it]2022-01-08 11:36:44,284 iteration 4472 : loss : 0.015782, loss_ce: 0.007287
2022-01-08 11:36:45,818 iteration 4473 : loss : 0.034983, loss_ce: 0.012285
2022-01-08 11:36:47,403 iteration 4474 : loss : 0.017507, loss_ce: 0.008584
2022-01-08 11:36:48,974 iteration 4475 : loss : 0.019691, loss_ce: 0.006300
2022-01-08 11:36:50,511 iteration 4476 : loss : 0.018266, loss_ce: 0.005384
2022-01-08 11:36:52,063 iteration 4477 : loss : 0.017112, loss_ce: 0.007663
2022-01-08 11:36:53,745 iteration 4478 : loss : 0.024831, loss_ce: 0.011844
2022-01-08 11:36:55,312 iteration 4479 : loss : 0.037280, loss_ce: 0.011301
2022-01-08 11:36:56,899 iteration 4480 : loss : 0.018080, loss_ce: 0.006233
2022-01-08 11:36:58,425 iteration 4481 : loss : 0.019966, loss_ce: 0.007436
2022-01-08 11:36:59,942 iteration 4482 : loss : 0.021589, loss_ce: 0.006111
2022-01-08 11:37:01,473 iteration 4483 : loss : 0.020304, loss_ce: 0.006647
2022-01-08 11:37:03,040 iteration 4484 : loss : 0.015611, loss_ce: 0.005496
2022-01-08 11:37:04,538 iteration 4485 : loss : 0.017042, loss_ce: 0.006684
2022-01-08 11:37:06,157 iteration 4486 : loss : 0.016219, loss_ce: 0.005963
2022-01-08 11:37:07,753 iteration 4487 : loss : 0.022074, loss_ce: 0.010057
2022-01-08 11:37:09,267 iteration 4488 : loss : 0.018164, loss_ce: 0.007088
 66%|█████████████████▊         | 264/400 [2:08:59<1:02:43, 27.67s/it]2022-01-08 11:37:10,901 iteration 4489 : loss : 0.019360, loss_ce: 0.008366
2022-01-08 11:37:12,457 iteration 4490 : loss : 0.023051, loss_ce: 0.009858
2022-01-08 11:37:14,119 iteration 4491 : loss : 0.024407, loss_ce: 0.008184
2022-01-08 11:37:15,656 iteration 4492 : loss : 0.017455, loss_ce: 0.004324
2022-01-08 11:37:17,304 iteration 4493 : loss : 0.021808, loss_ce: 0.013493
2022-01-08 11:37:18,836 iteration 4494 : loss : 0.018747, loss_ce: 0.006577
2022-01-08 11:37:20,415 iteration 4495 : loss : 0.022526, loss_ce: 0.005699
2022-01-08 11:37:21,995 iteration 4496 : loss : 0.016498, loss_ce: 0.005225
2022-01-08 11:37:23,457 iteration 4497 : loss : 0.017294, loss_ce: 0.008126
2022-01-08 11:37:25,015 iteration 4498 : loss : 0.019693, loss_ce: 0.006141
2022-01-08 11:37:26,545 iteration 4499 : loss : 0.013413, loss_ce: 0.005210
2022-01-08 11:37:28,105 iteration 4500 : loss : 0.026217, loss_ce: 0.009264
2022-01-08 11:37:29,642 iteration 4501 : loss : 0.015402, loss_ce: 0.006432
2022-01-08 11:37:31,186 iteration 4502 : loss : 0.016747, loss_ce: 0.005265
2022-01-08 11:37:32,830 iteration 4503 : loss : 0.017356, loss_ce: 0.006030
2022-01-08 11:37:34,306 iteration 4504 : loss : 0.020621, loss_ce: 0.007678
2022-01-08 11:37:34,306 Training Data Eval:
2022-01-08 11:37:42,192   Average segmentation loss on training set: 0.0121
2022-01-08 11:37:42,192 Validation Data Eval:
2022-01-08 11:37:44,913   Average segmentation loss on validation set: 0.0604
2022-01-08 11:37:50,699 Found new lowest validation loss at iteration 4504! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 11:37:52,148 iteration 4505 : loss : 0.019462, loss_ce: 0.007998
 66%|█████████████████▉         | 265/400 [2:09:42<1:12:32, 32.24s/it]2022-01-08 11:37:53,665 iteration 4506 : loss : 0.020690, loss_ce: 0.008765
2022-01-08 11:37:55,302 iteration 4507 : loss : 0.038008, loss_ce: 0.014571
2022-01-08 11:37:56,879 iteration 4508 : loss : 0.019200, loss_ce: 0.006681
2022-01-08 11:37:58,383 iteration 4509 : loss : 0.014516, loss_ce: 0.004755
2022-01-08 11:37:59,954 iteration 4510 : loss : 0.015382, loss_ce: 0.004609
2022-01-08 11:38:01,622 iteration 4511 : loss : 0.019754, loss_ce: 0.008436
2022-01-08 11:38:03,106 iteration 4512 : loss : 0.016873, loss_ce: 0.005840
2022-01-08 11:38:04,652 iteration 4513 : loss : 0.018236, loss_ce: 0.007240
2022-01-08 11:38:06,188 iteration 4514 : loss : 0.020742, loss_ce: 0.008004
2022-01-08 11:38:07,748 iteration 4515 : loss : 0.016206, loss_ce: 0.006722
2022-01-08 11:38:09,328 iteration 4516 : loss : 0.018772, loss_ce: 0.008475
2022-01-08 11:38:10,933 iteration 4517 : loss : 0.018909, loss_ce: 0.007294
2022-01-08 11:38:12,483 iteration 4518 : loss : 0.020413, loss_ce: 0.009242
2022-01-08 11:38:13,981 iteration 4519 : loss : 0.024310, loss_ce: 0.007890
2022-01-08 11:38:15,556 iteration 4520 : loss : 0.016926, loss_ce: 0.004982
2022-01-08 11:38:17,145 iteration 4521 : loss : 0.016004, loss_ce: 0.006974
2022-01-08 11:38:18,780 iteration 4522 : loss : 0.023715, loss_ce: 0.008133
 66%|█████████████████▉         | 266/400 [2:10:09<1:08:14, 30.56s/it]2022-01-08 11:38:20,462 iteration 4523 : loss : 0.022529, loss_ce: 0.011062
2022-01-08 11:38:22,047 iteration 4524 : loss : 0.018372, loss_ce: 0.007306
2022-01-08 11:38:23,647 iteration 4525 : loss : 0.019747, loss_ce: 0.008797
2022-01-08 11:38:25,206 iteration 4526 : loss : 0.018650, loss_ce: 0.005109
2022-01-08 11:38:26,698 iteration 4527 : loss : 0.018685, loss_ce: 0.006747
2022-01-08 11:38:28,228 iteration 4528 : loss : 0.017811, loss_ce: 0.006098
2022-01-08 11:38:29,813 iteration 4529 : loss : 0.016814, loss_ce: 0.008105
2022-01-08 11:38:31,346 iteration 4530 : loss : 0.020098, loss_ce: 0.004917
2022-01-08 11:38:32,920 iteration 4531 : loss : 0.018673, loss_ce: 0.006668
2022-01-08 11:38:34,514 iteration 4532 : loss : 0.015659, loss_ce: 0.004981
2022-01-08 11:38:36,051 iteration 4533 : loss : 0.016871, loss_ce: 0.007357
2022-01-08 11:38:37,597 iteration 4534 : loss : 0.027704, loss_ce: 0.007507
2022-01-08 11:38:39,189 iteration 4535 : loss : 0.026062, loss_ce: 0.009585
2022-01-08 11:38:40,770 iteration 4536 : loss : 0.017226, loss_ce: 0.006907
2022-01-08 11:38:42,337 iteration 4537 : loss : 0.020515, loss_ce: 0.008689
2022-01-08 11:38:43,895 iteration 4538 : loss : 0.018241, loss_ce: 0.006598
2022-01-08 11:38:45,451 iteration 4539 : loss : 0.019154, loss_ce: 0.007476
 67%|██████████████████         | 267/400 [2:10:35<1:05:08, 29.39s/it]2022-01-08 11:38:47,024 iteration 4540 : loss : 0.013332, loss_ce: 0.004685
2022-01-08 11:38:48,617 iteration 4541 : loss : 0.025184, loss_ce: 0.012537
2022-01-08 11:38:50,133 iteration 4542 : loss : 0.025323, loss_ce: 0.008762
2022-01-08 11:38:51,672 iteration 4543 : loss : 0.019832, loss_ce: 0.006384
2022-01-08 11:38:53,192 iteration 4544 : loss : 0.017820, loss_ce: 0.005839
2022-01-08 11:38:54,820 iteration 4545 : loss : 0.024318, loss_ce: 0.011765
2022-01-08 11:38:56,498 iteration 4546 : loss : 0.026804, loss_ce: 0.011603
2022-01-08 11:38:58,157 iteration 4547 : loss : 0.021800, loss_ce: 0.009194
2022-01-08 11:38:59,663 iteration 4548 : loss : 0.016703, loss_ce: 0.005953
2022-01-08 11:39:01,176 iteration 4549 : loss : 0.017685, loss_ce: 0.005227
2022-01-08 11:39:02,748 iteration 4550 : loss : 0.025782, loss_ce: 0.010995
2022-01-08 11:39:04,337 iteration 4551 : loss : 0.014981, loss_ce: 0.006101
2022-01-08 11:39:05,988 iteration 4552 : loss : 0.033093, loss_ce: 0.011703
2022-01-08 11:39:07,576 iteration 4553 : loss : 0.019014, loss_ce: 0.006456
2022-01-08 11:39:09,098 iteration 4554 : loss : 0.019741, loss_ce: 0.008240
2022-01-08 11:39:10,649 iteration 4555 : loss : 0.015377, loss_ce: 0.006362
2022-01-08 11:39:12,198 iteration 4556 : loss : 0.030834, loss_ce: 0.009889
 67%|██████████████████         | 268/400 [2:11:02<1:02:54, 28.60s/it]2022-01-08 11:39:13,767 iteration 4557 : loss : 0.019884, loss_ce: 0.009421
2022-01-08 11:39:15,365 iteration 4558 : loss : 0.024172, loss_ce: 0.008839
2022-01-08 11:39:17,000 iteration 4559 : loss : 0.021804, loss_ce: 0.008365
2022-01-08 11:39:18,604 iteration 4560 : loss : 0.016057, loss_ce: 0.008339
2022-01-08 11:39:20,135 iteration 4561 : loss : 0.013956, loss_ce: 0.004562
2022-01-08 11:39:21,713 iteration 4562 : loss : 0.022918, loss_ce: 0.009368
2022-01-08 11:39:23,262 iteration 4563 : loss : 0.016900, loss_ce: 0.005854
2022-01-08 11:39:24,801 iteration 4564 : loss : 0.019229, loss_ce: 0.007591
2022-01-08 11:39:26,355 iteration 4565 : loss : 0.018602, loss_ce: 0.007976
2022-01-08 11:39:27,931 iteration 4566 : loss : 0.017063, loss_ce: 0.006326
2022-01-08 11:39:29,492 iteration 4567 : loss : 0.021396, loss_ce: 0.007237
2022-01-08 11:39:30,977 iteration 4568 : loss : 0.016317, loss_ce: 0.004955
2022-01-08 11:39:32,482 iteration 4569 : loss : 0.027493, loss_ce: 0.011798
2022-01-08 11:39:34,004 iteration 4570 : loss : 0.015533, loss_ce: 0.005724
2022-01-08 11:39:35,607 iteration 4571 : loss : 0.019632, loss_ce: 0.008890
2022-01-08 11:39:37,210 iteration 4572 : loss : 0.024383, loss_ce: 0.008658
2022-01-08 11:39:38,841 iteration 4573 : loss : 0.019424, loss_ce: 0.006203
 67%|██████████████████▏        | 269/400 [2:11:29<1:01:09, 28.01s/it]2022-01-08 11:39:40,390 iteration 4574 : loss : 0.012022, loss_ce: 0.003923
2022-01-08 11:39:41,990 iteration 4575 : loss : 0.018910, loss_ce: 0.005072
2022-01-08 11:39:43,487 iteration 4576 : loss : 0.020249, loss_ce: 0.008443
2022-01-08 11:39:44,997 iteration 4577 : loss : 0.015193, loss_ce: 0.007060
2022-01-08 11:39:46,552 iteration 4578 : loss : 0.016219, loss_ce: 0.006223
2022-01-08 11:39:48,070 iteration 4579 : loss : 0.021497, loss_ce: 0.011427
2022-01-08 11:39:49,610 iteration 4580 : loss : 0.023228, loss_ce: 0.008720
2022-01-08 11:39:51,170 iteration 4581 : loss : 0.019296, loss_ce: 0.007998
2022-01-08 11:39:52,728 iteration 4582 : loss : 0.023278, loss_ce: 0.009809
2022-01-08 11:39:54,282 iteration 4583 : loss : 0.017079, loss_ce: 0.006381
2022-01-08 11:39:55,828 iteration 4584 : loss : 0.014720, loss_ce: 0.005290
2022-01-08 11:39:57,417 iteration 4585 : loss : 0.031302, loss_ce: 0.010412
2022-01-08 11:39:58,929 iteration 4586 : loss : 0.015226, loss_ce: 0.006476
2022-01-08 11:40:00,457 iteration 4587 : loss : 0.015885, loss_ce: 0.004752
2022-01-08 11:40:01,986 iteration 4588 : loss : 0.019930, loss_ce: 0.007268
2022-01-08 11:40:03,558 iteration 4589 : loss : 0.022244, loss_ce: 0.006824
2022-01-08 11:40:03,559 Training Data Eval:
2022-01-08 11:40:11,446   Average segmentation loss on training set: 0.0117
2022-01-08 11:40:11,446 Validation Data Eval:
2022-01-08 11:40:14,167   Average segmentation loss on validation set: 0.0653
2022-01-08 11:40:15,705 iteration 4590 : loss : 0.017500, loss_ce: 0.007524
 68%|██████████████████▏        | 270/400 [2:12:05<1:06:26, 30.67s/it]2022-01-08 11:40:17,307 iteration 4591 : loss : 0.034452, loss_ce: 0.007577
2022-01-08 11:40:18,918 iteration 4592 : loss : 0.019180, loss_ce: 0.006727
2022-01-08 11:40:20,513 iteration 4593 : loss : 0.018798, loss_ce: 0.008693
2022-01-08 11:40:22,144 iteration 4594 : loss : 0.040858, loss_ce: 0.011793
2022-01-08 11:40:23,699 iteration 4595 : loss : 0.020570, loss_ce: 0.006336
2022-01-08 11:40:25,233 iteration 4596 : loss : 0.019280, loss_ce: 0.004839
2022-01-08 11:40:26,810 iteration 4597 : loss : 0.015411, loss_ce: 0.007278
2022-01-08 11:40:28,438 iteration 4598 : loss : 0.025969, loss_ce: 0.009485
2022-01-08 11:40:30,045 iteration 4599 : loss : 0.016942, loss_ce: 0.007665
2022-01-08 11:40:31,591 iteration 4600 : loss : 0.018503, loss_ce: 0.005978
2022-01-08 11:40:33,184 iteration 4601 : loss : 0.023253, loss_ce: 0.009643
2022-01-08 11:40:34,861 iteration 4602 : loss : 0.032107, loss_ce: 0.008398
2022-01-08 11:40:36,453 iteration 4603 : loss : 0.021257, loss_ce: 0.007579
2022-01-08 11:40:38,070 iteration 4604 : loss : 0.016408, loss_ce: 0.006495
2022-01-08 11:40:39,703 iteration 4605 : loss : 0.048257, loss_ce: 0.009133
2022-01-08 11:40:41,322 iteration 4606 : loss : 0.020556, loss_ce: 0.007793
2022-01-08 11:40:42,937 iteration 4607 : loss : 0.024137, loss_ce: 0.011464
 68%|██████████████████▎        | 271/400 [2:12:33<1:03:43, 29.64s/it]2022-01-08 11:40:44,498 iteration 4608 : loss : 0.014317, loss_ce: 0.005366
2022-01-08 11:40:46,071 iteration 4609 : loss : 0.027178, loss_ce: 0.011366
2022-01-08 11:40:47,582 iteration 4610 : loss : 0.021330, loss_ce: 0.006981
2022-01-08 11:40:49,089 iteration 4611 : loss : 0.017123, loss_ce: 0.006859
2022-01-08 11:40:50,575 iteration 4612 : loss : 0.019713, loss_ce: 0.005973
2022-01-08 11:40:52,139 iteration 4613 : loss : 0.017520, loss_ce: 0.006257
2022-01-08 11:40:53,706 iteration 4614 : loss : 0.017635, loss_ce: 0.007977
2022-01-08 11:40:55,312 iteration 4615 : loss : 0.023955, loss_ce: 0.012513
2022-01-08 11:40:56,991 iteration 4616 : loss : 0.022627, loss_ce: 0.009715
2022-01-08 11:40:58,574 iteration 4617 : loss : 0.022159, loss_ce: 0.008929
2022-01-08 11:41:00,089 iteration 4618 : loss : 0.016323, loss_ce: 0.005187
2022-01-08 11:41:01,615 iteration 4619 : loss : 0.013161, loss_ce: 0.005877
2022-01-08 11:41:03,234 iteration 4620 : loss : 0.017826, loss_ce: 0.005283
2022-01-08 11:41:04,788 iteration 4621 : loss : 0.021301, loss_ce: 0.010857
2022-01-08 11:41:06,358 iteration 4622 : loss : 0.024053, loss_ce: 0.008274
2022-01-08 11:41:07,917 iteration 4623 : loss : 0.022581, loss_ce: 0.008126
2022-01-08 11:41:09,494 iteration 4624 : loss : 0.019227, loss_ce: 0.009291
 68%|██████████████████▎        | 272/400 [2:12:59<1:01:15, 28.71s/it]2022-01-08 11:41:11,139 iteration 4625 : loss : 0.017566, loss_ce: 0.007018
2022-01-08 11:41:12,772 iteration 4626 : loss : 0.019498, loss_ce: 0.008484
2022-01-08 11:41:14,443 iteration 4627 : loss : 0.022098, loss_ce: 0.009352
2022-01-08 11:41:15,985 iteration 4628 : loss : 0.018534, loss_ce: 0.006903
2022-01-08 11:41:17,486 iteration 4629 : loss : 0.018905, loss_ce: 0.006269
2022-01-08 11:41:19,042 iteration 4630 : loss : 0.014348, loss_ce: 0.005008
2022-01-08 11:41:20,613 iteration 4631 : loss : 0.020160, loss_ce: 0.007384
2022-01-08 11:41:22,183 iteration 4632 : loss : 0.016003, loss_ce: 0.005508
2022-01-08 11:41:23,730 iteration 4633 : loss : 0.020741, loss_ce: 0.006096
2022-01-08 11:41:25,260 iteration 4634 : loss : 0.014687, loss_ce: 0.006794
2022-01-08 11:41:26,747 iteration 4635 : loss : 0.014186, loss_ce: 0.005889
2022-01-08 11:41:28,406 iteration 4636 : loss : 0.021068, loss_ce: 0.008363
2022-01-08 11:41:29,955 iteration 4637 : loss : 0.016414, loss_ce: 0.004518
2022-01-08 11:41:31,632 iteration 4638 : loss : 0.024010, loss_ce: 0.011399
2022-01-08 11:41:33,242 iteration 4639 : loss : 0.026581, loss_ce: 0.008833
2022-01-08 11:41:34,926 iteration 4640 : loss : 0.030631, loss_ce: 0.011713
2022-01-08 11:41:36,540 iteration 4641 : loss : 0.020753, loss_ce: 0.008441
 68%|███████████████████▊         | 273/400 [2:13:26<59:43, 28.21s/it]2022-01-08 11:41:38,138 iteration 4642 : loss : 0.018785, loss_ce: 0.004300
2022-01-08 11:41:39,701 iteration 4643 : loss : 0.017605, loss_ce: 0.006879
2022-01-08 11:41:41,324 iteration 4644 : loss : 0.022792, loss_ce: 0.008369
2022-01-08 11:41:42,958 iteration 4645 : loss : 0.021136, loss_ce: 0.009987
2022-01-08 11:41:44,555 iteration 4646 : loss : 0.042023, loss_ce: 0.011912
2022-01-08 11:41:46,051 iteration 4647 : loss : 0.015078, loss_ce: 0.004871
2022-01-08 11:41:47,588 iteration 4648 : loss : 0.020817, loss_ce: 0.006424
2022-01-08 11:41:49,171 iteration 4649 : loss : 0.017315, loss_ce: 0.007746
2022-01-08 11:41:50,788 iteration 4650 : loss : 0.024057, loss_ce: 0.006945
2022-01-08 11:41:52,301 iteration 4651 : loss : 0.015889, loss_ce: 0.006569
2022-01-08 11:41:53,947 iteration 4652 : loss : 0.020383, loss_ce: 0.007965
2022-01-08 11:41:55,492 iteration 4653 : loss : 0.017634, loss_ce: 0.007264
2022-01-08 11:41:57,044 iteration 4654 : loss : 0.015394, loss_ce: 0.005130
2022-01-08 11:41:58,676 iteration 4655 : loss : 0.029636, loss_ce: 0.013333
2022-01-08 11:42:00,326 iteration 4656 : loss : 0.024551, loss_ce: 0.009088
2022-01-08 11:42:01,939 iteration 4657 : loss : 0.024434, loss_ce: 0.010283
2022-01-08 11:42:03,546 iteration 4658 : loss : 0.018341, loss_ce: 0.006875
 68%|███████████████████▊         | 274/400 [2:13:53<58:28, 27.85s/it]2022-01-08 11:42:05,188 iteration 4659 : loss : 0.022047, loss_ce: 0.009279
2022-01-08 11:42:06,825 iteration 4660 : loss : 0.025054, loss_ce: 0.010500
2022-01-08 11:42:08,437 iteration 4661 : loss : 0.020608, loss_ce: 0.007562
2022-01-08 11:42:09,966 iteration 4662 : loss : 0.013627, loss_ce: 0.004997
2022-01-08 11:42:11,568 iteration 4663 : loss : 0.024293, loss_ce: 0.010419
2022-01-08 11:42:13,118 iteration 4664 : loss : 0.012405, loss_ce: 0.004301
2022-01-08 11:42:14,675 iteration 4665 : loss : 0.023457, loss_ce: 0.008082
2022-01-08 11:42:16,140 iteration 4666 : loss : 0.014022, loss_ce: 0.004987
2022-01-08 11:42:17,666 iteration 4667 : loss : 0.014801, loss_ce: 0.005734
2022-01-08 11:42:19,362 iteration 4668 : loss : 0.037974, loss_ce: 0.018227
2022-01-08 11:42:20,893 iteration 4669 : loss : 0.013158, loss_ce: 0.005620
2022-01-08 11:42:22,430 iteration 4670 : loss : 0.020901, loss_ce: 0.008388
2022-01-08 11:42:23,947 iteration 4671 : loss : 0.016207, loss_ce: 0.004429
2022-01-08 11:42:25,621 iteration 4672 : loss : 0.031251, loss_ce: 0.011377
2022-01-08 11:42:27,259 iteration 4673 : loss : 0.020703, loss_ce: 0.009723
2022-01-08 11:42:28,764 iteration 4674 : loss : 0.013566, loss_ce: 0.003866
2022-01-08 11:42:28,765 Training Data Eval:
2022-01-08 11:42:36,646   Average segmentation loss on training set: 0.0110
2022-01-08 11:42:36,646 Validation Data Eval:
2022-01-08 11:42:39,364   Average segmentation loss on validation set: 0.0645
2022-01-08 11:42:40,982 iteration 4675 : loss : 0.023757, loss_ce: 0.010215
 69%|██████████████████▌        | 275/400 [2:14:31<1:04:00, 30.73s/it]2022-01-08 11:42:42,692 iteration 4676 : loss : 0.022679, loss_ce: 0.006713
2022-01-08 11:42:44,253 iteration 4677 : loss : 0.019539, loss_ce: 0.006488
2022-01-08 11:42:45,856 iteration 4678 : loss : 0.027395, loss_ce: 0.008157
2022-01-08 11:42:47,509 iteration 4679 : loss : 0.030393, loss_ce: 0.007055
2022-01-08 11:42:49,077 iteration 4680 : loss : 0.015405, loss_ce: 0.005987
2022-01-08 11:42:50,640 iteration 4681 : loss : 0.037212, loss_ce: 0.010472
2022-01-08 11:42:52,183 iteration 4682 : loss : 0.027591, loss_ce: 0.012087
2022-01-08 11:42:53,654 iteration 4683 : loss : 0.024392, loss_ce: 0.006204
2022-01-08 11:42:55,281 iteration 4684 : loss : 0.021570, loss_ce: 0.009940
2022-01-08 11:42:56,879 iteration 4685 : loss : 0.025652, loss_ce: 0.007993
2022-01-08 11:42:58,441 iteration 4686 : loss : 0.025599, loss_ce: 0.008262
2022-01-08 11:42:59,982 iteration 4687 : loss : 0.022940, loss_ce: 0.008596
2022-01-08 11:43:01,473 iteration 4688 : loss : 0.012204, loss_ce: 0.005564
2022-01-08 11:43:02,974 iteration 4689 : loss : 0.011995, loss_ce: 0.005236
2022-01-08 11:43:04,498 iteration 4690 : loss : 0.018265, loss_ce: 0.007392
2022-01-08 11:43:06,110 iteration 4691 : loss : 0.024989, loss_ce: 0.012655
2022-01-08 11:43:07,663 iteration 4692 : loss : 0.018578, loss_ce: 0.005140
 69%|██████████████████▋        | 276/400 [2:14:57<1:00:59, 29.51s/it]2022-01-08 11:43:09,333 iteration 4693 : loss : 0.027479, loss_ce: 0.009914
2022-01-08 11:43:10,925 iteration 4694 : loss : 0.023007, loss_ce: 0.008323
2022-01-08 11:43:12,351 iteration 4695 : loss : 0.013868, loss_ce: 0.004305
2022-01-08 11:43:13,985 iteration 4696 : loss : 0.020352, loss_ce: 0.007483
2022-01-08 11:43:15,518 iteration 4697 : loss : 0.022446, loss_ce: 0.007034
2022-01-08 11:43:17,082 iteration 4698 : loss : 0.021355, loss_ce: 0.009343
2022-01-08 11:43:18,707 iteration 4699 : loss : 0.014868, loss_ce: 0.006217
2022-01-08 11:43:20,225 iteration 4700 : loss : 0.017469, loss_ce: 0.007718
2022-01-08 11:43:21,889 iteration 4701 : loss : 0.020869, loss_ce: 0.008226
2022-01-08 11:43:23,434 iteration 4702 : loss : 0.016873, loss_ce: 0.006924
2022-01-08 11:43:24,958 iteration 4703 : loss : 0.019424, loss_ce: 0.004859
2022-01-08 11:43:26,517 iteration 4704 : loss : 0.015389, loss_ce: 0.005045
2022-01-08 11:43:27,952 iteration 4705 : loss : 0.013058, loss_ce: 0.005464
2022-01-08 11:43:29,509 iteration 4706 : loss : 0.027133, loss_ce: 0.005860
2022-01-08 11:43:31,124 iteration 4707 : loss : 0.014271, loss_ce: 0.005044
2022-01-08 11:43:32,751 iteration 4708 : loss : 0.016882, loss_ce: 0.005235
2022-01-08 11:43:34,350 iteration 4709 : loss : 0.022047, loss_ce: 0.007254
 69%|████████████████████         | 277/400 [2:15:24<58:45, 28.66s/it]2022-01-08 11:43:35,979 iteration 4710 : loss : 0.020301, loss_ce: 0.005509
2022-01-08 11:43:37,689 iteration 4711 : loss : 0.034471, loss_ce: 0.014884
2022-01-08 11:43:39,242 iteration 4712 : loss : 0.021143, loss_ce: 0.008325
2022-01-08 11:43:40,799 iteration 4713 : loss : 0.017193, loss_ce: 0.006443
2022-01-08 11:43:42,380 iteration 4714 : loss : 0.024926, loss_ce: 0.007742
2022-01-08 11:43:44,010 iteration 4715 : loss : 0.024035, loss_ce: 0.006880
2022-01-08 11:43:45,600 iteration 4716 : loss : 0.018283, loss_ce: 0.006899
2022-01-08 11:43:47,159 iteration 4717 : loss : 0.015403, loss_ce: 0.005225
2022-01-08 11:43:48,664 iteration 4718 : loss : 0.017533, loss_ce: 0.006377
2022-01-08 11:43:50,166 iteration 4719 : loss : 0.018354, loss_ce: 0.008460
2022-01-08 11:43:51,691 iteration 4720 : loss : 0.015456, loss_ce: 0.006250
2022-01-08 11:43:53,235 iteration 4721 : loss : 0.014826, loss_ce: 0.005841
2022-01-08 11:43:54,847 iteration 4722 : loss : 0.021177, loss_ce: 0.006786
2022-01-08 11:43:56,408 iteration 4723 : loss : 0.013752, loss_ce: 0.005442
2022-01-08 11:43:57,906 iteration 4724 : loss : 0.014590, loss_ce: 0.005807
2022-01-08 11:43:59,401 iteration 4725 : loss : 0.015724, loss_ce: 0.006266
2022-01-08 11:44:00,960 iteration 4726 : loss : 0.016197, loss_ce: 0.006452
 70%|████████████████████▏        | 278/400 [2:15:51<57:02, 28.05s/it]2022-01-08 11:44:02,617 iteration 4727 : loss : 0.016961, loss_ce: 0.005672
2022-01-08 11:44:04,124 iteration 4728 : loss : 0.013413, loss_ce: 0.005159
2022-01-08 11:44:05,645 iteration 4729 : loss : 0.013183, loss_ce: 0.004628
2022-01-08 11:44:07,162 iteration 4730 : loss : 0.011512, loss_ce: 0.003728
2022-01-08 11:44:08,775 iteration 4731 : loss : 0.024280, loss_ce: 0.008101
2022-01-08 11:44:10,352 iteration 4732 : loss : 0.019606, loss_ce: 0.007730
2022-01-08 11:44:11,918 iteration 4733 : loss : 0.017020, loss_ce: 0.006082
2022-01-08 11:44:13,456 iteration 4734 : loss : 0.014976, loss_ce: 0.005647
2022-01-08 11:44:15,110 iteration 4735 : loss : 0.014612, loss_ce: 0.004116
2022-01-08 11:44:16,607 iteration 4736 : loss : 0.023911, loss_ce: 0.009422
2022-01-08 11:44:18,245 iteration 4737 : loss : 0.043733, loss_ce: 0.018404
2022-01-08 11:44:19,810 iteration 4738 : loss : 0.018064, loss_ce: 0.006531
2022-01-08 11:44:21,376 iteration 4739 : loss : 0.019803, loss_ce: 0.007909
2022-01-08 11:44:22,982 iteration 4740 : loss : 0.017939, loss_ce: 0.007525
2022-01-08 11:44:24,527 iteration 4741 : loss : 0.013340, loss_ce: 0.004336
2022-01-08 11:44:26,076 iteration 4742 : loss : 0.018830, loss_ce: 0.009381
2022-01-08 11:44:27,718 iteration 4743 : loss : 0.020021, loss_ce: 0.006310
 70%|████████████████████▏        | 279/400 [2:16:17<55:47, 27.66s/it]2022-01-08 11:44:29,376 iteration 4744 : loss : 0.018187, loss_ce: 0.006472
2022-01-08 11:44:30,944 iteration 4745 : loss : 0.032304, loss_ce: 0.009539
2022-01-08 11:44:32,468 iteration 4746 : loss : 0.013908, loss_ce: 0.007151
2022-01-08 11:44:34,004 iteration 4747 : loss : 0.019484, loss_ce: 0.006341
2022-01-08 11:44:35,575 iteration 4748 : loss : 0.026493, loss_ce: 0.009410
2022-01-08 11:44:37,153 iteration 4749 : loss : 0.018413, loss_ce: 0.006452
2022-01-08 11:44:38,768 iteration 4750 : loss : 0.019067, loss_ce: 0.007319
2022-01-08 11:44:40,312 iteration 4751 : loss : 0.016843, loss_ce: 0.006677
2022-01-08 11:44:41,904 iteration 4752 : loss : 0.018404, loss_ce: 0.006633
2022-01-08 11:44:43,519 iteration 4753 : loss : 0.019209, loss_ce: 0.006479
2022-01-08 11:44:45,109 iteration 4754 : loss : 0.015717, loss_ce: 0.005280
2022-01-08 11:44:46,784 iteration 4755 : loss : 0.017994, loss_ce: 0.005114
2022-01-08 11:44:48,323 iteration 4756 : loss : 0.017235, loss_ce: 0.008615
2022-01-08 11:44:49,873 iteration 4757 : loss : 0.023427, loss_ce: 0.009663
2022-01-08 11:44:51,401 iteration 4758 : loss : 0.018276, loss_ce: 0.006031
2022-01-08 11:44:52,964 iteration 4759 : loss : 0.013312, loss_ce: 0.004648
2022-01-08 11:44:52,964 Training Data Eval:
2022-01-08 11:45:00,850   Average segmentation loss on training set: 0.0122
2022-01-08 11:45:00,850 Validation Data Eval:
2022-01-08 11:45:03,561   Average segmentation loss on validation set: 0.0772
2022-01-08 11:45:05,094 iteration 4760 : loss : 0.018875, loss_ce: 0.008194
 70%|██████████████████▉        | 280/400 [2:16:55<1:01:09, 30.58s/it]2022-01-08 11:45:06,681 iteration 4761 : loss : 0.017413, loss_ce: 0.006186
2022-01-08 11:45:08,277 iteration 4762 : loss : 0.026529, loss_ce: 0.009576
2022-01-08 11:45:09,786 iteration 4763 : loss : 0.013194, loss_ce: 0.005548
2022-01-08 11:45:11,380 iteration 4764 : loss : 0.025343, loss_ce: 0.008097
2022-01-08 11:45:12,925 iteration 4765 : loss : 0.021458, loss_ce: 0.009517
2022-01-08 11:45:14,552 iteration 4766 : loss : 0.022674, loss_ce: 0.007808
2022-01-08 11:45:16,143 iteration 4767 : loss : 0.016321, loss_ce: 0.006658
2022-01-08 11:45:17,664 iteration 4768 : loss : 0.024443, loss_ce: 0.009443
2022-01-08 11:45:19,265 iteration 4769 : loss : 0.026343, loss_ce: 0.010893
2022-01-08 11:45:20,872 iteration 4770 : loss : 0.032992, loss_ce: 0.014222
2022-01-08 11:45:22,376 iteration 4771 : loss : 0.016336, loss_ce: 0.005544
2022-01-08 11:45:23,985 iteration 4772 : loss : 0.024503, loss_ce: 0.011512
2022-01-08 11:45:25,581 iteration 4773 : loss : 0.020041, loss_ce: 0.007342
2022-01-08 11:45:27,178 iteration 4774 : loss : 0.015087, loss_ce: 0.005929
2022-01-08 11:45:28,712 iteration 4775 : loss : 0.027799, loss_ce: 0.008218
2022-01-08 11:45:30,271 iteration 4776 : loss : 0.016181, loss_ce: 0.009356
2022-01-08 11:45:31,977 iteration 4777 : loss : 0.020240, loss_ce: 0.009243
 70%|████████████████████▎        | 281/400 [2:17:22<58:26, 29.47s/it]2022-01-08 11:45:33,518 iteration 4778 : loss : 0.012407, loss_ce: 0.003395
2022-01-08 11:45:35,068 iteration 4779 : loss : 0.016290, loss_ce: 0.006232
2022-01-08 11:45:36,564 iteration 4780 : loss : 0.011994, loss_ce: 0.004263
2022-01-08 11:45:38,105 iteration 4781 : loss : 0.020913, loss_ce: 0.006734
2022-01-08 11:45:39,735 iteration 4782 : loss : 0.028366, loss_ce: 0.015766
2022-01-08 11:45:41,307 iteration 4783 : loss : 0.022461, loss_ce: 0.011440
2022-01-08 11:45:42,814 iteration 4784 : loss : 0.014168, loss_ce: 0.006019
2022-01-08 11:45:44,457 iteration 4785 : loss : 0.020431, loss_ce: 0.008275
2022-01-08 11:45:46,057 iteration 4786 : loss : 0.019996, loss_ce: 0.006938
2022-01-08 11:45:47,662 iteration 4787 : loss : 0.018675, loss_ce: 0.005953
2022-01-08 11:45:49,266 iteration 4788 : loss : 0.016872, loss_ce: 0.005707
2022-01-08 11:45:50,776 iteration 4789 : loss : 0.015091, loss_ce: 0.007456
2022-01-08 11:45:52,437 iteration 4790 : loss : 0.023645, loss_ce: 0.008430
2022-01-08 11:45:53,985 iteration 4791 : loss : 0.023198, loss_ce: 0.011476
2022-01-08 11:45:55,599 iteration 4792 : loss : 0.028077, loss_ce: 0.008327
2022-01-08 11:45:57,124 iteration 4793 : loss : 0.014058, loss_ce: 0.005882
2022-01-08 11:45:58,602 iteration 4794 : loss : 0.022526, loss_ce: 0.006835
 70%|████████████████████▍        | 282/400 [2:17:48<56:16, 28.61s/it]2022-01-08 11:46:00,229 iteration 4795 : loss : 0.023927, loss_ce: 0.008188
2022-01-08 11:46:01,811 iteration 4796 : loss : 0.018268, loss_ce: 0.006258
2022-01-08 11:46:03,406 iteration 4797 : loss : 0.019776, loss_ce: 0.008210
2022-01-08 11:46:05,046 iteration 4798 : loss : 0.031415, loss_ce: 0.008824
2022-01-08 11:46:06,613 iteration 4799 : loss : 0.012705, loss_ce: 0.004055
2022-01-08 11:46:08,186 iteration 4800 : loss : 0.022647, loss_ce: 0.006441
2022-01-08 11:46:09,834 iteration 4801 : loss : 0.020308, loss_ce: 0.008385
2022-01-08 11:46:11,329 iteration 4802 : loss : 0.016935, loss_ce: 0.007930
2022-01-08 11:46:12,873 iteration 4803 : loss : 0.014719, loss_ce: 0.006590
2022-01-08 11:46:14,510 iteration 4804 : loss : 0.046162, loss_ce: 0.010620
2022-01-08 11:46:16,132 iteration 4805 : loss : 0.022197, loss_ce: 0.007662
2022-01-08 11:46:17,765 iteration 4806 : loss : 0.031857, loss_ce: 0.016343
2022-01-08 11:46:19,264 iteration 4807 : loss : 0.017795, loss_ce: 0.007413
2022-01-08 11:46:20,757 iteration 4808 : loss : 0.016088, loss_ce: 0.006968
2022-01-08 11:46:22,335 iteration 4809 : loss : 0.021748, loss_ce: 0.010814
2022-01-08 11:46:23,895 iteration 4810 : loss : 0.020421, loss_ce: 0.005664
2022-01-08 11:46:25,475 iteration 4811 : loss : 0.016195, loss_ce: 0.006301
 71%|████████████████████▌        | 283/400 [2:18:15<54:46, 28.09s/it]2022-01-08 11:46:27,026 iteration 4812 : loss : 0.020714, loss_ce: 0.009235
2022-01-08 11:46:28,609 iteration 4813 : loss : 0.018350, loss_ce: 0.005497
2022-01-08 11:46:30,218 iteration 4814 : loss : 0.019298, loss_ce: 0.004588
2022-01-08 11:46:31,722 iteration 4815 : loss : 0.018304, loss_ce: 0.008597
2022-01-08 11:46:33,397 iteration 4816 : loss : 0.021436, loss_ce: 0.010296
2022-01-08 11:46:34,950 iteration 4817 : loss : 0.025682, loss_ce: 0.011487
2022-01-08 11:46:36,495 iteration 4818 : loss : 0.017297, loss_ce: 0.006517
2022-01-08 11:46:37,973 iteration 4819 : loss : 0.017147, loss_ce: 0.007031
2022-01-08 11:46:39,502 iteration 4820 : loss : 0.019628, loss_ce: 0.007096
2022-01-08 11:46:41,149 iteration 4821 : loss : 0.021557, loss_ce: 0.010630
2022-01-08 11:46:42,803 iteration 4822 : loss : 0.022036, loss_ce: 0.006572
2022-01-08 11:46:44,368 iteration 4823 : loss : 0.014871, loss_ce: 0.005902
2022-01-08 11:46:45,955 iteration 4824 : loss : 0.021490, loss_ce: 0.009324
2022-01-08 11:46:47,495 iteration 4825 : loss : 0.019922, loss_ce: 0.007758
2022-01-08 11:46:49,033 iteration 4826 : loss : 0.017969, loss_ce: 0.005743
2022-01-08 11:46:50,579 iteration 4827 : loss : 0.016501, loss_ce: 0.006725
2022-01-08 11:46:52,109 iteration 4828 : loss : 0.021179, loss_ce: 0.009147
 71%|████████████████████▌        | 284/400 [2:18:42<53:28, 27.66s/it]2022-01-08 11:46:53,654 iteration 4829 : loss : 0.018367, loss_ce: 0.005887
2022-01-08 11:46:55,248 iteration 4830 : loss : 0.020281, loss_ce: 0.008640
2022-01-08 11:46:56,824 iteration 4831 : loss : 0.016802, loss_ce: 0.007173
2022-01-08 11:46:58,378 iteration 4832 : loss : 0.015317, loss_ce: 0.005023
2022-01-08 11:46:59,979 iteration 4833 : loss : 0.024126, loss_ce: 0.009255
2022-01-08 11:47:01,489 iteration 4834 : loss : 0.012695, loss_ce: 0.004377
2022-01-08 11:47:03,181 iteration 4835 : loss : 0.017295, loss_ce: 0.007462
2022-01-08 11:47:04,758 iteration 4836 : loss : 0.020363, loss_ce: 0.005821
2022-01-08 11:47:06,373 iteration 4837 : loss : 0.019407, loss_ce: 0.005659
2022-01-08 11:47:07,915 iteration 4838 : loss : 0.014018, loss_ce: 0.004286
2022-01-08 11:47:09,505 iteration 4839 : loss : 0.018788, loss_ce: 0.007210
2022-01-08 11:47:11,074 iteration 4840 : loss : 0.015879, loss_ce: 0.007025
2022-01-08 11:47:12,569 iteration 4841 : loss : 0.012657, loss_ce: 0.004068
2022-01-08 11:47:14,144 iteration 4842 : loss : 0.014577, loss_ce: 0.004743
2022-01-08 11:47:15,797 iteration 4843 : loss : 0.015464, loss_ce: 0.004717
2022-01-08 11:47:17,362 iteration 4844 : loss : 0.021875, loss_ce: 0.007588
2022-01-08 11:47:17,362 Training Data Eval:
2022-01-08 11:47:25,241   Average segmentation loss on training set: 0.0106
2022-01-08 11:47:25,242 Validation Data Eval:
2022-01-08 11:47:27,961   Average segmentation loss on validation set: 0.0662
2022-01-08 11:47:29,575 iteration 4845 : loss : 0.018150, loss_ce: 0.008784
 71%|████████████████████▋        | 285/400 [2:19:19<58:38, 30.60s/it]2022-01-08 11:47:31,151 iteration 4846 : loss : 0.015544, loss_ce: 0.006244
2022-01-08 11:47:32,689 iteration 4847 : loss : 0.028338, loss_ce: 0.008388
2022-01-08 11:47:34,314 iteration 4848 : loss : 0.017658, loss_ce: 0.008460
2022-01-08 11:47:35,916 iteration 4849 : loss : 0.018181, loss_ce: 0.005967
2022-01-08 11:47:37,461 iteration 4850 : loss : 0.018345, loss_ce: 0.005579
2022-01-08 11:47:39,147 iteration 4851 : loss : 0.014003, loss_ce: 0.005109
2022-01-08 11:47:40,706 iteration 4852 : loss : 0.015600, loss_ce: 0.004769
2022-01-08 11:47:42,284 iteration 4853 : loss : 0.015884, loss_ce: 0.006955
2022-01-08 11:47:43,865 iteration 4854 : loss : 0.028738, loss_ce: 0.011243
2022-01-08 11:47:45,402 iteration 4855 : loss : 0.019069, loss_ce: 0.005440
2022-01-08 11:47:47,059 iteration 4856 : loss : 0.020566, loss_ce: 0.007654
2022-01-08 11:47:48,597 iteration 4857 : loss : 0.012406, loss_ce: 0.004560
2022-01-08 11:47:50,239 iteration 4858 : loss : 0.020294, loss_ce: 0.010262
2022-01-08 11:47:51,747 iteration 4859 : loss : 0.011061, loss_ce: 0.004389
2022-01-08 11:47:53,354 iteration 4860 : loss : 0.022643, loss_ce: 0.010229
2022-01-08 11:47:54,894 iteration 4861 : loss : 0.016332, loss_ce: 0.005117
2022-01-08 11:47:56,500 iteration 4862 : loss : 0.017217, loss_ce: 0.008394
 72%|████████████████████▋        | 286/400 [2:19:46<56:02, 29.49s/it]2022-01-08 11:47:58,186 iteration 4863 : loss : 0.018756, loss_ce: 0.005180
2022-01-08 11:47:59,751 iteration 4864 : loss : 0.014364, loss_ce: 0.005236
2022-01-08 11:48:01,295 iteration 4865 : loss : 0.015587, loss_ce: 0.006812
2022-01-08 11:48:02,925 iteration 4866 : loss : 0.014731, loss_ce: 0.004789
2022-01-08 11:48:04,503 iteration 4867 : loss : 0.016070, loss_ce: 0.005570
2022-01-08 11:48:05,973 iteration 4868 : loss : 0.015106, loss_ce: 0.006626
2022-01-08 11:48:07,635 iteration 4869 : loss : 0.025728, loss_ce: 0.012746
2022-01-08 11:48:09,198 iteration 4870 : loss : 0.015975, loss_ce: 0.005415
2022-01-08 11:48:10,862 iteration 4871 : loss : 0.024049, loss_ce: 0.008296
2022-01-08 11:48:12,366 iteration 4872 : loss : 0.010880, loss_ce: 0.003334
2022-01-08 11:48:14,027 iteration 4873 : loss : 0.015026, loss_ce: 0.005821
2022-01-08 11:48:15,623 iteration 4874 : loss : 0.021744, loss_ce: 0.006357
2022-01-08 11:48:17,176 iteration 4875 : loss : 0.021498, loss_ce: 0.009293
2022-01-08 11:48:18,657 iteration 4876 : loss : 0.017017, loss_ce: 0.005744
2022-01-08 11:48:20,269 iteration 4877 : loss : 0.017610, loss_ce: 0.008582
2022-01-08 11:48:21,911 iteration 4878 : loss : 0.030501, loss_ce: 0.009243
2022-01-08 11:48:23,480 iteration 4879 : loss : 0.012982, loss_ce: 0.004352
 72%|████████████████████▊        | 287/400 [2:20:13<54:07, 28.74s/it]2022-01-08 11:48:25,059 iteration 4880 : loss : 0.020100, loss_ce: 0.008381
2022-01-08 11:48:26,570 iteration 4881 : loss : 0.014217, loss_ce: 0.005564
2022-01-08 11:48:28,049 iteration 4882 : loss : 0.013426, loss_ce: 0.005845
2022-01-08 11:48:29,633 iteration 4883 : loss : 0.022950, loss_ce: 0.010886
2022-01-08 11:48:31,237 iteration 4884 : loss : 0.019369, loss_ce: 0.005703
2022-01-08 11:48:32,698 iteration 4885 : loss : 0.012306, loss_ce: 0.004767
2022-01-08 11:48:34,253 iteration 4886 : loss : 0.021881, loss_ce: 0.005830
2022-01-08 11:48:35,750 iteration 4887 : loss : 0.011978, loss_ce: 0.003623
2022-01-08 11:48:37,333 iteration 4888 : loss : 0.014416, loss_ce: 0.004182
2022-01-08 11:48:38,899 iteration 4889 : loss : 0.029344, loss_ce: 0.012239
2022-01-08 11:48:40,417 iteration 4890 : loss : 0.017153, loss_ce: 0.009313
2022-01-08 11:48:41,866 iteration 4891 : loss : 0.011169, loss_ce: 0.004047
2022-01-08 11:48:43,463 iteration 4892 : loss : 0.019590, loss_ce: 0.008216
2022-01-08 11:48:45,025 iteration 4893 : loss : 0.022949, loss_ce: 0.011151
2022-01-08 11:48:46,575 iteration 4894 : loss : 0.013126, loss_ce: 0.005595
2022-01-08 11:48:48,113 iteration 4895 : loss : 0.014976, loss_ce: 0.003485
2022-01-08 11:48:49,630 iteration 4896 : loss : 0.014695, loss_ce: 0.004155
 72%|████████████████████▉        | 288/400 [2:20:39<52:12, 27.96s/it]2022-01-08 11:48:51,198 iteration 4897 : loss : 0.031797, loss_ce: 0.015702
2022-01-08 11:48:52,818 iteration 4898 : loss : 0.028605, loss_ce: 0.014238
2022-01-08 11:48:54,384 iteration 4899 : loss : 0.013678, loss_ce: 0.003762
2022-01-08 11:48:55,907 iteration 4900 : loss : 0.017461, loss_ce: 0.006322
2022-01-08 11:48:57,444 iteration 4901 : loss : 0.030645, loss_ce: 0.008696
2022-01-08 11:48:58,980 iteration 4902 : loss : 0.014694, loss_ce: 0.005998
2022-01-08 11:49:00,566 iteration 4903 : loss : 0.015714, loss_ce: 0.007426
2022-01-08 11:49:02,024 iteration 4904 : loss : 0.013359, loss_ce: 0.004463
2022-01-08 11:49:03,539 iteration 4905 : loss : 0.016961, loss_ce: 0.004880
2022-01-08 11:49:05,130 iteration 4906 : loss : 0.017792, loss_ce: 0.006702
2022-01-08 11:49:06,643 iteration 4907 : loss : 0.021650, loss_ce: 0.008283
2022-01-08 11:49:08,277 iteration 4908 : loss : 0.020416, loss_ce: 0.009633
2022-01-08 11:49:09,868 iteration 4909 : loss : 0.014089, loss_ce: 0.004377
2022-01-08 11:49:11,385 iteration 4910 : loss : 0.014859, loss_ce: 0.006997
2022-01-08 11:49:12,915 iteration 4911 : loss : 0.011790, loss_ce: 0.004416
2022-01-08 11:49:14,476 iteration 4912 : loss : 0.014764, loss_ce: 0.006950
2022-01-08 11:49:16,024 iteration 4913 : loss : 0.028790, loss_ce: 0.011546
 72%|████████████████████▉        | 289/400 [2:21:06<50:51, 27.49s/it]2022-01-08 11:49:17,659 iteration 4914 : loss : 0.017650, loss_ce: 0.006741
2022-01-08 11:49:19,182 iteration 4915 : loss : 0.016372, loss_ce: 0.005737
2022-01-08 11:49:20,712 iteration 4916 : loss : 0.017801, loss_ce: 0.004946
2022-01-08 11:49:22,271 iteration 4917 : loss : 0.012058, loss_ce: 0.003908
2022-01-08 11:49:23,854 iteration 4918 : loss : 0.022649, loss_ce: 0.008489
2022-01-08 11:49:25,568 iteration 4919 : loss : 0.024836, loss_ce: 0.012627
2022-01-08 11:49:27,124 iteration 4920 : loss : 0.016547, loss_ce: 0.007052
2022-01-08 11:49:28,650 iteration 4921 : loss : 0.022415, loss_ce: 0.014085
2022-01-08 11:49:30,148 iteration 4922 : loss : 0.014411, loss_ce: 0.003836
2022-01-08 11:49:31,759 iteration 4923 : loss : 0.019510, loss_ce: 0.008270
2022-01-08 11:49:33,398 iteration 4924 : loss : 0.019241, loss_ce: 0.010227
2022-01-08 11:49:34,990 iteration 4925 : loss : 0.013861, loss_ce: 0.005222
2022-01-08 11:49:36,619 iteration 4926 : loss : 0.015977, loss_ce: 0.007128
2022-01-08 11:49:38,167 iteration 4927 : loss : 0.012715, loss_ce: 0.003872
2022-01-08 11:49:39,799 iteration 4928 : loss : 0.026783, loss_ce: 0.010852
2022-01-08 11:49:41,455 iteration 4929 : loss : 0.018923, loss_ce: 0.008348
2022-01-08 11:49:41,456 Training Data Eval:
2022-01-08 11:49:49,338   Average segmentation loss on training set: 0.0105
2022-01-08 11:49:49,338 Validation Data Eval:
2022-01-08 11:49:52,058   Average segmentation loss on validation set: 0.0640
2022-01-08 11:49:53,640 iteration 4930 : loss : 0.016078, loss_ce: 0.004920
 72%|█████████████████████        | 290/400 [2:21:43<55:58, 30.53s/it]2022-01-08 11:49:55,237 iteration 4931 : loss : 0.016711, loss_ce: 0.007768
2022-01-08 11:49:56,839 iteration 4932 : loss : 0.017664, loss_ce: 0.008222
2022-01-08 11:49:58,351 iteration 4933 : loss : 0.015009, loss_ce: 0.007965
2022-01-08 11:49:59,989 iteration 4934 : loss : 0.022103, loss_ce: 0.006791
2022-01-08 11:50:01,563 iteration 4935 : loss : 0.023474, loss_ce: 0.007077
2022-01-08 11:50:03,163 iteration 4936 : loss : 0.024030, loss_ce: 0.009738
2022-01-08 11:50:04,755 iteration 4937 : loss : 0.014157, loss_ce: 0.005994
2022-01-08 11:50:06,388 iteration 4938 : loss : 0.030487, loss_ce: 0.011364
2022-01-08 11:50:08,015 iteration 4939 : loss : 0.022884, loss_ce: 0.009119
2022-01-08 11:50:09,573 iteration 4940 : loss : 0.020183, loss_ce: 0.008128
2022-01-08 11:50:11,156 iteration 4941 : loss : 0.021038, loss_ce: 0.010158
2022-01-08 11:50:12,747 iteration 4942 : loss : 0.021136, loss_ce: 0.006597
2022-01-08 11:50:14,418 iteration 4943 : loss : 0.030849, loss_ce: 0.008001
2022-01-08 11:50:15,987 iteration 4944 : loss : 0.018850, loss_ce: 0.006090
2022-01-08 11:50:17,598 iteration 4945 : loss : 0.018380, loss_ce: 0.007055
2022-01-08 11:50:19,229 iteration 4946 : loss : 0.035086, loss_ce: 0.011822
2022-01-08 11:50:20,782 iteration 4947 : loss : 0.025464, loss_ce: 0.007979
 73%|█████████████████████        | 291/400 [2:22:11<53:36, 29.51s/it]2022-01-08 11:50:22,414 iteration 4948 : loss : 0.021190, loss_ce: 0.007463
2022-01-08 11:50:23,959 iteration 4949 : loss : 0.019913, loss_ce: 0.007914
2022-01-08 11:50:25,568 iteration 4950 : loss : 0.019967, loss_ce: 0.008206
2022-01-08 11:50:27,167 iteration 4951 : loss : 0.015600, loss_ce: 0.005159
2022-01-08 11:50:28,751 iteration 4952 : loss : 0.014701, loss_ce: 0.007706
2022-01-08 11:50:30,359 iteration 4953 : loss : 0.018357, loss_ce: 0.007928
2022-01-08 11:50:31,903 iteration 4954 : loss : 0.012228, loss_ce: 0.004115
2022-01-08 11:50:33,403 iteration 4955 : loss : 0.020077, loss_ce: 0.005638
2022-01-08 11:50:34,955 iteration 4956 : loss : 0.017159, loss_ce: 0.005510
2022-01-08 11:50:36,433 iteration 4957 : loss : 0.014597, loss_ce: 0.005564
2022-01-08 11:50:38,006 iteration 4958 : loss : 0.023520, loss_ce: 0.011275
2022-01-08 11:50:39,579 iteration 4959 : loss : 0.017104, loss_ce: 0.005813
2022-01-08 11:50:41,073 iteration 4960 : loss : 0.014272, loss_ce: 0.005052
2022-01-08 11:50:42,647 iteration 4961 : loss : 0.015321, loss_ce: 0.005918
2022-01-08 11:50:44,155 iteration 4962 : loss : 0.019861, loss_ce: 0.005128
2022-01-08 11:50:45,694 iteration 4963 : loss : 0.014699, loss_ce: 0.005604
2022-01-08 11:50:47,303 iteration 4964 : loss : 0.021403, loss_ce: 0.007624
 73%|█████████████████████▏       | 292/400 [2:22:37<51:30, 28.62s/it]2022-01-08 11:50:48,977 iteration 4965 : loss : 0.024080, loss_ce: 0.010144
2022-01-08 11:50:50,605 iteration 4966 : loss : 0.024092, loss_ce: 0.008881
2022-01-08 11:50:52,187 iteration 4967 : loss : 0.018206, loss_ce: 0.008156
2022-01-08 11:50:53,784 iteration 4968 : loss : 0.016876, loss_ce: 0.008175
2022-01-08 11:50:55,466 iteration 4969 : loss : 0.041543, loss_ce: 0.018356
2022-01-08 11:50:56,993 iteration 4970 : loss : 0.014821, loss_ce: 0.006457
2022-01-08 11:50:58,541 iteration 4971 : loss : 0.017638, loss_ce: 0.008758
2022-01-08 11:51:00,148 iteration 4972 : loss : 0.019022, loss_ce: 0.008550
2022-01-08 11:51:01,722 iteration 4973 : loss : 0.020414, loss_ce: 0.007697
2022-01-08 11:51:03,420 iteration 4974 : loss : 0.022570, loss_ce: 0.007442
2022-01-08 11:51:04,976 iteration 4975 : loss : 0.018779, loss_ce: 0.005873
2022-01-08 11:51:06,484 iteration 4976 : loss : 0.016395, loss_ce: 0.006520
2022-01-08 11:51:08,079 iteration 4977 : loss : 0.023885, loss_ce: 0.006097
2022-01-08 11:51:09,629 iteration 4978 : loss : 0.036971, loss_ce: 0.007460
2022-01-08 11:51:11,191 iteration 4979 : loss : 0.020378, loss_ce: 0.007686
2022-01-08 11:51:12,770 iteration 4980 : loss : 0.016455, loss_ce: 0.004633
2022-01-08 11:51:14,332 iteration 4981 : loss : 0.018699, loss_ce: 0.006795
 73%|█████████████████████▏       | 293/400 [2:23:04<50:11, 28.14s/it]2022-01-08 11:51:15,918 iteration 4982 : loss : 0.015266, loss_ce: 0.006606
2022-01-08 11:51:17,460 iteration 4983 : loss : 0.017931, loss_ce: 0.007337
2022-01-08 11:51:19,121 iteration 4984 : loss : 0.020263, loss_ce: 0.007409
2022-01-08 11:51:20,620 iteration 4985 : loss : 0.015885, loss_ce: 0.004211
2022-01-08 11:51:22,266 iteration 4986 : loss : 0.021247, loss_ce: 0.009350
2022-01-08 11:51:23,788 iteration 4987 : loss : 0.019815, loss_ce: 0.006163
2022-01-08 11:51:25,368 iteration 4988 : loss : 0.021871, loss_ce: 0.006394
2022-01-08 11:51:26,995 iteration 4989 : loss : 0.019983, loss_ce: 0.007792
2022-01-08 11:51:28,645 iteration 4990 : loss : 0.019163, loss_ce: 0.008172
2022-01-08 11:51:30,234 iteration 4991 : loss : 0.024843, loss_ce: 0.007304
2022-01-08 11:51:31,717 iteration 4992 : loss : 0.015166, loss_ce: 0.007345
2022-01-08 11:51:33,267 iteration 4993 : loss : 0.018004, loss_ce: 0.006425
2022-01-08 11:51:34,783 iteration 4994 : loss : 0.022186, loss_ce: 0.007269
2022-01-08 11:51:36,314 iteration 4995 : loss : 0.020114, loss_ce: 0.008779
2022-01-08 11:51:37,936 iteration 4996 : loss : 0.024058, loss_ce: 0.007050
2022-01-08 11:51:39,430 iteration 4997 : loss : 0.015626, loss_ce: 0.004709
2022-01-08 11:51:40,937 iteration 4998 : loss : 0.017722, loss_ce: 0.006413
 74%|█████████████████████▎       | 294/400 [2:23:31<48:54, 27.68s/it]2022-01-08 11:51:42,507 iteration 4999 : loss : 0.017369, loss_ce: 0.006299
2022-01-08 11:51:44,051 iteration 5000 : loss : 0.016920, loss_ce: 0.006801
2022-01-08 11:51:45,601 iteration 5001 : loss : 0.014983, loss_ce: 0.006823
2022-01-08 11:51:47,141 iteration 5002 : loss : 0.013743, loss_ce: 0.004592
2022-01-08 11:51:48,726 iteration 5003 : loss : 0.034313, loss_ce: 0.007716
2022-01-08 11:51:50,280 iteration 5004 : loss : 0.019449, loss_ce: 0.008559
2022-01-08 11:51:51,841 iteration 5005 : loss : 0.018502, loss_ce: 0.007886
2022-01-08 11:51:53,459 iteration 5006 : loss : 0.019925, loss_ce: 0.008059
2022-01-08 11:51:55,060 iteration 5007 : loss : 0.019700, loss_ce: 0.008269
2022-01-08 11:51:56,672 iteration 5008 : loss : 0.027252, loss_ce: 0.011681
2022-01-08 11:51:58,280 iteration 5009 : loss : 0.016967, loss_ce: 0.004851
2022-01-08 11:51:59,874 iteration 5010 : loss : 0.024406, loss_ce: 0.008964
2022-01-08 11:52:01,423 iteration 5011 : loss : 0.017255, loss_ce: 0.008158
2022-01-08 11:52:03,002 iteration 5012 : loss : 0.023720, loss_ce: 0.008747
2022-01-08 11:52:04,511 iteration 5013 : loss : 0.015046, loss_ce: 0.005635
2022-01-08 11:52:06,070 iteration 5014 : loss : 0.019315, loss_ce: 0.007040
2022-01-08 11:52:06,070 Training Data Eval:
2022-01-08 11:52:13,953   Average segmentation loss on training set: 0.0102
2022-01-08 11:52:13,953 Validation Data Eval:
2022-01-08 11:52:16,674   Average segmentation loss on validation set: 0.0685
2022-01-08 11:52:18,233 iteration 5015 : loss : 0.015708, loss_ce: 0.004967
 74%|█████████████████████▍       | 295/400 [2:24:08<53:29, 30.56s/it]2022-01-08 11:52:19,828 iteration 5016 : loss : 0.019381, loss_ce: 0.008815
2022-01-08 11:52:21,369 iteration 5017 : loss : 0.015112, loss_ce: 0.006225
2022-01-08 11:52:22,923 iteration 5018 : loss : 0.017530, loss_ce: 0.007308
2022-01-08 11:52:24,502 iteration 5019 : loss : 0.019709, loss_ce: 0.006167
2022-01-08 11:52:26,092 iteration 5020 : loss : 0.020096, loss_ce: 0.006075
2022-01-08 11:52:27,702 iteration 5021 : loss : 0.018212, loss_ce: 0.009382
2022-01-08 11:52:29,216 iteration 5022 : loss : 0.014083, loss_ce: 0.004733
2022-01-08 11:52:30,782 iteration 5023 : loss : 0.024891, loss_ce: 0.007020
2022-01-08 11:52:32,439 iteration 5024 : loss : 0.027859, loss_ce: 0.006801
2022-01-08 11:52:34,071 iteration 5025 : loss : 0.027366, loss_ce: 0.005762
2022-01-08 11:52:35,650 iteration 5026 : loss : 0.017403, loss_ce: 0.006356
2022-01-08 11:52:37,184 iteration 5027 : loss : 0.014269, loss_ce: 0.005743
2022-01-08 11:52:38,731 iteration 5028 : loss : 0.017020, loss_ce: 0.006331
2022-01-08 11:52:40,388 iteration 5029 : loss : 0.028129, loss_ce: 0.004759
2022-01-08 11:52:41,962 iteration 5030 : loss : 0.019343, loss_ce: 0.010472
2022-01-08 11:52:43,609 iteration 5031 : loss : 0.026352, loss_ce: 0.011203
2022-01-08 11:52:45,195 iteration 5032 : loss : 0.019328, loss_ce: 0.008912
 74%|█████████████████████▍       | 296/400 [2:24:35<51:06, 29.48s/it]2022-01-08 11:52:46,924 iteration 5033 : loss : 0.031916, loss_ce: 0.012414
2022-01-08 11:52:48,478 iteration 5034 : loss : 0.038427, loss_ce: 0.009232
2022-01-08 11:52:50,018 iteration 5035 : loss : 0.021209, loss_ce: 0.008370
2022-01-08 11:52:51,571 iteration 5036 : loss : 0.018716, loss_ce: 0.007055
2022-01-08 11:52:53,155 iteration 5037 : loss : 0.021789, loss_ce: 0.008201
2022-01-08 11:52:54,701 iteration 5038 : loss : 0.024136, loss_ce: 0.006145
2022-01-08 11:52:56,361 iteration 5039 : loss : 0.021228, loss_ce: 0.010384
2022-01-08 11:52:57,941 iteration 5040 : loss : 0.017687, loss_ce: 0.005277
2022-01-08 11:52:59,581 iteration 5041 : loss : 0.020690, loss_ce: 0.007433
2022-01-08 11:53:01,112 iteration 5042 : loss : 0.015327, loss_ce: 0.006163
2022-01-08 11:53:02,686 iteration 5043 : loss : 0.017754, loss_ce: 0.008359
2022-01-08 11:53:04,263 iteration 5044 : loss : 0.017301, loss_ce: 0.007738
2022-01-08 11:53:05,884 iteration 5045 : loss : 0.017473, loss_ce: 0.007496
2022-01-08 11:53:07,471 iteration 5046 : loss : 0.028204, loss_ce: 0.007954
2022-01-08 11:53:09,021 iteration 5047 : loss : 0.020338, loss_ce: 0.005298
2022-01-08 11:53:10,562 iteration 5048 : loss : 0.018808, loss_ce: 0.007651
2022-01-08 11:53:12,106 iteration 5049 : loss : 0.016144, loss_ce: 0.006603
 74%|█████████████████████▌       | 297/400 [2:25:02<49:17, 28.71s/it]2022-01-08 11:53:13,720 iteration 5050 : loss : 0.025980, loss_ce: 0.008687
2022-01-08 11:53:15,271 iteration 5051 : loss : 0.020721, loss_ce: 0.007142
2022-01-08 11:53:16,831 iteration 5052 : loss : 0.018115, loss_ce: 0.007822
2022-01-08 11:53:18,377 iteration 5053 : loss : 0.014763, loss_ce: 0.005354
2022-01-08 11:53:19,941 iteration 5054 : loss : 0.017679, loss_ce: 0.005133
2022-01-08 11:53:21,486 iteration 5055 : loss : 0.017865, loss_ce: 0.006150
2022-01-08 11:53:23,023 iteration 5056 : loss : 0.028915, loss_ce: 0.007217
2022-01-08 11:53:24,636 iteration 5057 : loss : 0.018291, loss_ce: 0.009434
2022-01-08 11:53:26,266 iteration 5058 : loss : 0.026368, loss_ce: 0.010854
2022-01-08 11:53:27,720 iteration 5059 : loss : 0.017200, loss_ce: 0.004734
2022-01-08 11:53:29,283 iteration 5060 : loss : 0.019067, loss_ce: 0.006137
2022-01-08 11:53:30,877 iteration 5061 : loss : 0.015264, loss_ce: 0.006273
2022-01-08 11:53:32,465 iteration 5062 : loss : 0.022431, loss_ce: 0.006932
2022-01-08 11:53:34,093 iteration 5063 : loss : 0.017962, loss_ce: 0.007346
2022-01-08 11:53:35,622 iteration 5064 : loss : 0.016659, loss_ce: 0.006224
2022-01-08 11:53:37,157 iteration 5065 : loss : 0.019583, loss_ce: 0.006857
2022-01-08 11:53:38,730 iteration 5066 : loss : 0.014810, loss_ce: 0.008079
 74%|█████████████████████▌       | 298/400 [2:25:28<47:44, 28.08s/it]2022-01-08 11:53:40,426 iteration 5067 : loss : 0.027096, loss_ce: 0.009146
2022-01-08 11:53:42,022 iteration 5068 : loss : 0.024775, loss_ce: 0.010578
2022-01-08 11:53:43,611 iteration 5069 : loss : 0.023274, loss_ce: 0.007947
2022-01-08 11:53:45,166 iteration 5070 : loss : 0.015844, loss_ce: 0.004500
2022-01-08 11:53:46,803 iteration 5071 : loss : 0.023870, loss_ce: 0.009799
2022-01-08 11:53:48,257 iteration 5072 : loss : 0.014978, loss_ce: 0.005018
2022-01-08 11:53:49,812 iteration 5073 : loss : 0.015717, loss_ce: 0.008184
2022-01-08 11:53:51,420 iteration 5074 : loss : 0.017091, loss_ce: 0.005068
2022-01-08 11:53:52,989 iteration 5075 : loss : 0.016189, loss_ce: 0.006939
2022-01-08 11:53:54,610 iteration 5076 : loss : 0.032305, loss_ce: 0.020396
2022-01-08 11:53:56,152 iteration 5077 : loss : 0.013319, loss_ce: 0.004782
2022-01-08 11:53:57,731 iteration 5078 : loss : 0.014594, loss_ce: 0.003970
2022-01-08 11:53:59,275 iteration 5079 : loss : 0.015145, loss_ce: 0.005801
2022-01-08 11:54:00,851 iteration 5080 : loss : 0.016637, loss_ce: 0.005627
2022-01-08 11:54:02,452 iteration 5081 : loss : 0.027936, loss_ce: 0.017276
2022-01-08 11:54:04,077 iteration 5082 : loss : 0.035632, loss_ce: 0.017722
2022-01-08 11:54:05,688 iteration 5083 : loss : 0.018894, loss_ce: 0.006048
 75%|█████████████████████▋       | 299/400 [2:25:55<46:42, 27.75s/it]2022-01-08 11:54:07,273 iteration 5084 : loss : 0.015954, loss_ce: 0.005101
2022-01-08 11:54:08,839 iteration 5085 : loss : 0.018263, loss_ce: 0.007096
2022-01-08 11:54:10,380 iteration 5086 : loss : 0.013441, loss_ce: 0.005185
2022-01-08 11:54:12,003 iteration 5087 : loss : 0.024884, loss_ce: 0.009719
2022-01-08 11:54:13,602 iteration 5088 : loss : 0.016573, loss_ce: 0.006674
2022-01-08 11:54:15,133 iteration 5089 : loss : 0.015608, loss_ce: 0.005211
2022-01-08 11:54:16,680 iteration 5090 : loss : 0.025123, loss_ce: 0.009192
2022-01-08 11:54:18,357 iteration 5091 : loss : 0.023735, loss_ce: 0.009240
2022-01-08 11:54:19,925 iteration 5092 : loss : 0.012089, loss_ce: 0.005442
2022-01-08 11:54:21,460 iteration 5093 : loss : 0.016053, loss_ce: 0.007273
2022-01-08 11:54:22,967 iteration 5094 : loss : 0.012986, loss_ce: 0.004917
2022-01-08 11:54:24,574 iteration 5095 : loss : 0.017416, loss_ce: 0.006408
2022-01-08 11:54:26,093 iteration 5096 : loss : 0.016073, loss_ce: 0.006122
2022-01-08 11:54:27,627 iteration 5097 : loss : 0.015280, loss_ce: 0.007331
2022-01-08 11:54:29,210 iteration 5098 : loss : 0.017066, loss_ce: 0.005669
2022-01-08 11:54:30,767 iteration 5099 : loss : 0.013649, loss_ce: 0.004317
2022-01-08 11:54:30,767 Training Data Eval:
2022-01-08 11:54:38,654   Average segmentation loss on training set: 0.0105
2022-01-08 11:54:38,654 Validation Data Eval:
2022-01-08 11:54:41,370   Average segmentation loss on validation set: 0.0702
2022-01-08 11:54:42,929 iteration 5100 : loss : 0.014794, loss_ce: 0.007706
 75%|█████████████████████▊       | 300/400 [2:26:33<50:59, 30.60s/it]2022-01-08 11:54:44,551 iteration 5101 : loss : 0.018381, loss_ce: 0.008666
2022-01-08 11:54:46,086 iteration 5102 : loss : 0.011106, loss_ce: 0.003951
2022-01-08 11:54:47,641 iteration 5103 : loss : 0.016239, loss_ce: 0.007187
2022-01-08 11:54:49,276 iteration 5104 : loss : 0.013289, loss_ce: 0.005419
2022-01-08 11:54:50,789 iteration 5105 : loss : 0.015091, loss_ce: 0.005233
2022-01-08 11:54:52,427 iteration 5106 : loss : 0.020289, loss_ce: 0.007362
2022-01-08 11:54:53,943 iteration 5107 : loss : 0.013692, loss_ce: 0.005346
2022-01-08 11:54:55,535 iteration 5108 : loss : 0.025266, loss_ce: 0.008970
2022-01-08 11:54:57,104 iteration 5109 : loss : 0.028095, loss_ce: 0.009201
2022-01-08 11:54:58,664 iteration 5110 : loss : 0.026133, loss_ce: 0.011879
2022-01-08 11:55:00,260 iteration 5111 : loss : 0.015422, loss_ce: 0.005513
2022-01-08 11:55:01,777 iteration 5112 : loss : 0.026123, loss_ce: 0.008762
2022-01-08 11:55:03,407 iteration 5113 : loss : 0.028696, loss_ce: 0.008350
2022-01-08 11:55:04,956 iteration 5114 : loss : 0.018377, loss_ce: 0.009052
2022-01-08 11:55:06,483 iteration 5115 : loss : 0.019604, loss_ce: 0.004872
2022-01-08 11:55:08,021 iteration 5116 : loss : 0.015973, loss_ce: 0.006184
2022-01-08 11:55:09,645 iteration 5117 : loss : 0.018059, loss_ce: 0.009656
 75%|█████████████████████▊       | 301/400 [2:26:59<48:33, 29.43s/it]2022-01-08 11:55:11,262 iteration 5118 : loss : 0.012981, loss_ce: 0.004838
2022-01-08 11:55:12,838 iteration 5119 : loss : 0.022467, loss_ce: 0.007091
2022-01-08 11:55:14,371 iteration 5120 : loss : 0.022227, loss_ce: 0.005106
2022-01-08 11:55:15,935 iteration 5121 : loss : 0.017099, loss_ce: 0.008093
2022-01-08 11:55:17,459 iteration 5122 : loss : 0.017530, loss_ce: 0.006497
2022-01-08 11:55:19,015 iteration 5123 : loss : 0.015242, loss_ce: 0.004329
2022-01-08 11:55:20,607 iteration 5124 : loss : 0.017787, loss_ce: 0.007861
2022-01-08 11:55:22,094 iteration 5125 : loss : 0.016544, loss_ce: 0.008535
2022-01-08 11:55:23,602 iteration 5126 : loss : 0.025788, loss_ce: 0.008094
2022-01-08 11:55:25,205 iteration 5127 : loss : 0.019210, loss_ce: 0.005907
2022-01-08 11:55:26,744 iteration 5128 : loss : 0.011559, loss_ce: 0.004389
2022-01-08 11:55:28,326 iteration 5129 : loss : 0.016676, loss_ce: 0.004835
2022-01-08 11:55:29,895 iteration 5130 : loss : 0.016127, loss_ce: 0.005162
2022-01-08 11:55:31,379 iteration 5131 : loss : 0.010456, loss_ce: 0.003796
2022-01-08 11:55:32,929 iteration 5132 : loss : 0.017790, loss_ce: 0.008435
2022-01-08 11:55:34,453 iteration 5133 : loss : 0.021988, loss_ce: 0.006513
2022-01-08 11:55:35,959 iteration 5134 : loss : 0.015043, loss_ce: 0.006036
 76%|█████████████████████▉       | 302/400 [2:27:26<46:32, 28.50s/it]2022-01-08 11:55:37,702 iteration 5135 : loss : 0.042679, loss_ce: 0.017645
2022-01-08 11:55:39,261 iteration 5136 : loss : 0.017874, loss_ce: 0.006057
2022-01-08 11:55:40,874 iteration 5137 : loss : 0.019990, loss_ce: 0.006532
2022-01-08 11:55:42,401 iteration 5138 : loss : 0.013179, loss_ce: 0.005719
2022-01-08 11:55:43,964 iteration 5139 : loss : 0.015340, loss_ce: 0.006203
2022-01-08 11:55:45,612 iteration 5140 : loss : 0.014555, loss_ce: 0.005993
2022-01-08 11:55:47,134 iteration 5141 : loss : 0.019787, loss_ce: 0.006740
2022-01-08 11:55:48,711 iteration 5142 : loss : 0.029155, loss_ce: 0.010503
2022-01-08 11:55:50,197 iteration 5143 : loss : 0.015531, loss_ce: 0.006028
2022-01-08 11:55:51,787 iteration 5144 : loss : 0.018902, loss_ce: 0.008840
2022-01-08 11:55:53,269 iteration 5145 : loss : 0.012253, loss_ce: 0.003832
2022-01-08 11:55:54,897 iteration 5146 : loss : 0.020621, loss_ce: 0.009137
2022-01-08 11:55:56,523 iteration 5147 : loss : 0.023396, loss_ce: 0.007648
2022-01-08 11:55:58,069 iteration 5148 : loss : 0.023267, loss_ce: 0.010911
2022-01-08 11:55:59,613 iteration 5149 : loss : 0.024345, loss_ce: 0.005686
2022-01-08 11:56:01,156 iteration 5150 : loss : 0.013174, loss_ce: 0.005284
2022-01-08 11:56:02,729 iteration 5151 : loss : 0.028907, loss_ce: 0.005264
 76%|█████████████████████▉       | 303/400 [2:27:53<45:13, 27.98s/it]2022-01-08 11:56:04,288 iteration 5152 : loss : 0.015067, loss_ce: 0.003367
2022-01-08 11:56:05,759 iteration 5153 : loss : 0.013665, loss_ce: 0.005391
2022-01-08 11:56:07,382 iteration 5154 : loss : 0.019142, loss_ce: 0.008504
2022-01-08 11:56:08,884 iteration 5155 : loss : 0.012158, loss_ce: 0.004137
2022-01-08 11:56:10,425 iteration 5156 : loss : 0.023459, loss_ce: 0.009586
2022-01-08 11:56:11,919 iteration 5157 : loss : 0.012275, loss_ce: 0.003855
2022-01-08 11:56:13,461 iteration 5158 : loss : 0.016559, loss_ce: 0.005318
2022-01-08 11:56:15,075 iteration 5159 : loss : 0.018636, loss_ce: 0.007387
2022-01-08 11:56:16,584 iteration 5160 : loss : 0.025175, loss_ce: 0.009766
2022-01-08 11:56:18,140 iteration 5161 : loss : 0.019726, loss_ce: 0.007382
2022-01-08 11:56:19,651 iteration 5162 : loss : 0.016515, loss_ce: 0.007230
2022-01-08 11:56:21,233 iteration 5163 : loss : 0.016138, loss_ce: 0.006444
2022-01-08 11:56:22,818 iteration 5164 : loss : 0.016795, loss_ce: 0.008267
2022-01-08 11:56:24,319 iteration 5165 : loss : 0.016214, loss_ce: 0.005106
2022-01-08 11:56:25,876 iteration 5166 : loss : 0.019792, loss_ce: 0.008288
2022-01-08 11:56:27,456 iteration 5167 : loss : 0.020244, loss_ce: 0.006394
2022-01-08 11:56:29,078 iteration 5168 : loss : 0.017428, loss_ce: 0.006093
 76%|██████████████████████       | 304/400 [2:28:19<43:59, 27.49s/it]2022-01-08 11:56:30,799 iteration 5169 : loss : 0.024342, loss_ce: 0.011475
2022-01-08 11:56:32,311 iteration 5170 : loss : 0.014387, loss_ce: 0.004175
2022-01-08 11:56:33,929 iteration 5171 : loss : 0.016637, loss_ce: 0.007566
2022-01-08 11:56:35,418 iteration 5172 : loss : 0.012568, loss_ce: 0.003615
2022-01-08 11:56:36,852 iteration 5173 : loss : 0.012203, loss_ce: 0.005167
2022-01-08 11:56:38,485 iteration 5174 : loss : 0.019791, loss_ce: 0.008289
2022-01-08 11:56:40,015 iteration 5175 : loss : 0.015571, loss_ce: 0.007322
2022-01-08 11:56:41,711 iteration 5176 : loss : 0.022030, loss_ce: 0.008646
2022-01-08 11:56:43,238 iteration 5177 : loss : 0.014592, loss_ce: 0.005900
2022-01-08 11:56:44,791 iteration 5178 : loss : 0.012429, loss_ce: 0.003733
2022-01-08 11:56:46,410 iteration 5179 : loss : 0.014995, loss_ce: 0.004538
2022-01-08 11:56:47,973 iteration 5180 : loss : 0.016270, loss_ce: 0.005769
2022-01-08 11:56:49,488 iteration 5181 : loss : 0.011318, loss_ce: 0.002552
2022-01-08 11:56:51,048 iteration 5182 : loss : 0.013552, loss_ce: 0.005029
2022-01-08 11:56:52,587 iteration 5183 : loss : 0.013728, loss_ce: 0.006301
2022-01-08 11:56:54,114 iteration 5184 : loss : 0.018980, loss_ce: 0.006961
2022-01-08 11:56:54,115 Training Data Eval:
2022-01-08 11:57:01,998   Average segmentation loss on training set: 0.0096
2022-01-08 11:57:01,998 Validation Data Eval:
2022-01-08 11:57:04,714   Average segmentation loss on validation set: 0.0642
2022-01-08 11:57:06,261 iteration 5185 : loss : 0.019987, loss_ce: 0.007418
 76%|██████████████████████       | 305/400 [2:28:56<48:07, 30.40s/it]2022-01-08 11:57:07,890 iteration 5186 : loss : 0.015733, loss_ce: 0.007273
2022-01-08 11:57:09,405 iteration 5187 : loss : 0.015424, loss_ce: 0.005644
2022-01-08 11:57:10,928 iteration 5188 : loss : 0.012785, loss_ce: 0.005104
2022-01-08 11:57:12,455 iteration 5189 : loss : 0.014195, loss_ce: 0.006368
2022-01-08 11:57:13,966 iteration 5190 : loss : 0.016788, loss_ce: 0.006981
2022-01-08 11:57:15,583 iteration 5191 : loss : 0.017678, loss_ce: 0.005333
2022-01-08 11:57:17,179 iteration 5192 : loss : 0.015517, loss_ce: 0.005426
2022-01-08 11:57:18,693 iteration 5193 : loss : 0.010640, loss_ce: 0.003576
2022-01-08 11:57:20,290 iteration 5194 : loss : 0.021870, loss_ce: 0.007980
2022-01-08 11:57:21,823 iteration 5195 : loss : 0.015160, loss_ce: 0.004906
2022-01-08 11:57:23,284 iteration 5196 : loss : 0.012095, loss_ce: 0.003040
2022-01-08 11:57:24,786 iteration 5197 : loss : 0.011856, loss_ce: 0.003655
2022-01-08 11:57:26,306 iteration 5198 : loss : 0.014241, loss_ce: 0.006844
2022-01-08 11:57:27,901 iteration 5199 : loss : 0.012759, loss_ce: 0.004446
2022-01-08 11:57:29,459 iteration 5200 : loss : 0.017641, loss_ce: 0.005206
2022-01-08 11:57:31,044 iteration 5201 : loss : 0.018389, loss_ce: 0.006207
2022-01-08 11:57:32,691 iteration 5202 : loss : 0.019268, loss_ce: 0.009190
 76%|██████████████████████▏      | 306/400 [2:29:22<45:45, 29.21s/it]2022-01-08 11:57:34,265 iteration 5203 : loss : 0.011554, loss_ce: 0.004274
2022-01-08 11:57:35,807 iteration 5204 : loss : 0.023760, loss_ce: 0.010601
2022-01-08 11:57:37,494 iteration 5205 : loss : 0.021333, loss_ce: 0.008527
2022-01-08 11:57:39,051 iteration 5206 : loss : 0.018729, loss_ce: 0.006100
2022-01-08 11:57:40,616 iteration 5207 : loss : 0.020430, loss_ce: 0.007706
2022-01-08 11:57:42,248 iteration 5208 : loss : 0.017496, loss_ce: 0.006402
2022-01-08 11:57:43,758 iteration 5209 : loss : 0.016555, loss_ce: 0.006803
2022-01-08 11:57:45,360 iteration 5210 : loss : 0.017169, loss_ce: 0.006139
2022-01-08 11:57:46,936 iteration 5211 : loss : 0.013363, loss_ce: 0.005399
2022-01-08 11:57:48,500 iteration 5212 : loss : 0.013912, loss_ce: 0.004990
2022-01-08 11:57:49,955 iteration 5213 : loss : 0.012622, loss_ce: 0.004501
2022-01-08 11:57:51,603 iteration 5214 : loss : 0.025137, loss_ce: 0.010011
2022-01-08 11:57:53,192 iteration 5215 : loss : 0.017773, loss_ce: 0.005687
2022-01-08 11:57:54,805 iteration 5216 : loss : 0.024550, loss_ce: 0.008543
2022-01-08 11:57:56,451 iteration 5217 : loss : 0.021453, loss_ce: 0.008348
2022-01-08 11:57:57,972 iteration 5218 : loss : 0.020303, loss_ce: 0.003432
2022-01-08 11:57:59,623 iteration 5219 : loss : 0.023341, loss_ce: 0.008949
 77%|██████████████████████▎      | 307/400 [2:29:49<44:12, 28.53s/it]2022-01-08 11:58:01,212 iteration 5220 : loss : 0.038207, loss_ce: 0.012015
2022-01-08 11:58:02,765 iteration 5221 : loss : 0.017583, loss_ce: 0.006034
2022-01-08 11:58:04,268 iteration 5222 : loss : 0.015276, loss_ce: 0.005468
2022-01-08 11:58:05,871 iteration 5223 : loss : 0.018981, loss_ce: 0.007306
2022-01-08 11:58:07,418 iteration 5224 : loss : 0.018554, loss_ce: 0.006580
2022-01-08 11:58:09,030 iteration 5225 : loss : 0.018553, loss_ce: 0.006411
2022-01-08 11:58:10,578 iteration 5226 : loss : 0.019802, loss_ce: 0.007455
2022-01-08 11:58:12,223 iteration 5227 : loss : 0.019312, loss_ce: 0.008449
2022-01-08 11:58:13,750 iteration 5228 : loss : 0.014905, loss_ce: 0.005972
2022-01-08 11:58:15,429 iteration 5229 : loss : 0.015992, loss_ce: 0.006971
2022-01-08 11:58:16,886 iteration 5230 : loss : 0.011147, loss_ce: 0.004852
2022-01-08 11:58:18,467 iteration 5231 : loss : 0.022830, loss_ce: 0.008763
2022-01-08 11:58:20,035 iteration 5232 : loss : 0.018980, loss_ce: 0.005814
2022-01-08 11:58:21,635 iteration 5233 : loss : 0.018635, loss_ce: 0.007037
2022-01-08 11:58:23,168 iteration 5234 : loss : 0.013714, loss_ce: 0.005657
2022-01-08 11:58:24,741 iteration 5235 : loss : 0.014361, loss_ce: 0.004891
2022-01-08 11:58:26,347 iteration 5236 : loss : 0.034609, loss_ce: 0.013608
 77%|██████████████████████▎      | 308/400 [2:30:16<42:54, 27.98s/it]2022-01-08 11:58:27,830 iteration 5237 : loss : 0.011497, loss_ce: 0.004687
2022-01-08 11:58:29,429 iteration 5238 : loss : 0.013729, loss_ce: 0.005509
2022-01-08 11:58:31,011 iteration 5239 : loss : 0.016540, loss_ce: 0.005414
2022-01-08 11:58:32,622 iteration 5240 : loss : 0.171087, loss_ce: 0.002706
2022-01-08 11:58:34,155 iteration 5241 : loss : 0.014891, loss_ce: 0.006995
2022-01-08 11:58:35,716 iteration 5242 : loss : 0.016231, loss_ce: 0.007046
2022-01-08 11:58:37,365 iteration 5243 : loss : 0.018962, loss_ce: 0.005643
2022-01-08 11:58:38,962 iteration 5244 : loss : 0.014057, loss_ce: 0.005682
2022-01-08 11:58:40,539 iteration 5245 : loss : 0.019045, loss_ce: 0.006676
2022-01-08 11:58:42,126 iteration 5246 : loss : 0.029497, loss_ce: 0.008871
2022-01-08 11:58:43,614 iteration 5247 : loss : 0.011718, loss_ce: 0.004557
2022-01-08 11:58:45,213 iteration 5248 : loss : 0.026877, loss_ce: 0.007996
2022-01-08 11:58:46,862 iteration 5249 : loss : 0.021270, loss_ce: 0.008641
2022-01-08 11:58:48,458 iteration 5250 : loss : 0.018825, loss_ce: 0.009793
2022-01-08 11:58:49,947 iteration 5251 : loss : 0.014131, loss_ce: 0.004725
2022-01-08 11:58:51,517 iteration 5252 : loss : 0.014705, loss_ce: 0.005748
2022-01-08 11:58:53,111 iteration 5253 : loss : 0.032369, loss_ce: 0.006708
 77%|██████████████████████▍      | 309/400 [2:30:43<41:53, 27.62s/it]2022-01-08 11:58:54,706 iteration 5254 : loss : 0.012599, loss_ce: 0.004577
2022-01-08 11:58:56,264 iteration 5255 : loss : 0.018858, loss_ce: 0.009995
2022-01-08 11:58:57,763 iteration 5256 : loss : 0.013356, loss_ce: 0.005564
2022-01-08 11:58:59,332 iteration 5257 : loss : 0.016963, loss_ce: 0.007476
2022-01-08 11:59:00,934 iteration 5258 : loss : 0.015501, loss_ce: 0.004212
2022-01-08 11:59:02,447 iteration 5259 : loss : 0.018989, loss_ce: 0.005208
2022-01-08 11:59:04,022 iteration 5260 : loss : 0.016503, loss_ce: 0.006141
2022-01-08 11:59:05,607 iteration 5261 : loss : 0.021852, loss_ce: 0.009249
2022-01-08 11:59:07,174 iteration 5262 : loss : 0.011709, loss_ce: 0.004707
2022-01-08 11:59:08,747 iteration 5263 : loss : 0.016096, loss_ce: 0.006287
2022-01-08 11:59:10,259 iteration 5264 : loss : 0.012452, loss_ce: 0.005422
2022-01-08 11:59:11,861 iteration 5265 : loss : 0.020003, loss_ce: 0.007912
2022-01-08 11:59:13,511 iteration 5266 : loss : 0.016325, loss_ce: 0.005472
2022-01-08 11:59:15,077 iteration 5267 : loss : 0.027159, loss_ce: 0.007087
2022-01-08 11:59:16,657 iteration 5268 : loss : 0.019519, loss_ce: 0.005481
2022-01-08 11:59:18,335 iteration 5269 : loss : 0.018389, loss_ce: 0.004937
2022-01-08 11:59:18,335 Training Data Eval:
2022-01-08 11:59:26,217   Average segmentation loss on training set: 0.0100
2022-01-08 11:59:26,218 Validation Data Eval:
2022-01-08 11:59:28,932   Average segmentation loss on validation set: 0.0837
2022-01-08 11:59:30,490 iteration 5270 : loss : 0.015313, loss_ce: 0.006749
 78%|██████████████████████▍      | 310/400 [2:31:20<45:49, 30.54s/it]2022-01-08 11:59:32,063 iteration 5271 : loss : 0.015264, loss_ce: 0.006084
2022-01-08 11:59:33,651 iteration 5272 : loss : 0.022663, loss_ce: 0.012668
2022-01-08 11:59:35,184 iteration 5273 : loss : 0.017697, loss_ce: 0.004420
2022-01-08 11:59:36,727 iteration 5274 : loss : 0.016323, loss_ce: 0.006784
2022-01-08 11:59:38,213 iteration 5275 : loss : 0.011455, loss_ce: 0.005226
2022-01-08 11:59:39,799 iteration 5276 : loss : 0.019104, loss_ce: 0.007006
2022-01-08 11:59:41,365 iteration 5277 : loss : 0.015312, loss_ce: 0.005500
2022-01-08 11:59:42,843 iteration 5278 : loss : 0.016626, loss_ce: 0.006477
2022-01-08 11:59:44,402 iteration 5279 : loss : 0.022824, loss_ce: 0.010495
2022-01-08 11:59:46,094 iteration 5280 : loss : 0.021478, loss_ce: 0.007661
2022-01-08 11:59:47,638 iteration 5281 : loss : 0.013114, loss_ce: 0.004357
2022-01-08 11:59:49,135 iteration 5282 : loss : 0.012524, loss_ce: 0.003920
2022-01-08 11:59:50,701 iteration 5283 : loss : 0.024927, loss_ce: 0.008249
2022-01-08 11:59:52,283 iteration 5284 : loss : 0.032540, loss_ce: 0.010792
2022-01-08 11:59:53,839 iteration 5285 : loss : 0.013804, loss_ce: 0.006117
2022-01-08 11:59:55,319 iteration 5286 : loss : 0.012240, loss_ce: 0.005272
2022-01-08 11:59:56,863 iteration 5287 : loss : 0.013263, loss_ce: 0.002803
 78%|██████████████████████▌      | 311/400 [2:31:47<43:27, 29.29s/it]2022-01-08 11:59:58,495 iteration 5288 : loss : 0.018030, loss_ce: 0.006851
2022-01-08 12:00:00,090 iteration 5289 : loss : 0.033160, loss_ce: 0.018694
2022-01-08 12:00:01,642 iteration 5290 : loss : 0.019824, loss_ce: 0.005106
2022-01-08 12:00:03,185 iteration 5291 : loss : 0.018416, loss_ce: 0.006862
2022-01-08 12:00:04,791 iteration 5292 : loss : 0.019359, loss_ce: 0.007094
2022-01-08 12:00:06,370 iteration 5293 : loss : 0.020265, loss_ce: 0.010302
2022-01-08 12:00:07,975 iteration 5294 : loss : 0.017361, loss_ce: 0.006728
2022-01-08 12:00:09,630 iteration 5295 : loss : 0.020101, loss_ce: 0.007411
2022-01-08 12:00:11,239 iteration 5296 : loss : 0.021914, loss_ce: 0.007339
2022-01-08 12:00:12,847 iteration 5297 : loss : 0.016923, loss_ce: 0.005798
2022-01-08 12:00:14,460 iteration 5298 : loss : 0.021126, loss_ce: 0.007367
2022-01-08 12:00:16,068 iteration 5299 : loss : 0.019862, loss_ce: 0.007423
2022-01-08 12:00:17,694 iteration 5300 : loss : 0.018701, loss_ce: 0.006347
2022-01-08 12:00:19,198 iteration 5301 : loss : 0.015599, loss_ce: 0.005865
2022-01-08 12:00:20,704 iteration 5302 : loss : 0.013943, loss_ce: 0.007120
2022-01-08 12:00:22,219 iteration 5303 : loss : 0.016281, loss_ce: 0.006070
2022-01-08 12:00:23,713 iteration 5304 : loss : 0.017073, loss_ce: 0.006052
 78%|██████████████████████▌      | 312/400 [2:32:13<41:53, 28.56s/it]2022-01-08 12:00:25,338 iteration 5305 : loss : 0.019076, loss_ce: 0.006391
2022-01-08 12:00:27,001 iteration 5306 : loss : 0.018645, loss_ce: 0.007579
2022-01-08 12:00:28,694 iteration 5307 : loss : 0.016732, loss_ce: 0.005560
2022-01-08 12:00:30,352 iteration 5308 : loss : 0.027383, loss_ce: 0.008347
2022-01-08 12:00:31,993 iteration 5309 : loss : 0.022045, loss_ce: 0.007623
2022-01-08 12:00:33,543 iteration 5310 : loss : 0.015935, loss_ce: 0.007464
2022-01-08 12:00:35,077 iteration 5311 : loss : 0.031166, loss_ce: 0.007360
2022-01-08 12:00:36,720 iteration 5312 : loss : 0.019957, loss_ce: 0.008960
2022-01-08 12:00:38,412 iteration 5313 : loss : 0.023808, loss_ce: 0.006247
2022-01-08 12:00:40,004 iteration 5314 : loss : 0.017280, loss_ce: 0.006080
2022-01-08 12:00:41,650 iteration 5315 : loss : 0.032229, loss_ce: 0.006846
2022-01-08 12:00:43,275 iteration 5316 : loss : 0.014686, loss_ce: 0.006001
2022-01-08 12:00:44,840 iteration 5317 : loss : 0.015213, loss_ce: 0.006767
2022-01-08 12:00:46,373 iteration 5318 : loss : 0.013365, loss_ce: 0.005411
2022-01-08 12:00:48,001 iteration 5319 : loss : 0.016863, loss_ce: 0.006460
2022-01-08 12:00:49,610 iteration 5320 : loss : 0.031717, loss_ce: 0.015271
2022-01-08 12:00:51,251 iteration 5321 : loss : 0.024552, loss_ce: 0.009716
 78%|██████████████████████▋      | 313/400 [2:32:41<40:57, 28.25s/it]2022-01-08 12:00:52,786 iteration 5322 : loss : 0.012255, loss_ce: 0.004882
2022-01-08 12:00:54,372 iteration 5323 : loss : 0.019953, loss_ce: 0.007848
2022-01-08 12:00:55,875 iteration 5324 : loss : 0.016008, loss_ce: 0.005105
2022-01-08 12:00:57,408 iteration 5325 : loss : 0.028375, loss_ce: 0.008332
2022-01-08 12:00:59,009 iteration 5326 : loss : 0.026732, loss_ce: 0.010131
2022-01-08 12:01:00,538 iteration 5327 : loss : 0.016576, loss_ce: 0.006098
2022-01-08 12:01:02,084 iteration 5328 : loss : 0.016278, loss_ce: 0.008440
2022-01-08 12:01:03,693 iteration 5329 : loss : 0.019881, loss_ce: 0.006729
2022-01-08 12:01:05,282 iteration 5330 : loss : 0.018556, loss_ce: 0.006658
2022-01-08 12:01:06,845 iteration 5331 : loss : 0.016301, loss_ce: 0.005165
2022-01-08 12:01:08,418 iteration 5332 : loss : 0.029759, loss_ce: 0.008430
2022-01-08 12:01:10,026 iteration 5333 : loss : 0.018400, loss_ce: 0.008692
2022-01-08 12:01:11,614 iteration 5334 : loss : 0.021471, loss_ce: 0.008600
2022-01-08 12:01:13,224 iteration 5335 : loss : 0.019402, loss_ce: 0.005448
2022-01-08 12:01:14,807 iteration 5336 : loss : 0.014819, loss_ce: 0.005631
2022-01-08 12:01:16,340 iteration 5337 : loss : 0.013342, loss_ce: 0.005094
2022-01-08 12:01:17,872 iteration 5338 : loss : 0.018867, loss_ce: 0.008761
 78%|██████████████████████▊      | 314/400 [2:33:08<39:47, 27.76s/it]2022-01-08 12:01:19,632 iteration 5339 : loss : 0.022211, loss_ce: 0.008995
2022-01-08 12:01:21,255 iteration 5340 : loss : 0.025532, loss_ce: 0.014632
2022-01-08 12:01:22,851 iteration 5341 : loss : 0.016828, loss_ce: 0.003736
2022-01-08 12:01:24,434 iteration 5342 : loss : 0.013142, loss_ce: 0.006872
2022-01-08 12:01:25,977 iteration 5343 : loss : 0.013710, loss_ce: 0.004496
2022-01-08 12:01:27,562 iteration 5344 : loss : 0.016783, loss_ce: 0.005791
2022-01-08 12:01:29,123 iteration 5345 : loss : 0.015777, loss_ce: 0.005396
2022-01-08 12:01:30,734 iteration 5346 : loss : 0.020288, loss_ce: 0.007028
2022-01-08 12:01:32,350 iteration 5347 : loss : 0.031248, loss_ce: 0.012820
2022-01-08 12:01:33,976 iteration 5348 : loss : 0.015461, loss_ce: 0.004632
2022-01-08 12:01:35,570 iteration 5349 : loss : 0.023180, loss_ce: 0.008341
2022-01-08 12:01:37,077 iteration 5350 : loss : 0.014256, loss_ce: 0.006194
2022-01-08 12:01:38,587 iteration 5351 : loss : 0.018245, loss_ce: 0.007378
2022-01-08 12:01:40,199 iteration 5352 : loss : 0.025848, loss_ce: 0.010630
2022-01-08 12:01:41,824 iteration 5353 : loss : 0.011696, loss_ce: 0.004332
2022-01-08 12:01:43,369 iteration 5354 : loss : 0.016617, loss_ce: 0.007624
2022-01-08 12:01:43,369 Training Data Eval:
2022-01-08 12:01:51,253   Average segmentation loss on training set: 0.0100
2022-01-08 12:01:51,254 Validation Data Eval:
2022-01-08 12:01:53,974   Average segmentation loss on validation set: 0.0718
2022-01-08 12:01:55,481 iteration 5355 : loss : 0.015214, loss_ce: 0.005731
 79%|██████████████████████▊      | 315/400 [2:33:45<43:31, 30.72s/it]2022-01-08 12:01:57,101 iteration 5356 : loss : 0.017121, loss_ce: 0.008151
2022-01-08 12:01:58,602 iteration 5357 : loss : 0.014530, loss_ce: 0.003135
2022-01-08 12:02:00,303 iteration 5358 : loss : 0.029656, loss_ce: 0.012847
2022-01-08 12:02:01,801 iteration 5359 : loss : 0.011986, loss_ce: 0.004495
2022-01-08 12:02:03,387 iteration 5360 : loss : 0.016553, loss_ce: 0.006742
2022-01-08 12:02:04,981 iteration 5361 : loss : 0.031924, loss_ce: 0.009050
2022-01-08 12:02:06,591 iteration 5362 : loss : 0.048336, loss_ce: 0.018235
2022-01-08 12:02:08,171 iteration 5363 : loss : 0.014867, loss_ce: 0.005917
2022-01-08 12:02:09,749 iteration 5364 : loss : 0.019896, loss_ce: 0.004185
2022-01-08 12:02:11,306 iteration 5365 : loss : 0.014626, loss_ce: 0.005508
2022-01-08 12:02:12,904 iteration 5366 : loss : 0.016025, loss_ce: 0.005607
2022-01-08 12:02:14,531 iteration 5367 : loss : 0.022957, loss_ce: 0.010037
2022-01-08 12:02:16,093 iteration 5368 : loss : 0.011215, loss_ce: 0.004873
2022-01-08 12:02:17,688 iteration 5369 : loss : 0.014197, loss_ce: 0.005674
2022-01-08 12:02:19,249 iteration 5370 : loss : 0.010973, loss_ce: 0.004270
2022-01-08 12:02:20,829 iteration 5371 : loss : 0.021817, loss_ce: 0.010546
2022-01-08 12:02:22,371 iteration 5372 : loss : 0.023263, loss_ce: 0.007178
 79%|██████████████████████▉      | 316/400 [2:34:12<41:23, 29.57s/it]2022-01-08 12:02:23,988 iteration 5373 : loss : 0.014084, loss_ce: 0.006662
2022-01-08 12:02:25,526 iteration 5374 : loss : 0.015576, loss_ce: 0.007183
2022-01-08 12:02:27,124 iteration 5375 : loss : 0.011262, loss_ce: 0.003648
2022-01-08 12:02:28,677 iteration 5376 : loss : 0.015848, loss_ce: 0.005819
2022-01-08 12:02:30,214 iteration 5377 : loss : 0.015576, loss_ce: 0.004756
2022-01-08 12:02:31,792 iteration 5378 : loss : 0.019686, loss_ce: 0.007776
2022-01-08 12:02:33,297 iteration 5379 : loss : 0.010652, loss_ce: 0.003969
2022-01-08 12:02:34,866 iteration 5380 : loss : 0.014618, loss_ce: 0.005964
2022-01-08 12:02:36,401 iteration 5381 : loss : 0.014919, loss_ce: 0.005922
2022-01-08 12:02:37,994 iteration 5382 : loss : 0.013208, loss_ce: 0.003848
2022-01-08 12:02:39,656 iteration 5383 : loss : 0.021989, loss_ce: 0.011691
2022-01-08 12:02:41,384 iteration 5384 : loss : 0.024884, loss_ce: 0.009508
2022-01-08 12:02:42,931 iteration 5385 : loss : 0.013639, loss_ce: 0.005200
2022-01-08 12:02:44,418 iteration 5386 : loss : 0.012231, loss_ce: 0.004033
2022-01-08 12:02:45,957 iteration 5387 : loss : 0.014063, loss_ce: 0.006243
2022-01-08 12:02:47,567 iteration 5388 : loss : 0.014008, loss_ce: 0.004708
2022-01-08 12:02:49,245 iteration 5389 : loss : 0.029707, loss_ce: 0.010477
 79%|██████████████████████▉      | 317/400 [2:34:39<39:47, 28.76s/it]2022-01-08 12:02:50,836 iteration 5390 : loss : 0.014846, loss_ce: 0.005266
2022-01-08 12:02:52,453 iteration 5391 : loss : 0.015302, loss_ce: 0.005228
2022-01-08 12:02:54,077 iteration 5392 : loss : 0.016615, loss_ce: 0.008127
2022-01-08 12:02:55,652 iteration 5393 : loss : 0.021133, loss_ce: 0.007948
2022-01-08 12:02:57,191 iteration 5394 : loss : 0.015221, loss_ce: 0.006873
2022-01-08 12:02:58,784 iteration 5395 : loss : 0.016197, loss_ce: 0.005146
2022-01-08 12:03:00,304 iteration 5396 : loss : 0.015145, loss_ce: 0.008581
2022-01-08 12:03:01,930 iteration 5397 : loss : 0.019105, loss_ce: 0.006366
2022-01-08 12:03:03,515 iteration 5398 : loss : 0.015356, loss_ce: 0.006761
2022-01-08 12:03:05,018 iteration 5399 : loss : 0.021637, loss_ce: 0.005760
2022-01-08 12:03:06,672 iteration 5400 : loss : 0.028746, loss_ce: 0.007285
2022-01-08 12:03:08,276 iteration 5401 : loss : 0.024027, loss_ce: 0.011008
2022-01-08 12:03:09,849 iteration 5402 : loss : 0.015725, loss_ce: 0.005202
2022-01-08 12:03:11,387 iteration 5403 : loss : 0.018211, loss_ce: 0.007015
2022-01-08 12:03:12,910 iteration 5404 : loss : 0.014328, loss_ce: 0.004159
2022-01-08 12:03:14,449 iteration 5405 : loss : 0.011338, loss_ce: 0.004851
2022-01-08 12:03:16,066 iteration 5406 : loss : 0.016916, loss_ce: 0.004893
 80%|███████████████████████      | 318/400 [2:35:06<38:30, 28.18s/it]2022-01-08 12:03:17,713 iteration 5407 : loss : 0.018239, loss_ce: 0.006072
2022-01-08 12:03:19,285 iteration 5408 : loss : 0.033056, loss_ce: 0.010232
2022-01-08 12:03:20,929 iteration 5409 : loss : 0.019705, loss_ce: 0.008735
2022-01-08 12:03:22,469 iteration 5410 : loss : 0.015146, loss_ce: 0.006808
2022-01-08 12:03:24,034 iteration 5411 : loss : 0.014960, loss_ce: 0.005580
2022-01-08 12:03:25,617 iteration 5412 : loss : 0.015796, loss_ce: 0.007120
2022-01-08 12:03:27,161 iteration 5413 : loss : 0.014675, loss_ce: 0.006844
2022-01-08 12:03:28,739 iteration 5414 : loss : 0.014495, loss_ce: 0.005854
2022-01-08 12:03:30,390 iteration 5415 : loss : 0.032785, loss_ce: 0.011474
2022-01-08 12:03:31,901 iteration 5416 : loss : 0.011187, loss_ce: 0.003903
2022-01-08 12:03:33,503 iteration 5417 : loss : 0.013982, loss_ce: 0.004474
2022-01-08 12:03:35,056 iteration 5418 : loss : 0.019940, loss_ce: 0.007750
2022-01-08 12:03:36,653 iteration 5419 : loss : 0.023729, loss_ce: 0.005679
2022-01-08 12:03:38,286 iteration 5420 : loss : 0.016687, loss_ce: 0.006146
2022-01-08 12:03:39,878 iteration 5421 : loss : 0.021083, loss_ce: 0.009692
2022-01-08 12:03:41,334 iteration 5422 : loss : 0.012764, loss_ce: 0.005636
2022-01-08 12:03:42,930 iteration 5423 : loss : 0.024364, loss_ce: 0.005324
 80%|███████████████████████▏     | 319/400 [2:35:33<37:30, 27.78s/it]2022-01-08 12:03:44,601 iteration 5424 : loss : 0.017877, loss_ce: 0.006215
2022-01-08 12:03:46,132 iteration 5425 : loss : 0.012955, loss_ce: 0.004032
2022-01-08 12:03:47,690 iteration 5426 : loss : 0.019186, loss_ce: 0.006051
2022-01-08 12:03:49,170 iteration 5427 : loss : 0.018490, loss_ce: 0.007081
2022-01-08 12:03:50,705 iteration 5428 : loss : 0.016362, loss_ce: 0.006521
2022-01-08 12:03:52,286 iteration 5429 : loss : 0.012914, loss_ce: 0.004434
2022-01-08 12:03:53,857 iteration 5430 : loss : 0.015365, loss_ce: 0.005966
2022-01-08 12:03:55,364 iteration 5431 : loss : 0.016678, loss_ce: 0.005835
2022-01-08 12:03:56,996 iteration 5432 : loss : 0.017480, loss_ce: 0.008109
2022-01-08 12:03:58,618 iteration 5433 : loss : 0.017471, loss_ce: 0.006830
2022-01-08 12:04:00,135 iteration 5434 : loss : 0.018516, loss_ce: 0.008235
2022-01-08 12:04:01,770 iteration 5435 : loss : 0.017708, loss_ce: 0.007156
2022-01-08 12:04:03,363 iteration 5436 : loss : 0.013818, loss_ce: 0.005026
2022-01-08 12:04:04,978 iteration 5437 : loss : 0.030062, loss_ce: 0.006767
2022-01-08 12:04:06,594 iteration 5438 : loss : 0.017733, loss_ce: 0.007200
2022-01-08 12:04:08,145 iteration 5439 : loss : 0.013703, loss_ce: 0.005558
2022-01-08 12:04:08,145 Training Data Eval:
2022-01-08 12:04:16,029   Average segmentation loss on training set: 0.0092
2022-01-08 12:04:16,029 Validation Data Eval:
2022-01-08 12:04:18,742   Average segmentation loss on validation set: 0.0701
2022-01-08 12:04:20,252 iteration 5440 : loss : 0.015688, loss_ce: 0.005024
 80%|███████████████████████▏     | 320/400 [2:36:10<40:51, 30.65s/it]2022-01-08 12:04:21,832 iteration 5441 : loss : 0.011933, loss_ce: 0.003865
2022-01-08 12:04:23,344 iteration 5442 : loss : 0.015461, loss_ce: 0.005360
2022-01-08 12:04:24,895 iteration 5443 : loss : 0.013210, loss_ce: 0.004095
2022-01-08 12:04:26,379 iteration 5444 : loss : 0.016091, loss_ce: 0.006132
2022-01-08 12:04:27,950 iteration 5445 : loss : 0.018516, loss_ce: 0.006411
2022-01-08 12:04:29,471 iteration 5446 : loss : 0.015026, loss_ce: 0.007157
2022-01-08 12:04:31,116 iteration 5447 : loss : 0.020644, loss_ce: 0.007037
2022-01-08 12:04:32,631 iteration 5448 : loss : 0.014708, loss_ce: 0.005090
2022-01-08 12:04:34,206 iteration 5449 : loss : 0.022307, loss_ce: 0.012919
2022-01-08 12:04:35,767 iteration 5450 : loss : 0.017858, loss_ce: 0.007186
2022-01-08 12:04:37,268 iteration 5451 : loss : 0.015948, loss_ce: 0.004990
2022-01-08 12:04:38,842 iteration 5452 : loss : 0.019827, loss_ce: 0.006657
2022-01-08 12:04:40,357 iteration 5453 : loss : 0.015295, loss_ce: 0.006203
2022-01-08 12:04:41,974 iteration 5454 : loss : 0.023147, loss_ce: 0.013512
2022-01-08 12:04:43,564 iteration 5455 : loss : 0.020371, loss_ce: 0.007667
2022-01-08 12:04:45,133 iteration 5456 : loss : 0.012626, loss_ce: 0.002789
2022-01-08 12:04:46,653 iteration 5457 : loss : 0.012500, loss_ce: 0.003723
 80%|███████████████████████▎     | 321/400 [2:36:36<38:40, 29.37s/it]2022-01-08 12:04:48,236 iteration 5458 : loss : 0.014939, loss_ce: 0.005316
2022-01-08 12:04:49,741 iteration 5459 : loss : 0.015552, loss_ce: 0.007949
2022-01-08 12:04:51,301 iteration 5460 : loss : 0.012931, loss_ce: 0.005169
2022-01-08 12:04:52,856 iteration 5461 : loss : 0.015615, loss_ce: 0.006253
2022-01-08 12:04:54,402 iteration 5462 : loss : 0.019162, loss_ce: 0.007097
2022-01-08 12:04:55,980 iteration 5463 : loss : 0.020113, loss_ce: 0.005606
2022-01-08 12:04:57,554 iteration 5464 : loss : 0.022814, loss_ce: 0.010170
2022-01-08 12:04:59,187 iteration 5465 : loss : 0.019083, loss_ce: 0.006709
2022-01-08 12:05:00,810 iteration 5466 : loss : 0.016785, loss_ce: 0.006251
2022-01-08 12:05:02,403 iteration 5467 : loss : 0.025936, loss_ce: 0.011590
2022-01-08 12:05:03,990 iteration 5468 : loss : 0.015401, loss_ce: 0.006219
2022-01-08 12:05:05,489 iteration 5469 : loss : 0.012565, loss_ce: 0.004245
2022-01-08 12:05:07,039 iteration 5470 : loss : 0.010159, loss_ce: 0.003958
2022-01-08 12:05:08,590 iteration 5471 : loss : 0.017424, loss_ce: 0.004385
2022-01-08 12:05:10,177 iteration 5472 : loss : 0.023445, loss_ce: 0.008192
2022-01-08 12:05:11,682 iteration 5473 : loss : 0.012939, loss_ce: 0.004609
2022-01-08 12:05:13,200 iteration 5474 : loss : 0.012552, loss_ce: 0.004112
 80%|███████████████████████▎     | 322/400 [2:37:03<37:04, 28.52s/it]2022-01-08 12:05:14,820 iteration 5475 : loss : 0.018314, loss_ce: 0.004665
2022-01-08 12:05:16,439 iteration 5476 : loss : 0.017296, loss_ce: 0.007704
2022-01-08 12:05:18,001 iteration 5477 : loss : 0.022765, loss_ce: 0.003254
2022-01-08 12:05:19,487 iteration 5478 : loss : 0.013679, loss_ce: 0.004249
2022-01-08 12:05:21,040 iteration 5479 : loss : 0.013068, loss_ce: 0.005427
2022-01-08 12:05:22,592 iteration 5480 : loss : 0.013983, loss_ce: 0.004379
2022-01-08 12:05:24,123 iteration 5481 : loss : 0.015971, loss_ce: 0.005582
2022-01-08 12:05:25,772 iteration 5482 : loss : 0.023997, loss_ce: 0.005927
2022-01-08 12:05:27,257 iteration 5483 : loss : 0.012931, loss_ce: 0.005084
2022-01-08 12:05:28,833 iteration 5484 : loss : 0.030935, loss_ce: 0.009938
2022-01-08 12:05:30,392 iteration 5485 : loss : 0.013541, loss_ce: 0.006183
2022-01-08 12:05:31,948 iteration 5486 : loss : 0.021908, loss_ce: 0.009467
2022-01-08 12:05:33,525 iteration 5487 : loss : 0.014950, loss_ce: 0.005363
2022-01-08 12:05:35,053 iteration 5488 : loss : 0.012178, loss_ce: 0.005639
2022-01-08 12:05:36,571 iteration 5489 : loss : 0.011707, loss_ce: 0.006102
2022-01-08 12:05:38,208 iteration 5490 : loss : 0.024977, loss_ce: 0.009805
2022-01-08 12:05:39,740 iteration 5491 : loss : 0.012256, loss_ce: 0.003716
 81%|███████████████████████▍     | 323/400 [2:37:30<35:50, 27.93s/it]2022-01-08 12:05:41,257 iteration 5492 : loss : 0.014439, loss_ce: 0.005606
2022-01-08 12:05:42,847 iteration 5493 : loss : 0.017305, loss_ce: 0.005260
2022-01-08 12:05:44,365 iteration 5494 : loss : 0.016735, loss_ce: 0.004902
2022-01-08 12:05:45,927 iteration 5495 : loss : 0.011505, loss_ce: 0.003341
2022-01-08 12:05:47,503 iteration 5496 : loss : 0.014734, loss_ce: 0.004039
2022-01-08 12:05:49,082 iteration 5497 : loss : 0.013550, loss_ce: 0.005264
2022-01-08 12:05:50,695 iteration 5498 : loss : 0.022790, loss_ce: 0.007935
2022-01-08 12:05:52,300 iteration 5499 : loss : 0.016041, loss_ce: 0.008630
2022-01-08 12:05:53,806 iteration 5500 : loss : 0.013903, loss_ce: 0.006257
2022-01-08 12:05:55,377 iteration 5501 : loss : 0.021635, loss_ce: 0.010138
2022-01-08 12:05:56,898 iteration 5502 : loss : 0.010017, loss_ce: 0.004320
2022-01-08 12:05:58,455 iteration 5503 : loss : 0.017150, loss_ce: 0.004391
2022-01-08 12:05:59,980 iteration 5504 : loss : 0.013790, loss_ce: 0.005243
2022-01-08 12:06:01,539 iteration 5505 : loss : 0.013977, loss_ce: 0.005422
2022-01-08 12:06:03,130 iteration 5506 : loss : 0.015509, loss_ce: 0.005601
2022-01-08 12:06:04,681 iteration 5507 : loss : 0.018472, loss_ce: 0.008254
2022-01-08 12:06:06,253 iteration 5508 : loss : 0.024818, loss_ce: 0.007216
 81%|███████████████████████▍     | 324/400 [2:37:56<34:50, 27.51s/it]2022-01-08 12:06:07,861 iteration 5509 : loss : 0.016657, loss_ce: 0.004944
2022-01-08 12:06:09,516 iteration 5510 : loss : 0.013380, loss_ce: 0.004339
2022-01-08 12:06:11,069 iteration 5511 : loss : 0.013694, loss_ce: 0.005148
2022-01-08 12:06:12,691 iteration 5512 : loss : 0.018046, loss_ce: 0.007673
2022-01-08 12:06:14,232 iteration 5513 : loss : 0.012620, loss_ce: 0.005463
2022-01-08 12:06:15,728 iteration 5514 : loss : 0.009679, loss_ce: 0.003202
2022-01-08 12:06:17,347 iteration 5515 : loss : 0.027002, loss_ce: 0.006012
2022-01-08 12:06:18,888 iteration 5516 : loss : 0.016738, loss_ce: 0.006761
2022-01-08 12:06:20,460 iteration 5517 : loss : 0.023281, loss_ce: 0.010180
2022-01-08 12:06:22,028 iteration 5518 : loss : 0.025216, loss_ce: 0.008152
2022-01-08 12:06:23,612 iteration 5519 : loss : 0.027125, loss_ce: 0.009551
2022-01-08 12:06:25,179 iteration 5520 : loss : 0.013796, loss_ce: 0.005603
2022-01-08 12:06:26,768 iteration 5521 : loss : 0.013906, loss_ce: 0.005138
2022-01-08 12:06:28,335 iteration 5522 : loss : 0.016980, loss_ce: 0.005992
2022-01-08 12:06:29,878 iteration 5523 : loss : 0.010247, loss_ce: 0.003592
2022-01-08 12:06:31,427 iteration 5524 : loss : 0.020049, loss_ce: 0.009608
2022-01-08 12:06:31,427 Training Data Eval:
2022-01-08 12:06:39,314   Average segmentation loss on training set: 0.0096
2022-01-08 12:06:39,314 Validation Data Eval:
2022-01-08 12:06:42,030   Average segmentation loss on validation set: 0.0670
2022-01-08 12:06:43,583 iteration 5525 : loss : 0.016517, loss_ce: 0.007773
 81%|███████████████████████▌     | 325/400 [2:38:33<38:03, 30.45s/it]2022-01-08 12:06:45,229 iteration 5526 : loss : 0.016492, loss_ce: 0.006399
2022-01-08 12:06:46,801 iteration 5527 : loss : 0.015440, loss_ce: 0.006445
2022-01-08 12:06:48,360 iteration 5528 : loss : 0.016894, loss_ce: 0.005814
2022-01-08 12:06:49,955 iteration 5529 : loss : 0.021461, loss_ce: 0.009409
2022-01-08 12:06:51,463 iteration 5530 : loss : 0.019460, loss_ce: 0.008514
2022-01-08 12:06:52,999 iteration 5531 : loss : 0.015254, loss_ce: 0.006551
2022-01-08 12:06:54,515 iteration 5532 : loss : 0.010785, loss_ce: 0.004293
2022-01-08 12:06:56,139 iteration 5533 : loss : 0.017870, loss_ce: 0.004751
2022-01-08 12:06:57,702 iteration 5534 : loss : 0.012625, loss_ce: 0.004507
2022-01-08 12:06:59,329 iteration 5535 : loss : 0.017119, loss_ce: 0.006268
2022-01-08 12:07:00,988 iteration 5536 : loss : 0.012403, loss_ce: 0.005197
2022-01-08 12:07:02,593 iteration 5537 : loss : 0.020497, loss_ce: 0.006072
2022-01-08 12:07:04,171 iteration 5538 : loss : 0.044855, loss_ce: 0.021759
2022-01-08 12:07:05,836 iteration 5539 : loss : 0.014729, loss_ce: 0.002995
2022-01-08 12:07:07,373 iteration 5540 : loss : 0.013389, loss_ce: 0.005960
2022-01-08 12:07:08,947 iteration 5541 : loss : 0.018792, loss_ce: 0.008117
2022-01-08 12:07:10,492 iteration 5542 : loss : 0.012536, loss_ce: 0.004398
 82%|███████████████████████▋     | 326/400 [2:39:00<36:14, 29.39s/it]2022-01-08 12:07:12,125 iteration 5543 : loss : 0.016011, loss_ce: 0.005185
2022-01-08 12:07:13,701 iteration 5544 : loss : 0.014507, loss_ce: 0.005786
2022-01-08 12:07:15,243 iteration 5545 : loss : 0.015327, loss_ce: 0.005948
2022-01-08 12:07:16,758 iteration 5546 : loss : 0.016083, loss_ce: 0.005058
2022-01-08 12:07:18,321 iteration 5547 : loss : 0.020566, loss_ce: 0.010563
2022-01-08 12:07:19,940 iteration 5548 : loss : 0.029219, loss_ce: 0.008130
2022-01-08 12:07:21,446 iteration 5549 : loss : 0.014626, loss_ce: 0.006060
2022-01-08 12:07:22,986 iteration 5550 : loss : 0.016229, loss_ce: 0.005938
2022-01-08 12:07:24,548 iteration 5551 : loss : 0.016317, loss_ce: 0.006922
2022-01-08 12:07:26,054 iteration 5552 : loss : 0.012311, loss_ce: 0.004826
2022-01-08 12:07:27,632 iteration 5553 : loss : 0.023811, loss_ce: 0.010024
2022-01-08 12:07:29,244 iteration 5554 : loss : 0.021587, loss_ce: 0.006733
2022-01-08 12:07:30,852 iteration 5555 : loss : 0.031956, loss_ce: 0.008317
2022-01-08 12:07:32,348 iteration 5556 : loss : 0.014745, loss_ce: 0.006181
2022-01-08 12:07:33,970 iteration 5557 : loss : 0.021909, loss_ce: 0.006410
2022-01-08 12:07:35,545 iteration 5558 : loss : 0.024665, loss_ce: 0.007164
2022-01-08 12:07:37,020 iteration 5559 : loss : 0.012056, loss_ce: 0.004692
 82%|███████████████████████▋     | 327/400 [2:39:27<34:42, 28.53s/it]2022-01-08 12:07:38,795 iteration 5560 : loss : 0.027532, loss_ce: 0.010445
2022-01-08 12:07:40,349 iteration 5561 : loss : 0.017901, loss_ce: 0.005817
2022-01-08 12:07:41,903 iteration 5562 : loss : 0.017319, loss_ce: 0.004700
2022-01-08 12:07:43,461 iteration 5563 : loss : 0.015823, loss_ce: 0.008247
2022-01-08 12:07:45,112 iteration 5564 : loss : 0.026310, loss_ce: 0.009592
2022-01-08 12:07:46,684 iteration 5565 : loss : 0.017971, loss_ce: 0.006845
2022-01-08 12:07:48,296 iteration 5566 : loss : 0.015905, loss_ce: 0.005771
2022-01-08 12:07:49,958 iteration 5567 : loss : 0.015948, loss_ce: 0.008031
2022-01-08 12:07:51,527 iteration 5568 : loss : 0.019841, loss_ce: 0.007465
2022-01-08 12:07:53,141 iteration 5569 : loss : 0.025835, loss_ce: 0.012054
2022-01-08 12:07:54,726 iteration 5570 : loss : 0.014304, loss_ce: 0.004597
2022-01-08 12:07:56,355 iteration 5571 : loss : 0.017966, loss_ce: 0.007060
2022-01-08 12:07:57,924 iteration 5572 : loss : 0.016559, loss_ce: 0.007050
2022-01-08 12:07:59,478 iteration 5573 : loss : 0.016494, loss_ce: 0.005647
2022-01-08 12:08:01,149 iteration 5574 : loss : 0.019184, loss_ce: 0.006913
2022-01-08 12:08:02,611 iteration 5575 : loss : 0.012646, loss_ce: 0.005407
2022-01-08 12:08:04,054 iteration 5576 : loss : 0.013404, loss_ce: 0.004509
 82%|███████████████████████▊     | 328/400 [2:39:54<33:41, 28.08s/it]2022-01-08 12:08:05,651 iteration 5577 : loss : 0.015661, loss_ce: 0.005567
2022-01-08 12:08:07,180 iteration 5578 : loss : 0.015529, loss_ce: 0.004238
2022-01-08 12:08:08,744 iteration 5579 : loss : 0.018405, loss_ce: 0.005620
2022-01-08 12:08:10,363 iteration 5580 : loss : 0.013317, loss_ce: 0.004768
2022-01-08 12:08:11,918 iteration 5581 : loss : 0.019557, loss_ce: 0.006910
2022-01-08 12:08:13,530 iteration 5582 : loss : 0.016317, loss_ce: 0.004783
2022-01-08 12:08:15,085 iteration 5583 : loss : 0.015482, loss_ce: 0.004290
2022-01-08 12:08:16,751 iteration 5584 : loss : 0.017116, loss_ce: 0.007396
2022-01-08 12:08:18,301 iteration 5585 : loss : 0.020167, loss_ce: 0.007406
2022-01-08 12:08:19,872 iteration 5586 : loss : 0.042049, loss_ce: 0.019812
2022-01-08 12:08:21,448 iteration 5587 : loss : 0.022245, loss_ce: 0.011632
2022-01-08 12:08:23,100 iteration 5588 : loss : 0.020387, loss_ce: 0.007524
2022-01-08 12:08:24,725 iteration 5589 : loss : 0.020471, loss_ce: 0.008306
2022-01-08 12:08:26,309 iteration 5590 : loss : 0.019400, loss_ce: 0.007417
2022-01-08 12:08:27,886 iteration 5591 : loss : 0.015111, loss_ce: 0.006915
2022-01-08 12:08:29,441 iteration 5592 : loss : 0.015505, loss_ce: 0.005547
2022-01-08 12:08:30,998 iteration 5593 : loss : 0.013985, loss_ce: 0.007921
 82%|███████████████████████▊     | 329/400 [2:40:21<32:49, 27.74s/it]2022-01-08 12:08:32,640 iteration 5594 : loss : 0.023691, loss_ce: 0.011394
2022-01-08 12:08:34,184 iteration 5595 : loss : 0.017858, loss_ce: 0.008546
2022-01-08 12:08:35,733 iteration 5596 : loss : 0.010905, loss_ce: 0.003242
2022-01-08 12:08:37,321 iteration 5597 : loss : 0.015823, loss_ce: 0.005529
2022-01-08 12:08:38,962 iteration 5598 : loss : 0.020033, loss_ce: 0.007442
2022-01-08 12:08:40,574 iteration 5599 : loss : 0.013352, loss_ce: 0.004883
2022-01-08 12:08:42,180 iteration 5600 : loss : 0.014138, loss_ce: 0.006409
2022-01-08 12:08:43,785 iteration 5601 : loss : 0.017376, loss_ce: 0.005039
2022-01-08 12:08:45,318 iteration 5602 : loss : 0.018568, loss_ce: 0.004375
2022-01-08 12:08:46,934 iteration 5603 : loss : 0.021255, loss_ce: 0.009121
2022-01-08 12:08:48,473 iteration 5604 : loss : 0.012697, loss_ce: 0.005603
2022-01-08 12:08:50,012 iteration 5605 : loss : 0.013440, loss_ce: 0.004353
2022-01-08 12:08:51,647 iteration 5606 : loss : 0.023398, loss_ce: 0.009742
2022-01-08 12:08:53,217 iteration 5607 : loss : 0.015489, loss_ce: 0.005534
2022-01-08 12:08:54,907 iteration 5608 : loss : 0.020244, loss_ce: 0.009651
2022-01-08 12:08:56,444 iteration 5609 : loss : 0.015261, loss_ce: 0.006643
2022-01-08 12:08:56,445 Training Data Eval:
2022-01-08 12:09:04,331   Average segmentation loss on training set: 0.0092
2022-01-08 12:09:04,331 Validation Data Eval:
2022-01-08 12:09:07,043   Average segmentation loss on validation set: 0.0697
2022-01-08 12:09:08,633 iteration 5610 : loss : 0.015680, loss_ce: 0.006221
 82%|███████████████████████▉     | 330/400 [2:40:58<35:49, 30.71s/it]2022-01-08 12:09:10,287 iteration 5611 : loss : 0.015070, loss_ce: 0.006573
2022-01-08 12:09:11,865 iteration 5612 : loss : 0.014295, loss_ce: 0.004842
2022-01-08 12:09:13,426 iteration 5613 : loss : 0.022400, loss_ce: 0.009371
2022-01-08 12:09:14,986 iteration 5614 : loss : 0.020128, loss_ce: 0.006822
2022-01-08 12:09:16,580 iteration 5615 : loss : 0.016982, loss_ce: 0.005694
2022-01-08 12:09:18,191 iteration 5616 : loss : 0.016371, loss_ce: 0.006001
2022-01-08 12:09:19,762 iteration 5617 : loss : 0.010878, loss_ce: 0.003563
2022-01-08 12:09:21,277 iteration 5618 : loss : 0.015258, loss_ce: 0.006580
2022-01-08 12:09:22,883 iteration 5619 : loss : 0.023490, loss_ce: 0.008378
2022-01-08 12:09:24,505 iteration 5620 : loss : 0.019354, loss_ce: 0.005183
2022-01-08 12:09:26,042 iteration 5621 : loss : 0.027254, loss_ce: 0.016803
2022-01-08 12:09:27,690 iteration 5622 : loss : 0.017586, loss_ce: 0.007454
2022-01-08 12:09:29,224 iteration 5623 : loss : 0.024403, loss_ce: 0.007599
2022-01-08 12:09:30,821 iteration 5624 : loss : 0.014957, loss_ce: 0.006258
2022-01-08 12:09:32,436 iteration 5625 : loss : 0.017444, loss_ce: 0.007349
2022-01-08 12:09:34,014 iteration 5626 : loss : 0.016143, loss_ce: 0.006255
2022-01-08 12:09:35,577 iteration 5627 : loss : 0.021284, loss_ce: 0.006233
 83%|███████████████████████▉     | 331/400 [2:41:25<34:00, 29.58s/it]2022-01-08 12:09:37,089 iteration 5628 : loss : 0.012775, loss_ce: 0.004629
2022-01-08 12:09:38,688 iteration 5629 : loss : 0.015759, loss_ce: 0.006785
2022-01-08 12:09:40,256 iteration 5630 : loss : 0.024005, loss_ce: 0.008548
2022-01-08 12:09:41,935 iteration 5631 : loss : 0.028152, loss_ce: 0.015568
2022-01-08 12:09:43,435 iteration 5632 : loss : 0.015092, loss_ce: 0.006443
2022-01-08 12:09:45,063 iteration 5633 : loss : 0.024881, loss_ce: 0.009472
2022-01-08 12:09:46,597 iteration 5634 : loss : 0.017847, loss_ce: 0.006308
2022-01-08 12:09:48,198 iteration 5635 : loss : 0.018673, loss_ce: 0.004205
2022-01-08 12:09:49,715 iteration 5636 : loss : 0.013050, loss_ce: 0.006195
2022-01-08 12:09:51,313 iteration 5637 : loss : 0.018931, loss_ce: 0.007771
2022-01-08 12:09:52,776 iteration 5638 : loss : 0.012591, loss_ce: 0.003877
2022-01-08 12:09:54,293 iteration 5639 : loss : 0.016381, loss_ce: 0.005434
2022-01-08 12:09:55,848 iteration 5640 : loss : 0.013328, loss_ce: 0.005962
2022-01-08 12:09:57,460 iteration 5641 : loss : 0.014505, loss_ce: 0.007795
2022-01-08 12:09:59,016 iteration 5642 : loss : 0.019258, loss_ce: 0.004006
2022-01-08 12:10:00,636 iteration 5643 : loss : 0.022254, loss_ce: 0.005280
2022-01-08 12:10:02,235 iteration 5644 : loss : 0.016312, loss_ce: 0.006258
 83%|████████████████████████     | 332/400 [2:41:52<32:31, 28.71s/it]2022-01-08 12:10:03,948 iteration 5645 : loss : 0.022647, loss_ce: 0.009721
2022-01-08 12:10:05,485 iteration 5646 : loss : 0.015555, loss_ce: 0.005720
2022-01-08 12:10:07,050 iteration 5647 : loss : 0.015101, loss_ce: 0.007268
2022-01-08 12:10:08,660 iteration 5648 : loss : 0.018362, loss_ce: 0.005450
2022-01-08 12:10:10,268 iteration 5649 : loss : 0.021485, loss_ce: 0.009755
2022-01-08 12:10:11,847 iteration 5650 : loss : 0.013478, loss_ce: 0.005371
2022-01-08 12:10:13,405 iteration 5651 : loss : 0.018261, loss_ce: 0.008638
2022-01-08 12:10:14,899 iteration 5652 : loss : 0.014300, loss_ce: 0.005686
2022-01-08 12:10:16,447 iteration 5653 : loss : 0.014672, loss_ce: 0.005404
2022-01-08 12:10:17,958 iteration 5654 : loss : 0.012790, loss_ce: 0.003689
2022-01-08 12:10:19,552 iteration 5655 : loss : 0.029085, loss_ce: 0.010594
2022-01-08 12:10:21,112 iteration 5656 : loss : 0.013261, loss_ce: 0.004421
2022-01-08 12:10:22,680 iteration 5657 : loss : 0.012990, loss_ce: 0.005036
2022-01-08 12:10:24,245 iteration 5658 : loss : 0.015403, loss_ce: 0.008368
2022-01-08 12:10:25,787 iteration 5659 : loss : 0.015074, loss_ce: 0.004215
2022-01-08 12:10:27,296 iteration 5660 : loss : 0.012161, loss_ce: 0.004212
2022-01-08 12:10:28,949 iteration 5661 : loss : 0.016491, loss_ce: 0.005849
 83%|████████████████████████▏    | 333/400 [2:42:19<31:23, 28.11s/it]2022-01-08 12:10:30,560 iteration 5662 : loss : 0.025157, loss_ce: 0.006091
2022-01-08 12:10:32,122 iteration 5663 : loss : 0.018610, loss_ce: 0.007281
2022-01-08 12:10:33,709 iteration 5664 : loss : 0.019588, loss_ce: 0.005490
2022-01-08 12:10:35,194 iteration 5665 : loss : 0.012061, loss_ce: 0.004436
2022-01-08 12:10:36,784 iteration 5666 : loss : 0.013591, loss_ce: 0.004906
2022-01-08 12:10:38,395 iteration 5667 : loss : 0.015193, loss_ce: 0.006299
2022-01-08 12:10:39,958 iteration 5668 : loss : 0.014719, loss_ce: 0.006666
2022-01-08 12:10:41,565 iteration 5669 : loss : 0.020791, loss_ce: 0.008987
2022-01-08 12:10:43,146 iteration 5670 : loss : 0.013189, loss_ce: 0.005089
2022-01-08 12:10:44,724 iteration 5671 : loss : 0.013793, loss_ce: 0.004677
2022-01-08 12:10:46,288 iteration 5672 : loss : 0.016886, loss_ce: 0.004572
2022-01-08 12:10:47,816 iteration 5673 : loss : 0.014626, loss_ce: 0.004575
2022-01-08 12:10:49,452 iteration 5674 : loss : 0.013034, loss_ce: 0.006517
2022-01-08 12:10:50,991 iteration 5675 : loss : 0.014670, loss_ce: 0.006456
2022-01-08 12:10:52,618 iteration 5676 : loss : 0.015062, loss_ce: 0.005171
2022-01-08 12:10:54,229 iteration 5677 : loss : 0.012730, loss_ce: 0.005720
2022-01-08 12:10:55,815 iteration 5678 : loss : 0.028966, loss_ce: 0.010198
 84%|████████████████████████▏    | 334/400 [2:42:46<30:30, 27.73s/it]2022-01-08 12:10:57,442 iteration 5679 : loss : 0.016876, loss_ce: 0.006698
2022-01-08 12:10:58,908 iteration 5680 : loss : 0.012416, loss_ce: 0.003259
2022-01-08 12:11:00,520 iteration 5681 : loss : 0.021740, loss_ce: 0.007278
2022-01-08 12:11:02,136 iteration 5682 : loss : 0.018582, loss_ce: 0.007822
2022-01-08 12:11:03,676 iteration 5683 : loss : 0.011283, loss_ce: 0.003772
2022-01-08 12:11:05,200 iteration 5684 : loss : 0.011663, loss_ce: 0.006331
2022-01-08 12:11:06,785 iteration 5685 : loss : 0.021580, loss_ce: 0.010306
2022-01-08 12:11:08,331 iteration 5686 : loss : 0.014418, loss_ce: 0.005585
2022-01-08 12:11:09,893 iteration 5687 : loss : 0.012488, loss_ce: 0.005136
2022-01-08 12:11:11,551 iteration 5688 : loss : 0.026224, loss_ce: 0.010321
2022-01-08 12:11:13,134 iteration 5689 : loss : 0.018156, loss_ce: 0.005730
2022-01-08 12:11:14,728 iteration 5690 : loss : 0.015924, loss_ce: 0.006481
2022-01-08 12:11:16,341 iteration 5691 : loss : 0.017786, loss_ce: 0.008128
2022-01-08 12:11:17,946 iteration 5692 : loss : 0.016400, loss_ce: 0.007050
2022-01-08 12:11:19,533 iteration 5693 : loss : 0.018921, loss_ce: 0.005537
2022-01-08 12:11:21,094 iteration 5694 : loss : 0.014702, loss_ce: 0.006436
2022-01-08 12:11:21,094 Training Data Eval:
2022-01-08 12:11:28,967   Average segmentation loss on training set: 0.0087
2022-01-08 12:11:28,967 Validation Data Eval:
2022-01-08 12:11:31,682   Average segmentation loss on validation set: 0.0641
2022-01-08 12:11:33,198 iteration 5695 : loss : 0.010781, loss_ce: 0.003518
 84%|████████████████████████▎    | 335/400 [2:43:23<33:10, 30.63s/it]2022-01-08 12:11:34,753 iteration 5696 : loss : 0.011046, loss_ce: 0.005530
2022-01-08 12:11:36,262 iteration 5697 : loss : 0.012615, loss_ce: 0.005680
2022-01-08 12:11:37,871 iteration 5698 : loss : 0.015344, loss_ce: 0.004960
2022-01-08 12:11:39,360 iteration 5699 : loss : 0.011843, loss_ce: 0.004984
2022-01-08 12:11:40,896 iteration 5700 : loss : 0.012092, loss_ce: 0.004217
2022-01-08 12:11:42,401 iteration 5701 : loss : 0.010809, loss_ce: 0.004750
2022-01-08 12:11:44,057 iteration 5702 : loss : 0.025381, loss_ce: 0.011601
2022-01-08 12:11:45,658 iteration 5703 : loss : 0.015680, loss_ce: 0.006659
2022-01-08 12:11:47,243 iteration 5704 : loss : 0.018074, loss_ce: 0.007450
2022-01-08 12:11:48,837 iteration 5705 : loss : 0.017355, loss_ce: 0.006083
2022-01-08 12:11:50,317 iteration 5706 : loss : 0.012716, loss_ce: 0.004817
2022-01-08 12:11:51,863 iteration 5707 : loss : 0.016522, loss_ce: 0.005525
2022-01-08 12:11:53,376 iteration 5708 : loss : 0.012418, loss_ce: 0.004484
2022-01-08 12:11:54,947 iteration 5709 : loss : 0.013277, loss_ce: 0.004476
2022-01-08 12:11:56,505 iteration 5710 : loss : 0.015201, loss_ce: 0.004832
2022-01-08 12:11:58,001 iteration 5711 : loss : 0.013308, loss_ce: 0.004409
2022-01-08 12:11:59,576 iteration 5712 : loss : 0.022399, loss_ce: 0.009875
 84%|████████████████████████▎    | 336/400 [2:43:49<31:18, 29.35s/it]2022-01-08 12:12:01,223 iteration 5713 : loss : 0.015508, loss_ce: 0.004816
2022-01-08 12:12:02,836 iteration 5714 : loss : 0.017536, loss_ce: 0.006716
2022-01-08 12:12:04,467 iteration 5715 : loss : 0.012208, loss_ce: 0.004774
2022-01-08 12:12:06,017 iteration 5716 : loss : 0.014943, loss_ce: 0.005672
2022-01-08 12:12:07,676 iteration 5717 : loss : 0.030591, loss_ce: 0.018293
2022-01-08 12:12:09,194 iteration 5718 : loss : 0.011407, loss_ce: 0.004144
2022-01-08 12:12:10,707 iteration 5719 : loss : 0.011729, loss_ce: 0.004779
2022-01-08 12:12:12,328 iteration 5720 : loss : 0.017548, loss_ce: 0.007213
2022-01-08 12:12:13,915 iteration 5721 : loss : 0.019256, loss_ce: 0.009951
2022-01-08 12:12:15,475 iteration 5722 : loss : 0.017071, loss_ce: 0.007769
2022-01-08 12:12:17,053 iteration 5723 : loss : 0.014914, loss_ce: 0.005022
2022-01-08 12:12:18,666 iteration 5724 : loss : 0.018276, loss_ce: 0.006562
2022-01-08 12:12:20,242 iteration 5725 : loss : 0.013836, loss_ce: 0.004988
2022-01-08 12:12:21,765 iteration 5726 : loss : 0.017700, loss_ce: 0.005266
2022-01-08 12:12:23,358 iteration 5727 : loss : 0.014401, loss_ce: 0.004421
2022-01-08 12:12:24,910 iteration 5728 : loss : 0.015575, loss_ce: 0.005253
2022-01-08 12:12:26,550 iteration 5729 : loss : 0.017875, loss_ce: 0.007520
 84%|████████████████████████▍    | 337/400 [2:44:16<30:04, 28.64s/it]2022-01-08 12:12:28,112 iteration 5730 : loss : 0.013064, loss_ce: 0.006041
2022-01-08 12:12:29,730 iteration 5731 : loss : 0.014318, loss_ce: 0.006432
2022-01-08 12:12:31,250 iteration 5732 : loss : 0.012890, loss_ce: 0.005126
2022-01-08 12:12:32,873 iteration 5733 : loss : 0.020004, loss_ce: 0.005451
2022-01-08 12:12:34,486 iteration 5734 : loss : 0.015136, loss_ce: 0.006635
2022-01-08 12:12:36,023 iteration 5735 : loss : 0.023079, loss_ce: 0.009092
2022-01-08 12:12:37,559 iteration 5736 : loss : 0.017857, loss_ce: 0.007247
2022-01-08 12:12:39,200 iteration 5737 : loss : 0.029793, loss_ce: 0.010400
2022-01-08 12:12:40,732 iteration 5738 : loss : 0.013019, loss_ce: 0.004205
2022-01-08 12:12:42,257 iteration 5739 : loss : 0.012940, loss_ce: 0.004768
2022-01-08 12:12:43,815 iteration 5740 : loss : 0.018844, loss_ce: 0.008464
2022-01-08 12:12:45,383 iteration 5741 : loss : 0.016496, loss_ce: 0.007653
2022-01-08 12:12:46,960 iteration 5742 : loss : 0.013811, loss_ce: 0.002867
2022-01-08 12:12:48,557 iteration 5743 : loss : 0.011598, loss_ce: 0.004962
2022-01-08 12:12:50,178 iteration 5744 : loss : 0.023424, loss_ce: 0.010804
2022-01-08 12:12:51,805 iteration 5745 : loss : 0.014952, loss_ce: 0.006516
2022-01-08 12:12:53,334 iteration 5746 : loss : 0.019323, loss_ce: 0.005907
 84%|████████████████████████▌    | 338/400 [2:44:43<29:01, 28.08s/it]2022-01-08 12:12:54,943 iteration 5747 : loss : 0.016434, loss_ce: 0.005883
2022-01-08 12:12:56,530 iteration 5748 : loss : 0.016964, loss_ce: 0.007054
2022-01-08 12:12:58,102 iteration 5749 : loss : 0.017726, loss_ce: 0.007548
2022-01-08 12:12:59,667 iteration 5750 : loss : 0.017325, loss_ce: 0.006263
2022-01-08 12:13:01,175 iteration 5751 : loss : 0.013558, loss_ce: 0.004775
2022-01-08 12:13:02,720 iteration 5752 : loss : 0.015613, loss_ce: 0.004729
2022-01-08 12:13:04,325 iteration 5753 : loss : 0.011274, loss_ce: 0.004148
2022-01-08 12:13:05,945 iteration 5754 : loss : 0.014398, loss_ce: 0.004148
2022-01-08 12:13:07,514 iteration 5755 : loss : 0.011302, loss_ce: 0.004599
2022-01-08 12:13:09,053 iteration 5756 : loss : 0.014323, loss_ce: 0.005295
2022-01-08 12:13:10,510 iteration 5757 : loss : 0.010933, loss_ce: 0.004378
2022-01-08 12:13:12,127 iteration 5758 : loss : 0.027972, loss_ce: 0.013724
2022-01-08 12:13:13,741 iteration 5759 : loss : 0.017303, loss_ce: 0.007983
2022-01-08 12:13:15,395 iteration 5760 : loss : 0.014483, loss_ce: 0.007351
2022-01-08 12:13:17,065 iteration 5761 : loss : 0.048298, loss_ce: 0.024158
2022-01-08 12:13:18,586 iteration 5762 : loss : 0.012865, loss_ce: 0.006005
2022-01-08 12:13:20,192 iteration 5763 : loss : 0.011692, loss_ce: 0.004640
 85%|████████████████████████▌    | 339/400 [2:45:10<28:10, 27.71s/it]2022-01-08 12:13:21,804 iteration 5764 : loss : 0.014454, loss_ce: 0.005102
2022-01-08 12:13:23,435 iteration 5765 : loss : 0.025620, loss_ce: 0.005181
2022-01-08 12:13:25,028 iteration 5766 : loss : 0.015687, loss_ce: 0.008861
2022-01-08 12:13:26,598 iteration 5767 : loss : 0.014298, loss_ce: 0.007169
2022-01-08 12:13:28,272 iteration 5768 : loss : 0.016014, loss_ce: 0.005916
2022-01-08 12:13:29,898 iteration 5769 : loss : 0.020618, loss_ce: 0.006943
2022-01-08 12:13:31,434 iteration 5770 : loss : 0.012663, loss_ce: 0.004549
2022-01-08 12:13:33,029 iteration 5771 : loss : 0.017287, loss_ce: 0.007227
2022-01-08 12:13:34,594 iteration 5772 : loss : 0.017045, loss_ce: 0.005767
2022-01-08 12:13:36,096 iteration 5773 : loss : 0.012346, loss_ce: 0.004453
2022-01-08 12:13:37,579 iteration 5774 : loss : 0.015065, loss_ce: 0.003955
2022-01-08 12:13:39,160 iteration 5775 : loss : 0.015448, loss_ce: 0.006550
2022-01-08 12:13:40,809 iteration 5776 : loss : 0.017485, loss_ce: 0.007559
2022-01-08 12:13:42,301 iteration 5777 : loss : 0.015826, loss_ce: 0.006557
2022-01-08 12:13:43,841 iteration 5778 : loss : 0.017708, loss_ce: 0.005185
2022-01-08 12:13:45,394 iteration 5779 : loss : 0.013291, loss_ce: 0.005006
2022-01-08 12:13:45,394 Training Data Eval:
2022-01-08 12:13:53,280   Average segmentation loss on training set: 0.0083
2022-01-08 12:13:53,280 Validation Data Eval:
2022-01-08 12:13:55,995   Average segmentation loss on validation set: 0.0625
2022-01-08 12:13:57,547 iteration 5780 : loss : 0.018602, loss_ce: 0.008206
 85%|████████████████████████▋    | 340/400 [2:45:47<30:36, 30.61s/it]2022-01-08 12:13:59,119 iteration 5781 : loss : 0.024096, loss_ce: 0.007375
2022-01-08 12:14:00,751 iteration 5782 : loss : 0.025680, loss_ce: 0.009894
2022-01-08 12:14:02,319 iteration 5783 : loss : 0.018050, loss_ce: 0.006965
2022-01-08 12:14:03,917 iteration 5784 : loss : 0.019281, loss_ce: 0.008585
2022-01-08 12:14:05,446 iteration 5785 : loss : 0.014941, loss_ce: 0.006877
2022-01-08 12:14:06,990 iteration 5786 : loss : 0.011995, loss_ce: 0.005519
2022-01-08 12:14:08,525 iteration 5787 : loss : 0.013209, loss_ce: 0.005195
2022-01-08 12:14:10,232 iteration 5788 : loss : 0.021336, loss_ce: 0.008567
2022-01-08 12:14:11,829 iteration 5789 : loss : 0.014566, loss_ce: 0.005699
2022-01-08 12:14:13,411 iteration 5790 : loss : 0.013347, loss_ce: 0.004635
2022-01-08 12:14:14,943 iteration 5791 : loss : 0.012191, loss_ce: 0.004431
2022-01-08 12:14:16,550 iteration 5792 : loss : 0.025442, loss_ce: 0.012792
2022-01-08 12:14:18,238 iteration 5793 : loss : 0.019955, loss_ce: 0.007924
2022-01-08 12:14:19,823 iteration 5794 : loss : 0.013053, loss_ce: 0.005123
2022-01-08 12:14:21,386 iteration 5795 : loss : 0.015386, loss_ce: 0.005398
2022-01-08 12:14:22,907 iteration 5796 : loss : 0.012033, loss_ce: 0.003276
2022-01-08 12:14:24,498 iteration 5797 : loss : 0.015251, loss_ce: 0.006023
 85%|████████████████████████▋    | 341/400 [2:46:14<29:01, 29.51s/it]2022-01-08 12:14:26,140 iteration 5798 : loss : 0.015438, loss_ce: 0.005903
2022-01-08 12:14:27,722 iteration 5799 : loss : 0.015264, loss_ce: 0.005745
2022-01-08 12:14:29,281 iteration 5800 : loss : 0.013740, loss_ce: 0.006128
2022-01-08 12:14:30,839 iteration 5801 : loss : 0.018196, loss_ce: 0.006663
2022-01-08 12:14:32,341 iteration 5802 : loss : 0.010413, loss_ce: 0.004330
2022-01-08 12:14:33,874 iteration 5803 : loss : 0.012066, loss_ce: 0.005439
2022-01-08 12:14:35,436 iteration 5804 : loss : 0.016302, loss_ce: 0.005966
2022-01-08 12:14:37,090 iteration 5805 : loss : 0.020467, loss_ce: 0.007764
2022-01-08 12:14:38,644 iteration 5806 : loss : 0.027382, loss_ce: 0.011357
2022-01-08 12:14:40,285 iteration 5807 : loss : 0.019131, loss_ce: 0.006804
2022-01-08 12:14:41,875 iteration 5808 : loss : 0.023355, loss_ce: 0.010334
2022-01-08 12:14:43,383 iteration 5809 : loss : 0.010153, loss_ce: 0.002633
2022-01-08 12:14:44,910 iteration 5810 : loss : 0.013552, loss_ce: 0.004622
2022-01-08 12:14:46,404 iteration 5811 : loss : 0.010652, loss_ce: 0.003136
2022-01-08 12:14:47,967 iteration 5812 : loss : 0.012137, loss_ce: 0.004345
2022-01-08 12:14:49,467 iteration 5813 : loss : 0.012831, loss_ce: 0.005855
2022-01-08 12:14:51,043 iteration 5814 : loss : 0.014510, loss_ce: 0.006083
 86%|████████████████████████▊    | 342/400 [2:46:41<27:40, 28.62s/it]2022-01-08 12:14:52,580 iteration 5815 : loss : 0.009915, loss_ce: 0.003149
2022-01-08 12:14:54,123 iteration 5816 : loss : 0.012359, loss_ce: 0.005751
2022-01-08 12:14:55,658 iteration 5817 : loss : 0.012288, loss_ce: 0.004450
2022-01-08 12:14:57,129 iteration 5818 : loss : 0.013362, loss_ce: 0.005877
2022-01-08 12:14:58,741 iteration 5819 : loss : 0.020122, loss_ce: 0.006949
2022-01-08 12:15:00,382 iteration 5820 : loss : 0.022297, loss_ce: 0.010507
2022-01-08 12:15:01,876 iteration 5821 : loss : 0.013706, loss_ce: 0.004438
2022-01-08 12:15:03,403 iteration 5822 : loss : 0.009529, loss_ce: 0.004444
2022-01-08 12:15:05,068 iteration 5823 : loss : 0.013993, loss_ce: 0.005814
2022-01-08 12:15:06,637 iteration 5824 : loss : 0.012087, loss_ce: 0.003644
2022-01-08 12:15:08,244 iteration 5825 : loss : 0.015907, loss_ce: 0.006012
2022-01-08 12:15:09,856 iteration 5826 : loss : 0.016459, loss_ce: 0.004043
2022-01-08 12:15:11,428 iteration 5827 : loss : 0.020849, loss_ce: 0.006310
2022-01-08 12:15:13,098 iteration 5828 : loss : 0.015166, loss_ce: 0.005044
2022-01-08 12:15:14,658 iteration 5829 : loss : 0.017027, loss_ce: 0.006595
2022-01-08 12:15:16,189 iteration 5830 : loss : 0.023136, loss_ce: 0.009561
2022-01-08 12:15:17,733 iteration 5831 : loss : 0.013142, loss_ce: 0.004480
 86%|████████████████████████▊    | 343/400 [2:47:07<26:38, 28.04s/it]2022-01-08 12:15:19,343 iteration 5832 : loss : 0.013222, loss_ce: 0.003654
2022-01-08 12:15:20,952 iteration 5833 : loss : 0.012328, loss_ce: 0.005255
2022-01-08 12:15:22,459 iteration 5834 : loss : 0.015324, loss_ce: 0.005614
2022-01-08 12:15:23,976 iteration 5835 : loss : 0.017355, loss_ce: 0.005046
2022-01-08 12:15:25,585 iteration 5836 : loss : 0.021057, loss_ce: 0.011231
2022-01-08 12:15:27,158 iteration 5837 : loss : 0.021053, loss_ce: 0.004161
2022-01-08 12:15:28,640 iteration 5838 : loss : 0.009221, loss_ce: 0.002734
2022-01-08 12:15:30,200 iteration 5839 : loss : 0.011944, loss_ce: 0.004072
2022-01-08 12:15:31,817 iteration 5840 : loss : 0.016806, loss_ce: 0.006649
2022-01-08 12:15:33,362 iteration 5841 : loss : 0.018497, loss_ce: 0.009821
2022-01-08 12:15:34,902 iteration 5842 : loss : 0.008745, loss_ce: 0.003221
2022-01-08 12:15:36,433 iteration 5843 : loss : 0.014630, loss_ce: 0.005722
2022-01-08 12:15:38,135 iteration 5844 : loss : 0.023659, loss_ce: 0.006242
2022-01-08 12:15:39,709 iteration 5845 : loss : 0.019000, loss_ce: 0.007034
2022-01-08 12:15:41,296 iteration 5846 : loss : 0.016997, loss_ce: 0.007885
2022-01-08 12:15:42,881 iteration 5847 : loss : 0.016915, loss_ce: 0.006487
2022-01-08 12:15:44,446 iteration 5848 : loss : 0.015213, loss_ce: 0.005539
 86%|████████████████████████▉    | 344/400 [2:47:34<25:48, 27.64s/it]2022-01-08 12:15:46,055 iteration 5849 : loss : 0.014251, loss_ce: 0.005957
2022-01-08 12:15:47,561 iteration 5850 : loss : 0.012224, loss_ce: 0.004855
2022-01-08 12:15:49,124 iteration 5851 : loss : 0.014448, loss_ce: 0.005482
2022-01-08 12:15:50,661 iteration 5852 : loss : 0.015033, loss_ce: 0.005585
2022-01-08 12:15:52,143 iteration 5853 : loss : 0.010007, loss_ce: 0.003115
2022-01-08 12:15:53,678 iteration 5854 : loss : 0.019017, loss_ce: 0.006439
2022-01-08 12:15:55,315 iteration 5855 : loss : 0.021266, loss_ce: 0.009550
2022-01-08 12:15:56,881 iteration 5856 : loss : 0.012519, loss_ce: 0.005096
2022-01-08 12:15:58,459 iteration 5857 : loss : 0.013154, loss_ce: 0.004563
2022-01-08 12:15:59,948 iteration 5858 : loss : 0.012227, loss_ce: 0.004481
2022-01-08 12:16:01,555 iteration 5859 : loss : 0.013924, loss_ce: 0.005162
2022-01-08 12:16:03,161 iteration 5860 : loss : 0.014200, loss_ce: 0.003680
2022-01-08 12:16:04,699 iteration 5861 : loss : 0.011171, loss_ce: 0.003486
2022-01-08 12:16:06,283 iteration 5862 : loss : 0.012021, loss_ce: 0.005550
2022-01-08 12:16:07,754 iteration 5863 : loss : 0.009751, loss_ce: 0.003550
2022-01-08 12:16:09,317 iteration 5864 : loss : 0.018112, loss_ce: 0.004969
2022-01-08 12:16:09,317 Training Data Eval:
2022-01-08 12:16:17,200   Average segmentation loss on training set: 0.0078
2022-01-08 12:16:17,201 Validation Data Eval:
2022-01-08 12:16:19,916   Average segmentation loss on validation set: 0.0646
2022-01-08 12:16:21,427 iteration 5865 : loss : 0.012452, loss_ce: 0.003852
 86%|█████████████████████████    | 345/400 [2:48:11<27:54, 30.45s/it]2022-01-08 12:16:23,112 iteration 5866 : loss : 0.013839, loss_ce: 0.005898
2022-01-08 12:16:24,729 iteration 5867 : loss : 0.017414, loss_ce: 0.005502
2022-01-08 12:16:26,329 iteration 5868 : loss : 0.016477, loss_ce: 0.006568
2022-01-08 12:16:27,819 iteration 5869 : loss : 0.017473, loss_ce: 0.007015
2022-01-08 12:16:29,464 iteration 5870 : loss : 0.020239, loss_ce: 0.006394
2022-01-08 12:16:31,058 iteration 5871 : loss : 0.011202, loss_ce: 0.003816
2022-01-08 12:16:32,663 iteration 5872 : loss : 0.020996, loss_ce: 0.008648
2022-01-08 12:16:34,352 iteration 5873 : loss : 0.024069, loss_ce: 0.007468
2022-01-08 12:16:35,882 iteration 5874 : loss : 0.014258, loss_ce: 0.004544
2022-01-08 12:16:37,426 iteration 5875 : loss : 0.012346, loss_ce: 0.005925
2022-01-08 12:16:38,993 iteration 5876 : loss : 0.013613, loss_ce: 0.004803
2022-01-08 12:16:40,557 iteration 5877 : loss : 0.017618, loss_ce: 0.006158
2022-01-08 12:16:42,146 iteration 5878 : loss : 0.013257, loss_ce: 0.005950
2022-01-08 12:16:43,670 iteration 5879 : loss : 0.013852, loss_ce: 0.006374
2022-01-08 12:16:45,330 iteration 5880 : loss : 0.019018, loss_ce: 0.005897
2022-01-08 12:16:46,835 iteration 5881 : loss : 0.012005, loss_ce: 0.004469
2022-01-08 12:16:48,379 iteration 5882 : loss : 0.014066, loss_ce: 0.006318
 86%|█████████████████████████    | 346/400 [2:48:38<26:27, 29.40s/it]2022-01-08 12:16:49,956 iteration 5883 : loss : 0.010843, loss_ce: 0.003660
2022-01-08 12:16:51,491 iteration 5884 : loss : 0.010990, loss_ce: 0.004614
2022-01-08 12:16:53,175 iteration 5885 : loss : 0.027761, loss_ce: 0.011254
2022-01-08 12:16:54,686 iteration 5886 : loss : 0.013356, loss_ce: 0.003759
2022-01-08 12:16:56,222 iteration 5887 : loss : 0.012695, loss_ce: 0.005254
2022-01-08 12:16:57,871 iteration 5888 : loss : 0.022879, loss_ce: 0.007886
2022-01-08 12:16:59,519 iteration 5889 : loss : 0.015532, loss_ce: 0.006307
2022-01-08 12:17:01,107 iteration 5890 : loss : 0.012874, loss_ce: 0.004954
2022-01-08 12:17:02,737 iteration 5891 : loss : 0.016395, loss_ce: 0.005537
2022-01-08 12:17:04,404 iteration 5892 : loss : 0.016940, loss_ce: 0.006800
2022-01-08 12:17:05,942 iteration 5893 : loss : 0.011461, loss_ce: 0.003666
2022-01-08 12:17:07,591 iteration 5894 : loss : 0.017413, loss_ce: 0.006649
2022-01-08 12:17:09,196 iteration 5895 : loss : 0.022379, loss_ce: 0.007984
2022-01-08 12:17:10,758 iteration 5896 : loss : 0.014241, loss_ce: 0.006581
2022-01-08 12:17:12,409 iteration 5897 : loss : 0.029221, loss_ce: 0.016245
2022-01-08 12:17:14,040 iteration 5898 : loss : 0.011381, loss_ce: 0.004000
2022-01-08 12:17:15,597 iteration 5899 : loss : 0.013518, loss_ce: 0.005069
 87%|█████████████████████████▏   | 347/400 [2:49:05<25:23, 28.74s/it]2022-01-08 12:17:17,231 iteration 5900 : loss : 0.015604, loss_ce: 0.005704
2022-01-08 12:17:18,886 iteration 5901 : loss : 0.016203, loss_ce: 0.005981
2022-01-08 12:17:20,456 iteration 5902 : loss : 0.015279, loss_ce: 0.005855
2022-01-08 12:17:22,095 iteration 5903 : loss : 0.013133, loss_ce: 0.006581
2022-01-08 12:17:23,651 iteration 5904 : loss : 0.019080, loss_ce: 0.007073
2022-01-08 12:17:25,269 iteration 5905 : loss : 0.020526, loss_ce: 0.005327
2022-01-08 12:17:26,789 iteration 5906 : loss : 0.015527, loss_ce: 0.005897
2022-01-08 12:17:28,395 iteration 5907 : loss : 0.009518, loss_ce: 0.003093
2022-01-08 12:17:29,972 iteration 5908 : loss : 0.009467, loss_ce: 0.002961
2022-01-08 12:17:31,454 iteration 5909 : loss : 0.014911, loss_ce: 0.004051
2022-01-08 12:17:33,033 iteration 5910 : loss : 0.019611, loss_ce: 0.007105
2022-01-08 12:17:34,649 iteration 5911 : loss : 0.021795, loss_ce: 0.009584
2022-01-08 12:17:36,162 iteration 5912 : loss : 0.012735, loss_ce: 0.004240
2022-01-08 12:17:37,704 iteration 5913 : loss : 0.012950, loss_ce: 0.006282
2022-01-08 12:17:39,279 iteration 5914 : loss : 0.015100, loss_ce: 0.005977
2022-01-08 12:17:40,765 iteration 5915 : loss : 0.016075, loss_ce: 0.006902
2022-01-08 12:17:42,375 iteration 5916 : loss : 0.018978, loss_ce: 0.006659
 87%|█████████████████████████▏   | 348/400 [2:49:32<24:23, 28.15s/it]2022-01-08 12:17:43,954 iteration 5917 : loss : 0.013524, loss_ce: 0.005584
2022-01-08 12:17:45,456 iteration 5918 : loss : 0.013535, loss_ce: 0.005196
2022-01-08 12:17:47,059 iteration 5919 : loss : 0.017097, loss_ce: 0.007451
2022-01-08 12:17:48,678 iteration 5920 : loss : 0.030203, loss_ce: 0.016353
2022-01-08 12:17:50,384 iteration 5921 : loss : 0.026461, loss_ce: 0.007022
2022-01-08 12:17:51,937 iteration 5922 : loss : 0.011510, loss_ce: 0.003073
2022-01-08 12:17:53,525 iteration 5923 : loss : 0.016365, loss_ce: 0.005407
2022-01-08 12:17:55,199 iteration 5924 : loss : 0.038895, loss_ce: 0.010580
2022-01-08 12:17:56,783 iteration 5925 : loss : 0.015056, loss_ce: 0.005399
2022-01-08 12:17:58,355 iteration 5926 : loss : 0.015743, loss_ce: 0.006099
2022-01-08 12:17:59,943 iteration 5927 : loss : 0.013682, loss_ce: 0.005475
2022-01-08 12:18:01,452 iteration 5928 : loss : 0.017769, loss_ce: 0.005345
2022-01-08 12:18:02,962 iteration 5929 : loss : 0.012976, loss_ce: 0.006167
2022-01-08 12:18:04,475 iteration 5930 : loss : 0.016083, loss_ce: 0.007483
2022-01-08 12:18:06,052 iteration 5931 : loss : 0.031558, loss_ce: 0.008784
2022-01-08 12:18:07,668 iteration 5932 : loss : 0.016482, loss_ce: 0.005614
2022-01-08 12:18:09,225 iteration 5933 : loss : 0.013019, loss_ce: 0.004553
 87%|█████████████████████████▎   | 349/400 [2:49:59<23:35, 27.76s/it]2022-01-08 12:18:10,790 iteration 5934 : loss : 0.012955, loss_ce: 0.004143
2022-01-08 12:18:12,488 iteration 5935 : loss : 0.019977, loss_ce: 0.007837
2022-01-08 12:18:14,062 iteration 5936 : loss : 0.013126, loss_ce: 0.005820
2022-01-08 12:18:15,598 iteration 5937 : loss : 0.010987, loss_ce: 0.003942
2022-01-08 12:18:17,088 iteration 5938 : loss : 0.009170, loss_ce: 0.003472
2022-01-08 12:18:18,687 iteration 5939 : loss : 0.012579, loss_ce: 0.005162
2022-01-08 12:18:20,200 iteration 5940 : loss : 0.012558, loss_ce: 0.005446
2022-01-08 12:18:21,728 iteration 5941 : loss : 0.015270, loss_ce: 0.006237
2022-01-08 12:18:23,263 iteration 5942 : loss : 0.015024, loss_ce: 0.006620
2022-01-08 12:18:24,838 iteration 5943 : loss : 0.015115, loss_ce: 0.004500
2022-01-08 12:18:26,411 iteration 5944 : loss : 0.013086, loss_ce: 0.006286
2022-01-08 12:18:27,947 iteration 5945 : loss : 0.013671, loss_ce: 0.003017
2022-01-08 12:18:29,607 iteration 5946 : loss : 0.016587, loss_ce: 0.006595
2022-01-08 12:18:31,210 iteration 5947 : loss : 0.019088, loss_ce: 0.007098
2022-01-08 12:18:32,753 iteration 5948 : loss : 0.016201, loss_ce: 0.005904
2022-01-08 12:18:34,388 iteration 5949 : loss : 0.024981, loss_ce: 0.008839
2022-01-08 12:18:34,389 Training Data Eval:
2022-01-08 12:18:42,276   Average segmentation loss on training set: 0.0080
2022-01-08 12:18:42,276 Validation Data Eval:
2022-01-08 12:18:44,997   Average segmentation loss on validation set: 0.0636
2022-01-08 12:18:46,576 iteration 5950 : loss : 0.016840, loss_ce: 0.005655
 88%|█████████████████████████▍   | 350/400 [2:50:36<25:32, 30.64s/it]2022-01-08 12:18:48,136 iteration 5951 : loss : 0.012845, loss_ce: 0.004475
2022-01-08 12:18:49,704 iteration 5952 : loss : 0.014186, loss_ce: 0.005411
2022-01-08 12:18:51,273 iteration 5953 : loss : 0.016159, loss_ce: 0.006307
2022-01-08 12:18:52,804 iteration 5954 : loss : 0.012906, loss_ce: 0.004647
2022-01-08 12:18:54,364 iteration 5955 : loss : 0.014599, loss_ce: 0.004797
2022-01-08 12:18:55,955 iteration 5956 : loss : 0.019495, loss_ce: 0.012111
2022-01-08 12:18:57,531 iteration 5957 : loss : 0.010897, loss_ce: 0.004094
2022-01-08 12:18:59,055 iteration 5958 : loss : 0.014180, loss_ce: 0.006285
2022-01-08 12:19:00,610 iteration 5959 : loss : 0.014407, loss_ce: 0.005524
2022-01-08 12:19:02,128 iteration 5960 : loss : 0.014022, loss_ce: 0.004272
2022-01-08 12:19:03,637 iteration 5961 : loss : 0.015210, loss_ce: 0.003438
2022-01-08 12:19:05,187 iteration 5962 : loss : 0.016054, loss_ce: 0.003397
2022-01-08 12:19:06,782 iteration 5963 : loss : 0.017892, loss_ce: 0.007971
2022-01-08 12:19:08,318 iteration 5964 : loss : 0.009591, loss_ce: 0.004257
2022-01-08 12:19:09,821 iteration 5965 : loss : 0.010620, loss_ce: 0.005029
2022-01-08 12:19:11,433 iteration 5966 : loss : 0.016884, loss_ce: 0.005492
2022-01-08 12:19:13,012 iteration 5967 : loss : 0.012200, loss_ce: 0.004817
 88%|█████████████████████████▍   | 351/400 [2:51:03<23:59, 29.38s/it]2022-01-08 12:19:14,690 iteration 5968 : loss : 0.014506, loss_ce: 0.006171
2022-01-08 12:19:16,232 iteration 5969 : loss : 0.014154, loss_ce: 0.006119
2022-01-08 12:19:17,842 iteration 5970 : loss : 0.013634, loss_ce: 0.005536
2022-01-08 12:19:19,347 iteration 5971 : loss : 0.010632, loss_ce: 0.003529
2022-01-08 12:19:20,822 iteration 5972 : loss : 0.010541, loss_ce: 0.003375
2022-01-08 12:19:22,395 iteration 5973 : loss : 0.015491, loss_ce: 0.007409
2022-01-08 12:19:24,063 iteration 5974 : loss : 0.026394, loss_ce: 0.010393
2022-01-08 12:19:25,579 iteration 5975 : loss : 0.014214, loss_ce: 0.004772
2022-01-08 12:19:27,116 iteration 5976 : loss : 0.014572, loss_ce: 0.004644
2022-01-08 12:19:28,628 iteration 5977 : loss : 0.016391, loss_ce: 0.004020
2022-01-08 12:19:30,136 iteration 5978 : loss : 0.011893, loss_ce: 0.005421
2022-01-08 12:19:31,691 iteration 5979 : loss : 0.011062, loss_ce: 0.005187
2022-01-08 12:19:33,293 iteration 5980 : loss : 0.015802, loss_ce: 0.004780
2022-01-08 12:19:34,821 iteration 5981 : loss : 0.019862, loss_ce: 0.008987
2022-01-08 12:19:36,385 iteration 5982 : loss : 0.019666, loss_ce: 0.008508
2022-01-08 12:19:38,009 iteration 5983 : loss : 0.014834, loss_ce: 0.004479
2022-01-08 12:19:39,543 iteration 5984 : loss : 0.014458, loss_ce: 0.005044
 88%|█████████████████████████▌   | 352/400 [2:51:29<22:49, 28.53s/it]2022-01-08 12:19:41,149 iteration 5985 : loss : 0.012394, loss_ce: 0.004000
2022-01-08 12:19:42,627 iteration 5986 : loss : 0.012033, loss_ce: 0.003609
2022-01-08 12:19:44,215 iteration 5987 : loss : 0.014455, loss_ce: 0.006494
2022-01-08 12:19:45,829 iteration 5988 : loss : 0.017493, loss_ce: 0.007848
2022-01-08 12:19:47,410 iteration 5989 : loss : 0.011117, loss_ce: 0.004303
2022-01-08 12:19:48,951 iteration 5990 : loss : 0.013884, loss_ce: 0.004679
2022-01-08 12:19:50,537 iteration 5991 : loss : 0.019974, loss_ce: 0.006418
2022-01-08 12:19:52,120 iteration 5992 : loss : 0.014367, loss_ce: 0.004820
2022-01-08 12:19:53,675 iteration 5993 : loss : 0.012064, loss_ce: 0.005502
2022-01-08 12:19:55,283 iteration 5994 : loss : 0.017153, loss_ce: 0.008098
2022-01-08 12:19:56,837 iteration 5995 : loss : 0.012106, loss_ce: 0.005016
2022-01-08 12:19:58,353 iteration 5996 : loss : 0.010464, loss_ce: 0.003053
2022-01-08 12:19:59,937 iteration 5997 : loss : 0.016656, loss_ce: 0.006146
2022-01-08 12:20:01,582 iteration 5998 : loss : 0.014322, loss_ce: 0.004861
2022-01-08 12:20:03,253 iteration 5999 : loss : 0.024122, loss_ce: 0.008447
2022-01-08 12:20:04,838 iteration 6000 : loss : 0.015158, loss_ce: 0.006534
2022-01-08 12:20:06,327 iteration 6001 : loss : 0.014094, loss_ce: 0.006445
 88%|█████████████████████████▌   | 353/400 [2:51:56<21:56, 28.00s/it]2022-01-08 12:20:07,944 iteration 6002 : loss : 0.013627, loss_ce: 0.004863
2022-01-08 12:20:09,474 iteration 6003 : loss : 0.012229, loss_ce: 0.004434
2022-01-08 12:20:11,090 iteration 6004 : loss : 0.014837, loss_ce: 0.006680
2022-01-08 12:20:12,686 iteration 6005 : loss : 0.019959, loss_ce: 0.007278
2022-01-08 12:20:14,356 iteration 6006 : loss : 0.016124, loss_ce: 0.007697
2022-01-08 12:20:15,945 iteration 6007 : loss : 0.011864, loss_ce: 0.004973
2022-01-08 12:20:17,519 iteration 6008 : loss : 0.012981, loss_ce: 0.004460
2022-01-08 12:20:19,005 iteration 6009 : loss : 0.010970, loss_ce: 0.005106
2022-01-08 12:20:20,511 iteration 6010 : loss : 0.011845, loss_ce: 0.005204
2022-01-08 12:20:22,105 iteration 6011 : loss : 0.015195, loss_ce: 0.005419
2022-01-08 12:20:23,731 iteration 6012 : loss : 0.015616, loss_ce: 0.003440
2022-01-08 12:20:25,338 iteration 6013 : loss : 0.012923, loss_ce: 0.004282
2022-01-08 12:20:26,932 iteration 6014 : loss : 0.014972, loss_ce: 0.004114
2022-01-08 12:20:28,464 iteration 6015 : loss : 0.012591, loss_ce: 0.005217
2022-01-08 12:20:30,030 iteration 6016 : loss : 0.012627, loss_ce: 0.004452
2022-01-08 12:20:31,590 iteration 6017 : loss : 0.014042, loss_ce: 0.006146
2022-01-08 12:20:33,173 iteration 6018 : loss : 0.016408, loss_ce: 0.006072
 88%|█████████████████████████▋   | 354/400 [2:52:23<21:12, 27.65s/it]2022-01-08 12:20:34,791 iteration 6019 : loss : 0.018624, loss_ce: 0.009433
2022-01-08 12:20:36,341 iteration 6020 : loss : 0.015389, loss_ce: 0.006616
2022-01-08 12:20:37,923 iteration 6021 : loss : 0.028672, loss_ce: 0.007316
2022-01-08 12:20:39,436 iteration 6022 : loss : 0.013684, loss_ce: 0.005637
2022-01-08 12:20:41,041 iteration 6023 : loss : 0.017110, loss_ce: 0.007313
2022-01-08 12:20:42,584 iteration 6024 : loss : 0.014502, loss_ce: 0.004867
2022-01-08 12:20:44,118 iteration 6025 : loss : 0.013241, loss_ce: 0.005070
2022-01-08 12:20:45,704 iteration 6026 : loss : 0.014489, loss_ce: 0.004713
2022-01-08 12:20:47,282 iteration 6027 : loss : 0.024055, loss_ce: 0.007243
2022-01-08 12:20:48,883 iteration 6028 : loss : 0.012862, loss_ce: 0.005064
2022-01-08 12:20:50,429 iteration 6029 : loss : 0.016516, loss_ce: 0.004667
2022-01-08 12:20:51,924 iteration 6030 : loss : 0.014298, loss_ce: 0.006228
2022-01-08 12:20:53,505 iteration 6031 : loss : 0.012532, loss_ce: 0.005867
2022-01-08 12:20:55,023 iteration 6032 : loss : 0.014916, loss_ce: 0.004969
2022-01-08 12:20:56,633 iteration 6033 : loss : 0.014798, loss_ce: 0.005326
2022-01-08 12:20:58,202 iteration 6034 : loss : 0.011915, loss_ce: 0.004724
2022-01-08 12:20:58,203 Training Data Eval:
2022-01-08 12:21:06,099   Average segmentation loss on training set: 0.0077
2022-01-08 12:21:06,099 Validation Data Eval:
2022-01-08 12:21:08,814   Average segmentation loss on validation set: 0.0604
2022-01-08 12:21:10,264 iteration 6035 : loss : 0.009610, loss_ce: 0.003728
 89%|█████████████████████████▋   | 355/400 [2:53:00<22:51, 30.49s/it]2022-01-08 12:21:11,954 iteration 6036 : loss : 0.016705, loss_ce: 0.004721
2022-01-08 12:21:13,561 iteration 6037 : loss : 0.020215, loss_ce: 0.005625
2022-01-08 12:21:15,108 iteration 6038 : loss : 0.014647, loss_ce: 0.005618
2022-01-08 12:21:16,640 iteration 6039 : loss : 0.012465, loss_ce: 0.003859
2022-01-08 12:21:18,211 iteration 6040 : loss : 0.014527, loss_ce: 0.005768
2022-01-08 12:21:19,735 iteration 6041 : loss : 0.011371, loss_ce: 0.003848
2022-01-08 12:21:21,313 iteration 6042 : loss : 0.011180, loss_ce: 0.005143
2022-01-08 12:21:22,825 iteration 6043 : loss : 0.010999, loss_ce: 0.005043
2022-01-08 12:21:24,320 iteration 6044 : loss : 0.010380, loss_ce: 0.004289
2022-01-08 12:21:25,951 iteration 6045 : loss : 0.014567, loss_ce: 0.005624
2022-01-08 12:21:27,455 iteration 6046 : loss : 0.013697, loss_ce: 0.005944
2022-01-08 12:21:28,959 iteration 6047 : loss : 0.010132, loss_ce: 0.004162
2022-01-08 12:21:30,586 iteration 6048 : loss : 0.015715, loss_ce: 0.005990
2022-01-08 12:21:32,076 iteration 6049 : loss : 0.013210, loss_ce: 0.004454
2022-01-08 12:21:33,566 iteration 6050 : loss : 0.012484, loss_ce: 0.003285
2022-01-08 12:21:35,055 iteration 6051 : loss : 0.009572, loss_ce: 0.004044
2022-01-08 12:21:36,635 iteration 6052 : loss : 0.013102, loss_ce: 0.005661
 89%|█████████████████████████▊   | 356/400 [2:53:26<21:27, 29.25s/it]2022-01-08 12:21:38,206 iteration 6053 : loss : 0.014921, loss_ce: 0.004991
2022-01-08 12:21:39,729 iteration 6054 : loss : 0.011992, loss_ce: 0.003962
2022-01-08 12:21:41,308 iteration 6055 : loss : 0.014867, loss_ce: 0.006174
2022-01-08 12:21:42,849 iteration 6056 : loss : 0.017632, loss_ce: 0.008248
2022-01-08 12:21:44,326 iteration 6057 : loss : 0.011626, loss_ce: 0.004595
2022-01-08 12:21:45,829 iteration 6058 : loss : 0.010640, loss_ce: 0.006027
2022-01-08 12:21:47,443 iteration 6059 : loss : 0.017359, loss_ce: 0.005048
2022-01-08 12:21:49,013 iteration 6060 : loss : 0.016401, loss_ce: 0.005206
2022-01-08 12:21:50,524 iteration 6061 : loss : 0.011884, loss_ce: 0.005538
2022-01-08 12:21:52,207 iteration 6062 : loss : 0.021371, loss_ce: 0.007761
2022-01-08 12:21:53,724 iteration 6063 : loss : 0.013283, loss_ce: 0.005606
2022-01-08 12:21:55,247 iteration 6064 : loss : 0.010039, loss_ce: 0.003149
2022-01-08 12:21:56,833 iteration 6065 : loss : 0.015548, loss_ce: 0.005692
2022-01-08 12:21:58,326 iteration 6066 : loss : 0.015381, loss_ce: 0.006618
2022-01-08 12:21:59,902 iteration 6067 : loss : 0.022280, loss_ce: 0.006658
2022-01-08 12:22:01,460 iteration 6068 : loss : 0.012911, loss_ce: 0.002403
2022-01-08 12:22:03,097 iteration 6069 : loss : 0.016162, loss_ce: 0.006644
 89%|█████████████████████████▉   | 357/400 [2:53:53<20:21, 28.41s/it]2022-01-08 12:22:04,712 iteration 6070 : loss : 0.015001, loss_ce: 0.006513
2022-01-08 12:22:06,277 iteration 6071 : loss : 0.013376, loss_ce: 0.005863
2022-01-08 12:22:07,826 iteration 6072 : loss : 0.018908, loss_ce: 0.006689
2022-01-08 12:22:09,390 iteration 6073 : loss : 0.020042, loss_ce: 0.005374
2022-01-08 12:22:10,865 iteration 6074 : loss : 0.012320, loss_ce: 0.005840
2022-01-08 12:22:12,459 iteration 6075 : loss : 0.018085, loss_ce: 0.008594
2022-01-08 12:22:14,030 iteration 6076 : loss : 0.012114, loss_ce: 0.005069
2022-01-08 12:22:15,482 iteration 6077 : loss : 0.010030, loss_ce: 0.003556
2022-01-08 12:22:17,024 iteration 6078 : loss : 0.013537, loss_ce: 0.006755
2022-01-08 12:22:18,654 iteration 6079 : loss : 0.017477, loss_ce: 0.007514
2022-01-08 12:22:20,148 iteration 6080 : loss : 0.011538, loss_ce: 0.004227
2022-01-08 12:22:21,780 iteration 6081 : loss : 0.017764, loss_ce: 0.007055
2022-01-08 12:22:23,339 iteration 6082 : loss : 0.017205, loss_ce: 0.004372
2022-01-08 12:22:24,949 iteration 6083 : loss : 0.013517, loss_ce: 0.003626
2022-01-08 12:22:26,474 iteration 6084 : loss : 0.013429, loss_ce: 0.003795
2022-01-08 12:22:27,993 iteration 6085 : loss : 0.016893, loss_ce: 0.006490
2022-01-08 12:22:29,721 iteration 6086 : loss : 0.032580, loss_ce: 0.008317
 90%|█████████████████████████▉   | 358/400 [2:54:19<19:30, 27.88s/it]2022-01-08 12:22:31,269 iteration 6087 : loss : 0.011213, loss_ce: 0.004417
2022-01-08 12:22:32,842 iteration 6088 : loss : 0.016125, loss_ce: 0.007298
2022-01-08 12:22:34,389 iteration 6089 : loss : 0.019575, loss_ce: 0.008872
2022-01-08 12:22:35,941 iteration 6090 : loss : 0.013668, loss_ce: 0.003815
2022-01-08 12:22:37,500 iteration 6091 : loss : 0.010199, loss_ce: 0.003821
2022-01-08 12:22:39,065 iteration 6092 : loss : 0.011186, loss_ce: 0.004848
2022-01-08 12:22:40,550 iteration 6093 : loss : 0.014584, loss_ce: 0.005043
2022-01-08 12:22:42,061 iteration 6094 : loss : 0.011921, loss_ce: 0.004272
2022-01-08 12:22:43,652 iteration 6095 : loss : 0.016028, loss_ce: 0.005998
2022-01-08 12:22:45,245 iteration 6096 : loss : 0.017482, loss_ce: 0.006462
2022-01-08 12:22:46,734 iteration 6097 : loss : 0.012595, loss_ce: 0.005624
2022-01-08 12:22:48,297 iteration 6098 : loss : 0.019326, loss_ce: 0.007677
2022-01-08 12:22:49,894 iteration 6099 : loss : 0.025190, loss_ce: 0.006651
2022-01-08 12:22:51,535 iteration 6100 : loss : 0.017678, loss_ce: 0.005646
2022-01-08 12:22:53,143 iteration 6101 : loss : 0.011053, loss_ce: 0.004008
2022-01-08 12:22:54,662 iteration 6102 : loss : 0.010947, loss_ce: 0.003270
2022-01-08 12:22:56,113 iteration 6103 : loss : 0.008416, loss_ce: 0.003065
 90%|██████████████████████████   | 359/400 [2:54:46<18:44, 27.43s/it]2022-01-08 12:22:57,755 iteration 6104 : loss : 0.020904, loss_ce: 0.006262
2022-01-08 12:22:59,263 iteration 6105 : loss : 0.014992, loss_ce: 0.006065
2022-01-08 12:23:00,736 iteration 6106 : loss : 0.013412, loss_ce: 0.004449
2022-01-08 12:23:02,240 iteration 6107 : loss : 0.013924, loss_ce: 0.004508
2022-01-08 12:23:03,841 iteration 6108 : loss : 0.020232, loss_ce: 0.006167
2022-01-08 12:23:05,370 iteration 6109 : loss : 0.013247, loss_ce: 0.004744
2022-01-08 12:23:06,942 iteration 6110 : loss : 0.012498, loss_ce: 0.005512
2022-01-08 12:23:08,553 iteration 6111 : loss : 0.026368, loss_ce: 0.010076
2022-01-08 12:23:10,143 iteration 6112 : loss : 0.014259, loss_ce: 0.005033
2022-01-08 12:23:11,765 iteration 6113 : loss : 0.013386, loss_ce: 0.004238
2022-01-08 12:23:13,306 iteration 6114 : loss : 0.015233, loss_ce: 0.006621
2022-01-08 12:23:14,944 iteration 6115 : loss : 0.029848, loss_ce: 0.016133
2022-01-08 12:23:16,535 iteration 6116 : loss : 0.014034, loss_ce: 0.006229
2022-01-08 12:23:18,062 iteration 6117 : loss : 0.016180, loss_ce: 0.006934
2022-01-08 12:23:19,661 iteration 6118 : loss : 0.015349, loss_ce: 0.007104
2022-01-08 12:23:21,206 iteration 6119 : loss : 0.015583, loss_ce: 0.004827
2022-01-08 12:23:21,206 Training Data Eval:
2022-01-08 12:23:29,090   Average segmentation loss on training set: 0.0084
2022-01-08 12:23:29,090 Validation Data Eval:
2022-01-08 12:23:31,807   Average segmentation loss on validation set: 0.0673
2022-01-08 12:23:33,353 iteration 6120 : loss : 0.015355, loss_ce: 0.007243
 90%|██████████████████████████   | 360/400 [2:55:23<20:15, 30.38s/it]2022-01-08 12:23:34,970 iteration 6121 : loss : 0.013586, loss_ce: 0.005959
2022-01-08 12:23:36,575 iteration 6122 : loss : 0.023618, loss_ce: 0.009786
2022-01-08 12:23:38,212 iteration 6123 : loss : 0.020131, loss_ce: 0.007718
2022-01-08 12:23:39,787 iteration 6124 : loss : 0.013906, loss_ce: 0.004319
2022-01-08 12:23:41,442 iteration 6125 : loss : 0.022057, loss_ce: 0.008524
2022-01-08 12:23:42,993 iteration 6126 : loss : 0.019730, loss_ce: 0.005101
2022-01-08 12:23:44,530 iteration 6127 : loss : 0.016978, loss_ce: 0.006796
2022-01-08 12:23:46,163 iteration 6128 : loss : 0.013467, loss_ce: 0.004290
2022-01-08 12:23:47,730 iteration 6129 : loss : 0.014677, loss_ce: 0.005736
2022-01-08 12:23:49,377 iteration 6130 : loss : 0.016350, loss_ce: 0.007105
2022-01-08 12:23:50,961 iteration 6131 : loss : 0.014227, loss_ce: 0.007575
2022-01-08 12:23:52,538 iteration 6132 : loss : 0.016418, loss_ce: 0.003378
2022-01-08 12:23:54,063 iteration 6133 : loss : 0.012018, loss_ce: 0.006429
2022-01-08 12:23:55,549 iteration 6134 : loss : 0.009415, loss_ce: 0.003944
2022-01-08 12:23:57,041 iteration 6135 : loss : 0.009796, loss_ce: 0.003781
2022-01-08 12:23:58,616 iteration 6136 : loss : 0.011699, loss_ce: 0.003724
2022-01-08 12:24:00,156 iteration 6137 : loss : 0.010778, loss_ce: 0.004055
 90%|██████████████████████████▏  | 361/400 [2:55:50<19:02, 29.30s/it]2022-01-08 12:24:01,710 iteration 6138 : loss : 0.010589, loss_ce: 0.003690
2022-01-08 12:24:03,305 iteration 6139 : loss : 0.014233, loss_ce: 0.006308
2022-01-08 12:24:04,857 iteration 6140 : loss : 0.010411, loss_ce: 0.003729
2022-01-08 12:24:06,448 iteration 6141 : loss : 0.012421, loss_ce: 0.004850
2022-01-08 12:24:08,159 iteration 6142 : loss : 0.023485, loss_ce: 0.008941
2022-01-08 12:24:09,664 iteration 6143 : loss : 0.016341, loss_ce: 0.008532
2022-01-08 12:24:11,238 iteration 6144 : loss : 0.013060, loss_ce: 0.005677
2022-01-08 12:24:12,816 iteration 6145 : loss : 0.014250, loss_ce: 0.005726
2022-01-08 12:24:14,373 iteration 6146 : loss : 0.012537, loss_ce: 0.005652
2022-01-08 12:24:15,927 iteration 6147 : loss : 0.011790, loss_ce: 0.003019
2022-01-08 12:24:17,436 iteration 6148 : loss : 0.009594, loss_ce: 0.003018
2022-01-08 12:24:18,943 iteration 6149 : loss : 0.035877, loss_ce: 0.024796
2022-01-08 12:24:20,474 iteration 6150 : loss : 0.021394, loss_ce: 0.006528
2022-01-08 12:24:22,025 iteration 6151 : loss : 0.015979, loss_ce: 0.003633
2022-01-08 12:24:23,537 iteration 6152 : loss : 0.015540, loss_ce: 0.005805
2022-01-08 12:24:25,152 iteration 6153 : loss : 0.017325, loss_ce: 0.007588
2022-01-08 12:24:26,804 iteration 6154 : loss : 0.019655, loss_ce: 0.008605
 90%|██████████████████████████▏  | 362/400 [2:56:17<18:03, 28.50s/it]2022-01-08 12:24:28,379 iteration 6155 : loss : 0.017205, loss_ce: 0.007554
2022-01-08 12:24:29,927 iteration 6156 : loss : 0.011269, loss_ce: 0.005029
2022-01-08 12:24:31,543 iteration 6157 : loss : 0.017475, loss_ce: 0.006051
2022-01-08 12:24:33,081 iteration 6158 : loss : 0.011989, loss_ce: 0.004189
2022-01-08 12:24:34,555 iteration 6159 : loss : 0.010117, loss_ce: 0.004083
2022-01-08 12:24:36,299 iteration 6160 : loss : 0.024714, loss_ce: 0.012149
2022-01-08 12:24:37,789 iteration 6161 : loss : 0.016874, loss_ce: 0.005263
2022-01-08 12:24:39,370 iteration 6162 : loss : 0.010605, loss_ce: 0.003974
2022-01-08 12:24:40,923 iteration 6163 : loss : 0.011803, loss_ce: 0.004601
2022-01-08 12:24:42,447 iteration 6164 : loss : 0.013671, loss_ce: 0.004145
2022-01-08 12:24:44,016 iteration 6165 : loss : 0.011798, loss_ce: 0.004819
2022-01-08 12:24:45,611 iteration 6166 : loss : 0.014924, loss_ce: 0.006321
2022-01-08 12:24:47,094 iteration 6167 : loss : 0.012242, loss_ce: 0.004485
2022-01-08 12:24:48,766 iteration 6168 : loss : 0.024596, loss_ce: 0.007519
2022-01-08 12:24:50,316 iteration 6169 : loss : 0.013327, loss_ce: 0.005334
2022-01-08 12:24:51,907 iteration 6170 : loss : 0.014433, loss_ce: 0.005936
2022-01-08 12:24:53,462 iteration 6171 : loss : 0.015782, loss_ce: 0.004312
 91%|██████████████████████████▎  | 363/400 [2:56:43<17:14, 27.95s/it]2022-01-08 12:24:55,052 iteration 6172 : loss : 0.015115, loss_ce: 0.005751
2022-01-08 12:24:56,517 iteration 6173 : loss : 0.010383, loss_ce: 0.003746
2022-01-08 12:24:58,049 iteration 6174 : loss : 0.012676, loss_ce: 0.006206
2022-01-08 12:24:59,543 iteration 6175 : loss : 0.008739, loss_ce: 0.002339
2022-01-08 12:25:01,126 iteration 6176 : loss : 0.013986, loss_ce: 0.006837
2022-01-08 12:25:02,798 iteration 6177 : loss : 0.016840, loss_ce: 0.009142
2022-01-08 12:25:04,413 iteration 6178 : loss : 0.012868, loss_ce: 0.004269
2022-01-08 12:25:06,007 iteration 6179 : loss : 0.017246, loss_ce: 0.005692
2022-01-08 12:25:07,598 iteration 6180 : loss : 0.020881, loss_ce: 0.002782
2022-01-08 12:25:09,134 iteration 6181 : loss : 0.014706, loss_ce: 0.005510
2022-01-08 12:25:10,713 iteration 6182 : loss : 0.012803, loss_ce: 0.004914
2022-01-08 12:25:12,233 iteration 6183 : loss : 0.019497, loss_ce: 0.007700
2022-01-08 12:25:13,746 iteration 6184 : loss : 0.013302, loss_ce: 0.005334
2022-01-08 12:25:15,348 iteration 6185 : loss : 0.021029, loss_ce: 0.005669
2022-01-08 12:25:16,870 iteration 6186 : loss : 0.011498, loss_ce: 0.004105
2022-01-08 12:25:18,384 iteration 6187 : loss : 0.020884, loss_ce: 0.007012
2022-01-08 12:25:19,891 iteration 6188 : loss : 0.014474, loss_ce: 0.005511
 91%|██████████████████████████▍  | 364/400 [2:57:10<16:29, 27.50s/it]2022-01-08 12:25:21,568 iteration 6189 : loss : 0.017924, loss_ce: 0.007068
2022-01-08 12:25:23,138 iteration 6190 : loss : 0.012066, loss_ce: 0.004509
2022-01-08 12:25:24,768 iteration 6191 : loss : 0.011764, loss_ce: 0.003656
2022-01-08 12:25:26,365 iteration 6192 : loss : 0.012686, loss_ce: 0.004260
2022-01-08 12:25:27,888 iteration 6193 : loss : 0.010293, loss_ce: 0.003338
2022-01-08 12:25:29,489 iteration 6194 : loss : 0.013116, loss_ce: 0.004821
2022-01-08 12:25:31,119 iteration 6195 : loss : 0.015051, loss_ce: 0.005045
2022-01-08 12:25:32,643 iteration 6196 : loss : 0.014043, loss_ce: 0.004601
2022-01-08 12:25:34,291 iteration 6197 : loss : 0.023061, loss_ce: 0.007717
2022-01-08 12:25:35,943 iteration 6198 : loss : 0.018649, loss_ce: 0.010290
2022-01-08 12:25:37,473 iteration 6199 : loss : 0.010803, loss_ce: 0.004108
2022-01-08 12:25:39,064 iteration 6200 : loss : 0.020086, loss_ce: 0.009915
2022-01-08 12:25:40,628 iteration 6201 : loss : 0.015804, loss_ce: 0.006485
2022-01-08 12:25:42,177 iteration 6202 : loss : 0.010565, loss_ce: 0.003143
2022-01-08 12:25:43,718 iteration 6203 : loss : 0.016769, loss_ce: 0.004979
2022-01-08 12:25:45,322 iteration 6204 : loss : 0.015019, loss_ce: 0.006802
2022-01-08 12:25:45,322 Training Data Eval:
2022-01-08 12:25:53,210   Average segmentation loss on training set: 0.0076
2022-01-08 12:25:53,211 Validation Data Eval:
2022-01-08 12:25:55,928   Average segmentation loss on validation set: 0.0575
2022-01-08 12:26:01,710 Found new lowest validation loss at iteration 6204! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_QUERY_best_val_loss_seed1234.pth
2022-01-08 12:26:03,244 iteration 6205 : loss : 0.021745, loss_ce: 0.010129
 91%|██████████████████████████▍  | 365/400 [2:57:53<18:48, 32.25s/it]2022-01-08 12:26:04,730 iteration 6206 : loss : 0.013511, loss_ce: 0.006171
2022-01-08 12:26:06,264 iteration 6207 : loss : 0.018241, loss_ce: 0.009693
2022-01-08 12:26:07,847 iteration 6208 : loss : 0.014143, loss_ce: 0.005028
2022-01-08 12:26:09,431 iteration 6209 : loss : 0.018320, loss_ce: 0.006578
2022-01-08 12:26:10,970 iteration 6210 : loss : 0.015906, loss_ce: 0.006163
2022-01-08 12:26:12,469 iteration 6211 : loss : 0.015986, loss_ce: 0.006202
2022-01-08 12:26:14,003 iteration 6212 : loss : 0.013452, loss_ce: 0.005272
2022-01-08 12:26:15,560 iteration 6213 : loss : 0.011749, loss_ce: 0.004262
2022-01-08 12:26:17,102 iteration 6214 : loss : 0.012127, loss_ce: 0.004787
2022-01-08 12:26:18,713 iteration 6215 : loss : 0.017781, loss_ce: 0.005755
2022-01-08 12:26:20,262 iteration 6216 : loss : 0.010592, loss_ce: 0.003943
2022-01-08 12:26:21,751 iteration 6217 : loss : 0.013698, loss_ce: 0.004357
2022-01-08 12:26:23,314 iteration 6218 : loss : 0.018632, loss_ce: 0.006982
2022-01-08 12:26:24,861 iteration 6219 : loss : 0.011083, loss_ce: 0.003814
2022-01-08 12:26:26,383 iteration 6220 : loss : 0.010843, loss_ce: 0.002401
2022-01-08 12:26:27,959 iteration 6221 : loss : 0.014166, loss_ce: 0.003889
2022-01-08 12:26:29,471 iteration 6222 : loss : 0.010739, loss_ce: 0.005092
 92%|██████████████████████████▌  | 366/400 [2:58:19<17:15, 30.44s/it]2022-01-08 12:26:31,099 iteration 6223 : loss : 0.016150, loss_ce: 0.006564
2022-01-08 12:26:32,649 iteration 6224 : loss : 0.016685, loss_ce: 0.006911
2022-01-08 12:26:34,216 iteration 6225 : loss : 0.016651, loss_ce: 0.004890
2022-01-08 12:26:35,737 iteration 6226 : loss : 0.011225, loss_ce: 0.003570
2022-01-08 12:26:37,240 iteration 6227 : loss : 0.013462, loss_ce: 0.005206
2022-01-08 12:26:38,806 iteration 6228 : loss : 0.013763, loss_ce: 0.004890
2022-01-08 12:26:40,378 iteration 6229 : loss : 0.009338, loss_ce: 0.004017
2022-01-08 12:26:41,969 iteration 6230 : loss : 0.023576, loss_ce: 0.006527
2022-01-08 12:26:43,577 iteration 6231 : loss : 0.015974, loss_ce: 0.006720
2022-01-08 12:26:45,155 iteration 6232 : loss : 0.010855, loss_ce: 0.004015
2022-01-08 12:26:46,713 iteration 6233 : loss : 0.015078, loss_ce: 0.007462
2022-01-08 12:26:48,250 iteration 6234 : loss : 0.013300, loss_ce: 0.002872
2022-01-08 12:26:49,764 iteration 6235 : loss : 0.015100, loss_ce: 0.008946
2022-01-08 12:26:51,325 iteration 6236 : loss : 0.011832, loss_ce: 0.005843
2022-01-08 12:26:52,904 iteration 6237 : loss : 0.020772, loss_ce: 0.009567
2022-01-08 12:26:54,534 iteration 6238 : loss : 0.018008, loss_ce: 0.005093
2022-01-08 12:26:56,105 iteration 6239 : loss : 0.013680, loss_ce: 0.003927
 92%|██████████████████████████▌  | 367/400 [2:58:46<16:06, 29.30s/it]2022-01-08 12:26:57,692 iteration 6240 : loss : 0.010067, loss_ce: 0.002995
2022-01-08 12:26:59,207 iteration 6241 : loss : 0.010171, loss_ce: 0.004762
2022-01-08 12:27:00,841 iteration 6242 : loss : 0.013211, loss_ce: 0.004222
2022-01-08 12:27:02,387 iteration 6243 : loss : 0.009745, loss_ce: 0.003940
2022-01-08 12:27:04,036 iteration 6244 : loss : 0.011813, loss_ce: 0.004663
2022-01-08 12:27:05,588 iteration 6245 : loss : 0.013303, loss_ce: 0.004787
2022-01-08 12:27:07,132 iteration 6246 : loss : 0.012935, loss_ce: 0.005917
2022-01-08 12:27:08,714 iteration 6247 : loss : 0.022225, loss_ce: 0.005762
2022-01-08 12:27:10,260 iteration 6248 : loss : 0.012969, loss_ce: 0.005358
2022-01-08 12:27:11,807 iteration 6249 : loss : 0.013828, loss_ce: 0.004141
2022-01-08 12:27:13,398 iteration 6250 : loss : 0.013687, loss_ce: 0.004789
2022-01-08 12:27:15,050 iteration 6251 : loss : 0.029445, loss_ce: 0.010081
2022-01-08 12:27:16,572 iteration 6252 : loss : 0.013104, loss_ce: 0.006044
2022-01-08 12:27:18,181 iteration 6253 : loss : 0.014695, loss_ce: 0.004988
2022-01-08 12:27:19,721 iteration 6254 : loss : 0.016357, loss_ce: 0.008567
2022-01-08 12:27:21,299 iteration 6255 : loss : 0.015531, loss_ce: 0.006580
2022-01-08 12:27:22,934 iteration 6256 : loss : 0.015944, loss_ce: 0.006971
 92%|██████████████████████████▋  | 368/400 [2:59:13<15:13, 28.56s/it]2022-01-08 12:27:24,583 iteration 6257 : loss : 0.016663, loss_ce: 0.005857
2022-01-08 12:27:26,197 iteration 6258 : loss : 0.014419, loss_ce: 0.005283
2022-01-08 12:27:27,844 iteration 6259 : loss : 0.016879, loss_ce: 0.007309
2022-01-08 12:27:29,401 iteration 6260 : loss : 0.015193, loss_ce: 0.006136
2022-01-08 12:27:30,955 iteration 6261 : loss : 0.017486, loss_ce: 0.006633
2022-01-08 12:27:32,546 iteration 6262 : loss : 0.014757, loss_ce: 0.004270
2022-01-08 12:27:34,128 iteration 6263 : loss : 0.016698, loss_ce: 0.003250
2022-01-08 12:27:35,716 iteration 6264 : loss : 0.014436, loss_ce: 0.006533
2022-01-08 12:27:37,303 iteration 6265 : loss : 0.014221, loss_ce: 0.005310
2022-01-08 12:27:38,849 iteration 6266 : loss : 0.016925, loss_ce: 0.008730
2022-01-08 12:27:40,384 iteration 6267 : loss : 0.014961, loss_ce: 0.006583
2022-01-08 12:27:41,959 iteration 6268 : loss : 0.013922, loss_ce: 0.005100
2022-01-08 12:27:43,395 iteration 6269 : loss : 0.009864, loss_ce: 0.003357
2022-01-08 12:27:44,940 iteration 6270 : loss : 0.013629, loss_ce: 0.006314
2022-01-08 12:27:46,494 iteration 6271 : loss : 0.013190, loss_ce: 0.003870
2022-01-08 12:27:48,112 iteration 6272 : loss : 0.018002, loss_ce: 0.007751
2022-01-08 12:27:49,714 iteration 6273 : loss : 0.011169, loss_ce: 0.004365
 92%|██████████████████████████▊  | 369/400 [2:59:39<14:28, 28.03s/it]2022-01-08 12:27:51,459 iteration 6274 : loss : 0.016910, loss_ce: 0.005715
2022-01-08 12:27:53,049 iteration 6275 : loss : 0.015003, loss_ce: 0.007433
2022-01-08 12:27:54,629 iteration 6276 : loss : 0.012951, loss_ce: 0.005109
2022-01-08 12:27:56,158 iteration 6277 : loss : 0.011783, loss_ce: 0.003206
2022-01-08 12:27:57,736 iteration 6278 : loss : 0.015832, loss_ce: 0.005335
2022-01-08 12:27:59,289 iteration 6279 : loss : 0.015020, loss_ce: 0.006459
2022-01-08 12:28:00,823 iteration 6280 : loss : 0.028104, loss_ce: 0.009469
2022-01-08 12:28:02,506 iteration 6281 : loss : 0.019761, loss_ce: 0.004935
2022-01-08 12:28:04,058 iteration 6282 : loss : 0.010814, loss_ce: 0.004307
2022-01-08 12:28:05,586 iteration 6283 : loss : 0.014150, loss_ce: 0.006942
2022-01-08 12:28:07,153 iteration 6284 : loss : 0.018267, loss_ce: 0.007487
2022-01-08 12:28:08,758 iteration 6285 : loss : 0.014983, loss_ce: 0.005292
2022-01-08 12:28:10,296 iteration 6286 : loss : 0.022292, loss_ce: 0.006082
2022-01-08 12:28:11,830 iteration 6287 : loss : 0.017378, loss_ce: 0.004920
2022-01-08 12:28:13,308 iteration 6288 : loss : 0.010054, loss_ce: 0.003251
2022-01-08 12:28:14,891 iteration 6289 : loss : 0.011693, loss_ce: 0.005277
2022-01-08 12:28:14,892 Training Data Eval:
2022-01-08 12:28:22,768   Average segmentation loss on training set: 0.0076
2022-01-08 12:28:22,768 Validation Data Eval:
2022-01-08 12:28:25,484   Average segmentation loss on validation set: 0.0632
2022-01-08 12:28:27,095 iteration 6290 : loss : 0.018566, loss_ce: 0.008377
 92%|██████████████████████████▊  | 370/400 [3:00:17<15:24, 30.83s/it]2022-01-08 12:28:28,707 iteration 6291 : loss : 0.010851, loss_ce: 0.004285
2022-01-08 12:28:30,278 iteration 6292 : loss : 0.016031, loss_ce: 0.005560
2022-01-08 12:28:31,870 iteration 6293 : loss : 0.019702, loss_ce: 0.009381
2022-01-08 12:28:33,474 iteration 6294 : loss : 0.019947, loss_ce: 0.005666
2022-01-08 12:28:35,032 iteration 6295 : loss : 0.011634, loss_ce: 0.004694
2022-01-08 12:28:36,667 iteration 6296 : loss : 0.021117, loss_ce: 0.007606
2022-01-08 12:28:38,240 iteration 6297 : loss : 0.018393, loss_ce: 0.006518
2022-01-08 12:28:39,851 iteration 6298 : loss : 0.025940, loss_ce: 0.013509
2022-01-08 12:28:41,408 iteration 6299 : loss : 0.013762, loss_ce: 0.005027
2022-01-08 12:28:42,945 iteration 6300 : loss : 0.013563, loss_ce: 0.004395
2022-01-08 12:28:44,480 iteration 6301 : loss : 0.010924, loss_ce: 0.006350
2022-01-08 12:28:46,063 iteration 6302 : loss : 0.019124, loss_ce: 0.006553
2022-01-08 12:28:47,541 iteration 6303 : loss : 0.011851, loss_ce: 0.002575
2022-01-08 12:28:49,036 iteration 6304 : loss : 0.015057, loss_ce: 0.004059
2022-01-08 12:28:50,614 iteration 6305 : loss : 0.015520, loss_ce: 0.007124
2022-01-08 12:28:52,158 iteration 6306 : loss : 0.021798, loss_ce: 0.005382
2022-01-08 12:28:53,705 iteration 6307 : loss : 0.013217, loss_ce: 0.005558
 93%|██████████████████████████▉  | 371/400 [3:00:43<14:17, 29.57s/it]2022-01-08 12:28:55,261 iteration 6308 : loss : 0.015854, loss_ce: 0.003102
2022-01-08 12:28:56,838 iteration 6309 : loss : 0.018642, loss_ce: 0.006844
2022-01-08 12:28:58,390 iteration 6310 : loss : 0.014079, loss_ce: 0.006130
2022-01-08 12:28:59,967 iteration 6311 : loss : 0.011164, loss_ce: 0.004568
2022-01-08 12:29:01,551 iteration 6312 : loss : 0.025518, loss_ce: 0.008851
2022-01-08 12:29:03,182 iteration 6313 : loss : 0.018143, loss_ce: 0.007500
2022-01-08 12:29:04,737 iteration 6314 : loss : 0.013806, loss_ce: 0.004617
2022-01-08 12:29:06,261 iteration 6315 : loss : 0.014216, loss_ce: 0.005063
2022-01-08 12:29:07,814 iteration 6316 : loss : 0.011182, loss_ce: 0.004063
2022-01-08 12:29:09,357 iteration 6317 : loss : 0.009530, loss_ce: 0.002717
2022-01-08 12:29:10,976 iteration 6318 : loss : 0.027445, loss_ce: 0.009457
2022-01-08 12:29:12,563 iteration 6319 : loss : 0.011285, loss_ce: 0.004189
2022-01-08 12:29:14,110 iteration 6320 : loss : 0.012685, loss_ce: 0.004008
2022-01-08 12:29:15,715 iteration 6321 : loss : 0.015407, loss_ce: 0.007825
2022-01-08 12:29:17,337 iteration 6322 : loss : 0.016192, loss_ce: 0.008876
2022-01-08 12:29:19,039 iteration 6323 : loss : 0.014805, loss_ce: 0.004840
2022-01-08 12:29:20,685 iteration 6324 : loss : 0.014566, loss_ce: 0.007056
 93%|██████████████████████████▉  | 372/400 [3:01:10<13:26, 28.79s/it]2022-01-08 12:29:22,322 iteration 6325 : loss : 0.018160, loss_ce: 0.005211
2022-01-08 12:29:23,865 iteration 6326 : loss : 0.016675, loss_ce: 0.007468
2022-01-08 12:29:25,420 iteration 6327 : loss : 0.013747, loss_ce: 0.004679
2022-01-08 12:29:27,038 iteration 6328 : loss : 0.011611, loss_ce: 0.005038
2022-01-08 12:29:28,534 iteration 6329 : loss : 0.012638, loss_ce: 0.006302
2022-01-08 12:29:30,084 iteration 6330 : loss : 0.015974, loss_ce: 0.006129
2022-01-08 12:29:31,708 iteration 6331 : loss : 0.011134, loss_ce: 0.004313
2022-01-08 12:29:33,309 iteration 6332 : loss : 0.010079, loss_ce: 0.002785
2022-01-08 12:29:34,876 iteration 6333 : loss : 0.010394, loss_ce: 0.004650
2022-01-08 12:29:36,447 iteration 6334 : loss : 0.018493, loss_ce: 0.004819
2022-01-08 12:29:38,108 iteration 6335 : loss : 0.010508, loss_ce: 0.004087
2022-01-08 12:29:39,653 iteration 6336 : loss : 0.015440, loss_ce: 0.006213
2022-01-08 12:29:41,246 iteration 6337 : loss : 0.014403, loss_ce: 0.004877
2022-01-08 12:29:42,821 iteration 6338 : loss : 0.022102, loss_ce: 0.005922
2022-01-08 12:29:44,390 iteration 6339 : loss : 0.012552, loss_ce: 0.004035
2022-01-08 12:29:45,968 iteration 6340 : loss : 0.017809, loss_ce: 0.008180
2022-01-08 12:29:47,464 iteration 6341 : loss : 0.009993, loss_ce: 0.003352
 93%|███████████████████████████  | 373/400 [3:01:37<12:41, 28.19s/it]2022-01-08 12:29:49,118 iteration 6342 : loss : 0.015403, loss_ce: 0.004663
2022-01-08 12:29:50,672 iteration 6343 : loss : 0.013361, loss_ce: 0.005688
2022-01-08 12:29:52,214 iteration 6344 : loss : 0.014707, loss_ce: 0.004573
2022-01-08 12:29:53,749 iteration 6345 : loss : 0.018065, loss_ce: 0.002280
2022-01-08 12:29:55,325 iteration 6346 : loss : 0.017094, loss_ce: 0.004709
2022-01-08 12:29:56,896 iteration 6347 : loss : 0.012916, loss_ce: 0.005036
2022-01-08 12:29:58,367 iteration 6348 : loss : 0.009091, loss_ce: 0.003879
2022-01-08 12:29:59,946 iteration 6349 : loss : 0.014304, loss_ce: 0.006087
2022-01-08 12:30:01,623 iteration 6350 : loss : 0.016024, loss_ce: 0.005773
2022-01-08 12:30:03,242 iteration 6351 : loss : 0.011784, loss_ce: 0.005116
2022-01-08 12:30:04,813 iteration 6352 : loss : 0.014531, loss_ce: 0.004504
2022-01-08 12:30:06,385 iteration 6353 : loss : 0.016404, loss_ce: 0.006717
2022-01-08 12:30:07,949 iteration 6354 : loss : 0.013666, loss_ce: 0.006042
2022-01-08 12:30:09,490 iteration 6355 : loss : 0.020020, loss_ce: 0.006535
2022-01-08 12:30:11,112 iteration 6356 : loss : 0.020602, loss_ce: 0.006643
2022-01-08 12:30:12,648 iteration 6357 : loss : 0.014443, loss_ce: 0.005212
2022-01-08 12:30:14,318 iteration 6358 : loss : 0.018942, loss_ce: 0.008761
 94%|███████████████████████████  | 374/400 [3:02:04<12:02, 27.79s/it]2022-01-08 12:30:15,979 iteration 6359 : loss : 0.015722, loss_ce: 0.004218
2022-01-08 12:30:17,493 iteration 6360 : loss : 0.013602, loss_ce: 0.005804
2022-01-08 12:30:19,085 iteration 6361 : loss : 0.015947, loss_ce: 0.005757
2022-01-08 12:30:20,685 iteration 6362 : loss : 0.014918, loss_ce: 0.006142
2022-01-08 12:30:22,306 iteration 6363 : loss : 0.014429, loss_ce: 0.005631
2022-01-08 12:30:23,838 iteration 6364 : loss : 0.010851, loss_ce: 0.002815
2022-01-08 12:30:25,415 iteration 6365 : loss : 0.012821, loss_ce: 0.006908
2022-01-08 12:30:27,070 iteration 6366 : loss : 0.021639, loss_ce: 0.009805
2022-01-08 12:30:28,753 iteration 6367 : loss : 0.026976, loss_ce: 0.012672
2022-01-08 12:30:30,368 iteration 6368 : loss : 0.019396, loss_ce: 0.007958
2022-01-08 12:30:31,942 iteration 6369 : loss : 0.012482, loss_ce: 0.005008
2022-01-08 12:30:33,520 iteration 6370 : loss : 0.017714, loss_ce: 0.006172
2022-01-08 12:30:35,197 iteration 6371 : loss : 0.019226, loss_ce: 0.006296
2022-01-08 12:30:36,738 iteration 6372 : loss : 0.011985, loss_ce: 0.004646
2022-01-08 12:30:38,267 iteration 6373 : loss : 0.012633, loss_ce: 0.004750
2022-01-08 12:30:39,847 iteration 6374 : loss : 0.015836, loss_ce: 0.005924
2022-01-08 12:30:39,847 Training Data Eval:
2022-01-08 12:30:47,730   Average segmentation loss on training set: 0.0072
2022-01-08 12:30:47,730 Validation Data Eval:
2022-01-08 12:30:50,444   Average segmentation loss on validation set: 0.0635
2022-01-08 12:30:52,015 iteration 6375 : loss : 0.019900, loss_ce: 0.006793
 94%|███████████████████████████▏ | 375/400 [3:02:42<12:48, 30.76s/it]2022-01-08 12:30:53,656 iteration 6376 : loss : 0.015926, loss_ce: 0.004820
2022-01-08 12:30:55,233 iteration 6377 : loss : 0.013449, loss_ce: 0.004896
2022-01-08 12:30:56,705 iteration 6378 : loss : 0.011290, loss_ce: 0.004826
2022-01-08 12:30:58,259 iteration 6379 : loss : 0.015723, loss_ce: 0.003776
2022-01-08 12:30:59,790 iteration 6380 : loss : 0.021025, loss_ce: 0.006657
2022-01-08 12:31:01,312 iteration 6381 : loss : 0.009208, loss_ce: 0.002899
2022-01-08 12:31:02,786 iteration 6382 : loss : 0.012467, loss_ce: 0.004253
2022-01-08 12:31:04,385 iteration 6383 : loss : 0.017329, loss_ce: 0.006158
2022-01-08 12:31:05,912 iteration 6384 : loss : 0.013703, loss_ce: 0.005622
2022-01-08 12:31:07,505 iteration 6385 : loss : 0.015171, loss_ce: 0.004495
2022-01-08 12:31:09,117 iteration 6386 : loss : 0.012947, loss_ce: 0.004251
2022-01-08 12:31:10,774 iteration 6387 : loss : 0.022518, loss_ce: 0.010432
2022-01-08 12:31:12,374 iteration 6388 : loss : 0.018232, loss_ce: 0.007212
2022-01-08 12:31:13,831 iteration 6389 : loss : 0.010566, loss_ce: 0.003743
2022-01-08 12:31:15,390 iteration 6390 : loss : 0.012498, loss_ce: 0.005762
2022-01-08 12:31:16,908 iteration 6391 : loss : 0.012220, loss_ce: 0.006746
2022-01-08 12:31:18,532 iteration 6392 : loss : 0.023243, loss_ce: 0.006232
 94%|███████████████████████████▎ | 376/400 [3:03:08<11:47, 29.49s/it]2022-01-08 12:31:20,130 iteration 6393 : loss : 0.014115, loss_ce: 0.005530
2022-01-08 12:31:21,747 iteration 6394 : loss : 0.016080, loss_ce: 0.006242
2022-01-08 12:31:23,295 iteration 6395 : loss : 0.012739, loss_ce: 0.003673
2022-01-08 12:31:24,855 iteration 6396 : loss : 0.015727, loss_ce: 0.004609
2022-01-08 12:31:26,440 iteration 6397 : loss : 0.011792, loss_ce: 0.004258
2022-01-08 12:31:28,033 iteration 6398 : loss : 0.014518, loss_ce: 0.006241
2022-01-08 12:31:29,603 iteration 6399 : loss : 0.017036, loss_ce: 0.005729
2022-01-08 12:31:31,158 iteration 6400 : loss : 0.011121, loss_ce: 0.004585
2022-01-08 12:31:32,760 iteration 6401 : loss : 0.018577, loss_ce: 0.006845
2022-01-08 12:31:34,321 iteration 6402 : loss : 0.012899, loss_ce: 0.006440
2022-01-08 12:31:35,971 iteration 6403 : loss : 0.021378, loss_ce: 0.009873
2022-01-08 12:31:37,520 iteration 6404 : loss : 0.011345, loss_ce: 0.004597
2022-01-08 12:31:39,049 iteration 6405 : loss : 0.015794, loss_ce: 0.004412
2022-01-08 12:31:40,760 iteration 6406 : loss : 0.021245, loss_ce: 0.005542
2022-01-08 12:31:42,359 iteration 6407 : loss : 0.017673, loss_ce: 0.004541
2022-01-08 12:31:43,822 iteration 6408 : loss : 0.009737, loss_ce: 0.003382
2022-01-08 12:31:45,370 iteration 6409 : loss : 0.013059, loss_ce: 0.006389
 94%|███████████████████████████▎ | 377/400 [3:03:35<10:59, 28.69s/it]2022-01-08 12:31:46,936 iteration 6410 : loss : 0.011172, loss_ce: 0.003600
2022-01-08 12:31:48,522 iteration 6411 : loss : 0.017770, loss_ce: 0.007065
2022-01-08 12:31:50,115 iteration 6412 : loss : 0.012965, loss_ce: 0.004312
2022-01-08 12:31:51,663 iteration 6413 : loss : 0.013732, loss_ce: 0.006167
2022-01-08 12:31:53,197 iteration 6414 : loss : 0.014714, loss_ce: 0.006036
2022-01-08 12:31:54,705 iteration 6415 : loss : 0.012203, loss_ce: 0.004149
2022-01-08 12:31:56,300 iteration 6416 : loss : 0.010652, loss_ce: 0.004570
2022-01-08 12:31:57,796 iteration 6417 : loss : 0.011839, loss_ce: 0.004559
2022-01-08 12:31:59,324 iteration 6418 : loss : 0.012222, loss_ce: 0.005101
2022-01-08 12:32:00,969 iteration 6419 : loss : 0.018965, loss_ce: 0.008329
2022-01-08 12:32:02,601 iteration 6420 : loss : 0.015792, loss_ce: 0.003850
2022-01-08 12:32:04,201 iteration 6421 : loss : 0.014747, loss_ce: 0.005526
2022-01-08 12:32:05,772 iteration 6422 : loss : 0.016455, loss_ce: 0.005029
2022-01-08 12:32:07,369 iteration 6423 : loss : 0.013298, loss_ce: 0.005571
2022-01-08 12:32:09,030 iteration 6424 : loss : 0.019210, loss_ce: 0.009954
2022-01-08 12:32:10,583 iteration 6425 : loss : 0.013368, loss_ce: 0.004233
2022-01-08 12:32:12,218 iteration 6426 : loss : 0.020817, loss_ce: 0.007438
 94%|███████████████████████████▍ | 378/400 [3:04:02<10:19, 28.14s/it]2022-01-08 12:32:13,859 iteration 6427 : loss : 0.011542, loss_ce: 0.003564
2022-01-08 12:32:15,433 iteration 6428 : loss : 0.015743, loss_ce: 0.004662
2022-01-08 12:32:17,019 iteration 6429 : loss : 0.014060, loss_ce: 0.004618
2022-01-08 12:32:18,640 iteration 6430 : loss : 0.015028, loss_ce: 0.006270
2022-01-08 12:32:20,145 iteration 6431 : loss : 0.011572, loss_ce: 0.004430
2022-01-08 12:32:21,720 iteration 6432 : loss : 0.011414, loss_ce: 0.004579
2022-01-08 12:32:23,287 iteration 6433 : loss : 0.012082, loss_ce: 0.003681
2022-01-08 12:32:24,822 iteration 6434 : loss : 0.013938, loss_ce: 0.005845
2022-01-08 12:32:26,384 iteration 6435 : loss : 0.018696, loss_ce: 0.006674
2022-01-08 12:32:27,934 iteration 6436 : loss : 0.011248, loss_ce: 0.004685
2022-01-08 12:32:29,510 iteration 6437 : loss : 0.016378, loss_ce: 0.005039
2022-01-08 12:32:31,124 iteration 6438 : loss : 0.012738, loss_ce: 0.003847
2022-01-08 12:32:32,651 iteration 6439 : loss : 0.010220, loss_ce: 0.004209
2022-01-08 12:32:34,262 iteration 6440 : loss : 0.013257, loss_ce: 0.006033
2022-01-08 12:32:35,769 iteration 6441 : loss : 0.008710, loss_ce: 0.003628
2022-01-08 12:32:37,355 iteration 6442 : loss : 0.011888, loss_ce: 0.003789
2022-01-08 12:32:38,851 iteration 6443 : loss : 0.009886, loss_ce: 0.003202
 95%|███████████████████████████▍ | 379/400 [3:04:29<09:41, 27.69s/it]2022-01-08 12:32:40,477 iteration 6444 : loss : 0.011840, loss_ce: 0.005805
2022-01-08 12:32:42,041 iteration 6445 : loss : 0.012321, loss_ce: 0.004497
2022-01-08 12:32:43,619 iteration 6446 : loss : 0.013514, loss_ce: 0.005702
2022-01-08 12:32:45,211 iteration 6447 : loss : 0.020440, loss_ce: 0.003285
2022-01-08 12:32:46,812 iteration 6448 : loss : 0.014763, loss_ce: 0.005327
2022-01-08 12:32:48,352 iteration 6449 : loss : 0.011856, loss_ce: 0.002720
2022-01-08 12:32:49,867 iteration 6450 : loss : 0.012304, loss_ce: 0.005519
2022-01-08 12:32:51,433 iteration 6451 : loss : 0.011765, loss_ce: 0.003857
2022-01-08 12:32:53,065 iteration 6452 : loss : 0.019160, loss_ce: 0.005903
2022-01-08 12:32:54,606 iteration 6453 : loss : 0.013659, loss_ce: 0.004710
2022-01-08 12:32:56,106 iteration 6454 : loss : 0.012345, loss_ce: 0.004604
2022-01-08 12:32:57,705 iteration 6455 : loss : 0.011976, loss_ce: 0.005156
2022-01-08 12:32:59,259 iteration 6456 : loss : 0.012384, loss_ce: 0.004504
2022-01-08 12:33:00,839 iteration 6457 : loss : 0.016664, loss_ce: 0.005562
2022-01-08 12:33:02,349 iteration 6458 : loss : 0.012322, loss_ce: 0.004672
2022-01-08 12:33:03,890 iteration 6459 : loss : 0.007386, loss_ce: 0.001877
2022-01-08 12:33:03,890 Training Data Eval:
2022-01-08 12:33:11,776   Average segmentation loss on training set: 0.0069
2022-01-08 12:33:11,776 Validation Data Eval:
2022-01-08 12:33:14,478   Average segmentation loss on validation set: 0.0710
2022-01-08 12:33:16,083 iteration 6460 : loss : 0.017206, loss_ce: 0.008893
 95%|███████████████████████████▌ | 380/400 [3:05:06<10:11, 30.55s/it]2022-01-08 12:33:17,741 iteration 6461 : loss : 0.015206, loss_ce: 0.006844
2022-01-08 12:33:19,279 iteration 6462 : loss : 0.008964, loss_ce: 0.003713
2022-01-08 12:33:20,894 iteration 6463 : loss : 0.024956, loss_ce: 0.012133
2022-01-08 12:33:22,462 iteration 6464 : loss : 0.017708, loss_ce: 0.004886
2022-01-08 12:33:24,069 iteration 6465 : loss : 0.012651, loss_ce: 0.004532
2022-01-08 12:33:25,588 iteration 6466 : loss : 0.013376, loss_ce: 0.004168
2022-01-08 12:33:27,128 iteration 6467 : loss : 0.012441, loss_ce: 0.004449
2022-01-08 12:33:28,753 iteration 6468 : loss : 0.015540, loss_ce: 0.005302
2022-01-08 12:33:30,302 iteration 6469 : loss : 0.011822, loss_ce: 0.004861
2022-01-08 12:33:31,831 iteration 6470 : loss : 0.012031, loss_ce: 0.003595
2022-01-08 12:33:33,395 iteration 6471 : loss : 0.009959, loss_ce: 0.003198
2022-01-08 12:33:34,970 iteration 6472 : loss : 0.020746, loss_ce: 0.006395
2022-01-08 12:33:36,500 iteration 6473 : loss : 0.010979, loss_ce: 0.003667
2022-01-08 12:33:38,123 iteration 6474 : loss : 0.010755, loss_ce: 0.005339
2022-01-08 12:33:39,706 iteration 6475 : loss : 0.017823, loss_ce: 0.010342
2022-01-08 12:33:41,160 iteration 6476 : loss : 0.011979, loss_ce: 0.004144
2022-01-08 12:33:42,810 iteration 6477 : loss : 0.020461, loss_ce: 0.007793
 95%|███████████████████████████▌ | 381/400 [3:05:33<09:18, 29.40s/it]2022-01-08 12:33:44,471 iteration 6478 : loss : 0.017622, loss_ce: 0.005608
2022-01-08 12:33:46,016 iteration 6479 : loss : 0.016620, loss_ce: 0.005548
2022-01-08 12:33:47,566 iteration 6480 : loss : 0.015609, loss_ce: 0.008550
2022-01-08 12:33:49,157 iteration 6481 : loss : 0.017724, loss_ce: 0.007795
2022-01-08 12:33:50,653 iteration 6482 : loss : 0.009145, loss_ce: 0.003123
2022-01-08 12:33:52,245 iteration 6483 : loss : 0.019895, loss_ce: 0.004996
2022-01-08 12:33:53,767 iteration 6484 : loss : 0.010891, loss_ce: 0.004064
2022-01-08 12:33:55,319 iteration 6485 : loss : 0.015298, loss_ce: 0.006559
2022-01-08 12:33:56,929 iteration 6486 : loss : 0.016019, loss_ce: 0.006796
2022-01-08 12:33:58,596 iteration 6487 : loss : 0.025697, loss_ce: 0.010751
2022-01-08 12:34:00,195 iteration 6488 : loss : 0.016782, loss_ce: 0.006512
2022-01-08 12:34:01,720 iteration 6489 : loss : 0.010212, loss_ce: 0.005091
2022-01-08 12:34:03,301 iteration 6490 : loss : 0.014276, loss_ce: 0.003768
2022-01-08 12:34:04,840 iteration 6491 : loss : 0.010668, loss_ce: 0.003964
2022-01-08 12:34:06,484 iteration 6492 : loss : 0.014243, loss_ce: 0.005485
2022-01-08 12:34:08,071 iteration 6493 : loss : 0.018418, loss_ce: 0.007303
2022-01-08 12:34:09,539 iteration 6494 : loss : 0.007925, loss_ce: 0.003141
 96%|███████████████████████████▋ | 382/400 [3:05:59<08:34, 28.60s/it]2022-01-08 12:34:11,113 iteration 6495 : loss : 0.019881, loss_ce: 0.009774
2022-01-08 12:34:12,678 iteration 6496 : loss : 0.014619, loss_ce: 0.003920
2022-01-08 12:34:14,278 iteration 6497 : loss : 0.012728, loss_ce: 0.004670
2022-01-08 12:34:15,877 iteration 6498 : loss : 0.018812, loss_ce: 0.006029
2022-01-08 12:34:17,475 iteration 6499 : loss : 0.012530, loss_ce: 0.004858
2022-01-08 12:34:19,025 iteration 6500 : loss : 0.010177, loss_ce: 0.003590
2022-01-08 12:34:20,611 iteration 6501 : loss : 0.017381, loss_ce: 0.006204
2022-01-08 12:34:22,228 iteration 6502 : loss : 0.015042, loss_ce: 0.008576
2022-01-08 12:34:23,763 iteration 6503 : loss : 0.008801, loss_ce: 0.002226
2022-01-08 12:34:25,269 iteration 6504 : loss : 0.009185, loss_ce: 0.003204
2022-01-08 12:34:26,708 iteration 6505 : loss : 0.010109, loss_ce: 0.004660
2022-01-08 12:34:28,248 iteration 6506 : loss : 0.011300, loss_ce: 0.003289
2022-01-08 12:34:29,874 iteration 6507 : loss : 0.017651, loss_ce: 0.009226
2022-01-08 12:34:31,472 iteration 6508 : loss : 0.013104, loss_ce: 0.005472
2022-01-08 12:34:33,051 iteration 6509 : loss : 0.018704, loss_ce: 0.007078
2022-01-08 12:34:34,633 iteration 6510 : loss : 0.011686, loss_ce: 0.003793
2022-01-08 12:34:36,119 iteration 6511 : loss : 0.010357, loss_ce: 0.003901
 96%|███████████████████████████▊ | 383/400 [3:06:26<07:55, 28.00s/it]2022-01-08 12:34:37,687 iteration 6512 : loss : 0.016432, loss_ce: 0.005681
2022-01-08 12:34:39,258 iteration 6513 : loss : 0.016569, loss_ce: 0.005605
2022-01-08 12:34:40,930 iteration 6514 : loss : 0.015046, loss_ce: 0.004391
2022-01-08 12:34:42,564 iteration 6515 : loss : 0.021578, loss_ce: 0.007397
2022-01-08 12:34:44,166 iteration 6516 : loss : 0.028076, loss_ce: 0.013203
2022-01-08 12:34:45,722 iteration 6517 : loss : 0.019055, loss_ce: 0.004486
2022-01-08 12:34:47,296 iteration 6518 : loss : 0.011549, loss_ce: 0.004079
2022-01-08 12:34:48,859 iteration 6519 : loss : 0.013894, loss_ce: 0.004285
2022-01-08 12:34:50,466 iteration 6520 : loss : 0.017327, loss_ce: 0.005732
2022-01-08 12:34:51,994 iteration 6521 : loss : 0.014591, loss_ce: 0.006099
2022-01-08 12:34:53,569 iteration 6522 : loss : 0.011221, loss_ce: 0.004834
2022-01-08 12:34:55,062 iteration 6523 : loss : 0.009561, loss_ce: 0.003626
2022-01-08 12:34:56,672 iteration 6524 : loss : 0.016045, loss_ce: 0.007488
2022-01-08 12:34:58,247 iteration 6525 : loss : 0.015609, loss_ce: 0.005923
2022-01-08 12:34:59,814 iteration 6526 : loss : 0.012915, loss_ce: 0.004950
2022-01-08 12:35:01,305 iteration 6527 : loss : 0.011709, loss_ce: 0.005135
2022-01-08 12:35:02,832 iteration 6528 : loss : 0.011156, loss_ce: 0.003406
 96%|███████████████████████████▊ | 384/400 [3:06:53<07:21, 27.61s/it]2022-01-08 12:35:04,484 iteration 6529 : loss : 0.022127, loss_ce: 0.006631
2022-01-08 12:35:06,073 iteration 6530 : loss : 0.013059, loss_ce: 0.004677
2022-01-08 12:35:07,578 iteration 6531 : loss : 0.010635, loss_ce: 0.003041
2022-01-08 12:35:09,170 iteration 6532 : loss : 0.012734, loss_ce: 0.004962
2022-01-08 12:35:10,660 iteration 6533 : loss : 0.008112, loss_ce: 0.003249
2022-01-08 12:35:12,224 iteration 6534 : loss : 0.014976, loss_ce: 0.007533
2022-01-08 12:35:13,801 iteration 6535 : loss : 0.013801, loss_ce: 0.005271
2022-01-08 12:35:15,390 iteration 6536 : loss : 0.013892, loss_ce: 0.004466
2022-01-08 12:35:16,973 iteration 6537 : loss : 0.025073, loss_ce: 0.007700
2022-01-08 12:35:18,568 iteration 6538 : loss : 0.013404, loss_ce: 0.003737
2022-01-08 12:35:20,178 iteration 6539 : loss : 0.012526, loss_ce: 0.005119
2022-01-08 12:35:21,666 iteration 6540 : loss : 0.009387, loss_ce: 0.003967
2022-01-08 12:35:23,192 iteration 6541 : loss : 0.014204, loss_ce: 0.008130
2022-01-08 12:35:24,840 iteration 6542 : loss : 0.010452, loss_ce: 0.002910
2022-01-08 12:35:26,371 iteration 6543 : loss : 0.011618, loss_ce: 0.003880
2022-01-08 12:35:27,969 iteration 6544 : loss : 0.019005, loss_ce: 0.007601
2022-01-08 12:35:27,969 Training Data Eval:
2022-01-08 12:35:35,847   Average segmentation loss on training set: 0.0068
2022-01-08 12:35:35,848 Validation Data Eval:
2022-01-08 12:35:38,568   Average segmentation loss on validation set: 0.0728
2022-01-08 12:35:40,206 iteration 6545 : loss : 0.015643, loss_ce: 0.005879
 96%|███████████████████████████▉ | 385/400 [3:07:30<07:38, 30.54s/it]2022-01-08 12:35:41,952 iteration 6546 : loss : 0.016054, loss_ce: 0.006482
2022-01-08 12:35:43,515 iteration 6547 : loss : 0.014067, loss_ce: 0.005976
2022-01-08 12:35:45,108 iteration 6548 : loss : 0.010943, loss_ce: 0.004125
2022-01-08 12:35:46,670 iteration 6549 : loss : 0.018112, loss_ce: 0.008377
2022-01-08 12:35:48,275 iteration 6550 : loss : 0.015562, loss_ce: 0.005652
2022-01-08 12:35:49,903 iteration 6551 : loss : 0.014071, loss_ce: 0.005824
2022-01-08 12:35:51,572 iteration 6552 : loss : 0.016538, loss_ce: 0.006685
2022-01-08 12:35:53,270 iteration 6553 : loss : 0.019766, loss_ce: 0.006412
2022-01-08 12:35:54,866 iteration 6554 : loss : 0.013615, loss_ce: 0.004909
2022-01-08 12:35:56,442 iteration 6555 : loss : 0.010568, loss_ce: 0.004452
2022-01-08 12:35:58,018 iteration 6556 : loss : 0.016001, loss_ce: 0.003787
2022-01-08 12:35:59,487 iteration 6557 : loss : 0.008060, loss_ce: 0.003325
2022-01-08 12:36:01,040 iteration 6558 : loss : 0.015579, loss_ce: 0.004817
2022-01-08 12:36:02,523 iteration 6559 : loss : 0.010768, loss_ce: 0.003820
2022-01-08 12:36:04,082 iteration 6560 : loss : 0.013276, loss_ce: 0.006107
2022-01-08 12:36:05,685 iteration 6561 : loss : 0.013690, loss_ce: 0.003995
2022-01-08 12:36:07,212 iteration 6562 : loss : 0.012041, loss_ce: 0.004548
 96%|███████████████████████████▉ | 386/400 [3:07:57<06:52, 29.48s/it]2022-01-08 12:36:08,955 iteration 6563 : loss : 0.015024, loss_ce: 0.006276
2022-01-08 12:36:10,511 iteration 6564 : loss : 0.015788, loss_ce: 0.006892
2022-01-08 12:36:12,028 iteration 6565 : loss : 0.017846, loss_ce: 0.004230
2022-01-08 12:36:13,540 iteration 6566 : loss : 0.010686, loss_ce: 0.004142
2022-01-08 12:36:15,025 iteration 6567 : loss : 0.012532, loss_ce: 0.005536
2022-01-08 12:36:16,569 iteration 6568 : loss : 0.011874, loss_ce: 0.005045
2022-01-08 12:36:18,138 iteration 6569 : loss : 0.009904, loss_ce: 0.003616
2022-01-08 12:36:19,653 iteration 6570 : loss : 0.009742, loss_ce: 0.004635
2022-01-08 12:36:21,195 iteration 6571 : loss : 0.017055, loss_ce: 0.005597
2022-01-08 12:36:22,674 iteration 6572 : loss : 0.010349, loss_ce: 0.004493
2022-01-08 12:36:24,297 iteration 6573 : loss : 0.014682, loss_ce: 0.004357
2022-01-08 12:36:25,937 iteration 6574 : loss : 0.010879, loss_ce: 0.004518
2022-01-08 12:36:27,580 iteration 6575 : loss : 0.026912, loss_ce: 0.006320
2022-01-08 12:36:29,110 iteration 6576 : loss : 0.012205, loss_ce: 0.005253
2022-01-08 12:36:30,693 iteration 6577 : loss : 0.032021, loss_ce: 0.005553
2022-01-08 12:36:32,208 iteration 6578 : loss : 0.012069, loss_ce: 0.004424
2022-01-08 12:36:33,778 iteration 6579 : loss : 0.013568, loss_ce: 0.005533
 97%|████████████████████████████ | 387/400 [3:08:24<06:11, 28.60s/it]2022-01-08 12:36:35,391 iteration 6580 : loss : 0.016628, loss_ce: 0.005058
2022-01-08 12:36:36,929 iteration 6581 : loss : 0.012093, loss_ce: 0.003702
2022-01-08 12:36:38,556 iteration 6582 : loss : 0.012646, loss_ce: 0.004013
2022-01-08 12:36:40,125 iteration 6583 : loss : 0.014439, loss_ce: 0.005490
2022-01-08 12:36:41,707 iteration 6584 : loss : 0.013643, loss_ce: 0.005418
2022-01-08 12:36:43,325 iteration 6585 : loss : 0.013423, loss_ce: 0.004319
2022-01-08 12:36:44,917 iteration 6586 : loss : 0.025062, loss_ce: 0.007942
2022-01-08 12:36:46,468 iteration 6587 : loss : 0.011346, loss_ce: 0.004263
2022-01-08 12:36:48,043 iteration 6588 : loss : 0.016542, loss_ce: 0.006864
2022-01-08 12:36:49,548 iteration 6589 : loss : 0.012147, loss_ce: 0.004460
2022-01-08 12:36:51,172 iteration 6590 : loss : 0.015508, loss_ce: 0.007676
2022-01-08 12:36:52,792 iteration 6591 : loss : 0.016090, loss_ce: 0.006024
2022-01-08 12:36:54,330 iteration 6592 : loss : 0.010347, loss_ce: 0.004168
2022-01-08 12:36:55,967 iteration 6593 : loss : 0.030596, loss_ce: 0.008212
2022-01-08 12:36:57,525 iteration 6594 : loss : 0.019990, loss_ce: 0.012056
2022-01-08 12:36:59,064 iteration 6595 : loss : 0.009866, loss_ce: 0.003233
2022-01-08 12:37:00,682 iteration 6596 : loss : 0.025409, loss_ce: 0.009035
 97%|████████████████████████████▏| 388/400 [3:08:50<05:37, 28.10s/it]2022-01-08 12:37:02,281 iteration 6597 : loss : 0.013158, loss_ce: 0.005894
2022-01-08 12:37:03,794 iteration 6598 : loss : 0.020022, loss_ce: 0.004169
2022-01-08 12:37:05,374 iteration 6599 : loss : 0.011935, loss_ce: 0.005002
2022-01-08 12:37:06,826 iteration 6600 : loss : 0.008672, loss_ce: 0.003690
2022-01-08 12:37:08,374 iteration 6601 : loss : 0.013410, loss_ce: 0.004675
2022-01-08 12:37:09,890 iteration 6602 : loss : 0.014224, loss_ce: 0.005081
2022-01-08 12:37:11,444 iteration 6603 : loss : 0.019212, loss_ce: 0.006501
2022-01-08 12:37:12,922 iteration 6604 : loss : 0.007869, loss_ce: 0.003239
2022-01-08 12:37:14,485 iteration 6605 : loss : 0.011184, loss_ce: 0.003783
2022-01-08 12:37:16,074 iteration 6606 : loss : 0.013045, loss_ce: 0.005629
2022-01-08 12:37:17,661 iteration 6607 : loss : 0.016430, loss_ce: 0.007902
2022-01-08 12:37:19,248 iteration 6608 : loss : 0.010363, loss_ce: 0.004217
2022-01-08 12:37:20,765 iteration 6609 : loss : 0.010114, loss_ce: 0.003943
2022-01-08 12:37:22,298 iteration 6610 : loss : 0.011824, loss_ce: 0.004754
2022-01-08 12:37:23,828 iteration 6611 : loss : 0.011258, loss_ce: 0.002626
2022-01-08 12:37:25,432 iteration 6612 : loss : 0.017605, loss_ce: 0.005782
2022-01-08 12:37:26,883 iteration 6613 : loss : 0.010200, loss_ce: 0.003906
 97%|████████████████████████████▏| 389/400 [3:09:17<05:02, 27.53s/it]2022-01-08 12:37:28,487 iteration 6614 : loss : 0.017926, loss_ce: 0.007018
2022-01-08 12:37:30,063 iteration 6615 : loss : 0.014583, loss_ce: 0.007236
2022-01-08 12:37:31,613 iteration 6616 : loss : 0.015218, loss_ce: 0.004939
2022-01-08 12:37:33,306 iteration 6617 : loss : 0.019410, loss_ce: 0.007323
2022-01-08 12:37:34,893 iteration 6618 : loss : 0.014909, loss_ce: 0.004467
2022-01-08 12:37:36,551 iteration 6619 : loss : 0.021590, loss_ce: 0.007669
2022-01-08 12:37:38,165 iteration 6620 : loss : 0.020952, loss_ce: 0.010405
2022-01-08 12:37:39,713 iteration 6621 : loss : 0.016923, loss_ce: 0.005362
2022-01-08 12:37:41,263 iteration 6622 : loss : 0.015241, loss_ce: 0.007038
2022-01-08 12:37:42,818 iteration 6623 : loss : 0.012111, loss_ce: 0.004209
2022-01-08 12:37:44,424 iteration 6624 : loss : 0.023033, loss_ce: 0.010731
2022-01-08 12:37:46,047 iteration 6625 : loss : 0.011617, loss_ce: 0.004625
2022-01-08 12:37:47,602 iteration 6626 : loss : 0.013876, loss_ce: 0.003715
2022-01-08 12:37:49,083 iteration 6627 : loss : 0.010780, loss_ce: 0.003619
2022-01-08 12:37:50,633 iteration 6628 : loss : 0.012170, loss_ce: 0.005096
2022-01-08 12:37:52,180 iteration 6629 : loss : 0.012085, loss_ce: 0.006335
2022-01-08 12:37:52,180 Training Data Eval:
2022-01-08 12:38:00,059   Average segmentation loss on training set: 0.0069
2022-01-08 12:38:00,059 Validation Data Eval:
2022-01-08 12:38:02,777   Average segmentation loss on validation set: 0.0646
2022-01-08 12:38:04,377 iteration 6630 : loss : 0.014691, loss_ce: 0.004219
 98%|████████████████████████████▎| 390/400 [3:09:54<05:05, 30.52s/it]2022-01-08 12:38:06,055 iteration 6631 : loss : 0.018363, loss_ce: 0.007802
2022-01-08 12:38:07,630 iteration 6632 : loss : 0.015403, loss_ce: 0.006255
2022-01-08 12:38:09,200 iteration 6633 : loss : 0.022281, loss_ce: 0.007430
2022-01-08 12:38:10,642 iteration 6634 : loss : 0.010264, loss_ce: 0.003147
2022-01-08 12:38:12,154 iteration 6635 : loss : 0.012563, loss_ce: 0.003035
2022-01-08 12:38:13,767 iteration 6636 : loss : 0.013630, loss_ce: 0.005189
2022-01-08 12:38:15,358 iteration 6637 : loss : 0.008907, loss_ce: 0.003761
2022-01-08 12:38:16,968 iteration 6638 : loss : 0.014571, loss_ce: 0.005239
2022-01-08 12:38:18,619 iteration 6639 : loss : 0.019282, loss_ce: 0.007028
2022-01-08 12:38:20,180 iteration 6640 : loss : 0.017026, loss_ce: 0.004864
2022-01-08 12:38:21,714 iteration 6641 : loss : 0.011148, loss_ce: 0.005511
2022-01-08 12:38:23,364 iteration 6642 : loss : 0.013327, loss_ce: 0.005622
2022-01-08 12:38:24,958 iteration 6643 : loss : 0.015481, loss_ce: 0.006752
2022-01-08 12:38:26,534 iteration 6644 : loss : 0.011624, loss_ce: 0.004078
2022-01-08 12:38:28,040 iteration 6645 : loss : 0.008120, loss_ce: 0.003648
2022-01-08 12:38:29,582 iteration 6646 : loss : 0.015399, loss_ce: 0.005953
2022-01-08 12:38:31,054 iteration 6647 : loss : 0.009654, loss_ce: 0.003231
 98%|████████████████████████████▎| 391/400 [3:10:21<04:24, 29.36s/it]2022-01-08 12:38:32,626 iteration 6648 : loss : 0.014033, loss_ce: 0.004412
2022-01-08 12:38:34,253 iteration 6649 : loss : 0.014248, loss_ce: 0.005848
2022-01-08 12:38:35,754 iteration 6650 : loss : 0.012583, loss_ce: 0.004420
2022-01-08 12:38:37,277 iteration 6651 : loss : 0.010866, loss_ce: 0.003471
2022-01-08 12:38:38,918 iteration 6652 : loss : 0.011253, loss_ce: 0.003740
2022-01-08 12:38:40,538 iteration 6653 : loss : 0.017353, loss_ce: 0.007107
2022-01-08 12:38:42,067 iteration 6654 : loss : 0.012597, loss_ce: 0.005129
2022-01-08 12:38:43,651 iteration 6655 : loss : 0.012471, loss_ce: 0.005096
2022-01-08 12:38:45,247 iteration 6656 : loss : 0.011166, loss_ce: 0.004494
2022-01-08 12:38:46,905 iteration 6657 : loss : 0.015474, loss_ce: 0.006232
2022-01-08 12:38:48,495 iteration 6658 : loss : 0.017280, loss_ce: 0.006716
2022-01-08 12:38:50,106 iteration 6659 : loss : 0.016699, loss_ce: 0.006905
2022-01-08 12:38:51,640 iteration 6660 : loss : 0.012125, loss_ce: 0.004040
2022-01-08 12:38:53,127 iteration 6661 : loss : 0.009339, loss_ce: 0.003400
2022-01-08 12:38:54,704 iteration 6662 : loss : 0.011305, loss_ce: 0.004330
2022-01-08 12:38:56,325 iteration 6663 : loss : 0.014865, loss_ce: 0.005571
2022-01-08 12:38:57,879 iteration 6664 : loss : 0.012214, loss_ce: 0.003882
 98%|████████████████████████████▍| 392/400 [3:10:48<03:48, 28.60s/it]2022-01-08 12:38:59,499 iteration 6665 : loss : 0.014698, loss_ce: 0.005307
2022-01-08 12:39:01,080 iteration 6666 : loss : 0.012535, loss_ce: 0.004373
2022-01-08 12:39:02,610 iteration 6667 : loss : 0.013559, loss_ce: 0.004757
2022-01-08 12:39:04,069 iteration 6668 : loss : 0.008336, loss_ce: 0.003388
2022-01-08 12:39:05,640 iteration 6669 : loss : 0.014527, loss_ce: 0.005109
2022-01-08 12:39:07,138 iteration 6670 : loss : 0.016276, loss_ce: 0.004979
2022-01-08 12:39:08,771 iteration 6671 : loss : 0.015060, loss_ce: 0.006278
2022-01-08 12:39:10,331 iteration 6672 : loss : 0.012393, loss_ce: 0.006073
2022-01-08 12:39:11,971 iteration 6673 : loss : 0.012878, loss_ce: 0.005716
2022-01-08 12:39:13,596 iteration 6674 : loss : 0.015391, loss_ce: 0.005968
2022-01-08 12:39:15,131 iteration 6675 : loss : 0.015122, loss_ce: 0.004485
2022-01-08 12:39:16,621 iteration 6676 : loss : 0.009922, loss_ce: 0.003814
2022-01-08 12:39:18,200 iteration 6677 : loss : 0.010811, loss_ce: 0.002429
2022-01-08 12:39:19,856 iteration 6678 : loss : 0.016022, loss_ce: 0.008237
2022-01-08 12:39:21,449 iteration 6679 : loss : 0.012194, loss_ce: 0.004342
2022-01-08 12:39:22,999 iteration 6680 : loss : 0.017226, loss_ce: 0.006989
2022-01-08 12:39:24,514 iteration 6681 : loss : 0.012322, loss_ce: 0.004141
 98%|████████████████████████████▍| 393/400 [3:11:14<03:16, 28.01s/it]2022-01-08 12:39:26,114 iteration 6682 : loss : 0.012370, loss_ce: 0.003749
2022-01-08 12:39:27,719 iteration 6683 : loss : 0.020276, loss_ce: 0.006345
2022-01-08 12:39:29,307 iteration 6684 : loss : 0.035109, loss_ce: 0.004666
2022-01-08 12:39:30,857 iteration 6685 : loss : 0.011867, loss_ce: 0.004398
2022-01-08 12:39:32,439 iteration 6686 : loss : 0.013121, loss_ce: 0.004283
2022-01-08 12:39:34,025 iteration 6687 : loss : 0.015590, loss_ce: 0.005971
2022-01-08 12:39:35,615 iteration 6688 : loss : 0.013847, loss_ce: 0.005087
2022-01-08 12:39:37,158 iteration 6689 : loss : 0.015058, loss_ce: 0.006425
2022-01-08 12:39:38,798 iteration 6690 : loss : 0.024507, loss_ce: 0.009355
2022-01-08 12:39:40,309 iteration 6691 : loss : 0.010808, loss_ce: 0.004054
2022-01-08 12:39:41,816 iteration 6692 : loss : 0.010580, loss_ce: 0.005583
2022-01-08 12:39:43,394 iteration 6693 : loss : 0.014084, loss_ce: 0.005529
2022-01-08 12:39:44,883 iteration 6694 : loss : 0.008547, loss_ce: 0.003644
2022-01-08 12:39:46,415 iteration 6695 : loss : 0.012439, loss_ce: 0.003298
2022-01-08 12:39:47,964 iteration 6696 : loss : 0.019079, loss_ce: 0.012777
2022-01-08 12:39:49,525 iteration 6697 : loss : 0.015718, loss_ce: 0.005439
2022-01-08 12:39:51,006 iteration 6698 : loss : 0.014828, loss_ce: 0.004978
 98%|████████████████████████████▌| 394/400 [3:11:41<02:45, 27.56s/it]2022-01-08 12:39:52,619 iteration 6699 : loss : 0.013350, loss_ce: 0.004349
2022-01-08 12:39:54,236 iteration 6700 : loss : 0.015268, loss_ce: 0.006415
2022-01-08 12:39:55,877 iteration 6701 : loss : 0.022129, loss_ce: 0.008423
2022-01-08 12:39:57,410 iteration 6702 : loss : 0.012577, loss_ce: 0.003929
2022-01-08 12:39:59,070 iteration 6703 : loss : 0.017186, loss_ce: 0.005272
2022-01-08 12:40:00,643 iteration 6704 : loss : 0.018059, loss_ce: 0.008865
2022-01-08 12:40:02,148 iteration 6705 : loss : 0.011502, loss_ce: 0.003553
2022-01-08 12:40:03,683 iteration 6706 : loss : 0.011567, loss_ce: 0.003838
2022-01-08 12:40:05,236 iteration 6707 : loss : 0.012533, loss_ce: 0.004519
2022-01-08 12:40:06,713 iteration 6708 : loss : 0.008535, loss_ce: 0.002488
2022-01-08 12:40:08,222 iteration 6709 : loss : 0.011082, loss_ce: 0.002947
2022-01-08 12:40:09,790 iteration 6710 : loss : 0.016135, loss_ce: 0.005662
2022-01-08 12:40:11,344 iteration 6711 : loss : 0.014484, loss_ce: 0.005632
2022-01-08 12:40:12,997 iteration 6712 : loss : 0.014141, loss_ce: 0.005474
2022-01-08 12:40:14,527 iteration 6713 : loss : 0.012562, loss_ce: 0.004114
2022-01-08 12:40:16,076 iteration 6714 : loss : 0.014606, loss_ce: 0.006383
2022-01-08 12:40:16,076 Training Data Eval:
2022-01-08 12:40:23,940   Average segmentation loss on training set: 0.0070
2022-01-08 12:40:23,941 Validation Data Eval:
2022-01-08 12:40:26,653   Average segmentation loss on validation set: 0.0676
2022-01-08 12:40:28,178 iteration 6715 : loss : 0.007537, loss_ce: 0.002178
 99%|████████████████████████████▋| 395/400 [3:12:18<02:32, 30.44s/it]2022-01-08 12:40:29,705 iteration 6716 : loss : 0.012394, loss_ce: 0.003944
2022-01-08 12:40:31,301 iteration 6717 : loss : 0.014071, loss_ce: 0.003730
2022-01-08 12:40:32,942 iteration 6718 : loss : 0.020916, loss_ce: 0.011130
2022-01-08 12:40:34,564 iteration 6719 : loss : 0.018237, loss_ce: 0.006299
2022-01-08 12:40:36,163 iteration 6720 : loss : 0.020274, loss_ce: 0.003999
2022-01-08 12:40:37,764 iteration 6721 : loss : 0.014546, loss_ce: 0.006323
2022-01-08 12:40:39,310 iteration 6722 : loss : 0.011557, loss_ce: 0.005164
2022-01-08 12:40:40,885 iteration 6723 : loss : 0.013739, loss_ce: 0.005246
2022-01-08 12:40:42,402 iteration 6724 : loss : 0.011848, loss_ce: 0.004869
2022-01-08 12:40:43,960 iteration 6725 : loss : 0.010332, loss_ce: 0.003462
2022-01-08 12:40:45,444 iteration 6726 : loss : 0.008064, loss_ce: 0.002804
2022-01-08 12:40:47,113 iteration 6727 : loss : 0.019172, loss_ce: 0.006712
2022-01-08 12:40:48,637 iteration 6728 : loss : 0.012412, loss_ce: 0.004505
2022-01-08 12:40:50,252 iteration 6729 : loss : 0.022264, loss_ce: 0.006997
2022-01-08 12:40:51,917 iteration 6730 : loss : 0.017047, loss_ce: 0.007114
2022-01-08 12:40:53,526 iteration 6731 : loss : 0.013478, loss_ce: 0.004049
2022-01-08 12:40:55,084 iteration 6732 : loss : 0.010718, loss_ce: 0.005270
 99%|████████████████████████████▋| 396/400 [3:12:45<01:57, 29.38s/it]2022-01-08 12:40:56,635 iteration 6733 : loss : 0.008016, loss_ce: 0.003222
2022-01-08 12:40:58,179 iteration 6734 : loss : 0.012569, loss_ce: 0.003468
2022-01-08 12:40:59,772 iteration 6735 : loss : 0.018264, loss_ce: 0.006058
2022-01-08 12:41:01,317 iteration 6736 : loss : 0.014756, loss_ce: 0.006968
2022-01-08 12:41:02,823 iteration 6737 : loss : 0.012564, loss_ce: 0.006133
2022-01-08 12:41:04,314 iteration 6738 : loss : 0.009613, loss_ce: 0.003225
2022-01-08 12:41:05,873 iteration 6739 : loss : 0.012594, loss_ce: 0.005756
2022-01-08 12:41:07,464 iteration 6740 : loss : 0.020784, loss_ce: 0.006451
2022-01-08 12:41:09,039 iteration 6741 : loss : 0.016878, loss_ce: 0.006162
2022-01-08 12:41:10,583 iteration 6742 : loss : 0.011717, loss_ce: 0.005450
2022-01-08 12:41:12,086 iteration 6743 : loss : 0.010472, loss_ce: 0.002378
2022-01-08 12:41:13,687 iteration 6744 : loss : 0.015448, loss_ce: 0.005983
2022-01-08 12:41:15,246 iteration 6745 : loss : 0.013428, loss_ce: 0.005342
2022-01-08 12:41:16,741 iteration 6746 : loss : 0.008958, loss_ce: 0.003035
2022-01-08 12:41:18,257 iteration 6747 : loss : 0.010654, loss_ce: 0.003614
2022-01-08 12:41:19,837 iteration 6748 : loss : 0.009994, loss_ce: 0.004433
2022-01-08 12:41:21,381 iteration 6749 : loss : 0.012843, loss_ce: 0.004445
 99%|████████████████████████████▊| 397/400 [3:13:11<01:25, 28.45s/it]2022-01-08 12:41:22,974 iteration 6750 : loss : 0.008915, loss_ce: 0.003756
2022-01-08 12:41:24,597 iteration 6751 : loss : 0.010359, loss_ce: 0.003901
2022-01-08 12:41:26,087 iteration 6752 : loss : 0.011605, loss_ce: 0.003635
2022-01-08 12:41:27,645 iteration 6753 : loss : 0.012927, loss_ce: 0.005579
2022-01-08 12:41:29,234 iteration 6754 : loss : 0.010248, loss_ce: 0.003512
2022-01-08 12:41:30,757 iteration 6755 : loss : 0.012352, loss_ce: 0.003991
2022-01-08 12:41:32,280 iteration 6756 : loss : 0.012608, loss_ce: 0.005751
2022-01-08 12:41:33,903 iteration 6757 : loss : 0.012796, loss_ce: 0.003789
2022-01-08 12:41:35,427 iteration 6758 : loss : 0.016056, loss_ce: 0.007947
2022-01-08 12:41:36,995 iteration 6759 : loss : 0.010638, loss_ce: 0.004874
2022-01-08 12:41:38,622 iteration 6760 : loss : 0.024226, loss_ce: 0.004508
2022-01-08 12:41:40,178 iteration 6761 : loss : 0.012929, loss_ce: 0.004972
2022-01-08 12:41:41,717 iteration 6762 : loss : 0.013731, loss_ce: 0.004056
2022-01-08 12:41:43,328 iteration 6763 : loss : 0.014294, loss_ce: 0.005610
2022-01-08 12:41:44,851 iteration 6764 : loss : 0.016557, loss_ce: 0.006439
2022-01-08 12:41:46,465 iteration 6765 : loss : 0.019388, loss_ce: 0.009604
2022-01-08 12:41:47,991 iteration 6766 : loss : 0.009935, loss_ce: 0.004607
100%|████████████████████████████▊| 398/400 [3:13:38<00:55, 27.90s/it]2022-01-08 12:41:49,582 iteration 6767 : loss : 0.011716, loss_ce: 0.004346
2022-01-08 12:41:51,224 iteration 6768 : loss : 0.013249, loss_ce: 0.004511
2022-01-08 12:41:52,805 iteration 6769 : loss : 0.014365, loss_ce: 0.005599
2022-01-08 12:41:54,345 iteration 6770 : loss : 0.014711, loss_ce: 0.007509
2022-01-08 12:41:56,001 iteration 6771 : loss : 0.012574, loss_ce: 0.002429
2022-01-08 12:41:57,533 iteration 6772 : loss : 0.010194, loss_ce: 0.003746
2022-01-08 12:41:58,969 iteration 6773 : loss : 0.009663, loss_ce: 0.002975
2022-01-08 12:42:00,553 iteration 6774 : loss : 0.020120, loss_ce: 0.004882
2022-01-08 12:42:02,141 iteration 6775 : loss : 0.012752, loss_ce: 0.003969
2022-01-08 12:42:03,687 iteration 6776 : loss : 0.013271, loss_ce: 0.003829
2022-01-08 12:42:05,320 iteration 6777 : loss : 0.014189, loss_ce: 0.007156
2022-01-08 12:42:06,875 iteration 6778 : loss : 0.009977, loss_ce: 0.003546
2022-01-08 12:42:08,533 iteration 6779 : loss : 0.018891, loss_ce: 0.007459
2022-01-08 12:42:10,121 iteration 6780 : loss : 0.011856, loss_ce: 0.005444
2022-01-08 12:42:11,741 iteration 6781 : loss : 0.015949, loss_ce: 0.007075
2022-01-08 12:42:13,355 iteration 6782 : loss : 0.013178, loss_ce: 0.005280
2022-01-08 12:42:14,896 iteration 6783 : loss : 0.012478, loss_ce: 0.004243
100%|████████████████████████████▉| 399/400 [3:14:05<00:27, 27.60s/it]2022-01-08 12:42:16,580 iteration 6784 : loss : 0.020209, loss_ce: 0.005854
2022-01-08 12:42:18,177 iteration 6785 : loss : 0.015389, loss_ce: 0.004436
2022-01-08 12:42:19,695 iteration 6786 : loss : 0.011049, loss_ce: 0.004264
2022-01-08 12:42:21,223 iteration 6787 : loss : 0.010975, loss_ce: 0.003647
2022-01-08 12:42:22,804 iteration 6788 : loss : 0.012845, loss_ce: 0.005368
2022-01-08 12:42:24,371 iteration 6789 : loss : 0.014586, loss_ce: 0.004576
2022-01-08 12:42:25,911 iteration 6790 : loss : 0.010810, loss_ce: 0.004306
2022-01-08 12:42:27,498 iteration 6791 : loss : 0.013263, loss_ce: 0.004944
2022-01-08 12:42:29,007 iteration 6792 : loss : 0.010649, loss_ce: 0.004490
2022-01-08 12:42:30,643 iteration 6793 : loss : 0.018420, loss_ce: 0.005823
2022-01-08 12:42:32,203 iteration 6794 : loss : 0.021907, loss_ce: 0.007051
2022-01-08 12:42:33,802 iteration 6795 : loss : 0.012538, loss_ce: 0.004295
2022-01-08 12:42:35,419 iteration 6796 : loss : 0.012723, loss_ce: 0.006948
2022-01-08 12:42:36,901 iteration 6797 : loss : 0.012148, loss_ce: 0.003987
2022-01-08 12:42:38,496 iteration 6798 : loss : 0.011675, loss_ce: 0.003743
2022-01-08 12:42:40,148 iteration 6799 : loss : 0.015898, loss_ce: 0.007679
2022-01-08 12:42:40,148 Training Data Eval:
2022-01-08 12:42:48,022   Average segmentation loss on training set: 0.0067
2022-01-08 12:42:48,022 Validation Data Eval:
2022-01-08 12:42:50,730   Average segmentation loss on validation set: 0.0644
2022-01-08 12:42:52,238 iteration 6800 : loss : 0.009474, loss_ce: 0.002422
100%|█████████████████████████████| 400/400 [3:14:42<00:00, 30.53s/it]100%|█████████████████████████████| 400/400 [3:14:42<00:00, 29.21s/it]
