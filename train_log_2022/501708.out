2022-01-08 00:14:36,666 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-08 00:14:36,667 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-08 00:14:36,667 ============================================================
2022-01-08 00:14:36,667 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-08 00:14:36,667 ============================================================
2022-01-08 00:14:36,667 Loading data...
2022-01-08 00:14:36,667 Reading NCI - RUNMC images...
2022-01-08 00:14:36,667 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-08 00:14:36,668 Already preprocessed this configuration. Loading now!
2022-01-08 00:14:36,686 Training Images: (256, 256, 286)
2022-01-08 00:14:36,686 Training Labels: (256, 256, 286)
2022-01-08 00:14:36,686 Validation Images: (256, 256, 98)
2022-01-08 00:14:36,686 Validation Labels: (256, 256, 98)
2022-01-08 00:14:36,686 ============================================================
2022-01-08 00:14:36,727 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-08 00:14:39,135 iteration 1 : loss : 1.022982, loss_ce: 1.287230
2022-01-08 00:14:40,423 iteration 2 : loss : 0.922262, loss_ce: 1.128664
2022-01-08 00:14:41,806 iteration 3 : loss : 0.846584, loss_ce: 1.007589
2022-01-08 00:14:43,123 iteration 4 : loss : 0.803606, loss_ce: 0.910093
2022-01-08 00:14:44,358 iteration 5 : loss : 0.758395, loss_ce: 0.824287
2022-01-08 00:14:45,650 iteration 6 : loss : 0.714744, loss_ce: 0.767654
2022-01-08 00:14:46,990 iteration 7 : loss : 0.681668, loss_ce: 0.716526
2022-01-08 00:14:48,393 iteration 8 : loss : 0.652344, loss_ce: 0.669564
2022-01-08 00:14:49,640 iteration 9 : loss : 0.643680, loss_ce: 0.626684
2022-01-08 00:14:50,890 iteration 10 : loss : 0.609339, loss_ce: 0.589004
2022-01-08 00:14:52,126 iteration 11 : loss : 0.590387, loss_ce: 0.547389
2022-01-08 00:14:53,423 iteration 12 : loss : 0.566198, loss_ce: 0.524359
2022-01-08 00:14:54,779 iteration 13 : loss : 0.538061, loss_ce: 0.499429
2022-01-08 00:14:56,011 iteration 14 : loss : 0.501807, loss_ce: 0.454702
2022-01-08 00:14:57,397 iteration 15 : loss : 0.492969, loss_ce: 0.428861
2022-01-08 00:14:58,770 iteration 16 : loss : 0.496356, loss_ce: 0.411251
2022-01-08 00:15:00,140 iteration 17 : loss : 0.496350, loss_ce: 0.414765
  0%|                               | 1/400 [00:23<2:36:04, 23.47s/it]2022-01-08 00:15:01,558 iteration 18 : loss : 0.465960, loss_ce: 0.357620
2022-01-08 00:15:02,967 iteration 19 : loss : 0.433275, loss_ce: 0.330619
2022-01-08 00:15:04,364 iteration 20 : loss : 0.418064, loss_ce: 0.315409
2022-01-08 00:15:05,825 iteration 21 : loss : 0.417007, loss_ce: 0.301504
2022-01-08 00:15:07,192 iteration 22 : loss : 0.400867, loss_ce: 0.279945
2022-01-08 00:15:08,591 iteration 23 : loss : 0.373927, loss_ce: 0.249380
2022-01-08 00:15:09,927 iteration 24 : loss : 0.377170, loss_ce: 0.258261
2022-01-08 00:15:11,219 iteration 25 : loss : 0.362697, loss_ce: 0.228694
2022-01-08 00:15:12,550 iteration 26 : loss : 0.375920, loss_ce: 0.221795
2022-01-08 00:15:13,936 iteration 27 : loss : 0.338638, loss_ce: 0.206365
2022-01-08 00:15:15,303 iteration 28 : loss : 0.332985, loss_ce: 0.187087
2022-01-08 00:15:16,823 iteration 29 : loss : 0.338566, loss_ce: 0.200500
2022-01-08 00:15:18,263 iteration 30 : loss : 0.332613, loss_ce: 0.181616
2022-01-08 00:15:19,602 iteration 31 : loss : 0.334022, loss_ce: 0.191740
2022-01-08 00:15:20,961 iteration 32 : loss : 0.320889, loss_ce: 0.177618
2022-01-08 00:15:22,418 iteration 33 : loss : 0.321708, loss_ce: 0.183973
2022-01-08 00:15:23,770 iteration 34 : loss : 0.307326, loss_ce: 0.149159
  0%|▏                              | 2/400 [00:47<2:36:14, 23.55s/it]2022-01-08 00:15:25,239 iteration 35 : loss : 0.303331, loss_ce: 0.169994
2022-01-08 00:15:26,627 iteration 36 : loss : 0.309701, loss_ce: 0.162034
2022-01-08 00:15:28,054 iteration 37 : loss : 0.302578, loss_ce: 0.168493
2022-01-08 00:15:29,549 iteration 38 : loss : 0.266636, loss_ce: 0.124899
2022-01-08 00:15:30,945 iteration 39 : loss : 0.332112, loss_ce: 0.149459
2022-01-08 00:15:32,378 iteration 40 : loss : 0.307027, loss_ce: 0.174380
2022-01-08 00:15:33,713 iteration 41 : loss : 0.286245, loss_ce: 0.127619
2022-01-08 00:15:35,226 iteration 42 : loss : 0.282699, loss_ce: 0.142718
2022-01-08 00:15:36,553 iteration 43 : loss : 0.287808, loss_ce: 0.119825
2022-01-08 00:15:37,962 iteration 44 : loss : 0.269866, loss_ce: 0.121360
2022-01-08 00:15:39,387 iteration 45 : loss : 0.311444, loss_ce: 0.132241
2022-01-08 00:15:40,680 iteration 46 : loss : 0.247624, loss_ce: 0.106138
2022-01-08 00:15:42,050 iteration 47 : loss : 0.281653, loss_ce: 0.104979
2022-01-08 00:15:43,465 iteration 48 : loss : 0.232702, loss_ce: 0.113604
2022-01-08 00:15:44,868 iteration 49 : loss : 0.281559, loss_ce: 0.112644
2022-01-08 00:15:46,299 iteration 50 : loss : 0.262186, loss_ce: 0.115365
2022-01-08 00:15:47,693 iteration 51 : loss : 0.258517, loss_ce: 0.139452
  1%|▏                              | 3/400 [01:11<2:36:57, 23.72s/it]2022-01-08 00:15:49,035 iteration 52 : loss : 0.294017, loss_ce: 0.147080
2022-01-08 00:15:50,333 iteration 53 : loss : 0.233645, loss_ce: 0.116979
2022-01-08 00:15:51,755 iteration 54 : loss : 0.215030, loss_ce: 0.101062
2022-01-08 00:15:53,157 iteration 55 : loss : 0.335420, loss_ce: 0.150268
2022-01-08 00:15:54,590 iteration 56 : loss : 0.240018, loss_ce: 0.101909
2022-01-08 00:15:56,008 iteration 57 : loss : 0.262344, loss_ce: 0.104549
2022-01-08 00:15:57,407 iteration 58 : loss : 0.275254, loss_ce: 0.127748
2022-01-08 00:15:58,775 iteration 59 : loss : 0.259161, loss_ce: 0.102650
2022-01-08 00:16:00,232 iteration 60 : loss : 0.257347, loss_ce: 0.106370
2022-01-08 00:16:01,693 iteration 61 : loss : 0.236992, loss_ce: 0.110177
2022-01-08 00:16:03,079 iteration 62 : loss : 0.249264, loss_ce: 0.106954
2022-01-08 00:16:04,390 iteration 63 : loss : 0.208082, loss_ce: 0.106662
2022-01-08 00:16:05,753 iteration 64 : loss : 0.245534, loss_ce: 0.134295
2022-01-08 00:16:07,121 iteration 65 : loss : 0.260810, loss_ce: 0.113975
2022-01-08 00:16:08,560 iteration 66 : loss : 0.243252, loss_ce: 0.111112
2022-01-08 00:16:09,900 iteration 67 : loss : 0.264129, loss_ce: 0.124459
2022-01-08 00:16:11,270 iteration 68 : loss : 0.293522, loss_ce: 0.111534
  1%|▎                              | 4/400 [01:34<2:36:11, 23.67s/it]2022-01-08 00:16:12,692 iteration 69 : loss : 0.237680, loss_ce: 0.088738
2022-01-08 00:16:14,097 iteration 70 : loss : 0.295158, loss_ce: 0.130486
2022-01-08 00:16:15,435 iteration 71 : loss : 0.222076, loss_ce: 0.090453
2022-01-08 00:16:16,870 iteration 72 : loss : 0.250715, loss_ce: 0.094130
2022-01-08 00:16:18,240 iteration 73 : loss : 0.247541, loss_ce: 0.125525
2022-01-08 00:16:19,711 iteration 74 : loss : 0.274833, loss_ce: 0.122407
2022-01-08 00:16:21,108 iteration 75 : loss : 0.253156, loss_ce: 0.139539
2022-01-08 00:16:22,547 iteration 76 : loss : 0.258259, loss_ce: 0.123882
2022-01-08 00:16:24,017 iteration 77 : loss : 0.256107, loss_ce: 0.104385
2022-01-08 00:16:25,459 iteration 78 : loss : 0.198909, loss_ce: 0.098786
2022-01-08 00:16:26,801 iteration 79 : loss : 0.195799, loss_ce: 0.079147
2022-01-08 00:16:28,237 iteration 80 : loss : 0.280841, loss_ce: 0.110676
2022-01-08 00:16:29,607 iteration 81 : loss : 0.218571, loss_ce: 0.086521
2022-01-08 00:16:30,978 iteration 82 : loss : 0.285931, loss_ce: 0.134917
2022-01-08 00:16:32,422 iteration 83 : loss : 0.231935, loss_ce: 0.088588
2022-01-08 00:16:33,777 iteration 84 : loss : 0.255736, loss_ce: 0.121249
2022-01-08 00:16:33,778 Training Data Eval:
2022-01-08 00:16:40,686   Average segmentation loss on training set: 0.3458
2022-01-08 00:16:40,687 Validation Data Eval:
2022-01-08 00:16:43,059   Average segmentation loss on validation set: 0.4140
2022-01-08 00:16:47,351 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:16:48,707 iteration 85 : loss : 0.199866, loss_ce: 0.093390
  1%|▍                              | 5/400 [02:12<3:08:29, 28.63s/it]2022-01-08 00:16:50,010 iteration 86 : loss : 0.294124, loss_ce: 0.113444
2022-01-08 00:16:51,203 iteration 87 : loss : 0.255372, loss_ce: 0.110437
2022-01-08 00:16:52,453 iteration 88 : loss : 0.210557, loss_ce: 0.078896
2022-01-08 00:16:53,804 iteration 89 : loss : 0.214945, loss_ce: 0.096083
2022-01-08 00:16:55,052 iteration 90 : loss : 0.197291, loss_ce: 0.086449
2022-01-08 00:16:56,358 iteration 91 : loss : 0.230605, loss_ce: 0.099970
2022-01-08 00:16:57,637 iteration 92 : loss : 0.187573, loss_ce: 0.073399
2022-01-08 00:16:59,062 iteration 93 : loss : 0.220689, loss_ce: 0.089941
2022-01-08 00:17:00,503 iteration 94 : loss : 0.210265, loss_ce: 0.107019
2022-01-08 00:17:01,869 iteration 95 : loss : 0.218296, loss_ce: 0.086019
2022-01-08 00:17:03,196 iteration 96 : loss : 0.193905, loss_ce: 0.075234
2022-01-08 00:17:04,526 iteration 97 : loss : 0.156328, loss_ce: 0.068649
2022-01-08 00:17:05,892 iteration 98 : loss : 0.245655, loss_ce: 0.100384
2022-01-08 00:17:07,400 iteration 99 : loss : 0.188828, loss_ce: 0.084877
2022-01-08 00:17:08,908 iteration 100 : loss : 0.246613, loss_ce: 0.107433
2022-01-08 00:17:10,257 iteration 101 : loss : 0.265149, loss_ce: 0.124156
2022-01-08 00:17:11,646 iteration 102 : loss : 0.227042, loss_ce: 0.086742
  2%|▍                              | 6/400 [02:34<2:55:19, 26.70s/it]2022-01-08 00:17:13,078 iteration 103 : loss : 0.218329, loss_ce: 0.091737
2022-01-08 00:17:14,502 iteration 104 : loss : 0.235937, loss_ce: 0.089349
2022-01-08 00:17:15,907 iteration 105 : loss : 0.194776, loss_ce: 0.083440
2022-01-08 00:17:17,321 iteration 106 : loss : 0.204889, loss_ce: 0.081392
2022-01-08 00:17:18,786 iteration 107 : loss : 0.191070, loss_ce: 0.082161
2022-01-08 00:17:20,166 iteration 108 : loss : 0.242010, loss_ce: 0.112774
2022-01-08 00:17:21,553 iteration 109 : loss : 0.158084, loss_ce: 0.063232
2022-01-08 00:17:22,988 iteration 110 : loss : 0.214756, loss_ce: 0.105265
2022-01-08 00:17:24,549 iteration 111 : loss : 0.185009, loss_ce: 0.070665
2022-01-08 00:17:25,901 iteration 112 : loss : 0.243962, loss_ce: 0.092289
2022-01-08 00:17:27,392 iteration 113 : loss : 0.167471, loss_ce: 0.062643
2022-01-08 00:17:28,791 iteration 114 : loss : 0.176283, loss_ce: 0.058622
2022-01-08 00:17:30,104 iteration 115 : loss : 0.157261, loss_ce: 0.062103
2022-01-08 00:17:31,498 iteration 116 : loss : 0.219063, loss_ce: 0.101227
2022-01-08 00:17:32,861 iteration 117 : loss : 0.146216, loss_ce: 0.066487
2022-01-08 00:17:34,301 iteration 118 : loss : 0.242813, loss_ce: 0.096988
2022-01-08 00:17:35,647 iteration 119 : loss : 0.247290, loss_ce: 0.113579
  2%|▌                              | 7/400 [02:58<2:49:05, 25.82s/it]2022-01-08 00:17:37,131 iteration 120 : loss : 0.241059, loss_ce: 0.123214
2022-01-08 00:17:38,554 iteration 121 : loss : 0.269001, loss_ce: 0.119984
2022-01-08 00:17:39,841 iteration 122 : loss : 0.172928, loss_ce: 0.073394
2022-01-08 00:17:41,236 iteration 123 : loss : 0.205918, loss_ce: 0.094475
2022-01-08 00:17:42,593 iteration 124 : loss : 0.185545, loss_ce: 0.073532
2022-01-08 00:17:43,956 iteration 125 : loss : 0.223200, loss_ce: 0.082632
2022-01-08 00:17:45,324 iteration 126 : loss : 0.142628, loss_ce: 0.058874
2022-01-08 00:17:46,708 iteration 127 : loss : 0.245742, loss_ce: 0.105236
2022-01-08 00:17:48,068 iteration 128 : loss : 0.148941, loss_ce: 0.057012
2022-01-08 00:17:49,528 iteration 129 : loss : 0.225120, loss_ce: 0.113042
2022-01-08 00:17:50,810 iteration 130 : loss : 0.204674, loss_ce: 0.092603
2022-01-08 00:17:52,172 iteration 131 : loss : 0.185421, loss_ce: 0.095061
2022-01-08 00:17:53,564 iteration 132 : loss : 0.157643, loss_ce: 0.067336
2022-01-08 00:17:54,952 iteration 133 : loss : 0.286763, loss_ce: 0.156704
2022-01-08 00:17:56,316 iteration 134 : loss : 0.194757, loss_ce: 0.077065
2022-01-08 00:17:57,670 iteration 135 : loss : 0.171829, loss_ce: 0.059041
2022-01-08 00:17:59,130 iteration 136 : loss : 0.173675, loss_ce: 0.075442
  2%|▌                              | 8/400 [03:22<2:43:48, 25.07s/it]2022-01-08 00:18:00,622 iteration 137 : loss : 0.168127, loss_ce: 0.060865
2022-01-08 00:18:01,937 iteration 138 : loss : 0.167073, loss_ce: 0.063618
2022-01-08 00:18:03,395 iteration 139 : loss : 0.197800, loss_ce: 0.058976
2022-01-08 00:18:04,750 iteration 140 : loss : 0.247555, loss_ce: 0.112409
2022-01-08 00:18:06,078 iteration 141 : loss : 0.244438, loss_ce: 0.105729
2022-01-08 00:18:07,448 iteration 142 : loss : 0.181928, loss_ce: 0.067640
2022-01-08 00:18:08,837 iteration 143 : loss : 0.180424, loss_ce: 0.070007
2022-01-08 00:18:10,201 iteration 144 : loss : 0.184131, loss_ce: 0.063708
2022-01-08 00:18:11,603 iteration 145 : loss : 0.195301, loss_ce: 0.101498
2022-01-08 00:18:13,036 iteration 146 : loss : 0.160489, loss_ce: 0.068189
2022-01-08 00:18:14,418 iteration 147 : loss : 0.235187, loss_ce: 0.115845
2022-01-08 00:18:15,876 iteration 148 : loss : 0.179640, loss_ce: 0.078954
2022-01-08 00:18:17,316 iteration 149 : loss : 0.170978, loss_ce: 0.078580
2022-01-08 00:18:18,757 iteration 150 : loss : 0.173792, loss_ce: 0.077255
2022-01-08 00:18:20,072 iteration 151 : loss : 0.164418, loss_ce: 0.090663
2022-01-08 00:18:21,353 iteration 152 : loss : 0.110313, loss_ce: 0.051592
2022-01-08 00:18:22,713 iteration 153 : loss : 0.148004, loss_ce: 0.063276
  2%|▋                              | 9/400 [03:46<2:40:21, 24.61s/it]2022-01-08 00:18:24,184 iteration 154 : loss : 0.206204, loss_ce: 0.092843
2022-01-08 00:18:25,559 iteration 155 : loss : 0.144831, loss_ce: 0.077387
2022-01-08 00:18:26,902 iteration 156 : loss : 0.176142, loss_ce: 0.080176
2022-01-08 00:18:28,241 iteration 157 : loss : 0.161905, loss_ce: 0.071055
2022-01-08 00:18:29,697 iteration 158 : loss : 0.163206, loss_ce: 0.067189
2022-01-08 00:18:31,140 iteration 159 : loss : 0.128335, loss_ce: 0.052795
2022-01-08 00:18:32,498 iteration 160 : loss : 0.147301, loss_ce: 0.057813
2022-01-08 00:18:33,971 iteration 161 : loss : 0.150792, loss_ce: 0.067046
2022-01-08 00:18:35,372 iteration 162 : loss : 0.170479, loss_ce: 0.055416
2022-01-08 00:18:36,780 iteration 163 : loss : 0.196559, loss_ce: 0.081939
2022-01-08 00:18:38,172 iteration 164 : loss : 0.153493, loss_ce: 0.057752
2022-01-08 00:18:39,498 iteration 165 : loss : 0.121366, loss_ce: 0.048609
2022-01-08 00:18:40,934 iteration 166 : loss : 0.212466, loss_ce: 0.122427
2022-01-08 00:18:42,434 iteration 167 : loss : 0.212429, loss_ce: 0.090937
2022-01-08 00:18:43,801 iteration 168 : loss : 0.191784, loss_ce: 0.070967
2022-01-08 00:18:45,199 iteration 169 : loss : 0.176130, loss_ce: 0.071622
2022-01-08 00:18:45,200 Training Data Eval:
2022-01-08 00:18:52,056   Average segmentation loss on training set: 0.2683
2022-01-08 00:18:52,056 Validation Data Eval:
2022-01-08 00:18:54,423   Average segmentation loss on validation set: 0.2705
2022-01-08 00:18:58,526 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:18:59,925 iteration 170 : loss : 0.171897, loss_ce: 0.075527
  2%|▊                             | 10/400 [04:23<3:05:13, 28.50s/it]2022-01-08 00:19:01,322 iteration 171 : loss : 0.134102, loss_ce: 0.059257
2022-01-08 00:19:02,561 iteration 172 : loss : 0.152487, loss_ce: 0.073275
2022-01-08 00:19:03,921 iteration 173 : loss : 0.174526, loss_ce: 0.066951
2022-01-08 00:19:05,129 iteration 174 : loss : 0.157376, loss_ce: 0.079072
2022-01-08 00:19:06,385 iteration 175 : loss : 0.183805, loss_ce: 0.068072
2022-01-08 00:19:07,643 iteration 176 : loss : 0.148975, loss_ce: 0.059407
2022-01-08 00:19:08,965 iteration 177 : loss : 0.135441, loss_ce: 0.057882
2022-01-08 00:19:10,265 iteration 178 : loss : 0.185070, loss_ce: 0.073517
2022-01-08 00:19:11,538 iteration 179 : loss : 0.128525, loss_ce: 0.047234
2022-01-08 00:19:13,021 iteration 180 : loss : 0.162390, loss_ce: 0.072338
2022-01-08 00:19:14,464 iteration 181 : loss : 0.155352, loss_ce: 0.067227
2022-01-08 00:19:15,858 iteration 182 : loss : 0.148384, loss_ce: 0.073239
2022-01-08 00:19:17,273 iteration 183 : loss : 0.186164, loss_ce: 0.063942
2022-01-08 00:19:18,812 iteration 184 : loss : 0.205940, loss_ce: 0.080013
2022-01-08 00:19:20,345 iteration 185 : loss : 0.155112, loss_ce: 0.074129
2022-01-08 00:19:21,717 iteration 186 : loss : 0.140621, loss_ce: 0.053576
2022-01-08 00:19:23,136 iteration 187 : loss : 0.105416, loss_ce: 0.048160
  3%|▊                             | 11/400 [04:46<2:54:16, 26.88s/it]2022-01-08 00:19:24,539 iteration 188 : loss : 0.148877, loss_ce: 0.063816
2022-01-08 00:19:26,018 iteration 189 : loss : 0.160942, loss_ce: 0.064265
2022-01-08 00:19:27,429 iteration 190 : loss : 0.176265, loss_ce: 0.084768
2022-01-08 00:19:28,844 iteration 191 : loss : 0.122829, loss_ce: 0.051081
2022-01-08 00:19:30,236 iteration 192 : loss : 0.140385, loss_ce: 0.056130
2022-01-08 00:19:31,555 iteration 193 : loss : 0.151456, loss_ce: 0.069793
2022-01-08 00:19:32,981 iteration 194 : loss : 0.134236, loss_ce: 0.054650
2022-01-08 00:19:34,413 iteration 195 : loss : 0.135373, loss_ce: 0.053001
2022-01-08 00:19:35,785 iteration 196 : loss : 0.116840, loss_ce: 0.054789
2022-01-08 00:19:37,214 iteration 197 : loss : 0.133290, loss_ce: 0.050973
2022-01-08 00:19:38,621 iteration 198 : loss : 0.128568, loss_ce: 0.046061
2022-01-08 00:19:39,904 iteration 199 : loss : 0.170766, loss_ce: 0.054518
2022-01-08 00:19:41,265 iteration 200 : loss : 0.138887, loss_ce: 0.064775
2022-01-08 00:19:42,777 iteration 201 : loss : 0.175906, loss_ce: 0.072244
2022-01-08 00:19:44,120 iteration 202 : loss : 0.140665, loss_ce: 0.056364
2022-01-08 00:19:45,463 iteration 203 : loss : 0.166358, loss_ce: 0.082597
2022-01-08 00:19:46,775 iteration 204 : loss : 0.116668, loss_ce: 0.051346
  3%|▉                             | 12/400 [05:10<2:47:27, 25.90s/it]2022-01-08 00:19:48,138 iteration 205 : loss : 0.147931, loss_ce: 0.067072
2022-01-08 00:19:49,556 iteration 206 : loss : 0.211274, loss_ce: 0.102188
2022-01-08 00:19:51,045 iteration 207 : loss : 0.141342, loss_ce: 0.063637
2022-01-08 00:19:52,524 iteration 208 : loss : 0.125200, loss_ce: 0.047261
2022-01-08 00:19:53,848 iteration 209 : loss : 0.117639, loss_ce: 0.046542
2022-01-08 00:19:55,162 iteration 210 : loss : 0.106568, loss_ce: 0.037329
2022-01-08 00:19:56,543 iteration 211 : loss : 0.155672, loss_ce: 0.056982
2022-01-08 00:19:57,842 iteration 212 : loss : 0.124669, loss_ce: 0.048019
2022-01-08 00:19:59,276 iteration 213 : loss : 0.125434, loss_ce: 0.055349
2022-01-08 00:20:00,679 iteration 214 : loss : 0.195026, loss_ce: 0.064173
2022-01-08 00:20:01,998 iteration 215 : loss : 0.151595, loss_ce: 0.061307
2022-01-08 00:20:03,437 iteration 216 : loss : 0.156767, loss_ce: 0.065578
2022-01-08 00:20:04,770 iteration 217 : loss : 0.125527, loss_ce: 0.056931
2022-01-08 00:20:06,130 iteration 218 : loss : 0.113286, loss_ce: 0.049711
2022-01-08 00:20:07,504 iteration 219 : loss : 0.148985, loss_ce: 0.052780
2022-01-08 00:20:08,826 iteration 220 : loss : 0.136233, loss_ce: 0.070287
2022-01-08 00:20:10,193 iteration 221 : loss : 0.119272, loss_ce: 0.061621
  3%|▉                             | 13/400 [05:33<2:42:11, 25.15s/it]2022-01-08 00:20:11,738 iteration 222 : loss : 0.140151, loss_ce: 0.054863
2022-01-08 00:20:13,156 iteration 223 : loss : 0.081743, loss_ce: 0.036693
2022-01-08 00:20:14,443 iteration 224 : loss : 0.131848, loss_ce: 0.064340
2022-01-08 00:20:15,868 iteration 225 : loss : 0.122155, loss_ce: 0.047739
2022-01-08 00:20:17,299 iteration 226 : loss : 0.219952, loss_ce: 0.088423
2022-01-08 00:20:18,720 iteration 227 : loss : 0.149915, loss_ce: 0.054583
2022-01-08 00:20:20,121 iteration 228 : loss : 0.160238, loss_ce: 0.058996
2022-01-08 00:20:21,510 iteration 229 : loss : 0.125061, loss_ce: 0.058429
2022-01-08 00:20:22,886 iteration 230 : loss : 0.222710, loss_ce: 0.109301
2022-01-08 00:20:24,237 iteration 231 : loss : 0.205112, loss_ce: 0.068716
2022-01-08 00:20:25,708 iteration 232 : loss : 0.152964, loss_ce: 0.082342
2022-01-08 00:20:27,155 iteration 233 : loss : 0.140125, loss_ce: 0.072398
2022-01-08 00:20:28,574 iteration 234 : loss : 0.169285, loss_ce: 0.054615
2022-01-08 00:20:29,943 iteration 235 : loss : 0.123104, loss_ce: 0.048325
2022-01-08 00:20:31,306 iteration 236 : loss : 0.191740, loss_ce: 0.080370
2022-01-08 00:20:32,664 iteration 237 : loss : 0.093848, loss_ce: 0.042263
2022-01-08 00:20:33,953 iteration 238 : loss : 0.141598, loss_ce: 0.071961
  4%|█                             | 14/400 [05:57<2:39:03, 24.72s/it]2022-01-08 00:20:35,338 iteration 239 : loss : 0.143945, loss_ce: 0.054608
2022-01-08 00:20:36,793 iteration 240 : loss : 0.172844, loss_ce: 0.076180
2022-01-08 00:20:38,195 iteration 241 : loss : 0.201547, loss_ce: 0.062701
2022-01-08 00:20:39,619 iteration 242 : loss : 0.133120, loss_ce: 0.053678
2022-01-08 00:20:41,061 iteration 243 : loss : 0.158594, loss_ce: 0.070046
2022-01-08 00:20:42,480 iteration 244 : loss : 0.144034, loss_ce: 0.065381
2022-01-08 00:20:43,778 iteration 245 : loss : 0.120311, loss_ce: 0.040807
2022-01-08 00:20:45,168 iteration 246 : loss : 0.148167, loss_ce: 0.061805
2022-01-08 00:20:46,499 iteration 247 : loss : 0.108769, loss_ce: 0.048900
2022-01-08 00:20:47,790 iteration 248 : loss : 0.117437, loss_ce: 0.042890
2022-01-08 00:20:49,282 iteration 249 : loss : 0.136621, loss_ce: 0.047807
2022-01-08 00:20:50,715 iteration 250 : loss : 0.168324, loss_ce: 0.080309
2022-01-08 00:20:52,083 iteration 251 : loss : 0.086024, loss_ce: 0.037487
2022-01-08 00:20:53,473 iteration 252 : loss : 0.129370, loss_ce: 0.064750
2022-01-08 00:20:54,815 iteration 253 : loss : 0.145817, loss_ce: 0.051925
2022-01-08 00:20:56,153 iteration 254 : loss : 0.119007, loss_ce: 0.050412
2022-01-08 00:20:56,153 Training Data Eval:
2022-01-08 00:21:03,079   Average segmentation loss on training set: 0.3260
2022-01-08 00:21:03,080 Validation Data Eval:
2022-01-08 00:21:05,453   Average segmentation loss on validation set: 0.3879
2022-01-08 00:21:06,832 iteration 255 : loss : 0.112367, loss_ce: 0.045221
  4%|█▏                            | 15/400 [06:30<2:54:25, 27.18s/it]2022-01-08 00:21:08,261 iteration 256 : loss : 0.159441, loss_ce: 0.061662
2022-01-08 00:21:09,635 iteration 257 : loss : 0.147162, loss_ce: 0.082262
2022-01-08 00:21:11,151 iteration 258 : loss : 0.173179, loss_ce: 0.063647
2022-01-08 00:21:12,482 iteration 259 : loss : 0.163393, loss_ce: 0.063582
2022-01-08 00:21:13,888 iteration 260 : loss : 0.158227, loss_ce: 0.071899
2022-01-08 00:21:15,300 iteration 261 : loss : 0.144972, loss_ce: 0.065012
2022-01-08 00:21:16,693 iteration 262 : loss : 0.109697, loss_ce: 0.058500
2022-01-08 00:21:18,025 iteration 263 : loss : 0.177690, loss_ce: 0.096774
2022-01-08 00:21:19,360 iteration 264 : loss : 0.146715, loss_ce: 0.055069
2022-01-08 00:21:20,788 iteration 265 : loss : 0.131606, loss_ce: 0.058311
2022-01-08 00:21:22,179 iteration 266 : loss : 0.166341, loss_ce: 0.052945
2022-01-08 00:21:23,548 iteration 267 : loss : 0.132201, loss_ce: 0.055120
2022-01-08 00:21:24,932 iteration 268 : loss : 0.095337, loss_ce: 0.044535
2022-01-08 00:21:26,286 iteration 269 : loss : 0.121382, loss_ce: 0.050192
2022-01-08 00:21:27,589 iteration 270 : loss : 0.120393, loss_ce: 0.045351
2022-01-08 00:21:29,005 iteration 271 : loss : 0.208399, loss_ce: 0.074327
2022-01-08 00:21:30,467 iteration 272 : loss : 0.148090, loss_ce: 0.060457
  4%|█▏                            | 16/400 [06:53<2:47:09, 26.12s/it]2022-01-08 00:21:31,871 iteration 273 : loss : 0.125015, loss_ce: 0.049101
2022-01-08 00:21:33,261 iteration 274 : loss : 0.160908, loss_ce: 0.064172
2022-01-08 00:21:34,638 iteration 275 : loss : 0.120251, loss_ce: 0.054031
2022-01-08 00:21:35,968 iteration 276 : loss : 0.120408, loss_ce: 0.055197
2022-01-08 00:21:37,434 iteration 277 : loss : 0.149072, loss_ce: 0.054679
2022-01-08 00:21:38,742 iteration 278 : loss : 0.225844, loss_ce: 0.076454
2022-01-08 00:21:40,122 iteration 279 : loss : 0.095458, loss_ce: 0.039379
2022-01-08 00:21:41,406 iteration 280 : loss : 0.094758, loss_ce: 0.048381
2022-01-08 00:21:42,809 iteration 281 : loss : 0.144743, loss_ce: 0.057022
2022-01-08 00:21:44,228 iteration 282 : loss : 0.132460, loss_ce: 0.055508
2022-01-08 00:21:45,620 iteration 283 : loss : 0.113951, loss_ce: 0.054091
2022-01-08 00:21:46,937 iteration 284 : loss : 0.087221, loss_ce: 0.037789
2022-01-08 00:21:48,322 iteration 285 : loss : 0.112508, loss_ce: 0.046549
2022-01-08 00:21:49,735 iteration 286 : loss : 0.094276, loss_ce: 0.039221
2022-01-08 00:21:51,068 iteration 287 : loss : 0.158697, loss_ce: 0.062237
2022-01-08 00:21:52,435 iteration 288 : loss : 0.078732, loss_ce: 0.037035
2022-01-08 00:21:53,806 iteration 289 : loss : 0.103869, loss_ce: 0.052708
  4%|█▎                            | 17/400 [07:17<2:41:23, 25.28s/it]2022-01-08 00:21:55,234 iteration 290 : loss : 0.110274, loss_ce: 0.048896
2022-01-08 00:21:56,603 iteration 291 : loss : 0.089730, loss_ce: 0.039064
2022-01-08 00:21:57,983 iteration 292 : loss : 0.096897, loss_ce: 0.039157
2022-01-08 00:21:59,505 iteration 293 : loss : 0.175531, loss_ce: 0.084129
2022-01-08 00:22:00,840 iteration 294 : loss : 0.105445, loss_ce: 0.044109
2022-01-08 00:22:02,184 iteration 295 : loss : 0.168160, loss_ce: 0.056567
2022-01-08 00:22:03,545 iteration 296 : loss : 0.124306, loss_ce: 0.054171
2022-01-08 00:22:05,001 iteration 297 : loss : 0.112744, loss_ce: 0.057899
2022-01-08 00:22:06,354 iteration 298 : loss : 0.136132, loss_ce: 0.054051
2022-01-08 00:22:07,753 iteration 299 : loss : 0.147273, loss_ce: 0.051665
2022-01-08 00:22:09,095 iteration 300 : loss : 0.115647, loss_ce: 0.044325
2022-01-08 00:22:10,471 iteration 301 : loss : 0.160418, loss_ce: 0.054604
2022-01-08 00:22:11,951 iteration 302 : loss : 0.166341, loss_ce: 0.079050
2022-01-08 00:22:13,357 iteration 303 : loss : 0.155821, loss_ce: 0.070173
2022-01-08 00:22:14,716 iteration 304 : loss : 0.100602, loss_ce: 0.039591
2022-01-08 00:22:16,095 iteration 305 : loss : 0.098729, loss_ce: 0.039471
2022-01-08 00:22:17,469 iteration 306 : loss : 0.148874, loss_ce: 0.057108
  4%|█▎                            | 18/400 [07:40<2:37:50, 24.79s/it]2022-01-08 00:22:18,862 iteration 307 : loss : 0.107877, loss_ce: 0.048554
2022-01-08 00:22:20,274 iteration 308 : loss : 0.151026, loss_ce: 0.048499
2022-01-08 00:22:21,664 iteration 309 : loss : 0.138029, loss_ce: 0.057562
2022-01-08 00:22:23,004 iteration 310 : loss : 0.103801, loss_ce: 0.040500
2022-01-08 00:22:24,355 iteration 311 : loss : 0.113570, loss_ce: 0.044743
2022-01-08 00:22:25,741 iteration 312 : loss : 0.168064, loss_ce: 0.052021
2022-01-08 00:22:27,166 iteration 313 : loss : 0.103993, loss_ce: 0.040431
2022-01-08 00:22:28,450 iteration 314 : loss : 0.116511, loss_ce: 0.047137
2022-01-08 00:22:29,813 iteration 315 : loss : 0.150285, loss_ce: 0.065300
2022-01-08 00:22:31,269 iteration 316 : loss : 0.140962, loss_ce: 0.060424
2022-01-08 00:22:32,717 iteration 317 : loss : 0.114924, loss_ce: 0.058005
2022-01-08 00:22:34,076 iteration 318 : loss : 0.170637, loss_ce: 0.095839
2022-01-08 00:22:35,449 iteration 319 : loss : 0.078762, loss_ce: 0.035752
2022-01-08 00:22:36,834 iteration 320 : loss : 0.119805, loss_ce: 0.042649
2022-01-08 00:22:38,164 iteration 321 : loss : 0.146025, loss_ce: 0.055672
2022-01-08 00:22:39,497 iteration 322 : loss : 0.094404, loss_ce: 0.043021
2022-01-08 00:22:40,866 iteration 323 : loss : 0.105418, loss_ce: 0.048086
  5%|█▍                            | 19/400 [08:04<2:34:47, 24.38s/it]2022-01-08 00:22:42,294 iteration 324 : loss : 0.068176, loss_ce: 0.030304
2022-01-08 00:22:43,630 iteration 325 : loss : 0.126774, loss_ce: 0.044300
2022-01-08 00:22:44,960 iteration 326 : loss : 0.157882, loss_ce: 0.069792
2022-01-08 00:22:46,287 iteration 327 : loss : 0.121664, loss_ce: 0.053975
2022-01-08 00:22:47,674 iteration 328 : loss : 0.090472, loss_ce: 0.031116
2022-01-08 00:22:49,111 iteration 329 : loss : 0.128226, loss_ce: 0.046563
2022-01-08 00:22:50,448 iteration 330 : loss : 0.116640, loss_ce: 0.053136
2022-01-08 00:22:51,836 iteration 331 : loss : 0.127730, loss_ce: 0.052813
2022-01-08 00:22:53,126 iteration 332 : loss : 0.090556, loss_ce: 0.042046
2022-01-08 00:22:54,563 iteration 333 : loss : 0.143549, loss_ce: 0.068845
2022-01-08 00:22:55,943 iteration 334 : loss : 0.086531, loss_ce: 0.034573
2022-01-08 00:22:57,381 iteration 335 : loss : 0.125047, loss_ce: 0.068473
2022-01-08 00:22:58,746 iteration 336 : loss : 0.113567, loss_ce: 0.052066
2022-01-08 00:23:00,079 iteration 337 : loss : 0.151604, loss_ce: 0.057024
2022-01-08 00:23:01,452 iteration 338 : loss : 0.084320, loss_ce: 0.035483
2022-01-08 00:23:02,799 iteration 339 : loss : 0.107450, loss_ce: 0.051258
2022-01-08 00:23:02,800 Training Data Eval:
2022-01-08 00:23:09,722   Average segmentation loss on training set: 0.2043
2022-01-08 00:23:09,723 Validation Data Eval:
2022-01-08 00:23:12,095   Average segmentation loss on validation set: 0.1727
2022-01-08 00:23:17,849 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:23:19,188 iteration 340 : loss : 0.098808, loss_ce: 0.045634
  5%|█▌                            | 20/400 [08:42<3:00:53, 28.56s/it]2022-01-08 00:23:20,492 iteration 341 : loss : 0.066270, loss_ce: 0.030073
2022-01-08 00:23:21,832 iteration 342 : loss : 0.111876, loss_ce: 0.049765
2022-01-08 00:23:23,155 iteration 343 : loss : 0.112803, loss_ce: 0.049396
2022-01-08 00:23:24,407 iteration 344 : loss : 0.093938, loss_ce: 0.041466
2022-01-08 00:23:25,674 iteration 345 : loss : 0.093916, loss_ce: 0.036201
2022-01-08 00:23:27,007 iteration 346 : loss : 0.078569, loss_ce: 0.035548
2022-01-08 00:23:28,348 iteration 347 : loss : 0.115792, loss_ce: 0.048008
2022-01-08 00:23:29,619 iteration 348 : loss : 0.088153, loss_ce: 0.033654
2022-01-08 00:23:30,897 iteration 349 : loss : 0.103743, loss_ce: 0.035642
2022-01-08 00:23:32,257 iteration 350 : loss : 0.127333, loss_ce: 0.054618
2022-01-08 00:23:33,611 iteration 351 : loss : 0.093625, loss_ce: 0.034872
2022-01-08 00:23:35,101 iteration 352 : loss : 0.097302, loss_ce: 0.056769
2022-01-08 00:23:36,581 iteration 353 : loss : 0.130192, loss_ce: 0.060269
2022-01-08 00:23:37,943 iteration 354 : loss : 0.086247, loss_ce: 0.030909
2022-01-08 00:23:39,361 iteration 355 : loss : 0.068830, loss_ce: 0.024914
2022-01-08 00:23:40,764 iteration 356 : loss : 0.078993, loss_ce: 0.028005
2022-01-08 00:23:42,185 iteration 357 : loss : 0.092896, loss_ce: 0.031503
  5%|█▌                            | 21/400 [09:05<2:49:51, 26.89s/it]2022-01-08 00:23:43,722 iteration 358 : loss : 0.146764, loss_ce: 0.067832
2022-01-08 00:23:45,188 iteration 359 : loss : 0.090360, loss_ce: 0.037425
2022-01-08 00:23:46,537 iteration 360 : loss : 0.090875, loss_ce: 0.031373
2022-01-08 00:23:47,948 iteration 361 : loss : 0.112873, loss_ce: 0.040402
2022-01-08 00:23:49,353 iteration 362 : loss : 0.102663, loss_ce: 0.058408
2022-01-08 00:23:50,671 iteration 363 : loss : 0.085061, loss_ce: 0.036565
2022-01-08 00:23:52,067 iteration 364 : loss : 0.208472, loss_ce: 0.087280
2022-01-08 00:23:53,408 iteration 365 : loss : 0.077487, loss_ce: 0.035223
2022-01-08 00:23:54,771 iteration 366 : loss : 0.119745, loss_ce: 0.041248
2022-01-08 00:23:56,167 iteration 367 : loss : 0.110350, loss_ce: 0.047852
2022-01-08 00:23:57,533 iteration 368 : loss : 0.123242, loss_ce: 0.053199
2022-01-08 00:23:58,880 iteration 369 : loss : 0.077397, loss_ce: 0.031327
2022-01-08 00:24:00,264 iteration 370 : loss : 0.130095, loss_ce: 0.067568
2022-01-08 00:24:01,658 iteration 371 : loss : 0.108416, loss_ce: 0.038549
2022-01-08 00:24:03,004 iteration 372 : loss : 0.096164, loss_ce: 0.040583
2022-01-08 00:24:04,376 iteration 373 : loss : 0.073968, loss_ce: 0.033389
2022-01-08 00:24:05,788 iteration 374 : loss : 0.099439, loss_ce: 0.044167
  6%|█▋                            | 22/400 [09:29<2:43:11, 25.90s/it]2022-01-08 00:24:07,205 iteration 375 : loss : 0.116497, loss_ce: 0.055854
2022-01-08 00:24:08,574 iteration 376 : loss : 0.111360, loss_ce: 0.031688
2022-01-08 00:24:09,965 iteration 377 : loss : 0.098738, loss_ce: 0.039625
2022-01-08 00:24:11,424 iteration 378 : loss : 0.076292, loss_ce: 0.030593
2022-01-08 00:24:12,821 iteration 379 : loss : 0.079794, loss_ce: 0.034984
2022-01-08 00:24:14,166 iteration 380 : loss : 0.100056, loss_ce: 0.044965
2022-01-08 00:24:15,557 iteration 381 : loss : 0.121412, loss_ce: 0.048472
2022-01-08 00:24:16,865 iteration 382 : loss : 0.077655, loss_ce: 0.035008
2022-01-08 00:24:18,249 iteration 383 : loss : 0.125198, loss_ce: 0.028032
2022-01-08 00:24:19,621 iteration 384 : loss : 0.100758, loss_ce: 0.038233
2022-01-08 00:24:20,977 iteration 385 : loss : 0.107674, loss_ce: 0.048746
2022-01-08 00:24:22,367 iteration 386 : loss : 0.102558, loss_ce: 0.036198
2022-01-08 00:24:23,721 iteration 387 : loss : 0.069528, loss_ce: 0.027051
2022-01-08 00:24:25,145 iteration 388 : loss : 0.086482, loss_ce: 0.039467
2022-01-08 00:24:26,503 iteration 389 : loss : 0.096157, loss_ce: 0.033800
2022-01-08 00:24:27,896 iteration 390 : loss : 0.148422, loss_ce: 0.067837
2022-01-08 00:24:29,314 iteration 391 : loss : 0.080742, loss_ce: 0.032027
  6%|█▋                            | 23/400 [09:52<2:38:17, 25.19s/it]2022-01-08 00:24:30,797 iteration 392 : loss : 0.099891, loss_ce: 0.042181
2022-01-08 00:24:32,119 iteration 393 : loss : 0.093487, loss_ce: 0.041900
2022-01-08 00:24:33,478 iteration 394 : loss : 0.123956, loss_ce: 0.043452
2022-01-08 00:24:34,817 iteration 395 : loss : 0.092537, loss_ce: 0.035923
2022-01-08 00:24:36,199 iteration 396 : loss : 0.098338, loss_ce: 0.039802
2022-01-08 00:24:37,663 iteration 397 : loss : 0.098135, loss_ce: 0.052877
2022-01-08 00:24:39,028 iteration 398 : loss : 0.098500, loss_ce: 0.040474
2022-01-08 00:24:40,404 iteration 399 : loss : 0.107931, loss_ce: 0.041259
2022-01-08 00:24:41,760 iteration 400 : loss : 0.066214, loss_ce: 0.028023
2022-01-08 00:24:43,166 iteration 401 : loss : 0.106764, loss_ce: 0.061564
2022-01-08 00:24:44,647 iteration 402 : loss : 0.103662, loss_ce: 0.046169
2022-01-08 00:24:46,064 iteration 403 : loss : 0.089847, loss_ce: 0.033421
2022-01-08 00:24:47,408 iteration 404 : loss : 0.066353, loss_ce: 0.025170
2022-01-08 00:24:48,775 iteration 405 : loss : 0.130174, loss_ce: 0.071927
2022-01-08 00:24:50,198 iteration 406 : loss : 0.098590, loss_ce: 0.043842
2022-01-08 00:24:51,612 iteration 407 : loss : 0.091880, loss_ce: 0.035429
2022-01-08 00:24:52,905 iteration 408 : loss : 0.064446, loss_ce: 0.029396
  6%|█▊                            | 24/400 [10:16<2:34:51, 24.71s/it]2022-01-08 00:24:54,383 iteration 409 : loss : 0.109623, loss_ce: 0.034076
2022-01-08 00:24:55,794 iteration 410 : loss : 0.101544, loss_ce: 0.039712
2022-01-08 00:24:57,214 iteration 411 : loss : 0.126863, loss_ce: 0.037736
2022-01-08 00:24:58,564 iteration 412 : loss : 0.097764, loss_ce: 0.042279
2022-01-08 00:24:59,941 iteration 413 : loss : 0.102547, loss_ce: 0.030686
2022-01-08 00:25:01,259 iteration 414 : loss : 0.101093, loss_ce: 0.037401
2022-01-08 00:25:02,703 iteration 415 : loss : 0.089408, loss_ce: 0.035454
2022-01-08 00:25:04,047 iteration 416 : loss : 0.092371, loss_ce: 0.042211
2022-01-08 00:25:05,446 iteration 417 : loss : 0.095621, loss_ce: 0.035515
2022-01-08 00:25:06,766 iteration 418 : loss : 0.083660, loss_ce: 0.051987
2022-01-08 00:25:08,132 iteration 419 : loss : 0.145913, loss_ce: 0.049063
2022-01-08 00:25:09,555 iteration 420 : loss : 0.069615, loss_ce: 0.032536
2022-01-08 00:25:11,006 iteration 421 : loss : 0.118499, loss_ce: 0.059294
2022-01-08 00:25:12,333 iteration 422 : loss : 0.084434, loss_ce: 0.030190
2022-01-08 00:25:13,721 iteration 423 : loss : 0.111451, loss_ce: 0.042270
2022-01-08 00:25:15,137 iteration 424 : loss : 0.112687, loss_ce: 0.049614
2022-01-08 00:25:15,137 Training Data Eval:
2022-01-08 00:25:22,043   Average segmentation loss on training set: 0.1379
2022-01-08 00:25:22,043 Validation Data Eval:
2022-01-08 00:25:24,423   Average segmentation loss on validation set: 0.1441
2022-01-08 00:25:28,779 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:25:30,132 iteration 425 : loss : 0.118280, loss_ce: 0.046630
  6%|█▉                            | 25/400 [10:53<2:57:54, 28.47s/it]2022-01-08 00:25:31,523 iteration 426 : loss : 0.073370, loss_ce: 0.033498
2022-01-08 00:25:32,808 iteration 427 : loss : 0.127808, loss_ce: 0.048581
2022-01-08 00:25:34,155 iteration 428 : loss : 0.081761, loss_ce: 0.032850
2022-01-08 00:25:35,414 iteration 429 : loss : 0.075558, loss_ce: 0.028489
2022-01-08 00:25:36,706 iteration 430 : loss : 0.107253, loss_ce: 0.040069
2022-01-08 00:25:37,934 iteration 431 : loss : 0.097412, loss_ce: 0.043766
2022-01-08 00:25:39,196 iteration 432 : loss : 0.049732, loss_ce: 0.020451
2022-01-08 00:25:40,500 iteration 433 : loss : 0.096484, loss_ce: 0.031871
2022-01-08 00:25:41,828 iteration 434 : loss : 0.073349, loss_ce: 0.027862
2022-01-08 00:25:43,194 iteration 435 : loss : 0.115695, loss_ce: 0.058050
2022-01-08 00:25:44,637 iteration 436 : loss : 0.121987, loss_ce: 0.040995
2022-01-08 00:25:46,038 iteration 437 : loss : 0.086952, loss_ce: 0.036527
2022-01-08 00:25:47,414 iteration 438 : loss : 0.103627, loss_ce: 0.056272
2022-01-08 00:25:48,804 iteration 439 : loss : 0.090976, loss_ce: 0.044776
2022-01-08 00:25:50,144 iteration 440 : loss : 0.076307, loss_ce: 0.034084
2022-01-08 00:25:51,517 iteration 441 : loss : 0.099364, loss_ce: 0.044577
2022-01-08 00:25:52,875 iteration 442 : loss : 0.143817, loss_ce: 0.048762
  6%|█▉                            | 26/400 [11:16<2:46:43, 26.75s/it]2022-01-08 00:25:54,296 iteration 443 : loss : 0.067926, loss_ce: 0.034012
2022-01-08 00:25:55,676 iteration 444 : loss : 0.109084, loss_ce: 0.042208
2022-01-08 00:25:57,227 iteration 445 : loss : 0.108036, loss_ce: 0.047177
2022-01-08 00:25:58,551 iteration 446 : loss : 0.151438, loss_ce: 0.055185
2022-01-08 00:26:00,016 iteration 447 : loss : 0.101597, loss_ce: 0.044418
2022-01-08 00:26:01,421 iteration 448 : loss : 0.068163, loss_ce: 0.032658
2022-01-08 00:26:02,775 iteration 449 : loss : 0.068554, loss_ce: 0.027048
2022-01-08 00:26:04,071 iteration 450 : loss : 0.069258, loss_ce: 0.031161
2022-01-08 00:26:05,449 iteration 451 : loss : 0.093459, loss_ce: 0.033070
2022-01-08 00:26:06,854 iteration 452 : loss : 0.077733, loss_ce: 0.036053
2022-01-08 00:26:08,217 iteration 453 : loss : 0.086192, loss_ce: 0.034050
2022-01-08 00:26:09,646 iteration 454 : loss : 0.133986, loss_ce: 0.045062
2022-01-08 00:26:11,089 iteration 455 : loss : 0.067880, loss_ce: 0.022506
2022-01-08 00:26:12,448 iteration 456 : loss : 0.132805, loss_ce: 0.040994
2022-01-08 00:26:13,798 iteration 457 : loss : 0.101577, loss_ce: 0.036860
2022-01-08 00:26:15,181 iteration 458 : loss : 0.112606, loss_ce: 0.042222
2022-01-08 00:26:16,561 iteration 459 : loss : 0.087631, loss_ce: 0.033254
  7%|██                            | 27/400 [11:39<2:40:35, 25.83s/it]2022-01-08 00:26:17,901 iteration 460 : loss : 0.092305, loss_ce: 0.030766
2022-01-08 00:26:19,306 iteration 461 : loss : 0.089155, loss_ce: 0.037292
2022-01-08 00:26:20,695 iteration 462 : loss : 0.088991, loss_ce: 0.025964
2022-01-08 00:26:21,991 iteration 463 : loss : 0.059521, loss_ce: 0.028981
2022-01-08 00:26:23,446 iteration 464 : loss : 0.086113, loss_ce: 0.040310
2022-01-08 00:26:24,804 iteration 465 : loss : 0.069663, loss_ce: 0.037488
2022-01-08 00:26:26,230 iteration 466 : loss : 0.087914, loss_ce: 0.042761
2022-01-08 00:26:27,591 iteration 467 : loss : 0.084673, loss_ce: 0.030278
2022-01-08 00:26:28,957 iteration 468 : loss : 0.082943, loss_ce: 0.035037
2022-01-08 00:26:30,288 iteration 469 : loss : 0.100973, loss_ce: 0.036736
2022-01-08 00:26:31,742 iteration 470 : loss : 0.104609, loss_ce: 0.045944
2022-01-08 00:26:33,068 iteration 471 : loss : 0.081816, loss_ce: 0.033208
2022-01-08 00:26:34,507 iteration 472 : loss : 0.129796, loss_ce: 0.048455
2022-01-08 00:26:35,926 iteration 473 : loss : 0.073474, loss_ce: 0.027017
2022-01-08 00:26:37,365 iteration 474 : loss : 0.085037, loss_ce: 0.032750
2022-01-08 00:26:38,756 iteration 475 : loss : 0.141926, loss_ce: 0.036742
2022-01-08 00:26:40,172 iteration 476 : loss : 0.089387, loss_ce: 0.044769
  7%|██                            | 28/400 [12:03<2:36:00, 25.16s/it]2022-01-08 00:26:41,581 iteration 477 : loss : 0.097984, loss_ce: 0.046822
2022-01-08 00:26:43,014 iteration 478 : loss : 0.116678, loss_ce: 0.045791
2022-01-08 00:26:44,338 iteration 479 : loss : 0.085311, loss_ce: 0.034154
2022-01-08 00:26:45,626 iteration 480 : loss : 0.091832, loss_ce: 0.044153
2022-01-08 00:26:46,984 iteration 481 : loss : 0.085315, loss_ce: 0.040167
2022-01-08 00:26:48,404 iteration 482 : loss : 0.100517, loss_ce: 0.034112
2022-01-08 00:26:49,787 iteration 483 : loss : 0.093678, loss_ce: 0.032903
2022-01-08 00:26:51,173 iteration 484 : loss : 0.058772, loss_ce: 0.023331
2022-01-08 00:26:52,487 iteration 485 : loss : 0.060661, loss_ce: 0.023798
2022-01-08 00:26:53,860 iteration 486 : loss : 0.215192, loss_ce: 0.066511
2022-01-08 00:26:55,263 iteration 487 : loss : 0.132853, loss_ce: 0.082793
2022-01-08 00:26:56,722 iteration 488 : loss : 0.124587, loss_ce: 0.047160
2022-01-08 00:26:58,132 iteration 489 : loss : 0.059539, loss_ce: 0.022660
2022-01-08 00:26:59,472 iteration 490 : loss : 0.070953, loss_ce: 0.029194
2022-01-08 00:27:00,893 iteration 491 : loss : 0.104570, loss_ce: 0.042424
2022-01-08 00:27:02,223 iteration 492 : loss : 0.091703, loss_ce: 0.038830
2022-01-08 00:27:03,580 iteration 493 : loss : 0.124035, loss_ce: 0.057183
  7%|██▏                           | 29/400 [12:26<2:32:20, 24.64s/it]2022-01-08 00:27:04,942 iteration 494 : loss : 0.091953, loss_ce: 0.034660
2022-01-08 00:27:06,326 iteration 495 : loss : 0.080685, loss_ce: 0.036717
2022-01-08 00:27:07,632 iteration 496 : loss : 0.101924, loss_ce: 0.050355
2022-01-08 00:27:09,042 iteration 497 : loss : 0.104328, loss_ce: 0.038115
2022-01-08 00:27:10,415 iteration 498 : loss : 0.136170, loss_ce: 0.053364
2022-01-08 00:27:11,867 iteration 499 : loss : 0.087383, loss_ce: 0.039271
2022-01-08 00:27:13,151 iteration 500 : loss : 0.067773, loss_ce: 0.027248
2022-01-08 00:27:14,544 iteration 501 : loss : 0.111641, loss_ce: 0.037188
2022-01-08 00:27:15,881 iteration 502 : loss : 0.121498, loss_ce: 0.043463
2022-01-08 00:27:17,256 iteration 503 : loss : 0.085463, loss_ce: 0.041447
2022-01-08 00:27:18,710 iteration 504 : loss : 0.068065, loss_ce: 0.025579
2022-01-08 00:27:20,139 iteration 505 : loss : 0.068205, loss_ce: 0.030741
2022-01-08 00:27:21,537 iteration 506 : loss : 0.099830, loss_ce: 0.047581
2022-01-08 00:27:22,908 iteration 507 : loss : 0.074406, loss_ce: 0.025566
2022-01-08 00:27:24,310 iteration 508 : loss : 0.083322, loss_ce: 0.028066
2022-01-08 00:27:25,745 iteration 509 : loss : 0.083212, loss_ce: 0.037567
2022-01-08 00:27:25,745 Training Data Eval:
2022-01-08 00:27:32,671   Average segmentation loss on training set: 0.0728
2022-01-08 00:27:32,671 Validation Data Eval:
2022-01-08 00:27:35,058   Average segmentation loss on validation set: 0.1497
2022-01-08 00:27:36,413 iteration 510 : loss : 0.064273, loss_ce: 0.027126
  8%|██▎                           | 30/400 [12:59<2:47:06, 27.10s/it]2022-01-08 00:27:37,801 iteration 511 : loss : 0.061597, loss_ce: 0.022526
2022-01-08 00:27:39,175 iteration 512 : loss : 0.079412, loss_ce: 0.039386
2022-01-08 00:27:40,548 iteration 513 : loss : 0.100020, loss_ce: 0.037449
2022-01-08 00:27:41,959 iteration 514 : loss : 0.074758, loss_ce: 0.030245
2022-01-08 00:27:43,264 iteration 515 : loss : 0.070380, loss_ce: 0.031286
2022-01-08 00:27:44,538 iteration 516 : loss : 0.050593, loss_ce: 0.018274
2022-01-08 00:27:45,995 iteration 517 : loss : 0.072136, loss_ce: 0.033119
2022-01-08 00:27:47,451 iteration 518 : loss : 0.117340, loss_ce: 0.043991
2022-01-08 00:27:48,911 iteration 519 : loss : 0.060829, loss_ce: 0.029829
2022-01-08 00:27:50,294 iteration 520 : loss : 0.071688, loss_ce: 0.030527
2022-01-08 00:27:51,599 iteration 521 : loss : 0.059264, loss_ce: 0.021707
2022-01-08 00:27:53,030 iteration 522 : loss : 0.090354, loss_ce: 0.036533
2022-01-08 00:27:54,397 iteration 523 : loss : 0.082914, loss_ce: 0.027661
2022-01-08 00:27:55,743 iteration 524 : loss : 0.079550, loss_ce: 0.041000
2022-01-08 00:27:57,112 iteration 525 : loss : 0.078125, loss_ce: 0.035577
2022-01-08 00:27:58,539 iteration 526 : loss : 0.069909, loss_ce: 0.026474
2022-01-08 00:27:59,936 iteration 527 : loss : 0.077047, loss_ce: 0.027926
  8%|██▎                           | 31/400 [13:23<2:40:03, 26.03s/it]2022-01-08 00:28:01,341 iteration 528 : loss : 0.067056, loss_ce: 0.030843
2022-01-08 00:28:02,833 iteration 529 : loss : 0.104437, loss_ce: 0.034860
2022-01-08 00:28:04,161 iteration 530 : loss : 0.050907, loss_ce: 0.019155
2022-01-08 00:28:05,491 iteration 531 : loss : 0.067546, loss_ce: 0.025005
2022-01-08 00:28:06,882 iteration 532 : loss : 0.091047, loss_ce: 0.038497
2022-01-08 00:28:08,248 iteration 533 : loss : 0.077826, loss_ce: 0.034358
2022-01-08 00:28:09,614 iteration 534 : loss : 0.091522, loss_ce: 0.041420
2022-01-08 00:28:11,020 iteration 535 : loss : 0.072679, loss_ce: 0.027140
2022-01-08 00:28:12,400 iteration 536 : loss : 0.062536, loss_ce: 0.031461
2022-01-08 00:28:13,761 iteration 537 : loss : 0.082918, loss_ce: 0.036621
2022-01-08 00:28:15,144 iteration 538 : loss : 0.085343, loss_ce: 0.039483
2022-01-08 00:28:16,454 iteration 539 : loss : 0.060437, loss_ce: 0.024437
2022-01-08 00:28:17,957 iteration 540 : loss : 0.099050, loss_ce: 0.047096
2022-01-08 00:28:19,404 iteration 541 : loss : 0.093903, loss_ce: 0.043581
2022-01-08 00:28:20,716 iteration 542 : loss : 0.113850, loss_ce: 0.050243
2022-01-08 00:28:22,100 iteration 543 : loss : 0.075425, loss_ce: 0.032577
2022-01-08 00:28:23,446 iteration 544 : loss : 0.140303, loss_ce: 0.048947
  8%|██▍                           | 32/400 [13:46<2:34:59, 25.27s/it]2022-01-08 00:28:24,926 iteration 545 : loss : 0.074811, loss_ce: 0.032571
2022-01-08 00:28:26,367 iteration 546 : loss : 0.075436, loss_ce: 0.032534
2022-01-08 00:28:27,774 iteration 547 : loss : 0.054150, loss_ce: 0.023039
2022-01-08 00:28:29,095 iteration 548 : loss : 0.142149, loss_ce: 0.046034
2022-01-08 00:28:30,419 iteration 549 : loss : 0.103245, loss_ce: 0.042159
2022-01-08 00:28:31,743 iteration 550 : loss : 0.095834, loss_ce: 0.036468
2022-01-08 00:28:33,069 iteration 551 : loss : 0.088259, loss_ce: 0.030913
2022-01-08 00:28:34,491 iteration 552 : loss : 0.125320, loss_ce: 0.046480
2022-01-08 00:28:35,875 iteration 553 : loss : 0.075287, loss_ce: 0.028757
2022-01-08 00:28:37,272 iteration 554 : loss : 0.118885, loss_ce: 0.039349
2022-01-08 00:28:38,682 iteration 555 : loss : 0.049159, loss_ce: 0.016048
2022-01-08 00:28:40,113 iteration 556 : loss : 0.129470, loss_ce: 0.069871
2022-01-08 00:28:41,442 iteration 557 : loss : 0.086736, loss_ce: 0.040638
2022-01-08 00:28:42,759 iteration 558 : loss : 0.069748, loss_ce: 0.027851
2022-01-08 00:28:44,111 iteration 559 : loss : 0.088520, loss_ce: 0.035990
2022-01-08 00:28:45,553 iteration 560 : loss : 0.091296, loss_ce: 0.041791
2022-01-08 00:28:46,963 iteration 561 : loss : 0.065868, loss_ce: 0.028979
  8%|██▍                           | 33/400 [14:10<2:31:20, 24.74s/it]2022-01-08 00:28:48,419 iteration 562 : loss : 0.100309, loss_ce: 0.038015
2022-01-08 00:28:49,750 iteration 563 : loss : 0.051158, loss_ce: 0.025434
2022-01-08 00:28:51,072 iteration 564 : loss : 0.072563, loss_ce: 0.037545
2022-01-08 00:28:52,486 iteration 565 : loss : 0.077538, loss_ce: 0.036621
2022-01-08 00:28:53,876 iteration 566 : loss : 0.137327, loss_ce: 0.047540
2022-01-08 00:28:55,267 iteration 567 : loss : 0.113624, loss_ce: 0.046035
2022-01-08 00:28:56,667 iteration 568 : loss : 0.123975, loss_ce: 0.033250
2022-01-08 00:28:58,134 iteration 569 : loss : 0.132558, loss_ce: 0.045416
2022-01-08 00:28:59,526 iteration 570 : loss : 0.135535, loss_ce: 0.063948
2022-01-08 00:29:00,916 iteration 571 : loss : 0.074235, loss_ce: 0.038087
2022-01-08 00:29:02,354 iteration 572 : loss : 0.111614, loss_ce: 0.061122
2022-01-08 00:29:03,695 iteration 573 : loss : 0.096727, loss_ce: 0.046281
2022-01-08 00:29:05,159 iteration 574 : loss : 0.080157, loss_ce: 0.032825
2022-01-08 00:29:06,530 iteration 575 : loss : 0.119718, loss_ce: 0.030081
2022-01-08 00:29:07,846 iteration 576 : loss : 0.084582, loss_ce: 0.040335
2022-01-08 00:29:09,349 iteration 577 : loss : 0.094264, loss_ce: 0.045329
2022-01-08 00:29:10,761 iteration 578 : loss : 0.060991, loss_ce: 0.028401
  8%|██▌                           | 34/400 [14:34<2:29:11, 24.46s/it]2022-01-08 00:29:12,106 iteration 579 : loss : 0.070045, loss_ce: 0.033785
2022-01-08 00:29:13,534 iteration 580 : loss : 0.079633, loss_ce: 0.031981
2022-01-08 00:29:15,049 iteration 581 : loss : 0.063606, loss_ce: 0.027188
2022-01-08 00:29:16,360 iteration 582 : loss : 0.083764, loss_ce: 0.041580
2022-01-08 00:29:17,750 iteration 583 : loss : 0.089271, loss_ce: 0.042946
2022-01-08 00:29:19,174 iteration 584 : loss : 0.112099, loss_ce: 0.057494
2022-01-08 00:29:20,529 iteration 585 : loss : 0.118300, loss_ce: 0.044469
2022-01-08 00:29:21,959 iteration 586 : loss : 0.105505, loss_ce: 0.040722
2022-01-08 00:29:23,359 iteration 587 : loss : 0.081755, loss_ce: 0.038595
2022-01-08 00:29:24,674 iteration 588 : loss : 0.084104, loss_ce: 0.032063
2022-01-08 00:29:26,113 iteration 589 : loss : 0.090627, loss_ce: 0.041054
2022-01-08 00:29:27,479 iteration 590 : loss : 0.073188, loss_ce: 0.028766
2022-01-08 00:29:28,843 iteration 591 : loss : 0.103745, loss_ce: 0.040027
2022-01-08 00:29:30,254 iteration 592 : loss : 0.066698, loss_ce: 0.032503
2022-01-08 00:29:31,580 iteration 593 : loss : 0.106058, loss_ce: 0.040281
2022-01-08 00:29:33,015 iteration 594 : loss : 0.079835, loss_ce: 0.029013
2022-01-08 00:29:33,015 Training Data Eval:
2022-01-08 00:29:39,929   Average segmentation loss on training set: 0.1600
2022-01-08 00:29:39,929 Validation Data Eval:
2022-01-08 00:29:42,308   Average segmentation loss on validation set: 0.2876
2022-01-08 00:29:43,711 iteration 595 : loss : 0.097144, loss_ce: 0.038672
  9%|██▋                           | 35/400 [15:07<2:44:17, 27.01s/it]2022-01-08 00:29:45,193 iteration 596 : loss : 0.083683, loss_ce: 0.039192
2022-01-08 00:29:46,575 iteration 597 : loss : 0.069315, loss_ce: 0.035378
2022-01-08 00:29:47,961 iteration 598 : loss : 0.073496, loss_ce: 0.026625
2022-01-08 00:29:49,366 iteration 599 : loss : 0.085610, loss_ce: 0.025360
2022-01-08 00:29:50,796 iteration 600 : loss : 0.076138, loss_ce: 0.028280
2022-01-08 00:29:52,209 iteration 601 : loss : 0.075278, loss_ce: 0.028407
2022-01-08 00:29:53,494 iteration 602 : loss : 0.082885, loss_ce: 0.036664
2022-01-08 00:29:54,878 iteration 603 : loss : 0.083707, loss_ce: 0.030407
2022-01-08 00:29:56,207 iteration 604 : loss : 0.112613, loss_ce: 0.037322
2022-01-08 00:29:57,658 iteration 605 : loss : 0.076728, loss_ce: 0.027830
2022-01-08 00:29:58,997 iteration 606 : loss : 0.061870, loss_ce: 0.024331
2022-01-08 00:30:00,341 iteration 607 : loss : 0.053103, loss_ce: 0.018028
2022-01-08 00:30:01,680 iteration 608 : loss : 0.055022, loss_ce: 0.026458
2022-01-08 00:30:03,011 iteration 609 : loss : 0.085779, loss_ce: 0.033564
2022-01-08 00:30:04,489 iteration 610 : loss : 0.118046, loss_ce: 0.063723
2022-01-08 00:30:05,835 iteration 611 : loss : 0.069253, loss_ce: 0.031673
2022-01-08 00:30:07,129 iteration 612 : loss : 0.086594, loss_ce: 0.042141
  9%|██▋                           | 36/400 [15:30<2:37:18, 25.93s/it]2022-01-08 00:30:08,535 iteration 613 : loss : 0.104446, loss_ce: 0.043637
2022-01-08 00:30:09,959 iteration 614 : loss : 0.075838, loss_ce: 0.031712
2022-01-08 00:30:11,334 iteration 615 : loss : 0.074010, loss_ce: 0.034224
2022-01-08 00:30:12,690 iteration 616 : loss : 0.087207, loss_ce: 0.030633
2022-01-08 00:30:14,104 iteration 617 : loss : 0.174135, loss_ce: 0.062190
2022-01-08 00:30:15,528 iteration 618 : loss : 0.055156, loss_ce: 0.025614
2022-01-08 00:30:16,924 iteration 619 : loss : 0.066688, loss_ce: 0.026851
2022-01-08 00:30:18,352 iteration 620 : loss : 0.077022, loss_ce: 0.033407
2022-01-08 00:30:19,635 iteration 621 : loss : 0.069155, loss_ce: 0.028096
2022-01-08 00:30:21,085 iteration 622 : loss : 0.088662, loss_ce: 0.034794
2022-01-08 00:30:22,536 iteration 623 : loss : 0.105721, loss_ce: 0.048108
2022-01-08 00:30:23,834 iteration 624 : loss : 0.080661, loss_ce: 0.030790
2022-01-08 00:30:25,356 iteration 625 : loss : 0.075899, loss_ce: 0.033537
2022-01-08 00:30:26,713 iteration 626 : loss : 0.064551, loss_ce: 0.026714
2022-01-08 00:30:28,011 iteration 627 : loss : 0.097561, loss_ce: 0.033434
2022-01-08 00:30:29,308 iteration 628 : loss : 0.116833, loss_ce: 0.045137
2022-01-08 00:30:30,717 iteration 629 : loss : 0.082007, loss_ce: 0.034038
  9%|██▊                           | 37/400 [15:54<2:32:37, 25.23s/it]2022-01-08 00:30:32,028 iteration 630 : loss : 0.068105, loss_ce: 0.032023
2022-01-08 00:30:33,393 iteration 631 : loss : 0.101159, loss_ce: 0.042593
2022-01-08 00:30:34,699 iteration 632 : loss : 0.062589, loss_ce: 0.022206
2022-01-08 00:30:36,103 iteration 633 : loss : 0.077430, loss_ce: 0.039508
2022-01-08 00:30:37,493 iteration 634 : loss : 0.057617, loss_ce: 0.025322
2022-01-08 00:30:38,901 iteration 635 : loss : 0.068966, loss_ce: 0.029233
2022-01-08 00:30:40,250 iteration 636 : loss : 0.121079, loss_ce: 0.036681
2022-01-08 00:30:41,576 iteration 637 : loss : 0.071278, loss_ce: 0.031115
2022-01-08 00:30:42,914 iteration 638 : loss : 0.059505, loss_ce: 0.029233
2022-01-08 00:30:44,338 iteration 639 : loss : 0.063313, loss_ce: 0.030512
2022-01-08 00:30:45,751 iteration 640 : loss : 0.085618, loss_ce: 0.030497
2022-01-08 00:30:47,088 iteration 641 : loss : 0.060136, loss_ce: 0.023150
2022-01-08 00:30:48,480 iteration 642 : loss : 0.050915, loss_ce: 0.019301
2022-01-08 00:30:49,825 iteration 643 : loss : 0.086229, loss_ce: 0.027038
2022-01-08 00:30:51,191 iteration 644 : loss : 0.070041, loss_ce: 0.020723
2022-01-08 00:30:52,535 iteration 645 : loss : 0.089424, loss_ce: 0.033004
2022-01-08 00:30:53,927 iteration 646 : loss : 0.069093, loss_ce: 0.028256
 10%|██▊                           | 38/400 [16:17<2:28:34, 24.62s/it]2022-01-08 00:30:55,313 iteration 647 : loss : 0.067407, loss_ce: 0.025336
2022-01-08 00:30:56,745 iteration 648 : loss : 0.083275, loss_ce: 0.036917
2022-01-08 00:30:58,133 iteration 649 : loss : 0.067607, loss_ce: 0.030365
2022-01-08 00:30:59,430 iteration 650 : loss : 0.063042, loss_ce: 0.034613
2022-01-08 00:31:00,823 iteration 651 : loss : 0.078904, loss_ce: 0.030943
2022-01-08 00:31:02,143 iteration 652 : loss : 0.080089, loss_ce: 0.025243
2022-01-08 00:31:03,522 iteration 653 : loss : 0.106534, loss_ce: 0.034455
2022-01-08 00:31:04,970 iteration 654 : loss : 0.092489, loss_ce: 0.029068
2022-01-08 00:31:06,380 iteration 655 : loss : 0.051591, loss_ce: 0.021152
2022-01-08 00:31:07,677 iteration 656 : loss : 0.048054, loss_ce: 0.019584
2022-01-08 00:31:09,053 iteration 657 : loss : 0.062045, loss_ce: 0.026957
2022-01-08 00:31:10,484 iteration 658 : loss : 0.147951, loss_ce: 0.044164
2022-01-08 00:31:11,897 iteration 659 : loss : 0.086718, loss_ce: 0.037396
2022-01-08 00:31:13,278 iteration 660 : loss : 0.076711, loss_ce: 0.028506
2022-01-08 00:31:14,662 iteration 661 : loss : 0.068036, loss_ce: 0.033375
2022-01-08 00:31:15,993 iteration 662 : loss : 0.080036, loss_ce: 0.045077
2022-01-08 00:31:17,356 iteration 663 : loss : 0.073521, loss_ce: 0.023607
 10%|██▉                           | 39/400 [16:40<2:26:00, 24.27s/it]2022-01-08 00:31:18,735 iteration 664 : loss : 0.075760, loss_ce: 0.040509
2022-01-08 00:31:20,100 iteration 665 : loss : 0.075324, loss_ce: 0.038192
2022-01-08 00:31:21,481 iteration 666 : loss : 0.080330, loss_ce: 0.031025
2022-01-08 00:31:22,815 iteration 667 : loss : 0.048345, loss_ce: 0.021192
2022-01-08 00:31:24,256 iteration 668 : loss : 0.071865, loss_ce: 0.027644
2022-01-08 00:31:25,583 iteration 669 : loss : 0.059248, loss_ce: 0.025707
2022-01-08 00:31:26,928 iteration 670 : loss : 0.061333, loss_ce: 0.028481
2022-01-08 00:31:28,358 iteration 671 : loss : 0.068440, loss_ce: 0.023512
2022-01-08 00:31:29,771 iteration 672 : loss : 0.056003, loss_ce: 0.022353
2022-01-08 00:31:31,110 iteration 673 : loss : 0.069409, loss_ce: 0.031234
2022-01-08 00:31:32,519 iteration 674 : loss : 0.094693, loss_ce: 0.039306
2022-01-08 00:31:33,925 iteration 675 : loss : 0.070332, loss_ce: 0.026044
2022-01-08 00:31:35,278 iteration 676 : loss : 0.070195, loss_ce: 0.022201
2022-01-08 00:31:36,677 iteration 677 : loss : 0.078407, loss_ce: 0.034049
2022-01-08 00:31:38,036 iteration 678 : loss : 0.063668, loss_ce: 0.023548
2022-01-08 00:31:39,357 iteration 679 : loss : 0.087356, loss_ce: 0.041647
2022-01-08 00:31:39,358 Training Data Eval:
2022-01-08 00:31:46,261   Average segmentation loss on training set: 0.0489
2022-01-08 00:31:46,261 Validation Data Eval:
2022-01-08 00:31:48,643   Average segmentation loss on validation set: 0.1002
2022-01-08 00:31:53,033 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:31:54,358 iteration 680 : loss : 0.148695, loss_ce: 0.043288
 10%|███                           | 40/400 [17:17<2:48:30, 28.08s/it]2022-01-08 00:31:55,732 iteration 681 : loss : 0.064119, loss_ce: 0.027057
2022-01-08 00:31:56,975 iteration 682 : loss : 0.045050, loss_ce: 0.018551
2022-01-08 00:31:58,293 iteration 683 : loss : 0.067683, loss_ce: 0.031163
2022-01-08 00:31:59,614 iteration 684 : loss : 0.049679, loss_ce: 0.022614
2022-01-08 00:32:00,947 iteration 685 : loss : 0.069364, loss_ce: 0.031164
2022-01-08 00:32:02,147 iteration 686 : loss : 0.068465, loss_ce: 0.023812
2022-01-08 00:32:03,396 iteration 687 : loss : 0.056914, loss_ce: 0.021141
2022-01-08 00:32:04,741 iteration 688 : loss : 0.086740, loss_ce: 0.041650
2022-01-08 00:32:06,037 iteration 689 : loss : 0.052443, loss_ce: 0.024860
2022-01-08 00:32:07,303 iteration 690 : loss : 0.081977, loss_ce: 0.045726
2022-01-08 00:32:08,731 iteration 691 : loss : 0.066411, loss_ce: 0.027177
2022-01-08 00:32:10,156 iteration 692 : loss : 0.093510, loss_ce: 0.035820
2022-01-08 00:32:11,599 iteration 693 : loss : 0.086821, loss_ce: 0.036839
2022-01-08 00:32:12,994 iteration 694 : loss : 0.063558, loss_ce: 0.027765
2022-01-08 00:32:14,376 iteration 695 : loss : 0.085483, loss_ce: 0.030091
2022-01-08 00:32:15,759 iteration 696 : loss : 0.051262, loss_ce: 0.021641
2022-01-08 00:32:17,157 iteration 697 : loss : 0.113758, loss_ce: 0.041358
 10%|███                           | 41/400 [17:40<2:38:33, 26.50s/it]2022-01-08 00:32:18,552 iteration 698 : loss : 0.065029, loss_ce: 0.027824
2022-01-08 00:32:19,953 iteration 699 : loss : 0.064987, loss_ce: 0.024481
2022-01-08 00:32:21,360 iteration 700 : loss : 0.077506, loss_ce: 0.031164
2022-01-08 00:32:22,736 iteration 701 : loss : 0.111089, loss_ce: 0.044411
2022-01-08 00:32:24,130 iteration 702 : loss : 0.079172, loss_ce: 0.028935
2022-01-08 00:32:25,505 iteration 703 : loss : 0.089067, loss_ce: 0.035231
2022-01-08 00:32:26,876 iteration 704 : loss : 0.084748, loss_ce: 0.031435
2022-01-08 00:32:28,213 iteration 705 : loss : 0.085227, loss_ce: 0.043652
2022-01-08 00:32:29,647 iteration 706 : loss : 0.046973, loss_ce: 0.021010
2022-01-08 00:32:31,052 iteration 707 : loss : 0.068315, loss_ce: 0.033654
2022-01-08 00:32:32,442 iteration 708 : loss : 0.064522, loss_ce: 0.026719
2022-01-08 00:32:33,842 iteration 709 : loss : 0.054442, loss_ce: 0.024148
2022-01-08 00:32:35,239 iteration 710 : loss : 0.062363, loss_ce: 0.022634
2022-01-08 00:32:36,677 iteration 711 : loss : 0.141740, loss_ce: 0.046856
2022-01-08 00:32:38,120 iteration 712 : loss : 0.055583, loss_ce: 0.027789
2022-01-08 00:32:39,449 iteration 713 : loss : 0.059694, loss_ce: 0.025888
2022-01-08 00:32:40,823 iteration 714 : loss : 0.043770, loss_ce: 0.018557
 10%|███▏                          | 42/400 [18:04<2:33:03, 25.65s/it]2022-01-08 00:32:42,238 iteration 715 : loss : 0.061590, loss_ce: 0.027268
2022-01-08 00:32:43,619 iteration 716 : loss : 0.120498, loss_ce: 0.037502
2022-01-08 00:32:44,978 iteration 717 : loss : 0.060559, loss_ce: 0.023272
2022-01-08 00:32:46,255 iteration 718 : loss : 0.057133, loss_ce: 0.020779
2022-01-08 00:32:47,636 iteration 719 : loss : 0.066666, loss_ce: 0.034251
2022-01-08 00:32:48,934 iteration 720 : loss : 0.056044, loss_ce: 0.021465
2022-01-08 00:32:50,436 iteration 721 : loss : 0.083648, loss_ce: 0.030539
2022-01-08 00:32:51,823 iteration 722 : loss : 0.042253, loss_ce: 0.019036
2022-01-08 00:32:53,220 iteration 723 : loss : 0.062867, loss_ce: 0.023695
2022-01-08 00:32:54,680 iteration 724 : loss : 0.073889, loss_ce: 0.026275
2022-01-08 00:32:56,016 iteration 725 : loss : 0.054863, loss_ce: 0.024178
2022-01-08 00:32:57,412 iteration 726 : loss : 0.067435, loss_ce: 0.031065
2022-01-08 00:32:58,757 iteration 727 : loss : 0.039306, loss_ce: 0.017011
2022-01-08 00:33:00,114 iteration 728 : loss : 0.073351, loss_ce: 0.022924
2022-01-08 00:33:01,470 iteration 729 : loss : 0.076190, loss_ce: 0.033903
2022-01-08 00:33:02,859 iteration 730 : loss : 0.063364, loss_ce: 0.022620
2022-01-08 00:33:04,197 iteration 731 : loss : 0.091980, loss_ce: 0.033333
 11%|███▏                          | 43/400 [18:27<2:28:33, 24.97s/it]2022-01-08 00:33:05,643 iteration 732 : loss : 0.073980, loss_ce: 0.029918
2022-01-08 00:33:07,032 iteration 733 : loss : 0.057649, loss_ce: 0.024589
2022-01-08 00:33:08,428 iteration 734 : loss : 0.104814, loss_ce: 0.047483
2022-01-08 00:33:09,830 iteration 735 : loss : 0.073386, loss_ce: 0.028150
2022-01-08 00:33:11,210 iteration 736 : loss : 0.053987, loss_ce: 0.019913
2022-01-08 00:33:12,527 iteration 737 : loss : 0.045403, loss_ce: 0.019534
2022-01-08 00:33:13,955 iteration 738 : loss : 0.075397, loss_ce: 0.032454
2022-01-08 00:33:15,364 iteration 739 : loss : 0.070804, loss_ce: 0.038788
2022-01-08 00:33:16,742 iteration 740 : loss : 0.055143, loss_ce: 0.017602
2022-01-08 00:33:18,135 iteration 741 : loss : 0.058397, loss_ce: 0.024702
2022-01-08 00:33:19,533 iteration 742 : loss : 0.068362, loss_ce: 0.030427
2022-01-08 00:33:20,885 iteration 743 : loss : 0.050081, loss_ce: 0.017641
2022-01-08 00:33:22,292 iteration 744 : loss : 0.046534, loss_ce: 0.020602
2022-01-08 00:33:23,665 iteration 745 : loss : 0.049467, loss_ce: 0.017555
2022-01-08 00:33:25,008 iteration 746 : loss : 0.103887, loss_ce: 0.036462
2022-01-08 00:33:26,349 iteration 747 : loss : 0.065556, loss_ce: 0.028539
2022-01-08 00:33:27,747 iteration 748 : loss : 0.083037, loss_ce: 0.031752
 11%|███▎                          | 44/400 [18:51<2:25:35, 24.54s/it]2022-01-08 00:33:29,134 iteration 749 : loss : 0.075235, loss_ce: 0.027005
2022-01-08 00:33:30,478 iteration 750 : loss : 0.039450, loss_ce: 0.013591
2022-01-08 00:33:31,899 iteration 751 : loss : 0.067246, loss_ce: 0.027100
2022-01-08 00:33:33,252 iteration 752 : loss : 0.059390, loss_ce: 0.024165
2022-01-08 00:33:34,665 iteration 753 : loss : 0.057170, loss_ce: 0.023912
2022-01-08 00:33:36,038 iteration 754 : loss : 0.064246, loss_ce: 0.024132
2022-01-08 00:33:37,411 iteration 755 : loss : 0.065346, loss_ce: 0.029317
2022-01-08 00:33:38,702 iteration 756 : loss : 0.048353, loss_ce: 0.020357
2022-01-08 00:33:40,017 iteration 757 : loss : 0.058938, loss_ce: 0.024917
2022-01-08 00:33:41,416 iteration 758 : loss : 0.062177, loss_ce: 0.026542
2022-01-08 00:33:42,841 iteration 759 : loss : 0.093110, loss_ce: 0.036253
2022-01-08 00:33:44,219 iteration 760 : loss : 0.064507, loss_ce: 0.024445
2022-01-08 00:33:45,618 iteration 761 : loss : 0.069759, loss_ce: 0.026720
2022-01-08 00:33:46,909 iteration 762 : loss : 0.052644, loss_ce: 0.023387
2022-01-08 00:33:48,286 iteration 763 : loss : 0.052047, loss_ce: 0.024812
2022-01-08 00:33:49,600 iteration 764 : loss : 0.056068, loss_ce: 0.020459
2022-01-08 00:33:49,600 Training Data Eval:
2022-01-08 00:33:56,513   Average segmentation loss on training set: 0.0454
2022-01-08 00:33:56,513 Validation Data Eval:
2022-01-08 00:33:58,895   Average segmentation loss on validation set: 0.0805
2022-01-08 00:34:03,024 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:34:04,231 iteration 765 : loss : 0.054135, loss_ce: 0.023980
 11%|███▍                          | 45/400 [19:27<2:46:23, 28.12s/it]2022-01-08 00:34:05,622 iteration 766 : loss : 0.053341, loss_ce: 0.020220
2022-01-08 00:34:07,043 iteration 767 : loss : 0.055929, loss_ce: 0.023468
2022-01-08 00:34:08,370 iteration 768 : loss : 0.077670, loss_ce: 0.026603
2022-01-08 00:34:09,711 iteration 769 : loss : 0.066065, loss_ce: 0.031141
2022-01-08 00:34:11,064 iteration 770 : loss : 0.072024, loss_ce: 0.038561
2022-01-08 00:34:12,374 iteration 771 : loss : 0.050867, loss_ce: 0.025938
2022-01-08 00:34:13,595 iteration 772 : loss : 0.057175, loss_ce: 0.025768
2022-01-08 00:34:14,866 iteration 773 : loss : 0.096153, loss_ce: 0.033233
2022-01-08 00:34:16,259 iteration 774 : loss : 0.056381, loss_ce: 0.024344
2022-01-08 00:34:17,561 iteration 775 : loss : 0.053039, loss_ce: 0.021219
2022-01-08 00:34:18,923 iteration 776 : loss : 0.090240, loss_ce: 0.024008
2022-01-08 00:34:20,410 iteration 777 : loss : 0.053410, loss_ce: 0.023947
2022-01-08 00:34:21,806 iteration 778 : loss : 0.049149, loss_ce: 0.016499
2022-01-08 00:34:23,198 iteration 779 : loss : 0.090103, loss_ce: 0.032958
2022-01-08 00:34:24,673 iteration 780 : loss : 0.116090, loss_ce: 0.047651
2022-01-08 00:34:26,098 iteration 781 : loss : 0.047868, loss_ce: 0.016934
2022-01-08 00:34:27,464 iteration 782 : loss : 0.068527, loss_ce: 0.028673
 12%|███▍                          | 46/400 [19:50<2:37:16, 26.66s/it]2022-01-08 00:34:28,876 iteration 783 : loss : 0.083063, loss_ce: 0.035370
2022-01-08 00:34:30,209 iteration 784 : loss : 0.058651, loss_ce: 0.018892
2022-01-08 00:34:31,652 iteration 785 : loss : 0.074651, loss_ce: 0.024014
2022-01-08 00:34:33,008 iteration 786 : loss : 0.074423, loss_ce: 0.031141
2022-01-08 00:34:34,422 iteration 787 : loss : 0.078948, loss_ce: 0.035675
2022-01-08 00:34:35,760 iteration 788 : loss : 0.092700, loss_ce: 0.039482
2022-01-08 00:34:37,181 iteration 789 : loss : 0.089260, loss_ce: 0.036093
2022-01-08 00:34:38,605 iteration 790 : loss : 0.046416, loss_ce: 0.020235
2022-01-08 00:34:39,914 iteration 791 : loss : 0.044469, loss_ce: 0.021513
2022-01-08 00:34:41,358 iteration 792 : loss : 0.070584, loss_ce: 0.027795
2022-01-08 00:34:42,784 iteration 793 : loss : 0.090966, loss_ce: 0.035380
2022-01-08 00:34:44,216 iteration 794 : loss : 0.050579, loss_ce: 0.019950
2022-01-08 00:34:45,618 iteration 795 : loss : 0.065947, loss_ce: 0.025516
2022-01-08 00:34:47,080 iteration 796 : loss : 0.056218, loss_ce: 0.022507
2022-01-08 00:34:48,449 iteration 797 : loss : 0.054736, loss_ce: 0.025191
2022-01-08 00:34:49,829 iteration 798 : loss : 0.072019, loss_ce: 0.030544
2022-01-08 00:34:51,327 iteration 799 : loss : 0.102448, loss_ce: 0.034107
 12%|███▌                          | 47/400 [20:14<2:31:53, 25.82s/it]2022-01-08 00:34:52,747 iteration 800 : loss : 0.070211, loss_ce: 0.035802
2022-01-08 00:34:54,101 iteration 801 : loss : 0.080759, loss_ce: 0.021214
2022-01-08 00:34:55,510 iteration 802 : loss : 0.093014, loss_ce: 0.047830
2022-01-08 00:34:56,831 iteration 803 : loss : 0.055649, loss_ce: 0.021686
2022-01-08 00:34:58,309 iteration 804 : loss : 0.076950, loss_ce: 0.031014
2022-01-08 00:34:59,664 iteration 805 : loss : 0.057905, loss_ce: 0.025141
2022-01-08 00:35:00,966 iteration 806 : loss : 0.077664, loss_ce: 0.036166
2022-01-08 00:35:02,328 iteration 807 : loss : 0.040248, loss_ce: 0.020701
2022-01-08 00:35:03,714 iteration 808 : loss : 0.069879, loss_ce: 0.022054
2022-01-08 00:35:05,144 iteration 809 : loss : 0.058951, loss_ce: 0.022709
2022-01-08 00:35:06,460 iteration 810 : loss : 0.057101, loss_ce: 0.020019
2022-01-08 00:35:07,857 iteration 811 : loss : 0.072194, loss_ce: 0.034519
2022-01-08 00:35:09,182 iteration 812 : loss : 0.066711, loss_ce: 0.031372
2022-01-08 00:35:10,635 iteration 813 : loss : 0.064825, loss_ce: 0.027949
2022-01-08 00:35:12,100 iteration 814 : loss : 0.140715, loss_ce: 0.036210
2022-01-08 00:35:13,338 iteration 815 : loss : 0.052081, loss_ce: 0.024610
2022-01-08 00:35:14,770 iteration 816 : loss : 0.076478, loss_ce: 0.032870
 12%|███▌                          | 48/400 [20:38<2:27:17, 25.11s/it]2022-01-08 00:35:16,135 iteration 817 : loss : 0.052834, loss_ce: 0.021356
2022-01-08 00:35:17,499 iteration 818 : loss : 0.050334, loss_ce: 0.024233
2022-01-08 00:35:19,001 iteration 819 : loss : 0.085360, loss_ce: 0.039767
2022-01-08 00:35:20,417 iteration 820 : loss : 0.096848, loss_ce: 0.036964
2022-01-08 00:35:21,814 iteration 821 : loss : 0.101118, loss_ce: 0.038357
2022-01-08 00:35:23,205 iteration 822 : loss : 0.058342, loss_ce: 0.021713
2022-01-08 00:35:24,628 iteration 823 : loss : 0.058962, loss_ce: 0.024431
2022-01-08 00:35:25,938 iteration 824 : loss : 0.053166, loss_ce: 0.019712
2022-01-08 00:35:27,301 iteration 825 : loss : 0.115988, loss_ce: 0.032041
2022-01-08 00:35:28,609 iteration 826 : loss : 0.059203, loss_ce: 0.022556
2022-01-08 00:35:29,988 iteration 827 : loss : 0.089124, loss_ce: 0.029526
2022-01-08 00:35:31,299 iteration 828 : loss : 0.064936, loss_ce: 0.025257
2022-01-08 00:35:32,741 iteration 829 : loss : 0.087548, loss_ce: 0.042725
2022-01-08 00:35:34,091 iteration 830 : loss : 0.081643, loss_ce: 0.035248
2022-01-08 00:35:35,497 iteration 831 : loss : 0.082423, loss_ce: 0.035989
2022-01-08 00:35:36,890 iteration 832 : loss : 0.075935, loss_ce: 0.023872
2022-01-08 00:35:38,275 iteration 833 : loss : 0.054226, loss_ce: 0.023393
 12%|███▋                          | 49/400 [21:01<2:24:03, 24.63s/it]2022-01-08 00:35:39,605 iteration 834 : loss : 0.051463, loss_ce: 0.018321
2022-01-08 00:35:40,897 iteration 835 : loss : 0.041488, loss_ce: 0.015278
2022-01-08 00:35:42,321 iteration 836 : loss : 0.071177, loss_ce: 0.025055
2022-01-08 00:35:43,677 iteration 837 : loss : 0.040904, loss_ce: 0.017222
2022-01-08 00:35:45,090 iteration 838 : loss : 0.078104, loss_ce: 0.037100
2022-01-08 00:35:46,379 iteration 839 : loss : 0.058127, loss_ce: 0.022426
2022-01-08 00:35:47,686 iteration 840 : loss : 0.044221, loss_ce: 0.018698
2022-01-08 00:35:49,017 iteration 841 : loss : 0.062027, loss_ce: 0.022958
2022-01-08 00:35:50,481 iteration 842 : loss : 0.089281, loss_ce: 0.037562
2022-01-08 00:35:51,816 iteration 843 : loss : 0.067100, loss_ce: 0.021540
2022-01-08 00:35:53,226 iteration 844 : loss : 0.051244, loss_ce: 0.019223
2022-01-08 00:35:54,572 iteration 845 : loss : 0.060188, loss_ce: 0.030470
2022-01-08 00:35:55,924 iteration 846 : loss : 0.052011, loss_ce: 0.027800
2022-01-08 00:35:57,391 iteration 847 : loss : 0.073085, loss_ce: 0.025048
2022-01-08 00:35:58,756 iteration 848 : loss : 0.039931, loss_ce: 0.017142
2022-01-08 00:36:00,098 iteration 849 : loss : 0.051694, loss_ce: 0.019392
2022-01-08 00:36:00,098 Training Data Eval:
2022-01-08 00:36:06,987   Average segmentation loss on training set: 0.0518
2022-01-08 00:36:06,988 Validation Data Eval:
2022-01-08 00:36:09,360   Average segmentation loss on validation set: 0.0957
2022-01-08 00:36:10,689 iteration 850 : loss : 0.067745, loss_ce: 0.034960
 12%|███▊                          | 50/400 [21:33<2:37:16, 26.96s/it]2022-01-08 00:36:12,111 iteration 851 : loss : 0.053248, loss_ce: 0.020891
2022-01-08 00:36:13,475 iteration 852 : loss : 0.079807, loss_ce: 0.026562
2022-01-08 00:36:14,855 iteration 853 : loss : 0.053410, loss_ce: 0.018464
2022-01-08 00:36:16,240 iteration 854 : loss : 0.045938, loss_ce: 0.025419
2022-01-08 00:36:17,587 iteration 855 : loss : 0.044749, loss_ce: 0.018898
2022-01-08 00:36:19,011 iteration 856 : loss : 0.058937, loss_ce: 0.025433
2022-01-08 00:36:20,372 iteration 857 : loss : 0.063203, loss_ce: 0.024449
2022-01-08 00:36:21,828 iteration 858 : loss : 0.031230, loss_ce: 0.012304
2022-01-08 00:36:23,077 iteration 859 : loss : 0.043133, loss_ce: 0.013980
2022-01-08 00:36:24,443 iteration 860 : loss : 0.042168, loss_ce: 0.015916
2022-01-08 00:36:25,813 iteration 861 : loss : 0.063860, loss_ce: 0.016817
2022-01-08 00:36:27,191 iteration 862 : loss : 0.067470, loss_ce: 0.030977
2022-01-08 00:36:28,552 iteration 863 : loss : 0.087955, loss_ce: 0.036216
2022-01-08 00:36:29,910 iteration 864 : loss : 0.052639, loss_ce: 0.024348
2022-01-08 00:36:31,309 iteration 865 : loss : 0.063674, loss_ce: 0.025874
2022-01-08 00:36:32,648 iteration 866 : loss : 0.054876, loss_ce: 0.023348
2022-01-08 00:36:34,050 iteration 867 : loss : 0.056437, loss_ce: 0.023966
 13%|███▊                          | 51/400 [21:57<2:30:32, 25.88s/it]2022-01-08 00:36:35,443 iteration 868 : loss : 0.045632, loss_ce: 0.021888
2022-01-08 00:36:36,845 iteration 869 : loss : 0.059035, loss_ce: 0.028405
2022-01-08 00:36:38,130 iteration 870 : loss : 0.055401, loss_ce: 0.026170
2022-01-08 00:36:39,482 iteration 871 : loss : 0.048048, loss_ce: 0.021874
2022-01-08 00:36:40,827 iteration 872 : loss : 0.072049, loss_ce: 0.036114
2022-01-08 00:36:42,212 iteration 873 : loss : 0.063074, loss_ce: 0.029670
2022-01-08 00:36:43,579 iteration 874 : loss : 0.057995, loss_ce: 0.024697
2022-01-08 00:36:44,963 iteration 875 : loss : 0.061934, loss_ce: 0.023469
2022-01-08 00:36:46,348 iteration 876 : loss : 0.044584, loss_ce: 0.016125
2022-01-08 00:36:47,786 iteration 877 : loss : 0.050193, loss_ce: 0.019657
2022-01-08 00:36:49,111 iteration 878 : loss : 0.105213, loss_ce: 0.025651
2022-01-08 00:36:50,465 iteration 879 : loss : 0.073447, loss_ce: 0.023096
2022-01-08 00:36:51,805 iteration 880 : loss : 0.050023, loss_ce: 0.022440
2022-01-08 00:36:53,250 iteration 881 : loss : 0.058204, loss_ce: 0.016261
2022-01-08 00:36:54,629 iteration 882 : loss : 0.110255, loss_ce: 0.040507
2022-01-08 00:36:55,954 iteration 883 : loss : 0.048049, loss_ce: 0.016271
2022-01-08 00:36:57,354 iteration 884 : loss : 0.074276, loss_ce: 0.033385
 13%|███▉                          | 52/400 [22:20<2:25:37, 25.11s/it]2022-01-08 00:36:58,785 iteration 885 : loss : 0.056443, loss_ce: 0.022976
2022-01-08 00:37:00,160 iteration 886 : loss : 0.072488, loss_ce: 0.031028
2022-01-08 00:37:01,529 iteration 887 : loss : 0.058356, loss_ce: 0.022693
2022-01-08 00:37:02,915 iteration 888 : loss : 0.073892, loss_ce: 0.033577
2022-01-08 00:37:04,282 iteration 889 : loss : 0.107038, loss_ce: 0.027009
2022-01-08 00:37:05,631 iteration 890 : loss : 0.045229, loss_ce: 0.018489
2022-01-08 00:37:07,018 iteration 891 : loss : 0.049103, loss_ce: 0.029658
2022-01-08 00:37:08,384 iteration 892 : loss : 0.052055, loss_ce: 0.023341
2022-01-08 00:37:09,744 iteration 893 : loss : 0.053429, loss_ce: 0.025842
2022-01-08 00:37:11,097 iteration 894 : loss : 0.046608, loss_ce: 0.017493
2022-01-08 00:37:12,460 iteration 895 : loss : 0.079276, loss_ce: 0.029529
2022-01-08 00:37:13,800 iteration 896 : loss : 0.055314, loss_ce: 0.025478
2022-01-08 00:37:15,288 iteration 897 : loss : 0.073234, loss_ce: 0.030177
2022-01-08 00:37:16,632 iteration 898 : loss : 0.073437, loss_ce: 0.029327
2022-01-08 00:37:18,040 iteration 899 : loss : 0.078756, loss_ce: 0.027695
2022-01-08 00:37:19,430 iteration 900 : loss : 0.047911, loss_ce: 0.016011
2022-01-08 00:37:20,769 iteration 901 : loss : 0.071339, loss_ce: 0.031496
 13%|███▉                          | 53/400 [22:44<2:22:16, 24.60s/it]2022-01-08 00:37:22,234 iteration 902 : loss : 0.062929, loss_ce: 0.030348
2022-01-08 00:37:23,627 iteration 903 : loss : 0.087542, loss_ce: 0.030628
2022-01-08 00:37:24,976 iteration 904 : loss : 0.076092, loss_ce: 0.039790
2022-01-08 00:37:26,332 iteration 905 : loss : 0.067209, loss_ce: 0.032429
2022-01-08 00:37:27,773 iteration 906 : loss : 0.056046, loss_ce: 0.027262
2022-01-08 00:37:29,102 iteration 907 : loss : 0.048753, loss_ce: 0.020943
2022-01-08 00:37:30,444 iteration 908 : loss : 0.060021, loss_ce: 0.031999
2022-01-08 00:37:31,839 iteration 909 : loss : 0.066402, loss_ce: 0.030641
2022-01-08 00:37:33,307 iteration 910 : loss : 0.048672, loss_ce: 0.021103
2022-01-08 00:37:34,668 iteration 911 : loss : 0.052909, loss_ce: 0.019297
2022-01-08 00:37:36,069 iteration 912 : loss : 0.061905, loss_ce: 0.026730
2022-01-08 00:37:37,431 iteration 913 : loss : 0.072203, loss_ce: 0.023527
2022-01-08 00:37:38,747 iteration 914 : loss : 0.085395, loss_ce: 0.031825
2022-01-08 00:37:40,138 iteration 915 : loss : 0.073802, loss_ce: 0.026594
2022-01-08 00:37:41,569 iteration 916 : loss : 0.085237, loss_ce: 0.030349
2022-01-08 00:37:42,946 iteration 917 : loss : 0.062476, loss_ce: 0.022345
2022-01-08 00:37:44,378 iteration 918 : loss : 0.131060, loss_ce: 0.034561
 14%|████                          | 54/400 [23:07<2:20:09, 24.30s/it]2022-01-08 00:37:45,759 iteration 919 : loss : 0.052624, loss_ce: 0.017192
2022-01-08 00:37:47,197 iteration 920 : loss : 0.055520, loss_ce: 0.026381
2022-01-08 00:37:48,509 iteration 921 : loss : 0.050398, loss_ce: 0.020327
2022-01-08 00:37:49,909 iteration 922 : loss : 0.057872, loss_ce: 0.018937
2022-01-08 00:37:51,219 iteration 923 : loss : 0.047258, loss_ce: 0.016033
2022-01-08 00:37:52,615 iteration 924 : loss : 0.084240, loss_ce: 0.025490
2022-01-08 00:37:54,033 iteration 925 : loss : 0.089693, loss_ce: 0.029965
2022-01-08 00:37:55,411 iteration 926 : loss : 0.048783, loss_ce: 0.015053
2022-01-08 00:37:56,705 iteration 927 : loss : 0.065654, loss_ce: 0.017691
2022-01-08 00:37:58,134 iteration 928 : loss : 0.066200, loss_ce: 0.032437
2022-01-08 00:37:59,486 iteration 929 : loss : 0.056469, loss_ce: 0.022628
2022-01-08 00:38:00,799 iteration 930 : loss : 0.064487, loss_ce: 0.024704
2022-01-08 00:38:02,130 iteration 931 : loss : 0.091889, loss_ce: 0.046976
2022-01-08 00:38:03,500 iteration 932 : loss : 0.048131, loss_ce: 0.023411
2022-01-08 00:38:04,900 iteration 933 : loss : 0.057732, loss_ce: 0.027756
2022-01-08 00:38:06,246 iteration 934 : loss : 0.067593, loss_ce: 0.022223
2022-01-08 00:38:06,246 Training Data Eval:
2022-01-08 00:38:13,145   Average segmentation loss on training set: 0.0417
2022-01-08 00:38:13,145 Validation Data Eval:
2022-01-08 00:38:15,531   Average segmentation loss on validation set: 0.0724
2022-01-08 00:38:19,675 Found new lowest validation loss at iteration 934! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:38:21,002 iteration 935 : loss : 0.039219, loss_ce: 0.017983
 14%|████▏                         | 55/400 [23:44<2:40:59, 28.00s/it]2022-01-08 00:38:22,328 iteration 936 : loss : 0.060682, loss_ce: 0.028587
2022-01-08 00:38:23,589 iteration 937 : loss : 0.089612, loss_ce: 0.030205
2022-01-08 00:38:24,888 iteration 938 : loss : 0.040712, loss_ce: 0.018357
2022-01-08 00:38:26,265 iteration 939 : loss : 0.082387, loss_ce: 0.029213
2022-01-08 00:38:27,492 iteration 940 : loss : 0.045879, loss_ce: 0.018907
2022-01-08 00:38:28,747 iteration 941 : loss : 0.078046, loss_ce: 0.028335
2022-01-08 00:38:30,018 iteration 942 : loss : 0.053881, loss_ce: 0.024324
2022-01-08 00:38:31,320 iteration 943 : loss : 0.092085, loss_ce: 0.028222
2022-01-08 00:38:32,652 iteration 944 : loss : 0.096517, loss_ce: 0.031393
2022-01-08 00:38:34,071 iteration 945 : loss : 0.050552, loss_ce: 0.022311
2022-01-08 00:38:35,462 iteration 946 : loss : 0.046442, loss_ce: 0.017964
2022-01-08 00:38:36,844 iteration 947 : loss : 0.059625, loss_ce: 0.019900
2022-01-08 00:38:38,290 iteration 948 : loss : 0.079686, loss_ce: 0.036131
2022-01-08 00:38:39,729 iteration 949 : loss : 0.077777, loss_ce: 0.035802
2022-01-08 00:38:41,193 iteration 950 : loss : 0.070545, loss_ce: 0.026028
2022-01-08 00:38:42,525 iteration 951 : loss : 0.066646, loss_ce: 0.029080
2022-01-08 00:38:43,926 iteration 952 : loss : 0.086149, loss_ce: 0.030386
 14%|████▏                         | 56/400 [24:07<2:31:48, 26.48s/it]2022-01-08 00:38:45,300 iteration 953 : loss : 0.041936, loss_ce: 0.015536
2022-01-08 00:38:46,644 iteration 954 : loss : 0.054841, loss_ce: 0.019477
2022-01-08 00:38:48,058 iteration 955 : loss : 0.080879, loss_ce: 0.030930
2022-01-08 00:38:49,452 iteration 956 : loss : 0.052662, loss_ce: 0.016038
2022-01-08 00:38:50,826 iteration 957 : loss : 0.068475, loss_ce: 0.025429
2022-01-08 00:38:52,216 iteration 958 : loss : 0.052568, loss_ce: 0.020614
2022-01-08 00:38:53,666 iteration 959 : loss : 0.050861, loss_ce: 0.025841
2022-01-08 00:38:55,075 iteration 960 : loss : 0.085243, loss_ce: 0.045108
2022-01-08 00:38:56,471 iteration 961 : loss : 0.061587, loss_ce: 0.025918
2022-01-08 00:38:57,873 iteration 962 : loss : 0.105446, loss_ce: 0.033838
2022-01-08 00:38:59,195 iteration 963 : loss : 0.046434, loss_ce: 0.019198
2022-01-08 00:39:00,603 iteration 964 : loss : 0.069304, loss_ce: 0.021497
2022-01-08 00:39:01,948 iteration 965 : loss : 0.052178, loss_ce: 0.017971
2022-01-08 00:39:03,423 iteration 966 : loss : 0.082488, loss_ce: 0.027175
2022-01-08 00:39:04,815 iteration 967 : loss : 0.064901, loss_ce: 0.030831
2022-01-08 00:39:06,108 iteration 968 : loss : 0.056948, loss_ce: 0.021392
2022-01-08 00:39:07,495 iteration 969 : loss : 0.051640, loss_ce: 0.025454
 14%|████▎                         | 57/400 [24:30<2:26:21, 25.60s/it]2022-01-08 00:39:08,865 iteration 970 : loss : 0.067758, loss_ce: 0.036964
2022-01-08 00:39:10,269 iteration 971 : loss : 0.063094, loss_ce: 0.029447
2022-01-08 00:39:11,638 iteration 972 : loss : 0.056277, loss_ce: 0.019909
2022-01-08 00:39:13,029 iteration 973 : loss : 0.041512, loss_ce: 0.017297
2022-01-08 00:39:14,406 iteration 974 : loss : 0.063680, loss_ce: 0.026365
2022-01-08 00:39:15,732 iteration 975 : loss : 0.066223, loss_ce: 0.022777
2022-01-08 00:39:17,159 iteration 976 : loss : 0.061743, loss_ce: 0.027262
2022-01-08 00:39:18,515 iteration 977 : loss : 0.048898, loss_ce: 0.019898
2022-01-08 00:39:19,914 iteration 978 : loss : 0.044176, loss_ce: 0.017702
2022-01-08 00:39:21,259 iteration 979 : loss : 0.051444, loss_ce: 0.023089
2022-01-08 00:39:22,623 iteration 980 : loss : 0.055630, loss_ce: 0.021650
2022-01-08 00:39:23,979 iteration 981 : loss : 0.027498, loss_ce: 0.009157
2022-01-08 00:39:25,385 iteration 982 : loss : 0.071806, loss_ce: 0.021965
2022-01-08 00:39:26,821 iteration 983 : loss : 0.090330, loss_ce: 0.041629
2022-01-08 00:39:28,264 iteration 984 : loss : 0.055815, loss_ce: 0.027300
2022-01-08 00:39:29,673 iteration 985 : loss : 0.052122, loss_ce: 0.022136
2022-01-08 00:39:31,009 iteration 986 : loss : 0.054922, loss_ce: 0.015580
 14%|████▎                         | 58/400 [24:54<2:22:22, 24.98s/it]2022-01-08 00:39:32,334 iteration 987 : loss : 0.043803, loss_ce: 0.014451
2022-01-08 00:39:33,701 iteration 988 : loss : 0.045909, loss_ce: 0.016861
2022-01-08 00:39:35,064 iteration 989 : loss : 0.039287, loss_ce: 0.014415
2022-01-08 00:39:36,415 iteration 990 : loss : 0.048791, loss_ce: 0.018446
2022-01-08 00:39:37,781 iteration 991 : loss : 0.045399, loss_ce: 0.016862
2022-01-08 00:39:39,125 iteration 992 : loss : 0.037801, loss_ce: 0.015613
2022-01-08 00:39:40,425 iteration 993 : loss : 0.045096, loss_ce: 0.016924
2022-01-08 00:39:41,678 iteration 994 : loss : 0.046981, loss_ce: 0.017733
2022-01-08 00:39:43,131 iteration 995 : loss : 0.050159, loss_ce: 0.029296
2022-01-08 00:39:44,524 iteration 996 : loss : 0.047708, loss_ce: 0.022780
2022-01-08 00:39:45,922 iteration 997 : loss : 0.066564, loss_ce: 0.029078
2022-01-08 00:39:47,311 iteration 998 : loss : 0.047263, loss_ce: 0.017350
2022-01-08 00:39:48,644 iteration 999 : loss : 0.053546, loss_ce: 0.023638
2022-01-08 00:39:50,068 iteration 1000 : loss : 0.049132, loss_ce: 0.026486
2022-01-08 00:39:51,398 iteration 1001 : loss : 0.043231, loss_ce: 0.020551
2022-01-08 00:39:52,823 iteration 1002 : loss : 0.068000, loss_ce: 0.026896
2022-01-08 00:39:54,113 iteration 1003 : loss : 0.092688, loss_ce: 0.028413
 15%|████▍                         | 59/400 [25:17<2:18:45, 24.41s/it]2022-01-08 00:39:55,497 iteration 1004 : loss : 0.061622, loss_ce: 0.020115
2022-01-08 00:39:56,865 iteration 1005 : loss : 0.062767, loss_ce: 0.031257
2022-01-08 00:39:58,327 iteration 1006 : loss : 0.045776, loss_ce: 0.021465
2022-01-08 00:39:59,744 iteration 1007 : loss : 0.052719, loss_ce: 0.025825
2022-01-08 00:40:01,162 iteration 1008 : loss : 0.061823, loss_ce: 0.034033
2022-01-08 00:40:02,503 iteration 1009 : loss : 0.052532, loss_ce: 0.020785
2022-01-08 00:40:03,807 iteration 1010 : loss : 0.039433, loss_ce: 0.011432
2022-01-08 00:40:05,160 iteration 1011 : loss : 0.056676, loss_ce: 0.023831
2022-01-08 00:40:06,582 iteration 1012 : loss : 0.057950, loss_ce: 0.021275
2022-01-08 00:40:07,946 iteration 1013 : loss : 0.043898, loss_ce: 0.017496
2022-01-08 00:40:09,298 iteration 1014 : loss : 0.032745, loss_ce: 0.012796
2022-01-08 00:40:10,804 iteration 1015 : loss : 0.063025, loss_ce: 0.022710
2022-01-08 00:40:12,173 iteration 1016 : loss : 0.049700, loss_ce: 0.019388
2022-01-08 00:40:13,503 iteration 1017 : loss : 0.058999, loss_ce: 0.023736
2022-01-08 00:40:14,923 iteration 1018 : loss : 0.083344, loss_ce: 0.027443
2022-01-08 00:40:16,312 iteration 1019 : loss : 0.057277, loss_ce: 0.021363
2022-01-08 00:40:16,312 Training Data Eval:
2022-01-08 00:40:23,207   Average segmentation loss on training set: 0.0474
2022-01-08 00:40:23,208 Validation Data Eval:
2022-01-08 00:40:25,582   Average segmentation loss on validation set: 0.1028
2022-01-08 00:40:27,024 iteration 1020 : loss : 0.100514, loss_ce: 0.053481
 15%|████▌                         | 60/400 [25:50<2:32:47, 26.96s/it]2022-01-08 00:40:28,413 iteration 1021 : loss : 0.057194, loss_ce: 0.020994
2022-01-08 00:40:29,866 iteration 1022 : loss : 0.060003, loss_ce: 0.028836
2022-01-08 00:40:31,182 iteration 1023 : loss : 0.040414, loss_ce: 0.014417
2022-01-08 00:40:32,516 iteration 1024 : loss : 0.043002, loss_ce: 0.017170
2022-01-08 00:40:33,947 iteration 1025 : loss : 0.050887, loss_ce: 0.023421
2022-01-08 00:40:35,327 iteration 1026 : loss : 0.075734, loss_ce: 0.020720
2022-01-08 00:40:36,669 iteration 1027 : loss : 0.053613, loss_ce: 0.016228
2022-01-08 00:40:38,124 iteration 1028 : loss : 0.075832, loss_ce: 0.027563
2022-01-08 00:40:39,536 iteration 1029 : loss : 0.070357, loss_ce: 0.025168
2022-01-08 00:40:40,901 iteration 1030 : loss : 0.075764, loss_ce: 0.032181
2022-01-08 00:40:42,358 iteration 1031 : loss : 0.062868, loss_ce: 0.027747
2022-01-08 00:40:43,735 iteration 1032 : loss : 0.052304, loss_ce: 0.023910
2022-01-08 00:40:45,095 iteration 1033 : loss : 0.070568, loss_ce: 0.033714
2022-01-08 00:40:46,437 iteration 1034 : loss : 0.044645, loss_ce: 0.018064
2022-01-08 00:40:47,847 iteration 1035 : loss : 0.054023, loss_ce: 0.025876
2022-01-08 00:40:49,283 iteration 1036 : loss : 0.054736, loss_ce: 0.023723
2022-01-08 00:40:50,569 iteration 1037 : loss : 0.041420, loss_ce: 0.020214
 15%|████▌                         | 61/400 [26:13<2:26:33, 25.94s/it]2022-01-08 00:40:51,928 iteration 1038 : loss : 0.040318, loss_ce: 0.020001
2022-01-08 00:40:53,269 iteration 1039 : loss : 0.050099, loss_ce: 0.022957
2022-01-08 00:40:54,613 iteration 1040 : loss : 0.051479, loss_ce: 0.017139
2022-01-08 00:40:55,994 iteration 1041 : loss : 0.059701, loss_ce: 0.028065
2022-01-08 00:40:57,345 iteration 1042 : loss : 0.046657, loss_ce: 0.024680
2022-01-08 00:40:58,700 iteration 1043 : loss : 0.041497, loss_ce: 0.016004
2022-01-08 00:41:00,130 iteration 1044 : loss : 0.053567, loss_ce: 0.022564
2022-01-08 00:41:01,515 iteration 1045 : loss : 0.063423, loss_ce: 0.027297
2022-01-08 00:41:02,844 iteration 1046 : loss : 0.039425, loss_ce: 0.019697
2022-01-08 00:41:04,095 iteration 1047 : loss : 0.042492, loss_ce: 0.017799
2022-01-08 00:41:05,469 iteration 1048 : loss : 0.039443, loss_ce: 0.019714
2022-01-08 00:41:06,880 iteration 1049 : loss : 0.054963, loss_ce: 0.022087
2022-01-08 00:41:08,250 iteration 1050 : loss : 0.092634, loss_ce: 0.022877
2022-01-08 00:41:09,588 iteration 1051 : loss : 0.057468, loss_ce: 0.023351
2022-01-08 00:41:10,995 iteration 1052 : loss : 0.078113, loss_ce: 0.025420
2022-01-08 00:41:12,379 iteration 1053 : loss : 0.068212, loss_ce: 0.018104
2022-01-08 00:41:13,746 iteration 1054 : loss : 0.054183, loss_ce: 0.020290
 16%|████▋                         | 62/400 [26:37<2:21:26, 25.11s/it]2022-01-08 00:41:15,144 iteration 1055 : loss : 0.040321, loss_ce: 0.013511
2022-01-08 00:41:16,431 iteration 1056 : loss : 0.049832, loss_ce: 0.023866
2022-01-08 00:41:17,826 iteration 1057 : loss : 0.069164, loss_ce: 0.029859
2022-01-08 00:41:19,211 iteration 1058 : loss : 0.076806, loss_ce: 0.027323
2022-01-08 00:41:20,604 iteration 1059 : loss : 0.062884, loss_ce: 0.024714
2022-01-08 00:41:21,900 iteration 1060 : loss : 0.052157, loss_ce: 0.015119
2022-01-08 00:41:23,349 iteration 1061 : loss : 0.101259, loss_ce: 0.052726
2022-01-08 00:41:24,724 iteration 1062 : loss : 0.071990, loss_ce: 0.026498
2022-01-08 00:41:26,210 iteration 1063 : loss : 0.050705, loss_ce: 0.020338
2022-01-08 00:41:27,583 iteration 1064 : loss : 0.069222, loss_ce: 0.021013
2022-01-08 00:41:29,026 iteration 1065 : loss : 0.056437, loss_ce: 0.029151
2022-01-08 00:41:30,367 iteration 1066 : loss : 0.038399, loss_ce: 0.015742
2022-01-08 00:41:31,713 iteration 1067 : loss : 0.042685, loss_ce: 0.016791
2022-01-08 00:41:33,121 iteration 1068 : loss : 0.050477, loss_ce: 0.020774
2022-01-08 00:41:34,514 iteration 1069 : loss : 0.043673, loss_ce: 0.020076
2022-01-08 00:41:35,988 iteration 1070 : loss : 0.103334, loss_ce: 0.027071
2022-01-08 00:41:37,357 iteration 1071 : loss : 0.063931, loss_ce: 0.030906
 16%|████▋                         | 63/400 [27:00<2:18:30, 24.66s/it]2022-01-08 00:41:38,844 iteration 1072 : loss : 0.067618, loss_ce: 0.021665
2022-01-08 00:41:40,234 iteration 1073 : loss : 0.077861, loss_ce: 0.032301
2022-01-08 00:41:41,632 iteration 1074 : loss : 0.076302, loss_ce: 0.028232
2022-01-08 00:41:43,040 iteration 1075 : loss : 0.082484, loss_ce: 0.022799
2022-01-08 00:41:44,372 iteration 1076 : loss : 0.047443, loss_ce: 0.018968
2022-01-08 00:41:45,812 iteration 1077 : loss : 0.059433, loss_ce: 0.020155
2022-01-08 00:41:47,146 iteration 1078 : loss : 0.049996, loss_ce: 0.021475
2022-01-08 00:41:48,576 iteration 1079 : loss : 0.051634, loss_ce: 0.022514
2022-01-08 00:41:49,880 iteration 1080 : loss : 0.026636, loss_ce: 0.008904
2022-01-08 00:41:51,237 iteration 1081 : loss : 0.038325, loss_ce: 0.018542
2022-01-08 00:41:52,640 iteration 1082 : loss : 0.064473, loss_ce: 0.032427
2022-01-08 00:41:53,979 iteration 1083 : loss : 0.047027, loss_ce: 0.019734
2022-01-08 00:41:55,337 iteration 1084 : loss : 0.054626, loss_ce: 0.015874
2022-01-08 00:41:56,706 iteration 1085 : loss : 0.055610, loss_ce: 0.025619
2022-01-08 00:41:58,055 iteration 1086 : loss : 0.065868, loss_ce: 0.030130
2022-01-08 00:41:59,492 iteration 1087 : loss : 0.042852, loss_ce: 0.016816
2022-01-08 00:42:00,864 iteration 1088 : loss : 0.037448, loss_ce: 0.015148
 16%|████▊                         | 64/400 [27:24<2:16:08, 24.31s/it]2022-01-08 00:42:02,235 iteration 1089 : loss : 0.072874, loss_ce: 0.025592
2022-01-08 00:42:03,565 iteration 1090 : loss : 0.050470, loss_ce: 0.017486
2022-01-08 00:42:04,994 iteration 1091 : loss : 0.051885, loss_ce: 0.026935
2022-01-08 00:42:06,382 iteration 1092 : loss : 0.061009, loss_ce: 0.028433
2022-01-08 00:42:07,760 iteration 1093 : loss : 0.040981, loss_ce: 0.017481
2022-01-08 00:42:09,157 iteration 1094 : loss : 0.054031, loss_ce: 0.026156
2022-01-08 00:42:10,562 iteration 1095 : loss : 0.090556, loss_ce: 0.026480
2022-01-08 00:42:11,929 iteration 1096 : loss : 0.035916, loss_ce: 0.015902
2022-01-08 00:42:13,251 iteration 1097 : loss : 0.068261, loss_ce: 0.020865
2022-01-08 00:42:14,591 iteration 1098 : loss : 0.043060, loss_ce: 0.023053
2022-01-08 00:42:15,975 iteration 1099 : loss : 0.041594, loss_ce: 0.018636
2022-01-08 00:42:17,332 iteration 1100 : loss : 0.063849, loss_ce: 0.026629
2022-01-08 00:42:18,677 iteration 1101 : loss : 0.055710, loss_ce: 0.021249
2022-01-08 00:42:20,116 iteration 1102 : loss : 0.060335, loss_ce: 0.017371
2022-01-08 00:42:21,461 iteration 1103 : loss : 0.078231, loss_ce: 0.032967
2022-01-08 00:42:22,784 iteration 1104 : loss : 0.051745, loss_ce: 0.019256
2022-01-08 00:42:22,784 Training Data Eval:
2022-01-08 00:42:29,662   Average segmentation loss on training set: 0.0365
2022-01-08 00:42:29,662 Validation Data Eval:
2022-01-08 00:42:32,029   Average segmentation loss on validation set: 0.0679
2022-01-08 00:42:36,151 Found new lowest validation loss at iteration 1104! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 00:42:37,447 iteration 1105 : loss : 0.046800, loss_ce: 0.021744
 16%|████▉                         | 65/400 [28:00<2:36:18, 27.99s/it]2022-01-08 00:42:38,759 iteration 1106 : loss : 0.061734, loss_ce: 0.024895
2022-01-08 00:42:40,117 iteration 1107 : loss : 0.046123, loss_ce: 0.019768
2022-01-08 00:42:41,356 iteration 1108 : loss : 0.040412, loss_ce: 0.015038
2022-01-08 00:42:42,571 iteration 1109 : loss : 0.061724, loss_ce: 0.023526
2022-01-08 00:42:43,836 iteration 1110 : loss : 0.054087, loss_ce: 0.024652
2022-01-08 00:42:45,120 iteration 1111 : loss : 0.043865, loss_ce: 0.020058
2022-01-08 00:42:46,420 iteration 1112 : loss : 0.043474, loss_ce: 0.019239
2022-01-08 00:42:47,677 iteration 1113 : loss : 0.055370, loss_ce: 0.019211
2022-01-08 00:42:48,947 iteration 1114 : loss : 0.060204, loss_ce: 0.023971
2022-01-08 00:42:50,228 iteration 1115 : loss : 0.042121, loss_ce: 0.014878
2022-01-08 00:42:51,702 iteration 1116 : loss : 0.052834, loss_ce: 0.022268
2022-01-08 00:42:53,159 iteration 1117 : loss : 0.058468, loss_ce: 0.021396
2022-01-08 00:42:54,489 iteration 1118 : loss : 0.032342, loss_ce: 0.012830
2022-01-08 00:42:55,826 iteration 1119 : loss : 0.032178, loss_ce: 0.011930
2022-01-08 00:42:57,154 iteration 1120 : loss : 0.039840, loss_ce: 0.013897
2022-01-08 00:42:58,490 iteration 1121 : loss : 0.045711, loss_ce: 0.014452
2022-01-08 00:42:59,815 iteration 1122 : loss : 0.052134, loss_ce: 0.020234
 16%|████▉                         | 66/400 [28:23<2:26:27, 26.31s/it]2022-01-08 00:43:01,223 iteration 1123 : loss : 0.048680, loss_ce: 0.020380
2022-01-08 00:43:02,571 iteration 1124 : loss : 0.045219, loss_ce: 0.020593
2022-01-08 00:43:04,000 iteration 1125 : loss : 0.061686, loss_ce: 0.026238
2022-01-08 00:43:05,467 iteration 1126 : loss : 0.049845, loss_ce: 0.020064
2022-01-08 00:43:06,856 iteration 1127 : loss : 0.075301, loss_ce: 0.031815
2022-01-08 00:43:08,240 iteration 1128 : loss : 0.051341, loss_ce: 0.015368
2022-01-08 00:43:09,603 iteration 1129 : loss : 0.039752, loss_ce: 0.014815
2022-01-08 00:43:11,025 iteration 1130 : loss : 0.045551, loss_ce: 0.022613
2022-01-08 00:43:12,462 iteration 1131 : loss : 0.115717, loss_ce: 0.035274
2022-01-08 00:43:13,862 iteration 1132 : loss : 0.084073, loss_ce: 0.034312
2022-01-08 00:43:15,260 iteration 1133 : loss : 0.074711, loss_ce: 0.032300
2022-01-08 00:43:16,639 iteration 1134 : loss : 0.076647, loss_ce: 0.033222
2022-01-08 00:43:17,973 iteration 1135 : loss : 0.047244, loss_ce: 0.016339
2022-01-08 00:43:19,362 iteration 1136 : loss : 0.066667, loss_ce: 0.029944
2022-01-08 00:43:20,713 iteration 1137 : loss : 0.049175, loss_ce: 0.018918
2022-01-08 00:43:22,149 iteration 1138 : loss : 0.072808, loss_ce: 0.033167
2022-01-08 00:43:23,575 iteration 1139 : loss : 0.045339, loss_ce: 0.018173
 17%|█████                         | 67/400 [28:46<2:21:46, 25.54s/it]2022-01-08 00:43:24,975 iteration 1140 : loss : 0.067236, loss_ce: 0.030343
2022-01-08 00:43:26,360 iteration 1141 : loss : 0.040890, loss_ce: 0.016604
2022-01-08 00:43:27,679 iteration 1142 : loss : 0.042348, loss_ce: 0.017516
2022-01-08 00:43:29,064 iteration 1143 : loss : 0.081139, loss_ce: 0.029366
2022-01-08 00:43:30,471 iteration 1144 : loss : 0.050098, loss_ce: 0.021813
2022-01-08 00:43:31,827 iteration 1145 : loss : 0.041199, loss_ce: 0.013451
2022-01-08 00:43:33,211 iteration 1146 : loss : 0.050452, loss_ce: 0.021100
2022-01-08 00:43:34,526 iteration 1147 : loss : 0.032827, loss_ce: 0.012576
2022-01-08 00:43:35,895 iteration 1148 : loss : 0.040086, loss_ce: 0.015017
2022-01-08 00:43:37,326 iteration 1149 : loss : 0.048398, loss_ce: 0.023934
2022-01-08 00:43:38,697 iteration 1150 : loss : 0.042641, loss_ce: 0.019796
2022-01-08 00:43:40,065 iteration 1151 : loss : 0.039715, loss_ce: 0.011387
2022-01-08 00:43:41,463 iteration 1152 : loss : 0.070083, loss_ce: 0.016699
2022-01-08 00:43:42,828 iteration 1153 : loss : 0.036310, loss_ce: 0.015122
2022-01-08 00:43:44,145 iteration 1154 : loss : 0.051029, loss_ce: 0.022326
2022-01-08 00:43:45,596 iteration 1155 : loss : 0.062355, loss_ce: 0.022230
2022-01-08 00:43:46,994 iteration 1156 : loss : 0.120211, loss_ce: 0.039090
 17%|█████                         | 68/400 [29:10<2:17:48, 24.90s/it]2022-01-08 00:43:48,350 iteration 1157 : loss : 0.047087, loss_ce: 0.019478
2022-01-08 00:43:49,847 iteration 1158 : loss : 0.059138, loss_ce: 0.020180
2022-01-08 00:43:51,270 iteration 1159 : loss : 0.060587, loss_ce: 0.023614
2022-01-08 00:43:52,561 iteration 1160 : loss : 0.043859, loss_ce: 0.020981
2022-01-08 00:43:53,923 iteration 1161 : loss : 0.042530, loss_ce: 0.014757
2022-01-08 00:43:55,283 iteration 1162 : loss : 0.053312, loss_ce: 0.011830
2022-01-08 00:43:56,732 iteration 1163 : loss : 0.073055, loss_ce: 0.024787
2022-01-08 00:43:58,125 iteration 1164 : loss : 0.054332, loss_ce: 0.023090
2022-01-08 00:43:59,456 iteration 1165 : loss : 0.060328, loss_ce: 0.027798
2022-01-08 00:44:00,834 iteration 1166 : loss : 0.046772, loss_ce: 0.022652
2022-01-08 00:44:02,230 iteration 1167 : loss : 0.050345, loss_ce: 0.022606
2022-01-08 00:44:03,527 iteration 1168 : loss : 0.039645, loss_ce: 0.014393
2022-01-08 00:44:04,897 iteration 1169 : loss : 0.056660, loss_ce: 0.020074
2022-01-08 00:44:06,279 iteration 1170 : loss : 0.038292, loss_ce: 0.013206
2022-01-08 00:44:07,578 iteration 1171 : loss : 0.043907, loss_ce: 0.016476
2022-01-08 00:44:08,945 iteration 1172 : loss : 0.046803, loss_ce: 0.018698
2022-01-08 00:44:10,340 iteration 1173 : loss : 0.046890, loss_ce: 0.028114
 17%|█████▏                        | 69/400 [29:33<2:14:49, 24.44s/it]2022-01-08 00:44:11,798 iteration 1174 : loss : 0.040394, loss_ce: 0.013146
2022-01-08 00:44:13,153 iteration 1175 : loss : 0.037592, loss_ce: 0.015989
2022-01-08 00:44:14,568 iteration 1176 : loss : 0.067334, loss_ce: 0.033736
2022-01-08 00:44:16,024 iteration 1177 : loss : 0.072785, loss_ce: 0.033412
2022-01-08 00:44:17,399 iteration 1178 : loss : 0.054938, loss_ce: 0.025364
2022-01-08 00:44:18,692 iteration 1179 : loss : 0.041752, loss_ce: 0.019158
2022-01-08 00:44:20,112 iteration 1180 : loss : 0.042002, loss_ce: 0.016517
2022-01-08 00:44:21,415 iteration 1181 : loss : 0.032816, loss_ce: 0.013864
2022-01-08 00:44:22,888 iteration 1182 : loss : 0.062525, loss_ce: 0.022311
2022-01-08 00:44:24,247 iteration 1183 : loss : 0.041983, loss_ce: 0.017728
2022-01-08 00:44:25,548 iteration 1184 : loss : 0.032713, loss_ce: 0.013965
2022-01-08 00:44:26,934 iteration 1185 : loss : 0.059914, loss_ce: 0.024108
2022-01-08 00:44:28,263 iteration 1186 : loss : 0.048187, loss_ce: 0.018810
2022-01-08 00:44:29,648 iteration 1187 : loss : 0.046693, loss_ce: 0.018544
2022-01-08 00:44:30,968 iteration 1188 : loss : 0.042633, loss_ce: 0.014826
2022-01-08 00:44:32,346 iteration 1189 : loss : 0.037997, loss_ce: 0.015315
2022-01-08 00:44:32,346 Training Data Eval:
2022-01-08 00:44:39,262   Average segmentation loss on training set: 0.0411
2022-01-08 00:44:39,262 Validation Data Eval:
2022-01-08 00:44:41,634   Average segmentation loss on validation set: 0.1079
2022-01-08 00:44:43,028 iteration 1190 : loss : 0.040646, loss_ce: 0.014272
 18%|█████▎                        | 70/400 [30:06<2:28:00, 26.91s/it]2022-01-08 00:44:44,403 iteration 1191 : loss : 0.041669, loss_ce: 0.018846
2022-01-08 00:44:45,739 iteration 1192 : loss : 0.038509, loss_ce: 0.017938
2022-01-08 00:44:47,125 iteration 1193 : loss : 0.049742, loss_ce: 0.014496
2022-01-08 00:44:48,459 iteration 1194 : loss : 0.052647, loss_ce: 0.017611
2022-01-08 00:44:49,865 iteration 1195 : loss : 0.066318, loss_ce: 0.028366
2022-01-08 00:44:51,163 iteration 1196 : loss : 0.055511, loss_ce: 0.017438
2022-01-08 00:44:52,565 iteration 1197 : loss : 0.050816, loss_ce: 0.026708
2022-01-08 00:44:53,929 iteration 1198 : loss : 0.043674, loss_ce: 0.022986
2022-01-08 00:44:55,298 iteration 1199 : loss : 0.065277, loss_ce: 0.023135
2022-01-08 00:44:56,711 iteration 1200 : loss : 0.054555, loss_ce: 0.019335
2022-01-08 00:44:58,086 iteration 1201 : loss : 0.031154, loss_ce: 0.013879
2022-01-08 00:44:59,509 iteration 1202 : loss : 0.032756, loss_ce: 0.014473
2022-01-08 00:45:00,864 iteration 1203 : loss : 0.046937, loss_ce: 0.016581
2022-01-08 00:45:02,215 iteration 1204 : loss : 0.038947, loss_ce: 0.016326
2022-01-08 00:45:03,632 iteration 1205 : loss : 0.064077, loss_ce: 0.034795
2022-01-08 00:45:05,053 iteration 1206 : loss : 0.081343, loss_ce: 0.024639
2022-01-08 00:45:06,376 iteration 1207 : loss : 0.068227, loss_ce: 0.027140
 18%|█████▎                        | 71/400 [30:29<2:21:43, 25.85s/it]2022-01-08 00:45:07,746 iteration 1208 : loss : 0.037103, loss_ce: 0.017693
2022-01-08 00:45:09,132 iteration 1209 : loss : 0.052302, loss_ce: 0.016552
2022-01-08 00:45:10,428 iteration 1210 : loss : 0.041226, loss_ce: 0.013627
2022-01-08 00:45:11,834 iteration 1211 : loss : 0.060545, loss_ce: 0.022579
2022-01-08 00:45:13,159 iteration 1212 : loss : 0.031591, loss_ce: 0.013344
2022-01-08 00:45:14,527 iteration 1213 : loss : 0.085523, loss_ce: 0.021191
2022-01-08 00:45:15,924 iteration 1214 : loss : 0.045582, loss_ce: 0.019354
2022-01-08 00:45:17,289 iteration 1215 : loss : 0.045369, loss_ce: 0.019830
2022-01-08 00:45:18,618 iteration 1216 : loss : 0.039134, loss_ce: 0.014997
2022-01-08 00:45:20,045 iteration 1217 : loss : 0.045794, loss_ce: 0.020015
2022-01-08 00:45:21,321 iteration 1218 : loss : 0.036653, loss_ce: 0.016228
2022-01-08 00:45:22,714 iteration 1219 : loss : 0.045139, loss_ce: 0.018371
2022-01-08 00:45:24,060 iteration 1220 : loss : 0.056782, loss_ce: 0.019343
2022-01-08 00:45:25,525 iteration 1221 : loss : 0.053106, loss_ce: 0.022023
2022-01-08 00:45:26,913 iteration 1222 : loss : 0.052001, loss_ce: 0.017530
2022-01-08 00:45:28,249 iteration 1223 : loss : 0.038201, loss_ce: 0.018734
2022-01-08 00:45:29,644 iteration 1224 : loss : 0.041323, loss_ce: 0.016835
 18%|█████▍                        | 72/400 [30:52<2:17:02, 25.07s/it]2022-01-08 00:45:31,100 iteration 1225 : loss : 0.041074, loss_ce: 0.015036
2022-01-08 00:45:32,498 iteration 1226 : loss : 0.052886, loss_ce: 0.020631
2022-01-08 00:45:33,832 iteration 1227 : loss : 0.037173, loss_ce: 0.012386
2022-01-08 00:45:35,298 iteration 1228 : loss : 0.062073, loss_ce: 0.018843
2022-01-08 00:45:36,718 iteration 1229 : loss : 0.053568, loss_ce: 0.025063
2022-01-08 00:45:38,111 iteration 1230 : loss : 0.049932, loss_ce: 0.024221
2022-01-08 00:45:39,462 iteration 1231 : loss : 0.052972, loss_ce: 0.018039
2022-01-08 00:45:40,871 iteration 1232 : loss : 0.037336, loss_ce: 0.015722
2022-01-08 00:45:42,363 iteration 1233 : loss : 0.043682, loss_ce: 0.020108
2022-01-08 00:45:43,760 iteration 1234 : loss : 0.046933, loss_ce: 0.020326
2022-01-08 00:45:45,117 iteration 1235 : loss : 0.044946, loss_ce: 0.020624
2022-01-08 00:45:46,452 iteration 1236 : loss : 0.031981, loss_ce: 0.012129
2022-01-08 00:45:47,744 iteration 1237 : loss : 0.061792, loss_ce: 0.027109
2022-01-08 00:45:49,107 iteration 1238 : loss : 0.040957, loss_ce: 0.017663
2022-01-08 00:45:50,519 iteration 1239 : loss : 0.042011, loss_ce: 0.016749
2022-01-08 00:45:51,809 iteration 1240 : loss : 0.049374, loss_ce: 0.017186
2022-01-08 00:45:53,259 iteration 1241 : loss : 0.042424, loss_ce: 0.018459
 18%|█████▍                        | 73/400 [31:16<2:14:16, 24.64s/it]2022-01-08 00:45:54,592 iteration 1242 : loss : 0.041628, loss_ce: 0.017609
2022-01-08 00:45:55,886 iteration 1243 : loss : 0.034736, loss_ce: 0.017820
2022-01-08 00:45:57,203 iteration 1244 : loss : 0.053997, loss_ce: 0.019900
2022-01-08 00:45:58,546 iteration 1245 : loss : 0.040702, loss_ce: 0.015882
2022-01-08 00:45:59,921 iteration 1246 : loss : 0.069490, loss_ce: 0.032479
2022-01-08 00:46:01,323 iteration 1247 : loss : 0.041047, loss_ce: 0.013382
2022-01-08 00:46:02,645 iteration 1248 : loss : 0.031684, loss_ce: 0.011766
2022-01-08 00:46:04,027 iteration 1249 : loss : 0.034688, loss_ce: 0.013705
2022-01-08 00:46:05,367 iteration 1250 : loss : 0.053988, loss_ce: 0.020984
2022-01-08 00:46:06,760 iteration 1251 : loss : 0.040367, loss_ce: 0.016611
2022-01-08 00:46:08,137 iteration 1252 : loss : 0.094770, loss_ce: 0.030220
2022-01-08 00:46:09,521 iteration 1253 : loss : 0.047603, loss_ce: 0.020057
2022-01-08 00:46:10,866 iteration 1254 : loss : 0.044632, loss_ce: 0.020627
2022-01-08 00:46:12,271 iteration 1255 : loss : 0.041251, loss_ce: 0.016616
2022-01-08 00:46:13,697 iteration 1256 : loss : 0.078354, loss_ce: 0.031347
2022-01-08 00:46:15,069 iteration 1257 : loss : 0.052638, loss_ce: 0.026124
2022-01-08 00:46:16,534 iteration 1258 : loss : 0.059578, loss_ce: 0.028358
 18%|█████▌                        | 74/400 [31:39<2:11:37, 24.23s/it]2022-01-08 00:46:17,941 iteration 1259 : loss : 0.041343, loss_ce: 0.014941
2022-01-08 00:46:19,339 iteration 1260 : loss : 0.047340, loss_ce: 0.016706
2022-01-08 00:46:20,717 iteration 1261 : loss : 0.046700, loss_ce: 0.020701
2022-01-08 00:46:22,050 iteration 1262 : loss : 0.037030, loss_ce: 0.017594
2022-01-08 00:46:23,486 iteration 1263 : loss : 0.047626, loss_ce: 0.016835
2022-01-08 00:46:24,795 iteration 1264 : loss : 0.046663, loss_ce: 0.021944
2022-01-08 00:46:26,108 iteration 1265 : loss : 0.044840, loss_ce: 0.017249
2022-01-08 00:46:27,401 iteration 1266 : loss : 0.042239, loss_ce: 0.019510
2022-01-08 00:46:28,865 iteration 1267 : loss : 0.058699, loss_ce: 0.023144
2022-01-08 00:46:30,216 iteration 1268 : loss : 0.134553, loss_ce: 0.040733
2022-01-08 00:46:31,622 iteration 1269 : loss : 0.072568, loss_ce: 0.025406
2022-01-08 00:46:32,936 iteration 1270 : loss : 0.049540, loss_ce: 0.018332
2022-01-08 00:46:34,298 iteration 1271 : loss : 0.042987, loss_ce: 0.017935
2022-01-08 00:46:35,649 iteration 1272 : loss : 0.066191, loss_ce: 0.032262
2022-01-08 00:46:37,056 iteration 1273 : loss : 0.050279, loss_ce: 0.025621
2022-01-08 00:46:38,437 iteration 1274 : loss : 0.048004, loss_ce: 0.017679
2022-01-08 00:46:38,595 Training Data Eval:
2022-01-08 00:46:45,493   Average segmentation loss on training set: 0.0560
2022-01-08 00:46:45,494 Validation Data Eval:
2022-01-08 00:46:47,873   Average segmentation loss on validation set: 0.1625
2022-01-08 00:46:49,275 iteration 1275 : loss : 0.058860, loss_ce: 0.018464
 19%|█████▋                        | 75/400 [32:12<2:25:04, 26.78s/it]2022-01-08 00:46:50,803 iteration 1276 : loss : 0.068213, loss_ce: 0.017736
2022-01-08 00:46:52,132 iteration 1277 : loss : 0.050472, loss_ce: 0.021194
2022-01-08 00:46:53,522 iteration 1278 : loss : 0.045447, loss_ce: 0.020380
2022-01-08 00:46:54,794 iteration 1279 : loss : 0.044672, loss_ce: 0.022632
2022-01-08 00:46:56,078 iteration 1280 : loss : 0.044426, loss_ce: 0.021929
2022-01-08 00:46:57,502 iteration 1281 : loss : 0.046545, loss_ce: 0.019150
2022-01-08 00:46:58,826 iteration 1282 : loss : 0.056279, loss_ce: 0.030156
2022-01-08 00:47:00,165 iteration 1283 : loss : 0.053241, loss_ce: 0.022028
2022-01-08 00:47:01,525 iteration 1284 : loss : 0.051065, loss_ce: 0.028443
2022-01-08 00:47:03,050 iteration 1285 : loss : 0.066321, loss_ce: 0.023013
2022-01-08 00:47:04,461 iteration 1286 : loss : 0.047241, loss_ce: 0.016396
2022-01-08 00:47:05,816 iteration 1287 : loss : 0.047818, loss_ce: 0.017103
2022-01-08 00:47:07,208 iteration 1288 : loss : 0.047325, loss_ce: 0.018431
2022-01-08 00:47:08,642 iteration 1289 : loss : 0.061319, loss_ce: 0.025394
2022-01-08 00:47:09,965 iteration 1290 : loss : 0.053832, loss_ce: 0.017078
2022-01-08 00:47:11,333 iteration 1291 : loss : 0.048919, loss_ce: 0.018358
2022-01-08 00:47:12,647 iteration 1292 : loss : 0.038864, loss_ce: 0.015388
 19%|█████▋                        | 76/400 [32:35<2:19:05, 25.76s/it]2022-01-08 00:47:14,066 iteration 1293 : loss : 0.057097, loss_ce: 0.027349
2022-01-08 00:47:15,396 iteration 1294 : loss : 0.054321, loss_ce: 0.021541
2022-01-08 00:47:16,741 iteration 1295 : loss : 0.035215, loss_ce: 0.014804
2022-01-08 00:47:18,094 iteration 1296 : loss : 0.032779, loss_ce: 0.012332
2022-01-08 00:47:19,440 iteration 1297 : loss : 0.039206, loss_ce: 0.014751
2022-01-08 00:47:20,818 iteration 1298 : loss : 0.030213, loss_ce: 0.012719
2022-01-08 00:47:22,135 iteration 1299 : loss : 0.046031, loss_ce: 0.016459
2022-01-08 00:47:23,554 iteration 1300 : loss : 0.059640, loss_ce: 0.014998
2022-01-08 00:47:24,950 iteration 1301 : loss : 0.048868, loss_ce: 0.022570
2022-01-08 00:47:26,389 iteration 1302 : loss : 0.056415, loss_ce: 0.016726
2022-01-08 00:47:27,713 iteration 1303 : loss : 0.052395, loss_ce: 0.020965
2022-01-08 00:47:29,118 iteration 1304 : loss : 0.043806, loss_ce: 0.012293
2022-01-08 00:47:30,481 iteration 1305 : loss : 0.048099, loss_ce: 0.020787
2022-01-08 00:47:31,904 iteration 1306 : loss : 0.063793, loss_ce: 0.031904
2022-01-08 00:47:33,334 iteration 1307 : loss : 0.046088, loss_ce: 0.016804
2022-01-08 00:47:34,716 iteration 1308 : loss : 0.050856, loss_ce: 0.027897
2022-01-08 00:47:36,095 iteration 1309 : loss : 0.058610, loss_ce: 0.020804
 19%|█████▊                        | 77/400 [32:59<2:14:55, 25.06s/it]2022-01-08 00:47:37,525 iteration 1310 : loss : 0.062285, loss_ce: 0.023339
2022-01-08 00:47:38,962 iteration 1311 : loss : 0.038946, loss_ce: 0.016024
2022-01-08 00:47:40,322 iteration 1312 : loss : 0.060357, loss_ce: 0.033458
2022-01-08 00:47:41,621 iteration 1313 : loss : 0.036114, loss_ce: 0.016807
2022-01-08 00:47:43,002 iteration 1314 : loss : 0.037326, loss_ce: 0.017007
2022-01-08 00:47:44,390 iteration 1315 : loss : 0.060085, loss_ce: 0.024286
2022-01-08 00:47:45,729 iteration 1316 : loss : 0.047229, loss_ce: 0.020172
2022-01-08 00:47:47,125 iteration 1317 : loss : 0.038772, loss_ce: 0.017857
2022-01-08 00:47:48,538 iteration 1318 : loss : 0.056429, loss_ce: 0.024823
2022-01-08 00:47:49,900 iteration 1319 : loss : 0.051989, loss_ce: 0.017859
2022-01-08 00:47:51,261 iteration 1320 : loss : 0.049946, loss_ce: 0.018799
2022-01-08 00:47:52,635 iteration 1321 : loss : 0.049256, loss_ce: 0.019340
2022-01-08 00:47:54,103 iteration 1322 : loss : 0.048924, loss_ce: 0.021456
2022-01-08 00:47:55,485 iteration 1323 : loss : 0.042587, loss_ce: 0.017243
2022-01-08 00:47:56,872 iteration 1324 : loss : 0.047882, loss_ce: 0.017052
2022-01-08 00:47:58,280 iteration 1325 : loss : 0.043142, loss_ce: 0.015349
2022-01-08 00:47:59,600 iteration 1326 : loss : 0.041301, loss_ce: 0.014112
 20%|█████▊                        | 78/400 [33:22<2:12:00, 24.60s/it]2022-01-08 00:48:01,053 iteration 1327 : loss : 0.052720, loss_ce: 0.031891
2022-01-08 00:48:02,513 iteration 1328 : loss : 0.062960, loss_ce: 0.023005
2022-01-08 00:48:03,811 iteration 1329 : loss : 0.046669, loss_ce: 0.022017
2022-01-08 00:48:05,179 iteration 1330 : loss : 0.074741, loss_ce: 0.031940
2022-01-08 00:48:06,554 iteration 1331 : loss : 0.040169, loss_ce: 0.012187
2022-01-08 00:48:07,911 iteration 1332 : loss : 0.028675, loss_ce: 0.012304
2022-01-08 00:48:09,309 iteration 1333 : loss : 0.060289, loss_ce: 0.014846
2022-01-08 00:48:10,679 iteration 1334 : loss : 0.030977, loss_ce: 0.013320
2022-01-08 00:48:12,156 iteration 1335 : loss : 0.038641, loss_ce: 0.014052
2022-01-08 00:48:13,548 iteration 1336 : loss : 0.042628, loss_ce: 0.017605
2022-01-08 00:48:14,975 iteration 1337 : loss : 0.042740, loss_ce: 0.019771
2022-01-08 00:48:16,407 iteration 1338 : loss : 0.055406, loss_ce: 0.023296
2022-01-08 00:48:17,810 iteration 1339 : loss : 0.037881, loss_ce: 0.015063
2022-01-08 00:48:19,134 iteration 1340 : loss : 0.065773, loss_ce: 0.023682
2022-01-08 00:48:20,493 iteration 1341 : loss : 0.042987, loss_ce: 0.017839
2022-01-08 00:48:21,857 iteration 1342 : loss : 0.056745, loss_ce: 0.020270
2022-01-08 00:48:23,209 iteration 1343 : loss : 0.042224, loss_ce: 0.014337
 20%|█████▉                        | 79/400 [33:46<2:10:00, 24.30s/it]2022-01-08 00:48:24,590 iteration 1344 : loss : 0.033545, loss_ce: 0.011981
2022-01-08 00:48:25,922 iteration 1345 : loss : 0.036476, loss_ce: 0.015989
2022-01-08 00:48:27,251 iteration 1346 : loss : 0.057324, loss_ce: 0.016768
2022-01-08 00:48:28,698 iteration 1347 : loss : 0.102832, loss_ce: 0.023443
2022-01-08 00:48:30,139 iteration 1348 : loss : 0.046424, loss_ce: 0.020714
2022-01-08 00:48:31,512 iteration 1349 : loss : 0.068843, loss_ce: 0.029770
2022-01-08 00:48:32,913 iteration 1350 : loss : 0.071949, loss_ce: 0.034314
2022-01-08 00:48:34,235 iteration 1351 : loss : 0.036827, loss_ce: 0.011379
2022-01-08 00:48:35,687 iteration 1352 : loss : 0.065700, loss_ce: 0.027711
2022-01-08 00:48:36,986 iteration 1353 : loss : 0.039650, loss_ce: 0.016965
2022-01-08 00:48:38,387 iteration 1354 : loss : 0.062263, loss_ce: 0.024313
2022-01-08 00:48:39,706 iteration 1355 : loss : 0.059916, loss_ce: 0.019836
2022-01-08 00:48:41,115 iteration 1356 : loss : 0.054277, loss_ce: 0.021839
2022-01-08 00:48:42,429 iteration 1357 : loss : 0.037033, loss_ce: 0.023088
2022-01-08 00:48:43,791 iteration 1358 : loss : 0.043377, loss_ce: 0.014508
2022-01-08 00:48:45,112 iteration 1359 : loss : 0.047304, loss_ce: 0.028637
2022-01-08 00:48:45,112 Training Data Eval:
2022-01-08 00:48:52,012   Average segmentation loss on training set: 0.0424
2022-01-08 00:48:52,013 Validation Data Eval:
2022-01-08 00:48:54,382   Average segmentation loss on validation set: 0.0804
2022-01-08 00:48:55,729 iteration 1360 : loss : 0.059380, loss_ce: 0.022856
 20%|██████                        | 80/400 [34:19<2:22:45, 26.77s/it]2022-01-08 00:48:57,196 iteration 1361 : loss : 0.073305, loss_ce: 0.023309
2022-01-08 00:48:58,591 iteration 1362 : loss : 0.039255, loss_ce: 0.015954
2022-01-08 00:48:59,985 iteration 1363 : loss : 0.037024, loss_ce: 0.013880
2022-01-08 00:49:01,400 iteration 1364 : loss : 0.062108, loss_ce: 0.021931
2022-01-08 00:49:02,846 iteration 1365 : loss : 0.052404, loss_ce: 0.023067
2022-01-08 00:49:04,214 iteration 1366 : loss : 0.052140, loss_ce: 0.026936
2022-01-08 00:49:05,553 iteration 1367 : loss : 0.041645, loss_ce: 0.020657
2022-01-08 00:49:06,934 iteration 1368 : loss : 0.086947, loss_ce: 0.041395
2022-01-08 00:49:08,298 iteration 1369 : loss : 0.051222, loss_ce: 0.015175
2022-01-08 00:49:09,682 iteration 1370 : loss : 0.053076, loss_ce: 0.020163
2022-01-08 00:49:10,996 iteration 1371 : loss : 0.036698, loss_ce: 0.014982
2022-01-08 00:49:12,386 iteration 1372 : loss : 0.068202, loss_ce: 0.019640
2022-01-08 00:49:13,856 iteration 1373 : loss : 0.079917, loss_ce: 0.044119
2022-01-08 00:49:15,213 iteration 1374 : loss : 0.052604, loss_ce: 0.017473
2022-01-08 00:49:16,623 iteration 1375 : loss : 0.041639, loss_ce: 0.015827
2022-01-08 00:49:17,919 iteration 1376 : loss : 0.036636, loss_ce: 0.016005
2022-01-08 00:49:19,354 iteration 1377 : loss : 0.063899, loss_ce: 0.024579
 20%|██████                        | 81/400 [34:42<2:17:18, 25.82s/it]2022-01-08 00:49:20,843 iteration 1378 : loss : 0.047039, loss_ce: 0.024556
2022-01-08 00:49:22,263 iteration 1379 : loss : 0.093104, loss_ce: 0.039381
2022-01-08 00:49:23,620 iteration 1380 : loss : 0.040132, loss_ce: 0.019642
2022-01-08 00:49:24,996 iteration 1381 : loss : 0.040710, loss_ce: 0.015928
2022-01-08 00:49:26,350 iteration 1382 : loss : 0.039801, loss_ce: 0.018042
2022-01-08 00:49:27,709 iteration 1383 : loss : 0.072676, loss_ce: 0.020372
2022-01-08 00:49:29,161 iteration 1384 : loss : 0.080316, loss_ce: 0.036302
2022-01-08 00:49:30,475 iteration 1385 : loss : 0.029842, loss_ce: 0.011371
2022-01-08 00:49:31,964 iteration 1386 : loss : 0.051054, loss_ce: 0.023779
2022-01-08 00:49:33,303 iteration 1387 : loss : 0.067195, loss_ce: 0.018710
2022-01-08 00:49:34,717 iteration 1388 : loss : 0.052860, loss_ce: 0.015786
2022-01-08 00:49:36,107 iteration 1389 : loss : 0.054981, loss_ce: 0.029697
2022-01-08 00:49:37,418 iteration 1390 : loss : 0.042655, loss_ce: 0.019071
2022-01-08 00:49:38,805 iteration 1391 : loss : 0.069152, loss_ce: 0.024253
2022-01-08 00:49:40,165 iteration 1392 : loss : 0.071699, loss_ce: 0.036705
2022-01-08 00:49:41,561 iteration 1393 : loss : 0.069992, loss_ce: 0.022491
2022-01-08 00:49:42,944 iteration 1394 : loss : 0.044495, loss_ce: 0.013235
 20%|██████▏                       | 82/400 [35:06<2:13:18, 25.15s/it]2022-01-08 00:49:44,394 iteration 1395 : loss : 0.055926, loss_ce: 0.026864
2022-01-08 00:49:45,699 iteration 1396 : loss : 0.046262, loss_ce: 0.021069
2022-01-08 00:49:47,121 iteration 1397 : loss : 0.041093, loss_ce: 0.017541
2022-01-08 00:49:48,422 iteration 1398 : loss : 0.030432, loss_ce: 0.012950
2022-01-08 00:49:49,817 iteration 1399 : loss : 0.038442, loss_ce: 0.016370
2022-01-08 00:49:51,244 iteration 1400 : loss : 0.059640, loss_ce: 0.023940
2022-01-08 00:49:52,579 iteration 1401 : loss : 0.045628, loss_ce: 0.015643
2022-01-08 00:49:54,037 iteration 1402 : loss : 0.044802, loss_ce: 0.017172
2022-01-08 00:49:55,440 iteration 1403 : loss : 0.057134, loss_ce: 0.019346
2022-01-08 00:49:56,881 iteration 1404 : loss : 0.060925, loss_ce: 0.025238
2022-01-08 00:49:58,105 iteration 1405 : loss : 0.031785, loss_ce: 0.011649
2022-01-08 00:49:59,506 iteration 1406 : loss : 0.052709, loss_ce: 0.022042
2022-01-08 00:50:00,848 iteration 1407 : loss : 0.040988, loss_ce: 0.016796
2022-01-08 00:50:02,177 iteration 1408 : loss : 0.053054, loss_ce: 0.027252
2022-01-08 00:50:03,630 iteration 1409 : loss : 0.083390, loss_ce: 0.022605
2022-01-08 00:50:04,968 iteration 1410 : loss : 0.059601, loss_ce: 0.028222
2022-01-08 00:50:06,272 iteration 1411 : loss : 0.029470, loss_ce: 0.011426
 21%|██████▏                       | 83/400 [35:29<2:09:59, 24.60s/it]2022-01-08 00:50:07,713 iteration 1412 : loss : 0.043747, loss_ce: 0.020911
2022-01-08 00:50:09,013 iteration 1413 : loss : 0.029549, loss_ce: 0.010760
2022-01-08 00:50:10,411 iteration 1414 : loss : 0.031462, loss_ce: 0.012947
2022-01-08 00:50:11,718 iteration 1415 : loss : 0.031745, loss_ce: 0.012452
2022-01-08 00:50:13,084 iteration 1416 : loss : 0.032947, loss_ce: 0.012671
2022-01-08 00:50:14,468 iteration 1417 : loss : 0.057812, loss_ce: 0.020179
2022-01-08 00:50:15,840 iteration 1418 : loss : 0.036262, loss_ce: 0.013407
2022-01-08 00:50:17,144 iteration 1419 : loss : 0.038077, loss_ce: 0.014945
2022-01-08 00:50:18,508 iteration 1420 : loss : 0.034970, loss_ce: 0.009725
2022-01-08 00:50:19,865 iteration 1421 : loss : 0.051975, loss_ce: 0.028492
2022-01-08 00:50:21,252 iteration 1422 : loss : 0.046297, loss_ce: 0.021374
2022-01-08 00:50:22,572 iteration 1423 : loss : 0.052925, loss_ce: 0.019319
2022-01-08 00:50:23,856 iteration 1424 : loss : 0.028974, loss_ce: 0.011169
2022-01-08 00:50:25,241 iteration 1425 : loss : 0.052859, loss_ce: 0.015669
2022-01-08 00:50:26,627 iteration 1426 : loss : 0.040704, loss_ce: 0.022294
2022-01-08 00:50:28,001 iteration 1427 : loss : 0.079162, loss_ce: 0.015484
2022-01-08 00:50:29,471 iteration 1428 : loss : 0.056289, loss_ce: 0.022276
 21%|██████▎                       | 84/400 [35:52<2:07:22, 24.18s/it]2022-01-08 00:50:30,898 iteration 1429 : loss : 0.069913, loss_ce: 0.036492
2022-01-08 00:50:32,243 iteration 1430 : loss : 0.031218, loss_ce: 0.012178
2022-01-08 00:50:33,559 iteration 1431 : loss : 0.045731, loss_ce: 0.016686
2022-01-08 00:50:34,930 iteration 1432 : loss : 0.051613, loss_ce: 0.022577
2022-01-08 00:50:36,229 iteration 1433 : loss : 0.037580, loss_ce: 0.017995
2022-01-08 00:50:37,689 iteration 1434 : loss : 0.043223, loss_ce: 0.018760
2022-01-08 00:50:39,114 iteration 1435 : loss : 0.054336, loss_ce: 0.017148
2022-01-08 00:50:40,543 iteration 1436 : loss : 0.057132, loss_ce: 0.029263
2022-01-08 00:50:41,924 iteration 1437 : loss : 0.033946, loss_ce: 0.015082
2022-01-08 00:50:43,289 iteration 1438 : loss : 0.034122, loss_ce: 0.014988
2022-01-08 00:50:44,661 iteration 1439 : loss : 0.054826, loss_ce: 0.020153
2022-01-08 00:50:45,991 iteration 1440 : loss : 0.048786, loss_ce: 0.016369
2022-01-08 00:50:47,397 iteration 1441 : loss : 0.051226, loss_ce: 0.019756
2022-01-08 00:50:48,773 iteration 1442 : loss : 0.052345, loss_ce: 0.016255
2022-01-08 00:50:50,198 iteration 1443 : loss : 0.038662, loss_ce: 0.015394
2022-01-08 00:50:51,563 iteration 1444 : loss : 0.053274, loss_ce: 0.014745
2022-01-08 00:50:51,563 Training Data Eval:
2022-01-08 00:50:58,455   Average segmentation loss on training set: 0.0358
2022-01-08 00:50:58,456 Validation Data Eval:
2022-01-08 00:51:00,831   Average segmentation loss on validation set: 0.1053
2022-01-08 00:51:02,273 iteration 1445 : loss : 0.040630, loss_ce: 0.018825
 21%|██████▍                       | 85/400 [36:25<2:20:32, 26.77s/it]2022-01-08 00:51:03,621 iteration 1446 : loss : 0.041937, loss_ce: 0.017294
2022-01-08 00:51:04,919 iteration 1447 : loss : 0.032935, loss_ce: 0.016293
2022-01-08 00:51:06,351 iteration 1448 : loss : 0.048990, loss_ce: 0.014194
2022-01-08 00:51:07,770 iteration 1449 : loss : 0.044356, loss_ce: 0.016063
2022-01-08 00:51:09,133 iteration 1450 : loss : 0.044506, loss_ce: 0.018687
2022-01-08 00:51:10,539 iteration 1451 : loss : 0.039079, loss_ce: 0.012300
2022-01-08 00:51:11,905 iteration 1452 : loss : 0.045790, loss_ce: 0.026194
2022-01-08 00:51:13,313 iteration 1453 : loss : 0.034069, loss_ce: 0.014448
2022-01-08 00:51:14,697 iteration 1454 : loss : 0.044290, loss_ce: 0.014265
2022-01-08 00:51:16,090 iteration 1455 : loss : 0.051736, loss_ce: 0.017873
2022-01-08 00:51:17,427 iteration 1456 : loss : 0.036421, loss_ce: 0.011045
2022-01-08 00:51:18,862 iteration 1457 : loss : 0.039388, loss_ce: 0.016239
2022-01-08 00:51:20,266 iteration 1458 : loss : 0.030671, loss_ce: 0.012489
2022-01-08 00:51:21,609 iteration 1459 : loss : 0.034320, loss_ce: 0.012091
2022-01-08 00:51:22,964 iteration 1460 : loss : 0.051691, loss_ce: 0.019347
2022-01-08 00:51:24,270 iteration 1461 : loss : 0.039702, loss_ce: 0.015335
2022-01-08 00:51:25,698 iteration 1462 : loss : 0.034176, loss_ce: 0.014482
 22%|██████▍                       | 86/400 [36:49<2:14:50, 25.77s/it]2022-01-08 00:51:27,201 iteration 1463 : loss : 0.049919, loss_ce: 0.015785
2022-01-08 00:51:28,515 iteration 1464 : loss : 0.052887, loss_ce: 0.027218
2022-01-08 00:51:29,997 iteration 1465 : loss : 0.045482, loss_ce: 0.015141
2022-01-08 00:51:31,364 iteration 1466 : loss : 0.043186, loss_ce: 0.015363
2022-01-08 00:51:32,698 iteration 1467 : loss : 0.043803, loss_ce: 0.016639
2022-01-08 00:51:34,075 iteration 1468 : loss : 0.046615, loss_ce: 0.020543
2022-01-08 00:51:35,437 iteration 1469 : loss : 0.069281, loss_ce: 0.024975
2022-01-08 00:51:36,758 iteration 1470 : loss : 0.073938, loss_ce: 0.015885
2022-01-08 00:51:38,131 iteration 1471 : loss : 0.032857, loss_ce: 0.015664
2022-01-08 00:51:39,538 iteration 1472 : loss : 0.058539, loss_ce: 0.024160
2022-01-08 00:51:40,862 iteration 1473 : loss : 0.036478, loss_ce: 0.019133
2022-01-08 00:51:42,257 iteration 1474 : loss : 0.064426, loss_ce: 0.019733
2022-01-08 00:51:43,597 iteration 1475 : loss : 0.051767, loss_ce: 0.021871
2022-01-08 00:51:44,939 iteration 1476 : loss : 0.032823, loss_ce: 0.012826
2022-01-08 00:51:46,390 iteration 1477 : loss : 0.039668, loss_ce: 0.018332
2022-01-08 00:51:47,722 iteration 1478 : loss : 0.085133, loss_ce: 0.025909
2022-01-08 00:51:49,096 iteration 1479 : loss : 0.052812, loss_ce: 0.016428
 22%|██████▌                       | 87/400 [37:12<2:10:42, 25.06s/it]2022-01-08 00:51:50,383 iteration 1480 : loss : 0.027285, loss_ce: 0.010453
2022-01-08 00:51:51,840 iteration 1481 : loss : 0.037904, loss_ce: 0.015749
2022-01-08 00:51:53,192 iteration 1482 : loss : 0.043548, loss_ce: 0.018895
2022-01-08 00:51:54,626 iteration 1483 : loss : 0.056616, loss_ce: 0.020973
2022-01-08 00:51:56,027 iteration 1484 : loss : 0.035763, loss_ce: 0.010457
2022-01-08 00:51:57,434 iteration 1485 : loss : 0.031602, loss_ce: 0.014865
2022-01-08 00:51:58,790 iteration 1486 : loss : 0.039926, loss_ce: 0.014462
2022-01-08 00:52:00,151 iteration 1487 : loss : 0.024776, loss_ce: 0.009592
2022-01-08 00:52:01,455 iteration 1488 : loss : 0.033731, loss_ce: 0.015031
2022-01-08 00:52:02,792 iteration 1489 : loss : 0.051466, loss_ce: 0.022973
2022-01-08 00:52:04,182 iteration 1490 : loss : 0.039524, loss_ce: 0.016182
2022-01-08 00:52:05,460 iteration 1491 : loss : 0.042633, loss_ce: 0.012831
2022-01-08 00:52:06,772 iteration 1492 : loss : 0.045025, loss_ce: 0.022914
2022-01-08 00:52:08,173 iteration 1493 : loss : 0.040232, loss_ce: 0.017246
2022-01-08 00:52:09,528 iteration 1494 : loss : 0.054834, loss_ce: 0.017253
2022-01-08 00:52:10,855 iteration 1495 : loss : 0.035513, loss_ce: 0.009863
2022-01-08 00:52:12,274 iteration 1496 : loss : 0.046732, loss_ce: 0.021385
 22%|██████▌                       | 88/400 [37:35<2:07:21, 24.49s/it]2022-01-08 00:52:13,607 iteration 1497 : loss : 0.026674, loss_ce: 0.010809
2022-01-08 00:52:15,003 iteration 1498 : loss : 0.052613, loss_ce: 0.020076
2022-01-08 00:52:16,302 iteration 1499 : loss : 0.047620, loss_ce: 0.016287
2022-01-08 00:52:17,670 iteration 1500 : loss : 0.048517, loss_ce: 0.025009
2022-01-08 00:52:19,021 iteration 1501 : loss : 0.046400, loss_ce: 0.021175
2022-01-08 00:52:20,466 iteration 1502 : loss : 0.094533, loss_ce: 0.022638
2022-01-08 00:52:21,942 iteration 1503 : loss : 0.088391, loss_ce: 0.026328
2022-01-08 00:52:23,279 iteration 1504 : loss : 0.031997, loss_ce: 0.013805
2022-01-08 00:52:24,667 iteration 1505 : loss : 0.034967, loss_ce: 0.016137
2022-01-08 00:52:26,048 iteration 1506 : loss : 0.035075, loss_ce: 0.015138
2022-01-08 00:52:27,452 iteration 1507 : loss : 0.071582, loss_ce: 0.019991
2022-01-08 00:52:28,844 iteration 1508 : loss : 0.052166, loss_ce: 0.026933
2022-01-08 00:52:30,216 iteration 1509 : loss : 0.036600, loss_ce: 0.013374
2022-01-08 00:52:31,666 iteration 1510 : loss : 0.039338, loss_ce: 0.012762
2022-01-08 00:52:33,097 iteration 1511 : loss : 0.044531, loss_ce: 0.020654
2022-01-08 00:52:34,465 iteration 1512 : loss : 0.034963, loss_ce: 0.011179
2022-01-08 00:52:35,843 iteration 1513 : loss : 0.053851, loss_ce: 0.020540
 22%|██████▋                       | 89/400 [37:59<2:05:30, 24.21s/it]2022-01-08 00:52:37,286 iteration 1514 : loss : 0.036322, loss_ce: 0.013649
2022-01-08 00:52:38,602 iteration 1515 : loss : 0.028949, loss_ce: 0.012120
2022-01-08 00:52:39,970 iteration 1516 : loss : 0.039771, loss_ce: 0.017455
2022-01-08 00:52:41,246 iteration 1517 : loss : 0.035409, loss_ce: 0.013190
2022-01-08 00:52:42,655 iteration 1518 : loss : 0.029268, loss_ce: 0.009930
2022-01-08 00:52:44,100 iteration 1519 : loss : 0.043862, loss_ce: 0.016396
2022-01-08 00:52:45,507 iteration 1520 : loss : 0.043884, loss_ce: 0.017257
2022-01-08 00:52:46,881 iteration 1521 : loss : 0.047307, loss_ce: 0.013926
2022-01-08 00:52:48,209 iteration 1522 : loss : 0.040340, loss_ce: 0.014994
2022-01-08 00:52:49,551 iteration 1523 : loss : 0.042585, loss_ce: 0.013493
2022-01-08 00:52:50,925 iteration 1524 : loss : 0.032211, loss_ce: 0.012411
2022-01-08 00:52:52,224 iteration 1525 : loss : 0.043277, loss_ce: 0.019475
2022-01-08 00:52:53,624 iteration 1526 : loss : 0.060410, loss_ce: 0.028364
2022-01-08 00:52:55,033 iteration 1527 : loss : 0.042443, loss_ce: 0.020467
2022-01-08 00:52:56,353 iteration 1528 : loss : 0.043151, loss_ce: 0.017870
2022-01-08 00:52:57,671 iteration 1529 : loss : 0.036308, loss_ce: 0.015117
2022-01-08 00:52:57,671 Training Data Eval:
2022-01-08 00:53:04,577   Average segmentation loss on training set: 0.0361
2022-01-08 00:53:04,578 Validation Data Eval:
2022-01-08 00:53:06,965   Average segmentation loss on validation set: 0.1059
2022-01-08 00:53:08,378 iteration 1530 : loss : 0.048969, loss_ce: 0.022943
 22%|██████▊                       | 90/400 [38:31<2:18:00, 26.71s/it]2022-01-08 00:53:09,811 iteration 1531 : loss : 0.036571, loss_ce: 0.014644
2022-01-08 00:53:11,176 iteration 1532 : loss : 0.041614, loss_ce: 0.014581
2022-01-08 00:53:12,493 iteration 1533 : loss : 0.026138, loss_ce: 0.009602
2022-01-08 00:53:13,844 iteration 1534 : loss : 0.045073, loss_ce: 0.021728
2022-01-08 00:53:15,285 iteration 1535 : loss : 0.061920, loss_ce: 0.023787
2022-01-08 00:53:16,710 iteration 1536 : loss : 0.047036, loss_ce: 0.023334
2022-01-08 00:53:18,060 iteration 1537 : loss : 0.040881, loss_ce: 0.014560
2022-01-08 00:53:19,462 iteration 1538 : loss : 0.032180, loss_ce: 0.013340
2022-01-08 00:53:20,832 iteration 1539 : loss : 0.031373, loss_ce: 0.013708
2022-01-08 00:53:22,190 iteration 1540 : loss : 0.034774, loss_ce: 0.015241
2022-01-08 00:53:23,518 iteration 1541 : loss : 0.024770, loss_ce: 0.010981
2022-01-08 00:53:24,827 iteration 1542 : loss : 0.031873, loss_ce: 0.014042
2022-01-08 00:53:26,170 iteration 1543 : loss : 0.052968, loss_ce: 0.019527
2022-01-08 00:53:27,475 iteration 1544 : loss : 0.027336, loss_ce: 0.010026
2022-01-08 00:53:28,955 iteration 1545 : loss : 0.034031, loss_ce: 0.013024
2022-01-08 00:53:30,266 iteration 1546 : loss : 0.061417, loss_ce: 0.016662
2022-01-08 00:53:31,730 iteration 1547 : loss : 0.032572, loss_ce: 0.013228
 23%|██████▊                       | 91/400 [38:55<2:12:22, 25.70s/it]2022-01-08 00:53:33,164 iteration 1548 : loss : 0.048285, loss_ce: 0.023542
2022-01-08 00:53:34,440 iteration 1549 : loss : 0.038389, loss_ce: 0.017063
2022-01-08 00:53:35,768 iteration 1550 : loss : 0.041509, loss_ce: 0.017465
2022-01-08 00:53:37,024 iteration 1551 : loss : 0.031522, loss_ce: 0.011346
2022-01-08 00:53:38,403 iteration 1552 : loss : 0.031699, loss_ce: 0.010786
2022-01-08 00:53:39,816 iteration 1553 : loss : 0.066609, loss_ce: 0.027948
2022-01-08 00:53:41,214 iteration 1554 : loss : 0.028813, loss_ce: 0.011798
2022-01-08 00:53:42,568 iteration 1555 : loss : 0.050463, loss_ce: 0.020984
2022-01-08 00:53:43,965 iteration 1556 : loss : 0.033307, loss_ce: 0.013075
2022-01-08 00:53:45,295 iteration 1557 : loss : 0.038446, loss_ce: 0.014491
2022-01-08 00:53:46,654 iteration 1558 : loss : 0.038435, loss_ce: 0.018907
2022-01-08 00:53:48,039 iteration 1559 : loss : 0.042311, loss_ce: 0.014458
2022-01-08 00:53:49,359 iteration 1560 : loss : 0.032225, loss_ce: 0.015203
2022-01-08 00:53:50,757 iteration 1561 : loss : 0.040254, loss_ce: 0.013740
2022-01-08 00:53:52,065 iteration 1562 : loss : 0.053743, loss_ce: 0.012766
2022-01-08 00:53:53,479 iteration 1563 : loss : 0.039442, loss_ce: 0.019216
2022-01-08 00:53:54,876 iteration 1564 : loss : 0.042314, loss_ce: 0.019720
 23%|██████▉                       | 92/400 [39:18<2:07:59, 24.93s/it]2022-01-08 00:53:56,239 iteration 1565 : loss : 0.037021, loss_ce: 0.019085
2022-01-08 00:53:57,596 iteration 1566 : loss : 0.031779, loss_ce: 0.012024
2022-01-08 00:53:58,990 iteration 1567 : loss : 0.038936, loss_ce: 0.012914
2022-01-08 00:54:00,327 iteration 1568 : loss : 0.064463, loss_ce: 0.019816
2022-01-08 00:54:01,716 iteration 1569 : loss : 0.060599, loss_ce: 0.032958
2022-01-08 00:54:03,088 iteration 1570 : loss : 0.028196, loss_ce: 0.012863
2022-01-08 00:54:04,424 iteration 1571 : loss : 0.055546, loss_ce: 0.010239
2022-01-08 00:54:05,845 iteration 1572 : loss : 0.043768, loss_ce: 0.016905
2022-01-08 00:54:07,219 iteration 1573 : loss : 0.048784, loss_ce: 0.024031
2022-01-08 00:54:08,533 iteration 1574 : loss : 0.028176, loss_ce: 0.014147
2022-01-08 00:54:09,799 iteration 1575 : loss : 0.032401, loss_ce: 0.013609
2022-01-08 00:54:11,151 iteration 1576 : loss : 0.035854, loss_ce: 0.015718
2022-01-08 00:54:12,505 iteration 1577 : loss : 0.027866, loss_ce: 0.011728
2022-01-08 00:54:13,871 iteration 1578 : loss : 0.056251, loss_ce: 0.020736
2022-01-08 00:54:15,268 iteration 1579 : loss : 0.038086, loss_ce: 0.013801
2022-01-08 00:54:16,594 iteration 1580 : loss : 0.034893, loss_ce: 0.013916
2022-01-08 00:54:17,942 iteration 1581 : loss : 0.041259, loss_ce: 0.014460
 23%|██████▉                       | 93/400 [39:41<2:04:43, 24.37s/it]2022-01-08 00:54:19,402 iteration 1582 : loss : 0.046599, loss_ce: 0.014246
2022-01-08 00:54:20,756 iteration 1583 : loss : 0.041762, loss_ce: 0.012025
2022-01-08 00:54:22,137 iteration 1584 : loss : 0.039589, loss_ce: 0.020197
2022-01-08 00:54:23,490 iteration 1585 : loss : 0.027476, loss_ce: 0.009235
2022-01-08 00:54:24,833 iteration 1586 : loss : 0.044584, loss_ce: 0.020379
2022-01-08 00:54:26,097 iteration 1587 : loss : 0.027673, loss_ce: 0.011282
2022-01-08 00:54:27,477 iteration 1588 : loss : 0.046507, loss_ce: 0.020766
2022-01-08 00:54:28,838 iteration 1589 : loss : 0.031986, loss_ce: 0.012718
2022-01-08 00:54:30,315 iteration 1590 : loss : 0.065783, loss_ce: 0.023293
2022-01-08 00:54:31,625 iteration 1591 : loss : 0.032931, loss_ce: 0.011988
2022-01-08 00:54:32,983 iteration 1592 : loss : 0.043840, loss_ce: 0.017190
2022-01-08 00:54:34,436 iteration 1593 : loss : 0.040611, loss_ce: 0.017676
2022-01-08 00:54:35,897 iteration 1594 : loss : 0.041517, loss_ce: 0.015071
2022-01-08 00:54:37,259 iteration 1595 : loss : 0.028071, loss_ce: 0.009379
2022-01-08 00:54:38,618 iteration 1596 : loss : 0.045416, loss_ce: 0.021411
2022-01-08 00:54:39,976 iteration 1597 : loss : 0.052752, loss_ce: 0.025600
2022-01-08 00:54:41,367 iteration 1598 : loss : 0.035565, loss_ce: 0.014008
 24%|███████                       | 94/400 [40:04<2:02:51, 24.09s/it]2022-01-08 00:54:42,745 iteration 1599 : loss : 0.036874, loss_ce: 0.015485
2022-01-08 00:54:44,097 iteration 1600 : loss : 0.041405, loss_ce: 0.014290
2022-01-08 00:54:45,515 iteration 1601 : loss : 0.029654, loss_ce: 0.011638
2022-01-08 00:54:46,993 iteration 1602 : loss : 0.085403, loss_ce: 0.029254
2022-01-08 00:54:48,341 iteration 1603 : loss : 0.043826, loss_ce: 0.015844
2022-01-08 00:54:49,668 iteration 1604 : loss : 0.035989, loss_ce: 0.013646
2022-01-08 00:54:51,071 iteration 1605 : loss : 0.047193, loss_ce: 0.017974
2022-01-08 00:54:52,432 iteration 1606 : loss : 0.047924, loss_ce: 0.025467
2022-01-08 00:54:53,772 iteration 1607 : loss : 0.039873, loss_ce: 0.017258
2022-01-08 00:54:55,209 iteration 1608 : loss : 0.044812, loss_ce: 0.015887
2022-01-08 00:54:56,565 iteration 1609 : loss : 0.067442, loss_ce: 0.033402
2022-01-08 00:54:57,941 iteration 1610 : loss : 0.034079, loss_ce: 0.012958
2022-01-08 00:54:59,391 iteration 1611 : loss : 0.055069, loss_ce: 0.020759
2022-01-08 00:55:00,736 iteration 1612 : loss : 0.028919, loss_ce: 0.015319
2022-01-08 00:55:01,963 iteration 1613 : loss : 0.049469, loss_ce: 0.025659
2022-01-08 00:55:03,358 iteration 1614 : loss : 0.052179, loss_ce: 0.016814
2022-01-08 00:55:03,359 Training Data Eval:
2022-01-08 00:55:10,277   Average segmentation loss on training set: 0.0622
2022-01-08 00:55:10,278 Validation Data Eval:
2022-01-08 00:55:12,654   Average segmentation loss on validation set: 0.1616
2022-01-08 00:55:14,037 iteration 1615 : loss : 0.039511, loss_ce: 0.018684
 24%|███████▏                      | 95/400 [40:37<2:15:32, 26.66s/it]2022-01-08 00:55:15,475 iteration 1616 : loss : 0.039681, loss_ce: 0.014184
2022-01-08 00:55:16,854 iteration 1617 : loss : 0.048148, loss_ce: 0.015009
2022-01-08 00:55:18,143 iteration 1618 : loss : 0.036687, loss_ce: 0.014423
2022-01-08 00:55:19,451 iteration 1619 : loss : 0.026002, loss_ce: 0.008773
2022-01-08 00:55:20,740 iteration 1620 : loss : 0.044043, loss_ce: 0.013607
2022-01-08 00:55:22,127 iteration 1621 : loss : 0.082916, loss_ce: 0.054417
2022-01-08 00:55:23,617 iteration 1622 : loss : 0.042315, loss_ce: 0.015189
2022-01-08 00:55:24,990 iteration 1623 : loss : 0.053936, loss_ce: 0.020033
2022-01-08 00:55:26,310 iteration 1624 : loss : 0.029645, loss_ce: 0.011324
2022-01-08 00:55:27,654 iteration 1625 : loss : 0.029136, loss_ce: 0.011666
2022-01-08 00:55:28,994 iteration 1626 : loss : 0.034427, loss_ce: 0.014257
2022-01-08 00:55:30,348 iteration 1627 : loss : 0.041932, loss_ce: 0.015659
2022-01-08 00:55:31,784 iteration 1628 : loss : 0.037212, loss_ce: 0.017075
2022-01-08 00:55:33,131 iteration 1629 : loss : 0.032950, loss_ce: 0.012415
2022-01-08 00:55:34,433 iteration 1630 : loss : 0.036615, loss_ce: 0.014495
2022-01-08 00:55:35,915 iteration 1631 : loss : 0.039884, loss_ce: 0.017898
2022-01-08 00:55:37,347 iteration 1632 : loss : 0.041659, loss_ce: 0.015214
 24%|███████▏                      | 96/400 [41:00<2:09:59, 25.65s/it]2022-01-08 00:55:38,795 iteration 1633 : loss : 0.030496, loss_ce: 0.011579
2022-01-08 00:55:40,192 iteration 1634 : loss : 0.047354, loss_ce: 0.016967
2022-01-08 00:55:41,562 iteration 1635 : loss : 0.029076, loss_ce: 0.013574
2022-01-08 00:55:43,004 iteration 1636 : loss : 0.038901, loss_ce: 0.018519
2022-01-08 00:55:44,385 iteration 1637 : loss : 0.046405, loss_ce: 0.018282
2022-01-08 00:55:45,732 iteration 1638 : loss : 0.043612, loss_ce: 0.016639
2022-01-08 00:55:47,186 iteration 1639 : loss : 0.051273, loss_ce: 0.012417
2022-01-08 00:55:48,574 iteration 1640 : loss : 0.063155, loss_ce: 0.015637
2022-01-08 00:55:49,861 iteration 1641 : loss : 0.027680, loss_ce: 0.012555
2022-01-08 00:55:51,204 iteration 1642 : loss : 0.027291, loss_ce: 0.009642
2022-01-08 00:55:52,597 iteration 1643 : loss : 0.035986, loss_ce: 0.010734
2022-01-08 00:55:53,934 iteration 1644 : loss : 0.032899, loss_ce: 0.014119
2022-01-08 00:55:55,284 iteration 1645 : loss : 0.040980, loss_ce: 0.015352
2022-01-08 00:55:56,647 iteration 1646 : loss : 0.039825, loss_ce: 0.015554
2022-01-08 00:55:57,995 iteration 1647 : loss : 0.042957, loss_ce: 0.015364
2022-01-08 00:55:59,370 iteration 1648 : loss : 0.052911, loss_ce: 0.022011
2022-01-08 00:56:00,684 iteration 1649 : loss : 0.049367, loss_ce: 0.022176
 24%|███████▎                      | 97/400 [41:23<2:06:03, 24.96s/it]2022-01-08 00:56:02,114 iteration 1650 : loss : 0.042490, loss_ce: 0.019793
2022-01-08 00:56:03,587 iteration 1651 : loss : 0.063759, loss_ce: 0.020007
2022-01-08 00:56:04,890 iteration 1652 : loss : 0.040952, loss_ce: 0.012836
2022-01-08 00:56:06,197 iteration 1653 : loss : 0.032593, loss_ce: 0.011729
2022-01-08 00:56:07,603 iteration 1654 : loss : 0.064806, loss_ce: 0.019071
2022-01-08 00:56:09,023 iteration 1655 : loss : 0.062651, loss_ce: 0.025875
2022-01-08 00:56:10,377 iteration 1656 : loss : 0.031866, loss_ce: 0.011651
2022-01-08 00:56:11,844 iteration 1657 : loss : 0.062978, loss_ce: 0.021023
2022-01-08 00:56:13,272 iteration 1658 : loss : 0.061956, loss_ce: 0.038252
2022-01-08 00:56:14,581 iteration 1659 : loss : 0.026783, loss_ce: 0.009451
2022-01-08 00:56:16,052 iteration 1660 : loss : 0.048078, loss_ce: 0.018117
2022-01-08 00:56:17,489 iteration 1661 : loss : 0.035301, loss_ce: 0.012097
2022-01-08 00:56:18,855 iteration 1662 : loss : 0.049180, loss_ce: 0.020451
2022-01-08 00:56:20,100 iteration 1663 : loss : 0.030057, loss_ce: 0.014897
2022-01-08 00:56:21,534 iteration 1664 : loss : 0.057026, loss_ce: 0.020990
2022-01-08 00:56:22,938 iteration 1665 : loss : 0.045552, loss_ce: 0.016645
2022-01-08 00:56:24,362 iteration 1666 : loss : 0.051344, loss_ce: 0.022817
 24%|███████▎                      | 98/400 [41:47<2:03:42, 24.58s/it]2022-01-08 00:56:25,797 iteration 1667 : loss : 0.057976, loss_ce: 0.019179
2022-01-08 00:56:27,167 iteration 1668 : loss : 0.038832, loss_ce: 0.014391
2022-01-08 00:56:28,504 iteration 1669 : loss : 0.033718, loss_ce: 0.013996
2022-01-08 00:56:29,911 iteration 1670 : loss : 0.061575, loss_ce: 0.030383
2022-01-08 00:56:31,272 iteration 1671 : loss : 0.034722, loss_ce: 0.013479
2022-01-08 00:56:32,704 iteration 1672 : loss : 0.038772, loss_ce: 0.016781
2022-01-08 00:56:34,086 iteration 1673 : loss : 0.047738, loss_ce: 0.020018
2022-01-08 00:56:35,513 iteration 1674 : loss : 0.048801, loss_ce: 0.026194
2022-01-08 00:56:36,851 iteration 1675 : loss : 0.045685, loss_ce: 0.013537
2022-01-08 00:56:38,205 iteration 1676 : loss : 0.055452, loss_ce: 0.020844
2022-01-08 00:56:39,645 iteration 1677 : loss : 0.050314, loss_ce: 0.023863
2022-01-08 00:56:40,896 iteration 1678 : loss : 0.035844, loss_ce: 0.015558
2022-01-08 00:56:42,229 iteration 1679 : loss : 0.033471, loss_ce: 0.009015
2022-01-08 00:56:43,636 iteration 1680 : loss : 0.042436, loss_ce: 0.020632
2022-01-08 00:56:45,048 iteration 1681 : loss : 0.052067, loss_ce: 0.036121
2022-01-08 00:56:46,456 iteration 1682 : loss : 0.033976, loss_ce: 0.012714
2022-01-08 00:56:47,765 iteration 1683 : loss : 0.032088, loss_ce: 0.010994
 25%|███████▍                      | 99/400 [42:11<2:01:31, 24.23s/it]2022-01-08 00:56:49,184 iteration 1684 : loss : 0.064305, loss_ce: 0.026839
2022-01-08 00:56:50,545 iteration 1685 : loss : 0.039690, loss_ce: 0.013640
2022-01-08 00:56:51,894 iteration 1686 : loss : 0.027066, loss_ce: 0.010286
2022-01-08 00:56:53,200 iteration 1687 : loss : 0.034308, loss_ce: 0.016036
2022-01-08 00:56:54,691 iteration 1688 : loss : 0.032372, loss_ce: 0.014156
2022-01-08 00:56:56,071 iteration 1689 : loss : 0.044988, loss_ce: 0.017710
2022-01-08 00:56:57,478 iteration 1690 : loss : 0.049349, loss_ce: 0.025510
2022-01-08 00:56:58,777 iteration 1691 : loss : 0.093205, loss_ce: 0.024174
2022-01-08 00:57:00,190 iteration 1692 : loss : 0.083969, loss_ce: 0.026630
2022-01-08 00:57:01,437 iteration 1693 : loss : 0.033627, loss_ce: 0.013862
2022-01-08 00:57:02,836 iteration 1694 : loss : 0.042618, loss_ce: 0.016885
2022-01-08 00:57:04,262 iteration 1695 : loss : 0.053693, loss_ce: 0.021303
2022-01-08 00:57:05,691 iteration 1696 : loss : 0.057048, loss_ce: 0.017333
2022-01-08 00:57:07,042 iteration 1697 : loss : 0.037849, loss_ce: 0.014989
2022-01-08 00:57:08,451 iteration 1698 : loss : 0.043744, loss_ce: 0.016571
2022-01-08 00:57:09,826 iteration 1699 : loss : 0.036720, loss_ce: 0.018649
2022-01-08 00:57:09,827 Training Data Eval:
2022-01-08 00:57:16,710   Average segmentation loss on training set: 0.0627
2022-01-08 00:57:16,710 Validation Data Eval:
2022-01-08 00:57:19,082   Average segmentation loss on validation set: 0.2131
2022-01-08 00:57:20,439 iteration 1700 : loss : 0.044757, loss_ce: 0.017821
 25%|███████▎                     | 100/400 [42:43<2:13:47, 26.76s/it]2022-01-08 00:57:21,854 iteration 1701 : loss : 0.032475, loss_ce: 0.014224
2022-01-08 00:57:23,190 iteration 1702 : loss : 0.047030, loss_ce: 0.019079
2022-01-08 00:57:24,624 iteration 1703 : loss : 0.039564, loss_ce: 0.016384
2022-01-08 00:57:26,047 iteration 1704 : loss : 0.039538, loss_ce: 0.015931
2022-01-08 00:57:27,418 iteration 1705 : loss : 0.030763, loss_ce: 0.011151
2022-01-08 00:57:28,835 iteration 1706 : loss : 0.066823, loss_ce: 0.022280
2022-01-08 00:57:30,182 iteration 1707 : loss : 0.045611, loss_ce: 0.019365
2022-01-08 00:57:31,534 iteration 1708 : loss : 0.035181, loss_ce: 0.014258
2022-01-08 00:57:33,081 iteration 1709 : loss : 0.059076, loss_ce: 0.023589
2022-01-08 00:57:34,419 iteration 1710 : loss : 0.029752, loss_ce: 0.013869
2022-01-08 00:57:35,812 iteration 1711 : loss : 0.044151, loss_ce: 0.017194
2022-01-08 00:57:37,188 iteration 1712 : loss : 0.045322, loss_ce: 0.014937
2022-01-08 00:57:38,543 iteration 1713 : loss : 0.046007, loss_ce: 0.016397
2022-01-08 00:57:39,885 iteration 1714 : loss : 0.028070, loss_ce: 0.011880
2022-01-08 00:57:41,321 iteration 1715 : loss : 0.069669, loss_ce: 0.026044
2022-01-08 00:57:42,674 iteration 1716 : loss : 0.050638, loss_ce: 0.016772
2022-01-08 00:57:44,067 iteration 1717 : loss : 0.033868, loss_ce: 0.014415
 25%|███████▎                     | 101/400 [43:07<2:08:40, 25.82s/it]2022-01-08 00:57:45,517 iteration 1718 : loss : 0.039085, loss_ce: 0.015571
2022-01-08 00:57:46,951 iteration 1719 : loss : 0.037467, loss_ce: 0.010835
2022-01-08 00:57:48,346 iteration 1720 : loss : 0.040012, loss_ce: 0.013116
2022-01-08 00:57:49,729 iteration 1721 : loss : 0.050447, loss_ce: 0.022856
2022-01-08 00:57:51,049 iteration 1722 : loss : 0.024328, loss_ce: 0.009494
2022-01-08 00:57:52,395 iteration 1723 : loss : 0.035656, loss_ce: 0.016537
2022-01-08 00:57:53,704 iteration 1724 : loss : 0.030427, loss_ce: 0.012592
2022-01-08 00:57:55,026 iteration 1725 : loss : 0.038755, loss_ce: 0.022403
2022-01-08 00:57:56,497 iteration 1726 : loss : 0.042012, loss_ce: 0.017416
2022-01-08 00:57:57,899 iteration 1727 : loss : 0.047975, loss_ce: 0.017920
2022-01-08 00:57:59,359 iteration 1728 : loss : 0.059118, loss_ce: 0.027591
2022-01-08 00:58:00,686 iteration 1729 : loss : 0.034739, loss_ce: 0.012342
2022-01-08 00:58:02,074 iteration 1730 : loss : 0.030292, loss_ce: 0.010914
2022-01-08 00:58:03,402 iteration 1731 : loss : 0.034265, loss_ce: 0.015581
2022-01-08 00:58:04,785 iteration 1732 : loss : 0.034462, loss_ce: 0.014684
2022-01-08 00:58:06,174 iteration 1733 : loss : 0.042121, loss_ce: 0.012999
2022-01-08 00:58:07,521 iteration 1734 : loss : 0.048843, loss_ce: 0.012319
 26%|███████▍                     | 102/400 [43:30<2:04:42, 25.11s/it]2022-01-08 00:58:08,993 iteration 1735 : loss : 0.038923, loss_ce: 0.010972
2022-01-08 00:58:10,361 iteration 1736 : loss : 0.039114, loss_ce: 0.014889
2022-01-08 00:58:11,753 iteration 1737 : loss : 0.049062, loss_ce: 0.023632
2022-01-08 00:58:13,109 iteration 1738 : loss : 0.032602, loss_ce: 0.012509
2022-01-08 00:58:14,460 iteration 1739 : loss : 0.040236, loss_ce: 0.022004
2022-01-08 00:58:15,802 iteration 1740 : loss : 0.038664, loss_ce: 0.010653
2022-01-08 00:58:17,252 iteration 1741 : loss : 0.034949, loss_ce: 0.010719
2022-01-08 00:58:18,627 iteration 1742 : loss : 0.043398, loss_ce: 0.015266
2022-01-08 00:58:19,979 iteration 1743 : loss : 0.042341, loss_ce: 0.024638
2022-01-08 00:58:21,300 iteration 1744 : loss : 0.042548, loss_ce: 0.015127
2022-01-08 00:58:22,737 iteration 1745 : loss : 0.039419, loss_ce: 0.016075
2022-01-08 00:58:24,075 iteration 1746 : loss : 0.041218, loss_ce: 0.015006
2022-01-08 00:58:25,429 iteration 1747 : loss : 0.030460, loss_ce: 0.010770
2022-01-08 00:58:26,756 iteration 1748 : loss : 0.024267, loss_ce: 0.011833
2022-01-08 00:58:28,147 iteration 1749 : loss : 0.032888, loss_ce: 0.011515
2022-01-08 00:58:29,467 iteration 1750 : loss : 0.032223, loss_ce: 0.017544
2022-01-08 00:58:30,794 iteration 1751 : loss : 0.032884, loss_ce: 0.011756
 26%|███████▍                     | 103/400 [43:54<2:01:34, 24.56s/it]2022-01-08 00:58:32,203 iteration 1752 : loss : 0.045196, loss_ce: 0.018399
2022-01-08 00:58:33,648 iteration 1753 : loss : 0.041351, loss_ce: 0.018186
2022-01-08 00:58:35,040 iteration 1754 : loss : 0.031862, loss_ce: 0.014679
2022-01-08 00:58:36,415 iteration 1755 : loss : 0.041130, loss_ce: 0.015187
2022-01-08 00:58:37,845 iteration 1756 : loss : 0.044363, loss_ce: 0.019940
2022-01-08 00:58:39,225 iteration 1757 : loss : 0.073922, loss_ce: 0.017057
2022-01-08 00:58:40,606 iteration 1758 : loss : 0.035377, loss_ce: 0.013905
2022-01-08 00:58:41,963 iteration 1759 : loss : 0.031332, loss_ce: 0.010010
2022-01-08 00:58:43,372 iteration 1760 : loss : 0.035622, loss_ce: 0.015476
2022-01-08 00:58:44,685 iteration 1761 : loss : 0.028396, loss_ce: 0.011674
2022-01-08 00:58:46,113 iteration 1762 : loss : 0.055109, loss_ce: 0.023235
2022-01-08 00:58:47,441 iteration 1763 : loss : 0.057285, loss_ce: 0.017563
2022-01-08 00:58:48,836 iteration 1764 : loss : 0.103220, loss_ce: 0.028543
2022-01-08 00:58:50,237 iteration 1765 : loss : 0.066056, loss_ce: 0.017537
2022-01-08 00:58:51,551 iteration 1766 : loss : 0.029581, loss_ce: 0.012213
2022-01-08 00:58:52,919 iteration 1767 : loss : 0.058901, loss_ce: 0.024492
2022-01-08 00:58:54,366 iteration 1768 : loss : 0.055184, loss_ce: 0.017422
 26%|███████▌                     | 104/400 [44:17<1:59:41, 24.26s/it]2022-01-08 00:58:55,757 iteration 1769 : loss : 0.041059, loss_ce: 0.011714
2022-01-08 00:58:57,303 iteration 1770 : loss : 0.068257, loss_ce: 0.032452
2022-01-08 00:58:58,659 iteration 1771 : loss : 0.045254, loss_ce: 0.016190
2022-01-08 00:58:59,975 iteration 1772 : loss : 0.043450, loss_ce: 0.014265
2022-01-08 00:59:01,279 iteration 1773 : loss : 0.039269, loss_ce: 0.012766
2022-01-08 00:59:02,666 iteration 1774 : loss : 0.033361, loss_ce: 0.014144
2022-01-08 00:59:04,094 iteration 1775 : loss : 0.037378, loss_ce: 0.017750
2022-01-08 00:59:05,449 iteration 1776 : loss : 0.056974, loss_ce: 0.017383
2022-01-08 00:59:06,789 iteration 1777 : loss : 0.052212, loss_ce: 0.021250
2022-01-08 00:59:08,147 iteration 1778 : loss : 0.049779, loss_ce: 0.019248
2022-01-08 00:59:09,535 iteration 1779 : loss : 0.039846, loss_ce: 0.016729
2022-01-08 00:59:10,890 iteration 1780 : loss : 0.036749, loss_ce: 0.014442
2022-01-08 00:59:12,254 iteration 1781 : loss : 0.042865, loss_ce: 0.021366
2022-01-08 00:59:13,654 iteration 1782 : loss : 0.036865, loss_ce: 0.014642
2022-01-08 00:59:14,981 iteration 1783 : loss : 0.037072, loss_ce: 0.018597
2022-01-08 00:59:16,431 iteration 1784 : loss : 0.039196, loss_ce: 0.015009
2022-01-08 00:59:16,432 Training Data Eval:
2022-01-08 00:59:23,292   Average segmentation loss on training set: 0.0386
2022-01-08 00:59:23,292 Validation Data Eval:
2022-01-08 00:59:25,669   Average segmentation loss on validation set: 0.1716
2022-01-08 00:59:27,078 iteration 1785 : loss : 0.052108, loss_ce: 0.020863
 26%|███████▌                     | 105/400 [44:50<2:11:45, 26.80s/it]2022-01-08 00:59:28,613 iteration 1786 : loss : 0.052076, loss_ce: 0.026724
2022-01-08 00:59:30,068 iteration 1787 : loss : 0.035239, loss_ce: 0.014408
2022-01-08 00:59:31,461 iteration 1788 : loss : 0.035109, loss_ce: 0.016465
2022-01-08 00:59:32,840 iteration 1789 : loss : 0.036751, loss_ce: 0.012730
2022-01-08 00:59:34,206 iteration 1790 : loss : 0.035425, loss_ce: 0.017625
2022-01-08 00:59:35,516 iteration 1791 : loss : 0.025401, loss_ce: 0.012186
2022-01-08 00:59:36,916 iteration 1792 : loss : 0.045072, loss_ce: 0.014670
2022-01-08 00:59:38,330 iteration 1793 : loss : 0.037476, loss_ce: 0.017529
2022-01-08 00:59:39,667 iteration 1794 : loss : 0.035903, loss_ce: 0.017510
2022-01-08 00:59:41,040 iteration 1795 : loss : 0.044205, loss_ce: 0.016213
2022-01-08 00:59:42,424 iteration 1796 : loss : 0.035011, loss_ce: 0.012286
2022-01-08 00:59:43,891 iteration 1797 : loss : 0.031232, loss_ce: 0.012763
2022-01-08 00:59:45,275 iteration 1798 : loss : 0.044345, loss_ce: 0.016847
2022-01-08 00:59:46,675 iteration 1799 : loss : 0.040231, loss_ce: 0.015751
2022-01-08 00:59:48,060 iteration 1800 : loss : 0.048734, loss_ce: 0.026136
2022-01-08 00:59:49,458 iteration 1801 : loss : 0.038362, loss_ce: 0.017905
2022-01-08 00:59:50,748 iteration 1802 : loss : 0.054678, loss_ce: 0.015200
 26%|███████▋                     | 106/400 [45:14<2:06:42, 25.86s/it]2022-01-08 00:59:52,189 iteration 1803 : loss : 0.042689, loss_ce: 0.016184
2022-01-08 00:59:53,539 iteration 1804 : loss : 0.038805, loss_ce: 0.011050
2022-01-08 00:59:54,842 iteration 1805 : loss : 0.029676, loss_ce: 0.013286
2022-01-08 00:59:56,174 iteration 1806 : loss : 0.039846, loss_ce: 0.013426
2022-01-08 00:59:57,574 iteration 1807 : loss : 0.034818, loss_ce: 0.011452
2022-01-08 00:59:58,967 iteration 1808 : loss : 0.043603, loss_ce: 0.017944
2022-01-08 01:00:00,305 iteration 1809 : loss : 0.043256, loss_ce: 0.018781
2022-01-08 01:00:01,691 iteration 1810 : loss : 0.050399, loss_ce: 0.022999
2022-01-08 01:00:03,098 iteration 1811 : loss : 0.032567, loss_ce: 0.011765
2022-01-08 01:00:04,541 iteration 1812 : loss : 0.036738, loss_ce: 0.017479
2022-01-08 01:00:05,941 iteration 1813 : loss : 0.056542, loss_ce: 0.017802
2022-01-08 01:00:07,393 iteration 1814 : loss : 0.051175, loss_ce: 0.021833
2022-01-08 01:00:08,781 iteration 1815 : loss : 0.030346, loss_ce: 0.014677
2022-01-08 01:00:10,204 iteration 1816 : loss : 0.061914, loss_ce: 0.022758
2022-01-08 01:00:11,576 iteration 1817 : loss : 0.033828, loss_ce: 0.010214
2022-01-08 01:00:13,032 iteration 1818 : loss : 0.027716, loss_ce: 0.014961
2022-01-08 01:00:14,393 iteration 1819 : loss : 0.029315, loss_ce: 0.011050
 27%|███████▊                     | 107/400 [45:37<2:03:01, 25.19s/it]2022-01-08 01:00:15,872 iteration 1820 : loss : 0.046906, loss_ce: 0.014349
2022-01-08 01:00:17,213 iteration 1821 : loss : 0.028815, loss_ce: 0.010163
2022-01-08 01:00:18,543 iteration 1822 : loss : 0.034918, loss_ce: 0.017598
2022-01-08 01:00:19,943 iteration 1823 : loss : 0.030468, loss_ce: 0.012348
2022-01-08 01:00:21,304 iteration 1824 : loss : 0.024276, loss_ce: 0.008737
2022-01-08 01:00:22,682 iteration 1825 : loss : 0.052819, loss_ce: 0.016924
2022-01-08 01:00:23,979 iteration 1826 : loss : 0.032268, loss_ce: 0.013938
2022-01-08 01:00:25,314 iteration 1827 : loss : 0.033431, loss_ce: 0.009779
2022-01-08 01:00:26,719 iteration 1828 : loss : 0.032680, loss_ce: 0.015747
2022-01-08 01:00:28,117 iteration 1829 : loss : 0.045942, loss_ce: 0.011173
2022-01-08 01:00:29,490 iteration 1830 : loss : 0.035532, loss_ce: 0.016525
2022-01-08 01:00:30,851 iteration 1831 : loss : 0.032794, loss_ce: 0.013711
2022-01-08 01:00:32,199 iteration 1832 : loss : 0.029317, loss_ce: 0.010592
2022-01-08 01:00:33,536 iteration 1833 : loss : 0.030337, loss_ce: 0.016683
2022-01-08 01:00:34,976 iteration 1834 : loss : 0.038765, loss_ce: 0.014605
2022-01-08 01:00:36,271 iteration 1835 : loss : 0.031436, loss_ce: 0.010894
2022-01-08 01:00:37,606 iteration 1836 : loss : 0.039328, loss_ce: 0.014972
 27%|███████▊                     | 108/400 [46:00<1:59:43, 24.60s/it]2022-01-08 01:00:38,962 iteration 1837 : loss : 0.063004, loss_ce: 0.014030
2022-01-08 01:00:40,397 iteration 1838 : loss : 0.037449, loss_ce: 0.016318
2022-01-08 01:00:41,708 iteration 1839 : loss : 0.043790, loss_ce: 0.023724
2022-01-08 01:00:43,087 iteration 1840 : loss : 0.046904, loss_ce: 0.013322
2022-01-08 01:00:44,465 iteration 1841 : loss : 0.032071, loss_ce: 0.011436
2022-01-08 01:00:45,926 iteration 1842 : loss : 0.031448, loss_ce: 0.011499
2022-01-08 01:00:47,287 iteration 1843 : loss : 0.033862, loss_ce: 0.015088
2022-01-08 01:00:48,656 iteration 1844 : loss : 0.036841, loss_ce: 0.016358
2022-01-08 01:00:50,020 iteration 1845 : loss : 0.030835, loss_ce: 0.012538
2022-01-08 01:00:51,393 iteration 1846 : loss : 0.048235, loss_ce: 0.012512
2022-01-08 01:00:52,674 iteration 1847 : loss : 0.027280, loss_ce: 0.009710
2022-01-08 01:00:54,062 iteration 1848 : loss : 0.034497, loss_ce: 0.011209
2022-01-08 01:00:55,436 iteration 1849 : loss : 0.050316, loss_ce: 0.018304
2022-01-08 01:00:56,819 iteration 1850 : loss : 0.041371, loss_ce: 0.014851
2022-01-08 01:00:58,228 iteration 1851 : loss : 0.032665, loss_ce: 0.013414
2022-01-08 01:00:59,512 iteration 1852 : loss : 0.028749, loss_ce: 0.015894
2022-01-08 01:01:00,908 iteration 1853 : loss : 0.031526, loss_ce: 0.014543
 27%|███████▉                     | 109/400 [46:24<1:57:25, 24.21s/it]2022-01-08 01:01:02,347 iteration 1854 : loss : 0.036824, loss_ce: 0.012271
2022-01-08 01:01:03,774 iteration 1855 : loss : 0.036820, loss_ce: 0.014254
2022-01-08 01:01:05,052 iteration 1856 : loss : 0.033505, loss_ce: 0.015660
2022-01-08 01:01:06,342 iteration 1857 : loss : 0.024576, loss_ce: 0.010923
2022-01-08 01:01:07,671 iteration 1858 : loss : 0.023509, loss_ce: 0.011205
2022-01-08 01:01:09,066 iteration 1859 : loss : 0.041874, loss_ce: 0.014920
2022-01-08 01:01:10,477 iteration 1860 : loss : 0.037482, loss_ce: 0.014164
2022-01-08 01:01:11,854 iteration 1861 : loss : 0.026537, loss_ce: 0.013337
2022-01-08 01:01:13,239 iteration 1862 : loss : 0.032109, loss_ce: 0.015844
2022-01-08 01:01:14,631 iteration 1863 : loss : 0.048653, loss_ce: 0.019273
2022-01-08 01:01:16,028 iteration 1864 : loss : 0.032897, loss_ce: 0.012988
2022-01-08 01:01:17,420 iteration 1865 : loss : 0.039692, loss_ce: 0.012074
2022-01-08 01:01:18,809 iteration 1866 : loss : 0.021581, loss_ce: 0.008006
2022-01-08 01:01:20,079 iteration 1867 : loss : 0.026370, loss_ce: 0.008899
2022-01-08 01:01:21,413 iteration 1868 : loss : 0.029072, loss_ce: 0.010334
2022-01-08 01:01:22,748 iteration 1869 : loss : 0.028461, loss_ce: 0.013067
2022-01-08 01:01:22,748 Training Data Eval:
2022-01-08 01:01:29,636   Average segmentation loss on training set: 0.0249
2022-01-08 01:01:29,637 Validation Data Eval:
2022-01-08 01:01:32,013   Average segmentation loss on validation set: 0.0803
2022-01-08 01:01:33,411 iteration 1870 : loss : 0.042305, loss_ce: 0.017753
 28%|███████▉                     | 110/400 [46:56<2:09:03, 26.70s/it]2022-01-08 01:01:34,887 iteration 1871 : loss : 0.046735, loss_ce: 0.014484
2022-01-08 01:01:36,295 iteration 1872 : loss : 0.027679, loss_ce: 0.008809
2022-01-08 01:01:37,725 iteration 1873 : loss : 0.025105, loss_ce: 0.008828
2022-01-08 01:01:39,128 iteration 1874 : loss : 0.026918, loss_ce: 0.009384
2022-01-08 01:01:40,430 iteration 1875 : loss : 0.023617, loss_ce: 0.009939
2022-01-08 01:01:41,718 iteration 1876 : loss : 0.024551, loss_ce: 0.011268
2022-01-08 01:01:43,132 iteration 1877 : loss : 0.030171, loss_ce: 0.010961
2022-01-08 01:01:44,524 iteration 1878 : loss : 0.060823, loss_ce: 0.035254
2022-01-08 01:01:45,903 iteration 1879 : loss : 0.040475, loss_ce: 0.014621
2022-01-08 01:01:47,301 iteration 1880 : loss : 0.037246, loss_ce: 0.017981
2022-01-08 01:01:48,657 iteration 1881 : loss : 0.035680, loss_ce: 0.016146
2022-01-08 01:01:50,108 iteration 1882 : loss : 0.060883, loss_ce: 0.018410
2022-01-08 01:01:51,504 iteration 1883 : loss : 0.025016, loss_ce: 0.009670
2022-01-08 01:01:52,840 iteration 1884 : loss : 0.030483, loss_ce: 0.012052
2022-01-08 01:01:54,139 iteration 1885 : loss : 0.022699, loss_ce: 0.008747
2022-01-08 01:01:55,529 iteration 1886 : loss : 0.053533, loss_ce: 0.022822
2022-01-08 01:01:56,918 iteration 1887 : loss : 0.045026, loss_ce: 0.021027
 28%|████████                     | 111/400 [47:20<2:03:58, 25.74s/it]2022-01-08 01:01:58,441 iteration 1888 : loss : 0.055247, loss_ce: 0.018851
2022-01-08 01:01:59,758 iteration 1889 : loss : 0.043894, loss_ce: 0.015740
2022-01-08 01:02:01,060 iteration 1890 : loss : 0.030499, loss_ce: 0.013321
2022-01-08 01:02:02,373 iteration 1891 : loss : 0.032623, loss_ce: 0.011089
2022-01-08 01:02:03,733 iteration 1892 : loss : 0.029484, loss_ce: 0.013015
2022-01-08 01:02:05,112 iteration 1893 : loss : 0.027171, loss_ce: 0.008784
2022-01-08 01:02:06,480 iteration 1894 : loss : 0.048044, loss_ce: 0.020983
2022-01-08 01:02:07,791 iteration 1895 : loss : 0.026501, loss_ce: 0.010272
2022-01-08 01:02:09,240 iteration 1896 : loss : 0.031239, loss_ce: 0.012637
2022-01-08 01:02:10,640 iteration 1897 : loss : 0.039986, loss_ce: 0.012313
2022-01-08 01:02:12,084 iteration 1898 : loss : 0.036626, loss_ce: 0.017102
2022-01-08 01:02:13,435 iteration 1899 : loss : 0.035163, loss_ce: 0.010900
2022-01-08 01:02:14,749 iteration 1900 : loss : 0.043618, loss_ce: 0.021487
2022-01-08 01:02:16,062 iteration 1901 : loss : 0.026240, loss_ce: 0.008916
2022-01-08 01:02:17,441 iteration 1902 : loss : 0.029429, loss_ce: 0.011948
2022-01-08 01:02:18,859 iteration 1903 : loss : 0.031142, loss_ce: 0.013223
2022-01-08 01:02:20,285 iteration 1904 : loss : 0.030211, loss_ce: 0.011487
 28%|████████                     | 112/400 [47:43<2:00:08, 25.03s/it]2022-01-08 01:02:21,618 iteration 1905 : loss : 0.034321, loss_ce: 0.016565
2022-01-08 01:02:23,037 iteration 1906 : loss : 0.027272, loss_ce: 0.008923
2022-01-08 01:02:24,458 iteration 1907 : loss : 0.026307, loss_ce: 0.013878
2022-01-08 01:02:25,961 iteration 1908 : loss : 0.045260, loss_ce: 0.019686
2022-01-08 01:02:27,243 iteration 1909 : loss : 0.021458, loss_ce: 0.009201
2022-01-08 01:02:28,602 iteration 1910 : loss : 0.028586, loss_ce: 0.010688
2022-01-08 01:02:30,009 iteration 1911 : loss : 0.035295, loss_ce: 0.014607
2022-01-08 01:02:31,487 iteration 1912 : loss : 0.040028, loss_ce: 0.016220
2022-01-08 01:02:32,886 iteration 1913 : loss : 0.041684, loss_ce: 0.016808
2022-01-08 01:02:34,332 iteration 1914 : loss : 0.060616, loss_ce: 0.026292
2022-01-08 01:02:35,702 iteration 1915 : loss : 0.033517, loss_ce: 0.013069
2022-01-08 01:02:37,051 iteration 1916 : loss : 0.033064, loss_ce: 0.012301
2022-01-08 01:02:38,418 iteration 1917 : loss : 0.025881, loss_ce: 0.010047
2022-01-08 01:02:39,843 iteration 1918 : loss : 0.031672, loss_ce: 0.014195
2022-01-08 01:02:41,179 iteration 1919 : loss : 0.034764, loss_ce: 0.011836
2022-01-08 01:02:42,478 iteration 1920 : loss : 0.028342, loss_ce: 0.012996
2022-01-08 01:02:43,928 iteration 1921 : loss : 0.040732, loss_ce: 0.014299
 28%|████████▏                    | 113/400 [48:07<1:57:44, 24.61s/it]2022-01-08 01:02:45,390 iteration 1922 : loss : 0.048593, loss_ce: 0.020825
2022-01-08 01:02:46,713 iteration 1923 : loss : 0.027306, loss_ce: 0.010718
2022-01-08 01:02:48,147 iteration 1924 : loss : 0.060776, loss_ce: 0.015324
2022-01-08 01:02:49,473 iteration 1925 : loss : 0.043332, loss_ce: 0.012610
2022-01-08 01:02:50,909 iteration 1926 : loss : 0.027065, loss_ce: 0.009840
2022-01-08 01:02:52,351 iteration 1927 : loss : 0.034652, loss_ce: 0.016328
2022-01-08 01:02:53,667 iteration 1928 : loss : 0.034156, loss_ce: 0.010362
2022-01-08 01:02:55,184 iteration 1929 : loss : 0.037425, loss_ce: 0.016847
2022-01-08 01:02:56,567 iteration 1930 : loss : 0.032491, loss_ce: 0.014243
2022-01-08 01:02:57,993 iteration 1931 : loss : 0.039816, loss_ce: 0.013559
2022-01-08 01:02:59,382 iteration 1932 : loss : 0.057134, loss_ce: 0.016071
2022-01-08 01:03:00,783 iteration 1933 : loss : 0.038670, loss_ce: 0.015367
2022-01-08 01:03:02,144 iteration 1934 : loss : 0.061788, loss_ce: 0.019588
2022-01-08 01:03:03,526 iteration 1935 : loss : 0.025000, loss_ce: 0.010316
2022-01-08 01:03:04,872 iteration 1936 : loss : 0.027921, loss_ce: 0.011528
2022-01-08 01:03:06,185 iteration 1937 : loss : 0.032312, loss_ce: 0.010927
2022-01-08 01:03:07,577 iteration 1938 : loss : 0.030079, loss_ce: 0.012771
 28%|████████▎                    | 114/400 [48:30<1:55:56, 24.32s/it]2022-01-08 01:03:09,021 iteration 1939 : loss : 0.032269, loss_ce: 0.014063
2022-01-08 01:03:10,345 iteration 1940 : loss : 0.026450, loss_ce: 0.012468
2022-01-08 01:03:11,729 iteration 1941 : loss : 0.029923, loss_ce: 0.011563
2022-01-08 01:03:13,116 iteration 1942 : loss : 0.040465, loss_ce: 0.015207
2022-01-08 01:03:14,474 iteration 1943 : loss : 0.033657, loss_ce: 0.010832
2022-01-08 01:03:15,857 iteration 1944 : loss : 0.043381, loss_ce: 0.017634
2022-01-08 01:03:17,247 iteration 1945 : loss : 0.035946, loss_ce: 0.014686
2022-01-08 01:03:18,586 iteration 1946 : loss : 0.041139, loss_ce: 0.013737
2022-01-08 01:03:19,944 iteration 1947 : loss : 0.044718, loss_ce: 0.011696
2022-01-08 01:03:21,337 iteration 1948 : loss : 0.041976, loss_ce: 0.020408
2022-01-08 01:03:22,656 iteration 1949 : loss : 0.031914, loss_ce: 0.011675
2022-01-08 01:03:24,057 iteration 1950 : loss : 0.067195, loss_ce: 0.039036
2022-01-08 01:03:25,465 iteration 1951 : loss : 0.031090, loss_ce: 0.011275
2022-01-08 01:03:26,871 iteration 1952 : loss : 0.035139, loss_ce: 0.015982
2022-01-08 01:03:28,174 iteration 1953 : loss : 0.059580, loss_ce: 0.035387
2022-01-08 01:03:29,564 iteration 1954 : loss : 0.029008, loss_ce: 0.013790
2022-01-08 01:03:29,565 Training Data Eval:
2022-01-08 01:03:36,450   Average segmentation loss on training set: 0.0319
2022-01-08 01:03:36,451 Validation Data Eval:
2022-01-08 01:03:38,827   Average segmentation loss on validation set: 0.0746
2022-01-08 01:03:40,241 iteration 1955 : loss : 0.044591, loss_ce: 0.023004
 29%|████████▎                    | 115/400 [49:03<2:07:25, 26.83s/it]2022-01-08 01:03:41,642 iteration 1956 : loss : 0.045470, loss_ce: 0.017984
2022-01-08 01:03:43,073 iteration 1957 : loss : 0.036610, loss_ce: 0.018047
2022-01-08 01:03:44,451 iteration 1958 : loss : 0.054776, loss_ce: 0.013960
2022-01-08 01:03:45,797 iteration 1959 : loss : 0.039255, loss_ce: 0.014233
2022-01-08 01:03:47,066 iteration 1960 : loss : 0.035232, loss_ce: 0.015391
2022-01-08 01:03:48,326 iteration 1961 : loss : 0.027822, loss_ce: 0.010907
2022-01-08 01:03:49,741 iteration 1962 : loss : 0.026209, loss_ce: 0.010952
2022-01-08 01:03:51,058 iteration 1963 : loss : 0.029294, loss_ce: 0.011433
2022-01-08 01:03:52,465 iteration 1964 : loss : 0.044931, loss_ce: 0.013897
2022-01-08 01:03:53,757 iteration 1965 : loss : 0.028562, loss_ce: 0.013266
2022-01-08 01:03:55,132 iteration 1966 : loss : 0.027620, loss_ce: 0.011635
2022-01-08 01:03:56,606 iteration 1967 : loss : 0.034590, loss_ce: 0.012722
2022-01-08 01:03:58,119 iteration 1968 : loss : 0.057682, loss_ce: 0.024607
2022-01-08 01:03:59,526 iteration 1969 : loss : 0.049233, loss_ce: 0.020038
2022-01-08 01:04:00,828 iteration 1970 : loss : 0.025454, loss_ce: 0.010700
2022-01-08 01:04:02,199 iteration 1971 : loss : 0.029703, loss_ce: 0.011150
2022-01-08 01:04:03,573 iteration 1972 : loss : 0.034652, loss_ce: 0.012059
 29%|████████▍                    | 116/400 [49:26<2:02:01, 25.78s/it]2022-01-08 01:04:04,922 iteration 1973 : loss : 0.025824, loss_ce: 0.011280
2022-01-08 01:04:06,244 iteration 1974 : loss : 0.034140, loss_ce: 0.012798
2022-01-08 01:04:07,644 iteration 1975 : loss : 0.027114, loss_ce: 0.010551
2022-01-08 01:04:08,976 iteration 1976 : loss : 0.034049, loss_ce: 0.015008
2022-01-08 01:04:10,404 iteration 1977 : loss : 0.031868, loss_ce: 0.011985
2022-01-08 01:04:11,843 iteration 1978 : loss : 0.029245, loss_ce: 0.011110
2022-01-08 01:04:13,147 iteration 1979 : loss : 0.030788, loss_ce: 0.013249
2022-01-08 01:04:14,465 iteration 1980 : loss : 0.025466, loss_ce: 0.008572
2022-01-08 01:04:15,818 iteration 1981 : loss : 0.031560, loss_ce: 0.015683
2022-01-08 01:04:17,234 iteration 1982 : loss : 0.037451, loss_ce: 0.015153
2022-01-08 01:04:18,531 iteration 1983 : loss : 0.030814, loss_ce: 0.010541
2022-01-08 01:04:19,955 iteration 1984 : loss : 0.038452, loss_ce: 0.015883
2022-01-08 01:04:21,319 iteration 1985 : loss : 0.042125, loss_ce: 0.014876
2022-01-08 01:04:22,740 iteration 1986 : loss : 0.042720, loss_ce: 0.014024
2022-01-08 01:04:24,070 iteration 1987 : loss : 0.030509, loss_ce: 0.011587
2022-01-08 01:04:25,358 iteration 1988 : loss : 0.029398, loss_ce: 0.015147
2022-01-08 01:04:26,724 iteration 1989 : loss : 0.039140, loss_ce: 0.012372
 29%|████████▍                    | 117/400 [49:50<1:57:52, 24.99s/it]2022-01-08 01:04:28,136 iteration 1990 : loss : 0.040476, loss_ce: 0.011763
2022-01-08 01:04:29,425 iteration 1991 : loss : 0.024707, loss_ce: 0.010432
2022-01-08 01:04:30,830 iteration 1992 : loss : 0.028754, loss_ce: 0.009060
2022-01-08 01:04:32,241 iteration 1993 : loss : 0.028764, loss_ce: 0.009545
2022-01-08 01:04:33,673 iteration 1994 : loss : 0.039733, loss_ce: 0.017599
2022-01-08 01:04:35,017 iteration 1995 : loss : 0.030782, loss_ce: 0.011337
2022-01-08 01:04:36,418 iteration 1996 : loss : 0.030403, loss_ce: 0.012883
2022-01-08 01:04:37,730 iteration 1997 : loss : 0.026183, loss_ce: 0.011118
2022-01-08 01:04:39,091 iteration 1998 : loss : 0.040446, loss_ce: 0.014332
2022-01-08 01:04:40,487 iteration 1999 : loss : 0.030553, loss_ce: 0.013219
2022-01-08 01:04:41,850 iteration 2000 : loss : 0.029069, loss_ce: 0.014276
2022-01-08 01:04:43,153 iteration 2001 : loss : 0.033730, loss_ce: 0.011310
2022-01-08 01:04:44,554 iteration 2002 : loss : 0.030813, loss_ce: 0.013137
2022-01-08 01:04:45,857 iteration 2003 : loss : 0.031285, loss_ce: 0.009631
2022-01-08 01:04:47,269 iteration 2004 : loss : 0.033425, loss_ce: 0.018050
2022-01-08 01:04:48,600 iteration 2005 : loss : 0.031906, loss_ce: 0.015523
2022-01-08 01:04:49,953 iteration 2006 : loss : 0.035489, loss_ce: 0.012880
 30%|████████▌                    | 118/400 [50:13<1:54:57, 24.46s/it]2022-01-08 01:04:51,433 iteration 2007 : loss : 0.045229, loss_ce: 0.015207
2022-01-08 01:04:52,828 iteration 2008 : loss : 0.029990, loss_ce: 0.012619
2022-01-08 01:04:54,151 iteration 2009 : loss : 0.029392, loss_ce: 0.013939
2022-01-08 01:04:55,524 iteration 2010 : loss : 0.040925, loss_ce: 0.019325
2022-01-08 01:04:56,829 iteration 2011 : loss : 0.041814, loss_ce: 0.012144
2022-01-08 01:04:58,120 iteration 2012 : loss : 0.020743, loss_ce: 0.007856
2022-01-08 01:04:59,454 iteration 2013 : loss : 0.036917, loss_ce: 0.019155
2022-01-08 01:05:00,845 iteration 2014 : loss : 0.031225, loss_ce: 0.012455
2022-01-08 01:05:02,246 iteration 2015 : loss : 0.040128, loss_ce: 0.013078
2022-01-08 01:05:03,685 iteration 2016 : loss : 0.038149, loss_ce: 0.015720
2022-01-08 01:05:05,070 iteration 2017 : loss : 0.033077, loss_ce: 0.012245
2022-01-08 01:05:06,367 iteration 2018 : loss : 0.032335, loss_ce: 0.009976
2022-01-08 01:05:07,738 iteration 2019 : loss : 0.027170, loss_ce: 0.009910
2022-01-08 01:05:09,092 iteration 2020 : loss : 0.029952, loss_ce: 0.011578
2022-01-08 01:05:10,431 iteration 2021 : loss : 0.031780, loss_ce: 0.011368
2022-01-08 01:05:11,801 iteration 2022 : loss : 0.042506, loss_ce: 0.016067
2022-01-08 01:05:13,186 iteration 2023 : loss : 0.029400, loss_ce: 0.008971
 30%|████████▋                    | 119/400 [50:36<1:52:49, 24.09s/it]2022-01-08 01:05:14,621 iteration 2024 : loss : 0.043510, loss_ce: 0.019633
2022-01-08 01:05:16,122 iteration 2025 : loss : 0.050471, loss_ce: 0.020208
2022-01-08 01:05:17,439 iteration 2026 : loss : 0.031555, loss_ce: 0.010815
2022-01-08 01:05:18,838 iteration 2027 : loss : 0.031428, loss_ce: 0.010339
2022-01-08 01:05:20,110 iteration 2028 : loss : 0.026970, loss_ce: 0.011778
2022-01-08 01:05:21,421 iteration 2029 : loss : 0.032500, loss_ce: 0.008896
2022-01-08 01:05:22,814 iteration 2030 : loss : 0.027656, loss_ce: 0.009449
2022-01-08 01:05:24,203 iteration 2031 : loss : 0.032970, loss_ce: 0.012049
2022-01-08 01:05:25,566 iteration 2032 : loss : 0.033058, loss_ce: 0.014767
2022-01-08 01:05:26,829 iteration 2033 : loss : 0.023925, loss_ce: 0.010841
2022-01-08 01:05:28,157 iteration 2034 : loss : 0.024520, loss_ce: 0.007773
2022-01-08 01:05:29,561 iteration 2035 : loss : 0.030063, loss_ce: 0.011247
2022-01-08 01:05:30,898 iteration 2036 : loss : 0.025193, loss_ce: 0.010960
2022-01-08 01:05:32,218 iteration 2037 : loss : 0.036814, loss_ce: 0.015403
2022-01-08 01:05:33,678 iteration 2038 : loss : 0.050958, loss_ce: 0.015410
2022-01-08 01:05:35,096 iteration 2039 : loss : 0.032722, loss_ce: 0.011613
2022-01-08 01:05:35,096 Training Data Eval:
2022-01-08 01:05:41,963   Average segmentation loss on training set: 0.0208
2022-01-08 01:05:41,964 Validation Data Eval:
2022-01-08 01:05:44,336   Average segmentation loss on validation set: 0.0688
2022-01-08 01:05:45,762 iteration 2040 : loss : 0.032893, loss_ce: 0.016571
 30%|████████▋                    | 120/400 [51:09<2:04:18, 26.64s/it]2022-01-08 01:05:47,133 iteration 2041 : loss : 0.028415, loss_ce: 0.013514
2022-01-08 01:05:48,519 iteration 2042 : loss : 0.034203, loss_ce: 0.011519
2022-01-08 01:05:49,893 iteration 2043 : loss : 0.049338, loss_ce: 0.018370
2022-01-08 01:05:51,254 iteration 2044 : loss : 0.042167, loss_ce: 0.020577
2022-01-08 01:05:52,727 iteration 2045 : loss : 0.044024, loss_ce: 0.017300
2022-01-08 01:05:54,095 iteration 2046 : loss : 0.054819, loss_ce: 0.013945
2022-01-08 01:05:55,420 iteration 2047 : loss : 0.027036, loss_ce: 0.013024
2022-01-08 01:05:56,708 iteration 2048 : loss : 0.031386, loss_ce: 0.012838
2022-01-08 01:05:58,124 iteration 2049 : loss : 0.038120, loss_ce: 0.019064
2022-01-08 01:05:59,538 iteration 2050 : loss : 0.035789, loss_ce: 0.010310
2022-01-08 01:06:00,887 iteration 2051 : loss : 0.033122, loss_ce: 0.013267
2022-01-08 01:06:02,309 iteration 2052 : loss : 0.034663, loss_ce: 0.014112
2022-01-08 01:06:03,683 iteration 2053 : loss : 0.036817, loss_ce: 0.015742
2022-01-08 01:06:05,039 iteration 2054 : loss : 0.034899, loss_ce: 0.016676
2022-01-08 01:06:06,424 iteration 2055 : loss : 0.038768, loss_ce: 0.013680
2022-01-08 01:06:07,766 iteration 2056 : loss : 0.024188, loss_ce: 0.008677
2022-01-08 01:06:09,104 iteration 2057 : loss : 0.027161, loss_ce: 0.012896
 30%|████████▊                    | 121/400 [51:32<1:59:16, 25.65s/it]2022-01-08 01:06:10,502 iteration 2058 : loss : 0.028188, loss_ce: 0.015581
2022-01-08 01:06:11,931 iteration 2059 : loss : 0.039726, loss_ce: 0.014887
2022-01-08 01:06:13,374 iteration 2060 : loss : 0.038484, loss_ce: 0.015295
2022-01-08 01:06:14,739 iteration 2061 : loss : 0.029178, loss_ce: 0.013128
2022-01-08 01:06:16,064 iteration 2062 : loss : 0.040570, loss_ce: 0.015057
2022-01-08 01:06:17,608 iteration 2063 : loss : 0.056557, loss_ce: 0.027414
2022-01-08 01:06:18,967 iteration 2064 : loss : 0.031620, loss_ce: 0.012108
2022-01-08 01:06:20,426 iteration 2065 : loss : 0.068163, loss_ce: 0.022789
2022-01-08 01:06:21,792 iteration 2066 : loss : 0.030053, loss_ce: 0.012922
2022-01-08 01:06:23,205 iteration 2067 : loss : 0.037833, loss_ce: 0.012777
2022-01-08 01:06:24,520 iteration 2068 : loss : 0.033358, loss_ce: 0.013108
2022-01-08 01:06:25,905 iteration 2069 : loss : 0.029097, loss_ce: 0.013524
2022-01-08 01:06:27,342 iteration 2070 : loss : 0.037258, loss_ce: 0.017693
2022-01-08 01:06:28,673 iteration 2071 : loss : 0.028086, loss_ce: 0.010644
2022-01-08 01:06:30,004 iteration 2072 : loss : 0.029900, loss_ce: 0.013665
2022-01-08 01:06:31,441 iteration 2073 : loss : 0.044719, loss_ce: 0.018720
2022-01-08 01:06:32,755 iteration 2074 : loss : 0.029615, loss_ce: 0.009024
 30%|████████▊                    | 122/400 [51:56<1:56:03, 25.05s/it]2022-01-08 01:06:34,166 iteration 2075 : loss : 0.031180, loss_ce: 0.014393
2022-01-08 01:06:35,574 iteration 2076 : loss : 0.030060, loss_ce: 0.012591
2022-01-08 01:06:36,975 iteration 2077 : loss : 0.033420, loss_ce: 0.016889
2022-01-08 01:06:38,394 iteration 2078 : loss : 0.029359, loss_ce: 0.011385
2022-01-08 01:06:39,812 iteration 2079 : loss : 0.036117, loss_ce: 0.013947
2022-01-08 01:06:41,146 iteration 2080 : loss : 0.039918, loss_ce: 0.016909
2022-01-08 01:06:42,454 iteration 2081 : loss : 0.024872, loss_ce: 0.008614
2022-01-08 01:06:43,831 iteration 2082 : loss : 0.029830, loss_ce: 0.008800
2022-01-08 01:06:45,193 iteration 2083 : loss : 0.037781, loss_ce: 0.014244
2022-01-08 01:06:46,512 iteration 2084 : loss : 0.031940, loss_ce: 0.014375
2022-01-08 01:06:47,911 iteration 2085 : loss : 0.060428, loss_ce: 0.009099
2022-01-08 01:06:49,395 iteration 2086 : loss : 0.041146, loss_ce: 0.012930
2022-01-08 01:06:50,682 iteration 2087 : loss : 0.025416, loss_ce: 0.008209
2022-01-08 01:06:52,024 iteration 2088 : loss : 0.028447, loss_ce: 0.010535
2022-01-08 01:06:53,404 iteration 2089 : loss : 0.029588, loss_ce: 0.011652
2022-01-08 01:06:54,775 iteration 2090 : loss : 0.038900, loss_ce: 0.018263
2022-01-08 01:06:56,217 iteration 2091 : loss : 0.042909, loss_ce: 0.016692
 31%|████████▉                    | 123/400 [52:19<1:53:26, 24.57s/it]2022-01-08 01:06:57,596 iteration 2092 : loss : 0.032607, loss_ce: 0.009905
2022-01-08 01:06:58,891 iteration 2093 : loss : 0.019910, loss_ce: 0.008235
2022-01-08 01:07:00,288 iteration 2094 : loss : 0.030857, loss_ce: 0.011510
2022-01-08 01:07:01,809 iteration 2095 : loss : 0.052751, loss_ce: 0.021789
2022-01-08 01:07:03,151 iteration 2096 : loss : 0.029588, loss_ce: 0.011999
2022-01-08 01:07:04,499 iteration 2097 : loss : 0.054805, loss_ce: 0.014842
2022-01-08 01:07:05,807 iteration 2098 : loss : 0.027909, loss_ce: 0.010375
2022-01-08 01:07:07,157 iteration 2099 : loss : 0.026104, loss_ce: 0.010951
2022-01-08 01:07:08,554 iteration 2100 : loss : 0.032668, loss_ce: 0.012622
2022-01-08 01:07:09,831 iteration 2101 : loss : 0.033509, loss_ce: 0.014983
2022-01-08 01:07:11,231 iteration 2102 : loss : 0.040123, loss_ce: 0.019071
2022-01-08 01:07:12,557 iteration 2103 : loss : 0.027786, loss_ce: 0.013764
2022-01-08 01:07:13,846 iteration 2104 : loss : 0.031595, loss_ce: 0.010509
2022-01-08 01:07:15,239 iteration 2105 : loss : 0.035616, loss_ce: 0.017311
2022-01-08 01:07:16,619 iteration 2106 : loss : 0.032867, loss_ce: 0.013112
2022-01-08 01:07:17,934 iteration 2107 : loss : 0.029478, loss_ce: 0.009581
2022-01-08 01:07:19,234 iteration 2108 : loss : 0.026759, loss_ce: 0.010309
 31%|████████▉                    | 124/400 [52:42<1:50:53, 24.11s/it]2022-01-08 01:07:20,643 iteration 2109 : loss : 0.064595, loss_ce: 0.020268
2022-01-08 01:07:22,028 iteration 2110 : loss : 0.068268, loss_ce: 0.023103
2022-01-08 01:07:23,446 iteration 2111 : loss : 0.027388, loss_ce: 0.009712
2022-01-08 01:07:24,894 iteration 2112 : loss : 0.032431, loss_ce: 0.013758
2022-01-08 01:07:26,302 iteration 2113 : loss : 0.072504, loss_ce: 0.029702
2022-01-08 01:07:27,701 iteration 2114 : loss : 0.029271, loss_ce: 0.009440
2022-01-08 01:07:29,017 iteration 2115 : loss : 0.025078, loss_ce: 0.008345
2022-01-08 01:07:30,336 iteration 2116 : loss : 0.041677, loss_ce: 0.014927
2022-01-08 01:07:31,791 iteration 2117 : loss : 0.055415, loss_ce: 0.015105
2022-01-08 01:07:33,181 iteration 2118 : loss : 0.034242, loss_ce: 0.015896
2022-01-08 01:07:34,676 iteration 2119 : loss : 0.034320, loss_ce: 0.013576
2022-01-08 01:07:36,080 iteration 2120 : loss : 0.038839, loss_ce: 0.020816
2022-01-08 01:07:37,488 iteration 2121 : loss : 0.043270, loss_ce: 0.020078
2022-01-08 01:07:38,897 iteration 2122 : loss : 0.060728, loss_ce: 0.037584
2022-01-08 01:07:40,196 iteration 2123 : loss : 0.051541, loss_ce: 0.017417
2022-01-08 01:07:41,636 iteration 2124 : loss : 0.044328, loss_ce: 0.019642
2022-01-08 01:07:41,637 Training Data Eval:
2022-01-08 01:07:48,542   Average segmentation loss on training set: 0.0261
2022-01-08 01:07:48,543 Validation Data Eval:
2022-01-08 01:07:50,920   Average segmentation loss on validation set: 0.0867
2022-01-08 01:07:52,280 iteration 2125 : loss : 0.028702, loss_ce: 0.014456
 31%|█████████                    | 125/400 [53:15<2:02:47, 26.79s/it]2022-01-08 01:07:53,748 iteration 2126 : loss : 0.068387, loss_ce: 0.021627
2022-01-08 01:07:55,052 iteration 2127 : loss : 0.028990, loss_ce: 0.013544
2022-01-08 01:07:56,349 iteration 2128 : loss : 0.029756, loss_ce: 0.012572
2022-01-08 01:07:57,796 iteration 2129 : loss : 0.050508, loss_ce: 0.021550
2022-01-08 01:07:59,152 iteration 2130 : loss : 0.034182, loss_ce: 0.015737
2022-01-08 01:08:00,472 iteration 2131 : loss : 0.025169, loss_ce: 0.010219
2022-01-08 01:08:01,872 iteration 2132 : loss : 0.038247, loss_ce: 0.012960
2022-01-08 01:08:03,231 iteration 2133 : loss : 0.030225, loss_ce: 0.013253
2022-01-08 01:08:04,671 iteration 2134 : loss : 0.048183, loss_ce: 0.019485
2022-01-08 01:08:05,960 iteration 2135 : loss : 0.039855, loss_ce: 0.012090
2022-01-08 01:08:07,387 iteration 2136 : loss : 0.038346, loss_ce: 0.015233
2022-01-08 01:08:08,743 iteration 2137 : loss : 0.036792, loss_ce: 0.015751
2022-01-08 01:08:10,184 iteration 2138 : loss : 0.038920, loss_ce: 0.011712
2022-01-08 01:08:11,536 iteration 2139 : loss : 0.034825, loss_ce: 0.018505
2022-01-08 01:08:12,948 iteration 2140 : loss : 0.043077, loss_ce: 0.027995
2022-01-08 01:08:14,341 iteration 2141 : loss : 0.028465, loss_ce: 0.008879
2022-01-08 01:08:15,752 iteration 2142 : loss : 0.055345, loss_ce: 0.017772
 32%|█████████▏                   | 126/400 [53:39<1:57:47, 25.79s/it]2022-01-08 01:08:17,158 iteration 2143 : loss : 0.035613, loss_ce: 0.018140
2022-01-08 01:08:18,540 iteration 2144 : loss : 0.028010, loss_ce: 0.011515
2022-01-08 01:08:19,911 iteration 2145 : loss : 0.032775, loss_ce: 0.013444
2022-01-08 01:08:21,252 iteration 2146 : loss : 0.034309, loss_ce: 0.012153
2022-01-08 01:08:22,570 iteration 2147 : loss : 0.028966, loss_ce: 0.010876
2022-01-08 01:08:23,921 iteration 2148 : loss : 0.026955, loss_ce: 0.010638
2022-01-08 01:08:25,335 iteration 2149 : loss : 0.050299, loss_ce: 0.017350
2022-01-08 01:08:26,660 iteration 2150 : loss : 0.040424, loss_ce: 0.018855
2022-01-08 01:08:28,024 iteration 2151 : loss : 0.038073, loss_ce: 0.014672
2022-01-08 01:08:29,339 iteration 2152 : loss : 0.036686, loss_ce: 0.018055
2022-01-08 01:08:30,693 iteration 2153 : loss : 0.033394, loss_ce: 0.010574
2022-01-08 01:08:32,057 iteration 2154 : loss : 0.023062, loss_ce: 0.007851
2022-01-08 01:08:33,458 iteration 2155 : loss : 0.028838, loss_ce: 0.013292
2022-01-08 01:08:34,766 iteration 2156 : loss : 0.031240, loss_ce: 0.012364
2022-01-08 01:08:36,248 iteration 2157 : loss : 0.031431, loss_ce: 0.012413
2022-01-08 01:08:37,677 iteration 2158 : loss : 0.030053, loss_ce: 0.010782
2022-01-08 01:08:39,065 iteration 2159 : loss : 0.053963, loss_ce: 0.022829
 32%|█████████▏                   | 127/400 [54:02<1:53:57, 25.05s/it]2022-01-08 01:08:40,467 iteration 2160 : loss : 0.037164, loss_ce: 0.014027
2022-01-08 01:08:41,915 iteration 2161 : loss : 0.053701, loss_ce: 0.032399
2022-01-08 01:08:43,333 iteration 2162 : loss : 0.038295, loss_ce: 0.014440
2022-01-08 01:08:44,643 iteration 2163 : loss : 0.035512, loss_ce: 0.013329
2022-01-08 01:08:46,063 iteration 2164 : loss : 0.033591, loss_ce: 0.012200
2022-01-08 01:08:47,473 iteration 2165 : loss : 0.027194, loss_ce: 0.012292
2022-01-08 01:08:48,866 iteration 2166 : loss : 0.036815, loss_ce: 0.012337
2022-01-08 01:08:50,296 iteration 2167 : loss : 0.037336, loss_ce: 0.014118
2022-01-08 01:08:51,677 iteration 2168 : loss : 0.030102, loss_ce: 0.013130
2022-01-08 01:08:53,033 iteration 2169 : loss : 0.032728, loss_ce: 0.013593
2022-01-08 01:08:54,351 iteration 2170 : loss : 0.062881, loss_ce: 0.027595
2022-01-08 01:08:55,765 iteration 2171 : loss : 0.045972, loss_ce: 0.015411
2022-01-08 01:08:57,259 iteration 2172 : loss : 0.037604, loss_ce: 0.012893
2022-01-08 01:08:58,575 iteration 2173 : loss : 0.034496, loss_ce: 0.011389
2022-01-08 01:08:59,863 iteration 2174 : loss : 0.027673, loss_ce: 0.013198
2022-01-08 01:09:01,185 iteration 2175 : loss : 0.027472, loss_ce: 0.009988
2022-01-08 01:09:02,660 iteration 2176 : loss : 0.043200, loss_ce: 0.015734
 32%|█████████▎                   | 128/400 [54:25<1:51:35, 24.61s/it]2022-01-08 01:09:04,091 iteration 2177 : loss : 0.030600, loss_ce: 0.010528
2022-01-08 01:09:05,506 iteration 2178 : loss : 0.031441, loss_ce: 0.013058
2022-01-08 01:09:06,855 iteration 2179 : loss : 0.038229, loss_ce: 0.019130
2022-01-08 01:09:08,209 iteration 2180 : loss : 0.029437, loss_ce: 0.011276
2022-01-08 01:09:09,614 iteration 2181 : loss : 0.028312, loss_ce: 0.014358
2022-01-08 01:09:10,967 iteration 2182 : loss : 0.039502, loss_ce: 0.009949
2022-01-08 01:09:12,275 iteration 2183 : loss : 0.027744, loss_ce: 0.011827
2022-01-08 01:09:13,708 iteration 2184 : loss : 0.039707, loss_ce: 0.011022
2022-01-08 01:09:15,024 iteration 2185 : loss : 0.031010, loss_ce: 0.011109
2022-01-08 01:09:16,364 iteration 2186 : loss : 0.034744, loss_ce: 0.012674
2022-01-08 01:09:17,746 iteration 2187 : loss : 0.023834, loss_ce: 0.008813
2022-01-08 01:09:19,101 iteration 2188 : loss : 0.045801, loss_ce: 0.011155
2022-01-08 01:09:20,531 iteration 2189 : loss : 0.047025, loss_ce: 0.020410
2022-01-08 01:09:21,886 iteration 2190 : loss : 0.031613, loss_ce: 0.010978
2022-01-08 01:09:23,271 iteration 2191 : loss : 0.048420, loss_ce: 0.015098
2022-01-08 01:09:24,596 iteration 2192 : loss : 0.030353, loss_ce: 0.011090
2022-01-08 01:09:25,963 iteration 2193 : loss : 0.041607, loss_ce: 0.015150
 32%|█████████▎                   | 129/400 [54:49<1:49:23, 24.22s/it]2022-01-08 01:09:27,418 iteration 2194 : loss : 0.044700, loss_ce: 0.029095
2022-01-08 01:09:28,765 iteration 2195 : loss : 0.028557, loss_ce: 0.009902
2022-01-08 01:09:30,117 iteration 2196 : loss : 0.025349, loss_ce: 0.009412
2022-01-08 01:09:31,508 iteration 2197 : loss : 0.025700, loss_ce: 0.012769
2022-01-08 01:09:32,903 iteration 2198 : loss : 0.032353, loss_ce: 0.016209
2022-01-08 01:09:34,282 iteration 2199 : loss : 0.031867, loss_ce: 0.014020
2022-01-08 01:09:35,620 iteration 2200 : loss : 0.038012, loss_ce: 0.013987
2022-01-08 01:09:36,943 iteration 2201 : loss : 0.028116, loss_ce: 0.011298
2022-01-08 01:09:38,373 iteration 2202 : loss : 0.073067, loss_ce: 0.015525
2022-01-08 01:09:39,806 iteration 2203 : loss : 0.040605, loss_ce: 0.021356
2022-01-08 01:09:41,122 iteration 2204 : loss : 0.027554, loss_ce: 0.012046
2022-01-08 01:09:42,409 iteration 2205 : loss : 0.022934, loss_ce: 0.010207
2022-01-08 01:09:43,712 iteration 2206 : loss : 0.033507, loss_ce: 0.010427
2022-01-08 01:09:45,045 iteration 2207 : loss : 0.038435, loss_ce: 0.012247
2022-01-08 01:09:46,437 iteration 2208 : loss : 0.043947, loss_ce: 0.014849
2022-01-08 01:09:47,786 iteration 2209 : loss : 0.037490, loss_ce: 0.012655
2022-01-08 01:09:47,786 Training Data Eval:
2022-01-08 01:09:54,690   Average segmentation loss on training set: 0.0315
2022-01-08 01:09:54,691 Validation Data Eval:
2022-01-08 01:09:57,063   Average segmentation loss on validation set: 0.1577
2022-01-08 01:09:58,453 iteration 2210 : loss : 0.037588, loss_ce: 0.014187
 32%|█████████▍                   | 130/400 [55:21<2:00:09, 26.70s/it]2022-01-08 01:09:59,910 iteration 2211 : loss : 0.040081, loss_ce: 0.012379
2022-01-08 01:10:01,313 iteration 2212 : loss : 0.044240, loss_ce: 0.022628
2022-01-08 01:10:02,713 iteration 2213 : loss : 0.042238, loss_ce: 0.016671
2022-01-08 01:10:04,100 iteration 2214 : loss : 0.032993, loss_ce: 0.011191
2022-01-08 01:10:05,511 iteration 2215 : loss : 0.041513, loss_ce: 0.017690
2022-01-08 01:10:06,807 iteration 2216 : loss : 0.031740, loss_ce: 0.010103
2022-01-08 01:10:08,228 iteration 2217 : loss : 0.045125, loss_ce: 0.018869
2022-01-08 01:10:09,544 iteration 2218 : loss : 0.032374, loss_ce: 0.011906
2022-01-08 01:10:10,956 iteration 2219 : loss : 0.040022, loss_ce: 0.013579
2022-01-08 01:10:12,231 iteration 2220 : loss : 0.033010, loss_ce: 0.011975
2022-01-08 01:10:13,574 iteration 2221 : loss : 0.024285, loss_ce: 0.008566
2022-01-08 01:10:14,996 iteration 2222 : loss : 0.030864, loss_ce: 0.012358
2022-01-08 01:10:16,381 iteration 2223 : loss : 0.042846, loss_ce: 0.014328
2022-01-08 01:10:17,823 iteration 2224 : loss : 0.066889, loss_ce: 0.029224
2022-01-08 01:10:19,136 iteration 2225 : loss : 0.023891, loss_ce: 0.009005
2022-01-08 01:10:20,604 iteration 2226 : loss : 0.049971, loss_ce: 0.028703
2022-01-08 01:10:21,961 iteration 2227 : loss : 0.027187, loss_ce: 0.010942
 33%|█████████▍                   | 131/400 [55:45<1:55:25, 25.74s/it]2022-01-08 01:10:23,419 iteration 2228 : loss : 0.031474, loss_ce: 0.019049
2022-01-08 01:10:24,786 iteration 2229 : loss : 0.030910, loss_ce: 0.012992
2022-01-08 01:10:26,194 iteration 2230 : loss : 0.040450, loss_ce: 0.013339
2022-01-08 01:10:27,475 iteration 2231 : loss : 0.018556, loss_ce: 0.007401
2022-01-08 01:10:28,815 iteration 2232 : loss : 0.054332, loss_ce: 0.022285
2022-01-08 01:10:30,164 iteration 2233 : loss : 0.027127, loss_ce: 0.010439
2022-01-08 01:10:31,518 iteration 2234 : loss : 0.036554, loss_ce: 0.011934
2022-01-08 01:10:32,862 iteration 2235 : loss : 0.060848, loss_ce: 0.011554
2022-01-08 01:10:34,358 iteration 2236 : loss : 0.061006, loss_ce: 0.026263
2022-01-08 01:10:35,731 iteration 2237 : loss : 0.036804, loss_ce: 0.016445
2022-01-08 01:10:37,113 iteration 2238 : loss : 0.036807, loss_ce: 0.020371
2022-01-08 01:10:38,482 iteration 2239 : loss : 0.030496, loss_ce: 0.011501
2022-01-08 01:10:39,885 iteration 2240 : loss : 0.037994, loss_ce: 0.014240
2022-01-08 01:10:41,331 iteration 2241 : loss : 0.040360, loss_ce: 0.011059
2022-01-08 01:10:42,761 iteration 2242 : loss : 0.028065, loss_ce: 0.009492
2022-01-08 01:10:44,222 iteration 2243 : loss : 0.033354, loss_ce: 0.010944
2022-01-08 01:10:45,587 iteration 2244 : loss : 0.031057, loss_ce: 0.014712
 33%|█████████▌                   | 132/400 [56:08<1:52:08, 25.11s/it]2022-01-08 01:10:47,096 iteration 2245 : loss : 0.043443, loss_ce: 0.016018
2022-01-08 01:10:48,432 iteration 2246 : loss : 0.027599, loss_ce: 0.012908
2022-01-08 01:10:49,892 iteration 2247 : loss : 0.061816, loss_ce: 0.016390
2022-01-08 01:10:51,283 iteration 2248 : loss : 0.037517, loss_ce: 0.016212
2022-01-08 01:10:52,654 iteration 2249 : loss : 0.026318, loss_ce: 0.008568
2022-01-08 01:10:54,020 iteration 2250 : loss : 0.046531, loss_ce: 0.021156
2022-01-08 01:10:55,326 iteration 2251 : loss : 0.025770, loss_ce: 0.008899
2022-01-08 01:10:56,809 iteration 2252 : loss : 0.053365, loss_ce: 0.020265
2022-01-08 01:10:58,246 iteration 2253 : loss : 0.046240, loss_ce: 0.020313
2022-01-08 01:10:59,584 iteration 2254 : loss : 0.032921, loss_ce: 0.013716
2022-01-08 01:11:00,915 iteration 2255 : loss : 0.031415, loss_ce: 0.011789
2022-01-08 01:11:02,324 iteration 2256 : loss : 0.027901, loss_ce: 0.011808
2022-01-08 01:11:03,681 iteration 2257 : loss : 0.025973, loss_ce: 0.010663
2022-01-08 01:11:04,972 iteration 2258 : loss : 0.027457, loss_ce: 0.011527
2022-01-08 01:11:06,413 iteration 2259 : loss : 0.020457, loss_ce: 0.007425
2022-01-08 01:11:07,752 iteration 2260 : loss : 0.033217, loss_ce: 0.013021
2022-01-08 01:11:09,146 iteration 2261 : loss : 0.028432, loss_ce: 0.011142
 33%|█████████▋                   | 133/400 [56:32<1:49:39, 24.64s/it]2022-01-08 01:11:10,576 iteration 2262 : loss : 0.029657, loss_ce: 0.013926
2022-01-08 01:11:11,950 iteration 2263 : loss : 0.029036, loss_ce: 0.012588
2022-01-08 01:11:13,372 iteration 2264 : loss : 0.059005, loss_ce: 0.026593
2022-01-08 01:11:14,741 iteration 2265 : loss : 0.025919, loss_ce: 0.009721
2022-01-08 01:11:16,099 iteration 2266 : loss : 0.025712, loss_ce: 0.007721
2022-01-08 01:11:17,392 iteration 2267 : loss : 0.020751, loss_ce: 0.008896
2022-01-08 01:11:18,749 iteration 2268 : loss : 0.025063, loss_ce: 0.012228
2022-01-08 01:11:20,077 iteration 2269 : loss : 0.031904, loss_ce: 0.014174
2022-01-08 01:11:21,379 iteration 2270 : loss : 0.023147, loss_ce: 0.008454
2022-01-08 01:11:22,744 iteration 2271 : loss : 0.043451, loss_ce: 0.018096
2022-01-08 01:11:24,185 iteration 2272 : loss : 0.032823, loss_ce: 0.013378
2022-01-08 01:11:25,618 iteration 2273 : loss : 0.038143, loss_ce: 0.015388
2022-01-08 01:11:26,987 iteration 2274 : loss : 0.030557, loss_ce: 0.012821
2022-01-08 01:11:28,344 iteration 2275 : loss : 0.054357, loss_ce: 0.018775
2022-01-08 01:11:29,656 iteration 2276 : loss : 0.035883, loss_ce: 0.014411
2022-01-08 01:11:31,017 iteration 2277 : loss : 0.021910, loss_ce: 0.006476
2022-01-08 01:11:32,413 iteration 2278 : loss : 0.029811, loss_ce: 0.010201
 34%|█████████▋                   | 134/400 [56:55<1:47:25, 24.23s/it]2022-01-08 01:11:33,756 iteration 2279 : loss : 0.022548, loss_ce: 0.007563
2022-01-08 01:11:35,063 iteration 2280 : loss : 0.031218, loss_ce: 0.014886
2022-01-08 01:11:36,431 iteration 2281 : loss : 0.038418, loss_ce: 0.012111
2022-01-08 01:11:37,730 iteration 2282 : loss : 0.037846, loss_ce: 0.014494
2022-01-08 01:11:39,102 iteration 2283 : loss : 0.048636, loss_ce: 0.015404
2022-01-08 01:11:40,400 iteration 2284 : loss : 0.032506, loss_ce: 0.015523
2022-01-08 01:11:41,805 iteration 2285 : loss : 0.031444, loss_ce: 0.013639
2022-01-08 01:11:43,164 iteration 2286 : loss : 0.030939, loss_ce: 0.012566
2022-01-08 01:11:44,538 iteration 2287 : loss : 0.024278, loss_ce: 0.007275
2022-01-08 01:11:45,897 iteration 2288 : loss : 0.042341, loss_ce: 0.012124
2022-01-08 01:11:47,279 iteration 2289 : loss : 0.032419, loss_ce: 0.010727
2022-01-08 01:11:48,659 iteration 2290 : loss : 0.045922, loss_ce: 0.019622
2022-01-08 01:11:50,127 iteration 2291 : loss : 0.039303, loss_ce: 0.018064
2022-01-08 01:11:51,530 iteration 2292 : loss : 0.037400, loss_ce: 0.014320
2022-01-08 01:11:52,910 iteration 2293 : loss : 0.050711, loss_ce: 0.020451
2022-01-08 01:11:54,258 iteration 2294 : loss : 0.028217, loss_ce: 0.014058
2022-01-08 01:11:54,258 Training Data Eval:
2022-01-08 01:12:01,172   Average segmentation loss on training set: 0.0263
2022-01-08 01:12:01,173 Validation Data Eval:
2022-01-08 01:12:03,552   Average segmentation loss on validation set: 0.0820
2022-01-08 01:12:04,935 iteration 2295 : loss : 0.038681, loss_ce: 0.012470
 34%|█████████▊                   | 135/400 [57:28<1:58:00, 26.72s/it]2022-01-08 01:12:06,302 iteration 2296 : loss : 0.028971, loss_ce: 0.010923
2022-01-08 01:12:07,724 iteration 2297 : loss : 0.028674, loss_ce: 0.010335
2022-01-08 01:12:09,108 iteration 2298 : loss : 0.029029, loss_ce: 0.011146
2022-01-08 01:12:10,518 iteration 2299 : loss : 0.051585, loss_ce: 0.016915
2022-01-08 01:12:11,856 iteration 2300 : loss : 0.023855, loss_ce: 0.010027
2022-01-08 01:12:13,231 iteration 2301 : loss : 0.037349, loss_ce: 0.013913
2022-01-08 01:12:14,639 iteration 2302 : loss : 0.028998, loss_ce: 0.011654
2022-01-08 01:12:16,035 iteration 2303 : loss : 0.036986, loss_ce: 0.011277
2022-01-08 01:12:17,436 iteration 2304 : loss : 0.039486, loss_ce: 0.014195
2022-01-08 01:12:18,784 iteration 2305 : loss : 0.034404, loss_ce: 0.016167
2022-01-08 01:12:20,127 iteration 2306 : loss : 0.033640, loss_ce: 0.013459
2022-01-08 01:12:21,580 iteration 2307 : loss : 0.032907, loss_ce: 0.011710
2022-01-08 01:12:22,916 iteration 2308 : loss : 0.027878, loss_ce: 0.012110
2022-01-08 01:12:24,323 iteration 2309 : loss : 0.026703, loss_ce: 0.010717
2022-01-08 01:12:25,733 iteration 2310 : loss : 0.035319, loss_ce: 0.015567
2022-01-08 01:12:27,069 iteration 2311 : loss : 0.024239, loss_ce: 0.009839
2022-01-08 01:12:28,382 iteration 2312 : loss : 0.021258, loss_ce: 0.007865
 34%|█████████▊                   | 136/400 [57:51<1:53:14, 25.74s/it]2022-01-08 01:12:29,785 iteration 2313 : loss : 0.041622, loss_ce: 0.015421
2022-01-08 01:12:31,160 iteration 2314 : loss : 0.029663, loss_ce: 0.009807
2022-01-08 01:12:32,615 iteration 2315 : loss : 0.049876, loss_ce: 0.013344
2022-01-08 01:12:33,908 iteration 2316 : loss : 0.036745, loss_ce: 0.014680
2022-01-08 01:12:35,339 iteration 2317 : loss : 0.060523, loss_ce: 0.018082
2022-01-08 01:12:36,728 iteration 2318 : loss : 0.024828, loss_ce: 0.010324
2022-01-08 01:12:38,108 iteration 2319 : loss : 0.034997, loss_ce: 0.016064
2022-01-08 01:12:39,532 iteration 2320 : loss : 0.032053, loss_ce: 0.014454
2022-01-08 01:12:40,843 iteration 2321 : loss : 0.029618, loss_ce: 0.009168
2022-01-08 01:12:42,207 iteration 2322 : loss : 0.049363, loss_ce: 0.026227
2022-01-08 01:12:43,565 iteration 2323 : loss : 0.060423, loss_ce: 0.027552
2022-01-08 01:12:44,955 iteration 2324 : loss : 0.027847, loss_ce: 0.008661
2022-01-08 01:12:46,302 iteration 2325 : loss : 0.027980, loss_ce: 0.011747
2022-01-08 01:12:47,692 iteration 2326 : loss : 0.041311, loss_ce: 0.019883
2022-01-08 01:12:49,048 iteration 2327 : loss : 0.029811, loss_ce: 0.012954
2022-01-08 01:12:50,463 iteration 2328 : loss : 0.036284, loss_ce: 0.018372
2022-01-08 01:12:51,794 iteration 2329 : loss : 0.022631, loss_ce: 0.009323
 34%|█████████▉                   | 137/400 [58:15<1:49:45, 25.04s/it]2022-01-08 01:12:53,178 iteration 2330 : loss : 0.028211, loss_ce: 0.010863
2022-01-08 01:12:54,543 iteration 2331 : loss : 0.035524, loss_ce: 0.015456
2022-01-08 01:12:55,963 iteration 2332 : loss : 0.045507, loss_ce: 0.022233
2022-01-08 01:12:57,390 iteration 2333 : loss : 0.025074, loss_ce: 0.012080
2022-01-08 01:12:58,739 iteration 2334 : loss : 0.022757, loss_ce: 0.010726
2022-01-08 01:13:00,098 iteration 2335 : loss : 0.040643, loss_ce: 0.013995
2022-01-08 01:13:01,511 iteration 2336 : loss : 0.039902, loss_ce: 0.010030
2022-01-08 01:13:02,930 iteration 2337 : loss : 0.026484, loss_ce: 0.011785
2022-01-08 01:13:04,276 iteration 2338 : loss : 0.024198, loss_ce: 0.009067
2022-01-08 01:13:05,669 iteration 2339 : loss : 0.035121, loss_ce: 0.011798
2022-01-08 01:13:07,114 iteration 2340 : loss : 0.038226, loss_ce: 0.017818
2022-01-08 01:13:08,418 iteration 2341 : loss : 0.022325, loss_ce: 0.006181
2022-01-08 01:13:09,792 iteration 2342 : loss : 0.029815, loss_ce: 0.013441
2022-01-08 01:13:11,229 iteration 2343 : loss : 0.045017, loss_ce: 0.015956
2022-01-08 01:13:12,633 iteration 2344 : loss : 0.029599, loss_ce: 0.011824
2022-01-08 01:13:14,055 iteration 2345 : loss : 0.032056, loss_ce: 0.009868
2022-01-08 01:13:15,431 iteration 2346 : loss : 0.027583, loss_ce: 0.011678
 34%|██████████                   | 138/400 [58:38<1:47:29, 24.62s/it]2022-01-08 01:13:16,840 iteration 2347 : loss : 0.029016, loss_ce: 0.009859
2022-01-08 01:13:18,187 iteration 2348 : loss : 0.034878, loss_ce: 0.011638
2022-01-08 01:13:19,618 iteration 2349 : loss : 0.031530, loss_ce: 0.015336
2022-01-08 01:13:20,975 iteration 2350 : loss : 0.042163, loss_ce: 0.013408
2022-01-08 01:13:22,361 iteration 2351 : loss : 0.059619, loss_ce: 0.009201
2022-01-08 01:13:23,731 iteration 2352 : loss : 0.030886, loss_ce: 0.011161
2022-01-08 01:13:25,156 iteration 2353 : loss : 0.035217, loss_ce: 0.015035
2022-01-08 01:13:26,585 iteration 2354 : loss : 0.039841, loss_ce: 0.018000
2022-01-08 01:13:27,892 iteration 2355 : loss : 0.030609, loss_ce: 0.011914
2022-01-08 01:13:29,267 iteration 2356 : loss : 0.032152, loss_ce: 0.012057
2022-01-08 01:13:30,631 iteration 2357 : loss : 0.052658, loss_ce: 0.018841
2022-01-08 01:13:32,029 iteration 2358 : loss : 0.032323, loss_ce: 0.013752
2022-01-08 01:13:33,522 iteration 2359 : loss : 0.027311, loss_ce: 0.011254
2022-01-08 01:13:34,869 iteration 2360 : loss : 0.037458, loss_ce: 0.016182
2022-01-08 01:13:36,271 iteration 2361 : loss : 0.043013, loss_ce: 0.017129
2022-01-08 01:13:37,739 iteration 2362 : loss : 0.048446, loss_ce: 0.019884
2022-01-08 01:13:39,049 iteration 2363 : loss : 0.027951, loss_ce: 0.012325
 35%|██████████                   | 139/400 [59:02<1:45:47, 24.32s/it]2022-01-08 01:13:40,386 iteration 2364 : loss : 0.061089, loss_ce: 0.012856
2022-01-08 01:13:41,722 iteration 2365 : loss : 0.049481, loss_ce: 0.013757
2022-01-08 01:13:43,117 iteration 2366 : loss : 0.030538, loss_ce: 0.011788
2022-01-08 01:13:44,460 iteration 2367 : loss : 0.053617, loss_ce: 0.029186
2022-01-08 01:13:45,799 iteration 2368 : loss : 0.044395, loss_ce: 0.022173
2022-01-08 01:13:47,183 iteration 2369 : loss : 0.044267, loss_ce: 0.019179
2022-01-08 01:13:48,490 iteration 2370 : loss : 0.050887, loss_ce: 0.019145
2022-01-08 01:13:49,929 iteration 2371 : loss : 0.037194, loss_ce: 0.012573
2022-01-08 01:13:51,295 iteration 2372 : loss : 0.035462, loss_ce: 0.013849
2022-01-08 01:13:52,621 iteration 2373 : loss : 0.029206, loss_ce: 0.011214
2022-01-08 01:13:53,957 iteration 2374 : loss : 0.032862, loss_ce: 0.015411
2022-01-08 01:13:55,347 iteration 2375 : loss : 0.050351, loss_ce: 0.020856
2022-01-08 01:13:56,759 iteration 2376 : loss : 0.032844, loss_ce: 0.013847
2022-01-08 01:13:58,110 iteration 2377 : loss : 0.021508, loss_ce: 0.007785
2022-01-08 01:13:59,464 iteration 2378 : loss : 0.024298, loss_ce: 0.009417
2022-01-08 01:14:00,836 iteration 2379 : loss : 0.044063, loss_ce: 0.027068
2022-01-08 01:14:00,836 Training Data Eval:
2022-01-08 01:14:07,733   Average segmentation loss on training set: 0.0371
2022-01-08 01:14:07,734 Validation Data Eval:
2022-01-08 01:14:10,105   Average segmentation loss on validation set: 0.1054
2022-01-08 01:14:11,439 iteration 2380 : loss : 0.025526, loss_ce: 0.014089
 35%|██████████▏                  | 140/400 [59:34<1:55:52, 26.74s/it]2022-01-08 01:14:12,956 iteration 2381 : loss : 0.069764, loss_ce: 0.018956
2022-01-08 01:14:14,345 iteration 2382 : loss : 0.033966, loss_ce: 0.012151
2022-01-08 01:14:15,700 iteration 2383 : loss : 0.035471, loss_ce: 0.013909
2022-01-08 01:14:17,014 iteration 2384 : loss : 0.024778, loss_ce: 0.008765
2022-01-08 01:14:18,387 iteration 2385 : loss : 0.030621, loss_ce: 0.010405
2022-01-08 01:14:19,801 iteration 2386 : loss : 0.041570, loss_ce: 0.011257
2022-01-08 01:14:21,216 iteration 2387 : loss : 0.042835, loss_ce: 0.014576
2022-01-08 01:14:22,593 iteration 2388 : loss : 0.035183, loss_ce: 0.016326
2022-01-08 01:14:23,971 iteration 2389 : loss : 0.027408, loss_ce: 0.009776
2022-01-08 01:14:25,303 iteration 2390 : loss : 0.037256, loss_ce: 0.018716
2022-01-08 01:14:26,600 iteration 2391 : loss : 0.023027, loss_ce: 0.009073
2022-01-08 01:14:28,106 iteration 2392 : loss : 0.038812, loss_ce: 0.010337
2022-01-08 01:14:29,560 iteration 2393 : loss : 0.032124, loss_ce: 0.013226
2022-01-08 01:14:30,940 iteration 2394 : loss : 0.037862, loss_ce: 0.015595
2022-01-08 01:14:32,361 iteration 2395 : loss : 0.035267, loss_ce: 0.017107
2022-01-08 01:14:33,679 iteration 2396 : loss : 0.032611, loss_ce: 0.013439
2022-01-08 01:14:35,112 iteration 2397 : loss : 0.025655, loss_ce: 0.013346
 35%|██████████▏                  | 141/400 [59:58<1:51:27, 25.82s/it]2022-01-08 01:14:36,491 iteration 2398 : loss : 0.027432, loss_ce: 0.013365
2022-01-08 01:14:37,942 iteration 2399 : loss : 0.030780, loss_ce: 0.013394
2022-01-08 01:14:39,283 iteration 2400 : loss : 0.046239, loss_ce: 0.016963
2022-01-08 01:14:40,771 iteration 2401 : loss : 0.055471, loss_ce: 0.019881
2022-01-08 01:14:42,131 iteration 2402 : loss : 0.035823, loss_ce: 0.015906
2022-01-08 01:14:43,473 iteration 2403 : loss : 0.029068, loss_ce: 0.011140
2022-01-08 01:14:44,810 iteration 2404 : loss : 0.024420, loss_ce: 0.008451
2022-01-08 01:14:46,231 iteration 2405 : loss : 0.042778, loss_ce: 0.014748
2022-01-08 01:14:47,531 iteration 2406 : loss : 0.032847, loss_ce: 0.014094
2022-01-08 01:14:48,884 iteration 2407 : loss : 0.024680, loss_ce: 0.008492
2022-01-08 01:14:50,426 iteration 2408 : loss : 0.044293, loss_ce: 0.014607
2022-01-08 01:14:51,757 iteration 2409 : loss : 0.027483, loss_ce: 0.011551
2022-01-08 01:14:53,093 iteration 2410 : loss : 0.034294, loss_ce: 0.012043
2022-01-08 01:14:54,423 iteration 2411 : loss : 0.024942, loss_ce: 0.009619
2022-01-08 01:14:55,760 iteration 2412 : loss : 0.023413, loss_ce: 0.008777
2022-01-08 01:14:57,094 iteration 2413 : loss : 0.031646, loss_ce: 0.012042
2022-01-08 01:14:58,411 iteration 2414 : loss : 0.029970, loss_ce: 0.012927
 36%|█████████▌                 | 142/400 [1:00:21<1:47:46, 25.06s/it]2022-01-08 01:14:59,863 iteration 2415 : loss : 0.026708, loss_ce: 0.008757
2022-01-08 01:15:01,207 iteration 2416 : loss : 0.028695, loss_ce: 0.011444
2022-01-08 01:15:02,577 iteration 2417 : loss : 0.028244, loss_ce: 0.013879
2022-01-08 01:15:03,884 iteration 2418 : loss : 0.028196, loss_ce: 0.008924
2022-01-08 01:15:05,248 iteration 2419 : loss : 0.036058, loss_ce: 0.013140
2022-01-08 01:15:06,675 iteration 2420 : loss : 0.033857, loss_ce: 0.017904
2022-01-08 01:15:08,031 iteration 2421 : loss : 0.026339, loss_ce: 0.011196
2022-01-08 01:15:09,397 iteration 2422 : loss : 0.041165, loss_ce: 0.016353
2022-01-08 01:15:10,690 iteration 2423 : loss : 0.036579, loss_ce: 0.015093
2022-01-08 01:15:12,064 iteration 2424 : loss : 0.026035, loss_ce: 0.010365
2022-01-08 01:15:13,460 iteration 2425 : loss : 0.023547, loss_ce: 0.008053
2022-01-08 01:15:14,823 iteration 2426 : loss : 0.039617, loss_ce: 0.018329
2022-01-08 01:15:16,133 iteration 2427 : loss : 0.029625, loss_ce: 0.014376
2022-01-08 01:15:17,476 iteration 2428 : loss : 0.029531, loss_ce: 0.011113
2022-01-08 01:15:18,796 iteration 2429 : loss : 0.030400, loss_ce: 0.009590
2022-01-08 01:15:20,157 iteration 2430 : loss : 0.032346, loss_ce: 0.013021
2022-01-08 01:15:21,541 iteration 2431 : loss : 0.028526, loss_ce: 0.008671
 36%|█████████▋                 | 143/400 [1:00:44<1:44:52, 24.48s/it]2022-01-08 01:15:22,881 iteration 2432 : loss : 0.037414, loss_ce: 0.015926
2022-01-08 01:15:24,205 iteration 2433 : loss : 0.024018, loss_ce: 0.011008
2022-01-08 01:15:25,591 iteration 2434 : loss : 0.032007, loss_ce: 0.012387
2022-01-08 01:15:26,953 iteration 2435 : loss : 0.025192, loss_ce: 0.013686
2022-01-08 01:15:28,388 iteration 2436 : loss : 0.024293, loss_ce: 0.009478
2022-01-08 01:15:29,758 iteration 2437 : loss : 0.026210, loss_ce: 0.012139
2022-01-08 01:15:31,069 iteration 2438 : loss : 0.025810, loss_ce: 0.009974
2022-01-08 01:15:32,443 iteration 2439 : loss : 0.027087, loss_ce: 0.010784
2022-01-08 01:15:33,818 iteration 2440 : loss : 0.035115, loss_ce: 0.013320
2022-01-08 01:15:35,281 iteration 2441 : loss : 0.039273, loss_ce: 0.012810
2022-01-08 01:15:36,667 iteration 2442 : loss : 0.024295, loss_ce: 0.008483
2022-01-08 01:15:38,054 iteration 2443 : loss : 0.032093, loss_ce: 0.015045
2022-01-08 01:15:39,447 iteration 2444 : loss : 0.032049, loss_ce: 0.012450
2022-01-08 01:15:40,912 iteration 2445 : loss : 0.022885, loss_ce: 0.008860
2022-01-08 01:15:42,255 iteration 2446 : loss : 0.024773, loss_ce: 0.010160
2022-01-08 01:15:43,678 iteration 2447 : loss : 0.032387, loss_ce: 0.011622
2022-01-08 01:15:44,992 iteration 2448 : loss : 0.033049, loss_ce: 0.018454
 36%|█████████▋                 | 144/400 [1:01:08<1:43:08, 24.17s/it]2022-01-08 01:15:46,394 iteration 2449 : loss : 0.019572, loss_ce: 0.006445
2022-01-08 01:15:47,705 iteration 2450 : loss : 0.032641, loss_ce: 0.012166
2022-01-08 01:15:49,023 iteration 2451 : loss : 0.021296, loss_ce: 0.007913
2022-01-08 01:15:50,360 iteration 2452 : loss : 0.030956, loss_ce: 0.013147
2022-01-08 01:15:51,707 iteration 2453 : loss : 0.025834, loss_ce: 0.012620
2022-01-08 01:15:53,058 iteration 2454 : loss : 0.046640, loss_ce: 0.015466
2022-01-08 01:15:54,524 iteration 2455 : loss : 0.029792, loss_ce: 0.011467
2022-01-08 01:15:55,892 iteration 2456 : loss : 0.024654, loss_ce: 0.011170
2022-01-08 01:15:57,224 iteration 2457 : loss : 0.037885, loss_ce: 0.017746
2022-01-08 01:15:58,615 iteration 2458 : loss : 0.040495, loss_ce: 0.012569
2022-01-08 01:15:59,968 iteration 2459 : loss : 0.023758, loss_ce: 0.008360
2022-01-08 01:16:01,387 iteration 2460 : loss : 0.053722, loss_ce: 0.013791
2022-01-08 01:16:02,777 iteration 2461 : loss : 0.039697, loss_ce: 0.010122
2022-01-08 01:16:04,240 iteration 2462 : loss : 0.037688, loss_ce: 0.017690
2022-01-08 01:16:05,571 iteration 2463 : loss : 0.022336, loss_ce: 0.008008
2022-01-08 01:16:06,944 iteration 2464 : loss : 0.031725, loss_ce: 0.018970
2022-01-08 01:16:06,944 Training Data Eval:
2022-01-08 01:16:13,822   Average segmentation loss on training set: 0.0235
2022-01-08 01:16:13,823 Validation Data Eval:
2022-01-08 01:16:16,197   Average segmentation loss on validation set: 0.1017
2022-01-08 01:16:17,587 iteration 2465 : loss : 0.036293, loss_ce: 0.019626
 36%|█████████▊                 | 145/400 [1:01:40<1:53:28, 26.70s/it]2022-01-08 01:16:19,021 iteration 2466 : loss : 0.028958, loss_ce: 0.010359
2022-01-08 01:16:20,365 iteration 2467 : loss : 0.019754, loss_ce: 0.007438
2022-01-08 01:16:21,802 iteration 2468 : loss : 0.044905, loss_ce: 0.018090
2022-01-08 01:16:23,106 iteration 2469 : loss : 0.024961, loss_ce: 0.011064
2022-01-08 01:16:24,496 iteration 2470 : loss : 0.028616, loss_ce: 0.011183
2022-01-08 01:16:25,878 iteration 2471 : loss : 0.033800, loss_ce: 0.015968
2022-01-08 01:16:27,265 iteration 2472 : loss : 0.024326, loss_ce: 0.008378
2022-01-08 01:16:28,623 iteration 2473 : loss : 0.021993, loss_ce: 0.008714
2022-01-08 01:16:29,899 iteration 2474 : loss : 0.023548, loss_ce: 0.010720
2022-01-08 01:16:31,297 iteration 2475 : loss : 0.037835, loss_ce: 0.023788
2022-01-08 01:16:32,678 iteration 2476 : loss : 0.024529, loss_ce: 0.007938
2022-01-08 01:16:34,058 iteration 2477 : loss : 0.025616, loss_ce: 0.009585
2022-01-08 01:16:35,413 iteration 2478 : loss : 0.025652, loss_ce: 0.008376
2022-01-08 01:16:36,823 iteration 2479 : loss : 0.030996, loss_ce: 0.009123
2022-01-08 01:16:38,164 iteration 2480 : loss : 0.021326, loss_ce: 0.007692
2022-01-08 01:16:39,491 iteration 2481 : loss : 0.022854, loss_ce: 0.009505
2022-01-08 01:16:40,949 iteration 2482 : loss : 0.046031, loss_ce: 0.018849
 36%|█████████▊                 | 146/400 [1:02:04<1:48:47, 25.70s/it]2022-01-08 01:16:42,332 iteration 2483 : loss : 0.036778, loss_ce: 0.016047
2022-01-08 01:16:43,720 iteration 2484 : loss : 0.035678, loss_ce: 0.007775
2022-01-08 01:16:45,103 iteration 2485 : loss : 0.042894, loss_ce: 0.014695
2022-01-08 01:16:46,471 iteration 2486 : loss : 0.034528, loss_ce: 0.014427
2022-01-08 01:16:47,783 iteration 2487 : loss : 0.019743, loss_ce: 0.008718
2022-01-08 01:16:49,204 iteration 2488 : loss : 0.023077, loss_ce: 0.011392
2022-01-08 01:16:50,608 iteration 2489 : loss : 0.032562, loss_ce: 0.014092
2022-01-08 01:16:52,027 iteration 2490 : loss : 0.035838, loss_ce: 0.019642
2022-01-08 01:16:53,405 iteration 2491 : loss : 0.037455, loss_ce: 0.010657
2022-01-08 01:16:54,802 iteration 2492 : loss : 0.058692, loss_ce: 0.022080
2022-01-08 01:16:56,117 iteration 2493 : loss : 0.023231, loss_ce: 0.008476
2022-01-08 01:16:57,474 iteration 2494 : loss : 0.025289, loss_ce: 0.009339
2022-01-08 01:16:58,911 iteration 2495 : loss : 0.022323, loss_ce: 0.009035
2022-01-08 01:17:00,174 iteration 2496 : loss : 0.018379, loss_ce: 0.006989
2022-01-08 01:17:01,506 iteration 2497 : loss : 0.027431, loss_ce: 0.009767
2022-01-08 01:17:02,991 iteration 2498 : loss : 0.039397, loss_ce: 0.017384
2022-01-08 01:17:04,346 iteration 2499 : loss : 0.023566, loss_ce: 0.010377
 37%|█████████▉                 | 147/400 [1:02:27<1:45:27, 25.01s/it]2022-01-08 01:17:05,758 iteration 2500 : loss : 0.028561, loss_ce: 0.010215
2022-01-08 01:17:07,158 iteration 2501 : loss : 0.027779, loss_ce: 0.010327
2022-01-08 01:17:08,558 iteration 2502 : loss : 0.024185, loss_ce: 0.012922
2022-01-08 01:17:09,907 iteration 2503 : loss : 0.025989, loss_ce: 0.011669
2022-01-08 01:17:11,300 iteration 2504 : loss : 0.030055, loss_ce: 0.010419
2022-01-08 01:17:12,679 iteration 2505 : loss : 0.025258, loss_ce: 0.008664
2022-01-08 01:17:14,065 iteration 2506 : loss : 0.026722, loss_ce: 0.011622
2022-01-08 01:17:15,389 iteration 2507 : loss : 0.019490, loss_ce: 0.007333
2022-01-08 01:17:16,685 iteration 2508 : loss : 0.021913, loss_ce: 0.009854
2022-01-08 01:17:18,030 iteration 2509 : loss : 0.027541, loss_ce: 0.013439
2022-01-08 01:17:19,336 iteration 2510 : loss : 0.042387, loss_ce: 0.011410
2022-01-08 01:17:20,691 iteration 2511 : loss : 0.024888, loss_ce: 0.011255
2022-01-08 01:17:22,037 iteration 2512 : loss : 0.024445, loss_ce: 0.009663
2022-01-08 01:17:23,379 iteration 2513 : loss : 0.035926, loss_ce: 0.014965
2022-01-08 01:17:24,751 iteration 2514 : loss : 0.030967, loss_ce: 0.011597
2022-01-08 01:17:26,093 iteration 2515 : loss : 0.032500, loss_ce: 0.009756
2022-01-08 01:17:27,415 iteration 2516 : loss : 0.029433, loss_ce: 0.011923
 37%|█████████▉                 | 148/400 [1:02:50<1:42:35, 24.43s/it]2022-01-08 01:17:28,801 iteration 2517 : loss : 0.027036, loss_ce: 0.009806
2022-01-08 01:17:30,250 iteration 2518 : loss : 0.029146, loss_ce: 0.011763
2022-01-08 01:17:31,597 iteration 2519 : loss : 0.029447, loss_ce: 0.012813
2022-01-08 01:17:32,940 iteration 2520 : loss : 0.026537, loss_ce: 0.008823
2022-01-08 01:17:34,348 iteration 2521 : loss : 0.023218, loss_ce: 0.010796
2022-01-08 01:17:35,644 iteration 2522 : loss : 0.020727, loss_ce: 0.008955
2022-01-08 01:17:36,898 iteration 2523 : loss : 0.030404, loss_ce: 0.008443
2022-01-08 01:17:38,305 iteration 2524 : loss : 0.024702, loss_ce: 0.011300
2022-01-08 01:17:39,631 iteration 2525 : loss : 0.028664, loss_ce: 0.011801
2022-01-08 01:17:40,959 iteration 2526 : loss : 0.026112, loss_ce: 0.008893
2022-01-08 01:17:42,364 iteration 2527 : loss : 0.027150, loss_ce: 0.012001
2022-01-08 01:17:43,684 iteration 2528 : loss : 0.035854, loss_ce: 0.013398
2022-01-08 01:17:45,058 iteration 2529 : loss : 0.034955, loss_ce: 0.013368
2022-01-08 01:17:46,418 iteration 2530 : loss : 0.032369, loss_ce: 0.010038
2022-01-08 01:17:47,754 iteration 2531 : loss : 0.029752, loss_ce: 0.010606
2022-01-08 01:17:49,140 iteration 2532 : loss : 0.043019, loss_ce: 0.013083
2022-01-08 01:17:50,505 iteration 2533 : loss : 0.037905, loss_ce: 0.022995
 37%|██████████                 | 149/400 [1:03:13<1:40:30, 24.03s/it]2022-01-08 01:17:51,842 iteration 2534 : loss : 0.020591, loss_ce: 0.009407
2022-01-08 01:17:53,229 iteration 2535 : loss : 0.028762, loss_ce: 0.008027
2022-01-08 01:17:54,635 iteration 2536 : loss : 0.025584, loss_ce: 0.010675
2022-01-08 01:17:56,046 iteration 2537 : loss : 0.041618, loss_ce: 0.011841
2022-01-08 01:17:57,480 iteration 2538 : loss : 0.035780, loss_ce: 0.013167
2022-01-08 01:17:58,791 iteration 2539 : loss : 0.024776, loss_ce: 0.011202
2022-01-08 01:18:00,149 iteration 2540 : loss : 0.038308, loss_ce: 0.013438
2022-01-08 01:18:01,626 iteration 2541 : loss : 0.042705, loss_ce: 0.013463
2022-01-08 01:18:02,920 iteration 2542 : loss : 0.023923, loss_ce: 0.008771
2022-01-08 01:18:04,280 iteration 2543 : loss : 0.031634, loss_ce: 0.010150
2022-01-08 01:18:05,703 iteration 2544 : loss : 0.043406, loss_ce: 0.016135
2022-01-08 01:18:07,116 iteration 2545 : loss : 0.026356, loss_ce: 0.012390
2022-01-08 01:18:08,423 iteration 2546 : loss : 0.022088, loss_ce: 0.009084
2022-01-08 01:18:09,790 iteration 2547 : loss : 0.050253, loss_ce: 0.012902
2022-01-08 01:18:11,133 iteration 2548 : loss : 0.033019, loss_ce: 0.012397
2022-01-08 01:18:12,541 iteration 2549 : loss : 0.047236, loss_ce: 0.021568
2022-01-08 01:18:12,541 Training Data Eval:
2022-01-08 01:18:19,449   Average segmentation loss on training set: 0.0237
2022-01-08 01:18:19,449 Validation Data Eval:
2022-01-08 01:18:21,817   Average segmentation loss on validation set: 0.1543
2022-01-08 01:18:23,167 iteration 2550 : loss : 0.030525, loss_ce: 0.017368
 38%|██████████▏                | 150/400 [1:03:46<1:50:53, 26.61s/it]2022-01-08 01:18:24,537 iteration 2551 : loss : 0.021981, loss_ce: 0.007561
2022-01-08 01:18:25,908 iteration 2552 : loss : 0.023001, loss_ce: 0.008928
2022-01-08 01:18:27,335 iteration 2553 : loss : 0.029562, loss_ce: 0.012036
2022-01-08 01:18:28,725 iteration 2554 : loss : 0.036234, loss_ce: 0.010119
2022-01-08 01:18:30,060 iteration 2555 : loss : 0.038398, loss_ce: 0.009557
2022-01-08 01:18:31,356 iteration 2556 : loss : 0.024776, loss_ce: 0.010909
2022-01-08 01:18:32,769 iteration 2557 : loss : 0.029526, loss_ce: 0.014395
2022-01-08 01:18:34,172 iteration 2558 : loss : 0.024055, loss_ce: 0.009721
2022-01-08 01:18:35,579 iteration 2559 : loss : 0.042651, loss_ce: 0.009615
2022-01-08 01:18:36,977 iteration 2560 : loss : 0.029734, loss_ce: 0.011813
2022-01-08 01:18:38,330 iteration 2561 : loss : 0.028939, loss_ce: 0.011714
2022-01-08 01:18:39,669 iteration 2562 : loss : 0.026634, loss_ce: 0.013725
2022-01-08 01:18:41,024 iteration 2563 : loss : 0.023366, loss_ce: 0.007462
2022-01-08 01:18:42,383 iteration 2564 : loss : 0.029080, loss_ce: 0.012977
2022-01-08 01:18:43,741 iteration 2565 : loss : 0.027333, loss_ce: 0.007888
2022-01-08 01:18:45,076 iteration 2566 : loss : 0.023175, loss_ce: 0.010229
2022-01-08 01:18:46,519 iteration 2567 : loss : 0.028687, loss_ce: 0.011595
 38%|██████████▏                | 151/400 [1:04:09<1:46:24, 25.64s/it]2022-01-08 01:18:47,916 iteration 2568 : loss : 0.027137, loss_ce: 0.011528
2022-01-08 01:18:49,283 iteration 2569 : loss : 0.029164, loss_ce: 0.010170
2022-01-08 01:18:50,619 iteration 2570 : loss : 0.034786, loss_ce: 0.009881
2022-01-08 01:18:51,986 iteration 2571 : loss : 0.036229, loss_ce: 0.014796
2022-01-08 01:18:53,279 iteration 2572 : loss : 0.023271, loss_ce: 0.006912
2022-01-08 01:18:54,600 iteration 2573 : loss : 0.020019, loss_ce: 0.007673
2022-01-08 01:18:55,944 iteration 2574 : loss : 0.036883, loss_ce: 0.013399
2022-01-08 01:18:57,332 iteration 2575 : loss : 0.029474, loss_ce: 0.011736
2022-01-08 01:18:58,784 iteration 2576 : loss : 0.029049, loss_ce: 0.012592
2022-01-08 01:19:00,137 iteration 2577 : loss : 0.029268, loss_ce: 0.010247
2022-01-08 01:19:01,540 iteration 2578 : loss : 0.031531, loss_ce: 0.014790
2022-01-08 01:19:02,869 iteration 2579 : loss : 0.031311, loss_ce: 0.011633
2022-01-08 01:19:04,262 iteration 2580 : loss : 0.029090, loss_ce: 0.010497
2022-01-08 01:19:05,730 iteration 2581 : loss : 0.043760, loss_ce: 0.016446
2022-01-08 01:19:06,998 iteration 2582 : loss : 0.024541, loss_ce: 0.009333
2022-01-08 01:19:08,406 iteration 2583 : loss : 0.023271, loss_ce: 0.007585
2022-01-08 01:19:09,750 iteration 2584 : loss : 0.020081, loss_ce: 0.007465
 38%|██████████▎                | 152/400 [1:04:33<1:42:58, 24.91s/it]2022-01-08 01:19:11,202 iteration 2585 : loss : 0.050051, loss_ce: 0.023810
2022-01-08 01:19:12,562 iteration 2586 : loss : 0.041121, loss_ce: 0.018247
2022-01-08 01:19:13,876 iteration 2587 : loss : 0.022561, loss_ce: 0.007364
2022-01-08 01:19:15,214 iteration 2588 : loss : 0.029134, loss_ce: 0.014203
2022-01-08 01:19:16,502 iteration 2589 : loss : 0.024553, loss_ce: 0.009628
2022-01-08 01:19:17,903 iteration 2590 : loss : 0.047147, loss_ce: 0.014866
2022-01-08 01:19:19,254 iteration 2591 : loss : 0.026682, loss_ce: 0.011400
2022-01-08 01:19:20,607 iteration 2592 : loss : 0.024765, loss_ce: 0.009470
2022-01-08 01:19:21,946 iteration 2593 : loss : 0.028887, loss_ce: 0.011040
2022-01-08 01:19:23,342 iteration 2594 : loss : 0.033364, loss_ce: 0.011919
2022-01-08 01:19:24,795 iteration 2595 : loss : 0.060324, loss_ce: 0.018142
2022-01-08 01:19:26,087 iteration 2596 : loss : 0.020381, loss_ce: 0.006659
2022-01-08 01:19:27,456 iteration 2597 : loss : 0.038807, loss_ce: 0.012498
2022-01-08 01:19:28,787 iteration 2598 : loss : 0.025354, loss_ce: 0.011751
2022-01-08 01:19:30,183 iteration 2599 : loss : 0.023424, loss_ce: 0.010351
2022-01-08 01:19:31,534 iteration 2600 : loss : 0.034546, loss_ce: 0.011625
2022-01-08 01:19:32,924 iteration 2601 : loss : 0.020655, loss_ce: 0.008020
 38%|██████████▎                | 153/400 [1:04:56<1:40:25, 24.39s/it]2022-01-08 01:19:34,345 iteration 2602 : loss : 0.027574, loss_ce: 0.008545
2022-01-08 01:19:35,780 iteration 2603 : loss : 0.032093, loss_ce: 0.014667
2022-01-08 01:19:37,122 iteration 2604 : loss : 0.037730, loss_ce: 0.018678
2022-01-08 01:19:38,360 iteration 2605 : loss : 0.020368, loss_ce: 0.005722
2022-01-08 01:19:39,840 iteration 2606 : loss : 0.051697, loss_ce: 0.017560
2022-01-08 01:19:41,274 iteration 2607 : loss : 0.025118, loss_ce: 0.008903
2022-01-08 01:19:42,639 iteration 2608 : loss : 0.049429, loss_ce: 0.028577
2022-01-08 01:19:44,090 iteration 2609 : loss : 0.043981, loss_ce: 0.017021
2022-01-08 01:19:45,486 iteration 2610 : loss : 0.029062, loss_ce: 0.011739
2022-01-08 01:19:46,865 iteration 2611 : loss : 0.048031, loss_ce: 0.017416
2022-01-08 01:19:48,278 iteration 2612 : loss : 0.035268, loss_ce: 0.017197
2022-01-08 01:19:49,600 iteration 2613 : loss : 0.028882, loss_ce: 0.012824
2022-01-08 01:19:51,037 iteration 2614 : loss : 0.032996, loss_ce: 0.010420
2022-01-08 01:19:52,363 iteration 2615 : loss : 0.032147, loss_ce: 0.012412
2022-01-08 01:19:53,757 iteration 2616 : loss : 0.026378, loss_ce: 0.013020
2022-01-08 01:19:55,115 iteration 2617 : loss : 0.029654, loss_ce: 0.010609
2022-01-08 01:19:56,446 iteration 2618 : loss : 0.022486, loss_ce: 0.009951
 38%|██████████▍                | 154/400 [1:05:19<1:38:56, 24.13s/it]2022-01-08 01:19:57,825 iteration 2619 : loss : 0.026317, loss_ce: 0.008596
2022-01-08 01:19:59,159 iteration 2620 : loss : 0.025708, loss_ce: 0.012107
2022-01-08 01:20:00,574 iteration 2621 : loss : 0.032670, loss_ce: 0.012535
2022-01-08 01:20:01,931 iteration 2622 : loss : 0.043533, loss_ce: 0.009885
2022-01-08 01:20:03,326 iteration 2623 : loss : 0.027734, loss_ce: 0.012615
2022-01-08 01:20:04,643 iteration 2624 : loss : 0.017993, loss_ce: 0.006297
2022-01-08 01:20:06,030 iteration 2625 : loss : 0.043888, loss_ce: 0.007129
2022-01-08 01:20:07,406 iteration 2626 : loss : 0.036405, loss_ce: 0.015652
2022-01-08 01:20:08,903 iteration 2627 : loss : 0.032643, loss_ce: 0.011301
2022-01-08 01:20:10,198 iteration 2628 : loss : 0.022237, loss_ce: 0.009578
2022-01-08 01:20:11,621 iteration 2629 : loss : 0.028474, loss_ce: 0.012656
2022-01-08 01:20:13,016 iteration 2630 : loss : 0.029279, loss_ce: 0.011757
2022-01-08 01:20:14,387 iteration 2631 : loss : 0.026205, loss_ce: 0.011914
2022-01-08 01:20:15,749 iteration 2632 : loss : 0.022383, loss_ce: 0.008468
2022-01-08 01:20:17,136 iteration 2633 : loss : 0.033868, loss_ce: 0.017605
2022-01-08 01:20:18,513 iteration 2634 : loss : 0.028606, loss_ce: 0.008756
2022-01-08 01:20:18,513 Training Data Eval:
2022-01-08 01:20:25,417   Average segmentation loss on training set: 0.0219
2022-01-08 01:20:25,417 Validation Data Eval:
2022-01-08 01:20:27,791   Average segmentation loss on validation set: 0.0665
2022-01-08 01:20:31,951 Found new lowest validation loss at iteration 2634! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 01:20:33,235 iteration 2635 : loss : 0.021897, loss_ce: 0.007444
 39%|██████████▍                | 155/400 [1:05:56<1:54:02, 27.93s/it]2022-01-08 01:20:34,546 iteration 2636 : loss : 0.038399, loss_ce: 0.016526
2022-01-08 01:20:35,899 iteration 2637 : loss : 0.033472, loss_ce: 0.013728
2022-01-08 01:20:37,153 iteration 2638 : loss : 0.026760, loss_ce: 0.008185
2022-01-08 01:20:38,380 iteration 2639 : loss : 0.023351, loss_ce: 0.010552
2022-01-08 01:20:39,631 iteration 2640 : loss : 0.023088, loss_ce: 0.008160
2022-01-08 01:20:40,893 iteration 2641 : loss : 0.036395, loss_ce: 0.016915
2022-01-08 01:20:42,123 iteration 2642 : loss : 0.029854, loss_ce: 0.010592
2022-01-08 01:20:43,545 iteration 2643 : loss : 0.028558, loss_ce: 0.009390
2022-01-08 01:20:44,880 iteration 2644 : loss : 0.022672, loss_ce: 0.007837
2022-01-08 01:20:46,216 iteration 2645 : loss : 0.020406, loss_ce: 0.006894
2022-01-08 01:20:47,749 iteration 2646 : loss : 0.031521, loss_ce: 0.012570
2022-01-08 01:20:49,183 iteration 2647 : loss : 0.034542, loss_ce: 0.016510
2022-01-08 01:20:50,546 iteration 2648 : loss : 0.028903, loss_ce: 0.011663
2022-01-08 01:20:51,930 iteration 2649 : loss : 0.026160, loss_ce: 0.010474
2022-01-08 01:20:53,354 iteration 2650 : loss : 0.031017, loss_ce: 0.010227
2022-01-08 01:20:54,742 iteration 2651 : loss : 0.032277, loss_ce: 0.012967
2022-01-08 01:20:56,057 iteration 2652 : loss : 0.019891, loss_ce: 0.008158
 39%|██████████▌                | 156/400 [1:06:19<1:47:20, 26.40s/it]2022-01-08 01:20:57,523 iteration 2653 : loss : 0.022146, loss_ce: 0.010615
2022-01-08 01:20:58,917 iteration 2654 : loss : 0.019791, loss_ce: 0.006873
2022-01-08 01:21:00,372 iteration 2655 : loss : 0.028991, loss_ce: 0.016714
2022-01-08 01:21:01,712 iteration 2656 : loss : 0.029776, loss_ce: 0.008999
2022-01-08 01:21:03,066 iteration 2657 : loss : 0.063298, loss_ce: 0.022523
2022-01-08 01:21:04,330 iteration 2658 : loss : 0.021858, loss_ce: 0.010605
2022-01-08 01:21:05,666 iteration 2659 : loss : 0.028639, loss_ce: 0.006028
2022-01-08 01:21:07,078 iteration 2660 : loss : 0.028410, loss_ce: 0.009068
2022-01-08 01:21:08,411 iteration 2661 : loss : 0.022639, loss_ce: 0.011061
2022-01-08 01:21:09,764 iteration 2662 : loss : 0.031322, loss_ce: 0.015650
2022-01-08 01:21:11,100 iteration 2663 : loss : 0.027008, loss_ce: 0.008159
2022-01-08 01:21:12,561 iteration 2664 : loss : 0.028615, loss_ce: 0.012901
2022-01-08 01:21:13,926 iteration 2665 : loss : 0.036764, loss_ce: 0.013625
2022-01-08 01:21:15,258 iteration 2666 : loss : 0.023046, loss_ce: 0.008191
2022-01-08 01:21:16,689 iteration 2667 : loss : 0.031729, loss_ce: 0.011115
2022-01-08 01:21:18,028 iteration 2668 : loss : 0.020045, loss_ce: 0.006791
2022-01-08 01:21:19,369 iteration 2669 : loss : 0.034311, loss_ce: 0.012177
 39%|██████████▌                | 157/400 [1:06:42<1:43:09, 25.47s/it]2022-01-08 01:21:20,799 iteration 2670 : loss : 0.023582, loss_ce: 0.010534
2022-01-08 01:21:22,272 iteration 2671 : loss : 0.031852, loss_ce: 0.011109
2022-01-08 01:21:23,668 iteration 2672 : loss : 0.022450, loss_ce: 0.008761
2022-01-08 01:21:25,000 iteration 2673 : loss : 0.020770, loss_ce: 0.010148
2022-01-08 01:21:26,425 iteration 2674 : loss : 0.039521, loss_ce: 0.020022
2022-01-08 01:21:27,694 iteration 2675 : loss : 0.017941, loss_ce: 0.007607
2022-01-08 01:21:29,170 iteration 2676 : loss : 0.027744, loss_ce: 0.008508
2022-01-08 01:21:30,559 iteration 2677 : loss : 0.039620, loss_ce: 0.015563
2022-01-08 01:21:31,909 iteration 2678 : loss : 0.026994, loss_ce: 0.010536
2022-01-08 01:21:33,277 iteration 2679 : loss : 0.029538, loss_ce: 0.010183
2022-01-08 01:21:34,637 iteration 2680 : loss : 0.019870, loss_ce: 0.008063
2022-01-08 01:21:36,011 iteration 2681 : loss : 0.021778, loss_ce: 0.006774
2022-01-08 01:21:37,386 iteration 2682 : loss : 0.038119, loss_ce: 0.013806
2022-01-08 01:21:38,740 iteration 2683 : loss : 0.046315, loss_ce: 0.010668
2022-01-08 01:21:40,055 iteration 2684 : loss : 0.022577, loss_ce: 0.010772
2022-01-08 01:21:41,434 iteration 2685 : loss : 0.026769, loss_ce: 0.011480
2022-01-08 01:21:42,755 iteration 2686 : loss : 0.029298, loss_ce: 0.013636
 40%|██████████▋                | 158/400 [1:07:06<1:40:12, 24.85s/it]2022-01-08 01:21:44,074 iteration 2687 : loss : 0.025366, loss_ce: 0.007242
2022-01-08 01:21:45,454 iteration 2688 : loss : 0.029417, loss_ce: 0.009383
2022-01-08 01:21:46,803 iteration 2689 : loss : 0.026098, loss_ce: 0.008718
2022-01-08 01:21:48,273 iteration 2690 : loss : 0.042758, loss_ce: 0.020377
2022-01-08 01:21:49,584 iteration 2691 : loss : 0.025087, loss_ce: 0.009796
2022-01-08 01:21:50,907 iteration 2692 : loss : 0.026869, loss_ce: 0.008477
2022-01-08 01:21:52,274 iteration 2693 : loss : 0.026862, loss_ce: 0.009907
2022-01-08 01:21:53,707 iteration 2694 : loss : 0.032580, loss_ce: 0.010495
2022-01-08 01:21:55,052 iteration 2695 : loss : 0.022148, loss_ce: 0.009128
2022-01-08 01:21:56,435 iteration 2696 : loss : 0.032465, loss_ce: 0.014487
2022-01-08 01:21:57,743 iteration 2697 : loss : 0.025819, loss_ce: 0.013294
2022-01-08 01:21:58,992 iteration 2698 : loss : 0.019186, loss_ce: 0.008741
2022-01-08 01:22:00,369 iteration 2699 : loss : 0.034085, loss_ce: 0.014590
2022-01-08 01:22:01,780 iteration 2700 : loss : 0.033943, loss_ce: 0.018868
2022-01-08 01:22:03,137 iteration 2701 : loss : 0.028687, loss_ce: 0.010268
2022-01-08 01:22:04,534 iteration 2702 : loss : 0.058644, loss_ce: 0.008905
2022-01-08 01:22:05,903 iteration 2703 : loss : 0.026263, loss_ce: 0.010264
 40%|██████████▋                | 159/400 [1:07:29<1:37:45, 24.34s/it]2022-01-08 01:22:07,428 iteration 2704 : loss : 0.040104, loss_ce: 0.018687
2022-01-08 01:22:08,753 iteration 2705 : loss : 0.030076, loss_ce: 0.008842
2022-01-08 01:22:10,074 iteration 2706 : loss : 0.030452, loss_ce: 0.016201
2022-01-08 01:22:11,503 iteration 2707 : loss : 0.035016, loss_ce: 0.009527
2022-01-08 01:22:12,790 iteration 2708 : loss : 0.020568, loss_ce: 0.007430
2022-01-08 01:22:14,160 iteration 2709 : loss : 0.027177, loss_ce: 0.007876
2022-01-08 01:22:15,472 iteration 2710 : loss : 0.020601, loss_ce: 0.007709
2022-01-08 01:22:16,935 iteration 2711 : loss : 0.038668, loss_ce: 0.013750
2022-01-08 01:22:18,248 iteration 2712 : loss : 0.020889, loss_ce: 0.007343
2022-01-08 01:22:19,723 iteration 2713 : loss : 0.054123, loss_ce: 0.017390
2022-01-08 01:22:21,029 iteration 2714 : loss : 0.037702, loss_ce: 0.011361
2022-01-08 01:22:22,406 iteration 2715 : loss : 0.039092, loss_ce: 0.021460
2022-01-08 01:22:23,737 iteration 2716 : loss : 0.026455, loss_ce: 0.009718
2022-01-08 01:22:25,042 iteration 2717 : loss : 0.018324, loss_ce: 0.008642
2022-01-08 01:22:26,366 iteration 2718 : loss : 0.024431, loss_ce: 0.009134
2022-01-08 01:22:27,809 iteration 2719 : loss : 0.043010, loss_ce: 0.014967
2022-01-08 01:22:27,810 Training Data Eval:
2022-01-08 01:22:34,718   Average segmentation loss on training set: 0.0210
2022-01-08 01:22:34,718 Validation Data Eval:
2022-01-08 01:22:37,087   Average segmentation loss on validation set: 0.1069
2022-01-08 01:22:38,400 iteration 2720 : loss : 0.022691, loss_ce: 0.010470
 40%|██████████▊                | 160/400 [1:08:01<1:47:08, 26.78s/it]2022-01-08 01:22:39,828 iteration 2721 : loss : 0.025858, loss_ce: 0.010528
2022-01-08 01:22:41,227 iteration 2722 : loss : 0.033290, loss_ce: 0.015722
2022-01-08 01:22:42,623 iteration 2723 : loss : 0.021286, loss_ce: 0.007961
2022-01-08 01:22:43,933 iteration 2724 : loss : 0.027947, loss_ce: 0.009481
2022-01-08 01:22:45,224 iteration 2725 : loss : 0.025954, loss_ce: 0.010063
2022-01-08 01:22:46,565 iteration 2726 : loss : 0.034028, loss_ce: 0.012745
2022-01-08 01:22:47,871 iteration 2727 : loss : 0.026330, loss_ce: 0.009879
2022-01-08 01:22:49,364 iteration 2728 : loss : 0.031562, loss_ce: 0.017039
2022-01-08 01:22:50,727 iteration 2729 : loss : 0.043409, loss_ce: 0.023432
2022-01-08 01:22:52,130 iteration 2730 : loss : 0.020713, loss_ce: 0.009176
2022-01-08 01:22:53,527 iteration 2731 : loss : 0.026377, loss_ce: 0.011035
2022-01-08 01:22:54,982 iteration 2732 : loss : 0.029489, loss_ce: 0.010688
2022-01-08 01:22:56,369 iteration 2733 : loss : 0.040189, loss_ce: 0.016379
2022-01-08 01:22:57,759 iteration 2734 : loss : 0.021649, loss_ce: 0.006883
2022-01-08 01:22:59,047 iteration 2735 : loss : 0.021615, loss_ce: 0.008680
2022-01-08 01:23:00,467 iteration 2736 : loss : 0.026364, loss_ce: 0.012218
2022-01-08 01:23:01,821 iteration 2737 : loss : 0.034280, loss_ce: 0.015255
 40%|██████████▊                | 161/400 [1:08:25<1:42:39, 25.77s/it]2022-01-08 01:23:03,216 iteration 2738 : loss : 0.022722, loss_ce: 0.010455
2022-01-08 01:23:04,564 iteration 2739 : loss : 0.023126, loss_ce: 0.011919
2022-01-08 01:23:05,946 iteration 2740 : loss : 0.034784, loss_ce: 0.013514
2022-01-08 01:23:07,333 iteration 2741 : loss : 0.023278, loss_ce: 0.007321
2022-01-08 01:23:08,637 iteration 2742 : loss : 0.020695, loss_ce: 0.008148
2022-01-08 01:23:10,022 iteration 2743 : loss : 0.040668, loss_ce: 0.022612
2022-01-08 01:23:11,419 iteration 2744 : loss : 0.024561, loss_ce: 0.009465
2022-01-08 01:23:12,792 iteration 2745 : loss : 0.023539, loss_ce: 0.009584
2022-01-08 01:23:14,090 iteration 2746 : loss : 0.025424, loss_ce: 0.007468
2022-01-08 01:23:15,451 iteration 2747 : loss : 0.023963, loss_ce: 0.007625
2022-01-08 01:23:16,858 iteration 2748 : loss : 0.024784, loss_ce: 0.008773
2022-01-08 01:23:18,159 iteration 2749 : loss : 0.025197, loss_ce: 0.010382
2022-01-08 01:23:19,556 iteration 2750 : loss : 0.025373, loss_ce: 0.007787
2022-01-08 01:23:20,885 iteration 2751 : loss : 0.019057, loss_ce: 0.007882
2022-01-08 01:23:22,169 iteration 2752 : loss : 0.032345, loss_ce: 0.006596
2022-01-08 01:23:23,613 iteration 2753 : loss : 0.030239, loss_ce: 0.013237
2022-01-08 01:23:25,023 iteration 2754 : loss : 0.021110, loss_ce: 0.008383
 40%|██████████▉                | 162/400 [1:08:48<1:39:10, 25.00s/it]2022-01-08 01:23:26,424 iteration 2755 : loss : 0.040437, loss_ce: 0.017345
2022-01-08 01:23:27,797 iteration 2756 : loss : 0.037751, loss_ce: 0.021551
2022-01-08 01:23:29,170 iteration 2757 : loss : 0.028555, loss_ce: 0.012713
2022-01-08 01:23:30,618 iteration 2758 : loss : 0.034001, loss_ce: 0.012133
2022-01-08 01:23:31,993 iteration 2759 : loss : 0.029442, loss_ce: 0.009151
2022-01-08 01:23:33,359 iteration 2760 : loss : 0.031526, loss_ce: 0.011916
2022-01-08 01:23:34,635 iteration 2761 : loss : 0.026223, loss_ce: 0.010059
2022-01-08 01:23:35,938 iteration 2762 : loss : 0.020198, loss_ce: 0.008535
2022-01-08 01:23:37,348 iteration 2763 : loss : 0.039108, loss_ce: 0.012518
2022-01-08 01:23:38,710 iteration 2764 : loss : 0.022762, loss_ce: 0.009184
2022-01-08 01:23:40,087 iteration 2765 : loss : 0.038995, loss_ce: 0.016696
2022-01-08 01:23:41,424 iteration 2766 : loss : 0.024948, loss_ce: 0.009296
2022-01-08 01:23:42,814 iteration 2767 : loss : 0.025337, loss_ce: 0.009504
2022-01-08 01:23:44,144 iteration 2768 : loss : 0.018424, loss_ce: 0.005068
2022-01-08 01:23:45,510 iteration 2769 : loss : 0.020024, loss_ce: 0.008342
2022-01-08 01:23:46,874 iteration 2770 : loss : 0.038775, loss_ce: 0.019288
2022-01-08 01:23:48,213 iteration 2771 : loss : 0.020087, loss_ce: 0.006721
 41%|███████████                | 163/400 [1:09:11<1:36:37, 24.46s/it]2022-01-08 01:23:49,683 iteration 2772 : loss : 0.030661, loss_ce: 0.014806
2022-01-08 01:23:51,011 iteration 2773 : loss : 0.019396, loss_ce: 0.008125
2022-01-08 01:23:52,312 iteration 2774 : loss : 0.029638, loss_ce: 0.007361
2022-01-08 01:23:53,649 iteration 2775 : loss : 0.031742, loss_ce: 0.012686
2022-01-08 01:23:54,972 iteration 2776 : loss : 0.019703, loss_ce: 0.006885
2022-01-08 01:23:56,339 iteration 2777 : loss : 0.017706, loss_ce: 0.006725
2022-01-08 01:23:57,774 iteration 2778 : loss : 0.038340, loss_ce: 0.016096
2022-01-08 01:23:59,230 iteration 2779 : loss : 0.027080, loss_ce: 0.008498
2022-01-08 01:24:00,544 iteration 2780 : loss : 0.021691, loss_ce: 0.007084
2022-01-08 01:24:01,954 iteration 2781 : loss : 0.038062, loss_ce: 0.012366
2022-01-08 01:24:03,303 iteration 2782 : loss : 0.023350, loss_ce: 0.008038
2022-01-08 01:24:04,623 iteration 2783 : loss : 0.050916, loss_ce: 0.033470
2022-01-08 01:24:06,092 iteration 2784 : loss : 0.038741, loss_ce: 0.010994
2022-01-08 01:24:07,518 iteration 2785 : loss : 0.037372, loss_ce: 0.011217
2022-01-08 01:24:08,908 iteration 2786 : loss : 0.033987, loss_ce: 0.014693
2022-01-08 01:24:10,330 iteration 2787 : loss : 0.026236, loss_ce: 0.011379
2022-01-08 01:24:11,669 iteration 2788 : loss : 0.024198, loss_ce: 0.013047
 41%|███████████                | 164/400 [1:09:34<1:35:01, 24.16s/it]2022-01-08 01:24:13,040 iteration 2789 : loss : 0.027167, loss_ce: 0.010556
2022-01-08 01:24:14,432 iteration 2790 : loss : 0.023476, loss_ce: 0.007952
2022-01-08 01:24:15,762 iteration 2791 : loss : 0.038836, loss_ce: 0.014835
2022-01-08 01:24:17,183 iteration 2792 : loss : 0.040336, loss_ce: 0.013572
2022-01-08 01:24:18,508 iteration 2793 : loss : 0.027512, loss_ce: 0.011800
2022-01-08 01:24:19,913 iteration 2794 : loss : 0.022956, loss_ce: 0.007398
2022-01-08 01:24:21,240 iteration 2795 : loss : 0.018327, loss_ce: 0.006125
2022-01-08 01:24:22,630 iteration 2796 : loss : 0.033302, loss_ce: 0.011317
2022-01-08 01:24:24,021 iteration 2797 : loss : 0.026093, loss_ce: 0.010700
2022-01-08 01:24:25,444 iteration 2798 : loss : 0.031232, loss_ce: 0.016171
2022-01-08 01:24:26,768 iteration 2799 : loss : 0.017406, loss_ce: 0.007323
2022-01-08 01:24:28,093 iteration 2800 : loss : 0.044157, loss_ce: 0.016535
2022-01-08 01:24:29,405 iteration 2801 : loss : 0.014858, loss_ce: 0.005188
2022-01-08 01:24:30,826 iteration 2802 : loss : 0.022551, loss_ce: 0.006975
2022-01-08 01:24:32,183 iteration 2803 : loss : 0.025201, loss_ce: 0.011626
2022-01-08 01:24:33,521 iteration 2804 : loss : 0.025176, loss_ce: 0.008634
2022-01-08 01:24:33,522 Training Data Eval:
2022-01-08 01:24:40,439   Average segmentation loss on training set: 0.0183
2022-01-08 01:24:40,439 Validation Data Eval:
2022-01-08 01:24:42,814   Average segmentation loss on validation set: 0.1330
2022-01-08 01:24:44,149 iteration 2805 : loss : 0.024404, loss_ce: 0.010313
 41%|███████████▏               | 165/400 [1:10:07<1:44:23, 26.65s/it]2022-01-08 01:24:45,663 iteration 2806 : loss : 0.041807, loss_ce: 0.020122
2022-01-08 01:24:46,953 iteration 2807 : loss : 0.022020, loss_ce: 0.007821
2022-01-08 01:24:48,281 iteration 2808 : loss : 0.017041, loss_ce: 0.006805
2022-01-08 01:24:49,666 iteration 2809 : loss : 0.017142, loss_ce: 0.005900
2022-01-08 01:24:51,007 iteration 2810 : loss : 0.030097, loss_ce: 0.014130
2022-01-08 01:24:52,478 iteration 2811 : loss : 0.031544, loss_ce: 0.010178
2022-01-08 01:24:53,956 iteration 2812 : loss : 0.022572, loss_ce: 0.009592
2022-01-08 01:24:55,229 iteration 2813 : loss : 0.021457, loss_ce: 0.009110
2022-01-08 01:24:56,615 iteration 2814 : loss : 0.025364, loss_ce: 0.012810
2022-01-08 01:24:58,016 iteration 2815 : loss : 0.029460, loss_ce: 0.013133
2022-01-08 01:24:59,357 iteration 2816 : loss : 0.034158, loss_ce: 0.013703
2022-01-08 01:25:00,691 iteration 2817 : loss : 0.027700, loss_ce: 0.011789
2022-01-08 01:25:02,151 iteration 2818 : loss : 0.029010, loss_ce: 0.010712
2022-01-08 01:25:03,569 iteration 2819 : loss : 0.026833, loss_ce: 0.013225
2022-01-08 01:25:04,914 iteration 2820 : loss : 0.022403, loss_ce: 0.008133
2022-01-08 01:25:06,287 iteration 2821 : loss : 0.023240, loss_ce: 0.010004
2022-01-08 01:25:07,698 iteration 2822 : loss : 0.062023, loss_ce: 0.012921
 42%|███████████▏               | 166/400 [1:10:31<1:40:18, 25.72s/it]2022-01-08 01:25:09,121 iteration 2823 : loss : 0.023607, loss_ce: 0.008381
2022-01-08 01:25:10,521 iteration 2824 : loss : 0.032819, loss_ce: 0.010242
2022-01-08 01:25:11,920 iteration 2825 : loss : 0.032048, loss_ce: 0.012186
2022-01-08 01:25:13,268 iteration 2826 : loss : 0.019544, loss_ce: 0.007236
2022-01-08 01:25:14,697 iteration 2827 : loss : 0.034976, loss_ce: 0.018829
2022-01-08 01:25:16,041 iteration 2828 : loss : 0.036056, loss_ce: 0.015297
2022-01-08 01:25:17,380 iteration 2829 : loss : 0.034687, loss_ce: 0.011567
2022-01-08 01:25:18,761 iteration 2830 : loss : 0.023377, loss_ce: 0.010590
2022-01-08 01:25:20,116 iteration 2831 : loss : 0.029750, loss_ce: 0.008569
2022-01-08 01:25:21,433 iteration 2832 : loss : 0.026629, loss_ce: 0.010590
2022-01-08 01:25:22,874 iteration 2833 : loss : 0.031497, loss_ce: 0.012583
2022-01-08 01:25:24,245 iteration 2834 : loss : 0.025219, loss_ce: 0.010666
2022-01-08 01:25:25,631 iteration 2835 : loss : 0.037778, loss_ce: 0.013960
2022-01-08 01:25:27,016 iteration 2836 : loss : 0.025938, loss_ce: 0.009188
2022-01-08 01:25:28,419 iteration 2837 : loss : 0.025249, loss_ce: 0.008303
2022-01-08 01:25:29,815 iteration 2838 : loss : 0.022827, loss_ce: 0.008031
2022-01-08 01:25:31,119 iteration 2839 : loss : 0.034462, loss_ce: 0.013942
 42%|███████████▎               | 167/400 [1:10:54<1:37:12, 25.03s/it]2022-01-08 01:25:32,510 iteration 2840 : loss : 0.025165, loss_ce: 0.009022
2022-01-08 01:25:33,849 iteration 2841 : loss : 0.020341, loss_ce: 0.009003
2022-01-08 01:25:35,156 iteration 2842 : loss : 0.017123, loss_ce: 0.006139
2022-01-08 01:25:36,525 iteration 2843 : loss : 0.047571, loss_ce: 0.016132
2022-01-08 01:25:37,942 iteration 2844 : loss : 0.031109, loss_ce: 0.013876
2022-01-08 01:25:39,368 iteration 2845 : loss : 0.021364, loss_ce: 0.007307
2022-01-08 01:25:40,712 iteration 2846 : loss : 0.028283, loss_ce: 0.010729
2022-01-08 01:25:42,083 iteration 2847 : loss : 0.028318, loss_ce: 0.011824
2022-01-08 01:25:43,535 iteration 2848 : loss : 0.028341, loss_ce: 0.014651
2022-01-08 01:25:44,881 iteration 2849 : loss : 0.029879, loss_ce: 0.009994
2022-01-08 01:25:46,214 iteration 2850 : loss : 0.028417, loss_ce: 0.012102
2022-01-08 01:25:47,588 iteration 2851 : loss : 0.021936, loss_ce: 0.009571
2022-01-08 01:25:48,956 iteration 2852 : loss : 0.037052, loss_ce: 0.014930
2022-01-08 01:25:50,277 iteration 2853 : loss : 0.025990, loss_ce: 0.010907
2022-01-08 01:25:51,677 iteration 2854 : loss : 0.030410, loss_ce: 0.013937
2022-01-08 01:25:53,050 iteration 2855 : loss : 0.040439, loss_ce: 0.018519
2022-01-08 01:25:54,489 iteration 2856 : loss : 0.023242, loss_ce: 0.009341
 42%|███████████▎               | 168/400 [1:11:17<1:34:51, 24.53s/it]2022-01-08 01:25:55,960 iteration 2857 : loss : 0.030591, loss_ce: 0.007550
2022-01-08 01:25:57,343 iteration 2858 : loss : 0.050167, loss_ce: 0.017539
2022-01-08 01:25:58,647 iteration 2859 : loss : 0.022206, loss_ce: 0.008484
2022-01-08 01:26:00,074 iteration 2860 : loss : 0.038387, loss_ce: 0.016807
2022-01-08 01:26:01,408 iteration 2861 : loss : 0.026802, loss_ce: 0.011189
2022-01-08 01:26:02,803 iteration 2862 : loss : 0.024567, loss_ce: 0.008394
2022-01-08 01:26:04,235 iteration 2863 : loss : 0.049242, loss_ce: 0.017133
2022-01-08 01:26:05,601 iteration 2864 : loss : 0.022128, loss_ce: 0.009697
2022-01-08 01:26:06,898 iteration 2865 : loss : 0.021491, loss_ce: 0.008015
2022-01-08 01:26:08,252 iteration 2866 : loss : 0.021820, loss_ce: 0.006264
2022-01-08 01:26:09,674 iteration 2867 : loss : 0.024607, loss_ce: 0.010729
2022-01-08 01:26:11,078 iteration 2868 : loss : 0.025089, loss_ce: 0.010064
2022-01-08 01:26:12,399 iteration 2869 : loss : 0.022400, loss_ce: 0.009749
2022-01-08 01:26:13,771 iteration 2870 : loss : 0.033522, loss_ce: 0.017274
2022-01-08 01:26:15,054 iteration 2871 : loss : 0.020567, loss_ce: 0.009676
2022-01-08 01:26:16,427 iteration 2872 : loss : 0.038154, loss_ce: 0.013826
2022-01-08 01:26:17,847 iteration 2873 : loss : 0.029427, loss_ce: 0.015445
 42%|███████████▍               | 169/400 [1:11:41<1:33:06, 24.18s/it]2022-01-08 01:26:19,223 iteration 2874 : loss : 0.039888, loss_ce: 0.012380
2022-01-08 01:26:20,503 iteration 2875 : loss : 0.018532, loss_ce: 0.007364
2022-01-08 01:26:21,840 iteration 2876 : loss : 0.023653, loss_ce: 0.010737
2022-01-08 01:26:23,222 iteration 2877 : loss : 0.023151, loss_ce: 0.008189
2022-01-08 01:26:24,617 iteration 2878 : loss : 0.029072, loss_ce: 0.009862
2022-01-08 01:26:25,928 iteration 2879 : loss : 0.025058, loss_ce: 0.012670
2022-01-08 01:26:27,349 iteration 2880 : loss : 0.037327, loss_ce: 0.015022
2022-01-08 01:26:28,701 iteration 2881 : loss : 0.033204, loss_ce: 0.014214
2022-01-08 01:26:30,148 iteration 2882 : loss : 0.033143, loss_ce: 0.008094
2022-01-08 01:26:31,584 iteration 2883 : loss : 0.036012, loss_ce: 0.014301
2022-01-08 01:26:33,020 iteration 2884 : loss : 0.028309, loss_ce: 0.009018
2022-01-08 01:26:34,386 iteration 2885 : loss : 0.023255, loss_ce: 0.011616
2022-01-08 01:26:35,720 iteration 2886 : loss : 0.034114, loss_ce: 0.008907
2022-01-08 01:26:37,094 iteration 2887 : loss : 0.021684, loss_ce: 0.007782
2022-01-08 01:26:38,556 iteration 2888 : loss : 0.019497, loss_ce: 0.007110
2022-01-08 01:26:39,938 iteration 2889 : loss : 0.041529, loss_ce: 0.015546
2022-01-08 01:26:39,938 Training Data Eval:
2022-01-08 01:26:46,824   Average segmentation loss on training set: 0.0168
2022-01-08 01:26:46,825 Validation Data Eval:
2022-01-08 01:26:49,197   Average segmentation loss on validation set: 0.0796
2022-01-08 01:26:50,623 iteration 2890 : loss : 0.034128, loss_ce: 0.012664
 42%|███████████▍               | 170/400 [1:12:13<1:42:34, 26.76s/it]2022-01-08 01:26:52,095 iteration 2891 : loss : 0.025658, loss_ce: 0.012213
2022-01-08 01:26:53,405 iteration 2892 : loss : 0.019514, loss_ce: 0.009082
2022-01-08 01:26:54,757 iteration 2893 : loss : 0.035276, loss_ce: 0.009311
2022-01-08 01:26:56,175 iteration 2894 : loss : 0.017676, loss_ce: 0.006921
2022-01-08 01:26:57,531 iteration 2895 : loss : 0.022879, loss_ce: 0.007164
2022-01-08 01:26:58,859 iteration 2896 : loss : 0.020608, loss_ce: 0.007331
2022-01-08 01:27:00,223 iteration 2897 : loss : 0.023399, loss_ce: 0.008623
2022-01-08 01:27:01,600 iteration 2898 : loss : 0.034848, loss_ce: 0.018151
2022-01-08 01:27:02,992 iteration 2899 : loss : 0.029781, loss_ce: 0.011066
2022-01-08 01:27:04,315 iteration 2900 : loss : 0.024912, loss_ce: 0.007760
2022-01-08 01:27:05,795 iteration 2901 : loss : 0.026425, loss_ce: 0.008550
2022-01-08 01:27:07,062 iteration 2902 : loss : 0.018654, loss_ce: 0.005956
2022-01-08 01:27:08,472 iteration 2903 : loss : 0.031744, loss_ce: 0.015947
2022-01-08 01:27:09,904 iteration 2904 : loss : 0.047047, loss_ce: 0.015524
2022-01-08 01:27:11,273 iteration 2905 : loss : 0.030838, loss_ce: 0.011796
2022-01-08 01:27:12,720 iteration 2906 : loss : 0.043333, loss_ce: 0.016446
2022-01-08 01:27:14,041 iteration 2907 : loss : 0.018033, loss_ce: 0.007401
 43%|███████████▌               | 171/400 [1:12:37<1:38:18, 25.76s/it]2022-01-08 01:27:15,455 iteration 2908 : loss : 0.026984, loss_ce: 0.010248
2022-01-08 01:27:16,758 iteration 2909 : loss : 0.022911, loss_ce: 0.005770
2022-01-08 01:27:18,173 iteration 2910 : loss : 0.035272, loss_ce: 0.010613
2022-01-08 01:27:19,539 iteration 2911 : loss : 0.023548, loss_ce: 0.009753
2022-01-08 01:27:20,906 iteration 2912 : loss : 0.023601, loss_ce: 0.008048
2022-01-08 01:27:22,327 iteration 2913 : loss : 0.027693, loss_ce: 0.009915
2022-01-08 01:27:23,710 iteration 2914 : loss : 0.026753, loss_ce: 0.010711
2022-01-08 01:27:25,149 iteration 2915 : loss : 0.023115, loss_ce: 0.008135
2022-01-08 01:27:26,466 iteration 2916 : loss : 0.027575, loss_ce: 0.014561
2022-01-08 01:27:27,899 iteration 2917 : loss : 0.036045, loss_ce: 0.017951
2022-01-08 01:27:29,297 iteration 2918 : loss : 0.021812, loss_ce: 0.006980
2022-01-08 01:27:30,709 iteration 2919 : loss : 0.037633, loss_ce: 0.019074
2022-01-08 01:27:32,107 iteration 2920 : loss : 0.033767, loss_ce: 0.008563
2022-01-08 01:27:33,565 iteration 2921 : loss : 0.020821, loss_ce: 0.006311
2022-01-08 01:27:34,914 iteration 2922 : loss : 0.024660, loss_ce: 0.010499
2022-01-08 01:27:36,324 iteration 2923 : loss : 0.029484, loss_ce: 0.012749
2022-01-08 01:27:37,686 iteration 2924 : loss : 0.018866, loss_ce: 0.005951
 43%|███████████▌               | 172/400 [1:13:01<1:35:28, 25.13s/it]2022-01-08 01:27:39,179 iteration 2925 : loss : 0.029233, loss_ce: 0.011292
2022-01-08 01:27:40,514 iteration 2926 : loss : 0.018855, loss_ce: 0.005708
2022-01-08 01:27:41,947 iteration 2927 : loss : 0.030463, loss_ce: 0.013576
2022-01-08 01:27:43,430 iteration 2928 : loss : 0.053957, loss_ce: 0.015578
2022-01-08 01:27:44,784 iteration 2929 : loss : 0.025088, loss_ce: 0.009186
2022-01-08 01:27:46,133 iteration 2930 : loss : 0.025503, loss_ce: 0.011180
2022-01-08 01:27:47,430 iteration 2931 : loss : 0.029659, loss_ce: 0.011207
2022-01-08 01:27:48,859 iteration 2932 : loss : 0.026058, loss_ce: 0.010075
2022-01-08 01:27:50,205 iteration 2933 : loss : 0.040259, loss_ce: 0.021724
2022-01-08 01:27:51,624 iteration 2934 : loss : 0.027754, loss_ce: 0.012343
2022-01-08 01:27:53,001 iteration 2935 : loss : 0.046258, loss_ce: 0.013664
2022-01-08 01:27:54,432 iteration 2936 : loss : 0.036551, loss_ce: 0.019918
2022-01-08 01:27:55,786 iteration 2937 : loss : 0.020870, loss_ce: 0.008845
2022-01-08 01:27:57,163 iteration 2938 : loss : 0.023400, loss_ce: 0.009039
2022-01-08 01:27:58,519 iteration 2939 : loss : 0.023641, loss_ce: 0.010777
2022-01-08 01:27:59,924 iteration 2940 : loss : 0.025140, loss_ce: 0.008422
2022-01-08 01:28:01,287 iteration 2941 : loss : 0.023272, loss_ce: 0.008044
 43%|███████████▋               | 173/400 [1:13:24<1:33:19, 24.67s/it]2022-01-08 01:28:02,716 iteration 2942 : loss : 0.024876, loss_ce: 0.010704
2022-01-08 01:28:03,999 iteration 2943 : loss : 0.019215, loss_ce: 0.007574
2022-01-08 01:28:05,315 iteration 2944 : loss : 0.027435, loss_ce: 0.011861
2022-01-08 01:28:06,758 iteration 2945 : loss : 0.034493, loss_ce: 0.016113
2022-01-08 01:28:08,093 iteration 2946 : loss : 0.025037, loss_ce: 0.010308
2022-01-08 01:28:09,414 iteration 2947 : loss : 0.015908, loss_ce: 0.005386
2022-01-08 01:28:10,787 iteration 2948 : loss : 0.027572, loss_ce: 0.009880
2022-01-08 01:28:12,185 iteration 2949 : loss : 0.025097, loss_ce: 0.009817
2022-01-08 01:28:13,539 iteration 2950 : loss : 0.024784, loss_ce: 0.011001
2022-01-08 01:28:14,842 iteration 2951 : loss : 0.053139, loss_ce: 0.023801
2022-01-08 01:28:16,215 iteration 2952 : loss : 0.030083, loss_ce: 0.006714
2022-01-08 01:28:17,577 iteration 2953 : loss : 0.020645, loss_ce: 0.008244
2022-01-08 01:28:18,973 iteration 2954 : loss : 0.037161, loss_ce: 0.011393
2022-01-08 01:28:20,296 iteration 2955 : loss : 0.021436, loss_ce: 0.009754
2022-01-08 01:28:21,648 iteration 2956 : loss : 0.020194, loss_ce: 0.009832
2022-01-08 01:28:23,011 iteration 2957 : loss : 0.031451, loss_ce: 0.007459
2022-01-08 01:28:24,379 iteration 2958 : loss : 0.042277, loss_ce: 0.013782
 44%|███████████▋               | 174/400 [1:13:47<1:31:07, 24.19s/it]2022-01-08 01:28:25,857 iteration 2959 : loss : 0.022785, loss_ce: 0.010462
2022-01-08 01:28:27,235 iteration 2960 : loss : 0.021925, loss_ce: 0.007700
2022-01-08 01:28:28,647 iteration 2961 : loss : 0.024146, loss_ce: 0.010922
2022-01-08 01:28:29,965 iteration 2962 : loss : 0.019875, loss_ce: 0.006562
2022-01-08 01:28:31,325 iteration 2963 : loss : 0.024188, loss_ce: 0.009235
2022-01-08 01:28:32,766 iteration 2964 : loss : 0.037998, loss_ce: 0.014274
2022-01-08 01:28:34,159 iteration 2965 : loss : 0.020308, loss_ce: 0.009287
2022-01-08 01:28:35,514 iteration 2966 : loss : 0.026688, loss_ce: 0.009244
2022-01-08 01:28:36,960 iteration 2967 : loss : 0.034014, loss_ce: 0.013860
2022-01-08 01:28:38,329 iteration 2968 : loss : 0.039864, loss_ce: 0.016029
2022-01-08 01:28:39,642 iteration 2969 : loss : 0.030686, loss_ce: 0.010183
2022-01-08 01:28:40,983 iteration 2970 : loss : 0.021617, loss_ce: 0.008612
2022-01-08 01:28:42,374 iteration 2971 : loss : 0.031280, loss_ce: 0.013105
2022-01-08 01:28:43,768 iteration 2972 : loss : 0.019641, loss_ce: 0.007035
2022-01-08 01:28:45,122 iteration 2973 : loss : 0.019933, loss_ce: 0.009886
2022-01-08 01:28:46,593 iteration 2974 : loss : 0.036869, loss_ce: 0.014232
2022-01-08 01:28:46,593 Training Data Eval:
2022-01-08 01:28:53,491   Average segmentation loss on training set: 0.0160
2022-01-08 01:28:53,492 Validation Data Eval:
2022-01-08 01:28:55,857   Average segmentation loss on validation set: 0.0845
2022-01-08 01:28:57,208 iteration 2975 : loss : 0.030569, loss_ce: 0.011205
 44%|███████████▊               | 175/400 [1:14:20<1:40:26, 26.79s/it]2022-01-08 01:28:58,661 iteration 2976 : loss : 0.024388, loss_ce: 0.009537
2022-01-08 01:29:00,152 iteration 2977 : loss : 0.023298, loss_ce: 0.008707
2022-01-08 01:29:01,516 iteration 2978 : loss : 0.028050, loss_ce: 0.011315
2022-01-08 01:29:02,955 iteration 2979 : loss : 0.026291, loss_ce: 0.006975
2022-01-08 01:29:04,474 iteration 2980 : loss : 0.038309, loss_ce: 0.015630
2022-01-08 01:29:05,824 iteration 2981 : loss : 0.022654, loss_ce: 0.009775
2022-01-08 01:29:07,190 iteration 2982 : loss : 0.030020, loss_ce: 0.010752
2022-01-08 01:29:08,587 iteration 2983 : loss : 0.026109, loss_ce: 0.007112
2022-01-08 01:29:09,972 iteration 2984 : loss : 0.034051, loss_ce: 0.018980
2022-01-08 01:29:11,341 iteration 2985 : loss : 0.021117, loss_ce: 0.009586
2022-01-08 01:29:12,698 iteration 2986 : loss : 0.035599, loss_ce: 0.015390
2022-01-08 01:29:14,042 iteration 2987 : loss : 0.027387, loss_ce: 0.009762
2022-01-08 01:29:15,439 iteration 2988 : loss : 0.024412, loss_ce: 0.009115
2022-01-08 01:29:16,781 iteration 2989 : loss : 0.017512, loss_ce: 0.006575
2022-01-08 01:29:18,147 iteration 2990 : loss : 0.028720, loss_ce: 0.012130
2022-01-08 01:29:19,469 iteration 2991 : loss : 0.026697, loss_ce: 0.008272
2022-01-08 01:29:20,855 iteration 2992 : loss : 0.021783, loss_ce: 0.009514
 44%|███████████▉               | 176/400 [1:14:44<1:36:28, 25.84s/it]2022-01-08 01:29:22,278 iteration 2993 : loss : 0.025838, loss_ce: 0.010221
2022-01-08 01:29:23,640 iteration 2994 : loss : 0.021328, loss_ce: 0.007842
2022-01-08 01:29:25,054 iteration 2995 : loss : 0.048409, loss_ce: 0.019406
2022-01-08 01:29:26,380 iteration 2996 : loss : 0.025033, loss_ce: 0.011006
2022-01-08 01:29:27,744 iteration 2997 : loss : 0.022497, loss_ce: 0.007535
2022-01-08 01:29:29,143 iteration 2998 : loss : 0.017918, loss_ce: 0.008834
2022-01-08 01:29:30,505 iteration 2999 : loss : 0.023121, loss_ce: 0.008031
2022-01-08 01:29:31,855 iteration 3000 : loss : 0.019350, loss_ce: 0.007492
2022-01-08 01:29:33,307 iteration 3001 : loss : 0.035570, loss_ce: 0.012592
2022-01-08 01:29:34,704 iteration 3002 : loss : 0.032854, loss_ce: 0.011929
2022-01-08 01:29:36,008 iteration 3003 : loss : 0.022632, loss_ce: 0.008382
2022-01-08 01:29:37,373 iteration 3004 : loss : 0.025591, loss_ce: 0.011861
2022-01-08 01:29:38,722 iteration 3005 : loss : 0.024345, loss_ce: 0.009853
2022-01-08 01:29:40,062 iteration 3006 : loss : 0.026486, loss_ce: 0.010585
2022-01-08 01:29:41,438 iteration 3007 : loss : 0.020371, loss_ce: 0.006695
2022-01-08 01:29:42,750 iteration 3008 : loss : 0.021161, loss_ce: 0.007429
2022-01-08 01:29:44,073 iteration 3009 : loss : 0.032925, loss_ce: 0.009562
 44%|███████████▉               | 177/400 [1:15:07<1:33:07, 25.06s/it]2022-01-08 01:29:45,550 iteration 3010 : loss : 0.041134, loss_ce: 0.014326
2022-01-08 01:29:46,895 iteration 3011 : loss : 0.026019, loss_ce: 0.008728
2022-01-08 01:29:48,225 iteration 3012 : loss : 0.023725, loss_ce: 0.007189
2022-01-08 01:29:49,689 iteration 3013 : loss : 0.031232, loss_ce: 0.009668
2022-01-08 01:29:51,089 iteration 3014 : loss : 0.034167, loss_ce: 0.014389
2022-01-08 01:29:52,471 iteration 3015 : loss : 0.022058, loss_ce: 0.006823
2022-01-08 01:29:53,770 iteration 3016 : loss : 0.019672, loss_ce: 0.006134
2022-01-08 01:29:55,199 iteration 3017 : loss : 0.027092, loss_ce: 0.013820
2022-01-08 01:29:56,656 iteration 3018 : loss : 0.025994, loss_ce: 0.013495
2022-01-08 01:29:58,149 iteration 3019 : loss : 0.038665, loss_ce: 0.014078
2022-01-08 01:29:59,478 iteration 3020 : loss : 0.018858, loss_ce: 0.006473
2022-01-08 01:30:00,836 iteration 3021 : loss : 0.024911, loss_ce: 0.009249
2022-01-08 01:30:02,133 iteration 3022 : loss : 0.017730, loss_ce: 0.008319
2022-01-08 01:30:03,475 iteration 3023 : loss : 0.022228, loss_ce: 0.009184
2022-01-08 01:30:04,854 iteration 3024 : loss : 0.027610, loss_ce: 0.011081
2022-01-08 01:30:06,220 iteration 3025 : loss : 0.023514, loss_ce: 0.009459
2022-01-08 01:30:07,543 iteration 3026 : loss : 0.022696, loss_ce: 0.006482
 44%|████████████               | 178/400 [1:15:30<1:30:56, 24.58s/it]2022-01-08 01:30:08,944 iteration 3027 : loss : 0.025573, loss_ce: 0.010552
2022-01-08 01:30:10,402 iteration 3028 : loss : 0.026885, loss_ce: 0.010334
2022-01-08 01:30:11,741 iteration 3029 : loss : 0.016595, loss_ce: 0.005650
2022-01-08 01:30:13,174 iteration 3030 : loss : 0.019325, loss_ce: 0.006110
2022-01-08 01:30:14,610 iteration 3031 : loss : 0.024193, loss_ce: 0.007872
2022-01-08 01:30:16,037 iteration 3032 : loss : 0.021917, loss_ce: 0.008370
2022-01-08 01:30:17,425 iteration 3033 : loss : 0.033712, loss_ce: 0.009927
2022-01-08 01:30:18,767 iteration 3034 : loss : 0.026954, loss_ce: 0.006448
2022-01-08 01:30:20,081 iteration 3035 : loss : 0.028154, loss_ce: 0.010619
2022-01-08 01:30:21,479 iteration 3036 : loss : 0.026640, loss_ce: 0.011714
2022-01-08 01:30:22,789 iteration 3037 : loss : 0.021786, loss_ce: 0.009060
2022-01-08 01:30:24,088 iteration 3038 : loss : 0.029114, loss_ce: 0.015818
2022-01-08 01:30:25,413 iteration 3039 : loss : 0.019683, loss_ce: 0.007638
2022-01-08 01:30:26,852 iteration 3040 : loss : 0.047015, loss_ce: 0.024600
2022-01-08 01:30:28,124 iteration 3041 : loss : 0.019396, loss_ce: 0.007494
2022-01-08 01:30:29,455 iteration 3042 : loss : 0.021892, loss_ce: 0.009934
2022-01-08 01:30:30,785 iteration 3043 : loss : 0.043648, loss_ce: 0.018592
 45%|████████████               | 179/400 [1:15:54<1:29:03, 24.18s/it]2022-01-08 01:30:32,220 iteration 3044 : loss : 0.024560, loss_ce: 0.010298
2022-01-08 01:30:33,512 iteration 3045 : loss : 0.032486, loss_ce: 0.010102
2022-01-08 01:30:34,970 iteration 3046 : loss : 0.047275, loss_ce: 0.028412
2022-01-08 01:30:36,423 iteration 3047 : loss : 0.020473, loss_ce: 0.006982
2022-01-08 01:30:37,805 iteration 3048 : loss : 0.024316, loss_ce: 0.008907
2022-01-08 01:30:39,206 iteration 3049 : loss : 0.029499, loss_ce: 0.008973
2022-01-08 01:30:40,643 iteration 3050 : loss : 0.034603, loss_ce: 0.007698
2022-01-08 01:30:42,106 iteration 3051 : loss : 0.019325, loss_ce: 0.007065
2022-01-08 01:30:43,496 iteration 3052 : loss : 0.031041, loss_ce: 0.014699
2022-01-08 01:30:44,785 iteration 3053 : loss : 0.015972, loss_ce: 0.005464
2022-01-08 01:30:46,125 iteration 3054 : loss : 0.022557, loss_ce: 0.009442
2022-01-08 01:30:47,424 iteration 3055 : loss : 0.026922, loss_ce: 0.011125
2022-01-08 01:30:48,715 iteration 3056 : loss : 0.018760, loss_ce: 0.005958
2022-01-08 01:30:50,204 iteration 3057 : loss : 0.032469, loss_ce: 0.014123
2022-01-08 01:30:51,532 iteration 3058 : loss : 0.020072, loss_ce: 0.007230
2022-01-08 01:30:52,868 iteration 3059 : loss : 0.018976, loss_ce: 0.009754
2022-01-08 01:30:52,869 Training Data Eval:
2022-01-08 01:30:59,753   Average segmentation loss on training set: 0.0153
2022-01-08 01:30:59,753 Validation Data Eval:
2022-01-08 01:31:02,126   Average segmentation loss on validation set: 0.0682
2022-01-08 01:31:03,545 iteration 3060 : loss : 0.032899, loss_ce: 0.011570
 45%|████████████▏              | 180/400 [1:16:26<1:38:05, 26.75s/it]2022-01-08 01:31:04,960 iteration 3061 : loss : 0.020539, loss_ce: 0.006448
2022-01-08 01:31:06,370 iteration 3062 : loss : 0.036759, loss_ce: 0.017029
2022-01-08 01:31:07,728 iteration 3063 : loss : 0.019539, loss_ce: 0.008296
2022-01-08 01:31:09,104 iteration 3064 : loss : 0.021731, loss_ce: 0.010494
2022-01-08 01:31:10,517 iteration 3065 : loss : 0.024865, loss_ce: 0.006880
2022-01-08 01:31:11,852 iteration 3066 : loss : 0.021096, loss_ce: 0.006890
2022-01-08 01:31:13,285 iteration 3067 : loss : 0.031156, loss_ce: 0.013243
2022-01-08 01:31:14,725 iteration 3068 : loss : 0.025351, loss_ce: 0.008778
2022-01-08 01:31:16,037 iteration 3069 : loss : 0.012987, loss_ce: 0.004882
2022-01-08 01:31:17,413 iteration 3070 : loss : 0.023749, loss_ce: 0.008454
2022-01-08 01:31:18,915 iteration 3071 : loss : 0.032794, loss_ce: 0.010485
2022-01-08 01:31:20,266 iteration 3072 : loss : 0.020029, loss_ce: 0.008018
2022-01-08 01:31:21,614 iteration 3073 : loss : 0.024287, loss_ce: 0.008010
2022-01-08 01:31:22,973 iteration 3074 : loss : 0.030304, loss_ce: 0.013013
2022-01-08 01:31:24,367 iteration 3075 : loss : 0.026469, loss_ce: 0.008396
2022-01-08 01:31:25,689 iteration 3076 : loss : 0.019952, loss_ce: 0.008093
2022-01-08 01:31:27,135 iteration 3077 : loss : 0.031522, loss_ce: 0.011615
 45%|████████████▏              | 181/400 [1:16:50<1:34:10, 25.80s/it]2022-01-08 01:31:28,583 iteration 3078 : loss : 0.046233, loss_ce: 0.008541
2022-01-08 01:31:29,930 iteration 3079 : loss : 0.019366, loss_ce: 0.005766
2022-01-08 01:31:31,281 iteration 3080 : loss : 0.019482, loss_ce: 0.006229
2022-01-08 01:31:32,658 iteration 3081 : loss : 0.026196, loss_ce: 0.010922
2022-01-08 01:31:34,035 iteration 3082 : loss : 0.027097, loss_ce: 0.009412
2022-01-08 01:31:35,405 iteration 3083 : loss : 0.026831, loss_ce: 0.008460
2022-01-08 01:31:36,790 iteration 3084 : loss : 0.031466, loss_ce: 0.012259
2022-01-08 01:31:38,093 iteration 3085 : loss : 0.024769, loss_ce: 0.009563
2022-01-08 01:31:39,441 iteration 3086 : loss : 0.025282, loss_ce: 0.014551
2022-01-08 01:31:40,850 iteration 3087 : loss : 0.030016, loss_ce: 0.014376
2022-01-08 01:31:42,219 iteration 3088 : loss : 0.022331, loss_ce: 0.010641
2022-01-08 01:31:43,669 iteration 3089 : loss : 0.060441, loss_ce: 0.026933
2022-01-08 01:31:44,970 iteration 3090 : loss : 0.020732, loss_ce: 0.008388
2022-01-08 01:31:46,382 iteration 3091 : loss : 0.028928, loss_ce: 0.012177
2022-01-08 01:31:47,713 iteration 3092 : loss : 0.021838, loss_ce: 0.008866
2022-01-08 01:31:49,038 iteration 3093 : loss : 0.019521, loss_ce: 0.008192
2022-01-08 01:31:50,399 iteration 3094 : loss : 0.034691, loss_ce: 0.008206
 46%|████████████▎              | 182/400 [1:17:13<1:30:59, 25.04s/it]2022-01-08 01:31:51,838 iteration 3095 : loss : 0.018740, loss_ce: 0.007775
2022-01-08 01:31:53,206 iteration 3096 : loss : 0.022749, loss_ce: 0.009015
2022-01-08 01:31:54,591 iteration 3097 : loss : 0.022364, loss_ce: 0.010204
2022-01-08 01:31:55,993 iteration 3098 : loss : 0.035902, loss_ce: 0.009805
2022-01-08 01:31:57,414 iteration 3099 : loss : 0.029807, loss_ce: 0.014091
2022-01-08 01:31:58,800 iteration 3100 : loss : 0.029210, loss_ce: 0.012211
2022-01-08 01:32:00,120 iteration 3101 : loss : 0.016340, loss_ce: 0.006112
2022-01-08 01:32:01,409 iteration 3102 : loss : 0.021301, loss_ce: 0.007777
2022-01-08 01:32:02,813 iteration 3103 : loss : 0.035887, loss_ce: 0.014289
2022-01-08 01:32:04,177 iteration 3104 : loss : 0.020392, loss_ce: 0.006535
2022-01-08 01:32:05,590 iteration 3105 : loss : 0.020598, loss_ce: 0.007973
2022-01-08 01:32:06,965 iteration 3106 : loss : 0.027764, loss_ce: 0.010276
2022-01-08 01:32:08,391 iteration 3107 : loss : 0.039423, loss_ce: 0.013339
2022-01-08 01:32:09,792 iteration 3108 : loss : 0.027104, loss_ce: 0.010216
2022-01-08 01:32:11,093 iteration 3109 : loss : 0.016673, loss_ce: 0.007755
2022-01-08 01:32:12,432 iteration 3110 : loss : 0.025696, loss_ce: 0.009061
2022-01-08 01:32:13,807 iteration 3111 : loss : 0.027854, loss_ce: 0.006807
 46%|████████████▎              | 183/400 [1:17:37<1:28:48, 24.55s/it]2022-01-08 01:32:15,171 iteration 3112 : loss : 0.017541, loss_ce: 0.006847
2022-01-08 01:32:16,565 iteration 3113 : loss : 0.040738, loss_ce: 0.014496
2022-01-08 01:32:17,898 iteration 3114 : loss : 0.020275, loss_ce: 0.009472
2022-01-08 01:32:19,289 iteration 3115 : loss : 0.033428, loss_ce: 0.009401
2022-01-08 01:32:20,578 iteration 3116 : loss : 0.021707, loss_ce: 0.008729
2022-01-08 01:32:22,003 iteration 3117 : loss : 0.024799, loss_ce: 0.006950
2022-01-08 01:32:23,390 iteration 3118 : loss : 0.029078, loss_ce: 0.014403
2022-01-08 01:32:24,773 iteration 3119 : loss : 0.018153, loss_ce: 0.008090
2022-01-08 01:32:26,094 iteration 3120 : loss : 0.020961, loss_ce: 0.006317
2022-01-08 01:32:27,511 iteration 3121 : loss : 0.043832, loss_ce: 0.011299
2022-01-08 01:32:28,868 iteration 3122 : loss : 0.040179, loss_ce: 0.017991
2022-01-08 01:32:30,338 iteration 3123 : loss : 0.045612, loss_ce: 0.015070
2022-01-08 01:32:31,663 iteration 3124 : loss : 0.020953, loss_ce: 0.007779
2022-01-08 01:32:33,004 iteration 3125 : loss : 0.014423, loss_ce: 0.006216
2022-01-08 01:32:34,336 iteration 3126 : loss : 0.024040, loss_ce: 0.010494
2022-01-08 01:32:35,680 iteration 3127 : loss : 0.023025, loss_ce: 0.009602
2022-01-08 01:32:37,024 iteration 3128 : loss : 0.056924, loss_ce: 0.016276
 46%|████████████▍              | 184/400 [1:18:00<1:26:56, 24.15s/it]2022-01-08 01:32:38,396 iteration 3129 : loss : 0.027455, loss_ce: 0.008774
2022-01-08 01:32:39,726 iteration 3130 : loss : 0.026286, loss_ce: 0.012200
2022-01-08 01:32:41,141 iteration 3131 : loss : 0.029671, loss_ce: 0.015565
2022-01-08 01:32:42,520 iteration 3132 : loss : 0.024349, loss_ce: 0.008942
2022-01-08 01:32:43,845 iteration 3133 : loss : 0.018375, loss_ce: 0.008316
2022-01-08 01:32:45,245 iteration 3134 : loss : 0.023213, loss_ce: 0.009599
2022-01-08 01:32:46,653 iteration 3135 : loss : 0.043290, loss_ce: 0.010561
2022-01-08 01:32:47,986 iteration 3136 : loss : 0.020841, loss_ce: 0.009754
2022-01-08 01:32:49,313 iteration 3137 : loss : 0.021754, loss_ce: 0.009478
2022-01-08 01:32:50,664 iteration 3138 : loss : 0.022538, loss_ce: 0.008247
2022-01-08 01:32:52,170 iteration 3139 : loss : 0.049937, loss_ce: 0.015765
2022-01-08 01:32:53,566 iteration 3140 : loss : 0.029123, loss_ce: 0.011791
2022-01-08 01:32:54,919 iteration 3141 : loss : 0.023531, loss_ce: 0.009620
2022-01-08 01:32:56,228 iteration 3142 : loss : 0.021870, loss_ce: 0.008369
2022-01-08 01:32:57,629 iteration 3143 : loss : 0.039726, loss_ce: 0.014922
2022-01-08 01:32:59,045 iteration 3144 : loss : 0.043736, loss_ce: 0.011261
2022-01-08 01:32:59,045 Training Data Eval:
2022-01-08 01:33:05,914   Average segmentation loss on training set: 0.0186
2022-01-08 01:33:05,914 Validation Data Eval:
2022-01-08 01:33:08,286   Average segmentation loss on validation set: 0.0743
2022-01-08 01:33:09,609 iteration 3145 : loss : 0.030189, loss_ce: 0.014778
 46%|████████████▍              | 185/400 [1:18:32<1:35:36, 26.68s/it]2022-01-08 01:33:11,107 iteration 3146 : loss : 0.026431, loss_ce: 0.011327
2022-01-08 01:33:12,526 iteration 3147 : loss : 0.039173, loss_ce: 0.015522
2022-01-08 01:33:13,842 iteration 3148 : loss : 0.023077, loss_ce: 0.008948
2022-01-08 01:33:15,235 iteration 3149 : loss : 0.041054, loss_ce: 0.011314
2022-01-08 01:33:16,609 iteration 3150 : loss : 0.036364, loss_ce: 0.010343
2022-01-08 01:33:17,900 iteration 3151 : loss : 0.019755, loss_ce: 0.008715
2022-01-08 01:33:19,318 iteration 3152 : loss : 0.020763, loss_ce: 0.010549
2022-01-08 01:33:20,660 iteration 3153 : loss : 0.031217, loss_ce: 0.014007
2022-01-08 01:33:22,030 iteration 3154 : loss : 0.031074, loss_ce: 0.008327
2022-01-08 01:33:23,412 iteration 3155 : loss : 0.017740, loss_ce: 0.005959
2022-01-08 01:33:24,801 iteration 3156 : loss : 0.021573, loss_ce: 0.007933
2022-01-08 01:33:26,106 iteration 3157 : loss : 0.040331, loss_ce: 0.012331
2022-01-08 01:33:27,546 iteration 3158 : loss : 0.022124, loss_ce: 0.008693
2022-01-08 01:33:28,910 iteration 3159 : loss : 0.024162, loss_ce: 0.010411
2022-01-08 01:33:30,261 iteration 3160 : loss : 0.020954, loss_ce: 0.009699
2022-01-08 01:33:31,702 iteration 3161 : loss : 0.030605, loss_ce: 0.011202
2022-01-08 01:33:33,160 iteration 3162 : loss : 0.027493, loss_ce: 0.013014
 46%|████████████▌              | 186/400 [1:18:56<1:31:48, 25.74s/it]2022-01-08 01:33:34,514 iteration 3163 : loss : 0.026794, loss_ce: 0.008524
2022-01-08 01:33:35,837 iteration 3164 : loss : 0.028664, loss_ce: 0.008415
2022-01-08 01:33:37,185 iteration 3165 : loss : 0.014740, loss_ce: 0.006126
2022-01-08 01:33:38,551 iteration 3166 : loss : 0.026469, loss_ce: 0.012464
2022-01-08 01:33:39,947 iteration 3167 : loss : 0.043214, loss_ce: 0.014833
2022-01-08 01:33:41,264 iteration 3168 : loss : 0.019443, loss_ce: 0.009910
2022-01-08 01:33:42,637 iteration 3169 : loss : 0.018028, loss_ce: 0.006888
2022-01-08 01:33:44,022 iteration 3170 : loss : 0.034265, loss_ce: 0.014721
2022-01-08 01:33:45,445 iteration 3171 : loss : 0.035084, loss_ce: 0.009492
2022-01-08 01:33:46,847 iteration 3172 : loss : 0.026820, loss_ce: 0.010478
2022-01-08 01:33:48,240 iteration 3173 : loss : 0.032306, loss_ce: 0.014629
2022-01-08 01:33:49,662 iteration 3174 : loss : 0.020206, loss_ce: 0.006208
2022-01-08 01:33:51,033 iteration 3175 : loss : 0.019452, loss_ce: 0.007790
2022-01-08 01:33:52,424 iteration 3176 : loss : 0.028328, loss_ce: 0.016300
2022-01-08 01:33:53,754 iteration 3177 : loss : 0.020610, loss_ce: 0.009061
2022-01-08 01:33:55,134 iteration 3178 : loss : 0.029693, loss_ce: 0.007449
2022-01-08 01:33:56,535 iteration 3179 : loss : 0.031368, loss_ce: 0.008536
 47%|████████████▌              | 187/400 [1:19:19<1:28:52, 25.03s/it]2022-01-08 01:33:58,002 iteration 3180 : loss : 0.024483, loss_ce: 0.008825
2022-01-08 01:33:59,367 iteration 3181 : loss : 0.027681, loss_ce: 0.013429
2022-01-08 01:34:00,772 iteration 3182 : loss : 0.046163, loss_ce: 0.015869
2022-01-08 01:34:02,184 iteration 3183 : loss : 0.026863, loss_ce: 0.010629
2022-01-08 01:34:03,473 iteration 3184 : loss : 0.020060, loss_ce: 0.008884
2022-01-08 01:34:04,810 iteration 3185 : loss : 0.025948, loss_ce: 0.010610
2022-01-08 01:34:06,172 iteration 3186 : loss : 0.027746, loss_ce: 0.017386
2022-01-08 01:34:07,502 iteration 3187 : loss : 0.018033, loss_ce: 0.005852
2022-01-08 01:34:08,878 iteration 3188 : loss : 0.025346, loss_ce: 0.008747
2022-01-08 01:34:10,192 iteration 3189 : loss : 0.016578, loss_ce: 0.007112
2022-01-08 01:34:11,603 iteration 3190 : loss : 0.032040, loss_ce: 0.011581
2022-01-08 01:34:12,998 iteration 3191 : loss : 0.022545, loss_ce: 0.007312
2022-01-08 01:34:14,354 iteration 3192 : loss : 0.019974, loss_ce: 0.008445
2022-01-08 01:34:15,678 iteration 3193 : loss : 0.020692, loss_ce: 0.008946
2022-01-08 01:34:16,982 iteration 3194 : loss : 0.021335, loss_ce: 0.007108
2022-01-08 01:34:18,346 iteration 3195 : loss : 0.024802, loss_ce: 0.007625
2022-01-08 01:34:19,752 iteration 3196 : loss : 0.018259, loss_ce: 0.005069
 47%|████████████▋              | 188/400 [1:19:43<1:26:31, 24.49s/it]2022-01-08 01:34:21,134 iteration 3197 : loss : 0.021600, loss_ce: 0.007026
2022-01-08 01:34:22,448 iteration 3198 : loss : 0.025123, loss_ce: 0.009292
2022-01-08 01:34:23,755 iteration 3199 : loss : 0.017599, loss_ce: 0.006610
2022-01-08 01:34:25,100 iteration 3200 : loss : 0.022144, loss_ce: 0.009233
2022-01-08 01:34:26,512 iteration 3201 : loss : 0.019688, loss_ce: 0.006923
2022-01-08 01:34:27,905 iteration 3202 : loss : 0.036587, loss_ce: 0.016585
2022-01-08 01:34:29,192 iteration 3203 : loss : 0.021305, loss_ce: 0.003161
2022-01-08 01:34:30,585 iteration 3204 : loss : 0.026261, loss_ce: 0.013139
2022-01-08 01:34:31,981 iteration 3205 : loss : 0.026419, loss_ce: 0.008122
2022-01-08 01:34:33,302 iteration 3206 : loss : 0.022560, loss_ce: 0.008786
2022-01-08 01:34:34,615 iteration 3207 : loss : 0.016657, loss_ce: 0.005435
2022-01-08 01:34:35,955 iteration 3208 : loss : 0.023608, loss_ce: 0.008372
2022-01-08 01:34:37,363 iteration 3209 : loss : 0.030394, loss_ce: 0.011375
2022-01-08 01:34:38,738 iteration 3210 : loss : 0.036113, loss_ce: 0.012249
2022-01-08 01:34:40,139 iteration 3211 : loss : 0.026055, loss_ce: 0.012383
2022-01-08 01:34:41,524 iteration 3212 : loss : 0.022577, loss_ce: 0.009621
2022-01-08 01:34:42,873 iteration 3213 : loss : 0.021030, loss_ce: 0.010099
 47%|████████████▊              | 189/400 [1:20:06<1:24:40, 24.08s/it]2022-01-08 01:34:44,258 iteration 3214 : loss : 0.023490, loss_ce: 0.008490
2022-01-08 01:34:45,598 iteration 3215 : loss : 0.015185, loss_ce: 0.006759
2022-01-08 01:34:46,956 iteration 3216 : loss : 0.036408, loss_ce: 0.019595
2022-01-08 01:34:48,357 iteration 3217 : loss : 0.031999, loss_ce: 0.008801
2022-01-08 01:34:49,675 iteration 3218 : loss : 0.017609, loss_ce: 0.006734
2022-01-08 01:34:50,996 iteration 3219 : loss : 0.019074, loss_ce: 0.006179
2022-01-08 01:34:52,374 iteration 3220 : loss : 0.027637, loss_ce: 0.011999
2022-01-08 01:34:53,772 iteration 3221 : loss : 0.031511, loss_ce: 0.010447
2022-01-08 01:34:55,096 iteration 3222 : loss : 0.017744, loss_ce: 0.006221
2022-01-08 01:34:56,490 iteration 3223 : loss : 0.034159, loss_ce: 0.007000
2022-01-08 01:34:57,846 iteration 3224 : loss : 0.022403, loss_ce: 0.009757
2022-01-08 01:34:59,143 iteration 3225 : loss : 0.024870, loss_ce: 0.009011
2022-01-08 01:35:00,504 iteration 3226 : loss : 0.043498, loss_ce: 0.019309
2022-01-08 01:35:01,884 iteration 3227 : loss : 0.025871, loss_ce: 0.008904
2022-01-08 01:35:03,246 iteration 3228 : loss : 0.030800, loss_ce: 0.013022
2022-01-08 01:35:04,635 iteration 3229 : loss : 0.020997, loss_ce: 0.007650
2022-01-08 01:35:04,636 Training Data Eval:
2022-01-08 01:35:11,510   Average segmentation loss on training set: 0.0159
2022-01-08 01:35:11,510 Validation Data Eval:
2022-01-08 01:35:13,889   Average segmentation loss on validation set: 0.0834
2022-01-08 01:35:15,244 iteration 3230 : loss : 0.018331, loss_ce: 0.009433
 48%|████████████▊              | 190/400 [1:20:38<1:32:58, 26.56s/it]2022-01-08 01:35:16,565 iteration 3231 : loss : 0.018761, loss_ce: 0.007018
2022-01-08 01:35:17,952 iteration 3232 : loss : 0.018784, loss_ce: 0.007306
2022-01-08 01:35:19,328 iteration 3233 : loss : 0.028077, loss_ce: 0.007334
2022-01-08 01:35:20,645 iteration 3234 : loss : 0.022673, loss_ce: 0.007887
2022-01-08 01:35:22,035 iteration 3235 : loss : 0.021674, loss_ce: 0.008518
2022-01-08 01:35:23,298 iteration 3236 : loss : 0.021350, loss_ce: 0.010244
2022-01-08 01:35:24,653 iteration 3237 : loss : 0.017114, loss_ce: 0.006683
2022-01-08 01:35:26,019 iteration 3238 : loss : 0.031485, loss_ce: 0.012566
2022-01-08 01:35:27,312 iteration 3239 : loss : 0.017526, loss_ce: 0.007284
2022-01-08 01:35:28,714 iteration 3240 : loss : 0.023485, loss_ce: 0.009756
2022-01-08 01:35:30,169 iteration 3241 : loss : 0.030555, loss_ce: 0.011506
2022-01-08 01:35:31,547 iteration 3242 : loss : 0.020095, loss_ce: 0.007150
2022-01-08 01:35:32,952 iteration 3243 : loss : 0.028065, loss_ce: 0.012094
2022-01-08 01:35:34,273 iteration 3244 : loss : 0.021197, loss_ce: 0.007474
2022-01-08 01:35:35,654 iteration 3245 : loss : 0.023472, loss_ce: 0.007237
2022-01-08 01:35:37,057 iteration 3246 : loss : 0.034059, loss_ce: 0.011258
2022-01-08 01:35:38,519 iteration 3247 : loss : 0.037815, loss_ce: 0.014712
 48%|████████████▉              | 191/400 [1:21:01<1:29:05, 25.58s/it]2022-01-08 01:35:39,965 iteration 3248 : loss : 0.035187, loss_ce: 0.010609
2022-01-08 01:35:41,362 iteration 3249 : loss : 0.026661, loss_ce: 0.008842
2022-01-08 01:35:42,741 iteration 3250 : loss : 0.029917, loss_ce: 0.011327
2022-01-08 01:35:44,128 iteration 3251 : loss : 0.022370, loss_ce: 0.008842
2022-01-08 01:35:45,407 iteration 3252 : loss : 0.015704, loss_ce: 0.007039
2022-01-08 01:35:46,782 iteration 3253 : loss : 0.024668, loss_ce: 0.008518
2022-01-08 01:35:48,211 iteration 3254 : loss : 0.020829, loss_ce: 0.009477
2022-01-08 01:35:49,577 iteration 3255 : loss : 0.024099, loss_ce: 0.009991
2022-01-08 01:35:50,978 iteration 3256 : loss : 0.021317, loss_ce: 0.008550
2022-01-08 01:35:52,347 iteration 3257 : loss : 0.031603, loss_ce: 0.012708
2022-01-08 01:35:53,658 iteration 3258 : loss : 0.018155, loss_ce: 0.004820
2022-01-08 01:35:55,027 iteration 3259 : loss : 0.028399, loss_ce: 0.010033
2022-01-08 01:35:56,420 iteration 3260 : loss : 0.023951, loss_ce: 0.009060
2022-01-08 01:35:57,780 iteration 3261 : loss : 0.024872, loss_ce: 0.009813
2022-01-08 01:35:59,227 iteration 3262 : loss : 0.042750, loss_ce: 0.014253
2022-01-08 01:36:00,562 iteration 3263 : loss : 0.030057, loss_ce: 0.009890
2022-01-08 01:36:01,990 iteration 3264 : loss : 0.042485, loss_ce: 0.017834
 48%|████████████▉              | 192/400 [1:21:25<1:26:28, 24.95s/it]2022-01-08 01:36:03,368 iteration 3265 : loss : 0.030209, loss_ce: 0.011979
2022-01-08 01:36:04,766 iteration 3266 : loss : 0.020336, loss_ce: 0.006713
2022-01-08 01:36:06,132 iteration 3267 : loss : 0.016641, loss_ce: 0.005519
2022-01-08 01:36:07,500 iteration 3268 : loss : 0.027430, loss_ce: 0.009849
2022-01-08 01:36:08,826 iteration 3269 : loss : 0.019238, loss_ce: 0.009337
2022-01-08 01:36:10,201 iteration 3270 : loss : 0.019781, loss_ce: 0.007398
2022-01-08 01:36:11,499 iteration 3271 : loss : 0.028512, loss_ce: 0.007949
2022-01-08 01:36:12,967 iteration 3272 : loss : 0.026259, loss_ce: 0.010997
2022-01-08 01:36:14,315 iteration 3273 : loss : 0.022192, loss_ce: 0.009705
2022-01-08 01:36:15,614 iteration 3274 : loss : 0.030248, loss_ce: 0.012414
2022-01-08 01:36:16,957 iteration 3275 : loss : 0.031120, loss_ce: 0.009940
2022-01-08 01:36:18,303 iteration 3276 : loss : 0.024168, loss_ce: 0.006667
2022-01-08 01:36:19,597 iteration 3277 : loss : 0.017871, loss_ce: 0.006539
2022-01-08 01:36:20,948 iteration 3278 : loss : 0.017383, loss_ce: 0.007104
2022-01-08 01:36:22,220 iteration 3279 : loss : 0.022992, loss_ce: 0.013174
2022-01-08 01:36:23,648 iteration 3280 : loss : 0.033395, loss_ce: 0.014023
2022-01-08 01:36:24,996 iteration 3281 : loss : 0.016895, loss_ce: 0.004486
 48%|█████████████              | 193/400 [1:21:48<1:24:02, 24.36s/it]2022-01-08 01:36:26,439 iteration 3282 : loss : 0.026499, loss_ce: 0.005852
2022-01-08 01:36:27,832 iteration 3283 : loss : 0.032956, loss_ce: 0.014497
2022-01-08 01:36:29,214 iteration 3284 : loss : 0.025138, loss_ce: 0.011590
2022-01-08 01:36:30,609 iteration 3285 : loss : 0.033330, loss_ce: 0.010174
2022-01-08 01:36:31,965 iteration 3286 : loss : 0.030855, loss_ce: 0.008795
2022-01-08 01:36:33,410 iteration 3287 : loss : 0.036059, loss_ce: 0.015172
2022-01-08 01:36:34,716 iteration 3288 : loss : 0.020007, loss_ce: 0.008002
2022-01-08 01:36:36,174 iteration 3289 : loss : 0.021774, loss_ce: 0.009075
2022-01-08 01:36:37,617 iteration 3290 : loss : 0.029979, loss_ce: 0.012093
2022-01-08 01:36:39,058 iteration 3291 : loss : 0.026217, loss_ce: 0.011970
2022-01-08 01:36:40,419 iteration 3292 : loss : 0.037766, loss_ce: 0.012806
2022-01-08 01:36:41,775 iteration 3293 : loss : 0.028546, loss_ce: 0.008450
2022-01-08 01:36:43,232 iteration 3294 : loss : 0.024805, loss_ce: 0.010929
2022-01-08 01:36:44,629 iteration 3295 : loss : 0.018441, loss_ce: 0.008202
2022-01-08 01:36:45,936 iteration 3296 : loss : 0.026543, loss_ce: 0.010734
2022-01-08 01:36:47,271 iteration 3297 : loss : 0.023228, loss_ce: 0.008296
2022-01-08 01:36:48,740 iteration 3298 : loss : 0.020641, loss_ce: 0.008445
 48%|█████████████              | 194/400 [1:22:12<1:23:00, 24.18s/it]2022-01-08 01:36:50,077 iteration 3299 : loss : 0.021952, loss_ce: 0.005223
2022-01-08 01:36:51,487 iteration 3300 : loss : 0.036220, loss_ce: 0.013722
2022-01-08 01:36:52,845 iteration 3301 : loss : 0.021202, loss_ce: 0.009055
2022-01-08 01:36:54,186 iteration 3302 : loss : 0.028733, loss_ce: 0.009428
2022-01-08 01:36:55,616 iteration 3303 : loss : 0.024604, loss_ce: 0.011870
2022-01-08 01:36:57,032 iteration 3304 : loss : 0.021390, loss_ce: 0.008457
2022-01-08 01:36:58,377 iteration 3305 : loss : 0.029053, loss_ce: 0.011047
2022-01-08 01:36:59,743 iteration 3306 : loss : 0.023661, loss_ce: 0.006291
2022-01-08 01:37:01,234 iteration 3307 : loss : 0.028541, loss_ce: 0.014452
2022-01-08 01:37:02,676 iteration 3308 : loss : 0.051277, loss_ce: 0.025069
2022-01-08 01:37:04,098 iteration 3309 : loss : 0.023518, loss_ce: 0.011286
2022-01-08 01:37:05,414 iteration 3310 : loss : 0.019344, loss_ce: 0.010188
2022-01-08 01:37:06,852 iteration 3311 : loss : 0.042695, loss_ce: 0.016628
2022-01-08 01:37:08,214 iteration 3312 : loss : 0.034979, loss_ce: 0.008253
2022-01-08 01:37:09,662 iteration 3313 : loss : 0.030545, loss_ce: 0.012054
2022-01-08 01:37:11,039 iteration 3314 : loss : 0.019632, loss_ce: 0.007268
2022-01-08 01:37:11,039 Training Data Eval:
2022-01-08 01:37:17,944   Average segmentation loss on training set: 0.0167
2022-01-08 01:37:17,945 Validation Data Eval:
2022-01-08 01:37:20,315   Average segmentation loss on validation set: 0.1005
2022-01-08 01:37:21,642 iteration 3315 : loss : 0.019356, loss_ce: 0.007793
 49%|█████████████▏             | 195/400 [1:22:44<1:31:33, 26.80s/it]2022-01-08 01:37:23,020 iteration 3316 : loss : 0.022626, loss_ce: 0.009829
2022-01-08 01:37:24,403 iteration 3317 : loss : 0.025160, loss_ce: 0.011794
2022-01-08 01:37:25,749 iteration 3318 : loss : 0.034296, loss_ce: 0.014169
2022-01-08 01:37:27,065 iteration 3319 : loss : 0.020041, loss_ce: 0.007542
2022-01-08 01:37:28,541 iteration 3320 : loss : 0.038516, loss_ce: 0.015023
2022-01-08 01:37:29,887 iteration 3321 : loss : 0.021052, loss_ce: 0.007415
2022-01-08 01:37:31,304 iteration 3322 : loss : 0.029062, loss_ce: 0.007845
2022-01-08 01:37:32,624 iteration 3323 : loss : 0.018808, loss_ce: 0.006126
2022-01-08 01:37:33,984 iteration 3324 : loss : 0.024221, loss_ce: 0.011235
2022-01-08 01:37:35,383 iteration 3325 : loss : 0.042147, loss_ce: 0.018923
2022-01-08 01:37:36,707 iteration 3326 : loss : 0.026807, loss_ce: 0.009451
2022-01-08 01:37:38,066 iteration 3327 : loss : 0.022182, loss_ce: 0.008420
2022-01-08 01:37:39,498 iteration 3328 : loss : 0.019160, loss_ce: 0.006552
2022-01-08 01:37:40,866 iteration 3329 : loss : 0.022311, loss_ce: 0.007690
2022-01-08 01:37:42,193 iteration 3330 : loss : 0.019642, loss_ce: 0.009668
2022-01-08 01:37:43,489 iteration 3331 : loss : 0.023740, loss_ce: 0.010295
2022-01-08 01:37:44,893 iteration 3332 : loss : 0.021579, loss_ce: 0.007138
 49%|█████████████▏             | 196/400 [1:23:08<1:27:29, 25.73s/it]2022-01-08 01:37:46,339 iteration 3333 : loss : 0.023340, loss_ce: 0.008533
2022-01-08 01:37:47,727 iteration 3334 : loss : 0.029764, loss_ce: 0.013888
2022-01-08 01:37:49,185 iteration 3335 : loss : 0.024435, loss_ce: 0.009918
2022-01-08 01:37:50,559 iteration 3336 : loss : 0.026375, loss_ce: 0.008096
2022-01-08 01:37:51,963 iteration 3337 : loss : 0.035230, loss_ce: 0.014885
2022-01-08 01:37:53,426 iteration 3338 : loss : 0.026414, loss_ce: 0.013347
2022-01-08 01:37:54,741 iteration 3339 : loss : 0.023093, loss_ce: 0.010626
2022-01-08 01:37:56,039 iteration 3340 : loss : 0.024556, loss_ce: 0.006191
2022-01-08 01:37:57,400 iteration 3341 : loss : 0.027282, loss_ce: 0.008802
2022-01-08 01:37:58,740 iteration 3342 : loss : 0.018619, loss_ce: 0.008855
2022-01-08 01:38:00,146 iteration 3343 : loss : 0.021608, loss_ce: 0.009117
2022-01-08 01:38:01,522 iteration 3344 : loss : 0.031854, loss_ce: 0.011002
2022-01-08 01:38:02,901 iteration 3345 : loss : 0.020203, loss_ce: 0.008888
2022-01-08 01:38:04,226 iteration 3346 : loss : 0.019891, loss_ce: 0.009882
2022-01-08 01:38:05,545 iteration 3347 : loss : 0.018595, loss_ce: 0.006879
2022-01-08 01:38:06,837 iteration 3348 : loss : 0.019588, loss_ce: 0.007554
2022-01-08 01:38:08,269 iteration 3349 : loss : 0.021213, loss_ce: 0.006192
 49%|█████████████▎             | 197/400 [1:23:31<1:24:40, 25.03s/it]2022-01-08 01:38:09,682 iteration 3350 : loss : 0.020884, loss_ce: 0.007294
2022-01-08 01:38:11,094 iteration 3351 : loss : 0.022377, loss_ce: 0.007179
2022-01-08 01:38:12,494 iteration 3352 : loss : 0.038771, loss_ce: 0.018629
2022-01-08 01:38:13,940 iteration 3353 : loss : 0.019515, loss_ce: 0.006476
2022-01-08 01:38:15,287 iteration 3354 : loss : 0.018940, loss_ce: 0.008567
2022-01-08 01:38:16,727 iteration 3355 : loss : 0.020262, loss_ce: 0.008339
2022-01-08 01:38:18,146 iteration 3356 : loss : 0.022094, loss_ce: 0.008856
2022-01-08 01:38:19,519 iteration 3357 : loss : 0.026030, loss_ce: 0.006931
2022-01-08 01:38:20,948 iteration 3358 : loss : 0.017661, loss_ce: 0.007466
2022-01-08 01:38:22,235 iteration 3359 : loss : 0.020234, loss_ce: 0.006956
2022-01-08 01:38:23,570 iteration 3360 : loss : 0.020988, loss_ce: 0.007489
2022-01-08 01:38:24,941 iteration 3361 : loss : 0.016797, loss_ce: 0.006397
2022-01-08 01:38:26,301 iteration 3362 : loss : 0.021771, loss_ce: 0.008422
2022-01-08 01:38:27,739 iteration 3363 : loss : 0.020798, loss_ce: 0.005838
2022-01-08 01:38:29,047 iteration 3364 : loss : 0.017670, loss_ce: 0.007885
2022-01-08 01:38:30,454 iteration 3365 : loss : 0.032392, loss_ce: 0.012393
2022-01-08 01:38:31,750 iteration 3366 : loss : 0.021117, loss_ce: 0.009891
 50%|█████████████▎             | 198/400 [1:23:55<1:22:41, 24.56s/it]2022-01-08 01:38:33,161 iteration 3367 : loss : 0.015517, loss_ce: 0.005988
2022-01-08 01:38:34,512 iteration 3368 : loss : 0.026721, loss_ce: 0.013499
2022-01-08 01:38:35,811 iteration 3369 : loss : 0.019321, loss_ce: 0.006467
2022-01-08 01:38:37,236 iteration 3370 : loss : 0.022055, loss_ce: 0.010478
2022-01-08 01:38:38,716 iteration 3371 : loss : 0.036153, loss_ce: 0.013486
2022-01-08 01:38:40,031 iteration 3372 : loss : 0.016559, loss_ce: 0.006468
2022-01-08 01:38:41,399 iteration 3373 : loss : 0.022521, loss_ce: 0.007722
2022-01-08 01:38:42,722 iteration 3374 : loss : 0.016012, loss_ce: 0.005313
2022-01-08 01:38:44,055 iteration 3375 : loss : 0.020487, loss_ce: 0.007975
2022-01-08 01:38:45,435 iteration 3376 : loss : 0.020546, loss_ce: 0.006034
2022-01-08 01:38:46,703 iteration 3377 : loss : 0.013338, loss_ce: 0.005207
2022-01-08 01:38:48,097 iteration 3378 : loss : 0.030243, loss_ce: 0.006883
2022-01-08 01:38:49,556 iteration 3379 : loss : 0.029671, loss_ce: 0.013896
2022-01-08 01:38:50,955 iteration 3380 : loss : 0.024055, loss_ce: 0.007271
2022-01-08 01:38:52,351 iteration 3381 : loss : 0.033529, loss_ce: 0.012533
2022-01-08 01:38:53,733 iteration 3382 : loss : 0.023009, loss_ce: 0.009839
2022-01-08 01:38:55,065 iteration 3383 : loss : 0.015882, loss_ce: 0.006272
 50%|█████████████▍             | 199/400 [1:24:18<1:21:01, 24.19s/it]2022-01-08 01:38:56,525 iteration 3384 : loss : 0.020278, loss_ce: 0.006389
2022-01-08 01:38:57,916 iteration 3385 : loss : 0.027817, loss_ce: 0.009854
2022-01-08 01:38:59,246 iteration 3386 : loss : 0.016929, loss_ce: 0.005203
2022-01-08 01:39:00,689 iteration 3387 : loss : 0.028917, loss_ce: 0.010664
2022-01-08 01:39:02,080 iteration 3388 : loss : 0.022548, loss_ce: 0.009609
2022-01-08 01:39:03,420 iteration 3389 : loss : 0.013912, loss_ce: 0.005373
2022-01-08 01:39:04,677 iteration 3390 : loss : 0.012974, loss_ce: 0.005179
2022-01-08 01:39:06,085 iteration 3391 : loss : 0.028457, loss_ce: 0.010560
2022-01-08 01:39:07,491 iteration 3392 : loss : 0.030680, loss_ce: 0.008604
2022-01-08 01:39:08,870 iteration 3393 : loss : 0.019072, loss_ce: 0.007033
2022-01-08 01:39:10,280 iteration 3394 : loss : 0.024386, loss_ce: 0.009007
2022-01-08 01:39:11,589 iteration 3395 : loss : 0.014001, loss_ce: 0.005780
2022-01-08 01:39:13,005 iteration 3396 : loss : 0.028034, loss_ce: 0.009625
2022-01-08 01:39:14,393 iteration 3397 : loss : 0.018528, loss_ce: 0.007401
2022-01-08 01:39:15,793 iteration 3398 : loss : 0.020687, loss_ce: 0.009164
2022-01-08 01:39:17,144 iteration 3399 : loss : 0.019239, loss_ce: 0.007550
2022-01-08 01:39:17,145 Training Data Eval:
2022-01-08 01:39:24,036   Average segmentation loss on training set: 0.0161
2022-01-08 01:39:24,037 Validation Data Eval:
2022-01-08 01:39:26,410   Average segmentation loss on validation set: 0.0990
2022-01-08 01:39:27,719 iteration 3400 : loss : 0.027163, loss_ce: 0.008880
 50%|█████████████▌             | 200/400 [1:24:51<1:29:05, 26.73s/it]2022-01-08 01:39:29,146 iteration 3401 : loss : 0.022651, loss_ce: 0.010879
2022-01-08 01:39:30,449 iteration 3402 : loss : 0.016957, loss_ce: 0.006737
2022-01-08 01:39:31,897 iteration 3403 : loss : 0.024932, loss_ce: 0.009305
2022-01-08 01:39:33,311 iteration 3404 : loss : 0.026778, loss_ce: 0.009968
2022-01-08 01:39:34,618 iteration 3405 : loss : 0.033854, loss_ce: 0.010736
2022-01-08 01:39:36,074 iteration 3406 : loss : 0.028507, loss_ce: 0.011444
2022-01-08 01:39:37,441 iteration 3407 : loss : 0.019995, loss_ce: 0.007779
2022-01-08 01:39:38,738 iteration 3408 : loss : 0.020033, loss_ce: 0.007368
2022-01-08 01:39:40,109 iteration 3409 : loss : 0.015383, loss_ce: 0.005010
2022-01-08 01:39:41,511 iteration 3410 : loss : 0.021427, loss_ce: 0.008358
2022-01-08 01:39:42,927 iteration 3411 : loss : 0.027541, loss_ce: 0.010752
2022-01-08 01:39:44,309 iteration 3412 : loss : 0.023323, loss_ce: 0.008780
2022-01-08 01:39:45,674 iteration 3413 : loss : 0.024445, loss_ce: 0.006913
2022-01-08 01:39:47,098 iteration 3414 : loss : 0.032252, loss_ce: 0.015148
2022-01-08 01:39:48,475 iteration 3415 : loss : 0.027230, loss_ce: 0.011987
2022-01-08 01:39:49,924 iteration 3416 : loss : 0.018410, loss_ce: 0.006948
2022-01-08 01:39:51,352 iteration 3417 : loss : 0.023992, loss_ce: 0.007813
 50%|█████████████▌             | 201/400 [1:25:14<1:25:33, 25.80s/it]2022-01-08 01:39:52,695 iteration 3418 : loss : 0.015563, loss_ce: 0.005020
2022-01-08 01:39:53,968 iteration 3419 : loss : 0.020671, loss_ce: 0.008695
2022-01-08 01:39:55,301 iteration 3420 : loss : 0.017636, loss_ce: 0.006123
2022-01-08 01:39:56,615 iteration 3421 : loss : 0.025462, loss_ce: 0.009814
2022-01-08 01:39:57,920 iteration 3422 : loss : 0.017029, loss_ce: 0.005965
2022-01-08 01:39:59,321 iteration 3423 : loss : 0.023715, loss_ce: 0.011903
2022-01-08 01:40:00,710 iteration 3424 : loss : 0.035512, loss_ce: 0.018656
2022-01-08 01:40:02,096 iteration 3425 : loss : 0.022660, loss_ce: 0.008049
2022-01-08 01:40:03,359 iteration 3426 : loss : 0.021853, loss_ce: 0.007797
2022-01-08 01:40:04,727 iteration 3427 : loss : 0.021277, loss_ce: 0.005894
2022-01-08 01:40:06,110 iteration 3428 : loss : 0.037082, loss_ce: 0.015416
2022-01-08 01:40:07,443 iteration 3429 : loss : 0.018760, loss_ce: 0.005896
2022-01-08 01:40:08,846 iteration 3430 : loss : 0.033956, loss_ce: 0.012076
2022-01-08 01:40:10,228 iteration 3431 : loss : 0.034613, loss_ce: 0.012496
2022-01-08 01:40:11,655 iteration 3432 : loss : 0.026524, loss_ce: 0.010452
2022-01-08 01:40:13,063 iteration 3433 : loss : 0.026765, loss_ce: 0.009276
2022-01-08 01:40:14,425 iteration 3434 : loss : 0.028751, loss_ce: 0.008246
 50%|█████████████▋             | 202/400 [1:25:37<1:22:25, 24.98s/it]2022-01-08 01:40:15,813 iteration 3435 : loss : 0.019714, loss_ce: 0.007037
2022-01-08 01:40:17,241 iteration 3436 : loss : 0.029400, loss_ce: 0.011778
2022-01-08 01:40:18,668 iteration 3437 : loss : 0.033466, loss_ce: 0.012592
2022-01-08 01:40:20,093 iteration 3438 : loss : 0.057479, loss_ce: 0.021196
2022-01-08 01:40:21,412 iteration 3439 : loss : 0.022489, loss_ce: 0.007547
2022-01-08 01:40:22,812 iteration 3440 : loss : 0.029533, loss_ce: 0.010888
2022-01-08 01:40:24,153 iteration 3441 : loss : 0.026829, loss_ce: 0.008142
2022-01-08 01:40:25,536 iteration 3442 : loss : 0.020778, loss_ce: 0.008625
2022-01-08 01:40:27,014 iteration 3443 : loss : 0.032341, loss_ce: 0.013659
2022-01-08 01:40:28,358 iteration 3444 : loss : 0.035989, loss_ce: 0.011180
2022-01-08 01:40:29,740 iteration 3445 : loss : 0.035780, loss_ce: 0.016820
2022-01-08 01:40:31,150 iteration 3446 : loss : 0.026761, loss_ce: 0.013025
2022-01-08 01:40:32,549 iteration 3447 : loss : 0.030402, loss_ce: 0.008411
2022-01-08 01:40:33,855 iteration 3448 : loss : 0.026296, loss_ce: 0.009118
2022-01-08 01:40:35,191 iteration 3449 : loss : 0.027684, loss_ce: 0.010584
2022-01-08 01:40:36,537 iteration 3450 : loss : 0.019261, loss_ce: 0.005843
2022-01-08 01:40:37,879 iteration 3451 : loss : 0.019038, loss_ce: 0.010531
 51%|█████████████▋             | 203/400 [1:26:01<1:20:31, 24.53s/it]2022-01-08 01:40:39,202 iteration 3452 : loss : 0.017384, loss_ce: 0.008079
2022-01-08 01:40:40,615 iteration 3453 : loss : 0.025954, loss_ce: 0.008867
2022-01-08 01:40:42,112 iteration 3454 : loss : 0.030658, loss_ce: 0.013408
2022-01-08 01:40:43,447 iteration 3455 : loss : 0.030298, loss_ce: 0.013964
2022-01-08 01:40:44,830 iteration 3456 : loss : 0.039402, loss_ce: 0.009403
2022-01-08 01:40:46,149 iteration 3457 : loss : 0.018404, loss_ce: 0.007291
2022-01-08 01:40:47,458 iteration 3458 : loss : 0.032240, loss_ce: 0.010481
2022-01-08 01:40:48,885 iteration 3459 : loss : 0.020114, loss_ce: 0.008638
2022-01-08 01:40:50,193 iteration 3460 : loss : 0.018856, loss_ce: 0.007190
2022-01-08 01:40:51,521 iteration 3461 : loss : 0.019392, loss_ce: 0.007067
2022-01-08 01:40:52,793 iteration 3462 : loss : 0.015781, loss_ce: 0.007101
2022-01-08 01:40:54,195 iteration 3463 : loss : 0.024927, loss_ce: 0.010706
2022-01-08 01:40:55,570 iteration 3464 : loss : 0.025960, loss_ce: 0.008730
2022-01-08 01:40:56,896 iteration 3465 : loss : 0.026188, loss_ce: 0.009650
2022-01-08 01:40:58,332 iteration 3466 : loss : 0.032137, loss_ce: 0.014400
2022-01-08 01:40:59,619 iteration 3467 : loss : 0.016494, loss_ce: 0.005793
2022-01-08 01:41:00,964 iteration 3468 : loss : 0.019676, loss_ce: 0.005511
 51%|█████████████▊             | 204/400 [1:26:24<1:18:41, 24.09s/it]2022-01-08 01:41:02,347 iteration 3469 : loss : 0.021507, loss_ce: 0.009604
2022-01-08 01:41:03,696 iteration 3470 : loss : 0.037294, loss_ce: 0.011250
2022-01-08 01:41:05,106 iteration 3471 : loss : 0.019287, loss_ce: 0.007193
2022-01-08 01:41:06,550 iteration 3472 : loss : 0.030492, loss_ce: 0.011357
2022-01-08 01:41:07,873 iteration 3473 : loss : 0.017503, loss_ce: 0.008881
2022-01-08 01:41:09,292 iteration 3474 : loss : 0.021563, loss_ce: 0.008163
2022-01-08 01:41:10,660 iteration 3475 : loss : 0.021473, loss_ce: 0.009320
2022-01-08 01:41:11,989 iteration 3476 : loss : 0.027291, loss_ce: 0.008987
2022-01-08 01:41:13,343 iteration 3477 : loss : 0.023165, loss_ce: 0.006895
2022-01-08 01:41:14,717 iteration 3478 : loss : 0.025698, loss_ce: 0.010884
2022-01-08 01:41:16,069 iteration 3479 : loss : 0.018628, loss_ce: 0.008601
2022-01-08 01:41:17,397 iteration 3480 : loss : 0.019731, loss_ce: 0.009048
2022-01-08 01:41:18,735 iteration 3481 : loss : 0.042350, loss_ce: 0.011588
2022-01-08 01:41:20,085 iteration 3482 : loss : 0.021571, loss_ce: 0.008275
2022-01-08 01:41:21,508 iteration 3483 : loss : 0.029656, loss_ce: 0.012997
2022-01-08 01:41:22,889 iteration 3484 : loss : 0.022295, loss_ce: 0.007594
2022-01-08 01:41:22,889 Training Data Eval:
2022-01-08 01:41:29,763   Average segmentation loss on training set: 0.0163
2022-01-08 01:41:29,764 Validation Data Eval:
2022-01-08 01:41:32,141   Average segmentation loss on validation set: 0.1078
2022-01-08 01:41:33,509 iteration 3485 : loss : 0.022599, loss_ce: 0.009017
 51%|█████████████▊             | 205/400 [1:26:56<1:26:32, 26.63s/it]2022-01-08 01:41:34,935 iteration 3486 : loss : 0.022112, loss_ce: 0.006173
2022-01-08 01:41:36,284 iteration 3487 : loss : 0.025394, loss_ce: 0.011306
2022-01-08 01:41:37,641 iteration 3488 : loss : 0.028626, loss_ce: 0.008081
2022-01-08 01:41:38,903 iteration 3489 : loss : 0.020992, loss_ce: 0.009366
2022-01-08 01:41:40,309 iteration 3490 : loss : 0.024299, loss_ce: 0.009626
2022-01-08 01:41:41,787 iteration 3491 : loss : 0.034789, loss_ce: 0.017009
2022-01-08 01:41:43,177 iteration 3492 : loss : 0.017143, loss_ce: 0.005464
2022-01-08 01:41:44,524 iteration 3493 : loss : 0.030935, loss_ce: 0.010721
2022-01-08 01:41:45,900 iteration 3494 : loss : 0.020657, loss_ce: 0.007375
2022-01-08 01:41:47,284 iteration 3495 : loss : 0.019707, loss_ce: 0.006760
2022-01-08 01:41:48,748 iteration 3496 : loss : 0.037187, loss_ce: 0.015055
2022-01-08 01:41:50,110 iteration 3497 : loss : 0.017661, loss_ce: 0.007165
2022-01-08 01:41:51,513 iteration 3498 : loss : 0.018772, loss_ce: 0.007806
2022-01-08 01:41:52,844 iteration 3499 : loss : 0.018313, loss_ce: 0.009493
2022-01-08 01:41:54,214 iteration 3500 : loss : 0.021787, loss_ce: 0.009099
2022-01-08 01:41:55,654 iteration 3501 : loss : 0.025227, loss_ce: 0.009642
2022-01-08 01:41:57,001 iteration 3502 : loss : 0.020923, loss_ce: 0.011544
 52%|█████████████▉             | 206/400 [1:27:20<1:23:03, 25.69s/it]2022-01-08 01:41:58,427 iteration 3503 : loss : 0.034015, loss_ce: 0.008170
2022-01-08 01:41:59,784 iteration 3504 : loss : 0.017556, loss_ce: 0.006383
2022-01-08 01:42:01,149 iteration 3505 : loss : 0.016818, loss_ce: 0.007828
2022-01-08 01:42:02,565 iteration 3506 : loss : 0.031342, loss_ce: 0.011556
2022-01-08 01:42:03,879 iteration 3507 : loss : 0.028601, loss_ce: 0.010104
2022-01-08 01:42:05,245 iteration 3508 : loss : 0.018976, loss_ce: 0.004767
2022-01-08 01:42:06,653 iteration 3509 : loss : 0.022422, loss_ce: 0.009312
2022-01-08 01:42:08,024 iteration 3510 : loss : 0.021731, loss_ce: 0.010114
2022-01-08 01:42:09,594 iteration 3511 : loss : 0.030705, loss_ce: 0.014816
2022-01-08 01:42:10,886 iteration 3512 : loss : 0.017707, loss_ce: 0.007096
2022-01-08 01:42:12,176 iteration 3513 : loss : 0.026665, loss_ce: 0.013944
2022-01-08 01:42:13,540 iteration 3514 : loss : 0.019537, loss_ce: 0.007711
2022-01-08 01:42:14,894 iteration 3515 : loss : 0.022185, loss_ce: 0.010577
2022-01-08 01:42:16,242 iteration 3516 : loss : 0.016795, loss_ce: 0.008410
2022-01-08 01:42:17,599 iteration 3517 : loss : 0.018339, loss_ce: 0.008165
2022-01-08 01:42:19,045 iteration 3518 : loss : 0.025643, loss_ce: 0.007993
2022-01-08 01:42:20,452 iteration 3519 : loss : 0.022328, loss_ce: 0.008820
 52%|█████████████▉             | 207/400 [1:27:43<1:20:28, 25.02s/it]2022-01-08 01:42:21,784 iteration 3520 : loss : 0.023894, loss_ce: 0.009106
2022-01-08 01:42:23,131 iteration 3521 : loss : 0.021853, loss_ce: 0.005415
2022-01-08 01:42:24,438 iteration 3522 : loss : 0.019355, loss_ce: 0.005443
2022-01-08 01:42:25,828 iteration 3523 : loss : 0.024574, loss_ce: 0.014355
2022-01-08 01:42:27,262 iteration 3524 : loss : 0.019966, loss_ce: 0.007985
2022-01-08 01:42:28,600 iteration 3525 : loss : 0.022106, loss_ce: 0.006460
2022-01-08 01:42:29,983 iteration 3526 : loss : 0.024078, loss_ce: 0.014222
2022-01-08 01:42:31,428 iteration 3527 : loss : 0.031276, loss_ce: 0.015957
2022-01-08 01:42:32,743 iteration 3528 : loss : 0.087266, loss_ce: 0.012023
2022-01-08 01:42:34,105 iteration 3529 : loss : 0.023048, loss_ce: 0.008428
2022-01-08 01:42:35,470 iteration 3530 : loss : 0.019974, loss_ce: 0.006854
2022-01-08 01:42:36,853 iteration 3531 : loss : 0.021888, loss_ce: 0.007870
2022-01-08 01:42:38,244 iteration 3532 : loss : 0.022197, loss_ce: 0.009004
2022-01-08 01:42:39,647 iteration 3533 : loss : 0.025991, loss_ce: 0.010362
2022-01-08 01:42:40,951 iteration 3534 : loss : 0.028078, loss_ce: 0.009728
2022-01-08 01:42:42,334 iteration 3535 : loss : 0.018426, loss_ce: 0.006975
2022-01-08 01:42:43,675 iteration 3536 : loss : 0.021509, loss_ce: 0.008598
 52%|██████████████             | 208/400 [1:28:06<1:18:19, 24.48s/it]2022-01-08 01:42:45,162 iteration 3537 : loss : 0.028136, loss_ce: 0.011847
2022-01-08 01:42:46,488 iteration 3538 : loss : 0.021373, loss_ce: 0.007955
2022-01-08 01:42:47,825 iteration 3539 : loss : 0.025815, loss_ce: 0.008972
2022-01-08 01:42:49,232 iteration 3540 : loss : 0.023313, loss_ce: 0.009988
2022-01-08 01:42:50,648 iteration 3541 : loss : 0.031737, loss_ce: 0.011991
2022-01-08 01:42:52,036 iteration 3542 : loss : 0.033141, loss_ce: 0.013132
2022-01-08 01:42:53,470 iteration 3543 : loss : 0.031670, loss_ce: 0.012522
2022-01-08 01:42:54,885 iteration 3544 : loss : 0.049529, loss_ce: 0.009620
2022-01-08 01:42:56,301 iteration 3545 : loss : 0.032328, loss_ce: 0.010571
2022-01-08 01:42:57,702 iteration 3546 : loss : 0.024545, loss_ce: 0.008689
2022-01-08 01:42:59,148 iteration 3547 : loss : 0.027375, loss_ce: 0.010707
2022-01-08 01:43:00,475 iteration 3548 : loss : 0.014898, loss_ce: 0.004719
2022-01-08 01:43:01,937 iteration 3549 : loss : 0.031846, loss_ce: 0.014938
2022-01-08 01:43:03,283 iteration 3550 : loss : 0.027805, loss_ce: 0.009296
2022-01-08 01:43:04,621 iteration 3551 : loss : 0.023583, loss_ce: 0.010933
2022-01-08 01:43:05,949 iteration 3552 : loss : 0.024188, loss_ce: 0.012349
2022-01-08 01:43:07,292 iteration 3553 : loss : 0.019867, loss_ce: 0.007566
 52%|██████████████             | 209/400 [1:28:30<1:17:06, 24.22s/it]2022-01-08 01:43:08,692 iteration 3554 : loss : 0.021948, loss_ce: 0.008355
2022-01-08 01:43:10,040 iteration 3555 : loss : 0.019001, loss_ce: 0.008562
2022-01-08 01:43:11,466 iteration 3556 : loss : 0.024543, loss_ce: 0.011081
2022-01-08 01:43:12,912 iteration 3557 : loss : 0.025450, loss_ce: 0.011317
2022-01-08 01:43:14,275 iteration 3558 : loss : 0.026858, loss_ce: 0.010197
2022-01-08 01:43:15,650 iteration 3559 : loss : 0.019035, loss_ce: 0.004589
2022-01-08 01:43:17,009 iteration 3560 : loss : 0.022216, loss_ce: 0.008911
2022-01-08 01:43:18,289 iteration 3561 : loss : 0.014892, loss_ce: 0.007372
2022-01-08 01:43:19,665 iteration 3562 : loss : 0.021399, loss_ce: 0.009014
2022-01-08 01:43:21,044 iteration 3563 : loss : 0.024371, loss_ce: 0.008621
2022-01-08 01:43:22,464 iteration 3564 : loss : 0.045633, loss_ce: 0.009441
2022-01-08 01:43:23,777 iteration 3565 : loss : 0.032693, loss_ce: 0.007041
2022-01-08 01:43:25,105 iteration 3566 : loss : 0.025917, loss_ce: 0.010281
2022-01-08 01:43:26,548 iteration 3567 : loss : 0.032508, loss_ce: 0.014924
2022-01-08 01:43:27,899 iteration 3568 : loss : 0.025197, loss_ce: 0.008088
2022-01-08 01:43:29,219 iteration 3569 : loss : 0.021976, loss_ce: 0.008255
2022-01-08 01:43:29,220 Training Data Eval:
2022-01-08 01:43:36,120   Average segmentation loss on training set: 0.0156
2022-01-08 01:43:36,120 Validation Data Eval:
2022-01-08 01:43:38,501   Average segmentation loss on validation set: 0.0801
2022-01-08 01:43:39,915 iteration 3570 : loss : 0.024622, loss_ce: 0.007615
 52%|██████████████▏            | 210/400 [1:29:03<1:24:41, 26.74s/it]2022-01-08 01:43:41,352 iteration 3571 : loss : 0.022835, loss_ce: 0.011034
2022-01-08 01:43:42,660 iteration 3572 : loss : 0.018813, loss_ce: 0.006223
2022-01-08 01:43:44,046 iteration 3573 : loss : 0.031658, loss_ce: 0.011236
2022-01-08 01:43:45,418 iteration 3574 : loss : 0.020396, loss_ce: 0.005849
2022-01-08 01:43:46,730 iteration 3575 : loss : 0.018416, loss_ce: 0.005651
2022-01-08 01:43:48,132 iteration 3576 : loss : 0.027900, loss_ce: 0.007722
2022-01-08 01:43:49,527 iteration 3577 : loss : 0.023172, loss_ce: 0.009771
2022-01-08 01:43:50,803 iteration 3578 : loss : 0.022738, loss_ce: 0.008865
2022-01-08 01:43:52,148 iteration 3579 : loss : 0.029296, loss_ce: 0.013293
2022-01-08 01:43:53,467 iteration 3580 : loss : 0.022317, loss_ce: 0.008883
2022-01-08 01:43:54,784 iteration 3581 : loss : 0.025134, loss_ce: 0.012935
2022-01-08 01:43:56,138 iteration 3582 : loss : 0.022018, loss_ce: 0.008057
2022-01-08 01:43:57,534 iteration 3583 : loss : 0.026287, loss_ce: 0.008913
2022-01-08 01:43:58,910 iteration 3584 : loss : 0.019379, loss_ce: 0.006635
2022-01-08 01:44:00,223 iteration 3585 : loss : 0.017633, loss_ce: 0.006216
2022-01-08 01:44:01,643 iteration 3586 : loss : 0.030333, loss_ce: 0.012960
2022-01-08 01:44:02,994 iteration 3587 : loss : 0.014972, loss_ce: 0.006290
 53%|██████████████▏            | 211/400 [1:29:26<1:20:46, 25.64s/it]2022-01-08 01:44:04,466 iteration 3588 : loss : 0.020331, loss_ce: 0.007827
2022-01-08 01:44:05,775 iteration 3589 : loss : 0.019093, loss_ce: 0.006612
2022-01-08 01:44:07,110 iteration 3590 : loss : 0.025594, loss_ce: 0.011672
2022-01-08 01:44:08,473 iteration 3591 : loss : 0.017481, loss_ce: 0.006181
2022-01-08 01:44:09,993 iteration 3592 : loss : 0.025734, loss_ce: 0.010658
2022-01-08 01:44:11,320 iteration 3593 : loss : 0.043466, loss_ce: 0.013422
2022-01-08 01:44:12,679 iteration 3594 : loss : 0.024135, loss_ce: 0.008557
2022-01-08 01:44:14,071 iteration 3595 : loss : 0.029372, loss_ce: 0.010722
2022-01-08 01:44:15,444 iteration 3596 : loss : 0.027642, loss_ce: 0.013357
2022-01-08 01:44:16,869 iteration 3597 : loss : 0.028081, loss_ce: 0.011926
2022-01-08 01:44:18,189 iteration 3598 : loss : 0.016632, loss_ce: 0.007025
2022-01-08 01:44:19,599 iteration 3599 : loss : 0.028611, loss_ce: 0.008834
2022-01-08 01:44:20,857 iteration 3600 : loss : 0.017191, loss_ce: 0.006760
2022-01-08 01:44:22,221 iteration 3601 : loss : 0.020518, loss_ce: 0.008626
2022-01-08 01:44:23,530 iteration 3602 : loss : 0.019327, loss_ce: 0.006060
2022-01-08 01:44:24,910 iteration 3603 : loss : 0.022447, loss_ce: 0.011723
2022-01-08 01:44:26,197 iteration 3604 : loss : 0.025814, loss_ce: 0.011574
 53%|██████████████▎            | 212/400 [1:29:49<1:18:03, 24.91s/it]2022-01-08 01:44:27,603 iteration 3605 : loss : 0.023795, loss_ce: 0.009936
2022-01-08 01:44:28,931 iteration 3606 : loss : 0.021217, loss_ce: 0.009900
2022-01-08 01:44:30,318 iteration 3607 : loss : 0.028384, loss_ce: 0.011991
2022-01-08 01:44:31,663 iteration 3608 : loss : 0.019630, loss_ce: 0.007357
2022-01-08 01:44:32,999 iteration 3609 : loss : 0.020896, loss_ce: 0.010158
2022-01-08 01:44:34,374 iteration 3610 : loss : 0.021433, loss_ce: 0.009557
2022-01-08 01:44:35,671 iteration 3611 : loss : 0.022378, loss_ce: 0.006162
2022-01-08 01:44:36,977 iteration 3612 : loss : 0.024529, loss_ce: 0.010408
2022-01-08 01:44:38,433 iteration 3613 : loss : 0.027916, loss_ce: 0.011362
2022-01-08 01:44:39,746 iteration 3614 : loss : 0.016321, loss_ce: 0.006164
2022-01-08 01:44:41,030 iteration 3615 : loss : 0.019217, loss_ce: 0.006796
2022-01-08 01:44:42,425 iteration 3616 : loss : 0.022482, loss_ce: 0.008321
2022-01-08 01:44:43,828 iteration 3617 : loss : 0.025170, loss_ce: 0.010176
2022-01-08 01:44:45,214 iteration 3618 : loss : 0.019004, loss_ce: 0.008511
2022-01-08 01:44:46,583 iteration 3619 : loss : 0.026599, loss_ce: 0.007540
2022-01-08 01:44:47,909 iteration 3620 : loss : 0.025696, loss_ce: 0.007866
2022-01-08 01:44:49,285 iteration 3621 : loss : 0.024835, loss_ce: 0.009414
 53%|██████████████▍            | 213/400 [1:30:12<1:15:55, 24.36s/it]2022-01-08 01:44:50,690 iteration 3622 : loss : 0.018213, loss_ce: 0.008272
2022-01-08 01:44:52,103 iteration 3623 : loss : 0.023327, loss_ce: 0.005304
2022-01-08 01:44:53,454 iteration 3624 : loss : 0.021689, loss_ce: 0.009239
2022-01-08 01:44:54,803 iteration 3625 : loss : 0.030168, loss_ce: 0.011237
2022-01-08 01:44:56,125 iteration 3626 : loss : 0.019266, loss_ce: 0.009481
2022-01-08 01:44:57,441 iteration 3627 : loss : 0.017705, loss_ce: 0.006726
2022-01-08 01:44:58,857 iteration 3628 : loss : 0.051227, loss_ce: 0.012880
2022-01-08 01:45:00,370 iteration 3629 : loss : 0.023860, loss_ce: 0.009395
2022-01-08 01:45:01,637 iteration 3630 : loss : 0.020107, loss_ce: 0.007644
2022-01-08 01:45:02,998 iteration 3631 : loss : 0.028620, loss_ce: 0.009116
2022-01-08 01:45:04,480 iteration 3632 : loss : 0.032060, loss_ce: 0.010967
2022-01-08 01:45:05,840 iteration 3633 : loss : 0.020884, loss_ce: 0.009597
2022-01-08 01:45:07,195 iteration 3634 : loss : 0.018790, loss_ce: 0.008477
2022-01-08 01:45:08,645 iteration 3635 : loss : 0.052765, loss_ce: 0.023038
2022-01-08 01:45:10,055 iteration 3636 : loss : 0.031140, loss_ce: 0.007829
2022-01-08 01:45:11,421 iteration 3637 : loss : 0.039767, loss_ce: 0.014440
2022-01-08 01:45:12,779 iteration 3638 : loss : 0.022063, loss_ce: 0.009077
 54%|██████████████▍            | 214/400 [1:30:36<1:14:43, 24.10s/it]2022-01-08 01:45:14,129 iteration 3639 : loss : 0.017869, loss_ce: 0.007062
2022-01-08 01:45:15,429 iteration 3640 : loss : 0.021061, loss_ce: 0.008089
2022-01-08 01:45:16,724 iteration 3641 : loss : 0.016865, loss_ce: 0.005779
2022-01-08 01:45:18,080 iteration 3642 : loss : 0.025880, loss_ce: 0.007568
2022-01-08 01:45:19,384 iteration 3643 : loss : 0.018504, loss_ce: 0.007978
2022-01-08 01:45:20,712 iteration 3644 : loss : 0.013588, loss_ce: 0.004783
2022-01-08 01:45:22,081 iteration 3645 : loss : 0.025538, loss_ce: 0.008733
2022-01-08 01:45:23,446 iteration 3646 : loss : 0.031744, loss_ce: 0.011369
2022-01-08 01:45:24,810 iteration 3647 : loss : 0.023759, loss_ce: 0.006307
2022-01-08 01:45:26,198 iteration 3648 : loss : 0.029534, loss_ce: 0.013043
2022-01-08 01:45:27,545 iteration 3649 : loss : 0.017996, loss_ce: 0.005100
2022-01-08 01:45:28,908 iteration 3650 : loss : 0.020583, loss_ce: 0.010712
2022-01-08 01:45:30,292 iteration 3651 : loss : 0.044697, loss_ce: 0.015166
2022-01-08 01:45:31,815 iteration 3652 : loss : 0.038457, loss_ce: 0.011903
2022-01-08 01:45:33,150 iteration 3653 : loss : 0.025210, loss_ce: 0.015628
2022-01-08 01:45:34,443 iteration 3654 : loss : 0.016807, loss_ce: 0.007973
2022-01-08 01:45:34,443 Training Data Eval:
2022-01-08 01:45:41,358   Average segmentation loss on training set: 0.0144
2022-01-08 01:45:41,359 Validation Data Eval:
2022-01-08 01:45:43,734   Average segmentation loss on validation set: 0.0625
2022-01-08 01:45:47,872 Found new lowest validation loss at iteration 3654! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed2.pth
2022-01-08 01:45:49,183 iteration 3655 : loss : 0.018694, loss_ce: 0.007477
 54%|██████████████▌            | 215/400 [1:31:12<1:25:41, 27.79s/it]2022-01-08 01:45:50,549 iteration 3656 : loss : 0.023598, loss_ce: 0.008136
2022-01-08 01:45:51,779 iteration 3657 : loss : 0.017264, loss_ce: 0.008669
2022-01-08 01:45:53,107 iteration 3658 : loss : 0.026566, loss_ce: 0.013448
2022-01-08 01:45:54,494 iteration 3659 : loss : 0.029251, loss_ce: 0.010111
2022-01-08 01:45:55,777 iteration 3660 : loss : 0.027614, loss_ce: 0.007583
2022-01-08 01:45:57,004 iteration 3661 : loss : 0.018435, loss_ce: 0.008273
2022-01-08 01:45:58,291 iteration 3662 : loss : 0.016508, loss_ce: 0.006394
2022-01-08 01:45:59,551 iteration 3663 : loss : 0.027061, loss_ce: 0.009076
2022-01-08 01:46:00,904 iteration 3664 : loss : 0.017004, loss_ce: 0.006586
2022-01-08 01:46:02,265 iteration 3665 : loss : 0.024120, loss_ce: 0.011859
2022-01-08 01:46:03,636 iteration 3666 : loss : 0.022920, loss_ce: 0.008137
2022-01-08 01:46:04,987 iteration 3667 : loss : 0.028956, loss_ce: 0.013410
2022-01-08 01:46:06,274 iteration 3668 : loss : 0.021676, loss_ce: 0.006494
2022-01-08 01:46:07,694 iteration 3669 : loss : 0.024542, loss_ce: 0.010744
2022-01-08 01:46:09,012 iteration 3670 : loss : 0.015184, loss_ce: 0.005513
2022-01-08 01:46:10,370 iteration 3671 : loss : 0.020443, loss_ce: 0.010685
2022-01-08 01:46:11,876 iteration 3672 : loss : 0.037336, loss_ce: 0.009174
 54%|██████████████▌            | 216/400 [1:31:35<1:20:32, 26.26s/it]2022-01-08 01:46:13,306 iteration 3673 : loss : 0.022531, loss_ce: 0.009864
2022-01-08 01:46:14,712 iteration 3674 : loss : 0.023229, loss_ce: 0.006636
2022-01-08 01:46:16,159 iteration 3675 : loss : 0.041189, loss_ce: 0.012662
2022-01-08 01:46:17,454 iteration 3676 : loss : 0.013855, loss_ce: 0.005604
2022-01-08 01:46:18,833 iteration 3677 : loss : 0.026287, loss_ce: 0.007120
2022-01-08 01:46:20,268 iteration 3678 : loss : 0.036708, loss_ce: 0.010762
2022-01-08 01:46:21,611 iteration 3679 : loss : 0.017057, loss_ce: 0.006318
2022-01-08 01:46:22,987 iteration 3680 : loss : 0.024828, loss_ce: 0.009699
2022-01-08 01:46:24,358 iteration 3681 : loss : 0.018158, loss_ce: 0.007459
2022-01-08 01:46:25,643 iteration 3682 : loss : 0.018175, loss_ce: 0.006821
2022-01-08 01:46:26,927 iteration 3683 : loss : 0.016759, loss_ce: 0.006013
2022-01-08 01:46:28,350 iteration 3684 : loss : 0.016567, loss_ce: 0.006731
2022-01-08 01:46:29,691 iteration 3685 : loss : 0.016615, loss_ce: 0.005751
2022-01-08 01:46:31,089 iteration 3686 : loss : 0.020321, loss_ce: 0.008217
2022-01-08 01:46:32,421 iteration 3687 : loss : 0.014934, loss_ce: 0.005019
2022-01-08 01:46:33,775 iteration 3688 : loss : 0.022577, loss_ce: 0.011602
2022-01-08 01:46:35,154 iteration 3689 : loss : 0.020185, loss_ce: 0.005997
 54%|██████████████▋            | 217/400 [1:31:58<1:17:22, 25.37s/it]2022-01-08 01:46:36,562 iteration 3690 : loss : 0.020927, loss_ce: 0.004882
2022-01-08 01:46:37,953 iteration 3691 : loss : 0.045246, loss_ce: 0.012962
2022-01-08 01:46:39,317 iteration 3692 : loss : 0.022744, loss_ce: 0.010795
2022-01-08 01:46:40,611 iteration 3693 : loss : 0.016288, loss_ce: 0.006632
2022-01-08 01:46:41,979 iteration 3694 : loss : 0.020671, loss_ce: 0.008799
2022-01-08 01:46:43,363 iteration 3695 : loss : 0.020497, loss_ce: 0.010743
2022-01-08 01:46:44,708 iteration 3696 : loss : 0.028817, loss_ce: 0.009650
2022-01-08 01:46:46,104 iteration 3697 : loss : 0.033736, loss_ce: 0.012431
2022-01-08 01:46:47,555 iteration 3698 : loss : 0.029490, loss_ce: 0.012566
2022-01-08 01:46:48,875 iteration 3699 : loss : 0.016618, loss_ce: 0.006812
2022-01-08 01:46:50,242 iteration 3700 : loss : 0.026261, loss_ce: 0.007724
2022-01-08 01:46:51,514 iteration 3701 : loss : 0.016111, loss_ce: 0.006998
2022-01-08 01:46:52,900 iteration 3702 : loss : 0.018325, loss_ce: 0.006471
2022-01-08 01:46:54,276 iteration 3703 : loss : 0.020685, loss_ce: 0.010815
2022-01-08 01:46:55,562 iteration 3704 : loss : 0.017577, loss_ce: 0.006573
2022-01-08 01:46:56,990 iteration 3705 : loss : 0.028105, loss_ce: 0.010406
2022-01-08 01:46:58,267 iteration 3706 : loss : 0.013852, loss_ce: 0.004016
 55%|██████████████▋            | 218/400 [1:32:21<1:14:53, 24.69s/it]2022-01-08 01:46:59,686 iteration 3707 : loss : 0.018802, loss_ce: 0.008635
2022-01-08 01:47:00,963 iteration 3708 : loss : 0.024518, loss_ce: 0.009246
2022-01-08 01:47:02,421 iteration 3709 : loss : 0.039719, loss_ce: 0.011544
2022-01-08 01:47:03,778 iteration 3710 : loss : 0.026453, loss_ce: 0.008859
2022-01-08 01:47:05,113 iteration 3711 : loss : 0.017244, loss_ce: 0.005716
2022-01-08 01:47:06,422 iteration 3712 : loss : 0.017331, loss_ce: 0.008234
2022-01-08 01:47:07,878 iteration 3713 : loss : 0.019142, loss_ce: 0.006872
2022-01-08 01:47:09,248 iteration 3714 : loss : 0.030374, loss_ce: 0.011128
2022-01-08 01:47:10,589 iteration 3715 : loss : 0.015335, loss_ce: 0.006560
2022-01-08 01:47:11,956 iteration 3716 : loss : 0.021831, loss_ce: 0.011427
2022-01-08 01:47:13,376 iteration 3717 : loss : 0.016043, loss_ce: 0.006101
2022-01-08 01:47:14,725 iteration 3718 : loss : 0.021068, loss_ce: 0.008935
2022-01-08 01:47:16,096 iteration 3719 : loss : 0.024853, loss_ce: 0.010005
2022-01-08 01:47:17,554 iteration 3720 : loss : 0.032132, loss_ce: 0.010479
2022-01-08 01:47:18,880 iteration 3721 : loss : 0.015478, loss_ce: 0.005784
2022-01-08 01:47:20,217 iteration 3722 : loss : 0.024826, loss_ce: 0.012997
2022-01-08 01:47:21,627 iteration 3723 : loss : 0.045725, loss_ce: 0.014850
 55%|██████████████▊            | 219/400 [1:32:44<1:13:17, 24.29s/it]2022-01-08 01:47:23,076 iteration 3724 : loss : 0.030150, loss_ce: 0.013184
2022-01-08 01:47:24,431 iteration 3725 : loss : 0.030395, loss_ce: 0.015491
2022-01-08 01:47:25,917 iteration 3726 : loss : 0.018959, loss_ce: 0.007280
2022-01-08 01:47:27,241 iteration 3727 : loss : 0.015965, loss_ce: 0.005247
2022-01-08 01:47:28,601 iteration 3728 : loss : 0.020970, loss_ce: 0.010336
2022-01-08 01:47:30,027 iteration 3729 : loss : 0.030847, loss_ce: 0.011154
2022-01-08 01:47:31,367 iteration 3730 : loss : 0.040552, loss_ce: 0.016116
2022-01-08 01:47:32,674 iteration 3731 : loss : 0.022526, loss_ce: 0.008556
2022-01-08 01:47:34,037 iteration 3732 : loss : 0.020809, loss_ce: 0.008652
2022-01-08 01:47:35,390 iteration 3733 : loss : 0.020699, loss_ce: 0.006102
2022-01-08 01:47:36,763 iteration 3734 : loss : 0.019965, loss_ce: 0.007698
2022-01-08 01:47:38,151 iteration 3735 : loss : 0.027393, loss_ce: 0.014596
2022-01-08 01:47:39,575 iteration 3736 : loss : 0.029773, loss_ce: 0.010600
2022-01-08 01:47:40,948 iteration 3737 : loss : 0.034663, loss_ce: 0.013922
2022-01-08 01:47:42,303 iteration 3738 : loss : 0.026363, loss_ce: 0.010871
2022-01-08 01:47:43,665 iteration 3739 : loss : 0.017507, loss_ce: 0.005835
2022-01-08 01:47:43,666 Training Data Eval:
2022-01-08 01:47:50,572   Average segmentation loss on training set: 0.0164
2022-01-08 01:47:50,572 Validation Data Eval:
2022-01-08 01:47:52,946   Average segmentation loss on validation set: 0.1036
2022-01-08 01:47:54,369 iteration 3740 : loss : 0.019737, loss_ce: 0.007131
 55%|██████████████▊            | 220/400 [1:33:17<1:20:28, 26.83s/it]2022-01-08 01:47:55,788 iteration 3741 : loss : 0.022828, loss_ce: 0.009682
2022-01-08 01:47:57,120 iteration 3742 : loss : 0.023663, loss_ce: 0.009821
2022-01-08 01:47:58,463 iteration 3743 : loss : 0.022134, loss_ce: 0.007563
2022-01-08 01:47:59,840 iteration 3744 : loss : 0.017795, loss_ce: 0.007407
2022-01-08 01:48:01,165 iteration 3745 : loss : 0.021219, loss_ce: 0.008680
2022-01-08 01:48:02,572 iteration 3746 : loss : 0.016316, loss_ce: 0.006639
2022-01-08 01:48:03,993 iteration 3747 : loss : 0.049539, loss_ce: 0.006721
2022-01-08 01:48:05,359 iteration 3748 : loss : 0.032209, loss_ce: 0.016512
2022-01-08 01:48:06,721 iteration 3749 : loss : 0.022342, loss_ce: 0.009871
2022-01-08 01:48:08,109 iteration 3750 : loss : 0.024411, loss_ce: 0.010350
2022-01-08 01:48:09,501 iteration 3751 : loss : 0.025002, loss_ce: 0.006238
2022-01-08 01:48:10,878 iteration 3752 : loss : 0.029389, loss_ce: 0.011393
2022-01-08 01:48:12,202 iteration 3753 : loss : 0.029473, loss_ce: 0.008296
2022-01-08 01:48:13,512 iteration 3754 : loss : 0.017268, loss_ce: 0.005065
2022-01-08 01:48:14,842 iteration 3755 : loss : 0.023051, loss_ce: 0.009106
2022-01-08 01:48:16,211 iteration 3756 : loss : 0.027109, loss_ce: 0.008868
2022-01-08 01:48:17,580 iteration 3757 : loss : 0.021405, loss_ce: 0.006671
 55%|██████████████▉            | 221/400 [1:33:40<1:16:47, 25.74s/it]2022-01-08 01:48:19,000 iteration 3758 : loss : 0.023894, loss_ce: 0.009294
2022-01-08 01:48:20,352 iteration 3759 : loss : 0.019286, loss_ce: 0.006844
2022-01-08 01:48:21,629 iteration 3760 : loss : 0.019865, loss_ce: 0.007040
2022-01-08 01:48:23,045 iteration 3761 : loss : 0.025867, loss_ce: 0.010758
2022-01-08 01:48:24,405 iteration 3762 : loss : 0.030125, loss_ce: 0.008267
2022-01-08 01:48:25,818 iteration 3763 : loss : 0.030804, loss_ce: 0.015464
2022-01-08 01:48:27,226 iteration 3764 : loss : 0.030718, loss_ce: 0.010704
2022-01-08 01:48:28,585 iteration 3765 : loss : 0.019670, loss_ce: 0.008464
2022-01-08 01:48:29,959 iteration 3766 : loss : 0.051035, loss_ce: 0.017282
2022-01-08 01:48:31,409 iteration 3767 : loss : 0.027335, loss_ce: 0.008955
2022-01-08 01:48:32,783 iteration 3768 : loss : 0.026466, loss_ce: 0.012294
2022-01-08 01:48:34,125 iteration 3769 : loss : 0.020270, loss_ce: 0.007866
2022-01-08 01:48:35,447 iteration 3770 : loss : 0.019937, loss_ce: 0.006857
2022-01-08 01:48:36,789 iteration 3771 : loss : 0.015825, loss_ce: 0.006206
2022-01-08 01:48:38,137 iteration 3772 : loss : 0.019900, loss_ce: 0.007147
2022-01-08 01:48:39,428 iteration 3773 : loss : 0.016550, loss_ce: 0.006094
2022-01-08 01:48:40,735 iteration 3774 : loss : 0.021487, loss_ce: 0.008361
 56%|██████████████▉            | 222/400 [1:34:04<1:14:03, 24.96s/it]2022-01-08 01:48:42,112 iteration 3775 : loss : 0.023809, loss_ce: 0.007057
2022-01-08 01:48:43,513 iteration 3776 : loss : 0.023993, loss_ce: 0.010134
2022-01-08 01:48:44,985 iteration 3777 : loss : 0.030354, loss_ce: 0.009671
2022-01-08 01:48:46,312 iteration 3778 : loss : 0.016054, loss_ce: 0.008289
2022-01-08 01:48:47,717 iteration 3779 : loss : 0.021839, loss_ce: 0.008052
2022-01-08 01:48:49,058 iteration 3780 : loss : 0.024176, loss_ce: 0.011078
2022-01-08 01:48:50,416 iteration 3781 : loss : 0.027569, loss_ce: 0.009548
2022-01-08 01:48:51,809 iteration 3782 : loss : 0.024454, loss_ce: 0.008950
2022-01-08 01:48:53,210 iteration 3783 : loss : 0.026060, loss_ce: 0.007501
2022-01-08 01:48:54,620 iteration 3784 : loss : 0.030372, loss_ce: 0.013166
2022-01-08 01:48:55,954 iteration 3785 : loss : 0.018131, loss_ce: 0.005871
2022-01-08 01:48:57,343 iteration 3786 : loss : 0.028488, loss_ce: 0.010771
2022-01-08 01:48:58,771 iteration 3787 : loss : 0.056540, loss_ce: 0.022411
2022-01-08 01:49:00,195 iteration 3788 : loss : 0.024650, loss_ce: 0.008517
2022-01-08 01:49:01,509 iteration 3789 : loss : 0.022299, loss_ce: 0.006180
2022-01-08 01:49:02,905 iteration 3790 : loss : 0.026136, loss_ce: 0.010832
2022-01-08 01:49:04,200 iteration 3791 : loss : 0.028646, loss_ce: 0.011553
 56%|███████████████            | 223/400 [1:34:27<1:12:19, 24.51s/it]2022-01-08 01:49:05,597 iteration 3792 : loss : 0.018472, loss_ce: 0.008241
2022-01-08 01:49:06,966 iteration 3793 : loss : 0.027705, loss_ce: 0.011350
2022-01-08 01:49:08,276 iteration 3794 : loss : 0.019422, loss_ce: 0.007689
2022-01-08 01:49:09,753 iteration 3795 : loss : 0.029548, loss_ce: 0.011713
2022-01-08 01:49:11,144 iteration 3796 : loss : 0.051033, loss_ce: 0.011848
2022-01-08 01:49:12,481 iteration 3797 : loss : 0.020274, loss_ce: 0.006893
2022-01-08 01:49:13,940 iteration 3798 : loss : 0.031587, loss_ce: 0.014959
2022-01-08 01:49:15,254 iteration 3799 : loss : 0.023527, loss_ce: 0.012815
2022-01-08 01:49:16,659 iteration 3800 : loss : 0.023480, loss_ce: 0.008575
2022-01-08 01:49:17,972 iteration 3801 : loss : 0.018609, loss_ce: 0.006781
2022-01-08 01:49:19,339 iteration 3802 : loss : 0.023066, loss_ce: 0.006309
2022-01-08 01:49:20,697 iteration 3803 : loss : 0.018264, loss_ce: 0.006812
2022-01-08 01:49:21,973 iteration 3804 : loss : 0.022741, loss_ce: 0.011330
2022-01-08 01:49:23,375 iteration 3805 : loss : 0.021483, loss_ce: 0.008898
2022-01-08 01:49:24,707 iteration 3806 : loss : 0.021835, loss_ce: 0.006531
2022-01-08 01:49:26,047 iteration 3807 : loss : 0.024483, loss_ce: 0.010596
2022-01-08 01:49:27,429 iteration 3808 : loss : 0.021425, loss_ce: 0.007401
 56%|███████████████            | 224/400 [1:34:50<1:10:47, 24.13s/it]2022-01-08 01:49:28,839 iteration 3809 : loss : 0.024115, loss_ce: 0.011570
2022-01-08 01:49:30,195 iteration 3810 : loss : 0.020744, loss_ce: 0.008508
2022-01-08 01:49:31,664 iteration 3811 : loss : 0.019574, loss_ce: 0.006200
2022-01-08 01:49:33,001 iteration 3812 : loss : 0.018113, loss_ce: 0.006406
2022-01-08 01:49:34,385 iteration 3813 : loss : 0.039080, loss_ce: 0.011388
2022-01-08 01:49:35,673 iteration 3814 : loss : 0.013285, loss_ce: 0.004553
2022-01-08 01:49:37,098 iteration 3815 : loss : 0.044572, loss_ce: 0.014735
2022-01-08 01:49:38,561 iteration 3816 : loss : 0.020774, loss_ce: 0.006940
2022-01-08 01:49:39,936 iteration 3817 : loss : 0.021205, loss_ce: 0.010475
2022-01-08 01:49:41,406 iteration 3818 : loss : 0.019620, loss_ce: 0.008271
2022-01-08 01:49:42,715 iteration 3819 : loss : 0.018632, loss_ce: 0.009204
2022-01-08 01:49:44,121 iteration 3820 : loss : 0.027011, loss_ce: 0.011563
2022-01-08 01:49:45,483 iteration 3821 : loss : 0.019030, loss_ce: 0.008750
2022-01-08 01:49:46,757 iteration 3822 : loss : 0.016597, loss_ce: 0.006987
2022-01-08 01:49:48,079 iteration 3823 : loss : 0.018779, loss_ce: 0.006931
2022-01-08 01:49:49,419 iteration 3824 : loss : 0.027266, loss_ce: 0.010956
2022-01-08 01:49:49,419 Training Data Eval:
2022-01-08 01:49:56,325   Average segmentation loss on training set: 0.0158
2022-01-08 01:49:56,325 Validation Data Eval:
2022-01-08 01:49:58,709   Average segmentation loss on validation set: 0.0889
2022-01-08 01:50:00,044 iteration 3825 : loss : 0.027234, loss_ce: 0.010406
 56%|███████████████▏           | 225/400 [1:35:23<1:17:48, 26.68s/it]2022-01-08 01:50:01,370 iteration 3826 : loss : 0.017506, loss_ce: 0.008168
2022-01-08 01:50:02,751 iteration 3827 : loss : 0.023320, loss_ce: 0.008290
2022-01-08 01:50:04,146 iteration 3828 : loss : 0.025022, loss_ce: 0.012327
2022-01-08 01:50:05,513 iteration 3829 : loss : 0.020729, loss_ce: 0.009346
2022-01-08 01:50:06,887 iteration 3830 : loss : 0.023381, loss_ce: 0.009378
2022-01-08 01:50:08,332 iteration 3831 : loss : 0.029613, loss_ce: 0.007769
2022-01-08 01:50:09,631 iteration 3832 : loss : 0.014966, loss_ce: 0.005317
2022-01-08 01:50:11,009 iteration 3833 : loss : 0.015831, loss_ce: 0.004984
2022-01-08 01:50:12,360 iteration 3834 : loss : 0.017316, loss_ce: 0.009002
2022-01-08 01:50:13,778 iteration 3835 : loss : 0.023253, loss_ce: 0.010587
2022-01-08 01:50:15,193 iteration 3836 : loss : 0.024087, loss_ce: 0.008128
2022-01-08 01:50:16,475 iteration 3837 : loss : 0.015897, loss_ce: 0.006140
2022-01-08 01:50:17,807 iteration 3838 : loss : 0.020432, loss_ce: 0.006517
2022-01-08 01:50:19,141 iteration 3839 : loss : 0.017891, loss_ce: 0.006226
2022-01-08 01:50:20,420 iteration 3840 : loss : 0.021737, loss_ce: 0.006003
2022-01-08 01:50:21,746 iteration 3841 : loss : 0.018370, loss_ce: 0.006911
2022-01-08 01:50:23,084 iteration 3842 : loss : 0.017767, loss_ce: 0.004095
 56%|███████████████▎           | 226/400 [1:35:46<1:14:11, 25.58s/it]2022-01-08 01:50:24,520 iteration 3843 : loss : 0.017554, loss_ce: 0.006863
2022-01-08 01:50:25,889 iteration 3844 : loss : 0.022703, loss_ce: 0.009966
2022-01-08 01:50:27,218 iteration 3845 : loss : 0.028770, loss_ce: 0.009674
2022-01-08 01:50:28,567 iteration 3846 : loss : 0.020294, loss_ce: 0.004677
2022-01-08 01:50:29,976 iteration 3847 : loss : 0.015686, loss_ce: 0.006123
2022-01-08 01:50:31,312 iteration 3848 : loss : 0.015306, loss_ce: 0.006042
2022-01-08 01:50:32,602 iteration 3849 : loss : 0.014665, loss_ce: 0.005659
2022-01-08 01:50:34,003 iteration 3850 : loss : 0.044827, loss_ce: 0.011872
2022-01-08 01:50:35,418 iteration 3851 : loss : 0.054197, loss_ce: 0.034569
2022-01-08 01:50:36,771 iteration 3852 : loss : 0.020395, loss_ce: 0.007508
2022-01-08 01:50:38,113 iteration 3853 : loss : 0.016254, loss_ce: 0.008503
2022-01-08 01:50:39,531 iteration 3854 : loss : 0.026683, loss_ce: 0.012397
2022-01-08 01:50:40,904 iteration 3855 : loss : 0.018133, loss_ce: 0.007455
2022-01-08 01:50:42,270 iteration 3856 : loss : 0.023268, loss_ce: 0.008433
2022-01-08 01:50:43,598 iteration 3857 : loss : 0.017620, loss_ce: 0.007471
2022-01-08 01:50:44,986 iteration 3858 : loss : 0.017586, loss_ce: 0.006297
2022-01-08 01:50:46,369 iteration 3859 : loss : 0.025211, loss_ce: 0.007595
 57%|███████████████▎           | 227/400 [1:36:09<1:11:46, 24.90s/it]2022-01-08 01:50:47,774 iteration 3860 : loss : 0.017106, loss_ce: 0.007191
2022-01-08 01:50:49,203 iteration 3861 : loss : 0.027390, loss_ce: 0.013230
2022-01-08 01:50:50,503 iteration 3862 : loss : 0.017176, loss_ce: 0.008071
2022-01-08 01:50:51,912 iteration 3863 : loss : 0.020322, loss_ce: 0.010068
2022-01-08 01:50:53,323 iteration 3864 : loss : 0.017911, loss_ce: 0.006836
2022-01-08 01:50:54,802 iteration 3865 : loss : 0.032314, loss_ce: 0.014305
2022-01-08 01:50:56,276 iteration 3866 : loss : 0.026970, loss_ce: 0.011336
2022-01-08 01:50:57,615 iteration 3867 : loss : 0.015331, loss_ce: 0.006041
2022-01-08 01:50:59,022 iteration 3868 : loss : 0.023819, loss_ce: 0.009019
2022-01-08 01:51:00,427 iteration 3869 : loss : 0.026349, loss_ce: 0.009754
2022-01-08 01:51:01,847 iteration 3870 : loss : 0.020569, loss_ce: 0.005890
2022-01-08 01:51:03,228 iteration 3871 : loss : 0.038114, loss_ce: 0.011386
2022-01-08 01:51:04,573 iteration 3872 : loss : 0.019292, loss_ce: 0.008264
2022-01-08 01:51:05,940 iteration 3873 : loss : 0.022788, loss_ce: 0.008500
2022-01-08 01:51:07,363 iteration 3874 : loss : 0.018215, loss_ce: 0.006791
2022-01-08 01:51:08,765 iteration 3875 : loss : 0.015500, loss_ce: 0.005739
2022-01-08 01:51:10,120 iteration 3876 : loss : 0.029526, loss_ce: 0.005699
 57%|███████████████▍           | 228/400 [1:36:33<1:10:22, 24.55s/it]2022-01-08 01:51:11,488 iteration 3877 : loss : 0.016772, loss_ce: 0.006482
2022-01-08 01:51:12,914 iteration 3878 : loss : 0.023354, loss_ce: 0.010698
2022-01-08 01:51:14,297 iteration 3879 : loss : 0.028344, loss_ce: 0.013325
2022-01-08 01:51:15,651 iteration 3880 : loss : 0.018950, loss_ce: 0.008355
2022-01-08 01:51:16,980 iteration 3881 : loss : 0.019927, loss_ce: 0.010343
2022-01-08 01:51:18,416 iteration 3882 : loss : 0.037778, loss_ce: 0.011550
2022-01-08 01:51:19,807 iteration 3883 : loss : 0.024743, loss_ce: 0.010518
2022-01-08 01:51:21,129 iteration 3884 : loss : 0.066901, loss_ce: 0.010952
2022-01-08 01:51:22,445 iteration 3885 : loss : 0.016356, loss_ce: 0.005645
2022-01-08 01:51:23,784 iteration 3886 : loss : 0.021174, loss_ce: 0.007185
2022-01-08 01:51:25,204 iteration 3887 : loss : 0.024101, loss_ce: 0.009150
2022-01-08 01:51:26,565 iteration 3888 : loss : 0.024321, loss_ce: 0.009333
2022-01-08 01:51:27,956 iteration 3889 : loss : 0.032197, loss_ce: 0.012683
2022-01-08 01:51:29,242 iteration 3890 : loss : 0.022571, loss_ce: 0.007844
2022-01-08 01:51:30,628 iteration 3891 : loss : 0.015957, loss_ce: 0.006715
2022-01-08 01:51:32,006 iteration 3892 : loss : 0.028462, loss_ce: 0.006331
2022-01-08 01:51:33,462 iteration 3893 : loss : 0.026596, loss_ce: 0.014020
 57%|███████████████▍           | 229/400 [1:36:56<1:08:56, 24.19s/it]2022-01-08 01:51:34,809 iteration 3894 : loss : 0.015198, loss_ce: 0.006780
2022-01-08 01:51:36,234 iteration 3895 : loss : 0.024414, loss_ce: 0.009345
2022-01-08 01:51:37,578 iteration 3896 : loss : 0.037282, loss_ce: 0.011632
2022-01-08 01:51:38,951 iteration 3897 : loss : 0.031400, loss_ce: 0.015232
2022-01-08 01:51:40,289 iteration 3898 : loss : 0.019769, loss_ce: 0.007248
2022-01-08 01:51:41,705 iteration 3899 : loss : 0.040125, loss_ce: 0.016338
2022-01-08 01:51:43,148 iteration 3900 : loss : 0.041348, loss_ce: 0.014083
2022-01-08 01:51:44,463 iteration 3901 : loss : 0.015924, loss_ce: 0.005093
2022-01-08 01:51:45,799 iteration 3902 : loss : 0.015382, loss_ce: 0.005842
2022-01-08 01:51:47,164 iteration 3903 : loss : 0.016236, loss_ce: 0.005790
2022-01-08 01:51:48,595 iteration 3904 : loss : 0.024531, loss_ce: 0.007517
2022-01-08 01:51:49,960 iteration 3905 : loss : 0.022754, loss_ce: 0.010877
2022-01-08 01:51:51,290 iteration 3906 : loss : 0.025590, loss_ce: 0.012306
2022-01-08 01:51:52,672 iteration 3907 : loss : 0.044524, loss_ce: 0.017947
2022-01-08 01:51:54,132 iteration 3908 : loss : 0.028980, loss_ce: 0.011558
2022-01-08 01:51:55,535 iteration 3909 : loss : 0.015917, loss_ce: 0.005920
2022-01-08 01:51:55,535 Training Data Eval:
2022-01-08 01:52:02,422   Average segmentation loss on training set: 0.0126
2022-01-08 01:52:02,423 Validation Data Eval:
2022-01-08 01:52:04,810   Average segmentation loss on validation set: 0.0958
2022-01-08 01:52:06,119 iteration 3910 : loss : 0.013979, loss_ce: 0.004844
 57%|███████████████▌           | 230/400 [1:37:29<1:15:43, 26.73s/it]2022-01-08 01:52:07,475 iteration 3911 : loss : 0.017748, loss_ce: 0.005891
2022-01-08 01:52:08,898 iteration 3912 : loss : 0.022549, loss_ce: 0.009439
2022-01-08 01:52:10,193 iteration 3913 : loss : 0.018566, loss_ce: 0.005856
2022-01-08 01:52:11,491 iteration 3914 : loss : 0.018378, loss_ce: 0.006373
2022-01-08 01:52:12,789 iteration 3915 : loss : 0.016500, loss_ce: 0.005671
2022-01-08 01:52:14,210 iteration 3916 : loss : 0.020235, loss_ce: 0.007139
2022-01-08 01:52:15,636 iteration 3917 : loss : 0.040092, loss_ce: 0.012864
2022-01-08 01:52:17,050 iteration 3918 : loss : 0.025148, loss_ce: 0.011288
2022-01-08 01:52:18,434 iteration 3919 : loss : 0.024911, loss_ce: 0.009388
2022-01-08 01:52:19,825 iteration 3920 : loss : 0.019272, loss_ce: 0.008355
2022-01-08 01:52:21,209 iteration 3921 : loss : 0.026201, loss_ce: 0.007817
2022-01-08 01:52:22,552 iteration 3922 : loss : 0.020305, loss_ce: 0.007835
2022-01-08 01:52:23,922 iteration 3923 : loss : 0.018092, loss_ce: 0.007095
2022-01-08 01:52:25,286 iteration 3924 : loss : 0.027079, loss_ce: 0.010699
2022-01-08 01:52:26,781 iteration 3925 : loss : 0.022984, loss_ce: 0.009112
2022-01-08 01:52:28,129 iteration 3926 : loss : 0.023213, loss_ce: 0.006966
2022-01-08 01:52:29,503 iteration 3927 : loss : 0.023018, loss_ce: 0.009016
 58%|███████████████▌           | 231/400 [1:37:52<1:12:27, 25.73s/it]2022-01-08 01:52:30,909 iteration 3928 : loss : 0.020674, loss_ce: 0.008506
2022-01-08 01:52:32,341 iteration 3929 : loss : 0.029788, loss_ce: 0.014309
2022-01-08 01:52:33,838 iteration 3930 : loss : 0.031007, loss_ce: 0.011603
2022-01-08 01:52:35,226 iteration 3931 : loss : 0.022671, loss_ce: 0.010209
2022-01-08 01:52:36,650 iteration 3932 : loss : 0.035769, loss_ce: 0.009318
2022-01-08 01:52:37,983 iteration 3933 : loss : 0.035791, loss_ce: 0.009590
2022-01-08 01:52:39,345 iteration 3934 : loss : 0.019892, loss_ce: 0.006198
2022-01-08 01:52:40,778 iteration 3935 : loss : 0.026376, loss_ce: 0.009747
2022-01-08 01:52:42,130 iteration 3936 : loss : 0.022499, loss_ce: 0.008734
2022-01-08 01:52:43,500 iteration 3937 : loss : 0.016281, loss_ce: 0.005336
2022-01-08 01:52:44,844 iteration 3938 : loss : 0.027088, loss_ce: 0.010727
2022-01-08 01:52:46,297 iteration 3939 : loss : 0.029347, loss_ce: 0.010280
2022-01-08 01:52:47,633 iteration 3940 : loss : 0.020526, loss_ce: 0.005475
2022-01-08 01:52:48,992 iteration 3941 : loss : 0.020172, loss_ce: 0.008928
2022-01-08 01:52:50,344 iteration 3942 : loss : 0.027594, loss_ce: 0.012537
2022-01-08 01:52:51,755 iteration 3943 : loss : 0.024495, loss_ce: 0.013085
2022-01-08 01:52:53,123 iteration 3944 : loss : 0.025656, loss_ce: 0.009845
 58%|███████████████▋           | 232/400 [1:38:16<1:10:15, 25.09s/it]2022-01-08 01:52:54,515 iteration 3945 : loss : 0.014485, loss_ce: 0.006447
2022-01-08 01:52:55,797 iteration 3946 : loss : 0.015293, loss_ce: 0.006248
2022-01-08 01:52:57,163 iteration 3947 : loss : 0.017973, loss_ce: 0.005779
2022-01-08 01:52:58,511 iteration 3948 : loss : 0.019594, loss_ce: 0.008577
2022-01-08 01:52:59,887 iteration 3949 : loss : 0.024052, loss_ce: 0.012574
2022-01-08 01:53:01,244 iteration 3950 : loss : 0.018587, loss_ce: 0.007155
2022-01-08 01:53:02,645 iteration 3951 : loss : 0.023128, loss_ce: 0.009755
2022-01-08 01:53:03,994 iteration 3952 : loss : 0.015781, loss_ce: 0.007044
2022-01-08 01:53:05,366 iteration 3953 : loss : 0.016008, loss_ce: 0.005113
2022-01-08 01:53:06,778 iteration 3954 : loss : 0.025813, loss_ce: 0.012579
2022-01-08 01:53:08,167 iteration 3955 : loss : 0.020905, loss_ce: 0.007186
2022-01-08 01:53:09,514 iteration 3956 : loss : 0.027441, loss_ce: 0.011562
2022-01-08 01:53:10,874 iteration 3957 : loss : 0.017893, loss_ce: 0.007711
2022-01-08 01:53:12,320 iteration 3958 : loss : 0.026401, loss_ce: 0.013637
2022-01-08 01:53:13,647 iteration 3959 : loss : 0.016227, loss_ce: 0.004841
2022-01-08 01:53:15,015 iteration 3960 : loss : 0.018031, loss_ce: 0.004138
2022-01-08 01:53:16,423 iteration 3961 : loss : 0.018654, loss_ce: 0.006019
 58%|███████████████▋           | 233/400 [1:38:39<1:08:20, 24.56s/it]2022-01-08 01:53:17,963 iteration 3962 : loss : 0.051609, loss_ce: 0.021341
2022-01-08 01:53:19,260 iteration 3963 : loss : 0.018851, loss_ce: 0.008150
2022-01-08 01:53:20,658 iteration 3964 : loss : 0.029018, loss_ce: 0.012368
2022-01-08 01:53:22,108 iteration 3965 : loss : 0.031311, loss_ce: 0.009629
2022-01-08 01:53:23,440 iteration 3966 : loss : 0.016637, loss_ce: 0.005443
2022-01-08 01:53:24,774 iteration 3967 : loss : 0.021882, loss_ce: 0.006922
2022-01-08 01:53:26,207 iteration 3968 : loss : 0.020713, loss_ce: 0.008011
2022-01-08 01:53:27,614 iteration 3969 : loss : 0.023581, loss_ce: 0.010428
2022-01-08 01:53:29,063 iteration 3970 : loss : 0.025875, loss_ce: 0.012565
2022-01-08 01:53:30,396 iteration 3971 : loss : 0.024333, loss_ce: 0.007005
2022-01-08 01:53:31,739 iteration 3972 : loss : 0.018436, loss_ce: 0.007048
2022-01-08 01:53:33,126 iteration 3973 : loss : 0.026077, loss_ce: 0.010108
2022-01-08 01:53:34,594 iteration 3974 : loss : 0.026516, loss_ce: 0.010821
2022-01-08 01:53:36,043 iteration 3975 : loss : 0.018587, loss_ce: 0.006352
2022-01-08 01:53:37,493 iteration 3976 : loss : 0.021153, loss_ce: 0.008818
2022-01-08 01:53:38,836 iteration 3977 : loss : 0.021128, loss_ce: 0.007995
2022-01-08 01:53:40,185 iteration 3978 : loss : 0.021652, loss_ce: 0.009494
 58%|███████████████▊           | 234/400 [1:39:03<1:07:16, 24.32s/it]2022-01-08 01:53:41,603 iteration 3979 : loss : 0.045586, loss_ce: 0.015990
2022-01-08 01:53:42,997 iteration 3980 : loss : 0.021611, loss_ce: 0.009898
2022-01-08 01:53:44,331 iteration 3981 : loss : 0.019096, loss_ce: 0.006897
2022-01-08 01:53:45,821 iteration 3982 : loss : 0.018243, loss_ce: 0.006299
2022-01-08 01:53:47,177 iteration 3983 : loss : 0.022438, loss_ce: 0.008110
2022-01-08 01:53:48,587 iteration 3984 : loss : 0.019269, loss_ce: 0.007086
2022-01-08 01:53:49,971 iteration 3985 : loss : 0.018424, loss_ce: 0.006481
2022-01-08 01:53:51,302 iteration 3986 : loss : 0.019154, loss_ce: 0.007947
2022-01-08 01:53:52,646 iteration 3987 : loss : 0.017662, loss_ce: 0.007462
2022-01-08 01:53:54,079 iteration 3988 : loss : 0.031060, loss_ce: 0.012420
2022-01-08 01:53:55,459 iteration 3989 : loss : 0.023629, loss_ce: 0.009120
2022-01-08 01:53:56,815 iteration 3990 : loss : 0.013832, loss_ce: 0.005275
2022-01-08 01:53:58,177 iteration 3991 : loss : 0.022908, loss_ce: 0.008318
2022-01-08 01:53:59,654 iteration 3992 : loss : 0.030758, loss_ce: 0.008762
2022-01-08 01:54:01,021 iteration 3993 : loss : 0.022231, loss_ce: 0.011034
2022-01-08 01:54:02,377 iteration 3994 : loss : 0.017002, loss_ce: 0.006561
2022-01-08 01:54:02,378 Training Data Eval:
2022-01-08 01:54:09,294   Average segmentation loss on training set: 0.0132
2022-01-08 01:54:09,295 Validation Data Eval:
2022-01-08 01:54:11,667   Average segmentation loss on validation set: 0.1027
2022-01-08 01:54:13,018 iteration 3995 : loss : 0.018957, loss_ce: 0.004711
 59%|███████████████▊           | 235/400 [1:39:36<1:13:53, 26.87s/it]2022-01-08 01:54:14,506 iteration 3996 : loss : 0.021489, loss_ce: 0.011357
2022-01-08 01:54:15,908 iteration 3997 : loss : 0.018159, loss_ce: 0.005996
2022-01-08 01:54:17,255 iteration 3998 : loss : 0.017595, loss_ce: 0.006649
2022-01-08 01:54:18,677 iteration 3999 : loss : 0.015534, loss_ce: 0.004387
2022-01-08 01:54:20,065 iteration 4000 : loss : 0.022475, loss_ce: 0.008840
2022-01-08 01:54:21,542 iteration 4001 : loss : 0.019072, loss_ce: 0.005148
2022-01-08 01:54:22,904 iteration 4002 : loss : 0.016081, loss_ce: 0.007330
2022-01-08 01:54:24,264 iteration 4003 : loss : 0.019506, loss_ce: 0.007460
2022-01-08 01:54:25,630 iteration 4004 : loss : 0.020277, loss_ce: 0.008272
2022-01-08 01:54:27,017 iteration 4005 : loss : 0.020664, loss_ce: 0.011577
2022-01-08 01:54:28,485 iteration 4006 : loss : 0.043963, loss_ce: 0.013872
2022-01-08 01:54:29,831 iteration 4007 : loss : 0.015877, loss_ce: 0.006034
2022-01-08 01:54:31,200 iteration 4008 : loss : 0.024439, loss_ce: 0.008221
2022-01-08 01:54:32,555 iteration 4009 : loss : 0.020882, loss_ce: 0.008430
2022-01-08 01:54:33,855 iteration 4010 : loss : 0.012200, loss_ce: 0.004567
2022-01-08 01:54:35,269 iteration 4011 : loss : 0.044856, loss_ce: 0.011443
2022-01-08 01:54:36,694 iteration 4012 : loss : 0.020816, loss_ce: 0.008411
 59%|███████████████▉           | 236/400 [1:40:00<1:10:49, 25.91s/it]2022-01-08 01:54:38,092 iteration 4013 : loss : 0.017524, loss_ce: 0.004327
2022-01-08 01:54:39,508 iteration 4014 : loss : 0.042121, loss_ce: 0.014793
2022-01-08 01:54:40,844 iteration 4015 : loss : 0.019863, loss_ce: 0.010052
2022-01-08 01:54:42,250 iteration 4016 : loss : 0.024562, loss_ce: 0.008729
2022-01-08 01:54:43,578 iteration 4017 : loss : 0.024192, loss_ce: 0.010571
2022-01-08 01:54:45,022 iteration 4018 : loss : 0.023110, loss_ce: 0.008959
2022-01-08 01:54:46,443 iteration 4019 : loss : 0.029950, loss_ce: 0.013433
2022-01-08 01:54:47,792 iteration 4020 : loss : 0.015848, loss_ce: 0.005864
2022-01-08 01:54:49,170 iteration 4021 : loss : 0.017591, loss_ce: 0.006139
2022-01-08 01:54:50,548 iteration 4022 : loss : 0.027527, loss_ce: 0.008929
2022-01-08 01:54:51,941 iteration 4023 : loss : 0.027205, loss_ce: 0.011975
2022-01-08 01:54:53,323 iteration 4024 : loss : 0.026580, loss_ce: 0.009294
2022-01-08 01:54:54,674 iteration 4025 : loss : 0.018634, loss_ce: 0.006905
2022-01-08 01:54:56,019 iteration 4026 : loss : 0.014147, loss_ce: 0.006131
2022-01-08 01:54:57,368 iteration 4027 : loss : 0.016511, loss_ce: 0.006900
2022-01-08 01:54:58,728 iteration 4028 : loss : 0.024839, loss_ce: 0.006779
2022-01-08 01:55:00,108 iteration 4029 : loss : 0.019300, loss_ce: 0.007015
 59%|███████████████▉           | 237/400 [1:40:23<1:08:21, 25.16s/it]2022-01-08 01:55:01,524 iteration 4030 : loss : 0.020926, loss_ce: 0.008521
2022-01-08 01:55:02,882 iteration 4031 : loss : 0.018187, loss_ce: 0.006981
2022-01-08 01:55:04,295 iteration 4032 : loss : 0.018589, loss_ce: 0.006556
2022-01-08 01:55:05,751 iteration 4033 : loss : 0.041270, loss_ce: 0.013160
2022-01-08 01:55:07,088 iteration 4034 : loss : 0.026407, loss_ce: 0.008946
2022-01-08 01:55:08,462 iteration 4035 : loss : 0.021585, loss_ce: 0.007357
2022-01-08 01:55:09,815 iteration 4036 : loss : 0.015842, loss_ce: 0.004748
2022-01-08 01:55:11,233 iteration 4037 : loss : 0.019158, loss_ce: 0.006287
2022-01-08 01:55:12,600 iteration 4038 : loss : 0.017062, loss_ce: 0.009376
2022-01-08 01:55:13,869 iteration 4039 : loss : 0.017566, loss_ce: 0.008377
2022-01-08 01:55:15,233 iteration 4040 : loss : 0.050024, loss_ce: 0.010444
2022-01-08 01:55:16,744 iteration 4041 : loss : 0.035983, loss_ce: 0.017414
2022-01-08 01:55:18,055 iteration 4042 : loss : 0.017702, loss_ce: 0.007830
2022-01-08 01:55:19,442 iteration 4043 : loss : 0.017225, loss_ce: 0.006066
2022-01-08 01:55:20,811 iteration 4044 : loss : 0.023040, loss_ce: 0.009412
2022-01-08 01:55:22,147 iteration 4045 : loss : 0.026328, loss_ce: 0.010921
2022-01-08 01:55:23,559 iteration 4046 : loss : 0.022377, loss_ce: 0.006769
 60%|████████████████           | 238/400 [1:40:46<1:06:33, 24.65s/it]2022-01-08 01:55:24,902 iteration 4047 : loss : 0.015118, loss_ce: 0.006547
2022-01-08 01:55:26,258 iteration 4048 : loss : 0.017663, loss_ce: 0.004956
2022-01-08 01:55:27,704 iteration 4049 : loss : 0.024385, loss_ce: 0.007767
2022-01-08 01:55:29,079 iteration 4050 : loss : 0.040935, loss_ce: 0.013062
2022-01-08 01:55:30,463 iteration 4051 : loss : 0.013142, loss_ce: 0.004234
2022-01-08 01:55:31,812 iteration 4052 : loss : 0.019141, loss_ce: 0.005688
2022-01-08 01:55:33,164 iteration 4053 : loss : 0.019807, loss_ce: 0.009442
2022-01-08 01:55:34,490 iteration 4054 : loss : 0.025597, loss_ce: 0.011065
2022-01-08 01:55:35,953 iteration 4055 : loss : 0.026999, loss_ce: 0.007892
2022-01-08 01:55:37,349 iteration 4056 : loss : 0.032495, loss_ce: 0.010888
2022-01-08 01:55:38,778 iteration 4057 : loss : 0.026008, loss_ce: 0.010641
2022-01-08 01:55:40,074 iteration 4058 : loss : 0.019966, loss_ce: 0.009587
2022-01-08 01:55:41,524 iteration 4059 : loss : 0.020453, loss_ce: 0.009225
2022-01-08 01:55:42,936 iteration 4060 : loss : 0.020125, loss_ce: 0.007688
2022-01-08 01:55:44,288 iteration 4061 : loss : 0.031111, loss_ce: 0.007420
2022-01-08 01:55:45,624 iteration 4062 : loss : 0.022559, loss_ce: 0.009728
2022-01-08 01:55:47,096 iteration 4063 : loss : 0.030285, loss_ce: 0.012297
 60%|████████████████▏          | 239/400 [1:41:10<1:05:15, 24.32s/it]2022-01-08 01:55:48,494 iteration 4064 : loss : 0.018949, loss_ce: 0.006452
2022-01-08 01:55:49,857 iteration 4065 : loss : 0.020153, loss_ce: 0.009403
2022-01-08 01:55:51,149 iteration 4066 : loss : 0.014375, loss_ce: 0.004718
2022-01-08 01:55:52,463 iteration 4067 : loss : 0.025061, loss_ce: 0.007701
2022-01-08 01:55:53,839 iteration 4068 : loss : 0.020238, loss_ce: 0.007585
2022-01-08 01:55:55,245 iteration 4069 : loss : 0.018197, loss_ce: 0.005795
2022-01-08 01:55:56,632 iteration 4070 : loss : 0.026133, loss_ce: 0.012792
2022-01-08 01:55:57,943 iteration 4071 : loss : 0.017196, loss_ce: 0.007233
2022-01-08 01:55:59,313 iteration 4072 : loss : 0.016353, loss_ce: 0.006711
2022-01-08 01:56:00,693 iteration 4073 : loss : 0.037459, loss_ce: 0.016134
2022-01-08 01:56:02,017 iteration 4074 : loss : 0.017562, loss_ce: 0.006068
2022-01-08 01:56:03,439 iteration 4075 : loss : 0.022032, loss_ce: 0.008229
2022-01-08 01:56:04,829 iteration 4076 : loss : 0.025307, loss_ce: 0.010733
2022-01-08 01:56:06,218 iteration 4077 : loss : 0.015914, loss_ce: 0.006954
2022-01-08 01:56:07,666 iteration 4078 : loss : 0.027267, loss_ce: 0.008267
2022-01-08 01:56:09,049 iteration 4079 : loss : 0.018747, loss_ce: 0.006371
2022-01-08 01:56:09,049 Training Data Eval:
2022-01-08 01:56:15,909   Average segmentation loss on training set: 0.0125
2022-01-08 01:56:15,909 Validation Data Eval:
2022-01-08 01:56:18,296   Average segmentation loss on validation set: 0.0783
2022-01-08 01:56:19,694 iteration 4080 : loss : 0.020988, loss_ce: 0.006796
 60%|████████████████▏          | 240/400 [1:41:43<1:11:28, 26.80s/it]2022-01-08 01:56:21,105 iteration 4081 : loss : 0.018947, loss_ce: 0.008906
2022-01-08 01:56:22,551 iteration 4082 : loss : 0.022358, loss_ce: 0.008240
2022-01-08 01:56:23,842 iteration 4083 : loss : 0.026658, loss_ce: 0.016020
2022-01-08 01:56:25,177 iteration 4084 : loss : 0.016412, loss_ce: 0.005803
2022-01-08 01:56:26,544 iteration 4085 : loss : 0.020005, loss_ce: 0.004432
2022-01-08 01:56:27,880 iteration 4086 : loss : 0.023677, loss_ce: 0.007814
2022-01-08 01:56:29,246 iteration 4087 : loss : 0.018642, loss_ce: 0.005912
2022-01-08 01:56:30,620 iteration 4088 : loss : 0.018227, loss_ce: 0.007992
2022-01-08 01:56:32,010 iteration 4089 : loss : 0.030557, loss_ce: 0.010398
2022-01-08 01:56:33,394 iteration 4090 : loss : 0.020288, loss_ce: 0.008666
2022-01-08 01:56:34,817 iteration 4091 : loss : 0.036645, loss_ce: 0.012242
2022-01-08 01:56:36,134 iteration 4092 : loss : 0.021516, loss_ce: 0.006062
2022-01-08 01:56:37,471 iteration 4093 : loss : 0.016137, loss_ce: 0.006739
2022-01-08 01:56:38,902 iteration 4094 : loss : 0.019881, loss_ce: 0.006010
2022-01-08 01:56:40,272 iteration 4095 : loss : 0.019210, loss_ce: 0.006016
2022-01-08 01:56:41,673 iteration 4096 : loss : 0.023905, loss_ce: 0.010099
2022-01-08 01:56:43,086 iteration 4097 : loss : 0.019806, loss_ce: 0.008847
 60%|████████████████▎          | 241/400 [1:42:06<1:08:18, 25.78s/it]2022-01-08 01:56:44,503 iteration 4098 : loss : 0.015929, loss_ce: 0.005601
2022-01-08 01:56:45,880 iteration 4099 : loss : 0.021118, loss_ce: 0.008448
2022-01-08 01:56:47,213 iteration 4100 : loss : 0.021407, loss_ce: 0.009098
2022-01-08 01:56:48,608 iteration 4101 : loss : 0.017006, loss_ce: 0.007228
2022-01-08 01:56:50,048 iteration 4102 : loss : 0.023550, loss_ce: 0.008713
2022-01-08 01:56:51,332 iteration 4103 : loss : 0.014714, loss_ce: 0.006023
2022-01-08 01:56:52,722 iteration 4104 : loss : 0.025822, loss_ce: 0.010517
2022-01-08 01:56:54,143 iteration 4105 : loss : 0.021461, loss_ce: 0.007353
2022-01-08 01:56:55,595 iteration 4106 : loss : 0.044038, loss_ce: 0.013377
2022-01-08 01:56:56,990 iteration 4107 : loss : 0.023656, loss_ce: 0.009301
2022-01-08 01:56:58,413 iteration 4108 : loss : 0.021115, loss_ce: 0.008567
2022-01-08 01:56:59,809 iteration 4109 : loss : 0.025351, loss_ce: 0.007137
2022-01-08 01:57:01,243 iteration 4110 : loss : 0.024697, loss_ce: 0.007296
2022-01-08 01:57:02,625 iteration 4111 : loss : 0.027977, loss_ce: 0.011145
2022-01-08 01:57:03,903 iteration 4112 : loss : 0.013263, loss_ce: 0.005284
2022-01-08 01:57:05,342 iteration 4113 : loss : 0.025628, loss_ce: 0.007763
2022-01-08 01:57:06,730 iteration 4114 : loss : 0.034576, loss_ce: 0.017711
 60%|████████████████▎          | 242/400 [1:42:30<1:06:12, 25.14s/it]2022-01-08 01:57:08,149 iteration 4115 : loss : 0.021202, loss_ce: 0.006020
2022-01-08 01:57:09,502 iteration 4116 : loss : 0.018882, loss_ce: 0.008083
2022-01-08 01:57:10,836 iteration 4117 : loss : 0.014718, loss_ce: 0.007321
2022-01-08 01:57:12,205 iteration 4118 : loss : 0.023319, loss_ce: 0.006469
2022-01-08 01:57:13,475 iteration 4119 : loss : 0.016254, loss_ce: 0.006385
2022-01-08 01:57:14,965 iteration 4120 : loss : 0.019614, loss_ce: 0.006224
2022-01-08 01:57:16,331 iteration 4121 : loss : 0.017792, loss_ce: 0.006508
2022-01-08 01:57:17,733 iteration 4122 : loss : 0.024160, loss_ce: 0.008953
2022-01-08 01:57:19,140 iteration 4123 : loss : 0.027121, loss_ce: 0.011368
2022-01-08 01:57:20,433 iteration 4124 : loss : 0.014971, loss_ce: 0.006073
2022-01-08 01:57:21,839 iteration 4125 : loss : 0.026977, loss_ce: 0.008801
2022-01-08 01:57:23,342 iteration 4126 : loss : 0.018336, loss_ce: 0.007413
2022-01-08 01:57:24,786 iteration 4127 : loss : 0.023659, loss_ce: 0.012183
2022-01-08 01:57:26,134 iteration 4128 : loss : 0.017287, loss_ce: 0.007810
2022-01-08 01:57:27,640 iteration 4129 : loss : 0.027974, loss_ce: 0.007448
2022-01-08 01:57:29,081 iteration 4130 : loss : 0.020424, loss_ce: 0.005991
2022-01-08 01:57:30,448 iteration 4131 : loss : 0.017247, loss_ce: 0.006720
 61%|████████████████▍          | 243/400 [1:42:53<1:04:39, 24.71s/it]2022-01-08 01:57:31,912 iteration 4132 : loss : 0.026795, loss_ce: 0.008745
2022-01-08 01:57:33,239 iteration 4133 : loss : 0.027706, loss_ce: 0.014669
2022-01-08 01:57:34,683 iteration 4134 : loss : 0.015404, loss_ce: 0.006727
2022-01-08 01:57:36,077 iteration 4135 : loss : 0.035944, loss_ce: 0.012759
2022-01-08 01:57:37,438 iteration 4136 : loss : 0.014943, loss_ce: 0.004440
2022-01-08 01:57:38,831 iteration 4137 : loss : 0.020428, loss_ce: 0.009948
2022-01-08 01:57:40,227 iteration 4138 : loss : 0.017579, loss_ce: 0.009383
2022-01-08 01:57:41,626 iteration 4139 : loss : 0.031675, loss_ce: 0.008760
2022-01-08 01:57:43,034 iteration 4140 : loss : 0.024359, loss_ce: 0.007929
2022-01-08 01:57:44,387 iteration 4141 : loss : 0.021880, loss_ce: 0.005896
2022-01-08 01:57:45,724 iteration 4142 : loss : 0.018395, loss_ce: 0.007952
2022-01-08 01:57:47,063 iteration 4143 : loss : 0.018037, loss_ce: 0.006229
2022-01-08 01:57:48,453 iteration 4144 : loss : 0.026047, loss_ce: 0.008120
2022-01-08 01:57:49,805 iteration 4145 : loss : 0.035585, loss_ce: 0.017201
2022-01-08 01:57:51,072 iteration 4146 : loss : 0.016787, loss_ce: 0.004310
2022-01-08 01:57:52,438 iteration 4147 : loss : 0.014037, loss_ce: 0.005571
2022-01-08 01:57:53,805 iteration 4148 : loss : 0.028147, loss_ce: 0.010238
 61%|████████████████▍          | 244/400 [1:43:17<1:03:11, 24.31s/it]2022-01-08 01:57:55,221 iteration 4149 : loss : 0.029605, loss_ce: 0.010340
2022-01-08 01:57:56,586 iteration 4150 : loss : 0.020102, loss_ce: 0.007003
2022-01-08 01:57:57,947 iteration 4151 : loss : 0.018681, loss_ce: 0.008732
2022-01-08 01:57:59,390 iteration 4152 : loss : 0.024765, loss_ce: 0.011974
2022-01-08 01:58:00,795 iteration 4153 : loss : 0.031257, loss_ce: 0.015796
2022-01-08 01:58:02,104 iteration 4154 : loss : 0.018833, loss_ce: 0.007433
2022-01-08 01:58:03,433 iteration 4155 : loss : 0.016018, loss_ce: 0.007437
2022-01-08 01:58:04,806 iteration 4156 : loss : 0.027275, loss_ce: 0.009347
2022-01-08 01:58:06,189 iteration 4157 : loss : 0.018296, loss_ce: 0.007716
2022-01-08 01:58:07,538 iteration 4158 : loss : 0.015066, loss_ce: 0.006237
2022-01-08 01:58:08,951 iteration 4159 : loss : 0.016480, loss_ce: 0.008213
2022-01-08 01:58:10,335 iteration 4160 : loss : 0.019754, loss_ce: 0.007465
2022-01-08 01:58:11,678 iteration 4161 : loss : 0.015889, loss_ce: 0.005185
2022-01-08 01:58:13,078 iteration 4162 : loss : 0.019381, loss_ce: 0.006456
2022-01-08 01:58:14,459 iteration 4163 : loss : 0.023450, loss_ce: 0.009067
2022-01-08 01:58:15,814 iteration 4164 : loss : 0.023039, loss_ce: 0.007856
2022-01-08 01:58:15,814 Training Data Eval:
2022-01-08 01:58:22,710   Average segmentation loss on training set: 0.0115
2022-01-08 01:58:22,711 Validation Data Eval:
2022-01-08 01:58:25,096   Average segmentation loss on validation set: 0.0955
2022-01-08 01:58:26,509 iteration 4165 : loss : 0.024758, loss_ce: 0.008212
 61%|████████████████▌          | 245/400 [1:43:49<1:09:18, 26.83s/it]2022-01-08 01:58:27,905 iteration 4166 : loss : 0.022172, loss_ce: 0.006303
2022-01-08 01:58:29,245 iteration 4167 : loss : 0.023403, loss_ce: 0.009135
2022-01-08 01:58:30,538 iteration 4168 : loss : 0.015996, loss_ce: 0.004541
2022-01-08 01:58:31,874 iteration 4169 : loss : 0.037590, loss_ce: 0.018075
2022-01-08 01:58:33,322 iteration 4170 : loss : 0.027294, loss_ce: 0.008809
2022-01-08 01:58:34,792 iteration 4171 : loss : 0.025102, loss_ce: 0.010009
2022-01-08 01:58:36,111 iteration 4172 : loss : 0.014945, loss_ce: 0.003651
2022-01-08 01:58:37,499 iteration 4173 : loss : 0.016057, loss_ce: 0.005544
2022-01-08 01:58:38,890 iteration 4174 : loss : 0.034063, loss_ce: 0.018941
2022-01-08 01:58:40,297 iteration 4175 : loss : 0.019215, loss_ce: 0.006741
2022-01-08 01:58:41,672 iteration 4176 : loss : 0.023278, loss_ce: 0.009872
2022-01-08 01:58:43,064 iteration 4177 : loss : 0.021610, loss_ce: 0.006824
2022-01-08 01:58:44,435 iteration 4178 : loss : 0.020206, loss_ce: 0.009739
2022-01-08 01:58:45,874 iteration 4179 : loss : 0.019723, loss_ce: 0.007827
2022-01-08 01:58:47,227 iteration 4180 : loss : 0.030007, loss_ce: 0.010749
2022-01-08 01:58:48,501 iteration 4181 : loss : 0.023322, loss_ce: 0.006754
2022-01-08 01:58:49,906 iteration 4182 : loss : 0.017384, loss_ce: 0.007604
 62%|████████████████▌          | 246/400 [1:44:13<1:06:12, 25.80s/it]2022-01-08 01:58:51,319 iteration 4183 : loss : 0.033252, loss_ce: 0.014211
2022-01-08 01:58:52,715 iteration 4184 : loss : 0.026668, loss_ce: 0.009856
2022-01-08 01:58:54,088 iteration 4185 : loss : 0.024095, loss_ce: 0.008259
2022-01-08 01:58:55,416 iteration 4186 : loss : 0.016355, loss_ce: 0.006174
2022-01-08 01:58:56,845 iteration 4187 : loss : 0.025737, loss_ce: 0.009467
2022-01-08 01:58:58,261 iteration 4188 : loss : 0.022836, loss_ce: 0.011536
2022-01-08 01:58:59,660 iteration 4189 : loss : 0.016904, loss_ce: 0.004895
2022-01-08 01:59:01,090 iteration 4190 : loss : 0.022858, loss_ce: 0.008386
2022-01-08 01:59:02,439 iteration 4191 : loss : 0.021635, loss_ce: 0.006426
2022-01-08 01:59:03,817 iteration 4192 : loss : 0.014535, loss_ce: 0.004154
2022-01-08 01:59:05,207 iteration 4193 : loss : 0.015558, loss_ce: 0.004986
2022-01-08 01:59:06,628 iteration 4194 : loss : 0.022603, loss_ce: 0.008921
2022-01-08 01:59:08,137 iteration 4195 : loss : 0.055213, loss_ce: 0.016217
2022-01-08 01:59:09,457 iteration 4196 : loss : 0.014553, loss_ce: 0.007485
2022-01-08 01:59:10,884 iteration 4197 : loss : 0.019292, loss_ce: 0.005520
2022-01-08 01:59:12,263 iteration 4198 : loss : 0.020937, loss_ce: 0.007476
2022-01-08 01:59:13,568 iteration 4199 : loss : 0.017519, loss_ce: 0.007988
 62%|████████████████▋          | 247/400 [1:44:36<1:04:09, 25.16s/it]2022-01-08 01:59:14,950 iteration 4200 : loss : 0.018795, loss_ce: 0.007178
2022-01-08 01:59:16,351 iteration 4201 : loss : 0.016524, loss_ce: 0.006797
2022-01-08 01:59:17,659 iteration 4202 : loss : 0.014868, loss_ce: 0.007028
2022-01-08 01:59:18,997 iteration 4203 : loss : 0.019586, loss_ce: 0.008032
2022-01-08 01:59:20,366 iteration 4204 : loss : 0.023808, loss_ce: 0.008339
2022-01-08 01:59:21,705 iteration 4205 : loss : 0.024552, loss_ce: 0.008009
2022-01-08 01:59:23,001 iteration 4206 : loss : 0.020693, loss_ce: 0.007424
2022-01-08 01:59:24,332 iteration 4207 : loss : 0.016894, loss_ce: 0.007873
2022-01-08 01:59:25,674 iteration 4208 : loss : 0.022812, loss_ce: 0.008810
2022-01-08 01:59:27,156 iteration 4209 : loss : 0.030720, loss_ce: 0.007880
2022-01-08 01:59:28,597 iteration 4210 : loss : 0.028437, loss_ce: 0.010927
2022-01-08 01:59:29,834 iteration 4211 : loss : 0.012528, loss_ce: 0.004875
2022-01-08 01:59:31,214 iteration 4212 : loss : 0.021386, loss_ce: 0.009437
2022-01-08 01:59:32,586 iteration 4213 : loss : 0.026467, loss_ce: 0.012622
2022-01-08 01:59:34,009 iteration 4214 : loss : 0.023485, loss_ce: 0.008188
2022-01-08 01:59:35,350 iteration 4215 : loss : 0.016386, loss_ce: 0.005761
2022-01-08 01:59:36,810 iteration 4216 : loss : 0.023861, loss_ce: 0.009025
 62%|████████████████▋          | 248/400 [1:45:00<1:02:16, 24.58s/it]2022-01-08 01:59:38,300 iteration 4217 : loss : 0.015620, loss_ce: 0.006715
2022-01-08 01:59:39,599 iteration 4218 : loss : 0.016932, loss_ce: 0.009295
2022-01-08 01:59:40,919 iteration 4219 : loss : 0.018969, loss_ce: 0.006865
2022-01-08 01:59:42,407 iteration 4220 : loss : 0.048610, loss_ce: 0.020018
2022-01-08 01:59:43,797 iteration 4221 : loss : 0.022138, loss_ce: 0.006302
2022-01-08 01:59:45,191 iteration 4222 : loss : 0.026905, loss_ce: 0.010790
2022-01-08 01:59:46,568 iteration 4223 : loss : 0.031828, loss_ce: 0.010880
2022-01-08 01:59:47,938 iteration 4224 : loss : 0.020746, loss_ce: 0.006731
2022-01-08 01:59:49,274 iteration 4225 : loss : 0.020399, loss_ce: 0.006984
2022-01-08 01:59:50,627 iteration 4226 : loss : 0.017723, loss_ce: 0.005335
2022-01-08 01:59:52,099 iteration 4227 : loss : 0.028274, loss_ce: 0.009245
2022-01-08 01:59:53,460 iteration 4228 : loss : 0.020517, loss_ce: 0.007724
2022-01-08 01:59:54,864 iteration 4229 : loss : 0.018137, loss_ce: 0.006495
2022-01-08 01:59:56,249 iteration 4230 : loss : 0.019529, loss_ce: 0.009451
2022-01-08 01:59:57,590 iteration 4231 : loss : 0.023404, loss_ce: 0.007102
2022-01-08 01:59:58,940 iteration 4232 : loss : 0.019140, loss_ce: 0.008477
2022-01-08 02:00:00,308 iteration 4233 : loss : 0.017936, loss_ce: 0.008337
 62%|████████████████▊          | 249/400 [1:45:23<1:01:02, 24.26s/it]2022-01-08 02:00:01,780 iteration 4234 : loss : 0.023718, loss_ce: 0.013511
2022-01-08 02:00:03,144 iteration 4235 : loss : 0.017294, loss_ce: 0.005140
2022-01-08 02:00:04,449 iteration 4236 : loss : 0.013723, loss_ce: 0.005735
2022-01-08 02:00:05,771 iteration 4237 : loss : 0.016404, loss_ce: 0.006626
2022-01-08 02:00:07,134 iteration 4238 : loss : 0.015436, loss_ce: 0.004981
2022-01-08 02:00:08,549 iteration 4239 : loss : 0.018478, loss_ce: 0.007306
2022-01-08 02:00:09,931 iteration 4240 : loss : 0.020902, loss_ce: 0.006419
2022-01-08 02:00:11,296 iteration 4241 : loss : 0.019901, loss_ce: 0.008344
2022-01-08 02:00:12,551 iteration 4242 : loss : 0.013521, loss_ce: 0.005372
2022-01-08 02:00:13,947 iteration 4243 : loss : 0.020191, loss_ce: 0.007059
2022-01-08 02:00:15,291 iteration 4244 : loss : 0.015299, loss_ce: 0.004711
2022-01-08 02:00:16,668 iteration 4245 : loss : 0.023398, loss_ce: 0.011456
2022-01-08 02:00:18,009 iteration 4246 : loss : 0.019551, loss_ce: 0.006675
2022-01-08 02:00:19,381 iteration 4247 : loss : 0.019988, loss_ce: 0.004885
2022-01-08 02:00:20,683 iteration 4248 : loss : 0.018882, loss_ce: 0.007800
2022-01-08 02:00:22,116 iteration 4249 : loss : 0.017281, loss_ce: 0.007060
2022-01-08 02:00:22,116 Training Data Eval:
2022-01-08 02:00:29,039   Average segmentation loss on training set: 0.0115
2022-01-08 02:00:29,040 Validation Data Eval:
2022-01-08 02:00:31,419   Average segmentation loss on validation set: 0.0776
2022-01-08 02:00:32,808 iteration 4250 : loss : 0.015347, loss_ce: 0.005878
 62%|████████████████▉          | 250/400 [1:45:56<1:06:49, 26.73s/it]2022-01-08 02:00:34,235 iteration 4251 : loss : 0.014230, loss_ce: 0.004708
2022-01-08 02:00:35,598 iteration 4252 : loss : 0.018707, loss_ce: 0.004501
2022-01-08 02:00:36,993 iteration 4253 : loss : 0.013696, loss_ce: 0.006305
2022-01-08 02:00:38,391 iteration 4254 : loss : 0.019916, loss_ce: 0.008338
2022-01-08 02:00:39,783 iteration 4255 : loss : 0.029803, loss_ce: 0.007466
2022-01-08 02:00:41,122 iteration 4256 : loss : 0.017135, loss_ce: 0.005787
2022-01-08 02:00:42,501 iteration 4257 : loss : 0.015615, loss_ce: 0.006211
2022-01-08 02:00:43,940 iteration 4258 : loss : 0.027457, loss_ce: 0.012165
2022-01-08 02:00:45,220 iteration 4259 : loss : 0.012380, loss_ce: 0.004584
2022-01-08 02:00:46,616 iteration 4260 : loss : 0.016335, loss_ce: 0.004684
2022-01-08 02:00:47,929 iteration 4261 : loss : 0.012821, loss_ce: 0.003962
2022-01-08 02:00:49,294 iteration 4262 : loss : 0.017105, loss_ce: 0.007623
2022-01-08 02:00:50,704 iteration 4263 : loss : 0.023017, loss_ce: 0.008536
2022-01-08 02:00:52,058 iteration 4264 : loss : 0.015659, loss_ce: 0.006674
2022-01-08 02:00:53,314 iteration 4265 : loss : 0.012896, loss_ce: 0.005489
2022-01-08 02:00:54,707 iteration 4266 : loss : 0.015202, loss_ce: 0.004947
2022-01-08 02:00:56,105 iteration 4267 : loss : 0.022941, loss_ce: 0.007955
 63%|████████████████▉          | 251/400 [1:46:19<1:03:49, 25.70s/it]2022-01-08 02:00:57,440 iteration 4268 : loss : 0.016727, loss_ce: 0.006380
2022-01-08 02:00:58,749 iteration 4269 : loss : 0.014230, loss_ce: 0.004126
2022-01-08 02:01:00,110 iteration 4270 : loss : 0.024141, loss_ce: 0.006434
2022-01-08 02:01:01,459 iteration 4271 : loss : 0.017257, loss_ce: 0.006424
2022-01-08 02:01:02,808 iteration 4272 : loss : 0.017997, loss_ce: 0.007516
2022-01-08 02:01:04,151 iteration 4273 : loss : 0.014632, loss_ce: 0.006381
2022-01-08 02:01:05,523 iteration 4274 : loss : 0.018113, loss_ce: 0.008689
2022-01-08 02:01:06,872 iteration 4275 : loss : 0.016632, loss_ce: 0.006588
2022-01-08 02:01:08,220 iteration 4276 : loss : 0.017197, loss_ce: 0.006009
2022-01-08 02:01:09,592 iteration 4277 : loss : 0.018462, loss_ce: 0.008383
2022-01-08 02:01:10,945 iteration 4278 : loss : 0.017903, loss_ce: 0.003976
2022-01-08 02:01:12,261 iteration 4279 : loss : 0.014643, loss_ce: 0.006023
2022-01-08 02:01:13,637 iteration 4280 : loss : 0.017723, loss_ce: 0.004639
2022-01-08 02:01:15,017 iteration 4281 : loss : 0.022920, loss_ce: 0.008703
2022-01-08 02:01:16,419 iteration 4282 : loss : 0.034886, loss_ce: 0.011760
2022-01-08 02:01:17,858 iteration 4283 : loss : 0.023213, loss_ce: 0.011794
2022-01-08 02:01:19,282 iteration 4284 : loss : 0.023928, loss_ce: 0.009866
 63%|█████████████████          | 252/400 [1:46:42<1:01:31, 24.94s/it]2022-01-08 02:01:20,619 iteration 4285 : loss : 0.019216, loss_ce: 0.009673
2022-01-08 02:01:21,955 iteration 4286 : loss : 0.020712, loss_ce: 0.008123
2022-01-08 02:01:23,338 iteration 4287 : loss : 0.015883, loss_ce: 0.006246
2022-01-08 02:01:24,662 iteration 4288 : loss : 0.013282, loss_ce: 0.004115
2022-01-08 02:01:25,989 iteration 4289 : loss : 0.013811, loss_ce: 0.006142
2022-01-08 02:01:27,350 iteration 4290 : loss : 0.013592, loss_ce: 0.004514
2022-01-08 02:01:28,776 iteration 4291 : loss : 0.016364, loss_ce: 0.007497
2022-01-08 02:01:30,176 iteration 4292 : loss : 0.034183, loss_ce: 0.015359
2022-01-08 02:01:31,448 iteration 4293 : loss : 0.022569, loss_ce: 0.008461
2022-01-08 02:01:32,788 iteration 4294 : loss : 0.016947, loss_ce: 0.008062
2022-01-08 02:01:34,293 iteration 4295 : loss : 0.025564, loss_ce: 0.009353
2022-01-08 02:01:35,636 iteration 4296 : loss : 0.014734, loss_ce: 0.005368
2022-01-08 02:01:36,958 iteration 4297 : loss : 0.013203, loss_ce: 0.004119
2022-01-08 02:01:38,384 iteration 4298 : loss : 0.016115, loss_ce: 0.005234
2022-01-08 02:01:39,777 iteration 4299 : loss : 0.020352, loss_ce: 0.006041
2022-01-08 02:01:41,236 iteration 4300 : loss : 0.038109, loss_ce: 0.011993
2022-01-08 02:01:42,599 iteration 4301 : loss : 0.026151, loss_ce: 0.010305
 63%|██████████████████▎          | 253/400 [1:47:05<59:54, 24.45s/it]2022-01-08 02:01:44,070 iteration 4302 : loss : 0.021207, loss_ce: 0.007643
2022-01-08 02:01:45,382 iteration 4303 : loss : 0.015393, loss_ce: 0.007330
2022-01-08 02:01:46,752 iteration 4304 : loss : 0.018527, loss_ce: 0.007735
2022-01-08 02:01:48,124 iteration 4305 : loss : 0.024986, loss_ce: 0.007065
2022-01-08 02:01:49,476 iteration 4306 : loss : 0.016672, loss_ce: 0.005602
2022-01-08 02:01:50,911 iteration 4307 : loss : 0.023473, loss_ce: 0.006775
2022-01-08 02:01:52,251 iteration 4308 : loss : 0.021998, loss_ce: 0.008333
2022-01-08 02:01:53,671 iteration 4309 : loss : 0.013922, loss_ce: 0.004645
2022-01-08 02:01:54,999 iteration 4310 : loss : 0.013759, loss_ce: 0.004753
2022-01-08 02:01:56,460 iteration 4311 : loss : 0.024998, loss_ce: 0.010398
2022-01-08 02:01:57,807 iteration 4312 : loss : 0.018241, loss_ce: 0.007903
2022-01-08 02:01:59,231 iteration 4313 : loss : 0.020176, loss_ce: 0.009114
2022-01-08 02:02:00,618 iteration 4314 : loss : 0.015389, loss_ce: 0.005640
2022-01-08 02:02:01,957 iteration 4315 : loss : 0.014874, loss_ce: 0.005251
2022-01-08 02:02:03,304 iteration 4316 : loss : 0.016550, loss_ce: 0.006103
2022-01-08 02:02:04,676 iteration 4317 : loss : 0.016466, loss_ce: 0.006608
2022-01-08 02:02:06,143 iteration 4318 : loss : 0.019855, loss_ce: 0.007792
 64%|██████████████████▍          | 254/400 [1:47:29<58:50, 24.18s/it]2022-01-08 02:02:07,522 iteration 4319 : loss : 0.014984, loss_ce: 0.005545
2022-01-08 02:02:08,793 iteration 4320 : loss : 0.016196, loss_ce: 0.006981
2022-01-08 02:02:10,305 iteration 4321 : loss : 0.029882, loss_ce: 0.016151
2022-01-08 02:02:11,658 iteration 4322 : loss : 0.019483, loss_ce: 0.007308
2022-01-08 02:02:13,021 iteration 4323 : loss : 0.020876, loss_ce: 0.007437
2022-01-08 02:02:14,360 iteration 4324 : loss : 0.016493, loss_ce: 0.007671
2022-01-08 02:02:15,707 iteration 4325 : loss : 0.025240, loss_ce: 0.007771
2022-01-08 02:02:17,125 iteration 4326 : loss : 0.029078, loss_ce: 0.011305
2022-01-08 02:02:18,576 iteration 4327 : loss : 0.015410, loss_ce: 0.006698
2022-01-08 02:02:19,978 iteration 4328 : loss : 0.015796, loss_ce: 0.007557
2022-01-08 02:02:21,326 iteration 4329 : loss : 0.013620, loss_ce: 0.005099
2022-01-08 02:02:22,650 iteration 4330 : loss : 0.016980, loss_ce: 0.007191
2022-01-08 02:02:23,995 iteration 4331 : loss : 0.015817, loss_ce: 0.006493
2022-01-08 02:02:25,305 iteration 4332 : loss : 0.013534, loss_ce: 0.004925
2022-01-08 02:02:26,582 iteration 4333 : loss : 0.012424, loss_ce: 0.005007
2022-01-08 02:02:27,966 iteration 4334 : loss : 0.013915, loss_ce: 0.004651
2022-01-08 02:02:27,966 Training Data Eval:
2022-01-08 02:02:34,879   Average segmentation loss on training set: 0.0105
2022-01-08 02:02:34,880 Validation Data Eval:
2022-01-08 02:02:37,256   Average segmentation loss on validation set: 0.0799
2022-01-08 02:02:38,652 iteration 4335 : loss : 0.040912, loss_ce: 0.009140
 64%|█████████████████▏         | 255/400 [1:48:01<1:04:28, 26.68s/it]2022-01-08 02:02:39,986 iteration 4336 : loss : 0.016456, loss_ce: 0.007609
2022-01-08 02:02:41,383 iteration 4337 : loss : 0.017757, loss_ce: 0.007858
2022-01-08 02:02:42,797 iteration 4338 : loss : 0.023573, loss_ce: 0.008409
2022-01-08 02:02:44,096 iteration 4339 : loss : 0.018564, loss_ce: 0.004417
2022-01-08 02:02:45,486 iteration 4340 : loss : 0.024558, loss_ce: 0.003980
2022-01-08 02:02:46,847 iteration 4341 : loss : 0.016015, loss_ce: 0.007341
2022-01-08 02:02:48,192 iteration 4342 : loss : 0.014841, loss_ce: 0.005851
2022-01-08 02:02:49,619 iteration 4343 : loss : 0.047165, loss_ce: 0.013580
2022-01-08 02:02:50,948 iteration 4344 : loss : 0.016529, loss_ce: 0.006760
2022-01-08 02:02:52,286 iteration 4345 : loss : 0.018099, loss_ce: 0.007296
2022-01-08 02:02:53,549 iteration 4346 : loss : 0.015569, loss_ce: 0.004727
2022-01-08 02:02:54,976 iteration 4347 : loss : 0.033595, loss_ce: 0.010561
2022-01-08 02:02:56,393 iteration 4348 : loss : 0.017222, loss_ce: 0.008077
2022-01-08 02:02:57,702 iteration 4349 : loss : 0.017063, loss_ce: 0.007311
2022-01-08 02:02:59,013 iteration 4350 : loss : 0.016488, loss_ce: 0.005804
2022-01-08 02:03:00,444 iteration 4351 : loss : 0.018170, loss_ce: 0.008278
2022-01-08 02:03:01,846 iteration 4352 : loss : 0.019589, loss_ce: 0.008667
 64%|█████████████████▎         | 256/400 [1:48:25<1:01:31, 25.63s/it]2022-01-08 02:03:03,205 iteration 4353 : loss : 0.015129, loss_ce: 0.005128
2022-01-08 02:03:04,626 iteration 4354 : loss : 0.019766, loss_ce: 0.006034
2022-01-08 02:03:06,022 iteration 4355 : loss : 0.016964, loss_ce: 0.005703
2022-01-08 02:03:07,345 iteration 4356 : loss : 0.026352, loss_ce: 0.009548
2022-01-08 02:03:08,780 iteration 4357 : loss : 0.018496, loss_ce: 0.006520
2022-01-08 02:03:10,113 iteration 4358 : loss : 0.020054, loss_ce: 0.007982
2022-01-08 02:03:11,508 iteration 4359 : loss : 0.025766, loss_ce: 0.007821
2022-01-08 02:03:12,811 iteration 4360 : loss : 0.014354, loss_ce: 0.005628
2022-01-08 02:03:14,236 iteration 4361 : loss : 0.023718, loss_ce: 0.008353
2022-01-08 02:03:15,540 iteration 4362 : loss : 0.014888, loss_ce: 0.004593
2022-01-08 02:03:16,931 iteration 4363 : loss : 0.016311, loss_ce: 0.006184
2022-01-08 02:03:18,296 iteration 4364 : loss : 0.019525, loss_ce: 0.009013
2022-01-08 02:03:19,705 iteration 4365 : loss : 0.027615, loss_ce: 0.007791
2022-01-08 02:03:21,033 iteration 4366 : loss : 0.015218, loss_ce: 0.005735
2022-01-08 02:03:22,339 iteration 4367 : loss : 0.015103, loss_ce: 0.006539
2022-01-08 02:03:23,683 iteration 4368 : loss : 0.011706, loss_ce: 0.004262
2022-01-08 02:03:25,090 iteration 4369 : loss : 0.021357, loss_ce: 0.008738
 64%|██████████████████▋          | 257/400 [1:48:48<59:23, 24.92s/it]2022-01-08 02:03:26,445 iteration 4370 : loss : 0.020245, loss_ce: 0.007653
2022-01-08 02:03:27,965 iteration 4371 : loss : 0.026707, loss_ce: 0.010504
2022-01-08 02:03:29,378 iteration 4372 : loss : 0.042823, loss_ce: 0.010569
2022-01-08 02:03:30,800 iteration 4373 : loss : 0.024402, loss_ce: 0.010189
2022-01-08 02:03:32,139 iteration 4374 : loss : 0.022357, loss_ce: 0.008294
2022-01-08 02:03:33,520 iteration 4375 : loss : 0.014572, loss_ce: 0.006546
2022-01-08 02:03:34,919 iteration 4376 : loss : 0.018024, loss_ce: 0.006466
2022-01-08 02:03:36,250 iteration 4377 : loss : 0.016440, loss_ce: 0.005675
2022-01-08 02:03:37,656 iteration 4378 : loss : 0.020516, loss_ce: 0.009532
2022-01-08 02:03:38,993 iteration 4379 : loss : 0.018508, loss_ce: 0.007430
2022-01-08 02:03:40,374 iteration 4380 : loss : 0.024119, loss_ce: 0.010102
2022-01-08 02:03:41,773 iteration 4381 : loss : 0.035559, loss_ce: 0.010611
2022-01-08 02:03:43,234 iteration 4382 : loss : 0.023972, loss_ce: 0.007725
2022-01-08 02:03:44,590 iteration 4383 : loss : 0.024776, loss_ce: 0.008131
2022-01-08 02:03:45,996 iteration 4384 : loss : 0.017140, loss_ce: 0.006116
2022-01-08 02:03:47,417 iteration 4385 : loss : 0.033181, loss_ce: 0.016464
2022-01-08 02:03:48,758 iteration 4386 : loss : 0.021090, loss_ce: 0.005641
 64%|██████████████████▋          | 258/400 [1:49:12<58:05, 24.54s/it]2022-01-08 02:03:50,234 iteration 4387 : loss : 0.022199, loss_ce: 0.006493
2022-01-08 02:03:51,610 iteration 4388 : loss : 0.023913, loss_ce: 0.012034
2022-01-08 02:03:52,980 iteration 4389 : loss : 0.017579, loss_ce: 0.007472
2022-01-08 02:03:54,389 iteration 4390 : loss : 0.019931, loss_ce: 0.008926
2022-01-08 02:03:55,834 iteration 4391 : loss : 0.030083, loss_ce: 0.014929
2022-01-08 02:03:57,230 iteration 4392 : loss : 0.025254, loss_ce: 0.010622
2022-01-08 02:03:58,686 iteration 4393 : loss : 0.016488, loss_ce: 0.006394
2022-01-08 02:03:59,966 iteration 4394 : loss : 0.013564, loss_ce: 0.005199
2022-01-08 02:04:01,342 iteration 4395 : loss : 0.016232, loss_ce: 0.007768
2022-01-08 02:04:02,728 iteration 4396 : loss : 0.025425, loss_ce: 0.008897
2022-01-08 02:04:04,041 iteration 4397 : loss : 0.013184, loss_ce: 0.004781
2022-01-08 02:04:05,411 iteration 4398 : loss : 0.016172, loss_ce: 0.005982
2022-01-08 02:04:06,782 iteration 4399 : loss : 0.021041, loss_ce: 0.006657
2022-01-08 02:04:08,100 iteration 4400 : loss : 0.015185, loss_ce: 0.004629
2022-01-08 02:04:09,507 iteration 4401 : loss : 0.019714, loss_ce: 0.009951
2022-01-08 02:04:10,803 iteration 4402 : loss : 0.019727, loss_ce: 0.006897
2022-01-08 02:04:12,187 iteration 4403 : loss : 0.020983, loss_ce: 0.005701
 65%|██████████████████▊          | 259/400 [1:49:35<56:53, 24.21s/it]2022-01-08 02:04:13,660 iteration 4404 : loss : 0.024380, loss_ce: 0.010778
2022-01-08 02:04:15,023 iteration 4405 : loss : 0.019938, loss_ce: 0.006638
2022-01-08 02:04:16,421 iteration 4406 : loss : 0.017725, loss_ce: 0.007541
2022-01-08 02:04:17,811 iteration 4407 : loss : 0.024632, loss_ce: 0.007105
2022-01-08 02:04:19,159 iteration 4408 : loss : 0.017207, loss_ce: 0.008269
2022-01-08 02:04:20,491 iteration 4409 : loss : 0.017445, loss_ce: 0.006439
2022-01-08 02:04:21,957 iteration 4410 : loss : 0.021873, loss_ce: 0.007176
2022-01-08 02:04:23,337 iteration 4411 : loss : 0.016446, loss_ce: 0.006599
2022-01-08 02:04:24,706 iteration 4412 : loss : 0.023528, loss_ce: 0.011673
2022-01-08 02:04:26,095 iteration 4413 : loss : 0.014862, loss_ce: 0.006470
2022-01-08 02:04:27,423 iteration 4414 : loss : 0.022867, loss_ce: 0.007757
2022-01-08 02:04:28,746 iteration 4415 : loss : 0.014944, loss_ce: 0.006309
2022-01-08 02:04:30,137 iteration 4416 : loss : 0.031515, loss_ce: 0.012619
2022-01-08 02:04:31,524 iteration 4417 : loss : 0.021839, loss_ce: 0.011603
2022-01-08 02:04:32,903 iteration 4418 : loss : 0.014222, loss_ce: 0.005788
2022-01-08 02:04:34,259 iteration 4419 : loss : 0.013964, loss_ce: 0.004623
2022-01-08 02:04:34,259 Training Data Eval:
2022-01-08 02:04:41,169   Average segmentation loss on training set: 0.0110
2022-01-08 02:04:41,170 Validation Data Eval:
2022-01-08 02:04:43,552   Average segmentation loss on validation set: 0.0826
2022-01-08 02:04:44,909 iteration 4420 : loss : 0.015602, loss_ce: 0.004619
 65%|█████████████████▌         | 260/400 [1:50:08<1:02:26, 26.76s/it]2022-01-08 02:04:46,324 iteration 4421 : loss : 0.015068, loss_ce: 0.003945
2022-01-08 02:04:47,679 iteration 4422 : loss : 0.016460, loss_ce: 0.006102
2022-01-08 02:04:49,027 iteration 4423 : loss : 0.017136, loss_ce: 0.005990
2022-01-08 02:04:50,433 iteration 4424 : loss : 0.019099, loss_ce: 0.006689
2022-01-08 02:04:51,801 iteration 4425 : loss : 0.030941, loss_ce: 0.011449
2022-01-08 02:04:53,155 iteration 4426 : loss : 0.020524, loss_ce: 0.006924
2022-01-08 02:04:54,539 iteration 4427 : loss : 0.016716, loss_ce: 0.006588
2022-01-08 02:04:55,932 iteration 4428 : loss : 0.018475, loss_ce: 0.007465
2022-01-08 02:04:57,204 iteration 4429 : loss : 0.019718, loss_ce: 0.008600
2022-01-08 02:04:58,579 iteration 4430 : loss : 0.035323, loss_ce: 0.015538
2022-01-08 02:04:59,993 iteration 4431 : loss : 0.031228, loss_ce: 0.010135
2022-01-08 02:05:01,417 iteration 4432 : loss : 0.026125, loss_ce: 0.011890
2022-01-08 02:05:02,798 iteration 4433 : loss : 0.016814, loss_ce: 0.004226
2022-01-08 02:05:04,205 iteration 4434 : loss : 0.024386, loss_ce: 0.011117
2022-01-08 02:05:05,520 iteration 4435 : loss : 0.015681, loss_ce: 0.005705
2022-01-08 02:05:06,880 iteration 4436 : loss : 0.015892, loss_ce: 0.007929
2022-01-08 02:05:08,261 iteration 4437 : loss : 0.020557, loss_ce: 0.008383
 65%|██████████████████▉          | 261/400 [1:50:31<59:37, 25.74s/it]2022-01-08 02:05:09,771 iteration 4438 : loss : 0.022098, loss_ce: 0.008904
2022-01-08 02:05:11,048 iteration 4439 : loss : 0.012965, loss_ce: 0.005842
2022-01-08 02:05:12,466 iteration 4440 : loss : 0.019807, loss_ce: 0.009299
2022-01-08 02:05:13,838 iteration 4441 : loss : 0.014049, loss_ce: 0.005231
2022-01-08 02:05:15,296 iteration 4442 : loss : 0.027899, loss_ce: 0.011823
2022-01-08 02:05:16,735 iteration 4443 : loss : 0.018379, loss_ce: 0.006823
2022-01-08 02:05:18,132 iteration 4444 : loss : 0.023246, loss_ce: 0.008935
2022-01-08 02:05:19,467 iteration 4445 : loss : 0.017730, loss_ce: 0.006007
2022-01-08 02:05:20,886 iteration 4446 : loss : 0.037076, loss_ce: 0.006767
2022-01-08 02:05:22,340 iteration 4447 : loss : 0.017629, loss_ce: 0.005599
2022-01-08 02:05:23,700 iteration 4448 : loss : 0.023273, loss_ce: 0.008674
2022-01-08 02:05:25,168 iteration 4449 : loss : 0.022274, loss_ce: 0.009361
2022-01-08 02:05:26,521 iteration 4450 : loss : 0.015396, loss_ce: 0.007344
2022-01-08 02:05:27,915 iteration 4451 : loss : 0.018404, loss_ce: 0.005808
2022-01-08 02:05:29,243 iteration 4452 : loss : 0.020150, loss_ce: 0.006608
2022-01-08 02:05:30,568 iteration 4453 : loss : 0.013399, loss_ce: 0.005180
2022-01-08 02:05:31,922 iteration 4454 : loss : 0.017724, loss_ce: 0.006491
 66%|██████████████████▉          | 262/400 [1:50:55<57:45, 25.11s/it]2022-01-08 02:05:33,311 iteration 4455 : loss : 0.026488, loss_ce: 0.008705
2022-01-08 02:05:34,606 iteration 4456 : loss : 0.016906, loss_ce: 0.005563
2022-01-08 02:05:35,967 iteration 4457 : loss : 0.021215, loss_ce: 0.010090
2022-01-08 02:05:37,304 iteration 4458 : loss : 0.017287, loss_ce: 0.006517
2022-01-08 02:05:38,706 iteration 4459 : loss : 0.048288, loss_ce: 0.010979
2022-01-08 02:05:40,034 iteration 4460 : loss : 0.014395, loss_ce: 0.005751
2022-01-08 02:05:41,383 iteration 4461 : loss : 0.014421, loss_ce: 0.006379
2022-01-08 02:05:42,830 iteration 4462 : loss : 0.022475, loss_ce: 0.007830
2022-01-08 02:05:44,135 iteration 4463 : loss : 0.018047, loss_ce: 0.006186
2022-01-08 02:05:45,472 iteration 4464 : loss : 0.013915, loss_ce: 0.006086
2022-01-08 02:05:46,931 iteration 4465 : loss : 0.089464, loss_ce: 0.016072
2022-01-08 02:05:48,215 iteration 4466 : loss : 0.014204, loss_ce: 0.005238
2022-01-08 02:05:49,599 iteration 4467 : loss : 0.020644, loss_ce: 0.009228
2022-01-08 02:05:50,959 iteration 4468 : loss : 0.024054, loss_ce: 0.008339
2022-01-08 02:05:52,317 iteration 4469 : loss : 0.025430, loss_ce: 0.008272
2022-01-08 02:05:53,659 iteration 4470 : loss : 0.029782, loss_ce: 0.016359
2022-01-08 02:05:55,064 iteration 4471 : loss : 0.038337, loss_ce: 0.013451
 66%|███████████████████          | 263/400 [1:51:18<55:59, 24.52s/it]2022-01-08 02:05:56,425 iteration 4472 : loss : 0.019109, loss_ce: 0.007636
2022-01-08 02:05:57,850 iteration 4473 : loss : 0.025015, loss_ce: 0.008725
2022-01-08 02:05:59,322 iteration 4474 : loss : 0.024236, loss_ce: 0.010978
2022-01-08 02:06:00,721 iteration 4475 : loss : 0.018477, loss_ce: 0.007365
2022-01-08 02:06:02,058 iteration 4476 : loss : 0.025287, loss_ce: 0.008718
2022-01-08 02:06:03,401 iteration 4477 : loss : 0.017823, loss_ce: 0.007718
2022-01-08 02:06:04,859 iteration 4478 : loss : 0.020989, loss_ce: 0.010219
2022-01-08 02:06:06,220 iteration 4479 : loss : 0.026454, loss_ce: 0.012160
2022-01-08 02:06:07,529 iteration 4480 : loss : 0.014459, loss_ce: 0.005573
2022-01-08 02:06:08,953 iteration 4481 : loss : 0.022996, loss_ce: 0.005509
2022-01-08 02:06:10,320 iteration 4482 : loss : 0.017026, loss_ce: 0.005724
2022-01-08 02:06:11,643 iteration 4483 : loss : 0.020918, loss_ce: 0.005913
2022-01-08 02:06:13,084 iteration 4484 : loss : 0.028471, loss_ce: 0.009396
2022-01-08 02:06:14,502 iteration 4485 : loss : 0.021933, loss_ce: 0.010177
2022-01-08 02:06:15,927 iteration 4486 : loss : 0.033862, loss_ce: 0.016797
2022-01-08 02:06:17,247 iteration 4487 : loss : 0.022415, loss_ce: 0.006925
2022-01-08 02:06:18,597 iteration 4488 : loss : 0.043758, loss_ce: 0.016766
 66%|███████████████████▏         | 264/400 [1:51:41<54:54, 24.23s/it]2022-01-08 02:06:20,047 iteration 4489 : loss : 0.020891, loss_ce: 0.008301
2022-01-08 02:06:21,371 iteration 4490 : loss : 0.014907, loss_ce: 0.008169
2022-01-08 02:06:22,784 iteration 4491 : loss : 0.019799, loss_ce: 0.006908
2022-01-08 02:06:24,238 iteration 4492 : loss : 0.024391, loss_ce: 0.013701
2022-01-08 02:06:25,577 iteration 4493 : loss : 0.025800, loss_ce: 0.011353
2022-01-08 02:06:26,940 iteration 4494 : loss : 0.024521, loss_ce: 0.007587
2022-01-08 02:06:28,321 iteration 4495 : loss : 0.032180, loss_ce: 0.011289
2022-01-08 02:06:29,651 iteration 4496 : loss : 0.014408, loss_ce: 0.006881
2022-01-08 02:06:31,086 iteration 4497 : loss : 0.017371, loss_ce: 0.008590
2022-01-08 02:06:32,450 iteration 4498 : loss : 0.020963, loss_ce: 0.006369
2022-01-08 02:06:33,847 iteration 4499 : loss : 0.021785, loss_ce: 0.007533
2022-01-08 02:06:35,150 iteration 4500 : loss : 0.013429, loss_ce: 0.004500
2022-01-08 02:06:36,482 iteration 4501 : loss : 0.017880, loss_ce: 0.005541
2022-01-08 02:06:37,819 iteration 4502 : loss : 0.029667, loss_ce: 0.010160
2022-01-08 02:06:39,262 iteration 4503 : loss : 0.017495, loss_ce: 0.006954
2022-01-08 02:06:40,621 iteration 4504 : loss : 0.020424, loss_ce: 0.007940
2022-01-08 02:06:40,621 Training Data Eval:
2022-01-08 02:06:47,527   Average segmentation loss on training set: 0.0109
2022-01-08 02:06:47,528 Validation Data Eval:
2022-01-08 02:06:49,908   Average segmentation loss on validation set: 0.0915
2022-01-08 02:06:51,321 iteration 4505 : loss : 0.014608, loss_ce: 0.005917
 66%|█████████████████▉         | 265/400 [1:52:14<1:00:14, 26.78s/it]2022-01-08 02:06:52,704 iteration 4506 : loss : 0.019106, loss_ce: 0.007568
2022-01-08 02:06:54,086 iteration 4507 : loss : 0.017052, loss_ce: 0.008177
2022-01-08 02:06:55,428 iteration 4508 : loss : 0.017868, loss_ce: 0.006437
2022-01-08 02:06:56,760 iteration 4509 : loss : 0.023698, loss_ce: 0.005123
2022-01-08 02:06:58,202 iteration 4510 : loss : 0.023262, loss_ce: 0.007820
2022-01-08 02:06:59,555 iteration 4511 : loss : 0.015657, loss_ce: 0.004264
2022-01-08 02:07:00,984 iteration 4512 : loss : 0.017320, loss_ce: 0.008693
2022-01-08 02:07:02,444 iteration 4513 : loss : 0.020169, loss_ce: 0.007070
2022-01-08 02:07:03,714 iteration 4514 : loss : 0.015217, loss_ce: 0.004426
2022-01-08 02:07:05,057 iteration 4515 : loss : 0.025419, loss_ce: 0.009074
2022-01-08 02:07:06,366 iteration 4516 : loss : 0.012821, loss_ce: 0.005201
2022-01-08 02:07:07,682 iteration 4517 : loss : 0.014250, loss_ce: 0.005382
2022-01-08 02:07:09,063 iteration 4518 : loss : 0.029903, loss_ce: 0.014426
2022-01-08 02:07:10,345 iteration 4519 : loss : 0.017422, loss_ce: 0.005828
2022-01-08 02:07:11,752 iteration 4520 : loss : 0.024204, loss_ce: 0.010065
2022-01-08 02:07:13,089 iteration 4521 : loss : 0.018435, loss_ce: 0.007677
2022-01-08 02:07:14,466 iteration 4522 : loss : 0.020653, loss_ce: 0.008098
 66%|███████████████████▎         | 266/400 [1:52:37<57:22, 25.69s/it]2022-01-08 02:07:15,952 iteration 4523 : loss : 0.014989, loss_ce: 0.006010
2022-01-08 02:07:17,280 iteration 4524 : loss : 0.021720, loss_ce: 0.007476
2022-01-08 02:07:18,567 iteration 4525 : loss : 0.016738, loss_ce: 0.008384
2022-01-08 02:07:19,895 iteration 4526 : loss : 0.018985, loss_ce: 0.006069
2022-01-08 02:07:21,223 iteration 4527 : loss : 0.012473, loss_ce: 0.005110
2022-01-08 02:07:22,582 iteration 4528 : loss : 0.013406, loss_ce: 0.005773
2022-01-08 02:07:23,892 iteration 4529 : loss : 0.017417, loss_ce: 0.006553
2022-01-08 02:07:25,265 iteration 4530 : loss : 0.021214, loss_ce: 0.008661
2022-01-08 02:07:26,731 iteration 4531 : loss : 0.035930, loss_ce: 0.011543
2022-01-08 02:07:28,007 iteration 4532 : loss : 0.012265, loss_ce: 0.004428
2022-01-08 02:07:29,511 iteration 4533 : loss : 0.022832, loss_ce: 0.008641
2022-01-08 02:07:30,839 iteration 4534 : loss : 0.013728, loss_ce: 0.006406
2022-01-08 02:07:32,231 iteration 4535 : loss : 0.036138, loss_ce: 0.017234
2022-01-08 02:07:33,596 iteration 4536 : loss : 0.014388, loss_ce: 0.004393
2022-01-08 02:07:34,946 iteration 4537 : loss : 0.033535, loss_ce: 0.010545
2022-01-08 02:07:36,318 iteration 4538 : loss : 0.014201, loss_ce: 0.005631
2022-01-08 02:07:37,599 iteration 4539 : loss : 0.012978, loss_ce: 0.004923
 67%|███████████████████▎         | 267/400 [1:53:00<55:14, 24.92s/it]2022-01-08 02:07:38,954 iteration 4540 : loss : 0.014346, loss_ce: 0.005730
2022-01-08 02:07:40,314 iteration 4541 : loss : 0.020585, loss_ce: 0.007562
2022-01-08 02:07:41,680 iteration 4542 : loss : 0.039162, loss_ce: 0.012007
2022-01-08 02:07:43,198 iteration 4543 : loss : 0.025019, loss_ce: 0.009823
2022-01-08 02:07:44,503 iteration 4544 : loss : 0.015040, loss_ce: 0.006177
2022-01-08 02:07:45,822 iteration 4545 : loss : 0.014262, loss_ce: 0.005853
2022-01-08 02:07:47,184 iteration 4546 : loss : 0.021472, loss_ce: 0.007454
2022-01-08 02:07:48,561 iteration 4547 : loss : 0.020691, loss_ce: 0.007604
2022-01-08 02:07:49,899 iteration 4548 : loss : 0.021294, loss_ce: 0.009442
2022-01-08 02:07:51,183 iteration 4549 : loss : 0.018207, loss_ce: 0.005256
2022-01-08 02:07:52,627 iteration 4550 : loss : 0.025406, loss_ce: 0.008761
2022-01-08 02:07:54,057 iteration 4551 : loss : 0.028971, loss_ce: 0.008812
2022-01-08 02:07:55,427 iteration 4552 : loss : 0.017337, loss_ce: 0.006481
2022-01-08 02:07:56,777 iteration 4553 : loss : 0.012774, loss_ce: 0.003982
2022-01-08 02:07:58,184 iteration 4554 : loss : 0.029886, loss_ce: 0.007066
2022-01-08 02:07:59,494 iteration 4555 : loss : 0.020565, loss_ce: 0.006809
2022-01-08 02:08:00,857 iteration 4556 : loss : 0.019601, loss_ce: 0.008759
 67%|███████████████████▍         | 268/400 [1:53:24<53:43, 24.42s/it]2022-01-08 02:08:02,251 iteration 4557 : loss : 0.017611, loss_ce: 0.008774
2022-01-08 02:08:03,629 iteration 4558 : loss : 0.017755, loss_ce: 0.007503
2022-01-08 02:08:05,036 iteration 4559 : loss : 0.016138, loss_ce: 0.004899
2022-01-08 02:08:06,396 iteration 4560 : loss : 0.023257, loss_ce: 0.007742
2022-01-08 02:08:07,788 iteration 4561 : loss : 0.026896, loss_ce: 0.006712
2022-01-08 02:08:09,116 iteration 4562 : loss : 0.019627, loss_ce: 0.005601
2022-01-08 02:08:10,499 iteration 4563 : loss : 0.019186, loss_ce: 0.008885
2022-01-08 02:08:11,896 iteration 4564 : loss : 0.031176, loss_ce: 0.011730
2022-01-08 02:08:13,340 iteration 4565 : loss : 0.028720, loss_ce: 0.009326
2022-01-08 02:08:14,741 iteration 4566 : loss : 0.021420, loss_ce: 0.008979
2022-01-08 02:08:16,092 iteration 4567 : loss : 0.016221, loss_ce: 0.006238
2022-01-08 02:08:17,440 iteration 4568 : loss : 0.020989, loss_ce: 0.007342
2022-01-08 02:08:18,887 iteration 4569 : loss : 0.026636, loss_ce: 0.004742
2022-01-08 02:08:20,357 iteration 4570 : loss : 0.040325, loss_ce: 0.017166
2022-01-08 02:08:21,706 iteration 4571 : loss : 0.017600, loss_ce: 0.007012
2022-01-08 02:08:23,011 iteration 4572 : loss : 0.016019, loss_ce: 0.005806
2022-01-08 02:08:24,421 iteration 4573 : loss : 0.021780, loss_ce: 0.009595
 67%|███████████████████▌         | 269/400 [1:53:47<52:45, 24.17s/it]2022-01-08 02:08:25,752 iteration 4574 : loss : 0.014830, loss_ce: 0.005891
2022-01-08 02:08:27,171 iteration 4575 : loss : 0.016831, loss_ce: 0.005397
2022-01-08 02:08:28,647 iteration 4576 : loss : 0.023676, loss_ce: 0.008610
2022-01-08 02:08:30,028 iteration 4577 : loss : 0.015985, loss_ce: 0.005178
2022-01-08 02:08:31,312 iteration 4578 : loss : 0.018342, loss_ce: 0.008166
2022-01-08 02:08:32,741 iteration 4579 : loss : 0.014523, loss_ce: 0.004773
2022-01-08 02:08:34,113 iteration 4580 : loss : 0.016578, loss_ce: 0.006895
2022-01-08 02:08:35,477 iteration 4581 : loss : 0.019267, loss_ce: 0.008262
2022-01-08 02:08:36,829 iteration 4582 : loss : 0.016777, loss_ce: 0.006882
2022-01-08 02:08:38,289 iteration 4583 : loss : 0.033261, loss_ce: 0.007155
2022-01-08 02:08:39,653 iteration 4584 : loss : 0.021506, loss_ce: 0.006333
2022-01-08 02:08:40,961 iteration 4585 : loss : 0.013696, loss_ce: 0.004641
2022-01-08 02:08:42,310 iteration 4586 : loss : 0.014660, loss_ce: 0.006823
2022-01-08 02:08:43,665 iteration 4587 : loss : 0.016499, loss_ce: 0.006420
2022-01-08 02:08:45,101 iteration 4588 : loss : 0.021131, loss_ce: 0.008950
2022-01-08 02:08:46,459 iteration 4589 : loss : 0.020913, loss_ce: 0.011048
2022-01-08 02:08:46,460 Training Data Eval:
2022-01-08 02:08:53,320   Average segmentation loss on training set: 0.0111
2022-01-08 02:08:53,321 Validation Data Eval:
2022-01-08 02:08:55,699   Average segmentation loss on validation set: 0.0928
2022-01-08 02:08:57,122 iteration 4590 : loss : 0.016627, loss_ce: 0.008451
 68%|███████████████████▌         | 270/400 [1:54:20<57:54, 26.73s/it]2022-01-08 02:08:58,671 iteration 4591 : loss : 0.021884, loss_ce: 0.008261
2022-01-08 02:09:00,034 iteration 4592 : loss : 0.018018, loss_ce: 0.007932
2022-01-08 02:09:01,391 iteration 4593 : loss : 0.026714, loss_ce: 0.009760
2022-01-08 02:09:02,772 iteration 4594 : loss : 0.021775, loss_ce: 0.008069
2022-01-08 02:09:04,140 iteration 4595 : loss : 0.013089, loss_ce: 0.005458
2022-01-08 02:09:05,610 iteration 4596 : loss : 0.018560, loss_ce: 0.007711
2022-01-08 02:09:06,921 iteration 4597 : loss : 0.024700, loss_ce: 0.008536
2022-01-08 02:09:08,199 iteration 4598 : loss : 0.013855, loss_ce: 0.004146
2022-01-08 02:09:09,546 iteration 4599 : loss : 0.018034, loss_ce: 0.007267
2022-01-08 02:09:10,933 iteration 4600 : loss : 0.017693, loss_ce: 0.006102
2022-01-08 02:09:12,343 iteration 4601 : loss : 0.018303, loss_ce: 0.008815
2022-01-08 02:09:13,810 iteration 4602 : loss : 0.026786, loss_ce: 0.011492
2022-01-08 02:09:15,260 iteration 4603 : loss : 0.022019, loss_ce: 0.007411
2022-01-08 02:09:16,577 iteration 4604 : loss : 0.016881, loss_ce: 0.005921
2022-01-08 02:09:17,939 iteration 4605 : loss : 0.012912, loss_ce: 0.003826
2022-01-08 02:09:19,282 iteration 4606 : loss : 0.027335, loss_ce: 0.009120
2022-01-08 02:09:20,632 iteration 4607 : loss : 0.020729, loss_ce: 0.008565
 68%|███████████████████▋         | 271/400 [1:54:43<55:22, 25.76s/it]2022-01-08 02:09:22,069 iteration 4608 : loss : 0.016100, loss_ce: 0.006009
2022-01-08 02:09:23,461 iteration 4609 : loss : 0.019574, loss_ce: 0.005614
2022-01-08 02:09:24,848 iteration 4610 : loss : 0.019774, loss_ce: 0.004750
2022-01-08 02:09:26,273 iteration 4611 : loss : 0.020963, loss_ce: 0.009784
2022-01-08 02:09:27,650 iteration 4612 : loss : 0.018577, loss_ce: 0.007956
2022-01-08 02:09:29,023 iteration 4613 : loss : 0.017316, loss_ce: 0.005999
2022-01-08 02:09:30,384 iteration 4614 : loss : 0.013360, loss_ce: 0.004150
2022-01-08 02:09:31,874 iteration 4615 : loss : 0.027516, loss_ce: 0.012126
2022-01-08 02:09:33,182 iteration 4616 : loss : 0.016903, loss_ce: 0.008321
2022-01-08 02:09:34,577 iteration 4617 : loss : 0.019294, loss_ce: 0.007059
2022-01-08 02:09:35,923 iteration 4618 : loss : 0.009637, loss_ce: 0.003797
2022-01-08 02:09:37,317 iteration 4619 : loss : 0.017493, loss_ce: 0.006994
2022-01-08 02:09:38,661 iteration 4620 : loss : 0.015940, loss_ce: 0.006531
2022-01-08 02:09:40,006 iteration 4621 : loss : 0.015862, loss_ce: 0.005448
2022-01-08 02:09:41,393 iteration 4622 : loss : 0.017574, loss_ce: 0.006846
2022-01-08 02:09:42,775 iteration 4623 : loss : 0.020962, loss_ce: 0.007925
2022-01-08 02:09:44,113 iteration 4624 : loss : 0.028628, loss_ce: 0.008301
 68%|███████████████████▋         | 272/400 [1:55:07<53:29, 25.08s/it]2022-01-08 02:09:45,586 iteration 4625 : loss : 0.022396, loss_ce: 0.007226
2022-01-08 02:09:46,988 iteration 4626 : loss : 0.020314, loss_ce: 0.006607
2022-01-08 02:09:48,326 iteration 4627 : loss : 0.021834, loss_ce: 0.009544
2022-01-08 02:09:49,687 iteration 4628 : loss : 0.021233, loss_ce: 0.007597
2022-01-08 02:09:50,998 iteration 4629 : loss : 0.025590, loss_ce: 0.004334
2022-01-08 02:09:52,383 iteration 4630 : loss : 0.013987, loss_ce: 0.008230
2022-01-08 02:09:53,730 iteration 4631 : loss : 0.023969, loss_ce: 0.009697
2022-01-08 02:09:55,117 iteration 4632 : loss : 0.028868, loss_ce: 0.008810
2022-01-08 02:09:56,505 iteration 4633 : loss : 0.016693, loss_ce: 0.005527
2022-01-08 02:09:57,941 iteration 4634 : loss : 0.018376, loss_ce: 0.007251
2022-01-08 02:09:59,199 iteration 4635 : loss : 0.013415, loss_ce: 0.003965
2022-01-08 02:10:00,500 iteration 4636 : loss : 0.013601, loss_ce: 0.004872
2022-01-08 02:10:01,876 iteration 4637 : loss : 0.018986, loss_ce: 0.006048
2022-01-08 02:10:03,244 iteration 4638 : loss : 0.023325, loss_ce: 0.011182
2022-01-08 02:10:04,646 iteration 4639 : loss : 0.022769, loss_ce: 0.007577
2022-01-08 02:10:06,044 iteration 4640 : loss : 0.016844, loss_ce: 0.007326
2022-01-08 02:10:07,402 iteration 4641 : loss : 0.017929, loss_ce: 0.006593
 68%|███████████████████▊         | 273/400 [1:55:30<51:56, 24.54s/it]2022-01-08 02:10:08,793 iteration 4642 : loss : 0.022189, loss_ce: 0.009720
2022-01-08 02:10:10,168 iteration 4643 : loss : 0.029927, loss_ce: 0.010764
2022-01-08 02:10:11,551 iteration 4644 : loss : 0.029661, loss_ce: 0.012796
2022-01-08 02:10:12,880 iteration 4645 : loss : 0.019113, loss_ce: 0.007016
2022-01-08 02:10:14,244 iteration 4646 : loss : 0.019568, loss_ce: 0.008821
2022-01-08 02:10:15,611 iteration 4647 : loss : 0.015111, loss_ce: 0.005938
2022-01-08 02:10:16,928 iteration 4648 : loss : 0.013226, loss_ce: 0.005853
2022-01-08 02:10:18,408 iteration 4649 : loss : 0.019043, loss_ce: 0.007324
2022-01-08 02:10:19,668 iteration 4650 : loss : 0.015117, loss_ce: 0.006346
2022-01-08 02:10:21,051 iteration 4651 : loss : 0.021470, loss_ce: 0.008041
2022-01-08 02:10:22,406 iteration 4652 : loss : 0.019070, loss_ce: 0.008609
2022-01-08 02:10:23,687 iteration 4653 : loss : 0.013775, loss_ce: 0.005475
2022-01-08 02:10:24,989 iteration 4654 : loss : 0.015672, loss_ce: 0.004661
2022-01-08 02:10:26,414 iteration 4655 : loss : 0.028875, loss_ce: 0.010774
2022-01-08 02:10:27,776 iteration 4656 : loss : 0.018288, loss_ce: 0.006640
2022-01-08 02:10:29,130 iteration 4657 : loss : 0.017077, loss_ce: 0.008073
2022-01-08 02:10:30,483 iteration 4658 : loss : 0.014121, loss_ce: 0.005303
 68%|███████████████████▊         | 274/400 [1:55:53<50:36, 24.10s/it]2022-01-08 02:10:31,905 iteration 4659 : loss : 0.020670, loss_ce: 0.007445
2022-01-08 02:10:33,228 iteration 4660 : loss : 0.012460, loss_ce: 0.004888
2022-01-08 02:10:34,631 iteration 4661 : loss : 0.021415, loss_ce: 0.008472
2022-01-08 02:10:35,890 iteration 4662 : loss : 0.011894, loss_ce: 0.005646
2022-01-08 02:10:37,338 iteration 4663 : loss : 0.017454, loss_ce: 0.006417
2022-01-08 02:10:38,699 iteration 4664 : loss : 0.015381, loss_ce: 0.005610
2022-01-08 02:10:40,063 iteration 4665 : loss : 0.024683, loss_ce: 0.009722
2022-01-08 02:10:41,484 iteration 4666 : loss : 0.020223, loss_ce: 0.008032
2022-01-08 02:10:42,905 iteration 4667 : loss : 0.022218, loss_ce: 0.006881
2022-01-08 02:10:44,248 iteration 4668 : loss : 0.012754, loss_ce: 0.005717
2022-01-08 02:10:45,653 iteration 4669 : loss : 0.020074, loss_ce: 0.007670
2022-01-08 02:10:47,022 iteration 4670 : loss : 0.049858, loss_ce: 0.015903
2022-01-08 02:10:48,338 iteration 4671 : loss : 0.015071, loss_ce: 0.005485
2022-01-08 02:10:49,678 iteration 4672 : loss : 0.011141, loss_ce: 0.003907
2022-01-08 02:10:50,977 iteration 4673 : loss : 0.012563, loss_ce: 0.004217
2022-01-08 02:10:52,371 iteration 4674 : loss : 0.012417, loss_ce: 0.002914
2022-01-08 02:10:52,371 Training Data Eval:
2022-01-08 02:10:59,263   Average segmentation loss on training set: 0.0117
2022-01-08 02:10:59,263 Validation Data Eval:
2022-01-08 02:11:01,640   Average segmentation loss on validation set: 0.0957
2022-01-08 02:11:02,942 iteration 4675 : loss : 0.012125, loss_ce: 0.004679
 69%|███████████████████▉         | 275/400 [1:56:26<55:26, 26.61s/it]2022-01-08 02:11:04,358 iteration 4676 : loss : 0.012028, loss_ce: 0.004759
2022-01-08 02:11:05,687 iteration 4677 : loss : 0.012726, loss_ce: 0.005221
2022-01-08 02:11:07,097 iteration 4678 : loss : 0.019423, loss_ce: 0.006228
2022-01-08 02:11:08,545 iteration 4679 : loss : 0.018725, loss_ce: 0.006809
2022-01-08 02:11:09,830 iteration 4680 : loss : 0.016959, loss_ce: 0.006571
2022-01-08 02:11:11,189 iteration 4681 : loss : 0.015440, loss_ce: 0.006538
2022-01-08 02:11:12,520 iteration 4682 : loss : 0.017299, loss_ce: 0.006991
2022-01-08 02:11:13,843 iteration 4683 : loss : 0.017340, loss_ce: 0.006277
2022-01-08 02:11:15,207 iteration 4684 : loss : 0.028317, loss_ce: 0.010925
2022-01-08 02:11:16,616 iteration 4685 : loss : 0.013916, loss_ce: 0.006179
2022-01-08 02:11:17,920 iteration 4686 : loss : 0.018797, loss_ce: 0.006691
2022-01-08 02:11:19,273 iteration 4687 : loss : 0.021755, loss_ce: 0.006650
2022-01-08 02:11:20,549 iteration 4688 : loss : 0.011883, loss_ce: 0.005406
2022-01-08 02:11:21,939 iteration 4689 : loss : 0.022574, loss_ce: 0.007935
2022-01-08 02:11:23,215 iteration 4690 : loss : 0.012713, loss_ce: 0.004375
2022-01-08 02:11:24,575 iteration 4691 : loss : 0.023765, loss_ce: 0.006673
2022-01-08 02:11:26,045 iteration 4692 : loss : 0.024766, loss_ce: 0.010558
 69%|████████████████████         | 276/400 [1:56:49<52:49, 25.56s/it]2022-01-08 02:11:27,420 iteration 4693 : loss : 0.025271, loss_ce: 0.012779
2022-01-08 02:11:28,750 iteration 4694 : loss : 0.018050, loss_ce: 0.005877
2022-01-08 02:11:30,114 iteration 4695 : loss : 0.022041, loss_ce: 0.007513
2022-01-08 02:11:31,497 iteration 4696 : loss : 0.034516, loss_ce: 0.010110
2022-01-08 02:11:32,807 iteration 4697 : loss : 0.019492, loss_ce: 0.007884
2022-01-08 02:11:34,127 iteration 4698 : loss : 0.021303, loss_ce: 0.010123
2022-01-08 02:11:35,467 iteration 4699 : loss : 0.024586, loss_ce: 0.008707
2022-01-08 02:11:36,822 iteration 4700 : loss : 0.020921, loss_ce: 0.008555
2022-01-08 02:11:38,162 iteration 4701 : loss : 0.014589, loss_ce: 0.004112
2022-01-08 02:11:39,592 iteration 4702 : loss : 0.036171, loss_ce: 0.012764
2022-01-08 02:11:40,962 iteration 4703 : loss : 0.022714, loss_ce: 0.011236
2022-01-08 02:11:42,352 iteration 4704 : loss : 0.019873, loss_ce: 0.004894
2022-01-08 02:11:43,732 iteration 4705 : loss : 0.027167, loss_ce: 0.006557
2022-01-08 02:11:45,018 iteration 4706 : loss : 0.011762, loss_ce: 0.004848
2022-01-08 02:11:46,358 iteration 4707 : loss : 0.021534, loss_ce: 0.006203
2022-01-08 02:11:47,720 iteration 4708 : loss : 0.016417, loss_ce: 0.007371
2022-01-08 02:11:49,060 iteration 4709 : loss : 0.025778, loss_ce: 0.009820
 69%|████████████████████         | 277/400 [1:57:12<50:49, 24.79s/it]2022-01-08 02:11:50,421 iteration 4710 : loss : 0.020366, loss_ce: 0.008516
2022-01-08 02:11:51,738 iteration 4711 : loss : 0.015122, loss_ce: 0.005676
2022-01-08 02:11:53,022 iteration 4712 : loss : 0.012574, loss_ce: 0.004431
2022-01-08 02:11:54,388 iteration 4713 : loss : 0.021766, loss_ce: 0.006935
2022-01-08 02:11:55,678 iteration 4714 : loss : 0.015716, loss_ce: 0.006271
2022-01-08 02:11:57,064 iteration 4715 : loss : 0.018092, loss_ce: 0.008709
2022-01-08 02:11:58,476 iteration 4716 : loss : 0.021959, loss_ce: 0.008786
2022-01-08 02:11:59,811 iteration 4717 : loss : 0.018662, loss_ce: 0.006409
2022-01-08 02:12:01,185 iteration 4718 : loss : 0.017138, loss_ce: 0.006998
2022-01-08 02:12:02,511 iteration 4719 : loss : 0.011948, loss_ce: 0.004237
2022-01-08 02:12:03,860 iteration 4720 : loss : 0.013616, loss_ce: 0.006725
2022-01-08 02:12:05,197 iteration 4721 : loss : 0.018789, loss_ce: 0.006333
2022-01-08 02:12:06,615 iteration 4722 : loss : 0.016196, loss_ce: 0.005827
2022-01-08 02:12:08,051 iteration 4723 : loss : 0.024171, loss_ce: 0.009263
2022-01-08 02:12:09,494 iteration 4724 : loss : 0.027093, loss_ce: 0.017687
2022-01-08 02:12:10,823 iteration 4725 : loss : 0.013018, loss_ce: 0.004567
2022-01-08 02:12:12,230 iteration 4726 : loss : 0.017952, loss_ce: 0.005455
 70%|████████████████████▏        | 278/400 [1:57:35<49:25, 24.31s/it]2022-01-08 02:12:13,649 iteration 4727 : loss : 0.020493, loss_ce: 0.007480
2022-01-08 02:12:15,020 iteration 4728 : loss : 0.014059, loss_ce: 0.005112
2022-01-08 02:12:16,341 iteration 4729 : loss : 0.014244, loss_ce: 0.004478
2022-01-08 02:12:17,707 iteration 4730 : loss : 0.024510, loss_ce: 0.011341
2022-01-08 02:12:19,073 iteration 4731 : loss : 0.017430, loss_ce: 0.004784
2022-01-08 02:12:20,446 iteration 4732 : loss : 0.019831, loss_ce: 0.008939
2022-01-08 02:12:21,753 iteration 4733 : loss : 0.013150, loss_ce: 0.004184
2022-01-08 02:12:23,056 iteration 4734 : loss : 0.015190, loss_ce: 0.005374
2022-01-08 02:12:24,484 iteration 4735 : loss : 0.021608, loss_ce: 0.011356
2022-01-08 02:12:25,855 iteration 4736 : loss : 0.019648, loss_ce: 0.008065
2022-01-08 02:12:27,314 iteration 4737 : loss : 0.029696, loss_ce: 0.008844
2022-01-08 02:12:28,751 iteration 4738 : loss : 0.033083, loss_ce: 0.006502
2022-01-08 02:12:30,089 iteration 4739 : loss : 0.013751, loss_ce: 0.005321
2022-01-08 02:12:31,383 iteration 4740 : loss : 0.012485, loss_ce: 0.005410
2022-01-08 02:12:32,729 iteration 4741 : loss : 0.020493, loss_ce: 0.012444
2022-01-08 02:12:34,001 iteration 4742 : loss : 0.012939, loss_ce: 0.004528
2022-01-08 02:12:35,373 iteration 4743 : loss : 0.012629, loss_ce: 0.004228
 70%|████████████████████▏        | 279/400 [1:57:58<48:19, 23.96s/it]2022-01-08 02:12:36,795 iteration 4744 : loss : 0.016396, loss_ce: 0.006526
2022-01-08 02:12:38,164 iteration 4745 : loss : 0.017580, loss_ce: 0.007301
2022-01-08 02:12:39,508 iteration 4746 : loss : 0.018060, loss_ce: 0.007604
2022-01-08 02:12:40,922 iteration 4747 : loss : 0.027203, loss_ce: 0.014865
2022-01-08 02:12:42,381 iteration 4748 : loss : 0.024197, loss_ce: 0.010736
2022-01-08 02:12:43,804 iteration 4749 : loss : 0.017031, loss_ce: 0.004830
2022-01-08 02:12:45,158 iteration 4750 : loss : 0.015116, loss_ce: 0.006149
2022-01-08 02:12:46,459 iteration 4751 : loss : 0.017280, loss_ce: 0.006061
2022-01-08 02:12:47,820 iteration 4752 : loss : 0.017047, loss_ce: 0.004491
2022-01-08 02:12:49,204 iteration 4753 : loss : 0.015685, loss_ce: 0.007840
2022-01-08 02:12:50,575 iteration 4754 : loss : 0.023178, loss_ce: 0.007758
2022-01-08 02:12:51,973 iteration 4755 : loss : 0.016054, loss_ce: 0.005740
2022-01-08 02:12:53,353 iteration 4756 : loss : 0.017483, loss_ce: 0.007589
2022-01-08 02:12:54,725 iteration 4757 : loss : 0.016908, loss_ce: 0.007025
2022-01-08 02:12:56,177 iteration 4758 : loss : 0.023143, loss_ce: 0.007649
2022-01-08 02:12:57,474 iteration 4759 : loss : 0.010659, loss_ce: 0.004026
2022-01-08 02:12:57,474 Training Data Eval:
2022-01-08 02:13:04,371   Average segmentation loss on training set: 0.0102
2022-01-08 02:13:04,372 Validation Data Eval:
2022-01-08 02:13:06,751   Average segmentation loss on validation set: 0.0652
2022-01-08 02:13:08,129 iteration 4760 : loss : 0.034724, loss_ce: 0.009862
 70%|████████████████████▎        | 280/400 [1:58:31<53:11, 26.60s/it]2022-01-08 02:13:09,550 iteration 4761 : loss : 0.018098, loss_ce: 0.009517
2022-01-08 02:13:10,956 iteration 4762 : loss : 0.020848, loss_ce: 0.005774
2022-01-08 02:13:12,326 iteration 4763 : loss : 0.021021, loss_ce: 0.006929
2022-01-08 02:13:13,747 iteration 4764 : loss : 0.022534, loss_ce: 0.006478
2022-01-08 02:13:15,155 iteration 4765 : loss : 0.015051, loss_ce: 0.005651
2022-01-08 02:13:16,656 iteration 4766 : loss : 0.018007, loss_ce: 0.005067
2022-01-08 02:13:18,146 iteration 4767 : loss : 0.021625, loss_ce: 0.010957
2022-01-08 02:13:19,518 iteration 4768 : loss : 0.016922, loss_ce: 0.006812
2022-01-08 02:13:20,812 iteration 4769 : loss : 0.013353, loss_ce: 0.004720
2022-01-08 02:13:22,229 iteration 4770 : loss : 0.024462, loss_ce: 0.011585
2022-01-08 02:13:23,589 iteration 4771 : loss : 0.024333, loss_ce: 0.007883
2022-01-08 02:13:24,916 iteration 4772 : loss : 0.015812, loss_ce: 0.006054
2022-01-08 02:13:26,213 iteration 4773 : loss : 0.012518, loss_ce: 0.005676
2022-01-08 02:13:27,618 iteration 4774 : loss : 0.025774, loss_ce: 0.009542
2022-01-08 02:13:28,915 iteration 4775 : loss : 0.016050, loss_ce: 0.006527
2022-01-08 02:13:30,272 iteration 4776 : loss : 0.013508, loss_ce: 0.004517
2022-01-08 02:13:31,618 iteration 4777 : loss : 0.013990, loss_ce: 0.004596
 70%|████████████████████▎        | 281/400 [1:58:54<50:53, 25.66s/it]2022-01-08 02:13:33,029 iteration 4778 : loss : 0.017727, loss_ce: 0.008825
2022-01-08 02:13:34,355 iteration 4779 : loss : 0.012596, loss_ce: 0.005401
2022-01-08 02:13:35,682 iteration 4780 : loss : 0.015488, loss_ce: 0.006777
2022-01-08 02:13:37,087 iteration 4781 : loss : 0.016185, loss_ce: 0.005150
2022-01-08 02:13:38,498 iteration 4782 : loss : 0.018436, loss_ce: 0.007285
2022-01-08 02:13:39,847 iteration 4783 : loss : 0.016232, loss_ce: 0.007724
2022-01-08 02:13:41,258 iteration 4784 : loss : 0.026401, loss_ce: 0.008473
2022-01-08 02:13:42,611 iteration 4785 : loss : 0.016099, loss_ce: 0.005647
2022-01-08 02:13:43,966 iteration 4786 : loss : 0.011685, loss_ce: 0.004790
2022-01-08 02:13:45,317 iteration 4787 : loss : 0.027479, loss_ce: 0.008850
2022-01-08 02:13:46,697 iteration 4788 : loss : 0.021106, loss_ce: 0.006318
2022-01-08 02:13:47,967 iteration 4789 : loss : 0.012591, loss_ce: 0.004809
2022-01-08 02:13:49,244 iteration 4790 : loss : 0.012155, loss_ce: 0.004408
2022-01-08 02:13:50,635 iteration 4791 : loss : 0.034591, loss_ce: 0.019105
2022-01-08 02:13:52,019 iteration 4792 : loss : 0.014378, loss_ce: 0.005288
2022-01-08 02:13:53,452 iteration 4793 : loss : 0.019033, loss_ce: 0.006556
2022-01-08 02:13:54,805 iteration 4794 : loss : 0.014995, loss_ce: 0.004174
 70%|████████████████████▍        | 282/400 [1:59:18<49:00, 24.92s/it]2022-01-08 02:13:56,242 iteration 4795 : loss : 0.024554, loss_ce: 0.008088
2022-01-08 02:13:57,630 iteration 4796 : loss : 0.015799, loss_ce: 0.004748
2022-01-08 02:13:58,991 iteration 4797 : loss : 0.012996, loss_ce: 0.004662
2022-01-08 02:14:00,377 iteration 4798 : loss : 0.019370, loss_ce: 0.008159
2022-01-08 02:14:01,711 iteration 4799 : loss : 0.016651, loss_ce: 0.004021
2022-01-08 02:14:03,099 iteration 4800 : loss : 0.018668, loss_ce: 0.007031
2022-01-08 02:14:04,444 iteration 4801 : loss : 0.014296, loss_ce: 0.005956
2022-01-08 02:14:05,872 iteration 4802 : loss : 0.014358, loss_ce: 0.005551
2022-01-08 02:14:07,217 iteration 4803 : loss : 0.015441, loss_ce: 0.007044
2022-01-08 02:14:08,609 iteration 4804 : loss : 0.019224, loss_ce: 0.008103
2022-01-08 02:14:10,059 iteration 4805 : loss : 0.022204, loss_ce: 0.009137
2022-01-08 02:14:11,462 iteration 4806 : loss : 0.023312, loss_ce: 0.005038
2022-01-08 02:14:12,766 iteration 4807 : loss : 0.014451, loss_ce: 0.004110
2022-01-08 02:14:14,178 iteration 4808 : loss : 0.019351, loss_ce: 0.007665
2022-01-08 02:14:15,500 iteration 4809 : loss : 0.013920, loss_ce: 0.006080
2022-01-08 02:14:16,863 iteration 4810 : loss : 0.016933, loss_ce: 0.006095
2022-01-08 02:14:18,262 iteration 4811 : loss : 0.020075, loss_ce: 0.007473
 71%|████████████████████▌        | 283/400 [1:59:41<47:44, 24.48s/it]2022-01-08 02:14:19,655 iteration 4812 : loss : 0.013363, loss_ce: 0.004786
2022-01-08 02:14:20,982 iteration 4813 : loss : 0.012776, loss_ce: 0.005168
2022-01-08 02:14:22,477 iteration 4814 : loss : 0.022261, loss_ce: 0.009050
2022-01-08 02:14:23,819 iteration 4815 : loss : 0.012847, loss_ce: 0.004525
2022-01-08 02:14:25,140 iteration 4816 : loss : 0.019464, loss_ce: 0.007159
2022-01-08 02:14:26,463 iteration 4817 : loss : 0.011937, loss_ce: 0.004696
2022-01-08 02:14:27,793 iteration 4818 : loss : 0.014557, loss_ce: 0.005283
2022-01-08 02:14:29,174 iteration 4819 : loss : 0.017380, loss_ce: 0.005176
2022-01-08 02:14:30,508 iteration 4820 : loss : 0.015028, loss_ce: 0.006271
2022-01-08 02:14:31,884 iteration 4821 : loss : 0.012892, loss_ce: 0.004618
2022-01-08 02:14:33,304 iteration 4822 : loss : 0.025635, loss_ce: 0.008940
2022-01-08 02:14:34,674 iteration 4823 : loss : 0.026549, loss_ce: 0.006940
2022-01-08 02:14:36,009 iteration 4824 : loss : 0.015749, loss_ce: 0.004326
2022-01-08 02:14:37,430 iteration 4825 : loss : 0.017715, loss_ce: 0.009710
2022-01-08 02:14:38,955 iteration 4826 : loss : 0.020112, loss_ce: 0.009104
2022-01-08 02:14:40,285 iteration 4827 : loss : 0.017997, loss_ce: 0.009771
2022-01-08 02:14:41,604 iteration 4828 : loss : 0.012244, loss_ce: 0.004292
 71%|████████████████████▌        | 284/400 [2:00:04<46:39, 24.14s/it]2022-01-08 02:14:42,990 iteration 4829 : loss : 0.013091, loss_ce: 0.006309
2022-01-08 02:14:44,395 iteration 4830 : loss : 0.016918, loss_ce: 0.007239
2022-01-08 02:14:45,782 iteration 4831 : loss : 0.019037, loss_ce: 0.010637
2022-01-08 02:14:47,175 iteration 4832 : loss : 0.016167, loss_ce: 0.005521
2022-01-08 02:14:48,537 iteration 4833 : loss : 0.018673, loss_ce: 0.009758
2022-01-08 02:14:49,988 iteration 4834 : loss : 0.013771, loss_ce: 0.004923
2022-01-08 02:14:51,345 iteration 4835 : loss : 0.019580, loss_ce: 0.007410
2022-01-08 02:14:52,673 iteration 4836 : loss : 0.016318, loss_ce: 0.005047
2022-01-08 02:14:54,073 iteration 4837 : loss : 0.013932, loss_ce: 0.003919
2022-01-08 02:14:55,484 iteration 4838 : loss : 0.019044, loss_ce: 0.007568
2022-01-08 02:14:56,759 iteration 4839 : loss : 0.017631, loss_ce: 0.007489
2022-01-08 02:14:58,105 iteration 4840 : loss : 0.016162, loss_ce: 0.005310
2022-01-08 02:14:59,505 iteration 4841 : loss : 0.017311, loss_ce: 0.006426
2022-01-08 02:15:00,878 iteration 4842 : loss : 0.032732, loss_ce: 0.012853
2022-01-08 02:15:02,218 iteration 4843 : loss : 0.023740, loss_ce: 0.011830
2022-01-08 02:15:03,643 iteration 4844 : loss : 0.016871, loss_ce: 0.005162
2022-01-08 02:15:03,643 Training Data Eval:
2022-01-08 02:15:10,549   Average segmentation loss on training set: 0.0112
2022-01-08 02:15:10,549 Validation Data Eval:
2022-01-08 02:15:12,932   Average segmentation loss on validation set: 0.1053
2022-01-08 02:15:14,268 iteration 4845 : loss : 0.014251, loss_ce: 0.005111
 71%|████████████████████▋        | 285/400 [2:00:37<51:10, 26.70s/it]2022-01-08 02:15:15,677 iteration 4846 : loss : 0.016471, loss_ce: 0.006779
2022-01-08 02:15:16,988 iteration 4847 : loss : 0.011880, loss_ce: 0.003956
2022-01-08 02:15:18,445 iteration 4848 : loss : 0.036649, loss_ce: 0.015207
2022-01-08 02:15:19,784 iteration 4849 : loss : 0.018349, loss_ce: 0.006719
2022-01-08 02:15:21,072 iteration 4850 : loss : 0.011556, loss_ce: 0.005229
2022-01-08 02:15:22,534 iteration 4851 : loss : 0.018295, loss_ce: 0.005573
2022-01-08 02:15:23,892 iteration 4852 : loss : 0.015926, loss_ce: 0.007777
2022-01-08 02:15:25,287 iteration 4853 : loss : 0.027195, loss_ce: 0.005611
2022-01-08 02:15:26,792 iteration 4854 : loss : 0.020894, loss_ce: 0.006711
2022-01-08 02:15:28,169 iteration 4855 : loss : 0.024021, loss_ce: 0.007742
2022-01-08 02:15:29,473 iteration 4856 : loss : 0.012244, loss_ce: 0.003951
2022-01-08 02:15:30,894 iteration 4857 : loss : 0.020350, loss_ce: 0.011879
2022-01-08 02:15:32,290 iteration 4858 : loss : 0.022802, loss_ce: 0.009599
2022-01-08 02:15:33,653 iteration 4859 : loss : 0.015392, loss_ce: 0.006379
2022-01-08 02:15:35,042 iteration 4860 : loss : 0.026604, loss_ce: 0.009599
2022-01-08 02:15:36,418 iteration 4861 : loss : 0.015523, loss_ce: 0.006115
2022-01-08 02:15:37,735 iteration 4862 : loss : 0.013531, loss_ce: 0.005306
 72%|████████████████████▋        | 286/400 [2:01:01<48:53, 25.73s/it]2022-01-08 02:15:39,100 iteration 4863 : loss : 0.014687, loss_ce: 0.004870
2022-01-08 02:15:40,523 iteration 4864 : loss : 0.020551, loss_ce: 0.007308
2022-01-08 02:15:41,988 iteration 4865 : loss : 0.018663, loss_ce: 0.004398
2022-01-08 02:15:43,345 iteration 4866 : loss : 0.012272, loss_ce: 0.004727
2022-01-08 02:15:44,712 iteration 4867 : loss : 0.025681, loss_ce: 0.012120
2022-01-08 02:15:46,044 iteration 4868 : loss : 0.017723, loss_ce: 0.008896
2022-01-08 02:15:47,377 iteration 4869 : loss : 0.016845, loss_ce: 0.006626
2022-01-08 02:15:48,829 iteration 4870 : loss : 0.028591, loss_ce: 0.010455
2022-01-08 02:15:50,210 iteration 4871 : loss : 0.012271, loss_ce: 0.004856
2022-01-08 02:15:51,526 iteration 4872 : loss : 0.015412, loss_ce: 0.005720
2022-01-08 02:15:52,827 iteration 4873 : loss : 0.013925, loss_ce: 0.004669
2022-01-08 02:15:54,120 iteration 4874 : loss : 0.011903, loss_ce: 0.005088
2022-01-08 02:15:55,498 iteration 4875 : loss : 0.012377, loss_ce: 0.003918
2022-01-08 02:15:56,805 iteration 4876 : loss : 0.015520, loss_ce: 0.004848
2022-01-08 02:15:58,270 iteration 4877 : loss : 0.021796, loss_ce: 0.008296
2022-01-08 02:15:59,622 iteration 4878 : loss : 0.016328, loss_ce: 0.005316
2022-01-08 02:16:00,977 iteration 4879 : loss : 0.014622, loss_ce: 0.005756
 72%|████████████████████▊        | 287/400 [2:01:24<47:02, 24.98s/it]2022-01-08 02:16:02,444 iteration 4880 : loss : 0.018482, loss_ce: 0.010187
2022-01-08 02:16:03,774 iteration 4881 : loss : 0.027162, loss_ce: 0.010008
2022-01-08 02:16:05,132 iteration 4882 : loss : 0.013911, loss_ce: 0.003713
2022-01-08 02:16:06,513 iteration 4883 : loss : 0.017925, loss_ce: 0.005984
2022-01-08 02:16:07,892 iteration 4884 : loss : 0.012124, loss_ce: 0.006603
2022-01-08 02:16:09,393 iteration 4885 : loss : 0.022502, loss_ce: 0.007914
2022-01-08 02:16:10,802 iteration 4886 : loss : 0.018893, loss_ce: 0.007573
2022-01-08 02:16:12,230 iteration 4887 : loss : 0.020091, loss_ce: 0.007224
2022-01-08 02:16:13,573 iteration 4888 : loss : 0.015624, loss_ce: 0.004254
2022-01-08 02:16:14,923 iteration 4889 : loss : 0.017357, loss_ce: 0.006820
2022-01-08 02:16:16,348 iteration 4890 : loss : 0.017635, loss_ce: 0.005669
2022-01-08 02:16:17,778 iteration 4891 : loss : 0.017083, loss_ce: 0.008919
2022-01-08 02:16:19,085 iteration 4892 : loss : 0.013521, loss_ce: 0.005479
2022-01-08 02:16:20,426 iteration 4893 : loss : 0.015147, loss_ce: 0.006881
2022-01-08 02:16:21,788 iteration 4894 : loss : 0.016334, loss_ce: 0.003638
2022-01-08 02:16:23,112 iteration 4895 : loss : 0.014191, loss_ce: 0.005825
2022-01-08 02:16:24,485 iteration 4896 : loss : 0.019109, loss_ce: 0.006046
 72%|████████████████████▉        | 288/400 [2:01:47<45:48, 24.54s/it]2022-01-08 02:16:25,866 iteration 4897 : loss : 0.014570, loss_ce: 0.005677
2022-01-08 02:16:27,269 iteration 4898 : loss : 0.020967, loss_ce: 0.008524
2022-01-08 02:16:28,516 iteration 4899 : loss : 0.013906, loss_ce: 0.005540
2022-01-08 02:16:29,971 iteration 4900 : loss : 0.022718, loss_ce: 0.008944
2022-01-08 02:16:31,322 iteration 4901 : loss : 0.011871, loss_ce: 0.003016
2022-01-08 02:16:32,763 iteration 4902 : loss : 0.023890, loss_ce: 0.009296
2022-01-08 02:16:34,209 iteration 4903 : loss : 0.030380, loss_ce: 0.016697
2022-01-08 02:16:35,645 iteration 4904 : loss : 0.025046, loss_ce: 0.011680
2022-01-08 02:16:37,043 iteration 4905 : loss : 0.034865, loss_ce: 0.014044
2022-01-08 02:16:38,440 iteration 4906 : loss : 0.031193, loss_ce: 0.012759
2022-01-08 02:16:39,713 iteration 4907 : loss : 0.012757, loss_ce: 0.005332
2022-01-08 02:16:41,034 iteration 4908 : loss : 0.014124, loss_ce: 0.005747
2022-01-08 02:16:42,403 iteration 4909 : loss : 0.015254, loss_ce: 0.004938
2022-01-08 02:16:43,766 iteration 4910 : loss : 0.013584, loss_ce: 0.005033
2022-01-08 02:16:45,093 iteration 4911 : loss : 0.018255, loss_ce: 0.006425
2022-01-08 02:16:46,543 iteration 4912 : loss : 0.024077, loss_ce: 0.007934
2022-01-08 02:16:47,894 iteration 4913 : loss : 0.019242, loss_ce: 0.006620
 72%|████████████████████▉        | 289/400 [2:02:11<44:46, 24.20s/it]2022-01-08 02:16:49,231 iteration 4914 : loss : 0.018125, loss_ce: 0.007851
2022-01-08 02:16:50,605 iteration 4915 : loss : 0.013509, loss_ce: 0.006347
2022-01-08 02:16:52,024 iteration 4916 : loss : 0.023732, loss_ce: 0.007370
2022-01-08 02:16:53,388 iteration 4917 : loss : 0.015408, loss_ce: 0.005211
2022-01-08 02:16:54,765 iteration 4918 : loss : 0.016543, loss_ce: 0.007425
2022-01-08 02:16:56,128 iteration 4919 : loss : 0.016173, loss_ce: 0.007767
2022-01-08 02:16:57,572 iteration 4920 : loss : 0.022821, loss_ce: 0.009406
2022-01-08 02:16:58,880 iteration 4921 : loss : 0.016370, loss_ce: 0.006303
2022-01-08 02:17:00,282 iteration 4922 : loss : 0.015634, loss_ce: 0.005666
2022-01-08 02:17:01,661 iteration 4923 : loss : 0.015629, loss_ce: 0.005606
2022-01-08 02:17:03,064 iteration 4924 : loss : 0.015203, loss_ce: 0.006468
2022-01-08 02:17:04,410 iteration 4925 : loss : 0.015568, loss_ce: 0.006087
2022-01-08 02:17:05,675 iteration 4926 : loss : 0.012437, loss_ce: 0.005703
2022-01-08 02:17:07,041 iteration 4927 : loss : 0.011736, loss_ce: 0.004341
2022-01-08 02:17:08,429 iteration 4928 : loss : 0.017711, loss_ce: 0.007985
2022-01-08 02:17:09,748 iteration 4929 : loss : 0.021614, loss_ce: 0.009233
2022-01-08 02:17:09,748 Training Data Eval:
2022-01-08 02:17:16,666   Average segmentation loss on training set: 0.0097
2022-01-08 02:17:16,666 Validation Data Eval:
2022-01-08 02:17:19,048   Average segmentation loss on validation set: 0.0760
2022-01-08 02:17:20,454 iteration 4930 : loss : 0.015009, loss_ce: 0.005044
 72%|█████████████████████        | 290/400 [2:02:43<48:58, 26.71s/it]2022-01-08 02:17:21,902 iteration 4931 : loss : 0.019775, loss_ce: 0.009493
2022-01-08 02:17:23,307 iteration 4932 : loss : 0.019648, loss_ce: 0.006187
2022-01-08 02:17:24,620 iteration 4933 : loss : 0.018423, loss_ce: 0.007623
2022-01-08 02:17:25,993 iteration 4934 : loss : 0.014284, loss_ce: 0.005020
2022-01-08 02:17:27,344 iteration 4935 : loss : 0.011688, loss_ce: 0.003168
2022-01-08 02:17:28,741 iteration 4936 : loss : 0.012996, loss_ce: 0.004784
2022-01-08 02:17:30,071 iteration 4937 : loss : 0.017888, loss_ce: 0.005437
2022-01-08 02:17:31,387 iteration 4938 : loss : 0.014328, loss_ce: 0.005507
2022-01-08 02:17:32,857 iteration 4939 : loss : 0.025479, loss_ce: 0.012991
2022-01-08 02:17:34,169 iteration 4940 : loss : 0.018025, loss_ce: 0.008068
2022-01-08 02:17:35,534 iteration 4941 : loss : 0.016612, loss_ce: 0.007208
2022-01-08 02:17:37,076 iteration 4942 : loss : 0.018172, loss_ce: 0.007276
2022-01-08 02:17:38,415 iteration 4943 : loss : 0.013568, loss_ce: 0.005335
2022-01-08 02:17:39,759 iteration 4944 : loss : 0.014742, loss_ce: 0.007307
2022-01-08 02:17:41,126 iteration 4945 : loss : 0.012798, loss_ce: 0.004955
2022-01-08 02:17:42,436 iteration 4946 : loss : 0.013734, loss_ce: 0.005007
2022-01-08 02:17:43,872 iteration 4947 : loss : 0.022839, loss_ce: 0.008262
 73%|█████████████████████        | 291/400 [2:03:07<46:43, 25.72s/it]2022-01-08 02:17:45,310 iteration 4948 : loss : 0.017721, loss_ce: 0.006058
2022-01-08 02:17:46,781 iteration 4949 : loss : 0.017222, loss_ce: 0.007109
2022-01-08 02:17:48,094 iteration 4950 : loss : 0.013452, loss_ce: 0.006880
2022-01-08 02:17:49,442 iteration 4951 : loss : 0.018411, loss_ce: 0.006809
2022-01-08 02:17:50,808 iteration 4952 : loss : 0.015377, loss_ce: 0.005461
2022-01-08 02:17:52,256 iteration 4953 : loss : 0.017044, loss_ce: 0.004871
2022-01-08 02:17:53,535 iteration 4954 : loss : 0.013528, loss_ce: 0.003929
2022-01-08 02:17:54,864 iteration 4955 : loss : 0.013268, loss_ce: 0.006126
2022-01-08 02:17:56,272 iteration 4956 : loss : 0.014916, loss_ce: 0.004601
2022-01-08 02:17:57,640 iteration 4957 : loss : 0.014007, loss_ce: 0.006144
2022-01-08 02:17:59,153 iteration 4958 : loss : 0.018810, loss_ce: 0.007215
2022-01-08 02:18:00,591 iteration 4959 : loss : 0.015932, loss_ce: 0.005684
2022-01-08 02:18:02,061 iteration 4960 : loss : 0.014855, loss_ce: 0.006514
2022-01-08 02:18:03,402 iteration 4961 : loss : 0.013784, loss_ce: 0.003153
2022-01-08 02:18:04,768 iteration 4962 : loss : 0.014832, loss_ce: 0.006975
2022-01-08 02:18:06,185 iteration 4963 : loss : 0.013966, loss_ce: 0.004838
2022-01-08 02:18:07,607 iteration 4964 : loss : 0.011016, loss_ce: 0.003742
 73%|█████████████████████▏       | 292/400 [2:03:30<45:13, 25.13s/it]2022-01-08 02:18:09,009 iteration 4965 : loss : 0.014984, loss_ce: 0.004557
2022-01-08 02:18:10,381 iteration 4966 : loss : 0.016308, loss_ce: 0.005077
2022-01-08 02:18:11,747 iteration 4967 : loss : 0.017738, loss_ce: 0.004814
2022-01-08 02:18:13,106 iteration 4968 : loss : 0.018778, loss_ce: 0.004050
2022-01-08 02:18:14,494 iteration 4969 : loss : 0.026544, loss_ce: 0.011444
2022-01-08 02:18:15,892 iteration 4970 : loss : 0.021011, loss_ce: 0.010368
2022-01-08 02:18:17,239 iteration 4971 : loss : 0.014755, loss_ce: 0.005944
2022-01-08 02:18:18,559 iteration 4972 : loss : 0.016586, loss_ce: 0.005314
2022-01-08 02:18:19,904 iteration 4973 : loss : 0.011652, loss_ce: 0.004039
2022-01-08 02:18:21,301 iteration 4974 : loss : 0.016647, loss_ce: 0.010546
2022-01-08 02:18:22,751 iteration 4975 : loss : 0.012349, loss_ce: 0.004310
2022-01-08 02:18:24,078 iteration 4976 : loss : 0.014460, loss_ce: 0.005851
2022-01-08 02:18:25,439 iteration 4977 : loss : 0.011754, loss_ce: 0.004686
2022-01-08 02:18:26,823 iteration 4978 : loss : 0.014545, loss_ce: 0.005780
2022-01-08 02:18:28,146 iteration 4979 : loss : 0.012151, loss_ce: 0.004332
2022-01-08 02:18:29,435 iteration 4980 : loss : 0.010732, loss_ce: 0.003487
2022-01-08 02:18:30,791 iteration 4981 : loss : 0.018175, loss_ce: 0.006411
 73%|█████████████████████▏       | 293/400 [2:03:54<43:46, 24.54s/it]2022-01-08 02:18:32,222 iteration 4982 : loss : 0.023840, loss_ce: 0.005745
2022-01-08 02:18:33,517 iteration 4983 : loss : 0.011104, loss_ce: 0.003442
2022-01-08 02:18:34,901 iteration 4984 : loss : 0.015631, loss_ce: 0.003930
2022-01-08 02:18:36,199 iteration 4985 : loss : 0.011615, loss_ce: 0.004315
2022-01-08 02:18:37,535 iteration 4986 : loss : 0.021261, loss_ce: 0.006140
2022-01-08 02:18:38,909 iteration 4987 : loss : 0.013817, loss_ce: 0.006562
2022-01-08 02:18:40,287 iteration 4988 : loss : 0.016550, loss_ce: 0.008138
2022-01-08 02:18:41,613 iteration 4989 : loss : 0.016983, loss_ce: 0.006771
2022-01-08 02:18:43,007 iteration 4990 : loss : 0.017029, loss_ce: 0.008556
2022-01-08 02:18:44,349 iteration 4991 : loss : 0.017243, loss_ce: 0.006265
2022-01-08 02:18:45,724 iteration 4992 : loss : 0.017349, loss_ce: 0.004786
2022-01-08 02:18:47,089 iteration 4993 : loss : 0.016029, loss_ce: 0.006883
2022-01-08 02:18:48,581 iteration 4994 : loss : 0.018055, loss_ce: 0.006760
2022-01-08 02:18:50,005 iteration 4995 : loss : 0.019331, loss_ce: 0.008547
2022-01-08 02:18:51,331 iteration 4996 : loss : 0.013255, loss_ce: 0.003595
2022-01-08 02:18:52,713 iteration 4997 : loss : 0.014641, loss_ce: 0.005729
2022-01-08 02:18:54,091 iteration 4998 : loss : 0.024958, loss_ce: 0.011779
 74%|█████████████████████▎       | 294/400 [2:04:17<42:42, 24.17s/it]2022-01-08 02:18:55,501 iteration 4999 : loss : 0.015439, loss_ce: 0.006401
2022-01-08 02:18:56,809 iteration 5000 : loss : 0.012645, loss_ce: 0.004046
2022-01-08 02:18:58,220 iteration 5001 : loss : 0.017017, loss_ce: 0.004554
2022-01-08 02:18:59,688 iteration 5002 : loss : 0.017320, loss_ce: 0.005104
2022-01-08 02:19:01,149 iteration 5003 : loss : 0.018384, loss_ce: 0.008300
2022-01-08 02:19:02,512 iteration 5004 : loss : 0.012196, loss_ce: 0.005475
2022-01-08 02:19:03,905 iteration 5005 : loss : 0.016530, loss_ce: 0.006540
2022-01-08 02:19:05,207 iteration 5006 : loss : 0.012675, loss_ce: 0.004660
2022-01-08 02:19:06,596 iteration 5007 : loss : 0.017408, loss_ce: 0.003667
2022-01-08 02:19:07,911 iteration 5008 : loss : 0.012082, loss_ce: 0.003896
2022-01-08 02:19:09,237 iteration 5009 : loss : 0.014904, loss_ce: 0.007605
2022-01-08 02:19:10,651 iteration 5010 : loss : 0.014310, loss_ce: 0.006308
2022-01-08 02:19:11,968 iteration 5011 : loss : 0.012932, loss_ce: 0.005285
2022-01-08 02:19:13,309 iteration 5012 : loss : 0.019047, loss_ce: 0.004484
2022-01-08 02:19:14,753 iteration 5013 : loss : 0.017831, loss_ce: 0.008044
2022-01-08 02:19:16,045 iteration 5014 : loss : 0.014878, loss_ce: 0.005390
2022-01-08 02:19:16,045 Training Data Eval:
2022-01-08 02:19:22,927   Average segmentation loss on training set: 0.0084
2022-01-08 02:19:22,928 Validation Data Eval:
2022-01-08 02:19:25,304   Average segmentation loss on validation set: 0.0744
2022-01-08 02:19:26,665 iteration 5015 : loss : 0.016438, loss_ce: 0.005883
 74%|█████████████████████▍       | 295/400 [2:04:49<46:42, 26.69s/it]2022-01-08 02:19:28,094 iteration 5016 : loss : 0.017791, loss_ce: 0.008032
2022-01-08 02:19:29,370 iteration 5017 : loss : 0.009040, loss_ce: 0.003546
2022-01-08 02:19:30,844 iteration 5018 : loss : 0.020264, loss_ce: 0.009217
2022-01-08 02:19:32,136 iteration 5019 : loss : 0.013123, loss_ce: 0.004836
2022-01-08 02:19:33,541 iteration 5020 : loss : 0.018934, loss_ce: 0.005366
2022-01-08 02:19:34,896 iteration 5021 : loss : 0.017822, loss_ce: 0.006530
2022-01-08 02:19:36,260 iteration 5022 : loss : 0.013764, loss_ce: 0.004170
2022-01-08 02:19:37,618 iteration 5023 : loss : 0.023691, loss_ce: 0.015063
2022-01-08 02:19:39,052 iteration 5024 : loss : 0.020493, loss_ce: 0.007512
2022-01-08 02:19:40,423 iteration 5025 : loss : 0.021485, loss_ce: 0.007139
2022-01-08 02:19:41,839 iteration 5026 : loss : 0.027504, loss_ce: 0.011205
2022-01-08 02:19:43,156 iteration 5027 : loss : 0.012436, loss_ce: 0.004592
2022-01-08 02:19:44,457 iteration 5028 : loss : 0.010628, loss_ce: 0.004370
2022-01-08 02:19:45,903 iteration 5029 : loss : 0.018158, loss_ce: 0.007865
2022-01-08 02:19:47,252 iteration 5030 : loss : 0.013974, loss_ce: 0.006973
2022-01-08 02:19:48,671 iteration 5031 : loss : 0.031427, loss_ce: 0.010743
2022-01-08 02:19:49,970 iteration 5032 : loss : 0.011019, loss_ce: 0.004250
 74%|█████████████████████▍       | 296/400 [2:05:13<44:30, 25.67s/it]2022-01-08 02:19:51,308 iteration 5033 : loss : 0.013958, loss_ce: 0.004852
2022-01-08 02:19:52,622 iteration 5034 : loss : 0.018846, loss_ce: 0.005805
2022-01-08 02:19:53,973 iteration 5035 : loss : 0.016806, loss_ce: 0.005729
2022-01-08 02:19:55,286 iteration 5036 : loss : 0.014809, loss_ce: 0.005515
2022-01-08 02:19:56,625 iteration 5037 : loss : 0.013281, loss_ce: 0.004297
2022-01-08 02:19:57,952 iteration 5038 : loss : 0.016432, loss_ce: 0.005678
2022-01-08 02:19:59,318 iteration 5039 : loss : 0.020905, loss_ce: 0.008192
2022-01-08 02:20:00,723 iteration 5040 : loss : 0.011967, loss_ce: 0.004691
2022-01-08 02:20:02,076 iteration 5041 : loss : 0.016712, loss_ce: 0.007862
2022-01-08 02:20:03,531 iteration 5042 : loss : 0.015534, loss_ce: 0.006058
2022-01-08 02:20:04,801 iteration 5043 : loss : 0.014585, loss_ce: 0.004679
2022-01-08 02:20:06,216 iteration 5044 : loss : 0.021291, loss_ce: 0.008501
2022-01-08 02:20:07,603 iteration 5045 : loss : 0.018078, loss_ce: 0.007632
2022-01-08 02:20:08,983 iteration 5046 : loss : 0.018536, loss_ce: 0.004903
2022-01-08 02:20:10,291 iteration 5047 : loss : 0.010365, loss_ce: 0.003068
2022-01-08 02:20:11,585 iteration 5048 : loss : 0.013887, loss_ce: 0.006648
2022-01-08 02:20:12,860 iteration 5049 : loss : 0.010151, loss_ce: 0.004337
 74%|█████████████████████▌       | 297/400 [2:05:36<42:38, 24.84s/it]2022-01-08 02:20:14,279 iteration 5050 : loss : 0.011484, loss_ce: 0.003532
2022-01-08 02:20:15,686 iteration 5051 : loss : 0.023384, loss_ce: 0.007457
2022-01-08 02:20:17,136 iteration 5052 : loss : 0.021535, loss_ce: 0.007974
2022-01-08 02:20:18,578 iteration 5053 : loss : 0.022016, loss_ce: 0.007879
2022-01-08 02:20:19,914 iteration 5054 : loss : 0.015405, loss_ce: 0.003173
2022-01-08 02:20:21,296 iteration 5055 : loss : 0.018232, loss_ce: 0.006864
2022-01-08 02:20:22,670 iteration 5056 : loss : 0.018236, loss_ce: 0.007324
2022-01-08 02:20:24,117 iteration 5057 : loss : 0.018552, loss_ce: 0.008760
2022-01-08 02:20:25,556 iteration 5058 : loss : 0.027088, loss_ce: 0.010470
2022-01-08 02:20:26,890 iteration 5059 : loss : 0.013359, loss_ce: 0.005311
2022-01-08 02:20:28,293 iteration 5060 : loss : 0.015595, loss_ce: 0.006472
2022-01-08 02:20:29,711 iteration 5061 : loss : 0.016694, loss_ce: 0.006263
2022-01-08 02:20:31,160 iteration 5062 : loss : 0.025594, loss_ce: 0.008990
2022-01-08 02:20:32,591 iteration 5063 : loss : 0.023277, loss_ce: 0.010792
2022-01-08 02:20:33,946 iteration 5064 : loss : 0.018173, loss_ce: 0.006402
2022-01-08 02:20:35,394 iteration 5065 : loss : 0.012905, loss_ce: 0.006216
2022-01-08 02:20:36,709 iteration 5066 : loss : 0.013421, loss_ce: 0.005428
 74%|█████████████████████▌       | 298/400 [2:06:00<41:43, 24.54s/it]2022-01-08 02:20:37,995 iteration 5067 : loss : 0.012067, loss_ce: 0.004577
2022-01-08 02:20:39,396 iteration 5068 : loss : 0.014673, loss_ce: 0.003852
2022-01-08 02:20:40,782 iteration 5069 : loss : 0.015432, loss_ce: 0.007329
2022-01-08 02:20:42,089 iteration 5070 : loss : 0.015748, loss_ce: 0.005427
2022-01-08 02:20:43,495 iteration 5071 : loss : 0.020569, loss_ce: 0.009117
2022-01-08 02:20:44,793 iteration 5072 : loss : 0.013042, loss_ce: 0.004706
2022-01-08 02:20:46,224 iteration 5073 : loss : 0.018255, loss_ce: 0.008713
2022-01-08 02:20:47,562 iteration 5074 : loss : 0.013370, loss_ce: 0.005541
2022-01-08 02:20:48,908 iteration 5075 : loss : 0.016601, loss_ce: 0.006301
2022-01-08 02:20:50,297 iteration 5076 : loss : 0.041323, loss_ce: 0.007777
2022-01-08 02:20:51,675 iteration 5077 : loss : 0.040395, loss_ce: 0.013217
2022-01-08 02:20:52,992 iteration 5078 : loss : 0.014775, loss_ce: 0.006861
2022-01-08 02:20:54,302 iteration 5079 : loss : 0.011439, loss_ce: 0.004661
2022-01-08 02:20:55,634 iteration 5080 : loss : 0.014818, loss_ce: 0.006036
2022-01-08 02:20:57,032 iteration 5081 : loss : 0.018350, loss_ce: 0.005631
2022-01-08 02:20:58,370 iteration 5082 : loss : 0.013438, loss_ce: 0.005065
2022-01-08 02:20:59,749 iteration 5083 : loss : 0.019074, loss_ce: 0.004153
 75%|█████████████████████▋       | 299/400 [2:06:23<40:33, 24.09s/it]2022-01-08 02:21:01,193 iteration 5084 : loss : 0.017087, loss_ce: 0.008802
2022-01-08 02:21:02,639 iteration 5085 : loss : 0.017480, loss_ce: 0.005990
2022-01-08 02:21:04,031 iteration 5086 : loss : 0.014988, loss_ce: 0.005132
2022-01-08 02:21:05,331 iteration 5087 : loss : 0.017492, loss_ce: 0.007439
2022-01-08 02:21:06,646 iteration 5088 : loss : 0.016556, loss_ce: 0.006968
2022-01-08 02:21:08,039 iteration 5089 : loss : 0.018292, loss_ce: 0.004867
2022-01-08 02:21:09,411 iteration 5090 : loss : 0.017487, loss_ce: 0.003206
2022-01-08 02:21:10,837 iteration 5091 : loss : 0.015157, loss_ce: 0.004872
2022-01-08 02:21:12,206 iteration 5092 : loss : 0.016300, loss_ce: 0.006791
2022-01-08 02:21:13,512 iteration 5093 : loss : 0.015544, loss_ce: 0.004510
2022-01-08 02:21:14,907 iteration 5094 : loss : 0.019365, loss_ce: 0.006594
2022-01-08 02:21:16,228 iteration 5095 : loss : 0.013168, loss_ce: 0.004796
2022-01-08 02:21:17,588 iteration 5096 : loss : 0.016004, loss_ce: 0.007274
2022-01-08 02:21:18,889 iteration 5097 : loss : 0.013056, loss_ce: 0.005685
2022-01-08 02:21:20,332 iteration 5098 : loss : 0.017339, loss_ce: 0.005403
2022-01-08 02:21:21,716 iteration 5099 : loss : 0.025741, loss_ce: 0.008554
2022-01-08 02:21:21,716 Training Data Eval:
2022-01-08 02:21:28,597   Average segmentation loss on training set: 0.0094
2022-01-08 02:21:28,598 Validation Data Eval:
2022-01-08 02:21:30,978   Average segmentation loss on validation set: 0.0888
2022-01-08 02:21:32,415 iteration 5100 : loss : 0.038278, loss_ce: 0.011616
 75%|█████████████████████▊       | 300/400 [2:06:55<44:26, 26.66s/it]2022-01-08 02:21:33,907 iteration 5101 : loss : 0.022864, loss_ce: 0.010070
2022-01-08 02:21:35,251 iteration 5102 : loss : 0.015507, loss_ce: 0.005611
2022-01-08 02:21:36,609 iteration 5103 : loss : 0.018816, loss_ce: 0.008884
2022-01-08 02:21:38,039 iteration 5104 : loss : 0.018782, loss_ce: 0.005795
2022-01-08 02:21:39,375 iteration 5105 : loss : 0.018255, loss_ce: 0.007133
2022-01-08 02:21:40,714 iteration 5106 : loss : 0.015772, loss_ce: 0.005891
2022-01-08 02:21:42,145 iteration 5107 : loss : 0.025615, loss_ce: 0.007266
2022-01-08 02:21:43,528 iteration 5108 : loss : 0.014955, loss_ce: 0.006903
2022-01-08 02:21:44,891 iteration 5109 : loss : 0.022436, loss_ce: 0.009086
2022-01-08 02:21:46,198 iteration 5110 : loss : 0.013750, loss_ce: 0.004159
2022-01-08 02:21:47,572 iteration 5111 : loss : 0.016078, loss_ce: 0.005484
2022-01-08 02:21:49,023 iteration 5112 : loss : 0.021954, loss_ce: 0.008219
2022-01-08 02:21:50,415 iteration 5113 : loss : 0.014261, loss_ce: 0.004407
2022-01-08 02:21:51,846 iteration 5114 : loss : 0.016020, loss_ce: 0.005465
2022-01-08 02:21:53,201 iteration 5115 : loss : 0.019887, loss_ce: 0.006459
2022-01-08 02:21:54,542 iteration 5116 : loss : 0.016354, loss_ce: 0.006135
2022-01-08 02:21:55,945 iteration 5117 : loss : 0.020497, loss_ce: 0.008966
 75%|█████████████████████▊       | 301/400 [2:07:19<42:26, 25.73s/it]2022-01-08 02:21:57,387 iteration 5118 : loss : 0.017015, loss_ce: 0.004702
2022-01-08 02:21:58,733 iteration 5119 : loss : 0.013565, loss_ce: 0.005970
2022-01-08 02:22:00,045 iteration 5120 : loss : 0.013528, loss_ce: 0.005113
2022-01-08 02:22:01,467 iteration 5121 : loss : 0.031517, loss_ce: 0.009604
2022-01-08 02:22:02,913 iteration 5122 : loss : 0.018712, loss_ce: 0.010114
2022-01-08 02:22:04,333 iteration 5123 : loss : 0.023951, loss_ce: 0.008319
2022-01-08 02:22:05,630 iteration 5124 : loss : 0.013176, loss_ce: 0.006268
2022-01-08 02:22:06,934 iteration 5125 : loss : 0.019140, loss_ce: 0.006931
2022-01-08 02:22:08,304 iteration 5126 : loss : 0.019551, loss_ce: 0.008005
2022-01-08 02:22:09,666 iteration 5127 : loss : 0.023640, loss_ce: 0.010617
2022-01-08 02:22:11,007 iteration 5128 : loss : 0.018350, loss_ce: 0.006061
2022-01-08 02:22:12,399 iteration 5129 : loss : 0.017020, loss_ce: 0.007087
2022-01-08 02:22:13,757 iteration 5130 : loss : 0.015337, loss_ce: 0.006260
2022-01-08 02:22:15,139 iteration 5131 : loss : 0.016027, loss_ce: 0.006657
2022-01-08 02:22:16,495 iteration 5132 : loss : 0.015031, loss_ce: 0.005666
2022-01-08 02:22:17,912 iteration 5133 : loss : 0.029116, loss_ce: 0.010788
2022-01-08 02:22:19,258 iteration 5134 : loss : 0.019809, loss_ce: 0.004985
 76%|█████████████████████▉       | 302/400 [2:07:42<40:49, 25.00s/it]2022-01-08 02:22:20,635 iteration 5135 : loss : 0.016261, loss_ce: 0.008043
2022-01-08 02:22:21,997 iteration 5136 : loss : 0.013517, loss_ce: 0.004323
2022-01-08 02:22:23,308 iteration 5137 : loss : 0.013900, loss_ce: 0.004991
2022-01-08 02:22:24,707 iteration 5138 : loss : 0.013689, loss_ce: 0.005769
2022-01-08 02:22:26,158 iteration 5139 : loss : 0.020562, loss_ce: 0.008421
2022-01-08 02:22:27,583 iteration 5140 : loss : 0.017930, loss_ce: 0.006486
2022-01-08 02:22:28,900 iteration 5141 : loss : 0.022151, loss_ce: 0.006913
2022-01-08 02:22:30,281 iteration 5142 : loss : 0.016020, loss_ce: 0.005737
2022-01-08 02:22:31,632 iteration 5143 : loss : 0.013770, loss_ce: 0.006879
2022-01-08 02:22:33,074 iteration 5144 : loss : 0.014884, loss_ce: 0.006103
2022-01-08 02:22:34,569 iteration 5145 : loss : 0.010713, loss_ce: 0.003036
2022-01-08 02:22:35,963 iteration 5146 : loss : 0.016554, loss_ce: 0.005715
2022-01-08 02:22:37,261 iteration 5147 : loss : 0.014560, loss_ce: 0.003137
2022-01-08 02:22:38,607 iteration 5148 : loss : 0.017621, loss_ce: 0.008350
2022-01-08 02:22:39,948 iteration 5149 : loss : 0.014917, loss_ce: 0.005058
2022-01-08 02:22:41,299 iteration 5150 : loss : 0.026837, loss_ce: 0.018201
2022-01-08 02:22:42,712 iteration 5151 : loss : 0.017515, loss_ce: 0.009364
 76%|█████████████████████▉       | 303/400 [2:08:06<39:40, 24.54s/it]2022-01-08 02:22:44,086 iteration 5152 : loss : 0.017179, loss_ce: 0.007300
2022-01-08 02:22:45,458 iteration 5153 : loss : 0.017030, loss_ce: 0.006431
2022-01-08 02:22:46,751 iteration 5154 : loss : 0.012764, loss_ce: 0.004427
2022-01-08 02:22:48,062 iteration 5155 : loss : 0.012326, loss_ce: 0.004809
2022-01-08 02:22:49,419 iteration 5156 : loss : 0.018478, loss_ce: 0.004573
2022-01-08 02:22:50,734 iteration 5157 : loss : 0.011671, loss_ce: 0.005117
2022-01-08 02:22:52,068 iteration 5158 : loss : 0.019460, loss_ce: 0.006713
2022-01-08 02:22:53,451 iteration 5159 : loss : 0.013930, loss_ce: 0.005333
2022-01-08 02:22:54,812 iteration 5160 : loss : 0.020015, loss_ce: 0.010705
2022-01-08 02:22:56,189 iteration 5161 : loss : 0.016536, loss_ce: 0.006663
2022-01-08 02:22:57,613 iteration 5162 : loss : 0.012322, loss_ce: 0.003252
2022-01-08 02:22:59,000 iteration 5163 : loss : 0.015257, loss_ce: 0.008304
2022-01-08 02:23:00,319 iteration 5164 : loss : 0.013171, loss_ce: 0.004630
2022-01-08 02:23:01,755 iteration 5165 : loss : 0.016104, loss_ce: 0.005547
2022-01-08 02:23:03,132 iteration 5166 : loss : 0.015101, loss_ce: 0.005472
2022-01-08 02:23:04,542 iteration 5167 : loss : 0.018600, loss_ce: 0.004775
2022-01-08 02:23:05,811 iteration 5168 : loss : 0.012190, loss_ce: 0.005734
 76%|██████████████████████       | 304/400 [2:08:29<38:34, 24.10s/it]2022-01-08 02:23:07,256 iteration 5169 : loss : 0.012671, loss_ce: 0.003390
2022-01-08 02:23:08,604 iteration 5170 : loss : 0.012057, loss_ce: 0.006086
2022-01-08 02:23:09,960 iteration 5171 : loss : 0.015606, loss_ce: 0.004905
2022-01-08 02:23:11,338 iteration 5172 : loss : 0.019294, loss_ce: 0.006253
2022-01-08 02:23:12,781 iteration 5173 : loss : 0.016305, loss_ce: 0.006356
2022-01-08 02:23:14,151 iteration 5174 : loss : 0.011017, loss_ce: 0.003335
2022-01-08 02:23:15,551 iteration 5175 : loss : 0.012881, loss_ce: 0.004889
2022-01-08 02:23:16,997 iteration 5176 : loss : 0.017539, loss_ce: 0.007236
2022-01-08 02:23:18,368 iteration 5177 : loss : 0.016363, loss_ce: 0.007782
2022-01-08 02:23:19,751 iteration 5178 : loss : 0.012621, loss_ce: 0.005264
2022-01-08 02:23:21,090 iteration 5179 : loss : 0.016970, loss_ce: 0.008536
2022-01-08 02:23:22,383 iteration 5180 : loss : 0.013369, loss_ce: 0.005312
2022-01-08 02:23:23,710 iteration 5181 : loss : 0.013014, loss_ce: 0.004026
2022-01-08 02:23:25,065 iteration 5182 : loss : 0.013993, loss_ce: 0.005615
2022-01-08 02:23:26,413 iteration 5183 : loss : 0.011584, loss_ce: 0.003963
2022-01-08 02:23:27,727 iteration 5184 : loss : 0.014379, loss_ce: 0.006783
2022-01-08 02:23:27,727 Training Data Eval:
2022-01-08 02:23:34,581   Average segmentation loss on training set: 0.0083
2022-01-08 02:23:34,581 Validation Data Eval:
2022-01-08 02:23:36,957   Average segmentation loss on validation set: 0.0802
2022-01-08 02:23:38,312 iteration 5185 : loss : 0.012472, loss_ce: 0.004705
 76%|██████████████████████       | 305/400 [2:09:01<42:09, 26.62s/it]2022-01-08 02:23:39,754 iteration 5186 : loss : 0.018410, loss_ce: 0.005449
2022-01-08 02:23:41,060 iteration 5187 : loss : 0.011930, loss_ce: 0.004728
2022-01-08 02:23:42,419 iteration 5188 : loss : 0.013137, loss_ce: 0.003908
2022-01-08 02:23:43,791 iteration 5189 : loss : 0.021654, loss_ce: 0.006332
2022-01-08 02:23:45,247 iteration 5190 : loss : 0.017162, loss_ce: 0.004742
2022-01-08 02:23:46,581 iteration 5191 : loss : 0.015488, loss_ce: 0.005278
2022-01-08 02:23:47,959 iteration 5192 : loss : 0.015978, loss_ce: 0.005320
2022-01-08 02:23:49,379 iteration 5193 : loss : 0.013505, loss_ce: 0.006135
2022-01-08 02:23:50,762 iteration 5194 : loss : 0.016662, loss_ce: 0.004613
2022-01-08 02:23:52,097 iteration 5195 : loss : 0.020968, loss_ce: 0.006294
2022-01-08 02:23:53,553 iteration 5196 : loss : 0.013538, loss_ce: 0.004533
2022-01-08 02:23:54,900 iteration 5197 : loss : 0.013763, loss_ce: 0.004375
2022-01-08 02:23:56,215 iteration 5198 : loss : 0.015004, loss_ce: 0.009328
2022-01-08 02:23:57,681 iteration 5199 : loss : 0.017018, loss_ce: 0.006451
2022-01-08 02:23:58,953 iteration 5200 : loss : 0.015708, loss_ce: 0.006291
2022-01-08 02:24:00,253 iteration 5201 : loss : 0.012258, loss_ce: 0.005223
2022-01-08 02:24:01,597 iteration 5202 : loss : 0.014539, loss_ce: 0.005330
 76%|██████████████████████▏      | 306/400 [2:09:24<40:08, 25.62s/it]2022-01-08 02:24:02,977 iteration 5203 : loss : 0.014499, loss_ce: 0.005960
2022-01-08 02:24:04,392 iteration 5204 : loss : 0.015469, loss_ce: 0.006891
2022-01-08 02:24:05,798 iteration 5205 : loss : 0.018449, loss_ce: 0.004615
2022-01-08 02:24:07,293 iteration 5206 : loss : 0.022346, loss_ce: 0.008423
2022-01-08 02:24:08,692 iteration 5207 : loss : 0.026817, loss_ce: 0.005573
2022-01-08 02:24:10,056 iteration 5208 : loss : 0.010500, loss_ce: 0.004229
2022-01-08 02:24:11,469 iteration 5209 : loss : 0.017959, loss_ce: 0.007090
2022-01-08 02:24:12,886 iteration 5210 : loss : 0.018131, loss_ce: 0.008674
2022-01-08 02:24:14,246 iteration 5211 : loss : 0.014628, loss_ce: 0.004770
2022-01-08 02:24:15,651 iteration 5212 : loss : 0.026714, loss_ce: 0.008157
2022-01-08 02:24:17,047 iteration 5213 : loss : 0.016036, loss_ce: 0.005964
2022-01-08 02:24:18,410 iteration 5214 : loss : 0.014378, loss_ce: 0.006384
2022-01-08 02:24:19,691 iteration 5215 : loss : 0.011291, loss_ce: 0.004054
2022-01-08 02:24:21,016 iteration 5216 : loss : 0.014398, loss_ce: 0.004612
2022-01-08 02:24:22,443 iteration 5217 : loss : 0.019817, loss_ce: 0.009432
2022-01-08 02:24:23,749 iteration 5218 : loss : 0.010850, loss_ce: 0.003457
2022-01-08 02:24:25,117 iteration 5219 : loss : 0.015747, loss_ce: 0.006637
 77%|██████████████████████▎      | 307/400 [2:09:48<38:44, 24.99s/it]2022-01-08 02:24:26,438 iteration 5220 : loss : 0.016669, loss_ce: 0.008422
2022-01-08 02:24:27,737 iteration 5221 : loss : 0.013556, loss_ce: 0.003769
2022-01-08 02:24:29,148 iteration 5222 : loss : 0.019242, loss_ce: 0.005968
2022-01-08 02:24:30,504 iteration 5223 : loss : 0.012432, loss_ce: 0.005661
2022-01-08 02:24:31,941 iteration 5224 : loss : 0.026180, loss_ce: 0.006333
2022-01-08 02:24:33,262 iteration 5225 : loss : 0.011878, loss_ce: 0.004996
2022-01-08 02:24:34,623 iteration 5226 : loss : 0.015021, loss_ce: 0.006263
2022-01-08 02:24:35,989 iteration 5227 : loss : 0.022095, loss_ce: 0.007904
2022-01-08 02:24:37,270 iteration 5228 : loss : 0.009489, loss_ce: 0.002919
2022-01-08 02:24:38,668 iteration 5229 : loss : 0.022275, loss_ce: 0.008332
2022-01-08 02:24:39,982 iteration 5230 : loss : 0.011663, loss_ce: 0.005127
2022-01-08 02:24:41,363 iteration 5231 : loss : 0.015137, loss_ce: 0.004459
2022-01-08 02:24:42,811 iteration 5232 : loss : 0.020767, loss_ce: 0.007628
2022-01-08 02:24:44,054 iteration 5233 : loss : 0.012425, loss_ce: 0.005139
2022-01-08 02:24:45,420 iteration 5234 : loss : 0.015494, loss_ce: 0.005949
2022-01-08 02:24:46,824 iteration 5235 : loss : 0.015377, loss_ce: 0.006746
2022-01-08 02:24:48,261 iteration 5236 : loss : 0.036983, loss_ce: 0.019897
 77%|██████████████████████▎      | 308/400 [2:10:11<37:28, 24.44s/it]2022-01-08 02:24:49,582 iteration 5237 : loss : 0.010518, loss_ce: 0.004634
2022-01-08 02:24:50,930 iteration 5238 : loss : 0.015545, loss_ce: 0.006333
2022-01-08 02:24:52,278 iteration 5239 : loss : 0.012187, loss_ce: 0.004893
2022-01-08 02:24:53,705 iteration 5240 : loss : 0.037905, loss_ce: 0.007257
2022-01-08 02:24:55,083 iteration 5241 : loss : 0.017586, loss_ce: 0.006438
2022-01-08 02:24:56,549 iteration 5242 : loss : 0.019999, loss_ce: 0.009378
2022-01-08 02:24:57,945 iteration 5243 : loss : 0.020700, loss_ce: 0.009246
2022-01-08 02:24:59,285 iteration 5244 : loss : 0.015086, loss_ce: 0.004495
2022-01-08 02:25:00,707 iteration 5245 : loss : 0.020493, loss_ce: 0.006639
2022-01-08 02:25:02,094 iteration 5246 : loss : 0.017276, loss_ce: 0.006510
2022-01-08 02:25:03,438 iteration 5247 : loss : 0.014292, loss_ce: 0.004764
2022-01-08 02:25:04,850 iteration 5248 : loss : 0.016589, loss_ce: 0.007636
2022-01-08 02:25:06,177 iteration 5249 : loss : 0.013903, loss_ce: 0.004952
2022-01-08 02:25:07,540 iteration 5250 : loss : 0.017379, loss_ce: 0.006677
2022-01-08 02:25:08,941 iteration 5251 : loss : 0.018139, loss_ce: 0.008259
2022-01-08 02:25:10,276 iteration 5252 : loss : 0.012538, loss_ce: 0.005996
2022-01-08 02:25:11,612 iteration 5253 : loss : 0.011036, loss_ce: 0.003982
 77%|██████████████████████▍      | 309/400 [2:10:34<36:34, 24.11s/it]2022-01-08 02:25:12,991 iteration 5254 : loss : 0.015075, loss_ce: 0.006192
2022-01-08 02:25:14,401 iteration 5255 : loss : 0.016623, loss_ce: 0.004878
2022-01-08 02:25:15,780 iteration 5256 : loss : 0.020350, loss_ce: 0.009175
2022-01-08 02:25:17,180 iteration 5257 : loss : 0.021936, loss_ce: 0.011692
2022-01-08 02:25:18,471 iteration 5258 : loss : 0.012659, loss_ce: 0.005122
2022-01-08 02:25:19,802 iteration 5259 : loss : 0.019823, loss_ce: 0.006283
2022-01-08 02:25:21,254 iteration 5260 : loss : 0.017445, loss_ce: 0.005773
2022-01-08 02:25:22,609 iteration 5261 : loss : 0.014757, loss_ce: 0.006643
2022-01-08 02:25:23,943 iteration 5262 : loss : 0.022779, loss_ce: 0.011716
2022-01-08 02:25:25,360 iteration 5263 : loss : 0.017492, loss_ce: 0.007855
2022-01-08 02:25:26,774 iteration 5264 : loss : 0.022279, loss_ce: 0.008858
2022-01-08 02:25:28,175 iteration 5265 : loss : 0.021592, loss_ce: 0.008747
2022-01-08 02:25:29,511 iteration 5266 : loss : 0.015262, loss_ce: 0.005904
2022-01-08 02:25:30,952 iteration 5267 : loss : 0.017192, loss_ce: 0.007307
2022-01-08 02:25:32,182 iteration 5268 : loss : 0.009575, loss_ce: 0.002195
2022-01-08 02:25:33,452 iteration 5269 : loss : 0.013097, loss_ce: 0.004572
2022-01-08 02:25:33,452 Training Data Eval:
2022-01-08 02:25:40,317   Average segmentation loss on training set: 0.0084
2022-01-08 02:25:40,318 Validation Data Eval:
2022-01-08 02:25:42,693   Average segmentation loss on validation set: 0.0742
2022-01-08 02:25:44,089 iteration 5270 : loss : 0.017174, loss_ce: 0.002278
 78%|██████████████████████▍      | 310/400 [2:11:07<39:55, 26.62s/it]2022-01-08 02:25:45,594 iteration 5271 : loss : 0.021270, loss_ce: 0.008392
2022-01-08 02:25:47,025 iteration 5272 : loss : 0.022166, loss_ce: 0.008716
2022-01-08 02:25:48,399 iteration 5273 : loss : 0.016967, loss_ce: 0.007105
2022-01-08 02:25:49,769 iteration 5274 : loss : 0.017850, loss_ce: 0.006394
2022-01-08 02:25:51,104 iteration 5275 : loss : 0.013896, loss_ce: 0.005550
2022-01-08 02:25:52,432 iteration 5276 : loss : 0.022065, loss_ce: 0.008341
2022-01-08 02:25:53,891 iteration 5277 : loss : 0.021155, loss_ce: 0.009006
2022-01-08 02:25:55,230 iteration 5278 : loss : 0.019905, loss_ce: 0.004798
2022-01-08 02:25:56,625 iteration 5279 : loss : 0.023640, loss_ce: 0.008735
2022-01-08 02:25:57,930 iteration 5280 : loss : 0.012745, loss_ce: 0.003782
2022-01-08 02:25:59,276 iteration 5281 : loss : 0.012389, loss_ce: 0.005574
2022-01-08 02:26:00,614 iteration 5282 : loss : 0.015606, loss_ce: 0.006054
2022-01-08 02:26:01,936 iteration 5283 : loss : 0.014964, loss_ce: 0.005999
2022-01-08 02:26:03,310 iteration 5284 : loss : 0.014186, loss_ce: 0.005403
2022-01-08 02:26:04,648 iteration 5285 : loss : 0.019506, loss_ce: 0.008556
2022-01-08 02:26:05,954 iteration 5286 : loss : 0.016971, loss_ce: 0.008088
2022-01-08 02:26:07,334 iteration 5287 : loss : 0.016372, loss_ce: 0.006201
 78%|██████████████████████▌      | 311/400 [2:11:30<37:59, 25.61s/it]2022-01-08 02:26:08,737 iteration 5288 : loss : 0.017406, loss_ce: 0.008124
2022-01-08 02:26:10,053 iteration 5289 : loss : 0.015354, loss_ce: 0.005782
2022-01-08 02:26:11,532 iteration 5290 : loss : 0.023713, loss_ce: 0.007056
2022-01-08 02:26:12,884 iteration 5291 : loss : 0.015448, loss_ce: 0.006399
2022-01-08 02:26:14,279 iteration 5292 : loss : 0.017828, loss_ce: 0.004202
2022-01-08 02:26:15,611 iteration 5293 : loss : 0.020436, loss_ce: 0.007377
2022-01-08 02:26:17,020 iteration 5294 : loss : 0.020625, loss_ce: 0.005521
2022-01-08 02:26:18,389 iteration 5295 : loss : 0.013359, loss_ce: 0.004021
2022-01-08 02:26:19,683 iteration 5296 : loss : 0.015495, loss_ce: 0.004327
2022-01-08 02:26:21,023 iteration 5297 : loss : 0.015640, loss_ce: 0.005334
2022-01-08 02:26:22,346 iteration 5298 : loss : 0.016756, loss_ce: 0.007219
2022-01-08 02:26:23,678 iteration 5299 : loss : 0.014182, loss_ce: 0.002884
2022-01-08 02:26:25,014 iteration 5300 : loss : 0.012761, loss_ce: 0.005016
2022-01-08 02:26:26,355 iteration 5301 : loss : 0.016994, loss_ce: 0.003866
2022-01-08 02:26:27,782 iteration 5302 : loss : 0.015896, loss_ce: 0.007159
2022-01-08 02:26:29,181 iteration 5303 : loss : 0.012008, loss_ce: 0.005632
2022-01-08 02:26:30,527 iteration 5304 : loss : 0.019352, loss_ce: 0.010260
 78%|██████████████████████▌      | 312/400 [2:11:53<36:29, 24.88s/it]2022-01-08 02:26:31,852 iteration 5305 : loss : 0.009517, loss_ce: 0.003360
2022-01-08 02:26:33,262 iteration 5306 : loss : 0.018066, loss_ce: 0.007677
2022-01-08 02:26:34,647 iteration 5307 : loss : 0.016777, loss_ce: 0.006079
2022-01-08 02:26:35,973 iteration 5308 : loss : 0.013576, loss_ce: 0.005101
2022-01-08 02:26:37,356 iteration 5309 : loss : 0.012385, loss_ce: 0.005911
2022-01-08 02:26:38,829 iteration 5310 : loss : 0.013779, loss_ce: 0.004316
2022-01-08 02:26:40,172 iteration 5311 : loss : 0.021852, loss_ce: 0.009102
2022-01-08 02:26:41,590 iteration 5312 : loss : 0.017144, loss_ce: 0.007197
2022-01-08 02:26:42,927 iteration 5313 : loss : 0.014558, loss_ce: 0.007284
2022-01-08 02:26:44,289 iteration 5314 : loss : 0.015251, loss_ce: 0.005536
2022-01-08 02:26:45,606 iteration 5315 : loss : 0.021450, loss_ce: 0.008010
2022-01-08 02:26:46,958 iteration 5316 : loss : 0.011577, loss_ce: 0.003554
2022-01-08 02:26:48,277 iteration 5317 : loss : 0.012453, loss_ce: 0.003917
2022-01-08 02:26:49,676 iteration 5318 : loss : 0.015107, loss_ce: 0.005557
2022-01-08 02:26:51,022 iteration 5319 : loss : 0.022729, loss_ce: 0.006859
2022-01-08 02:26:52,344 iteration 5320 : loss : 0.014645, loss_ce: 0.005079
2022-01-08 02:26:53,674 iteration 5321 : loss : 0.017814, loss_ce: 0.006694
 78%|██████████████████████▋      | 313/400 [2:12:16<35:19, 24.36s/it]2022-01-08 02:26:55,006 iteration 5322 : loss : 0.013435, loss_ce: 0.004896
2022-01-08 02:26:56,409 iteration 5323 : loss : 0.019229, loss_ce: 0.005218
2022-01-08 02:26:57,756 iteration 5324 : loss : 0.016529, loss_ce: 0.004949
2022-01-08 02:26:59,180 iteration 5325 : loss : 0.014666, loss_ce: 0.005652
2022-01-08 02:27:00,577 iteration 5326 : loss : 0.018447, loss_ce: 0.006092
2022-01-08 02:27:01,953 iteration 5327 : loss : 0.016929, loss_ce: 0.006368
2022-01-08 02:27:03,355 iteration 5328 : loss : 0.012267, loss_ce: 0.004191
2022-01-08 02:27:04,737 iteration 5329 : loss : 0.018438, loss_ce: 0.009654
2022-01-08 02:27:06,140 iteration 5330 : loss : 0.013761, loss_ce: 0.004631
2022-01-08 02:27:07,443 iteration 5331 : loss : 0.018641, loss_ce: 0.005879
2022-01-08 02:27:08,768 iteration 5332 : loss : 0.014327, loss_ce: 0.005807
2022-01-08 02:27:10,119 iteration 5333 : loss : 0.016110, loss_ce: 0.004898
2022-01-08 02:27:11,453 iteration 5334 : loss : 0.013862, loss_ce: 0.004269
2022-01-08 02:27:12,910 iteration 5335 : loss : 0.022893, loss_ce: 0.007988
2022-01-08 02:27:14,282 iteration 5336 : loss : 0.025330, loss_ce: 0.009514
2022-01-08 02:27:15,637 iteration 5337 : loss : 0.013415, loss_ce: 0.004823
2022-01-08 02:27:16,904 iteration 5338 : loss : 0.011310, loss_ce: 0.004228
 78%|██████████████████████▊      | 314/400 [2:12:40<34:25, 24.02s/it]2022-01-08 02:27:18,296 iteration 5339 : loss : 0.013121, loss_ce: 0.004376
2022-01-08 02:27:19,662 iteration 5340 : loss : 0.014670, loss_ce: 0.010446
2022-01-08 02:27:21,038 iteration 5341 : loss : 0.015534, loss_ce: 0.006204
2022-01-08 02:27:22,432 iteration 5342 : loss : 0.016298, loss_ce: 0.004184
2022-01-08 02:27:23,761 iteration 5343 : loss : 0.018668, loss_ce: 0.006042
2022-01-08 02:27:25,101 iteration 5344 : loss : 0.015504, loss_ce: 0.007860
2022-01-08 02:27:26,409 iteration 5345 : loss : 0.014454, loss_ce: 0.004077
2022-01-08 02:27:27,754 iteration 5346 : loss : 0.018366, loss_ce: 0.005865
2022-01-08 02:27:29,205 iteration 5347 : loss : 0.018769, loss_ce: 0.008072
2022-01-08 02:27:30,590 iteration 5348 : loss : 0.017599, loss_ce: 0.008081
2022-01-08 02:27:31,935 iteration 5349 : loss : 0.012026, loss_ce: 0.005553
2022-01-08 02:27:33,235 iteration 5350 : loss : 0.010151, loss_ce: 0.003709
2022-01-08 02:27:34,648 iteration 5351 : loss : 0.029585, loss_ce: 0.004490
2022-01-08 02:27:36,003 iteration 5352 : loss : 0.009957, loss_ce: 0.003675
2022-01-08 02:27:37,404 iteration 5353 : loss : 0.017358, loss_ce: 0.007518
2022-01-08 02:27:38,714 iteration 5354 : loss : 0.011894, loss_ce: 0.004739
2022-01-08 02:27:38,715 Training Data Eval:
2022-01-08 02:27:45,630   Average segmentation loss on training set: 0.0080
2022-01-08 02:27:45,631 Validation Data Eval:
2022-01-08 02:27:48,002   Average segmentation loss on validation set: 0.0773
2022-01-08 02:27:49,321 iteration 5355 : loss : 0.011863, loss_ce: 0.004518
 79%|██████████████████████▊      | 315/400 [2:13:12<37:36, 26.54s/it]2022-01-08 02:27:50,794 iteration 5356 : loss : 0.017632, loss_ce: 0.007254
2022-01-08 02:27:52,075 iteration 5357 : loss : 0.009240, loss_ce: 0.002544
2022-01-08 02:27:53,441 iteration 5358 : loss : 0.012044, loss_ce: 0.003344
2022-01-08 02:27:54,776 iteration 5359 : loss : 0.011667, loss_ce: 0.006450
2022-01-08 02:27:56,134 iteration 5360 : loss : 0.012616, loss_ce: 0.004244
2022-01-08 02:27:57,553 iteration 5361 : loss : 0.018411, loss_ce: 0.005777
2022-01-08 02:27:58,948 iteration 5362 : loss : 0.016594, loss_ce: 0.009612
2022-01-08 02:28:00,306 iteration 5363 : loss : 0.012865, loss_ce: 0.005099
2022-01-08 02:28:01,575 iteration 5364 : loss : 0.013201, loss_ce: 0.001811
2022-01-08 02:28:02,993 iteration 5365 : loss : 0.012834, loss_ce: 0.004465
2022-01-08 02:28:04,291 iteration 5366 : loss : 0.013498, loss_ce: 0.004621
2022-01-08 02:28:05,690 iteration 5367 : loss : 0.019092, loss_ce: 0.004820
2022-01-08 02:28:07,127 iteration 5368 : loss : 0.016386, loss_ce: 0.006537
2022-01-08 02:28:08,491 iteration 5369 : loss : 0.019276, loss_ce: 0.008158
2022-01-08 02:28:09,865 iteration 5370 : loss : 0.021316, loss_ce: 0.005956
2022-01-08 02:28:11,305 iteration 5371 : loss : 0.016616, loss_ce: 0.004151
2022-01-08 02:28:12,651 iteration 5372 : loss : 0.014455, loss_ce: 0.006120
 79%|██████████████████████▉      | 316/400 [2:13:35<35:48, 25.58s/it]2022-01-08 02:28:13,978 iteration 5373 : loss : 0.010329, loss_ce: 0.004691
2022-01-08 02:28:15,353 iteration 5374 : loss : 0.017416, loss_ce: 0.005260
2022-01-08 02:28:16,681 iteration 5375 : loss : 0.016303, loss_ce: 0.007463
2022-01-08 02:28:17,997 iteration 5376 : loss : 0.011542, loss_ce: 0.003612
2022-01-08 02:28:19,261 iteration 5377 : loss : 0.015828, loss_ce: 0.002295
2022-01-08 02:28:20,519 iteration 5378 : loss : 0.010311, loss_ce: 0.003875
2022-01-08 02:28:21,900 iteration 5379 : loss : 0.023347, loss_ce: 0.008491
2022-01-08 02:28:23,231 iteration 5380 : loss : 0.011642, loss_ce: 0.004881
2022-01-08 02:28:24,593 iteration 5381 : loss : 0.016935, loss_ce: 0.006923
2022-01-08 02:28:25,995 iteration 5382 : loss : 0.030447, loss_ce: 0.010570
2022-01-08 02:28:27,395 iteration 5383 : loss : 0.018323, loss_ce: 0.007796
2022-01-08 02:28:28,730 iteration 5384 : loss : 0.014788, loss_ce: 0.007031
2022-01-08 02:28:30,071 iteration 5385 : loss : 0.015153, loss_ce: 0.006509
2022-01-08 02:28:31,467 iteration 5386 : loss : 0.012322, loss_ce: 0.004733
2022-01-08 02:28:32,832 iteration 5387 : loss : 0.015010, loss_ce: 0.006567
2022-01-08 02:28:34,116 iteration 5388 : loss : 0.011554, loss_ce: 0.004545
2022-01-08 02:28:35,462 iteration 5389 : loss : 0.011064, loss_ce: 0.003816
 79%|██████████████████████▉      | 317/400 [2:13:58<34:13, 24.75s/it]2022-01-08 02:28:36,935 iteration 5390 : loss : 0.025495, loss_ce: 0.008281
2022-01-08 02:28:38,199 iteration 5391 : loss : 0.020297, loss_ce: 0.007153
2022-01-08 02:28:39,607 iteration 5392 : loss : 0.013158, loss_ce: 0.006012
2022-01-08 02:28:41,019 iteration 5393 : loss : 0.026035, loss_ce: 0.007491
2022-01-08 02:28:42,370 iteration 5394 : loss : 0.012956, loss_ce: 0.005096
2022-01-08 02:28:43,677 iteration 5395 : loss : 0.015026, loss_ce: 0.007303
2022-01-08 02:28:45,092 iteration 5396 : loss : 0.019383, loss_ce: 0.005216
2022-01-08 02:28:46,453 iteration 5397 : loss : 0.018497, loss_ce: 0.006878
2022-01-08 02:28:47,707 iteration 5398 : loss : 0.009855, loss_ce: 0.004790
2022-01-08 02:28:49,038 iteration 5399 : loss : 0.010498, loss_ce: 0.004627
2022-01-08 02:28:50,474 iteration 5400 : loss : 0.021257, loss_ce: 0.006581
2022-01-08 02:28:51,877 iteration 5401 : loss : 0.013053, loss_ce: 0.003684
2022-01-08 02:28:53,223 iteration 5402 : loss : 0.016341, loss_ce: 0.005494
2022-01-08 02:28:54,615 iteration 5403 : loss : 0.025209, loss_ce: 0.007284
2022-01-08 02:28:55,949 iteration 5404 : loss : 0.013975, loss_ce: 0.007110
2022-01-08 02:28:57,260 iteration 5405 : loss : 0.011832, loss_ce: 0.003906
2022-01-08 02:28:58,529 iteration 5406 : loss : 0.014832, loss_ce: 0.003389
 80%|███████████████████████      | 318/400 [2:14:21<33:08, 24.25s/it]2022-01-08 02:28:59,993 iteration 5407 : loss : 0.016335, loss_ce: 0.005516
2022-01-08 02:29:01,333 iteration 5408 : loss : 0.013314, loss_ce: 0.004123
2022-01-08 02:29:02,645 iteration 5409 : loss : 0.011501, loss_ce: 0.005647
2022-01-08 02:29:03,969 iteration 5410 : loss : 0.013106, loss_ce: 0.004444
2022-01-08 02:29:05,364 iteration 5411 : loss : 0.014746, loss_ce: 0.004831
2022-01-08 02:29:06,772 iteration 5412 : loss : 0.020176, loss_ce: 0.009971
2022-01-08 02:29:08,079 iteration 5413 : loss : 0.013599, loss_ce: 0.004852
2022-01-08 02:29:09,482 iteration 5414 : loss : 0.018131, loss_ce: 0.006586
2022-01-08 02:29:10,814 iteration 5415 : loss : 0.014077, loss_ce: 0.004888
2022-01-08 02:29:12,214 iteration 5416 : loss : 0.015766, loss_ce: 0.007100
2022-01-08 02:29:13,546 iteration 5417 : loss : 0.014897, loss_ce: 0.005501
2022-01-08 02:29:14,894 iteration 5418 : loss : 0.015880, loss_ce: 0.004813
2022-01-08 02:29:16,227 iteration 5419 : loss : 0.010830, loss_ce: 0.004677
2022-01-08 02:29:17,593 iteration 5420 : loss : 0.011993, loss_ce: 0.005258
2022-01-08 02:29:18,989 iteration 5421 : loss : 0.015428, loss_ce: 0.004984
2022-01-08 02:29:20,365 iteration 5422 : loss : 0.015892, loss_ce: 0.005166
2022-01-08 02:29:21,769 iteration 5423 : loss : 0.017383, loss_ce: 0.005363
 80%|███████████████████████▏     | 319/400 [2:14:45<32:19, 23.94s/it]2022-01-08 02:29:23,191 iteration 5424 : loss : 0.021394, loss_ce: 0.007641
2022-01-08 02:29:24,556 iteration 5425 : loss : 0.019335, loss_ce: 0.008286
2022-01-08 02:29:25,933 iteration 5426 : loss : 0.013750, loss_ce: 0.004037
2022-01-08 02:29:27,401 iteration 5427 : loss : 0.015852, loss_ce: 0.006061
2022-01-08 02:29:28,838 iteration 5428 : loss : 0.026577, loss_ce: 0.010717
2022-01-08 02:29:30,157 iteration 5429 : loss : 0.012132, loss_ce: 0.004971
2022-01-08 02:29:31,437 iteration 5430 : loss : 0.010379, loss_ce: 0.004350
2022-01-08 02:29:32,867 iteration 5431 : loss : 0.016963, loss_ce: 0.006174
2022-01-08 02:29:34,201 iteration 5432 : loss : 0.012336, loss_ce: 0.004243
2022-01-08 02:29:35,679 iteration 5433 : loss : 0.014796, loss_ce: 0.004997
2022-01-08 02:29:37,125 iteration 5434 : loss : 0.024307, loss_ce: 0.008592
2022-01-08 02:29:38,514 iteration 5435 : loss : 0.015421, loss_ce: 0.006279
2022-01-08 02:29:39,915 iteration 5436 : loss : 0.015169, loss_ce: 0.007050
2022-01-08 02:29:41,238 iteration 5437 : loss : 0.017197, loss_ce: 0.004704
2022-01-08 02:29:42,601 iteration 5438 : loss : 0.017129, loss_ce: 0.007654
2022-01-08 02:29:43,988 iteration 5439 : loss : 0.030158, loss_ce: 0.009337
2022-01-08 02:29:43,988 Training Data Eval:
2022-01-08 02:29:50,885   Average segmentation loss on training set: 0.0085
2022-01-08 02:29:50,886 Validation Data Eval:
2022-01-08 02:29:53,257   Average segmentation loss on validation set: 0.0973
2022-01-08 02:29:54,636 iteration 5440 : loss : 0.012902, loss_ce: 0.005071
 80%|███████████████████████▏     | 320/400 [2:15:17<35:29, 26.62s/it]2022-01-08 02:29:55,975 iteration 5441 : loss : 0.012051, loss_ce: 0.005263
2022-01-08 02:29:57,452 iteration 5442 : loss : 0.019774, loss_ce: 0.007336
2022-01-08 02:29:58,845 iteration 5443 : loss : 0.020701, loss_ce: 0.007686
2022-01-08 02:30:00,189 iteration 5444 : loss : 0.013741, loss_ce: 0.006756
2022-01-08 02:30:01,527 iteration 5445 : loss : 0.015593, loss_ce: 0.006228
2022-01-08 02:30:02,892 iteration 5446 : loss : 0.015374, loss_ce: 0.006343
2022-01-08 02:30:04,191 iteration 5447 : loss : 0.014411, loss_ce: 0.002354
2022-01-08 02:30:05,581 iteration 5448 : loss : 0.017750, loss_ce: 0.006449
2022-01-08 02:30:06,915 iteration 5449 : loss : 0.015644, loss_ce: 0.003993
2022-01-08 02:30:08,231 iteration 5450 : loss : 0.019006, loss_ce: 0.006074
2022-01-08 02:30:09,601 iteration 5451 : loss : 0.009304, loss_ce: 0.002615
2022-01-08 02:30:10,986 iteration 5452 : loss : 0.011873, loss_ce: 0.005063
2022-01-08 02:30:12,400 iteration 5453 : loss : 0.014651, loss_ce: 0.005471
2022-01-08 02:30:13,789 iteration 5454 : loss : 0.022123, loss_ce: 0.010407
2022-01-08 02:30:15,159 iteration 5455 : loss : 0.016437, loss_ce: 0.008546
2022-01-08 02:30:16,559 iteration 5456 : loss : 0.021279, loss_ce: 0.006602
2022-01-08 02:30:17,958 iteration 5457 : loss : 0.012443, loss_ce: 0.004901
 80%|███████████████████████▎     | 321/400 [2:15:41<33:44, 25.63s/it]2022-01-08 02:30:19,433 iteration 5458 : loss : 0.016317, loss_ce: 0.004331
2022-01-08 02:30:20,739 iteration 5459 : loss : 0.013130, loss_ce: 0.004553
2022-01-08 02:30:22,095 iteration 5460 : loss : 0.019178, loss_ce: 0.007925
2022-01-08 02:30:23,478 iteration 5461 : loss : 0.016374, loss_ce: 0.006223
2022-01-08 02:30:24,884 iteration 5462 : loss : 0.028356, loss_ce: 0.016905
2022-01-08 02:30:26,210 iteration 5463 : loss : 0.013620, loss_ce: 0.004351
2022-01-08 02:30:27,696 iteration 5464 : loss : 0.026299, loss_ce: 0.009865
2022-01-08 02:30:29,018 iteration 5465 : loss : 0.014819, loss_ce: 0.005287
2022-01-08 02:30:30,354 iteration 5466 : loss : 0.014125, loss_ce: 0.005306
2022-01-08 02:30:31,791 iteration 5467 : loss : 0.014894, loss_ce: 0.005136
2022-01-08 02:30:33,199 iteration 5468 : loss : 0.014303, loss_ce: 0.006539
2022-01-08 02:30:34,613 iteration 5469 : loss : 0.019609, loss_ce: 0.008082
2022-01-08 02:30:35,961 iteration 5470 : loss : 0.013766, loss_ce: 0.006537
2022-01-08 02:30:37,308 iteration 5471 : loss : 0.011005, loss_ce: 0.005320
2022-01-08 02:30:38,737 iteration 5472 : loss : 0.021173, loss_ce: 0.008595
2022-01-08 02:30:40,069 iteration 5473 : loss : 0.013029, loss_ce: 0.003685
2022-01-08 02:30:41,481 iteration 5474 : loss : 0.018080, loss_ce: 0.005669
 80%|███████████████████████▎     | 322/400 [2:16:04<32:29, 25.00s/it]2022-01-08 02:30:42,837 iteration 5475 : loss : 0.015561, loss_ce: 0.006843
2022-01-08 02:30:44,173 iteration 5476 : loss : 0.011880, loss_ce: 0.005371
2022-01-08 02:30:45,574 iteration 5477 : loss : 0.010062, loss_ce: 0.003348
2022-01-08 02:30:47,002 iteration 5478 : loss : 0.017791, loss_ce: 0.008667
2022-01-08 02:30:48,333 iteration 5479 : loss : 0.015892, loss_ce: 0.004214
2022-01-08 02:30:49,689 iteration 5480 : loss : 0.020918, loss_ce: 0.005267
2022-01-08 02:30:51,059 iteration 5481 : loss : 0.016160, loss_ce: 0.006571
2022-01-08 02:30:52,415 iteration 5482 : loss : 0.015664, loss_ce: 0.005930
2022-01-08 02:30:53,722 iteration 5483 : loss : 0.014425, loss_ce: 0.005470
2022-01-08 02:30:55,151 iteration 5484 : loss : 0.015327, loss_ce: 0.006019
2022-01-08 02:30:56,442 iteration 5485 : loss : 0.013825, loss_ce: 0.007156
2022-01-08 02:30:57,921 iteration 5486 : loss : 0.022169, loss_ce: 0.007209
2022-01-08 02:30:59,233 iteration 5487 : loss : 0.011003, loss_ce: 0.005649
2022-01-08 02:31:00,622 iteration 5488 : loss : 0.025599, loss_ce: 0.006909
2022-01-08 02:31:01,932 iteration 5489 : loss : 0.015930, loss_ce: 0.004206
2022-01-08 02:31:03,339 iteration 5490 : loss : 0.014483, loss_ce: 0.005133
2022-01-08 02:31:04,671 iteration 5491 : loss : 0.015290, loss_ce: 0.005400
 81%|███████████████████████▍     | 323/400 [2:16:27<31:23, 24.46s/it]2022-01-08 02:31:06,116 iteration 5492 : loss : 0.014969, loss_ce: 0.006390
2022-01-08 02:31:07,417 iteration 5493 : loss : 0.012259, loss_ce: 0.003888
2022-01-08 02:31:08,756 iteration 5494 : loss : 0.021857, loss_ce: 0.009412
2022-01-08 02:31:10,194 iteration 5495 : loss : 0.023838, loss_ce: 0.004988
2022-01-08 02:31:11,516 iteration 5496 : loss : 0.012244, loss_ce: 0.003821
2022-01-08 02:31:12,891 iteration 5497 : loss : 0.013418, loss_ce: 0.006872
2022-01-08 02:31:14,291 iteration 5498 : loss : 0.016731, loss_ce: 0.007185
2022-01-08 02:31:15,652 iteration 5499 : loss : 0.022110, loss_ce: 0.008202
2022-01-08 02:31:17,003 iteration 5500 : loss : 0.010992, loss_ce: 0.003439
2022-01-08 02:31:18,338 iteration 5501 : loss : 0.012808, loss_ce: 0.004742
2022-01-08 02:31:19,678 iteration 5502 : loss : 0.014034, loss_ce: 0.006613
2022-01-08 02:31:21,178 iteration 5503 : loss : 0.016213, loss_ce: 0.004378
2022-01-08 02:31:22,559 iteration 5504 : loss : 0.014472, loss_ce: 0.005695
2022-01-08 02:31:23,857 iteration 5505 : loss : 0.011394, loss_ce: 0.004014
2022-01-08 02:31:25,178 iteration 5506 : loss : 0.013156, loss_ce: 0.004823
2022-01-08 02:31:26,611 iteration 5507 : loss : 0.019485, loss_ce: 0.007163
2022-01-08 02:31:28,002 iteration 5508 : loss : 0.017304, loss_ce: 0.007887
 81%|███████████████████████▍     | 324/400 [2:16:51<30:33, 24.12s/it]2022-01-08 02:31:29,439 iteration 5509 : loss : 0.017345, loss_ce: 0.007780
2022-01-08 02:31:30,776 iteration 5510 : loss : 0.011921, loss_ce: 0.004311
2022-01-08 02:31:32,155 iteration 5511 : loss : 0.014860, loss_ce: 0.005028
2022-01-08 02:31:33,511 iteration 5512 : loss : 0.014802, loss_ce: 0.006559
2022-01-08 02:31:34,868 iteration 5513 : loss : 0.011256, loss_ce: 0.005814
2022-01-08 02:31:36,286 iteration 5514 : loss : 0.019800, loss_ce: 0.008833
2022-01-08 02:31:37,638 iteration 5515 : loss : 0.013736, loss_ce: 0.005165
2022-01-08 02:31:39,000 iteration 5516 : loss : 0.011709, loss_ce: 0.005491
2022-01-08 02:31:40,386 iteration 5517 : loss : 0.030393, loss_ce: 0.011558
2022-01-08 02:31:41,684 iteration 5518 : loss : 0.010537, loss_ce: 0.003579
2022-01-08 02:31:43,135 iteration 5519 : loss : 0.022248, loss_ce: 0.006262
2022-01-08 02:31:44,491 iteration 5520 : loss : 0.010161, loss_ce: 0.001675
2022-01-08 02:31:45,948 iteration 5521 : loss : 0.017714, loss_ce: 0.007385
2022-01-08 02:31:47,306 iteration 5522 : loss : 0.019142, loss_ce: 0.006697
2022-01-08 02:31:48,707 iteration 5523 : loss : 0.021724, loss_ce: 0.011182
2022-01-08 02:31:50,138 iteration 5524 : loss : 0.057251, loss_ce: 0.024757
2022-01-08 02:31:50,138 Training Data Eval:
2022-01-08 02:31:57,043   Average segmentation loss on training set: 0.0078
2022-01-08 02:31:57,043 Validation Data Eval:
2022-01-08 02:31:59,417   Average segmentation loss on validation set: 0.0733
2022-01-08 02:32:00,885 iteration 5525 : loss : 0.016580, loss_ce: 0.007005
 81%|███████████████████████▌     | 325/400 [2:17:24<33:26, 26.75s/it]2022-01-08 02:32:02,281 iteration 5526 : loss : 0.015648, loss_ce: 0.005047
2022-01-08 02:32:03,581 iteration 5527 : loss : 0.011551, loss_ce: 0.004827
2022-01-08 02:32:05,013 iteration 5528 : loss : 0.028235, loss_ce: 0.009575
2022-01-08 02:32:06,332 iteration 5529 : loss : 0.012624, loss_ce: 0.004763
2022-01-08 02:32:07,653 iteration 5530 : loss : 0.012995, loss_ce: 0.005382
2022-01-08 02:32:08,934 iteration 5531 : loss : 0.010960, loss_ce: 0.003310
2022-01-08 02:32:10,256 iteration 5532 : loss : 0.012558, loss_ce: 0.004909
2022-01-08 02:32:11,658 iteration 5533 : loss : 0.019702, loss_ce: 0.009676
2022-01-08 02:32:12,941 iteration 5534 : loss : 0.013063, loss_ce: 0.005496
2022-01-08 02:32:14,279 iteration 5535 : loss : 0.013839, loss_ce: 0.005770
2022-01-08 02:32:15,693 iteration 5536 : loss : 0.021562, loss_ce: 0.008190
2022-01-08 02:32:17,098 iteration 5537 : loss : 0.017067, loss_ce: 0.007766
2022-01-08 02:32:18,539 iteration 5538 : loss : 0.018318, loss_ce: 0.005423
2022-01-08 02:32:19,901 iteration 5539 : loss : 0.012426, loss_ce: 0.004515
2022-01-08 02:32:21,203 iteration 5540 : loss : 0.014712, loss_ce: 0.006136
2022-01-08 02:32:22,666 iteration 5541 : loss : 0.027560, loss_ce: 0.012949
2022-01-08 02:32:24,040 iteration 5542 : loss : 0.019788, loss_ce: 0.007898
 82%|███████████████████████▋     | 326/400 [2:17:47<31:39, 25.67s/it]2022-01-08 02:32:25,493 iteration 5543 : loss : 0.018401, loss_ce: 0.008529
2022-01-08 02:32:26,807 iteration 5544 : loss : 0.013438, loss_ce: 0.004835
2022-01-08 02:32:28,222 iteration 5545 : loss : 0.019994, loss_ce: 0.009453
2022-01-08 02:32:29,690 iteration 5546 : loss : 0.033621, loss_ce: 0.009564
2022-01-08 02:32:31,095 iteration 5547 : loss : 0.016849, loss_ce: 0.007989
2022-01-08 02:32:32,442 iteration 5548 : loss : 0.015751, loss_ce: 0.004030
2022-01-08 02:32:33,789 iteration 5549 : loss : 0.011603, loss_ce: 0.003316
2022-01-08 02:32:35,173 iteration 5550 : loss : 0.012979, loss_ce: 0.005210
2022-01-08 02:32:36,511 iteration 5551 : loss : 0.021651, loss_ce: 0.011395
2022-01-08 02:32:37,759 iteration 5552 : loss : 0.012192, loss_ce: 0.003982
2022-01-08 02:32:39,132 iteration 5553 : loss : 0.019873, loss_ce: 0.008956
2022-01-08 02:32:40,480 iteration 5554 : loss : 0.013860, loss_ce: 0.005929
2022-01-08 02:32:41,766 iteration 5555 : loss : 0.012951, loss_ce: 0.005978
2022-01-08 02:32:43,194 iteration 5556 : loss : 0.015552, loss_ce: 0.005585
2022-01-08 02:32:44,575 iteration 5557 : loss : 0.010788, loss_ce: 0.004420
2022-01-08 02:32:45,931 iteration 5558 : loss : 0.017009, loss_ce: 0.005508
2022-01-08 02:32:47,284 iteration 5559 : loss : 0.017638, loss_ce: 0.005050
 82%|███████████████████████▋     | 327/400 [2:18:10<30:20, 24.94s/it]2022-01-08 02:32:48,713 iteration 5560 : loss : 0.016291, loss_ce: 0.005361
2022-01-08 02:32:50,057 iteration 5561 : loss : 0.017463, loss_ce: 0.006177
2022-01-08 02:32:51,406 iteration 5562 : loss : 0.013772, loss_ce: 0.005834
2022-01-08 02:32:52,749 iteration 5563 : loss : 0.017445, loss_ce: 0.007945
2022-01-08 02:32:54,150 iteration 5564 : loss : 0.017203, loss_ce: 0.007998
2022-01-08 02:32:55,539 iteration 5565 : loss : 0.018484, loss_ce: 0.008902
2022-01-08 02:32:56,900 iteration 5566 : loss : 0.017093, loss_ce: 0.006385
2022-01-08 02:32:58,306 iteration 5567 : loss : 0.019062, loss_ce: 0.009857
2022-01-08 02:32:59,650 iteration 5568 : loss : 0.018362, loss_ce: 0.006641
2022-01-08 02:33:01,014 iteration 5569 : loss : 0.013094, loss_ce: 0.004489
2022-01-08 02:33:02,322 iteration 5570 : loss : 0.008997, loss_ce: 0.002544
2022-01-08 02:33:03,597 iteration 5571 : loss : 0.009676, loss_ce: 0.003670
2022-01-08 02:33:04,920 iteration 5572 : loss : 0.017233, loss_ce: 0.005364
2022-01-08 02:33:06,208 iteration 5573 : loss : 0.008413, loss_ce: 0.002410
2022-01-08 02:33:07,563 iteration 5574 : loss : 0.010941, loss_ce: 0.003438
2022-01-08 02:33:09,022 iteration 5575 : loss : 0.022335, loss_ce: 0.007620
2022-01-08 02:33:10,400 iteration 5576 : loss : 0.015424, loss_ce: 0.005458
 82%|███████████████████████▊     | 328/400 [2:18:33<29:16, 24.39s/it]2022-01-08 02:33:11,803 iteration 5577 : loss : 0.016490, loss_ce: 0.008153
2022-01-08 02:33:13,194 iteration 5578 : loss : 0.017786, loss_ce: 0.006412
2022-01-08 02:33:14,626 iteration 5579 : loss : 0.034549, loss_ce: 0.017767
2022-01-08 02:33:15,968 iteration 5580 : loss : 0.021085, loss_ce: 0.006824
2022-01-08 02:33:17,287 iteration 5581 : loss : 0.010359, loss_ce: 0.004031
2022-01-08 02:33:18,661 iteration 5582 : loss : 0.012222, loss_ce: 0.003586
2022-01-08 02:33:20,022 iteration 5583 : loss : 0.009097, loss_ce: 0.003347
2022-01-08 02:33:21,447 iteration 5584 : loss : 0.018524, loss_ce: 0.008096
2022-01-08 02:33:22,798 iteration 5585 : loss : 0.014099, loss_ce: 0.004130
2022-01-08 02:33:24,221 iteration 5586 : loss : 0.020356, loss_ce: 0.006603
2022-01-08 02:33:25,526 iteration 5587 : loss : 0.015611, loss_ce: 0.006091
2022-01-08 02:33:26,867 iteration 5588 : loss : 0.013294, loss_ce: 0.004745
2022-01-08 02:33:28,174 iteration 5589 : loss : 0.009915, loss_ce: 0.002701
2022-01-08 02:33:29,506 iteration 5590 : loss : 0.017871, loss_ce: 0.003850
2022-01-08 02:33:30,860 iteration 5591 : loss : 0.012047, loss_ce: 0.005112
2022-01-08 02:33:32,313 iteration 5592 : loss : 0.016623, loss_ce: 0.005607
2022-01-08 02:33:33,700 iteration 5593 : loss : 0.013531, loss_ce: 0.005875
 82%|███████████████████████▊     | 329/400 [2:18:57<28:28, 24.07s/it]2022-01-08 02:33:35,076 iteration 5594 : loss : 0.015772, loss_ce: 0.004367
2022-01-08 02:33:36,469 iteration 5595 : loss : 0.017516, loss_ce: 0.008317
2022-01-08 02:33:37,812 iteration 5596 : loss : 0.015126, loss_ce: 0.005516
2022-01-08 02:33:39,235 iteration 5597 : loss : 0.014056, loss_ce: 0.005535
2022-01-08 02:33:40,543 iteration 5598 : loss : 0.009959, loss_ce: 0.004251
2022-01-08 02:33:41,909 iteration 5599 : loss : 0.018210, loss_ce: 0.008424
2022-01-08 02:33:43,261 iteration 5600 : loss : 0.011899, loss_ce: 0.005140
2022-01-08 02:33:44,597 iteration 5601 : loss : 0.011251, loss_ce: 0.003792
2022-01-08 02:33:45,985 iteration 5602 : loss : 0.013525, loss_ce: 0.006149
2022-01-08 02:33:47,293 iteration 5603 : loss : 0.011003, loss_ce: 0.004804
2022-01-08 02:33:48,679 iteration 5604 : loss : 0.016879, loss_ce: 0.004271
2022-01-08 02:33:50,070 iteration 5605 : loss : 0.025052, loss_ce: 0.009584
2022-01-08 02:33:51,443 iteration 5606 : loss : 0.012323, loss_ce: 0.003749
2022-01-08 02:33:52,798 iteration 5607 : loss : 0.016000, loss_ce: 0.004772
2022-01-08 02:33:54,163 iteration 5608 : loss : 0.014446, loss_ce: 0.004436
2022-01-08 02:33:55,561 iteration 5609 : loss : 0.018469, loss_ce: 0.007408
2022-01-08 02:33:55,561 Training Data Eval:
2022-01-08 02:34:02,459   Average segmentation loss on training set: 0.0077
2022-01-08 02:34:02,460 Validation Data Eval:
2022-01-08 02:34:04,834   Average segmentation loss on validation set: 0.0805
2022-01-08 02:34:06,173 iteration 5610 : loss : 0.012333, loss_ce: 0.004327
 82%|███████████████████████▉     | 330/400 [2:19:29<31:00, 26.59s/it]2022-01-08 02:34:07,559 iteration 5611 : loss : 0.017902, loss_ce: 0.005877
2022-01-08 02:34:08,941 iteration 5612 : loss : 0.020547, loss_ce: 0.003982
2022-01-08 02:34:10,305 iteration 5613 : loss : 0.012231, loss_ce: 0.004680
2022-01-08 02:34:11,654 iteration 5614 : loss : 0.014317, loss_ce: 0.006019
2022-01-08 02:34:12,925 iteration 5615 : loss : 0.009395, loss_ce: 0.004008
2022-01-08 02:34:14,311 iteration 5616 : loss : 0.015332, loss_ce: 0.006571
2022-01-08 02:34:15,726 iteration 5617 : loss : 0.013058, loss_ce: 0.004799
2022-01-08 02:34:17,106 iteration 5618 : loss : 0.016078, loss_ce: 0.007407
2022-01-08 02:34:18,426 iteration 5619 : loss : 0.017357, loss_ce: 0.005925
2022-01-08 02:34:19,861 iteration 5620 : loss : 0.014298, loss_ce: 0.005413
2022-01-08 02:34:21,158 iteration 5621 : loss : 0.020089, loss_ce: 0.007010
2022-01-08 02:34:22,524 iteration 5622 : loss : 0.012473, loss_ce: 0.004431
2022-01-08 02:34:23,962 iteration 5623 : loss : 0.017091, loss_ce: 0.006155
2022-01-08 02:34:25,268 iteration 5624 : loss : 0.013438, loss_ce: 0.003656
2022-01-08 02:34:26,573 iteration 5625 : loss : 0.014760, loss_ce: 0.007180
2022-01-08 02:34:27,917 iteration 5626 : loss : 0.013228, loss_ce: 0.003752
2022-01-08 02:34:29,316 iteration 5627 : loss : 0.012878, loss_ce: 0.005332
 83%|███████████████████████▉     | 331/400 [2:19:52<29:23, 25.55s/it]2022-01-08 02:34:30,814 iteration 5628 : loss : 0.016965, loss_ce: 0.008397
2022-01-08 02:34:32,120 iteration 5629 : loss : 0.014030, loss_ce: 0.006322
2022-01-08 02:34:33,513 iteration 5630 : loss : 0.023333, loss_ce: 0.011495
2022-01-08 02:34:34,866 iteration 5631 : loss : 0.012207, loss_ce: 0.004665
2022-01-08 02:34:36,265 iteration 5632 : loss : 0.016382, loss_ce: 0.008051
2022-01-08 02:34:37,680 iteration 5633 : loss : 0.020242, loss_ce: 0.008247
2022-01-08 02:34:39,052 iteration 5634 : loss : 0.008921, loss_ce: 0.003828
2022-01-08 02:34:40,403 iteration 5635 : loss : 0.015565, loss_ce: 0.005468
2022-01-08 02:34:41,780 iteration 5636 : loss : 0.015528, loss_ce: 0.003814
2022-01-08 02:34:43,132 iteration 5637 : loss : 0.012994, loss_ce: 0.004075
2022-01-08 02:34:44,498 iteration 5638 : loss : 0.013880, loss_ce: 0.005656
2022-01-08 02:34:45,881 iteration 5639 : loss : 0.010326, loss_ce: 0.003759
2022-01-08 02:34:47,261 iteration 5640 : loss : 0.012401, loss_ce: 0.005056
2022-01-08 02:34:48,555 iteration 5641 : loss : 0.010570, loss_ce: 0.003139
2022-01-08 02:34:49,870 iteration 5642 : loss : 0.012832, loss_ce: 0.004761
2022-01-08 02:34:51,215 iteration 5643 : loss : 0.009942, loss_ce: 0.003215
2022-01-08 02:34:52,561 iteration 5644 : loss : 0.010889, loss_ce: 0.004597
 83%|████████████████████████     | 332/400 [2:20:15<28:10, 24.86s/it]2022-01-08 02:34:53,940 iteration 5645 : loss : 0.015033, loss_ce: 0.004449
2022-01-08 02:34:55,294 iteration 5646 : loss : 0.012380, loss_ce: 0.004496
2022-01-08 02:34:56,683 iteration 5647 : loss : 0.015056, loss_ce: 0.004635
2022-01-08 02:34:58,065 iteration 5648 : loss : 0.017409, loss_ce: 0.004537
2022-01-08 02:34:59,425 iteration 5649 : loss : 0.010602, loss_ce: 0.003931
2022-01-08 02:35:00,827 iteration 5650 : loss : 0.022575, loss_ce: 0.005773
2022-01-08 02:35:02,180 iteration 5651 : loss : 0.011270, loss_ce: 0.005455
2022-01-08 02:35:03,511 iteration 5652 : loss : 0.015385, loss_ce: 0.006173
2022-01-08 02:35:04,845 iteration 5653 : loss : 0.011484, loss_ce: 0.004047
2022-01-08 02:35:06,279 iteration 5654 : loss : 0.012683, loss_ce: 0.004592
2022-01-08 02:35:07,641 iteration 5655 : loss : 0.013829, loss_ce: 0.005219
2022-01-08 02:35:09,051 iteration 5656 : loss : 0.026386, loss_ce: 0.014986
2022-01-08 02:35:10,388 iteration 5657 : loss : 0.019965, loss_ce: 0.010674
2022-01-08 02:35:11,817 iteration 5658 : loss : 0.020800, loss_ce: 0.006571
2022-01-08 02:35:13,263 iteration 5659 : loss : 0.018417, loss_ce: 0.012006
2022-01-08 02:35:14,535 iteration 5660 : loss : 0.009860, loss_ce: 0.004071
2022-01-08 02:35:15,932 iteration 5661 : loss : 0.018570, loss_ce: 0.006625
 83%|████████████████████████▏    | 333/400 [2:20:39<27:15, 24.41s/it]2022-01-08 02:35:17,294 iteration 5662 : loss : 0.012490, loss_ce: 0.005265
2022-01-08 02:35:18,693 iteration 5663 : loss : 0.012558, loss_ce: 0.005282
2022-01-08 02:35:20,116 iteration 5664 : loss : 0.019658, loss_ce: 0.007057
2022-01-08 02:35:21,463 iteration 5665 : loss : 0.009767, loss_ce: 0.002192
2022-01-08 02:35:22,895 iteration 5666 : loss : 0.016643, loss_ce: 0.006180
2022-01-08 02:35:24,279 iteration 5667 : loss : 0.016074, loss_ce: 0.005520
2022-01-08 02:35:25,654 iteration 5668 : loss : 0.013626, loss_ce: 0.004485
2022-01-08 02:35:27,096 iteration 5669 : loss : 0.019278, loss_ce: 0.010325
2022-01-08 02:35:28,472 iteration 5670 : loss : 0.016292, loss_ce: 0.005057
2022-01-08 02:35:29,896 iteration 5671 : loss : 0.021010, loss_ce: 0.005884
2022-01-08 02:35:31,172 iteration 5672 : loss : 0.014349, loss_ce: 0.003945
2022-01-08 02:35:32,532 iteration 5673 : loss : 0.013524, loss_ce: 0.005926
2022-01-08 02:35:33,845 iteration 5674 : loss : 0.017906, loss_ce: 0.005994
2022-01-08 02:35:35,319 iteration 5675 : loss : 0.022429, loss_ce: 0.008523
2022-01-08 02:35:36,659 iteration 5676 : loss : 0.017833, loss_ce: 0.009215
2022-01-08 02:35:37,989 iteration 5677 : loss : 0.016244, loss_ce: 0.005688
2022-01-08 02:35:39,340 iteration 5678 : loss : 0.014426, loss_ce: 0.005469
 84%|████████████████████████▏    | 334/400 [2:21:02<26:31, 24.11s/it]2022-01-08 02:35:40,756 iteration 5679 : loss : 0.016885, loss_ce: 0.005516
2022-01-08 02:35:42,104 iteration 5680 : loss : 0.011636, loss_ce: 0.005023
2022-01-08 02:35:43,493 iteration 5681 : loss : 0.014128, loss_ce: 0.004024
2022-01-08 02:35:44,780 iteration 5682 : loss : 0.011573, loss_ce: 0.003566
2022-01-08 02:35:46,059 iteration 5683 : loss : 0.008777, loss_ce: 0.003816
2022-01-08 02:35:47,469 iteration 5684 : loss : 0.015935, loss_ce: 0.008303
2022-01-08 02:35:48,844 iteration 5685 : loss : 0.017957, loss_ce: 0.006257
2022-01-08 02:35:50,203 iteration 5686 : loss : 0.011868, loss_ce: 0.004455
2022-01-08 02:35:51,596 iteration 5687 : loss : 0.013043, loss_ce: 0.003686
2022-01-08 02:35:53,024 iteration 5688 : loss : 0.022607, loss_ce: 0.007897
2022-01-08 02:35:54,362 iteration 5689 : loss : 0.011688, loss_ce: 0.004897
2022-01-08 02:35:55,668 iteration 5690 : loss : 0.011953, loss_ce: 0.004180
2022-01-08 02:35:57,077 iteration 5691 : loss : 0.013582, loss_ce: 0.005649
2022-01-08 02:35:58,449 iteration 5692 : loss : 0.009423, loss_ce: 0.002688
2022-01-08 02:35:59,690 iteration 5693 : loss : 0.009806, loss_ce: 0.004226
2022-01-08 02:36:01,072 iteration 5694 : loss : 0.012716, loss_ce: 0.005252
2022-01-08 02:36:01,073 Training Data Eval:
2022-01-08 02:36:07,973   Average segmentation loss on training set: 0.0074
2022-01-08 02:36:07,974 Validation Data Eval:
2022-01-08 02:36:10,339   Average segmentation loss on validation set: 0.0751
2022-01-08 02:36:11,692 iteration 5695 : loss : 0.013747, loss_ce: 0.005485
 84%|████████████████████████▎    | 335/400 [2:21:35<28:47, 26.58s/it]2022-01-08 02:36:13,104 iteration 5696 : loss : 0.016484, loss_ce: 0.004975
2022-01-08 02:36:14,415 iteration 5697 : loss : 0.015648, loss_ce: 0.005883
2022-01-08 02:36:15,834 iteration 5698 : loss : 0.013481, loss_ce: 0.004752
2022-01-08 02:36:17,269 iteration 5699 : loss : 0.013642, loss_ce: 0.003901
2022-01-08 02:36:18,545 iteration 5700 : loss : 0.010302, loss_ce: 0.004277
2022-01-08 02:36:19,993 iteration 5701 : loss : 0.022504, loss_ce: 0.004634
2022-01-08 02:36:21,376 iteration 5702 : loss : 0.014280, loss_ce: 0.005242
2022-01-08 02:36:22,807 iteration 5703 : loss : 0.021114, loss_ce: 0.008933
2022-01-08 02:36:24,135 iteration 5704 : loss : 0.017133, loss_ce: 0.007275
2022-01-08 02:36:25,457 iteration 5705 : loss : 0.012064, loss_ce: 0.004860
2022-01-08 02:36:26,771 iteration 5706 : loss : 0.012231, loss_ce: 0.005202
2022-01-08 02:36:28,154 iteration 5707 : loss : 0.029689, loss_ce: 0.007897
2022-01-08 02:36:29,506 iteration 5708 : loss : 0.012369, loss_ce: 0.006734
2022-01-08 02:36:30,810 iteration 5709 : loss : 0.014489, loss_ce: 0.005479
2022-01-08 02:36:32,151 iteration 5710 : loss : 0.013333, loss_ce: 0.006362
2022-01-08 02:36:33,531 iteration 5711 : loss : 0.011883, loss_ce: 0.005287
2022-01-08 02:36:34,912 iteration 5712 : loss : 0.016447, loss_ce: 0.007060
 84%|████████████████████████▎    | 336/400 [2:21:58<27:16, 25.58s/it]2022-01-08 02:36:36,333 iteration 5713 : loss : 0.012156, loss_ce: 0.006219
2022-01-08 02:36:37,718 iteration 5714 : loss : 0.015499, loss_ce: 0.005187
2022-01-08 02:36:39,022 iteration 5715 : loss : 0.011546, loss_ce: 0.004571
2022-01-08 02:36:40,421 iteration 5716 : loss : 0.018536, loss_ce: 0.007871
2022-01-08 02:36:41,719 iteration 5717 : loss : 0.010800, loss_ce: 0.003964
2022-01-08 02:36:43,040 iteration 5718 : loss : 0.017604, loss_ce: 0.007757
2022-01-08 02:36:44,441 iteration 5719 : loss : 0.016122, loss_ce: 0.004848
2022-01-08 02:36:45,872 iteration 5720 : loss : 0.023422, loss_ce: 0.011642
2022-01-08 02:36:47,193 iteration 5721 : loss : 0.014487, loss_ce: 0.004376
2022-01-08 02:36:48,590 iteration 5722 : loss : 0.013888, loss_ce: 0.005007
2022-01-08 02:36:50,008 iteration 5723 : loss : 0.034141, loss_ce: 0.014391
2022-01-08 02:36:51,384 iteration 5724 : loss : 0.014900, loss_ce: 0.006867
2022-01-08 02:36:52,721 iteration 5725 : loss : 0.012948, loss_ce: 0.005156
2022-01-08 02:36:54,120 iteration 5726 : loss : 0.011973, loss_ce: 0.005604
2022-01-08 02:36:55,489 iteration 5727 : loss : 0.016574, loss_ce: 0.008304
2022-01-08 02:36:56,809 iteration 5728 : loss : 0.011858, loss_ce: 0.003932
2022-01-08 02:36:58,094 iteration 5729 : loss : 0.013422, loss_ce: 0.003966
 84%|████████████████████████▍    | 337/400 [2:22:21<26:05, 24.86s/it]2022-01-08 02:36:59,567 iteration 5730 : loss : 0.019027, loss_ce: 0.006964
2022-01-08 02:37:00,988 iteration 5731 : loss : 0.023040, loss_ce: 0.007306
2022-01-08 02:37:02,382 iteration 5732 : loss : 0.031411, loss_ce: 0.005098
2022-01-08 02:37:03,728 iteration 5733 : loss : 0.014561, loss_ce: 0.005899
2022-01-08 02:37:05,053 iteration 5734 : loss : 0.008768, loss_ce: 0.003037
2022-01-08 02:37:06,524 iteration 5735 : loss : 0.021552, loss_ce: 0.007124
2022-01-08 02:37:07,950 iteration 5736 : loss : 0.014818, loss_ce: 0.006963
2022-01-08 02:37:09,306 iteration 5737 : loss : 0.012526, loss_ce: 0.005258
2022-01-08 02:37:10,628 iteration 5738 : loss : 0.012899, loss_ce: 0.005545
2022-01-08 02:37:12,014 iteration 5739 : loss : 0.012738, loss_ce: 0.003780
2022-01-08 02:37:13,433 iteration 5740 : loss : 0.019405, loss_ce: 0.007627
2022-01-08 02:37:14,792 iteration 5741 : loss : 0.011253, loss_ce: 0.004514
2022-01-08 02:37:16,125 iteration 5742 : loss : 0.028669, loss_ce: 0.004047
2022-01-08 02:37:17,497 iteration 5743 : loss : 0.014016, loss_ce: 0.004935
2022-01-08 02:37:18,831 iteration 5744 : loss : 0.011875, loss_ce: 0.005243
2022-01-08 02:37:20,205 iteration 5745 : loss : 0.010418, loss_ce: 0.003607
2022-01-08 02:37:21,527 iteration 5746 : loss : 0.014157, loss_ce: 0.006658
 84%|████████████████████████▌    | 338/400 [2:22:44<25:14, 24.43s/it]2022-01-08 02:37:22,963 iteration 5747 : loss : 0.016307, loss_ce: 0.005740
2022-01-08 02:37:24,298 iteration 5748 : loss : 0.015437, loss_ce: 0.006253
2022-01-08 02:37:25,706 iteration 5749 : loss : 0.021468, loss_ce: 0.010362
2022-01-08 02:37:27,080 iteration 5750 : loss : 0.016098, loss_ce: 0.004584
2022-01-08 02:37:28,500 iteration 5751 : loss : 0.019538, loss_ce: 0.007272
2022-01-08 02:37:29,872 iteration 5752 : loss : 0.015322, loss_ce: 0.005465
2022-01-08 02:37:31,283 iteration 5753 : loss : 0.015664, loss_ce: 0.007717
2022-01-08 02:37:32,630 iteration 5754 : loss : 0.020607, loss_ce: 0.006462
2022-01-08 02:37:33,970 iteration 5755 : loss : 0.023030, loss_ce: 0.006866
2022-01-08 02:37:35,312 iteration 5756 : loss : 0.022669, loss_ce: 0.009664
2022-01-08 02:37:36,695 iteration 5757 : loss : 0.015572, loss_ce: 0.004795
2022-01-08 02:37:37,993 iteration 5758 : loss : 0.008340, loss_ce: 0.002463
2022-01-08 02:37:39,414 iteration 5759 : loss : 0.021644, loss_ce: 0.005205
2022-01-08 02:37:40,754 iteration 5760 : loss : 0.012247, loss_ce: 0.004496
2022-01-08 02:37:42,153 iteration 5761 : loss : 0.020136, loss_ce: 0.006916
2022-01-08 02:37:43,583 iteration 5762 : loss : 0.019882, loss_ce: 0.009173
2022-01-08 02:37:44,974 iteration 5763 : loss : 0.018728, loss_ce: 0.007434
 85%|████████████████████████▌    | 339/400 [2:23:08<24:32, 24.13s/it]2022-01-08 02:37:46,426 iteration 5764 : loss : 0.020958, loss_ce: 0.012846
2022-01-08 02:37:47,788 iteration 5765 : loss : 0.016225, loss_ce: 0.004996
2022-01-08 02:37:49,118 iteration 5766 : loss : 0.014860, loss_ce: 0.005077
2022-01-08 02:37:50,519 iteration 5767 : loss : 0.016493, loss_ce: 0.007282
2022-01-08 02:37:51,912 iteration 5768 : loss : 0.021123, loss_ce: 0.006898
2022-01-08 02:37:53,280 iteration 5769 : loss : 0.013579, loss_ce: 0.003898
2022-01-08 02:37:54,665 iteration 5770 : loss : 0.022148, loss_ce: 0.008619
2022-01-08 02:37:55,944 iteration 5771 : loss : 0.009453, loss_ce: 0.003866
2022-01-08 02:37:57,307 iteration 5772 : loss : 0.012527, loss_ce: 0.004629
2022-01-08 02:37:58,596 iteration 5773 : loss : 0.011194, loss_ce: 0.003850
2022-01-08 02:37:59,918 iteration 5774 : loss : 0.011399, loss_ce: 0.004540
2022-01-08 02:38:01,295 iteration 5775 : loss : 0.018885, loss_ce: 0.007442
2022-01-08 02:38:02,560 iteration 5776 : loss : 0.013785, loss_ce: 0.005386
2022-01-08 02:38:03,901 iteration 5777 : loss : 0.013577, loss_ce: 0.005439
2022-01-08 02:38:05,289 iteration 5778 : loss : 0.012544, loss_ce: 0.004651
2022-01-08 02:38:06,658 iteration 5779 : loss : 0.009768, loss_ce: 0.004119
2022-01-08 02:38:06,659 Training Data Eval:
2022-01-08 02:38:13,562   Average segmentation loss on training set: 0.0076
2022-01-08 02:38:13,562 Validation Data Eval:
2022-01-08 02:38:15,943   Average segmentation loss on validation set: 0.0793
2022-01-08 02:38:17,300 iteration 5780 : loss : 0.014817, loss_ce: 0.005709
 85%|████████████████████████▋    | 340/400 [2:23:40<26:35, 26.59s/it]2022-01-08 02:38:18,698 iteration 5781 : loss : 0.015372, loss_ce: 0.005410
2022-01-08 02:38:19,978 iteration 5782 : loss : 0.008879, loss_ce: 0.003154
2022-01-08 02:38:21,362 iteration 5783 : loss : 0.011111, loss_ce: 0.003548
2022-01-08 02:38:22,775 iteration 5784 : loss : 0.013659, loss_ce: 0.004345
2022-01-08 02:38:24,161 iteration 5785 : loss : 0.012970, loss_ce: 0.006513
2022-01-08 02:38:25,575 iteration 5786 : loss : 0.020449, loss_ce: 0.008465
2022-01-08 02:38:26,931 iteration 5787 : loss : 0.014106, loss_ce: 0.004606
2022-01-08 02:38:28,297 iteration 5788 : loss : 0.015957, loss_ce: 0.006806
2022-01-08 02:38:29,650 iteration 5789 : loss : 0.017547, loss_ce: 0.005420
2022-01-08 02:38:31,038 iteration 5790 : loss : 0.020559, loss_ce: 0.009144
2022-01-08 02:38:32,433 iteration 5791 : loss : 0.010933, loss_ce: 0.004364
2022-01-08 02:38:33,863 iteration 5792 : loss : 0.019339, loss_ce: 0.005545
2022-01-08 02:38:35,189 iteration 5793 : loss : 0.013357, loss_ce: 0.005428
2022-01-08 02:38:36,501 iteration 5794 : loss : 0.013492, loss_ce: 0.004406
2022-01-08 02:38:37,834 iteration 5795 : loss : 0.013607, loss_ce: 0.006073
2022-01-08 02:38:39,174 iteration 5796 : loss : 0.013086, loss_ce: 0.005210
2022-01-08 02:38:40,520 iteration 5797 : loss : 0.014083, loss_ce: 0.005399
 85%|████████████████████████▋    | 341/400 [2:24:03<25:09, 25.58s/it]2022-01-08 02:38:41,873 iteration 5798 : loss : 0.011527, loss_ce: 0.003581
2022-01-08 02:38:43,197 iteration 5799 : loss : 0.010833, loss_ce: 0.004063
2022-01-08 02:38:44,463 iteration 5800 : loss : 0.008205, loss_ce: 0.003299
2022-01-08 02:38:45,835 iteration 5801 : loss : 0.013319, loss_ce: 0.006525
2022-01-08 02:38:47,145 iteration 5802 : loss : 0.015068, loss_ce: 0.006730
2022-01-08 02:38:48,532 iteration 5803 : loss : 0.014736, loss_ce: 0.005415
2022-01-08 02:38:49,846 iteration 5804 : loss : 0.015635, loss_ce: 0.005140
2022-01-08 02:38:51,157 iteration 5805 : loss : 0.013725, loss_ce: 0.004115
2022-01-08 02:38:52,452 iteration 5806 : loss : 0.009543, loss_ce: 0.004290
2022-01-08 02:38:53,837 iteration 5807 : loss : 0.034237, loss_ce: 0.012513
2022-01-08 02:38:55,246 iteration 5808 : loss : 0.017947, loss_ce: 0.006271
2022-01-08 02:38:56,647 iteration 5809 : loss : 0.014261, loss_ce: 0.005020
2022-01-08 02:38:58,079 iteration 5810 : loss : 0.015729, loss_ce: 0.004198
2022-01-08 02:38:59,468 iteration 5811 : loss : 0.015773, loss_ce: 0.006111
2022-01-08 02:39:00,802 iteration 5812 : loss : 0.012976, loss_ce: 0.005565
2022-01-08 02:39:02,121 iteration 5813 : loss : 0.011928, loss_ce: 0.003710
2022-01-08 02:39:03,457 iteration 5814 : loss : 0.021588, loss_ce: 0.006827
 86%|████████████████████████▊    | 342/400 [2:24:26<23:57, 24.79s/it]2022-01-08 02:39:04,898 iteration 5815 : loss : 0.016057, loss_ce: 0.006144
2022-01-08 02:39:06,248 iteration 5816 : loss : 0.012658, loss_ce: 0.004477
2022-01-08 02:39:07,672 iteration 5817 : loss : 0.023095, loss_ce: 0.015630
2022-01-08 02:39:09,061 iteration 5818 : loss : 0.021857, loss_ce: 0.008763
2022-01-08 02:39:10,397 iteration 5819 : loss : 0.009586, loss_ce: 0.002572
2022-01-08 02:39:11,731 iteration 5820 : loss : 0.014341, loss_ce: 0.004823
2022-01-08 02:39:13,115 iteration 5821 : loss : 0.012215, loss_ce: 0.004618
2022-01-08 02:39:14,550 iteration 5822 : loss : 0.016344, loss_ce: 0.005116
2022-01-08 02:39:15,852 iteration 5823 : loss : 0.010500, loss_ce: 0.004902
2022-01-08 02:39:17,293 iteration 5824 : loss : 0.038115, loss_ce: 0.008332
2022-01-08 02:39:18,662 iteration 5825 : loss : 0.017018, loss_ce: 0.005192
2022-01-08 02:39:20,027 iteration 5826 : loss : 0.013641, loss_ce: 0.006348
2022-01-08 02:39:21,329 iteration 5827 : loss : 0.013860, loss_ce: 0.006382
2022-01-08 02:39:22,768 iteration 5828 : loss : 0.027424, loss_ce: 0.005609
2022-01-08 02:39:24,126 iteration 5829 : loss : 0.016700, loss_ce: 0.005664
2022-01-08 02:39:25,439 iteration 5830 : loss : 0.015189, loss_ce: 0.005305
2022-01-08 02:39:26,851 iteration 5831 : loss : 0.020517, loss_ce: 0.007373
 86%|████████████████████████▊    | 343/400 [2:24:50<23:09, 24.37s/it]2022-01-08 02:39:28,273 iteration 5832 : loss : 0.018387, loss_ce: 0.007349
2022-01-08 02:39:29,686 iteration 5833 : loss : 0.015605, loss_ce: 0.005299
2022-01-08 02:39:31,038 iteration 5834 : loss : 0.013452, loss_ce: 0.006432
2022-01-08 02:39:32,385 iteration 5835 : loss : 0.013206, loss_ce: 0.005113
2022-01-08 02:39:33,728 iteration 5836 : loss : 0.022999, loss_ce: 0.007734
2022-01-08 02:39:35,154 iteration 5837 : loss : 0.020336, loss_ce: 0.007710
2022-01-08 02:39:36,516 iteration 5838 : loss : 0.014873, loss_ce: 0.004014
2022-01-08 02:39:37,884 iteration 5839 : loss : 0.016119, loss_ce: 0.005945
2022-01-08 02:39:39,247 iteration 5840 : loss : 0.011574, loss_ce: 0.004075
2022-01-08 02:39:40,655 iteration 5841 : loss : 0.022853, loss_ce: 0.007712
2022-01-08 02:39:41,957 iteration 5842 : loss : 0.022791, loss_ce: 0.006492
2022-01-08 02:39:43,267 iteration 5843 : loss : 0.013504, loss_ce: 0.004924
2022-01-08 02:39:44,632 iteration 5844 : loss : 0.014087, loss_ce: 0.005811
2022-01-08 02:39:45,943 iteration 5845 : loss : 0.010334, loss_ce: 0.003884
2022-01-08 02:39:47,262 iteration 5846 : loss : 0.010537, loss_ce: 0.004426
2022-01-08 02:39:48,587 iteration 5847 : loss : 0.014023, loss_ce: 0.005831
2022-01-08 02:39:49,917 iteration 5848 : loss : 0.011241, loss_ce: 0.003705
 86%|████████████████████████▉    | 344/400 [2:25:13<22:22, 23.98s/it]2022-01-08 02:39:51,318 iteration 5849 : loss : 0.016620, loss_ce: 0.004617
2022-01-08 02:39:52,743 iteration 5850 : loss : 0.016778, loss_ce: 0.008989
2022-01-08 02:39:54,135 iteration 5851 : loss : 0.012653, loss_ce: 0.003284
2022-01-08 02:39:55,499 iteration 5852 : loss : 0.015160, loss_ce: 0.007569
2022-01-08 02:39:56,812 iteration 5853 : loss : 0.021304, loss_ce: 0.005873
2022-01-08 02:39:58,207 iteration 5854 : loss : 0.018143, loss_ce: 0.006338
2022-01-08 02:39:59,572 iteration 5855 : loss : 0.012195, loss_ce: 0.005729
2022-01-08 02:40:00,948 iteration 5856 : loss : 0.021874, loss_ce: 0.008679
2022-01-08 02:40:02,322 iteration 5857 : loss : 0.011451, loss_ce: 0.004345
2022-01-08 02:40:03,710 iteration 5858 : loss : 0.015434, loss_ce: 0.005040
2022-01-08 02:40:05,151 iteration 5859 : loss : 0.014746, loss_ce: 0.004933
2022-01-08 02:40:06,523 iteration 5860 : loss : 0.013582, loss_ce: 0.004714
2022-01-08 02:40:07,861 iteration 5861 : loss : 0.014848, loss_ce: 0.006245
2022-01-08 02:40:09,246 iteration 5862 : loss : 0.013551, loss_ce: 0.005496
2022-01-08 02:40:10,625 iteration 5863 : loss : 0.022070, loss_ce: 0.007752
2022-01-08 02:40:12,080 iteration 5864 : loss : 0.016969, loss_ce: 0.008025
2022-01-08 02:40:12,080 Training Data Eval:
2022-01-08 02:40:18,927   Average segmentation loss on training set: 0.0074
2022-01-08 02:40:18,928 Validation Data Eval:
2022-01-08 02:40:21,308   Average segmentation loss on validation set: 0.0718
2022-01-08 02:40:22,768 iteration 5865 : loss : 0.031408, loss_ce: 0.008537
 86%|█████████████████████████    | 345/400 [2:25:46<24:25, 26.64s/it]2022-01-08 02:40:24,118 iteration 5866 : loss : 0.012145, loss_ce: 0.002934
2022-01-08 02:40:25,441 iteration 5867 : loss : 0.011657, loss_ce: 0.004940
2022-01-08 02:40:26,809 iteration 5868 : loss : 0.013919, loss_ce: 0.004973
2022-01-08 02:40:28,138 iteration 5869 : loss : 0.011837, loss_ce: 0.003388
2022-01-08 02:40:29,605 iteration 5870 : loss : 0.023716, loss_ce: 0.008558
2022-01-08 02:40:30,940 iteration 5871 : loss : 0.014969, loss_ce: 0.005161
2022-01-08 02:40:32,264 iteration 5872 : loss : 0.014260, loss_ce: 0.004090
2022-01-08 02:40:33,634 iteration 5873 : loss : 0.017307, loss_ce: 0.009722
2022-01-08 02:40:35,017 iteration 5874 : loss : 0.015384, loss_ce: 0.005552
2022-01-08 02:40:36,397 iteration 5875 : loss : 0.010631, loss_ce: 0.004803
2022-01-08 02:40:37,844 iteration 5876 : loss : 0.017258, loss_ce: 0.008147
2022-01-08 02:40:39,255 iteration 5877 : loss : 0.020380, loss_ce: 0.009094
2022-01-08 02:40:40,561 iteration 5878 : loss : 0.013384, loss_ce: 0.004090
2022-01-08 02:40:41,919 iteration 5879 : loss : 0.017609, loss_ce: 0.006435
2022-01-08 02:40:43,303 iteration 5880 : loss : 0.015505, loss_ce: 0.003691
2022-01-08 02:40:44,629 iteration 5881 : loss : 0.010776, loss_ce: 0.004410
2022-01-08 02:40:45,932 iteration 5882 : loss : 0.028091, loss_ce: 0.008162
 86%|█████████████████████████    | 346/400 [2:26:09<23:02, 25.60s/it]2022-01-08 02:40:47,292 iteration 5883 : loss : 0.012879, loss_ce: 0.003957
2022-01-08 02:40:48,579 iteration 5884 : loss : 0.009605, loss_ce: 0.003559
2022-01-08 02:40:50,005 iteration 5885 : loss : 0.020945, loss_ce: 0.005373
2022-01-08 02:40:51,363 iteration 5886 : loss : 0.014259, loss_ce: 0.004155
2022-01-08 02:40:52,765 iteration 5887 : loss : 0.013388, loss_ce: 0.005193
2022-01-08 02:40:54,113 iteration 5888 : loss : 0.008830, loss_ce: 0.003549
2022-01-08 02:40:55,529 iteration 5889 : loss : 0.011349, loss_ce: 0.003581
2022-01-08 02:40:56,874 iteration 5890 : loss : 0.013561, loss_ce: 0.005333
2022-01-08 02:40:58,230 iteration 5891 : loss : 0.043526, loss_ce: 0.012170
2022-01-08 02:40:59,608 iteration 5892 : loss : 0.018430, loss_ce: 0.005243
2022-01-08 02:41:00,947 iteration 5893 : loss : 0.013443, loss_ce: 0.005889
2022-01-08 02:41:02,261 iteration 5894 : loss : 0.014080, loss_ce: 0.005697
2022-01-08 02:41:03,569 iteration 5895 : loss : 0.009286, loss_ce: 0.003571
2022-01-08 02:41:04,887 iteration 5896 : loss : 0.012518, loss_ce: 0.004827
2022-01-08 02:41:06,250 iteration 5897 : loss : 0.016251, loss_ce: 0.007754
2022-01-08 02:41:07,572 iteration 5898 : loss : 0.016272, loss_ce: 0.005019
2022-01-08 02:41:08,978 iteration 5899 : loss : 0.015246, loss_ce: 0.006183
 87%|█████████████████████████▏   | 347/400 [2:26:32<21:56, 24.83s/it]2022-01-08 02:41:10,296 iteration 5900 : loss : 0.013483, loss_ce: 0.004780
2022-01-08 02:41:11,746 iteration 5901 : loss : 0.015363, loss_ce: 0.006471
2022-01-08 02:41:13,188 iteration 5902 : loss : 0.018241, loss_ce: 0.008577
2022-01-08 02:41:14,541 iteration 5903 : loss : 0.014251, loss_ce: 0.004629
2022-01-08 02:41:15,919 iteration 5904 : loss : 0.013908, loss_ce: 0.005922
2022-01-08 02:41:17,216 iteration 5905 : loss : 0.013039, loss_ce: 0.004579
2022-01-08 02:41:18,573 iteration 5906 : loss : 0.029207, loss_ce: 0.012148
2022-01-08 02:41:19,952 iteration 5907 : loss : 0.015814, loss_ce: 0.004997
2022-01-08 02:41:21,311 iteration 5908 : loss : 0.018282, loss_ce: 0.006426
2022-01-08 02:41:22,632 iteration 5909 : loss : 0.010425, loss_ce: 0.004167
2022-01-08 02:41:23,975 iteration 5910 : loss : 0.013800, loss_ce: 0.005485
2022-01-08 02:41:25,330 iteration 5911 : loss : 0.012842, loss_ce: 0.004597
2022-01-08 02:41:26,806 iteration 5912 : loss : 0.031777, loss_ce: 0.006033
2022-01-08 02:41:28,088 iteration 5913 : loss : 0.014375, loss_ce: 0.004970
2022-01-08 02:41:29,518 iteration 5914 : loss : 0.015851, loss_ce: 0.006280
2022-01-08 02:41:30,931 iteration 5915 : loss : 0.019858, loss_ce: 0.008171
2022-01-08 02:41:32,294 iteration 5916 : loss : 0.017561, loss_ce: 0.006577
 87%|█████████████████████████▏   | 348/400 [2:26:55<21:07, 24.38s/it]2022-01-08 02:41:33,663 iteration 5917 : loss : 0.022011, loss_ce: 0.004628
2022-01-08 02:41:35,088 iteration 5918 : loss : 0.018850, loss_ce: 0.007498
2022-01-08 02:41:36,400 iteration 5919 : loss : 0.012139, loss_ce: 0.007031
2022-01-08 02:41:37,870 iteration 5920 : loss : 0.031339, loss_ce: 0.011608
2022-01-08 02:41:39,218 iteration 5921 : loss : 0.013064, loss_ce: 0.004017
2022-01-08 02:41:40,614 iteration 5922 : loss : 0.014238, loss_ce: 0.006605
2022-01-08 02:41:41,963 iteration 5923 : loss : 0.017700, loss_ce: 0.005050
2022-01-08 02:41:43,271 iteration 5924 : loss : 0.011108, loss_ce: 0.004494
2022-01-08 02:41:44,614 iteration 5925 : loss : 0.013023, loss_ce: 0.003862
2022-01-08 02:41:45,970 iteration 5926 : loss : 0.010446, loss_ce: 0.003589
2022-01-08 02:41:47,398 iteration 5927 : loss : 0.015947, loss_ce: 0.006171
2022-01-08 02:41:48,678 iteration 5928 : loss : 0.010973, loss_ce: 0.004647
2022-01-08 02:41:49,999 iteration 5929 : loss : 0.014589, loss_ce: 0.003929
2022-01-08 02:41:51,378 iteration 5930 : loss : 0.013239, loss_ce: 0.004440
2022-01-08 02:41:52,753 iteration 5931 : loss : 0.012599, loss_ce: 0.005366
2022-01-08 02:41:54,091 iteration 5932 : loss : 0.019894, loss_ce: 0.008236
2022-01-08 02:41:55,409 iteration 5933 : loss : 0.010813, loss_ce: 0.004693
 87%|█████████████████████████▎   | 349/400 [2:27:18<20:23, 24.00s/it]2022-01-08 02:41:56,739 iteration 5934 : loss : 0.012002, loss_ce: 0.004519
2022-01-08 02:41:58,041 iteration 5935 : loss : 0.014309, loss_ce: 0.006520
2022-01-08 02:41:59,333 iteration 5936 : loss : 0.016318, loss_ce: 0.006995
2022-01-08 02:42:00,723 iteration 5937 : loss : 0.015437, loss_ce: 0.006411
2022-01-08 02:42:02,107 iteration 5938 : loss : 0.015086, loss_ce: 0.006871
2022-01-08 02:42:03,513 iteration 5939 : loss : 0.035272, loss_ce: 0.010623
2022-01-08 02:42:04,822 iteration 5940 : loss : 0.009709, loss_ce: 0.004097
2022-01-08 02:42:06,211 iteration 5941 : loss : 0.014517, loss_ce: 0.005530
2022-01-08 02:42:07,581 iteration 5942 : loss : 0.010832, loss_ce: 0.004187
2022-01-08 02:42:08,986 iteration 5943 : loss : 0.018207, loss_ce: 0.007051
2022-01-08 02:42:10,265 iteration 5944 : loss : 0.014538, loss_ce: 0.005883
2022-01-08 02:42:11,743 iteration 5945 : loss : 0.023187, loss_ce: 0.008045
2022-01-08 02:42:13,104 iteration 5946 : loss : 0.014261, loss_ce: 0.006790
2022-01-08 02:42:14,551 iteration 5947 : loss : 0.021456, loss_ce: 0.006920
2022-01-08 02:42:15,965 iteration 5948 : loss : 0.020651, loss_ce: 0.006693
2022-01-08 02:42:17,318 iteration 5949 : loss : 0.036109, loss_ce: 0.007621
2022-01-08 02:42:17,318 Training Data Eval:
2022-01-08 02:42:24,247   Average segmentation loss on training set: 0.0087
2022-01-08 02:42:24,247 Validation Data Eval:
2022-01-08 02:42:26,638   Average segmentation loss on validation set: 0.0862
2022-01-08 02:42:27,962 iteration 5950 : loss : 0.025467, loss_ce: 0.009807
 88%|█████████████████████████▍   | 350/400 [2:27:51<22:08, 26.57s/it]2022-01-08 02:42:29,296 iteration 5951 : loss : 0.014643, loss_ce: 0.004651
2022-01-08 02:42:30,671 iteration 5952 : loss : 0.015213, loss_ce: 0.007453
2022-01-08 02:42:32,126 iteration 5953 : loss : 0.021913, loss_ce: 0.010042
2022-01-08 02:42:33,444 iteration 5954 : loss : 0.019967, loss_ce: 0.006180
2022-01-08 02:42:34,905 iteration 5955 : loss : 0.024300, loss_ce: 0.010851
2022-01-08 02:42:36,249 iteration 5956 : loss : 0.026561, loss_ce: 0.007267
2022-01-08 02:42:37,660 iteration 5957 : loss : 0.017897, loss_ce: 0.007785
2022-01-08 02:42:39,068 iteration 5958 : loss : 0.021455, loss_ce: 0.009557
2022-01-08 02:42:40,444 iteration 5959 : loss : 0.013965, loss_ce: 0.003275
2022-01-08 02:42:41,773 iteration 5960 : loss : 0.018439, loss_ce: 0.004630
2022-01-08 02:42:43,168 iteration 5961 : loss : 0.023557, loss_ce: 0.013563
2022-01-08 02:42:44,556 iteration 5962 : loss : 0.014807, loss_ce: 0.005578
2022-01-08 02:42:45,886 iteration 5963 : loss : 0.012661, loss_ce: 0.004955
2022-01-08 02:42:47,299 iteration 5964 : loss : 0.029099, loss_ce: 0.010571
2022-01-08 02:42:48,689 iteration 5965 : loss : 0.015021, loss_ce: 0.008656
2022-01-08 02:42:50,020 iteration 5966 : loss : 0.010087, loss_ce: 0.003524
2022-01-08 02:42:51,452 iteration 5967 : loss : 0.015674, loss_ce: 0.005628
 88%|█████████████████████████▍   | 351/400 [2:28:14<20:56, 25.64s/it]2022-01-08 02:42:52,824 iteration 5968 : loss : 0.017710, loss_ce: 0.006749
2022-01-08 02:42:54,154 iteration 5969 : loss : 0.013839, loss_ce: 0.006641
2022-01-08 02:42:55,541 iteration 5970 : loss : 0.015857, loss_ce: 0.006781
2022-01-08 02:42:56,868 iteration 5971 : loss : 0.009948, loss_ce: 0.004348
2022-01-08 02:42:58,185 iteration 5972 : loss : 0.013968, loss_ce: 0.003930
2022-01-08 02:42:59,504 iteration 5973 : loss : 0.013140, loss_ce: 0.005228
2022-01-08 02:43:00,858 iteration 5974 : loss : 0.022470, loss_ce: 0.006681
2022-01-08 02:43:02,235 iteration 5975 : loss : 0.015837, loss_ce: 0.008892
2022-01-08 02:43:03,627 iteration 5976 : loss : 0.015022, loss_ce: 0.004675
2022-01-08 02:43:04,925 iteration 5977 : loss : 0.016023, loss_ce: 0.006006
2022-01-08 02:43:06,270 iteration 5978 : loss : 0.013774, loss_ce: 0.004816
2022-01-08 02:43:07,627 iteration 5979 : loss : 0.017296, loss_ce: 0.004530
2022-01-08 02:43:09,044 iteration 5980 : loss : 0.020504, loss_ce: 0.005109
2022-01-08 02:43:10,468 iteration 5981 : loss : 0.023054, loss_ce: 0.010457
2022-01-08 02:43:11,838 iteration 5982 : loss : 0.020889, loss_ce: 0.009152
2022-01-08 02:43:13,209 iteration 5983 : loss : 0.028720, loss_ce: 0.006828
2022-01-08 02:43:14,662 iteration 5984 : loss : 0.013417, loss_ce: 0.004297
 88%|█████████████████████████▌   | 352/400 [2:28:37<19:55, 24.91s/it]2022-01-08 02:43:16,105 iteration 5985 : loss : 0.018589, loss_ce: 0.006859
2022-01-08 02:43:17,378 iteration 5986 : loss : 0.009553, loss_ce: 0.004389
2022-01-08 02:43:18,795 iteration 5987 : loss : 0.027416, loss_ce: 0.008828
2022-01-08 02:43:20,085 iteration 5988 : loss : 0.011158, loss_ce: 0.004269
2022-01-08 02:43:21,448 iteration 5989 : loss : 0.011854, loss_ce: 0.004316
2022-01-08 02:43:22,781 iteration 5990 : loss : 0.017766, loss_ce: 0.005512
2022-01-08 02:43:24,204 iteration 5991 : loss : 0.014913, loss_ce: 0.004073
2022-01-08 02:43:25,624 iteration 5992 : loss : 0.021885, loss_ce: 0.005661
2022-01-08 02:43:26,976 iteration 5993 : loss : 0.020265, loss_ce: 0.009134
2022-01-08 02:43:28,270 iteration 5994 : loss : 0.010600, loss_ce: 0.004745
2022-01-08 02:43:29,722 iteration 5995 : loss : 0.014391, loss_ce: 0.005037
2022-01-08 02:43:31,117 iteration 5996 : loss : 0.012094, loss_ce: 0.006233
2022-01-08 02:43:32,439 iteration 5997 : loss : 0.012778, loss_ce: 0.004026
2022-01-08 02:43:33,902 iteration 5998 : loss : 0.020418, loss_ce: 0.007984
2022-01-08 02:43:35,240 iteration 5999 : loss : 0.016545, loss_ce: 0.005887
2022-01-08 02:43:36,712 iteration 6000 : loss : 0.021278, loss_ce: 0.007587
2022-01-08 02:43:38,013 iteration 6001 : loss : 0.009305, loss_ce: 0.003561
 88%|█████████████████████████▌   | 353/400 [2:29:01<19:08, 24.44s/it]2022-01-08 02:43:39,352 iteration 6002 : loss : 0.013910, loss_ce: 0.005524
2022-01-08 02:43:40,727 iteration 6003 : loss : 0.011059, loss_ce: 0.005049
2022-01-08 02:43:42,062 iteration 6004 : loss : 0.013987, loss_ce: 0.005433
2022-01-08 02:43:43,408 iteration 6005 : loss : 0.012823, loss_ce: 0.004645
2022-01-08 02:43:44,828 iteration 6006 : loss : 0.016702, loss_ce: 0.006433
2022-01-08 02:43:46,147 iteration 6007 : loss : 0.014011, loss_ce: 0.005420
2022-01-08 02:43:47,497 iteration 6008 : loss : 0.014741, loss_ce: 0.004876
2022-01-08 02:43:48,906 iteration 6009 : loss : 0.012678, loss_ce: 0.003745
2022-01-08 02:43:50,223 iteration 6010 : loss : 0.017076, loss_ce: 0.003950
2022-01-08 02:43:51,621 iteration 6011 : loss : 0.035701, loss_ce: 0.012222
2022-01-08 02:43:53,049 iteration 6012 : loss : 0.018716, loss_ce: 0.008470
2022-01-08 02:43:54,393 iteration 6013 : loss : 0.017691, loss_ce: 0.005940
2022-01-08 02:43:55,714 iteration 6014 : loss : 0.014845, loss_ce: 0.004643
2022-01-08 02:43:56,999 iteration 6015 : loss : 0.012476, loss_ce: 0.006142
2022-01-08 02:43:58,338 iteration 6016 : loss : 0.009790, loss_ce: 0.002987
2022-01-08 02:43:59,701 iteration 6017 : loss : 0.012947, loss_ce: 0.004374
2022-01-08 02:44:00,987 iteration 6018 : loss : 0.013508, loss_ce: 0.005480
 88%|█████████████████████████▋   | 354/400 [2:29:24<18:24, 24.00s/it]2022-01-08 02:44:02,452 iteration 6019 : loss : 0.014927, loss_ce: 0.005422
2022-01-08 02:44:03,834 iteration 6020 : loss : 0.019613, loss_ce: 0.006695
2022-01-08 02:44:05,155 iteration 6021 : loss : 0.009641, loss_ce: 0.003458
2022-01-08 02:44:06,550 iteration 6022 : loss : 0.014391, loss_ce: 0.005697
2022-01-08 02:44:07,959 iteration 6023 : loss : 0.012920, loss_ce: 0.005788
2022-01-08 02:44:09,327 iteration 6024 : loss : 0.011577, loss_ce: 0.004306
2022-01-08 02:44:10,634 iteration 6025 : loss : 0.017666, loss_ce: 0.003409
2022-01-08 02:44:11,963 iteration 6026 : loss : 0.014755, loss_ce: 0.005903
2022-01-08 02:44:13,365 iteration 6027 : loss : 0.012148, loss_ce: 0.005253
2022-01-08 02:44:14,683 iteration 6028 : loss : 0.009950, loss_ce: 0.004630
2022-01-08 02:44:16,039 iteration 6029 : loss : 0.015327, loss_ce: 0.007170
2022-01-08 02:44:17,451 iteration 6030 : loss : 0.019672, loss_ce: 0.009645
2022-01-08 02:44:18,847 iteration 6031 : loss : 0.023063, loss_ce: 0.006864
2022-01-08 02:44:20,203 iteration 6032 : loss : 0.018407, loss_ce: 0.004961
2022-01-08 02:44:21,552 iteration 6033 : loss : 0.014887, loss_ce: 0.005310
2022-01-08 02:44:22,867 iteration 6034 : loss : 0.010015, loss_ce: 0.004081
2022-01-08 02:44:22,867 Training Data Eval:
2022-01-08 02:44:29,791   Average segmentation loss on training set: 0.0075
2022-01-08 02:44:29,792 Validation Data Eval:
2022-01-08 02:44:32,178   Average segmentation loss on validation set: 0.0850
2022-01-08 02:44:33,506 iteration 6035 : loss : 0.012987, loss_ce: 0.005042
 89%|█████████████████████████▋   | 355/400 [2:29:56<19:55, 26.56s/it]2022-01-08 02:44:34,917 iteration 6036 : loss : 0.012028, loss_ce: 0.005410
2022-01-08 02:44:36,224 iteration 6037 : loss : 0.009311, loss_ce: 0.004007
2022-01-08 02:44:37,549 iteration 6038 : loss : 0.010966, loss_ce: 0.004041
2022-01-08 02:44:38,811 iteration 6039 : loss : 0.010199, loss_ce: 0.003300
2022-01-08 02:44:40,142 iteration 6040 : loss : 0.012130, loss_ce: 0.003743
2022-01-08 02:44:41,548 iteration 6041 : loss : 0.014556, loss_ce: 0.005747
2022-01-08 02:44:42,846 iteration 6042 : loss : 0.012518, loss_ce: 0.002913
2022-01-08 02:44:44,123 iteration 6043 : loss : 0.008074, loss_ce: 0.002114
2022-01-08 02:44:45,510 iteration 6044 : loss : 0.013460, loss_ce: 0.003941
2022-01-08 02:44:46,838 iteration 6045 : loss : 0.013033, loss_ce: 0.005905
2022-01-08 02:44:48,196 iteration 6046 : loss : 0.012947, loss_ce: 0.005368
2022-01-08 02:44:49,526 iteration 6047 : loss : 0.010282, loss_ce: 0.003836
2022-01-08 02:44:50,932 iteration 6048 : loss : 0.015312, loss_ce: 0.006462
2022-01-08 02:44:52,279 iteration 6049 : loss : 0.010915, loss_ce: 0.005153
2022-01-08 02:44:53,631 iteration 6050 : loss : 0.010263, loss_ce: 0.003539
2022-01-08 02:44:55,080 iteration 6051 : loss : 0.013107, loss_ce: 0.004260
2022-01-08 02:44:56,395 iteration 6052 : loss : 0.015224, loss_ce: 0.006155
 89%|█████████████████████████▊   | 356/400 [2:30:19<18:40, 25.46s/it]2022-01-08 02:44:57,748 iteration 6053 : loss : 0.014091, loss_ce: 0.005439
2022-01-08 02:44:59,053 iteration 6054 : loss : 0.012935, loss_ce: 0.006155
2022-01-08 02:45:00,457 iteration 6055 : loss : 0.018645, loss_ce: 0.006506
2022-01-08 02:45:01,803 iteration 6056 : loss : 0.011664, loss_ce: 0.004808
2022-01-08 02:45:03,119 iteration 6057 : loss : 0.012823, loss_ce: 0.003917
2022-01-08 02:45:04,496 iteration 6058 : loss : 0.013413, loss_ce: 0.004159
2022-01-08 02:45:05,896 iteration 6059 : loss : 0.013567, loss_ce: 0.006683
2022-01-08 02:45:07,383 iteration 6060 : loss : 0.022457, loss_ce: 0.009052
2022-01-08 02:45:08,734 iteration 6061 : loss : 0.010865, loss_ce: 0.004431
2022-01-08 02:45:10,166 iteration 6062 : loss : 0.021568, loss_ce: 0.007071
2022-01-08 02:45:11,482 iteration 6063 : loss : 0.016135, loss_ce: 0.004581
2022-01-08 02:45:12,905 iteration 6064 : loss : 0.016131, loss_ce: 0.006658
2022-01-08 02:45:14,208 iteration 6065 : loss : 0.008893, loss_ce: 0.003470
2022-01-08 02:45:15,526 iteration 6066 : loss : 0.008021, loss_ce: 0.002294
2022-01-08 02:45:16,872 iteration 6067 : loss : 0.014541, loss_ce: 0.007002
2022-01-08 02:45:18,190 iteration 6068 : loss : 0.009685, loss_ce: 0.002632
2022-01-08 02:45:19,530 iteration 6069 : loss : 0.013255, loss_ce: 0.004976
 89%|█████████████████████████▉   | 357/400 [2:30:42<17:44, 24.76s/it]2022-01-08 02:45:20,918 iteration 6070 : loss : 0.008739, loss_ce: 0.004428
2022-01-08 02:45:22,223 iteration 6071 : loss : 0.010073, loss_ce: 0.003874
2022-01-08 02:45:23,552 iteration 6072 : loss : 0.010854, loss_ce: 0.005687
2022-01-08 02:45:24,914 iteration 6073 : loss : 0.011614, loss_ce: 0.003771
2022-01-08 02:45:26,293 iteration 6074 : loss : 0.013326, loss_ce: 0.005522
2022-01-08 02:45:27,565 iteration 6075 : loss : 0.008844, loss_ce: 0.003853
2022-01-08 02:45:28,899 iteration 6076 : loss : 0.012447, loss_ce: 0.002706
2022-01-08 02:45:30,269 iteration 6077 : loss : 0.014670, loss_ce: 0.005807
2022-01-08 02:45:31,680 iteration 6078 : loss : 0.018925, loss_ce: 0.006817
2022-01-08 02:45:33,146 iteration 6079 : loss : 0.014355, loss_ce: 0.005701
2022-01-08 02:45:34,506 iteration 6080 : loss : 0.013169, loss_ce: 0.004690
2022-01-08 02:45:35,887 iteration 6081 : loss : 0.016201, loss_ce: 0.005785
2022-01-08 02:45:37,168 iteration 6082 : loss : 0.013978, loss_ce: 0.003807
2022-01-08 02:45:38,480 iteration 6083 : loss : 0.013305, loss_ce: 0.003681
2022-01-08 02:45:39,845 iteration 6084 : loss : 0.018347, loss_ce: 0.003948
2022-01-08 02:45:41,199 iteration 6085 : loss : 0.016315, loss_ce: 0.005756
2022-01-08 02:45:42,536 iteration 6086 : loss : 0.009697, loss_ce: 0.003096
 90%|█████████████████████████▉   | 358/400 [2:31:05<16:57, 24.23s/it]2022-01-08 02:45:43,871 iteration 6087 : loss : 0.012543, loss_ce: 0.003854
2022-01-08 02:45:45,267 iteration 6088 : loss : 0.014546, loss_ce: 0.005769
2022-01-08 02:45:46,600 iteration 6089 : loss : 0.012635, loss_ce: 0.003906
2022-01-08 02:45:47,861 iteration 6090 : loss : 0.009934, loss_ce: 0.004321
2022-01-08 02:45:49,235 iteration 6091 : loss : 0.012506, loss_ce: 0.004826
2022-01-08 02:45:50,612 iteration 6092 : loss : 0.012747, loss_ce: 0.003942
2022-01-08 02:45:51,965 iteration 6093 : loss : 0.013334, loss_ce: 0.006580
2022-01-08 02:45:53,276 iteration 6094 : loss : 0.016106, loss_ce: 0.005809
2022-01-08 02:45:54,590 iteration 6095 : loss : 0.016944, loss_ce: 0.004910
2022-01-08 02:45:55,956 iteration 6096 : loss : 0.011965, loss_ce: 0.003358
2022-01-08 02:45:57,335 iteration 6097 : loss : 0.011404, loss_ce: 0.005285
2022-01-08 02:45:58,776 iteration 6098 : loss : 0.017941, loss_ce: 0.007256
2022-01-08 02:46:00,180 iteration 6099 : loss : 0.010525, loss_ce: 0.004411
2022-01-08 02:46:01,568 iteration 6100 : loss : 0.013455, loss_ce: 0.007165
2022-01-08 02:46:02,943 iteration 6101 : loss : 0.014002, loss_ce: 0.005528
2022-01-08 02:46:04,339 iteration 6102 : loss : 0.016877, loss_ce: 0.008951
2022-01-08 02:46:05,755 iteration 6103 : loss : 0.021679, loss_ce: 0.007413
 90%|██████████████████████████   | 359/400 [2:31:29<16:21, 23.93s/it]2022-01-08 02:46:07,208 iteration 6104 : loss : 0.015158, loss_ce: 0.006947
2022-01-08 02:46:08,618 iteration 6105 : loss : 0.012564, loss_ce: 0.006193
2022-01-08 02:46:10,000 iteration 6106 : loss : 0.017518, loss_ce: 0.007424
2022-01-08 02:46:11,360 iteration 6107 : loss : 0.016588, loss_ce: 0.007638
2022-01-08 02:46:12,696 iteration 6108 : loss : 0.011622, loss_ce: 0.004927
2022-01-08 02:46:14,045 iteration 6109 : loss : 0.016453, loss_ce: 0.003882
2022-01-08 02:46:15,375 iteration 6110 : loss : 0.012054, loss_ce: 0.005424
2022-01-08 02:46:16,716 iteration 6111 : loss : 0.011928, loss_ce: 0.005254
2022-01-08 02:46:18,064 iteration 6112 : loss : 0.029248, loss_ce: 0.013561
2022-01-08 02:46:19,421 iteration 6113 : loss : 0.013729, loss_ce: 0.005771
2022-01-08 02:46:20,814 iteration 6114 : loss : 0.019053, loss_ce: 0.005734
2022-01-08 02:46:22,166 iteration 6115 : loss : 0.012909, loss_ce: 0.004530
2022-01-08 02:46:23,533 iteration 6116 : loss : 0.018263, loss_ce: 0.005910
2022-01-08 02:46:24,837 iteration 6117 : loss : 0.014572, loss_ce: 0.006252
2022-01-08 02:46:26,129 iteration 6118 : loss : 0.009673, loss_ce: 0.003142
2022-01-08 02:46:27,570 iteration 6119 : loss : 0.015521, loss_ce: 0.007554
2022-01-08 02:46:27,570 Training Data Eval:
2022-01-08 02:46:34,479   Average segmentation loss on training set: 0.0069
2022-01-08 02:46:34,479 Validation Data Eval:
2022-01-08 02:46:36,854   Average segmentation loss on validation set: 0.0788
2022-01-08 02:46:38,210 iteration 6120 : loss : 0.013475, loss_ce: 0.004480
 90%|██████████████████████████   | 360/400 [2:32:01<17:39, 26.49s/it]2022-01-08 02:46:39,600 iteration 6121 : loss : 0.015117, loss_ce: 0.004325
2022-01-08 02:46:40,957 iteration 6122 : loss : 0.014384, loss_ce: 0.006311
2022-01-08 02:46:42,303 iteration 6123 : loss : 0.014767, loss_ce: 0.005818
2022-01-08 02:46:43,600 iteration 6124 : loss : 0.008672, loss_ce: 0.003003
2022-01-08 02:46:44,955 iteration 6125 : loss : 0.022344, loss_ce: 0.008062
2022-01-08 02:46:46,268 iteration 6126 : loss : 0.010253, loss_ce: 0.005327
2022-01-08 02:46:47,659 iteration 6127 : loss : 0.010583, loss_ce: 0.003673
2022-01-08 02:46:49,000 iteration 6128 : loss : 0.012477, loss_ce: 0.004827
2022-01-08 02:46:50,453 iteration 6129 : loss : 0.013074, loss_ce: 0.005168
2022-01-08 02:46:51,781 iteration 6130 : loss : 0.010680, loss_ce: 0.002809
2022-01-08 02:46:53,218 iteration 6131 : loss : 0.018701, loss_ce: 0.011768
2022-01-08 02:46:54,545 iteration 6132 : loss : 0.013089, loss_ce: 0.006786
2022-01-08 02:46:55,919 iteration 6133 : loss : 0.018458, loss_ce: 0.005382
2022-01-08 02:46:57,276 iteration 6134 : loss : 0.014425, loss_ce: 0.005207
2022-01-08 02:46:58,729 iteration 6135 : loss : 0.020076, loss_ce: 0.006608
2022-01-08 02:47:00,083 iteration 6136 : loss : 0.011873, loss_ce: 0.003536
2022-01-08 02:47:01,407 iteration 6137 : loss : 0.015951, loss_ce: 0.006518
 90%|██████████████████████████▏  | 361/400 [2:32:24<16:34, 25.50s/it]2022-01-08 02:47:02,763 iteration 6138 : loss : 0.017212, loss_ce: 0.005769
2022-01-08 02:47:04,118 iteration 6139 : loss : 0.010952, loss_ce: 0.004343
2022-01-08 02:47:05,450 iteration 6140 : loss : 0.012834, loss_ce: 0.005931
2022-01-08 02:47:06,732 iteration 6141 : loss : 0.008459, loss_ce: 0.003224
2022-01-08 02:47:08,010 iteration 6142 : loss : 0.009143, loss_ce: 0.003978
2022-01-08 02:47:09,425 iteration 6143 : loss : 0.016543, loss_ce: 0.004880
2022-01-08 02:47:10,756 iteration 6144 : loss : 0.013592, loss_ce: 0.005178
2022-01-08 02:47:12,083 iteration 6145 : loss : 0.013121, loss_ce: 0.006121
2022-01-08 02:47:13,386 iteration 6146 : loss : 0.019728, loss_ce: 0.004266
2022-01-08 02:47:14,730 iteration 6147 : loss : 0.010061, loss_ce: 0.004476
2022-01-08 02:47:16,132 iteration 6148 : loss : 0.013910, loss_ce: 0.005193
2022-01-08 02:47:17,529 iteration 6149 : loss : 0.019520, loss_ce: 0.009068
2022-01-08 02:47:18,972 iteration 6150 : loss : 0.016365, loss_ce: 0.005593
2022-01-08 02:47:20,401 iteration 6151 : loss : 0.013573, loss_ce: 0.003179
2022-01-08 02:47:21,708 iteration 6152 : loss : 0.012108, loss_ce: 0.004713
2022-01-08 02:47:23,073 iteration 6153 : loss : 0.013913, loss_ce: 0.004598
2022-01-08 02:47:24,443 iteration 6154 : loss : 0.016523, loss_ce: 0.006850
 90%|██████████████████████████▏  | 362/400 [2:32:47<15:40, 24.76s/it]2022-01-08 02:47:25,882 iteration 6155 : loss : 0.022722, loss_ce: 0.006902
2022-01-08 02:47:27,267 iteration 6156 : loss : 0.013997, loss_ce: 0.005956
2022-01-08 02:47:28,639 iteration 6157 : loss : 0.012466, loss_ce: 0.004313
2022-01-08 02:47:30,072 iteration 6158 : loss : 0.012186, loss_ce: 0.003629
2022-01-08 02:47:31,389 iteration 6159 : loss : 0.010947, loss_ce: 0.003618
2022-01-08 02:47:32,722 iteration 6160 : loss : 0.011751, loss_ce: 0.004564
2022-01-08 02:47:34,110 iteration 6161 : loss : 0.017026, loss_ce: 0.005525
2022-01-08 02:47:35,526 iteration 6162 : loss : 0.019303, loss_ce: 0.005181
2022-01-08 02:47:36,889 iteration 6163 : loss : 0.016137, loss_ce: 0.006357
2022-01-08 02:47:38,179 iteration 6164 : loss : 0.011328, loss_ce: 0.003288
2022-01-08 02:47:39,603 iteration 6165 : loss : 0.015115, loss_ce: 0.006244
2022-01-08 02:47:40,934 iteration 6166 : loss : 0.013344, loss_ce: 0.004208
2022-01-08 02:47:42,354 iteration 6167 : loss : 0.015841, loss_ce: 0.007813
2022-01-08 02:47:43,725 iteration 6168 : loss : 0.016180, loss_ce: 0.005575
2022-01-08 02:47:45,072 iteration 6169 : loss : 0.036262, loss_ce: 0.010376
2022-01-08 02:47:46,370 iteration 6170 : loss : 0.010679, loss_ce: 0.005085
2022-01-08 02:47:47,701 iteration 6171 : loss : 0.015496, loss_ce: 0.007253
 91%|██████████████████████████▎  | 363/400 [2:33:11<14:59, 24.31s/it]2022-01-08 02:47:49,082 iteration 6172 : loss : 0.018320, loss_ce: 0.009794
2022-01-08 02:47:50,519 iteration 6173 : loss : 0.035391, loss_ce: 0.008670
2022-01-08 02:47:51,894 iteration 6174 : loss : 0.012008, loss_ce: 0.004996
2022-01-08 02:47:53,347 iteration 6175 : loss : 0.014484, loss_ce: 0.004248
2022-01-08 02:47:54,707 iteration 6176 : loss : 0.011025, loss_ce: 0.004066
2022-01-08 02:47:56,045 iteration 6177 : loss : 0.009699, loss_ce: 0.004469
2022-01-08 02:47:57,413 iteration 6178 : loss : 0.012736, loss_ce: 0.004311
2022-01-08 02:47:58,740 iteration 6179 : loss : 0.015710, loss_ce: 0.005379
2022-01-08 02:48:00,155 iteration 6180 : loss : 0.017492, loss_ce: 0.007385
2022-01-08 02:48:01,475 iteration 6181 : loss : 0.010002, loss_ce: 0.003021
2022-01-08 02:48:02,806 iteration 6182 : loss : 0.011209, loss_ce: 0.003073
2022-01-08 02:48:04,214 iteration 6183 : loss : 0.013287, loss_ce: 0.004921
2022-01-08 02:48:05,601 iteration 6184 : loss : 0.013298, loss_ce: 0.006071
2022-01-08 02:48:07,025 iteration 6185 : loss : 0.019330, loss_ce: 0.004747
2022-01-08 02:48:08,365 iteration 6186 : loss : 0.010600, loss_ce: 0.003823
2022-01-08 02:48:09,739 iteration 6187 : loss : 0.020368, loss_ce: 0.011199
2022-01-08 02:48:11,182 iteration 6188 : loss : 0.017605, loss_ce: 0.008064
 91%|██████████████████████████▍  | 364/400 [2:33:34<14:26, 24.06s/it]2022-01-08 02:48:12,632 iteration 6189 : loss : 0.012805, loss_ce: 0.002638
2022-01-08 02:48:14,041 iteration 6190 : loss : 0.017687, loss_ce: 0.004839
2022-01-08 02:48:15,362 iteration 6191 : loss : 0.012237, loss_ce: 0.005997
2022-01-08 02:48:16,656 iteration 6192 : loss : 0.009379, loss_ce: 0.003733
2022-01-08 02:48:18,009 iteration 6193 : loss : 0.015198, loss_ce: 0.004523
2022-01-08 02:48:19,309 iteration 6194 : loss : 0.011303, loss_ce: 0.004424
2022-01-08 02:48:20,743 iteration 6195 : loss : 0.011230, loss_ce: 0.003921
2022-01-08 02:48:22,081 iteration 6196 : loss : 0.019635, loss_ce: 0.006024
2022-01-08 02:48:23,391 iteration 6197 : loss : 0.015436, loss_ce: 0.009495
2022-01-08 02:48:24,865 iteration 6198 : loss : 0.030465, loss_ce: 0.008792
2022-01-08 02:48:26,215 iteration 6199 : loss : 0.015357, loss_ce: 0.006494
2022-01-08 02:48:27,537 iteration 6200 : loss : 0.010401, loss_ce: 0.004469
2022-01-08 02:48:28,910 iteration 6201 : loss : 0.014309, loss_ce: 0.005569
2022-01-08 02:48:30,209 iteration 6202 : loss : 0.012876, loss_ce: 0.004233
2022-01-08 02:48:31,707 iteration 6203 : loss : 0.015092, loss_ce: 0.006593
2022-01-08 02:48:33,001 iteration 6204 : loss : 0.009819, loss_ce: 0.004495
2022-01-08 02:48:33,001 Training Data Eval:
2022-01-08 02:48:39,873   Average segmentation loss on training set: 0.0068
2022-01-08 02:48:39,874 Validation Data Eval:
2022-01-08 02:48:42,254   Average segmentation loss on validation set: 0.0802
2022-01-08 02:48:43,678 iteration 6205 : loss : 0.020824, loss_ce: 0.008331
 91%|██████████████████████████▍  | 365/400 [2:34:06<15:30, 26.59s/it]2022-01-08 02:48:45,096 iteration 6206 : loss : 0.017022, loss_ce: 0.007031
2022-01-08 02:48:46,481 iteration 6207 : loss : 0.012645, loss_ce: 0.005116
2022-01-08 02:48:47,885 iteration 6208 : loss : 0.016686, loss_ce: 0.005904
2022-01-08 02:48:49,252 iteration 6209 : loss : 0.014873, loss_ce: 0.005775
2022-01-08 02:48:50,621 iteration 6210 : loss : 0.009307, loss_ce: 0.003363
2022-01-08 02:48:51,957 iteration 6211 : loss : 0.013773, loss_ce: 0.006755
2022-01-08 02:48:53,282 iteration 6212 : loss : 0.009467, loss_ce: 0.003391
2022-01-08 02:48:54,556 iteration 6213 : loss : 0.010726, loss_ce: 0.003651
2022-01-08 02:48:56,007 iteration 6214 : loss : 0.016983, loss_ce: 0.006565
2022-01-08 02:48:57,374 iteration 6215 : loss : 0.012030, loss_ce: 0.003507
2022-01-08 02:48:58,738 iteration 6216 : loss : 0.011774, loss_ce: 0.004083
2022-01-08 02:49:00,121 iteration 6217 : loss : 0.018556, loss_ce: 0.008362
2022-01-08 02:49:01,426 iteration 6218 : loss : 0.013111, loss_ce: 0.005102
2022-01-08 02:49:02,801 iteration 6219 : loss : 0.016125, loss_ce: 0.005666
2022-01-08 02:49:04,087 iteration 6220 : loss : 0.010312, loss_ce: 0.002675
2022-01-08 02:49:05,531 iteration 6221 : loss : 0.017385, loss_ce: 0.004867
2022-01-08 02:49:06,896 iteration 6222 : loss : 0.011786, loss_ce: 0.005231
 92%|██████████████████████████▌  | 366/400 [2:34:30<14:29, 25.58s/it]2022-01-08 02:49:08,300 iteration 6223 : loss : 0.028287, loss_ce: 0.013045
2022-01-08 02:49:09,692 iteration 6224 : loss : 0.013106, loss_ce: 0.006005
2022-01-08 02:49:11,077 iteration 6225 : loss : 0.012366, loss_ce: 0.006015
2022-01-08 02:49:12,429 iteration 6226 : loss : 0.013415, loss_ce: 0.004943
2022-01-08 02:49:13,867 iteration 6227 : loss : 0.010236, loss_ce: 0.003421
2022-01-08 02:49:15,213 iteration 6228 : loss : 0.015600, loss_ce: 0.005869
2022-01-08 02:49:16,610 iteration 6229 : loss : 0.014137, loss_ce: 0.003877
2022-01-08 02:49:18,018 iteration 6230 : loss : 0.033833, loss_ce: 0.007576
2022-01-08 02:49:19,313 iteration 6231 : loss : 0.012926, loss_ce: 0.005955
2022-01-08 02:49:20,645 iteration 6232 : loss : 0.013489, loss_ce: 0.004695
2022-01-08 02:49:22,081 iteration 6233 : loss : 0.031965, loss_ce: 0.011377
2022-01-08 02:49:23,427 iteration 6234 : loss : 0.015125, loss_ce: 0.006006
2022-01-08 02:49:24,784 iteration 6235 : loss : 0.011629, loss_ce: 0.003798
2022-01-08 02:49:26,186 iteration 6236 : loss : 0.023918, loss_ce: 0.010106
2022-01-08 02:49:27,561 iteration 6237 : loss : 0.017670, loss_ce: 0.005886
2022-01-08 02:49:28,974 iteration 6238 : loss : 0.019631, loss_ce: 0.008121
2022-01-08 02:49:30,402 iteration 6239 : loss : 0.017509, loss_ce: 0.007420
 92%|██████████████████████████▌  | 367/400 [2:34:53<13:43, 24.96s/it]2022-01-08 02:49:31,752 iteration 6240 : loss : 0.011622, loss_ce: 0.004002
2022-01-08 02:49:33,051 iteration 6241 : loss : 0.014844, loss_ce: 0.005862
2022-01-08 02:49:34,435 iteration 6242 : loss : 0.019271, loss_ce: 0.009852
2022-01-08 02:49:35,789 iteration 6243 : loss : 0.017493, loss_ce: 0.006799
2022-01-08 02:49:37,160 iteration 6244 : loss : 0.028818, loss_ce: 0.008172
2022-01-08 02:49:38,550 iteration 6245 : loss : 0.010667, loss_ce: 0.004423
2022-01-08 02:49:39,892 iteration 6246 : loss : 0.010727, loss_ce: 0.004312
2022-01-08 02:49:41,216 iteration 6247 : loss : 0.013057, loss_ce: 0.004439
2022-01-08 02:49:42,617 iteration 6248 : loss : 0.027711, loss_ce: 0.006278
2022-01-08 02:49:44,023 iteration 6249 : loss : 0.013651, loss_ce: 0.004469
2022-01-08 02:49:45,292 iteration 6250 : loss : 0.010610, loss_ce: 0.002553
2022-01-08 02:49:46,699 iteration 6251 : loss : 0.018454, loss_ce: 0.011032
2022-01-08 02:49:47,981 iteration 6252 : loss : 0.012054, loss_ce: 0.003799
2022-01-08 02:49:49,428 iteration 6253 : loss : 0.015920, loss_ce: 0.006256
2022-01-08 02:49:50,704 iteration 6254 : loss : 0.007253, loss_ce: 0.003143
2022-01-08 02:49:52,199 iteration 6255 : loss : 0.026625, loss_ce: 0.009566
2022-01-08 02:49:53,521 iteration 6256 : loss : 0.014678, loss_ce: 0.005653
 92%|██████████████████████████▋  | 368/400 [2:35:16<13:00, 24.41s/it]2022-01-08 02:49:54,964 iteration 6257 : loss : 0.017280, loss_ce: 0.007173
2022-01-08 02:49:56,300 iteration 6258 : loss : 0.010025, loss_ce: 0.004897
2022-01-08 02:49:57,672 iteration 6259 : loss : 0.015227, loss_ce: 0.006320
2022-01-08 02:49:59,028 iteration 6260 : loss : 0.016372, loss_ce: 0.004391
2022-01-08 02:50:00,343 iteration 6261 : loss : 0.011545, loss_ce: 0.005188
2022-01-08 02:50:01,727 iteration 6262 : loss : 0.016271, loss_ce: 0.007041
2022-01-08 02:50:03,131 iteration 6263 : loss : 0.016733, loss_ce: 0.004261
2022-01-08 02:50:04,464 iteration 6264 : loss : 0.011769, loss_ce: 0.005395
2022-01-08 02:50:05,790 iteration 6265 : loss : 0.012549, loss_ce: 0.005097
2022-01-08 02:50:07,156 iteration 6266 : loss : 0.010306, loss_ce: 0.004354
2022-01-08 02:50:08,510 iteration 6267 : loss : 0.014390, loss_ce: 0.005440
2022-01-08 02:50:09,854 iteration 6268 : loss : 0.014374, loss_ce: 0.005343
2022-01-08 02:50:11,179 iteration 6269 : loss : 0.015615, loss_ce: 0.004867
2022-01-08 02:50:12,566 iteration 6270 : loss : 0.011828, loss_ce: 0.003272
2022-01-08 02:50:13,926 iteration 6271 : loss : 0.014276, loss_ce: 0.003330
2022-01-08 02:50:15,262 iteration 6272 : loss : 0.008484, loss_ce: 0.002400
2022-01-08 02:50:16,542 iteration 6273 : loss : 0.008458, loss_ce: 0.003419
 92%|██████████████████████████▊  | 369/400 [2:35:39<12:23, 23.99s/it]2022-01-08 02:50:17,920 iteration 6274 : loss : 0.014698, loss_ce: 0.004207
2022-01-08 02:50:19,268 iteration 6275 : loss : 0.014257, loss_ce: 0.006403
2022-01-08 02:50:20,602 iteration 6276 : loss : 0.012837, loss_ce: 0.004508
2022-01-08 02:50:22,005 iteration 6277 : loss : 0.013243, loss_ce: 0.003627
2022-01-08 02:50:23,427 iteration 6278 : loss : 0.015442, loss_ce: 0.006741
2022-01-08 02:50:24,891 iteration 6279 : loss : 0.020464, loss_ce: 0.005948
2022-01-08 02:50:26,259 iteration 6280 : loss : 0.014285, loss_ce: 0.007090
2022-01-08 02:50:27,600 iteration 6281 : loss : 0.011667, loss_ce: 0.003926
2022-01-08 02:50:28,931 iteration 6282 : loss : 0.010012, loss_ce: 0.004058
2022-01-08 02:50:30,351 iteration 6283 : loss : 0.020588, loss_ce: 0.005837
2022-01-08 02:50:31,791 iteration 6284 : loss : 0.013301, loss_ce: 0.006492
2022-01-08 02:50:33,149 iteration 6285 : loss : 0.018394, loss_ce: 0.007852
2022-01-08 02:50:34,505 iteration 6286 : loss : 0.012093, loss_ce: 0.005932
2022-01-08 02:50:35,903 iteration 6287 : loss : 0.011621, loss_ce: 0.003843
2022-01-08 02:50:37,327 iteration 6288 : loss : 0.018537, loss_ce: 0.005628
2022-01-08 02:50:38,672 iteration 6289 : loss : 0.011627, loss_ce: 0.003744
2022-01-08 02:50:38,673 Training Data Eval:
2022-01-08 02:50:45,544   Average segmentation loss on training set: 0.0068
2022-01-08 02:50:45,545 Validation Data Eval:
2022-01-08 02:50:47,917   Average segmentation loss on validation set: 0.0770
2022-01-08 02:50:49,244 iteration 6290 : loss : 0.010192, loss_ce: 0.003565
 92%|██████████████████████████▊  | 370/400 [2:36:12<13:18, 26.61s/it]2022-01-08 02:50:50,648 iteration 6291 : loss : 0.011577, loss_ce: 0.002534
2022-01-08 02:50:52,092 iteration 6292 : loss : 0.012436, loss_ce: 0.004342
2022-01-08 02:50:53,435 iteration 6293 : loss : 0.015082, loss_ce: 0.005516
2022-01-08 02:50:54,831 iteration 6294 : loss : 0.014649, loss_ce: 0.005909
2022-01-08 02:50:56,223 iteration 6295 : loss : 0.017263, loss_ce: 0.006514
2022-01-08 02:50:57,564 iteration 6296 : loss : 0.011328, loss_ce: 0.004194
2022-01-08 02:50:58,902 iteration 6297 : loss : 0.013171, loss_ce: 0.003264
2022-01-08 02:51:00,231 iteration 6298 : loss : 0.010868, loss_ce: 0.003625
2022-01-08 02:51:01,633 iteration 6299 : loss : 0.012728, loss_ce: 0.005263
2022-01-08 02:51:02,979 iteration 6300 : loss : 0.012990, loss_ce: 0.004229
2022-01-08 02:51:04,301 iteration 6301 : loss : 0.018175, loss_ce: 0.007560
2022-01-08 02:51:05,626 iteration 6302 : loss : 0.010757, loss_ce: 0.004463
2022-01-08 02:51:07,004 iteration 6303 : loss : 0.017201, loss_ce: 0.008852
2022-01-08 02:51:08,334 iteration 6304 : loss : 0.011023, loss_ce: 0.004098
2022-01-08 02:51:09,625 iteration 6305 : loss : 0.008828, loss_ce: 0.003792
2022-01-08 02:51:10,933 iteration 6306 : loss : 0.011808, loss_ce: 0.004066
2022-01-08 02:51:12,367 iteration 6307 : loss : 0.017797, loss_ce: 0.005615
 93%|██████████████████████████▉  | 371/400 [2:36:35<12:21, 25.56s/it]2022-01-08 02:51:13,736 iteration 6308 : loss : 0.012736, loss_ce: 0.005777
2022-01-08 02:51:15,115 iteration 6309 : loss : 0.018905, loss_ce: 0.008201
2022-01-08 02:51:16,485 iteration 6310 : loss : 0.013845, loss_ce: 0.005976
2022-01-08 02:51:17,808 iteration 6311 : loss : 0.021589, loss_ce: 0.005842
2022-01-08 02:51:19,195 iteration 6312 : loss : 0.013396, loss_ce: 0.004975
2022-01-08 02:51:20,546 iteration 6313 : loss : 0.008020, loss_ce: 0.001876
2022-01-08 02:51:21,948 iteration 6314 : loss : 0.025779, loss_ce: 0.010437
2022-01-08 02:51:23,350 iteration 6315 : loss : 0.016195, loss_ce: 0.006263
2022-01-08 02:51:24,691 iteration 6316 : loss : 0.011391, loss_ce: 0.005500
2022-01-08 02:51:26,084 iteration 6317 : loss : 0.010915, loss_ce: 0.004170
2022-01-08 02:51:27,512 iteration 6318 : loss : 0.018592, loss_ce: 0.008427
2022-01-08 02:51:28,964 iteration 6319 : loss : 0.020878, loss_ce: 0.007334
2022-01-08 02:51:30,407 iteration 6320 : loss : 0.016755, loss_ce: 0.008959
2022-01-08 02:51:31,807 iteration 6321 : loss : 0.016897, loss_ce: 0.006925
2022-01-08 02:51:33,232 iteration 6322 : loss : 0.015761, loss_ce: 0.006786
2022-01-08 02:51:34,469 iteration 6323 : loss : 0.009159, loss_ce: 0.004180
2022-01-08 02:51:35,848 iteration 6324 : loss : 0.013928, loss_ce: 0.004042
 93%|██████████████████████████▉  | 372/400 [2:36:59<11:38, 24.94s/it]2022-01-08 02:51:37,184 iteration 6325 : loss : 0.013622, loss_ce: 0.004341
2022-01-08 02:51:38,464 iteration 6326 : loss : 0.009164, loss_ce: 0.003904
2022-01-08 02:51:39,744 iteration 6327 : loss : 0.009030, loss_ce: 0.003392
2022-01-08 02:51:41,176 iteration 6328 : loss : 0.017177, loss_ce: 0.008310
2022-01-08 02:51:42,618 iteration 6329 : loss : 0.012745, loss_ce: 0.004734
2022-01-08 02:51:43,982 iteration 6330 : loss : 0.011977, loss_ce: 0.004258
2022-01-08 02:51:45,386 iteration 6331 : loss : 0.011547, loss_ce: 0.005487
2022-01-08 02:51:46,729 iteration 6332 : loss : 0.013231, loss_ce: 0.004770
2022-01-08 02:51:48,081 iteration 6333 : loss : 0.012411, loss_ce: 0.005148
2022-01-08 02:51:49,455 iteration 6334 : loss : 0.009292, loss_ce: 0.003702
2022-01-08 02:51:50,963 iteration 6335 : loss : 0.023185, loss_ce: 0.011166
2022-01-08 02:51:52,347 iteration 6336 : loss : 0.011816, loss_ce: 0.004032
2022-01-08 02:51:53,631 iteration 6337 : loss : 0.013451, loss_ce: 0.005178
2022-01-08 02:51:55,073 iteration 6338 : loss : 0.011503, loss_ce: 0.003665
2022-01-08 02:51:56,401 iteration 6339 : loss : 0.010467, loss_ce: 0.004035
2022-01-08 02:51:57,770 iteration 6340 : loss : 0.011988, loss_ce: 0.005789
2022-01-08 02:51:59,157 iteration 6341 : loss : 0.015652, loss_ce: 0.005150
 93%|███████████████████████████  | 373/400 [2:37:22<11:00, 24.45s/it]2022-01-08 02:52:00,656 iteration 6342 : loss : 0.013742, loss_ce: 0.005511
2022-01-08 02:52:01,992 iteration 6343 : loss : 0.013471, loss_ce: 0.003939
2022-01-08 02:52:03,367 iteration 6344 : loss : 0.013236, loss_ce: 0.006440
2022-01-08 02:52:04,746 iteration 6345 : loss : 0.015722, loss_ce: 0.006825
2022-01-08 02:52:06,067 iteration 6346 : loss : 0.009787, loss_ce: 0.002631
2022-01-08 02:52:07,428 iteration 6347 : loss : 0.014160, loss_ce: 0.005330
2022-01-08 02:52:08,893 iteration 6348 : loss : 0.016105, loss_ce: 0.005332
2022-01-08 02:52:10,212 iteration 6349 : loss : 0.010691, loss_ce: 0.004590
2022-01-08 02:52:11,555 iteration 6350 : loss : 0.011170, loss_ce: 0.002887
2022-01-08 02:52:12,895 iteration 6351 : loss : 0.012272, loss_ce: 0.004368
2022-01-08 02:52:14,317 iteration 6352 : loss : 0.009966, loss_ce: 0.004062
2022-01-08 02:52:15,741 iteration 6353 : loss : 0.012563, loss_ce: 0.004966
2022-01-08 02:52:17,194 iteration 6354 : loss : 0.014656, loss_ce: 0.006826
2022-01-08 02:52:18,507 iteration 6355 : loss : 0.015533, loss_ce: 0.007894
2022-01-08 02:52:19,887 iteration 6356 : loss : 0.015641, loss_ce: 0.008204
2022-01-08 02:52:21,218 iteration 6357 : loss : 0.024026, loss_ce: 0.009267
2022-01-08 02:52:22,643 iteration 6358 : loss : 0.012865, loss_ce: 0.004687
 94%|███████████████████████████  | 374/400 [2:37:45<10:28, 24.16s/it]2022-01-08 02:52:24,042 iteration 6359 : loss : 0.014121, loss_ce: 0.004969
2022-01-08 02:52:25,355 iteration 6360 : loss : 0.014469, loss_ce: 0.004025
2022-01-08 02:52:26,725 iteration 6361 : loss : 0.020977, loss_ce: 0.004336
2022-01-08 02:52:28,119 iteration 6362 : loss : 0.012562, loss_ce: 0.005979
2022-01-08 02:52:29,548 iteration 6363 : loss : 0.020532, loss_ce: 0.009471
2022-01-08 02:52:30,851 iteration 6364 : loss : 0.009642, loss_ce: 0.004192
2022-01-08 02:52:32,154 iteration 6365 : loss : 0.010574, loss_ce: 0.004186
2022-01-08 02:52:33,562 iteration 6366 : loss : 0.015762, loss_ce: 0.007057
2022-01-08 02:52:34,910 iteration 6367 : loss : 0.011158, loss_ce: 0.004988
2022-01-08 02:52:36,245 iteration 6368 : loss : 0.019328, loss_ce: 0.009955
2022-01-08 02:52:37,628 iteration 6369 : loss : 0.010467, loss_ce: 0.005078
2022-01-08 02:52:39,025 iteration 6370 : loss : 0.015185, loss_ce: 0.004786
2022-01-08 02:52:40,422 iteration 6371 : loss : 0.023188, loss_ce: 0.008119
2022-01-08 02:52:41,791 iteration 6372 : loss : 0.013317, loss_ce: 0.006491
2022-01-08 02:52:43,035 iteration 6373 : loss : 0.006940, loss_ce: 0.001868
2022-01-08 02:52:44,359 iteration 6374 : loss : 0.011963, loss_ce: 0.004795
2022-01-08 02:52:44,359 Training Data Eval:
2022-01-08 02:52:51,229   Average segmentation loss on training set: 0.0066
2022-01-08 02:52:51,230 Validation Data Eval:
2022-01-08 02:52:53,580   Average segmentation loss on validation set: 0.0766
2022-01-08 02:52:54,946 iteration 6375 : loss : 0.013509, loss_ce: 0.004829
 94%|███████████████████████████▏ | 375/400 [2:38:18<11:05, 26.60s/it]2022-01-08 02:52:56,574 iteration 6376 : loss : 0.020529, loss_ce: 0.008688
2022-01-08 02:52:57,932 iteration 6377 : loss : 0.014445, loss_ce: 0.005738
2022-01-08 02:52:59,269 iteration 6378 : loss : 0.014165, loss_ce: 0.005138
2022-01-08 02:53:00,674 iteration 6379 : loss : 0.017199, loss_ce: 0.003667
2022-01-08 02:53:01,971 iteration 6380 : loss : 0.009450, loss_ce: 0.004185
2022-01-08 02:53:03,266 iteration 6381 : loss : 0.011927, loss_ce: 0.003910
2022-01-08 02:53:04,640 iteration 6382 : loss : 0.016416, loss_ce: 0.006843
2022-01-08 02:53:06,021 iteration 6383 : loss : 0.011063, loss_ce: 0.004560
2022-01-08 02:53:07,360 iteration 6384 : loss : 0.010969, loss_ce: 0.004458
2022-01-08 02:53:08,800 iteration 6385 : loss : 0.016389, loss_ce: 0.007110
2022-01-08 02:53:10,154 iteration 6386 : loss : 0.012501, loss_ce: 0.004214
2022-01-08 02:53:11,632 iteration 6387 : loss : 0.023198, loss_ce: 0.011510
2022-01-08 02:53:12,971 iteration 6388 : loss : 0.008344, loss_ce: 0.002904
2022-01-08 02:53:14,247 iteration 6389 : loss : 0.012485, loss_ce: 0.003724
2022-01-08 02:53:15,597 iteration 6390 : loss : 0.012526, loss_ce: 0.004693
2022-01-08 02:53:17,055 iteration 6391 : loss : 0.017426, loss_ce: 0.005013
2022-01-08 02:53:18,349 iteration 6392 : loss : 0.008500, loss_ce: 0.003740
 94%|███████████████████████████▎ | 376/400 [2:38:41<10:15, 25.64s/it]2022-01-08 02:53:19,755 iteration 6393 : loss : 0.009873, loss_ce: 0.003133
2022-01-08 02:53:21,141 iteration 6394 : loss : 0.015731, loss_ce: 0.005706
2022-01-08 02:53:22,463 iteration 6395 : loss : 0.008986, loss_ce: 0.004221
2022-01-08 02:53:23,907 iteration 6396 : loss : 0.014665, loss_ce: 0.006333
2022-01-08 02:53:25,233 iteration 6397 : loss : 0.013587, loss_ce: 0.005449
2022-01-08 02:53:26,539 iteration 6398 : loss : 0.012193, loss_ce: 0.004362
2022-01-08 02:53:27,903 iteration 6399 : loss : 0.014689, loss_ce: 0.005110
2022-01-08 02:53:29,267 iteration 6400 : loss : 0.016448, loss_ce: 0.007430
2022-01-08 02:53:30,569 iteration 6401 : loss : 0.010425, loss_ce: 0.003955
2022-01-08 02:53:31,936 iteration 6402 : loss : 0.017353, loss_ce: 0.006339
2022-01-08 02:53:33,413 iteration 6403 : loss : 0.013678, loss_ce: 0.005133
2022-01-08 02:53:34,746 iteration 6404 : loss : 0.014994, loss_ce: 0.004382
2022-01-08 02:53:36,095 iteration 6405 : loss : 0.016015, loss_ce: 0.006151
2022-01-08 02:53:37,606 iteration 6406 : loss : 0.017744, loss_ce: 0.008431
2022-01-08 02:53:38,907 iteration 6407 : loss : 0.015979, loss_ce: 0.004835
2022-01-08 02:53:40,260 iteration 6408 : loss : 0.012800, loss_ce: 0.003205
2022-01-08 02:53:41,587 iteration 6409 : loss : 0.016263, loss_ce: 0.006704
 94%|███████████████████████████▎ | 377/400 [2:39:04<09:33, 24.92s/it]2022-01-08 02:53:43,117 iteration 6410 : loss : 0.019411, loss_ce: 0.007996
2022-01-08 02:53:44,555 iteration 6411 : loss : 0.018341, loss_ce: 0.006986
2022-01-08 02:53:45,941 iteration 6412 : loss : 0.014530, loss_ce: 0.006933
2022-01-08 02:53:47,293 iteration 6413 : loss : 0.009596, loss_ce: 0.003535
2022-01-08 02:53:48,690 iteration 6414 : loss : 0.016030, loss_ce: 0.005792
2022-01-08 02:53:50,035 iteration 6415 : loss : 0.011951, loss_ce: 0.004174
2022-01-08 02:53:51,398 iteration 6416 : loss : 0.020166, loss_ce: 0.005449
2022-01-08 02:53:52,790 iteration 6417 : loss : 0.015872, loss_ce: 0.008527
2022-01-08 02:53:54,181 iteration 6418 : loss : 0.016515, loss_ce: 0.006887
2022-01-08 02:53:55,544 iteration 6419 : loss : 0.011047, loss_ce: 0.003392
2022-01-08 02:53:56,953 iteration 6420 : loss : 0.015820, loss_ce: 0.004571
2022-01-08 02:53:58,313 iteration 6421 : loss : 0.012977, loss_ce: 0.005118
2022-01-08 02:53:59,615 iteration 6422 : loss : 0.013259, loss_ce: 0.005739
2022-01-08 02:54:01,022 iteration 6423 : loss : 0.014073, loss_ce: 0.004397
2022-01-08 02:54:02,437 iteration 6424 : loss : 0.014051, loss_ce: 0.006482
2022-01-08 02:54:03,724 iteration 6425 : loss : 0.008934, loss_ce: 0.002958
2022-01-08 02:54:05,125 iteration 6426 : loss : 0.013859, loss_ce: 0.005676
 94%|███████████████████████████▍ | 378/400 [2:39:28<08:59, 24.51s/it]2022-01-08 02:54:06,535 iteration 6427 : loss : 0.013425, loss_ce: 0.005260
2022-01-08 02:54:07,820 iteration 6428 : loss : 0.009883, loss_ce: 0.004668
2022-01-08 02:54:09,219 iteration 6429 : loss : 0.015063, loss_ce: 0.005950
2022-01-08 02:54:10,518 iteration 6430 : loss : 0.010095, loss_ce: 0.002961
2022-01-08 02:54:11,892 iteration 6431 : loss : 0.015611, loss_ce: 0.006569
2022-01-08 02:54:13,263 iteration 6432 : loss : 0.011266, loss_ce: 0.004387
2022-01-08 02:54:14,689 iteration 6433 : loss : 0.012804, loss_ce: 0.004476
2022-01-08 02:54:15,957 iteration 6434 : loss : 0.008619, loss_ce: 0.002991
2022-01-08 02:54:17,323 iteration 6435 : loss : 0.010841, loss_ce: 0.005088
2022-01-08 02:54:18,672 iteration 6436 : loss : 0.012812, loss_ce: 0.005312
2022-01-08 02:54:20,060 iteration 6437 : loss : 0.015931, loss_ce: 0.003582
2022-01-08 02:54:21,460 iteration 6438 : loss : 0.016301, loss_ce: 0.006627
2022-01-08 02:54:22,850 iteration 6439 : loss : 0.013851, loss_ce: 0.006473
2022-01-08 02:54:24,273 iteration 6440 : loss : 0.018978, loss_ce: 0.007212
2022-01-08 02:54:25,656 iteration 6441 : loss : 0.018692, loss_ce: 0.007546
2022-01-08 02:54:27,015 iteration 6442 : loss : 0.017041, loss_ce: 0.007713
2022-01-08 02:54:28,412 iteration 6443 : loss : 0.013951, loss_ce: 0.005341
 95%|███████████████████████████▍ | 379/400 [2:39:51<08:26, 24.14s/it]2022-01-08 02:54:29,857 iteration 6444 : loss : 0.015635, loss_ce: 0.007632
2022-01-08 02:54:31,287 iteration 6445 : loss : 0.013121, loss_ce: 0.004859
2022-01-08 02:54:32,658 iteration 6446 : loss : 0.011753, loss_ce: 0.003119
2022-01-08 02:54:34,041 iteration 6447 : loss : 0.015356, loss_ce: 0.008195
2022-01-08 02:54:35,427 iteration 6448 : loss : 0.012298, loss_ce: 0.006006
2022-01-08 02:54:36,852 iteration 6449 : loss : 0.013464, loss_ce: 0.004095
2022-01-08 02:54:38,167 iteration 6450 : loss : 0.015448, loss_ce: 0.007233
2022-01-08 02:54:39,466 iteration 6451 : loss : 0.011015, loss_ce: 0.004042
2022-01-08 02:54:40,864 iteration 6452 : loss : 0.012043, loss_ce: 0.005289
2022-01-08 02:54:42,239 iteration 6453 : loss : 0.014780, loss_ce: 0.006137
2022-01-08 02:54:43,577 iteration 6454 : loss : 0.011208, loss_ce: 0.003805
2022-01-08 02:54:44,916 iteration 6455 : loss : 0.013319, loss_ce: 0.004294
2022-01-08 02:54:46,374 iteration 6456 : loss : 0.018179, loss_ce: 0.007097
2022-01-08 02:54:47,742 iteration 6457 : loss : 0.012638, loss_ce: 0.004126
2022-01-08 02:54:49,072 iteration 6458 : loss : 0.010982, loss_ce: 0.003452
2022-01-08 02:54:50,383 iteration 6459 : loss : 0.011931, loss_ce: 0.004019
2022-01-08 02:54:50,384 Training Data Eval:
2022-01-08 02:54:57,284   Average segmentation loss on training set: 0.0063
2022-01-08 02:54:57,284 Validation Data Eval:
2022-01-08 02:54:59,649   Average segmentation loss on validation set: 0.0942
2022-01-08 02:55:01,050 iteration 6460 : loss : 0.027635, loss_ce: 0.009587
 95%|███████████████████████████▌ | 380/400 [2:40:24<08:53, 26.69s/it]2022-01-08 02:55:02,462 iteration 6461 : loss : 0.019415, loss_ce: 0.004907
2022-01-08 02:55:03,841 iteration 6462 : loss : 0.012099, loss_ce: 0.004498
2022-01-08 02:55:05,162 iteration 6463 : loss : 0.008311, loss_ce: 0.002291
2022-01-08 02:55:06,481 iteration 6464 : loss : 0.015777, loss_ce: 0.007182
2022-01-08 02:55:07,837 iteration 6465 : loss : 0.018593, loss_ce: 0.004902
2022-01-08 02:55:09,231 iteration 6466 : loss : 0.011499, loss_ce: 0.004644
2022-01-08 02:55:10,647 iteration 6467 : loss : 0.012084, loss_ce: 0.004309
2022-01-08 02:55:12,083 iteration 6468 : loss : 0.014880, loss_ce: 0.007789
2022-01-08 02:55:13,439 iteration 6469 : loss : 0.010810, loss_ce: 0.005281
2022-01-08 02:55:14,771 iteration 6470 : loss : 0.016301, loss_ce: 0.005021
2022-01-08 02:55:16,109 iteration 6471 : loss : 0.017214, loss_ce: 0.006461
2022-01-08 02:55:17,468 iteration 6472 : loss : 0.013748, loss_ce: 0.003975
2022-01-08 02:55:18,780 iteration 6473 : loss : 0.011265, loss_ce: 0.005575
2022-01-08 02:55:20,205 iteration 6474 : loss : 0.017831, loss_ce: 0.009371
2022-01-08 02:55:21,495 iteration 6475 : loss : 0.009108, loss_ce: 0.003134
2022-01-08 02:55:22,841 iteration 6476 : loss : 0.015140, loss_ce: 0.005797
2022-01-08 02:55:24,145 iteration 6477 : loss : 0.008916, loss_ce: 0.003151
 95%|███████████████████████████▌ | 381/400 [2:40:47<08:06, 25.61s/it]2022-01-08 02:55:25,575 iteration 6478 : loss : 0.010541, loss_ce: 0.004145
2022-01-08 02:55:27,088 iteration 6479 : loss : 0.015900, loss_ce: 0.009003
2022-01-08 02:55:28,476 iteration 6480 : loss : 0.019488, loss_ce: 0.004763
2022-01-08 02:55:29,862 iteration 6481 : loss : 0.014402, loss_ce: 0.005908
2022-01-08 02:55:31,245 iteration 6482 : loss : 0.014088, loss_ce: 0.005864
2022-01-08 02:55:32,588 iteration 6483 : loss : 0.019577, loss_ce: 0.006516
2022-01-08 02:55:33,938 iteration 6484 : loss : 0.012696, loss_ce: 0.003824
2022-01-08 02:55:35,330 iteration 6485 : loss : 0.014149, loss_ce: 0.006428
2022-01-08 02:55:36,785 iteration 6486 : loss : 0.013598, loss_ce: 0.003982
2022-01-08 02:55:38,168 iteration 6487 : loss : 0.010800, loss_ce: 0.003218
2022-01-08 02:55:39,440 iteration 6488 : loss : 0.007980, loss_ce: 0.003343
2022-01-08 02:55:40,861 iteration 6489 : loss : 0.014749, loss_ce: 0.004843
2022-01-08 02:55:42,135 iteration 6490 : loss : 0.011071, loss_ce: 0.004164
2022-01-08 02:55:43,540 iteration 6491 : loss : 0.019520, loss_ce: 0.005518
2022-01-08 02:55:44,890 iteration 6492 : loss : 0.010314, loss_ce: 0.003716
2022-01-08 02:55:46,294 iteration 6493 : loss : 0.013493, loss_ce: 0.003914
2022-01-08 02:55:47,659 iteration 6494 : loss : 0.017681, loss_ce: 0.006131
 96%|███████████████████████████▋ | 382/400 [2:41:10<07:29, 24.98s/it]2022-01-08 02:55:49,100 iteration 6495 : loss : 0.009196, loss_ce: 0.002654
2022-01-08 02:55:50,441 iteration 6496 : loss : 0.014916, loss_ce: 0.005156
2022-01-08 02:55:51,835 iteration 6497 : loss : 0.008037, loss_ce: 0.002863
2022-01-08 02:55:53,216 iteration 6498 : loss : 0.021350, loss_ce: 0.010066
2022-01-08 02:55:54,680 iteration 6499 : loss : 0.017120, loss_ce: 0.005669
2022-01-08 02:55:56,074 iteration 6500 : loss : 0.015325, loss_ce: 0.006704
2022-01-08 02:55:57,488 iteration 6501 : loss : 0.010589, loss_ce: 0.002612
2022-01-08 02:55:58,894 iteration 6502 : loss : 0.014721, loss_ce: 0.005282
2022-01-08 02:56:00,351 iteration 6503 : loss : 0.017522, loss_ce: 0.006137
2022-01-08 02:56:01,667 iteration 6504 : loss : 0.011535, loss_ce: 0.006290
2022-01-08 02:56:03,012 iteration 6505 : loss : 0.013080, loss_ce: 0.003408
2022-01-08 02:56:04,372 iteration 6506 : loss : 0.010160, loss_ce: 0.002939
2022-01-08 02:56:05,728 iteration 6507 : loss : 0.011542, loss_ce: 0.004388
2022-01-08 02:56:07,102 iteration 6508 : loss : 0.010957, loss_ce: 0.004140
2022-01-08 02:56:08,469 iteration 6509 : loss : 0.010705, loss_ce: 0.004338
2022-01-08 02:56:09,801 iteration 6510 : loss : 0.012144, loss_ce: 0.003667
2022-01-08 02:56:11,186 iteration 6511 : loss : 0.011495, loss_ce: 0.004772
 96%|███████████████████████████▊ | 383/400 [2:41:34<06:57, 24.55s/it]2022-01-08 02:56:12,610 iteration 6512 : loss : 0.011059, loss_ce: 0.004430
2022-01-08 02:56:13,915 iteration 6513 : loss : 0.016226, loss_ce: 0.006008
2022-01-08 02:56:15,268 iteration 6514 : loss : 0.010112, loss_ce: 0.002381
2022-01-08 02:56:16,631 iteration 6515 : loss : 0.011852, loss_ce: 0.003535
2022-01-08 02:56:17,983 iteration 6516 : loss : 0.011186, loss_ce: 0.003198
2022-01-08 02:56:19,411 iteration 6517 : loss : 0.014791, loss_ce: 0.006997
2022-01-08 02:56:20,709 iteration 6518 : loss : 0.013558, loss_ce: 0.007202
2022-01-08 02:56:22,147 iteration 6519 : loss : 0.015005, loss_ce: 0.005245
2022-01-08 02:56:23,447 iteration 6520 : loss : 0.010216, loss_ce: 0.004475
2022-01-08 02:56:24,865 iteration 6521 : loss : 0.010360, loss_ce: 0.004217
2022-01-08 02:56:26,218 iteration 6522 : loss : 0.011740, loss_ce: 0.004769
2022-01-08 02:56:27,531 iteration 6523 : loss : 0.011225, loss_ce: 0.003440
2022-01-08 02:56:28,906 iteration 6524 : loss : 0.014674, loss_ce: 0.004587
2022-01-08 02:56:30,257 iteration 6525 : loss : 0.009439, loss_ce: 0.003106
2022-01-08 02:56:31,649 iteration 6526 : loss : 0.011381, loss_ce: 0.004651
2022-01-08 02:56:33,076 iteration 6527 : loss : 0.010521, loss_ce: 0.003617
2022-01-08 02:56:34,473 iteration 6528 : loss : 0.011113, loss_ce: 0.004206
 96%|███████████████████████████▊ | 384/400 [2:41:57<06:26, 24.17s/it]2022-01-08 02:56:35,836 iteration 6529 : loss : 0.009678, loss_ce: 0.003529
2022-01-08 02:56:37,122 iteration 6530 : loss : 0.012215, loss_ce: 0.006962
2022-01-08 02:56:38,490 iteration 6531 : loss : 0.009318, loss_ce: 0.002413
2022-01-08 02:56:39,905 iteration 6532 : loss : 0.008623, loss_ce: 0.002760
2022-01-08 02:56:41,239 iteration 6533 : loss : 0.009100, loss_ce: 0.003774
2022-01-08 02:56:42,650 iteration 6534 : loss : 0.018999, loss_ce: 0.004806
2022-01-08 02:56:44,013 iteration 6535 : loss : 0.016875, loss_ce: 0.007367
2022-01-08 02:56:45,408 iteration 6536 : loss : 0.017387, loss_ce: 0.006151
2022-01-08 02:56:46,878 iteration 6537 : loss : 0.024088, loss_ce: 0.006640
2022-01-08 02:56:48,203 iteration 6538 : loss : 0.011543, loss_ce: 0.003776
2022-01-08 02:56:49,694 iteration 6539 : loss : 0.016444, loss_ce: 0.006218
2022-01-08 02:56:51,012 iteration 6540 : loss : 0.011151, loss_ce: 0.005002
2022-01-08 02:56:52,303 iteration 6541 : loss : 0.009877, loss_ce: 0.004318
2022-01-08 02:56:53,648 iteration 6542 : loss : 0.013875, loss_ce: 0.005541
2022-01-08 02:56:55,089 iteration 6543 : loss : 0.016816, loss_ce: 0.006505
2022-01-08 02:56:56,449 iteration 6544 : loss : 0.019357, loss_ce: 0.008870
2022-01-08 02:56:56,450 Training Data Eval:
2022-01-08 02:57:03,311   Average segmentation loss on training set: 0.0062
2022-01-08 02:57:03,312 Validation Data Eval:
2022-01-08 02:57:05,676   Average segmentation loss on validation set: 0.0840
2022-01-08 02:57:07,057 iteration 6545 : loss : 0.011505, loss_ce: 0.003548
 96%|███████████████████████████▉ | 385/400 [2:42:30<06:40, 26.69s/it]2022-01-08 02:57:08,534 iteration 6546 : loss : 0.012702, loss_ce: 0.004417
2022-01-08 02:57:09,844 iteration 6547 : loss : 0.011593, loss_ce: 0.002363
2022-01-08 02:57:11,222 iteration 6548 : loss : 0.018968, loss_ce: 0.006174
2022-01-08 02:57:12,676 iteration 6549 : loss : 0.018551, loss_ce: 0.005155
2022-01-08 02:57:14,038 iteration 6550 : loss : 0.014850, loss_ce: 0.007234
2022-01-08 02:57:15,462 iteration 6551 : loss : 0.022555, loss_ce: 0.011474
2022-01-08 02:57:16,909 iteration 6552 : loss : 0.015141, loss_ce: 0.005799
2022-01-08 02:57:18,258 iteration 6553 : loss : 0.012570, loss_ce: 0.005080
2022-01-08 02:57:19,759 iteration 6554 : loss : 0.022189, loss_ce: 0.008102
2022-01-08 02:57:21,089 iteration 6555 : loss : 0.014396, loss_ce: 0.005163
2022-01-08 02:57:22,470 iteration 6556 : loss : 0.015882, loss_ce: 0.005948
2022-01-08 02:57:23,763 iteration 6557 : loss : 0.009860, loss_ce: 0.003846
2022-01-08 02:57:25,192 iteration 6558 : loss : 0.016207, loss_ce: 0.006478
2022-01-08 02:57:26,506 iteration 6559 : loss : 0.008715, loss_ce: 0.003076
2022-01-08 02:57:27,831 iteration 6560 : loss : 0.009361, loss_ce: 0.004267
2022-01-08 02:57:29,210 iteration 6561 : loss : 0.010463, loss_ce: 0.003477
2022-01-08 02:57:30,582 iteration 6562 : loss : 0.012258, loss_ce: 0.005064
 96%|███████████████████████████▉ | 386/400 [2:42:53<06:00, 25.74s/it]2022-01-08 02:57:32,007 iteration 6563 : loss : 0.014616, loss_ce: 0.006276
2022-01-08 02:57:33,433 iteration 6564 : loss : 0.016983, loss_ce: 0.005103
2022-01-08 02:57:34,744 iteration 6565 : loss : 0.012435, loss_ce: 0.003936
2022-01-08 02:57:36,181 iteration 6566 : loss : 0.017841, loss_ce: 0.006686
2022-01-08 02:57:37,561 iteration 6567 : loss : 0.025797, loss_ce: 0.007411
2022-01-08 02:57:38,914 iteration 6568 : loss : 0.009527, loss_ce: 0.003361
2022-01-08 02:57:40,253 iteration 6569 : loss : 0.011387, loss_ce: 0.003638
2022-01-08 02:57:41,726 iteration 6570 : loss : 0.013197, loss_ce: 0.005279
2022-01-08 02:57:43,024 iteration 6571 : loss : 0.010281, loss_ce: 0.004455
2022-01-08 02:57:44,356 iteration 6572 : loss : 0.011587, loss_ce: 0.003982
2022-01-08 02:57:45,667 iteration 6573 : loss : 0.008325, loss_ce: 0.002516
2022-01-08 02:57:47,029 iteration 6574 : loss : 0.011520, loss_ce: 0.004394
2022-01-08 02:57:48,305 iteration 6575 : loss : 0.008234, loss_ce: 0.003486
2022-01-08 02:57:49,687 iteration 6576 : loss : 0.015396, loss_ce: 0.006651
2022-01-08 02:57:51,094 iteration 6577 : loss : 0.012940, loss_ce: 0.004496
2022-01-08 02:57:52,484 iteration 6578 : loss : 0.011136, loss_ce: 0.004780
2022-01-08 02:57:53,836 iteration 6579 : loss : 0.015702, loss_ce: 0.003880
 97%|████████████████████████████ | 387/400 [2:43:17<05:24, 25.00s/it]2022-01-08 02:57:55,288 iteration 6580 : loss : 0.020711, loss_ce: 0.007349
2022-01-08 02:57:56,690 iteration 6581 : loss : 0.021183, loss_ce: 0.006298
2022-01-08 02:57:58,097 iteration 6582 : loss : 0.030666, loss_ce: 0.016386
2022-01-08 02:57:59,473 iteration 6583 : loss : 0.009444, loss_ce: 0.003789
2022-01-08 02:58:00,860 iteration 6584 : loss : 0.021731, loss_ce: 0.009448
2022-01-08 02:58:02,185 iteration 6585 : loss : 0.012322, loss_ce: 0.004091
2022-01-08 02:58:03,569 iteration 6586 : loss : 0.011224, loss_ce: 0.004446
2022-01-08 02:58:04,870 iteration 6587 : loss : 0.010863, loss_ce: 0.004974
2022-01-08 02:58:06,290 iteration 6588 : loss : 0.019233, loss_ce: 0.007234
2022-01-08 02:58:07,656 iteration 6589 : loss : 0.013598, loss_ce: 0.004259
2022-01-08 02:58:08,992 iteration 6590 : loss : 0.012631, loss_ce: 0.005927
2022-01-08 02:58:10,316 iteration 6591 : loss : 0.011853, loss_ce: 0.005100
2022-01-08 02:58:11,686 iteration 6592 : loss : 0.011008, loss_ce: 0.003507
2022-01-08 02:58:13,108 iteration 6593 : loss : 0.018216, loss_ce: 0.008393
2022-01-08 02:58:14,452 iteration 6594 : loss : 0.010623, loss_ce: 0.003856
2022-01-08 02:58:15,751 iteration 6595 : loss : 0.010456, loss_ce: 0.003477
2022-01-08 02:58:17,072 iteration 6596 : loss : 0.006827, loss_ce: 0.002637
 97%|████████████████████████████▏| 388/400 [2:43:40<04:53, 24.47s/it]2022-01-08 02:58:18,534 iteration 6597 : loss : 0.014023, loss_ce: 0.007708
2022-01-08 02:58:19,953 iteration 6598 : loss : 0.017939, loss_ce: 0.005042
2022-01-08 02:58:21,259 iteration 6599 : loss : 0.011495, loss_ce: 0.004292
2022-01-08 02:58:22,668 iteration 6600 : loss : 0.016156, loss_ce: 0.006439
2022-01-08 02:58:23,983 iteration 6601 : loss : 0.010840, loss_ce: 0.004452
2022-01-08 02:58:25,260 iteration 6602 : loss : 0.007075, loss_ce: 0.002414
2022-01-08 02:58:26,660 iteration 6603 : loss : 0.024035, loss_ce: 0.007415
2022-01-08 02:58:28,100 iteration 6604 : loss : 0.014591, loss_ce: 0.005599
2022-01-08 02:58:29,484 iteration 6605 : loss : 0.020099, loss_ce: 0.008896
2022-01-08 02:58:30,864 iteration 6606 : loss : 0.012033, loss_ce: 0.006383
2022-01-08 02:58:32,196 iteration 6607 : loss : 0.019100, loss_ce: 0.007947
2022-01-08 02:58:33,587 iteration 6608 : loss : 0.012659, loss_ce: 0.004163
2022-01-08 02:58:35,017 iteration 6609 : loss : 0.010898, loss_ce: 0.004460
2022-01-08 02:58:36,373 iteration 6610 : loss : 0.017889, loss_ce: 0.005103
2022-01-08 02:58:37,655 iteration 6611 : loss : 0.010546, loss_ce: 0.004991
2022-01-08 02:58:39,035 iteration 6612 : loss : 0.022117, loss_ce: 0.007904
2022-01-08 02:58:40,456 iteration 6613 : loss : 0.017711, loss_ce: 0.004406
 97%|████████████████████████████▏| 389/400 [2:44:03<04:25, 24.14s/it]2022-01-08 02:58:41,759 iteration 6614 : loss : 0.007850, loss_ce: 0.003097
2022-01-08 02:58:43,135 iteration 6615 : loss : 0.012945, loss_ce: 0.005078
2022-01-08 02:58:44,525 iteration 6616 : loss : 0.011243, loss_ce: 0.004503
2022-01-08 02:58:45,849 iteration 6617 : loss : 0.008634, loss_ce: 0.001606
2022-01-08 02:58:47,224 iteration 6618 : loss : 0.013837, loss_ce: 0.004053
2022-01-08 02:58:48,629 iteration 6619 : loss : 0.013438, loss_ce: 0.005754
2022-01-08 02:58:49,994 iteration 6620 : loss : 0.014499, loss_ce: 0.004440
2022-01-08 02:58:51,301 iteration 6621 : loss : 0.010641, loss_ce: 0.004318
2022-01-08 02:58:52,573 iteration 6622 : loss : 0.008902, loss_ce: 0.002937
2022-01-08 02:58:53,944 iteration 6623 : loss : 0.013063, loss_ce: 0.006650
2022-01-08 02:58:55,193 iteration 6624 : loss : 0.010257, loss_ce: 0.003741
2022-01-08 02:58:56,453 iteration 6625 : loss : 0.007886, loss_ce: 0.003228
2022-01-08 02:58:57,819 iteration 6626 : loss : 0.018804, loss_ce: 0.006766
2022-01-08 02:58:59,129 iteration 6627 : loss : 0.008253, loss_ce: 0.002779
2022-01-08 02:59:00,531 iteration 6628 : loss : 0.014853, loss_ce: 0.005872
2022-01-08 02:59:01,856 iteration 6629 : loss : 0.013743, loss_ce: 0.006597
2022-01-08 02:59:01,856 Training Data Eval:
2022-01-08 02:59:08,719   Average segmentation loss on training set: 0.0061
2022-01-08 02:59:08,720 Validation Data Eval:
2022-01-08 02:59:11,083   Average segmentation loss on validation set: 0.0785
2022-01-08 02:59:12,459 iteration 6630 : loss : 0.038187, loss_ce: 0.019256
 98%|████████████████████████████▎| 390/400 [2:44:35<04:24, 26.50s/it]2022-01-08 02:59:13,764 iteration 6631 : loss : 0.009044, loss_ce: 0.003409
2022-01-08 02:59:15,149 iteration 6632 : loss : 0.014045, loss_ce: 0.002817
2022-01-08 02:59:16,490 iteration 6633 : loss : 0.009912, loss_ce: 0.004108
2022-01-08 02:59:17,806 iteration 6634 : loss : 0.011017, loss_ce: 0.004659
2022-01-08 02:59:19,200 iteration 6635 : loss : 0.012115, loss_ce: 0.004895
2022-01-08 02:59:20,516 iteration 6636 : loss : 0.010492, loss_ce: 0.005496
2022-01-08 02:59:21,893 iteration 6637 : loss : 0.012998, loss_ce: 0.004493
2022-01-08 02:59:23,275 iteration 6638 : loss : 0.008468, loss_ce: 0.002478
2022-01-08 02:59:24,652 iteration 6639 : loss : 0.014173, loss_ce: 0.005102
2022-01-08 02:59:26,060 iteration 6640 : loss : 0.016963, loss_ce: 0.007458
2022-01-08 02:59:27,412 iteration 6641 : loss : 0.012617, loss_ce: 0.005114
2022-01-08 02:59:28,770 iteration 6642 : loss : 0.011021, loss_ce: 0.003461
2022-01-08 02:59:30,191 iteration 6643 : loss : 0.013411, loss_ce: 0.005820
2022-01-08 02:59:31,625 iteration 6644 : loss : 0.016065, loss_ce: 0.007575
2022-01-08 02:59:32,965 iteration 6645 : loss : 0.013699, loss_ce: 0.005552
2022-01-08 02:59:34,326 iteration 6646 : loss : 0.009737, loss_ce: 0.004616
2022-01-08 02:59:35,693 iteration 6647 : loss : 0.025040, loss_ce: 0.005741
 98%|████████████████████████████▎| 391/400 [2:44:59<03:49, 25.52s/it]2022-01-08 02:59:37,117 iteration 6648 : loss : 0.012333, loss_ce: 0.004549
2022-01-08 02:59:38,507 iteration 6649 : loss : 0.013851, loss_ce: 0.003952
2022-01-08 02:59:39,816 iteration 6650 : loss : 0.011771, loss_ce: 0.004663
2022-01-08 02:59:41,160 iteration 6651 : loss : 0.010965, loss_ce: 0.004720
2022-01-08 02:59:42,557 iteration 6652 : loss : 0.010502, loss_ce: 0.004590
2022-01-08 02:59:43,976 iteration 6653 : loss : 0.011074, loss_ce: 0.003915
2022-01-08 02:59:45,458 iteration 6654 : loss : 0.014906, loss_ce: 0.004581
2022-01-08 02:59:46,792 iteration 6655 : loss : 0.012286, loss_ce: 0.006086
2022-01-08 02:59:48,219 iteration 6656 : loss : 0.011760, loss_ce: 0.004536
2022-01-08 02:59:49,581 iteration 6657 : loss : 0.016859, loss_ce: 0.005052
2022-01-08 02:59:51,004 iteration 6658 : loss : 0.016288, loss_ce: 0.005928
2022-01-08 02:59:52,375 iteration 6659 : loss : 0.016776, loss_ce: 0.005069
2022-01-08 02:59:53,732 iteration 6660 : loss : 0.010642, loss_ce: 0.004244
2022-01-08 02:59:55,058 iteration 6661 : loss : 0.010702, loss_ce: 0.004228
2022-01-08 02:59:56,464 iteration 6662 : loss : 0.016382, loss_ce: 0.006204
2022-01-08 02:59:57,801 iteration 6663 : loss : 0.012847, loss_ce: 0.005124
2022-01-08 02:59:59,193 iteration 6664 : loss : 0.012504, loss_ce: 0.005684
 98%|████████████████████████████▍| 392/400 [2:45:22<03:19, 24.92s/it]2022-01-08 03:00:00,602 iteration 6665 : loss : 0.016538, loss_ce: 0.005809
2022-01-08 03:00:02,001 iteration 6666 : loss : 0.009222, loss_ce: 0.002809
2022-01-08 03:00:03,367 iteration 6667 : loss : 0.013851, loss_ce: 0.005465
2022-01-08 03:00:04,883 iteration 6668 : loss : 0.017450, loss_ce: 0.008327
2022-01-08 03:00:06,193 iteration 6669 : loss : 0.012062, loss_ce: 0.004473
2022-01-08 03:00:07,516 iteration 6670 : loss : 0.009391, loss_ce: 0.003960
2022-01-08 03:00:08,856 iteration 6671 : loss : 0.013368, loss_ce: 0.005880
2022-01-08 03:00:10,148 iteration 6672 : loss : 0.011330, loss_ce: 0.002751
2022-01-08 03:00:11,560 iteration 6673 : loss : 0.016488, loss_ce: 0.006116
2022-01-08 03:00:12,918 iteration 6674 : loss : 0.010480, loss_ce: 0.004409
2022-01-08 03:00:14,340 iteration 6675 : loss : 0.013760, loss_ce: 0.004873
2022-01-08 03:00:15,651 iteration 6676 : loss : 0.009270, loss_ce: 0.002767
2022-01-08 03:00:16,982 iteration 6677 : loss : 0.012617, loss_ce: 0.003520
2022-01-08 03:00:18,237 iteration 6678 : loss : 0.007809, loss_ce: 0.003319
2022-01-08 03:00:19,715 iteration 6679 : loss : 0.011843, loss_ce: 0.005475
2022-01-08 03:00:21,070 iteration 6680 : loss : 0.018820, loss_ce: 0.006275
2022-01-08 03:00:22,467 iteration 6681 : loss : 0.012478, loss_ce: 0.005444
 98%|████████████████████████████▍| 393/400 [2:45:45<02:50, 24.42s/it]2022-01-08 03:00:23,885 iteration 6682 : loss : 0.009614, loss_ce: 0.004460
2022-01-08 03:00:25,344 iteration 6683 : loss : 0.012603, loss_ce: 0.003947
2022-01-08 03:00:26,667 iteration 6684 : loss : 0.010586, loss_ce: 0.003409
2022-01-08 03:00:28,086 iteration 6685 : loss : 0.017690, loss_ce: 0.007887
2022-01-08 03:00:29,390 iteration 6686 : loss : 0.010049, loss_ce: 0.004447
2022-01-08 03:00:30,729 iteration 6687 : loss : 0.012591, loss_ce: 0.002996
2022-01-08 03:00:32,131 iteration 6688 : loss : 0.015568, loss_ce: 0.006280
2022-01-08 03:00:33,466 iteration 6689 : loss : 0.008895, loss_ce: 0.003061
2022-01-08 03:00:34,799 iteration 6690 : loss : 0.012798, loss_ce: 0.005071
2022-01-08 03:00:36,219 iteration 6691 : loss : 0.018621, loss_ce: 0.007210
2022-01-08 03:00:37,606 iteration 6692 : loss : 0.017497, loss_ce: 0.005535
2022-01-08 03:00:39,028 iteration 6693 : loss : 0.011832, loss_ce: 0.004006
2022-01-08 03:00:40,423 iteration 6694 : loss : 0.013761, loss_ce: 0.005456
2022-01-08 03:00:41,833 iteration 6695 : loss : 0.011545, loss_ce: 0.004715
2022-01-08 03:00:43,143 iteration 6696 : loss : 0.016330, loss_ce: 0.010532
2022-01-08 03:00:44,569 iteration 6697 : loss : 0.015639, loss_ce: 0.005743
2022-01-08 03:00:45,935 iteration 6698 : loss : 0.012524, loss_ce: 0.004261
 98%|████████████████████████████▌| 394/400 [2:46:09<02:24, 24.14s/it]2022-01-08 03:00:47,291 iteration 6699 : loss : 0.010188, loss_ce: 0.005281
2022-01-08 03:00:48,717 iteration 6700 : loss : 0.017539, loss_ce: 0.008588
2022-01-08 03:00:50,067 iteration 6701 : loss : 0.010745, loss_ce: 0.004791
2022-01-08 03:00:51,394 iteration 6702 : loss : 0.017499, loss_ce: 0.005587
2022-01-08 03:00:52,751 iteration 6703 : loss : 0.012163, loss_ce: 0.003822
2022-01-08 03:00:54,160 iteration 6704 : loss : 0.014209, loss_ce: 0.004826
2022-01-08 03:00:55,482 iteration 6705 : loss : 0.016370, loss_ce: 0.005837
2022-01-08 03:00:56,805 iteration 6706 : loss : 0.008335, loss_ce: 0.003740
2022-01-08 03:00:58,123 iteration 6707 : loss : 0.012894, loss_ce: 0.003546
2022-01-08 03:00:59,400 iteration 6708 : loss : 0.009491, loss_ce: 0.003288
2022-01-08 03:01:00,725 iteration 6709 : loss : 0.012483, loss_ce: 0.003795
2022-01-08 03:01:02,065 iteration 6710 : loss : 0.009714, loss_ce: 0.003542
2022-01-08 03:01:03,398 iteration 6711 : loss : 0.011462, loss_ce: 0.002159
2022-01-08 03:01:04,777 iteration 6712 : loss : 0.021141, loss_ce: 0.008752
2022-01-08 03:01:06,215 iteration 6713 : loss : 0.014333, loss_ce: 0.004855
2022-01-08 03:01:07,592 iteration 6714 : loss : 0.013204, loss_ce: 0.005723
2022-01-08 03:01:07,593 Training Data Eval:
2022-01-08 03:01:14,456   Average segmentation loss on training set: 0.0059
2022-01-08 03:01:14,457 Validation Data Eval:
2022-01-08 03:01:16,826   Average segmentation loss on validation set: 0.0815
2022-01-08 03:01:18,239 iteration 6715 : loss : 0.015096, loss_ce: 0.004956
 99%|████████████████████████████▋| 395/400 [2:46:41<02:12, 26.59s/it]2022-01-08 03:01:19,655 iteration 6716 : loss : 0.006961, loss_ce: 0.002428
2022-01-08 03:01:21,013 iteration 6717 : loss : 0.014963, loss_ce: 0.005449
2022-01-08 03:01:22,318 iteration 6718 : loss : 0.009297, loss_ce: 0.003077
2022-01-08 03:01:23,665 iteration 6719 : loss : 0.009469, loss_ce: 0.003028
2022-01-08 03:01:25,020 iteration 6720 : loss : 0.009378, loss_ce: 0.003383
2022-01-08 03:01:26,420 iteration 6721 : loss : 0.022364, loss_ce: 0.008198
2022-01-08 03:01:27,830 iteration 6722 : loss : 0.014850, loss_ce: 0.004152
2022-01-08 03:01:29,186 iteration 6723 : loss : 0.012658, loss_ce: 0.004147
2022-01-08 03:01:30,509 iteration 6724 : loss : 0.009801, loss_ce: 0.004330
2022-01-08 03:01:31,905 iteration 6725 : loss : 0.009896, loss_ce: 0.004062
2022-01-08 03:01:33,283 iteration 6726 : loss : 0.015888, loss_ce: 0.005144
2022-01-08 03:01:34,674 iteration 6727 : loss : 0.012431, loss_ce: 0.005604
2022-01-08 03:01:36,067 iteration 6728 : loss : 0.013100, loss_ce: 0.006026
2022-01-08 03:01:37,379 iteration 6729 : loss : 0.009304, loss_ce: 0.003054
2022-01-08 03:01:38,715 iteration 6730 : loss : 0.011872, loss_ce: 0.003989
2022-01-08 03:01:40,048 iteration 6731 : loss : 0.011220, loss_ce: 0.004894
2022-01-08 03:01:41,405 iteration 6732 : loss : 0.010416, loss_ce: 0.005206
 99%|████████████████████████████▋| 396/400 [2:47:04<01:42, 25.56s/it]2022-01-08 03:01:42,844 iteration 6733 : loss : 0.013705, loss_ce: 0.006871
2022-01-08 03:01:44,157 iteration 6734 : loss : 0.006090, loss_ce: 0.001869
2022-01-08 03:01:45,524 iteration 6735 : loss : 0.011785, loss_ce: 0.005601
2022-01-08 03:01:46,883 iteration 6736 : loss : 0.010180, loss_ce: 0.003012
2022-01-08 03:01:48,216 iteration 6737 : loss : 0.008908, loss_ce: 0.004212
2022-01-08 03:01:49,567 iteration 6738 : loss : 0.008412, loss_ce: 0.002982
2022-01-08 03:01:50,968 iteration 6739 : loss : 0.013879, loss_ce: 0.005106
2022-01-08 03:01:52,415 iteration 6740 : loss : 0.014269, loss_ce: 0.005445
2022-01-08 03:01:53,720 iteration 6741 : loss : 0.008110, loss_ce: 0.003447
2022-01-08 03:01:55,124 iteration 6742 : loss : 0.012235, loss_ce: 0.005714
2022-01-08 03:01:56,518 iteration 6743 : loss : 0.013443, loss_ce: 0.004754
2022-01-08 03:01:57,946 iteration 6744 : loss : 0.012111, loss_ce: 0.004677
2022-01-08 03:01:59,275 iteration 6745 : loss : 0.014730, loss_ce: 0.004329
2022-01-08 03:02:00,606 iteration 6746 : loss : 0.012523, loss_ce: 0.005273
2022-01-08 03:02:01,962 iteration 6747 : loss : 0.016957, loss_ce: 0.006081
2022-01-08 03:02:03,334 iteration 6748 : loss : 0.009762, loss_ce: 0.003843
2022-01-08 03:02:04,725 iteration 6749 : loss : 0.015789, loss_ce: 0.005305
 99%|████████████████████████████▊| 397/400 [2:47:28<01:14, 24.89s/it]2022-01-08 03:02:06,097 iteration 6750 : loss : 0.010568, loss_ce: 0.003964
2022-01-08 03:02:07,498 iteration 6751 : loss : 0.014816, loss_ce: 0.007467
2022-01-08 03:02:08,889 iteration 6752 : loss : 0.016254, loss_ce: 0.008493
2022-01-08 03:02:10,256 iteration 6753 : loss : 0.011158, loss_ce: 0.004699
2022-01-08 03:02:11,661 iteration 6754 : loss : 0.027836, loss_ce: 0.009433
2022-01-08 03:02:13,020 iteration 6755 : loss : 0.006832, loss_ce: 0.002150
2022-01-08 03:02:14,391 iteration 6756 : loss : 0.010363, loss_ce: 0.004148
2022-01-08 03:02:15,778 iteration 6757 : loss : 0.014654, loss_ce: 0.007118
2022-01-08 03:02:17,092 iteration 6758 : loss : 0.007735, loss_ce: 0.003308
2022-01-08 03:02:18,485 iteration 6759 : loss : 0.015776, loss_ce: 0.005736
2022-01-08 03:02:19,826 iteration 6760 : loss : 0.010890, loss_ce: 0.004223
2022-01-08 03:02:21,235 iteration 6761 : loss : 0.012525, loss_ce: 0.005108
2022-01-08 03:02:22,500 iteration 6762 : loss : 0.007485, loss_ce: 0.002734
2022-01-08 03:02:23,869 iteration 6763 : loss : 0.014130, loss_ce: 0.003670
2022-01-08 03:02:25,205 iteration 6764 : loss : 0.013405, loss_ce: 0.004663
2022-01-08 03:02:26,524 iteration 6765 : loss : 0.011965, loss_ce: 0.004024
2022-01-08 03:02:27,861 iteration 6766 : loss : 0.012500, loss_ce: 0.004305
100%|████████████████████████████▊| 398/400 [2:47:51<00:48, 24.36s/it]2022-01-08 03:02:29,363 iteration 6767 : loss : 0.010304, loss_ce: 0.003624
2022-01-08 03:02:30,653 iteration 6768 : loss : 0.012699, loss_ce: 0.006967
2022-01-08 03:02:32,011 iteration 6769 : loss : 0.010614, loss_ce: 0.003417
2022-01-08 03:02:33,319 iteration 6770 : loss : 0.008940, loss_ce: 0.003654
2022-01-08 03:02:34,664 iteration 6771 : loss : 0.012071, loss_ce: 0.003975
2022-01-08 03:02:36,086 iteration 6772 : loss : 0.020605, loss_ce: 0.005873
2022-01-08 03:02:37,454 iteration 6773 : loss : 0.010684, loss_ce: 0.004562
2022-01-08 03:02:38,830 iteration 6774 : loss : 0.013127, loss_ce: 0.005998
2022-01-08 03:02:40,181 iteration 6775 : loss : 0.011157, loss_ce: 0.004151
2022-01-08 03:02:41,591 iteration 6776 : loss : 0.029545, loss_ce: 0.012485
2022-01-08 03:02:42,952 iteration 6777 : loss : 0.013563, loss_ce: 0.005424
2022-01-08 03:02:44,268 iteration 6778 : loss : 0.009614, loss_ce: 0.003234
2022-01-08 03:02:45,616 iteration 6779 : loss : 0.009342, loss_ce: 0.002763
2022-01-08 03:02:46,898 iteration 6780 : loss : 0.008193, loss_ce: 0.002817
2022-01-08 03:02:48,210 iteration 6781 : loss : 0.012936, loss_ce: 0.006427
2022-01-08 03:02:49,562 iteration 6782 : loss : 0.007963, loss_ce: 0.003540
2022-01-08 03:02:50,929 iteration 6783 : loss : 0.009241, loss_ce: 0.003682
100%|████████████████████████████▉| 399/400 [2:48:14<00:23, 23.97s/it]2022-01-08 03:02:52,422 iteration 6784 : loss : 0.014821, loss_ce: 0.004488
2022-01-08 03:02:53,708 iteration 6785 : loss : 0.013745, loss_ce: 0.005712
2022-01-08 03:02:54,973 iteration 6786 : loss : 0.010336, loss_ce: 0.004698
2022-01-08 03:02:56,359 iteration 6787 : loss : 0.017687, loss_ce: 0.008220
2022-01-08 03:02:57,649 iteration 6788 : loss : 0.014110, loss_ce: 0.003112
2022-01-08 03:02:59,031 iteration 6789 : loss : 0.008332, loss_ce: 0.003343
2022-01-08 03:03:00,413 iteration 6790 : loss : 0.012546, loss_ce: 0.004051
2022-01-08 03:03:01,827 iteration 6791 : loss : 0.014615, loss_ce: 0.004288
2022-01-08 03:03:03,235 iteration 6792 : loss : 0.009225, loss_ce: 0.003343
2022-01-08 03:03:04,602 iteration 6793 : loss : 0.013316, loss_ce: 0.004481
2022-01-08 03:03:05,966 iteration 6794 : loss : 0.011061, loss_ce: 0.004009
2022-01-08 03:03:07,297 iteration 6795 : loss : 0.011214, loss_ce: 0.004435
2022-01-08 03:03:08,645 iteration 6796 : loss : 0.015119, loss_ce: 0.005506
2022-01-08 03:03:09,969 iteration 6797 : loss : 0.012090, loss_ce: 0.004300
2022-01-08 03:03:11,268 iteration 6798 : loss : 0.009893, loss_ce: 0.003462
2022-01-08 03:03:12,632 iteration 6799 : loss : 0.018721, loss_ce: 0.009800
2022-01-08 03:03:12,633 Training Data Eval:
2022-01-08 03:03:19,505   Average segmentation loss on training set: 0.0060
2022-01-08 03:03:19,505 Validation Data Eval:
2022-01-08 03:03:21,882   Average segmentation loss on validation set: 0.0786
2022-01-08 03:03:23,246 iteration 6800 : loss : 0.011797, loss_ce: 0.005548
100%|█████████████████████████████| 400/400 [2:48:46<00:00, 26.48s/it]100%|█████████████████████████████| 400/400 [2:48:46<00:00, 25.32s/it]
