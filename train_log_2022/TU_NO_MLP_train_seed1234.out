2022-01-06 12:41:32,958 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-06 12:41:32,959 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-06 12:41:32,959 ============================================================
2022-01-06 12:41:32,959 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-06 12:41:32,959 ============================================================
2022-01-06 12:41:32,959 Loading data...
2022-01-06 12:41:32,959 Reading NCI - RUNMC images...
2022-01-06 12:41:32,959 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-06 12:41:32,962 Already preprocessed this configuration. Loading now!
2022-01-06 12:41:32,995 Training Images: (256, 256, 286)
2022-01-06 12:41:32,995 Training Labels: (256, 256, 286)
2022-01-06 12:41:32,995 Validation Images: (256, 256, 98)
2022-01-06 12:41:32,995 Validation Labels: (256, 256, 98)
2022-01-06 12:41:32,995 ============================================================
2022-01-06 12:41:33,111 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-06 12:41:35,545 iteration 1 : loss : 0.982594, loss_ce: 1.211083
2022-01-06 12:41:36,616 iteration 2 : loss : 0.919970, loss_ce: 1.108883
2022-01-06 12:41:37,803 iteration 3 : loss : 0.863774, loss_ce: 1.007098
2022-01-06 12:41:38,880 iteration 4 : loss : 0.819760, loss_ce: 0.933725
2022-01-06 12:41:39,969 iteration 5 : loss : 0.762481, loss_ce: 0.853909
2022-01-06 12:41:41,067 iteration 6 : loss : 0.738700, loss_ce: 0.801091
2022-01-06 12:41:42,247 iteration 7 : loss : 0.689825, loss_ce: 0.735194
2022-01-06 12:41:43,342 iteration 8 : loss : 0.672767, loss_ce: 0.678995
2022-01-06 12:41:44,466 iteration 9 : loss : 0.612864, loss_ce: 0.643238
2022-01-06 12:41:45,649 iteration 10 : loss : 0.611244, loss_ce: 0.579734
2022-01-06 12:41:46,853 iteration 11 : loss : 0.573654, loss_ce: 0.546732
2022-01-06 12:41:47,936 iteration 12 : loss : 0.545755, loss_ce: 0.498703
2022-01-06 12:41:49,009 iteration 13 : loss : 0.535300, loss_ce: 0.468920
2022-01-06 12:41:50,055 iteration 14 : loss : 0.502355, loss_ce: 0.433448
2022-01-06 12:41:51,185 iteration 15 : loss : 0.484283, loss_ce: 0.400319
2022-01-06 12:41:52,282 iteration 16 : loss : 0.481531, loss_ce: 0.381691
2022-01-06 12:41:53,367 iteration 17 : loss : 0.433595, loss_ce: 0.343951
  0%|                               | 1/400 [00:20<2:15:16, 20.34s/it]2022-01-06 12:41:54,587 iteration 18 : loss : 0.443468, loss_ce: 0.305363
2022-01-06 12:41:55,631 iteration 19 : loss : 0.396648, loss_ce: 0.282228
2022-01-06 12:41:56,802 iteration 20 : loss : 0.387563, loss_ce: 0.259207
2022-01-06 12:41:57,898 iteration 21 : loss : 0.401097, loss_ce: 0.246038
2022-01-06 12:41:58,986 iteration 22 : loss : 0.354635, loss_ce: 0.230420
2022-01-06 12:42:00,174 iteration 23 : loss : 0.352043, loss_ce: 0.201146
2022-01-06 12:42:01,282 iteration 24 : loss : 0.329840, loss_ce: 0.199488
2022-01-06 12:42:02,445 iteration 25 : loss : 0.381372, loss_ce: 0.237812
2022-01-06 12:42:03,526 iteration 26 : loss : 0.307405, loss_ce: 0.172948
2022-01-06 12:42:04,550 iteration 27 : loss : 0.320945, loss_ce: 0.186890
2022-01-06 12:42:05,581 iteration 28 : loss : 0.304191, loss_ce: 0.166693
2022-01-06 12:42:06,723 iteration 29 : loss : 0.300604, loss_ce: 0.153411
2022-01-06 12:42:07,854 iteration 30 : loss : 0.287296, loss_ce: 0.143523
2022-01-06 12:42:08,894 iteration 31 : loss : 0.295405, loss_ce: 0.150552
2022-01-06 12:42:10,055 iteration 32 : loss : 0.299253, loss_ce: 0.163765
2022-01-06 12:42:11,199 iteration 33 : loss : 0.288698, loss_ce: 0.158260
2022-01-06 12:42:12,342 iteration 34 : loss : 0.274188, loss_ce: 0.155364
  0%|▏                              | 2/400 [00:39<2:09:30, 19.53s/it]2022-01-06 12:42:13,546 iteration 35 : loss : 0.272357, loss_ce: 0.133201
2022-01-06 12:42:14,707 iteration 36 : loss : 0.284863, loss_ce: 0.153389
2022-01-06 12:42:15,877 iteration 37 : loss : 0.275397, loss_ce: 0.126829
2022-01-06 12:42:16,949 iteration 38 : loss : 0.256422, loss_ce: 0.128057
2022-01-06 12:42:18,039 iteration 39 : loss : 0.244866, loss_ce: 0.115632
2022-01-06 12:42:19,212 iteration 40 : loss : 0.278942, loss_ce: 0.138243
2022-01-06 12:42:20,375 iteration 41 : loss : 0.325198, loss_ce: 0.159936
2022-01-06 12:42:21,500 iteration 42 : loss : 0.268573, loss_ce: 0.131046
2022-01-06 12:42:22,554 iteration 43 : loss : 0.268379, loss_ce: 0.122755
2022-01-06 12:42:23,749 iteration 44 : loss : 0.240854, loss_ce: 0.110742
2022-01-06 12:42:24,865 iteration 45 : loss : 0.232100, loss_ce: 0.105415
2022-01-06 12:42:26,000 iteration 46 : loss : 0.255804, loss_ce: 0.104848
2022-01-06 12:42:27,180 iteration 47 : loss : 0.223694, loss_ce: 0.089000
2022-01-06 12:42:28,324 iteration 48 : loss : 0.226015, loss_ce: 0.095849
2022-01-06 12:42:29,513 iteration 49 : loss : 0.265905, loss_ce: 0.120775
2022-01-06 12:42:30,594 iteration 50 : loss : 0.320749, loss_ce: 0.145560
2022-01-06 12:42:31,643 iteration 51 : loss : 0.269110, loss_ce: 0.128234
  1%|▏                              | 3/400 [00:58<2:08:30, 19.42s/it]2022-01-06 12:42:32,878 iteration 52 : loss : 0.283638, loss_ce: 0.144457
2022-01-06 12:42:34,031 iteration 53 : loss : 0.267956, loss_ce: 0.130681
2022-01-06 12:42:35,146 iteration 54 : loss : 0.251911, loss_ce: 0.109678
2022-01-06 12:42:36,280 iteration 55 : loss : 0.276930, loss_ce: 0.140241
2022-01-06 12:42:37,399 iteration 56 : loss : 0.258248, loss_ce: 0.112232
2022-01-06 12:42:38,547 iteration 57 : loss : 0.219022, loss_ce: 0.089974
2022-01-06 12:42:39,686 iteration 58 : loss : 0.257227, loss_ce: 0.104915
2022-01-06 12:42:40,793 iteration 59 : loss : 0.218976, loss_ce: 0.094777
2022-01-06 12:42:41,941 iteration 60 : loss : 0.225667, loss_ce: 0.091873
2022-01-06 12:42:43,066 iteration 61 : loss : 0.255124, loss_ce: 0.119156
2022-01-06 12:42:44,180 iteration 62 : loss : 0.336590, loss_ce: 0.125378
2022-01-06 12:42:45,221 iteration 63 : loss : 0.305344, loss_ce: 0.152351
2022-01-06 12:42:46,327 iteration 64 : loss : 0.342174, loss_ce: 0.155975
2022-01-06 12:42:47,379 iteration 65 : loss : 0.260225, loss_ce: 0.103236
2022-01-06 12:42:48,472 iteration 66 : loss : 0.226205, loss_ce: 0.098205
2022-01-06 12:42:49,642 iteration 67 : loss : 0.258517, loss_ce: 0.096704
2022-01-06 12:42:50,754 iteration 68 : loss : 0.215691, loss_ce: 0.092850
  1%|▎                              | 4/400 [01:17<2:07:21, 19.30s/it]2022-01-06 12:42:51,950 iteration 69 : loss : 0.221949, loss_ce: 0.091752
2022-01-06 12:42:53,151 iteration 70 : loss : 0.235888, loss_ce: 0.096862
2022-01-06 12:42:54,235 iteration 71 : loss : 0.219179, loss_ce: 0.088680
2022-01-06 12:42:55,363 iteration 72 : loss : 0.220836, loss_ce: 0.090280
2022-01-06 12:42:56,434 iteration 73 : loss : 0.240902, loss_ce: 0.117179
2022-01-06 12:42:57,471 iteration 74 : loss : 0.217809, loss_ce: 0.090230
2022-01-06 12:42:58,530 iteration 75 : loss : 0.219407, loss_ce: 0.091434
2022-01-06 12:42:59,589 iteration 76 : loss : 0.234081, loss_ce: 0.099268
2022-01-06 12:43:00,568 iteration 77 : loss : 0.226750, loss_ce: 0.104471
2022-01-06 12:43:01,659 iteration 78 : loss : 0.261418, loss_ce: 0.120265
2022-01-06 12:43:02,731 iteration 79 : loss : 0.260944, loss_ce: 0.100561
2022-01-06 12:43:03,781 iteration 80 : loss : 0.240915, loss_ce: 0.106839
2022-01-06 12:43:04,831 iteration 81 : loss : 0.228795, loss_ce: 0.098175
2022-01-06 12:43:05,920 iteration 82 : loss : 0.223423, loss_ce: 0.085091
2022-01-06 12:43:06,999 iteration 83 : loss : 0.250926, loss_ce: 0.090848
2022-01-06 12:43:08,180 iteration 84 : loss : 0.250526, loss_ce: 0.126129
2022-01-06 12:43:08,180 Training Data Eval:
2022-01-06 12:43:13,692   Average segmentation loss on training set: 0.7054
2022-01-06 12:43:13,692 Validation Data Eval:
2022-01-06 12:43:15,720   Average segmentation loss on validation set: 0.6704
2022-01-06 12:43:21,615 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 12:43:22,716 iteration 85 : loss : 0.264456, loss_ce: 0.111492
  1%|▍                              | 5/400 [01:49<2:37:05, 23.86s/it]2022-01-06 12:43:23,877 iteration 86 : loss : 0.267835, loss_ce: 0.098230
2022-01-06 12:43:25,122 iteration 87 : loss : 0.208004, loss_ce: 0.086380
2022-01-06 12:43:26,179 iteration 88 : loss : 0.232442, loss_ce: 0.100086
2022-01-06 12:43:27,314 iteration 89 : loss : 0.240619, loss_ce: 0.097326
2022-01-06 12:43:28,435 iteration 90 : loss : 0.220579, loss_ce: 0.086812
2022-01-06 12:43:29,634 iteration 91 : loss : 0.218728, loss_ce: 0.105658
2022-01-06 12:43:30,670 iteration 92 : loss : 0.216248, loss_ce: 0.093552
2022-01-06 12:43:31,751 iteration 93 : loss : 0.244675, loss_ce: 0.092834
2022-01-06 12:43:32,845 iteration 94 : loss : 0.244258, loss_ce: 0.097317
2022-01-06 12:43:34,079 iteration 95 : loss : 0.233087, loss_ce: 0.107792
2022-01-06 12:43:35,153 iteration 96 : loss : 0.213562, loss_ce: 0.093756
2022-01-06 12:43:36,228 iteration 97 : loss : 0.252364, loss_ce: 0.098955
2022-01-06 12:43:37,322 iteration 98 : loss : 0.233789, loss_ce: 0.102131
2022-01-06 12:43:38,521 iteration 99 : loss : 0.222950, loss_ce: 0.091756
2022-01-06 12:43:39,653 iteration 100 : loss : 0.221407, loss_ce: 0.090914
2022-01-06 12:43:40,750 iteration 101 : loss : 0.173579, loss_ce: 0.067477
2022-01-06 12:43:41,777 iteration 102 : loss : 0.235306, loss_ce: 0.098967
  2%|▍                              | 6/400 [02:08<2:26:00, 22.23s/it]2022-01-06 12:43:43,054 iteration 103 : loss : 0.221487, loss_ce: 0.098845
2022-01-06 12:43:44,245 iteration 104 : loss : 0.238673, loss_ce: 0.094832
2022-01-06 12:43:45,498 iteration 105 : loss : 0.230395, loss_ce: 0.084417
2022-01-06 12:43:46,568 iteration 106 : loss : 0.226103, loss_ce: 0.090451
2022-01-06 12:43:47,835 iteration 107 : loss : 0.255611, loss_ce: 0.111014
2022-01-06 12:43:48,877 iteration 108 : loss : 0.258126, loss_ce: 0.099919
2022-01-06 12:43:49,956 iteration 109 : loss : 0.206822, loss_ce: 0.085275
2022-01-06 12:43:50,977 iteration 110 : loss : 0.197452, loss_ce: 0.080089
2022-01-06 12:43:52,104 iteration 111 : loss : 0.247861, loss_ce: 0.113122
2022-01-06 12:43:53,132 iteration 112 : loss : 0.203230, loss_ce: 0.079484
2022-01-06 12:43:54,242 iteration 113 : loss : 0.262293, loss_ce: 0.133277
2022-01-06 12:43:55,305 iteration 114 : loss : 0.251495, loss_ce: 0.088054
2022-01-06 12:43:56,391 iteration 115 : loss : 0.224099, loss_ce: 0.095514
2022-01-06 12:43:57,546 iteration 116 : loss : 0.242071, loss_ce: 0.104185
2022-01-06 12:43:58,706 iteration 117 : loss : 0.229756, loss_ce: 0.096809
2022-01-06 12:43:59,817 iteration 118 : loss : 0.265134, loss_ce: 0.114131
2022-01-06 12:44:00,946 iteration 119 : loss : 0.208827, loss_ce: 0.074551
  2%|▌                              | 7/400 [02:27<2:19:04, 21.23s/it]2022-01-06 12:44:02,068 iteration 120 : loss : 0.319030, loss_ce: 0.153185
2022-01-06 12:44:03,108 iteration 121 : loss : 0.210964, loss_ce: 0.085688
2022-01-06 12:44:04,262 iteration 122 : loss : 0.234917, loss_ce: 0.093869
2022-01-06 12:44:05,375 iteration 123 : loss : 0.211780, loss_ce: 0.085178
2022-01-06 12:44:06,462 iteration 124 : loss : 0.225209, loss_ce: 0.080823
2022-01-06 12:44:07,521 iteration 125 : loss : 0.204559, loss_ce: 0.092889
2022-01-06 12:44:08,640 iteration 126 : loss : 0.234157, loss_ce: 0.085182
2022-01-06 12:44:09,700 iteration 127 : loss : 0.210250, loss_ce: 0.089672
2022-01-06 12:44:10,792 iteration 128 : loss : 0.195040, loss_ce: 0.075210
2022-01-06 12:44:11,962 iteration 129 : loss : 0.211330, loss_ce: 0.076701
2022-01-06 12:44:13,061 iteration 130 : loss : 0.210953, loss_ce: 0.079554
2022-01-06 12:44:14,257 iteration 131 : loss : 0.238999, loss_ce: 0.109464
2022-01-06 12:44:15,481 iteration 132 : loss : 0.174200, loss_ce: 0.049995
2022-01-06 12:44:16,560 iteration 133 : loss : 0.206876, loss_ce: 0.075535
2022-01-06 12:44:17,746 iteration 134 : loss : 0.214170, loss_ce: 0.091068
2022-01-06 12:44:18,946 iteration 135 : loss : 0.225653, loss_ce: 0.094157
2022-01-06 12:44:20,042 iteration 136 : loss : 0.169198, loss_ce: 0.070575
  2%|▌                              | 8/400 [02:46<2:14:15, 20.55s/it]2022-01-06 12:44:21,269 iteration 137 : loss : 0.222609, loss_ce: 0.076001
2022-01-06 12:44:22,339 iteration 138 : loss : 0.245917, loss_ce: 0.125465
2022-01-06 12:44:23,474 iteration 139 : loss : 0.232115, loss_ce: 0.094062
2022-01-06 12:44:24,579 iteration 140 : loss : 0.216264, loss_ce: 0.076412
2022-01-06 12:44:25,748 iteration 141 : loss : 0.207777, loss_ce: 0.090514
2022-01-06 12:44:26,920 iteration 142 : loss : 0.227227, loss_ce: 0.089105
2022-01-06 12:44:28,120 iteration 143 : loss : 0.231505, loss_ce: 0.103064
2022-01-06 12:44:29,282 iteration 144 : loss : 0.222767, loss_ce: 0.079158
2022-01-06 12:44:30,382 iteration 145 : loss : 0.233268, loss_ce: 0.084616
2022-01-06 12:44:31,530 iteration 146 : loss : 0.218468, loss_ce: 0.094048
2022-01-06 12:44:32,679 iteration 147 : loss : 0.206842, loss_ce: 0.082882
2022-01-06 12:44:33,690 iteration 148 : loss : 0.213379, loss_ce: 0.085801
2022-01-06 12:44:34,779 iteration 149 : loss : 0.250508, loss_ce: 0.106884
2022-01-06 12:44:35,847 iteration 150 : loss : 0.244622, loss_ce: 0.087445
2022-01-06 12:44:36,917 iteration 151 : loss : 0.203810, loss_ce: 0.084950
2022-01-06 12:44:38,000 iteration 152 : loss : 0.234094, loss_ce: 0.081531
2022-01-06 12:44:39,134 iteration 153 : loss : 0.239760, loss_ce: 0.104752
  2%|▋                              | 9/400 [03:06<2:10:57, 20.10s/it]2022-01-06 12:44:40,247 iteration 154 : loss : 0.239093, loss_ce: 0.099402
2022-01-06 12:44:41,381 iteration 155 : loss : 0.213349, loss_ce: 0.073265
2022-01-06 12:44:42,591 iteration 156 : loss : 0.202796, loss_ce: 0.075869
2022-01-06 12:44:43,726 iteration 157 : loss : 0.228743, loss_ce: 0.082142
2022-01-06 12:44:44,984 iteration 158 : loss : 0.203815, loss_ce: 0.080771
2022-01-06 12:44:45,976 iteration 159 : loss : 0.184930, loss_ce: 0.071421
2022-01-06 12:44:47,177 iteration 160 : loss : 0.178461, loss_ce: 0.056714
2022-01-06 12:44:48,348 iteration 161 : loss : 0.247401, loss_ce: 0.094969
2022-01-06 12:44:49,535 iteration 162 : loss : 0.215212, loss_ce: 0.097617
2022-01-06 12:44:50,660 iteration 163 : loss : 0.234020, loss_ce: 0.093066
2022-01-06 12:44:51,800 iteration 164 : loss : 0.218014, loss_ce: 0.075313
2022-01-06 12:44:52,904 iteration 165 : loss : 0.209178, loss_ce: 0.087956
2022-01-06 12:44:53,986 iteration 166 : loss : 0.221237, loss_ce: 0.085504
2022-01-06 12:44:55,085 iteration 167 : loss : 0.182361, loss_ce: 0.068035
2022-01-06 12:44:56,241 iteration 168 : loss : 0.214676, loss_ce: 0.104817
2022-01-06 12:44:57,355 iteration 169 : loss : 0.199132, loss_ce: 0.089791
2022-01-06 12:44:57,355 Training Data Eval:
2022-01-06 12:45:02,843   Average segmentation loss on training set: 1.5144
2022-01-06 12:45:02,843 Validation Data Eval:
2022-01-06 12:45:04,733   Average segmentation loss on validation set: 1.4319
2022-01-06 12:45:05,901 iteration 170 : loss : 0.178349, loss_ce: 0.074478
  2%|▊                             | 10/400 [03:32<2:23:59, 22.15s/it]2022-01-06 12:45:07,102 iteration 171 : loss : 0.192111, loss_ce: 0.076888
2022-01-06 12:45:08,251 iteration 172 : loss : 0.211142, loss_ce: 0.087962
2022-01-06 12:45:09,416 iteration 173 : loss : 0.163346, loss_ce: 0.069896
2022-01-06 12:45:10,488 iteration 174 : loss : 0.218153, loss_ce: 0.093688
2022-01-06 12:45:11,546 iteration 175 : loss : 0.246519, loss_ce: 0.093583
2022-01-06 12:45:12,691 iteration 176 : loss : 0.173371, loss_ce: 0.073701
2022-01-06 12:45:13,807 iteration 177 : loss : 0.180411, loss_ce: 0.057380
2022-01-06 12:45:14,921 iteration 178 : loss : 0.170934, loss_ce: 0.067034
2022-01-06 12:45:16,034 iteration 179 : loss : 0.220886, loss_ce: 0.091326
2022-01-06 12:45:17,129 iteration 180 : loss : 0.180326, loss_ce: 0.064020
2022-01-06 12:45:18,284 iteration 181 : loss : 0.194444, loss_ce: 0.082484
2022-01-06 12:45:19,371 iteration 182 : loss : 0.152487, loss_ce: 0.063006
2022-01-06 12:45:20,445 iteration 183 : loss : 0.178043, loss_ce: 0.060372
2022-01-06 12:45:21,476 iteration 184 : loss : 0.203560, loss_ce: 0.071522
2022-01-06 12:45:22,629 iteration 185 : loss : 0.226301, loss_ce: 0.088840
2022-01-06 12:45:23,787 iteration 186 : loss : 0.236249, loss_ce: 0.109252
2022-01-06 12:45:24,983 iteration 187 : loss : 0.208935, loss_ce: 0.076268
  3%|▊                             | 11/400 [03:51<2:17:31, 21.21s/it]2022-01-06 12:45:26,186 iteration 188 : loss : 0.282881, loss_ce: 0.114210
2022-01-06 12:45:27,331 iteration 189 : loss : 0.189897, loss_ce: 0.078757
2022-01-06 12:45:28,483 iteration 190 : loss : 0.216443, loss_ce: 0.085964
2022-01-06 12:45:29,583 iteration 191 : loss : 0.180187, loss_ce: 0.072520
2022-01-06 12:45:30,629 iteration 192 : loss : 0.210255, loss_ce: 0.100370
2022-01-06 12:45:31,794 iteration 193 : loss : 0.184212, loss_ce: 0.081037
2022-01-06 12:45:32,880 iteration 194 : loss : 0.264033, loss_ce: 0.081883
2022-01-06 12:45:33,945 iteration 195 : loss : 0.189917, loss_ce: 0.090255
2022-01-06 12:45:34,976 iteration 196 : loss : 0.195940, loss_ce: 0.072907
2022-01-06 12:45:36,098 iteration 197 : loss : 0.225143, loss_ce: 0.088590
2022-01-06 12:45:37,246 iteration 198 : loss : 0.192683, loss_ce: 0.085926
2022-01-06 12:45:38,318 iteration 199 : loss : 0.204889, loss_ce: 0.081907
2022-01-06 12:45:39,407 iteration 200 : loss : 0.185835, loss_ce: 0.086103
2022-01-06 12:45:40,534 iteration 201 : loss : 0.138135, loss_ce: 0.056266
2022-01-06 12:45:41,682 iteration 202 : loss : 0.193006, loss_ce: 0.082948
2022-01-06 12:45:42,755 iteration 203 : loss : 0.158583, loss_ce: 0.060923
2022-01-06 12:45:43,969 iteration 204 : loss : 0.217909, loss_ce: 0.077827
  3%|▉                             | 12/400 [04:10<2:12:47, 20.53s/it]2022-01-06 12:45:45,092 iteration 205 : loss : 0.179275, loss_ce: 0.064203
2022-01-06 12:45:46,194 iteration 206 : loss : 0.160689, loss_ce: 0.056234
2022-01-06 12:45:47,335 iteration 207 : loss : 0.166081, loss_ce: 0.071852
2022-01-06 12:45:48,427 iteration 208 : loss : 0.215533, loss_ce: 0.072451
2022-01-06 12:45:49,618 iteration 209 : loss : 0.270826, loss_ce: 0.129949
2022-01-06 12:45:50,632 iteration 210 : loss : 0.160352, loss_ce: 0.061663
2022-01-06 12:45:51,782 iteration 211 : loss : 0.234056, loss_ce: 0.110745
2022-01-06 12:45:52,915 iteration 212 : loss : 0.217347, loss_ce: 0.084184
2022-01-06 12:45:54,142 iteration 213 : loss : 0.145003, loss_ce: 0.071554
2022-01-06 12:45:55,257 iteration 214 : loss : 0.195847, loss_ce: 0.067150
2022-01-06 12:45:56,371 iteration 215 : loss : 0.205864, loss_ce: 0.082473
2022-01-06 12:45:57,660 iteration 216 : loss : 0.217680, loss_ce: 0.083253
2022-01-06 12:45:58,878 iteration 217 : loss : 0.157588, loss_ce: 0.060598
2022-01-06 12:46:00,009 iteration 218 : loss : 0.191974, loss_ce: 0.086200
2022-01-06 12:46:01,014 iteration 219 : loss : 0.183576, loss_ce: 0.078128
2022-01-06 12:46:02,147 iteration 220 : loss : 0.145985, loss_ce: 0.065378
2022-01-06 12:46:03,272 iteration 221 : loss : 0.209086, loss_ce: 0.084501
  3%|▉                             | 13/400 [04:30<2:10:02, 20.16s/it]2022-01-06 12:46:04,460 iteration 222 : loss : 0.214997, loss_ce: 0.108408
2022-01-06 12:46:05,628 iteration 223 : loss : 0.159622, loss_ce: 0.067290
2022-01-06 12:46:06,739 iteration 224 : loss : 0.290648, loss_ce: 0.142708
2022-01-06 12:46:07,950 iteration 225 : loss : 0.305311, loss_ce: 0.095248
2022-01-06 12:46:09,048 iteration 226 : loss : 0.206899, loss_ce: 0.074139
2022-01-06 12:46:10,197 iteration 227 : loss : 0.268164, loss_ce: 0.122520
2022-01-06 12:46:11,323 iteration 228 : loss : 0.216365, loss_ce: 0.088542
2022-01-06 12:46:12,382 iteration 229 : loss : 0.151608, loss_ce: 0.057509
2022-01-06 12:46:13,509 iteration 230 : loss : 0.176493, loss_ce: 0.076559
2022-01-06 12:46:14,671 iteration 231 : loss : 0.257609, loss_ce: 0.099857
2022-01-06 12:46:15,822 iteration 232 : loss : 0.150566, loss_ce: 0.064189
2022-01-06 12:46:16,875 iteration 233 : loss : 0.209774, loss_ce: 0.092884
2022-01-06 12:46:18,064 iteration 234 : loss : 0.193616, loss_ce: 0.079184
2022-01-06 12:46:19,156 iteration 235 : loss : 0.180238, loss_ce: 0.082375
2022-01-06 12:46:20,299 iteration 236 : loss : 0.189773, loss_ce: 0.078285
2022-01-06 12:46:21,396 iteration 237 : loss : 0.155589, loss_ce: 0.063978
2022-01-06 12:46:22,508 iteration 238 : loss : 0.179003, loss_ce: 0.087669
  4%|█                             | 14/400 [04:49<2:07:54, 19.88s/it]2022-01-06 12:46:23,624 iteration 239 : loss : 0.224911, loss_ce: 0.081361
2022-01-06 12:46:24,777 iteration 240 : loss : 0.166418, loss_ce: 0.073146
2022-01-06 12:46:25,936 iteration 241 : loss : 0.204726, loss_ce: 0.089247
2022-01-06 12:46:27,073 iteration 242 : loss : 0.218795, loss_ce: 0.112780
2022-01-06 12:46:28,360 iteration 243 : loss : 0.204413, loss_ce: 0.091158
2022-01-06 12:46:29,424 iteration 244 : loss : 0.205796, loss_ce: 0.058862
2022-01-06 12:46:30,559 iteration 245 : loss : 0.211055, loss_ce: 0.107447
2022-01-06 12:46:31,746 iteration 246 : loss : 0.158331, loss_ce: 0.066300
2022-01-06 12:46:32,862 iteration 247 : loss : 0.210144, loss_ce: 0.076151
2022-01-06 12:46:33,971 iteration 248 : loss : 0.181924, loss_ce: 0.072528
2022-01-06 12:46:35,064 iteration 249 : loss : 0.193802, loss_ce: 0.101228
2022-01-06 12:46:36,182 iteration 250 : loss : 0.229221, loss_ce: 0.097073
2022-01-06 12:46:37,236 iteration 251 : loss : 0.228770, loss_ce: 0.074332
2022-01-06 12:46:38,342 iteration 252 : loss : 0.144768, loss_ce: 0.060122
2022-01-06 12:46:39,487 iteration 253 : loss : 0.194397, loss_ce: 0.078655
2022-01-06 12:46:40,698 iteration 254 : loss : 0.270571, loss_ce: 0.086735
2022-01-06 12:46:40,698 Training Data Eval:
2022-01-06 12:46:46,243   Average segmentation loss on training set: 1.8211
2022-01-06 12:46:46,244 Validation Data Eval:
2022-01-06 12:46:48,157   Average segmentation loss on validation set: 1.7766
2022-01-06 12:46:49,263 iteration 255 : loss : 0.167419, loss_ce: 0.073296
  4%|█▏                            | 15/400 [05:16<2:20:53, 21.96s/it]2022-01-06 12:46:50,494 iteration 256 : loss : 0.201039, loss_ce: 0.078419
2022-01-06 12:46:51,684 iteration 257 : loss : 0.157137, loss_ce: 0.066827
2022-01-06 12:46:52,919 iteration 258 : loss : 0.162267, loss_ce: 0.060879
2022-01-06 12:46:53,956 iteration 259 : loss : 0.151854, loss_ce: 0.063649
2022-01-06 12:46:55,148 iteration 260 : loss : 0.167181, loss_ce: 0.074096
2022-01-06 12:46:56,288 iteration 261 : loss : 0.239523, loss_ce: 0.085058
2022-01-06 12:46:57,520 iteration 262 : loss : 0.199662, loss_ce: 0.079714
2022-01-06 12:46:58,628 iteration 263 : loss : 0.167570, loss_ce: 0.066299
2022-01-06 12:46:59,762 iteration 264 : loss : 0.210428, loss_ce: 0.101884
2022-01-06 12:47:00,851 iteration 265 : loss : 0.134555, loss_ce: 0.057205
2022-01-06 12:47:02,009 iteration 266 : loss : 0.207145, loss_ce: 0.101344
2022-01-06 12:47:03,127 iteration 267 : loss : 0.167925, loss_ce: 0.055947
2022-01-06 12:47:04,263 iteration 268 : loss : 0.211503, loss_ce: 0.098653
2022-01-06 12:47:05,610 iteration 269 : loss : 0.197773, loss_ce: 0.066179
2022-01-06 12:47:06,851 iteration 270 : loss : 0.178470, loss_ce: 0.070267
2022-01-06 12:47:07,836 iteration 271 : loss : 0.199394, loss_ce: 0.053989
2022-01-06 12:47:08,961 iteration 272 : loss : 0.202440, loss_ce: 0.091265
  4%|█▏                            | 16/400 [05:35<2:16:10, 21.28s/it]2022-01-06 12:47:10,095 iteration 273 : loss : 0.182839, loss_ce: 0.066637
2022-01-06 12:47:11,148 iteration 274 : loss : 0.150971, loss_ce: 0.063776
2022-01-06 12:47:12,219 iteration 275 : loss : 0.161192, loss_ce: 0.067602
2022-01-06 12:47:13,356 iteration 276 : loss : 0.154602, loss_ce: 0.077213
2022-01-06 12:47:14,515 iteration 277 : loss : 0.155941, loss_ce: 0.064337
2022-01-06 12:47:15,509 iteration 278 : loss : 0.187500, loss_ce: 0.064921
2022-01-06 12:47:16,579 iteration 279 : loss : 0.200165, loss_ce: 0.072725
2022-01-06 12:47:17,640 iteration 280 : loss : 0.189431, loss_ce: 0.076733
2022-01-06 12:47:18,777 iteration 281 : loss : 0.180134, loss_ce: 0.076799
2022-01-06 12:47:19,873 iteration 282 : loss : 0.214035, loss_ce: 0.083223
2022-01-06 12:47:20,950 iteration 283 : loss : 0.173928, loss_ce: 0.084417
2022-01-06 12:47:22,134 iteration 284 : loss : 0.209644, loss_ce: 0.104492
2022-01-06 12:47:23,190 iteration 285 : loss : 0.195675, loss_ce: 0.070369
2022-01-06 12:47:24,253 iteration 286 : loss : 0.164609, loss_ce: 0.073486
2022-01-06 12:47:25,444 iteration 287 : loss : 0.170102, loss_ce: 0.074471
2022-01-06 12:47:26,631 iteration 288 : loss : 0.218588, loss_ce: 0.088397
2022-01-06 12:47:27,723 iteration 289 : loss : 0.171784, loss_ce: 0.069741
  4%|█▎                            | 17/400 [05:54<2:10:58, 20.52s/it]2022-01-06 12:47:28,901 iteration 290 : loss : 0.149428, loss_ce: 0.054012
2022-01-06 12:47:29,986 iteration 291 : loss : 0.210896, loss_ce: 0.083695
2022-01-06 12:47:31,082 iteration 292 : loss : 0.194166, loss_ce: 0.080569
2022-01-06 12:47:32,189 iteration 293 : loss : 0.219651, loss_ce: 0.099784
2022-01-06 12:47:33,334 iteration 294 : loss : 0.167192, loss_ce: 0.065621
2022-01-06 12:47:34,576 iteration 295 : loss : 0.203426, loss_ce: 0.070399
2022-01-06 12:47:35,683 iteration 296 : loss : 0.160244, loss_ce: 0.064414
2022-01-06 12:47:36,826 iteration 297 : loss : 0.179815, loss_ce: 0.060852
2022-01-06 12:47:37,930 iteration 298 : loss : 0.139622, loss_ce: 0.049381
2022-01-06 12:47:39,067 iteration 299 : loss : 0.161344, loss_ce: 0.060835
2022-01-06 12:47:40,200 iteration 300 : loss : 0.206154, loss_ce: 0.081244
2022-01-06 12:47:41,350 iteration 301 : loss : 0.228329, loss_ce: 0.137969
2022-01-06 12:47:42,535 iteration 302 : loss : 0.141191, loss_ce: 0.064486
2022-01-06 12:47:43,689 iteration 303 : loss : 0.218926, loss_ce: 0.099129
2022-01-06 12:47:44,838 iteration 304 : loss : 0.253493, loss_ce: 0.115641
2022-01-06 12:47:46,019 iteration 305 : loss : 0.204746, loss_ce: 0.081069
2022-01-06 12:47:47,190 iteration 306 : loss : 0.126380, loss_ce: 0.060198
  4%|█▎                            | 18/400 [06:14<2:08:38, 20.20s/it]2022-01-06 12:47:48,381 iteration 307 : loss : 0.182841, loss_ce: 0.067791
2022-01-06 12:47:49,454 iteration 308 : loss : 0.175603, loss_ce: 0.086122
2022-01-06 12:47:50,590 iteration 309 : loss : 0.302505, loss_ce: 0.138739
2022-01-06 12:47:51,747 iteration 310 : loss : 0.207215, loss_ce: 0.092387
2022-01-06 12:47:52,811 iteration 311 : loss : 0.188593, loss_ce: 0.072884
2022-01-06 12:47:53,916 iteration 312 : loss : 0.152452, loss_ce: 0.074251
2022-01-06 12:47:55,034 iteration 313 : loss : 0.178771, loss_ce: 0.089009
2022-01-06 12:47:56,073 iteration 314 : loss : 0.146547, loss_ce: 0.066488
2022-01-06 12:47:57,167 iteration 315 : loss : 0.170641, loss_ce: 0.066602
2022-01-06 12:47:58,309 iteration 316 : loss : 0.190156, loss_ce: 0.067670
2022-01-06 12:47:59,436 iteration 317 : loss : 0.206172, loss_ce: 0.078392
2022-01-06 12:48:00,591 iteration 318 : loss : 0.175208, loss_ce: 0.068652
2022-01-06 12:48:01,659 iteration 319 : loss : 0.169613, loss_ce: 0.067227
2022-01-06 12:48:02,876 iteration 320 : loss : 0.160902, loss_ce: 0.057085
2022-01-06 12:48:04,030 iteration 321 : loss : 0.179250, loss_ce: 0.060071
2022-01-06 12:48:05,141 iteration 322 : loss : 0.225324, loss_ce: 0.102660
2022-01-06 12:48:06,320 iteration 323 : loss : 0.188730, loss_ce: 0.061940
  5%|█▍                            | 19/400 [06:33<2:06:15, 19.88s/it]2022-01-06 12:48:07,492 iteration 324 : loss : 0.161796, loss_ce: 0.062756
2022-01-06 12:48:08,622 iteration 325 : loss : 0.220883, loss_ce: 0.080284
2022-01-06 12:48:09,789 iteration 326 : loss : 0.138264, loss_ce: 0.060324
2022-01-06 12:48:11,032 iteration 327 : loss : 0.183936, loss_ce: 0.068666
2022-01-06 12:48:12,146 iteration 328 : loss : 0.185712, loss_ce: 0.079459
2022-01-06 12:48:13,312 iteration 329 : loss : 0.219648, loss_ce: 0.111001
2022-01-06 12:48:14,482 iteration 330 : loss : 0.184523, loss_ce: 0.079312
2022-01-06 12:48:15,553 iteration 331 : loss : 0.165777, loss_ce: 0.081066
2022-01-06 12:48:16,655 iteration 332 : loss : 0.146402, loss_ce: 0.053436
2022-01-06 12:48:17,806 iteration 333 : loss : 0.116160, loss_ce: 0.051751
2022-01-06 12:48:18,939 iteration 334 : loss : 0.155385, loss_ce: 0.066737
2022-01-06 12:48:20,037 iteration 335 : loss : 0.204338, loss_ce: 0.072814
2022-01-06 12:48:21,213 iteration 336 : loss : 0.155113, loss_ce: 0.056965
2022-01-06 12:48:22,331 iteration 337 : loss : 0.159646, loss_ce: 0.056368
2022-01-06 12:48:23,399 iteration 338 : loss : 0.206959, loss_ce: 0.108027
2022-01-06 12:48:24,477 iteration 339 : loss : 0.203330, loss_ce: 0.076102
2022-01-06 12:48:24,477 Training Data Eval:
2022-01-06 12:48:29,964   Average segmentation loss on training set: 0.4449
2022-01-06 12:48:29,964 Validation Data Eval:
2022-01-06 12:48:31,850   Average segmentation loss on validation set: 0.5263
2022-01-06 12:48:37,634 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 12:48:38,840 iteration 340 : loss : 0.175138, loss_ce: 0.074547
  5%|█▌                            | 20/400 [07:05<2:29:55, 23.67s/it]2022-01-06 12:48:40,060 iteration 341 : loss : 0.135727, loss_ce: 0.064865
2022-01-06 12:48:41,204 iteration 342 : loss : 0.157871, loss_ce: 0.055524
2022-01-06 12:48:42,293 iteration 343 : loss : 0.172061, loss_ce: 0.070540
2022-01-06 12:48:43,446 iteration 344 : loss : 0.196060, loss_ce: 0.084257
2022-01-06 12:48:44,566 iteration 345 : loss : 0.131659, loss_ce: 0.041464
2022-01-06 12:48:45,764 iteration 346 : loss : 0.189297, loss_ce: 0.064812
2022-01-06 12:48:46,829 iteration 347 : loss : 0.171426, loss_ce: 0.082723
2022-01-06 12:48:47,981 iteration 348 : loss : 0.172337, loss_ce: 0.068150
2022-01-06 12:48:49,251 iteration 349 : loss : 0.177085, loss_ce: 0.092780
2022-01-06 12:48:50,367 iteration 350 : loss : 0.146127, loss_ce: 0.055880
2022-01-06 12:48:51,545 iteration 351 : loss : 0.154659, loss_ce: 0.071819
2022-01-06 12:48:52,705 iteration 352 : loss : 0.131800, loss_ce: 0.057559
2022-01-06 12:48:53,764 iteration 353 : loss : 0.157846, loss_ce: 0.057364
2022-01-06 12:48:54,893 iteration 354 : loss : 0.160015, loss_ce: 0.057718
2022-01-06 12:48:56,075 iteration 355 : loss : 0.174056, loss_ce: 0.074865
2022-01-06 12:48:57,344 iteration 356 : loss : 0.174031, loss_ce: 0.072996
2022-01-06 12:48:58,393 iteration 357 : loss : 0.125222, loss_ce: 0.051706
  5%|█▌                            | 21/400 [07:25<2:21:43, 22.44s/it]2022-01-06 12:48:59,609 iteration 358 : loss : 0.191037, loss_ce: 0.081524
2022-01-06 12:49:00,820 iteration 359 : loss : 0.162500, loss_ce: 0.061046
2022-01-06 12:49:01,880 iteration 360 : loss : 0.140109, loss_ce: 0.052835
2022-01-06 12:49:03,038 iteration 361 : loss : 0.180555, loss_ce: 0.062595
2022-01-06 12:49:04,254 iteration 362 : loss : 0.152668, loss_ce: 0.048876
2022-01-06 12:49:05,468 iteration 363 : loss : 0.173266, loss_ce: 0.062407
2022-01-06 12:49:06,626 iteration 364 : loss : 0.138379, loss_ce: 0.056702
2022-01-06 12:49:07,647 iteration 365 : loss : 0.169601, loss_ce: 0.083103
2022-01-06 12:49:08,783 iteration 366 : loss : 0.167788, loss_ce: 0.080585
2022-01-06 12:49:09,816 iteration 367 : loss : 0.176247, loss_ce: 0.078112
2022-01-06 12:49:10,996 iteration 368 : loss : 0.154292, loss_ce: 0.054723
2022-01-06 12:49:12,205 iteration 369 : loss : 0.193863, loss_ce: 0.063271
2022-01-06 12:49:13,398 iteration 370 : loss : 0.140068, loss_ce: 0.072412
2022-01-06 12:49:14,552 iteration 371 : loss : 0.217517, loss_ce: 0.073231
2022-01-06 12:49:15,715 iteration 372 : loss : 0.208727, loss_ce: 0.107259
2022-01-06 12:49:16,782 iteration 373 : loss : 0.154091, loss_ce: 0.048545
2022-01-06 12:49:17,881 iteration 374 : loss : 0.174139, loss_ce: 0.064056
  6%|█▋                            | 22/400 [07:44<2:15:47, 21.55s/it]2022-01-06 12:49:19,134 iteration 375 : loss : 0.219315, loss_ce: 0.086126
2022-01-06 12:49:20,196 iteration 376 : loss : 0.097084, loss_ce: 0.034138
2022-01-06 12:49:21,306 iteration 377 : loss : 0.109954, loss_ce: 0.034484
2022-01-06 12:49:22,521 iteration 378 : loss : 0.196055, loss_ce: 0.080300
2022-01-06 12:49:23,555 iteration 379 : loss : 0.130322, loss_ce: 0.054394
2022-01-06 12:49:24,712 iteration 380 : loss : 0.147490, loss_ce: 0.058179
2022-01-06 12:49:25,826 iteration 381 : loss : 0.142275, loss_ce: 0.053684
2022-01-06 12:49:26,849 iteration 382 : loss : 0.161853, loss_ce: 0.074789
2022-01-06 12:49:27,964 iteration 383 : loss : 0.204682, loss_ce: 0.090856
2022-01-06 12:49:29,109 iteration 384 : loss : 0.117164, loss_ce: 0.047116
2022-01-06 12:49:30,227 iteration 385 : loss : 0.125300, loss_ce: 0.056933
2022-01-06 12:49:31,367 iteration 386 : loss : 0.169914, loss_ce: 0.056606
2022-01-06 12:49:32,515 iteration 387 : loss : 0.204988, loss_ce: 0.070353
2022-01-06 12:49:33,673 iteration 388 : loss : 0.187247, loss_ce: 0.074396
2022-01-06 12:49:34,828 iteration 389 : loss : 0.251461, loss_ce: 0.110545
2022-01-06 12:49:35,925 iteration 390 : loss : 0.158413, loss_ce: 0.065119
2022-01-06 12:49:37,025 iteration 391 : loss : 0.205107, loss_ce: 0.115394
  6%|█▋                            | 23/400 [08:03<2:10:51, 20.83s/it]2022-01-06 12:49:38,246 iteration 392 : loss : 0.156540, loss_ce: 0.073316
2022-01-06 12:49:39,328 iteration 393 : loss : 0.134601, loss_ce: 0.052548
2022-01-06 12:49:40,504 iteration 394 : loss : 0.185962, loss_ce: 0.079596
2022-01-06 12:49:41,649 iteration 395 : loss : 0.204965, loss_ce: 0.090232
2022-01-06 12:49:42,737 iteration 396 : loss : 0.179273, loss_ce: 0.068235
2022-01-06 12:49:43,855 iteration 397 : loss : 0.175155, loss_ce: 0.080902
2022-01-06 12:49:44,991 iteration 398 : loss : 0.154191, loss_ce: 0.068221
2022-01-06 12:49:45,999 iteration 399 : loss : 0.196388, loss_ce: 0.085218
2022-01-06 12:49:47,139 iteration 400 : loss : 0.199033, loss_ce: 0.076290
2022-01-06 12:49:48,333 iteration 401 : loss : 0.175035, loss_ce: 0.073377
2022-01-06 12:49:49,436 iteration 402 : loss : 0.130072, loss_ce: 0.053785
2022-01-06 12:49:50,530 iteration 403 : loss : 0.118331, loss_ce: 0.044019
2022-01-06 12:49:51,670 iteration 404 : loss : 0.200302, loss_ce: 0.089297
2022-01-06 12:49:52,847 iteration 405 : loss : 0.180526, loss_ce: 0.071635
2022-01-06 12:49:53,947 iteration 406 : loss : 0.142754, loss_ce: 0.054275
2022-01-06 12:49:54,998 iteration 407 : loss : 0.164574, loss_ce: 0.060678
2022-01-06 12:49:56,102 iteration 408 : loss : 0.197516, loss_ce: 0.073067
  6%|█▊                            | 24/400 [08:23<2:07:15, 20.31s/it]2022-01-06 12:49:57,315 iteration 409 : loss : 0.169099, loss_ce: 0.080034
2022-01-06 12:49:58,468 iteration 410 : loss : 0.149751, loss_ce: 0.053350
2022-01-06 12:49:59,575 iteration 411 : loss : 0.260473, loss_ce: 0.072659
2022-01-06 12:50:00,592 iteration 412 : loss : 0.148560, loss_ce: 0.052142
2022-01-06 12:50:01,699 iteration 413 : loss : 0.149820, loss_ce: 0.052422
2022-01-06 12:50:02,765 iteration 414 : loss : 0.201542, loss_ce: 0.110341
2022-01-06 12:50:03,793 iteration 415 : loss : 0.153421, loss_ce: 0.059489
2022-01-06 12:50:04,917 iteration 416 : loss : 0.212112, loss_ce: 0.107178
2022-01-06 12:50:06,008 iteration 417 : loss : 0.293233, loss_ce: 0.127409
2022-01-06 12:50:07,210 iteration 418 : loss : 0.166834, loss_ce: 0.063882
2022-01-06 12:50:08,309 iteration 419 : loss : 0.281131, loss_ce: 0.150287
2022-01-06 12:50:09,439 iteration 420 : loss : 0.227226, loss_ce: 0.094635
2022-01-06 12:50:10,504 iteration 421 : loss : 0.236890, loss_ce: 0.099898
2022-01-06 12:50:11,568 iteration 422 : loss : 0.229940, loss_ce: 0.099349
2022-01-06 12:50:12,639 iteration 423 : loss : 0.193807, loss_ce: 0.069040
2022-01-06 12:50:13,776 iteration 424 : loss : 0.205262, loss_ce: 0.080363
2022-01-06 12:50:13,805 Training Data Eval:
2022-01-06 12:50:19,324   Average segmentation loss on training set: 5.8409
2022-01-06 12:50:19,324 Validation Data Eval:
2022-01-06 12:50:21,244   Average segmentation loss on validation set: 5.6874
2022-01-06 12:50:22,363 iteration 425 : loss : 0.244720, loss_ce: 0.096412
  6%|█▉                            | 25/400 [08:49<2:18:04, 22.09s/it]2022-01-06 12:50:23,508 iteration 426 : loss : 0.192936, loss_ce: 0.070237
2022-01-06 12:50:24,684 iteration 427 : loss : 0.152728, loss_ce: 0.056615
2022-01-06 12:50:25,816 iteration 428 : loss : 0.225467, loss_ce: 0.094563
2022-01-06 12:50:26,894 iteration 429 : loss : 0.247712, loss_ce: 0.125344
2022-01-06 12:50:27,955 iteration 430 : loss : 0.178891, loss_ce: 0.066793
2022-01-06 12:50:29,049 iteration 431 : loss : 0.264710, loss_ce: 0.110929
2022-01-06 12:50:30,212 iteration 432 : loss : 0.140096, loss_ce: 0.056096
2022-01-06 12:50:31,413 iteration 433 : loss : 0.147563, loss_ce: 0.060861
2022-01-06 12:50:32,545 iteration 434 : loss : 0.151201, loss_ce: 0.062165
2022-01-06 12:50:33,613 iteration 435 : loss : 0.171034, loss_ce: 0.078750
2022-01-06 12:50:34,708 iteration 436 : loss : 0.142381, loss_ce: 0.064730
2022-01-06 12:50:35,810 iteration 437 : loss : 0.256766, loss_ce: 0.137283
2022-01-06 12:50:36,881 iteration 438 : loss : 0.216973, loss_ce: 0.077877
2022-01-06 12:50:38,033 iteration 439 : loss : 0.218997, loss_ce: 0.075271
2022-01-06 12:50:39,156 iteration 440 : loss : 0.196669, loss_ce: 0.075095
2022-01-06 12:50:40,353 iteration 441 : loss : 0.183928, loss_ce: 0.071111
2022-01-06 12:50:41,524 iteration 442 : loss : 0.191115, loss_ce: 0.091811
  6%|█▉                            | 26/400 [09:08<2:12:13, 21.21s/it]2022-01-06 12:50:42,698 iteration 443 : loss : 0.175527, loss_ce: 0.085714
2022-01-06 12:50:43,779 iteration 444 : loss : 0.186986, loss_ce: 0.069650
2022-01-06 12:50:44,903 iteration 445 : loss : 0.220974, loss_ce: 0.074403
2022-01-06 12:50:46,167 iteration 446 : loss : 0.173453, loss_ce: 0.071967
2022-01-06 12:50:47,362 iteration 447 : loss : 0.135142, loss_ce: 0.071509
2022-01-06 12:50:48,543 iteration 448 : loss : 0.253047, loss_ce: 0.078787
2022-01-06 12:50:49,673 iteration 449 : loss : 0.189791, loss_ce: 0.097775
2022-01-06 12:50:50,792 iteration 450 : loss : 0.195319, loss_ce: 0.076286
2022-01-06 12:50:52,004 iteration 451 : loss : 0.125528, loss_ce: 0.055162
2022-01-06 12:50:53,168 iteration 452 : loss : 0.143777, loss_ce: 0.062820
2022-01-06 12:50:54,325 iteration 453 : loss : 0.220886, loss_ce: 0.095757
2022-01-06 12:50:55,487 iteration 454 : loss : 0.201330, loss_ce: 0.071351
2022-01-06 12:50:56,676 iteration 455 : loss : 0.161545, loss_ce: 0.062948
2022-01-06 12:50:57,821 iteration 456 : loss : 0.175021, loss_ce: 0.056046
2022-01-06 12:50:58,960 iteration 457 : loss : 0.188598, loss_ce: 0.070552
2022-01-06 12:51:00,173 iteration 458 : loss : 0.233021, loss_ce: 0.110551
2022-01-06 12:51:01,227 iteration 459 : loss : 0.144878, loss_ce: 0.052379
  7%|██                            | 27/400 [09:28<2:09:02, 20.76s/it]2022-01-06 12:51:02,422 iteration 460 : loss : 0.174589, loss_ce: 0.072475
2022-01-06 12:51:03,562 iteration 461 : loss : 0.199512, loss_ce: 0.090521
2022-01-06 12:51:04,664 iteration 462 : loss : 0.172265, loss_ce: 0.066824
2022-01-06 12:51:05,853 iteration 463 : loss : 0.182138, loss_ce: 0.081756
2022-01-06 12:51:06,989 iteration 464 : loss : 0.150186, loss_ce: 0.057444
2022-01-06 12:51:08,067 iteration 465 : loss : 0.183413, loss_ce: 0.062459
2022-01-06 12:51:09,180 iteration 466 : loss : 0.176447, loss_ce: 0.067532
2022-01-06 12:51:10,284 iteration 467 : loss : 0.165275, loss_ce: 0.060770
2022-01-06 12:51:11,518 iteration 468 : loss : 0.181976, loss_ce: 0.080934
2022-01-06 12:51:12,620 iteration 469 : loss : 0.188067, loss_ce: 0.061606
2022-01-06 12:51:13,748 iteration 470 : loss : 0.204123, loss_ce: 0.072098
2022-01-06 12:51:14,872 iteration 471 : loss : 0.215650, loss_ce: 0.077911
2022-01-06 12:51:16,051 iteration 472 : loss : 0.189873, loss_ce: 0.076293
2022-01-06 12:51:17,333 iteration 473 : loss : 0.213840, loss_ce: 0.086850
2022-01-06 12:51:18,539 iteration 474 : loss : 0.229002, loss_ce: 0.099276
2022-01-06 12:51:19,631 iteration 475 : loss : 0.126692, loss_ce: 0.051008
2022-01-06 12:51:20,821 iteration 476 : loss : 0.211276, loss_ce: 0.113658
  7%|██                            | 28/400 [09:47<2:06:32, 20.41s/it]2022-01-06 12:51:21,999 iteration 477 : loss : 0.175032, loss_ce: 0.062258
2022-01-06 12:51:23,163 iteration 478 : loss : 0.140637, loss_ce: 0.049838
2022-01-06 12:51:24,283 iteration 479 : loss : 0.152085, loss_ce: 0.066558
2022-01-06 12:51:25,433 iteration 480 : loss : 0.174802, loss_ce: 0.079305
2022-01-06 12:51:26,474 iteration 481 : loss : 0.170406, loss_ce: 0.065445
2022-01-06 12:51:27,628 iteration 482 : loss : 0.169761, loss_ce: 0.048341
2022-01-06 12:51:28,792 iteration 483 : loss : 0.189973, loss_ce: 0.064866
2022-01-06 12:51:29,976 iteration 484 : loss : 0.228228, loss_ce: 0.103081
2022-01-06 12:51:31,087 iteration 485 : loss : 0.152675, loss_ce: 0.066744
2022-01-06 12:51:32,198 iteration 486 : loss : 0.178831, loss_ce: 0.056805
2022-01-06 12:51:33,386 iteration 487 : loss : 0.133663, loss_ce: 0.054473
2022-01-06 12:51:34,458 iteration 488 : loss : 0.137290, loss_ce: 0.054838
2022-01-06 12:51:35,587 iteration 489 : loss : 0.146133, loss_ce: 0.063358
2022-01-06 12:51:36,606 iteration 490 : loss : 0.187419, loss_ce: 0.079371
2022-01-06 12:51:37,750 iteration 491 : loss : 0.178071, loss_ce: 0.082238
2022-01-06 12:51:38,876 iteration 492 : loss : 0.158820, loss_ce: 0.077403
2022-01-06 12:51:40,043 iteration 493 : loss : 0.168411, loss_ce: 0.078735
  7%|██▏                           | 29/400 [10:06<2:03:59, 20.05s/it]2022-01-06 12:51:41,228 iteration 494 : loss : 0.165212, loss_ce: 0.078626
2022-01-06 12:51:42,404 iteration 495 : loss : 0.216720, loss_ce: 0.078783
2022-01-06 12:51:43,528 iteration 496 : loss : 0.207807, loss_ce: 0.097654
2022-01-06 12:51:44,682 iteration 497 : loss : 0.188906, loss_ce: 0.084500
2022-01-06 12:51:45,873 iteration 498 : loss : 0.157773, loss_ce: 0.073926
2022-01-06 12:51:46,929 iteration 499 : loss : 0.141831, loss_ce: 0.059743
2022-01-06 12:51:48,002 iteration 500 : loss : 0.157271, loss_ce: 0.061762
2022-01-06 12:51:49,092 iteration 501 : loss : 0.173895, loss_ce: 0.074120
2022-01-06 12:51:50,293 iteration 502 : loss : 0.158698, loss_ce: 0.064822
2022-01-06 12:51:51,393 iteration 503 : loss : 0.183406, loss_ce: 0.060997
2022-01-06 12:51:52,552 iteration 504 : loss : 0.177976, loss_ce: 0.066688
2022-01-06 12:51:53,638 iteration 505 : loss : 0.119855, loss_ce: 0.047128
2022-01-06 12:51:54,871 iteration 506 : loss : 0.127755, loss_ce: 0.050287
2022-01-06 12:51:56,012 iteration 507 : loss : 0.216816, loss_ce: 0.086381
2022-01-06 12:51:57,145 iteration 508 : loss : 0.228259, loss_ce: 0.098477
2022-01-06 12:51:58,329 iteration 509 : loss : 0.135263, loss_ce: 0.044609
2022-01-06 12:51:58,329 Training Data Eval:
2022-01-06 12:52:03,848   Average segmentation loss on training set: 14.9912
2022-01-06 12:52:03,848 Validation Data Eval:
2022-01-06 12:52:05,745   Average segmentation loss on validation set: 14.4028
2022-01-06 12:52:06,926 iteration 510 : loss : 0.167632, loss_ce: 0.071903
  8%|██▎                           | 30/400 [10:33<2:16:16, 22.10s/it]2022-01-06 12:52:08,145 iteration 511 : loss : 0.210313, loss_ce: 0.101896
2022-01-06 12:52:09,240 iteration 512 : loss : 0.179465, loss_ce: 0.085748
2022-01-06 12:52:10,354 iteration 513 : loss : 0.145412, loss_ce: 0.044287
2022-01-06 12:52:11,490 iteration 514 : loss : 0.162687, loss_ce: 0.066810
2022-01-06 12:52:12,600 iteration 515 : loss : 0.204894, loss_ce: 0.081970
2022-01-06 12:52:13,758 iteration 516 : loss : 0.158459, loss_ce: 0.050442
2022-01-06 12:52:14,945 iteration 517 : loss : 0.192077, loss_ce: 0.074238
2022-01-06 12:52:16,010 iteration 518 : loss : 0.209488, loss_ce: 0.073909
2022-01-06 12:52:17,136 iteration 519 : loss : 0.220317, loss_ce: 0.099927
2022-01-06 12:52:18,194 iteration 520 : loss : 0.182102, loss_ce: 0.077651
2022-01-06 12:52:19,312 iteration 521 : loss : 0.176682, loss_ce: 0.060377
2022-01-06 12:52:20,481 iteration 522 : loss : 0.143392, loss_ce: 0.055492
2022-01-06 12:52:21,689 iteration 523 : loss : 0.162792, loss_ce: 0.069895
2022-01-06 12:52:22,919 iteration 524 : loss : 0.154378, loss_ce: 0.062507
2022-01-06 12:52:24,032 iteration 525 : loss : 0.198496, loss_ce: 0.089726
2022-01-06 12:52:25,147 iteration 526 : loss : 0.166544, loss_ce: 0.073268
2022-01-06 12:52:26,274 iteration 527 : loss : 0.184627, loss_ce: 0.063345
  8%|██▎                           | 31/400 [10:53<2:10:50, 21.28s/it]2022-01-06 12:52:27,441 iteration 528 : loss : 0.152464, loss_ce: 0.062844
2022-01-06 12:52:28,587 iteration 529 : loss : 0.199828, loss_ce: 0.065632
2022-01-06 12:52:29,675 iteration 530 : loss : 0.162970, loss_ce: 0.065031
2022-01-06 12:52:30,747 iteration 531 : loss : 0.176244, loss_ce: 0.066569
2022-01-06 12:52:31,834 iteration 532 : loss : 0.181671, loss_ce: 0.085599
2022-01-06 12:52:33,063 iteration 533 : loss : 0.146846, loss_ce: 0.058083
2022-01-06 12:52:34,133 iteration 534 : loss : 0.132196, loss_ce: 0.060721
2022-01-06 12:52:35,307 iteration 535 : loss : 0.157667, loss_ce: 0.066080
2022-01-06 12:52:36,516 iteration 536 : loss : 0.219388, loss_ce: 0.104391
2022-01-06 12:52:37,735 iteration 537 : loss : 0.186005, loss_ce: 0.063885
2022-01-06 12:52:38,877 iteration 538 : loss : 0.162455, loss_ce: 0.076603
2022-01-06 12:52:39,964 iteration 539 : loss : 0.144164, loss_ce: 0.059716
2022-01-06 12:52:41,090 iteration 540 : loss : 0.194080, loss_ce: 0.077587
2022-01-06 12:52:42,166 iteration 541 : loss : 0.144327, loss_ce: 0.066702
2022-01-06 12:52:43,288 iteration 542 : loss : 0.201161, loss_ce: 0.094063
2022-01-06 12:52:44,345 iteration 543 : loss : 0.121001, loss_ce: 0.052220
2022-01-06 12:52:45,442 iteration 544 : loss : 0.154177, loss_ce: 0.055925
  8%|██▍                           | 32/400 [11:12<2:06:36, 20.64s/it]2022-01-06 12:52:46,642 iteration 545 : loss : 0.123451, loss_ce: 0.058140
2022-01-06 12:52:47,758 iteration 546 : loss : 0.127777, loss_ce: 0.048705
2022-01-06 12:52:48,878 iteration 547 : loss : 0.148985, loss_ce: 0.061891
2022-01-06 12:52:49,980 iteration 548 : loss : 0.224133, loss_ce: 0.080631
2022-01-06 12:52:51,119 iteration 549 : loss : 0.171654, loss_ce: 0.071527
2022-01-06 12:52:52,200 iteration 550 : loss : 0.138677, loss_ce: 0.056360
2022-01-06 12:52:53,308 iteration 551 : loss : 0.154110, loss_ce: 0.066236
2022-01-06 12:52:54,395 iteration 552 : loss : 0.161429, loss_ce: 0.067091
2022-01-06 12:52:55,554 iteration 553 : loss : 0.153000, loss_ce: 0.058214
2022-01-06 12:52:56,612 iteration 554 : loss : 0.155785, loss_ce: 0.074171
2022-01-06 12:52:57,730 iteration 555 : loss : 0.164463, loss_ce: 0.049511
2022-01-06 12:52:58,899 iteration 556 : loss : 0.153125, loss_ce: 0.066697
2022-01-06 12:52:59,935 iteration 557 : loss : 0.235501, loss_ce: 0.086708
2022-01-06 12:53:01,100 iteration 558 : loss : 0.178460, loss_ce: 0.057237
2022-01-06 12:53:02,218 iteration 559 : loss : 0.152073, loss_ce: 0.062783
2022-01-06 12:53:03,354 iteration 560 : loss : 0.180114, loss_ce: 0.050479
2022-01-06 12:53:04,518 iteration 561 : loss : 0.206571, loss_ce: 0.096917
  8%|██▍                           | 33/400 [11:31<2:03:24, 20.18s/it]2022-01-06 12:53:05,816 iteration 562 : loss : 0.187358, loss_ce: 0.088303
2022-01-06 12:53:06,838 iteration 563 : loss : 0.153045, loss_ce: 0.066591
2022-01-06 12:53:07,980 iteration 564 : loss : 0.181101, loss_ce: 0.084032
2022-01-06 12:53:09,119 iteration 565 : loss : 0.188870, loss_ce: 0.080162
2022-01-06 12:53:10,290 iteration 566 : loss : 0.168797, loss_ce: 0.074350
2022-01-06 12:53:11,379 iteration 567 : loss : 0.169471, loss_ce: 0.065950
2022-01-06 12:53:12,460 iteration 568 : loss : 0.236174, loss_ce: 0.083058
2022-01-06 12:53:13,566 iteration 569 : loss : 0.220360, loss_ce: 0.082290
2022-01-06 12:53:14,716 iteration 570 : loss : 0.179076, loss_ce: 0.068711
2022-01-06 12:53:15,813 iteration 571 : loss : 0.243512, loss_ce: 0.086076
2022-01-06 12:53:16,946 iteration 572 : loss : 0.176277, loss_ce: 0.063865
2022-01-06 12:53:18,019 iteration 573 : loss : 0.179936, loss_ce: 0.071317
2022-01-06 12:53:19,131 iteration 574 : loss : 0.192539, loss_ce: 0.073775
2022-01-06 12:53:20,328 iteration 575 : loss : 0.175846, loss_ce: 0.058523
2022-01-06 12:53:21,487 iteration 576 : loss : 0.162568, loss_ce: 0.065966
2022-01-06 12:53:22,602 iteration 577 : loss : 0.147460, loss_ce: 0.055754
2022-01-06 12:53:23,675 iteration 578 : loss : 0.197951, loss_ce: 0.075710
  8%|██▌                           | 34/400 [11:50<2:01:11, 19.87s/it]2022-01-06 12:53:25,042 iteration 579 : loss : 0.192148, loss_ce: 0.075406
2022-01-06 12:53:26,180 iteration 580 : loss : 0.201823, loss_ce: 0.079359
2022-01-06 12:53:27,353 iteration 581 : loss : 0.178050, loss_ce: 0.060778
2022-01-06 12:53:28,465 iteration 582 : loss : 0.154359, loss_ce: 0.059570
2022-01-06 12:53:29,586 iteration 583 : loss : 0.256838, loss_ce: 0.083514
2022-01-06 12:53:30,668 iteration 584 : loss : 0.137929, loss_ce: 0.057959
2022-01-06 12:53:31,838 iteration 585 : loss : 0.136532, loss_ce: 0.050615
2022-01-06 12:53:32,920 iteration 586 : loss : 0.185387, loss_ce: 0.079062
2022-01-06 12:53:33,997 iteration 587 : loss : 0.148384, loss_ce: 0.061767
2022-01-06 12:53:35,143 iteration 588 : loss : 0.157324, loss_ce: 0.054013
2022-01-06 12:53:36,251 iteration 589 : loss : 0.124246, loss_ce: 0.043771
2022-01-06 12:53:37,369 iteration 590 : loss : 0.186497, loss_ce: 0.071166
2022-01-06 12:53:38,489 iteration 591 : loss : 0.156333, loss_ce: 0.062130
2022-01-06 12:53:39,608 iteration 592 : loss : 0.150737, loss_ce: 0.061022
2022-01-06 12:53:40,763 iteration 593 : loss : 0.116808, loss_ce: 0.040069
2022-01-06 12:53:41,899 iteration 594 : loss : 0.140012, loss_ce: 0.057694
2022-01-06 12:53:41,900 Training Data Eval:
2022-01-06 12:53:47,387   Average segmentation loss on training set: 0.7601
2022-01-06 12:53:47,387 Validation Data Eval:
2022-01-06 12:53:49,284   Average segmentation loss on validation set: 0.7550
2022-01-06 12:53:50,483 iteration 595 : loss : 0.230504, loss_ce: 0.117939
  9%|██▋                           | 35/400 [12:17<2:13:31, 21.95s/it]2022-01-06 12:53:51,670 iteration 596 : loss : 0.154392, loss_ce: 0.065458
2022-01-06 12:53:52,755 iteration 597 : loss : 0.135643, loss_ce: 0.044300
2022-01-06 12:53:53,831 iteration 598 : loss : 0.171605, loss_ce: 0.070481
2022-01-06 12:53:54,935 iteration 599 : loss : 0.198263, loss_ce: 0.070911
2022-01-06 12:53:56,028 iteration 600 : loss : 0.159604, loss_ce: 0.067587
2022-01-06 12:53:57,149 iteration 601 : loss : 0.160565, loss_ce: 0.060485
2022-01-06 12:53:58,302 iteration 602 : loss : 0.116665, loss_ce: 0.047558
2022-01-06 12:53:59,349 iteration 603 : loss : 0.149883, loss_ce: 0.052794
2022-01-06 12:54:00,411 iteration 604 : loss : 0.107218, loss_ce: 0.039886
2022-01-06 12:54:01,470 iteration 605 : loss : 0.132833, loss_ce: 0.046824
2022-01-06 12:54:02,739 iteration 606 : loss : 0.124863, loss_ce: 0.056799
2022-01-06 12:54:03,849 iteration 607 : loss : 0.171839, loss_ce: 0.068924
2022-01-06 12:54:04,968 iteration 608 : loss : 0.141612, loss_ce: 0.063344
2022-01-06 12:54:06,005 iteration 609 : loss : 0.129598, loss_ce: 0.051780
2022-01-06 12:54:07,169 iteration 610 : loss : 0.112173, loss_ce: 0.047887
2022-01-06 12:54:08,423 iteration 611 : loss : 0.182184, loss_ce: 0.079640
2022-01-06 12:54:09,561 iteration 612 : loss : 0.141461, loss_ce: 0.050688
  9%|██▋                           | 36/400 [12:36<2:07:57, 21.09s/it]2022-01-06 12:54:10,774 iteration 613 : loss : 0.148276, loss_ce: 0.054451
2022-01-06 12:54:11,846 iteration 614 : loss : 0.149472, loss_ce: 0.048276
2022-01-06 12:54:12,907 iteration 615 : loss : 0.208657, loss_ce: 0.095755
2022-01-06 12:54:14,100 iteration 616 : loss : 0.183767, loss_ce: 0.083282
2022-01-06 12:54:15,173 iteration 617 : loss : 0.148335, loss_ce: 0.064344
2022-01-06 12:54:16,298 iteration 618 : loss : 0.166949, loss_ce: 0.045125
2022-01-06 12:54:17,426 iteration 619 : loss : 0.268829, loss_ce: 0.131463
2022-01-06 12:54:18,658 iteration 620 : loss : 0.207972, loss_ce: 0.059714
2022-01-06 12:54:19,785 iteration 621 : loss : 0.128091, loss_ce: 0.047526
2022-01-06 12:54:21,040 iteration 622 : loss : 0.133784, loss_ce: 0.047537
2022-01-06 12:54:22,266 iteration 623 : loss : 0.194379, loss_ce: 0.100876
2022-01-06 12:54:23,371 iteration 624 : loss : 0.147759, loss_ce: 0.044473
2022-01-06 12:54:24,433 iteration 625 : loss : 0.145130, loss_ce: 0.059633
2022-01-06 12:54:25,600 iteration 626 : loss : 0.149586, loss_ce: 0.063145
2022-01-06 12:54:26,780 iteration 627 : loss : 0.172206, loss_ce: 0.060253
2022-01-06 12:54:28,005 iteration 628 : loss : 0.191762, loss_ce: 0.073008
2022-01-06 12:54:29,114 iteration 629 : loss : 0.130350, loss_ce: 0.052580
  9%|██▊                           | 37/400 [12:56<2:04:48, 20.63s/it]2022-01-06 12:54:30,336 iteration 630 : loss : 0.137695, loss_ce: 0.055997
2022-01-06 12:54:31,429 iteration 631 : loss : 0.123385, loss_ce: 0.055381
2022-01-06 12:54:32,517 iteration 632 : loss : 0.182090, loss_ce: 0.068516
2022-01-06 12:54:33,704 iteration 633 : loss : 0.126625, loss_ce: 0.053005
2022-01-06 12:54:34,822 iteration 634 : loss : 0.218932, loss_ce: 0.076856
2022-01-06 12:54:36,061 iteration 635 : loss : 0.141490, loss_ce: 0.057780
2022-01-06 12:54:37,211 iteration 636 : loss : 0.155304, loss_ce: 0.063134
2022-01-06 12:54:38,262 iteration 637 : loss : 0.158010, loss_ce: 0.069094
2022-01-06 12:54:39,460 iteration 638 : loss : 0.177496, loss_ce: 0.054762
2022-01-06 12:54:40,535 iteration 639 : loss : 0.154177, loss_ce: 0.061707
2022-01-06 12:54:41,587 iteration 640 : loss : 0.178940, loss_ce: 0.053799
2022-01-06 12:54:42,636 iteration 641 : loss : 0.139622, loss_ce: 0.052242
2022-01-06 12:54:43,695 iteration 642 : loss : 0.175778, loss_ce: 0.050260
2022-01-06 12:54:44,788 iteration 643 : loss : 0.165031, loss_ce: 0.063866
2022-01-06 12:54:45,890 iteration 644 : loss : 0.136664, loss_ce: 0.052299
2022-01-06 12:54:46,959 iteration 645 : loss : 0.116010, loss_ce: 0.037023
2022-01-06 12:54:48,137 iteration 646 : loss : 0.123341, loss_ce: 0.049565
 10%|██▊                           | 38/400 [13:15<2:01:32, 20.15s/it]2022-01-06 12:54:49,284 iteration 647 : loss : 0.166919, loss_ce: 0.068837
2022-01-06 12:54:50,397 iteration 648 : loss : 0.124854, loss_ce: 0.052652
2022-01-06 12:54:51,547 iteration 649 : loss : 0.150363, loss_ce: 0.063288
2022-01-06 12:54:52,740 iteration 650 : loss : 0.150523, loss_ce: 0.057529
2022-01-06 12:54:53,850 iteration 651 : loss : 0.099853, loss_ce: 0.042852
2022-01-06 12:54:54,862 iteration 652 : loss : 0.096019, loss_ce: 0.033729
2022-01-06 12:54:56,053 iteration 653 : loss : 0.161864, loss_ce: 0.061626
2022-01-06 12:54:57,110 iteration 654 : loss : 0.175423, loss_ce: 0.064139
2022-01-06 12:54:58,219 iteration 655 : loss : 0.187483, loss_ce: 0.060347
2022-01-06 12:54:59,345 iteration 656 : loss : 0.149150, loss_ce: 0.053936
2022-01-06 12:55:00,548 iteration 657 : loss : 0.102179, loss_ce: 0.038395
2022-01-06 12:55:01,697 iteration 658 : loss : 0.170453, loss_ce: 0.055834
2022-01-06 12:55:02,848 iteration 659 : loss : 0.129982, loss_ce: 0.053004
2022-01-06 12:55:03,926 iteration 660 : loss : 0.147682, loss_ce: 0.065320
2022-01-06 12:55:05,076 iteration 661 : loss : 0.143912, loss_ce: 0.053036
2022-01-06 12:55:06,305 iteration 662 : loss : 0.193781, loss_ce: 0.046835
2022-01-06 12:55:07,426 iteration 663 : loss : 0.122891, loss_ce: 0.055277
 10%|██▉                           | 39/400 [13:34<1:59:39, 19.89s/it]2022-01-06 12:55:08,633 iteration 664 : loss : 0.138222, loss_ce: 0.060502
2022-01-06 12:55:09,666 iteration 665 : loss : 0.146374, loss_ce: 0.066509
2022-01-06 12:55:10,761 iteration 666 : loss : 0.131260, loss_ce: 0.050791
2022-01-06 12:55:11,899 iteration 667 : loss : 0.111676, loss_ce: 0.045267
2022-01-06 12:55:13,011 iteration 668 : loss : 0.184106, loss_ce: 0.069596
2022-01-06 12:55:14,084 iteration 669 : loss : 0.128230, loss_ce: 0.034696
2022-01-06 12:55:15,329 iteration 670 : loss : 0.171229, loss_ce: 0.067396
2022-01-06 12:55:16,460 iteration 671 : loss : 0.180418, loss_ce: 0.061340
2022-01-06 12:55:17,664 iteration 672 : loss : 0.115213, loss_ce: 0.037812
2022-01-06 12:55:18,826 iteration 673 : loss : 0.140284, loss_ce: 0.056433
2022-01-06 12:55:19,959 iteration 674 : loss : 0.125131, loss_ce: 0.054244
2022-01-06 12:55:21,154 iteration 675 : loss : 0.208001, loss_ce: 0.083880
2022-01-06 12:55:22,266 iteration 676 : loss : 0.166424, loss_ce: 0.060770
2022-01-06 12:55:23,404 iteration 677 : loss : 0.120016, loss_ce: 0.055702
2022-01-06 12:55:24,560 iteration 678 : loss : 0.172816, loss_ce: 0.069688
2022-01-06 12:55:25,693 iteration 679 : loss : 0.166438, loss_ce: 0.075458
2022-01-06 12:55:25,693 Training Data Eval:
2022-01-06 12:55:31,187   Average segmentation loss on training set: 4.1961
2022-01-06 12:55:31,188 Validation Data Eval:
2022-01-06 12:55:33,084   Average segmentation loss on validation set: 4.1054
2022-01-06 12:55:34,214 iteration 680 : loss : 0.187339, loss_ce: 0.073288
 10%|███                           | 40/400 [14:01<2:11:45, 21.96s/it]2022-01-06 12:55:35,420 iteration 681 : loss : 0.136866, loss_ce: 0.052241
2022-01-06 12:55:36,466 iteration 682 : loss : 0.176033, loss_ce: 0.052432
2022-01-06 12:55:37,590 iteration 683 : loss : 0.111938, loss_ce: 0.048578
2022-01-06 12:55:38,666 iteration 684 : loss : 0.119158, loss_ce: 0.041784
2022-01-06 12:55:39,858 iteration 685 : loss : 0.138989, loss_ce: 0.048209
2022-01-06 12:55:40,984 iteration 686 : loss : 0.182768, loss_ce: 0.063351
2022-01-06 12:55:42,103 iteration 687 : loss : 0.149972, loss_ce: 0.059694
2022-01-06 12:55:43,197 iteration 688 : loss : 0.106612, loss_ce: 0.040315
2022-01-06 12:55:44,344 iteration 689 : loss : 0.136511, loss_ce: 0.043809
2022-01-06 12:55:45,451 iteration 690 : loss : 0.107981, loss_ce: 0.037640
2022-01-06 12:55:46,578 iteration 691 : loss : 0.148589, loss_ce: 0.056474
2022-01-06 12:55:47,757 iteration 692 : loss : 0.132422, loss_ce: 0.043867
2022-01-06 12:55:48,806 iteration 693 : loss : 0.128676, loss_ce: 0.056398
2022-01-06 12:55:49,933 iteration 694 : loss : 0.131502, loss_ce: 0.052369
2022-01-06 12:55:51,007 iteration 695 : loss : 0.156271, loss_ce: 0.084136
2022-01-06 12:55:52,192 iteration 696 : loss : 0.125459, loss_ce: 0.060221
2022-01-06 12:55:53,310 iteration 697 : loss : 0.147385, loss_ce: 0.059913
 10%|███                           | 41/400 [14:20<2:06:15, 21.10s/it]2022-01-06 12:55:54,455 iteration 698 : loss : 0.159886, loss_ce: 0.047702
2022-01-06 12:55:55,615 iteration 699 : loss : 0.156554, loss_ce: 0.065574
2022-01-06 12:55:56,705 iteration 700 : loss : 0.127144, loss_ce: 0.054174
2022-01-06 12:55:57,798 iteration 701 : loss : 0.160017, loss_ce: 0.072653
2022-01-06 12:55:58,945 iteration 702 : loss : 0.170707, loss_ce: 0.060439
2022-01-06 12:56:00,150 iteration 703 : loss : 0.093025, loss_ce: 0.037076
2022-01-06 12:56:01,208 iteration 704 : loss : 0.121147, loss_ce: 0.044289
2022-01-06 12:56:02,326 iteration 705 : loss : 0.134319, loss_ce: 0.048296
2022-01-06 12:56:03,470 iteration 706 : loss : 0.093471, loss_ce: 0.032390
2022-01-06 12:56:04,535 iteration 707 : loss : 0.120565, loss_ce: 0.047616
2022-01-06 12:56:05,649 iteration 708 : loss : 0.118667, loss_ce: 0.040549
2022-01-06 12:56:06,765 iteration 709 : loss : 0.153288, loss_ce: 0.059743
2022-01-06 12:56:07,919 iteration 710 : loss : 0.133619, loss_ce: 0.055729
2022-01-06 12:56:09,047 iteration 711 : loss : 0.136585, loss_ce: 0.064026
2022-01-06 12:56:10,262 iteration 712 : loss : 0.124124, loss_ce: 0.045001
2022-01-06 12:56:11,382 iteration 713 : loss : 0.108829, loss_ce: 0.050099
2022-01-06 12:56:12,593 iteration 714 : loss : 0.138078, loss_ce: 0.054772
 10%|███▏                          | 42/400 [14:39<2:02:39, 20.56s/it]2022-01-06 12:56:13,833 iteration 715 : loss : 0.147141, loss_ce: 0.052128
2022-01-06 12:56:15,011 iteration 716 : loss : 0.122650, loss_ce: 0.058047
2022-01-06 12:56:16,140 iteration 717 : loss : 0.179741, loss_ce: 0.064246
2022-01-06 12:56:17,284 iteration 718 : loss : 0.151140, loss_ce: 0.064006
2022-01-06 12:56:18,488 iteration 719 : loss : 0.116354, loss_ce: 0.042336
2022-01-06 12:56:19,613 iteration 720 : loss : 0.190178, loss_ce: 0.078006
2022-01-06 12:56:20,742 iteration 721 : loss : 0.123791, loss_ce: 0.042316
2022-01-06 12:56:21,858 iteration 722 : loss : 0.118188, loss_ce: 0.041456
2022-01-06 12:56:23,031 iteration 723 : loss : 0.149806, loss_ce: 0.063846
2022-01-06 12:56:24,135 iteration 724 : loss : 0.168477, loss_ce: 0.066005
2022-01-06 12:56:25,332 iteration 725 : loss : 0.123530, loss_ce: 0.046464
2022-01-06 12:56:26,455 iteration 726 : loss : 0.156480, loss_ce: 0.055934
2022-01-06 12:56:27,506 iteration 727 : loss : 0.111382, loss_ce: 0.038828
2022-01-06 12:56:28,671 iteration 728 : loss : 0.120367, loss_ce: 0.046816
2022-01-06 12:56:29,854 iteration 729 : loss : 0.093357, loss_ce: 0.040337
2022-01-06 12:56:30,994 iteration 730 : loss : 0.114004, loss_ce: 0.045324
2022-01-06 12:56:32,029 iteration 731 : loss : 0.099424, loss_ce: 0.033070
 11%|███▏                          | 43/400 [14:58<2:00:18, 20.22s/it]2022-01-06 12:56:33,157 iteration 732 : loss : 0.118528, loss_ce: 0.049622
2022-01-06 12:56:34,213 iteration 733 : loss : 0.141951, loss_ce: 0.070024
2022-01-06 12:56:35,403 iteration 734 : loss : 0.120085, loss_ce: 0.047096
2022-01-06 12:56:36,538 iteration 735 : loss : 0.164733, loss_ce: 0.065120
2022-01-06 12:56:37,649 iteration 736 : loss : 0.181683, loss_ce: 0.075000
2022-01-06 12:56:38,914 iteration 737 : loss : 0.137123, loss_ce: 0.059433
2022-01-06 12:56:39,954 iteration 738 : loss : 0.128081, loss_ce: 0.056607
2022-01-06 12:56:41,116 iteration 739 : loss : 0.133767, loss_ce: 0.040316
2022-01-06 12:56:42,212 iteration 740 : loss : 0.146790, loss_ce: 0.051024
2022-01-06 12:56:43,295 iteration 741 : loss : 0.142610, loss_ce: 0.058014
2022-01-06 12:56:44,351 iteration 742 : loss : 0.174214, loss_ce: 0.083462
2022-01-06 12:56:45,495 iteration 743 : loss : 0.126915, loss_ce: 0.046830
2022-01-06 12:56:46,667 iteration 744 : loss : 0.118947, loss_ce: 0.044104
2022-01-06 12:56:47,756 iteration 745 : loss : 0.172382, loss_ce: 0.038973
2022-01-06 12:56:48,856 iteration 746 : loss : 0.111544, loss_ce: 0.044985
2022-01-06 12:56:49,950 iteration 747 : loss : 0.154992, loss_ce: 0.059399
2022-01-06 12:56:51,029 iteration 748 : loss : 0.107392, loss_ce: 0.041456
 11%|███▎                          | 44/400 [15:17<1:57:48, 19.85s/it]2022-01-06 12:56:52,146 iteration 749 : loss : 0.218775, loss_ce: 0.044009
2022-01-06 12:56:53,244 iteration 750 : loss : 0.079429, loss_ce: 0.026046
2022-01-06 12:56:54,380 iteration 751 : loss : 0.152721, loss_ce: 0.047667
2022-01-06 12:56:55,476 iteration 752 : loss : 0.166528, loss_ce: 0.073158
2022-01-06 12:56:56,540 iteration 753 : loss : 0.149619, loss_ce: 0.054353
2022-01-06 12:56:57,609 iteration 754 : loss : 0.150606, loss_ce: 0.053474
2022-01-06 12:56:58,635 iteration 755 : loss : 0.153444, loss_ce: 0.068968
2022-01-06 12:56:59,673 iteration 756 : loss : 0.076897, loss_ce: 0.029613
2022-01-06 12:57:00,777 iteration 757 : loss : 0.167481, loss_ce: 0.074531
2022-01-06 12:57:01,986 iteration 758 : loss : 0.145366, loss_ce: 0.061099
2022-01-06 12:57:03,255 iteration 759 : loss : 0.157968, loss_ce: 0.064600
2022-01-06 12:57:04,347 iteration 760 : loss : 0.113411, loss_ce: 0.059177
2022-01-06 12:57:05,498 iteration 761 : loss : 0.152499, loss_ce: 0.062829
2022-01-06 12:57:06,673 iteration 762 : loss : 0.154316, loss_ce: 0.062143
2022-01-06 12:57:07,819 iteration 763 : loss : 0.129282, loss_ce: 0.056038
2022-01-06 12:57:08,894 iteration 764 : loss : 0.137191, loss_ce: 0.055570
2022-01-06 12:57:08,895 Training Data Eval:
2022-01-06 12:57:14,330   Average segmentation loss on training set: 0.3653
2022-01-06 12:57:14,330 Validation Data Eval:
2022-01-06 12:57:16,217   Average segmentation loss on validation set: 0.4450
2022-01-06 12:57:22,150 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 12:57:23,465 iteration 765 : loss : 0.126747, loss_ce: 0.042148
 11%|███▍                          | 45/400 [15:50<2:19:48, 23.63s/it]2022-01-06 12:57:24,691 iteration 766 : loss : 0.165677, loss_ce: 0.078375
2022-01-06 12:57:25,846 iteration 767 : loss : 0.129985, loss_ce: 0.046015
2022-01-06 12:57:26,981 iteration 768 : loss : 0.160820, loss_ce: 0.048098
2022-01-06 12:57:28,139 iteration 769 : loss : 0.121840, loss_ce: 0.050196
2022-01-06 12:57:29,164 iteration 770 : loss : 0.089407, loss_ce: 0.033277
2022-01-06 12:57:30,290 iteration 771 : loss : 0.174033, loss_ce: 0.053682
2022-01-06 12:57:31,414 iteration 772 : loss : 0.109624, loss_ce: 0.042852
2022-01-06 12:57:32,561 iteration 773 : loss : 0.121491, loss_ce: 0.030632
2022-01-06 12:57:33,795 iteration 774 : loss : 0.117692, loss_ce: 0.050099
2022-01-06 12:57:34,887 iteration 775 : loss : 0.180019, loss_ce: 0.082464
2022-01-06 12:57:35,952 iteration 776 : loss : 0.130933, loss_ce: 0.045952
2022-01-06 12:57:37,256 iteration 777 : loss : 0.154189, loss_ce: 0.068941
2022-01-06 12:57:38,298 iteration 778 : loss : 0.185240, loss_ce: 0.064193
2022-01-06 12:57:39,475 iteration 779 : loss : 0.117013, loss_ce: 0.048705
2022-01-06 12:57:40,653 iteration 780 : loss : 0.116957, loss_ce: 0.041430
2022-01-06 12:57:41,716 iteration 781 : loss : 0.120163, loss_ce: 0.055284
2022-01-06 12:57:42,739 iteration 782 : loss : 0.124226, loss_ce: 0.063847
 12%|███▍                          | 46/400 [16:09<2:11:41, 22.32s/it]2022-01-06 12:57:43,897 iteration 783 : loss : 0.144255, loss_ce: 0.063448
2022-01-06 12:57:44,976 iteration 784 : loss : 0.134401, loss_ce: 0.054513
2022-01-06 12:57:46,087 iteration 785 : loss : 0.164016, loss_ce: 0.082514
2022-01-06 12:57:47,170 iteration 786 : loss : 0.157400, loss_ce: 0.059907
2022-01-06 12:57:48,270 iteration 787 : loss : 0.118746, loss_ce: 0.047210
2022-01-06 12:57:49,462 iteration 788 : loss : 0.134305, loss_ce: 0.053508
2022-01-06 12:57:50,481 iteration 789 : loss : 0.146950, loss_ce: 0.052300
2022-01-06 12:57:51,639 iteration 790 : loss : 0.152974, loss_ce: 0.049670
2022-01-06 12:57:52,745 iteration 791 : loss : 0.121979, loss_ce: 0.046294
2022-01-06 12:57:53,790 iteration 792 : loss : 0.124058, loss_ce: 0.050917
2022-01-06 12:57:54,972 iteration 793 : loss : 0.118409, loss_ce: 0.054241
2022-01-06 12:57:55,988 iteration 794 : loss : 0.090593, loss_ce: 0.037900
2022-01-06 12:57:57,024 iteration 795 : loss : 0.135048, loss_ce: 0.052673
2022-01-06 12:57:58,063 iteration 796 : loss : 0.139773, loss_ce: 0.065553
2022-01-06 12:57:59,213 iteration 797 : loss : 0.182397, loss_ce: 0.076496
2022-01-06 12:58:00,301 iteration 798 : loss : 0.131273, loss_ce: 0.040248
2022-01-06 12:58:01,325 iteration 799 : loss : 0.090419, loss_ce: 0.039839
 12%|███▌                          | 47/400 [16:28<2:04:44, 21.20s/it]2022-01-06 12:58:02,613 iteration 800 : loss : 0.111874, loss_ce: 0.038432
2022-01-06 12:58:03,755 iteration 801 : loss : 0.153560, loss_ce: 0.063717
2022-01-06 12:58:04,871 iteration 802 : loss : 0.147648, loss_ce: 0.048295
2022-01-06 12:58:06,002 iteration 803 : loss : 0.127767, loss_ce: 0.062416
2022-01-06 12:58:07,204 iteration 804 : loss : 0.157295, loss_ce: 0.066760
2022-01-06 12:58:08,481 iteration 805 : loss : 0.083160, loss_ce: 0.029456
2022-01-06 12:58:09,566 iteration 806 : loss : 0.121855, loss_ce: 0.053067
2022-01-06 12:58:10,642 iteration 807 : loss : 0.107628, loss_ce: 0.043684
2022-01-06 12:58:11,809 iteration 808 : loss : 0.124135, loss_ce: 0.055993
2022-01-06 12:58:12,939 iteration 809 : loss : 0.110125, loss_ce: 0.040100
2022-01-06 12:58:14,015 iteration 810 : loss : 0.147406, loss_ce: 0.054911
2022-01-06 12:58:15,053 iteration 811 : loss : 0.136078, loss_ce: 0.067301
2022-01-06 12:58:16,128 iteration 812 : loss : 0.108410, loss_ce: 0.046074
2022-01-06 12:58:17,232 iteration 813 : loss : 0.150837, loss_ce: 0.057558
2022-01-06 12:58:18,471 iteration 814 : loss : 0.123782, loss_ce: 0.045263
2022-01-06 12:58:19,661 iteration 815 : loss : 0.133925, loss_ce: 0.053070
2022-01-06 12:58:20,795 iteration 816 : loss : 0.109014, loss_ce: 0.037842
 12%|███▌                          | 48/400 [16:47<2:01:19, 20.68s/it]2022-01-06 12:58:21,966 iteration 817 : loss : 0.108254, loss_ce: 0.042522
2022-01-06 12:58:23,025 iteration 818 : loss : 0.111122, loss_ce: 0.047517
2022-01-06 12:58:24,138 iteration 819 : loss : 0.128079, loss_ce: 0.044068
2022-01-06 12:58:25,177 iteration 820 : loss : 0.090956, loss_ce: 0.035569
2022-01-06 12:58:26,365 iteration 821 : loss : 0.087981, loss_ce: 0.031042
2022-01-06 12:58:27,545 iteration 822 : loss : 0.143608, loss_ce: 0.053924
2022-01-06 12:58:28,624 iteration 823 : loss : 0.092625, loss_ce: 0.032872
2022-01-06 12:58:29,706 iteration 824 : loss : 0.161691, loss_ce: 0.058969
2022-01-06 12:58:30,833 iteration 825 : loss : 0.107777, loss_ce: 0.040177
2022-01-06 12:58:31,946 iteration 826 : loss : 0.131817, loss_ce: 0.038759
2022-01-06 12:58:33,062 iteration 827 : loss : 0.165176, loss_ce: 0.057074
2022-01-06 12:58:34,168 iteration 828 : loss : 0.092591, loss_ce: 0.035035
2022-01-06 12:58:35,309 iteration 829 : loss : 0.132038, loss_ce: 0.048333
2022-01-06 12:58:36,481 iteration 830 : loss : 0.103823, loss_ce: 0.038328
2022-01-06 12:58:37,598 iteration 831 : loss : 0.128605, loss_ce: 0.053392
2022-01-06 12:58:38,689 iteration 832 : loss : 0.083047, loss_ce: 0.029192
2022-01-06 12:58:39,869 iteration 833 : loss : 0.128808, loss_ce: 0.064162
 12%|███▋                          | 49/400 [17:06<1:58:10, 20.20s/it]2022-01-06 12:58:41,059 iteration 834 : loss : 0.093490, loss_ce: 0.036417
2022-01-06 12:58:42,105 iteration 835 : loss : 0.105069, loss_ce: 0.030591
2022-01-06 12:58:43,257 iteration 836 : loss : 0.088833, loss_ce: 0.037125
2022-01-06 12:58:44,341 iteration 837 : loss : 0.096861, loss_ce: 0.046986
2022-01-06 12:58:45,392 iteration 838 : loss : 0.105511, loss_ce: 0.039299
2022-01-06 12:58:46,538 iteration 839 : loss : 0.115553, loss_ce: 0.052603
2022-01-06 12:58:47,641 iteration 840 : loss : 0.135357, loss_ce: 0.049296
2022-01-06 12:58:48,711 iteration 841 : loss : 0.130820, loss_ce: 0.058166
2022-01-06 12:58:49,824 iteration 842 : loss : 0.135793, loss_ce: 0.054372
2022-01-06 12:58:51,044 iteration 843 : loss : 0.090788, loss_ce: 0.034774
2022-01-06 12:58:52,133 iteration 844 : loss : 0.107902, loss_ce: 0.035975
2022-01-06 12:58:53,267 iteration 845 : loss : 0.122116, loss_ce: 0.042429
2022-01-06 12:58:54,511 iteration 846 : loss : 0.106792, loss_ce: 0.048608
2022-01-06 12:58:55,635 iteration 847 : loss : 0.120838, loss_ce: 0.041222
2022-01-06 12:58:56,642 iteration 848 : loss : 0.093479, loss_ce: 0.037545
2022-01-06 12:58:57,769 iteration 849 : loss : 0.086591, loss_ce: 0.035961
2022-01-06 12:58:57,769 Training Data Eval:
2022-01-06 12:59:03,241   Average segmentation loss on training set: 0.1628
2022-01-06 12:59:03,242 Validation Data Eval:
2022-01-06 12:59:05,117   Average segmentation loss on validation set: 0.2248
2022-01-06 12:59:11,548 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 12:59:12,719 iteration 850 : loss : 0.104983, loss_ce: 0.037420
 12%|███▊                          | 50/400 [17:39<2:19:58, 23.99s/it]2022-01-06 12:59:13,908 iteration 851 : loss : 0.118532, loss_ce: 0.049970
2022-01-06 12:59:14,989 iteration 852 : loss : 0.094713, loss_ce: 0.038184
2022-01-06 12:59:16,174 iteration 853 : loss : 0.108159, loss_ce: 0.041992
2022-01-06 12:59:17,310 iteration 854 : loss : 0.101731, loss_ce: 0.032001
2022-01-06 12:59:18,436 iteration 855 : loss : 0.107249, loss_ce: 0.044976
2022-01-06 12:59:19,586 iteration 856 : loss : 0.116892, loss_ce: 0.040889
2022-01-06 12:59:20,666 iteration 857 : loss : 0.122208, loss_ce: 0.046768
2022-01-06 12:59:21,670 iteration 858 : loss : 0.121679, loss_ce: 0.063527
2022-01-06 12:59:22,754 iteration 859 : loss : 0.124366, loss_ce: 0.036197
2022-01-06 12:59:23,800 iteration 860 : loss : 0.093123, loss_ce: 0.031359
2022-01-06 12:59:24,978 iteration 861 : loss : 0.128165, loss_ce: 0.054214
2022-01-06 12:59:26,007 iteration 862 : loss : 0.103080, loss_ce: 0.031987
2022-01-06 12:59:27,193 iteration 863 : loss : 0.118300, loss_ce: 0.051733
2022-01-06 12:59:28,333 iteration 864 : loss : 0.106607, loss_ce: 0.040830
2022-01-06 12:59:29,350 iteration 865 : loss : 0.108096, loss_ce: 0.046689
2022-01-06 12:59:30,520 iteration 866 : loss : 0.126469, loss_ce: 0.057847
2022-01-06 12:59:31,554 iteration 867 : loss : 0.070370, loss_ce: 0.025637
 13%|███▊                          | 51/400 [17:58<2:10:33, 22.45s/it]2022-01-06 12:59:32,756 iteration 868 : loss : 0.131384, loss_ce: 0.043110
2022-01-06 12:59:33,862 iteration 869 : loss : 0.090932, loss_ce: 0.036405
2022-01-06 12:59:34,867 iteration 870 : loss : 0.082472, loss_ce: 0.032562
2022-01-06 12:59:35,908 iteration 871 : loss : 0.085273, loss_ce: 0.032555
2022-01-06 12:59:36,971 iteration 872 : loss : 0.103880, loss_ce: 0.046855
2022-01-06 12:59:38,173 iteration 873 : loss : 0.107636, loss_ce: 0.034317
2022-01-06 12:59:39,189 iteration 874 : loss : 0.086210, loss_ce: 0.035165
2022-01-06 12:59:40,446 iteration 875 : loss : 0.113630, loss_ce: 0.037793
2022-01-06 12:59:41,552 iteration 876 : loss : 0.069392, loss_ce: 0.030470
2022-01-06 12:59:42,543 iteration 877 : loss : 0.083064, loss_ce: 0.026555
2022-01-06 12:59:43,684 iteration 878 : loss : 0.103641, loss_ce: 0.043627
2022-01-06 12:59:44,753 iteration 879 : loss : 0.120480, loss_ce: 0.049566
2022-01-06 12:59:45,900 iteration 880 : loss : 0.077939, loss_ce: 0.026507
2022-01-06 12:59:46,970 iteration 881 : loss : 0.143887, loss_ce: 0.070480
2022-01-06 12:59:48,119 iteration 882 : loss : 0.114657, loss_ce: 0.038391
2022-01-06 12:59:49,221 iteration 883 : loss : 0.126197, loss_ce: 0.053575
2022-01-06 12:59:50,422 iteration 884 : loss : 0.112236, loss_ce: 0.046570
 13%|███▉                          | 52/400 [18:17<2:03:58, 21.37s/it]2022-01-06 12:59:51,624 iteration 885 : loss : 0.092818, loss_ce: 0.036108
2022-01-06 12:59:52,773 iteration 886 : loss : 0.107484, loss_ce: 0.037784
2022-01-06 12:59:53,925 iteration 887 : loss : 0.130283, loss_ce: 0.055300
2022-01-06 12:59:55,039 iteration 888 : loss : 0.097146, loss_ce: 0.038705
2022-01-06 12:59:56,223 iteration 889 : loss : 0.092743, loss_ce: 0.032160
2022-01-06 12:59:57,454 iteration 890 : loss : 0.113937, loss_ce: 0.041403
2022-01-06 12:59:58,579 iteration 891 : loss : 0.093621, loss_ce: 0.036666
2022-01-06 12:59:59,690 iteration 892 : loss : 0.108990, loss_ce: 0.054629
2022-01-06 13:00:00,783 iteration 893 : loss : 0.127685, loss_ce: 0.049119
2022-01-06 13:00:02,009 iteration 894 : loss : 0.126116, loss_ce: 0.040136
2022-01-06 13:00:03,087 iteration 895 : loss : 0.136142, loss_ce: 0.043453
2022-01-06 13:00:04,273 iteration 896 : loss : 0.152642, loss_ce: 0.057051
2022-01-06 13:00:05,480 iteration 897 : loss : 0.120072, loss_ce: 0.050900
2022-01-06 13:00:06,575 iteration 898 : loss : 0.100750, loss_ce: 0.049187
2022-01-06 13:00:07,675 iteration 899 : loss : 0.119548, loss_ce: 0.050468
2022-01-06 13:00:08,749 iteration 900 : loss : 0.087468, loss_ce: 0.038598
2022-01-06 13:00:09,851 iteration 901 : loss : 0.143664, loss_ce: 0.051955
 13%|███▉                          | 53/400 [18:36<2:00:13, 20.79s/it]2022-01-06 13:00:11,083 iteration 902 : loss : 0.091077, loss_ce: 0.038182
2022-01-06 13:00:12,309 iteration 903 : loss : 0.097472, loss_ce: 0.035776
2022-01-06 13:00:13,479 iteration 904 : loss : 0.119922, loss_ce: 0.040111
2022-01-06 13:00:14,635 iteration 905 : loss : 0.115385, loss_ce: 0.048253
2022-01-06 13:00:15,753 iteration 906 : loss : 0.088377, loss_ce: 0.034736
2022-01-06 13:00:16,943 iteration 907 : loss : 0.103328, loss_ce: 0.037192
2022-01-06 13:00:17,967 iteration 908 : loss : 0.102912, loss_ce: 0.044263
2022-01-06 13:00:19,115 iteration 909 : loss : 0.110657, loss_ce: 0.040855
2022-01-06 13:00:20,172 iteration 910 : loss : 0.116639, loss_ce: 0.043883
2022-01-06 13:00:21,275 iteration 911 : loss : 0.092810, loss_ce: 0.036916
2022-01-06 13:00:22,368 iteration 912 : loss : 0.089907, loss_ce: 0.032153
2022-01-06 13:00:23,477 iteration 913 : loss : 0.114406, loss_ce: 0.035001
2022-01-06 13:00:24,597 iteration 914 : loss : 0.162511, loss_ce: 0.070048
2022-01-06 13:00:25,689 iteration 915 : loss : 0.208026, loss_ce: 0.062736
2022-01-06 13:00:26,782 iteration 916 : loss : 0.075871, loss_ce: 0.026765
2022-01-06 13:00:27,943 iteration 917 : loss : 0.123974, loss_ce: 0.048970
2022-01-06 13:00:29,120 iteration 918 : loss : 0.115612, loss_ce: 0.043921
 14%|████                          | 54/400 [18:56<1:57:14, 20.33s/it]2022-01-06 13:00:30,343 iteration 919 : loss : 0.088222, loss_ce: 0.037565
2022-01-06 13:00:31,484 iteration 920 : loss : 0.081926, loss_ce: 0.035576
2022-01-06 13:00:32,645 iteration 921 : loss : 0.117698, loss_ce: 0.043780
2022-01-06 13:00:33,825 iteration 922 : loss : 0.123271, loss_ce: 0.059824
2022-01-06 13:00:34,917 iteration 923 : loss : 0.096810, loss_ce: 0.042849
2022-01-06 13:00:36,093 iteration 924 : loss : 0.169234, loss_ce: 0.059580
2022-01-06 13:00:37,165 iteration 925 : loss : 0.089317, loss_ce: 0.027976
2022-01-06 13:00:38,329 iteration 926 : loss : 0.092124, loss_ce: 0.038905
2022-01-06 13:00:39,365 iteration 927 : loss : 0.121453, loss_ce: 0.046320
2022-01-06 13:00:40,470 iteration 928 : loss : 0.101553, loss_ce: 0.041668
2022-01-06 13:00:41,646 iteration 929 : loss : 0.093321, loss_ce: 0.031988
2022-01-06 13:00:42,787 iteration 930 : loss : 0.096421, loss_ce: 0.039493
2022-01-06 13:00:43,927 iteration 931 : loss : 0.145385, loss_ce: 0.034988
2022-01-06 13:00:45,042 iteration 932 : loss : 0.083528, loss_ce: 0.026951
2022-01-06 13:00:46,126 iteration 933 : loss : 0.111363, loss_ce: 0.044227
2022-01-06 13:00:47,235 iteration 934 : loss : 0.078928, loss_ce: 0.035312
2022-01-06 13:00:47,235 Training Data Eval:
2022-01-06 13:00:52,727   Average segmentation loss on training set: 0.3197
2022-01-06 13:00:52,727 Validation Data Eval:
2022-01-06 13:00:54,605   Average segmentation loss on validation set: 0.3388
2022-01-06 13:00:55,810 iteration 935 : loss : 0.149219, loss_ce: 0.041157
 14%|████▏                         | 55/400 [19:22<2:07:52, 22.24s/it]2022-01-06 13:00:56,973 iteration 936 : loss : 0.094701, loss_ce: 0.027797
2022-01-06 13:00:58,114 iteration 937 : loss : 0.107131, loss_ce: 0.034818
2022-01-06 13:00:59,276 iteration 938 : loss : 0.082883, loss_ce: 0.028410
2022-01-06 13:01:00,338 iteration 939 : loss : 0.083459, loss_ce: 0.029156
2022-01-06 13:01:01,488 iteration 940 : loss : 0.115075, loss_ce: 0.049214
2022-01-06 13:01:02,730 iteration 941 : loss : 0.122870, loss_ce: 0.047314
2022-01-06 13:01:03,800 iteration 942 : loss : 0.097406, loss_ce: 0.036177
2022-01-06 13:01:04,891 iteration 943 : loss : 0.095784, loss_ce: 0.043516
2022-01-06 13:01:05,968 iteration 944 : loss : 0.089598, loss_ce: 0.035673
2022-01-06 13:01:07,054 iteration 945 : loss : 0.089428, loss_ce: 0.031396
2022-01-06 13:01:08,135 iteration 946 : loss : 0.071543, loss_ce: 0.030519
2022-01-06 13:01:09,204 iteration 947 : loss : 0.090523, loss_ce: 0.029166
2022-01-06 13:01:10,268 iteration 948 : loss : 0.108261, loss_ce: 0.047077
2022-01-06 13:01:11,375 iteration 949 : loss : 0.091003, loss_ce: 0.032655
2022-01-06 13:01:12,472 iteration 950 : loss : 0.104774, loss_ce: 0.043401
2022-01-06 13:01:13,634 iteration 951 : loss : 0.081491, loss_ce: 0.046065
2022-01-06 13:01:14,698 iteration 952 : loss : 0.111232, loss_ce: 0.044924
 14%|████▏                         | 56/400 [19:41<2:01:44, 21.23s/it]2022-01-06 13:01:15,896 iteration 953 : loss : 0.110186, loss_ce: 0.039964
2022-01-06 13:01:17,070 iteration 954 : loss : 0.075498, loss_ce: 0.029561
2022-01-06 13:01:18,113 iteration 955 : loss : 0.078680, loss_ce: 0.031902
2022-01-06 13:01:19,272 iteration 956 : loss : 0.096555, loss_ce: 0.039535
2022-01-06 13:01:20,411 iteration 957 : loss : 0.075871, loss_ce: 0.024724
2022-01-06 13:01:21,568 iteration 958 : loss : 0.099487, loss_ce: 0.029285
2022-01-06 13:01:22,797 iteration 959 : loss : 0.134127, loss_ce: 0.058316
2022-01-06 13:01:24,078 iteration 960 : loss : 0.110631, loss_ce: 0.043377
2022-01-06 13:01:25,157 iteration 961 : loss : 0.149835, loss_ce: 0.044059
2022-01-06 13:01:26,203 iteration 962 : loss : 0.092785, loss_ce: 0.028602
2022-01-06 13:01:27,406 iteration 963 : loss : 0.122924, loss_ce: 0.050012
2022-01-06 13:01:28,571 iteration 964 : loss : 0.110180, loss_ce: 0.055125
2022-01-06 13:01:29,668 iteration 965 : loss : 0.077130, loss_ce: 0.033805
2022-01-06 13:01:30,796 iteration 966 : loss : 0.080060, loss_ce: 0.031973
2022-01-06 13:01:31,940 iteration 967 : loss : 0.084351, loss_ce: 0.031300
2022-01-06 13:01:33,013 iteration 968 : loss : 0.148583, loss_ce: 0.058326
2022-01-06 13:01:34,161 iteration 969 : loss : 0.069023, loss_ce: 0.028374
 14%|████▎                         | 57/400 [20:01<1:58:21, 20.70s/it]2022-01-06 13:01:35,323 iteration 970 : loss : 0.090345, loss_ce: 0.041206
2022-01-06 13:01:36,469 iteration 971 : loss : 0.140203, loss_ce: 0.046849
2022-01-06 13:01:37,513 iteration 972 : loss : 0.102412, loss_ce: 0.047312
2022-01-06 13:01:38,632 iteration 973 : loss : 0.095561, loss_ce: 0.031878
2022-01-06 13:01:39,696 iteration 974 : loss : 0.100852, loss_ce: 0.039208
2022-01-06 13:01:40,869 iteration 975 : loss : 0.108282, loss_ce: 0.035516
2022-01-06 13:01:41,960 iteration 976 : loss : 0.087234, loss_ce: 0.034942
2022-01-06 13:01:43,064 iteration 977 : loss : 0.070640, loss_ce: 0.026397
2022-01-06 13:01:44,209 iteration 978 : loss : 0.071434, loss_ce: 0.024515
2022-01-06 13:01:45,308 iteration 979 : loss : 0.139938, loss_ce: 0.072645
2022-01-06 13:01:46,410 iteration 980 : loss : 0.074913, loss_ce: 0.025260
2022-01-06 13:01:47,636 iteration 981 : loss : 0.083645, loss_ce: 0.036292
2022-01-06 13:01:48,754 iteration 982 : loss : 0.088234, loss_ce: 0.041067
2022-01-06 13:01:49,945 iteration 983 : loss : 0.098668, loss_ce: 0.033994
2022-01-06 13:01:51,000 iteration 984 : loss : 0.092917, loss_ce: 0.041150
2022-01-06 13:01:52,141 iteration 985 : loss : 0.077509, loss_ce: 0.026001
2022-01-06 13:01:53,321 iteration 986 : loss : 0.129622, loss_ce: 0.052570
 14%|████▎                         | 58/400 [20:20<1:55:23, 20.24s/it]2022-01-06 13:01:54,574 iteration 987 : loss : 0.081362, loss_ce: 0.034792
2022-01-06 13:01:55,725 iteration 988 : loss : 0.097624, loss_ce: 0.041067
2022-01-06 13:01:56,861 iteration 989 : loss : 0.114497, loss_ce: 0.041850
2022-01-06 13:01:57,981 iteration 990 : loss : 0.104617, loss_ce: 0.047416
2022-01-06 13:01:59,094 iteration 991 : loss : 0.103416, loss_ce: 0.044800
2022-01-06 13:02:00,303 iteration 992 : loss : 0.071155, loss_ce: 0.033906
2022-01-06 13:02:01,467 iteration 993 : loss : 0.072680, loss_ce: 0.025596
2022-01-06 13:02:02,706 iteration 994 : loss : 0.090170, loss_ce: 0.038877
2022-01-06 13:02:03,838 iteration 995 : loss : 0.129580, loss_ce: 0.041922
2022-01-06 13:02:05,031 iteration 996 : loss : 0.062927, loss_ce: 0.027087
2022-01-06 13:02:06,212 iteration 997 : loss : 0.116984, loss_ce: 0.046385
2022-01-06 13:02:07,277 iteration 998 : loss : 0.125908, loss_ce: 0.040019
2022-01-06 13:02:08,447 iteration 999 : loss : 0.126349, loss_ce: 0.054559
2022-01-06 13:02:09,524 iteration 1000 : loss : 0.120310, loss_ce: 0.039979
2022-01-06 13:02:10,615 iteration 1001 : loss : 0.166621, loss_ce: 0.041632
2022-01-06 13:02:11,844 iteration 1002 : loss : 0.103161, loss_ce: 0.044627
2022-01-06 13:02:13,046 iteration 1003 : loss : 0.113426, loss_ce: 0.038402
 15%|████▍                         | 59/400 [20:39<1:54:09, 20.09s/it]2022-01-06 13:02:14,230 iteration 1004 : loss : 0.141719, loss_ce: 0.058827
2022-01-06 13:02:15,328 iteration 1005 : loss : 0.083802, loss_ce: 0.031782
2022-01-06 13:02:16,518 iteration 1006 : loss : 0.129713, loss_ce: 0.057590
2022-01-06 13:02:17,601 iteration 1007 : loss : 0.111365, loss_ce: 0.062455
2022-01-06 13:02:18,666 iteration 1008 : loss : 0.195562, loss_ce: 0.082681
2022-01-06 13:02:19,737 iteration 1009 : loss : 0.110315, loss_ce: 0.040985
2022-01-06 13:02:20,813 iteration 1010 : loss : 0.131529, loss_ce: 0.049826
2022-01-06 13:02:21,990 iteration 1011 : loss : 0.087259, loss_ce: 0.035249
2022-01-06 13:02:23,088 iteration 1012 : loss : 0.073157, loss_ce: 0.026165
2022-01-06 13:02:24,148 iteration 1013 : loss : 0.099898, loss_ce: 0.031314
2022-01-06 13:02:25,281 iteration 1014 : loss : 0.127455, loss_ce: 0.048896
2022-01-06 13:02:26,336 iteration 1015 : loss : 0.137736, loss_ce: 0.059550
2022-01-06 13:02:27,477 iteration 1016 : loss : 0.109486, loss_ce: 0.040292
2022-01-06 13:02:28,499 iteration 1017 : loss : 0.074097, loss_ce: 0.026608
2022-01-06 13:02:29,521 iteration 1018 : loss : 0.110786, loss_ce: 0.046640
2022-01-06 13:02:30,620 iteration 1019 : loss : 0.100246, loss_ce: 0.041187
2022-01-06 13:02:30,621 Training Data Eval:
2022-01-06 13:02:36,128   Average segmentation loss on training set: 0.3263
2022-01-06 13:02:36,128 Validation Data Eval:
2022-01-06 13:02:38,035   Average segmentation loss on validation set: 0.4575
2022-01-06 13:02:39,118 iteration 1020 : loss : 0.080978, loss_ce: 0.026108
 15%|████▌                         | 60/400 [21:06<2:04:00, 21.88s/it]2022-01-06 13:02:40,397 iteration 1021 : loss : 0.126325, loss_ce: 0.031809
2022-01-06 13:02:41,521 iteration 1022 : loss : 0.119704, loss_ce: 0.041631
2022-01-06 13:02:42,662 iteration 1023 : loss : 0.158192, loss_ce: 0.047338
2022-01-06 13:02:43,859 iteration 1024 : loss : 0.073374, loss_ce: 0.029056
2022-01-06 13:02:45,034 iteration 1025 : loss : 0.078901, loss_ce: 0.031051
2022-01-06 13:02:46,060 iteration 1026 : loss : 0.077103, loss_ce: 0.029113
2022-01-06 13:02:47,234 iteration 1027 : loss : 0.109584, loss_ce: 0.039015
2022-01-06 13:02:48,398 iteration 1028 : loss : 0.108124, loss_ce: 0.042499
2022-01-06 13:02:49,516 iteration 1029 : loss : 0.066310, loss_ce: 0.030441
2022-01-06 13:02:50,639 iteration 1030 : loss : 0.108275, loss_ce: 0.039891
2022-01-06 13:02:51,819 iteration 1031 : loss : 0.115426, loss_ce: 0.047745
2022-01-06 13:02:52,947 iteration 1032 : loss : 0.120165, loss_ce: 0.055127
2022-01-06 13:02:54,026 iteration 1033 : loss : 0.081078, loss_ce: 0.031965
2022-01-06 13:02:55,227 iteration 1034 : loss : 0.128992, loss_ce: 0.058497
2022-01-06 13:02:56,371 iteration 1035 : loss : 0.085318, loss_ce: 0.034713
2022-01-06 13:02:57,511 iteration 1036 : loss : 0.089590, loss_ce: 0.036468
2022-01-06 13:02:58,592 iteration 1037 : loss : 0.100639, loss_ce: 0.037802
 15%|████▌                         | 61/400 [21:25<1:59:32, 21.16s/it]2022-01-06 13:02:59,786 iteration 1038 : loss : 0.071293, loss_ce: 0.029208
2022-01-06 13:03:00,904 iteration 1039 : loss : 0.091445, loss_ce: 0.042167
2022-01-06 13:03:02,073 iteration 1040 : loss : 0.131094, loss_ce: 0.061099
2022-01-06 13:03:03,218 iteration 1041 : loss : 0.099024, loss_ce: 0.041293
2022-01-06 13:03:04,377 iteration 1042 : loss : 0.109919, loss_ce: 0.043419
2022-01-06 13:03:05,517 iteration 1043 : loss : 0.121077, loss_ce: 0.039079
2022-01-06 13:03:06,687 iteration 1044 : loss : 0.062844, loss_ce: 0.024903
2022-01-06 13:03:07,736 iteration 1045 : loss : 0.075298, loss_ce: 0.032646
2022-01-06 13:03:08,821 iteration 1046 : loss : 0.110690, loss_ce: 0.030651
2022-01-06 13:03:10,010 iteration 1047 : loss : 0.139305, loss_ce: 0.057155
2022-01-06 13:03:11,144 iteration 1048 : loss : 0.094438, loss_ce: 0.035783
2022-01-06 13:03:12,272 iteration 1049 : loss : 0.126477, loss_ce: 0.049371
2022-01-06 13:03:13,432 iteration 1050 : loss : 0.112989, loss_ce: 0.037001
2022-01-06 13:03:14,588 iteration 1051 : loss : 0.111247, loss_ce: 0.045654
2022-01-06 13:03:15,717 iteration 1052 : loss : 0.147375, loss_ce: 0.055877
2022-01-06 13:03:16,762 iteration 1053 : loss : 0.113695, loss_ce: 0.033160
2022-01-06 13:03:17,963 iteration 1054 : loss : 0.103552, loss_ce: 0.046158
 16%|████▋                         | 62/400 [21:44<1:56:11, 20.62s/it]2022-01-06 13:03:19,150 iteration 1055 : loss : 0.098478, loss_ce: 0.030292
2022-01-06 13:03:20,244 iteration 1056 : loss : 0.123518, loss_ce: 0.058146
2022-01-06 13:03:21,348 iteration 1057 : loss : 0.118927, loss_ce: 0.051009
2022-01-06 13:03:22,473 iteration 1058 : loss : 0.086686, loss_ce: 0.036861
2022-01-06 13:03:23,513 iteration 1059 : loss : 0.091061, loss_ce: 0.033474
2022-01-06 13:03:24,632 iteration 1060 : loss : 0.071077, loss_ce: 0.025305
2022-01-06 13:03:25,785 iteration 1061 : loss : 0.091747, loss_ce: 0.037026
2022-01-06 13:03:26,924 iteration 1062 : loss : 0.080121, loss_ce: 0.031268
2022-01-06 13:03:28,122 iteration 1063 : loss : 0.122615, loss_ce: 0.039643
2022-01-06 13:03:29,238 iteration 1064 : loss : 0.116341, loss_ce: 0.043022
2022-01-06 13:03:30,298 iteration 1065 : loss : 0.093516, loss_ce: 0.038378
2022-01-06 13:03:31,463 iteration 1066 : loss : 0.093602, loss_ce: 0.039217
2022-01-06 13:03:32,574 iteration 1067 : loss : 0.184914, loss_ce: 0.054492
2022-01-06 13:03:33,780 iteration 1068 : loss : 0.103141, loss_ce: 0.047532
2022-01-06 13:03:34,943 iteration 1069 : loss : 0.095862, loss_ce: 0.037054
2022-01-06 13:03:36,063 iteration 1070 : loss : 0.073276, loss_ce: 0.028410
2022-01-06 13:03:37,173 iteration 1071 : loss : 0.079585, loss_ce: 0.035557
 16%|████▋                         | 63/400 [22:04<1:53:26, 20.20s/it]2022-01-06 13:03:38,455 iteration 1072 : loss : 0.074671, loss_ce: 0.026233
2022-01-06 13:03:39,540 iteration 1073 : loss : 0.160336, loss_ce: 0.036952
2022-01-06 13:03:40,656 iteration 1074 : loss : 0.096307, loss_ce: 0.037191
2022-01-06 13:03:41,820 iteration 1075 : loss : 0.122357, loss_ce: 0.038551
2022-01-06 13:03:42,886 iteration 1076 : loss : 0.067255, loss_ce: 0.023266
2022-01-06 13:03:43,992 iteration 1077 : loss : 0.092769, loss_ce: 0.032235
2022-01-06 13:03:45,201 iteration 1078 : loss : 0.167076, loss_ce: 0.087534
2022-01-06 13:03:46,280 iteration 1079 : loss : 0.102016, loss_ce: 0.035063
2022-01-06 13:03:47,337 iteration 1080 : loss : 0.083316, loss_ce: 0.035106
2022-01-06 13:03:48,502 iteration 1081 : loss : 0.076247, loss_ce: 0.023988
2022-01-06 13:03:49,662 iteration 1082 : loss : 0.111015, loss_ce: 0.048698
2022-01-06 13:03:50,844 iteration 1083 : loss : 0.099349, loss_ce: 0.045520
2022-01-06 13:03:51,948 iteration 1084 : loss : 0.125668, loss_ce: 0.064578
2022-01-06 13:03:53,166 iteration 1085 : loss : 0.102791, loss_ce: 0.037685
2022-01-06 13:03:54,271 iteration 1086 : loss : 0.103452, loss_ce: 0.042902
2022-01-06 13:03:55,401 iteration 1087 : loss : 0.097803, loss_ce: 0.039990
2022-01-06 13:03:56,602 iteration 1088 : loss : 0.094591, loss_ce: 0.042089
 16%|████▊                         | 64/400 [22:23<1:51:49, 19.97s/it]2022-01-06 13:03:57,755 iteration 1089 : loss : 0.130062, loss_ce: 0.056475
2022-01-06 13:03:58,879 iteration 1090 : loss : 0.102591, loss_ce: 0.046876
2022-01-06 13:04:00,042 iteration 1091 : loss : 0.105921, loss_ce: 0.051554
2022-01-06 13:04:01,190 iteration 1092 : loss : 0.062579, loss_ce: 0.026655
2022-01-06 13:04:02,341 iteration 1093 : loss : 0.107987, loss_ce: 0.045969
2022-01-06 13:04:03,548 iteration 1094 : loss : 0.089162, loss_ce: 0.036573
2022-01-06 13:04:04,630 iteration 1095 : loss : 0.064189, loss_ce: 0.027830
2022-01-06 13:04:05,876 iteration 1096 : loss : 0.090221, loss_ce: 0.035716
2022-01-06 13:04:07,020 iteration 1097 : loss : 0.108358, loss_ce: 0.044643
2022-01-06 13:04:08,102 iteration 1098 : loss : 0.063238, loss_ce: 0.027564
2022-01-06 13:04:09,252 iteration 1099 : loss : 0.119590, loss_ce: 0.035932
2022-01-06 13:04:10,469 iteration 1100 : loss : 0.089890, loss_ce: 0.035323
2022-01-06 13:04:11,570 iteration 1101 : loss : 0.097646, loss_ce: 0.042708
2022-01-06 13:04:12,750 iteration 1102 : loss : 0.158210, loss_ce: 0.051617
2022-01-06 13:04:13,857 iteration 1103 : loss : 0.101590, loss_ce: 0.037322
2022-01-06 13:04:14,957 iteration 1104 : loss : 0.130986, loss_ce: 0.061933
2022-01-06 13:04:14,958 Training Data Eval:
2022-01-06 13:04:20,433   Average segmentation loss on training set: 0.4355
2022-01-06 13:04:20,433 Validation Data Eval:
2022-01-06 13:04:22,343   Average segmentation loss on validation set: 0.5511
2022-01-06 13:04:23,537 iteration 1105 : loss : 0.090930, loss_ce: 0.028550
 16%|████▉                         | 65/400 [22:50<2:03:08, 22.06s/it]2022-01-06 13:04:24,662 iteration 1106 : loss : 0.108822, loss_ce: 0.043587
2022-01-06 13:04:25,769 iteration 1107 : loss : 0.123604, loss_ce: 0.061476
2022-01-06 13:04:26,904 iteration 1108 : loss : 0.098570, loss_ce: 0.025498
2022-01-06 13:04:27,974 iteration 1109 : loss : 0.065723, loss_ce: 0.027920
2022-01-06 13:04:29,116 iteration 1110 : loss : 0.102856, loss_ce: 0.040998
2022-01-06 13:04:30,221 iteration 1111 : loss : 0.069141, loss_ce: 0.029552
2022-01-06 13:04:31,325 iteration 1112 : loss : 0.076697, loss_ce: 0.031317
2022-01-06 13:04:32,412 iteration 1113 : loss : 0.102820, loss_ce: 0.032228
2022-01-06 13:04:33,603 iteration 1114 : loss : 0.081453, loss_ce: 0.036066
2022-01-06 13:04:34,863 iteration 1115 : loss : 0.088169, loss_ce: 0.039238
2022-01-06 13:04:36,041 iteration 1116 : loss : 0.160233, loss_ce: 0.050533
2022-01-06 13:04:37,301 iteration 1117 : loss : 0.090754, loss_ce: 0.030183
2022-01-06 13:04:38,470 iteration 1118 : loss : 0.124244, loss_ce: 0.045997
2022-01-06 13:04:39,552 iteration 1119 : loss : 0.081945, loss_ce: 0.033556
2022-01-06 13:04:40,619 iteration 1120 : loss : 0.080866, loss_ce: 0.027387
2022-01-06 13:04:41,737 iteration 1121 : loss : 0.097618, loss_ce: 0.031009
2022-01-06 13:04:42,865 iteration 1122 : loss : 0.107342, loss_ce: 0.051382
 16%|████▉                         | 66/400 [23:09<1:58:13, 21.24s/it]2022-01-06 13:04:43,997 iteration 1123 : loss : 0.144510, loss_ce: 0.053899
2022-01-06 13:04:45,152 iteration 1124 : loss : 0.098897, loss_ce: 0.037584
2022-01-06 13:04:46,260 iteration 1125 : loss : 0.070893, loss_ce: 0.020885
2022-01-06 13:04:47,352 iteration 1126 : loss : 0.098392, loss_ce: 0.036005
2022-01-06 13:04:48,437 iteration 1127 : loss : 0.108503, loss_ce: 0.040002
2022-01-06 13:04:49,599 iteration 1128 : loss : 0.097457, loss_ce: 0.038029
2022-01-06 13:04:50,794 iteration 1129 : loss : 0.093976, loss_ce: 0.033313
2022-01-06 13:04:51,886 iteration 1130 : loss : 0.085127, loss_ce: 0.039962
2022-01-06 13:04:52,982 iteration 1131 : loss : 0.071189, loss_ce: 0.028914
2022-01-06 13:04:54,142 iteration 1132 : loss : 0.075825, loss_ce: 0.029809
2022-01-06 13:04:55,190 iteration 1133 : loss : 0.061393, loss_ce: 0.027748
2022-01-06 13:04:56,305 iteration 1134 : loss : 0.125849, loss_ce: 0.044371
2022-01-06 13:04:57,393 iteration 1135 : loss : 0.090303, loss_ce: 0.038518
2022-01-06 13:04:58,509 iteration 1136 : loss : 0.060907, loss_ce: 0.022527
2022-01-06 13:04:59,614 iteration 1137 : loss : 0.122893, loss_ce: 0.049524
2022-01-06 13:05:00,793 iteration 1138 : loss : 0.098349, loss_ce: 0.039557
2022-01-06 13:05:01,938 iteration 1139 : loss : 0.108835, loss_ce: 0.034336
 17%|█████                         | 67/400 [23:28<1:54:16, 20.59s/it]2022-01-06 13:05:03,121 iteration 1140 : loss : 0.094790, loss_ce: 0.035674
2022-01-06 13:05:04,192 iteration 1141 : loss : 0.116186, loss_ce: 0.047737
2022-01-06 13:05:05,378 iteration 1142 : loss : 0.120149, loss_ce: 0.049170
2022-01-06 13:05:06,494 iteration 1143 : loss : 0.116407, loss_ce: 0.043148
2022-01-06 13:05:07,649 iteration 1144 : loss : 0.147350, loss_ce: 0.075005
2022-01-06 13:05:08,719 iteration 1145 : loss : 0.094263, loss_ce: 0.036962
2022-01-06 13:05:09,853 iteration 1146 : loss : 0.117831, loss_ce: 0.049596
2022-01-06 13:05:10,884 iteration 1147 : loss : 0.084449, loss_ce: 0.031536
2022-01-06 13:05:12,061 iteration 1148 : loss : 0.094859, loss_ce: 0.035887
2022-01-06 13:05:13,128 iteration 1149 : loss : 0.090504, loss_ce: 0.044191
2022-01-06 13:05:14,254 iteration 1150 : loss : 0.092543, loss_ce: 0.038794
2022-01-06 13:05:15,416 iteration 1151 : loss : 0.097483, loss_ce: 0.035198
2022-01-06 13:05:16,665 iteration 1152 : loss : 0.110943, loss_ce: 0.044253
2022-01-06 13:05:17,688 iteration 1153 : loss : 0.079290, loss_ce: 0.029633
2022-01-06 13:05:18,838 iteration 1154 : loss : 0.096383, loss_ce: 0.034462
2022-01-06 13:05:19,910 iteration 1155 : loss : 0.093416, loss_ce: 0.032463
2022-01-06 13:05:21,081 iteration 1156 : loss : 0.130518, loss_ce: 0.057694
 17%|█████                         | 68/400 [23:48<1:51:31, 20.15s/it]2022-01-06 13:05:22,241 iteration 1157 : loss : 0.208987, loss_ce: 0.061375
2022-01-06 13:05:23,378 iteration 1158 : loss : 0.087369, loss_ce: 0.030351
2022-01-06 13:05:24,567 iteration 1159 : loss : 0.069759, loss_ce: 0.020117
2022-01-06 13:05:25,785 iteration 1160 : loss : 0.115188, loss_ce: 0.042617
2022-01-06 13:05:26,953 iteration 1161 : loss : 0.081519, loss_ce: 0.036046
2022-01-06 13:05:28,101 iteration 1162 : loss : 0.094673, loss_ce: 0.031721
2022-01-06 13:05:29,207 iteration 1163 : loss : 0.097702, loss_ce: 0.038178
2022-01-06 13:05:30,315 iteration 1164 : loss : 0.139902, loss_ce: 0.049325
2022-01-06 13:05:31,422 iteration 1165 : loss : 0.088288, loss_ce: 0.040243
2022-01-06 13:05:32,621 iteration 1166 : loss : 0.071135, loss_ce: 0.025202
2022-01-06 13:05:33,710 iteration 1167 : loss : 0.098067, loss_ce: 0.039821
2022-01-06 13:05:34,825 iteration 1168 : loss : 0.081819, loss_ce: 0.035237
2022-01-06 13:05:35,938 iteration 1169 : loss : 0.100106, loss_ce: 0.039808
2022-01-06 13:05:37,054 iteration 1170 : loss : 0.084627, loss_ce: 0.034935
2022-01-06 13:05:38,220 iteration 1171 : loss : 0.089708, loss_ce: 0.031840
2022-01-06 13:05:39,363 iteration 1172 : loss : 0.140062, loss_ce: 0.066019
2022-01-06 13:05:40,533 iteration 1173 : loss : 0.101990, loss_ce: 0.036392
 17%|█████▏                        | 69/400 [24:07<1:50:00, 19.94s/it]2022-01-06 13:05:41,695 iteration 1174 : loss : 0.094075, loss_ce: 0.038186
2022-01-06 13:05:42,812 iteration 1175 : loss : 0.093957, loss_ce: 0.038349
2022-01-06 13:05:43,922 iteration 1176 : loss : 0.076876, loss_ce: 0.029487
2022-01-06 13:05:45,005 iteration 1177 : loss : 0.081053, loss_ce: 0.031835
2022-01-06 13:05:46,169 iteration 1178 : loss : 0.061652, loss_ce: 0.020950
2022-01-06 13:05:47,386 iteration 1179 : loss : 0.097461, loss_ce: 0.040558
2022-01-06 13:05:48,470 iteration 1180 : loss : 0.075159, loss_ce: 0.029334
2022-01-06 13:05:49,571 iteration 1181 : loss : 0.114887, loss_ce: 0.042985
2022-01-06 13:05:50,705 iteration 1182 : loss : 0.100931, loss_ce: 0.040193
2022-01-06 13:05:51,770 iteration 1183 : loss : 0.106630, loss_ce: 0.039788
2022-01-06 13:05:52,848 iteration 1184 : loss : 0.079496, loss_ce: 0.037547
2022-01-06 13:05:54,008 iteration 1185 : loss : 0.088916, loss_ce: 0.033167
2022-01-06 13:05:55,090 iteration 1186 : loss : 0.076743, loss_ce: 0.032600
2022-01-06 13:05:56,271 iteration 1187 : loss : 0.094693, loss_ce: 0.030593
2022-01-06 13:05:57,345 iteration 1188 : loss : 0.122444, loss_ce: 0.044396
2022-01-06 13:05:58,538 iteration 1189 : loss : 0.099261, loss_ce: 0.043451
2022-01-06 13:05:58,539 Training Data Eval:
2022-01-06 13:06:04,016   Average segmentation loss on training set: 0.2397
2022-01-06 13:06:04,016 Validation Data Eval:
2022-01-06 13:06:05,916   Average segmentation loss on validation set: 0.3158
2022-01-06 13:06:07,059 iteration 1190 : loss : 0.092321, loss_ce: 0.042397
 18%|█████▎                        | 70/400 [24:34<2:00:34, 21.92s/it]2022-01-06 13:06:08,312 iteration 1191 : loss : 0.110218, loss_ce: 0.044598
2022-01-06 13:06:09,377 iteration 1192 : loss : 0.068733, loss_ce: 0.030816
2022-01-06 13:06:10,598 iteration 1193 : loss : 0.118504, loss_ce: 0.037866
2022-01-06 13:06:11,706 iteration 1194 : loss : 0.083196, loss_ce: 0.029805
2022-01-06 13:06:12,768 iteration 1195 : loss : 0.077851, loss_ce: 0.032999
2022-01-06 13:06:13,836 iteration 1196 : loss : 0.070259, loss_ce: 0.028090
2022-01-06 13:06:14,857 iteration 1197 : loss : 0.081903, loss_ce: 0.030134
2022-01-06 13:06:16,017 iteration 1198 : loss : 0.068322, loss_ce: 0.031858
2022-01-06 13:06:17,138 iteration 1199 : loss : 0.100016, loss_ce: 0.038857
2022-01-06 13:06:18,230 iteration 1200 : loss : 0.140121, loss_ce: 0.047712
2022-01-06 13:06:19,300 iteration 1201 : loss : 0.074203, loss_ce: 0.032946
2022-01-06 13:06:20,445 iteration 1202 : loss : 0.081334, loss_ce: 0.031925
2022-01-06 13:06:21,576 iteration 1203 : loss : 0.098015, loss_ce: 0.037427
2022-01-06 13:06:22,692 iteration 1204 : loss : 0.109861, loss_ce: 0.041465
2022-01-06 13:06:23,824 iteration 1205 : loss : 0.121550, loss_ce: 0.049182
2022-01-06 13:06:24,958 iteration 1206 : loss : 0.091369, loss_ce: 0.034186
2022-01-06 13:06:26,138 iteration 1207 : loss : 0.066308, loss_ce: 0.021592
 18%|█████▎                        | 71/400 [24:53<1:55:31, 21.07s/it]2022-01-06 13:06:27,362 iteration 1208 : loss : 0.088057, loss_ce: 0.041561
2022-01-06 13:06:28,487 iteration 1209 : loss : 0.080589, loss_ce: 0.030669
2022-01-06 13:06:29,602 iteration 1210 : loss : 0.076624, loss_ce: 0.028879
2022-01-06 13:06:30,663 iteration 1211 : loss : 0.054085, loss_ce: 0.020575
2022-01-06 13:06:31,702 iteration 1212 : loss : 0.108374, loss_ce: 0.031975
2022-01-06 13:06:32,945 iteration 1213 : loss : 0.090426, loss_ce: 0.036443
2022-01-06 13:06:34,055 iteration 1214 : loss : 0.098170, loss_ce: 0.039407
2022-01-06 13:06:35,168 iteration 1215 : loss : 0.109891, loss_ce: 0.049913
2022-01-06 13:06:36,409 iteration 1216 : loss : 0.095679, loss_ce: 0.034240
2022-01-06 13:06:37,522 iteration 1217 : loss : 0.069732, loss_ce: 0.023206
2022-01-06 13:06:38,592 iteration 1218 : loss : 0.086596, loss_ce: 0.033072
2022-01-06 13:06:39,623 iteration 1219 : loss : 0.074733, loss_ce: 0.026664
2022-01-06 13:06:40,718 iteration 1220 : loss : 0.080885, loss_ce: 0.027724
2022-01-06 13:06:41,783 iteration 1221 : loss : 0.107598, loss_ce: 0.038294
2022-01-06 13:06:42,951 iteration 1222 : loss : 0.081438, loss_ce: 0.029270
2022-01-06 13:06:44,033 iteration 1223 : loss : 0.113168, loss_ce: 0.041172
2022-01-06 13:06:45,203 iteration 1224 : loss : 0.105796, loss_ce: 0.046312
 18%|█████▍                        | 72/400 [25:12<1:51:53, 20.47s/it]2022-01-06 13:06:46,351 iteration 1225 : loss : 0.097013, loss_ce: 0.036844
2022-01-06 13:06:47,507 iteration 1226 : loss : 0.120857, loss_ce: 0.045658
2022-01-06 13:06:48,572 iteration 1227 : loss : 0.076863, loss_ce: 0.024699
2022-01-06 13:06:49,659 iteration 1228 : loss : 0.104244, loss_ce: 0.033451
2022-01-06 13:06:50,691 iteration 1229 : loss : 0.105728, loss_ce: 0.027666
2022-01-06 13:06:51,746 iteration 1230 : loss : 0.073729, loss_ce: 0.027457
2022-01-06 13:06:52,900 iteration 1231 : loss : 0.077361, loss_ce: 0.034436
2022-01-06 13:06:53,972 iteration 1232 : loss : 0.085332, loss_ce: 0.036072
2022-01-06 13:06:55,054 iteration 1233 : loss : 0.060151, loss_ce: 0.026640
2022-01-06 13:06:56,214 iteration 1234 : loss : 0.091574, loss_ce: 0.038407
2022-01-06 13:06:57,361 iteration 1235 : loss : 0.124210, loss_ce: 0.039164
2022-01-06 13:06:58,435 iteration 1236 : loss : 0.104932, loss_ce: 0.035281
2022-01-06 13:06:59,463 iteration 1237 : loss : 0.112904, loss_ce: 0.049982
2022-01-06 13:07:00,632 iteration 1238 : loss : 0.074196, loss_ce: 0.028992
2022-01-06 13:07:01,695 iteration 1239 : loss : 0.067955, loss_ce: 0.024769
2022-01-06 13:07:02,765 iteration 1240 : loss : 0.068111, loss_ce: 0.028336
2022-01-06 13:07:03,840 iteration 1241 : loss : 0.092454, loss_ce: 0.043781
 18%|█████▍                        | 73/400 [25:30<1:48:32, 19.92s/it]2022-01-06 13:07:05,039 iteration 1242 : loss : 0.082924, loss_ce: 0.041723
2022-01-06 13:07:06,114 iteration 1243 : loss : 0.100907, loss_ce: 0.034247
2022-01-06 13:07:07,193 iteration 1244 : loss : 0.077086, loss_ce: 0.028294
2022-01-06 13:07:08,324 iteration 1245 : loss : 0.102992, loss_ce: 0.038747
2022-01-06 13:07:09,444 iteration 1246 : loss : 0.064737, loss_ce: 0.023356
2022-01-06 13:07:10,509 iteration 1247 : loss : 0.071671, loss_ce: 0.029569
2022-01-06 13:07:11,636 iteration 1248 : loss : 0.083055, loss_ce: 0.029226
2022-01-06 13:07:12,698 iteration 1249 : loss : 0.053798, loss_ce: 0.015669
2022-01-06 13:07:13,914 iteration 1250 : loss : 0.090358, loss_ce: 0.033981
2022-01-06 13:07:15,160 iteration 1251 : loss : 0.120026, loss_ce: 0.055052
2022-01-06 13:07:16,333 iteration 1252 : loss : 0.065091, loss_ce: 0.025979
2022-01-06 13:07:17,355 iteration 1253 : loss : 0.065627, loss_ce: 0.023380
2022-01-06 13:07:18,453 iteration 1254 : loss : 0.078030, loss_ce: 0.039348
2022-01-06 13:07:19,576 iteration 1255 : loss : 0.089396, loss_ce: 0.035796
2022-01-06 13:07:20,663 iteration 1256 : loss : 0.083754, loss_ce: 0.038207
2022-01-06 13:07:21,763 iteration 1257 : loss : 0.103302, loss_ce: 0.032091
2022-01-06 13:07:22,893 iteration 1258 : loss : 0.078241, loss_ce: 0.034199
 18%|█████▌                        | 74/400 [25:49<1:46:48, 19.66s/it]2022-01-06 13:07:24,013 iteration 1259 : loss : 0.065945, loss_ce: 0.029524
2022-01-06 13:07:25,179 iteration 1260 : loss : 0.057839, loss_ce: 0.023254
2022-01-06 13:07:26,269 iteration 1261 : loss : 0.067462, loss_ce: 0.023217
2022-01-06 13:07:27,462 iteration 1262 : loss : 0.081191, loss_ce: 0.030239
2022-01-06 13:07:28,684 iteration 1263 : loss : 0.138939, loss_ce: 0.054610
2022-01-06 13:07:29,770 iteration 1264 : loss : 0.060255, loss_ce: 0.025813
2022-01-06 13:07:30,947 iteration 1265 : loss : 0.076388, loss_ce: 0.035950
2022-01-06 13:07:32,121 iteration 1266 : loss : 0.076645, loss_ce: 0.030357
2022-01-06 13:07:33,266 iteration 1267 : loss : 0.085606, loss_ce: 0.030047
2022-01-06 13:07:34,343 iteration 1268 : loss : 0.101499, loss_ce: 0.038109
2022-01-06 13:07:35,530 iteration 1269 : loss : 0.120648, loss_ce: 0.032157
2022-01-06 13:07:36,695 iteration 1270 : loss : 0.082181, loss_ce: 0.034635
2022-01-06 13:07:37,803 iteration 1271 : loss : 0.089372, loss_ce: 0.039640
2022-01-06 13:07:38,831 iteration 1272 : loss : 0.085502, loss_ce: 0.028558
2022-01-06 13:07:39,923 iteration 1273 : loss : 0.082819, loss_ce: 0.031016
2022-01-06 13:07:41,045 iteration 1274 : loss : 0.083619, loss_ce: 0.023079
2022-01-06 13:07:41,045 Training Data Eval:
2022-01-06 13:07:46,596   Average segmentation loss on training set: 0.3810
2022-01-06 13:07:46,596 Validation Data Eval:
2022-01-06 13:07:48,492   Average segmentation loss on validation set: 0.4585
2022-01-06 13:07:49,608 iteration 1275 : loss : 0.076424, loss_ce: 0.026715
 19%|█████▋                        | 75/400 [26:16<1:57:56, 21.77s/it]2022-01-06 13:07:50,695 iteration 1276 : loss : 0.107820, loss_ce: 0.041634
2022-01-06 13:07:51,927 iteration 1277 : loss : 0.073190, loss_ce: 0.029420
2022-01-06 13:07:52,995 iteration 1278 : loss : 0.072309, loss_ce: 0.023510
2022-01-06 13:07:54,203 iteration 1279 : loss : 0.072659, loss_ce: 0.026945
2022-01-06 13:07:55,329 iteration 1280 : loss : 0.092107, loss_ce: 0.040823
2022-01-06 13:07:56,458 iteration 1281 : loss : 0.074884, loss_ce: 0.032575
2022-01-06 13:07:57,504 iteration 1282 : loss : 0.059722, loss_ce: 0.026374
2022-01-06 13:07:58,668 iteration 1283 : loss : 0.133258, loss_ce: 0.045522
2022-01-06 13:07:59,736 iteration 1284 : loss : 0.085774, loss_ce: 0.032708
2022-01-06 13:08:00,931 iteration 1285 : loss : 0.077049, loss_ce: 0.028840
2022-01-06 13:08:02,145 iteration 1286 : loss : 0.163602, loss_ce: 0.054472
2022-01-06 13:08:03,230 iteration 1287 : loss : 0.067634, loss_ce: 0.032195
2022-01-06 13:08:04,291 iteration 1288 : loss : 0.089425, loss_ce: 0.040633
2022-01-06 13:08:05,401 iteration 1289 : loss : 0.105108, loss_ce: 0.044112
2022-01-06 13:08:06,547 iteration 1290 : loss : 0.114199, loss_ce: 0.042014
2022-01-06 13:08:07,709 iteration 1291 : loss : 0.084522, loss_ce: 0.036166
2022-01-06 13:08:08,886 iteration 1292 : loss : 0.084197, loss_ce: 0.034699
 19%|█████▋                        | 76/400 [26:35<1:53:32, 21.03s/it]2022-01-06 13:08:10,092 iteration 1293 : loss : 0.080290, loss_ce: 0.032199
2022-01-06 13:08:11,236 iteration 1294 : loss : 0.082919, loss_ce: 0.032989
2022-01-06 13:08:12,435 iteration 1295 : loss : 0.107112, loss_ce: 0.033004
2022-01-06 13:08:13,572 iteration 1296 : loss : 0.098905, loss_ce: 0.036757
2022-01-06 13:08:14,643 iteration 1297 : loss : 0.089986, loss_ce: 0.045337
2022-01-06 13:08:15,677 iteration 1298 : loss : 0.058248, loss_ce: 0.021363
2022-01-06 13:08:16,829 iteration 1299 : loss : 0.089529, loss_ce: 0.041661
2022-01-06 13:08:17,913 iteration 1300 : loss : 0.080448, loss_ce: 0.028592
2022-01-06 13:08:19,064 iteration 1301 : loss : 0.045487, loss_ce: 0.017723
2022-01-06 13:08:20,149 iteration 1302 : loss : 0.070460, loss_ce: 0.029235
2022-01-06 13:08:21,249 iteration 1303 : loss : 0.074037, loss_ce: 0.028519
2022-01-06 13:08:22,383 iteration 1304 : loss : 0.084188, loss_ce: 0.034074
2022-01-06 13:08:23,501 iteration 1305 : loss : 0.095712, loss_ce: 0.040231
2022-01-06 13:08:24,657 iteration 1306 : loss : 0.143530, loss_ce: 0.051737
2022-01-06 13:08:25,823 iteration 1307 : loss : 0.073247, loss_ce: 0.037989
2022-01-06 13:08:26,949 iteration 1308 : loss : 0.117279, loss_ce: 0.033722
2022-01-06 13:08:27,996 iteration 1309 : loss : 0.075914, loss_ce: 0.025451
 19%|█████▊                        | 77/400 [26:54<1:50:06, 20.45s/it]2022-01-06 13:08:29,244 iteration 1310 : loss : 0.068218, loss_ce: 0.022306
2022-01-06 13:08:30,333 iteration 1311 : loss : 0.071837, loss_ce: 0.030127
2022-01-06 13:08:31,408 iteration 1312 : loss : 0.052704, loss_ce: 0.019397
2022-01-06 13:08:32,490 iteration 1313 : loss : 0.095720, loss_ce: 0.032697
2022-01-06 13:08:33,607 iteration 1314 : loss : 0.104529, loss_ce: 0.047651
2022-01-06 13:08:34,749 iteration 1315 : loss : 0.085863, loss_ce: 0.035869
2022-01-06 13:08:35,980 iteration 1316 : loss : 0.081384, loss_ce: 0.030472
2022-01-06 13:08:37,075 iteration 1317 : loss : 0.083953, loss_ce: 0.041756
2022-01-06 13:08:38,293 iteration 1318 : loss : 0.198705, loss_ce: 0.041269
2022-01-06 13:08:39,449 iteration 1319 : loss : 0.073884, loss_ce: 0.034594
2022-01-06 13:08:40,552 iteration 1320 : loss : 0.074148, loss_ce: 0.025986
2022-01-06 13:08:41,689 iteration 1321 : loss : 0.129005, loss_ce: 0.036058
2022-01-06 13:08:42,742 iteration 1322 : loss : 0.086424, loss_ce: 0.040053
2022-01-06 13:08:43,844 iteration 1323 : loss : 0.088461, loss_ce: 0.031856
2022-01-06 13:08:44,981 iteration 1324 : loss : 0.071828, loss_ce: 0.030177
2022-01-06 13:08:46,156 iteration 1325 : loss : 0.088714, loss_ce: 0.032019
2022-01-06 13:08:47,161 iteration 1326 : loss : 0.083092, loss_ce: 0.032554
 20%|█████▊                        | 78/400 [27:14<1:47:41, 20.07s/it]2022-01-06 13:08:48,298 iteration 1327 : loss : 0.103745, loss_ce: 0.031329
2022-01-06 13:08:49,445 iteration 1328 : loss : 0.099437, loss_ce: 0.047602
2022-01-06 13:08:50,565 iteration 1329 : loss : 0.055158, loss_ce: 0.021699
2022-01-06 13:08:51,653 iteration 1330 : loss : 0.118375, loss_ce: 0.037305
2022-01-06 13:08:52,712 iteration 1331 : loss : 0.078171, loss_ce: 0.037469
2022-01-06 13:08:53,793 iteration 1332 : loss : 0.063057, loss_ce: 0.021292
2022-01-06 13:08:54,897 iteration 1333 : loss : 0.081565, loss_ce: 0.024761
2022-01-06 13:08:56,018 iteration 1334 : loss : 0.067875, loss_ce: 0.024308
2022-01-06 13:08:57,110 iteration 1335 : loss : 0.068451, loss_ce: 0.024412
2022-01-06 13:08:58,203 iteration 1336 : loss : 0.056472, loss_ce: 0.021951
2022-01-06 13:08:59,377 iteration 1337 : loss : 0.095036, loss_ce: 0.038591
2022-01-06 13:09:00,524 iteration 1338 : loss : 0.082013, loss_ce: 0.027750
2022-01-06 13:09:01,681 iteration 1339 : loss : 0.072178, loss_ce: 0.031603
2022-01-06 13:09:02,824 iteration 1340 : loss : 0.066146, loss_ce: 0.028638
2022-01-06 13:09:03,955 iteration 1341 : loss : 0.079560, loss_ce: 0.028983
2022-01-06 13:09:05,058 iteration 1342 : loss : 0.085843, loss_ce: 0.033307
2022-01-06 13:09:06,121 iteration 1343 : loss : 0.054947, loss_ce: 0.019859
 20%|█████▉                        | 79/400 [27:33<1:45:34, 19.73s/it]2022-01-06 13:09:07,295 iteration 1344 : loss : 0.069060, loss_ce: 0.028279
2022-01-06 13:09:08,485 iteration 1345 : loss : 0.098890, loss_ce: 0.036880
2022-01-06 13:09:09,580 iteration 1346 : loss : 0.107501, loss_ce: 0.047151
2022-01-06 13:09:10,770 iteration 1347 : loss : 0.077363, loss_ce: 0.034760
2022-01-06 13:09:12,003 iteration 1348 : loss : 0.107977, loss_ce: 0.034946
2022-01-06 13:09:13,130 iteration 1349 : loss : 0.073884, loss_ce: 0.027396
2022-01-06 13:09:14,249 iteration 1350 : loss : 0.076767, loss_ce: 0.034286
2022-01-06 13:09:15,359 iteration 1351 : loss : 0.074239, loss_ce: 0.036198
2022-01-06 13:09:16,549 iteration 1352 : loss : 0.071544, loss_ce: 0.033564
2022-01-06 13:09:17,698 iteration 1353 : loss : 0.097155, loss_ce: 0.031511
2022-01-06 13:09:18,874 iteration 1354 : loss : 0.102174, loss_ce: 0.035395
2022-01-06 13:09:20,103 iteration 1355 : loss : 0.190118, loss_ce: 0.048272
2022-01-06 13:09:21,245 iteration 1356 : loss : 0.082301, loss_ce: 0.036888
2022-01-06 13:09:22,399 iteration 1357 : loss : 0.059916, loss_ce: 0.022511
2022-01-06 13:09:23,573 iteration 1358 : loss : 0.086857, loss_ce: 0.032047
2022-01-06 13:09:24,692 iteration 1359 : loss : 0.099935, loss_ce: 0.038887
2022-01-06 13:09:24,692 Training Data Eval:
2022-01-06 13:09:30,184   Average segmentation loss on training set: 0.1667
2022-01-06 13:09:30,184 Validation Data Eval:
2022-01-06 13:09:32,074   Average segmentation loss on validation set: 0.1916
2022-01-06 13:09:37,842 Found new lowest validation loss at iteration 1359! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 13:09:39,066 iteration 1360 : loss : 0.057898, loss_ce: 0.017163
 20%|██████                        | 80/400 [28:06<2:06:23, 23.70s/it]2022-01-06 13:09:40,348 iteration 1361 : loss : 0.117215, loss_ce: 0.051672
2022-01-06 13:09:41,468 iteration 1362 : loss : 0.098757, loss_ce: 0.031044
2022-01-06 13:09:42,760 iteration 1363 : loss : 0.103255, loss_ce: 0.045668
2022-01-06 13:09:43,733 iteration 1364 : loss : 0.080808, loss_ce: 0.021231
2022-01-06 13:09:44,903 iteration 1365 : loss : 0.087167, loss_ce: 0.035816
2022-01-06 13:09:46,056 iteration 1366 : loss : 0.080738, loss_ce: 0.032880
2022-01-06 13:09:47,158 iteration 1367 : loss : 0.075744, loss_ce: 0.028795
2022-01-06 13:09:48,279 iteration 1368 : loss : 0.054337, loss_ce: 0.025984
2022-01-06 13:09:49,355 iteration 1369 : loss : 0.088166, loss_ce: 0.033069
2022-01-06 13:09:50,514 iteration 1370 : loss : 0.082160, loss_ce: 0.038813
2022-01-06 13:09:51,600 iteration 1371 : loss : 0.065714, loss_ce: 0.026348
2022-01-06 13:09:52,676 iteration 1372 : loss : 0.087114, loss_ce: 0.034938
2022-01-06 13:09:53,745 iteration 1373 : loss : 0.080464, loss_ce: 0.030289
2022-01-06 13:09:54,871 iteration 1374 : loss : 0.075612, loss_ce: 0.025067
2022-01-06 13:09:56,015 iteration 1375 : loss : 0.079486, loss_ce: 0.036927
2022-01-06 13:09:57,135 iteration 1376 : loss : 0.080222, loss_ce: 0.022058
2022-01-06 13:09:58,233 iteration 1377 : loss : 0.040998, loss_ce: 0.015957
 20%|██████                        | 81/400 [28:25<1:58:46, 22.34s/it]2022-01-06 13:09:59,335 iteration 1378 : loss : 0.080074, loss_ce: 0.032028
2022-01-06 13:10:00,495 iteration 1379 : loss : 0.062441, loss_ce: 0.025052
2022-01-06 13:10:01,616 iteration 1380 : loss : 0.078237, loss_ce: 0.031662
2022-01-06 13:10:02,826 iteration 1381 : loss : 0.070384, loss_ce: 0.030211
2022-01-06 13:10:03,880 iteration 1382 : loss : 0.072057, loss_ce: 0.026506
2022-01-06 13:10:05,072 iteration 1383 : loss : 0.056605, loss_ce: 0.020343
2022-01-06 13:10:06,196 iteration 1384 : loss : 0.079972, loss_ce: 0.028306
2022-01-06 13:10:07,248 iteration 1385 : loss : 0.080497, loss_ce: 0.032562
2022-01-06 13:10:08,330 iteration 1386 : loss : 0.068606, loss_ce: 0.027763
2022-01-06 13:10:09,423 iteration 1387 : loss : 0.079349, loss_ce: 0.028614
2022-01-06 13:10:10,539 iteration 1388 : loss : 0.077206, loss_ce: 0.021898
2022-01-06 13:10:11,645 iteration 1389 : loss : 0.067656, loss_ce: 0.025654
2022-01-06 13:10:12,806 iteration 1390 : loss : 0.060743, loss_ce: 0.024983
2022-01-06 13:10:13,996 iteration 1391 : loss : 0.071926, loss_ce: 0.027172
2022-01-06 13:10:15,153 iteration 1392 : loss : 0.074592, loss_ce: 0.038926
2022-01-06 13:10:16,239 iteration 1393 : loss : 0.079099, loss_ce: 0.027656
2022-01-06 13:10:17,424 iteration 1394 : loss : 0.052256, loss_ce: 0.022503
 20%|██████▏                       | 82/400 [28:44<1:53:22, 21.39s/it]2022-01-06 13:10:18,588 iteration 1395 : loss : 0.078433, loss_ce: 0.032916
2022-01-06 13:10:19,676 iteration 1396 : loss : 0.060212, loss_ce: 0.026657
2022-01-06 13:10:20,858 iteration 1397 : loss : 0.082413, loss_ce: 0.032760
2022-01-06 13:10:22,051 iteration 1398 : loss : 0.076294, loss_ce: 0.034958
2022-01-06 13:10:23,081 iteration 1399 : loss : 0.070544, loss_ce: 0.030866
2022-01-06 13:10:24,110 iteration 1400 : loss : 0.081750, loss_ce: 0.026412
2022-01-06 13:10:25,291 iteration 1401 : loss : 0.067534, loss_ce: 0.022066
2022-01-06 13:10:26,407 iteration 1402 : loss : 0.074932, loss_ce: 0.027265
2022-01-06 13:10:27,458 iteration 1403 : loss : 0.059873, loss_ce: 0.022722
2022-01-06 13:10:28,518 iteration 1404 : loss : 0.045539, loss_ce: 0.021263
2022-01-06 13:10:29,628 iteration 1405 : loss : 0.072901, loss_ce: 0.025432
2022-01-06 13:10:30,748 iteration 1406 : loss : 0.077908, loss_ce: 0.031037
2022-01-06 13:10:31,902 iteration 1407 : loss : 0.058008, loss_ce: 0.021583
2022-01-06 13:10:33,004 iteration 1408 : loss : 0.075874, loss_ce: 0.024081
2022-01-06 13:10:34,138 iteration 1409 : loss : 0.112662, loss_ce: 0.036986
2022-01-06 13:10:35,340 iteration 1410 : loss : 0.048507, loss_ce: 0.018495
2022-01-06 13:10:36,427 iteration 1411 : loss : 0.071272, loss_ce: 0.023634
 21%|██████▏                       | 83/400 [29:03<1:49:14, 20.68s/it]2022-01-06 13:10:37,562 iteration 1412 : loss : 0.081280, loss_ce: 0.028119
2022-01-06 13:10:38,749 iteration 1413 : loss : 0.078666, loss_ce: 0.025165
2022-01-06 13:10:39,841 iteration 1414 : loss : 0.093513, loss_ce: 0.037453
2022-01-06 13:10:41,009 iteration 1415 : loss : 0.071111, loss_ce: 0.019281
2022-01-06 13:10:42,103 iteration 1416 : loss : 0.065107, loss_ce: 0.027630
2022-01-06 13:10:43,184 iteration 1417 : loss : 0.079118, loss_ce: 0.025415
2022-01-06 13:10:44,271 iteration 1418 : loss : 0.085628, loss_ce: 0.036263
2022-01-06 13:10:45,333 iteration 1419 : loss : 0.056536, loss_ce: 0.021044
2022-01-06 13:10:46,508 iteration 1420 : loss : 0.089976, loss_ce: 0.029660
2022-01-06 13:10:47,715 iteration 1421 : loss : 0.111816, loss_ce: 0.050756
2022-01-06 13:10:48,865 iteration 1422 : loss : 0.089015, loss_ce: 0.032969
2022-01-06 13:10:49,913 iteration 1423 : loss : 0.066116, loss_ce: 0.025993
2022-01-06 13:10:51,059 iteration 1424 : loss : 0.069833, loss_ce: 0.033358
2022-01-06 13:10:52,194 iteration 1425 : loss : 0.058728, loss_ce: 0.027628
2022-01-06 13:10:53,265 iteration 1426 : loss : 0.066794, loss_ce: 0.027893
2022-01-06 13:10:54,371 iteration 1427 : loss : 0.084227, loss_ce: 0.031371
2022-01-06 13:10:55,380 iteration 1428 : loss : 0.111680, loss_ce: 0.035969
 21%|██████▎                       | 84/400 [29:22<1:46:10, 20.16s/it]2022-01-06 13:10:56,629 iteration 1429 : loss : 0.066390, loss_ce: 0.029815
2022-01-06 13:10:57,697 iteration 1430 : loss : 0.205238, loss_ce: 0.059515
2022-01-06 13:10:58,807 iteration 1431 : loss : 0.110389, loss_ce: 0.050638
2022-01-06 13:10:59,937 iteration 1432 : loss : 0.074560, loss_ce: 0.031540
2022-01-06 13:11:01,113 iteration 1433 : loss : 0.066814, loss_ce: 0.023453
2022-01-06 13:11:02,197 iteration 1434 : loss : 0.067775, loss_ce: 0.027533
2022-01-06 13:11:03,359 iteration 1435 : loss : 0.089129, loss_ce: 0.035956
2022-01-06 13:11:04,528 iteration 1436 : loss : 0.071040, loss_ce: 0.029917
2022-01-06 13:11:05,707 iteration 1437 : loss : 0.071805, loss_ce: 0.020938
2022-01-06 13:11:06,848 iteration 1438 : loss : 0.053716, loss_ce: 0.024892
2022-01-06 13:11:08,031 iteration 1439 : loss : 0.073459, loss_ce: 0.031492
2022-01-06 13:11:09,183 iteration 1440 : loss : 0.060802, loss_ce: 0.024986
2022-01-06 13:11:10,270 iteration 1441 : loss : 0.063138, loss_ce: 0.024341
2022-01-06 13:11:11,396 iteration 1442 : loss : 0.051346, loss_ce: 0.020585
2022-01-06 13:11:12,562 iteration 1443 : loss : 0.091509, loss_ce: 0.030215
2022-01-06 13:11:13,705 iteration 1444 : loss : 0.144649, loss_ce: 0.039060
2022-01-06 13:11:13,705 Training Data Eval:
2022-01-06 13:11:19,389   Average segmentation loss on training set: 0.3239
2022-01-06 13:11:19,390 Validation Data Eval:
2022-01-06 13:11:21,334   Average segmentation loss on validation set: 0.3632
2022-01-06 13:11:22,494 iteration 1445 : loss : 0.053506, loss_ce: 0.017366
 21%|██████▍                       | 85/400 [29:49<1:56:46, 22.24s/it]2022-01-06 13:11:23,801 iteration 1446 : loss : 0.058205, loss_ce: 0.020110
2022-01-06 13:11:24,923 iteration 1447 : loss : 0.072977, loss_ce: 0.025313
2022-01-06 13:11:25,993 iteration 1448 : loss : 0.084786, loss_ce: 0.048867
2022-01-06 13:11:27,153 iteration 1449 : loss : 0.053514, loss_ce: 0.017414
2022-01-06 13:11:28,393 iteration 1450 : loss : 0.090704, loss_ce: 0.043929
2022-01-06 13:11:29,644 iteration 1451 : loss : 0.071434, loss_ce: 0.030835
2022-01-06 13:11:30,735 iteration 1452 : loss : 0.073841, loss_ce: 0.023375
2022-01-06 13:11:31,844 iteration 1453 : loss : 0.091620, loss_ce: 0.039331
2022-01-06 13:11:33,003 iteration 1454 : loss : 0.106455, loss_ce: 0.037805
2022-01-06 13:11:34,113 iteration 1455 : loss : 0.067558, loss_ce: 0.029880
2022-01-06 13:11:35,342 iteration 1456 : loss : 0.109808, loss_ce: 0.034276
2022-01-06 13:11:36,483 iteration 1457 : loss : 0.080216, loss_ce: 0.030323
2022-01-06 13:11:37,613 iteration 1458 : loss : 0.066030, loss_ce: 0.021747
2022-01-06 13:11:38,707 iteration 1459 : loss : 0.070860, loss_ce: 0.019695
2022-01-06 13:11:39,922 iteration 1460 : loss : 0.117591, loss_ce: 0.045490
2022-01-06 13:11:41,063 iteration 1461 : loss : 0.102780, loss_ce: 0.042714
2022-01-06 13:11:42,241 iteration 1462 : loss : 0.055827, loss_ce: 0.022263
 22%|██████▍                       | 86/400 [30:09<1:52:29, 21.50s/it]2022-01-06 13:11:43,411 iteration 1463 : loss : 0.064108, loss_ce: 0.027886
2022-01-06 13:11:44,504 iteration 1464 : loss : 0.072784, loss_ce: 0.032393
2022-01-06 13:11:45,666 iteration 1465 : loss : 0.105634, loss_ce: 0.030249
2022-01-06 13:11:46,828 iteration 1466 : loss : 0.063194, loss_ce: 0.024870
2022-01-06 13:11:47,969 iteration 1467 : loss : 0.071524, loss_ce: 0.033755
2022-01-06 13:11:49,073 iteration 1468 : loss : 0.062740, loss_ce: 0.031777
2022-01-06 13:11:50,196 iteration 1469 : loss : 0.084783, loss_ce: 0.034702
2022-01-06 13:11:51,360 iteration 1470 : loss : 0.117829, loss_ce: 0.044006
2022-01-06 13:11:52,514 iteration 1471 : loss : 0.082965, loss_ce: 0.043750
2022-01-06 13:11:53,527 iteration 1472 : loss : 0.063122, loss_ce: 0.021587
2022-01-06 13:11:54,741 iteration 1473 : loss : 0.068568, loss_ce: 0.026823
2022-01-06 13:11:55,924 iteration 1474 : loss : 0.094645, loss_ce: 0.032523
2022-01-06 13:11:57,046 iteration 1475 : loss : 0.076595, loss_ce: 0.030012
2022-01-06 13:11:58,270 iteration 1476 : loss : 0.152479, loss_ce: 0.055018
2022-01-06 13:11:59,448 iteration 1477 : loss : 0.079126, loss_ce: 0.027441
2022-01-06 13:12:00,647 iteration 1478 : loss : 0.079726, loss_ce: 0.030285
2022-01-06 13:12:01,730 iteration 1479 : loss : 0.098301, loss_ce: 0.029954
 22%|██████▌                       | 87/400 [30:28<1:49:00, 20.90s/it]2022-01-06 13:12:02,916 iteration 1480 : loss : 0.096345, loss_ce: 0.033631
2022-01-06 13:12:04,009 iteration 1481 : loss : 0.061586, loss_ce: 0.023812
2022-01-06 13:12:05,120 iteration 1482 : loss : 0.062644, loss_ce: 0.028200
2022-01-06 13:12:06,331 iteration 1483 : loss : 0.069234, loss_ce: 0.025686
2022-01-06 13:12:07,602 iteration 1484 : loss : 0.090001, loss_ce: 0.032896
2022-01-06 13:12:08,797 iteration 1485 : loss : 0.061282, loss_ce: 0.024498
2022-01-06 13:12:09,903 iteration 1486 : loss : 0.085623, loss_ce: 0.034515
2022-01-06 13:12:11,021 iteration 1487 : loss : 0.086487, loss_ce: 0.036252
2022-01-06 13:12:12,163 iteration 1488 : loss : 0.060017, loss_ce: 0.024201
2022-01-06 13:12:13,309 iteration 1489 : loss : 0.061598, loss_ce: 0.024329
2022-01-06 13:12:14,489 iteration 1490 : loss : 0.090420, loss_ce: 0.037568
2022-01-06 13:12:15,704 iteration 1491 : loss : 0.080752, loss_ce: 0.032665
2022-01-06 13:12:16,794 iteration 1492 : loss : 0.065721, loss_ce: 0.025346
2022-01-06 13:12:17,960 iteration 1493 : loss : 0.092951, loss_ce: 0.034305
2022-01-06 13:12:19,077 iteration 1494 : loss : 0.075188, loss_ce: 0.030098
2022-01-06 13:12:20,185 iteration 1495 : loss : 0.074715, loss_ce: 0.029441
2022-01-06 13:12:21,313 iteration 1496 : loss : 0.110645, loss_ce: 0.029821
 22%|██████▌                       | 88/400 [30:48<1:46:35, 20.50s/it]2022-01-06 13:12:22,443 iteration 1497 : loss : 0.096734, loss_ce: 0.040715
2022-01-06 13:12:23,534 iteration 1498 : loss : 0.067064, loss_ce: 0.017690
2022-01-06 13:12:24,703 iteration 1499 : loss : 0.087962, loss_ce: 0.037875
2022-01-06 13:12:25,749 iteration 1500 : loss : 0.085784, loss_ce: 0.028333
2022-01-06 13:12:26,856 iteration 1501 : loss : 0.050528, loss_ce: 0.017457
2022-01-06 13:12:28,013 iteration 1502 : loss : 0.074417, loss_ce: 0.028957
2022-01-06 13:12:29,115 iteration 1503 : loss : 0.053496, loss_ce: 0.021357
2022-01-06 13:12:30,219 iteration 1504 : loss : 0.063285, loss_ce: 0.020698
2022-01-06 13:12:31,285 iteration 1505 : loss : 0.060378, loss_ce: 0.021572
2022-01-06 13:12:32,405 iteration 1506 : loss : 0.078005, loss_ce: 0.030908
2022-01-06 13:12:33,549 iteration 1507 : loss : 0.072004, loss_ce: 0.027335
2022-01-06 13:12:34,652 iteration 1508 : loss : 0.075792, loss_ce: 0.032181
2022-01-06 13:12:35,779 iteration 1509 : loss : 0.090083, loss_ce: 0.034037
2022-01-06 13:12:36,859 iteration 1510 : loss : 0.044679, loss_ce: 0.016484
2022-01-06 13:12:38,011 iteration 1511 : loss : 0.065010, loss_ce: 0.028584
2022-01-06 13:12:39,104 iteration 1512 : loss : 0.066870, loss_ce: 0.026391
2022-01-06 13:12:40,245 iteration 1513 : loss : 0.056753, loss_ce: 0.026660
 22%|██████▋                       | 89/400 [31:07<1:43:48, 20.03s/it]2022-01-06 13:12:41,398 iteration 1514 : loss : 0.072244, loss_ce: 0.024961
2022-01-06 13:12:42,514 iteration 1515 : loss : 0.080555, loss_ce: 0.031617
2022-01-06 13:12:43,657 iteration 1516 : loss : 0.062330, loss_ce: 0.022848
2022-01-06 13:12:44,768 iteration 1517 : loss : 0.079067, loss_ce: 0.033134
2022-01-06 13:12:45,921 iteration 1518 : loss : 0.061412, loss_ce: 0.017758
2022-01-06 13:12:47,013 iteration 1519 : loss : 0.066536, loss_ce: 0.024098
2022-01-06 13:12:48,103 iteration 1520 : loss : 0.045081, loss_ce: 0.015470
2022-01-06 13:12:49,195 iteration 1521 : loss : 0.065655, loss_ce: 0.023788
2022-01-06 13:12:50,425 iteration 1522 : loss : 0.050572, loss_ce: 0.016605
2022-01-06 13:12:51,483 iteration 1523 : loss : 0.085440, loss_ce: 0.045082
2022-01-06 13:12:52,625 iteration 1524 : loss : 0.072655, loss_ce: 0.029323
2022-01-06 13:12:53,713 iteration 1525 : loss : 0.057433, loss_ce: 0.022670
2022-01-06 13:12:54,897 iteration 1526 : loss : 0.053056, loss_ce: 0.022336
2022-01-06 13:12:56,082 iteration 1527 : loss : 0.069602, loss_ce: 0.032947
2022-01-06 13:12:57,247 iteration 1528 : loss : 0.064704, loss_ce: 0.025046
2022-01-06 13:12:58,329 iteration 1529 : loss : 0.078938, loss_ce: 0.030801
2022-01-06 13:12:58,329 Training Data Eval:
2022-01-06 13:13:03,802   Average segmentation loss on training set: 0.0737
2022-01-06 13:13:03,803 Validation Data Eval:
2022-01-06 13:13:05,687   Average segmentation loss on validation set: 0.1102
2022-01-06 13:13:11,639 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 13:13:12,764 iteration 1530 : loss : 0.086139, loss_ce: 0.040158
 22%|██████▊                       | 90/400 [31:39<2:02:51, 23.78s/it]2022-01-06 13:13:14,064 iteration 1531 : loss : 0.079647, loss_ce: 0.034188
2022-01-06 13:13:15,225 iteration 1532 : loss : 0.062931, loss_ce: 0.023287
2022-01-06 13:13:16,297 iteration 1533 : loss : 0.053539, loss_ce: 0.022848
2022-01-06 13:13:17,485 iteration 1534 : loss : 0.075966, loss_ce: 0.029935
2022-01-06 13:13:18,579 iteration 1535 : loss : 0.049819, loss_ce: 0.019153
2022-01-06 13:13:19,664 iteration 1536 : loss : 0.099213, loss_ce: 0.053092
2022-01-06 13:13:20,738 iteration 1537 : loss : 0.078639, loss_ce: 0.025986
2022-01-06 13:13:21,793 iteration 1538 : loss : 0.201414, loss_ce: 0.043181
2022-01-06 13:13:22,891 iteration 1539 : loss : 0.063998, loss_ce: 0.024025
2022-01-06 13:13:23,995 iteration 1540 : loss : 0.092384, loss_ce: 0.043858
2022-01-06 13:13:25,014 iteration 1541 : loss : 0.062935, loss_ce: 0.021511
2022-01-06 13:13:26,134 iteration 1542 : loss : 0.064294, loss_ce: 0.029554
2022-01-06 13:13:27,223 iteration 1543 : loss : 0.067559, loss_ce: 0.022454
2022-01-06 13:13:28,256 iteration 1544 : loss : 0.066071, loss_ce: 0.026585
2022-01-06 13:13:29,348 iteration 1545 : loss : 0.069716, loss_ce: 0.026289
2022-01-06 13:13:30,440 iteration 1546 : loss : 0.066465, loss_ce: 0.030652
2022-01-06 13:13:31,522 iteration 1547 : loss : 0.066289, loss_ce: 0.025469
 23%|██████▊                       | 91/400 [31:58<1:54:41, 22.27s/it]2022-01-06 13:13:32,679 iteration 1548 : loss : 0.059619, loss_ce: 0.021525
2022-01-06 13:13:33,730 iteration 1549 : loss : 0.049283, loss_ce: 0.021236
2022-01-06 13:13:34,936 iteration 1550 : loss : 0.081988, loss_ce: 0.035872
2022-01-06 13:13:36,038 iteration 1551 : loss : 0.082369, loss_ce: 0.037013
2022-01-06 13:13:37,159 iteration 1552 : loss : 0.069893, loss_ce: 0.029680
2022-01-06 13:13:38,176 iteration 1553 : loss : 0.045704, loss_ce: 0.023724
2022-01-06 13:13:39,236 iteration 1554 : loss : 0.081453, loss_ce: 0.030371
2022-01-06 13:13:40,402 iteration 1555 : loss : 0.066117, loss_ce: 0.023616
2022-01-06 13:13:41,645 iteration 1556 : loss : 0.077168, loss_ce: 0.029127
2022-01-06 13:13:42,829 iteration 1557 : loss : 0.099174, loss_ce: 0.025610
2022-01-06 13:13:43,955 iteration 1558 : loss : 0.088950, loss_ce: 0.027632
2022-01-06 13:13:45,183 iteration 1559 : loss : 0.082098, loss_ce: 0.033690
2022-01-06 13:13:46,276 iteration 1560 : loss : 0.065005, loss_ce: 0.023008
2022-01-06 13:13:47,376 iteration 1561 : loss : 0.072778, loss_ce: 0.025611
2022-01-06 13:13:48,495 iteration 1562 : loss : 0.069679, loss_ce: 0.028100
2022-01-06 13:13:49,594 iteration 1563 : loss : 0.070991, loss_ce: 0.034676
2022-01-06 13:13:50,713 iteration 1564 : loss : 0.084652, loss_ce: 0.027981
 23%|██████▉                       | 92/400 [32:17<1:49:35, 21.35s/it]2022-01-06 13:13:51,917 iteration 1565 : loss : 0.040115, loss_ce: 0.016042
2022-01-06 13:13:53,009 iteration 1566 : loss : 0.090935, loss_ce: 0.037660
2022-01-06 13:13:54,152 iteration 1567 : loss : 0.080878, loss_ce: 0.023042
2022-01-06 13:13:55,368 iteration 1568 : loss : 0.050197, loss_ce: 0.023617
2022-01-06 13:13:56,445 iteration 1569 : loss : 0.065599, loss_ce: 0.024606
2022-01-06 13:13:57,562 iteration 1570 : loss : 0.071650, loss_ce: 0.028110
2022-01-06 13:13:58,673 iteration 1571 : loss : 0.067714, loss_ce: 0.027084
2022-01-06 13:13:59,777 iteration 1572 : loss : 0.083592, loss_ce: 0.037574
2022-01-06 13:14:00,898 iteration 1573 : loss : 0.090680, loss_ce: 0.023728
2022-01-06 13:14:02,022 iteration 1574 : loss : 0.057476, loss_ce: 0.027522
2022-01-06 13:14:03,168 iteration 1575 : loss : 0.095643, loss_ce: 0.042595
2022-01-06 13:14:04,265 iteration 1576 : loss : 0.086787, loss_ce: 0.027047
2022-01-06 13:14:05,335 iteration 1577 : loss : 0.080488, loss_ce: 0.024654
2022-01-06 13:14:06,343 iteration 1578 : loss : 0.053241, loss_ce: 0.020485
2022-01-06 13:14:07,434 iteration 1579 : loss : 0.056135, loss_ce: 0.023202
2022-01-06 13:14:08,574 iteration 1580 : loss : 0.076332, loss_ce: 0.023214
2022-01-06 13:14:09,709 iteration 1581 : loss : 0.059018, loss_ce: 0.025639
 23%|██████▉                       | 93/400 [32:36<1:45:37, 20.64s/it]2022-01-06 13:14:10,924 iteration 1582 : loss : 0.054124, loss_ce: 0.023906
2022-01-06 13:14:12,043 iteration 1583 : loss : 0.068713, loss_ce: 0.021943
2022-01-06 13:14:13,100 iteration 1584 : loss : 0.054537, loss_ce: 0.018246
2022-01-06 13:14:14,192 iteration 1585 : loss : 0.063499, loss_ce: 0.028699
2022-01-06 13:14:15,217 iteration 1586 : loss : 0.069609, loss_ce: 0.028139
2022-01-06 13:14:16,296 iteration 1587 : loss : 0.091083, loss_ce: 0.037654
2022-01-06 13:14:17,386 iteration 1588 : loss : 0.047706, loss_ce: 0.013999
2022-01-06 13:14:18,498 iteration 1589 : loss : 0.058695, loss_ce: 0.023569
2022-01-06 13:14:19,693 iteration 1590 : loss : 0.070888, loss_ce: 0.023649
2022-01-06 13:14:20,794 iteration 1591 : loss : 0.059496, loss_ce: 0.029197
2022-01-06 13:14:21,830 iteration 1592 : loss : 0.051686, loss_ce: 0.022159
2022-01-06 13:14:22,963 iteration 1593 : loss : 0.086965, loss_ce: 0.026477
2022-01-06 13:14:24,073 iteration 1594 : loss : 0.063942, loss_ce: 0.021977
2022-01-06 13:14:25,152 iteration 1595 : loss : 0.062871, loss_ce: 0.032562
2022-01-06 13:14:26,198 iteration 1596 : loss : 0.080512, loss_ce: 0.027448
2022-01-06 13:14:27,329 iteration 1597 : loss : 0.116603, loss_ce: 0.028445
2022-01-06 13:14:28,482 iteration 1598 : loss : 0.067367, loss_ce: 0.025363
 24%|███████                       | 94/400 [32:55<1:42:24, 20.08s/it]2022-01-06 13:14:29,694 iteration 1599 : loss : 0.059612, loss_ce: 0.025899
2022-01-06 13:14:30,784 iteration 1600 : loss : 0.057780, loss_ce: 0.025513
2022-01-06 13:14:31,929 iteration 1601 : loss : 0.057950, loss_ce: 0.021361
2022-01-06 13:14:33,042 iteration 1602 : loss : 0.083442, loss_ce: 0.025033
2022-01-06 13:14:34,125 iteration 1603 : loss : 0.058958, loss_ce: 0.022985
2022-01-06 13:14:35,217 iteration 1604 : loss : 0.076341, loss_ce: 0.022619
2022-01-06 13:14:36,311 iteration 1605 : loss : 0.073280, loss_ce: 0.027180
2022-01-06 13:14:37,436 iteration 1606 : loss : 0.048036, loss_ce: 0.018866
2022-01-06 13:14:38,544 iteration 1607 : loss : 0.084953, loss_ce: 0.030182
2022-01-06 13:14:39,738 iteration 1608 : loss : 0.086437, loss_ce: 0.036161
2022-01-06 13:14:40,831 iteration 1609 : loss : 0.054283, loss_ce: 0.018990
2022-01-06 13:14:42,043 iteration 1610 : loss : 0.063857, loss_ce: 0.022370
2022-01-06 13:14:43,242 iteration 1611 : loss : 0.066507, loss_ce: 0.030866
2022-01-06 13:14:44,462 iteration 1612 : loss : 0.048897, loss_ce: 0.017602
2022-01-06 13:14:45,582 iteration 1613 : loss : 0.066019, loss_ce: 0.022356
2022-01-06 13:14:46,757 iteration 1614 : loss : 0.056665, loss_ce: 0.025368
2022-01-06 13:14:46,758 Training Data Eval:
2022-01-06 13:14:52,283   Average segmentation loss on training set: 0.0937
2022-01-06 13:14:52,283 Validation Data Eval:
2022-01-06 13:14:54,199   Average segmentation loss on validation set: 0.1681
2022-01-06 13:14:55,386 iteration 1615 : loss : 0.075844, loss_ce: 0.025630
 24%|███████▏                      | 95/400 [33:22<1:52:29, 22.13s/it]2022-01-06 13:14:56,572 iteration 1616 : loss : 0.056031, loss_ce: 0.020721
2022-01-06 13:14:57,674 iteration 1617 : loss : 0.066616, loss_ce: 0.026605
2022-01-06 13:14:58,735 iteration 1618 : loss : 0.074226, loss_ce: 0.024212
2022-01-06 13:14:59,847 iteration 1619 : loss : 0.077135, loss_ce: 0.046833
2022-01-06 13:15:01,091 iteration 1620 : loss : 0.073865, loss_ce: 0.026244
2022-01-06 13:15:02,334 iteration 1621 : loss : 0.060920, loss_ce: 0.023709
2022-01-06 13:15:03,443 iteration 1622 : loss : 0.061100, loss_ce: 0.022492
2022-01-06 13:15:04,513 iteration 1623 : loss : 0.069915, loss_ce: 0.023184
2022-01-06 13:15:05,649 iteration 1624 : loss : 0.052804, loss_ce: 0.020498
2022-01-06 13:15:06,699 iteration 1625 : loss : 0.060168, loss_ce: 0.020026
2022-01-06 13:15:07,799 iteration 1626 : loss : 0.049071, loss_ce: 0.019709
2022-01-06 13:15:09,050 iteration 1627 : loss : 0.076663, loss_ce: 0.028726
2022-01-06 13:15:10,252 iteration 1628 : loss : 0.074894, loss_ce: 0.030017
2022-01-06 13:15:11,336 iteration 1629 : loss : 0.039524, loss_ce: 0.014468
2022-01-06 13:15:12,411 iteration 1630 : loss : 0.042653, loss_ce: 0.016659
2022-01-06 13:15:13,484 iteration 1631 : loss : 0.082372, loss_ce: 0.028770
2022-01-06 13:15:14,574 iteration 1632 : loss : 0.067841, loss_ce: 0.032610
 24%|███████▏                      | 96/400 [33:41<1:47:38, 21.25s/it]2022-01-06 13:15:15,756 iteration 1633 : loss : 0.065004, loss_ce: 0.019922
2022-01-06 13:15:16,900 iteration 1634 : loss : 0.068981, loss_ce: 0.023222
2022-01-06 13:15:17,985 iteration 1635 : loss : 0.070623, loss_ce: 0.020356
2022-01-06 13:15:19,015 iteration 1636 : loss : 0.064671, loss_ce: 0.034329
2022-01-06 13:15:20,161 iteration 1637 : loss : 0.063317, loss_ce: 0.024830
2022-01-06 13:15:21,275 iteration 1638 : loss : 0.063540, loss_ce: 0.027681
2022-01-06 13:15:22,451 iteration 1639 : loss : 0.068179, loss_ce: 0.027820
2022-01-06 13:15:23,528 iteration 1640 : loss : 0.074217, loss_ce: 0.023385
2022-01-06 13:15:24,727 iteration 1641 : loss : 0.056097, loss_ce: 0.020863
2022-01-06 13:15:25,788 iteration 1642 : loss : 0.046244, loss_ce: 0.016858
2022-01-06 13:15:26,827 iteration 1643 : loss : 0.074177, loss_ce: 0.025293
2022-01-06 13:15:27,995 iteration 1644 : loss : 0.067829, loss_ce: 0.028323
2022-01-06 13:15:29,202 iteration 1645 : loss : 0.078972, loss_ce: 0.035329
2022-01-06 13:15:30,280 iteration 1646 : loss : 0.071441, loss_ce: 0.024971
2022-01-06 13:15:31,378 iteration 1647 : loss : 0.101391, loss_ce: 0.035994
2022-01-06 13:15:32,444 iteration 1648 : loss : 0.052239, loss_ce: 0.021792
2022-01-06 13:15:33,504 iteration 1649 : loss : 0.053321, loss_ce: 0.020119
 24%|███████▎                      | 97/400 [34:00<1:43:47, 20.55s/it]2022-01-06 13:15:34,813 iteration 1650 : loss : 0.097598, loss_ce: 0.047608
2022-01-06 13:15:35,919 iteration 1651 : loss : 0.119413, loss_ce: 0.046078
2022-01-06 13:15:37,051 iteration 1652 : loss : 0.085567, loss_ce: 0.026745
2022-01-06 13:15:38,134 iteration 1653 : loss : 0.059100, loss_ce: 0.018872
2022-01-06 13:15:39,240 iteration 1654 : loss : 0.070147, loss_ce: 0.022523
2022-01-06 13:15:40,310 iteration 1655 : loss : 0.066652, loss_ce: 0.024287
2022-01-06 13:15:41,474 iteration 1656 : loss : 0.068218, loss_ce: 0.026907
2022-01-06 13:15:42,582 iteration 1657 : loss : 0.051530, loss_ce: 0.022057
2022-01-06 13:15:43,755 iteration 1658 : loss : 0.088159, loss_ce: 0.051979
2022-01-06 13:15:44,964 iteration 1659 : loss : 0.073409, loss_ce: 0.031010
2022-01-06 13:15:46,098 iteration 1660 : loss : 0.043978, loss_ce: 0.018157
2022-01-06 13:15:47,153 iteration 1661 : loss : 0.064497, loss_ce: 0.025729
2022-01-06 13:15:48,310 iteration 1662 : loss : 0.072202, loss_ce: 0.028415
2022-01-06 13:15:49,480 iteration 1663 : loss : 0.056861, loss_ce: 0.024739
2022-01-06 13:15:50,620 iteration 1664 : loss : 0.079335, loss_ce: 0.030606
2022-01-06 13:15:51,717 iteration 1665 : loss : 0.062449, loss_ce: 0.026213
2022-01-06 13:15:52,857 iteration 1666 : loss : 0.066277, loss_ce: 0.020057
 24%|███████▎                      | 98/400 [34:19<1:41:36, 20.19s/it]2022-01-06 13:15:53,984 iteration 1667 : loss : 0.047783, loss_ce: 0.022820
2022-01-06 13:15:55,191 iteration 1668 : loss : 0.070388, loss_ce: 0.023038
2022-01-06 13:15:56,348 iteration 1669 : loss : 0.066247, loss_ce: 0.022236
2022-01-06 13:15:57,394 iteration 1670 : loss : 0.050847, loss_ce: 0.016480
2022-01-06 13:15:58,463 iteration 1671 : loss : 0.055441, loss_ce: 0.023835
2022-01-06 13:15:59,650 iteration 1672 : loss : 0.071376, loss_ce: 0.031081
2022-01-06 13:16:00,816 iteration 1673 : loss : 0.051628, loss_ce: 0.016603
2022-01-06 13:16:02,040 iteration 1674 : loss : 0.089581, loss_ce: 0.031797
2022-01-06 13:16:03,135 iteration 1675 : loss : 0.083488, loss_ce: 0.029433
2022-01-06 13:16:04,318 iteration 1676 : loss : 0.067077, loss_ce: 0.028554
2022-01-06 13:16:05,460 iteration 1677 : loss : 0.103226, loss_ce: 0.032994
2022-01-06 13:16:06,623 iteration 1678 : loss : 0.081111, loss_ce: 0.036602
2022-01-06 13:16:07,729 iteration 1679 : loss : 0.105230, loss_ce: 0.062897
2022-01-06 13:16:08,883 iteration 1680 : loss : 0.091404, loss_ce: 0.040526
2022-01-06 13:16:09,993 iteration 1681 : loss : 0.075069, loss_ce: 0.027616
2022-01-06 13:16:11,121 iteration 1682 : loss : 0.073611, loss_ce: 0.034097
2022-01-06 13:16:12,210 iteration 1683 : loss : 0.083343, loss_ce: 0.036552
 25%|███████▍                      | 99/400 [34:39<1:40:02, 19.94s/it]2022-01-06 13:16:13,420 iteration 1684 : loss : 0.072569, loss_ce: 0.029975
2022-01-06 13:16:14,490 iteration 1685 : loss : 0.061426, loss_ce: 0.031473
2022-01-06 13:16:15,528 iteration 1686 : loss : 0.058777, loss_ce: 0.022600
2022-01-06 13:16:16,618 iteration 1687 : loss : 0.056871, loss_ce: 0.021729
2022-01-06 13:16:17,780 iteration 1688 : loss : 0.062957, loss_ce: 0.026347
2022-01-06 13:16:18,851 iteration 1689 : loss : 0.052961, loss_ce: 0.018060
2022-01-06 13:16:19,993 iteration 1690 : loss : 0.107728, loss_ce: 0.041076
2022-01-06 13:16:21,096 iteration 1691 : loss : 0.049603, loss_ce: 0.020023
2022-01-06 13:16:22,265 iteration 1692 : loss : 0.082413, loss_ce: 0.029686
2022-01-06 13:16:23,453 iteration 1693 : loss : 0.068444, loss_ce: 0.031264
2022-01-06 13:16:24,609 iteration 1694 : loss : 0.077022, loss_ce: 0.023765
2022-01-06 13:16:25,653 iteration 1695 : loss : 0.072063, loss_ce: 0.025149
2022-01-06 13:16:26,754 iteration 1696 : loss : 0.084239, loss_ce: 0.031564
2022-01-06 13:16:27,928 iteration 1697 : loss : 0.087969, loss_ce: 0.028422
2022-01-06 13:16:29,025 iteration 1698 : loss : 0.048996, loss_ce: 0.016380
2022-01-06 13:16:30,027 iteration 1699 : loss : 0.072186, loss_ce: 0.035211
2022-01-06 13:16:30,028 Training Data Eval:
2022-01-06 13:16:35,563   Average segmentation loss on training set: 0.1913
2022-01-06 13:16:35,563 Validation Data Eval:
2022-01-06 13:16:37,464   Average segmentation loss on validation set: 0.2522
2022-01-06 13:16:38,605 iteration 1700 : loss : 0.058907, loss_ce: 0.016862
 25%|███████▎                     | 100/400 [35:05<1:49:22, 21.88s/it]2022-01-06 13:16:39,822 iteration 1701 : loss : 0.070289, loss_ce: 0.032955
2022-01-06 13:16:40,955 iteration 1702 : loss : 0.079857, loss_ce: 0.036039
2022-01-06 13:16:42,021 iteration 1703 : loss : 0.054439, loss_ce: 0.021771
2022-01-06 13:16:43,222 iteration 1704 : loss : 0.056800, loss_ce: 0.021071
2022-01-06 13:16:44,343 iteration 1705 : loss : 0.064818, loss_ce: 0.026995
2022-01-06 13:16:45,474 iteration 1706 : loss : 0.064401, loss_ce: 0.029213
2022-01-06 13:16:46,583 iteration 1707 : loss : 0.061580, loss_ce: 0.022110
2022-01-06 13:16:47,743 iteration 1708 : loss : 0.078952, loss_ce: 0.028611
2022-01-06 13:16:48,790 iteration 1709 : loss : 0.058941, loss_ce: 0.026451
2022-01-06 13:16:49,818 iteration 1710 : loss : 0.068265, loss_ce: 0.028617
2022-01-06 13:16:50,945 iteration 1711 : loss : 0.065122, loss_ce: 0.025224
2022-01-06 13:16:52,011 iteration 1712 : loss : 0.050381, loss_ce: 0.017727
2022-01-06 13:16:53,070 iteration 1713 : loss : 0.050831, loss_ce: 0.018066
2022-01-06 13:16:54,277 iteration 1714 : loss : 0.078187, loss_ce: 0.023200
2022-01-06 13:16:55,434 iteration 1715 : loss : 0.111331, loss_ce: 0.029517
2022-01-06 13:16:56,634 iteration 1716 : loss : 0.076239, loss_ce: 0.029490
2022-01-06 13:16:57,750 iteration 1717 : loss : 0.071025, loss_ce: 0.026708
 25%|███████▎                     | 101/400 [35:24<1:44:56, 21.06s/it]2022-01-06 13:16:58,891 iteration 1718 : loss : 0.066796, loss_ce: 0.023813
2022-01-06 13:17:00,029 iteration 1719 : loss : 0.075769, loss_ce: 0.031790
2022-01-06 13:17:01,036 iteration 1720 : loss : 0.063996, loss_ce: 0.020218
2022-01-06 13:17:02,134 iteration 1721 : loss : 0.071258, loss_ce: 0.030532
2022-01-06 13:17:03,281 iteration 1722 : loss : 0.050897, loss_ce: 0.016638
2022-01-06 13:17:04,376 iteration 1723 : loss : 0.100329, loss_ce: 0.042542
2022-01-06 13:17:05,538 iteration 1724 : loss : 0.059513, loss_ce: 0.021883
2022-01-06 13:17:06,780 iteration 1725 : loss : 0.093114, loss_ce: 0.034523
2022-01-06 13:17:07,985 iteration 1726 : loss : 0.076876, loss_ce: 0.024945
2022-01-06 13:17:09,115 iteration 1727 : loss : 0.061202, loss_ce: 0.028395
2022-01-06 13:17:10,215 iteration 1728 : loss : 0.071642, loss_ce: 0.022746
2022-01-06 13:17:11,323 iteration 1729 : loss : 0.058121, loss_ce: 0.024845
2022-01-06 13:17:12,464 iteration 1730 : loss : 0.075451, loss_ce: 0.027760
2022-01-06 13:17:13,656 iteration 1731 : loss : 0.073083, loss_ce: 0.024344
2022-01-06 13:17:14,784 iteration 1732 : loss : 0.044901, loss_ce: 0.021009
2022-01-06 13:17:15,985 iteration 1733 : loss : 0.078012, loss_ce: 0.032441
2022-01-06 13:17:17,056 iteration 1734 : loss : 0.060630, loss_ce: 0.018180
 26%|███████▍                     | 102/400 [35:43<1:41:58, 20.53s/it]2022-01-06 13:17:18,188 iteration 1735 : loss : 0.095995, loss_ce: 0.034650
2022-01-06 13:17:19,287 iteration 1736 : loss : 0.050697, loss_ce: 0.019847
2022-01-06 13:17:20,397 iteration 1737 : loss : 0.044781, loss_ce: 0.016799
2022-01-06 13:17:21,480 iteration 1738 : loss : 0.055663, loss_ce: 0.019972
2022-01-06 13:17:22,695 iteration 1739 : loss : 0.071620, loss_ce: 0.028953
2022-01-06 13:17:23,850 iteration 1740 : loss : 0.089388, loss_ce: 0.030726
2022-01-06 13:17:24,965 iteration 1741 : loss : 0.058454, loss_ce: 0.020812
2022-01-06 13:17:26,191 iteration 1742 : loss : 0.063613, loss_ce: 0.028768
2022-01-06 13:17:27,293 iteration 1743 : loss : 0.080397, loss_ce: 0.025630
2022-01-06 13:17:28,439 iteration 1744 : loss : 0.043614, loss_ce: 0.015680
2022-01-06 13:17:29,711 iteration 1745 : loss : 0.089315, loss_ce: 0.042029
2022-01-06 13:17:30,812 iteration 1746 : loss : 0.057789, loss_ce: 0.021324
2022-01-06 13:17:31,903 iteration 1747 : loss : 0.051217, loss_ce: 0.021723
2022-01-06 13:17:33,020 iteration 1748 : loss : 0.059111, loss_ce: 0.025707
2022-01-06 13:17:34,143 iteration 1749 : loss : 0.075210, loss_ce: 0.023933
2022-01-06 13:17:35,200 iteration 1750 : loss : 0.053397, loss_ce: 0.023090
2022-01-06 13:17:36,416 iteration 1751 : loss : 0.070804, loss_ce: 0.026276
 26%|███████▍                     | 103/400 [36:03<1:39:52, 20.18s/it]2022-01-06 13:17:37,560 iteration 1752 : loss : 0.049214, loss_ce: 0.017830
2022-01-06 13:17:38,635 iteration 1753 : loss : 0.060350, loss_ce: 0.027650
2022-01-06 13:17:39,735 iteration 1754 : loss : 0.061846, loss_ce: 0.027195
2022-01-06 13:17:40,839 iteration 1755 : loss : 0.068088, loss_ce: 0.025399
2022-01-06 13:17:41,956 iteration 1756 : loss : 0.056793, loss_ce: 0.024151
2022-01-06 13:17:43,162 iteration 1757 : loss : 0.058979, loss_ce: 0.030898
2022-01-06 13:17:44,323 iteration 1758 : loss : 0.074381, loss_ce: 0.027086
2022-01-06 13:17:45,366 iteration 1759 : loss : 0.094323, loss_ce: 0.030755
2022-01-06 13:17:46,472 iteration 1760 : loss : 0.067361, loss_ce: 0.030550
2022-01-06 13:17:47,562 iteration 1761 : loss : 0.038848, loss_ce: 0.015226
2022-01-06 13:17:48,694 iteration 1762 : loss : 0.059676, loss_ce: 0.021042
2022-01-06 13:17:49,749 iteration 1763 : loss : 0.060494, loss_ce: 0.024943
2022-01-06 13:17:50,982 iteration 1764 : loss : 0.042925, loss_ce: 0.013394
2022-01-06 13:17:52,115 iteration 1765 : loss : 0.083059, loss_ce: 0.025181
2022-01-06 13:17:53,294 iteration 1766 : loss : 0.054302, loss_ce: 0.020589
2022-01-06 13:17:54,444 iteration 1767 : loss : 0.085389, loss_ce: 0.034746
2022-01-06 13:17:55,552 iteration 1768 : loss : 0.056705, loss_ce: 0.022085
 26%|███████▌                     | 104/400 [36:22<1:38:01, 19.87s/it]2022-01-06 13:17:56,771 iteration 1769 : loss : 0.057652, loss_ce: 0.021982
2022-01-06 13:17:57,870 iteration 1770 : loss : 0.055729, loss_ce: 0.018914
2022-01-06 13:17:58,950 iteration 1771 : loss : 0.044583, loss_ce: 0.014178
2022-01-06 13:18:00,130 iteration 1772 : loss : 0.098479, loss_ce: 0.055900
2022-01-06 13:18:01,269 iteration 1773 : loss : 0.086283, loss_ce: 0.039359
2022-01-06 13:18:02,356 iteration 1774 : loss : 0.078922, loss_ce: 0.033658
2022-01-06 13:18:03,456 iteration 1775 : loss : 0.063820, loss_ce: 0.023006
2022-01-06 13:18:04,538 iteration 1776 : loss : 0.065624, loss_ce: 0.022759
2022-01-06 13:18:05,605 iteration 1777 : loss : 0.053062, loss_ce: 0.020781
2022-01-06 13:18:06,781 iteration 1778 : loss : 0.066946, loss_ce: 0.028176
2022-01-06 13:18:07,920 iteration 1779 : loss : 0.078622, loss_ce: 0.035842
2022-01-06 13:18:09,060 iteration 1780 : loss : 0.113673, loss_ce: 0.037297
2022-01-06 13:18:10,197 iteration 1781 : loss : 0.077054, loss_ce: 0.027869
2022-01-06 13:18:11,250 iteration 1782 : loss : 0.108727, loss_ce: 0.029073
2022-01-06 13:18:12,339 iteration 1783 : loss : 0.056837, loss_ce: 0.027221
2022-01-06 13:18:13,489 iteration 1784 : loss : 0.089139, loss_ce: 0.043066
2022-01-06 13:18:13,489 Training Data Eval:
2022-01-06 13:18:18,934   Average segmentation loss on training set: 0.2969
2022-01-06 13:18:18,934 Validation Data Eval:
2022-01-06 13:18:20,827   Average segmentation loss on validation set: 0.3564
2022-01-06 13:18:21,964 iteration 1785 : loss : 0.051329, loss_ce: 0.024214
 26%|███████▌                     | 105/400 [36:48<1:47:20, 21.83s/it]2022-01-06 13:18:23,199 iteration 1786 : loss : 0.084396, loss_ce: 0.028845
2022-01-06 13:18:24,264 iteration 1787 : loss : 0.057363, loss_ce: 0.013825
2022-01-06 13:18:25,419 iteration 1788 : loss : 0.068168, loss_ce: 0.021573
2022-01-06 13:18:26,574 iteration 1789 : loss : 0.072411, loss_ce: 0.034887
2022-01-06 13:18:27,872 iteration 1790 : loss : 0.051689, loss_ce: 0.020885
2022-01-06 13:18:29,023 iteration 1791 : loss : 0.060551, loss_ce: 0.024933
2022-01-06 13:18:30,186 iteration 1792 : loss : 0.049727, loss_ce: 0.019007
2022-01-06 13:18:31,347 iteration 1793 : loss : 0.086787, loss_ce: 0.041524
2022-01-06 13:18:32,457 iteration 1794 : loss : 0.062517, loss_ce: 0.031420
2022-01-06 13:18:33,478 iteration 1795 : loss : 0.053137, loss_ce: 0.022646
2022-01-06 13:18:34,686 iteration 1796 : loss : 0.056351, loss_ce: 0.025171
2022-01-06 13:18:35,765 iteration 1797 : loss : 0.066065, loss_ce: 0.025981
2022-01-06 13:18:36,899 iteration 1798 : loss : 0.050687, loss_ce: 0.020391
2022-01-06 13:18:38,103 iteration 1799 : loss : 0.071605, loss_ce: 0.028032
2022-01-06 13:18:39,168 iteration 1800 : loss : 0.059243, loss_ce: 0.018718
2022-01-06 13:18:40,298 iteration 1801 : loss : 0.082746, loss_ce: 0.024077
2022-01-06 13:18:41,485 iteration 1802 : loss : 0.052520, loss_ce: 0.016623
 26%|███████▋                     | 106/400 [37:08<1:43:33, 21.13s/it]2022-01-06 13:18:42,718 iteration 1803 : loss : 0.063861, loss_ce: 0.024818
2022-01-06 13:18:43,769 iteration 1804 : loss : 0.059109, loss_ce: 0.020831
2022-01-06 13:18:45,030 iteration 1805 : loss : 0.062235, loss_ce: 0.020651
2022-01-06 13:18:46,243 iteration 1806 : loss : 0.069732, loss_ce: 0.032087
2022-01-06 13:18:47,359 iteration 1807 : loss : 0.062603, loss_ce: 0.025595
2022-01-06 13:18:48,535 iteration 1808 : loss : 0.068015, loss_ce: 0.031827
2022-01-06 13:18:49,636 iteration 1809 : loss : 0.055860, loss_ce: 0.024588
2022-01-06 13:18:50,799 iteration 1810 : loss : 0.077255, loss_ce: 0.035210
2022-01-06 13:18:51,906 iteration 1811 : loss : 0.053908, loss_ce: 0.019780
2022-01-06 13:18:53,000 iteration 1812 : loss : 0.058977, loss_ce: 0.022304
2022-01-06 13:18:54,163 iteration 1813 : loss : 0.050484, loss_ce: 0.024188
2022-01-06 13:18:55,264 iteration 1814 : loss : 0.061951, loss_ce: 0.022073
2022-01-06 13:18:56,475 iteration 1815 : loss : 0.075586, loss_ce: 0.025482
2022-01-06 13:18:57,593 iteration 1816 : loss : 0.054642, loss_ce: 0.022360
2022-01-06 13:18:58,767 iteration 1817 : loss : 0.060637, loss_ce: 0.021308
2022-01-06 13:18:59,828 iteration 1818 : loss : 0.040045, loss_ce: 0.015008
2022-01-06 13:19:01,013 iteration 1819 : loss : 0.077779, loss_ce: 0.030046
 27%|███████▊                     | 107/400 [37:27<1:40:52, 20.66s/it]2022-01-06 13:19:02,085 iteration 1820 : loss : 0.049851, loss_ce: 0.021567
2022-01-06 13:19:03,334 iteration 1821 : loss : 0.070551, loss_ce: 0.025881
2022-01-06 13:19:04,519 iteration 1822 : loss : 0.056438, loss_ce: 0.026373
2022-01-06 13:19:05,732 iteration 1823 : loss : 0.075798, loss_ce: 0.027634
2022-01-06 13:19:06,893 iteration 1824 : loss : 0.070924, loss_ce: 0.020291
2022-01-06 13:19:07,971 iteration 1825 : loss : 0.052420, loss_ce: 0.020739
2022-01-06 13:19:09,162 iteration 1826 : loss : 0.058255, loss_ce: 0.018457
2022-01-06 13:19:10,238 iteration 1827 : loss : 0.048475, loss_ce: 0.017136
2022-01-06 13:19:11,375 iteration 1828 : loss : 0.073708, loss_ce: 0.028812
2022-01-06 13:19:12,463 iteration 1829 : loss : 0.061163, loss_ce: 0.026252
2022-01-06 13:19:13,678 iteration 1830 : loss : 0.065744, loss_ce: 0.021570
2022-01-06 13:19:14,813 iteration 1831 : loss : 0.065668, loss_ce: 0.024968
2022-01-06 13:19:15,907 iteration 1832 : loss : 0.058354, loss_ce: 0.021154
2022-01-06 13:19:16,967 iteration 1833 : loss : 0.064827, loss_ce: 0.028365
2022-01-06 13:19:18,073 iteration 1834 : loss : 0.057019, loss_ce: 0.020161
2022-01-06 13:19:19,204 iteration 1835 : loss : 0.044442, loss_ce: 0.017946
2022-01-06 13:19:20,287 iteration 1836 : loss : 0.072627, loss_ce: 0.024729
 27%|███████▊                     | 108/400 [37:47<1:38:30, 20.24s/it]2022-01-06 13:19:21,492 iteration 1837 : loss : 0.046512, loss_ce: 0.015763
2022-01-06 13:19:22,536 iteration 1838 : loss : 0.054277, loss_ce: 0.023928
2022-01-06 13:19:23,633 iteration 1839 : loss : 0.055343, loss_ce: 0.021483
2022-01-06 13:19:24,779 iteration 1840 : loss : 0.052923, loss_ce: 0.024305
2022-01-06 13:19:25,851 iteration 1841 : loss : 0.050129, loss_ce: 0.019885
2022-01-06 13:19:26,969 iteration 1842 : loss : 0.047209, loss_ce: 0.014410
2022-01-06 13:19:28,130 iteration 1843 : loss : 0.045549, loss_ce: 0.023002
2022-01-06 13:19:29,253 iteration 1844 : loss : 0.049749, loss_ce: 0.014168
2022-01-06 13:19:30,361 iteration 1845 : loss : 0.046289, loss_ce: 0.013324
2022-01-06 13:19:31,561 iteration 1846 : loss : 0.056813, loss_ce: 0.023382
2022-01-06 13:19:32,646 iteration 1847 : loss : 0.056783, loss_ce: 0.019879
2022-01-06 13:19:33,754 iteration 1848 : loss : 0.074832, loss_ce: 0.030504
2022-01-06 13:19:34,830 iteration 1849 : loss : 0.075757, loss_ce: 0.032835
2022-01-06 13:19:36,058 iteration 1850 : loss : 0.094018, loss_ce: 0.033946
2022-01-06 13:19:37,186 iteration 1851 : loss : 0.091631, loss_ce: 0.030609
2022-01-06 13:19:38,315 iteration 1852 : loss : 0.050547, loss_ce: 0.022828
2022-01-06 13:19:39,395 iteration 1853 : loss : 0.055528, loss_ce: 0.023295
 27%|███████▉                     | 109/400 [38:06<1:36:31, 19.90s/it]2022-01-06 13:19:40,543 iteration 1854 : loss : 0.049763, loss_ce: 0.017558
2022-01-06 13:19:41,711 iteration 1855 : loss : 0.042518, loss_ce: 0.015478
2022-01-06 13:19:42,893 iteration 1856 : loss : 0.071065, loss_ce: 0.023511
2022-01-06 13:19:44,083 iteration 1857 : loss : 0.039824, loss_ce: 0.014885
2022-01-06 13:19:45,214 iteration 1858 : loss : 0.052434, loss_ce: 0.021003
2022-01-06 13:19:46,351 iteration 1859 : loss : 0.068232, loss_ce: 0.024638
2022-01-06 13:19:47,426 iteration 1860 : loss : 0.053628, loss_ce: 0.018920
2022-01-06 13:19:48,609 iteration 1861 : loss : 0.049439, loss_ce: 0.017184
2022-01-06 13:19:49,674 iteration 1862 : loss : 0.044461, loss_ce: 0.020046
2022-01-06 13:19:50,791 iteration 1863 : loss : 0.068428, loss_ce: 0.030286
2022-01-06 13:19:51,951 iteration 1864 : loss : 0.104269, loss_ce: 0.026503
2022-01-06 13:19:52,968 iteration 1865 : loss : 0.062011, loss_ce: 0.028752
2022-01-06 13:19:54,163 iteration 1866 : loss : 0.049232, loss_ce: 0.025212
2022-01-06 13:19:55,289 iteration 1867 : loss : 0.036876, loss_ce: 0.016107
2022-01-06 13:19:56,370 iteration 1868 : loss : 0.077541, loss_ce: 0.034655
2022-01-06 13:19:57,441 iteration 1869 : loss : 0.086481, loss_ce: 0.030035
2022-01-06 13:19:57,441 Training Data Eval:
2022-01-06 13:20:02,881   Average segmentation loss on training set: 0.1031
2022-01-06 13:20:02,882 Validation Data Eval:
2022-01-06 13:20:04,774   Average segmentation loss on validation set: 0.2300
2022-01-06 13:20:05,942 iteration 1870 : loss : 0.055597, loss_ce: 0.021113
 28%|███████▉                     | 110/400 [38:32<1:45:49, 21.90s/it]2022-01-06 13:20:07,179 iteration 1871 : loss : 0.036712, loss_ce: 0.014340
2022-01-06 13:20:08,424 iteration 1872 : loss : 0.095158, loss_ce: 0.024029
2022-01-06 13:20:09,489 iteration 1873 : loss : 0.057985, loss_ce: 0.021341
2022-01-06 13:20:10,681 iteration 1874 : loss : 0.075501, loss_ce: 0.035015
2022-01-06 13:20:11,711 iteration 1875 : loss : 0.061921, loss_ce: 0.022207
2022-01-06 13:20:12,902 iteration 1876 : loss : 0.038862, loss_ce: 0.011728
2022-01-06 13:20:14,054 iteration 1877 : loss : 0.082355, loss_ce: 0.031454
2022-01-06 13:20:15,134 iteration 1878 : loss : 0.061304, loss_ce: 0.025100
2022-01-06 13:20:16,222 iteration 1879 : loss : 0.049229, loss_ce: 0.018398
2022-01-06 13:20:17,300 iteration 1880 : loss : 0.061517, loss_ce: 0.026834
2022-01-06 13:20:18,422 iteration 1881 : loss : 0.072115, loss_ce: 0.022069
2022-01-06 13:20:19,499 iteration 1882 : loss : 0.061894, loss_ce: 0.023909
2022-01-06 13:20:20,531 iteration 1883 : loss : 0.064211, loss_ce: 0.019398
2022-01-06 13:20:21,689 iteration 1884 : loss : 0.062074, loss_ce: 0.024494
2022-01-06 13:20:22,713 iteration 1885 : loss : 0.051597, loss_ce: 0.021810
2022-01-06 13:20:23,913 iteration 1886 : loss : 0.048467, loss_ce: 0.020164
2022-01-06 13:20:25,107 iteration 1887 : loss : 0.060780, loss_ce: 0.026145
 28%|████████                     | 111/400 [38:52<1:41:30, 21.08s/it]2022-01-06 13:20:26,266 iteration 1888 : loss : 0.059218, loss_ce: 0.025332
2022-01-06 13:20:27,472 iteration 1889 : loss : 0.063727, loss_ce: 0.025796
2022-01-06 13:20:28,624 iteration 1890 : loss : 0.074882, loss_ce: 0.024698
2022-01-06 13:20:29,797 iteration 1891 : loss : 0.059291, loss_ce: 0.022437
2022-01-06 13:20:30,925 iteration 1892 : loss : 0.051281, loss_ce: 0.022995
2022-01-06 13:20:32,063 iteration 1893 : loss : 0.074632, loss_ce: 0.044750
2022-01-06 13:20:33,181 iteration 1894 : loss : 0.066678, loss_ce: 0.028973
2022-01-06 13:20:34,286 iteration 1895 : loss : 0.053898, loss_ce: 0.017672
2022-01-06 13:20:35,536 iteration 1896 : loss : 0.078137, loss_ce: 0.040206
2022-01-06 13:20:36,641 iteration 1897 : loss : 0.065308, loss_ce: 0.019601
2022-01-06 13:20:37,817 iteration 1898 : loss : 0.049354, loss_ce: 0.018581
2022-01-06 13:20:38,924 iteration 1899 : loss : 0.073759, loss_ce: 0.032723
2022-01-06 13:20:39,960 iteration 1900 : loss : 0.042245, loss_ce: 0.017058
2022-01-06 13:20:41,070 iteration 1901 : loss : 0.051656, loss_ce: 0.019317
2022-01-06 13:20:42,218 iteration 1902 : loss : 0.058014, loss_ce: 0.026423
2022-01-06 13:20:43,392 iteration 1903 : loss : 0.058714, loss_ce: 0.016676
2022-01-06 13:20:44,628 iteration 1904 : loss : 0.053478, loss_ce: 0.018922
 28%|████████                     | 112/400 [39:11<1:38:54, 20.61s/it]2022-01-06 13:20:45,745 iteration 1905 : loss : 0.051722, loss_ce: 0.020814
2022-01-06 13:20:46,928 iteration 1906 : loss : 0.050933, loss_ce: 0.018688
2022-01-06 13:20:48,174 iteration 1907 : loss : 0.055724, loss_ce: 0.020667
2022-01-06 13:20:49,331 iteration 1908 : loss : 0.065450, loss_ce: 0.024321
2022-01-06 13:20:50,520 iteration 1909 : loss : 0.051462, loss_ce: 0.024583
2022-01-06 13:20:51,686 iteration 1910 : loss : 0.041747, loss_ce: 0.019591
2022-01-06 13:20:52,916 iteration 1911 : loss : 0.047029, loss_ce: 0.019919
2022-01-06 13:20:54,095 iteration 1912 : loss : 0.059922, loss_ce: 0.018282
2022-01-06 13:20:55,210 iteration 1913 : loss : 0.048252, loss_ce: 0.017390
2022-01-06 13:20:56,286 iteration 1914 : loss : 0.046153, loss_ce: 0.019454
2022-01-06 13:20:57,411 iteration 1915 : loss : 0.063959, loss_ce: 0.021008
2022-01-06 13:20:58,521 iteration 1916 : loss : 0.050769, loss_ce: 0.018105
2022-01-06 13:20:59,684 iteration 1917 : loss : 0.052441, loss_ce: 0.020039
2022-01-06 13:21:00,760 iteration 1918 : loss : 0.058063, loss_ce: 0.023970
2022-01-06 13:21:01,852 iteration 1919 : loss : 0.050523, loss_ce: 0.022707
2022-01-06 13:21:02,938 iteration 1920 : loss : 0.044999, loss_ce: 0.016778
2022-01-06 13:21:04,112 iteration 1921 : loss : 0.059395, loss_ce: 0.020464
 28%|████████▏                    | 113/400 [39:31<1:36:57, 20.27s/it]2022-01-06 13:21:05,235 iteration 1922 : loss : 0.049376, loss_ce: 0.016772
2022-01-06 13:21:06,276 iteration 1923 : loss : 0.038652, loss_ce: 0.016646
2022-01-06 13:21:07,415 iteration 1924 : loss : 0.071309, loss_ce: 0.031137
2022-01-06 13:21:08,477 iteration 1925 : loss : 0.063515, loss_ce: 0.025606
2022-01-06 13:21:09,557 iteration 1926 : loss : 0.046036, loss_ce: 0.018507
2022-01-06 13:21:10,712 iteration 1927 : loss : 0.060295, loss_ce: 0.023397
2022-01-06 13:21:11,877 iteration 1928 : loss : 0.048972, loss_ce: 0.019992
2022-01-06 13:21:12,961 iteration 1929 : loss : 0.039951, loss_ce: 0.017462
2022-01-06 13:21:14,074 iteration 1930 : loss : 0.074771, loss_ce: 0.028095
2022-01-06 13:21:15,165 iteration 1931 : loss : 0.057095, loss_ce: 0.016635
2022-01-06 13:21:16,279 iteration 1932 : loss : 0.054770, loss_ce: 0.016851
2022-01-06 13:21:17,406 iteration 1933 : loss : 0.055204, loss_ce: 0.021814
2022-01-06 13:21:18,393 iteration 1934 : loss : 0.045051, loss_ce: 0.020057
2022-01-06 13:21:19,469 iteration 1935 : loss : 0.055926, loss_ce: 0.027616
2022-01-06 13:21:20,636 iteration 1936 : loss : 0.044227, loss_ce: 0.017371
2022-01-06 13:21:21,789 iteration 1937 : loss : 0.037576, loss_ce: 0.016483
2022-01-06 13:21:22,950 iteration 1938 : loss : 0.048346, loss_ce: 0.013151
 28%|████████▎                    | 114/400 [39:49<1:34:35, 19.84s/it]2022-01-06 13:21:24,060 iteration 1939 : loss : 0.037718, loss_ce: 0.016555
2022-01-06 13:21:25,144 iteration 1940 : loss : 0.037064, loss_ce: 0.014308
2022-01-06 13:21:26,314 iteration 1941 : loss : 0.054715, loss_ce: 0.026863
2022-01-06 13:21:27,422 iteration 1942 : loss : 0.058023, loss_ce: 0.020963
2022-01-06 13:21:28,458 iteration 1943 : loss : 0.043289, loss_ce: 0.014890
2022-01-06 13:21:29,523 iteration 1944 : loss : 0.055533, loss_ce: 0.019124
2022-01-06 13:21:30,602 iteration 1945 : loss : 0.044324, loss_ce: 0.023714
2022-01-06 13:21:31,707 iteration 1946 : loss : 0.067725, loss_ce: 0.024575
2022-01-06 13:21:32,788 iteration 1947 : loss : 0.076564, loss_ce: 0.027586
2022-01-06 13:21:33,863 iteration 1948 : loss : 0.052218, loss_ce: 0.020542
2022-01-06 13:21:34,894 iteration 1949 : loss : 0.052843, loss_ce: 0.027212
2022-01-06 13:21:36,053 iteration 1950 : loss : 0.045143, loss_ce: 0.017323
2022-01-06 13:21:37,284 iteration 1951 : loss : 0.059904, loss_ce: 0.023865
2022-01-06 13:21:38,348 iteration 1952 : loss : 0.067636, loss_ce: 0.017922
2022-01-06 13:21:39,485 iteration 1953 : loss : 0.057600, loss_ce: 0.029618
2022-01-06 13:21:40,693 iteration 1954 : loss : 0.058500, loss_ce: 0.021207
2022-01-06 13:21:40,693 Training Data Eval:
2022-01-06 13:21:46,191   Average segmentation loss on training set: 0.0721
2022-01-06 13:21:46,192 Validation Data Eval:
2022-01-06 13:21:48,078   Average segmentation loss on validation set: 0.1426
2022-01-06 13:21:49,190 iteration 1955 : loss : 0.039202, loss_ce: 0.014383
 29%|████████▎                    | 115/400 [40:16<1:43:22, 21.76s/it]2022-01-06 13:21:50,431 iteration 1956 : loss : 0.062441, loss_ce: 0.024964
2022-01-06 13:21:51,488 iteration 1957 : loss : 0.042008, loss_ce: 0.016954
2022-01-06 13:21:52,583 iteration 1958 : loss : 0.046514, loss_ce: 0.021326
2022-01-06 13:21:53,789 iteration 1959 : loss : 0.102098, loss_ce: 0.024828
2022-01-06 13:21:54,882 iteration 1960 : loss : 0.059564, loss_ce: 0.019271
2022-01-06 13:21:56,004 iteration 1961 : loss : 0.052121, loss_ce: 0.020055
2022-01-06 13:21:57,108 iteration 1962 : loss : 0.040019, loss_ce: 0.015581
2022-01-06 13:21:58,242 iteration 1963 : loss : 0.055368, loss_ce: 0.023130
2022-01-06 13:21:59,339 iteration 1964 : loss : 0.053037, loss_ce: 0.015786
2022-01-06 13:22:00,415 iteration 1965 : loss : 0.055173, loss_ce: 0.022218
2022-01-06 13:22:01,479 iteration 1966 : loss : 0.059182, loss_ce: 0.026087
2022-01-06 13:22:02,566 iteration 1967 : loss : 0.063606, loss_ce: 0.018355
2022-01-06 13:22:03,605 iteration 1968 : loss : 0.056253, loss_ce: 0.024986
2022-01-06 13:22:04,753 iteration 1969 : loss : 0.054475, loss_ce: 0.021522
2022-01-06 13:22:06,016 iteration 1970 : loss : 0.055707, loss_ce: 0.020713
2022-01-06 13:22:07,028 iteration 1971 : loss : 0.047890, loss_ce: 0.015591
2022-01-06 13:22:08,259 iteration 1972 : loss : 0.071152, loss_ce: 0.026452
 29%|████████▍                    | 116/400 [40:35<1:39:11, 20.95s/it]2022-01-06 13:22:09,346 iteration 1973 : loss : 0.040594, loss_ce: 0.016008
2022-01-06 13:22:10,388 iteration 1974 : loss : 0.049569, loss_ce: 0.020684
2022-01-06 13:22:11,581 iteration 1975 : loss : 0.060312, loss_ce: 0.022167
2022-01-06 13:22:12,612 iteration 1976 : loss : 0.051547, loss_ce: 0.023153
2022-01-06 13:22:13,811 iteration 1977 : loss : 0.065748, loss_ce: 0.030549
2022-01-06 13:22:14,883 iteration 1978 : loss : 0.057574, loss_ce: 0.024756
2022-01-06 13:22:15,982 iteration 1979 : loss : 0.055354, loss_ce: 0.029379
2022-01-06 13:22:17,135 iteration 1980 : loss : 0.062093, loss_ce: 0.019130
2022-01-06 13:22:18,190 iteration 1981 : loss : 0.049197, loss_ce: 0.019738
2022-01-06 13:22:19,362 iteration 1982 : loss : 0.060294, loss_ce: 0.022772
2022-01-06 13:22:20,572 iteration 1983 : loss : 0.052146, loss_ce: 0.020248
2022-01-06 13:22:21,650 iteration 1984 : loss : 0.058246, loss_ce: 0.017761
2022-01-06 13:22:22,738 iteration 1985 : loss : 0.041112, loss_ce: 0.013656
2022-01-06 13:22:23,819 iteration 1986 : loss : 0.068661, loss_ce: 0.023133
2022-01-06 13:22:24,924 iteration 1987 : loss : 0.035669, loss_ce: 0.014776
2022-01-06 13:22:26,124 iteration 1988 : loss : 0.042214, loss_ce: 0.018076
2022-01-06 13:22:27,248 iteration 1989 : loss : 0.044859, loss_ce: 0.020037
 29%|████████▍                    | 117/400 [40:54<1:36:02, 20.36s/it]2022-01-06 13:22:28,420 iteration 1990 : loss : 0.064566, loss_ce: 0.024806
2022-01-06 13:22:29,441 iteration 1991 : loss : 0.040157, loss_ce: 0.016808
2022-01-06 13:22:30,603 iteration 1992 : loss : 0.057646, loss_ce: 0.025014
2022-01-06 13:22:31,707 iteration 1993 : loss : 0.069917, loss_ce: 0.020166
2022-01-06 13:22:32,859 iteration 1994 : loss : 0.056030, loss_ce: 0.022360
2022-01-06 13:22:33,967 iteration 1995 : loss : 0.063057, loss_ce: 0.021949
2022-01-06 13:22:35,095 iteration 1996 : loss : 0.051179, loss_ce: 0.023612
2022-01-06 13:22:36,220 iteration 1997 : loss : 0.052170, loss_ce: 0.021944
2022-01-06 13:22:37,366 iteration 1998 : loss : 0.068821, loss_ce: 0.028733
2022-01-06 13:22:38,504 iteration 1999 : loss : 0.052555, loss_ce: 0.021234
2022-01-06 13:22:39,752 iteration 2000 : loss : 0.080271, loss_ce: 0.021065
2022-01-06 13:22:40,877 iteration 2001 : loss : 0.053129, loss_ce: 0.020014
2022-01-06 13:22:41,903 iteration 2002 : loss : 0.055410, loss_ce: 0.020954
2022-01-06 13:22:42,881 iteration 2003 : loss : 0.034705, loss_ce: 0.016390
2022-01-06 13:22:43,931 iteration 2004 : loss : 0.069581, loss_ce: 0.020480
2022-01-06 13:22:45,072 iteration 2005 : loss : 0.055115, loss_ce: 0.019569
2022-01-06 13:22:46,220 iteration 2006 : loss : 0.064451, loss_ce: 0.023661
 30%|████████▌                    | 118/400 [41:13<1:33:44, 19.94s/it]2022-01-06 13:22:47,460 iteration 2007 : loss : 0.047835, loss_ce: 0.019154
2022-01-06 13:22:48,706 iteration 2008 : loss : 0.062934, loss_ce: 0.023575
2022-01-06 13:22:49,821 iteration 2009 : loss : 0.052862, loss_ce: 0.020545
2022-01-06 13:22:50,927 iteration 2010 : loss : 0.053883, loss_ce: 0.022421
2022-01-06 13:22:52,136 iteration 2011 : loss : 0.081643, loss_ce: 0.030990
2022-01-06 13:22:53,263 iteration 2012 : loss : 0.042415, loss_ce: 0.020960
2022-01-06 13:22:54,296 iteration 2013 : loss : 0.040025, loss_ce: 0.016010
2022-01-06 13:22:55,381 iteration 2014 : loss : 0.058485, loss_ce: 0.017641
2022-01-06 13:22:56,530 iteration 2015 : loss : 0.064912, loss_ce: 0.028677
2022-01-06 13:22:57,630 iteration 2016 : loss : 0.069599, loss_ce: 0.025224
2022-01-06 13:22:58,795 iteration 2017 : loss : 0.035035, loss_ce: 0.012148
2022-01-06 13:23:00,024 iteration 2018 : loss : 0.048894, loss_ce: 0.021499
2022-01-06 13:23:01,123 iteration 2019 : loss : 0.061700, loss_ce: 0.021638
2022-01-06 13:23:02,206 iteration 2020 : loss : 0.044681, loss_ce: 0.016629
2022-01-06 13:23:03,354 iteration 2021 : loss : 0.070783, loss_ce: 0.023015
2022-01-06 13:23:04,508 iteration 2022 : loss : 0.059658, loss_ce: 0.026012
2022-01-06 13:23:05,707 iteration 2023 : loss : 0.059980, loss_ce: 0.021514
 30%|████████▋                    | 119/400 [41:32<1:32:45, 19.81s/it]2022-01-06 13:23:06,836 iteration 2024 : loss : 0.052666, loss_ce: 0.015985
2022-01-06 13:23:07,961 iteration 2025 : loss : 0.060162, loss_ce: 0.033700
2022-01-06 13:23:09,122 iteration 2026 : loss : 0.066230, loss_ce: 0.025374
2022-01-06 13:23:10,231 iteration 2027 : loss : 0.050708, loss_ce: 0.014957
2022-01-06 13:23:11,291 iteration 2028 : loss : 0.051357, loss_ce: 0.028955
2022-01-06 13:23:12,382 iteration 2029 : loss : 0.044161, loss_ce: 0.017887
2022-01-06 13:23:13,459 iteration 2030 : loss : 0.046278, loss_ce: 0.017417
2022-01-06 13:23:14,574 iteration 2031 : loss : 0.045784, loss_ce: 0.015166
2022-01-06 13:23:15,788 iteration 2032 : loss : 0.055384, loss_ce: 0.023454
2022-01-06 13:23:17,032 iteration 2033 : loss : 0.061938, loss_ce: 0.026092
2022-01-06 13:23:18,114 iteration 2034 : loss : 0.043350, loss_ce: 0.017093
2022-01-06 13:23:19,243 iteration 2035 : loss : 0.079061, loss_ce: 0.025065
2022-01-06 13:23:20,415 iteration 2036 : loss : 0.043896, loss_ce: 0.018579
2022-01-06 13:23:21,535 iteration 2037 : loss : 0.084281, loss_ce: 0.022179
2022-01-06 13:23:22,700 iteration 2038 : loss : 0.060879, loss_ce: 0.024191
2022-01-06 13:23:23,935 iteration 2039 : loss : 0.060369, loss_ce: 0.021056
2022-01-06 13:23:23,936 Training Data Eval:
2022-01-06 13:23:29,439   Average segmentation loss on training set: 0.2093
2022-01-06 13:23:29,440 Validation Data Eval:
2022-01-06 13:23:31,353   Average segmentation loss on validation set: 0.2280
2022-01-06 13:23:32,563 iteration 2040 : loss : 0.069632, loss_ce: 0.025383
 30%|████████▋                    | 120/400 [41:59<1:42:19, 21.93s/it]2022-01-06 13:23:33,805 iteration 2041 : loss : 0.079784, loss_ce: 0.024683
2022-01-06 13:23:34,897 iteration 2042 : loss : 0.058812, loss_ce: 0.021782
2022-01-06 13:23:35,995 iteration 2043 : loss : 0.045188, loss_ce: 0.014000
2022-01-06 13:23:37,114 iteration 2044 : loss : 0.047376, loss_ce: 0.019175
2022-01-06 13:23:38,204 iteration 2045 : loss : 0.037874, loss_ce: 0.011958
2022-01-06 13:23:39,335 iteration 2046 : loss : 0.063719, loss_ce: 0.027592
2022-01-06 13:23:40,425 iteration 2047 : loss : 0.051700, loss_ce: 0.020255
2022-01-06 13:23:41,552 iteration 2048 : loss : 0.056905, loss_ce: 0.020785
2022-01-06 13:23:42,650 iteration 2049 : loss : 0.054185, loss_ce: 0.012923
2022-01-06 13:23:43,788 iteration 2050 : loss : 0.071577, loss_ce: 0.029383
2022-01-06 13:23:44,927 iteration 2051 : loss : 0.049830, loss_ce: 0.024351
2022-01-06 13:23:45,991 iteration 2052 : loss : 0.071938, loss_ce: 0.030195
2022-01-06 13:23:47,127 iteration 2053 : loss : 0.061296, loss_ce: 0.020246
2022-01-06 13:23:48,255 iteration 2054 : loss : 0.038492, loss_ce: 0.015571
2022-01-06 13:23:49,366 iteration 2055 : loss : 0.056931, loss_ce: 0.017898
2022-01-06 13:23:50,523 iteration 2056 : loss : 0.056364, loss_ce: 0.022659
2022-01-06 13:23:51,582 iteration 2057 : loss : 0.056418, loss_ce: 0.023114
 30%|████████▊                    | 121/400 [42:18<1:37:52, 21.05s/it]2022-01-06 13:23:52,752 iteration 2058 : loss : 0.045967, loss_ce: 0.020901
2022-01-06 13:23:53,911 iteration 2059 : loss : 0.068400, loss_ce: 0.029573
2022-01-06 13:23:54,934 iteration 2060 : loss : 0.055129, loss_ce: 0.022489
2022-01-06 13:23:56,101 iteration 2061 : loss : 0.073145, loss_ce: 0.032173
2022-01-06 13:23:57,196 iteration 2062 : loss : 0.070783, loss_ce: 0.029640
2022-01-06 13:23:58,299 iteration 2063 : loss : 0.106200, loss_ce: 0.028353
2022-01-06 13:23:59,358 iteration 2064 : loss : 0.039354, loss_ce: 0.014367
2022-01-06 13:24:00,456 iteration 2065 : loss : 0.041499, loss_ce: 0.012685
2022-01-06 13:24:01,573 iteration 2066 : loss : 0.050524, loss_ce: 0.019651
2022-01-06 13:24:02,733 iteration 2067 : loss : 0.041812, loss_ce: 0.015548
2022-01-06 13:24:03,927 iteration 2068 : loss : 0.049962, loss_ce: 0.021713
2022-01-06 13:24:05,039 iteration 2069 : loss : 0.040663, loss_ce: 0.012084
2022-01-06 13:24:06,213 iteration 2070 : loss : 0.049527, loss_ce: 0.021522
2022-01-06 13:24:07,406 iteration 2071 : loss : 0.064580, loss_ce: 0.022176
2022-01-06 13:24:08,539 iteration 2072 : loss : 0.074815, loss_ce: 0.033480
2022-01-06 13:24:09,721 iteration 2073 : loss : 0.057380, loss_ce: 0.019749
2022-01-06 13:24:10,882 iteration 2074 : loss : 0.050140, loss_ce: 0.025013
 30%|████████▊                    | 122/400 [42:37<1:35:06, 20.53s/it]2022-01-06 13:24:12,029 iteration 2075 : loss : 0.038284, loss_ce: 0.012616
2022-01-06 13:24:13,158 iteration 2076 : loss : 0.045741, loss_ce: 0.015435
2022-01-06 13:24:14,312 iteration 2077 : loss : 0.041409, loss_ce: 0.016744
2022-01-06 13:24:15,563 iteration 2078 : loss : 0.059415, loss_ce: 0.020671
2022-01-06 13:24:16,652 iteration 2079 : loss : 0.048054, loss_ce: 0.017786
2022-01-06 13:24:17,832 iteration 2080 : loss : 0.061546, loss_ce: 0.019780
2022-01-06 13:24:18,854 iteration 2081 : loss : 0.048488, loss_ce: 0.016741
2022-01-06 13:24:19,861 iteration 2082 : loss : 0.030844, loss_ce: 0.011910
2022-01-06 13:24:20,967 iteration 2083 : loss : 0.062032, loss_ce: 0.033761
2022-01-06 13:24:22,073 iteration 2084 : loss : 0.062351, loss_ce: 0.029988
2022-01-06 13:24:23,145 iteration 2085 : loss : 0.048246, loss_ce: 0.012728
2022-01-06 13:24:24,257 iteration 2086 : loss : 0.065926, loss_ce: 0.025800
2022-01-06 13:24:25,337 iteration 2087 : loss : 0.048922, loss_ce: 0.019820
2022-01-06 13:24:26,528 iteration 2088 : loss : 0.050504, loss_ce: 0.020019
2022-01-06 13:24:27,696 iteration 2089 : loss : 0.046532, loss_ce: 0.021251
2022-01-06 13:24:28,740 iteration 2090 : loss : 0.041160, loss_ce: 0.015693
2022-01-06 13:24:29,901 iteration 2091 : loss : 0.041307, loss_ce: 0.017468
 31%|████████▉                    | 123/400 [42:56<1:32:40, 20.07s/it]2022-01-06 13:24:31,004 iteration 2092 : loss : 0.036174, loss_ce: 0.015564
2022-01-06 13:24:32,101 iteration 2093 : loss : 0.056985, loss_ce: 0.022780
2022-01-06 13:24:33,225 iteration 2094 : loss : 0.054694, loss_ce: 0.014841
2022-01-06 13:24:34,284 iteration 2095 : loss : 0.034898, loss_ce: 0.015405
2022-01-06 13:24:35,490 iteration 2096 : loss : 0.055217, loss_ce: 0.025872
2022-01-06 13:24:36,624 iteration 2097 : loss : 0.069186, loss_ce: 0.025223
2022-01-06 13:24:37,747 iteration 2098 : loss : 0.056890, loss_ce: 0.022666
2022-01-06 13:24:38,894 iteration 2099 : loss : 0.039656, loss_ce: 0.011906
2022-01-06 13:24:40,052 iteration 2100 : loss : 0.057121, loss_ce: 0.029980
2022-01-06 13:24:41,196 iteration 2101 : loss : 0.042788, loss_ce: 0.013224
2022-01-06 13:24:42,384 iteration 2102 : loss : 0.052836, loss_ce: 0.025616
2022-01-06 13:24:43,419 iteration 2103 : loss : 0.039951, loss_ce: 0.018331
2022-01-06 13:24:44,471 iteration 2104 : loss : 0.061720, loss_ce: 0.019544
2022-01-06 13:24:45,681 iteration 2105 : loss : 0.060540, loss_ce: 0.019654
2022-01-06 13:24:46,760 iteration 2106 : loss : 0.034779, loss_ce: 0.015488
2022-01-06 13:24:47,968 iteration 2107 : loss : 0.039954, loss_ce: 0.016089
2022-01-06 13:24:49,103 iteration 2108 : loss : 0.071691, loss_ce: 0.030093
 31%|████████▉                    | 124/400 [43:16<1:31:08, 19.81s/it]2022-01-06 13:24:50,276 iteration 2109 : loss : 0.058119, loss_ce: 0.024105
2022-01-06 13:24:51,432 iteration 2110 : loss : 0.045900, loss_ce: 0.017065
2022-01-06 13:24:52,643 iteration 2111 : loss : 0.053044, loss_ce: 0.017931
2022-01-06 13:24:53,774 iteration 2112 : loss : 0.059132, loss_ce: 0.023031
2022-01-06 13:24:54,853 iteration 2113 : loss : 0.050314, loss_ce: 0.022633
2022-01-06 13:24:56,007 iteration 2114 : loss : 0.048335, loss_ce: 0.021582
2022-01-06 13:24:57,151 iteration 2115 : loss : 0.044039, loss_ce: 0.013033
2022-01-06 13:24:58,288 iteration 2116 : loss : 0.036803, loss_ce: 0.013378
2022-01-06 13:24:59,423 iteration 2117 : loss : 0.041166, loss_ce: 0.015522
2022-01-06 13:25:00,584 iteration 2118 : loss : 0.049440, loss_ce: 0.015674
2022-01-06 13:25:01,769 iteration 2119 : loss : 0.066562, loss_ce: 0.027211
2022-01-06 13:25:02,891 iteration 2120 : loss : 0.046364, loss_ce: 0.014398
2022-01-06 13:25:03,909 iteration 2121 : loss : 0.055841, loss_ce: 0.023874
2022-01-06 13:25:05,008 iteration 2122 : loss : 0.050698, loss_ce: 0.017194
2022-01-06 13:25:06,193 iteration 2123 : loss : 0.072215, loss_ce: 0.031618
2022-01-06 13:25:07,395 iteration 2124 : loss : 0.051709, loss_ce: 0.020337
2022-01-06 13:25:07,395 Training Data Eval:
2022-01-06 13:25:12,759   Average segmentation loss on training set: 0.0817
2022-01-06 13:25:12,760 Validation Data Eval:
2022-01-06 13:25:14,666   Average segmentation loss on validation set: 0.1511
2022-01-06 13:25:15,836 iteration 2125 : loss : 0.059659, loss_ce: 0.028747
 31%|█████████                    | 125/400 [43:42<1:40:19, 21.89s/it]2022-01-06 13:25:17,090 iteration 2126 : loss : 0.039012, loss_ce: 0.014045
2022-01-06 13:25:18,251 iteration 2127 : loss : 0.051525, loss_ce: 0.023178
2022-01-06 13:25:19,404 iteration 2128 : loss : 0.068983, loss_ce: 0.023041
2022-01-06 13:25:20,482 iteration 2129 : loss : 0.056286, loss_ce: 0.020718
2022-01-06 13:25:21,640 iteration 2130 : loss : 0.081179, loss_ce: 0.025828
2022-01-06 13:25:22,745 iteration 2131 : loss : 0.055215, loss_ce: 0.017704
2022-01-06 13:25:23,864 iteration 2132 : loss : 0.041315, loss_ce: 0.017636
2022-01-06 13:25:25,063 iteration 2133 : loss : 0.063387, loss_ce: 0.021363
2022-01-06 13:25:26,110 iteration 2134 : loss : 0.043853, loss_ce: 0.021407
2022-01-06 13:25:27,228 iteration 2135 : loss : 0.056997, loss_ce: 0.023121
2022-01-06 13:25:28,335 iteration 2136 : loss : 0.048544, loss_ce: 0.020660
2022-01-06 13:25:29,456 iteration 2137 : loss : 0.058003, loss_ce: 0.024605
2022-01-06 13:25:30,533 iteration 2138 : loss : 0.054655, loss_ce: 0.020012
2022-01-06 13:25:31,692 iteration 2139 : loss : 0.039642, loss_ce: 0.019591
2022-01-06 13:25:32,815 iteration 2140 : loss : 0.063171, loss_ce: 0.027255
2022-01-06 13:25:34,010 iteration 2141 : loss : 0.062453, loss_ce: 0.019620
2022-01-06 13:25:35,091 iteration 2142 : loss : 0.048101, loss_ce: 0.015069
 32%|█████████▏                   | 126/400 [44:02<1:36:21, 21.10s/it]2022-01-06 13:25:36,191 iteration 2143 : loss : 0.043817, loss_ce: 0.016577
2022-01-06 13:25:37,332 iteration 2144 : loss : 0.045081, loss_ce: 0.014084
2022-01-06 13:25:38,486 iteration 2145 : loss : 0.068178, loss_ce: 0.022160
2022-01-06 13:25:39,544 iteration 2146 : loss : 0.033197, loss_ce: 0.013662
2022-01-06 13:25:40,681 iteration 2147 : loss : 0.060826, loss_ce: 0.019709
2022-01-06 13:25:41,772 iteration 2148 : loss : 0.051768, loss_ce: 0.019764
2022-01-06 13:25:42,887 iteration 2149 : loss : 0.055073, loss_ce: 0.019782
2022-01-06 13:25:44,010 iteration 2150 : loss : 0.043438, loss_ce: 0.018514
2022-01-06 13:25:45,208 iteration 2151 : loss : 0.066852, loss_ce: 0.027818
2022-01-06 13:25:46,283 iteration 2152 : loss : 0.038605, loss_ce: 0.016027
2022-01-06 13:25:47,338 iteration 2153 : loss : 0.037281, loss_ce: 0.014013
2022-01-06 13:25:48,608 iteration 2154 : loss : 0.058065, loss_ce: 0.021753
2022-01-06 13:25:49,729 iteration 2155 : loss : 0.039851, loss_ce: 0.015303
2022-01-06 13:25:50,794 iteration 2156 : loss : 0.057083, loss_ce: 0.024544
2022-01-06 13:25:52,073 iteration 2157 : loss : 0.099534, loss_ce: 0.027338
2022-01-06 13:25:53,312 iteration 2158 : loss : 0.058737, loss_ce: 0.026362
2022-01-06 13:25:54,468 iteration 2159 : loss : 0.072780, loss_ce: 0.024955
 32%|█████████▏                   | 127/400 [44:21<1:33:38, 20.58s/it]2022-01-06 13:25:55,608 iteration 2160 : loss : 0.066976, loss_ce: 0.029657
2022-01-06 13:25:56,662 iteration 2161 : loss : 0.052115, loss_ce: 0.019246
2022-01-06 13:25:57,768 iteration 2162 : loss : 0.048507, loss_ce: 0.018980
2022-01-06 13:25:58,847 iteration 2163 : loss : 0.039773, loss_ce: 0.016421
2022-01-06 13:26:00,007 iteration 2164 : loss : 0.045326, loss_ce: 0.021428
2022-01-06 13:26:01,141 iteration 2165 : loss : 0.076338, loss_ce: 0.026982
2022-01-06 13:26:02,214 iteration 2166 : loss : 0.048756, loss_ce: 0.021532
2022-01-06 13:26:03,240 iteration 2167 : loss : 0.053873, loss_ce: 0.022490
2022-01-06 13:26:04,314 iteration 2168 : loss : 0.037265, loss_ce: 0.014992
2022-01-06 13:26:05,460 iteration 2169 : loss : 0.056840, loss_ce: 0.019496
2022-01-06 13:26:06,457 iteration 2170 : loss : 0.040328, loss_ce: 0.011834
2022-01-06 13:26:07,546 iteration 2171 : loss : 0.057626, loss_ce: 0.020241
2022-01-06 13:26:08,631 iteration 2172 : loss : 0.070267, loss_ce: 0.023106
2022-01-06 13:26:09,714 iteration 2173 : loss : 0.052781, loss_ce: 0.014832
2022-01-06 13:26:10,854 iteration 2174 : loss : 0.065166, loss_ce: 0.027148
2022-01-06 13:26:12,092 iteration 2175 : loss : 0.039474, loss_ce: 0.014661
2022-01-06 13:26:13,121 iteration 2176 : loss : 0.042476, loss_ce: 0.020031
 32%|█████████▎                   | 128/400 [44:40<1:30:41, 20.01s/it]2022-01-06 13:26:14,247 iteration 2177 : loss : 0.066257, loss_ce: 0.036881
2022-01-06 13:26:15,386 iteration 2178 : loss : 0.048193, loss_ce: 0.014399
2022-01-06 13:26:16,611 iteration 2179 : loss : 0.071862, loss_ce: 0.023485
2022-01-06 13:26:17,788 iteration 2180 : loss : 0.043575, loss_ce: 0.020257
2022-01-06 13:26:18,836 iteration 2181 : loss : 0.049150, loss_ce: 0.016544
2022-01-06 13:26:19,964 iteration 2182 : loss : 0.056539, loss_ce: 0.019679
2022-01-06 13:26:21,118 iteration 2183 : loss : 0.059814, loss_ce: 0.018260
2022-01-06 13:26:22,189 iteration 2184 : loss : 0.056274, loss_ce: 0.025502
2022-01-06 13:26:23,268 iteration 2185 : loss : 0.044227, loss_ce: 0.015691
2022-01-06 13:26:24,485 iteration 2186 : loss : 0.061816, loss_ce: 0.024562
2022-01-06 13:26:25,715 iteration 2187 : loss : 0.051321, loss_ce: 0.015957
2022-01-06 13:26:27,015 iteration 2188 : loss : 0.054571, loss_ce: 0.026439
2022-01-06 13:26:28,296 iteration 2189 : loss : 0.054418, loss_ce: 0.018042
2022-01-06 13:26:29,442 iteration 2190 : loss : 0.059900, loss_ce: 0.025800
2022-01-06 13:26:30,596 iteration 2191 : loss : 0.042557, loss_ce: 0.017233
2022-01-06 13:26:31,688 iteration 2192 : loss : 0.040040, loss_ce: 0.014129
2022-01-06 13:26:32,833 iteration 2193 : loss : 0.042497, loss_ce: 0.013732
 32%|█████████▎                   | 129/400 [44:59<1:29:57, 19.92s/it]2022-01-06 13:26:34,037 iteration 2194 : loss : 0.050741, loss_ce: 0.018995
2022-01-06 13:26:35,188 iteration 2195 : loss : 0.045668, loss_ce: 0.023278
2022-01-06 13:26:36,369 iteration 2196 : loss : 0.043769, loss_ce: 0.018052
2022-01-06 13:26:37,590 iteration 2197 : loss : 0.053539, loss_ce: 0.020006
2022-01-06 13:26:38,722 iteration 2198 : loss : 0.052123, loss_ce: 0.016225
2022-01-06 13:26:39,866 iteration 2199 : loss : 0.050282, loss_ce: 0.021243
2022-01-06 13:26:41,090 iteration 2200 : loss : 0.102581, loss_ce: 0.021709
2022-01-06 13:26:42,157 iteration 2201 : loss : 0.046546, loss_ce: 0.018107
2022-01-06 13:26:43,405 iteration 2202 : loss : 0.052152, loss_ce: 0.020693
2022-01-06 13:26:44,556 iteration 2203 : loss : 0.073001, loss_ce: 0.019036
2022-01-06 13:26:45,608 iteration 2204 : loss : 0.040774, loss_ce: 0.012570
2022-01-06 13:26:46,742 iteration 2205 : loss : 0.066218, loss_ce: 0.027268
2022-01-06 13:26:47,938 iteration 2206 : loss : 0.054092, loss_ce: 0.020004
2022-01-06 13:26:49,114 iteration 2207 : loss : 0.047443, loss_ce: 0.017496
2022-01-06 13:26:50,310 iteration 2208 : loss : 0.047368, loss_ce: 0.013266
2022-01-06 13:26:51,445 iteration 2209 : loss : 0.076482, loss_ce: 0.027245
2022-01-06 13:26:51,445 Training Data Eval:
2022-01-06 13:26:57,133   Average segmentation loss on training set: 0.1143
2022-01-06 13:26:57,133 Validation Data Eval:
2022-01-06 13:26:59,081   Average segmentation loss on validation set: 0.1848
2022-01-06 13:27:00,266 iteration 2210 : loss : 0.051749, loss_ce: 0.025659
 32%|█████████▍                   | 130/400 [45:27<1:39:46, 22.17s/it]2022-01-06 13:27:01,470 iteration 2211 : loss : 0.064594, loss_ce: 0.017201
2022-01-06 13:27:02,555 iteration 2212 : loss : 0.057891, loss_ce: 0.025740
2022-01-06 13:27:03,694 iteration 2213 : loss : 0.088836, loss_ce: 0.018685
2022-01-06 13:27:04,745 iteration 2214 : loss : 0.059043, loss_ce: 0.020234
2022-01-06 13:27:05,803 iteration 2215 : loss : 0.033928, loss_ce: 0.013115
2022-01-06 13:27:06,905 iteration 2216 : loss : 0.045465, loss_ce: 0.020281
2022-01-06 13:27:07,974 iteration 2217 : loss : 0.053340, loss_ce: 0.023978
2022-01-06 13:27:09,080 iteration 2218 : loss : 0.043858, loss_ce: 0.014844
2022-01-06 13:27:10,227 iteration 2219 : loss : 0.068295, loss_ce: 0.033374
2022-01-06 13:27:11,436 iteration 2220 : loss : 0.057334, loss_ce: 0.026882
2022-01-06 13:27:12,629 iteration 2221 : loss : 0.066082, loss_ce: 0.023961
2022-01-06 13:27:13,645 iteration 2222 : loss : 0.048157, loss_ce: 0.019348
2022-01-06 13:27:14,697 iteration 2223 : loss : 0.054950, loss_ce: 0.020097
2022-01-06 13:27:15,900 iteration 2224 : loss : 0.057339, loss_ce: 0.023001
2022-01-06 13:27:17,041 iteration 2225 : loss : 0.063361, loss_ce: 0.021358
2022-01-06 13:27:18,238 iteration 2226 : loss : 0.048466, loss_ce: 0.023289
2022-01-06 13:27:19,409 iteration 2227 : loss : 0.055336, loss_ce: 0.017408
 33%|█████████▍                   | 131/400 [45:46<1:35:19, 21.26s/it]2022-01-06 13:27:20,673 iteration 2228 : loss : 0.067703, loss_ce: 0.029057
2022-01-06 13:27:21,809 iteration 2229 : loss : 0.055449, loss_ce: 0.018793
2022-01-06 13:27:23,086 iteration 2230 : loss : 0.068118, loss_ce: 0.025096
2022-01-06 13:27:24,203 iteration 2231 : loss : 0.092654, loss_ce: 0.055857
2022-01-06 13:27:25,307 iteration 2232 : loss : 0.069662, loss_ce: 0.033990
2022-01-06 13:27:26,632 iteration 2233 : loss : 0.133330, loss_ce: 0.038741
2022-01-06 13:27:27,791 iteration 2234 : loss : 0.060075, loss_ce: 0.017703
2022-01-06 13:27:28,916 iteration 2235 : loss : 0.052017, loss_ce: 0.019781
2022-01-06 13:27:30,019 iteration 2236 : loss : 0.041391, loss_ce: 0.017666
2022-01-06 13:27:31,156 iteration 2237 : loss : 0.058339, loss_ce: 0.025517
2022-01-06 13:27:32,227 iteration 2238 : loss : 0.055343, loss_ce: 0.016159
2022-01-06 13:27:33,327 iteration 2239 : loss : 0.071313, loss_ce: 0.021162
2022-01-06 13:27:34,370 iteration 2240 : loss : 0.066294, loss_ce: 0.018803
2022-01-06 13:27:35,496 iteration 2241 : loss : 0.058433, loss_ce: 0.017603
2022-01-06 13:27:36,694 iteration 2242 : loss : 0.089278, loss_ce: 0.027613
2022-01-06 13:27:37,868 iteration 2243 : loss : 0.078095, loss_ce: 0.032699
2022-01-06 13:27:38,976 iteration 2244 : loss : 0.032650, loss_ce: 0.010838
 33%|█████████▌                   | 132/400 [46:05<1:32:41, 20.75s/it]2022-01-06 13:27:40,142 iteration 2245 : loss : 0.040077, loss_ce: 0.012454
2022-01-06 13:27:41,163 iteration 2246 : loss : 0.045641, loss_ce: 0.012650
2022-01-06 13:27:42,303 iteration 2247 : loss : 0.049439, loss_ce: 0.021952
2022-01-06 13:27:43,503 iteration 2248 : loss : 0.070306, loss_ce: 0.027903
2022-01-06 13:27:44,625 iteration 2249 : loss : 0.039913, loss_ce: 0.016761
2022-01-06 13:27:45,754 iteration 2250 : loss : 0.047573, loss_ce: 0.016492
2022-01-06 13:27:46,908 iteration 2251 : loss : 0.061853, loss_ce: 0.025716
2022-01-06 13:27:48,031 iteration 2252 : loss : 0.059100, loss_ce: 0.019764
2022-01-06 13:27:49,258 iteration 2253 : loss : 0.066117, loss_ce: 0.028362
2022-01-06 13:27:50,439 iteration 2254 : loss : 0.041705, loss_ce: 0.017843
2022-01-06 13:27:51,547 iteration 2255 : loss : 0.047770, loss_ce: 0.017274
2022-01-06 13:27:52,652 iteration 2256 : loss : 0.040660, loss_ce: 0.017594
2022-01-06 13:27:53,716 iteration 2257 : loss : 0.065205, loss_ce: 0.023647
2022-01-06 13:27:54,786 iteration 2258 : loss : 0.069949, loss_ce: 0.023109
2022-01-06 13:27:55,970 iteration 2259 : loss : 0.045676, loss_ce: 0.017938
2022-01-06 13:27:57,014 iteration 2260 : loss : 0.044138, loss_ce: 0.017253
2022-01-06 13:27:58,154 iteration 2261 : loss : 0.057685, loss_ce: 0.027390
 33%|█████████▋                   | 133/400 [46:25<1:30:15, 20.28s/it]2022-01-06 13:27:59,277 iteration 2262 : loss : 0.049662, loss_ce: 0.024866
2022-01-06 13:28:00,469 iteration 2263 : loss : 0.059872, loss_ce: 0.019828
2022-01-06 13:28:01,688 iteration 2264 : loss : 0.042581, loss_ce: 0.012101
2022-01-06 13:28:02,835 iteration 2265 : loss : 0.057254, loss_ce: 0.018726
2022-01-06 13:28:03,922 iteration 2266 : loss : 0.047617, loss_ce: 0.020126
2022-01-06 13:28:05,014 iteration 2267 : loss : 0.039014, loss_ce: 0.015553
2022-01-06 13:28:06,138 iteration 2268 : loss : 0.041544, loss_ce: 0.016621
2022-01-06 13:28:07,221 iteration 2269 : loss : 0.044489, loss_ce: 0.023438
2022-01-06 13:28:08,334 iteration 2270 : loss : 0.057331, loss_ce: 0.019912
2022-01-06 13:28:09,422 iteration 2271 : loss : 0.039610, loss_ce: 0.017261
2022-01-06 13:28:10,524 iteration 2272 : loss : 0.064728, loss_ce: 0.030877
2022-01-06 13:28:11,670 iteration 2273 : loss : 0.043740, loss_ce: 0.015158
2022-01-06 13:28:12,813 iteration 2274 : loss : 0.041960, loss_ce: 0.013830
2022-01-06 13:28:14,005 iteration 2275 : loss : 0.069813, loss_ce: 0.022299
2022-01-06 13:28:15,060 iteration 2276 : loss : 0.039376, loss_ce: 0.016314
2022-01-06 13:28:16,183 iteration 2277 : loss : 0.046345, loss_ce: 0.016785
2022-01-06 13:28:17,251 iteration 2278 : loss : 0.043705, loss_ce: 0.014812
 34%|█████████▋                   | 134/400 [46:44<1:28:20, 19.92s/it]2022-01-06 13:28:18,362 iteration 2279 : loss : 0.108737, loss_ce: 0.042463
2022-01-06 13:28:19,485 iteration 2280 : loss : 0.042504, loss_ce: 0.017377
2022-01-06 13:28:20,598 iteration 2281 : loss : 0.088461, loss_ce: 0.024433
2022-01-06 13:28:21,743 iteration 2282 : loss : 0.041713, loss_ce: 0.015489
2022-01-06 13:28:22,943 iteration 2283 : loss : 0.050758, loss_ce: 0.020585
2022-01-06 13:28:24,037 iteration 2284 : loss : 0.050365, loss_ce: 0.021323
2022-01-06 13:28:25,134 iteration 2285 : loss : 0.045348, loss_ce: 0.017901
2022-01-06 13:28:26,233 iteration 2286 : loss : 0.043940, loss_ce: 0.016053
2022-01-06 13:28:27,335 iteration 2287 : loss : 0.043826, loss_ce: 0.018161
2022-01-06 13:28:28,517 iteration 2288 : loss : 0.040211, loss_ce: 0.014972
2022-01-06 13:28:29,573 iteration 2289 : loss : 0.057085, loss_ce: 0.016492
2022-01-06 13:28:30,756 iteration 2290 : loss : 0.048602, loss_ce: 0.022708
2022-01-06 13:28:31,857 iteration 2291 : loss : 0.033486, loss_ce: 0.012942
2022-01-06 13:28:32,947 iteration 2292 : loss : 0.051045, loss_ce: 0.021652
2022-01-06 13:28:33,971 iteration 2293 : loss : 0.051372, loss_ce: 0.025260
2022-01-06 13:28:35,118 iteration 2294 : loss : 0.088750, loss_ce: 0.029570
2022-01-06 13:28:35,119 Training Data Eval:
2022-01-06 13:28:40,636   Average segmentation loss on training set: 0.1257
2022-01-06 13:28:40,636 Validation Data Eval:
2022-01-06 13:28:42,543   Average segmentation loss on validation set: 0.2819
2022-01-06 13:28:43,756 iteration 2295 : loss : 0.044596, loss_ce: 0.017477
 34%|█████████▊                   | 135/400 [47:10<1:36:43, 21.90s/it]2022-01-06 13:28:44,859 iteration 2296 : loss : 0.031722, loss_ce: 0.012527
2022-01-06 13:28:45,976 iteration 2297 : loss : 0.043763, loss_ce: 0.017482
2022-01-06 13:28:47,086 iteration 2298 : loss : 0.034256, loss_ce: 0.014362
2022-01-06 13:28:48,284 iteration 2299 : loss : 0.062246, loss_ce: 0.029879
2022-01-06 13:28:49,366 iteration 2300 : loss : 0.047204, loss_ce: 0.020621
2022-01-06 13:28:50,566 iteration 2301 : loss : 0.041471, loss_ce: 0.012983
2022-01-06 13:28:51,730 iteration 2302 : loss : 0.068704, loss_ce: 0.021784
2022-01-06 13:28:52,862 iteration 2303 : loss : 0.056263, loss_ce: 0.021020
2022-01-06 13:28:54,068 iteration 2304 : loss : 0.063922, loss_ce: 0.023565
2022-01-06 13:28:55,232 iteration 2305 : loss : 0.041955, loss_ce: 0.015866
2022-01-06 13:28:56,459 iteration 2306 : loss : 0.062286, loss_ce: 0.030226
2022-01-06 13:28:57,556 iteration 2307 : loss : 0.063811, loss_ce: 0.021162
2022-01-06 13:28:58,636 iteration 2308 : loss : 0.052275, loss_ce: 0.016778
2022-01-06 13:28:59,721 iteration 2309 : loss : 0.047750, loss_ce: 0.020430
2022-01-06 13:29:00,915 iteration 2310 : loss : 0.047589, loss_ce: 0.020008
2022-01-06 13:29:01,954 iteration 2311 : loss : 0.065460, loss_ce: 0.021076
2022-01-06 13:29:03,084 iteration 2312 : loss : 0.061558, loss_ce: 0.022390
 34%|█████████▊                   | 136/400 [47:30<1:32:56, 21.12s/it]2022-01-06 13:29:04,300 iteration 2313 : loss : 0.056296, loss_ce: 0.021871
2022-01-06 13:29:05,307 iteration 2314 : loss : 0.040949, loss_ce: 0.012890
2022-01-06 13:29:06,366 iteration 2315 : loss : 0.044349, loss_ce: 0.016197
2022-01-06 13:29:07,595 iteration 2316 : loss : 0.055693, loss_ce: 0.019413
2022-01-06 13:29:08,660 iteration 2317 : loss : 0.047644, loss_ce: 0.015922
2022-01-06 13:29:09,762 iteration 2318 : loss : 0.045143, loss_ce: 0.022401
2022-01-06 13:29:10,894 iteration 2319 : loss : 0.059879, loss_ce: 0.025655
2022-01-06 13:29:12,104 iteration 2320 : loss : 0.051667, loss_ce: 0.020462
2022-01-06 13:29:13,224 iteration 2321 : loss : 0.063462, loss_ce: 0.029567
2022-01-06 13:29:14,356 iteration 2322 : loss : 0.046058, loss_ce: 0.018581
2022-01-06 13:29:15,384 iteration 2323 : loss : 0.034049, loss_ce: 0.012402
2022-01-06 13:29:16,499 iteration 2324 : loss : 0.034690, loss_ce: 0.014549
2022-01-06 13:29:17,506 iteration 2325 : loss : 0.036286, loss_ce: 0.015391
2022-01-06 13:29:18,610 iteration 2326 : loss : 0.061159, loss_ce: 0.011623
2022-01-06 13:29:19,692 iteration 2327 : loss : 0.045658, loss_ce: 0.017838
2022-01-06 13:29:20,809 iteration 2328 : loss : 0.048971, loss_ce: 0.021754
2022-01-06 13:29:21,874 iteration 2329 : loss : 0.038752, loss_ce: 0.015446
 34%|█████████▉                   | 137/400 [47:48<1:29:33, 20.43s/it]2022-01-06 13:29:23,066 iteration 2330 : loss : 0.050068, loss_ce: 0.018937
2022-01-06 13:29:24,137 iteration 2331 : loss : 0.039971, loss_ce: 0.014111
2022-01-06 13:29:25,336 iteration 2332 : loss : 0.040685, loss_ce: 0.013539
2022-01-06 13:29:26,452 iteration 2333 : loss : 0.057605, loss_ce: 0.026816
2022-01-06 13:29:27,706 iteration 2334 : loss : 0.040620, loss_ce: 0.015857
2022-01-06 13:29:28,939 iteration 2335 : loss : 0.058660, loss_ce: 0.024363
2022-01-06 13:29:30,139 iteration 2336 : loss : 0.039288, loss_ce: 0.016278
2022-01-06 13:29:31,262 iteration 2337 : loss : 0.071823, loss_ce: 0.027737
2022-01-06 13:29:32,416 iteration 2338 : loss : 0.062841, loss_ce: 0.025140
2022-01-06 13:29:33,557 iteration 2339 : loss : 0.076002, loss_ce: 0.027668
2022-01-06 13:29:34,609 iteration 2340 : loss : 0.045219, loss_ce: 0.018855
2022-01-06 13:29:35,737 iteration 2341 : loss : 0.043673, loss_ce: 0.018580
2022-01-06 13:29:36,883 iteration 2342 : loss : 0.043840, loss_ce: 0.018306
2022-01-06 13:29:38,131 iteration 2343 : loss : 0.036786, loss_ce: 0.012747
2022-01-06 13:29:39,204 iteration 2344 : loss : 0.045708, loss_ce: 0.011394
2022-01-06 13:29:40,282 iteration 2345 : loss : 0.064506, loss_ce: 0.033107
2022-01-06 13:29:41,370 iteration 2346 : loss : 0.054760, loss_ce: 0.023348
 34%|██████████                   | 138/400 [48:08<1:27:58, 20.15s/it]2022-01-06 13:29:42,618 iteration 2347 : loss : 0.056862, loss_ce: 0.024418
2022-01-06 13:29:43,758 iteration 2348 : loss : 0.044927, loss_ce: 0.014904
2022-01-06 13:29:44,850 iteration 2349 : loss : 0.047605, loss_ce: 0.016558
2022-01-06 13:29:45,948 iteration 2350 : loss : 0.062622, loss_ce: 0.020741
2022-01-06 13:29:47,130 iteration 2351 : loss : 0.051730, loss_ce: 0.021738
2022-01-06 13:29:48,209 iteration 2352 : loss : 0.041646, loss_ce: 0.019561
2022-01-06 13:29:49,290 iteration 2353 : loss : 0.038609, loss_ce: 0.017212
2022-01-06 13:29:50,354 iteration 2354 : loss : 0.029142, loss_ce: 0.012855
2022-01-06 13:29:51,431 iteration 2355 : loss : 0.045949, loss_ce: 0.016028
2022-01-06 13:29:52,598 iteration 2356 : loss : 0.049892, loss_ce: 0.021582
2022-01-06 13:29:53,724 iteration 2357 : loss : 0.060617, loss_ce: 0.016662
2022-01-06 13:29:54,858 iteration 2358 : loss : 0.044235, loss_ce: 0.018333
2022-01-06 13:29:55,941 iteration 2359 : loss : 0.054486, loss_ce: 0.017639
2022-01-06 13:29:57,004 iteration 2360 : loss : 0.064033, loss_ce: 0.016625
2022-01-06 13:29:58,114 iteration 2361 : loss : 0.045797, loss_ce: 0.014242
2022-01-06 13:29:59,359 iteration 2362 : loss : 0.058019, loss_ce: 0.023069
2022-01-06 13:30:00,484 iteration 2363 : loss : 0.035978, loss_ce: 0.016175
 35%|██████████                   | 139/400 [48:27<1:26:17, 19.84s/it]2022-01-06 13:30:01,760 iteration 2364 : loss : 0.040569, loss_ce: 0.014666
2022-01-06 13:30:02,971 iteration 2365 : loss : 0.044776, loss_ce: 0.016402
2022-01-06 13:30:04,150 iteration 2366 : loss : 0.057168, loss_ce: 0.023543
2022-01-06 13:30:05,213 iteration 2367 : loss : 0.042837, loss_ce: 0.010110
2022-01-06 13:30:06,273 iteration 2368 : loss : 0.040006, loss_ce: 0.014940
2022-01-06 13:30:07,390 iteration 2369 : loss : 0.044737, loss_ce: 0.013599
2022-01-06 13:30:08,492 iteration 2370 : loss : 0.041895, loss_ce: 0.017240
2022-01-06 13:30:09,642 iteration 2371 : loss : 0.044346, loss_ce: 0.017215
2022-01-06 13:30:10,708 iteration 2372 : loss : 0.039250, loss_ce: 0.011663
2022-01-06 13:30:11,805 iteration 2373 : loss : 0.048524, loss_ce: 0.017975
2022-01-06 13:30:12,857 iteration 2374 : loss : 0.049005, loss_ce: 0.019494
2022-01-06 13:30:13,934 iteration 2375 : loss : 0.037226, loss_ce: 0.012653
2022-01-06 13:30:15,073 iteration 2376 : loss : 0.046649, loss_ce: 0.020660
2022-01-06 13:30:16,202 iteration 2377 : loss : 0.047296, loss_ce: 0.019999
2022-01-06 13:30:17,331 iteration 2378 : loss : 0.035325, loss_ce: 0.013620
2022-01-06 13:30:18,480 iteration 2379 : loss : 0.052668, loss_ce: 0.024300
2022-01-06 13:30:18,481 Training Data Eval:
2022-01-06 13:30:23,946   Average segmentation loss on training set: 0.0792
2022-01-06 13:30:23,947 Validation Data Eval:
2022-01-06 13:30:25,882   Average segmentation loss on validation set: 0.2137
2022-01-06 13:30:26,989 iteration 2380 : loss : 0.036998, loss_ce: 0.016268
 35%|██████████▏                  | 140/400 [48:53<1:34:36, 21.83s/it]2022-01-06 13:30:28,135 iteration 2381 : loss : 0.063993, loss_ce: 0.019412
2022-01-06 13:30:29,315 iteration 2382 : loss : 0.042610, loss_ce: 0.013181
2022-01-06 13:30:30,404 iteration 2383 : loss : 0.034094, loss_ce: 0.011881
2022-01-06 13:30:31,589 iteration 2384 : loss : 0.059359, loss_ce: 0.021258
2022-01-06 13:30:32,773 iteration 2385 : loss : 0.064360, loss_ce: 0.029693
2022-01-06 13:30:33,879 iteration 2386 : loss : 0.038451, loss_ce: 0.012473
2022-01-06 13:30:34,961 iteration 2387 : loss : 0.045807, loss_ce: 0.015613
2022-01-06 13:30:36,081 iteration 2388 : loss : 0.052008, loss_ce: 0.022930
2022-01-06 13:30:37,133 iteration 2389 : loss : 0.022640, loss_ce: 0.007644
2022-01-06 13:30:38,267 iteration 2390 : loss : 0.051265, loss_ce: 0.026231
2022-01-06 13:30:39,359 iteration 2391 : loss : 0.041518, loss_ce: 0.023287
2022-01-06 13:30:40,506 iteration 2392 : loss : 0.044482, loss_ce: 0.016521
2022-01-06 13:30:41,652 iteration 2393 : loss : 0.057437, loss_ce: 0.016287
2022-01-06 13:30:42,669 iteration 2394 : loss : 0.042586, loss_ce: 0.014293
2022-01-06 13:30:43,850 iteration 2395 : loss : 0.056504, loss_ce: 0.027860
2022-01-06 13:30:44,924 iteration 2396 : loss : 0.036458, loss_ce: 0.016681
2022-01-06 13:30:46,065 iteration 2397 : loss : 0.040940, loss_ce: 0.014718
 35%|██████████▏                  | 141/400 [49:13<1:30:41, 21.01s/it]2022-01-06 13:30:47,226 iteration 2398 : loss : 0.051630, loss_ce: 0.021238
2022-01-06 13:30:48,307 iteration 2399 : loss : 0.046907, loss_ce: 0.015815
2022-01-06 13:30:49,482 iteration 2400 : loss : 0.038774, loss_ce: 0.018120
2022-01-06 13:30:50,598 iteration 2401 : loss : 0.036734, loss_ce: 0.011604
2022-01-06 13:30:51,774 iteration 2402 : loss : 0.037097, loss_ce: 0.017596
2022-01-06 13:30:52,858 iteration 2403 : loss : 0.038210, loss_ce: 0.014349
2022-01-06 13:30:53,916 iteration 2404 : loss : 0.046420, loss_ce: 0.018495
2022-01-06 13:30:54,933 iteration 2405 : loss : 0.048295, loss_ce: 0.017097
2022-01-06 13:30:56,180 iteration 2406 : loss : 0.044051, loss_ce: 0.013545
2022-01-06 13:30:57,229 iteration 2407 : loss : 0.047475, loss_ce: 0.025486
2022-01-06 13:30:58,272 iteration 2408 : loss : 0.039040, loss_ce: 0.019015
2022-01-06 13:30:59,431 iteration 2409 : loss : 0.033453, loss_ce: 0.010919
2022-01-06 13:31:00,605 iteration 2410 : loss : 0.042104, loss_ce: 0.016894
2022-01-06 13:31:01,740 iteration 2411 : loss : 0.050490, loss_ce: 0.016056
2022-01-06 13:31:02,812 iteration 2412 : loss : 0.032757, loss_ce: 0.012038
2022-01-06 13:31:04,007 iteration 2413 : loss : 0.052399, loss_ce: 0.021997
2022-01-06 13:31:05,129 iteration 2414 : loss : 0.040309, loss_ce: 0.016029
 36%|██████████▎                  | 142/400 [49:32<1:27:49, 20.43s/it]2022-01-06 13:31:06,318 iteration 2415 : loss : 0.052543, loss_ce: 0.017336
2022-01-06 13:31:07,412 iteration 2416 : loss : 0.031352, loss_ce: 0.010470
2022-01-06 13:31:08,523 iteration 2417 : loss : 0.032037, loss_ce: 0.010912
2022-01-06 13:31:09,742 iteration 2418 : loss : 0.049503, loss_ce: 0.016910
2022-01-06 13:31:10,870 iteration 2419 : loss : 0.047395, loss_ce: 0.022006
2022-01-06 13:31:12,100 iteration 2420 : loss : 0.056769, loss_ce: 0.023731
2022-01-06 13:31:13,302 iteration 2421 : loss : 0.048085, loss_ce: 0.015969
2022-01-06 13:31:14,486 iteration 2422 : loss : 0.046829, loss_ce: 0.018133
2022-01-06 13:31:15,605 iteration 2423 : loss : 0.048361, loss_ce: 0.019003
2022-01-06 13:31:16,703 iteration 2424 : loss : 0.048889, loss_ce: 0.021332
2022-01-06 13:31:17,946 iteration 2425 : loss : 0.051473, loss_ce: 0.015621
2022-01-06 13:31:19,041 iteration 2426 : loss : 0.053825, loss_ce: 0.015023
2022-01-06 13:31:20,064 iteration 2427 : loss : 0.042600, loss_ce: 0.017624
2022-01-06 13:31:21,137 iteration 2428 : loss : 0.037567, loss_ce: 0.011614
2022-01-06 13:31:22,208 iteration 2429 : loss : 0.042355, loss_ce: 0.016121
2022-01-06 13:31:23,282 iteration 2430 : loss : 0.024796, loss_ce: 0.009190
2022-01-06 13:31:24,385 iteration 2431 : loss : 0.050638, loss_ce: 0.025192
 36%|██████████▎                  | 143/400 [49:51<1:25:58, 20.07s/it]2022-01-06 13:31:25,511 iteration 2432 : loss : 0.046991, loss_ce: 0.014732
2022-01-06 13:31:26,608 iteration 2433 : loss : 0.042812, loss_ce: 0.016819
2022-01-06 13:31:27,730 iteration 2434 : loss : 0.048597, loss_ce: 0.015829
2022-01-06 13:31:28,948 iteration 2435 : loss : 0.063320, loss_ce: 0.033214
2022-01-06 13:31:29,983 iteration 2436 : loss : 0.035575, loss_ce: 0.013669
2022-01-06 13:31:31,150 iteration 2437 : loss : 0.032936, loss_ce: 0.011046
2022-01-06 13:31:32,329 iteration 2438 : loss : 0.051838, loss_ce: 0.016370
2022-01-06 13:31:33,496 iteration 2439 : loss : 0.031459, loss_ce: 0.011163
2022-01-06 13:31:34,670 iteration 2440 : loss : 0.043259, loss_ce: 0.018248
2022-01-06 13:31:35,772 iteration 2441 : loss : 0.031268, loss_ce: 0.010693
2022-01-06 13:31:36,863 iteration 2442 : loss : 0.036620, loss_ce: 0.014411
2022-01-06 13:31:37,907 iteration 2443 : loss : 0.033340, loss_ce: 0.012961
2022-01-06 13:31:39,023 iteration 2444 : loss : 0.050314, loss_ce: 0.018743
2022-01-06 13:31:40,166 iteration 2445 : loss : 0.037542, loss_ce: 0.014408
2022-01-06 13:31:41,382 iteration 2446 : loss : 0.054294, loss_ce: 0.027975
2022-01-06 13:31:42,437 iteration 2447 : loss : 0.038423, loss_ce: 0.012715
2022-01-06 13:31:43,610 iteration 2448 : loss : 0.035672, loss_ce: 0.013168
 36%|██████████▍                  | 144/400 [50:10<1:24:33, 19.82s/it]2022-01-06 13:31:44,714 iteration 2449 : loss : 0.051304, loss_ce: 0.017065
2022-01-06 13:31:45,762 iteration 2450 : loss : 0.033808, loss_ce: 0.017040
2022-01-06 13:31:46,847 iteration 2451 : loss : 0.040443, loss_ce: 0.012643
2022-01-06 13:31:47,916 iteration 2452 : loss : 0.041990, loss_ce: 0.013601
2022-01-06 13:31:49,036 iteration 2453 : loss : 0.049271, loss_ce: 0.016290
2022-01-06 13:31:50,130 iteration 2454 : loss : 0.045759, loss_ce: 0.015674
2022-01-06 13:31:51,203 iteration 2455 : loss : 0.048163, loss_ce: 0.021420
2022-01-06 13:31:52,262 iteration 2456 : loss : 0.036300, loss_ce: 0.015609
2022-01-06 13:31:53,367 iteration 2457 : loss : 0.036357, loss_ce: 0.013901
2022-01-06 13:31:54,437 iteration 2458 : loss : 0.039427, loss_ce: 0.014987
2022-01-06 13:31:55,464 iteration 2459 : loss : 0.031234, loss_ce: 0.011853
2022-01-06 13:31:56,554 iteration 2460 : loss : 0.044725, loss_ce: 0.013066
2022-01-06 13:31:57,750 iteration 2461 : loss : 0.038086, loss_ce: 0.013521
2022-01-06 13:31:58,899 iteration 2462 : loss : 0.049049, loss_ce: 0.015084
2022-01-06 13:31:59,994 iteration 2463 : loss : 0.035900, loss_ce: 0.015212
2022-01-06 13:32:01,137 iteration 2464 : loss : 0.033261, loss_ce: 0.013072
2022-01-06 13:32:01,137 Training Data Eval:
2022-01-06 13:32:06,660   Average segmentation loss on training set: 0.1349
2022-01-06 13:32:06,660 Validation Data Eval:
2022-01-06 13:32:08,581   Average segmentation loss on validation set: 0.3139
2022-01-06 13:32:09,699 iteration 2465 : loss : 0.044088, loss_ce: 0.016526
 36%|██████████▌                  | 145/400 [50:36<1:32:13, 21.70s/it]2022-01-06 13:32:10,950 iteration 2466 : loss : 0.053744, loss_ce: 0.023561
2022-01-06 13:32:12,155 iteration 2467 : loss : 0.045844, loss_ce: 0.022370
2022-01-06 13:32:13,252 iteration 2468 : loss : 0.040302, loss_ce: 0.015722
2022-01-06 13:32:14,313 iteration 2469 : loss : 0.032006, loss_ce: 0.014234
2022-01-06 13:32:15,411 iteration 2470 : loss : 0.046053, loss_ce: 0.016247
2022-01-06 13:32:16,598 iteration 2471 : loss : 0.079067, loss_ce: 0.030281
2022-01-06 13:32:17,685 iteration 2472 : loss : 0.066145, loss_ce: 0.027969
2022-01-06 13:32:18,864 iteration 2473 : loss : 0.049479, loss_ce: 0.019543
2022-01-06 13:32:19,944 iteration 2474 : loss : 0.040305, loss_ce: 0.014152
2022-01-06 13:32:21,056 iteration 2475 : loss : 0.045675, loss_ce: 0.015812
2022-01-06 13:32:22,115 iteration 2476 : loss : 0.046886, loss_ce: 0.025231
2022-01-06 13:32:23,318 iteration 2477 : loss : 0.044670, loss_ce: 0.018125
2022-01-06 13:32:24,465 iteration 2478 : loss : 0.045114, loss_ce: 0.016124
2022-01-06 13:32:25,491 iteration 2479 : loss : 0.032442, loss_ce: 0.011207
2022-01-06 13:32:26,612 iteration 2480 : loss : 0.043303, loss_ce: 0.014971
2022-01-06 13:32:27,786 iteration 2481 : loss : 0.038799, loss_ce: 0.015061
2022-01-06 13:32:28,875 iteration 2482 : loss : 0.033421, loss_ce: 0.016228
 36%|██████████▌                  | 146/400 [50:55<1:28:39, 20.94s/it]2022-01-06 13:32:30,141 iteration 2483 : loss : 0.045020, loss_ce: 0.018428
2022-01-06 13:32:31,281 iteration 2484 : loss : 0.065615, loss_ce: 0.015015
2022-01-06 13:32:32,345 iteration 2485 : loss : 0.032308, loss_ce: 0.014861
2022-01-06 13:32:33,443 iteration 2486 : loss : 0.037033, loss_ce: 0.014849
2022-01-06 13:32:34,545 iteration 2487 : loss : 0.060436, loss_ce: 0.019529
2022-01-06 13:32:35,716 iteration 2488 : loss : 0.046854, loss_ce: 0.017216
2022-01-06 13:32:36,812 iteration 2489 : loss : 0.044553, loss_ce: 0.021878
2022-01-06 13:32:37,972 iteration 2490 : loss : 0.050736, loss_ce: 0.021886
2022-01-06 13:32:39,171 iteration 2491 : loss : 0.041294, loss_ce: 0.018179
2022-01-06 13:32:40,187 iteration 2492 : loss : 0.031404, loss_ce: 0.012097
2022-01-06 13:32:41,315 iteration 2493 : loss : 0.038342, loss_ce: 0.016785
2022-01-06 13:32:42,399 iteration 2494 : loss : 0.039966, loss_ce: 0.011509
2022-01-06 13:32:43,479 iteration 2495 : loss : 0.035279, loss_ce: 0.013742
2022-01-06 13:32:44,787 iteration 2496 : loss : 0.049964, loss_ce: 0.016356
2022-01-06 13:32:45,944 iteration 2497 : loss : 0.047409, loss_ce: 0.014419
2022-01-06 13:32:47,120 iteration 2498 : loss : 0.041972, loss_ce: 0.016418
2022-01-06 13:32:48,166 iteration 2499 : loss : 0.028105, loss_ce: 0.007132
 37%|██████████▋                  | 147/400 [51:15<1:26:13, 20.45s/it]2022-01-06 13:32:49,314 iteration 2500 : loss : 0.039964, loss_ce: 0.014216
2022-01-06 13:32:50,410 iteration 2501 : loss : 0.035756, loss_ce: 0.010048
2022-01-06 13:32:51,601 iteration 2502 : loss : 0.049588, loss_ce: 0.018068
2022-01-06 13:32:52,650 iteration 2503 : loss : 0.036464, loss_ce: 0.014146
2022-01-06 13:32:53,717 iteration 2504 : loss : 0.027410, loss_ce: 0.010194
2022-01-06 13:32:54,873 iteration 2505 : loss : 0.045726, loss_ce: 0.017120
2022-01-06 13:32:55,972 iteration 2506 : loss : 0.039857, loss_ce: 0.013072
2022-01-06 13:32:57,024 iteration 2507 : loss : 0.046605, loss_ce: 0.018359
2022-01-06 13:32:58,098 iteration 2508 : loss : 0.038470, loss_ce: 0.012652
2022-01-06 13:32:59,187 iteration 2509 : loss : 0.039578, loss_ce: 0.013158
2022-01-06 13:33:00,257 iteration 2510 : loss : 0.035105, loss_ce: 0.014145
2022-01-06 13:33:01,374 iteration 2511 : loss : 0.045759, loss_ce: 0.020486
2022-01-06 13:33:02,519 iteration 2512 : loss : 0.042438, loss_ce: 0.015179
2022-01-06 13:33:03,693 iteration 2513 : loss : 0.063335, loss_ce: 0.017833
2022-01-06 13:33:04,798 iteration 2514 : loss : 0.050253, loss_ce: 0.015603
2022-01-06 13:33:05,915 iteration 2515 : loss : 0.031935, loss_ce: 0.016762
2022-01-06 13:33:07,019 iteration 2516 : loss : 0.030756, loss_ce: 0.013320
 37%|██████████▋                  | 148/400 [51:33<1:23:52, 19.97s/it]2022-01-06 13:33:08,175 iteration 2517 : loss : 0.033913, loss_ce: 0.013317
2022-01-06 13:33:09,310 iteration 2518 : loss : 0.042377, loss_ce: 0.013096
2022-01-06 13:33:10,464 iteration 2519 : loss : 0.056381, loss_ce: 0.024180
2022-01-06 13:33:11,598 iteration 2520 : loss : 0.050209, loss_ce: 0.021530
2022-01-06 13:33:12,712 iteration 2521 : loss : 0.034629, loss_ce: 0.012528
2022-01-06 13:33:13,845 iteration 2522 : loss : 0.050496, loss_ce: 0.024235
2022-01-06 13:33:15,022 iteration 2523 : loss : 0.063051, loss_ce: 0.016609
2022-01-06 13:33:16,127 iteration 2524 : loss : 0.034864, loss_ce: 0.015853
2022-01-06 13:33:17,261 iteration 2525 : loss : 0.031138, loss_ce: 0.012950
2022-01-06 13:33:18,407 iteration 2526 : loss : 0.062276, loss_ce: 0.023518
2022-01-06 13:33:19,527 iteration 2527 : loss : 0.035107, loss_ce: 0.011983
2022-01-06 13:33:20,676 iteration 2528 : loss : 0.051985, loss_ce: 0.017309
2022-01-06 13:33:21,807 iteration 2529 : loss : 0.055418, loss_ce: 0.019923
2022-01-06 13:33:22,949 iteration 2530 : loss : 0.037307, loss_ce: 0.017554
2022-01-06 13:33:24,158 iteration 2531 : loss : 0.040869, loss_ce: 0.016274
2022-01-06 13:33:25,277 iteration 2532 : loss : 0.054045, loss_ce: 0.017048
2022-01-06 13:33:26,410 iteration 2533 : loss : 0.052910, loss_ce: 0.016037
 37%|██████████▊                  | 149/400 [51:53<1:22:48, 19.79s/it]2022-01-06 13:33:27,507 iteration 2534 : loss : 0.039805, loss_ce: 0.018562
2022-01-06 13:33:28,706 iteration 2535 : loss : 0.046583, loss_ce: 0.016068
2022-01-06 13:33:29,781 iteration 2536 : loss : 0.037986, loss_ce: 0.019341
2022-01-06 13:33:30,870 iteration 2537 : loss : 0.037592, loss_ce: 0.015272
2022-01-06 13:33:31,870 iteration 2538 : loss : 0.033286, loss_ce: 0.015673
2022-01-06 13:33:33,008 iteration 2539 : loss : 0.043215, loss_ce: 0.015518
2022-01-06 13:33:34,072 iteration 2540 : loss : 0.038504, loss_ce: 0.011410
2022-01-06 13:33:35,166 iteration 2541 : loss : 0.039515, loss_ce: 0.016252
2022-01-06 13:33:36,223 iteration 2542 : loss : 0.025714, loss_ce: 0.009258
2022-01-06 13:33:37,276 iteration 2543 : loss : 0.037922, loss_ce: 0.012753
2022-01-06 13:33:38,391 iteration 2544 : loss : 0.049724, loss_ce: 0.017581
2022-01-06 13:33:39,638 iteration 2545 : loss : 0.043618, loss_ce: 0.016994
2022-01-06 13:33:40,898 iteration 2546 : loss : 0.046617, loss_ce: 0.020334
2022-01-06 13:33:41,950 iteration 2547 : loss : 0.029609, loss_ce: 0.013265
2022-01-06 13:33:43,099 iteration 2548 : loss : 0.038047, loss_ce: 0.015854
2022-01-06 13:33:44,229 iteration 2549 : loss : 0.052723, loss_ce: 0.016599
2022-01-06 13:33:44,230 Training Data Eval:
2022-01-06 13:33:49,718   Average segmentation loss on training set: 0.0729
2022-01-06 13:33:49,718 Validation Data Eval:
2022-01-06 13:33:51,619   Average segmentation loss on validation set: 0.1011
2022-01-06 13:33:57,515 Found new lowest validation loss at iteration 2549! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 13:33:58,623 iteration 2550 : loss : 0.055653, loss_ce: 0.023048
 38%|██████████▉                  | 150/400 [52:25<1:38:01, 23.52s/it]2022-01-06 13:33:59,767 iteration 2551 : loss : 0.035858, loss_ce: 0.016619
2022-01-06 13:34:00,941 iteration 2552 : loss : 0.034535, loss_ce: 0.014058
2022-01-06 13:34:01,963 iteration 2553 : loss : 0.029584, loss_ce: 0.010550
2022-01-06 13:34:03,061 iteration 2554 : loss : 0.033064, loss_ce: 0.013091
2022-01-06 13:34:04,158 iteration 2555 : loss : 0.041706, loss_ce: 0.011543
2022-01-06 13:34:05,320 iteration 2556 : loss : 0.044244, loss_ce: 0.018210
2022-01-06 13:34:06,411 iteration 2557 : loss : 0.044292, loss_ce: 0.020913
2022-01-06 13:34:07,534 iteration 2558 : loss : 0.043627, loss_ce: 0.023144
2022-01-06 13:34:08,743 iteration 2559 : loss : 0.042134, loss_ce: 0.015445
2022-01-06 13:34:09,845 iteration 2560 : loss : 0.037883, loss_ce: 0.018825
2022-01-06 13:34:11,078 iteration 2561 : loss : 0.040991, loss_ce: 0.014451
2022-01-06 13:34:12,214 iteration 2562 : loss : 0.042790, loss_ce: 0.016460
2022-01-06 13:34:13,369 iteration 2563 : loss : 0.041143, loss_ce: 0.017842
2022-01-06 13:34:14,432 iteration 2564 : loss : 0.040534, loss_ce: 0.017900
2022-01-06 13:34:15,497 iteration 2565 : loss : 0.047579, loss_ce: 0.016656
2022-01-06 13:34:16,585 iteration 2566 : loss : 0.056991, loss_ce: 0.023203
2022-01-06 13:34:17,713 iteration 2567 : loss : 0.059283, loss_ce: 0.016125
 38%|██████████▉                  | 151/400 [52:44<1:32:05, 22.19s/it]2022-01-06 13:34:18,910 iteration 2568 : loss : 0.036130, loss_ce: 0.015882
2022-01-06 13:34:20,001 iteration 2569 : loss : 0.036449, loss_ce: 0.017538
2022-01-06 13:34:21,089 iteration 2570 : loss : 0.045349, loss_ce: 0.018465
2022-01-06 13:34:22,181 iteration 2571 : loss : 0.027766, loss_ce: 0.010609
2022-01-06 13:34:23,235 iteration 2572 : loss : 0.037705, loss_ce: 0.011277
2022-01-06 13:34:24,286 iteration 2573 : loss : 0.044200, loss_ce: 0.016943
2022-01-06 13:34:25,372 iteration 2574 : loss : 0.056049, loss_ce: 0.018916
2022-01-06 13:34:26,602 iteration 2575 : loss : 0.052061, loss_ce: 0.020454
2022-01-06 13:34:27,737 iteration 2576 : loss : 0.030865, loss_ce: 0.012210
2022-01-06 13:34:28,858 iteration 2577 : loss : 0.057078, loss_ce: 0.021464
2022-01-06 13:34:29,980 iteration 2578 : loss : 0.067328, loss_ce: 0.021321
2022-01-06 13:34:31,029 iteration 2579 : loss : 0.054029, loss_ce: 0.013050
2022-01-06 13:34:32,196 iteration 2580 : loss : 0.055335, loss_ce: 0.012029
2022-01-06 13:34:33,357 iteration 2581 : loss : 0.049440, loss_ce: 0.018422
2022-01-06 13:34:34,549 iteration 2582 : loss : 0.038352, loss_ce: 0.013724
2022-01-06 13:34:35,647 iteration 2583 : loss : 0.035834, loss_ce: 0.012982
2022-01-06 13:34:36,669 iteration 2584 : loss : 0.033337, loss_ce: 0.013392
 38%|███████████                  | 152/400 [53:03<1:27:43, 21.22s/it]2022-01-06 13:34:37,839 iteration 2585 : loss : 0.032805, loss_ce: 0.011072
2022-01-06 13:34:38,954 iteration 2586 : loss : 0.071171, loss_ce: 0.026540
2022-01-06 13:34:39,997 iteration 2587 : loss : 0.042574, loss_ce: 0.015164
2022-01-06 13:34:41,079 iteration 2588 : loss : 0.044313, loss_ce: 0.019628
2022-01-06 13:34:42,231 iteration 2589 : loss : 0.042078, loss_ce: 0.012723
2022-01-06 13:34:43,391 iteration 2590 : loss : 0.035954, loss_ce: 0.013600
2022-01-06 13:34:44,463 iteration 2591 : loss : 0.038037, loss_ce: 0.011584
2022-01-06 13:34:45,537 iteration 2592 : loss : 0.042065, loss_ce: 0.014194
2022-01-06 13:34:46,610 iteration 2593 : loss : 0.037392, loss_ce: 0.015046
2022-01-06 13:34:47,873 iteration 2594 : loss : 0.048296, loss_ce: 0.019983
2022-01-06 13:34:49,034 iteration 2595 : loss : 0.051136, loss_ce: 0.019022
2022-01-06 13:34:50,100 iteration 2596 : loss : 0.033374, loss_ce: 0.014743
2022-01-06 13:34:51,154 iteration 2597 : loss : 0.040416, loss_ce: 0.014498
2022-01-06 13:34:52,236 iteration 2598 : loss : 0.046102, loss_ce: 0.022026
2022-01-06 13:34:53,328 iteration 2599 : loss : 0.049376, loss_ce: 0.012240
2022-01-06 13:34:54,425 iteration 2600 : loss : 0.050300, loss_ce: 0.016644
2022-01-06 13:34:55,478 iteration 2601 : loss : 0.037038, loss_ce: 0.014726
 38%|███████████                  | 153/400 [53:22<1:24:23, 20.50s/it]2022-01-06 13:34:56,670 iteration 2602 : loss : 0.040218, loss_ce: 0.018517
2022-01-06 13:34:57,772 iteration 2603 : loss : 0.036727, loss_ce: 0.014881
2022-01-06 13:34:58,850 iteration 2604 : loss : 0.037299, loss_ce: 0.013537
2022-01-06 13:35:00,082 iteration 2605 : loss : 0.035652, loss_ce: 0.011121
2022-01-06 13:35:01,197 iteration 2606 : loss : 0.041958, loss_ce: 0.016867
2022-01-06 13:35:02,320 iteration 2607 : loss : 0.050462, loss_ce: 0.017336
2022-01-06 13:35:03,373 iteration 2608 : loss : 0.036399, loss_ce: 0.015140
2022-01-06 13:35:04,441 iteration 2609 : loss : 0.041494, loss_ce: 0.012831
2022-01-06 13:35:05,676 iteration 2610 : loss : 0.048341, loss_ce: 0.018293
2022-01-06 13:35:06,759 iteration 2611 : loss : 0.038269, loss_ce: 0.012177
2022-01-06 13:35:07,766 iteration 2612 : loss : 0.043172, loss_ce: 0.020022
2022-01-06 13:35:08,839 iteration 2613 : loss : 0.045226, loss_ce: 0.013892
2022-01-06 13:35:10,012 iteration 2614 : loss : 0.048021, loss_ce: 0.015992
2022-01-06 13:35:11,086 iteration 2615 : loss : 0.032747, loss_ce: 0.013223
2022-01-06 13:35:12,173 iteration 2616 : loss : 0.041220, loss_ce: 0.016830
2022-01-06 13:35:13,377 iteration 2617 : loss : 0.035061, loss_ce: 0.009751
2022-01-06 13:35:14,527 iteration 2618 : loss : 0.051182, loss_ce: 0.025147
 38%|███████████▏                 | 154/400 [53:41<1:22:15, 20.06s/it]2022-01-06 13:35:15,768 iteration 2619 : loss : 0.051281, loss_ce: 0.014834
2022-01-06 13:35:16,865 iteration 2620 : loss : 0.037647, loss_ce: 0.014822
2022-01-06 13:35:18,018 iteration 2621 : loss : 0.043722, loss_ce: 0.016503
2022-01-06 13:35:19,170 iteration 2622 : loss : 0.039199, loss_ce: 0.019796
2022-01-06 13:35:20,331 iteration 2623 : loss : 0.050178, loss_ce: 0.021870
2022-01-06 13:35:21,418 iteration 2624 : loss : 0.039179, loss_ce: 0.012570
2022-01-06 13:35:22,519 iteration 2625 : loss : 0.038122, loss_ce: 0.015050
2022-01-06 13:35:23,717 iteration 2626 : loss : 0.041999, loss_ce: 0.019891
2022-01-06 13:35:24,878 iteration 2627 : loss : 0.068436, loss_ce: 0.019642
2022-01-06 13:35:25,969 iteration 2628 : loss : 0.040748, loss_ce: 0.014314
2022-01-06 13:35:27,169 iteration 2629 : loss : 0.048247, loss_ce: 0.017014
2022-01-06 13:35:28,247 iteration 2630 : loss : 0.028887, loss_ce: 0.010453
2022-01-06 13:35:29,393 iteration 2631 : loss : 0.041569, loss_ce: 0.018189
2022-01-06 13:35:30,544 iteration 2632 : loss : 0.044162, loss_ce: 0.018246
2022-01-06 13:35:31,650 iteration 2633 : loss : 0.037783, loss_ce: 0.018979
2022-01-06 13:35:32,799 iteration 2634 : loss : 0.050451, loss_ce: 0.017123
2022-01-06 13:35:32,799 Training Data Eval:
2022-01-06 13:35:38,387   Average segmentation loss on training set: 0.0392
2022-01-06 13:35:38,388 Validation Data Eval:
2022-01-06 13:35:40,295   Average segmentation loss on validation set: 0.1645
2022-01-06 13:35:41,491 iteration 2635 : loss : 0.043859, loss_ce: 0.017972
 39%|███████████▏                 | 155/400 [54:08<1:30:22, 22.13s/it]2022-01-06 13:35:42,625 iteration 2636 : loss : 0.031600, loss_ce: 0.009764
2022-01-06 13:35:43,877 iteration 2637 : loss : 0.048081, loss_ce: 0.015911
2022-01-06 13:35:45,010 iteration 2638 : loss : 0.038400, loss_ce: 0.013907
2022-01-06 13:35:46,019 iteration 2639 : loss : 0.026654, loss_ce: 0.009555
2022-01-06 13:35:47,098 iteration 2640 : loss : 0.036627, loss_ce: 0.013895
2022-01-06 13:35:48,324 iteration 2641 : loss : 0.043662, loss_ce: 0.016694
2022-01-06 13:35:49,500 iteration 2642 : loss : 0.049939, loss_ce: 0.015204
2022-01-06 13:35:50,656 iteration 2643 : loss : 0.037220, loss_ce: 0.013149
2022-01-06 13:35:51,740 iteration 2644 : loss : 0.046681, loss_ce: 0.013590
2022-01-06 13:35:52,820 iteration 2645 : loss : 0.031843, loss_ce: 0.016020
2022-01-06 13:35:53,913 iteration 2646 : loss : 0.048674, loss_ce: 0.014861
2022-01-06 13:35:55,077 iteration 2647 : loss : 0.046663, loss_ce: 0.021112
2022-01-06 13:35:56,102 iteration 2648 : loss : 0.024714, loss_ce: 0.011366
2022-01-06 13:35:57,259 iteration 2649 : loss : 0.040781, loss_ce: 0.017916
2022-01-06 13:35:58,411 iteration 2650 : loss : 0.036782, loss_ce: 0.014899
2022-01-06 13:35:59,500 iteration 2651 : loss : 0.052383, loss_ce: 0.017334
2022-01-06 13:36:00,620 iteration 2652 : loss : 0.035859, loss_ce: 0.016491
 39%|███████████▎                 | 156/400 [54:27<1:26:21, 21.23s/it]2022-01-06 13:36:01,901 iteration 2653 : loss : 0.043578, loss_ce: 0.013407
2022-01-06 13:36:03,015 iteration 2654 : loss : 0.036119, loss_ce: 0.013937
2022-01-06 13:36:04,074 iteration 2655 : loss : 0.028888, loss_ce: 0.010486
2022-01-06 13:36:05,112 iteration 2656 : loss : 0.028798, loss_ce: 0.012808
2022-01-06 13:36:06,170 iteration 2657 : loss : 0.030348, loss_ce: 0.010634
2022-01-06 13:36:07,246 iteration 2658 : loss : 0.032582, loss_ce: 0.012435
2022-01-06 13:36:08,491 iteration 2659 : loss : 0.043922, loss_ce: 0.019243
2022-01-06 13:36:09,599 iteration 2660 : loss : 0.042922, loss_ce: 0.016387
2022-01-06 13:36:10,622 iteration 2661 : loss : 0.034993, loss_ce: 0.012885
2022-01-06 13:36:11,780 iteration 2662 : loss : 0.053330, loss_ce: 0.020000
2022-01-06 13:36:12,839 iteration 2663 : loss : 0.042403, loss_ce: 0.016296
2022-01-06 13:36:13,998 iteration 2664 : loss : 0.032711, loss_ce: 0.011953
2022-01-06 13:36:15,095 iteration 2665 : loss : 0.035270, loss_ce: 0.013522
2022-01-06 13:36:16,231 iteration 2666 : loss : 0.040628, loss_ce: 0.012027
2022-01-06 13:36:17,397 iteration 2667 : loss : 0.032983, loss_ce: 0.013247
2022-01-06 13:36:18,544 iteration 2668 : loss : 0.048048, loss_ce: 0.017978
2022-01-06 13:36:19,712 iteration 2669 : loss : 0.043712, loss_ce: 0.016911
 39%|███████████▍                 | 157/400 [54:46<1:23:23, 20.59s/it]2022-01-06 13:36:20,914 iteration 2670 : loss : 0.068145, loss_ce: 0.018193
2022-01-06 13:36:22,043 iteration 2671 : loss : 0.049574, loss_ce: 0.013342
2022-01-06 13:36:23,186 iteration 2672 : loss : 0.033538, loss_ce: 0.011807
2022-01-06 13:36:24,361 iteration 2673 : loss : 0.050736, loss_ce: 0.019367
2022-01-06 13:36:25,434 iteration 2674 : loss : 0.040131, loss_ce: 0.019425
2022-01-06 13:36:26,601 iteration 2675 : loss : 0.045326, loss_ce: 0.016541
2022-01-06 13:36:27,732 iteration 2676 : loss : 0.036268, loss_ce: 0.009800
2022-01-06 13:36:28,782 iteration 2677 : loss : 0.025477, loss_ce: 0.011725
2022-01-06 13:36:29,818 iteration 2678 : loss : 0.034377, loss_ce: 0.011843
2022-01-06 13:36:30,960 iteration 2679 : loss : 0.041087, loss_ce: 0.018766
2022-01-06 13:36:32,136 iteration 2680 : loss : 0.040595, loss_ce: 0.018018
2022-01-06 13:36:33,237 iteration 2681 : loss : 0.035635, loss_ce: 0.017187
2022-01-06 13:36:34,297 iteration 2682 : loss : 0.035816, loss_ce: 0.016067
2022-01-06 13:36:35,427 iteration 2683 : loss : 0.038845, loss_ce: 0.020140
2022-01-06 13:36:36,545 iteration 2684 : loss : 0.057915, loss_ce: 0.020146
2022-01-06 13:36:37,674 iteration 2685 : loss : 0.042757, loss_ce: 0.019894
2022-01-06 13:36:38,786 iteration 2686 : loss : 0.033186, loss_ce: 0.012773
 40%|███████████▍                 | 158/400 [55:05<1:21:13, 20.14s/it]2022-01-06 13:36:39,942 iteration 2687 : loss : 0.023594, loss_ce: 0.010805
2022-01-06 13:36:41,039 iteration 2688 : loss : 0.040121, loss_ce: 0.015822
2022-01-06 13:36:42,039 iteration 2689 : loss : 0.028199, loss_ce: 0.011211
2022-01-06 13:36:43,136 iteration 2690 : loss : 0.031488, loss_ce: 0.013217
2022-01-06 13:36:44,265 iteration 2691 : loss : 0.028884, loss_ce: 0.009575
2022-01-06 13:36:45,374 iteration 2692 : loss : 0.030242, loss_ce: 0.009473
2022-01-06 13:36:46,446 iteration 2693 : loss : 0.042130, loss_ce: 0.016259
2022-01-06 13:36:47,471 iteration 2694 : loss : 0.035048, loss_ce: 0.012731
2022-01-06 13:36:48,638 iteration 2695 : loss : 0.037061, loss_ce: 0.013363
2022-01-06 13:36:49,870 iteration 2696 : loss : 0.053960, loss_ce: 0.021204
2022-01-06 13:36:51,053 iteration 2697 : loss : 0.043090, loss_ce: 0.023589
2022-01-06 13:36:52,102 iteration 2698 : loss : 0.032771, loss_ce: 0.013443
2022-01-06 13:36:53,173 iteration 2699 : loss : 0.036241, loss_ce: 0.013580
2022-01-06 13:36:54,239 iteration 2700 : loss : 0.041133, loss_ce: 0.012335
2022-01-06 13:36:55,350 iteration 2701 : loss : 0.044974, loss_ce: 0.021982
2022-01-06 13:36:56,502 iteration 2702 : loss : 0.055399, loss_ce: 0.012736
2022-01-06 13:36:57,658 iteration 2703 : loss : 0.046776, loss_ce: 0.016387
 40%|███████████▌                 | 159/400 [55:24<1:19:21, 19.76s/it]2022-01-06 13:36:58,781 iteration 2704 : loss : 0.036842, loss_ce: 0.011121
2022-01-06 13:36:59,841 iteration 2705 : loss : 0.034805, loss_ce: 0.011899
2022-01-06 13:37:00,987 iteration 2706 : loss : 0.049284, loss_ce: 0.023039
2022-01-06 13:37:02,089 iteration 2707 : loss : 0.026885, loss_ce: 0.009991
2022-01-06 13:37:03,176 iteration 2708 : loss : 0.051671, loss_ce: 0.016482
2022-01-06 13:37:04,353 iteration 2709 : loss : 0.048199, loss_ce: 0.016024
2022-01-06 13:37:05,512 iteration 2710 : loss : 0.053197, loss_ce: 0.022682
2022-01-06 13:37:06,593 iteration 2711 : loss : 0.033305, loss_ce: 0.011333
2022-01-06 13:37:07,697 iteration 2712 : loss : 0.056436, loss_ce: 0.018185
2022-01-06 13:37:08,826 iteration 2713 : loss : 0.035098, loss_ce: 0.011968
2022-01-06 13:37:09,945 iteration 2714 : loss : 0.044437, loss_ce: 0.017306
2022-01-06 13:37:11,046 iteration 2715 : loss : 0.040578, loss_ce: 0.020644
2022-01-06 13:37:12,148 iteration 2716 : loss : 0.051149, loss_ce: 0.013669
2022-01-06 13:37:13,206 iteration 2717 : loss : 0.033441, loss_ce: 0.011963
2022-01-06 13:37:14,354 iteration 2718 : loss : 0.037406, loss_ce: 0.014261
2022-01-06 13:37:15,445 iteration 2719 : loss : 0.029235, loss_ce: 0.013601
2022-01-06 13:37:15,446 Training Data Eval:
2022-01-06 13:37:20,997   Average segmentation loss on training set: 0.2647
2022-01-06 13:37:20,998 Validation Data Eval:
2022-01-06 13:37:22,900   Average segmentation loss on validation set: 0.4232
2022-01-06 13:37:24,004 iteration 2720 : loss : 0.033113, loss_ce: 0.013241
 40%|███████████▌                 | 160/400 [55:50<1:26:55, 21.73s/it]2022-01-06 13:37:25,191 iteration 2721 : loss : 0.036481, loss_ce: 0.011909
2022-01-06 13:37:26,340 iteration 2722 : loss : 0.033489, loss_ce: 0.016863
2022-01-06 13:37:27,538 iteration 2723 : loss : 0.050834, loss_ce: 0.024277
2022-01-06 13:37:28,596 iteration 2724 : loss : 0.039368, loss_ce: 0.014245
2022-01-06 13:37:29,688 iteration 2725 : loss : 0.039340, loss_ce: 0.012030
2022-01-06 13:37:30,889 iteration 2726 : loss : 0.029815, loss_ce: 0.010536
2022-01-06 13:37:32,000 iteration 2727 : loss : 0.043862, loss_ce: 0.016942
2022-01-06 13:37:33,156 iteration 2728 : loss : 0.033933, loss_ce: 0.013216
2022-01-06 13:37:34,303 iteration 2729 : loss : 0.057944, loss_ce: 0.027407
2022-01-06 13:37:35,479 iteration 2730 : loss : 0.049628, loss_ce: 0.017701
2022-01-06 13:37:36,652 iteration 2731 : loss : 0.051630, loss_ce: 0.020048
2022-01-06 13:37:37,666 iteration 2732 : loss : 0.030365, loss_ce: 0.011711
2022-01-06 13:37:38,766 iteration 2733 : loss : 0.037929, loss_ce: 0.016614
2022-01-06 13:37:39,896 iteration 2734 : loss : 0.065933, loss_ce: 0.015451
2022-01-06 13:37:41,020 iteration 2735 : loss : 0.043717, loss_ce: 0.022866
2022-01-06 13:37:42,109 iteration 2736 : loss : 0.037109, loss_ce: 0.011813
2022-01-06 13:37:43,207 iteration 2737 : loss : 0.053093, loss_ce: 0.014993
 40%|███████████▋                 | 161/400 [56:10<1:23:32, 20.97s/it]2022-01-06 13:37:44,420 iteration 2738 : loss : 0.067060, loss_ce: 0.031530
2022-01-06 13:37:45,476 iteration 2739 : loss : 0.044342, loss_ce: 0.011689
2022-01-06 13:37:46,678 iteration 2740 : loss : 0.093256, loss_ce: 0.038188
2022-01-06 13:37:47,781 iteration 2741 : loss : 0.055207, loss_ce: 0.024303
2022-01-06 13:37:48,901 iteration 2742 : loss : 0.047249, loss_ce: 0.021535
2022-01-06 13:37:49,947 iteration 2743 : loss : 0.050507, loss_ce: 0.018601
2022-01-06 13:37:51,048 iteration 2744 : loss : 0.033590, loss_ce: 0.011725
2022-01-06 13:37:52,114 iteration 2745 : loss : 0.035395, loss_ce: 0.011000
2022-01-06 13:37:53,333 iteration 2746 : loss : 0.095868, loss_ce: 0.025324
2022-01-06 13:37:54,407 iteration 2747 : loss : 0.047349, loss_ce: 0.019001
2022-01-06 13:37:55,561 iteration 2748 : loss : 0.047557, loss_ce: 0.012850
2022-01-06 13:37:56,751 iteration 2749 : loss : 0.052733, loss_ce: 0.017112
2022-01-06 13:37:57,840 iteration 2750 : loss : 0.045238, loss_ce: 0.017155
2022-01-06 13:37:58,948 iteration 2751 : loss : 0.051940, loss_ce: 0.015154
2022-01-06 13:38:00,166 iteration 2752 : loss : 0.051213, loss_ce: 0.019186
2022-01-06 13:38:01,391 iteration 2753 : loss : 0.050528, loss_ce: 0.018744
2022-01-06 13:38:02,523 iteration 2754 : loss : 0.036648, loss_ce: 0.018177
 40%|███████████▋                 | 162/400 [56:29<1:21:12, 20.47s/it]2022-01-06 13:38:03,560 iteration 2755 : loss : 0.032919, loss_ce: 0.010961
2022-01-06 13:38:04,688 iteration 2756 : loss : 0.037117, loss_ce: 0.016006
2022-01-06 13:38:05,797 iteration 2757 : loss : 0.037011, loss_ce: 0.015571
2022-01-06 13:38:06,899 iteration 2758 : loss : 0.039586, loss_ce: 0.014123
2022-01-06 13:38:08,023 iteration 2759 : loss : 0.038778, loss_ce: 0.014780
2022-01-06 13:38:09,140 iteration 2760 : loss : 0.043799, loss_ce: 0.018642
2022-01-06 13:38:10,285 iteration 2761 : loss : 0.056066, loss_ce: 0.020640
2022-01-06 13:38:11,388 iteration 2762 : loss : 0.043267, loss_ce: 0.018668
2022-01-06 13:38:12,534 iteration 2763 : loss : 0.069863, loss_ce: 0.020352
2022-01-06 13:38:13,580 iteration 2764 : loss : 0.031618, loss_ce: 0.011583
2022-01-06 13:38:14,805 iteration 2765 : loss : 0.028644, loss_ce: 0.009862
2022-01-06 13:38:15,843 iteration 2766 : loss : 0.031474, loss_ce: 0.010509
2022-01-06 13:38:16,986 iteration 2767 : loss : 0.043835, loss_ce: 0.018053
2022-01-06 13:38:18,075 iteration 2768 : loss : 0.053403, loss_ce: 0.018309
2022-01-06 13:38:19,118 iteration 2769 : loss : 0.034486, loss_ce: 0.014454
2022-01-06 13:38:20,206 iteration 2770 : loss : 0.044339, loss_ce: 0.020708
2022-01-06 13:38:21,326 iteration 2771 : loss : 0.037562, loss_ce: 0.017643
 41%|███████████▊                 | 163/400 [56:48<1:18:53, 19.97s/it]2022-01-06 13:38:22,436 iteration 2772 : loss : 0.047433, loss_ce: 0.015658
2022-01-06 13:38:23,572 iteration 2773 : loss : 0.033272, loss_ce: 0.013409
2022-01-06 13:38:24,704 iteration 2774 : loss : 0.043474, loss_ce: 0.010579
2022-01-06 13:38:25,843 iteration 2775 : loss : 0.048066, loss_ce: 0.018528
2022-01-06 13:38:26,937 iteration 2776 : loss : 0.035589, loss_ce: 0.014408
2022-01-06 13:38:28,112 iteration 2777 : loss : 0.029226, loss_ce: 0.010412
2022-01-06 13:38:29,178 iteration 2778 : loss : 0.031692, loss_ce: 0.010073
2022-01-06 13:38:30,249 iteration 2779 : loss : 0.032241, loss_ce: 0.012844
2022-01-06 13:38:31,345 iteration 2780 : loss : 0.033890, loss_ce: 0.015566
2022-01-06 13:38:32,585 iteration 2781 : loss : 0.035150, loss_ce: 0.012474
2022-01-06 13:38:33,785 iteration 2782 : loss : 0.035372, loss_ce: 0.016031
2022-01-06 13:38:34,884 iteration 2783 : loss : 0.041712, loss_ce: 0.014127
2022-01-06 13:38:35,877 iteration 2784 : loss : 0.035885, loss_ce: 0.010308
2022-01-06 13:38:36,924 iteration 2785 : loss : 0.030919, loss_ce: 0.013580
2022-01-06 13:38:37,966 iteration 2786 : loss : 0.029283, loss_ce: 0.012102
2022-01-06 13:38:39,036 iteration 2787 : loss : 0.031576, loss_ce: 0.013161
2022-01-06 13:38:40,289 iteration 2788 : loss : 0.046006, loss_ce: 0.016860
 41%|███████████▉                 | 164/400 [57:07<1:17:22, 19.67s/it]2022-01-06 13:38:41,484 iteration 2789 : loss : 0.036147, loss_ce: 0.012204
2022-01-06 13:38:42,690 iteration 2790 : loss : 0.060945, loss_ce: 0.021759
2022-01-06 13:38:43,730 iteration 2791 : loss : 0.026106, loss_ce: 0.007967
2022-01-06 13:38:44,814 iteration 2792 : loss : 0.035931, loss_ce: 0.015175
2022-01-06 13:38:45,935 iteration 2793 : loss : 0.042127, loss_ce: 0.013698
2022-01-06 13:38:47,109 iteration 2794 : loss : 0.042146, loss_ce: 0.020559
2022-01-06 13:38:48,207 iteration 2795 : loss : 0.043186, loss_ce: 0.016420
2022-01-06 13:38:49,286 iteration 2796 : loss : 0.058858, loss_ce: 0.019262
2022-01-06 13:38:50,445 iteration 2797 : loss : 0.036255, loss_ce: 0.015428
2022-01-06 13:38:51,576 iteration 2798 : loss : 0.041438, loss_ce: 0.014109
2022-01-06 13:38:52,783 iteration 2799 : loss : 0.048776, loss_ce: 0.017360
2022-01-06 13:38:53,910 iteration 2800 : loss : 0.036600, loss_ce: 0.017322
2022-01-06 13:38:54,926 iteration 2801 : loss : 0.065057, loss_ce: 0.015566
2022-01-06 13:38:55,991 iteration 2802 : loss : 0.024992, loss_ce: 0.008501
2022-01-06 13:38:57,129 iteration 2803 : loss : 0.053777, loss_ce: 0.022602
2022-01-06 13:38:58,160 iteration 2804 : loss : 0.051382, loss_ce: 0.025342
2022-01-06 13:38:58,160 Training Data Eval:
2022-01-06 13:39:03,693   Average segmentation loss on training set: 0.0312
2022-01-06 13:39:03,694 Validation Data Eval:
2022-01-06 13:39:05,605   Average segmentation loss on validation set: 0.0841
2022-01-06 13:39:11,361 Found new lowest validation loss at iteration 2804! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 13:39:12,607 iteration 2805 : loss : 0.047064, loss_ce: 0.017952
 41%|███████████▉                 | 165/400 [57:39<1:31:54, 23.47s/it]2022-01-06 13:39:13,847 iteration 2806 : loss : 0.064381, loss_ce: 0.030010
2022-01-06 13:39:15,046 iteration 2807 : loss : 0.071848, loss_ce: 0.019031
2022-01-06 13:39:16,208 iteration 2808 : loss : 0.045450, loss_ce: 0.012709
2022-01-06 13:39:17,346 iteration 2809 : loss : 0.054800, loss_ce: 0.020759
2022-01-06 13:39:18,383 iteration 2810 : loss : 0.050774, loss_ce: 0.017222
2022-01-06 13:39:19,550 iteration 2811 : loss : 0.032433, loss_ce: 0.014961
2022-01-06 13:39:20,591 iteration 2812 : loss : 0.024028, loss_ce: 0.009451
2022-01-06 13:39:21,760 iteration 2813 : loss : 0.043092, loss_ce: 0.019686
2022-01-06 13:39:22,866 iteration 2814 : loss : 0.042646, loss_ce: 0.015942
2022-01-06 13:39:23,985 iteration 2815 : loss : 0.027743, loss_ce: 0.012890
2022-01-06 13:39:25,093 iteration 2816 : loss : 0.041731, loss_ce: 0.015738
2022-01-06 13:39:26,285 iteration 2817 : loss : 0.078302, loss_ce: 0.018670
2022-01-06 13:39:27,455 iteration 2818 : loss : 0.053418, loss_ce: 0.020734
2022-01-06 13:39:28,543 iteration 2819 : loss : 0.035576, loss_ce: 0.016363
2022-01-06 13:39:29,761 iteration 2820 : loss : 0.059918, loss_ce: 0.018761
2022-01-06 13:39:30,954 iteration 2821 : loss : 0.048868, loss_ce: 0.018454
2022-01-06 13:39:32,102 iteration 2822 : loss : 0.031761, loss_ce: 0.010521
 42%|████████████                 | 166/400 [57:59<1:26:52, 22.27s/it]2022-01-06 13:39:33,387 iteration 2823 : loss : 0.046043, loss_ce: 0.021467
2022-01-06 13:39:34,468 iteration 2824 : loss : 0.035487, loss_ce: 0.016539
2022-01-06 13:39:35,589 iteration 2825 : loss : 0.038988, loss_ce: 0.015254
2022-01-06 13:39:36,814 iteration 2826 : loss : 0.053609, loss_ce: 0.021429
2022-01-06 13:39:38,053 iteration 2827 : loss : 0.048235, loss_ce: 0.015276
2022-01-06 13:39:39,208 iteration 2828 : loss : 0.039822, loss_ce: 0.014275
2022-01-06 13:39:40,322 iteration 2829 : loss : 0.043718, loss_ce: 0.019978
2022-01-06 13:39:41,424 iteration 2830 : loss : 0.043759, loss_ce: 0.014537
2022-01-06 13:39:42,440 iteration 2831 : loss : 0.029482, loss_ce: 0.012951
2022-01-06 13:39:43,691 iteration 2832 : loss : 0.038388, loss_ce: 0.018124
2022-01-06 13:39:44,793 iteration 2833 : loss : 0.044250, loss_ce: 0.014338
2022-01-06 13:39:45,971 iteration 2834 : loss : 0.040721, loss_ce: 0.013458
2022-01-06 13:39:47,116 iteration 2835 : loss : 0.043679, loss_ce: 0.012618
2022-01-06 13:39:48,232 iteration 2836 : loss : 0.035944, loss_ce: 0.012535
2022-01-06 13:39:49,420 iteration 2837 : loss : 0.049613, loss_ce: 0.019111
2022-01-06 13:39:50,471 iteration 2838 : loss : 0.030829, loss_ce: 0.013318
2022-01-06 13:39:51,615 iteration 2839 : loss : 0.031642, loss_ce: 0.011563
 42%|████████████                 | 167/400 [58:18<1:23:16, 21.45s/it]2022-01-06 13:39:52,856 iteration 2840 : loss : 0.049968, loss_ce: 0.017710
2022-01-06 13:39:53,957 iteration 2841 : loss : 0.041791, loss_ce: 0.012627
2022-01-06 13:39:55,057 iteration 2842 : loss : 0.033357, loss_ce: 0.011900
2022-01-06 13:39:56,222 iteration 2843 : loss : 0.038494, loss_ce: 0.010516
2022-01-06 13:39:57,355 iteration 2844 : loss : 0.039162, loss_ce: 0.016237
2022-01-06 13:39:58,465 iteration 2845 : loss : 0.038293, loss_ce: 0.013852
2022-01-06 13:39:59,586 iteration 2846 : loss : 0.033149, loss_ce: 0.013095
2022-01-06 13:40:00,562 iteration 2847 : loss : 0.023335, loss_ce: 0.012718
2022-01-06 13:40:01,691 iteration 2848 : loss : 0.059546, loss_ce: 0.024155
2022-01-06 13:40:02,867 iteration 2849 : loss : 0.038516, loss_ce: 0.014624
2022-01-06 13:40:04,022 iteration 2850 : loss : 0.047746, loss_ce: 0.018439
2022-01-06 13:40:05,197 iteration 2851 : loss : 0.035858, loss_ce: 0.013375
2022-01-06 13:40:06,344 iteration 2852 : loss : 0.052706, loss_ce: 0.014268
2022-01-06 13:40:07,540 iteration 2853 : loss : 0.027175, loss_ce: 0.009696
2022-01-06 13:40:08,706 iteration 2854 : loss : 0.033049, loss_ce: 0.011018
2022-01-06 13:40:09,795 iteration 2855 : loss : 0.038850, loss_ce: 0.021556
2022-01-06 13:40:10,967 iteration 2856 : loss : 0.046321, loss_ce: 0.022119
 42%|████████████▏                | 168/400 [58:37<1:20:29, 20.82s/it]2022-01-06 13:40:12,065 iteration 2857 : loss : 0.030379, loss_ce: 0.010836
2022-01-06 13:40:13,207 iteration 2858 : loss : 0.036405, loss_ce: 0.016533
2022-01-06 13:40:14,306 iteration 2859 : loss : 0.027381, loss_ce: 0.011883
2022-01-06 13:40:15,469 iteration 2860 : loss : 0.040804, loss_ce: 0.013617
2022-01-06 13:40:16,503 iteration 2861 : loss : 0.036391, loss_ce: 0.015661
2022-01-06 13:40:17,717 iteration 2862 : loss : 0.081867, loss_ce: 0.025401
2022-01-06 13:40:18,790 iteration 2863 : loss : 0.038184, loss_ce: 0.011205
2022-01-06 13:40:19,954 iteration 2864 : loss : 0.029416, loss_ce: 0.011945
2022-01-06 13:40:21,119 iteration 2865 : loss : 0.038842, loss_ce: 0.014206
2022-01-06 13:40:22,213 iteration 2866 : loss : 0.040369, loss_ce: 0.015538
2022-01-06 13:40:23,438 iteration 2867 : loss : 0.034352, loss_ce: 0.013379
2022-01-06 13:40:24,505 iteration 2868 : loss : 0.047948, loss_ce: 0.016627
2022-01-06 13:40:25,727 iteration 2869 : loss : 0.040233, loss_ce: 0.016109
2022-01-06 13:40:26,888 iteration 2870 : loss : 0.031216, loss_ce: 0.011285
2022-01-06 13:40:27,979 iteration 2871 : loss : 0.032583, loss_ce: 0.013049
2022-01-06 13:40:29,023 iteration 2872 : loss : 0.048767, loss_ce: 0.018893
2022-01-06 13:40:30,203 iteration 2873 : loss : 0.065899, loss_ce: 0.025225
 42%|████████████▎                | 169/400 [58:57<1:18:19, 20.35s/it]2022-01-06 13:40:31,433 iteration 2874 : loss : 0.056009, loss_ce: 0.018876
2022-01-06 13:40:32,580 iteration 2875 : loss : 0.030507, loss_ce: 0.012894
2022-01-06 13:40:33,709 iteration 2876 : loss : 0.044578, loss_ce: 0.018268
2022-01-06 13:40:34,903 iteration 2877 : loss : 0.101089, loss_ce: 0.030973
2022-01-06 13:40:36,029 iteration 2878 : loss : 0.037191, loss_ce: 0.016990
2022-01-06 13:40:37,189 iteration 2879 : loss : 0.045135, loss_ce: 0.014062
2022-01-06 13:40:38,364 iteration 2880 : loss : 0.039881, loss_ce: 0.013801
2022-01-06 13:40:39,512 iteration 2881 : loss : 0.033553, loss_ce: 0.013256
2022-01-06 13:40:40,709 iteration 2882 : loss : 0.048700, loss_ce: 0.024735
2022-01-06 13:40:41,800 iteration 2883 : loss : 0.047627, loss_ce: 0.019762
2022-01-06 13:40:42,905 iteration 2884 : loss : 0.042712, loss_ce: 0.018295
2022-01-06 13:40:44,058 iteration 2885 : loss : 0.041554, loss_ce: 0.017512
2022-01-06 13:40:45,094 iteration 2886 : loss : 0.052266, loss_ce: 0.017528
2022-01-06 13:40:46,296 iteration 2887 : loss : 0.040978, loss_ce: 0.019570
2022-01-06 13:40:47,439 iteration 2888 : loss : 0.062219, loss_ce: 0.014608
2022-01-06 13:40:48,684 iteration 2889 : loss : 0.034180, loss_ce: 0.011415
2022-01-06 13:40:48,684 Training Data Eval:
2022-01-06 13:40:54,037   Average segmentation loss on training set: 0.1163
2022-01-06 13:40:54,038 Validation Data Eval:
2022-01-06 13:40:55,910   Average segmentation loss on validation set: 0.2834
2022-01-06 13:40:57,118 iteration 2890 : loss : 0.043286, loss_ce: 0.014745
 42%|████████████▎                | 170/400 [59:24<1:25:31, 22.31s/it]2022-01-06 13:40:58,356 iteration 2891 : loss : 0.046891, loss_ce: 0.019479
2022-01-06 13:40:59,554 iteration 2892 : loss : 0.041305, loss_ce: 0.009137
2022-01-06 13:41:00,644 iteration 2893 : loss : 0.040388, loss_ce: 0.014160
2022-01-06 13:41:01,871 iteration 2894 : loss : 0.056796, loss_ce: 0.017690
2022-01-06 13:41:03,080 iteration 2895 : loss : 0.054827, loss_ce: 0.023134
2022-01-06 13:41:04,177 iteration 2896 : loss : 0.035642, loss_ce: 0.010825
2022-01-06 13:41:05,310 iteration 2897 : loss : 0.054241, loss_ce: 0.022025
2022-01-06 13:41:06,383 iteration 2898 : loss : 0.047661, loss_ce: 0.020804
2022-01-06 13:41:07,551 iteration 2899 : loss : 0.043697, loss_ce: 0.015595
2022-01-06 13:41:08,663 iteration 2900 : loss : 0.041266, loss_ce: 0.015975
2022-01-06 13:41:09,779 iteration 2901 : loss : 0.046069, loss_ce: 0.015554
2022-01-06 13:41:10,836 iteration 2902 : loss : 0.060027, loss_ce: 0.022661
2022-01-06 13:41:11,917 iteration 2903 : loss : 0.031363, loss_ce: 0.015308
2022-01-06 13:41:12,991 iteration 2904 : loss : 0.029722, loss_ce: 0.012302
2022-01-06 13:41:14,043 iteration 2905 : loss : 0.042431, loss_ce: 0.021692
2022-01-06 13:41:15,211 iteration 2906 : loss : 0.045644, loss_ce: 0.025621
2022-01-06 13:41:16,285 iteration 2907 : loss : 0.051942, loss_ce: 0.021758
 43%|████████████▍                | 171/400 [59:43<1:21:33, 21.37s/it]2022-01-06 13:41:17,501 iteration 2908 : loss : 0.054990, loss_ce: 0.026934
2022-01-06 13:41:18,695 iteration 2909 : loss : 0.046959, loss_ce: 0.024267
2022-01-06 13:41:19,832 iteration 2910 : loss : 0.032836, loss_ce: 0.013336
2022-01-06 13:41:21,011 iteration 2911 : loss : 0.036952, loss_ce: 0.011635
2022-01-06 13:41:22,103 iteration 2912 : loss : 0.032677, loss_ce: 0.011010
2022-01-06 13:41:23,301 iteration 2913 : loss : 0.042154, loss_ce: 0.017636
2022-01-06 13:41:24,402 iteration 2914 : loss : 0.059988, loss_ce: 0.024728
2022-01-06 13:41:25,509 iteration 2915 : loss : 0.032506, loss_ce: 0.013377
2022-01-06 13:41:26,650 iteration 2916 : loss : 0.033324, loss_ce: 0.013398
2022-01-06 13:41:27,724 iteration 2917 : loss : 0.033089, loss_ce: 0.013766
2022-01-06 13:41:28,842 iteration 2918 : loss : 0.047916, loss_ce: 0.013347
2022-01-06 13:41:29,989 iteration 2919 : loss : 0.038427, loss_ce: 0.010839
2022-01-06 13:41:31,144 iteration 2920 : loss : 0.050394, loss_ce: 0.020258
2022-01-06 13:41:32,260 iteration 2921 : loss : 0.031772, loss_ce: 0.014171
2022-01-06 13:41:33,317 iteration 2922 : loss : 0.043410, loss_ce: 0.015729
2022-01-06 13:41:34,418 iteration 2923 : loss : 0.043496, loss_ce: 0.015587
2022-01-06 13:41:35,562 iteration 2924 : loss : 0.049116, loss_ce: 0.017931
 43%|███████████▌               | 172/400 [1:00:02<1:18:49, 20.74s/it]2022-01-06 13:41:36,650 iteration 2925 : loss : 0.046994, loss_ce: 0.025294
2022-01-06 13:41:37,866 iteration 2926 : loss : 0.051223, loss_ce: 0.023680
2022-01-06 13:41:38,979 iteration 2927 : loss : 0.034632, loss_ce: 0.013538
2022-01-06 13:41:39,975 iteration 2928 : loss : 0.053951, loss_ce: 0.015601
2022-01-06 13:41:41,069 iteration 2929 : loss : 0.034417, loss_ce: 0.016249
2022-01-06 13:41:42,272 iteration 2930 : loss : 0.041598, loss_ce: 0.018507
2022-01-06 13:41:43,359 iteration 2931 : loss : 0.043474, loss_ce: 0.015522
2022-01-06 13:41:44,429 iteration 2932 : loss : 0.047468, loss_ce: 0.020393
2022-01-06 13:41:45,631 iteration 2933 : loss : 0.050799, loss_ce: 0.018441
2022-01-06 13:41:46,751 iteration 2934 : loss : 0.043169, loss_ce: 0.015046
2022-01-06 13:41:47,892 iteration 2935 : loss : 0.039639, loss_ce: 0.012977
2022-01-06 13:41:49,004 iteration 2936 : loss : 0.039957, loss_ce: 0.014753
2022-01-06 13:41:50,157 iteration 2937 : loss : 0.045335, loss_ce: 0.021243
2022-01-06 13:41:51,332 iteration 2938 : loss : 0.039033, loss_ce: 0.012849
2022-01-06 13:41:52,505 iteration 2939 : loss : 0.038660, loss_ce: 0.016124
2022-01-06 13:41:53,725 iteration 2940 : loss : 0.063798, loss_ce: 0.033509
2022-01-06 13:41:54,802 iteration 2941 : loss : 0.031152, loss_ce: 0.010662
 43%|███████████▋               | 173/400 [1:00:21<1:16:46, 20.29s/it]2022-01-06 13:41:55,922 iteration 2942 : loss : 0.029598, loss_ce: 0.008874
2022-01-06 13:41:57,086 iteration 2943 : loss : 0.053375, loss_ce: 0.016731
2022-01-06 13:41:58,174 iteration 2944 : loss : 0.047257, loss_ce: 0.024691
2022-01-06 13:41:59,315 iteration 2945 : loss : 0.041233, loss_ce: 0.010243
2022-01-06 13:42:00,341 iteration 2946 : loss : 0.033421, loss_ce: 0.013613
2022-01-06 13:42:01,444 iteration 2947 : loss : 0.027547, loss_ce: 0.010813
2022-01-06 13:42:02,538 iteration 2948 : loss : 0.033913, loss_ce: 0.009220
2022-01-06 13:42:03,735 iteration 2949 : loss : 0.039189, loss_ce: 0.015422
2022-01-06 13:42:04,881 iteration 2950 : loss : 0.029751, loss_ce: 0.013222
2022-01-06 13:42:06,051 iteration 2951 : loss : 0.036597, loss_ce: 0.015921
2022-01-06 13:42:07,202 iteration 2952 : loss : 0.036488, loss_ce: 0.013811
2022-01-06 13:42:08,393 iteration 2953 : loss : 0.035840, loss_ce: 0.017432
2022-01-06 13:42:09,620 iteration 2954 : loss : 0.063168, loss_ce: 0.016633
2022-01-06 13:42:10,700 iteration 2955 : loss : 0.045808, loss_ce: 0.021698
2022-01-06 13:42:11,794 iteration 2956 : loss : 0.030621, loss_ce: 0.011327
2022-01-06 13:42:12,855 iteration 2957 : loss : 0.051281, loss_ce: 0.018329
2022-01-06 13:42:13,981 iteration 2958 : loss : 0.035181, loss_ce: 0.015773
 44%|███████████▋               | 174/400 [1:00:40<1:15:10, 19.96s/it]2022-01-06 13:42:15,120 iteration 2959 : loss : 0.028857, loss_ce: 0.010198
2022-01-06 13:42:16,210 iteration 2960 : loss : 0.035613, loss_ce: 0.016066
2022-01-06 13:42:17,330 iteration 2961 : loss : 0.037636, loss_ce: 0.013825
2022-01-06 13:42:18,613 iteration 2962 : loss : 0.028991, loss_ce: 0.009224
2022-01-06 13:42:19,750 iteration 2963 : loss : 0.046427, loss_ce: 0.016266
2022-01-06 13:42:20,768 iteration 2964 : loss : 0.030628, loss_ce: 0.010709
2022-01-06 13:42:21,899 iteration 2965 : loss : 0.031063, loss_ce: 0.016328
2022-01-06 13:42:23,063 iteration 2966 : loss : 0.038959, loss_ce: 0.013589
2022-01-06 13:42:24,170 iteration 2967 : loss : 0.055908, loss_ce: 0.021729
2022-01-06 13:42:25,295 iteration 2968 : loss : 0.032399, loss_ce: 0.012568
2022-01-06 13:42:26,388 iteration 2969 : loss : 0.030385, loss_ce: 0.012518
2022-01-06 13:42:27,515 iteration 2970 : loss : 0.046098, loss_ce: 0.018444
2022-01-06 13:42:28,618 iteration 2971 : loss : 0.029182, loss_ce: 0.011041
2022-01-06 13:42:29,724 iteration 2972 : loss : 0.037178, loss_ce: 0.016330
2022-01-06 13:42:30,776 iteration 2973 : loss : 0.044268, loss_ce: 0.009354
2022-01-06 13:42:31,964 iteration 2974 : loss : 0.065001, loss_ce: 0.021684
2022-01-06 13:42:31,964 Training Data Eval:
2022-01-06 13:42:37,477   Average segmentation loss on training set: 0.1441
2022-01-06 13:42:37,491 Validation Data Eval:
2022-01-06 13:42:39,396   Average segmentation loss on validation set: 0.1631
2022-01-06 13:42:40,508 iteration 2975 : loss : 0.041122, loss_ce: 0.018385
 44%|███████████▊               | 175/400 [1:01:07<1:22:13, 21.93s/it]2022-01-06 13:42:41,613 iteration 2976 : loss : 0.035074, loss_ce: 0.015562
2022-01-06 13:42:42,773 iteration 2977 : loss : 0.040294, loss_ce: 0.020351
2022-01-06 13:42:43,906 iteration 2978 : loss : 0.035469, loss_ce: 0.011954
2022-01-06 13:42:45,082 iteration 2979 : loss : 0.034809, loss_ce: 0.011069
2022-01-06 13:42:46,245 iteration 2980 : loss : 0.048396, loss_ce: 0.013449
2022-01-06 13:42:47,482 iteration 2981 : loss : 0.054250, loss_ce: 0.022877
2022-01-06 13:42:48,613 iteration 2982 : loss : 0.029965, loss_ce: 0.011594
2022-01-06 13:42:49,743 iteration 2983 : loss : 0.037196, loss_ce: 0.014587
2022-01-06 13:42:50,874 iteration 2984 : loss : 0.039695, loss_ce: 0.014036
2022-01-06 13:42:52,033 iteration 2985 : loss : 0.044490, loss_ce: 0.018894
2022-01-06 13:42:53,087 iteration 2986 : loss : 0.035707, loss_ce: 0.015275
2022-01-06 13:42:54,225 iteration 2987 : loss : 0.043082, loss_ce: 0.014684
2022-01-06 13:42:55,404 iteration 2988 : loss : 0.032369, loss_ce: 0.012594
2022-01-06 13:42:56,565 iteration 2989 : loss : 0.056318, loss_ce: 0.021610
2022-01-06 13:42:57,616 iteration 2990 : loss : 0.022520, loss_ce: 0.009233
2022-01-06 13:42:58,732 iteration 2991 : loss : 0.028076, loss_ce: 0.010961
2022-01-06 13:42:59,923 iteration 2992 : loss : 0.030847, loss_ce: 0.012384
 44%|███████████▉               | 176/400 [1:01:26<1:19:03, 21.17s/it]2022-01-06 13:43:01,042 iteration 2993 : loss : 0.031913, loss_ce: 0.015659
2022-01-06 13:43:02,188 iteration 2994 : loss : 0.058323, loss_ce: 0.014623
2022-01-06 13:43:03,282 iteration 2995 : loss : 0.032327, loss_ce: 0.010937
2022-01-06 13:43:04,397 iteration 2996 : loss : 0.030531, loss_ce: 0.014287
2022-01-06 13:43:05,503 iteration 2997 : loss : 0.037753, loss_ce: 0.012174
2022-01-06 13:43:06,634 iteration 2998 : loss : 0.033773, loss_ce: 0.012992
2022-01-06 13:43:07,800 iteration 2999 : loss : 0.086423, loss_ce: 0.014712
2022-01-06 13:43:08,989 iteration 3000 : loss : 0.037772, loss_ce: 0.016515
2022-01-06 13:43:10,200 iteration 3001 : loss : 0.081803, loss_ce: 0.020296
2022-01-06 13:43:11,261 iteration 3002 : loss : 0.030521, loss_ce: 0.011721
2022-01-06 13:43:12,340 iteration 3003 : loss : 0.030417, loss_ce: 0.013066
2022-01-06 13:43:13,435 iteration 3004 : loss : 0.027819, loss_ce: 0.011342
2022-01-06 13:43:14,562 iteration 3005 : loss : 0.034193, loss_ce: 0.014045
2022-01-06 13:43:15,719 iteration 3006 : loss : 0.036704, loss_ce: 0.013743
2022-01-06 13:43:16,868 iteration 3007 : loss : 0.043539, loss_ce: 0.017622
2022-01-06 13:43:18,019 iteration 3008 : loss : 0.042321, loss_ce: 0.014735
2022-01-06 13:43:19,062 iteration 3009 : loss : 0.047331, loss_ce: 0.018817
 44%|███████████▉               | 177/400 [1:01:46<1:16:25, 20.56s/it]2022-01-06 13:43:20,370 iteration 3010 : loss : 0.049795, loss_ce: 0.015944
2022-01-06 13:43:21,528 iteration 3011 : loss : 0.064792, loss_ce: 0.041497
2022-01-06 13:43:22,597 iteration 3012 : loss : 0.034627, loss_ce: 0.014911
2022-01-06 13:43:23,640 iteration 3013 : loss : 0.046272, loss_ce: 0.013050
2022-01-06 13:43:24,769 iteration 3014 : loss : 0.036825, loss_ce: 0.013513
2022-01-06 13:43:25,884 iteration 3015 : loss : 0.045286, loss_ce: 0.011576
2022-01-06 13:43:26,985 iteration 3016 : loss : 0.052668, loss_ce: 0.017988
2022-01-06 13:43:28,061 iteration 3017 : loss : 0.040269, loss_ce: 0.019607
2022-01-06 13:43:29,139 iteration 3018 : loss : 0.030645, loss_ce: 0.013251
2022-01-06 13:43:30,183 iteration 3019 : loss : 0.029843, loss_ce: 0.010672
2022-01-06 13:43:31,326 iteration 3020 : loss : 0.063730, loss_ce: 0.021972
2022-01-06 13:43:32,457 iteration 3021 : loss : 0.025100, loss_ce: 0.009188
2022-01-06 13:43:33,548 iteration 3022 : loss : 0.033571, loss_ce: 0.010220
2022-01-06 13:43:34,668 iteration 3023 : loss : 0.051816, loss_ce: 0.016117
2022-01-06 13:43:35,878 iteration 3024 : loss : 0.041834, loss_ce: 0.015336
2022-01-06 13:43:37,059 iteration 3025 : loss : 0.046761, loss_ce: 0.018036
2022-01-06 13:43:38,124 iteration 3026 : loss : 0.033607, loss_ce: 0.014182
 44%|████████████               | 178/400 [1:02:05<1:14:25, 20.11s/it]2022-01-06 13:43:39,368 iteration 3027 : loss : 0.065169, loss_ce: 0.021823
2022-01-06 13:43:40,473 iteration 3028 : loss : 0.035572, loss_ce: 0.015710
2022-01-06 13:43:41,606 iteration 3029 : loss : 0.046129, loss_ce: 0.023502
2022-01-06 13:43:42,695 iteration 3030 : loss : 0.029586, loss_ce: 0.012506
2022-01-06 13:43:43,819 iteration 3031 : loss : 0.054968, loss_ce: 0.017277
2022-01-06 13:43:44,889 iteration 3032 : loss : 0.041410, loss_ce: 0.014462
2022-01-06 13:43:46,038 iteration 3033 : loss : 0.049742, loss_ce: 0.014562
2022-01-06 13:43:47,227 iteration 3034 : loss : 0.042569, loss_ce: 0.015106
2022-01-06 13:43:48,373 iteration 3035 : loss : 0.041483, loss_ce: 0.016979
2022-01-06 13:43:49,473 iteration 3036 : loss : 0.055858, loss_ce: 0.035451
2022-01-06 13:43:50,596 iteration 3037 : loss : 0.035327, loss_ce: 0.013704
2022-01-06 13:43:51,753 iteration 3038 : loss : 0.033984, loss_ce: 0.012904
2022-01-06 13:43:52,794 iteration 3039 : loss : 0.037591, loss_ce: 0.011483
2022-01-06 13:43:53,918 iteration 3040 : loss : 0.044425, loss_ce: 0.013126
2022-01-06 13:43:55,084 iteration 3041 : loss : 0.031242, loss_ce: 0.012007
2022-01-06 13:43:56,194 iteration 3042 : loss : 0.044234, loss_ce: 0.020777
2022-01-06 13:43:57,495 iteration 3043 : loss : 0.061961, loss_ce: 0.017808
 45%|████████████               | 179/400 [1:02:24<1:13:16, 19.89s/it]2022-01-06 13:43:58,688 iteration 3044 : loss : 0.037509, loss_ce: 0.013198
2022-01-06 13:43:59,739 iteration 3045 : loss : 0.033272, loss_ce: 0.012286
2022-01-06 13:44:00,826 iteration 3046 : loss : 0.032477, loss_ce: 0.011114
2022-01-06 13:44:02,051 iteration 3047 : loss : 0.042701, loss_ce: 0.022765
2022-01-06 13:44:03,127 iteration 3048 : loss : 0.036260, loss_ce: 0.011980
2022-01-06 13:44:04,236 iteration 3049 : loss : 0.037885, loss_ce: 0.016455
2022-01-06 13:44:05,370 iteration 3050 : loss : 0.040129, loss_ce: 0.018240
2022-01-06 13:44:06,504 iteration 3051 : loss : 0.037753, loss_ce: 0.011511
2022-01-06 13:44:07,610 iteration 3052 : loss : 0.037632, loss_ce: 0.013267
2022-01-06 13:44:08,822 iteration 3053 : loss : 0.035700, loss_ce: 0.014142
2022-01-06 13:44:09,894 iteration 3054 : loss : 0.037859, loss_ce: 0.010887
2022-01-06 13:44:11,033 iteration 3055 : loss : 0.046109, loss_ce: 0.016861
2022-01-06 13:44:12,118 iteration 3056 : loss : 0.029651, loss_ce: 0.011525
2022-01-06 13:44:13,178 iteration 3057 : loss : 0.029819, loss_ce: 0.011976
2022-01-06 13:44:14,327 iteration 3058 : loss : 0.029373, loss_ce: 0.011362
2022-01-06 13:44:15,493 iteration 3059 : loss : 0.037276, loss_ce: 0.011249
2022-01-06 13:44:15,493 Training Data Eval:
2022-01-06 13:44:21,003   Average segmentation loss on training set: 0.0267
2022-01-06 13:44:21,003 Validation Data Eval:
2022-01-06 13:44:22,930   Average segmentation loss on validation set: 0.0740
2022-01-06 13:44:30,304 Found new lowest validation loss at iteration 3059! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 13:44:31,415 iteration 3060 : loss : 0.027105, loss_ce: 0.013367
 45%|████████████▏              | 180/400 [1:02:58<1:28:22, 24.10s/it]2022-01-06 13:44:32,495 iteration 3061 : loss : 0.036959, loss_ce: 0.013557
2022-01-06 13:44:33,603 iteration 3062 : loss : 0.053642, loss_ce: 0.021843
2022-01-06 13:44:34,746 iteration 3063 : loss : 0.041614, loss_ce: 0.014759
2022-01-06 13:44:35,894 iteration 3064 : loss : 0.026242, loss_ce: 0.011051
2022-01-06 13:44:36,895 iteration 3065 : loss : 0.024303, loss_ce: 0.008449
2022-01-06 13:44:37,959 iteration 3066 : loss : 0.040839, loss_ce: 0.018628
2022-01-06 13:44:39,094 iteration 3067 : loss : 0.039434, loss_ce: 0.015691
2022-01-06 13:44:40,221 iteration 3068 : loss : 0.051353, loss_ce: 0.016471
2022-01-06 13:44:41,339 iteration 3069 : loss : 0.047345, loss_ce: 0.011891
2022-01-06 13:44:42,459 iteration 3070 : loss : 0.029060, loss_ce: 0.012035
2022-01-06 13:44:43,517 iteration 3071 : loss : 0.028708, loss_ce: 0.010483
2022-01-06 13:44:44,568 iteration 3072 : loss : 0.028941, loss_ce: 0.011268
2022-01-06 13:44:45,658 iteration 3073 : loss : 0.037982, loss_ce: 0.013993
2022-01-06 13:44:46,843 iteration 3074 : loss : 0.033070, loss_ce: 0.011847
2022-01-06 13:44:47,982 iteration 3075 : loss : 0.041597, loss_ce: 0.012925
2022-01-06 13:44:49,161 iteration 3076 : loss : 0.039528, loss_ce: 0.014534
2022-01-06 13:44:50,228 iteration 3077 : loss : 0.041853, loss_ce: 0.017094
 45%|████████████▏              | 181/400 [1:03:17<1:22:10, 22.51s/it]2022-01-06 13:44:51,339 iteration 3078 : loss : 0.033890, loss_ce: 0.012919
2022-01-06 13:44:52,571 iteration 3079 : loss : 0.045457, loss_ce: 0.020181
2022-01-06 13:44:53,601 iteration 3080 : loss : 0.030284, loss_ce: 0.008771
2022-01-06 13:44:54,736 iteration 3081 : loss : 0.033863, loss_ce: 0.011303
2022-01-06 13:44:55,823 iteration 3082 : loss : 0.052725, loss_ce: 0.010554
2022-01-06 13:44:56,989 iteration 3083 : loss : 0.035430, loss_ce: 0.013007
2022-01-06 13:44:57,970 iteration 3084 : loss : 0.033046, loss_ce: 0.014274
2022-01-06 13:44:59,107 iteration 3085 : loss : 0.035394, loss_ce: 0.015227
2022-01-06 13:45:00,323 iteration 3086 : loss : 0.038570, loss_ce: 0.021421
2022-01-06 13:45:01,414 iteration 3087 : loss : 0.033338, loss_ce: 0.012754
2022-01-06 13:45:02,531 iteration 3088 : loss : 0.026843, loss_ce: 0.008127
2022-01-06 13:45:03,728 iteration 3089 : loss : 0.057120, loss_ce: 0.019013
2022-01-06 13:45:04,761 iteration 3090 : loss : 0.037889, loss_ce: 0.019001
2022-01-06 13:45:05,873 iteration 3091 : loss : 0.035171, loss_ce: 0.016279
2022-01-06 13:45:07,147 iteration 3092 : loss : 0.037967, loss_ce: 0.016325
2022-01-06 13:45:08,219 iteration 3093 : loss : 0.041416, loss_ce: 0.017076
2022-01-06 13:45:09,341 iteration 3094 : loss : 0.028833, loss_ce: 0.012138
 46%|████████████▎              | 182/400 [1:03:36<1:18:05, 21.50s/it]2022-01-06 13:45:10,508 iteration 3095 : loss : 0.026844, loss_ce: 0.011049
2022-01-06 13:45:11,637 iteration 3096 : loss : 0.032713, loss_ce: 0.013101
2022-01-06 13:45:12,726 iteration 3097 : loss : 0.035073, loss_ce: 0.012350
2022-01-06 13:45:13,789 iteration 3098 : loss : 0.028653, loss_ce: 0.012170
2022-01-06 13:45:14,905 iteration 3099 : loss : 0.023692, loss_ce: 0.007913
2022-01-06 13:45:15,961 iteration 3100 : loss : 0.031958, loss_ce: 0.010591
2022-01-06 13:45:17,129 iteration 3101 : loss : 0.028787, loss_ce: 0.010660
2022-01-06 13:45:18,243 iteration 3102 : loss : 0.037635, loss_ce: 0.014058
2022-01-06 13:45:19,366 iteration 3103 : loss : 0.045070, loss_ce: 0.021583
2022-01-06 13:45:20,435 iteration 3104 : loss : 0.029578, loss_ce: 0.012883
2022-01-06 13:45:21,475 iteration 3105 : loss : 0.026038, loss_ce: 0.012054
2022-01-06 13:45:22,566 iteration 3106 : loss : 0.035419, loss_ce: 0.016221
2022-01-06 13:45:23,655 iteration 3107 : loss : 0.031616, loss_ce: 0.009771
2022-01-06 13:45:24,788 iteration 3108 : loss : 0.057282, loss_ce: 0.019877
2022-01-06 13:45:25,947 iteration 3109 : loss : 0.042296, loss_ce: 0.013885
2022-01-06 13:45:27,103 iteration 3110 : loss : 0.039795, loss_ce: 0.018328
2022-01-06 13:45:28,239 iteration 3111 : loss : 0.031063, loss_ce: 0.012963
 46%|████████████▎              | 183/400 [1:03:55<1:14:54, 20.71s/it]2022-01-06 13:45:29,435 iteration 3112 : loss : 0.038174, loss_ce: 0.013102
2022-01-06 13:45:30,513 iteration 3113 : loss : 0.036765, loss_ce: 0.013734
2022-01-06 13:45:31,675 iteration 3114 : loss : 0.038505, loss_ce: 0.013780
2022-01-06 13:45:32,800 iteration 3115 : loss : 0.030563, loss_ce: 0.012470
2022-01-06 13:45:33,903 iteration 3116 : loss : 0.032137, loss_ce: 0.013827
2022-01-06 13:45:35,092 iteration 3117 : loss : 0.031437, loss_ce: 0.013023
2022-01-06 13:45:36,187 iteration 3118 : loss : 0.059735, loss_ce: 0.023367
2022-01-06 13:45:37,408 iteration 3119 : loss : 0.046808, loss_ce: 0.017706
2022-01-06 13:45:38,464 iteration 3120 : loss : 0.029635, loss_ce: 0.012340
2022-01-06 13:45:39,635 iteration 3121 : loss : 0.029542, loss_ce: 0.012923
2022-01-06 13:45:40,713 iteration 3122 : loss : 0.041102, loss_ce: 0.010837
2022-01-06 13:45:41,888 iteration 3123 : loss : 0.033914, loss_ce: 0.013452
2022-01-06 13:45:42,915 iteration 3124 : loss : 0.030414, loss_ce: 0.012287
2022-01-06 13:45:44,022 iteration 3125 : loss : 0.032211, loss_ce: 0.009585
2022-01-06 13:45:45,194 iteration 3126 : loss : 0.032594, loss_ce: 0.015618
2022-01-06 13:45:46,382 iteration 3127 : loss : 0.035647, loss_ce: 0.011629
2022-01-06 13:45:47,563 iteration 3128 : loss : 0.042266, loss_ce: 0.014112
 46%|████████████▍              | 184/400 [1:04:14<1:13:04, 20.30s/it]2022-01-06 13:45:48,701 iteration 3129 : loss : 0.034837, loss_ce: 0.012583
2022-01-06 13:45:49,917 iteration 3130 : loss : 0.034365, loss_ce: 0.013976
2022-01-06 13:45:51,019 iteration 3131 : loss : 0.049502, loss_ce: 0.017230
2022-01-06 13:45:52,139 iteration 3132 : loss : 0.029338, loss_ce: 0.009861
2022-01-06 13:45:53,281 iteration 3133 : loss : 0.036442, loss_ce: 0.013202
2022-01-06 13:45:54,355 iteration 3134 : loss : 0.027222, loss_ce: 0.011880
2022-01-06 13:45:55,473 iteration 3135 : loss : 0.046912, loss_ce: 0.023852
2022-01-06 13:45:56,566 iteration 3136 : loss : 0.035315, loss_ce: 0.014929
2022-01-06 13:45:57,797 iteration 3137 : loss : 0.030721, loss_ce: 0.013100
2022-01-06 13:45:58,980 iteration 3138 : loss : 0.046190, loss_ce: 0.019490
2022-01-06 13:46:00,031 iteration 3139 : loss : 0.032237, loss_ce: 0.011632
2022-01-06 13:46:01,089 iteration 3140 : loss : 0.034036, loss_ce: 0.013332
2022-01-06 13:46:02,223 iteration 3141 : loss : 0.041729, loss_ce: 0.018235
2022-01-06 13:46:03,317 iteration 3142 : loss : 0.038302, loss_ce: 0.013003
2022-01-06 13:46:04,483 iteration 3143 : loss : 0.042154, loss_ce: 0.015704
2022-01-06 13:46:05,592 iteration 3144 : loss : 0.028420, loss_ce: 0.009872
2022-01-06 13:46:05,592 Training Data Eval:
2022-01-06 13:46:11,079   Average segmentation loss on training set: 0.0380
2022-01-06 13:46:11,080 Validation Data Eval:
2022-01-06 13:46:12,962   Average segmentation loss on validation set: 0.1008
2022-01-06 13:46:14,042 iteration 3145 : loss : 0.024595, loss_ce: 0.010497
 46%|████████████▍              | 185/400 [1:04:40<1:19:22, 22.15s/it]2022-01-06 13:46:15,382 iteration 3146 : loss : 0.036131, loss_ce: 0.010170
2022-01-06 13:46:16,549 iteration 3147 : loss : 0.032482, loss_ce: 0.014473
2022-01-06 13:46:17,593 iteration 3148 : loss : 0.025743, loss_ce: 0.009974
2022-01-06 13:46:18,751 iteration 3149 : loss : 0.040217, loss_ce: 0.013362
2022-01-06 13:46:19,915 iteration 3150 : loss : 0.068443, loss_ce: 0.019311
2022-01-06 13:46:21,069 iteration 3151 : loss : 0.045000, loss_ce: 0.021515
2022-01-06 13:46:22,220 iteration 3152 : loss : 0.054466, loss_ce: 0.017135
2022-01-06 13:46:23,346 iteration 3153 : loss : 0.031644, loss_ce: 0.015679
2022-01-06 13:46:24,549 iteration 3154 : loss : 0.039045, loss_ce: 0.015465
2022-01-06 13:46:25,707 iteration 3155 : loss : 0.025692, loss_ce: 0.008913
2022-01-06 13:46:26,757 iteration 3156 : loss : 0.041063, loss_ce: 0.012827
2022-01-06 13:46:27,900 iteration 3157 : loss : 0.035519, loss_ce: 0.013889
2022-01-06 13:46:28,974 iteration 3158 : loss : 0.038843, loss_ce: 0.009904
2022-01-06 13:46:30,118 iteration 3159 : loss : 0.033811, loss_ce: 0.016978
2022-01-06 13:46:31,267 iteration 3160 : loss : 0.027786, loss_ce: 0.011570
2022-01-06 13:46:32,469 iteration 3161 : loss : 0.034659, loss_ce: 0.014176
2022-01-06 13:46:33,597 iteration 3162 : loss : 0.038304, loss_ce: 0.013903
 46%|████████████▌              | 186/400 [1:05:00<1:16:14, 21.37s/it]2022-01-06 13:46:34,730 iteration 3163 : loss : 0.033897, loss_ce: 0.011849
2022-01-06 13:46:35,963 iteration 3164 : loss : 0.042082, loss_ce: 0.017623
2022-01-06 13:46:37,136 iteration 3165 : loss : 0.034816, loss_ce: 0.015334
2022-01-06 13:46:38,208 iteration 3166 : loss : 0.037061, loss_ce: 0.015748
2022-01-06 13:46:39,378 iteration 3167 : loss : 0.035835, loss_ce: 0.012077
2022-01-06 13:46:40,499 iteration 3168 : loss : 0.028493, loss_ce: 0.012144
2022-01-06 13:46:41,643 iteration 3169 : loss : 0.041132, loss_ce: 0.017702
2022-01-06 13:46:42,762 iteration 3170 : loss : 0.033981, loss_ce: 0.014981
2022-01-06 13:46:43,764 iteration 3171 : loss : 0.030289, loss_ce: 0.008907
2022-01-06 13:46:44,845 iteration 3172 : loss : 0.023572, loss_ce: 0.008701
2022-01-06 13:46:45,962 iteration 3173 : loss : 0.046714, loss_ce: 0.015565
2022-01-06 13:46:47,049 iteration 3174 : loss : 0.031288, loss_ce: 0.012555
2022-01-06 13:46:48,106 iteration 3175 : loss : 0.042554, loss_ce: 0.014401
2022-01-06 13:46:49,313 iteration 3176 : loss : 0.049927, loss_ce: 0.015572
2022-01-06 13:46:50,439 iteration 3177 : loss : 0.027819, loss_ce: 0.009878
2022-01-06 13:46:51,662 iteration 3178 : loss : 0.037777, loss_ce: 0.010503
2022-01-06 13:46:52,796 iteration 3179 : loss : 0.030734, loss_ce: 0.015517
 47%|████████████▌              | 187/400 [1:05:19<1:13:33, 20.72s/it]2022-01-06 13:46:53,995 iteration 3180 : loss : 0.051799, loss_ce: 0.021174
2022-01-06 13:46:55,079 iteration 3181 : loss : 0.027721, loss_ce: 0.012224
2022-01-06 13:46:56,136 iteration 3182 : loss : 0.026241, loss_ce: 0.011647
2022-01-06 13:46:57,297 iteration 3183 : loss : 0.041896, loss_ce: 0.016069
2022-01-06 13:46:58,417 iteration 3184 : loss : 0.032499, loss_ce: 0.010625
2022-01-06 13:46:59,557 iteration 3185 : loss : 0.027284, loss_ce: 0.008041
2022-01-06 13:47:00,724 iteration 3186 : loss : 0.037556, loss_ce: 0.016729
2022-01-06 13:47:01,903 iteration 3187 : loss : 0.027239, loss_ce: 0.010937
2022-01-06 13:47:03,104 iteration 3188 : loss : 0.038659, loss_ce: 0.017333
2022-01-06 13:47:04,238 iteration 3189 : loss : 0.025464, loss_ce: 0.008290
2022-01-06 13:47:05,486 iteration 3190 : loss : 0.037767, loss_ce: 0.013715
2022-01-06 13:47:06,679 iteration 3191 : loss : 0.039650, loss_ce: 0.014007
2022-01-06 13:47:07,747 iteration 3192 : loss : 0.038910, loss_ce: 0.008534
2022-01-06 13:47:08,887 iteration 3193 : loss : 0.028375, loss_ce: 0.011483
2022-01-06 13:47:10,157 iteration 3194 : loss : 0.053231, loss_ce: 0.021597
2022-01-06 13:47:11,308 iteration 3195 : loss : 0.034019, loss_ce: 0.015286
2022-01-06 13:47:12,437 iteration 3196 : loss : 0.034523, loss_ce: 0.016194
 47%|████████████▋              | 188/400 [1:05:39<1:12:03, 20.40s/it]2022-01-06 13:47:13,607 iteration 3197 : loss : 0.025617, loss_ce: 0.010833
2022-01-06 13:47:14,784 iteration 3198 : loss : 0.034681, loss_ce: 0.015103
2022-01-06 13:47:15,961 iteration 3199 : loss : 0.028669, loss_ce: 0.007415
2022-01-06 13:47:17,156 iteration 3200 : loss : 0.043288, loss_ce: 0.014754
2022-01-06 13:47:18,339 iteration 3201 : loss : 0.031939, loss_ce: 0.010766
2022-01-06 13:47:19,526 iteration 3202 : loss : 0.039877, loss_ce: 0.010985
2022-01-06 13:47:20,586 iteration 3203 : loss : 0.028980, loss_ce: 0.012321
2022-01-06 13:47:21,632 iteration 3204 : loss : 0.028532, loss_ce: 0.011895
2022-01-06 13:47:22,677 iteration 3205 : loss : 0.035564, loss_ce: 0.011835
2022-01-06 13:47:23,691 iteration 3206 : loss : 0.025683, loss_ce: 0.008230
2022-01-06 13:47:24,855 iteration 3207 : loss : 0.047818, loss_ce: 0.019864
2022-01-06 13:47:25,980 iteration 3208 : loss : 0.030262, loss_ce: 0.012226
2022-01-06 13:47:27,075 iteration 3209 : loss : 0.043205, loss_ce: 0.014207
2022-01-06 13:47:28,230 iteration 3210 : loss : 0.034600, loss_ce: 0.015175
2022-01-06 13:47:29,297 iteration 3211 : loss : 0.029574, loss_ce: 0.008811
2022-01-06 13:47:30,501 iteration 3212 : loss : 0.044460, loss_ce: 0.022074
2022-01-06 13:47:31,716 iteration 3213 : loss : 0.036446, loss_ce: 0.012546
 47%|████████████▊              | 189/400 [1:05:58<1:10:32, 20.06s/it]2022-01-06 13:47:32,840 iteration 3214 : loss : 0.024096, loss_ce: 0.010999
2022-01-06 13:47:33,956 iteration 3215 : loss : 0.032943, loss_ce: 0.015109
2022-01-06 13:47:35,082 iteration 3216 : loss : 0.047614, loss_ce: 0.017816
2022-01-06 13:47:36,270 iteration 3217 : loss : 0.037223, loss_ce: 0.017207
2022-01-06 13:47:37,411 iteration 3218 : loss : 0.044168, loss_ce: 0.013344
2022-01-06 13:47:38,548 iteration 3219 : loss : 0.041266, loss_ce: 0.015557
2022-01-06 13:47:39,634 iteration 3220 : loss : 0.025031, loss_ce: 0.010941
2022-01-06 13:47:40,868 iteration 3221 : loss : 0.042709, loss_ce: 0.016632
2022-01-06 13:47:41,962 iteration 3222 : loss : 0.023352, loss_ce: 0.005860
2022-01-06 13:47:43,109 iteration 3223 : loss : 0.033120, loss_ce: 0.011294
2022-01-06 13:47:44,325 iteration 3224 : loss : 0.048495, loss_ce: 0.012364
2022-01-06 13:47:45,428 iteration 3225 : loss : 0.047392, loss_ce: 0.019470
2022-01-06 13:47:46,555 iteration 3226 : loss : 0.031820, loss_ce: 0.013199
2022-01-06 13:47:47,706 iteration 3227 : loss : 0.038690, loss_ce: 0.014434
2022-01-06 13:47:48,800 iteration 3228 : loss : 0.072593, loss_ce: 0.022830
2022-01-06 13:47:49,855 iteration 3229 : loss : 0.039288, loss_ce: 0.011355
2022-01-06 13:47:49,855 Training Data Eval:
2022-01-06 13:47:55,381   Average segmentation loss on training set: 0.1806
2022-01-06 13:47:55,381 Validation Data Eval:
2022-01-06 13:47:57,289   Average segmentation loss on validation set: 0.1956
2022-01-06 13:47:58,347 iteration 3230 : loss : 0.031333, loss_ce: 0.012037
 48%|████████████▊              | 190/400 [1:06:25<1:17:06, 22.03s/it]2022-01-06 13:47:59,493 iteration 3231 : loss : 0.056889, loss_ce: 0.011565
2022-01-06 13:48:00,597 iteration 3232 : loss : 0.028509, loss_ce: 0.009188
2022-01-06 13:48:01,734 iteration 3233 : loss : 0.036345, loss_ce: 0.016369
2022-01-06 13:48:02,892 iteration 3234 : loss : 0.029112, loss_ce: 0.007924
2022-01-06 13:48:04,036 iteration 3235 : loss : 0.036992, loss_ce: 0.013850
2022-01-06 13:48:05,093 iteration 3236 : loss : 0.023324, loss_ce: 0.011190
2022-01-06 13:48:06,325 iteration 3237 : loss : 0.034650, loss_ce: 0.016065
2022-01-06 13:48:07,456 iteration 3238 : loss : 0.032060, loss_ce: 0.014157
2022-01-06 13:48:08,591 iteration 3239 : loss : 0.031749, loss_ce: 0.013120
2022-01-06 13:48:09,659 iteration 3240 : loss : 0.039176, loss_ce: 0.009729
2022-01-06 13:48:10,693 iteration 3241 : loss : 0.037063, loss_ce: 0.008628
2022-01-06 13:48:11,802 iteration 3242 : loss : 0.035481, loss_ce: 0.013689
2022-01-06 13:48:12,839 iteration 3243 : loss : 0.037792, loss_ce: 0.012249
2022-01-06 13:48:13,990 iteration 3244 : loss : 0.033231, loss_ce: 0.013069
2022-01-06 13:48:15,072 iteration 3245 : loss : 0.023376, loss_ce: 0.010861
2022-01-06 13:48:16,265 iteration 3246 : loss : 0.032597, loss_ce: 0.014345
2022-01-06 13:48:17,360 iteration 3247 : loss : 0.032616, loss_ce: 0.013568
 48%|████████████▉              | 191/400 [1:06:44<1:13:35, 21.13s/it]2022-01-06 13:48:18,503 iteration 3248 : loss : 0.033796, loss_ce: 0.012383
2022-01-06 13:48:19,586 iteration 3249 : loss : 0.027572, loss_ce: 0.009234
2022-01-06 13:48:20,701 iteration 3250 : loss : 0.031647, loss_ce: 0.010896
2022-01-06 13:48:21,825 iteration 3251 : loss : 0.036737, loss_ce: 0.014156
2022-01-06 13:48:22,984 iteration 3252 : loss : 0.030537, loss_ce: 0.010207
2022-01-06 13:48:24,112 iteration 3253 : loss : 0.026157, loss_ce: 0.011261
2022-01-06 13:48:25,228 iteration 3254 : loss : 0.032044, loss_ce: 0.011669
2022-01-06 13:48:26,276 iteration 3255 : loss : 0.041779, loss_ce: 0.015225
2022-01-06 13:48:27,395 iteration 3256 : loss : 0.021853, loss_ce: 0.007475
2022-01-06 13:48:28,477 iteration 3257 : loss : 0.029549, loss_ce: 0.013671
2022-01-06 13:48:29,611 iteration 3258 : loss : 0.030758, loss_ce: 0.012502
2022-01-06 13:48:30,678 iteration 3259 : loss : 0.040111, loss_ce: 0.015365
2022-01-06 13:48:31,821 iteration 3260 : loss : 0.030371, loss_ce: 0.012206
2022-01-06 13:48:32,911 iteration 3261 : loss : 0.053029, loss_ce: 0.013723
2022-01-06 13:48:34,066 iteration 3262 : loss : 0.031752, loss_ce: 0.012466
2022-01-06 13:48:35,309 iteration 3263 : loss : 0.050913, loss_ce: 0.014198
2022-01-06 13:48:36,437 iteration 3264 : loss : 0.033313, loss_ce: 0.014508
 48%|████████████▉              | 192/400 [1:07:03<1:11:07, 20.51s/it]2022-01-06 13:48:37,574 iteration 3265 : loss : 0.027531, loss_ce: 0.010984
2022-01-06 13:48:38,750 iteration 3266 : loss : 0.034678, loss_ce: 0.013818
2022-01-06 13:48:39,900 iteration 3267 : loss : 0.040907, loss_ce: 0.016115
2022-01-06 13:48:40,974 iteration 3268 : loss : 0.026512, loss_ce: 0.010297
2022-01-06 13:48:42,157 iteration 3269 : loss : 0.035542, loss_ce: 0.016899
2022-01-06 13:48:43,372 iteration 3270 : loss : 0.039167, loss_ce: 0.012602
2022-01-06 13:48:44,520 iteration 3271 : loss : 0.038828, loss_ce: 0.013790
2022-01-06 13:48:45,709 iteration 3272 : loss : 0.034195, loss_ce: 0.013722
2022-01-06 13:48:46,761 iteration 3273 : loss : 0.038776, loss_ce: 0.012626
2022-01-06 13:48:47,862 iteration 3274 : loss : 0.027722, loss_ce: 0.013301
2022-01-06 13:48:49,043 iteration 3275 : loss : 0.041264, loss_ce: 0.010864
2022-01-06 13:48:50,123 iteration 3276 : loss : 0.024955, loss_ce: 0.010565
2022-01-06 13:48:51,268 iteration 3277 : loss : 0.040255, loss_ce: 0.020207
2022-01-06 13:48:52,390 iteration 3278 : loss : 0.038567, loss_ce: 0.013739
2022-01-06 13:48:53,468 iteration 3279 : loss : 0.029539, loss_ce: 0.011376
2022-01-06 13:48:54,574 iteration 3280 : loss : 0.052233, loss_ce: 0.018683
2022-01-06 13:48:55,641 iteration 3281 : loss : 0.028158, loss_ce: 0.007952
 48%|█████████████              | 193/400 [1:07:22<1:09:24, 20.12s/it]2022-01-06 13:48:56,859 iteration 3282 : loss : 0.046429, loss_ce: 0.019472
2022-01-06 13:48:57,982 iteration 3283 : loss : 0.031249, loss_ce: 0.011029
2022-01-06 13:48:59,101 iteration 3284 : loss : 0.049510, loss_ce: 0.021874
2022-01-06 13:49:00,199 iteration 3285 : loss : 0.048434, loss_ce: 0.024208
2022-01-06 13:49:01,301 iteration 3286 : loss : 0.021691, loss_ce: 0.007857
2022-01-06 13:49:02,434 iteration 3287 : loss : 0.035283, loss_ce: 0.012834
2022-01-06 13:49:03,494 iteration 3288 : loss : 0.041409, loss_ce: 0.022808
2022-01-06 13:49:04,570 iteration 3289 : loss : 0.045803, loss_ce: 0.012410
2022-01-06 13:49:05,746 iteration 3290 : loss : 0.040124, loss_ce: 0.012870
2022-01-06 13:49:06,933 iteration 3291 : loss : 0.043840, loss_ce: 0.016725
2022-01-06 13:49:08,074 iteration 3292 : loss : 0.034261, loss_ce: 0.009977
2022-01-06 13:49:09,184 iteration 3293 : loss : 0.034337, loss_ce: 0.014680
2022-01-06 13:49:10,263 iteration 3294 : loss : 0.033868, loss_ce: 0.016458
2022-01-06 13:49:11,469 iteration 3295 : loss : 0.033566, loss_ce: 0.014442
2022-01-06 13:49:12,609 iteration 3296 : loss : 0.036835, loss_ce: 0.012577
2022-01-06 13:49:13,648 iteration 3297 : loss : 0.022227, loss_ce: 0.008822
2022-01-06 13:49:14,806 iteration 3298 : loss : 0.030758, loss_ce: 0.011980
 48%|█████████████              | 194/400 [1:07:41<1:08:05, 19.83s/it]2022-01-06 13:49:15,895 iteration 3299 : loss : 0.029055, loss_ce: 0.012147
2022-01-06 13:49:16,972 iteration 3300 : loss : 0.039970, loss_ce: 0.014260
2022-01-06 13:49:18,140 iteration 3301 : loss : 0.026791, loss_ce: 0.011234
2022-01-06 13:49:19,245 iteration 3302 : loss : 0.030429, loss_ce: 0.012609
2022-01-06 13:49:20,333 iteration 3303 : loss : 0.025918, loss_ce: 0.010477
2022-01-06 13:49:21,447 iteration 3304 : loss : 0.031442, loss_ce: 0.011453
2022-01-06 13:49:22,604 iteration 3305 : loss : 0.025636, loss_ce: 0.008938
2022-01-06 13:49:23,785 iteration 3306 : loss : 0.031385, loss_ce: 0.012550
2022-01-06 13:49:24,881 iteration 3307 : loss : 0.027351, loss_ce: 0.010383
2022-01-06 13:49:26,015 iteration 3308 : loss : 0.039928, loss_ce: 0.018112
2022-01-06 13:49:27,115 iteration 3309 : loss : 0.048557, loss_ce: 0.010532
2022-01-06 13:49:28,293 iteration 3310 : loss : 0.074417, loss_ce: 0.026843
2022-01-06 13:49:29,524 iteration 3311 : loss : 0.033196, loss_ce: 0.011433
2022-01-06 13:49:30,585 iteration 3312 : loss : 0.033356, loss_ce: 0.011433
2022-01-06 13:49:31,656 iteration 3313 : loss : 0.028463, loss_ce: 0.009963
2022-01-06 13:49:32,843 iteration 3314 : loss : 0.034148, loss_ce: 0.011622
2022-01-06 13:49:32,843 Training Data Eval:
2022-01-06 13:49:38,315   Average segmentation loss on training set: 0.0329
2022-01-06 13:49:38,316 Validation Data Eval:
2022-01-06 13:49:40,219   Average segmentation loss on validation set: 0.1176
2022-01-06 13:49:41,293 iteration 3315 : loss : 0.022438, loss_ce: 0.010786
 49%|█████████████▏             | 195/400 [1:08:08<1:14:34, 21.83s/it]2022-01-06 13:49:42,427 iteration 3316 : loss : 0.019245, loss_ce: 0.006811
2022-01-06 13:49:43,518 iteration 3317 : loss : 0.028915, loss_ce: 0.010853
2022-01-06 13:49:44,743 iteration 3318 : loss : 0.029553, loss_ce: 0.012526
2022-01-06 13:49:45,827 iteration 3319 : loss : 0.029038, loss_ce: 0.010593
2022-01-06 13:49:46,915 iteration 3320 : loss : 0.029601, loss_ce: 0.011166
2022-01-06 13:49:48,045 iteration 3321 : loss : 0.037752, loss_ce: 0.011856
2022-01-06 13:49:49,165 iteration 3322 : loss : 0.038774, loss_ce: 0.017496
2022-01-06 13:49:50,226 iteration 3323 : loss : 0.024842, loss_ce: 0.008085
2022-01-06 13:49:51,426 iteration 3324 : loss : 0.037988, loss_ce: 0.017238
2022-01-06 13:49:52,486 iteration 3325 : loss : 0.031663, loss_ce: 0.010604
2022-01-06 13:49:53,634 iteration 3326 : loss : 0.030242, loss_ce: 0.010673
2022-01-06 13:49:54,857 iteration 3327 : loss : 0.049840, loss_ce: 0.019095
2022-01-06 13:49:55,979 iteration 3328 : loss : 0.035339, loss_ce: 0.013816
2022-01-06 13:49:57,065 iteration 3329 : loss : 0.025961, loss_ce: 0.011063
2022-01-06 13:49:58,181 iteration 3330 : loss : 0.036741, loss_ce: 0.012387
2022-01-06 13:49:59,320 iteration 3331 : loss : 0.044791, loss_ce: 0.018157
2022-01-06 13:50:00,338 iteration 3332 : loss : 0.024644, loss_ce: 0.010958
 49%|█████████████▏             | 196/400 [1:08:27<1:11:23, 21.00s/it]2022-01-06 13:50:01,472 iteration 3333 : loss : 0.029205, loss_ce: 0.010554
2022-01-06 13:50:02,584 iteration 3334 : loss : 0.019794, loss_ce: 0.008970
2022-01-06 13:50:03,610 iteration 3335 : loss : 0.023095, loss_ce: 0.010232
2022-01-06 13:50:04,717 iteration 3336 : loss : 0.051711, loss_ce: 0.018222
2022-01-06 13:50:05,868 iteration 3337 : loss : 0.038663, loss_ce: 0.014914
2022-01-06 13:50:07,065 iteration 3338 : loss : 0.035040, loss_ce: 0.019559
2022-01-06 13:50:08,170 iteration 3339 : loss : 0.045255, loss_ce: 0.015540
2022-01-06 13:50:09,250 iteration 3340 : loss : 0.025805, loss_ce: 0.008672
2022-01-06 13:50:10,345 iteration 3341 : loss : 0.024669, loss_ce: 0.009159
2022-01-06 13:50:11,494 iteration 3342 : loss : 0.048894, loss_ce: 0.022903
2022-01-06 13:50:12,619 iteration 3343 : loss : 0.028202, loss_ce: 0.009835
2022-01-06 13:50:13,744 iteration 3344 : loss : 0.041753, loss_ce: 0.014027
2022-01-06 13:50:14,841 iteration 3345 : loss : 0.025892, loss_ce: 0.008705
2022-01-06 13:50:15,971 iteration 3346 : loss : 0.027453, loss_ce: 0.012239
2022-01-06 13:50:17,119 iteration 3347 : loss : 0.056057, loss_ce: 0.014906
2022-01-06 13:50:18,312 iteration 3348 : loss : 0.040559, loss_ce: 0.014144
2022-01-06 13:50:19,439 iteration 3349 : loss : 0.035456, loss_ce: 0.013699
 49%|█████████████▎             | 197/400 [1:08:46<1:09:06, 20.43s/it]2022-01-06 13:50:20,561 iteration 3350 : loss : 0.027786, loss_ce: 0.011064
2022-01-06 13:50:21,625 iteration 3351 : loss : 0.030984, loss_ce: 0.011749
2022-01-06 13:50:22,708 iteration 3352 : loss : 0.025914, loss_ce: 0.010121
2022-01-06 13:50:23,834 iteration 3353 : loss : 0.029008, loss_ce: 0.011839
2022-01-06 13:50:24,960 iteration 3354 : loss : 0.024557, loss_ce: 0.008093
2022-01-06 13:50:26,136 iteration 3355 : loss : 0.035120, loss_ce: 0.017575
2022-01-06 13:50:27,285 iteration 3356 : loss : 0.033166, loss_ce: 0.009812
2022-01-06 13:50:28,449 iteration 3357 : loss : 0.038372, loss_ce: 0.012753
2022-01-06 13:50:29,626 iteration 3358 : loss : 0.050079, loss_ce: 0.016098
2022-01-06 13:50:30,762 iteration 3359 : loss : 0.031014, loss_ce: 0.012196
2022-01-06 13:50:31,953 iteration 3360 : loss : 0.035947, loss_ce: 0.017075
2022-01-06 13:50:32,998 iteration 3361 : loss : 0.036037, loss_ce: 0.008374
2022-01-06 13:50:34,062 iteration 3362 : loss : 0.031617, loss_ce: 0.013372
2022-01-06 13:50:35,118 iteration 3363 : loss : 0.032880, loss_ce: 0.016190
2022-01-06 13:50:36,238 iteration 3364 : loss : 0.031504, loss_ce: 0.010004
2022-01-06 13:50:37,293 iteration 3365 : loss : 0.030503, loss_ce: 0.016856
2022-01-06 13:50:38,383 iteration 3366 : loss : 0.031319, loss_ce: 0.011984
 50%|█████████████▎             | 198/400 [1:09:05<1:07:16, 19.98s/it]2022-01-06 13:50:39,538 iteration 3367 : loss : 0.038440, loss_ce: 0.012493
2022-01-06 13:50:40,626 iteration 3368 : loss : 0.029173, loss_ce: 0.013504
2022-01-06 13:50:41,746 iteration 3369 : loss : 0.033445, loss_ce: 0.013543
2022-01-06 13:50:42,869 iteration 3370 : loss : 0.037586, loss_ce: 0.014984
2022-01-06 13:50:43,882 iteration 3371 : loss : 0.022134, loss_ce: 0.009709
2022-01-06 13:50:44,960 iteration 3372 : loss : 0.026296, loss_ce: 0.013171
2022-01-06 13:50:46,047 iteration 3373 : loss : 0.024519, loss_ce: 0.012224
2022-01-06 13:50:47,092 iteration 3374 : loss : 0.027408, loss_ce: 0.011609
2022-01-06 13:50:48,148 iteration 3375 : loss : 0.029825, loss_ce: 0.011974
2022-01-06 13:50:49,215 iteration 3376 : loss : 0.039894, loss_ce: 0.015885
2022-01-06 13:50:50,352 iteration 3377 : loss : 0.056447, loss_ce: 0.017686
2022-01-06 13:50:51,443 iteration 3378 : loss : 0.030636, loss_ce: 0.010804
2022-01-06 13:50:52,463 iteration 3379 : loss : 0.027104, loss_ce: 0.012447
2022-01-06 13:50:53,618 iteration 3380 : loss : 0.042265, loss_ce: 0.017321
2022-01-06 13:50:54,662 iteration 3381 : loss : 0.052998, loss_ce: 0.010330
2022-01-06 13:50:55,785 iteration 3382 : loss : 0.037384, loss_ce: 0.013629
2022-01-06 13:50:56,895 iteration 3383 : loss : 0.031679, loss_ce: 0.011064
 50%|█████████████▍             | 199/400 [1:09:23<1:05:27, 19.54s/it]2022-01-06 13:50:58,037 iteration 3384 : loss : 0.038418, loss_ce: 0.012743
2022-01-06 13:50:59,214 iteration 3385 : loss : 0.034477, loss_ce: 0.013997
2022-01-06 13:51:00,406 iteration 3386 : loss : 0.038025, loss_ce: 0.016204
2022-01-06 13:51:01,575 iteration 3387 : loss : 0.035570, loss_ce: 0.013403
2022-01-06 13:51:02,650 iteration 3388 : loss : 0.033782, loss_ce: 0.012379
2022-01-06 13:51:03,818 iteration 3389 : loss : 0.037325, loss_ce: 0.013751
2022-01-06 13:51:04,900 iteration 3390 : loss : 0.040966, loss_ce: 0.012762
2022-01-06 13:51:06,136 iteration 3391 : loss : 0.042964, loss_ce: 0.014652
2022-01-06 13:51:07,302 iteration 3392 : loss : 0.043072, loss_ce: 0.016186
2022-01-06 13:51:08,459 iteration 3393 : loss : 0.043810, loss_ce: 0.016576
2022-01-06 13:51:09,507 iteration 3394 : loss : 0.034035, loss_ce: 0.011866
2022-01-06 13:51:10,742 iteration 3395 : loss : 0.049264, loss_ce: 0.016917
2022-01-06 13:51:11,892 iteration 3396 : loss : 0.032774, loss_ce: 0.011449
2022-01-06 13:51:12,989 iteration 3397 : loss : 0.027602, loss_ce: 0.011182
2022-01-06 13:51:14,137 iteration 3398 : loss : 0.032371, loss_ce: 0.010991
2022-01-06 13:51:15,253 iteration 3399 : loss : 0.028544, loss_ce: 0.011398
2022-01-06 13:51:15,254 Training Data Eval:
2022-01-06 13:51:20,752   Average segmentation loss on training set: 0.0520
2022-01-06 13:51:20,753 Validation Data Eval:
2022-01-06 13:51:22,664   Average segmentation loss on validation set: 0.2002
2022-01-06 13:51:23,780 iteration 3400 : loss : 0.030232, loss_ce: 0.008797
 50%|█████████████▌             | 200/400 [1:09:50<1:12:28, 21.74s/it]2022-01-06 13:51:25,062 iteration 3401 : loss : 0.037175, loss_ce: 0.014488
2022-01-06 13:51:26,207 iteration 3402 : loss : 0.030782, loss_ce: 0.011349
2022-01-06 13:51:27,276 iteration 3403 : loss : 0.035767, loss_ce: 0.017633
2022-01-06 13:51:28,347 iteration 3404 : loss : 0.027626, loss_ce: 0.008721
2022-01-06 13:51:29,422 iteration 3405 : loss : 0.033445, loss_ce: 0.012732
2022-01-06 13:51:30,557 iteration 3406 : loss : 0.038831, loss_ce: 0.014326
2022-01-06 13:51:31,657 iteration 3407 : loss : 0.037748, loss_ce: 0.015047
2022-01-06 13:51:32,767 iteration 3408 : loss : 0.024751, loss_ce: 0.009273
2022-01-06 13:51:33,889 iteration 3409 : loss : 0.034116, loss_ce: 0.015053
2022-01-06 13:51:35,005 iteration 3410 : loss : 0.038014, loss_ce: 0.013340
2022-01-06 13:51:36,153 iteration 3411 : loss : 0.035440, loss_ce: 0.012350
2022-01-06 13:51:37,198 iteration 3412 : loss : 0.046573, loss_ce: 0.015561
2022-01-06 13:51:38,225 iteration 3413 : loss : 0.023281, loss_ce: 0.010117
2022-01-06 13:51:39,346 iteration 3414 : loss : 0.047041, loss_ce: 0.011871
2022-01-06 13:51:40,485 iteration 3415 : loss : 0.024470, loss_ce: 0.009290
2022-01-06 13:51:41,636 iteration 3416 : loss : 0.024380, loss_ce: 0.007206
2022-01-06 13:51:42,834 iteration 3417 : loss : 0.044069, loss_ce: 0.017227
 50%|█████████████▌             | 201/400 [1:10:09<1:09:26, 20.94s/it]2022-01-06 13:51:44,018 iteration 3418 : loss : 0.026061, loss_ce: 0.011844
2022-01-06 13:51:45,169 iteration 3419 : loss : 0.030065, loss_ce: 0.010293
2022-01-06 13:51:46,394 iteration 3420 : loss : 0.041078, loss_ce: 0.012718
2022-01-06 13:51:47,551 iteration 3421 : loss : 0.040499, loss_ce: 0.014365
2022-01-06 13:51:48,638 iteration 3422 : loss : 0.025617, loss_ce: 0.007949
2022-01-06 13:51:49,774 iteration 3423 : loss : 0.023899, loss_ce: 0.009863
2022-01-06 13:51:50,906 iteration 3424 : loss : 0.040841, loss_ce: 0.015772
2022-01-06 13:51:51,998 iteration 3425 : loss : 0.028146, loss_ce: 0.012679
2022-01-06 13:51:53,144 iteration 3426 : loss : 0.030929, loss_ce: 0.010614
2022-01-06 13:51:54,302 iteration 3427 : loss : 0.035216, loss_ce: 0.011381
2022-01-06 13:51:55,452 iteration 3428 : loss : 0.030349, loss_ce: 0.012581
2022-01-06 13:51:56,617 iteration 3429 : loss : 0.034122, loss_ce: 0.011849
2022-01-06 13:51:57,782 iteration 3430 : loss : 0.045843, loss_ce: 0.018354
2022-01-06 13:51:58,821 iteration 3431 : loss : 0.040068, loss_ce: 0.014679
2022-01-06 13:51:59,893 iteration 3432 : loss : 0.036834, loss_ce: 0.011343
2022-01-06 13:52:00,964 iteration 3433 : loss : 0.025794, loss_ce: 0.009259
2022-01-06 13:52:02,128 iteration 3434 : loss : 0.047929, loss_ce: 0.014212
 50%|█████████████▋             | 202/400 [1:10:29<1:07:28, 20.45s/it]2022-01-06 13:52:03,226 iteration 3435 : loss : 0.031904, loss_ce: 0.011166
2022-01-06 13:52:04,315 iteration 3436 : loss : 0.021515, loss_ce: 0.008110
2022-01-06 13:52:05,524 iteration 3437 : loss : 0.039451, loss_ce: 0.021595
2022-01-06 13:52:06,711 iteration 3438 : loss : 0.028467, loss_ce: 0.012465
2022-01-06 13:52:07,878 iteration 3439 : loss : 0.029739, loss_ce: 0.010191
2022-01-06 13:52:08,974 iteration 3440 : loss : 0.031367, loss_ce: 0.011529
2022-01-06 13:52:10,098 iteration 3441 : loss : 0.029764, loss_ce: 0.011367
2022-01-06 13:52:11,320 iteration 3442 : loss : 0.034876, loss_ce: 0.012623
2022-01-06 13:52:12,475 iteration 3443 : loss : 0.030098, loss_ce: 0.009889
2022-01-06 13:52:13,635 iteration 3444 : loss : 0.046200, loss_ce: 0.015791
2022-01-06 13:52:14,771 iteration 3445 : loss : 0.035788, loss_ce: 0.011025
2022-01-06 13:52:15,891 iteration 3446 : loss : 0.027442, loss_ce: 0.010821
2022-01-06 13:52:17,005 iteration 3447 : loss : 0.033947, loss_ce: 0.010798
2022-01-06 13:52:18,180 iteration 3448 : loss : 0.042028, loss_ce: 0.011007
2022-01-06 13:52:19,315 iteration 3449 : loss : 0.043912, loss_ce: 0.015889
2022-01-06 13:52:20,421 iteration 3450 : loss : 0.036251, loss_ce: 0.014849
2022-01-06 13:52:21,607 iteration 3451 : loss : 0.033253, loss_ce: 0.012791
 51%|█████████████▋             | 203/400 [1:10:48<1:06:10, 20.15s/it]2022-01-06 13:52:22,766 iteration 3452 : loss : 0.029063, loss_ce: 0.010396
2022-01-06 13:52:23,849 iteration 3453 : loss : 0.032313, loss_ce: 0.012347
2022-01-06 13:52:24,921 iteration 3454 : loss : 0.033801, loss_ce: 0.010600
2022-01-06 13:52:26,104 iteration 3455 : loss : 0.040137, loss_ce: 0.017760
2022-01-06 13:52:27,186 iteration 3456 : loss : 0.027314, loss_ce: 0.008600
2022-01-06 13:52:28,395 iteration 3457 : loss : 0.035809, loss_ce: 0.010386
2022-01-06 13:52:29,546 iteration 3458 : loss : 0.030399, loss_ce: 0.010560
2022-01-06 13:52:30,650 iteration 3459 : loss : 0.033127, loss_ce: 0.013138
2022-01-06 13:52:31,742 iteration 3460 : loss : 0.030227, loss_ce: 0.012606
2022-01-06 13:52:32,841 iteration 3461 : loss : 0.028240, loss_ce: 0.010136
2022-01-06 13:52:33,921 iteration 3462 : loss : 0.033037, loss_ce: 0.016314
2022-01-06 13:52:35,052 iteration 3463 : loss : 0.035182, loss_ce: 0.013212
2022-01-06 13:52:36,093 iteration 3464 : loss : 0.023640, loss_ce: 0.009101
2022-01-06 13:52:37,237 iteration 3465 : loss : 0.039918, loss_ce: 0.012333
2022-01-06 13:52:38,347 iteration 3466 : loss : 0.020603, loss_ce: 0.006953
2022-01-06 13:52:39,473 iteration 3467 : loss : 0.027988, loss_ce: 0.013929
2022-01-06 13:52:40,559 iteration 3468 : loss : 0.040122, loss_ce: 0.019131
 51%|█████████████▊             | 204/400 [1:11:07<1:04:39, 19.79s/it]2022-01-06 13:52:41,671 iteration 3469 : loss : 0.027216, loss_ce: 0.008598
2022-01-06 13:52:42,818 iteration 3470 : loss : 0.044988, loss_ce: 0.013963
2022-01-06 13:52:43,849 iteration 3471 : loss : 0.027747, loss_ce: 0.009837
2022-01-06 13:52:44,961 iteration 3472 : loss : 0.030156, loss_ce: 0.013315
2022-01-06 13:52:46,168 iteration 3473 : loss : 0.043121, loss_ce: 0.016160
2022-01-06 13:52:47,260 iteration 3474 : loss : 0.033487, loss_ce: 0.010515
2022-01-06 13:52:48,423 iteration 3475 : loss : 0.054852, loss_ce: 0.018688
2022-01-06 13:52:49,482 iteration 3476 : loss : 0.030034, loss_ce: 0.008314
2022-01-06 13:52:50,606 iteration 3477 : loss : 0.030757, loss_ce: 0.009892
2022-01-06 13:52:51,697 iteration 3478 : loss : 0.024685, loss_ce: 0.010274
2022-01-06 13:52:52,756 iteration 3479 : loss : 0.030677, loss_ce: 0.011697
2022-01-06 13:52:53,765 iteration 3480 : loss : 0.026705, loss_ce: 0.010359
2022-01-06 13:52:54,852 iteration 3481 : loss : 0.044499, loss_ce: 0.017474
2022-01-06 13:52:55,942 iteration 3482 : loss : 0.034652, loss_ce: 0.012937
2022-01-06 13:52:57,094 iteration 3483 : loss : 0.034486, loss_ce: 0.015576
2022-01-06 13:52:58,104 iteration 3484 : loss : 0.030346, loss_ce: 0.012126
2022-01-06 13:52:58,104 Training Data Eval:
2022-01-06 13:53:03,525   Average segmentation loss on training set: 0.0469
2022-01-06 13:53:03,525 Validation Data Eval:
2022-01-06 13:53:05,450   Average segmentation loss on validation set: 0.0905
2022-01-06 13:53:06,659 iteration 3485 : loss : 0.039336, loss_ce: 0.016781
 51%|█████████████▊             | 205/400 [1:11:33<1:10:28, 21.69s/it]2022-01-06 13:53:07,837 iteration 3486 : loss : 0.030406, loss_ce: 0.012300
2022-01-06 13:53:08,904 iteration 3487 : loss : 0.027848, loss_ce: 0.012454
2022-01-06 13:53:09,995 iteration 3488 : loss : 0.034005, loss_ce: 0.012696
2022-01-06 13:53:11,094 iteration 3489 : loss : 0.030192, loss_ce: 0.011101
2022-01-06 13:53:12,217 iteration 3490 : loss : 0.034572, loss_ce: 0.014160
2022-01-06 13:53:13,372 iteration 3491 : loss : 0.034111, loss_ce: 0.012815
2022-01-06 13:53:14,409 iteration 3492 : loss : 0.034923, loss_ce: 0.012952
2022-01-06 13:53:15,607 iteration 3493 : loss : 0.026982, loss_ce: 0.010236
2022-01-06 13:53:16,736 iteration 3494 : loss : 0.052602, loss_ce: 0.023195
2022-01-06 13:53:17,824 iteration 3495 : loss : 0.036416, loss_ce: 0.012483
2022-01-06 13:53:18,945 iteration 3496 : loss : 0.028075, loss_ce: 0.011524
2022-01-06 13:53:20,071 iteration 3497 : loss : 0.033811, loss_ce: 0.014051
2022-01-06 13:53:21,274 iteration 3498 : loss : 0.034829, loss_ce: 0.012650
2022-01-06 13:53:22,341 iteration 3499 : loss : 0.027464, loss_ce: 0.010764
2022-01-06 13:53:23,353 iteration 3500 : loss : 0.024886, loss_ce: 0.007829
2022-01-06 13:53:24,408 iteration 3501 : loss : 0.035626, loss_ce: 0.012356
2022-01-06 13:53:25,517 iteration 3502 : loss : 0.042026, loss_ce: 0.018771
 52%|█████████████▉             | 206/400 [1:11:52<1:07:22, 20.84s/it]2022-01-06 13:53:26,679 iteration 3503 : loss : 0.046754, loss_ce: 0.021701
2022-01-06 13:53:27,753 iteration 3504 : loss : 0.025496, loss_ce: 0.009042
2022-01-06 13:53:28,754 iteration 3505 : loss : 0.024547, loss_ce: 0.012599
2022-01-06 13:53:29,861 iteration 3506 : loss : 0.036382, loss_ce: 0.011392
2022-01-06 13:53:30,938 iteration 3507 : loss : 0.035845, loss_ce: 0.011104
2022-01-06 13:53:32,144 iteration 3508 : loss : 0.040749, loss_ce: 0.016031
2022-01-06 13:53:33,255 iteration 3509 : loss : 0.026446, loss_ce: 0.009666
2022-01-06 13:53:34,301 iteration 3510 : loss : 0.027600, loss_ce: 0.010884
2022-01-06 13:53:35,338 iteration 3511 : loss : 0.022444, loss_ce: 0.009930
2022-01-06 13:53:36,493 iteration 3512 : loss : 0.038460, loss_ce: 0.011536
2022-01-06 13:53:37,587 iteration 3513 : loss : 0.026841, loss_ce: 0.007882
2022-01-06 13:53:38,800 iteration 3514 : loss : 0.047424, loss_ce: 0.023672
2022-01-06 13:53:39,875 iteration 3515 : loss : 0.025225, loss_ce: 0.007676
2022-01-06 13:53:40,946 iteration 3516 : loss : 0.023972, loss_ce: 0.009723
2022-01-06 13:53:42,019 iteration 3517 : loss : 0.038753, loss_ce: 0.014747
2022-01-06 13:53:43,093 iteration 3518 : loss : 0.026825, loss_ce: 0.013639
2022-01-06 13:53:44,204 iteration 3519 : loss : 0.032871, loss_ce: 0.012505
 52%|█████████████▉             | 207/400 [1:12:11<1:04:57, 20.19s/it]2022-01-06 13:53:45,360 iteration 3520 : loss : 0.032214, loss_ce: 0.011570
2022-01-06 13:53:46,420 iteration 3521 : loss : 0.044137, loss_ce: 0.020646
2022-01-06 13:53:47,446 iteration 3522 : loss : 0.022957, loss_ce: 0.010765
2022-01-06 13:53:48,522 iteration 3523 : loss : 0.027490, loss_ce: 0.009864
2022-01-06 13:53:49,643 iteration 3524 : loss : 0.024657, loss_ce: 0.010274
2022-01-06 13:53:50,836 iteration 3525 : loss : 0.027109, loss_ce: 0.010367
2022-01-06 13:53:51,960 iteration 3526 : loss : 0.033373, loss_ce: 0.013180
2022-01-06 13:53:53,099 iteration 3527 : loss : 0.040472, loss_ce: 0.011573
2022-01-06 13:53:54,323 iteration 3528 : loss : 0.035757, loss_ce: 0.014955
2022-01-06 13:53:55,458 iteration 3529 : loss : 0.045729, loss_ce: 0.015484
2022-01-06 13:53:56,582 iteration 3530 : loss : 0.020481, loss_ce: 0.007765
2022-01-06 13:53:57,638 iteration 3531 : loss : 0.034076, loss_ce: 0.009978
2022-01-06 13:53:58,810 iteration 3532 : loss : 0.040700, loss_ce: 0.017630
2022-01-06 13:53:59,893 iteration 3533 : loss : 0.022083, loss_ce: 0.007590
2022-01-06 13:54:01,013 iteration 3534 : loss : 0.039062, loss_ce: 0.014855
2022-01-06 13:54:02,194 iteration 3535 : loss : 0.035796, loss_ce: 0.013795
2022-01-06 13:54:03,333 iteration 3536 : loss : 0.028105, loss_ce: 0.010303
 52%|██████████████             | 208/400 [1:12:30<1:03:35, 19.87s/it]2022-01-06 13:54:04,412 iteration 3537 : loss : 0.025587, loss_ce: 0.010079
2022-01-06 13:54:05,436 iteration 3538 : loss : 0.021090, loss_ce: 0.010745
2022-01-06 13:54:06,481 iteration 3539 : loss : 0.022749, loss_ce: 0.010425
2022-01-06 13:54:07,468 iteration 3540 : loss : 0.027027, loss_ce: 0.008354
2022-01-06 13:54:08,629 iteration 3541 : loss : 0.031842, loss_ce: 0.011847
2022-01-06 13:54:09,717 iteration 3542 : loss : 0.026502, loss_ce: 0.011715
2022-01-06 13:54:10,923 iteration 3543 : loss : 0.043078, loss_ce: 0.012334
2022-01-06 13:54:11,945 iteration 3544 : loss : 0.020905, loss_ce: 0.007983
2022-01-06 13:54:13,032 iteration 3545 : loss : 0.027596, loss_ce: 0.012103
2022-01-06 13:54:14,062 iteration 3546 : loss : 0.027252, loss_ce: 0.010101
2022-01-06 13:54:15,157 iteration 3547 : loss : 0.038458, loss_ce: 0.018555
2022-01-06 13:54:16,214 iteration 3548 : loss : 0.029921, loss_ce: 0.009633
2022-01-06 13:54:17,333 iteration 3549 : loss : 0.027931, loss_ce: 0.010607
2022-01-06 13:54:18,480 iteration 3550 : loss : 0.028393, loss_ce: 0.012798
2022-01-06 13:54:19,645 iteration 3551 : loss : 0.038498, loss_ce: 0.011623
2022-01-06 13:54:20,808 iteration 3552 : loss : 0.045223, loss_ce: 0.013373
2022-01-06 13:54:21,963 iteration 3553 : loss : 0.054934, loss_ce: 0.025381
 52%|██████████████             | 209/400 [1:12:48<1:02:04, 19.50s/it]2022-01-06 13:54:23,128 iteration 3554 : loss : 0.040817, loss_ce: 0.018514
2022-01-06 13:54:24,196 iteration 3555 : loss : 0.032886, loss_ce: 0.010119
2022-01-06 13:54:25,264 iteration 3556 : loss : 0.028242, loss_ce: 0.012379
2022-01-06 13:54:26,434 iteration 3557 : loss : 0.040142, loss_ce: 0.012296
2022-01-06 13:54:27,474 iteration 3558 : loss : 0.042822, loss_ce: 0.011896
2022-01-06 13:54:28,552 iteration 3559 : loss : 0.029554, loss_ce: 0.009473
2022-01-06 13:54:29,532 iteration 3560 : loss : 0.025598, loss_ce: 0.012640
2022-01-06 13:54:30,702 iteration 3561 : loss : 0.036967, loss_ce: 0.014525
2022-01-06 13:54:31,761 iteration 3562 : loss : 0.025230, loss_ce: 0.009774
2022-01-06 13:54:32,887 iteration 3563 : loss : 0.027870, loss_ce: 0.010229
2022-01-06 13:54:33,985 iteration 3564 : loss : 0.036547, loss_ce: 0.013386
2022-01-06 13:54:35,186 iteration 3565 : loss : 0.040063, loss_ce: 0.015433
2022-01-06 13:54:36,295 iteration 3566 : loss : 0.038513, loss_ce: 0.014175
2022-01-06 13:54:37,467 iteration 3567 : loss : 0.028622, loss_ce: 0.013637
2022-01-06 13:54:38,622 iteration 3568 : loss : 0.035933, loss_ce: 0.015968
2022-01-06 13:54:39,765 iteration 3569 : loss : 0.057442, loss_ce: 0.012897
2022-01-06 13:54:39,765 Training Data Eval:
2022-01-06 13:54:45,085   Average segmentation loss on training set: 0.0253
2022-01-06 13:54:45,085 Validation Data Eval:
2022-01-06 13:54:46,906   Average segmentation loss on validation set: 0.0725
2022-01-06 13:54:52,882 Found new lowest validation loss at iteration 3569! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 13:54:54,038 iteration 3570 : loss : 0.031560, loss_ce: 0.011574
 52%|██████████████▏            | 210/400 [1:13:20<1:13:41, 23.27s/it]2022-01-06 13:54:55,177 iteration 3571 : loss : 0.039594, loss_ce: 0.015539
2022-01-06 13:54:56,267 iteration 3572 : loss : 0.024231, loss_ce: 0.006686
2022-01-06 13:54:57,357 iteration 3573 : loss : 0.044428, loss_ce: 0.017988
2022-01-06 13:54:58,420 iteration 3574 : loss : 0.027642, loss_ce: 0.011000
2022-01-06 13:54:59,489 iteration 3575 : loss : 0.024068, loss_ce: 0.009571
2022-01-06 13:55:00,590 iteration 3576 : loss : 0.038867, loss_ce: 0.015896
2022-01-06 13:55:01,731 iteration 3577 : loss : 0.029347, loss_ce: 0.012412
2022-01-06 13:55:02,868 iteration 3578 : loss : 0.043412, loss_ce: 0.016942
2022-01-06 13:55:03,939 iteration 3579 : loss : 0.026732, loss_ce: 0.010099
2022-01-06 13:55:05,060 iteration 3580 : loss : 0.029682, loss_ce: 0.009795
2022-01-06 13:55:06,119 iteration 3581 : loss : 0.038005, loss_ce: 0.008459
2022-01-06 13:55:07,204 iteration 3582 : loss : 0.027192, loss_ce: 0.010798
2022-01-06 13:55:08,326 iteration 3583 : loss : 0.033073, loss_ce: 0.012810
2022-01-06 13:55:09,381 iteration 3584 : loss : 0.029272, loss_ce: 0.011934
2022-01-06 13:55:10,457 iteration 3585 : loss : 0.030821, loss_ce: 0.011709
2022-01-06 13:55:11,488 iteration 3586 : loss : 0.027385, loss_ce: 0.011366
2022-01-06 13:55:12,565 iteration 3587 : loss : 0.040312, loss_ce: 0.017986
 53%|██████████████▏            | 211/400 [1:13:39<1:08:49, 21.85s/it]2022-01-06 13:55:13,629 iteration 3588 : loss : 0.027356, loss_ce: 0.009939
2022-01-06 13:55:14,755 iteration 3589 : loss : 0.032820, loss_ce: 0.012601
2022-01-06 13:55:15,844 iteration 3590 : loss : 0.030255, loss_ce: 0.012073
2022-01-06 13:55:16,916 iteration 3591 : loss : 0.023283, loss_ce: 0.007157
2022-01-06 13:55:18,058 iteration 3592 : loss : 0.026448, loss_ce: 0.010873
2022-01-06 13:55:19,165 iteration 3593 : loss : 0.039672, loss_ce: 0.009712
2022-01-06 13:55:20,269 iteration 3594 : loss : 0.025700, loss_ce: 0.007448
2022-01-06 13:55:21,489 iteration 3595 : loss : 0.068005, loss_ce: 0.015081
2022-01-06 13:55:22,516 iteration 3596 : loss : 0.029392, loss_ce: 0.013149
2022-01-06 13:55:23,685 iteration 3597 : loss : 0.040283, loss_ce: 0.013840
2022-01-06 13:55:24,775 iteration 3598 : loss : 0.039557, loss_ce: 0.016872
2022-01-06 13:55:25,877 iteration 3599 : loss : 0.033747, loss_ce: 0.013114
2022-01-06 13:55:26,952 iteration 3600 : loss : 0.030368, loss_ce: 0.013873
2022-01-06 13:55:28,041 iteration 3601 : loss : 0.036009, loss_ce: 0.017728
2022-01-06 13:55:29,109 iteration 3602 : loss : 0.032087, loss_ce: 0.012884
2022-01-06 13:55:30,155 iteration 3603 : loss : 0.022322, loss_ce: 0.008994
2022-01-06 13:55:31,192 iteration 3604 : loss : 0.022960, loss_ce: 0.008851
 53%|██████████████▎            | 212/400 [1:13:58<1:05:26, 20.88s/it]2022-01-06 13:55:32,327 iteration 3605 : loss : 0.022587, loss_ce: 0.009104
2022-01-06 13:55:33,417 iteration 3606 : loss : 0.042966, loss_ce: 0.015478
2022-01-06 13:55:34,439 iteration 3607 : loss : 0.027712, loss_ce: 0.010024
2022-01-06 13:55:35,465 iteration 3608 : loss : 0.030272, loss_ce: 0.012793
2022-01-06 13:55:36,574 iteration 3609 : loss : 0.038970, loss_ce: 0.010903
2022-01-06 13:55:37,620 iteration 3610 : loss : 0.030963, loss_ce: 0.013056
2022-01-06 13:55:38,735 iteration 3611 : loss : 0.026512, loss_ce: 0.010467
2022-01-06 13:55:39,874 iteration 3612 : loss : 0.028901, loss_ce: 0.010172
2022-01-06 13:55:40,968 iteration 3613 : loss : 0.027357, loss_ce: 0.011034
2022-01-06 13:55:42,100 iteration 3614 : loss : 0.021983, loss_ce: 0.006710
2022-01-06 13:55:43,159 iteration 3615 : loss : 0.046298, loss_ce: 0.017880
2022-01-06 13:55:44,313 iteration 3616 : loss : 0.034100, loss_ce: 0.009592
2022-01-06 13:55:45,378 iteration 3617 : loss : 0.028909, loss_ce: 0.010811
2022-01-06 13:55:46,456 iteration 3618 : loss : 0.031606, loss_ce: 0.012948
2022-01-06 13:55:47,532 iteration 3619 : loss : 0.024864, loss_ce: 0.010460
2022-01-06 13:55:48,649 iteration 3620 : loss : 0.023543, loss_ce: 0.009455
2022-01-06 13:55:49,720 iteration 3621 : loss : 0.027329, loss_ce: 0.011639
 53%|██████████████▍            | 213/400 [1:14:16<1:02:52, 20.18s/it]2022-01-06 13:55:50,928 iteration 3622 : loss : 0.033400, loss_ce: 0.015333
2022-01-06 13:55:52,021 iteration 3623 : loss : 0.026889, loss_ce: 0.010964
2022-01-06 13:55:53,187 iteration 3624 : loss : 0.024217, loss_ce: 0.007468
2022-01-06 13:55:54,342 iteration 3625 : loss : 0.044647, loss_ce: 0.018259
2022-01-06 13:55:55,428 iteration 3626 : loss : 0.040881, loss_ce: 0.015831
2022-01-06 13:55:56,560 iteration 3627 : loss : 0.025066, loss_ce: 0.011288
2022-01-06 13:55:57,665 iteration 3628 : loss : 0.030482, loss_ce: 0.008887
2022-01-06 13:55:58,828 iteration 3629 : loss : 0.026089, loss_ce: 0.009136
2022-01-06 13:55:59,916 iteration 3630 : loss : 0.033966, loss_ce: 0.014855
2022-01-06 13:56:00,949 iteration 3631 : loss : 0.023382, loss_ce: 0.009241
2022-01-06 13:56:01,977 iteration 3632 : loss : 0.026289, loss_ce: 0.011829
2022-01-06 13:56:03,115 iteration 3633 : loss : 0.055687, loss_ce: 0.029892
2022-01-06 13:56:04,198 iteration 3634 : loss : 0.034310, loss_ce: 0.012460
2022-01-06 13:56:05,249 iteration 3635 : loss : 0.030066, loss_ce: 0.010271
2022-01-06 13:56:06,300 iteration 3636 : loss : 0.027685, loss_ce: 0.010565
2022-01-06 13:56:07,439 iteration 3637 : loss : 0.045008, loss_ce: 0.014669
2022-01-06 13:56:08,475 iteration 3638 : loss : 0.030669, loss_ce: 0.011226
 54%|██████████████▍            | 214/400 [1:14:35<1:01:13, 19.75s/it]2022-01-06 13:56:09,599 iteration 3639 : loss : 0.028462, loss_ce: 0.013193
2022-01-06 13:56:10,730 iteration 3640 : loss : 0.023314, loss_ce: 0.008745
2022-01-06 13:56:11,829 iteration 3641 : loss : 0.033076, loss_ce: 0.010144
2022-01-06 13:56:12,958 iteration 3642 : loss : 0.030101, loss_ce: 0.012734
2022-01-06 13:56:14,046 iteration 3643 : loss : 0.033240, loss_ce: 0.015331
2022-01-06 13:56:15,117 iteration 3644 : loss : 0.027120, loss_ce: 0.014934
2022-01-06 13:56:16,170 iteration 3645 : loss : 0.022554, loss_ce: 0.009276
2022-01-06 13:56:17,267 iteration 3646 : loss : 0.022556, loss_ce: 0.007916
2022-01-06 13:56:18,394 iteration 3647 : loss : 0.030248, loss_ce: 0.012143
2022-01-06 13:56:19,594 iteration 3648 : loss : 0.046092, loss_ce: 0.018998
2022-01-06 13:56:20,706 iteration 3649 : loss : 0.028401, loss_ce: 0.009803
2022-01-06 13:56:21,792 iteration 3650 : loss : 0.026745, loss_ce: 0.008028
2022-01-06 13:56:22,904 iteration 3651 : loss : 0.028279, loss_ce: 0.010499
2022-01-06 13:56:23,966 iteration 3652 : loss : 0.026962, loss_ce: 0.011594
2022-01-06 13:56:25,120 iteration 3653 : loss : 0.043622, loss_ce: 0.009352
2022-01-06 13:56:26,219 iteration 3654 : loss : 0.027085, loss_ce: 0.007382
2022-01-06 13:56:26,219 Training Data Eval:
2022-01-06 13:56:31,512   Average segmentation loss on training set: 0.0239
2022-01-06 13:56:31,513 Validation Data Eval:
2022-01-06 13:56:33,324   Average segmentation loss on validation set: 0.1036
2022-01-06 13:56:34,319 iteration 3655 : loss : 0.025469, loss_ce: 0.008512
 54%|██████████████▌            | 215/400 [1:15:01<1:06:31, 21.58s/it]2022-01-06 13:56:35,519 iteration 3656 : loss : 0.049668, loss_ce: 0.009988
2022-01-06 13:56:36,579 iteration 3657 : loss : 0.032217, loss_ce: 0.015729
2022-01-06 13:56:37,669 iteration 3658 : loss : 0.023510, loss_ce: 0.009238
2022-01-06 13:56:38,799 iteration 3659 : loss : 0.039223, loss_ce: 0.010964
2022-01-06 13:56:39,986 iteration 3660 : loss : 0.041109, loss_ce: 0.012937
2022-01-06 13:56:41,183 iteration 3661 : loss : 0.031981, loss_ce: 0.014594
2022-01-06 13:56:42,199 iteration 3662 : loss : 0.024872, loss_ce: 0.009176
2022-01-06 13:56:43,258 iteration 3663 : loss : 0.025460, loss_ce: 0.008714
2022-01-06 13:56:44,326 iteration 3664 : loss : 0.035245, loss_ce: 0.011469
2022-01-06 13:56:45,408 iteration 3665 : loss : 0.024477, loss_ce: 0.010424
2022-01-06 13:56:46,595 iteration 3666 : loss : 0.037277, loss_ce: 0.015816
2022-01-06 13:56:47,733 iteration 3667 : loss : 0.039316, loss_ce: 0.015471
2022-01-06 13:56:48,865 iteration 3668 : loss : 0.035564, loss_ce: 0.013883
2022-01-06 13:56:49,971 iteration 3669 : loss : 0.036838, loss_ce: 0.016299
2022-01-06 13:56:51,039 iteration 3670 : loss : 0.042215, loss_ce: 0.012523
2022-01-06 13:56:52,173 iteration 3671 : loss : 0.034755, loss_ce: 0.013727
2022-01-06 13:56:53,222 iteration 3672 : loss : 0.036635, loss_ce: 0.014448
 54%|██████████████▌            | 216/400 [1:15:20<1:03:42, 20.77s/it]2022-01-06 13:56:54,353 iteration 3673 : loss : 0.033249, loss_ce: 0.016558
2022-01-06 13:56:55,449 iteration 3674 : loss : 0.028508, loss_ce: 0.011854
2022-01-06 13:56:56,520 iteration 3675 : loss : 0.026431, loss_ce: 0.013288
2022-01-06 13:56:57,587 iteration 3676 : loss : 0.025121, loss_ce: 0.008355
2022-01-06 13:56:58,659 iteration 3677 : loss : 0.036276, loss_ce: 0.018088
2022-01-06 13:56:59,822 iteration 3678 : loss : 0.031160, loss_ce: 0.010350
2022-01-06 13:57:00,899 iteration 3679 : loss : 0.033192, loss_ce: 0.012254
2022-01-06 13:57:01,963 iteration 3680 : loss : 0.038956, loss_ce: 0.012687
2022-01-06 13:57:03,097 iteration 3681 : loss : 0.029631, loss_ce: 0.008741
2022-01-06 13:57:04,163 iteration 3682 : loss : 0.035533, loss_ce: 0.014950
2022-01-06 13:57:05,294 iteration 3683 : loss : 0.056889, loss_ce: 0.009348
2022-01-06 13:57:06,361 iteration 3684 : loss : 0.018878, loss_ce: 0.005356
2022-01-06 13:57:07,435 iteration 3685 : loss : 0.039105, loss_ce: 0.016868
2022-01-06 13:57:08,539 iteration 3686 : loss : 0.034172, loss_ce: 0.012947
2022-01-06 13:57:09,635 iteration 3687 : loss : 0.033095, loss_ce: 0.010485
2022-01-06 13:57:10,711 iteration 3688 : loss : 0.038034, loss_ce: 0.017690
2022-01-06 13:57:11,739 iteration 3689 : loss : 0.026311, loss_ce: 0.009189
 54%|██████████████▋            | 217/400 [1:15:38<1:01:18, 20.10s/it]2022-01-06 13:57:12,840 iteration 3690 : loss : 0.025360, loss_ce: 0.006543
2022-01-06 13:57:13,997 iteration 3691 : loss : 0.044867, loss_ce: 0.009433
2022-01-06 13:57:15,096 iteration 3692 : loss : 0.030039, loss_ce: 0.013405
2022-01-06 13:57:16,261 iteration 3693 : loss : 0.038828, loss_ce: 0.014547
2022-01-06 13:57:17,369 iteration 3694 : loss : 0.023100, loss_ce: 0.009673
2022-01-06 13:57:18,419 iteration 3695 : loss : 0.033474, loss_ce: 0.014425
2022-01-06 13:57:19,556 iteration 3696 : loss : 0.043128, loss_ce: 0.021937
2022-01-06 13:57:20,636 iteration 3697 : loss : 0.025774, loss_ce: 0.007750
2022-01-06 13:57:21,725 iteration 3698 : loss : 0.030735, loss_ce: 0.010914
2022-01-06 13:57:22,885 iteration 3699 : loss : 0.036855, loss_ce: 0.013260
2022-01-06 13:57:23,979 iteration 3700 : loss : 0.027830, loss_ce: 0.012876
2022-01-06 13:57:25,043 iteration 3701 : loss : 0.023383, loss_ce: 0.009821
2022-01-06 13:57:26,096 iteration 3702 : loss : 0.034199, loss_ce: 0.012987
2022-01-06 13:57:27,290 iteration 3703 : loss : 0.040160, loss_ce: 0.019671
2022-01-06 13:57:28,344 iteration 3704 : loss : 0.028234, loss_ce: 0.008652
2022-01-06 13:57:29,386 iteration 3705 : loss : 0.028808, loss_ce: 0.010380
2022-01-06 13:57:30,427 iteration 3706 : loss : 0.031802, loss_ce: 0.011947
 55%|███████████████▊             | 218/400 [1:15:57<59:40, 19.67s/it]2022-01-06 13:57:31,568 iteration 3707 : loss : 0.034602, loss_ce: 0.012183
2022-01-06 13:57:32,662 iteration 3708 : loss : 0.028684, loss_ce: 0.011541
2022-01-06 13:57:33,759 iteration 3709 : loss : 0.025617, loss_ce: 0.007920
2022-01-06 13:57:34,902 iteration 3710 : loss : 0.049503, loss_ce: 0.019553
2022-01-06 13:57:35,972 iteration 3711 : loss : 0.024782, loss_ce: 0.009717
2022-01-06 13:57:37,088 iteration 3712 : loss : 0.039419, loss_ce: 0.011740
2022-01-06 13:57:38,137 iteration 3713 : loss : 0.026610, loss_ce: 0.009696
2022-01-06 13:57:39,195 iteration 3714 : loss : 0.030130, loss_ce: 0.011085
2022-01-06 13:57:40,294 iteration 3715 : loss : 0.041530, loss_ce: 0.017583
2022-01-06 13:57:41,350 iteration 3716 : loss : 0.029543, loss_ce: 0.011109
2022-01-06 13:57:42,448 iteration 3717 : loss : 0.043402, loss_ce: 0.015600
2022-01-06 13:57:43,590 iteration 3718 : loss : 0.029817, loss_ce: 0.013298
2022-01-06 13:57:44,632 iteration 3719 : loss : 0.031081, loss_ce: 0.008369
2022-01-06 13:57:45,738 iteration 3720 : loss : 0.027711, loss_ce: 0.013707
2022-01-06 13:57:46,827 iteration 3721 : loss : 0.031505, loss_ce: 0.012074
2022-01-06 13:57:47,902 iteration 3722 : loss : 0.031241, loss_ce: 0.012757
2022-01-06 13:57:48,960 iteration 3723 : loss : 0.035080, loss_ce: 0.010974
 55%|███████████████▉             | 219/400 [1:16:15<58:19, 19.33s/it]2022-01-06 13:57:50,039 iteration 3724 : loss : 0.027026, loss_ce: 0.012118
2022-01-06 13:57:51,079 iteration 3725 : loss : 0.031748, loss_ce: 0.014038
2022-01-06 13:57:52,224 iteration 3726 : loss : 0.032129, loss_ce: 0.014021
2022-01-06 13:57:53,317 iteration 3727 : loss : 0.030627, loss_ce: 0.011303
2022-01-06 13:57:54,422 iteration 3728 : loss : 0.031775, loss_ce: 0.008117
2022-01-06 13:57:55,476 iteration 3729 : loss : 0.025954, loss_ce: 0.007532
2022-01-06 13:57:56,553 iteration 3730 : loss : 0.027494, loss_ce: 0.010772
2022-01-06 13:57:57,616 iteration 3731 : loss : 0.028536, loss_ce: 0.010346
2022-01-06 13:57:58,731 iteration 3732 : loss : 0.038144, loss_ce: 0.013584
2022-01-06 13:57:59,775 iteration 3733 : loss : 0.031391, loss_ce: 0.012424
2022-01-06 13:58:00,875 iteration 3734 : loss : 0.027433, loss_ce: 0.011844
2022-01-06 13:58:01,965 iteration 3735 : loss : 0.033694, loss_ce: 0.009687
2022-01-06 13:58:03,104 iteration 3736 : loss : 0.026064, loss_ce: 0.011703
2022-01-06 13:58:04,162 iteration 3737 : loss : 0.028209, loss_ce: 0.009159
2022-01-06 13:58:05,245 iteration 3738 : loss : 0.028055, loss_ce: 0.009593
2022-01-06 13:58:06,282 iteration 3739 : loss : 0.027522, loss_ce: 0.012156
2022-01-06 13:58:06,282 Training Data Eval:
2022-01-06 13:58:11,592   Average segmentation loss on training set: 0.0453
2022-01-06 13:58:11,592 Validation Data Eval:
2022-01-06 13:58:13,412   Average segmentation loss on validation set: 0.1366
2022-01-06 13:58:14,641 iteration 3740 : loss : 0.036337, loss_ce: 0.017158
 55%|██████████████▊            | 220/400 [1:16:41<1:03:42, 21.24s/it]2022-01-06 13:58:15,910 iteration 3741 : loss : 0.048028, loss_ce: 0.015575
2022-01-06 13:58:17,029 iteration 3742 : loss : 0.032060, loss_ce: 0.015574
2022-01-06 13:58:18,115 iteration 3743 : loss : 0.029351, loss_ce: 0.011594
2022-01-06 13:58:19,244 iteration 3744 : loss : 0.035894, loss_ce: 0.012430
2022-01-06 13:58:20,392 iteration 3745 : loss : 0.025239, loss_ce: 0.007596
2022-01-06 13:58:21,553 iteration 3746 : loss : 0.025741, loss_ce: 0.009848
2022-01-06 13:58:22,633 iteration 3747 : loss : 0.027631, loss_ce: 0.008508
2022-01-06 13:58:23,696 iteration 3748 : loss : 0.028304, loss_ce: 0.014181
2022-01-06 13:58:24,836 iteration 3749 : loss : 0.028700, loss_ce: 0.010120
2022-01-06 13:58:26,004 iteration 3750 : loss : 0.037139, loss_ce: 0.012503
2022-01-06 13:58:27,123 iteration 3751 : loss : 0.040205, loss_ce: 0.012633
2022-01-06 13:58:28,192 iteration 3752 : loss : 0.031528, loss_ce: 0.009133
2022-01-06 13:58:29,240 iteration 3753 : loss : 0.035168, loss_ce: 0.011759
2022-01-06 13:58:30,333 iteration 3754 : loss : 0.023211, loss_ce: 0.007782
2022-01-06 13:58:31,477 iteration 3755 : loss : 0.027884, loss_ce: 0.011796
2022-01-06 13:58:32,495 iteration 3756 : loss : 0.038541, loss_ce: 0.018271
2022-01-06 13:58:33,538 iteration 3757 : loss : 0.027519, loss_ce: 0.012253
 55%|██████████████▉            | 221/400 [1:17:00<1:01:16, 20.54s/it]2022-01-06 13:58:34,761 iteration 3758 : loss : 0.035320, loss_ce: 0.015757
2022-01-06 13:58:35,812 iteration 3759 : loss : 0.026337, loss_ce: 0.008741
2022-01-06 13:58:36,881 iteration 3760 : loss : 0.025386, loss_ce: 0.010428
2022-01-06 13:58:38,004 iteration 3761 : loss : 0.030835, loss_ce: 0.012199
2022-01-06 13:58:39,153 iteration 3762 : loss : 0.033610, loss_ce: 0.018574
2022-01-06 13:58:40,333 iteration 3763 : loss : 0.031311, loss_ce: 0.010683
2022-01-06 13:58:41,415 iteration 3764 : loss : 0.036292, loss_ce: 0.011083
2022-01-06 13:58:42,462 iteration 3765 : loss : 0.026540, loss_ce: 0.011775
2022-01-06 13:58:43,599 iteration 3766 : loss : 0.046103, loss_ce: 0.015362
2022-01-06 13:58:44,684 iteration 3767 : loss : 0.023219, loss_ce: 0.008491
2022-01-06 13:58:45,736 iteration 3768 : loss : 0.035797, loss_ce: 0.010783
2022-01-06 13:58:46,790 iteration 3769 : loss : 0.033659, loss_ce: 0.015364
2022-01-06 13:58:47,903 iteration 3770 : loss : 0.037336, loss_ce: 0.009554
2022-01-06 13:58:49,112 iteration 3771 : loss : 0.035788, loss_ce: 0.013138
2022-01-06 13:58:50,267 iteration 3772 : loss : 0.032351, loss_ce: 0.008848
2022-01-06 13:58:51,384 iteration 3773 : loss : 0.031561, loss_ce: 0.012678
2022-01-06 13:58:52,537 iteration 3774 : loss : 0.047722, loss_ce: 0.024194
 56%|████████████████             | 222/400 [1:17:19<59:33, 20.07s/it]2022-01-06 13:58:53,659 iteration 3775 : loss : 0.028491, loss_ce: 0.011798
2022-01-06 13:58:54,671 iteration 3776 : loss : 0.030542, loss_ce: 0.013855
2022-01-06 13:58:55,743 iteration 3777 : loss : 0.024110, loss_ce: 0.006891
2022-01-06 13:58:56,793 iteration 3778 : loss : 0.021784, loss_ce: 0.008869
2022-01-06 13:58:57,844 iteration 3779 : loss : 0.024217, loss_ce: 0.010727
2022-01-06 13:58:58,938 iteration 3780 : loss : 0.031325, loss_ce: 0.011729
2022-01-06 13:59:00,034 iteration 3781 : loss : 0.030602, loss_ce: 0.012860
2022-01-06 13:59:01,140 iteration 3782 : loss : 0.079302, loss_ce: 0.016118
2022-01-06 13:59:02,260 iteration 3783 : loss : 0.028932, loss_ce: 0.012832
2022-01-06 13:59:03,377 iteration 3784 : loss : 0.041165, loss_ce: 0.019529
2022-01-06 13:59:04,438 iteration 3785 : loss : 0.031818, loss_ce: 0.013241
2022-01-06 13:59:05,497 iteration 3786 : loss : 0.023715, loss_ce: 0.008890
2022-01-06 13:59:06,543 iteration 3787 : loss : 0.029885, loss_ce: 0.010472
2022-01-06 13:59:07,688 iteration 3788 : loss : 0.055716, loss_ce: 0.013017
2022-01-06 13:59:08,774 iteration 3789 : loss : 0.026093, loss_ce: 0.007654
2022-01-06 13:59:09,892 iteration 3790 : loss : 0.030385, loss_ce: 0.011474
2022-01-06 13:59:11,006 iteration 3791 : loss : 0.027579, loss_ce: 0.011777
 56%|████████████████▏            | 223/400 [1:17:37<57:48, 19.59s/it]2022-01-06 13:59:12,123 iteration 3792 : loss : 0.020837, loss_ce: 0.007309
2022-01-06 13:59:13,257 iteration 3793 : loss : 0.028986, loss_ce: 0.010172
2022-01-06 13:59:14,366 iteration 3794 : loss : 0.027418, loss_ce: 0.010909
2022-01-06 13:59:15,455 iteration 3795 : loss : 0.020387, loss_ce: 0.006570
2022-01-06 13:59:16,457 iteration 3796 : loss : 0.023804, loss_ce: 0.007936
2022-01-06 13:59:17,524 iteration 3797 : loss : 0.026131, loss_ce: 0.009556
2022-01-06 13:59:18,543 iteration 3798 : loss : 0.028010, loss_ce: 0.011433
2022-01-06 13:59:19,606 iteration 3799 : loss : 0.034468, loss_ce: 0.010862
2022-01-06 13:59:20,676 iteration 3800 : loss : 0.028581, loss_ce: 0.009034
2022-01-06 13:59:21,746 iteration 3801 : loss : 0.042484, loss_ce: 0.009563
2022-01-06 13:59:22,793 iteration 3802 : loss : 0.044962, loss_ce: 0.026480
2022-01-06 13:59:23,871 iteration 3803 : loss : 0.021658, loss_ce: 0.007378
2022-01-06 13:59:24,945 iteration 3804 : loss : 0.033666, loss_ce: 0.012712
2022-01-06 13:59:25,995 iteration 3805 : loss : 0.032615, loss_ce: 0.013551
2022-01-06 13:59:27,048 iteration 3806 : loss : 0.026961, loss_ce: 0.013498
2022-01-06 13:59:28,162 iteration 3807 : loss : 0.036202, loss_ce: 0.012535
2022-01-06 13:59:29,255 iteration 3808 : loss : 0.026080, loss_ce: 0.010278
 56%|████████████████▏            | 224/400 [1:17:56<56:17, 19.19s/it]2022-01-06 13:59:30,330 iteration 3809 : loss : 0.022971, loss_ce: 0.008524
2022-01-06 13:59:31,429 iteration 3810 : loss : 0.025089, loss_ce: 0.009983
2022-01-06 13:59:32,501 iteration 3811 : loss : 0.026105, loss_ce: 0.007262
2022-01-06 13:59:33,595 iteration 3812 : loss : 0.052210, loss_ce: 0.027448
2022-01-06 13:59:34,667 iteration 3813 : loss : 0.030121, loss_ce: 0.014276
2022-01-06 13:59:35,778 iteration 3814 : loss : 0.028560, loss_ce: 0.009879
2022-01-06 13:59:36,867 iteration 3815 : loss : 0.028468, loss_ce: 0.010114
2022-01-06 13:59:37,972 iteration 3816 : loss : 0.024326, loss_ce: 0.009613
2022-01-06 13:59:39,012 iteration 3817 : loss : 0.025204, loss_ce: 0.008813
2022-01-06 13:59:40,130 iteration 3818 : loss : 0.022339, loss_ce: 0.009579
2022-01-06 13:59:41,259 iteration 3819 : loss : 0.021865, loss_ce: 0.009530
2022-01-06 13:59:42,368 iteration 3820 : loss : 0.028058, loss_ce: 0.010462
2022-01-06 13:59:43,482 iteration 3821 : loss : 0.035072, loss_ce: 0.013108
2022-01-06 13:59:44,538 iteration 3822 : loss : 0.036417, loss_ce: 0.011070
2022-01-06 13:59:45,704 iteration 3823 : loss : 0.040256, loss_ce: 0.009115
2022-01-06 13:59:46,800 iteration 3824 : loss : 0.037056, loss_ce: 0.012954
2022-01-06 13:59:46,801 Training Data Eval:
2022-01-06 13:59:52,090   Average segmentation loss on training set: 0.0250
2022-01-06 13:59:52,090 Validation Data Eval:
2022-01-06 13:59:53,909   Average segmentation loss on validation set: 0.1082
2022-01-06 13:59:55,066 iteration 3825 : loss : 0.033866, loss_ce: 0.013264
 56%|███████████████▏           | 225/400 [1:18:22<1:01:45, 21.17s/it]2022-01-06 13:59:56,233 iteration 3826 : loss : 0.029036, loss_ce: 0.014261
2022-01-06 13:59:57,280 iteration 3827 : loss : 0.031600, loss_ce: 0.010821
2022-01-06 13:59:58,380 iteration 3828 : loss : 0.026046, loss_ce: 0.009318
2022-01-06 13:59:59,474 iteration 3829 : loss : 0.025103, loss_ce: 0.008020
2022-01-06 14:00:00,529 iteration 3830 : loss : 0.022576, loss_ce: 0.007896
2022-01-06 14:00:01,571 iteration 3831 : loss : 0.028520, loss_ce: 0.010563
2022-01-06 14:00:02,611 iteration 3832 : loss : 0.029588, loss_ce: 0.007589
2022-01-06 14:00:03,691 iteration 3833 : loss : 0.021819, loss_ce: 0.009578
2022-01-06 14:00:04,808 iteration 3834 : loss : 0.027550, loss_ce: 0.012426
2022-01-06 14:00:05,904 iteration 3835 : loss : 0.028507, loss_ce: 0.010067
2022-01-06 14:00:06,995 iteration 3836 : loss : 0.027466, loss_ce: 0.010754
2022-01-06 14:00:08,107 iteration 3837 : loss : 0.026735, loss_ce: 0.008741
2022-01-06 14:00:09,285 iteration 3838 : loss : 0.034611, loss_ce: 0.011940
2022-01-06 14:00:10,371 iteration 3839 : loss : 0.023865, loss_ce: 0.010120
2022-01-06 14:00:11,470 iteration 3840 : loss : 0.036933, loss_ce: 0.009841
2022-01-06 14:00:12,525 iteration 3841 : loss : 0.026668, loss_ce: 0.009176
2022-01-06 14:00:13,638 iteration 3842 : loss : 0.041251, loss_ce: 0.020491
 56%|████████████████▍            | 226/400 [1:18:40<59:08, 20.39s/it]2022-01-06 14:00:14,745 iteration 3843 : loss : 0.028901, loss_ce: 0.011574
2022-01-06 14:00:15,873 iteration 3844 : loss : 0.036880, loss_ce: 0.010023
2022-01-06 14:00:16,985 iteration 3845 : loss : 0.023061, loss_ce: 0.010296
2022-01-06 14:00:18,033 iteration 3846 : loss : 0.019940, loss_ce: 0.007191
2022-01-06 14:00:19,098 iteration 3847 : loss : 0.031615, loss_ce: 0.011502
2022-01-06 14:00:20,229 iteration 3848 : loss : 0.022049, loss_ce: 0.011323
2022-01-06 14:00:21,420 iteration 3849 : loss : 0.026721, loss_ce: 0.011098
2022-01-06 14:00:22,593 iteration 3850 : loss : 0.064386, loss_ce: 0.014005
2022-01-06 14:00:23,694 iteration 3851 : loss : 0.023381, loss_ce: 0.009588
2022-01-06 14:00:24,807 iteration 3852 : loss : 0.029443, loss_ce: 0.007969
2022-01-06 14:00:25,861 iteration 3853 : loss : 0.024283, loss_ce: 0.007488
2022-01-06 14:00:26,976 iteration 3854 : loss : 0.029856, loss_ce: 0.009343
2022-01-06 14:00:28,198 iteration 3855 : loss : 0.033860, loss_ce: 0.012779
2022-01-06 14:00:29,176 iteration 3856 : loss : 0.023803, loss_ce: 0.010380
2022-01-06 14:00:30,248 iteration 3857 : loss : 0.043579, loss_ce: 0.021059
2022-01-06 14:00:31,246 iteration 3858 : loss : 0.029989, loss_ce: 0.012189
2022-01-06 14:00:32,319 iteration 3859 : loss : 0.026090, loss_ce: 0.011170
 57%|████████████████▍            | 227/400 [1:18:59<57:19, 19.88s/it]2022-01-06 14:00:33,464 iteration 3860 : loss : 0.026507, loss_ce: 0.009518
2022-01-06 14:00:34,628 iteration 3861 : loss : 0.040172, loss_ce: 0.018349
2022-01-06 14:00:35,743 iteration 3862 : loss : 0.029792, loss_ce: 0.011529
2022-01-06 14:00:36,845 iteration 3863 : loss : 0.047658, loss_ce: 0.019635
2022-01-06 14:00:37,978 iteration 3864 : loss : 0.030155, loss_ce: 0.011879
2022-01-06 14:00:39,066 iteration 3865 : loss : 0.034141, loss_ce: 0.011207
2022-01-06 14:00:40,258 iteration 3866 : loss : 0.041204, loss_ce: 0.013190
2022-01-06 14:00:41,337 iteration 3867 : loss : 0.034252, loss_ce: 0.011394
2022-01-06 14:00:42,480 iteration 3868 : loss : 0.052892, loss_ce: 0.014701
2022-01-06 14:00:43,603 iteration 3869 : loss : 0.030642, loss_ce: 0.014445
2022-01-06 14:00:44,742 iteration 3870 : loss : 0.025239, loss_ce: 0.009790
2022-01-06 14:00:45,859 iteration 3871 : loss : 0.031307, loss_ce: 0.008884
2022-01-06 14:00:46,918 iteration 3872 : loss : 0.036416, loss_ce: 0.011205
2022-01-06 14:00:47,971 iteration 3873 : loss : 0.027776, loss_ce: 0.013982
2022-01-06 14:00:49,082 iteration 3874 : loss : 0.028498, loss_ce: 0.011683
2022-01-06 14:00:50,188 iteration 3875 : loss : 0.038828, loss_ce: 0.015538
2022-01-06 14:00:51,304 iteration 3876 : loss : 0.036143, loss_ce: 0.010934
 57%|████████████████▌            | 228/400 [1:19:18<56:13, 19.61s/it]2022-01-06 14:00:52,425 iteration 3877 : loss : 0.028979, loss_ce: 0.010461
2022-01-06 14:00:53,543 iteration 3878 : loss : 0.027281, loss_ce: 0.010389
2022-01-06 14:00:54,605 iteration 3879 : loss : 0.029039, loss_ce: 0.010314
2022-01-06 14:00:55,732 iteration 3880 : loss : 0.028489, loss_ce: 0.011789
2022-01-06 14:00:56,792 iteration 3881 : loss : 0.027339, loss_ce: 0.012281
2022-01-06 14:00:57,970 iteration 3882 : loss : 0.057273, loss_ce: 0.013660
2022-01-06 14:00:59,112 iteration 3883 : loss : 0.043451, loss_ce: 0.013530
2022-01-06 14:01:00,230 iteration 3884 : loss : 0.029331, loss_ce: 0.012417
2022-01-06 14:01:01,375 iteration 3885 : loss : 0.056656, loss_ce: 0.012117
2022-01-06 14:01:02,524 iteration 3886 : loss : 0.037614, loss_ce: 0.013638
2022-01-06 14:01:03,576 iteration 3887 : loss : 0.027473, loss_ce: 0.009299
2022-01-06 14:01:04,703 iteration 3888 : loss : 0.029841, loss_ce: 0.015977
2022-01-06 14:01:05,804 iteration 3889 : loss : 0.028315, loss_ce: 0.011128
2022-01-06 14:01:07,003 iteration 3890 : loss : 0.040331, loss_ce: 0.011827
2022-01-06 14:01:08,077 iteration 3891 : loss : 0.042563, loss_ce: 0.015930
2022-01-06 14:01:09,187 iteration 3892 : loss : 0.037073, loss_ce: 0.013234
2022-01-06 14:01:10,308 iteration 3893 : loss : 0.045750, loss_ce: 0.016441
 57%|████████████████▌            | 229/400 [1:19:37<55:22, 19.43s/it]2022-01-06 14:01:11,442 iteration 3894 : loss : 0.029424, loss_ce: 0.008981
2022-01-06 14:01:12,491 iteration 3895 : loss : 0.036734, loss_ce: 0.014401
2022-01-06 14:01:13,587 iteration 3896 : loss : 0.042634, loss_ce: 0.017797
2022-01-06 14:01:14,722 iteration 3897 : loss : 0.034935, loss_ce: 0.014847
2022-01-06 14:01:15,845 iteration 3898 : loss : 0.042249, loss_ce: 0.014911
2022-01-06 14:01:16,879 iteration 3899 : loss : 0.036118, loss_ce: 0.013224
2022-01-06 14:01:17,941 iteration 3900 : loss : 0.037204, loss_ce: 0.011522
2022-01-06 14:01:18,981 iteration 3901 : loss : 0.038150, loss_ce: 0.015552
2022-01-06 14:01:20,032 iteration 3902 : loss : 0.031904, loss_ce: 0.011339
2022-01-06 14:01:21,053 iteration 3903 : loss : 0.029702, loss_ce: 0.009183
2022-01-06 14:01:22,119 iteration 3904 : loss : 0.040883, loss_ce: 0.012582
2022-01-06 14:01:23,188 iteration 3905 : loss : 0.027536, loss_ce: 0.008505
2022-01-06 14:01:24,194 iteration 3906 : loss : 0.024576, loss_ce: 0.009633
2022-01-06 14:01:25,185 iteration 3907 : loss : 0.022516, loss_ce: 0.008915
2022-01-06 14:01:26,196 iteration 3908 : loss : 0.027987, loss_ce: 0.010407
2022-01-06 14:01:27,273 iteration 3909 : loss : 0.047791, loss_ce: 0.027117
2022-01-06 14:01:27,273 Training Data Eval:
2022-01-06 14:01:32,564   Average segmentation loss on training set: 0.0225
2022-01-06 14:01:32,564 Validation Data Eval:
2022-01-06 14:01:34,382   Average segmentation loss on validation set: 0.1062
2022-01-06 14:01:35,528 iteration 3910 : loss : 0.024027, loss_ce: 0.007835
 57%|████████████████▋            | 230/400 [1:20:02<59:58, 21.17s/it]2022-01-06 14:01:36,633 iteration 3911 : loss : 0.026349, loss_ce: 0.010612
2022-01-06 14:01:37,757 iteration 3912 : loss : 0.030080, loss_ce: 0.010698
2022-01-06 14:01:38,840 iteration 3913 : loss : 0.024732, loss_ce: 0.010031
2022-01-06 14:01:39,909 iteration 3914 : loss : 0.024385, loss_ce: 0.011611
2022-01-06 14:01:40,984 iteration 3915 : loss : 0.027315, loss_ce: 0.011167
2022-01-06 14:01:42,027 iteration 3916 : loss : 0.021473, loss_ce: 0.007672
2022-01-06 14:01:43,137 iteration 3917 : loss : 0.028747, loss_ce: 0.011886
2022-01-06 14:01:44,253 iteration 3918 : loss : 0.040662, loss_ce: 0.012337
2022-01-06 14:01:45,465 iteration 3919 : loss : 0.042132, loss_ce: 0.011424
2022-01-06 14:01:46,515 iteration 3920 : loss : 0.025927, loss_ce: 0.009563
2022-01-06 14:01:47,589 iteration 3921 : loss : 0.024496, loss_ce: 0.009598
2022-01-06 14:01:48,709 iteration 3922 : loss : 0.026845, loss_ce: 0.009402
2022-01-06 14:01:49,777 iteration 3923 : loss : 0.021498, loss_ce: 0.009991
2022-01-06 14:01:50,819 iteration 3924 : loss : 0.033376, loss_ce: 0.008614
2022-01-06 14:01:51,981 iteration 3925 : loss : 0.038306, loss_ce: 0.013205
2022-01-06 14:01:53,121 iteration 3926 : loss : 0.023708, loss_ce: 0.007646
2022-01-06 14:01:54,204 iteration 3927 : loss : 0.034533, loss_ce: 0.013835
 58%|████████████████▋            | 231/400 [1:20:21<57:31, 20.42s/it]2022-01-06 14:01:55,393 iteration 3928 : loss : 0.050056, loss_ce: 0.025226
2022-01-06 14:01:56,484 iteration 3929 : loss : 0.029422, loss_ce: 0.013085
2022-01-06 14:01:57,631 iteration 3930 : loss : 0.026535, loss_ce: 0.010712
2022-01-06 14:01:58,759 iteration 3931 : loss : 0.021962, loss_ce: 0.008290
2022-01-06 14:01:59,885 iteration 3932 : loss : 0.034122, loss_ce: 0.011056
2022-01-06 14:02:01,052 iteration 3933 : loss : 0.027498, loss_ce: 0.010166
2022-01-06 14:02:02,175 iteration 3934 : loss : 0.025776, loss_ce: 0.009873
2022-01-06 14:02:03,196 iteration 3935 : loss : 0.029731, loss_ce: 0.012397
2022-01-06 14:02:04,406 iteration 3936 : loss : 0.050285, loss_ce: 0.018531
2022-01-06 14:02:05,458 iteration 3937 : loss : 0.040170, loss_ce: 0.017898
2022-01-06 14:02:06,596 iteration 3938 : loss : 0.118084, loss_ce: 0.021734
2022-01-06 14:02:07,612 iteration 3939 : loss : 0.027023, loss_ce: 0.007962
2022-01-06 14:02:08,672 iteration 3940 : loss : 0.027172, loss_ce: 0.009481
2022-01-06 14:02:09,754 iteration 3941 : loss : 0.028661, loss_ce: 0.012443
2022-01-06 14:02:10,890 iteration 3942 : loss : 0.076894, loss_ce: 0.034370
2022-01-06 14:02:11,943 iteration 3943 : loss : 0.036936, loss_ce: 0.006915
2022-01-06 14:02:13,085 iteration 3944 : loss : 0.037119, loss_ce: 0.014065
 58%|████████████████▊            | 232/400 [1:20:40<55:52, 19.96s/it]2022-01-06 14:02:14,248 iteration 3945 : loss : 0.040469, loss_ce: 0.016860
2022-01-06 14:02:15,246 iteration 3946 : loss : 0.030402, loss_ce: 0.016280
2022-01-06 14:02:16,315 iteration 3947 : loss : 0.022913, loss_ce: 0.006921
2022-01-06 14:02:17,527 iteration 3948 : loss : 0.041810, loss_ce: 0.013822
2022-01-06 14:02:18,663 iteration 3949 : loss : 0.050600, loss_ce: 0.016497
2022-01-06 14:02:19,827 iteration 3950 : loss : 0.039092, loss_ce: 0.011453
2022-01-06 14:02:20,864 iteration 3951 : loss : 0.025978, loss_ce: 0.011517
2022-01-06 14:02:22,003 iteration 3952 : loss : 0.024502, loss_ce: 0.008920
2022-01-06 14:02:23,149 iteration 3953 : loss : 0.044429, loss_ce: 0.018797
2022-01-06 14:02:24,230 iteration 3954 : loss : 0.031819, loss_ce: 0.010885
2022-01-06 14:02:25,304 iteration 3955 : loss : 0.023012, loss_ce: 0.008275
2022-01-06 14:02:26,507 iteration 3956 : loss : 0.034400, loss_ce: 0.014676
2022-01-06 14:02:27,573 iteration 3957 : loss : 0.026976, loss_ce: 0.010425
2022-01-06 14:02:28,644 iteration 3958 : loss : 0.028185, loss_ce: 0.010501
2022-01-06 14:02:29,855 iteration 3959 : loss : 0.037851, loss_ce: 0.010794
2022-01-06 14:02:30,924 iteration 3960 : loss : 0.033909, loss_ce: 0.018100
2022-01-06 14:02:32,008 iteration 3961 : loss : 0.025022, loss_ce: 0.009356
 58%|████████████████▉            | 233/400 [1:20:58<54:40, 19.65s/it]2022-01-06 14:02:33,191 iteration 3962 : loss : 0.026931, loss_ce: 0.010459
2022-01-06 14:02:34,310 iteration 3963 : loss : 0.026907, loss_ce: 0.011441
2022-01-06 14:02:35,507 iteration 3964 : loss : 0.035857, loss_ce: 0.016600
2022-01-06 14:02:36,569 iteration 3965 : loss : 0.030956, loss_ce: 0.008267
2022-01-06 14:02:37,884 iteration 3966 : loss : 0.026479, loss_ce: 0.009918
2022-01-06 14:02:38,992 iteration 3967 : loss : 0.025075, loss_ce: 0.009189
2022-01-06 14:02:39,996 iteration 3968 : loss : 0.041145, loss_ce: 0.010889
2022-01-06 14:02:41,103 iteration 3969 : loss : 0.032868, loss_ce: 0.008419
2022-01-06 14:02:42,182 iteration 3970 : loss : 0.033403, loss_ce: 0.010680
2022-01-06 14:02:43,176 iteration 3971 : loss : 0.024885, loss_ce: 0.009735
2022-01-06 14:02:44,295 iteration 3972 : loss : 0.034193, loss_ce: 0.008100
2022-01-06 14:02:45,339 iteration 3973 : loss : 0.038543, loss_ce: 0.019374
2022-01-06 14:02:46,343 iteration 3974 : loss : 0.022578, loss_ce: 0.007103
2022-01-06 14:02:47,446 iteration 3975 : loss : 0.025243, loss_ce: 0.009546
2022-01-06 14:02:48,605 iteration 3976 : loss : 0.036288, loss_ce: 0.013396
2022-01-06 14:02:49,763 iteration 3977 : loss : 0.031883, loss_ce: 0.012299
2022-01-06 14:02:50,947 iteration 3978 : loss : 0.039053, loss_ce: 0.012603
 58%|████████████████▉            | 234/400 [1:21:17<53:46, 19.44s/it]2022-01-06 14:02:52,106 iteration 3979 : loss : 0.034838, loss_ce: 0.015112
2022-01-06 14:02:53,251 iteration 3980 : loss : 0.038860, loss_ce: 0.013478
2022-01-06 14:02:54,330 iteration 3981 : loss : 0.025381, loss_ce: 0.009794
2022-01-06 14:02:55,318 iteration 3982 : loss : 0.021678, loss_ce: 0.007913
2022-01-06 14:02:56,327 iteration 3983 : loss : 0.024275, loss_ce: 0.011454
2022-01-06 14:02:57,380 iteration 3984 : loss : 0.028736, loss_ce: 0.011964
2022-01-06 14:02:58,486 iteration 3985 : loss : 0.053271, loss_ce: 0.035974
2022-01-06 14:02:59,710 iteration 3986 : loss : 0.039400, loss_ce: 0.018167
2022-01-06 14:03:00,830 iteration 3987 : loss : 0.034465, loss_ce: 0.013155
2022-01-06 14:03:01,919 iteration 3988 : loss : 0.027106, loss_ce: 0.011664
2022-01-06 14:03:02,990 iteration 3989 : loss : 0.025958, loss_ce: 0.005503
2022-01-06 14:03:04,055 iteration 3990 : loss : 0.068274, loss_ce: 0.020788
2022-01-06 14:03:05,203 iteration 3991 : loss : 0.029898, loss_ce: 0.007942
2022-01-06 14:03:06,318 iteration 3992 : loss : 0.028763, loss_ce: 0.010342
2022-01-06 14:03:07,474 iteration 3993 : loss : 0.028893, loss_ce: 0.010529
2022-01-06 14:03:08,554 iteration 3994 : loss : 0.026936, loss_ce: 0.013315
2022-01-06 14:03:08,554 Training Data Eval:
2022-01-06 14:03:13,906   Average segmentation loss on training set: 0.0282
2022-01-06 14:03:13,906 Validation Data Eval:
2022-01-06 14:03:16,098   Average segmentation loss on validation set: 0.1007
2022-01-06 14:03:17,134 iteration 3995 : loss : 0.026395, loss_ce: 0.010497
 59%|█████████████████            | 235/400 [1:21:44<59:01, 21.46s/it]2022-01-06 14:03:18,345 iteration 3996 : loss : 0.026571, loss_ce: 0.007966
2022-01-06 14:03:19,382 iteration 3997 : loss : 0.031871, loss_ce: 0.011361
2022-01-06 14:03:20,493 iteration 3998 : loss : 0.024505, loss_ce: 0.010494
2022-01-06 14:03:21,572 iteration 3999 : loss : 0.030396, loss_ce: 0.007872
2022-01-06 14:03:22,708 iteration 4000 : loss : 0.027196, loss_ce: 0.013141
2022-01-06 14:03:23,732 iteration 4001 : loss : 0.024283, loss_ce: 0.009455
2022-01-06 14:03:24,820 iteration 4002 : loss : 0.027068, loss_ce: 0.011064
2022-01-06 14:03:25,920 iteration 4003 : loss : 0.032315, loss_ce: 0.014914
2022-01-06 14:03:27,048 iteration 4004 : loss : 0.042079, loss_ce: 0.009863
2022-01-06 14:03:28,146 iteration 4005 : loss : 0.037738, loss_ce: 0.016152
2022-01-06 14:03:29,190 iteration 4006 : loss : 0.026438, loss_ce: 0.009691
2022-01-06 14:03:30,331 iteration 4007 : loss : 0.030307, loss_ce: 0.011077
2022-01-06 14:03:31,388 iteration 4008 : loss : 0.029987, loss_ce: 0.011728
2022-01-06 14:03:32,472 iteration 4009 : loss : 0.033465, loss_ce: 0.014945
2022-01-06 14:03:33,622 iteration 4010 : loss : 0.054341, loss_ce: 0.022869
2022-01-06 14:03:34,668 iteration 4011 : loss : 0.040712, loss_ce: 0.012088
2022-01-06 14:03:35,837 iteration 4012 : loss : 0.029565, loss_ce: 0.011977
 59%|█████████████████            | 236/400 [1:22:02<56:24, 20.63s/it]2022-01-06 14:03:37,070 iteration 4013 : loss : 0.041589, loss_ce: 0.012946
2022-01-06 14:03:38,112 iteration 4014 : loss : 0.027243, loss_ce: 0.009351
2022-01-06 14:03:39,251 iteration 4015 : loss : 0.039127, loss_ce: 0.017840
2022-01-06 14:03:40,379 iteration 4016 : loss : 0.023370, loss_ce: 0.008171
2022-01-06 14:03:41,529 iteration 4017 : loss : 0.040452, loss_ce: 0.012163
2022-01-06 14:03:42,584 iteration 4018 : loss : 0.028805, loss_ce: 0.009983
2022-01-06 14:03:43,670 iteration 4019 : loss : 0.035950, loss_ce: 0.017904
2022-01-06 14:03:44,717 iteration 4020 : loss : 0.025371, loss_ce: 0.013284
2022-01-06 14:03:45,782 iteration 4021 : loss : 0.026398, loss_ce: 0.011126
2022-01-06 14:03:46,899 iteration 4022 : loss : 0.027393, loss_ce: 0.013281
2022-01-06 14:03:47,991 iteration 4023 : loss : 0.020788, loss_ce: 0.007269
2022-01-06 14:03:49,018 iteration 4024 : loss : 0.025162, loss_ce: 0.008172
2022-01-06 14:03:50,070 iteration 4025 : loss : 0.022917, loss_ce: 0.007861
2022-01-06 14:03:51,196 iteration 4026 : loss : 0.026364, loss_ce: 0.012222
2022-01-06 14:03:52,310 iteration 4027 : loss : 0.033003, loss_ce: 0.011334
2022-01-06 14:03:53,458 iteration 4028 : loss : 0.040379, loss_ce: 0.012509
2022-01-06 14:03:54,510 iteration 4029 : loss : 0.023547, loss_ce: 0.008562
 59%|█████████████████▏           | 237/400 [1:22:21<54:27, 20.05s/it]2022-01-06 14:03:55,722 iteration 4030 : loss : 0.029841, loss_ce: 0.012079
2022-01-06 14:03:56,836 iteration 4031 : loss : 0.040429, loss_ce: 0.013411
2022-01-06 14:03:57,924 iteration 4032 : loss : 0.028519, loss_ce: 0.011558
2022-01-06 14:03:59,023 iteration 4033 : loss : 0.030724, loss_ce: 0.010772
2022-01-06 14:04:00,060 iteration 4034 : loss : 0.033769, loss_ce: 0.010757
2022-01-06 14:04:01,134 iteration 4035 : loss : 0.023238, loss_ce: 0.008210
2022-01-06 14:04:02,262 iteration 4036 : loss : 0.035999, loss_ce: 0.011480
2022-01-06 14:04:03,308 iteration 4037 : loss : 0.020875, loss_ce: 0.009890
2022-01-06 14:04:04,462 iteration 4038 : loss : 0.033105, loss_ce: 0.011080
2022-01-06 14:04:05,640 iteration 4039 : loss : 0.029358, loss_ce: 0.010378
2022-01-06 14:04:06,813 iteration 4040 : loss : 0.032361, loss_ce: 0.013506
2022-01-06 14:04:07,974 iteration 4041 : loss : 0.022188, loss_ce: 0.007254
2022-01-06 14:04:09,038 iteration 4042 : loss : 0.023163, loss_ce: 0.008427
2022-01-06 14:04:10,147 iteration 4043 : loss : 0.183927, loss_ce: 0.011600
2022-01-06 14:04:11,205 iteration 4044 : loss : 0.026476, loss_ce: 0.011019
2022-01-06 14:04:12,297 iteration 4045 : loss : 0.023196, loss_ce: 0.011129
2022-01-06 14:04:13,439 iteration 4046 : loss : 0.030663, loss_ce: 0.014878
 60%|█████████████████▎           | 238/400 [1:22:40<53:12, 19.71s/it]2022-01-06 14:04:14,677 iteration 4047 : loss : 0.023626, loss_ce: 0.011759
2022-01-06 14:04:15,818 iteration 4048 : loss : 0.033811, loss_ce: 0.013622
2022-01-06 14:04:16,879 iteration 4049 : loss : 0.022880, loss_ce: 0.008888
2022-01-06 14:04:18,004 iteration 4050 : loss : 0.025377, loss_ce: 0.010001
2022-01-06 14:04:19,213 iteration 4051 : loss : 0.056887, loss_ce: 0.010040
2022-01-06 14:04:20,253 iteration 4052 : loss : 0.026620, loss_ce: 0.009543
2022-01-06 14:04:21,264 iteration 4053 : loss : 0.024475, loss_ce: 0.008597
2022-01-06 14:04:22,309 iteration 4054 : loss : 0.040962, loss_ce: 0.012056
2022-01-06 14:04:23,387 iteration 4055 : loss : 0.024869, loss_ce: 0.010392
2022-01-06 14:04:24,489 iteration 4056 : loss : 0.038534, loss_ce: 0.021678
2022-01-06 14:04:25,530 iteration 4057 : loss : 0.024973, loss_ce: 0.006845
2022-01-06 14:04:26,612 iteration 4058 : loss : 0.033974, loss_ce: 0.012608
2022-01-06 14:04:27,804 iteration 4059 : loss : 0.044059, loss_ce: 0.020274
2022-01-06 14:04:28,919 iteration 4060 : loss : 0.028941, loss_ce: 0.012124
2022-01-06 14:04:30,013 iteration 4061 : loss : 0.029731, loss_ce: 0.013382
2022-01-06 14:04:31,136 iteration 4062 : loss : 0.029887, loss_ce: 0.012520
2022-01-06 14:04:32,246 iteration 4063 : loss : 0.057344, loss_ce: 0.020282
 60%|█████████████████▎           | 239/400 [1:22:59<52:09, 19.44s/it]2022-01-06 14:04:33,381 iteration 4064 : loss : 0.025024, loss_ce: 0.005682
2022-01-06 14:04:34,442 iteration 4065 : loss : 0.019645, loss_ce: 0.006775
2022-01-06 14:04:35,604 iteration 4066 : loss : 0.038806, loss_ce: 0.015052
2022-01-06 14:04:36,665 iteration 4067 : loss : 0.026290, loss_ce: 0.011190
2022-01-06 14:04:37,757 iteration 4068 : loss : 0.041212, loss_ce: 0.014825
2022-01-06 14:04:38,878 iteration 4069 : loss : 0.027542, loss_ce: 0.011471
2022-01-06 14:04:39,994 iteration 4070 : loss : 0.034120, loss_ce: 0.013417
2022-01-06 14:04:41,156 iteration 4071 : loss : 0.037262, loss_ce: 0.014703
2022-01-06 14:04:42,246 iteration 4072 : loss : 0.023778, loss_ce: 0.007740
2022-01-06 14:04:43,311 iteration 4073 : loss : 0.023701, loss_ce: 0.008608
2022-01-06 14:04:44,405 iteration 4074 : loss : 0.024245, loss_ce: 0.010497
2022-01-06 14:04:45,538 iteration 4075 : loss : 0.025328, loss_ce: 0.009145
2022-01-06 14:04:46,586 iteration 4076 : loss : 0.039212, loss_ce: 0.017067
2022-01-06 14:04:47,665 iteration 4077 : loss : 0.022938, loss_ce: 0.008032
2022-01-06 14:04:48,813 iteration 4078 : loss : 0.036398, loss_ce: 0.014001
2022-01-06 14:04:49,937 iteration 4079 : loss : 0.026484, loss_ce: 0.011490
2022-01-06 14:04:49,938 Training Data Eval:
2022-01-06 14:04:55,291   Average segmentation loss on training set: 0.0374
2022-01-06 14:04:55,292 Validation Data Eval:
2022-01-06 14:04:57,117   Average segmentation loss on validation set: 0.0925
2022-01-06 14:04:58,258 iteration 4080 : loss : 0.024058, loss_ce: 0.009793
 60%|█████████████████▍           | 240/400 [1:23:25<57:06, 21.41s/it]2022-01-06 14:04:59,434 iteration 4081 : loss : 0.041990, loss_ce: 0.017043
2022-01-06 14:05:00,535 iteration 4082 : loss : 0.024267, loss_ce: 0.009960
2022-01-06 14:05:01,583 iteration 4083 : loss : 0.021201, loss_ce: 0.008936
2022-01-06 14:05:02,675 iteration 4084 : loss : 0.022731, loss_ce: 0.006936
2022-01-06 14:05:03,725 iteration 4085 : loss : 0.034246, loss_ce: 0.009624
2022-01-06 14:05:04,937 iteration 4086 : loss : 0.036711, loss_ce: 0.012658
2022-01-06 14:05:06,050 iteration 4087 : loss : 0.058535, loss_ce: 0.014662
2022-01-06 14:05:07,082 iteration 4088 : loss : 0.025856, loss_ce: 0.013803
2022-01-06 14:05:08,171 iteration 4089 : loss : 0.038274, loss_ce: 0.014427
2022-01-06 14:05:09,187 iteration 4090 : loss : 0.026661, loss_ce: 0.007957
2022-01-06 14:05:10,281 iteration 4091 : loss : 0.038869, loss_ce: 0.015687
2022-01-06 14:05:11,381 iteration 4092 : loss : 0.026824, loss_ce: 0.008162
2022-01-06 14:05:12,492 iteration 4093 : loss : 0.034802, loss_ce: 0.014117
2022-01-06 14:05:13,562 iteration 4094 : loss : 0.032591, loss_ce: 0.012875
2022-01-06 14:05:14,683 iteration 4095 : loss : 0.034472, loss_ce: 0.015418
2022-01-06 14:05:15,747 iteration 4096 : loss : 0.026350, loss_ce: 0.013612
2022-01-06 14:05:16,822 iteration 4097 : loss : 0.027081, loss_ce: 0.009838
 60%|█████████████████▍           | 241/400 [1:23:43<54:28, 20.56s/it]2022-01-06 14:05:17,955 iteration 4098 : loss : 0.036301, loss_ce: 0.014736
2022-01-06 14:05:19,014 iteration 4099 : loss : 0.031038, loss_ce: 0.007871
2022-01-06 14:05:20,116 iteration 4100 : loss : 0.028747, loss_ce: 0.010630
2022-01-06 14:05:21,251 iteration 4101 : loss : 0.032541, loss_ce: 0.010557
2022-01-06 14:05:22,388 iteration 4102 : loss : 0.041590, loss_ce: 0.015627
2022-01-06 14:05:23,506 iteration 4103 : loss : 0.056030, loss_ce: 0.021967
2022-01-06 14:05:24,605 iteration 4104 : loss : 0.031316, loss_ce: 0.015459
2022-01-06 14:05:25,722 iteration 4105 : loss : 0.025077, loss_ce: 0.010165
2022-01-06 14:05:26,804 iteration 4106 : loss : 0.029424, loss_ce: 0.011239
2022-01-06 14:05:27,882 iteration 4107 : loss : 0.022354, loss_ce: 0.008466
2022-01-06 14:05:28,979 iteration 4108 : loss : 0.024761, loss_ce: 0.009932
2022-01-06 14:05:30,017 iteration 4109 : loss : 0.029412, loss_ce: 0.010182
2022-01-06 14:05:31,149 iteration 4110 : loss : 0.039629, loss_ce: 0.012785
2022-01-06 14:05:32,398 iteration 4111 : loss : 0.054194, loss_ce: 0.030314
2022-01-06 14:05:33,471 iteration 4112 : loss : 0.029614, loss_ce: 0.010036
2022-01-06 14:05:34,613 iteration 4113 : loss : 0.035726, loss_ce: 0.009272
2022-01-06 14:05:35,713 iteration 4114 : loss : 0.025305, loss_ce: 0.010884
 60%|█████████████████▌           | 242/400 [1:24:02<52:48, 20.06s/it]2022-01-06 14:05:36,826 iteration 4115 : loss : 0.026408, loss_ce: 0.011386
2022-01-06 14:05:37,971 iteration 4116 : loss : 0.033491, loss_ce: 0.012948
2022-01-06 14:05:39,014 iteration 4117 : loss : 0.023260, loss_ce: 0.009486
2022-01-06 14:05:40,167 iteration 4118 : loss : 0.036867, loss_ce: 0.013447
2022-01-06 14:05:41,373 iteration 4119 : loss : 0.060484, loss_ce: 0.017418
2022-01-06 14:05:42,492 iteration 4120 : loss : 0.041340, loss_ce: 0.009468
2022-01-06 14:05:43,552 iteration 4121 : loss : 0.022999, loss_ce: 0.008288
2022-01-06 14:05:44,641 iteration 4122 : loss : 0.051940, loss_ce: 0.017671
2022-01-06 14:05:45,684 iteration 4123 : loss : 0.017072, loss_ce: 0.006906
2022-01-06 14:05:46,814 iteration 4124 : loss : 0.030076, loss_ce: 0.014260
2022-01-06 14:05:47,941 iteration 4125 : loss : 0.039408, loss_ce: 0.017158
2022-01-06 14:05:49,053 iteration 4126 : loss : 0.031692, loss_ce: 0.014107
2022-01-06 14:05:50,112 iteration 4127 : loss : 0.021033, loss_ce: 0.011009
2022-01-06 14:05:51,314 iteration 4128 : loss : 0.045872, loss_ce: 0.013765
2022-01-06 14:05:52,360 iteration 4129 : loss : 0.040982, loss_ce: 0.010377
2022-01-06 14:05:53,387 iteration 4130 : loss : 0.024074, loss_ce: 0.008036
2022-01-06 14:05:54,463 iteration 4131 : loss : 0.026156, loss_ce: 0.011753
 61%|█████████████████▌           | 243/400 [1:24:21<51:27, 19.66s/it]2022-01-06 14:05:55,667 iteration 4132 : loss : 0.041243, loss_ce: 0.012492
2022-01-06 14:05:56,785 iteration 4133 : loss : 0.038250, loss_ce: 0.014645
2022-01-06 14:05:57,920 iteration 4134 : loss : 0.033785, loss_ce: 0.014163
2022-01-06 14:05:58,947 iteration 4135 : loss : 0.025320, loss_ce: 0.011827
2022-01-06 14:06:00,031 iteration 4136 : loss : 0.034989, loss_ce: 0.017713
2022-01-06 14:06:01,053 iteration 4137 : loss : 0.020458, loss_ce: 0.007434
2022-01-06 14:06:02,166 iteration 4138 : loss : 0.030783, loss_ce: 0.014976
2022-01-06 14:06:03,291 iteration 4139 : loss : 0.027791, loss_ce: 0.011680
2022-01-06 14:06:04,408 iteration 4140 : loss : 0.030899, loss_ce: 0.012911
2022-01-06 14:06:05,521 iteration 4141 : loss : 0.031172, loss_ce: 0.010953
2022-01-06 14:06:06,692 iteration 4142 : loss : 0.038218, loss_ce: 0.013560
2022-01-06 14:06:07,706 iteration 4143 : loss : 0.031242, loss_ce: 0.010298
2022-01-06 14:06:08,732 iteration 4144 : loss : 0.024680, loss_ce: 0.006172
2022-01-06 14:06:09,874 iteration 4145 : loss : 0.032109, loss_ce: 0.011789
2022-01-06 14:06:10,946 iteration 4146 : loss : 0.028025, loss_ce: 0.012869
2022-01-06 14:06:12,074 iteration 4147 : loss : 0.023206, loss_ce: 0.009279
2022-01-06 14:06:13,163 iteration 4148 : loss : 0.031937, loss_ce: 0.012517
 61%|█████████████████▋           | 244/400 [1:24:40<50:22, 19.38s/it]2022-01-06 14:06:14,256 iteration 4149 : loss : 0.026063, loss_ce: 0.009554
2022-01-06 14:06:15,378 iteration 4150 : loss : 0.024575, loss_ce: 0.007819
2022-01-06 14:06:16,528 iteration 4151 : loss : 0.032303, loss_ce: 0.011730
2022-01-06 14:06:17,636 iteration 4152 : loss : 0.022706, loss_ce: 0.008512
2022-01-06 14:06:18,669 iteration 4153 : loss : 0.019453, loss_ce: 0.007373
2022-01-06 14:06:19,777 iteration 4154 : loss : 0.021579, loss_ce: 0.008550
2022-01-06 14:06:20,867 iteration 4155 : loss : 0.022481, loss_ce: 0.007257
2022-01-06 14:06:21,907 iteration 4156 : loss : 0.018824, loss_ce: 0.006770
2022-01-06 14:06:23,062 iteration 4157 : loss : 0.030960, loss_ce: 0.016049
2022-01-06 14:06:24,123 iteration 4158 : loss : 0.023625, loss_ce: 0.008872
2022-01-06 14:06:25,197 iteration 4159 : loss : 0.020196, loss_ce: 0.008660
2022-01-06 14:06:26,399 iteration 4160 : loss : 0.032209, loss_ce: 0.010904
2022-01-06 14:06:27,580 iteration 4161 : loss : 0.027343, loss_ce: 0.008300
2022-01-06 14:06:28,750 iteration 4162 : loss : 0.045181, loss_ce: 0.014498
2022-01-06 14:06:29,898 iteration 4163 : loss : 0.048725, loss_ce: 0.028383
2022-01-06 14:06:31,009 iteration 4164 : loss : 0.024332, loss_ce: 0.009719
2022-01-06 14:06:31,009 Training Data Eval:
2022-01-06 14:06:36,464   Average segmentation loss on training set: 0.0253
2022-01-06 14:06:36,464 Validation Data Eval:
2022-01-06 14:06:38,355   Average segmentation loss on validation set: 0.1296
2022-01-06 14:06:39,498 iteration 4165 : loss : 0.040419, loss_ce: 0.012270
 61%|█████████████████▊           | 245/400 [1:25:06<55:26, 21.46s/it]2022-01-06 14:06:40,648 iteration 4166 : loss : 0.022635, loss_ce: 0.008759
2022-01-06 14:06:41,677 iteration 4167 : loss : 0.025666, loss_ce: 0.008204
2022-01-06 14:06:42,733 iteration 4168 : loss : 0.021108, loss_ce: 0.008827
2022-01-06 14:06:43,794 iteration 4169 : loss : 0.019249, loss_ce: 0.009182
2022-01-06 14:06:44,998 iteration 4170 : loss : 0.025489, loss_ce: 0.010699
2022-01-06 14:06:46,025 iteration 4171 : loss : 0.025093, loss_ce: 0.007616
2022-01-06 14:06:47,114 iteration 4172 : loss : 0.022051, loss_ce: 0.009814
2022-01-06 14:06:48,250 iteration 4173 : loss : 0.030789, loss_ce: 0.009529
2022-01-06 14:06:49,325 iteration 4174 : loss : 0.026516, loss_ce: 0.008097
2022-01-06 14:06:50,371 iteration 4175 : loss : 0.020512, loss_ce: 0.008476
2022-01-06 14:06:51,449 iteration 4176 : loss : 0.025537, loss_ce: 0.011888
2022-01-06 14:06:52,588 iteration 4177 : loss : 0.027153, loss_ce: 0.007921
2022-01-06 14:06:53,640 iteration 4178 : loss : 0.036497, loss_ce: 0.013203
2022-01-06 14:06:54,712 iteration 4179 : loss : 0.025210, loss_ce: 0.010712
2022-01-06 14:06:55,781 iteration 4180 : loss : 0.023922, loss_ce: 0.008136
2022-01-06 14:06:56,865 iteration 4181 : loss : 0.027205, loss_ce: 0.010759
2022-01-06 14:06:57,964 iteration 4182 : loss : 0.024882, loss_ce: 0.006156
 62%|█████████████████▊           | 246/400 [1:25:24<52:47, 20.57s/it]2022-01-06 14:06:59,149 iteration 4183 : loss : 0.036387, loss_ce: 0.014744
2022-01-06 14:07:00,295 iteration 4184 : loss : 0.027125, loss_ce: 0.011625
2022-01-06 14:07:01,410 iteration 4185 : loss : 0.026032, loss_ce: 0.010920
2022-01-06 14:07:02,502 iteration 4186 : loss : 0.022517, loss_ce: 0.006150
2022-01-06 14:07:03,648 iteration 4187 : loss : 0.041137, loss_ce: 0.015044
2022-01-06 14:07:04,712 iteration 4188 : loss : 0.025886, loss_ce: 0.008116
2022-01-06 14:07:05,919 iteration 4189 : loss : 0.026728, loss_ce: 0.011167
2022-01-06 14:07:07,072 iteration 4190 : loss : 0.030320, loss_ce: 0.012944
2022-01-06 14:07:08,241 iteration 4191 : loss : 0.030840, loss_ce: 0.013750
2022-01-06 14:07:09,252 iteration 4192 : loss : 0.016923, loss_ce: 0.005305
2022-01-06 14:07:10,369 iteration 4193 : loss : 0.033599, loss_ce: 0.017592
2022-01-06 14:07:11,514 iteration 4194 : loss : 0.024641, loss_ce: 0.008596
2022-01-06 14:07:12,602 iteration 4195 : loss : 0.039364, loss_ce: 0.014150
2022-01-06 14:07:13,731 iteration 4196 : loss : 0.025132, loss_ce: 0.012260
2022-01-06 14:07:14,815 iteration 4197 : loss : 0.021595, loss_ce: 0.006523
2022-01-06 14:07:15,900 iteration 4198 : loss : 0.024167, loss_ce: 0.009355
2022-01-06 14:07:16,932 iteration 4199 : loss : 0.021304, loss_ce: 0.008861
 62%|█████████████████▉           | 247/400 [1:25:43<51:13, 20.09s/it]2022-01-06 14:07:18,082 iteration 4200 : loss : 0.028209, loss_ce: 0.010584
2022-01-06 14:07:19,212 iteration 4201 : loss : 0.025114, loss_ce: 0.010877
2022-01-06 14:07:20,339 iteration 4202 : loss : 0.020279, loss_ce: 0.008672
2022-01-06 14:07:21,542 iteration 4203 : loss : 0.029997, loss_ce: 0.014601
2022-01-06 14:07:22,632 iteration 4204 : loss : 0.022141, loss_ce: 0.008453
2022-01-06 14:07:23,732 iteration 4205 : loss : 0.021864, loss_ce: 0.008733
2022-01-06 14:07:24,960 iteration 4206 : loss : 0.034004, loss_ce: 0.012085
2022-01-06 14:07:26,102 iteration 4207 : loss : 0.030745, loss_ce: 0.011241
2022-01-06 14:07:27,128 iteration 4208 : loss : 0.019087, loss_ce: 0.007239
2022-01-06 14:07:28,216 iteration 4209 : loss : 0.021381, loss_ce: 0.009453
2022-01-06 14:07:29,303 iteration 4210 : loss : 0.025178, loss_ce: 0.011296
2022-01-06 14:07:30,446 iteration 4211 : loss : 0.025837, loss_ce: 0.009904
2022-01-06 14:07:31,606 iteration 4212 : loss : 0.028655, loss_ce: 0.010751
2022-01-06 14:07:32,691 iteration 4213 : loss : 0.029463, loss_ce: 0.007472
2022-01-06 14:07:33,843 iteration 4214 : loss : 0.034947, loss_ce: 0.009216
2022-01-06 14:07:34,944 iteration 4215 : loss : 0.025376, loss_ce: 0.007335
2022-01-06 14:07:36,037 iteration 4216 : loss : 0.027743, loss_ce: 0.008600
 62%|█████████████████▉           | 248/400 [1:26:02<50:08, 19.79s/it]2022-01-06 14:07:37,239 iteration 4217 : loss : 0.025421, loss_ce: 0.008243
2022-01-06 14:07:38,324 iteration 4218 : loss : 0.028268, loss_ce: 0.009760
2022-01-06 14:07:39,392 iteration 4219 : loss : 0.024075, loss_ce: 0.008912
2022-01-06 14:07:40,409 iteration 4220 : loss : 0.034518, loss_ce: 0.008097
2022-01-06 14:07:41,598 iteration 4221 : loss : 0.045898, loss_ce: 0.019821
2022-01-06 14:07:42,780 iteration 4222 : loss : 0.029298, loss_ce: 0.008804
2022-01-06 14:07:43,857 iteration 4223 : loss : 0.024179, loss_ce: 0.008247
2022-01-06 14:07:45,012 iteration 4224 : loss : 0.037449, loss_ce: 0.008672
2022-01-06 14:07:46,130 iteration 4225 : loss : 0.030368, loss_ce: 0.008205
2022-01-06 14:07:47,276 iteration 4226 : loss : 0.023054, loss_ce: 0.009984
2022-01-06 14:07:48,393 iteration 4227 : loss : 0.025027, loss_ce: 0.010095
2022-01-06 14:07:49,446 iteration 4228 : loss : 0.024727, loss_ce: 0.009802
2022-01-06 14:07:50,530 iteration 4229 : loss : 0.025461, loss_ce: 0.009295
2022-01-06 14:07:51,613 iteration 4230 : loss : 0.034641, loss_ce: 0.015500
2022-01-06 14:07:52,757 iteration 4231 : loss : 0.035790, loss_ce: 0.016745
2022-01-06 14:07:53,894 iteration 4232 : loss : 0.027892, loss_ce: 0.009339
2022-01-06 14:07:54,990 iteration 4233 : loss : 0.022788, loss_ce: 0.012204
 62%|██████████████████           | 249/400 [1:26:21<49:10, 19.54s/it]2022-01-06 14:07:56,073 iteration 4234 : loss : 0.030644, loss_ce: 0.006580
2022-01-06 14:07:57,116 iteration 4235 : loss : 0.016013, loss_ce: 0.005884
2022-01-06 14:07:58,249 iteration 4236 : loss : 0.029988, loss_ce: 0.010133
2022-01-06 14:07:59,325 iteration 4237 : loss : 0.025732, loss_ce: 0.011099
2022-01-06 14:08:00,432 iteration 4238 : loss : 0.028402, loss_ce: 0.013242
2022-01-06 14:08:01,667 iteration 4239 : loss : 0.029450, loss_ce: 0.014533
2022-01-06 14:08:02,786 iteration 4240 : loss : 0.022824, loss_ce: 0.006513
2022-01-06 14:08:03,838 iteration 4241 : loss : 0.027875, loss_ce: 0.011937
2022-01-06 14:08:04,936 iteration 4242 : loss : 0.033081, loss_ce: 0.009802
2022-01-06 14:08:06,011 iteration 4243 : loss : 0.023895, loss_ce: 0.008439
2022-01-06 14:08:07,034 iteration 4244 : loss : 0.020516, loss_ce: 0.009032
2022-01-06 14:08:08,176 iteration 4245 : loss : 0.039295, loss_ce: 0.025090
2022-01-06 14:08:09,343 iteration 4246 : loss : 0.028031, loss_ce: 0.006722
2022-01-06 14:08:10,476 iteration 4247 : loss : 0.032381, loss_ce: 0.014129
2022-01-06 14:08:11,524 iteration 4248 : loss : 0.020873, loss_ce: 0.008249
2022-01-06 14:08:12,683 iteration 4249 : loss : 0.032560, loss_ce: 0.011342
2022-01-06 14:08:12,683 Training Data Eval:
2022-01-06 14:08:18,169   Average segmentation loss on training set: 0.0183
2022-01-06 14:08:18,169 Validation Data Eval:
2022-01-06 14:08:20,071   Average segmentation loss on validation set: 0.0754
2022-01-06 14:08:21,186 iteration 4250 : loss : 0.032010, loss_ce: 0.010316
 62%|██████████████████▏          | 250/400 [1:26:48<53:50, 21.53s/it]2022-01-06 14:08:22,425 iteration 4251 : loss : 0.024578, loss_ce: 0.007428
2022-01-06 14:08:23,586 iteration 4252 : loss : 0.025221, loss_ce: 0.011335
2022-01-06 14:08:24,720 iteration 4253 : loss : 0.027697, loss_ce: 0.008831
2022-01-06 14:08:25,890 iteration 4254 : loss : 0.025016, loss_ce: 0.009091
2022-01-06 14:08:27,062 iteration 4255 : loss : 0.023846, loss_ce: 0.009059
2022-01-06 14:08:28,097 iteration 4256 : loss : 0.019328, loss_ce: 0.009174
2022-01-06 14:08:29,214 iteration 4257 : loss : 0.028346, loss_ce: 0.011535
2022-01-06 14:08:30,264 iteration 4258 : loss : 0.019677, loss_ce: 0.006541
2022-01-06 14:08:31,393 iteration 4259 : loss : 0.025242, loss_ce: 0.010460
2022-01-06 14:08:32,520 iteration 4260 : loss : 0.031488, loss_ce: 0.010745
2022-01-06 14:08:33,648 iteration 4261 : loss : 0.022940, loss_ce: 0.008182
2022-01-06 14:08:34,709 iteration 4262 : loss : 0.023988, loss_ce: 0.010321
2022-01-06 14:08:35,831 iteration 4263 : loss : 0.027173, loss_ce: 0.012183
2022-01-06 14:08:36,986 iteration 4264 : loss : 0.028763, loss_ce: 0.012774
2022-01-06 14:08:38,084 iteration 4265 : loss : 0.032196, loss_ce: 0.007595
2022-01-06 14:08:39,212 iteration 4266 : loss : 0.025103, loss_ce: 0.007503
2022-01-06 14:08:40,358 iteration 4267 : loss : 0.022477, loss_ce: 0.008224
 63%|██████████████████▏          | 251/400 [1:27:07<51:43, 20.83s/it]2022-01-06 14:08:41,524 iteration 4268 : loss : 0.022398, loss_ce: 0.005031
2022-01-06 14:08:42,612 iteration 4269 : loss : 0.033685, loss_ce: 0.011892
2022-01-06 14:08:43,677 iteration 4270 : loss : 0.021219, loss_ce: 0.010491
2022-01-06 14:08:44,705 iteration 4271 : loss : 0.023555, loss_ce: 0.009000
2022-01-06 14:08:45,789 iteration 4272 : loss : 0.024294, loss_ce: 0.011781
2022-01-06 14:08:46,823 iteration 4273 : loss : 0.018433, loss_ce: 0.007642
2022-01-06 14:08:48,002 iteration 4274 : loss : 0.024515, loss_ce: 0.010973
2022-01-06 14:08:49,074 iteration 4275 : loss : 0.032561, loss_ce: 0.011713
2022-01-06 14:08:50,215 iteration 4276 : loss : 0.021340, loss_ce: 0.006349
2022-01-06 14:08:51,357 iteration 4277 : loss : 0.033128, loss_ce: 0.009049
2022-01-06 14:08:52,500 iteration 4278 : loss : 0.043602, loss_ce: 0.019383
2022-01-06 14:08:53,579 iteration 4279 : loss : 0.028609, loss_ce: 0.014649
2022-01-06 14:08:54,686 iteration 4280 : loss : 0.032925, loss_ce: 0.011726
2022-01-06 14:08:55,791 iteration 4281 : loss : 0.024536, loss_ce: 0.007276
2022-01-06 14:08:56,831 iteration 4282 : loss : 0.026650, loss_ce: 0.012382
2022-01-06 14:08:57,912 iteration 4283 : loss : 0.027786, loss_ce: 0.007775
2022-01-06 14:08:58,970 iteration 4284 : loss : 0.021873, loss_ce: 0.008509
 63%|██████████████████▎          | 252/400 [1:27:25<49:43, 20.16s/it]2022-01-06 14:09:00,063 iteration 4285 : loss : 0.022888, loss_ce: 0.006945
2022-01-06 14:09:01,210 iteration 4286 : loss : 0.026426, loss_ce: 0.010391
2022-01-06 14:09:02,269 iteration 4287 : loss : 0.025836, loss_ce: 0.012171
2022-01-06 14:09:03,327 iteration 4288 : loss : 0.028848, loss_ce: 0.013783
2022-01-06 14:09:04,429 iteration 4289 : loss : 0.021526, loss_ce: 0.008994
2022-01-06 14:09:05,515 iteration 4290 : loss : 0.057987, loss_ce: 0.014552
2022-01-06 14:09:06,636 iteration 4291 : loss : 0.023335, loss_ce: 0.009070
2022-01-06 14:09:07,757 iteration 4292 : loss : 0.021819, loss_ce: 0.009358
2022-01-06 14:09:08,900 iteration 4293 : loss : 0.033988, loss_ce: 0.010546
2022-01-06 14:09:09,974 iteration 4294 : loss : 0.030842, loss_ce: 0.007929
2022-01-06 14:09:11,173 iteration 4295 : loss : 0.037833, loss_ce: 0.013335
2022-01-06 14:09:12,278 iteration 4296 : loss : 0.028657, loss_ce: 0.010483
2022-01-06 14:09:13,400 iteration 4297 : loss : 0.025767, loss_ce: 0.009568
2022-01-06 14:09:14,469 iteration 4298 : loss : 0.026327, loss_ce: 0.013663
2022-01-06 14:09:15,611 iteration 4299 : loss : 0.026993, loss_ce: 0.011815
2022-01-06 14:09:16,780 iteration 4300 : loss : 0.030506, loss_ce: 0.012156
2022-01-06 14:09:17,937 iteration 4301 : loss : 0.031459, loss_ce: 0.016223
 63%|██████████████████▎          | 253/400 [1:27:44<48:31, 19.81s/it]2022-01-06 14:09:19,135 iteration 4302 : loss : 0.033969, loss_ce: 0.016599
2022-01-06 14:09:20,295 iteration 4303 : loss : 0.059621, loss_ce: 0.009821
2022-01-06 14:09:21,365 iteration 4304 : loss : 0.019421, loss_ce: 0.007427
2022-01-06 14:09:22,444 iteration 4305 : loss : 0.024820, loss_ce: 0.011627
2022-01-06 14:09:23,528 iteration 4306 : loss : 0.020433, loss_ce: 0.006335
2022-01-06 14:09:24,576 iteration 4307 : loss : 0.023279, loss_ce: 0.011694
2022-01-06 14:09:25,706 iteration 4308 : loss : 0.024920, loss_ce: 0.008115
2022-01-06 14:09:26,867 iteration 4309 : loss : 0.034413, loss_ce: 0.011390
2022-01-06 14:09:27,972 iteration 4310 : loss : 0.021955, loss_ce: 0.007914
2022-01-06 14:09:29,029 iteration 4311 : loss : 0.027118, loss_ce: 0.011225
2022-01-06 14:09:30,075 iteration 4312 : loss : 0.023364, loss_ce: 0.008931
2022-01-06 14:09:31,132 iteration 4313 : loss : 0.023703, loss_ce: 0.012176
2022-01-06 14:09:32,199 iteration 4314 : loss : 0.025472, loss_ce: 0.008368
2022-01-06 14:09:33,330 iteration 4315 : loss : 0.023120, loss_ce: 0.007608
2022-01-06 14:09:34,398 iteration 4316 : loss : 0.030294, loss_ce: 0.010050
2022-01-06 14:09:35,559 iteration 4317 : loss : 0.022519, loss_ce: 0.009119
2022-01-06 14:09:36,610 iteration 4318 : loss : 0.023294, loss_ce: 0.008696
 64%|██████████████████▍          | 254/400 [1:28:03<47:21, 19.46s/it]2022-01-06 14:09:37,842 iteration 4319 : loss : 0.022221, loss_ce: 0.007594
2022-01-06 14:09:38,982 iteration 4320 : loss : 0.034065, loss_ce: 0.012546
2022-01-06 14:09:39,983 iteration 4321 : loss : 0.023995, loss_ce: 0.006813
2022-01-06 14:09:41,073 iteration 4322 : loss : 0.023313, loss_ce: 0.008468
2022-01-06 14:09:42,114 iteration 4323 : loss : 0.022384, loss_ce: 0.011660
2022-01-06 14:09:43,313 iteration 4324 : loss : 0.039805, loss_ce: 0.017416
2022-01-06 14:09:44,413 iteration 4325 : loss : 0.048127, loss_ce: 0.021173
2022-01-06 14:09:45,488 iteration 4326 : loss : 0.022541, loss_ce: 0.010716
2022-01-06 14:09:46,610 iteration 4327 : loss : 0.035170, loss_ce: 0.012715
2022-01-06 14:09:47,671 iteration 4328 : loss : 0.036505, loss_ce: 0.012552
2022-01-06 14:09:48,827 iteration 4329 : loss : 0.043619, loss_ce: 0.009596
2022-01-06 14:09:49,955 iteration 4330 : loss : 0.021258, loss_ce: 0.009029
2022-01-06 14:09:51,141 iteration 4331 : loss : 0.047546, loss_ce: 0.014690
2022-01-06 14:09:52,287 iteration 4332 : loss : 0.031181, loss_ce: 0.010172
2022-01-06 14:09:53,456 iteration 4333 : loss : 0.036029, loss_ce: 0.017501
2022-01-06 14:09:54,560 iteration 4334 : loss : 0.037600, loss_ce: 0.022762
2022-01-06 14:09:54,560 Training Data Eval:
2022-01-06 14:10:00,013   Average segmentation loss on training set: 0.0379
2022-01-06 14:10:00,013 Validation Data Eval:
2022-01-06 14:10:01,901   Average segmentation loss on validation set: 0.1684
2022-01-06 14:10:03,051 iteration 4335 : loss : 0.034805, loss_ce: 0.008893
 64%|██████████████████▍          | 255/400 [1:28:29<52:05, 21.56s/it]2022-01-06 14:10:04,110 iteration 4336 : loss : 0.018286, loss_ce: 0.008923
2022-01-06 14:10:05,297 iteration 4337 : loss : 0.027663, loss_ce: 0.011523
2022-01-06 14:10:06,403 iteration 4338 : loss : 0.024284, loss_ce: 0.008719
2022-01-06 14:10:07,453 iteration 4339 : loss : 0.025025, loss_ce: 0.008752
2022-01-06 14:10:08,585 iteration 4340 : loss : 0.029110, loss_ce: 0.014187
2022-01-06 14:10:09,635 iteration 4341 : loss : 0.020878, loss_ce: 0.007209
2022-01-06 14:10:10,781 iteration 4342 : loss : 0.028730, loss_ce: 0.007843
2022-01-06 14:10:11,922 iteration 4343 : loss : 0.039385, loss_ce: 0.011584
2022-01-06 14:10:13,066 iteration 4344 : loss : 0.028822, loss_ce: 0.014062
2022-01-06 14:10:14,263 iteration 4345 : loss : 0.025026, loss_ce: 0.010394
2022-01-06 14:10:15,309 iteration 4346 : loss : 0.028666, loss_ce: 0.009148
2022-01-06 14:10:16,421 iteration 4347 : loss : 0.019104, loss_ce: 0.008954
2022-01-06 14:10:17,483 iteration 4348 : loss : 0.020235, loss_ce: 0.009856
2022-01-06 14:10:18,503 iteration 4349 : loss : 0.021280, loss_ce: 0.006278
2022-01-06 14:10:19,532 iteration 4350 : loss : 0.027937, loss_ce: 0.010106
2022-01-06 14:10:20,660 iteration 4351 : loss : 0.034512, loss_ce: 0.010441
2022-01-06 14:10:21,768 iteration 4352 : loss : 0.026138, loss_ce: 0.010835
 64%|██████████████████▌          | 256/400 [1:28:48<49:41, 20.71s/it]2022-01-06 14:10:22,890 iteration 4353 : loss : 0.024381, loss_ce: 0.008374
2022-01-06 14:10:24,082 iteration 4354 : loss : 0.030746, loss_ce: 0.014589
2022-01-06 14:10:25,240 iteration 4355 : loss : 0.022732, loss_ce: 0.009616
2022-01-06 14:10:26,434 iteration 4356 : loss : 0.027178, loss_ce: 0.007148
2022-01-06 14:10:27,586 iteration 4357 : loss : 0.023323, loss_ce: 0.010773
2022-01-06 14:10:28,758 iteration 4358 : loss : 0.032404, loss_ce: 0.011506
2022-01-06 14:10:29,885 iteration 4359 : loss : 0.034499, loss_ce: 0.009680
2022-01-06 14:10:30,979 iteration 4360 : loss : 0.031221, loss_ce: 0.015419
2022-01-06 14:10:32,141 iteration 4361 : loss : 0.026151, loss_ce: 0.010222
2022-01-06 14:10:33,253 iteration 4362 : loss : 0.023720, loss_ce: 0.010722
2022-01-06 14:10:34,362 iteration 4363 : loss : 0.022257, loss_ce: 0.008961
2022-01-06 14:10:35,490 iteration 4364 : loss : 0.025161, loss_ce: 0.009428
2022-01-06 14:10:36,687 iteration 4365 : loss : 0.029957, loss_ce: 0.014500
2022-01-06 14:10:37,742 iteration 4366 : loss : 0.023819, loss_ce: 0.007403
2022-01-06 14:10:38,745 iteration 4367 : loss : 0.026670, loss_ce: 0.010573
2022-01-06 14:10:39,782 iteration 4368 : loss : 0.026917, loss_ce: 0.009168
2022-01-06 14:10:40,895 iteration 4369 : loss : 0.025829, loss_ce: 0.009645
 64%|██████████████████▋          | 257/400 [1:29:07<48:13, 20.23s/it]2022-01-06 14:10:42,061 iteration 4370 : loss : 0.026074, loss_ce: 0.008610
2022-01-06 14:10:43,191 iteration 4371 : loss : 0.037052, loss_ce: 0.017997
2022-01-06 14:10:44,328 iteration 4372 : loss : 0.023467, loss_ce: 0.009665
2022-01-06 14:10:45,455 iteration 4373 : loss : 0.026199, loss_ce: 0.013742
2022-01-06 14:10:46,594 iteration 4374 : loss : 0.029193, loss_ce: 0.011118
2022-01-06 14:10:47,739 iteration 4375 : loss : 0.032685, loss_ce: 0.015737
2022-01-06 14:10:48,831 iteration 4376 : loss : 0.031357, loss_ce: 0.010141
2022-01-06 14:10:49,931 iteration 4377 : loss : 0.027002, loss_ce: 0.008126
2022-01-06 14:10:50,988 iteration 4378 : loss : 0.022893, loss_ce: 0.008404
2022-01-06 14:10:52,052 iteration 4379 : loss : 0.031295, loss_ce: 0.013608
2022-01-06 14:10:53,092 iteration 4380 : loss : 0.022403, loss_ce: 0.007056
2022-01-06 14:10:54,239 iteration 4381 : loss : 0.034067, loss_ce: 0.011655
2022-01-06 14:10:55,347 iteration 4382 : loss : 0.028988, loss_ce: 0.007148
2022-01-06 14:10:56,427 iteration 4383 : loss : 0.027160, loss_ce: 0.007822
2022-01-06 14:10:57,590 iteration 4384 : loss : 0.023762, loss_ce: 0.009854
2022-01-06 14:10:58,757 iteration 4385 : loss : 0.036332, loss_ce: 0.010706
2022-01-06 14:10:59,831 iteration 4386 : loss : 0.021348, loss_ce: 0.009836
 64%|██████████████████▋          | 258/400 [1:29:26<46:57, 19.84s/it]2022-01-06 14:11:00,939 iteration 4387 : loss : 0.023027, loss_ce: 0.007801
2022-01-06 14:11:02,055 iteration 4388 : loss : 0.025673, loss_ce: 0.011165
2022-01-06 14:11:03,173 iteration 4389 : loss : 0.025959, loss_ce: 0.008731
2022-01-06 14:11:04,226 iteration 4390 : loss : 0.025099, loss_ce: 0.009086
2022-01-06 14:11:05,349 iteration 4391 : loss : 0.034783, loss_ce: 0.010902
2022-01-06 14:11:06,453 iteration 4392 : loss : 0.021211, loss_ce: 0.007610
2022-01-06 14:11:07,629 iteration 4393 : loss : 0.027839, loss_ce: 0.010863
2022-01-06 14:11:08,773 iteration 4394 : loss : 0.024415, loss_ce: 0.010153
2022-01-06 14:11:09,982 iteration 4395 : loss : 0.031311, loss_ce: 0.010850
2022-01-06 14:11:11,050 iteration 4396 : loss : 0.023080, loss_ce: 0.007529
2022-01-06 14:11:12,260 iteration 4397 : loss : 0.029083, loss_ce: 0.013003
2022-01-06 14:11:13,320 iteration 4398 : loss : 0.021963, loss_ce: 0.009198
2022-01-06 14:11:14,477 iteration 4399 : loss : 0.043795, loss_ce: 0.020635
2022-01-06 14:11:15,598 iteration 4400 : loss : 0.032368, loss_ce: 0.010052
2022-01-06 14:11:16,843 iteration 4401 : loss : 0.039179, loss_ce: 0.015019
2022-01-06 14:11:17,960 iteration 4402 : loss : 0.031303, loss_ce: 0.009427
2022-01-06 14:11:19,088 iteration 4403 : loss : 0.028021, loss_ce: 0.009630
 65%|██████████████████▊          | 259/400 [1:29:46<46:13, 19.67s/it]2022-01-06 14:11:20,236 iteration 4404 : loss : 0.024652, loss_ce: 0.011206
2022-01-06 14:11:21,400 iteration 4405 : loss : 0.024136, loss_ce: 0.010718
2022-01-06 14:11:22,557 iteration 4406 : loss : 0.039554, loss_ce: 0.015806
2022-01-06 14:11:23,740 iteration 4407 : loss : 0.039834, loss_ce: 0.014790
2022-01-06 14:11:24,840 iteration 4408 : loss : 0.022800, loss_ce: 0.009948
2022-01-06 14:11:25,986 iteration 4409 : loss : 0.027689, loss_ce: 0.011021
2022-01-06 14:11:27,279 iteration 4410 : loss : 0.049885, loss_ce: 0.020479
2022-01-06 14:11:28,439 iteration 4411 : loss : 0.029863, loss_ce: 0.012686
2022-01-06 14:11:29,547 iteration 4412 : loss : 0.022356, loss_ce: 0.008341
2022-01-06 14:11:30,685 iteration 4413 : loss : 0.019729, loss_ce: 0.006339
2022-01-06 14:11:31,751 iteration 4414 : loss : 0.023387, loss_ce: 0.006732
2022-01-06 14:11:32,907 iteration 4415 : loss : 0.021804, loss_ce: 0.007382
2022-01-06 14:11:34,079 iteration 4416 : loss : 0.036649, loss_ce: 0.015292
2022-01-06 14:11:35,192 iteration 4417 : loss : 0.027012, loss_ce: 0.007838
2022-01-06 14:11:36,324 iteration 4418 : loss : 0.028493, loss_ce: 0.012383
2022-01-06 14:11:37,493 iteration 4419 : loss : 0.024436, loss_ce: 0.007897
2022-01-06 14:11:37,493 Training Data Eval:
2022-01-06 14:11:43,025   Average segmentation loss on training set: 0.0291
2022-01-06 14:11:43,025 Validation Data Eval:
2022-01-06 14:11:44,923   Average segmentation loss on validation set: 0.1296
2022-01-06 14:11:45,985 iteration 4420 : loss : 0.021104, loss_ce: 0.008979
 65%|██████████████████▊          | 260/400 [1:30:12<50:57, 21.84s/it]2022-01-06 14:11:47,129 iteration 4421 : loss : 0.024253, loss_ce: 0.008775
2022-01-06 14:11:48,254 iteration 4422 : loss : 0.034756, loss_ce: 0.017464
2022-01-06 14:11:49,418 iteration 4423 : loss : 0.036485, loss_ce: 0.013558
2022-01-06 14:11:50,465 iteration 4424 : loss : 0.030722, loss_ce: 0.010698
2022-01-06 14:11:51,512 iteration 4425 : loss : 0.021383, loss_ce: 0.007237
2022-01-06 14:11:52,675 iteration 4426 : loss : 0.019684, loss_ce: 0.007869
2022-01-06 14:11:53,730 iteration 4427 : loss : 0.040072, loss_ce: 0.016022
2022-01-06 14:11:54,743 iteration 4428 : loss : 0.018701, loss_ce: 0.008619
2022-01-06 14:11:55,787 iteration 4429 : loss : 0.029137, loss_ce: 0.010743
2022-01-06 14:11:56,850 iteration 4430 : loss : 0.019805, loss_ce: 0.008243
2022-01-06 14:11:57,991 iteration 4431 : loss : 0.027649, loss_ce: 0.011413
2022-01-06 14:11:59,158 iteration 4432 : loss : 0.030406, loss_ce: 0.011878
2022-01-06 14:12:00,255 iteration 4433 : loss : 0.031200, loss_ce: 0.008333
2022-01-06 14:12:01,323 iteration 4434 : loss : 0.018980, loss_ce: 0.007889
2022-01-06 14:12:02,457 iteration 4435 : loss : 0.044803, loss_ce: 0.011286
2022-01-06 14:12:03,572 iteration 4436 : loss : 0.025196, loss_ce: 0.010749
2022-01-06 14:12:04,746 iteration 4437 : loss : 0.030250, loss_ce: 0.009999
 65%|██████████████████▉          | 261/400 [1:30:31<48:27, 20.91s/it]2022-01-06 14:12:05,982 iteration 4438 : loss : 0.049347, loss_ce: 0.014995
2022-01-06 14:12:07,124 iteration 4439 : loss : 0.021985, loss_ce: 0.007653
2022-01-06 14:12:08,247 iteration 4440 : loss : 0.023185, loss_ce: 0.007744
2022-01-06 14:12:09,474 iteration 4441 : loss : 0.030690, loss_ce: 0.009564
2022-01-06 14:12:10,656 iteration 4442 : loss : 0.019679, loss_ce: 0.008994
2022-01-06 14:12:11,794 iteration 4443 : loss : 0.030930, loss_ce: 0.010334
2022-01-06 14:12:12,854 iteration 4444 : loss : 0.023410, loss_ce: 0.007886
2022-01-06 14:12:13,974 iteration 4445 : loss : 0.037942, loss_ce: 0.014450
2022-01-06 14:12:15,147 iteration 4446 : loss : 0.039384, loss_ce: 0.021559
2022-01-06 14:12:16,263 iteration 4447 : loss : 0.022342, loss_ce: 0.007550
2022-01-06 14:12:17,420 iteration 4448 : loss : 0.030287, loss_ce: 0.011411
2022-01-06 14:12:18,551 iteration 4449 : loss : 0.036300, loss_ce: 0.015268
2022-01-06 14:12:19,726 iteration 4450 : loss : 0.047681, loss_ce: 0.014821
2022-01-06 14:12:20,814 iteration 4451 : loss : 0.027560, loss_ce: 0.010574
2022-01-06 14:12:21,956 iteration 4452 : loss : 0.024465, loss_ce: 0.011123
2022-01-06 14:12:23,057 iteration 4453 : loss : 0.019464, loss_ce: 0.006999
2022-01-06 14:12:24,161 iteration 4454 : loss : 0.025120, loss_ce: 0.011395
 66%|██████████████████▉          | 262/400 [1:30:51<47:03, 20.46s/it]2022-01-06 14:12:25,361 iteration 4455 : loss : 0.027426, loss_ce: 0.007749
2022-01-06 14:12:26,406 iteration 4456 : loss : 0.020105, loss_ce: 0.007491
2022-01-06 14:12:27,557 iteration 4457 : loss : 0.025602, loss_ce: 0.010015
2022-01-06 14:12:28,791 iteration 4458 : loss : 0.032373, loss_ce: 0.013029
2022-01-06 14:12:30,065 iteration 4459 : loss : 0.024813, loss_ce: 0.010023
2022-01-06 14:12:31,143 iteration 4460 : loss : 0.020525, loss_ce: 0.007438
2022-01-06 14:12:32,265 iteration 4461 : loss : 0.028392, loss_ce: 0.009240
2022-01-06 14:12:33,421 iteration 4462 : loss : 0.026759, loss_ce: 0.013266
2022-01-06 14:12:34,523 iteration 4463 : loss : 0.033063, loss_ce: 0.012837
2022-01-06 14:12:35,606 iteration 4464 : loss : 0.019033, loss_ce: 0.006079
2022-01-06 14:12:36,643 iteration 4465 : loss : 0.024801, loss_ce: 0.005734
2022-01-06 14:12:37,708 iteration 4466 : loss : 0.029727, loss_ce: 0.009286
2022-01-06 14:12:38,817 iteration 4467 : loss : 0.042935, loss_ce: 0.018844
2022-01-06 14:12:39,957 iteration 4468 : loss : 0.025800, loss_ce: 0.010428
2022-01-06 14:12:41,132 iteration 4469 : loss : 0.029441, loss_ce: 0.013377
2022-01-06 14:12:42,290 iteration 4470 : loss : 0.025116, loss_ce: 0.009608
2022-01-06 14:12:43,471 iteration 4471 : loss : 0.037448, loss_ce: 0.012184
 66%|███████████████████          | 263/400 [1:31:10<45:55, 20.11s/it]2022-01-06 14:12:44,595 iteration 4472 : loss : 0.019056, loss_ce: 0.008079
2022-01-06 14:12:45,651 iteration 4473 : loss : 0.025165, loss_ce: 0.010245
2022-01-06 14:12:46,759 iteration 4474 : loss : 0.026266, loss_ce: 0.013044
2022-01-06 14:12:47,855 iteration 4475 : loss : 0.021144, loss_ce: 0.006544
2022-01-06 14:12:48,929 iteration 4476 : loss : 0.022284, loss_ce: 0.006247
2022-01-06 14:12:50,027 iteration 4477 : loss : 0.030573, loss_ce: 0.011446
2022-01-06 14:12:51,262 iteration 4478 : loss : 0.029125, loss_ce: 0.014054
2022-01-06 14:12:52,386 iteration 4479 : loss : 0.033277, loss_ce: 0.009593
2022-01-06 14:12:53,526 iteration 4480 : loss : 0.022864, loss_ce: 0.008735
2022-01-06 14:12:54,613 iteration 4481 : loss : 0.031861, loss_ce: 0.012161
2022-01-06 14:12:55,689 iteration 4482 : loss : 0.019532, loss_ce: 0.005698
2022-01-06 14:12:56,778 iteration 4483 : loss : 0.023500, loss_ce: 0.007588
2022-01-06 14:12:57,893 iteration 4484 : loss : 0.018920, loss_ce: 0.005789
2022-01-06 14:12:58,940 iteration 4485 : loss : 0.018781, loss_ce: 0.007466
2022-01-06 14:13:00,105 iteration 4486 : loss : 0.028614, loss_ce: 0.011274
2022-01-06 14:13:01,242 iteration 4487 : loss : 0.026057, loss_ce: 0.011107
2022-01-06 14:13:02,304 iteration 4488 : loss : 0.023891, loss_ce: 0.008841
 66%|███████████████████▏         | 264/400 [1:31:29<44:43, 19.73s/it]2022-01-06 14:13:03,503 iteration 4489 : loss : 0.026152, loss_ce: 0.010659
2022-01-06 14:13:04,600 iteration 4490 : loss : 0.023862, loss_ce: 0.012875
2022-01-06 14:13:05,797 iteration 4491 : loss : 0.027851, loss_ce: 0.009080
2022-01-06 14:13:06,870 iteration 4492 : loss : 0.022573, loss_ce: 0.005865
2022-01-06 14:13:08,054 iteration 4493 : loss : 0.032363, loss_ce: 0.019771
2022-01-06 14:13:09,119 iteration 4494 : loss : 0.022820, loss_ce: 0.007761
2022-01-06 14:13:10,228 iteration 4495 : loss : 0.033999, loss_ce: 0.008922
2022-01-06 14:13:11,349 iteration 4496 : loss : 0.024891, loss_ce: 0.007945
2022-01-06 14:13:12,353 iteration 4497 : loss : 0.031900, loss_ce: 0.014053
2022-01-06 14:13:13,451 iteration 4498 : loss : 0.022863, loss_ce: 0.007787
2022-01-06 14:13:14,530 iteration 4499 : loss : 0.021364, loss_ce: 0.009063
2022-01-06 14:13:15,637 iteration 4500 : loss : 0.026977, loss_ce: 0.008283
2022-01-06 14:13:16,725 iteration 4501 : loss : 0.023140, loss_ce: 0.009777
2022-01-06 14:13:17,820 iteration 4502 : loss : 0.023220, loss_ce: 0.007772
2022-01-06 14:13:19,005 iteration 4503 : loss : 0.023011, loss_ce: 0.008385
2022-01-06 14:13:20,023 iteration 4504 : loss : 0.023820, loss_ce: 0.009603
2022-01-06 14:13:20,023 Training Data Eval:
2022-01-06 14:13:25,536   Average segmentation loss on training set: 0.0264
2022-01-06 14:13:25,536 Validation Data Eval:
2022-01-06 14:13:27,424   Average segmentation loss on validation set: 0.0770
2022-01-06 14:13:28,530 iteration 4505 : loss : 0.024304, loss_ce: 0.008484
 66%|███████████████████▏         | 265/400 [1:31:55<48:47, 21.68s/it]2022-01-06 14:13:29,719 iteration 4506 : loss : 0.032826, loss_ce: 0.010581
2022-01-06 14:13:30,894 iteration 4507 : loss : 0.026239, loss_ce: 0.010607
2022-01-06 14:13:32,003 iteration 4508 : loss : 0.030015, loss_ce: 0.008744
2022-01-06 14:13:33,052 iteration 4509 : loss : 0.022322, loss_ce: 0.008115
2022-01-06 14:13:34,152 iteration 4510 : loss : 0.016753, loss_ce: 0.005454
2022-01-06 14:13:35,347 iteration 4511 : loss : 0.029055, loss_ce: 0.012788
2022-01-06 14:13:36,371 iteration 4512 : loss : 0.020437, loss_ce: 0.007599
2022-01-06 14:13:37,469 iteration 4513 : loss : 0.019571, loss_ce: 0.007091
2022-01-06 14:13:38,565 iteration 4514 : loss : 0.024531, loss_ce: 0.009540
2022-01-06 14:13:39,683 iteration 4515 : loss : 0.019837, loss_ce: 0.007827
2022-01-06 14:13:40,827 iteration 4516 : loss : 0.021238, loss_ce: 0.008729
2022-01-06 14:13:41,997 iteration 4517 : loss : 0.038799, loss_ce: 0.015694
2022-01-06 14:13:43,110 iteration 4518 : loss : 0.025565, loss_ce: 0.010573
2022-01-06 14:13:44,177 iteration 4519 : loss : 0.027041, loss_ce: 0.009227
2022-01-06 14:13:45,314 iteration 4520 : loss : 0.024192, loss_ce: 0.006266
2022-01-06 14:13:46,459 iteration 4521 : loss : 0.020231, loss_ce: 0.008110
2022-01-06 14:13:47,641 iteration 4522 : loss : 0.043138, loss_ce: 0.016059
 66%|███████████████████▎         | 266/400 [1:32:14<46:42, 20.91s/it]2022-01-06 14:13:48,899 iteration 4523 : loss : 0.030416, loss_ce: 0.015469
2022-01-06 14:13:50,034 iteration 4524 : loss : 0.024662, loss_ce: 0.008503
2022-01-06 14:13:51,180 iteration 4525 : loss : 0.027556, loss_ce: 0.011914
2022-01-06 14:13:52,289 iteration 4526 : loss : 0.027231, loss_ce: 0.006738
2022-01-06 14:13:53,338 iteration 4527 : loss : 0.024590, loss_ce: 0.008907
2022-01-06 14:13:54,418 iteration 4528 : loss : 0.027699, loss_ce: 0.009546
2022-01-06 14:13:55,551 iteration 4529 : loss : 0.021703, loss_ce: 0.011146
2022-01-06 14:13:56,636 iteration 4530 : loss : 0.029593, loss_ce: 0.007459
2022-01-06 14:13:57,758 iteration 4531 : loss : 0.022977, loss_ce: 0.008122
2022-01-06 14:13:58,896 iteration 4532 : loss : 0.015350, loss_ce: 0.005144
2022-01-06 14:13:59,991 iteration 4533 : loss : 0.026524, loss_ce: 0.011667
2022-01-06 14:14:01,092 iteration 4534 : loss : 0.032890, loss_ce: 0.009232
2022-01-06 14:14:02,257 iteration 4535 : loss : 0.023523, loss_ce: 0.008878
2022-01-06 14:14:03,401 iteration 4536 : loss : 0.024775, loss_ce: 0.009762
2022-01-06 14:14:04,537 iteration 4537 : loss : 0.030437, loss_ce: 0.011691
2022-01-06 14:14:05,665 iteration 4538 : loss : 0.026706, loss_ce: 0.009081
2022-01-06 14:14:06,791 iteration 4539 : loss : 0.022972, loss_ce: 0.008538
 67%|███████████████████▎         | 267/400 [1:32:33<45:10, 20.38s/it]2022-01-06 14:14:07,940 iteration 4540 : loss : 0.017276, loss_ce: 0.006094
2022-01-06 14:14:09,081 iteration 4541 : loss : 0.023522, loss_ce: 0.010681
2022-01-06 14:14:10,146 iteration 4542 : loss : 0.029939, loss_ce: 0.011292
2022-01-06 14:14:11,244 iteration 4543 : loss : 0.040311, loss_ce: 0.015594
2022-01-06 14:14:12,326 iteration 4544 : loss : 0.023815, loss_ce: 0.008421
2022-01-06 14:14:13,494 iteration 4545 : loss : 0.027238, loss_ce: 0.011420
2022-01-06 14:14:14,705 iteration 4546 : loss : 0.029269, loss_ce: 0.012131
2022-01-06 14:14:15,895 iteration 4547 : loss : 0.025428, loss_ce: 0.009646
2022-01-06 14:14:16,932 iteration 4548 : loss : 0.027594, loss_ce: 0.009499
2022-01-06 14:14:17,983 iteration 4549 : loss : 0.026761, loss_ce: 0.007098
2022-01-06 14:14:19,098 iteration 4550 : loss : 0.028767, loss_ce: 0.011359
2022-01-06 14:14:20,241 iteration 4551 : loss : 0.023635, loss_ce: 0.008848
2022-01-06 14:14:21,440 iteration 4552 : loss : 0.045742, loss_ce: 0.012425
2022-01-06 14:14:22,574 iteration 4553 : loss : 0.020993, loss_ce: 0.007340
2022-01-06 14:14:23,641 iteration 4554 : loss : 0.022754, loss_ce: 0.009699
2022-01-06 14:14:24,732 iteration 4555 : loss : 0.022914, loss_ce: 0.009244
2022-01-06 14:14:25,828 iteration 4556 : loss : 0.034488, loss_ce: 0.010319
 67%|███████████████████▍         | 268/400 [1:32:52<43:56, 19.98s/it]2022-01-06 14:14:26,960 iteration 4557 : loss : 0.023162, loss_ce: 0.009469
2022-01-06 14:14:28,103 iteration 4558 : loss : 0.024820, loss_ce: 0.007502
2022-01-06 14:14:29,273 iteration 4559 : loss : 0.036460, loss_ce: 0.014030
2022-01-06 14:14:30,408 iteration 4560 : loss : 0.029825, loss_ce: 0.017581
2022-01-06 14:14:31,467 iteration 4561 : loss : 0.018426, loss_ce: 0.006164
2022-01-06 14:14:32,583 iteration 4562 : loss : 0.034364, loss_ce: 0.014194
2022-01-06 14:14:33,666 iteration 4563 : loss : 0.023419, loss_ce: 0.008481
2022-01-06 14:14:34,734 iteration 4564 : loss : 0.026651, loss_ce: 0.011134
2022-01-06 14:14:35,819 iteration 4565 : loss : 0.029128, loss_ce: 0.012426
2022-01-06 14:14:36,929 iteration 4566 : loss : 0.024497, loss_ce: 0.009618
2022-01-06 14:14:38,027 iteration 4567 : loss : 0.039604, loss_ce: 0.015483
2022-01-06 14:14:39,056 iteration 4568 : loss : 0.025052, loss_ce: 0.007944
2022-01-06 14:14:40,115 iteration 4569 : loss : 0.027029, loss_ce: 0.010007
2022-01-06 14:14:41,186 iteration 4570 : loss : 0.023212, loss_ce: 0.008940
2022-01-06 14:14:42,339 iteration 4571 : loss : 0.022121, loss_ce: 0.009594
2022-01-06 14:14:43,488 iteration 4572 : loss : 0.028719, loss_ce: 0.011082
2022-01-06 14:14:44,657 iteration 4573 : loss : 0.038853, loss_ce: 0.014178
 67%|███████████████████▌         | 269/400 [1:33:11<42:51, 19.63s/it]2022-01-06 14:14:45,768 iteration 4574 : loss : 0.016816, loss_ce: 0.005384
2022-01-06 14:14:46,938 iteration 4575 : loss : 0.019245, loss_ce: 0.005738
2022-01-06 14:14:47,981 iteration 4576 : loss : 0.023344, loss_ce: 0.008722
2022-01-06 14:14:49,040 iteration 4577 : loss : 0.019025, loss_ce: 0.009070
2022-01-06 14:14:50,143 iteration 4578 : loss : 0.024277, loss_ce: 0.009251
2022-01-06 14:14:51,212 iteration 4579 : loss : 0.024141, loss_ce: 0.011311
2022-01-06 14:14:52,302 iteration 4580 : loss : 0.020424, loss_ce: 0.007177
2022-01-06 14:14:53,411 iteration 4581 : loss : 0.037425, loss_ce: 0.013629
2022-01-06 14:14:54,523 iteration 4582 : loss : 0.033745, loss_ce: 0.010904
2022-01-06 14:14:55,630 iteration 4583 : loss : 0.031299, loss_ce: 0.011963
2022-01-06 14:14:56,716 iteration 4584 : loss : 0.019042, loss_ce: 0.006448
2022-01-06 14:14:57,845 iteration 4585 : loss : 0.031089, loss_ce: 0.010287
2022-01-06 14:14:58,899 iteration 4586 : loss : 0.023305, loss_ce: 0.010870
2022-01-06 14:14:59,959 iteration 4587 : loss : 0.022438, loss_ce: 0.006956
2022-01-06 14:15:01,025 iteration 4588 : loss : 0.029493, loss_ce: 0.010672
2022-01-06 14:15:02,141 iteration 4589 : loss : 0.027665, loss_ce: 0.009373
2022-01-06 14:15:02,141 Training Data Eval:
2022-01-06 14:15:07,657   Average segmentation loss on training set: 0.0370
2022-01-06 14:15:07,657 Validation Data Eval:
2022-01-06 14:15:09,558   Average segmentation loss on validation set: 0.0865
2022-01-06 14:15:10,662 iteration 4590 : loss : 0.024702, loss_ce: 0.010738
 68%|███████████████████▌         | 270/400 [1:33:37<46:41, 21.55s/it]2022-01-06 14:15:11,835 iteration 4591 : loss : 0.041215, loss_ce: 0.010195
2022-01-06 14:15:12,999 iteration 4592 : loss : 0.027362, loss_ce: 0.009446
2022-01-06 14:15:14,143 iteration 4593 : loss : 0.025121, loss_ce: 0.011426
2022-01-06 14:15:15,329 iteration 4594 : loss : 0.053567, loss_ce: 0.017050
2022-01-06 14:15:16,434 iteration 4595 : loss : 0.026529, loss_ce: 0.008529
2022-01-06 14:15:17,513 iteration 4596 : loss : 0.028638, loss_ce: 0.008200
2022-01-06 14:15:18,638 iteration 4597 : loss : 0.019688, loss_ce: 0.008815
2022-01-06 14:15:19,808 iteration 4598 : loss : 0.031166, loss_ce: 0.010724
2022-01-06 14:15:20,953 iteration 4599 : loss : 0.018937, loss_ce: 0.009209
2022-01-06 14:15:22,041 iteration 4600 : loss : 0.033068, loss_ce: 0.010670
2022-01-06 14:15:23,173 iteration 4601 : loss : 0.030355, loss_ce: 0.013196
2022-01-06 14:15:24,374 iteration 4602 : loss : 0.037699, loss_ce: 0.009555
2022-01-06 14:15:25,496 iteration 4603 : loss : 0.036128, loss_ce: 0.011346
2022-01-06 14:15:26,644 iteration 4604 : loss : 0.022544, loss_ce: 0.009318
2022-01-06 14:15:27,810 iteration 4605 : loss : 0.047673, loss_ce: 0.009060
2022-01-06 14:15:28,965 iteration 4606 : loss : 0.024836, loss_ce: 0.009319
2022-01-06 14:15:30,120 iteration 4607 : loss : 0.022472, loss_ce: 0.011239
 68%|███████████████████▋         | 271/400 [1:33:57<44:58, 20.92s/it]2022-01-06 14:15:31,240 iteration 4608 : loss : 0.018788, loss_ce: 0.006996
2022-01-06 14:15:32,350 iteration 4609 : loss : 0.032537, loss_ce: 0.012370
2022-01-06 14:15:33,403 iteration 4610 : loss : 0.028810, loss_ce: 0.009058
2022-01-06 14:15:34,445 iteration 4611 : loss : 0.024549, loss_ce: 0.010866
2022-01-06 14:15:35,469 iteration 4612 : loss : 0.023131, loss_ce: 0.006376
2022-01-06 14:15:36,571 iteration 4613 : loss : 0.024123, loss_ce: 0.007905
2022-01-06 14:15:37,676 iteration 4614 : loss : 0.018489, loss_ce: 0.007936
2022-01-06 14:15:38,826 iteration 4615 : loss : 0.030372, loss_ce: 0.010861
2022-01-06 14:15:40,045 iteration 4616 : loss : 0.021378, loss_ce: 0.008812
2022-01-06 14:15:41,165 iteration 4617 : loss : 0.024695, loss_ce: 0.008884
2022-01-06 14:15:42,222 iteration 4618 : loss : 0.019234, loss_ce: 0.005297
2022-01-06 14:15:43,289 iteration 4619 : loss : 0.016528, loss_ce: 0.006861
2022-01-06 14:15:44,448 iteration 4620 : loss : 0.026022, loss_ce: 0.007731
2022-01-06 14:15:45,546 iteration 4621 : loss : 0.032650, loss_ce: 0.016923
2022-01-06 14:15:46,660 iteration 4622 : loss : 0.031987, loss_ce: 0.011500
2022-01-06 14:15:47,760 iteration 4623 : loss : 0.021025, loss_ce: 0.007028
2022-01-06 14:15:48,877 iteration 4624 : loss : 0.021933, loss_ce: 0.009711
 68%|███████████████████▋         | 272/400 [1:34:15<43:14, 20.27s/it]2022-01-06 14:15:50,087 iteration 4625 : loss : 0.030632, loss_ce: 0.015979
2022-01-06 14:15:51,257 iteration 4626 : loss : 0.029058, loss_ce: 0.013248
2022-01-06 14:15:52,465 iteration 4627 : loss : 0.025356, loss_ce: 0.008912
2022-01-06 14:15:53,547 iteration 4628 : loss : 0.022187, loss_ce: 0.008767
2022-01-06 14:15:54,591 iteration 4629 : loss : 0.022655, loss_ce: 0.008048
2022-01-06 14:15:55,677 iteration 4630 : loss : 0.025102, loss_ce: 0.008693
2022-01-06 14:15:56,780 iteration 4631 : loss : 0.029381, loss_ce: 0.009187
2022-01-06 14:15:57,883 iteration 4632 : loss : 0.034663, loss_ce: 0.010924
2022-01-06 14:15:58,962 iteration 4633 : loss : 0.023348, loss_ce: 0.007502
2022-01-06 14:16:00,023 iteration 4634 : loss : 0.019400, loss_ce: 0.009008
2022-01-06 14:16:01,051 iteration 4635 : loss : 0.017001, loss_ce: 0.006945
2022-01-06 14:16:02,233 iteration 4636 : loss : 0.028649, loss_ce: 0.010970
2022-01-06 14:16:03,317 iteration 4637 : loss : 0.036824, loss_ce: 0.009997
2022-01-06 14:16:04,536 iteration 4638 : loss : 0.041486, loss_ce: 0.018688
2022-01-06 14:16:05,686 iteration 4639 : loss : 0.031051, loss_ce: 0.010989
2022-01-06 14:16:06,908 iteration 4640 : loss : 0.034191, loss_ce: 0.012963
2022-01-06 14:16:08,060 iteration 4641 : loss : 0.025304, loss_ce: 0.009458
 68%|███████████████████▊         | 273/400 [1:34:35<42:13, 19.95s/it]2022-01-06 14:16:09,216 iteration 4642 : loss : 0.030369, loss_ce: 0.006457
2022-01-06 14:16:10,317 iteration 4643 : loss : 0.021076, loss_ce: 0.007245
2022-01-06 14:16:11,467 iteration 4644 : loss : 0.032324, loss_ce: 0.011113
2022-01-06 14:16:12,630 iteration 4645 : loss : 0.031802, loss_ce: 0.015695
2022-01-06 14:16:13,760 iteration 4646 : loss : 0.032055, loss_ce: 0.009592
2022-01-06 14:16:14,795 iteration 4647 : loss : 0.021968, loss_ce: 0.007524
2022-01-06 14:16:15,864 iteration 4648 : loss : 0.023275, loss_ce: 0.007247
2022-01-06 14:16:16,973 iteration 4649 : loss : 0.026758, loss_ce: 0.010234
2022-01-06 14:16:18,123 iteration 4650 : loss : 0.032722, loss_ce: 0.011483
2022-01-06 14:16:19,173 iteration 4651 : loss : 0.024775, loss_ce: 0.010345
2022-01-06 14:16:20,357 iteration 4652 : loss : 0.033757, loss_ce: 0.011580
2022-01-06 14:16:21,462 iteration 4653 : loss : 0.030712, loss_ce: 0.012767
2022-01-06 14:16:22,579 iteration 4654 : loss : 0.035335, loss_ce: 0.010726
2022-01-06 14:16:23,783 iteration 4655 : loss : 0.032276, loss_ce: 0.012464
2022-01-06 14:16:24,998 iteration 4656 : loss : 0.029531, loss_ce: 0.010949
2022-01-06 14:16:26,159 iteration 4657 : loss : 0.024659, loss_ce: 0.011696
2022-01-06 14:16:27,316 iteration 4658 : loss : 0.032745, loss_ce: 0.011681
 68%|███████████████████▊         | 274/400 [1:34:54<41:26, 19.74s/it]2022-01-06 14:16:28,517 iteration 4659 : loss : 0.027493, loss_ce: 0.012426
2022-01-06 14:16:29,682 iteration 4660 : loss : 0.034117, loss_ce: 0.014714
2022-01-06 14:16:30,820 iteration 4661 : loss : 0.031820, loss_ce: 0.012012
2022-01-06 14:16:31,880 iteration 4662 : loss : 0.021781, loss_ce: 0.008106
2022-01-06 14:16:33,021 iteration 4663 : loss : 0.023810, loss_ce: 0.010083
2022-01-06 14:16:34,112 iteration 4664 : loss : 0.018135, loss_ce: 0.006852
2022-01-06 14:16:35,219 iteration 4665 : loss : 0.025591, loss_ce: 0.009264
2022-01-06 14:16:36,233 iteration 4666 : loss : 0.025262, loss_ce: 0.008053
2022-01-06 14:16:37,309 iteration 4667 : loss : 0.019379, loss_ce: 0.007904
2022-01-06 14:16:38,543 iteration 4668 : loss : 0.030070, loss_ce: 0.014718
2022-01-06 14:16:39,614 iteration 4669 : loss : 0.022856, loss_ce: 0.010839
2022-01-06 14:16:40,678 iteration 4670 : loss : 0.034003, loss_ce: 0.014140
2022-01-06 14:16:41,729 iteration 4671 : loss : 0.026959, loss_ce: 0.007837
2022-01-06 14:16:42,921 iteration 4672 : loss : 0.038212, loss_ce: 0.014911
2022-01-06 14:16:44,076 iteration 4673 : loss : 0.027810, loss_ce: 0.011591
2022-01-06 14:16:45,121 iteration 4674 : loss : 0.019241, loss_ce: 0.005672
2022-01-06 14:16:45,121 Training Data Eval:
2022-01-06 14:16:50,638   Average segmentation loss on training set: 0.0268
2022-01-06 14:16:50,638 Validation Data Eval:
2022-01-06 14:16:52,546   Average segmentation loss on validation set: 0.0886
2022-01-06 14:16:53,730 iteration 4675 : loss : 0.022450, loss_ce: 0.009651
 69%|███████████████████▉         | 275/400 [1:35:20<45:17, 21.74s/it]2022-01-06 14:16:55,004 iteration 4676 : loss : 0.029273, loss_ce: 0.009909
2022-01-06 14:16:56,102 iteration 4677 : loss : 0.033967, loss_ce: 0.010541
2022-01-06 14:16:57,240 iteration 4678 : loss : 0.028168, loss_ce: 0.008970
2022-01-06 14:16:58,431 iteration 4679 : loss : 0.037058, loss_ce: 0.009649
2022-01-06 14:16:59,548 iteration 4680 : loss : 0.021221, loss_ce: 0.007850
2022-01-06 14:17:00,652 iteration 4681 : loss : 0.029276, loss_ce: 0.008461
2022-01-06 14:17:01,741 iteration 4682 : loss : 0.023766, loss_ce: 0.008407
2022-01-06 14:17:02,779 iteration 4683 : loss : 0.028696, loss_ce: 0.007094
2022-01-06 14:17:03,964 iteration 4684 : loss : 0.022385, loss_ce: 0.010097
2022-01-06 14:17:05,127 iteration 4685 : loss : 0.030930, loss_ce: 0.010136
2022-01-06 14:17:06,252 iteration 4686 : loss : 0.036387, loss_ce: 0.016892
2022-01-06 14:17:07,357 iteration 4687 : loss : 0.023248, loss_ce: 0.010689
2022-01-06 14:17:08,417 iteration 4688 : loss : 0.016870, loss_ce: 0.007534
2022-01-06 14:17:09,481 iteration 4689 : loss : 0.015196, loss_ce: 0.006668
2022-01-06 14:17:10,565 iteration 4690 : loss : 0.030694, loss_ce: 0.012300
2022-01-06 14:17:11,723 iteration 4691 : loss : 0.038572, loss_ce: 0.020632
2022-01-06 14:17:12,826 iteration 4692 : loss : 0.026175, loss_ce: 0.007941
 69%|████████████████████         | 276/400 [1:35:39<43:17, 20.95s/it]2022-01-06 14:17:14,070 iteration 4693 : loss : 0.033137, loss_ce: 0.011515
2022-01-06 14:17:15,200 iteration 4694 : loss : 0.025284, loss_ce: 0.010410
2022-01-06 14:17:16,175 iteration 4695 : loss : 0.018659, loss_ce: 0.005797
2022-01-06 14:17:17,353 iteration 4696 : loss : 0.031486, loss_ce: 0.012844
2022-01-06 14:17:18,433 iteration 4697 : loss : 0.023129, loss_ce: 0.008306
2022-01-06 14:17:19,547 iteration 4698 : loss : 0.025018, loss_ce: 0.010912
2022-01-06 14:17:20,719 iteration 4699 : loss : 0.021268, loss_ce: 0.009822
2022-01-06 14:17:21,781 iteration 4700 : loss : 0.022509, loss_ce: 0.009974
2022-01-06 14:17:22,999 iteration 4701 : loss : 0.021634, loss_ce: 0.008460
2022-01-06 14:17:24,103 iteration 4702 : loss : 0.019645, loss_ce: 0.007769
2022-01-06 14:17:25,193 iteration 4703 : loss : 0.024037, loss_ce: 0.006491
2022-01-06 14:17:26,326 iteration 4704 : loss : 0.021878, loss_ce: 0.006835
2022-01-06 14:17:27,345 iteration 4705 : loss : 0.016466, loss_ce: 0.007073
2022-01-06 14:17:28,494 iteration 4706 : loss : 0.021336, loss_ce: 0.006827
2022-01-06 14:17:29,692 iteration 4707 : loss : 0.022511, loss_ce: 0.008147
2022-01-06 14:17:30,880 iteration 4708 : loss : 0.019540, loss_ce: 0.006033
2022-01-06 14:17:32,040 iteration 4709 : loss : 0.031153, loss_ce: 0.011774
 69%|████████████████████         | 277/400 [1:35:58<41:52, 20.43s/it]2022-01-06 14:17:33,241 iteration 4710 : loss : 0.041330, loss_ce: 0.008239
2022-01-06 14:17:34,499 iteration 4711 : loss : 0.030403, loss_ce: 0.012814
2022-01-06 14:17:35,593 iteration 4712 : loss : 0.041435, loss_ce: 0.017116
2022-01-06 14:17:36,698 iteration 4713 : loss : 0.022340, loss_ce: 0.008567
2022-01-06 14:17:37,821 iteration 4714 : loss : 0.039540, loss_ce: 0.014323
2022-01-06 14:17:38,986 iteration 4715 : loss : 0.044646, loss_ce: 0.013885
2022-01-06 14:17:40,119 iteration 4716 : loss : 0.021103, loss_ce: 0.007346
2022-01-06 14:17:41,226 iteration 4717 : loss : 0.024289, loss_ce: 0.008744
2022-01-06 14:17:42,276 iteration 4718 : loss : 0.024049, loss_ce: 0.008577
2022-01-06 14:17:43,325 iteration 4719 : loss : 0.019480, loss_ce: 0.008197
2022-01-06 14:17:44,397 iteration 4720 : loss : 0.023549, loss_ce: 0.009692
2022-01-06 14:17:45,495 iteration 4721 : loss : 0.021314, loss_ce: 0.008587
2022-01-06 14:17:46,657 iteration 4722 : loss : 0.029691, loss_ce: 0.010172
2022-01-06 14:17:47,767 iteration 4723 : loss : 0.019033, loss_ce: 0.007082
2022-01-06 14:17:48,813 iteration 4724 : loss : 0.023942, loss_ce: 0.008635
2022-01-06 14:17:49,857 iteration 4725 : loss : 0.021007, loss_ce: 0.009150
2022-01-06 14:17:50,965 iteration 4726 : loss : 0.026243, loss_ce: 0.010371
 70%|████████████████████▏        | 278/400 [1:36:17<40:37, 19.98s/it]2022-01-06 14:17:52,189 iteration 4727 : loss : 0.024859, loss_ce: 0.008199
2022-01-06 14:17:53,256 iteration 4728 : loss : 0.023576, loss_ce: 0.008524
2022-01-06 14:17:54,334 iteration 4729 : loss : 0.022657, loss_ce: 0.007840
2022-01-06 14:17:55,406 iteration 4730 : loss : 0.022502, loss_ce: 0.009058
2022-01-06 14:17:56,568 iteration 4731 : loss : 0.023615, loss_ce: 0.007858
2022-01-06 14:17:57,690 iteration 4732 : loss : 0.029174, loss_ce: 0.008591
2022-01-06 14:17:58,808 iteration 4733 : loss : 0.021708, loss_ce: 0.008671
2022-01-06 14:17:59,893 iteration 4734 : loss : 0.023947, loss_ce: 0.009200
2022-01-06 14:18:01,079 iteration 4735 : loss : 0.034080, loss_ce: 0.012743
2022-01-06 14:18:02,119 iteration 4736 : loss : 0.026771, loss_ce: 0.012253
2022-01-06 14:18:03,296 iteration 4737 : loss : 0.039072, loss_ce: 0.012900
2022-01-06 14:18:04,400 iteration 4738 : loss : 0.024933, loss_ce: 0.008654
2022-01-06 14:18:05,504 iteration 4739 : loss : 0.029694, loss_ce: 0.012014
2022-01-06 14:18:06,655 iteration 4740 : loss : 0.035545, loss_ce: 0.022613
2022-01-06 14:18:07,739 iteration 4741 : loss : 0.021471, loss_ce: 0.008205
2022-01-06 14:18:08,837 iteration 4742 : loss : 0.021380, loss_ce: 0.010223
2022-01-06 14:18:10,028 iteration 4743 : loss : 0.039312, loss_ce: 0.011733
 70%|████████████████████▏        | 279/400 [1:36:36<39:44, 19.70s/it]2022-01-06 14:18:11,250 iteration 4744 : loss : 0.041552, loss_ce: 0.015346
2022-01-06 14:18:12,367 iteration 4745 : loss : 0.038092, loss_ce: 0.010923
2022-01-06 14:18:13,444 iteration 4746 : loss : 0.020602, loss_ce: 0.009839
2022-01-06 14:18:14,532 iteration 4747 : loss : 0.026259, loss_ce: 0.008869
2022-01-06 14:18:15,656 iteration 4748 : loss : 0.032066, loss_ce: 0.011312
2022-01-06 14:18:16,783 iteration 4749 : loss : 0.042924, loss_ce: 0.022479
2022-01-06 14:18:17,948 iteration 4750 : loss : 0.025023, loss_ce: 0.009713
2022-01-06 14:18:19,037 iteration 4751 : loss : 0.025122, loss_ce: 0.010280
2022-01-06 14:18:20,174 iteration 4752 : loss : 0.019515, loss_ce: 0.006715
2022-01-06 14:18:21,330 iteration 4753 : loss : 0.024361, loss_ce: 0.008861
2022-01-06 14:18:22,457 iteration 4754 : loss : 0.021450, loss_ce: 0.007160
2022-01-06 14:18:23,672 iteration 4755 : loss : 0.021979, loss_ce: 0.006698
2022-01-06 14:18:24,747 iteration 4756 : loss : 0.020835, loss_ce: 0.010309
2022-01-06 14:18:25,841 iteration 4757 : loss : 0.028288, loss_ce: 0.010605
2022-01-06 14:18:26,918 iteration 4758 : loss : 0.023367, loss_ce: 0.007417
2022-01-06 14:18:28,033 iteration 4759 : loss : 0.018354, loss_ce: 0.006630
2022-01-06 14:18:28,034 Training Data Eval:
2022-01-06 14:18:33,651   Average segmentation loss on training set: 0.0201
2022-01-06 14:18:33,651 Validation Data Eval:
2022-01-06 14:18:35,564   Average segmentation loss on validation set: 0.1432
2022-01-06 14:18:36,662 iteration 4760 : loss : 0.022073, loss_ce: 0.009514
 70%|████████████████████▎        | 280/400 [1:37:03<43:33, 21.78s/it]2022-01-06 14:18:37,821 iteration 4761 : loss : 0.022301, loss_ce: 0.007063
2022-01-06 14:18:38,974 iteration 4762 : loss : 0.025879, loss_ce: 0.009145
2022-01-06 14:18:40,036 iteration 4763 : loss : 0.018305, loss_ce: 0.007579
2022-01-06 14:18:41,185 iteration 4764 : loss : 0.027585, loss_ce: 0.009351
2022-01-06 14:18:42,285 iteration 4765 : loss : 0.028590, loss_ce: 0.012259
2022-01-06 14:18:43,471 iteration 4766 : loss : 0.025305, loss_ce: 0.009053
2022-01-06 14:18:44,619 iteration 4767 : loss : 0.022000, loss_ce: 0.008573
2022-01-06 14:18:45,707 iteration 4768 : loss : 0.035389, loss_ce: 0.010398
2022-01-06 14:18:46,876 iteration 4769 : loss : 0.024241, loss_ce: 0.009097
2022-01-06 14:18:48,040 iteration 4770 : loss : 0.019322, loss_ce: 0.006959
2022-01-06 14:18:49,102 iteration 4771 : loss : 0.020440, loss_ce: 0.006835
2022-01-06 14:18:50,271 iteration 4772 : loss : 0.027526, loss_ce: 0.011333
2022-01-06 14:18:51,428 iteration 4773 : loss : 0.026404, loss_ce: 0.009766
2022-01-06 14:18:52,576 iteration 4774 : loss : 0.024842, loss_ce: 0.011089
2022-01-06 14:18:53,672 iteration 4775 : loss : 0.033817, loss_ce: 0.009979
2022-01-06 14:18:54,791 iteration 4776 : loss : 0.025695, loss_ce: 0.012408
2022-01-06 14:18:56,046 iteration 4777 : loss : 0.027252, loss_ce: 0.012677
 70%|████████████████████▎        | 281/400 [1:37:22<41:46, 21.06s/it]2022-01-06 14:18:57,167 iteration 4778 : loss : 0.015632, loss_ce: 0.004613
2022-01-06 14:18:58,268 iteration 4779 : loss : 0.027794, loss_ce: 0.009833
2022-01-06 14:18:59,317 iteration 4780 : loss : 0.016507, loss_ce: 0.005526
2022-01-06 14:19:00,409 iteration 4781 : loss : 0.027999, loss_ce: 0.009277
2022-01-06 14:19:01,571 iteration 4782 : loss : 0.047988, loss_ce: 0.021319
2022-01-06 14:19:02,682 iteration 4783 : loss : 0.019236, loss_ce: 0.008418
2022-01-06 14:19:03,737 iteration 4784 : loss : 0.016619, loss_ce: 0.007154
2022-01-06 14:19:04,917 iteration 4785 : loss : 0.023457, loss_ce: 0.008377
2022-01-06 14:19:06,049 iteration 4786 : loss : 0.034116, loss_ce: 0.012469
2022-01-06 14:19:07,187 iteration 4787 : loss : 0.026957, loss_ce: 0.009265
2022-01-06 14:19:08,330 iteration 4788 : loss : 0.023453, loss_ce: 0.008057
2022-01-06 14:19:09,377 iteration 4789 : loss : 0.020060, loss_ce: 0.009565
2022-01-06 14:19:10,576 iteration 4790 : loss : 0.033934, loss_ce: 0.010777
2022-01-06 14:19:11,659 iteration 4791 : loss : 0.024382, loss_ce: 0.011284
2022-01-06 14:19:12,814 iteration 4792 : loss : 0.027918, loss_ce: 0.008161
2022-01-06 14:19:13,892 iteration 4793 : loss : 0.017665, loss_ce: 0.006180
2022-01-06 14:19:14,935 iteration 4794 : loss : 0.022399, loss_ce: 0.006403
 70%|████████████████████▍        | 282/400 [1:37:41<40:08, 20.41s/it]2022-01-06 14:19:16,137 iteration 4795 : loss : 0.024848, loss_ce: 0.007309
2022-01-06 14:19:17,276 iteration 4796 : loss : 0.019115, loss_ce: 0.005873
2022-01-06 14:19:18,418 iteration 4797 : loss : 0.034756, loss_ce: 0.014730
2022-01-06 14:19:19,609 iteration 4798 : loss : 0.035905, loss_ce: 0.010530
2022-01-06 14:19:20,740 iteration 4799 : loss : 0.018791, loss_ce: 0.006306
2022-01-06 14:19:21,866 iteration 4800 : loss : 0.028039, loss_ce: 0.009824
2022-01-06 14:19:23,068 iteration 4801 : loss : 0.024190, loss_ce: 0.009359
2022-01-06 14:19:24,118 iteration 4802 : loss : 0.021442, loss_ce: 0.009059
2022-01-06 14:19:25,214 iteration 4803 : loss : 0.021228, loss_ce: 0.009429
2022-01-06 14:19:26,393 iteration 4804 : loss : 0.052989, loss_ce: 0.013734
2022-01-06 14:19:27,558 iteration 4805 : loss : 0.023684, loss_ce: 0.007554
2022-01-06 14:19:28,731 iteration 4806 : loss : 0.020308, loss_ce: 0.008738
2022-01-06 14:19:29,771 iteration 4807 : loss : 0.023089, loss_ce: 0.009245
2022-01-06 14:19:30,811 iteration 4808 : loss : 0.018878, loss_ce: 0.008253
2022-01-06 14:19:31,931 iteration 4809 : loss : 0.033256, loss_ce: 0.016029
2022-01-06 14:19:33,037 iteration 4810 : loss : 0.023663, loss_ce: 0.007044
2022-01-06 14:19:34,172 iteration 4811 : loss : 0.023608, loss_ce: 0.007805
 71%|████████████████████▌        | 283/400 [1:38:01<39:06, 20.06s/it]2022-01-06 14:19:35,296 iteration 4812 : loss : 0.021551, loss_ce: 0.008301
2022-01-06 14:19:36,430 iteration 4813 : loss : 0.025998, loss_ce: 0.005584
2022-01-06 14:19:37,587 iteration 4814 : loss : 0.027746, loss_ce: 0.007026
2022-01-06 14:19:38,637 iteration 4815 : loss : 0.020886, loss_ce: 0.008164
2022-01-06 14:19:39,849 iteration 4816 : loss : 0.020501, loss_ce: 0.008116
2022-01-06 14:19:40,938 iteration 4817 : loss : 0.027025, loss_ce: 0.012405
2022-01-06 14:19:42,023 iteration 4818 : loss : 0.020220, loss_ce: 0.007135
2022-01-06 14:19:43,046 iteration 4819 : loss : 0.020498, loss_ce: 0.007648
2022-01-06 14:19:44,122 iteration 4820 : loss : 0.031920, loss_ce: 0.012347
2022-01-06 14:19:45,320 iteration 4821 : loss : 0.039794, loss_ce: 0.017089
2022-01-06 14:19:46,508 iteration 4822 : loss : 0.022004, loss_ce: 0.007123
2022-01-06 14:19:47,605 iteration 4823 : loss : 0.023085, loss_ce: 0.009296
2022-01-06 14:19:48,736 iteration 4824 : loss : 0.022154, loss_ce: 0.009181
2022-01-06 14:19:49,814 iteration 4825 : loss : 0.025871, loss_ce: 0.009989
2022-01-06 14:19:50,885 iteration 4826 : loss : 0.023721, loss_ce: 0.007212
2022-01-06 14:19:51,969 iteration 4827 : loss : 0.018501, loss_ce: 0.007368
2022-01-06 14:19:53,043 iteration 4828 : loss : 0.020849, loss_ce: 0.008516
 71%|████████████████████▌        | 284/400 [1:38:19<38:05, 19.70s/it]2022-01-06 14:19:54,162 iteration 4829 : loss : 0.025460, loss_ce: 0.008196
2022-01-06 14:19:55,299 iteration 4830 : loss : 0.020426, loss_ce: 0.008138
2022-01-06 14:19:56,419 iteration 4831 : loss : 0.021918, loss_ce: 0.009055
2022-01-06 14:19:57,515 iteration 4832 : loss : 0.028558, loss_ce: 0.009005
2022-01-06 14:19:58,653 iteration 4833 : loss : 0.023144, loss_ce: 0.008043
2022-01-06 14:19:59,688 iteration 4834 : loss : 0.018350, loss_ce: 0.005919
2022-01-06 14:20:00,891 iteration 4835 : loss : 0.019552, loss_ce: 0.008204
2022-01-06 14:20:02,004 iteration 4836 : loss : 0.035286, loss_ce: 0.009928
2022-01-06 14:20:03,148 iteration 4837 : loss : 0.032003, loss_ce: 0.008234
2022-01-06 14:20:04,230 iteration 4838 : loss : 0.018465, loss_ce: 0.005507
2022-01-06 14:20:05,359 iteration 4839 : loss : 0.030924, loss_ce: 0.010742
2022-01-06 14:20:06,469 iteration 4840 : loss : 0.023153, loss_ce: 0.011572
2022-01-06 14:20:07,499 iteration 4841 : loss : 0.015441, loss_ce: 0.004919
2022-01-06 14:20:08,625 iteration 4842 : loss : 0.019213, loss_ce: 0.006657
2022-01-06 14:20:09,831 iteration 4843 : loss : 0.020313, loss_ce: 0.005803
2022-01-06 14:20:10,955 iteration 4844 : loss : 0.035463, loss_ce: 0.011595
2022-01-06 14:20:10,955 Training Data Eval:
2022-01-06 14:20:16,562   Average segmentation loss on training set: 0.0169
2022-01-06 14:20:16,562 Validation Data Eval:
2022-01-06 14:20:18,465   Average segmentation loss on validation set: 0.1076
2022-01-06 14:20:19,622 iteration 4845 : loss : 0.028665, loss_ce: 0.012529
 71%|████████████████████▋        | 285/400 [1:38:46<41:43, 21.77s/it]2022-01-06 14:20:20,765 iteration 4846 : loss : 0.020256, loss_ce: 0.007904
2022-01-06 14:20:21,856 iteration 4847 : loss : 0.023597, loss_ce: 0.008784
2022-01-06 14:20:23,025 iteration 4848 : loss : 0.028248, loss_ce: 0.014972
2022-01-06 14:20:24,169 iteration 4849 : loss : 0.023275, loss_ce: 0.006508
2022-01-06 14:20:25,259 iteration 4850 : loss : 0.023203, loss_ce: 0.006864
2022-01-06 14:20:26,483 iteration 4851 : loss : 0.016626, loss_ce: 0.005789
2022-01-06 14:20:27,581 iteration 4852 : loss : 0.020070, loss_ce: 0.006710
2022-01-06 14:20:28,696 iteration 4853 : loss : 0.026262, loss_ce: 0.012924
2022-01-06 14:20:29,816 iteration 4854 : loss : 0.027934, loss_ce: 0.010328
2022-01-06 14:20:30,898 iteration 4855 : loss : 0.031373, loss_ce: 0.008960
2022-01-06 14:20:32,106 iteration 4856 : loss : 0.032567, loss_ce: 0.012037
2022-01-06 14:20:33,188 iteration 4857 : loss : 0.016421, loss_ce: 0.006244
2022-01-06 14:20:34,378 iteration 4858 : loss : 0.021231, loss_ce: 0.008345
2022-01-06 14:20:35,431 iteration 4859 : loss : 0.017240, loss_ce: 0.007057
2022-01-06 14:20:36,584 iteration 4860 : loss : 0.025846, loss_ce: 0.011330
2022-01-06 14:20:37,660 iteration 4861 : loss : 0.024680, loss_ce: 0.008445
2022-01-06 14:20:38,806 iteration 4862 : loss : 0.022230, loss_ce: 0.011406
 72%|████████████████████▋        | 286/400 [1:39:05<39:52, 20.99s/it]2022-01-06 14:20:40,055 iteration 4863 : loss : 0.027158, loss_ce: 0.008912
2022-01-06 14:20:41,159 iteration 4864 : loss : 0.018310, loss_ce: 0.006531
2022-01-06 14:20:42,237 iteration 4865 : loss : 0.021389, loss_ce: 0.010655
2022-01-06 14:20:43,400 iteration 4866 : loss : 0.024769, loss_ce: 0.007954
2022-01-06 14:20:44,515 iteration 4867 : loss : 0.021814, loss_ce: 0.008423
2022-01-06 14:20:45,518 iteration 4868 : loss : 0.023574, loss_ce: 0.009581
2022-01-06 14:20:46,709 iteration 4869 : loss : 0.027550, loss_ce: 0.012032
2022-01-06 14:20:47,812 iteration 4870 : loss : 0.019174, loss_ce: 0.007554
2022-01-06 14:20:49,020 iteration 4871 : loss : 0.040331, loss_ce: 0.013957
2022-01-06 14:20:50,060 iteration 4872 : loss : 0.018607, loss_ce: 0.005831
2022-01-06 14:20:51,264 iteration 4873 : loss : 0.019144, loss_ce: 0.007538
2022-01-06 14:20:52,391 iteration 4874 : loss : 0.034937, loss_ce: 0.010401
2022-01-06 14:20:53,484 iteration 4875 : loss : 0.025822, loss_ce: 0.010876
2022-01-06 14:20:54,503 iteration 4876 : loss : 0.022091, loss_ce: 0.007599
2022-01-06 14:20:55,658 iteration 4877 : loss : 0.026493, loss_ce: 0.013342
2022-01-06 14:20:56,839 iteration 4878 : loss : 0.042459, loss_ce: 0.012450
2022-01-06 14:20:57,945 iteration 4879 : loss : 0.016793, loss_ce: 0.005911
 72%|████████████████████▊        | 287/400 [1:39:24<38:29, 20.43s/it]2022-01-06 14:20:59,078 iteration 4880 : loss : 0.022694, loss_ce: 0.008904
2022-01-06 14:21:00,126 iteration 4881 : loss : 0.018392, loss_ce: 0.007306
2022-01-06 14:21:01,140 iteration 4882 : loss : 0.020332, loss_ce: 0.008668
2022-01-06 14:21:02,259 iteration 4883 : loss : 0.028144, loss_ce: 0.013180
2022-01-06 14:21:03,399 iteration 4884 : loss : 0.025916, loss_ce: 0.007906
2022-01-06 14:21:04,401 iteration 4885 : loss : 0.016350, loss_ce: 0.006423
2022-01-06 14:21:05,503 iteration 4886 : loss : 0.036816, loss_ce: 0.008874
2022-01-06 14:21:06,550 iteration 4887 : loss : 0.019034, loss_ce: 0.005738
2022-01-06 14:21:07,683 iteration 4888 : loss : 0.025828, loss_ce: 0.007361
2022-01-06 14:21:08,799 iteration 4889 : loss : 0.032575, loss_ce: 0.014052
2022-01-06 14:21:09,869 iteration 4890 : loss : 0.018601, loss_ce: 0.009838
2022-01-06 14:21:10,872 iteration 4891 : loss : 0.019406, loss_ce: 0.007118
2022-01-06 14:21:12,024 iteration 4892 : loss : 0.021632, loss_ce: 0.008750
2022-01-06 14:21:13,145 iteration 4893 : loss : 0.025322, loss_ce: 0.012539
2022-01-06 14:21:14,245 iteration 4894 : loss : 0.019514, loss_ce: 0.008563
2022-01-06 14:21:15,323 iteration 4895 : loss : 0.024816, loss_ce: 0.005693
2022-01-06 14:21:16,379 iteration 4896 : loss : 0.025023, loss_ce: 0.007339
 72%|████████████████████▉        | 288/400 [1:39:43<37:01, 19.84s/it]2022-01-06 14:21:17,511 iteration 4897 : loss : 0.022882, loss_ce: 0.008309
2022-01-06 14:21:18,679 iteration 4898 : loss : 0.030566, loss_ce: 0.016174
2022-01-06 14:21:19,782 iteration 4899 : loss : 0.016526, loss_ce: 0.004430
2022-01-06 14:21:20,850 iteration 4900 : loss : 0.026331, loss_ce: 0.010277
2022-01-06 14:21:21,942 iteration 4901 : loss : 0.033692, loss_ce: 0.008939
2022-01-06 14:21:23,042 iteration 4902 : loss : 0.017928, loss_ce: 0.006509
2022-01-06 14:21:24,188 iteration 4903 : loss : 0.021085, loss_ce: 0.010165
2022-01-06 14:21:25,216 iteration 4904 : loss : 0.015967, loss_ce: 0.005474
2022-01-06 14:21:26,297 iteration 4905 : loss : 0.022350, loss_ce: 0.005898
2022-01-06 14:21:27,449 iteration 4906 : loss : 0.020837, loss_ce: 0.008101
2022-01-06 14:21:28,531 iteration 4907 : loss : 0.023905, loss_ce: 0.008622
2022-01-06 14:21:29,713 iteration 4908 : loss : 0.031056, loss_ce: 0.014348
2022-01-06 14:21:30,845 iteration 4909 : loss : 0.024296, loss_ce: 0.008048
2022-01-06 14:21:31,907 iteration 4910 : loss : 0.019644, loss_ce: 0.009539
2022-01-06 14:21:32,986 iteration 4911 : loss : 0.017702, loss_ce: 0.006910
2022-01-06 14:21:34,097 iteration 4912 : loss : 0.021551, loss_ce: 0.009257
2022-01-06 14:21:35,196 iteration 4913 : loss : 0.027962, loss_ce: 0.012182
 72%|████████████████████▉        | 289/400 [1:40:02<36:07, 19.53s/it]2022-01-06 14:21:36,402 iteration 4914 : loss : 0.024464, loss_ce: 0.008336
2022-01-06 14:21:37,469 iteration 4915 : loss : 0.020878, loss_ce: 0.007887
2022-01-06 14:21:38,539 iteration 4916 : loss : 0.019259, loss_ce: 0.005077
2022-01-06 14:21:39,639 iteration 4917 : loss : 0.019956, loss_ce: 0.006241
2022-01-06 14:21:40,762 iteration 4918 : loss : 0.026457, loss_ce: 0.009065
2022-01-06 14:21:42,010 iteration 4919 : loss : 0.030681, loss_ce: 0.014231
2022-01-06 14:21:43,095 iteration 4920 : loss : 0.016948, loss_ce: 0.006669
2022-01-06 14:21:44,154 iteration 4921 : loss : 0.017709, loss_ce: 0.008277
2022-01-06 14:21:45,186 iteration 4922 : loss : 0.021297, loss_ce: 0.005952
2022-01-06 14:21:46,329 iteration 4923 : loss : 0.038587, loss_ce: 0.015912
2022-01-06 14:21:47,494 iteration 4924 : loss : 0.026507, loss_ce: 0.015285
2022-01-06 14:21:48,612 iteration 4925 : loss : 0.024582, loss_ce: 0.008830
2022-01-06 14:21:49,766 iteration 4926 : loss : 0.031647, loss_ce: 0.015442
2022-01-06 14:21:50,850 iteration 4927 : loss : 0.020769, loss_ce: 0.006749
2022-01-06 14:21:52,003 iteration 4928 : loss : 0.031048, loss_ce: 0.010230
2022-01-06 14:21:53,186 iteration 4929 : loss : 0.024799, loss_ce: 0.010760
2022-01-06 14:21:53,186 Training Data Eval:
2022-01-06 14:21:58,748   Average segmentation loss on training set: 0.0179
2022-01-06 14:21:58,748 Validation Data Eval:
2022-01-06 14:22:00,650   Average segmentation loss on validation set: 0.0726
2022-01-06 14:22:01,771 iteration 4930 : loss : 0.021185, loss_ce: 0.007488
 72%|█████████████████████        | 290/400 [1:40:28<39:40, 21.64s/it]2022-01-06 14:22:02,926 iteration 4931 : loss : 0.035099, loss_ce: 0.017898
2022-01-06 14:22:04,056 iteration 4932 : loss : 0.022747, loss_ce: 0.011538
2022-01-06 14:22:05,116 iteration 4933 : loss : 0.021124, loss_ce: 0.010000
2022-01-06 14:22:06,290 iteration 4934 : loss : 0.027246, loss_ce: 0.009163
2022-01-06 14:22:07,399 iteration 4935 : loss : 0.022831, loss_ce: 0.007812
2022-01-06 14:22:08,532 iteration 4936 : loss : 0.026496, loss_ce: 0.010291
2022-01-06 14:22:09,651 iteration 4937 : loss : 0.021157, loss_ce: 0.008589
2022-01-06 14:22:10,808 iteration 4938 : loss : 0.034925, loss_ce: 0.012356
2022-01-06 14:22:11,964 iteration 4939 : loss : 0.029533, loss_ce: 0.011250
2022-01-06 14:22:13,050 iteration 4940 : loss : 0.028837, loss_ce: 0.012514
2022-01-06 14:22:14,164 iteration 4941 : loss : 0.022340, loss_ce: 0.010536
2022-01-06 14:22:15,286 iteration 4942 : loss : 0.021807, loss_ce: 0.006878
2022-01-06 14:22:16,486 iteration 4943 : loss : 0.038267, loss_ce: 0.008222
2022-01-06 14:22:17,591 iteration 4944 : loss : 0.026184, loss_ce: 0.009016
2022-01-06 14:22:18,734 iteration 4945 : loss : 0.027956, loss_ce: 0.012168
2022-01-06 14:22:19,895 iteration 4946 : loss : 0.025964, loss_ce: 0.006792
2022-01-06 14:22:20,987 iteration 4947 : loss : 0.029472, loss_ce: 0.009639
 73%|█████████████████████        | 291/400 [1:40:47<37:59, 20.91s/it]2022-01-06 14:22:22,180 iteration 4948 : loss : 0.022712, loss_ce: 0.008502
2022-01-06 14:22:23,272 iteration 4949 : loss : 0.026364, loss_ce: 0.010714
2022-01-06 14:22:24,437 iteration 4950 : loss : 0.027539, loss_ce: 0.010968
2022-01-06 14:22:25,582 iteration 4951 : loss : 0.024506, loss_ce: 0.008921
2022-01-06 14:22:26,718 iteration 4952 : loss : 0.018736, loss_ce: 0.009726
2022-01-06 14:22:27,882 iteration 4953 : loss : 0.020555, loss_ce: 0.008033
2022-01-06 14:22:28,985 iteration 4954 : loss : 0.019408, loss_ce: 0.006001
2022-01-06 14:22:30,051 iteration 4955 : loss : 0.030381, loss_ce: 0.008870
2022-01-06 14:22:31,165 iteration 4956 : loss : 0.035471, loss_ce: 0.010645
2022-01-06 14:22:32,205 iteration 4957 : loss : 0.019457, loss_ce: 0.007377
2022-01-06 14:22:33,342 iteration 4958 : loss : 0.024148, loss_ce: 0.011842
2022-01-06 14:22:34,478 iteration 4959 : loss : 0.023308, loss_ce: 0.008343
2022-01-06 14:22:35,535 iteration 4960 : loss : 0.020102, loss_ce: 0.007634
2022-01-06 14:22:36,657 iteration 4961 : loss : 0.020642, loss_ce: 0.007978
2022-01-06 14:22:37,719 iteration 4962 : loss : 0.024817, loss_ce: 0.005064
2022-01-06 14:22:38,807 iteration 4963 : loss : 0.020745, loss_ce: 0.008171
2022-01-06 14:22:39,969 iteration 4964 : loss : 0.032791, loss_ce: 0.011053
 73%|█████████████████████▏       | 292/400 [1:41:06<36:36, 20.34s/it]2022-01-06 14:22:41,219 iteration 4965 : loss : 0.037926, loss_ce: 0.019151
2022-01-06 14:22:42,382 iteration 4966 : loss : 0.024460, loss_ce: 0.009334
2022-01-06 14:22:43,498 iteration 4967 : loss : 0.023881, loss_ce: 0.010066
2022-01-06 14:22:44,627 iteration 4968 : loss : 0.019604, loss_ce: 0.008921
2022-01-06 14:22:45,834 iteration 4969 : loss : 0.047595, loss_ce: 0.013548
2022-01-06 14:22:46,894 iteration 4970 : loss : 0.021307, loss_ce: 0.009045
2022-01-06 14:22:47,975 iteration 4971 : loss : 0.023739, loss_ce: 0.011720
2022-01-06 14:22:49,112 iteration 4972 : loss : 0.040511, loss_ce: 0.017406
2022-01-06 14:22:50,222 iteration 4973 : loss : 0.019842, loss_ce: 0.007962
2022-01-06 14:22:51,450 iteration 4974 : loss : 0.031433, loss_ce: 0.011413
2022-01-06 14:22:52,546 iteration 4975 : loss : 0.017492, loss_ce: 0.004717
2022-01-06 14:22:53,601 iteration 4976 : loss : 0.020700, loss_ce: 0.007705
2022-01-06 14:22:54,757 iteration 4977 : loss : 0.031848, loss_ce: 0.007078
2022-01-06 14:22:55,864 iteration 4978 : loss : 0.019586, loss_ce: 0.004781
2022-01-06 14:22:56,988 iteration 4979 : loss : 0.020108, loss_ce: 0.007191
2022-01-06 14:22:58,123 iteration 4980 : loss : 0.020856, loss_ce: 0.005834
2022-01-06 14:22:59,254 iteration 4981 : loss : 0.019685, loss_ce: 0.006319
 73%|█████████████████████▏       | 293/400 [1:41:26<35:42, 20.02s/it]2022-01-06 14:23:00,423 iteration 4982 : loss : 0.018757, loss_ce: 0.008354
2022-01-06 14:23:01,523 iteration 4983 : loss : 0.019340, loss_ce: 0.007620
2022-01-06 14:23:02,733 iteration 4984 : loss : 0.024906, loss_ce: 0.009338
2022-01-06 14:23:03,777 iteration 4985 : loss : 0.023130, loss_ce: 0.007884
2022-01-06 14:23:04,967 iteration 4986 : loss : 0.025519, loss_ce: 0.010810
2022-01-06 14:23:06,023 iteration 4987 : loss : 0.019966, loss_ce: 0.006131
2022-01-06 14:23:07,145 iteration 4988 : loss : 0.021690, loss_ce: 0.006367
2022-01-06 14:23:08,299 iteration 4989 : loss : 0.023217, loss_ce: 0.008053
2022-01-06 14:23:09,480 iteration 4990 : loss : 0.021713, loss_ce: 0.009799
2022-01-06 14:23:10,599 iteration 4991 : loss : 0.025459, loss_ce: 0.008462
2022-01-06 14:23:11,628 iteration 4992 : loss : 0.018440, loss_ce: 0.009133
2022-01-06 14:23:12,742 iteration 4993 : loss : 0.025106, loss_ce: 0.008954
2022-01-06 14:23:13,836 iteration 4994 : loss : 0.026182, loss_ce: 0.007499
2022-01-06 14:23:14,956 iteration 4995 : loss : 0.020886, loss_ce: 0.008854
2022-01-06 14:23:16,162 iteration 4996 : loss : 0.025062, loss_ce: 0.007084
2022-01-06 14:23:17,230 iteration 4997 : loss : 0.018501, loss_ce: 0.005206
2022-01-06 14:23:18,315 iteration 4998 : loss : 0.020712, loss_ce: 0.007544
 74%|█████████████████████▎       | 294/400 [1:41:45<34:51, 19.73s/it]2022-01-06 14:23:19,464 iteration 4999 : loss : 0.019517, loss_ce: 0.006806
2022-01-06 14:23:20,568 iteration 5000 : loss : 0.017734, loss_ce: 0.007031
2022-01-06 14:23:21,670 iteration 5001 : loss : 0.019821, loss_ce: 0.008152
2022-01-06 14:23:22,775 iteration 5002 : loss : 0.014714, loss_ce: 0.004753
2022-01-06 14:23:23,923 iteration 5003 : loss : 0.032804, loss_ce: 0.007027
2022-01-06 14:23:25,048 iteration 5004 : loss : 0.022719, loss_ce: 0.008231
2022-01-06 14:23:26,182 iteration 5005 : loss : 0.023899, loss_ce: 0.010806
2022-01-06 14:23:27,376 iteration 5006 : loss : 0.021369, loss_ce: 0.008789
2022-01-06 14:23:28,555 iteration 5007 : loss : 0.030327, loss_ce: 0.013573
2022-01-06 14:23:29,738 iteration 5008 : loss : 0.027928, loss_ce: 0.013216
2022-01-06 14:23:30,905 iteration 5009 : loss : 0.022877, loss_ce: 0.006740
2022-01-06 14:23:32,046 iteration 5010 : loss : 0.025256, loss_ce: 0.009192
2022-01-06 14:23:33,140 iteration 5011 : loss : 0.020152, loss_ce: 0.009219
2022-01-06 14:23:34,269 iteration 5012 : loss : 0.032028, loss_ce: 0.013033
2022-01-06 14:23:35,335 iteration 5013 : loss : 0.022360, loss_ce: 0.008173
2022-01-06 14:23:36,505 iteration 5014 : loss : 0.023276, loss_ce: 0.008685
2022-01-06 14:23:36,505 Training Data Eval:
2022-01-06 14:23:42,026   Average segmentation loss on training set: 0.0158
2022-01-06 14:23:42,026 Validation Data Eval:
2022-01-06 14:23:43,916   Average segmentation loss on validation set: 0.0976
2022-01-06 14:23:45,030 iteration 5015 : loss : 0.020919, loss_ce: 0.006862
 74%|█████████████████████▍       | 295/400 [1:42:11<38:11, 21.82s/it]2022-01-06 14:23:46,202 iteration 5016 : loss : 0.021774, loss_ce: 0.010246
2022-01-06 14:23:47,296 iteration 5017 : loss : 0.017977, loss_ce: 0.007079
2022-01-06 14:23:48,401 iteration 5018 : loss : 0.022500, loss_ce: 0.009730
2022-01-06 14:23:49,528 iteration 5019 : loss : 0.018546, loss_ce: 0.005920
2022-01-06 14:23:50,665 iteration 5020 : loss : 0.019911, loss_ce: 0.005500
2022-01-06 14:23:51,825 iteration 5021 : loss : 0.021562, loss_ce: 0.010109
2022-01-06 14:23:52,895 iteration 5022 : loss : 0.018024, loss_ce: 0.005756
2022-01-06 14:23:54,024 iteration 5023 : loss : 0.034072, loss_ce: 0.010039
2022-01-06 14:23:55,249 iteration 5024 : loss : 0.033202, loss_ce: 0.006968
2022-01-06 14:23:56,446 iteration 5025 : loss : 0.030495, loss_ce: 0.005786
2022-01-06 14:23:57,593 iteration 5026 : loss : 0.029388, loss_ce: 0.010572
2022-01-06 14:23:58,694 iteration 5027 : loss : 0.020543, loss_ce: 0.008851
2022-01-06 14:23:59,804 iteration 5028 : loss : 0.027024, loss_ce: 0.010132
2022-01-06 14:24:01,001 iteration 5029 : loss : 0.040139, loss_ce: 0.008520
2022-01-06 14:24:02,116 iteration 5030 : loss : 0.020942, loss_ce: 0.009916
2022-01-06 14:24:03,291 iteration 5031 : loss : 0.025013, loss_ce: 0.010620
2022-01-06 14:24:04,410 iteration 5032 : loss : 0.020751, loss_ce: 0.010262
 74%|█████████████████████▍       | 296/400 [1:42:31<36:33, 21.09s/it]2022-01-06 14:24:05,694 iteration 5033 : loss : 0.025281, loss_ce: 0.009419
2022-01-06 14:24:06,787 iteration 5034 : loss : 0.053103, loss_ce: 0.014429
2022-01-06 14:24:07,867 iteration 5035 : loss : 0.021967, loss_ce: 0.009023
2022-01-06 14:24:08,953 iteration 5036 : loss : 0.026407, loss_ce: 0.009437
2022-01-06 14:24:10,076 iteration 5037 : loss : 0.019225, loss_ce: 0.006732
2022-01-06 14:24:11,169 iteration 5038 : loss : 0.037856, loss_ce: 0.009473
2022-01-06 14:24:12,375 iteration 5039 : loss : 0.025580, loss_ce: 0.012086
2022-01-06 14:24:13,494 iteration 5040 : loss : 0.021128, loss_ce: 0.006370
2022-01-06 14:24:14,666 iteration 5041 : loss : 0.024135, loss_ce: 0.009563
2022-01-06 14:24:15,740 iteration 5042 : loss : 0.025233, loss_ce: 0.011019
2022-01-06 14:24:16,844 iteration 5043 : loss : 0.019817, loss_ce: 0.009009
2022-01-06 14:24:17,956 iteration 5044 : loss : 0.024347, loss_ce: 0.010449
2022-01-06 14:24:19,107 iteration 5045 : loss : 0.027294, loss_ce: 0.011878
2022-01-06 14:24:20,232 iteration 5046 : loss : 0.029920, loss_ce: 0.008229
2022-01-06 14:24:21,321 iteration 5047 : loss : 0.027392, loss_ce: 0.007551
2022-01-06 14:24:22,404 iteration 5048 : loss : 0.023185, loss_ce: 0.008930
2022-01-06 14:24:23,488 iteration 5049 : loss : 0.019866, loss_ce: 0.008251
 74%|█████████████████████▌       | 297/400 [1:42:50<35:10, 20.49s/it]2022-01-06 14:24:24,661 iteration 5050 : loss : 0.021759, loss_ce: 0.006524
2022-01-06 14:24:25,756 iteration 5051 : loss : 0.024569, loss_ce: 0.009106
2022-01-06 14:24:26,860 iteration 5052 : loss : 0.020074, loss_ce: 0.008733
2022-01-06 14:24:27,951 iteration 5053 : loss : 0.023623, loss_ce: 0.008458
2022-01-06 14:24:29,057 iteration 5054 : loss : 0.019459, loss_ce: 0.006010
2022-01-06 14:24:30,146 iteration 5055 : loss : 0.024929, loss_ce: 0.007812
2022-01-06 14:24:31,231 iteration 5056 : loss : 0.029966, loss_ce: 0.007091
2022-01-06 14:24:32,389 iteration 5057 : loss : 0.018965, loss_ce: 0.008773
2022-01-06 14:24:33,557 iteration 5058 : loss : 0.033598, loss_ce: 0.014414
2022-01-06 14:24:34,552 iteration 5059 : loss : 0.020796, loss_ce: 0.005162
2022-01-06 14:24:35,654 iteration 5060 : loss : 0.035617, loss_ce: 0.009820
2022-01-06 14:24:36,788 iteration 5061 : loss : 0.019026, loss_ce: 0.007386
2022-01-06 14:24:37,946 iteration 5062 : loss : 0.030854, loss_ce: 0.010134
2022-01-06 14:24:39,117 iteration 5063 : loss : 0.024199, loss_ce: 0.009188
2022-01-06 14:24:40,190 iteration 5064 : loss : 0.025224, loss_ce: 0.009148
2022-01-06 14:24:41,268 iteration 5065 : loss : 0.029474, loss_ce: 0.010367
2022-01-06 14:24:42,378 iteration 5066 : loss : 0.016953, loss_ce: 0.007848
 74%|█████████████████████▌       | 298/400 [1:43:09<34:00, 20.01s/it]2022-01-06 14:24:43,629 iteration 5067 : loss : 0.021786, loss_ce: 0.008241
2022-01-06 14:24:44,762 iteration 5068 : loss : 0.020221, loss_ce: 0.008256
2022-01-06 14:24:45,887 iteration 5069 : loss : 0.023012, loss_ce: 0.007539
2022-01-06 14:24:46,985 iteration 5070 : loss : 0.019394, loss_ce: 0.005266
2022-01-06 14:24:48,159 iteration 5071 : loss : 0.022720, loss_ce: 0.008324
2022-01-06 14:24:49,171 iteration 5072 : loss : 0.018916, loss_ce: 0.006494
2022-01-06 14:24:50,295 iteration 5073 : loss : 0.020939, loss_ce: 0.010792
2022-01-06 14:24:51,465 iteration 5074 : loss : 0.025930, loss_ce: 0.007385
2022-01-06 14:24:52,593 iteration 5075 : loss : 0.023351, loss_ce: 0.011222
2022-01-06 14:24:53,771 iteration 5076 : loss : 0.019505, loss_ce: 0.007875
2022-01-06 14:24:54,863 iteration 5077 : loss : 0.025560, loss_ce: 0.009287
2022-01-06 14:24:55,994 iteration 5078 : loss : 0.018533, loss_ce: 0.005371
2022-01-06 14:24:57,087 iteration 5079 : loss : 0.019579, loss_ce: 0.008042
2022-01-06 14:24:58,212 iteration 5080 : loss : 0.027787, loss_ce: 0.010310
2022-01-06 14:24:59,358 iteration 5081 : loss : 0.018586, loss_ce: 0.007673
2022-01-06 14:25:00,522 iteration 5082 : loss : 0.041916, loss_ce: 0.020689
2022-01-06 14:25:01,653 iteration 5083 : loss : 0.020766, loss_ce: 0.006588
 75%|█████████████████████▋       | 299/400 [1:43:28<33:18, 19.79s/it]2022-01-06 14:25:02,800 iteration 5084 : loss : 0.021680, loss_ce: 0.007607
2022-01-06 14:25:03,905 iteration 5085 : loss : 0.023009, loss_ce: 0.008022
2022-01-06 14:25:04,988 iteration 5086 : loss : 0.017098, loss_ce: 0.006145
2022-01-06 14:25:06,153 iteration 5087 : loss : 0.038746, loss_ce: 0.017902
2022-01-06 14:25:07,290 iteration 5088 : loss : 0.024631, loss_ce: 0.008388
2022-01-06 14:25:08,366 iteration 5089 : loss : 0.020478, loss_ce: 0.006700
2022-01-06 14:25:09,453 iteration 5090 : loss : 0.019559, loss_ce: 0.006764
2022-01-06 14:25:10,662 iteration 5091 : loss : 0.031916, loss_ce: 0.012256
2022-01-06 14:25:11,783 iteration 5092 : loss : 0.016328, loss_ce: 0.007489
2022-01-06 14:25:12,866 iteration 5093 : loss : 0.024552, loss_ce: 0.011185
2022-01-06 14:25:13,928 iteration 5094 : loss : 0.018892, loss_ce: 0.006313
2022-01-06 14:25:15,093 iteration 5095 : loss : 0.031009, loss_ce: 0.012352
2022-01-06 14:25:16,168 iteration 5096 : loss : 0.017913, loss_ce: 0.007697
2022-01-06 14:25:17,253 iteration 5097 : loss : 0.017605, loss_ce: 0.007939
2022-01-06 14:25:18,390 iteration 5098 : loss : 0.028772, loss_ce: 0.010015
2022-01-06 14:25:19,498 iteration 5099 : loss : 0.019400, loss_ce: 0.006008
2022-01-06 14:25:19,498 Training Data Eval:
2022-01-06 14:25:25,001   Average segmentation loss on training set: 0.0178
2022-01-06 14:25:25,001 Validation Data Eval:
2022-01-06 14:25:26,888   Average segmentation loss on validation set: 0.0830
2022-01-06 14:25:28,000 iteration 5100 : loss : 0.017852, loss_ce: 0.008086
 75%|█████████████████████▊       | 300/400 [1:43:54<36:15, 21.76s/it]2022-01-06 14:25:29,187 iteration 5101 : loss : 0.021565, loss_ce: 0.009370
2022-01-06 14:25:30,264 iteration 5102 : loss : 0.014187, loss_ce: 0.005207
2022-01-06 14:25:31,358 iteration 5103 : loss : 0.024041, loss_ce: 0.009614
2022-01-06 14:25:32,532 iteration 5104 : loss : 0.018792, loss_ce: 0.007713
2022-01-06 14:25:33,583 iteration 5105 : loss : 0.021698, loss_ce: 0.007994
2022-01-06 14:25:34,756 iteration 5106 : loss : 0.022111, loss_ce: 0.008069
2022-01-06 14:25:35,812 iteration 5107 : loss : 0.020336, loss_ce: 0.008314
2022-01-06 14:25:36,947 iteration 5108 : loss : 0.027198, loss_ce: 0.009065
2022-01-06 14:25:38,059 iteration 5109 : loss : 0.028193, loss_ce: 0.009173
2022-01-06 14:25:39,162 iteration 5110 : loss : 0.024599, loss_ce: 0.011790
2022-01-06 14:25:40,302 iteration 5111 : loss : 0.023102, loss_ce: 0.007663
2022-01-06 14:25:41,365 iteration 5112 : loss : 0.028184, loss_ce: 0.009171
2022-01-06 14:25:42,542 iteration 5113 : loss : 0.021589, loss_ce: 0.004691
2022-01-06 14:25:43,639 iteration 5114 : loss : 0.024550, loss_ce: 0.012248
2022-01-06 14:25:44,714 iteration 5115 : loss : 0.025000, loss_ce: 0.006839
2022-01-06 14:25:45,795 iteration 5116 : loss : 0.017934, loss_ce: 0.007030
2022-01-06 14:25:46,961 iteration 5117 : loss : 0.022599, loss_ce: 0.010374
 75%|█████████████████████▊       | 301/400 [1:44:13<34:30, 20.92s/it]2022-01-06 14:25:48,136 iteration 5118 : loss : 0.015677, loss_ce: 0.005789
2022-01-06 14:25:49,261 iteration 5119 : loss : 0.024659, loss_ce: 0.009196
2022-01-06 14:25:50,339 iteration 5120 : loss : 0.030431, loss_ce: 0.007928
2022-01-06 14:25:51,444 iteration 5121 : loss : 0.024816, loss_ce: 0.012196
2022-01-06 14:25:52,514 iteration 5122 : loss : 0.032308, loss_ce: 0.012671
2022-01-06 14:25:53,618 iteration 5123 : loss : 0.038165, loss_ce: 0.010052
2022-01-06 14:25:54,755 iteration 5124 : loss : 0.021118, loss_ce: 0.009257
2022-01-06 14:25:55,779 iteration 5125 : loss : 0.021092, loss_ce: 0.010234
2022-01-06 14:25:56,843 iteration 5126 : loss : 0.024220, loss_ce: 0.008321
2022-01-06 14:25:57,996 iteration 5127 : loss : 0.028273, loss_ce: 0.008843
2022-01-06 14:25:59,087 iteration 5128 : loss : 0.015029, loss_ce: 0.005913
2022-01-06 14:26:00,212 iteration 5129 : loss : 0.022349, loss_ce: 0.007563
2022-01-06 14:26:01,324 iteration 5130 : loss : 0.020939, loss_ce: 0.006304
2022-01-06 14:26:02,356 iteration 5131 : loss : 0.014362, loss_ce: 0.005246
2022-01-06 14:26:03,459 iteration 5132 : loss : 0.021594, loss_ce: 0.009316
2022-01-06 14:26:04,531 iteration 5133 : loss : 0.031122, loss_ce: 0.008304
2022-01-06 14:26:05,593 iteration 5134 : loss : 0.020591, loss_ce: 0.008684
 76%|█████████████████████▉       | 302/400 [1:44:32<33:02, 20.23s/it]2022-01-06 14:26:06,902 iteration 5135 : loss : 0.027358, loss_ce: 0.010400
2022-01-06 14:26:08,004 iteration 5136 : loss : 0.031446, loss_ce: 0.010619
2022-01-06 14:26:09,154 iteration 5137 : loss : 0.022291, loss_ce: 0.006996
2022-01-06 14:26:10,219 iteration 5138 : loss : 0.021061, loss_ce: 0.008810
2022-01-06 14:26:11,324 iteration 5139 : loss : 0.017674, loss_ce: 0.006313
2022-01-06 14:26:12,512 iteration 5140 : loss : 0.017304, loss_ce: 0.007011
2022-01-06 14:26:13,578 iteration 5141 : loss : 0.022176, loss_ce: 0.007385
2022-01-06 14:26:14,694 iteration 5142 : loss : 0.027153, loss_ce: 0.009519
2022-01-06 14:26:15,728 iteration 5143 : loss : 0.020783, loss_ce: 0.008118
2022-01-06 14:26:16,864 iteration 5144 : loss : 0.027896, loss_ce: 0.013430
2022-01-06 14:26:17,885 iteration 5145 : loss : 0.015960, loss_ce: 0.005279
2022-01-06 14:26:19,054 iteration 5146 : loss : 0.027301, loss_ce: 0.012747
2022-01-06 14:26:20,219 iteration 5147 : loss : 0.023050, loss_ce: 0.008119
2022-01-06 14:26:21,304 iteration 5148 : loss : 0.023073, loss_ce: 0.010171
2022-01-06 14:26:22,389 iteration 5149 : loss : 0.023990, loss_ce: 0.006310
2022-01-06 14:26:23,468 iteration 5150 : loss : 0.013794, loss_ce: 0.005324
2022-01-06 14:26:24,577 iteration 5151 : loss : 0.031658, loss_ce: 0.006052
 76%|█████████████████████▉       | 303/400 [1:44:51<32:06, 19.86s/it]2022-01-06 14:26:25,698 iteration 5152 : loss : 0.028736, loss_ce: 0.004870
2022-01-06 14:26:26,712 iteration 5153 : loss : 0.016818, loss_ce: 0.006545
2022-01-06 14:26:27,875 iteration 5154 : loss : 0.024696, loss_ce: 0.009742
2022-01-06 14:26:28,915 iteration 5155 : loss : 0.017087, loss_ce: 0.005633
2022-01-06 14:26:30,002 iteration 5156 : loss : 0.028367, loss_ce: 0.011431
2022-01-06 14:26:31,049 iteration 5157 : loss : 0.018105, loss_ce: 0.005416
2022-01-06 14:26:32,133 iteration 5158 : loss : 0.020284, loss_ce: 0.006536
2022-01-06 14:26:33,295 iteration 5159 : loss : 0.024063, loss_ce: 0.007798
2022-01-06 14:26:34,342 iteration 5160 : loss : 0.019383, loss_ce: 0.006528
2022-01-06 14:26:35,434 iteration 5161 : loss : 0.024286, loss_ce: 0.008993
2022-01-06 14:26:36,485 iteration 5162 : loss : 0.019291, loss_ce: 0.008421
2022-01-06 14:26:37,597 iteration 5163 : loss : 0.018687, loss_ce: 0.007472
2022-01-06 14:26:38,710 iteration 5164 : loss : 0.019790, loss_ce: 0.009497
2022-01-06 14:26:39,742 iteration 5165 : loss : 0.024621, loss_ce: 0.008215
2022-01-06 14:26:40,837 iteration 5166 : loss : 0.021783, loss_ce: 0.009580
2022-01-06 14:26:41,945 iteration 5167 : loss : 0.027417, loss_ce: 0.009272
2022-01-06 14:26:43,095 iteration 5168 : loss : 0.023642, loss_ce: 0.008958
 76%|██████████████████████       | 304/400 [1:45:10<31:07, 19.46s/it]2022-01-06 14:26:44,356 iteration 5169 : loss : 0.024122, loss_ce: 0.010727
2022-01-06 14:26:45,406 iteration 5170 : loss : 0.031443, loss_ce: 0.010105
2022-01-06 14:26:46,562 iteration 5171 : loss : 0.023342, loss_ce: 0.011605
2022-01-06 14:26:47,590 iteration 5172 : loss : 0.018470, loss_ce: 0.005442
2022-01-06 14:26:48,564 iteration 5173 : loss : 0.015758, loss_ce: 0.006762
2022-01-06 14:26:49,730 iteration 5174 : loss : 0.027533, loss_ce: 0.011776
2022-01-06 14:26:50,795 iteration 5175 : loss : 0.023749, loss_ce: 0.011055
2022-01-06 14:26:52,022 iteration 5176 : loss : 0.032504, loss_ce: 0.014478
2022-01-06 14:26:53,088 iteration 5177 : loss : 0.025509, loss_ce: 0.009776
2022-01-06 14:26:54,189 iteration 5178 : loss : 0.016610, loss_ce: 0.004648
2022-01-06 14:26:55,352 iteration 5179 : loss : 0.016711, loss_ce: 0.004748
2022-01-06 14:26:56,459 iteration 5180 : loss : 0.022347, loss_ce: 0.007607
2022-01-06 14:26:57,520 iteration 5181 : loss : 0.017530, loss_ce: 0.004210
2022-01-06 14:26:58,616 iteration 5182 : loss : 0.033385, loss_ce: 0.014978
2022-01-06 14:26:59,699 iteration 5183 : loss : 0.023160, loss_ce: 0.010254
2022-01-06 14:27:00,767 iteration 5184 : loss : 0.023378, loss_ce: 0.008284
2022-01-06 14:27:00,767 Training Data Eval:
2022-01-06 14:27:06,332   Average segmentation loss on training set: 0.0164
2022-01-06 14:27:06,332 Validation Data Eval:
2022-01-06 14:27:08,243   Average segmentation loss on validation set: 0.0795
2022-01-06 14:27:09,358 iteration 5185 : loss : 0.022799, loss_ce: 0.009291
 76%|██████████████████████       | 305/400 [1:45:36<34:02, 21.50s/it]2022-01-06 14:27:10,561 iteration 5186 : loss : 0.021112, loss_ce: 0.011525
2022-01-06 14:27:11,621 iteration 5187 : loss : 0.021775, loss_ce: 0.008053
2022-01-06 14:27:12,691 iteration 5188 : loss : 0.019061, loss_ce: 0.008217
2022-01-06 14:27:13,764 iteration 5189 : loss : 0.021609, loss_ce: 0.011323
2022-01-06 14:27:14,824 iteration 5190 : loss : 0.019576, loss_ce: 0.008746
2022-01-06 14:27:15,983 iteration 5191 : loss : 0.023706, loss_ce: 0.008102
2022-01-06 14:27:17,134 iteration 5192 : loss : 0.026918, loss_ce: 0.009064
2022-01-06 14:27:18,208 iteration 5193 : loss : 0.014498, loss_ce: 0.004908
2022-01-06 14:27:19,354 iteration 5194 : loss : 0.022868, loss_ce: 0.009156
2022-01-06 14:27:20,448 iteration 5195 : loss : 0.022987, loss_ce: 0.008223
2022-01-06 14:27:21,491 iteration 5196 : loss : 0.013931, loss_ce: 0.003725
2022-01-06 14:27:22,573 iteration 5197 : loss : 0.020586, loss_ce: 0.006724
2022-01-06 14:27:23,677 iteration 5198 : loss : 0.019476, loss_ce: 0.008740
2022-01-06 14:27:24,836 iteration 5199 : loss : 0.016609, loss_ce: 0.005527
2022-01-06 14:27:25,967 iteration 5200 : loss : 0.030079, loss_ce: 0.010484
2022-01-06 14:27:27,111 iteration 5201 : loss : 0.019751, loss_ce: 0.006583
2022-01-06 14:27:28,319 iteration 5202 : loss : 0.029244, loss_ce: 0.013245
 76%|██████████████████████▏      | 306/400 [1:45:55<32:29, 20.74s/it]2022-01-06 14:27:29,466 iteration 5203 : loss : 0.017164, loss_ce: 0.005885
2022-01-06 14:27:30,561 iteration 5204 : loss : 0.018820, loss_ce: 0.007470
2022-01-06 14:27:31,801 iteration 5205 : loss : 0.033536, loss_ce: 0.011966
2022-01-06 14:27:32,910 iteration 5206 : loss : 0.026577, loss_ce: 0.012100
2022-01-06 14:27:34,025 iteration 5207 : loss : 0.027724, loss_ce: 0.010458
2022-01-06 14:27:35,208 iteration 5208 : loss : 0.021715, loss_ce: 0.009631
2022-01-06 14:27:36,277 iteration 5209 : loss : 0.021531, loss_ce: 0.008787
2022-01-06 14:27:37,426 iteration 5210 : loss : 0.019794, loss_ce: 0.007316
2022-01-06 14:27:38,548 iteration 5211 : loss : 0.021752, loss_ce: 0.008732
2022-01-06 14:27:39,658 iteration 5212 : loss : 0.027394, loss_ce: 0.009876
2022-01-06 14:27:40,666 iteration 5213 : loss : 0.017507, loss_ce: 0.006537
2022-01-06 14:27:41,862 iteration 5214 : loss : 0.028266, loss_ce: 0.012941
2022-01-06 14:27:42,995 iteration 5215 : loss : 0.021098, loss_ce: 0.007019
2022-01-06 14:27:44,148 iteration 5216 : loss : 0.021649, loss_ce: 0.006813
2022-01-06 14:27:45,335 iteration 5217 : loss : 0.021198, loss_ce: 0.007582
2022-01-06 14:27:46,400 iteration 5218 : loss : 0.026450, loss_ce: 0.003763
2022-01-06 14:27:47,594 iteration 5219 : loss : 0.032648, loss_ce: 0.009870
 77%|██████████████████████▎      | 307/400 [1:46:14<31:27, 20.30s/it]2022-01-06 14:27:48,734 iteration 5220 : loss : 0.027446, loss_ce: 0.007425
2022-01-06 14:27:49,818 iteration 5221 : loss : 0.019245, loss_ce: 0.006117
2022-01-06 14:27:50,859 iteration 5222 : loss : 0.016510, loss_ce: 0.006390
2022-01-06 14:27:52,001 iteration 5223 : loss : 0.020673, loss_ce: 0.007533
2022-01-06 14:27:53,090 iteration 5224 : loss : 0.021204, loss_ce: 0.007081
2022-01-06 14:27:54,248 iteration 5225 : loss : 0.025438, loss_ce: 0.009365
2022-01-06 14:27:55,335 iteration 5226 : loss : 0.023100, loss_ce: 0.008855
2022-01-06 14:27:56,518 iteration 5227 : loss : 0.023822, loss_ce: 0.009011
2022-01-06 14:27:57,580 iteration 5228 : loss : 0.020446, loss_ce: 0.006312
2022-01-06 14:27:58,796 iteration 5229 : loss : 0.022724, loss_ce: 0.009583
2022-01-06 14:27:59,794 iteration 5230 : loss : 0.014781, loss_ce: 0.006322
2022-01-06 14:28:00,914 iteration 5231 : loss : 0.026485, loss_ce: 0.010824
2022-01-06 14:28:02,020 iteration 5232 : loss : 0.021822, loss_ce: 0.007093
2022-01-06 14:28:03,160 iteration 5233 : loss : 0.020197, loss_ce: 0.008722
2022-01-06 14:28:04,239 iteration 5234 : loss : 0.020759, loss_ce: 0.009323
2022-01-06 14:28:05,351 iteration 5235 : loss : 0.023336, loss_ce: 0.007832
2022-01-06 14:28:06,497 iteration 5236 : loss : 0.030852, loss_ce: 0.009765
 77%|██████████████████████▎      | 308/400 [1:46:33<30:28, 19.88s/it]2022-01-06 14:28:07,542 iteration 5237 : loss : 0.014878, loss_ce: 0.006049
2022-01-06 14:28:08,680 iteration 5238 : loss : 0.019453, loss_ce: 0.008448
2022-01-06 14:28:09,801 iteration 5239 : loss : 0.021661, loss_ce: 0.007501
2022-01-06 14:28:10,950 iteration 5240 : loss : 0.175355, loss_ce: 0.005614
2022-01-06 14:28:12,017 iteration 5241 : loss : 0.020003, loss_ce: 0.008666
2022-01-06 14:28:13,117 iteration 5242 : loss : 0.024042, loss_ce: 0.010073
2022-01-06 14:28:14,300 iteration 5243 : loss : 0.027041, loss_ce: 0.009066
2022-01-06 14:28:15,441 iteration 5244 : loss : 0.017068, loss_ce: 0.006593
2022-01-06 14:28:16,560 iteration 5245 : loss : 0.021115, loss_ce: 0.007241
2022-01-06 14:28:17,706 iteration 5246 : loss : 0.037245, loss_ce: 0.011752
2022-01-06 14:28:18,758 iteration 5247 : loss : 0.015865, loss_ce: 0.006127
2022-01-06 14:28:19,924 iteration 5248 : loss : 0.021162, loss_ce: 0.006783
2022-01-06 14:28:21,128 iteration 5249 : loss : 0.023823, loss_ce: 0.008101
2022-01-06 14:28:22,277 iteration 5250 : loss : 0.033946, loss_ce: 0.017634
2022-01-06 14:28:23,316 iteration 5251 : loss : 0.015906, loss_ce: 0.005315
2022-01-06 14:28:24,429 iteration 5252 : loss : 0.018930, loss_ce: 0.008092
2022-01-06 14:28:25,570 iteration 5253 : loss : 0.026302, loss_ce: 0.006706
 77%|██████████████████████▍      | 309/400 [1:46:52<29:47, 19.64s/it]2022-01-06 14:28:26,730 iteration 5254 : loss : 0.016630, loss_ce: 0.006064
2022-01-06 14:28:27,826 iteration 5255 : loss : 0.021522, loss_ce: 0.011684
2022-01-06 14:28:28,870 iteration 5256 : loss : 0.018242, loss_ce: 0.007343
2022-01-06 14:28:29,981 iteration 5257 : loss : 0.022163, loss_ce: 0.008264
2022-01-06 14:28:31,121 iteration 5258 : loss : 0.019972, loss_ce: 0.006735
2022-01-06 14:28:32,171 iteration 5259 : loss : 0.026666, loss_ce: 0.006514
2022-01-06 14:28:33,287 iteration 5260 : loss : 0.018643, loss_ce: 0.006635
2022-01-06 14:28:34,411 iteration 5261 : loss : 0.036058, loss_ce: 0.012402
2022-01-06 14:28:35,518 iteration 5262 : loss : 0.014529, loss_ce: 0.006268
2022-01-06 14:28:36,629 iteration 5263 : loss : 0.023618, loss_ce: 0.009584
2022-01-06 14:28:37,682 iteration 5264 : loss : 0.016185, loss_ce: 0.006394
2022-01-06 14:28:38,833 iteration 5265 : loss : 0.029699, loss_ce: 0.009726
2022-01-06 14:28:40,027 iteration 5266 : loss : 0.026955, loss_ce: 0.008660
2022-01-06 14:28:41,134 iteration 5267 : loss : 0.026841, loss_ce: 0.008720
2022-01-06 14:28:42,261 iteration 5268 : loss : 0.026794, loss_ce: 0.006542
2022-01-06 14:28:43,476 iteration 5269 : loss : 0.041357, loss_ce: 0.009927
2022-01-06 14:28:43,477 Training Data Eval:
2022-01-06 14:28:49,008   Average segmentation loss on training set: 0.0141
2022-01-06 14:28:49,009 Validation Data Eval:
2022-01-06 14:28:50,953   Average segmentation loss on validation set: 0.0748
2022-01-06 14:28:52,103 iteration 5270 : loss : 0.019213, loss_ce: 0.008198
 78%|██████████████████████▍      | 310/400 [1:47:19<32:33, 21.70s/it]2022-01-06 14:28:53,263 iteration 5271 : loss : 0.019876, loss_ce: 0.007226
2022-01-06 14:28:54,427 iteration 5272 : loss : 0.023047, loss_ce: 0.010092
2022-01-06 14:28:55,532 iteration 5273 : loss : 0.024388, loss_ce: 0.006176
2022-01-06 14:28:56,644 iteration 5274 : loss : 0.019583, loss_ce: 0.007684
2022-01-06 14:28:57,700 iteration 5275 : loss : 0.018094, loss_ce: 0.007692
2022-01-06 14:28:58,857 iteration 5276 : loss : 0.025476, loss_ce: 0.009576
2022-01-06 14:28:59,978 iteration 5277 : loss : 0.020025, loss_ce: 0.007211
2022-01-06 14:29:01,006 iteration 5278 : loss : 0.017830, loss_ce: 0.007247
2022-01-06 14:29:02,118 iteration 5279 : loss : 0.017087, loss_ce: 0.008059
2022-01-06 14:29:03,359 iteration 5280 : loss : 0.035863, loss_ce: 0.013322
2022-01-06 14:29:04,462 iteration 5281 : loss : 0.017580, loss_ce: 0.005974
2022-01-06 14:29:05,525 iteration 5282 : loss : 0.015948, loss_ce: 0.004771
2022-01-06 14:29:06,668 iteration 5283 : loss : 0.020707, loss_ce: 0.007055
2022-01-06 14:29:07,816 iteration 5284 : loss : 0.024716, loss_ce: 0.007363
2022-01-06 14:29:08,929 iteration 5285 : loss : 0.017310, loss_ce: 0.007177
2022-01-06 14:29:09,968 iteration 5286 : loss : 0.016270, loss_ce: 0.007030
2022-01-06 14:29:11,058 iteration 5287 : loss : 0.019111, loss_ce: 0.003956
 78%|██████████████████████▌      | 311/400 [1:47:38<30:58, 20.88s/it]2022-01-06 14:29:12,252 iteration 5288 : loss : 0.019927, loss_ce: 0.007873
2022-01-06 14:29:13,389 iteration 5289 : loss : 0.023564, loss_ce: 0.007627
2022-01-06 14:29:14,481 iteration 5290 : loss : 0.025533, loss_ce: 0.005589
2022-01-06 14:29:15,563 iteration 5291 : loss : 0.036220, loss_ce: 0.013086
2022-01-06 14:29:16,706 iteration 5292 : loss : 0.026483, loss_ce: 0.010031
2022-01-06 14:29:17,829 iteration 5293 : loss : 0.024564, loss_ce: 0.011059
2022-01-06 14:29:18,982 iteration 5294 : loss : 0.027199, loss_ce: 0.010892
2022-01-06 14:29:20,182 iteration 5295 : loss : 0.037934, loss_ce: 0.012359
2022-01-06 14:29:21,333 iteration 5296 : loss : 0.026774, loss_ce: 0.007439
2022-01-06 14:29:22,484 iteration 5297 : loss : 0.018918, loss_ce: 0.005878
2022-01-06 14:29:23,636 iteration 5298 : loss : 0.029671, loss_ce: 0.012172
2022-01-06 14:29:24,778 iteration 5299 : loss : 0.021456, loss_ce: 0.007312
2022-01-06 14:29:25,935 iteration 5300 : loss : 0.018100, loss_ce: 0.006297
2022-01-06 14:29:26,973 iteration 5301 : loss : 0.023553, loss_ce: 0.008941
2022-01-06 14:29:28,017 iteration 5302 : loss : 0.018608, loss_ce: 0.008638
2022-01-06 14:29:29,067 iteration 5303 : loss : 0.018703, loss_ce: 0.007257
2022-01-06 14:29:30,094 iteration 5304 : loss : 0.024366, loss_ce: 0.009530
 78%|██████████████████████▌      | 312/400 [1:47:57<29:48, 20.33s/it]2022-01-06 14:29:31,277 iteration 5305 : loss : 0.023643, loss_ce: 0.009167
2022-01-06 14:29:32,484 iteration 5306 : loss : 0.025621, loss_ce: 0.010887
2022-01-06 14:29:33,704 iteration 5307 : loss : 0.030101, loss_ce: 0.010624
2022-01-06 14:29:34,884 iteration 5308 : loss : 0.026287, loss_ce: 0.010016
2022-01-06 14:29:36,052 iteration 5309 : loss : 0.036856, loss_ce: 0.010491
2022-01-06 14:29:37,124 iteration 5310 : loss : 0.019110, loss_ce: 0.008888
2022-01-06 14:29:38,191 iteration 5311 : loss : 0.032599, loss_ce: 0.007773
2022-01-06 14:29:39,361 iteration 5312 : loss : 0.027942, loss_ce: 0.013061
2022-01-06 14:29:40,579 iteration 5313 : loss : 0.026622, loss_ce: 0.008882
2022-01-06 14:29:41,716 iteration 5314 : loss : 0.028303, loss_ce: 0.009069
2022-01-06 14:29:42,904 iteration 5315 : loss : 0.030004, loss_ce: 0.007608
2022-01-06 14:29:44,077 iteration 5316 : loss : 0.026036, loss_ce: 0.008559
2022-01-06 14:29:45,205 iteration 5317 : loss : 0.018849, loss_ce: 0.007961
2022-01-06 14:29:46,307 iteration 5318 : loss : 0.026267, loss_ce: 0.009942
2022-01-06 14:29:47,490 iteration 5319 : loss : 0.026246, loss_ce: 0.010623
2022-01-06 14:29:48,657 iteration 5320 : loss : 0.024146, loss_ce: 0.009962
2022-01-06 14:29:49,851 iteration 5321 : loss : 0.028907, loss_ce: 0.011505
 78%|██████████████████████▋      | 313/400 [1:48:16<29:13, 20.15s/it]2022-01-06 14:29:50,973 iteration 5322 : loss : 0.018983, loss_ce: 0.007786
2022-01-06 14:29:52,130 iteration 5323 : loss : 0.023918, loss_ce: 0.009641
2022-01-06 14:29:53,222 iteration 5324 : loss : 0.021408, loss_ce: 0.006923
2022-01-06 14:29:54,343 iteration 5325 : loss : 0.019792, loss_ce: 0.004824
2022-01-06 14:29:55,531 iteration 5326 : loss : 0.025081, loss_ce: 0.008680
2022-01-06 14:29:56,651 iteration 5327 : loss : 0.017949, loss_ce: 0.006511
2022-01-06 14:29:57,788 iteration 5328 : loss : 0.019334, loss_ce: 0.009516
2022-01-06 14:29:58,994 iteration 5329 : loss : 0.032670, loss_ce: 0.009927
2022-01-06 14:30:00,180 iteration 5330 : loss : 0.029665, loss_ce: 0.010658
2022-01-06 14:30:01,338 iteration 5331 : loss : 0.017684, loss_ce: 0.005954
2022-01-06 14:30:02,499 iteration 5332 : loss : 0.022216, loss_ce: 0.006513
2022-01-06 14:30:03,682 iteration 5333 : loss : 0.023389, loss_ce: 0.009641
2022-01-06 14:30:04,844 iteration 5334 : loss : 0.026340, loss_ce: 0.011317
2022-01-06 14:30:06,019 iteration 5335 : loss : 0.024749, loss_ce: 0.007955
2022-01-06 14:30:07,167 iteration 5336 : loss : 0.017525, loss_ce: 0.006442
2022-01-06 14:30:08,283 iteration 5337 : loss : 0.019111, loss_ce: 0.007239
2022-01-06 14:30:09,390 iteration 5338 : loss : 0.021427, loss_ce: 0.007871
 78%|██████████████████████▊      | 314/400 [1:48:36<28:37, 19.97s/it]2022-01-06 14:30:10,716 iteration 5339 : loss : 0.030173, loss_ce: 0.013540
2022-01-06 14:30:11,882 iteration 5340 : loss : 0.023869, loss_ce: 0.012691
2022-01-06 14:30:13,015 iteration 5341 : loss : 0.024759, loss_ce: 0.005942
2022-01-06 14:30:14,138 iteration 5342 : loss : 0.020037, loss_ce: 0.010326
2022-01-06 14:30:15,222 iteration 5343 : loss : 0.016166, loss_ce: 0.005186
2022-01-06 14:30:16,356 iteration 5344 : loss : 0.025355, loss_ce: 0.008823
2022-01-06 14:30:17,465 iteration 5345 : loss : 0.017803, loss_ce: 0.005920
2022-01-06 14:30:18,617 iteration 5346 : loss : 0.026940, loss_ce: 0.008179
2022-01-06 14:30:19,772 iteration 5347 : loss : 0.029047, loss_ce: 0.011618
2022-01-06 14:30:20,936 iteration 5348 : loss : 0.018699, loss_ce: 0.004929
2022-01-06 14:30:22,063 iteration 5349 : loss : 0.019931, loss_ce: 0.007610
2022-01-06 14:30:23,109 iteration 5350 : loss : 0.017151, loss_ce: 0.006463
2022-01-06 14:30:24,167 iteration 5351 : loss : 0.034099, loss_ce: 0.014444
2022-01-06 14:30:25,329 iteration 5352 : loss : 0.021898, loss_ce: 0.008175
2022-01-06 14:30:26,490 iteration 5353 : loss : 0.017406, loss_ce: 0.006658
2022-01-06 14:30:27,576 iteration 5354 : loss : 0.020483, loss_ce: 0.009190
2022-01-06 14:30:27,576 Training Data Eval:
2022-01-06 14:30:33,147   Average segmentation loss on training set: 0.0146
2022-01-06 14:30:33,147 Validation Data Eval:
2022-01-06 14:30:35,065   Average segmentation loss on validation set: 0.0962
2022-01-06 14:30:36,131 iteration 5355 : loss : 0.018573, loss_ce: 0.006881
 79%|██████████████████████▊      | 315/400 [1:49:03<31:10, 22.00s/it]2022-01-06 14:30:37,327 iteration 5356 : loss : 0.021454, loss_ce: 0.008305
2022-01-06 14:30:38,386 iteration 5357 : loss : 0.022281, loss_ce: 0.004711
2022-01-06 14:30:39,629 iteration 5358 : loss : 0.034540, loss_ce: 0.015835
2022-01-06 14:30:40,668 iteration 5359 : loss : 0.018146, loss_ce: 0.006686
2022-01-06 14:30:41,792 iteration 5360 : loss : 0.024695, loss_ce: 0.010799
2022-01-06 14:30:42,912 iteration 5361 : loss : 0.025954, loss_ce: 0.010094
2022-01-06 14:30:44,058 iteration 5362 : loss : 0.022113, loss_ce: 0.006305
2022-01-06 14:30:45,182 iteration 5363 : loss : 0.020977, loss_ce: 0.008137
2022-01-06 14:30:46,301 iteration 5364 : loss : 0.034247, loss_ce: 0.006429
2022-01-06 14:30:47,402 iteration 5365 : loss : 0.018284, loss_ce: 0.006905
2022-01-06 14:30:48,537 iteration 5366 : loss : 0.026702, loss_ce: 0.011636
2022-01-06 14:30:49,694 iteration 5367 : loss : 0.017060, loss_ce: 0.007063
2022-01-06 14:30:50,786 iteration 5368 : loss : 0.017568, loss_ce: 0.008164
2022-01-06 14:30:51,907 iteration 5369 : loss : 0.017724, loss_ce: 0.006241
2022-01-06 14:30:53,002 iteration 5370 : loss : 0.016205, loss_ce: 0.006083
2022-01-06 14:30:54,126 iteration 5371 : loss : 0.019683, loss_ce: 0.008438
2022-01-06 14:30:55,214 iteration 5372 : loss : 0.025032, loss_ce: 0.008002
 79%|██████████████████████▉      | 316/400 [1:49:22<29:34, 21.13s/it]2022-01-06 14:30:56,392 iteration 5373 : loss : 0.018989, loss_ce: 0.009015
2022-01-06 14:30:57,464 iteration 5374 : loss : 0.021751, loss_ce: 0.009125
2022-01-06 14:30:58,589 iteration 5375 : loss : 0.015339, loss_ce: 0.004748
2022-01-06 14:30:59,669 iteration 5376 : loss : 0.020098, loss_ce: 0.006959
2022-01-06 14:31:00,738 iteration 5377 : loss : 0.015201, loss_ce: 0.004967
2022-01-06 14:31:01,855 iteration 5378 : loss : 0.026921, loss_ce: 0.011666
2022-01-06 14:31:02,902 iteration 5379 : loss : 0.015419, loss_ce: 0.005295
2022-01-06 14:31:04,019 iteration 5380 : loss : 0.018281, loss_ce: 0.007552
2022-01-06 14:31:05,110 iteration 5381 : loss : 0.020163, loss_ce: 0.008995
2022-01-06 14:31:06,251 iteration 5382 : loss : 0.021033, loss_ce: 0.006036
2022-01-06 14:31:07,492 iteration 5383 : loss : 0.022672, loss_ce: 0.010657
2022-01-06 14:31:08,758 iteration 5384 : loss : 0.026432, loss_ce: 0.010685
2022-01-06 14:31:09,843 iteration 5385 : loss : 0.019049, loss_ce: 0.007301
2022-01-06 14:31:10,875 iteration 5386 : loss : 0.017275, loss_ce: 0.005864
2022-01-06 14:31:11,961 iteration 5387 : loss : 0.027148, loss_ce: 0.011671
2022-01-06 14:31:13,115 iteration 5388 : loss : 0.017773, loss_ce: 0.006008
2022-01-06 14:31:14,323 iteration 5389 : loss : 0.029539, loss_ce: 0.010677
 79%|██████████████████████▉      | 317/400 [1:49:41<28:23, 20.52s/it]2022-01-06 14:31:15,467 iteration 5390 : loss : 0.028614, loss_ce: 0.006643
2022-01-06 14:31:16,657 iteration 5391 : loss : 0.027662, loss_ce: 0.009355
2022-01-06 14:31:17,816 iteration 5392 : loss : 0.019608, loss_ce: 0.010714
2022-01-06 14:31:18,926 iteration 5393 : loss : 0.026056, loss_ce: 0.009600
2022-01-06 14:31:20,007 iteration 5394 : loss : 0.019614, loss_ce: 0.008455
2022-01-06 14:31:21,130 iteration 5395 : loss : 0.016549, loss_ce: 0.004929
2022-01-06 14:31:22,191 iteration 5396 : loss : 0.020053, loss_ce: 0.011593
2022-01-06 14:31:23,360 iteration 5397 : loss : 0.023670, loss_ce: 0.007177
2022-01-06 14:31:24,481 iteration 5398 : loss : 0.027304, loss_ce: 0.009460
2022-01-06 14:31:25,529 iteration 5399 : loss : 0.029830, loss_ce: 0.008413
2022-01-06 14:31:26,726 iteration 5400 : loss : 0.030791, loss_ce: 0.007763
2022-01-06 14:31:27,875 iteration 5401 : loss : 0.020507, loss_ce: 0.008416
2022-01-06 14:31:29,014 iteration 5402 : loss : 0.021320, loss_ce: 0.007338
2022-01-06 14:31:30,106 iteration 5403 : loss : 0.023423, loss_ce: 0.009711
2022-01-06 14:31:31,181 iteration 5404 : loss : 0.019259, loss_ce: 0.004922
2022-01-06 14:31:32,289 iteration 5405 : loss : 0.016057, loss_ce: 0.006594
2022-01-06 14:31:33,454 iteration 5406 : loss : 0.018197, loss_ce: 0.005506
 80%|███████████████████████      | 318/400 [1:50:00<27:28, 20.10s/it]2022-01-06 14:31:34,663 iteration 5407 : loss : 0.027066, loss_ce: 0.008627
2022-01-06 14:31:35,783 iteration 5408 : loss : 0.026048, loss_ce: 0.008699
2022-01-06 14:31:36,980 iteration 5409 : loss : 0.028364, loss_ce: 0.012593
2022-01-06 14:31:38,065 iteration 5410 : loss : 0.020368, loss_ce: 0.008273
2022-01-06 14:31:39,184 iteration 5411 : loss : 0.020321, loss_ce: 0.007646
2022-01-06 14:31:40,320 iteration 5412 : loss : 0.020874, loss_ce: 0.007949
2022-01-06 14:31:41,412 iteration 5413 : loss : 0.016980, loss_ce: 0.008142
2022-01-06 14:31:42,535 iteration 5414 : loss : 0.018422, loss_ce: 0.007470
2022-01-06 14:31:43,728 iteration 5415 : loss : 0.031300, loss_ce: 0.013807
2022-01-06 14:31:44,778 iteration 5416 : loss : 0.014697, loss_ce: 0.004876
2022-01-06 14:31:45,919 iteration 5417 : loss : 0.016381, loss_ce: 0.004988
2022-01-06 14:31:47,013 iteration 5418 : loss : 0.028887, loss_ce: 0.011655
2022-01-06 14:31:48,147 iteration 5419 : loss : 0.027891, loss_ce: 0.009296
2022-01-06 14:31:49,317 iteration 5420 : loss : 0.022834, loss_ce: 0.008503
2022-01-06 14:31:50,458 iteration 5421 : loss : 0.020805, loss_ce: 0.010256
2022-01-06 14:31:51,457 iteration 5422 : loss : 0.019663, loss_ce: 0.009353
2022-01-06 14:31:52,600 iteration 5423 : loss : 0.035463, loss_ce: 0.007175
 80%|███████████████████████▏     | 319/400 [1:50:19<26:44, 19.81s/it]2022-01-06 14:31:53,834 iteration 5424 : loss : 0.023303, loss_ce: 0.008313
2022-01-06 14:31:54,912 iteration 5425 : loss : 0.017451, loss_ce: 0.005514
2022-01-06 14:31:56,017 iteration 5426 : loss : 0.019878, loss_ce: 0.005045
2022-01-06 14:31:57,046 iteration 5427 : loss : 0.018568, loss_ce: 0.007291
2022-01-06 14:31:58,129 iteration 5428 : loss : 0.030972, loss_ce: 0.012525
2022-01-06 14:31:59,258 iteration 5429 : loss : 0.017026, loss_ce: 0.005989
2022-01-06 14:32:00,379 iteration 5430 : loss : 0.025554, loss_ce: 0.013421
2022-01-06 14:32:01,436 iteration 5431 : loss : 0.021106, loss_ce: 0.007140
2022-01-06 14:32:02,628 iteration 5432 : loss : 0.023832, loss_ce: 0.011420
2022-01-06 14:32:03,810 iteration 5433 : loss : 0.021457, loss_ce: 0.008380
2022-01-06 14:32:04,886 iteration 5434 : loss : 0.018435, loss_ce: 0.007628
2022-01-06 14:32:06,073 iteration 5435 : loss : 0.025910, loss_ce: 0.011379
2022-01-06 14:32:07,210 iteration 5436 : loss : 0.018085, loss_ce: 0.006599
2022-01-06 14:32:08,367 iteration 5437 : loss : 0.059490, loss_ce: 0.013643
2022-01-06 14:32:09,527 iteration 5438 : loss : 0.019287, loss_ce: 0.008128
2022-01-06 14:32:10,627 iteration 5439 : loss : 0.020426, loss_ce: 0.008602
2022-01-06 14:32:10,627 Training Data Eval:
2022-01-06 14:32:16,283   Average segmentation loss on training set: 0.0153
2022-01-06 14:32:16,283 Validation Data Eval:
2022-01-06 14:32:18,222   Average segmentation loss on validation set: 0.0864
2022-01-06 14:32:19,305 iteration 5440 : loss : 0.020806, loss_ce: 0.006543
 80%|███████████████████████▏     | 320/400 [1:50:46<29:10, 21.88s/it]2022-01-06 14:32:20,476 iteration 5441 : loss : 0.018752, loss_ce: 0.006030
2022-01-06 14:32:21,569 iteration 5442 : loss : 0.019997, loss_ce: 0.006675
2022-01-06 14:32:22,706 iteration 5443 : loss : 0.016962, loss_ce: 0.005458
2022-01-06 14:32:23,787 iteration 5444 : loss : 0.017340, loss_ce: 0.006165
2022-01-06 14:32:24,962 iteration 5445 : loss : 0.022046, loss_ce: 0.007231
2022-01-06 14:32:26,074 iteration 5446 : loss : 0.026245, loss_ce: 0.012435
2022-01-06 14:32:27,302 iteration 5447 : loss : 0.040105, loss_ce: 0.016163
2022-01-06 14:32:28,395 iteration 5448 : loss : 0.018253, loss_ce: 0.006749
2022-01-06 14:32:29,534 iteration 5449 : loss : 0.025085, loss_ce: 0.012910
2022-01-06 14:32:30,655 iteration 5450 : loss : 0.024778, loss_ce: 0.010303
2022-01-06 14:32:31,721 iteration 5451 : loss : 0.018619, loss_ce: 0.006408
2022-01-06 14:32:32,860 iteration 5452 : loss : 0.025318, loss_ce: 0.009461
2022-01-06 14:32:33,949 iteration 5453 : loss : 0.021041, loss_ce: 0.007818
2022-01-06 14:32:35,126 iteration 5454 : loss : 0.031849, loss_ce: 0.018980
2022-01-06 14:32:36,267 iteration 5455 : loss : 0.028035, loss_ce: 0.011635
2022-01-06 14:32:37,392 iteration 5456 : loss : 0.025929, loss_ce: 0.005509
2022-01-06 14:32:38,481 iteration 5457 : loss : 0.018762, loss_ce: 0.006104
 80%|███████████████████████▎     | 321/400 [1:51:05<27:44, 21.07s/it]2022-01-06 14:32:39,659 iteration 5458 : loss : 0.032160, loss_ce: 0.010564
2022-01-06 14:32:40,725 iteration 5459 : loss : 0.019794, loss_ce: 0.008944
2022-01-06 14:32:41,838 iteration 5460 : loss : 0.017825, loss_ce: 0.006144
2022-01-06 14:32:42,942 iteration 5461 : loss : 0.022381, loss_ce: 0.008768
2022-01-06 14:32:44,034 iteration 5462 : loss : 0.017868, loss_ce: 0.006922
2022-01-06 14:32:45,155 iteration 5463 : loss : 0.029206, loss_ce: 0.008510
2022-01-06 14:32:46,272 iteration 5464 : loss : 0.027604, loss_ce: 0.013363
2022-01-06 14:32:47,450 iteration 5465 : loss : 0.025882, loss_ce: 0.009652
2022-01-06 14:32:48,616 iteration 5466 : loss : 0.017768, loss_ce: 0.006829
2022-01-06 14:32:49,750 iteration 5467 : loss : 0.040128, loss_ce: 0.018681
2022-01-06 14:32:50,882 iteration 5468 : loss : 0.018115, loss_ce: 0.007086
2022-01-06 14:32:51,935 iteration 5469 : loss : 0.019322, loss_ce: 0.007214
2022-01-06 14:32:53,034 iteration 5470 : loss : 0.013038, loss_ce: 0.004976
2022-01-06 14:32:54,137 iteration 5471 : loss : 0.023007, loss_ce: 0.006176
2022-01-06 14:32:55,278 iteration 5472 : loss : 0.028750, loss_ce: 0.009928
2022-01-06 14:32:56,333 iteration 5473 : loss : 0.017440, loss_ce: 0.006238
2022-01-06 14:32:57,399 iteration 5474 : loss : 0.018668, loss_ce: 0.005997
 80%|███████████████████████▎     | 322/400 [1:51:24<26:33, 20.42s/it]2022-01-06 14:32:58,582 iteration 5475 : loss : 0.022595, loss_ce: 0.006328
2022-01-06 14:32:59,745 iteration 5476 : loss : 0.024728, loss_ce: 0.010526
2022-01-06 14:33:00,847 iteration 5477 : loss : 0.033065, loss_ce: 0.004522
2022-01-06 14:33:01,879 iteration 5478 : loss : 0.015432, loss_ce: 0.005106
2022-01-06 14:33:03,019 iteration 5479 : loss : 0.015516, loss_ce: 0.006482
2022-01-06 14:33:04,109 iteration 5480 : loss : 0.023426, loss_ce: 0.007063
2022-01-06 14:33:05,187 iteration 5481 : loss : 0.020952, loss_ce: 0.007324
2022-01-06 14:33:06,396 iteration 5482 : loss : 0.031661, loss_ce: 0.007617
2022-01-06 14:33:07,447 iteration 5483 : loss : 0.018708, loss_ce: 0.007056
2022-01-06 14:33:08,588 iteration 5484 : loss : 0.029270, loss_ce: 0.011773
2022-01-06 14:33:09,710 iteration 5485 : loss : 0.019802, loss_ce: 0.008271
2022-01-06 14:33:10,826 iteration 5486 : loss : 0.035655, loss_ce: 0.014930
2022-01-06 14:33:11,958 iteration 5487 : loss : 0.022184, loss_ce: 0.008577
2022-01-06 14:33:13,042 iteration 5488 : loss : 0.017760, loss_ce: 0.008019
2022-01-06 14:33:14,118 iteration 5489 : loss : 0.016673, loss_ce: 0.008192
2022-01-06 14:33:15,306 iteration 5490 : loss : 0.023248, loss_ce: 0.009623
2022-01-06 14:33:16,390 iteration 5491 : loss : 0.017509, loss_ce: 0.004953
 81%|███████████████████████▍     | 323/400 [1:51:43<25:39, 19.99s/it]2022-01-06 14:33:17,470 iteration 5492 : loss : 0.021356, loss_ce: 0.008157
2022-01-06 14:33:18,604 iteration 5493 : loss : 0.027994, loss_ce: 0.009385
2022-01-06 14:33:19,674 iteration 5494 : loss : 0.029723, loss_ce: 0.009646
2022-01-06 14:33:20,788 iteration 5495 : loss : 0.016514, loss_ce: 0.004752
2022-01-06 14:33:21,915 iteration 5496 : loss : 0.018187, loss_ce: 0.005183
2022-01-06 14:33:23,055 iteration 5497 : loss : 0.022244, loss_ce: 0.008288
2022-01-06 14:33:24,219 iteration 5498 : loss : 0.023977, loss_ce: 0.007299
2022-01-06 14:33:25,365 iteration 5499 : loss : 0.024075, loss_ce: 0.013263
2022-01-06 14:33:26,420 iteration 5500 : loss : 0.019010, loss_ce: 0.008800
2022-01-06 14:33:27,544 iteration 5501 : loss : 0.026315, loss_ce: 0.011721
2022-01-06 14:33:28,618 iteration 5502 : loss : 0.013901, loss_ce: 0.005860
2022-01-06 14:33:29,717 iteration 5503 : loss : 0.023110, loss_ce: 0.006148
2022-01-06 14:33:30,787 iteration 5504 : loss : 0.021684, loss_ce: 0.007692
2022-01-06 14:33:31,896 iteration 5505 : loss : 0.020670, loss_ce: 0.007931
2022-01-06 14:33:33,037 iteration 5506 : loss : 0.026065, loss_ce: 0.011022
2022-01-06 14:33:34,149 iteration 5507 : loss : 0.022894, loss_ce: 0.009517
2022-01-06 14:33:35,267 iteration 5508 : loss : 0.020186, loss_ce: 0.007006
 81%|███████████████████████▍     | 324/400 [1:52:02<24:54, 19.66s/it]2022-01-06 14:33:36,450 iteration 5509 : loss : 0.025531, loss_ce: 0.007630
2022-01-06 14:33:37,644 iteration 5510 : loss : 0.017380, loss_ce: 0.005693
2022-01-06 14:33:38,731 iteration 5511 : loss : 0.016423, loss_ce: 0.006329
2022-01-06 14:33:39,882 iteration 5512 : loss : 0.027593, loss_ce: 0.011102
2022-01-06 14:33:40,963 iteration 5513 : loss : 0.017533, loss_ce: 0.007802
2022-01-06 14:33:42,007 iteration 5514 : loss : 0.014004, loss_ce: 0.004560
2022-01-06 14:33:43,170 iteration 5515 : loss : 0.030274, loss_ce: 0.007475
2022-01-06 14:33:44,256 iteration 5516 : loss : 0.017583, loss_ce: 0.006644
2022-01-06 14:33:45,365 iteration 5517 : loss : 0.018164, loss_ce: 0.007964
2022-01-06 14:33:46,473 iteration 5518 : loss : 0.033888, loss_ce: 0.010911
2022-01-06 14:33:47,602 iteration 5519 : loss : 0.023688, loss_ce: 0.009207
2022-01-06 14:33:48,710 iteration 5520 : loss : 0.017358, loss_ce: 0.007240
2022-01-06 14:33:49,835 iteration 5521 : loss : 0.024132, loss_ce: 0.009006
2022-01-06 14:33:50,948 iteration 5522 : loss : 0.040167, loss_ce: 0.017971
2022-01-06 14:33:52,035 iteration 5523 : loss : 0.015134, loss_ce: 0.004949
2022-01-06 14:33:53,142 iteration 5524 : loss : 0.025693, loss_ce: 0.010629
2022-01-06 14:33:53,143 Training Data Eval:
2022-01-06 14:33:58,836   Average segmentation loss on training set: 0.0158
2022-01-06 14:33:58,836 Validation Data Eval:
2022-01-06 14:34:00,757   Average segmentation loss on validation set: 0.0806
2022-01-06 14:34:01,869 iteration 5525 : loss : 0.027014, loss_ce: 0.013169
 81%|███████████████████████▌     | 325/400 [1:52:28<27:10, 21.74s/it]2022-01-06 14:34:03,084 iteration 5526 : loss : 0.026421, loss_ce: 0.008667
2022-01-06 14:34:04,201 iteration 5527 : loss : 0.019867, loss_ce: 0.008424
2022-01-06 14:34:05,313 iteration 5528 : loss : 0.019404, loss_ce: 0.006563
2022-01-06 14:34:06,457 iteration 5529 : loss : 0.022267, loss_ce: 0.009785
2022-01-06 14:34:07,525 iteration 5530 : loss : 0.018205, loss_ce: 0.008253
2022-01-06 14:34:08,615 iteration 5531 : loss : 0.023318, loss_ce: 0.010279
2022-01-06 14:34:09,680 iteration 5532 : loss : 0.014735, loss_ce: 0.005851
2022-01-06 14:34:10,841 iteration 5533 : loss : 0.017330, loss_ce: 0.004527
2022-01-06 14:34:11,946 iteration 5534 : loss : 0.016592, loss_ce: 0.005901
2022-01-06 14:34:13,113 iteration 5535 : loss : 0.024459, loss_ce: 0.009113
2022-01-06 14:34:14,314 iteration 5536 : loss : 0.017040, loss_ce: 0.007145
2022-01-06 14:34:15,451 iteration 5537 : loss : 0.023704, loss_ce: 0.007061
2022-01-06 14:34:16,570 iteration 5538 : loss : 0.023647, loss_ce: 0.010768
2022-01-06 14:34:17,766 iteration 5539 : loss : 0.025899, loss_ce: 0.006002
2022-01-06 14:34:18,840 iteration 5540 : loss : 0.017679, loss_ce: 0.007908
2022-01-06 14:34:19,954 iteration 5541 : loss : 0.021028, loss_ce: 0.007153
2022-01-06 14:34:21,042 iteration 5542 : loss : 0.015185, loss_ce: 0.005393
 82%|███████████████████████▋     | 326/400 [1:52:47<25:51, 20.97s/it]2022-01-06 14:34:22,232 iteration 5543 : loss : 0.024058, loss_ce: 0.007748
2022-01-06 14:34:23,345 iteration 5544 : loss : 0.019972, loss_ce: 0.008007
2022-01-06 14:34:24,427 iteration 5545 : loss : 0.024115, loss_ce: 0.010188
2022-01-06 14:34:25,486 iteration 5546 : loss : 0.019914, loss_ce: 0.006446
2022-01-06 14:34:26,588 iteration 5547 : loss : 0.018952, loss_ce: 0.006971
2022-01-06 14:34:27,740 iteration 5548 : loss : 0.027394, loss_ce: 0.009386
2022-01-06 14:34:28,789 iteration 5549 : loss : 0.018518, loss_ce: 0.007021
2022-01-06 14:34:29,878 iteration 5550 : loss : 0.019988, loss_ce: 0.007254
2022-01-06 14:34:30,991 iteration 5551 : loss : 0.019881, loss_ce: 0.008500
2022-01-06 14:34:32,051 iteration 5552 : loss : 0.017464, loss_ce: 0.006526
2022-01-06 14:34:33,176 iteration 5553 : loss : 0.020000, loss_ce: 0.007870
2022-01-06 14:34:34,333 iteration 5554 : loss : 0.021027, loss_ce: 0.006849
2022-01-06 14:34:35,487 iteration 5555 : loss : 0.034855, loss_ce: 0.008612
2022-01-06 14:34:36,518 iteration 5556 : loss : 0.024219, loss_ce: 0.008308
2022-01-06 14:34:37,678 iteration 5557 : loss : 0.028561, loss_ce: 0.007860
2022-01-06 14:34:38,789 iteration 5558 : loss : 0.018106, loss_ce: 0.006214
2022-01-06 14:34:39,806 iteration 5559 : loss : 0.017690, loss_ce: 0.007622
 82%|███████████████████████▋     | 327/400 [1:53:06<24:42, 20.31s/it]2022-01-06 14:34:41,136 iteration 5560 : loss : 0.035148, loss_ce: 0.013883
2022-01-06 14:34:42,225 iteration 5561 : loss : 0.019948, loss_ce: 0.006600
2022-01-06 14:34:43,314 iteration 5562 : loss : 0.023753, loss_ce: 0.006401
2022-01-06 14:34:44,410 iteration 5563 : loss : 0.016753, loss_ce: 0.006685
2022-01-06 14:34:45,602 iteration 5564 : loss : 0.023194, loss_ce: 0.007997
2022-01-06 14:34:46,716 iteration 5565 : loss : 0.016665, loss_ce: 0.006785
2022-01-06 14:34:47,878 iteration 5566 : loss : 0.020496, loss_ce: 0.009089
2022-01-06 14:34:49,086 iteration 5567 : loss : 0.028485, loss_ce: 0.012780
2022-01-06 14:34:50,191 iteration 5568 : loss : 0.025341, loss_ce: 0.008752
2022-01-06 14:34:51,341 iteration 5569 : loss : 0.021140, loss_ce: 0.008792
2022-01-06 14:34:52,460 iteration 5570 : loss : 0.020445, loss_ce: 0.007964
2022-01-06 14:34:53,606 iteration 5571 : loss : 0.021853, loss_ce: 0.009775
2022-01-06 14:34:54,710 iteration 5572 : loss : 0.017168, loss_ce: 0.006729
2022-01-06 14:34:55,849 iteration 5573 : loss : 0.018701, loss_ce: 0.005574
2022-01-06 14:34:57,049 iteration 5574 : loss : 0.019936, loss_ce: 0.005998
2022-01-06 14:34:58,082 iteration 5575 : loss : 0.014218, loss_ce: 0.006144
2022-01-06 14:34:59,060 iteration 5576 : loss : 0.017304, loss_ce: 0.005027
 82%|███████████████████████▊     | 328/400 [1:53:26<23:59, 19.99s/it]2022-01-06 14:35:00,217 iteration 5577 : loss : 0.020011, loss_ce: 0.006680
2022-01-06 14:35:01,291 iteration 5578 : loss : 0.021785, loss_ce: 0.005810
2022-01-06 14:35:02,394 iteration 5579 : loss : 0.015854, loss_ce: 0.004938
2022-01-06 14:35:03,547 iteration 5580 : loss : 0.016004, loss_ce: 0.005627
2022-01-06 14:35:04,636 iteration 5581 : loss : 0.017357, loss_ce: 0.005496
2022-01-06 14:35:05,782 iteration 5582 : loss : 0.022088, loss_ce: 0.006241
2022-01-06 14:35:06,863 iteration 5583 : loss : 0.020654, loss_ce: 0.005666
2022-01-06 14:35:08,050 iteration 5584 : loss : 0.024059, loss_ce: 0.010856
2022-01-06 14:35:09,131 iteration 5585 : loss : 0.022035, loss_ce: 0.007924
2022-01-06 14:35:10,230 iteration 5586 : loss : 0.028155, loss_ce: 0.012184
2022-01-06 14:35:11,334 iteration 5587 : loss : 0.024743, loss_ce: 0.010595
2022-01-06 14:35:12,509 iteration 5588 : loss : 0.022950, loss_ce: 0.007405
2022-01-06 14:35:13,663 iteration 5589 : loss : 0.025668, loss_ce: 0.011541
2022-01-06 14:35:14,778 iteration 5590 : loss : 0.022953, loss_ce: 0.009858
2022-01-06 14:35:15,883 iteration 5591 : loss : 0.017606, loss_ce: 0.007411
2022-01-06 14:35:16,974 iteration 5592 : loss : 0.017382, loss_ce: 0.005506
2022-01-06 14:35:18,060 iteration 5593 : loss : 0.017036, loss_ce: 0.009031
 82%|███████████████████████▊     | 329/400 [1:53:45<23:18, 19.69s/it]2022-01-06 14:35:19,247 iteration 5594 : loss : 0.016656, loss_ce: 0.005842
2022-01-06 14:35:20,329 iteration 5595 : loss : 0.017236, loss_ce: 0.007132
2022-01-06 14:35:21,405 iteration 5596 : loss : 0.014955, loss_ce: 0.003944
2022-01-06 14:35:22,524 iteration 5597 : loss : 0.020544, loss_ce: 0.007435
2022-01-06 14:35:23,704 iteration 5598 : loss : 0.027862, loss_ce: 0.012823
2022-01-06 14:35:24,853 iteration 5599 : loss : 0.018070, loss_ce: 0.006075
2022-01-06 14:35:25,998 iteration 5600 : loss : 0.016462, loss_ce: 0.006064
2022-01-06 14:35:27,143 iteration 5601 : loss : 0.017653, loss_ce: 0.005290
2022-01-06 14:35:28,215 iteration 5602 : loss : 0.024952, loss_ce: 0.005859
2022-01-06 14:35:29,370 iteration 5603 : loss : 0.021766, loss_ce: 0.008989
2022-01-06 14:35:30,452 iteration 5604 : loss : 0.022996, loss_ce: 0.010046
2022-01-06 14:35:31,539 iteration 5605 : loss : 0.017963, loss_ce: 0.005324
2022-01-06 14:35:32,719 iteration 5606 : loss : 0.036822, loss_ce: 0.010419
2022-01-06 14:35:33,826 iteration 5607 : loss : 0.022530, loss_ce: 0.008277
2022-01-06 14:35:35,052 iteration 5608 : loss : 0.030673, loss_ce: 0.014628
2022-01-06 14:35:36,123 iteration 5609 : loss : 0.020166, loss_ce: 0.008297
2022-01-06 14:35:36,124 Training Data Eval:
2022-01-06 14:35:41,632   Average segmentation loss on training set: 0.0162
2022-01-06 14:35:41,632 Validation Data Eval:
2022-01-06 14:35:43,526   Average segmentation loss on validation set: 0.1259
2022-01-06 14:35:44,667 iteration 5610 : loss : 0.017222, loss_ce: 0.006389
 82%|███████████████████████▉     | 330/400 [1:54:11<25:23, 21.77s/it]2022-01-06 14:35:45,877 iteration 5611 : loss : 0.019122, loss_ce: 0.008366
2022-01-06 14:35:46,995 iteration 5612 : loss : 0.021208, loss_ce: 0.006399
2022-01-06 14:35:48,095 iteration 5613 : loss : 0.031023, loss_ce: 0.011288
2022-01-06 14:35:49,190 iteration 5614 : loss : 0.023611, loss_ce: 0.008215
2022-01-06 14:35:50,314 iteration 5615 : loss : 0.018496, loss_ce: 0.006604
2022-01-06 14:35:51,463 iteration 5616 : loss : 0.025767, loss_ce: 0.010537
2022-01-06 14:35:52,566 iteration 5617 : loss : 0.015269, loss_ce: 0.005204
2022-01-06 14:35:53,620 iteration 5618 : loss : 0.019703, loss_ce: 0.008571
2022-01-06 14:35:54,756 iteration 5619 : loss : 0.026969, loss_ce: 0.009545
2022-01-06 14:35:55,907 iteration 5620 : loss : 0.021021, loss_ce: 0.005664
2022-01-06 14:35:56,986 iteration 5621 : loss : 0.019057, loss_ce: 0.007405
2022-01-06 14:35:58,168 iteration 5622 : loss : 0.014953, loss_ce: 0.005271
2022-01-06 14:35:59,235 iteration 5623 : loss : 0.027860, loss_ce: 0.008183
2022-01-06 14:36:00,357 iteration 5624 : loss : 0.017962, loss_ce: 0.007487
2022-01-06 14:36:01,501 iteration 5625 : loss : 0.026554, loss_ce: 0.011273
2022-01-06 14:36:02,604 iteration 5626 : loss : 0.024787, loss_ce: 0.009250
2022-01-06 14:36:03,703 iteration 5627 : loss : 0.027048, loss_ce: 0.009404
 83%|███████████████████████▉     | 331/400 [1:54:30<24:05, 20.95s/it]2022-01-06 14:36:04,768 iteration 5628 : loss : 0.016499, loss_ce: 0.006035
2022-01-06 14:36:05,899 iteration 5629 : loss : 0.017413, loss_ce: 0.007692
2022-01-06 14:36:07,002 iteration 5630 : loss : 0.015611, loss_ce: 0.005433
2022-01-06 14:36:08,217 iteration 5631 : loss : 0.020032, loss_ce: 0.008797
2022-01-06 14:36:09,250 iteration 5632 : loss : 0.020717, loss_ce: 0.008765
2022-01-06 14:36:10,440 iteration 5633 : loss : 0.026023, loss_ce: 0.010342
2022-01-06 14:36:11,501 iteration 5634 : loss : 0.018129, loss_ce: 0.006162
2022-01-06 14:36:12,627 iteration 5635 : loss : 0.027254, loss_ce: 0.006762
2022-01-06 14:36:13,667 iteration 5636 : loss : 0.015410, loss_ce: 0.006735
2022-01-06 14:36:14,788 iteration 5637 : loss : 0.020805, loss_ce: 0.008905
2022-01-06 14:36:15,784 iteration 5638 : loss : 0.017107, loss_ce: 0.005047
2022-01-06 14:36:16,837 iteration 5639 : loss : 0.019422, loss_ce: 0.005630
2022-01-06 14:36:17,923 iteration 5640 : loss : 0.018076, loss_ce: 0.008414
2022-01-06 14:36:19,067 iteration 5641 : loss : 0.014746, loss_ce: 0.006709
2022-01-06 14:36:20,159 iteration 5642 : loss : 0.016988, loss_ce: 0.003126
2022-01-06 14:36:21,316 iteration 5643 : loss : 0.017945, loss_ce: 0.004768
2022-01-06 14:36:22,456 iteration 5644 : loss : 0.019635, loss_ce: 0.007503
 83%|████████████████████████     | 332/400 [1:54:49<22:59, 20.29s/it]2022-01-06 14:36:23,731 iteration 5645 : loss : 0.025669, loss_ce: 0.010554
2022-01-06 14:36:24,816 iteration 5646 : loss : 0.021867, loss_ce: 0.007581
2022-01-06 14:36:25,943 iteration 5647 : loss : 0.025161, loss_ce: 0.014542
2022-01-06 14:36:27,104 iteration 5648 : loss : 0.028086, loss_ce: 0.009669
2022-01-06 14:36:28,260 iteration 5649 : loss : 0.025371, loss_ce: 0.010808
2022-01-06 14:36:29,385 iteration 5650 : loss : 0.025258, loss_ce: 0.010369
2022-01-06 14:36:30,492 iteration 5651 : loss : 0.024966, loss_ce: 0.012114
2022-01-06 14:36:31,534 iteration 5652 : loss : 0.014625, loss_ce: 0.005233
2022-01-06 14:36:32,626 iteration 5653 : loss : 0.017135, loss_ce: 0.005794
2022-01-06 14:36:33,675 iteration 5654 : loss : 0.019051, loss_ce: 0.005572
2022-01-06 14:36:34,808 iteration 5655 : loss : 0.026350, loss_ce: 0.009234
2022-01-06 14:36:35,917 iteration 5656 : loss : 0.019588, loss_ce: 0.006695
2022-01-06 14:36:37,030 iteration 5657 : loss : 0.017721, loss_ce: 0.007146
2022-01-06 14:36:38,144 iteration 5658 : loss : 0.021471, loss_ce: 0.010665
2022-01-06 14:36:39,234 iteration 5659 : loss : 0.015624, loss_ce: 0.004340
2022-01-06 14:36:40,291 iteration 5660 : loss : 0.014490, loss_ce: 0.004856
2022-01-06 14:36:41,488 iteration 5661 : loss : 0.022506, loss_ce: 0.008805
 83%|████████████████████████▏    | 333/400 [1:55:08<22:14, 19.92s/it]2022-01-06 14:36:42,649 iteration 5662 : loss : 0.017946, loss_ce: 0.004552
2022-01-06 14:36:43,751 iteration 5663 : loss : 0.025266, loss_ce: 0.010780
2022-01-06 14:36:44,869 iteration 5664 : loss : 0.034333, loss_ce: 0.012806
2022-01-06 14:36:45,891 iteration 5665 : loss : 0.016794, loss_ce: 0.006120
2022-01-06 14:36:47,014 iteration 5666 : loss : 0.019144, loss_ce: 0.006952
2022-01-06 14:36:48,155 iteration 5667 : loss : 0.022985, loss_ce: 0.009285
2022-01-06 14:36:49,253 iteration 5668 : loss : 0.019714, loss_ce: 0.008465
2022-01-06 14:36:50,400 iteration 5669 : loss : 0.023624, loss_ce: 0.012220
2022-01-06 14:36:51,525 iteration 5670 : loss : 0.018904, loss_ce: 0.006181
2022-01-06 14:36:52,645 iteration 5671 : loss : 0.016442, loss_ce: 0.006049
2022-01-06 14:36:53,750 iteration 5672 : loss : 0.020398, loss_ce: 0.005486
2022-01-06 14:36:54,828 iteration 5673 : loss : 0.016314, loss_ce: 0.004972
2022-01-06 14:36:56,008 iteration 5674 : loss : 0.017628, loss_ce: 0.008699
2022-01-06 14:36:57,091 iteration 5675 : loss : 0.015956, loss_ce: 0.006887
2022-01-06 14:36:58,262 iteration 5676 : loss : 0.022042, loss_ce: 0.008315
2022-01-06 14:36:59,421 iteration 5677 : loss : 0.014877, loss_ce: 0.006788
2022-01-06 14:37:00,553 iteration 5678 : loss : 0.029992, loss_ce: 0.009512
 84%|████████████████████████▏    | 334/400 [1:55:27<21:37, 19.65s/it]2022-01-06 14:37:01,724 iteration 5679 : loss : 0.029635, loss_ce: 0.011734
2022-01-06 14:37:02,728 iteration 5680 : loss : 0.016072, loss_ce: 0.004149
2022-01-06 14:37:03,874 iteration 5681 : loss : 0.025180, loss_ce: 0.009218
2022-01-06 14:37:05,022 iteration 5682 : loss : 0.023943, loss_ce: 0.010197
2022-01-06 14:37:06,091 iteration 5683 : loss : 0.014539, loss_ce: 0.004945
2022-01-06 14:37:07,154 iteration 5684 : loss : 0.015962, loss_ce: 0.008274
2022-01-06 14:37:08,284 iteration 5685 : loss : 0.024150, loss_ce: 0.011401
2022-01-06 14:37:09,365 iteration 5686 : loss : 0.017255, loss_ce: 0.006200
2022-01-06 14:37:10,465 iteration 5687 : loss : 0.021134, loss_ce: 0.008326
2022-01-06 14:37:11,658 iteration 5688 : loss : 0.021727, loss_ce: 0.008255
2022-01-06 14:37:12,766 iteration 5689 : loss : 0.036330, loss_ce: 0.009200
2022-01-06 14:37:13,884 iteration 5690 : loss : 0.019608, loss_ce: 0.008202
2022-01-06 14:37:15,019 iteration 5691 : loss : 0.022650, loss_ce: 0.009448
2022-01-06 14:37:16,156 iteration 5692 : loss : 0.021941, loss_ce: 0.009331
2022-01-06 14:37:17,264 iteration 5693 : loss : 0.024021, loss_ce: 0.006709
2022-01-06 14:37:18,354 iteration 5694 : loss : 0.020701, loss_ce: 0.009551
2022-01-06 14:37:18,354 Training Data Eval:
2022-01-06 14:37:23,891   Average segmentation loss on training set: 0.0129
2022-01-06 14:37:23,891 Validation Data Eval:
2022-01-06 14:37:25,787   Average segmentation loss on validation set: 0.0918
2022-01-06 14:37:26,856 iteration 5695 : loss : 0.015816, loss_ce: 0.004881
 84%|████████████████████████▎    | 335/400 [1:55:53<23:27, 21.65s/it]2022-01-06 14:37:27,981 iteration 5696 : loss : 0.016598, loss_ce: 0.008513
2022-01-06 14:37:29,051 iteration 5697 : loss : 0.015342, loss_ce: 0.006664
2022-01-06 14:37:30,218 iteration 5698 : loss : 0.028844, loss_ce: 0.012393
2022-01-06 14:37:31,260 iteration 5699 : loss : 0.016942, loss_ce: 0.007320
2022-01-06 14:37:32,345 iteration 5700 : loss : 0.017786, loss_ce: 0.006059
2022-01-06 14:37:33,400 iteration 5701 : loss : 0.018124, loss_ce: 0.007577
2022-01-06 14:37:34,603 iteration 5702 : loss : 0.018359, loss_ce: 0.006829
2022-01-06 14:37:35,744 iteration 5703 : loss : 0.018490, loss_ce: 0.007566
2022-01-06 14:37:36,858 iteration 5704 : loss : 0.025713, loss_ce: 0.010348
2022-01-06 14:37:37,982 iteration 5705 : loss : 0.021485, loss_ce: 0.007586
2022-01-06 14:37:39,005 iteration 5706 : loss : 0.016624, loss_ce: 0.005860
2022-01-06 14:37:40,091 iteration 5707 : loss : 0.021087, loss_ce: 0.006902
2022-01-06 14:37:41,152 iteration 5708 : loss : 0.021188, loss_ce: 0.007522
2022-01-06 14:37:42,277 iteration 5709 : loss : 0.018688, loss_ce: 0.005717
2022-01-06 14:37:43,384 iteration 5710 : loss : 0.027158, loss_ce: 0.007126
2022-01-06 14:37:44,437 iteration 5711 : loss : 0.019633, loss_ce: 0.006421
2022-01-06 14:37:45,574 iteration 5712 : loss : 0.025941, loss_ce: 0.012064
 84%|████████████████████████▎    | 336/400 [1:56:12<22:09, 20.77s/it]2022-01-06 14:37:46,795 iteration 5713 : loss : 0.027042, loss_ce: 0.009377
2022-01-06 14:37:47,957 iteration 5714 : loss : 0.027168, loss_ce: 0.008258
2022-01-06 14:37:49,138 iteration 5715 : loss : 0.018455, loss_ce: 0.008483
2022-01-06 14:37:50,241 iteration 5716 : loss : 0.017331, loss_ce: 0.007083
2022-01-06 14:37:51,450 iteration 5717 : loss : 0.020226, loss_ce: 0.007864
2022-01-06 14:37:52,525 iteration 5718 : loss : 0.015509, loss_ce: 0.005252
2022-01-06 14:37:53,604 iteration 5719 : loss : 0.020112, loss_ce: 0.008994
2022-01-06 14:37:54,765 iteration 5720 : loss : 0.026758, loss_ce: 0.010311
2022-01-06 14:37:55,895 iteration 5721 : loss : 0.025727, loss_ce: 0.012323
2022-01-06 14:37:56,997 iteration 5722 : loss : 0.019153, loss_ce: 0.008527
2022-01-06 14:37:58,120 iteration 5723 : loss : 0.022495, loss_ce: 0.009256
2022-01-06 14:37:59,268 iteration 5724 : loss : 0.018445, loss_ce: 0.006397
2022-01-06 14:38:00,386 iteration 5725 : loss : 0.016667, loss_ce: 0.005867
2022-01-06 14:38:01,448 iteration 5726 : loss : 0.023033, loss_ce: 0.005347
2022-01-06 14:38:02,569 iteration 5727 : loss : 0.022828, loss_ce: 0.007360
2022-01-06 14:38:03,650 iteration 5728 : loss : 0.022751, loss_ce: 0.008667
2022-01-06 14:38:04,817 iteration 5729 : loss : 0.022925, loss_ce: 0.008659
 84%|████████████████████████▍    | 337/400 [1:56:31<21:19, 20.32s/it]2022-01-06 14:38:05,930 iteration 5730 : loss : 0.017570, loss_ce: 0.008345
2022-01-06 14:38:07,085 iteration 5731 : loss : 0.032026, loss_ce: 0.016714
2022-01-06 14:38:08,149 iteration 5732 : loss : 0.017781, loss_ce: 0.006510
2022-01-06 14:38:09,311 iteration 5733 : loss : 0.029231, loss_ce: 0.007733
2022-01-06 14:38:10,462 iteration 5734 : loss : 0.018402, loss_ce: 0.007639
2022-01-06 14:38:11,546 iteration 5735 : loss : 0.034795, loss_ce: 0.014008
2022-01-06 14:38:12,629 iteration 5736 : loss : 0.024103, loss_ce: 0.008638
2022-01-06 14:38:13,812 iteration 5737 : loss : 0.030950, loss_ce: 0.011962
2022-01-06 14:38:14,880 iteration 5738 : loss : 0.022231, loss_ce: 0.007758
2022-01-06 14:38:15,951 iteration 5739 : loss : 0.017797, loss_ce: 0.006399
2022-01-06 14:38:17,050 iteration 5740 : loss : 0.024511, loss_ce: 0.010756
2022-01-06 14:38:18,163 iteration 5741 : loss : 0.020801, loss_ce: 0.009696
2022-01-06 14:38:19,296 iteration 5742 : loss : 0.018384, loss_ce: 0.003786
2022-01-06 14:38:20,456 iteration 5743 : loss : 0.021390, loss_ce: 0.009786
2022-01-06 14:38:21,646 iteration 5744 : loss : 0.030205, loss_ce: 0.012378
2022-01-06 14:38:22,836 iteration 5745 : loss : 0.029160, loss_ce: 0.011026
2022-01-06 14:38:23,935 iteration 5746 : loss : 0.018134, loss_ce: 0.004655
 84%|████████████████████████▌    | 338/400 [1:56:50<20:37, 19.96s/it]2022-01-06 14:38:25,127 iteration 5747 : loss : 0.021173, loss_ce: 0.007090
2022-01-06 14:38:26,263 iteration 5748 : loss : 0.019763, loss_ce: 0.007957
2022-01-06 14:38:27,389 iteration 5749 : loss : 0.024813, loss_ce: 0.009549
2022-01-06 14:38:28,503 iteration 5750 : loss : 0.023669, loss_ce: 0.007536
2022-01-06 14:38:29,555 iteration 5751 : loss : 0.021256, loss_ce: 0.008029
2022-01-06 14:38:30,651 iteration 5752 : loss : 0.022319, loss_ce: 0.007013
2022-01-06 14:38:31,813 iteration 5753 : loss : 0.016438, loss_ce: 0.006036
2022-01-06 14:38:32,983 iteration 5754 : loss : 0.018461, loss_ce: 0.005402
2022-01-06 14:38:34,100 iteration 5755 : loss : 0.016814, loss_ce: 0.005368
2022-01-06 14:38:35,187 iteration 5756 : loss : 0.018717, loss_ce: 0.006753
2022-01-06 14:38:36,199 iteration 5757 : loss : 0.014740, loss_ce: 0.005827
2022-01-06 14:38:37,365 iteration 5758 : loss : 0.030554, loss_ce: 0.011736
2022-01-06 14:38:38,525 iteration 5759 : loss : 0.017450, loss_ce: 0.007509
2022-01-06 14:38:39,716 iteration 5760 : loss : 0.018942, loss_ce: 0.009257
2022-01-06 14:38:40,917 iteration 5761 : loss : 0.032636, loss_ce: 0.012805
2022-01-06 14:38:41,967 iteration 5762 : loss : 0.016232, loss_ce: 0.007482
2022-01-06 14:38:43,111 iteration 5763 : loss : 0.016581, loss_ce: 0.006551
 85%|████████████████████████▌    | 339/400 [1:57:10<20:02, 19.72s/it]2022-01-06 14:38:44,271 iteration 5764 : loss : 0.023132, loss_ce: 0.009510
2022-01-06 14:38:45,443 iteration 5765 : loss : 0.040444, loss_ce: 0.009178
2022-01-06 14:38:46,573 iteration 5766 : loss : 0.022131, loss_ce: 0.011778
2022-01-06 14:38:47,672 iteration 5767 : loss : 0.018477, loss_ce: 0.010185
2022-01-06 14:38:48,877 iteration 5768 : loss : 0.023968, loss_ce: 0.009244
2022-01-06 14:38:50,039 iteration 5769 : loss : 0.019331, loss_ce: 0.006109
2022-01-06 14:38:51,107 iteration 5770 : loss : 0.015376, loss_ce: 0.005813
2022-01-06 14:38:52,228 iteration 5771 : loss : 0.024285, loss_ce: 0.008974
2022-01-06 14:38:53,320 iteration 5772 : loss : 0.020685, loss_ce: 0.006895
2022-01-06 14:38:54,358 iteration 5773 : loss : 0.020646, loss_ce: 0.007030
2022-01-06 14:38:55,382 iteration 5774 : loss : 0.020755, loss_ce: 0.005414
2022-01-06 14:38:56,507 iteration 5775 : loss : 0.016396, loss_ce: 0.006569
2022-01-06 14:38:57,685 iteration 5776 : loss : 0.023505, loss_ce: 0.010630
2022-01-06 14:38:58,711 iteration 5777 : loss : 0.021756, loss_ce: 0.009230
2022-01-06 14:38:59,791 iteration 5778 : loss : 0.031299, loss_ce: 0.009877
2022-01-06 14:39:00,884 iteration 5779 : loss : 0.013706, loss_ce: 0.004743
2022-01-06 14:39:00,885 Training Data Eval:
2022-01-06 14:39:06,393   Average segmentation loss on training set: 0.0131
2022-01-06 14:39:06,394 Validation Data Eval:
2022-01-06 14:39:08,278   Average segmentation loss on validation set: 0.0959
2022-01-06 14:39:09,376 iteration 5780 : loss : 0.020179, loss_ce: 0.006589
 85%|████████████████████████▋    | 340/400 [1:57:36<21:41, 21.69s/it]2022-01-06 14:39:10,510 iteration 5781 : loss : 0.028115, loss_ce: 0.007323
2022-01-06 14:39:11,673 iteration 5782 : loss : 0.026693, loss_ce: 0.009815
2022-01-06 14:39:12,770 iteration 5783 : loss : 0.020122, loss_ce: 0.007267
2022-01-06 14:39:13,893 iteration 5784 : loss : 0.021285, loss_ce: 0.011450
2022-01-06 14:39:14,956 iteration 5785 : loss : 0.019140, loss_ce: 0.007709
2022-01-06 14:39:16,039 iteration 5786 : loss : 0.018369, loss_ce: 0.008162
2022-01-06 14:39:17,124 iteration 5787 : loss : 0.019193, loss_ce: 0.007147
2022-01-06 14:39:18,366 iteration 5788 : loss : 0.024956, loss_ce: 0.008528
2022-01-06 14:39:19,504 iteration 5789 : loss : 0.015338, loss_ce: 0.005242
2022-01-06 14:39:20,628 iteration 5790 : loss : 0.018474, loss_ce: 0.005806
2022-01-06 14:39:21,709 iteration 5791 : loss : 0.018413, loss_ce: 0.006804
2022-01-06 14:39:22,859 iteration 5792 : loss : 0.021064, loss_ce: 0.007406
2022-01-06 14:39:24,085 iteration 5793 : loss : 0.018294, loss_ce: 0.006655
2022-01-06 14:39:25,212 iteration 5794 : loss : 0.018631, loss_ce: 0.007336
2022-01-06 14:39:26,320 iteration 5795 : loss : 0.017108, loss_ce: 0.005790
2022-01-06 14:39:27,393 iteration 5796 : loss : 0.014099, loss_ce: 0.003697
2022-01-06 14:39:28,530 iteration 5797 : loss : 0.030509, loss_ce: 0.012054
 85%|████████████████████████▋    | 341/400 [1:57:55<20:34, 20.92s/it]2022-01-06 14:39:29,740 iteration 5798 : loss : 0.026246, loss_ce: 0.009787
2022-01-06 14:39:30,874 iteration 5799 : loss : 0.019481, loss_ce: 0.006962
2022-01-06 14:39:31,989 iteration 5800 : loss : 0.015247, loss_ce: 0.006269
2022-01-06 14:39:33,112 iteration 5801 : loss : 0.061855, loss_ce: 0.027732
2022-01-06 14:39:34,185 iteration 5802 : loss : 0.016248, loss_ce: 0.006941
2022-01-06 14:39:35,283 iteration 5803 : loss : 0.023421, loss_ce: 0.011521
2022-01-06 14:39:36,399 iteration 5804 : loss : 0.016540, loss_ce: 0.005676
2022-01-06 14:39:37,600 iteration 5805 : loss : 0.021184, loss_ce: 0.007486
2022-01-06 14:39:38,696 iteration 5806 : loss : 0.024813, loss_ce: 0.007542
2022-01-06 14:39:39,876 iteration 5807 : loss : 0.035403, loss_ce: 0.013034
2022-01-06 14:39:41,004 iteration 5808 : loss : 0.020877, loss_ce: 0.010234
2022-01-06 14:39:42,055 iteration 5809 : loss : 0.017811, loss_ce: 0.004383
2022-01-06 14:39:43,130 iteration 5810 : loss : 0.024640, loss_ce: 0.009766
2022-01-06 14:39:44,176 iteration 5811 : loss : 0.014319, loss_ce: 0.004256
2022-01-06 14:39:45,291 iteration 5812 : loss : 0.020861, loss_ce: 0.006920
2022-01-06 14:39:46,338 iteration 5813 : loss : 0.023347, loss_ce: 0.010246
2022-01-06 14:39:47,463 iteration 5814 : loss : 0.025692, loss_ce: 0.011569
 86%|████████████████████████▊    | 342/400 [1:58:14<19:39, 20.33s/it]2022-01-06 14:39:48,562 iteration 5815 : loss : 0.016256, loss_ce: 0.005348
2022-01-06 14:39:49,650 iteration 5816 : loss : 0.014026, loss_ce: 0.006381
2022-01-06 14:39:50,720 iteration 5817 : loss : 0.015423, loss_ce: 0.005722
2022-01-06 14:39:51,735 iteration 5818 : loss : 0.019455, loss_ce: 0.008183
2022-01-06 14:39:52,894 iteration 5819 : loss : 0.018826, loss_ce: 0.005850
2022-01-06 14:39:54,072 iteration 5820 : loss : 0.022779, loss_ce: 0.011605
2022-01-06 14:39:55,113 iteration 5821 : loss : 0.016712, loss_ce: 0.004983
2022-01-06 14:39:56,187 iteration 5822 : loss : 0.014897, loss_ce: 0.006420
2022-01-06 14:39:57,394 iteration 5823 : loss : 0.028014, loss_ce: 0.010690
2022-01-06 14:39:58,502 iteration 5824 : loss : 0.017672, loss_ce: 0.005216
2022-01-06 14:39:59,649 iteration 5825 : loss : 0.023274, loss_ce: 0.010557
2022-01-06 14:40:00,797 iteration 5826 : loss : 0.023742, loss_ce: 0.005677
2022-01-06 14:40:01,906 iteration 5827 : loss : 0.022096, loss_ce: 0.006626
2022-01-06 14:40:03,113 iteration 5828 : loss : 0.038056, loss_ce: 0.013259
2022-01-06 14:40:04,220 iteration 5829 : loss : 0.020116, loss_ce: 0.008482
2022-01-06 14:40:05,289 iteration 5830 : loss : 0.021249, loss_ce: 0.007985
2022-01-06 14:40:06,371 iteration 5831 : loss : 0.026008, loss_ce: 0.008884
 86%|████████████████████████▊    | 343/400 [1:58:33<18:54, 19.90s/it]2022-01-06 14:40:07,537 iteration 5832 : loss : 0.015941, loss_ce: 0.004566
2022-01-06 14:40:08,681 iteration 5833 : loss : 0.018787, loss_ce: 0.008303
2022-01-06 14:40:09,740 iteration 5834 : loss : 0.016358, loss_ce: 0.005781
2022-01-06 14:40:10,817 iteration 5835 : loss : 0.027614, loss_ce: 0.009585
2022-01-06 14:40:11,989 iteration 5836 : loss : 0.027458, loss_ce: 0.013226
2022-01-06 14:40:13,123 iteration 5837 : loss : 0.019104, loss_ce: 0.004641
2022-01-06 14:40:14,173 iteration 5838 : loss : 0.012844, loss_ce: 0.003713
2022-01-06 14:40:15,293 iteration 5839 : loss : 0.017475, loss_ce: 0.006075
2022-01-06 14:40:16,456 iteration 5840 : loss : 0.021025, loss_ce: 0.007224
2022-01-06 14:40:17,556 iteration 5841 : loss : 0.018348, loss_ce: 0.007752
2022-01-06 14:40:18,662 iteration 5842 : loss : 0.013682, loss_ce: 0.005615
2022-01-06 14:40:19,757 iteration 5843 : loss : 0.019868, loss_ce: 0.007949
2022-01-06 14:40:21,001 iteration 5844 : loss : 0.031990, loss_ce: 0.008889
2022-01-06 14:40:22,117 iteration 5845 : loss : 0.020976, loss_ce: 0.007896
2022-01-06 14:40:23,241 iteration 5846 : loss : 0.035893, loss_ce: 0.013362
2022-01-06 14:40:24,375 iteration 5847 : loss : 0.022879, loss_ce: 0.009108
2022-01-06 14:40:25,480 iteration 5848 : loss : 0.020033, loss_ce: 0.006976
 86%|████████████████████████▉    | 344/400 [1:58:52<18:21, 19.67s/it]2022-01-06 14:40:26,641 iteration 5849 : loss : 0.024993, loss_ce: 0.009666
2022-01-06 14:40:27,687 iteration 5850 : loss : 0.015639, loss_ce: 0.006264
2022-01-06 14:40:28,792 iteration 5851 : loss : 0.017678, loss_ce: 0.007533
2022-01-06 14:40:29,879 iteration 5852 : loss : 0.019007, loss_ce: 0.006915
2022-01-06 14:40:30,916 iteration 5853 : loss : 0.016805, loss_ce: 0.006018
2022-01-06 14:40:32,002 iteration 5854 : loss : 0.026452, loss_ce: 0.009793
2022-01-06 14:40:33,191 iteration 5855 : loss : 0.021058, loss_ce: 0.008825
2022-01-06 14:40:34,308 iteration 5856 : loss : 0.018740, loss_ce: 0.008009
2022-01-06 14:40:35,442 iteration 5857 : loss : 0.016944, loss_ce: 0.006051
2022-01-06 14:40:36,502 iteration 5858 : loss : 0.016662, loss_ce: 0.005910
2022-01-06 14:40:37,676 iteration 5859 : loss : 0.017233, loss_ce: 0.007231
2022-01-06 14:40:38,846 iteration 5860 : loss : 0.017734, loss_ce: 0.004809
2022-01-06 14:40:39,943 iteration 5861 : loss : 0.018188, loss_ce: 0.005447
2022-01-06 14:40:41,070 iteration 5862 : loss : 0.017379, loss_ce: 0.007728
2022-01-06 14:40:42,095 iteration 5863 : loss : 0.013799, loss_ce: 0.004599
2022-01-06 14:40:43,218 iteration 5864 : loss : 0.021769, loss_ce: 0.006733
2022-01-06 14:40:43,219 Training Data Eval:
2022-01-06 14:40:48,767   Average segmentation loss on training set: 0.0117
2022-01-06 14:40:48,767 Validation Data Eval:
2022-01-06 14:40:50,654   Average segmentation loss on validation set: 0.0738
2022-01-06 14:40:51,713 iteration 5865 : loss : 0.019163, loss_ce: 0.006705
 86%|█████████████████████████    | 345/400 [1:59:18<19:49, 21.64s/it]2022-01-06 14:40:52,958 iteration 5866 : loss : 0.016133, loss_ce: 0.007092
2022-01-06 14:40:54,117 iteration 5867 : loss : 0.021620, loss_ce: 0.006477
2022-01-06 14:40:55,263 iteration 5868 : loss : 0.023429, loss_ce: 0.007860
2022-01-06 14:40:56,296 iteration 5869 : loss : 0.021101, loss_ce: 0.010288
2022-01-06 14:40:57,490 iteration 5870 : loss : 0.022899, loss_ce: 0.005044
2022-01-06 14:40:58,627 iteration 5871 : loss : 0.015083, loss_ce: 0.005218
2022-01-06 14:40:59,775 iteration 5872 : loss : 0.025105, loss_ce: 0.009276
2022-01-06 14:41:01,007 iteration 5873 : loss : 0.028264, loss_ce: 0.008667
2022-01-06 14:41:02,074 iteration 5874 : loss : 0.016934, loss_ce: 0.005631
2022-01-06 14:41:03,165 iteration 5875 : loss : 0.018314, loss_ce: 0.009283
2022-01-06 14:41:04,280 iteration 5876 : loss : 0.020769, loss_ce: 0.007427
2022-01-06 14:41:05,392 iteration 5877 : loss : 0.019683, loss_ce: 0.007336
2022-01-06 14:41:06,535 iteration 5878 : loss : 0.019633, loss_ce: 0.007963
2022-01-06 14:41:07,617 iteration 5879 : loss : 0.017510, loss_ce: 0.007640
2022-01-06 14:41:08,830 iteration 5880 : loss : 0.023553, loss_ce: 0.006273
2022-01-06 14:41:09,886 iteration 5881 : loss : 0.014063, loss_ce: 0.005186
2022-01-06 14:41:10,979 iteration 5882 : loss : 0.017539, loss_ce: 0.008268
 86%|█████████████████████████    | 346/400 [1:59:37<18:49, 20.93s/it]2022-01-06 14:41:12,124 iteration 5883 : loss : 0.017301, loss_ce: 0.006282
2022-01-06 14:41:13,206 iteration 5884 : loss : 0.014292, loss_ce: 0.005789
2022-01-06 14:41:14,434 iteration 5885 : loss : 0.038585, loss_ce: 0.014517
2022-01-06 14:41:15,489 iteration 5886 : loss : 0.016319, loss_ce: 0.004626
2022-01-06 14:41:16,570 iteration 5887 : loss : 0.015323, loss_ce: 0.006908
2022-01-06 14:41:17,760 iteration 5888 : loss : 0.023654, loss_ce: 0.007573
2022-01-06 14:41:18,942 iteration 5889 : loss : 0.019721, loss_ce: 0.007752
2022-01-06 14:41:20,060 iteration 5890 : loss : 0.014464, loss_ce: 0.004668
2022-01-06 14:41:21,219 iteration 5891 : loss : 0.024604, loss_ce: 0.008460
2022-01-06 14:41:22,422 iteration 5892 : loss : 0.029091, loss_ce: 0.011509
2022-01-06 14:41:23,489 iteration 5893 : loss : 0.014442, loss_ce: 0.004685
2022-01-06 14:41:24,675 iteration 5894 : loss : 0.030586, loss_ce: 0.012233
2022-01-06 14:41:25,819 iteration 5895 : loss : 0.029546, loss_ce: 0.012823
2022-01-06 14:41:26,919 iteration 5896 : loss : 0.014776, loss_ce: 0.006595
2022-01-06 14:41:28,096 iteration 5897 : loss : 0.046847, loss_ce: 0.020473
2022-01-06 14:41:29,252 iteration 5898 : loss : 0.015480, loss_ce: 0.005242
2022-01-06 14:41:30,327 iteration 5899 : loss : 0.020204, loss_ce: 0.007547
 87%|█████████████████████████▏   | 347/400 [1:59:57<18:03, 20.45s/it]2022-01-06 14:41:31,514 iteration 5900 : loss : 0.018584, loss_ce: 0.005934
2022-01-06 14:41:32,696 iteration 5901 : loss : 0.020424, loss_ce: 0.007098
2022-01-06 14:41:33,798 iteration 5902 : loss : 0.017983, loss_ce: 0.007060
2022-01-06 14:41:34,968 iteration 5903 : loss : 0.016977, loss_ce: 0.008505
2022-01-06 14:41:36,059 iteration 5904 : loss : 0.031001, loss_ce: 0.014331
2022-01-06 14:41:37,207 iteration 5905 : loss : 0.028198, loss_ce: 0.008452
2022-01-06 14:41:38,262 iteration 5906 : loss : 0.021329, loss_ce: 0.007517
2022-01-06 14:41:39,392 iteration 5907 : loss : 0.015813, loss_ce: 0.005234
2022-01-06 14:41:40,511 iteration 5908 : loss : 0.017582, loss_ce: 0.006236
2022-01-06 14:41:41,527 iteration 5909 : loss : 0.019435, loss_ce: 0.005280
2022-01-06 14:41:42,634 iteration 5910 : loss : 0.022594, loss_ce: 0.007580
2022-01-06 14:41:43,776 iteration 5911 : loss : 0.023712, loss_ce: 0.009127
2022-01-06 14:41:44,822 iteration 5912 : loss : 0.018871, loss_ce: 0.006420
2022-01-06 14:41:45,907 iteration 5913 : loss : 0.018184, loss_ce: 0.008428
2022-01-06 14:41:47,030 iteration 5914 : loss : 0.021006, loss_ce: 0.007276
2022-01-06 14:41:48,068 iteration 5915 : loss : 0.015985, loss_ce: 0.006769
2022-01-06 14:41:49,232 iteration 5916 : loss : 0.029866, loss_ce: 0.010942
 87%|█████████████████████████▏   | 348/400 [2:00:16<17:19, 19.98s/it]2022-01-06 14:41:50,373 iteration 5917 : loss : 0.015822, loss_ce: 0.006461
2022-01-06 14:41:51,421 iteration 5918 : loss : 0.018612, loss_ce: 0.007324
2022-01-06 14:41:52,567 iteration 5919 : loss : 0.022705, loss_ce: 0.009508
2022-01-06 14:41:53,726 iteration 5920 : loss : 0.021868, loss_ce: 0.011108
2022-01-06 14:41:54,978 iteration 5921 : loss : 0.023417, loss_ce: 0.007366
2022-01-06 14:41:56,074 iteration 5922 : loss : 0.021013, loss_ce: 0.007151
2022-01-06 14:41:57,207 iteration 5923 : loss : 0.019781, loss_ce: 0.006260
2022-01-06 14:41:58,424 iteration 5924 : loss : 0.027571, loss_ce: 0.005700
2022-01-06 14:41:59,552 iteration 5925 : loss : 0.017087, loss_ce: 0.006303
2022-01-06 14:42:00,662 iteration 5926 : loss : 0.019337, loss_ce: 0.007614
2022-01-06 14:42:01,784 iteration 5927 : loss : 0.023228, loss_ce: 0.008926
2022-01-06 14:42:02,825 iteration 5928 : loss : 0.018573, loss_ce: 0.005080
2022-01-06 14:42:03,865 iteration 5929 : loss : 0.018778, loss_ce: 0.008607
2022-01-06 14:42:04,912 iteration 5930 : loss : 0.022122, loss_ce: 0.009227
2022-01-06 14:42:06,023 iteration 5931 : loss : 0.028354, loss_ce: 0.007571
2022-01-06 14:42:07,169 iteration 5932 : loss : 0.022345, loss_ce: 0.008637
2022-01-06 14:42:08,259 iteration 5933 : loss : 0.018921, loss_ce: 0.006753
 87%|█████████████████████████▎   | 349/400 [2:00:35<16:44, 19.70s/it]2022-01-06 14:42:09,369 iteration 5934 : loss : 0.020608, loss_ce: 0.006847
2022-01-06 14:42:10,598 iteration 5935 : loss : 0.030080, loss_ce: 0.011759
2022-01-06 14:42:11,706 iteration 5936 : loss : 0.014249, loss_ce: 0.005667
2022-01-06 14:42:12,782 iteration 5937 : loss : 0.016619, loss_ce: 0.005457
2022-01-06 14:42:13,811 iteration 5938 : loss : 0.013421, loss_ce: 0.004886
2022-01-06 14:42:14,949 iteration 5939 : loss : 0.016391, loss_ce: 0.007235
2022-01-06 14:42:15,997 iteration 5940 : loss : 0.014997, loss_ce: 0.006067
2022-01-06 14:42:17,065 iteration 5941 : loss : 0.018349, loss_ce: 0.007096
2022-01-06 14:42:18,144 iteration 5942 : loss : 0.017905, loss_ce: 0.007822
2022-01-06 14:42:19,261 iteration 5943 : loss : 0.018992, loss_ce: 0.005479
2022-01-06 14:42:20,376 iteration 5944 : loss : 0.020266, loss_ce: 0.010982
2022-01-06 14:42:21,452 iteration 5945 : loss : 0.020540, loss_ce: 0.004485
2022-01-06 14:42:22,653 iteration 5946 : loss : 0.026900, loss_ce: 0.010536
2022-01-06 14:42:23,796 iteration 5947 : loss : 0.022360, loss_ce: 0.008327
2022-01-06 14:42:24,896 iteration 5948 : loss : 0.021331, loss_ce: 0.008306
2022-01-06 14:42:26,097 iteration 5949 : loss : 0.018255, loss_ce: 0.006374
2022-01-06 14:42:26,097 Training Data Eval:
2022-01-06 14:42:31,768   Average segmentation loss on training set: 0.0130
2022-01-06 14:42:31,769 Validation Data Eval:
2022-01-06 14:42:33,691   Average segmentation loss on validation set: 0.1007
2022-01-06 14:42:34,828 iteration 5950 : loss : 0.018707, loss_ce: 0.005639
 88%|█████████████████████████▍   | 350/400 [2:01:01<18:08, 21.76s/it]2022-01-06 14:42:35,959 iteration 5951 : loss : 0.015151, loss_ce: 0.005543
2022-01-06 14:42:37,078 iteration 5952 : loss : 0.018236, loss_ce: 0.006092
2022-01-06 14:42:38,197 iteration 5953 : loss : 0.024686, loss_ce: 0.010087
2022-01-06 14:42:39,278 iteration 5954 : loss : 0.046598, loss_ce: 0.014081
2022-01-06 14:42:40,400 iteration 5955 : loss : 0.020364, loss_ce: 0.006201
2022-01-06 14:42:41,556 iteration 5956 : loss : 0.029255, loss_ce: 0.015302
2022-01-06 14:42:42,683 iteration 5957 : loss : 0.014736, loss_ce: 0.005571
2022-01-06 14:42:43,759 iteration 5958 : loss : 0.016906, loss_ce: 0.006826
2022-01-06 14:42:44,870 iteration 5959 : loss : 0.017085, loss_ce: 0.006893
2022-01-06 14:42:45,936 iteration 5960 : loss : 0.020206, loss_ce: 0.006200
2022-01-06 14:42:46,994 iteration 5961 : loss : 0.022290, loss_ce: 0.005066
2022-01-06 14:42:48,091 iteration 5962 : loss : 0.034375, loss_ce: 0.009547
2022-01-06 14:42:49,227 iteration 5963 : loss : 0.024975, loss_ce: 0.009427
2022-01-06 14:42:50,301 iteration 5964 : loss : 0.020179, loss_ce: 0.009558
2022-01-06 14:42:51,346 iteration 5965 : loss : 0.018260, loss_ce: 0.008277
2022-01-06 14:42:52,499 iteration 5966 : loss : 0.020730, loss_ce: 0.005984
2022-01-06 14:42:53,621 iteration 5967 : loss : 0.022363, loss_ce: 0.008895
 88%|█████████████████████████▍   | 351/400 [2:01:20<17:02, 20.87s/it]2022-01-06 14:42:54,856 iteration 5968 : loss : 0.020530, loss_ce: 0.007902
2022-01-06 14:42:55,935 iteration 5969 : loss : 0.019462, loss_ce: 0.007992
2022-01-06 14:42:57,065 iteration 5970 : loss : 0.024593, loss_ce: 0.009942
2022-01-06 14:42:58,102 iteration 5971 : loss : 0.017234, loss_ce: 0.005007
2022-01-06 14:42:59,122 iteration 5972 : loss : 0.014156, loss_ce: 0.004435
2022-01-06 14:43:00,236 iteration 5973 : loss : 0.021703, loss_ce: 0.009925
2022-01-06 14:43:01,442 iteration 5974 : loss : 0.027370, loss_ce: 0.011951
2022-01-06 14:43:02,502 iteration 5975 : loss : 0.018842, loss_ce: 0.006275
2022-01-06 14:43:03,583 iteration 5976 : loss : 0.025503, loss_ce: 0.007466
2022-01-06 14:43:04,634 iteration 5977 : loss : 0.020378, loss_ce: 0.005803
2022-01-06 14:43:05,682 iteration 5978 : loss : 0.015958, loss_ce: 0.007077
2022-01-06 14:43:06,779 iteration 5979 : loss : 0.015210, loss_ce: 0.006647
2022-01-06 14:43:07,919 iteration 5980 : loss : 0.018253, loss_ce: 0.006636
2022-01-06 14:43:08,983 iteration 5981 : loss : 0.017943, loss_ce: 0.008736
2022-01-06 14:43:10,081 iteration 5982 : loss : 0.017796, loss_ce: 0.006899
2022-01-06 14:43:11,241 iteration 5983 : loss : 0.034045, loss_ce: 0.009871
2022-01-06 14:43:12,309 iteration 5984 : loss : 0.017885, loss_ce: 0.005898
 88%|█████████████████████████▌   | 352/400 [2:01:39<16:10, 20.22s/it]2022-01-06 14:43:13,469 iteration 5985 : loss : 0.017674, loss_ce: 0.005943
2022-01-06 14:43:14,484 iteration 5986 : loss : 0.021540, loss_ce: 0.006962
2022-01-06 14:43:15,621 iteration 5987 : loss : 0.018068, loss_ce: 0.007643
2022-01-06 14:43:16,785 iteration 5988 : loss : 0.019792, loss_ce: 0.008189
2022-01-06 14:43:17,912 iteration 5989 : loss : 0.017833, loss_ce: 0.006529
2022-01-06 14:43:18,994 iteration 5990 : loss : 0.016880, loss_ce: 0.005685
2022-01-06 14:43:20,120 iteration 5991 : loss : 0.022563, loss_ce: 0.007281
2022-01-06 14:43:21,237 iteration 5992 : loss : 0.019191, loss_ce: 0.006108
2022-01-06 14:43:22,341 iteration 5993 : loss : 0.015505, loss_ce: 0.006405
2022-01-06 14:43:23,499 iteration 5994 : loss : 0.034729, loss_ce: 0.013597
2022-01-06 14:43:24,604 iteration 5995 : loss : 0.016098, loss_ce: 0.006504
2022-01-06 14:43:25,668 iteration 5996 : loss : 0.014141, loss_ce: 0.004159
2022-01-06 14:43:26,803 iteration 5997 : loss : 0.020755, loss_ce: 0.007957
2022-01-06 14:43:27,988 iteration 5998 : loss : 0.018189, loss_ce: 0.006158
2022-01-06 14:43:29,198 iteration 5999 : loss : 0.026350, loss_ce: 0.008817
2022-01-06 14:43:30,322 iteration 6000 : loss : 0.021437, loss_ce: 0.009170
2022-01-06 14:43:31,348 iteration 6001 : loss : 0.017108, loss_ce: 0.008318
 88%|█████████████████████████▌   | 353/400 [2:01:58<15:33, 19.86s/it]2022-01-06 14:43:32,522 iteration 6002 : loss : 0.016395, loss_ce: 0.006174
2022-01-06 14:43:33,597 iteration 6003 : loss : 0.016415, loss_ce: 0.005763
2022-01-06 14:43:34,745 iteration 6004 : loss : 0.020052, loss_ce: 0.009737
2022-01-06 14:43:35,879 iteration 6005 : loss : 0.020246, loss_ce: 0.007362
2022-01-06 14:43:37,085 iteration 6006 : loss : 0.017998, loss_ce: 0.007639
2022-01-06 14:43:38,208 iteration 6007 : loss : 0.012489, loss_ce: 0.004781
2022-01-06 14:43:39,323 iteration 6008 : loss : 0.021602, loss_ce: 0.007456
2022-01-06 14:43:40,355 iteration 6009 : loss : 0.014358, loss_ce: 0.006595
2022-01-06 14:43:41,415 iteration 6010 : loss : 0.019836, loss_ce: 0.010044
2022-01-06 14:43:42,567 iteration 6011 : loss : 0.019641, loss_ce: 0.005543
2022-01-06 14:43:43,746 iteration 6012 : loss : 0.023285, loss_ce: 0.005583
2022-01-06 14:43:44,900 iteration 6013 : loss : 0.021444, loss_ce: 0.007055
2022-01-06 14:43:46,041 iteration 6014 : loss : 0.025684, loss_ce: 0.008490
2022-01-06 14:43:47,118 iteration 6015 : loss : 0.018231, loss_ce: 0.007166
2022-01-06 14:43:48,224 iteration 6016 : loss : 0.016843, loss_ce: 0.006147
2022-01-06 14:43:49,324 iteration 6017 : loss : 0.019971, loss_ce: 0.009236
2022-01-06 14:43:50,452 iteration 6018 : loss : 0.027973, loss_ce: 0.012343
 88%|█████████████████████████▋   | 354/400 [2:02:17<15:03, 19.63s/it]2022-01-06 14:43:51,634 iteration 6019 : loss : 0.032722, loss_ce: 0.017106
2022-01-06 14:43:52,716 iteration 6020 : loss : 0.018787, loss_ce: 0.008702
2022-01-06 14:43:53,830 iteration 6021 : loss : 0.029289, loss_ce: 0.007125
2022-01-06 14:43:54,877 iteration 6022 : loss : 0.018764, loss_ce: 0.007792
2022-01-06 14:43:56,015 iteration 6023 : loss : 0.020437, loss_ce: 0.008004
2022-01-06 14:43:57,098 iteration 6024 : loss : 0.021119, loss_ce: 0.007917
2022-01-06 14:43:58,181 iteration 6025 : loss : 0.020072, loss_ce: 0.007729
2022-01-06 14:43:59,314 iteration 6026 : loss : 0.017754, loss_ce: 0.006094
2022-01-06 14:44:00,436 iteration 6027 : loss : 0.023121, loss_ce: 0.009171
2022-01-06 14:44:01,577 iteration 6028 : loss : 0.024575, loss_ce: 0.009293
2022-01-06 14:44:02,667 iteration 6029 : loss : 0.023775, loss_ce: 0.006631
2022-01-06 14:44:03,706 iteration 6030 : loss : 0.014838, loss_ce: 0.006934
2022-01-06 14:44:04,829 iteration 6031 : loss : 0.017235, loss_ce: 0.008420
2022-01-06 14:44:05,890 iteration 6032 : loss : 0.017906, loss_ce: 0.005568
2022-01-06 14:44:07,048 iteration 6033 : loss : 0.019668, loss_ce: 0.007318
2022-01-06 14:44:08,168 iteration 6034 : loss : 0.018086, loss_ce: 0.006429
2022-01-06 14:44:08,168 Training Data Eval:
2022-01-06 14:44:13,768   Average segmentation loss on training set: 0.0119
2022-01-06 14:44:13,769 Validation Data Eval:
2022-01-06 14:44:15,698   Average segmentation loss on validation set: 0.0789
2022-01-06 14:44:16,714 iteration 6035 : loss : 0.012565, loss_ce: 0.004898
 89%|█████████████████████████▋   | 355/400 [2:02:43<16:13, 21.63s/it]2022-01-06 14:44:17,983 iteration 6036 : loss : 0.030879, loss_ce: 0.009042
2022-01-06 14:44:19,145 iteration 6037 : loss : 0.021580, loss_ce: 0.006627
2022-01-06 14:44:20,251 iteration 6038 : loss : 0.018947, loss_ce: 0.006855
2022-01-06 14:44:21,344 iteration 6039 : loss : 0.018030, loss_ce: 0.006008
2022-01-06 14:44:22,473 iteration 6040 : loss : 0.020532, loss_ce: 0.007418
2022-01-06 14:44:23,548 iteration 6041 : loss : 0.015290, loss_ce: 0.005249
2022-01-06 14:44:24,671 iteration 6042 : loss : 0.016207, loss_ce: 0.007338
2022-01-06 14:44:25,723 iteration 6043 : loss : 0.014938, loss_ce: 0.006633
2022-01-06 14:44:26,759 iteration 6044 : loss : 0.014554, loss_ce: 0.006200
2022-01-06 14:44:27,933 iteration 6045 : loss : 0.022553, loss_ce: 0.008869
2022-01-06 14:44:28,980 iteration 6046 : loss : 0.018811, loss_ce: 0.007520
2022-01-06 14:44:30,034 iteration 6047 : loss : 0.013046, loss_ce: 0.005671
2022-01-06 14:44:31,209 iteration 6048 : loss : 0.017978, loss_ce: 0.005678
2022-01-06 14:44:32,245 iteration 6049 : loss : 0.015387, loss_ce: 0.005372
2022-01-06 14:44:33,284 iteration 6050 : loss : 0.020879, loss_ce: 0.005463
2022-01-06 14:44:34,321 iteration 6051 : loss : 0.016443, loss_ce: 0.007463
2022-01-06 14:44:35,449 iteration 6052 : loss : 0.015210, loss_ce: 0.005230
 89%|█████████████████████████▊   | 356/400 [2:03:02<15:13, 20.76s/it]2022-01-06 14:44:36,583 iteration 6053 : loss : 0.030437, loss_ce: 0.009931
2022-01-06 14:44:37,650 iteration 6054 : loss : 0.013479, loss_ce: 0.004701
2022-01-06 14:44:38,779 iteration 6055 : loss : 0.021271, loss_ce: 0.008733
2022-01-06 14:44:39,878 iteration 6056 : loss : 0.017339, loss_ce: 0.005826
2022-01-06 14:44:40,917 iteration 6057 : loss : 0.016363, loss_ce: 0.006047
2022-01-06 14:44:41,981 iteration 6058 : loss : 0.013451, loss_ce: 0.006973
2022-01-06 14:44:43,157 iteration 6059 : loss : 0.025616, loss_ce: 0.008815
2022-01-06 14:44:44,289 iteration 6060 : loss : 0.018359, loss_ce: 0.005678
2022-01-06 14:44:45,363 iteration 6061 : loss : 0.016343, loss_ce: 0.007509
2022-01-06 14:44:46,597 iteration 6062 : loss : 0.031530, loss_ce: 0.012984
2022-01-06 14:44:47,666 iteration 6063 : loss : 0.016937, loss_ce: 0.007471
2022-01-06 14:44:48,748 iteration 6064 : loss : 0.016248, loss_ce: 0.005283
2022-01-06 14:44:49,881 iteration 6065 : loss : 0.024602, loss_ce: 0.009339
2022-01-06 14:44:50,926 iteration 6066 : loss : 0.022153, loss_ce: 0.009189
2022-01-06 14:44:52,051 iteration 6067 : loss : 0.022889, loss_ce: 0.008575
2022-01-06 14:44:53,154 iteration 6068 : loss : 0.013604, loss_ce: 0.002457
2022-01-06 14:44:54,333 iteration 6069 : loss : 0.022883, loss_ce: 0.010355
 89%|█████████████████████████▉   | 357/400 [2:03:21<14:28, 20.19s/it]2022-01-06 14:44:55,508 iteration 6070 : loss : 0.019068, loss_ce: 0.008297
2022-01-06 14:44:56,619 iteration 6071 : loss : 0.015807, loss_ce: 0.006702
2022-01-06 14:44:57,703 iteration 6072 : loss : 0.016712, loss_ce: 0.005574
2022-01-06 14:44:58,807 iteration 6073 : loss : 0.020730, loss_ce: 0.006205
2022-01-06 14:44:59,830 iteration 6074 : loss : 0.015020, loss_ce: 0.006706
2022-01-06 14:45:00,971 iteration 6075 : loss : 0.026078, loss_ce: 0.011636
2022-01-06 14:45:02,099 iteration 6076 : loss : 0.016027, loss_ce: 0.006290
2022-01-06 14:45:03,115 iteration 6077 : loss : 0.013235, loss_ce: 0.004687
2022-01-06 14:45:04,217 iteration 6078 : loss : 0.019065, loss_ce: 0.009658
2022-01-06 14:45:05,392 iteration 6079 : loss : 0.019819, loss_ce: 0.008209
2022-01-06 14:45:06,451 iteration 6080 : loss : 0.016202, loss_ce: 0.005932
2022-01-06 14:45:07,644 iteration 6081 : loss : 0.022252, loss_ce: 0.008254
2022-01-06 14:45:08,756 iteration 6082 : loss : 0.018492, loss_ce: 0.004020
2022-01-06 14:45:09,918 iteration 6083 : loss : 0.023150, loss_ce: 0.010978
2022-01-06 14:45:11,006 iteration 6084 : loss : 0.021082, loss_ce: 0.005419
2022-01-06 14:45:12,100 iteration 6085 : loss : 0.025839, loss_ce: 0.009470
2022-01-06 14:45:13,379 iteration 6086 : loss : 0.025737, loss_ce: 0.007830
 90%|█████████████████████████▉   | 358/400 [2:03:40<13:53, 19.85s/it]2022-01-06 14:45:14,505 iteration 6087 : loss : 0.015037, loss_ce: 0.005850
2022-01-06 14:45:15,628 iteration 6088 : loss : 0.016252, loss_ce: 0.006878
2022-01-06 14:45:16,721 iteration 6089 : loss : 0.020769, loss_ce: 0.009833
2022-01-06 14:45:17,828 iteration 6090 : loss : 0.021592, loss_ce: 0.009029
2022-01-06 14:45:18,931 iteration 6091 : loss : 0.013772, loss_ce: 0.004947
2022-01-06 14:45:20,048 iteration 6092 : loss : 0.012899, loss_ce: 0.004999
2022-01-06 14:45:21,087 iteration 6093 : loss : 0.016230, loss_ce: 0.005479
2022-01-06 14:45:22,150 iteration 6094 : loss : 0.016600, loss_ce: 0.005514
2022-01-06 14:45:23,286 iteration 6095 : loss : 0.018383, loss_ce: 0.007768
2022-01-06 14:45:24,429 iteration 6096 : loss : 0.020853, loss_ce: 0.006302
2022-01-06 14:45:25,470 iteration 6097 : loss : 0.015103, loss_ce: 0.006785
2022-01-06 14:45:26,580 iteration 6098 : loss : 0.026008, loss_ce: 0.010514
2022-01-06 14:45:27,728 iteration 6099 : loss : 0.026678, loss_ce: 0.006270
2022-01-06 14:45:28,904 iteration 6100 : loss : 0.021132, loss_ce: 0.006520
2022-01-06 14:45:30,050 iteration 6101 : loss : 0.013199, loss_ce: 0.004910
2022-01-06 14:45:31,119 iteration 6102 : loss : 0.016683, loss_ce: 0.005301
2022-01-06 14:45:32,132 iteration 6103 : loss : 0.011731, loss_ce: 0.004379
 90%|██████████████████████████   | 359/400 [2:03:59<13:20, 19.52s/it]2022-01-06 14:45:33,358 iteration 6104 : loss : 0.021767, loss_ce: 0.006651
2022-01-06 14:45:34,434 iteration 6105 : loss : 0.022643, loss_ce: 0.009571
2022-01-06 14:45:35,479 iteration 6106 : loss : 0.013698, loss_ce: 0.004582
2022-01-06 14:45:36,557 iteration 6107 : loss : 0.014279, loss_ce: 0.004300
2022-01-06 14:45:37,716 iteration 6108 : loss : 0.017077, loss_ce: 0.005398
2022-01-06 14:45:38,798 iteration 6109 : loss : 0.015128, loss_ce: 0.005747
2022-01-06 14:45:39,923 iteration 6110 : loss : 0.019006, loss_ce: 0.008317
2022-01-06 14:45:41,089 iteration 6111 : loss : 0.038190, loss_ce: 0.016914
2022-01-06 14:45:42,227 iteration 6112 : loss : 0.018299, loss_ce: 0.006041
2022-01-06 14:45:43,390 iteration 6113 : loss : 0.017779, loss_ce: 0.006914
2022-01-06 14:45:44,476 iteration 6114 : loss : 0.017204, loss_ce: 0.007105
2022-01-06 14:45:45,657 iteration 6115 : loss : 0.028976, loss_ce: 0.009101
2022-01-06 14:45:46,794 iteration 6116 : loss : 0.016136, loss_ce: 0.006993
2022-01-06 14:45:47,879 iteration 6117 : loss : 0.018703, loss_ce: 0.007472
2022-01-06 14:45:49,028 iteration 6118 : loss : 0.018397, loss_ce: 0.008435
2022-01-06 14:45:50,134 iteration 6119 : loss : 0.022283, loss_ce: 0.006619
2022-01-06 14:45:50,134 Training Data Eval:
2022-01-06 14:45:55,747   Average segmentation loss on training set: 0.0126
2022-01-06 14:45:55,747 Validation Data Eval:
2022-01-06 14:45:57,666   Average segmentation loss on validation set: 0.0800
2022-01-06 14:45:58,773 iteration 6120 : loss : 0.020723, loss_ce: 0.007429
 90%|██████████████████████████   | 360/400 [2:04:25<14:26, 21.66s/it]2022-01-06 14:45:59,956 iteration 6121 : loss : 0.018093, loss_ce: 0.007833
2022-01-06 14:46:01,111 iteration 6122 : loss : 0.018822, loss_ce: 0.006114
2022-01-06 14:46:02,304 iteration 6123 : loss : 0.020996, loss_ce: 0.008936
2022-01-06 14:46:03,428 iteration 6124 : loss : 0.020698, loss_ce: 0.006286
2022-01-06 14:46:04,630 iteration 6125 : loss : 0.027537, loss_ce: 0.009171
2022-01-06 14:46:05,729 iteration 6126 : loss : 0.021054, loss_ce: 0.004492
2022-01-06 14:46:06,810 iteration 6127 : loss : 0.017099, loss_ce: 0.006036
2022-01-06 14:46:07,985 iteration 6128 : loss : 0.015805, loss_ce: 0.005088
2022-01-06 14:46:09,093 iteration 6129 : loss : 0.014309, loss_ce: 0.005864
2022-01-06 14:46:10,277 iteration 6130 : loss : 0.017924, loss_ce: 0.006097
2022-01-06 14:46:11,401 iteration 6131 : loss : 0.019717, loss_ce: 0.010326
2022-01-06 14:46:12,522 iteration 6132 : loss : 0.020883, loss_ce: 0.004455
2022-01-06 14:46:13,598 iteration 6133 : loss : 0.016175, loss_ce: 0.008466
2022-01-06 14:46:14,653 iteration 6134 : loss : 0.012072, loss_ce: 0.004987
2022-01-06 14:46:15,713 iteration 6135 : loss : 0.014587, loss_ce: 0.005569
2022-01-06 14:46:16,861 iteration 6136 : loss : 0.016241, loss_ce: 0.005305
2022-01-06 14:46:17,955 iteration 6137 : loss : 0.016796, loss_ce: 0.005703
 90%|██████████████████████████▏  | 361/400 [2:04:44<13:35, 20.92s/it]2022-01-06 14:46:19,084 iteration 6138 : loss : 0.016744, loss_ce: 0.005458
2022-01-06 14:46:20,224 iteration 6139 : loss : 0.019688, loss_ce: 0.007843
2022-01-06 14:46:21,327 iteration 6140 : loss : 0.014704, loss_ce: 0.005306
2022-01-06 14:46:22,469 iteration 6141 : loss : 0.016081, loss_ce: 0.006221
2022-01-06 14:46:23,722 iteration 6142 : loss : 0.030970, loss_ce: 0.009291
2022-01-06 14:46:24,780 iteration 6143 : loss : 0.017906, loss_ce: 0.008338
2022-01-06 14:46:25,904 iteration 6144 : loss : 0.015688, loss_ce: 0.006845
2022-01-06 14:46:27,038 iteration 6145 : loss : 0.021202, loss_ce: 0.008139
2022-01-06 14:46:28,143 iteration 6146 : loss : 0.023690, loss_ce: 0.011003
2022-01-06 14:46:29,248 iteration 6147 : loss : 0.013714, loss_ce: 0.003485
2022-01-06 14:46:30,319 iteration 6148 : loss : 0.011012, loss_ce: 0.003305
2022-01-06 14:46:31,386 iteration 6149 : loss : 0.021695, loss_ce: 0.007199
2022-01-06 14:46:32,461 iteration 6150 : loss : 0.027007, loss_ce: 0.008134
2022-01-06 14:46:33,551 iteration 6151 : loss : 0.020148, loss_ce: 0.004579
2022-01-06 14:46:34,605 iteration 6152 : loss : 0.035388, loss_ce: 0.013232
2022-01-06 14:46:35,759 iteration 6153 : loss : 0.025891, loss_ce: 0.012914
2022-01-06 14:46:36,950 iteration 6154 : loss : 0.025600, loss_ce: 0.010899
 90%|██████████████████████████▏  | 362/400 [2:05:03<12:52, 20.34s/it]2022-01-06 14:46:38,081 iteration 6155 : loss : 0.017158, loss_ce: 0.007469
2022-01-06 14:46:39,173 iteration 6156 : loss : 0.014708, loss_ce: 0.006444
2022-01-06 14:46:40,320 iteration 6157 : loss : 0.017072, loss_ce: 0.006221
2022-01-06 14:46:41,398 iteration 6158 : loss : 0.019344, loss_ce: 0.006214
2022-01-06 14:46:42,417 iteration 6159 : loss : 0.014065, loss_ce: 0.005107
2022-01-06 14:46:43,700 iteration 6160 : loss : 0.022071, loss_ce: 0.009554
2022-01-06 14:46:44,729 iteration 6161 : loss : 0.021004, loss_ce: 0.006650
2022-01-06 14:46:45,848 iteration 6162 : loss : 0.016222, loss_ce: 0.005549
2022-01-06 14:46:46,947 iteration 6163 : loss : 0.015845, loss_ce: 0.006430
2022-01-06 14:46:48,014 iteration 6164 : loss : 0.017874, loss_ce: 0.004985
2022-01-06 14:46:49,126 iteration 6165 : loss : 0.014090, loss_ce: 0.005092
2022-01-06 14:46:50,258 iteration 6166 : loss : 0.017168, loss_ce: 0.007249
2022-01-06 14:46:51,286 iteration 6167 : loss : 0.013537, loss_ce: 0.004019
2022-01-06 14:46:52,487 iteration 6168 : loss : 0.025452, loss_ce: 0.007349
2022-01-06 14:46:53,579 iteration 6169 : loss : 0.019593, loss_ce: 0.010038
2022-01-06 14:46:54,712 iteration 6170 : loss : 0.016037, loss_ce: 0.007242
2022-01-06 14:46:55,808 iteration 6171 : loss : 0.021012, loss_ce: 0.005502
 91%|██████████████████████████▎  | 363/400 [2:05:22<12:16, 19.89s/it]2022-01-06 14:46:56,966 iteration 6172 : loss : 0.016835, loss_ce: 0.005706
2022-01-06 14:46:57,985 iteration 6173 : loss : 0.014631, loss_ce: 0.005046
2022-01-06 14:46:59,076 iteration 6174 : loss : 0.025577, loss_ce: 0.013900
2022-01-06 14:47:00,121 iteration 6175 : loss : 0.012826, loss_ce: 0.003355
2022-01-06 14:47:01,250 iteration 6176 : loss : 0.017598, loss_ce: 0.009376
2022-01-06 14:47:02,451 iteration 6177 : loss : 0.020667, loss_ce: 0.009259
2022-01-06 14:47:03,602 iteration 6178 : loss : 0.017075, loss_ce: 0.005723
2022-01-06 14:47:04,739 iteration 6179 : loss : 0.026533, loss_ce: 0.009567
2022-01-06 14:47:05,874 iteration 6180 : loss : 0.036656, loss_ce: 0.009136
2022-01-06 14:47:06,950 iteration 6181 : loss : 0.017805, loss_ce: 0.006694
2022-01-06 14:47:08,072 iteration 6182 : loss : 0.016397, loss_ce: 0.006041
2022-01-06 14:47:09,139 iteration 6183 : loss : 0.020659, loss_ce: 0.007561
2022-01-06 14:47:10,196 iteration 6184 : loss : 0.016175, loss_ce: 0.005957
2022-01-06 14:47:11,345 iteration 6185 : loss : 0.027034, loss_ce: 0.008067
2022-01-06 14:47:12,389 iteration 6186 : loss : 0.012887, loss_ce: 0.004692
2022-01-06 14:47:13,448 iteration 6187 : loss : 0.021014, loss_ce: 0.006967
2022-01-06 14:47:14,499 iteration 6188 : loss : 0.019312, loss_ce: 0.006182
 91%|██████████████████████████▍  | 364/400 [2:05:41<11:43, 19.53s/it]2022-01-06 14:47:15,740 iteration 6189 : loss : 0.021797, loss_ce: 0.009286
2022-01-06 14:47:16,853 iteration 6190 : loss : 0.025790, loss_ce: 0.012308
2022-01-06 14:47:18,022 iteration 6191 : loss : 0.016847, loss_ce: 0.005280
2022-01-06 14:47:19,155 iteration 6192 : loss : 0.018550, loss_ce: 0.006815
2022-01-06 14:47:20,210 iteration 6193 : loss : 0.014256, loss_ce: 0.004516
2022-01-06 14:47:21,345 iteration 6194 : loss : 0.014955, loss_ce: 0.005663
2022-01-06 14:47:22,508 iteration 6195 : loss : 0.023098, loss_ce: 0.008164
2022-01-06 14:47:23,563 iteration 6196 : loss : 0.023878, loss_ce: 0.007298
2022-01-06 14:47:24,752 iteration 6197 : loss : 0.024319, loss_ce: 0.008604
2022-01-06 14:47:25,939 iteration 6198 : loss : 0.018684, loss_ce: 0.007225
2022-01-06 14:47:27,006 iteration 6199 : loss : 0.014090, loss_ce: 0.005261
2022-01-06 14:47:28,139 iteration 6200 : loss : 0.021489, loss_ce: 0.008631
2022-01-06 14:47:29,249 iteration 6201 : loss : 0.019331, loss_ce: 0.007653
2022-01-06 14:47:30,349 iteration 6202 : loss : 0.014657, loss_ce: 0.004194
2022-01-06 14:47:31,443 iteration 6203 : loss : 0.031440, loss_ce: 0.010512
2022-01-06 14:47:32,603 iteration 6204 : loss : 0.020792, loss_ce: 0.009073
2022-01-06 14:47:32,604 Training Data Eval:
2022-01-06 14:47:38,142   Average segmentation loss on training set: 0.0108
2022-01-06 14:47:38,143 Validation Data Eval:
2022-01-06 14:47:40,033   Average segmentation loss on validation set: 0.0744
2022-01-06 14:47:41,238 iteration 6205 : loss : 0.022044, loss_ce: 0.010128
 91%|██████████████████████████▍  | 365/400 [2:06:08<12:39, 21.69s/it]2022-01-06 14:47:42,424 iteration 6206 : loss : 0.020013, loss_ce: 0.010330
2022-01-06 14:47:43,531 iteration 6207 : loss : 0.019434, loss_ce: 0.009973
2022-01-06 14:47:44,658 iteration 6208 : loss : 0.019237, loss_ce: 0.005873
2022-01-06 14:47:45,782 iteration 6209 : loss : 0.022880, loss_ce: 0.008456
2022-01-06 14:47:46,865 iteration 6210 : loss : 0.022887, loss_ce: 0.008848
2022-01-06 14:47:47,916 iteration 6211 : loss : 0.017478, loss_ce: 0.006503
2022-01-06 14:47:49,001 iteration 6212 : loss : 0.018659, loss_ce: 0.006096
2022-01-06 14:47:50,103 iteration 6213 : loss : 0.014258, loss_ce: 0.005546
2022-01-06 14:47:51,187 iteration 6214 : loss : 0.014117, loss_ce: 0.005724
2022-01-06 14:47:52,347 iteration 6215 : loss : 0.024978, loss_ce: 0.007870
2022-01-06 14:47:53,443 iteration 6216 : loss : 0.015610, loss_ce: 0.006240
2022-01-06 14:47:54,481 iteration 6217 : loss : 0.019702, loss_ce: 0.006338
2022-01-06 14:47:55,594 iteration 6218 : loss : 0.016429, loss_ce: 0.006174
2022-01-06 14:47:56,692 iteration 6219 : loss : 0.015868, loss_ce: 0.005129
2022-01-06 14:47:57,775 iteration 6220 : loss : 0.020303, loss_ce: 0.006056
2022-01-06 14:47:58,904 iteration 6221 : loss : 0.030130, loss_ce: 0.009461
2022-01-06 14:47:59,971 iteration 6222 : loss : 0.013170, loss_ce: 0.006024
 92%|██████████████████████████▌  | 366/400 [2:06:26<11:47, 20.81s/it]2022-01-06 14:48:01,173 iteration 6223 : loss : 0.016422, loss_ce: 0.006888
2022-01-06 14:48:02,322 iteration 6224 : loss : 0.028106, loss_ce: 0.010243
2022-01-06 14:48:03,436 iteration 6225 : loss : 0.024638, loss_ce: 0.007937
2022-01-06 14:48:04,509 iteration 6226 : loss : 0.016373, loss_ce: 0.005339
2022-01-06 14:48:05,559 iteration 6227 : loss : 0.016368, loss_ce: 0.006703
2022-01-06 14:48:06,668 iteration 6228 : loss : 0.016086, loss_ce: 0.006023
2022-01-06 14:48:07,777 iteration 6229 : loss : 0.016607, loss_ce: 0.008503
2022-01-06 14:48:08,908 iteration 6230 : loss : 0.021110, loss_ce: 0.005893
2022-01-06 14:48:10,058 iteration 6231 : loss : 0.024193, loss_ce: 0.009263
2022-01-06 14:48:11,179 iteration 6232 : loss : 0.017080, loss_ce: 0.006623
2022-01-06 14:48:12,276 iteration 6233 : loss : 0.015791, loss_ce: 0.006616
2022-01-06 14:48:13,353 iteration 6234 : loss : 0.024728, loss_ce: 0.005198
2022-01-06 14:48:14,412 iteration 6235 : loss : 0.016176, loss_ce: 0.008092
2022-01-06 14:48:15,525 iteration 6236 : loss : 0.018150, loss_ce: 0.009247
2022-01-06 14:48:16,654 iteration 6237 : loss : 0.019763, loss_ce: 0.008382
2022-01-06 14:48:17,825 iteration 6238 : loss : 0.021728, loss_ce: 0.006124
2022-01-06 14:48:18,944 iteration 6239 : loss : 0.017833, loss_ce: 0.005240
 92%|██████████████████████████▌  | 367/400 [2:06:45<11:08, 20.26s/it]2022-01-06 14:48:20,100 iteration 6240 : loss : 0.013184, loss_ce: 0.004005
2022-01-06 14:48:21,174 iteration 6241 : loss : 0.014047, loss_ce: 0.006340
2022-01-06 14:48:22,372 iteration 6242 : loss : 0.017482, loss_ce: 0.005155
2022-01-06 14:48:23,481 iteration 6243 : loss : 0.014463, loss_ce: 0.006059
2022-01-06 14:48:24,683 iteration 6244 : loss : 0.019342, loss_ce: 0.008075
2022-01-06 14:48:25,792 iteration 6245 : loss : 0.019286, loss_ce: 0.006516
2022-01-06 14:48:26,900 iteration 6246 : loss : 0.015360, loss_ce: 0.006230
2022-01-06 14:48:28,044 iteration 6247 : loss : 0.021011, loss_ce: 0.005117
2022-01-06 14:48:29,157 iteration 6248 : loss : 0.013112, loss_ce: 0.004938
2022-01-06 14:48:30,262 iteration 6249 : loss : 0.023798, loss_ce: 0.006979
2022-01-06 14:48:31,413 iteration 6250 : loss : 0.018396, loss_ce: 0.007086
2022-01-06 14:48:32,622 iteration 6251 : loss : 0.018530, loss_ce: 0.007250
2022-01-06 14:48:33,697 iteration 6252 : loss : 0.016385, loss_ce: 0.007650
2022-01-06 14:48:34,849 iteration 6253 : loss : 0.017884, loss_ce: 0.006076
2022-01-06 14:48:35,935 iteration 6254 : loss : 0.016997, loss_ce: 0.007534
2022-01-06 14:48:37,061 iteration 6255 : loss : 0.017478, loss_ce: 0.006546
2022-01-06 14:48:38,247 iteration 6256 : loss : 0.021985, loss_ce: 0.008962
 92%|██████████████████████████▋  | 368/400 [2:07:05<10:39, 19.97s/it]2022-01-06 14:48:39,452 iteration 6257 : loss : 0.018032, loss_ce: 0.006189
2022-01-06 14:48:40,604 iteration 6258 : loss : 0.017749, loss_ce: 0.005862
2022-01-06 14:48:41,778 iteration 6259 : loss : 0.016142, loss_ce: 0.006462
2022-01-06 14:48:42,878 iteration 6260 : loss : 0.016130, loss_ce: 0.006232
2022-01-06 14:48:43,974 iteration 6261 : loss : 0.017928, loss_ce: 0.006371
2022-01-06 14:48:45,108 iteration 6262 : loss : 0.024322, loss_ce: 0.007448
2022-01-06 14:48:46,234 iteration 6263 : loss : 0.020145, loss_ce: 0.004087
2022-01-06 14:48:47,360 iteration 6264 : loss : 0.014017, loss_ce: 0.004784
2022-01-06 14:48:48,486 iteration 6265 : loss : 0.020775, loss_ce: 0.008099
2022-01-06 14:48:49,573 iteration 6266 : loss : 0.016847, loss_ce: 0.007890
2022-01-06 14:48:50,652 iteration 6267 : loss : 0.018330, loss_ce: 0.008050
2022-01-06 14:48:51,764 iteration 6268 : loss : 0.015543, loss_ce: 0.006298
2022-01-06 14:48:52,755 iteration 6269 : loss : 0.013112, loss_ce: 0.004793
2022-01-06 14:48:53,847 iteration 6270 : loss : 0.017059, loss_ce: 0.007468
2022-01-06 14:48:54,942 iteration 6271 : loss : 0.016980, loss_ce: 0.005438
2022-01-06 14:48:56,096 iteration 6272 : loss : 0.018173, loss_ce: 0.007061
2022-01-06 14:48:57,244 iteration 6273 : loss : 0.015789, loss_ce: 0.005908
 92%|██████████████████████████▊  | 369/400 [2:07:24<10:10, 19.68s/it]2022-01-06 14:48:58,549 iteration 6274 : loss : 0.025760, loss_ce: 0.008466
2022-01-06 14:48:59,676 iteration 6275 : loss : 0.022830, loss_ce: 0.011501
2022-01-06 14:49:00,795 iteration 6276 : loss : 0.014324, loss_ce: 0.005387
2022-01-06 14:49:01,865 iteration 6277 : loss : 0.015690, loss_ce: 0.004069
2022-01-06 14:49:02,978 iteration 6278 : loss : 0.015020, loss_ce: 0.005673
2022-01-06 14:49:04,071 iteration 6279 : loss : 0.020803, loss_ce: 0.009469
2022-01-06 14:49:05,145 iteration 6280 : loss : 0.019254, loss_ce: 0.007406
2022-01-06 14:49:06,361 iteration 6281 : loss : 0.024413, loss_ce: 0.007406
2022-01-06 14:49:07,446 iteration 6282 : loss : 0.013897, loss_ce: 0.005423
2022-01-06 14:49:08,509 iteration 6283 : loss : 0.013731, loss_ce: 0.006088
2022-01-06 14:49:09,611 iteration 6284 : loss : 0.018929, loss_ce: 0.008208
2022-01-06 14:49:10,752 iteration 6285 : loss : 0.015247, loss_ce: 0.004957
2022-01-06 14:49:11,830 iteration 6286 : loss : 0.017848, loss_ce: 0.005537
2022-01-06 14:49:12,907 iteration 6287 : loss : 0.018068, loss_ce: 0.004480
2022-01-06 14:49:13,939 iteration 6288 : loss : 0.013973, loss_ce: 0.004476
2022-01-06 14:49:15,069 iteration 6289 : loss : 0.015565, loss_ce: 0.006802
2022-01-06 14:49:15,070 Training Data Eval:
2022-01-06 14:49:20,705   Average segmentation loss on training set: 0.0108
2022-01-06 14:49:20,706 Validation Data Eval:
2022-01-06 14:49:22,622   Average segmentation loss on validation set: 0.0696
2022-01-06 14:49:30,125 Found new lowest validation loss at iteration 6289! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_MLP_best_val_loss_seed1234.pth
2022-01-06 14:49:31,320 iteration 6290 : loss : 0.022272, loss_ce: 0.010323
 92%|██████████████████████████▊  | 370/400 [2:07:58<11:59, 24.00s/it]2022-01-06 14:49:32,471 iteration 6291 : loss : 0.014333, loss_ce: 0.005824
2022-01-06 14:49:33,572 iteration 6292 : loss : 0.019004, loss_ce: 0.006133
2022-01-06 14:49:34,688 iteration 6293 : loss : 0.019002, loss_ce: 0.008522
2022-01-06 14:49:35,820 iteration 6294 : loss : 0.019495, loss_ce: 0.005142
2022-01-06 14:49:36,908 iteration 6295 : loss : 0.017915, loss_ce: 0.008068
2022-01-06 14:49:38,072 iteration 6296 : loss : 0.017991, loss_ce: 0.005564
2022-01-06 14:49:39,173 iteration 6297 : loss : 0.020752, loss_ce: 0.008071
2022-01-06 14:49:40,314 iteration 6298 : loss : 0.031754, loss_ce: 0.011648
2022-01-06 14:49:41,402 iteration 6299 : loss : 0.016374, loss_ce: 0.005452
2022-01-06 14:49:42,463 iteration 6300 : loss : 0.017169, loss_ce: 0.005202
2022-01-06 14:49:43,518 iteration 6301 : loss : 0.016997, loss_ce: 0.009489
2022-01-06 14:49:44,621 iteration 6302 : loss : 0.015928, loss_ce: 0.006052
2022-01-06 14:49:45,631 iteration 6303 : loss : 0.016349, loss_ce: 0.003434
2022-01-06 14:49:46,654 iteration 6304 : loss : 0.017522, loss_ce: 0.004657
2022-01-06 14:49:47,762 iteration 6305 : loss : 0.030142, loss_ce: 0.012071
2022-01-06 14:49:48,830 iteration 6306 : loss : 0.019679, loss_ce: 0.004655
2022-01-06 14:49:49,914 iteration 6307 : loss : 0.014147, loss_ce: 0.005856
 93%|██████████████████████████▉  | 371/400 [2:08:16<10:48, 22.38s/it]2022-01-06 14:49:51,015 iteration 6308 : loss : 0.018847, loss_ce: 0.003962
2022-01-06 14:49:52,115 iteration 6309 : loss : 0.018498, loss_ce: 0.006724
2022-01-06 14:49:53,200 iteration 6310 : loss : 0.019631, loss_ce: 0.008496
2022-01-06 14:49:54,307 iteration 6311 : loss : 0.014575, loss_ce: 0.006209
2022-01-06 14:49:55,423 iteration 6312 : loss : 0.021854, loss_ce: 0.007359
2022-01-06 14:49:56,580 iteration 6313 : loss : 0.018560, loss_ce: 0.007973
2022-01-06 14:49:57,666 iteration 6314 : loss : 0.014942, loss_ce: 0.004988
2022-01-06 14:49:58,726 iteration 6315 : loss : 0.017584, loss_ce: 0.006705
2022-01-06 14:49:59,813 iteration 6316 : loss : 0.024179, loss_ce: 0.012338
2022-01-06 14:50:00,897 iteration 6317 : loss : 0.015059, loss_ce: 0.004026
2022-01-06 14:50:02,062 iteration 6318 : loss : 0.029445, loss_ce: 0.010466
2022-01-06 14:50:03,185 iteration 6319 : loss : 0.018790, loss_ce: 0.007339
2022-01-06 14:50:04,265 iteration 6320 : loss : 0.015541, loss_ce: 0.005605
2022-01-06 14:50:05,411 iteration 6321 : loss : 0.018810, loss_ce: 0.008977
2022-01-06 14:50:06,568 iteration 6322 : loss : 0.022526, loss_ce: 0.011705
2022-01-06 14:50:07,809 iteration 6323 : loss : 0.020891, loss_ce: 0.006580
2022-01-06 14:50:08,988 iteration 6324 : loss : 0.026126, loss_ce: 0.013566
 93%|██████████████████████████▉  | 372/400 [2:08:35<09:58, 21.39s/it]2022-01-06 14:50:10,181 iteration 6325 : loss : 0.026433, loss_ce: 0.007000
2022-01-06 14:50:11,249 iteration 6326 : loss : 0.016325, loss_ce: 0.006318
2022-01-06 14:50:12,338 iteration 6327 : loss : 0.018753, loss_ce: 0.005662
2022-01-06 14:50:13,487 iteration 6328 : loss : 0.014679, loss_ce: 0.006757
2022-01-06 14:50:14,529 iteration 6329 : loss : 0.014478, loss_ce: 0.006880
2022-01-06 14:50:15,631 iteration 6330 : loss : 0.016866, loss_ce: 0.006526
2022-01-06 14:50:16,807 iteration 6331 : loss : 0.017234, loss_ce: 0.006136
2022-01-06 14:50:17,966 iteration 6332 : loss : 0.015144, loss_ce: 0.004416
2022-01-06 14:50:19,104 iteration 6333 : loss : 0.015702, loss_ce: 0.007866
2022-01-06 14:50:20,241 iteration 6334 : loss : 0.024333, loss_ce: 0.006119
2022-01-06 14:50:21,464 iteration 6335 : loss : 0.018110, loss_ce: 0.007268
2022-01-06 14:50:22,575 iteration 6336 : loss : 0.018046, loss_ce: 0.007611
2022-01-06 14:50:23,716 iteration 6337 : loss : 0.018258, loss_ce: 0.005974
2022-01-06 14:50:24,835 iteration 6338 : loss : 0.025280, loss_ce: 0.007231
2022-01-06 14:50:25,952 iteration 6339 : loss : 0.019723, loss_ce: 0.007787
2022-01-06 14:50:27,083 iteration 6340 : loss : 0.016099, loss_ce: 0.007140
2022-01-06 14:50:28,142 iteration 6341 : loss : 0.013832, loss_ce: 0.004668
 93%|███████████████████████████  | 373/400 [2:08:55<09:19, 20.72s/it]2022-01-06 14:50:29,362 iteration 6342 : loss : 0.020955, loss_ce: 0.005751
2022-01-06 14:50:30,455 iteration 6343 : loss : 0.015234, loss_ce: 0.005732
2022-01-06 14:50:31,543 iteration 6344 : loss : 0.019101, loss_ce: 0.005423
2022-01-06 14:50:32,624 iteration 6345 : loss : 0.021462, loss_ce: 0.003141
2022-01-06 14:50:33,742 iteration 6346 : loss : 0.029780, loss_ce: 0.009330
2022-01-06 14:50:34,862 iteration 6347 : loss : 0.018980, loss_ce: 0.007268
2022-01-06 14:50:35,883 iteration 6348 : loss : 0.011640, loss_ce: 0.005025
2022-01-06 14:50:37,008 iteration 6349 : loss : 0.020834, loss_ce: 0.009790
2022-01-06 14:50:38,238 iteration 6350 : loss : 0.022872, loss_ce: 0.007494
2022-01-06 14:50:39,405 iteration 6351 : loss : 0.013856, loss_ce: 0.005730
2022-01-06 14:50:40,523 iteration 6352 : loss : 0.016125, loss_ce: 0.004716
2022-01-06 14:50:41,647 iteration 6353 : loss : 0.017922, loss_ce: 0.007475
2022-01-06 14:50:42,758 iteration 6354 : loss : 0.018550, loss_ce: 0.007939
2022-01-06 14:50:43,849 iteration 6355 : loss : 0.029110, loss_ce: 0.010213
2022-01-06 14:50:45,014 iteration 6356 : loss : 0.021905, loss_ce: 0.005553
2022-01-06 14:50:46,093 iteration 6357 : loss : 0.023448, loss_ce: 0.008496
2022-01-06 14:50:47,308 iteration 6358 : loss : 0.017300, loss_ce: 0.007504
 94%|███████████████████████████  | 374/400 [2:09:14<08:46, 20.25s/it]2022-01-06 14:50:48,531 iteration 6359 : loss : 0.023322, loss_ce: 0.006671
2022-01-06 14:50:49,593 iteration 6360 : loss : 0.018003, loss_ce: 0.007326
2022-01-06 14:50:50,734 iteration 6361 : loss : 0.016650, loss_ce: 0.005367
2022-01-06 14:50:51,894 iteration 6362 : loss : 0.018196, loss_ce: 0.007827
2022-01-06 14:50:53,076 iteration 6363 : loss : 0.021151, loss_ce: 0.008996
2022-01-06 14:50:54,167 iteration 6364 : loss : 0.013719, loss_ce: 0.003525
2022-01-06 14:50:55,300 iteration 6365 : loss : 0.020890, loss_ce: 0.010394
2022-01-06 14:50:56,504 iteration 6366 : loss : 0.020560, loss_ce: 0.008109
2022-01-06 14:50:57,725 iteration 6367 : loss : 0.020549, loss_ce: 0.008131
2022-01-06 14:50:58,879 iteration 6368 : loss : 0.023206, loss_ce: 0.008926
2022-01-06 14:50:59,997 iteration 6369 : loss : 0.015559, loss_ce: 0.006283
2022-01-06 14:51:01,122 iteration 6370 : loss : 0.015824, loss_ce: 0.005459
2022-01-06 14:51:02,340 iteration 6371 : loss : 0.027408, loss_ce: 0.007216
2022-01-06 14:51:03,413 iteration 6372 : loss : 0.010615, loss_ce: 0.004122
2022-01-06 14:51:04,478 iteration 6373 : loss : 0.020417, loss_ce: 0.008867
2022-01-06 14:51:05,602 iteration 6374 : loss : 0.019376, loss_ce: 0.008026
2022-01-06 14:51:05,602 Training Data Eval:
2022-01-06 14:51:11,199   Average segmentation loss on training set: 0.0106
2022-01-06 14:51:11,200 Validation Data Eval:
2022-01-06 14:51:13,162   Average segmentation loss on validation set: 0.0847
2022-01-06 14:51:14,317 iteration 6375 : loss : 0.019479, loss_ce: 0.006280
 94%|███████████████████████████▏ | 375/400 [2:09:41<09:16, 22.28s/it]2022-01-06 14:51:15,537 iteration 6376 : loss : 0.021297, loss_ce: 0.006049
2022-01-06 14:51:16,662 iteration 6377 : loss : 0.020705, loss_ce: 0.007492
2022-01-06 14:51:17,687 iteration 6378 : loss : 0.017009, loss_ce: 0.007790
2022-01-06 14:51:18,795 iteration 6379 : loss : 0.022929, loss_ce: 0.005348
2022-01-06 14:51:19,886 iteration 6380 : loss : 0.018596, loss_ce: 0.008439
2022-01-06 14:51:20,971 iteration 6381 : loss : 0.011944, loss_ce: 0.003777
2022-01-06 14:51:22,003 iteration 6382 : loss : 0.016564, loss_ce: 0.005357
2022-01-06 14:51:23,160 iteration 6383 : loss : 0.024426, loss_ce: 0.009087
2022-01-06 14:51:24,242 iteration 6384 : loss : 0.014699, loss_ce: 0.005416
2022-01-06 14:51:25,377 iteration 6385 : loss : 0.025152, loss_ce: 0.009073
2022-01-06 14:51:26,530 iteration 6386 : loss : 0.024710, loss_ce: 0.007777
2022-01-06 14:51:27,725 iteration 6387 : loss : 0.023492, loss_ce: 0.010504
2022-01-06 14:51:28,865 iteration 6388 : loss : 0.019283, loss_ce: 0.007859
2022-01-06 14:51:29,863 iteration 6389 : loss : 0.016858, loss_ce: 0.005812
2022-01-06 14:51:30,964 iteration 6390 : loss : 0.022176, loss_ce: 0.010404
2022-01-06 14:51:32,032 iteration 6391 : loss : 0.016543, loss_ce: 0.009924
2022-01-06 14:51:33,206 iteration 6392 : loss : 0.027397, loss_ce: 0.008772
 94%|███████████████████████████▎ | 376/400 [2:10:00<08:30, 21.26s/it]2022-01-06 14:51:34,371 iteration 6393 : loss : 0.017556, loss_ce: 0.006890
2022-01-06 14:51:35,530 iteration 6394 : loss : 0.023405, loss_ce: 0.008537
2022-01-06 14:51:36,623 iteration 6395 : loss : 0.015964, loss_ce: 0.004948
2022-01-06 14:51:37,731 iteration 6396 : loss : 0.023318, loss_ce: 0.008901
2022-01-06 14:51:38,862 iteration 6397 : loss : 0.026216, loss_ce: 0.010865
2022-01-06 14:51:40,012 iteration 6398 : loss : 0.014451, loss_ce: 0.005215
2022-01-06 14:51:41,121 iteration 6399 : loss : 0.020293, loss_ce: 0.006721
2022-01-06 14:51:42,221 iteration 6400 : loss : 0.014289, loss_ce: 0.005830
2022-01-06 14:51:43,361 iteration 6401 : loss : 0.015382, loss_ce: 0.006024
2022-01-06 14:51:44,464 iteration 6402 : loss : 0.017335, loss_ce: 0.008952
2022-01-06 14:51:45,673 iteration 6403 : loss : 0.027807, loss_ce: 0.013129
2022-01-06 14:51:46,773 iteration 6404 : loss : 0.015585, loss_ce: 0.006279
2022-01-06 14:51:47,852 iteration 6405 : loss : 0.023852, loss_ce: 0.007593
2022-01-06 14:51:49,105 iteration 6406 : loss : 0.034465, loss_ce: 0.010095
2022-01-06 14:51:50,248 iteration 6407 : loss : 0.028484, loss_ce: 0.006717
2022-01-06 14:51:51,270 iteration 6408 : loss : 0.013820, loss_ce: 0.004760
2022-01-06 14:51:52,370 iteration 6409 : loss : 0.018890, loss_ce: 0.008092
 94%|███████████████████████████▎ | 377/400 [2:10:19<07:54, 20.63s/it]2022-01-06 14:51:53,513 iteration 6410 : loss : 0.015849, loss_ce: 0.005365
2022-01-06 14:51:54,647 iteration 6411 : loss : 0.024173, loss_ce: 0.010142
2022-01-06 14:51:55,784 iteration 6412 : loss : 0.018461, loss_ce: 0.007938
2022-01-06 14:51:56,877 iteration 6413 : loss : 0.019203, loss_ce: 0.008449
2022-01-06 14:51:57,962 iteration 6414 : loss : 0.015505, loss_ce: 0.006085
2022-01-06 14:51:59,031 iteration 6415 : loss : 0.013028, loss_ce: 0.004469
2022-01-06 14:52:00,156 iteration 6416 : loss : 0.013168, loss_ce: 0.004793
2022-01-06 14:52:01,206 iteration 6417 : loss : 0.017898, loss_ce: 0.007438
2022-01-06 14:52:02,298 iteration 6418 : loss : 0.015898, loss_ce: 0.006725
2022-01-06 14:52:03,497 iteration 6419 : loss : 0.020607, loss_ce: 0.008337
2022-01-06 14:52:04,674 iteration 6420 : loss : 0.021803, loss_ce: 0.006204
2022-01-06 14:52:05,814 iteration 6421 : loss : 0.018963, loss_ce: 0.007377
2022-01-06 14:52:06,921 iteration 6422 : loss : 0.019351, loss_ce: 0.005801
2022-01-06 14:52:08,060 iteration 6423 : loss : 0.014586, loss_ce: 0.005540
2022-01-06 14:52:09,260 iteration 6424 : loss : 0.026384, loss_ce: 0.012984
2022-01-06 14:52:10,356 iteration 6425 : loss : 0.020922, loss_ce: 0.008265
2022-01-06 14:52:11,540 iteration 6426 : loss : 0.023546, loss_ce: 0.007787
 94%|███████████████████████████▍ | 378/400 [2:10:38<07:24, 20.19s/it]2022-01-06 14:52:12,735 iteration 6427 : loss : 0.018066, loss_ce: 0.005840
2022-01-06 14:52:13,851 iteration 6428 : loss : 0.020840, loss_ce: 0.005727
2022-01-06 14:52:14,973 iteration 6429 : loss : 0.018457, loss_ce: 0.006323
2022-01-06 14:52:16,129 iteration 6430 : loss : 0.020784, loss_ce: 0.009014
2022-01-06 14:52:17,169 iteration 6431 : loss : 0.024312, loss_ce: 0.007866
2022-01-06 14:52:18,283 iteration 6432 : loss : 0.015192, loss_ce: 0.006156
2022-01-06 14:52:19,386 iteration 6433 : loss : 0.015302, loss_ce: 0.005061
2022-01-06 14:52:20,470 iteration 6434 : loss : 0.015377, loss_ce: 0.005960
2022-01-06 14:52:21,594 iteration 6435 : loss : 0.029649, loss_ce: 0.012689
2022-01-06 14:52:22,714 iteration 6436 : loss : 0.013332, loss_ce: 0.005573
2022-01-06 14:52:23,863 iteration 6437 : loss : 0.018305, loss_ce: 0.005799
2022-01-06 14:52:25,046 iteration 6438 : loss : 0.024320, loss_ce: 0.008117
2022-01-06 14:52:26,137 iteration 6439 : loss : 0.018493, loss_ce: 0.007974
2022-01-06 14:52:27,300 iteration 6440 : loss : 0.020366, loss_ce: 0.008842
2022-01-06 14:52:28,369 iteration 6441 : loss : 0.014301, loss_ce: 0.005895
2022-01-06 14:52:29,502 iteration 6442 : loss : 0.019220, loss_ce: 0.005725
2022-01-06 14:52:30,551 iteration 6443 : loss : 0.016289, loss_ce: 0.005297
 95%|███████████████████████████▍ | 379/400 [2:10:57<06:56, 19.84s/it]2022-01-06 14:52:31,736 iteration 6444 : loss : 0.020442, loss_ce: 0.010560
2022-01-06 14:52:32,847 iteration 6445 : loss : 0.018518, loss_ce: 0.006507
2022-01-06 14:52:33,974 iteration 6446 : loss : 0.018607, loss_ce: 0.007790
2022-01-06 14:52:35,109 iteration 6447 : loss : 0.026794, loss_ce: 0.004423
2022-01-06 14:52:36,245 iteration 6448 : loss : 0.015781, loss_ce: 0.005607
2022-01-06 14:52:37,324 iteration 6449 : loss : 0.015076, loss_ce: 0.004083
2022-01-06 14:52:38,388 iteration 6450 : loss : 0.016887, loss_ce: 0.007957
2022-01-06 14:52:39,498 iteration 6451 : loss : 0.020408, loss_ce: 0.007904
2022-01-06 14:52:40,669 iteration 6452 : loss : 0.022578, loss_ce: 0.007189
2022-01-06 14:52:41,749 iteration 6453 : loss : 0.021032, loss_ce: 0.008280
2022-01-06 14:52:42,794 iteration 6454 : loss : 0.021327, loss_ce: 0.007779
2022-01-06 14:52:43,933 iteration 6455 : loss : 0.015544, loss_ce: 0.007244
2022-01-06 14:52:45,026 iteration 6456 : loss : 0.016943, loss_ce: 0.006336
2022-01-06 14:52:46,160 iteration 6457 : loss : 0.031051, loss_ce: 0.013150
2022-01-06 14:52:47,229 iteration 6458 : loss : 0.015408, loss_ce: 0.005737
2022-01-06 14:52:48,333 iteration 6459 : loss : 0.010856, loss_ce: 0.002695
2022-01-06 14:52:48,334 Training Data Eval:
2022-01-06 14:52:53,944   Average segmentation loss on training set: 0.0103
2022-01-06 14:52:53,945 Validation Data Eval:
2022-01-06 14:52:55,850   Average segmentation loss on validation set: 0.0874
2022-01-06 14:52:57,015 iteration 6460 : loss : 0.020610, loss_ce: 0.010682
 95%|███████████████████████████▌ | 380/400 [2:11:23<07:16, 21.83s/it]2022-01-06 14:52:58,241 iteration 6461 : loss : 0.021269, loss_ce: 0.008223
2022-01-06 14:52:59,317 iteration 6462 : loss : 0.014774, loss_ce: 0.006061
2022-01-06 14:53:00,474 iteration 6463 : loss : 0.042254, loss_ce: 0.020842
2022-01-06 14:53:01,580 iteration 6464 : loss : 0.022033, loss_ce: 0.005890
2022-01-06 14:53:02,731 iteration 6465 : loss : 0.018285, loss_ce: 0.006588
2022-01-06 14:53:03,797 iteration 6466 : loss : 0.015159, loss_ce: 0.004959
2022-01-06 14:53:04,884 iteration 6467 : loss : 0.019554, loss_ce: 0.007213
2022-01-06 14:53:06,052 iteration 6468 : loss : 0.024591, loss_ce: 0.008811
2022-01-06 14:53:07,146 iteration 6469 : loss : 0.017257, loss_ce: 0.007157
2022-01-06 14:53:08,217 iteration 6470 : loss : 0.015713, loss_ce: 0.004690
2022-01-06 14:53:09,328 iteration 6471 : loss : 0.021921, loss_ce: 0.008120
2022-01-06 14:53:10,455 iteration 6472 : loss : 0.020803, loss_ce: 0.006880
2022-01-06 14:53:11,551 iteration 6473 : loss : 0.016608, loss_ce: 0.005656
2022-01-06 14:53:12,736 iteration 6474 : loss : 0.015865, loss_ce: 0.006791
2022-01-06 14:53:13,883 iteration 6475 : loss : 0.022290, loss_ce: 0.012810
2022-01-06 14:53:14,918 iteration 6476 : loss : 0.018360, loss_ce: 0.006329
2022-01-06 14:53:16,146 iteration 6477 : loss : 0.017932, loss_ce: 0.006965
 95%|███████████████████████████▌ | 381/400 [2:11:43<06:39, 21.02s/it]2022-01-06 14:53:17,387 iteration 6478 : loss : 0.021900, loss_ce: 0.006462
2022-01-06 14:53:18,492 iteration 6479 : loss : 0.021167, loss_ce: 0.007649
2022-01-06 14:53:19,594 iteration 6480 : loss : 0.019559, loss_ce: 0.009209
2022-01-06 14:53:20,737 iteration 6481 : loss : 0.018480, loss_ce: 0.008540
2022-01-06 14:53:21,793 iteration 6482 : loss : 0.014600, loss_ce: 0.004858
2022-01-06 14:53:22,929 iteration 6483 : loss : 0.020053, loss_ce: 0.005153
2022-01-06 14:53:23,994 iteration 6484 : loss : 0.014269, loss_ce: 0.005472
2022-01-06 14:53:25,102 iteration 6485 : loss : 0.018497, loss_ce: 0.007731
2022-01-06 14:53:26,267 iteration 6486 : loss : 0.016544, loss_ce: 0.005631
2022-01-06 14:53:27,488 iteration 6487 : loss : 0.027594, loss_ce: 0.012106
2022-01-06 14:53:28,644 iteration 6488 : loss : 0.017994, loss_ce: 0.006666
2022-01-06 14:53:29,721 iteration 6489 : loss : 0.012924, loss_ce: 0.006193
2022-01-06 14:53:30,850 iteration 6490 : loss : 0.015694, loss_ce: 0.004522
2022-01-06 14:53:31,933 iteration 6491 : loss : 0.017348, loss_ce: 0.006527
2022-01-06 14:53:33,118 iteration 6492 : loss : 0.016727, loss_ce: 0.006219
2022-01-06 14:53:34,251 iteration 6493 : loss : 0.028773, loss_ce: 0.010368
2022-01-06 14:53:35,266 iteration 6494 : loss : 0.010851, loss_ce: 0.004170
 96%|███████████████████████████▋ | 382/400 [2:12:02<06:08, 20.45s/it]2022-01-06 14:53:36,404 iteration 6495 : loss : 0.015909, loss_ce: 0.006375
2022-01-06 14:53:37,513 iteration 6496 : loss : 0.031736, loss_ce: 0.007261
2022-01-06 14:53:38,656 iteration 6497 : loss : 0.013828, loss_ce: 0.005163
2022-01-06 14:53:39,794 iteration 6498 : loss : 0.026852, loss_ce: 0.009326
2022-01-06 14:53:40,928 iteration 6499 : loss : 0.017965, loss_ce: 0.006443
2022-01-06 14:53:42,014 iteration 6500 : loss : 0.015124, loss_ce: 0.005948
2022-01-06 14:53:43,133 iteration 6501 : loss : 0.017934, loss_ce: 0.006764
2022-01-06 14:53:44,289 iteration 6502 : loss : 0.016976, loss_ce: 0.009858
2022-01-06 14:53:45,368 iteration 6503 : loss : 0.012093, loss_ce: 0.003032
2022-01-06 14:53:46,423 iteration 6504 : loss : 0.017118, loss_ce: 0.006296
2022-01-06 14:53:47,425 iteration 6505 : loss : 0.013064, loss_ce: 0.006105
2022-01-06 14:53:48,529 iteration 6506 : loss : 0.018541, loss_ce: 0.005861
2022-01-06 14:53:49,717 iteration 6507 : loss : 0.021080, loss_ce: 0.010868
2022-01-06 14:53:50,864 iteration 6508 : loss : 0.018725, loss_ce: 0.006731
2022-01-06 14:53:51,987 iteration 6509 : loss : 0.018446, loss_ce: 0.007591
2022-01-06 14:53:53,117 iteration 6510 : loss : 0.014804, loss_ce: 0.004962
2022-01-06 14:53:54,152 iteration 6511 : loss : 0.015401, loss_ce: 0.005603
 96%|███████████████████████████▊ | 383/400 [2:12:21<05:39, 19.98s/it]2022-01-06 14:53:55,277 iteration 6512 : loss : 0.017758, loss_ce: 0.005126
2022-01-06 14:53:56,380 iteration 6513 : loss : 0.020413, loss_ce: 0.006690
2022-01-06 14:53:57,579 iteration 6514 : loss : 0.019346, loss_ce: 0.006015
2022-01-06 14:53:58,730 iteration 6515 : loss : 0.017404, loss_ce: 0.006074
2022-01-06 14:53:59,861 iteration 6516 : loss : 0.021342, loss_ce: 0.009316
2022-01-06 14:54:00,951 iteration 6517 : loss : 0.024320, loss_ce: 0.004972
2022-01-06 14:54:02,055 iteration 6518 : loss : 0.015427, loss_ce: 0.005355
2022-01-06 14:54:03,149 iteration 6519 : loss : 0.017402, loss_ce: 0.005818
2022-01-06 14:54:04,295 iteration 6520 : loss : 0.017807, loss_ce: 0.005860
2022-01-06 14:54:05,362 iteration 6521 : loss : 0.027796, loss_ce: 0.013301
2022-01-06 14:54:06,475 iteration 6522 : loss : 0.012925, loss_ce: 0.005227
2022-01-06 14:54:07,509 iteration 6523 : loss : 0.015369, loss_ce: 0.006393
2022-01-06 14:54:08,662 iteration 6524 : loss : 0.019427, loss_ce: 0.008631
2022-01-06 14:54:09,783 iteration 6525 : loss : 0.028494, loss_ce: 0.011676
2022-01-06 14:54:10,894 iteration 6526 : loss : 0.023969, loss_ce: 0.011436
2022-01-06 14:54:11,933 iteration 6527 : loss : 0.017945, loss_ce: 0.008598
2022-01-06 14:54:12,993 iteration 6528 : loss : 0.016903, loss_ce: 0.005304
 96%|███████████████████████████▊ | 384/400 [2:12:39<05:14, 19.64s/it]2022-01-06 14:54:14,208 iteration 6529 : loss : 0.026241, loss_ce: 0.007938
2022-01-06 14:54:15,331 iteration 6530 : loss : 0.016233, loss_ce: 0.005867
2022-01-06 14:54:16,374 iteration 6531 : loss : 0.015445, loss_ce: 0.004560
2022-01-06 14:54:17,500 iteration 6532 : loss : 0.020353, loss_ce: 0.007938
2022-01-06 14:54:18,533 iteration 6533 : loss : 0.012112, loss_ce: 0.004491
2022-01-06 14:54:19,635 iteration 6534 : loss : 0.018481, loss_ce: 0.007897
2022-01-06 14:54:20,751 iteration 6535 : loss : 0.016741, loss_ce: 0.006415
2022-01-06 14:54:21,879 iteration 6536 : loss : 0.021741, loss_ce: 0.006821
2022-01-06 14:54:23,002 iteration 6537 : loss : 0.025058, loss_ce: 0.006748
2022-01-06 14:54:24,129 iteration 6538 : loss : 0.015848, loss_ce: 0.004613
2022-01-06 14:54:25,272 iteration 6539 : loss : 0.016860, loss_ce: 0.007472
2022-01-06 14:54:26,295 iteration 6540 : loss : 0.012961, loss_ce: 0.005366
2022-01-06 14:54:27,359 iteration 6541 : loss : 0.016572, loss_ce: 0.008937
2022-01-06 14:54:28,540 iteration 6542 : loss : 0.018352, loss_ce: 0.005275
2022-01-06 14:54:29,609 iteration 6543 : loss : 0.014960, loss_ce: 0.004586
2022-01-06 14:54:30,743 iteration 6544 : loss : 0.020431, loss_ce: 0.008130
2022-01-06 14:54:30,743 Training Data Eval:
2022-01-06 14:54:36,296   Average segmentation loss on training set: 0.0102
2022-01-06 14:54:36,296 Validation Data Eval:
2022-01-06 14:54:38,218   Average segmentation loss on validation set: 0.0975
2022-01-06 14:54:39,414 iteration 6545 : loss : 0.024088, loss_ce: 0.008862
 96%|███████████████████████████▉ | 385/400 [2:13:06<05:25, 21.67s/it]2022-01-06 14:54:40,722 iteration 6546 : loss : 0.018799, loss_ce: 0.007204
2022-01-06 14:54:41,827 iteration 6547 : loss : 0.016237, loss_ce: 0.006874
2022-01-06 14:54:42,967 iteration 6548 : loss : 0.013923, loss_ce: 0.005581
2022-01-06 14:54:44,053 iteration 6549 : loss : 0.021214, loss_ce: 0.007417
2022-01-06 14:54:45,220 iteration 6550 : loss : 0.017791, loss_ce: 0.005976
2022-01-06 14:54:46,421 iteration 6551 : loss : 0.039235, loss_ce: 0.017715
2022-01-06 14:54:47,665 iteration 6552 : loss : 0.023495, loss_ce: 0.009437
2022-01-06 14:54:48,928 iteration 6553 : loss : 0.022703, loss_ce: 0.007860
2022-01-06 14:54:50,101 iteration 6554 : loss : 0.018809, loss_ce: 0.007577
2022-01-06 14:54:51,252 iteration 6555 : loss : 0.015663, loss_ce: 0.007039
2022-01-06 14:54:52,378 iteration 6556 : loss : 0.017216, loss_ce: 0.004477
2022-01-06 14:54:53,411 iteration 6557 : loss : 0.010928, loss_ce: 0.004599
2022-01-06 14:54:54,523 iteration 6558 : loss : 0.018147, loss_ce: 0.005827
2022-01-06 14:54:55,571 iteration 6559 : loss : 0.013366, loss_ce: 0.004665
2022-01-06 14:54:56,693 iteration 6560 : loss : 0.021815, loss_ce: 0.010990
2022-01-06 14:54:57,858 iteration 6561 : loss : 0.023576, loss_ce: 0.006636
2022-01-06 14:54:58,942 iteration 6562 : loss : 0.014966, loss_ce: 0.005517
 96%|███████████████████████████▉ | 386/400 [2:13:25<04:54, 21.03s/it]2022-01-06 14:55:00,252 iteration 6563 : loss : 0.022942, loss_ce: 0.010501
2022-01-06 14:55:01,367 iteration 6564 : loss : 0.021477, loss_ce: 0.008363
2022-01-06 14:55:02,439 iteration 6565 : loss : 0.025301, loss_ce: 0.005741
2022-01-06 14:55:03,507 iteration 6566 : loss : 0.017433, loss_ce: 0.007068
2022-01-06 14:55:04,553 iteration 6567 : loss : 0.015741, loss_ce: 0.006296
2022-01-06 14:55:05,650 iteration 6568 : loss : 0.016261, loss_ce: 0.007123
2022-01-06 14:55:06,768 iteration 6569 : loss : 0.013397, loss_ce: 0.005231
2022-01-06 14:55:07,833 iteration 6570 : loss : 0.011973, loss_ce: 0.005321
2022-01-06 14:55:08,931 iteration 6571 : loss : 0.017631, loss_ce: 0.004953
2022-01-06 14:55:09,965 iteration 6572 : loss : 0.014721, loss_ce: 0.006271
2022-01-06 14:55:11,136 iteration 6573 : loss : 0.019423, loss_ce: 0.006352
2022-01-06 14:55:12,308 iteration 6574 : loss : 0.013779, loss_ce: 0.005351
2022-01-06 14:55:13,491 iteration 6575 : loss : 0.040133, loss_ce: 0.010930
2022-01-06 14:55:14,558 iteration 6576 : loss : 0.016648, loss_ce: 0.006897
2022-01-06 14:55:15,682 iteration 6577 : loss : 0.030470, loss_ce: 0.006803
2022-01-06 14:55:16,733 iteration 6578 : loss : 0.018538, loss_ce: 0.007314
2022-01-06 14:55:17,844 iteration 6579 : loss : 0.017503, loss_ce: 0.006341
 97%|████████████████████████████ | 387/400 [2:13:44<04:25, 20.39s/it]2022-01-06 14:55:19,021 iteration 6580 : loss : 0.019242, loss_ce: 0.005756
2022-01-06 14:55:20,099 iteration 6581 : loss : 0.018855, loss_ce: 0.005823
2022-01-06 14:55:21,265 iteration 6582 : loss : 0.018231, loss_ce: 0.006042
2022-01-06 14:55:22,370 iteration 6583 : loss : 0.021954, loss_ce: 0.008221
2022-01-06 14:55:23,493 iteration 6584 : loss : 0.016945, loss_ce: 0.006944
2022-01-06 14:55:24,656 iteration 6585 : loss : 0.028495, loss_ce: 0.008893
2022-01-06 14:55:25,791 iteration 6586 : loss : 0.025000, loss_ce: 0.008063
2022-01-06 14:55:26,892 iteration 6587 : loss : 0.012859, loss_ce: 0.005363
2022-01-06 14:55:28,018 iteration 6588 : loss : 0.018622, loss_ce: 0.007025
2022-01-06 14:55:29,069 iteration 6589 : loss : 0.032379, loss_ce: 0.010753
2022-01-06 14:55:30,234 iteration 6590 : loss : 0.018582, loss_ce: 0.008523
2022-01-06 14:55:31,396 iteration 6591 : loss : 0.022571, loss_ce: 0.009075
2022-01-06 14:55:32,474 iteration 6592 : loss : 0.013950, loss_ce: 0.005528
2022-01-06 14:55:33,641 iteration 6593 : loss : 0.035410, loss_ce: 0.008860
2022-01-06 14:55:34,734 iteration 6594 : loss : 0.016200, loss_ce: 0.007436
2022-01-06 14:55:35,817 iteration 6595 : loss : 0.014243, loss_ce: 0.004639
2022-01-06 14:55:36,967 iteration 6596 : loss : 0.023997, loss_ce: 0.007874
 97%|████████████████████████████▏| 388/400 [2:14:03<04:00, 20.01s/it]2022-01-06 14:55:38,119 iteration 6597 : loss : 0.021764, loss_ce: 0.009574
2022-01-06 14:55:39,165 iteration 6598 : loss : 0.035197, loss_ce: 0.006610
2022-01-06 14:55:40,283 iteration 6599 : loss : 0.015545, loss_ce: 0.006098
2022-01-06 14:55:41,278 iteration 6600 : loss : 0.012831, loss_ce: 0.005138
2022-01-06 14:55:42,363 iteration 6601 : loss : 0.016121, loss_ce: 0.005597
2022-01-06 14:55:43,420 iteration 6602 : loss : 0.017060, loss_ce: 0.005833
2022-01-06 14:55:44,517 iteration 6603 : loss : 0.021456, loss_ce: 0.007654
2022-01-06 14:55:45,540 iteration 6604 : loss : 0.011470, loss_ce: 0.004791
2022-01-06 14:55:46,646 iteration 6605 : loss : 0.015159, loss_ce: 0.005031
2022-01-06 14:55:47,781 iteration 6606 : loss : 0.015750, loss_ce: 0.006695
2022-01-06 14:55:48,910 iteration 6607 : loss : 0.019943, loss_ce: 0.008148
2022-01-06 14:55:50,035 iteration 6608 : loss : 0.024385, loss_ce: 0.014993
2022-01-06 14:55:51,098 iteration 6609 : loss : 0.012734, loss_ce: 0.004778
2022-01-06 14:55:52,178 iteration 6610 : loss : 0.018492, loss_ce: 0.007690
2022-01-06 14:55:53,257 iteration 6611 : loss : 0.015288, loss_ce: 0.003529
2022-01-06 14:55:54,393 iteration 6612 : loss : 0.021361, loss_ce: 0.008668
2022-01-06 14:55:55,385 iteration 6613 : loss : 0.014966, loss_ce: 0.005454
 97%|████████████████████████████▏| 389/400 [2:14:22<03:34, 19.53s/it]2022-01-06 14:55:56,543 iteration 6614 : loss : 0.014743, loss_ce: 0.005472
2022-01-06 14:55:57,656 iteration 6615 : loss : 0.014571, loss_ce: 0.006464
2022-01-06 14:55:58,742 iteration 6616 : loss : 0.017949, loss_ce: 0.006785
2022-01-06 14:55:59,970 iteration 6617 : loss : 0.020324, loss_ce: 0.007079
2022-01-06 14:56:01,091 iteration 6618 : loss : 0.014024, loss_ce: 0.003891
2022-01-06 14:56:02,288 iteration 6619 : loss : 0.021839, loss_ce: 0.006916
2022-01-06 14:56:03,439 iteration 6620 : loss : 0.023474, loss_ce: 0.011526
2022-01-06 14:56:04,526 iteration 6621 : loss : 0.030109, loss_ce: 0.011641
2022-01-06 14:56:05,611 iteration 6622 : loss : 0.016737, loss_ce: 0.007634
2022-01-06 14:56:06,705 iteration 6623 : loss : 0.025461, loss_ce: 0.008172
2022-01-06 14:56:07,848 iteration 6624 : loss : 0.042723, loss_ce: 0.018829
2022-01-06 14:56:09,017 iteration 6625 : loss : 0.016734, loss_ce: 0.006455
2022-01-06 14:56:10,118 iteration 6626 : loss : 0.020412, loss_ce: 0.004898
2022-01-06 14:56:11,150 iteration 6627 : loss : 0.017293, loss_ce: 0.006007
2022-01-06 14:56:12,249 iteration 6628 : loss : 0.019824, loss_ce: 0.009409
2022-01-06 14:56:13,348 iteration 6629 : loss : 0.014108, loss_ce: 0.006693
2022-01-06 14:56:13,348 Training Data Eval:
2022-01-06 14:56:18,882   Average segmentation loss on training set: 0.0106
2022-01-06 14:56:18,882 Validation Data Eval:
2022-01-06 14:56:20,774   Average segmentation loss on validation set: 0.0774
2022-01-06 14:56:21,921 iteration 6630 : loss : 0.020025, loss_ce: 0.005706
 98%|████████████████████████████▎| 390/400 [2:14:48<03:36, 21.63s/it]2022-01-06 14:56:23,161 iteration 6631 : loss : 0.021567, loss_ce: 0.008379
2022-01-06 14:56:24,277 iteration 6632 : loss : 0.021545, loss_ce: 0.009276
2022-01-06 14:56:25,383 iteration 6633 : loss : 0.021592, loss_ce: 0.005335
2022-01-06 14:56:26,364 iteration 6634 : loss : 0.014192, loss_ce: 0.004474
2022-01-06 14:56:27,426 iteration 6635 : loss : 0.016638, loss_ce: 0.004328
2022-01-06 14:56:28,581 iteration 6636 : loss : 0.022156, loss_ce: 0.008502
2022-01-06 14:56:29,710 iteration 6637 : loss : 0.012613, loss_ce: 0.005490
2022-01-06 14:56:30,861 iteration 6638 : loss : 0.036396, loss_ce: 0.015967
2022-01-06 14:56:32,045 iteration 6639 : loss : 0.026328, loss_ce: 0.008769
2022-01-06 14:56:33,138 iteration 6640 : loss : 0.016514, loss_ce: 0.005157
2022-01-06 14:56:34,209 iteration 6641 : loss : 0.015600, loss_ce: 0.007924
2022-01-06 14:56:35,397 iteration 6642 : loss : 0.017145, loss_ce: 0.007553
2022-01-06 14:56:36,542 iteration 6643 : loss : 0.017180, loss_ce: 0.007530
2022-01-06 14:56:37,671 iteration 6644 : loss : 0.017506, loss_ce: 0.005824
2022-01-06 14:56:38,747 iteration 6645 : loss : 0.011516, loss_ce: 0.004649
2022-01-06 14:56:39,865 iteration 6646 : loss : 0.014103, loss_ce: 0.006170
2022-01-06 14:56:40,925 iteration 6647 : loss : 0.014580, loss_ce: 0.004627
 98%|████████████████████████████▎| 391/400 [2:15:07<03:07, 20.84s/it]2022-01-06 14:56:42,096 iteration 6648 : loss : 0.014711, loss_ce: 0.005039
2022-01-06 14:56:43,284 iteration 6649 : loss : 0.018640, loss_ce: 0.008123
2022-01-06 14:56:44,353 iteration 6650 : loss : 0.022244, loss_ce: 0.008284
2022-01-06 14:56:45,439 iteration 6651 : loss : 0.014216, loss_ce: 0.004603
2022-01-06 14:56:46,627 iteration 6652 : loss : 0.019707, loss_ce: 0.006145
2022-01-06 14:56:47,793 iteration 6653 : loss : 0.015450, loss_ce: 0.006541
2022-01-06 14:56:48,874 iteration 6654 : loss : 0.012581, loss_ce: 0.004451
2022-01-06 14:56:50,014 iteration 6655 : loss : 0.016730, loss_ce: 0.007106
2022-01-06 14:56:51,164 iteration 6656 : loss : 0.013713, loss_ce: 0.005111
2022-01-06 14:56:52,377 iteration 6657 : loss : 0.021203, loss_ce: 0.006925
2022-01-06 14:56:53,513 iteration 6658 : loss : 0.023556, loss_ce: 0.009350
2022-01-06 14:56:54,676 iteration 6659 : loss : 0.022692, loss_ce: 0.007959
2022-01-06 14:56:55,764 iteration 6660 : loss : 0.012717, loss_ce: 0.004477
2022-01-06 14:56:56,809 iteration 6661 : loss : 0.011859, loss_ce: 0.004266
2022-01-06 14:56:57,936 iteration 6662 : loss : 0.019488, loss_ce: 0.008093
2022-01-06 14:56:59,102 iteration 6663 : loss : 0.019810, loss_ce: 0.007411
2022-01-06 14:57:00,201 iteration 6664 : loss : 0.016556, loss_ce: 0.005084
 98%|████████████████████████████▍| 392/400 [2:15:27<02:43, 20.38s/it]2022-01-06 14:57:01,386 iteration 6665 : loss : 0.017831, loss_ce: 0.005990
2022-01-06 14:57:02,513 iteration 6666 : loss : 0.015842, loss_ce: 0.005689
2022-01-06 14:57:03,589 iteration 6667 : loss : 0.017358, loss_ce: 0.006177
2022-01-06 14:57:04,599 iteration 6668 : loss : 0.012048, loss_ce: 0.004699
2022-01-06 14:57:05,716 iteration 6669 : loss : 0.015570, loss_ce: 0.005204
2022-01-06 14:57:06,758 iteration 6670 : loss : 0.014388, loss_ce: 0.004852
2022-01-06 14:57:07,934 iteration 6671 : loss : 0.014397, loss_ce: 0.005759
2022-01-06 14:57:09,039 iteration 6672 : loss : 0.015337, loss_ce: 0.007481
2022-01-06 14:57:10,214 iteration 6673 : loss : 0.025211, loss_ce: 0.010994
2022-01-06 14:57:11,382 iteration 6674 : loss : 0.019934, loss_ce: 0.007358
2022-01-06 14:57:12,451 iteration 6675 : loss : 0.020194, loss_ce: 0.005983
2022-01-06 14:57:13,482 iteration 6676 : loss : 0.012472, loss_ce: 0.004685
2022-01-06 14:57:14,601 iteration 6677 : loss : 0.015534, loss_ce: 0.003959
2022-01-06 14:57:15,794 iteration 6678 : loss : 0.017927, loss_ce: 0.007889
2022-01-06 14:57:16,923 iteration 6679 : loss : 0.015719, loss_ce: 0.004677
2022-01-06 14:57:18,010 iteration 6680 : loss : 0.030751, loss_ce: 0.012842
2022-01-06 14:57:19,064 iteration 6681 : loss : 0.021840, loss_ce: 0.008555
 98%|████████████████████████████▍| 393/400 [2:15:46<02:19, 19.92s/it]2022-01-06 14:57:20,219 iteration 6682 : loss : 0.016228, loss_ce: 0.005212
2022-01-06 14:57:21,363 iteration 6683 : loss : 0.020609, loss_ce: 0.007255
2022-01-06 14:57:22,492 iteration 6684 : loss : 0.031310, loss_ce: 0.005424
2022-01-06 14:57:23,584 iteration 6685 : loss : 0.016805, loss_ce: 0.006210
2022-01-06 14:57:24,715 iteration 6686 : loss : 0.018488, loss_ce: 0.005621
2022-01-06 14:57:25,835 iteration 6687 : loss : 0.016772, loss_ce: 0.006831
2022-01-06 14:57:26,960 iteration 6688 : loss : 0.017799, loss_ce: 0.006603
2022-01-06 14:57:28,042 iteration 6689 : loss : 0.016245, loss_ce: 0.006497
2022-01-06 14:57:29,210 iteration 6690 : loss : 0.022675, loss_ce: 0.008031
2022-01-06 14:57:30,260 iteration 6691 : loss : 0.020002, loss_ce: 0.009636
2022-01-06 14:57:31,312 iteration 6692 : loss : 0.015643, loss_ce: 0.008084
2022-01-06 14:57:32,435 iteration 6693 : loss : 0.021649, loss_ce: 0.008450
2022-01-06 14:57:33,472 iteration 6694 : loss : 0.011985, loss_ce: 0.004984
2022-01-06 14:57:34,555 iteration 6695 : loss : 0.014847, loss_ce: 0.003967
2022-01-06 14:57:35,655 iteration 6696 : loss : 0.018761, loss_ce: 0.009837
2022-01-06 14:57:36,765 iteration 6697 : loss : 0.017225, loss_ce: 0.006579
2022-01-06 14:57:37,797 iteration 6698 : loss : 0.021757, loss_ce: 0.006241
 98%|████████████████████████████▌| 394/400 [2:16:04<01:57, 19.57s/it]2022-01-06 14:57:38,970 iteration 6699 : loss : 0.027952, loss_ce: 0.006317
2022-01-06 14:57:40,116 iteration 6700 : loss : 0.024933, loss_ce: 0.011150
2022-01-06 14:57:41,292 iteration 6701 : loss : 0.039469, loss_ce: 0.013368
2022-01-06 14:57:42,358 iteration 6702 : loss : 0.015186, loss_ce: 0.004881
2022-01-06 14:57:43,552 iteration 6703 : loss : 0.018203, loss_ce: 0.005446
2022-01-06 14:57:44,660 iteration 6704 : loss : 0.028417, loss_ce: 0.014650
2022-01-06 14:57:45,701 iteration 6705 : loss : 0.014532, loss_ce: 0.004432
2022-01-06 14:57:46,778 iteration 6706 : loss : 0.017680, loss_ce: 0.006712
2022-01-06 14:57:47,875 iteration 6707 : loss : 0.015613, loss_ce: 0.005393
2022-01-06 14:57:48,909 iteration 6708 : loss : 0.011894, loss_ce: 0.003940
2022-01-06 14:57:49,983 iteration 6709 : loss : 0.014830, loss_ce: 0.004416
2022-01-06 14:57:51,114 iteration 6710 : loss : 0.016075, loss_ce: 0.006102
2022-01-06 14:57:52,227 iteration 6711 : loss : 0.021841, loss_ce: 0.009733
2022-01-06 14:57:53,379 iteration 6712 : loss : 0.015105, loss_ce: 0.006042
2022-01-06 14:57:54,455 iteration 6713 : loss : 0.017597, loss_ce: 0.005531
2022-01-06 14:57:55,553 iteration 6714 : loss : 0.017922, loss_ce: 0.006798
2022-01-06 14:57:55,554 Training Data Eval:
2022-01-06 14:58:01,095   Average segmentation loss on training set: 0.0102
2022-01-06 14:58:01,095 Validation Data Eval:
2022-01-06 14:58:02,989   Average segmentation loss on validation set: 0.0808
2022-01-06 14:58:04,065 iteration 6715 : loss : 0.011105, loss_ce: 0.003170
 99%|████████████████████████████▋| 395/400 [2:16:31<01:47, 21.57s/it]2022-01-06 14:58:05,165 iteration 6716 : loss : 0.012937, loss_ce: 0.004162
2022-01-06 14:58:06,317 iteration 6717 : loss : 0.021241, loss_ce: 0.005931
2022-01-06 14:58:07,509 iteration 6718 : loss : 0.020608, loss_ce: 0.010029
2022-01-06 14:58:08,676 iteration 6719 : loss : 0.025648, loss_ce: 0.009609
2022-01-06 14:58:09,846 iteration 6720 : loss : 0.027803, loss_ce: 0.004787
2022-01-06 14:58:11,009 iteration 6721 : loss : 0.022692, loss_ce: 0.010733
2022-01-06 14:58:12,119 iteration 6722 : loss : 0.013851, loss_ce: 0.005990
2022-01-06 14:58:13,253 iteration 6723 : loss : 0.016810, loss_ce: 0.006289
2022-01-06 14:58:14,324 iteration 6724 : loss : 0.021946, loss_ce: 0.009702
2022-01-06 14:58:15,432 iteration 6725 : loss : 0.013316, loss_ce: 0.004678
2022-01-06 14:58:16,470 iteration 6726 : loss : 0.010641, loss_ce: 0.003416
2022-01-06 14:58:17,670 iteration 6727 : loss : 0.023874, loss_ce: 0.009066
2022-01-06 14:58:18,743 iteration 6728 : loss : 0.019085, loss_ce: 0.007304
2022-01-06 14:58:19,903 iteration 6729 : loss : 0.018363, loss_ce: 0.006104
2022-01-06 14:58:21,118 iteration 6730 : loss : 0.018418, loss_ce: 0.007538
2022-01-06 14:58:22,293 iteration 6731 : loss : 0.023942, loss_ce: 0.008720
2022-01-06 14:58:23,430 iteration 6732 : loss : 0.012461, loss_ce: 0.005460
 99%|████████████████████████████▋| 396/400 [2:16:50<01:23, 20.91s/it]2022-01-06 14:58:24,572 iteration 6733 : loss : 0.011798, loss_ce: 0.005050
2022-01-06 14:58:25,687 iteration 6734 : loss : 0.026448, loss_ce: 0.006793
2022-01-06 14:58:26,844 iteration 6735 : loss : 0.033037, loss_ce: 0.009414
2022-01-06 14:58:27,950 iteration 6736 : loss : 0.012816, loss_ce: 0.005834
2022-01-06 14:58:29,024 iteration 6737 : loss : 0.017073, loss_ce: 0.008987
2022-01-06 14:58:30,082 iteration 6738 : loss : 0.017392, loss_ce: 0.005290
2022-01-06 14:58:31,200 iteration 6739 : loss : 0.014798, loss_ce: 0.006499
2022-01-06 14:58:32,340 iteration 6740 : loss : 0.022183, loss_ce: 0.008133
2022-01-06 14:58:33,465 iteration 6741 : loss : 0.018718, loss_ce: 0.007434
2022-01-06 14:58:34,545 iteration 6742 : loss : 0.016800, loss_ce: 0.007708
2022-01-06 14:58:35,589 iteration 6743 : loss : 0.017266, loss_ce: 0.004117
2022-01-06 14:58:36,735 iteration 6744 : loss : 0.026631, loss_ce: 0.010624
2022-01-06 14:58:37,833 iteration 6745 : loss : 0.015676, loss_ce: 0.006779
2022-01-06 14:58:38,872 iteration 6746 : loss : 0.012610, loss_ce: 0.004041
2022-01-06 14:58:39,943 iteration 6747 : loss : 0.016802, loss_ce: 0.006016
2022-01-06 14:58:41,072 iteration 6748 : loss : 0.012560, loss_ce: 0.005417
2022-01-06 14:58:42,162 iteration 6749 : loss : 0.015805, loss_ce: 0.004293
 99%|████████████████████████████▊| 397/400 [2:17:09<01:00, 20.26s/it]2022-01-06 14:58:43,315 iteration 6750 : loss : 0.013423, loss_ce: 0.006443
2022-01-06 14:58:44,485 iteration 6751 : loss : 0.016047, loss_ce: 0.006566
2022-01-06 14:58:45,522 iteration 6752 : loss : 0.016023, loss_ce: 0.004883
2022-01-06 14:58:46,633 iteration 6753 : loss : 0.020108, loss_ce: 0.007899
2022-01-06 14:58:47,776 iteration 6754 : loss : 0.012955, loss_ce: 0.004348
2022-01-06 14:58:48,862 iteration 6755 : loss : 0.016216, loss_ce: 0.004808
2022-01-06 14:58:49,955 iteration 6756 : loss : 0.015454, loss_ce: 0.007750
2022-01-06 14:58:51,147 iteration 6757 : loss : 0.018083, loss_ce: 0.005381
2022-01-06 14:58:52,243 iteration 6758 : loss : 0.019966, loss_ce: 0.009970
2022-01-06 14:58:53,389 iteration 6759 : loss : 0.017935, loss_ce: 0.007563
2022-01-06 14:58:54,584 iteration 6760 : loss : 0.022073, loss_ce: 0.004037
2022-01-06 14:58:55,701 iteration 6761 : loss : 0.018631, loss_ce: 0.006823
2022-01-06 14:58:56,785 iteration 6762 : loss : 0.023494, loss_ce: 0.006649
2022-01-06 14:58:57,936 iteration 6763 : loss : 0.023247, loss_ce: 0.008082
2022-01-06 14:58:58,999 iteration 6764 : loss : 0.019343, loss_ce: 0.007667
2022-01-06 14:59:00,157 iteration 6765 : loss : 0.020375, loss_ce: 0.007661
2022-01-06 14:59:01,232 iteration 6766 : loss : 0.012684, loss_ce: 0.005566
100%|████████████████████████████▊| 398/400 [2:17:28<00:39, 19.90s/it]2022-01-06 14:59:02,403 iteration 6767 : loss : 0.016968, loss_ce: 0.005413
2022-01-06 14:59:03,589 iteration 6768 : loss : 0.015157, loss_ce: 0.005333
2022-01-06 14:59:04,713 iteration 6769 : loss : 0.016729, loss_ce: 0.006764
2022-01-06 14:59:05,795 iteration 6770 : loss : 0.017291, loss_ce: 0.008527
2022-01-06 14:59:06,991 iteration 6771 : loss : 0.023478, loss_ce: 0.005089
2022-01-06 14:59:08,057 iteration 6772 : loss : 0.015115, loss_ce: 0.005523
2022-01-06 14:59:09,042 iteration 6773 : loss : 0.014640, loss_ce: 0.004306
2022-01-06 14:59:10,182 iteration 6774 : loss : 0.041645, loss_ce: 0.016828
2022-01-06 14:59:11,334 iteration 6775 : loss : 0.021169, loss_ce: 0.006801
2022-01-06 14:59:12,427 iteration 6776 : loss : 0.017027, loss_ce: 0.005748
2022-01-06 14:59:13,603 iteration 6777 : loss : 0.026082, loss_ce: 0.015590
2022-01-06 14:59:14,687 iteration 6778 : loss : 0.012890, loss_ce: 0.004378
2022-01-06 14:59:15,861 iteration 6779 : loss : 0.018346, loss_ce: 0.007427
2022-01-06 14:59:16,978 iteration 6780 : loss : 0.016815, loss_ce: 0.007830
2022-01-06 14:59:18,139 iteration 6781 : loss : 0.019467, loss_ce: 0.008833
2022-01-06 14:59:19,299 iteration 6782 : loss : 0.015999, loss_ce: 0.006148
2022-01-06 14:59:20,397 iteration 6783 : loss : 0.016038, loss_ce: 0.005502
100%|████████████████████████████▉| 399/400 [2:17:47<00:19, 19.68s/it]2022-01-06 14:59:21,647 iteration 6784 : loss : 0.018575, loss_ce: 0.005990
2022-01-06 14:59:22,799 iteration 6785 : loss : 0.015135, loss_ce: 0.004841
2022-01-06 14:59:23,884 iteration 6786 : loss : 0.011785, loss_ce: 0.004586
2022-01-06 14:59:24,968 iteration 6787 : loss : 0.013255, loss_ce: 0.004492
2022-01-06 14:59:26,109 iteration 6788 : loss : 0.018603, loss_ce: 0.007687
2022-01-06 14:59:27,228 iteration 6789 : loss : 0.024024, loss_ce: 0.006600
2022-01-06 14:59:28,315 iteration 6790 : loss : 0.011295, loss_ce: 0.004811
2022-01-06 14:59:29,448 iteration 6791 : loss : 0.018807, loss_ce: 0.007019
2022-01-06 14:59:30,508 iteration 6792 : loss : 0.018531, loss_ce: 0.008073
2022-01-06 14:59:31,690 iteration 6793 : loss : 0.019314, loss_ce: 0.006056
2022-01-06 14:59:32,801 iteration 6794 : loss : 0.035963, loss_ce: 0.012475
2022-01-06 14:59:33,946 iteration 6795 : loss : 0.030856, loss_ce: 0.011658
2022-01-06 14:59:35,110 iteration 6796 : loss : 0.016267, loss_ce: 0.007709
2022-01-06 14:59:36,144 iteration 6797 : loss : 0.014737, loss_ce: 0.005206
2022-01-06 14:59:37,291 iteration 6798 : loss : 0.019473, loss_ce: 0.006243
2022-01-06 14:59:38,492 iteration 6799 : loss : 0.022940, loss_ce: 0.011062
2022-01-06 14:59:38,493 Training Data Eval:
2022-01-06 14:59:44,040   Average segmentation loss on training set: 0.0099
2022-01-06 14:59:44,041 Validation Data Eval:
2022-01-06 14:59:45,972   Average segmentation loss on validation set: 0.0743
2022-01-06 14:59:47,055 iteration 6800 : loss : 0.013781, loss_ce: 0.003483
100%|█████████████████████████████| 400/400 [2:18:14<00:00, 21.78s/it]100%|█████████████████████████████| 400/400 [2:18:14<00:00, 20.74s/it]
