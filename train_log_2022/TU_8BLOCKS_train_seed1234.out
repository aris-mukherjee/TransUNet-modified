2022-01-08 09:20:29,288 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:20:29,289 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:20:29,289 ============================================================
2022-01-08 09:20:29,289 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:20:29,289 ============================================================
2022-01-08 09:20:29,289 Loading data...
2022-01-08 09:20:29,289 Reading NCI - RUNMC images...
2022-01-08 09:20:29,289 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-08 09:20:29,292 Already preprocessed this configuration. Loading now!
2022-01-08 09:20:29,316 Training Images: (256, 256, 286)
2022-01-08 09:20:29,316 Training Labels: (256, 256, 286)
2022-01-08 09:20:29,316 Validation Images: (256, 256, 98)
2022-01-08 09:20:29,316 Validation Labels: (256, 256, 98)
2022-01-08 09:20:29,317 ============================================================
2022-01-08 09:20:29,365 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-08 09:20:31,975 iteration 1 : loss : 1.077009, loss_ce: 1.371862
2022-01-08 09:20:33,213 iteration 2 : loss : 0.997308, loss_ce: 1.236072
2022-01-08 09:20:34,574 iteration 3 : loss : 0.930859, loss_ce: 1.122022
2022-01-08 09:20:35,834 iteration 4 : loss : 0.908347, loss_ce: 1.076706
2022-01-08 09:20:37,096 iteration 5 : loss : 0.856479, loss_ce: 1.001336
2022-01-08 09:20:38,377 iteration 6 : loss : 0.811111, loss_ce: 0.925673
2022-01-08 09:20:39,720 iteration 7 : loss : 0.764414, loss_ce: 0.857401
2022-01-08 09:20:41,000 iteration 8 : loss : 0.741903, loss_ce: 0.794856
2022-01-08 09:20:42,296 iteration 9 : loss : 0.682056, loss_ce: 0.751037
2022-01-08 09:20:43,621 iteration 10 : loss : 0.680520, loss_ce: 0.689384
2022-01-08 09:20:45,000 iteration 11 : loss : 0.636643, loss_ce: 0.651122
2022-01-08 09:20:46,268 iteration 12 : loss : 0.606484, loss_ce: 0.597808
2022-01-08 09:20:47,512 iteration 13 : loss : 0.589032, loss_ce: 0.558258
2022-01-08 09:20:48,730 iteration 14 : loss : 0.554818, loss_ce: 0.515585
2022-01-08 09:20:50,022 iteration 15 : loss : 0.523049, loss_ce: 0.474969
2022-01-08 09:20:51,298 iteration 16 : loss : 0.522714, loss_ce: 0.449095
2022-01-08 09:20:52,562 iteration 17 : loss : 0.471029, loss_ce: 0.411922
  0%|                               | 1/400 [00:23<2:34:42, 23.27s/it]2022-01-08 09:20:53,922 iteration 18 : loss : 0.483876, loss_ce: 0.365917
2022-01-08 09:20:55,118 iteration 19 : loss : 0.433833, loss_ce: 0.335497
2022-01-08 09:20:56,452 iteration 20 : loss : 0.413555, loss_ce: 0.305190
2022-01-08 09:20:57,695 iteration 21 : loss : 0.419159, loss_ce: 0.279176
2022-01-08 09:20:58,966 iteration 22 : loss : 0.382701, loss_ce: 0.270686
2022-01-08 09:21:00,315 iteration 23 : loss : 0.379708, loss_ce: 0.237271
2022-01-08 09:21:01,583 iteration 24 : loss : 0.361169, loss_ce: 0.229463
2022-01-08 09:21:02,908 iteration 25 : loss : 0.384702, loss_ce: 0.261545
2022-01-08 09:21:04,154 iteration 26 : loss : 0.341471, loss_ce: 0.204102
2022-01-08 09:21:05,350 iteration 27 : loss : 0.330266, loss_ce: 0.202447
2022-01-08 09:21:06,555 iteration 28 : loss : 0.324530, loss_ce: 0.184037
2022-01-08 09:21:07,865 iteration 29 : loss : 0.319333, loss_ce: 0.176018
2022-01-08 09:21:09,163 iteration 30 : loss : 0.315151, loss_ce: 0.170451
2022-01-08 09:21:10,369 iteration 31 : loss : 0.297392, loss_ce: 0.161671
2022-01-08 09:21:11,701 iteration 32 : loss : 0.323294, loss_ce: 0.188619
2022-01-08 09:21:13,011 iteration 33 : loss : 0.307789, loss_ce: 0.173132
2022-01-08 09:21:14,316 iteration 34 : loss : 0.313141, loss_ce: 0.182729
  0%|▏                              | 2/400 [00:45<2:28:21, 22.37s/it]2022-01-08 09:21:15,662 iteration 35 : loss : 0.284758, loss_ce: 0.143957
2022-01-08 09:21:16,986 iteration 36 : loss : 0.292217, loss_ce: 0.156802
2022-01-08 09:21:18,318 iteration 37 : loss : 0.288549, loss_ce: 0.134984
2022-01-08 09:21:19,565 iteration 38 : loss : 0.260344, loss_ce: 0.131290
2022-01-08 09:21:20,821 iteration 39 : loss : 0.250666, loss_ce: 0.124890
2022-01-08 09:21:22,151 iteration 40 : loss : 0.296683, loss_ce: 0.148944
2022-01-08 09:21:23,472 iteration 41 : loss : 0.317308, loss_ce: 0.157717
2022-01-08 09:21:24,770 iteration 42 : loss : 0.275182, loss_ce: 0.134627
2022-01-08 09:21:25,978 iteration 43 : loss : 0.266976, loss_ce: 0.124602
2022-01-08 09:21:27,324 iteration 44 : loss : 0.232896, loss_ce: 0.113028
2022-01-08 09:21:28,623 iteration 45 : loss : 0.231781, loss_ce: 0.110732
2022-01-08 09:21:29,927 iteration 46 : loss : 0.266684, loss_ce: 0.112108
2022-01-08 09:21:31,260 iteration 47 : loss : 0.229324, loss_ce: 0.098536
2022-01-08 09:21:32,568 iteration 48 : loss : 0.249529, loss_ce: 0.107739
2022-01-08 09:21:33,917 iteration 49 : loss : 0.303723, loss_ce: 0.143635
2022-01-08 09:21:35,179 iteration 50 : loss : 0.331691, loss_ce: 0.154461
2022-01-08 09:21:36,466 iteration 51 : loss : 0.273072, loss_ce: 0.129031
  1%|▏                              | 3/400 [01:07<2:27:19, 22.27s/it]2022-01-08 09:21:37,931 iteration 52 : loss : 0.291022, loss_ce: 0.148263
2022-01-08 09:21:39,349 iteration 53 : loss : 0.258900, loss_ce: 0.125308
2022-01-08 09:21:40,728 iteration 54 : loss : 0.286243, loss_ce: 0.123489
2022-01-08 09:21:42,113 iteration 55 : loss : 0.338025, loss_ce: 0.182058
2022-01-08 09:21:43,483 iteration 56 : loss : 0.281481, loss_ce: 0.130999
2022-01-08 09:21:44,873 iteration 57 : loss : 0.280433, loss_ce: 0.135539
2022-01-08 09:21:46,263 iteration 58 : loss : 0.291460, loss_ce: 0.126736
2022-01-08 09:21:47,615 iteration 59 : loss : 0.259155, loss_ce: 0.118628
2022-01-08 09:21:49,012 iteration 60 : loss : 0.290096, loss_ce: 0.122853
2022-01-08 09:21:50,385 iteration 61 : loss : 0.282225, loss_ce: 0.139859
2022-01-08 09:21:51,750 iteration 62 : loss : 0.343868, loss_ce: 0.142536
2022-01-08 09:21:53,040 iteration 63 : loss : 0.297086, loss_ce: 0.148998
2022-01-08 09:21:54,394 iteration 64 : loss : 0.368839, loss_ce: 0.170555
2022-01-08 09:21:55,702 iteration 65 : loss : 0.270778, loss_ce: 0.104479
2022-01-08 09:21:57,043 iteration 66 : loss : 0.255608, loss_ce: 0.108321
2022-01-08 09:21:58,453 iteration 67 : loss : 0.272098, loss_ce: 0.095848
2022-01-08 09:21:59,802 iteration 68 : loss : 0.282332, loss_ce: 0.128447
  1%|▎                              | 4/400 [01:30<2:29:43, 22.69s/it]2022-01-08 09:22:01,222 iteration 69 : loss : 0.244880, loss_ce: 0.109878
2022-01-08 09:22:02,659 iteration 70 : loss : 0.253542, loss_ce: 0.112019
2022-01-08 09:22:04,014 iteration 71 : loss : 0.247033, loss_ce: 0.105838
2022-01-08 09:22:05,398 iteration 72 : loss : 0.235972, loss_ce: 0.104946
2022-01-08 09:22:06,720 iteration 73 : loss : 0.260183, loss_ce: 0.129962
2022-01-08 09:22:08,020 iteration 74 : loss : 0.233011, loss_ce: 0.100751
2022-01-08 09:22:09,365 iteration 75 : loss : 0.252354, loss_ce: 0.110627
2022-01-08 09:22:10,702 iteration 76 : loss : 0.253345, loss_ce: 0.109598
2022-01-08 09:22:11,946 iteration 77 : loss : 0.246264, loss_ce: 0.114896
2022-01-08 09:22:13,297 iteration 78 : loss : 0.271721, loss_ce: 0.123942
2022-01-08 09:22:14,614 iteration 79 : loss : 0.278786, loss_ce: 0.109183
2022-01-08 09:22:15,924 iteration 80 : loss : 0.281058, loss_ce: 0.132742
2022-01-08 09:22:17,239 iteration 81 : loss : 0.254628, loss_ce: 0.109223
2022-01-08 09:22:18,588 iteration 82 : loss : 0.245019, loss_ce: 0.097302
2022-01-08 09:22:19,924 iteration 83 : loss : 0.270821, loss_ce: 0.103182
2022-01-08 09:22:21,338 iteration 84 : loss : 0.265135, loss_ce: 0.148673
2022-01-08 09:22:21,338 Training Data Eval:
2022-01-08 09:22:28,199   Average segmentation loss on training set: 0.2166
2022-01-08 09:22:28,200 Validation Data Eval:
2022-01-08 09:22:30,689   Average segmentation loss on validation set: 0.2188
2022-01-08 09:22:34,819 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:22:36,071 iteration 85 : loss : 0.279032, loss_ce: 0.122429
  1%|▍                              | 5/400 [02:06<3:01:35, 27.58s/it]2022-01-08 09:22:37,389 iteration 86 : loss : 0.291421, loss_ce: 0.112407
2022-01-08 09:22:38,788 iteration 87 : loss : 0.203904, loss_ce: 0.083138
2022-01-08 09:22:40,004 iteration 88 : loss : 0.245966, loss_ce: 0.106186
2022-01-08 09:22:41,302 iteration 89 : loss : 0.276450, loss_ce: 0.113801
2022-01-08 09:22:42,591 iteration 90 : loss : 0.245850, loss_ce: 0.100478
2022-01-08 09:22:43,944 iteration 91 : loss : 0.234362, loss_ce: 0.117802
2022-01-08 09:22:45,152 iteration 92 : loss : 0.233637, loss_ce: 0.108366
2022-01-08 09:22:46,412 iteration 93 : loss : 0.250057, loss_ce: 0.094475
2022-01-08 09:22:47,720 iteration 94 : loss : 0.253619, loss_ce: 0.100737
2022-01-08 09:22:49,196 iteration 95 : loss : 0.239679, loss_ce: 0.109503
2022-01-08 09:22:50,543 iteration 96 : loss : 0.230993, loss_ce: 0.099283
2022-01-08 09:22:51,897 iteration 97 : loss : 0.263437, loss_ce: 0.108867
2022-01-08 09:22:53,257 iteration 98 : loss : 0.267992, loss_ce: 0.119622
2022-01-08 09:22:54,698 iteration 99 : loss : 0.239222, loss_ce: 0.108650
2022-01-08 09:22:56,088 iteration 100 : loss : 0.230491, loss_ce: 0.098093
2022-01-08 09:22:57,458 iteration 101 : loss : 0.168977, loss_ce: 0.065937
2022-01-08 09:22:58,760 iteration 102 : loss : 0.235749, loss_ce: 0.098224
  2%|▍                              | 6/400 [02:29<2:50:13, 25.92s/it]2022-01-08 09:23:00,257 iteration 103 : loss : 0.214259, loss_ce: 0.092928
2022-01-08 09:23:01,705 iteration 104 : loss : 0.248156, loss_ce: 0.105323
2022-01-08 09:23:03,179 iteration 105 : loss : 0.286375, loss_ce: 0.118242
2022-01-08 09:23:04,497 iteration 106 : loss : 0.229354, loss_ce: 0.095166
2022-01-08 09:23:06,024 iteration 107 : loss : 0.247011, loss_ce: 0.110944
2022-01-08 09:23:07,317 iteration 108 : loss : 0.268230, loss_ce: 0.109048
2022-01-08 09:23:08,647 iteration 109 : loss : 0.209490, loss_ce: 0.089057
2022-01-08 09:23:09,931 iteration 110 : loss : 0.219914, loss_ce: 0.096852
2022-01-08 09:23:11,314 iteration 111 : loss : 0.253385, loss_ce: 0.117796
2022-01-08 09:23:12,618 iteration 112 : loss : 0.220365, loss_ce: 0.087605
2022-01-08 09:23:13,987 iteration 113 : loss : 0.266860, loss_ce: 0.137742
2022-01-08 09:23:15,303 iteration 114 : loss : 0.260065, loss_ce: 0.095320
2022-01-08 09:23:16,640 iteration 115 : loss : 0.229905, loss_ce: 0.098222
2022-01-08 09:23:18,035 iteration 116 : loss : 0.248114, loss_ce: 0.115693
2022-01-08 09:23:19,413 iteration 117 : loss : 0.228898, loss_ce: 0.104091
2022-01-08 09:23:20,775 iteration 118 : loss : 0.251431, loss_ce: 0.114837
2022-01-08 09:23:22,150 iteration 119 : loss : 0.204024, loss_ce: 0.081007
  2%|▌                              | 7/400 [02:52<2:44:22, 25.10s/it]2022-01-08 09:23:23,511 iteration 120 : loss : 0.320991, loss_ce: 0.157243
2022-01-08 09:23:24,810 iteration 121 : loss : 0.204362, loss_ce: 0.082464
2022-01-08 09:23:26,221 iteration 122 : loss : 0.207286, loss_ce: 0.073018
2022-01-08 09:23:27,592 iteration 123 : loss : 0.222502, loss_ce: 0.086246
2022-01-08 09:23:28,944 iteration 124 : loss : 0.241242, loss_ce: 0.091851
2022-01-08 09:23:30,257 iteration 125 : loss : 0.223675, loss_ce: 0.105304
2022-01-08 09:23:31,635 iteration 126 : loss : 0.230935, loss_ce: 0.083563
2022-01-08 09:23:32,947 iteration 127 : loss : 0.218362, loss_ce: 0.099691
2022-01-08 09:23:34,286 iteration 128 : loss : 0.204913, loss_ce: 0.087972
2022-01-08 09:23:35,685 iteration 129 : loss : 0.224129, loss_ce: 0.088511
2022-01-08 09:23:37,026 iteration 130 : loss : 0.239695, loss_ce: 0.093078
2022-01-08 09:23:38,462 iteration 131 : loss : 0.225640, loss_ce: 0.114613
2022-01-08 09:23:39,910 iteration 132 : loss : 0.194187, loss_ce: 0.061584
2022-01-08 09:23:41,236 iteration 133 : loss : 0.203433, loss_ce: 0.077654
2022-01-08 09:23:42,658 iteration 134 : loss : 0.232065, loss_ce: 0.093557
2022-01-08 09:23:44,091 iteration 135 : loss : 0.233824, loss_ce: 0.102156
2022-01-08 09:23:45,430 iteration 136 : loss : 0.199449, loss_ce: 0.091147
  2%|▌                              | 8/400 [03:16<2:40:09, 24.51s/it]2022-01-08 09:23:46,882 iteration 137 : loss : 0.232747, loss_ce: 0.087192
2022-01-08 09:23:48,213 iteration 138 : loss : 0.236131, loss_ce: 0.126347
2022-01-08 09:23:49,598 iteration 139 : loss : 0.262783, loss_ce: 0.109191
2022-01-08 09:23:50,946 iteration 140 : loss : 0.217569, loss_ce: 0.079937
2022-01-08 09:23:52,350 iteration 141 : loss : 0.210558, loss_ce: 0.103036
2022-01-08 09:23:53,765 iteration 142 : loss : 0.224159, loss_ce: 0.090381
2022-01-08 09:23:55,218 iteration 143 : loss : 0.204572, loss_ce: 0.095037
2022-01-08 09:23:56,643 iteration 144 : loss : 0.223890, loss_ce: 0.090885
2022-01-08 09:23:57,998 iteration 145 : loss : 0.247724, loss_ce: 0.099888
2022-01-08 09:23:59,391 iteration 146 : loss : 0.177663, loss_ce: 0.084040
2022-01-08 09:24:00,786 iteration 147 : loss : 0.205568, loss_ce: 0.083406
2022-01-08 09:24:02,058 iteration 148 : loss : 0.207548, loss_ce: 0.090586
2022-01-08 09:24:03,411 iteration 149 : loss : 0.250561, loss_ce: 0.108705
2022-01-08 09:24:04,736 iteration 150 : loss : 0.258256, loss_ce: 0.093158
2022-01-08 09:24:06,066 iteration 151 : loss : 0.207361, loss_ce: 0.094949
2022-01-08 09:24:07,400 iteration 152 : loss : 0.209204, loss_ce: 0.071459
2022-01-08 09:24:08,782 iteration 153 : loss : 0.231898, loss_ce: 0.097047
  2%|▋                              | 9/400 [03:39<2:37:23, 24.15s/it]2022-01-08 09:24:10,130 iteration 154 : loss : 0.262356, loss_ce: 0.112430
2022-01-08 09:24:11,500 iteration 155 : loss : 0.208850, loss_ce: 0.074591
2022-01-08 09:24:12,940 iteration 156 : loss : 0.154736, loss_ce: 0.061575
2022-01-08 09:24:14,318 iteration 157 : loss : 0.241579, loss_ce: 0.090088
2022-01-08 09:24:15,798 iteration 158 : loss : 0.227482, loss_ce: 0.099858
2022-01-08 09:24:17,054 iteration 159 : loss : 0.219268, loss_ce: 0.097863
2022-01-08 09:24:18,493 iteration 160 : loss : 0.164530, loss_ce: 0.056351
2022-01-08 09:24:19,909 iteration 161 : loss : 0.253198, loss_ce: 0.102831
2022-01-08 09:24:21,347 iteration 162 : loss : 0.226597, loss_ce: 0.109020
2022-01-08 09:24:22,707 iteration 163 : loss : 0.242208, loss_ce: 0.099178
2022-01-08 09:24:24,085 iteration 164 : loss : 0.197570, loss_ce: 0.076863
2022-01-08 09:24:25,435 iteration 165 : loss : 0.245160, loss_ce: 0.110139
2022-01-08 09:24:26,773 iteration 166 : loss : 0.220883, loss_ce: 0.088075
2022-01-08 09:24:28,133 iteration 167 : loss : 0.181150, loss_ce: 0.077435
2022-01-08 09:24:29,524 iteration 168 : loss : 0.220312, loss_ce: 0.112782
2022-01-08 09:24:30,894 iteration 169 : loss : 0.201594, loss_ce: 0.105337
2022-01-08 09:24:30,895 Training Data Eval:
2022-01-08 09:24:37,712   Average segmentation loss on training set: 3.8434
2022-01-08 09:24:37,712 Validation Data Eval:
2022-01-08 09:24:40,070   Average segmentation loss on validation set: 3.7157
2022-01-08 09:24:41,486 iteration 170 : loss : 0.198115, loss_ce: 0.092886
  2%|▊                             | 10/400 [04:12<2:54:08, 26.79s/it]2022-01-08 09:24:42,915 iteration 171 : loss : 0.222300, loss_ce: 0.094125
2022-01-08 09:24:44,329 iteration 172 : loss : 0.204221, loss_ce: 0.094135
2022-01-08 09:24:45,765 iteration 173 : loss : 0.191323, loss_ce: 0.089270
2022-01-08 09:24:47,085 iteration 174 : loss : 0.226847, loss_ce: 0.099753
2022-01-08 09:24:48,394 iteration 175 : loss : 0.274332, loss_ce: 0.118258
2022-01-08 09:24:49,784 iteration 176 : loss : 0.204909, loss_ce: 0.094746
2022-01-08 09:24:51,157 iteration 177 : loss : 0.224659, loss_ce: 0.092951
2022-01-08 09:24:52,526 iteration 178 : loss : 0.164286, loss_ce: 0.072341
2022-01-08 09:24:53,879 iteration 179 : loss : 0.210953, loss_ce: 0.092999
2022-01-08 09:24:55,222 iteration 180 : loss : 0.189922, loss_ce: 0.076252
2022-01-08 09:24:56,614 iteration 181 : loss : 0.174886, loss_ce: 0.077150
2022-01-08 09:24:57,940 iteration 182 : loss : 0.161148, loss_ce: 0.071912
2022-01-08 09:24:59,255 iteration 183 : loss : 0.176521, loss_ce: 0.066371
2022-01-08 09:25:00,525 iteration 184 : loss : 0.214419, loss_ce: 0.085099
2022-01-08 09:25:01,921 iteration 185 : loss : 0.283593, loss_ce: 0.108351
2022-01-08 09:25:03,308 iteration 186 : loss : 0.262280, loss_ce: 0.114130
2022-01-08 09:25:04,736 iteration 187 : loss : 0.229459, loss_ce: 0.094150
  3%|▊                             | 11/400 [04:35<2:46:39, 25.71s/it]2022-01-08 09:25:06,171 iteration 188 : loss : 0.244731, loss_ce: 0.101106
2022-01-08 09:25:07,553 iteration 189 : loss : 0.206973, loss_ce: 0.090494
2022-01-08 09:25:08,944 iteration 190 : loss : 0.214740, loss_ce: 0.082940
2022-01-08 09:25:10,281 iteration 191 : loss : 0.175445, loss_ce: 0.071287
2022-01-08 09:25:11,579 iteration 192 : loss : 0.215124, loss_ce: 0.104403
2022-01-08 09:25:13,004 iteration 193 : loss : 0.222488, loss_ce: 0.106507
2022-01-08 09:25:14,349 iteration 194 : loss : 0.283186, loss_ce: 0.093270
2022-01-08 09:25:15,686 iteration 195 : loss : 0.233556, loss_ce: 0.109807
2022-01-08 09:25:16,967 iteration 196 : loss : 0.217931, loss_ce: 0.083622
2022-01-08 09:25:18,336 iteration 197 : loss : 0.269301, loss_ce: 0.118896
2022-01-08 09:25:19,728 iteration 198 : loss : 0.183485, loss_ce: 0.086547
2022-01-08 09:25:21,053 iteration 199 : loss : 0.208057, loss_ce: 0.091516
2022-01-08 09:25:22,399 iteration 200 : loss : 0.197102, loss_ce: 0.084353
2022-01-08 09:25:23,783 iteration 201 : loss : 0.144311, loss_ce: 0.059317
2022-01-08 09:25:25,181 iteration 202 : loss : 0.196852, loss_ce: 0.087418
2022-01-08 09:25:26,504 iteration 203 : loss : 0.179772, loss_ce: 0.073248
2022-01-08 09:25:27,948 iteration 204 : loss : 0.255719, loss_ce: 0.101738
  3%|▉                             | 12/400 [04:58<2:41:19, 24.95s/it]2022-01-08 09:25:29,296 iteration 205 : loss : 0.166209, loss_ce: 0.070639
2022-01-08 09:25:30,649 iteration 206 : loss : 0.173668, loss_ce: 0.060213
2022-01-08 09:25:32,042 iteration 207 : loss : 0.153623, loss_ce: 0.069867
2022-01-08 09:25:33,392 iteration 208 : loss : 0.213238, loss_ce: 0.067074
2022-01-08 09:25:34,829 iteration 209 : loss : 0.256527, loss_ce: 0.119536
2022-01-08 09:25:36,102 iteration 210 : loss : 0.204861, loss_ce: 0.074499
2022-01-08 09:25:37,502 iteration 211 : loss : 0.213509, loss_ce: 0.091548
2022-01-08 09:25:38,877 iteration 212 : loss : 0.216416, loss_ce: 0.082964
2022-01-08 09:25:40,347 iteration 213 : loss : 0.166233, loss_ce: 0.087805
2022-01-08 09:25:41,712 iteration 214 : loss : 0.200674, loss_ce: 0.072221
2022-01-08 09:25:43,083 iteration 215 : loss : 0.205447, loss_ce: 0.087869
2022-01-08 09:25:44,601 iteration 216 : loss : 0.173217, loss_ce: 0.071027
2022-01-08 09:25:46,051 iteration 217 : loss : 0.157018, loss_ce: 0.062986
2022-01-08 09:25:47,430 iteration 218 : loss : 0.176972, loss_ce: 0.088812
2022-01-08 09:25:48,700 iteration 219 : loss : 0.183365, loss_ce: 0.081141
2022-01-08 09:25:50,097 iteration 220 : loss : 0.185079, loss_ce: 0.088811
2022-01-08 09:25:51,467 iteration 221 : loss : 0.219610, loss_ce: 0.086623
  3%|▉                             | 13/400 [05:22<2:38:07, 24.52s/it]2022-01-08 09:25:52,889 iteration 222 : loss : 0.198132, loss_ce: 0.097485
2022-01-08 09:25:54,298 iteration 223 : loss : 0.165734, loss_ce: 0.072898
2022-01-08 09:25:55,650 iteration 224 : loss : 0.251901, loss_ce: 0.132498
2022-01-08 09:25:57,099 iteration 225 : loss : 0.298493, loss_ce: 0.098380
2022-01-08 09:25:58,460 iteration 226 : loss : 0.154720, loss_ce: 0.060278
2022-01-08 09:25:59,874 iteration 227 : loss : 0.231144, loss_ce: 0.106605
2022-01-08 09:26:01,237 iteration 228 : loss : 0.192228, loss_ce: 0.076235
2022-01-08 09:26:02,561 iteration 229 : loss : 0.204835, loss_ce: 0.077084
2022-01-08 09:26:03,936 iteration 230 : loss : 0.180983, loss_ce: 0.078715
2022-01-08 09:26:05,345 iteration 231 : loss : 0.245853, loss_ce: 0.097042
2022-01-08 09:26:06,739 iteration 232 : loss : 0.167237, loss_ce: 0.071210
2022-01-08 09:26:08,062 iteration 233 : loss : 0.248073, loss_ce: 0.110098
2022-01-08 09:26:09,496 iteration 234 : loss : 0.224514, loss_ce: 0.090390
2022-01-08 09:26:10,842 iteration 235 : loss : 0.204942, loss_ce: 0.089361
2022-01-08 09:26:12,237 iteration 236 : loss : 0.187482, loss_ce: 0.081450
2022-01-08 09:26:13,591 iteration 237 : loss : 0.159128, loss_ce: 0.062251
2022-01-08 09:26:14,959 iteration 238 : loss : 0.182852, loss_ce: 0.083760
  4%|█                             | 14/400 [05:45<2:35:43, 24.21s/it]2022-01-08 09:26:16,313 iteration 239 : loss : 0.194303, loss_ce: 0.073098
2022-01-08 09:26:17,719 iteration 240 : loss : 0.176592, loss_ce: 0.086780
2022-01-08 09:26:19,129 iteration 241 : loss : 0.223675, loss_ce: 0.090646
2022-01-08 09:26:20,515 iteration 242 : loss : 0.201486, loss_ce: 0.108520
2022-01-08 09:26:22,041 iteration 243 : loss : 0.240149, loss_ce: 0.103013
2022-01-08 09:26:23,336 iteration 244 : loss : 0.215643, loss_ce: 0.063978
2022-01-08 09:26:24,718 iteration 245 : loss : 0.179792, loss_ce: 0.085988
2022-01-08 09:26:26,134 iteration 246 : loss : 0.191228, loss_ce: 0.079431
2022-01-08 09:26:27,489 iteration 247 : loss : 0.240259, loss_ce: 0.079206
2022-01-08 09:26:28,860 iteration 248 : loss : 0.167118, loss_ce: 0.053026
2022-01-08 09:26:30,195 iteration 249 : loss : 0.226513, loss_ce: 0.119879
2022-01-08 09:26:31,550 iteration 250 : loss : 0.201196, loss_ce: 0.084770
2022-01-08 09:26:32,853 iteration 251 : loss : 0.196941, loss_ce: 0.063965
2022-01-08 09:26:34,209 iteration 252 : loss : 0.159405, loss_ce: 0.064000
2022-01-08 09:26:35,595 iteration 253 : loss : 0.223287, loss_ce: 0.099236
2022-01-08 09:26:37,032 iteration 254 : loss : 0.276175, loss_ce: 0.090530
2022-01-08 09:26:37,032 Training Data Eval:
2022-01-08 09:26:43,869   Average segmentation loss on training set: 0.3743
2022-01-08 09:26:43,870 Validation Data Eval:
2022-01-08 09:26:46,227   Average segmentation loss on validation set: 0.3437
2022-01-08 09:26:47,566 iteration 255 : loss : 0.179673, loss_ce: 0.080123
  4%|█▏                            | 15/400 [06:18<2:51:35, 26.74s/it]2022-01-08 09:26:49,007 iteration 256 : loss : 0.171442, loss_ce: 0.078075
2022-01-08 09:26:50,439 iteration 257 : loss : 0.172321, loss_ce: 0.076245
2022-01-08 09:26:51,911 iteration 258 : loss : 0.180353, loss_ce: 0.070943
2022-01-08 09:26:53,203 iteration 259 : loss : 0.132824, loss_ce: 0.064034
2022-01-08 09:26:54,645 iteration 260 : loss : 0.161159, loss_ce: 0.071348
2022-01-08 09:26:56,044 iteration 261 : loss : 0.247429, loss_ce: 0.092747
2022-01-08 09:26:57,518 iteration 262 : loss : 0.196638, loss_ce: 0.078676
2022-01-08 09:26:58,891 iteration 263 : loss : 0.147827, loss_ce: 0.063511
2022-01-08 09:27:00,297 iteration 264 : loss : 0.230497, loss_ce: 0.123589
2022-01-08 09:27:01,626 iteration 265 : loss : 0.130929, loss_ce: 0.057567
2022-01-08 09:27:03,034 iteration 266 : loss : 0.165013, loss_ce: 0.083157
2022-01-08 09:27:04,407 iteration 267 : loss : 0.148301, loss_ce: 0.048291
2022-01-08 09:27:05,781 iteration 268 : loss : 0.186952, loss_ce: 0.081775
2022-01-08 09:27:07,344 iteration 269 : loss : 0.197536, loss_ce: 0.075697
2022-01-08 09:27:08,809 iteration 270 : loss : 0.177732, loss_ce: 0.082473
2022-01-08 09:27:10,061 iteration 271 : loss : 0.206178, loss_ce: 0.065669
2022-01-08 09:27:11,457 iteration 272 : loss : 0.204894, loss_ce: 0.100612
  4%|█▏                            | 16/400 [06:42<2:45:38, 25.88s/it]2022-01-08 09:27:12,823 iteration 273 : loss : 0.154411, loss_ce: 0.058983
2022-01-08 09:27:14,138 iteration 274 : loss : 0.150768, loss_ce: 0.065080
2022-01-08 09:27:15,466 iteration 275 : loss : 0.150800, loss_ce: 0.060218
2022-01-08 09:27:16,846 iteration 276 : loss : 0.151521, loss_ce: 0.076952
2022-01-08 09:27:18,239 iteration 277 : loss : 0.181688, loss_ce: 0.070419
2022-01-08 09:27:19,476 iteration 278 : loss : 0.193327, loss_ce: 0.069190
2022-01-08 09:27:20,793 iteration 279 : loss : 0.195700, loss_ce: 0.070514
2022-01-08 09:27:22,114 iteration 280 : loss : 0.194341, loss_ce: 0.082315
2022-01-08 09:27:23,497 iteration 281 : loss : 0.161584, loss_ce: 0.065264
2022-01-08 09:27:24,848 iteration 282 : loss : 0.234164, loss_ce: 0.095692
2022-01-08 09:27:26,184 iteration 283 : loss : 0.215727, loss_ce: 0.106899
2022-01-08 09:27:27,611 iteration 284 : loss : 0.279714, loss_ce: 0.129214
2022-01-08 09:27:28,909 iteration 285 : loss : 0.223684, loss_ce: 0.070098
2022-01-08 09:27:30,235 iteration 286 : loss : 0.176148, loss_ce: 0.073628
2022-01-08 09:27:31,682 iteration 287 : loss : 0.184487, loss_ce: 0.081820
2022-01-08 09:27:33,106 iteration 288 : loss : 0.215862, loss_ce: 0.091301
2022-01-08 09:27:34,454 iteration 289 : loss : 0.171853, loss_ce: 0.073069
  4%|█▎                            | 17/400 [07:05<2:39:40, 25.01s/it]2022-01-08 09:27:35,873 iteration 290 : loss : 0.177137, loss_ce: 0.073267
2022-01-08 09:27:37,201 iteration 291 : loss : 0.191368, loss_ce: 0.080182
2022-01-08 09:27:38,543 iteration 292 : loss : 0.150570, loss_ce: 0.064611
2022-01-08 09:27:39,896 iteration 293 : loss : 0.216999, loss_ce: 0.092895
2022-01-08 09:27:41,266 iteration 294 : loss : 0.176342, loss_ce: 0.075009
2022-01-08 09:27:42,724 iteration 295 : loss : 0.179643, loss_ce: 0.061413
2022-01-08 09:27:44,058 iteration 296 : loss : 0.173973, loss_ce: 0.063749
2022-01-08 09:27:45,436 iteration 297 : loss : 0.166575, loss_ce: 0.062725
2022-01-08 09:27:46,774 iteration 298 : loss : 0.142494, loss_ce: 0.050116
2022-01-08 09:27:48,136 iteration 299 : loss : 0.170620, loss_ce: 0.058955
2022-01-08 09:27:49,501 iteration 300 : loss : 0.183950, loss_ce: 0.064697
2022-01-08 09:27:50,876 iteration 301 : loss : 0.230364, loss_ce: 0.126077
2022-01-08 09:27:52,302 iteration 302 : loss : 0.136172, loss_ce: 0.058421
2022-01-08 09:27:53,704 iteration 303 : loss : 0.179079, loss_ce: 0.080384
2022-01-08 09:27:55,088 iteration 304 : loss : 0.237141, loss_ce: 0.109445
2022-01-08 09:27:56,498 iteration 305 : loss : 0.171589, loss_ce: 0.074429
2022-01-08 09:27:57,906 iteration 306 : loss : 0.132393, loss_ce: 0.070642
  4%|█▎                            | 18/400 [07:28<2:36:16, 24.55s/it]2022-01-08 09:27:59,338 iteration 307 : loss : 0.201199, loss_ce: 0.078248
2022-01-08 09:28:00,683 iteration 308 : loss : 0.155755, loss_ce: 0.080620
2022-01-08 09:28:02,078 iteration 309 : loss : 0.265314, loss_ce: 0.124778
2022-01-08 09:28:03,495 iteration 310 : loss : 0.180444, loss_ce: 0.083619
2022-01-08 09:28:04,824 iteration 311 : loss : 0.206395, loss_ce: 0.070953
2022-01-08 09:28:06,186 iteration 312 : loss : 0.145001, loss_ce: 0.069276
2022-01-08 09:28:07,612 iteration 313 : loss : 0.180680, loss_ce: 0.099101
2022-01-08 09:28:08,905 iteration 314 : loss : 0.147579, loss_ce: 0.071337
2022-01-08 09:28:10,250 iteration 315 : loss : 0.204356, loss_ce: 0.075654
2022-01-08 09:28:11,635 iteration 316 : loss : 0.177288, loss_ce: 0.065022
2022-01-08 09:28:13,012 iteration 317 : loss : 0.210939, loss_ce: 0.088874
2022-01-08 09:28:14,404 iteration 318 : loss : 0.132761, loss_ce: 0.059044
2022-01-08 09:28:15,724 iteration 319 : loss : 0.135410, loss_ce: 0.061835
2022-01-08 09:28:17,169 iteration 320 : loss : 0.145814, loss_ce: 0.053642
2022-01-08 09:28:18,574 iteration 321 : loss : 0.144271, loss_ce: 0.049252
2022-01-08 09:28:19,934 iteration 322 : loss : 0.211895, loss_ce: 0.099559
2022-01-08 09:28:21,358 iteration 323 : loss : 0.171167, loss_ce: 0.055557
  5%|█▍                            | 19/400 [07:52<2:33:46, 24.22s/it]2022-01-08 09:28:22,754 iteration 324 : loss : 0.177897, loss_ce: 0.064047
2022-01-08 09:28:24,129 iteration 325 : loss : 0.192183, loss_ce: 0.067791
2022-01-08 09:28:25,532 iteration 326 : loss : 0.152749, loss_ce: 0.070580
2022-01-08 09:28:26,997 iteration 327 : loss : 0.202222, loss_ce: 0.076861
2022-01-08 09:28:28,355 iteration 328 : loss : 0.198017, loss_ce: 0.090020
2022-01-08 09:28:29,759 iteration 329 : loss : 0.156035, loss_ce: 0.083090
2022-01-08 09:28:31,181 iteration 330 : loss : 0.167401, loss_ce: 0.071757
2022-01-08 09:28:32,517 iteration 331 : loss : 0.153065, loss_ce: 0.073391
2022-01-08 09:28:33,882 iteration 332 : loss : 0.152194, loss_ce: 0.054727
2022-01-08 09:28:35,269 iteration 333 : loss : 0.126289, loss_ce: 0.061483
2022-01-08 09:28:36,644 iteration 334 : loss : 0.175131, loss_ce: 0.079054
2022-01-08 09:28:37,985 iteration 335 : loss : 0.234924, loss_ce: 0.077910
2022-01-08 09:28:39,394 iteration 336 : loss : 0.144410, loss_ce: 0.055339
2022-01-08 09:28:40,765 iteration 337 : loss : 0.153029, loss_ce: 0.055958
2022-01-08 09:28:42,102 iteration 338 : loss : 0.173015, loss_ce: 0.096168
2022-01-08 09:28:43,435 iteration 339 : loss : 0.167203, loss_ce: 0.061652
2022-01-08 09:28:43,435 Training Data Eval:
2022-01-08 09:28:50,316   Average segmentation loss on training set: 0.1327
2022-01-08 09:28:50,316 Validation Data Eval:
2022-01-08 09:28:52,681   Average segmentation loss on validation set: 0.1747
2022-01-08 09:28:56,850 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:28:58,194 iteration 340 : loss : 0.167695, loss_ce: 0.069735
  5%|█▌                            | 20/400 [08:28<2:57:21, 28.00s/it]2022-01-08 09:28:59,562 iteration 341 : loss : 0.110535, loss_ce: 0.048896
2022-01-08 09:29:00,870 iteration 342 : loss : 0.137381, loss_ce: 0.050168
2022-01-08 09:29:02,145 iteration 343 : loss : 0.157741, loss_ce: 0.057061
2022-01-08 09:29:03,465 iteration 344 : loss : 0.191382, loss_ce: 0.079453
2022-01-08 09:29:04,740 iteration 345 : loss : 0.124128, loss_ce: 0.040407
2022-01-08 09:29:06,083 iteration 346 : loss : 0.174313, loss_ce: 0.063870
2022-01-08 09:29:07,315 iteration 347 : loss : 0.173731, loss_ce: 0.086867
2022-01-08 09:29:08,618 iteration 348 : loss : 0.166458, loss_ce: 0.061658
2022-01-08 09:29:10,049 iteration 349 : loss : 0.166111, loss_ce: 0.086059
2022-01-08 09:29:11,389 iteration 350 : loss : 0.137719, loss_ce: 0.052091
2022-01-08 09:29:12,828 iteration 351 : loss : 0.127991, loss_ce: 0.063830
2022-01-08 09:29:14,252 iteration 352 : loss : 0.137112, loss_ce: 0.058817
2022-01-08 09:29:15,588 iteration 353 : loss : 0.148838, loss_ce: 0.061275
2022-01-08 09:29:16,987 iteration 354 : loss : 0.196695, loss_ce: 0.074796
2022-01-08 09:29:18,432 iteration 355 : loss : 0.153304, loss_ce: 0.060998
2022-01-08 09:29:19,944 iteration 356 : loss : 0.167958, loss_ce: 0.077229
2022-01-08 09:29:21,265 iteration 357 : loss : 0.116954, loss_ce: 0.047723
  5%|█▌                            | 21/400 [08:51<2:47:32, 26.52s/it]2022-01-08 09:29:22,720 iteration 358 : loss : 0.167125, loss_ce: 0.077673
2022-01-08 09:29:24,164 iteration 359 : loss : 0.154495, loss_ce: 0.059499
2022-01-08 09:29:25,480 iteration 360 : loss : 0.120874, loss_ce: 0.046167
2022-01-08 09:29:26,897 iteration 361 : loss : 0.161176, loss_ce: 0.057279
2022-01-08 09:29:28,360 iteration 362 : loss : 0.150009, loss_ce: 0.045458
2022-01-08 09:29:29,805 iteration 363 : loss : 0.149259, loss_ce: 0.057685
2022-01-08 09:29:31,191 iteration 364 : loss : 0.173361, loss_ce: 0.067259
2022-01-08 09:29:32,480 iteration 365 : loss : 0.154778, loss_ce: 0.072221
2022-01-08 09:29:33,898 iteration 366 : loss : 0.159869, loss_ce: 0.072823
2022-01-08 09:29:35,214 iteration 367 : loss : 0.154082, loss_ce: 0.067113
2022-01-08 09:29:36,648 iteration 368 : loss : 0.187016, loss_ce: 0.070143
2022-01-08 09:29:38,109 iteration 369 : loss : 0.189853, loss_ce: 0.063704
2022-01-08 09:29:39,545 iteration 370 : loss : 0.112353, loss_ce: 0.060170
2022-01-08 09:29:40,945 iteration 371 : loss : 0.224721, loss_ce: 0.074890
2022-01-08 09:29:42,359 iteration 372 : loss : 0.186601, loss_ce: 0.096817
2022-01-08 09:29:43,684 iteration 373 : loss : 0.163189, loss_ce: 0.053416
2022-01-08 09:29:45,052 iteration 374 : loss : 0.143461, loss_ce: 0.055163
  6%|█▋                            | 22/400 [09:15<2:41:56, 25.70s/it]2022-01-08 09:29:46,528 iteration 375 : loss : 0.230326, loss_ce: 0.099596
2022-01-08 09:29:47,850 iteration 376 : loss : 0.100397, loss_ce: 0.037193
2022-01-08 09:29:49,209 iteration 377 : loss : 0.126261, loss_ce: 0.040298
2022-01-08 09:29:50,651 iteration 378 : loss : 0.120065, loss_ce: 0.046115
2022-01-08 09:29:51,948 iteration 379 : loss : 0.131325, loss_ce: 0.062628
2022-01-08 09:29:53,358 iteration 380 : loss : 0.106909, loss_ce: 0.041082
2022-01-08 09:29:54,718 iteration 381 : loss : 0.118374, loss_ce: 0.042399
2022-01-08 09:29:56,006 iteration 382 : loss : 0.153509, loss_ce: 0.082113
2022-01-08 09:29:57,381 iteration 383 : loss : 0.174723, loss_ce: 0.073146
2022-01-08 09:29:58,774 iteration 384 : loss : 0.107766, loss_ce: 0.051979
2022-01-08 09:30:00,161 iteration 385 : loss : 0.116820, loss_ce: 0.054562
2022-01-08 09:30:01,542 iteration 386 : loss : 0.177747, loss_ce: 0.068293
2022-01-08 09:30:02,932 iteration 387 : loss : 0.217162, loss_ce: 0.079752
2022-01-08 09:30:04,347 iteration 388 : loss : 0.160979, loss_ce: 0.060826
2022-01-08 09:30:05,749 iteration 389 : loss : 0.247400, loss_ce: 0.129469
2022-01-08 09:30:07,110 iteration 390 : loss : 0.154889, loss_ce: 0.064469
2022-01-08 09:30:08,466 iteration 391 : loss : 0.181297, loss_ce: 0.102143
  6%|█▋                            | 23/400 [09:39<2:37:10, 25.01s/it]2022-01-08 09:30:09,901 iteration 392 : loss : 0.147844, loss_ce: 0.074845
2022-01-08 09:30:11,237 iteration 393 : loss : 0.131780, loss_ce: 0.054605
2022-01-08 09:30:12,676 iteration 394 : loss : 0.125007, loss_ce: 0.052239
2022-01-08 09:30:14,073 iteration 395 : loss : 0.155991, loss_ce: 0.076713
2022-01-08 09:30:15,416 iteration 396 : loss : 0.168292, loss_ce: 0.067210
2022-01-08 09:30:16,793 iteration 397 : loss : 0.163519, loss_ce: 0.076041
2022-01-08 09:30:18,202 iteration 398 : loss : 0.116365, loss_ce: 0.050107
2022-01-08 09:30:19,489 iteration 399 : loss : 0.144851, loss_ce: 0.067746
2022-01-08 09:30:20,881 iteration 400 : loss : 0.151984, loss_ce: 0.053126
2022-01-08 09:30:22,311 iteration 401 : loss : 0.161387, loss_ce: 0.069530
2022-01-08 09:30:23,660 iteration 402 : loss : 0.106260, loss_ce: 0.047493
2022-01-08 09:30:25,027 iteration 403 : loss : 0.108034, loss_ce: 0.043724
2022-01-08 09:30:26,443 iteration 404 : loss : 0.169808, loss_ce: 0.071149
2022-01-08 09:30:27,885 iteration 405 : loss : 0.163628, loss_ce: 0.065956
2022-01-08 09:30:29,229 iteration 406 : loss : 0.126821, loss_ce: 0.053749
2022-01-08 09:30:30,543 iteration 407 : loss : 0.144846, loss_ce: 0.055048
2022-01-08 09:30:31,912 iteration 408 : loss : 0.169634, loss_ce: 0.063795
  6%|█▊                            | 24/400 [10:02<2:33:49, 24.55s/it]2022-01-08 09:30:33,353 iteration 409 : loss : 0.160190, loss_ce: 0.079054
2022-01-08 09:30:34,752 iteration 410 : loss : 0.146227, loss_ce: 0.059078
2022-01-08 09:30:36,120 iteration 411 : loss : 0.217787, loss_ce: 0.066861
2022-01-08 09:30:37,404 iteration 412 : loss : 0.115724, loss_ce: 0.044819
2022-01-08 09:30:38,773 iteration 413 : loss : 0.163279, loss_ce: 0.065169
2022-01-08 09:30:40,098 iteration 414 : loss : 0.194586, loss_ce: 0.106048
2022-01-08 09:30:41,370 iteration 415 : loss : 0.109957, loss_ce: 0.044827
2022-01-08 09:30:42,735 iteration 416 : loss : 0.155939, loss_ce: 0.082347
2022-01-08 09:30:44,065 iteration 417 : loss : 0.211050, loss_ce: 0.094753
2022-01-08 09:30:45,494 iteration 418 : loss : 0.120927, loss_ce: 0.053805
2022-01-08 09:30:46,835 iteration 419 : loss : 0.165129, loss_ce: 0.100811
2022-01-08 09:30:48,221 iteration 420 : loss : 0.143858, loss_ce: 0.058109
2022-01-08 09:30:49,549 iteration 421 : loss : 0.184367, loss_ce: 0.082237
2022-01-08 09:30:50,865 iteration 422 : loss : 0.162628, loss_ce: 0.074548
2022-01-08 09:30:52,200 iteration 423 : loss : 0.153141, loss_ce: 0.053898
2022-01-08 09:30:53,583 iteration 424 : loss : 0.158119, loss_ce: 0.074444
2022-01-08 09:30:53,583 Training Data Eval:
2022-01-08 09:31:00,455   Average segmentation loss on training set: 0.1229
2022-01-08 09:31:00,455 Validation Data Eval:
2022-01-08 09:31:02,818   Average segmentation loss on validation set: 0.1693
2022-01-08 09:31:06,971 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:31:08,245 iteration 425 : loss : 0.149653, loss_ce: 0.063805
  6%|█▉                            | 25/400 [10:38<2:55:30, 28.08s/it]2022-01-08 09:31:09,507 iteration 426 : loss : 0.118545, loss_ce: 0.038950
2022-01-08 09:31:10,823 iteration 427 : loss : 0.144982, loss_ce: 0.057460
2022-01-08 09:31:12,106 iteration 428 : loss : 0.212857, loss_ce: 0.092705
2022-01-08 09:31:13,344 iteration 429 : loss : 0.204166, loss_ce: 0.108761
2022-01-08 09:31:14,566 iteration 430 : loss : 0.154633, loss_ce: 0.056862
2022-01-08 09:31:15,814 iteration 431 : loss : 0.190440, loss_ce: 0.071475
2022-01-08 09:31:17,118 iteration 432 : loss : 0.117324, loss_ce: 0.052040
2022-01-08 09:31:18,485 iteration 433 : loss : 0.123639, loss_ce: 0.053399
2022-01-08 09:31:19,846 iteration 434 : loss : 0.148438, loss_ce: 0.055856
2022-01-08 09:31:21,167 iteration 435 : loss : 0.107358, loss_ce: 0.051562
2022-01-08 09:31:22,524 iteration 436 : loss : 0.125468, loss_ce: 0.059630
2022-01-08 09:31:23,889 iteration 437 : loss : 0.201514, loss_ce: 0.104606
2022-01-08 09:31:25,201 iteration 438 : loss : 0.138507, loss_ce: 0.053634
2022-01-08 09:31:26,587 iteration 439 : loss : 0.190624, loss_ce: 0.068241
2022-01-08 09:31:27,946 iteration 440 : loss : 0.196613, loss_ce: 0.078369
2022-01-08 09:31:29,365 iteration 441 : loss : 0.220259, loss_ce: 0.090030
2022-01-08 09:31:30,755 iteration 442 : loss : 0.138754, loss_ce: 0.062286
  6%|█▉                            | 26/400 [11:01<2:44:37, 26.41s/it]2022-01-08 09:31:32,155 iteration 443 : loss : 0.161228, loss_ce: 0.078751
2022-01-08 09:31:33,480 iteration 444 : loss : 0.153426, loss_ce: 0.062921
2022-01-08 09:31:34,846 iteration 445 : loss : 0.174712, loss_ce: 0.061177
2022-01-08 09:31:36,341 iteration 446 : loss : 0.167473, loss_ce: 0.067893
2022-01-08 09:31:37,777 iteration 447 : loss : 0.126962, loss_ce: 0.072174
2022-01-08 09:31:39,195 iteration 448 : loss : 0.257078, loss_ce: 0.081775
2022-01-08 09:31:40,565 iteration 449 : loss : 0.146900, loss_ce: 0.073228
2022-01-08 09:31:41,916 iteration 450 : loss : 0.167044, loss_ce: 0.071772
2022-01-08 09:31:43,365 iteration 451 : loss : 0.102838, loss_ce: 0.046513
2022-01-08 09:31:44,760 iteration 452 : loss : 0.127769, loss_ce: 0.057095
2022-01-08 09:31:46,144 iteration 453 : loss : 0.183111, loss_ce: 0.080602
2022-01-08 09:31:47,531 iteration 454 : loss : 0.155324, loss_ce: 0.059590
2022-01-08 09:31:48,956 iteration 455 : loss : 0.114395, loss_ce: 0.044257
2022-01-08 09:31:50,352 iteration 456 : loss : 0.109257, loss_ce: 0.042320
2022-01-08 09:31:51,743 iteration 457 : loss : 0.170537, loss_ce: 0.070898
2022-01-08 09:31:53,199 iteration 458 : loss : 0.180441, loss_ce: 0.080794
2022-01-08 09:31:54,512 iteration 459 : loss : 0.095234, loss_ce: 0.037427
  7%|██                            | 27/400 [11:25<2:39:13, 25.61s/it]2022-01-08 09:31:55,939 iteration 460 : loss : 0.142798, loss_ce: 0.072068
2022-01-08 09:31:57,328 iteration 461 : loss : 0.124002, loss_ce: 0.059688
2022-01-08 09:31:58,689 iteration 462 : loss : 0.147116, loss_ce: 0.060190
2022-01-08 09:32:00,129 iteration 463 : loss : 0.115161, loss_ce: 0.049831
2022-01-08 09:32:01,505 iteration 464 : loss : 0.098945, loss_ce: 0.042096
2022-01-08 09:32:02,834 iteration 465 : loss : 0.156613, loss_ce: 0.050863
2022-01-08 09:32:04,202 iteration 466 : loss : 0.130219, loss_ce: 0.044067
2022-01-08 09:32:05,551 iteration 467 : loss : 0.140788, loss_ce: 0.048706
2022-01-08 09:32:07,017 iteration 468 : loss : 0.130460, loss_ce: 0.056744
2022-01-08 09:32:08,352 iteration 469 : loss : 0.111145, loss_ce: 0.039938
2022-01-08 09:32:09,705 iteration 470 : loss : 0.140222, loss_ce: 0.058991
2022-01-08 09:32:11,076 iteration 471 : loss : 0.131508, loss_ce: 0.046102
2022-01-08 09:32:12,511 iteration 472 : loss : 0.116319, loss_ce: 0.047602
2022-01-08 09:32:14,024 iteration 473 : loss : 0.135534, loss_ce: 0.054443
2022-01-08 09:32:15,485 iteration 474 : loss : 0.177197, loss_ce: 0.073784
2022-01-08 09:32:16,831 iteration 475 : loss : 0.096345, loss_ce: 0.038420
2022-01-08 09:32:18,268 iteration 476 : loss : 0.135771, loss_ce: 0.071436
  7%|██                            | 28/400 [11:48<2:35:21, 25.06s/it]2022-01-08 09:32:19,676 iteration 477 : loss : 0.125574, loss_ce: 0.042869
2022-01-08 09:32:21,091 iteration 478 : loss : 0.083852, loss_ce: 0.031502
2022-01-08 09:32:22,458 iteration 479 : loss : 0.132070, loss_ce: 0.056021
2022-01-08 09:32:23,839 iteration 480 : loss : 0.112007, loss_ce: 0.057722
2022-01-08 09:32:25,137 iteration 481 : loss : 0.165552, loss_ce: 0.074516
2022-01-08 09:32:26,529 iteration 482 : loss : 0.136938, loss_ce: 0.034635
2022-01-08 09:32:27,928 iteration 483 : loss : 0.158379, loss_ce: 0.058395
2022-01-08 09:32:29,331 iteration 484 : loss : 0.180549, loss_ce: 0.094796
2022-01-08 09:32:30,686 iteration 485 : loss : 0.108325, loss_ce: 0.046857
2022-01-08 09:32:32,030 iteration 486 : loss : 0.148687, loss_ce: 0.048816
2022-01-08 09:32:33,466 iteration 487 : loss : 0.111989, loss_ce: 0.048536
2022-01-08 09:32:34,779 iteration 488 : loss : 0.106896, loss_ce: 0.047395
2022-01-08 09:32:36,150 iteration 489 : loss : 0.134668, loss_ce: 0.074050
2022-01-08 09:32:37,424 iteration 490 : loss : 0.167759, loss_ce: 0.071311
2022-01-08 09:32:38,806 iteration 491 : loss : 0.119216, loss_ce: 0.058320
2022-01-08 09:32:40,162 iteration 492 : loss : 0.092155, loss_ce: 0.044223
2022-01-08 09:32:41,564 iteration 493 : loss : 0.163970, loss_ce: 0.085888
  7%|██▏                           | 29/400 [12:12<2:31:39, 24.53s/it]2022-01-08 09:32:42,977 iteration 494 : loss : 0.119249, loss_ce: 0.059642
2022-01-08 09:32:44,389 iteration 495 : loss : 0.117365, loss_ce: 0.046194
2022-01-08 09:32:45,745 iteration 496 : loss : 0.174847, loss_ce: 0.094271
2022-01-08 09:32:47,122 iteration 497 : loss : 0.131617, loss_ce: 0.057632
2022-01-08 09:32:48,540 iteration 498 : loss : 0.128878, loss_ce: 0.060803
2022-01-08 09:32:49,848 iteration 499 : loss : 0.094807, loss_ce: 0.042976
2022-01-08 09:32:51,193 iteration 500 : loss : 0.092799, loss_ce: 0.033974
2022-01-08 09:32:52,537 iteration 501 : loss : 0.093060, loss_ce: 0.036231
2022-01-08 09:32:53,979 iteration 502 : loss : 0.092232, loss_ce: 0.039871
2022-01-08 09:32:55,321 iteration 503 : loss : 0.144740, loss_ce: 0.050165
2022-01-08 09:32:56,736 iteration 504 : loss : 0.092567, loss_ce: 0.031306
2022-01-08 09:32:58,074 iteration 505 : loss : 0.091069, loss_ce: 0.035227
2022-01-08 09:32:59,534 iteration 506 : loss : 0.096150, loss_ce: 0.039945
2022-01-08 09:33:00,905 iteration 507 : loss : 0.151204, loss_ce: 0.054164
2022-01-08 09:33:02,280 iteration 508 : loss : 0.141246, loss_ce: 0.059529
2022-01-08 09:33:03,695 iteration 509 : loss : 0.110078, loss_ce: 0.041595
2022-01-08 09:33:03,695 Training Data Eval:
2022-01-08 09:33:10,534   Average segmentation loss on training set: 0.1299
2022-01-08 09:33:10,535 Validation Data Eval:
2022-01-08 09:33:12,902   Average segmentation loss on validation set: 0.2082
2022-01-08 09:33:14,331 iteration 510 : loss : 0.096069, loss_ce: 0.043618
  8%|██▎                           | 30/400 [12:44<2:46:29, 27.00s/it]2022-01-08 09:33:15,772 iteration 511 : loss : 0.153790, loss_ce: 0.075785
2022-01-08 09:33:17,101 iteration 512 : loss : 0.159036, loss_ce: 0.067329
2022-01-08 09:33:18,452 iteration 513 : loss : 0.064214, loss_ce: 0.021350
2022-01-08 09:33:19,830 iteration 514 : loss : 0.108005, loss_ce: 0.048365
2022-01-08 09:33:21,180 iteration 515 : loss : 0.139293, loss_ce: 0.062911
2022-01-08 09:33:22,573 iteration 516 : loss : 0.084961, loss_ce: 0.034660
2022-01-08 09:33:23,999 iteration 517 : loss : 0.118199, loss_ce: 0.057381
2022-01-08 09:33:25,323 iteration 518 : loss : 0.128526, loss_ce: 0.050453
2022-01-08 09:33:26,689 iteration 519 : loss : 0.126805, loss_ce: 0.057571
2022-01-08 09:33:28,006 iteration 520 : loss : 0.103659, loss_ce: 0.048801
2022-01-08 09:33:29,376 iteration 521 : loss : 0.098274, loss_ce: 0.036320
2022-01-08 09:33:30,785 iteration 522 : loss : 0.108347, loss_ce: 0.041367
2022-01-08 09:33:32,215 iteration 523 : loss : 0.118141, loss_ce: 0.048055
2022-01-08 09:33:33,678 iteration 524 : loss : 0.125895, loss_ce: 0.056280
2022-01-08 09:33:35,041 iteration 525 : loss : 0.155437, loss_ce: 0.069022
2022-01-08 09:33:36,404 iteration 526 : loss : 0.128549, loss_ce: 0.063986
2022-01-08 09:33:37,772 iteration 527 : loss : 0.108456, loss_ce: 0.039034
  8%|██▎                           | 31/400 [13:08<2:39:29, 25.93s/it]2022-01-08 09:33:39,153 iteration 528 : loss : 0.114264, loss_ce: 0.051560
2022-01-08 09:33:40,549 iteration 529 : loss : 0.140124, loss_ce: 0.042456
2022-01-08 09:33:41,894 iteration 530 : loss : 0.108386, loss_ce: 0.042406
2022-01-08 09:33:43,224 iteration 531 : loss : 0.108042, loss_ce: 0.033688
2022-01-08 09:33:44,571 iteration 532 : loss : 0.119930, loss_ce: 0.058569
2022-01-08 09:33:46,033 iteration 533 : loss : 0.082204, loss_ce: 0.031350
2022-01-08 09:33:47,335 iteration 534 : loss : 0.077146, loss_ce: 0.035697
2022-01-08 09:33:48,734 iteration 535 : loss : 0.079288, loss_ce: 0.027847
2022-01-08 09:33:50,167 iteration 536 : loss : 0.121029, loss_ce: 0.055994
2022-01-08 09:33:51,620 iteration 537 : loss : 0.115388, loss_ce: 0.040181
2022-01-08 09:33:53,012 iteration 538 : loss : 0.084676, loss_ce: 0.035749
2022-01-08 09:33:54,362 iteration 539 : loss : 0.110610, loss_ce: 0.045556
2022-01-08 09:33:55,747 iteration 540 : loss : 0.115710, loss_ce: 0.039928
2022-01-08 09:33:57,090 iteration 541 : loss : 0.100998, loss_ce: 0.048275
2022-01-08 09:33:58,454 iteration 542 : loss : 0.096142, loss_ce: 0.044219
2022-01-08 09:33:59,770 iteration 543 : loss : 0.085429, loss_ce: 0.036495
2022-01-08 09:34:01,121 iteration 544 : loss : 0.095659, loss_ce: 0.034351
  8%|██▍                           | 32/400 [13:31<2:34:17, 25.16s/it]2022-01-08 09:34:02,559 iteration 545 : loss : 0.081298, loss_ce: 0.040725
2022-01-08 09:34:03,924 iteration 546 : loss : 0.130373, loss_ce: 0.047082
2022-01-08 09:34:05,298 iteration 547 : loss : 0.130478, loss_ce: 0.064280
2022-01-08 09:34:06,654 iteration 548 : loss : 0.198829, loss_ce: 0.074527
2022-01-08 09:34:08,038 iteration 549 : loss : 0.117085, loss_ce: 0.055514
2022-01-08 09:34:09,369 iteration 550 : loss : 0.122381, loss_ce: 0.062051
2022-01-08 09:34:10,728 iteration 551 : loss : 0.110762, loss_ce: 0.059087
2022-01-08 09:34:12,070 iteration 552 : loss : 0.102145, loss_ce: 0.043426
2022-01-08 09:34:13,478 iteration 553 : loss : 0.125369, loss_ce: 0.050750
2022-01-08 09:34:14,795 iteration 554 : loss : 0.088399, loss_ce: 0.044027
2022-01-08 09:34:16,152 iteration 555 : loss : 0.132061, loss_ce: 0.046447
2022-01-08 09:34:17,555 iteration 556 : loss : 0.105897, loss_ce: 0.050454
2022-01-08 09:34:18,851 iteration 557 : loss : 0.103672, loss_ce: 0.039834
2022-01-08 09:34:20,265 iteration 558 : loss : 0.133817, loss_ce: 0.042895
2022-01-08 09:34:21,629 iteration 559 : loss : 0.111088, loss_ce: 0.046304
2022-01-08 09:34:23,008 iteration 560 : loss : 0.098020, loss_ce: 0.026170
2022-01-08 09:34:24,402 iteration 561 : loss : 0.177307, loss_ce: 0.096800
  8%|██▍                           | 33/400 [13:55<2:30:26, 24.60s/it]2022-01-08 09:34:25,916 iteration 562 : loss : 0.157134, loss_ce: 0.079689
2022-01-08 09:34:27,191 iteration 563 : loss : 0.109714, loss_ce: 0.054507
2022-01-08 09:34:28,584 iteration 564 : loss : 0.110904, loss_ce: 0.058728
2022-01-08 09:34:29,983 iteration 565 : loss : 0.139509, loss_ce: 0.067101
2022-01-08 09:34:31,400 iteration 566 : loss : 0.095454, loss_ce: 0.039468
2022-01-08 09:34:32,749 iteration 567 : loss : 0.114436, loss_ce: 0.039816
2022-01-08 09:34:34,082 iteration 568 : loss : 0.193307, loss_ce: 0.069077
2022-01-08 09:34:35,441 iteration 569 : loss : 0.116864, loss_ce: 0.043892
2022-01-08 09:34:36,841 iteration 570 : loss : 0.080947, loss_ce: 0.039348
2022-01-08 09:34:38,235 iteration 571 : loss : 0.181374, loss_ce: 0.077254
2022-01-08 09:34:39,611 iteration 572 : loss : 0.099400, loss_ce: 0.036937
2022-01-08 09:34:40,948 iteration 573 : loss : 0.090144, loss_ce: 0.035533
2022-01-08 09:34:42,315 iteration 574 : loss : 0.129720, loss_ce: 0.048156
2022-01-08 09:34:43,743 iteration 575 : loss : 0.093459, loss_ce: 0.031825
2022-01-08 09:34:45,159 iteration 576 : loss : 0.119155, loss_ce: 0.056383
2022-01-08 09:34:46,525 iteration 577 : loss : 0.077646, loss_ce: 0.033459
2022-01-08 09:34:47,857 iteration 578 : loss : 0.096904, loss_ce: 0.040405
  8%|██▌                           | 34/400 [14:18<2:27:56, 24.25s/it]2022-01-08 09:34:49,431 iteration 579 : loss : 0.092594, loss_ce: 0.043088
2022-01-08 09:34:50,800 iteration 580 : loss : 0.111749, loss_ce: 0.042315
2022-01-08 09:34:52,211 iteration 581 : loss : 0.128861, loss_ce: 0.038285
2022-01-08 09:34:53,565 iteration 582 : loss : 0.124594, loss_ce: 0.054840
2022-01-08 09:34:54,931 iteration 583 : loss : 0.170891, loss_ce: 0.046874
2022-01-08 09:34:56,264 iteration 584 : loss : 0.080004, loss_ce: 0.037061
2022-01-08 09:34:57,676 iteration 585 : loss : 0.097312, loss_ce: 0.036771
2022-01-08 09:34:59,010 iteration 586 : loss : 0.145433, loss_ce: 0.057707
2022-01-08 09:35:00,346 iteration 587 : loss : 0.106999, loss_ce: 0.050718
2022-01-08 09:35:01,741 iteration 588 : loss : 0.104643, loss_ce: 0.037964
2022-01-08 09:35:03,082 iteration 589 : loss : 0.083749, loss_ce: 0.031656
2022-01-08 09:35:04,438 iteration 590 : loss : 0.121601, loss_ce: 0.046773
2022-01-08 09:35:05,811 iteration 591 : loss : 0.099453, loss_ce: 0.046786
2022-01-08 09:35:07,184 iteration 592 : loss : 0.119692, loss_ce: 0.045359
2022-01-08 09:35:08,592 iteration 593 : loss : 0.060061, loss_ce: 0.024713
2022-01-08 09:35:09,978 iteration 594 : loss : 0.109189, loss_ce: 0.046108
2022-01-08 09:35:09,979 Training Data Eval:
2022-01-08 09:35:16,841   Average segmentation loss on training set: 0.2364
2022-01-08 09:35:16,842 Validation Data Eval:
2022-01-08 09:35:19,219   Average segmentation loss on validation set: 0.2283
2022-01-08 09:35:20,664 iteration 595 : loss : 0.119201, loss_ce: 0.066767
  9%|██▋                           | 35/400 [14:51<2:43:08, 26.82s/it]2022-01-08 09:35:22,077 iteration 596 : loss : 0.097138, loss_ce: 0.039744
2022-01-08 09:35:23,405 iteration 597 : loss : 0.116181, loss_ce: 0.044988
2022-01-08 09:35:24,738 iteration 598 : loss : 0.076048, loss_ce: 0.031810
2022-01-08 09:35:26,094 iteration 599 : loss : 0.111276, loss_ce: 0.044122
2022-01-08 09:35:27,452 iteration 600 : loss : 0.097723, loss_ce: 0.044868
2022-01-08 09:35:28,843 iteration 601 : loss : 0.084709, loss_ce: 0.039033
2022-01-08 09:35:30,241 iteration 602 : loss : 0.065165, loss_ce: 0.028529
2022-01-08 09:35:31,556 iteration 603 : loss : 0.113999, loss_ce: 0.040967
2022-01-08 09:35:32,878 iteration 604 : loss : 0.086119, loss_ce: 0.036590
2022-01-08 09:35:34,194 iteration 605 : loss : 0.099140, loss_ce: 0.038847
2022-01-08 09:35:35,689 iteration 606 : loss : 0.092047, loss_ce: 0.044860
2022-01-08 09:35:37,036 iteration 607 : loss : 0.117567, loss_ce: 0.051752
2022-01-08 09:35:38,384 iteration 608 : loss : 0.113465, loss_ce: 0.054265
2022-01-08 09:35:39,665 iteration 609 : loss : 0.084595, loss_ce: 0.036599
2022-01-08 09:35:41,075 iteration 610 : loss : 0.077141, loss_ce: 0.033836
2022-01-08 09:35:42,541 iteration 611 : loss : 0.107600, loss_ce: 0.051103
2022-01-08 09:35:43,923 iteration 612 : loss : 0.081660, loss_ce: 0.028854
  9%|██▋                           | 36/400 [15:14<2:36:14, 25.75s/it]2022-01-08 09:35:45,350 iteration 613 : loss : 0.090505, loss_ce: 0.037546
2022-01-08 09:35:46,675 iteration 614 : loss : 0.096501, loss_ce: 0.034936
2022-01-08 09:35:47,995 iteration 615 : loss : 0.135878, loss_ce: 0.065719
2022-01-08 09:35:49,439 iteration 616 : loss : 0.115946, loss_ce: 0.054843
2022-01-08 09:35:50,769 iteration 617 : loss : 0.100193, loss_ce: 0.045621
2022-01-08 09:35:52,140 iteration 618 : loss : 0.130860, loss_ce: 0.039648
2022-01-08 09:35:53,505 iteration 619 : loss : 0.199616, loss_ce: 0.076600
2022-01-08 09:35:54,967 iteration 620 : loss : 0.158992, loss_ce: 0.053689
2022-01-08 09:35:56,352 iteration 621 : loss : 0.098251, loss_ce: 0.035589
2022-01-08 09:35:57,852 iteration 622 : loss : 0.127246, loss_ce: 0.053397
2022-01-08 09:35:59,314 iteration 623 : loss : 0.135551, loss_ce: 0.067560
2022-01-08 09:36:00,666 iteration 624 : loss : 0.099309, loss_ce: 0.034666
2022-01-08 09:36:01,988 iteration 625 : loss : 0.109802, loss_ce: 0.053625
2022-01-08 09:36:03,403 iteration 626 : loss : 0.093148, loss_ce: 0.040314
2022-01-08 09:36:04,815 iteration 627 : loss : 0.132781, loss_ce: 0.049308
2022-01-08 09:36:06,290 iteration 628 : loss : 0.087486, loss_ce: 0.034522
2022-01-08 09:36:07,649 iteration 629 : loss : 0.092140, loss_ce: 0.041155
  9%|██▊                           | 37/400 [15:38<2:32:07, 25.14s/it]2022-01-08 09:36:09,094 iteration 630 : loss : 0.092442, loss_ce: 0.043252
2022-01-08 09:36:10,441 iteration 631 : loss : 0.063689, loss_ce: 0.031614
2022-01-08 09:36:11,789 iteration 632 : loss : 0.153275, loss_ce: 0.063617
2022-01-08 09:36:13,243 iteration 633 : loss : 0.083035, loss_ce: 0.034887
2022-01-08 09:36:14,620 iteration 634 : loss : 0.154572, loss_ce: 0.055095
2022-01-08 09:36:16,082 iteration 635 : loss : 0.108271, loss_ce: 0.051525
2022-01-08 09:36:17,458 iteration 636 : loss : 0.130415, loss_ce: 0.057736
2022-01-08 09:36:18,754 iteration 637 : loss : 0.097819, loss_ce: 0.044424
2022-01-08 09:36:20,203 iteration 638 : loss : 0.173723, loss_ce: 0.067296
2022-01-08 09:36:21,523 iteration 639 : loss : 0.087983, loss_ce: 0.040550
2022-01-08 09:36:22,821 iteration 640 : loss : 0.134919, loss_ce: 0.042919
2022-01-08 09:36:24,141 iteration 641 : loss : 0.121298, loss_ce: 0.051139
2022-01-08 09:36:25,467 iteration 642 : loss : 0.129580, loss_ce: 0.031923
2022-01-08 09:36:26,826 iteration 643 : loss : 0.105105, loss_ce: 0.039849
2022-01-08 09:36:28,184 iteration 644 : loss : 0.094200, loss_ce: 0.041783
2022-01-08 09:36:29,515 iteration 645 : loss : 0.104481, loss_ce: 0.033852
2022-01-08 09:36:30,946 iteration 646 : loss : 0.091700, loss_ce: 0.051073
 10%|██▊                           | 38/400 [16:01<2:28:21, 24.59s/it]2022-01-08 09:36:32,320 iteration 647 : loss : 0.118120, loss_ce: 0.052532
2022-01-08 09:36:33,683 iteration 648 : loss : 0.097095, loss_ce: 0.044221
2022-01-08 09:36:35,070 iteration 649 : loss : 0.091664, loss_ce: 0.040051
2022-01-08 09:36:36,500 iteration 650 : loss : 0.146557, loss_ce: 0.053001
2022-01-08 09:36:37,861 iteration 651 : loss : 0.067144, loss_ce: 0.025480
2022-01-08 09:36:39,130 iteration 652 : loss : 0.062085, loss_ce: 0.022798
2022-01-08 09:36:40,581 iteration 653 : loss : 0.126320, loss_ce: 0.055651
2022-01-08 09:36:41,907 iteration 654 : loss : 0.087222, loss_ce: 0.032068
2022-01-08 09:36:43,279 iteration 655 : loss : 0.149361, loss_ce: 0.050485
2022-01-08 09:36:44,655 iteration 656 : loss : 0.100881, loss_ce: 0.040217
2022-01-08 09:36:46,093 iteration 657 : loss : 0.104031, loss_ce: 0.037452
2022-01-08 09:36:47,481 iteration 658 : loss : 0.109655, loss_ce: 0.039797
2022-01-08 09:36:48,864 iteration 659 : loss : 0.101989, loss_ce: 0.040375
2022-01-08 09:36:50,202 iteration 660 : loss : 0.086212, loss_ce: 0.038406
2022-01-08 09:36:51,593 iteration 661 : loss : 0.092220, loss_ce: 0.039317
2022-01-08 09:36:53,046 iteration 662 : loss : 0.155131, loss_ce: 0.040874
2022-01-08 09:36:54,408 iteration 663 : loss : 0.091883, loss_ce: 0.041866
 10%|██▉                           | 39/400 [16:25<2:25:54, 24.25s/it]2022-01-08 09:36:55,858 iteration 664 : loss : 0.097783, loss_ce: 0.046483
2022-01-08 09:36:57,162 iteration 665 : loss : 0.092169, loss_ce: 0.046487
2022-01-08 09:36:58,522 iteration 666 : loss : 0.078465, loss_ce: 0.028642
2022-01-08 09:36:59,923 iteration 667 : loss : 0.070125, loss_ce: 0.029163
2022-01-08 09:37:01,290 iteration 668 : loss : 0.125596, loss_ce: 0.054720
2022-01-08 09:37:02,645 iteration 669 : loss : 0.097317, loss_ce: 0.036776
2022-01-08 09:37:04,135 iteration 670 : loss : 0.096037, loss_ce: 0.038442
2022-01-08 09:37:05,486 iteration 671 : loss : 0.098031, loss_ce: 0.034409
2022-01-08 09:37:06,924 iteration 672 : loss : 0.075036, loss_ce: 0.027414
2022-01-08 09:37:08,333 iteration 673 : loss : 0.098591, loss_ce: 0.044704
2022-01-08 09:37:09,729 iteration 674 : loss : 0.075158, loss_ce: 0.038751
2022-01-08 09:37:11,137 iteration 675 : loss : 0.162161, loss_ce: 0.066280
2022-01-08 09:37:12,493 iteration 676 : loss : 0.101194, loss_ce: 0.039755
2022-01-08 09:37:13,869 iteration 677 : loss : 0.064034, loss_ce: 0.029498
2022-01-08 09:37:15,269 iteration 678 : loss : 0.134909, loss_ce: 0.047309
2022-01-08 09:37:16,652 iteration 679 : loss : 0.112639, loss_ce: 0.051849
2022-01-08 09:37:16,652 Training Data Eval:
2022-01-08 09:37:23,502   Average segmentation loss on training set: 0.0811
2022-01-08 09:37:23,502 Validation Data Eval:
2022-01-08 09:37:25,875   Average segmentation loss on validation set: 0.1066
2022-01-08 09:37:29,979 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:37:31,271 iteration 680 : loss : 0.081367, loss_ce: 0.028749
 10%|███                           | 40/400 [17:01<2:48:12, 28.04s/it]2022-01-08 09:37:32,618 iteration 681 : loss : 0.067173, loss_ce: 0.025186
2022-01-08 09:37:33,825 iteration 682 : loss : 0.123246, loss_ce: 0.034684
2022-01-08 09:37:35,103 iteration 683 : loss : 0.068067, loss_ce: 0.032144
2022-01-08 09:37:36,341 iteration 684 : loss : 0.070783, loss_ce: 0.025052
2022-01-08 09:37:37,685 iteration 685 : loss : 0.082118, loss_ce: 0.036387
2022-01-08 09:37:38,967 iteration 686 : loss : 0.130693, loss_ce: 0.048823
2022-01-08 09:37:40,247 iteration 687 : loss : 0.116948, loss_ce: 0.055561
2022-01-08 09:37:41,521 iteration 688 : loss : 0.091899, loss_ce: 0.031423
2022-01-08 09:37:42,880 iteration 689 : loss : 0.075085, loss_ce: 0.034576
2022-01-08 09:37:44,264 iteration 690 : loss : 0.086814, loss_ce: 0.033744
2022-01-08 09:37:45,662 iteration 691 : loss : 0.078362, loss_ce: 0.029004
2022-01-08 09:37:47,092 iteration 692 : loss : 0.079634, loss_ce: 0.026219
2022-01-08 09:37:48,406 iteration 693 : loss : 0.094614, loss_ce: 0.044810
2022-01-08 09:37:49,802 iteration 694 : loss : 0.090921, loss_ce: 0.037512
2022-01-08 09:37:51,136 iteration 695 : loss : 0.079352, loss_ce: 0.032747
2022-01-08 09:37:52,573 iteration 696 : loss : 0.079640, loss_ce: 0.034618
2022-01-08 09:37:53,946 iteration 697 : loss : 0.082648, loss_ce: 0.033501
 10%|███                           | 41/400 [17:24<2:38:08, 26.43s/it]2022-01-08 09:37:55,333 iteration 698 : loss : 0.102967, loss_ce: 0.034444
2022-01-08 09:37:56,747 iteration 699 : loss : 0.094658, loss_ce: 0.046128
2022-01-08 09:37:58,087 iteration 700 : loss : 0.081756, loss_ce: 0.038470
2022-01-08 09:37:59,430 iteration 701 : loss : 0.093185, loss_ce: 0.044179
2022-01-08 09:38:00,838 iteration 702 : loss : 0.107535, loss_ce: 0.036840
2022-01-08 09:38:02,287 iteration 703 : loss : 0.060753, loss_ce: 0.025973
2022-01-08 09:38:03,583 iteration 704 : loss : 0.063624, loss_ce: 0.024965
2022-01-08 09:38:04,948 iteration 705 : loss : 0.125718, loss_ce: 0.046997
2022-01-08 09:38:06,339 iteration 706 : loss : 0.085534, loss_ce: 0.035336
2022-01-08 09:38:07,652 iteration 707 : loss : 0.068887, loss_ce: 0.031588
2022-01-08 09:38:09,017 iteration 708 : loss : 0.061673, loss_ce: 0.022688
2022-01-08 09:38:10,372 iteration 709 : loss : 0.104597, loss_ce: 0.047090
2022-01-08 09:38:11,769 iteration 710 : loss : 0.095155, loss_ce: 0.039338
2022-01-08 09:38:13,166 iteration 711 : loss : 0.077424, loss_ce: 0.032668
2022-01-08 09:38:14,613 iteration 712 : loss : 0.060934, loss_ce: 0.023913
2022-01-08 09:38:15,971 iteration 713 : loss : 0.061591, loss_ce: 0.027138
2022-01-08 09:38:17,397 iteration 714 : loss : 0.093132, loss_ce: 0.035082
 10%|███▏                          | 42/400 [17:48<2:32:21, 25.54s/it]2022-01-08 09:38:18,848 iteration 715 : loss : 0.069201, loss_ce: 0.026073
2022-01-08 09:38:20,260 iteration 716 : loss : 0.058388, loss_ce: 0.026987
2022-01-08 09:38:21,620 iteration 717 : loss : 0.093093, loss_ce: 0.032296
2022-01-08 09:38:23,002 iteration 718 : loss : 0.095309, loss_ce: 0.050391
2022-01-08 09:38:24,427 iteration 719 : loss : 0.116706, loss_ce: 0.043560
2022-01-08 09:38:25,804 iteration 720 : loss : 0.088859, loss_ce: 0.033811
2022-01-08 09:38:27,171 iteration 721 : loss : 0.108300, loss_ce: 0.042224
2022-01-08 09:38:28,535 iteration 722 : loss : 0.092893, loss_ce: 0.033537
2022-01-08 09:38:29,946 iteration 723 : loss : 0.098678, loss_ce: 0.051052
2022-01-08 09:38:31,290 iteration 724 : loss : 0.133212, loss_ce: 0.064144
2022-01-08 09:38:32,737 iteration 725 : loss : 0.084333, loss_ce: 0.034726
2022-01-08 09:38:34,106 iteration 726 : loss : 0.091315, loss_ce: 0.042567
2022-01-08 09:38:35,406 iteration 727 : loss : 0.071577, loss_ce: 0.029372
2022-01-08 09:38:36,815 iteration 728 : loss : 0.084421, loss_ce: 0.040480
2022-01-08 09:38:38,230 iteration 729 : loss : 0.066089, loss_ce: 0.030142
2022-01-08 09:38:39,610 iteration 730 : loss : 0.075249, loss_ce: 0.031724
2022-01-08 09:38:40,893 iteration 731 : loss : 0.054098, loss_ce: 0.020990
 11%|███▏                          | 43/400 [18:11<2:28:17, 24.92s/it]2022-01-08 09:38:42,257 iteration 732 : loss : 0.087422, loss_ce: 0.040322
2022-01-08 09:38:43,556 iteration 733 : loss : 0.073431, loss_ce: 0.035160
2022-01-08 09:38:44,971 iteration 734 : loss : 0.071464, loss_ce: 0.028221
2022-01-08 09:38:46,320 iteration 735 : loss : 0.092877, loss_ce: 0.035769
2022-01-08 09:38:47,657 iteration 736 : loss : 0.089324, loss_ce: 0.039382
2022-01-08 09:38:49,145 iteration 737 : loss : 0.085507, loss_ce: 0.037495
2022-01-08 09:38:50,423 iteration 738 : loss : 0.088065, loss_ce: 0.043495
2022-01-08 09:38:51,820 iteration 739 : loss : 0.099852, loss_ce: 0.033735
2022-01-08 09:38:53,170 iteration 740 : loss : 0.064967, loss_ce: 0.025453
2022-01-08 09:38:54,507 iteration 741 : loss : 0.065960, loss_ce: 0.025421
2022-01-08 09:38:55,810 iteration 742 : loss : 0.086876, loss_ce: 0.041948
2022-01-08 09:38:57,187 iteration 743 : loss : 0.094073, loss_ce: 0.037158
2022-01-08 09:38:58,589 iteration 744 : loss : 0.073280, loss_ce: 0.028938
2022-01-08 09:38:59,918 iteration 745 : loss : 0.108829, loss_ce: 0.022677
2022-01-08 09:39:01,260 iteration 746 : loss : 0.074451, loss_ce: 0.026969
2022-01-08 09:39:02,594 iteration 747 : loss : 0.092651, loss_ce: 0.042262
2022-01-08 09:39:03,918 iteration 748 : loss : 0.089646, loss_ce: 0.038312
 11%|███▎                          | 44/400 [18:34<2:24:30, 24.35s/it]2022-01-08 09:39:05,265 iteration 749 : loss : 0.206932, loss_ce: 0.042599
2022-01-08 09:39:06,611 iteration 750 : loss : 0.058617, loss_ce: 0.023695
2022-01-08 09:39:07,996 iteration 751 : loss : 0.072287, loss_ce: 0.023849
2022-01-08 09:39:09,337 iteration 752 : loss : 0.135672, loss_ce: 0.068923
2022-01-08 09:39:10,649 iteration 753 : loss : 0.112606, loss_ce: 0.051333
2022-01-08 09:39:11,979 iteration 754 : loss : 0.127729, loss_ce: 0.045616
2022-01-08 09:39:13,263 iteration 755 : loss : 0.114446, loss_ce: 0.061315
2022-01-08 09:39:14,558 iteration 756 : loss : 0.058161, loss_ce: 0.022174
2022-01-08 09:39:15,915 iteration 757 : loss : 0.137410, loss_ce: 0.069195
2022-01-08 09:39:17,367 iteration 758 : loss : 0.107313, loss_ce: 0.046615
2022-01-08 09:39:18,856 iteration 759 : loss : 0.128148, loss_ce: 0.048792
2022-01-08 09:39:20,189 iteration 760 : loss : 0.104525, loss_ce: 0.063841
2022-01-08 09:39:21,583 iteration 761 : loss : 0.076145, loss_ce: 0.036048
2022-01-08 09:39:22,995 iteration 762 : loss : 0.106851, loss_ce: 0.045091
2022-01-08 09:39:24,410 iteration 763 : loss : 0.087985, loss_ce: 0.036605
2022-01-08 09:39:25,755 iteration 764 : loss : 0.122112, loss_ce: 0.058426
2022-01-08 09:39:25,755 Training Data Eval:
2022-01-08 09:39:32,649   Average segmentation loss on training set: 0.0913
2022-01-08 09:39:32,649 Validation Data Eval:
2022-01-08 09:39:35,012   Average segmentation loss on validation set: 0.1091
2022-01-08 09:39:36,535 iteration 765 : loss : 0.110168, loss_ce: 0.045711
 11%|███▍                          | 45/400 [19:07<2:38:45, 26.83s/it]2022-01-08 09:39:37,986 iteration 766 : loss : 0.146024, loss_ce: 0.077152
2022-01-08 09:39:39,372 iteration 767 : loss : 0.090376, loss_ce: 0.036206
2022-01-08 09:39:40,754 iteration 768 : loss : 0.111241, loss_ce: 0.041342
2022-01-08 09:39:42,162 iteration 769 : loss : 0.093339, loss_ce: 0.043306
2022-01-08 09:39:43,456 iteration 770 : loss : 0.067984, loss_ce: 0.026311
2022-01-08 09:39:44,838 iteration 771 : loss : 0.131736, loss_ce: 0.030515
2022-01-08 09:39:46,220 iteration 772 : loss : 0.070642, loss_ce: 0.027327
2022-01-08 09:39:47,611 iteration 773 : loss : 0.120435, loss_ce: 0.030676
2022-01-08 09:39:49,080 iteration 774 : loss : 0.091725, loss_ce: 0.037900
2022-01-08 09:39:50,431 iteration 775 : loss : 0.110075, loss_ce: 0.059476
2022-01-08 09:39:51,759 iteration 776 : loss : 0.086182, loss_ce: 0.036162
2022-01-08 09:39:53,307 iteration 777 : loss : 0.141973, loss_ce: 0.067881
2022-01-08 09:39:54,593 iteration 778 : loss : 0.109105, loss_ce: 0.035016
2022-01-08 09:39:56,015 iteration 779 : loss : 0.104335, loss_ce: 0.045439
2022-01-08 09:39:57,435 iteration 780 : loss : 0.099009, loss_ce: 0.037553
2022-01-08 09:39:58,761 iteration 781 : loss : 0.077202, loss_ce: 0.042428
2022-01-08 09:40:00,054 iteration 782 : loss : 0.069639, loss_ce: 0.036201
 12%|███▍                          | 46/400 [19:30<2:32:26, 25.84s/it]2022-01-08 09:40:01,459 iteration 783 : loss : 0.079780, loss_ce: 0.037583
2022-01-08 09:40:02,791 iteration 784 : loss : 0.091564, loss_ce: 0.035828
2022-01-08 09:40:04,165 iteration 785 : loss : 0.105459, loss_ce: 0.049444
2022-01-08 09:40:05,503 iteration 786 : loss : 0.114724, loss_ce: 0.046978
2022-01-08 09:40:06,848 iteration 787 : loss : 0.096914, loss_ce: 0.037104
2022-01-08 09:40:08,256 iteration 788 : loss : 0.095529, loss_ce: 0.041462
2022-01-08 09:40:09,540 iteration 789 : loss : 0.089457, loss_ce: 0.032647
2022-01-08 09:40:10,970 iteration 790 : loss : 0.083956, loss_ce: 0.030561
2022-01-08 09:40:12,333 iteration 791 : loss : 0.075568, loss_ce: 0.028326
2022-01-08 09:40:13,636 iteration 792 : loss : 0.074014, loss_ce: 0.031090
2022-01-08 09:40:15,053 iteration 793 : loss : 0.089647, loss_ce: 0.042005
2022-01-08 09:40:16,328 iteration 794 : loss : 0.066151, loss_ce: 0.026496
2022-01-08 09:40:17,641 iteration 795 : loss : 0.077490, loss_ce: 0.030100
2022-01-08 09:40:18,947 iteration 796 : loss : 0.069600, loss_ce: 0.034128
2022-01-08 09:40:20,348 iteration 797 : loss : 0.143974, loss_ce: 0.065918
2022-01-08 09:40:21,695 iteration 798 : loss : 0.077722, loss_ce: 0.028034
2022-01-08 09:40:22,999 iteration 799 : loss : 0.063197, loss_ce: 0.034750
 12%|███▌                          | 47/400 [19:53<2:26:54, 24.97s/it]2022-01-08 09:40:24,507 iteration 800 : loss : 0.072235, loss_ce: 0.024920
2022-01-08 09:40:25,883 iteration 801 : loss : 0.075528, loss_ce: 0.031855
2022-01-08 09:40:27,245 iteration 802 : loss : 0.101072, loss_ce: 0.038169
2022-01-08 09:40:28,622 iteration 803 : loss : 0.076511, loss_ce: 0.040733
2022-01-08 09:40:30,074 iteration 804 : loss : 0.125820, loss_ce: 0.055874
2022-01-08 09:40:31,576 iteration 805 : loss : 0.066910, loss_ce: 0.022717
2022-01-08 09:40:32,907 iteration 806 : loss : 0.070517, loss_ce: 0.029215
2022-01-08 09:40:34,217 iteration 807 : loss : 0.063353, loss_ce: 0.023504
2022-01-08 09:40:35,626 iteration 808 : loss : 0.077577, loss_ce: 0.036755
2022-01-08 09:40:36,998 iteration 809 : loss : 0.083383, loss_ce: 0.028819
2022-01-08 09:40:38,327 iteration 810 : loss : 0.129102, loss_ce: 0.047886
2022-01-08 09:40:39,633 iteration 811 : loss : 0.104335, loss_ce: 0.057258
2022-01-08 09:40:40,980 iteration 812 : loss : 0.058578, loss_ce: 0.024462
2022-01-08 09:40:42,347 iteration 813 : loss : 0.117824, loss_ce: 0.036008
2022-01-08 09:40:43,820 iteration 814 : loss : 0.101660, loss_ce: 0.038283
2022-01-08 09:40:45,242 iteration 815 : loss : 0.107267, loss_ce: 0.042893
2022-01-08 09:40:46,625 iteration 816 : loss : 0.070481, loss_ce: 0.027606
 12%|███▌                          | 48/400 [20:17<2:24:07, 24.57s/it]2022-01-08 09:40:48,017 iteration 817 : loss : 0.096830, loss_ce: 0.046146
2022-01-08 09:40:49,327 iteration 818 : loss : 0.075927, loss_ce: 0.030042
2022-01-08 09:40:50,690 iteration 819 : loss : 0.065451, loss_ce: 0.023521
2022-01-08 09:40:51,982 iteration 820 : loss : 0.073725, loss_ce: 0.033203
2022-01-08 09:40:53,403 iteration 821 : loss : 0.067283, loss_ce: 0.022454
2022-01-08 09:40:54,800 iteration 822 : loss : 0.100550, loss_ce: 0.035452
2022-01-08 09:40:56,110 iteration 823 : loss : 0.068805, loss_ce: 0.025800
2022-01-08 09:40:57,424 iteration 824 : loss : 0.077856, loss_ce: 0.036175
2022-01-08 09:40:58,801 iteration 825 : loss : 0.066408, loss_ce: 0.025722
2022-01-08 09:41:00,146 iteration 826 : loss : 0.106426, loss_ce: 0.031134
2022-01-08 09:41:01,498 iteration 827 : loss : 0.117518, loss_ce: 0.035369
2022-01-08 09:41:02,856 iteration 828 : loss : 0.085451, loss_ce: 0.036535
2022-01-08 09:41:04,238 iteration 829 : loss : 0.097648, loss_ce: 0.037962
2022-01-08 09:41:05,659 iteration 830 : loss : 0.061890, loss_ce: 0.024192
2022-01-08 09:41:07,019 iteration 831 : loss : 0.118029, loss_ce: 0.058584
2022-01-08 09:41:08,350 iteration 832 : loss : 0.046361, loss_ce: 0.017417
2022-01-08 09:41:09,765 iteration 833 : loss : 0.086124, loss_ce: 0.043423
 12%|███▋                          | 49/400 [20:40<2:21:12, 24.14s/it]2022-01-08 09:41:11,154 iteration 834 : loss : 0.073041, loss_ce: 0.031880
2022-01-08 09:41:12,432 iteration 835 : loss : 0.086037, loss_ce: 0.023107
2022-01-08 09:41:13,804 iteration 836 : loss : 0.079520, loss_ce: 0.038861
2022-01-08 09:41:15,111 iteration 837 : loss : 0.053690, loss_ce: 0.031469
2022-01-08 09:41:16,397 iteration 838 : loss : 0.072698, loss_ce: 0.033581
2022-01-08 09:41:17,773 iteration 839 : loss : 0.070875, loss_ce: 0.031768
2022-01-08 09:41:19,116 iteration 840 : loss : 0.097313, loss_ce: 0.038542
2022-01-08 09:41:20,432 iteration 841 : loss : 0.069074, loss_ce: 0.030615
2022-01-08 09:41:21,790 iteration 842 : loss : 0.062172, loss_ce: 0.027660
2022-01-08 09:41:23,251 iteration 843 : loss : 0.075223, loss_ce: 0.028943
2022-01-08 09:41:24,599 iteration 844 : loss : 0.088426, loss_ce: 0.027821
2022-01-08 09:41:25,981 iteration 845 : loss : 0.081049, loss_ce: 0.030925
2022-01-08 09:41:27,454 iteration 846 : loss : 0.071555, loss_ce: 0.035473
2022-01-08 09:41:28,823 iteration 847 : loss : 0.057733, loss_ce: 0.024150
2022-01-08 09:41:30,093 iteration 848 : loss : 0.058551, loss_ce: 0.026136
2022-01-08 09:41:31,460 iteration 849 : loss : 0.096411, loss_ce: 0.036239
2022-01-08 09:41:31,460 Training Data Eval:
2022-01-08 09:41:38,342   Average segmentation loss on training set: 0.0541
2022-01-08 09:41:38,342 Validation Data Eval:
2022-01-08 09:41:40,704   Average segmentation loss on validation set: 0.0857
2022-01-08 09:41:45,015 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:41:46,361 iteration 850 : loss : 0.074419, loss_ce: 0.026791
 12%|███▊                          | 50/400 [21:17<2:42:36, 27.88s/it]2022-01-08 09:41:47,714 iteration 851 : loss : 0.053964, loss_ce: 0.021895
2022-01-08 09:41:48,965 iteration 852 : loss : 0.053727, loss_ce: 0.023830
2022-01-08 09:41:50,313 iteration 853 : loss : 0.079161, loss_ce: 0.034942
2022-01-08 09:41:51,618 iteration 854 : loss : 0.048967, loss_ce: 0.015532
2022-01-08 09:41:52,914 iteration 855 : loss : 0.078171, loss_ce: 0.032404
2022-01-08 09:41:54,228 iteration 856 : loss : 0.066900, loss_ce: 0.023033
2022-01-08 09:41:55,470 iteration 857 : loss : 0.081943, loss_ce: 0.032886
2022-01-08 09:41:56,650 iteration 858 : loss : 0.064966, loss_ce: 0.032516
2022-01-08 09:41:57,923 iteration 859 : loss : 0.064522, loss_ce: 0.026693
2022-01-08 09:41:59,201 iteration 860 : loss : 0.064285, loss_ce: 0.020966
2022-01-08 09:42:00,639 iteration 861 : loss : 0.099837, loss_ce: 0.048602
2022-01-08 09:42:01,945 iteration 862 : loss : 0.067677, loss_ce: 0.022239
2022-01-08 09:42:03,390 iteration 863 : loss : 0.103932, loss_ce: 0.055543
2022-01-08 09:42:04,806 iteration 864 : loss : 0.060322, loss_ce: 0.027780
2022-01-08 09:42:06,103 iteration 865 : loss : 0.054684, loss_ce: 0.022811
2022-01-08 09:42:07,529 iteration 866 : loss : 0.094104, loss_ce: 0.049176
2022-01-08 09:42:08,841 iteration 867 : loss : 0.044881, loss_ce: 0.017627
 13%|███▊                          | 51/400 [21:39<2:32:43, 26.26s/it]2022-01-08 09:42:10,287 iteration 868 : loss : 0.105062, loss_ce: 0.033465
2022-01-08 09:42:11,658 iteration 869 : loss : 0.079580, loss_ce: 0.035379
2022-01-08 09:42:12,939 iteration 870 : loss : 0.052596, loss_ce: 0.019946
2022-01-08 09:42:14,261 iteration 871 : loss : 0.059610, loss_ce: 0.022035
2022-01-08 09:42:15,601 iteration 872 : loss : 0.056813, loss_ce: 0.029067
2022-01-08 09:42:17,037 iteration 873 : loss : 0.070874, loss_ce: 0.029414
2022-01-08 09:42:18,328 iteration 874 : loss : 0.045864, loss_ce: 0.020319
2022-01-08 09:42:19,840 iteration 875 : loss : 0.098220, loss_ce: 0.037232
2022-01-08 09:42:21,200 iteration 876 : loss : 0.054521, loss_ce: 0.024688
2022-01-08 09:42:22,461 iteration 877 : loss : 0.050579, loss_ce: 0.020068
2022-01-08 09:42:23,864 iteration 878 : loss : 0.081244, loss_ce: 0.034282
2022-01-08 09:42:25,207 iteration 879 : loss : 0.064951, loss_ce: 0.026974
2022-01-08 09:42:26,594 iteration 880 : loss : 0.059998, loss_ce: 0.021948
2022-01-08 09:42:27,929 iteration 881 : loss : 0.118990, loss_ce: 0.061936
2022-01-08 09:42:29,339 iteration 882 : loss : 0.088821, loss_ce: 0.035844
2022-01-08 09:42:30,690 iteration 883 : loss : 0.082520, loss_ce: 0.037522
2022-01-08 09:42:32,135 iteration 884 : loss : 0.080152, loss_ce: 0.034496
 13%|███▉                          | 52/400 [22:02<2:27:08, 25.37s/it]2022-01-08 09:42:33,572 iteration 885 : loss : 0.078166, loss_ce: 0.034007
2022-01-08 09:42:34,995 iteration 886 : loss : 0.061188, loss_ce: 0.025373
2022-01-08 09:42:36,410 iteration 887 : loss : 0.103279, loss_ce: 0.044854
2022-01-08 09:42:37,782 iteration 888 : loss : 0.068695, loss_ce: 0.024528
2022-01-08 09:42:39,211 iteration 889 : loss : 0.059038, loss_ce: 0.021499
2022-01-08 09:42:40,682 iteration 890 : loss : 0.083798, loss_ce: 0.030629
2022-01-08 09:42:42,051 iteration 891 : loss : 0.050443, loss_ce: 0.018099
2022-01-08 09:42:43,418 iteration 892 : loss : 0.064306, loss_ce: 0.035204
2022-01-08 09:42:44,763 iteration 893 : loss : 0.066046, loss_ce: 0.024598
2022-01-08 09:42:46,225 iteration 894 : loss : 0.074667, loss_ce: 0.026014
2022-01-08 09:42:47,568 iteration 895 : loss : 0.092182, loss_ce: 0.030342
2022-01-08 09:42:49,005 iteration 896 : loss : 0.088565, loss_ce: 0.034300
2022-01-08 09:42:50,454 iteration 897 : loss : 0.091665, loss_ce: 0.049776
2022-01-08 09:42:51,798 iteration 898 : loss : 0.054935, loss_ce: 0.024739
2022-01-08 09:42:53,152 iteration 899 : loss : 0.067276, loss_ce: 0.028756
2022-01-08 09:42:54,469 iteration 900 : loss : 0.054991, loss_ce: 0.024984
2022-01-08 09:42:55,814 iteration 901 : loss : 0.082924, loss_ce: 0.028932
 13%|███▉                          | 53/400 [22:26<2:23:46, 24.86s/it]2022-01-08 09:42:57,255 iteration 902 : loss : 0.061929, loss_ce: 0.024360
2022-01-08 09:42:58,710 iteration 903 : loss : 0.081220, loss_ce: 0.030975
2022-01-08 09:43:00,091 iteration 904 : loss : 0.089829, loss_ce: 0.029090
2022-01-08 09:43:01,469 iteration 905 : loss : 0.088874, loss_ce: 0.036306
2022-01-08 09:43:02,831 iteration 906 : loss : 0.053700, loss_ce: 0.025321
2022-01-08 09:43:04,260 iteration 907 : loss : 0.055452, loss_ce: 0.018456
2022-01-08 09:43:05,538 iteration 908 : loss : 0.057360, loss_ce: 0.021706
2022-01-08 09:43:06,923 iteration 909 : loss : 0.085762, loss_ce: 0.040961
2022-01-08 09:43:08,224 iteration 910 : loss : 0.080036, loss_ce: 0.030005
2022-01-08 09:43:09,592 iteration 911 : loss : 0.060289, loss_ce: 0.023616
2022-01-08 09:43:10,955 iteration 912 : loss : 0.070881, loss_ce: 0.026445
2022-01-08 09:43:12,298 iteration 913 : loss : 0.089878, loss_ce: 0.031739
2022-01-08 09:43:13,655 iteration 914 : loss : 0.086751, loss_ce: 0.038271
2022-01-08 09:43:14,988 iteration 915 : loss : 0.135620, loss_ce: 0.040625
2022-01-08 09:43:16,322 iteration 916 : loss : 0.062321, loss_ce: 0.020930
2022-01-08 09:43:17,706 iteration 917 : loss : 0.089892, loss_ce: 0.043001
2022-01-08 09:43:19,108 iteration 918 : loss : 0.080193, loss_ce: 0.031168
 14%|████                          | 54/400 [22:49<2:20:38, 24.39s/it]2022-01-08 09:43:20,551 iteration 919 : loss : 0.064423, loss_ce: 0.028101
2022-01-08 09:43:21,943 iteration 920 : loss : 0.059975, loss_ce: 0.026343
2022-01-08 09:43:23,364 iteration 921 : loss : 0.072918, loss_ce: 0.027912
2022-01-08 09:43:24,793 iteration 922 : loss : 0.107966, loss_ce: 0.052622
2022-01-08 09:43:26,128 iteration 923 : loss : 0.063951, loss_ce: 0.030342
2022-01-08 09:43:27,542 iteration 924 : loss : 0.126049, loss_ce: 0.047161
2022-01-08 09:43:28,877 iteration 925 : loss : 0.071388, loss_ce: 0.031362
2022-01-08 09:43:30,305 iteration 926 : loss : 0.059313, loss_ce: 0.028329
2022-01-08 09:43:31,610 iteration 927 : loss : 0.091222, loss_ce: 0.035627
2022-01-08 09:43:32,980 iteration 928 : loss : 0.061090, loss_ce: 0.029666
2022-01-08 09:43:34,396 iteration 929 : loss : 0.085982, loss_ce: 0.033688
2022-01-08 09:43:35,767 iteration 930 : loss : 0.073737, loss_ce: 0.030886
2022-01-08 09:43:37,149 iteration 931 : loss : 0.097630, loss_ce: 0.022623
2022-01-08 09:43:38,527 iteration 932 : loss : 0.074434, loss_ce: 0.026426
2022-01-08 09:43:39,865 iteration 933 : loss : 0.085662, loss_ce: 0.033082
2022-01-08 09:43:41,232 iteration 934 : loss : 0.071916, loss_ce: 0.036461
2022-01-08 09:43:41,232 Training Data Eval:
2022-01-08 09:43:48,092   Average segmentation loss on training set: 0.1722
2022-01-08 09:43:48,093 Validation Data Eval:
2022-01-08 09:43:50,455   Average segmentation loss on validation set: 0.3080
2022-01-08 09:43:51,926 iteration 935 : loss : 0.109353, loss_ce: 0.030576
 14%|████▏                         | 55/400 [23:22<2:34:46, 26.92s/it]2022-01-08 09:43:53,319 iteration 936 : loss : 0.064911, loss_ce: 0.023153
2022-01-08 09:43:54,708 iteration 937 : loss : 0.073364, loss_ce: 0.025492
2022-01-08 09:43:56,115 iteration 938 : loss : 0.065231, loss_ce: 0.024236
2022-01-08 09:43:57,434 iteration 939 : loss : 0.066471, loss_ce: 0.027635
2022-01-08 09:43:58,821 iteration 940 : loss : 0.076409, loss_ce: 0.031351
2022-01-08 09:44:00,288 iteration 941 : loss : 0.074326, loss_ce: 0.031340
2022-01-08 09:44:01,613 iteration 942 : loss : 0.058674, loss_ce: 0.023375
2022-01-08 09:44:02,967 iteration 943 : loss : 0.061012, loss_ce: 0.029491
2022-01-08 09:44:04,301 iteration 944 : loss : 0.055211, loss_ce: 0.021593
2022-01-08 09:44:05,652 iteration 945 : loss : 0.076884, loss_ce: 0.029269
2022-01-08 09:44:06,996 iteration 946 : loss : 0.045146, loss_ce: 0.021524
2022-01-08 09:44:08,316 iteration 947 : loss : 0.061643, loss_ce: 0.021486
2022-01-08 09:44:09,640 iteration 948 : loss : 0.079799, loss_ce: 0.029318
2022-01-08 09:44:10,988 iteration 949 : loss : 0.058168, loss_ce: 0.022115
2022-01-08 09:44:12,351 iteration 950 : loss : 0.086617, loss_ce: 0.037573
2022-01-08 09:44:13,765 iteration 951 : loss : 0.071789, loss_ce: 0.041507
2022-01-08 09:44:15,079 iteration 952 : loss : 0.068589, loss_ce: 0.027727
 14%|████▏                         | 56/400 [23:45<2:27:51, 25.79s/it]2022-01-08 09:44:16,502 iteration 953 : loss : 0.080538, loss_ce: 0.030255
2022-01-08 09:44:17,911 iteration 954 : loss : 0.063986, loss_ce: 0.025073
2022-01-08 09:44:19,203 iteration 955 : loss : 0.048084, loss_ce: 0.019829
2022-01-08 09:44:20,603 iteration 956 : loss : 0.068429, loss_ce: 0.030452
2022-01-08 09:44:21,984 iteration 957 : loss : 0.050756, loss_ce: 0.018836
2022-01-08 09:44:23,377 iteration 958 : loss : 0.075524, loss_ce: 0.025356
2022-01-08 09:44:24,829 iteration 959 : loss : 0.081822, loss_ce: 0.038579
2022-01-08 09:44:26,327 iteration 960 : loss : 0.088729, loss_ce: 0.041804
2022-01-08 09:44:27,649 iteration 961 : loss : 0.080851, loss_ce: 0.025318
2022-01-08 09:44:28,956 iteration 962 : loss : 0.076879, loss_ce: 0.020949
2022-01-08 09:44:30,404 iteration 963 : loss : 0.064094, loss_ce: 0.025020
2022-01-08 09:44:31,806 iteration 964 : loss : 0.060876, loss_ce: 0.029493
2022-01-08 09:44:33,155 iteration 965 : loss : 0.050985, loss_ce: 0.024343
2022-01-08 09:44:34,521 iteration 966 : loss : 0.051243, loss_ce: 0.021819
2022-01-08 09:44:35,906 iteration 967 : loss : 0.055290, loss_ce: 0.025088
2022-01-08 09:44:37,225 iteration 968 : loss : 0.099387, loss_ce: 0.033268
2022-01-08 09:44:38,618 iteration 969 : loss : 0.062604, loss_ce: 0.026523
 14%|████▎                         | 57/400 [24:09<2:23:34, 25.11s/it]2022-01-08 09:44:40,007 iteration 970 : loss : 0.055746, loss_ce: 0.027094
2022-01-08 09:44:41,402 iteration 971 : loss : 0.078089, loss_ce: 0.024920
2022-01-08 09:44:42,713 iteration 972 : loss : 0.056696, loss_ce: 0.026737
2022-01-08 09:44:44,112 iteration 973 : loss : 0.069614, loss_ce: 0.023538
2022-01-08 09:44:45,425 iteration 974 : loss : 0.070942, loss_ce: 0.029635
2022-01-08 09:44:46,838 iteration 975 : loss : 0.079151, loss_ce: 0.029909
2022-01-08 09:44:48,179 iteration 976 : loss : 0.053064, loss_ce: 0.022181
2022-01-08 09:44:49,523 iteration 977 : loss : 0.055840, loss_ce: 0.018833
2022-01-08 09:44:50,910 iteration 978 : loss : 0.049310, loss_ce: 0.018593
2022-01-08 09:44:52,267 iteration 979 : loss : 0.118027, loss_ce: 0.069524
2022-01-08 09:44:53,612 iteration 980 : loss : 0.055509, loss_ce: 0.020254
2022-01-08 09:44:55,075 iteration 981 : loss : 0.061075, loss_ce: 0.031060
2022-01-08 09:44:56,412 iteration 982 : loss : 0.057803, loss_ce: 0.029424
2022-01-08 09:44:57,834 iteration 983 : loss : 0.118916, loss_ce: 0.043040
2022-01-08 09:44:59,127 iteration 984 : loss : 0.058785, loss_ce: 0.028193
2022-01-08 09:45:00,492 iteration 985 : loss : 0.055561, loss_ce: 0.018462
2022-01-08 09:45:01,930 iteration 986 : loss : 0.082758, loss_ce: 0.034606
 14%|████▎                         | 58/400 [24:32<2:20:05, 24.58s/it]2022-01-08 09:45:03,409 iteration 987 : loss : 0.096003, loss_ce: 0.053445
2022-01-08 09:45:04,795 iteration 988 : loss : 0.066426, loss_ce: 0.032215
2022-01-08 09:45:06,169 iteration 989 : loss : 0.082070, loss_ce: 0.036034
2022-01-08 09:45:07,523 iteration 990 : loss : 0.080592, loss_ce: 0.036773
2022-01-08 09:45:08,880 iteration 991 : loss : 0.054932, loss_ce: 0.024582
2022-01-08 09:45:10,330 iteration 992 : loss : 0.062992, loss_ce: 0.031075
2022-01-08 09:45:11,721 iteration 993 : loss : 0.064734, loss_ce: 0.024635
2022-01-08 09:45:13,191 iteration 994 : loss : 0.083047, loss_ce: 0.037221
2022-01-08 09:45:14,563 iteration 995 : loss : 0.101437, loss_ce: 0.036781
2022-01-08 09:45:15,990 iteration 996 : loss : 0.052358, loss_ce: 0.023914
2022-01-08 09:45:17,416 iteration 997 : loss : 0.068833, loss_ce: 0.025802
2022-01-08 09:45:18,738 iteration 998 : loss : 0.082019, loss_ce: 0.024481
2022-01-08 09:45:20,163 iteration 999 : loss : 0.064335, loss_ce: 0.027865
2022-01-08 09:45:21,499 iteration 1000 : loss : 0.065071, loss_ce: 0.020429
2022-01-08 09:45:22,843 iteration 1001 : loss : 0.098340, loss_ce: 0.023791
2022-01-08 09:45:24,304 iteration 1002 : loss : 0.062008, loss_ce: 0.024621
2022-01-08 09:45:25,731 iteration 1003 : loss : 0.094894, loss_ce: 0.034856
 15%|████▍                         | 59/400 [24:56<2:18:20, 24.34s/it]2022-01-08 09:45:27,130 iteration 1004 : loss : 0.121210, loss_ce: 0.044106
2022-01-08 09:45:28,477 iteration 1005 : loss : 0.070613, loss_ce: 0.026119
2022-01-08 09:45:29,934 iteration 1006 : loss : 0.084162, loss_ce: 0.035400
2022-01-08 09:45:31,260 iteration 1007 : loss : 0.064728, loss_ce: 0.031977
2022-01-08 09:45:32,578 iteration 1008 : loss : 0.144484, loss_ce: 0.057238
2022-01-08 09:45:33,920 iteration 1009 : loss : 0.088929, loss_ce: 0.034555
2022-01-08 09:45:35,258 iteration 1010 : loss : 0.080932, loss_ce: 0.030415
2022-01-08 09:45:36,681 iteration 1011 : loss : 0.079048, loss_ce: 0.036796
2022-01-08 09:45:38,027 iteration 1012 : loss : 0.046478, loss_ce: 0.018812
2022-01-08 09:45:39,339 iteration 1013 : loss : 0.055674, loss_ce: 0.018888
2022-01-08 09:45:40,718 iteration 1014 : loss : 0.097640, loss_ce: 0.046315
2022-01-08 09:45:42,018 iteration 1015 : loss : 0.079967, loss_ce: 0.034526
2022-01-08 09:45:43,395 iteration 1016 : loss : 0.062842, loss_ce: 0.027661
2022-01-08 09:45:44,672 iteration 1017 : loss : 0.047147, loss_ce: 0.018229
2022-01-08 09:45:45,956 iteration 1018 : loss : 0.083120, loss_ce: 0.032107
2022-01-08 09:45:47,321 iteration 1019 : loss : 0.081869, loss_ce: 0.033335
2022-01-08 09:45:47,321 Training Data Eval:
2022-01-08 09:45:54,221   Average segmentation loss on training set: 0.0728
2022-01-08 09:45:54,221 Validation Data Eval:
2022-01-08 09:45:56,586   Average segmentation loss on validation set: 0.1468
2022-01-08 09:45:57,912 iteration 1020 : loss : 0.055726, loss_ce: 0.021621
 15%|████▌                         | 60/400 [25:28<2:31:16, 26.69s/it]2022-01-08 09:45:59,398 iteration 1021 : loss : 0.094603, loss_ce: 0.023769
2022-01-08 09:46:00,747 iteration 1022 : loss : 0.061425, loss_ce: 0.026748
2022-01-08 09:46:02,123 iteration 1023 : loss : 0.104839, loss_ce: 0.027454
2022-01-08 09:46:03,550 iteration 1024 : loss : 0.049111, loss_ce: 0.018047
2022-01-08 09:46:04,957 iteration 1025 : loss : 0.049287, loss_ce: 0.022488
2022-01-08 09:46:06,223 iteration 1026 : loss : 0.054030, loss_ce: 0.019956
2022-01-08 09:46:07,633 iteration 1027 : loss : 0.078730, loss_ce: 0.028421
2022-01-08 09:46:09,038 iteration 1028 : loss : 0.080926, loss_ce: 0.037822
2022-01-08 09:46:10,395 iteration 1029 : loss : 0.047825, loss_ce: 0.019758
2022-01-08 09:46:11,761 iteration 1030 : loss : 0.062937, loss_ce: 0.027716
2022-01-08 09:46:13,192 iteration 1031 : loss : 0.099270, loss_ce: 0.044348
2022-01-08 09:46:14,582 iteration 1032 : loss : 0.081776, loss_ce: 0.038826
2022-01-08 09:46:15,905 iteration 1033 : loss : 0.053287, loss_ce: 0.020895
2022-01-08 09:46:17,333 iteration 1034 : loss : 0.085690, loss_ce: 0.032927
2022-01-08 09:46:18,714 iteration 1035 : loss : 0.061517, loss_ce: 0.026645
2022-01-08 09:46:20,094 iteration 1036 : loss : 0.058404, loss_ce: 0.025837
2022-01-08 09:46:21,431 iteration 1037 : loss : 0.070956, loss_ce: 0.025790
 15%|████▌                         | 61/400 [25:52<2:25:25, 25.74s/it]2022-01-08 09:46:22,855 iteration 1038 : loss : 0.057624, loss_ce: 0.023053
2022-01-08 09:46:24,216 iteration 1039 : loss : 0.059762, loss_ce: 0.028792
2022-01-08 09:46:25,605 iteration 1040 : loss : 0.088146, loss_ce: 0.044433
2022-01-08 09:46:26,986 iteration 1041 : loss : 0.068517, loss_ce: 0.030439
2022-01-08 09:46:28,380 iteration 1042 : loss : 0.069504, loss_ce: 0.028417
2022-01-08 09:46:29,771 iteration 1043 : loss : 0.080001, loss_ce: 0.032845
2022-01-08 09:46:31,179 iteration 1044 : loss : 0.060374, loss_ce: 0.025744
2022-01-08 09:46:32,467 iteration 1045 : loss : 0.055986, loss_ce: 0.028139
2022-01-08 09:46:33,794 iteration 1046 : loss : 0.080694, loss_ce: 0.025757
2022-01-08 09:46:35,202 iteration 1047 : loss : 0.088982, loss_ce: 0.034162
2022-01-08 09:46:36,577 iteration 1048 : loss : 0.075373, loss_ce: 0.023937
2022-01-08 09:46:37,971 iteration 1049 : loss : 0.081841, loss_ce: 0.029382
2022-01-08 09:46:39,409 iteration 1050 : loss : 0.094142, loss_ce: 0.035848
2022-01-08 09:46:40,770 iteration 1051 : loss : 0.069830, loss_ce: 0.033191
2022-01-08 09:46:42,129 iteration 1052 : loss : 0.099882, loss_ce: 0.047485
2022-01-08 09:46:43,418 iteration 1053 : loss : 0.065120, loss_ce: 0.020971
2022-01-08 09:46:44,855 iteration 1054 : loss : 0.083944, loss_ce: 0.041422
 16%|████▋                         | 62/400 [26:15<2:21:06, 25.05s/it]2022-01-08 09:46:46,256 iteration 1055 : loss : 0.062183, loss_ce: 0.021028
2022-01-08 09:46:47,591 iteration 1056 : loss : 0.069234, loss_ce: 0.034640
2022-01-08 09:46:48,925 iteration 1057 : loss : 0.096039, loss_ce: 0.048660
2022-01-08 09:46:50,293 iteration 1058 : loss : 0.075307, loss_ce: 0.036848
2022-01-08 09:46:51,588 iteration 1059 : loss : 0.070208, loss_ce: 0.026566
2022-01-08 09:46:52,951 iteration 1060 : loss : 0.063090, loss_ce: 0.024232
2022-01-08 09:46:54,355 iteration 1061 : loss : 0.076393, loss_ce: 0.033521
2022-01-08 09:46:55,741 iteration 1062 : loss : 0.077727, loss_ce: 0.033772
2022-01-08 09:46:57,171 iteration 1063 : loss : 0.126232, loss_ce: 0.046634
2022-01-08 09:46:58,521 iteration 1064 : loss : 0.113596, loss_ce: 0.045829
2022-01-08 09:46:59,819 iteration 1065 : loss : 0.063789, loss_ce: 0.023803
2022-01-08 09:47:01,220 iteration 1066 : loss : 0.067371, loss_ce: 0.029993
2022-01-08 09:47:02,572 iteration 1067 : loss : 0.157376, loss_ce: 0.042393
2022-01-08 09:47:03,988 iteration 1068 : loss : 0.088586, loss_ce: 0.045446
2022-01-08 09:47:05,375 iteration 1069 : loss : 0.115716, loss_ce: 0.052101
2022-01-08 09:47:06,727 iteration 1070 : loss : 0.061706, loss_ce: 0.025491
2022-01-08 09:47:08,075 iteration 1071 : loss : 0.057132, loss_ce: 0.027374
 16%|████▋                         | 63/400 [26:38<2:17:35, 24.50s/it]2022-01-08 09:47:09,563 iteration 1072 : loss : 0.054605, loss_ce: 0.018953
2022-01-08 09:47:10,903 iteration 1073 : loss : 0.107962, loss_ce: 0.029069
2022-01-08 09:47:12,256 iteration 1074 : loss : 0.084703, loss_ce: 0.037416
2022-01-08 09:47:13,650 iteration 1075 : loss : 0.087327, loss_ce: 0.031045
2022-01-08 09:47:14,948 iteration 1076 : loss : 0.052072, loss_ce: 0.018645
2022-01-08 09:47:16,307 iteration 1077 : loss : 0.051235, loss_ce: 0.017129
2022-01-08 09:47:17,746 iteration 1078 : loss : 0.112569, loss_ce: 0.062512
2022-01-08 09:47:19,073 iteration 1079 : loss : 0.057863, loss_ce: 0.023298
2022-01-08 09:47:20,391 iteration 1080 : loss : 0.053770, loss_ce: 0.025165
2022-01-08 09:47:21,788 iteration 1081 : loss : 0.075478, loss_ce: 0.023362
2022-01-08 09:47:23,178 iteration 1082 : loss : 0.065457, loss_ce: 0.035133
2022-01-08 09:47:24,585 iteration 1083 : loss : 0.095543, loss_ce: 0.043501
2022-01-08 09:47:25,934 iteration 1084 : loss : 0.081890, loss_ce: 0.042509
2022-01-08 09:47:27,393 iteration 1085 : loss : 0.068308, loss_ce: 0.024057
2022-01-08 09:47:28,736 iteration 1086 : loss : 0.064395, loss_ce: 0.027776
2022-01-08 09:47:30,095 iteration 1087 : loss : 0.079332, loss_ce: 0.029810
2022-01-08 09:47:31,511 iteration 1088 : loss : 0.063056, loss_ce: 0.028571
 16%|████▊                         | 64/400 [27:02<2:15:24, 24.18s/it]2022-01-08 09:47:32,887 iteration 1089 : loss : 0.074889, loss_ce: 0.029837
2022-01-08 09:47:34,228 iteration 1090 : loss : 0.059309, loss_ce: 0.030041
2022-01-08 09:47:35,581 iteration 1091 : loss : 0.062673, loss_ce: 0.028734
2022-01-08 09:47:36,950 iteration 1092 : loss : 0.059418, loss_ce: 0.027392
2022-01-08 09:47:38,346 iteration 1093 : loss : 0.071241, loss_ce: 0.024870
2022-01-08 09:47:39,776 iteration 1094 : loss : 0.063743, loss_ce: 0.026438
2022-01-08 09:47:41,087 iteration 1095 : loss : 0.051087, loss_ce: 0.022592
2022-01-08 09:47:42,547 iteration 1096 : loss : 0.069795, loss_ce: 0.031105
2022-01-08 09:47:43,921 iteration 1097 : loss : 0.082379, loss_ce: 0.032323
2022-01-08 09:47:45,246 iteration 1098 : loss : 0.048129, loss_ce: 0.021482
2022-01-08 09:47:46,641 iteration 1099 : loss : 0.059209, loss_ce: 0.019497
2022-01-08 09:47:48,079 iteration 1100 : loss : 0.053417, loss_ce: 0.019586
2022-01-08 09:47:49,444 iteration 1101 : loss : 0.062873, loss_ce: 0.025956
2022-01-08 09:47:50,884 iteration 1102 : loss : 0.094857, loss_ce: 0.031354
2022-01-08 09:47:52,224 iteration 1103 : loss : 0.077660, loss_ce: 0.027590
2022-01-08 09:47:53,574 iteration 1104 : loss : 0.076144, loss_ce: 0.033481
2022-01-08 09:47:53,574 Training Data Eval:
2022-01-08 09:48:00,456   Average segmentation loss on training set: 0.0734
2022-01-08 09:48:00,457 Validation Data Eval:
2022-01-08 09:48:02,829   Average segmentation loss on validation set: 0.1995
2022-01-08 09:48:04,255 iteration 1105 : loss : 0.058439, loss_ce: 0.019323
 16%|████▉                         | 65/400 [27:35<2:29:32, 26.78s/it]2022-01-08 09:48:05,708 iteration 1106 : loss : 0.061987, loss_ce: 0.022961
2022-01-08 09:48:07,048 iteration 1107 : loss : 0.074104, loss_ce: 0.042776
2022-01-08 09:48:08,419 iteration 1108 : loss : 0.070587, loss_ce: 0.019033
2022-01-08 09:48:09,737 iteration 1109 : loss : 0.058991, loss_ce: 0.027436
2022-01-08 09:48:11,121 iteration 1110 : loss : 0.074627, loss_ce: 0.036235
2022-01-08 09:48:12,456 iteration 1111 : loss : 0.063933, loss_ce: 0.028392
2022-01-08 09:48:13,803 iteration 1112 : loss : 0.060246, loss_ce: 0.026168
2022-01-08 09:48:15,141 iteration 1113 : loss : 0.062592, loss_ce: 0.021676
2022-01-08 09:48:16,565 iteration 1114 : loss : 0.077514, loss_ce: 0.038682
2022-01-08 09:48:18,052 iteration 1115 : loss : 0.079476, loss_ce: 0.037034
2022-01-08 09:48:19,457 iteration 1116 : loss : 0.087792, loss_ce: 0.026331
2022-01-08 09:48:20,933 iteration 1117 : loss : 0.043073, loss_ce: 0.015445
2022-01-08 09:48:22,340 iteration 1118 : loss : 0.106195, loss_ce: 0.034371
2022-01-08 09:48:23,681 iteration 1119 : loss : 0.045743, loss_ce: 0.021229
2022-01-08 09:48:25,011 iteration 1120 : loss : 0.056920, loss_ce: 0.022387
2022-01-08 09:48:26,391 iteration 1121 : loss : 0.082233, loss_ce: 0.026962
2022-01-08 09:48:27,760 iteration 1122 : loss : 0.067312, loss_ce: 0.032779
 16%|████▉                         | 66/400 [27:58<2:23:25, 25.76s/it]2022-01-08 09:48:29,122 iteration 1123 : loss : 0.079623, loss_ce: 0.027007
2022-01-08 09:48:30,524 iteration 1124 : loss : 0.073618, loss_ce: 0.032919
2022-01-08 09:48:31,884 iteration 1125 : loss : 0.051053, loss_ce: 0.016284
2022-01-08 09:48:33,221 iteration 1126 : loss : 0.099421, loss_ce: 0.040467
2022-01-08 09:48:34,553 iteration 1127 : loss : 0.070329, loss_ce: 0.029406
2022-01-08 09:48:35,961 iteration 1128 : loss : 0.067870, loss_ce: 0.031252
2022-01-08 09:48:37,374 iteration 1129 : loss : 0.096231, loss_ce: 0.032405
2022-01-08 09:48:38,720 iteration 1130 : loss : 0.053981, loss_ce: 0.025206
2022-01-08 09:48:40,069 iteration 1131 : loss : 0.069169, loss_ce: 0.032492
2022-01-08 09:48:41,470 iteration 1132 : loss : 0.068851, loss_ce: 0.028814
2022-01-08 09:48:42,761 iteration 1133 : loss : 0.044401, loss_ce: 0.021645
2022-01-08 09:48:44,125 iteration 1134 : loss : 0.074129, loss_ce: 0.028619
2022-01-08 09:48:45,452 iteration 1135 : loss : 0.084250, loss_ce: 0.037370
2022-01-08 09:48:46,806 iteration 1136 : loss : 0.045500, loss_ce: 0.017438
2022-01-08 09:48:48,143 iteration 1137 : loss : 0.092552, loss_ce: 0.039268
2022-01-08 09:48:49,547 iteration 1138 : loss : 0.065761, loss_ce: 0.030124
2022-01-08 09:48:50,918 iteration 1139 : loss : 0.085816, loss_ce: 0.026804
 17%|█████                         | 67/400 [28:21<2:18:39, 24.98s/it]2022-01-08 09:48:52,331 iteration 1140 : loss : 0.066144, loss_ce: 0.025166
2022-01-08 09:48:53,660 iteration 1141 : loss : 0.061914, loss_ce: 0.023784
2022-01-08 09:48:55,085 iteration 1142 : loss : 0.083265, loss_ce: 0.043938
2022-01-08 09:48:56,433 iteration 1143 : loss : 0.064682, loss_ce: 0.023765
2022-01-08 09:48:57,831 iteration 1144 : loss : 0.107261, loss_ce: 0.058773
2022-01-08 09:48:59,141 iteration 1145 : loss : 0.050570, loss_ce: 0.019879
2022-01-08 09:49:00,518 iteration 1146 : loss : 0.088137, loss_ce: 0.041522
2022-01-08 09:49:01,793 iteration 1147 : loss : 0.069829, loss_ce: 0.027725
2022-01-08 09:49:03,198 iteration 1148 : loss : 0.048960, loss_ce: 0.018640
2022-01-08 09:49:04,524 iteration 1149 : loss : 0.055833, loss_ce: 0.026278
2022-01-08 09:49:05,900 iteration 1150 : loss : 0.067492, loss_ce: 0.030063
2022-01-08 09:49:07,295 iteration 1151 : loss : 0.066316, loss_ce: 0.024354
2022-01-08 09:49:08,772 iteration 1152 : loss : 0.108842, loss_ce: 0.046463
2022-01-08 09:49:10,040 iteration 1153 : loss : 0.072432, loss_ce: 0.027002
2022-01-08 09:49:11,436 iteration 1154 : loss : 0.079599, loss_ce: 0.029225
2022-01-08 09:49:12,750 iteration 1155 : loss : 0.069071, loss_ce: 0.025097
2022-01-08 09:49:14,166 iteration 1156 : loss : 0.082929, loss_ce: 0.040770
 17%|█████                         | 68/400 [28:44<2:15:21, 24.46s/it]2022-01-08 09:49:15,548 iteration 1157 : loss : 0.104443, loss_ce: 0.032862
2022-01-08 09:49:16,908 iteration 1158 : loss : 0.058021, loss_ce: 0.020255
2022-01-08 09:49:18,337 iteration 1159 : loss : 0.051173, loss_ce: 0.017033
2022-01-08 09:49:19,816 iteration 1160 : loss : 0.088681, loss_ce: 0.034814
2022-01-08 09:49:21,237 iteration 1161 : loss : 0.051530, loss_ce: 0.024012
2022-01-08 09:49:22,630 iteration 1162 : loss : 0.054266, loss_ce: 0.019596
2022-01-08 09:49:23,964 iteration 1163 : loss : 0.049559, loss_ce: 0.023204
2022-01-08 09:49:25,327 iteration 1164 : loss : 0.087886, loss_ce: 0.031262
2022-01-08 09:49:26,700 iteration 1165 : loss : 0.052069, loss_ce: 0.024646
2022-01-08 09:49:28,137 iteration 1166 : loss : 0.047586, loss_ce: 0.016851
2022-01-08 09:49:29,459 iteration 1167 : loss : 0.051384, loss_ce: 0.024148
2022-01-08 09:49:30,814 iteration 1168 : loss : 0.063762, loss_ce: 0.029149
2022-01-08 09:49:32,143 iteration 1169 : loss : 0.048378, loss_ce: 0.020199
2022-01-08 09:49:33,475 iteration 1170 : loss : 0.072992, loss_ce: 0.031295
2022-01-08 09:49:34,854 iteration 1171 : loss : 0.050453, loss_ce: 0.017605
2022-01-08 09:49:36,226 iteration 1172 : loss : 0.071737, loss_ce: 0.031810
2022-01-08 09:49:37,614 iteration 1173 : loss : 0.072245, loss_ce: 0.024793
 17%|█████▏                        | 69/400 [29:08<2:13:15, 24.16s/it]2022-01-08 09:49:38,999 iteration 1174 : loss : 0.076185, loss_ce: 0.026672
2022-01-08 09:49:40,353 iteration 1175 : loss : 0.050657, loss_ce: 0.021484
2022-01-08 09:49:41,706 iteration 1176 : loss : 0.056457, loss_ce: 0.021495
2022-01-08 09:49:43,033 iteration 1177 : loss : 0.052580, loss_ce: 0.021575
2022-01-08 09:49:44,427 iteration 1178 : loss : 0.044948, loss_ce: 0.014407
2022-01-08 09:49:45,857 iteration 1179 : loss : 0.059421, loss_ce: 0.025792
2022-01-08 09:49:47,183 iteration 1180 : loss : 0.051363, loss_ce: 0.023300
2022-01-08 09:49:48,541 iteration 1181 : loss : 0.051192, loss_ce: 0.020748
2022-01-08 09:49:49,914 iteration 1182 : loss : 0.074768, loss_ce: 0.029151
2022-01-08 09:49:51,225 iteration 1183 : loss : 0.057564, loss_ce: 0.022827
2022-01-08 09:49:52,565 iteration 1184 : loss : 0.039764, loss_ce: 0.018332
2022-01-08 09:49:53,984 iteration 1185 : loss : 0.054710, loss_ce: 0.022157
2022-01-08 09:49:55,315 iteration 1186 : loss : 0.056771, loss_ce: 0.030622
2022-01-08 09:49:56,747 iteration 1187 : loss : 0.047944, loss_ce: 0.016744
2022-01-08 09:49:58,072 iteration 1188 : loss : 0.056319, loss_ce: 0.022225
2022-01-08 09:49:59,510 iteration 1189 : loss : 0.053646, loss_ce: 0.025627
2022-01-08 09:49:59,510 Training Data Eval:
2022-01-08 09:50:06,371   Average segmentation loss on training set: 0.0610
2022-01-08 09:50:06,372 Validation Data Eval:
2022-01-08 09:50:08,746   Average segmentation loss on validation set: 0.0875
2022-01-08 09:50:10,130 iteration 1190 : loss : 0.057581, loss_ce: 0.027450
 18%|█████▎                        | 70/400 [29:40<2:26:40, 26.67s/it]2022-01-08 09:50:11,603 iteration 1191 : loss : 0.056311, loss_ce: 0.024695
2022-01-08 09:50:12,920 iteration 1192 : loss : 0.046933, loss_ce: 0.021622
2022-01-08 09:50:14,362 iteration 1193 : loss : 0.079450, loss_ce: 0.029396
2022-01-08 09:50:15,712 iteration 1194 : loss : 0.054510, loss_ce: 0.017853
2022-01-08 09:50:17,026 iteration 1195 : loss : 0.039368, loss_ce: 0.017806
2022-01-08 09:50:18,345 iteration 1196 : loss : 0.045385, loss_ce: 0.018705
2022-01-08 09:50:19,635 iteration 1197 : loss : 0.046555, loss_ce: 0.017345
2022-01-08 09:50:21,043 iteration 1198 : loss : 0.048723, loss_ce: 0.022579
2022-01-08 09:50:22,412 iteration 1199 : loss : 0.046378, loss_ce: 0.018141
2022-01-08 09:50:23,747 iteration 1200 : loss : 0.077033, loss_ce: 0.023587
2022-01-08 09:50:25,039 iteration 1201 : loss : 0.041709, loss_ce: 0.020460
2022-01-08 09:50:26,418 iteration 1202 : loss : 0.049257, loss_ce: 0.018297
2022-01-08 09:50:27,785 iteration 1203 : loss : 0.072489, loss_ce: 0.023886
2022-01-08 09:50:29,149 iteration 1204 : loss : 0.065251, loss_ce: 0.024583
2022-01-08 09:50:30,522 iteration 1205 : loss : 0.081661, loss_ce: 0.036285
2022-01-08 09:50:31,897 iteration 1206 : loss : 0.042297, loss_ce: 0.018169
2022-01-08 09:50:33,308 iteration 1207 : loss : 0.055479, loss_ce: 0.017583
 18%|█████▎                        | 71/400 [30:03<2:20:29, 25.62s/it]2022-01-08 09:50:34,746 iteration 1208 : loss : 0.059016, loss_ce: 0.030707
2022-01-08 09:50:36,113 iteration 1209 : loss : 0.054566, loss_ce: 0.025113
2022-01-08 09:50:37,471 iteration 1210 : loss : 0.053342, loss_ce: 0.020064
2022-01-08 09:50:38,787 iteration 1211 : loss : 0.040821, loss_ce: 0.015665
2022-01-08 09:50:40,087 iteration 1212 : loss : 0.076083, loss_ce: 0.025650
2022-01-08 09:50:41,586 iteration 1213 : loss : 0.068458, loss_ce: 0.030329
2022-01-08 09:50:42,946 iteration 1214 : loss : 0.048395, loss_ce: 0.020448
2022-01-08 09:50:44,311 iteration 1215 : loss : 0.054995, loss_ce: 0.021797
2022-01-08 09:50:45,792 iteration 1216 : loss : 0.053920, loss_ce: 0.021101
2022-01-08 09:50:47,162 iteration 1217 : loss : 0.037004, loss_ce: 0.013360
2022-01-08 09:50:48,475 iteration 1218 : loss : 0.039891, loss_ce: 0.015132
2022-01-08 09:50:49,773 iteration 1219 : loss : 0.046592, loss_ce: 0.016570
2022-01-08 09:50:51,141 iteration 1220 : loss : 0.047345, loss_ce: 0.017889
2022-01-08 09:50:52,471 iteration 1221 : loss : 0.058632, loss_ce: 0.024050
2022-01-08 09:50:53,898 iteration 1222 : loss : 0.056678, loss_ce: 0.018991
2022-01-08 09:50:55,233 iteration 1223 : loss : 0.057494, loss_ce: 0.025603
2022-01-08 09:50:56,635 iteration 1224 : loss : 0.083537, loss_ce: 0.035411
 18%|█████▍                        | 72/400 [30:27<2:16:17, 24.93s/it]2022-01-08 09:50:58,012 iteration 1225 : loss : 0.037617, loss_ce: 0.016386
2022-01-08 09:50:59,409 iteration 1226 : loss : 0.076680, loss_ce: 0.034026
2022-01-08 09:51:00,717 iteration 1227 : loss : 0.056404, loss_ce: 0.022016
2022-01-08 09:51:02,056 iteration 1228 : loss : 0.059677, loss_ce: 0.021446
2022-01-08 09:51:03,350 iteration 1229 : loss : 0.051518, loss_ce: 0.015392
2022-01-08 09:51:04,672 iteration 1230 : loss : 0.045557, loss_ce: 0.018340
2022-01-08 09:51:06,073 iteration 1231 : loss : 0.058153, loss_ce: 0.031364
2022-01-08 09:51:07,386 iteration 1232 : loss : 0.068902, loss_ce: 0.032645
2022-01-08 09:51:08,718 iteration 1233 : loss : 0.039250, loss_ce: 0.015235
2022-01-08 09:51:10,110 iteration 1234 : loss : 0.047256, loss_ce: 0.019658
2022-01-08 09:51:11,491 iteration 1235 : loss : 0.088357, loss_ce: 0.030166
2022-01-08 09:51:12,819 iteration 1236 : loss : 0.043181, loss_ce: 0.016533
2022-01-08 09:51:14,105 iteration 1237 : loss : 0.113700, loss_ce: 0.058691
2022-01-08 09:51:15,516 iteration 1238 : loss : 0.056663, loss_ce: 0.016664
2022-01-08 09:51:16,824 iteration 1239 : loss : 0.041392, loss_ce: 0.016106
2022-01-08 09:51:18,152 iteration 1240 : loss : 0.049816, loss_ce: 0.019963
2022-01-08 09:51:19,480 iteration 1241 : loss : 0.050576, loss_ce: 0.024802
 18%|█████▍                        | 73/400 [30:50<2:12:27, 24.30s/it]2022-01-08 09:51:20,895 iteration 1242 : loss : 0.066015, loss_ce: 0.040609
2022-01-08 09:51:22,200 iteration 1243 : loss : 0.079385, loss_ce: 0.027891
2022-01-08 09:51:23,523 iteration 1244 : loss : 0.071853, loss_ce: 0.029469
2022-01-08 09:51:24,898 iteration 1245 : loss : 0.080687, loss_ce: 0.030585
2022-01-08 09:51:26,264 iteration 1246 : loss : 0.069978, loss_ce: 0.021985
2022-01-08 09:51:27,583 iteration 1247 : loss : 0.044231, loss_ce: 0.019891
2022-01-08 09:51:28,962 iteration 1248 : loss : 0.053697, loss_ce: 0.016694
2022-01-08 09:51:30,288 iteration 1249 : loss : 0.040717, loss_ce: 0.013961
2022-01-08 09:51:31,748 iteration 1250 : loss : 0.057638, loss_ce: 0.027224
2022-01-08 09:51:33,216 iteration 1251 : loss : 0.067302, loss_ce: 0.036049
2022-01-08 09:51:34,594 iteration 1252 : loss : 0.044386, loss_ce: 0.017729
2022-01-08 09:51:35,867 iteration 1253 : loss : 0.039229, loss_ce: 0.014799
2022-01-08 09:51:37,226 iteration 1254 : loss : 0.062031, loss_ce: 0.032676
2022-01-08 09:51:38,602 iteration 1255 : loss : 0.045628, loss_ce: 0.018932
2022-01-08 09:51:39,949 iteration 1256 : loss : 0.062262, loss_ce: 0.026717
2022-01-08 09:51:41,321 iteration 1257 : loss : 0.074624, loss_ce: 0.022893
2022-01-08 09:51:42,700 iteration 1258 : loss : 0.047897, loss_ce: 0.022900
 18%|█████▌                        | 74/400 [31:13<2:10:16, 23.98s/it]2022-01-08 09:51:44,055 iteration 1259 : loss : 0.043351, loss_ce: 0.019057
2022-01-08 09:51:45,467 iteration 1260 : loss : 0.039322, loss_ce: 0.017086
2022-01-08 09:51:46,798 iteration 1261 : loss : 0.050533, loss_ce: 0.017605
2022-01-08 09:51:48,225 iteration 1262 : loss : 0.063245, loss_ce: 0.025213
2022-01-08 09:51:49,671 iteration 1263 : loss : 0.067610, loss_ce: 0.028856
2022-01-08 09:51:51,008 iteration 1264 : loss : 0.050649, loss_ce: 0.022990
2022-01-08 09:51:52,429 iteration 1265 : loss : 0.067252, loss_ce: 0.034890
2022-01-08 09:51:53,833 iteration 1266 : loss : 0.061768, loss_ce: 0.025771
2022-01-08 09:51:55,219 iteration 1267 : loss : 0.049065, loss_ce: 0.018687
2022-01-08 09:51:56,557 iteration 1268 : loss : 0.077700, loss_ce: 0.024635
2022-01-08 09:51:57,999 iteration 1269 : loss : 0.099497, loss_ce: 0.028923
2022-01-08 09:51:59,386 iteration 1270 : loss : 0.042514, loss_ce: 0.018750
2022-01-08 09:52:00,742 iteration 1271 : loss : 0.052147, loss_ce: 0.025685
2022-01-08 09:52:02,020 iteration 1272 : loss : 0.052444, loss_ce: 0.018431
2022-01-08 09:52:03,359 iteration 1273 : loss : 0.046847, loss_ce: 0.017615
2022-01-08 09:52:04,715 iteration 1274 : loss : 0.048174, loss_ce: 0.015971
2022-01-08 09:52:04,715 Training Data Eval:
2022-01-08 09:52:11,580   Average segmentation loss on training set: 0.0392
2022-01-08 09:52:11,581 Validation Data Eval:
2022-01-08 09:52:13,952   Average segmentation loss on validation set: 0.1141
2022-01-08 09:52:15,315 iteration 1275 : loss : 0.072609, loss_ce: 0.028610
 19%|█████▋                        | 75/400 [31:45<2:23:55, 26.57s/it]2022-01-08 09:52:16,634 iteration 1276 : loss : 0.041268, loss_ce: 0.016710
2022-01-08 09:52:18,116 iteration 1277 : loss : 0.050490, loss_ce: 0.020651
2022-01-08 09:52:19,434 iteration 1278 : loss : 0.036820, loss_ce: 0.013159
2022-01-08 09:52:20,882 iteration 1279 : loss : 0.051638, loss_ce: 0.016957
2022-01-08 09:52:22,259 iteration 1280 : loss : 0.064880, loss_ce: 0.030326
2022-01-08 09:52:23,635 iteration 1281 : loss : 0.056426, loss_ce: 0.027900
2022-01-08 09:52:24,942 iteration 1282 : loss : 0.034028, loss_ce: 0.014739
2022-01-08 09:52:26,360 iteration 1283 : loss : 0.066012, loss_ce: 0.021139
2022-01-08 09:52:27,688 iteration 1284 : loss : 0.048156, loss_ce: 0.019850
2022-01-08 09:52:29,127 iteration 1285 : loss : 0.050109, loss_ce: 0.017288
2022-01-08 09:52:30,572 iteration 1286 : loss : 0.117713, loss_ce: 0.033312
2022-01-08 09:52:31,906 iteration 1287 : loss : 0.035874, loss_ce: 0.019247
2022-01-08 09:52:33,230 iteration 1288 : loss : 0.063600, loss_ce: 0.030001
2022-01-08 09:52:34,595 iteration 1289 : loss : 0.065234, loss_ce: 0.024993
2022-01-08 09:52:35,982 iteration 1290 : loss : 0.071868, loss_ce: 0.033325
2022-01-08 09:52:37,388 iteration 1291 : loss : 0.074689, loss_ce: 0.035935
2022-01-08 09:52:38,802 iteration 1292 : loss : 0.050783, loss_ce: 0.017941
 19%|█████▋                        | 76/400 [32:09<2:18:29, 25.65s/it]2022-01-08 09:52:40,216 iteration 1293 : loss : 0.047818, loss_ce: 0.018330
2022-01-08 09:52:41,603 iteration 1294 : loss : 0.083950, loss_ce: 0.035212
2022-01-08 09:52:43,041 iteration 1295 : loss : 0.069493, loss_ce: 0.019022
2022-01-08 09:52:44,420 iteration 1296 : loss : 0.068814, loss_ce: 0.021603
2022-01-08 09:52:45,739 iteration 1297 : loss : 0.052749, loss_ce: 0.028991
2022-01-08 09:52:47,039 iteration 1298 : loss : 0.038018, loss_ce: 0.014885
2022-01-08 09:52:48,414 iteration 1299 : loss : 0.050599, loss_ce: 0.025704
2022-01-08 09:52:49,761 iteration 1300 : loss : 0.058353, loss_ce: 0.019788
2022-01-08 09:52:51,160 iteration 1301 : loss : 0.042185, loss_ce: 0.016699
2022-01-08 09:52:52,501 iteration 1302 : loss : 0.060283, loss_ce: 0.025006
2022-01-08 09:52:53,841 iteration 1303 : loss : 0.056751, loss_ce: 0.023347
2022-01-08 09:52:55,215 iteration 1304 : loss : 0.047756, loss_ce: 0.020230
2022-01-08 09:52:56,573 iteration 1305 : loss : 0.063801, loss_ce: 0.026817
2022-01-08 09:52:57,963 iteration 1306 : loss : 0.086708, loss_ce: 0.030354
2022-01-08 09:52:59,359 iteration 1307 : loss : 0.043251, loss_ce: 0.021928
2022-01-08 09:53:00,733 iteration 1308 : loss : 0.075139, loss_ce: 0.020984
2022-01-08 09:53:02,033 iteration 1309 : loss : 0.054908, loss_ce: 0.022230
 19%|█████▊                        | 77/400 [32:32<2:14:10, 24.92s/it]2022-01-08 09:53:03,507 iteration 1310 : loss : 0.058887, loss_ce: 0.022553
2022-01-08 09:53:04,837 iteration 1311 : loss : 0.045253, loss_ce: 0.019682
2022-01-08 09:53:06,159 iteration 1312 : loss : 0.050004, loss_ce: 0.019001
2022-01-08 09:53:07,501 iteration 1313 : loss : 0.059255, loss_ce: 0.017880
2022-01-08 09:53:08,875 iteration 1314 : loss : 0.063649, loss_ce: 0.029079
2022-01-08 09:53:10,275 iteration 1315 : loss : 0.065486, loss_ce: 0.024273
2022-01-08 09:53:11,751 iteration 1316 : loss : 0.058966, loss_ce: 0.025618
2022-01-08 09:53:13,100 iteration 1317 : loss : 0.069595, loss_ce: 0.036857
2022-01-08 09:53:14,542 iteration 1318 : loss : 0.135639, loss_ce: 0.021847
2022-01-08 09:53:15,930 iteration 1319 : loss : 0.060831, loss_ce: 0.029955
2022-01-08 09:53:17,267 iteration 1320 : loss : 0.035271, loss_ce: 0.014128
2022-01-08 09:53:18,648 iteration 1321 : loss : 0.060806, loss_ce: 0.018052
2022-01-08 09:53:19,955 iteration 1322 : loss : 0.057229, loss_ce: 0.026540
2022-01-08 09:53:21,301 iteration 1323 : loss : 0.077071, loss_ce: 0.028250
2022-01-08 09:53:22,685 iteration 1324 : loss : 0.062719, loss_ce: 0.026061
2022-01-08 09:53:24,103 iteration 1325 : loss : 0.070542, loss_ce: 0.026521
2022-01-08 09:53:25,371 iteration 1326 : loss : 0.058310, loss_ce: 0.022441
 20%|█████▊                        | 78/400 [32:56<2:11:11, 24.45s/it]2022-01-08 09:53:26,746 iteration 1327 : loss : 0.101443, loss_ce: 0.032264
2022-01-08 09:53:28,134 iteration 1328 : loss : 0.062820, loss_ce: 0.028222
2022-01-08 09:53:29,490 iteration 1329 : loss : 0.046178, loss_ce: 0.019487
2022-01-08 09:53:30,819 iteration 1330 : loss : 0.094886, loss_ce: 0.028278
2022-01-08 09:53:32,128 iteration 1331 : loss : 0.056965, loss_ce: 0.028629
2022-01-08 09:53:33,456 iteration 1332 : loss : 0.055812, loss_ce: 0.017748
2022-01-08 09:53:34,794 iteration 1333 : loss : 0.046796, loss_ce: 0.015320
2022-01-08 09:53:36,156 iteration 1334 : loss : 0.072214, loss_ce: 0.026523
2022-01-08 09:53:37,481 iteration 1335 : loss : 0.053125, loss_ce: 0.015969
2022-01-08 09:53:38,816 iteration 1336 : loss : 0.057156, loss_ce: 0.024642
2022-01-08 09:53:40,232 iteration 1337 : loss : 0.075014, loss_ce: 0.029586
2022-01-08 09:53:41,619 iteration 1338 : loss : 0.055380, loss_ce: 0.019213
2022-01-08 09:53:43,000 iteration 1339 : loss : 0.052844, loss_ce: 0.026217
2022-01-08 09:53:44,388 iteration 1340 : loss : 0.061595, loss_ce: 0.025605
2022-01-08 09:53:45,762 iteration 1341 : loss : 0.050955, loss_ce: 0.019887
2022-01-08 09:53:47,107 iteration 1342 : loss : 0.074055, loss_ce: 0.031192
2022-01-08 09:53:48,412 iteration 1343 : loss : 0.046385, loss_ce: 0.017323
 20%|█████▉                        | 79/400 [33:19<2:08:31, 24.02s/it]2022-01-08 09:53:49,804 iteration 1344 : loss : 0.058974, loss_ce: 0.024009
2022-01-08 09:53:51,235 iteration 1345 : loss : 0.067787, loss_ce: 0.026352
2022-01-08 09:53:52,591 iteration 1346 : loss : 0.054972, loss_ce: 0.026120
2022-01-08 09:53:54,036 iteration 1347 : loss : 0.051884, loss_ce: 0.024127
2022-01-08 09:53:55,528 iteration 1348 : loss : 0.084835, loss_ce: 0.031492
2022-01-08 09:53:56,907 iteration 1349 : loss : 0.051613, loss_ce: 0.022853
2022-01-08 09:53:58,257 iteration 1350 : loss : 0.053744, loss_ce: 0.023171
2022-01-08 09:53:59,611 iteration 1351 : loss : 0.050741, loss_ce: 0.024150
2022-01-08 09:54:01,023 iteration 1352 : loss : 0.043785, loss_ce: 0.021269
2022-01-08 09:54:02,424 iteration 1353 : loss : 0.066183, loss_ce: 0.024245
2022-01-08 09:54:03,823 iteration 1354 : loss : 0.066845, loss_ce: 0.021556
2022-01-08 09:54:05,278 iteration 1355 : loss : 0.130870, loss_ce: 0.031677
2022-01-08 09:54:06,662 iteration 1356 : loss : 0.048727, loss_ce: 0.022397
2022-01-08 09:54:08,060 iteration 1357 : loss : 0.034548, loss_ce: 0.014710
2022-01-08 09:54:09,481 iteration 1358 : loss : 0.071938, loss_ce: 0.031508
2022-01-08 09:54:10,839 iteration 1359 : loss : 0.063899, loss_ce: 0.029112
2022-01-08 09:54:10,839 Training Data Eval:
2022-01-08 09:54:17,689   Average segmentation loss on training set: 0.0417
2022-01-08 09:54:17,689 Validation Data Eval:
2022-01-08 09:54:20,053   Average segmentation loss on validation set: 0.0800
2022-01-08 09:54:24,180 Found new lowest validation loss at iteration 1359! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:54:25,524 iteration 1360 : loss : 0.060799, loss_ce: 0.015835
 20%|██████                        | 80/400 [33:56<2:29:04, 27.95s/it]2022-01-08 09:54:26,942 iteration 1361 : loss : 0.072282, loss_ce: 0.029919
2022-01-08 09:54:28,241 iteration 1362 : loss : 0.048384, loss_ce: 0.016566
2022-01-08 09:54:29,684 iteration 1363 : loss : 0.079807, loss_ce: 0.034820
2022-01-08 09:54:30,845 iteration 1364 : loss : 0.069654, loss_ce: 0.019603
2022-01-08 09:54:32,197 iteration 1365 : loss : 0.051749, loss_ce: 0.021850
2022-01-08 09:54:33,501 iteration 1366 : loss : 0.053797, loss_ce: 0.026253
2022-01-08 09:54:34,771 iteration 1367 : loss : 0.052132, loss_ce: 0.021542
2022-01-08 09:54:36,068 iteration 1368 : loss : 0.045898, loss_ce: 0.023263
2022-01-08 09:54:37,321 iteration 1369 : loss : 0.071780, loss_ce: 0.029485
2022-01-08 09:54:38,681 iteration 1370 : loss : 0.094562, loss_ce: 0.047213
2022-01-08 09:54:40,021 iteration 1371 : loss : 0.036706, loss_ce: 0.013877
2022-01-08 09:54:41,374 iteration 1372 : loss : 0.043763, loss_ce: 0.018494
2022-01-08 09:54:42,714 iteration 1373 : loss : 0.052794, loss_ce: 0.019995
2022-01-08 09:54:44,117 iteration 1374 : loss : 0.050968, loss_ce: 0.016302
2022-01-08 09:54:45,534 iteration 1375 : loss : 0.052416, loss_ce: 0.024442
2022-01-08 09:54:46,920 iteration 1376 : loss : 0.050452, loss_ce: 0.016410
2022-01-08 09:54:48,292 iteration 1377 : loss : 0.029058, loss_ce: 0.011564
 20%|██████                        | 81/400 [34:18<2:20:20, 26.40s/it]2022-01-08 09:54:49,644 iteration 1378 : loss : 0.053345, loss_ce: 0.021301
2022-01-08 09:54:51,060 iteration 1379 : loss : 0.053619, loss_ce: 0.020450
2022-01-08 09:54:52,437 iteration 1380 : loss : 0.057968, loss_ce: 0.026727
2022-01-08 09:54:53,889 iteration 1381 : loss : 0.064383, loss_ce: 0.025521
2022-01-08 09:54:55,210 iteration 1382 : loss : 0.041180, loss_ce: 0.017096
2022-01-08 09:54:56,652 iteration 1383 : loss : 0.038178, loss_ce: 0.015323
2022-01-08 09:54:58,031 iteration 1384 : loss : 0.058474, loss_ce: 0.021785
2022-01-08 09:54:59,340 iteration 1385 : loss : 0.051268, loss_ce: 0.021482
2022-01-08 09:55:00,691 iteration 1386 : loss : 0.045464, loss_ce: 0.018216
2022-01-08 09:55:02,044 iteration 1387 : loss : 0.047859, loss_ce: 0.020041
2022-01-08 09:55:03,421 iteration 1388 : loss : 0.056712, loss_ce: 0.019920
2022-01-08 09:55:04,787 iteration 1389 : loss : 0.057534, loss_ce: 0.024523
2022-01-08 09:55:06,195 iteration 1390 : loss : 0.042715, loss_ce: 0.018334
2022-01-08 09:55:07,623 iteration 1391 : loss : 0.049803, loss_ce: 0.019666
2022-01-08 09:55:09,019 iteration 1392 : loss : 0.052076, loss_ce: 0.025985
2022-01-08 09:55:10,354 iteration 1393 : loss : 0.043266, loss_ce: 0.014621
2022-01-08 09:55:11,787 iteration 1394 : loss : 0.040743, loss_ce: 0.020583
 20%|██████▏                       | 82/400 [34:42<2:15:16, 25.53s/it]2022-01-08 09:55:13,177 iteration 1395 : loss : 0.051663, loss_ce: 0.021720
2022-01-08 09:55:14,519 iteration 1396 : loss : 0.063969, loss_ce: 0.033498
2022-01-08 09:55:15,942 iteration 1397 : loss : 0.062629, loss_ce: 0.028430
2022-01-08 09:55:17,383 iteration 1398 : loss : 0.050199, loss_ce: 0.024799
2022-01-08 09:55:18,683 iteration 1399 : loss : 0.044261, loss_ce: 0.018801
2022-01-08 09:55:19,976 iteration 1400 : loss : 0.046905, loss_ce: 0.017734
2022-01-08 09:55:21,396 iteration 1401 : loss : 0.050053, loss_ce: 0.016385
2022-01-08 09:55:22,769 iteration 1402 : loss : 0.052440, loss_ce: 0.019220
2022-01-08 09:55:24,088 iteration 1403 : loss : 0.049753, loss_ce: 0.017256
2022-01-08 09:55:25,405 iteration 1404 : loss : 0.029185, loss_ce: 0.013200
2022-01-08 09:55:26,787 iteration 1405 : loss : 0.087189, loss_ce: 0.029413
2022-01-08 09:55:28,178 iteration 1406 : loss : 0.054301, loss_ce: 0.020245
2022-01-08 09:55:29,595 iteration 1407 : loss : 0.031145, loss_ce: 0.013512
2022-01-08 09:55:30,962 iteration 1408 : loss : 0.072962, loss_ce: 0.026218
2022-01-08 09:55:32,343 iteration 1409 : loss : 0.054506, loss_ce: 0.017113
2022-01-08 09:55:33,791 iteration 1410 : loss : 0.065045, loss_ce: 0.023893
2022-01-08 09:55:35,130 iteration 1411 : loss : 0.053576, loss_ce: 0.018936
 21%|██████▏                       | 83/400 [35:05<2:11:24, 24.87s/it]2022-01-08 09:55:36,506 iteration 1412 : loss : 0.053020, loss_ce: 0.020767
2022-01-08 09:55:37,932 iteration 1413 : loss : 0.069877, loss_ce: 0.024422
2022-01-08 09:55:39,283 iteration 1414 : loss : 0.033669, loss_ce: 0.011972
2022-01-08 09:55:40,698 iteration 1415 : loss : 0.038673, loss_ce: 0.011216
2022-01-08 09:55:42,052 iteration 1416 : loss : 0.049885, loss_ce: 0.020607
2022-01-08 09:55:43,388 iteration 1417 : loss : 0.076292, loss_ce: 0.029757
2022-01-08 09:55:44,733 iteration 1418 : loss : 0.061636, loss_ce: 0.027064
2022-01-08 09:55:46,054 iteration 1419 : loss : 0.046946, loss_ce: 0.017528
2022-01-08 09:55:47,465 iteration 1420 : loss : 0.062198, loss_ce: 0.024278
2022-01-08 09:55:48,906 iteration 1421 : loss : 0.056903, loss_ce: 0.022545
2022-01-08 09:55:50,291 iteration 1422 : loss : 0.037647, loss_ce: 0.012729
2022-01-08 09:55:51,581 iteration 1423 : loss : 0.059011, loss_ce: 0.024394
2022-01-08 09:55:52,978 iteration 1424 : loss : 0.052419, loss_ce: 0.021136
2022-01-08 09:55:54,373 iteration 1425 : loss : 0.041541, loss_ce: 0.019645
2022-01-08 09:55:55,698 iteration 1426 : loss : 0.060919, loss_ce: 0.025516
2022-01-08 09:55:57,064 iteration 1427 : loss : 0.083927, loss_ce: 0.034967
2022-01-08 09:55:58,343 iteration 1428 : loss : 0.056902, loss_ce: 0.025140
 21%|██████▎                       | 84/400 [35:29<2:08:22, 24.37s/it]2022-01-08 09:55:59,821 iteration 1429 : loss : 0.052025, loss_ce: 0.022892
2022-01-08 09:56:01,147 iteration 1430 : loss : 0.170448, loss_ce: 0.041259
2022-01-08 09:56:02,516 iteration 1431 : loss : 0.078920, loss_ce: 0.039945
2022-01-08 09:56:03,906 iteration 1432 : loss : 0.061833, loss_ce: 0.028345
2022-01-08 09:56:05,324 iteration 1433 : loss : 0.047310, loss_ce: 0.015727
2022-01-08 09:56:06,667 iteration 1434 : loss : 0.052335, loss_ce: 0.025248
2022-01-08 09:56:08,054 iteration 1435 : loss : 0.054904, loss_ce: 0.023644
2022-01-08 09:56:09,453 iteration 1436 : loss : 0.054203, loss_ce: 0.022416
2022-01-08 09:56:10,852 iteration 1437 : loss : 0.067667, loss_ce: 0.021725
2022-01-08 09:56:12,232 iteration 1438 : loss : 0.054604, loss_ce: 0.027972
2022-01-08 09:56:13,652 iteration 1439 : loss : 0.081383, loss_ce: 0.031716
2022-01-08 09:56:15,052 iteration 1440 : loss : 0.051199, loss_ce: 0.023194
2022-01-08 09:56:16,394 iteration 1441 : loss : 0.044318, loss_ce: 0.021734
2022-01-08 09:56:17,773 iteration 1442 : loss : 0.046932, loss_ce: 0.019470
2022-01-08 09:56:19,176 iteration 1443 : loss : 0.056929, loss_ce: 0.022199
2022-01-08 09:56:20,553 iteration 1444 : loss : 0.095303, loss_ce: 0.029083
2022-01-08 09:56:20,553 Training Data Eval:
2022-01-08 09:56:27,446   Average segmentation loss on training set: 0.0465
2022-01-08 09:56:27,447 Validation Data Eval:
2022-01-08 09:56:29,810   Average segmentation loss on validation set: 0.0819
2022-01-08 09:56:31,187 iteration 1445 : loss : 0.045336, loss_ce: 0.015727
 21%|██████▍                       | 85/400 [36:01<2:21:17, 26.91s/it]2022-01-08 09:56:32,688 iteration 1446 : loss : 0.043529, loss_ce: 0.015993
2022-01-08 09:56:34,086 iteration 1447 : loss : 0.073330, loss_ce: 0.030796
2022-01-08 09:56:35,394 iteration 1448 : loss : 0.059170, loss_ce: 0.031513
2022-01-08 09:56:36,796 iteration 1449 : loss : 0.037716, loss_ce: 0.012703
2022-01-08 09:56:38,263 iteration 1450 : loss : 0.064944, loss_ce: 0.033855
2022-01-08 09:56:39,738 iteration 1451 : loss : 0.068711, loss_ce: 0.033836
2022-01-08 09:56:41,078 iteration 1452 : loss : 0.042866, loss_ce: 0.013022
2022-01-08 09:56:42,426 iteration 1453 : loss : 0.060598, loss_ce: 0.028388
2022-01-08 09:56:43,812 iteration 1454 : loss : 0.074736, loss_ce: 0.024826
2022-01-08 09:56:45,152 iteration 1455 : loss : 0.038830, loss_ce: 0.017410
2022-01-08 09:56:46,607 iteration 1456 : loss : 0.085889, loss_ce: 0.026697
2022-01-08 09:56:47,967 iteration 1457 : loss : 0.069273, loss_ce: 0.029948
2022-01-08 09:56:49,321 iteration 1458 : loss : 0.046790, loss_ce: 0.015307
2022-01-08 09:56:50,649 iteration 1459 : loss : 0.050540, loss_ce: 0.015331
2022-01-08 09:56:52,090 iteration 1460 : loss : 0.085530, loss_ce: 0.041875
2022-01-08 09:56:53,467 iteration 1461 : loss : 0.075082, loss_ce: 0.033338
2022-01-08 09:56:54,875 iteration 1462 : loss : 0.054831, loss_ce: 0.025557
 22%|██████▍                       | 86/400 [36:25<2:15:47, 25.95s/it]2022-01-08 09:56:56,273 iteration 1463 : loss : 0.039831, loss_ce: 0.018242
2022-01-08 09:56:57,611 iteration 1464 : loss : 0.049971, loss_ce: 0.021344
2022-01-08 09:56:59,014 iteration 1465 : loss : 0.068519, loss_ce: 0.022997
2022-01-08 09:57:00,414 iteration 1466 : loss : 0.055644, loss_ce: 0.025919
2022-01-08 09:57:01,800 iteration 1467 : loss : 0.046333, loss_ce: 0.020421
2022-01-08 09:57:03,150 iteration 1468 : loss : 0.042283, loss_ce: 0.021491
2022-01-08 09:57:04,519 iteration 1469 : loss : 0.062757, loss_ce: 0.025489
2022-01-08 09:57:05,927 iteration 1470 : loss : 0.064057, loss_ce: 0.024950
2022-01-08 09:57:07,328 iteration 1471 : loss : 0.059524, loss_ce: 0.030551
2022-01-08 09:57:08,601 iteration 1472 : loss : 0.040668, loss_ce: 0.015986
2022-01-08 09:57:10,077 iteration 1473 : loss : 0.042034, loss_ce: 0.017101
2022-01-08 09:57:11,496 iteration 1474 : loss : 0.062655, loss_ce: 0.022037
2022-01-08 09:57:12,850 iteration 1475 : loss : 0.040872, loss_ce: 0.020787
2022-01-08 09:57:14,296 iteration 1476 : loss : 0.097068, loss_ce: 0.033488
2022-01-08 09:57:15,714 iteration 1477 : loss : 0.047987, loss_ce: 0.017700
2022-01-08 09:57:17,155 iteration 1478 : loss : 0.052851, loss_ce: 0.022008
2022-01-08 09:57:18,473 iteration 1479 : loss : 0.057858, loss_ce: 0.020997
 22%|██████▌                       | 87/400 [36:49<2:11:41, 25.24s/it]2022-01-08 09:57:19,880 iteration 1480 : loss : 0.054597, loss_ce: 0.021870
2022-01-08 09:57:21,229 iteration 1481 : loss : 0.040162, loss_ce: 0.017013
2022-01-08 09:57:22,590 iteration 1482 : loss : 0.046714, loss_ce: 0.021400
2022-01-08 09:57:24,036 iteration 1483 : loss : 0.059074, loss_ce: 0.026035
2022-01-08 09:57:25,537 iteration 1484 : loss : 0.052753, loss_ce: 0.018803
2022-01-08 09:57:26,970 iteration 1485 : loss : 0.041240, loss_ce: 0.018184
2022-01-08 09:57:28,318 iteration 1486 : loss : 0.048729, loss_ce: 0.020803
2022-01-08 09:57:29,696 iteration 1487 : loss : 0.040770, loss_ce: 0.015911
2022-01-08 09:57:31,110 iteration 1488 : loss : 0.034532, loss_ce: 0.015676
2022-01-08 09:57:32,506 iteration 1489 : loss : 0.028274, loss_ce: 0.011092
2022-01-08 09:57:33,917 iteration 1490 : loss : 0.070039, loss_ce: 0.030322
2022-01-08 09:57:35,351 iteration 1491 : loss : 0.051615, loss_ce: 0.024061
2022-01-08 09:57:36,675 iteration 1492 : loss : 0.034262, loss_ce: 0.013155
2022-01-08 09:57:38,090 iteration 1493 : loss : 0.056084, loss_ce: 0.024972
2022-01-08 09:57:39,437 iteration 1494 : loss : 0.037643, loss_ce: 0.016438
2022-01-08 09:57:40,784 iteration 1495 : loss : 0.041948, loss_ce: 0.018247
2022-01-08 09:57:42,157 iteration 1496 : loss : 0.084689, loss_ce: 0.023552
 22%|██████▌                       | 88/400 [37:12<2:08:49, 24.77s/it]2022-01-08 09:57:43,523 iteration 1497 : loss : 0.052553, loss_ce: 0.028899
2022-01-08 09:57:44,868 iteration 1498 : loss : 0.044436, loss_ce: 0.015151
2022-01-08 09:57:46,280 iteration 1499 : loss : 0.056588, loss_ce: 0.025080
2022-01-08 09:57:47,584 iteration 1500 : loss : 0.050610, loss_ce: 0.017414
2022-01-08 09:57:48,938 iteration 1501 : loss : 0.041225, loss_ce: 0.016619
2022-01-08 09:57:50,336 iteration 1502 : loss : 0.040000, loss_ce: 0.014694
2022-01-08 09:57:51,689 iteration 1503 : loss : 0.030802, loss_ce: 0.014259
2022-01-08 09:57:53,049 iteration 1504 : loss : 0.036667, loss_ce: 0.011790
2022-01-08 09:57:54,361 iteration 1505 : loss : 0.037681, loss_ce: 0.014057
2022-01-08 09:57:55,733 iteration 1506 : loss : 0.049902, loss_ce: 0.020949
2022-01-08 09:57:57,108 iteration 1507 : loss : 0.057108, loss_ce: 0.019465
2022-01-08 09:57:58,456 iteration 1508 : loss : 0.047164, loss_ce: 0.020292
2022-01-08 09:57:59,828 iteration 1509 : loss : 0.060971, loss_ce: 0.023701
2022-01-08 09:58:01,150 iteration 1510 : loss : 0.031719, loss_ce: 0.012505
2022-01-08 09:58:02,536 iteration 1511 : loss : 0.047383, loss_ce: 0.022413
2022-01-08 09:58:03,865 iteration 1512 : loss : 0.044051, loss_ce: 0.019479
2022-01-08 09:58:05,242 iteration 1513 : loss : 0.050364, loss_ce: 0.026102
 22%|██████▋                       | 89/400 [37:35<2:05:46, 24.27s/it]2022-01-08 09:58:06,610 iteration 1514 : loss : 0.066866, loss_ce: 0.024222
2022-01-08 09:58:07,975 iteration 1515 : loss : 0.071476, loss_ce: 0.029604
2022-01-08 09:58:09,374 iteration 1516 : loss : 0.044914, loss_ce: 0.016280
2022-01-08 09:58:10,736 iteration 1517 : loss : 0.048293, loss_ce: 0.021658
2022-01-08 09:58:12,128 iteration 1518 : loss : 0.042563, loss_ce: 0.012975
2022-01-08 09:58:13,466 iteration 1519 : loss : 0.038385, loss_ce: 0.014501
2022-01-08 09:58:14,799 iteration 1520 : loss : 0.042152, loss_ce: 0.013508
2022-01-08 09:58:16,149 iteration 1521 : loss : 0.068179, loss_ce: 0.025553
2022-01-08 09:58:17,616 iteration 1522 : loss : 0.046604, loss_ce: 0.017619
2022-01-08 09:58:18,917 iteration 1523 : loss : 0.053181, loss_ce: 0.025683
2022-01-08 09:58:20,305 iteration 1524 : loss : 0.065463, loss_ce: 0.027020
2022-01-08 09:58:21,649 iteration 1525 : loss : 0.039884, loss_ce: 0.017597
2022-01-08 09:58:23,091 iteration 1526 : loss : 0.054683, loss_ce: 0.020971
2022-01-08 09:58:24,525 iteration 1527 : loss : 0.048395, loss_ce: 0.021397
2022-01-08 09:58:25,926 iteration 1528 : loss : 0.051165, loss_ce: 0.022303
2022-01-08 09:58:27,261 iteration 1529 : loss : 0.050126, loss_ce: 0.020492
2022-01-08 09:58:27,262 Training Data Eval:
2022-01-08 09:58:34,167   Average segmentation loss on training set: 0.0327
2022-01-08 09:58:34,168 Validation Data Eval:
2022-01-08 09:58:36,535   Average segmentation loss on validation set: 0.0799
2022-01-08 09:58:40,627 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:58:41,896 iteration 1530 : loss : 0.055202, loss_ce: 0.025464
 22%|██████▊                       | 90/400 [38:12<2:24:35, 27.99s/it]2022-01-08 09:58:43,338 iteration 1531 : loss : 0.049155, loss_ce: 0.021016
2022-01-08 09:58:44,662 iteration 1532 : loss : 0.068070, loss_ce: 0.022419
2022-01-08 09:58:45,901 iteration 1533 : loss : 0.037754, loss_ce: 0.017624
2022-01-08 09:58:47,262 iteration 1534 : loss : 0.083733, loss_ce: 0.032009
2022-01-08 09:58:48,538 iteration 1535 : loss : 0.040705, loss_ce: 0.016013
2022-01-08 09:58:49,780 iteration 1536 : loss : 0.054951, loss_ce: 0.029929
2022-01-08 09:58:51,023 iteration 1537 : loss : 0.060470, loss_ce: 0.018504
2022-01-08 09:58:52,247 iteration 1538 : loss : 0.159077, loss_ce: 0.034401
2022-01-08 09:58:53,555 iteration 1539 : loss : 0.048149, loss_ce: 0.017672
2022-01-08 09:58:54,921 iteration 1540 : loss : 0.060766, loss_ce: 0.030493
2022-01-08 09:58:56,219 iteration 1541 : loss : 0.038077, loss_ce: 0.012736
2022-01-08 09:58:57,606 iteration 1542 : loss : 0.059450, loss_ce: 0.028062
2022-01-08 09:58:58,971 iteration 1543 : loss : 0.050993, loss_ce: 0.018529
2022-01-08 09:59:00,287 iteration 1544 : loss : 0.051298, loss_ce: 0.018872
2022-01-08 09:59:01,657 iteration 1545 : loss : 0.042921, loss_ce: 0.017119
2022-01-08 09:59:03,022 iteration 1546 : loss : 0.058767, loss_ce: 0.022877
2022-01-08 09:59:04,377 iteration 1547 : loss : 0.056708, loss_ce: 0.021025
 23%|██████▊                       | 91/400 [38:35<2:15:36, 26.33s/it]2022-01-08 09:59:05,784 iteration 1548 : loss : 0.038831, loss_ce: 0.014360
2022-01-08 09:59:07,097 iteration 1549 : loss : 0.039125, loss_ce: 0.016544
2022-01-08 09:59:08,562 iteration 1550 : loss : 0.059017, loss_ce: 0.027557
2022-01-08 09:59:09,920 iteration 1551 : loss : 0.054542, loss_ce: 0.023223
2022-01-08 09:59:11,288 iteration 1552 : loss : 0.052447, loss_ce: 0.021474
2022-01-08 09:59:12,582 iteration 1553 : loss : 0.033093, loss_ce: 0.018313
2022-01-08 09:59:13,917 iteration 1554 : loss : 0.057260, loss_ce: 0.023607
2022-01-08 09:59:15,341 iteration 1555 : loss : 0.043931, loss_ce: 0.017366
2022-01-08 09:59:16,825 iteration 1556 : loss : 0.057110, loss_ce: 0.022003
2022-01-08 09:59:18,236 iteration 1557 : loss : 0.072272, loss_ce: 0.020376
2022-01-08 09:59:19,599 iteration 1558 : loss : 0.077313, loss_ce: 0.023710
2022-01-08 09:59:21,067 iteration 1559 : loss : 0.054320, loss_ce: 0.024658
2022-01-08 09:59:22,412 iteration 1560 : loss : 0.053485, loss_ce: 0.018533
2022-01-08 09:59:23,758 iteration 1561 : loss : 0.055686, loss_ce: 0.020940
2022-01-08 09:59:25,144 iteration 1562 : loss : 0.042677, loss_ce: 0.018389
2022-01-08 09:59:26,502 iteration 1563 : loss : 0.063249, loss_ce: 0.034877
2022-01-08 09:59:27,872 iteration 1564 : loss : 0.047952, loss_ce: 0.016060
 23%|██████▉                       | 92/400 [38:58<2:10:48, 25.48s/it]2022-01-08 09:59:29,301 iteration 1565 : loss : 0.035269, loss_ce: 0.013866
2022-01-08 09:59:30,645 iteration 1566 : loss : 0.051242, loss_ce: 0.023825
2022-01-08 09:59:32,043 iteration 1567 : loss : 0.077513, loss_ce: 0.022054
2022-01-08 09:59:33,506 iteration 1568 : loss : 0.038720, loss_ce: 0.017954
2022-01-08 09:59:34,837 iteration 1569 : loss : 0.039104, loss_ce: 0.016322
2022-01-08 09:59:36,235 iteration 1570 : loss : 0.043654, loss_ce: 0.016011
2022-01-08 09:59:37,599 iteration 1571 : loss : 0.054210, loss_ce: 0.024586
2022-01-08 09:59:38,963 iteration 1572 : loss : 0.062087, loss_ce: 0.029989
2022-01-08 09:59:40,338 iteration 1573 : loss : 0.043785, loss_ce: 0.012028
2022-01-08 09:59:41,716 iteration 1574 : loss : 0.037983, loss_ce: 0.017804
2022-01-08 09:59:43,116 iteration 1575 : loss : 0.049561, loss_ce: 0.020385
2022-01-08 09:59:44,469 iteration 1576 : loss : 0.046766, loss_ce: 0.017231
2022-01-08 09:59:45,791 iteration 1577 : loss : 0.046393, loss_ce: 0.013888
2022-01-08 09:59:47,068 iteration 1578 : loss : 0.039187, loss_ce: 0.015186
2022-01-08 09:59:48,427 iteration 1579 : loss : 0.033822, loss_ce: 0.012915
2022-01-08 09:59:49,815 iteration 1580 : loss : 0.034934, loss_ce: 0.012266
2022-01-08 09:59:51,189 iteration 1581 : loss : 0.050627, loss_ce: 0.022396
 23%|██████▉                       | 93/400 [39:21<2:07:03, 24.83s/it]2022-01-08 09:59:52,623 iteration 1582 : loss : 0.034559, loss_ce: 0.015647
2022-01-08 09:59:53,984 iteration 1583 : loss : 0.050378, loss_ce: 0.017386
2022-01-08 09:59:55,306 iteration 1584 : loss : 0.037386, loss_ce: 0.013020
2022-01-08 09:59:56,658 iteration 1585 : loss : 0.032870, loss_ce: 0.015134
2022-01-08 09:59:57,951 iteration 1586 : loss : 0.033476, loss_ce: 0.014968
2022-01-08 09:59:59,299 iteration 1587 : loss : 0.049209, loss_ce: 0.019848
2022-01-08 10:00:00,643 iteration 1588 : loss : 0.046956, loss_ce: 0.014547
2022-01-08 10:00:02,007 iteration 1589 : loss : 0.047045, loss_ce: 0.021372
2022-01-08 10:00:03,450 iteration 1590 : loss : 0.048121, loss_ce: 0.018428
2022-01-08 10:00:04,784 iteration 1591 : loss : 0.040412, loss_ce: 0.020243
2022-01-08 10:00:06,066 iteration 1592 : loss : 0.030901, loss_ce: 0.013445
2022-01-08 10:00:07,445 iteration 1593 : loss : 0.049542, loss_ce: 0.018573
2022-01-08 10:00:08,797 iteration 1594 : loss : 0.055462, loss_ce: 0.020134
2022-01-08 10:00:10,105 iteration 1595 : loss : 0.038800, loss_ce: 0.020559
2022-01-08 10:00:11,388 iteration 1596 : loss : 0.063881, loss_ce: 0.025176
2022-01-08 10:00:12,762 iteration 1597 : loss : 0.071680, loss_ce: 0.016651
2022-01-08 10:00:14,153 iteration 1598 : loss : 0.059686, loss_ce: 0.020770
 24%|███████                       | 94/400 [39:44<2:03:47, 24.27s/it]2022-01-08 10:00:15,588 iteration 1599 : loss : 0.043502, loss_ce: 0.021691
2022-01-08 10:00:16,934 iteration 1600 : loss : 0.031998, loss_ce: 0.015286
2022-01-08 10:00:18,331 iteration 1601 : loss : 0.052571, loss_ce: 0.022957
2022-01-08 10:00:19,711 iteration 1602 : loss : 0.076932, loss_ce: 0.023690
2022-01-08 10:00:21,071 iteration 1603 : loss : 0.038537, loss_ce: 0.015259
2022-01-08 10:00:22,420 iteration 1604 : loss : 0.053063, loss_ce: 0.012266
2022-01-08 10:00:23,779 iteration 1605 : loss : 0.053836, loss_ce: 0.020991
2022-01-08 10:00:25,170 iteration 1606 : loss : 0.032756, loss_ce: 0.012824
2022-01-08 10:00:26,548 iteration 1607 : loss : 0.051845, loss_ce: 0.020281
2022-01-08 10:00:27,979 iteration 1608 : loss : 0.079140, loss_ce: 0.035502
2022-01-08 10:00:29,308 iteration 1609 : loss : 0.042402, loss_ce: 0.015141
2022-01-08 10:00:30,751 iteration 1610 : loss : 0.050484, loss_ce: 0.020142
2022-01-08 10:00:32,198 iteration 1611 : loss : 0.049228, loss_ce: 0.025002
2022-01-08 10:00:33,652 iteration 1612 : loss : 0.037647, loss_ce: 0.014531
2022-01-08 10:00:35,018 iteration 1613 : loss : 0.049549, loss_ce: 0.021782
2022-01-08 10:00:36,447 iteration 1614 : loss : 0.055215, loss_ce: 0.024426
2022-01-08 10:00:36,447 Training Data Eval:
2022-01-08 10:00:43,301   Average segmentation loss on training set: 0.0372
2022-01-08 10:00:43,301 Validation Data Eval:
2022-01-08 10:00:45,671   Average segmentation loss on validation set: 0.0668
2022-01-08 10:00:49,787 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 10:00:51,122 iteration 1615 : loss : 0.072078, loss_ce: 0.023199
 24%|███████▏                      | 95/400 [40:21<2:22:44, 28.08s/it]2022-01-08 10:00:52,439 iteration 1616 : loss : 0.046215, loss_ce: 0.018585
2022-01-08 10:00:53,701 iteration 1617 : loss : 0.046539, loss_ce: 0.021150
2022-01-08 10:00:54,923 iteration 1618 : loss : 0.044304, loss_ce: 0.014931
2022-01-08 10:00:56,189 iteration 1619 : loss : 0.053937, loss_ce: 0.032442
2022-01-08 10:00:57,589 iteration 1620 : loss : 0.039931, loss_ce: 0.013241
2022-01-08 10:00:58,987 iteration 1621 : loss : 0.045424, loss_ce: 0.016832
2022-01-08 10:01:00,272 iteration 1622 : loss : 0.053004, loss_ce: 0.022783
2022-01-08 10:01:01,516 iteration 1623 : loss : 0.031530, loss_ce: 0.010884
2022-01-08 10:01:02,880 iteration 1624 : loss : 0.049973, loss_ce: 0.022457
2022-01-08 10:01:04,194 iteration 1625 : loss : 0.035958, loss_ce: 0.013168
2022-01-08 10:01:05,555 iteration 1626 : loss : 0.037144, loss_ce: 0.016281
2022-01-08 10:01:07,044 iteration 1627 : loss : 0.062069, loss_ce: 0.024797
2022-01-08 10:01:08,499 iteration 1628 : loss : 0.045512, loss_ce: 0.018396
2022-01-08 10:01:09,850 iteration 1629 : loss : 0.047533, loss_ce: 0.016534
2022-01-08 10:01:11,188 iteration 1630 : loss : 0.027649, loss_ce: 0.011530
2022-01-08 10:01:12,526 iteration 1631 : loss : 0.051308, loss_ce: 0.016713
2022-01-08 10:01:13,882 iteration 1632 : loss : 0.035191, loss_ce: 0.015587
 24%|███████▏                      | 96/400 [40:44<2:14:11, 26.48s/it]2022-01-08 10:01:15,304 iteration 1633 : loss : 0.049061, loss_ce: 0.016329
2022-01-08 10:01:16,691 iteration 1634 : loss : 0.063798, loss_ce: 0.025892
2022-01-08 10:01:18,038 iteration 1635 : loss : 0.048643, loss_ce: 0.015640
2022-01-08 10:01:19,337 iteration 1636 : loss : 0.035447, loss_ce: 0.017359
2022-01-08 10:01:20,718 iteration 1637 : loss : 0.042759, loss_ce: 0.014941
2022-01-08 10:01:22,079 iteration 1638 : loss : 0.040180, loss_ce: 0.017654
2022-01-08 10:01:23,485 iteration 1639 : loss : 0.045132, loss_ce: 0.021416
2022-01-08 10:01:24,817 iteration 1640 : loss : 0.051523, loss_ce: 0.017926
2022-01-08 10:01:26,262 iteration 1641 : loss : 0.046837, loss_ce: 0.017583
2022-01-08 10:01:27,582 iteration 1642 : loss : 0.037373, loss_ce: 0.014011
2022-01-08 10:01:28,880 iteration 1643 : loss : 0.041137, loss_ce: 0.014559
2022-01-08 10:01:30,308 iteration 1644 : loss : 0.049587, loss_ce: 0.020432
2022-01-08 10:01:31,765 iteration 1645 : loss : 0.045850, loss_ce: 0.019458
2022-01-08 10:01:33,104 iteration 1646 : loss : 0.042736, loss_ce: 0.015788
2022-01-08 10:01:34,456 iteration 1647 : loss : 0.072322, loss_ce: 0.029944
2022-01-08 10:01:35,748 iteration 1648 : loss : 0.037084, loss_ce: 0.016397
2022-01-08 10:01:37,051 iteration 1649 : loss : 0.032811, loss_ce: 0.012754
 24%|███████▎                      | 97/400 [41:07<2:08:43, 25.49s/it]2022-01-08 10:01:38,564 iteration 1650 : loss : 0.050241, loss_ce: 0.025455
2022-01-08 10:01:39,909 iteration 1651 : loss : 0.057363, loss_ce: 0.017809
2022-01-08 10:01:41,279 iteration 1652 : loss : 0.053724, loss_ce: 0.020858
2022-01-08 10:01:42,616 iteration 1653 : loss : 0.041909, loss_ce: 0.014612
2022-01-08 10:01:43,961 iteration 1654 : loss : 0.047166, loss_ce: 0.015259
2022-01-08 10:01:45,276 iteration 1655 : loss : 0.048173, loss_ce: 0.016879
2022-01-08 10:01:46,681 iteration 1656 : loss : 0.054907, loss_ce: 0.022590
2022-01-08 10:01:48,033 iteration 1657 : loss : 0.036790, loss_ce: 0.015981
2022-01-08 10:01:49,442 iteration 1658 : loss : 0.065396, loss_ce: 0.039209
2022-01-08 10:01:50,877 iteration 1659 : loss : 0.047375, loss_ce: 0.023264
2022-01-08 10:01:52,234 iteration 1660 : loss : 0.029850, loss_ce: 0.011998
2022-01-08 10:01:53,534 iteration 1661 : loss : 0.043463, loss_ce: 0.016119
2022-01-08 10:01:54,939 iteration 1662 : loss : 0.042525, loss_ce: 0.015471
2022-01-08 10:01:56,365 iteration 1663 : loss : 0.044339, loss_ce: 0.021931
2022-01-08 10:01:57,741 iteration 1664 : loss : 0.076466, loss_ce: 0.027447
2022-01-08 10:01:59,098 iteration 1665 : loss : 0.048051, loss_ce: 0.025032
2022-01-08 10:02:00,478 iteration 1666 : loss : 0.048682, loss_ce: 0.017224
 24%|███████▎                      | 98/400 [41:31<2:05:10, 24.87s/it]2022-01-08 10:02:01,842 iteration 1667 : loss : 0.037234, loss_ce: 0.019466
2022-01-08 10:02:03,294 iteration 1668 : loss : 0.063029, loss_ce: 0.022032
2022-01-08 10:02:04,694 iteration 1669 : loss : 0.048265, loss_ce: 0.018394
2022-01-08 10:02:05,986 iteration 1670 : loss : 0.029009, loss_ce: 0.009789
2022-01-08 10:02:07,301 iteration 1671 : loss : 0.037910, loss_ce: 0.016835
2022-01-08 10:02:08,727 iteration 1672 : loss : 0.061942, loss_ce: 0.028368
2022-01-08 10:02:10,123 iteration 1673 : loss : 0.044271, loss_ce: 0.014875
2022-01-08 10:02:11,582 iteration 1674 : loss : 0.060943, loss_ce: 0.020653
2022-01-08 10:02:12,925 iteration 1675 : loss : 0.061751, loss_ce: 0.019472
2022-01-08 10:02:14,343 iteration 1676 : loss : 0.043560, loss_ce: 0.018490
2022-01-08 10:02:15,731 iteration 1677 : loss : 0.097542, loss_ce: 0.034330
2022-01-08 10:02:17,137 iteration 1678 : loss : 0.046377, loss_ce: 0.020159
2022-01-08 10:02:18,481 iteration 1679 : loss : 0.081134, loss_ce: 0.044589
2022-01-08 10:02:19,863 iteration 1680 : loss : 0.042996, loss_ce: 0.020774
2022-01-08 10:02:21,209 iteration 1681 : loss : 0.063427, loss_ce: 0.025527
2022-01-08 10:02:22,573 iteration 1682 : loss : 0.047975, loss_ce: 0.020198
2022-01-08 10:02:23,906 iteration 1683 : loss : 0.043502, loss_ce: 0.019840
 25%|███████▍                      | 99/400 [41:54<2:02:36, 24.44s/it]2022-01-08 10:02:25,340 iteration 1684 : loss : 0.056618, loss_ce: 0.025902
2022-01-08 10:02:26,666 iteration 1685 : loss : 0.043145, loss_ce: 0.021100
2022-01-08 10:02:27,969 iteration 1686 : loss : 0.045255, loss_ce: 0.015693
2022-01-08 10:02:29,320 iteration 1687 : loss : 0.045624, loss_ce: 0.018141
2022-01-08 10:02:30,707 iteration 1688 : loss : 0.056294, loss_ce: 0.027110
2022-01-08 10:02:32,032 iteration 1689 : loss : 0.068239, loss_ce: 0.026253
2022-01-08 10:02:33,412 iteration 1690 : loss : 0.093388, loss_ce: 0.035552
2022-01-08 10:02:34,756 iteration 1691 : loss : 0.039610, loss_ce: 0.015453
2022-01-08 10:02:36,168 iteration 1692 : loss : 0.067206, loss_ce: 0.024489
2022-01-08 10:02:37,598 iteration 1693 : loss : 0.058289, loss_ce: 0.030277
2022-01-08 10:02:38,987 iteration 1694 : loss : 0.052749, loss_ce: 0.020385
2022-01-08 10:02:40,279 iteration 1695 : loss : 0.049429, loss_ce: 0.017065
2022-01-08 10:02:41,632 iteration 1696 : loss : 0.083261, loss_ce: 0.028118
2022-01-08 10:02:43,040 iteration 1697 : loss : 0.056230, loss_ce: 0.018855
2022-01-08 10:02:44,382 iteration 1698 : loss : 0.039990, loss_ce: 0.012588
2022-01-08 10:02:45,641 iteration 1699 : loss : 0.053174, loss_ce: 0.027731
2022-01-08 10:02:45,642 Training Data Eval:
2022-01-08 10:02:52,533   Average segmentation loss on training set: 0.0311
2022-01-08 10:02:52,534 Validation Data Eval:
2022-01-08 10:02:54,906   Average segmentation loss on validation set: 0.0944
2022-01-08 10:02:56,300 iteration 1700 : loss : 0.044400, loss_ce: 0.012667
 25%|███████▎                     | 100/400 [42:26<2:14:07, 26.82s/it]2022-01-08 10:02:57,742 iteration 1701 : loss : 0.048933, loss_ce: 0.021533
2022-01-08 10:02:59,114 iteration 1702 : loss : 0.046122, loss_ce: 0.020375
2022-01-08 10:03:00,431 iteration 1703 : loss : 0.042756, loss_ce: 0.018637
2022-01-08 10:03:01,875 iteration 1704 : loss : 0.040233, loss_ce: 0.017198
2022-01-08 10:03:03,412 iteration 1705 : loss : 0.041547, loss_ce: 0.019165
2022-01-08 10:03:04,764 iteration 1706 : loss : 0.040692, loss_ce: 0.018746
2022-01-08 10:03:06,118 iteration 1707 : loss : 0.048526, loss_ce: 0.016248
2022-01-08 10:03:07,519 iteration 1708 : loss : 0.048862, loss_ce: 0.015501
2022-01-08 10:03:08,823 iteration 1709 : loss : 0.034112, loss_ce: 0.015327
2022-01-08 10:03:10,109 iteration 1710 : loss : 0.044483, loss_ce: 0.017225
2022-01-08 10:03:11,484 iteration 1711 : loss : 0.044277, loss_ce: 0.016188
2022-01-08 10:03:12,808 iteration 1712 : loss : 0.029209, loss_ce: 0.010129
2022-01-08 10:03:14,128 iteration 1713 : loss : 0.034970, loss_ce: 0.012346
2022-01-08 10:03:15,574 iteration 1714 : loss : 0.061345, loss_ce: 0.020096
2022-01-08 10:03:16,966 iteration 1715 : loss : 0.062826, loss_ce: 0.018824
2022-01-08 10:03:18,400 iteration 1716 : loss : 0.065482, loss_ce: 0.021923
2022-01-08 10:03:19,759 iteration 1717 : loss : 0.048076, loss_ce: 0.019831
 25%|███████▎                     | 101/400 [42:50<2:08:39, 25.82s/it]2022-01-08 10:03:21,137 iteration 1718 : loss : 0.048792, loss_ce: 0.016216
2022-01-08 10:03:22,523 iteration 1719 : loss : 0.046617, loss_ce: 0.021517
2022-01-08 10:03:23,799 iteration 1720 : loss : 0.031267, loss_ce: 0.009602
2022-01-08 10:03:25,153 iteration 1721 : loss : 0.041696, loss_ce: 0.018322
2022-01-08 10:03:26,558 iteration 1722 : loss : 0.056353, loss_ce: 0.017968
2022-01-08 10:03:27,912 iteration 1723 : loss : 0.047519, loss_ce: 0.020088
2022-01-08 10:03:29,310 iteration 1724 : loss : 0.043086, loss_ce: 0.013799
2022-01-08 10:03:30,778 iteration 1725 : loss : 0.072579, loss_ce: 0.025855
2022-01-08 10:03:32,198 iteration 1726 : loss : 0.067929, loss_ce: 0.017998
2022-01-08 10:03:33,566 iteration 1727 : loss : 0.039222, loss_ce: 0.020845
2022-01-08 10:03:34,911 iteration 1728 : loss : 0.036094, loss_ce: 0.012707
2022-01-08 10:03:36,276 iteration 1729 : loss : 0.035108, loss_ce: 0.015225
2022-01-08 10:03:37,654 iteration 1730 : loss : 0.043651, loss_ce: 0.016168
2022-01-08 10:03:39,103 iteration 1731 : loss : 0.063554, loss_ce: 0.021886
2022-01-08 10:03:40,487 iteration 1732 : loss : 0.031429, loss_ce: 0.015475
2022-01-08 10:03:41,917 iteration 1733 : loss : 0.050840, loss_ce: 0.020636
2022-01-08 10:03:43,231 iteration 1734 : loss : 0.034977, loss_ce: 0.012744
 26%|███████▍                     | 102/400 [43:13<2:04:43, 25.11s/it]2022-01-08 10:03:44,599 iteration 1735 : loss : 0.046310, loss_ce: 0.017629
2022-01-08 10:03:45,947 iteration 1736 : loss : 0.042010, loss_ce: 0.022187
2022-01-08 10:03:47,315 iteration 1737 : loss : 0.039551, loss_ce: 0.015217
2022-01-08 10:03:48,661 iteration 1738 : loss : 0.038081, loss_ce: 0.014010
2022-01-08 10:03:50,120 iteration 1739 : loss : 0.052627, loss_ce: 0.024411
2022-01-08 10:03:51,513 iteration 1740 : loss : 0.070211, loss_ce: 0.024631
2022-01-08 10:03:52,871 iteration 1741 : loss : 0.034828, loss_ce: 0.012006
2022-01-08 10:03:54,335 iteration 1742 : loss : 0.050458, loss_ce: 0.023665
2022-01-08 10:03:55,685 iteration 1743 : loss : 0.045696, loss_ce: 0.013356
2022-01-08 10:03:57,081 iteration 1744 : loss : 0.041027, loss_ce: 0.017583
2022-01-08 10:03:58,587 iteration 1745 : loss : 0.050607, loss_ce: 0.026887
2022-01-08 10:03:59,939 iteration 1746 : loss : 0.032351, loss_ce: 0.012791
2022-01-08 10:04:01,284 iteration 1747 : loss : 0.035509, loss_ce: 0.015890
2022-01-08 10:04:02,655 iteration 1748 : loss : 0.052666, loss_ce: 0.020007
2022-01-08 10:04:04,013 iteration 1749 : loss : 0.048814, loss_ce: 0.018665
2022-01-08 10:04:05,333 iteration 1750 : loss : 0.035752, loss_ce: 0.015098
2022-01-08 10:04:06,784 iteration 1751 : loss : 0.057325, loss_ce: 0.020840
 26%|███████▍                     | 103/400 [43:37<2:01:58, 24.64s/it]2022-01-08 10:04:08,158 iteration 1752 : loss : 0.037165, loss_ce: 0.013140
2022-01-08 10:04:09,472 iteration 1753 : loss : 0.038357, loss_ce: 0.018887
2022-01-08 10:04:10,815 iteration 1754 : loss : 0.032889, loss_ce: 0.013616
2022-01-08 10:04:12,164 iteration 1755 : loss : 0.042868, loss_ce: 0.015189
2022-01-08 10:04:13,554 iteration 1756 : loss : 0.050854, loss_ce: 0.021804
2022-01-08 10:04:14,999 iteration 1757 : loss : 0.033361, loss_ce: 0.015927
2022-01-08 10:04:16,407 iteration 1758 : loss : 0.060021, loss_ce: 0.021434
2022-01-08 10:04:17,702 iteration 1759 : loss : 0.099348, loss_ce: 0.034614
2022-01-08 10:04:19,052 iteration 1760 : loss : 0.036727, loss_ce: 0.015329
2022-01-08 10:04:20,373 iteration 1761 : loss : 0.027146, loss_ce: 0.010703
2022-01-08 10:04:21,731 iteration 1762 : loss : 0.038780, loss_ce: 0.015171
2022-01-08 10:04:23,019 iteration 1763 : loss : 0.046410, loss_ce: 0.020632
2022-01-08 10:04:24,477 iteration 1764 : loss : 0.036384, loss_ce: 0.010888
2022-01-08 10:04:25,849 iteration 1765 : loss : 0.039523, loss_ce: 0.012976
2022-01-08 10:04:27,288 iteration 1766 : loss : 0.039774, loss_ce: 0.014940
2022-01-08 10:04:28,685 iteration 1767 : loss : 0.040749, loss_ce: 0.014641
2022-01-08 10:04:30,039 iteration 1768 : loss : 0.052082, loss_ce: 0.018435
 26%|███████▌                     | 104/400 [44:00<1:59:31, 24.23s/it]2022-01-08 10:04:31,478 iteration 1769 : loss : 0.050257, loss_ce: 0.019889
2022-01-08 10:04:32,831 iteration 1770 : loss : 0.042085, loss_ce: 0.014771
2022-01-08 10:04:34,155 iteration 1771 : loss : 0.026249, loss_ce: 0.008343
2022-01-08 10:04:35,574 iteration 1772 : loss : 0.050916, loss_ce: 0.025567
2022-01-08 10:04:36,964 iteration 1773 : loss : 0.045037, loss_ce: 0.020131
2022-01-08 10:04:38,307 iteration 1774 : loss : 0.038655, loss_ce: 0.012203
2022-01-08 10:04:39,657 iteration 1775 : loss : 0.035003, loss_ce: 0.013384
2022-01-08 10:04:41,002 iteration 1776 : loss : 0.042854, loss_ce: 0.015377
2022-01-08 10:04:42,324 iteration 1777 : loss : 0.032685, loss_ce: 0.012703
2022-01-08 10:04:43,735 iteration 1778 : loss : 0.044168, loss_ce: 0.021198
2022-01-08 10:04:45,096 iteration 1779 : loss : 0.045153, loss_ce: 0.019861
2022-01-08 10:04:46,489 iteration 1780 : loss : 0.064785, loss_ce: 0.017923
2022-01-08 10:04:47,878 iteration 1781 : loss : 0.055109, loss_ce: 0.020550
2022-01-08 10:04:49,177 iteration 1782 : loss : 0.053423, loss_ce: 0.016171
2022-01-08 10:04:50,509 iteration 1783 : loss : 0.040363, loss_ce: 0.017141
2022-01-08 10:04:51,898 iteration 1784 : loss : 0.046389, loss_ce: 0.023700
2022-01-08 10:04:51,899 Training Data Eval:
2022-01-08 10:04:58,787   Average segmentation loss on training set: 0.0346
2022-01-08 10:04:58,787 Validation Data Eval:
2022-01-08 10:05:01,153   Average segmentation loss on validation set: 0.1056
2022-01-08 10:05:02,531 iteration 1785 : loss : 0.039675, loss_ce: 0.018318
 26%|███████▌                     | 105/400 [44:33<2:11:18, 26.71s/it]2022-01-08 10:05:03,977 iteration 1786 : loss : 0.053989, loss_ce: 0.017516
2022-01-08 10:05:05,282 iteration 1787 : loss : 0.078456, loss_ce: 0.019058
2022-01-08 10:05:06,655 iteration 1788 : loss : 0.039173, loss_ce: 0.015262
2022-01-08 10:05:08,039 iteration 1789 : loss : 0.050295, loss_ce: 0.024738
2022-01-08 10:05:09,547 iteration 1790 : loss : 0.035147, loss_ce: 0.014636
2022-01-08 10:05:10,921 iteration 1791 : loss : 0.038487, loss_ce: 0.016348
2022-01-08 10:05:12,304 iteration 1792 : loss : 0.035761, loss_ce: 0.014410
2022-01-08 10:05:13,704 iteration 1793 : loss : 0.088781, loss_ce: 0.041792
2022-01-08 10:05:15,060 iteration 1794 : loss : 0.052697, loss_ce: 0.031167
2022-01-08 10:05:16,338 iteration 1795 : loss : 0.033790, loss_ce: 0.015061
2022-01-08 10:05:17,802 iteration 1796 : loss : 0.039587, loss_ce: 0.018455
2022-01-08 10:05:19,103 iteration 1797 : loss : 0.041840, loss_ce: 0.019968
2022-01-08 10:05:20,479 iteration 1798 : loss : 0.056292, loss_ce: 0.023481
2022-01-08 10:05:21,911 iteration 1799 : loss : 0.050546, loss_ce: 0.020971
2022-01-08 10:05:23,216 iteration 1800 : loss : 0.089093, loss_ce: 0.036005
2022-01-08 10:05:24,587 iteration 1801 : loss : 0.059118, loss_ce: 0.023118
2022-01-08 10:05:26,004 iteration 1802 : loss : 0.042096, loss_ce: 0.016176
 26%|███████▋                     | 106/400 [44:56<2:06:05, 25.73s/it]2022-01-08 10:05:27,447 iteration 1803 : loss : 0.051861, loss_ce: 0.020941
2022-01-08 10:05:28,746 iteration 1804 : loss : 0.043237, loss_ce: 0.015596
2022-01-08 10:05:30,232 iteration 1805 : loss : 0.053170, loss_ce: 0.018044
2022-01-08 10:05:31,675 iteration 1806 : loss : 0.061397, loss_ce: 0.028642
2022-01-08 10:05:33,022 iteration 1807 : loss : 0.050962, loss_ce: 0.020000
2022-01-08 10:05:34,423 iteration 1808 : loss : 0.037795, loss_ce: 0.019163
2022-01-08 10:05:35,775 iteration 1809 : loss : 0.044311, loss_ce: 0.020343
2022-01-08 10:05:37,177 iteration 1810 : loss : 0.077421, loss_ce: 0.043867
2022-01-08 10:05:38,531 iteration 1811 : loss : 0.055683, loss_ce: 0.022933
2022-01-08 10:05:39,875 iteration 1812 : loss : 0.051092, loss_ce: 0.017079
2022-01-08 10:05:41,246 iteration 1813 : loss : 0.059488, loss_ce: 0.029956
2022-01-08 10:05:42,591 iteration 1814 : loss : 0.047119, loss_ce: 0.017326
2022-01-08 10:05:44,013 iteration 1815 : loss : 0.058088, loss_ce: 0.020033
2022-01-08 10:05:45,382 iteration 1816 : loss : 0.036432, loss_ce: 0.016547
2022-01-08 10:05:46,814 iteration 1817 : loss : 0.048256, loss_ce: 0.020053
2022-01-08 10:05:48,127 iteration 1818 : loss : 0.036203, loss_ce: 0.013516
2022-01-08 10:05:49,548 iteration 1819 : loss : 0.076270, loss_ce: 0.029517
 27%|███████▊                     | 107/400 [45:20<2:02:28, 25.08s/it]2022-01-08 10:05:50,852 iteration 1820 : loss : 0.046804, loss_ce: 0.022885
2022-01-08 10:05:52,324 iteration 1821 : loss : 0.063196, loss_ce: 0.025025
2022-01-08 10:05:53,726 iteration 1822 : loss : 0.045432, loss_ce: 0.020281
2022-01-08 10:05:55,163 iteration 1823 : loss : 0.075823, loss_ce: 0.030859
2022-01-08 10:05:56,567 iteration 1824 : loss : 0.103149, loss_ce: 0.031975
2022-01-08 10:05:57,906 iteration 1825 : loss : 0.038650, loss_ce: 0.016054
2022-01-08 10:05:59,349 iteration 1826 : loss : 0.035291, loss_ce: 0.010660
2022-01-08 10:06:00,671 iteration 1827 : loss : 0.031977, loss_ce: 0.011597
2022-01-08 10:06:02,050 iteration 1828 : loss : 0.067139, loss_ce: 0.028275
2022-01-08 10:06:03,378 iteration 1829 : loss : 0.051236, loss_ce: 0.026101
2022-01-08 10:06:04,841 iteration 1830 : loss : 0.047095, loss_ce: 0.017845
2022-01-08 10:06:06,231 iteration 1831 : loss : 0.059070, loss_ce: 0.024284
2022-01-08 10:06:07,579 iteration 1832 : loss : 0.061647, loss_ce: 0.025991
2022-01-08 10:06:08,892 iteration 1833 : loss : 0.046134, loss_ce: 0.023114
2022-01-08 10:06:10,259 iteration 1834 : loss : 0.056878, loss_ce: 0.022609
2022-01-08 10:06:11,635 iteration 1835 : loss : 0.038043, loss_ce: 0.015087
2022-01-08 10:06:12,974 iteration 1836 : loss : 0.069634, loss_ce: 0.023109
 27%|███████▊                     | 108/400 [45:43<1:59:38, 24.58s/it]2022-01-08 10:06:14,424 iteration 1837 : loss : 0.050273, loss_ce: 0.018376
2022-01-08 10:06:15,717 iteration 1838 : loss : 0.050064, loss_ce: 0.020408
2022-01-08 10:06:17,050 iteration 1839 : loss : 0.059334, loss_ce: 0.024084
2022-01-08 10:06:18,440 iteration 1840 : loss : 0.044335, loss_ce: 0.017861
2022-01-08 10:06:19,750 iteration 1841 : loss : 0.046549, loss_ce: 0.018799
2022-01-08 10:06:21,105 iteration 1842 : loss : 0.032321, loss_ce: 0.012076
2022-01-08 10:06:22,489 iteration 1843 : loss : 0.046533, loss_ce: 0.023928
2022-01-08 10:06:23,836 iteration 1844 : loss : 0.050579, loss_ce: 0.016612
2022-01-08 10:06:25,186 iteration 1845 : loss : 0.036960, loss_ce: 0.011670
2022-01-08 10:06:26,627 iteration 1846 : loss : 0.053013, loss_ce: 0.025921
2022-01-08 10:06:27,951 iteration 1847 : loss : 0.048971, loss_ce: 0.019042
2022-01-08 10:06:29,314 iteration 1848 : loss : 0.048594, loss_ce: 0.020886
2022-01-08 10:06:30,651 iteration 1849 : loss : 0.060222, loss_ce: 0.027810
2022-01-08 10:06:32,136 iteration 1850 : loss : 0.070947, loss_ce: 0.032607
2022-01-08 10:06:33,491 iteration 1851 : loss : 0.073002, loss_ce: 0.023795
2022-01-08 10:06:34,867 iteration 1852 : loss : 0.048307, loss_ce: 0.021548
2022-01-08 10:06:36,201 iteration 1853 : loss : 0.049142, loss_ce: 0.020670
 27%|███████▉                     | 109/400 [46:06<1:57:15, 24.18s/it]2022-01-08 10:06:37,571 iteration 1854 : loss : 0.042396, loss_ce: 0.015650
2022-01-08 10:06:38,980 iteration 1855 : loss : 0.042567, loss_ce: 0.016621
2022-01-08 10:06:40,398 iteration 1856 : loss : 0.048486, loss_ce: 0.018800
2022-01-08 10:06:41,821 iteration 1857 : loss : 0.029734, loss_ce: 0.012261
2022-01-08 10:06:43,211 iteration 1858 : loss : 0.035961, loss_ce: 0.013980
2022-01-08 10:06:44,591 iteration 1859 : loss : 0.039916, loss_ce: 0.016000
2022-01-08 10:06:45,921 iteration 1860 : loss : 0.037201, loss_ce: 0.013554
2022-01-08 10:06:47,350 iteration 1861 : loss : 0.038947, loss_ce: 0.014274
2022-01-08 10:06:48,662 iteration 1862 : loss : 0.034366, loss_ce: 0.016338
2022-01-08 10:06:50,033 iteration 1863 : loss : 0.071570, loss_ce: 0.038213
2022-01-08 10:06:51,443 iteration 1864 : loss : 0.064198, loss_ce: 0.017079
2022-01-08 10:06:52,727 iteration 1865 : loss : 0.033874, loss_ce: 0.015299
2022-01-08 10:06:54,182 iteration 1866 : loss : 0.029176, loss_ce: 0.013390
2022-01-08 10:06:55,550 iteration 1867 : loss : 0.032196, loss_ce: 0.014619
2022-01-08 10:06:56,888 iteration 1868 : loss : 0.055685, loss_ce: 0.025532
2022-01-08 10:06:58,221 iteration 1869 : loss : 0.084477, loss_ce: 0.031873
2022-01-08 10:06:58,221 Training Data Eval:
2022-01-08 10:07:05,095   Average segmentation loss on training set: 0.0374
2022-01-08 10:07:05,096 Validation Data Eval:
2022-01-08 10:07:07,470   Average segmentation loss on validation set: 0.0816
2022-01-08 10:07:08,882 iteration 1870 : loss : 0.047100, loss_ce: 0.018257
 28%|███████▉                     | 110/400 [46:39<2:09:11, 26.73s/it]2022-01-08 10:07:10,342 iteration 1871 : loss : 0.036702, loss_ce: 0.014126
2022-01-08 10:07:11,813 iteration 1872 : loss : 0.072995, loss_ce: 0.021686
2022-01-08 10:07:13,135 iteration 1873 : loss : 0.044980, loss_ce: 0.016442
2022-01-08 10:07:14,577 iteration 1874 : loss : 0.083600, loss_ce: 0.052605
2022-01-08 10:07:15,863 iteration 1875 : loss : 0.036965, loss_ce: 0.012386
2022-01-08 10:07:17,304 iteration 1876 : loss : 0.031430, loss_ce: 0.008951
2022-01-08 10:07:18,707 iteration 1877 : loss : 0.066465, loss_ce: 0.027155
2022-01-08 10:07:20,043 iteration 1878 : loss : 0.038352, loss_ce: 0.017054
2022-01-08 10:07:21,391 iteration 1879 : loss : 0.035942, loss_ce: 0.013901
2022-01-08 10:07:22,741 iteration 1880 : loss : 0.044007, loss_ce: 0.021428
2022-01-08 10:07:24,110 iteration 1881 : loss : 0.049146, loss_ce: 0.018300
2022-01-08 10:07:25,438 iteration 1882 : loss : 0.040009, loss_ce: 0.013637
2022-01-08 10:07:26,728 iteration 1883 : loss : 0.042220, loss_ce: 0.014499
2022-01-08 10:07:28,124 iteration 1884 : loss : 0.070484, loss_ce: 0.026219
2022-01-08 10:07:29,392 iteration 1885 : loss : 0.033698, loss_ce: 0.014310
2022-01-08 10:07:30,832 iteration 1886 : loss : 0.033788, loss_ce: 0.014668
2022-01-08 10:07:32,261 iteration 1887 : loss : 0.064438, loss_ce: 0.029282
 28%|████████                     | 111/400 [47:02<2:03:53, 25.72s/it]2022-01-08 10:07:33,649 iteration 1888 : loss : 0.039563, loss_ce: 0.017796
2022-01-08 10:07:35,069 iteration 1889 : loss : 0.055066, loss_ce: 0.024406
2022-01-08 10:07:36,470 iteration 1890 : loss : 0.040983, loss_ce: 0.013087
2022-01-08 10:07:37,881 iteration 1891 : loss : 0.061153, loss_ce: 0.022004
2022-01-08 10:07:39,262 iteration 1892 : loss : 0.034923, loss_ce: 0.014843
2022-01-08 10:07:40,631 iteration 1893 : loss : 0.045451, loss_ce: 0.023279
2022-01-08 10:07:42,006 iteration 1894 : loss : 0.047374, loss_ce: 0.019450
2022-01-08 10:07:43,375 iteration 1895 : loss : 0.047119, loss_ce: 0.016762
2022-01-08 10:07:44,867 iteration 1896 : loss : 0.052365, loss_ce: 0.025661
2022-01-08 10:07:46,215 iteration 1897 : loss : 0.047193, loss_ce: 0.014329
2022-01-08 10:07:47,646 iteration 1898 : loss : 0.040005, loss_ce: 0.015217
2022-01-08 10:07:49,011 iteration 1899 : loss : 0.044399, loss_ce: 0.021601
2022-01-08 10:07:50,298 iteration 1900 : loss : 0.029773, loss_ce: 0.012058
2022-01-08 10:07:51,653 iteration 1901 : loss : 0.035685, loss_ce: 0.012725
2022-01-08 10:07:53,043 iteration 1902 : loss : 0.046423, loss_ce: 0.021989
2022-01-08 10:07:54,467 iteration 1903 : loss : 0.060453, loss_ce: 0.018937
2022-01-08 10:07:55,932 iteration 1904 : loss : 0.052810, loss_ce: 0.018210
 28%|████████                     | 112/400 [47:26<2:00:30, 25.11s/it]2022-01-08 10:07:57,281 iteration 1905 : loss : 0.035636, loss_ce: 0.014974
2022-01-08 10:07:58,708 iteration 1906 : loss : 0.055952, loss_ce: 0.018125
2022-01-08 10:08:00,185 iteration 1907 : loss : 0.035126, loss_ce: 0.013063
2022-01-08 10:08:01,587 iteration 1908 : loss : 0.037411, loss_ce: 0.015912
2022-01-08 10:08:03,019 iteration 1909 : loss : 0.041366, loss_ce: 0.022593
2022-01-08 10:08:04,435 iteration 1910 : loss : 0.033714, loss_ce: 0.015305
2022-01-08 10:08:05,919 iteration 1911 : loss : 0.037380, loss_ce: 0.016100
2022-01-08 10:08:07,326 iteration 1912 : loss : 0.043722, loss_ce: 0.012198
2022-01-08 10:08:08,683 iteration 1913 : loss : 0.030417, loss_ce: 0.011200
2022-01-08 10:08:10,018 iteration 1914 : loss : 0.030990, loss_ce: 0.012747
2022-01-08 10:08:11,403 iteration 1915 : loss : 0.047288, loss_ce: 0.015583
2022-01-08 10:08:12,776 iteration 1916 : loss : 0.044953, loss_ce: 0.021296
2022-01-08 10:08:14,188 iteration 1917 : loss : 0.068575, loss_ce: 0.027147
2022-01-08 10:08:15,522 iteration 1918 : loss : 0.046066, loss_ce: 0.022578
2022-01-08 10:08:16,878 iteration 1919 : loss : 0.038579, loss_ce: 0.018518
2022-01-08 10:08:18,244 iteration 1920 : loss : 0.029636, loss_ce: 0.010510
2022-01-08 10:08:19,652 iteration 1921 : loss : 0.043712, loss_ce: 0.015286
 28%|████████▏                    | 113/400 [47:50<1:58:05, 24.69s/it]2022-01-08 10:08:21,002 iteration 1922 : loss : 0.047002, loss_ce: 0.018462
2022-01-08 10:08:22,299 iteration 1923 : loss : 0.029112, loss_ce: 0.013307
2022-01-08 10:08:23,697 iteration 1924 : loss : 0.057944, loss_ce: 0.032963
2022-01-08 10:08:25,026 iteration 1925 : loss : 0.039548, loss_ce: 0.016346
2022-01-08 10:08:26,363 iteration 1926 : loss : 0.027965, loss_ce: 0.013646
2022-01-08 10:08:27,769 iteration 1927 : loss : 0.049803, loss_ce: 0.022634
2022-01-08 10:08:29,167 iteration 1928 : loss : 0.029646, loss_ce: 0.011736
2022-01-08 10:08:30,523 iteration 1929 : loss : 0.035744, loss_ce: 0.016857
2022-01-08 10:08:31,900 iteration 1930 : loss : 0.056490, loss_ce: 0.019805
2022-01-08 10:08:33,242 iteration 1931 : loss : 0.043648, loss_ce: 0.012782
2022-01-08 10:08:34,615 iteration 1932 : loss : 0.040711, loss_ce: 0.012670
2022-01-08 10:08:35,995 iteration 1933 : loss : 0.039547, loss_ce: 0.015200
2022-01-08 10:08:37,252 iteration 1934 : loss : 0.030788, loss_ce: 0.015233
2022-01-08 10:08:38,592 iteration 1935 : loss : 0.043391, loss_ce: 0.018462
2022-01-08 10:08:39,996 iteration 1936 : loss : 0.039656, loss_ce: 0.015726
2022-01-08 10:08:41,394 iteration 1937 : loss : 0.034185, loss_ce: 0.013525
2022-01-08 10:08:42,791 iteration 1938 : loss : 0.044350, loss_ce: 0.013234
 28%|████████▎                    | 114/400 [48:13<1:55:29, 24.23s/it]2022-01-08 10:08:44,121 iteration 1939 : loss : 0.030356, loss_ce: 0.014625
2022-01-08 10:08:45,460 iteration 1940 : loss : 0.035797, loss_ce: 0.014526
2022-01-08 10:08:46,878 iteration 1941 : loss : 0.033024, loss_ce: 0.014496
2022-01-08 10:08:48,235 iteration 1942 : loss : 0.055546, loss_ce: 0.017940
2022-01-08 10:08:49,527 iteration 1943 : loss : 0.031926, loss_ce: 0.011396
2022-01-08 10:08:50,867 iteration 1944 : loss : 0.033839, loss_ce: 0.012613
2022-01-08 10:08:52,218 iteration 1945 : loss : 0.037651, loss_ce: 0.018990
2022-01-08 10:08:53,567 iteration 1946 : loss : 0.044567, loss_ce: 0.016582
2022-01-08 10:08:54,908 iteration 1947 : loss : 0.049913, loss_ce: 0.017775
2022-01-08 10:08:56,239 iteration 1948 : loss : 0.046342, loss_ce: 0.021179
2022-01-08 10:08:57,518 iteration 1949 : loss : 0.032673, loss_ce: 0.017272
2022-01-08 10:08:58,920 iteration 1950 : loss : 0.043763, loss_ce: 0.016297
2022-01-08 10:09:00,389 iteration 1951 : loss : 0.056286, loss_ce: 0.023061
2022-01-08 10:09:01,706 iteration 1952 : loss : 0.037027, loss_ce: 0.011005
2022-01-08 10:09:03,098 iteration 1953 : loss : 0.036785, loss_ce: 0.019360
2022-01-08 10:09:04,540 iteration 1954 : loss : 0.043140, loss_ce: 0.015352
2022-01-08 10:09:04,540 Training Data Eval:
2022-01-08 10:09:11,402   Average segmentation loss on training set: 0.0346
2022-01-08 10:09:11,402 Validation Data Eval:
2022-01-08 10:09:13,768   Average segmentation loss on validation set: 0.0877
2022-01-08 10:09:15,121 iteration 1955 : loss : 0.029765, loss_ce: 0.011377
 29%|████████▎                    | 115/400 [48:45<2:06:37, 26.66s/it]2022-01-08 10:09:16,573 iteration 1956 : loss : 0.040098, loss_ce: 0.015009
2022-01-08 10:09:17,878 iteration 1957 : loss : 0.035031, loss_ce: 0.016022
2022-01-08 10:09:19,233 iteration 1958 : loss : 0.032649, loss_ce: 0.014990
2022-01-08 10:09:20,693 iteration 1959 : loss : 0.044533, loss_ce: 0.013611
2022-01-08 10:09:22,022 iteration 1960 : loss : 0.060971, loss_ce: 0.020093
2022-01-08 10:09:23,394 iteration 1961 : loss : 0.046288, loss_ce: 0.021086
2022-01-08 10:09:24,751 iteration 1962 : loss : 0.036916, loss_ce: 0.016534
2022-01-08 10:09:26,135 iteration 1963 : loss : 0.037993, loss_ce: 0.017474
2022-01-08 10:09:27,483 iteration 1964 : loss : 0.031832, loss_ce: 0.009789
2022-01-08 10:09:28,809 iteration 1965 : loss : 0.035106, loss_ce: 0.017286
2022-01-08 10:09:30,129 iteration 1966 : loss : 0.033757, loss_ce: 0.015481
2022-01-08 10:09:31,441 iteration 1967 : loss : 0.044235, loss_ce: 0.014859
2022-01-08 10:09:32,750 iteration 1968 : loss : 0.030220, loss_ce: 0.017967
2022-01-08 10:09:34,139 iteration 1969 : loss : 0.049558, loss_ce: 0.019549
2022-01-08 10:09:35,630 iteration 1970 : loss : 0.047092, loss_ce: 0.017135
2022-01-08 10:09:36,898 iteration 1971 : loss : 0.036117, loss_ce: 0.010188
2022-01-08 10:09:38,364 iteration 1972 : loss : 0.051969, loss_ce: 0.018926
 29%|████████▍                    | 116/400 [49:09<2:01:19, 25.63s/it]2022-01-08 10:09:39,684 iteration 1973 : loss : 0.026443, loss_ce: 0.010747
2022-01-08 10:09:40,984 iteration 1974 : loss : 0.029706, loss_ce: 0.012322
2022-01-08 10:09:42,423 iteration 1975 : loss : 0.043251, loss_ce: 0.017190
2022-01-08 10:09:43,704 iteration 1976 : loss : 0.035324, loss_ce: 0.015253
2022-01-08 10:09:45,140 iteration 1977 : loss : 0.068862, loss_ce: 0.029354
2022-01-08 10:09:46,462 iteration 1978 : loss : 0.032143, loss_ce: 0.013229
2022-01-08 10:09:47,801 iteration 1979 : loss : 0.046850, loss_ce: 0.022631
2022-01-08 10:09:49,204 iteration 1980 : loss : 0.036338, loss_ce: 0.013210
2022-01-08 10:09:50,525 iteration 1981 : loss : 0.031365, loss_ce: 0.012556
2022-01-08 10:09:51,945 iteration 1982 : loss : 0.032378, loss_ce: 0.011047
2022-01-08 10:09:53,387 iteration 1983 : loss : 0.037356, loss_ce: 0.014371
2022-01-08 10:09:54,721 iteration 1984 : loss : 0.047001, loss_ce: 0.016685
2022-01-08 10:09:56,058 iteration 1985 : loss : 0.030985, loss_ce: 0.012075
2022-01-08 10:09:57,385 iteration 1986 : loss : 0.033530, loss_ce: 0.012730
2022-01-08 10:09:58,701 iteration 1987 : loss : 0.025273, loss_ce: 0.009953
2022-01-08 10:10:00,140 iteration 1988 : loss : 0.034318, loss_ce: 0.014422
2022-01-08 10:10:01,505 iteration 1989 : loss : 0.024357, loss_ce: 0.011263
 29%|████████▍                    | 117/400 [49:32<1:57:21, 24.88s/it]2022-01-08 10:10:02,909 iteration 1990 : loss : 0.047744, loss_ce: 0.018534
2022-01-08 10:10:04,187 iteration 1991 : loss : 0.030629, loss_ce: 0.013335
2022-01-08 10:10:05,584 iteration 1992 : loss : 0.032788, loss_ce: 0.014041
2022-01-08 10:10:06,948 iteration 1993 : loss : 0.047595, loss_ce: 0.012511
2022-01-08 10:10:08,356 iteration 1994 : loss : 0.036946, loss_ce: 0.014667
2022-01-08 10:10:09,705 iteration 1995 : loss : 0.056705, loss_ce: 0.026015
2022-01-08 10:10:11,073 iteration 1996 : loss : 0.032754, loss_ce: 0.014550
2022-01-08 10:10:12,443 iteration 1997 : loss : 0.036311, loss_ce: 0.016329
2022-01-08 10:10:13,838 iteration 1998 : loss : 0.037954, loss_ce: 0.019055
2022-01-08 10:10:15,226 iteration 1999 : loss : 0.048656, loss_ce: 0.022712
2022-01-08 10:10:16,705 iteration 2000 : loss : 0.061717, loss_ce: 0.017571
2022-01-08 10:10:18,065 iteration 2001 : loss : 0.053202, loss_ce: 0.018548
2022-01-08 10:10:19,346 iteration 2002 : loss : 0.035088, loss_ce: 0.015898
2022-01-08 10:10:20,589 iteration 2003 : loss : 0.025167, loss_ce: 0.012925
2022-01-08 10:10:21,916 iteration 2004 : loss : 0.047676, loss_ce: 0.014499
2022-01-08 10:10:23,317 iteration 2005 : loss : 0.034902, loss_ce: 0.012190
2022-01-08 10:10:24,702 iteration 2006 : loss : 0.037897, loss_ce: 0.014352
 30%|████████▌                    | 118/400 [49:55<1:54:34, 24.38s/it]2022-01-08 10:10:26,150 iteration 2007 : loss : 0.045806, loss_ce: 0.016469
2022-01-08 10:10:27,626 iteration 2008 : loss : 0.040145, loss_ce: 0.015686
2022-01-08 10:10:28,986 iteration 2009 : loss : 0.034309, loss_ce: 0.014447
2022-01-08 10:10:30,346 iteration 2010 : loss : 0.035295, loss_ce: 0.014634
2022-01-08 10:10:31,798 iteration 2011 : loss : 0.049220, loss_ce: 0.020170
2022-01-08 10:10:33,169 iteration 2012 : loss : 0.041214, loss_ce: 0.019594
2022-01-08 10:10:34,460 iteration 2013 : loss : 0.030410, loss_ce: 0.012324
2022-01-08 10:10:35,791 iteration 2014 : loss : 0.071249, loss_ce: 0.030219
2022-01-08 10:10:37,188 iteration 2015 : loss : 0.031095, loss_ce: 0.013397
2022-01-08 10:10:38,546 iteration 2016 : loss : 0.057224, loss_ce: 0.018142
2022-01-08 10:10:39,955 iteration 2017 : loss : 0.031769, loss_ce: 0.009336
2022-01-08 10:10:41,412 iteration 2018 : loss : 0.049743, loss_ce: 0.022569
2022-01-08 10:10:42,746 iteration 2019 : loss : 0.053322, loss_ce: 0.020037
2022-01-08 10:10:44,083 iteration 2020 : loss : 0.034135, loss_ce: 0.011594
2022-01-08 10:10:45,475 iteration 2021 : loss : 0.036098, loss_ce: 0.011661
2022-01-08 10:10:46,864 iteration 2022 : loss : 0.035174, loss_ce: 0.015999
2022-01-08 10:10:48,286 iteration 2023 : loss : 0.051062, loss_ce: 0.020354
 30%|████████▋                    | 119/400 [50:18<1:53:03, 24.14s/it]2022-01-08 10:10:49,634 iteration 2024 : loss : 0.035480, loss_ce: 0.012549
2022-01-08 10:10:51,010 iteration 2025 : loss : 0.032003, loss_ce: 0.015075
2022-01-08 10:10:52,423 iteration 2026 : loss : 0.036783, loss_ce: 0.015647
2022-01-08 10:10:53,765 iteration 2027 : loss : 0.033236, loss_ce: 0.009227
2022-01-08 10:10:55,078 iteration 2028 : loss : 0.031772, loss_ce: 0.017208
2022-01-08 10:10:56,435 iteration 2029 : loss : 0.033976, loss_ce: 0.013347
2022-01-08 10:10:57,782 iteration 2030 : loss : 0.030369, loss_ce: 0.012949
2022-01-08 10:10:59,140 iteration 2031 : loss : 0.037140, loss_ce: 0.013525
2022-01-08 10:11:00,589 iteration 2032 : loss : 0.069061, loss_ce: 0.032169
2022-01-08 10:11:02,054 iteration 2033 : loss : 0.043900, loss_ce: 0.019256
2022-01-08 10:11:03,373 iteration 2034 : loss : 0.032535, loss_ce: 0.014331
2022-01-08 10:11:04,748 iteration 2035 : loss : 0.049980, loss_ce: 0.017546
2022-01-08 10:11:06,160 iteration 2036 : loss : 0.043214, loss_ce: 0.023738
2022-01-08 10:11:07,516 iteration 2037 : loss : 0.068878, loss_ce: 0.016842
2022-01-08 10:11:08,921 iteration 2038 : loss : 0.049746, loss_ce: 0.019416
2022-01-08 10:11:10,368 iteration 2039 : loss : 0.062102, loss_ce: 0.025812
2022-01-08 10:11:10,369 Training Data Eval:
2022-01-08 10:11:17,217   Average segmentation loss on training set: 0.0271
2022-01-08 10:11:17,217 Validation Data Eval:
2022-01-08 10:11:19,574   Average segmentation loss on validation set: 0.0724
2022-01-08 10:11:21,022 iteration 2040 : loss : 0.050964, loss_ce: 0.019529
 30%|████████▋                    | 120/400 [50:51<2:04:42, 26.72s/it]2022-01-08 10:11:22,474 iteration 2041 : loss : 0.068854, loss_ce: 0.019609
2022-01-08 10:11:23,815 iteration 2042 : loss : 0.044873, loss_ce: 0.016261
2022-01-08 10:11:25,165 iteration 2043 : loss : 0.026754, loss_ce: 0.008626
2022-01-08 10:11:26,530 iteration 2044 : loss : 0.054941, loss_ce: 0.022080
2022-01-08 10:11:27,872 iteration 2045 : loss : 0.037319, loss_ce: 0.011255
2022-01-08 10:11:29,247 iteration 2046 : loss : 0.038446, loss_ce: 0.017237
2022-01-08 10:11:30,601 iteration 2047 : loss : 0.041325, loss_ce: 0.015627
2022-01-08 10:11:31,978 iteration 2048 : loss : 0.039102, loss_ce: 0.015720
2022-01-08 10:11:33,311 iteration 2049 : loss : 0.046797, loss_ce: 0.013933
2022-01-08 10:11:34,696 iteration 2050 : loss : 0.042526, loss_ce: 0.019690
2022-01-08 10:11:36,082 iteration 2051 : loss : 0.036141, loss_ce: 0.017889
2022-01-08 10:11:37,396 iteration 2052 : loss : 0.073868, loss_ce: 0.030897
2022-01-08 10:11:38,780 iteration 2053 : loss : 0.052586, loss_ce: 0.015568
2022-01-08 10:11:40,117 iteration 2054 : loss : 0.034992, loss_ce: 0.014295
2022-01-08 10:11:41,465 iteration 2055 : loss : 0.034626, loss_ce: 0.011146
2022-01-08 10:11:42,868 iteration 2056 : loss : 0.042269, loss_ce: 0.018314
2022-01-08 10:11:44,197 iteration 2057 : loss : 0.037555, loss_ce: 0.015598
 30%|████████▊                    | 121/400 [51:14<1:59:17, 25.65s/it]2022-01-08 10:11:45,620 iteration 2058 : loss : 0.036523, loss_ce: 0.016954
2022-01-08 10:11:47,035 iteration 2059 : loss : 0.046764, loss_ce: 0.016538
2022-01-08 10:11:48,315 iteration 2060 : loss : 0.036656, loss_ce: 0.013826
2022-01-08 10:11:49,706 iteration 2061 : loss : 0.052048, loss_ce: 0.017512
2022-01-08 10:11:51,044 iteration 2062 : loss : 0.039605, loss_ce: 0.016962
2022-01-08 10:11:52,395 iteration 2063 : loss : 0.062180, loss_ce: 0.016579
2022-01-08 10:11:53,714 iteration 2064 : loss : 0.027840, loss_ce: 0.009650
2022-01-08 10:11:55,058 iteration 2065 : loss : 0.046822, loss_ce: 0.013490
2022-01-08 10:11:56,411 iteration 2066 : loss : 0.044430, loss_ce: 0.015818
2022-01-08 10:11:57,798 iteration 2067 : loss : 0.027214, loss_ce: 0.009971
2022-01-08 10:11:59,217 iteration 2068 : loss : 0.032290, loss_ce: 0.013793
2022-01-08 10:12:00,547 iteration 2069 : loss : 0.035141, loss_ce: 0.009655
2022-01-08 10:12:01,953 iteration 2070 : loss : 0.033867, loss_ce: 0.013635
2022-01-08 10:12:03,370 iteration 2071 : loss : 0.051879, loss_ce: 0.018468
2022-01-08 10:12:04,746 iteration 2072 : loss : 0.035117, loss_ce: 0.013638
2022-01-08 10:12:06,165 iteration 2073 : loss : 0.050172, loss_ce: 0.016834
2022-01-08 10:12:07,553 iteration 2074 : loss : 0.035425, loss_ce: 0.018981
 30%|████████▊                    | 122/400 [51:38<1:55:41, 24.97s/it]2022-01-08 10:12:08,919 iteration 2075 : loss : 0.036908, loss_ce: 0.012788
2022-01-08 10:12:10,293 iteration 2076 : loss : 0.034985, loss_ce: 0.011096
2022-01-08 10:12:11,678 iteration 2077 : loss : 0.036195, loss_ce: 0.016597
2022-01-08 10:12:13,164 iteration 2078 : loss : 0.052699, loss_ce: 0.018219
2022-01-08 10:12:14,503 iteration 2079 : loss : 0.047862, loss_ce: 0.016116
2022-01-08 10:12:15,927 iteration 2080 : loss : 0.045856, loss_ce: 0.017625
2022-01-08 10:12:17,192 iteration 2081 : loss : 0.033598, loss_ce: 0.013099
2022-01-08 10:12:18,458 iteration 2082 : loss : 0.023887, loss_ce: 0.009134
2022-01-08 10:12:19,815 iteration 2083 : loss : 0.052451, loss_ce: 0.031454
2022-01-08 10:12:21,169 iteration 2084 : loss : 0.058032, loss_ce: 0.033691
2022-01-08 10:12:22,477 iteration 2085 : loss : 0.040240, loss_ce: 0.012684
2022-01-08 10:12:23,819 iteration 2086 : loss : 0.056083, loss_ce: 0.029216
2022-01-08 10:12:25,130 iteration 2087 : loss : 0.038585, loss_ce: 0.015525
2022-01-08 10:12:26,557 iteration 2088 : loss : 0.040284, loss_ce: 0.015072
2022-01-08 10:12:27,946 iteration 2089 : loss : 0.033478, loss_ce: 0.015138
2022-01-08 10:12:29,236 iteration 2090 : loss : 0.035238, loss_ce: 0.015216
2022-01-08 10:12:30,640 iteration 2091 : loss : 0.047771, loss_ce: 0.021201
 31%|████████▉                    | 123/400 [52:01<1:52:39, 24.40s/it]2022-01-08 10:12:31,978 iteration 2092 : loss : 0.029524, loss_ce: 0.013185
2022-01-08 10:12:33,330 iteration 2093 : loss : 0.061563, loss_ce: 0.026594
2022-01-08 10:12:34,704 iteration 2094 : loss : 0.047652, loss_ce: 0.014879
2022-01-08 10:12:36,014 iteration 2095 : loss : 0.025922, loss_ce: 0.011843
2022-01-08 10:12:37,471 iteration 2096 : loss : 0.036079, loss_ce: 0.018445
2022-01-08 10:12:38,858 iteration 2097 : loss : 0.052820, loss_ce: 0.016074
2022-01-08 10:12:40,216 iteration 2098 : loss : 0.048261, loss_ce: 0.018103
2022-01-08 10:12:41,592 iteration 2099 : loss : 0.034283, loss_ce: 0.011672
2022-01-08 10:12:42,993 iteration 2100 : loss : 0.054542, loss_ce: 0.029364
2022-01-08 10:12:44,372 iteration 2101 : loss : 0.032699, loss_ce: 0.011710
2022-01-08 10:12:45,778 iteration 2102 : loss : 0.048081, loss_ce: 0.030312
2022-01-08 10:12:47,069 iteration 2103 : loss : 0.038908, loss_ce: 0.018047
2022-01-08 10:12:48,378 iteration 2104 : loss : 0.044858, loss_ce: 0.015017
2022-01-08 10:12:49,821 iteration 2105 : loss : 0.040033, loss_ce: 0.013330
2022-01-08 10:12:51,138 iteration 2106 : loss : 0.029528, loss_ce: 0.013050
2022-01-08 10:12:52,574 iteration 2107 : loss : 0.040890, loss_ce: 0.013276
2022-01-08 10:12:53,964 iteration 2108 : loss : 0.037762, loss_ce: 0.014140
 31%|████████▉                    | 124/400 [52:24<1:50:45, 24.08s/it]2022-01-08 10:12:55,366 iteration 2109 : loss : 0.032459, loss_ce: 0.013341
2022-01-08 10:12:56,768 iteration 2110 : loss : 0.043449, loss_ce: 0.015056
2022-01-08 10:12:58,214 iteration 2111 : loss : 0.046290, loss_ce: 0.014812
2022-01-08 10:12:59,574 iteration 2112 : loss : 0.055673, loss_ce: 0.021456
2022-01-08 10:13:00,900 iteration 2113 : loss : 0.043149, loss_ce: 0.020074
2022-01-08 10:13:02,307 iteration 2114 : loss : 0.044691, loss_ce: 0.021256
2022-01-08 10:13:03,704 iteration 2115 : loss : 0.044577, loss_ce: 0.014667
2022-01-08 10:13:05,082 iteration 2116 : loss : 0.041131, loss_ce: 0.014760
2022-01-08 10:13:06,453 iteration 2117 : loss : 0.034036, loss_ce: 0.013422
2022-01-08 10:13:07,857 iteration 2118 : loss : 0.045974, loss_ce: 0.014979
2022-01-08 10:13:09,267 iteration 2119 : loss : 0.040189, loss_ce: 0.013034
2022-01-08 10:13:10,638 iteration 2120 : loss : 0.046904, loss_ce: 0.014650
2022-01-08 10:13:11,910 iteration 2121 : loss : 0.035827, loss_ce: 0.012317
2022-01-08 10:13:13,257 iteration 2122 : loss : 0.041593, loss_ce: 0.014089
2022-01-08 10:13:14,675 iteration 2123 : loss : 0.069433, loss_ce: 0.033458
2022-01-08 10:13:16,119 iteration 2124 : loss : 0.047147, loss_ce: 0.017483
2022-01-08 10:13:16,119 Training Data Eval:
2022-01-08 10:13:22,971   Average segmentation loss on training set: 0.0299
2022-01-08 10:13:22,971 Validation Data Eval:
2022-01-08 10:13:25,339   Average segmentation loss on validation set: 0.0839
2022-01-08 10:13:26,743 iteration 2125 : loss : 0.049741, loss_ce: 0.030545
 31%|█████████                    | 125/400 [52:57<2:02:19, 26.69s/it]2022-01-08 10:13:28,202 iteration 2126 : loss : 0.029233, loss_ce: 0.011650
2022-01-08 10:13:29,592 iteration 2127 : loss : 0.069717, loss_ce: 0.041344
2022-01-08 10:13:30,989 iteration 2128 : loss : 0.031662, loss_ce: 0.010140
2022-01-08 10:13:32,319 iteration 2129 : loss : 0.043795, loss_ce: 0.017271
2022-01-08 10:13:33,730 iteration 2130 : loss : 0.072999, loss_ce: 0.022592
2022-01-08 10:13:35,078 iteration 2131 : loss : 0.051981, loss_ce: 0.015838
2022-01-08 10:13:36,442 iteration 2132 : loss : 0.029572, loss_ce: 0.013025
2022-01-08 10:13:37,868 iteration 2133 : loss : 0.069419, loss_ce: 0.025762
2022-01-08 10:13:39,164 iteration 2134 : loss : 0.034368, loss_ce: 0.017950
2022-01-08 10:13:40,528 iteration 2135 : loss : 0.049037, loss_ce: 0.020259
2022-01-08 10:13:41,891 iteration 2136 : loss : 0.034284, loss_ce: 0.014500
2022-01-08 10:13:43,265 iteration 2137 : loss : 0.047068, loss_ce: 0.019825
2022-01-08 10:13:44,590 iteration 2138 : loss : 0.037281, loss_ce: 0.015217
2022-01-08 10:13:45,989 iteration 2139 : loss : 0.058786, loss_ce: 0.032995
2022-01-08 10:13:47,351 iteration 2140 : loss : 0.044015, loss_ce: 0.021453
2022-01-08 10:13:48,776 iteration 2141 : loss : 0.044475, loss_ce: 0.014493
2022-01-08 10:13:50,103 iteration 2142 : loss : 0.039304, loss_ce: 0.012559
 32%|█████████▏                   | 126/400 [53:20<1:57:19, 25.69s/it]2022-01-08 10:13:51,438 iteration 2143 : loss : 0.038001, loss_ce: 0.016109
2022-01-08 10:13:52,824 iteration 2144 : loss : 0.035329, loss_ce: 0.010878
2022-01-08 10:13:54,213 iteration 2145 : loss : 0.037372, loss_ce: 0.012190
2022-01-08 10:13:55,521 iteration 2146 : loss : 0.021866, loss_ce: 0.009784
2022-01-08 10:13:56,903 iteration 2147 : loss : 0.044320, loss_ce: 0.013897
2022-01-08 10:13:58,244 iteration 2148 : loss : 0.038428, loss_ce: 0.015259
2022-01-08 10:13:59,613 iteration 2149 : loss : 0.041880, loss_ce: 0.014955
2022-01-08 10:14:00,958 iteration 2150 : loss : 0.030272, loss_ce: 0.011701
2022-01-08 10:14:02,395 iteration 2151 : loss : 0.043519, loss_ce: 0.017468
2022-01-08 10:14:03,718 iteration 2152 : loss : 0.029658, loss_ce: 0.011324
2022-01-08 10:14:05,031 iteration 2153 : loss : 0.026753, loss_ce: 0.010971
2022-01-08 10:14:06,533 iteration 2154 : loss : 0.061745, loss_ce: 0.018860
2022-01-08 10:14:07,893 iteration 2155 : loss : 0.038606, loss_ce: 0.014570
2022-01-08 10:14:09,202 iteration 2156 : loss : 0.036028, loss_ce: 0.014830
2022-01-08 10:14:10,721 iteration 2157 : loss : 0.069536, loss_ce: 0.023812
2022-01-08 10:14:12,185 iteration 2158 : loss : 0.052722, loss_ce: 0.023578
2022-01-08 10:14:13,565 iteration 2159 : loss : 0.048580, loss_ce: 0.015542
 32%|█████████▏                   | 127/400 [53:44<1:53:50, 25.02s/it]2022-01-08 10:14:14,926 iteration 2160 : loss : 0.037238, loss_ce: 0.014900
2022-01-08 10:14:16,234 iteration 2161 : loss : 0.036639, loss_ce: 0.012774
2022-01-08 10:14:17,571 iteration 2162 : loss : 0.036349, loss_ce: 0.015765
2022-01-08 10:14:18,910 iteration 2163 : loss : 0.034526, loss_ce: 0.012434
2022-01-08 10:14:20,313 iteration 2164 : loss : 0.028222, loss_ce: 0.013781
2022-01-08 10:14:21,689 iteration 2165 : loss : 0.051086, loss_ce: 0.018636
2022-01-08 10:14:23,013 iteration 2166 : loss : 0.032441, loss_ce: 0.014319
2022-01-08 10:14:24,307 iteration 2167 : loss : 0.029599, loss_ce: 0.011941
2022-01-08 10:14:25,642 iteration 2168 : loss : 0.027122, loss_ce: 0.011051
2022-01-08 10:14:27,024 iteration 2169 : loss : 0.059872, loss_ce: 0.023738
2022-01-08 10:14:28,283 iteration 2170 : loss : 0.025341, loss_ce: 0.007907
2022-01-08 10:14:29,634 iteration 2171 : loss : 0.038293, loss_ce: 0.014595
2022-01-08 10:14:30,974 iteration 2172 : loss : 0.049768, loss_ce: 0.016236
2022-01-08 10:14:32,292 iteration 2173 : loss : 0.033907, loss_ce: 0.008673
2022-01-08 10:14:33,647 iteration 2174 : loss : 0.047581, loss_ce: 0.016703
2022-01-08 10:14:35,085 iteration 2175 : loss : 0.039018, loss_ce: 0.017909
2022-01-08 10:14:36,343 iteration 2176 : loss : 0.026502, loss_ce: 0.012734
 32%|█████████▎                   | 128/400 [54:07<1:50:23, 24.35s/it]2022-01-08 10:14:37,668 iteration 2177 : loss : 0.043498, loss_ce: 0.022755
2022-01-08 10:14:39,024 iteration 2178 : loss : 0.055889, loss_ce: 0.019830
2022-01-08 10:14:40,453 iteration 2179 : loss : 0.041517, loss_ce: 0.013924
2022-01-08 10:14:41,855 iteration 2180 : loss : 0.051901, loss_ce: 0.024451
2022-01-08 10:14:43,155 iteration 2181 : loss : 0.032315, loss_ce: 0.012020
2022-01-08 10:14:44,521 iteration 2182 : loss : 0.044510, loss_ce: 0.018183
2022-01-08 10:14:45,902 iteration 2183 : loss : 0.048723, loss_ce: 0.015596
2022-01-08 10:14:47,204 iteration 2184 : loss : 0.040958, loss_ce: 0.021427
2022-01-08 10:14:48,522 iteration 2185 : loss : 0.035779, loss_ce: 0.013445
2022-01-08 10:14:49,968 iteration 2186 : loss : 0.037101, loss_ce: 0.015647
2022-01-08 10:14:51,403 iteration 2187 : loss : 0.033572, loss_ce: 0.010672
2022-01-08 10:14:52,903 iteration 2188 : loss : 0.058106, loss_ce: 0.031396
2022-01-08 10:14:54,368 iteration 2189 : loss : 0.055714, loss_ce: 0.022515
2022-01-08 10:14:55,721 iteration 2190 : loss : 0.043289, loss_ce: 0.020700
2022-01-08 10:14:57,095 iteration 2191 : loss : 0.035344, loss_ce: 0.014013
2022-01-08 10:14:58,424 iteration 2192 : loss : 0.033212, loss_ce: 0.013606
2022-01-08 10:14:59,829 iteration 2193 : loss : 0.030332, loss_ce: 0.011149
 32%|█████████▎                   | 129/400 [54:30<1:48:48, 24.09s/it]2022-01-08 10:15:01,262 iteration 2194 : loss : 0.034934, loss_ce: 0.014455
2022-01-08 10:15:02,640 iteration 2195 : loss : 0.039800, loss_ce: 0.020140
2022-01-08 10:15:04,049 iteration 2196 : loss : 0.037744, loss_ce: 0.015701
2022-01-08 10:15:05,489 iteration 2197 : loss : 0.036234, loss_ce: 0.012991
2022-01-08 10:15:06,854 iteration 2198 : loss : 0.042289, loss_ce: 0.014232
2022-01-08 10:15:08,228 iteration 2199 : loss : 0.041324, loss_ce: 0.016638
2022-01-08 10:15:09,680 iteration 2200 : loss : 0.074124, loss_ce: 0.016889
2022-01-08 10:15:10,989 iteration 2201 : loss : 0.027067, loss_ce: 0.010648
2022-01-08 10:15:12,456 iteration 2202 : loss : 0.046902, loss_ce: 0.020281
2022-01-08 10:15:13,841 iteration 2203 : loss : 0.034792, loss_ce: 0.010342
2022-01-08 10:15:15,139 iteration 2204 : loss : 0.031587, loss_ce: 0.010051
2022-01-08 10:15:16,526 iteration 2205 : loss : 0.034677, loss_ce: 0.016283
2022-01-08 10:15:17,977 iteration 2206 : loss : 0.063284, loss_ce: 0.023637
2022-01-08 10:15:19,368 iteration 2207 : loss : 0.036889, loss_ce: 0.011017
2022-01-08 10:15:20,788 iteration 2208 : loss : 0.041895, loss_ce: 0.013903
2022-01-08 10:15:22,151 iteration 2209 : loss : 0.041350, loss_ce: 0.015848
2022-01-08 10:15:22,151 Training Data Eval:
2022-01-08 10:15:29,049   Average segmentation loss on training set: 0.0263
2022-01-08 10:15:29,050 Validation Data Eval:
2022-01-08 10:15:31,432   Average segmentation loss on validation set: 0.0869
2022-01-08 10:15:32,848 iteration 2210 : loss : 0.043396, loss_ce: 0.021555
 32%|█████████▍                   | 130/400 [55:03<2:00:27, 26.77s/it]2022-01-08 10:15:34,257 iteration 2211 : loss : 0.040338, loss_ce: 0.010626
2022-01-08 10:15:35,572 iteration 2212 : loss : 0.042363, loss_ce: 0.019698
2022-01-08 10:15:36,943 iteration 2213 : loss : 0.042231, loss_ce: 0.010974
2022-01-08 10:15:38,235 iteration 2214 : loss : 0.032524, loss_ce: 0.010552
2022-01-08 10:15:39,530 iteration 2215 : loss : 0.034772, loss_ce: 0.013331
2022-01-08 10:15:40,869 iteration 2216 : loss : 0.038491, loss_ce: 0.016796
2022-01-08 10:15:42,181 iteration 2217 : loss : 0.036735, loss_ce: 0.017714
2022-01-08 10:15:43,524 iteration 2218 : loss : 0.027559, loss_ce: 0.010196
2022-01-08 10:15:44,922 iteration 2219 : loss : 0.070211, loss_ce: 0.042092
2022-01-08 10:15:46,368 iteration 2220 : loss : 0.041235, loss_ce: 0.021123
2022-01-08 10:15:47,805 iteration 2221 : loss : 0.041520, loss_ce: 0.013828
2022-01-08 10:15:49,077 iteration 2222 : loss : 0.033663, loss_ce: 0.013293
2022-01-08 10:15:50,394 iteration 2223 : loss : 0.033202, loss_ce: 0.011759
2022-01-08 10:15:51,834 iteration 2224 : loss : 0.041247, loss_ce: 0.019148
2022-01-08 10:15:53,222 iteration 2225 : loss : 0.039756, loss_ce: 0.013680
2022-01-08 10:15:54,644 iteration 2226 : loss : 0.055976, loss_ce: 0.024546
2022-01-08 10:15:56,035 iteration 2227 : loss : 0.057283, loss_ce: 0.021876
 33%|█████████▍                   | 131/400 [55:26<1:55:11, 25.69s/it]2022-01-08 10:15:57,504 iteration 2228 : loss : 0.045259, loss_ce: 0.023291
2022-01-08 10:15:58,878 iteration 2229 : loss : 0.035237, loss_ce: 0.013858
2022-01-08 10:16:00,381 iteration 2230 : loss : 0.054074, loss_ce: 0.022479
2022-01-08 10:16:01,724 iteration 2231 : loss : 0.034841, loss_ce: 0.017776
2022-01-08 10:16:03,070 iteration 2232 : loss : 0.049635, loss_ce: 0.024604
2022-01-08 10:16:04,619 iteration 2233 : loss : 0.059180, loss_ce: 0.017699
2022-01-08 10:16:06,011 iteration 2234 : loss : 0.036199, loss_ce: 0.009669
2022-01-08 10:16:07,368 iteration 2235 : loss : 0.037750, loss_ce: 0.014596
2022-01-08 10:16:08,712 iteration 2236 : loss : 0.026934, loss_ce: 0.011821
2022-01-08 10:16:10,094 iteration 2237 : loss : 0.043442, loss_ce: 0.017020
2022-01-08 10:16:11,406 iteration 2238 : loss : 0.033017, loss_ce: 0.010162
2022-01-08 10:16:12,754 iteration 2239 : loss : 0.035252, loss_ce: 0.009255
2022-01-08 10:16:14,047 iteration 2240 : loss : 0.045537, loss_ce: 0.013239
2022-01-08 10:16:15,414 iteration 2241 : loss : 0.030155, loss_ce: 0.011944
2022-01-08 10:16:16,834 iteration 2242 : loss : 0.044309, loss_ce: 0.014106
2022-01-08 10:16:18,224 iteration 2243 : loss : 0.050012, loss_ce: 0.024088
2022-01-08 10:16:19,570 iteration 2244 : loss : 0.025331, loss_ce: 0.009151
 33%|█████████▌                   | 132/400 [55:50<1:51:52, 25.05s/it]2022-01-08 10:16:20,964 iteration 2245 : loss : 0.033784, loss_ce: 0.011262
2022-01-08 10:16:22,240 iteration 2246 : loss : 0.025628, loss_ce: 0.007517
2022-01-08 10:16:23,617 iteration 2247 : loss : 0.048444, loss_ce: 0.022911
2022-01-08 10:16:25,051 iteration 2248 : loss : 0.057765, loss_ce: 0.025091
2022-01-08 10:16:26,397 iteration 2249 : loss : 0.028090, loss_ce: 0.012334
2022-01-08 10:16:27,749 iteration 2250 : loss : 0.030178, loss_ce: 0.009530
2022-01-08 10:16:29,138 iteration 2251 : loss : 0.045540, loss_ce: 0.017129
2022-01-08 10:16:30,504 iteration 2252 : loss : 0.044360, loss_ce: 0.015739
2022-01-08 10:16:31,969 iteration 2253 : loss : 0.054207, loss_ce: 0.023181
2022-01-08 10:16:33,383 iteration 2254 : loss : 0.028409, loss_ce: 0.011394
2022-01-08 10:16:34,745 iteration 2255 : loss : 0.036802, loss_ce: 0.013155
2022-01-08 10:16:36,115 iteration 2256 : loss : 0.035111, loss_ce: 0.014802
2022-01-08 10:16:37,436 iteration 2257 : loss : 0.043564, loss_ce: 0.016026
2022-01-08 10:16:38,762 iteration 2258 : loss : 0.066433, loss_ce: 0.023786
2022-01-08 10:16:40,193 iteration 2259 : loss : 0.034387, loss_ce: 0.014091
2022-01-08 10:16:41,491 iteration 2260 : loss : 0.035517, loss_ce: 0.015370
2022-01-08 10:16:42,870 iteration 2261 : loss : 0.046121, loss_ce: 0.019712
 33%|█████████▋                   | 133/400 [56:13<1:49:07, 24.52s/it]2022-01-08 10:16:44,219 iteration 2262 : loss : 0.028998, loss_ce: 0.015816
2022-01-08 10:16:45,641 iteration 2263 : loss : 0.061589, loss_ce: 0.026609
2022-01-08 10:16:47,084 iteration 2264 : loss : 0.033764, loss_ce: 0.013182
2022-01-08 10:16:48,461 iteration 2265 : loss : 0.030235, loss_ce: 0.011221
2022-01-08 10:16:49,795 iteration 2266 : loss : 0.027951, loss_ce: 0.011572
2022-01-08 10:16:51,145 iteration 2267 : loss : 0.027311, loss_ce: 0.008982
2022-01-08 10:16:52,510 iteration 2268 : loss : 0.036400, loss_ce: 0.014622
2022-01-08 10:16:53,826 iteration 2269 : loss : 0.033057, loss_ce: 0.018812
2022-01-08 10:16:55,195 iteration 2270 : loss : 0.039492, loss_ce: 0.013452
2022-01-08 10:16:56,534 iteration 2271 : loss : 0.040900, loss_ce: 0.015930
2022-01-08 10:16:57,886 iteration 2272 : loss : 0.043687, loss_ce: 0.025133
2022-01-08 10:16:59,274 iteration 2273 : loss : 0.030728, loss_ce: 0.010278
2022-01-08 10:17:00,665 iteration 2274 : loss : 0.048578, loss_ce: 0.019295
2022-01-08 10:17:02,080 iteration 2275 : loss : 0.051365, loss_ce: 0.021020
2022-01-08 10:17:03,389 iteration 2276 : loss : 0.025874, loss_ce: 0.010920
2022-01-08 10:17:04,762 iteration 2277 : loss : 0.036293, loss_ce: 0.012051
2022-01-08 10:17:06,070 iteration 2278 : loss : 0.025556, loss_ce: 0.009948
 34%|█████████▋                   | 134/400 [56:36<1:46:57, 24.12s/it]2022-01-08 10:17:07,407 iteration 2279 : loss : 0.087933, loss_ce: 0.042163
2022-01-08 10:17:08,780 iteration 2280 : loss : 0.037446, loss_ce: 0.017118
2022-01-08 10:17:10,146 iteration 2281 : loss : 0.065117, loss_ce: 0.017377
2022-01-08 10:17:11,543 iteration 2282 : loss : 0.035484, loss_ce: 0.012546
2022-01-08 10:17:12,980 iteration 2283 : loss : 0.036629, loss_ce: 0.017276
2022-01-08 10:17:14,309 iteration 2284 : loss : 0.037454, loss_ce: 0.017715
2022-01-08 10:17:15,660 iteration 2285 : loss : 0.031368, loss_ce: 0.012476
2022-01-08 10:17:17,011 iteration 2286 : loss : 0.051213, loss_ce: 0.021657
2022-01-08 10:17:18,364 iteration 2287 : loss : 0.033695, loss_ce: 0.013720
2022-01-08 10:17:19,774 iteration 2288 : loss : 0.039818, loss_ce: 0.014390
2022-01-08 10:17:21,094 iteration 2289 : loss : 0.043655, loss_ce: 0.011429
2022-01-08 10:17:22,517 iteration 2290 : loss : 0.028544, loss_ce: 0.014559
2022-01-08 10:17:23,865 iteration 2291 : loss : 0.027462, loss_ce: 0.010543
2022-01-08 10:17:25,223 iteration 2292 : loss : 0.040246, loss_ce: 0.017787
2022-01-08 10:17:26,522 iteration 2293 : loss : 0.034694, loss_ce: 0.017910
2022-01-08 10:17:27,930 iteration 2294 : loss : 0.065821, loss_ce: 0.026296
2022-01-08 10:17:27,931 Training Data Eval:
2022-01-08 10:17:34,802   Average segmentation loss on training set: 0.0354
2022-01-08 10:17:34,802 Validation Data Eval:
2022-01-08 10:17:37,171   Average segmentation loss on validation set: 0.0870
2022-01-08 10:17:38,634 iteration 2295 : loss : 0.051486, loss_ce: 0.019688
 34%|█████████▊                   | 135/400 [57:09<1:57:44, 26.66s/it]2022-01-08 10:17:39,963 iteration 2296 : loss : 0.032233, loss_ce: 0.013655
2022-01-08 10:17:41,320 iteration 2297 : loss : 0.037161, loss_ce: 0.018597
2022-01-08 10:17:42,669 iteration 2298 : loss : 0.026267, loss_ce: 0.011404
2022-01-08 10:17:44,092 iteration 2299 : loss : 0.049186, loss_ce: 0.022004
2022-01-08 10:17:45,429 iteration 2300 : loss : 0.030806, loss_ce: 0.014872
2022-01-08 10:17:46,863 iteration 2301 : loss : 0.034350, loss_ce: 0.010114
2022-01-08 10:17:48,267 iteration 2302 : loss : 0.045320, loss_ce: 0.016411
2022-01-08 10:17:49,655 iteration 2303 : loss : 0.037437, loss_ce: 0.012109
2022-01-08 10:17:51,106 iteration 2304 : loss : 0.037579, loss_ce: 0.012701
2022-01-08 10:17:52,515 iteration 2305 : loss : 0.038623, loss_ce: 0.016045
2022-01-08 10:17:53,980 iteration 2306 : loss : 0.043117, loss_ce: 0.020077
2022-01-08 10:17:55,328 iteration 2307 : loss : 0.039202, loss_ce: 0.012767
2022-01-08 10:17:56,652 iteration 2308 : loss : 0.033382, loss_ce: 0.011485
2022-01-08 10:17:57,989 iteration 2309 : loss : 0.032854, loss_ce: 0.015538
2022-01-08 10:17:59,412 iteration 2310 : loss : 0.036486, loss_ce: 0.017119
2022-01-08 10:18:00,691 iteration 2311 : loss : 0.031549, loss_ce: 0.010663
2022-01-08 10:18:02,059 iteration 2312 : loss : 0.050041, loss_ce: 0.016139
 34%|█████████▊                   | 136/400 [57:32<1:53:00, 25.68s/it]2022-01-08 10:18:03,500 iteration 2313 : loss : 0.036299, loss_ce: 0.013348
2022-01-08 10:18:04,771 iteration 2314 : loss : 0.024278, loss_ce: 0.007953
2022-01-08 10:18:06,098 iteration 2315 : loss : 0.026881, loss_ce: 0.009234
2022-01-08 10:18:07,554 iteration 2316 : loss : 0.029166, loss_ce: 0.012877
2022-01-08 10:18:08,848 iteration 2317 : loss : 0.024018, loss_ce: 0.008167
2022-01-08 10:18:10,190 iteration 2318 : loss : 0.035861, loss_ce: 0.019646
2022-01-08 10:18:11,549 iteration 2319 : loss : 0.034865, loss_ce: 0.013007
2022-01-08 10:18:12,997 iteration 2320 : loss : 0.034808, loss_ce: 0.015298
2022-01-08 10:18:14,366 iteration 2321 : loss : 0.033492, loss_ce: 0.014394
2022-01-08 10:18:15,740 iteration 2322 : loss : 0.034138, loss_ce: 0.014349
2022-01-08 10:18:17,013 iteration 2323 : loss : 0.033761, loss_ce: 0.012872
2022-01-08 10:18:18,357 iteration 2324 : loss : 0.023003, loss_ce: 0.010410
2022-01-08 10:18:19,609 iteration 2325 : loss : 0.021553, loss_ce: 0.010048
2022-01-08 10:18:20,969 iteration 2326 : loss : 0.032981, loss_ce: 0.008592
2022-01-08 10:18:22,293 iteration 2327 : loss : 0.039441, loss_ce: 0.014194
2022-01-08 10:18:23,654 iteration 2328 : loss : 0.042460, loss_ce: 0.020999
2022-01-08 10:18:24,965 iteration 2329 : loss : 0.025680, loss_ce: 0.010486
 34%|█████████▉                   | 137/400 [57:55<1:48:56, 24.86s/it]2022-01-08 10:18:26,380 iteration 2330 : loss : 0.033338, loss_ce: 0.014123
2022-01-08 10:18:27,684 iteration 2331 : loss : 0.028421, loss_ce: 0.011139
2022-01-08 10:18:29,121 iteration 2332 : loss : 0.028240, loss_ce: 0.011348
2022-01-08 10:18:30,473 iteration 2333 : loss : 0.030731, loss_ce: 0.013365
2022-01-08 10:18:31,952 iteration 2334 : loss : 0.027237, loss_ce: 0.011877
2022-01-08 10:18:33,419 iteration 2335 : loss : 0.040067, loss_ce: 0.018088
2022-01-08 10:18:34,849 iteration 2336 : loss : 0.030221, loss_ce: 0.015205
2022-01-08 10:18:36,215 iteration 2337 : loss : 0.042924, loss_ce: 0.015309
2022-01-08 10:18:37,610 iteration 2338 : loss : 0.035826, loss_ce: 0.013146
2022-01-08 10:18:39,003 iteration 2339 : loss : 0.037927, loss_ce: 0.013206
2022-01-08 10:18:40,305 iteration 2340 : loss : 0.027831, loss_ce: 0.011002
2022-01-08 10:18:41,693 iteration 2341 : loss : 0.038966, loss_ce: 0.017816
2022-01-08 10:18:43,097 iteration 2342 : loss : 0.032096, loss_ce: 0.013909
2022-01-08 10:18:44,585 iteration 2343 : loss : 0.025968, loss_ce: 0.009335
2022-01-08 10:18:45,895 iteration 2344 : loss : 0.051941, loss_ce: 0.014796
2022-01-08 10:18:47,220 iteration 2345 : loss : 0.046583, loss_ce: 0.022970
2022-01-08 10:18:48,567 iteration 2346 : loss : 0.044721, loss_ce: 0.021221
 34%|██████████                   | 138/400 [58:19<1:46:53, 24.48s/it]2022-01-08 10:18:50,060 iteration 2347 : loss : 0.044365, loss_ce: 0.018800
2022-01-08 10:18:51,442 iteration 2348 : loss : 0.028497, loss_ce: 0.011015
2022-01-08 10:18:52,783 iteration 2349 : loss : 0.034738, loss_ce: 0.014150
2022-01-08 10:18:54,136 iteration 2350 : loss : 0.044670, loss_ce: 0.014078
2022-01-08 10:18:55,571 iteration 2351 : loss : 0.037906, loss_ce: 0.016330
2022-01-08 10:18:56,889 iteration 2352 : loss : 0.022947, loss_ce: 0.011812
2022-01-08 10:18:58,222 iteration 2353 : loss : 0.026122, loss_ce: 0.011529
2022-01-08 10:18:59,546 iteration 2354 : loss : 0.023688, loss_ce: 0.011729
2022-01-08 10:19:00,877 iteration 2355 : loss : 0.032857, loss_ce: 0.011364
2022-01-08 10:19:02,312 iteration 2356 : loss : 0.043823, loss_ce: 0.016576
2022-01-08 10:19:03,690 iteration 2357 : loss : 0.041531, loss_ce: 0.012003
2022-01-08 10:19:05,063 iteration 2358 : loss : 0.041555, loss_ce: 0.017942
2022-01-08 10:19:06,406 iteration 2359 : loss : 0.044361, loss_ce: 0.015569
2022-01-08 10:19:07,731 iteration 2360 : loss : 0.047123, loss_ce: 0.014754
2022-01-08 10:19:09,094 iteration 2361 : loss : 0.027506, loss_ce: 0.011058
2022-01-08 10:19:10,571 iteration 2362 : loss : 0.038594, loss_ce: 0.016749
2022-01-08 10:19:11,938 iteration 2363 : loss : 0.024164, loss_ce: 0.010920
 35%|██████████                   | 139/400 [58:42<1:45:02, 24.15s/it]2022-01-08 10:19:13,419 iteration 2364 : loss : 0.026595, loss_ce: 0.009309
2022-01-08 10:19:14,851 iteration 2365 : loss : 0.037813, loss_ce: 0.015506
2022-01-08 10:19:16,245 iteration 2366 : loss : 0.038036, loss_ce: 0.017069
2022-01-08 10:19:17,549 iteration 2367 : loss : 0.034968, loss_ce: 0.009200
2022-01-08 10:19:18,856 iteration 2368 : loss : 0.029622, loss_ce: 0.013157
2022-01-08 10:19:20,237 iteration 2369 : loss : 0.045730, loss_ce: 0.015401
2022-01-08 10:19:21,598 iteration 2370 : loss : 0.061251, loss_ce: 0.023637
2022-01-08 10:19:23,005 iteration 2371 : loss : 0.026748, loss_ce: 0.009792
2022-01-08 10:19:24,331 iteration 2372 : loss : 0.030199, loss_ce: 0.010004
2022-01-08 10:19:25,687 iteration 2373 : loss : 0.040261, loss_ce: 0.013999
2022-01-08 10:19:27,001 iteration 2374 : loss : 0.034945, loss_ce: 0.014345
2022-01-08 10:19:28,338 iteration 2375 : loss : 0.030214, loss_ce: 0.010973
2022-01-08 10:19:29,727 iteration 2376 : loss : 0.051514, loss_ce: 0.023878
2022-01-08 10:19:31,107 iteration 2377 : loss : 0.030741, loss_ce: 0.013082
2022-01-08 10:19:32,484 iteration 2378 : loss : 0.023653, loss_ce: 0.009285
2022-01-08 10:19:33,881 iteration 2379 : loss : 0.039374, loss_ce: 0.016823
2022-01-08 10:19:33,881 Training Data Eval:
2022-01-08 10:19:40,757   Average segmentation loss on training set: 0.0214
2022-01-08 10:19:40,757 Validation Data Eval:
2022-01-08 10:19:43,127   Average segmentation loss on validation set: 0.0724
2022-01-08 10:19:44,464 iteration 2380 : loss : 0.022094, loss_ce: 0.010473
 35%|██████████▏                  | 140/400 [59:15<1:55:30, 26.66s/it]2022-01-08 10:19:45,824 iteration 2381 : loss : 0.034852, loss_ce: 0.012725
2022-01-08 10:19:47,228 iteration 2382 : loss : 0.029269, loss_ce: 0.009397
2022-01-08 10:19:48,562 iteration 2383 : loss : 0.023908, loss_ce: 0.008112
2022-01-08 10:19:50,006 iteration 2384 : loss : 0.040176, loss_ce: 0.017592
2022-01-08 10:19:51,448 iteration 2385 : loss : 0.036883, loss_ce: 0.013205
2022-01-08 10:19:52,799 iteration 2386 : loss : 0.025762, loss_ce: 0.007164
2022-01-08 10:19:54,144 iteration 2387 : loss : 0.033994, loss_ce: 0.011008
2022-01-08 10:19:55,511 iteration 2388 : loss : 0.036991, loss_ce: 0.016046
2022-01-08 10:19:56,815 iteration 2389 : loss : 0.016800, loss_ce: 0.005475
2022-01-08 10:19:58,196 iteration 2390 : loss : 0.033658, loss_ce: 0.016600
2022-01-08 10:19:59,541 iteration 2391 : loss : 0.038187, loss_ce: 0.023379
2022-01-08 10:20:00,931 iteration 2392 : loss : 0.047573, loss_ce: 0.017034
2022-01-08 10:20:02,318 iteration 2393 : loss : 0.044010, loss_ce: 0.015328
2022-01-08 10:20:03,602 iteration 2394 : loss : 0.028180, loss_ce: 0.009386
2022-01-08 10:20:05,018 iteration 2395 : loss : 0.034953, loss_ce: 0.017919
2022-01-08 10:20:06,332 iteration 2396 : loss : 0.026069, loss_ce: 0.011935
2022-01-08 10:20:07,727 iteration 2397 : loss : 0.029681, loss_ce: 0.011352
 35%|██████████▏                  | 141/400 [59:38<1:50:41, 25.64s/it]2022-01-08 10:20:09,135 iteration 2398 : loss : 0.037940, loss_ce: 0.013667
2022-01-08 10:20:10,464 iteration 2399 : loss : 0.030505, loss_ce: 0.010674
2022-01-08 10:20:11,889 iteration 2400 : loss : 0.028583, loss_ce: 0.014077
2022-01-08 10:20:13,242 iteration 2401 : loss : 0.023156, loss_ce: 0.008125
2022-01-08 10:20:14,641 iteration 2402 : loss : 0.035830, loss_ce: 0.017876
2022-01-08 10:20:15,965 iteration 2403 : loss : 0.031975, loss_ce: 0.013700
2022-01-08 10:20:17,280 iteration 2404 : loss : 0.031565, loss_ce: 0.015639
2022-01-08 10:20:18,577 iteration 2405 : loss : 0.027166, loss_ce: 0.010095
2022-01-08 10:20:20,074 iteration 2406 : loss : 0.050952, loss_ce: 0.014578
2022-01-08 10:20:21,368 iteration 2407 : loss : 0.033500, loss_ce: 0.017496
2022-01-08 10:20:22,683 iteration 2408 : loss : 0.021522, loss_ce: 0.009770
2022-01-08 10:20:24,101 iteration 2409 : loss : 0.025794, loss_ce: 0.008912
2022-01-08 10:20:25,524 iteration 2410 : loss : 0.042772, loss_ce: 0.017831
2022-01-08 10:20:26,920 iteration 2411 : loss : 0.050892, loss_ce: 0.015725
2022-01-08 10:20:28,246 iteration 2412 : loss : 0.020114, loss_ce: 0.007701
2022-01-08 10:20:29,683 iteration 2413 : loss : 0.077424, loss_ce: 0.036122
2022-01-08 10:20:31,056 iteration 2414 : loss : 0.033355, loss_ce: 0.013811
 36%|█████████▌                 | 142/400 [1:00:01<1:47:15, 24.95s/it]2022-01-08 10:20:32,477 iteration 2415 : loss : 0.039708, loss_ce: 0.014327
2022-01-08 10:20:33,826 iteration 2416 : loss : 0.032083, loss_ce: 0.011685
2022-01-08 10:20:35,193 iteration 2417 : loss : 0.037720, loss_ce: 0.012979
2022-01-08 10:20:36,637 iteration 2418 : loss : 0.036402, loss_ce: 0.014490
2022-01-08 10:20:38,008 iteration 2419 : loss : 0.030385, loss_ce: 0.016028
2022-01-08 10:20:39,490 iteration 2420 : loss : 0.038550, loss_ce: 0.015765
2022-01-08 10:20:40,925 iteration 2421 : loss : 0.041265, loss_ce: 0.014501
2022-01-08 10:20:42,341 iteration 2422 : loss : 0.033751, loss_ce: 0.016083
2022-01-08 10:20:43,703 iteration 2423 : loss : 0.032428, loss_ce: 0.012380
2022-01-08 10:20:45,059 iteration 2424 : loss : 0.032287, loss_ce: 0.015128
2022-01-08 10:20:46,538 iteration 2425 : loss : 0.048465, loss_ce: 0.015372
2022-01-08 10:20:47,885 iteration 2426 : loss : 0.031603, loss_ce: 0.009732
2022-01-08 10:20:49,156 iteration 2427 : loss : 0.023569, loss_ce: 0.010946
2022-01-08 10:20:50,481 iteration 2428 : loss : 0.032770, loss_ce: 0.008878
2022-01-08 10:20:51,812 iteration 2429 : loss : 0.035708, loss_ce: 0.014504
2022-01-08 10:20:53,152 iteration 2430 : loss : 0.017799, loss_ce: 0.006834
2022-01-08 10:20:54,520 iteration 2431 : loss : 0.036186, loss_ce: 0.021087
 36%|█████████▋                 | 143/400 [1:00:25<1:44:56, 24.50s/it]2022-01-08 10:20:55,891 iteration 2432 : loss : 0.029979, loss_ce: 0.009817
2022-01-08 10:20:57,251 iteration 2433 : loss : 0.031238, loss_ce: 0.013331
2022-01-08 10:20:58,632 iteration 2434 : loss : 0.033005, loss_ce: 0.011541
2022-01-08 10:21:00,083 iteration 2435 : loss : 0.055777, loss_ce: 0.036571
2022-01-08 10:21:01,368 iteration 2436 : loss : 0.025979, loss_ce: 0.011306
2022-01-08 10:21:02,784 iteration 2437 : loss : 0.025822, loss_ce: 0.008301
2022-01-08 10:21:04,203 iteration 2438 : loss : 0.045377, loss_ce: 0.013417
2022-01-08 10:21:05,596 iteration 2439 : loss : 0.048949, loss_ce: 0.016788
2022-01-08 10:21:07,010 iteration 2440 : loss : 0.031236, loss_ce: 0.011656
2022-01-08 10:21:08,364 iteration 2441 : loss : 0.029473, loss_ce: 0.010200
2022-01-08 10:21:09,709 iteration 2442 : loss : 0.025950, loss_ce: 0.010967
2022-01-08 10:21:11,007 iteration 2443 : loss : 0.029195, loss_ce: 0.011275
2022-01-08 10:21:12,375 iteration 2444 : loss : 0.030263, loss_ce: 0.011245
2022-01-08 10:21:13,752 iteration 2445 : loss : 0.033821, loss_ce: 0.013956
2022-01-08 10:21:15,194 iteration 2446 : loss : 0.035283, loss_ce: 0.015249
2022-01-08 10:21:16,499 iteration 2447 : loss : 0.025303, loss_ce: 0.008561
2022-01-08 10:21:17,920 iteration 2448 : loss : 0.033755, loss_ce: 0.013142
 36%|█████████▋                 | 144/400 [1:00:48<1:43:07, 24.17s/it]2022-01-08 10:21:19,254 iteration 2449 : loss : 0.032523, loss_ce: 0.012115
2022-01-08 10:21:20,558 iteration 2450 : loss : 0.026127, loss_ce: 0.013214
2022-01-08 10:21:21,903 iteration 2451 : loss : 0.033462, loss_ce: 0.011752
2022-01-08 10:21:23,226 iteration 2452 : loss : 0.035166, loss_ce: 0.011098
2022-01-08 10:21:24,605 iteration 2453 : loss : 0.040613, loss_ce: 0.013193
2022-01-08 10:21:25,949 iteration 2454 : loss : 0.033139, loss_ce: 0.012049
2022-01-08 10:21:27,275 iteration 2455 : loss : 0.033477, loss_ce: 0.016295
2022-01-08 10:21:28,589 iteration 2456 : loss : 0.029814, loss_ce: 0.014278
2022-01-08 10:21:29,934 iteration 2457 : loss : 0.028914, loss_ce: 0.010691
2022-01-08 10:21:31,250 iteration 2458 : loss : 0.029849, loss_ce: 0.011616
2022-01-08 10:21:32,522 iteration 2459 : loss : 0.027210, loss_ce: 0.010469
2022-01-08 10:21:33,868 iteration 2460 : loss : 0.042138, loss_ce: 0.009761
2022-01-08 10:21:35,322 iteration 2461 : loss : 0.038829, loss_ce: 0.014199
2022-01-08 10:21:36,694 iteration 2462 : loss : 0.045799, loss_ce: 0.017490
2022-01-08 10:21:38,029 iteration 2463 : loss : 0.031705, loss_ce: 0.013922
2022-01-08 10:21:39,426 iteration 2464 : loss : 0.025357, loss_ce: 0.009389
2022-01-08 10:21:39,427 Training Data Eval:
2022-01-08 10:21:46,327   Average segmentation loss on training set: 0.0276
2022-01-08 10:21:46,327 Validation Data Eval:
2022-01-08 10:21:48,698   Average segmentation loss on validation set: 0.1183
2022-01-08 10:21:50,057 iteration 2465 : loss : 0.034634, loss_ce: 0.014329
 36%|█████████▊                 | 145/400 [1:01:20<1:52:53, 26.56s/it]2022-01-08 10:21:51,532 iteration 2466 : loss : 0.040510, loss_ce: 0.015225
2022-01-08 10:21:52,970 iteration 2467 : loss : 0.035991, loss_ce: 0.014931
2022-01-08 10:21:54,302 iteration 2468 : loss : 0.039023, loss_ce: 0.017393
2022-01-08 10:21:55,615 iteration 2469 : loss : 0.022903, loss_ce: 0.010147
2022-01-08 10:21:56,973 iteration 2470 : loss : 0.036142, loss_ce: 0.012253
2022-01-08 10:21:58,419 iteration 2471 : loss : 0.045285, loss_ce: 0.014812
2022-01-08 10:21:59,754 iteration 2472 : loss : 0.042354, loss_ce: 0.017625
2022-01-08 10:22:01,187 iteration 2473 : loss : 0.039001, loss_ce: 0.016645
2022-01-08 10:22:02,526 iteration 2474 : loss : 0.027283, loss_ce: 0.009424
2022-01-08 10:22:03,893 iteration 2475 : loss : 0.037026, loss_ce: 0.014252
2022-01-08 10:22:05,209 iteration 2476 : loss : 0.037319, loss_ce: 0.018501
2022-01-08 10:22:06,662 iteration 2477 : loss : 0.040733, loss_ce: 0.014764
2022-01-08 10:22:08,054 iteration 2478 : loss : 0.029954, loss_ce: 0.011296
2022-01-08 10:22:09,340 iteration 2479 : loss : 0.030411, loss_ce: 0.010329
2022-01-08 10:22:10,707 iteration 2480 : loss : 0.030676, loss_ce: 0.012104
2022-01-08 10:22:12,133 iteration 2481 : loss : 0.021899, loss_ce: 0.008420
2022-01-08 10:22:13,472 iteration 2482 : loss : 0.026371, loss_ce: 0.012913
 36%|█████████▊                 | 146/400 [1:01:44<1:48:26, 25.62s/it]2022-01-08 10:22:14,952 iteration 2483 : loss : 0.050483, loss_ce: 0.023600
2022-01-08 10:22:16,348 iteration 2484 : loss : 0.045541, loss_ce: 0.008402
2022-01-08 10:22:17,667 iteration 2485 : loss : 0.031392, loss_ce: 0.013291
2022-01-08 10:22:19,020 iteration 2486 : loss : 0.027602, loss_ce: 0.010835
2022-01-08 10:22:20,374 iteration 2487 : loss : 0.036578, loss_ce: 0.012270
2022-01-08 10:22:21,800 iteration 2488 : loss : 0.032933, loss_ce: 0.014402
2022-01-08 10:22:23,149 iteration 2489 : loss : 0.029283, loss_ce: 0.012540
2022-01-08 10:22:24,537 iteration 2490 : loss : 0.034680, loss_ce: 0.014629
2022-01-08 10:22:25,962 iteration 2491 : loss : 0.026379, loss_ce: 0.010747
2022-01-08 10:22:27,236 iteration 2492 : loss : 0.024173, loss_ce: 0.009279
2022-01-08 10:22:28,637 iteration 2493 : loss : 0.028631, loss_ce: 0.012784
2022-01-08 10:22:29,985 iteration 2494 : loss : 0.031492, loss_ce: 0.009087
2022-01-08 10:22:31,324 iteration 2495 : loss : 0.026554, loss_ce: 0.012398
2022-01-08 10:22:32,863 iteration 2496 : loss : 0.053224, loss_ce: 0.015529
2022-01-08 10:22:34,257 iteration 2497 : loss : 0.038656, loss_ce: 0.013519
2022-01-08 10:22:35,662 iteration 2498 : loss : 0.044375, loss_ce: 0.019755
2022-01-08 10:22:36,972 iteration 2499 : loss : 0.021871, loss_ce: 0.005698
 37%|█████████▉                 | 147/400 [1:02:07<1:45:20, 24.98s/it]2022-01-08 10:22:38,367 iteration 2500 : loss : 0.027575, loss_ce: 0.009875
2022-01-08 10:22:39,723 iteration 2501 : loss : 0.030503, loss_ce: 0.010126
2022-01-08 10:22:41,160 iteration 2502 : loss : 0.056173, loss_ce: 0.023577
2022-01-08 10:22:42,468 iteration 2503 : loss : 0.026582, loss_ce: 0.010078
2022-01-08 10:22:43,806 iteration 2504 : loss : 0.029862, loss_ce: 0.010388
2022-01-08 10:22:45,219 iteration 2505 : loss : 0.045282, loss_ce: 0.018584
2022-01-08 10:22:46,582 iteration 2506 : loss : 0.041377, loss_ce: 0.013161
2022-01-08 10:22:47,892 iteration 2507 : loss : 0.032927, loss_ce: 0.012436
2022-01-08 10:22:49,231 iteration 2508 : loss : 0.033481, loss_ce: 0.012363
2022-01-08 10:22:50,585 iteration 2509 : loss : 0.032722, loss_ce: 0.011037
2022-01-08 10:22:51,916 iteration 2510 : loss : 0.026266, loss_ce: 0.011060
2022-01-08 10:22:53,287 iteration 2511 : loss : 0.031486, loss_ce: 0.013929
2022-01-08 10:22:54,667 iteration 2512 : loss : 0.039413, loss_ce: 0.014184
2022-01-08 10:22:56,085 iteration 2513 : loss : 0.044664, loss_ce: 0.016561
2022-01-08 10:22:57,459 iteration 2514 : loss : 0.041922, loss_ce: 0.011259
2022-01-08 10:22:58,808 iteration 2515 : loss : 0.022242, loss_ce: 0.012688
2022-01-08 10:23:00,162 iteration 2516 : loss : 0.025776, loss_ce: 0.012088
 37%|█████████▉                 | 148/400 [1:02:30<1:42:40, 24.44s/it]2022-01-08 10:23:01,548 iteration 2517 : loss : 0.026802, loss_ce: 0.010678
2022-01-08 10:23:02,918 iteration 2518 : loss : 0.037112, loss_ce: 0.010357
2022-01-08 10:23:04,327 iteration 2519 : loss : 0.041692, loss_ce: 0.014699
2022-01-08 10:23:05,700 iteration 2520 : loss : 0.037248, loss_ce: 0.014060
2022-01-08 10:23:07,053 iteration 2521 : loss : 0.029068, loss_ce: 0.010636
2022-01-08 10:23:08,431 iteration 2522 : loss : 0.032069, loss_ce: 0.016970
2022-01-08 10:23:09,848 iteration 2523 : loss : 0.040218, loss_ce: 0.010007
2022-01-08 10:23:11,190 iteration 2524 : loss : 0.023906, loss_ce: 0.010611
2022-01-08 10:23:12,566 iteration 2525 : loss : 0.026985, loss_ce: 0.011094
2022-01-08 10:23:13,947 iteration 2526 : loss : 0.038844, loss_ce: 0.013087
2022-01-08 10:23:15,322 iteration 2527 : loss : 0.026803, loss_ce: 0.010401
2022-01-08 10:23:16,730 iteration 2528 : loss : 0.035368, loss_ce: 0.011968
2022-01-08 10:23:18,111 iteration 2529 : loss : 0.051732, loss_ce: 0.020666
2022-01-08 10:23:19,488 iteration 2530 : loss : 0.027662, loss_ce: 0.013245
2022-01-08 10:23:20,910 iteration 2531 : loss : 0.025950, loss_ce: 0.010284
2022-01-08 10:23:22,269 iteration 2532 : loss : 0.037478, loss_ce: 0.012720
2022-01-08 10:23:23,645 iteration 2533 : loss : 0.046593, loss_ce: 0.013062
 37%|██████████                 | 149/400 [1:02:54<1:41:02, 24.15s/it]2022-01-08 10:23:24,977 iteration 2534 : loss : 0.035705, loss_ce: 0.016670
2022-01-08 10:23:26,410 iteration 2535 : loss : 0.049067, loss_ce: 0.017084
2022-01-08 10:23:27,742 iteration 2536 : loss : 0.031755, loss_ce: 0.014414
2022-01-08 10:23:29,078 iteration 2537 : loss : 0.026122, loss_ce: 0.010162
2022-01-08 10:23:30,335 iteration 2538 : loss : 0.024826, loss_ce: 0.011288
2022-01-08 10:23:31,713 iteration 2539 : loss : 0.025246, loss_ce: 0.009631
2022-01-08 10:23:33,037 iteration 2540 : loss : 0.020850, loss_ce: 0.006536
2022-01-08 10:23:34,390 iteration 2541 : loss : 0.035434, loss_ce: 0.015011
2022-01-08 10:23:35,693 iteration 2542 : loss : 0.023850, loss_ce: 0.008960
2022-01-08 10:23:37,010 iteration 2543 : loss : 0.026192, loss_ce: 0.009212
2022-01-08 10:23:38,370 iteration 2544 : loss : 0.027338, loss_ce: 0.010401
2022-01-08 10:23:39,843 iteration 2545 : loss : 0.030404, loss_ce: 0.011679
2022-01-08 10:23:41,327 iteration 2546 : loss : 0.049771, loss_ce: 0.020209
2022-01-08 10:23:42,626 iteration 2547 : loss : 0.022140, loss_ce: 0.010986
2022-01-08 10:23:44,009 iteration 2548 : loss : 0.027525, loss_ce: 0.013002
2022-01-08 10:23:45,389 iteration 2549 : loss : 0.032756, loss_ce: 0.010008
2022-01-08 10:23:45,390 Training Data Eval:
2022-01-08 10:23:52,266   Average segmentation loss on training set: 0.0219
2022-01-08 10:23:52,267 Validation Data Eval:
2022-01-08 10:23:54,638   Average segmentation loss on validation set: 0.0744
2022-01-08 10:23:55,987 iteration 2550 : loss : 0.024352, loss_ce: 0.007559
 38%|██████████▏                | 150/400 [1:03:26<1:50:53, 26.61s/it]2022-01-08 10:23:57,379 iteration 2551 : loss : 0.025336, loss_ce: 0.012355
2022-01-08 10:23:58,800 iteration 2552 : loss : 0.025471, loss_ce: 0.011252
2022-01-08 10:24:00,082 iteration 2553 : loss : 0.020432, loss_ce: 0.006940
2022-01-08 10:24:01,450 iteration 2554 : loss : 0.037784, loss_ce: 0.016591
2022-01-08 10:24:02,802 iteration 2555 : loss : 0.025778, loss_ce: 0.006786
2022-01-08 10:24:04,222 iteration 2556 : loss : 0.056649, loss_ce: 0.034131
2022-01-08 10:24:05,566 iteration 2557 : loss : 0.036215, loss_ce: 0.017229
2022-01-08 10:24:06,938 iteration 2558 : loss : 0.030286, loss_ce: 0.016859
2022-01-08 10:24:08,384 iteration 2559 : loss : 0.025856, loss_ce: 0.010083
2022-01-08 10:24:09,745 iteration 2560 : loss : 0.025791, loss_ce: 0.013478
2022-01-08 10:24:11,196 iteration 2561 : loss : 0.038466, loss_ce: 0.014507
2022-01-08 10:24:12,569 iteration 2562 : loss : 0.029442, loss_ce: 0.012553
2022-01-08 10:24:13,960 iteration 2563 : loss : 0.047793, loss_ce: 0.026198
2022-01-08 10:24:15,281 iteration 2564 : loss : 0.035086, loss_ce: 0.013362
2022-01-08 10:24:16,613 iteration 2565 : loss : 0.028662, loss_ce: 0.010919
2022-01-08 10:24:17,963 iteration 2566 : loss : 0.031863, loss_ce: 0.013186
2022-01-08 10:24:19,337 iteration 2567 : loss : 0.055564, loss_ce: 0.021350
 38%|██████████▏                | 151/400 [1:03:50<1:46:22, 25.63s/it]2022-01-08 10:24:20,757 iteration 2568 : loss : 0.030824, loss_ce: 0.014476
2022-01-08 10:24:22,109 iteration 2569 : loss : 0.024986, loss_ce: 0.012608
2022-01-08 10:24:23,465 iteration 2570 : loss : 0.038722, loss_ce: 0.016623
2022-01-08 10:24:24,824 iteration 2571 : loss : 0.020443, loss_ce: 0.007945
2022-01-08 10:24:26,126 iteration 2572 : loss : 0.024507, loss_ce: 0.008178
2022-01-08 10:24:27,449 iteration 2573 : loss : 0.030957, loss_ce: 0.012572
2022-01-08 10:24:28,793 iteration 2574 : loss : 0.037249, loss_ce: 0.013187
2022-01-08 10:24:30,263 iteration 2575 : loss : 0.038045, loss_ce: 0.015044
2022-01-08 10:24:31,639 iteration 2576 : loss : 0.026367, loss_ce: 0.009962
2022-01-08 10:24:33,004 iteration 2577 : loss : 0.031948, loss_ce: 0.013081
2022-01-08 10:24:34,386 iteration 2578 : loss : 0.031585, loss_ce: 0.010442
2022-01-08 10:24:35,692 iteration 2579 : loss : 0.030172, loss_ce: 0.007011
2022-01-08 10:24:37,104 iteration 2580 : loss : 0.028928, loss_ce: 0.007444
2022-01-08 10:24:38,500 iteration 2581 : loss : 0.038328, loss_ce: 0.015527
2022-01-08 10:24:39,930 iteration 2582 : loss : 0.043230, loss_ce: 0.016284
2022-01-08 10:24:41,287 iteration 2583 : loss : 0.021266, loss_ce: 0.007851
2022-01-08 10:24:42,581 iteration 2584 : loss : 0.019433, loss_ce: 0.007695
 38%|██████████▎                | 152/400 [1:04:13<1:42:59, 24.92s/it]2022-01-08 10:24:43,998 iteration 2585 : loss : 0.020123, loss_ce: 0.006369
2022-01-08 10:24:45,379 iteration 2586 : loss : 0.046170, loss_ce: 0.021023
2022-01-08 10:24:46,685 iteration 2587 : loss : 0.023217, loss_ce: 0.008712
2022-01-08 10:24:48,027 iteration 2588 : loss : 0.028352, loss_ce: 0.014128
2022-01-08 10:24:49,427 iteration 2589 : loss : 0.029625, loss_ce: 0.010127
2022-01-08 10:24:50,823 iteration 2590 : loss : 0.023240, loss_ce: 0.009482
2022-01-08 10:24:52,151 iteration 2591 : loss : 0.023134, loss_ce: 0.007796
2022-01-08 10:24:53,481 iteration 2592 : loss : 0.027405, loss_ce: 0.009206
2022-01-08 10:24:54,824 iteration 2593 : loss : 0.025309, loss_ce: 0.011831
2022-01-08 10:24:56,314 iteration 2594 : loss : 0.036150, loss_ce: 0.015739
2022-01-08 10:24:57,713 iteration 2595 : loss : 0.052543, loss_ce: 0.024103
2022-01-08 10:24:59,038 iteration 2596 : loss : 0.028112, loss_ce: 0.013368
2022-01-08 10:25:00,346 iteration 2597 : loss : 0.032239, loss_ce: 0.013027
2022-01-08 10:25:01,683 iteration 2598 : loss : 0.022916, loss_ce: 0.009584
2022-01-08 10:25:03,038 iteration 2599 : loss : 0.023780, loss_ce: 0.007178
2022-01-08 10:25:04,394 iteration 2600 : loss : 0.035734, loss_ce: 0.012734
2022-01-08 10:25:05,705 iteration 2601 : loss : 0.024038, loss_ce: 0.009888
 38%|██████████▎                | 153/400 [1:04:36<1:40:21, 24.38s/it]2022-01-08 10:25:07,136 iteration 2602 : loss : 0.022665, loss_ce: 0.010279
2022-01-08 10:25:08,417 iteration 2603 : loss : 0.021807, loss_ce: 0.009037
2022-01-08 10:25:09,765 iteration 2604 : loss : 0.025131, loss_ce: 0.010026
2022-01-08 10:25:11,248 iteration 2605 : loss : 0.033242, loss_ce: 0.011300
2022-01-08 10:25:12,603 iteration 2606 : loss : 0.023780, loss_ce: 0.009817
2022-01-08 10:25:13,954 iteration 2607 : loss : 0.027937, loss_ce: 0.010588
2022-01-08 10:25:15,257 iteration 2608 : loss : 0.028993, loss_ce: 0.015052
2022-01-08 10:25:16,587 iteration 2609 : loss : 0.028484, loss_ce: 0.009095
2022-01-08 10:25:18,064 iteration 2610 : loss : 0.039512, loss_ce: 0.016825
2022-01-08 10:25:19,387 iteration 2611 : loss : 0.031825, loss_ce: 0.010437
2022-01-08 10:25:20,653 iteration 2612 : loss : 0.026184, loss_ce: 0.013900
2022-01-08 10:25:21,995 iteration 2613 : loss : 0.023905, loss_ce: 0.007792
2022-01-08 10:25:23,413 iteration 2614 : loss : 0.037748, loss_ce: 0.012649
2022-01-08 10:25:24,737 iteration 2615 : loss : 0.026603, loss_ce: 0.010760
2022-01-08 10:25:26,058 iteration 2616 : loss : 0.027982, loss_ce: 0.010810
2022-01-08 10:25:27,496 iteration 2617 : loss : 0.031935, loss_ce: 0.007934
2022-01-08 10:25:28,875 iteration 2618 : loss : 0.037865, loss_ce: 0.018584
 38%|██████████▍                | 154/400 [1:04:59<1:38:27, 24.02s/it]2022-01-08 10:25:30,344 iteration 2619 : loss : 0.031327, loss_ce: 0.010189
2022-01-08 10:25:31,692 iteration 2620 : loss : 0.029374, loss_ce: 0.010693
2022-01-08 10:25:33,093 iteration 2621 : loss : 0.031693, loss_ce: 0.011553
2022-01-08 10:25:34,478 iteration 2622 : loss : 0.024785, loss_ce: 0.012387
2022-01-08 10:25:35,870 iteration 2623 : loss : 0.045278, loss_ce: 0.020757
2022-01-08 10:25:37,194 iteration 2624 : loss : 0.024876, loss_ce: 0.008053
2022-01-08 10:25:38,538 iteration 2625 : loss : 0.020203, loss_ce: 0.008012
2022-01-08 10:25:39,968 iteration 2626 : loss : 0.050995, loss_ce: 0.032469
2022-01-08 10:25:41,356 iteration 2627 : loss : 0.037681, loss_ce: 0.009827
2022-01-08 10:25:42,671 iteration 2628 : loss : 0.025837, loss_ce: 0.008970
2022-01-08 10:25:44,081 iteration 2629 : loss : 0.035380, loss_ce: 0.012245
2022-01-08 10:25:45,379 iteration 2630 : loss : 0.023654, loss_ce: 0.007795
2022-01-08 10:25:46,753 iteration 2631 : loss : 0.046149, loss_ce: 0.024348
2022-01-08 10:25:48,136 iteration 2632 : loss : 0.027075, loss_ce: 0.011031
2022-01-08 10:25:49,466 iteration 2633 : loss : 0.026683, loss_ce: 0.013333
2022-01-08 10:25:50,852 iteration 2634 : loss : 0.028316, loss_ce: 0.009505
2022-01-08 10:25:50,852 Training Data Eval:
2022-01-08 10:25:57,757   Average segmentation loss on training set: 0.0213
2022-01-08 10:25:57,757 Validation Data Eval:
2022-01-08 10:26:00,129   Average segmentation loss on validation set: 0.0725
2022-01-08 10:26:01,559 iteration 2635 : loss : 0.038873, loss_ce: 0.016050
 39%|██████████▍                | 155/400 [1:05:32<1:48:40, 26.61s/it]2022-01-08 10:26:02,916 iteration 2636 : loss : 0.023331, loss_ce: 0.007200
2022-01-08 10:26:04,406 iteration 2637 : loss : 0.043220, loss_ce: 0.014169
2022-01-08 10:26:05,774 iteration 2638 : loss : 0.023761, loss_ce: 0.008919
2022-01-08 10:26:07,028 iteration 2639 : loss : 0.021490, loss_ce: 0.007871
2022-01-08 10:26:08,344 iteration 2640 : loss : 0.030170, loss_ce: 0.011978
2022-01-08 10:26:09,786 iteration 2641 : loss : 0.036927, loss_ce: 0.014247
2022-01-08 10:26:11,193 iteration 2642 : loss : 0.047367, loss_ce: 0.014137
2022-01-08 10:26:12,576 iteration 2643 : loss : 0.030076, loss_ce: 0.010543
2022-01-08 10:26:13,902 iteration 2644 : loss : 0.038620, loss_ce: 0.011167
2022-01-08 10:26:15,220 iteration 2645 : loss : 0.028440, loss_ce: 0.015269
2022-01-08 10:26:16,569 iteration 2646 : loss : 0.029435, loss_ce: 0.009251
2022-01-08 10:26:17,981 iteration 2647 : loss : 0.040980, loss_ce: 0.022651
2022-01-08 10:26:19,271 iteration 2648 : loss : 0.018244, loss_ce: 0.008543
2022-01-08 10:26:20,682 iteration 2649 : loss : 0.057041, loss_ce: 0.029560
2022-01-08 10:26:22,080 iteration 2650 : loss : 0.033485, loss_ce: 0.016069
2022-01-08 10:26:23,413 iteration 2651 : loss : 0.044285, loss_ce: 0.017506
2022-01-08 10:26:24,784 iteration 2652 : loss : 0.024540, loss_ce: 0.011666
 39%|██████████▌                | 156/400 [1:05:55<1:44:06, 25.60s/it]2022-01-08 10:26:26,284 iteration 2653 : loss : 0.045424, loss_ce: 0.017101
2022-01-08 10:26:27,634 iteration 2654 : loss : 0.026390, loss_ce: 0.010455
2022-01-08 10:26:28,927 iteration 2655 : loss : 0.021620, loss_ce: 0.008331
2022-01-08 10:26:30,216 iteration 2656 : loss : 0.024248, loss_ce: 0.011789
2022-01-08 10:26:31,536 iteration 2657 : loss : 0.024254, loss_ce: 0.008588
2022-01-08 10:26:32,865 iteration 2658 : loss : 0.034228, loss_ce: 0.014783
2022-01-08 10:26:34,340 iteration 2659 : loss : 0.068456, loss_ce: 0.032344
2022-01-08 10:26:35,687 iteration 2660 : loss : 0.032586, loss_ce: 0.012981
2022-01-08 10:26:36,967 iteration 2661 : loss : 0.025889, loss_ce: 0.010499
2022-01-08 10:26:38,373 iteration 2662 : loss : 0.033660, loss_ce: 0.013602
2022-01-08 10:26:39,689 iteration 2663 : loss : 0.038173, loss_ce: 0.015707
2022-01-08 10:26:41,106 iteration 2664 : loss : 0.027050, loss_ce: 0.009840
2022-01-08 10:26:42,431 iteration 2665 : loss : 0.026794, loss_ce: 0.010531
2022-01-08 10:26:43,800 iteration 2666 : loss : 0.039598, loss_ce: 0.012709
2022-01-08 10:26:45,196 iteration 2667 : loss : 0.026463, loss_ce: 0.009884
2022-01-08 10:26:46,581 iteration 2668 : loss : 0.050894, loss_ce: 0.020071
2022-01-08 10:26:47,985 iteration 2669 : loss : 0.032763, loss_ce: 0.013527
 39%|██████████▌                | 157/400 [1:06:18<1:40:45, 24.88s/it]2022-01-08 10:26:49,392 iteration 2670 : loss : 0.051923, loss_ce: 0.014223
2022-01-08 10:26:50,770 iteration 2671 : loss : 0.059475, loss_ce: 0.017744
2022-01-08 10:26:52,169 iteration 2672 : loss : 0.026142, loss_ce: 0.009656
2022-01-08 10:26:53,577 iteration 2673 : loss : 0.068647, loss_ce: 0.033503
2022-01-08 10:26:54,905 iteration 2674 : loss : 0.030128, loss_ce: 0.014850
2022-01-08 10:26:56,324 iteration 2675 : loss : 0.040632, loss_ce: 0.015370
2022-01-08 10:26:57,704 iteration 2676 : loss : 0.049550, loss_ce: 0.015950
2022-01-08 10:26:58,998 iteration 2677 : loss : 0.023732, loss_ce: 0.011071
2022-01-08 10:27:00,285 iteration 2678 : loss : 0.029428, loss_ce: 0.010084
2022-01-08 10:27:01,671 iteration 2679 : loss : 0.039117, loss_ce: 0.019841
2022-01-08 10:27:03,088 iteration 2680 : loss : 0.034904, loss_ce: 0.015400
2022-01-08 10:27:04,432 iteration 2681 : loss : 0.027687, loss_ce: 0.012968
2022-01-08 10:27:05,745 iteration 2682 : loss : 0.028311, loss_ce: 0.011781
2022-01-08 10:27:07,138 iteration 2683 : loss : 0.029320, loss_ce: 0.014905
2022-01-08 10:27:08,516 iteration 2684 : loss : 0.055755, loss_ce: 0.019743
2022-01-08 10:27:09,881 iteration 2685 : loss : 0.028652, loss_ce: 0.013033
2022-01-08 10:27:11,230 iteration 2686 : loss : 0.034707, loss_ce: 0.012621
 40%|██████████▋                | 158/400 [1:06:41<1:38:22, 24.39s/it]2022-01-08 10:27:12,591 iteration 2687 : loss : 0.019549, loss_ce: 0.009023
2022-01-08 10:27:13,928 iteration 2688 : loss : 0.035539, loss_ce: 0.013628
2022-01-08 10:27:15,186 iteration 2689 : loss : 0.022142, loss_ce: 0.008562
2022-01-08 10:27:16,546 iteration 2690 : loss : 0.027510, loss_ce: 0.012356
2022-01-08 10:27:17,911 iteration 2691 : loss : 0.031849, loss_ce: 0.010032
2022-01-08 10:27:19,258 iteration 2692 : loss : 0.028671, loss_ce: 0.010208
2022-01-08 10:27:20,576 iteration 2693 : loss : 0.027382, loss_ce: 0.010582
2022-01-08 10:27:21,853 iteration 2694 : loss : 0.026836, loss_ce: 0.009485
2022-01-08 10:27:23,256 iteration 2695 : loss : 0.037992, loss_ce: 0.015160
2022-01-08 10:27:24,714 iteration 2696 : loss : 0.038138, loss_ce: 0.015446
2022-01-08 10:27:26,125 iteration 2697 : loss : 0.025486, loss_ce: 0.013500
2022-01-08 10:27:27,427 iteration 2698 : loss : 0.023278, loss_ce: 0.009370
2022-01-08 10:27:28,744 iteration 2699 : loss : 0.026193, loss_ce: 0.009152
2022-01-08 10:27:30,061 iteration 2700 : loss : 0.031919, loss_ce: 0.009367
2022-01-08 10:27:31,423 iteration 2701 : loss : 0.034852, loss_ce: 0.016706
2022-01-08 10:27:32,817 iteration 2702 : loss : 0.048704, loss_ce: 0.010266
2022-01-08 10:27:34,206 iteration 2703 : loss : 0.029608, loss_ce: 0.011894
 40%|██████████▋                | 159/400 [1:07:04<1:36:15, 23.96s/it]2022-01-08 10:27:35,548 iteration 2704 : loss : 0.023849, loss_ce: 0.007283
2022-01-08 10:27:36,852 iteration 2705 : loss : 0.028721, loss_ce: 0.010454
2022-01-08 10:27:38,243 iteration 2706 : loss : 0.046346, loss_ce: 0.024186
2022-01-08 10:27:39,587 iteration 2707 : loss : 0.025331, loss_ce: 0.008580
2022-01-08 10:27:40,922 iteration 2708 : loss : 0.034104, loss_ce: 0.010844
2022-01-08 10:27:42,339 iteration 2709 : loss : 0.046110, loss_ce: 0.015110
2022-01-08 10:27:43,752 iteration 2710 : loss : 0.042590, loss_ce: 0.016590
2022-01-08 10:27:45,085 iteration 2711 : loss : 0.024188, loss_ce: 0.008417
2022-01-08 10:27:46,429 iteration 2712 : loss : 0.036228, loss_ce: 0.013410
2022-01-08 10:27:47,790 iteration 2713 : loss : 0.043753, loss_ce: 0.015216
2022-01-08 10:27:49,158 iteration 2714 : loss : 0.031334, loss_ce: 0.012549
2022-01-08 10:27:50,501 iteration 2715 : loss : 0.028504, loss_ce: 0.016756
2022-01-08 10:27:51,842 iteration 2716 : loss : 0.049885, loss_ce: 0.012309
2022-01-08 10:27:53,147 iteration 2717 : loss : 0.029348, loss_ce: 0.010483
2022-01-08 10:27:54,528 iteration 2718 : loss : 0.036167, loss_ce: 0.014228
2022-01-08 10:27:55,862 iteration 2719 : loss : 0.025859, loss_ce: 0.010622
2022-01-08 10:27:55,862 Training Data Eval:
2022-01-08 10:28:02,751   Average segmentation loss on training set: 0.0234
2022-01-08 10:28:02,751 Validation Data Eval:
2022-01-08 10:28:05,127   Average segmentation loss on validation set: 0.0740
2022-01-08 10:28:06,490 iteration 2720 : loss : 0.031818, loss_ce: 0.014577
 40%|██████████▊                | 160/400 [1:07:37<1:45:49, 26.46s/it]2022-01-08 10:28:07,898 iteration 2721 : loss : 0.028713, loss_ce: 0.010374
2022-01-08 10:28:09,284 iteration 2722 : loss : 0.023254, loss_ce: 0.010672
2022-01-08 10:28:10,714 iteration 2723 : loss : 0.029025, loss_ce: 0.013347
2022-01-08 10:28:12,010 iteration 2724 : loss : 0.030429, loss_ce: 0.011723
2022-01-08 10:28:13,350 iteration 2725 : loss : 0.043448, loss_ce: 0.015883
2022-01-08 10:28:14,779 iteration 2726 : loss : 0.023839, loss_ce: 0.008909
2022-01-08 10:28:16,132 iteration 2727 : loss : 0.034840, loss_ce: 0.013875
2022-01-08 10:28:17,536 iteration 2728 : loss : 0.025058, loss_ce: 0.009879
2022-01-08 10:28:18,923 iteration 2729 : loss : 0.046711, loss_ce: 0.021230
2022-01-08 10:28:20,340 iteration 2730 : loss : 0.044757, loss_ce: 0.019114
2022-01-08 10:28:21,688 iteration 2731 : loss : 0.048704, loss_ce: 0.019525
2022-01-08 10:28:22,955 iteration 2732 : loss : 0.021865, loss_ce: 0.008266
2022-01-08 10:28:24,307 iteration 2733 : loss : 0.022025, loss_ce: 0.010384
2022-01-08 10:28:25,675 iteration 2734 : loss : 0.037067, loss_ce: 0.012193
2022-01-08 10:28:27,044 iteration 2735 : loss : 0.032838, loss_ce: 0.017201
2022-01-08 10:28:28,390 iteration 2736 : loss : 0.027187, loss_ce: 0.008680
2022-01-08 10:28:29,735 iteration 2737 : loss : 0.031570, loss_ce: 0.008254
 40%|██████████▊                | 161/400 [1:08:00<1:41:33, 25.50s/it]2022-01-08 10:28:31,172 iteration 2738 : loss : 0.045426, loss_ce: 0.023322
2022-01-08 10:28:32,485 iteration 2739 : loss : 0.030633, loss_ce: 0.008884
2022-01-08 10:28:33,932 iteration 2740 : loss : 0.050602, loss_ce: 0.022391
2022-01-08 10:28:35,282 iteration 2741 : loss : 0.028945, loss_ce: 0.011845
2022-01-08 10:28:36,652 iteration 2742 : loss : 0.051273, loss_ce: 0.024367
2022-01-08 10:28:37,951 iteration 2743 : loss : 0.030889, loss_ce: 0.012098
2022-01-08 10:28:39,301 iteration 2744 : loss : 0.022775, loss_ce: 0.007587
2022-01-08 10:28:40,618 iteration 2745 : loss : 0.023430, loss_ce: 0.007309
2022-01-08 10:28:42,080 iteration 2746 : loss : 0.067909, loss_ce: 0.016377
2022-01-08 10:28:43,409 iteration 2747 : loss : 0.027040, loss_ce: 0.010768
2022-01-08 10:28:44,836 iteration 2748 : loss : 0.028153, loss_ce: 0.007843
2022-01-08 10:28:46,292 iteration 2749 : loss : 0.034347, loss_ce: 0.013074
2022-01-08 10:28:47,630 iteration 2750 : loss : 0.038145, loss_ce: 0.015286
2022-01-08 10:28:48,985 iteration 2751 : loss : 0.039945, loss_ce: 0.011069
2022-01-08 10:28:50,436 iteration 2752 : loss : 0.031207, loss_ce: 0.012106
2022-01-08 10:28:51,886 iteration 2753 : loss : 0.037379, loss_ce: 0.016012
2022-01-08 10:28:53,247 iteration 2754 : loss : 0.023201, loss_ce: 0.011717
 40%|██████████▉                | 162/400 [1:08:23<1:38:45, 24.90s/it]2022-01-08 10:28:54,519 iteration 2755 : loss : 0.022384, loss_ce: 0.007314
2022-01-08 10:28:55,898 iteration 2756 : loss : 0.026476, loss_ce: 0.009928
2022-01-08 10:28:57,260 iteration 2757 : loss : 0.024002, loss_ce: 0.009673
2022-01-08 10:28:58,621 iteration 2758 : loss : 0.029891, loss_ce: 0.011507
2022-01-08 10:28:59,981 iteration 2759 : loss : 0.031576, loss_ce: 0.011706
2022-01-08 10:29:01,342 iteration 2760 : loss : 0.030022, loss_ce: 0.013325
2022-01-08 10:29:02,740 iteration 2761 : loss : 0.032134, loss_ce: 0.011336
2022-01-08 10:29:04,084 iteration 2762 : loss : 0.024143, loss_ce: 0.010770
2022-01-08 10:29:05,468 iteration 2763 : loss : 0.051313, loss_ce: 0.017993
2022-01-08 10:29:06,774 iteration 2764 : loss : 0.019625, loss_ce: 0.007297
2022-01-08 10:29:08,242 iteration 2765 : loss : 0.022772, loss_ce: 0.007980
2022-01-08 10:29:09,527 iteration 2766 : loss : 0.026396, loss_ce: 0.009190
2022-01-08 10:29:10,893 iteration 2767 : loss : 0.047721, loss_ce: 0.023240
2022-01-08 10:29:12,231 iteration 2768 : loss : 0.029277, loss_ce: 0.008924
2022-01-08 10:29:13,526 iteration 2769 : loss : 0.022135, loss_ce: 0.009394
2022-01-08 10:29:14,856 iteration 2770 : loss : 0.028295, loss_ce: 0.013204
2022-01-08 10:29:16,209 iteration 2771 : loss : 0.021676, loss_ce: 0.009217
 41%|███████████                | 163/400 [1:08:46<1:36:03, 24.32s/it]2022-01-08 10:29:17,537 iteration 2772 : loss : 0.034226, loss_ce: 0.011117
2022-01-08 10:29:18,907 iteration 2773 : loss : 0.025725, loss_ce: 0.009850
2022-01-08 10:29:20,275 iteration 2774 : loss : 0.055134, loss_ce: 0.011237
2022-01-08 10:29:21,654 iteration 2775 : loss : 0.030457, loss_ce: 0.012556
2022-01-08 10:29:22,987 iteration 2776 : loss : 0.037677, loss_ce: 0.015436
2022-01-08 10:29:24,406 iteration 2777 : loss : 0.022779, loss_ce: 0.007993
2022-01-08 10:29:25,733 iteration 2778 : loss : 0.024284, loss_ce: 0.008327
2022-01-08 10:29:27,055 iteration 2779 : loss : 0.026386, loss_ce: 0.010729
2022-01-08 10:29:28,411 iteration 2780 : loss : 0.023228, loss_ce: 0.011388
2022-01-08 10:29:29,889 iteration 2781 : loss : 0.031562, loss_ce: 0.012882
2022-01-08 10:29:31,302 iteration 2782 : loss : 0.027059, loss_ce: 0.011511
2022-01-08 10:29:32,652 iteration 2783 : loss : 0.033588, loss_ce: 0.010066
2022-01-08 10:29:33,909 iteration 2784 : loss : 0.029905, loss_ce: 0.008463
2022-01-08 10:29:35,228 iteration 2785 : loss : 0.032509, loss_ce: 0.013865
2022-01-08 10:29:36,532 iteration 2786 : loss : 0.027357, loss_ce: 0.011392
2022-01-08 10:29:37,855 iteration 2787 : loss : 0.026142, loss_ce: 0.010414
2022-01-08 10:29:39,346 iteration 2788 : loss : 0.041828, loss_ce: 0.016360
 41%|███████████                | 164/400 [1:09:10<1:34:15, 23.97s/it]2022-01-08 10:29:40,763 iteration 2789 : loss : 0.024676, loss_ce: 0.007841
2022-01-08 10:29:42,207 iteration 2790 : loss : 0.040037, loss_ce: 0.017562
2022-01-08 10:29:43,496 iteration 2791 : loss : 0.026716, loss_ce: 0.008128
2022-01-08 10:29:44,864 iteration 2792 : loss : 0.025242, loss_ce: 0.009934
2022-01-08 10:29:46,257 iteration 2793 : loss : 0.026422, loss_ce: 0.008814
2022-01-08 10:29:47,689 iteration 2794 : loss : 0.044679, loss_ce: 0.023488
2022-01-08 10:29:49,047 iteration 2795 : loss : 0.034913, loss_ce: 0.014415
2022-01-08 10:29:50,391 iteration 2796 : loss : 0.039731, loss_ce: 0.013135
2022-01-08 10:29:51,807 iteration 2797 : loss : 0.029537, loss_ce: 0.013951
2022-01-08 10:29:53,193 iteration 2798 : loss : 0.036781, loss_ce: 0.011449
2022-01-08 10:29:54,641 iteration 2799 : loss : 0.044730, loss_ce: 0.016285
2022-01-08 10:29:56,006 iteration 2800 : loss : 0.027009, loss_ce: 0.012784
2022-01-08 10:29:57,276 iteration 2801 : loss : 0.037216, loss_ce: 0.011051
2022-01-08 10:29:58,608 iteration 2802 : loss : 0.024660, loss_ce: 0.008495
2022-01-08 10:30:00,010 iteration 2803 : loss : 0.036943, loss_ce: 0.015249
2022-01-08 10:30:01,290 iteration 2804 : loss : 0.029412, loss_ce: 0.012192
2022-01-08 10:30:01,291 Training Data Eval:
2022-01-08 10:30:08,207   Average segmentation loss on training set: 0.0202
2022-01-08 10:30:08,207 Validation Data Eval:
2022-01-08 10:30:10,565   Average segmentation loss on validation set: 0.0959
2022-01-08 10:30:12,025 iteration 2805 : loss : 0.042121, loss_ce: 0.017723
 41%|███████████▏               | 165/400 [1:09:42<1:44:06, 26.58s/it]2022-01-08 10:30:13,489 iteration 2806 : loss : 0.031245, loss_ce: 0.013586
2022-01-08 10:30:14,917 iteration 2807 : loss : 0.037522, loss_ce: 0.012635
2022-01-08 10:30:16,320 iteration 2808 : loss : 0.036853, loss_ce: 0.012236
2022-01-08 10:30:17,702 iteration 2809 : loss : 0.029472, loss_ce: 0.011487
2022-01-08 10:30:18,986 iteration 2810 : loss : 0.030870, loss_ce: 0.011738
2022-01-08 10:30:20,392 iteration 2811 : loss : 0.025790, loss_ce: 0.012382
2022-01-08 10:30:21,691 iteration 2812 : loss : 0.020508, loss_ce: 0.008654
2022-01-08 10:30:23,131 iteration 2813 : loss : 0.038238, loss_ce: 0.017293
2022-01-08 10:30:24,505 iteration 2814 : loss : 0.031936, loss_ce: 0.012283
2022-01-08 10:30:25,856 iteration 2815 : loss : 0.027478, loss_ce: 0.013398
2022-01-08 10:30:27,214 iteration 2816 : loss : 0.031196, loss_ce: 0.010539
2022-01-08 10:30:28,645 iteration 2817 : loss : 0.047817, loss_ce: 0.011719
2022-01-08 10:30:30,056 iteration 2818 : loss : 0.027654, loss_ce: 0.011705
2022-01-08 10:30:31,391 iteration 2819 : loss : 0.033745, loss_ce: 0.014464
2022-01-08 10:30:32,848 iteration 2820 : loss : 0.050859, loss_ce: 0.015422
2022-01-08 10:30:34,270 iteration 2821 : loss : 0.051192, loss_ce: 0.018821
2022-01-08 10:30:35,665 iteration 2822 : loss : 0.026446, loss_ce: 0.008317
 42%|███████████▏               | 166/400 [1:10:06<1:40:13, 25.70s/it]2022-01-08 10:30:37,165 iteration 2823 : loss : 0.040968, loss_ce: 0.017576
2022-01-08 10:30:38,500 iteration 2824 : loss : 0.027368, loss_ce: 0.012496
2022-01-08 10:30:39,871 iteration 2825 : loss : 0.045371, loss_ce: 0.019550
2022-01-08 10:30:41,341 iteration 2826 : loss : 0.034262, loss_ce: 0.014478
2022-01-08 10:30:42,814 iteration 2827 : loss : 0.028181, loss_ce: 0.009394
2022-01-08 10:30:44,205 iteration 2828 : loss : 0.028187, loss_ce: 0.009420
2022-01-08 10:30:45,568 iteration 2829 : loss : 0.037817, loss_ce: 0.019383
2022-01-08 10:30:46,925 iteration 2830 : loss : 0.031786, loss_ce: 0.010252
2022-01-08 10:30:48,195 iteration 2831 : loss : 0.022898, loss_ce: 0.009842
2022-01-08 10:30:49,674 iteration 2832 : loss : 0.035129, loss_ce: 0.014567
2022-01-08 10:30:51,015 iteration 2833 : loss : 0.028721, loss_ce: 0.011487
2022-01-08 10:30:52,444 iteration 2834 : loss : 0.035161, loss_ce: 0.011785
2022-01-08 10:30:53,841 iteration 2835 : loss : 0.037536, loss_ce: 0.013407
2022-01-08 10:30:55,199 iteration 2836 : loss : 0.032682, loss_ce: 0.011589
2022-01-08 10:30:56,620 iteration 2837 : loss : 0.032257, loss_ce: 0.014255
2022-01-08 10:30:57,924 iteration 2838 : loss : 0.024018, loss_ce: 0.010759
2022-01-08 10:30:59,322 iteration 2839 : loss : 0.029178, loss_ce: 0.012347
 42%|███████████▎               | 167/400 [1:10:29<1:37:24, 25.09s/it]2022-01-08 10:31:00,788 iteration 2840 : loss : 0.042479, loss_ce: 0.017537
2022-01-08 10:31:02,135 iteration 2841 : loss : 0.028856, loss_ce: 0.008908
2022-01-08 10:31:03,478 iteration 2842 : loss : 0.024312, loss_ce: 0.008395
2022-01-08 10:31:04,887 iteration 2843 : loss : 0.033387, loss_ce: 0.010258
2022-01-08 10:31:06,274 iteration 2844 : loss : 0.038847, loss_ce: 0.019473
2022-01-08 10:31:07,643 iteration 2845 : loss : 0.030514, loss_ce: 0.010910
2022-01-08 10:31:09,008 iteration 2846 : loss : 0.024947, loss_ce: 0.009213
2022-01-08 10:31:10,248 iteration 2847 : loss : 0.016968, loss_ce: 0.008772
2022-01-08 10:31:11,628 iteration 2848 : loss : 0.033154, loss_ce: 0.012169
2022-01-08 10:31:13,054 iteration 2849 : loss : 0.039664, loss_ce: 0.015909
2022-01-08 10:31:14,459 iteration 2850 : loss : 0.035257, loss_ce: 0.012055
2022-01-08 10:31:15,865 iteration 2851 : loss : 0.027220, loss_ce: 0.010890
2022-01-08 10:31:17,233 iteration 2852 : loss : 0.041322, loss_ce: 0.011633
2022-01-08 10:31:18,657 iteration 2853 : loss : 0.025639, loss_ce: 0.009735
2022-01-08 10:31:20,053 iteration 2854 : loss : 0.028973, loss_ce: 0.010820
2022-01-08 10:31:21,376 iteration 2855 : loss : 0.028588, loss_ce: 0.014621
2022-01-08 10:31:22,773 iteration 2856 : loss : 0.034052, loss_ce: 0.016524
 42%|███████████▎               | 168/400 [1:10:53<1:35:05, 24.59s/it]2022-01-08 10:31:24,084 iteration 2857 : loss : 0.022841, loss_ce: 0.007742
2022-01-08 10:31:25,461 iteration 2858 : loss : 0.023708, loss_ce: 0.010326
2022-01-08 10:31:26,803 iteration 2859 : loss : 0.018198, loss_ce: 0.007955
2022-01-08 10:31:28,215 iteration 2860 : loss : 0.026487, loss_ce: 0.008641
2022-01-08 10:31:29,504 iteration 2861 : loss : 0.025603, loss_ce: 0.011648
2022-01-08 10:31:30,967 iteration 2862 : loss : 0.065280, loss_ce: 0.018887
2022-01-08 10:31:32,283 iteration 2863 : loss : 0.028928, loss_ce: 0.007747
2022-01-08 10:31:33,678 iteration 2864 : loss : 0.024844, loss_ce: 0.010784
2022-01-08 10:31:35,072 iteration 2865 : loss : 0.035715, loss_ce: 0.013678
2022-01-08 10:31:36,429 iteration 2866 : loss : 0.029586, loss_ce: 0.011931
2022-01-08 10:31:37,911 iteration 2867 : loss : 0.041266, loss_ce: 0.017470
2022-01-08 10:31:39,226 iteration 2868 : loss : 0.027427, loss_ce: 0.009526
2022-01-08 10:31:40,665 iteration 2869 : loss : 0.035744, loss_ce: 0.012972
2022-01-08 10:31:42,044 iteration 2870 : loss : 0.020682, loss_ce: 0.007009
2022-01-08 10:31:43,378 iteration 2871 : loss : 0.034197, loss_ce: 0.014326
2022-01-08 10:31:44,679 iteration 2872 : loss : 0.040695, loss_ce: 0.017444
2022-01-08 10:31:46,096 iteration 2873 : loss : 0.033968, loss_ce: 0.012323
 42%|███████████▍               | 169/400 [1:11:16<1:33:13, 24.22s/it]2022-01-08 10:31:47,529 iteration 2874 : loss : 0.035695, loss_ce: 0.012373
2022-01-08 10:31:48,905 iteration 2875 : loss : 0.026155, loss_ce: 0.010998
2022-01-08 10:31:50,274 iteration 2876 : loss : 0.034264, loss_ce: 0.015604
2022-01-08 10:31:51,686 iteration 2877 : loss : 0.044759, loss_ce: 0.014279
2022-01-08 10:31:53,055 iteration 2878 : loss : 0.025377, loss_ce: 0.011429
2022-01-08 10:31:54,477 iteration 2879 : loss : 0.028536, loss_ce: 0.009680
2022-01-08 10:31:55,917 iteration 2880 : loss : 0.040787, loss_ce: 0.012990
2022-01-08 10:31:57,300 iteration 2881 : loss : 0.028237, loss_ce: 0.011018
2022-01-08 10:31:58,729 iteration 2882 : loss : 0.027945, loss_ce: 0.014613
2022-01-08 10:32:00,049 iteration 2883 : loss : 0.040066, loss_ce: 0.021894
2022-01-08 10:32:01,395 iteration 2884 : loss : 0.029198, loss_ce: 0.011146
2022-01-08 10:32:02,783 iteration 2885 : loss : 0.038174, loss_ce: 0.016333
2022-01-08 10:32:04,076 iteration 2886 : loss : 0.052700, loss_ce: 0.015367
2022-01-08 10:32:05,521 iteration 2887 : loss : 0.038415, loss_ce: 0.019315
2022-01-08 10:32:06,904 iteration 2888 : loss : 0.047020, loss_ce: 0.011467
2022-01-08 10:32:08,379 iteration 2889 : loss : 0.026905, loss_ce: 0.010237
2022-01-08 10:32:08,379 Training Data Eval:
2022-01-08 10:32:15,247   Average segmentation loss on training set: 0.0194
2022-01-08 10:32:15,248 Validation Data Eval:
2022-01-08 10:32:17,609   Average segmentation loss on validation set: 0.0684
2022-01-08 10:32:19,032 iteration 2890 : loss : 0.028108, loss_ce: 0.009648
 42%|███████████▍               | 170/400 [1:11:49<1:42:50, 26.83s/it]2022-01-08 10:32:20,469 iteration 2891 : loss : 0.028397, loss_ce: 0.011956
2022-01-08 10:32:21,909 iteration 2892 : loss : 0.035283, loss_ce: 0.012491
2022-01-08 10:32:23,236 iteration 2893 : loss : 0.022743, loss_ce: 0.008248
2022-01-08 10:32:24,685 iteration 2894 : loss : 0.044716, loss_ce: 0.011662
2022-01-08 10:32:26,112 iteration 2895 : loss : 0.033592, loss_ce: 0.016081
2022-01-08 10:32:27,443 iteration 2896 : loss : 0.020172, loss_ce: 0.006676
2022-01-08 10:32:28,817 iteration 2897 : loss : 0.033424, loss_ce: 0.013859
2022-01-08 10:32:30,136 iteration 2898 : loss : 0.026273, loss_ce: 0.011359
2022-01-08 10:32:31,545 iteration 2899 : loss : 0.042714, loss_ce: 0.015073
2022-01-08 10:32:32,906 iteration 2900 : loss : 0.025399, loss_ce: 0.009595
2022-01-08 10:32:34,277 iteration 2901 : loss : 0.030790, loss_ce: 0.008654
2022-01-08 10:32:35,583 iteration 2902 : loss : 0.025203, loss_ce: 0.009284
2022-01-08 10:32:36,930 iteration 2903 : loss : 0.031022, loss_ce: 0.015735
2022-01-08 10:32:38,260 iteration 2904 : loss : 0.026060, loss_ce: 0.010029
2022-01-08 10:32:39,565 iteration 2905 : loss : 0.024495, loss_ce: 0.012988
2022-01-08 10:32:40,966 iteration 2906 : loss : 0.039250, loss_ce: 0.021911
2022-01-08 10:32:42,284 iteration 2907 : loss : 0.025121, loss_ce: 0.009167
 43%|███████████▌               | 171/400 [1:12:12<1:38:18, 25.76s/it]2022-01-08 10:32:43,726 iteration 2908 : loss : 0.030960, loss_ce: 0.014173
2022-01-08 10:32:45,144 iteration 2909 : loss : 0.033138, loss_ce: 0.016179
2022-01-08 10:32:46,523 iteration 2910 : loss : 0.042651, loss_ce: 0.016080
2022-01-08 10:32:47,935 iteration 2911 : loss : 0.031080, loss_ce: 0.010523
2022-01-08 10:32:49,286 iteration 2912 : loss : 0.024656, loss_ce: 0.009968
2022-01-08 10:32:50,740 iteration 2913 : loss : 0.027682, loss_ce: 0.012226
2022-01-08 10:32:52,076 iteration 2914 : loss : 0.035060, loss_ce: 0.013892
2022-01-08 10:32:53,430 iteration 2915 : loss : 0.018023, loss_ce: 0.007217
2022-01-08 10:32:54,811 iteration 2916 : loss : 0.022044, loss_ce: 0.008481
2022-01-08 10:32:56,116 iteration 2917 : loss : 0.022033, loss_ce: 0.010277
2022-01-08 10:32:57,475 iteration 2918 : loss : 0.050757, loss_ce: 0.015828
2022-01-08 10:32:58,838 iteration 2919 : loss : 0.041220, loss_ce: 0.011894
2022-01-08 10:33:00,230 iteration 2920 : loss : 0.028649, loss_ce: 0.010734
2022-01-08 10:33:01,596 iteration 2921 : loss : 0.030354, loss_ce: 0.015226
2022-01-08 10:33:02,900 iteration 2922 : loss : 0.029025, loss_ce: 0.010350
2022-01-08 10:33:04,242 iteration 2923 : loss : 0.036632, loss_ce: 0.012917
2022-01-08 10:33:05,612 iteration 2924 : loss : 0.054192, loss_ce: 0.023746
 43%|███████████▌               | 172/400 [1:12:36<1:35:06, 25.03s/it]2022-01-08 10:33:06,920 iteration 2925 : loss : 0.022359, loss_ce: 0.011556
2022-01-08 10:33:08,358 iteration 2926 : loss : 0.054580, loss_ce: 0.024337
2022-01-08 10:33:09,706 iteration 2927 : loss : 0.033472, loss_ce: 0.014233
2022-01-08 10:33:10,968 iteration 2928 : loss : 0.025164, loss_ce: 0.008875
2022-01-08 10:33:12,310 iteration 2929 : loss : 0.028485, loss_ce: 0.013140
2022-01-08 10:33:13,754 iteration 2930 : loss : 0.031207, loss_ce: 0.015489
2022-01-08 10:33:15,092 iteration 2931 : loss : 0.037539, loss_ce: 0.017548
2022-01-08 10:33:16,416 iteration 2932 : loss : 0.035254, loss_ce: 0.014825
2022-01-08 10:33:17,851 iteration 2933 : loss : 0.063032, loss_ce: 0.018845
2022-01-08 10:33:19,196 iteration 2934 : loss : 0.043618, loss_ce: 0.018067
2022-01-08 10:33:20,562 iteration 2935 : loss : 0.035311, loss_ce: 0.013027
2022-01-08 10:33:21,923 iteration 2936 : loss : 0.022638, loss_ce: 0.007736
2022-01-08 10:33:23,314 iteration 2937 : loss : 0.026587, loss_ce: 0.013080
2022-01-08 10:33:24,719 iteration 2938 : loss : 0.034383, loss_ce: 0.011743
2022-01-08 10:33:26,125 iteration 2939 : loss : 0.024565, loss_ce: 0.008449
2022-01-08 10:33:27,591 iteration 2940 : loss : 0.045511, loss_ce: 0.026183
2022-01-08 10:33:28,916 iteration 2941 : loss : 0.026651, loss_ce: 0.008738
 43%|███████████▋               | 173/400 [1:12:59<1:32:44, 24.51s/it]2022-01-08 10:33:30,270 iteration 2942 : loss : 0.032940, loss_ce: 0.010521
2022-01-08 10:33:31,665 iteration 2943 : loss : 0.042681, loss_ce: 0.012302
2022-01-08 10:33:32,991 iteration 2944 : loss : 0.026485, loss_ce: 0.014805
2022-01-08 10:33:34,358 iteration 2945 : loss : 0.031122, loss_ce: 0.009414
2022-01-08 10:33:35,629 iteration 2946 : loss : 0.028037, loss_ce: 0.010271
2022-01-08 10:33:36,960 iteration 2947 : loss : 0.026087, loss_ce: 0.010195
2022-01-08 10:33:38,291 iteration 2948 : loss : 0.034672, loss_ce: 0.010381
2022-01-08 10:33:39,727 iteration 2949 : loss : 0.034566, loss_ce: 0.013995
2022-01-08 10:33:41,114 iteration 2950 : loss : 0.023314, loss_ce: 0.010054
2022-01-08 10:33:42,513 iteration 2951 : loss : 0.023778, loss_ce: 0.011330
2022-01-08 10:33:43,891 iteration 2952 : loss : 0.024750, loss_ce: 0.008430
2022-01-08 10:33:45,317 iteration 2953 : loss : 0.034443, loss_ce: 0.016466
2022-01-08 10:33:46,782 iteration 2954 : loss : 0.054253, loss_ce: 0.014137
2022-01-08 10:33:48,113 iteration 2955 : loss : 0.027432, loss_ce: 0.012164
2022-01-08 10:33:49,454 iteration 2956 : loss : 0.026584, loss_ce: 0.010545
2022-01-08 10:33:50,762 iteration 2957 : loss : 0.024087, loss_ce: 0.009757
2022-01-08 10:33:52,133 iteration 2958 : loss : 0.026022, loss_ce: 0.012643
 44%|███████████▋               | 174/400 [1:13:22<1:30:51, 24.12s/it]2022-01-08 10:33:53,513 iteration 2959 : loss : 0.025175, loss_ce: 0.007688
2022-01-08 10:33:54,864 iteration 2960 : loss : 0.028099, loss_ce: 0.012575
2022-01-08 10:33:56,234 iteration 2961 : loss : 0.024204, loss_ce: 0.008737
2022-01-08 10:33:57,742 iteration 2962 : loss : 0.026872, loss_ce: 0.009404
2022-01-08 10:33:59,128 iteration 2963 : loss : 0.043850, loss_ce: 0.016777
2022-01-08 10:34:00,405 iteration 2964 : loss : 0.032483, loss_ce: 0.012409
2022-01-08 10:34:01,788 iteration 2965 : loss : 0.025153, loss_ce: 0.012860
2022-01-08 10:34:03,201 iteration 2966 : loss : 0.030991, loss_ce: 0.010440
2022-01-08 10:34:04,559 iteration 2967 : loss : 0.032286, loss_ce: 0.013385
2022-01-08 10:34:05,937 iteration 2968 : loss : 0.030452, loss_ce: 0.012777
2022-01-08 10:34:07,272 iteration 2969 : loss : 0.039623, loss_ce: 0.021540
2022-01-08 10:34:08,652 iteration 2970 : loss : 0.044852, loss_ce: 0.017181
2022-01-08 10:34:10,009 iteration 2971 : loss : 0.028893, loss_ce: 0.012314
2022-01-08 10:34:11,367 iteration 2972 : loss : 0.032237, loss_ce: 0.014532
2022-01-08 10:34:12,680 iteration 2973 : loss : 0.024405, loss_ce: 0.006100
2022-01-08 10:34:14,098 iteration 2974 : loss : 0.045211, loss_ce: 0.014153
2022-01-08 10:34:14,098 Training Data Eval:
2022-01-08 10:34:20,989   Average segmentation loss on training set: 0.0191
2022-01-08 10:34:20,990 Validation Data Eval:
2022-01-08 10:34:23,361   Average segmentation loss on validation set: 0.0761
2022-01-08 10:34:24,706 iteration 2975 : loss : 0.021591, loss_ce: 0.008676
 44%|███████████▊               | 175/400 [1:13:55<1:39:57, 26.66s/it]2022-01-08 10:34:26,028 iteration 2976 : loss : 0.020511, loss_ce: 0.008996
2022-01-08 10:34:27,434 iteration 2977 : loss : 0.026569, loss_ce: 0.012034
2022-01-08 10:34:28,809 iteration 2978 : loss : 0.032378, loss_ce: 0.009704
2022-01-08 10:34:30,216 iteration 2979 : loss : 0.032078, loss_ce: 0.010943
2022-01-08 10:34:31,609 iteration 2980 : loss : 0.024501, loss_ce: 0.007177
2022-01-08 10:34:33,075 iteration 2981 : loss : 0.048910, loss_ce: 0.021495
2022-01-08 10:34:34,436 iteration 2982 : loss : 0.039782, loss_ce: 0.015593
2022-01-08 10:34:35,808 iteration 2983 : loss : 0.030048, loss_ce: 0.010621
2022-01-08 10:34:37,184 iteration 2984 : loss : 0.038157, loss_ce: 0.013574
2022-01-08 10:34:38,582 iteration 2985 : loss : 0.028344, loss_ce: 0.011244
2022-01-08 10:34:39,888 iteration 2986 : loss : 0.027885, loss_ce: 0.012831
2022-01-08 10:34:41,263 iteration 2987 : loss : 0.042345, loss_ce: 0.016422
2022-01-08 10:34:42,683 iteration 2988 : loss : 0.031779, loss_ce: 0.013899
2022-01-08 10:34:44,085 iteration 2989 : loss : 0.028901, loss_ce: 0.011225
2022-01-08 10:34:45,395 iteration 2990 : loss : 0.022302, loss_ce: 0.009289
2022-01-08 10:34:46,764 iteration 2991 : loss : 0.023126, loss_ce: 0.009686
2022-01-08 10:34:48,212 iteration 2992 : loss : 0.027966, loss_ce: 0.012323
 44%|███████████▉               | 176/400 [1:14:18<1:35:59, 25.71s/it]2022-01-08 10:34:49,565 iteration 2993 : loss : 0.031234, loss_ce: 0.015058
2022-01-08 10:34:50,940 iteration 2994 : loss : 0.053563, loss_ce: 0.012239
2022-01-08 10:34:52,277 iteration 2995 : loss : 0.029448, loss_ce: 0.012678
2022-01-08 10:34:53,631 iteration 2996 : loss : 0.019529, loss_ce: 0.008877
2022-01-08 10:34:54,976 iteration 2997 : loss : 0.035540, loss_ce: 0.011857
2022-01-08 10:34:56,353 iteration 2998 : loss : 0.032388, loss_ce: 0.015964
2022-01-08 10:34:57,784 iteration 2999 : loss : 0.062357, loss_ce: 0.013278
2022-01-08 10:34:59,220 iteration 3000 : loss : 0.025617, loss_ce: 0.010777
2022-01-08 10:35:00,653 iteration 3001 : loss : 0.034315, loss_ce: 0.008405
2022-01-08 10:35:01,956 iteration 3002 : loss : 0.021878, loss_ce: 0.008464
2022-01-08 10:35:03,287 iteration 3003 : loss : 0.024813, loss_ce: 0.009300
2022-01-08 10:35:04,639 iteration 3004 : loss : 0.023900, loss_ce: 0.010088
2022-01-08 10:35:06,019 iteration 3005 : loss : 0.034960, loss_ce: 0.016212
2022-01-08 10:35:07,423 iteration 3006 : loss : 0.029475, loss_ce: 0.011239
2022-01-08 10:35:08,812 iteration 3007 : loss : 0.031467, loss_ce: 0.014083
2022-01-08 10:35:10,213 iteration 3008 : loss : 0.026236, loss_ce: 0.009125
2022-01-08 10:35:11,519 iteration 3009 : loss : 0.029237, loss_ce: 0.012683
 44%|███████████▉               | 177/400 [1:14:42<1:32:52, 24.99s/it]2022-01-08 10:35:13,047 iteration 3010 : loss : 0.048320, loss_ce: 0.018503
2022-01-08 10:35:14,461 iteration 3011 : loss : 0.030254, loss_ce: 0.014828
2022-01-08 10:35:15,772 iteration 3012 : loss : 0.018920, loss_ce: 0.008025
2022-01-08 10:35:17,075 iteration 3013 : loss : 0.039604, loss_ce: 0.013233
2022-01-08 10:35:18,467 iteration 3014 : loss : 0.027146, loss_ce: 0.011952
2022-01-08 10:35:19,830 iteration 3015 : loss : 0.049309, loss_ce: 0.013637
2022-01-08 10:35:21,184 iteration 3016 : loss : 0.031588, loss_ce: 0.010396
2022-01-08 10:35:22,519 iteration 3017 : loss : 0.023913, loss_ce: 0.012424
2022-01-08 10:35:23,851 iteration 3018 : loss : 0.018550, loss_ce: 0.008790
2022-01-08 10:35:25,148 iteration 3019 : loss : 0.027829, loss_ce: 0.011110
2022-01-08 10:35:26,540 iteration 3020 : loss : 0.035634, loss_ce: 0.012831
2022-01-08 10:35:27,916 iteration 3021 : loss : 0.018932, loss_ce: 0.007119
2022-01-08 10:35:29,266 iteration 3022 : loss : 0.022739, loss_ce: 0.009335
2022-01-08 10:35:30,635 iteration 3023 : loss : 0.045799, loss_ce: 0.017591
2022-01-08 10:35:32,068 iteration 3024 : loss : 0.030896, loss_ce: 0.013593
2022-01-08 10:35:33,479 iteration 3025 : loss : 0.038809, loss_ce: 0.015988
2022-01-08 10:35:34,806 iteration 3026 : loss : 0.024184, loss_ce: 0.010519
 44%|████████████               | 178/400 [1:15:05<1:30:34, 24.48s/it]2022-01-08 10:35:36,274 iteration 3027 : loss : 0.041009, loss_ce: 0.014314
2022-01-08 10:35:37,586 iteration 3028 : loss : 0.024393, loss_ce: 0.010038
2022-01-08 10:35:38,971 iteration 3029 : loss : 0.044669, loss_ce: 0.016855
2022-01-08 10:35:40,314 iteration 3030 : loss : 0.028254, loss_ce: 0.012991
2022-01-08 10:35:41,681 iteration 3031 : loss : 0.038845, loss_ce: 0.011840
2022-01-08 10:35:43,019 iteration 3032 : loss : 0.027556, loss_ce: 0.009919
2022-01-08 10:35:44,418 iteration 3033 : loss : 0.044283, loss_ce: 0.014660
2022-01-08 10:35:45,834 iteration 3034 : loss : 0.028701, loss_ce: 0.010359
2022-01-08 10:35:47,213 iteration 3035 : loss : 0.028687, loss_ce: 0.013279
2022-01-08 10:35:48,568 iteration 3036 : loss : 0.032317, loss_ce: 0.017982
2022-01-08 10:35:49,942 iteration 3037 : loss : 0.025957, loss_ce: 0.009485
2022-01-08 10:35:51,348 iteration 3038 : loss : 0.032464, loss_ce: 0.010799
2022-01-08 10:35:52,657 iteration 3039 : loss : 0.034202, loss_ce: 0.010087
2022-01-08 10:35:54,027 iteration 3040 : loss : 0.034411, loss_ce: 0.009602
2022-01-08 10:35:55,428 iteration 3041 : loss : 0.022867, loss_ce: 0.008898
2022-01-08 10:35:56,785 iteration 3042 : loss : 0.037133, loss_ce: 0.018562
2022-01-08 10:35:58,315 iteration 3043 : loss : 0.038480, loss_ce: 0.012754
 45%|████████████               | 179/400 [1:15:28<1:29:05, 24.19s/it]2022-01-08 10:35:59,728 iteration 3044 : loss : 0.033785, loss_ce: 0.012467
2022-01-08 10:36:01,040 iteration 3045 : loss : 0.024560, loss_ce: 0.009005
2022-01-08 10:36:02,393 iteration 3046 : loss : 0.020334, loss_ce: 0.006917
2022-01-08 10:36:03,863 iteration 3047 : loss : 0.068597, loss_ce: 0.040571
2022-01-08 10:36:05,187 iteration 3048 : loss : 0.033478, loss_ce: 0.011511
2022-01-08 10:36:06,550 iteration 3049 : loss : 0.031936, loss_ce: 0.013058
2022-01-08 10:36:07,954 iteration 3050 : loss : 0.030465, loss_ce: 0.014645
2022-01-08 10:36:09,323 iteration 3051 : loss : 0.029174, loss_ce: 0.008630
2022-01-08 10:36:10,677 iteration 3052 : loss : 0.024352, loss_ce: 0.008968
2022-01-08 10:36:12,125 iteration 3053 : loss : 0.023647, loss_ce: 0.010163
2022-01-08 10:36:13,448 iteration 3054 : loss : 0.037746, loss_ce: 0.013753
2022-01-08 10:36:14,829 iteration 3055 : loss : 0.053762, loss_ce: 0.020604
2022-01-08 10:36:16,174 iteration 3056 : loss : 0.019211, loss_ce: 0.008355
2022-01-08 10:36:17,496 iteration 3057 : loss : 0.022484, loss_ce: 0.009891
2022-01-08 10:36:18,897 iteration 3058 : loss : 0.020954, loss_ce: 0.008310
2022-01-08 10:36:20,289 iteration 3059 : loss : 0.027964, loss_ce: 0.009497
2022-01-08 10:36:20,290 Training Data Eval:
2022-01-08 10:36:27,170   Average segmentation loss on training set: 0.0192
2022-01-08 10:36:27,171 Validation Data Eval:
2022-01-08 10:36:29,552   Average segmentation loss on validation set: 0.0887
2022-01-08 10:36:30,899 iteration 3060 : loss : 0.024018, loss_ce: 0.011404
 45%|████████████▏              | 180/400 [1:16:01<1:37:55, 26.71s/it]2022-01-08 10:36:32,233 iteration 3061 : loss : 0.022130, loss_ce: 0.008564
2022-01-08 10:36:33,606 iteration 3062 : loss : 0.035745, loss_ce: 0.012682
2022-01-08 10:36:35,010 iteration 3063 : loss : 0.031914, loss_ce: 0.011655
2022-01-08 10:36:36,410 iteration 3064 : loss : 0.025848, loss_ce: 0.011553
2022-01-08 10:36:37,686 iteration 3065 : loss : 0.020152, loss_ce: 0.007044
2022-01-08 10:36:39,019 iteration 3066 : loss : 0.028404, loss_ce: 0.014675
2022-01-08 10:36:40,406 iteration 3067 : loss : 0.041310, loss_ce: 0.020600
2022-01-08 10:36:41,787 iteration 3068 : loss : 0.037690, loss_ce: 0.011609
2022-01-08 10:36:43,152 iteration 3069 : loss : 0.028994, loss_ce: 0.008980
2022-01-08 10:36:44,531 iteration 3070 : loss : 0.018801, loss_ce: 0.008227
2022-01-08 10:36:45,852 iteration 3071 : loss : 0.024505, loss_ce: 0.010269
2022-01-08 10:36:47,169 iteration 3072 : loss : 0.025270, loss_ce: 0.009437
2022-01-08 10:36:48,523 iteration 3073 : loss : 0.025001, loss_ce: 0.009791
2022-01-08 10:36:49,958 iteration 3074 : loss : 0.027152, loss_ce: 0.012096
2022-01-08 10:36:51,354 iteration 3075 : loss : 0.035080, loss_ce: 0.013390
2022-01-08 10:36:52,778 iteration 3076 : loss : 0.029533, loss_ce: 0.011719
2022-01-08 10:36:54,101 iteration 3077 : loss : 0.027175, loss_ce: 0.010746
 45%|████████████▏              | 181/400 [1:16:24<1:33:38, 25.65s/it]2022-01-08 10:36:55,453 iteration 3078 : loss : 0.022255, loss_ce: 0.008853
2022-01-08 10:36:56,924 iteration 3079 : loss : 0.021350, loss_ce: 0.007863
2022-01-08 10:36:58,212 iteration 3080 : loss : 0.032242, loss_ce: 0.010159
2022-01-08 10:36:59,592 iteration 3081 : loss : 0.025491, loss_ce: 0.009212
2022-01-08 10:37:00,929 iteration 3082 : loss : 0.028033, loss_ce: 0.007139
2022-01-08 10:37:02,332 iteration 3083 : loss : 0.028500, loss_ce: 0.012409
2022-01-08 10:37:03,582 iteration 3084 : loss : 0.023456, loss_ce: 0.010202
2022-01-08 10:37:04,977 iteration 3085 : loss : 0.027757, loss_ce: 0.012415
2022-01-08 10:37:06,450 iteration 3086 : loss : 0.022426, loss_ce: 0.011495
2022-01-08 10:37:07,793 iteration 3087 : loss : 0.045963, loss_ce: 0.022164
2022-01-08 10:37:09,171 iteration 3088 : loss : 0.026105, loss_ce: 0.008177
2022-01-08 10:37:10,628 iteration 3089 : loss : 0.033501, loss_ce: 0.011966
2022-01-08 10:37:11,924 iteration 3090 : loss : 0.023660, loss_ce: 0.010602
2022-01-08 10:37:13,289 iteration 3091 : loss : 0.024510, loss_ce: 0.011589
2022-01-08 10:37:14,797 iteration 3092 : loss : 0.032613, loss_ce: 0.012046
2022-01-08 10:37:16,118 iteration 3093 : loss : 0.021220, loss_ce: 0.008037
2022-01-08 10:37:17,477 iteration 3094 : loss : 0.032719, loss_ce: 0.013127
 46%|████████████▎              | 182/400 [1:16:48<1:30:44, 24.97s/it]2022-01-08 10:37:18,874 iteration 3095 : loss : 0.021898, loss_ce: 0.008516
2022-01-08 10:37:20,259 iteration 3096 : loss : 0.025008, loss_ce: 0.010557
2022-01-08 10:37:21,605 iteration 3097 : loss : 0.018431, loss_ce: 0.006603
2022-01-08 10:37:22,931 iteration 3098 : loss : 0.019564, loss_ce: 0.008245
2022-01-08 10:37:24,291 iteration 3099 : loss : 0.017308, loss_ce: 0.005658
2022-01-08 10:37:25,607 iteration 3100 : loss : 0.029082, loss_ce: 0.009477
2022-01-08 10:37:27,024 iteration 3101 : loss : 0.020938, loss_ce: 0.007786
2022-01-08 10:37:28,379 iteration 3102 : loss : 0.021072, loss_ce: 0.008494
2022-01-08 10:37:29,752 iteration 3103 : loss : 0.026032, loss_ce: 0.010136
2022-01-08 10:37:31,063 iteration 3104 : loss : 0.023874, loss_ce: 0.011293
2022-01-08 10:37:32,359 iteration 3105 : loss : 0.019884, loss_ce: 0.009313
2022-01-08 10:37:33,710 iteration 3106 : loss : 0.023210, loss_ce: 0.010726
2022-01-08 10:37:35,056 iteration 3107 : loss : 0.017165, loss_ce: 0.004946
2022-01-08 10:37:36,440 iteration 3108 : loss : 0.032687, loss_ce: 0.010965
2022-01-08 10:37:37,850 iteration 3109 : loss : 0.022311, loss_ce: 0.008381
2022-01-08 10:37:39,259 iteration 3110 : loss : 0.040763, loss_ce: 0.017315
2022-01-08 10:37:40,640 iteration 3111 : loss : 0.041537, loss_ce: 0.016157
 46%|████████████▎              | 183/400 [1:17:11<1:28:20, 24.43s/it]2022-01-08 10:37:42,063 iteration 3112 : loss : 0.027314, loss_ce: 0.009906
2022-01-08 10:37:43,388 iteration 3113 : loss : 0.022093, loss_ce: 0.009069
2022-01-08 10:37:44,795 iteration 3114 : loss : 0.022847, loss_ce: 0.008929
2022-01-08 10:37:46,170 iteration 3115 : loss : 0.027485, loss_ce: 0.011445
2022-01-08 10:37:47,534 iteration 3116 : loss : 0.026237, loss_ce: 0.013176
2022-01-08 10:37:48,964 iteration 3117 : loss : 0.021391, loss_ce: 0.008099
2022-01-08 10:37:50,287 iteration 3118 : loss : 0.031214, loss_ce: 0.011603
2022-01-08 10:37:51,733 iteration 3119 : loss : 0.032059, loss_ce: 0.011048
2022-01-08 10:37:53,044 iteration 3120 : loss : 0.022308, loss_ce: 0.008553
2022-01-08 10:37:54,480 iteration 3121 : loss : 0.021697, loss_ce: 0.009800
2022-01-08 10:37:55,825 iteration 3122 : loss : 0.037307, loss_ce: 0.010159
2022-01-08 10:37:57,234 iteration 3123 : loss : 0.022286, loss_ce: 0.009041
2022-01-08 10:37:58,535 iteration 3124 : loss : 0.019894, loss_ce: 0.007925
2022-01-08 10:37:59,908 iteration 3125 : loss : 0.025826, loss_ce: 0.008392
2022-01-08 10:38:01,330 iteration 3126 : loss : 0.019981, loss_ce: 0.009225
2022-01-08 10:38:02,756 iteration 3127 : loss : 0.025251, loss_ce: 0.010723
2022-01-08 10:38:04,176 iteration 3128 : loss : 0.029303, loss_ce: 0.011271
 46%|████████████▍              | 184/400 [1:17:34<1:26:58, 24.16s/it]2022-01-08 10:38:05,552 iteration 3129 : loss : 0.023702, loss_ce: 0.008502
2022-01-08 10:38:06,997 iteration 3130 : loss : 0.032494, loss_ce: 0.014870
2022-01-08 10:38:08,332 iteration 3131 : loss : 0.024323, loss_ce: 0.007768
2022-01-08 10:38:09,699 iteration 3132 : loss : 0.021791, loss_ce: 0.006470
2022-01-08 10:38:11,067 iteration 3133 : loss : 0.027916, loss_ce: 0.012390
2022-01-08 10:38:12,395 iteration 3134 : loss : 0.020787, loss_ce: 0.010502
2022-01-08 10:38:13,760 iteration 3135 : loss : 0.023773, loss_ce: 0.011299
2022-01-08 10:38:15,101 iteration 3136 : loss : 0.027243, loss_ce: 0.012250
2022-01-08 10:38:16,572 iteration 3137 : loss : 0.025808, loss_ce: 0.010809
2022-01-08 10:38:17,993 iteration 3138 : loss : 0.034372, loss_ce: 0.014878
2022-01-08 10:38:19,293 iteration 3139 : loss : 0.022827, loss_ce: 0.007894
2022-01-08 10:38:20,601 iteration 3140 : loss : 0.025574, loss_ce: 0.011812
2022-01-08 10:38:21,997 iteration 3141 : loss : 0.032455, loss_ce: 0.013074
2022-01-08 10:38:23,348 iteration 3142 : loss : 0.026328, loss_ce: 0.009380
2022-01-08 10:38:24,757 iteration 3143 : loss : 0.031185, loss_ce: 0.011716
2022-01-08 10:38:26,099 iteration 3144 : loss : 0.023499, loss_ce: 0.008468
2022-01-08 10:38:26,099 Training Data Eval:
2022-01-08 10:38:32,957   Average segmentation loss on training set: 0.0165
2022-01-08 10:38:32,958 Validation Data Eval:
2022-01-08 10:38:35,330   Average segmentation loss on validation set: 0.0829
2022-01-08 10:38:36,667 iteration 3145 : loss : 0.018973, loss_ce: 0.007725
 46%|████████████▍              | 185/400 [1:18:07<1:35:31, 26.66s/it]2022-01-08 10:38:38,218 iteration 3146 : loss : 0.029211, loss_ce: 0.008762
2022-01-08 10:38:39,626 iteration 3147 : loss : 0.024153, loss_ce: 0.011375
2022-01-08 10:38:40,936 iteration 3148 : loss : 0.020092, loss_ce: 0.007321
2022-01-08 10:38:42,350 iteration 3149 : loss : 0.027403, loss_ce: 0.007977
2022-01-08 10:38:43,753 iteration 3150 : loss : 0.020880, loss_ce: 0.006727
2022-01-08 10:38:45,143 iteration 3151 : loss : 0.040989, loss_ce: 0.019640
2022-01-08 10:38:46,537 iteration 3152 : loss : 0.028607, loss_ce: 0.008924
2022-01-08 10:38:47,921 iteration 3153 : loss : 0.034218, loss_ce: 0.016682
2022-01-08 10:38:49,359 iteration 3154 : loss : 0.033178, loss_ce: 0.013395
2022-01-08 10:38:50,781 iteration 3155 : loss : 0.024348, loss_ce: 0.008179
2022-01-08 10:38:52,101 iteration 3156 : loss : 0.021050, loss_ce: 0.006936
2022-01-08 10:38:53,482 iteration 3157 : loss : 0.030287, loss_ce: 0.013422
2022-01-08 10:38:54,802 iteration 3158 : loss : 0.028913, loss_ce: 0.006592
2022-01-08 10:38:56,200 iteration 3159 : loss : 0.026967, loss_ce: 0.013678
2022-01-08 10:38:57,606 iteration 3160 : loss : 0.027159, loss_ce: 0.011977
2022-01-08 10:38:59,038 iteration 3161 : loss : 0.026708, loss_ce: 0.011364
2022-01-08 10:39:00,402 iteration 3162 : loss : 0.027208, loss_ce: 0.008835
 46%|████████████▌              | 186/400 [1:18:31<1:31:57, 25.78s/it]2022-01-08 10:39:01,764 iteration 3163 : loss : 0.022975, loss_ce: 0.008208
2022-01-08 10:39:03,238 iteration 3164 : loss : 0.037596, loss_ce: 0.016305
2022-01-08 10:39:04,637 iteration 3165 : loss : 0.019943, loss_ce: 0.007906
2022-01-08 10:39:05,967 iteration 3166 : loss : 0.025866, loss_ce: 0.011059
2022-01-08 10:39:07,383 iteration 3167 : loss : 0.023469, loss_ce: 0.008157
2022-01-08 10:39:08,740 iteration 3168 : loss : 0.025765, loss_ce: 0.010948
2022-01-08 10:39:10,134 iteration 3169 : loss : 0.024434, loss_ce: 0.010447
2022-01-08 10:39:11,511 iteration 3170 : loss : 0.020518, loss_ce: 0.008962
2022-01-08 10:39:12,785 iteration 3171 : loss : 0.019068, loss_ce: 0.005429
2022-01-08 10:39:14,134 iteration 3172 : loss : 0.022441, loss_ce: 0.008707
2022-01-08 10:39:15,497 iteration 3173 : loss : 0.038081, loss_ce: 0.011645
2022-01-08 10:39:16,823 iteration 3174 : loss : 0.021871, loss_ce: 0.009579
2022-01-08 10:39:18,132 iteration 3175 : loss : 0.028438, loss_ce: 0.008864
2022-01-08 10:39:19,580 iteration 3176 : loss : 0.035497, loss_ce: 0.009469
2022-01-08 10:39:20,948 iteration 3177 : loss : 0.038650, loss_ce: 0.015493
2022-01-08 10:39:22,394 iteration 3178 : loss : 0.054983, loss_ce: 0.021037
2022-01-08 10:39:23,763 iteration 3179 : loss : 0.025073, loss_ce: 0.014307
 47%|████████████▌              | 187/400 [1:18:54<1:28:57, 25.06s/it]2022-01-08 10:39:25,191 iteration 3180 : loss : 0.046519, loss_ce: 0.023089
2022-01-08 10:39:26,525 iteration 3181 : loss : 0.026605, loss_ce: 0.014647
2022-01-08 10:39:27,837 iteration 3182 : loss : 0.020106, loss_ce: 0.009561
2022-01-08 10:39:29,266 iteration 3183 : loss : 0.038517, loss_ce: 0.012323
2022-01-08 10:39:30,636 iteration 3184 : loss : 0.022847, loss_ce: 0.008320
2022-01-08 10:39:32,004 iteration 3185 : loss : 0.043069, loss_ce: 0.014968
2022-01-08 10:39:33,404 iteration 3186 : loss : 0.022792, loss_ce: 0.009621
2022-01-08 10:39:34,820 iteration 3187 : loss : 0.024495, loss_ce: 0.009050
2022-01-08 10:39:36,243 iteration 3188 : loss : 0.033935, loss_ce: 0.012882
2022-01-08 10:39:37,594 iteration 3189 : loss : 0.028200, loss_ce: 0.009994
2022-01-08 10:39:39,055 iteration 3190 : loss : 0.035553, loss_ce: 0.012668
2022-01-08 10:39:40,460 iteration 3191 : loss : 0.027435, loss_ce: 0.009690
2022-01-08 10:39:41,760 iteration 3192 : loss : 0.028273, loss_ce: 0.006153
2022-01-08 10:39:43,138 iteration 3193 : loss : 0.028521, loss_ce: 0.011880
2022-01-08 10:39:44,634 iteration 3194 : loss : 0.038275, loss_ce: 0.016728
2022-01-08 10:39:46,010 iteration 3195 : loss : 0.025470, loss_ce: 0.011602
2022-01-08 10:39:47,386 iteration 3196 : loss : 0.025746, loss_ce: 0.011637
 47%|████████████▋              | 188/400 [1:19:18<1:27:00, 24.63s/it]2022-01-08 10:39:48,777 iteration 3197 : loss : 0.021598, loss_ce: 0.009782
2022-01-08 10:39:50,186 iteration 3198 : loss : 0.024477, loss_ce: 0.012132
2022-01-08 10:39:51,588 iteration 3199 : loss : 0.025755, loss_ce: 0.007457
2022-01-08 10:39:52,997 iteration 3200 : loss : 0.045631, loss_ce: 0.013177
2022-01-08 10:39:54,406 iteration 3201 : loss : 0.043276, loss_ce: 0.018204
2022-01-08 10:39:55,820 iteration 3202 : loss : 0.040351, loss_ce: 0.013189
2022-01-08 10:39:57,116 iteration 3203 : loss : 0.024819, loss_ce: 0.012260
2022-01-08 10:39:58,405 iteration 3204 : loss : 0.019148, loss_ce: 0.008266
2022-01-08 10:39:59,706 iteration 3205 : loss : 0.032199, loss_ce: 0.010183
2022-01-08 10:40:00,976 iteration 3206 : loss : 0.016896, loss_ce: 0.005769
2022-01-08 10:40:02,402 iteration 3207 : loss : 0.043303, loss_ce: 0.017799
2022-01-08 10:40:03,765 iteration 3208 : loss : 0.022100, loss_ce: 0.008637
2022-01-08 10:40:05,092 iteration 3209 : loss : 0.027936, loss_ce: 0.009356
2022-01-08 10:40:06,478 iteration 3210 : loss : 0.024396, loss_ce: 0.011500
2022-01-08 10:40:07,792 iteration 3211 : loss : 0.022276, loss_ce: 0.006777
2022-01-08 10:40:09,235 iteration 3212 : loss : 0.038483, loss_ce: 0.016031
2022-01-08 10:40:10,675 iteration 3213 : loss : 0.023827, loss_ce: 0.007821
 47%|████████████▊              | 189/400 [1:19:41<1:25:11, 24.22s/it]2022-01-08 10:40:12,012 iteration 3214 : loss : 0.020072, loss_ce: 0.008729
2022-01-08 10:40:13,373 iteration 3215 : loss : 0.026679, loss_ce: 0.011870
2022-01-08 10:40:14,733 iteration 3216 : loss : 0.050363, loss_ce: 0.022680
2022-01-08 10:40:16,135 iteration 3217 : loss : 0.028476, loss_ce: 0.012549
2022-01-08 10:40:17,500 iteration 3218 : loss : 0.027762, loss_ce: 0.008512
2022-01-08 10:40:18,864 iteration 3219 : loss : 0.028092, loss_ce: 0.011329
2022-01-08 10:40:20,182 iteration 3220 : loss : 0.019286, loss_ce: 0.009248
2022-01-08 10:40:21,637 iteration 3221 : loss : 0.042739, loss_ce: 0.017355
2022-01-08 10:40:22,984 iteration 3222 : loss : 0.026207, loss_ce: 0.006726
2022-01-08 10:40:24,379 iteration 3223 : loss : 0.020296, loss_ce: 0.006750
2022-01-08 10:40:25,831 iteration 3224 : loss : 0.034928, loss_ce: 0.010370
2022-01-08 10:40:27,183 iteration 3225 : loss : 0.025733, loss_ce: 0.012181
2022-01-08 10:40:28,545 iteration 3226 : loss : 0.026925, loss_ce: 0.010431
2022-01-08 10:40:29,931 iteration 3227 : loss : 0.030974, loss_ce: 0.011874
2022-01-08 10:40:31,259 iteration 3228 : loss : 0.022220, loss_ce: 0.006611
2022-01-08 10:40:32,561 iteration 3229 : loss : 0.025322, loss_ce: 0.007465
2022-01-08 10:40:32,561 Training Data Eval:
2022-01-08 10:40:39,462   Average segmentation loss on training set: 0.0183
2022-01-08 10:40:39,463 Validation Data Eval:
2022-01-08 10:40:41,831   Average segmentation loss on validation set: 0.0791
2022-01-08 10:40:43,143 iteration 3230 : loss : 0.021774, loss_ce: 0.007996
 48%|████████████▊              | 190/400 [1:20:13<1:33:26, 26.70s/it]2022-01-08 10:40:44,525 iteration 3231 : loss : 0.038926, loss_ce: 0.011410
2022-01-08 10:40:45,885 iteration 3232 : loss : 0.020944, loss_ce: 0.005816
2022-01-08 10:40:47,256 iteration 3233 : loss : 0.028014, loss_ce: 0.014514
2022-01-08 10:40:48,641 iteration 3234 : loss : 0.021823, loss_ce: 0.009612
2022-01-08 10:40:50,027 iteration 3235 : loss : 0.045650, loss_ce: 0.015576
2022-01-08 10:40:51,332 iteration 3236 : loss : 0.017801, loss_ce: 0.009038
2022-01-08 10:40:52,779 iteration 3237 : loss : 0.028319, loss_ce: 0.011417
2022-01-08 10:40:54,156 iteration 3238 : loss : 0.022432, loss_ce: 0.010607
2022-01-08 10:40:55,532 iteration 3239 : loss : 0.027651, loss_ce: 0.010173
2022-01-08 10:40:56,852 iteration 3240 : loss : 0.027923, loss_ce: 0.007395
2022-01-08 10:40:58,151 iteration 3241 : loss : 0.027975, loss_ce: 0.006670
2022-01-08 10:40:59,536 iteration 3242 : loss : 0.028002, loss_ce: 0.010276
2022-01-08 10:41:00,844 iteration 3243 : loss : 0.031845, loss_ce: 0.009891
2022-01-08 10:41:02,240 iteration 3244 : loss : 0.020917, loss_ce: 0.008297
2022-01-08 10:41:03,575 iteration 3245 : loss : 0.020848, loss_ce: 0.009964
2022-01-08 10:41:05,006 iteration 3246 : loss : 0.043351, loss_ce: 0.023292
2022-01-08 10:41:06,333 iteration 3247 : loss : 0.027089, loss_ce: 0.011519
 48%|████████████▉              | 191/400 [1:20:37<1:29:19, 25.65s/it]2022-01-08 10:41:07,692 iteration 3248 : loss : 0.020290, loss_ce: 0.007861
2022-01-08 10:41:09,006 iteration 3249 : loss : 0.020287, loss_ce: 0.007919
2022-01-08 10:41:10,352 iteration 3250 : loss : 0.030757, loss_ce: 0.011006
2022-01-08 10:41:11,697 iteration 3251 : loss : 0.020722, loss_ce: 0.008877
2022-01-08 10:41:13,062 iteration 3252 : loss : 0.027239, loss_ce: 0.009549
2022-01-08 10:41:14,421 iteration 3253 : loss : 0.020877, loss_ce: 0.010381
2022-01-08 10:41:15,769 iteration 3254 : loss : 0.028306, loss_ce: 0.011388
2022-01-08 10:41:17,056 iteration 3255 : loss : 0.024864, loss_ce: 0.009746
2022-01-08 10:41:18,404 iteration 3256 : loss : 0.018777, loss_ce: 0.006695
2022-01-08 10:41:19,722 iteration 3257 : loss : 0.019575, loss_ce: 0.008044
2022-01-08 10:41:21,093 iteration 3258 : loss : 0.021770, loss_ce: 0.010283
2022-01-08 10:41:22,395 iteration 3259 : loss : 0.025831, loss_ce: 0.009425
2022-01-08 10:41:23,766 iteration 3260 : loss : 0.018328, loss_ce: 0.007287
2022-01-08 10:41:25,101 iteration 3261 : loss : 0.032352, loss_ce: 0.008252
2022-01-08 10:41:26,508 iteration 3262 : loss : 0.043872, loss_ce: 0.016486
2022-01-08 10:41:27,982 iteration 3263 : loss : 0.027347, loss_ce: 0.007836
2022-01-08 10:41:29,347 iteration 3264 : loss : 0.027523, loss_ce: 0.012110
 48%|████████████▉              | 192/400 [1:21:00<1:26:10, 24.86s/it]2022-01-08 10:41:30,683 iteration 3265 : loss : 0.019103, loss_ce: 0.007999
2022-01-08 10:41:32,077 iteration 3266 : loss : 0.039097, loss_ce: 0.017813
2022-01-08 10:41:33,456 iteration 3267 : loss : 0.030160, loss_ce: 0.013224
2022-01-08 10:41:34,775 iteration 3268 : loss : 0.019003, loss_ce: 0.006865
2022-01-08 10:41:36,180 iteration 3269 : loss : 0.028532, loss_ce: 0.011108
2022-01-08 10:41:37,615 iteration 3270 : loss : 0.031232, loss_ce: 0.009732
2022-01-08 10:41:39,030 iteration 3271 : loss : 0.024196, loss_ce: 0.008987
2022-01-08 10:41:40,455 iteration 3272 : loss : 0.032933, loss_ce: 0.012941
2022-01-08 10:41:41,764 iteration 3273 : loss : 0.022705, loss_ce: 0.006878
2022-01-08 10:41:43,127 iteration 3274 : loss : 0.022277, loss_ce: 0.010664
2022-01-08 10:41:44,552 iteration 3275 : loss : 0.032526, loss_ce: 0.008819
2022-01-08 10:41:45,882 iteration 3276 : loss : 0.023896, loss_ce: 0.009601
2022-01-08 10:41:47,273 iteration 3277 : loss : 0.026789, loss_ce: 0.013013
2022-01-08 10:41:48,636 iteration 3278 : loss : 0.026666, loss_ce: 0.009772
2022-01-08 10:41:49,962 iteration 3279 : loss : 0.019990, loss_ce: 0.007518
2022-01-08 10:41:51,309 iteration 3280 : loss : 0.019206, loss_ce: 0.006765
2022-01-08 10:41:52,614 iteration 3281 : loss : 0.017181, loss_ce: 0.004882
 48%|█████████████              | 193/400 [1:21:23<1:24:06, 24.38s/it]2022-01-08 10:41:54,043 iteration 3282 : loss : 0.032548, loss_ce: 0.012834
2022-01-08 10:41:55,406 iteration 3283 : loss : 0.020339, loss_ce: 0.007533
2022-01-08 10:41:56,767 iteration 3284 : loss : 0.020122, loss_ce: 0.008833
2022-01-08 10:41:58,106 iteration 3285 : loss : 0.026222, loss_ce: 0.010177
2022-01-08 10:41:59,456 iteration 3286 : loss : 0.019187, loss_ce: 0.007370
2022-01-08 10:42:00,833 iteration 3287 : loss : 0.028796, loss_ce: 0.011356
2022-01-08 10:42:02,149 iteration 3288 : loss : 0.024631, loss_ce: 0.013600
2022-01-08 10:42:03,469 iteration 3289 : loss : 0.026125, loss_ce: 0.007987
2022-01-08 10:42:04,872 iteration 3290 : loss : 0.035962, loss_ce: 0.011167
2022-01-08 10:42:06,283 iteration 3291 : loss : 0.029227, loss_ce: 0.010948
2022-01-08 10:42:07,665 iteration 3292 : loss : 0.020304, loss_ce: 0.006515
2022-01-08 10:42:09,029 iteration 3293 : loss : 0.019666, loss_ce: 0.008946
2022-01-08 10:42:10,360 iteration 3294 : loss : 0.026250, loss_ce: 0.011948
2022-01-08 10:42:11,796 iteration 3295 : loss : 0.032746, loss_ce: 0.012387
2022-01-08 10:42:13,187 iteration 3296 : loss : 0.030521, loss_ce: 0.010133
2022-01-08 10:42:14,475 iteration 3297 : loss : 0.015339, loss_ce: 0.006198
2022-01-08 10:42:15,880 iteration 3298 : loss : 0.023385, loss_ce: 0.010579
 48%|█████████████              | 194/400 [1:21:46<1:22:33, 24.04s/it]2022-01-08 10:42:17,210 iteration 3299 : loss : 0.018219, loss_ce: 0.008015
2022-01-08 10:42:18,549 iteration 3300 : loss : 0.025006, loss_ce: 0.009311
2022-01-08 10:42:19,964 iteration 3301 : loss : 0.024265, loss_ce: 0.010479
2022-01-08 10:42:21,317 iteration 3302 : loss : 0.023214, loss_ce: 0.008399
2022-01-08 10:42:22,649 iteration 3303 : loss : 0.020036, loss_ce: 0.008045
2022-01-08 10:42:24,017 iteration 3304 : loss : 0.025145, loss_ce: 0.009642
2022-01-08 10:42:25,398 iteration 3305 : loss : 0.021797, loss_ce: 0.007503
2022-01-08 10:42:26,836 iteration 3306 : loss : 0.027389, loss_ce: 0.010438
2022-01-08 10:42:28,181 iteration 3307 : loss : 0.020316, loss_ce: 0.008021
2022-01-08 10:42:29,554 iteration 3308 : loss : 0.030467, loss_ce: 0.012380
2022-01-08 10:42:30,898 iteration 3309 : loss : 0.025859, loss_ce: 0.005861
2022-01-08 10:42:32,315 iteration 3310 : loss : 0.045021, loss_ce: 0.015356
2022-01-08 10:42:33,770 iteration 3311 : loss : 0.024364, loss_ce: 0.009298
2022-01-08 10:42:35,078 iteration 3312 : loss : 0.023782, loss_ce: 0.009391
2022-01-08 10:42:36,404 iteration 3313 : loss : 0.016665, loss_ce: 0.006064
2022-01-08 10:42:37,820 iteration 3314 : loss : 0.029015, loss_ce: 0.009480
2022-01-08 10:42:37,820 Training Data Eval:
2022-01-08 10:42:44,676   Average segmentation loss on training set: 0.0166
2022-01-08 10:42:44,677 Validation Data Eval:
2022-01-08 10:42:47,060   Average segmentation loss on validation set: 0.0753
2022-01-08 10:42:48,390 iteration 3315 : loss : 0.017628, loss_ce: 0.008224
 49%|█████████████▏             | 195/400 [1:22:19<1:30:49, 26.58s/it]2022-01-08 10:42:49,749 iteration 3316 : loss : 0.014202, loss_ce: 0.005378
2022-01-08 10:42:51,091 iteration 3317 : loss : 0.020567, loss_ce: 0.008052
2022-01-08 10:42:52,562 iteration 3318 : loss : 0.035214, loss_ce: 0.013562
2022-01-08 10:42:53,898 iteration 3319 : loss : 0.023164, loss_ce: 0.008247
2022-01-08 10:42:55,243 iteration 3320 : loss : 0.022460, loss_ce: 0.009478
2022-01-08 10:42:56,618 iteration 3321 : loss : 0.022918, loss_ce: 0.007550
2022-01-08 10:42:57,990 iteration 3322 : loss : 0.033307, loss_ce: 0.014759
2022-01-08 10:42:59,307 iteration 3323 : loss : 0.026576, loss_ce: 0.008965
2022-01-08 10:43:00,749 iteration 3324 : loss : 0.033997, loss_ce: 0.019721
2022-01-08 10:43:02,061 iteration 3325 : loss : 0.026523, loss_ce: 0.008435
2022-01-08 10:43:03,443 iteration 3326 : loss : 0.023446, loss_ce: 0.008992
2022-01-08 10:43:04,899 iteration 3327 : loss : 0.034977, loss_ce: 0.011761
2022-01-08 10:43:06,258 iteration 3328 : loss : 0.026717, loss_ce: 0.010434
2022-01-08 10:43:07,585 iteration 3329 : loss : 0.018578, loss_ce: 0.008468
2022-01-08 10:43:08,932 iteration 3330 : loss : 0.029422, loss_ce: 0.009220
2022-01-08 10:43:10,310 iteration 3331 : loss : 0.037140, loss_ce: 0.014324
2022-01-08 10:43:11,585 iteration 3332 : loss : 0.016758, loss_ce: 0.007683
 49%|█████████████▏             | 196/400 [1:22:42<1:26:56, 25.57s/it]2022-01-08 10:43:12,974 iteration 3333 : loss : 0.021451, loss_ce: 0.009699
2022-01-08 10:43:14,361 iteration 3334 : loss : 0.017824, loss_ce: 0.007907
2022-01-08 10:43:15,637 iteration 3335 : loss : 0.016147, loss_ce: 0.006486
2022-01-08 10:43:16,995 iteration 3336 : loss : 0.026791, loss_ce: 0.008952
2022-01-08 10:43:18,396 iteration 3337 : loss : 0.034486, loss_ce: 0.015493
2022-01-08 10:43:19,847 iteration 3338 : loss : 0.029872, loss_ce: 0.017377
2022-01-08 10:43:21,204 iteration 3339 : loss : 0.030628, loss_ce: 0.009202
2022-01-08 10:43:22,539 iteration 3340 : loss : 0.021608, loss_ce: 0.008709
2022-01-08 10:43:23,898 iteration 3341 : loss : 0.018358, loss_ce: 0.006553
2022-01-08 10:43:25,304 iteration 3342 : loss : 0.043437, loss_ce: 0.021994
2022-01-08 10:43:26,683 iteration 3343 : loss : 0.024213, loss_ce: 0.009563
2022-01-08 10:43:28,055 iteration 3344 : loss : 0.029991, loss_ce: 0.010834
2022-01-08 10:43:29,405 iteration 3345 : loss : 0.021487, loss_ce: 0.007936
2022-01-08 10:43:30,778 iteration 3346 : loss : 0.018064, loss_ce: 0.007818
2022-01-08 10:43:32,176 iteration 3347 : loss : 0.039388, loss_ce: 0.012459
2022-01-08 10:43:33,619 iteration 3348 : loss : 0.030768, loss_ce: 0.011322
2022-01-08 10:43:34,993 iteration 3349 : loss : 0.030123, loss_ce: 0.011911
 49%|█████████████▎             | 197/400 [1:23:05<1:24:18, 24.92s/it]2022-01-08 10:43:36,341 iteration 3350 : loss : 0.019705, loss_ce: 0.007732
2022-01-08 10:43:37,648 iteration 3351 : loss : 0.020985, loss_ce: 0.008615
2022-01-08 10:43:39,011 iteration 3352 : loss : 0.024490, loss_ce: 0.009787
2022-01-08 10:43:40,409 iteration 3353 : loss : 0.023072, loss_ce: 0.008576
2022-01-08 10:43:41,781 iteration 3354 : loss : 0.020198, loss_ce: 0.006357
2022-01-08 10:43:43,211 iteration 3355 : loss : 0.026501, loss_ce: 0.016062
2022-01-08 10:43:44,605 iteration 3356 : loss : 0.056186, loss_ce: 0.016169
2022-01-08 10:43:46,008 iteration 3357 : loss : 0.028614, loss_ce: 0.010212
2022-01-08 10:43:47,438 iteration 3358 : loss : 0.026706, loss_ce: 0.009708
2022-01-08 10:43:48,836 iteration 3359 : loss : 0.026091, loss_ce: 0.011017
2022-01-08 10:43:50,283 iteration 3360 : loss : 0.026462, loss_ce: 0.010833
2022-01-08 10:43:51,596 iteration 3361 : loss : 0.026235, loss_ce: 0.006540
2022-01-08 10:43:52,933 iteration 3362 : loss : 0.021307, loss_ce: 0.008564
2022-01-08 10:43:54,249 iteration 3363 : loss : 0.021448, loss_ce: 0.010010
2022-01-08 10:43:55,631 iteration 3364 : loss : 0.024857, loss_ce: 0.008432
2022-01-08 10:43:56,949 iteration 3365 : loss : 0.021575, loss_ce: 0.011647
2022-01-08 10:43:58,305 iteration 3366 : loss : 0.020348, loss_ce: 0.007423
 50%|█████████████▎             | 198/400 [1:23:28<1:22:16, 24.44s/it]2022-01-08 10:43:59,706 iteration 3367 : loss : 0.026606, loss_ce: 0.010947
2022-01-08 10:44:01,063 iteration 3368 : loss : 0.023075, loss_ce: 0.010186
2022-01-08 10:44:02,448 iteration 3369 : loss : 0.023266, loss_ce: 0.007806
2022-01-08 10:44:03,830 iteration 3370 : loss : 0.025473, loss_ce: 0.010970
2022-01-08 10:44:05,109 iteration 3371 : loss : 0.015834, loss_ce: 0.006522
2022-01-08 10:44:06,463 iteration 3372 : loss : 0.024581, loss_ce: 0.012474
2022-01-08 10:44:07,820 iteration 3373 : loss : 0.019653, loss_ce: 0.009584
2022-01-08 10:44:09,132 iteration 3374 : loss : 0.018335, loss_ce: 0.007982
2022-01-08 10:44:10,457 iteration 3375 : loss : 0.019009, loss_ce: 0.007917
2022-01-08 10:44:11,796 iteration 3376 : loss : 0.026876, loss_ce: 0.011587
2022-01-08 10:44:13,206 iteration 3377 : loss : 0.028309, loss_ce: 0.008451
2022-01-08 10:44:14,552 iteration 3378 : loss : 0.027056, loss_ce: 0.009750
2022-01-08 10:44:15,822 iteration 3379 : loss : 0.018618, loss_ce: 0.008156
2022-01-08 10:44:17,240 iteration 3380 : loss : 0.028588, loss_ce: 0.010884
2022-01-08 10:44:18,544 iteration 3381 : loss : 0.032080, loss_ce: 0.004878
2022-01-08 10:44:19,918 iteration 3382 : loss : 0.054825, loss_ce: 0.024240
2022-01-08 10:44:21,257 iteration 3383 : loss : 0.024016, loss_ce: 0.008290
 50%|█████████████▍             | 199/400 [1:23:51<1:20:22, 23.99s/it]2022-01-08 10:44:22,607 iteration 3384 : loss : 0.020348, loss_ce: 0.007475
2022-01-08 10:44:24,007 iteration 3385 : loss : 0.025785, loss_ce: 0.013717
2022-01-08 10:44:25,431 iteration 3386 : loss : 0.028406, loss_ce: 0.012815
2022-01-08 10:44:26,829 iteration 3387 : loss : 0.025565, loss_ce: 0.010253
2022-01-08 10:44:28,131 iteration 3388 : loss : 0.019928, loss_ce: 0.008194
2022-01-08 10:44:29,521 iteration 3389 : loss : 0.034447, loss_ce: 0.013580
2022-01-08 10:44:30,818 iteration 3390 : loss : 0.028102, loss_ce: 0.010170
2022-01-08 10:44:32,264 iteration 3391 : loss : 0.036292, loss_ce: 0.012001
2022-01-08 10:44:33,649 iteration 3392 : loss : 0.022869, loss_ce: 0.009163
2022-01-08 10:44:35,039 iteration 3393 : loss : 0.031928, loss_ce: 0.011122
2022-01-08 10:44:36,321 iteration 3394 : loss : 0.027055, loss_ce: 0.009359
2022-01-08 10:44:37,799 iteration 3395 : loss : 0.037670, loss_ce: 0.012571
2022-01-08 10:44:39,185 iteration 3396 : loss : 0.037509, loss_ce: 0.016464
2022-01-08 10:44:40,511 iteration 3397 : loss : 0.025287, loss_ce: 0.010031
2022-01-08 10:44:41,907 iteration 3398 : loss : 0.030200, loss_ce: 0.010455
2022-01-08 10:44:43,289 iteration 3399 : loss : 0.025105, loss_ce: 0.011763
2022-01-08 10:44:43,289 Training Data Eval:
2022-01-08 10:44:50,142   Average segmentation loss on training set: 0.0173
2022-01-08 10:44:50,142 Validation Data Eval:
2022-01-08 10:44:52,508   Average segmentation loss on validation set: 0.0800
2022-01-08 10:44:53,879 iteration 3400 : loss : 0.025710, loss_ce: 0.007853
 50%|█████████████▌             | 200/400 [1:24:24<1:28:36, 26.58s/it]2022-01-08 10:44:55,408 iteration 3401 : loss : 0.026730, loss_ce: 0.011621
2022-01-08 10:44:56,824 iteration 3402 : loss : 0.023571, loss_ce: 0.008717
2022-01-08 10:44:58,156 iteration 3403 : loss : 0.024035, loss_ce: 0.011287
2022-01-08 10:44:59,505 iteration 3404 : loss : 0.030150, loss_ce: 0.011406
2022-01-08 10:45:00,861 iteration 3405 : loss : 0.027514, loss_ce: 0.011152
2022-01-08 10:45:02,282 iteration 3406 : loss : 0.032334, loss_ce: 0.011382
2022-01-08 10:45:03,646 iteration 3407 : loss : 0.036341, loss_ce: 0.014296
2022-01-08 10:45:05,036 iteration 3408 : loss : 0.022171, loss_ce: 0.007720
2022-01-08 10:45:06,438 iteration 3409 : loss : 0.034229, loss_ce: 0.016443
2022-01-08 10:45:07,819 iteration 3410 : loss : 0.027189, loss_ce: 0.009835
2022-01-08 10:45:09,235 iteration 3411 : loss : 0.027650, loss_ce: 0.010957
2022-01-08 10:45:10,545 iteration 3412 : loss : 0.029558, loss_ce: 0.009991
2022-01-08 10:45:11,839 iteration 3413 : loss : 0.017889, loss_ce: 0.008227
2022-01-08 10:45:13,248 iteration 3414 : loss : 0.026762, loss_ce: 0.006270
2022-01-08 10:45:14,661 iteration 3415 : loss : 0.018970, loss_ce: 0.007267
2022-01-08 10:45:16,075 iteration 3416 : loss : 0.020135, loss_ce: 0.006618
2022-01-08 10:45:17,512 iteration 3417 : loss : 0.036501, loss_ce: 0.014258
 50%|█████████████▌             | 201/400 [1:24:48<1:25:13, 25.70s/it]2022-01-08 10:45:18,918 iteration 3418 : loss : 0.020137, loss_ce: 0.009444
2022-01-08 10:45:20,312 iteration 3419 : loss : 0.025470, loss_ce: 0.008575
2022-01-08 10:45:21,764 iteration 3420 : loss : 0.032399, loss_ce: 0.010498
2022-01-08 10:45:23,146 iteration 3421 : loss : 0.032147, loss_ce: 0.011928
2022-01-08 10:45:24,464 iteration 3422 : loss : 0.019993, loss_ce: 0.006540
2022-01-08 10:45:25,838 iteration 3423 : loss : 0.019129, loss_ce: 0.007727
2022-01-08 10:45:27,207 iteration 3424 : loss : 0.032682, loss_ce: 0.014583
2022-01-08 10:45:28,520 iteration 3425 : loss : 0.022123, loss_ce: 0.010844
2022-01-08 10:45:29,894 iteration 3426 : loss : 0.027217, loss_ce: 0.009177
2022-01-08 10:45:31,273 iteration 3427 : loss : 0.038217, loss_ce: 0.012803
2022-01-08 10:45:32,659 iteration 3428 : loss : 0.041936, loss_ce: 0.015767
2022-01-08 10:45:34,076 iteration 3429 : loss : 0.027716, loss_ce: 0.010995
2022-01-08 10:45:35,492 iteration 3430 : loss : 0.036545, loss_ce: 0.015338
2022-01-08 10:45:36,789 iteration 3431 : loss : 0.021569, loss_ce: 0.008352
2022-01-08 10:45:38,121 iteration 3432 : loss : 0.023777, loss_ce: 0.007694
2022-01-08 10:45:39,455 iteration 3433 : loss : 0.027202, loss_ce: 0.010384
2022-01-08 10:45:40,869 iteration 3434 : loss : 0.044815, loss_ce: 0.016553
 50%|█████████████▋             | 202/400 [1:25:11<1:22:29, 25.00s/it]2022-01-08 10:45:42,196 iteration 3435 : loss : 0.026234, loss_ce: 0.007832
2022-01-08 10:45:43,536 iteration 3436 : loss : 0.019249, loss_ce: 0.007103
2022-01-08 10:45:44,994 iteration 3437 : loss : 0.025620, loss_ce: 0.012599
2022-01-08 10:45:46,422 iteration 3438 : loss : 0.019533, loss_ce: 0.008837
2022-01-08 10:45:47,825 iteration 3439 : loss : 0.025956, loss_ce: 0.009622
2022-01-08 10:45:49,151 iteration 3440 : loss : 0.018731, loss_ce: 0.006883
2022-01-08 10:45:50,503 iteration 3441 : loss : 0.027194, loss_ce: 0.010309
2022-01-08 10:45:51,975 iteration 3442 : loss : 0.026670, loss_ce: 0.009001
2022-01-08 10:45:53,383 iteration 3443 : loss : 0.022490, loss_ce: 0.007062
2022-01-08 10:45:54,793 iteration 3444 : loss : 0.027016, loss_ce: 0.010637
2022-01-08 10:45:56,165 iteration 3445 : loss : 0.022524, loss_ce: 0.006379
2022-01-08 10:45:57,517 iteration 3446 : loss : 0.022146, loss_ce: 0.008963
2022-01-08 10:45:58,877 iteration 3447 : loss : 0.026520, loss_ce: 0.010995
2022-01-08 10:46:00,296 iteration 3448 : loss : 0.031789, loss_ce: 0.008389
2022-01-08 10:46:01,679 iteration 3449 : loss : 0.027007, loss_ce: 0.010048
2022-01-08 10:46:03,058 iteration 3450 : loss : 0.028655, loss_ce: 0.012884
2022-01-08 10:46:04,542 iteration 3451 : loss : 0.023373, loss_ce: 0.008936
 51%|█████████████▋             | 203/400 [1:25:35<1:20:45, 24.60s/it]2022-01-08 10:46:05,945 iteration 3452 : loss : 0.017949, loss_ce: 0.006734
2022-01-08 10:46:07,291 iteration 3453 : loss : 0.029420, loss_ce: 0.013561
2022-01-08 10:46:08,628 iteration 3454 : loss : 0.022520, loss_ce: 0.007697
2022-01-08 10:46:10,068 iteration 3455 : loss : 0.038699, loss_ce: 0.016103
2022-01-08 10:46:11,394 iteration 3456 : loss : 0.018229, loss_ce: 0.005991
2022-01-08 10:46:12,858 iteration 3457 : loss : 0.026404, loss_ce: 0.010685
2022-01-08 10:46:14,268 iteration 3458 : loss : 0.034244, loss_ce: 0.013411
2022-01-08 10:46:15,633 iteration 3459 : loss : 0.023073, loss_ce: 0.009524
2022-01-08 10:46:16,990 iteration 3460 : loss : 0.023847, loss_ce: 0.009819
2022-01-08 10:46:18,353 iteration 3461 : loss : 0.019905, loss_ce: 0.007320
2022-01-08 10:46:19,704 iteration 3462 : loss : 0.023323, loss_ce: 0.009859
2022-01-08 10:46:21,108 iteration 3463 : loss : 0.025195, loss_ce: 0.010040
2022-01-08 10:46:22,415 iteration 3464 : loss : 0.022634, loss_ce: 0.007960
2022-01-08 10:46:23,827 iteration 3465 : loss : 0.024299, loss_ce: 0.006960
2022-01-08 10:46:25,214 iteration 3466 : loss : 0.018432, loss_ce: 0.006133
2022-01-08 10:46:26,588 iteration 3467 : loss : 0.024306, loss_ce: 0.013343
2022-01-08 10:46:27,952 iteration 3468 : loss : 0.025151, loss_ce: 0.010599
 51%|█████████████▊             | 204/400 [1:25:58<1:19:11, 24.24s/it]2022-01-08 10:46:29,324 iteration 3469 : loss : 0.016608, loss_ce: 0.005028
2022-01-08 10:46:30,746 iteration 3470 : loss : 0.036360, loss_ce: 0.011350
2022-01-08 10:46:32,044 iteration 3471 : loss : 0.017964, loss_ce: 0.006750
2022-01-08 10:46:33,434 iteration 3472 : loss : 0.034997, loss_ce: 0.018545
2022-01-08 10:46:34,916 iteration 3473 : loss : 0.025968, loss_ce: 0.010722
2022-01-08 10:46:36,266 iteration 3474 : loss : 0.020304, loss_ce: 0.005407
2022-01-08 10:46:37,692 iteration 3475 : loss : 0.041016, loss_ce: 0.010698
2022-01-08 10:46:39,026 iteration 3476 : loss : 0.024506, loss_ce: 0.007195
2022-01-08 10:46:40,424 iteration 3477 : loss : 0.034849, loss_ce: 0.010532
2022-01-08 10:46:41,788 iteration 3478 : loss : 0.017451, loss_ce: 0.006462
2022-01-08 10:46:43,123 iteration 3479 : loss : 0.021217, loss_ce: 0.008768
2022-01-08 10:46:44,414 iteration 3480 : loss : 0.019185, loss_ce: 0.007538
2022-01-08 10:46:45,774 iteration 3481 : loss : 0.022161, loss_ce: 0.008563
2022-01-08 10:46:47,137 iteration 3482 : loss : 0.025408, loss_ce: 0.010384
2022-01-08 10:46:48,566 iteration 3483 : loss : 0.024194, loss_ce: 0.011893
2022-01-08 10:46:49,850 iteration 3484 : loss : 0.020693, loss_ce: 0.008205
2022-01-08 10:46:49,850 Training Data Eval:
2022-01-08 10:46:56,733   Average segmentation loss on training set: 0.0193
2022-01-08 10:46:56,733 Validation Data Eval:
2022-01-08 10:46:59,113   Average segmentation loss on validation set: 0.0687
2022-01-08 10:47:00,578 iteration 3485 : loss : 0.040308, loss_ce: 0.018783
 51%|█████████████▊             | 205/400 [1:26:31<1:26:57, 26.76s/it]2022-01-08 10:47:01,985 iteration 3486 : loss : 0.031421, loss_ce: 0.013390
2022-01-08 10:47:03,296 iteration 3487 : loss : 0.018909, loss_ce: 0.008205
2022-01-08 10:47:04,644 iteration 3488 : loss : 0.031351, loss_ce: 0.012600
2022-01-08 10:47:06,004 iteration 3489 : loss : 0.022974, loss_ce: 0.008779
2022-01-08 10:47:07,396 iteration 3490 : loss : 0.036915, loss_ce: 0.016449
2022-01-08 10:47:08,815 iteration 3491 : loss : 0.025444, loss_ce: 0.009671
2022-01-08 10:47:10,128 iteration 3492 : loss : 0.024978, loss_ce: 0.010199
2022-01-08 10:47:11,596 iteration 3493 : loss : 0.026293, loss_ce: 0.010203
2022-01-08 10:47:12,990 iteration 3494 : loss : 0.027215, loss_ce: 0.007491
2022-01-08 10:47:14,359 iteration 3495 : loss : 0.022326, loss_ce: 0.006820
2022-01-08 10:47:15,778 iteration 3496 : loss : 0.018848, loss_ce: 0.007988
2022-01-08 10:47:17,170 iteration 3497 : loss : 0.026540, loss_ce: 0.012592
2022-01-08 10:47:18,642 iteration 3498 : loss : 0.021085, loss_ce: 0.008050
2022-01-08 10:47:19,973 iteration 3499 : loss : 0.022308, loss_ce: 0.008734
2022-01-08 10:47:21,267 iteration 3500 : loss : 0.016696, loss_ce: 0.005334
2022-01-08 10:47:22,605 iteration 3501 : loss : 0.025141, loss_ce: 0.008075
2022-01-08 10:47:23,994 iteration 3502 : loss : 0.032573, loss_ce: 0.011059
 52%|█████████████▉             | 206/400 [1:26:54<1:23:16, 25.75s/it]2022-01-08 10:47:25,416 iteration 3503 : loss : 0.022607, loss_ce: 0.008766
2022-01-08 10:47:26,755 iteration 3504 : loss : 0.016003, loss_ce: 0.004663
2022-01-08 10:47:28,033 iteration 3505 : loss : 0.018129, loss_ce: 0.008607
2022-01-08 10:47:29,426 iteration 3506 : loss : 0.022254, loss_ce: 0.006752
2022-01-08 10:47:30,779 iteration 3507 : loss : 0.022159, loss_ce: 0.006350
2022-01-08 10:47:32,252 iteration 3508 : loss : 0.030709, loss_ce: 0.013522
2022-01-08 10:47:33,621 iteration 3509 : loss : 0.017848, loss_ce: 0.006722
2022-01-08 10:47:34,929 iteration 3510 : loss : 0.021248, loss_ce: 0.008131
2022-01-08 10:47:36,236 iteration 3511 : loss : 0.016275, loss_ce: 0.007554
2022-01-08 10:47:37,671 iteration 3512 : loss : 0.029583, loss_ce: 0.009062
2022-01-08 10:47:39,045 iteration 3513 : loss : 0.020764, loss_ce: 0.006748
2022-01-08 10:47:40,529 iteration 3514 : loss : 0.033907, loss_ce: 0.017084
2022-01-08 10:47:41,873 iteration 3515 : loss : 0.025154, loss_ce: 0.008200
2022-01-08 10:47:43,204 iteration 3516 : loss : 0.016557, loss_ce: 0.006160
2022-01-08 10:47:44,549 iteration 3517 : loss : 0.023500, loss_ce: 0.008715
2022-01-08 10:47:45,898 iteration 3518 : loss : 0.025229, loss_ce: 0.013831
2022-01-08 10:47:47,284 iteration 3519 : loss : 0.020452, loss_ce: 0.008855
 52%|█████████████▉             | 207/400 [1:27:17<1:20:27, 25.01s/it]2022-01-08 10:47:48,704 iteration 3520 : loss : 0.020258, loss_ce: 0.008654
2022-01-08 10:47:50,040 iteration 3521 : loss : 0.031891, loss_ce: 0.014061
2022-01-08 10:47:51,345 iteration 3522 : loss : 0.019081, loss_ce: 0.008862
2022-01-08 10:47:52,699 iteration 3523 : loss : 0.018325, loss_ce: 0.007219
2022-01-08 10:47:54,095 iteration 3524 : loss : 0.025093, loss_ce: 0.010423
2022-01-08 10:47:55,557 iteration 3525 : loss : 0.026174, loss_ce: 0.011342
2022-01-08 10:47:56,936 iteration 3526 : loss : 0.027112, loss_ce: 0.010788
2022-01-08 10:47:58,347 iteration 3527 : loss : 0.031835, loss_ce: 0.009371
2022-01-08 10:47:59,831 iteration 3528 : loss : 0.036818, loss_ce: 0.016802
2022-01-08 10:48:01,215 iteration 3529 : loss : 0.033117, loss_ce: 0.009212
2022-01-08 10:48:02,620 iteration 3530 : loss : 0.015513, loss_ce: 0.006146
2022-01-08 10:48:03,944 iteration 3531 : loss : 0.024136, loss_ce: 0.007255
2022-01-08 10:48:05,389 iteration 3532 : loss : 0.034946, loss_ce: 0.016024
2022-01-08 10:48:06,745 iteration 3533 : loss : 0.019544, loss_ce: 0.006001
2022-01-08 10:48:08,144 iteration 3534 : loss : 0.029526, loss_ce: 0.011917
2022-01-08 10:48:09,576 iteration 3535 : loss : 0.025733, loss_ce: 0.009782
2022-01-08 10:48:10,973 iteration 3536 : loss : 0.022190, loss_ce: 0.008062
 52%|██████████████             | 208/400 [1:27:41<1:18:46, 24.62s/it]2022-01-08 10:48:12,289 iteration 3537 : loss : 0.024090, loss_ce: 0.010174
2022-01-08 10:48:13,584 iteration 3538 : loss : 0.018036, loss_ce: 0.007139
2022-01-08 10:48:14,910 iteration 3539 : loss : 0.016317, loss_ce: 0.007318
2022-01-08 10:48:16,179 iteration 3540 : loss : 0.018590, loss_ce: 0.004989
2022-01-08 10:48:17,615 iteration 3541 : loss : 0.031254, loss_ce: 0.011842
2022-01-08 10:48:18,982 iteration 3542 : loss : 0.023846, loss_ce: 0.009874
2022-01-08 10:48:20,464 iteration 3543 : loss : 0.033487, loss_ce: 0.010656
2022-01-08 10:48:21,760 iteration 3544 : loss : 0.015694, loss_ce: 0.006436
2022-01-08 10:48:23,123 iteration 3545 : loss : 0.020832, loss_ce: 0.008390
2022-01-08 10:48:24,429 iteration 3546 : loss : 0.019892, loss_ce: 0.007970
2022-01-08 10:48:25,802 iteration 3547 : loss : 0.025545, loss_ce: 0.011522
2022-01-08 10:48:27,136 iteration 3548 : loss : 0.023644, loss_ce: 0.007945
2022-01-08 10:48:28,514 iteration 3549 : loss : 0.023788, loss_ce: 0.010511
2022-01-08 10:48:29,914 iteration 3550 : loss : 0.018376, loss_ce: 0.008061
2022-01-08 10:48:31,335 iteration 3551 : loss : 0.026101, loss_ce: 0.008033
2022-01-08 10:48:32,757 iteration 3552 : loss : 0.045629, loss_ce: 0.020319
2022-01-08 10:48:34,169 iteration 3553 : loss : 0.025009, loss_ce: 0.011224
 52%|██████████████             | 209/400 [1:28:04<1:17:00, 24.19s/it]2022-01-08 10:48:35,587 iteration 3554 : loss : 0.029894, loss_ce: 0.013268
2022-01-08 10:48:36,924 iteration 3555 : loss : 0.021592, loss_ce: 0.006680
2022-01-08 10:48:38,277 iteration 3556 : loss : 0.020491, loss_ce: 0.009690
2022-01-08 10:48:39,735 iteration 3557 : loss : 0.021568, loss_ce: 0.006237
2022-01-08 10:48:41,053 iteration 3558 : loss : 0.033701, loss_ce: 0.010789
2022-01-08 10:48:42,405 iteration 3559 : loss : 0.018312, loss_ce: 0.005177
2022-01-08 10:48:43,666 iteration 3560 : loss : 0.018734, loss_ce: 0.009621
2022-01-08 10:48:45,109 iteration 3561 : loss : 0.024629, loss_ce: 0.011280
2022-01-08 10:48:46,444 iteration 3562 : loss : 0.021803, loss_ce: 0.007784
2022-01-08 10:48:47,844 iteration 3563 : loss : 0.019976, loss_ce: 0.007385
2022-01-08 10:48:49,204 iteration 3564 : loss : 0.030538, loss_ce: 0.011595
2022-01-08 10:48:50,674 iteration 3565 : loss : 0.027484, loss_ce: 0.009700
2022-01-08 10:48:52,034 iteration 3566 : loss : 0.026788, loss_ce: 0.009917
2022-01-08 10:48:53,476 iteration 3567 : loss : 0.029038, loss_ce: 0.014592
2022-01-08 10:48:54,890 iteration 3568 : loss : 0.039178, loss_ce: 0.015127
2022-01-08 10:48:56,310 iteration 3569 : loss : 0.032919, loss_ce: 0.008515
2022-01-08 10:48:56,310 Training Data Eval:
2022-01-08 10:49:03,162   Average segmentation loss on training set: 0.0145
2022-01-08 10:49:03,162 Validation Data Eval:
2022-01-08 10:49:05,537   Average segmentation loss on validation set: 0.0634
2022-01-08 10:49:10,804 Found new lowest validation loss at iteration 3569! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 10:49:12,126 iteration 3570 : loss : 0.030555, loss_ce: 0.011831
 52%|██████████████▏            | 210/400 [1:28:42<1:29:40, 28.32s/it]2022-01-08 10:49:13,443 iteration 3571 : loss : 0.019451, loss_ce: 0.008508
2022-01-08 10:49:14,729 iteration 3572 : loss : 0.021987, loss_ce: 0.006170
2022-01-08 10:49:16,007 iteration 3573 : loss : 0.028540, loss_ce: 0.011733
2022-01-08 10:49:17,254 iteration 3574 : loss : 0.017481, loss_ce: 0.006985
2022-01-08 10:49:18,514 iteration 3575 : loss : 0.024587, loss_ce: 0.011407
2022-01-08 10:49:19,810 iteration 3576 : loss : 0.026440, loss_ce: 0.011569
2022-01-08 10:49:21,145 iteration 3577 : loss : 0.019128, loss_ce: 0.008679
2022-01-08 10:49:22,483 iteration 3578 : loss : 0.029879, loss_ce: 0.013480
2022-01-08 10:49:23,743 iteration 3579 : loss : 0.021645, loss_ce: 0.007161
2022-01-08 10:49:25,074 iteration 3580 : loss : 0.023923, loss_ce: 0.009028
2022-01-08 10:49:26,395 iteration 3581 : loss : 0.022232, loss_ce: 0.005037
2022-01-08 10:49:27,774 iteration 3582 : loss : 0.019124, loss_ce: 0.006820
2022-01-08 10:49:29,193 iteration 3583 : loss : 0.030683, loss_ce: 0.011665
2022-01-08 10:49:30,533 iteration 3584 : loss : 0.022186, loss_ce: 0.009336
2022-01-08 10:49:31,905 iteration 3585 : loss : 0.017782, loss_ce: 0.006112
2022-01-08 10:49:33,228 iteration 3586 : loss : 0.025863, loss_ce: 0.009014
2022-01-08 10:49:34,599 iteration 3587 : loss : 0.024334, loss_ce: 0.010394
 53%|██████████████▏            | 211/400 [1:29:05<1:23:41, 26.57s/it]2022-01-08 10:49:35,939 iteration 3588 : loss : 0.022770, loss_ce: 0.008137
2022-01-08 10:49:37,344 iteration 3589 : loss : 0.034249, loss_ce: 0.011821
2022-01-08 10:49:38,730 iteration 3590 : loss : 0.021051, loss_ce: 0.009096
2022-01-08 10:49:40,094 iteration 3591 : loss : 0.017440, loss_ce: 0.005292
2022-01-08 10:49:41,521 iteration 3592 : loss : 0.023906, loss_ce: 0.009724
2022-01-08 10:49:42,905 iteration 3593 : loss : 0.038856, loss_ce: 0.011090
2022-01-08 10:49:44,293 iteration 3594 : loss : 0.018906, loss_ce: 0.005152
2022-01-08 10:49:45,794 iteration 3595 : loss : 0.066057, loss_ce: 0.013090
2022-01-08 10:49:47,098 iteration 3596 : loss : 0.033503, loss_ce: 0.015388
2022-01-08 10:49:48,541 iteration 3597 : loss : 0.030697, loss_ce: 0.010720
2022-01-08 10:49:49,903 iteration 3598 : loss : 0.039029, loss_ce: 0.022895
2022-01-08 10:49:51,276 iteration 3599 : loss : 0.034521, loss_ce: 0.013662
2022-01-08 10:49:52,635 iteration 3600 : loss : 0.030177, loss_ce: 0.013427
2022-01-08 10:49:54,007 iteration 3601 : loss : 0.024836, loss_ce: 0.011102
2022-01-08 10:49:55,352 iteration 3602 : loss : 0.025421, loss_ce: 0.010716
2022-01-08 10:49:56,690 iteration 3603 : loss : 0.016197, loss_ce: 0.005914
2022-01-08 10:49:58,011 iteration 3604 : loss : 0.018241, loss_ce: 0.006612
 53%|██████████████▎            | 212/400 [1:29:28<1:20:16, 25.62s/it]2022-01-08 10:49:59,407 iteration 3605 : loss : 0.023126, loss_ce: 0.008601
2022-01-08 10:50:00,777 iteration 3606 : loss : 0.027293, loss_ce: 0.009056
2022-01-08 10:50:02,093 iteration 3607 : loss : 0.023081, loss_ce: 0.008402
2022-01-08 10:50:03,412 iteration 3608 : loss : 0.024609, loss_ce: 0.010835
2022-01-08 10:50:04,804 iteration 3609 : loss : 0.026314, loss_ce: 0.008550
2022-01-08 10:50:06,140 iteration 3610 : loss : 0.025035, loss_ce: 0.010871
2022-01-08 10:50:07,543 iteration 3611 : loss : 0.028350, loss_ce: 0.014061
2022-01-08 10:50:08,971 iteration 3612 : loss : 0.029585, loss_ce: 0.011926
2022-01-08 10:50:10,344 iteration 3613 : loss : 0.034734, loss_ce: 0.010831
2022-01-08 10:50:11,743 iteration 3614 : loss : 0.019800, loss_ce: 0.006441
2022-01-08 10:50:13,085 iteration 3615 : loss : 0.034548, loss_ce: 0.012039
2022-01-08 10:50:14,521 iteration 3616 : loss : 0.029613, loss_ce: 0.009008
2022-01-08 10:50:15,865 iteration 3617 : loss : 0.039262, loss_ce: 0.016734
2022-01-08 10:50:17,217 iteration 3618 : loss : 0.029647, loss_ce: 0.014207
2022-01-08 10:50:18,571 iteration 3619 : loss : 0.020754, loss_ce: 0.008751
2022-01-08 10:50:19,985 iteration 3620 : loss : 0.036968, loss_ce: 0.020393
2022-01-08 10:50:21,339 iteration 3621 : loss : 0.035768, loss_ce: 0.017881
 53%|██████████████▍            | 213/400 [1:29:52<1:17:42, 24.93s/it]2022-01-08 10:50:22,806 iteration 3622 : loss : 0.040919, loss_ce: 0.016353
2022-01-08 10:50:24,165 iteration 3623 : loss : 0.030142, loss_ce: 0.013385
2022-01-08 10:50:25,602 iteration 3624 : loss : 0.037158, loss_ce: 0.013046
2022-01-08 10:50:27,027 iteration 3625 : loss : 0.038502, loss_ce: 0.018670
2022-01-08 10:50:28,395 iteration 3626 : loss : 0.028343, loss_ce: 0.010871
2022-01-08 10:50:29,804 iteration 3627 : loss : 0.023866, loss_ce: 0.011479
2022-01-08 10:50:31,192 iteration 3628 : loss : 0.025462, loss_ce: 0.008013
2022-01-08 10:50:32,635 iteration 3629 : loss : 0.034580, loss_ce: 0.015397
2022-01-08 10:50:34,005 iteration 3630 : loss : 0.023940, loss_ce: 0.010301
2022-01-08 10:50:35,309 iteration 3631 : loss : 0.020662, loss_ce: 0.008911
2022-01-08 10:50:36,627 iteration 3632 : loss : 0.023456, loss_ce: 0.010661
2022-01-08 10:50:38,048 iteration 3633 : loss : 0.045437, loss_ce: 0.017488
2022-01-08 10:50:39,407 iteration 3634 : loss : 0.030894, loss_ce: 0.013010
2022-01-08 10:50:40,731 iteration 3635 : loss : 0.022027, loss_ce: 0.007915
2022-01-08 10:50:42,059 iteration 3636 : loss : 0.022163, loss_ce: 0.008718
2022-01-08 10:50:43,480 iteration 3637 : loss : 0.037720, loss_ce: 0.013027
2022-01-08 10:50:44,793 iteration 3638 : loss : 0.021520, loss_ce: 0.007573
 54%|██████████████▍            | 214/400 [1:30:15<1:15:54, 24.49s/it]2022-01-08 10:50:46,178 iteration 3639 : loss : 0.032908, loss_ce: 0.018198
2022-01-08 10:50:47,592 iteration 3640 : loss : 0.028216, loss_ce: 0.011291
2022-01-08 10:50:48,960 iteration 3641 : loss : 0.027816, loss_ce: 0.009965
2022-01-08 10:50:50,373 iteration 3642 : loss : 0.023344, loss_ce: 0.010869
2022-01-08 10:50:51,753 iteration 3643 : loss : 0.028599, loss_ce: 0.013718
2022-01-08 10:50:53,107 iteration 3644 : loss : 0.038028, loss_ce: 0.019684
2022-01-08 10:50:54,440 iteration 3645 : loss : 0.022238, loss_ce: 0.008552
2022-01-08 10:50:55,827 iteration 3646 : loss : 0.020665, loss_ce: 0.008254
2022-01-08 10:50:57,240 iteration 3647 : loss : 0.039193, loss_ce: 0.017181
2022-01-08 10:50:58,700 iteration 3648 : loss : 0.058617, loss_ce: 0.031430
2022-01-08 10:51:00,071 iteration 3649 : loss : 0.035971, loss_ce: 0.010923
2022-01-08 10:51:01,438 iteration 3650 : loss : 0.033065, loss_ce: 0.012053
2022-01-08 10:51:02,832 iteration 3651 : loss : 0.028527, loss_ce: 0.011792
2022-01-08 10:51:04,167 iteration 3652 : loss : 0.026934, loss_ce: 0.011640
2022-01-08 10:51:05,599 iteration 3653 : loss : 0.064101, loss_ce: 0.013710
2022-01-08 10:51:06,966 iteration 3654 : loss : 0.029736, loss_ce: 0.009511
2022-01-08 10:51:06,966 Training Data Eval:
2022-01-08 10:51:13,824   Average segmentation loss on training set: 0.0207
2022-01-08 10:51:13,824 Validation Data Eval:
2022-01-08 10:51:16,191   Average segmentation loss on validation set: 0.1237
2022-01-08 10:51:17,478 iteration 3655 : loss : 0.025337, loss_ce: 0.007835
 54%|██████████████▌            | 215/400 [1:30:48<1:23:05, 26.95s/it]2022-01-08 10:51:18,950 iteration 3656 : loss : 0.043242, loss_ce: 0.009695
2022-01-08 10:51:20,289 iteration 3657 : loss : 0.024800, loss_ce: 0.011577
2022-01-08 10:51:21,666 iteration 3658 : loss : 0.027195, loss_ce: 0.010907
2022-01-08 10:51:23,081 iteration 3659 : loss : 0.054633, loss_ce: 0.012673
2022-01-08 10:51:24,555 iteration 3660 : loss : 0.038160, loss_ce: 0.011892
2022-01-08 10:51:26,028 iteration 3661 : loss : 0.034539, loss_ce: 0.015247
2022-01-08 10:51:27,308 iteration 3662 : loss : 0.032171, loss_ce: 0.011925
2022-01-08 10:51:28,647 iteration 3663 : loss : 0.017214, loss_ce: 0.005301
2022-01-08 10:51:30,004 iteration 3664 : loss : 0.037213, loss_ce: 0.011418
2022-01-08 10:51:31,370 iteration 3665 : loss : 0.017700, loss_ce: 0.007738
2022-01-08 10:51:32,846 iteration 3666 : loss : 0.031064, loss_ce: 0.014198
2022-01-08 10:51:34,249 iteration 3667 : loss : 0.031898, loss_ce: 0.013248
2022-01-08 10:51:35,660 iteration 3668 : loss : 0.029804, loss_ce: 0.014988
2022-01-08 10:51:37,033 iteration 3669 : loss : 0.027782, loss_ce: 0.013112
2022-01-08 10:51:38,382 iteration 3670 : loss : 0.035863, loss_ce: 0.012021
2022-01-08 10:51:39,796 iteration 3671 : loss : 0.024311, loss_ce: 0.009366
2022-01-08 10:51:41,123 iteration 3672 : loss : 0.030608, loss_ce: 0.014844
 54%|██████████████▌            | 216/400 [1:31:11<1:19:35, 25.96s/it]2022-01-08 10:51:42,512 iteration 3673 : loss : 0.022436, loss_ce: 0.011414
2022-01-08 10:51:43,883 iteration 3674 : loss : 0.035518, loss_ce: 0.017891
2022-01-08 10:51:45,233 iteration 3675 : loss : 0.018418, loss_ce: 0.009772
2022-01-08 10:51:46,586 iteration 3676 : loss : 0.019677, loss_ce: 0.006906
2022-01-08 10:51:47,936 iteration 3677 : loss : 0.032613, loss_ce: 0.016296
2022-01-08 10:51:49,380 iteration 3678 : loss : 0.039493, loss_ce: 0.014332
2022-01-08 10:51:50,734 iteration 3679 : loss : 0.026499, loss_ce: 0.010747
2022-01-08 10:51:52,077 iteration 3680 : loss : 0.034393, loss_ce: 0.011594
2022-01-08 10:51:53,485 iteration 3681 : loss : 0.032361, loss_ce: 0.014760
2022-01-08 10:51:54,825 iteration 3682 : loss : 0.022243, loss_ce: 0.009133
2022-01-08 10:51:56,235 iteration 3683 : loss : 0.085105, loss_ce: 0.009631
2022-01-08 10:51:57,563 iteration 3684 : loss : 0.016423, loss_ce: 0.005231
2022-01-08 10:51:58,910 iteration 3685 : loss : 0.026725, loss_ce: 0.011473
2022-01-08 10:52:00,296 iteration 3686 : loss : 0.032168, loss_ce: 0.012029
2022-01-08 10:52:01,666 iteration 3687 : loss : 0.037109, loss_ce: 0.012791
2022-01-08 10:52:03,041 iteration 3688 : loss : 0.067124, loss_ce: 0.040079
2022-01-08 10:52:04,346 iteration 3689 : loss : 0.047767, loss_ce: 0.018710
 54%|██████████████▋            | 217/400 [1:31:35<1:16:40, 25.14s/it]2022-01-08 10:52:05,707 iteration 3690 : loss : 0.029891, loss_ce: 0.007066
2022-01-08 10:52:07,142 iteration 3691 : loss : 0.053544, loss_ce: 0.014401
2022-01-08 10:52:08,516 iteration 3692 : loss : 0.032677, loss_ce: 0.014974
2022-01-08 10:52:09,952 iteration 3693 : loss : 0.046765, loss_ce: 0.015545
2022-01-08 10:52:11,314 iteration 3694 : loss : 0.020543, loss_ce: 0.008684
2022-01-08 10:52:12,625 iteration 3695 : loss : 0.036393, loss_ce: 0.015780
2022-01-08 10:52:14,028 iteration 3696 : loss : 0.039796, loss_ce: 0.021249
2022-01-08 10:52:15,371 iteration 3697 : loss : 0.028445, loss_ce: 0.009557
2022-01-08 10:52:16,741 iteration 3698 : loss : 0.034779, loss_ce: 0.014053
2022-01-08 10:52:18,177 iteration 3699 : loss : 0.048824, loss_ce: 0.019003
2022-01-08 10:52:19,547 iteration 3700 : loss : 0.041899, loss_ce: 0.018006
2022-01-08 10:52:20,886 iteration 3701 : loss : 0.028757, loss_ce: 0.011140
2022-01-08 10:52:22,219 iteration 3702 : loss : 0.042624, loss_ce: 0.017906
2022-01-08 10:52:23,691 iteration 3703 : loss : 0.031778, loss_ce: 0.013613
2022-01-08 10:52:25,012 iteration 3704 : loss : 0.020829, loss_ce: 0.007719
2022-01-08 10:52:26,335 iteration 3705 : loss : 0.020761, loss_ce: 0.007615
2022-01-08 10:52:27,664 iteration 3706 : loss : 0.024812, loss_ce: 0.008312
 55%|██████████████▋            | 218/400 [1:31:58<1:14:35, 24.59s/it]2022-01-08 10:52:29,063 iteration 3707 : loss : 0.030793, loss_ce: 0.011386
2022-01-08 10:52:30,432 iteration 3708 : loss : 0.029744, loss_ce: 0.011640
2022-01-08 10:52:31,792 iteration 3709 : loss : 0.023281, loss_ce: 0.009339
2022-01-08 10:52:33,199 iteration 3710 : loss : 0.041511, loss_ce: 0.017565
2022-01-08 10:52:34,544 iteration 3711 : loss : 0.024406, loss_ce: 0.009419
2022-01-08 10:52:35,931 iteration 3712 : loss : 0.027097, loss_ce: 0.008700
2022-01-08 10:52:37,253 iteration 3713 : loss : 0.027367, loss_ce: 0.009689
2022-01-08 10:52:38,594 iteration 3714 : loss : 0.018746, loss_ce: 0.006389
2022-01-08 10:52:39,968 iteration 3715 : loss : 0.030216, loss_ce: 0.013240
2022-01-08 10:52:41,292 iteration 3716 : loss : 0.022710, loss_ce: 0.007663
2022-01-08 10:52:42,666 iteration 3717 : loss : 0.026166, loss_ce: 0.010814
2022-01-08 10:52:44,085 iteration 3718 : loss : 0.028537, loss_ce: 0.013034
2022-01-08 10:52:45,395 iteration 3719 : loss : 0.019083, loss_ce: 0.005117
2022-01-08 10:52:46,781 iteration 3720 : loss : 0.026833, loss_ce: 0.014024
2022-01-08 10:52:48,145 iteration 3721 : loss : 0.027852, loss_ce: 0.013157
2022-01-08 10:52:49,493 iteration 3722 : loss : 0.018035, loss_ce: 0.006924
2022-01-08 10:52:50,813 iteration 3723 : loss : 0.032990, loss_ce: 0.010011
 55%|██████████████▊            | 219/400 [1:32:21<1:12:52, 24.16s/it]2022-01-08 10:52:52,154 iteration 3724 : loss : 0.021051, loss_ce: 0.010304
2022-01-08 10:52:53,488 iteration 3725 : loss : 0.026695, loss_ce: 0.012737
2022-01-08 10:52:54,910 iteration 3726 : loss : 0.038799, loss_ce: 0.021873
2022-01-08 10:52:56,279 iteration 3727 : loss : 0.021112, loss_ce: 0.008642
2022-01-08 10:52:57,665 iteration 3728 : loss : 0.028629, loss_ce: 0.009972
2022-01-08 10:52:58,997 iteration 3729 : loss : 0.021001, loss_ce: 0.006266
2022-01-08 10:53:00,351 iteration 3730 : loss : 0.023566, loss_ce: 0.009602
2022-01-08 10:53:01,693 iteration 3731 : loss : 0.024116, loss_ce: 0.008270
2022-01-08 10:53:03,078 iteration 3732 : loss : 0.021165, loss_ce: 0.006663
2022-01-08 10:53:04,395 iteration 3733 : loss : 0.029093, loss_ce: 0.012431
2022-01-08 10:53:05,770 iteration 3734 : loss : 0.036213, loss_ce: 0.022515
2022-01-08 10:53:07,138 iteration 3735 : loss : 0.024970, loss_ce: 0.007092
2022-01-08 10:53:08,554 iteration 3736 : loss : 0.035608, loss_ce: 0.015741
2022-01-08 10:53:09,870 iteration 3737 : loss : 0.024009, loss_ce: 0.008166
2022-01-08 10:53:11,224 iteration 3738 : loss : 0.019058, loss_ce: 0.006464
2022-01-08 10:53:12,538 iteration 3739 : loss : 0.021512, loss_ce: 0.009154
2022-01-08 10:53:12,538 Training Data Eval:
2022-01-08 10:53:19,453   Average segmentation loss on training set: 0.0208
2022-01-08 10:53:19,453 Validation Data Eval:
2022-01-08 10:53:21,833   Average segmentation loss on validation set: 0.0730
2022-01-08 10:53:23,352 iteration 3740 : loss : 0.034111, loss_ce: 0.016707
 55%|██████████████▊            | 220/400 [1:32:54<1:20:00, 26.67s/it]2022-01-08 10:53:24,869 iteration 3741 : loss : 0.030929, loss_ce: 0.009060
2022-01-08 10:53:26,217 iteration 3742 : loss : 0.035009, loss_ce: 0.020908
2022-01-08 10:53:27,580 iteration 3743 : loss : 0.028926, loss_ce: 0.010265
2022-01-08 10:53:28,995 iteration 3744 : loss : 0.023249, loss_ce: 0.008872
2022-01-08 10:53:30,420 iteration 3745 : loss : 0.027439, loss_ce: 0.009745
2022-01-08 10:53:31,847 iteration 3746 : loss : 0.031595, loss_ce: 0.013107
2022-01-08 10:53:33,190 iteration 3747 : loss : 0.032716, loss_ce: 0.013080
2022-01-08 10:53:34,540 iteration 3748 : loss : 0.021036, loss_ce: 0.009642
2022-01-08 10:53:35,953 iteration 3749 : loss : 0.031520, loss_ce: 0.010222
2022-01-08 10:53:37,389 iteration 3750 : loss : 0.034041, loss_ce: 0.015400
2022-01-08 10:53:38,775 iteration 3751 : loss : 0.033132, loss_ce: 0.011530
2022-01-08 10:53:40,125 iteration 3752 : loss : 0.025515, loss_ce: 0.008086
2022-01-08 10:53:41,453 iteration 3753 : loss : 0.028867, loss_ce: 0.009562
2022-01-08 10:53:42,825 iteration 3754 : loss : 0.017561, loss_ce: 0.006481
2022-01-08 10:53:44,240 iteration 3755 : loss : 0.024867, loss_ce: 0.011771
2022-01-08 10:53:45,536 iteration 3756 : loss : 0.025656, loss_ce: 0.012317
2022-01-08 10:53:46,863 iteration 3757 : loss : 0.022705, loss_ce: 0.010572
 55%|██████████████▉            | 221/400 [1:33:17<1:16:44, 25.73s/it]2022-01-08 10:53:48,343 iteration 3758 : loss : 0.026677, loss_ce: 0.008926
2022-01-08 10:53:49,665 iteration 3759 : loss : 0.024275, loss_ce: 0.008540
2022-01-08 10:53:51,009 iteration 3760 : loss : 0.021132, loss_ce: 0.008525
2022-01-08 10:53:52,405 iteration 3761 : loss : 0.027213, loss_ce: 0.011074
2022-01-08 10:53:53,821 iteration 3762 : loss : 0.023468, loss_ce: 0.011708
2022-01-08 10:53:55,280 iteration 3763 : loss : 0.041382, loss_ce: 0.017176
2022-01-08 10:53:56,641 iteration 3764 : loss : 0.027172, loss_ce: 0.008682
2022-01-08 10:53:57,960 iteration 3765 : loss : 0.021707, loss_ce: 0.009416
2022-01-08 10:53:59,382 iteration 3766 : loss : 0.035454, loss_ce: 0.011565
2022-01-08 10:54:00,741 iteration 3767 : loss : 0.019205, loss_ce: 0.007178
2022-01-08 10:54:02,056 iteration 3768 : loss : 0.029615, loss_ce: 0.010725
2022-01-08 10:54:03,393 iteration 3769 : loss : 0.022014, loss_ce: 0.009481
2022-01-08 10:54:04,788 iteration 3770 : loss : 0.029319, loss_ce: 0.006045
2022-01-08 10:54:06,261 iteration 3771 : loss : 0.029005, loss_ce: 0.010209
2022-01-08 10:54:07,677 iteration 3772 : loss : 0.029162, loss_ce: 0.007362
2022-01-08 10:54:09,060 iteration 3773 : loss : 0.020605, loss_ce: 0.007756
2022-01-08 10:54:10,480 iteration 3774 : loss : 0.045726, loss_ce: 0.023529
 56%|██████████████▉            | 222/400 [1:33:41<1:14:26, 25.09s/it]2022-01-08 10:54:11,866 iteration 3775 : loss : 0.022515, loss_ce: 0.009200
2022-01-08 10:54:13,169 iteration 3776 : loss : 0.024981, loss_ce: 0.010916
2022-01-08 10:54:14,520 iteration 3777 : loss : 0.017975, loss_ce: 0.005390
2022-01-08 10:54:15,844 iteration 3778 : loss : 0.017387, loss_ce: 0.006861
2022-01-08 10:54:17,167 iteration 3779 : loss : 0.024842, loss_ce: 0.010355
2022-01-08 10:54:18,539 iteration 3780 : loss : 0.027842, loss_ce: 0.010070
2022-01-08 10:54:19,900 iteration 3781 : loss : 0.028210, loss_ce: 0.012738
2022-01-08 10:54:21,285 iteration 3782 : loss : 0.050828, loss_ce: 0.011552
2022-01-08 10:54:22,672 iteration 3783 : loss : 0.026013, loss_ce: 0.011084
2022-01-08 10:54:24,064 iteration 3784 : loss : 0.019208, loss_ce: 0.008552
2022-01-08 10:54:25,399 iteration 3785 : loss : 0.024706, loss_ce: 0.011807
2022-01-08 10:54:26,741 iteration 3786 : loss : 0.028751, loss_ce: 0.014031
2022-01-08 10:54:28,071 iteration 3787 : loss : 0.023197, loss_ce: 0.008283
2022-01-08 10:54:29,498 iteration 3788 : loss : 0.044628, loss_ce: 0.011667
2022-01-08 10:54:30,858 iteration 3789 : loss : 0.025280, loss_ce: 0.007138
2022-01-08 10:54:32,253 iteration 3790 : loss : 0.046416, loss_ce: 0.023719
2022-01-08 10:54:33,642 iteration 3791 : loss : 0.023858, loss_ce: 0.010134
 56%|███████████████            | 223/400 [1:34:04<1:12:18, 24.51s/it]2022-01-08 10:54:35,002 iteration 3792 : loss : 0.019197, loss_ce: 0.007670
2022-01-08 10:54:36,422 iteration 3793 : loss : 0.022292, loss_ce: 0.007658
2022-01-08 10:54:37,810 iteration 3794 : loss : 0.028894, loss_ce: 0.012167
2022-01-08 10:54:39,171 iteration 3795 : loss : 0.015691, loss_ce: 0.004882
2022-01-08 10:54:40,445 iteration 3796 : loss : 0.019370, loss_ce: 0.006724
2022-01-08 10:54:41,791 iteration 3797 : loss : 0.021225, loss_ce: 0.007920
2022-01-08 10:54:43,099 iteration 3798 : loss : 0.019176, loss_ce: 0.007086
2022-01-08 10:54:44,445 iteration 3799 : loss : 0.021132, loss_ce: 0.008163
2022-01-08 10:54:45,793 iteration 3800 : loss : 0.018092, loss_ce: 0.006064
2022-01-08 10:54:47,129 iteration 3801 : loss : 0.043611, loss_ce: 0.008828
2022-01-08 10:54:48,474 iteration 3802 : loss : 0.022521, loss_ce: 0.010568
2022-01-08 10:54:49,838 iteration 3803 : loss : 0.017899, loss_ce: 0.005729
2022-01-08 10:54:51,200 iteration 3804 : loss : 0.026820, loss_ce: 0.009528
2022-01-08 10:54:52,531 iteration 3805 : loss : 0.025275, loss_ce: 0.010681
2022-01-08 10:54:53,870 iteration 3806 : loss : 0.026587, loss_ce: 0.013788
2022-01-08 10:54:55,266 iteration 3807 : loss : 0.029226, loss_ce: 0.009638
2022-01-08 10:54:56,630 iteration 3808 : loss : 0.024532, loss_ce: 0.009613
 56%|███████████████            | 224/400 [1:34:27<1:10:33, 24.06s/it]2022-01-08 10:54:57,972 iteration 3809 : loss : 0.017422, loss_ce: 0.005671
2022-01-08 10:54:59,315 iteration 3810 : loss : 0.021504, loss_ce: 0.008149
2022-01-08 10:55:00,672 iteration 3811 : loss : 0.019866, loss_ce: 0.005551
2022-01-08 10:55:02,037 iteration 3812 : loss : 0.024048, loss_ce: 0.011330
2022-01-08 10:55:03,382 iteration 3813 : loss : 0.017856, loss_ce: 0.008019
2022-01-08 10:55:04,758 iteration 3814 : loss : 0.024433, loss_ce: 0.008589
2022-01-08 10:55:06,121 iteration 3815 : loss : 0.022925, loss_ce: 0.007267
2022-01-08 10:55:07,507 iteration 3816 : loss : 0.023703, loss_ce: 0.009173
2022-01-08 10:55:08,828 iteration 3817 : loss : 0.034520, loss_ce: 0.014143
2022-01-08 10:55:10,225 iteration 3818 : loss : 0.024156, loss_ce: 0.012042
2022-01-08 10:55:11,633 iteration 3819 : loss : 0.022901, loss_ce: 0.010665
2022-01-08 10:55:13,019 iteration 3820 : loss : 0.027332, loss_ce: 0.011051
2022-01-08 10:55:14,414 iteration 3821 : loss : 0.022825, loss_ce: 0.009016
2022-01-08 10:55:15,745 iteration 3822 : loss : 0.031639, loss_ce: 0.010125
2022-01-08 10:55:17,189 iteration 3823 : loss : 0.022186, loss_ce: 0.006244
2022-01-08 10:55:18,565 iteration 3824 : loss : 0.023513, loss_ce: 0.008986
2022-01-08 10:55:18,565 Training Data Eval:
2022-01-08 10:55:25,447   Average segmentation loss on training set: 0.0144
2022-01-08 10:55:25,447 Validation Data Eval:
2022-01-08 10:55:27,828   Average segmentation loss on validation set: 0.0726
2022-01-08 10:55:29,277 iteration 3825 : loss : 0.021821, loss_ce: 0.007086
 56%|███████████████▏           | 225/400 [1:34:59<1:17:40, 26.63s/it]2022-01-08 10:55:30,701 iteration 3826 : loss : 0.020957, loss_ce: 0.009067
2022-01-08 10:55:32,018 iteration 3827 : loss : 0.023935, loss_ce: 0.008051
2022-01-08 10:55:33,385 iteration 3828 : loss : 0.022853, loss_ce: 0.008587
2022-01-08 10:55:34,749 iteration 3829 : loss : 0.021759, loss_ce: 0.006067
2022-01-08 10:55:36,076 iteration 3830 : loss : 0.014419, loss_ce: 0.005415
2022-01-08 10:55:37,399 iteration 3831 : loss : 0.016677, loss_ce: 0.006056
2022-01-08 10:55:38,740 iteration 3832 : loss : 0.025918, loss_ce: 0.007056
2022-01-08 10:55:40,120 iteration 3833 : loss : 0.015306, loss_ce: 0.007059
2022-01-08 10:55:41,512 iteration 3834 : loss : 0.030750, loss_ce: 0.016137
2022-01-08 10:55:42,884 iteration 3835 : loss : 0.028654, loss_ce: 0.011309
2022-01-08 10:55:44,258 iteration 3836 : loss : 0.027087, loss_ce: 0.013725
2022-01-08 10:55:45,649 iteration 3837 : loss : 0.024499, loss_ce: 0.009475
2022-01-08 10:55:47,090 iteration 3838 : loss : 0.022072, loss_ce: 0.008775
2022-01-08 10:55:48,441 iteration 3839 : loss : 0.027049, loss_ce: 0.014258
2022-01-08 10:55:49,813 iteration 3840 : loss : 0.028446, loss_ce: 0.008153
2022-01-08 10:55:51,149 iteration 3841 : loss : 0.021439, loss_ce: 0.008249
2022-01-08 10:55:52,536 iteration 3842 : loss : 0.029065, loss_ce: 0.014927
 56%|███████████████▎           | 226/400 [1:35:23<1:14:17, 25.62s/it]2022-01-08 10:55:53,898 iteration 3843 : loss : 0.020021, loss_ce: 0.007934
2022-01-08 10:55:55,307 iteration 3844 : loss : 0.050488, loss_ce: 0.017375
2022-01-08 10:55:56,705 iteration 3845 : loss : 0.022371, loss_ce: 0.012177
2022-01-08 10:55:58,034 iteration 3846 : loss : 0.017299, loss_ce: 0.006514
2022-01-08 10:55:59,387 iteration 3847 : loss : 0.025156, loss_ce: 0.009602
2022-01-08 10:56:00,804 iteration 3848 : loss : 0.033337, loss_ce: 0.019446
2022-01-08 10:56:02,263 iteration 3849 : loss : 0.028093, loss_ce: 0.012294
2022-01-08 10:56:03,697 iteration 3850 : loss : 0.059049, loss_ce: 0.015008
2022-01-08 10:56:05,070 iteration 3851 : loss : 0.031052, loss_ce: 0.016124
2022-01-08 10:56:06,450 iteration 3852 : loss : 0.021425, loss_ce: 0.006212
2022-01-08 10:56:07,775 iteration 3853 : loss : 0.019136, loss_ce: 0.005837
2022-01-08 10:56:09,170 iteration 3854 : loss : 0.018998, loss_ce: 0.005923
2022-01-08 10:56:10,659 iteration 3855 : loss : 0.030336, loss_ce: 0.011329
2022-01-08 10:56:11,905 iteration 3856 : loss : 0.022288, loss_ce: 0.010283
2022-01-08 10:56:13,259 iteration 3857 : loss : 0.021515, loss_ce: 0.009347
2022-01-08 10:56:14,530 iteration 3858 : loss : 0.020461, loss_ce: 0.008838
2022-01-08 10:56:15,889 iteration 3859 : loss : 0.022715, loss_ce: 0.009932
 57%|███████████████▎           | 227/400 [1:35:46<1:11:54, 24.94s/it]2022-01-08 10:56:17,295 iteration 3860 : loss : 0.018616, loss_ce: 0.007094
2022-01-08 10:56:18,726 iteration 3861 : loss : 0.041005, loss_ce: 0.021061
2022-01-08 10:56:20,118 iteration 3862 : loss : 0.030317, loss_ce: 0.011086
2022-01-08 10:56:21,501 iteration 3863 : loss : 0.017611, loss_ce: 0.006332
2022-01-08 10:56:22,912 iteration 3864 : loss : 0.023521, loss_ce: 0.008670
2022-01-08 10:56:24,269 iteration 3865 : loss : 0.024537, loss_ce: 0.008358
2022-01-08 10:56:25,745 iteration 3866 : loss : 0.028096, loss_ce: 0.009900
2022-01-08 10:56:27,088 iteration 3867 : loss : 0.025208, loss_ce: 0.008974
2022-01-08 10:56:28,501 iteration 3868 : loss : 0.062346, loss_ce: 0.022241
2022-01-08 10:56:29,902 iteration 3869 : loss : 0.021953, loss_ce: 0.009647
2022-01-08 10:56:31,315 iteration 3870 : loss : 0.022797, loss_ce: 0.008810
2022-01-08 10:56:32,709 iteration 3871 : loss : 0.029024, loss_ce: 0.007773
2022-01-08 10:56:34,038 iteration 3872 : loss : 0.023197, loss_ce: 0.008461
2022-01-08 10:56:35,370 iteration 3873 : loss : 0.020931, loss_ce: 0.010450
2022-01-08 10:56:36,765 iteration 3874 : loss : 0.027357, loss_ce: 0.011919
2022-01-08 10:56:38,151 iteration 3875 : loss : 0.032152, loss_ce: 0.012157
2022-01-08 10:56:39,544 iteration 3876 : loss : 0.031590, loss_ce: 0.010056
 57%|███████████████▍           | 228/400 [1:36:10<1:10:23, 24.56s/it]2022-01-08 10:56:40,930 iteration 3877 : loss : 0.022621, loss_ce: 0.009503
2022-01-08 10:56:42,332 iteration 3878 : loss : 0.024184, loss_ce: 0.008805
2022-01-08 10:56:43,672 iteration 3879 : loss : 0.021985, loss_ce: 0.008660
2022-01-08 10:56:45,077 iteration 3880 : loss : 0.029543, loss_ce: 0.016096
2022-01-08 10:56:46,418 iteration 3881 : loss : 0.021428, loss_ce: 0.009555
2022-01-08 10:56:47,862 iteration 3882 : loss : 0.043102, loss_ce: 0.013457
2022-01-08 10:56:49,277 iteration 3883 : loss : 0.048209, loss_ce: 0.019824
2022-01-08 10:56:50,669 iteration 3884 : loss : 0.024004, loss_ce: 0.011143
2022-01-08 10:56:52,069 iteration 3885 : loss : 0.040474, loss_ce: 0.010285
2022-01-08 10:56:53,484 iteration 3886 : loss : 0.025867, loss_ce: 0.009928
2022-01-08 10:56:54,803 iteration 3887 : loss : 0.017978, loss_ce: 0.006650
2022-01-08 10:56:56,208 iteration 3888 : loss : 0.032491, loss_ce: 0.017012
2022-01-08 10:56:57,588 iteration 3889 : loss : 0.019169, loss_ce: 0.007997
2022-01-08 10:56:59,070 iteration 3890 : loss : 0.038000, loss_ce: 0.010751
2022-01-08 10:57:00,408 iteration 3891 : loss : 0.031628, loss_ce: 0.011738
2022-01-08 10:57:01,785 iteration 3892 : loss : 0.023491, loss_ce: 0.010847
2022-01-08 10:57:03,193 iteration 3893 : loss : 0.044959, loss_ce: 0.015576
 57%|███████████████▍           | 229/400 [1:36:33<1:09:12, 24.28s/it]2022-01-08 10:57:04,580 iteration 3894 : loss : 0.019503, loss_ce: 0.005703
2022-01-08 10:57:05,911 iteration 3895 : loss : 0.019824, loss_ce: 0.007504
2022-01-08 10:57:07,279 iteration 3896 : loss : 0.030816, loss_ce: 0.013854
2022-01-08 10:57:08,681 iteration 3897 : loss : 0.027205, loss_ce: 0.011348
2022-01-08 10:57:10,066 iteration 3898 : loss : 0.032026, loss_ce: 0.010088
2022-01-08 10:57:11,373 iteration 3899 : loss : 0.028919, loss_ce: 0.009866
2022-01-08 10:57:12,717 iteration 3900 : loss : 0.036259, loss_ce: 0.013584
2022-01-08 10:57:14,043 iteration 3901 : loss : 0.024972, loss_ce: 0.010065
2022-01-08 10:57:15,373 iteration 3902 : loss : 0.024417, loss_ce: 0.008246
2022-01-08 10:57:16,669 iteration 3903 : loss : 0.026519, loss_ce: 0.008768
2022-01-08 10:57:18,023 iteration 3904 : loss : 0.024120, loss_ce: 0.008161
2022-01-08 10:57:19,374 iteration 3905 : loss : 0.025069, loss_ce: 0.007729
2022-01-08 10:57:20,663 iteration 3906 : loss : 0.016368, loss_ce: 0.006880
2022-01-08 10:57:21,944 iteration 3907 : loss : 0.017386, loss_ce: 0.006233
2022-01-08 10:57:23,249 iteration 3908 : loss : 0.018288, loss_ce: 0.007245
2022-01-08 10:57:24,609 iteration 3909 : loss : 0.026362, loss_ce: 0.015027
2022-01-08 10:57:24,609 Training Data Eval:
2022-01-08 10:57:31,502   Average segmentation loss on training set: 0.0158
2022-01-08 10:57:31,502 Validation Data Eval:
2022-01-08 10:57:33,873   Average segmentation loss on validation set: 0.0779
2022-01-08 10:57:35,313 iteration 3910 : loss : 0.026424, loss_ce: 0.008928
 57%|███████████████▌           | 230/400 [1:37:05<1:15:28, 26.64s/it]2022-01-08 10:57:36,676 iteration 3911 : loss : 0.022462, loss_ce: 0.009336
2022-01-08 10:57:38,075 iteration 3912 : loss : 0.021041, loss_ce: 0.007477
2022-01-08 10:57:39,430 iteration 3913 : loss : 0.034244, loss_ce: 0.012420
2022-01-08 10:57:40,790 iteration 3914 : loss : 0.016494, loss_ce: 0.007674
2022-01-08 10:57:42,146 iteration 3915 : loss : 0.021803, loss_ce: 0.008978
2022-01-08 10:57:43,464 iteration 3916 : loss : 0.018506, loss_ce: 0.006341
2022-01-08 10:57:44,859 iteration 3917 : loss : 0.026060, loss_ce: 0.010580
2022-01-08 10:57:46,265 iteration 3918 : loss : 0.039339, loss_ce: 0.011763
2022-01-08 10:57:47,751 iteration 3919 : loss : 0.035645, loss_ce: 0.010295
2022-01-08 10:57:49,070 iteration 3920 : loss : 0.019172, loss_ce: 0.007855
2022-01-08 10:57:50,429 iteration 3921 : loss : 0.031158, loss_ce: 0.014040
2022-01-08 10:57:51,837 iteration 3922 : loss : 0.026542, loss_ce: 0.011631
2022-01-08 10:57:53,184 iteration 3923 : loss : 0.018581, loss_ce: 0.009951
2022-01-08 10:57:54,500 iteration 3924 : loss : 0.022049, loss_ce: 0.006266
2022-01-08 10:57:55,937 iteration 3925 : loss : 0.029113, loss_ce: 0.011351
2022-01-08 10:57:57,341 iteration 3926 : loss : 0.035369, loss_ce: 0.012798
2022-01-08 10:57:58,697 iteration 3927 : loss : 0.023945, loss_ce: 0.009571
 58%|███████████████▌           | 231/400 [1:37:29<1:12:16, 25.66s/it]2022-01-08 10:58:00,146 iteration 3928 : loss : 0.034122, loss_ce: 0.015621
2022-01-08 10:58:01,511 iteration 3929 : loss : 0.019755, loss_ce: 0.009937
2022-01-08 10:58:02,945 iteration 3930 : loss : 0.028092, loss_ce: 0.011694
2022-01-08 10:58:04,346 iteration 3931 : loss : 0.029893, loss_ce: 0.013544
2022-01-08 10:58:05,746 iteration 3932 : loss : 0.030702, loss_ce: 0.009621
2022-01-08 10:58:07,186 iteration 3933 : loss : 0.025263, loss_ce: 0.010736
2022-01-08 10:58:08,582 iteration 3934 : loss : 0.015866, loss_ce: 0.006697
2022-01-08 10:58:09,877 iteration 3935 : loss : 0.017609, loss_ce: 0.007305
2022-01-08 10:58:11,372 iteration 3936 : loss : 0.021597, loss_ce: 0.009324
2022-01-08 10:58:12,690 iteration 3937 : loss : 0.020623, loss_ce: 0.008836
2022-01-08 10:58:14,110 iteration 3938 : loss : 0.056277, loss_ce: 0.011223
2022-01-08 10:58:15,399 iteration 3939 : loss : 0.023264, loss_ce: 0.006799
2022-01-08 10:58:16,738 iteration 3940 : loss : 0.021384, loss_ce: 0.007799
2022-01-08 10:58:18,098 iteration 3941 : loss : 0.021776, loss_ce: 0.009565
2022-01-08 10:58:19,518 iteration 3942 : loss : 0.050404, loss_ce: 0.023038
2022-01-08 10:58:20,848 iteration 3943 : loss : 0.033108, loss_ce: 0.010761
2022-01-08 10:58:22,274 iteration 3944 : loss : 0.030042, loss_ce: 0.011676
 58%|███████████████▋           | 232/400 [1:37:52<1:10:05, 25.03s/it]2022-01-08 10:58:23,693 iteration 3945 : loss : 0.027040, loss_ce: 0.012031
2022-01-08 10:58:24,972 iteration 3946 : loss : 0.016603, loss_ce: 0.008476
2022-01-08 10:58:26,330 iteration 3947 : loss : 0.024295, loss_ce: 0.008436
2022-01-08 10:58:27,817 iteration 3948 : loss : 0.026034, loss_ce: 0.007716
2022-01-08 10:58:29,213 iteration 3949 : loss : 0.034216, loss_ce: 0.011935
2022-01-08 10:58:30,646 iteration 3950 : loss : 0.025699, loss_ce: 0.008060
2022-01-08 10:58:31,955 iteration 3951 : loss : 0.023910, loss_ce: 0.011684
2022-01-08 10:58:33,373 iteration 3952 : loss : 0.021047, loss_ce: 0.007877
2022-01-08 10:58:34,799 iteration 3953 : loss : 0.050771, loss_ce: 0.020760
2022-01-08 10:58:36,152 iteration 3954 : loss : 0.044247, loss_ce: 0.018720
2022-01-08 10:58:37,502 iteration 3955 : loss : 0.017701, loss_ce: 0.006685
2022-01-08 10:58:38,984 iteration 3956 : loss : 0.031007, loss_ce: 0.011958
2022-01-08 10:58:40,306 iteration 3957 : loss : 0.019861, loss_ce: 0.008491
2022-01-08 10:58:41,641 iteration 3958 : loss : 0.028566, loss_ce: 0.013192
2022-01-08 10:58:43,095 iteration 3959 : loss : 0.023616, loss_ce: 0.007090
2022-01-08 10:58:44,430 iteration 3960 : loss : 0.020043, loss_ce: 0.009803
2022-01-08 10:58:45,784 iteration 3961 : loss : 0.028846, loss_ce: 0.010976
 58%|███████████████▋           | 233/400 [1:38:16<1:08:24, 24.58s/it]2022-01-08 10:58:47,224 iteration 3962 : loss : 0.023205, loss_ce: 0.008629
2022-01-08 10:58:48,592 iteration 3963 : loss : 0.027784, loss_ce: 0.012117
2022-01-08 10:58:50,042 iteration 3964 : loss : 0.032071, loss_ce: 0.014592
2022-01-08 10:58:51,358 iteration 3965 : loss : 0.022974, loss_ce: 0.007124
2022-01-08 10:58:52,761 iteration 3966 : loss : 0.026009, loss_ce: 0.010765
2022-01-08 10:58:54,128 iteration 3967 : loss : 0.026877, loss_ce: 0.010291
2022-01-08 10:58:55,405 iteration 3968 : loss : 0.023203, loss_ce: 0.007840
2022-01-08 10:58:56,787 iteration 3969 : loss : 0.025006, loss_ce: 0.007412
2022-01-08 10:58:58,149 iteration 3970 : loss : 0.021604, loss_ce: 0.008351
2022-01-08 10:58:59,427 iteration 3971 : loss : 0.017795, loss_ce: 0.007086
2022-01-08 10:59:00,830 iteration 3972 : loss : 0.017880, loss_ce: 0.005396
2022-01-08 10:59:02,158 iteration 3973 : loss : 0.027885, loss_ce: 0.015032
2022-01-08 10:59:03,445 iteration 3974 : loss : 0.018302, loss_ce: 0.005502
2022-01-08 10:59:04,826 iteration 3975 : loss : 0.020008, loss_ce: 0.007546
2022-01-08 10:59:06,260 iteration 3976 : loss : 0.033613, loss_ce: 0.014174
2022-01-08 10:59:07,681 iteration 3977 : loss : 0.055592, loss_ce: 0.024640
2022-01-08 10:59:09,122 iteration 3978 : loss : 0.032587, loss_ce: 0.012192
 58%|███████████████▊           | 234/400 [1:38:39<1:06:58, 24.21s/it]2022-01-08 10:59:10,514 iteration 3979 : loss : 0.020477, loss_ce: 0.007295
2022-01-08 10:59:11,912 iteration 3980 : loss : 0.028045, loss_ce: 0.008978
2022-01-08 10:59:13,263 iteration 3981 : loss : 0.022798, loss_ce: 0.008886
2022-01-08 10:59:14,541 iteration 3982 : loss : 0.014956, loss_ce: 0.005531
2022-01-08 10:59:15,844 iteration 3983 : loss : 0.015929, loss_ce: 0.007215
2022-01-08 10:59:17,177 iteration 3984 : loss : 0.014646, loss_ce: 0.006181
2022-01-08 10:59:18,562 iteration 3985 : loss : 0.020717, loss_ce: 0.010016
2022-01-08 10:59:20,014 iteration 3986 : loss : 0.022596, loss_ce: 0.009374
2022-01-08 10:59:21,409 iteration 3987 : loss : 0.027000, loss_ce: 0.010543
2022-01-08 10:59:22,764 iteration 3988 : loss : 0.023881, loss_ce: 0.011317
2022-01-08 10:59:24,109 iteration 3989 : loss : 0.019601, loss_ce: 0.004262
2022-01-08 10:59:25,455 iteration 3990 : loss : 0.030222, loss_ce: 0.008896
2022-01-08 10:59:26,877 iteration 3991 : loss : 0.028538, loss_ce: 0.006408
2022-01-08 10:59:28,258 iteration 3992 : loss : 0.028937, loss_ce: 0.008628
2022-01-08 10:59:29,667 iteration 3993 : loss : 0.028198, loss_ce: 0.009519
2022-01-08 10:59:31,006 iteration 3994 : loss : 0.018075, loss_ce: 0.008655
2022-01-08 10:59:31,006 Training Data Eval:
2022-01-08 10:59:37,909   Average segmentation loss on training set: 0.0131
2022-01-08 10:59:37,909 Validation Data Eval:
2022-01-08 10:59:40,287   Average segmentation loss on validation set: 0.0755
2022-01-08 10:59:41,613 iteration 3995 : loss : 0.017844, loss_ce: 0.007321
 59%|███████████████▊           | 235/400 [1:39:12<1:13:24, 26.69s/it]2022-01-08 10:59:43,082 iteration 3996 : loss : 0.018322, loss_ce: 0.005386
2022-01-08 10:59:44,392 iteration 3997 : loss : 0.022320, loss_ce: 0.006763
2022-01-08 10:59:45,774 iteration 3998 : loss : 0.027023, loss_ce: 0.012922
2022-01-08 10:59:47,127 iteration 3999 : loss : 0.021855, loss_ce: 0.005311
2022-01-08 10:59:48,551 iteration 4000 : loss : 0.022264, loss_ce: 0.012631
2022-01-08 10:59:49,852 iteration 4001 : loss : 0.017257, loss_ce: 0.006480
2022-01-08 10:59:51,222 iteration 4002 : loss : 0.016047, loss_ce: 0.006524
2022-01-08 10:59:52,599 iteration 4003 : loss : 0.022090, loss_ce: 0.010324
2022-01-08 10:59:53,995 iteration 4004 : loss : 0.025235, loss_ce: 0.007102
2022-01-08 10:59:55,365 iteration 4005 : loss : 0.021222, loss_ce: 0.010002
2022-01-08 10:59:56,681 iteration 4006 : loss : 0.017516, loss_ce: 0.007411
2022-01-08 10:59:58,098 iteration 4007 : loss : 0.028189, loss_ce: 0.009113
2022-01-08 10:59:59,431 iteration 4008 : loss : 0.017351, loss_ce: 0.006870
2022-01-08 11:00:00,793 iteration 4009 : loss : 0.024349, loss_ce: 0.010020
2022-01-08 11:00:02,218 iteration 4010 : loss : 0.027788, loss_ce: 0.012615
2022-01-08 11:00:03,535 iteration 4011 : loss : 0.017691, loss_ce: 0.005074
2022-01-08 11:00:04,973 iteration 4012 : loss : 0.024336, loss_ce: 0.010430
 59%|███████████████▉           | 236/400 [1:39:35<1:10:13, 25.69s/it]2022-01-08 11:00:06,445 iteration 4013 : loss : 0.052481, loss_ce: 0.018424
2022-01-08 11:00:07,752 iteration 4014 : loss : 0.017126, loss_ce: 0.005268
2022-01-08 11:00:09,164 iteration 4015 : loss : 0.031985, loss_ce: 0.013952
2022-01-08 11:00:10,559 iteration 4016 : loss : 0.016815, loss_ce: 0.005662
2022-01-08 11:00:11,964 iteration 4017 : loss : 0.024361, loss_ce: 0.009864
2022-01-08 11:00:13,285 iteration 4018 : loss : 0.020477, loss_ce: 0.007582
2022-01-08 11:00:14,649 iteration 4019 : loss : 0.025039, loss_ce: 0.012307
2022-01-08 11:00:15,977 iteration 4020 : loss : 0.017169, loss_ce: 0.008882
2022-01-08 11:00:17,318 iteration 4021 : loss : 0.018406, loss_ce: 0.007776
2022-01-08 11:00:18,712 iteration 4022 : loss : 0.030862, loss_ce: 0.014908
2022-01-08 11:00:20,085 iteration 4023 : loss : 0.016233, loss_ce: 0.005780
2022-01-08 11:00:21,385 iteration 4024 : loss : 0.017087, loss_ce: 0.005273
2022-01-08 11:00:22,719 iteration 4025 : loss : 0.017666, loss_ce: 0.006569
2022-01-08 11:00:24,121 iteration 4026 : loss : 0.022577, loss_ce: 0.009401
2022-01-08 11:00:25,505 iteration 4027 : loss : 0.023867, loss_ce: 0.008800
2022-01-08 11:00:26,929 iteration 4028 : loss : 0.028847, loss_ce: 0.009082
2022-01-08 11:00:28,254 iteration 4029 : loss : 0.018121, loss_ce: 0.007091
 59%|███████████████▉           | 237/400 [1:39:58<1:07:50, 24.97s/it]2022-01-08 11:00:29,722 iteration 4030 : loss : 0.026146, loss_ce: 0.012575
2022-01-08 11:00:31,106 iteration 4031 : loss : 0.032575, loss_ce: 0.009187
2022-01-08 11:00:32,455 iteration 4032 : loss : 0.024428, loss_ce: 0.010447
2022-01-08 11:00:33,818 iteration 4033 : loss : 0.034514, loss_ce: 0.010843
2022-01-08 11:00:35,130 iteration 4034 : loss : 0.025214, loss_ce: 0.008631
2022-01-08 11:00:36,486 iteration 4035 : loss : 0.014940, loss_ce: 0.004866
2022-01-08 11:00:37,889 iteration 4036 : loss : 0.029503, loss_ce: 0.010161
2022-01-08 11:00:39,203 iteration 4037 : loss : 0.017139, loss_ce: 0.008481
2022-01-08 11:00:40,629 iteration 4038 : loss : 0.025376, loss_ce: 0.008339
2022-01-08 11:00:42,073 iteration 4039 : loss : 0.023476, loss_ce: 0.008745
2022-01-08 11:00:43,499 iteration 4040 : loss : 0.026578, loss_ce: 0.012415
2022-01-08 11:00:44,915 iteration 4041 : loss : 0.026476, loss_ce: 0.007989
2022-01-08 11:00:46,247 iteration 4042 : loss : 0.018923, loss_ce: 0.006823
2022-01-08 11:00:47,629 iteration 4043 : loss : 0.175875, loss_ce: 0.005594
2022-01-08 11:00:48,970 iteration 4044 : loss : 0.019307, loss_ce: 0.008205
2022-01-08 11:00:50,339 iteration 4045 : loss : 0.018720, loss_ce: 0.009435
2022-01-08 11:00:51,762 iteration 4046 : loss : 0.020528, loss_ce: 0.009208
 60%|████████████████           | 238/400 [1:40:22<1:06:13, 24.53s/it]2022-01-08 11:00:53,254 iteration 4047 : loss : 0.024939, loss_ce: 0.012430
2022-01-08 11:00:54,661 iteration 4048 : loss : 0.028681, loss_ce: 0.010874
2022-01-08 11:00:55,988 iteration 4049 : loss : 0.017141, loss_ce: 0.006209
2022-01-08 11:00:57,379 iteration 4050 : loss : 0.018607, loss_ce: 0.006791
2022-01-08 11:00:58,845 iteration 4051 : loss : 0.049735, loss_ce: 0.007543
2022-01-08 11:01:00,148 iteration 4052 : loss : 0.025393, loss_ce: 0.009007
2022-01-08 11:01:01,439 iteration 4053 : loss : 0.018765, loss_ce: 0.006237
2022-01-08 11:01:02,767 iteration 4054 : loss : 0.034326, loss_ce: 0.009503
2022-01-08 11:01:04,121 iteration 4055 : loss : 0.018562, loss_ce: 0.008589
2022-01-08 11:01:05,508 iteration 4056 : loss : 0.028618, loss_ce: 0.013127
2022-01-08 11:01:06,834 iteration 4057 : loss : 0.014992, loss_ce: 0.003753
2022-01-08 11:01:08,191 iteration 4058 : loss : 0.018196, loss_ce: 0.006764
2022-01-08 11:01:09,655 iteration 4059 : loss : 0.051768, loss_ce: 0.026851
2022-01-08 11:01:11,029 iteration 4060 : loss : 0.030081, loss_ce: 0.013923
2022-01-08 11:01:12,392 iteration 4061 : loss : 0.020039, loss_ce: 0.008291
2022-01-08 11:01:13,795 iteration 4062 : loss : 0.027750, loss_ce: 0.012124
2022-01-08 11:01:15,191 iteration 4063 : loss : 0.038242, loss_ce: 0.012393
 60%|████████████████▏          | 239/400 [1:40:45<1:04:55, 24.20s/it]2022-01-08 11:01:16,572 iteration 4064 : loss : 0.026223, loss_ce: 0.005875
2022-01-08 11:01:17,909 iteration 4065 : loss : 0.015859, loss_ce: 0.005575
2022-01-08 11:01:19,347 iteration 4066 : loss : 0.047100, loss_ce: 0.014749
2022-01-08 11:01:20,681 iteration 4067 : loss : 0.017433, loss_ce: 0.007881
2022-01-08 11:01:22,043 iteration 4068 : loss : 0.020505, loss_ce: 0.008070
2022-01-08 11:01:23,423 iteration 4069 : loss : 0.020986, loss_ce: 0.008762
2022-01-08 11:01:24,802 iteration 4070 : loss : 0.024719, loss_ce: 0.009451
2022-01-08 11:01:26,231 iteration 4071 : loss : 0.023613, loss_ce: 0.008660
2022-01-08 11:01:27,596 iteration 4072 : loss : 0.016397, loss_ce: 0.005187
2022-01-08 11:01:28,934 iteration 4073 : loss : 0.022950, loss_ce: 0.007120
2022-01-08 11:01:30,309 iteration 4074 : loss : 0.023658, loss_ce: 0.010604
2022-01-08 11:01:31,716 iteration 4075 : loss : 0.021349, loss_ce: 0.007397
2022-01-08 11:01:33,028 iteration 4076 : loss : 0.023130, loss_ce: 0.009017
2022-01-08 11:01:34,384 iteration 4077 : loss : 0.018677, loss_ce: 0.006414
2022-01-08 11:01:35,804 iteration 4078 : loss : 0.024912, loss_ce: 0.010394
2022-01-08 11:01:37,212 iteration 4079 : loss : 0.021515, loss_ce: 0.009133
2022-01-08 11:01:37,212 Training Data Eval:
2022-01-08 11:01:44,088   Average segmentation loss on training set: 0.0135
2022-01-08 11:01:44,089 Validation Data Eval:
2022-01-08 11:01:46,463   Average segmentation loss on validation set: 0.0874
2022-01-08 11:01:47,888 iteration 4080 : loss : 0.021388, loss_ce: 0.010803
 60%|████████████████▏          | 240/400 [1:41:18<1:11:20, 26.75s/it]2022-01-08 11:01:49,323 iteration 4081 : loss : 0.033558, loss_ce: 0.014160
2022-01-08 11:01:50,700 iteration 4082 : loss : 0.020033, loss_ce: 0.008107
2022-01-08 11:01:52,018 iteration 4083 : loss : 0.017661, loss_ce: 0.006834
2022-01-08 11:01:53,387 iteration 4084 : loss : 0.022452, loss_ce: 0.007689
2022-01-08 11:01:54,725 iteration 4085 : loss : 0.036161, loss_ce: 0.010311
2022-01-08 11:01:56,216 iteration 4086 : loss : 0.027019, loss_ce: 0.009349
2022-01-08 11:01:57,606 iteration 4087 : loss : 0.019410, loss_ce: 0.006571
2022-01-08 11:01:58,910 iteration 4088 : loss : 0.025326, loss_ce: 0.012386
2022-01-08 11:02:00,278 iteration 4089 : loss : 0.032745, loss_ce: 0.014447
2022-01-08 11:02:01,558 iteration 4090 : loss : 0.018103, loss_ce: 0.005551
2022-01-08 11:02:02,923 iteration 4091 : loss : 0.032367, loss_ce: 0.014398
2022-01-08 11:02:04,306 iteration 4092 : loss : 0.018127, loss_ce: 0.006875
2022-01-08 11:02:05,674 iteration 4093 : loss : 0.026219, loss_ce: 0.010766
2022-01-08 11:02:07,021 iteration 4094 : loss : 0.025036, loss_ce: 0.010740
2022-01-08 11:02:08,415 iteration 4095 : loss : 0.024217, loss_ce: 0.011128
2022-01-08 11:02:09,744 iteration 4096 : loss : 0.016795, loss_ce: 0.008809
2022-01-08 11:02:11,083 iteration 4097 : loss : 0.023575, loss_ce: 0.009384
 60%|████████████████▎          | 241/400 [1:41:41<1:08:03, 25.68s/it]2022-01-08 11:02:12,470 iteration 4098 : loss : 0.023628, loss_ce: 0.010149
2022-01-08 11:02:13,805 iteration 4099 : loss : 0.042395, loss_ce: 0.013482
2022-01-08 11:02:15,174 iteration 4100 : loss : 0.017981, loss_ce: 0.005959
2022-01-08 11:02:16,584 iteration 4101 : loss : 0.021914, loss_ce: 0.007502
2022-01-08 11:02:17,984 iteration 4102 : loss : 0.027282, loss_ce: 0.011790
2022-01-08 11:02:19,378 iteration 4103 : loss : 0.031917, loss_ce: 0.017685
2022-01-08 11:02:20,743 iteration 4104 : loss : 0.028531, loss_ce: 0.013753
2022-01-08 11:02:22,124 iteration 4105 : loss : 0.025501, loss_ce: 0.011014
2022-01-08 11:02:23,467 iteration 4106 : loss : 0.026714, loss_ce: 0.011844
2022-01-08 11:02:24,820 iteration 4107 : loss : 0.020374, loss_ce: 0.008632
2022-01-08 11:02:26,189 iteration 4108 : loss : 0.024081, loss_ce: 0.010109
2022-01-08 11:02:27,504 iteration 4109 : loss : 0.017875, loss_ce: 0.006420
2022-01-08 11:02:28,914 iteration 4110 : loss : 0.030680, loss_ce: 0.010658
2022-01-08 11:02:30,416 iteration 4111 : loss : 0.033520, loss_ce: 0.016602
2022-01-08 11:02:31,742 iteration 4112 : loss : 0.018959, loss_ce: 0.006891
2022-01-08 11:02:33,144 iteration 4113 : loss : 0.027325, loss_ce: 0.009016
2022-01-08 11:02:34,497 iteration 4114 : loss : 0.020835, loss_ce: 0.008792
 60%|████████████████▎          | 242/400 [1:42:05<1:05:50, 25.00s/it]2022-01-08 11:02:35,866 iteration 4115 : loss : 0.017942, loss_ce: 0.008996
2022-01-08 11:02:37,285 iteration 4116 : loss : 0.018798, loss_ce: 0.006025
2022-01-08 11:02:38,583 iteration 4117 : loss : 0.016666, loss_ce: 0.006076
2022-01-08 11:02:40,004 iteration 4118 : loss : 0.023378, loss_ce: 0.009871
2022-01-08 11:02:41,479 iteration 4119 : loss : 0.030803, loss_ce: 0.010366
2022-01-08 11:02:42,865 iteration 4120 : loss : 0.020049, loss_ce: 0.006141
2022-01-08 11:02:44,200 iteration 4121 : loss : 0.017564, loss_ce: 0.005698
2022-01-08 11:02:45,563 iteration 4122 : loss : 0.029254, loss_ce: 0.009269
2022-01-08 11:02:46,870 iteration 4123 : loss : 0.012220, loss_ce: 0.005136
2022-01-08 11:02:48,260 iteration 4124 : loss : 0.026530, loss_ce: 0.013843
2022-01-08 11:02:49,663 iteration 4125 : loss : 0.032334, loss_ce: 0.017768
2022-01-08 11:02:51,062 iteration 4126 : loss : 0.021094, loss_ce: 0.008211
2022-01-08 11:02:52,397 iteration 4127 : loss : 0.016735, loss_ce: 0.008443
2022-01-08 11:02:53,864 iteration 4128 : loss : 0.044947, loss_ce: 0.011854
2022-01-08 11:02:55,162 iteration 4129 : loss : 0.023149, loss_ce: 0.005441
2022-01-08 11:02:56,462 iteration 4130 : loss : 0.017294, loss_ce: 0.005642
2022-01-08 11:02:57,815 iteration 4131 : loss : 0.015863, loss_ce: 0.007120
 61%|████████████████▍          | 243/400 [1:42:28<1:04:05, 24.50s/it]2022-01-08 11:02:59,272 iteration 4132 : loss : 0.034877, loss_ce: 0.010700
2022-01-08 11:03:00,658 iteration 4133 : loss : 0.019379, loss_ce: 0.005827
2022-01-08 11:03:02,061 iteration 4134 : loss : 0.028393, loss_ce: 0.012282
2022-01-08 11:03:03,356 iteration 4135 : loss : 0.022497, loss_ce: 0.010001
2022-01-08 11:03:04,721 iteration 4136 : loss : 0.028467, loss_ce: 0.014084
2022-01-08 11:03:06,017 iteration 4137 : loss : 0.015506, loss_ce: 0.005376
2022-01-08 11:03:07,414 iteration 4138 : loss : 0.019450, loss_ce: 0.009426
2022-01-08 11:03:08,812 iteration 4139 : loss : 0.017628, loss_ce: 0.007761
2022-01-08 11:03:10,190 iteration 4140 : loss : 0.016222, loss_ce: 0.007375
2022-01-08 11:03:11,564 iteration 4141 : loss : 0.021160, loss_ce: 0.007429
2022-01-08 11:03:12,998 iteration 4142 : loss : 0.035664, loss_ce: 0.014125
2022-01-08 11:03:14,274 iteration 4143 : loss : 0.024423, loss_ce: 0.008611
2022-01-08 11:03:15,590 iteration 4144 : loss : 0.020406, loss_ce: 0.004404
2022-01-08 11:03:17,012 iteration 4145 : loss : 0.018909, loss_ce: 0.007342
2022-01-08 11:03:18,346 iteration 4146 : loss : 0.015540, loss_ce: 0.006729
2022-01-08 11:03:19,728 iteration 4147 : loss : 0.018772, loss_ce: 0.006740
2022-01-08 11:03:21,061 iteration 4148 : loss : 0.024514, loss_ce: 0.011322
 61%|████████████████▍          | 244/400 [1:42:51<1:02:43, 24.12s/it]2022-01-08 11:03:22,381 iteration 4149 : loss : 0.015863, loss_ce: 0.005745
2022-01-08 11:03:23,765 iteration 4150 : loss : 0.018514, loss_ce: 0.005052
2022-01-08 11:03:25,174 iteration 4151 : loss : 0.025305, loss_ce: 0.009386
2022-01-08 11:03:26,539 iteration 4152 : loss : 0.022824, loss_ce: 0.008580
2022-01-08 11:03:27,824 iteration 4153 : loss : 0.016030, loss_ce: 0.006530
2022-01-08 11:03:29,203 iteration 4154 : loss : 0.016216, loss_ce: 0.005227
2022-01-08 11:03:30,555 iteration 4155 : loss : 0.019452, loss_ce: 0.006455
2022-01-08 11:03:31,868 iteration 4156 : loss : 0.013962, loss_ce: 0.004930
2022-01-08 11:03:33,283 iteration 4157 : loss : 0.023668, loss_ce: 0.011475
2022-01-08 11:03:34,609 iteration 4158 : loss : 0.016663, loss_ce: 0.006361
2022-01-08 11:03:35,939 iteration 4159 : loss : 0.020776, loss_ce: 0.010025
2022-01-08 11:03:37,401 iteration 4160 : loss : 0.035072, loss_ce: 0.012673
2022-01-08 11:03:38,830 iteration 4161 : loss : 0.022769, loss_ce: 0.007411
2022-01-08 11:03:40,262 iteration 4162 : loss : 0.028875, loss_ce: 0.010590
2022-01-08 11:03:41,678 iteration 4163 : loss : 0.025147, loss_ce: 0.009975
2022-01-08 11:03:43,054 iteration 4164 : loss : 0.019710, loss_ce: 0.007535
2022-01-08 11:03:43,054 Training Data Eval:
2022-01-08 11:03:49,926   Average segmentation loss on training set: 0.0143
2022-01-08 11:03:49,927 Validation Data Eval:
2022-01-08 11:03:52,304   Average segmentation loss on validation set: 0.0712
2022-01-08 11:03:53,718 iteration 4165 : loss : 0.028928, loss_ce: 0.009041
 61%|████████████████▌          | 245/400 [1:43:24<1:08:55, 26.68s/it]2022-01-08 11:03:55,136 iteration 4166 : loss : 0.017139, loss_ce: 0.006520
2022-01-08 11:03:56,432 iteration 4167 : loss : 0.018161, loss_ce: 0.005545
2022-01-08 11:03:57,760 iteration 4168 : loss : 0.014369, loss_ce: 0.005830
2022-01-08 11:03:59,090 iteration 4169 : loss : 0.015118, loss_ce: 0.007022
2022-01-08 11:04:00,571 iteration 4170 : loss : 0.036867, loss_ce: 0.016910
2022-01-08 11:04:01,878 iteration 4171 : loss : 0.016576, loss_ce: 0.005201
2022-01-08 11:04:03,234 iteration 4172 : loss : 0.016124, loss_ce: 0.007379
2022-01-08 11:04:04,618 iteration 4173 : loss : 0.018675, loss_ce: 0.005202
2022-01-08 11:04:05,966 iteration 4174 : loss : 0.021236, loss_ce: 0.005809
2022-01-08 11:04:07,284 iteration 4175 : loss : 0.015721, loss_ce: 0.006003
2022-01-08 11:04:08,639 iteration 4176 : loss : 0.019333, loss_ce: 0.008171
2022-01-08 11:04:10,052 iteration 4177 : loss : 0.016096, loss_ce: 0.004870
2022-01-08 11:04:11,378 iteration 4178 : loss : 0.019268, loss_ce: 0.007443
2022-01-08 11:04:12,724 iteration 4179 : loss : 0.018812, loss_ce: 0.008411
2022-01-08 11:04:14,053 iteration 4180 : loss : 0.014182, loss_ce: 0.004484
2022-01-08 11:04:15,397 iteration 4181 : loss : 0.029802, loss_ce: 0.014120
2022-01-08 11:04:16,756 iteration 4182 : loss : 0.023194, loss_ce: 0.007192
 62%|████████████████▌          | 246/400 [1:43:47<1:05:40, 25.59s/it]2022-01-08 11:04:18,170 iteration 4183 : loss : 0.017274, loss_ce: 0.006112
2022-01-08 11:04:19,562 iteration 4184 : loss : 0.026024, loss_ce: 0.010011
2022-01-08 11:04:20,933 iteration 4185 : loss : 0.019479, loss_ce: 0.008292
2022-01-08 11:04:22,276 iteration 4186 : loss : 0.017898, loss_ce: 0.005470
2022-01-08 11:04:23,679 iteration 4187 : loss : 0.028083, loss_ce: 0.009886
2022-01-08 11:04:24,997 iteration 4188 : loss : 0.019259, loss_ce: 0.006273
2022-01-08 11:04:26,468 iteration 4189 : loss : 0.025064, loss_ce: 0.011540
2022-01-08 11:04:27,879 iteration 4190 : loss : 0.021870, loss_ce: 0.009884
2022-01-08 11:04:29,298 iteration 4191 : loss : 0.031256, loss_ce: 0.013986
2022-01-08 11:04:30,565 iteration 4192 : loss : 0.012993, loss_ce: 0.004100
2022-01-08 11:04:31,947 iteration 4193 : loss : 0.020624, loss_ce: 0.009978
2022-01-08 11:04:33,355 iteration 4194 : loss : 0.023762, loss_ce: 0.008342
2022-01-08 11:04:34,709 iteration 4195 : loss : 0.023138, loss_ce: 0.006539
2022-01-08 11:04:36,116 iteration 4196 : loss : 0.020728, loss_ce: 0.008808
2022-01-08 11:04:37,466 iteration 4197 : loss : 0.030702, loss_ce: 0.010333
2022-01-08 11:04:38,815 iteration 4198 : loss : 0.021146, loss_ce: 0.008771
2022-01-08 11:04:40,117 iteration 4199 : loss : 0.015794, loss_ce: 0.006204
 62%|████████████████▋          | 247/400 [1:44:10<1:03:33, 24.92s/it]2022-01-08 11:04:41,520 iteration 4200 : loss : 0.018695, loss_ce: 0.006436
2022-01-08 11:04:42,902 iteration 4201 : loss : 0.024906, loss_ce: 0.011850
2022-01-08 11:04:44,287 iteration 4202 : loss : 0.020369, loss_ce: 0.008309
2022-01-08 11:04:45,734 iteration 4203 : loss : 0.018261, loss_ce: 0.008115
2022-01-08 11:04:47,067 iteration 4204 : loss : 0.016310, loss_ce: 0.006388
2022-01-08 11:04:48,419 iteration 4205 : loss : 0.030045, loss_ce: 0.012240
2022-01-08 11:04:49,893 iteration 4206 : loss : 0.025412, loss_ce: 0.008373
2022-01-08 11:04:51,289 iteration 4207 : loss : 0.024207, loss_ce: 0.009981
2022-01-08 11:04:52,569 iteration 4208 : loss : 0.013871, loss_ce: 0.005304
2022-01-08 11:04:53,914 iteration 4209 : loss : 0.022069, loss_ce: 0.009440
2022-01-08 11:04:55,253 iteration 4210 : loss : 0.020173, loss_ce: 0.008253
2022-01-08 11:04:56,639 iteration 4211 : loss : 0.018556, loss_ce: 0.007633
2022-01-08 11:04:58,049 iteration 4212 : loss : 0.019040, loss_ce: 0.007389
2022-01-08 11:04:59,384 iteration 4213 : loss : 0.018356, loss_ce: 0.005501
2022-01-08 11:05:00,789 iteration 4214 : loss : 0.026770, loss_ce: 0.010586
2022-01-08 11:05:02,143 iteration 4215 : loss : 0.020289, loss_ce: 0.008489
2022-01-08 11:05:03,496 iteration 4216 : loss : 0.016939, loss_ce: 0.005591
 62%|████████████████▋          | 248/400 [1:44:34<1:01:57, 24.46s/it]2022-01-08 11:05:04,940 iteration 4217 : loss : 0.019226, loss_ce: 0.006441
2022-01-08 11:05:06,271 iteration 4218 : loss : 0.026741, loss_ce: 0.011041
2022-01-08 11:05:07,600 iteration 4219 : loss : 0.014321, loss_ce: 0.005543
2022-01-08 11:05:08,882 iteration 4220 : loss : 0.022771, loss_ce: 0.005653
2022-01-08 11:05:10,347 iteration 4221 : loss : 0.022794, loss_ce: 0.010196
2022-01-08 11:05:11,796 iteration 4222 : loss : 0.025247, loss_ce: 0.007666
2022-01-08 11:05:13,137 iteration 4223 : loss : 0.016291, loss_ce: 0.005599
2022-01-08 11:05:14,563 iteration 4224 : loss : 0.028737, loss_ce: 0.005465
2022-01-08 11:05:15,932 iteration 4225 : loss : 0.030780, loss_ce: 0.010336
2022-01-08 11:05:17,339 iteration 4226 : loss : 0.016653, loss_ce: 0.007237
2022-01-08 11:05:18,711 iteration 4227 : loss : 0.014198, loss_ce: 0.005930
2022-01-08 11:05:20,024 iteration 4228 : loss : 0.018301, loss_ce: 0.008162
2022-01-08 11:05:21,392 iteration 4229 : loss : 0.022422, loss_ce: 0.008354
2022-01-08 11:05:22,758 iteration 4230 : loss : 0.022272, loss_ce: 0.010779
2022-01-08 11:05:24,159 iteration 4231 : loss : 0.029421, loss_ce: 0.010332
2022-01-08 11:05:25,557 iteration 4232 : loss : 0.024114, loss_ce: 0.008515
2022-01-08 11:05:26,919 iteration 4233 : loss : 0.029801, loss_ce: 0.014670
 62%|████████████████▊          | 249/400 [1:44:57<1:00:46, 24.15s/it]2022-01-08 11:05:28,244 iteration 4234 : loss : 0.021014, loss_ce: 0.004669
2022-01-08 11:05:29,547 iteration 4235 : loss : 0.011067, loss_ce: 0.004049
2022-01-08 11:05:30,935 iteration 4236 : loss : 0.027350, loss_ce: 0.011276
2022-01-08 11:05:32,248 iteration 4237 : loss : 0.019282, loss_ce: 0.007524
2022-01-08 11:05:33,604 iteration 4238 : loss : 0.025148, loss_ce: 0.012116
2022-01-08 11:05:35,094 iteration 4239 : loss : 0.027309, loss_ce: 0.011094
2022-01-08 11:05:36,468 iteration 4240 : loss : 0.018344, loss_ce: 0.005217
2022-01-08 11:05:37,749 iteration 4241 : loss : 0.016894, loss_ce: 0.007576
2022-01-08 11:05:39,121 iteration 4242 : loss : 0.020941, loss_ce: 0.005876
2022-01-08 11:05:40,453 iteration 4243 : loss : 0.020395, loss_ce: 0.008351
2022-01-08 11:05:41,739 iteration 4244 : loss : 0.015014, loss_ce: 0.006801
2022-01-08 11:05:43,153 iteration 4245 : loss : 0.025451, loss_ce: 0.010679
2022-01-08 11:05:44,590 iteration 4246 : loss : 0.024552, loss_ce: 0.005843
2022-01-08 11:05:45,983 iteration 4247 : loss : 0.021428, loss_ce: 0.009463
2022-01-08 11:05:47,296 iteration 4248 : loss : 0.020522, loss_ce: 0.008261
2022-01-08 11:05:48,731 iteration 4249 : loss : 0.034729, loss_ce: 0.012899
2022-01-08 11:05:48,731 Training Data Eval:
2022-01-08 11:05:55,617   Average segmentation loss on training set: 0.0117
2022-01-08 11:05:55,617 Validation Data Eval:
2022-01-08 11:05:57,990   Average segmentation loss on validation set: 0.0746
2022-01-08 11:05:59,356 iteration 4250 : loss : 0.023472, loss_ce: 0.008487
 62%|████████████████▉          | 250/400 [1:45:30<1:06:34, 26.63s/it]2022-01-08 11:06:00,830 iteration 4251 : loss : 0.024280, loss_ce: 0.006937
2022-01-08 11:06:02,241 iteration 4252 : loss : 0.031590, loss_ce: 0.014564
2022-01-08 11:06:03,599 iteration 4253 : loss : 0.019724, loss_ce: 0.006463
2022-01-08 11:06:04,996 iteration 4254 : loss : 0.017053, loss_ce: 0.006564
2022-01-08 11:06:06,405 iteration 4255 : loss : 0.016343, loss_ce: 0.005888
2022-01-08 11:06:07,685 iteration 4256 : loss : 0.015081, loss_ce: 0.007015
2022-01-08 11:06:09,062 iteration 4257 : loss : 0.029090, loss_ce: 0.011279
2022-01-08 11:06:10,351 iteration 4258 : loss : 0.027785, loss_ce: 0.008147
2022-01-08 11:06:11,739 iteration 4259 : loss : 0.020531, loss_ce: 0.007956
2022-01-08 11:06:13,129 iteration 4260 : loss : 0.029874, loss_ce: 0.008490
2022-01-08 11:06:14,508 iteration 4261 : loss : 0.016946, loss_ce: 0.006287
2022-01-08 11:06:15,835 iteration 4262 : loss : 0.015397, loss_ce: 0.006754
2022-01-08 11:06:17,226 iteration 4263 : loss : 0.019743, loss_ce: 0.008079
2022-01-08 11:06:18,640 iteration 4264 : loss : 0.026670, loss_ce: 0.011050
2022-01-08 11:06:19,988 iteration 4265 : loss : 0.047352, loss_ce: 0.009825
2022-01-08 11:06:21,372 iteration 4266 : loss : 0.019553, loss_ce: 0.006357
2022-01-08 11:06:22,780 iteration 4267 : loss : 0.019737, loss_ce: 0.007719
 63%|████████████████▉          | 251/400 [1:45:53<1:03:45, 25.67s/it]2022-01-08 11:06:24,197 iteration 4268 : loss : 0.015946, loss_ce: 0.003779
2022-01-08 11:06:25,554 iteration 4269 : loss : 0.018539, loss_ce: 0.006213
2022-01-08 11:06:26,882 iteration 4270 : loss : 0.030169, loss_ce: 0.017300
2022-01-08 11:06:28,175 iteration 4271 : loss : 0.016812, loss_ce: 0.006061
2022-01-08 11:06:29,538 iteration 4272 : loss : 0.019605, loss_ce: 0.009689
2022-01-08 11:06:30,837 iteration 4273 : loss : 0.015856, loss_ce: 0.006282
2022-01-08 11:06:32,282 iteration 4274 : loss : 0.019472, loss_ce: 0.008798
2022-01-08 11:06:33,620 iteration 4275 : loss : 0.016216, loss_ce: 0.005747
2022-01-08 11:06:35,027 iteration 4276 : loss : 0.026534, loss_ce: 0.007347
2022-01-08 11:06:36,441 iteration 4277 : loss : 0.022171, loss_ce: 0.006497
2022-01-08 11:06:37,848 iteration 4278 : loss : 0.027378, loss_ce: 0.008386
2022-01-08 11:06:39,197 iteration 4279 : loss : 0.019881, loss_ce: 0.010549
2022-01-08 11:06:40,569 iteration 4280 : loss : 0.019069, loss_ce: 0.006150
2022-01-08 11:06:41,937 iteration 4281 : loss : 0.020346, loss_ce: 0.005837
2022-01-08 11:06:43,242 iteration 4282 : loss : 0.018070, loss_ce: 0.007105
2022-01-08 11:06:44,593 iteration 4283 : loss : 0.021427, loss_ce: 0.006398
2022-01-08 11:06:45,921 iteration 4284 : loss : 0.016190, loss_ce: 0.006470
 63%|█████████████████          | 252/400 [1:46:16<1:01:26, 24.91s/it]2022-01-08 11:06:47,258 iteration 4285 : loss : 0.019969, loss_ce: 0.006629
2022-01-08 11:06:48,658 iteration 4286 : loss : 0.023712, loss_ce: 0.009806
2022-01-08 11:06:49,979 iteration 4287 : loss : 0.019241, loss_ce: 0.008464
2022-01-08 11:06:51,305 iteration 4288 : loss : 0.017506, loss_ce: 0.008140
2022-01-08 11:06:52,679 iteration 4289 : loss : 0.015997, loss_ce: 0.007001
2022-01-08 11:06:54,025 iteration 4290 : loss : 0.032976, loss_ce: 0.006921
2022-01-08 11:06:55,403 iteration 4291 : loss : 0.021558, loss_ce: 0.009007
2022-01-08 11:06:56,787 iteration 4292 : loss : 0.019629, loss_ce: 0.009692
2022-01-08 11:06:58,154 iteration 4293 : loss : 0.028963, loss_ce: 0.008115
2022-01-08 11:06:59,489 iteration 4294 : loss : 0.027716, loss_ce: 0.008024
2022-01-08 11:07:00,942 iteration 4295 : loss : 0.016924, loss_ce: 0.005632
2022-01-08 11:07:02,305 iteration 4296 : loss : 0.017171, loss_ce: 0.005678
2022-01-08 11:07:03,694 iteration 4297 : loss : 0.022889, loss_ce: 0.009192
2022-01-08 11:07:05,024 iteration 4298 : loss : 0.018613, loss_ce: 0.009862
2022-01-08 11:07:06,435 iteration 4299 : loss : 0.026721, loss_ce: 0.015785
2022-01-08 11:07:07,881 iteration 4300 : loss : 0.031823, loss_ce: 0.014832
2022-01-08 11:07:09,312 iteration 4301 : loss : 0.022983, loss_ce: 0.012404
 63%|██████████████████▎          | 253/400 [1:46:39<59:55, 24.46s/it]2022-01-08 11:07:10,749 iteration 4302 : loss : 0.026498, loss_ce: 0.011977
2022-01-08 11:07:12,163 iteration 4303 : loss : 0.035905, loss_ce: 0.007746
2022-01-08 11:07:13,482 iteration 4304 : loss : 0.021934, loss_ce: 0.009094
2022-01-08 11:07:14,820 iteration 4305 : loss : 0.018928, loss_ce: 0.007089
2022-01-08 11:07:16,168 iteration 4306 : loss : 0.014480, loss_ce: 0.004501
2022-01-08 11:07:17,479 iteration 4307 : loss : 0.018936, loss_ce: 0.008750
2022-01-08 11:07:18,874 iteration 4308 : loss : 0.024341, loss_ce: 0.010539
2022-01-08 11:07:20,301 iteration 4309 : loss : 0.027568, loss_ce: 0.010089
2022-01-08 11:07:21,654 iteration 4310 : loss : 0.019717, loss_ce: 0.008386
2022-01-08 11:07:22,975 iteration 4311 : loss : 0.023203, loss_ce: 0.007292
2022-01-08 11:07:24,298 iteration 4312 : loss : 0.029975, loss_ce: 0.010386
2022-01-08 11:07:25,627 iteration 4313 : loss : 0.016604, loss_ce: 0.008295
2022-01-08 11:07:26,962 iteration 4314 : loss : 0.027126, loss_ce: 0.010420
2022-01-08 11:07:28,358 iteration 4315 : loss : 0.020178, loss_ce: 0.007145
2022-01-08 11:07:29,682 iteration 4316 : loss : 0.024109, loss_ce: 0.007778
2022-01-08 11:07:31,092 iteration 4317 : loss : 0.025804, loss_ce: 0.011354
2022-01-08 11:07:32,397 iteration 4318 : loss : 0.022970, loss_ce: 0.009260
 64%|██████████████████▍          | 254/400 [1:47:03<58:30, 24.04s/it]2022-01-08 11:07:33,833 iteration 4319 : loss : 0.019858, loss_ce: 0.007148
2022-01-08 11:07:35,222 iteration 4320 : loss : 0.027967, loss_ce: 0.011063
2022-01-08 11:07:36,497 iteration 4321 : loss : 0.016509, loss_ce: 0.004817
2022-01-08 11:07:37,878 iteration 4322 : loss : 0.020573, loss_ce: 0.006636
2022-01-08 11:07:39,182 iteration 4323 : loss : 0.017556, loss_ce: 0.008389
2022-01-08 11:07:40,641 iteration 4324 : loss : 0.030985, loss_ce: 0.013858
2022-01-08 11:07:42,006 iteration 4325 : loss : 0.025652, loss_ce: 0.012313
2022-01-08 11:07:43,349 iteration 4326 : loss : 0.022071, loss_ce: 0.010161
2022-01-08 11:07:44,742 iteration 4327 : loss : 0.021183, loss_ce: 0.006297
2022-01-08 11:07:46,059 iteration 4328 : loss : 0.020924, loss_ce: 0.007182
2022-01-08 11:07:47,488 iteration 4329 : loss : 0.082838, loss_ce: 0.027079
2022-01-08 11:07:48,872 iteration 4330 : loss : 0.027322, loss_ce: 0.012653
2022-01-08 11:07:50,316 iteration 4331 : loss : 0.033623, loss_ce: 0.007507
2022-01-08 11:07:51,723 iteration 4332 : loss : 0.024577, loss_ce: 0.008751
2022-01-08 11:07:53,161 iteration 4333 : loss : 0.039762, loss_ce: 0.020586
2022-01-08 11:07:54,543 iteration 4334 : loss : 0.032683, loss_ce: 0.020307
2022-01-08 11:07:54,544 Training Data Eval:
2022-01-08 11:08:01,413   Average segmentation loss on training set: 0.0141
2022-01-08 11:08:01,414 Validation Data Eval:
2022-01-08 11:08:03,786   Average segmentation loss on validation set: 0.0769
2022-01-08 11:08:05,208 iteration 4335 : loss : 0.035901, loss_ce: 0.008540
 64%|█████████████████▏         | 255/400 [1:47:35<1:04:27, 26.67s/it]2022-01-08 11:08:06,522 iteration 4336 : loss : 0.014806, loss_ce: 0.007011
2022-01-08 11:08:07,969 iteration 4337 : loss : 0.024572, loss_ce: 0.010692
2022-01-08 11:08:09,333 iteration 4338 : loss : 0.026367, loss_ce: 0.009165
2022-01-08 11:08:10,649 iteration 4339 : loss : 0.019830, loss_ce: 0.007450
2022-01-08 11:08:12,047 iteration 4340 : loss : 0.023768, loss_ce: 0.011000
2022-01-08 11:08:13,354 iteration 4341 : loss : 0.019037, loss_ce: 0.005843
2022-01-08 11:08:14,759 iteration 4342 : loss : 0.027320, loss_ce: 0.009492
2022-01-08 11:08:16,160 iteration 4343 : loss : 0.020056, loss_ce: 0.006774
2022-01-08 11:08:17,559 iteration 4344 : loss : 0.024592, loss_ce: 0.012387
2022-01-08 11:08:19,002 iteration 4345 : loss : 0.026923, loss_ce: 0.012051
2022-01-08 11:08:20,293 iteration 4346 : loss : 0.023193, loss_ce: 0.006933
2022-01-08 11:08:21,657 iteration 4347 : loss : 0.016750, loss_ce: 0.007802
2022-01-08 11:08:22,975 iteration 4348 : loss : 0.015511, loss_ce: 0.007825
2022-01-08 11:08:24,262 iteration 4349 : loss : 0.015643, loss_ce: 0.004511
2022-01-08 11:08:25,539 iteration 4350 : loss : 0.020297, loss_ce: 0.007534
2022-01-08 11:08:26,923 iteration 4351 : loss : 0.038336, loss_ce: 0.010845
2022-01-08 11:08:28,294 iteration 4352 : loss : 0.038689, loss_ce: 0.016464
 64%|█████████████████▎         | 256/400 [1:47:58<1:01:26, 25.60s/it]2022-01-08 11:08:29,650 iteration 4353 : loss : 0.026138, loss_ce: 0.008565
2022-01-08 11:08:31,081 iteration 4354 : loss : 0.022581, loss_ce: 0.009993
2022-01-08 11:08:32,483 iteration 4355 : loss : 0.017626, loss_ce: 0.006789
2022-01-08 11:08:33,931 iteration 4356 : loss : 0.028552, loss_ce: 0.007760
2022-01-08 11:08:35,335 iteration 4357 : loss : 0.016582, loss_ce: 0.008000
2022-01-08 11:08:36,761 iteration 4358 : loss : 0.024714, loss_ce: 0.009412
2022-01-08 11:08:38,151 iteration 4359 : loss : 0.024400, loss_ce: 0.008641
2022-01-08 11:08:39,495 iteration 4360 : loss : 0.029762, loss_ce: 0.012956
2022-01-08 11:08:40,907 iteration 4361 : loss : 0.022780, loss_ce: 0.009542
2022-01-08 11:08:42,277 iteration 4362 : loss : 0.018774, loss_ce: 0.008123
2022-01-08 11:08:43,641 iteration 4363 : loss : 0.018892, loss_ce: 0.009266
2022-01-08 11:08:45,022 iteration 4364 : loss : 0.023474, loss_ce: 0.010073
2022-01-08 11:08:46,468 iteration 4365 : loss : 0.034470, loss_ce: 0.018012
2022-01-08 11:08:47,801 iteration 4366 : loss : 0.016852, loss_ce: 0.004860
2022-01-08 11:08:49,073 iteration 4367 : loss : 0.017244, loss_ce: 0.007479
2022-01-08 11:08:50,369 iteration 4368 : loss : 0.021002, loss_ce: 0.006958
2022-01-08 11:08:51,748 iteration 4369 : loss : 0.019439, loss_ce: 0.006984
 64%|██████████████████▋          | 257/400 [1:48:22<59:28, 24.95s/it]2022-01-08 11:08:53,167 iteration 4370 : loss : 0.017085, loss_ce: 0.006111
2022-01-08 11:08:54,553 iteration 4371 : loss : 0.024072, loss_ce: 0.011081
2022-01-08 11:08:55,942 iteration 4372 : loss : 0.024350, loss_ce: 0.010154
2022-01-08 11:08:57,328 iteration 4373 : loss : 0.023284, loss_ce: 0.014246
2022-01-08 11:08:58,733 iteration 4374 : loss : 0.026123, loss_ce: 0.009419
2022-01-08 11:09:00,118 iteration 4375 : loss : 0.021467, loss_ce: 0.009629
2022-01-08 11:09:01,456 iteration 4376 : loss : 0.017559, loss_ce: 0.006517
2022-01-08 11:09:02,811 iteration 4377 : loss : 0.020384, loss_ce: 0.006199
2022-01-08 11:09:04,131 iteration 4378 : loss : 0.016105, loss_ce: 0.005710
2022-01-08 11:09:05,457 iteration 4379 : loss : 0.017350, loss_ce: 0.005546
2022-01-08 11:09:06,742 iteration 4380 : loss : 0.016436, loss_ce: 0.005575
2022-01-08 11:09:08,121 iteration 4381 : loss : 0.026102, loss_ce: 0.009184
2022-01-08 11:09:09,480 iteration 4382 : loss : 0.053935, loss_ce: 0.008813
2022-01-08 11:09:10,806 iteration 4383 : loss : 0.025447, loss_ce: 0.007344
2022-01-08 11:09:12,227 iteration 4384 : loss : 0.031300, loss_ce: 0.012895
2022-01-08 11:09:13,646 iteration 4385 : loss : 0.037669, loss_ce: 0.012017
2022-01-08 11:09:14,967 iteration 4386 : loss : 0.036269, loss_ce: 0.018849
 64%|██████████████████▋          | 258/400 [1:48:45<57:49, 24.44s/it]2022-01-08 11:09:16,312 iteration 4387 : loss : 0.034395, loss_ce: 0.009004
2022-01-08 11:09:17,690 iteration 4388 : loss : 0.039214, loss_ce: 0.017164
2022-01-08 11:09:19,063 iteration 4389 : loss : 0.027615, loss_ce: 0.009825
2022-01-08 11:09:20,371 iteration 4390 : loss : 0.016167, loss_ce: 0.005558
2022-01-08 11:09:21,746 iteration 4391 : loss : 0.038424, loss_ce: 0.010670
2022-01-08 11:09:23,091 iteration 4392 : loss : 0.018375, loss_ce: 0.006350
2022-01-08 11:09:24,518 iteration 4393 : loss : 0.031037, loss_ce: 0.012021
2022-01-08 11:09:25,914 iteration 4394 : loss : 0.025743, loss_ce: 0.009933
2022-01-08 11:09:27,376 iteration 4395 : loss : 0.027141, loss_ce: 0.009260
2022-01-08 11:09:28,705 iteration 4396 : loss : 0.027464, loss_ce: 0.010045
2022-01-08 11:09:30,179 iteration 4397 : loss : 0.028810, loss_ce: 0.013222
2022-01-08 11:09:31,501 iteration 4398 : loss : 0.019614, loss_ce: 0.007959
2022-01-08 11:09:32,906 iteration 4399 : loss : 0.025164, loss_ce: 0.010241
2022-01-08 11:09:34,286 iteration 4400 : loss : 0.028391, loss_ce: 0.007797
2022-01-08 11:09:35,781 iteration 4401 : loss : 0.034156, loss_ce: 0.013288
2022-01-08 11:09:37,159 iteration 4402 : loss : 0.025926, loss_ce: 0.009385
2022-01-08 11:09:38,549 iteration 4403 : loss : 0.023815, loss_ce: 0.007777
 65%|██████████████████▊          | 259/400 [1:49:09<56:49, 24.18s/it]2022-01-08 11:09:39,908 iteration 4404 : loss : 0.024300, loss_ce: 0.012450
2022-01-08 11:09:41,308 iteration 4405 : loss : 0.020637, loss_ce: 0.008636
2022-01-08 11:09:42,708 iteration 4406 : loss : 0.021545, loss_ce: 0.008035
2022-01-08 11:09:44,137 iteration 4407 : loss : 0.026970, loss_ce: 0.009499
2022-01-08 11:09:45,489 iteration 4408 : loss : 0.018498, loss_ce: 0.007965
2022-01-08 11:09:46,887 iteration 4409 : loss : 0.023543, loss_ce: 0.008723
2022-01-08 11:09:48,393 iteration 4410 : loss : 0.038375, loss_ce: 0.015734
2022-01-08 11:09:49,802 iteration 4411 : loss : 0.025469, loss_ce: 0.011246
2022-01-08 11:09:51,159 iteration 4412 : loss : 0.024411, loss_ce: 0.009463
2022-01-08 11:09:52,545 iteration 4413 : loss : 0.016434, loss_ce: 0.005499
2022-01-08 11:09:53,859 iteration 4414 : loss : 0.018702, loss_ce: 0.005406
2022-01-08 11:09:55,263 iteration 4415 : loss : 0.016569, loss_ce: 0.005533
2022-01-08 11:09:56,679 iteration 4416 : loss : 0.026965, loss_ce: 0.009586
2022-01-08 11:09:58,046 iteration 4417 : loss : 0.016935, loss_ce: 0.005616
2022-01-08 11:09:59,447 iteration 4418 : loss : 0.025125, loss_ce: 0.011907
2022-01-08 11:10:00,881 iteration 4419 : loss : 0.029887, loss_ce: 0.009097
2022-01-08 11:10:00,881 Training Data Eval:
2022-01-08 11:10:07,742   Average segmentation loss on training set: 0.0134
2022-01-08 11:10:07,742 Validation Data Eval:
2022-01-08 11:10:10,111   Average segmentation loss on validation set: 0.0848
2022-01-08 11:10:11,440 iteration 4420 : loss : 0.014218, loss_ce: 0.006444
 65%|█████████████████▌         | 260/400 [1:49:42<1:02:31, 26.79s/it]2022-01-08 11:10:12,823 iteration 4421 : loss : 0.017962, loss_ce: 0.006582
2022-01-08 11:10:14,213 iteration 4422 : loss : 0.018562, loss_ce: 0.007502
2022-01-08 11:10:15,640 iteration 4423 : loss : 0.025405, loss_ce: 0.011739
2022-01-08 11:10:16,948 iteration 4424 : loss : 0.020629, loss_ce: 0.006785
2022-01-08 11:10:18,248 iteration 4425 : loss : 0.017264, loss_ce: 0.005944
2022-01-08 11:10:19,677 iteration 4426 : loss : 0.020428, loss_ce: 0.008597
2022-01-08 11:10:20,997 iteration 4427 : loss : 0.015574, loss_ce: 0.006525
2022-01-08 11:10:22,284 iteration 4428 : loss : 0.013459, loss_ce: 0.006184
2022-01-08 11:10:23,586 iteration 4429 : loss : 0.027046, loss_ce: 0.010594
2022-01-08 11:10:24,895 iteration 4430 : loss : 0.015090, loss_ce: 0.006062
2022-01-08 11:10:26,287 iteration 4431 : loss : 0.031906, loss_ce: 0.015274
2022-01-08 11:10:27,692 iteration 4432 : loss : 0.021603, loss_ce: 0.007992
2022-01-08 11:10:29,023 iteration 4433 : loss : 0.020914, loss_ce: 0.005790
2022-01-08 11:10:30,333 iteration 4434 : loss : 0.015369, loss_ce: 0.006283
2022-01-08 11:10:31,696 iteration 4435 : loss : 0.035657, loss_ce: 0.009360
2022-01-08 11:10:33,061 iteration 4436 : loss : 0.019236, loss_ce: 0.009129
2022-01-08 11:10:34,478 iteration 4437 : loss : 0.018198, loss_ce: 0.006405
 65%|██████████████████▉          | 261/400 [1:50:05<59:27, 25.67s/it]2022-01-08 11:10:35,935 iteration 4438 : loss : 0.038237, loss_ce: 0.010350
2022-01-08 11:10:37,325 iteration 4439 : loss : 0.018356, loss_ce: 0.007098
2022-01-08 11:10:38,697 iteration 4440 : loss : 0.020666, loss_ce: 0.008680
2022-01-08 11:10:40,174 iteration 4441 : loss : 0.029982, loss_ce: 0.009009
2022-01-08 11:10:41,597 iteration 4442 : loss : 0.019466, loss_ce: 0.009490
2022-01-08 11:10:42,995 iteration 4443 : loss : 0.019045, loss_ce: 0.006764
2022-01-08 11:10:44,295 iteration 4444 : loss : 0.023018, loss_ce: 0.007092
2022-01-08 11:10:45,649 iteration 4445 : loss : 0.026463, loss_ce: 0.010577
2022-01-08 11:10:47,043 iteration 4446 : loss : 0.018812, loss_ce: 0.008182
2022-01-08 11:10:48,382 iteration 4447 : loss : 0.017769, loss_ce: 0.006128
2022-01-08 11:10:49,765 iteration 4448 : loss : 0.027859, loss_ce: 0.009287
2022-01-08 11:10:51,144 iteration 4449 : loss : 0.022569, loss_ce: 0.009700
2022-01-08 11:10:52,575 iteration 4450 : loss : 0.028432, loss_ce: 0.008745
2022-01-08 11:10:53,907 iteration 4451 : loss : 0.022327, loss_ce: 0.009559
2022-01-08 11:10:55,283 iteration 4452 : loss : 0.016393, loss_ce: 0.007185
2022-01-08 11:10:56,625 iteration 4453 : loss : 0.020642, loss_ce: 0.008452
2022-01-08 11:10:57,981 iteration 4454 : loss : 0.021196, loss_ce: 0.010538
 66%|██████████████████▉          | 262/400 [1:50:28<57:32, 25.02s/it]2022-01-08 11:10:59,406 iteration 4455 : loss : 0.022892, loss_ce: 0.005986
2022-01-08 11:11:00,687 iteration 4456 : loss : 0.016059, loss_ce: 0.006084
2022-01-08 11:11:02,074 iteration 4457 : loss : 0.028860, loss_ce: 0.011989
2022-01-08 11:11:03,537 iteration 4458 : loss : 0.039530, loss_ce: 0.016970
2022-01-08 11:11:05,048 iteration 4459 : loss : 0.035931, loss_ce: 0.015715
2022-01-08 11:11:06,366 iteration 4460 : loss : 0.019823, loss_ce: 0.006752
2022-01-08 11:11:07,741 iteration 4461 : loss : 0.023580, loss_ce: 0.007529
2022-01-08 11:11:09,151 iteration 4462 : loss : 0.034867, loss_ce: 0.013879
2022-01-08 11:11:10,516 iteration 4463 : loss : 0.031431, loss_ce: 0.016389
2022-01-08 11:11:11,860 iteration 4464 : loss : 0.015652, loss_ce: 0.005538
2022-01-08 11:11:13,144 iteration 4465 : loss : 0.033346, loss_ce: 0.010109
2022-01-08 11:11:14,472 iteration 4466 : loss : 0.032791, loss_ce: 0.011317
2022-01-08 11:11:15,855 iteration 4467 : loss : 0.036190, loss_ce: 0.017757
2022-01-08 11:11:17,250 iteration 4468 : loss : 0.017291, loss_ce: 0.007913
2022-01-08 11:11:18,684 iteration 4469 : loss : 0.035308, loss_ce: 0.018932
2022-01-08 11:11:20,101 iteration 4470 : loss : 0.019295, loss_ce: 0.008040
2022-01-08 11:11:21,544 iteration 4471 : loss : 0.029721, loss_ce: 0.011279
 66%|███████████████████          | 263/400 [1:50:52<56:07, 24.58s/it]2022-01-08 11:11:22,909 iteration 4472 : loss : 0.018964, loss_ce: 0.008815
2022-01-08 11:11:24,239 iteration 4473 : loss : 0.025417, loss_ce: 0.010441
2022-01-08 11:11:25,608 iteration 4474 : loss : 0.022067, loss_ce: 0.010428
2022-01-08 11:11:26,968 iteration 4475 : loss : 0.024547, loss_ce: 0.007200
2022-01-08 11:11:28,301 iteration 4476 : loss : 0.025652, loss_ce: 0.008199
2022-01-08 11:11:29,643 iteration 4477 : loss : 0.021329, loss_ce: 0.010568
2022-01-08 11:11:31,120 iteration 4478 : loss : 0.027314, loss_ce: 0.014269
2022-01-08 11:11:32,493 iteration 4479 : loss : 0.022770, loss_ce: 0.006590
2022-01-08 11:11:33,882 iteration 4480 : loss : 0.019557, loss_ce: 0.007303
2022-01-08 11:11:35,193 iteration 4481 : loss : 0.024285, loss_ce: 0.009016
2022-01-08 11:11:36,504 iteration 4482 : loss : 0.042235, loss_ce: 0.013400
2022-01-08 11:11:37,842 iteration 4483 : loss : 0.024684, loss_ce: 0.007328
2022-01-08 11:11:39,221 iteration 4484 : loss : 0.017803, loss_ce: 0.006348
2022-01-08 11:11:40,519 iteration 4485 : loss : 0.017968, loss_ce: 0.007941
2022-01-08 11:11:41,947 iteration 4486 : loss : 0.015507, loss_ce: 0.005754
2022-01-08 11:11:43,343 iteration 4487 : loss : 0.025076, loss_ce: 0.011597
2022-01-08 11:11:44,657 iteration 4488 : loss : 0.022396, loss_ce: 0.008396
 66%|███████████████████▏         | 264/400 [1:51:15<54:42, 24.14s/it]2022-01-08 11:11:46,095 iteration 4489 : loss : 0.023100, loss_ce: 0.010406
2022-01-08 11:11:47,459 iteration 4490 : loss : 0.051328, loss_ce: 0.025456
2022-01-08 11:11:48,924 iteration 4491 : loss : 0.030381, loss_ce: 0.010719
2022-01-08 11:11:50,259 iteration 4492 : loss : 0.021641, loss_ce: 0.005319
2022-01-08 11:11:51,711 iteration 4493 : loss : 0.028466, loss_ce: 0.017243
2022-01-08 11:11:53,041 iteration 4494 : loss : 0.022006, loss_ce: 0.008266
2022-01-08 11:11:54,418 iteration 4495 : loss : 0.022163, loss_ce: 0.005360
2022-01-08 11:11:55,807 iteration 4496 : loss : 0.025566, loss_ce: 0.008062
2022-01-08 11:11:57,074 iteration 4497 : loss : 0.022073, loss_ce: 0.010296
2022-01-08 11:11:58,443 iteration 4498 : loss : 0.022105, loss_ce: 0.006778
2022-01-08 11:11:59,779 iteration 4499 : loss : 0.018469, loss_ce: 0.007786
2022-01-08 11:12:01,150 iteration 4500 : loss : 0.022378, loss_ce: 0.008097
2022-01-08 11:12:02,494 iteration 4501 : loss : 0.019091, loss_ce: 0.008632
2022-01-08 11:12:03,843 iteration 4502 : loss : 0.021131, loss_ce: 0.008186
2022-01-08 11:12:05,286 iteration 4503 : loss : 0.018026, loss_ce: 0.006044
2022-01-08 11:12:06,560 iteration 4504 : loss : 0.027047, loss_ce: 0.012772
2022-01-08 11:12:06,560 Training Data Eval:
2022-01-08 11:12:13,471   Average segmentation loss on training set: 0.0119
2022-01-08 11:12:13,472 Validation Data Eval:
2022-01-08 11:12:15,852   Average segmentation loss on validation set: 0.0649
2022-01-08 11:12:17,234 iteration 4505 : loss : 0.022336, loss_ce: 0.008428
 66%|█████████████████▉         | 265/400 [1:51:47<1:00:00, 26.67s/it]2022-01-08 11:12:18,676 iteration 4506 : loss : 0.018036, loss_ce: 0.006555
2022-01-08 11:12:20,118 iteration 4507 : loss : 0.023094, loss_ce: 0.008913
2022-01-08 11:12:21,489 iteration 4508 : loss : 0.026385, loss_ce: 0.009142
2022-01-08 11:12:22,799 iteration 4509 : loss : 0.014501, loss_ce: 0.004979
2022-01-08 11:12:24,181 iteration 4510 : loss : 0.013859, loss_ce: 0.004844
2022-01-08 11:12:25,634 iteration 4511 : loss : 0.024051, loss_ce: 0.009916
2022-01-08 11:12:26,908 iteration 4512 : loss : 0.017760, loss_ce: 0.006410
2022-01-08 11:12:28,246 iteration 4513 : loss : 0.015744, loss_ce: 0.005981
2022-01-08 11:12:29,582 iteration 4514 : loss : 0.016532, loss_ce: 0.006732
2022-01-08 11:12:30,937 iteration 4515 : loss : 0.018141, loss_ce: 0.008164
2022-01-08 11:12:32,313 iteration 4516 : loss : 0.019283, loss_ce: 0.008239
2022-01-08 11:12:33,712 iteration 4517 : loss : 0.019826, loss_ce: 0.007402
2022-01-08 11:12:35,044 iteration 4518 : loss : 0.041448, loss_ce: 0.016308
2022-01-08 11:12:36,334 iteration 4519 : loss : 0.019312, loss_ce: 0.007180
2022-01-08 11:12:37,705 iteration 4520 : loss : 0.022087, loss_ce: 0.007010
2022-01-08 11:12:39,088 iteration 4521 : loss : 0.020380, loss_ce: 0.009450
2022-01-08 11:12:40,510 iteration 4522 : loss : 0.029341, loss_ce: 0.012693
 66%|███████████████████▎         | 266/400 [1:52:11<57:17, 25.65s/it]2022-01-08 11:12:41,975 iteration 4523 : loss : 0.026899, loss_ce: 0.014822
2022-01-08 11:12:43,353 iteration 4524 : loss : 0.023035, loss_ce: 0.008963
2022-01-08 11:12:44,758 iteration 4525 : loss : 0.021934, loss_ce: 0.010574
2022-01-08 11:12:46,118 iteration 4526 : loss : 0.017984, loss_ce: 0.005509
2022-01-08 11:12:47,416 iteration 4527 : loss : 0.015660, loss_ce: 0.005964
2022-01-08 11:12:48,745 iteration 4528 : loss : 0.015991, loss_ce: 0.005305
2022-01-08 11:12:50,116 iteration 4529 : loss : 0.021108, loss_ce: 0.009586
2022-01-08 11:12:51,448 iteration 4530 : loss : 0.021656, loss_ce: 0.005569
2022-01-08 11:12:52,820 iteration 4531 : loss : 0.018579, loss_ce: 0.006760
2022-01-08 11:12:54,210 iteration 4532 : loss : 0.018237, loss_ce: 0.005976
2022-01-08 11:12:55,556 iteration 4533 : loss : 0.018165, loss_ce: 0.008543
2022-01-08 11:12:56,899 iteration 4534 : loss : 0.021067, loss_ce: 0.006155
2022-01-08 11:12:58,294 iteration 4535 : loss : 0.017125, loss_ce: 0.006501
2022-01-08 11:12:59,678 iteration 4536 : loss : 0.019580, loss_ce: 0.007942
2022-01-08 11:13:01,041 iteration 4537 : loss : 0.026971, loss_ce: 0.014058
2022-01-08 11:13:02,411 iteration 4538 : loss : 0.024019, loss_ce: 0.009427
2022-01-08 11:13:03,781 iteration 4539 : loss : 0.022157, loss_ce: 0.008603
 67%|███████████████████▎         | 267/400 [1:52:34<55:16, 24.94s/it]2022-01-08 11:13:05,161 iteration 4540 : loss : 0.013824, loss_ce: 0.004943
2022-01-08 11:13:06,547 iteration 4541 : loss : 0.022993, loss_ce: 0.013547
2022-01-08 11:13:07,863 iteration 4542 : loss : 0.021227, loss_ce: 0.007374
2022-01-08 11:13:09,202 iteration 4543 : loss : 0.016280, loss_ce: 0.005573
2022-01-08 11:13:10,528 iteration 4544 : loss : 0.018345, loss_ce: 0.006109
2022-01-08 11:13:11,961 iteration 4545 : loss : 0.020118, loss_ce: 0.009083
2022-01-08 11:13:13,436 iteration 4546 : loss : 0.028308, loss_ce: 0.012423
2022-01-08 11:13:14,889 iteration 4547 : loss : 0.021117, loss_ce: 0.008892
2022-01-08 11:13:16,196 iteration 4548 : loss : 0.016802, loss_ce: 0.005882
2022-01-08 11:13:17,522 iteration 4549 : loss : 0.015407, loss_ce: 0.004310
2022-01-08 11:13:18,902 iteration 4550 : loss : 0.021665, loss_ce: 0.009331
2022-01-08 11:13:20,293 iteration 4551 : loss : 0.028383, loss_ce: 0.010569
2022-01-08 11:13:21,744 iteration 4552 : loss : 0.021393, loss_ce: 0.006404
2022-01-08 11:13:23,130 iteration 4553 : loss : 0.021469, loss_ce: 0.008146
2022-01-08 11:13:24,443 iteration 4554 : loss : 0.016364, loss_ce: 0.007059
2022-01-08 11:13:25,795 iteration 4555 : loss : 0.016883, loss_ce: 0.006774
2022-01-08 11:13:27,143 iteration 4556 : loss : 0.021010, loss_ce: 0.005998
 67%|███████████████████▍         | 268/400 [1:52:57<53:49, 24.46s/it]2022-01-08 11:13:28,513 iteration 4557 : loss : 0.013888, loss_ce: 0.005679
2022-01-08 11:13:29,914 iteration 4558 : loss : 0.022468, loss_ce: 0.010385
2022-01-08 11:13:31,351 iteration 4559 : loss : 0.034223, loss_ce: 0.015563
2022-01-08 11:13:32,752 iteration 4560 : loss : 0.017984, loss_ce: 0.009619
2022-01-08 11:13:34,080 iteration 4561 : loss : 0.012658, loss_ce: 0.004124
2022-01-08 11:13:35,459 iteration 4562 : loss : 0.017977, loss_ce: 0.006962
2022-01-08 11:13:36,809 iteration 4563 : loss : 0.020733, loss_ce: 0.006617
2022-01-08 11:13:38,150 iteration 4564 : loss : 0.022911, loss_ce: 0.009341
2022-01-08 11:13:39,506 iteration 4565 : loss : 0.019744, loss_ce: 0.007648
2022-01-08 11:13:40,890 iteration 4566 : loss : 0.018946, loss_ce: 0.006932
2022-01-08 11:13:42,255 iteration 4567 : loss : 0.019512, loss_ce: 0.007686
2022-01-08 11:13:43,561 iteration 4568 : loss : 0.015661, loss_ce: 0.005340
2022-01-08 11:13:44,885 iteration 4569 : loss : 0.014706, loss_ce: 0.006296
2022-01-08 11:13:46,217 iteration 4570 : loss : 0.018788, loss_ce: 0.006314
2022-01-08 11:13:47,628 iteration 4571 : loss : 0.015530, loss_ce: 0.006928
2022-01-08 11:13:49,021 iteration 4572 : loss : 0.018365, loss_ce: 0.007361
2022-01-08 11:13:50,422 iteration 4573 : loss : 0.030213, loss_ce: 0.010902
 67%|███████████████████▌         | 269/400 [1:53:21<52:38, 24.11s/it]2022-01-08 11:13:51,754 iteration 4574 : loss : 0.011196, loss_ce: 0.003588
2022-01-08 11:13:53,141 iteration 4575 : loss : 0.022604, loss_ce: 0.006121
2022-01-08 11:13:54,435 iteration 4576 : loss : 0.016490, loss_ce: 0.006864
2022-01-08 11:13:55,753 iteration 4577 : loss : 0.015562, loss_ce: 0.007231
2022-01-08 11:13:57,104 iteration 4578 : loss : 0.014961, loss_ce: 0.005549
2022-01-08 11:13:58,423 iteration 4579 : loss : 0.016177, loss_ce: 0.007241
2022-01-08 11:13:59,766 iteration 4580 : loss : 0.017269, loss_ce: 0.006606
2022-01-08 11:14:01,132 iteration 4581 : loss : 0.016937, loss_ce: 0.007681
2022-01-08 11:14:02,495 iteration 4582 : loss : 0.021426, loss_ce: 0.008817
2022-01-08 11:14:03,842 iteration 4583 : loss : 0.020783, loss_ce: 0.007307
2022-01-08 11:14:05,181 iteration 4584 : loss : 0.014863, loss_ce: 0.005376
2022-01-08 11:14:06,564 iteration 4585 : loss : 0.026922, loss_ce: 0.009158
2022-01-08 11:14:07,872 iteration 4586 : loss : 0.019466, loss_ce: 0.007966
2022-01-08 11:14:09,207 iteration 4587 : loss : 0.017789, loss_ce: 0.005943
2022-01-08 11:14:10,556 iteration 4588 : loss : 0.024659, loss_ce: 0.011032
2022-01-08 11:14:11,934 iteration 4589 : loss : 0.018311, loss_ce: 0.006538
2022-01-08 11:14:11,934 Training Data Eval:
2022-01-08 11:14:18,831   Average segmentation loss on training set: 0.0116
2022-01-08 11:14:18,831 Validation Data Eval:
2022-01-08 11:14:21,207   Average segmentation loss on validation set: 0.0608
2022-01-08 11:14:25,375 Found new lowest validation loss at iteration 4589! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 11:14:26,640 iteration 4590 : loss : 0.017103, loss_ce: 0.007447
 68%|██████████████████▏        | 270/400 [1:53:57<1:00:06, 27.74s/it]2022-01-08 11:14:27,953 iteration 4591 : loss : 0.028086, loss_ce: 0.010086
2022-01-08 11:14:29,279 iteration 4592 : loss : 0.021070, loss_ce: 0.007438
2022-01-08 11:14:30,580 iteration 4593 : loss : 0.020567, loss_ce: 0.011056
2022-01-08 11:14:31,929 iteration 4594 : loss : 0.033972, loss_ce: 0.010274
2022-01-08 11:14:33,197 iteration 4595 : loss : 0.017640, loss_ce: 0.006072
2022-01-08 11:14:34,450 iteration 4596 : loss : 0.022448, loss_ce: 0.006603
2022-01-08 11:14:35,743 iteration 4597 : loss : 0.015008, loss_ce: 0.006956
2022-01-08 11:14:37,094 iteration 4598 : loss : 0.022774, loss_ce: 0.007909
2022-01-08 11:14:38,460 iteration 4599 : loss : 0.017015, loss_ce: 0.009197
2022-01-08 11:14:39,809 iteration 4600 : loss : 0.016852, loss_ce: 0.005577
2022-01-08 11:14:41,212 iteration 4601 : loss : 0.021248, loss_ce: 0.010186
2022-01-08 11:14:42,691 iteration 4602 : loss : 0.025264, loss_ce: 0.006781
2022-01-08 11:14:44,083 iteration 4603 : loss : 0.019137, loss_ce: 0.006789
2022-01-08 11:14:45,492 iteration 4604 : loss : 0.018013, loss_ce: 0.007796
2022-01-08 11:14:46,931 iteration 4605 : loss : 0.034256, loss_ce: 0.005334
2022-01-08 11:14:48,348 iteration 4606 : loss : 0.018590, loss_ce: 0.007144
2022-01-08 11:14:49,754 iteration 4607 : loss : 0.025929, loss_ce: 0.015610
 68%|███████████████████▋         | 271/400 [1:54:20<56:39, 26.35s/it]2022-01-08 11:14:51,122 iteration 4608 : loss : 0.012710, loss_ce: 0.005089
2022-01-08 11:14:52,506 iteration 4609 : loss : 0.020140, loss_ce: 0.007848
2022-01-08 11:14:53,823 iteration 4610 : loss : 0.014939, loss_ce: 0.004763
2022-01-08 11:14:55,143 iteration 4611 : loss : 0.025889, loss_ce: 0.012304
2022-01-08 11:14:56,446 iteration 4612 : loss : 0.015081, loss_ce: 0.004531
2022-01-08 11:14:57,831 iteration 4613 : loss : 0.019150, loss_ce: 0.006323
2022-01-08 11:14:59,198 iteration 4614 : loss : 0.014753, loss_ce: 0.006598
2022-01-08 11:15:00,608 iteration 4615 : loss : 0.019991, loss_ce: 0.006800
2022-01-08 11:15:02,072 iteration 4616 : loss : 0.029436, loss_ce: 0.014790
2022-01-08 11:15:03,462 iteration 4617 : loss : 0.022520, loss_ce: 0.008868
2022-01-08 11:15:04,785 iteration 4618 : loss : 0.015475, loss_ce: 0.004629
2022-01-08 11:15:06,131 iteration 4619 : loss : 0.012481, loss_ce: 0.005403
2022-01-08 11:15:07,580 iteration 4620 : loss : 0.022566, loss_ce: 0.005764
2022-01-08 11:15:08,931 iteration 4621 : loss : 0.021316, loss_ce: 0.010276
2022-01-08 11:15:10,298 iteration 4622 : loss : 0.033190, loss_ce: 0.013779
2022-01-08 11:15:11,652 iteration 4623 : loss : 0.019708, loss_ce: 0.006222
2022-01-08 11:15:13,035 iteration 4624 : loss : 0.016607, loss_ce: 0.007982
 68%|███████████████████▋         | 272/400 [1:54:43<54:15, 25.43s/it]2022-01-08 11:15:14,480 iteration 4625 : loss : 0.016470, loss_ce: 0.006257
2022-01-08 11:15:15,911 iteration 4626 : loss : 0.018492, loss_ce: 0.008437
2022-01-08 11:15:17,378 iteration 4627 : loss : 0.019556, loss_ce: 0.007250
2022-01-08 11:15:18,722 iteration 4628 : loss : 0.024938, loss_ce: 0.008813
2022-01-08 11:15:20,022 iteration 4629 : loss : 0.015492, loss_ce: 0.005370
2022-01-08 11:15:21,386 iteration 4630 : loss : 0.022071, loss_ce: 0.007949
2022-01-08 11:15:22,766 iteration 4631 : loss : 0.026487, loss_ce: 0.008630
2022-01-08 11:15:24,138 iteration 4632 : loss : 0.022575, loss_ce: 0.008100
2022-01-08 11:15:25,494 iteration 4633 : loss : 0.018648, loss_ce: 0.006089
2022-01-08 11:15:26,823 iteration 4634 : loss : 0.013769, loss_ce: 0.005931
2022-01-08 11:15:28,115 iteration 4635 : loss : 0.011941, loss_ce: 0.004727
2022-01-08 11:15:29,561 iteration 4636 : loss : 0.028105, loss_ce: 0.012100
2022-01-08 11:15:30,902 iteration 4637 : loss : 0.022904, loss_ce: 0.007313
2022-01-08 11:15:32,380 iteration 4638 : loss : 0.020807, loss_ce: 0.009838
2022-01-08 11:15:33,777 iteration 4639 : loss : 0.023460, loss_ce: 0.008961
2022-01-08 11:15:35,254 iteration 4640 : loss : 0.031584, loss_ce: 0.012909
2022-01-08 11:15:36,652 iteration 4641 : loss : 0.022296, loss_ce: 0.008781
 68%|███████████████████▊         | 273/400 [1:55:07<52:40, 24.89s/it]2022-01-08 11:15:38,045 iteration 4642 : loss : 0.022627, loss_ce: 0.004912
2022-01-08 11:15:39,411 iteration 4643 : loss : 0.014887, loss_ce: 0.004697
2022-01-08 11:15:40,833 iteration 4644 : loss : 0.020826, loss_ce: 0.006476
2022-01-08 11:15:42,268 iteration 4645 : loss : 0.025060, loss_ce: 0.012258
2022-01-08 11:15:43,660 iteration 4646 : loss : 0.029642, loss_ce: 0.007140
2022-01-08 11:15:44,964 iteration 4647 : loss : 0.015018, loss_ce: 0.004930
2022-01-08 11:15:46,303 iteration 4648 : loss : 0.019071, loss_ce: 0.006112
2022-01-08 11:15:47,680 iteration 4649 : loss : 0.020121, loss_ce: 0.009136
2022-01-08 11:15:49,094 iteration 4650 : loss : 0.039809, loss_ce: 0.010496
2022-01-08 11:15:50,407 iteration 4651 : loss : 0.016810, loss_ce: 0.007591
2022-01-08 11:15:51,847 iteration 4652 : loss : 0.025624, loss_ce: 0.010965
2022-01-08 11:15:53,203 iteration 4653 : loss : 0.020375, loss_ce: 0.008368
2022-01-08 11:15:54,561 iteration 4654 : loss : 0.022613, loss_ce: 0.005863
2022-01-08 11:15:55,993 iteration 4655 : loss : 0.022868, loss_ce: 0.009377
2022-01-08 11:15:57,436 iteration 4656 : loss : 0.022009, loss_ce: 0.011098
2022-01-08 11:15:58,858 iteration 4657 : loss : 0.035405, loss_ce: 0.014237
2022-01-08 11:16:00,281 iteration 4658 : loss : 0.024687, loss_ce: 0.009392
 68%|███████████████████▊         | 274/400 [1:55:30<51:28, 24.51s/it]2022-01-08 11:16:01,723 iteration 4659 : loss : 0.027964, loss_ce: 0.011829
2022-01-08 11:16:03,158 iteration 4660 : loss : 0.033012, loss_ce: 0.014053
2022-01-08 11:16:04,558 iteration 4661 : loss : 0.027976, loss_ce: 0.010549
2022-01-08 11:16:05,882 iteration 4662 : loss : 0.012308, loss_ce: 0.004514
2022-01-08 11:16:07,280 iteration 4663 : loss : 0.024804, loss_ce: 0.009594
2022-01-08 11:16:08,620 iteration 4664 : loss : 0.020311, loss_ce: 0.008301
2022-01-08 11:16:09,975 iteration 4665 : loss : 0.025833, loss_ce: 0.009537
2022-01-08 11:16:11,253 iteration 4666 : loss : 0.015373, loss_ce: 0.005338
2022-01-08 11:16:12,595 iteration 4667 : loss : 0.014507, loss_ce: 0.005949
2022-01-08 11:16:14,075 iteration 4668 : loss : 0.023291, loss_ce: 0.010735
2022-01-08 11:16:15,390 iteration 4669 : loss : 0.016536, loss_ce: 0.007320
2022-01-08 11:16:16,728 iteration 4670 : loss : 0.018442, loss_ce: 0.007108
2022-01-08 11:16:18,052 iteration 4671 : loss : 0.016121, loss_ce: 0.004772
2022-01-08 11:16:19,520 iteration 4672 : loss : 0.030013, loss_ce: 0.009907
2022-01-08 11:16:20,942 iteration 4673 : loss : 0.030014, loss_ce: 0.014929
2022-01-08 11:16:22,254 iteration 4674 : loss : 0.015101, loss_ce: 0.004386
2022-01-08 11:16:22,254 Training Data Eval:
2022-01-08 11:16:29,131   Average segmentation loss on training set: 0.0116
2022-01-08 11:16:29,131 Validation Data Eval:
2022-01-08 11:16:31,508   Average segmentation loss on validation set: 0.0711
2022-01-08 11:16:32,941 iteration 4675 : loss : 0.015995, loss_ce: 0.006235
 69%|███████████████████▉         | 275/400 [1:56:03<56:09, 26.96s/it]2022-01-08 11:16:34,453 iteration 4676 : loss : 0.022922, loss_ce: 0.007451
2022-01-08 11:16:35,810 iteration 4677 : loss : 0.019437, loss_ce: 0.006710
2022-01-08 11:16:37,200 iteration 4678 : loss : 0.021603, loss_ce: 0.006399
2022-01-08 11:16:38,639 iteration 4679 : loss : 0.042189, loss_ce: 0.010392
2022-01-08 11:16:40,004 iteration 4680 : loss : 0.014657, loss_ce: 0.005532
2022-01-08 11:16:41,358 iteration 4681 : loss : 0.027063, loss_ce: 0.008071
2022-01-08 11:16:42,702 iteration 4682 : loss : 0.023777, loss_ce: 0.008528
2022-01-08 11:16:43,987 iteration 4683 : loss : 0.020310, loss_ce: 0.005487
2022-01-08 11:16:45,415 iteration 4684 : loss : 0.017963, loss_ce: 0.008328
2022-01-08 11:16:46,806 iteration 4685 : loss : 0.021710, loss_ce: 0.006985
2022-01-08 11:16:48,160 iteration 4686 : loss : 0.018663, loss_ce: 0.005667
2022-01-08 11:16:49,502 iteration 4687 : loss : 0.020835, loss_ce: 0.010245
2022-01-08 11:16:50,805 iteration 4688 : loss : 0.012883, loss_ce: 0.005794
2022-01-08 11:16:52,114 iteration 4689 : loss : 0.011135, loss_ce: 0.004921
2022-01-08 11:16:53,442 iteration 4690 : loss : 0.018886, loss_ce: 0.008912
2022-01-08 11:16:54,854 iteration 4691 : loss : 0.057469, loss_ce: 0.032683
2022-01-08 11:16:56,204 iteration 4692 : loss : 0.019570, loss_ce: 0.006202
 69%|████████████████████         | 276/400 [1:56:26<53:25, 25.85s/it]2022-01-08 11:16:57,677 iteration 4693 : loss : 0.023894, loss_ce: 0.008229
2022-01-08 11:16:59,068 iteration 4694 : loss : 0.022689, loss_ce: 0.008689
2022-01-08 11:17:00,304 iteration 4695 : loss : 0.016405, loss_ce: 0.005999
2022-01-08 11:17:01,743 iteration 4696 : loss : 0.027505, loss_ce: 0.010695
2022-01-08 11:17:03,079 iteration 4697 : loss : 0.019340, loss_ce: 0.006924
2022-01-08 11:17:04,457 iteration 4698 : loss : 0.022329, loss_ce: 0.010053
2022-01-08 11:17:05,891 iteration 4699 : loss : 0.021336, loss_ce: 0.010031
2022-01-08 11:17:07,213 iteration 4700 : loss : 0.017567, loss_ce: 0.008133
2022-01-08 11:17:08,669 iteration 4701 : loss : 0.024506, loss_ce: 0.011108
2022-01-08 11:17:10,025 iteration 4702 : loss : 0.024520, loss_ce: 0.011214
2022-01-08 11:17:11,364 iteration 4703 : loss : 0.018542, loss_ce: 0.005310
2022-01-08 11:17:12,727 iteration 4704 : loss : 0.021565, loss_ce: 0.007061
2022-01-08 11:17:13,966 iteration 4705 : loss : 0.012510, loss_ce: 0.005339
2022-01-08 11:17:15,331 iteration 4706 : loss : 0.024185, loss_ce: 0.006236
2022-01-08 11:17:16,745 iteration 4707 : loss : 0.018437, loss_ce: 0.006873
2022-01-08 11:17:18,171 iteration 4708 : loss : 0.023295, loss_ce: 0.007160
2022-01-08 11:17:19,565 iteration 4709 : loss : 0.025111, loss_ce: 0.008213
 69%|████████████████████         | 277/400 [1:56:50<51:27, 25.10s/it]2022-01-08 11:17:20,997 iteration 4710 : loss : 0.034035, loss_ce: 0.006116
2022-01-08 11:17:22,495 iteration 4711 : loss : 0.026029, loss_ce: 0.009017
2022-01-08 11:17:23,843 iteration 4712 : loss : 0.020538, loss_ce: 0.007793
2022-01-08 11:17:25,202 iteration 4713 : loss : 0.014985, loss_ce: 0.005458
2022-01-08 11:17:26,580 iteration 4714 : loss : 0.019393, loss_ce: 0.006021
2022-01-08 11:17:28,016 iteration 4715 : loss : 0.025391, loss_ce: 0.011049
2022-01-08 11:17:29,402 iteration 4716 : loss : 0.015795, loss_ce: 0.005669
2022-01-08 11:17:30,763 iteration 4717 : loss : 0.021202, loss_ce: 0.008508
2022-01-08 11:17:32,073 iteration 4718 : loss : 0.013711, loss_ce: 0.004712
2022-01-08 11:17:33,383 iteration 4719 : loss : 0.016618, loss_ce: 0.007577
2022-01-08 11:17:34,715 iteration 4720 : loss : 0.015544, loss_ce: 0.006141
2022-01-08 11:17:36,057 iteration 4721 : loss : 0.015209, loss_ce: 0.006834
2022-01-08 11:17:37,479 iteration 4722 : loss : 0.026003, loss_ce: 0.007587
2022-01-08 11:17:38,854 iteration 4723 : loss : 0.013487, loss_ce: 0.005424
2022-01-08 11:17:40,151 iteration 4724 : loss : 0.013963, loss_ce: 0.005430
2022-01-08 11:17:41,453 iteration 4725 : loss : 0.015584, loss_ce: 0.006208
2022-01-08 11:17:42,827 iteration 4726 : loss : 0.017068, loss_ce: 0.006364
 70%|████████████████████▏        | 278/400 [1:57:13<49:55, 24.55s/it]2022-01-08 11:17:44,297 iteration 4727 : loss : 0.019166, loss_ce: 0.006284
2022-01-08 11:17:45,603 iteration 4728 : loss : 0.016511, loss_ce: 0.006252
2022-01-08 11:17:46,920 iteration 4729 : loss : 0.014643, loss_ce: 0.005304
2022-01-08 11:17:48,246 iteration 4730 : loss : 0.014383, loss_ce: 0.005504
2022-01-08 11:17:49,667 iteration 4731 : loss : 0.023012, loss_ce: 0.007433
2022-01-08 11:17:51,041 iteration 4732 : loss : 0.025353, loss_ce: 0.007811
2022-01-08 11:17:52,407 iteration 4733 : loss : 0.017503, loss_ce: 0.007078
2022-01-08 11:17:53,751 iteration 4734 : loss : 0.018134, loss_ce: 0.007211
2022-01-08 11:17:55,204 iteration 4735 : loss : 0.025234, loss_ce: 0.008050
2022-01-08 11:17:56,489 iteration 4736 : loss : 0.016060, loss_ce: 0.007264
2022-01-08 11:17:57,919 iteration 4737 : loss : 0.018508, loss_ce: 0.007339
2022-01-08 11:17:59,282 iteration 4738 : loss : 0.021651, loss_ce: 0.008193
2022-01-08 11:18:00,648 iteration 4739 : loss : 0.021825, loss_ce: 0.009344
2022-01-08 11:18:02,059 iteration 4740 : loss : 0.024798, loss_ce: 0.012047
2022-01-08 11:18:03,395 iteration 4741 : loss : 0.016295, loss_ce: 0.005452
2022-01-08 11:18:04,737 iteration 4742 : loss : 0.019490, loss_ce: 0.009263
2022-01-08 11:18:06,177 iteration 4743 : loss : 0.028911, loss_ce: 0.009858
 70%|████████████████████▏        | 279/400 [1:57:36<48:47, 24.19s/it]2022-01-08 11:18:07,635 iteration 4744 : loss : 0.019869, loss_ce: 0.006061
2022-01-08 11:18:09,009 iteration 4745 : loss : 0.028907, loss_ce: 0.008652
2022-01-08 11:18:10,335 iteration 4746 : loss : 0.014654, loss_ce: 0.007451
2022-01-08 11:18:11,668 iteration 4747 : loss : 0.017230, loss_ce: 0.005715
2022-01-08 11:18:13,046 iteration 4748 : loss : 0.022681, loss_ce: 0.008133
2022-01-08 11:18:14,425 iteration 4749 : loss : 0.028076, loss_ce: 0.009868
2022-01-08 11:18:15,841 iteration 4750 : loss : 0.025053, loss_ce: 0.010377
2022-01-08 11:18:17,172 iteration 4751 : loss : 0.023007, loss_ce: 0.009489
2022-01-08 11:18:18,568 iteration 4752 : loss : 0.023191, loss_ce: 0.009512
2022-01-08 11:18:19,980 iteration 4753 : loss : 0.028832, loss_ce: 0.010321
2022-01-08 11:18:21,389 iteration 4754 : loss : 0.014860, loss_ce: 0.005215
2022-01-08 11:18:22,877 iteration 4755 : loss : 0.018952, loss_ce: 0.006002
2022-01-08 11:18:24,209 iteration 4756 : loss : 0.018698, loss_ce: 0.009460
2022-01-08 11:18:25,560 iteration 4757 : loss : 0.020711, loss_ce: 0.008656
2022-01-08 11:18:26,887 iteration 4758 : loss : 0.017526, loss_ce: 0.005454
2022-01-08 11:18:28,243 iteration 4759 : loss : 0.013888, loss_ce: 0.004850
2022-01-08 11:18:28,243 Training Data Eval:
2022-01-08 11:18:35,109   Average segmentation loss on training set: 0.0111
2022-01-08 11:18:35,110 Validation Data Eval:
2022-01-08 11:18:37,476   Average segmentation loss on validation set: 0.0691
2022-01-08 11:18:38,831 iteration 4760 : loss : 0.017198, loss_ce: 0.007717
 70%|████████████████████▎        | 280/400 [1:58:09<53:27, 26.73s/it]2022-01-08 11:18:40,224 iteration 4761 : loss : 0.017933, loss_ce: 0.006459
2022-01-08 11:18:41,631 iteration 4762 : loss : 0.020815, loss_ce: 0.008535
2022-01-08 11:18:42,946 iteration 4763 : loss : 0.013419, loss_ce: 0.005393
2022-01-08 11:18:44,339 iteration 4764 : loss : 0.025971, loss_ce: 0.010153
2022-01-08 11:18:45,677 iteration 4765 : loss : 0.017766, loss_ce: 0.008040
2022-01-08 11:18:47,104 iteration 4766 : loss : 0.022351, loss_ce: 0.008066
2022-01-08 11:18:48,494 iteration 4767 : loss : 0.015218, loss_ce: 0.006133
2022-01-08 11:18:49,817 iteration 4768 : loss : 0.018966, loss_ce: 0.005945
2022-01-08 11:18:51,223 iteration 4769 : loss : 0.019004, loss_ce: 0.007291
2022-01-08 11:18:52,630 iteration 4770 : loss : 0.022118, loss_ce: 0.008107
2022-01-08 11:18:53,932 iteration 4771 : loss : 0.015694, loss_ce: 0.005521
2022-01-08 11:18:55,349 iteration 4772 : loss : 0.022924, loss_ce: 0.009420
2022-01-08 11:18:56,741 iteration 4773 : loss : 0.020345, loss_ce: 0.007430
2022-01-08 11:18:58,132 iteration 4774 : loss : 0.015652, loss_ce: 0.006723
2022-01-08 11:18:59,463 iteration 4775 : loss : 0.023948, loss_ce: 0.007325
2022-01-08 11:19:00,823 iteration 4776 : loss : 0.015333, loss_ce: 0.008649
2022-01-08 11:19:02,319 iteration 4777 : loss : 0.024434, loss_ce: 0.012720
 70%|████████████████████▎        | 281/400 [1:58:32<51:04, 25.76s/it]2022-01-08 11:19:03,658 iteration 4778 : loss : 0.011535, loss_ce: 0.003388
2022-01-08 11:19:05,006 iteration 4779 : loss : 0.020603, loss_ce: 0.007139
2022-01-08 11:19:06,299 iteration 4780 : loss : 0.012206, loss_ce: 0.004339
2022-01-08 11:19:07,636 iteration 4781 : loss : 0.018603, loss_ce: 0.005989
2022-01-08 11:19:09,052 iteration 4782 : loss : 0.019221, loss_ce: 0.008588
2022-01-08 11:19:10,408 iteration 4783 : loss : 0.013191, loss_ce: 0.005815
2022-01-08 11:19:11,707 iteration 4784 : loss : 0.016645, loss_ce: 0.007048
2022-01-08 11:19:13,136 iteration 4785 : loss : 0.020164, loss_ce: 0.006869
2022-01-08 11:19:14,529 iteration 4786 : loss : 0.018256, loss_ce: 0.007202
2022-01-08 11:19:15,925 iteration 4787 : loss : 0.018111, loss_ce: 0.005883
2022-01-08 11:19:17,327 iteration 4788 : loss : 0.018631, loss_ce: 0.007184
2022-01-08 11:19:18,635 iteration 4789 : loss : 0.019615, loss_ce: 0.011710
2022-01-08 11:19:20,096 iteration 4790 : loss : 0.028598, loss_ce: 0.010364
2022-01-08 11:19:21,440 iteration 4791 : loss : 0.037389, loss_ce: 0.022588
2022-01-08 11:19:22,855 iteration 4792 : loss : 0.022300, loss_ce: 0.006142
2022-01-08 11:19:24,178 iteration 4793 : loss : 0.013846, loss_ce: 0.005182
2022-01-08 11:19:25,460 iteration 4794 : loss : 0.021319, loss_ce: 0.006556
 70%|████████████████████▍        | 282/400 [1:58:56<49:06, 24.97s/it]2022-01-08 11:19:26,893 iteration 4795 : loss : 0.019954, loss_ce: 0.006503
2022-01-08 11:19:28,266 iteration 4796 : loss : 0.018352, loss_ce: 0.005990
2022-01-08 11:19:29,650 iteration 4797 : loss : 0.025179, loss_ce: 0.008773
2022-01-08 11:19:31,083 iteration 4798 : loss : 0.026287, loss_ce: 0.008917
2022-01-08 11:19:32,445 iteration 4799 : loss : 0.013093, loss_ce: 0.004190
2022-01-08 11:19:33,832 iteration 4800 : loss : 0.021670, loss_ce: 0.005905
2022-01-08 11:19:35,290 iteration 4801 : loss : 0.016610, loss_ce: 0.005964
2022-01-08 11:19:36,591 iteration 4802 : loss : 0.014818, loss_ce: 0.006878
2022-01-08 11:19:37,933 iteration 4803 : loss : 0.016063, loss_ce: 0.006852
2022-01-08 11:19:39,363 iteration 4804 : loss : 0.040575, loss_ce: 0.011035
2022-01-08 11:19:40,773 iteration 4805 : loss : 0.034547, loss_ce: 0.013037
2022-01-08 11:19:42,191 iteration 4806 : loss : 0.017429, loss_ce: 0.006642
2022-01-08 11:19:43,490 iteration 4807 : loss : 0.017926, loss_ce: 0.007158
2022-01-08 11:19:44,783 iteration 4808 : loss : 0.015396, loss_ce: 0.006634
2022-01-08 11:19:46,164 iteration 4809 : loss : 0.020097, loss_ce: 0.009454
2022-01-08 11:19:47,524 iteration 4810 : loss : 0.021672, loss_ce: 0.006509
2022-01-08 11:19:48,909 iteration 4811 : loss : 0.016001, loss_ce: 0.006862
 71%|████████████████████▌        | 283/400 [1:59:19<47:48, 24.51s/it]2022-01-08 11:19:50,271 iteration 4812 : loss : 0.016501, loss_ce: 0.006648
2022-01-08 11:19:51,657 iteration 4813 : loss : 0.022902, loss_ce: 0.005132
2022-01-08 11:19:53,062 iteration 4814 : loss : 0.026628, loss_ce: 0.007304
2022-01-08 11:19:54,368 iteration 4815 : loss : 0.013585, loss_ce: 0.005780
2022-01-08 11:19:55,838 iteration 4816 : loss : 0.021540, loss_ce: 0.010905
2022-01-08 11:19:57,185 iteration 4817 : loss : 0.017606, loss_ce: 0.006904
2022-01-08 11:19:58,532 iteration 4818 : loss : 0.019576, loss_ce: 0.006378
2022-01-08 11:19:59,814 iteration 4819 : loss : 0.018204, loss_ce: 0.006621
2022-01-08 11:20:01,156 iteration 4820 : loss : 0.025543, loss_ce: 0.009861
2022-01-08 11:20:02,608 iteration 4821 : loss : 0.020357, loss_ce: 0.008772
2022-01-08 11:20:04,042 iteration 4822 : loss : 0.020442, loss_ce: 0.007200
2022-01-08 11:20:05,391 iteration 4823 : loss : 0.019062, loss_ce: 0.007719
2022-01-08 11:20:06,780 iteration 4824 : loss : 0.021375, loss_ce: 0.008497
2022-01-08 11:20:08,123 iteration 4825 : loss : 0.018616, loss_ce: 0.007184
2022-01-08 11:20:09,463 iteration 4826 : loss : 0.014849, loss_ce: 0.004527
2022-01-08 11:20:10,808 iteration 4827 : loss : 0.013513, loss_ce: 0.005222
2022-01-08 11:20:12,131 iteration 4828 : loss : 0.021613, loss_ce: 0.008140
 71%|████████████████████▌        | 284/400 [1:59:42<46:38, 24.13s/it]2022-01-08 11:20:13,480 iteration 4829 : loss : 0.017106, loss_ce: 0.005570
2022-01-08 11:20:14,885 iteration 4830 : loss : 0.015699, loss_ce: 0.006819
2022-01-08 11:20:16,272 iteration 4831 : loss : 0.014622, loss_ce: 0.005961
2022-01-08 11:20:17,628 iteration 4832 : loss : 0.017016, loss_ce: 0.005602
2022-01-08 11:20:19,026 iteration 4833 : loss : 0.021225, loss_ce: 0.007507
2022-01-08 11:20:20,331 iteration 4834 : loss : 0.014186, loss_ce: 0.005002
2022-01-08 11:20:21,810 iteration 4835 : loss : 0.021123, loss_ce: 0.010502
2022-01-08 11:20:23,180 iteration 4836 : loss : 0.026316, loss_ce: 0.008462
2022-01-08 11:20:24,587 iteration 4837 : loss : 0.017406, loss_ce: 0.004355
2022-01-08 11:20:25,936 iteration 4838 : loss : 0.013674, loss_ce: 0.004512
2022-01-08 11:20:27,332 iteration 4839 : loss : 0.025947, loss_ce: 0.010720
2022-01-08 11:20:28,700 iteration 4840 : loss : 0.026259, loss_ce: 0.013540
2022-01-08 11:20:29,997 iteration 4841 : loss : 0.012140, loss_ce: 0.004031
2022-01-08 11:20:31,377 iteration 4842 : loss : 0.016402, loss_ce: 0.006325
2022-01-08 11:20:32,822 iteration 4843 : loss : 0.030864, loss_ce: 0.009751
2022-01-08 11:20:34,180 iteration 4844 : loss : 0.030446, loss_ce: 0.011483
2022-01-08 11:20:34,181 Training Data Eval:
2022-01-08 11:20:41,044   Average segmentation loss on training set: 0.0108
2022-01-08 11:20:41,044 Validation Data Eval:
2022-01-08 11:20:43,424   Average segmentation loss on validation set: 0.0652
2022-01-08 11:20:44,864 iteration 4845 : loss : 0.022773, loss_ce: 0.009843
 71%|████████████████████▋        | 285/400 [2:00:15<51:11, 26.71s/it]2022-01-08 11:20:46,249 iteration 4846 : loss : 0.017062, loss_ce: 0.006814
2022-01-08 11:20:47,566 iteration 4847 : loss : 0.018535, loss_ce: 0.007167
2022-01-08 11:20:48,963 iteration 4848 : loss : 0.035133, loss_ce: 0.022038
2022-01-08 11:20:50,363 iteration 4849 : loss : 0.015711, loss_ce: 0.004096
2022-01-08 11:20:51,708 iteration 4850 : loss : 0.015931, loss_ce: 0.004747
2022-01-08 11:20:53,188 iteration 4851 : loss : 0.026165, loss_ce: 0.011275
2022-01-08 11:20:54,529 iteration 4852 : loss : 0.015327, loss_ce: 0.005178
2022-01-08 11:20:55,901 iteration 4853 : loss : 0.015965, loss_ce: 0.006987
2022-01-08 11:20:57,282 iteration 4854 : loss : 0.016443, loss_ce: 0.006635
2022-01-08 11:20:58,626 iteration 4855 : loss : 0.022671, loss_ce: 0.006631
2022-01-08 11:21:00,082 iteration 4856 : loss : 0.027093, loss_ce: 0.010952
2022-01-08 11:21:01,416 iteration 4857 : loss : 0.014762, loss_ce: 0.005218
2022-01-08 11:21:02,863 iteration 4858 : loss : 0.023088, loss_ce: 0.009436
2022-01-08 11:21:04,170 iteration 4859 : loss : 0.010812, loss_ce: 0.004464
2022-01-08 11:21:05,577 iteration 4860 : loss : 0.029586, loss_ce: 0.012590
2022-01-08 11:21:06,926 iteration 4861 : loss : 0.019403, loss_ce: 0.006580
2022-01-08 11:21:08,335 iteration 4862 : loss : 0.016119, loss_ce: 0.007197
 72%|████████████████████▋        | 286/400 [2:00:39<48:53, 25.74s/it]2022-01-08 11:21:09,796 iteration 4863 : loss : 0.018284, loss_ce: 0.005416
2022-01-08 11:21:11,144 iteration 4864 : loss : 0.014829, loss_ce: 0.005080
2022-01-08 11:21:12,492 iteration 4865 : loss : 0.016646, loss_ce: 0.007343
2022-01-08 11:21:13,932 iteration 4866 : loss : 0.018781, loss_ce: 0.005698
2022-01-08 11:21:15,308 iteration 4867 : loss : 0.016605, loss_ce: 0.005811
2022-01-08 11:21:16,575 iteration 4868 : loss : 0.015413, loss_ce: 0.006405
2022-01-08 11:21:18,029 iteration 4869 : loss : 0.024813, loss_ce: 0.013580
2022-01-08 11:21:19,394 iteration 4870 : loss : 0.014901, loss_ce: 0.005444
2022-01-08 11:21:20,854 iteration 4871 : loss : 0.036859, loss_ce: 0.014137
2022-01-08 11:21:22,147 iteration 4872 : loss : 0.014065, loss_ce: 0.004147
2022-01-08 11:21:23,600 iteration 4873 : loss : 0.020927, loss_ce: 0.010583
2022-01-08 11:21:24,976 iteration 4874 : loss : 0.021684, loss_ce: 0.007006
2022-01-08 11:21:26,323 iteration 4875 : loss : 0.019035, loss_ce: 0.007470
2022-01-08 11:21:27,613 iteration 4876 : loss : 0.019036, loss_ce: 0.005995
2022-01-08 11:21:29,039 iteration 4877 : loss : 0.029918, loss_ce: 0.015150
2022-01-08 11:21:30,479 iteration 4878 : loss : 0.035663, loss_ce: 0.015388
2022-01-08 11:21:31,831 iteration 4879 : loss : 0.013675, loss_ce: 0.004695
 72%|████████████████████▊        | 287/400 [2:01:02<47:12, 25.06s/it]2022-01-08 11:21:33,210 iteration 4880 : loss : 0.025082, loss_ce: 0.010004
2022-01-08 11:21:34,524 iteration 4881 : loss : 0.015958, loss_ce: 0.006497
2022-01-08 11:21:35,805 iteration 4882 : loss : 0.013179, loss_ce: 0.005838
2022-01-08 11:21:37,190 iteration 4883 : loss : 0.015503, loss_ce: 0.006406
2022-01-08 11:21:38,590 iteration 4884 : loss : 0.019084, loss_ce: 0.005421
2022-01-08 11:21:39,861 iteration 4885 : loss : 0.014464, loss_ce: 0.005737
2022-01-08 11:21:41,227 iteration 4886 : loss : 0.022317, loss_ce: 0.006693
2022-01-08 11:21:42,535 iteration 4887 : loss : 0.013996, loss_ce: 0.004291
2022-01-08 11:21:43,922 iteration 4888 : loss : 0.022525, loss_ce: 0.006371
2022-01-08 11:21:45,291 iteration 4889 : loss : 0.024169, loss_ce: 0.012599
2022-01-08 11:21:46,602 iteration 4890 : loss : 0.015737, loss_ce: 0.008093
2022-01-08 11:21:47,866 iteration 4891 : loss : 0.011784, loss_ce: 0.004593
2022-01-08 11:21:49,270 iteration 4892 : loss : 0.014420, loss_ce: 0.005604
2022-01-08 11:21:50,635 iteration 4893 : loss : 0.022256, loss_ce: 0.010329
2022-01-08 11:21:51,984 iteration 4894 : loss : 0.012704, loss_ce: 0.005631
2022-01-08 11:21:53,320 iteration 4895 : loss : 0.014226, loss_ce: 0.002929
2022-01-08 11:21:54,645 iteration 4896 : loss : 0.016024, loss_ce: 0.004903
 72%|████████████████████▉        | 288/400 [2:01:25<45:31, 24.39s/it]2022-01-08 11:21:56,026 iteration 4897 : loss : 0.016567, loss_ce: 0.006506
2022-01-08 11:21:57,469 iteration 4898 : loss : 0.023801, loss_ce: 0.012483
2022-01-08 11:21:58,829 iteration 4899 : loss : 0.014157, loss_ce: 0.003932
2022-01-08 11:22:00,157 iteration 4900 : loss : 0.024292, loss_ce: 0.008472
2022-01-08 11:22:01,491 iteration 4901 : loss : 0.028843, loss_ce: 0.008139
2022-01-08 11:22:02,832 iteration 4902 : loss : 0.011505, loss_ce: 0.004061
2022-01-08 11:22:04,219 iteration 4903 : loss : 0.016230, loss_ce: 0.007662
2022-01-08 11:22:05,487 iteration 4904 : loss : 0.013978, loss_ce: 0.004549
2022-01-08 11:22:06,808 iteration 4905 : loss : 0.017124, loss_ce: 0.005011
2022-01-08 11:22:08,201 iteration 4906 : loss : 0.019075, loss_ce: 0.006997
2022-01-08 11:22:09,519 iteration 4907 : loss : 0.021288, loss_ce: 0.008441
2022-01-08 11:22:10,948 iteration 4908 : loss : 0.021503, loss_ce: 0.009617
2022-01-08 11:22:12,329 iteration 4909 : loss : 0.015310, loss_ce: 0.004819
2022-01-08 11:22:13,639 iteration 4910 : loss : 0.014626, loss_ce: 0.006698
2022-01-08 11:22:14,972 iteration 4911 : loss : 0.012400, loss_ce: 0.004504
2022-01-08 11:22:16,340 iteration 4912 : loss : 0.014657, loss_ce: 0.006547
2022-01-08 11:22:17,691 iteration 4913 : loss : 0.020534, loss_ce: 0.008898
 72%|████████████████████▉        | 289/400 [2:01:48<44:22, 23.99s/it]2022-01-08 11:22:19,122 iteration 4914 : loss : 0.019575, loss_ce: 0.007644
2022-01-08 11:22:20,444 iteration 4915 : loss : 0.020675, loss_ce: 0.007414
2022-01-08 11:22:21,779 iteration 4916 : loss : 0.013066, loss_ce: 0.003405
2022-01-08 11:22:23,146 iteration 4917 : loss : 0.022255, loss_ce: 0.006339
2022-01-08 11:22:24,542 iteration 4918 : loss : 0.015429, loss_ce: 0.006054
2022-01-08 11:22:26,053 iteration 4919 : loss : 0.029020, loss_ce: 0.012441
2022-01-08 11:22:27,392 iteration 4920 : loss : 0.013822, loss_ce: 0.005445
2022-01-08 11:22:28,710 iteration 4921 : loss : 0.015521, loss_ce: 0.007702
2022-01-08 11:22:30,007 iteration 4922 : loss : 0.014009, loss_ce: 0.003766
2022-01-08 11:22:31,411 iteration 4923 : loss : 0.021203, loss_ce: 0.008303
2022-01-08 11:22:32,838 iteration 4924 : loss : 0.022868, loss_ce: 0.012044
2022-01-08 11:22:34,210 iteration 4925 : loss : 0.013381, loss_ce: 0.004946
2022-01-08 11:22:35,621 iteration 4926 : loss : 0.023832, loss_ce: 0.012216
2022-01-08 11:22:36,974 iteration 4927 : loss : 0.030265, loss_ce: 0.012553
2022-01-08 11:22:38,406 iteration 4928 : loss : 0.023313, loss_ce: 0.008475
2022-01-08 11:22:39,860 iteration 4929 : loss : 0.019809, loss_ce: 0.007999
2022-01-08 11:22:39,861 Training Data Eval:
2022-01-08 11:22:46,715   Average segmentation loss on training set: 0.0109
2022-01-08 11:22:46,715 Validation Data Eval:
2022-01-08 11:22:49,094   Average segmentation loss on validation set: 0.0672
2022-01-08 11:22:50,477 iteration 4930 : loss : 0.017423, loss_ce: 0.005193
 72%|█████████████████████        | 290/400 [2:02:21<48:48, 26.63s/it]2022-01-08 11:22:51,867 iteration 4931 : loss : 0.016927, loss_ce: 0.007793
2022-01-08 11:22:53,256 iteration 4932 : loss : 0.016301, loss_ce: 0.007925
2022-01-08 11:22:54,572 iteration 4933 : loss : 0.015869, loss_ce: 0.007075
2022-01-08 11:22:56,002 iteration 4934 : loss : 0.027231, loss_ce: 0.009863
2022-01-08 11:22:57,360 iteration 4935 : loss : 0.014753, loss_ce: 0.004945
2022-01-08 11:22:58,752 iteration 4936 : loss : 0.020135, loss_ce: 0.007449
2022-01-08 11:23:00,129 iteration 4937 : loss : 0.016540, loss_ce: 0.007215
2022-01-08 11:23:01,540 iteration 4938 : loss : 0.040469, loss_ce: 0.015418
2022-01-08 11:23:02,953 iteration 4939 : loss : 0.034573, loss_ce: 0.016111
2022-01-08 11:23:04,302 iteration 4940 : loss : 0.017863, loss_ce: 0.007415
2022-01-08 11:23:05,687 iteration 4941 : loss : 0.018249, loss_ce: 0.007988
2022-01-08 11:23:07,067 iteration 4942 : loss : 0.015476, loss_ce: 0.005234
2022-01-08 11:23:08,547 iteration 4943 : loss : 0.028223, loss_ce: 0.007340
2022-01-08 11:23:09,914 iteration 4944 : loss : 0.027685, loss_ce: 0.008668
2022-01-08 11:23:11,315 iteration 4945 : loss : 0.020608, loss_ce: 0.008495
2022-01-08 11:23:12,749 iteration 4946 : loss : 0.023123, loss_ce: 0.007318
2022-01-08 11:23:14,101 iteration 4947 : loss : 0.038975, loss_ce: 0.012956
 73%|█████████████████████        | 291/400 [2:02:44<46:44, 25.73s/it]2022-01-08 11:23:15,528 iteration 4948 : loss : 0.019843, loss_ce: 0.006756
2022-01-08 11:23:16,865 iteration 4949 : loss : 0.020021, loss_ce: 0.008239
2022-01-08 11:23:18,270 iteration 4950 : loss : 0.022186, loss_ce: 0.009616
2022-01-08 11:23:19,670 iteration 4951 : loss : 0.022897, loss_ce: 0.008581
2022-01-08 11:23:21,064 iteration 4952 : loss : 0.015315, loss_ce: 0.007870
2022-01-08 11:23:22,472 iteration 4953 : loss : 0.027231, loss_ce: 0.010678
2022-01-08 11:23:23,812 iteration 4954 : loss : 0.014033, loss_ce: 0.004561
2022-01-08 11:23:25,116 iteration 4955 : loss : 0.020335, loss_ce: 0.007741
2022-01-08 11:23:26,473 iteration 4956 : loss : 0.025766, loss_ce: 0.007236
2022-01-08 11:23:27,760 iteration 4957 : loss : 0.014112, loss_ce: 0.005593
2022-01-08 11:23:29,142 iteration 4958 : loss : 0.036047, loss_ce: 0.017280
2022-01-08 11:23:30,517 iteration 4959 : loss : 0.017185, loss_ce: 0.006248
2022-01-08 11:23:31,802 iteration 4960 : loss : 0.018050, loss_ce: 0.008321
2022-01-08 11:23:33,189 iteration 4961 : loss : 0.015696, loss_ce: 0.006270
2022-01-08 11:23:34,506 iteration 4962 : loss : 0.020140, loss_ce: 0.004550
2022-01-08 11:23:35,865 iteration 4963 : loss : 0.019670, loss_ce: 0.008678
2022-01-08 11:23:37,280 iteration 4964 : loss : 0.031070, loss_ce: 0.013762
 73%|█████████████████████▏       | 292/400 [2:03:07<44:55, 24.96s/it]2022-01-08 11:23:38,750 iteration 4965 : loss : 0.027586, loss_ce: 0.010179
2022-01-08 11:23:40,167 iteration 4966 : loss : 0.025953, loss_ce: 0.010878
2022-01-08 11:23:41,551 iteration 4967 : loss : 0.017219, loss_ce: 0.007430
2022-01-08 11:23:42,951 iteration 4968 : loss : 0.014533, loss_ce: 0.006850
2022-01-08 11:23:44,420 iteration 4969 : loss : 0.037456, loss_ce: 0.012806
2022-01-08 11:23:45,729 iteration 4970 : loss : 0.014458, loss_ce: 0.005911
2022-01-08 11:23:47,078 iteration 4971 : loss : 0.019180, loss_ce: 0.009855
2022-01-08 11:23:48,486 iteration 4972 : loss : 0.022583, loss_ce: 0.009833
2022-01-08 11:23:49,853 iteration 4973 : loss : 0.025655, loss_ce: 0.008561
2022-01-08 11:23:51,336 iteration 4974 : loss : 0.031459, loss_ce: 0.011475
2022-01-08 11:23:52,676 iteration 4975 : loss : 0.016952, loss_ce: 0.004802
2022-01-08 11:23:53,987 iteration 4976 : loss : 0.016204, loss_ce: 0.005693
2022-01-08 11:23:55,388 iteration 4977 : loss : 0.023399, loss_ce: 0.004598
2022-01-08 11:23:56,744 iteration 4978 : loss : 0.015225, loss_ce: 0.004241
2022-01-08 11:23:58,109 iteration 4979 : loss : 0.020466, loss_ce: 0.007655
2022-01-08 11:23:59,480 iteration 4980 : loss : 0.027680, loss_ce: 0.007691
2022-01-08 11:24:00,832 iteration 4981 : loss : 0.021129, loss_ce: 0.006474
 73%|█████████████████████▏       | 293/400 [2:03:31<43:45, 24.54s/it]2022-01-08 11:24:02,218 iteration 4982 : loss : 0.014453, loss_ce: 0.006686
2022-01-08 11:24:03,567 iteration 4983 : loss : 0.015202, loss_ce: 0.006162
2022-01-08 11:24:05,033 iteration 4984 : loss : 0.025179, loss_ce: 0.010264
2022-01-08 11:24:06,326 iteration 4985 : loss : 0.018756, loss_ce: 0.005110
2022-01-08 11:24:07,770 iteration 4986 : loss : 0.021716, loss_ce: 0.008752
2022-01-08 11:24:09,083 iteration 4987 : loss : 0.017311, loss_ce: 0.005488
2022-01-08 11:24:10,469 iteration 4988 : loss : 0.017762, loss_ce: 0.005171
2022-01-08 11:24:11,901 iteration 4989 : loss : 0.029182, loss_ce: 0.010718
2022-01-08 11:24:13,360 iteration 4990 : loss : 0.017079, loss_ce: 0.007319
2022-01-08 11:24:14,738 iteration 4991 : loss : 0.023507, loss_ce: 0.007931
2022-01-08 11:24:16,017 iteration 4992 : loss : 0.014501, loss_ce: 0.006842
2022-01-08 11:24:17,379 iteration 4993 : loss : 0.018367, loss_ce: 0.006422
2022-01-08 11:24:18,700 iteration 4994 : loss : 0.015460, loss_ce: 0.005331
2022-01-08 11:24:20,050 iteration 4995 : loss : 0.018212, loss_ce: 0.007862
2022-01-08 11:24:21,486 iteration 4996 : loss : 0.034836, loss_ce: 0.008830
2022-01-08 11:24:22,775 iteration 4997 : loss : 0.014327, loss_ce: 0.004256
2022-01-08 11:24:24,093 iteration 4998 : loss : 0.014879, loss_ce: 0.005618
 74%|█████████████████████▎       | 294/400 [2:03:54<42:40, 24.16s/it]2022-01-08 11:24:25,474 iteration 4999 : loss : 0.015835, loss_ce: 0.006408
2022-01-08 11:24:26,827 iteration 5000 : loss : 0.015215, loss_ce: 0.006011
2022-01-08 11:24:28,184 iteration 5001 : loss : 0.015493, loss_ce: 0.006512
2022-01-08 11:24:29,527 iteration 5002 : loss : 0.011492, loss_ce: 0.003824
2022-01-08 11:24:30,915 iteration 5003 : loss : 0.038446, loss_ce: 0.007102
2022-01-08 11:24:32,271 iteration 5004 : loss : 0.030848, loss_ce: 0.011064
2022-01-08 11:24:33,631 iteration 5005 : loss : 0.021335, loss_ce: 0.009261
2022-01-08 11:24:35,045 iteration 5006 : loss : 0.018453, loss_ce: 0.007737
2022-01-08 11:24:36,447 iteration 5007 : loss : 0.034553, loss_ce: 0.014110
2022-01-08 11:24:37,860 iteration 5008 : loss : 0.030379, loss_ce: 0.015648
2022-01-08 11:24:39,262 iteration 5009 : loss : 0.019897, loss_ce: 0.005505
2022-01-08 11:24:40,651 iteration 5010 : loss : 0.023875, loss_ce: 0.009245
2022-01-08 11:24:42,000 iteration 5011 : loss : 0.015008, loss_ce: 0.006399
2022-01-08 11:24:43,382 iteration 5012 : loss : 0.017248, loss_ce: 0.006438
2022-01-08 11:24:44,693 iteration 5013 : loss : 0.015757, loss_ce: 0.005940
2022-01-08 11:24:46,060 iteration 5014 : loss : 0.019649, loss_ce: 0.007292
2022-01-08 11:24:46,060 Training Data Eval:
2022-01-08 11:24:52,950   Average segmentation loss on training set: 0.0099
2022-01-08 11:24:52,950 Validation Data Eval:
2022-01-08 11:24:55,328   Average segmentation loss on validation set: 0.0766
2022-01-08 11:24:56,708 iteration 5015 : loss : 0.017634, loss_ce: 0.005494
 74%|█████████████████████▍       | 295/400 [2:04:27<46:42, 26.69s/it]2022-01-08 11:24:58,115 iteration 5016 : loss : 0.015811, loss_ce: 0.007196
2022-01-08 11:24:59,467 iteration 5017 : loss : 0.021336, loss_ce: 0.008054
2022-01-08 11:25:00,833 iteration 5018 : loss : 0.015046, loss_ce: 0.006220
2022-01-08 11:25:02,223 iteration 5019 : loss : 0.017766, loss_ce: 0.006062
2022-01-08 11:25:03,615 iteration 5020 : loss : 0.026525, loss_ce: 0.007535
2022-01-08 11:25:05,034 iteration 5021 : loss : 0.022928, loss_ce: 0.012821
2022-01-08 11:25:06,356 iteration 5022 : loss : 0.016161, loss_ce: 0.005396
2022-01-08 11:25:07,722 iteration 5023 : loss : 0.026322, loss_ce: 0.007162
2022-01-08 11:25:09,178 iteration 5024 : loss : 0.026912, loss_ce: 0.005310
2022-01-08 11:25:10,608 iteration 5025 : loss : 0.024434, loss_ce: 0.004222
2022-01-08 11:25:11,983 iteration 5026 : loss : 0.020324, loss_ce: 0.007348
2022-01-08 11:25:13,321 iteration 5027 : loss : 0.017572, loss_ce: 0.007665
2022-01-08 11:25:14,670 iteration 5028 : loss : 0.021324, loss_ce: 0.007438
2022-01-08 11:25:16,127 iteration 5029 : loss : 0.047209, loss_ce: 0.012421
2022-01-08 11:25:17,498 iteration 5030 : loss : 0.021861, loss_ce: 0.011350
2022-01-08 11:25:18,926 iteration 5031 : loss : 0.026858, loss_ce: 0.010537
2022-01-08 11:25:20,303 iteration 5032 : loss : 0.016654, loss_ce: 0.008079
 74%|█████████████████████▍       | 296/400 [2:04:50<44:39, 25.76s/it]2022-01-08 11:25:21,818 iteration 5033 : loss : 0.021902, loss_ce: 0.007288
2022-01-08 11:25:23,171 iteration 5034 : loss : 0.019232, loss_ce: 0.005819
2022-01-08 11:25:24,508 iteration 5035 : loss : 0.018270, loss_ce: 0.007731
2022-01-08 11:25:25,858 iteration 5036 : loss : 0.017688, loss_ce: 0.006094
2022-01-08 11:25:27,237 iteration 5037 : loss : 0.020401, loss_ce: 0.007117
2022-01-08 11:25:28,578 iteration 5038 : loss : 0.024758, loss_ce: 0.005327
2022-01-08 11:25:30,026 iteration 5039 : loss : 0.024466, loss_ce: 0.012403
2022-01-08 11:25:31,406 iteration 5040 : loss : 0.016314, loss_ce: 0.004899
2022-01-08 11:25:32,850 iteration 5041 : loss : 0.017186, loss_ce: 0.006466
2022-01-08 11:25:34,178 iteration 5042 : loss : 0.018649, loss_ce: 0.008338
2022-01-08 11:25:35,555 iteration 5043 : loss : 0.016861, loss_ce: 0.007415
2022-01-08 11:25:36,936 iteration 5044 : loss : 0.023982, loss_ce: 0.010880
2022-01-08 11:25:38,355 iteration 5045 : loss : 0.026316, loss_ce: 0.011098
2022-01-08 11:25:39,738 iteration 5046 : loss : 0.035265, loss_ce: 0.008767
2022-01-08 11:25:41,087 iteration 5047 : loss : 0.028997, loss_ce: 0.007985
2022-01-08 11:25:42,437 iteration 5048 : loss : 0.018081, loss_ce: 0.006759
2022-01-08 11:25:43,787 iteration 5049 : loss : 0.013857, loss_ce: 0.005887
 74%|█████████████████████▌       | 297/400 [2:05:14<43:03, 25.08s/it]2022-01-08 11:25:45,204 iteration 5050 : loss : 0.018745, loss_ce: 0.005375
2022-01-08 11:25:46,549 iteration 5051 : loss : 0.021399, loss_ce: 0.007639
2022-01-08 11:25:47,908 iteration 5052 : loss : 0.020507, loss_ce: 0.008938
2022-01-08 11:25:49,248 iteration 5053 : loss : 0.016431, loss_ce: 0.007031
2022-01-08 11:25:50,616 iteration 5054 : loss : 0.015144, loss_ce: 0.004199
2022-01-08 11:25:51,973 iteration 5055 : loss : 0.020707, loss_ce: 0.006351
2022-01-08 11:25:53,324 iteration 5056 : loss : 0.032477, loss_ce: 0.008385
2022-01-08 11:25:54,744 iteration 5057 : loss : 0.016881, loss_ce: 0.008614
2022-01-08 11:25:56,180 iteration 5058 : loss : 0.027632, loss_ce: 0.010988
2022-01-08 11:25:57,434 iteration 5059 : loss : 0.017269, loss_ce: 0.004247
2022-01-08 11:25:58,818 iteration 5060 : loss : 0.022634, loss_ce: 0.005869
2022-01-08 11:26:00,226 iteration 5061 : loss : 0.018187, loss_ce: 0.007576
2022-01-08 11:26:01,618 iteration 5062 : loss : 0.022908, loss_ce: 0.006892
2022-01-08 11:26:03,053 iteration 5063 : loss : 0.024713, loss_ce: 0.009682
2022-01-08 11:26:04,376 iteration 5064 : loss : 0.018269, loss_ce: 0.006487
2022-01-08 11:26:05,715 iteration 5065 : loss : 0.019044, loss_ce: 0.006771
2022-01-08 11:26:07,091 iteration 5066 : loss : 0.015975, loss_ce: 0.008100
 74%|█████████████████████▌       | 298/400 [2:05:37<41:43, 24.55s/it]2022-01-08 11:26:08,586 iteration 5067 : loss : 0.019750, loss_ce: 0.007883
2022-01-08 11:26:09,977 iteration 5068 : loss : 0.019560, loss_ce: 0.008901
2022-01-08 11:26:11,377 iteration 5069 : loss : 0.032548, loss_ce: 0.009251
2022-01-08 11:26:12,737 iteration 5070 : loss : 0.014873, loss_ce: 0.004233
2022-01-08 11:26:14,164 iteration 5071 : loss : 0.024311, loss_ce: 0.008533
2022-01-08 11:26:15,426 iteration 5072 : loss : 0.015561, loss_ce: 0.005197
2022-01-08 11:26:16,802 iteration 5073 : loss : 0.018963, loss_ce: 0.008066
2022-01-08 11:26:18,213 iteration 5074 : loss : 0.016616, loss_ce: 0.005013
2022-01-08 11:26:19,579 iteration 5075 : loss : 0.023649, loss_ce: 0.009298
2022-01-08 11:26:21,002 iteration 5076 : loss : 0.022554, loss_ce: 0.009911
2022-01-08 11:26:22,333 iteration 5077 : loss : 0.016309, loss_ce: 0.005781
2022-01-08 11:26:23,719 iteration 5078 : loss : 0.012605, loss_ce: 0.003485
2022-01-08 11:26:25,074 iteration 5079 : loss : 0.014640, loss_ce: 0.006137
2022-01-08 11:26:26,456 iteration 5080 : loss : 0.017042, loss_ce: 0.005962
2022-01-08 11:26:27,864 iteration 5081 : loss : 0.017861, loss_ce: 0.007771
2022-01-08 11:26:29,298 iteration 5082 : loss : 0.028570, loss_ce: 0.013842
2022-01-08 11:26:30,687 iteration 5083 : loss : 0.023826, loss_ce: 0.007549
 75%|█████████████████████▋       | 299/400 [2:06:01<40:50, 24.26s/it]2022-01-08 11:26:32,076 iteration 5084 : loss : 0.015011, loss_ce: 0.005083
2022-01-08 11:26:33,433 iteration 5085 : loss : 0.026075, loss_ce: 0.012801
2022-01-08 11:26:34,764 iteration 5086 : loss : 0.013374, loss_ce: 0.005060
2022-01-08 11:26:36,192 iteration 5087 : loss : 0.021339, loss_ce: 0.008335
2022-01-08 11:26:37,585 iteration 5088 : loss : 0.015200, loss_ce: 0.005519
2022-01-08 11:26:38,911 iteration 5089 : loss : 0.014261, loss_ce: 0.004264
2022-01-08 11:26:40,263 iteration 5090 : loss : 0.019265, loss_ce: 0.007346
2022-01-08 11:26:41,731 iteration 5091 : loss : 0.022711, loss_ce: 0.009003
2022-01-08 11:26:43,106 iteration 5092 : loss : 0.013584, loss_ce: 0.006905
2022-01-08 11:26:44,451 iteration 5093 : loss : 0.016490, loss_ce: 0.007630
2022-01-08 11:26:45,772 iteration 5094 : loss : 0.013691, loss_ce: 0.004950
2022-01-08 11:26:47,181 iteration 5095 : loss : 0.024886, loss_ce: 0.008410
2022-01-08 11:26:48,494 iteration 5096 : loss : 0.017390, loss_ce: 0.005870
2022-01-08 11:26:49,834 iteration 5097 : loss : 0.024705, loss_ce: 0.012984
2022-01-08 11:26:51,226 iteration 5098 : loss : 0.032552, loss_ce: 0.011871
2022-01-08 11:26:52,594 iteration 5099 : loss : 0.014746, loss_ce: 0.004835
2022-01-08 11:26:52,595 Training Data Eval:
2022-01-08 11:26:59,487   Average segmentation loss on training set: 0.0102
2022-01-08 11:26:59,487 Validation Data Eval:
2022-01-08 11:27:01,856   Average segmentation loss on validation set: 0.0719
2022-01-08 11:27:03,241 iteration 5100 : loss : 0.013621, loss_ce: 0.006384
 75%|█████████████████████▊       | 300/400 [2:06:33<44:34, 26.75s/it]2022-01-08 11:27:04,674 iteration 5101 : loss : 0.016267, loss_ce: 0.006841
2022-01-08 11:27:06,017 iteration 5102 : loss : 0.014790, loss_ce: 0.006204
2022-01-08 11:27:07,380 iteration 5103 : loss : 0.016611, loss_ce: 0.007082
2022-01-08 11:27:08,812 iteration 5104 : loss : 0.022885, loss_ce: 0.010558
2022-01-08 11:27:10,124 iteration 5105 : loss : 0.017976, loss_ce: 0.006797
2022-01-08 11:27:11,562 iteration 5106 : loss : 0.015869, loss_ce: 0.006097
2022-01-08 11:27:12,882 iteration 5107 : loss : 0.015388, loss_ce: 0.006080
2022-01-08 11:27:14,281 iteration 5108 : loss : 0.022642, loss_ce: 0.007637
2022-01-08 11:27:15,653 iteration 5109 : loss : 0.041600, loss_ce: 0.012562
2022-01-08 11:27:17,014 iteration 5110 : loss : 0.034818, loss_ce: 0.022117
2022-01-08 11:27:18,408 iteration 5111 : loss : 0.022936, loss_ce: 0.008511
2022-01-08 11:27:19,732 iteration 5112 : loss : 0.020315, loss_ce: 0.007089
2022-01-08 11:27:21,159 iteration 5113 : loss : 0.055564, loss_ce: 0.010081
2022-01-08 11:27:22,513 iteration 5114 : loss : 0.023434, loss_ce: 0.010765
2022-01-08 11:27:23,841 iteration 5115 : loss : 0.017749, loss_ce: 0.005121
2022-01-08 11:27:25,176 iteration 5116 : loss : 0.018985, loss_ce: 0.007326
2022-01-08 11:27:26,594 iteration 5117 : loss : 0.021485, loss_ce: 0.010001
 75%|█████████████████████▊       | 301/400 [2:06:57<42:27, 25.73s/it]2022-01-08 11:27:28,004 iteration 5118 : loss : 0.015282, loss_ce: 0.005513
2022-01-08 11:27:29,387 iteration 5119 : loss : 0.028117, loss_ce: 0.008766
2022-01-08 11:27:30,720 iteration 5120 : loss : 0.043004, loss_ce: 0.010270
2022-01-08 11:27:32,086 iteration 5121 : loss : 0.026722, loss_ce: 0.012463
2022-01-08 11:27:33,428 iteration 5122 : loss : 0.023167, loss_ce: 0.008540
2022-01-08 11:27:34,785 iteration 5123 : loss : 0.021337, loss_ce: 0.006873
2022-01-08 11:27:36,169 iteration 5124 : loss : 0.021554, loss_ce: 0.010389
2022-01-08 11:27:37,445 iteration 5125 : loss : 0.020842, loss_ce: 0.009996
2022-01-08 11:27:38,765 iteration 5126 : loss : 0.019452, loss_ce: 0.006213
2022-01-08 11:27:40,176 iteration 5127 : loss : 0.023861, loss_ce: 0.007975
2022-01-08 11:27:41,514 iteration 5128 : loss : 0.013896, loss_ce: 0.005733
2022-01-08 11:27:42,893 iteration 5129 : loss : 0.019207, loss_ce: 0.006066
2022-01-08 11:27:44,268 iteration 5130 : loss : 0.016698, loss_ce: 0.005226
2022-01-08 11:27:45,554 iteration 5131 : loss : 0.013030, loss_ce: 0.004445
2022-01-08 11:27:46,913 iteration 5132 : loss : 0.017822, loss_ce: 0.007479
2022-01-08 11:27:48,246 iteration 5133 : loss : 0.020781, loss_ce: 0.007333
2022-01-08 11:27:49,563 iteration 5134 : loss : 0.015258, loss_ce: 0.006411
 76%|█████████████████████▉       | 302/400 [2:07:20<40:40, 24.90s/it]2022-01-08 11:27:51,110 iteration 5135 : loss : 0.023081, loss_ce: 0.009682
2022-01-08 11:27:52,471 iteration 5136 : loss : 0.027569, loss_ce: 0.008445
2022-01-08 11:27:53,886 iteration 5137 : loss : 0.016878, loss_ce: 0.005440
2022-01-08 11:27:55,234 iteration 5138 : loss : 0.019834, loss_ce: 0.008434
2022-01-08 11:27:56,614 iteration 5139 : loss : 0.014889, loss_ce: 0.005123
2022-01-08 11:27:58,056 iteration 5140 : loss : 0.017132, loss_ce: 0.007447
2022-01-08 11:27:59,376 iteration 5141 : loss : 0.018280, loss_ce: 0.006957
2022-01-08 11:28:00,755 iteration 5142 : loss : 0.026134, loss_ce: 0.009063
2022-01-08 11:28:02,052 iteration 5143 : loss : 0.015737, loss_ce: 0.006352
2022-01-08 11:28:03,448 iteration 5144 : loss : 0.022985, loss_ce: 0.014710
2022-01-08 11:28:04,729 iteration 5145 : loss : 0.013627, loss_ce: 0.004472
2022-01-08 11:28:06,169 iteration 5146 : loss : 0.025067, loss_ce: 0.010806
2022-01-08 11:28:07,609 iteration 5147 : loss : 0.024756, loss_ce: 0.008733
2022-01-08 11:28:08,957 iteration 5148 : loss : 0.016373, loss_ce: 0.007140
2022-01-08 11:28:10,308 iteration 5149 : loss : 0.026094, loss_ce: 0.007980
2022-01-08 11:28:11,663 iteration 5150 : loss : 0.013683, loss_ce: 0.005396
2022-01-08 11:28:13,044 iteration 5151 : loss : 0.033007, loss_ce: 0.005992
 76%|█████████████████████▉       | 303/400 [2:07:43<39:34, 24.48s/it]2022-01-08 11:28:14,403 iteration 5152 : loss : 0.012678, loss_ce: 0.003177
2022-01-08 11:28:15,683 iteration 5153 : loss : 0.012077, loss_ce: 0.004620
2022-01-08 11:28:17,114 iteration 5154 : loss : 0.031124, loss_ce: 0.018873
2022-01-08 11:28:18,425 iteration 5155 : loss : 0.010978, loss_ce: 0.003911
2022-01-08 11:28:19,769 iteration 5156 : loss : 0.016003, loss_ce: 0.006165
2022-01-08 11:28:21,075 iteration 5157 : loss : 0.013330, loss_ce: 0.004346
2022-01-08 11:28:22,426 iteration 5158 : loss : 0.017209, loss_ce: 0.006005
2022-01-08 11:28:23,843 iteration 5159 : loss : 0.026879, loss_ce: 0.009177
2022-01-08 11:28:25,137 iteration 5160 : loss : 0.018741, loss_ce: 0.006558
2022-01-08 11:28:26,494 iteration 5161 : loss : 0.015858, loss_ce: 0.005865
2022-01-08 11:28:27,804 iteration 5162 : loss : 0.015488, loss_ce: 0.007094
2022-01-08 11:28:29,191 iteration 5163 : loss : 0.016917, loss_ce: 0.006846
2022-01-08 11:28:30,576 iteration 5164 : loss : 0.019312, loss_ce: 0.009722
2022-01-08 11:28:31,883 iteration 5165 : loss : 0.014610, loss_ce: 0.004659
2022-01-08 11:28:33,259 iteration 5166 : loss : 0.014860, loss_ce: 0.006770
2022-01-08 11:28:34,648 iteration 5167 : loss : 0.032889, loss_ce: 0.009380
2022-01-08 11:28:36,053 iteration 5168 : loss : 0.019596, loss_ce: 0.007011
 76%|██████████████████████       | 304/400 [2:08:06<38:27, 24.04s/it]2022-01-08 11:28:37,553 iteration 5169 : loss : 0.024804, loss_ce: 0.010076
2022-01-08 11:28:38,859 iteration 5170 : loss : 0.018348, loss_ce: 0.006357
2022-01-08 11:28:40,282 iteration 5171 : loss : 0.015625, loss_ce: 0.006333
2022-01-08 11:28:41,570 iteration 5172 : loss : 0.017453, loss_ce: 0.005930
2022-01-08 11:28:42,815 iteration 5173 : loss : 0.012049, loss_ce: 0.005303
2022-01-08 11:28:44,250 iteration 5174 : loss : 0.017393, loss_ce: 0.005895
2022-01-08 11:28:45,575 iteration 5175 : loss : 0.017290, loss_ce: 0.008045
2022-01-08 11:28:47,058 iteration 5176 : loss : 0.023580, loss_ce: 0.010725
2022-01-08 11:28:48,381 iteration 5177 : loss : 0.015622, loss_ce: 0.006342
2022-01-08 11:28:49,729 iteration 5178 : loss : 0.019101, loss_ce: 0.006739
2022-01-08 11:28:51,142 iteration 5179 : loss : 0.017923, loss_ce: 0.005412
2022-01-08 11:28:52,486 iteration 5180 : loss : 0.016504, loss_ce: 0.005522
2022-01-08 11:28:53,804 iteration 5181 : loss : 0.019508, loss_ce: 0.004019
2022-01-08 11:28:55,166 iteration 5182 : loss : 0.014972, loss_ce: 0.005410
2022-01-08 11:28:56,501 iteration 5183 : loss : 0.024221, loss_ce: 0.009704
2022-01-08 11:28:57,826 iteration 5184 : loss : 0.019273, loss_ce: 0.006967
2022-01-08 11:28:57,826 Training Data Eval:
2022-01-08 11:29:04,724   Average segmentation loss on training set: 0.0098
2022-01-08 11:29:04,725 Validation Data Eval:
2022-01-08 11:29:07,095   Average segmentation loss on validation set: 0.0776
2022-01-08 11:29:08,469 iteration 5185 : loss : 0.022353, loss_ce: 0.008418
 76%|██████████████████████       | 305/400 [2:08:39<42:02, 26.55s/it]2022-01-08 11:29:09,893 iteration 5186 : loss : 0.014413, loss_ce: 0.006282
2022-01-08 11:29:11,206 iteration 5187 : loss : 0.017506, loss_ce: 0.006775
2022-01-08 11:29:12,540 iteration 5188 : loss : 0.013663, loss_ce: 0.005394
2022-01-08 11:29:13,867 iteration 5189 : loss : 0.012711, loss_ce: 0.005602
2022-01-08 11:29:15,180 iteration 5190 : loss : 0.017914, loss_ce: 0.008639
2022-01-08 11:29:16,599 iteration 5191 : loss : 0.049973, loss_ce: 0.017747
2022-01-08 11:29:17,995 iteration 5192 : loss : 0.023379, loss_ce: 0.008266
2022-01-08 11:29:19,307 iteration 5193 : loss : 0.011185, loss_ce: 0.003773
2022-01-08 11:29:20,703 iteration 5194 : loss : 0.023168, loss_ce: 0.009253
2022-01-08 11:29:22,029 iteration 5195 : loss : 0.016281, loss_ce: 0.005707
2022-01-08 11:29:23,291 iteration 5196 : loss : 0.012294, loss_ce: 0.003020
2022-01-08 11:29:24,609 iteration 5197 : loss : 0.012276, loss_ce: 0.004090
2022-01-08 11:29:25,925 iteration 5198 : loss : 0.015728, loss_ce: 0.007321
2022-01-08 11:29:27,308 iteration 5199 : loss : 0.024965, loss_ce: 0.007871
2022-01-08 11:29:28,662 iteration 5200 : loss : 0.025556, loss_ce: 0.008452
2022-01-08 11:29:30,025 iteration 5201 : loss : 0.017415, loss_ce: 0.006591
2022-01-08 11:29:31,456 iteration 5202 : loss : 0.024957, loss_ce: 0.011052
 76%|██████████████████████▏      | 306/400 [2:09:02<39:55, 25.48s/it]2022-01-08 11:29:32,825 iteration 5203 : loss : 0.012505, loss_ce: 0.004455
2022-01-08 11:29:34,170 iteration 5204 : loss : 0.017095, loss_ce: 0.006970
2022-01-08 11:29:35,657 iteration 5205 : loss : 0.032406, loss_ce: 0.014155
2022-01-08 11:29:37,007 iteration 5206 : loss : 0.018053, loss_ce: 0.006811
2022-01-08 11:29:38,371 iteration 5207 : loss : 0.017079, loss_ce: 0.007178
2022-01-08 11:29:39,799 iteration 5208 : loss : 0.014601, loss_ce: 0.006141
2022-01-08 11:29:41,098 iteration 5209 : loss : 0.015555, loss_ce: 0.007489
2022-01-08 11:29:42,506 iteration 5210 : loss : 0.017085, loss_ce: 0.006528
2022-01-08 11:29:43,888 iteration 5211 : loss : 0.015978, loss_ce: 0.006486
2022-01-08 11:29:45,261 iteration 5212 : loss : 0.017263, loss_ce: 0.006608
2022-01-08 11:29:46,525 iteration 5213 : loss : 0.012105, loss_ce: 0.004616
2022-01-08 11:29:47,978 iteration 5214 : loss : 0.027409, loss_ce: 0.011686
2022-01-08 11:29:49,374 iteration 5215 : loss : 0.019893, loss_ce: 0.007538
2022-01-08 11:29:50,797 iteration 5216 : loss : 0.021566, loss_ce: 0.008330
2022-01-08 11:29:52,240 iteration 5217 : loss : 0.019132, loss_ce: 0.007087
2022-01-08 11:29:53,555 iteration 5218 : loss : 0.022146, loss_ce: 0.003680
2022-01-08 11:29:55,005 iteration 5219 : loss : 0.016240, loss_ce: 0.005479
 77%|██████████████████████▎      | 307/400 [2:09:25<38:35, 24.90s/it]2022-01-08 11:29:56,385 iteration 5220 : loss : 0.017105, loss_ce: 0.005471
2022-01-08 11:29:57,743 iteration 5221 : loss : 0.029254, loss_ce: 0.011518
2022-01-08 11:29:59,051 iteration 5222 : loss : 0.012089, loss_ce: 0.004703
2022-01-08 11:30:00,463 iteration 5223 : loss : 0.018900, loss_ce: 0.006929
2022-01-08 11:30:01,811 iteration 5224 : loss : 0.014278, loss_ce: 0.004877
2022-01-08 11:30:03,231 iteration 5225 : loss : 0.014993, loss_ce: 0.004955
2022-01-08 11:30:04,588 iteration 5226 : loss : 0.014685, loss_ce: 0.005955
2022-01-08 11:30:06,037 iteration 5227 : loss : 0.033675, loss_ce: 0.018231
2022-01-08 11:30:07,358 iteration 5228 : loss : 0.014593, loss_ce: 0.004367
2022-01-08 11:30:08,839 iteration 5229 : loss : 0.017090, loss_ce: 0.007026
2022-01-08 11:30:10,096 iteration 5230 : loss : 0.011651, loss_ce: 0.005072
2022-01-08 11:30:11,482 iteration 5231 : loss : 0.027216, loss_ce: 0.010633
2022-01-08 11:30:12,850 iteration 5232 : loss : 0.017748, loss_ce: 0.006493
2022-01-08 11:30:14,251 iteration 5233 : loss : 0.020679, loss_ce: 0.010230
2022-01-08 11:30:15,572 iteration 5234 : loss : 0.017686, loss_ce: 0.007330
2022-01-08 11:30:16,940 iteration 5235 : loss : 0.015565, loss_ce: 0.005452
2022-01-08 11:30:18,339 iteration 5236 : loss : 0.020620, loss_ce: 0.007323
 77%|██████████████████████▎      | 308/400 [2:09:49<37:27, 24.43s/it]2022-01-08 11:30:19,626 iteration 5237 : loss : 0.011970, loss_ce: 0.004923
2022-01-08 11:30:21,047 iteration 5238 : loss : 0.015151, loss_ce: 0.005981
2022-01-08 11:30:22,430 iteration 5239 : loss : 0.018176, loss_ce: 0.005655
2022-01-08 11:30:23,832 iteration 5240 : loss : 0.172139, loss_ce: 0.003441
2022-01-08 11:30:25,166 iteration 5241 : loss : 0.013333, loss_ce: 0.005214
2022-01-08 11:30:26,528 iteration 5242 : loss : 0.024205, loss_ce: 0.011341
2022-01-08 11:30:27,965 iteration 5243 : loss : 0.026793, loss_ce: 0.008382
2022-01-08 11:30:29,358 iteration 5244 : loss : 0.015152, loss_ce: 0.006629
2022-01-08 11:30:30,736 iteration 5245 : loss : 0.017258, loss_ce: 0.006352
2022-01-08 11:30:32,120 iteration 5246 : loss : 0.028155, loss_ce: 0.008690
2022-01-08 11:30:33,415 iteration 5247 : loss : 0.012333, loss_ce: 0.004896
2022-01-08 11:30:34,826 iteration 5248 : loss : 0.029162, loss_ce: 0.010944
2022-01-08 11:30:36,269 iteration 5249 : loss : 0.021527, loss_ce: 0.009175
2022-01-08 11:30:37,658 iteration 5250 : loss : 0.019167, loss_ce: 0.009370
2022-01-08 11:30:38,944 iteration 5251 : loss : 0.012848, loss_ce: 0.004456
2022-01-08 11:30:40,320 iteration 5252 : loss : 0.013351, loss_ce: 0.005148
2022-01-08 11:30:41,722 iteration 5253 : loss : 0.047750, loss_ce: 0.010714
 77%|██████████████████████▍      | 309/400 [2:10:12<36:34, 24.12s/it]2022-01-08 11:30:43,114 iteration 5254 : loss : 0.011035, loss_ce: 0.003643
2022-01-08 11:30:44,475 iteration 5255 : loss : 0.023708, loss_ce: 0.013608
2022-01-08 11:30:45,775 iteration 5256 : loss : 0.014525, loss_ce: 0.005844
2022-01-08 11:30:47,147 iteration 5257 : loss : 0.017318, loss_ce: 0.006642
2022-01-08 11:30:48,553 iteration 5258 : loss : 0.016615, loss_ce: 0.004611
2022-01-08 11:30:49,867 iteration 5259 : loss : 0.016492, loss_ce: 0.003935
2022-01-08 11:30:51,246 iteration 5260 : loss : 0.016963, loss_ce: 0.006254
2022-01-08 11:30:52,639 iteration 5261 : loss : 0.018546, loss_ce: 0.007651
2022-01-08 11:30:54,007 iteration 5262 : loss : 0.011924, loss_ce: 0.004857
2022-01-08 11:30:55,379 iteration 5263 : loss : 0.023823, loss_ce: 0.010352
2022-01-08 11:30:56,691 iteration 5264 : loss : 0.013610, loss_ce: 0.006044
2022-01-08 11:30:58,094 iteration 5265 : loss : 0.020812, loss_ce: 0.007394
2022-01-08 11:30:59,532 iteration 5266 : loss : 0.019229, loss_ce: 0.006859
2022-01-08 11:31:00,881 iteration 5267 : loss : 0.025757, loss_ce: 0.007453
2022-01-08 11:31:02,265 iteration 5268 : loss : 0.017829, loss_ce: 0.004695
2022-01-08 11:31:03,744 iteration 5269 : loss : 0.022748, loss_ce: 0.007285
2022-01-08 11:31:03,745 Training Data Eval:
2022-01-08 11:31:10,603   Average segmentation loss on training set: 0.0095
2022-01-08 11:31:10,603 Validation Data Eval:
2022-01-08 11:31:12,974   Average segmentation loss on validation set: 0.0756
2022-01-08 11:31:14,354 iteration 5270 : loss : 0.014772, loss_ce: 0.006627
 78%|██████████████████████▍      | 310/400 [2:10:45<40:00, 26.67s/it]2022-01-08 11:31:15,730 iteration 5271 : loss : 0.014076, loss_ce: 0.005526
2022-01-08 11:31:17,110 iteration 5272 : loss : 0.018195, loss_ce: 0.007766
2022-01-08 11:31:18,437 iteration 5273 : loss : 0.015357, loss_ce: 0.004365
2022-01-08 11:31:19,788 iteration 5274 : loss : 0.014740, loss_ce: 0.005535
2022-01-08 11:31:21,084 iteration 5275 : loss : 0.012402, loss_ce: 0.005591
2022-01-08 11:31:22,484 iteration 5276 : loss : 0.021808, loss_ce: 0.008770
2022-01-08 11:31:23,864 iteration 5277 : loss : 0.016000, loss_ce: 0.005611
2022-01-08 11:31:25,144 iteration 5278 : loss : 0.012395, loss_ce: 0.005041
2022-01-08 11:31:26,510 iteration 5279 : loss : 0.014507, loss_ce: 0.005939
2022-01-08 11:31:28,003 iteration 5280 : loss : 0.043967, loss_ce: 0.018814
2022-01-08 11:31:29,345 iteration 5281 : loss : 0.024467, loss_ce: 0.009300
2022-01-08 11:31:30,647 iteration 5282 : loss : 0.011985, loss_ce: 0.003904
2022-01-08 11:31:32,030 iteration 5283 : loss : 0.022357, loss_ce: 0.007236
2022-01-08 11:31:33,423 iteration 5284 : loss : 0.026035, loss_ce: 0.007790
2022-01-08 11:31:34,777 iteration 5285 : loss : 0.015588, loss_ce: 0.006424
2022-01-08 11:31:36,059 iteration 5286 : loss : 0.013562, loss_ce: 0.005809
2022-01-08 11:31:37,392 iteration 5287 : loss : 0.015380, loss_ce: 0.003238
 78%|██████████████████████▌      | 311/400 [2:11:08<37:56, 25.58s/it]2022-01-08 11:31:38,815 iteration 5288 : loss : 0.021562, loss_ce: 0.008769
2022-01-08 11:31:40,204 iteration 5289 : loss : 0.033652, loss_ce: 0.008311
2022-01-08 11:31:41,559 iteration 5290 : loss : 0.018814, loss_ce: 0.004603
2022-01-08 11:31:42,900 iteration 5291 : loss : 0.019080, loss_ce: 0.007012
2022-01-08 11:31:44,300 iteration 5292 : loss : 0.017764, loss_ce: 0.006657
2022-01-08 11:31:45,679 iteration 5293 : loss : 0.023718, loss_ce: 0.010645
2022-01-08 11:31:47,092 iteration 5294 : loss : 0.021405, loss_ce: 0.007711
2022-01-08 11:31:48,546 iteration 5295 : loss : 0.050262, loss_ce: 0.017519
2022-01-08 11:31:49,939 iteration 5296 : loss : 0.022906, loss_ce: 0.005486
2022-01-08 11:31:51,343 iteration 5297 : loss : 0.013346, loss_ce: 0.003858
2022-01-08 11:31:52,745 iteration 5298 : loss : 0.021484, loss_ce: 0.008682
2022-01-08 11:31:54,137 iteration 5299 : loss : 0.015499, loss_ce: 0.005446
2022-01-08 11:31:55,556 iteration 5300 : loss : 0.016333, loss_ce: 0.005856
2022-01-08 11:31:56,864 iteration 5301 : loss : 0.016512, loss_ce: 0.006216
2022-01-08 11:31:58,175 iteration 5302 : loss : 0.013640, loss_ce: 0.006667
2022-01-08 11:31:59,500 iteration 5303 : loss : 0.014628, loss_ce: 0.005595
2022-01-08 11:32:00,800 iteration 5304 : loss : 0.017633, loss_ce: 0.006834
 78%|██████████████████████▌      | 312/400 [2:11:31<36:33, 24.93s/it]2022-01-08 11:32:02,235 iteration 5305 : loss : 0.021116, loss_ce: 0.007293
2022-01-08 11:32:03,702 iteration 5306 : loss : 0.037422, loss_ce: 0.018562
2022-01-08 11:32:05,162 iteration 5307 : loss : 0.022191, loss_ce: 0.006794
2022-01-08 11:32:06,593 iteration 5308 : loss : 0.025450, loss_ce: 0.008106
2022-01-08 11:32:08,024 iteration 5309 : loss : 0.026797, loss_ce: 0.008302
2022-01-08 11:32:09,352 iteration 5310 : loss : 0.022685, loss_ce: 0.010791
2022-01-08 11:32:10,675 iteration 5311 : loss : 0.031540, loss_ce: 0.007918
2022-01-08 11:32:12,112 iteration 5312 : loss : 0.021573, loss_ce: 0.010527
2022-01-08 11:32:13,581 iteration 5313 : loss : 0.025332, loss_ce: 0.007619
2022-01-08 11:32:14,955 iteration 5314 : loss : 0.018444, loss_ce: 0.007322
2022-01-08 11:32:16,386 iteration 5315 : loss : 0.033209, loss_ce: 0.008395
2022-01-08 11:32:17,801 iteration 5316 : loss : 0.019874, loss_ce: 0.007996
2022-01-08 11:32:19,161 iteration 5317 : loss : 0.017360, loss_ce: 0.007222
2022-01-08 11:32:20,496 iteration 5318 : loss : 0.013353, loss_ce: 0.005106
2022-01-08 11:32:21,918 iteration 5319 : loss : 0.024323, loss_ce: 0.009713
2022-01-08 11:32:23,320 iteration 5320 : loss : 0.025019, loss_ce: 0.012330
2022-01-08 11:32:24,749 iteration 5321 : loss : 0.041254, loss_ce: 0.018033
 78%|██████████████████████▋      | 313/400 [2:11:55<35:43, 24.63s/it]2022-01-08 11:32:26,084 iteration 5322 : loss : 0.012221, loss_ce: 0.004923
2022-01-08 11:32:27,469 iteration 5323 : loss : 0.025740, loss_ce: 0.010407
2022-01-08 11:32:28,781 iteration 5324 : loss : 0.019511, loss_ce: 0.005935
2022-01-08 11:32:30,121 iteration 5325 : loss : 0.029533, loss_ce: 0.006161
2022-01-08 11:32:31,523 iteration 5326 : loss : 0.024265, loss_ce: 0.008339
2022-01-08 11:32:32,852 iteration 5327 : loss : 0.014785, loss_ce: 0.005500
2022-01-08 11:32:34,201 iteration 5328 : loss : 0.022508, loss_ce: 0.012923
2022-01-08 11:32:35,612 iteration 5329 : loss : 0.038163, loss_ce: 0.013099
2022-01-08 11:32:36,997 iteration 5330 : loss : 0.024468, loss_ce: 0.011581
2022-01-08 11:32:38,352 iteration 5331 : loss : 0.022714, loss_ce: 0.006586
2022-01-08 11:32:39,733 iteration 5332 : loss : 0.036935, loss_ce: 0.010609
2022-01-08 11:32:41,148 iteration 5333 : loss : 0.015052, loss_ce: 0.006896
2022-01-08 11:32:42,556 iteration 5334 : loss : 0.025531, loss_ce: 0.009872
2022-01-08 11:32:43,977 iteration 5335 : loss : 0.022749, loss_ce: 0.007362
2022-01-08 11:32:45,355 iteration 5336 : loss : 0.016474, loss_ce: 0.006590
2022-01-08 11:32:46,685 iteration 5337 : loss : 0.019679, loss_ce: 0.007889
2022-01-08 11:32:48,022 iteration 5338 : loss : 0.021081, loss_ce: 0.009729
 78%|██████████████████████▊      | 314/400 [2:12:18<34:43, 24.22s/it]2022-01-08 11:32:49,580 iteration 5339 : loss : 0.022504, loss_ce: 0.008614
2022-01-08 11:32:50,996 iteration 5340 : loss : 0.023283, loss_ce: 0.012007
2022-01-08 11:32:52,373 iteration 5341 : loss : 0.022845, loss_ce: 0.007039
2022-01-08 11:32:53,748 iteration 5342 : loss : 0.017418, loss_ce: 0.009203
2022-01-08 11:32:55,092 iteration 5343 : loss : 0.014540, loss_ce: 0.004605
2022-01-08 11:32:56,482 iteration 5344 : loss : 0.019600, loss_ce: 0.007157
2022-01-08 11:32:57,848 iteration 5345 : loss : 0.021948, loss_ce: 0.007714
2022-01-08 11:32:59,262 iteration 5346 : loss : 0.028155, loss_ce: 0.008832
2022-01-08 11:33:00,670 iteration 5347 : loss : 0.015587, loss_ce: 0.004693
2022-01-08 11:33:02,089 iteration 5348 : loss : 0.016357, loss_ce: 0.004318
2022-01-08 11:33:03,466 iteration 5349 : loss : 0.016150, loss_ce: 0.006412
2022-01-08 11:33:04,769 iteration 5350 : loss : 0.012526, loss_ce: 0.004679
2022-01-08 11:33:06,083 iteration 5351 : loss : 0.019858, loss_ce: 0.008143
2022-01-08 11:33:07,484 iteration 5352 : loss : 0.030126, loss_ce: 0.013802
2022-01-08 11:33:08,902 iteration 5353 : loss : 0.014959, loss_ce: 0.005098
2022-01-08 11:33:10,244 iteration 5354 : loss : 0.014780, loss_ce: 0.007644
2022-01-08 11:33:10,244 Training Data Eval:
2022-01-08 11:33:17,144   Average segmentation loss on training set: 0.0105
2022-01-08 11:33:17,145 Validation Data Eval:
2022-01-08 11:33:19,525   Average segmentation loss on validation set: 0.0738
2022-01-08 11:33:20,846 iteration 5355 : loss : 0.016613, loss_ce: 0.006443
 79%|██████████████████████▊      | 315/400 [2:12:51<37:58, 26.81s/it]2022-01-08 11:33:22,265 iteration 5356 : loss : 0.016467, loss_ce: 0.007572
2022-01-08 11:33:23,570 iteration 5357 : loss : 0.021705, loss_ce: 0.004772
2022-01-08 11:33:25,081 iteration 5358 : loss : 0.034248, loss_ce: 0.016606
2022-01-08 11:33:26,378 iteration 5359 : loss : 0.012165, loss_ce: 0.004573
2022-01-08 11:33:27,772 iteration 5360 : loss : 0.019085, loss_ce: 0.008243
2022-01-08 11:33:29,166 iteration 5361 : loss : 0.026639, loss_ce: 0.009702
2022-01-08 11:33:30,579 iteration 5362 : loss : 0.019389, loss_ce: 0.005615
2022-01-08 11:33:31,951 iteration 5363 : loss : 0.022091, loss_ce: 0.010321
2022-01-08 11:33:33,316 iteration 5364 : loss : 0.025714, loss_ce: 0.005291
2022-01-08 11:33:34,673 iteration 5365 : loss : 0.019642, loss_ce: 0.008135
2022-01-08 11:33:36,065 iteration 5366 : loss : 0.018292, loss_ce: 0.006789
2022-01-08 11:33:37,483 iteration 5367 : loss : 0.013686, loss_ce: 0.006023
2022-01-08 11:33:38,835 iteration 5368 : loss : 0.013389, loss_ce: 0.006127
2022-01-08 11:33:40,219 iteration 5369 : loss : 0.014648, loss_ce: 0.005546
2022-01-08 11:33:41,580 iteration 5370 : loss : 0.012432, loss_ce: 0.004873
2022-01-08 11:33:42,966 iteration 5371 : loss : 0.019367, loss_ce: 0.008815
2022-01-08 11:33:44,316 iteration 5372 : loss : 0.020315, loss_ce: 0.006160
 79%|██████████████████████▉      | 316/400 [2:13:14<36:07, 25.81s/it]2022-01-08 11:33:45,736 iteration 5373 : loss : 0.016037, loss_ce: 0.007400
2022-01-08 11:33:47,074 iteration 5374 : loss : 0.016208, loss_ce: 0.006650
2022-01-08 11:33:48,476 iteration 5375 : loss : 0.014122, loss_ce: 0.004848
2022-01-08 11:33:49,825 iteration 5376 : loss : 0.014129, loss_ce: 0.004972
2022-01-08 11:33:51,161 iteration 5377 : loss : 0.011864, loss_ce: 0.003862
2022-01-08 11:33:52,538 iteration 5378 : loss : 0.019241, loss_ce: 0.007012
2022-01-08 11:33:53,842 iteration 5379 : loss : 0.011335, loss_ce: 0.004037
2022-01-08 11:33:55,221 iteration 5380 : loss : 0.015813, loss_ce: 0.006736
2022-01-08 11:33:56,565 iteration 5381 : loss : 0.014394, loss_ce: 0.006021
2022-01-08 11:33:57,957 iteration 5382 : loss : 0.013961, loss_ce: 0.004083
2022-01-08 11:33:59,409 iteration 5383 : loss : 0.023043, loss_ce: 0.011363
2022-01-08 11:34:00,916 iteration 5384 : loss : 0.023434, loss_ce: 0.007635
2022-01-08 11:34:02,247 iteration 5385 : loss : 0.016215, loss_ce: 0.006465
2022-01-08 11:34:03,543 iteration 5386 : loss : 0.012804, loss_ce: 0.004206
2022-01-08 11:34:04,890 iteration 5387 : loss : 0.017879, loss_ce: 0.007591
2022-01-08 11:34:06,314 iteration 5388 : loss : 0.016135, loss_ce: 0.005382
2022-01-08 11:34:07,792 iteration 5389 : loss : 0.028662, loss_ce: 0.010966
 79%|██████████████████████▉      | 317/400 [2:13:38<34:43, 25.11s/it]2022-01-08 11:34:09,175 iteration 5390 : loss : 0.022218, loss_ce: 0.005900
2022-01-08 11:34:10,583 iteration 5391 : loss : 0.017564, loss_ce: 0.005898
2022-01-08 11:34:12,004 iteration 5392 : loss : 0.017252, loss_ce: 0.008262
2022-01-08 11:34:13,381 iteration 5393 : loss : 0.016244, loss_ce: 0.005850
2022-01-08 11:34:14,719 iteration 5394 : loss : 0.018307, loss_ce: 0.008713
2022-01-08 11:34:16,119 iteration 5395 : loss : 0.019642, loss_ce: 0.005890
2022-01-08 11:34:17,452 iteration 5396 : loss : 0.016843, loss_ce: 0.009917
2022-01-08 11:34:18,887 iteration 5397 : loss : 0.021572, loss_ce: 0.007772
2022-01-08 11:34:20,265 iteration 5398 : loss : 0.013506, loss_ce: 0.004955
2022-01-08 11:34:21,571 iteration 5399 : loss : 0.021353, loss_ce: 0.008059
2022-01-08 11:34:23,038 iteration 5400 : loss : 0.030746, loss_ce: 0.010689
2022-01-08 11:34:24,445 iteration 5401 : loss : 0.016945, loss_ce: 0.007259
2022-01-08 11:34:25,822 iteration 5402 : loss : 0.016992, loss_ce: 0.006142
2022-01-08 11:34:27,164 iteration 5403 : loss : 0.016533, loss_ce: 0.006635
2022-01-08 11:34:28,498 iteration 5404 : loss : 0.014754, loss_ce: 0.004106
2022-01-08 11:34:29,836 iteration 5405 : loss : 0.015653, loss_ce: 0.006454
2022-01-08 11:34:31,254 iteration 5406 : loss : 0.017491, loss_ce: 0.004903
 80%|███████████████████████      | 318/400 [2:14:01<33:38, 24.61s/it]2022-01-08 11:34:32,692 iteration 5407 : loss : 0.021592, loss_ce: 0.007403
2022-01-08 11:34:34,068 iteration 5408 : loss : 0.034535, loss_ce: 0.012766
2022-01-08 11:34:35,523 iteration 5409 : loss : 0.016077, loss_ce: 0.006523
2022-01-08 11:34:36,857 iteration 5410 : loss : 0.016253, loss_ce: 0.007599
2022-01-08 11:34:38,209 iteration 5411 : loss : 0.016187, loss_ce: 0.006333
2022-01-08 11:34:39,588 iteration 5412 : loss : 0.017337, loss_ce: 0.007451
2022-01-08 11:34:40,935 iteration 5413 : loss : 0.018998, loss_ce: 0.009158
2022-01-08 11:34:42,324 iteration 5414 : loss : 0.018154, loss_ce: 0.007129
2022-01-08 11:34:43,773 iteration 5415 : loss : 0.035323, loss_ce: 0.015888
2022-01-08 11:34:45,082 iteration 5416 : loss : 0.013386, loss_ce: 0.004190
2022-01-08 11:34:46,480 iteration 5417 : loss : 0.013646, loss_ce: 0.004566
2022-01-08 11:34:47,825 iteration 5418 : loss : 0.014027, loss_ce: 0.005539
2022-01-08 11:34:49,231 iteration 5419 : loss : 0.017591, loss_ce: 0.004691
2022-01-08 11:34:50,664 iteration 5420 : loss : 0.019317, loss_ce: 0.007433
2022-01-08 11:34:52,063 iteration 5421 : loss : 0.018489, loss_ce: 0.007986
2022-01-08 11:34:53,324 iteration 5422 : loss : 0.011393, loss_ce: 0.005133
2022-01-08 11:34:54,735 iteration 5423 : loss : 0.028731, loss_ce: 0.004888
 80%|███████████████████████▏     | 319/400 [2:14:25<32:46, 24.27s/it]2022-01-08 11:34:56,207 iteration 5424 : loss : 0.018516, loss_ce: 0.006382
2022-01-08 11:34:57,539 iteration 5425 : loss : 0.011326, loss_ce: 0.003409
2022-01-08 11:34:58,905 iteration 5426 : loss : 0.016249, loss_ce: 0.004139
2022-01-08 11:35:00,194 iteration 5427 : loss : 0.016525, loss_ce: 0.007074
2022-01-08 11:35:01,548 iteration 5428 : loss : 0.019305, loss_ce: 0.009302
2022-01-08 11:35:02,949 iteration 5429 : loss : 0.014508, loss_ce: 0.005898
2022-01-08 11:35:04,319 iteration 5430 : loss : 0.016977, loss_ce: 0.007362
2022-01-08 11:35:05,623 iteration 5431 : loss : 0.024007, loss_ce: 0.007687
2022-01-08 11:35:07,083 iteration 5432 : loss : 0.019741, loss_ce: 0.009723
2022-01-08 11:35:08,520 iteration 5433 : loss : 0.016696, loss_ce: 0.006496
2022-01-08 11:35:09,842 iteration 5434 : loss : 0.016061, loss_ce: 0.006876
2022-01-08 11:35:11,275 iteration 5435 : loss : 0.018763, loss_ce: 0.007138
2022-01-08 11:35:12,661 iteration 5436 : loss : 0.014303, loss_ce: 0.005235
2022-01-08 11:35:14,062 iteration 5437 : loss : 0.030792, loss_ce: 0.007448
2022-01-08 11:35:15,484 iteration 5438 : loss : 0.018622, loss_ce: 0.008606
2022-01-08 11:35:16,845 iteration 5439 : loss : 0.014605, loss_ce: 0.006193
2022-01-08 11:35:16,846 Training Data Eval:
2022-01-08 11:35:23,740   Average segmentation loss on training set: 0.0093
2022-01-08 11:35:23,740 Validation Data Eval:
2022-01-08 11:35:26,114   Average segmentation loss on validation set: 0.0736
2022-01-08 11:35:27,445 iteration 5440 : loss : 0.017514, loss_ce: 0.004953
 80%|███████████████████████▏     | 320/400 [2:14:58<35:44, 26.80s/it]2022-01-08 11:35:28,832 iteration 5441 : loss : 0.012617, loss_ce: 0.004204
2022-01-08 11:35:30,149 iteration 5442 : loss : 0.017703, loss_ce: 0.006283
2022-01-08 11:35:31,515 iteration 5443 : loss : 0.017983, loss_ce: 0.005720
2022-01-08 11:35:32,818 iteration 5444 : loss : 0.015452, loss_ce: 0.004909
2022-01-08 11:35:34,206 iteration 5445 : loss : 0.017591, loss_ce: 0.006699
2022-01-08 11:35:35,545 iteration 5446 : loss : 0.016166, loss_ce: 0.007648
2022-01-08 11:35:36,998 iteration 5447 : loss : 0.022622, loss_ce: 0.008017
2022-01-08 11:35:38,322 iteration 5448 : loss : 0.018538, loss_ce: 0.008206
2022-01-08 11:35:39,704 iteration 5449 : loss : 0.018869, loss_ce: 0.009786
2022-01-08 11:35:41,059 iteration 5450 : loss : 0.018503, loss_ce: 0.007613
2022-01-08 11:35:42,373 iteration 5451 : loss : 0.014218, loss_ce: 0.005080
2022-01-08 11:35:43,760 iteration 5452 : loss : 0.020155, loss_ce: 0.006847
2022-01-08 11:35:45,089 iteration 5453 : loss : 0.015453, loss_ce: 0.005786
2022-01-08 11:35:46,513 iteration 5454 : loss : 0.016216, loss_ce: 0.007965
2022-01-08 11:35:47,897 iteration 5455 : loss : 0.021126, loss_ce: 0.006783
2022-01-08 11:35:49,270 iteration 5456 : loss : 0.012789, loss_ce: 0.002809
2022-01-08 11:35:50,597 iteration 5457 : loss : 0.013873, loss_ce: 0.004289
 80%|███████████████████████▎     | 321/400 [2:15:21<33:51, 25.71s/it]2022-01-08 11:35:51,990 iteration 5458 : loss : 0.018444, loss_ce: 0.005862
2022-01-08 11:35:53,302 iteration 5459 : loss : 0.014913, loss_ce: 0.006369
2022-01-08 11:35:54,667 iteration 5460 : loss : 0.011645, loss_ce: 0.004168
2022-01-08 11:35:56,036 iteration 5461 : loss : 0.019375, loss_ce: 0.007103
2022-01-08 11:35:57,403 iteration 5462 : loss : 0.016504, loss_ce: 0.005984
2022-01-08 11:35:58,792 iteration 5463 : loss : 0.024148, loss_ce: 0.006699
2022-01-08 11:36:00,165 iteration 5464 : loss : 0.022972, loss_ce: 0.010115
2022-01-08 11:36:01,590 iteration 5465 : loss : 0.022984, loss_ce: 0.008400
2022-01-08 11:36:03,010 iteration 5466 : loss : 0.016833, loss_ce: 0.005961
2022-01-08 11:36:04,412 iteration 5467 : loss : 0.018587, loss_ce: 0.007638
2022-01-08 11:36:05,805 iteration 5468 : loss : 0.013471, loss_ce: 0.005455
2022-01-08 11:36:07,104 iteration 5469 : loss : 0.012526, loss_ce: 0.004477
2022-01-08 11:36:08,452 iteration 5470 : loss : 0.009111, loss_ce: 0.003101
2022-01-08 11:36:09,794 iteration 5471 : loss : 0.016059, loss_ce: 0.004662
2022-01-08 11:36:11,184 iteration 5472 : loss : 0.021726, loss_ce: 0.006794
2022-01-08 11:36:12,499 iteration 5473 : loss : 0.012082, loss_ce: 0.004434
2022-01-08 11:36:13,831 iteration 5474 : loss : 0.013773, loss_ce: 0.004339
 80%|███████████████████████▎     | 322/400 [2:15:44<32:27, 24.97s/it]2022-01-08 11:36:15,257 iteration 5475 : loss : 0.016060, loss_ce: 0.004494
2022-01-08 11:36:16,677 iteration 5476 : loss : 0.015324, loss_ce: 0.006954
2022-01-08 11:36:18,064 iteration 5477 : loss : 0.045414, loss_ce: 0.006638
2022-01-08 11:36:19,366 iteration 5478 : loss : 0.013972, loss_ce: 0.004320
2022-01-08 11:36:20,744 iteration 5479 : loss : 0.013255, loss_ce: 0.006060
2022-01-08 11:36:22,097 iteration 5480 : loss : 0.019688, loss_ce: 0.007180
2022-01-08 11:36:23,438 iteration 5481 : loss : 0.016706, loss_ce: 0.005928
2022-01-08 11:36:24,898 iteration 5482 : loss : 0.037697, loss_ce: 0.007671
2022-01-08 11:36:26,179 iteration 5483 : loss : 0.012228, loss_ce: 0.005188
2022-01-08 11:36:27,557 iteration 5484 : loss : 0.023901, loss_ce: 0.008691
2022-01-08 11:36:28,921 iteration 5485 : loss : 0.013429, loss_ce: 0.005563
2022-01-08 11:36:30,284 iteration 5486 : loss : 0.018862, loss_ce: 0.009232
2022-01-08 11:36:31,667 iteration 5487 : loss : 0.016429, loss_ce: 0.005972
2022-01-08 11:36:33,007 iteration 5488 : loss : 0.016603, loss_ce: 0.007703
2022-01-08 11:36:34,333 iteration 5489 : loss : 0.013016, loss_ce: 0.006664
2022-01-08 11:36:35,768 iteration 5490 : loss : 0.017611, loss_ce: 0.007148
2022-01-08 11:36:37,097 iteration 5491 : loss : 0.022146, loss_ce: 0.008493
 81%|███████████████████████▍     | 323/400 [2:16:07<31:23, 24.46s/it]2022-01-08 11:36:38,423 iteration 5492 : loss : 0.013661, loss_ce: 0.004890
2022-01-08 11:36:39,817 iteration 5493 : loss : 0.025163, loss_ce: 0.007982
2022-01-08 11:36:41,138 iteration 5494 : loss : 0.017701, loss_ce: 0.006123
2022-01-08 11:36:42,511 iteration 5495 : loss : 0.012950, loss_ce: 0.003400
2022-01-08 11:36:43,898 iteration 5496 : loss : 0.015967, loss_ce: 0.004154
2022-01-08 11:36:45,280 iteration 5497 : loss : 0.014983, loss_ce: 0.005624
2022-01-08 11:36:46,697 iteration 5498 : loss : 0.021828, loss_ce: 0.007814
2022-01-08 11:36:48,092 iteration 5499 : loss : 0.018347, loss_ce: 0.010346
2022-01-08 11:36:49,399 iteration 5500 : loss : 0.013240, loss_ce: 0.005706
2022-01-08 11:36:50,783 iteration 5501 : loss : 0.016475, loss_ce: 0.006323
2022-01-08 11:36:52,119 iteration 5502 : loss : 0.010845, loss_ce: 0.004687
2022-01-08 11:36:53,480 iteration 5503 : loss : 0.015884, loss_ce: 0.004185
2022-01-08 11:36:54,811 iteration 5504 : loss : 0.014102, loss_ce: 0.005096
2022-01-08 11:36:56,173 iteration 5505 : loss : 0.018018, loss_ce: 0.006794
2022-01-08 11:36:57,573 iteration 5506 : loss : 0.015740, loss_ce: 0.006502
2022-01-08 11:36:58,931 iteration 5507 : loss : 0.017582, loss_ce: 0.007883
2022-01-08 11:37:00,304 iteration 5508 : loss : 0.015987, loss_ce: 0.005576
 81%|███████████████████████▍     | 324/400 [2:16:30<30:30, 24.08s/it]2022-01-08 11:37:01,713 iteration 5509 : loss : 0.019244, loss_ce: 0.006056
2022-01-08 11:37:03,163 iteration 5510 : loss : 0.020791, loss_ce: 0.007715
2022-01-08 11:37:04,509 iteration 5511 : loss : 0.012548, loss_ce: 0.004622
2022-01-08 11:37:05,926 iteration 5512 : loss : 0.024721, loss_ce: 0.009884
2022-01-08 11:37:07,272 iteration 5513 : loss : 0.013659, loss_ce: 0.005781
2022-01-08 11:37:08,579 iteration 5514 : loss : 0.009963, loss_ce: 0.003290
2022-01-08 11:37:10,007 iteration 5515 : loss : 0.039402, loss_ce: 0.010140
2022-01-08 11:37:11,352 iteration 5516 : loss : 0.014530, loss_ce: 0.005375
2022-01-08 11:37:12,725 iteration 5517 : loss : 0.021837, loss_ce: 0.009941
2022-01-08 11:37:14,093 iteration 5518 : loss : 0.019810, loss_ce: 0.006365
2022-01-08 11:37:15,478 iteration 5519 : loss : 0.016719, loss_ce: 0.006071
2022-01-08 11:37:16,847 iteration 5520 : loss : 0.013740, loss_ce: 0.005268
2022-01-08 11:37:18,226 iteration 5521 : loss : 0.022192, loss_ce: 0.007703
2022-01-08 11:37:19,596 iteration 5522 : loss : 0.027114, loss_ce: 0.010052
2022-01-08 11:37:20,938 iteration 5523 : loss : 0.010492, loss_ce: 0.003756
2022-01-08 11:37:22,282 iteration 5524 : loss : 0.016833, loss_ce: 0.007154
2022-01-08 11:37:22,283 Training Data Eval:
2022-01-08 11:37:29,177   Average segmentation loss on training set: 0.0095
2022-01-08 11:37:29,178 Validation Data Eval:
2022-01-08 11:37:31,553   Average segmentation loss on validation set: 0.0642
2022-01-08 11:37:32,924 iteration 5525 : loss : 0.016103, loss_ce: 0.006956
 81%|███████████████████████▌     | 325/400 [2:17:03<33:18, 26.64s/it]2022-01-08 11:37:34,387 iteration 5526 : loss : 0.016465, loss_ce: 0.005731
2022-01-08 11:37:35,761 iteration 5527 : loss : 0.020632, loss_ce: 0.010032
2022-01-08 11:37:37,134 iteration 5528 : loss : 0.017518, loss_ce: 0.005778
2022-01-08 11:37:38,527 iteration 5529 : loss : 0.022992, loss_ce: 0.009155
2022-01-08 11:37:39,841 iteration 5530 : loss : 0.014984, loss_ce: 0.007510
2022-01-08 11:37:41,185 iteration 5531 : loss : 0.022274, loss_ce: 0.009031
2022-01-08 11:37:42,520 iteration 5532 : loss : 0.011260, loss_ce: 0.004910
2022-01-08 11:37:43,943 iteration 5533 : loss : 0.044483, loss_ce: 0.016916
2022-01-08 11:37:45,306 iteration 5534 : loss : 0.013538, loss_ce: 0.004801
2022-01-08 11:37:46,740 iteration 5535 : loss : 0.021700, loss_ce: 0.008648
2022-01-08 11:37:48,197 iteration 5536 : loss : 0.012837, loss_ce: 0.005270
2022-01-08 11:37:49,591 iteration 5537 : loss : 0.023726, loss_ce: 0.008392
2022-01-08 11:37:50,971 iteration 5538 : loss : 0.017999, loss_ce: 0.007524
2022-01-08 11:37:52,433 iteration 5539 : loss : 0.018075, loss_ce: 0.003971
2022-01-08 11:37:53,774 iteration 5540 : loss : 0.012384, loss_ce: 0.005555
2022-01-08 11:37:55,154 iteration 5541 : loss : 0.017126, loss_ce: 0.005970
2022-01-08 11:37:56,501 iteration 5542 : loss : 0.012639, loss_ce: 0.004562
 82%|███████████████████████▋     | 326/400 [2:17:27<31:43, 25.72s/it]2022-01-08 11:37:57,935 iteration 5543 : loss : 0.020649, loss_ce: 0.007257
2022-01-08 11:37:59,302 iteration 5544 : loss : 0.020097, loss_ce: 0.010574
2022-01-08 11:38:00,646 iteration 5545 : loss : 0.016679, loss_ce: 0.006324
2022-01-08 11:38:01,976 iteration 5546 : loss : 0.019586, loss_ce: 0.007692
2022-01-08 11:38:03,356 iteration 5547 : loss : 0.014965, loss_ce: 0.005556
2022-01-08 11:38:04,776 iteration 5548 : loss : 0.022747, loss_ce: 0.006403
2022-01-08 11:38:06,081 iteration 5549 : loss : 0.020199, loss_ce: 0.008162
2022-01-08 11:38:07,425 iteration 5550 : loss : 0.013050, loss_ce: 0.004766
2022-01-08 11:38:08,791 iteration 5551 : loss : 0.013842, loss_ce: 0.005943
2022-01-08 11:38:10,116 iteration 5552 : loss : 0.012521, loss_ce: 0.004826
2022-01-08 11:38:11,495 iteration 5553 : loss : 0.022648, loss_ce: 0.008331
2022-01-08 11:38:12,909 iteration 5554 : loss : 0.021126, loss_ce: 0.006006
2022-01-08 11:38:14,307 iteration 5555 : loss : 0.033632, loss_ce: 0.008617
2022-01-08 11:38:15,606 iteration 5556 : loss : 0.014025, loss_ce: 0.005448
2022-01-08 11:38:17,032 iteration 5557 : loss : 0.028504, loss_ce: 0.007784
2022-01-08 11:38:18,402 iteration 5558 : loss : 0.013248, loss_ce: 0.004305
2022-01-08 11:38:19,685 iteration 5559 : loss : 0.013871, loss_ce: 0.005962
 82%|███████████████████████▋     | 327/400 [2:17:50<30:22, 24.96s/it]2022-01-08 11:38:21,256 iteration 5560 : loss : 0.027555, loss_ce: 0.010075
2022-01-08 11:38:22,606 iteration 5561 : loss : 0.017057, loss_ce: 0.005759
2022-01-08 11:38:23,963 iteration 5562 : loss : 0.016762, loss_ce: 0.005346
2022-01-08 11:38:25,321 iteration 5563 : loss : 0.011830, loss_ce: 0.004991
2022-01-08 11:38:26,768 iteration 5564 : loss : 0.018313, loss_ce: 0.005381
2022-01-08 11:38:28,138 iteration 5565 : loss : 0.021251, loss_ce: 0.008390
2022-01-08 11:38:29,544 iteration 5566 : loss : 0.018142, loss_ce: 0.008837
2022-01-08 11:38:30,998 iteration 5567 : loss : 0.016367, loss_ce: 0.007376
2022-01-08 11:38:32,346 iteration 5568 : loss : 0.020014, loss_ce: 0.007463
2022-01-08 11:38:33,744 iteration 5569 : loss : 0.015409, loss_ce: 0.006557
2022-01-08 11:38:35,134 iteration 5570 : loss : 0.012042, loss_ce: 0.004255
2022-01-08 11:38:36,557 iteration 5571 : loss : 0.038210, loss_ce: 0.019749
2022-01-08 11:38:37,927 iteration 5572 : loss : 0.019261, loss_ce: 0.008353
2022-01-08 11:38:39,276 iteration 5573 : loss : 0.014259, loss_ce: 0.004562
2022-01-08 11:38:40,749 iteration 5574 : loss : 0.023155, loss_ce: 0.008431
2022-01-08 11:38:42,018 iteration 5575 : loss : 0.010586, loss_ce: 0.004576
2022-01-08 11:38:43,275 iteration 5576 : loss : 0.013599, loss_ce: 0.004807
 82%|███████████████████████▊     | 328/400 [2:18:13<29:27, 24.55s/it]2022-01-08 11:38:44,683 iteration 5577 : loss : 0.020531, loss_ce: 0.008072
2022-01-08 11:38:46,033 iteration 5578 : loss : 0.038007, loss_ce: 0.013879
2022-01-08 11:38:47,396 iteration 5579 : loss : 0.013350, loss_ce: 0.004316
2022-01-08 11:38:48,807 iteration 5580 : loss : 0.012236, loss_ce: 0.004575
2022-01-08 11:38:50,147 iteration 5581 : loss : 0.015427, loss_ce: 0.006135
2022-01-08 11:38:51,550 iteration 5582 : loss : 0.016848, loss_ce: 0.005399
2022-01-08 11:38:52,908 iteration 5583 : loss : 0.023087, loss_ce: 0.006280
2022-01-08 11:38:54,367 iteration 5584 : loss : 0.022561, loss_ce: 0.010288
2022-01-08 11:38:55,711 iteration 5585 : loss : 0.025388, loss_ce: 0.011339
2022-01-08 11:38:57,073 iteration 5586 : loss : 0.023487, loss_ce: 0.011477
2022-01-08 11:38:58,447 iteration 5587 : loss : 0.017669, loss_ce: 0.008528
2022-01-08 11:38:59,894 iteration 5588 : loss : 0.021359, loss_ce: 0.008437
2022-01-08 11:39:01,313 iteration 5589 : loss : 0.026346, loss_ce: 0.012865
2022-01-08 11:39:02,682 iteration 5590 : loss : 0.017364, loss_ce: 0.006744
2022-01-08 11:39:04,047 iteration 5591 : loss : 0.016199, loss_ce: 0.006815
2022-01-08 11:39:05,404 iteration 5592 : loss : 0.017809, loss_ce: 0.005918
2022-01-08 11:39:06,747 iteration 5593 : loss : 0.014362, loss_ce: 0.008590
 82%|███████████████████████▊     | 329/400 [2:18:37<28:40, 24.23s/it]2022-01-08 11:39:08,190 iteration 5594 : loss : 0.013179, loss_ce: 0.004950
2022-01-08 11:39:09,542 iteration 5595 : loss : 0.014288, loss_ce: 0.006767
2022-01-08 11:39:10,895 iteration 5596 : loss : 0.010222, loss_ce: 0.003156
2022-01-08 11:39:12,285 iteration 5597 : loss : 0.014294, loss_ce: 0.004466
2022-01-08 11:39:13,726 iteration 5598 : loss : 0.027811, loss_ce: 0.009738
2022-01-08 11:39:15,134 iteration 5599 : loss : 0.014330, loss_ce: 0.005069
2022-01-08 11:39:16,532 iteration 5600 : loss : 0.013811, loss_ce: 0.005333
2022-01-08 11:39:17,929 iteration 5601 : loss : 0.014280, loss_ce: 0.004250
2022-01-08 11:39:19,261 iteration 5602 : loss : 0.018695, loss_ce: 0.004590
2022-01-08 11:39:20,678 iteration 5603 : loss : 0.019205, loss_ce: 0.008519
2022-01-08 11:39:22,011 iteration 5604 : loss : 0.013528, loss_ce: 0.006145
2022-01-08 11:39:23,345 iteration 5605 : loss : 0.014869, loss_ce: 0.004613
2022-01-08 11:39:24,780 iteration 5606 : loss : 0.018953, loss_ce: 0.006314
2022-01-08 11:39:26,157 iteration 5607 : loss : 0.024998, loss_ce: 0.008010
2022-01-08 11:39:27,651 iteration 5608 : loss : 0.024280, loss_ce: 0.011732
2022-01-08 11:39:28,994 iteration 5609 : loss : 0.017013, loss_ce: 0.008828
2022-01-08 11:39:28,994 Training Data Eval:
2022-01-08 11:39:35,867   Average segmentation loss on training set: 0.0089
2022-01-08 11:39:35,867 Validation Data Eval:
2022-01-08 11:39:38,236   Average segmentation loss on validation set: 0.0677
2022-01-08 11:39:39,647 iteration 5610 : loss : 0.016734, loss_ce: 0.006372
 82%|███████████████████████▉     | 330/400 [2:19:10<31:17, 26.83s/it]2022-01-08 11:39:41,109 iteration 5611 : loss : 0.020575, loss_ce: 0.009557
2022-01-08 11:39:42,493 iteration 5612 : loss : 0.015099, loss_ce: 0.004984
2022-01-08 11:39:43,856 iteration 5613 : loss : 0.020042, loss_ce: 0.010153
2022-01-08 11:39:45,219 iteration 5614 : loss : 0.017115, loss_ce: 0.005881
2022-01-08 11:39:46,619 iteration 5615 : loss : 0.017914, loss_ce: 0.006521
2022-01-08 11:39:48,035 iteration 5616 : loss : 0.017634, loss_ce: 0.006451
2022-01-08 11:39:49,400 iteration 5617 : loss : 0.012621, loss_ce: 0.005056
2022-01-08 11:39:50,722 iteration 5618 : loss : 0.014546, loss_ce: 0.006438
2022-01-08 11:39:52,140 iteration 5619 : loss : 0.031572, loss_ce: 0.011588
2022-01-08 11:39:53,593 iteration 5620 : loss : 0.016027, loss_ce: 0.004728
2022-01-08 11:39:54,940 iteration 5621 : loss : 0.016960, loss_ce: 0.006638
2022-01-08 11:39:56,385 iteration 5622 : loss : 0.017911, loss_ce: 0.008036
2022-01-08 11:39:57,708 iteration 5623 : loss : 0.014840, loss_ce: 0.004114
2022-01-08 11:39:59,107 iteration 5624 : loss : 0.014360, loss_ce: 0.006555
2022-01-08 11:40:00,521 iteration 5625 : loss : 0.020532, loss_ce: 0.009949
2022-01-08 11:40:01,890 iteration 5626 : loss : 0.014018, loss_ce: 0.005525
2022-01-08 11:40:03,251 iteration 5627 : loss : 0.018322, loss_ce: 0.005744
 83%|███████████████████████▉     | 331/400 [2:19:33<29:44, 25.86s/it]2022-01-08 11:40:04,561 iteration 5628 : loss : 0.013156, loss_ce: 0.004808
2022-01-08 11:40:05,954 iteration 5629 : loss : 0.017303, loss_ce: 0.008440
2022-01-08 11:40:07,326 iteration 5630 : loss : 0.010605, loss_ce: 0.003639
2022-01-08 11:40:08,792 iteration 5631 : loss : 0.022270, loss_ce: 0.010480
2022-01-08 11:40:10,080 iteration 5632 : loss : 0.020354, loss_ce: 0.010252
2022-01-08 11:40:11,504 iteration 5633 : loss : 0.023063, loss_ce: 0.010002
2022-01-08 11:40:12,846 iteration 5634 : loss : 0.015573, loss_ce: 0.005860
2022-01-08 11:40:14,247 iteration 5635 : loss : 0.015141, loss_ce: 0.004105
2022-01-08 11:40:15,554 iteration 5636 : loss : 0.011571, loss_ce: 0.005182
2022-01-08 11:40:16,941 iteration 5637 : loss : 0.018968, loss_ce: 0.008533
2022-01-08 11:40:18,207 iteration 5638 : loss : 0.012814, loss_ce: 0.003789
2022-01-08 11:40:19,535 iteration 5639 : loss : 0.025958, loss_ce: 0.006717
2022-01-08 11:40:20,877 iteration 5640 : loss : 0.015784, loss_ce: 0.007850
2022-01-08 11:40:22,275 iteration 5641 : loss : 0.012885, loss_ce: 0.005757
2022-01-08 11:40:23,635 iteration 5642 : loss : 0.016859, loss_ce: 0.003201
2022-01-08 11:40:25,050 iteration 5643 : loss : 0.019795, loss_ce: 0.005456
2022-01-08 11:40:26,445 iteration 5644 : loss : 0.016755, loss_ce: 0.006243
 83%|████████████████████████     | 332/400 [2:19:57<28:24, 25.06s/it]2022-01-08 11:40:27,952 iteration 5645 : loss : 0.024063, loss_ce: 0.009479
2022-01-08 11:40:29,280 iteration 5646 : loss : 0.015908, loss_ce: 0.005214
2022-01-08 11:40:30,637 iteration 5647 : loss : 0.013442, loss_ce: 0.006240
2022-01-08 11:40:32,054 iteration 5648 : loss : 0.031456, loss_ce: 0.009819
2022-01-08 11:40:33,465 iteration 5649 : loss : 0.017169, loss_ce: 0.007846
2022-01-08 11:40:34,844 iteration 5650 : loss : 0.016891, loss_ce: 0.006473
2022-01-08 11:40:36,206 iteration 5651 : loss : 0.017300, loss_ce: 0.008121
2022-01-08 11:40:37,507 iteration 5652 : loss : 0.012020, loss_ce: 0.004818
2022-01-08 11:40:38,874 iteration 5653 : loss : 0.014272, loss_ce: 0.004717
2022-01-08 11:40:40,196 iteration 5654 : loss : 0.012891, loss_ce: 0.003503
2022-01-08 11:40:41,580 iteration 5655 : loss : 0.023004, loss_ce: 0.009243
2022-01-08 11:40:42,942 iteration 5656 : loss : 0.014452, loss_ce: 0.004986
2022-01-08 11:40:44,307 iteration 5657 : loss : 0.012926, loss_ce: 0.004975
2022-01-08 11:40:45,663 iteration 5658 : loss : 0.023233, loss_ce: 0.010352
2022-01-08 11:40:47,013 iteration 5659 : loss : 0.016217, loss_ce: 0.004563
2022-01-08 11:40:48,327 iteration 5660 : loss : 0.013451, loss_ce: 0.004981
2022-01-08 11:40:49,777 iteration 5661 : loss : 0.023507, loss_ce: 0.007849
 83%|████████████████████████▏    | 333/400 [2:20:20<27:24, 24.54s/it]2022-01-08 11:40:51,173 iteration 5662 : loss : 0.018677, loss_ce: 0.004730
2022-01-08 11:40:52,544 iteration 5663 : loss : 0.017804, loss_ce: 0.007314
2022-01-08 11:40:53,954 iteration 5664 : loss : 0.023712, loss_ce: 0.007462
2022-01-08 11:40:55,246 iteration 5665 : loss : 0.014954, loss_ce: 0.005381
2022-01-08 11:40:56,637 iteration 5666 : loss : 0.014279, loss_ce: 0.004602
2022-01-08 11:40:58,048 iteration 5667 : loss : 0.014751, loss_ce: 0.005512
2022-01-08 11:40:59,396 iteration 5668 : loss : 0.015976, loss_ce: 0.006872
2022-01-08 11:41:00,796 iteration 5669 : loss : 0.023462, loss_ce: 0.011304
2022-01-08 11:41:02,179 iteration 5670 : loss : 0.011705, loss_ce: 0.004128
2022-01-08 11:41:03,566 iteration 5671 : loss : 0.013185, loss_ce: 0.004843
2022-01-08 11:41:04,936 iteration 5672 : loss : 0.015920, loss_ce: 0.004065
2022-01-08 11:41:06,279 iteration 5673 : loss : 0.018075, loss_ce: 0.006710
2022-01-08 11:41:07,723 iteration 5674 : loss : 0.013008, loss_ce: 0.006291
2022-01-08 11:41:09,065 iteration 5675 : loss : 0.015822, loss_ce: 0.007286
2022-01-08 11:41:10,496 iteration 5676 : loss : 0.013939, loss_ce: 0.004926
2022-01-08 11:41:11,900 iteration 5677 : loss : 0.011116, loss_ce: 0.004993
2022-01-08 11:41:13,285 iteration 5678 : loss : 0.030103, loss_ce: 0.009932
 84%|████████████████████████▏    | 334/400 [2:20:43<26:39, 24.23s/it]2022-01-08 11:41:14,696 iteration 5679 : loss : 0.020006, loss_ce: 0.008624
2022-01-08 11:41:15,976 iteration 5680 : loss : 0.012944, loss_ce: 0.003620
2022-01-08 11:41:17,391 iteration 5681 : loss : 0.025567, loss_ce: 0.007216
2022-01-08 11:41:18,799 iteration 5682 : loss : 0.017496, loss_ce: 0.007785
2022-01-08 11:41:20,131 iteration 5683 : loss : 0.010984, loss_ce: 0.003579
2022-01-08 11:41:21,470 iteration 5684 : loss : 0.013513, loss_ce: 0.007193
2022-01-08 11:41:22,869 iteration 5685 : loss : 0.019997, loss_ce: 0.009936
2022-01-08 11:41:24,227 iteration 5686 : loss : 0.015805, loss_ce: 0.006137
2022-01-08 11:41:25,589 iteration 5687 : loss : 0.011203, loss_ce: 0.004663
2022-01-08 11:41:27,036 iteration 5688 : loss : 0.020966, loss_ce: 0.008252
2022-01-08 11:41:28,402 iteration 5689 : loss : 0.042752, loss_ce: 0.011319
2022-01-08 11:41:29,783 iteration 5690 : loss : 0.019550, loss_ce: 0.008693
2022-01-08 11:41:31,175 iteration 5691 : loss : 0.019131, loss_ce: 0.008520
2022-01-08 11:41:32,565 iteration 5692 : loss : 0.018318, loss_ce: 0.007410
2022-01-08 11:41:33,944 iteration 5693 : loss : 0.019187, loss_ce: 0.005354
2022-01-08 11:41:35,294 iteration 5694 : loss : 0.016633, loss_ce: 0.007458
2022-01-08 11:41:35,294 Training Data Eval:
2022-01-08 11:41:42,180   Average segmentation loss on training set: 0.0095
2022-01-08 11:41:42,181 Validation Data Eval:
2022-01-08 11:41:44,551   Average segmentation loss on validation set: 0.0770
2022-01-08 11:41:45,869 iteration 5695 : loss : 0.012010, loss_ce: 0.003983
 84%|████████████████████████▎    | 335/400 [2:21:16<28:57, 26.74s/it]2022-01-08 11:41:47,230 iteration 5696 : loss : 0.012254, loss_ce: 0.005913
2022-01-08 11:41:48,559 iteration 5697 : loss : 0.012791, loss_ce: 0.005664
2022-01-08 11:41:49,976 iteration 5698 : loss : 0.016018, loss_ce: 0.005329
2022-01-08 11:41:51,267 iteration 5699 : loss : 0.014101, loss_ce: 0.006318
2022-01-08 11:41:52,620 iteration 5700 : loss : 0.014232, loss_ce: 0.005052
2022-01-08 11:41:53,926 iteration 5701 : loss : 0.012726, loss_ce: 0.005665
2022-01-08 11:41:55,391 iteration 5702 : loss : 0.018668, loss_ce: 0.006629
2022-01-08 11:41:56,792 iteration 5703 : loss : 0.018489, loss_ce: 0.007627
2022-01-08 11:41:58,177 iteration 5704 : loss : 0.014040, loss_ce: 0.005442
2022-01-08 11:41:59,569 iteration 5705 : loss : 0.017903, loss_ce: 0.005870
2022-01-08 11:42:00,847 iteration 5706 : loss : 0.012164, loss_ce: 0.004655
2022-01-08 11:42:02,199 iteration 5707 : loss : 0.025507, loss_ce: 0.009870
2022-01-08 11:42:03,515 iteration 5708 : loss : 0.014152, loss_ce: 0.004740
2022-01-08 11:42:04,893 iteration 5709 : loss : 0.015028, loss_ce: 0.004639
2022-01-08 11:42:06,258 iteration 5710 : loss : 0.015668, loss_ce: 0.004440
2022-01-08 11:42:07,564 iteration 5711 : loss : 0.014018, loss_ce: 0.005040
2022-01-08 11:42:08,946 iteration 5712 : loss : 0.018391, loss_ce: 0.007626
 84%|████████████████████████▎    | 336/400 [2:21:39<27:20, 25.64s/it]2022-01-08 11:42:10,392 iteration 5713 : loss : 0.019329, loss_ce: 0.006464
2022-01-08 11:42:11,799 iteration 5714 : loss : 0.027762, loss_ce: 0.007889
2022-01-08 11:42:13,217 iteration 5715 : loss : 0.012221, loss_ce: 0.004962
2022-01-08 11:42:14,547 iteration 5716 : loss : 0.017363, loss_ce: 0.006685
2022-01-08 11:42:15,985 iteration 5717 : loss : 0.018670, loss_ce: 0.007538
2022-01-08 11:42:17,311 iteration 5718 : loss : 0.011963, loss_ce: 0.004141
2022-01-08 11:42:18,634 iteration 5719 : loss : 0.013083, loss_ce: 0.005358
2022-01-08 11:42:20,041 iteration 5720 : loss : 0.023972, loss_ce: 0.009178
2022-01-08 11:42:21,420 iteration 5721 : loss : 0.019787, loss_ce: 0.011381
2022-01-08 11:42:22,774 iteration 5722 : loss : 0.016604, loss_ce: 0.007766
2022-01-08 11:42:24,156 iteration 5723 : loss : 0.016957, loss_ce: 0.006486
2022-01-08 11:42:25,571 iteration 5724 : loss : 0.015053, loss_ce: 0.005166
2022-01-08 11:42:26,950 iteration 5725 : loss : 0.013195, loss_ce: 0.004816
2022-01-08 11:42:28,268 iteration 5726 : loss : 0.016875, loss_ce: 0.003804
2022-01-08 11:42:29,639 iteration 5727 : loss : 0.016877, loss_ce: 0.005107
2022-01-08 11:42:30,985 iteration 5728 : loss : 0.018683, loss_ce: 0.006603
2022-01-08 11:42:32,424 iteration 5729 : loss : 0.026168, loss_ce: 0.011856
 84%|████████████████████████▍    | 337/400 [2:22:03<26:14, 24.99s/it]2022-01-08 11:42:33,773 iteration 5730 : loss : 0.013998, loss_ce: 0.006578
2022-01-08 11:42:35,191 iteration 5731 : loss : 0.020512, loss_ce: 0.010367
2022-01-08 11:42:36,517 iteration 5732 : loss : 0.013902, loss_ce: 0.005543
2022-01-08 11:42:37,943 iteration 5733 : loss : 0.027201, loss_ce: 0.007698
2022-01-08 11:42:39,345 iteration 5734 : loss : 0.022775, loss_ce: 0.010576
2022-01-08 11:42:40,686 iteration 5735 : loss : 0.013585, loss_ce: 0.003785
2022-01-08 11:42:42,020 iteration 5736 : loss : 0.018569, loss_ce: 0.006922
2022-01-08 11:42:43,461 iteration 5737 : loss : 0.030297, loss_ce: 0.010461
2022-01-08 11:42:44,793 iteration 5738 : loss : 0.011711, loss_ce: 0.003830
2022-01-08 11:42:46,118 iteration 5739 : loss : 0.014425, loss_ce: 0.005630
2022-01-08 11:42:47,481 iteration 5740 : loss : 0.018239, loss_ce: 0.006912
2022-01-08 11:42:48,855 iteration 5741 : loss : 0.017762, loss_ce: 0.009201
2022-01-08 11:42:50,236 iteration 5742 : loss : 0.012546, loss_ce: 0.002557
2022-01-08 11:42:51,625 iteration 5743 : loss : 0.014714, loss_ce: 0.006691
2022-01-08 11:42:53,045 iteration 5744 : loss : 0.024223, loss_ce: 0.010819
2022-01-08 11:42:54,457 iteration 5745 : loss : 0.017691, loss_ce: 0.007046
2022-01-08 11:42:55,784 iteration 5746 : loss : 0.015746, loss_ce: 0.003928
 84%|████████████████████████▌    | 338/400 [2:22:26<25:19, 24.50s/it]2022-01-08 11:42:57,195 iteration 5747 : loss : 0.016733, loss_ce: 0.006283
2022-01-08 11:42:58,571 iteration 5748 : loss : 0.021197, loss_ce: 0.008391
2022-01-08 11:42:59,945 iteration 5749 : loss : 0.018046, loss_ce: 0.007595
2022-01-08 11:43:01,319 iteration 5750 : loss : 0.018027, loss_ce: 0.006304
2022-01-08 11:43:02,630 iteration 5751 : loss : 0.016127, loss_ce: 0.005743
2022-01-08 11:43:03,982 iteration 5752 : loss : 0.016658, loss_ce: 0.006141
2022-01-08 11:43:05,389 iteration 5753 : loss : 0.018700, loss_ce: 0.008493
2022-01-08 11:43:06,806 iteration 5754 : loss : 0.014860, loss_ce: 0.004560
2022-01-08 11:43:08,167 iteration 5755 : loss : 0.015403, loss_ce: 0.006115
2022-01-08 11:43:09,504 iteration 5756 : loss : 0.019921, loss_ce: 0.007511
2022-01-08 11:43:10,759 iteration 5757 : loss : 0.010844, loss_ce: 0.004264
2022-01-08 11:43:12,181 iteration 5758 : loss : 0.018273, loss_ce: 0.006773
2022-01-08 11:43:13,600 iteration 5759 : loss : 0.016020, loss_ce: 0.006871
2022-01-08 11:43:15,046 iteration 5760 : loss : 0.017104, loss_ce: 0.009568
2022-01-08 11:43:16,499 iteration 5761 : loss : 0.022217, loss_ce: 0.009118
2022-01-08 11:43:17,801 iteration 5762 : loss : 0.012180, loss_ce: 0.005301
2022-01-08 11:43:19,202 iteration 5763 : loss : 0.011666, loss_ce: 0.004625
 85%|████████████████████████▌    | 339/400 [2:22:49<24:34, 24.18s/it]2022-01-08 11:43:20,615 iteration 5764 : loss : 0.014185, loss_ce: 0.004998
2022-01-08 11:43:22,032 iteration 5765 : loss : 0.040774, loss_ce: 0.008946
2022-01-08 11:43:23,418 iteration 5766 : loss : 0.020216, loss_ce: 0.011690
2022-01-08 11:43:24,781 iteration 5767 : loss : 0.015909, loss_ce: 0.009173
2022-01-08 11:43:26,263 iteration 5768 : loss : 0.033720, loss_ce: 0.014935
2022-01-08 11:43:27,675 iteration 5769 : loss : 0.016936, loss_ce: 0.005335
2022-01-08 11:43:29,017 iteration 5770 : loss : 0.019383, loss_ce: 0.007856
2022-01-08 11:43:30,438 iteration 5771 : loss : 0.019009, loss_ce: 0.006814
2022-01-08 11:43:31,802 iteration 5772 : loss : 0.017866, loss_ce: 0.006810
2022-01-08 11:43:33,097 iteration 5773 : loss : 0.014034, loss_ce: 0.005057
2022-01-08 11:43:34,381 iteration 5774 : loss : 0.015391, loss_ce: 0.004587
2022-01-08 11:43:35,774 iteration 5775 : loss : 0.012714, loss_ce: 0.005202
2022-01-08 11:43:37,214 iteration 5776 : loss : 0.019592, loss_ce: 0.009687
2022-01-08 11:43:38,495 iteration 5777 : loss : 0.013373, loss_ce: 0.005727
2022-01-08 11:43:39,835 iteration 5778 : loss : 0.023061, loss_ce: 0.008162
2022-01-08 11:43:41,181 iteration 5779 : loss : 0.014299, loss_ce: 0.005012
2022-01-08 11:43:41,182 Training Data Eval:
2022-01-08 11:43:48,076   Average segmentation loss on training set: 0.0092
2022-01-08 11:43:48,076 Validation Data Eval:
2022-01-08 11:43:50,441   Average segmentation loss on validation set: 0.0653
2022-01-08 11:43:51,803 iteration 5780 : loss : 0.016209, loss_ce: 0.005452
 85%|████████████████████████▋    | 340/400 [2:23:22<26:42, 26.70s/it]2022-01-08 11:43:53,185 iteration 5781 : loss : 0.021649, loss_ce: 0.006785
2022-01-08 11:43:54,612 iteration 5782 : loss : 0.035797, loss_ce: 0.012202
2022-01-08 11:43:55,971 iteration 5783 : loss : 0.016380, loss_ce: 0.005704
2022-01-08 11:43:57,361 iteration 5784 : loss : 0.014990, loss_ce: 0.007846
2022-01-08 11:43:58,684 iteration 5785 : loss : 0.013156, loss_ce: 0.005405
2022-01-08 11:44:00,020 iteration 5786 : loss : 0.014608, loss_ce: 0.006321
2022-01-08 11:44:01,355 iteration 5787 : loss : 0.016523, loss_ce: 0.005906
2022-01-08 11:44:02,843 iteration 5788 : loss : 0.019070, loss_ce: 0.007450
2022-01-08 11:44:04,233 iteration 5789 : loss : 0.015707, loss_ce: 0.006340
2022-01-08 11:44:05,613 iteration 5790 : loss : 0.014559, loss_ce: 0.005182
2022-01-08 11:44:06,936 iteration 5791 : loss : 0.014341, loss_ce: 0.005163
2022-01-08 11:44:08,332 iteration 5792 : loss : 0.020657, loss_ce: 0.007968
2022-01-08 11:44:09,802 iteration 5793 : loss : 0.017356, loss_ce: 0.006548
2022-01-08 11:44:11,176 iteration 5794 : loss : 0.016852, loss_ce: 0.006073
2022-01-08 11:44:12,531 iteration 5795 : loss : 0.012412, loss_ce: 0.004297
2022-01-08 11:44:13,855 iteration 5796 : loss : 0.013960, loss_ce: 0.004014
2022-01-08 11:44:15,254 iteration 5797 : loss : 0.018230, loss_ce: 0.006825
 85%|████████████████████████▋    | 341/400 [2:23:45<25:17, 25.73s/it]2022-01-08 11:44:16,689 iteration 5798 : loss : 0.021439, loss_ce: 0.009574
2022-01-08 11:44:18,062 iteration 5799 : loss : 0.023127, loss_ce: 0.007987
2022-01-08 11:44:19,421 iteration 5800 : loss : 0.012432, loss_ce: 0.005303
2022-01-08 11:44:20,780 iteration 5801 : loss : 0.018240, loss_ce: 0.006638
2022-01-08 11:44:22,084 iteration 5802 : loss : 0.014231, loss_ce: 0.005804
2022-01-08 11:44:23,421 iteration 5803 : loss : 0.013192, loss_ce: 0.006252
2022-01-08 11:44:24,784 iteration 5804 : loss : 0.016957, loss_ce: 0.006391
2022-01-08 11:44:26,230 iteration 5805 : loss : 0.016398, loss_ce: 0.005900
2022-01-08 11:44:27,574 iteration 5806 : loss : 0.019385, loss_ce: 0.006272
2022-01-08 11:44:29,015 iteration 5807 : loss : 0.028981, loss_ce: 0.010735
2022-01-08 11:44:30,400 iteration 5808 : loss : 0.015879, loss_ce: 0.007469
2022-01-08 11:44:31,711 iteration 5809 : loss : 0.009648, loss_ce: 0.002500
2022-01-08 11:44:33,048 iteration 5810 : loss : 0.016834, loss_ce: 0.006514
2022-01-08 11:44:34,350 iteration 5811 : loss : 0.010646, loss_ce: 0.003148
2022-01-08 11:44:35,716 iteration 5812 : loss : 0.012422, loss_ce: 0.004232
2022-01-08 11:44:37,013 iteration 5813 : loss : 0.014269, loss_ce: 0.006487
2022-01-08 11:44:38,393 iteration 5814 : loss : 0.014618, loss_ce: 0.006075
 86%|████████████████████████▊    | 342/400 [2:24:09<24:07, 24.95s/it]2022-01-08 11:44:39,723 iteration 5815 : loss : 0.015669, loss_ce: 0.004476
2022-01-08 11:44:41,088 iteration 5816 : loss : 0.012676, loss_ce: 0.005772
2022-01-08 11:44:42,429 iteration 5817 : loss : 0.012637, loss_ce: 0.004699
2022-01-08 11:44:43,707 iteration 5818 : loss : 0.012802, loss_ce: 0.005217
2022-01-08 11:44:45,133 iteration 5819 : loss : 0.015973, loss_ce: 0.005963
2022-01-08 11:44:46,577 iteration 5820 : loss : 0.016360, loss_ce: 0.007416
2022-01-08 11:44:47,869 iteration 5821 : loss : 0.013849, loss_ce: 0.004391
2022-01-08 11:44:49,194 iteration 5822 : loss : 0.010380, loss_ce: 0.004644
2022-01-08 11:44:50,654 iteration 5823 : loss : 0.014494, loss_ce: 0.006497
2022-01-08 11:44:52,003 iteration 5824 : loss : 0.015359, loss_ce: 0.004885
2022-01-08 11:44:53,401 iteration 5825 : loss : 0.015986, loss_ce: 0.006260
2022-01-08 11:44:54,812 iteration 5826 : loss : 0.019048, loss_ce: 0.004556
2022-01-08 11:44:56,176 iteration 5827 : loss : 0.016648, loss_ce: 0.005022
2022-01-08 11:44:57,646 iteration 5828 : loss : 0.021653, loss_ce: 0.008120
2022-01-08 11:44:58,997 iteration 5829 : loss : 0.013141, loss_ce: 0.004946
2022-01-08 11:45:00,323 iteration 5830 : loss : 0.021079, loss_ce: 0.007786
2022-01-08 11:45:01,667 iteration 5831 : loss : 0.019199, loss_ce: 0.006033
 86%|████████████████████████▊    | 343/400 [2:24:32<23:13, 24.45s/it]2022-01-08 11:45:03,071 iteration 5832 : loss : 0.012569, loss_ce: 0.003669
2022-01-08 11:45:04,476 iteration 5833 : loss : 0.015705, loss_ce: 0.006603
2022-01-08 11:45:05,780 iteration 5834 : loss : 0.011573, loss_ce: 0.004056
2022-01-08 11:45:07,093 iteration 5835 : loss : 0.014663, loss_ce: 0.004595
2022-01-08 11:45:08,511 iteration 5836 : loss : 0.020940, loss_ce: 0.010618
2022-01-08 11:45:09,875 iteration 5837 : loss : 0.017918, loss_ce: 0.005264
2022-01-08 11:45:11,152 iteration 5838 : loss : 0.009524, loss_ce: 0.002759
2022-01-08 11:45:12,503 iteration 5839 : loss : 0.012671, loss_ce: 0.004221
2022-01-08 11:45:13,903 iteration 5840 : loss : 0.015808, loss_ce: 0.005819
2022-01-08 11:45:15,241 iteration 5841 : loss : 0.016365, loss_ce: 0.007926
2022-01-08 11:45:16,590 iteration 5842 : loss : 0.013868, loss_ce: 0.005572
2022-01-08 11:45:17,935 iteration 5843 : loss : 0.017700, loss_ce: 0.007745
2022-01-08 11:45:19,423 iteration 5844 : loss : 0.025795, loss_ce: 0.007278
2022-01-08 11:45:20,777 iteration 5845 : loss : 0.021287, loss_ce: 0.008881
2022-01-08 11:45:22,148 iteration 5846 : loss : 0.022386, loss_ce: 0.010429
2022-01-08 11:45:23,529 iteration 5847 : loss : 0.018355, loss_ce: 0.007436
2022-01-08 11:45:24,890 iteration 5848 : loss : 0.017181, loss_ce: 0.006143
 86%|████████████████████████▉    | 344/400 [2:24:55<22:28, 24.08s/it]2022-01-08 11:45:26,291 iteration 5849 : loss : 0.017357, loss_ce: 0.006784
2022-01-08 11:45:27,596 iteration 5850 : loss : 0.010992, loss_ce: 0.004392
2022-01-08 11:45:28,958 iteration 5851 : loss : 0.017873, loss_ce: 0.008403
2022-01-08 11:45:30,297 iteration 5852 : loss : 0.013554, loss_ce: 0.004942
2022-01-08 11:45:31,589 iteration 5853 : loss : 0.013538, loss_ce: 0.004642
2022-01-08 11:45:32,939 iteration 5854 : loss : 0.019522, loss_ce: 0.006641
2022-01-08 11:45:34,382 iteration 5855 : loss : 0.023327, loss_ce: 0.010423
2022-01-08 11:45:35,758 iteration 5856 : loss : 0.013893, loss_ce: 0.005701
2022-01-08 11:45:37,134 iteration 5857 : loss : 0.014442, loss_ce: 0.005393
2022-01-08 11:45:38,426 iteration 5858 : loss : 0.012193, loss_ce: 0.004434
2022-01-08 11:45:39,826 iteration 5859 : loss : 0.027456, loss_ce: 0.015626
2022-01-08 11:45:41,215 iteration 5860 : loss : 0.013042, loss_ce: 0.003813
2022-01-08 11:45:42,555 iteration 5861 : loss : 0.012908, loss_ce: 0.003788
2022-01-08 11:45:43,944 iteration 5862 : loss : 0.012812, loss_ce: 0.005904
2022-01-08 11:45:45,213 iteration 5863 : loss : 0.009594, loss_ce: 0.003353
2022-01-08 11:45:46,581 iteration 5864 : loss : 0.025798, loss_ce: 0.009456
2022-01-08 11:45:46,581 Training Data Eval:
2022-01-08 11:45:53,467   Average segmentation loss on training set: 0.0080
2022-01-08 11:45:53,467 Validation Data Eval:
2022-01-08 11:45:55,839   Average segmentation loss on validation set: 0.0635
2022-01-08 11:45:57,155 iteration 5865 : loss : 0.015138, loss_ce: 0.004452
 86%|█████████████████████████    | 345/400 [2:25:27<24:19, 26.54s/it]2022-01-08 11:45:58,633 iteration 5866 : loss : 0.020465, loss_ce: 0.010010
2022-01-08 11:46:00,041 iteration 5867 : loss : 0.016357, loss_ce: 0.004843
2022-01-08 11:46:01,439 iteration 5868 : loss : 0.016688, loss_ce: 0.005868
2022-01-08 11:46:02,728 iteration 5869 : loss : 0.011890, loss_ce: 0.005184
2022-01-08 11:46:04,180 iteration 5870 : loss : 0.022438, loss_ce: 0.007276
2022-01-08 11:46:05,568 iteration 5871 : loss : 0.011609, loss_ce: 0.004197
2022-01-08 11:46:06,960 iteration 5872 : loss : 0.027128, loss_ce: 0.008744
2022-01-08 11:46:08,436 iteration 5873 : loss : 0.029199, loss_ce: 0.009523
2022-01-08 11:46:09,755 iteration 5874 : loss : 0.015204, loss_ce: 0.005511
2022-01-08 11:46:11,091 iteration 5875 : loss : 0.014328, loss_ce: 0.007307
2022-01-08 11:46:12,474 iteration 5876 : loss : 0.014486, loss_ce: 0.005248
2022-01-08 11:46:13,853 iteration 5877 : loss : 0.013672, loss_ce: 0.005584
2022-01-08 11:46:15,248 iteration 5878 : loss : 0.015817, loss_ce: 0.006833
2022-01-08 11:46:16,572 iteration 5879 : loss : 0.013397, loss_ce: 0.005716
2022-01-08 11:46:18,029 iteration 5880 : loss : 0.033488, loss_ce: 0.009654
2022-01-08 11:46:19,327 iteration 5881 : loss : 0.012235, loss_ce: 0.004145
2022-01-08 11:46:20,671 iteration 5882 : loss : 0.013904, loss_ce: 0.006242
 86%|█████████████████████████    | 346/400 [2:25:51<23:04, 25.63s/it]2022-01-08 11:46:22,050 iteration 5883 : loss : 0.011996, loss_ce: 0.003843
2022-01-08 11:46:23,390 iteration 5884 : loss : 0.014992, loss_ce: 0.006283
2022-01-08 11:46:24,878 iteration 5885 : loss : 0.024367, loss_ce: 0.008963
2022-01-08 11:46:26,190 iteration 5886 : loss : 0.014388, loss_ce: 0.003900
2022-01-08 11:46:27,538 iteration 5887 : loss : 0.014757, loss_ce: 0.005459
2022-01-08 11:46:28,994 iteration 5888 : loss : 0.022348, loss_ce: 0.009654
2022-01-08 11:46:30,430 iteration 5889 : loss : 0.016707, loss_ce: 0.006540
2022-01-08 11:46:31,801 iteration 5890 : loss : 0.010966, loss_ce: 0.003655
2022-01-08 11:46:33,218 iteration 5891 : loss : 0.020531, loss_ce: 0.006385
2022-01-08 11:46:34,680 iteration 5892 : loss : 0.021647, loss_ce: 0.008643
2022-01-08 11:46:36,020 iteration 5893 : loss : 0.010673, loss_ce: 0.003492
2022-01-08 11:46:37,474 iteration 5894 : loss : 0.018855, loss_ce: 0.008072
2022-01-08 11:46:38,874 iteration 5895 : loss : 0.020600, loss_ce: 0.007229
2022-01-08 11:46:40,232 iteration 5896 : loss : 0.013504, loss_ce: 0.006211
2022-01-08 11:46:41,666 iteration 5897 : loss : 0.024310, loss_ce: 0.009217
2022-01-08 11:46:43,069 iteration 5898 : loss : 0.011535, loss_ce: 0.004186
2022-01-08 11:46:44,406 iteration 5899 : loss : 0.015578, loss_ce: 0.005655
 87%|█████████████████████████▏   | 347/400 [2:26:15<22:08, 25.06s/it]2022-01-08 11:46:45,831 iteration 5900 : loss : 0.013304, loss_ce: 0.004851
2022-01-08 11:46:47,276 iteration 5901 : loss : 0.014841, loss_ce: 0.004952
2022-01-08 11:46:48,646 iteration 5902 : loss : 0.015690, loss_ce: 0.006389
2022-01-08 11:46:50,079 iteration 5903 : loss : 0.013678, loss_ce: 0.005939
2022-01-08 11:46:51,426 iteration 5904 : loss : 0.015271, loss_ce: 0.005440
2022-01-08 11:46:52,842 iteration 5905 : loss : 0.021466, loss_ce: 0.005941
2022-01-08 11:46:54,162 iteration 5906 : loss : 0.014164, loss_ce: 0.005355
2022-01-08 11:46:55,567 iteration 5907 : loss : 0.010618, loss_ce: 0.003837
2022-01-08 11:46:56,952 iteration 5908 : loss : 0.014143, loss_ce: 0.004436
2022-01-08 11:46:58,236 iteration 5909 : loss : 0.017280, loss_ce: 0.004474
2022-01-08 11:46:59,617 iteration 5910 : loss : 0.019317, loss_ce: 0.007228
2022-01-08 11:47:01,021 iteration 5911 : loss : 0.030376, loss_ce: 0.011255
2022-01-08 11:47:02,323 iteration 5912 : loss : 0.015457, loss_ce: 0.005504
2022-01-08 11:47:03,679 iteration 5913 : loss : 0.012727, loss_ce: 0.005712
2022-01-08 11:47:05,079 iteration 5914 : loss : 0.019101, loss_ce: 0.007246
2022-01-08 11:47:06,371 iteration 5915 : loss : 0.013288, loss_ce: 0.005231
2022-01-08 11:47:07,789 iteration 5916 : loss : 0.019552, loss_ce: 0.006857
 87%|█████████████████████████▏   | 348/400 [2:26:38<21:16, 24.56s/it]2022-01-08 11:47:09,168 iteration 5917 : loss : 0.014155, loss_ce: 0.006383
2022-01-08 11:47:10,471 iteration 5918 : loss : 0.012801, loss_ce: 0.004941
2022-01-08 11:47:11,865 iteration 5919 : loss : 0.018285, loss_ce: 0.008287
2022-01-08 11:47:13,289 iteration 5920 : loss : 0.019267, loss_ce: 0.010851
2022-01-08 11:47:14,792 iteration 5921 : loss : 0.027720, loss_ce: 0.008698
2022-01-08 11:47:16,134 iteration 5922 : loss : 0.012925, loss_ce: 0.003388
2022-01-08 11:47:17,520 iteration 5923 : loss : 0.019160, loss_ce: 0.006286
2022-01-08 11:47:18,990 iteration 5924 : loss : 0.020989, loss_ce: 0.004136
2022-01-08 11:47:20,367 iteration 5925 : loss : 0.014663, loss_ce: 0.004987
2022-01-08 11:47:21,727 iteration 5926 : loss : 0.019495, loss_ce: 0.007362
2022-01-08 11:47:23,114 iteration 5927 : loss : 0.017035, loss_ce: 0.007358
2022-01-08 11:47:24,422 iteration 5928 : loss : 0.019179, loss_ce: 0.005011
2022-01-08 11:47:25,742 iteration 5929 : loss : 0.011292, loss_ce: 0.005064
2022-01-08 11:47:27,065 iteration 5930 : loss : 0.017078, loss_ce: 0.007472
2022-01-08 11:47:28,443 iteration 5931 : loss : 0.021262, loss_ce: 0.006108
2022-01-08 11:47:29,866 iteration 5932 : loss : 0.022305, loss_ce: 0.008125
2022-01-08 11:47:31,225 iteration 5933 : loss : 0.020640, loss_ce: 0.006955
 87%|█████████████████████████▎   | 349/400 [2:27:01<20:35, 24.22s/it]2022-01-08 11:47:32,581 iteration 5934 : loss : 0.012767, loss_ce: 0.004236
2022-01-08 11:47:34,083 iteration 5935 : loss : 0.032511, loss_ce: 0.012798
2022-01-08 11:47:35,454 iteration 5936 : loss : 0.011710, loss_ce: 0.004497
2022-01-08 11:47:36,784 iteration 5937 : loss : 0.012629, loss_ce: 0.004414
2022-01-08 11:47:38,076 iteration 5938 : loss : 0.009443, loss_ce: 0.003505
2022-01-08 11:47:39,482 iteration 5939 : loss : 0.015502, loss_ce: 0.007500
2022-01-08 11:47:40,796 iteration 5940 : loss : 0.013827, loss_ce: 0.006305
2022-01-08 11:47:42,130 iteration 5941 : loss : 0.013110, loss_ce: 0.004951
2022-01-08 11:47:43,466 iteration 5942 : loss : 0.010702, loss_ce: 0.004472
2022-01-08 11:47:44,834 iteration 5943 : loss : 0.014113, loss_ce: 0.003894
2022-01-08 11:47:46,209 iteration 5944 : loss : 0.014452, loss_ce: 0.007387
2022-01-08 11:47:47,539 iteration 5945 : loss : 0.014391, loss_ce: 0.003177
2022-01-08 11:47:49,005 iteration 5946 : loss : 0.018642, loss_ce: 0.007265
2022-01-08 11:47:50,409 iteration 5947 : loss : 0.023357, loss_ce: 0.010865
2022-01-08 11:47:51,764 iteration 5948 : loss : 0.023279, loss_ce: 0.007395
2022-01-08 11:47:53,212 iteration 5949 : loss : 0.013969, loss_ce: 0.004226
2022-01-08 11:47:53,212 Training Data Eval:
2022-01-08 11:48:00,074   Average segmentation loss on training set: 0.0081
2022-01-08 11:48:00,075 Validation Data Eval:
2022-01-08 11:48:02,452   Average segmentation loss on validation set: 0.0672
2022-01-08 11:48:03,840 iteration 5950 : loss : 0.020486, loss_ce: 0.007179
 88%|█████████████████████████▍   | 350/400 [2:27:34<22:17, 26.74s/it]2022-01-08 11:48:05,199 iteration 5951 : loss : 0.012055, loss_ce: 0.004077
2022-01-08 11:48:06,574 iteration 5952 : loss : 0.016211, loss_ce: 0.006415
2022-01-08 11:48:07,952 iteration 5953 : loss : 0.022781, loss_ce: 0.009341
2022-01-08 11:48:09,291 iteration 5954 : loss : 0.020341, loss_ce: 0.006574
2022-01-08 11:48:10,659 iteration 5955 : loss : 0.013183, loss_ce: 0.004142
2022-01-08 11:48:12,052 iteration 5956 : loss : 0.023015, loss_ce: 0.012865
2022-01-08 11:48:13,423 iteration 5957 : loss : 0.013984, loss_ce: 0.005406
2022-01-08 11:48:14,762 iteration 5958 : loss : 0.014130, loss_ce: 0.005315
2022-01-08 11:48:16,138 iteration 5959 : loss : 0.016662, loss_ce: 0.007929
2022-01-08 11:48:17,456 iteration 5960 : loss : 0.016142, loss_ce: 0.004698
2022-01-08 11:48:18,774 iteration 5961 : loss : 0.017936, loss_ce: 0.004738
2022-01-08 11:48:20,127 iteration 5962 : loss : 0.022057, loss_ce: 0.005779
2022-01-08 11:48:21,516 iteration 5963 : loss : 0.015299, loss_ce: 0.006590
2022-01-08 11:48:22,860 iteration 5964 : loss : 0.010346, loss_ce: 0.004635
2022-01-08 11:48:24,169 iteration 5965 : loss : 0.016495, loss_ce: 0.007079
2022-01-08 11:48:25,593 iteration 5966 : loss : 0.024539, loss_ce: 0.006786
2022-01-08 11:48:26,984 iteration 5967 : loss : 0.014243, loss_ce: 0.005737
 88%|█████████████████████████▍   | 351/400 [2:27:57<20:57, 25.66s/it]2022-01-08 11:48:28,457 iteration 5968 : loss : 0.018124, loss_ce: 0.006252
2022-01-08 11:48:29,793 iteration 5969 : loss : 0.014519, loss_ce: 0.006287
2022-01-08 11:48:31,185 iteration 5970 : loss : 0.017528, loss_ce: 0.007390
2022-01-08 11:48:32,479 iteration 5971 : loss : 0.019040, loss_ce: 0.006032
2022-01-08 11:48:33,759 iteration 5972 : loss : 0.010812, loss_ce: 0.003625
2022-01-08 11:48:35,136 iteration 5973 : loss : 0.015992, loss_ce: 0.007360
2022-01-08 11:48:36,596 iteration 5974 : loss : 0.029940, loss_ce: 0.010304
2022-01-08 11:48:37,911 iteration 5975 : loss : 0.015085, loss_ce: 0.005108
2022-01-08 11:48:39,254 iteration 5976 : loss : 0.014425, loss_ce: 0.004112
2022-01-08 11:48:40,575 iteration 5977 : loss : 0.021590, loss_ce: 0.005236
2022-01-08 11:48:41,887 iteration 5978 : loss : 0.011660, loss_ce: 0.005387
2022-01-08 11:48:43,243 iteration 5979 : loss : 0.012118, loss_ce: 0.005978
2022-01-08 11:48:44,651 iteration 5980 : loss : 0.011824, loss_ce: 0.004161
2022-01-08 11:48:45,986 iteration 5981 : loss : 0.013020, loss_ce: 0.005734
2022-01-08 11:48:47,355 iteration 5982 : loss : 0.016918, loss_ce: 0.006338
2022-01-08 11:48:48,777 iteration 5983 : loss : 0.015230, loss_ce: 0.004940
2022-01-08 11:48:50,116 iteration 5984 : loss : 0.014535, loss_ce: 0.005379
 88%|█████████████████████████▌   | 352/400 [2:28:20<19:55, 24.90s/it]2022-01-08 11:48:51,521 iteration 5985 : loss : 0.014154, loss_ce: 0.004408
2022-01-08 11:48:52,803 iteration 5986 : loss : 0.014441, loss_ce: 0.004437
2022-01-08 11:48:54,197 iteration 5987 : loss : 0.017906, loss_ce: 0.009796
2022-01-08 11:48:55,614 iteration 5988 : loss : 0.023308, loss_ce: 0.008705
2022-01-08 11:48:56,986 iteration 5989 : loss : 0.011512, loss_ce: 0.004242
2022-01-08 11:48:58,328 iteration 5990 : loss : 0.011942, loss_ce: 0.004171
2022-01-08 11:48:59,718 iteration 5991 : loss : 0.016004, loss_ce: 0.005271
2022-01-08 11:49:01,100 iteration 5992 : loss : 0.023196, loss_ce: 0.008553
2022-01-08 11:49:02,459 iteration 5993 : loss : 0.014073, loss_ce: 0.006252
2022-01-08 11:49:03,871 iteration 5994 : loss : 0.027412, loss_ce: 0.014846
2022-01-08 11:49:05,221 iteration 5995 : loss : 0.017470, loss_ce: 0.006632
2022-01-08 11:49:06,541 iteration 5996 : loss : 0.010817, loss_ce: 0.003298
2022-01-08 11:49:07,933 iteration 5997 : loss : 0.021620, loss_ce: 0.008832
2022-01-08 11:49:09,369 iteration 5998 : loss : 0.015293, loss_ce: 0.005119
2022-01-08 11:49:10,833 iteration 5999 : loss : 0.022516, loss_ce: 0.007543
2022-01-08 11:49:12,223 iteration 6000 : loss : 0.016115, loss_ce: 0.007286
2022-01-08 11:49:13,517 iteration 6001 : loss : 0.013651, loss_ce: 0.006285
 88%|█████████████████████████▌   | 353/400 [2:28:44<19:09, 24.45s/it]2022-01-08 11:49:14,937 iteration 6002 : loss : 0.013478, loss_ce: 0.004842
2022-01-08 11:49:16,268 iteration 6003 : loss : 0.012087, loss_ce: 0.004272
2022-01-08 11:49:17,684 iteration 6004 : loss : 0.021287, loss_ce: 0.009100
2022-01-08 11:49:19,073 iteration 6005 : loss : 0.019012, loss_ce: 0.006632
2022-01-08 11:49:20,540 iteration 6006 : loss : 0.013855, loss_ce: 0.005514
2022-01-08 11:49:21,915 iteration 6007 : loss : 0.013406, loss_ce: 0.006959
2022-01-08 11:49:23,288 iteration 6008 : loss : 0.014285, loss_ce: 0.005222
2022-01-08 11:49:24,581 iteration 6009 : loss : 0.010594, loss_ce: 0.004988
2022-01-08 11:49:25,899 iteration 6010 : loss : 0.012479, loss_ce: 0.005282
2022-01-08 11:49:27,320 iteration 6011 : loss : 0.017934, loss_ce: 0.005676
2022-01-08 11:49:28,755 iteration 6012 : loss : 0.021479, loss_ce: 0.004582
2022-01-08 11:49:30,152 iteration 6013 : loss : 0.017865, loss_ce: 0.006021
2022-01-08 11:49:31,541 iteration 6014 : loss : 0.022939, loss_ce: 0.005653
2022-01-08 11:49:32,876 iteration 6015 : loss : 0.013216, loss_ce: 0.004776
2022-01-08 11:49:34,242 iteration 6016 : loss : 0.014500, loss_ce: 0.005535
2022-01-08 11:49:35,595 iteration 6017 : loss : 0.015448, loss_ce: 0.007062
2022-01-08 11:49:36,975 iteration 6018 : loss : 0.020432, loss_ce: 0.007164
 88%|█████████████████████████▋   | 354/400 [2:29:07<18:31, 24.15s/it]2022-01-08 11:49:38,395 iteration 6019 : loss : 0.014878, loss_ce: 0.007595
2022-01-08 11:49:39,738 iteration 6020 : loss : 0.012674, loss_ce: 0.005604
2022-01-08 11:49:41,123 iteration 6021 : loss : 0.015528, loss_ce: 0.003735
2022-01-08 11:49:42,443 iteration 6022 : loss : 0.013149, loss_ce: 0.005147
2022-01-08 11:49:43,856 iteration 6023 : loss : 0.013806, loss_ce: 0.005375
2022-01-08 11:49:45,207 iteration 6024 : loss : 0.027039, loss_ce: 0.009027
2022-01-08 11:49:46,539 iteration 6025 : loss : 0.015663, loss_ce: 0.006429
2022-01-08 11:49:47,938 iteration 6026 : loss : 0.018196, loss_ce: 0.005809
2022-01-08 11:49:49,318 iteration 6027 : loss : 0.015866, loss_ce: 0.005860
2022-01-08 11:49:50,714 iteration 6028 : loss : 0.017667, loss_ce: 0.006583
2022-01-08 11:49:52,053 iteration 6029 : loss : 0.014585, loss_ce: 0.003937
2022-01-08 11:49:53,349 iteration 6030 : loss : 0.016766, loss_ce: 0.007082
2022-01-08 11:49:54,735 iteration 6031 : loss : 0.013356, loss_ce: 0.006647
2022-01-08 11:49:56,056 iteration 6032 : loss : 0.015997, loss_ce: 0.004998
2022-01-08 11:49:57,471 iteration 6033 : loss : 0.020566, loss_ce: 0.006951
2022-01-08 11:49:58,832 iteration 6034 : loss : 0.014284, loss_ce: 0.006182
2022-01-08 11:49:58,833 Training Data Eval:
2022-01-08 11:50:05,730   Average segmentation loss on training set: 0.0082
2022-01-08 11:50:05,731 Validation Data Eval:
2022-01-08 11:50:08,098   Average segmentation loss on validation set: 0.0655
2022-01-08 11:50:09,363 iteration 6035 : loss : 0.011090, loss_ce: 0.004192
 89%|█████████████████████████▋   | 355/400 [2:29:40<19:58, 26.63s/it]2022-01-08 11:50:10,867 iteration 6036 : loss : 0.032650, loss_ce: 0.009340
2022-01-08 11:50:12,275 iteration 6037 : loss : 0.016626, loss_ce: 0.005202
2022-01-08 11:50:13,616 iteration 6038 : loss : 0.012766, loss_ce: 0.004444
2022-01-08 11:50:14,955 iteration 6039 : loss : 0.012974, loss_ce: 0.004231
2022-01-08 11:50:16,372 iteration 6040 : loss : 0.021612, loss_ce: 0.008121
2022-01-08 11:50:17,700 iteration 6041 : loss : 0.011472, loss_ce: 0.004172
2022-01-08 11:50:19,071 iteration 6042 : loss : 0.012799, loss_ce: 0.006180
2022-01-08 11:50:20,389 iteration 6043 : loss : 0.011658, loss_ce: 0.005326
2022-01-08 11:50:21,702 iteration 6044 : loss : 0.010390, loss_ce: 0.004334
2022-01-08 11:50:23,150 iteration 6045 : loss : 0.024762, loss_ce: 0.009561
2022-01-08 11:50:24,461 iteration 6046 : loss : 0.015984, loss_ce: 0.006448
2022-01-08 11:50:25,782 iteration 6047 : loss : 0.011270, loss_ce: 0.004764
2022-01-08 11:50:27,213 iteration 6048 : loss : 0.018634, loss_ce: 0.005898
2022-01-08 11:50:28,505 iteration 6049 : loss : 0.012921, loss_ce: 0.004512
2022-01-08 11:50:29,799 iteration 6050 : loss : 0.016828, loss_ce: 0.004094
2022-01-08 11:50:31,097 iteration 6051 : loss : 0.010942, loss_ce: 0.004602
2022-01-08 11:50:32,489 iteration 6052 : loss : 0.015490, loss_ce: 0.006185
 89%|█████████████████████████▊   | 356/400 [2:30:03<18:45, 25.57s/it]2022-01-08 11:50:33,866 iteration 6053 : loss : 0.015847, loss_ce: 0.004684
2022-01-08 11:50:35,192 iteration 6054 : loss : 0.012072, loss_ce: 0.003991
2022-01-08 11:50:36,575 iteration 6055 : loss : 0.012954, loss_ce: 0.005167
2022-01-08 11:50:37,940 iteration 6056 : loss : 0.014712, loss_ce: 0.005181
2022-01-08 11:50:39,236 iteration 6057 : loss : 0.012370, loss_ce: 0.004664
2022-01-08 11:50:40,559 iteration 6058 : loss : 0.010799, loss_ce: 0.005917
2022-01-08 11:50:41,988 iteration 6059 : loss : 0.020532, loss_ce: 0.005382
2022-01-08 11:50:43,365 iteration 6060 : loss : 0.016051, loss_ce: 0.004503
2022-01-08 11:50:44,685 iteration 6061 : loss : 0.015342, loss_ce: 0.007255
2022-01-08 11:50:46,177 iteration 6062 : loss : 0.020132, loss_ce: 0.006917
2022-01-08 11:50:47,489 iteration 6063 : loss : 0.013825, loss_ce: 0.006450
2022-01-08 11:50:48,801 iteration 6064 : loss : 0.009904, loss_ce: 0.003263
2022-01-08 11:50:50,186 iteration 6065 : loss : 0.027077, loss_ce: 0.011202
2022-01-08 11:50:51,492 iteration 6066 : loss : 0.030550, loss_ce: 0.011679
2022-01-08 11:50:52,873 iteration 6067 : loss : 0.023762, loss_ce: 0.009256
2022-01-08 11:50:54,243 iteration 6068 : loss : 0.011711, loss_ce: 0.002160
2022-01-08 11:50:55,683 iteration 6069 : loss : 0.019242, loss_ce: 0.010454
 89%|█████████████████████████▉   | 357/400 [2:30:26<17:49, 24.86s/it]2022-01-08 11:50:57,088 iteration 6070 : loss : 0.011963, loss_ce: 0.005219
2022-01-08 11:50:58,436 iteration 6071 : loss : 0.011177, loss_ce: 0.004460
2022-01-08 11:50:59,775 iteration 6072 : loss : 0.017825, loss_ce: 0.005610
2022-01-08 11:51:01,131 iteration 6073 : loss : 0.017909, loss_ce: 0.004436
2022-01-08 11:51:02,408 iteration 6074 : loss : 0.016386, loss_ce: 0.006601
2022-01-08 11:51:03,801 iteration 6075 : loss : 0.016196, loss_ce: 0.006673
2022-01-08 11:51:05,175 iteration 6076 : loss : 0.014683, loss_ce: 0.006128
2022-01-08 11:51:06,440 iteration 6077 : loss : 0.009721, loss_ce: 0.003485
2022-01-08 11:51:07,794 iteration 6078 : loss : 0.013284, loss_ce: 0.006740
2022-01-08 11:51:09,231 iteration 6079 : loss : 0.013868, loss_ce: 0.005775
2022-01-08 11:51:10,526 iteration 6080 : loss : 0.011933, loss_ce: 0.004612
2022-01-08 11:51:11,952 iteration 6081 : loss : 0.020241, loss_ce: 0.007370
2022-01-08 11:51:13,310 iteration 6082 : loss : 0.015050, loss_ce: 0.004674
2022-01-08 11:51:14,722 iteration 6083 : loss : 0.014057, loss_ce: 0.003947
2022-01-08 11:51:16,040 iteration 6084 : loss : 0.015685, loss_ce: 0.004284
2022-01-08 11:51:17,363 iteration 6085 : loss : 0.021719, loss_ce: 0.009223
2022-01-08 11:51:18,881 iteration 6086 : loss : 0.026130, loss_ce: 0.006648
 90%|█████████████████████████▉   | 358/400 [2:30:49<17:03, 24.36s/it]2022-01-08 11:51:20,224 iteration 6087 : loss : 0.017091, loss_ce: 0.006601
2022-01-08 11:51:21,585 iteration 6088 : loss : 0.012357, loss_ce: 0.005360
2022-01-08 11:51:22,928 iteration 6089 : loss : 0.013453, loss_ce: 0.006678
2022-01-08 11:51:24,278 iteration 6090 : loss : 0.016146, loss_ce: 0.004362
2022-01-08 11:51:25,639 iteration 6091 : loss : 0.011468, loss_ce: 0.005368
2022-01-08 11:51:27,012 iteration 6092 : loss : 0.011822, loss_ce: 0.005288
2022-01-08 11:51:28,300 iteration 6093 : loss : 0.016406, loss_ce: 0.005739
2022-01-08 11:51:29,621 iteration 6094 : loss : 0.015459, loss_ce: 0.005252
2022-01-08 11:51:31,013 iteration 6095 : loss : 0.020286, loss_ce: 0.009067
2022-01-08 11:51:32,403 iteration 6096 : loss : 0.014395, loss_ce: 0.004280
2022-01-08 11:51:33,687 iteration 6097 : loss : 0.011761, loss_ce: 0.005082
2022-01-08 11:51:35,056 iteration 6098 : loss : 0.019901, loss_ce: 0.007607
2022-01-08 11:51:36,458 iteration 6099 : loss : 0.028549, loss_ce: 0.007266
2022-01-08 11:51:37,885 iteration 6100 : loss : 0.019812, loss_ce: 0.006652
2022-01-08 11:51:39,283 iteration 6101 : loss : 0.017469, loss_ce: 0.007210
2022-01-08 11:51:40,595 iteration 6102 : loss : 0.011437, loss_ce: 0.003501
2022-01-08 11:51:41,846 iteration 6103 : loss : 0.008435, loss_ce: 0.003114
 90%|██████████████████████████   | 359/400 [2:31:12<16:21, 23.94s/it]2022-01-08 11:51:43,294 iteration 6104 : loss : 0.024017, loss_ce: 0.007766
2022-01-08 11:51:44,608 iteration 6105 : loss : 0.015528, loss_ce: 0.005357
2022-01-08 11:51:45,898 iteration 6106 : loss : 0.011941, loss_ce: 0.003940
2022-01-08 11:51:47,206 iteration 6107 : loss : 0.014552, loss_ce: 0.004367
2022-01-08 11:51:48,613 iteration 6108 : loss : 0.014147, loss_ce: 0.004594
2022-01-08 11:51:49,962 iteration 6109 : loss : 0.014273, loss_ce: 0.005123
2022-01-08 11:51:51,358 iteration 6110 : loss : 0.012895, loss_ce: 0.005534
2022-01-08 11:51:52,777 iteration 6111 : loss : 0.027115, loss_ce: 0.012130
2022-01-08 11:51:54,169 iteration 6112 : loss : 0.022765, loss_ce: 0.007662
2022-01-08 11:51:55,587 iteration 6113 : loss : 0.025596, loss_ce: 0.008464
2022-01-08 11:51:56,930 iteration 6114 : loss : 0.013770, loss_ce: 0.006315
2022-01-08 11:51:58,367 iteration 6115 : loss : 0.025248, loss_ce: 0.007947
2022-01-08 11:51:59,741 iteration 6116 : loss : 0.013020, loss_ce: 0.005842
2022-01-08 11:52:01,072 iteration 6117 : loss : 0.021125, loss_ce: 0.010022
2022-01-08 11:52:02,470 iteration 6118 : loss : 0.015022, loss_ce: 0.006656
2022-01-08 11:52:03,823 iteration 6119 : loss : 0.016188, loss_ce: 0.005212
2022-01-08 11:52:03,824 Training Data Eval:
2022-01-08 11:52:10,720   Average segmentation loss on training set: 0.0082
2022-01-08 11:52:10,721 Validation Data Eval:
2022-01-08 11:52:13,093   Average segmentation loss on validation set: 0.0717
2022-01-08 11:52:14,447 iteration 6120 : loss : 0.014357, loss_ce: 0.006196
 90%|██████████████████████████   | 360/400 [2:31:45<17:41, 26.54s/it]2022-01-08 11:52:15,870 iteration 6121 : loss : 0.014544, loss_ce: 0.006347
2022-01-08 11:52:17,283 iteration 6122 : loss : 0.022152, loss_ce: 0.008671
2022-01-08 11:52:18,721 iteration 6123 : loss : 0.024416, loss_ce: 0.010910
2022-01-08 11:52:20,086 iteration 6124 : loss : 0.015975, loss_ce: 0.005084
2022-01-08 11:52:21,535 iteration 6125 : loss : 0.020056, loss_ce: 0.006029
2022-01-08 11:52:22,894 iteration 6126 : loss : 0.021942, loss_ce: 0.005451
2022-01-08 11:52:24,236 iteration 6127 : loss : 0.013087, loss_ce: 0.004917
2022-01-08 11:52:25,664 iteration 6128 : loss : 0.013347, loss_ce: 0.004693
2022-01-08 11:52:27,041 iteration 6129 : loss : 0.012168, loss_ce: 0.004789
2022-01-08 11:52:28,491 iteration 6130 : loss : 0.014754, loss_ce: 0.005706
2022-01-08 11:52:29,873 iteration 6131 : loss : 0.019595, loss_ce: 0.010884
2022-01-08 11:52:31,236 iteration 6132 : loss : 0.019184, loss_ce: 0.004166
2022-01-08 11:52:32,552 iteration 6133 : loss : 0.011792, loss_ce: 0.006192
2022-01-08 11:52:33,849 iteration 6134 : loss : 0.008769, loss_ce: 0.003776
2022-01-08 11:52:35,160 iteration 6135 : loss : 0.009612, loss_ce: 0.003925
2022-01-08 11:52:36,546 iteration 6136 : loss : 0.012977, loss_ce: 0.004116
2022-01-08 11:52:37,890 iteration 6137 : loss : 0.012507, loss_ce: 0.004770
 90%|██████████████████████████▏  | 361/400 [2:32:08<16:38, 25.61s/it]2022-01-08 11:52:39,246 iteration 6138 : loss : 0.010550, loss_ce: 0.003769
2022-01-08 11:52:40,644 iteration 6139 : loss : 0.013358, loss_ce: 0.005515
2022-01-08 11:52:42,005 iteration 6140 : loss : 0.011667, loss_ce: 0.004024
2022-01-08 11:52:43,399 iteration 6141 : loss : 0.015400, loss_ce: 0.005510
2022-01-08 11:52:44,898 iteration 6142 : loss : 0.019315, loss_ce: 0.006993
2022-01-08 11:52:46,192 iteration 6143 : loss : 0.013082, loss_ce: 0.006127
2022-01-08 11:52:47,573 iteration 6144 : loss : 0.013437, loss_ce: 0.006051
2022-01-08 11:52:48,960 iteration 6145 : loss : 0.015587, loss_ce: 0.005693
2022-01-08 11:52:50,326 iteration 6146 : loss : 0.012224, loss_ce: 0.005187
2022-01-08 11:52:51,686 iteration 6147 : loss : 0.011372, loss_ce: 0.003303
2022-01-08 11:52:52,999 iteration 6148 : loss : 0.009700, loss_ce: 0.003110
2022-01-08 11:52:54,318 iteration 6149 : loss : 0.016683, loss_ce: 0.005828
2022-01-08 11:52:55,656 iteration 6150 : loss : 0.018451, loss_ce: 0.005938
2022-01-08 11:52:57,004 iteration 6151 : loss : 0.015514, loss_ce: 0.003680
2022-01-08 11:52:58,322 iteration 6152 : loss : 0.017236, loss_ce: 0.008123
2022-01-08 11:52:59,738 iteration 6153 : loss : 0.023765, loss_ce: 0.012249
2022-01-08 11:53:01,200 iteration 6154 : loss : 0.018023, loss_ce: 0.007806
 90%|██████████████████████████▏  | 362/400 [2:32:31<15:46, 24.92s/it]2022-01-08 11:53:02,577 iteration 6155 : loss : 0.016525, loss_ce: 0.007232
2022-01-08 11:53:03,933 iteration 6156 : loss : 0.010774, loss_ce: 0.004470
2022-01-08 11:53:05,343 iteration 6157 : loss : 0.019457, loss_ce: 0.006637
2022-01-08 11:53:06,678 iteration 6158 : loss : 0.013475, loss_ce: 0.004036
2022-01-08 11:53:07,961 iteration 6159 : loss : 0.009929, loss_ce: 0.003847
2022-01-08 11:53:09,507 iteration 6160 : loss : 0.023867, loss_ce: 0.012349
2022-01-08 11:53:10,786 iteration 6161 : loss : 0.022141, loss_ce: 0.006391
2022-01-08 11:53:12,170 iteration 6162 : loss : 0.010208, loss_ce: 0.003589
2022-01-08 11:53:13,531 iteration 6163 : loss : 0.012747, loss_ce: 0.004736
2022-01-08 11:53:14,861 iteration 6164 : loss : 0.012467, loss_ce: 0.003639
2022-01-08 11:53:16,241 iteration 6165 : loss : 0.010461, loss_ce: 0.003856
2022-01-08 11:53:17,630 iteration 6166 : loss : 0.015264, loss_ce: 0.006250
2022-01-08 11:53:18,923 iteration 6167 : loss : 0.011677, loss_ce: 0.003980
2022-01-08 11:53:20,398 iteration 6168 : loss : 0.031585, loss_ce: 0.010120
2022-01-08 11:53:21,740 iteration 6169 : loss : 0.012841, loss_ce: 0.006022
2022-01-08 11:53:23,145 iteration 6170 : loss : 0.013695, loss_ce: 0.005741
2022-01-08 11:53:24,500 iteration 6171 : loss : 0.014177, loss_ce: 0.003806
 91%|██████████████████████████▎  | 363/400 [2:32:55<15:04, 24.43s/it]2022-01-08 11:53:25,896 iteration 6172 : loss : 0.019869, loss_ce: 0.006350
2022-01-08 11:53:27,160 iteration 6173 : loss : 0.011132, loss_ce: 0.004080
2022-01-08 11:53:28,501 iteration 6174 : loss : 0.013908, loss_ce: 0.006490
2022-01-08 11:53:29,800 iteration 6175 : loss : 0.009272, loss_ce: 0.002469
2022-01-08 11:53:31,188 iteration 6176 : loss : 0.018268, loss_ce: 0.010308
2022-01-08 11:53:32,650 iteration 6177 : loss : 0.018785, loss_ce: 0.009121
2022-01-08 11:53:34,056 iteration 6178 : loss : 0.020880, loss_ce: 0.007115
2022-01-08 11:53:35,450 iteration 6179 : loss : 0.016914, loss_ce: 0.005392
2022-01-08 11:53:36,840 iteration 6180 : loss : 0.031668, loss_ce: 0.004343
2022-01-08 11:53:38,164 iteration 6181 : loss : 0.018444, loss_ce: 0.007015
2022-01-08 11:53:39,540 iteration 6182 : loss : 0.011937, loss_ce: 0.004310
2022-01-08 11:53:40,865 iteration 6183 : loss : 0.027181, loss_ce: 0.009677
2022-01-08 11:53:42,182 iteration 6184 : loss : 0.013206, loss_ce: 0.005223
2022-01-08 11:53:43,581 iteration 6185 : loss : 0.020870, loss_ce: 0.005936
2022-01-08 11:53:44,874 iteration 6186 : loss : 0.013349, loss_ce: 0.005543
2022-01-08 11:53:46,185 iteration 6187 : loss : 0.016657, loss_ce: 0.005240
2022-01-08 11:53:47,494 iteration 6188 : loss : 0.015953, loss_ce: 0.005966
 91%|██████████████████████████▍  | 364/400 [2:33:18<14:24, 24.00s/it]2022-01-08 11:53:48,977 iteration 6189 : loss : 0.016713, loss_ce: 0.006505
2022-01-08 11:53:50,356 iteration 6190 : loss : 0.013003, loss_ce: 0.005265
2022-01-08 11:53:51,787 iteration 6191 : loss : 0.011805, loss_ce: 0.003883
2022-01-08 11:53:53,184 iteration 6192 : loss : 0.016194, loss_ce: 0.006366
2022-01-08 11:53:54,510 iteration 6193 : loss : 0.011673, loss_ce: 0.003557
2022-01-08 11:53:55,916 iteration 6194 : loss : 0.013442, loss_ce: 0.005269
2022-01-08 11:53:57,347 iteration 6195 : loss : 0.020695, loss_ce: 0.007708
2022-01-08 11:53:58,681 iteration 6196 : loss : 0.020087, loss_ce: 0.006560
2022-01-08 11:54:00,143 iteration 6197 : loss : 0.021336, loss_ce: 0.006884
2022-01-08 11:54:01,587 iteration 6198 : loss : 0.019953, loss_ce: 0.008013
2022-01-08 11:54:02,909 iteration 6199 : loss : 0.012505, loss_ce: 0.004627
2022-01-08 11:54:04,307 iteration 6200 : loss : 0.021244, loss_ce: 0.009170
2022-01-08 11:54:05,672 iteration 6201 : loss : 0.013410, loss_ce: 0.005081
2022-01-08 11:54:07,014 iteration 6202 : loss : 0.012101, loss_ce: 0.003673
2022-01-08 11:54:08,357 iteration 6203 : loss : 0.019861, loss_ce: 0.006409
2022-01-08 11:54:09,763 iteration 6204 : loss : 0.020328, loss_ce: 0.008227
2022-01-08 11:54:09,763 Training Data Eval:
2022-01-08 11:54:16,669   Average segmentation loss on training set: 0.0077
2022-01-08 11:54:16,670 Validation Data Eval:
2022-01-08 11:54:19,043   Average segmentation loss on validation set: 0.0649
2022-01-08 11:54:20,502 iteration 6205 : loss : 0.018769, loss_ce: 0.009540
 91%|██████████████████████████▍  | 365/400 [2:33:51<15:34, 26.70s/it]2022-01-08 11:54:21,928 iteration 6206 : loss : 0.015666, loss_ce: 0.007058
2022-01-08 11:54:23,289 iteration 6207 : loss : 0.016540, loss_ce: 0.008735
2022-01-08 11:54:24,677 iteration 6208 : loss : 0.013760, loss_ce: 0.005240
2022-01-08 11:54:26,055 iteration 6209 : loss : 0.017051, loss_ce: 0.005866
2022-01-08 11:54:27,402 iteration 6210 : loss : 0.015539, loss_ce: 0.005458
2022-01-08 11:54:28,708 iteration 6211 : loss : 0.013239, loss_ce: 0.004438
2022-01-08 11:54:30,053 iteration 6212 : loss : 0.013759, loss_ce: 0.004507
2022-01-08 11:54:31,416 iteration 6213 : loss : 0.010539, loss_ce: 0.004052
2022-01-08 11:54:32,772 iteration 6214 : loss : 0.009705, loss_ce: 0.003842
2022-01-08 11:54:34,190 iteration 6215 : loss : 0.015804, loss_ce: 0.005270
2022-01-08 11:54:35,533 iteration 6216 : loss : 0.011330, loss_ce: 0.004973
2022-01-08 11:54:36,825 iteration 6217 : loss : 0.015760, loss_ce: 0.005636
2022-01-08 11:54:38,192 iteration 6218 : loss : 0.017306, loss_ce: 0.007346
2022-01-08 11:54:39,537 iteration 6219 : loss : 0.015740, loss_ce: 0.005483
2022-01-08 11:54:40,865 iteration 6220 : loss : 0.019644, loss_ce: 0.006834
2022-01-08 11:54:42,248 iteration 6221 : loss : 0.014911, loss_ce: 0.004103
2022-01-08 11:54:43,575 iteration 6222 : loss : 0.011021, loss_ce: 0.005179
 92%|██████████████████████████▌  | 366/400 [2:34:14<14:30, 25.61s/it]2022-01-08 11:54:45,013 iteration 6223 : loss : 0.014787, loss_ce: 0.006830
2022-01-08 11:54:46,359 iteration 6224 : loss : 0.016173, loss_ce: 0.006726
2022-01-08 11:54:47,739 iteration 6225 : loss : 0.021427, loss_ce: 0.005857
2022-01-08 11:54:49,068 iteration 6226 : loss : 0.012864, loss_ce: 0.004046
2022-01-08 11:54:50,379 iteration 6227 : loss : 0.012940, loss_ce: 0.005136
2022-01-08 11:54:51,751 iteration 6228 : loss : 0.013171, loss_ce: 0.005062
2022-01-08 11:54:53,127 iteration 6229 : loss : 0.009130, loss_ce: 0.004069
2022-01-08 11:54:54,512 iteration 6230 : loss : 0.017303, loss_ce: 0.005329
2022-01-08 11:54:55,919 iteration 6231 : loss : 0.016476, loss_ce: 0.006563
2022-01-08 11:54:57,295 iteration 6232 : loss : 0.012152, loss_ce: 0.004889
2022-01-08 11:54:58,656 iteration 6233 : loss : 0.017090, loss_ce: 0.007645
2022-01-08 11:54:59,999 iteration 6234 : loss : 0.014619, loss_ce: 0.003805
2022-01-08 11:55:01,316 iteration 6235 : loss : 0.013450, loss_ce: 0.006871
2022-01-08 11:55:02,692 iteration 6236 : loss : 0.012329, loss_ce: 0.006126
2022-01-08 11:55:04,087 iteration 6237 : loss : 0.014163, loss_ce: 0.006782
2022-01-08 11:55:05,521 iteration 6238 : loss : 0.017414, loss_ce: 0.005157
2022-01-08 11:55:06,884 iteration 6239 : loss : 0.020426, loss_ce: 0.006975
 92%|██████████████████████████▌  | 367/400 [2:34:37<13:42, 24.92s/it]2022-01-08 11:55:08,266 iteration 6240 : loss : 0.010232, loss_ce: 0.003100
2022-01-08 11:55:09,583 iteration 6241 : loss : 0.010803, loss_ce: 0.005173
2022-01-08 11:55:11,020 iteration 6242 : loss : 0.018338, loss_ce: 0.006429
2022-01-08 11:55:12,364 iteration 6243 : loss : 0.010370, loss_ce: 0.004019
2022-01-08 11:55:13,804 iteration 6244 : loss : 0.013906, loss_ce: 0.005384
2022-01-08 11:55:15,160 iteration 6245 : loss : 0.015994, loss_ce: 0.005692
2022-01-08 11:55:16,500 iteration 6246 : loss : 0.015374, loss_ce: 0.006666
2022-01-08 11:55:17,872 iteration 6247 : loss : 0.019124, loss_ce: 0.004303
2022-01-08 11:55:19,216 iteration 6248 : loss : 0.011235, loss_ce: 0.004560
2022-01-08 11:55:20,553 iteration 6249 : loss : 0.015497, loss_ce: 0.004295
2022-01-08 11:55:21,950 iteration 6250 : loss : 0.015278, loss_ce: 0.005902
2022-01-08 11:55:23,396 iteration 6251 : loss : 0.019326, loss_ce: 0.008947
2022-01-08 11:55:24,734 iteration 6252 : loss : 0.012641, loss_ce: 0.005401
2022-01-08 11:55:26,163 iteration 6253 : loss : 0.016110, loss_ce: 0.005938
2022-01-08 11:55:27,505 iteration 6254 : loss : 0.012389, loss_ce: 0.005734
2022-01-08 11:55:28,887 iteration 6255 : loss : 0.012159, loss_ce: 0.005300
2022-01-08 11:55:30,315 iteration 6256 : loss : 0.015908, loss_ce: 0.006980
 92%|██████████████████████████▋  | 368/400 [2:35:00<13:03, 24.48s/it]2022-01-08 11:55:31,748 iteration 6257 : loss : 0.022084, loss_ce: 0.006159
2022-01-08 11:55:33,152 iteration 6258 : loss : 0.020763, loss_ce: 0.009193
2022-01-08 11:55:34,573 iteration 6259 : loss : 0.015787, loss_ce: 0.006518
2022-01-08 11:55:35,916 iteration 6260 : loss : 0.017995, loss_ce: 0.007217
2022-01-08 11:55:37,265 iteration 6261 : loss : 0.015551, loss_ce: 0.005703
2022-01-08 11:55:38,653 iteration 6262 : loss : 0.021859, loss_ce: 0.007290
2022-01-08 11:55:40,034 iteration 6263 : loss : 0.017516, loss_ce: 0.003644
2022-01-08 11:55:41,411 iteration 6264 : loss : 0.013224, loss_ce: 0.004750
2022-01-08 11:55:42,791 iteration 6265 : loss : 0.015316, loss_ce: 0.005985
2022-01-08 11:55:44,124 iteration 6266 : loss : 0.015904, loss_ce: 0.008201
2022-01-08 11:55:45,458 iteration 6267 : loss : 0.013949, loss_ce: 0.006596
2022-01-08 11:55:46,818 iteration 6268 : loss : 0.014017, loss_ce: 0.005777
2022-01-08 11:55:48,056 iteration 6269 : loss : 0.009700, loss_ce: 0.003290
2022-01-08 11:55:49,409 iteration 6270 : loss : 0.011954, loss_ce: 0.005317
2022-01-08 11:55:50,759 iteration 6271 : loss : 0.013360, loss_ce: 0.004005
2022-01-08 11:55:52,156 iteration 6272 : loss : 0.022081, loss_ce: 0.007613
2022-01-08 11:55:53,551 iteration 6273 : loss : 0.012789, loss_ce: 0.005108
 92%|██████████████████████████▊  | 369/400 [2:35:24<12:27, 24.10s/it]2022-01-08 11:55:55,070 iteration 6274 : loss : 0.030914, loss_ce: 0.011598
2022-01-08 11:55:56,447 iteration 6275 : loss : 0.012575, loss_ce: 0.006181
2022-01-08 11:55:57,824 iteration 6276 : loss : 0.013406, loss_ce: 0.005322
2022-01-08 11:55:59,157 iteration 6277 : loss : 0.013589, loss_ce: 0.003658
2022-01-08 11:56:00,541 iteration 6278 : loss : 0.017401, loss_ce: 0.005872
2022-01-08 11:56:01,887 iteration 6279 : loss : 0.015874, loss_ce: 0.007447
2022-01-08 11:56:03,223 iteration 6280 : loss : 0.016924, loss_ce: 0.006112
2022-01-08 11:56:04,711 iteration 6281 : loss : 0.031766, loss_ce: 0.009366
2022-01-08 11:56:06,065 iteration 6282 : loss : 0.010867, loss_ce: 0.004396
2022-01-08 11:56:07,402 iteration 6283 : loss : 0.010838, loss_ce: 0.004930
2022-01-08 11:56:08,768 iteration 6284 : loss : 0.018047, loss_ce: 0.007777
2022-01-08 11:56:10,171 iteration 6285 : loss : 0.014914, loss_ce: 0.005849
2022-01-08 11:56:11,515 iteration 6286 : loss : 0.015589, loss_ce: 0.004641
2022-01-08 11:56:12,848 iteration 6287 : loss : 0.017811, loss_ce: 0.005338
2022-01-08 11:56:14,125 iteration 6288 : loss : 0.009312, loss_ce: 0.003113
2022-01-08 11:56:15,500 iteration 6289 : loss : 0.011228, loss_ce: 0.004592
2022-01-08 11:56:15,500 Training Data Eval:
2022-01-08 11:56:22,381   Average segmentation loss on training set: 0.0073
2022-01-08 11:56:22,382 Validation Data Eval:
2022-01-08 11:56:24,752   Average segmentation loss on validation set: 0.0650
2022-01-08 11:56:26,162 iteration 6290 : loss : 0.017284, loss_ce: 0.008331
 92%|██████████████████████████▊  | 370/400 [2:35:56<13:19, 26.65s/it]2022-01-08 11:56:27,563 iteration 6291 : loss : 0.011517, loss_ce: 0.005125
2022-01-08 11:56:28,928 iteration 6292 : loss : 0.018620, loss_ce: 0.006612
2022-01-08 11:56:30,323 iteration 6293 : loss : 0.021334, loss_ce: 0.010499
2022-01-08 11:56:31,727 iteration 6294 : loss : 0.018535, loss_ce: 0.005460
2022-01-08 11:56:33,091 iteration 6295 : loss : 0.012699, loss_ce: 0.005155
2022-01-08 11:56:34,517 iteration 6296 : loss : 0.018302, loss_ce: 0.006654
2022-01-08 11:56:35,892 iteration 6297 : loss : 0.023458, loss_ce: 0.008216
2022-01-08 11:56:37,316 iteration 6298 : loss : 0.017084, loss_ce: 0.006295
2022-01-08 11:56:38,676 iteration 6299 : loss : 0.014635, loss_ce: 0.005176
2022-01-08 11:56:40,013 iteration 6300 : loss : 0.014511, loss_ce: 0.005446
2022-01-08 11:56:41,350 iteration 6301 : loss : 0.015438, loss_ce: 0.009139
2022-01-08 11:56:42,738 iteration 6302 : loss : 0.013526, loss_ce: 0.004858
2022-01-08 11:56:44,021 iteration 6303 : loss : 0.010574, loss_ce: 0.002396
2022-01-08 11:56:45,315 iteration 6304 : loss : 0.013596, loss_ce: 0.003637
2022-01-08 11:56:46,701 iteration 6305 : loss : 0.017954, loss_ce: 0.008279
2022-01-08 11:56:48,042 iteration 6306 : loss : 0.020993, loss_ce: 0.004837
2022-01-08 11:56:49,384 iteration 6307 : loss : 0.013267, loss_ce: 0.006048
 93%|██████████████████████████▉  | 371/400 [2:36:20<12:23, 25.63s/it]2022-01-08 11:56:50,734 iteration 6308 : loss : 0.019398, loss_ce: 0.003859
2022-01-08 11:56:52,099 iteration 6309 : loss : 0.018240, loss_ce: 0.007246
2022-01-08 11:56:53,449 iteration 6310 : loss : 0.014284, loss_ce: 0.005778
2022-01-08 11:56:54,816 iteration 6311 : loss : 0.009764, loss_ce: 0.004206
2022-01-08 11:56:56,199 iteration 6312 : loss : 0.023077, loss_ce: 0.007553
2022-01-08 11:56:57,636 iteration 6313 : loss : 0.026341, loss_ce: 0.011901
2022-01-08 11:56:58,988 iteration 6314 : loss : 0.014037, loss_ce: 0.005067
2022-01-08 11:57:00,313 iteration 6315 : loss : 0.012455, loss_ce: 0.004357
2022-01-08 11:57:01,668 iteration 6316 : loss : 0.013644, loss_ce: 0.004821
2022-01-08 11:57:03,008 iteration 6317 : loss : 0.024774, loss_ce: 0.006726
2022-01-08 11:57:04,432 iteration 6318 : loss : 0.016898, loss_ce: 0.006796
2022-01-08 11:57:05,812 iteration 6319 : loss : 0.010323, loss_ce: 0.003879
2022-01-08 11:57:07,159 iteration 6320 : loss : 0.013282, loss_ce: 0.005389
2022-01-08 11:57:08,557 iteration 6321 : loss : 0.016042, loss_ce: 0.008563
2022-01-08 11:57:09,972 iteration 6322 : loss : 0.017604, loss_ce: 0.008138
2022-01-08 11:57:11,470 iteration 6323 : loss : 0.015635, loss_ce: 0.005339
2022-01-08 11:57:12,903 iteration 6324 : loss : 0.015513, loss_ce: 0.007340
 93%|██████████████████████████▉  | 372/400 [2:36:43<11:39, 24.99s/it]2022-01-08 11:57:14,331 iteration 6325 : loss : 0.016261, loss_ce: 0.004431
2022-01-08 11:57:15,655 iteration 6326 : loss : 0.021110, loss_ce: 0.007820
2022-01-08 11:57:17,006 iteration 6327 : loss : 0.020486, loss_ce: 0.007450
2022-01-08 11:57:18,415 iteration 6328 : loss : 0.011120, loss_ce: 0.005446
2022-01-08 11:57:19,719 iteration 6329 : loss : 0.011663, loss_ce: 0.005718
2022-01-08 11:57:21,077 iteration 6330 : loss : 0.015430, loss_ce: 0.005895
2022-01-08 11:57:22,499 iteration 6331 : loss : 0.011551, loss_ce: 0.004190
2022-01-08 11:57:23,890 iteration 6332 : loss : 0.011877, loss_ce: 0.003382
2022-01-08 11:57:25,249 iteration 6333 : loss : 0.010435, loss_ce: 0.004678
2022-01-08 11:57:26,625 iteration 6334 : loss : 0.014460, loss_ce: 0.003787
2022-01-08 11:57:28,072 iteration 6335 : loss : 0.015600, loss_ce: 0.006355
2022-01-08 11:57:29,401 iteration 6336 : loss : 0.011487, loss_ce: 0.004563
2022-01-08 11:57:30,775 iteration 6337 : loss : 0.018275, loss_ce: 0.006277
2022-01-08 11:57:32,148 iteration 6338 : loss : 0.015061, loss_ce: 0.004608
2022-01-08 11:57:33,503 iteration 6339 : loss : 0.012183, loss_ce: 0.004100
2022-01-08 11:57:34,869 iteration 6340 : loss : 0.015103, loss_ce: 0.006769
2022-01-08 11:57:36,158 iteration 6341 : loss : 0.009267, loss_ce: 0.003043
 93%|███████████████████████████  | 373/400 [2:37:06<11:00, 24.47s/it]2022-01-08 11:57:37,610 iteration 6342 : loss : 0.019938, loss_ce: 0.006036
2022-01-08 11:57:38,953 iteration 6343 : loss : 0.012372, loss_ce: 0.004696
2022-01-08 11:57:40,289 iteration 6344 : loss : 0.015558, loss_ce: 0.004944
2022-01-08 11:57:41,622 iteration 6345 : loss : 0.034259, loss_ce: 0.003630
2022-01-08 11:57:43,006 iteration 6346 : loss : 0.015824, loss_ce: 0.005287
2022-01-08 11:57:44,375 iteration 6347 : loss : 0.013244, loss_ce: 0.005119
2022-01-08 11:57:45,647 iteration 6348 : loss : 0.008940, loss_ce: 0.003722
2022-01-08 11:57:47,016 iteration 6349 : loss : 0.018707, loss_ce: 0.008258
2022-01-08 11:57:48,501 iteration 6350 : loss : 0.020125, loss_ce: 0.007578
2022-01-08 11:57:49,925 iteration 6351 : loss : 0.013203, loss_ce: 0.005603
2022-01-08 11:57:51,289 iteration 6352 : loss : 0.025563, loss_ce: 0.006492
2022-01-08 11:57:52,662 iteration 6353 : loss : 0.025862, loss_ce: 0.012772
2022-01-08 11:57:54,021 iteration 6354 : loss : 0.015371, loss_ce: 0.005991
2022-01-08 11:57:55,367 iteration 6355 : loss : 0.022168, loss_ce: 0.008253
2022-01-08 11:57:56,796 iteration 6356 : loss : 0.022332, loss_ce: 0.007281
2022-01-08 11:57:58,121 iteration 6357 : loss : 0.014958, loss_ce: 0.005018
2022-01-08 11:57:59,584 iteration 6358 : loss : 0.018550, loss_ce: 0.008523
 94%|███████████████████████████  | 374/400 [2:37:30<10:28, 24.16s/it]2022-01-08 11:58:01,032 iteration 6359 : loss : 0.017446, loss_ce: 0.004738
2022-01-08 11:58:02,329 iteration 6360 : loss : 0.015023, loss_ce: 0.005982
2022-01-08 11:58:03,717 iteration 6361 : loss : 0.014249, loss_ce: 0.005072
2022-01-08 11:58:05,118 iteration 6362 : loss : 0.016428, loss_ce: 0.006385
2022-01-08 11:58:06,525 iteration 6363 : loss : 0.022025, loss_ce: 0.008748
2022-01-08 11:58:07,856 iteration 6364 : loss : 0.011911, loss_ce: 0.003350
2022-01-08 11:58:09,239 iteration 6365 : loss : 0.014646, loss_ce: 0.007548
2022-01-08 11:58:10,695 iteration 6366 : loss : 0.014380, loss_ce: 0.005280
2022-01-08 11:58:12,178 iteration 6367 : loss : 0.031556, loss_ce: 0.014425
2022-01-08 11:58:13,587 iteration 6368 : loss : 0.019288, loss_ce: 0.008805
2022-01-08 11:58:14,953 iteration 6369 : loss : 0.009904, loss_ce: 0.003987
2022-01-08 11:58:16,316 iteration 6370 : loss : 0.016184, loss_ce: 0.005274
2022-01-08 11:58:17,768 iteration 6371 : loss : 0.024524, loss_ce: 0.009678
2022-01-08 11:58:19,094 iteration 6372 : loss : 0.009978, loss_ce: 0.004145
2022-01-08 11:58:20,421 iteration 6373 : loss : 0.012311, loss_ce: 0.004788
2022-01-08 11:58:21,797 iteration 6374 : loss : 0.018440, loss_ce: 0.007819
2022-01-08 11:58:21,797 Training Data Eval:
2022-01-08 11:58:28,674   Average segmentation loss on training set: 0.0075
2022-01-08 11:58:28,675 Validation Data Eval:
2022-01-08 11:58:31,036   Average segmentation loss on validation set: 0.0626
2022-01-08 11:58:32,428 iteration 6375 : loss : 0.017202, loss_ce: 0.005813
 94%|███████████████████████████▏ | 375/400 [2:38:03<11:09, 26.76s/it]2022-01-08 11:58:33,882 iteration 6376 : loss : 0.016927, loss_ce: 0.004902
2022-01-08 11:58:35,262 iteration 6377 : loss : 0.013571, loss_ce: 0.004915
2022-01-08 11:58:36,534 iteration 6378 : loss : 0.010712, loss_ce: 0.004905
2022-01-08 11:58:37,896 iteration 6379 : loss : 0.018385, loss_ce: 0.004241
2022-01-08 11:58:39,229 iteration 6380 : loss : 0.016249, loss_ce: 0.005928
2022-01-08 11:58:40,554 iteration 6381 : loss : 0.009708, loss_ce: 0.002972
2022-01-08 11:58:41,838 iteration 6382 : loss : 0.012380, loss_ce: 0.004458
2022-01-08 11:58:43,242 iteration 6383 : loss : 0.016258, loss_ce: 0.006020
2022-01-08 11:58:44,562 iteration 6384 : loss : 0.015432, loss_ce: 0.006040
2022-01-08 11:58:45,958 iteration 6385 : loss : 0.014763, loss_ce: 0.004720
2022-01-08 11:58:47,377 iteration 6386 : loss : 0.015678, loss_ce: 0.004882
2022-01-08 11:58:48,828 iteration 6387 : loss : 0.018125, loss_ce: 0.007673
2022-01-08 11:58:50,230 iteration 6388 : loss : 0.014276, loss_ce: 0.006105
2022-01-08 11:58:51,489 iteration 6389 : loss : 0.011315, loss_ce: 0.003962
2022-01-08 11:58:52,872 iteration 6390 : loss : 0.013250, loss_ce: 0.006192
2022-01-08 11:58:54,201 iteration 6391 : loss : 0.010916, loss_ce: 0.005808
2022-01-08 11:58:55,630 iteration 6392 : loss : 0.022104, loss_ce: 0.005777
 94%|███████████████████████████▎ | 376/400 [2:38:26<10:16, 25.70s/it]2022-01-08 11:58:57,020 iteration 6393 : loss : 0.013340, loss_ce: 0.005055
2022-01-08 11:58:58,428 iteration 6394 : loss : 0.017737, loss_ce: 0.006750
2022-01-08 11:58:59,793 iteration 6395 : loss : 0.016005, loss_ce: 0.004689
2022-01-08 11:59:01,156 iteration 6396 : loss : 0.013533, loss_ce: 0.004547
2022-01-08 11:59:02,524 iteration 6397 : loss : 0.013905, loss_ce: 0.005011
2022-01-08 11:59:03,910 iteration 6398 : loss : 0.014588, loss_ce: 0.005720
2022-01-08 11:59:05,260 iteration 6399 : loss : 0.019843, loss_ce: 0.006613
2022-01-08 11:59:06,603 iteration 6400 : loss : 0.010930, loss_ce: 0.004558
2022-01-08 11:59:08,000 iteration 6401 : loss : 0.012966, loss_ce: 0.005234
2022-01-08 11:59:09,358 iteration 6402 : loss : 0.014008, loss_ce: 0.007164
2022-01-08 11:59:10,798 iteration 6403 : loss : 0.016794, loss_ce: 0.006999
2022-01-08 11:59:12,141 iteration 6404 : loss : 0.013944, loss_ce: 0.006468
2022-01-08 11:59:13,468 iteration 6405 : loss : 0.017590, loss_ce: 0.005382
2022-01-08 11:59:14,975 iteration 6406 : loss : 0.041739, loss_ce: 0.009668
2022-01-08 11:59:16,364 iteration 6407 : loss : 0.024732, loss_ce: 0.006265
2022-01-08 11:59:17,628 iteration 6408 : loss : 0.009642, loss_ce: 0.003370
2022-01-08 11:59:18,975 iteration 6409 : loss : 0.021807, loss_ce: 0.010351
 94%|███████████████████████████▎ | 377/400 [2:38:49<09:34, 24.99s/it]2022-01-08 11:59:20,343 iteration 6410 : loss : 0.016681, loss_ce: 0.005602
2022-01-08 11:59:21,739 iteration 6411 : loss : 0.016225, loss_ce: 0.006297
2022-01-08 11:59:23,130 iteration 6412 : loss : 0.011361, loss_ce: 0.003910
2022-01-08 11:59:24,486 iteration 6413 : loss : 0.013772, loss_ce: 0.005982
2022-01-08 11:59:25,828 iteration 6414 : loss : 0.016427, loss_ce: 0.007359
2022-01-08 11:59:27,145 iteration 6415 : loss : 0.009766, loss_ce: 0.003398
2022-01-08 11:59:28,521 iteration 6416 : loss : 0.010975, loss_ce: 0.004514
2022-01-08 11:59:29,817 iteration 6417 : loss : 0.014107, loss_ce: 0.005183
2022-01-08 11:59:31,152 iteration 6418 : loss : 0.015597, loss_ce: 0.006553
2022-01-08 11:59:32,597 iteration 6419 : loss : 0.027654, loss_ce: 0.010126
2022-01-08 11:59:34,022 iteration 6420 : loss : 0.024059, loss_ce: 0.007636
2022-01-08 11:59:35,411 iteration 6421 : loss : 0.015261, loss_ce: 0.005983
2022-01-08 11:59:36,774 iteration 6422 : loss : 0.017026, loss_ce: 0.005098
2022-01-08 11:59:38,162 iteration 6423 : loss : 0.014907, loss_ce: 0.006675
2022-01-08 11:59:39,619 iteration 6424 : loss : 0.021450, loss_ce: 0.012159
2022-01-08 11:59:40,977 iteration 6425 : loss : 0.014829, loss_ce: 0.004964
2022-01-08 11:59:42,414 iteration 6426 : loss : 0.018988, loss_ce: 0.006965
 94%|███████████████████████████▍ | 378/400 [2:39:13<08:59, 24.52s/it]2022-01-08 11:59:43,848 iteration 6427 : loss : 0.013892, loss_ce: 0.004064
2022-01-08 11:59:45,216 iteration 6428 : loss : 0.017183, loss_ce: 0.005053
2022-01-08 11:59:46,590 iteration 6429 : loss : 0.020017, loss_ce: 0.006651
2022-01-08 11:59:48,000 iteration 6430 : loss : 0.018365, loss_ce: 0.007629
2022-01-08 11:59:49,306 iteration 6431 : loss : 0.012241, loss_ce: 0.004370
2022-01-08 11:59:50,688 iteration 6432 : loss : 0.013609, loss_ce: 0.005602
2022-01-08 11:59:52,055 iteration 6433 : loss : 0.015270, loss_ce: 0.004891
2022-01-08 11:59:53,388 iteration 6434 : loss : 0.013423, loss_ce: 0.005346
2022-01-08 11:59:54,745 iteration 6435 : loss : 0.014149, loss_ce: 0.005101
2022-01-08 11:59:56,098 iteration 6436 : loss : 0.011579, loss_ce: 0.004621
2022-01-08 11:59:57,472 iteration 6437 : loss : 0.013611, loss_ce: 0.004368
2022-01-08 11:59:58,876 iteration 6438 : loss : 0.019880, loss_ce: 0.005202
2022-01-08 12:00:00,199 iteration 6439 : loss : 0.013432, loss_ce: 0.005568
2022-01-08 12:00:01,607 iteration 6440 : loss : 0.018376, loss_ce: 0.009615
2022-01-08 12:00:02,918 iteration 6441 : loss : 0.009942, loss_ce: 0.004458
2022-01-08 12:00:04,300 iteration 6442 : loss : 0.014917, loss_ce: 0.004692
2022-01-08 12:00:05,584 iteration 6443 : loss : 0.010855, loss_ce: 0.003474
 95%|███████████████████████████▍ | 379/400 [2:39:36<08:26, 24.12s/it]2022-01-08 12:00:07,006 iteration 6444 : loss : 0.012892, loss_ce: 0.005947
2022-01-08 12:00:08,365 iteration 6445 : loss : 0.015691, loss_ce: 0.006744
2022-01-08 12:00:09,743 iteration 6446 : loss : 0.014953, loss_ce: 0.006167
2022-01-08 12:00:11,142 iteration 6447 : loss : 0.038772, loss_ce: 0.005088
2022-01-08 12:00:12,545 iteration 6448 : loss : 0.013046, loss_ce: 0.004852
2022-01-08 12:00:13,878 iteration 6449 : loss : 0.013120, loss_ce: 0.003268
2022-01-08 12:00:15,194 iteration 6450 : loss : 0.013871, loss_ce: 0.006423
2022-01-08 12:00:16,561 iteration 6451 : loss : 0.011787, loss_ce: 0.003914
2022-01-08 12:00:17,991 iteration 6452 : loss : 0.015895, loss_ce: 0.005437
2022-01-08 12:00:19,339 iteration 6453 : loss : 0.017711, loss_ce: 0.006733
2022-01-08 12:00:20,645 iteration 6454 : loss : 0.014419, loss_ce: 0.005592
2022-01-08 12:00:22,052 iteration 6455 : loss : 0.014369, loss_ce: 0.006632
2022-01-08 12:00:23,406 iteration 6456 : loss : 0.017959, loss_ce: 0.006348
2022-01-08 12:00:24,795 iteration 6457 : loss : 0.015500, loss_ce: 0.005691
2022-01-08 12:00:26,118 iteration 6458 : loss : 0.013493, loss_ce: 0.005066
2022-01-08 12:00:27,469 iteration 6459 : loss : 0.008888, loss_ce: 0.002179
2022-01-08 12:00:27,469 Training Data Eval:
2022-01-08 12:00:34,365   Average segmentation loss on training set: 0.0074
2022-01-08 12:00:34,365 Validation Data Eval:
2022-01-08 12:00:36,727   Average segmentation loss on validation set: 0.0675
2022-01-08 12:00:38,147 iteration 6460 : loss : 0.017156, loss_ce: 0.008120
 95%|███████████████████████████▌ | 380/400 [2:40:08<08:53, 26.65s/it]2022-01-08 12:00:39,610 iteration 6461 : loss : 0.016942, loss_ce: 0.006118
2022-01-08 12:00:40,946 iteration 6462 : loss : 0.011084, loss_ce: 0.004882
2022-01-08 12:00:42,366 iteration 6463 : loss : 0.024823, loss_ce: 0.012499
2022-01-08 12:00:43,735 iteration 6464 : loss : 0.030446, loss_ce: 0.009978
2022-01-08 12:00:45,143 iteration 6465 : loss : 0.015473, loss_ce: 0.005448
2022-01-08 12:00:46,463 iteration 6466 : loss : 0.012028, loss_ce: 0.003704
2022-01-08 12:00:47,814 iteration 6467 : loss : 0.015724, loss_ce: 0.005989
2022-01-08 12:00:49,247 iteration 6468 : loss : 0.016643, loss_ce: 0.005949
2022-01-08 12:00:50,595 iteration 6469 : loss : 0.011874, loss_ce: 0.004998
2022-01-08 12:00:51,920 iteration 6470 : loss : 0.010710, loss_ce: 0.003217
2022-01-08 12:00:53,297 iteration 6471 : loss : 0.014264, loss_ce: 0.005543
2022-01-08 12:00:54,671 iteration 6472 : loss : 0.017286, loss_ce: 0.005552
2022-01-08 12:00:56,004 iteration 6473 : loss : 0.012906, loss_ce: 0.004309
2022-01-08 12:00:57,433 iteration 6474 : loss : 0.012701, loss_ce: 0.006645
2022-01-08 12:00:58,818 iteration 6475 : loss : 0.017807, loss_ce: 0.009366
2022-01-08 12:01:00,074 iteration 6476 : loss : 0.011226, loss_ce: 0.004090
2022-01-08 12:01:01,528 iteration 6477 : loss : 0.020063, loss_ce: 0.008476
 95%|███████████████████████████▌ | 381/400 [2:40:32<08:07, 25.67s/it]2022-01-08 12:01:02,989 iteration 6478 : loss : 0.016103, loss_ce: 0.005280
2022-01-08 12:01:04,338 iteration 6479 : loss : 0.019792, loss_ce: 0.006974
2022-01-08 12:01:05,678 iteration 6480 : loss : 0.015847, loss_ce: 0.007701
2022-01-08 12:01:07,072 iteration 6481 : loss : 0.019175, loss_ce: 0.008899
2022-01-08 12:01:08,372 iteration 6482 : loss : 0.010740, loss_ce: 0.003961
2022-01-08 12:01:09,764 iteration 6483 : loss : 0.014407, loss_ce: 0.003663
2022-01-08 12:01:11,084 iteration 6484 : loss : 0.012251, loss_ce: 0.005005
2022-01-08 12:01:12,443 iteration 6485 : loss : 0.017945, loss_ce: 0.007607
2022-01-08 12:01:13,854 iteration 6486 : loss : 0.014653, loss_ce: 0.005394
2022-01-08 12:01:15,311 iteration 6487 : loss : 0.022175, loss_ce: 0.010500
2022-01-08 12:01:16,699 iteration 6488 : loss : 0.017039, loss_ce: 0.006805
2022-01-08 12:01:18,031 iteration 6489 : loss : 0.009982, loss_ce: 0.004681
2022-01-08 12:01:19,419 iteration 6490 : loss : 0.015165, loss_ce: 0.004231
2022-01-08 12:01:20,757 iteration 6491 : loss : 0.015638, loss_ce: 0.006751
2022-01-08 12:01:22,220 iteration 6492 : loss : 0.035387, loss_ce: 0.017042
2022-01-08 12:01:23,626 iteration 6493 : loss : 0.018400, loss_ce: 0.006442
2022-01-08 12:01:24,896 iteration 6494 : loss : 0.008285, loss_ce: 0.003245
 96%|███████████████████████████▋ | 382/400 [2:40:55<07:29, 24.98s/it]2022-01-08 12:01:26,275 iteration 6495 : loss : 0.013138, loss_ce: 0.005408
2022-01-08 12:01:27,640 iteration 6496 : loss : 0.023027, loss_ce: 0.006638
2022-01-08 12:01:29,045 iteration 6497 : loss : 0.013532, loss_ce: 0.005129
2022-01-08 12:01:30,439 iteration 6498 : loss : 0.020798, loss_ce: 0.008076
2022-01-08 12:01:31,840 iteration 6499 : loss : 0.015256, loss_ce: 0.006776
2022-01-08 12:01:33,198 iteration 6500 : loss : 0.012379, loss_ce: 0.004892
2022-01-08 12:01:34,583 iteration 6501 : loss : 0.018674, loss_ce: 0.007558
2022-01-08 12:01:35,996 iteration 6502 : loss : 0.019606, loss_ce: 0.010729
2022-01-08 12:01:37,333 iteration 6503 : loss : 0.008394, loss_ce: 0.002174
2022-01-08 12:01:38,638 iteration 6504 : loss : 0.010805, loss_ce: 0.003912
2022-01-08 12:01:39,883 iteration 6505 : loss : 0.010661, loss_ce: 0.004961
2022-01-08 12:01:41,245 iteration 6506 : loss : 0.018134, loss_ce: 0.004566
2022-01-08 12:01:42,684 iteration 6507 : loss : 0.021533, loss_ce: 0.012014
2022-01-08 12:01:44,076 iteration 6508 : loss : 0.014891, loss_ce: 0.005766
2022-01-08 12:01:45,446 iteration 6509 : loss : 0.016329, loss_ce: 0.006206
2022-01-08 12:01:46,821 iteration 6510 : loss : 0.015983, loss_ce: 0.008009
2022-01-08 12:01:48,109 iteration 6511 : loss : 0.011349, loss_ce: 0.004214
 96%|███████████████████████████▊ | 383/400 [2:41:18<06:55, 24.45s/it]2022-01-08 12:01:49,475 iteration 6512 : loss : 0.013447, loss_ce: 0.004530
2022-01-08 12:01:50,848 iteration 6513 : loss : 0.013885, loss_ce: 0.004981
2022-01-08 12:01:52,308 iteration 6514 : loss : 0.012246, loss_ce: 0.003572
2022-01-08 12:01:53,734 iteration 6515 : loss : 0.019804, loss_ce: 0.006368
2022-01-08 12:01:55,132 iteration 6516 : loss : 0.020937, loss_ce: 0.008671
2022-01-08 12:01:56,491 iteration 6517 : loss : 0.016778, loss_ce: 0.003427
2022-01-08 12:01:57,853 iteration 6518 : loss : 0.011574, loss_ce: 0.003927
2022-01-08 12:01:59,221 iteration 6519 : loss : 0.013195, loss_ce: 0.003887
2022-01-08 12:02:00,634 iteration 6520 : loss : 0.016566, loss_ce: 0.006351
2022-01-08 12:02:01,970 iteration 6521 : loss : 0.015590, loss_ce: 0.007142
2022-01-08 12:02:03,358 iteration 6522 : loss : 0.012540, loss_ce: 0.005025
2022-01-08 12:02:04,651 iteration 6523 : loss : 0.010019, loss_ce: 0.004134
2022-01-08 12:02:06,071 iteration 6524 : loss : 0.022250, loss_ce: 0.009274
2022-01-08 12:02:07,456 iteration 6525 : loss : 0.017185, loss_ce: 0.006530
2022-01-08 12:02:08,824 iteration 6526 : loss : 0.013622, loss_ce: 0.005409
2022-01-08 12:02:10,119 iteration 6527 : loss : 0.011703, loss_ce: 0.005088
2022-01-08 12:02:11,453 iteration 6528 : loss : 0.024170, loss_ce: 0.009073
 96%|███████████████████████████▊ | 384/400 [2:41:42<06:25, 24.12s/it]2022-01-08 12:02:12,913 iteration 6529 : loss : 0.022224, loss_ce: 0.011692
2022-01-08 12:02:14,307 iteration 6530 : loss : 0.021251, loss_ce: 0.007920
2022-01-08 12:02:15,611 iteration 6531 : loss : 0.013678, loss_ce: 0.004044
2022-01-08 12:02:17,002 iteration 6532 : loss : 0.020450, loss_ce: 0.007611
2022-01-08 12:02:18,298 iteration 6533 : loss : 0.008109, loss_ce: 0.003173
2022-01-08 12:02:19,663 iteration 6534 : loss : 0.016179, loss_ce: 0.007306
2022-01-08 12:02:21,042 iteration 6535 : loss : 0.018241, loss_ce: 0.007481
2022-01-08 12:02:22,434 iteration 6536 : loss : 0.014034, loss_ce: 0.004722
2022-01-08 12:02:23,823 iteration 6537 : loss : 0.020367, loss_ce: 0.006849
2022-01-08 12:02:25,218 iteration 6538 : loss : 0.012348, loss_ce: 0.003875
2022-01-08 12:02:26,630 iteration 6539 : loss : 0.016391, loss_ce: 0.007167
2022-01-08 12:02:27,924 iteration 6540 : loss : 0.010002, loss_ce: 0.004182
2022-01-08 12:02:29,269 iteration 6541 : loss : 0.016900, loss_ce: 0.010320
2022-01-08 12:02:30,724 iteration 6542 : loss : 0.012114, loss_ce: 0.004148
2022-01-08 12:02:32,053 iteration 6543 : loss : 0.012240, loss_ce: 0.004454
2022-01-08 12:02:33,472 iteration 6544 : loss : 0.017149, loss_ce: 0.006666
2022-01-08 12:02:33,473 Training Data Eval:
2022-01-08 12:02:40,337   Average segmentation loss on training set: 0.0071
2022-01-08 12:02:40,338 Validation Data Eval:
2022-01-08 12:02:42,720   Average segmentation loss on validation set: 0.0688
2022-01-08 12:02:44,174 iteration 6545 : loss : 0.014477, loss_ce: 0.006028
 96%|███████████████████████████▉ | 385/400 [2:42:14<06:40, 26.70s/it]2022-01-08 12:02:45,720 iteration 6546 : loss : 0.013355, loss_ce: 0.005671
2022-01-08 12:02:47,079 iteration 6547 : loss : 0.017882, loss_ce: 0.007322
2022-01-08 12:02:48,475 iteration 6548 : loss : 0.012283, loss_ce: 0.005457
2022-01-08 12:02:49,809 iteration 6549 : loss : 0.025421, loss_ce: 0.008496
2022-01-08 12:02:51,219 iteration 6550 : loss : 0.018302, loss_ce: 0.006097
2022-01-08 12:02:52,649 iteration 6551 : loss : 0.017324, loss_ce: 0.007962
2022-01-08 12:02:54,108 iteration 6552 : loss : 0.021625, loss_ce: 0.008732
2022-01-08 12:02:55,584 iteration 6553 : loss : 0.021956, loss_ce: 0.007437
2022-01-08 12:02:56,963 iteration 6554 : loss : 0.015025, loss_ce: 0.005300
2022-01-08 12:02:58,333 iteration 6555 : loss : 0.010002, loss_ce: 0.003852
2022-01-08 12:02:59,716 iteration 6556 : loss : 0.014640, loss_ce: 0.003743
2022-01-08 12:03:00,989 iteration 6557 : loss : 0.008517, loss_ce: 0.003505
2022-01-08 12:03:02,363 iteration 6558 : loss : 0.014481, loss_ce: 0.004691
2022-01-08 12:03:03,665 iteration 6559 : loss : 0.011066, loss_ce: 0.003735
2022-01-08 12:03:05,039 iteration 6560 : loss : 0.014602, loss_ce: 0.007029
2022-01-08 12:03:06,446 iteration 6561 : loss : 0.014105, loss_ce: 0.004105
2022-01-08 12:03:07,770 iteration 6562 : loss : 0.011316, loss_ce: 0.004279
 96%|███████████████████████████▉ | 386/400 [2:42:38<06:00, 25.77s/it]2022-01-08 12:03:09,298 iteration 6563 : loss : 0.018725, loss_ce: 0.008324
2022-01-08 12:03:10,640 iteration 6564 : loss : 0.019824, loss_ce: 0.009527
2022-01-08 12:03:11,963 iteration 6565 : loss : 0.016034, loss_ce: 0.003703
2022-01-08 12:03:13,286 iteration 6566 : loss : 0.011839, loss_ce: 0.004834
2022-01-08 12:03:14,570 iteration 6567 : loss : 0.011695, loss_ce: 0.005355
2022-01-08 12:03:15,908 iteration 6568 : loss : 0.011678, loss_ce: 0.004868
2022-01-08 12:03:17,281 iteration 6569 : loss : 0.015050, loss_ce: 0.005677
2022-01-08 12:03:18,601 iteration 6570 : loss : 0.009971, loss_ce: 0.004690
2022-01-08 12:03:19,950 iteration 6571 : loss : 0.014389, loss_ce: 0.004441
2022-01-08 12:03:21,235 iteration 6572 : loss : 0.009800, loss_ce: 0.004160
2022-01-08 12:03:22,664 iteration 6573 : loss : 0.017554, loss_ce: 0.005541
2022-01-08 12:03:24,097 iteration 6574 : loss : 0.011190, loss_ce: 0.005306
2022-01-08 12:03:25,534 iteration 6575 : loss : 0.046274, loss_ce: 0.010408
2022-01-08 12:03:26,869 iteration 6576 : loss : 0.012400, loss_ce: 0.005202
2022-01-08 12:03:28,249 iteration 6577 : loss : 0.028731, loss_ce: 0.006564
2022-01-08 12:03:29,572 iteration 6578 : loss : 0.010322, loss_ce: 0.003944
2022-01-08 12:03:30,963 iteration 6579 : loss : 0.012912, loss_ce: 0.004937
 97%|████████████████████████████ | 387/400 [2:43:01<05:24, 24.99s/it]2022-01-08 12:03:32,376 iteration 6580 : loss : 0.014789, loss_ce: 0.004334
2022-01-08 12:03:33,708 iteration 6581 : loss : 0.020139, loss_ce: 0.007005
2022-01-08 12:03:35,131 iteration 6582 : loss : 0.014133, loss_ce: 0.005135
2022-01-08 12:03:36,503 iteration 6583 : loss : 0.019290, loss_ce: 0.007806
2022-01-08 12:03:37,886 iteration 6584 : loss : 0.014583, loss_ce: 0.005955
2022-01-08 12:03:39,310 iteration 6585 : loss : 0.023751, loss_ce: 0.007556
2022-01-08 12:03:40,708 iteration 6586 : loss : 0.022564, loss_ce: 0.008086
2022-01-08 12:03:42,060 iteration 6587 : loss : 0.010763, loss_ce: 0.004306
2022-01-08 12:03:43,447 iteration 6588 : loss : 0.015795, loss_ce: 0.006849
2022-01-08 12:03:44,776 iteration 6589 : loss : 0.024206, loss_ce: 0.009126
2022-01-08 12:03:46,220 iteration 6590 : loss : 0.018871, loss_ce: 0.008445
2022-01-08 12:03:47,635 iteration 6591 : loss : 0.017219, loss_ce: 0.007019
2022-01-08 12:03:48,966 iteration 6592 : loss : 0.010923, loss_ce: 0.004036
2022-01-08 12:03:50,398 iteration 6593 : loss : 0.019964, loss_ce: 0.004012
2022-01-08 12:03:51,746 iteration 6594 : loss : 0.012723, loss_ce: 0.006133
2022-01-08 12:03:53,090 iteration 6595 : loss : 0.011305, loss_ce: 0.003767
2022-01-08 12:03:54,512 iteration 6596 : loss : 0.017126, loss_ce: 0.006952
 97%|████████████████████████████▏| 388/400 [2:43:25<04:54, 24.56s/it]2022-01-08 12:03:55,916 iteration 6597 : loss : 0.013160, loss_ce: 0.005566
2022-01-08 12:03:57,235 iteration 6598 : loss : 0.017689, loss_ce: 0.003588
2022-01-08 12:03:58,621 iteration 6599 : loss : 0.011685, loss_ce: 0.004682
2022-01-08 12:03:59,878 iteration 6600 : loss : 0.008728, loss_ce: 0.003652
2022-01-08 12:04:01,239 iteration 6601 : loss : 0.012322, loss_ce: 0.004209
2022-01-08 12:04:02,578 iteration 6602 : loss : 0.016036, loss_ce: 0.004983
2022-01-08 12:04:03,945 iteration 6603 : loss : 0.013945, loss_ce: 0.005120
2022-01-08 12:04:05,233 iteration 6604 : loss : 0.008545, loss_ce: 0.003476
2022-01-08 12:04:06,616 iteration 6605 : loss : 0.011035, loss_ce: 0.003838
2022-01-08 12:04:08,007 iteration 6606 : loss : 0.015481, loss_ce: 0.007303
2022-01-08 12:04:09,395 iteration 6607 : loss : 0.014692, loss_ce: 0.005912
2022-01-08 12:04:10,774 iteration 6608 : loss : 0.011302, loss_ce: 0.005105
2022-01-08 12:04:12,099 iteration 6609 : loss : 0.010925, loss_ce: 0.004045
2022-01-08 12:04:13,451 iteration 6610 : loss : 0.013268, loss_ce: 0.005172
2022-01-08 12:04:14,797 iteration 6611 : loss : 0.011225, loss_ce: 0.002681
2022-01-08 12:04:16,206 iteration 6612 : loss : 0.025736, loss_ce: 0.010693
2022-01-08 12:04:17,455 iteration 6613 : loss : 0.012243, loss_ce: 0.005486
 97%|████████████████████████████▏| 389/400 [2:43:48<04:24, 24.08s/it]2022-01-08 12:04:18,857 iteration 6614 : loss : 0.014512, loss_ce: 0.006069
2022-01-08 12:04:20,228 iteration 6615 : loss : 0.016894, loss_ce: 0.007879
2022-01-08 12:04:21,576 iteration 6616 : loss : 0.014298, loss_ce: 0.004423
2022-01-08 12:04:23,071 iteration 6617 : loss : 0.021257, loss_ce: 0.007089
2022-01-08 12:04:24,461 iteration 6618 : loss : 0.013638, loss_ce: 0.003981
2022-01-08 12:04:25,916 iteration 6619 : loss : 0.032860, loss_ce: 0.013931
2022-01-08 12:04:27,334 iteration 6620 : loss : 0.024672, loss_ce: 0.012082
2022-01-08 12:04:28,689 iteration 6621 : loss : 0.014701, loss_ce: 0.004176
2022-01-08 12:04:30,052 iteration 6622 : loss : 0.014495, loss_ce: 0.007638
2022-01-08 12:04:31,419 iteration 6623 : loss : 0.013103, loss_ce: 0.004673
2022-01-08 12:04:32,832 iteration 6624 : loss : 0.022197, loss_ce: 0.009907
2022-01-08 12:04:34,251 iteration 6625 : loss : 0.011303, loss_ce: 0.004262
2022-01-08 12:04:35,602 iteration 6626 : loss : 0.013472, loss_ce: 0.003349
2022-01-08 12:04:36,881 iteration 6627 : loss : 0.012481, loss_ce: 0.004304
2022-01-08 12:04:38,243 iteration 6628 : loss : 0.013786, loss_ce: 0.006667
2022-01-08 12:04:39,605 iteration 6629 : loss : 0.014250, loss_ce: 0.007234
2022-01-08 12:04:39,605 Training Data Eval:
2022-01-08 12:04:46,478   Average segmentation loss on training set: 0.0073
2022-01-08 12:04:46,478 Validation Data Eval:
2022-01-08 12:04:48,850   Average segmentation loss on validation set: 0.0687
2022-01-08 12:04:50,269 iteration 6630 : loss : 0.020060, loss_ce: 0.005835
 98%|████████████████████████████▎| 390/400 [2:44:20<04:26, 26.70s/it]2022-01-08 12:04:51,768 iteration 6631 : loss : 0.013038, loss_ce: 0.004672
2022-01-08 12:04:53,154 iteration 6632 : loss : 0.015303, loss_ce: 0.005939
2022-01-08 12:04:54,528 iteration 6633 : loss : 0.020693, loss_ce: 0.004885
2022-01-08 12:04:55,780 iteration 6634 : loss : 0.009874, loss_ce: 0.003080
2022-01-08 12:04:57,114 iteration 6635 : loss : 0.011358, loss_ce: 0.002728
2022-01-08 12:04:58,551 iteration 6636 : loss : 0.013451, loss_ce: 0.004891
2022-01-08 12:04:59,938 iteration 6637 : loss : 0.010353, loss_ce: 0.004577
2022-01-08 12:05:01,353 iteration 6638 : loss : 0.018468, loss_ce: 0.007256
2022-01-08 12:05:02,804 iteration 6639 : loss : 0.018901, loss_ce: 0.006402
2022-01-08 12:05:04,167 iteration 6640 : loss : 0.015815, loss_ce: 0.004860
2022-01-08 12:05:05,492 iteration 6641 : loss : 0.012056, loss_ce: 0.006133
2022-01-08 12:05:06,923 iteration 6642 : loss : 0.013576, loss_ce: 0.005763
2022-01-08 12:05:08,313 iteration 6643 : loss : 0.023646, loss_ce: 0.009718
2022-01-08 12:05:09,695 iteration 6644 : loss : 0.019928, loss_ce: 0.006943
2022-01-08 12:05:11,008 iteration 6645 : loss : 0.012471, loss_ce: 0.004935
2022-01-08 12:05:12,352 iteration 6646 : loss : 0.013321, loss_ce: 0.005620
2022-01-08 12:05:13,620 iteration 6647 : loss : 0.009845, loss_ce: 0.003195
 98%|████████████████████████████▎| 391/400 [2:44:44<03:51, 25.69s/it]2022-01-08 12:05:14,995 iteration 6648 : loss : 0.013854, loss_ce: 0.004345
2022-01-08 12:05:16,421 iteration 6649 : loss : 0.015695, loss_ce: 0.006467
2022-01-08 12:05:17,731 iteration 6650 : loss : 0.014931, loss_ce: 0.005392
2022-01-08 12:05:19,062 iteration 6651 : loss : 0.009873, loss_ce: 0.003312
2022-01-08 12:05:20,511 iteration 6652 : loss : 0.015237, loss_ce: 0.005545
2022-01-08 12:05:21,933 iteration 6653 : loss : 0.021493, loss_ce: 0.009003
2022-01-08 12:05:23,253 iteration 6654 : loss : 0.012375, loss_ce: 0.004935
2022-01-08 12:05:24,640 iteration 6655 : loss : 0.014842, loss_ce: 0.005424
2022-01-08 12:05:26,046 iteration 6656 : loss : 0.010518, loss_ce: 0.004033
2022-01-08 12:05:27,504 iteration 6657 : loss : 0.015853, loss_ce: 0.006446
2022-01-08 12:05:28,887 iteration 6658 : loss : 0.018382, loss_ce: 0.008185
2022-01-08 12:05:30,307 iteration 6659 : loss : 0.019087, loss_ce: 0.006581
2022-01-08 12:05:31,644 iteration 6660 : loss : 0.011695, loss_ce: 0.003852
2022-01-08 12:05:32,932 iteration 6661 : loss : 0.009625, loss_ce: 0.003446
2022-01-08 12:05:34,322 iteration 6662 : loss : 0.017050, loss_ce: 0.006640
2022-01-08 12:05:35,745 iteration 6663 : loss : 0.015819, loss_ce: 0.005585
2022-01-08 12:05:37,104 iteration 6664 : loss : 0.014644, loss_ce: 0.005006
 98%|████████████████████████████▍| 392/400 [2:45:07<03:20, 25.03s/it]2022-01-08 12:05:38,520 iteration 6665 : loss : 0.016221, loss_ce: 0.005406
2022-01-08 12:05:39,901 iteration 6666 : loss : 0.011869, loss_ce: 0.004430
2022-01-08 12:05:41,245 iteration 6667 : loss : 0.016515, loss_ce: 0.006010
2022-01-08 12:05:42,524 iteration 6668 : loss : 0.008715, loss_ce: 0.003562
2022-01-08 12:05:43,906 iteration 6669 : loss : 0.018985, loss_ce: 0.006526
2022-01-08 12:05:45,209 iteration 6670 : loss : 0.012605, loss_ce: 0.004342
2022-01-08 12:05:46,647 iteration 6671 : loss : 0.011772, loss_ce: 0.004224
2022-01-08 12:05:48,011 iteration 6672 : loss : 0.013325, loss_ce: 0.005936
2022-01-08 12:05:49,448 iteration 6673 : loss : 0.015531, loss_ce: 0.006933
2022-01-08 12:05:50,868 iteration 6674 : loss : 0.017207, loss_ce: 0.006006
2022-01-08 12:05:52,193 iteration 6675 : loss : 0.014061, loss_ce: 0.004396
2022-01-08 12:05:53,487 iteration 6676 : loss : 0.009773, loss_ce: 0.003813
2022-01-08 12:05:54,879 iteration 6677 : loss : 0.010313, loss_ce: 0.002380
2022-01-08 12:05:56,350 iteration 6678 : loss : 0.016477, loss_ce: 0.007387
2022-01-08 12:05:57,745 iteration 6679 : loss : 0.013117, loss_ce: 0.004178
2022-01-08 12:05:59,095 iteration 6680 : loss : 0.023785, loss_ce: 0.012445
2022-01-08 12:06:00,409 iteration 6681 : loss : 0.012717, loss_ce: 0.004149
 98%|████████████████████████████▍| 393/400 [2:45:31<02:51, 24.51s/it]2022-01-08 12:06:01,812 iteration 6682 : loss : 0.013364, loss_ce: 0.004028
2022-01-08 12:06:03,214 iteration 6683 : loss : 0.018537, loss_ce: 0.006345
2022-01-08 12:06:04,594 iteration 6684 : loss : 0.030789, loss_ce: 0.005725
2022-01-08 12:06:05,943 iteration 6685 : loss : 0.015540, loss_ce: 0.005523
2022-01-08 12:06:07,330 iteration 6686 : loss : 0.012480, loss_ce: 0.003840
2022-01-08 12:06:08,729 iteration 6687 : loss : 0.018483, loss_ce: 0.007703
2022-01-08 12:06:10,137 iteration 6688 : loss : 0.015810, loss_ce: 0.005304
2022-01-08 12:06:11,475 iteration 6689 : loss : 0.014369, loss_ce: 0.005925
2022-01-08 12:06:12,905 iteration 6690 : loss : 0.018673, loss_ce: 0.007124
2022-01-08 12:06:14,225 iteration 6691 : loss : 0.017079, loss_ce: 0.007275
2022-01-08 12:06:15,538 iteration 6692 : loss : 0.011074, loss_ce: 0.005727
2022-01-08 12:06:16,921 iteration 6693 : loss : 0.011375, loss_ce: 0.004700
2022-01-08 12:06:18,219 iteration 6694 : loss : 0.008911, loss_ce: 0.003635
2022-01-08 12:06:19,560 iteration 6695 : loss : 0.012443, loss_ce: 0.003370
2022-01-08 12:06:20,914 iteration 6696 : loss : 0.014666, loss_ce: 0.007745
2022-01-08 12:06:22,279 iteration 6697 : loss : 0.013383, loss_ce: 0.004873
2022-01-08 12:06:23,573 iteration 6698 : loss : 0.013873, loss_ce: 0.005112
 98%|████████████████████████████▌| 394/400 [2:45:54<02:24, 24.11s/it]2022-01-08 12:06:24,997 iteration 6699 : loss : 0.015264, loss_ce: 0.004353
2022-01-08 12:06:26,417 iteration 6700 : loss : 0.022834, loss_ce: 0.011632
2022-01-08 12:06:27,862 iteration 6701 : loss : 0.022195, loss_ce: 0.006960
2022-01-08 12:06:29,193 iteration 6702 : loss : 0.011919, loss_ce: 0.003798
2022-01-08 12:06:30,648 iteration 6703 : loss : 0.013379, loss_ce: 0.004584
2022-01-08 12:06:32,015 iteration 6704 : loss : 0.015471, loss_ce: 0.008115
2022-01-08 12:06:33,328 iteration 6705 : loss : 0.011048, loss_ce: 0.003578
2022-01-08 12:06:34,675 iteration 6706 : loss : 0.013593, loss_ce: 0.004771
2022-01-08 12:06:36,029 iteration 6707 : loss : 0.012424, loss_ce: 0.004544
2022-01-08 12:06:37,306 iteration 6708 : loss : 0.009476, loss_ce: 0.003211
2022-01-08 12:06:38,623 iteration 6709 : loss : 0.017388, loss_ce: 0.004813
2022-01-08 12:06:39,996 iteration 6710 : loss : 0.013999, loss_ce: 0.005329
2022-01-08 12:06:41,355 iteration 6711 : loss : 0.018353, loss_ce: 0.008275
2022-01-08 12:06:42,763 iteration 6712 : loss : 0.015024, loss_ce: 0.006064
2022-01-08 12:06:44,099 iteration 6713 : loss : 0.011494, loss_ce: 0.003676
2022-01-08 12:06:45,451 iteration 6714 : loss : 0.015417, loss_ce: 0.005926
2022-01-08 12:06:45,451 Training Data Eval:
2022-01-08 12:06:52,345   Average segmentation loss on training set: 0.0071
2022-01-08 12:06:52,345 Validation Data Eval:
2022-01-08 12:06:54,721   Average segmentation loss on validation set: 0.0661
2022-01-08 12:06:56,060 iteration 6715 : loss : 0.007713, loss_ce: 0.002320
 99%|████████████████████████████▋| 395/400 [2:46:26<02:13, 26.62s/it]2022-01-08 12:06:57,403 iteration 6716 : loss : 0.009631, loss_ce: 0.003236
2022-01-08 12:06:58,805 iteration 6717 : loss : 0.033935, loss_ce: 0.015961
2022-01-08 12:07:00,243 iteration 6718 : loss : 0.021716, loss_ce: 0.011438
2022-01-08 12:07:01,652 iteration 6719 : loss : 0.018339, loss_ce: 0.006113
2022-01-08 12:07:03,041 iteration 6720 : loss : 0.023254, loss_ce: 0.005522
2022-01-08 12:07:04,431 iteration 6721 : loss : 0.021200, loss_ce: 0.010974
2022-01-08 12:07:05,784 iteration 6722 : loss : 0.012951, loss_ce: 0.005348
2022-01-08 12:07:07,163 iteration 6723 : loss : 0.019315, loss_ce: 0.008236
2022-01-08 12:07:08,472 iteration 6724 : loss : 0.012966, loss_ce: 0.005371
2022-01-08 12:07:09,826 iteration 6725 : loss : 0.009688, loss_ce: 0.003288
2022-01-08 12:07:11,113 iteration 6726 : loss : 0.007729, loss_ce: 0.002590
2022-01-08 12:07:12,577 iteration 6727 : loss : 0.018291, loss_ce: 0.006254
2022-01-08 12:07:13,900 iteration 6728 : loss : 0.016515, loss_ce: 0.006910
2022-01-08 12:07:15,310 iteration 6729 : loss : 0.021707, loss_ce: 0.007942
2022-01-08 12:07:16,772 iteration 6730 : loss : 0.013272, loss_ce: 0.005620
2022-01-08 12:07:18,176 iteration 6731 : loss : 0.017451, loss_ce: 0.005956
2022-01-08 12:07:19,545 iteration 6732 : loss : 0.017351, loss_ce: 0.010050
 99%|████████████████████████████▋| 396/400 [2:46:50<01:42, 25.68s/it]2022-01-08 12:07:20,910 iteration 6733 : loss : 0.010421, loss_ce: 0.004699
2022-01-08 12:07:22,265 iteration 6734 : loss : 0.014554, loss_ce: 0.004212
2022-01-08 12:07:23,658 iteration 6735 : loss : 0.027811, loss_ce: 0.008211
2022-01-08 12:07:24,998 iteration 6736 : loss : 0.011791, loss_ce: 0.005657
2022-01-08 12:07:26,311 iteration 6737 : loss : 0.012694, loss_ce: 0.006097
2022-01-08 12:07:27,614 iteration 6738 : loss : 0.010805, loss_ce: 0.003402
2022-01-08 12:07:28,985 iteration 6739 : loss : 0.011661, loss_ce: 0.004591
2022-01-08 12:07:30,382 iteration 6740 : loss : 0.024267, loss_ce: 0.009619
2022-01-08 12:07:31,758 iteration 6741 : loss : 0.016841, loss_ce: 0.006798
2022-01-08 12:07:33,112 iteration 6742 : loss : 0.011649, loss_ce: 0.005485
2022-01-08 12:07:34,416 iteration 6743 : loss : 0.011263, loss_ce: 0.002633
2022-01-08 12:07:35,827 iteration 6744 : loss : 0.013577, loss_ce: 0.005432
2022-01-08 12:07:37,180 iteration 6745 : loss : 0.013242, loss_ce: 0.006745
2022-01-08 12:07:38,479 iteration 6746 : loss : 0.010580, loss_ce: 0.003379
2022-01-08 12:07:39,819 iteration 6747 : loss : 0.014636, loss_ce: 0.005895
2022-01-08 12:07:41,206 iteration 6748 : loss : 0.010255, loss_ce: 0.004351
2022-01-08 12:07:42,549 iteration 6749 : loss : 0.010795, loss_ce: 0.002961
 99%|████████████████████████████▊| 397/400 [2:47:13<01:14, 24.88s/it]2022-01-08 12:07:43,945 iteration 6750 : loss : 0.009415, loss_ce: 0.004146
2022-01-08 12:07:45,367 iteration 6751 : loss : 0.010637, loss_ce: 0.004323
2022-01-08 12:07:46,650 iteration 6752 : loss : 0.012355, loss_ce: 0.003804
2022-01-08 12:07:48,011 iteration 6753 : loss : 0.012348, loss_ce: 0.005288
2022-01-08 12:07:49,404 iteration 6754 : loss : 0.010861, loss_ce: 0.003824
2022-01-08 12:07:50,740 iteration 6755 : loss : 0.013721, loss_ce: 0.004096
2022-01-08 12:07:52,084 iteration 6756 : loss : 0.018700, loss_ce: 0.006469
2022-01-08 12:07:53,506 iteration 6757 : loss : 0.013373, loss_ce: 0.004153
2022-01-08 12:07:54,829 iteration 6758 : loss : 0.014624, loss_ce: 0.006494
2022-01-08 12:07:56,198 iteration 6759 : loss : 0.011303, loss_ce: 0.004682
2022-01-08 12:07:57,619 iteration 6760 : loss : 0.024977, loss_ce: 0.005286
2022-01-08 12:07:58,966 iteration 6761 : loss : 0.013043, loss_ce: 0.005132
2022-01-08 12:08:00,311 iteration 6762 : loss : 0.013478, loss_ce: 0.003989
2022-01-08 12:08:01,731 iteration 6763 : loss : 0.021231, loss_ce: 0.007290
2022-01-08 12:08:03,061 iteration 6764 : loss : 0.015467, loss_ce: 0.006518
2022-01-08 12:08:04,472 iteration 6765 : loss : 0.013030, loss_ce: 0.004899
2022-01-08 12:08:05,811 iteration 6766 : loss : 0.010089, loss_ce: 0.004832
100%|████████████████████████████▊| 398/400 [2:47:36<00:48, 24.39s/it]2022-01-08 12:08:07,206 iteration 6767 : loss : 0.014469, loss_ce: 0.005512
2022-01-08 12:08:08,644 iteration 6768 : loss : 0.026646, loss_ce: 0.009107
2022-01-08 12:08:10,017 iteration 6769 : loss : 0.014515, loss_ce: 0.006347
2022-01-08 12:08:11,387 iteration 6770 : loss : 0.014807, loss_ce: 0.007679
2022-01-08 12:08:12,839 iteration 6771 : loss : 0.021234, loss_ce: 0.004438
2022-01-08 12:08:14,164 iteration 6772 : loss : 0.010937, loss_ce: 0.003998
2022-01-08 12:08:15,411 iteration 6773 : loss : 0.010350, loss_ce: 0.003201
2022-01-08 12:08:16,803 iteration 6774 : loss : 0.017519, loss_ce: 0.005561
2022-01-08 12:08:18,189 iteration 6775 : loss : 0.019841, loss_ce: 0.006460
2022-01-08 12:08:19,542 iteration 6776 : loss : 0.015364, loss_ce: 0.004883
2022-01-08 12:08:20,979 iteration 6777 : loss : 0.016030, loss_ce: 0.008177
2022-01-08 12:08:22,319 iteration 6778 : loss : 0.012697, loss_ce: 0.004322
2022-01-08 12:08:23,767 iteration 6779 : loss : 0.015948, loss_ce: 0.006262
2022-01-08 12:08:25,148 iteration 6780 : loss : 0.011917, loss_ce: 0.005483
2022-01-08 12:08:26,563 iteration 6781 : loss : 0.015681, loss_ce: 0.006720
2022-01-08 12:08:27,979 iteration 6782 : loss : 0.012168, loss_ce: 0.004924
2022-01-08 12:08:29,315 iteration 6783 : loss : 0.012829, loss_ce: 0.004670
100%|████████████████████████████▉| 399/400 [2:47:59<00:24, 24.13s/it]2022-01-08 12:08:30,776 iteration 6784 : loss : 0.012491, loss_ce: 0.004229
2022-01-08 12:08:32,170 iteration 6785 : loss : 0.017115, loss_ce: 0.006218
2022-01-08 12:08:33,483 iteration 6786 : loss : 0.009984, loss_ce: 0.003735
2022-01-08 12:08:34,812 iteration 6787 : loss : 0.010645, loss_ce: 0.003605
2022-01-08 12:08:36,191 iteration 6788 : loss : 0.012408, loss_ce: 0.005290
2022-01-08 12:08:37,552 iteration 6789 : loss : 0.013601, loss_ce: 0.004184
2022-01-08 12:08:38,895 iteration 6790 : loss : 0.008874, loss_ce: 0.003745
2022-01-08 12:08:40,278 iteration 6791 : loss : 0.013673, loss_ce: 0.005219
2022-01-08 12:08:41,587 iteration 6792 : loss : 0.011412, loss_ce: 0.004945
2022-01-08 12:08:43,021 iteration 6793 : loss : 0.032873, loss_ce: 0.013356
2022-01-08 12:08:44,384 iteration 6794 : loss : 0.021963, loss_ce: 0.007426
2022-01-08 12:08:45,785 iteration 6795 : loss : 0.015985, loss_ce: 0.005631
2022-01-08 12:08:47,208 iteration 6796 : loss : 0.014744, loss_ce: 0.008353
2022-01-08 12:08:48,488 iteration 6797 : loss : 0.010225, loss_ce: 0.003354
2022-01-08 12:08:49,889 iteration 6798 : loss : 0.013957, loss_ce: 0.004415
2022-01-08 12:08:51,345 iteration 6799 : loss : 0.016082, loss_ce: 0.007780
2022-01-08 12:08:51,346 Training Data Eval:
2022-01-08 12:08:58,185   Average segmentation loss on training set: 0.0069
2022-01-08 12:08:58,185 Validation Data Eval:
2022-01-08 12:09:00,554   Average segmentation loss on validation set: 0.0639
2022-01-08 12:09:01,884 iteration 6800 : loss : 0.010396, loss_ce: 0.002695
100%|█████████████████████████████| 400/400 [2:48:32<00:00, 26.66s/it]100%|█████████████████████████████| 400/400 [2:48:32<00:00, 25.28s/it]
