2022-01-09 00:36:31,102 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-09 00:36:31,103 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-09 00:36:31,103 ============================================================
2022-01-09 00:36:31,103 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-09 00:36:31,103 ============================================================
2022-01-09 00:36:31,103 Loading data...
2022-01-09 00:36:31,103 Reading NCI - RUNMC images...
2022-01-09 00:36:31,103 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-09 00:36:31,104 Already preprocessed this configuration. Loading now!
2022-01-09 00:36:31,130 Training Images: (256, 256, 286)
2022-01-09 00:36:31,130 Training Labels: (256, 256, 286)
2022-01-09 00:36:31,130 Validation Images: (256, 256, 98)
2022-01-09 00:36:31,130 Validation Labels: (256, 256, 98)
2022-01-09 00:36:31,130 ============================================================
2022-01-09 00:36:31,163 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-09 00:36:33,622 iteration 1 : loss : 0.984550, loss_ce: 1.213918
2022-01-09 00:36:35,010 iteration 2 : loss : 0.923492, loss_ce: 1.112709
2022-01-09 00:36:36,542 iteration 3 : loss : 0.860080, loss_ce: 1.007163
2022-01-09 00:36:37,958 iteration 4 : loss : 0.802065, loss_ce: 0.912113
2022-01-09 00:36:39,380 iteration 5 : loss : 0.756756, loss_ce: 0.845399
2022-01-09 00:36:40,820 iteration 6 : loss : 0.722328, loss_ce: 0.776839
2022-01-09 00:36:42,330 iteration 7 : loss : 0.678134, loss_ce: 0.721151
2022-01-09 00:36:43,777 iteration 8 : loss : 0.665847, loss_ce: 0.669273
2022-01-09 00:36:45,252 iteration 9 : loss : 0.608964, loss_ce: 0.637650
2022-01-09 00:36:46,760 iteration 10 : loss : 0.606126, loss_ce: 0.574766
2022-01-09 00:36:48,341 iteration 11 : loss : 0.562474, loss_ce: 0.542886
2022-01-09 00:36:49,772 iteration 12 : loss : 0.542251, loss_ce: 0.500022
2022-01-09 00:36:51,180 iteration 13 : loss : 0.529907, loss_ce: 0.467549
2022-01-09 00:36:52,545 iteration 14 : loss : 0.502844, loss_ce: 0.438103
2022-01-09 00:36:53,990 iteration 15 : loss : 0.466962, loss_ce: 0.397446
2022-01-09 00:36:55,430 iteration 16 : loss : 0.469529, loss_ce: 0.380078
2022-01-09 00:36:56,866 iteration 17 : loss : 0.429036, loss_ce: 0.345769
  0%|                               | 1/400 [00:25<2:51:28, 25.79s/it]2022-01-09 00:36:58,433 iteration 18 : loss : 0.453878, loss_ce: 0.319656
2022-01-09 00:36:59,777 iteration 19 : loss : 0.389900, loss_ce: 0.285402
2022-01-09 00:37:01,313 iteration 20 : loss : 0.382149, loss_ce: 0.264474
2022-01-09 00:37:02,728 iteration 21 : loss : 0.392609, loss_ce: 0.238040
2022-01-09 00:37:04,166 iteration 22 : loss : 0.361592, loss_ce: 0.238956
2022-01-09 00:37:05,719 iteration 23 : loss : 0.346554, loss_ce: 0.193965
2022-01-09 00:37:07,162 iteration 24 : loss : 0.325865, loss_ce: 0.193984
2022-01-09 00:37:08,694 iteration 25 : loss : 0.366816, loss_ce: 0.232355
2022-01-09 00:37:10,113 iteration 26 : loss : 0.312805, loss_ce: 0.178977
2022-01-09 00:37:11,466 iteration 27 : loss : 0.321987, loss_ce: 0.190249
2022-01-09 00:37:12,827 iteration 28 : loss : 0.296165, loss_ce: 0.165866
2022-01-09 00:37:14,329 iteration 29 : loss : 0.298804, loss_ce: 0.158319
2022-01-09 00:37:15,815 iteration 30 : loss : 0.288283, loss_ce: 0.153418
2022-01-09 00:37:17,179 iteration 31 : loss : 0.262589, loss_ce: 0.138195
2022-01-09 00:37:18,696 iteration 32 : loss : 0.290072, loss_ce: 0.160717
2022-01-09 00:37:20,194 iteration 33 : loss : 0.294467, loss_ce: 0.161393
2022-01-09 00:37:21,684 iteration 34 : loss : 0.278155, loss_ce: 0.154255
  0%|▏                              | 2/400 [00:50<2:47:14, 25.21s/it]2022-01-09 00:37:23,282 iteration 35 : loss : 0.255090, loss_ce: 0.121922
2022-01-09 00:37:24,807 iteration 36 : loss : 0.278363, loss_ce: 0.147275
2022-01-09 00:37:26,334 iteration 37 : loss : 0.275221, loss_ce: 0.128611
2022-01-09 00:37:27,744 iteration 38 : loss : 0.243633, loss_ce: 0.121555
2022-01-09 00:37:29,157 iteration 39 : loss : 0.221669, loss_ce: 0.108967
2022-01-09 00:37:30,628 iteration 40 : loss : 0.272255, loss_ce: 0.131553
2022-01-09 00:37:32,145 iteration 41 : loss : 0.326316, loss_ce: 0.161774
2022-01-09 00:37:33,628 iteration 42 : loss : 0.246650, loss_ce: 0.120116
2022-01-09 00:37:34,990 iteration 43 : loss : 0.253276, loss_ce: 0.117309
2022-01-09 00:37:36,546 iteration 44 : loss : 0.227357, loss_ce: 0.107122
2022-01-09 00:37:38,003 iteration 45 : loss : 0.211426, loss_ce: 0.100479
2022-01-09 00:37:39,482 iteration 46 : loss : 0.242031, loss_ce: 0.101885
2022-01-09 00:37:41,010 iteration 47 : loss : 0.221446, loss_ce: 0.090396
2022-01-09 00:37:42,504 iteration 48 : loss : 0.229700, loss_ce: 0.095192
2022-01-09 00:37:44,050 iteration 49 : loss : 0.291044, loss_ce: 0.131319
2022-01-09 00:37:45,458 iteration 50 : loss : 0.326949, loss_ce: 0.143121
2022-01-09 00:37:46,841 iteration 51 : loss : 0.237864, loss_ce: 0.107064
  1%|▏                              | 3/400 [01:15<2:46:38, 25.18s/it]2022-01-09 00:37:48,439 iteration 52 : loss : 0.298974, loss_ce: 0.154371
2022-01-09 00:37:49,948 iteration 53 : loss : 0.249722, loss_ce: 0.109834
2022-01-09 00:37:51,417 iteration 54 : loss : 0.254611, loss_ce: 0.106335
2022-01-09 00:37:52,904 iteration 55 : loss : 0.309685, loss_ce: 0.158485
2022-01-09 00:37:54,373 iteration 56 : loss : 0.267110, loss_ce: 0.120038
2022-01-09 00:37:55,857 iteration 57 : loss : 0.246828, loss_ce: 0.103533
2022-01-09 00:37:57,346 iteration 58 : loss : 0.290994, loss_ce: 0.123156
2022-01-09 00:37:58,791 iteration 59 : loss : 0.215303, loss_ce: 0.097704
2022-01-09 00:38:00,288 iteration 60 : loss : 0.218716, loss_ce: 0.088299
2022-01-09 00:38:01,759 iteration 61 : loss : 0.259109, loss_ce: 0.120961
2022-01-09 00:38:03,224 iteration 62 : loss : 0.329585, loss_ce: 0.131910
2022-01-09 00:38:04,586 iteration 63 : loss : 0.286205, loss_ce: 0.139938
2022-01-09 00:38:06,024 iteration 64 : loss : 0.291206, loss_ce: 0.123804
2022-01-09 00:38:07,407 iteration 65 : loss : 0.259028, loss_ce: 0.092706
2022-01-09 00:38:08,847 iteration 66 : loss : 0.229068, loss_ce: 0.099413
2022-01-09 00:38:10,388 iteration 67 : loss : 0.248397, loss_ce: 0.087274
2022-01-09 00:38:11,843 iteration 68 : loss : 0.244978, loss_ce: 0.108849
  1%|▎                              | 4/400 [01:40<2:45:43, 25.11s/it]2022-01-09 00:38:13,407 iteration 69 : loss : 0.208648, loss_ce: 0.087457
2022-01-09 00:38:14,983 iteration 70 : loss : 0.235407, loss_ce: 0.107039
2022-01-09 00:38:16,403 iteration 71 : loss : 0.221451, loss_ce: 0.091254
2022-01-09 00:38:17,884 iteration 72 : loss : 0.203776, loss_ce: 0.085981
2022-01-09 00:38:19,288 iteration 73 : loss : 0.252454, loss_ce: 0.122645
2022-01-09 00:38:20,652 iteration 74 : loss : 0.221834, loss_ce: 0.094456
2022-01-09 00:38:22,064 iteration 75 : loss : 0.217713, loss_ce: 0.093721
2022-01-09 00:38:23,460 iteration 76 : loss : 0.217456, loss_ce: 0.090065
2022-01-09 00:38:24,754 iteration 77 : loss : 0.211940, loss_ce: 0.095873
2022-01-09 00:38:26,195 iteration 78 : loss : 0.249741, loss_ce: 0.110198
2022-01-09 00:38:27,595 iteration 79 : loss : 0.319843, loss_ce: 0.133715
2022-01-09 00:38:28,972 iteration 80 : loss : 0.257508, loss_ce: 0.108388
2022-01-09 00:38:30,344 iteration 81 : loss : 0.198858, loss_ce: 0.080684
2022-01-09 00:38:31,748 iteration 82 : loss : 0.244099, loss_ce: 0.088723
2022-01-09 00:38:33,143 iteration 83 : loss : 0.238971, loss_ce: 0.082353
2022-01-09 00:38:34,661 iteration 84 : loss : 0.262782, loss_ce: 0.142351
2022-01-09 00:38:34,661 Training Data Eval:
2022-01-09 00:38:41,915   Average segmentation loss on training set: 0.5192
2022-01-09 00:38:41,915 Validation Data Eval:
2022-01-09 00:38:44,589   Average segmentation loss on validation set: 0.4960
2022-01-09 00:38:50,722 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 00:38:52,157 iteration 85 : loss : 0.256057, loss_ce: 0.109244
  1%|▍                              | 5/400 [02:21<3:21:23, 30.59s/it]2022-01-09 00:38:53,679 iteration 86 : loss : 0.253376, loss_ce: 0.093332
2022-01-09 00:38:55,297 iteration 87 : loss : 0.203469, loss_ce: 0.089475
2022-01-09 00:38:56,674 iteration 88 : loss : 0.231897, loss_ce: 0.103123
2022-01-09 00:38:58,156 iteration 89 : loss : 0.242590, loss_ce: 0.102002
2022-01-09 00:38:59,628 iteration 90 : loss : 0.215891, loss_ce: 0.088186
2022-01-09 00:39:01,185 iteration 91 : loss : 0.222341, loss_ce: 0.108279
2022-01-09 00:39:02,553 iteration 92 : loss : 0.204160, loss_ce: 0.088930
2022-01-09 00:39:03,980 iteration 93 : loss : 0.239116, loss_ce: 0.086364
2022-01-09 00:39:05,417 iteration 94 : loss : 0.225538, loss_ce: 0.089900
2022-01-09 00:39:07,023 iteration 95 : loss : 0.220530, loss_ce: 0.103029
2022-01-09 00:39:08,433 iteration 96 : loss : 0.198692, loss_ce: 0.093059
2022-01-09 00:39:09,852 iteration 97 : loss : 0.249508, loss_ce: 0.104514
2022-01-09 00:39:11,291 iteration 98 : loss : 0.225972, loss_ce: 0.096652
2022-01-09 00:39:12,840 iteration 99 : loss : 0.213799, loss_ce: 0.088763
2022-01-09 00:39:14,318 iteration 100 : loss : 0.206562, loss_ce: 0.086916
2022-01-09 00:39:15,761 iteration 101 : loss : 0.147433, loss_ce: 0.056210
2022-01-09 00:39:17,123 iteration 102 : loss : 0.213809, loss_ce: 0.089415
  2%|▍                              | 6/400 [02:46<3:08:21, 28.68s/it]2022-01-09 00:39:18,766 iteration 103 : loss : 0.172181, loss_ce: 0.078698
2022-01-09 00:39:20,325 iteration 104 : loss : 0.250434, loss_ce: 0.107934
2022-01-09 00:39:21,945 iteration 105 : loss : 0.206134, loss_ce: 0.082249
2022-01-09 00:39:23,357 iteration 106 : loss : 0.217626, loss_ce: 0.091549
2022-01-09 00:39:25,020 iteration 107 : loss : 0.180547, loss_ce: 0.082099
2022-01-09 00:39:26,396 iteration 108 : loss : 0.240320, loss_ce: 0.096933
2022-01-09 00:39:27,820 iteration 109 : loss : 0.169964, loss_ce: 0.077894
2022-01-09 00:39:29,168 iteration 110 : loss : 0.167544, loss_ce: 0.075109
2022-01-09 00:39:30,643 iteration 111 : loss : 0.194078, loss_ce: 0.087487
2022-01-09 00:39:32,007 iteration 112 : loss : 0.183273, loss_ce: 0.074204
2022-01-09 00:39:33,459 iteration 113 : loss : 0.192101, loss_ce: 0.095266
2022-01-09 00:39:34,862 iteration 114 : loss : 0.189849, loss_ce: 0.063414
2022-01-09 00:39:36,296 iteration 115 : loss : 0.185360, loss_ce: 0.088604
2022-01-09 00:39:37,803 iteration 116 : loss : 0.223414, loss_ce: 0.103248
2022-01-09 00:39:39,317 iteration 117 : loss : 0.199154, loss_ce: 0.086733
2022-01-09 00:39:40,772 iteration 118 : loss : 0.231683, loss_ce: 0.105883
2022-01-09 00:39:42,244 iteration 119 : loss : 0.167389, loss_ce: 0.066382
  2%|▌                              | 7/400 [03:11<3:00:15, 27.52s/it]2022-01-09 00:39:43,707 iteration 120 : loss : 0.258838, loss_ce: 0.123568
2022-01-09 00:39:45,075 iteration 121 : loss : 0.146625, loss_ce: 0.061073
2022-01-09 00:39:46,598 iteration 122 : loss : 0.188520, loss_ce: 0.063728
2022-01-09 00:39:48,072 iteration 123 : loss : 0.195993, loss_ce: 0.079460
2022-01-09 00:39:49,508 iteration 124 : loss : 0.226003, loss_ce: 0.086682
2022-01-09 00:39:50,886 iteration 125 : loss : 0.195922, loss_ce: 0.093649
2022-01-09 00:39:52,345 iteration 126 : loss : 0.235720, loss_ce: 0.090705
2022-01-09 00:39:53,728 iteration 127 : loss : 0.198771, loss_ce: 0.090584
2022-01-09 00:39:55,132 iteration 128 : loss : 0.162163, loss_ce: 0.068032
2022-01-09 00:39:56,639 iteration 129 : loss : 0.199189, loss_ce: 0.076651
2022-01-09 00:39:58,068 iteration 130 : loss : 0.178993, loss_ce: 0.070802
2022-01-09 00:39:59,621 iteration 131 : loss : 0.226022, loss_ce: 0.116896
2022-01-09 00:40:01,210 iteration 132 : loss : 0.188278, loss_ce: 0.052662
2022-01-09 00:40:02,621 iteration 133 : loss : 0.161261, loss_ce: 0.057350
2022-01-09 00:40:04,157 iteration 134 : loss : 0.185381, loss_ce: 0.070410
2022-01-09 00:40:05,716 iteration 135 : loss : 0.206683, loss_ce: 0.092849
2022-01-09 00:40:07,152 iteration 136 : loss : 0.143747, loss_ce: 0.067108
  2%|▌                              | 8/400 [03:36<2:54:20, 26.68s/it]2022-01-09 00:40:08,757 iteration 137 : loss : 0.227097, loss_ce: 0.083796
2022-01-09 00:40:10,168 iteration 138 : loss : 0.212824, loss_ce: 0.121619
2022-01-09 00:40:11,657 iteration 139 : loss : 0.254779, loss_ce: 0.112932
2022-01-09 00:40:13,111 iteration 140 : loss : 0.200655, loss_ce: 0.080471
2022-01-09 00:40:14,624 iteration 141 : loss : 0.204455, loss_ce: 0.101647
2022-01-09 00:40:16,136 iteration 142 : loss : 0.206144, loss_ce: 0.079302
2022-01-09 00:40:17,691 iteration 143 : loss : 0.194639, loss_ce: 0.092128
2022-01-09 00:40:19,208 iteration 144 : loss : 0.175101, loss_ce: 0.064561
2022-01-09 00:40:20,651 iteration 145 : loss : 0.206099, loss_ce: 0.079996
2022-01-09 00:40:22,149 iteration 146 : loss : 0.139179, loss_ce: 0.062355
2022-01-09 00:40:23,645 iteration 147 : loss : 0.150757, loss_ce: 0.056420
2022-01-09 00:40:24,988 iteration 148 : loss : 0.176826, loss_ce: 0.072017
2022-01-09 00:40:26,426 iteration 149 : loss : 0.219212, loss_ce: 0.092576
2022-01-09 00:40:27,826 iteration 150 : loss : 0.216379, loss_ce: 0.078842
2022-01-09 00:40:29,235 iteration 151 : loss : 0.215279, loss_ce: 0.095675
2022-01-09 00:40:30,652 iteration 152 : loss : 0.205525, loss_ce: 0.066018
2022-01-09 00:40:32,139 iteration 153 : loss : 0.214528, loss_ce: 0.090962
  2%|▋                              | 9/400 [04:01<2:50:27, 26.16s/it]2022-01-09 00:40:33,586 iteration 154 : loss : 0.214005, loss_ce: 0.086309
2022-01-09 00:40:35,057 iteration 155 : loss : 0.166167, loss_ce: 0.060434
2022-01-09 00:40:36,627 iteration 156 : loss : 0.133276, loss_ce: 0.051756
2022-01-09 00:40:38,107 iteration 157 : loss : 0.170344, loss_ce: 0.061079
2022-01-09 00:40:39,726 iteration 158 : loss : 0.224404, loss_ce: 0.108382
2022-01-09 00:40:41,034 iteration 159 : loss : 0.159517, loss_ce: 0.068960
2022-01-09 00:40:42,596 iteration 160 : loss : 0.159894, loss_ce: 0.054606
2022-01-09 00:40:44,141 iteration 161 : loss : 0.210520, loss_ce: 0.080146
2022-01-09 00:40:45,686 iteration 162 : loss : 0.201895, loss_ce: 0.093439
2022-01-09 00:40:47,165 iteration 163 : loss : 0.191694, loss_ce: 0.074610
2022-01-09 00:40:48,699 iteration 164 : loss : 0.170406, loss_ce: 0.059812
2022-01-09 00:40:50,208 iteration 165 : loss : 0.188876, loss_ce: 0.077282
2022-01-09 00:40:51,701 iteration 166 : loss : 0.175141, loss_ce: 0.066955
2022-01-09 00:40:53,224 iteration 167 : loss : 0.161902, loss_ce: 0.061715
2022-01-09 00:40:54,761 iteration 168 : loss : 0.218282, loss_ce: 0.102395
2022-01-09 00:40:56,284 iteration 169 : loss : 0.195109, loss_ce: 0.094844
2022-01-09 00:40:56,285 Training Data Eval:
2022-01-09 00:41:03,769   Average segmentation loss on training set: 0.1676
2022-01-09 00:41:03,770 Validation Data Eval:
2022-01-09 00:41:06,329   Average segmentation loss on validation set: 0.1739
2022-01-09 00:41:12,178 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 00:41:13,705 iteration 170 : loss : 0.159103, loss_ce: 0.070687
  2%|▊                             | 10/400 [04:42<3:20:54, 30.91s/it]2022-01-09 00:41:15,241 iteration 171 : loss : 0.188377, loss_ce: 0.072888
2022-01-09 00:41:16,738 iteration 172 : loss : 0.180904, loss_ce: 0.084846
2022-01-09 00:41:18,262 iteration 173 : loss : 0.141861, loss_ce: 0.065851
2022-01-09 00:41:19,659 iteration 174 : loss : 0.189842, loss_ce: 0.085706
2022-01-09 00:41:21,043 iteration 175 : loss : 0.219092, loss_ce: 0.090684
2022-01-09 00:41:22,526 iteration 176 : loss : 0.155128, loss_ce: 0.065723
2022-01-09 00:41:23,967 iteration 177 : loss : 0.167631, loss_ce: 0.050618
2022-01-09 00:41:25,414 iteration 178 : loss : 0.151523, loss_ce: 0.067948
2022-01-09 00:41:26,854 iteration 179 : loss : 0.184374, loss_ce: 0.074166
2022-01-09 00:41:28,273 iteration 180 : loss : 0.183404, loss_ce: 0.065816
2022-01-09 00:41:29,764 iteration 181 : loss : 0.197641, loss_ce: 0.083063
2022-01-09 00:41:31,148 iteration 182 : loss : 0.140215, loss_ce: 0.057426
2022-01-09 00:41:32,524 iteration 183 : loss : 0.160438, loss_ce: 0.057743
2022-01-09 00:41:33,859 iteration 184 : loss : 0.192473, loss_ce: 0.070879
2022-01-09 00:41:35,348 iteration 185 : loss : 0.209293, loss_ce: 0.070579
2022-01-09 00:41:36,838 iteration 186 : loss : 0.261542, loss_ce: 0.113125
2022-01-09 00:41:38,375 iteration 187 : loss : 0.208019, loss_ce: 0.078217
  3%|▊                             | 11/400 [05:07<3:08:00, 29.00s/it]2022-01-09 00:41:39,939 iteration 188 : loss : 0.251052, loss_ce: 0.101661
2022-01-09 00:41:41,419 iteration 189 : loss : 0.159424, loss_ce: 0.059706
2022-01-09 00:41:42,904 iteration 190 : loss : 0.173199, loss_ce: 0.068821
2022-01-09 00:41:44,328 iteration 191 : loss : 0.175234, loss_ce: 0.069254
2022-01-09 00:41:45,695 iteration 192 : loss : 0.162211, loss_ce: 0.070559
2022-01-09 00:41:47,224 iteration 193 : loss : 0.158752, loss_ce: 0.064921
2022-01-09 00:41:48,657 iteration 194 : loss : 0.206305, loss_ce: 0.065711
2022-01-09 00:41:50,054 iteration 195 : loss : 0.145346, loss_ce: 0.060497
2022-01-09 00:41:51,402 iteration 196 : loss : 0.174744, loss_ce: 0.063587
2022-01-09 00:41:52,861 iteration 197 : loss : 0.174839, loss_ce: 0.070536
2022-01-09 00:41:54,361 iteration 198 : loss : 0.172377, loss_ce: 0.070867
2022-01-09 00:41:55,763 iteration 199 : loss : 0.188340, loss_ce: 0.066118
2022-01-09 00:41:57,189 iteration 200 : loss : 0.174942, loss_ce: 0.076992
2022-01-09 00:41:58,662 iteration 201 : loss : 0.128775, loss_ce: 0.051192
2022-01-09 00:42:00,168 iteration 202 : loss : 0.172913, loss_ce: 0.072599
2022-01-09 00:42:01,563 iteration 203 : loss : 0.162243, loss_ce: 0.067006
2022-01-09 00:42:03,144 iteration 204 : loss : 0.185093, loss_ce: 0.075630
  3%|▉                             | 12/400 [05:32<2:59:12, 27.71s/it]2022-01-09 00:42:04,672 iteration 205 : loss : 0.159562, loss_ce: 0.068447
2022-01-09 00:42:06,181 iteration 206 : loss : 0.166846, loss_ce: 0.056759
2022-01-09 00:42:07,699 iteration 207 : loss : 0.161449, loss_ce: 0.064861
2022-01-09 00:42:09,171 iteration 208 : loss : 0.193140, loss_ce: 0.059816
2022-01-09 00:42:10,766 iteration 209 : loss : 0.237533, loss_ce: 0.101898
2022-01-09 00:42:12,139 iteration 210 : loss : 0.151062, loss_ce: 0.049351
2022-01-09 00:42:13,702 iteration 211 : loss : 0.204549, loss_ce: 0.079261
2022-01-09 00:42:15,241 iteration 212 : loss : 0.154339, loss_ce: 0.057133
2022-01-09 00:42:16,905 iteration 213 : loss : 0.164341, loss_ce: 0.073202
2022-01-09 00:42:18,401 iteration 214 : loss : 0.167760, loss_ce: 0.051756
2022-01-09 00:42:19,910 iteration 215 : loss : 0.190497, loss_ce: 0.080348
2022-01-09 00:42:21,614 iteration 216 : loss : 0.149447, loss_ce: 0.055823
2022-01-09 00:42:23,214 iteration 217 : loss : 0.146149, loss_ce: 0.055194
2022-01-09 00:42:24,699 iteration 218 : loss : 0.135864, loss_ce: 0.060638
2022-01-09 00:42:26,029 iteration 219 : loss : 0.154911, loss_ce: 0.064077
2022-01-09 00:42:27,519 iteration 220 : loss : 0.146776, loss_ce: 0.062259
2022-01-09 00:42:28,987 iteration 221 : loss : 0.196384, loss_ce: 0.075163
  3%|▉                             | 13/400 [05:57<2:55:05, 27.15s/it]2022-01-09 00:42:30,544 iteration 222 : loss : 0.175466, loss_ce: 0.077199
2022-01-09 00:42:32,064 iteration 223 : loss : 0.166672, loss_ce: 0.074503
2022-01-09 00:42:33,522 iteration 224 : loss : 0.203627, loss_ce: 0.114237
2022-01-09 00:42:35,108 iteration 225 : loss : 0.261881, loss_ce: 0.076963
2022-01-09 00:42:36,561 iteration 226 : loss : 0.172644, loss_ce: 0.066421
2022-01-09 00:42:38,086 iteration 227 : loss : 0.214529, loss_ce: 0.092928
2022-01-09 00:42:39,538 iteration 228 : loss : 0.186867, loss_ce: 0.067479
2022-01-09 00:42:40,930 iteration 229 : loss : 0.153336, loss_ce: 0.049678
2022-01-09 00:42:42,397 iteration 230 : loss : 0.171266, loss_ce: 0.056275
2022-01-09 00:42:43,915 iteration 231 : loss : 0.221623, loss_ce: 0.083637
2022-01-09 00:42:45,425 iteration 232 : loss : 0.128476, loss_ce: 0.051191
2022-01-09 00:42:46,828 iteration 233 : loss : 0.190962, loss_ce: 0.084188
2022-01-09 00:42:48,376 iteration 234 : loss : 0.183746, loss_ce: 0.077891
2022-01-09 00:42:49,814 iteration 235 : loss : 0.188587, loss_ce: 0.090348
2022-01-09 00:42:51,320 iteration 236 : loss : 0.164575, loss_ce: 0.066015
2022-01-09 00:42:52,755 iteration 237 : loss : 0.135505, loss_ce: 0.053119
2022-01-09 00:42:54,217 iteration 238 : loss : 0.154273, loss_ce: 0.067058
  4%|█                             | 14/400 [06:23<2:50:55, 26.57s/it]2022-01-09 00:42:55,671 iteration 239 : loss : 0.182548, loss_ce: 0.057817
2022-01-09 00:42:57,169 iteration 240 : loss : 0.148981, loss_ce: 0.066334
2022-01-09 00:42:58,681 iteration 241 : loss : 0.171597, loss_ce: 0.067291
2022-01-09 00:43:00,170 iteration 242 : loss : 0.186565, loss_ce: 0.092299
2022-01-09 00:43:01,842 iteration 243 : loss : 0.175278, loss_ce: 0.070534
2022-01-09 00:43:03,242 iteration 244 : loss : 0.187142, loss_ce: 0.051453
2022-01-09 00:43:04,727 iteration 245 : loss : 0.163624, loss_ce: 0.079387
2022-01-09 00:43:06,263 iteration 246 : loss : 0.176076, loss_ce: 0.081856
2022-01-09 00:43:07,710 iteration 247 : loss : 0.202493, loss_ce: 0.072170
2022-01-09 00:43:09,157 iteration 248 : loss : 0.170974, loss_ce: 0.063161
2022-01-09 00:43:10,576 iteration 249 : loss : 0.187782, loss_ce: 0.086691
2022-01-09 00:43:12,021 iteration 250 : loss : 0.181083, loss_ce: 0.074947
2022-01-09 00:43:13,390 iteration 251 : loss : 0.186848, loss_ce: 0.065637
2022-01-09 00:43:14,835 iteration 252 : loss : 0.108528, loss_ce: 0.047592
2022-01-09 00:43:16,313 iteration 253 : loss : 0.176734, loss_ce: 0.071915
2022-01-09 00:43:17,884 iteration 254 : loss : 0.230630, loss_ce: 0.066407
2022-01-09 00:43:17,884 Training Data Eval:
2022-01-09 00:43:25,144   Average segmentation loss on training set: 0.1608
2022-01-09 00:43:25,145 Validation Data Eval:
2022-01-09 00:43:27,630   Average segmentation loss on validation set: 0.2273
2022-01-09 00:43:29,034 iteration 255 : loss : 0.163420, loss_ce: 0.066028
  4%|█▏                            | 15/400 [06:57<3:06:27, 29.06s/it]2022-01-09 00:43:30,596 iteration 256 : loss : 0.196470, loss_ce: 0.075241
2022-01-09 00:43:32,121 iteration 257 : loss : 0.156284, loss_ce: 0.060481
2022-01-09 00:43:33,718 iteration 258 : loss : 0.156044, loss_ce: 0.053766
2022-01-09 00:43:35,076 iteration 259 : loss : 0.146285, loss_ce: 0.058414
2022-01-09 00:43:36,629 iteration 260 : loss : 0.129550, loss_ce: 0.052509
2022-01-09 00:43:38,123 iteration 261 : loss : 0.211258, loss_ce: 0.070085
2022-01-09 00:43:39,734 iteration 262 : loss : 0.154768, loss_ce: 0.065812
2022-01-09 00:43:41,205 iteration 263 : loss : 0.147538, loss_ce: 0.060806
2022-01-09 00:43:42,724 iteration 264 : loss : 0.222947, loss_ce: 0.096313
2022-01-09 00:43:44,143 iteration 265 : loss : 0.146438, loss_ce: 0.058785
2022-01-09 00:43:45,657 iteration 266 : loss : 0.155713, loss_ce: 0.075095
2022-01-09 00:43:47,119 iteration 267 : loss : 0.181231, loss_ce: 0.067628
2022-01-09 00:43:48,600 iteration 268 : loss : 0.212337, loss_ce: 0.099593
2022-01-09 00:43:50,341 iteration 269 : loss : 0.202127, loss_ce: 0.074000
2022-01-09 00:43:51,955 iteration 270 : loss : 0.141987, loss_ce: 0.055581
2022-01-09 00:43:53,265 iteration 271 : loss : 0.178945, loss_ce: 0.059019
2022-01-09 00:43:54,733 iteration 272 : loss : 0.136839, loss_ce: 0.063650
  4%|█▏                            | 16/400 [07:23<2:59:30, 28.05s/it]2022-01-09 00:43:56,201 iteration 273 : loss : 0.157927, loss_ce: 0.054635
2022-01-09 00:43:57,569 iteration 274 : loss : 0.122155, loss_ce: 0.044618
2022-01-09 00:43:58,991 iteration 275 : loss : 0.152892, loss_ce: 0.057642
2022-01-09 00:44:00,468 iteration 276 : loss : 0.143949, loss_ce: 0.065357
2022-01-09 00:44:01,971 iteration 277 : loss : 0.122532, loss_ce: 0.044613
2022-01-09 00:44:03,266 iteration 278 : loss : 0.156921, loss_ce: 0.046039
2022-01-09 00:44:04,659 iteration 279 : loss : 0.161095, loss_ce: 0.054099
2022-01-09 00:44:06,036 iteration 280 : loss : 0.156136, loss_ce: 0.059560
2022-01-09 00:44:07,493 iteration 281 : loss : 0.130818, loss_ce: 0.053230
2022-01-09 00:44:08,924 iteration 282 : loss : 0.234894, loss_ce: 0.095559
2022-01-09 00:44:10,324 iteration 283 : loss : 0.163990, loss_ce: 0.076140
2022-01-09 00:44:11,861 iteration 284 : loss : 0.174270, loss_ce: 0.071456
2022-01-09 00:44:13,236 iteration 285 : loss : 0.176723, loss_ce: 0.056403
2022-01-09 00:44:14,629 iteration 286 : loss : 0.140854, loss_ce: 0.060795
2022-01-09 00:44:16,191 iteration 287 : loss : 0.179170, loss_ce: 0.073791
2022-01-09 00:44:17,724 iteration 288 : loss : 0.192895, loss_ce: 0.075272
2022-01-09 00:44:19,152 iteration 289 : loss : 0.157585, loss_ce: 0.053894
  4%|█▎                            | 17/400 [07:48<2:52:02, 26.95s/it]2022-01-09 00:44:20,685 iteration 290 : loss : 0.123297, loss_ce: 0.045875
2022-01-09 00:44:22,085 iteration 291 : loss : 0.156018, loss_ce: 0.055909
2022-01-09 00:44:23,497 iteration 292 : loss : 0.157168, loss_ce: 0.059885
2022-01-09 00:44:24,928 iteration 293 : loss : 0.219628, loss_ce: 0.089088
2022-01-09 00:44:26,391 iteration 294 : loss : 0.155108, loss_ce: 0.058433
2022-01-09 00:44:27,977 iteration 295 : loss : 0.168264, loss_ce: 0.053679
2022-01-09 00:44:29,395 iteration 296 : loss : 0.137879, loss_ce: 0.049829
2022-01-09 00:44:30,858 iteration 297 : loss : 0.169551, loss_ce: 0.056317
2022-01-09 00:44:32,277 iteration 298 : loss : 0.108735, loss_ce: 0.038416
2022-01-09 00:44:33,728 iteration 299 : loss : 0.153409, loss_ce: 0.052770
2022-01-09 00:44:35,185 iteration 300 : loss : 0.180955, loss_ce: 0.059280
2022-01-09 00:44:36,656 iteration 301 : loss : 0.204499, loss_ce: 0.106303
2022-01-09 00:44:38,205 iteration 302 : loss : 0.127216, loss_ce: 0.048219
2022-01-09 00:44:39,720 iteration 303 : loss : 0.160613, loss_ce: 0.061173
2022-01-09 00:44:41,219 iteration 304 : loss : 0.184298, loss_ce: 0.075774
2022-01-09 00:44:42,756 iteration 305 : loss : 0.159274, loss_ce: 0.060379
2022-01-09 00:44:44,288 iteration 306 : loss : 0.122543, loss_ce: 0.052333
  4%|█▎                            | 18/400 [08:13<2:48:08, 26.41s/it]2022-01-09 00:44:45,861 iteration 307 : loss : 0.153095, loss_ce: 0.061979
2022-01-09 00:44:47,312 iteration 308 : loss : 0.143186, loss_ce: 0.065767
2022-01-09 00:44:48,844 iteration 309 : loss : 0.224469, loss_ce: 0.082696
2022-01-09 00:44:50,392 iteration 310 : loss : 0.168709, loss_ce: 0.071838
2022-01-09 00:44:51,823 iteration 311 : loss : 0.175076, loss_ce: 0.050502
2022-01-09 00:44:53,298 iteration 312 : loss : 0.125513, loss_ce: 0.057124
2022-01-09 00:44:54,786 iteration 313 : loss : 0.156164, loss_ce: 0.066241
2022-01-09 00:44:56,186 iteration 314 : loss : 0.123246, loss_ce: 0.051625
2022-01-09 00:44:57,655 iteration 315 : loss : 0.161395, loss_ce: 0.058204
2022-01-09 00:44:59,160 iteration 316 : loss : 0.154445, loss_ce: 0.054556
2022-01-09 00:45:00,628 iteration 317 : loss : 0.172083, loss_ce: 0.069132
2022-01-09 00:45:02,132 iteration 318 : loss : 0.142234, loss_ce: 0.058021
2022-01-09 00:45:03,537 iteration 319 : loss : 0.168024, loss_ce: 0.066243
2022-01-09 00:45:05,110 iteration 320 : loss : 0.167088, loss_ce: 0.070353
2022-01-09 00:45:06,627 iteration 321 : loss : 0.158282, loss_ce: 0.055430
2022-01-09 00:45:08,087 iteration 322 : loss : 0.207990, loss_ce: 0.091026
2022-01-09 00:45:09,649 iteration 323 : loss : 0.148952, loss_ce: 0.050339
  5%|█▍                            | 19/400 [08:38<2:45:41, 26.09s/it]2022-01-09 00:45:11,175 iteration 324 : loss : 0.157527, loss_ce: 0.054684
2022-01-09 00:45:12,652 iteration 325 : loss : 0.122498, loss_ce: 0.038621
2022-01-09 00:45:14,173 iteration 326 : loss : 0.101959, loss_ce: 0.045107
2022-01-09 00:45:15,791 iteration 327 : loss : 0.204907, loss_ce: 0.072826
2022-01-09 00:45:17,252 iteration 328 : loss : 0.180975, loss_ce: 0.075095
2022-01-09 00:45:18,768 iteration 329 : loss : 0.151435, loss_ce: 0.068560
2022-01-09 00:45:20,313 iteration 330 : loss : 0.148682, loss_ce: 0.059126
2022-01-09 00:45:21,744 iteration 331 : loss : 0.188238, loss_ce: 0.096668
2022-01-09 00:45:23,208 iteration 332 : loss : 0.130937, loss_ce: 0.046710
2022-01-09 00:45:24,721 iteration 333 : loss : 0.118055, loss_ce: 0.052721
2022-01-09 00:45:26,204 iteration 334 : loss : 0.157950, loss_ce: 0.069842
2022-01-09 00:45:27,651 iteration 335 : loss : 0.208391, loss_ce: 0.075482
2022-01-09 00:45:29,187 iteration 336 : loss : 0.131677, loss_ce: 0.048369
2022-01-09 00:45:30,656 iteration 337 : loss : 0.120537, loss_ce: 0.039991
2022-01-09 00:45:32,071 iteration 338 : loss : 0.167154, loss_ce: 0.073658
2022-01-09 00:45:33,504 iteration 339 : loss : 0.150423, loss_ce: 0.048753
2022-01-09 00:45:33,505 Training Data Eval:
2022-01-09 00:45:40,952   Average segmentation loss on training set: 0.1443
2022-01-09 00:45:40,952 Validation Data Eval:
2022-01-09 00:45:43,515   Average segmentation loss on validation set: 0.2064
2022-01-09 00:45:45,054 iteration 340 : loss : 0.177654, loss_ce: 0.064485
  5%|█▌                            | 20/400 [09:13<3:02:56, 28.89s/it]2022-01-09 00:45:46,659 iteration 341 : loss : 0.138415, loss_ce: 0.060559
2022-01-09 00:45:48,166 iteration 342 : loss : 0.133910, loss_ce: 0.047338
2022-01-09 00:45:49,631 iteration 343 : loss : 0.191757, loss_ce: 0.078825
2022-01-09 00:45:51,145 iteration 344 : loss : 0.188331, loss_ce: 0.072847
2022-01-09 00:45:52,599 iteration 345 : loss : 0.111955, loss_ce: 0.038136
2022-01-09 00:45:54,136 iteration 346 : loss : 0.164026, loss_ce: 0.067931
2022-01-09 00:45:55,524 iteration 347 : loss : 0.145966, loss_ce: 0.066212
2022-01-09 00:45:57,024 iteration 348 : loss : 0.156959, loss_ce: 0.056094
2022-01-09 00:45:58,662 iteration 349 : loss : 0.155808, loss_ce: 0.076713
2022-01-09 00:46:00,126 iteration 350 : loss : 0.154265, loss_ce: 0.061049
2022-01-09 00:46:01,667 iteration 351 : loss : 0.133019, loss_ce: 0.059040
2022-01-09 00:46:03,175 iteration 352 : loss : 0.133570, loss_ce: 0.051721
2022-01-09 00:46:04,571 iteration 353 : loss : 0.142493, loss_ce: 0.054067
2022-01-09 00:46:06,045 iteration 354 : loss : 0.157900, loss_ce: 0.051758
2022-01-09 00:46:07,580 iteration 355 : loss : 0.153059, loss_ce: 0.058309
2022-01-09 00:46:09,221 iteration 356 : loss : 0.140866, loss_ce: 0.049428
2022-01-09 00:46:10,599 iteration 357 : loss : 0.138632, loss_ce: 0.055718
  5%|█▌                            | 21/400 [09:39<2:56:07, 27.88s/it]2022-01-09 00:46:12,173 iteration 358 : loss : 0.137026, loss_ce: 0.057543
2022-01-09 00:46:13,742 iteration 359 : loss : 0.169827, loss_ce: 0.058853
2022-01-09 00:46:15,139 iteration 360 : loss : 0.133491, loss_ce: 0.048730
2022-01-09 00:46:16,661 iteration 361 : loss : 0.170977, loss_ce: 0.061217
2022-01-09 00:46:18,245 iteration 362 : loss : 0.147533, loss_ce: 0.044800
2022-01-09 00:46:19,820 iteration 363 : loss : 0.167414, loss_ce: 0.058293
2022-01-09 00:46:21,335 iteration 364 : loss : 0.120335, loss_ce: 0.046204
2022-01-09 00:46:22,682 iteration 365 : loss : 0.145227, loss_ce: 0.063084
2022-01-09 00:46:24,200 iteration 366 : loss : 0.158788, loss_ce: 0.073527
2022-01-09 00:46:25,572 iteration 367 : loss : 0.128808, loss_ce: 0.050886
2022-01-09 00:46:27,095 iteration 368 : loss : 0.143323, loss_ce: 0.048317
2022-01-09 00:46:28,686 iteration 369 : loss : 0.187616, loss_ce: 0.057705
2022-01-09 00:46:30,235 iteration 370 : loss : 0.143746, loss_ce: 0.067737
2022-01-09 00:46:31,729 iteration 371 : loss : 0.188795, loss_ce: 0.057844
2022-01-09 00:46:33,229 iteration 372 : loss : 0.227189, loss_ce: 0.102232
2022-01-09 00:46:34,632 iteration 373 : loss : 0.156450, loss_ce: 0.050651
2022-01-09 00:46:36,078 iteration 374 : loss : 0.127827, loss_ce: 0.049641
  6%|█▋                            | 22/400 [10:04<2:51:08, 27.17s/it]2022-01-09 00:46:37,692 iteration 375 : loss : 0.247257, loss_ce: 0.119032
2022-01-09 00:46:39,099 iteration 376 : loss : 0.096684, loss_ce: 0.034559
2022-01-09 00:46:40,548 iteration 377 : loss : 0.134749, loss_ce: 0.042377
2022-01-09 00:46:42,114 iteration 378 : loss : 0.134790, loss_ce: 0.044549
2022-01-09 00:46:43,470 iteration 379 : loss : 0.121351, loss_ce: 0.048998
2022-01-09 00:46:44,976 iteration 380 : loss : 0.113843, loss_ce: 0.039897
2022-01-09 00:46:46,433 iteration 381 : loss : 0.084925, loss_ce: 0.028785
2022-01-09 00:46:47,787 iteration 382 : loss : 0.127297, loss_ce: 0.053954
2022-01-09 00:46:49,241 iteration 383 : loss : 0.179876, loss_ce: 0.060905
2022-01-09 00:46:50,726 iteration 384 : loss : 0.111587, loss_ce: 0.043623
2022-01-09 00:46:52,188 iteration 385 : loss : 0.122755, loss_ce: 0.057446
2022-01-09 00:46:53,662 iteration 386 : loss : 0.130604, loss_ce: 0.046391
2022-01-09 00:46:55,155 iteration 387 : loss : 0.179017, loss_ce: 0.056386
2022-01-09 00:46:56,672 iteration 388 : loss : 0.152811, loss_ce: 0.067448
2022-01-09 00:46:58,159 iteration 389 : loss : 0.202862, loss_ce: 0.093836
2022-01-09 00:46:59,592 iteration 390 : loss : 0.112022, loss_ce: 0.044687
2022-01-09 00:47:01,026 iteration 391 : loss : 0.174806, loss_ce: 0.097881
  6%|█▋                            | 23/400 [10:29<2:46:28, 26.49s/it]2022-01-09 00:47:02,600 iteration 392 : loss : 0.139694, loss_ce: 0.071543
2022-01-09 00:47:03,986 iteration 393 : loss : 0.111916, loss_ce: 0.042948
2022-01-09 00:47:05,522 iteration 394 : loss : 0.158513, loss_ce: 0.065235
2022-01-09 00:47:07,027 iteration 395 : loss : 0.150751, loss_ce: 0.066225
2022-01-09 00:47:08,459 iteration 396 : loss : 0.125483, loss_ce: 0.048494
2022-01-09 00:47:09,933 iteration 397 : loss : 0.143185, loss_ce: 0.060855
2022-01-09 00:47:11,431 iteration 398 : loss : 0.105211, loss_ce: 0.040513
2022-01-09 00:47:12,762 iteration 399 : loss : 0.149911, loss_ce: 0.061251
2022-01-09 00:47:14,243 iteration 400 : loss : 0.145592, loss_ce: 0.053567
2022-01-09 00:47:15,806 iteration 401 : loss : 0.157451, loss_ce: 0.066750
2022-01-09 00:47:17,244 iteration 402 : loss : 0.101287, loss_ce: 0.041642
2022-01-09 00:47:18,678 iteration 403 : loss : 0.099603, loss_ce: 0.036591
2022-01-09 00:47:20,188 iteration 404 : loss : 0.160917, loss_ce: 0.062427
2022-01-09 00:47:21,754 iteration 405 : loss : 0.166199, loss_ce: 0.072920
2022-01-09 00:47:23,189 iteration 406 : loss : 0.145432, loss_ce: 0.060331
2022-01-09 00:47:24,574 iteration 407 : loss : 0.125433, loss_ce: 0.045541
2022-01-09 00:47:26,021 iteration 408 : loss : 0.159492, loss_ce: 0.062583
  6%|█▊                            | 24/400 [10:54<2:43:14, 26.05s/it]2022-01-09 00:47:27,598 iteration 409 : loss : 0.132487, loss_ce: 0.061882
2022-01-09 00:47:29,086 iteration 410 : loss : 0.121118, loss_ce: 0.046679
2022-01-09 00:47:30,545 iteration 411 : loss : 0.239526, loss_ce: 0.071156
2022-01-09 00:47:31,882 iteration 412 : loss : 0.117604, loss_ce: 0.050474
2022-01-09 00:47:33,340 iteration 413 : loss : 0.097218, loss_ce: 0.034717
2022-01-09 00:47:34,722 iteration 414 : loss : 0.188296, loss_ce: 0.102833
2022-01-09 00:47:36,039 iteration 415 : loss : 0.139172, loss_ce: 0.054276
2022-01-09 00:47:37,476 iteration 416 : loss : 0.162272, loss_ce: 0.076408
2022-01-09 00:47:38,880 iteration 417 : loss : 0.219554, loss_ce: 0.083622
2022-01-09 00:47:40,418 iteration 418 : loss : 0.141251, loss_ce: 0.061234
2022-01-09 00:47:41,839 iteration 419 : loss : 0.166147, loss_ce: 0.095374
2022-01-09 00:47:43,311 iteration 420 : loss : 0.136291, loss_ce: 0.059478
2022-01-09 00:47:44,702 iteration 421 : loss : 0.167177, loss_ce: 0.071570
2022-01-09 00:47:46,088 iteration 422 : loss : 0.164303, loss_ce: 0.071111
2022-01-09 00:47:47,484 iteration 423 : loss : 0.160235, loss_ce: 0.053261
2022-01-09 00:47:48,954 iteration 424 : loss : 0.144641, loss_ce: 0.056814
2022-01-09 00:47:48,955 Training Data Eval:
2022-01-09 00:47:56,251   Average segmentation loss on training set: 0.1098
2022-01-09 00:47:56,252 Validation Data Eval:
2022-01-09 00:47:58,783   Average segmentation loss on validation set: 0.1424
2022-01-09 00:48:04,563 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 00:48:06,018 iteration 425 : loss : 0.159660, loss_ce: 0.069614
  6%|█▉                            | 25/400 [11:34<3:08:57, 30.23s/it]2022-01-09 00:48:07,474 iteration 426 : loss : 0.129759, loss_ce: 0.048035
2022-01-09 00:48:08,977 iteration 427 : loss : 0.119253, loss_ce: 0.043827
2022-01-09 00:48:10,438 iteration 428 : loss : 0.175528, loss_ce: 0.072837
2022-01-09 00:48:11,826 iteration 429 : loss : 0.197337, loss_ce: 0.106965
2022-01-09 00:48:13,201 iteration 430 : loss : 0.147913, loss_ce: 0.049929
2022-01-09 00:48:14,610 iteration 431 : loss : 0.230009, loss_ce: 0.084657
2022-01-09 00:48:16,105 iteration 432 : loss : 0.102895, loss_ce: 0.040561
2022-01-09 00:48:17,650 iteration 433 : loss : 0.125920, loss_ce: 0.048691
2022-01-09 00:48:19,124 iteration 434 : loss : 0.104534, loss_ce: 0.039906
2022-01-09 00:48:20,505 iteration 435 : loss : 0.123365, loss_ce: 0.050112
2022-01-09 00:48:21,935 iteration 436 : loss : 0.113590, loss_ce: 0.049087
2022-01-09 00:48:23,368 iteration 437 : loss : 0.188546, loss_ce: 0.097057
2022-01-09 00:48:24,737 iteration 438 : loss : 0.130793, loss_ce: 0.041907
2022-01-09 00:48:26,197 iteration 439 : loss : 0.168392, loss_ce: 0.050355
2022-01-09 00:48:27,623 iteration 440 : loss : 0.114349, loss_ce: 0.041843
2022-01-09 00:48:29,142 iteration 441 : loss : 0.126589, loss_ce: 0.047850
2022-01-09 00:48:30,632 iteration 442 : loss : 0.159391, loss_ce: 0.070732
  6%|█▉                            | 26/400 [11:59<2:57:57, 28.55s/it]2022-01-09 00:48:32,124 iteration 443 : loss : 0.155922, loss_ce: 0.076664
2022-01-09 00:48:33,509 iteration 444 : loss : 0.148419, loss_ce: 0.054932
2022-01-09 00:48:34,943 iteration 445 : loss : 0.156720, loss_ce: 0.051724
2022-01-09 00:48:36,562 iteration 446 : loss : 0.159480, loss_ce: 0.061933
2022-01-09 00:48:38,100 iteration 447 : loss : 0.115201, loss_ce: 0.060578
2022-01-09 00:48:39,621 iteration 448 : loss : 0.238856, loss_ce: 0.078749
2022-01-09 00:48:41,083 iteration 449 : loss : 0.116149, loss_ce: 0.053565
2022-01-09 00:48:42,515 iteration 450 : loss : 0.154354, loss_ce: 0.056689
2022-01-09 00:48:44,076 iteration 451 : loss : 0.119028, loss_ce: 0.055426
2022-01-09 00:48:45,585 iteration 452 : loss : 0.125837, loss_ce: 0.053370
2022-01-09 00:48:47,083 iteration 453 : loss : 0.166805, loss_ce: 0.070071
2022-01-09 00:48:48,581 iteration 454 : loss : 0.155477, loss_ce: 0.051663
2022-01-09 00:48:50,124 iteration 455 : loss : 0.110937, loss_ce: 0.042365
2022-01-09 00:48:51,608 iteration 456 : loss : 0.124205, loss_ce: 0.039671
2022-01-09 00:48:53,080 iteration 457 : loss : 0.158165, loss_ce: 0.061132
2022-01-09 00:48:54,652 iteration 458 : loss : 0.179687, loss_ce: 0.075742
2022-01-09 00:48:56,039 iteration 459 : loss : 0.143257, loss_ce: 0.053042
  7%|██                            | 27/400 [12:24<2:51:36, 27.60s/it]2022-01-09 00:48:57,600 iteration 460 : loss : 0.148247, loss_ce: 0.061839
2022-01-09 00:48:59,084 iteration 461 : loss : 0.128259, loss_ce: 0.056942
2022-01-09 00:49:00,535 iteration 462 : loss : 0.107876, loss_ce: 0.045768
2022-01-09 00:49:02,077 iteration 463 : loss : 0.148814, loss_ce: 0.078531
2022-01-09 00:49:03,556 iteration 464 : loss : 0.105398, loss_ce: 0.039396
2022-01-09 00:49:04,959 iteration 465 : loss : 0.158292, loss_ce: 0.049103
2022-01-09 00:49:06,411 iteration 466 : loss : 0.135868, loss_ce: 0.052770
2022-01-09 00:49:07,845 iteration 467 : loss : 0.135819, loss_ce: 0.054880
2022-01-09 00:49:09,442 iteration 468 : loss : 0.137702, loss_ce: 0.061187
2022-01-09 00:49:10,878 iteration 469 : loss : 0.109934, loss_ce: 0.038551
2022-01-09 00:49:12,331 iteration 470 : loss : 0.138685, loss_ce: 0.053742
2022-01-09 00:49:13,783 iteration 471 : loss : 0.152726, loss_ce: 0.051251
2022-01-09 00:49:15,310 iteration 472 : loss : 0.173133, loss_ce: 0.066084
2022-01-09 00:49:16,976 iteration 473 : loss : 0.137333, loss_ce: 0.051215
2022-01-09 00:49:18,561 iteration 474 : loss : 0.193602, loss_ce: 0.080149
2022-01-09 00:49:19,998 iteration 475 : loss : 0.104096, loss_ce: 0.037087
2022-01-09 00:49:21,548 iteration 476 : loss : 0.142420, loss_ce: 0.060323
  7%|██                            | 28/400 [12:50<2:47:15, 26.98s/it]2022-01-09 00:49:23,076 iteration 477 : loss : 0.134155, loss_ce: 0.041629
2022-01-09 00:49:24,607 iteration 478 : loss : 0.075761, loss_ce: 0.025172
2022-01-09 00:49:26,062 iteration 479 : loss : 0.136939, loss_ce: 0.054774
2022-01-09 00:49:27,559 iteration 480 : loss : 0.116395, loss_ce: 0.056251
2022-01-09 00:49:28,924 iteration 481 : loss : 0.156897, loss_ce: 0.063167
2022-01-09 00:49:30,412 iteration 482 : loss : 0.170520, loss_ce: 0.050628
2022-01-09 00:49:31,907 iteration 483 : loss : 0.154410, loss_ce: 0.057464
2022-01-09 00:49:33,425 iteration 484 : loss : 0.161075, loss_ce: 0.065149
2022-01-09 00:49:34,866 iteration 485 : loss : 0.119508, loss_ce: 0.050976
2022-01-09 00:49:36,298 iteration 486 : loss : 0.156819, loss_ce: 0.049069
2022-01-09 00:49:37,840 iteration 487 : loss : 0.133847, loss_ce: 0.050651
2022-01-09 00:49:39,232 iteration 488 : loss : 0.125964, loss_ce: 0.052503
2022-01-09 00:49:40,700 iteration 489 : loss : 0.115639, loss_ce: 0.048894
2022-01-09 00:49:42,077 iteration 490 : loss : 0.149865, loss_ce: 0.057019
2022-01-09 00:49:43,620 iteration 491 : loss : 0.137123, loss_ce: 0.060142
2022-01-09 00:49:45,116 iteration 492 : loss : 0.090530, loss_ce: 0.047593
2022-01-09 00:49:46,669 iteration 493 : loss : 0.121827, loss_ce: 0.060349
  7%|██▏                           | 29/400 [13:15<2:43:21, 26.42s/it]2022-01-09 00:49:48,223 iteration 494 : loss : 0.128208, loss_ce: 0.059278
2022-01-09 00:49:49,715 iteration 495 : loss : 0.132470, loss_ce: 0.048764
2022-01-09 00:49:51,181 iteration 496 : loss : 0.170303, loss_ce: 0.076335
2022-01-09 00:49:52,685 iteration 497 : loss : 0.151899, loss_ce: 0.065928
2022-01-09 00:49:54,231 iteration 498 : loss : 0.120592, loss_ce: 0.056444
2022-01-09 00:49:55,603 iteration 499 : loss : 0.096815, loss_ce: 0.040925
2022-01-09 00:49:57,001 iteration 500 : loss : 0.134392, loss_ce: 0.053130
2022-01-09 00:49:58,418 iteration 501 : loss : 0.132112, loss_ce: 0.049163
2022-01-09 00:49:59,947 iteration 502 : loss : 0.103577, loss_ce: 0.036361
2022-01-09 00:50:01,387 iteration 503 : loss : 0.142379, loss_ce: 0.046155
2022-01-09 00:50:02,912 iteration 504 : loss : 0.113208, loss_ce: 0.039821
2022-01-09 00:50:04,348 iteration 505 : loss : 0.082598, loss_ce: 0.030638
2022-01-09 00:50:05,966 iteration 506 : loss : 0.089866, loss_ce: 0.033524
2022-01-09 00:50:07,481 iteration 507 : loss : 0.177849, loss_ce: 0.075292
2022-01-09 00:50:08,990 iteration 508 : loss : 0.163840, loss_ce: 0.075270
2022-01-09 00:50:10,554 iteration 509 : loss : 0.113997, loss_ce: 0.041666
2022-01-09 00:50:10,554 Training Data Eval:
2022-01-09 00:50:18,204   Average segmentation loss on training set: 0.2272
2022-01-09 00:50:18,204 Validation Data Eval:
2022-01-09 00:50:20,822   Average segmentation loss on validation set: 0.3660
2022-01-09 00:50:22,383 iteration 510 : loss : 0.117085, loss_ce: 0.049016
  8%|██▎                           | 30/400 [13:51<3:00:05, 29.20s/it]2022-01-09 00:50:23,975 iteration 511 : loss : 0.170121, loss_ce: 0.081411
2022-01-09 00:50:25,408 iteration 512 : loss : 0.203352, loss_ce: 0.100869
2022-01-09 00:50:26,863 iteration 513 : loss : 0.096843, loss_ce: 0.031351
2022-01-09 00:50:28,341 iteration 514 : loss : 0.103940, loss_ce: 0.045963
2022-01-09 00:50:29,790 iteration 515 : loss : 0.116752, loss_ce: 0.051550
2022-01-09 00:50:31,300 iteration 516 : loss : 0.091825, loss_ce: 0.034496
2022-01-09 00:50:32,852 iteration 517 : loss : 0.137287, loss_ce: 0.068785
2022-01-09 00:50:34,257 iteration 518 : loss : 0.138478, loss_ce: 0.050967
2022-01-09 00:50:35,723 iteration 519 : loss : 0.116525, loss_ce: 0.047577
2022-01-09 00:50:37,119 iteration 520 : loss : 0.092697, loss_ce: 0.033884
2022-01-09 00:50:38,585 iteration 521 : loss : 0.103900, loss_ce: 0.037397
2022-01-09 00:50:40,102 iteration 522 : loss : 0.108915, loss_ce: 0.037361
2022-01-09 00:50:41,660 iteration 523 : loss : 0.092319, loss_ce: 0.037122
2022-01-09 00:50:43,251 iteration 524 : loss : 0.130222, loss_ce: 0.054940
2022-01-09 00:50:44,712 iteration 525 : loss : 0.175976, loss_ce: 0.088431
2022-01-09 00:50:46,167 iteration 526 : loss : 0.122850, loss_ce: 0.056557
2022-01-09 00:50:47,624 iteration 527 : loss : 0.081523, loss_ce: 0.030062
  8%|██▎                           | 31/400 [14:16<2:52:18, 28.02s/it]2022-01-09 00:50:49,121 iteration 528 : loss : 0.104527, loss_ce: 0.044621
2022-01-09 00:50:50,600 iteration 529 : loss : 0.123934, loss_ce: 0.038489
2022-01-09 00:50:52,015 iteration 530 : loss : 0.117919, loss_ce: 0.042954
2022-01-09 00:50:53,406 iteration 531 : loss : 0.122000, loss_ce: 0.038813
2022-01-09 00:50:54,820 iteration 532 : loss : 0.114846, loss_ce: 0.052463
2022-01-09 00:50:56,410 iteration 533 : loss : 0.075119, loss_ce: 0.027318
2022-01-09 00:50:57,798 iteration 534 : loss : 0.084268, loss_ce: 0.038360
2022-01-09 00:50:59,312 iteration 535 : loss : 0.094199, loss_ce: 0.031988
2022-01-09 00:51:00,874 iteration 536 : loss : 0.151440, loss_ce: 0.068722
2022-01-09 00:51:02,453 iteration 537 : loss : 0.133152, loss_ce: 0.036114
2022-01-09 00:51:03,943 iteration 538 : loss : 0.130212, loss_ce: 0.060172
2022-01-09 00:51:05,381 iteration 539 : loss : 0.122746, loss_ce: 0.055760
2022-01-09 00:51:06,873 iteration 540 : loss : 0.104025, loss_ce: 0.033112
2022-01-09 00:51:08,305 iteration 541 : loss : 0.102371, loss_ce: 0.042911
2022-01-09 00:51:09,765 iteration 542 : loss : 0.133397, loss_ce: 0.058494
2022-01-09 00:51:11,154 iteration 543 : loss : 0.085957, loss_ce: 0.035281
2022-01-09 00:51:12,590 iteration 544 : loss : 0.099131, loss_ce: 0.039139
  8%|██▍                           | 32/400 [14:41<2:46:13, 27.10s/it]2022-01-09 00:51:14,140 iteration 545 : loss : 0.080141, loss_ce: 0.039143
2022-01-09 00:51:15,593 iteration 546 : loss : 0.132761, loss_ce: 0.052163
2022-01-09 00:51:17,052 iteration 547 : loss : 0.126966, loss_ce: 0.053522
2022-01-09 00:51:18,498 iteration 548 : loss : 0.185997, loss_ce: 0.071469
2022-01-09 00:51:19,993 iteration 549 : loss : 0.102525, loss_ce: 0.050710
2022-01-09 00:51:21,411 iteration 550 : loss : 0.113945, loss_ce: 0.049033
2022-01-09 00:51:22,859 iteration 551 : loss : 0.108283, loss_ce: 0.046136
2022-01-09 00:51:24,279 iteration 552 : loss : 0.111168, loss_ce: 0.041351
2022-01-09 00:51:25,738 iteration 553 : loss : 0.135922, loss_ce: 0.050301
2022-01-09 00:51:27,118 iteration 554 : loss : 0.107657, loss_ce: 0.049032
2022-01-09 00:51:28,563 iteration 555 : loss : 0.132454, loss_ce: 0.034532
2022-01-09 00:51:30,078 iteration 556 : loss : 0.112438, loss_ce: 0.052827
2022-01-09 00:51:31,430 iteration 557 : loss : 0.132947, loss_ce: 0.050699
2022-01-09 00:51:32,934 iteration 558 : loss : 0.124895, loss_ce: 0.035587
2022-01-09 00:51:34,380 iteration 559 : loss : 0.123926, loss_ce: 0.045095
2022-01-09 00:51:35,865 iteration 560 : loss : 0.106052, loss_ce: 0.027721
2022-01-09 00:51:37,373 iteration 561 : loss : 0.173934, loss_ce: 0.083088
  8%|██▍                           | 33/400 [15:06<2:41:32, 26.41s/it]2022-01-09 00:51:39,056 iteration 562 : loss : 0.152769, loss_ce: 0.066675
2022-01-09 00:51:40,396 iteration 563 : loss : 0.100674, loss_ce: 0.040718
2022-01-09 00:51:41,887 iteration 564 : loss : 0.139935, loss_ce: 0.071661
2022-01-09 00:51:43,379 iteration 565 : loss : 0.119330, loss_ce: 0.051570
2022-01-09 00:51:44,912 iteration 566 : loss : 0.106168, loss_ce: 0.046077
2022-01-09 00:51:46,339 iteration 567 : loss : 0.132384, loss_ce: 0.045693
2022-01-09 00:51:47,763 iteration 568 : loss : 0.166548, loss_ce: 0.056185
2022-01-09 00:51:49,222 iteration 569 : loss : 0.130863, loss_ce: 0.053749
2022-01-09 00:51:50,730 iteration 570 : loss : 0.085409, loss_ce: 0.039155
2022-01-09 00:51:52,162 iteration 571 : loss : 0.207264, loss_ce: 0.093879
2022-01-09 00:51:53,646 iteration 572 : loss : 0.125969, loss_ce: 0.042365
2022-01-09 00:51:55,061 iteration 573 : loss : 0.142170, loss_ce: 0.067552
2022-01-09 00:51:56,520 iteration 574 : loss : 0.146851, loss_ce: 0.063923
2022-01-09 00:51:58,065 iteration 575 : loss : 0.094430, loss_ce: 0.026577
2022-01-09 00:51:59,583 iteration 576 : loss : 0.139100, loss_ce: 0.055831
2022-01-09 00:52:01,040 iteration 577 : loss : 0.110688, loss_ce: 0.046390
2022-01-09 00:52:02,442 iteration 578 : loss : 0.103408, loss_ce: 0.042853
  8%|██▌                           | 34/400 [15:31<2:38:37, 26.00s/it]2022-01-09 00:52:04,198 iteration 579 : loss : 0.109843, loss_ce: 0.044672
2022-01-09 00:52:05,686 iteration 580 : loss : 0.142253, loss_ce: 0.057653
2022-01-09 00:52:07,214 iteration 581 : loss : 0.133845, loss_ce: 0.047713
2022-01-09 00:52:08,673 iteration 582 : loss : 0.119270, loss_ce: 0.044619
2022-01-09 00:52:10,147 iteration 583 : loss : 0.229226, loss_ce: 0.085023
2022-01-09 00:52:11,572 iteration 584 : loss : 0.096446, loss_ce: 0.042420
2022-01-09 00:52:13,103 iteration 585 : loss : 0.103610, loss_ce: 0.041376
2022-01-09 00:52:14,516 iteration 586 : loss : 0.163387, loss_ce: 0.064785
2022-01-09 00:52:15,921 iteration 587 : loss : 0.092151, loss_ce: 0.040889
2022-01-09 00:52:17,455 iteration 588 : loss : 0.133018, loss_ce: 0.048041
2022-01-09 00:52:18,901 iteration 589 : loss : 0.099374, loss_ce: 0.034542
2022-01-09 00:52:20,361 iteration 590 : loss : 0.117872, loss_ce: 0.043796
2022-01-09 00:52:21,832 iteration 591 : loss : 0.105660, loss_ce: 0.040282
2022-01-09 00:52:23,301 iteration 592 : loss : 0.118383, loss_ce: 0.048662
2022-01-09 00:52:24,811 iteration 593 : loss : 0.069263, loss_ce: 0.025602
2022-01-09 00:52:26,294 iteration 594 : loss : 0.097874, loss_ce: 0.038728
2022-01-09 00:52:26,295 Training Data Eval:
2022-01-09 00:52:33,569   Average segmentation loss on training set: 0.0914
2022-01-09 00:52:33,569 Validation Data Eval:
2022-01-09 00:52:36,067   Average segmentation loss on validation set: 0.1698
2022-01-09 00:52:37,650 iteration 595 : loss : 0.155514, loss_ce: 0.088869
  9%|██▋                           | 35/400 [16:06<2:54:59, 28.77s/it]2022-01-09 00:52:39,215 iteration 596 : loss : 0.101560, loss_ce: 0.044964
2022-01-09 00:52:40,654 iteration 597 : loss : 0.130537, loss_ce: 0.040566
2022-01-09 00:52:42,068 iteration 598 : loss : 0.089027, loss_ce: 0.031604
2022-01-09 00:52:43,504 iteration 599 : loss : 0.104500, loss_ce: 0.036601
2022-01-09 00:52:44,944 iteration 600 : loss : 0.113611, loss_ce: 0.049608
2022-01-09 00:52:46,416 iteration 601 : loss : 0.117430, loss_ce: 0.049530
2022-01-09 00:52:47,904 iteration 602 : loss : 0.110371, loss_ce: 0.042905
2022-01-09 00:52:49,275 iteration 603 : loss : 0.121013, loss_ce: 0.045046
2022-01-09 00:52:50,668 iteration 604 : loss : 0.105246, loss_ce: 0.039091
2022-01-09 00:52:52,042 iteration 605 : loss : 0.107989, loss_ce: 0.039887
2022-01-09 00:52:53,682 iteration 606 : loss : 0.096776, loss_ce: 0.043468
2022-01-09 00:52:55,131 iteration 607 : loss : 0.141294, loss_ce: 0.056408
2022-01-09 00:52:56,590 iteration 608 : loss : 0.127879, loss_ce: 0.055609
2022-01-09 00:52:57,942 iteration 609 : loss : 0.101350, loss_ce: 0.044140
2022-01-09 00:52:59,463 iteration 610 : loss : 0.079201, loss_ce: 0.033245
2022-01-09 00:53:01,064 iteration 611 : loss : 0.156951, loss_ce: 0.074642
2022-01-09 00:53:02,556 iteration 612 : loss : 0.088197, loss_ce: 0.027960
  9%|██▋                           | 36/400 [16:31<2:47:30, 27.61s/it]2022-01-09 00:53:04,131 iteration 613 : loss : 0.096961, loss_ce: 0.037397
2022-01-09 00:53:05,542 iteration 614 : loss : 0.098964, loss_ce: 0.034401
2022-01-09 00:53:06,946 iteration 615 : loss : 0.169160, loss_ce: 0.083896
2022-01-09 00:53:08,504 iteration 616 : loss : 0.104904, loss_ce: 0.043475
2022-01-09 00:53:09,917 iteration 617 : loss : 0.113576, loss_ce: 0.054085
2022-01-09 00:53:11,398 iteration 618 : loss : 0.152619, loss_ce: 0.041614
2022-01-09 00:53:12,869 iteration 619 : loss : 0.204216, loss_ce: 0.082464
2022-01-09 00:53:14,471 iteration 620 : loss : 0.205891, loss_ce: 0.065710
2022-01-09 00:53:15,942 iteration 621 : loss : 0.091375, loss_ce: 0.033864
2022-01-09 00:53:17,578 iteration 622 : loss : 0.103661, loss_ce: 0.042298
2022-01-09 00:53:19,187 iteration 623 : loss : 0.144749, loss_ce: 0.076139
2022-01-09 00:53:20,640 iteration 624 : loss : 0.114474, loss_ce: 0.035852
2022-01-09 00:53:22,039 iteration 625 : loss : 0.105609, loss_ce: 0.043133
2022-01-09 00:53:23,563 iteration 626 : loss : 0.099476, loss_ce: 0.043355
2022-01-09 00:53:25,102 iteration 627 : loss : 0.113645, loss_ce: 0.045853
2022-01-09 00:53:26,712 iteration 628 : loss : 0.124935, loss_ce: 0.041546
2022-01-09 00:53:28,161 iteration 629 : loss : 0.092914, loss_ce: 0.039438
  9%|██▊                           | 37/400 [16:57<2:43:23, 27.01s/it]2022-01-09 00:53:29,742 iteration 630 : loss : 0.091495, loss_ce: 0.039409
2022-01-09 00:53:31,177 iteration 631 : loss : 0.070971, loss_ce: 0.031006
2022-01-09 00:53:32,615 iteration 632 : loss : 0.147015, loss_ce: 0.059129
2022-01-09 00:53:34,185 iteration 633 : loss : 0.083465, loss_ce: 0.030170
2022-01-09 00:53:35,664 iteration 634 : loss : 0.179918, loss_ce: 0.068643
2022-01-09 00:53:37,267 iteration 635 : loss : 0.113107, loss_ce: 0.044966
2022-01-09 00:53:38,769 iteration 636 : loss : 0.107579, loss_ce: 0.041758
2022-01-09 00:53:40,156 iteration 637 : loss : 0.094854, loss_ce: 0.039049
2022-01-09 00:53:41,736 iteration 638 : loss : 0.155792, loss_ce: 0.051749
2022-01-09 00:53:43,155 iteration 639 : loss : 0.119371, loss_ce: 0.054520
2022-01-09 00:53:44,543 iteration 640 : loss : 0.146389, loss_ce: 0.044352
2022-01-09 00:53:45,930 iteration 641 : loss : 0.118705, loss_ce: 0.047770
2022-01-09 00:53:47,327 iteration 642 : loss : 0.133616, loss_ce: 0.038387
2022-01-09 00:53:48,766 iteration 643 : loss : 0.115295, loss_ce: 0.043390
2022-01-09 00:53:50,217 iteration 644 : loss : 0.107357, loss_ce: 0.041787
2022-01-09 00:53:51,625 iteration 645 : loss : 0.108453, loss_ce: 0.035199
2022-01-09 00:53:53,175 iteration 646 : loss : 0.102174, loss_ce: 0.043485
 10%|██▊                           | 38/400 [17:22<2:39:19, 26.41s/it]2022-01-09 00:53:54,677 iteration 647 : loss : 0.135109, loss_ce: 0.053372
2022-01-09 00:53:56,141 iteration 648 : loss : 0.106074, loss_ce: 0.043992
2022-01-09 00:53:57,646 iteration 649 : loss : 0.102981, loss_ce: 0.038981
2022-01-09 00:53:59,207 iteration 650 : loss : 0.109635, loss_ce: 0.040833
2022-01-09 00:54:00,673 iteration 651 : loss : 0.070134, loss_ce: 0.028245
2022-01-09 00:54:01,999 iteration 652 : loss : 0.064835, loss_ce: 0.024026
2022-01-09 00:54:03,558 iteration 653 : loss : 0.103531, loss_ce: 0.041941
2022-01-09 00:54:04,957 iteration 654 : loss : 0.111011, loss_ce: 0.039868
2022-01-09 00:54:06,437 iteration 655 : loss : 0.126805, loss_ce: 0.038649
2022-01-09 00:54:07,937 iteration 656 : loss : 0.084989, loss_ce: 0.030817
2022-01-09 00:54:09,521 iteration 657 : loss : 0.062314, loss_ce: 0.023914
2022-01-09 00:54:11,035 iteration 658 : loss : 0.106810, loss_ce: 0.036980
2022-01-09 00:54:12,545 iteration 659 : loss : 0.134876, loss_ce: 0.052866
2022-01-09 00:54:13,985 iteration 660 : loss : 0.078419, loss_ce: 0.035908
2022-01-09 00:54:15,494 iteration 661 : loss : 0.113778, loss_ce: 0.050464
2022-01-09 00:54:17,101 iteration 662 : loss : 0.138266, loss_ce: 0.036866
2022-01-09 00:54:18,579 iteration 663 : loss : 0.115787, loss_ce: 0.050087
 10%|██▉                           | 39/400 [17:47<2:37:03, 26.10s/it]2022-01-09 00:54:20,151 iteration 664 : loss : 0.083630, loss_ce: 0.033855
2022-01-09 00:54:21,526 iteration 665 : loss : 0.096506, loss_ce: 0.038260
2022-01-09 00:54:22,978 iteration 666 : loss : 0.081193, loss_ce: 0.028004
2022-01-09 00:54:24,490 iteration 667 : loss : 0.060554, loss_ce: 0.024865
2022-01-09 00:54:25,964 iteration 668 : loss : 0.109953, loss_ce: 0.041790
2022-01-09 00:54:27,392 iteration 669 : loss : 0.097863, loss_ce: 0.033263
2022-01-09 00:54:29,025 iteration 670 : loss : 0.078726, loss_ce: 0.030805
2022-01-09 00:54:30,469 iteration 671 : loss : 0.090707, loss_ce: 0.029410
2022-01-09 00:54:32,031 iteration 672 : loss : 0.090213, loss_ce: 0.030254
2022-01-09 00:54:33,559 iteration 673 : loss : 0.094753, loss_ce: 0.040614
2022-01-09 00:54:35,065 iteration 674 : loss : 0.082401, loss_ce: 0.039033
2022-01-09 00:54:36,600 iteration 675 : loss : 0.139786, loss_ce: 0.067857
2022-01-09 00:54:38,061 iteration 676 : loss : 0.093482, loss_ce: 0.035844
2022-01-09 00:54:39,538 iteration 677 : loss : 0.080449, loss_ce: 0.038803
2022-01-09 00:54:41,052 iteration 678 : loss : 0.135619, loss_ce: 0.046425
2022-01-09 00:54:42,533 iteration 679 : loss : 0.083484, loss_ce: 0.035235
2022-01-09 00:54:42,534 Training Data Eval:
2022-01-09 00:54:49,803   Average segmentation loss on training set: 0.0820
2022-01-09 00:54:49,803 Validation Data Eval:
2022-01-09 00:54:52,293   Average segmentation loss on validation set: 0.1594
2022-01-09 00:54:53,765 iteration 680 : loss : 0.086576, loss_ce: 0.031132
 10%|███                           | 40/400 [18:22<2:52:59, 28.83s/it]2022-01-09 00:54:55,335 iteration 681 : loss : 0.073041, loss_ce: 0.026815
2022-01-09 00:54:56,705 iteration 682 : loss : 0.159568, loss_ce: 0.053248
2022-01-09 00:54:58,169 iteration 683 : loss : 0.085671, loss_ce: 0.038779
2022-01-09 00:54:59,582 iteration 684 : loss : 0.088529, loss_ce: 0.032201
2022-01-09 00:55:01,129 iteration 685 : loss : 0.080113, loss_ce: 0.033991
2022-01-09 00:55:02,601 iteration 686 : loss : 0.144801, loss_ce: 0.052318
2022-01-09 00:55:04,059 iteration 687 : loss : 0.089970, loss_ce: 0.042292
2022-01-09 00:55:05,491 iteration 688 : loss : 0.109115, loss_ce: 0.042185
2022-01-09 00:55:06,985 iteration 689 : loss : 0.073824, loss_ce: 0.037868
2022-01-09 00:55:08,439 iteration 690 : loss : 0.072530, loss_ce: 0.026432
2022-01-09 00:55:09,917 iteration 691 : loss : 0.075407, loss_ce: 0.029755
2022-01-09 00:55:11,461 iteration 692 : loss : 0.090855, loss_ce: 0.029762
2022-01-09 00:55:12,845 iteration 693 : loss : 0.090832, loss_ce: 0.040742
2022-01-09 00:55:14,328 iteration 694 : loss : 0.087912, loss_ce: 0.038396
2022-01-09 00:55:15,743 iteration 695 : loss : 0.118347, loss_ce: 0.053158
2022-01-09 00:55:17,291 iteration 696 : loss : 0.055791, loss_ce: 0.022512
2022-01-09 00:55:18,760 iteration 697 : loss : 0.084556, loss_ce: 0.030927
 10%|███                           | 41/400 [18:47<2:45:38, 27.68s/it]2022-01-09 00:55:20,246 iteration 698 : loss : 0.110436, loss_ce: 0.037175
2022-01-09 00:55:21,760 iteration 699 : loss : 0.103334, loss_ce: 0.047353
2022-01-09 00:55:23,197 iteration 700 : loss : 0.100310, loss_ce: 0.042625
2022-01-09 00:55:24,633 iteration 701 : loss : 0.109059, loss_ce: 0.050861
2022-01-09 00:55:26,139 iteration 702 : loss : 0.130102, loss_ce: 0.050426
2022-01-09 00:55:27,714 iteration 703 : loss : 0.059974, loss_ce: 0.027104
2022-01-09 00:55:29,101 iteration 704 : loss : 0.066329, loss_ce: 0.026524
2022-01-09 00:55:30,572 iteration 705 : loss : 0.105994, loss_ce: 0.038884
2022-01-09 00:55:32,055 iteration 706 : loss : 0.085755, loss_ce: 0.032966
2022-01-09 00:55:33,432 iteration 707 : loss : 0.077900, loss_ce: 0.035079
2022-01-09 00:55:34,884 iteration 708 : loss : 0.062041, loss_ce: 0.022916
2022-01-09 00:55:36,341 iteration 709 : loss : 0.095567, loss_ce: 0.040792
2022-01-09 00:55:37,845 iteration 710 : loss : 0.100984, loss_ce: 0.041102
2022-01-09 00:55:39,341 iteration 711 : loss : 0.104532, loss_ce: 0.046162
2022-01-09 00:55:40,929 iteration 712 : loss : 0.096883, loss_ce: 0.037589
2022-01-09 00:55:42,378 iteration 713 : loss : 0.075639, loss_ce: 0.031509
2022-01-09 00:55:43,935 iteration 714 : loss : 0.114282, loss_ce: 0.038978
 10%|███▏                          | 42/400 [19:12<2:40:41, 26.93s/it]2022-01-09 00:55:45,530 iteration 715 : loss : 0.074851, loss_ce: 0.027018
2022-01-09 00:55:47,066 iteration 716 : loss : 0.068684, loss_ce: 0.028869
2022-01-09 00:55:48,538 iteration 717 : loss : 0.085564, loss_ce: 0.027610
2022-01-09 00:55:50,033 iteration 718 : loss : 0.101203, loss_ce: 0.049707
2022-01-09 00:55:51,580 iteration 719 : loss : 0.095354, loss_ce: 0.037941
2022-01-09 00:55:53,046 iteration 720 : loss : 0.097020, loss_ce: 0.040343
2022-01-09 00:55:54,510 iteration 721 : loss : 0.074377, loss_ce: 0.025118
2022-01-09 00:55:55,961 iteration 722 : loss : 0.109193, loss_ce: 0.040919
2022-01-09 00:55:57,482 iteration 723 : loss : 0.101573, loss_ce: 0.049259
2022-01-09 00:55:58,925 iteration 724 : loss : 0.129995, loss_ce: 0.058989
2022-01-09 00:56:00,466 iteration 725 : loss : 0.082070, loss_ce: 0.031161
2022-01-09 00:56:01,929 iteration 726 : loss : 0.111791, loss_ce: 0.042273
2022-01-09 00:56:03,304 iteration 727 : loss : 0.064573, loss_ce: 0.025973
2022-01-09 00:56:04,810 iteration 728 : loss : 0.069192, loss_ce: 0.029915
2022-01-09 00:56:06,335 iteration 729 : loss : 0.068344, loss_ce: 0.028737
2022-01-09 00:56:07,822 iteration 730 : loss : 0.082090, loss_ce: 0.029577
2022-01-09 00:56:09,207 iteration 731 : loss : 0.069554, loss_ce: 0.025915
 11%|███▏                          | 43/400 [19:38<2:37:16, 26.43s/it]2022-01-09 00:56:10,657 iteration 732 : loss : 0.094179, loss_ce: 0.042988
2022-01-09 00:56:12,016 iteration 733 : loss : 0.095904, loss_ce: 0.049109
2022-01-09 00:56:13,534 iteration 734 : loss : 0.062789, loss_ce: 0.024556
2022-01-09 00:56:14,973 iteration 735 : loss : 0.084483, loss_ce: 0.030362
2022-01-09 00:56:16,390 iteration 736 : loss : 0.087578, loss_ce: 0.039776
2022-01-09 00:56:18,009 iteration 737 : loss : 0.096072, loss_ce: 0.043960
2022-01-09 00:56:19,361 iteration 738 : loss : 0.120828, loss_ce: 0.066526
2022-01-09 00:56:20,864 iteration 739 : loss : 0.109893, loss_ce: 0.033160
2022-01-09 00:56:22,298 iteration 740 : loss : 0.096742, loss_ce: 0.037274
2022-01-09 00:56:23,711 iteration 741 : loss : 0.083100, loss_ce: 0.031471
2022-01-09 00:56:25,072 iteration 742 : loss : 0.118187, loss_ce: 0.055139
2022-01-09 00:56:26,539 iteration 743 : loss : 0.068638, loss_ce: 0.024927
2022-01-09 00:56:28,039 iteration 744 : loss : 0.087604, loss_ce: 0.035082
2022-01-09 00:56:29,421 iteration 745 : loss : 0.149156, loss_ce: 0.036295
2022-01-09 00:56:30,853 iteration 746 : loss : 0.077591, loss_ce: 0.027580
2022-01-09 00:56:32,260 iteration 747 : loss : 0.094585, loss_ce: 0.039794
2022-01-09 00:56:33,658 iteration 748 : loss : 0.077112, loss_ce: 0.029389
 11%|███▎                          | 44/400 [20:02<2:33:18, 25.84s/it]2022-01-09 00:56:35,104 iteration 749 : loss : 0.222076, loss_ce: 0.054835
2022-01-09 00:56:36,533 iteration 750 : loss : 0.072800, loss_ce: 0.027221
2022-01-09 00:56:38,009 iteration 751 : loss : 0.077670, loss_ce: 0.025811
2022-01-09 00:56:39,440 iteration 752 : loss : 0.127367, loss_ce: 0.058458
2022-01-09 00:56:40,830 iteration 753 : loss : 0.095399, loss_ce: 0.038626
2022-01-09 00:56:42,233 iteration 754 : loss : 0.128012, loss_ce: 0.048154
2022-01-09 00:56:43,570 iteration 755 : loss : 0.110851, loss_ce: 0.055134
2022-01-09 00:56:44,921 iteration 756 : loss : 0.085211, loss_ce: 0.030290
2022-01-09 00:56:46,357 iteration 757 : loss : 0.104231, loss_ce: 0.048206
2022-01-09 00:56:47,929 iteration 758 : loss : 0.101598, loss_ce: 0.045465
2022-01-09 00:56:49,574 iteration 759 : loss : 0.126164, loss_ce: 0.052553
2022-01-09 00:56:51,005 iteration 760 : loss : 0.087714, loss_ce: 0.046610
2022-01-09 00:56:52,508 iteration 761 : loss : 0.097185, loss_ce: 0.040412
2022-01-09 00:56:54,029 iteration 762 : loss : 0.113658, loss_ce: 0.041671
2022-01-09 00:56:55,555 iteration 763 : loss : 0.077938, loss_ce: 0.029287
2022-01-09 00:56:56,977 iteration 764 : loss : 0.095824, loss_ce: 0.039766
2022-01-09 00:56:56,977 Training Data Eval:
2022-01-09 00:57:04,253   Average segmentation loss on training set: 0.0776
2022-01-09 00:57:04,254 Validation Data Eval:
2022-01-09 00:57:06,739   Average segmentation loss on validation set: 0.1413
2022-01-09 00:57:12,578 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 00:57:14,285 iteration 765 : loss : 0.094890, loss_ce: 0.035992
 11%|███▍                          | 45/400 [20:43<2:59:08, 30.28s/it]2022-01-09 00:57:15,882 iteration 766 : loss : 0.146253, loss_ce: 0.067607
2022-01-09 00:57:17,389 iteration 767 : loss : 0.095295, loss_ce: 0.032909
2022-01-09 00:57:18,876 iteration 768 : loss : 0.146539, loss_ce: 0.045087
2022-01-09 00:57:20,393 iteration 769 : loss : 0.107976, loss_ce: 0.043934
2022-01-09 00:57:21,750 iteration 770 : loss : 0.084558, loss_ce: 0.034157
2022-01-09 00:57:23,226 iteration 771 : loss : 0.098763, loss_ce: 0.026442
2022-01-09 00:57:24,708 iteration 772 : loss : 0.072290, loss_ce: 0.028780
2022-01-09 00:57:26,209 iteration 773 : loss : 0.116316, loss_ce: 0.029954
2022-01-09 00:57:27,819 iteration 774 : loss : 0.102560, loss_ce: 0.044146
2022-01-09 00:57:29,256 iteration 775 : loss : 0.107129, loss_ce: 0.055201
2022-01-09 00:57:30,660 iteration 776 : loss : 0.077243, loss_ce: 0.027536
2022-01-09 00:57:32,360 iteration 777 : loss : 0.111494, loss_ce: 0.048298
2022-01-09 00:57:33,730 iteration 778 : loss : 0.165025, loss_ce: 0.055297
2022-01-09 00:57:35,274 iteration 779 : loss : 0.095167, loss_ce: 0.040473
2022-01-09 00:57:36,816 iteration 780 : loss : 0.097767, loss_ce: 0.038072
2022-01-09 00:57:38,221 iteration 781 : loss : 0.089556, loss_ce: 0.045520
2022-01-09 00:57:39,572 iteration 782 : loss : 0.073283, loss_ce: 0.037264
 12%|███▍                          | 46/400 [21:08<2:49:47, 28.78s/it]2022-01-09 00:57:41,073 iteration 783 : loss : 0.092962, loss_ce: 0.045261
2022-01-09 00:57:42,483 iteration 784 : loss : 0.103789, loss_ce: 0.036005
2022-01-09 00:57:43,943 iteration 785 : loss : 0.107481, loss_ce: 0.048412
2022-01-09 00:57:45,372 iteration 786 : loss : 0.117105, loss_ce: 0.039315
2022-01-09 00:57:46,813 iteration 787 : loss : 0.074001, loss_ce: 0.029522
2022-01-09 00:57:48,348 iteration 788 : loss : 0.079595, loss_ce: 0.036095
2022-01-09 00:57:49,695 iteration 789 : loss : 0.101394, loss_ce: 0.037557
2022-01-09 00:57:51,222 iteration 790 : loss : 0.105860, loss_ce: 0.033150
2022-01-09 00:57:52,673 iteration 791 : loss : 0.066943, loss_ce: 0.027391
2022-01-09 00:57:54,054 iteration 792 : loss : 0.082499, loss_ce: 0.035563
2022-01-09 00:57:55,594 iteration 793 : loss : 0.102105, loss_ce: 0.050331
2022-01-09 00:57:56,937 iteration 794 : loss : 0.060669, loss_ce: 0.027458
2022-01-09 00:57:58,308 iteration 795 : loss : 0.074419, loss_ce: 0.029524
2022-01-09 00:57:59,682 iteration 796 : loss : 0.087831, loss_ce: 0.039258
2022-01-09 00:58:01,189 iteration 797 : loss : 0.131936, loss_ce: 0.051090
2022-01-09 00:58:02,617 iteration 798 : loss : 0.094466, loss_ce: 0.034646
2022-01-09 00:58:03,975 iteration 799 : loss : 0.067968, loss_ce: 0.033725
 12%|███▌                          | 47/400 [21:32<2:41:35, 27.47s/it]2022-01-09 00:58:05,642 iteration 800 : loss : 0.086611, loss_ce: 0.035052
2022-01-09 00:58:07,132 iteration 801 : loss : 0.083948, loss_ce: 0.034092
2022-01-09 00:58:08,589 iteration 802 : loss : 0.108831, loss_ce: 0.037569
2022-01-09 00:58:10,069 iteration 803 : loss : 0.075285, loss_ce: 0.039815
2022-01-09 00:58:11,638 iteration 804 : loss : 0.131660, loss_ce: 0.061074
2022-01-09 00:58:13,313 iteration 805 : loss : 0.075539, loss_ce: 0.024501
2022-01-09 00:58:14,746 iteration 806 : loss : 0.116898, loss_ce: 0.062401
2022-01-09 00:58:16,132 iteration 807 : loss : 0.075416, loss_ce: 0.030339
2022-01-09 00:58:17,640 iteration 808 : loss : 0.081198, loss_ce: 0.038020
2022-01-09 00:58:19,115 iteration 809 : loss : 0.079908, loss_ce: 0.028024
2022-01-09 00:58:20,532 iteration 810 : loss : 0.124836, loss_ce: 0.044693
2022-01-09 00:58:21,903 iteration 811 : loss : 0.124470, loss_ce: 0.067133
2022-01-09 00:58:23,317 iteration 812 : loss : 0.075883, loss_ce: 0.036595
2022-01-09 00:58:24,769 iteration 813 : loss : 0.139907, loss_ce: 0.042385
2022-01-09 00:58:26,374 iteration 814 : loss : 0.121446, loss_ce: 0.042235
2022-01-09 00:58:27,927 iteration 815 : loss : 0.115276, loss_ce: 0.050064
2022-01-09 00:58:29,405 iteration 816 : loss : 0.085168, loss_ce: 0.030581
 12%|███▌                          | 48/400 [21:58<2:37:32, 26.85s/it]2022-01-09 00:58:30,923 iteration 817 : loss : 0.090679, loss_ce: 0.038177
2022-01-09 00:58:32,309 iteration 818 : loss : 0.079416, loss_ce: 0.036291
2022-01-09 00:58:33,764 iteration 819 : loss : 0.067386, loss_ce: 0.024968
2022-01-09 00:58:35,145 iteration 820 : loss : 0.076683, loss_ce: 0.034370
2022-01-09 00:58:36,702 iteration 821 : loss : 0.075497, loss_ce: 0.026899
2022-01-09 00:58:38,219 iteration 822 : loss : 0.104669, loss_ce: 0.037508
2022-01-09 00:58:39,628 iteration 823 : loss : 0.063184, loss_ce: 0.024385
2022-01-09 00:58:41,040 iteration 824 : loss : 0.074394, loss_ce: 0.033157
2022-01-09 00:58:42,513 iteration 825 : loss : 0.074205, loss_ce: 0.029746
2022-01-09 00:58:43,958 iteration 826 : loss : 0.119396, loss_ce: 0.036890
2022-01-09 00:58:45,404 iteration 827 : loss : 0.145129, loss_ce: 0.052972
2022-01-09 00:58:46,849 iteration 828 : loss : 0.073277, loss_ce: 0.025493
2022-01-09 00:58:48,328 iteration 829 : loss : 0.076335, loss_ce: 0.030806
2022-01-09 00:58:49,857 iteration 830 : loss : 0.076948, loss_ce: 0.026252
2022-01-09 00:58:51,311 iteration 831 : loss : 0.088331, loss_ce: 0.038454
2022-01-09 00:58:52,731 iteration 832 : loss : 0.058405, loss_ce: 0.019975
2022-01-09 00:58:54,262 iteration 833 : loss : 0.103323, loss_ce: 0.053631
 12%|███▋                          | 49/400 [22:23<2:33:35, 26.25s/it]2022-01-09 00:58:55,775 iteration 834 : loss : 0.069895, loss_ce: 0.027246
2022-01-09 00:58:57,096 iteration 835 : loss : 0.100154, loss_ce: 0.027140
2022-01-09 00:58:58,563 iteration 836 : loss : 0.074545, loss_ce: 0.034872
2022-01-09 00:58:59,934 iteration 837 : loss : 0.067788, loss_ce: 0.034295
2022-01-09 00:59:01,263 iteration 838 : loss : 0.079637, loss_ce: 0.033017
2022-01-09 00:59:02,750 iteration 839 : loss : 0.092153, loss_ce: 0.039837
2022-01-09 00:59:04,183 iteration 840 : loss : 0.087668, loss_ce: 0.028459
2022-01-09 00:59:05,568 iteration 841 : loss : 0.099617, loss_ce: 0.044078
2022-01-09 00:59:06,992 iteration 842 : loss : 0.078937, loss_ce: 0.035138
2022-01-09 00:59:08,562 iteration 843 : loss : 0.103319, loss_ce: 0.036980
2022-01-09 00:59:09,996 iteration 844 : loss : 0.092091, loss_ce: 0.031920
2022-01-09 00:59:11,486 iteration 845 : loss : 0.108328, loss_ce: 0.036535
2022-01-09 00:59:13,104 iteration 846 : loss : 0.072421, loss_ce: 0.032333
2022-01-09 00:59:14,580 iteration 847 : loss : 0.088218, loss_ce: 0.035589
2022-01-09 00:59:15,914 iteration 848 : loss : 0.077732, loss_ce: 0.029817
2022-01-09 00:59:17,391 iteration 849 : loss : 0.074880, loss_ce: 0.029568
2022-01-09 00:59:17,391 Training Data Eval:
2022-01-09 00:59:24,963   Average segmentation loss on training set: 0.0977
2022-01-09 00:59:24,964 Validation Data Eval:
2022-01-09 00:59:27,620   Average segmentation loss on validation set: 0.1993
2022-01-09 00:59:29,189 iteration 850 : loss : 0.116553, loss_ce: 0.048341
 12%|███▊                          | 50/400 [22:58<2:48:20, 28.86s/it]2022-01-09 00:59:30,785 iteration 851 : loss : 0.057538, loss_ce: 0.027764
2022-01-09 00:59:32,249 iteration 852 : loss : 0.070896, loss_ce: 0.029463
2022-01-09 00:59:33,807 iteration 853 : loss : 0.079997, loss_ce: 0.033417
2022-01-09 00:59:35,313 iteration 854 : loss : 0.085063, loss_ce: 0.027693
2022-01-09 00:59:36,813 iteration 855 : loss : 0.068411, loss_ce: 0.027879
2022-01-09 00:59:38,333 iteration 856 : loss : 0.076650, loss_ce: 0.026962
2022-01-09 00:59:39,753 iteration 857 : loss : 0.073668, loss_ce: 0.031283
2022-01-09 00:59:41,091 iteration 858 : loss : 0.068964, loss_ce: 0.034923
2022-01-09 00:59:42,531 iteration 859 : loss : 0.071709, loss_ce: 0.028147
2022-01-09 00:59:43,917 iteration 860 : loss : 0.091965, loss_ce: 0.028480
2022-01-09 00:59:45,476 iteration 861 : loss : 0.089644, loss_ce: 0.036292
2022-01-09 00:59:46,855 iteration 862 : loss : 0.088657, loss_ce: 0.026303
2022-01-09 00:59:48,429 iteration 863 : loss : 0.118328, loss_ce: 0.053420
2022-01-09 00:59:49,968 iteration 864 : loss : 0.084661, loss_ce: 0.038328
2022-01-09 00:59:51,357 iteration 865 : loss : 0.078584, loss_ce: 0.029837
2022-01-09 00:59:52,909 iteration 866 : loss : 0.103444, loss_ce: 0.048105
2022-01-09 00:59:54,303 iteration 867 : loss : 0.047090, loss_ce: 0.018127
 13%|███▊                          | 51/400 [23:23<2:41:18, 27.73s/it]2022-01-09 00:59:55,886 iteration 868 : loss : 0.135609, loss_ce: 0.045467
2022-01-09 00:59:57,345 iteration 869 : loss : 0.088808, loss_ce: 0.037021
2022-01-09 00:59:58,689 iteration 870 : loss : 0.069313, loss_ce: 0.025612
2022-01-09 01:00:00,096 iteration 871 : loss : 0.066897, loss_ce: 0.026843
2022-01-09 01:00:01,525 iteration 872 : loss : 0.060663, loss_ce: 0.029267
2022-01-09 01:00:03,090 iteration 873 : loss : 0.077080, loss_ce: 0.026436
2022-01-09 01:00:04,442 iteration 874 : loss : 0.056368, loss_ce: 0.026999
2022-01-09 01:00:06,084 iteration 875 : loss : 0.113329, loss_ce: 0.040796
2022-01-09 01:00:07,542 iteration 876 : loss : 0.047573, loss_ce: 0.020299
2022-01-09 01:00:08,863 iteration 877 : loss : 0.055065, loss_ce: 0.019503
2022-01-09 01:00:10,372 iteration 878 : loss : 0.070456, loss_ce: 0.029473
2022-01-09 01:00:11,781 iteration 879 : loss : 0.088858, loss_ce: 0.037450
2022-01-09 01:00:13,266 iteration 880 : loss : 0.059384, loss_ce: 0.020512
2022-01-09 01:00:14,676 iteration 881 : loss : 0.086595, loss_ce: 0.037992
2022-01-09 01:00:16,183 iteration 882 : loss : 0.076840, loss_ce: 0.028548
2022-01-09 01:00:17,632 iteration 883 : loss : 0.082414, loss_ce: 0.038660
2022-01-09 01:00:19,216 iteration 884 : loss : 0.085679, loss_ce: 0.041059
 13%|███▉                          | 52/400 [23:48<2:35:57, 26.89s/it]2022-01-09 01:00:20,784 iteration 885 : loss : 0.082468, loss_ce: 0.033257
2022-01-09 01:00:22,289 iteration 886 : loss : 0.067887, loss_ce: 0.026354
2022-01-09 01:00:23,804 iteration 887 : loss : 0.119477, loss_ce: 0.049149
2022-01-09 01:00:25,266 iteration 888 : loss : 0.065242, loss_ce: 0.024802
2022-01-09 01:00:26,819 iteration 889 : loss : 0.077039, loss_ce: 0.025470
2022-01-09 01:00:28,426 iteration 890 : loss : 0.083223, loss_ce: 0.028499
2022-01-09 01:00:29,903 iteration 891 : loss : 0.063672, loss_ce: 0.022583
2022-01-09 01:00:31,367 iteration 892 : loss : 0.076799, loss_ce: 0.039040
2022-01-09 01:00:32,804 iteration 893 : loss : 0.101590, loss_ce: 0.043261
2022-01-09 01:00:34,415 iteration 894 : loss : 0.108187, loss_ce: 0.043900
2022-01-09 01:00:35,837 iteration 895 : loss : 0.104488, loss_ce: 0.030166
2022-01-09 01:00:37,378 iteration 896 : loss : 0.098980, loss_ce: 0.037980
2022-01-09 01:00:38,951 iteration 897 : loss : 0.095518, loss_ce: 0.043541
2022-01-09 01:00:40,391 iteration 898 : loss : 0.077450, loss_ce: 0.037707
2022-01-09 01:00:41,829 iteration 899 : loss : 0.077473, loss_ce: 0.035501
2022-01-09 01:00:43,210 iteration 900 : loss : 0.066368, loss_ce: 0.028206
2022-01-09 01:00:44,624 iteration 901 : loss : 0.102873, loss_ce: 0.030795
 13%|███▉                          | 53/400 [24:13<2:32:55, 26.44s/it]2022-01-09 01:00:46,195 iteration 902 : loss : 0.068353, loss_ce: 0.027694
2022-01-09 01:00:47,776 iteration 903 : loss : 0.072235, loss_ce: 0.029276
2022-01-09 01:00:49,284 iteration 904 : loss : 0.101163, loss_ce: 0.032562
2022-01-09 01:00:50,768 iteration 905 : loss : 0.093700, loss_ce: 0.045594
2022-01-09 01:00:52,219 iteration 906 : loss : 0.083279, loss_ce: 0.040397
2022-01-09 01:00:53,760 iteration 907 : loss : 0.078345, loss_ce: 0.025314
2022-01-09 01:00:55,091 iteration 908 : loss : 0.065381, loss_ce: 0.026007
2022-01-09 01:00:56,585 iteration 909 : loss : 0.069058, loss_ce: 0.027298
2022-01-09 01:00:57,952 iteration 910 : loss : 0.095615, loss_ce: 0.037720
2022-01-09 01:00:59,407 iteration 911 : loss : 0.066151, loss_ce: 0.026297
2022-01-09 01:01:00,878 iteration 912 : loss : 0.061352, loss_ce: 0.022081
2022-01-09 01:01:02,359 iteration 913 : loss : 0.086364, loss_ce: 0.026595
2022-01-09 01:01:03,863 iteration 914 : loss : 0.122730, loss_ce: 0.054423
2022-01-09 01:01:05,302 iteration 915 : loss : 0.099975, loss_ce: 0.030289
2022-01-09 01:01:06,751 iteration 916 : loss : 0.057463, loss_ce: 0.019316
2022-01-09 01:01:08,252 iteration 917 : loss : 0.095109, loss_ce: 0.037383
2022-01-09 01:01:09,783 iteration 918 : loss : 0.079962, loss_ce: 0.029036
 14%|████                          | 54/400 [24:38<2:30:15, 26.05s/it]2022-01-09 01:01:11,394 iteration 919 : loss : 0.071074, loss_ce: 0.032443
2022-01-09 01:01:12,907 iteration 920 : loss : 0.082906, loss_ce: 0.035918
2022-01-09 01:01:14,442 iteration 921 : loss : 0.078900, loss_ce: 0.031619
2022-01-09 01:01:15,998 iteration 922 : loss : 0.097750, loss_ce: 0.050115
2022-01-09 01:01:17,432 iteration 923 : loss : 0.067565, loss_ce: 0.029895
2022-01-09 01:01:18,961 iteration 924 : loss : 0.137693, loss_ce: 0.040155
2022-01-09 01:01:20,360 iteration 925 : loss : 0.051648, loss_ce: 0.018457
2022-01-09 01:01:21,884 iteration 926 : loss : 0.060306, loss_ce: 0.025128
2022-01-09 01:01:23,248 iteration 927 : loss : 0.090165, loss_ce: 0.033511
2022-01-09 01:01:24,690 iteration 928 : loss : 0.070717, loss_ce: 0.029657
2022-01-09 01:01:26,218 iteration 929 : loss : 0.086758, loss_ce: 0.035385
2022-01-09 01:01:27,714 iteration 930 : loss : 0.069824, loss_ce: 0.026401
2022-01-09 01:01:29,207 iteration 931 : loss : 0.130853, loss_ce: 0.028548
2022-01-09 01:01:30,700 iteration 932 : loss : 0.081795, loss_ce: 0.025927
2022-01-09 01:01:32,148 iteration 933 : loss : 0.097843, loss_ce: 0.038295
2022-01-09 01:01:33,639 iteration 934 : loss : 0.061789, loss_ce: 0.026369
2022-01-09 01:01:33,639 Training Data Eval:
2022-01-09 01:01:41,185   Average segmentation loss on training set: 0.0783
2022-01-09 01:01:41,186 Validation Data Eval:
2022-01-09 01:01:43,794   Average segmentation loss on validation set: 0.1748
2022-01-09 01:01:45,419 iteration 935 : loss : 0.096340, loss_ce: 0.036700
 14%|████▏                         | 55/400 [25:14<2:46:20, 28.93s/it]2022-01-09 01:01:46,932 iteration 936 : loss : 0.070650, loss_ce: 0.020481
2022-01-09 01:01:48,441 iteration 937 : loss : 0.069914, loss_ce: 0.020816
2022-01-09 01:01:49,993 iteration 938 : loss : 0.069565, loss_ce: 0.024736
2022-01-09 01:01:51,438 iteration 939 : loss : 0.067053, loss_ce: 0.025021
2022-01-09 01:01:52,956 iteration 940 : loss : 0.091962, loss_ce: 0.040537
2022-01-09 01:01:54,588 iteration 941 : loss : 0.082651, loss_ce: 0.028286
2022-01-09 01:01:56,014 iteration 942 : loss : 0.055910, loss_ce: 0.021155
2022-01-09 01:01:57,452 iteration 943 : loss : 0.063278, loss_ce: 0.027800
2022-01-09 01:01:58,855 iteration 944 : loss : 0.071544, loss_ce: 0.024505
2022-01-09 01:02:00,268 iteration 945 : loss : 0.066608, loss_ce: 0.023635
2022-01-09 01:02:01,687 iteration 946 : loss : 0.049281, loss_ce: 0.021563
2022-01-09 01:02:03,090 iteration 947 : loss : 0.067568, loss_ce: 0.024197
2022-01-09 01:02:04,485 iteration 948 : loss : 0.083494, loss_ce: 0.029734
2022-01-09 01:02:05,930 iteration 949 : loss : 0.069215, loss_ce: 0.024643
2022-01-09 01:02:07,362 iteration 950 : loss : 0.082835, loss_ce: 0.035530
2022-01-09 01:02:08,873 iteration 951 : loss : 0.072125, loss_ce: 0.045573
2022-01-09 01:02:10,276 iteration 952 : loss : 0.069870, loss_ce: 0.030636
 14%|████▏                         | 56/400 [25:39<2:38:51, 27.71s/it]2022-01-09 01:02:11,845 iteration 953 : loss : 0.075739, loss_ce: 0.026430
2022-01-09 01:02:13,368 iteration 954 : loss : 0.059377, loss_ce: 0.023937
2022-01-09 01:02:14,750 iteration 955 : loss : 0.053709, loss_ce: 0.024215
2022-01-09 01:02:16,268 iteration 956 : loss : 0.064813, loss_ce: 0.024705
2022-01-09 01:02:17,755 iteration 957 : loss : 0.067022, loss_ce: 0.022486
2022-01-09 01:02:19,257 iteration 958 : loss : 0.073509, loss_ce: 0.021412
2022-01-09 01:02:20,864 iteration 959 : loss : 0.080520, loss_ce: 0.036052
2022-01-09 01:02:22,535 iteration 960 : loss : 0.067302, loss_ce: 0.027752
2022-01-09 01:02:23,958 iteration 961 : loss : 0.118227, loss_ce: 0.035414
2022-01-09 01:02:25,331 iteration 962 : loss : 0.079140, loss_ce: 0.021820
2022-01-09 01:02:26,898 iteration 963 : loss : 0.092808, loss_ce: 0.040906
2022-01-09 01:02:28,407 iteration 964 : loss : 0.070022, loss_ce: 0.034174
2022-01-09 01:02:29,852 iteration 965 : loss : 0.063952, loss_ce: 0.026858
2022-01-09 01:02:31,331 iteration 966 : loss : 0.060623, loss_ce: 0.024403
2022-01-09 01:02:32,827 iteration 967 : loss : 0.052976, loss_ce: 0.021072
2022-01-09 01:02:34,262 iteration 968 : loss : 0.129542, loss_ce: 0.046499
2022-01-09 01:02:35,759 iteration 969 : loss : 0.062479, loss_ce: 0.025775
 14%|████▎                         | 57/400 [26:04<2:34:35, 27.04s/it]2022-01-09 01:02:37,260 iteration 970 : loss : 0.066575, loss_ce: 0.032966
2022-01-09 01:02:38,751 iteration 971 : loss : 0.080325, loss_ce: 0.023473
2022-01-09 01:02:40,124 iteration 972 : loss : 0.065219, loss_ce: 0.029702
2022-01-09 01:02:41,609 iteration 973 : loss : 0.069078, loss_ce: 0.025184
2022-01-09 01:02:42,996 iteration 974 : loss : 0.071571, loss_ce: 0.031972
2022-01-09 01:02:44,525 iteration 975 : loss : 0.094802, loss_ce: 0.036095
2022-01-09 01:02:45,958 iteration 976 : loss : 0.059113, loss_ce: 0.026288
2022-01-09 01:02:47,402 iteration 977 : loss : 0.067614, loss_ce: 0.024623
2022-01-09 01:02:48,890 iteration 978 : loss : 0.053900, loss_ce: 0.018338
2022-01-09 01:02:50,317 iteration 979 : loss : 0.112121, loss_ce: 0.064767
2022-01-09 01:02:51,749 iteration 980 : loss : 0.058716, loss_ce: 0.019909
2022-01-09 01:02:53,342 iteration 981 : loss : 0.070848, loss_ce: 0.035287
2022-01-09 01:02:54,773 iteration 982 : loss : 0.063062, loss_ce: 0.029332
2022-01-09 01:02:56,297 iteration 983 : loss : 0.105369, loss_ce: 0.035281
2022-01-09 01:02:57,655 iteration 984 : loss : 0.058280, loss_ce: 0.025837
2022-01-09 01:02:59,108 iteration 985 : loss : 0.054250, loss_ce: 0.017846
2022-01-09 01:03:00,646 iteration 986 : loss : 0.102607, loss_ce: 0.037738
 14%|████▎                         | 58/400 [26:29<2:30:28, 26.40s/it]2022-01-09 01:03:02,261 iteration 987 : loss : 0.060270, loss_ce: 0.026762
2022-01-09 01:03:03,765 iteration 988 : loss : 0.076560, loss_ce: 0.037646
2022-01-09 01:03:05,251 iteration 989 : loss : 0.076534, loss_ce: 0.026826
2022-01-09 01:03:06,710 iteration 990 : loss : 0.089129, loss_ce: 0.036270
2022-01-09 01:03:08,166 iteration 991 : loss : 0.086704, loss_ce: 0.038653
2022-01-09 01:03:09,746 iteration 992 : loss : 0.054172, loss_ce: 0.026221
2022-01-09 01:03:11,274 iteration 993 : loss : 0.070519, loss_ce: 0.026393
2022-01-09 01:03:12,904 iteration 994 : loss : 0.089609, loss_ce: 0.045574
2022-01-09 01:03:14,400 iteration 995 : loss : 0.102416, loss_ce: 0.033340
2022-01-09 01:03:15,937 iteration 996 : loss : 0.042555, loss_ce: 0.017504
2022-01-09 01:03:17,483 iteration 997 : loss : 0.086406, loss_ce: 0.034344
2022-01-09 01:03:18,881 iteration 998 : loss : 0.089990, loss_ce: 0.031843
2022-01-09 01:03:20,409 iteration 999 : loss : 0.088977, loss_ce: 0.033267
2022-01-09 01:03:21,821 iteration 1000 : loss : 0.107220, loss_ce: 0.027871
2022-01-09 01:03:23,248 iteration 1001 : loss : 0.118740, loss_ce: 0.028514
2022-01-09 01:03:24,850 iteration 1002 : loss : 0.060700, loss_ce: 0.024318
2022-01-09 01:03:26,417 iteration 1003 : loss : 0.084176, loss_ce: 0.032669
 15%|████▍                         | 59/400 [26:55<2:28:57, 26.21s/it]2022-01-09 01:03:27,961 iteration 1004 : loss : 0.136075, loss_ce: 0.061551
2022-01-09 01:03:29,416 iteration 1005 : loss : 0.078469, loss_ce: 0.027492
2022-01-09 01:03:31,004 iteration 1006 : loss : 0.117212, loss_ce: 0.048703
2022-01-09 01:03:32,420 iteration 1007 : loss : 0.071237, loss_ce: 0.037699
2022-01-09 01:03:33,811 iteration 1008 : loss : 0.138553, loss_ce: 0.052769
2022-01-09 01:03:35,210 iteration 1009 : loss : 0.078662, loss_ce: 0.030826
2022-01-09 01:03:36,617 iteration 1010 : loss : 0.076337, loss_ce: 0.028083
2022-01-09 01:03:38,153 iteration 1011 : loss : 0.077707, loss_ce: 0.031159
2022-01-09 01:03:39,594 iteration 1012 : loss : 0.050937, loss_ce: 0.018436
2022-01-09 01:03:40,976 iteration 1013 : loss : 0.063154, loss_ce: 0.020657
2022-01-09 01:03:42,449 iteration 1014 : loss : 0.084894, loss_ce: 0.034229
2022-01-09 01:03:43,817 iteration 1015 : loss : 0.070930, loss_ce: 0.030658
2022-01-09 01:03:45,291 iteration 1016 : loss : 0.070784, loss_ce: 0.029996
2022-01-09 01:03:46,623 iteration 1017 : loss : 0.056001, loss_ce: 0.020616
2022-01-09 01:03:47,952 iteration 1018 : loss : 0.077166, loss_ce: 0.032346
2022-01-09 01:03:49,379 iteration 1019 : loss : 0.114331, loss_ce: 0.056055
2022-01-09 01:03:49,379 Training Data Eval:
2022-01-09 01:03:56,652   Average segmentation loss on training set: 0.0698
2022-01-09 01:03:56,652 Validation Data Eval:
2022-01-09 01:03:59,134   Average segmentation loss on validation set: 0.1331
2022-01-09 01:04:05,011 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 01:04:06,417 iteration 1020 : loss : 0.052133, loss_ce: 0.018438
 15%|████▌                         | 60/400 [27:35<2:51:57, 30.35s/it]2022-01-09 01:04:08,035 iteration 1021 : loss : 0.120462, loss_ce: 0.030833
2022-01-09 01:04:09,472 iteration 1022 : loss : 0.070745, loss_ce: 0.028227
2022-01-09 01:04:10,921 iteration 1023 : loss : 0.121976, loss_ce: 0.034914
2022-01-09 01:04:12,450 iteration 1024 : loss : 0.054256, loss_ce: 0.020155
2022-01-09 01:04:13,914 iteration 1025 : loss : 0.061958, loss_ce: 0.026461
2022-01-09 01:04:15,253 iteration 1026 : loss : 0.059757, loss_ce: 0.021735
2022-01-09 01:04:16,763 iteration 1027 : loss : 0.086658, loss_ce: 0.034444
2022-01-09 01:04:18,263 iteration 1028 : loss : 0.094264, loss_ce: 0.040284
2022-01-09 01:04:19,703 iteration 1029 : loss : 0.071714, loss_ce: 0.029856
2022-01-09 01:04:21,143 iteration 1030 : loss : 0.072598, loss_ce: 0.026564
2022-01-09 01:04:22,671 iteration 1031 : loss : 0.084487, loss_ce: 0.033707
2022-01-09 01:04:24,171 iteration 1032 : loss : 0.090078, loss_ce: 0.034853
2022-01-09 01:04:25,591 iteration 1033 : loss : 0.075796, loss_ce: 0.029466
2022-01-09 01:04:27,141 iteration 1034 : loss : 0.096281, loss_ce: 0.035598
2022-01-09 01:04:28,618 iteration 1035 : loss : 0.064339, loss_ce: 0.028425
2022-01-09 01:04:30,106 iteration 1036 : loss : 0.072397, loss_ce: 0.032742
2022-01-09 01:04:31,524 iteration 1037 : loss : 0.084477, loss_ce: 0.035250
 15%|████▌                         | 61/400 [28:00<2:42:32, 28.77s/it]2022-01-09 01:04:33,073 iteration 1038 : loss : 0.051949, loss_ce: 0.020143
2022-01-09 01:04:34,538 iteration 1039 : loss : 0.068926, loss_ce: 0.032637
2022-01-09 01:04:36,050 iteration 1040 : loss : 0.081474, loss_ce: 0.034660
2022-01-09 01:04:37,541 iteration 1041 : loss : 0.067578, loss_ce: 0.029611
2022-01-09 01:04:39,036 iteration 1042 : loss : 0.094241, loss_ce: 0.040438
2022-01-09 01:04:40,514 iteration 1043 : loss : 0.111401, loss_ce: 0.034828
2022-01-09 01:04:42,011 iteration 1044 : loss : 0.063026, loss_ce: 0.021752
2022-01-09 01:04:43,357 iteration 1045 : loss : 0.053002, loss_ce: 0.023924
2022-01-09 01:04:44,747 iteration 1046 : loss : 0.095298, loss_ce: 0.022668
2022-01-09 01:04:46,269 iteration 1047 : loss : 0.122200, loss_ce: 0.047092
2022-01-09 01:04:47,733 iteration 1048 : loss : 0.094995, loss_ce: 0.033910
2022-01-09 01:04:49,201 iteration 1049 : loss : 0.094510, loss_ce: 0.031210
2022-01-09 01:04:50,723 iteration 1050 : loss : 0.090303, loss_ce: 0.030886
2022-01-09 01:04:52,181 iteration 1051 : loss : 0.082890, loss_ce: 0.035072
2022-01-09 01:04:53,639 iteration 1052 : loss : 0.086686, loss_ce: 0.038764
2022-01-09 01:04:54,999 iteration 1053 : loss : 0.078957, loss_ce: 0.021586
2022-01-09 01:04:56,554 iteration 1054 : loss : 0.070816, loss_ce: 0.029287
 16%|████▋                         | 62/400 [28:25<2:35:46, 27.65s/it]2022-01-09 01:04:58,083 iteration 1055 : loss : 0.058895, loss_ce: 0.020646
2022-01-09 01:04:59,506 iteration 1056 : loss : 0.068676, loss_ce: 0.032132
2022-01-09 01:05:00,943 iteration 1057 : loss : 0.105078, loss_ce: 0.046837
2022-01-09 01:05:02,400 iteration 1058 : loss : 0.088126, loss_ce: 0.042182
2022-01-09 01:05:03,805 iteration 1059 : loss : 0.071072, loss_ce: 0.025639
2022-01-09 01:05:05,328 iteration 1060 : loss : 0.056411, loss_ce: 0.020823
2022-01-09 01:05:06,891 iteration 1061 : loss : 0.066214, loss_ce: 0.025407
2022-01-09 01:05:08,449 iteration 1062 : loss : 0.091464, loss_ce: 0.039849
2022-01-09 01:05:10,079 iteration 1063 : loss : 0.097320, loss_ce: 0.030971
2022-01-09 01:05:11,601 iteration 1064 : loss : 0.095646, loss_ce: 0.030548
2022-01-09 01:05:13,046 iteration 1065 : loss : 0.078229, loss_ce: 0.031573
2022-01-09 01:05:14,636 iteration 1066 : loss : 0.065327, loss_ce: 0.026036
2022-01-09 01:05:16,152 iteration 1067 : loss : 0.160717, loss_ce: 0.041315
2022-01-09 01:05:17,744 iteration 1068 : loss : 0.069608, loss_ce: 0.033935
2022-01-09 01:05:19,300 iteration 1069 : loss : 0.092072, loss_ce: 0.040081
2022-01-09 01:05:20,815 iteration 1070 : loss : 0.062927, loss_ce: 0.027084
2022-01-09 01:05:22,321 iteration 1071 : loss : 0.082972, loss_ce: 0.043968
 16%|████▋                         | 63/400 [28:51<2:32:07, 27.09s/it]2022-01-09 01:05:24,023 iteration 1072 : loss : 0.061929, loss_ce: 0.021805
2022-01-09 01:05:25,491 iteration 1073 : loss : 0.102443, loss_ce: 0.026800
2022-01-09 01:05:26,967 iteration 1074 : loss : 0.064686, loss_ce: 0.026876
2022-01-09 01:05:28,513 iteration 1075 : loss : 0.088614, loss_ce: 0.032683
2022-01-09 01:05:29,944 iteration 1076 : loss : 0.051501, loss_ce: 0.019628
2022-01-09 01:05:31,409 iteration 1077 : loss : 0.058591, loss_ce: 0.020265
2022-01-09 01:05:32,959 iteration 1078 : loss : 0.094769, loss_ce: 0.049671
2022-01-09 01:05:34,368 iteration 1079 : loss : 0.052862, loss_ce: 0.019874
2022-01-09 01:05:35,759 iteration 1080 : loss : 0.052285, loss_ce: 0.022650
2022-01-09 01:05:37,251 iteration 1081 : loss : 0.076880, loss_ce: 0.026643
2022-01-09 01:05:38,745 iteration 1082 : loss : 0.071149, loss_ce: 0.037853
2022-01-09 01:05:40,272 iteration 1083 : loss : 0.100834, loss_ce: 0.048937
2022-01-09 01:05:41,702 iteration 1084 : loss : 0.079938, loss_ce: 0.039822
2022-01-09 01:05:43,267 iteration 1085 : loss : 0.069179, loss_ce: 0.026498
2022-01-09 01:05:44,700 iteration 1086 : loss : 0.074271, loss_ce: 0.030535
2022-01-09 01:05:46,159 iteration 1087 : loss : 0.065225, loss_ce: 0.028401
2022-01-09 01:05:47,701 iteration 1088 : loss : 0.069258, loss_ce: 0.030482
 16%|████▊                         | 64/400 [29:16<2:28:49, 26.58s/it]2022-01-09 01:05:49,222 iteration 1089 : loss : 0.097547, loss_ce: 0.036997
2022-01-09 01:05:50,652 iteration 1090 : loss : 0.059398, loss_ce: 0.030741
2022-01-09 01:05:52,110 iteration 1091 : loss : 0.057664, loss_ce: 0.025398
2022-01-09 01:05:53,578 iteration 1092 : loss : 0.049427, loss_ce: 0.020605
2022-01-09 01:05:55,060 iteration 1093 : loss : 0.074470, loss_ce: 0.027351
2022-01-09 01:05:56,619 iteration 1094 : loss : 0.061068, loss_ce: 0.023058
2022-01-09 01:05:58,035 iteration 1095 : loss : 0.050052, loss_ce: 0.020572
2022-01-09 01:05:59,646 iteration 1096 : loss : 0.069885, loss_ce: 0.028205
2022-01-09 01:06:01,142 iteration 1097 : loss : 0.068322, loss_ce: 0.027806
2022-01-09 01:06:02,561 iteration 1098 : loss : 0.049655, loss_ce: 0.021533
2022-01-09 01:06:04,056 iteration 1099 : loss : 0.067610, loss_ce: 0.021093
2022-01-09 01:06:05,630 iteration 1100 : loss : 0.057491, loss_ce: 0.022316
2022-01-09 01:06:07,086 iteration 1101 : loss : 0.055913, loss_ce: 0.023505
2022-01-09 01:06:08,663 iteration 1102 : loss : 0.113187, loss_ce: 0.036650
2022-01-09 01:06:10,124 iteration 1103 : loss : 0.087005, loss_ce: 0.028351
2022-01-09 01:06:11,577 iteration 1104 : loss : 0.069633, loss_ce: 0.029815
2022-01-09 01:06:11,578 Training Data Eval:
2022-01-09 01:06:18,915   Average segmentation loss on training set: 0.0640
2022-01-09 01:06:18,915 Validation Data Eval:
2022-01-09 01:06:21,407   Average segmentation loss on validation set: 0.1483
2022-01-09 01:06:22,939 iteration 1105 : loss : 0.074702, loss_ce: 0.023098
 16%|████▉                         | 65/400 [29:51<2:42:52, 29.17s/it]2022-01-09 01:06:24,374 iteration 1106 : loss : 0.084078, loss_ce: 0.030909
2022-01-09 01:06:25,803 iteration 1107 : loss : 0.070076, loss_ce: 0.034535
2022-01-09 01:06:27,268 iteration 1108 : loss : 0.125498, loss_ce: 0.039245
2022-01-09 01:06:28,641 iteration 1109 : loss : 0.053052, loss_ce: 0.024477
2022-01-09 01:06:30,103 iteration 1110 : loss : 0.076739, loss_ce: 0.033149
2022-01-09 01:06:31,524 iteration 1111 : loss : 0.055001, loss_ce: 0.023653
2022-01-09 01:06:32,965 iteration 1112 : loss : 0.080443, loss_ce: 0.034607
2022-01-09 01:06:34,381 iteration 1113 : loss : 0.087965, loss_ce: 0.033873
2022-01-09 01:06:35,909 iteration 1114 : loss : 0.064101, loss_ce: 0.030190
2022-01-09 01:06:37,540 iteration 1115 : loss : 0.067227, loss_ce: 0.033862
2022-01-09 01:06:39,070 iteration 1116 : loss : 0.099396, loss_ce: 0.030177
2022-01-09 01:06:40,678 iteration 1117 : loss : 0.081597, loss_ce: 0.026085
2022-01-09 01:06:42,210 iteration 1118 : loss : 0.084750, loss_ce: 0.029034
2022-01-09 01:06:43,639 iteration 1119 : loss : 0.061201, loss_ce: 0.026292
2022-01-09 01:06:45,047 iteration 1120 : loss : 0.077714, loss_ce: 0.026496
2022-01-09 01:06:46,532 iteration 1121 : loss : 0.108908, loss_ce: 0.030049
2022-01-09 01:06:47,996 iteration 1122 : loss : 0.106975, loss_ce: 0.049043
 16%|████▉                         | 66/400 [30:16<2:35:31, 27.94s/it]2022-01-09 01:06:49,474 iteration 1123 : loss : 0.072655, loss_ce: 0.024033
2022-01-09 01:06:50,978 iteration 1124 : loss : 0.069562, loss_ce: 0.030517
2022-01-09 01:06:52,432 iteration 1125 : loss : 0.053797, loss_ce: 0.016311
2022-01-09 01:06:53,868 iteration 1126 : loss : 0.075570, loss_ce: 0.029343
2022-01-09 01:06:55,290 iteration 1127 : loss : 0.076634, loss_ce: 0.027397
2022-01-09 01:06:56,839 iteration 1128 : loss : 0.098847, loss_ce: 0.047017
2022-01-09 01:06:58,387 iteration 1129 : loss : 0.090646, loss_ce: 0.034005
2022-01-09 01:06:59,826 iteration 1130 : loss : 0.066186, loss_ce: 0.030752
2022-01-09 01:07:01,262 iteration 1131 : loss : 0.057106, loss_ce: 0.025466
2022-01-09 01:07:02,781 iteration 1132 : loss : 0.062664, loss_ce: 0.027478
2022-01-09 01:07:04,158 iteration 1133 : loss : 0.054552, loss_ce: 0.024917
2022-01-09 01:07:05,613 iteration 1134 : loss : 0.085834, loss_ce: 0.028951
2022-01-09 01:07:07,023 iteration 1135 : loss : 0.095475, loss_ce: 0.039084
2022-01-09 01:07:08,475 iteration 1136 : loss : 0.042306, loss_ce: 0.015839
2022-01-09 01:07:09,889 iteration 1137 : loss : 0.104801, loss_ce: 0.048403
2022-01-09 01:07:11,396 iteration 1138 : loss : 0.068242, loss_ce: 0.029163
2022-01-09 01:07:12,874 iteration 1139 : loss : 0.065475, loss_ce: 0.023181
 17%|█████                         | 67/400 [30:41<2:29:57, 27.02s/it]2022-01-09 01:07:14,404 iteration 1140 : loss : 0.062695, loss_ce: 0.024160
2022-01-09 01:07:15,807 iteration 1141 : loss : 0.070194, loss_ce: 0.027376
2022-01-09 01:07:17,343 iteration 1142 : loss : 0.066381, loss_ce: 0.027166
2022-01-09 01:07:18,796 iteration 1143 : loss : 0.074482, loss_ce: 0.028495
2022-01-09 01:07:20,300 iteration 1144 : loss : 0.101237, loss_ce: 0.055950
2022-01-09 01:07:21,694 iteration 1145 : loss : 0.062666, loss_ce: 0.023915
2022-01-09 01:07:23,170 iteration 1146 : loss : 0.128724, loss_ce: 0.072481
2022-01-09 01:07:24,517 iteration 1147 : loss : 0.061192, loss_ce: 0.022234
2022-01-09 01:07:26,040 iteration 1148 : loss : 0.089616, loss_ce: 0.031472
2022-01-09 01:07:27,426 iteration 1149 : loss : 0.058245, loss_ce: 0.028759
2022-01-09 01:07:28,888 iteration 1150 : loss : 0.060207, loss_ce: 0.026416
2022-01-09 01:07:30,393 iteration 1151 : loss : 0.058000, loss_ce: 0.021676
2022-01-09 01:07:32,019 iteration 1152 : loss : 0.069921, loss_ce: 0.029089
2022-01-09 01:07:33,363 iteration 1153 : loss : 0.062875, loss_ce: 0.022789
2022-01-09 01:07:34,873 iteration 1154 : loss : 0.075610, loss_ce: 0.024058
2022-01-09 01:07:36,276 iteration 1155 : loss : 0.061404, loss_ce: 0.020871
2022-01-09 01:07:37,803 iteration 1156 : loss : 0.095792, loss_ce: 0.051177
 17%|█████                         | 68/400 [31:06<2:26:02, 26.39s/it]2022-01-09 01:07:39,308 iteration 1157 : loss : 0.119095, loss_ce: 0.028703
2022-01-09 01:07:40,791 iteration 1158 : loss : 0.063194, loss_ce: 0.022158
2022-01-09 01:07:42,360 iteration 1159 : loss : 0.059396, loss_ce: 0.017464
2022-01-09 01:07:43,978 iteration 1160 : loss : 0.094975, loss_ce: 0.040243
2022-01-09 01:07:45,520 iteration 1161 : loss : 0.055773, loss_ce: 0.024910
2022-01-09 01:07:47,027 iteration 1162 : loss : 0.058280, loss_ce: 0.020006
2022-01-09 01:07:48,462 iteration 1163 : loss : 0.074854, loss_ce: 0.036662
2022-01-09 01:07:49,916 iteration 1164 : loss : 0.085732, loss_ce: 0.033675
2022-01-09 01:07:51,368 iteration 1165 : loss : 0.074161, loss_ce: 0.032601
2022-01-09 01:07:52,931 iteration 1166 : loss : 0.048016, loss_ce: 0.017142
2022-01-09 01:07:54,346 iteration 1167 : loss : 0.059760, loss_ce: 0.027870
2022-01-09 01:07:55,780 iteration 1168 : loss : 0.058014, loss_ce: 0.022448
2022-01-09 01:07:57,199 iteration 1169 : loss : 0.062295, loss_ce: 0.025601
2022-01-09 01:07:58,631 iteration 1170 : loss : 0.076438, loss_ce: 0.031395
2022-01-09 01:08:00,145 iteration 1171 : loss : 0.065980, loss_ce: 0.024356
2022-01-09 01:08:01,646 iteration 1172 : loss : 0.062301, loss_ce: 0.025655
2022-01-09 01:08:03,173 iteration 1173 : loss : 0.068024, loss_ce: 0.024693
 17%|█████▏                        | 69/400 [31:32<2:23:53, 26.08s/it]2022-01-09 01:08:04,687 iteration 1174 : loss : 0.075359, loss_ce: 0.026069
2022-01-09 01:08:06,159 iteration 1175 : loss : 0.053505, loss_ce: 0.020068
2022-01-09 01:08:07,612 iteration 1176 : loss : 0.043576, loss_ce: 0.016158
2022-01-09 01:08:09,019 iteration 1177 : loss : 0.056918, loss_ce: 0.022808
2022-01-09 01:08:10,521 iteration 1178 : loss : 0.049708, loss_ce: 0.015926
2022-01-09 01:08:12,100 iteration 1179 : loss : 0.072886, loss_ce: 0.029349
2022-01-09 01:08:13,518 iteration 1180 : loss : 0.068501, loss_ce: 0.031132
2022-01-09 01:08:14,975 iteration 1181 : loss : 0.083498, loss_ce: 0.035346
2022-01-09 01:08:16,459 iteration 1182 : loss : 0.059538, loss_ce: 0.022033
2022-01-09 01:08:17,868 iteration 1183 : loss : 0.084977, loss_ce: 0.029703
2022-01-09 01:08:19,298 iteration 1184 : loss : 0.050572, loss_ce: 0.023378
2022-01-09 01:08:20,822 iteration 1185 : loss : 0.064983, loss_ce: 0.023497
2022-01-09 01:08:22,237 iteration 1186 : loss : 0.093230, loss_ce: 0.046866
2022-01-09 01:08:23,785 iteration 1187 : loss : 0.052832, loss_ce: 0.016872
2022-01-09 01:08:25,196 iteration 1188 : loss : 0.068889, loss_ce: 0.026105
2022-01-09 01:08:26,765 iteration 1189 : loss : 0.055649, loss_ce: 0.025822
2022-01-09 01:08:26,765 Training Data Eval:
2022-01-09 01:08:34,145   Average segmentation loss on training set: 0.0455
2022-01-09 01:08:34,146 Validation Data Eval:
2022-01-09 01:08:36,714   Average segmentation loss on validation set: 0.0867
2022-01-09 01:08:42,517 Found new lowest validation loss at iteration 1189! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 01:08:44,007 iteration 1190 : loss : 0.055190, loss_ce: 0.025634
 18%|█████▎                        | 70/400 [32:12<2:47:49, 30.51s/it]2022-01-09 01:08:45,626 iteration 1191 : loss : 0.079793, loss_ce: 0.037783
2022-01-09 01:08:47,027 iteration 1192 : loss : 0.050522, loss_ce: 0.022101
2022-01-09 01:08:48,586 iteration 1193 : loss : 0.085424, loss_ce: 0.028852
2022-01-09 01:08:50,018 iteration 1194 : loss : 0.061538, loss_ce: 0.020569
2022-01-09 01:08:51,402 iteration 1195 : loss : 0.047531, loss_ce: 0.020095
2022-01-09 01:08:52,786 iteration 1196 : loss : 0.084283, loss_ce: 0.040006
2022-01-09 01:08:54,130 iteration 1197 : loss : 0.070899, loss_ce: 0.025645
2022-01-09 01:08:55,643 iteration 1198 : loss : 0.062788, loss_ce: 0.029961
2022-01-09 01:08:57,105 iteration 1199 : loss : 0.059641, loss_ce: 0.021693
2022-01-09 01:08:58,527 iteration 1200 : loss : 0.123339, loss_ce: 0.042255
2022-01-09 01:08:59,906 iteration 1201 : loss : 0.053220, loss_ce: 0.023666
2022-01-09 01:09:01,385 iteration 1202 : loss : 0.054891, loss_ce: 0.020366
2022-01-09 01:09:02,861 iteration 1203 : loss : 0.073758, loss_ce: 0.024073
2022-01-09 01:09:04,275 iteration 1204 : loss : 0.073328, loss_ce: 0.029884
2022-01-09 01:09:05,739 iteration 1205 : loss : 0.106307, loss_ce: 0.049398
2022-01-09 01:09:07,210 iteration 1206 : loss : 0.046956, loss_ce: 0.019058
2022-01-09 01:09:08,732 iteration 1207 : loss : 0.043553, loss_ce: 0.015279
 18%|█████▎                        | 71/400 [32:37<2:37:47, 28.78s/it]2022-01-09 01:09:10,304 iteration 1208 : loss : 0.069812, loss_ce: 0.032803
2022-01-09 01:09:11,773 iteration 1209 : loss : 0.051253, loss_ce: 0.018966
2022-01-09 01:09:13,244 iteration 1210 : loss : 0.058472, loss_ce: 0.024911
2022-01-09 01:09:14,636 iteration 1211 : loss : 0.048985, loss_ce: 0.020064
2022-01-09 01:09:16,002 iteration 1212 : loss : 0.087233, loss_ce: 0.027647
2022-01-09 01:09:17,613 iteration 1213 : loss : 0.065037, loss_ce: 0.027743
2022-01-09 01:09:19,075 iteration 1214 : loss : 0.046338, loss_ce: 0.020677
2022-01-09 01:09:20,545 iteration 1215 : loss : 0.127820, loss_ce: 0.067807
2022-01-09 01:09:22,166 iteration 1216 : loss : 0.056223, loss_ce: 0.021040
2022-01-09 01:09:23,639 iteration 1217 : loss : 0.047231, loss_ce: 0.015204
2022-01-09 01:09:25,033 iteration 1218 : loss : 0.049169, loss_ce: 0.017120
2022-01-09 01:09:26,392 iteration 1219 : loss : 0.050019, loss_ce: 0.017989
2022-01-09 01:09:27,833 iteration 1220 : loss : 0.056114, loss_ce: 0.020104
2022-01-09 01:09:29,246 iteration 1221 : loss : 0.074492, loss_ce: 0.032775
2022-01-09 01:09:30,784 iteration 1222 : loss : 0.065190, loss_ce: 0.021457
2022-01-09 01:09:32,211 iteration 1223 : loss : 0.068162, loss_ce: 0.029728
2022-01-09 01:09:33,721 iteration 1224 : loss : 0.097186, loss_ce: 0.042819
 18%|█████▍                        | 72/400 [33:02<2:31:05, 27.64s/it]2022-01-09 01:09:35,215 iteration 1225 : loss : 0.064716, loss_ce: 0.024555
2022-01-09 01:09:36,722 iteration 1226 : loss : 0.078277, loss_ce: 0.034666
2022-01-09 01:09:38,114 iteration 1227 : loss : 0.058430, loss_ce: 0.020327
2022-01-09 01:09:39,545 iteration 1228 : loss : 0.055844, loss_ce: 0.020654
2022-01-09 01:09:40,908 iteration 1229 : loss : 0.067569, loss_ce: 0.019628
2022-01-09 01:09:42,303 iteration 1230 : loss : 0.056232, loss_ce: 0.021746
2022-01-09 01:09:43,803 iteration 1231 : loss : 0.053564, loss_ce: 0.025822
2022-01-09 01:09:45,197 iteration 1232 : loss : 0.062257, loss_ce: 0.026856
2022-01-09 01:09:46,581 iteration 1233 : loss : 0.048686, loss_ce: 0.020062
2022-01-09 01:09:48,089 iteration 1234 : loss : 0.054680, loss_ce: 0.023504
2022-01-09 01:09:49,614 iteration 1235 : loss : 0.102448, loss_ce: 0.030216
2022-01-09 01:09:51,067 iteration 1236 : loss : 0.051754, loss_ce: 0.018505
2022-01-09 01:09:52,496 iteration 1237 : loss : 0.076111, loss_ce: 0.029707
2022-01-09 01:09:54,096 iteration 1238 : loss : 0.070145, loss_ce: 0.025646
2022-01-09 01:09:55,546 iteration 1239 : loss : 0.047464, loss_ce: 0.017346
2022-01-09 01:09:57,028 iteration 1240 : loss : 0.047001, loss_ce: 0.017240
2022-01-09 01:09:58,501 iteration 1241 : loss : 0.063152, loss_ce: 0.029547
 18%|█████▍                        | 73/400 [33:27<2:25:56, 26.78s/it]2022-01-09 01:10:00,116 iteration 1242 : loss : 0.072578, loss_ce: 0.039617
2022-01-09 01:10:01,584 iteration 1243 : loss : 0.097868, loss_ce: 0.037217
2022-01-09 01:10:03,041 iteration 1244 : loss : 0.075687, loss_ce: 0.034639
2022-01-09 01:10:04,562 iteration 1245 : loss : 0.064623, loss_ce: 0.021784
2022-01-09 01:10:06,056 iteration 1246 : loss : 0.072720, loss_ce: 0.023729
2022-01-09 01:10:07,480 iteration 1247 : loss : 0.051002, loss_ce: 0.023174
2022-01-09 01:10:08,969 iteration 1248 : loss : 0.053886, loss_ce: 0.017506
2022-01-09 01:10:10,366 iteration 1249 : loss : 0.042419, loss_ce: 0.013946
2022-01-09 01:10:11,959 iteration 1250 : loss : 0.065359, loss_ce: 0.024610
2022-01-09 01:10:13,563 iteration 1251 : loss : 0.066483, loss_ce: 0.034311
2022-01-09 01:10:15,054 iteration 1252 : loss : 0.048160, loss_ce: 0.020504
2022-01-09 01:10:16,403 iteration 1253 : loss : 0.051086, loss_ce: 0.017365
2022-01-09 01:10:17,846 iteration 1254 : loss : 0.055328, loss_ce: 0.026744
2022-01-09 01:10:19,322 iteration 1255 : loss : 0.054457, loss_ce: 0.022939
2022-01-09 01:10:20,786 iteration 1256 : loss : 0.066256, loss_ce: 0.027529
2022-01-09 01:10:22,270 iteration 1257 : loss : 0.087518, loss_ce: 0.026232
2022-01-09 01:10:23,765 iteration 1258 : loss : 0.055506, loss_ce: 0.025255
 18%|█████▌                        | 74/400 [33:52<2:23:01, 26.32s/it]2022-01-09 01:10:25,248 iteration 1259 : loss : 0.048107, loss_ce: 0.021639
2022-01-09 01:10:26,783 iteration 1260 : loss : 0.051995, loss_ce: 0.022097
2022-01-09 01:10:28,210 iteration 1261 : loss : 0.049003, loss_ce: 0.016775
2022-01-09 01:10:29,776 iteration 1262 : loss : 0.080301, loss_ce: 0.035591
2022-01-09 01:10:31,373 iteration 1263 : loss : 0.087525, loss_ce: 0.033829
2022-01-09 01:10:32,800 iteration 1264 : loss : 0.051289, loss_ce: 0.024239
2022-01-09 01:10:34,332 iteration 1265 : loss : 0.056085, loss_ce: 0.028452
2022-01-09 01:10:35,868 iteration 1266 : loss : 0.047051, loss_ce: 0.018607
2022-01-09 01:10:37,364 iteration 1267 : loss : 0.066605, loss_ce: 0.022369
2022-01-09 01:10:38,783 iteration 1268 : loss : 0.070154, loss_ce: 0.027743
2022-01-09 01:10:40,338 iteration 1269 : loss : 0.107798, loss_ce: 0.030415
2022-01-09 01:10:41,853 iteration 1270 : loss : 0.052103, loss_ce: 0.021725
2022-01-09 01:10:43,308 iteration 1271 : loss : 0.061765, loss_ce: 0.026886
2022-01-09 01:10:44,660 iteration 1272 : loss : 0.054466, loss_ce: 0.019118
2022-01-09 01:10:46,087 iteration 1273 : loss : 0.069501, loss_ce: 0.022247
2022-01-09 01:10:47,540 iteration 1274 : loss : 0.053494, loss_ce: 0.016733
2022-01-09 01:10:47,540 Training Data Eval:
2022-01-09 01:10:54,949   Average segmentation loss on training set: 0.0522
2022-01-09 01:10:54,949 Validation Data Eval:
2022-01-09 01:10:57,516   Average segmentation loss on validation set: 0.1296
2022-01-09 01:10:59,001 iteration 1275 : loss : 0.054733, loss_ce: 0.020170
 19%|█████▋                        | 75/400 [34:27<2:37:04, 29.00s/it]2022-01-09 01:11:00,444 iteration 1276 : loss : 0.051658, loss_ce: 0.020247
2022-01-09 01:11:02,075 iteration 1277 : loss : 0.054851, loss_ce: 0.021693
2022-01-09 01:11:03,529 iteration 1278 : loss : 0.041104, loss_ce: 0.014352
2022-01-09 01:11:05,146 iteration 1279 : loss : 0.048058, loss_ce: 0.018143
2022-01-09 01:11:06,654 iteration 1280 : loss : 0.090953, loss_ce: 0.046232
2022-01-09 01:11:08,142 iteration 1281 : loss : 0.086579, loss_ce: 0.036003
2022-01-09 01:11:09,537 iteration 1282 : loss : 0.039423, loss_ce: 0.018600
2022-01-09 01:11:11,073 iteration 1283 : loss : 0.097185, loss_ce: 0.036780
2022-01-09 01:11:12,486 iteration 1284 : loss : 0.068528, loss_ce: 0.026391
2022-01-09 01:11:14,049 iteration 1285 : loss : 0.047656, loss_ce: 0.017126
2022-01-09 01:11:15,640 iteration 1286 : loss : 0.205199, loss_ce: 0.070449
2022-01-09 01:11:17,077 iteration 1287 : loss : 0.045066, loss_ce: 0.022816
2022-01-09 01:11:18,494 iteration 1288 : loss : 0.074025, loss_ce: 0.034527
2022-01-09 01:11:19,966 iteration 1289 : loss : 0.085875, loss_ce: 0.036126
2022-01-09 01:11:21,497 iteration 1290 : loss : 0.107508, loss_ce: 0.034534
2022-01-09 01:11:23,031 iteration 1291 : loss : 0.081639, loss_ce: 0.032336
2022-01-09 01:11:24,582 iteration 1292 : loss : 0.071382, loss_ce: 0.029692
 19%|█████▋                        | 76/400 [34:53<2:31:03, 27.97s/it]2022-01-09 01:11:26,165 iteration 1293 : loss : 0.053343, loss_ce: 0.018066
2022-01-09 01:11:27,684 iteration 1294 : loss : 0.082210, loss_ce: 0.033231
2022-01-09 01:11:29,263 iteration 1295 : loss : 0.122433, loss_ce: 0.044342
2022-01-09 01:11:30,762 iteration 1296 : loss : 0.098937, loss_ce: 0.032545
2022-01-09 01:11:32,175 iteration 1297 : loss : 0.064238, loss_ce: 0.033528
2022-01-09 01:11:33,556 iteration 1298 : loss : 0.051186, loss_ce: 0.020640
2022-01-09 01:11:35,026 iteration 1299 : loss : 0.060210, loss_ce: 0.029022
2022-01-09 01:11:36,481 iteration 1300 : loss : 0.082848, loss_ce: 0.026899
2022-01-09 01:11:37,979 iteration 1301 : loss : 0.042367, loss_ce: 0.016007
2022-01-09 01:11:39,408 iteration 1302 : loss : 0.074413, loss_ce: 0.030025
2022-01-09 01:11:40,843 iteration 1303 : loss : 0.103552, loss_ce: 0.046661
2022-01-09 01:11:42,323 iteration 1304 : loss : 0.078374, loss_ce: 0.032732
2022-01-09 01:11:43,782 iteration 1305 : loss : 0.078341, loss_ce: 0.032327
2022-01-09 01:11:45,311 iteration 1306 : loss : 0.096880, loss_ce: 0.033780
2022-01-09 01:11:46,828 iteration 1307 : loss : 0.074115, loss_ce: 0.038116
2022-01-09 01:11:48,302 iteration 1308 : loss : 0.093313, loss_ce: 0.028002
2022-01-09 01:11:49,686 iteration 1309 : loss : 0.062348, loss_ce: 0.025573
 19%|█████▊                        | 77/400 [35:18<2:25:58, 27.11s/it]2022-01-09 01:11:51,302 iteration 1310 : loss : 0.066754, loss_ce: 0.024166
2022-01-09 01:11:52,734 iteration 1311 : loss : 0.049553, loss_ce: 0.021537
2022-01-09 01:11:54,142 iteration 1312 : loss : 0.044584, loss_ce: 0.017269
2022-01-09 01:11:55,566 iteration 1313 : loss : 0.064125, loss_ce: 0.020488
2022-01-09 01:11:57,028 iteration 1314 : loss : 0.062690, loss_ce: 0.031611
2022-01-09 01:11:58,522 iteration 1315 : loss : 0.062793, loss_ce: 0.024921
2022-01-09 01:12:00,139 iteration 1316 : loss : 0.070669, loss_ce: 0.025464
2022-01-09 01:12:01,595 iteration 1317 : loss : 0.109940, loss_ce: 0.064423
2022-01-09 01:12:03,168 iteration 1318 : loss : 0.154941, loss_ce: 0.026743
2022-01-09 01:12:04,688 iteration 1319 : loss : 0.073198, loss_ce: 0.033410
2022-01-09 01:12:06,129 iteration 1320 : loss : 0.051915, loss_ce: 0.022091
2022-01-09 01:12:07,615 iteration 1321 : loss : 0.081255, loss_ce: 0.025017
2022-01-09 01:12:09,004 iteration 1322 : loss : 0.075888, loss_ce: 0.035597
2022-01-09 01:12:10,450 iteration 1323 : loss : 0.081684, loss_ce: 0.031235
2022-01-09 01:12:11,948 iteration 1324 : loss : 0.084346, loss_ce: 0.038165
2022-01-09 01:12:13,502 iteration 1325 : loss : 0.076812, loss_ce: 0.025671
2022-01-09 01:12:14,846 iteration 1326 : loss : 0.056860, loss_ce: 0.021408
 20%|█████▊                        | 78/400 [35:43<2:22:21, 26.53s/it]2022-01-09 01:12:16,325 iteration 1327 : loss : 0.075342, loss_ce: 0.024977
2022-01-09 01:12:17,823 iteration 1328 : loss : 0.082262, loss_ce: 0.037613
2022-01-09 01:12:19,287 iteration 1329 : loss : 0.049262, loss_ce: 0.020843
2022-01-09 01:12:20,713 iteration 1330 : loss : 0.108629, loss_ce: 0.031944
2022-01-09 01:12:22,092 iteration 1331 : loss : 0.056824, loss_ce: 0.027590
2022-01-09 01:12:23,519 iteration 1332 : loss : 0.055289, loss_ce: 0.018966
2022-01-09 01:12:24,987 iteration 1333 : loss : 0.058851, loss_ce: 0.015885
2022-01-09 01:12:26,489 iteration 1334 : loss : 0.057700, loss_ce: 0.023978
2022-01-09 01:12:27,942 iteration 1335 : loss : 0.089512, loss_ce: 0.025986
2022-01-09 01:12:29,389 iteration 1336 : loss : 0.050562, loss_ce: 0.019943
2022-01-09 01:12:30,929 iteration 1337 : loss : 0.070189, loss_ce: 0.028952
2022-01-09 01:12:32,430 iteration 1338 : loss : 0.050980, loss_ce: 0.014860
2022-01-09 01:12:33,916 iteration 1339 : loss : 0.059612, loss_ce: 0.025604
2022-01-09 01:12:35,405 iteration 1340 : loss : 0.060732, loss_ce: 0.024884
2022-01-09 01:12:36,883 iteration 1341 : loss : 0.075299, loss_ce: 0.028227
2022-01-09 01:12:38,334 iteration 1342 : loss : 0.095429, loss_ce: 0.039836
2022-01-09 01:12:39,712 iteration 1343 : loss : 0.053620, loss_ce: 0.021627
 20%|█████▉                        | 79/400 [36:08<2:19:15, 26.03s/it]2022-01-09 01:12:41,241 iteration 1344 : loss : 0.068425, loss_ce: 0.027223
2022-01-09 01:12:42,796 iteration 1345 : loss : 0.086083, loss_ce: 0.041349
2022-01-09 01:12:44,232 iteration 1346 : loss : 0.087799, loss_ce: 0.044029
2022-01-09 01:12:45,791 iteration 1347 : loss : 0.039928, loss_ce: 0.016831
2022-01-09 01:12:47,395 iteration 1348 : loss : 0.081356, loss_ce: 0.030638
2022-01-09 01:12:48,872 iteration 1349 : loss : 0.058203, loss_ce: 0.021294
2022-01-09 01:12:50,329 iteration 1350 : loss : 0.075743, loss_ce: 0.036206
2022-01-09 01:12:51,787 iteration 1351 : loss : 0.056125, loss_ce: 0.025937
2022-01-09 01:12:53,326 iteration 1352 : loss : 0.048929, loss_ce: 0.022453
2022-01-09 01:12:54,822 iteration 1353 : loss : 0.058805, loss_ce: 0.020853
2022-01-09 01:12:56,342 iteration 1354 : loss : 0.076238, loss_ce: 0.027394
2022-01-09 01:12:57,952 iteration 1355 : loss : 0.163029, loss_ce: 0.050217
2022-01-09 01:12:59,453 iteration 1356 : loss : 0.055581, loss_ce: 0.023300
2022-01-09 01:13:00,970 iteration 1357 : loss : 0.054444, loss_ce: 0.020416
2022-01-09 01:13:02,511 iteration 1358 : loss : 0.058118, loss_ce: 0.026421
2022-01-09 01:13:03,975 iteration 1359 : loss : 0.064469, loss_ce: 0.030980
2022-01-09 01:13:03,975 Training Data Eval:
2022-01-09 01:13:11,259   Average segmentation loss on training set: 0.0464
2022-01-09 01:13:11,260 Validation Data Eval:
2022-01-09 01:13:13,744   Average segmentation loss on validation set: 0.1281
2022-01-09 01:13:15,253 iteration 1360 : loss : 0.070553, loss_ce: 0.016114
 20%|██████                        | 80/400 [36:44<2:34:02, 28.88s/it]2022-01-09 01:13:16,907 iteration 1361 : loss : 0.078969, loss_ce: 0.036008
2022-01-09 01:13:18,383 iteration 1362 : loss : 0.069531, loss_ce: 0.025785
2022-01-09 01:13:20,089 iteration 1363 : loss : 0.077089, loss_ce: 0.032689
2022-01-09 01:13:21,433 iteration 1364 : loss : 0.069279, loss_ce: 0.016858
2022-01-09 01:13:22,979 iteration 1365 : loss : 0.073491, loss_ce: 0.029493
2022-01-09 01:13:24,505 iteration 1366 : loss : 0.066387, loss_ce: 0.025834
2022-01-09 01:13:25,995 iteration 1367 : loss : 0.068012, loss_ce: 0.026593
2022-01-09 01:13:27,511 iteration 1368 : loss : 0.073092, loss_ce: 0.036406
2022-01-09 01:13:28,961 iteration 1369 : loss : 0.060668, loss_ce: 0.021957
2022-01-09 01:13:30,524 iteration 1370 : loss : 0.065483, loss_ce: 0.031677
2022-01-09 01:13:32,006 iteration 1371 : loss : 0.047768, loss_ce: 0.017861
2022-01-09 01:13:33,465 iteration 1372 : loss : 0.076400, loss_ce: 0.027729
2022-01-09 01:13:34,907 iteration 1373 : loss : 0.068951, loss_ce: 0.026219
2022-01-09 01:13:36,429 iteration 1374 : loss : 0.053446, loss_ce: 0.017835
2022-01-09 01:13:37,941 iteration 1375 : loss : 0.058205, loss_ce: 0.024807
2022-01-09 01:13:39,423 iteration 1376 : loss : 0.069727, loss_ce: 0.021933
2022-01-09 01:13:40,871 iteration 1377 : loss : 0.034933, loss_ce: 0.012766
 20%|██████                        | 81/400 [37:09<2:28:21, 27.91s/it]2022-01-09 01:13:42,313 iteration 1378 : loss : 0.066225, loss_ce: 0.024222
2022-01-09 01:13:43,841 iteration 1379 : loss : 0.036053, loss_ce: 0.014274
2022-01-09 01:13:45,324 iteration 1380 : loss : 0.065173, loss_ce: 0.028265
2022-01-09 01:13:46,902 iteration 1381 : loss : 0.047857, loss_ce: 0.019960
2022-01-09 01:13:48,265 iteration 1382 : loss : 0.049305, loss_ce: 0.019188
2022-01-09 01:13:49,820 iteration 1383 : loss : 0.058763, loss_ce: 0.023813
2022-01-09 01:13:51,289 iteration 1384 : loss : 0.071564, loss_ce: 0.025886
2022-01-09 01:13:52,668 iteration 1385 : loss : 0.067471, loss_ce: 0.021544
2022-01-09 01:13:54,086 iteration 1386 : loss : 0.047109, loss_ce: 0.017793
2022-01-09 01:13:55,529 iteration 1387 : loss : 0.061074, loss_ce: 0.022829
2022-01-09 01:13:56,988 iteration 1388 : loss : 0.057343, loss_ce: 0.016049
2022-01-09 01:13:58,444 iteration 1389 : loss : 0.058023, loss_ce: 0.020305
2022-01-09 01:13:59,964 iteration 1390 : loss : 0.055066, loss_ce: 0.020217
2022-01-09 01:14:01,516 iteration 1391 : loss : 0.062373, loss_ce: 0.023442
2022-01-09 01:14:03,041 iteration 1392 : loss : 0.066726, loss_ce: 0.034401
2022-01-09 01:14:04,468 iteration 1393 : loss : 0.050079, loss_ce: 0.018006
2022-01-09 01:14:06,015 iteration 1394 : loss : 0.057245, loss_ce: 0.024325
 20%|██████▏                       | 82/400 [37:34<2:23:29, 27.07s/it]2022-01-09 01:14:07,534 iteration 1395 : loss : 0.066405, loss_ce: 0.025341
2022-01-09 01:14:08,967 iteration 1396 : loss : 0.037892, loss_ce: 0.017113
2022-01-09 01:14:10,506 iteration 1397 : loss : 0.060547, loss_ce: 0.024385
2022-01-09 01:14:12,058 iteration 1398 : loss : 0.064253, loss_ce: 0.030555
2022-01-09 01:14:13,422 iteration 1399 : loss : 0.054546, loss_ce: 0.025723
2022-01-09 01:14:14,776 iteration 1400 : loss : 0.059224, loss_ce: 0.018027
2022-01-09 01:14:16,312 iteration 1401 : loss : 0.055512, loss_ce: 0.019815
2022-01-09 01:14:17,793 iteration 1402 : loss : 0.069137, loss_ce: 0.026678
2022-01-09 01:14:19,187 iteration 1403 : loss : 0.071866, loss_ce: 0.021556
2022-01-09 01:14:20,576 iteration 1404 : loss : 0.035310, loss_ce: 0.016392
2022-01-09 01:14:22,037 iteration 1405 : loss : 0.048467, loss_ce: 0.016136
2022-01-09 01:14:23,508 iteration 1406 : loss : 0.067238, loss_ce: 0.020975
2022-01-09 01:14:25,024 iteration 1407 : loss : 0.046414, loss_ce: 0.018135
2022-01-09 01:14:26,481 iteration 1408 : loss : 0.061744, loss_ce: 0.019822
2022-01-09 01:14:27,974 iteration 1409 : loss : 0.079350, loss_ce: 0.026124
2022-01-09 01:14:29,542 iteration 1410 : loss : 0.043053, loss_ce: 0.016577
2022-01-09 01:14:30,975 iteration 1411 : loss : 0.079321, loss_ce: 0.031246
 21%|██████▏                       | 83/400 [37:59<2:19:41, 26.44s/it]2022-01-09 01:14:32,469 iteration 1412 : loss : 0.059725, loss_ce: 0.021703
2022-01-09 01:14:34,023 iteration 1413 : loss : 0.085075, loss_ce: 0.031492
2022-01-09 01:14:35,462 iteration 1414 : loss : 0.043604, loss_ce: 0.015553
2022-01-09 01:14:36,995 iteration 1415 : loss : 0.075645, loss_ce: 0.020766
2022-01-09 01:14:38,439 iteration 1416 : loss : 0.054540, loss_ce: 0.021757
2022-01-09 01:14:39,866 iteration 1417 : loss : 0.056482, loss_ce: 0.022171
2022-01-09 01:14:41,285 iteration 1418 : loss : 0.057754, loss_ce: 0.025669
2022-01-09 01:14:42,665 iteration 1419 : loss : 0.043337, loss_ce: 0.016600
2022-01-09 01:14:44,192 iteration 1420 : loss : 0.054466, loss_ce: 0.019828
2022-01-09 01:14:45,768 iteration 1421 : loss : 0.074758, loss_ce: 0.033566
2022-01-09 01:14:47,261 iteration 1422 : loss : 0.049032, loss_ce: 0.016433
2022-01-09 01:14:48,635 iteration 1423 : loss : 0.054326, loss_ce: 0.023017
2022-01-09 01:14:50,141 iteration 1424 : loss : 0.051398, loss_ce: 0.022642
2022-01-09 01:14:51,638 iteration 1425 : loss : 0.036214, loss_ce: 0.014655
2022-01-09 01:14:53,041 iteration 1426 : loss : 0.055391, loss_ce: 0.023638
2022-01-09 01:14:54,495 iteration 1427 : loss : 0.062689, loss_ce: 0.023184
2022-01-09 01:14:55,827 iteration 1428 : loss : 0.085066, loss_ce: 0.028359
 21%|██████▎                       | 84/400 [38:24<2:16:45, 25.97s/it]2022-01-09 01:14:57,452 iteration 1429 : loss : 0.059638, loss_ce: 0.024633
2022-01-09 01:14:58,862 iteration 1430 : loss : 0.150544, loss_ce: 0.039049
2022-01-09 01:15:00,318 iteration 1431 : loss : 0.056299, loss_ce: 0.023305
2022-01-09 01:15:01,799 iteration 1432 : loss : 0.053329, loss_ce: 0.023815
2022-01-09 01:15:03,334 iteration 1433 : loss : 0.052803, loss_ce: 0.018798
2022-01-09 01:15:04,772 iteration 1434 : loss : 0.053524, loss_ce: 0.025003
2022-01-09 01:15:06,275 iteration 1435 : loss : 0.064051, loss_ce: 0.024650
2022-01-09 01:15:07,783 iteration 1436 : loss : 0.082316, loss_ce: 0.033031
2022-01-09 01:15:09,301 iteration 1437 : loss : 0.062757, loss_ce: 0.024057
2022-01-09 01:15:10,785 iteration 1438 : loss : 0.043397, loss_ce: 0.019197
2022-01-09 01:15:12,324 iteration 1439 : loss : 0.063643, loss_ce: 0.027458
2022-01-09 01:15:13,844 iteration 1440 : loss : 0.050408, loss_ce: 0.023088
2022-01-09 01:15:15,286 iteration 1441 : loss : 0.047921, loss_ce: 0.021308
2022-01-09 01:15:16,768 iteration 1442 : loss : 0.062401, loss_ce: 0.029431
2022-01-09 01:15:18,273 iteration 1443 : loss : 0.074458, loss_ce: 0.031912
2022-01-09 01:15:19,747 iteration 1444 : loss : 0.178755, loss_ce: 0.064938
2022-01-09 01:15:19,747 Training Data Eval:
2022-01-09 01:15:27,012   Average segmentation loss on training set: 0.0400
2022-01-09 01:15:27,013 Validation Data Eval:
2022-01-09 01:15:29,484   Average segmentation loss on validation set: 0.1009
2022-01-09 01:15:30,942 iteration 1445 : loss : 0.049380, loss_ce: 0.014209
 21%|██████▍                       | 85/400 [38:59<2:30:42, 28.71s/it]2022-01-09 01:15:32,585 iteration 1446 : loss : 0.054368, loss_ce: 0.016904
2022-01-09 01:15:34,033 iteration 1447 : loss : 0.060046, loss_ce: 0.021704
2022-01-09 01:15:35,418 iteration 1448 : loss : 0.060118, loss_ce: 0.032347
2022-01-09 01:15:36,918 iteration 1449 : loss : 0.070512, loss_ce: 0.024587
2022-01-09 01:15:38,520 iteration 1450 : loss : 0.064088, loss_ce: 0.030532
2022-01-09 01:15:40,135 iteration 1451 : loss : 0.060529, loss_ce: 0.027651
2022-01-09 01:15:41,561 iteration 1452 : loss : 0.041283, loss_ce: 0.013377
2022-01-09 01:15:43,005 iteration 1453 : loss : 0.075580, loss_ce: 0.035695
2022-01-09 01:15:44,507 iteration 1454 : loss : 0.065142, loss_ce: 0.022993
2022-01-09 01:15:45,930 iteration 1455 : loss : 0.048040, loss_ce: 0.020403
2022-01-09 01:15:47,501 iteration 1456 : loss : 0.081370, loss_ce: 0.025163
2022-01-09 01:15:48,971 iteration 1457 : loss : 0.057666, loss_ce: 0.025923
2022-01-09 01:15:50,420 iteration 1458 : loss : 0.055145, loss_ce: 0.018751
2022-01-09 01:15:51,831 iteration 1459 : loss : 0.062636, loss_ce: 0.017932
2022-01-09 01:15:53,384 iteration 1460 : loss : 0.112521, loss_ce: 0.045942
2022-01-09 01:15:54,859 iteration 1461 : loss : 0.063590, loss_ce: 0.027302
2022-01-09 01:15:56,395 iteration 1462 : loss : 0.041892, loss_ce: 0.018548
 22%|██████▍                       | 86/400 [39:25<2:25:08, 27.73s/it]2022-01-09 01:15:57,915 iteration 1463 : loss : 0.059042, loss_ce: 0.026170
2022-01-09 01:15:59,352 iteration 1464 : loss : 0.050676, loss_ce: 0.020371
2022-01-09 01:16:00,862 iteration 1465 : loss : 0.067309, loss_ce: 0.018491
2022-01-09 01:16:02,370 iteration 1466 : loss : 0.049520, loss_ce: 0.020478
2022-01-09 01:16:03,848 iteration 1467 : loss : 0.062974, loss_ce: 0.028631
2022-01-09 01:16:05,290 iteration 1468 : loss : 0.045198, loss_ce: 0.019738
2022-01-09 01:16:06,757 iteration 1469 : loss : 0.054095, loss_ce: 0.020990
2022-01-09 01:16:08,276 iteration 1470 : loss : 0.069245, loss_ce: 0.025399
2022-01-09 01:16:09,781 iteration 1471 : loss : 0.072712, loss_ce: 0.038950
2022-01-09 01:16:11,124 iteration 1472 : loss : 0.042825, loss_ce: 0.015352
2022-01-09 01:16:12,719 iteration 1473 : loss : 0.051595, loss_ce: 0.018648
2022-01-09 01:16:14,249 iteration 1474 : loss : 0.055171, loss_ce: 0.019874
2022-01-09 01:16:15,707 iteration 1475 : loss : 0.050280, loss_ce: 0.024572
2022-01-09 01:16:17,286 iteration 1476 : loss : 0.087348, loss_ce: 0.032991
2022-01-09 01:16:18,815 iteration 1477 : loss : 0.050682, loss_ce: 0.015933
2022-01-09 01:16:20,390 iteration 1478 : loss : 0.060162, loss_ce: 0.023472
2022-01-09 01:16:21,809 iteration 1479 : loss : 0.059962, loss_ce: 0.019781
 22%|██████▌                       | 87/400 [39:50<2:21:03, 27.04s/it]2022-01-09 01:16:23,355 iteration 1480 : loss : 0.054930, loss_ce: 0.019297
2022-01-09 01:16:24,793 iteration 1481 : loss : 0.049722, loss_ce: 0.020589
2022-01-09 01:16:26,244 iteration 1482 : loss : 0.047813, loss_ce: 0.019402
2022-01-09 01:16:27,820 iteration 1483 : loss : 0.060701, loss_ce: 0.023912
2022-01-09 01:16:29,479 iteration 1484 : loss : 0.081588, loss_ce: 0.028440
2022-01-09 01:16:31,031 iteration 1485 : loss : 0.053354, loss_ce: 0.022976
2022-01-09 01:16:32,482 iteration 1486 : loss : 0.060931, loss_ce: 0.023494
2022-01-09 01:16:33,951 iteration 1487 : loss : 0.046727, loss_ce: 0.018827
2022-01-09 01:16:35,441 iteration 1488 : loss : 0.056127, loss_ce: 0.023307
2022-01-09 01:16:36,937 iteration 1489 : loss : 0.033827, loss_ce: 0.012965
2022-01-09 01:16:38,461 iteration 1490 : loss : 0.092910, loss_ce: 0.034327
2022-01-09 01:16:40,028 iteration 1491 : loss : 0.059318, loss_ce: 0.025264
2022-01-09 01:16:41,450 iteration 1492 : loss : 0.047655, loss_ce: 0.019464
2022-01-09 01:16:42,983 iteration 1493 : loss : 0.061435, loss_ce: 0.020817
2022-01-09 01:16:44,416 iteration 1494 : loss : 0.045660, loss_ce: 0.019656
2022-01-09 01:16:45,860 iteration 1495 : loss : 0.055148, loss_ce: 0.021484
2022-01-09 01:16:47,335 iteration 1496 : loss : 0.071700, loss_ce: 0.022236
 22%|██████▌                       | 88/400 [40:16<2:18:13, 26.58s/it]2022-01-09 01:16:48,814 iteration 1497 : loss : 0.069629, loss_ce: 0.038588
2022-01-09 01:16:50,244 iteration 1498 : loss : 0.058929, loss_ce: 0.019320
2022-01-09 01:16:51,763 iteration 1499 : loss : 0.064704, loss_ce: 0.028673
2022-01-09 01:16:53,141 iteration 1500 : loss : 0.068511, loss_ce: 0.022424
2022-01-09 01:16:54,584 iteration 1501 : loss : 0.041744, loss_ce: 0.015858
2022-01-09 01:16:56,099 iteration 1502 : loss : 0.058720, loss_ce: 0.020737
2022-01-09 01:16:57,543 iteration 1503 : loss : 0.032066, loss_ce: 0.013269
2022-01-09 01:16:59,012 iteration 1504 : loss : 0.069711, loss_ce: 0.023332
2022-01-09 01:17:00,456 iteration 1505 : loss : 0.050542, loss_ce: 0.017663
2022-01-09 01:17:01,978 iteration 1506 : loss : 0.048296, loss_ce: 0.018124
2022-01-09 01:17:03,503 iteration 1507 : loss : 0.066478, loss_ce: 0.023157
2022-01-09 01:17:04,982 iteration 1508 : loss : 0.054818, loss_ce: 0.026167
2022-01-09 01:17:06,479 iteration 1509 : loss : 0.069706, loss_ce: 0.025946
2022-01-09 01:17:07,913 iteration 1510 : loss : 0.031839, loss_ce: 0.012812
2022-01-09 01:17:09,417 iteration 1511 : loss : 0.051322, loss_ce: 0.023291
2022-01-09 01:17:10,837 iteration 1512 : loss : 0.053556, loss_ce: 0.023276
2022-01-09 01:17:12,337 iteration 1513 : loss : 0.048215, loss_ce: 0.020097
 22%|██████▋                       | 89/400 [40:41<2:15:19, 26.11s/it]2022-01-09 01:17:13,834 iteration 1514 : loss : 0.079758, loss_ce: 0.030045
2022-01-09 01:17:15,294 iteration 1515 : loss : 0.057146, loss_ce: 0.021781
2022-01-09 01:17:16,793 iteration 1516 : loss : 0.053124, loss_ce: 0.020741
2022-01-09 01:17:18,256 iteration 1517 : loss : 0.038502, loss_ce: 0.017336
2022-01-09 01:17:19,780 iteration 1518 : loss : 0.038459, loss_ce: 0.011176
2022-01-09 01:17:21,239 iteration 1519 : loss : 0.057184, loss_ce: 0.023091
2022-01-09 01:17:22,698 iteration 1520 : loss : 0.034785, loss_ce: 0.012633
2022-01-09 01:17:24,146 iteration 1521 : loss : 0.052452, loss_ce: 0.016485
2022-01-09 01:17:25,763 iteration 1522 : loss : 0.044329, loss_ce: 0.013650
2022-01-09 01:17:27,177 iteration 1523 : loss : 0.063679, loss_ce: 0.031377
2022-01-09 01:17:28,685 iteration 1524 : loss : 0.059538, loss_ce: 0.022184
2022-01-09 01:17:30,131 iteration 1525 : loss : 0.049044, loss_ce: 0.021956
2022-01-09 01:17:31,696 iteration 1526 : loss : 0.042855, loss_ce: 0.018132
2022-01-09 01:17:33,255 iteration 1527 : loss : 0.074104, loss_ce: 0.042101
2022-01-09 01:17:34,781 iteration 1528 : loss : 0.051449, loss_ce: 0.020580
2022-01-09 01:17:36,213 iteration 1529 : loss : 0.048250, loss_ce: 0.018836
2022-01-09 01:17:36,213 Training Data Eval:
2022-01-09 01:17:43,627   Average segmentation loss on training set: 0.0429
2022-01-09 01:17:43,627 Validation Data Eval:
2022-01-09 01:17:46,202   Average segmentation loss on validation set: 0.0836
2022-01-09 01:17:52,065 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 01:17:53,549 iteration 1530 : loss : 0.064655, loss_ce: 0.036309
 22%|██████▊                       | 90/400 [41:22<2:38:19, 30.64s/it]2022-01-09 01:17:55,248 iteration 1531 : loss : 0.056524, loss_ce: 0.021332
2022-01-09 01:17:56,774 iteration 1532 : loss : 0.052072, loss_ce: 0.017986
2022-01-09 01:17:58,192 iteration 1533 : loss : 0.048601, loss_ce: 0.020000
2022-01-09 01:17:59,726 iteration 1534 : loss : 0.064490, loss_ce: 0.025666
2022-01-09 01:18:01,168 iteration 1535 : loss : 0.040513, loss_ce: 0.016604
2022-01-09 01:18:02,585 iteration 1536 : loss : 0.078111, loss_ce: 0.045283
2022-01-09 01:18:03,995 iteration 1537 : loss : 0.062713, loss_ce: 0.019933
2022-01-09 01:18:05,382 iteration 1538 : loss : 0.156646, loss_ce: 0.036892
2022-01-09 01:18:06,826 iteration 1539 : loss : 0.046589, loss_ce: 0.017366
2022-01-09 01:18:08,287 iteration 1540 : loss : 0.065761, loss_ce: 0.030274
2022-01-09 01:18:09,634 iteration 1541 : loss : 0.053500, loss_ce: 0.018542
2022-01-09 01:18:11,101 iteration 1542 : loss : 0.053426, loss_ce: 0.024078
2022-01-09 01:18:12,535 iteration 1543 : loss : 0.056765, loss_ce: 0.019831
2022-01-09 01:18:13,907 iteration 1544 : loss : 0.063279, loss_ce: 0.023436
2022-01-09 01:18:15,348 iteration 1545 : loss : 0.053990, loss_ce: 0.022049
2022-01-09 01:18:16,780 iteration 1546 : loss : 0.056972, loss_ce: 0.022854
2022-01-09 01:18:18,208 iteration 1547 : loss : 0.056739, loss_ce: 0.022151
 23%|██████▊                       | 91/400 [41:47<2:28:33, 28.85s/it]2022-01-09 01:18:19,729 iteration 1548 : loss : 0.050376, loss_ce: 0.017274
2022-01-09 01:18:21,110 iteration 1549 : loss : 0.045645, loss_ce: 0.020406
2022-01-09 01:18:22,690 iteration 1550 : loss : 0.064864, loss_ce: 0.029206
2022-01-09 01:18:24,143 iteration 1551 : loss : 0.063672, loss_ce: 0.026223
2022-01-09 01:18:25,613 iteration 1552 : loss : 0.056077, loss_ce: 0.026259
2022-01-09 01:18:26,964 iteration 1553 : loss : 0.037944, loss_ce: 0.020160
2022-01-09 01:18:28,367 iteration 1554 : loss : 0.074032, loss_ce: 0.029574
2022-01-09 01:18:29,894 iteration 1555 : loss : 0.045180, loss_ce: 0.017066
2022-01-09 01:18:31,521 iteration 1556 : loss : 0.059308, loss_ce: 0.022312
2022-01-09 01:18:33,079 iteration 1557 : loss : 0.095125, loss_ce: 0.029779
2022-01-09 01:18:34,548 iteration 1558 : loss : 0.067045, loss_ce: 0.020010
2022-01-09 01:18:36,147 iteration 1559 : loss : 0.051498, loss_ce: 0.019786
2022-01-09 01:18:37,587 iteration 1560 : loss : 0.060925, loss_ce: 0.022270
2022-01-09 01:18:39,029 iteration 1561 : loss : 0.075136, loss_ce: 0.022856
2022-01-09 01:18:40,505 iteration 1562 : loss : 0.061949, loss_ce: 0.026163
2022-01-09 01:18:41,943 iteration 1563 : loss : 0.082350, loss_ce: 0.041640
2022-01-09 01:18:43,417 iteration 1564 : loss : 0.066285, loss_ce: 0.020210
 23%|██████▉                       | 92/400 [42:12<2:22:28, 27.76s/it]2022-01-09 01:18:44,975 iteration 1565 : loss : 0.037817, loss_ce: 0.014071
2022-01-09 01:18:46,414 iteration 1566 : loss : 0.053558, loss_ce: 0.023653
2022-01-09 01:18:47,923 iteration 1567 : loss : 0.052760, loss_ce: 0.015381
2022-01-09 01:18:49,526 iteration 1568 : loss : 0.040570, loss_ce: 0.018971
2022-01-09 01:18:50,962 iteration 1569 : loss : 0.045673, loss_ce: 0.018339
2022-01-09 01:18:52,447 iteration 1570 : loss : 0.046913, loss_ce: 0.018017
2022-01-09 01:18:53,894 iteration 1571 : loss : 0.055150, loss_ce: 0.024449
2022-01-09 01:18:55,354 iteration 1572 : loss : 0.075309, loss_ce: 0.036421
2022-01-09 01:18:56,828 iteration 1573 : loss : 0.076358, loss_ce: 0.027713
2022-01-09 01:18:58,307 iteration 1574 : loss : 0.049886, loss_ce: 0.023745
2022-01-09 01:18:59,803 iteration 1575 : loss : 0.067857, loss_ce: 0.029086
2022-01-09 01:19:01,248 iteration 1576 : loss : 0.063300, loss_ce: 0.023569
2022-01-09 01:19:02,647 iteration 1577 : loss : 0.099811, loss_ce: 0.032000
2022-01-09 01:19:04,002 iteration 1578 : loss : 0.044265, loss_ce: 0.017214
2022-01-09 01:19:05,444 iteration 1579 : loss : 0.048547, loss_ce: 0.017132
2022-01-09 01:19:06,928 iteration 1580 : loss : 0.049733, loss_ce: 0.014824
2022-01-09 01:19:08,416 iteration 1581 : loss : 0.036813, loss_ce: 0.013312
 23%|██████▉                       | 93/400 [42:37<2:17:47, 26.93s/it]2022-01-09 01:19:09,997 iteration 1582 : loss : 0.045979, loss_ce: 0.023974
2022-01-09 01:19:11,468 iteration 1583 : loss : 0.043759, loss_ce: 0.013169
2022-01-09 01:19:12,875 iteration 1584 : loss : 0.047001, loss_ce: 0.014829
2022-01-09 01:19:14,341 iteration 1585 : loss : 0.056997, loss_ce: 0.025667
2022-01-09 01:19:15,753 iteration 1586 : loss : 0.039063, loss_ce: 0.017162
2022-01-09 01:19:17,238 iteration 1587 : loss : 0.049189, loss_ce: 0.017332
2022-01-09 01:19:18,722 iteration 1588 : loss : 0.035842, loss_ce: 0.010703
2022-01-09 01:19:20,236 iteration 1589 : loss : 0.042737, loss_ce: 0.017792
2022-01-09 01:19:21,838 iteration 1590 : loss : 0.056914, loss_ce: 0.016864
2022-01-09 01:19:23,296 iteration 1591 : loss : 0.051972, loss_ce: 0.028308
2022-01-09 01:19:24,669 iteration 1592 : loss : 0.035022, loss_ce: 0.015506
2022-01-09 01:19:26,145 iteration 1593 : loss : 0.050830, loss_ce: 0.017894
2022-01-09 01:19:27,587 iteration 1594 : loss : 0.046373, loss_ce: 0.015581
2022-01-09 01:19:28,980 iteration 1595 : loss : 0.048639, loss_ce: 0.025363
2022-01-09 01:19:30,323 iteration 1596 : loss : 0.098581, loss_ce: 0.034896
2022-01-09 01:19:31,795 iteration 1597 : loss : 0.091901, loss_ce: 0.023933
2022-01-09 01:19:33,295 iteration 1598 : loss : 0.055648, loss_ce: 0.019085
 24%|███████                       | 94/400 [43:02<2:14:11, 26.31s/it]2022-01-09 01:19:34,875 iteration 1599 : loss : 0.048872, loss_ce: 0.024063
2022-01-09 01:19:36,310 iteration 1600 : loss : 0.051819, loss_ce: 0.023832
2022-01-09 01:19:37,815 iteration 1601 : loss : 0.060136, loss_ce: 0.024786
2022-01-09 01:19:39,289 iteration 1602 : loss : 0.094345, loss_ce: 0.034625
2022-01-09 01:19:40,728 iteration 1603 : loss : 0.059851, loss_ce: 0.026154
2022-01-09 01:19:42,148 iteration 1604 : loss : 0.086040, loss_ce: 0.028617
2022-01-09 01:19:43,582 iteration 1605 : loss : 0.067861, loss_ce: 0.024769
2022-01-09 01:19:45,053 iteration 1606 : loss : 0.060569, loss_ce: 0.023970
2022-01-09 01:19:46,504 iteration 1607 : loss : 0.068294, loss_ce: 0.027731
2022-01-09 01:19:48,067 iteration 1608 : loss : 0.055976, loss_ce: 0.023453
2022-01-09 01:19:49,506 iteration 1609 : loss : 0.057731, loss_ce: 0.021840
2022-01-09 01:19:51,081 iteration 1610 : loss : 0.052230, loss_ce: 0.020523
2022-01-09 01:19:52,660 iteration 1611 : loss : 0.065081, loss_ce: 0.027407
2022-01-09 01:19:54,248 iteration 1612 : loss : 0.047973, loss_ce: 0.016729
2022-01-09 01:19:55,723 iteration 1613 : loss : 0.051204, loss_ce: 0.018245
2022-01-09 01:19:57,266 iteration 1614 : loss : 0.076080, loss_ce: 0.041131
2022-01-09 01:19:57,266 Training Data Eval:
2022-01-09 01:20:04,673   Average segmentation loss on training set: 0.0395
2022-01-09 01:20:04,674 Validation Data Eval:
2022-01-09 01:20:07,369   Average segmentation loss on validation set: 0.1189
2022-01-09 01:20:08,959 iteration 1615 : loss : 0.076243, loss_ce: 0.028130
 24%|███████▏                      | 95/400 [43:37<2:28:01, 29.12s/it]2022-01-09 01:20:10,512 iteration 1616 : loss : 0.057140, loss_ce: 0.022642
2022-01-09 01:20:11,966 iteration 1617 : loss : 0.055984, loss_ce: 0.025487
2022-01-09 01:20:13,360 iteration 1618 : loss : 0.049929, loss_ce: 0.017766
2022-01-09 01:20:14,814 iteration 1619 : loss : 0.062972, loss_ce: 0.038770
2022-01-09 01:20:16,429 iteration 1620 : loss : 0.052057, loss_ce: 0.017819
2022-01-09 01:20:18,042 iteration 1621 : loss : 0.050910, loss_ce: 0.017396
2022-01-09 01:20:19,505 iteration 1622 : loss : 0.056377, loss_ce: 0.020783
2022-01-09 01:20:20,909 iteration 1623 : loss : 0.041546, loss_ce: 0.014213
2022-01-09 01:20:22,382 iteration 1624 : loss : 0.044682, loss_ce: 0.016029
2022-01-09 01:20:23,756 iteration 1625 : loss : 0.042436, loss_ce: 0.015118
2022-01-09 01:20:25,182 iteration 1626 : loss : 0.046225, loss_ce: 0.018424
2022-01-09 01:20:26,799 iteration 1627 : loss : 0.081564, loss_ce: 0.035964
2022-01-09 01:20:28,390 iteration 1628 : loss : 0.052966, loss_ce: 0.020511
2022-01-09 01:20:29,826 iteration 1629 : loss : 0.038389, loss_ce: 0.014323
2022-01-09 01:20:31,226 iteration 1630 : loss : 0.033103, loss_ce: 0.012583
2022-01-09 01:20:32,629 iteration 1631 : loss : 0.075211, loss_ce: 0.026803
2022-01-09 01:20:34,061 iteration 1632 : loss : 0.042391, loss_ce: 0.015901
 24%|███████▏                      | 96/400 [44:02<2:21:25, 27.91s/it]2022-01-09 01:20:35,595 iteration 1633 : loss : 0.053011, loss_ce: 0.017176
2022-01-09 01:20:37,083 iteration 1634 : loss : 0.073486, loss_ce: 0.037441
2022-01-09 01:20:38,510 iteration 1635 : loss : 0.064319, loss_ce: 0.021350
2022-01-09 01:20:39,876 iteration 1636 : loss : 0.047981, loss_ce: 0.024365
2022-01-09 01:20:41,368 iteration 1637 : loss : 0.062629, loss_ce: 0.018494
2022-01-09 01:20:42,827 iteration 1638 : loss : 0.048846, loss_ce: 0.022941
2022-01-09 01:20:44,365 iteration 1639 : loss : 0.059547, loss_ce: 0.026996
2022-01-09 01:20:45,782 iteration 1640 : loss : 0.051487, loss_ce: 0.018059
2022-01-09 01:20:47,346 iteration 1641 : loss : 0.065831, loss_ce: 0.027734
2022-01-09 01:20:48,747 iteration 1642 : loss : 0.047708, loss_ce: 0.016340
2022-01-09 01:20:50,122 iteration 1643 : loss : 0.053395, loss_ce: 0.017895
2022-01-09 01:20:51,652 iteration 1644 : loss : 0.057295, loss_ce: 0.023558
2022-01-09 01:20:53,228 iteration 1645 : loss : 0.075784, loss_ce: 0.035016
2022-01-09 01:20:54,646 iteration 1646 : loss : 0.061190, loss_ce: 0.023976
2022-01-09 01:20:56,079 iteration 1647 : loss : 0.081630, loss_ce: 0.032033
2022-01-09 01:20:57,429 iteration 1648 : loss : 0.049971, loss_ce: 0.021667
2022-01-09 01:20:58,800 iteration 1649 : loss : 0.045805, loss_ce: 0.017076
 24%|███████▎                      | 97/400 [44:27<2:16:09, 26.96s/it]2022-01-09 01:21:00,477 iteration 1650 : loss : 0.090659, loss_ce: 0.055912
2022-01-09 01:21:01,912 iteration 1651 : loss : 0.063040, loss_ce: 0.021131
2022-01-09 01:21:03,396 iteration 1652 : loss : 0.072967, loss_ce: 0.021082
2022-01-09 01:21:04,811 iteration 1653 : loss : 0.053728, loss_ce: 0.017745
2022-01-09 01:21:06,252 iteration 1654 : loss : 0.050294, loss_ce: 0.016421
2022-01-09 01:21:07,647 iteration 1655 : loss : 0.053442, loss_ce: 0.020841
2022-01-09 01:21:09,157 iteration 1656 : loss : 0.061533, loss_ce: 0.024891
2022-01-09 01:21:10,563 iteration 1657 : loss : 0.042716, loss_ce: 0.018825
2022-01-09 01:21:12,094 iteration 1658 : loss : 0.084599, loss_ce: 0.051588
2022-01-09 01:21:13,662 iteration 1659 : loss : 0.051753, loss_ce: 0.021366
2022-01-09 01:21:15,130 iteration 1660 : loss : 0.032698, loss_ce: 0.012289
2022-01-09 01:21:16,515 iteration 1661 : loss : 0.056146, loss_ce: 0.019530
2022-01-09 01:21:18,028 iteration 1662 : loss : 0.055906, loss_ce: 0.020616
2022-01-09 01:21:19,576 iteration 1663 : loss : 0.043194, loss_ce: 0.018712
2022-01-09 01:21:21,046 iteration 1664 : loss : 0.073876, loss_ce: 0.025498
2022-01-09 01:21:22,493 iteration 1665 : loss : 0.051031, loss_ce: 0.022964
2022-01-09 01:21:23,985 iteration 1666 : loss : 0.057924, loss_ce: 0.019198
 24%|███████▎                      | 98/400 [44:52<2:13:00, 26.42s/it]2022-01-09 01:21:25,454 iteration 1667 : loss : 0.045900, loss_ce: 0.021489
2022-01-09 01:21:27,023 iteration 1668 : loss : 0.079747, loss_ce: 0.023606
2022-01-09 01:21:28,532 iteration 1669 : loss : 0.050927, loss_ce: 0.018944
2022-01-09 01:21:29,897 iteration 1670 : loss : 0.037176, loss_ce: 0.012463
2022-01-09 01:21:31,290 iteration 1671 : loss : 0.046130, loss_ce: 0.020375
2022-01-09 01:21:32,841 iteration 1672 : loss : 0.071454, loss_ce: 0.026047
2022-01-09 01:21:34,359 iteration 1673 : loss : 0.046796, loss_ce: 0.013564
2022-01-09 01:21:35,952 iteration 1674 : loss : 0.059537, loss_ce: 0.022507
2022-01-09 01:21:37,385 iteration 1675 : loss : 0.073228, loss_ce: 0.028346
2022-01-09 01:21:38,928 iteration 1676 : loss : 0.054714, loss_ce: 0.023742
2022-01-09 01:21:40,425 iteration 1677 : loss : 0.070855, loss_ce: 0.021580
2022-01-09 01:21:41,944 iteration 1678 : loss : 0.050715, loss_ce: 0.024892
2022-01-09 01:21:43,384 iteration 1679 : loss : 0.089562, loss_ce: 0.050258
2022-01-09 01:21:44,885 iteration 1680 : loss : 0.068720, loss_ce: 0.032847
2022-01-09 01:21:46,336 iteration 1681 : loss : 0.056492, loss_ce: 0.025079
2022-01-09 01:21:47,806 iteration 1682 : loss : 0.041919, loss_ce: 0.020627
2022-01-09 01:21:49,232 iteration 1683 : loss : 0.067488, loss_ce: 0.028337
 25%|███████▍                      | 99/400 [45:18<2:10:48, 26.08s/it]2022-01-09 01:21:50,833 iteration 1684 : loss : 0.061998, loss_ce: 0.022232
2022-01-09 01:21:52,293 iteration 1685 : loss : 0.050203, loss_ce: 0.024220
2022-01-09 01:21:53,708 iteration 1686 : loss : 0.041570, loss_ce: 0.015158
2022-01-09 01:21:55,185 iteration 1687 : loss : 0.037008, loss_ce: 0.015634
2022-01-09 01:21:56,702 iteration 1688 : loss : 0.051662, loss_ce: 0.024623
2022-01-09 01:21:58,155 iteration 1689 : loss : 0.042460, loss_ce: 0.016088
2022-01-09 01:21:59,688 iteration 1690 : loss : 0.065018, loss_ce: 0.024809
2022-01-09 01:22:01,171 iteration 1691 : loss : 0.035450, loss_ce: 0.015162
2022-01-09 01:22:02,718 iteration 1692 : loss : 0.083924, loss_ce: 0.027398
2022-01-09 01:22:04,282 iteration 1693 : loss : 0.050003, loss_ce: 0.022666
2022-01-09 01:22:05,795 iteration 1694 : loss : 0.051239, loss_ce: 0.016232
2022-01-09 01:22:07,165 iteration 1695 : loss : 0.038640, loss_ce: 0.013300
2022-01-09 01:22:08,621 iteration 1696 : loss : 0.055330, loss_ce: 0.022786
2022-01-09 01:22:10,147 iteration 1697 : loss : 0.056185, loss_ce: 0.014883
2022-01-09 01:22:11,588 iteration 1698 : loss : 0.036983, loss_ce: 0.011329
2022-01-09 01:22:12,912 iteration 1699 : loss : 0.057282, loss_ce: 0.026870
2022-01-09 01:22:12,913 Training Data Eval:
2022-01-09 01:22:20,362   Average segmentation loss on training set: 0.0486
2022-01-09 01:22:20,362 Validation Data Eval:
2022-01-09 01:22:22,954   Average segmentation loss on validation set: 0.1953
2022-01-09 01:22:24,472 iteration 1700 : loss : 0.052440, loss_ce: 0.014196
 25%|███████▎                     | 100/400 [45:53<2:24:06, 28.82s/it]2022-01-09 01:22:26,071 iteration 1701 : loss : 0.056238, loss_ce: 0.026850
2022-01-09 01:22:27,566 iteration 1702 : loss : 0.045875, loss_ce: 0.017424
2022-01-09 01:22:28,976 iteration 1703 : loss : 0.044223, loss_ce: 0.017498
2022-01-09 01:22:30,549 iteration 1704 : loss : 0.063575, loss_ce: 0.024434
2022-01-09 01:22:32,004 iteration 1705 : loss : 0.061148, loss_ce: 0.027338
2022-01-09 01:22:33,491 iteration 1706 : loss : 0.054541, loss_ce: 0.027320
2022-01-09 01:22:34,953 iteration 1707 : loss : 0.056750, loss_ce: 0.018181
2022-01-09 01:22:36,467 iteration 1708 : loss : 0.049373, loss_ce: 0.017290
2022-01-09 01:22:37,839 iteration 1709 : loss : 0.046216, loss_ce: 0.020103
2022-01-09 01:22:39,188 iteration 1710 : loss : 0.031138, loss_ce: 0.013788
2022-01-09 01:22:40,653 iteration 1711 : loss : 0.064094, loss_ce: 0.027712
2022-01-09 01:22:42,048 iteration 1712 : loss : 0.048798, loss_ce: 0.016782
2022-01-09 01:22:43,430 iteration 1713 : loss : 0.048204, loss_ce: 0.016550
2022-01-09 01:22:44,992 iteration 1714 : loss : 0.075041, loss_ce: 0.023550
2022-01-09 01:22:46,488 iteration 1715 : loss : 0.084555, loss_ce: 0.025069
2022-01-09 01:22:48,050 iteration 1716 : loss : 0.061884, loss_ce: 0.020375
2022-01-09 01:22:49,506 iteration 1717 : loss : 0.062908, loss_ce: 0.025768
 25%|███████▎                     | 101/400 [46:18<2:17:58, 27.69s/it]2022-01-09 01:22:50,990 iteration 1718 : loss : 0.048505, loss_ce: 0.016862
2022-01-09 01:22:52,482 iteration 1719 : loss : 0.062363, loss_ce: 0.030068
2022-01-09 01:22:53,820 iteration 1720 : loss : 0.036272, loss_ce: 0.011419
2022-01-09 01:22:55,239 iteration 1721 : loss : 0.052483, loss_ce: 0.023958
2022-01-09 01:22:56,771 iteration 1722 : loss : 0.044906, loss_ce: 0.015441
2022-01-09 01:22:58,223 iteration 1723 : loss : 0.070354, loss_ce: 0.026855
2022-01-09 01:22:59,735 iteration 1724 : loss : 0.047756, loss_ce: 0.016743
2022-01-09 01:23:01,333 iteration 1725 : loss : 0.090826, loss_ce: 0.031677
2022-01-09 01:23:02,900 iteration 1726 : loss : 0.072219, loss_ce: 0.021053
2022-01-09 01:23:04,380 iteration 1727 : loss : 0.036700, loss_ce: 0.018228
2022-01-09 01:23:05,826 iteration 1728 : loss : 0.061414, loss_ce: 0.019973
2022-01-09 01:23:07,283 iteration 1729 : loss : 0.043304, loss_ce: 0.018577
2022-01-09 01:23:08,773 iteration 1730 : loss : 0.054110, loss_ce: 0.020768
2022-01-09 01:23:10,330 iteration 1731 : loss : 0.068648, loss_ce: 0.024234
2022-01-09 01:23:11,803 iteration 1732 : loss : 0.032873, loss_ce: 0.016028
2022-01-09 01:23:13,374 iteration 1733 : loss : 0.048860, loss_ce: 0.020821
2022-01-09 01:23:14,782 iteration 1734 : loss : 0.034091, loss_ce: 0.012012
 26%|███████▍                     | 102/400 [46:43<2:13:54, 26.96s/it]2022-01-09 01:23:16,263 iteration 1735 : loss : 0.061827, loss_ce: 0.021908
2022-01-09 01:23:17,709 iteration 1736 : loss : 0.051446, loss_ce: 0.017765
2022-01-09 01:23:19,179 iteration 1737 : loss : 0.047060, loss_ce: 0.020275
2022-01-09 01:23:20,603 iteration 1738 : loss : 0.050544, loss_ce: 0.016197
2022-01-09 01:23:22,188 iteration 1739 : loss : 0.048257, loss_ce: 0.022249
2022-01-09 01:23:23,715 iteration 1740 : loss : 0.062328, loss_ce: 0.021310
2022-01-09 01:23:25,170 iteration 1741 : loss : 0.047510, loss_ce: 0.016137
2022-01-09 01:23:26,773 iteration 1742 : loss : 0.050738, loss_ce: 0.023849
2022-01-09 01:23:28,221 iteration 1743 : loss : 0.050666, loss_ce: 0.015974
2022-01-09 01:23:29,728 iteration 1744 : loss : 0.048292, loss_ce: 0.019748
2022-01-09 01:23:31,397 iteration 1745 : loss : 0.072280, loss_ce: 0.037179
2022-01-09 01:23:32,849 iteration 1746 : loss : 0.036804, loss_ce: 0.014910
2022-01-09 01:23:34,286 iteration 1747 : loss : 0.047614, loss_ce: 0.020630
2022-01-09 01:23:35,753 iteration 1748 : loss : 0.035497, loss_ce: 0.014312
2022-01-09 01:23:37,238 iteration 1749 : loss : 0.043544, loss_ce: 0.015409
2022-01-09 01:23:38,639 iteration 1750 : loss : 0.037968, loss_ce: 0.018478
2022-01-09 01:23:40,228 iteration 1751 : loss : 0.060509, loss_ce: 0.022198
 26%|███████▍                     | 103/400 [47:09<2:11:11, 26.51s/it]2022-01-09 01:23:41,718 iteration 1752 : loss : 0.040236, loss_ce: 0.014874
2022-01-09 01:23:43,132 iteration 1753 : loss : 0.047394, loss_ce: 0.022996
2022-01-09 01:23:44,563 iteration 1754 : loss : 0.043778, loss_ce: 0.017036
2022-01-09 01:23:46,030 iteration 1755 : loss : 0.058762, loss_ce: 0.029989
2022-01-09 01:23:47,534 iteration 1756 : loss : 0.080145, loss_ce: 0.039569
2022-01-09 01:23:49,125 iteration 1757 : loss : 0.067532, loss_ce: 0.033199
2022-01-09 01:23:50,680 iteration 1758 : loss : 0.077079, loss_ce: 0.028832
2022-01-09 01:23:52,093 iteration 1759 : loss : 0.098110, loss_ce: 0.031823
2022-01-09 01:23:53,579 iteration 1760 : loss : 0.055216, loss_ce: 0.024599
2022-01-09 01:23:55,008 iteration 1761 : loss : 0.034015, loss_ce: 0.013116
2022-01-09 01:23:56,480 iteration 1762 : loss : 0.043888, loss_ce: 0.019047
2022-01-09 01:23:57,852 iteration 1763 : loss : 0.051058, loss_ce: 0.021779
2022-01-09 01:23:59,440 iteration 1764 : loss : 0.050609, loss_ce: 0.017902
2022-01-09 01:24:00,926 iteration 1765 : loss : 0.055490, loss_ce: 0.017267
2022-01-09 01:24:02,482 iteration 1766 : loss : 0.043503, loss_ce: 0.015666
2022-01-09 01:24:03,977 iteration 1767 : loss : 0.092072, loss_ce: 0.049635
2022-01-09 01:24:05,433 iteration 1768 : loss : 0.052838, loss_ce: 0.020820
 26%|███████▌                     | 104/400 [47:34<2:08:51, 26.12s/it]2022-01-09 01:24:07,022 iteration 1769 : loss : 0.043304, loss_ce: 0.016998
2022-01-09 01:24:08,487 iteration 1770 : loss : 0.047598, loss_ce: 0.016213
2022-01-09 01:24:09,915 iteration 1771 : loss : 0.031057, loss_ce: 0.010439
2022-01-09 01:24:11,470 iteration 1772 : loss : 0.064522, loss_ce: 0.035531
2022-01-09 01:24:12,981 iteration 1773 : loss : 0.061901, loss_ce: 0.028160
2022-01-09 01:24:14,420 iteration 1774 : loss : 0.063505, loss_ce: 0.023667
2022-01-09 01:24:15,875 iteration 1775 : loss : 0.047256, loss_ce: 0.016645
2022-01-09 01:24:17,307 iteration 1776 : loss : 0.054874, loss_ce: 0.021533
2022-01-09 01:24:18,713 iteration 1777 : loss : 0.042272, loss_ce: 0.017364
2022-01-09 01:24:20,249 iteration 1778 : loss : 0.052726, loss_ce: 0.023423
2022-01-09 01:24:21,739 iteration 1779 : loss : 0.049526, loss_ce: 0.020355
2022-01-09 01:24:23,272 iteration 1780 : loss : 0.081872, loss_ce: 0.023547
2022-01-09 01:24:24,808 iteration 1781 : loss : 0.051387, loss_ce: 0.020120
2022-01-09 01:24:26,216 iteration 1782 : loss : 0.086189, loss_ce: 0.022150
2022-01-09 01:24:27,683 iteration 1783 : loss : 0.057085, loss_ce: 0.026459
2022-01-09 01:24:29,220 iteration 1784 : loss : 0.061553, loss_ce: 0.030813
2022-01-09 01:24:29,220 Training Data Eval:
2022-01-09 01:24:36,809   Average segmentation loss on training set: 0.0378
2022-01-09 01:24:36,809 Validation Data Eval:
2022-01-09 01:24:39,399   Average segmentation loss on validation set: 0.1440
2022-01-09 01:24:40,892 iteration 1785 : loss : 0.052174, loss_ce: 0.024137
 26%|███████▌                     | 105/400 [48:09<2:22:11, 28.92s/it]2022-01-09 01:24:42,477 iteration 1786 : loss : 0.046189, loss_ce: 0.015096
2022-01-09 01:24:43,874 iteration 1787 : loss : 0.045232, loss_ce: 0.011467
2022-01-09 01:24:45,350 iteration 1788 : loss : 0.057559, loss_ce: 0.022879
2022-01-09 01:24:46,835 iteration 1789 : loss : 0.053465, loss_ce: 0.025225
2022-01-09 01:24:48,494 iteration 1790 : loss : 0.057161, loss_ce: 0.020324
2022-01-09 01:24:49,982 iteration 1791 : loss : 0.058246, loss_ce: 0.022033
2022-01-09 01:24:51,474 iteration 1792 : loss : 0.050332, loss_ce: 0.020567
2022-01-09 01:24:52,985 iteration 1793 : loss : 0.084530, loss_ce: 0.044931
2022-01-09 01:24:54,441 iteration 1794 : loss : 0.064194, loss_ce: 0.035956
2022-01-09 01:24:55,776 iteration 1795 : loss : 0.041534, loss_ce: 0.016427
2022-01-09 01:24:57,331 iteration 1796 : loss : 0.052048, loss_ce: 0.021744
2022-01-09 01:24:58,734 iteration 1797 : loss : 0.054438, loss_ce: 0.021929
2022-01-09 01:25:00,206 iteration 1798 : loss : 0.055224, loss_ce: 0.018823
2022-01-09 01:25:01,765 iteration 1799 : loss : 0.049498, loss_ce: 0.018186
2022-01-09 01:25:03,162 iteration 1800 : loss : 0.047499, loss_ce: 0.014967
2022-01-09 01:25:04,625 iteration 1801 : loss : 0.088220, loss_ce: 0.034184
2022-01-09 01:25:06,151 iteration 1802 : loss : 0.052754, loss_ce: 0.018361
 26%|███████▋                     | 106/400 [48:35<2:16:18, 27.82s/it]2022-01-09 01:25:07,745 iteration 1803 : loss : 0.067371, loss_ce: 0.026393
2022-01-09 01:25:09,128 iteration 1804 : loss : 0.055912, loss_ce: 0.017619
2022-01-09 01:25:10,749 iteration 1805 : loss : 0.057056, loss_ce: 0.020986
2022-01-09 01:25:12,330 iteration 1806 : loss : 0.055464, loss_ce: 0.026263
2022-01-09 01:25:13,787 iteration 1807 : loss : 0.040198, loss_ce: 0.017053
2022-01-09 01:25:15,316 iteration 1808 : loss : 0.043612, loss_ce: 0.018879
2022-01-09 01:25:16,752 iteration 1809 : loss : 0.053092, loss_ce: 0.023515
2022-01-09 01:25:18,260 iteration 1810 : loss : 0.063724, loss_ce: 0.034387
2022-01-09 01:25:19,703 iteration 1811 : loss : 0.052459, loss_ce: 0.017754
2022-01-09 01:25:21,130 iteration 1812 : loss : 0.046520, loss_ce: 0.016175
2022-01-09 01:25:22,604 iteration 1813 : loss : 0.052933, loss_ce: 0.024918
2022-01-09 01:25:24,043 iteration 1814 : loss : 0.068109, loss_ce: 0.024937
2022-01-09 01:25:25,611 iteration 1815 : loss : 0.061524, loss_ce: 0.022087
2022-01-09 01:25:27,097 iteration 1816 : loss : 0.040277, loss_ce: 0.016530
2022-01-09 01:25:28,664 iteration 1817 : loss : 0.056103, loss_ce: 0.021388
2022-01-09 01:25:30,056 iteration 1818 : loss : 0.038862, loss_ce: 0.013597
2022-01-09 01:25:31,591 iteration 1819 : loss : 0.058472, loss_ce: 0.022137
 27%|███████▊                     | 107/400 [49:00<2:12:22, 27.11s/it]2022-01-09 01:25:32,990 iteration 1820 : loss : 0.043090, loss_ce: 0.018479
2022-01-09 01:25:34,603 iteration 1821 : loss : 0.070807, loss_ce: 0.026198
2022-01-09 01:25:36,135 iteration 1822 : loss : 0.058316, loss_ce: 0.026544
2022-01-09 01:25:37,706 iteration 1823 : loss : 0.064972, loss_ce: 0.022999
2022-01-09 01:25:39,220 iteration 1824 : loss : 0.065773, loss_ce: 0.017638
2022-01-09 01:25:40,634 iteration 1825 : loss : 0.053547, loss_ce: 0.018979
2022-01-09 01:25:42,193 iteration 1826 : loss : 0.077181, loss_ce: 0.022829
2022-01-09 01:25:43,596 iteration 1827 : loss : 0.060260, loss_ce: 0.023892
2022-01-09 01:25:45,074 iteration 1828 : loss : 0.055879, loss_ce: 0.018806
2022-01-09 01:25:46,493 iteration 1829 : loss : 0.067897, loss_ce: 0.031845
2022-01-09 01:25:48,071 iteration 1830 : loss : 0.055221, loss_ce: 0.019804
2022-01-09 01:25:49,552 iteration 1831 : loss : 0.042481, loss_ce: 0.016280
2022-01-09 01:25:50,988 iteration 1832 : loss : 0.053980, loss_ce: 0.021386
2022-01-09 01:25:52,386 iteration 1833 : loss : 0.053020, loss_ce: 0.023504
2022-01-09 01:25:53,839 iteration 1834 : loss : 0.050847, loss_ce: 0.017933
2022-01-09 01:25:55,316 iteration 1835 : loss : 0.042383, loss_ce: 0.016356
2022-01-09 01:25:56,738 iteration 1836 : loss : 0.080314, loss_ce: 0.030103
 27%|███████▊                     | 108/400 [49:25<2:09:03, 26.52s/it]2022-01-09 01:25:58,317 iteration 1837 : loss : 0.046373, loss_ce: 0.017112
2022-01-09 01:25:59,690 iteration 1838 : loss : 0.040132, loss_ce: 0.017686
2022-01-09 01:26:01,099 iteration 1839 : loss : 0.051073, loss_ce: 0.020289
2022-01-09 01:26:02,589 iteration 1840 : loss : 0.044408, loss_ce: 0.020451
2022-01-09 01:26:03,978 iteration 1841 : loss : 0.052480, loss_ce: 0.021159
2022-01-09 01:26:05,416 iteration 1842 : loss : 0.038047, loss_ce: 0.013566
2022-01-09 01:26:06,907 iteration 1843 : loss : 0.040911, loss_ce: 0.018715
2022-01-09 01:26:08,349 iteration 1844 : loss : 0.046603, loss_ce: 0.015082
2022-01-09 01:26:09,783 iteration 1845 : loss : 0.033208, loss_ce: 0.009708
2022-01-09 01:26:11,340 iteration 1846 : loss : 0.043033, loss_ce: 0.019862
2022-01-09 01:26:12,756 iteration 1847 : loss : 0.051435, loss_ce: 0.019859
2022-01-09 01:26:14,201 iteration 1848 : loss : 0.055782, loss_ce: 0.023108
2022-01-09 01:26:15,631 iteration 1849 : loss : 0.059225, loss_ce: 0.028428
2022-01-09 01:26:17,253 iteration 1850 : loss : 0.061522, loss_ce: 0.026642
2022-01-09 01:26:18,718 iteration 1851 : loss : 0.081257, loss_ce: 0.024338
2022-01-09 01:26:20,205 iteration 1852 : loss : 0.048352, loss_ce: 0.020348
2022-01-09 01:26:21,630 iteration 1853 : loss : 0.042808, loss_ce: 0.018544
 27%|███████▉                     | 109/400 [49:50<2:06:15, 26.03s/it]2022-01-09 01:26:23,126 iteration 1854 : loss : 0.041700, loss_ce: 0.015614
2022-01-09 01:26:24,645 iteration 1855 : loss : 0.046169, loss_ce: 0.017292
2022-01-09 01:26:26,188 iteration 1856 : loss : 0.053771, loss_ce: 0.016441
2022-01-09 01:26:27,738 iteration 1857 : loss : 0.042392, loss_ce: 0.014667
2022-01-09 01:26:29,228 iteration 1858 : loss : 0.062911, loss_ce: 0.026282
2022-01-09 01:26:30,723 iteration 1859 : loss : 0.055995, loss_ce: 0.023098
2022-01-09 01:26:32,144 iteration 1860 : loss : 0.050469, loss_ce: 0.017679
2022-01-09 01:26:33,692 iteration 1861 : loss : 0.053693, loss_ce: 0.018389
2022-01-09 01:26:35,095 iteration 1862 : loss : 0.041500, loss_ce: 0.020219
2022-01-09 01:26:36,566 iteration 1863 : loss : 0.048726, loss_ce: 0.020855
2022-01-09 01:26:38,080 iteration 1864 : loss : 0.066264, loss_ce: 0.017739
2022-01-09 01:26:39,433 iteration 1865 : loss : 0.036097, loss_ce: 0.015786
2022-01-09 01:26:40,986 iteration 1866 : loss : 0.029946, loss_ce: 0.014518
2022-01-09 01:26:42,454 iteration 1867 : loss : 0.031315, loss_ce: 0.013226
2022-01-09 01:26:43,876 iteration 1868 : loss : 0.046073, loss_ce: 0.019512
2022-01-09 01:26:45,283 iteration 1869 : loss : 0.058387, loss_ce: 0.019942
2022-01-09 01:26:45,283 Training Data Eval:
2022-01-09 01:26:52,576   Average segmentation loss on training set: 0.0329
2022-01-09 01:26:52,577 Validation Data Eval:
2022-01-09 01:26:55,091   Average segmentation loss on validation set: 0.0800
2022-01-09 01:27:00,928 Found new lowest validation loss at iteration 1869! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 01:27:02,478 iteration 1870 : loss : 0.044022, loss_ce: 0.017276
 28%|███████▉                     | 110/400 [50:31<2:27:18, 30.48s/it]2022-01-09 01:27:04,077 iteration 1871 : loss : 0.028718, loss_ce: 0.011065
2022-01-09 01:27:05,689 iteration 1872 : loss : 0.073917, loss_ce: 0.022631
2022-01-09 01:27:07,087 iteration 1873 : loss : 0.048991, loss_ce: 0.019085
2022-01-09 01:27:08,643 iteration 1874 : loss : 0.060430, loss_ce: 0.032812
2022-01-09 01:27:09,997 iteration 1875 : loss : 0.043971, loss_ce: 0.016685
2022-01-09 01:27:11,555 iteration 1876 : loss : 0.043410, loss_ce: 0.011290
2022-01-09 01:27:13,079 iteration 1877 : loss : 0.054852, loss_ce: 0.021283
2022-01-09 01:27:14,508 iteration 1878 : loss : 0.039875, loss_ce: 0.018147
2022-01-09 01:27:15,933 iteration 1879 : loss : 0.039941, loss_ce: 0.014529
2022-01-09 01:27:17,349 iteration 1880 : loss : 0.049513, loss_ce: 0.023079
2022-01-09 01:27:18,811 iteration 1881 : loss : 0.058643, loss_ce: 0.020542
2022-01-09 01:27:20,213 iteration 1882 : loss : 0.036782, loss_ce: 0.014724
2022-01-09 01:27:21,562 iteration 1883 : loss : 0.054479, loss_ce: 0.019293
2022-01-09 01:27:23,068 iteration 1884 : loss : 0.059112, loss_ce: 0.022369
2022-01-09 01:27:24,412 iteration 1885 : loss : 0.038076, loss_ce: 0.015497
2022-01-09 01:27:25,980 iteration 1886 : loss : 0.043344, loss_ce: 0.017916
2022-01-09 01:27:27,540 iteration 1887 : loss : 0.058787, loss_ce: 0.025261
 28%|████████                     | 111/400 [50:56<2:18:58, 28.85s/it]2022-01-09 01:27:29,052 iteration 1888 : loss : 0.045473, loss_ce: 0.018677
2022-01-09 01:27:30,605 iteration 1889 : loss : 0.042050, loss_ce: 0.018052
2022-01-09 01:27:32,113 iteration 1890 : loss : 0.054388, loss_ce: 0.019391
2022-01-09 01:27:33,642 iteration 1891 : loss : 0.071447, loss_ce: 0.028769
2022-01-09 01:27:35,136 iteration 1892 : loss : 0.044497, loss_ce: 0.019321
2022-01-09 01:27:36,634 iteration 1893 : loss : 0.051558, loss_ce: 0.027825
2022-01-09 01:27:38,091 iteration 1894 : loss : 0.056245, loss_ce: 0.023914
2022-01-09 01:27:39,525 iteration 1895 : loss : 0.046485, loss_ce: 0.017903
2022-01-09 01:27:41,159 iteration 1896 : loss : 0.065768, loss_ce: 0.030254
2022-01-09 01:27:42,609 iteration 1897 : loss : 0.070187, loss_ce: 0.022468
2022-01-09 01:27:44,120 iteration 1898 : loss : 0.076139, loss_ce: 0.035533
2022-01-09 01:27:45,585 iteration 1899 : loss : 0.049638, loss_ce: 0.025236
2022-01-09 01:27:46,948 iteration 1900 : loss : 0.035821, loss_ce: 0.014563
2022-01-09 01:27:48,405 iteration 1901 : loss : 0.045881, loss_ce: 0.016703
2022-01-09 01:27:49,908 iteration 1902 : loss : 0.046601, loss_ce: 0.020934
2022-01-09 01:27:51,441 iteration 1903 : loss : 0.054703, loss_ce: 0.016939
2022-01-09 01:27:53,041 iteration 1904 : loss : 0.054451, loss_ce: 0.020521
 28%|████████                     | 112/400 [51:21<2:13:39, 27.84s/it]2022-01-09 01:27:54,512 iteration 1905 : loss : 0.035927, loss_ce: 0.014019
2022-01-09 01:27:56,055 iteration 1906 : loss : 0.078511, loss_ce: 0.028375
2022-01-09 01:27:57,678 iteration 1907 : loss : 0.073273, loss_ce: 0.027044
2022-01-09 01:27:59,195 iteration 1908 : loss : 0.066688, loss_ce: 0.023900
2022-01-09 01:28:00,755 iteration 1909 : loss : 0.053834, loss_ce: 0.030011
2022-01-09 01:28:02,303 iteration 1910 : loss : 0.038061, loss_ce: 0.017157
2022-01-09 01:28:03,939 iteration 1911 : loss : 0.043652, loss_ce: 0.015655
2022-01-09 01:28:05,475 iteration 1912 : loss : 0.046614, loss_ce: 0.013246
2022-01-09 01:28:06,938 iteration 1913 : loss : 0.041856, loss_ce: 0.016090
2022-01-09 01:28:08,358 iteration 1914 : loss : 0.035179, loss_ce: 0.016100
2022-01-09 01:28:09,836 iteration 1915 : loss : 0.062284, loss_ce: 0.021657
2022-01-09 01:28:11,304 iteration 1916 : loss : 0.050060, loss_ce: 0.019252
2022-01-09 01:28:12,812 iteration 1917 : loss : 0.059421, loss_ce: 0.022420
2022-01-09 01:28:14,226 iteration 1918 : loss : 0.058975, loss_ce: 0.024024
2022-01-09 01:28:15,673 iteration 1919 : loss : 0.048045, loss_ce: 0.022242
2022-01-09 01:28:17,123 iteration 1920 : loss : 0.042417, loss_ce: 0.015004
2022-01-09 01:28:18,640 iteration 1921 : loss : 0.054757, loss_ce: 0.019045
 28%|████████▏                    | 113/400 [51:47<2:09:57, 27.17s/it]2022-01-09 01:28:20,105 iteration 1922 : loss : 0.043486, loss_ce: 0.014934
2022-01-09 01:28:21,472 iteration 1923 : loss : 0.033740, loss_ce: 0.014507
2022-01-09 01:28:22,965 iteration 1924 : loss : 0.057635, loss_ce: 0.028184
2022-01-09 01:28:24,356 iteration 1925 : loss : 0.043507, loss_ce: 0.018951
2022-01-09 01:28:25,769 iteration 1926 : loss : 0.042245, loss_ce: 0.017119
2022-01-09 01:28:27,286 iteration 1927 : loss : 0.062230, loss_ce: 0.025008
2022-01-09 01:28:28,822 iteration 1928 : loss : 0.039204, loss_ce: 0.016447
2022-01-09 01:28:30,258 iteration 1929 : loss : 0.042236, loss_ce: 0.019260
2022-01-09 01:28:31,719 iteration 1930 : loss : 0.093272, loss_ce: 0.031065
2022-01-09 01:28:33,149 iteration 1931 : loss : 0.053427, loss_ce: 0.017889
2022-01-09 01:28:34,617 iteration 1932 : loss : 0.040442, loss_ce: 0.012357
2022-01-09 01:28:36,082 iteration 1933 : loss : 0.048495, loss_ce: 0.016403
2022-01-09 01:28:37,388 iteration 1934 : loss : 0.037638, loss_ce: 0.016092
2022-01-09 01:28:38,805 iteration 1935 : loss : 0.049815, loss_ce: 0.023388
2022-01-09 01:28:40,324 iteration 1936 : loss : 0.029846, loss_ce: 0.010791
2022-01-09 01:28:41,830 iteration 1937 : loss : 0.046783, loss_ce: 0.019185
2022-01-09 01:28:43,338 iteration 1938 : loss : 0.053198, loss_ce: 0.016289
 28%|████████▎                    | 114/400 [52:12<2:05:59, 26.43s/it]2022-01-09 01:28:44,787 iteration 1939 : loss : 0.042123, loss_ce: 0.020338
2022-01-09 01:28:46,218 iteration 1940 : loss : 0.033306, loss_ce: 0.012766
2022-01-09 01:28:47,701 iteration 1941 : loss : 0.034060, loss_ce: 0.014791
2022-01-09 01:28:49,161 iteration 1942 : loss : 0.059453, loss_ce: 0.018556
2022-01-09 01:28:50,521 iteration 1943 : loss : 0.037648, loss_ce: 0.012580
2022-01-09 01:28:51,944 iteration 1944 : loss : 0.041117, loss_ce: 0.016618
2022-01-09 01:28:53,375 iteration 1945 : loss : 0.044255, loss_ce: 0.023190
2022-01-09 01:28:54,818 iteration 1946 : loss : 0.059213, loss_ce: 0.024138
2022-01-09 01:28:56,244 iteration 1947 : loss : 0.055306, loss_ce: 0.021081
2022-01-09 01:28:57,661 iteration 1948 : loss : 0.041691, loss_ce: 0.018464
2022-01-09 01:28:59,020 iteration 1949 : loss : 0.038076, loss_ce: 0.018880
2022-01-09 01:29:00,525 iteration 1950 : loss : 0.045046, loss_ce: 0.016666
2022-01-09 01:29:02,125 iteration 1951 : loss : 0.081248, loss_ce: 0.029568
2022-01-09 01:29:03,517 iteration 1952 : loss : 0.038574, loss_ce: 0.011587
2022-01-09 01:29:05,003 iteration 1953 : loss : 0.046490, loss_ce: 0.024309
2022-01-09 01:29:06,563 iteration 1954 : loss : 0.043322, loss_ce: 0.014568
2022-01-09 01:29:06,563 Training Data Eval:
2022-01-09 01:29:13,836   Average segmentation loss on training set: 0.0354
2022-01-09 01:29:13,836 Validation Data Eval:
2022-01-09 01:29:16,322   Average segmentation loss on validation set: 0.0716
2022-01-09 01:29:22,224 Found new lowest validation loss at iteration 1954! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 01:29:23,699 iteration 1955 : loss : 0.038340, loss_ce: 0.015384
 29%|████████▎                    | 115/400 [52:52<2:25:24, 30.61s/it]2022-01-09 01:29:25,283 iteration 1956 : loss : 0.058525, loss_ce: 0.022214
2022-01-09 01:29:26,676 iteration 1957 : loss : 0.042645, loss_ce: 0.018364
2022-01-09 01:29:28,101 iteration 1958 : loss : 0.040506, loss_ce: 0.019976
2022-01-09 01:29:29,659 iteration 1959 : loss : 0.051004, loss_ce: 0.014606
2022-01-09 01:29:31,083 iteration 1960 : loss : 0.047546, loss_ce: 0.015034
2022-01-09 01:29:32,553 iteration 1961 : loss : 0.053889, loss_ce: 0.020831
2022-01-09 01:29:34,001 iteration 1962 : loss : 0.037108, loss_ce: 0.014989
2022-01-09 01:29:35,480 iteration 1963 : loss : 0.042969, loss_ce: 0.018228
2022-01-09 01:29:36,912 iteration 1964 : loss : 0.054149, loss_ce: 0.014708
2022-01-09 01:29:38,318 iteration 1965 : loss : 0.042756, loss_ce: 0.018879
2022-01-09 01:29:39,721 iteration 1966 : loss : 0.046373, loss_ce: 0.020978
2022-01-09 01:29:41,109 iteration 1967 : loss : 0.052613, loss_ce: 0.016706
2022-01-09 01:29:42,475 iteration 1968 : loss : 0.040538, loss_ce: 0.022092
2022-01-09 01:29:43,964 iteration 1969 : loss : 0.042422, loss_ce: 0.014816
2022-01-09 01:29:45,603 iteration 1970 : loss : 0.047218, loss_ce: 0.018336
2022-01-09 01:29:46,943 iteration 1971 : loss : 0.043435, loss_ce: 0.014010
2022-01-09 01:29:48,561 iteration 1972 : loss : 0.055885, loss_ce: 0.020119
 29%|████████▍                    | 116/400 [53:17<2:16:43, 28.89s/it]2022-01-09 01:29:49,979 iteration 1973 : loss : 0.042851, loss_ce: 0.016959
2022-01-09 01:29:51,346 iteration 1974 : loss : 0.043624, loss_ce: 0.017634
2022-01-09 01:29:52,903 iteration 1975 : loss : 0.044368, loss_ce: 0.016360
2022-01-09 01:29:54,261 iteration 1976 : loss : 0.040816, loss_ce: 0.016160
2022-01-09 01:29:55,824 iteration 1977 : loss : 0.049655, loss_ce: 0.019925
2022-01-09 01:29:57,238 iteration 1978 : loss : 0.046486, loss_ce: 0.018121
2022-01-09 01:29:58,671 iteration 1979 : loss : 0.047729, loss_ce: 0.023924
2022-01-09 01:30:00,185 iteration 1980 : loss : 0.050921, loss_ce: 0.017530
2022-01-09 01:30:01,585 iteration 1981 : loss : 0.039100, loss_ce: 0.015548
2022-01-09 01:30:03,125 iteration 1982 : loss : 0.039577, loss_ce: 0.014988
2022-01-09 01:30:04,715 iteration 1983 : loss : 0.039364, loss_ce: 0.015907
2022-01-09 01:30:06,130 iteration 1984 : loss : 0.040235, loss_ce: 0.012259
2022-01-09 01:30:07,560 iteration 1985 : loss : 0.037654, loss_ce: 0.014041
2022-01-09 01:30:09,000 iteration 1986 : loss : 0.043785, loss_ce: 0.015060
2022-01-09 01:30:10,441 iteration 1987 : loss : 0.032930, loss_ce: 0.012730
2022-01-09 01:30:12,036 iteration 1988 : loss : 0.036131, loss_ce: 0.015664
2022-01-09 01:30:13,499 iteration 1989 : loss : 0.038288, loss_ce: 0.016569
 29%|████████▍                    | 117/400 [53:42<2:10:38, 27.70s/it]2022-01-09 01:30:15,047 iteration 1990 : loss : 0.069869, loss_ce: 0.029214
2022-01-09 01:30:16,393 iteration 1991 : loss : 0.033225, loss_ce: 0.014653
2022-01-09 01:30:17,903 iteration 1992 : loss : 0.038602, loss_ce: 0.017043
2022-01-09 01:30:19,344 iteration 1993 : loss : 0.063914, loss_ce: 0.019137
2022-01-09 01:30:20,851 iteration 1994 : loss : 0.036977, loss_ce: 0.015910
2022-01-09 01:30:22,307 iteration 1995 : loss : 0.045212, loss_ce: 0.016178
2022-01-09 01:30:23,781 iteration 1996 : loss : 0.053721, loss_ce: 0.025962
2022-01-09 01:30:25,248 iteration 1997 : loss : 0.045440, loss_ce: 0.019745
2022-01-09 01:30:26,752 iteration 1998 : loss : 0.054995, loss_ce: 0.022028
2022-01-09 01:30:28,241 iteration 1999 : loss : 0.045621, loss_ce: 0.018575
2022-01-09 01:30:29,864 iteration 2000 : loss : 0.064742, loss_ce: 0.014646
2022-01-09 01:30:31,338 iteration 2001 : loss : 0.045097, loss_ce: 0.018552
2022-01-09 01:30:32,705 iteration 2002 : loss : 0.040444, loss_ce: 0.015432
2022-01-09 01:30:34,005 iteration 2003 : loss : 0.027411, loss_ce: 0.012868
2022-01-09 01:30:35,407 iteration 2004 : loss : 0.052041, loss_ce: 0.014268
2022-01-09 01:30:36,913 iteration 2005 : loss : 0.064575, loss_ce: 0.022264
2022-01-09 01:30:38,408 iteration 2006 : loss : 0.045617, loss_ce: 0.015699
 30%|████████▌                    | 118/400 [54:07<2:06:14, 26.86s/it]2022-01-09 01:30:40,018 iteration 2007 : loss : 0.046971, loss_ce: 0.018181
2022-01-09 01:30:41,639 iteration 2008 : loss : 0.044605, loss_ce: 0.015933
2022-01-09 01:30:43,096 iteration 2009 : loss : 0.040647, loss_ce: 0.016524
2022-01-09 01:30:44,551 iteration 2010 : loss : 0.074371, loss_ce: 0.027097
2022-01-09 01:30:46,128 iteration 2011 : loss : 0.060184, loss_ce: 0.024012
2022-01-09 01:30:47,611 iteration 2012 : loss : 0.037919, loss_ce: 0.020133
2022-01-09 01:30:48,958 iteration 2013 : loss : 0.043110, loss_ce: 0.017546
2022-01-09 01:30:50,382 iteration 2014 : loss : 0.042598, loss_ce: 0.012590
2022-01-09 01:30:51,887 iteration 2015 : loss : 0.038949, loss_ce: 0.015538
2022-01-09 01:30:53,331 iteration 2016 : loss : 0.058780, loss_ce: 0.020255
2022-01-09 01:30:54,854 iteration 2017 : loss : 0.027734, loss_ce: 0.007486
2022-01-09 01:30:56,451 iteration 2018 : loss : 0.043246, loss_ce: 0.018747
2022-01-09 01:30:57,899 iteration 2019 : loss : 0.046643, loss_ce: 0.016910
2022-01-09 01:30:59,311 iteration 2020 : loss : 0.030779, loss_ce: 0.010030
2022-01-09 01:31:00,791 iteration 2021 : loss : 0.051529, loss_ce: 0.019147
2022-01-09 01:31:02,294 iteration 2022 : loss : 0.042421, loss_ce: 0.018685
2022-01-09 01:31:03,844 iteration 2023 : loss : 0.044701, loss_ce: 0.016488
 30%|████████▋                    | 119/400 [54:32<2:03:47, 26.43s/it]2022-01-09 01:31:05,310 iteration 2024 : loss : 0.041136, loss_ce: 0.014500
2022-01-09 01:31:06,805 iteration 2025 : loss : 0.035232, loss_ce: 0.015690
2022-01-09 01:31:08,343 iteration 2026 : loss : 0.050706, loss_ce: 0.021736
2022-01-09 01:31:09,784 iteration 2027 : loss : 0.040169, loss_ce: 0.012988
2022-01-09 01:31:11,179 iteration 2028 : loss : 0.033887, loss_ce: 0.017420
2022-01-09 01:31:12,604 iteration 2029 : loss : 0.035093, loss_ce: 0.014163
2022-01-09 01:31:14,018 iteration 2030 : loss : 0.049877, loss_ce: 0.019524
2022-01-09 01:31:15,480 iteration 2031 : loss : 0.038440, loss_ce: 0.013146
2022-01-09 01:31:17,061 iteration 2032 : loss : 0.070807, loss_ce: 0.035332
2022-01-09 01:31:18,679 iteration 2033 : loss : 0.047223, loss_ce: 0.021679
2022-01-09 01:31:20,105 iteration 2034 : loss : 0.035668, loss_ce: 0.014073
2022-01-09 01:31:21,580 iteration 2035 : loss : 0.040955, loss_ce: 0.014540
2022-01-09 01:31:23,103 iteration 2036 : loss : 0.044763, loss_ce: 0.023485
2022-01-09 01:31:24,566 iteration 2037 : loss : 0.058437, loss_ce: 0.014886
2022-01-09 01:31:26,092 iteration 2038 : loss : 0.049446, loss_ce: 0.019065
2022-01-09 01:31:27,701 iteration 2039 : loss : 0.046739, loss_ce: 0.016542
2022-01-09 01:31:27,701 Training Data Eval:
2022-01-09 01:31:35,365   Average segmentation loss on training set: 0.0418
2022-01-09 01:31:35,366 Validation Data Eval:
2022-01-09 01:31:37,894   Average segmentation loss on validation set: 0.0652
2022-01-09 01:31:43,887 Found new lowest validation loss at iteration 2039! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 01:31:45,483 iteration 2040 : loss : 0.044481, loss_ce: 0.017039
 30%|████████▋                    | 120/400 [55:14<2:24:40, 31.00s/it]2022-01-09 01:31:47,078 iteration 2041 : loss : 0.076390, loss_ce: 0.025569
2022-01-09 01:31:48,518 iteration 2042 : loss : 0.042134, loss_ce: 0.015326
2022-01-09 01:31:49,948 iteration 2043 : loss : 0.038798, loss_ce: 0.013340
2022-01-09 01:31:51,401 iteration 2044 : loss : 0.040984, loss_ce: 0.015194
2022-01-09 01:31:52,820 iteration 2045 : loss : 0.034587, loss_ce: 0.010508
2022-01-09 01:31:54,323 iteration 2046 : loss : 0.048570, loss_ce: 0.023881
2022-01-09 01:31:55,749 iteration 2047 : loss : 0.038430, loss_ce: 0.016275
2022-01-09 01:31:57,222 iteration 2048 : loss : 0.049330, loss_ce: 0.021172
2022-01-09 01:31:58,645 iteration 2049 : loss : 0.041386, loss_ce: 0.010132
2022-01-09 01:32:00,129 iteration 2050 : loss : 0.057768, loss_ce: 0.027984
2022-01-09 01:32:01,614 iteration 2051 : loss : 0.048687, loss_ce: 0.022979
2022-01-09 01:32:03,005 iteration 2052 : loss : 0.048890, loss_ce: 0.019222
2022-01-09 01:32:04,478 iteration 2053 : loss : 0.061652, loss_ce: 0.019234
2022-01-09 01:32:05,905 iteration 2054 : loss : 0.039131, loss_ce: 0.016009
2022-01-09 01:32:07,349 iteration 2055 : loss : 0.054383, loss_ce: 0.017158
2022-01-09 01:32:08,862 iteration 2056 : loss : 0.046240, loss_ce: 0.019974
2022-01-09 01:32:10,262 iteration 2057 : loss : 0.049108, loss_ce: 0.021776
 30%|████████▊                    | 121/400 [55:39<2:15:26, 29.13s/it]2022-01-09 01:32:11,793 iteration 2058 : loss : 0.042920, loss_ce: 0.019672
2022-01-09 01:32:13,328 iteration 2059 : loss : 0.071416, loss_ce: 0.027609
2022-01-09 01:32:14,676 iteration 2060 : loss : 0.047861, loss_ce: 0.019830
2022-01-09 01:32:16,174 iteration 2061 : loss : 0.055471, loss_ce: 0.021869
2022-01-09 01:32:17,604 iteration 2062 : loss : 0.054105, loss_ce: 0.019700
2022-01-09 01:32:19,054 iteration 2063 : loss : 0.080562, loss_ce: 0.016433
2022-01-09 01:32:20,444 iteration 2064 : loss : 0.035878, loss_ce: 0.014374
2022-01-09 01:32:21,871 iteration 2065 : loss : 0.044398, loss_ce: 0.012736
2022-01-09 01:32:23,322 iteration 2066 : loss : 0.050758, loss_ce: 0.018954
2022-01-09 01:32:24,820 iteration 2067 : loss : 0.037132, loss_ce: 0.012869
2022-01-09 01:32:26,367 iteration 2068 : loss : 0.034544, loss_ce: 0.014585
2022-01-09 01:32:27,795 iteration 2069 : loss : 0.038357, loss_ce: 0.012502
2022-01-09 01:32:29,314 iteration 2070 : loss : 0.040696, loss_ce: 0.016038
2022-01-09 01:32:30,857 iteration 2071 : loss : 0.063672, loss_ce: 0.022660
2022-01-09 01:32:32,330 iteration 2072 : loss : 0.040148, loss_ce: 0.013578
2022-01-09 01:32:33,867 iteration 2073 : loss : 0.056757, loss_ce: 0.017270
2022-01-09 01:32:35,380 iteration 2074 : loss : 0.043127, loss_ce: 0.022987
 30%|████████▊                    | 122/400 [56:04<2:09:24, 27.93s/it]2022-01-09 01:32:36,873 iteration 2075 : loss : 0.028559, loss_ce: 0.010254
2022-01-09 01:32:38,341 iteration 2076 : loss : 0.042510, loss_ce: 0.014000
2022-01-09 01:32:39,842 iteration 2077 : loss : 0.044124, loss_ce: 0.018581
2022-01-09 01:32:41,484 iteration 2078 : loss : 0.053079, loss_ce: 0.017419
2022-01-09 01:32:42,930 iteration 2079 : loss : 0.043704, loss_ce: 0.014075
2022-01-09 01:32:44,478 iteration 2080 : loss : 0.055533, loss_ce: 0.020882
2022-01-09 01:32:45,828 iteration 2081 : loss : 0.054438, loss_ce: 0.021263
2022-01-09 01:32:47,150 iteration 2082 : loss : 0.030499, loss_ce: 0.011325
2022-01-09 01:32:48,582 iteration 2083 : loss : 0.065761, loss_ce: 0.035105
2022-01-09 01:32:50,021 iteration 2084 : loss : 0.054010, loss_ce: 0.025721
2022-01-09 01:32:51,417 iteration 2085 : loss : 0.038480, loss_ce: 0.011312
2022-01-09 01:32:52,866 iteration 2086 : loss : 0.050570, loss_ce: 0.020642
2022-01-09 01:32:54,275 iteration 2087 : loss : 0.050341, loss_ce: 0.019915
2022-01-09 01:32:55,829 iteration 2088 : loss : 0.031479, loss_ce: 0.013851
2022-01-09 01:32:57,352 iteration 2089 : loss : 0.038209, loss_ce: 0.016079
2022-01-09 01:32:58,756 iteration 2090 : loss : 0.048570, loss_ce: 0.020271
2022-01-09 01:33:00,318 iteration 2091 : loss : 0.042819, loss_ce: 0.018565
 31%|████████▉                    | 123/400 [56:29<2:04:47, 27.03s/it]2022-01-09 01:33:01,798 iteration 2092 : loss : 0.033324, loss_ce: 0.014535
2022-01-09 01:33:03,273 iteration 2093 : loss : 0.060644, loss_ce: 0.025679
2022-01-09 01:33:04,756 iteration 2094 : loss : 0.058799, loss_ce: 0.017775
2022-01-09 01:33:06,159 iteration 2095 : loss : 0.028779, loss_ce: 0.012624
2022-01-09 01:33:07,736 iteration 2096 : loss : 0.045096, loss_ce: 0.023810
2022-01-09 01:33:09,247 iteration 2097 : loss : 0.068494, loss_ce: 0.025012
2022-01-09 01:33:10,738 iteration 2098 : loss : 0.060384, loss_ce: 0.024184
2022-01-09 01:33:12,238 iteration 2099 : loss : 0.037551, loss_ce: 0.011915
2022-01-09 01:33:13,766 iteration 2100 : loss : 0.056169, loss_ce: 0.031174
2022-01-09 01:33:15,264 iteration 2101 : loss : 0.042992, loss_ce: 0.013726
2022-01-09 01:33:16,808 iteration 2102 : loss : 0.058449, loss_ce: 0.031527
2022-01-09 01:33:18,184 iteration 2103 : loss : 0.049453, loss_ce: 0.024124
2022-01-09 01:33:19,578 iteration 2104 : loss : 0.053028, loss_ce: 0.017725
2022-01-09 01:33:21,164 iteration 2105 : loss : 0.047586, loss_ce: 0.014219
2022-01-09 01:33:22,585 iteration 2106 : loss : 0.039293, loss_ce: 0.016600
2022-01-09 01:33:24,156 iteration 2107 : loss : 0.036447, loss_ce: 0.013494
2022-01-09 01:33:25,657 iteration 2108 : loss : 0.081868, loss_ce: 0.035219
 31%|████████▉                    | 124/400 [56:54<2:02:00, 26.52s/it]2022-01-09 01:33:27,189 iteration 2109 : loss : 0.046278, loss_ce: 0.019018
2022-01-09 01:33:28,705 iteration 2110 : loss : 0.050459, loss_ce: 0.019801
2022-01-09 01:33:30,285 iteration 2111 : loss : 0.056830, loss_ce: 0.017706
2022-01-09 01:33:31,774 iteration 2112 : loss : 0.080080, loss_ce: 0.035255
2022-01-09 01:33:33,203 iteration 2113 : loss : 0.047590, loss_ce: 0.023283
2022-01-09 01:33:34,721 iteration 2114 : loss : 0.040863, loss_ce: 0.019690
2022-01-09 01:33:36,224 iteration 2115 : loss : 0.042435, loss_ce: 0.015776
2022-01-09 01:33:37,726 iteration 2116 : loss : 0.052758, loss_ce: 0.018227
2022-01-09 01:33:39,204 iteration 2117 : loss : 0.041091, loss_ce: 0.016278
2022-01-09 01:33:40,730 iteration 2118 : loss : 0.061125, loss_ce: 0.023783
2022-01-09 01:33:42,261 iteration 2119 : loss : 0.060145, loss_ce: 0.025857
2022-01-09 01:33:43,735 iteration 2120 : loss : 0.044814, loss_ce: 0.012081
2022-01-09 01:33:45,092 iteration 2121 : loss : 0.046227, loss_ce: 0.016843
2022-01-09 01:33:46,543 iteration 2122 : loss : 0.042109, loss_ce: 0.014711
2022-01-09 01:33:48,141 iteration 2123 : loss : 0.080845, loss_ce: 0.042085
2022-01-09 01:33:49,776 iteration 2124 : loss : 0.042782, loss_ce: 0.015837
2022-01-09 01:33:49,776 Training Data Eval:
2022-01-09 01:33:57,553   Average segmentation loss on training set: 0.0389
2022-01-09 01:33:57,553 Validation Data Eval:
2022-01-09 01:34:00,238   Average segmentation loss on validation set: 0.1102
2022-01-09 01:34:01,792 iteration 2125 : loss : 0.053704, loss_ce: 0.028713
 31%|█████████                    | 125/400 [57:30<2:14:47, 29.41s/it]2022-01-09 01:34:03,419 iteration 2126 : loss : 0.032389, loss_ce: 0.012837
2022-01-09 01:34:04,960 iteration 2127 : loss : 0.063729, loss_ce: 0.030816
2022-01-09 01:34:06,484 iteration 2128 : loss : 0.038976, loss_ce: 0.013309
2022-01-09 01:34:07,952 iteration 2129 : loss : 0.058416, loss_ce: 0.020809
2022-01-09 01:34:09,505 iteration 2130 : loss : 0.077503, loss_ce: 0.024535
2022-01-09 01:34:10,971 iteration 2131 : loss : 0.056559, loss_ce: 0.016904
2022-01-09 01:34:12,461 iteration 2132 : loss : 0.049653, loss_ce: 0.022232
2022-01-09 01:34:14,046 iteration 2133 : loss : 0.057277, loss_ce: 0.019450
2022-01-09 01:34:15,450 iteration 2134 : loss : 0.038886, loss_ce: 0.018699
2022-01-09 01:34:16,947 iteration 2135 : loss : 0.039293, loss_ce: 0.015223
2022-01-09 01:34:18,427 iteration 2136 : loss : 0.045954, loss_ce: 0.018904
2022-01-09 01:34:19,918 iteration 2137 : loss : 0.048229, loss_ce: 0.020205
2022-01-09 01:34:21,349 iteration 2138 : loss : 0.052253, loss_ce: 0.020355
2022-01-09 01:34:22,875 iteration 2139 : loss : 0.045832, loss_ce: 0.024593
2022-01-09 01:34:24,332 iteration 2140 : loss : 0.049460, loss_ce: 0.023053
2022-01-09 01:34:25,886 iteration 2141 : loss : 0.061489, loss_ce: 0.016219
2022-01-09 01:34:27,304 iteration 2142 : loss : 0.053130, loss_ce: 0.018372
 32%|█████████▏                   | 126/400 [57:56<2:08:57, 28.24s/it]2022-01-09 01:34:28,740 iteration 2143 : loss : 0.046587, loss_ce: 0.019955
2022-01-09 01:34:30,229 iteration 2144 : loss : 0.043112, loss_ce: 0.013358
2022-01-09 01:34:31,736 iteration 2145 : loss : 0.058716, loss_ce: 0.021287
2022-01-09 01:34:33,133 iteration 2146 : loss : 0.026709, loss_ce: 0.011791
2022-01-09 01:34:34,618 iteration 2147 : loss : 0.048933, loss_ce: 0.018368
2022-01-09 01:34:36,045 iteration 2148 : loss : 0.042036, loss_ce: 0.016672
2022-01-09 01:34:37,510 iteration 2149 : loss : 0.035124, loss_ce: 0.012600
2022-01-09 01:34:38,944 iteration 2150 : loss : 0.026796, loss_ce: 0.010366
2022-01-09 01:34:40,497 iteration 2151 : loss : 0.062944, loss_ce: 0.026570
2022-01-09 01:34:41,902 iteration 2152 : loss : 0.038657, loss_ce: 0.014793
2022-01-09 01:34:43,296 iteration 2153 : loss : 0.037948, loss_ce: 0.014673
2022-01-09 01:34:44,933 iteration 2154 : loss : 0.095053, loss_ce: 0.032982
2022-01-09 01:34:46,394 iteration 2155 : loss : 0.037481, loss_ce: 0.014043
2022-01-09 01:34:47,797 iteration 2156 : loss : 0.041578, loss_ce: 0.017123
2022-01-09 01:34:49,448 iteration 2157 : loss : 0.120482, loss_ce: 0.043441
2022-01-09 01:34:51,045 iteration 2158 : loss : 0.054148, loss_ce: 0.024776
2022-01-09 01:34:52,546 iteration 2159 : loss : 0.066182, loss_ce: 0.021034
 32%|█████████▏                   | 127/400 [58:21<2:04:22, 27.34s/it]2022-01-09 01:34:54,032 iteration 2160 : loss : 0.046559, loss_ce: 0.018188
2022-01-09 01:34:55,423 iteration 2161 : loss : 0.047716, loss_ce: 0.016245
2022-01-09 01:34:56,860 iteration 2162 : loss : 0.049032, loss_ce: 0.019431
2022-01-09 01:34:58,281 iteration 2163 : loss : 0.048914, loss_ce: 0.017769
2022-01-09 01:34:59,794 iteration 2164 : loss : 0.043884, loss_ce: 0.021756
2022-01-09 01:35:01,285 iteration 2165 : loss : 0.065646, loss_ce: 0.023138
2022-01-09 01:35:02,686 iteration 2166 : loss : 0.042134, loss_ce: 0.018440
2022-01-09 01:35:04,045 iteration 2167 : loss : 0.035152, loss_ce: 0.013147
2022-01-09 01:35:05,472 iteration 2168 : loss : 0.038481, loss_ce: 0.016835
2022-01-09 01:35:06,953 iteration 2169 : loss : 0.070418, loss_ce: 0.024246
2022-01-09 01:35:08,270 iteration 2170 : loss : 0.036617, loss_ce: 0.012696
2022-01-09 01:35:09,698 iteration 2171 : loss : 0.048869, loss_ce: 0.017444
2022-01-09 01:35:11,106 iteration 2172 : loss : 0.064834, loss_ce: 0.022913
2022-01-09 01:35:12,514 iteration 2173 : loss : 0.046423, loss_ce: 0.012925
2022-01-09 01:35:13,966 iteration 2174 : loss : 0.046228, loss_ce: 0.018834
2022-01-09 01:35:15,549 iteration 2175 : loss : 0.052807, loss_ce: 0.024873
2022-01-09 01:35:16,869 iteration 2176 : loss : 0.035010, loss_ce: 0.017427
 32%|█████████▎                   | 128/400 [58:45<1:59:50, 26.44s/it]2022-01-09 01:35:18,299 iteration 2177 : loss : 0.066893, loss_ce: 0.036534
2022-01-09 01:35:19,752 iteration 2178 : loss : 0.057835, loss_ce: 0.020829
2022-01-09 01:35:21,306 iteration 2179 : loss : 0.043271, loss_ce: 0.015077
2022-01-09 01:35:22,829 iteration 2180 : loss : 0.040390, loss_ce: 0.019917
2022-01-09 01:35:24,199 iteration 2181 : loss : 0.049839, loss_ce: 0.019506
2022-01-09 01:35:25,653 iteration 2182 : loss : 0.049053, loss_ce: 0.016506
2022-01-09 01:35:27,137 iteration 2183 : loss : 0.058627, loss_ce: 0.018619
2022-01-09 01:35:28,526 iteration 2184 : loss : 0.052936, loss_ce: 0.026092
2022-01-09 01:35:29,914 iteration 2185 : loss : 0.045724, loss_ce: 0.018508
2022-01-09 01:35:31,470 iteration 2186 : loss : 0.055543, loss_ce: 0.023428
2022-01-09 01:35:33,048 iteration 2187 : loss : 0.044658, loss_ce: 0.013668
2022-01-09 01:35:34,713 iteration 2188 : loss : 0.053880, loss_ce: 0.028866
2022-01-09 01:35:36,378 iteration 2189 : loss : 0.066132, loss_ce: 0.020908
2022-01-09 01:35:37,886 iteration 2190 : loss : 0.057126, loss_ce: 0.021565
2022-01-09 01:35:39,447 iteration 2191 : loss : 0.031521, loss_ce: 0.010832
2022-01-09 01:35:40,944 iteration 2192 : loss : 0.048675, loss_ce: 0.018648
2022-01-09 01:35:42,490 iteration 2193 : loss : 0.041919, loss_ce: 0.012781
 32%|█████████▎                   | 129/400 [59:11<1:58:18, 26.19s/it]2022-01-09 01:35:44,109 iteration 2194 : loss : 0.046716, loss_ce: 0.016895
2022-01-09 01:35:45,646 iteration 2195 : loss : 0.059787, loss_ce: 0.029701
2022-01-09 01:35:47,210 iteration 2196 : loss : 0.043553, loss_ce: 0.018894
2022-01-09 01:35:48,817 iteration 2197 : loss : 0.041801, loss_ce: 0.014621
2022-01-09 01:35:50,321 iteration 2198 : loss : 0.056756, loss_ce: 0.020530
2022-01-09 01:35:51,843 iteration 2199 : loss : 0.047648, loss_ce: 0.019419
2022-01-09 01:35:53,465 iteration 2200 : loss : 0.107592, loss_ce: 0.022814
2022-01-09 01:35:54,913 iteration 2201 : loss : 0.037808, loss_ce: 0.013910
2022-01-09 01:35:56,561 iteration 2202 : loss : 0.075704, loss_ce: 0.032107
2022-01-09 01:35:58,102 iteration 2203 : loss : 0.048587, loss_ce: 0.016807
2022-01-09 01:35:59,548 iteration 2204 : loss : 0.034623, loss_ce: 0.011020
2022-01-09 01:36:01,091 iteration 2205 : loss : 0.049585, loss_ce: 0.021417
2022-01-09 01:36:02,733 iteration 2206 : loss : 0.058477, loss_ce: 0.023444
2022-01-09 01:36:04,290 iteration 2207 : loss : 0.048338, loss_ce: 0.015012
2022-01-09 01:36:05,885 iteration 2208 : loss : 0.064858, loss_ce: 0.017799
2022-01-09 01:36:07,409 iteration 2209 : loss : 0.053996, loss_ce: 0.018024
2022-01-09 01:36:07,410 Training Data Eval:
2022-01-09 01:36:15,053   Average segmentation loss on training set: 0.0313
2022-01-09 01:36:15,054 Validation Data Eval:
2022-01-09 01:36:17,643   Average segmentation loss on validation set: 0.0692
2022-01-09 01:36:19,160 iteration 2210 : loss : 0.053052, loss_ce: 0.028111
 32%|█████████▍                   | 130/400 [59:48<2:12:00, 29.33s/it]2022-01-09 01:36:20,704 iteration 2211 : loss : 0.045045, loss_ce: 0.012218
2022-01-09 01:36:22,118 iteration 2212 : loss : 0.048703, loss_ce: 0.020564
2022-01-09 01:36:23,606 iteration 2213 : loss : 0.080281, loss_ce: 0.016033
2022-01-09 01:36:24,999 iteration 2214 : loss : 0.051667, loss_ce: 0.017093
2022-01-09 01:36:26,388 iteration 2215 : loss : 0.031758, loss_ce: 0.012052
2022-01-09 01:36:27,837 iteration 2216 : loss : 0.065094, loss_ce: 0.037342
2022-01-09 01:36:29,269 iteration 2217 : loss : 0.053061, loss_ce: 0.025306
2022-01-09 01:36:30,724 iteration 2218 : loss : 0.036167, loss_ce: 0.013869
2022-01-09 01:36:32,230 iteration 2219 : loss : 0.058303, loss_ce: 0.026527
2022-01-09 01:36:33,803 iteration 2220 : loss : 0.042907, loss_ce: 0.019470
2022-01-09 01:36:35,360 iteration 2221 : loss : 0.048672, loss_ce: 0.016071
2022-01-09 01:36:36,713 iteration 2222 : loss : 0.040578, loss_ce: 0.015403
2022-01-09 01:36:38,102 iteration 2223 : loss : 0.050300, loss_ce: 0.017463
2022-01-09 01:36:39,648 iteration 2224 : loss : 0.042711, loss_ce: 0.016940
2022-01-09 01:36:41,143 iteration 2225 : loss : 0.049232, loss_ce: 0.017209
2022-01-09 01:36:42,711 iteration 2226 : loss : 0.050033, loss_ce: 0.025853
2022-01-09 01:36:44,240 iteration 2227 : loss : 0.063288, loss_ce: 0.017609
 33%|████████▊                  | 131/400 [1:00:13<2:05:47, 28.06s/it]2022-01-09 01:36:45,873 iteration 2228 : loss : 0.052181, loss_ce: 0.024775
2022-01-09 01:36:47,362 iteration 2229 : loss : 0.051344, loss_ce: 0.017726
2022-01-09 01:36:49,028 iteration 2230 : loss : 0.059944, loss_ce: 0.023325
2022-01-09 01:36:50,483 iteration 2231 : loss : 0.045545, loss_ce: 0.023315
2022-01-09 01:36:51,936 iteration 2232 : loss : 0.039457, loss_ce: 0.018997
2022-01-09 01:36:53,668 iteration 2233 : loss : 0.071705, loss_ce: 0.019870
2022-01-09 01:36:55,187 iteration 2234 : loss : 0.046941, loss_ce: 0.012729
2022-01-09 01:36:56,670 iteration 2235 : loss : 0.039262, loss_ce: 0.015744
2022-01-09 01:36:58,132 iteration 2236 : loss : 0.034911, loss_ce: 0.014012
2022-01-09 01:36:59,649 iteration 2237 : loss : 0.038902, loss_ce: 0.017182
2022-01-09 01:37:01,077 iteration 2238 : loss : 0.039510, loss_ce: 0.011496
2022-01-09 01:37:02,547 iteration 2239 : loss : 0.027413, loss_ce: 0.008568
2022-01-09 01:37:03,930 iteration 2240 : loss : 0.050081, loss_ce: 0.013581
2022-01-09 01:37:05,398 iteration 2241 : loss : 0.040833, loss_ce: 0.013945
2022-01-09 01:37:06,957 iteration 2242 : loss : 0.039873, loss_ce: 0.013475
2022-01-09 01:37:08,488 iteration 2243 : loss : 0.062297, loss_ce: 0.027773
2022-01-09 01:37:09,950 iteration 2244 : loss : 0.029560, loss_ce: 0.010332
 33%|████████▉                  | 132/400 [1:00:38<2:02:10, 27.35s/it]2022-01-09 01:37:11,484 iteration 2245 : loss : 0.028873, loss_ce: 0.009459
2022-01-09 01:37:12,840 iteration 2246 : loss : 0.027468, loss_ce: 0.008137
2022-01-09 01:37:14,317 iteration 2247 : loss : 0.043316, loss_ce: 0.019642
2022-01-09 01:37:15,880 iteration 2248 : loss : 0.054263, loss_ce: 0.024610
2022-01-09 01:37:17,347 iteration 2249 : loss : 0.034113, loss_ce: 0.014620
2022-01-09 01:37:18,817 iteration 2250 : loss : 0.037880, loss_ce: 0.013223
2022-01-09 01:37:20,322 iteration 2251 : loss : 0.045618, loss_ce: 0.020326
2022-01-09 01:37:21,793 iteration 2252 : loss : 0.051690, loss_ce: 0.019355
2022-01-09 01:37:23,388 iteration 2253 : loss : 0.037845, loss_ce: 0.014767
2022-01-09 01:37:24,931 iteration 2254 : loss : 0.036111, loss_ce: 0.015305
2022-01-09 01:37:26,395 iteration 2255 : loss : 0.037980, loss_ce: 0.013282
2022-01-09 01:37:27,842 iteration 2256 : loss : 0.033614, loss_ce: 0.013762
2022-01-09 01:37:29,242 iteration 2257 : loss : 0.062734, loss_ce: 0.021506
2022-01-09 01:37:30,642 iteration 2258 : loss : 0.057934, loss_ce: 0.019904
2022-01-09 01:37:32,165 iteration 2259 : loss : 0.037197, loss_ce: 0.015409
2022-01-09 01:37:33,541 iteration 2260 : loss : 0.055753, loss_ce: 0.022225
2022-01-09 01:37:35,026 iteration 2261 : loss : 0.045229, loss_ce: 0.019019
 33%|████████▉                  | 133/400 [1:01:03<1:58:41, 26.67s/it]2022-01-09 01:37:36,487 iteration 2262 : loss : 0.040776, loss_ce: 0.021278
2022-01-09 01:37:38,036 iteration 2263 : loss : 0.056805, loss_ce: 0.021170
2022-01-09 01:37:39,626 iteration 2264 : loss : 0.042672, loss_ce: 0.010860
2022-01-09 01:37:41,135 iteration 2265 : loss : 0.035155, loss_ce: 0.014117
2022-01-09 01:37:42,565 iteration 2266 : loss : 0.038886, loss_ce: 0.015401
2022-01-09 01:37:44,002 iteration 2267 : loss : 0.031611, loss_ce: 0.011020
2022-01-09 01:37:45,470 iteration 2268 : loss : 0.044617, loss_ce: 0.018007
2022-01-09 01:37:46,879 iteration 2269 : loss : 0.044355, loss_ce: 0.025206
2022-01-09 01:37:48,337 iteration 2270 : loss : 0.044814, loss_ce: 0.016166
2022-01-09 01:37:49,763 iteration 2271 : loss : 0.033010, loss_ce: 0.012057
2022-01-09 01:37:51,210 iteration 2272 : loss : 0.052532, loss_ce: 0.026454
2022-01-09 01:37:52,706 iteration 2273 : loss : 0.030157, loss_ce: 0.009845
2022-01-09 01:37:54,198 iteration 2274 : loss : 0.038213, loss_ce: 0.010956
2022-01-09 01:37:55,742 iteration 2275 : loss : 0.044870, loss_ce: 0.014825
2022-01-09 01:37:57,134 iteration 2276 : loss : 0.031164, loss_ce: 0.013197
2022-01-09 01:37:58,609 iteration 2277 : loss : 0.036361, loss_ce: 0.012188
2022-01-09 01:38:00,010 iteration 2278 : loss : 0.032545, loss_ce: 0.011405
 34%|█████████                  | 134/400 [1:01:28<1:55:59, 26.16s/it]2022-01-09 01:38:01,450 iteration 2279 : loss : 0.063295, loss_ce: 0.024125
2022-01-09 01:38:02,916 iteration 2280 : loss : 0.030251, loss_ce: 0.013528
2022-01-09 01:38:04,377 iteration 2281 : loss : 0.055206, loss_ce: 0.013216
2022-01-09 01:38:05,877 iteration 2282 : loss : 0.036643, loss_ce: 0.011561
2022-01-09 01:38:07,443 iteration 2283 : loss : 0.040446, loss_ce: 0.018192
2022-01-09 01:38:08,875 iteration 2284 : loss : 0.049238, loss_ce: 0.022933
2022-01-09 01:38:10,288 iteration 2285 : loss : 0.030589, loss_ce: 0.012430
2022-01-09 01:38:11,736 iteration 2286 : loss : 0.035128, loss_ce: 0.011225
2022-01-09 01:38:13,189 iteration 2287 : loss : 0.034524, loss_ce: 0.012333
2022-01-09 01:38:14,700 iteration 2288 : loss : 0.035913, loss_ce: 0.012635
2022-01-09 01:38:16,101 iteration 2289 : loss : 0.038588, loss_ce: 0.010490
2022-01-09 01:38:17,636 iteration 2290 : loss : 0.047463, loss_ce: 0.024301
2022-01-09 01:38:19,095 iteration 2291 : loss : 0.029700, loss_ce: 0.010889
2022-01-09 01:38:20,533 iteration 2292 : loss : 0.042615, loss_ce: 0.020073
2022-01-09 01:38:21,886 iteration 2293 : loss : 0.043504, loss_ce: 0.022537
2022-01-09 01:38:23,391 iteration 2294 : loss : 0.065152, loss_ce: 0.023622
2022-01-09 01:38:23,392 Training Data Eval:
2022-01-09 01:38:30,724   Average segmentation loss on training set: 0.0306
2022-01-09 01:38:30,724 Validation Data Eval:
2022-01-09 01:38:33,336   Average segmentation loss on validation set: 0.0895
2022-01-09 01:38:34,923 iteration 2295 : loss : 0.041789, loss_ce: 0.017338
 34%|█████████                  | 135/400 [1:02:03<2:07:08, 28.79s/it]2022-01-09 01:38:36,346 iteration 2296 : loss : 0.027535, loss_ce: 0.011200
2022-01-09 01:38:37,794 iteration 2297 : loss : 0.034709, loss_ce: 0.015650
2022-01-09 01:38:39,241 iteration 2298 : loss : 0.029681, loss_ce: 0.011953
2022-01-09 01:38:40,807 iteration 2299 : loss : 0.048818, loss_ce: 0.023038
2022-01-09 01:38:42,234 iteration 2300 : loss : 0.034575, loss_ce: 0.014273
2022-01-09 01:38:43,807 iteration 2301 : loss : 0.033033, loss_ce: 0.009036
2022-01-09 01:38:45,336 iteration 2302 : loss : 0.056808, loss_ce: 0.018079
2022-01-09 01:38:46,825 iteration 2303 : loss : 0.058121, loss_ce: 0.020727
2022-01-09 01:38:48,412 iteration 2304 : loss : 0.048416, loss_ce: 0.021027
2022-01-09 01:38:49,933 iteration 2305 : loss : 0.042117, loss_ce: 0.016966
2022-01-09 01:38:51,530 iteration 2306 : loss : 0.039363, loss_ce: 0.020340
2022-01-09 01:38:52,979 iteration 2307 : loss : 0.075362, loss_ce: 0.030229
2022-01-09 01:38:54,386 iteration 2308 : loss : 0.049969, loss_ce: 0.018709
2022-01-09 01:38:55,821 iteration 2309 : loss : 0.043316, loss_ce: 0.020944
2022-01-09 01:38:57,373 iteration 2310 : loss : 0.051561, loss_ce: 0.020333
2022-01-09 01:38:58,720 iteration 2311 : loss : 0.046226, loss_ce: 0.015274
2022-01-09 01:39:00,188 iteration 2312 : loss : 0.056918, loss_ce: 0.019055
 34%|█████████▏                 | 136/400 [1:02:29<2:01:59, 27.73s/it]2022-01-09 01:39:01,765 iteration 2313 : loss : 0.041232, loss_ce: 0.015326
2022-01-09 01:39:03,096 iteration 2314 : loss : 0.029373, loss_ce: 0.009764
2022-01-09 01:39:04,499 iteration 2315 : loss : 0.029034, loss_ce: 0.010144
2022-01-09 01:39:06,064 iteration 2316 : loss : 0.036518, loss_ce: 0.015907
2022-01-09 01:39:07,444 iteration 2317 : loss : 0.034167, loss_ce: 0.011609
2022-01-09 01:39:08,874 iteration 2318 : loss : 0.040578, loss_ce: 0.018312
2022-01-09 01:39:10,337 iteration 2319 : loss : 0.044647, loss_ce: 0.018011
2022-01-09 01:39:11,899 iteration 2320 : loss : 0.048384, loss_ce: 0.022674
2022-01-09 01:39:13,359 iteration 2321 : loss : 0.044944, loss_ce: 0.018225
2022-01-09 01:39:14,840 iteration 2322 : loss : 0.037766, loss_ce: 0.017147
2022-01-09 01:39:16,190 iteration 2323 : loss : 0.030637, loss_ce: 0.011880
2022-01-09 01:39:17,625 iteration 2324 : loss : 0.031260, loss_ce: 0.013716
2022-01-09 01:39:18,931 iteration 2325 : loss : 0.035229, loss_ce: 0.015733
2022-01-09 01:39:20,368 iteration 2326 : loss : 0.054571, loss_ce: 0.013383
2022-01-09 01:39:21,774 iteration 2327 : loss : 0.039476, loss_ce: 0.014923
2022-01-09 01:39:23,225 iteration 2328 : loss : 0.053483, loss_ce: 0.022702
2022-01-09 01:39:24,615 iteration 2329 : loss : 0.036252, loss_ce: 0.013355
 34%|█████████▏                 | 137/400 [1:02:53<1:57:13, 26.74s/it]2022-01-09 01:39:26,154 iteration 2330 : loss : 0.045993, loss_ce: 0.016440
2022-01-09 01:39:27,552 iteration 2331 : loss : 0.033292, loss_ce: 0.012431
2022-01-09 01:39:29,138 iteration 2332 : loss : 0.038435, loss_ce: 0.013369
2022-01-09 01:39:30,624 iteration 2333 : loss : 0.063114, loss_ce: 0.032335
2022-01-09 01:39:32,273 iteration 2334 : loss : 0.037937, loss_ce: 0.015082
2022-01-09 01:39:33,893 iteration 2335 : loss : 0.040882, loss_ce: 0.019043
2022-01-09 01:39:35,506 iteration 2336 : loss : 0.031358, loss_ce: 0.013774
2022-01-09 01:39:36,990 iteration 2337 : loss : 0.063749, loss_ce: 0.025259
2022-01-09 01:39:38,502 iteration 2338 : loss : 0.049586, loss_ce: 0.018800
2022-01-09 01:39:40,007 iteration 2339 : loss : 0.050405, loss_ce: 0.016757
2022-01-09 01:39:41,411 iteration 2340 : loss : 0.034894, loss_ce: 0.014302
2022-01-09 01:39:42,914 iteration 2341 : loss : 0.037957, loss_ce: 0.014949
2022-01-09 01:39:44,426 iteration 2342 : loss : 0.041204, loss_ce: 0.017922
2022-01-09 01:39:46,073 iteration 2343 : loss : 0.032880, loss_ce: 0.011345
2022-01-09 01:39:47,486 iteration 2344 : loss : 0.061852, loss_ce: 0.019703
2022-01-09 01:39:48,912 iteration 2345 : loss : 0.055491, loss_ce: 0.030019
2022-01-09 01:39:50,360 iteration 2346 : loss : 0.049878, loss_ce: 0.019694
 34%|█████████▎                 | 138/400 [1:03:19<1:55:28, 26.44s/it]2022-01-09 01:39:52,020 iteration 2347 : loss : 0.040913, loss_ce: 0.016428
2022-01-09 01:39:53,512 iteration 2348 : loss : 0.038188, loss_ce: 0.014145
2022-01-09 01:39:54,948 iteration 2349 : loss : 0.035542, loss_ce: 0.014724
2022-01-09 01:39:56,427 iteration 2350 : loss : 0.054271, loss_ce: 0.016905
2022-01-09 01:39:58,029 iteration 2351 : loss : 0.071893, loss_ce: 0.038099
2022-01-09 01:39:59,480 iteration 2352 : loss : 0.032789, loss_ce: 0.015295
2022-01-09 01:40:00,944 iteration 2353 : loss : 0.029566, loss_ce: 0.012367
2022-01-09 01:40:02,399 iteration 2354 : loss : 0.023282, loss_ce: 0.010828
2022-01-09 01:40:03,876 iteration 2355 : loss : 0.044886, loss_ce: 0.016514
2022-01-09 01:40:05,456 iteration 2356 : loss : 0.042496, loss_ce: 0.015931
2022-01-09 01:40:06,984 iteration 2357 : loss : 0.051788, loss_ce: 0.014347
2022-01-09 01:40:08,516 iteration 2358 : loss : 0.039776, loss_ce: 0.017434
2022-01-09 01:40:09,988 iteration 2359 : loss : 0.058664, loss_ce: 0.022468
2022-01-09 01:40:11,427 iteration 2360 : loss : 0.047481, loss_ce: 0.013313
2022-01-09 01:40:12,904 iteration 2361 : loss : 0.035203, loss_ce: 0.012526
2022-01-09 01:40:14,553 iteration 2362 : loss : 0.078640, loss_ce: 0.032871
2022-01-09 01:40:16,056 iteration 2363 : loss : 0.038999, loss_ce: 0.016441
 35%|█████████▍                 | 139/400 [1:03:44<1:54:03, 26.22s/it]2022-01-09 01:40:17,720 iteration 2364 : loss : 0.029264, loss_ce: 0.010710
2022-01-09 01:40:19,323 iteration 2365 : loss : 0.040982, loss_ce: 0.014509
2022-01-09 01:40:20,875 iteration 2366 : loss : 0.047997, loss_ce: 0.018448
2022-01-09 01:40:22,307 iteration 2367 : loss : 0.042528, loss_ce: 0.010708
2022-01-09 01:40:23,765 iteration 2368 : loss : 0.028913, loss_ce: 0.011651
2022-01-09 01:40:25,309 iteration 2369 : loss : 0.033562, loss_ce: 0.010905
2022-01-09 01:40:26,801 iteration 2370 : loss : 0.042812, loss_ce: 0.018404
2022-01-09 01:40:28,340 iteration 2371 : loss : 0.030389, loss_ce: 0.011581
2022-01-09 01:40:29,781 iteration 2372 : loss : 0.034873, loss_ce: 0.011342
2022-01-09 01:40:31,244 iteration 2373 : loss : 0.041582, loss_ce: 0.014445
2022-01-09 01:40:32,660 iteration 2374 : loss : 0.040749, loss_ce: 0.016639
2022-01-09 01:40:34,083 iteration 2375 : loss : 0.040835, loss_ce: 0.015365
2022-01-09 01:40:35,566 iteration 2376 : loss : 0.043875, loss_ce: 0.018714
2022-01-09 01:40:37,067 iteration 2377 : loss : 0.036571, loss_ce: 0.016409
2022-01-09 01:40:38,549 iteration 2378 : loss : 0.027355, loss_ce: 0.010722
2022-01-09 01:40:40,067 iteration 2379 : loss : 0.034646, loss_ce: 0.014785
2022-01-09 01:40:40,067 Training Data Eval:
2022-01-09 01:40:47,442   Average segmentation loss on training set: 0.0257
2022-01-09 01:40:47,443 Validation Data Eval:
2022-01-09 01:40:50,037   Average segmentation loss on validation set: 0.0846
2022-01-09 01:40:51,497 iteration 2380 : loss : 0.028830, loss_ce: 0.013624
 35%|█████████▍                 | 140/400 [1:04:20<2:05:35, 28.98s/it]2022-01-09 01:40:52,985 iteration 2381 : loss : 0.037475, loss_ce: 0.013124
2022-01-09 01:40:54,507 iteration 2382 : loss : 0.043961, loss_ce: 0.016243
2022-01-09 01:40:55,925 iteration 2383 : loss : 0.028744, loss_ce: 0.009800
2022-01-09 01:40:57,454 iteration 2384 : loss : 0.043487, loss_ce: 0.015316
2022-01-09 01:40:59,008 iteration 2385 : loss : 0.047527, loss_ce: 0.018785
2022-01-09 01:41:00,467 iteration 2386 : loss : 0.035040, loss_ce: 0.010728
2022-01-09 01:41:01,891 iteration 2387 : loss : 0.039898, loss_ce: 0.013431
2022-01-09 01:41:03,367 iteration 2388 : loss : 0.035786, loss_ce: 0.015538
2022-01-09 01:41:04,764 iteration 2389 : loss : 0.018600, loss_ce: 0.006224
2022-01-09 01:41:06,259 iteration 2390 : loss : 0.031950, loss_ce: 0.016020
2022-01-09 01:41:07,709 iteration 2391 : loss : 0.044889, loss_ce: 0.026974
2022-01-09 01:41:09,215 iteration 2392 : loss : 0.046059, loss_ce: 0.016722
2022-01-09 01:41:10,716 iteration 2393 : loss : 0.041233, loss_ce: 0.014572
2022-01-09 01:41:12,055 iteration 2394 : loss : 0.035866, loss_ce: 0.012023
2022-01-09 01:41:13,581 iteration 2395 : loss : 0.053339, loss_ce: 0.027473
2022-01-09 01:41:14,981 iteration 2396 : loss : 0.026446, loss_ce: 0.011851
2022-01-09 01:41:16,474 iteration 2397 : loss : 0.035316, loss_ce: 0.013577
 35%|█████████▌                 | 141/400 [1:04:45<1:59:56, 27.78s/it]2022-01-09 01:41:17,992 iteration 2398 : loss : 0.040616, loss_ce: 0.013987
2022-01-09 01:41:19,411 iteration 2399 : loss : 0.033392, loss_ce: 0.011804
2022-01-09 01:41:20,948 iteration 2400 : loss : 0.034624, loss_ce: 0.016576
2022-01-09 01:41:22,418 iteration 2401 : loss : 0.027586, loss_ce: 0.009549
2022-01-09 01:41:23,952 iteration 2402 : loss : 0.040669, loss_ce: 0.022094
2022-01-09 01:41:25,367 iteration 2403 : loss : 0.040846, loss_ce: 0.018257
2022-01-09 01:41:26,757 iteration 2404 : loss : 0.033687, loss_ce: 0.013511
2022-01-09 01:41:28,108 iteration 2405 : loss : 0.028743, loss_ce: 0.010513
2022-01-09 01:41:29,741 iteration 2406 : loss : 0.052055, loss_ce: 0.017520
2022-01-09 01:41:31,122 iteration 2407 : loss : 0.034391, loss_ce: 0.017647
2022-01-09 01:41:32,502 iteration 2408 : loss : 0.024767, loss_ce: 0.010527
2022-01-09 01:41:34,027 iteration 2409 : loss : 0.031574, loss_ce: 0.010656
2022-01-09 01:41:35,564 iteration 2410 : loss : 0.035570, loss_ce: 0.015970
2022-01-09 01:41:37,073 iteration 2411 : loss : 0.040957, loss_ce: 0.011883
2022-01-09 01:41:38,480 iteration 2412 : loss : 0.022759, loss_ce: 0.009326
2022-01-09 01:41:40,038 iteration 2413 : loss : 0.042776, loss_ce: 0.017956
2022-01-09 01:41:41,515 iteration 2414 : loss : 0.030649, loss_ce: 0.011502
 36%|█████████▌                 | 142/400 [1:05:10<1:55:55, 26.96s/it]2022-01-09 01:41:43,070 iteration 2415 : loss : 0.041366, loss_ce: 0.013832
2022-01-09 01:41:44,510 iteration 2416 : loss : 0.029077, loss_ce: 0.010725
2022-01-09 01:41:45,967 iteration 2417 : loss : 0.041165, loss_ce: 0.013366
2022-01-09 01:41:47,544 iteration 2418 : loss : 0.046536, loss_ce: 0.016193
2022-01-09 01:41:49,028 iteration 2419 : loss : 0.056684, loss_ce: 0.026208
2022-01-09 01:41:50,639 iteration 2420 : loss : 0.047949, loss_ce: 0.024957
2022-01-09 01:41:52,218 iteration 2421 : loss : 0.055087, loss_ce: 0.019067
2022-01-09 01:41:53,763 iteration 2422 : loss : 0.036874, loss_ce: 0.013857
2022-01-09 01:41:55,232 iteration 2423 : loss : 0.033212, loss_ce: 0.011314
2022-01-09 01:41:56,677 iteration 2424 : loss : 0.034980, loss_ce: 0.014964
2022-01-09 01:41:58,282 iteration 2425 : loss : 0.036518, loss_ce: 0.010486
2022-01-09 01:41:59,730 iteration 2426 : loss : 0.043065, loss_ce: 0.013930
2022-01-09 01:42:01,075 iteration 2427 : loss : 0.032073, loss_ce: 0.015033
2022-01-09 01:42:02,485 iteration 2428 : loss : 0.037057, loss_ce: 0.010756
2022-01-09 01:42:03,896 iteration 2429 : loss : 0.041063, loss_ce: 0.015381
2022-01-09 01:42:05,315 iteration 2430 : loss : 0.020410, loss_ce: 0.007515
2022-01-09 01:42:06,776 iteration 2431 : loss : 0.036560, loss_ce: 0.018908
 36%|█████████▋                 | 143/400 [1:05:35<1:53:17, 26.45s/it]2022-01-09 01:42:08,246 iteration 2432 : loss : 0.040907, loss_ce: 0.014361
2022-01-09 01:42:09,683 iteration 2433 : loss : 0.030185, loss_ce: 0.012438
2022-01-09 01:42:11,144 iteration 2434 : loss : 0.036524, loss_ce: 0.011670
2022-01-09 01:42:12,720 iteration 2435 : loss : 0.054357, loss_ce: 0.031039
2022-01-09 01:42:14,078 iteration 2436 : loss : 0.032859, loss_ce: 0.013404
2022-01-09 01:42:15,596 iteration 2437 : loss : 0.029670, loss_ce: 0.009920
2022-01-09 01:42:17,137 iteration 2438 : loss : 0.050299, loss_ce: 0.014624
2022-01-09 01:42:18,653 iteration 2439 : loss : 0.038937, loss_ce: 0.011670
2022-01-09 01:42:20,193 iteration 2440 : loss : 0.043885, loss_ce: 0.016474
2022-01-09 01:42:21,644 iteration 2441 : loss : 0.034564, loss_ce: 0.011136
2022-01-09 01:42:23,087 iteration 2442 : loss : 0.041603, loss_ce: 0.018281
2022-01-09 01:42:24,480 iteration 2443 : loss : 0.032990, loss_ce: 0.013494
2022-01-09 01:42:25,988 iteration 2444 : loss : 0.053659, loss_ce: 0.025566
2022-01-09 01:42:27,485 iteration 2445 : loss : 0.035877, loss_ce: 0.014740
2022-01-09 01:42:29,102 iteration 2446 : loss : 0.043992, loss_ce: 0.018742
2022-01-09 01:42:30,490 iteration 2447 : loss : 0.029845, loss_ce: 0.010086
2022-01-09 01:42:31,995 iteration 2448 : loss : 0.037011, loss_ce: 0.014204
 36%|█████████▋                 | 144/400 [1:06:00<1:51:16, 26.08s/it]2022-01-09 01:42:33,413 iteration 2449 : loss : 0.038398, loss_ce: 0.014828
2022-01-09 01:42:34,795 iteration 2450 : loss : 0.029345, loss_ce: 0.016596
2022-01-09 01:42:36,222 iteration 2451 : loss : 0.037853, loss_ce: 0.013017
2022-01-09 01:42:37,612 iteration 2452 : loss : 0.036428, loss_ce: 0.013419
2022-01-09 01:42:39,084 iteration 2453 : loss : 0.042541, loss_ce: 0.015945
2022-01-09 01:42:40,516 iteration 2454 : loss : 0.036709, loss_ce: 0.013089
2022-01-09 01:42:41,915 iteration 2455 : loss : 0.040174, loss_ce: 0.019742
2022-01-09 01:42:43,289 iteration 2456 : loss : 0.032545, loss_ce: 0.015497
2022-01-09 01:42:44,712 iteration 2457 : loss : 0.030593, loss_ce: 0.011982
2022-01-09 01:42:46,101 iteration 2458 : loss : 0.037894, loss_ce: 0.014927
2022-01-09 01:42:47,437 iteration 2459 : loss : 0.025703, loss_ce: 0.010039
2022-01-09 01:42:48,849 iteration 2460 : loss : 0.041205, loss_ce: 0.010700
2022-01-09 01:42:50,393 iteration 2461 : loss : 0.031685, loss_ce: 0.009669
2022-01-09 01:42:51,880 iteration 2462 : loss : 0.042423, loss_ce: 0.014293
2022-01-09 01:42:53,311 iteration 2463 : loss : 0.033249, loss_ce: 0.014248
2022-01-09 01:42:54,799 iteration 2464 : loss : 0.031476, loss_ce: 0.012056
2022-01-09 01:42:54,800 Training Data Eval:
2022-01-09 01:43:02,079   Average segmentation loss on training set: 0.0258
2022-01-09 01:43:02,079 Validation Data Eval:
2022-01-09 01:43:04,573   Average segmentation loss on validation set: 0.0894
2022-01-09 01:43:06,007 iteration 2465 : loss : 0.034501, loss_ce: 0.012788
 36%|█████████▊                 | 145/400 [1:06:34<2:00:57, 28.46s/it]2022-01-09 01:43:07,611 iteration 2466 : loss : 0.038547, loss_ce: 0.015361
2022-01-09 01:43:09,169 iteration 2467 : loss : 0.035591, loss_ce: 0.014566
2022-01-09 01:43:10,605 iteration 2468 : loss : 0.031748, loss_ce: 0.013092
2022-01-09 01:43:12,002 iteration 2469 : loss : 0.024314, loss_ce: 0.010480
2022-01-09 01:43:13,445 iteration 2470 : loss : 0.034051, loss_ce: 0.011716
2022-01-09 01:43:15,017 iteration 2471 : loss : 0.040972, loss_ce: 0.015677
2022-01-09 01:43:16,424 iteration 2472 : loss : 0.043089, loss_ce: 0.013974
2022-01-09 01:43:17,956 iteration 2473 : loss : 0.037851, loss_ce: 0.015159
2022-01-09 01:43:19,365 iteration 2474 : loss : 0.035524, loss_ce: 0.012767
2022-01-09 01:43:20,831 iteration 2475 : loss : 0.029972, loss_ce: 0.011936
2022-01-09 01:43:22,219 iteration 2476 : loss : 0.036703, loss_ce: 0.017223
2022-01-09 01:43:23,797 iteration 2477 : loss : 0.041041, loss_ce: 0.016501
2022-01-09 01:43:25,310 iteration 2478 : loss : 0.033795, loss_ce: 0.013374
2022-01-09 01:43:26,659 iteration 2479 : loss : 0.030308, loss_ce: 0.010586
2022-01-09 01:43:28,129 iteration 2480 : loss : 0.051319, loss_ce: 0.017679
2022-01-09 01:43:29,666 iteration 2481 : loss : 0.027908, loss_ce: 0.009492
2022-01-09 01:43:31,091 iteration 2482 : loss : 0.024345, loss_ce: 0.011498
 36%|█████████▊                 | 146/400 [1:06:59<1:56:11, 27.45s/it]2022-01-09 01:43:32,727 iteration 2483 : loss : 0.042270, loss_ce: 0.018185
2022-01-09 01:43:34,224 iteration 2484 : loss : 0.038536, loss_ce: 0.008875
2022-01-09 01:43:35,630 iteration 2485 : loss : 0.032305, loss_ce: 0.014004
2022-01-09 01:43:37,078 iteration 2486 : loss : 0.031069, loss_ce: 0.012919
2022-01-09 01:43:38,512 iteration 2487 : loss : 0.044140, loss_ce: 0.015592
2022-01-09 01:43:40,041 iteration 2488 : loss : 0.032375, loss_ce: 0.013733
2022-01-09 01:43:41,477 iteration 2489 : loss : 0.030193, loss_ce: 0.013123
2022-01-09 01:43:42,998 iteration 2490 : loss : 0.038348, loss_ce: 0.016464
2022-01-09 01:43:44,565 iteration 2491 : loss : 0.036355, loss_ce: 0.015264
2022-01-09 01:43:45,905 iteration 2492 : loss : 0.028965, loss_ce: 0.010610
2022-01-09 01:43:47,391 iteration 2493 : loss : 0.038662, loss_ce: 0.015832
2022-01-09 01:43:48,822 iteration 2494 : loss : 0.044658, loss_ce: 0.013493
2022-01-09 01:43:50,249 iteration 2495 : loss : 0.043544, loss_ce: 0.020836
2022-01-09 01:43:51,952 iteration 2496 : loss : 0.059668, loss_ce: 0.018759
2022-01-09 01:43:53,475 iteration 2497 : loss : 0.046025, loss_ce: 0.015816
2022-01-09 01:43:55,019 iteration 2498 : loss : 0.031768, loss_ce: 0.013368
2022-01-09 01:43:56,402 iteration 2499 : loss : 0.031748, loss_ce: 0.008672
 37%|█████████▉                 | 147/400 [1:07:25<1:53:01, 26.81s/it]2022-01-09 01:43:57,898 iteration 2500 : loss : 0.038205, loss_ce: 0.015219
2022-01-09 01:43:59,341 iteration 2501 : loss : 0.030751, loss_ce: 0.008703
2022-01-09 01:44:00,910 iteration 2502 : loss : 0.048843, loss_ce: 0.021217
2022-01-09 01:44:02,296 iteration 2503 : loss : 0.032880, loss_ce: 0.012869
2022-01-09 01:44:03,705 iteration 2504 : loss : 0.036719, loss_ce: 0.015994
2022-01-09 01:44:05,219 iteration 2505 : loss : 0.045377, loss_ce: 0.018161
2022-01-09 01:44:06,651 iteration 2506 : loss : 0.034518, loss_ce: 0.010798
2022-01-09 01:44:08,018 iteration 2507 : loss : 0.041867, loss_ce: 0.020556
2022-01-09 01:44:09,442 iteration 2508 : loss : 0.036779, loss_ce: 0.012825
2022-01-09 01:44:10,878 iteration 2509 : loss : 0.054114, loss_ce: 0.019750
2022-01-09 01:44:12,285 iteration 2510 : loss : 0.032750, loss_ce: 0.015140
2022-01-09 01:44:13,742 iteration 2511 : loss : 0.032358, loss_ce: 0.014457
2022-01-09 01:44:15,229 iteration 2512 : loss : 0.044209, loss_ce: 0.017454
2022-01-09 01:44:16,767 iteration 2513 : loss : 0.053772, loss_ce: 0.015820
2022-01-09 01:44:18,233 iteration 2514 : loss : 0.048053, loss_ce: 0.016514
2022-01-09 01:44:19,682 iteration 2515 : loss : 0.025565, loss_ce: 0.014165
2022-01-09 01:44:21,117 iteration 2516 : loss : 0.026999, loss_ce: 0.012917
 37%|█████████▉                 | 148/400 [1:07:50<1:49:57, 26.18s/it]2022-01-09 01:44:22,604 iteration 2517 : loss : 0.030895, loss_ce: 0.012241
2022-01-09 01:44:24,062 iteration 2518 : loss : 0.050521, loss_ce: 0.017385
2022-01-09 01:44:25,554 iteration 2519 : loss : 0.042217, loss_ce: 0.015363
2022-01-09 01:44:27,015 iteration 2520 : loss : 0.038267, loss_ce: 0.016945
2022-01-09 01:44:28,449 iteration 2521 : loss : 0.033159, loss_ce: 0.012191
2022-01-09 01:44:29,919 iteration 2522 : loss : 0.040002, loss_ce: 0.020377
2022-01-09 01:44:31,448 iteration 2523 : loss : 0.036160, loss_ce: 0.009397
2022-01-09 01:44:32,880 iteration 2524 : loss : 0.034298, loss_ce: 0.015164
2022-01-09 01:44:34,355 iteration 2525 : loss : 0.027886, loss_ce: 0.011454
2022-01-09 01:44:35,850 iteration 2526 : loss : 0.039937, loss_ce: 0.013965
2022-01-09 01:44:37,318 iteration 2527 : loss : 0.037738, loss_ce: 0.013889
2022-01-09 01:44:38,812 iteration 2528 : loss : 0.048298, loss_ce: 0.019349
2022-01-09 01:44:40,287 iteration 2529 : loss : 0.059228, loss_ce: 0.022490
2022-01-09 01:44:41,761 iteration 2530 : loss : 0.042109, loss_ce: 0.020016
2022-01-09 01:44:43,333 iteration 2531 : loss : 0.049177, loss_ce: 0.020682
2022-01-09 01:44:44,781 iteration 2532 : loss : 0.053433, loss_ce: 0.018834
2022-01-09 01:44:46,253 iteration 2533 : loss : 0.054866, loss_ce: 0.014658
 37%|██████████                 | 149/400 [1:08:15<1:48:11, 25.86s/it]2022-01-09 01:44:47,715 iteration 2534 : loss : 0.033166, loss_ce: 0.013747
2022-01-09 01:44:49,316 iteration 2535 : loss : 0.053289, loss_ce: 0.020819
2022-01-09 01:44:50,773 iteration 2536 : loss : 0.029610, loss_ce: 0.014767
2022-01-09 01:44:52,243 iteration 2537 : loss : 0.038028, loss_ce: 0.016236
2022-01-09 01:44:53,614 iteration 2538 : loss : 0.026622, loss_ce: 0.011820
2022-01-09 01:44:55,160 iteration 2539 : loss : 0.032478, loss_ce: 0.013321
2022-01-09 01:44:56,607 iteration 2540 : loss : 0.026526, loss_ce: 0.008656
2022-01-09 01:44:58,080 iteration 2541 : loss : 0.032098, loss_ce: 0.012439
2022-01-09 01:44:59,505 iteration 2542 : loss : 0.021161, loss_ce: 0.008597
2022-01-09 01:45:00,923 iteration 2543 : loss : 0.034109, loss_ce: 0.011676
2022-01-09 01:45:02,405 iteration 2544 : loss : 0.044624, loss_ce: 0.016985
2022-01-09 01:45:04,022 iteration 2545 : loss : 0.031013, loss_ce: 0.011458
2022-01-09 01:45:05,660 iteration 2546 : loss : 0.040386, loss_ce: 0.017175
2022-01-09 01:45:07,047 iteration 2547 : loss : 0.028245, loss_ce: 0.013348
2022-01-09 01:45:08,546 iteration 2548 : loss : 0.030956, loss_ce: 0.014689
2022-01-09 01:45:10,023 iteration 2549 : loss : 0.071740, loss_ce: 0.035111
2022-01-09 01:45:10,023 Training Data Eval:
2022-01-09 01:45:17,332   Average segmentation loss on training set: 0.0236
2022-01-09 01:45:17,333 Validation Data Eval:
2022-01-09 01:45:19,871   Average segmentation loss on validation set: 0.0744
2022-01-09 01:45:21,297 iteration 2550 : loss : 0.034531, loss_ce: 0.010887
 38%|██████████▏                | 150/400 [1:08:50<1:59:15, 28.62s/it]2022-01-09 01:45:22,821 iteration 2551 : loss : 0.031030, loss_ce: 0.015851
2022-01-09 01:45:24,369 iteration 2552 : loss : 0.030077, loss_ce: 0.013015
2022-01-09 01:45:25,758 iteration 2553 : loss : 0.024574, loss_ce: 0.008866
2022-01-09 01:45:27,251 iteration 2554 : loss : 0.040344, loss_ce: 0.014144
2022-01-09 01:45:28,706 iteration 2555 : loss : 0.049292, loss_ce: 0.013317
2022-01-09 01:45:30,243 iteration 2556 : loss : 0.032813, loss_ce: 0.013433
2022-01-09 01:45:31,697 iteration 2557 : loss : 0.038622, loss_ce: 0.018204
2022-01-09 01:45:33,180 iteration 2558 : loss : 0.044978, loss_ce: 0.024713
2022-01-09 01:45:34,750 iteration 2559 : loss : 0.045970, loss_ce: 0.016442
2022-01-09 01:45:36,204 iteration 2560 : loss : 0.034662, loss_ce: 0.018272
2022-01-09 01:45:37,780 iteration 2561 : loss : 0.038436, loss_ce: 0.013945
2022-01-09 01:45:39,276 iteration 2562 : loss : 0.038720, loss_ce: 0.015212
2022-01-09 01:45:40,777 iteration 2563 : loss : 0.039779, loss_ce: 0.017240
2022-01-09 01:45:42,183 iteration 2564 : loss : 0.040768, loss_ce: 0.019494
2022-01-09 01:45:43,586 iteration 2565 : loss : 0.034719, loss_ce: 0.012139
2022-01-09 01:45:45,021 iteration 2566 : loss : 0.032851, loss_ce: 0.011593
2022-01-09 01:45:46,490 iteration 2567 : loss : 0.048742, loss_ce: 0.011342
 38%|██████████▏                | 151/400 [1:09:15<1:54:30, 27.59s/it]2022-01-09 01:45:48,048 iteration 2568 : loss : 0.034148, loss_ce: 0.015415
2022-01-09 01:45:49,481 iteration 2569 : loss : 0.034753, loss_ce: 0.016406
2022-01-09 01:45:50,914 iteration 2570 : loss : 0.034532, loss_ce: 0.013504
2022-01-09 01:45:52,353 iteration 2571 : loss : 0.023168, loss_ce: 0.009060
2022-01-09 01:45:53,727 iteration 2572 : loss : 0.034455, loss_ce: 0.009704
2022-01-09 01:45:55,116 iteration 2573 : loss : 0.036048, loss_ce: 0.014023
2022-01-09 01:45:56,536 iteration 2574 : loss : 0.043473, loss_ce: 0.013178
2022-01-09 01:45:58,078 iteration 2575 : loss : 0.050988, loss_ce: 0.023579
2022-01-09 01:45:59,559 iteration 2576 : loss : 0.037489, loss_ce: 0.016383
2022-01-09 01:46:01,028 iteration 2577 : loss : 0.043698, loss_ce: 0.020566
2022-01-09 01:46:02,492 iteration 2578 : loss : 0.047572, loss_ce: 0.015754
2022-01-09 01:46:03,876 iteration 2579 : loss : 0.049601, loss_ce: 0.012130
2022-01-09 01:46:05,410 iteration 2580 : loss : 0.040596, loss_ce: 0.009327
2022-01-09 01:46:06,922 iteration 2581 : loss : 0.046737, loss_ce: 0.018217
2022-01-09 01:46:08,481 iteration 2582 : loss : 0.036602, loss_ce: 0.013813
2022-01-09 01:46:09,928 iteration 2583 : loss : 0.024676, loss_ce: 0.008988
2022-01-09 01:46:11,270 iteration 2584 : loss : 0.024781, loss_ce: 0.009698
 38%|██████████▎                | 152/400 [1:09:40<1:50:34, 26.75s/it]2022-01-09 01:46:12,796 iteration 2585 : loss : 0.033650, loss_ce: 0.009362
2022-01-09 01:46:14,262 iteration 2586 : loss : 0.039570, loss_ce: 0.017749
2022-01-09 01:46:15,640 iteration 2587 : loss : 0.031677, loss_ce: 0.012739
2022-01-09 01:46:17,070 iteration 2588 : loss : 0.035884, loss_ce: 0.016088
2022-01-09 01:46:18,580 iteration 2589 : loss : 0.032642, loss_ce: 0.012378
2022-01-09 01:46:20,105 iteration 2590 : loss : 0.033045, loss_ce: 0.012421
2022-01-09 01:46:21,519 iteration 2591 : loss : 0.033743, loss_ce: 0.010247
2022-01-09 01:46:22,934 iteration 2592 : loss : 0.038096, loss_ce: 0.013063
2022-01-09 01:46:24,376 iteration 2593 : loss : 0.035103, loss_ce: 0.014650
2022-01-09 01:46:26,011 iteration 2594 : loss : 0.053861, loss_ce: 0.021776
2022-01-09 01:46:27,531 iteration 2595 : loss : 0.054361, loss_ce: 0.019023
2022-01-09 01:46:28,957 iteration 2596 : loss : 0.027448, loss_ce: 0.012435
2022-01-09 01:46:30,354 iteration 2597 : loss : 0.036165, loss_ce: 0.014260
2022-01-09 01:46:31,800 iteration 2598 : loss : 0.028345, loss_ce: 0.010502
2022-01-09 01:46:33,272 iteration 2599 : loss : 0.032949, loss_ce: 0.007824
2022-01-09 01:46:34,749 iteration 2600 : loss : 0.043219, loss_ce: 0.016104
2022-01-09 01:46:36,162 iteration 2601 : loss : 0.027557, loss_ce: 0.011741
 38%|██████████▎                | 153/400 [1:10:05<1:47:49, 26.19s/it]2022-01-09 01:46:37,739 iteration 2602 : loss : 0.027949, loss_ce: 0.013005
2022-01-09 01:46:39,132 iteration 2603 : loss : 0.024593, loss_ce: 0.009324
2022-01-09 01:46:40,594 iteration 2604 : loss : 0.041150, loss_ce: 0.017083
2022-01-09 01:46:42,227 iteration 2605 : loss : 0.028893, loss_ce: 0.009169
2022-01-09 01:46:43,688 iteration 2606 : loss : 0.053544, loss_ce: 0.022437
2022-01-09 01:46:45,166 iteration 2607 : loss : 0.037554, loss_ce: 0.016246
2022-01-09 01:46:46,567 iteration 2608 : loss : 0.037504, loss_ce: 0.018163
2022-01-09 01:46:47,989 iteration 2609 : loss : 0.048660, loss_ce: 0.015929
2022-01-09 01:46:49,614 iteration 2610 : loss : 0.037265, loss_ce: 0.014540
2022-01-09 01:46:51,067 iteration 2611 : loss : 0.030537, loss_ce: 0.010116
2022-01-09 01:46:52,416 iteration 2612 : loss : 0.029201, loss_ce: 0.013305
2022-01-09 01:46:53,854 iteration 2613 : loss : 0.040834, loss_ce: 0.012819
2022-01-09 01:46:55,418 iteration 2614 : loss : 0.036563, loss_ce: 0.012528
2022-01-09 01:46:56,862 iteration 2615 : loss : 0.039675, loss_ce: 0.015678
2022-01-09 01:46:58,322 iteration 2616 : loss : 0.030627, loss_ce: 0.011393
2022-01-09 01:46:59,916 iteration 2617 : loss : 0.031890, loss_ce: 0.008357
2022-01-09 01:47:01,418 iteration 2618 : loss : 0.051445, loss_ce: 0.024559
 38%|██████████▍                | 154/400 [1:10:30<1:46:13, 25.91s/it]2022-01-09 01:47:03,034 iteration 2619 : loss : 0.054769, loss_ce: 0.015377
2022-01-09 01:47:04,480 iteration 2620 : loss : 0.031973, loss_ce: 0.012903
2022-01-09 01:47:06,012 iteration 2621 : loss : 0.039672, loss_ce: 0.014319
2022-01-09 01:47:07,549 iteration 2622 : loss : 0.035409, loss_ce: 0.017499
2022-01-09 01:47:09,098 iteration 2623 : loss : 0.036754, loss_ce: 0.015986
2022-01-09 01:47:10,555 iteration 2624 : loss : 0.036067, loss_ce: 0.012384
2022-01-09 01:47:12,038 iteration 2625 : loss : 0.025926, loss_ce: 0.010609
2022-01-09 01:47:13,620 iteration 2626 : loss : 0.043724, loss_ce: 0.017197
2022-01-09 01:47:15,143 iteration 2627 : loss : 0.043428, loss_ce: 0.011198
2022-01-09 01:47:16,590 iteration 2628 : loss : 0.031133, loss_ce: 0.011202
2022-01-09 01:47:18,175 iteration 2629 : loss : 0.041337, loss_ce: 0.013579
2022-01-09 01:47:19,605 iteration 2630 : loss : 0.024779, loss_ce: 0.008109
2022-01-09 01:47:21,128 iteration 2631 : loss : 0.034140, loss_ce: 0.014899
2022-01-09 01:47:22,651 iteration 2632 : loss : 0.041334, loss_ce: 0.014577
2022-01-09 01:47:24,099 iteration 2633 : loss : 0.033694, loss_ce: 0.014794
2022-01-09 01:47:25,614 iteration 2634 : loss : 0.042030, loss_ce: 0.013382
2022-01-09 01:47:25,614 Training Data Eval:
2022-01-09 01:47:33,195   Average segmentation loss on training set: 0.0269
2022-01-09 01:47:33,196 Validation Data Eval:
2022-01-09 01:47:35,752   Average segmentation loss on validation set: 0.0998
2022-01-09 01:47:37,309 iteration 2635 : loss : 0.035713, loss_ce: 0.015386
 39%|██████████▍                | 155/400 [1:11:06<1:58:01, 28.90s/it]2022-01-09 01:47:38,786 iteration 2636 : loss : 0.028594, loss_ce: 0.008949
2022-01-09 01:47:40,423 iteration 2637 : loss : 0.036705, loss_ce: 0.011644
2022-01-09 01:47:41,896 iteration 2638 : loss : 0.024191, loss_ce: 0.009107
2022-01-09 01:47:43,205 iteration 2639 : loss : 0.026306, loss_ce: 0.009421
2022-01-09 01:47:44,583 iteration 2640 : loss : 0.030756, loss_ce: 0.012640
2022-01-09 01:47:46,136 iteration 2641 : loss : 0.032190, loss_ce: 0.013735
2022-01-09 01:47:47,653 iteration 2642 : loss : 0.054519, loss_ce: 0.016151
2022-01-09 01:47:49,158 iteration 2643 : loss : 0.039924, loss_ce: 0.014520
2022-01-09 01:47:50,542 iteration 2644 : loss : 0.037943, loss_ce: 0.011526
2022-01-09 01:47:51,952 iteration 2645 : loss : 0.028484, loss_ce: 0.014688
2022-01-09 01:47:53,369 iteration 2646 : loss : 0.038004, loss_ce: 0.012374
2022-01-09 01:47:54,889 iteration 2647 : loss : 0.031052, loss_ce: 0.012897
2022-01-09 01:47:56,242 iteration 2648 : loss : 0.022306, loss_ce: 0.010478
2022-01-09 01:47:57,747 iteration 2649 : loss : 0.033340, loss_ce: 0.014926
2022-01-09 01:47:59,249 iteration 2650 : loss : 0.033476, loss_ce: 0.014148
2022-01-09 01:48:00,675 iteration 2651 : loss : 0.045177, loss_ce: 0.014592
2022-01-09 01:48:02,140 iteration 2652 : loss : 0.035482, loss_ce: 0.016300
 39%|██████████▌                | 156/400 [1:11:31<1:52:35, 27.68s/it]2022-01-09 01:48:03,795 iteration 2653 : loss : 0.039493, loss_ce: 0.012745
2022-01-09 01:48:05,257 iteration 2654 : loss : 0.027436, loss_ce: 0.009947
2022-01-09 01:48:06,658 iteration 2655 : loss : 0.026813, loss_ce: 0.009548
2022-01-09 01:48:08,016 iteration 2656 : loss : 0.029043, loss_ce: 0.012673
2022-01-09 01:48:09,403 iteration 2657 : loss : 0.025132, loss_ce: 0.008820
2022-01-09 01:48:10,811 iteration 2658 : loss : 0.032714, loss_ce: 0.012343
2022-01-09 01:48:12,429 iteration 2659 : loss : 0.039116, loss_ce: 0.016455
2022-01-09 01:48:13,883 iteration 2660 : loss : 0.028834, loss_ce: 0.010060
2022-01-09 01:48:15,220 iteration 2661 : loss : 0.030483, loss_ce: 0.011793
2022-01-09 01:48:16,737 iteration 2662 : loss : 0.037856, loss_ce: 0.015532
2022-01-09 01:48:18,125 iteration 2663 : loss : 0.032260, loss_ce: 0.012650
2022-01-09 01:48:19,647 iteration 2664 : loss : 0.031180, loss_ce: 0.010403
2022-01-09 01:48:21,047 iteration 2665 : loss : 0.028706, loss_ce: 0.010740
2022-01-09 01:48:22,518 iteration 2666 : loss : 0.033765, loss_ce: 0.010974
2022-01-09 01:48:24,019 iteration 2667 : loss : 0.031968, loss_ce: 0.010921
2022-01-09 01:48:25,509 iteration 2668 : loss : 0.040303, loss_ce: 0.014202
2022-01-09 01:48:27,028 iteration 2669 : loss : 0.036174, loss_ce: 0.012410
 39%|██████████▌                | 157/400 [1:11:55<1:48:43, 26.85s/it]2022-01-09 01:48:28,587 iteration 2670 : loss : 0.045278, loss_ce: 0.012687
2022-01-09 01:48:30,069 iteration 2671 : loss : 0.032537, loss_ce: 0.009545
2022-01-09 01:48:31,570 iteration 2672 : loss : 0.024546, loss_ce: 0.009103
2022-01-09 01:48:33,109 iteration 2673 : loss : 0.046929, loss_ce: 0.016732
2022-01-09 01:48:34,524 iteration 2674 : loss : 0.037328, loss_ce: 0.020842
2022-01-09 01:48:36,055 iteration 2675 : loss : 0.040069, loss_ce: 0.014197
2022-01-09 01:48:37,542 iteration 2676 : loss : 0.038039, loss_ce: 0.010204
2022-01-09 01:48:38,926 iteration 2677 : loss : 0.030133, loss_ce: 0.013573
2022-01-09 01:48:40,288 iteration 2678 : loss : 0.032887, loss_ce: 0.010655
2022-01-09 01:48:41,773 iteration 2679 : loss : 0.029575, loss_ce: 0.013025
2022-01-09 01:48:43,307 iteration 2680 : loss : 0.032708, loss_ce: 0.013411
2022-01-09 01:48:44,751 iteration 2681 : loss : 0.029606, loss_ce: 0.015347
2022-01-09 01:48:46,132 iteration 2682 : loss : 0.024354, loss_ce: 0.009940
2022-01-09 01:48:47,595 iteration 2683 : loss : 0.037674, loss_ce: 0.020299
2022-01-09 01:48:49,048 iteration 2684 : loss : 0.046481, loss_ce: 0.012954
2022-01-09 01:48:50,507 iteration 2685 : loss : 0.036829, loss_ce: 0.017887
2022-01-09 01:48:51,952 iteration 2686 : loss : 0.028456, loss_ce: 0.010165
 40%|██████████▋                | 158/400 [1:12:20<1:45:57, 26.27s/it]2022-01-09 01:48:53,445 iteration 2687 : loss : 0.021227, loss_ce: 0.009094
2022-01-09 01:48:54,879 iteration 2688 : loss : 0.029548, loss_ce: 0.012005
2022-01-09 01:48:56,186 iteration 2689 : loss : 0.027280, loss_ce: 0.010259
2022-01-09 01:48:57,611 iteration 2690 : loss : 0.031040, loss_ce: 0.013194
2022-01-09 01:48:59,081 iteration 2691 : loss : 0.027480, loss_ce: 0.009037
2022-01-09 01:49:00,525 iteration 2692 : loss : 0.032112, loss_ce: 0.010945
2022-01-09 01:49:01,927 iteration 2693 : loss : 0.038084, loss_ce: 0.014749
2022-01-09 01:49:03,286 iteration 2694 : loss : 0.035499, loss_ce: 0.012439
2022-01-09 01:49:04,794 iteration 2695 : loss : 0.046702, loss_ce: 0.017192
2022-01-09 01:49:06,389 iteration 2696 : loss : 0.043847, loss_ce: 0.017568
2022-01-09 01:49:07,926 iteration 2697 : loss : 0.036471, loss_ce: 0.019338
2022-01-09 01:49:09,306 iteration 2698 : loss : 0.023832, loss_ce: 0.009580
2022-01-09 01:49:10,705 iteration 2699 : loss : 0.028212, loss_ce: 0.009664
2022-01-09 01:49:12,104 iteration 2700 : loss : 0.034877, loss_ce: 0.010478
2022-01-09 01:49:13,560 iteration 2701 : loss : 0.041246, loss_ce: 0.018701
2022-01-09 01:49:15,060 iteration 2702 : loss : 0.040278, loss_ce: 0.008919
2022-01-09 01:49:16,569 iteration 2703 : loss : 0.040520, loss_ce: 0.014675
 40%|██████████▋                | 159/400 [1:12:45<1:43:30, 25.77s/it]2022-01-09 01:49:18,017 iteration 2704 : loss : 0.030365, loss_ce: 0.008791
2022-01-09 01:49:19,395 iteration 2705 : loss : 0.035384, loss_ce: 0.013404
2022-01-09 01:49:20,892 iteration 2706 : loss : 0.048401, loss_ce: 0.026256
2022-01-09 01:49:22,341 iteration 2707 : loss : 0.027710, loss_ce: 0.010090
2022-01-09 01:49:23,767 iteration 2708 : loss : 0.039594, loss_ce: 0.013169
2022-01-09 01:49:25,314 iteration 2709 : loss : 0.042314, loss_ce: 0.013704
2022-01-09 01:49:26,834 iteration 2710 : loss : 0.041892, loss_ce: 0.017971
2022-01-09 01:49:28,254 iteration 2711 : loss : 0.031630, loss_ce: 0.011335
2022-01-09 01:49:29,688 iteration 2712 : loss : 0.034527, loss_ce: 0.013220
2022-01-09 01:49:31,152 iteration 2713 : loss : 0.039331, loss_ce: 0.013083
2022-01-09 01:49:32,606 iteration 2714 : loss : 0.036143, loss_ce: 0.013382
2022-01-09 01:49:34,042 iteration 2715 : loss : 0.036335, loss_ce: 0.019005
2022-01-09 01:49:35,465 iteration 2716 : loss : 0.045683, loss_ce: 0.012564
2022-01-09 01:49:36,837 iteration 2717 : loss : 0.037308, loss_ce: 0.013888
2022-01-09 01:49:38,326 iteration 2718 : loss : 0.033507, loss_ce: 0.012318
2022-01-09 01:49:39,729 iteration 2719 : loss : 0.025631, loss_ce: 0.011715
2022-01-09 01:49:39,730 Training Data Eval:
2022-01-09 01:49:47,008   Average segmentation loss on training set: 0.0221
2022-01-09 01:49:47,009 Validation Data Eval:
2022-01-09 01:49:49,497   Average segmentation loss on validation set: 0.0685
2022-01-09 01:49:50,936 iteration 2720 : loss : 0.030888, loss_ce: 0.014374
 40%|██████████▊                | 160/400 [1:13:19<1:53:23, 28.35s/it]2022-01-09 01:49:52,464 iteration 2721 : loss : 0.035073, loss_ce: 0.012299
2022-01-09 01:49:53,951 iteration 2722 : loss : 0.023853, loss_ce: 0.011102
2022-01-09 01:49:55,509 iteration 2723 : loss : 0.028812, loss_ce: 0.012587
2022-01-09 01:49:56,889 iteration 2724 : loss : 0.033329, loss_ce: 0.013011
2022-01-09 01:49:58,311 iteration 2725 : loss : 0.043527, loss_ce: 0.011985
2022-01-09 01:49:59,855 iteration 2726 : loss : 0.037472, loss_ce: 0.012939
2022-01-09 01:50:01,314 iteration 2727 : loss : 0.035980, loss_ce: 0.012646
2022-01-09 01:50:02,820 iteration 2728 : loss : 0.034257, loss_ce: 0.013447
2022-01-09 01:50:04,317 iteration 2729 : loss : 0.050616, loss_ce: 0.025814
2022-01-09 01:50:05,867 iteration 2730 : loss : 0.035815, loss_ce: 0.013450
2022-01-09 01:50:07,309 iteration 2731 : loss : 0.030909, loss_ce: 0.011450
2022-01-09 01:50:08,643 iteration 2732 : loss : 0.023728, loss_ce: 0.009433
2022-01-09 01:50:10,081 iteration 2733 : loss : 0.025382, loss_ce: 0.011596
2022-01-09 01:50:11,547 iteration 2734 : loss : 0.070056, loss_ce: 0.017674
2022-01-09 01:50:13,013 iteration 2735 : loss : 0.039391, loss_ce: 0.018954
2022-01-09 01:50:14,442 iteration 2736 : loss : 0.038246, loss_ce: 0.013153
2022-01-09 01:50:15,871 iteration 2737 : loss : 0.034702, loss_ce: 0.009847
 40%|██████████▊                | 161/400 [1:13:44<1:48:50, 27.32s/it]2022-01-09 01:50:17,455 iteration 2738 : loss : 0.048836, loss_ce: 0.027449
2022-01-09 01:50:18,842 iteration 2739 : loss : 0.034641, loss_ce: 0.010816
2022-01-09 01:50:20,392 iteration 2740 : loss : 0.046771, loss_ce: 0.018855
2022-01-09 01:50:21,840 iteration 2741 : loss : 0.033863, loss_ce: 0.013478
2022-01-09 01:50:23,328 iteration 2742 : loss : 0.030452, loss_ce: 0.011811
2022-01-09 01:50:24,768 iteration 2743 : loss : 0.033550, loss_ce: 0.013124
2022-01-09 01:50:26,265 iteration 2744 : loss : 0.024946, loss_ce: 0.008539
2022-01-09 01:50:27,700 iteration 2745 : loss : 0.031611, loss_ce: 0.011465
2022-01-09 01:50:29,303 iteration 2746 : loss : 0.064856, loss_ce: 0.017568
2022-01-09 01:50:30,732 iteration 2747 : loss : 0.025471, loss_ce: 0.010696
2022-01-09 01:50:32,262 iteration 2748 : loss : 0.036595, loss_ce: 0.009494
2022-01-09 01:50:33,812 iteration 2749 : loss : 0.035136, loss_ce: 0.013660
2022-01-09 01:50:35,257 iteration 2750 : loss : 0.049880, loss_ce: 0.022163
2022-01-09 01:50:36,708 iteration 2751 : loss : 0.042550, loss_ce: 0.011750
2022-01-09 01:50:38,295 iteration 2752 : loss : 0.036992, loss_ce: 0.013106
2022-01-09 01:50:39,893 iteration 2753 : loss : 0.035571, loss_ce: 0.016627
2022-01-09 01:50:41,367 iteration 2754 : loss : 0.031445, loss_ce: 0.015515
 40%|██████████▉                | 162/400 [1:14:10<1:46:12, 26.77s/it]2022-01-09 01:50:42,720 iteration 2755 : loss : 0.026409, loss_ce: 0.008634
2022-01-09 01:50:44,197 iteration 2756 : loss : 0.034368, loss_ce: 0.014102
2022-01-09 01:50:45,651 iteration 2757 : loss : 0.036623, loss_ce: 0.015920
2022-01-09 01:50:47,101 iteration 2758 : loss : 0.030964, loss_ce: 0.011380
2022-01-09 01:50:48,551 iteration 2759 : loss : 0.036494, loss_ce: 0.015061
2022-01-09 01:50:50,009 iteration 2760 : loss : 0.033314, loss_ce: 0.014037
2022-01-09 01:50:51,515 iteration 2761 : loss : 0.030897, loss_ce: 0.010990
2022-01-09 01:50:52,961 iteration 2762 : loss : 0.028907, loss_ce: 0.012134
2022-01-09 01:50:54,455 iteration 2763 : loss : 0.047613, loss_ce: 0.015596
2022-01-09 01:50:55,831 iteration 2764 : loss : 0.022867, loss_ce: 0.008313
2022-01-09 01:50:57,425 iteration 2765 : loss : 0.026203, loss_ce: 0.008866
2022-01-09 01:50:58,787 iteration 2766 : loss : 0.028467, loss_ce: 0.010383
2022-01-09 01:51:00,256 iteration 2767 : loss : 0.034769, loss_ce: 0.013387
2022-01-09 01:51:01,675 iteration 2768 : loss : 0.038789, loss_ce: 0.012489
2022-01-09 01:51:03,040 iteration 2769 : loss : 0.026238, loss_ce: 0.011903
2022-01-09 01:51:04,443 iteration 2770 : loss : 0.030061, loss_ce: 0.013770
2022-01-09 01:51:05,888 iteration 2771 : loss : 0.024943, loss_ce: 0.010876
 41%|███████████                | 163/400 [1:14:34<1:43:05, 26.10s/it]2022-01-09 01:51:07,321 iteration 2772 : loss : 0.041992, loss_ce: 0.014376
2022-01-09 01:51:08,792 iteration 2773 : loss : 0.032073, loss_ce: 0.012117
2022-01-09 01:51:10,253 iteration 2774 : loss : 0.031851, loss_ce: 0.008009
2022-01-09 01:51:11,733 iteration 2775 : loss : 0.030588, loss_ce: 0.012470
2022-01-09 01:51:13,163 iteration 2776 : loss : 0.037677, loss_ce: 0.013741
2022-01-09 01:51:14,640 iteration 2777 : loss : 0.024166, loss_ce: 0.008759
2022-01-09 01:51:16,038 iteration 2778 : loss : 0.029584, loss_ce: 0.009964
2022-01-09 01:51:17,444 iteration 2779 : loss : 0.029470, loss_ce: 0.012091
2022-01-09 01:51:18,896 iteration 2780 : loss : 0.028141, loss_ce: 0.013563
2022-01-09 01:51:20,508 iteration 2781 : loss : 0.035373, loss_ce: 0.012788
2022-01-09 01:51:22,066 iteration 2782 : loss : 0.035042, loss_ce: 0.016289
2022-01-09 01:51:23,514 iteration 2783 : loss : 0.036793, loss_ce: 0.012631
2022-01-09 01:51:24,835 iteration 2784 : loss : 0.043635, loss_ce: 0.013707
2022-01-09 01:51:26,237 iteration 2785 : loss : 0.027319, loss_ce: 0.012590
2022-01-09 01:51:27,641 iteration 2786 : loss : 0.024505, loss_ce: 0.009971
2022-01-09 01:51:29,056 iteration 2787 : loss : 0.029272, loss_ce: 0.011375
2022-01-09 01:51:30,690 iteration 2788 : loss : 0.048748, loss_ce: 0.021024
 41%|███████████                | 164/400 [1:14:59<1:41:07, 25.71s/it]2022-01-09 01:51:32,236 iteration 2789 : loss : 0.025011, loss_ce: 0.007875
2022-01-09 01:51:33,807 iteration 2790 : loss : 0.045712, loss_ce: 0.022457
2022-01-09 01:51:35,182 iteration 2791 : loss : 0.022800, loss_ce: 0.006856
2022-01-09 01:51:36,617 iteration 2792 : loss : 0.034285, loss_ce: 0.012781
2022-01-09 01:51:38,072 iteration 2793 : loss : 0.036216, loss_ce: 0.012821
2022-01-09 01:51:39,600 iteration 2794 : loss : 0.037799, loss_ce: 0.017415
2022-01-09 01:51:41,039 iteration 2795 : loss : 0.046068, loss_ce: 0.018704
2022-01-09 01:51:42,452 iteration 2796 : loss : 0.043685, loss_ce: 0.015601
2022-01-09 01:51:43,958 iteration 2797 : loss : 0.031828, loss_ce: 0.014389
2022-01-09 01:51:45,439 iteration 2798 : loss : 0.039691, loss_ce: 0.012825
2022-01-09 01:51:47,020 iteration 2799 : loss : 0.048052, loss_ce: 0.018296
2022-01-09 01:51:48,494 iteration 2800 : loss : 0.035421, loss_ce: 0.016818
2022-01-09 01:51:49,831 iteration 2801 : loss : 0.037366, loss_ce: 0.010858
2022-01-09 01:51:51,218 iteration 2802 : loss : 0.021879, loss_ce: 0.007380
2022-01-09 01:51:52,695 iteration 2803 : loss : 0.043450, loss_ce: 0.017966
2022-01-09 01:51:54,033 iteration 2804 : loss : 0.033721, loss_ce: 0.013615
2022-01-09 01:51:54,033 Training Data Eval:
2022-01-09 01:52:01,325   Average segmentation loss on training set: 0.0212
2022-01-09 01:52:01,326 Validation Data Eval:
2022-01-09 01:52:03,900   Average segmentation loss on validation set: 0.0848
2022-01-09 01:52:05,511 iteration 2805 : loss : 0.055454, loss_ce: 0.021621
 41%|███████████▏               | 165/400 [1:15:34<1:51:24, 28.45s/it]2022-01-09 01:52:07,152 iteration 2806 : loss : 0.030553, loss_ce: 0.012828
2022-01-09 01:52:08,733 iteration 2807 : loss : 0.039071, loss_ce: 0.012317
2022-01-09 01:52:10,270 iteration 2808 : loss : 0.050158, loss_ce: 0.018320
2022-01-09 01:52:11,783 iteration 2809 : loss : 0.035245, loss_ce: 0.013037
2022-01-09 01:52:13,182 iteration 2810 : loss : 0.035410, loss_ce: 0.013934
2022-01-09 01:52:14,736 iteration 2811 : loss : 0.028670, loss_ce: 0.013168
2022-01-09 01:52:16,148 iteration 2812 : loss : 0.021323, loss_ce: 0.009037
2022-01-09 01:52:17,730 iteration 2813 : loss : 0.034189, loss_ce: 0.015157
2022-01-09 01:52:19,250 iteration 2814 : loss : 0.030864, loss_ce: 0.011687
2022-01-09 01:52:20,747 iteration 2815 : loss : 0.028753, loss_ce: 0.013046
2022-01-09 01:52:22,244 iteration 2816 : loss : 0.029533, loss_ce: 0.009871
2022-01-09 01:52:23,841 iteration 2817 : loss : 0.054186, loss_ce: 0.011203
2022-01-09 01:52:25,413 iteration 2818 : loss : 0.041702, loss_ce: 0.018207
2022-01-09 01:52:26,870 iteration 2819 : loss : 0.033511, loss_ce: 0.013938
2022-01-09 01:52:28,448 iteration 2820 : loss : 0.041245, loss_ce: 0.011703
2022-01-09 01:52:30,011 iteration 2821 : loss : 0.048680, loss_ce: 0.019437
2022-01-09 01:52:31,512 iteration 2822 : loss : 0.034421, loss_ce: 0.010606
 42%|███████████▏               | 166/400 [1:16:00<1:48:04, 27.71s/it]2022-01-09 01:52:33,190 iteration 2823 : loss : 0.039396, loss_ce: 0.019665
2022-01-09 01:52:34,599 iteration 2824 : loss : 0.032375, loss_ce: 0.016604
2022-01-09 01:52:36,075 iteration 2825 : loss : 0.026374, loss_ce: 0.010255
2022-01-09 01:52:37,673 iteration 2826 : loss : 0.050852, loss_ce: 0.019758
2022-01-09 01:52:39,306 iteration 2827 : loss : 0.059727, loss_ce: 0.020315
2022-01-09 01:52:40,809 iteration 2828 : loss : 0.041097, loss_ce: 0.013269
2022-01-09 01:52:42,274 iteration 2829 : loss : 0.037519, loss_ce: 0.018199
2022-01-09 01:52:43,715 iteration 2830 : loss : 0.040882, loss_ce: 0.013753
2022-01-09 01:52:45,049 iteration 2831 : loss : 0.033364, loss_ce: 0.013942
2022-01-09 01:52:46,683 iteration 2832 : loss : 0.040230, loss_ce: 0.015882
2022-01-09 01:52:48,126 iteration 2833 : loss : 0.031180, loss_ce: 0.011626
2022-01-09 01:52:49,676 iteration 2834 : loss : 0.052678, loss_ce: 0.017878
2022-01-09 01:52:51,185 iteration 2835 : loss : 0.046566, loss_ce: 0.013425
2022-01-09 01:52:52,647 iteration 2836 : loss : 0.037341, loss_ce: 0.014490
2022-01-09 01:52:54,193 iteration 2837 : loss : 0.036733, loss_ce: 0.014370
2022-01-09 01:52:55,572 iteration 2838 : loss : 0.031622, loss_ce: 0.012127
2022-01-09 01:52:57,071 iteration 2839 : loss : 0.037938, loss_ce: 0.014429
 42%|███████████▎               | 167/400 [1:16:25<1:45:06, 27.07s/it]2022-01-09 01:52:58,711 iteration 2840 : loss : 0.041473, loss_ce: 0.016486
2022-01-09 01:53:00,157 iteration 2841 : loss : 0.039979, loss_ce: 0.011684
2022-01-09 01:53:01,599 iteration 2842 : loss : 0.032591, loss_ce: 0.011839
2022-01-09 01:53:03,130 iteration 2843 : loss : 0.043424, loss_ce: 0.012414
2022-01-09 01:53:04,624 iteration 2844 : loss : 0.040355, loss_ce: 0.016144
2022-01-09 01:53:06,081 iteration 2845 : loss : 0.037224, loss_ce: 0.014167
2022-01-09 01:53:07,551 iteration 2846 : loss : 0.026033, loss_ce: 0.009482
2022-01-09 01:53:08,854 iteration 2847 : loss : 0.021327, loss_ce: 0.011538
2022-01-09 01:53:10,332 iteration 2848 : loss : 0.046073, loss_ce: 0.016826
2022-01-09 01:53:11,869 iteration 2849 : loss : 0.042272, loss_ce: 0.013954
2022-01-09 01:53:13,387 iteration 2850 : loss : 0.054789, loss_ce: 0.019646
2022-01-09 01:53:14,922 iteration 2851 : loss : 0.036763, loss_ce: 0.014560
2022-01-09 01:53:16,412 iteration 2852 : loss : 0.058264, loss_ce: 0.016239
2022-01-09 01:53:17,958 iteration 2853 : loss : 0.027837, loss_ce: 0.009927
2022-01-09 01:53:19,466 iteration 2854 : loss : 0.036800, loss_ce: 0.013733
2022-01-09 01:53:20,879 iteration 2855 : loss : 0.030658, loss_ce: 0.013284
2022-01-09 01:53:22,391 iteration 2856 : loss : 0.037257, loss_ce: 0.016709
 42%|███████████▎               | 168/400 [1:16:51<1:42:37, 26.54s/it]2022-01-09 01:53:23,827 iteration 2857 : loss : 0.027377, loss_ce: 0.008814
2022-01-09 01:53:25,303 iteration 2858 : loss : 0.033052, loss_ce: 0.014229
2022-01-09 01:53:26,727 iteration 2859 : loss : 0.025105, loss_ce: 0.010778
2022-01-09 01:53:28,240 iteration 2860 : loss : 0.032743, loss_ce: 0.010785
2022-01-09 01:53:29,590 iteration 2861 : loss : 0.036554, loss_ce: 0.015897
2022-01-09 01:53:31,169 iteration 2862 : loss : 0.077034, loss_ce: 0.024473
2022-01-09 01:53:32,574 iteration 2863 : loss : 0.038872, loss_ce: 0.011047
2022-01-09 01:53:34,078 iteration 2864 : loss : 0.032147, loss_ce: 0.012743
2022-01-09 01:53:35,593 iteration 2865 : loss : 0.057550, loss_ce: 0.021007
2022-01-09 01:53:37,025 iteration 2866 : loss : 0.036053, loss_ce: 0.013265
2022-01-09 01:53:38,631 iteration 2867 : loss : 0.038615, loss_ce: 0.015547
2022-01-09 01:53:40,046 iteration 2868 : loss : 0.041576, loss_ce: 0.014832
2022-01-09 01:53:41,621 iteration 2869 : loss : 0.037133, loss_ce: 0.014295
2022-01-09 01:53:43,112 iteration 2870 : loss : 0.038156, loss_ce: 0.013635
2022-01-09 01:53:44,526 iteration 2871 : loss : 0.027454, loss_ce: 0.010690
2022-01-09 01:53:45,888 iteration 2872 : loss : 0.040916, loss_ce: 0.018097
2022-01-09 01:53:47,425 iteration 2873 : loss : 0.030974, loss_ce: 0.009376
 42%|███████████▍               | 169/400 [1:17:16<1:40:27, 26.09s/it]2022-01-09 01:53:49,009 iteration 2874 : loss : 0.037069, loss_ce: 0.012268
2022-01-09 01:53:50,500 iteration 2875 : loss : 0.026304, loss_ce: 0.011016
2022-01-09 01:53:51,979 iteration 2876 : loss : 0.038327, loss_ce: 0.016324
2022-01-09 01:53:53,525 iteration 2877 : loss : 0.040736, loss_ce: 0.011715
2022-01-09 01:53:55,000 iteration 2878 : loss : 0.028635, loss_ce: 0.012459
2022-01-09 01:53:56,533 iteration 2879 : loss : 0.030229, loss_ce: 0.010858
2022-01-09 01:53:58,079 iteration 2880 : loss : 0.039303, loss_ce: 0.013092
2022-01-09 01:53:59,560 iteration 2881 : loss : 0.028241, loss_ce: 0.011756
2022-01-09 01:54:01,113 iteration 2882 : loss : 0.029799, loss_ce: 0.015918
2022-01-09 01:54:02,535 iteration 2883 : loss : 0.037362, loss_ce: 0.016169
2022-01-09 01:54:03,979 iteration 2884 : loss : 0.034022, loss_ce: 0.015067
2022-01-09 01:54:05,473 iteration 2885 : loss : 0.034585, loss_ce: 0.014530
2022-01-09 01:54:06,828 iteration 2886 : loss : 0.046554, loss_ce: 0.015040
2022-01-09 01:54:08,394 iteration 2887 : loss : 0.043183, loss_ce: 0.021020
2022-01-09 01:54:09,881 iteration 2888 : loss : 0.070606, loss_ce: 0.019021
2022-01-09 01:54:11,498 iteration 2889 : loss : 0.040089, loss_ce: 0.014221
2022-01-09 01:54:11,499 Training Data Eval:
2022-01-09 01:54:18,767   Average segmentation loss on training set: 0.0224
2022-01-09 01:54:18,767 Validation Data Eval:
2022-01-09 01:54:21,240   Average segmentation loss on validation set: 0.0796
2022-01-09 01:54:22,772 iteration 2890 : loss : 0.030127, loss_ce: 0.010229
 42%|███████████▍               | 170/400 [1:17:51<1:50:38, 28.86s/it]2022-01-09 01:54:24,354 iteration 2891 : loss : 0.039180, loss_ce: 0.017124
2022-01-09 01:54:25,931 iteration 2892 : loss : 0.048581, loss_ce: 0.010702
2022-01-09 01:54:27,342 iteration 2893 : loss : 0.024816, loss_ce: 0.008312
2022-01-09 01:54:28,908 iteration 2894 : loss : 0.041344, loss_ce: 0.012481
2022-01-09 01:54:30,456 iteration 2895 : loss : 0.037111, loss_ce: 0.015945
2022-01-09 01:54:31,888 iteration 2896 : loss : 0.029235, loss_ce: 0.009109
2022-01-09 01:54:33,369 iteration 2897 : loss : 0.040466, loss_ce: 0.018055
2022-01-09 01:54:34,764 iteration 2898 : loss : 0.027822, loss_ce: 0.012696
2022-01-09 01:54:36,275 iteration 2899 : loss : 0.031395, loss_ce: 0.011565
2022-01-09 01:54:37,734 iteration 2900 : loss : 0.027278, loss_ce: 0.008988
2022-01-09 01:54:39,210 iteration 2901 : loss : 0.031444, loss_ce: 0.008877
2022-01-09 01:54:40,581 iteration 2902 : loss : 0.035038, loss_ce: 0.012608
2022-01-09 01:54:41,993 iteration 2903 : loss : 0.035483, loss_ce: 0.016664
2022-01-09 01:54:43,402 iteration 2904 : loss : 0.036486, loss_ce: 0.014928
2022-01-09 01:54:44,763 iteration 2905 : loss : 0.029611, loss_ce: 0.015335
2022-01-09 01:54:46,273 iteration 2906 : loss : 0.043342, loss_ce: 0.022859
2022-01-09 01:54:47,679 iteration 2907 : loss : 0.034644, loss_ce: 0.012983
 43%|███████████▌               | 171/400 [1:18:16<1:45:38, 27.68s/it]2022-01-09 01:54:49,256 iteration 2908 : loss : 0.063578, loss_ce: 0.029258
2022-01-09 01:54:50,797 iteration 2909 : loss : 0.044674, loss_ce: 0.021226
2022-01-09 01:54:52,273 iteration 2910 : loss : 0.050907, loss_ce: 0.026253
2022-01-09 01:54:53,797 iteration 2911 : loss : 0.029939, loss_ce: 0.009612
2022-01-09 01:54:55,223 iteration 2912 : loss : 0.037209, loss_ce: 0.015154
2022-01-09 01:54:56,773 iteration 2913 : loss : 0.037877, loss_ce: 0.016102
2022-01-09 01:54:58,219 iteration 2914 : loss : 0.034455, loss_ce: 0.012683
2022-01-09 01:54:59,663 iteration 2915 : loss : 0.030014, loss_ce: 0.011520
2022-01-09 01:55:01,151 iteration 2916 : loss : 0.033468, loss_ce: 0.015866
2022-01-09 01:55:02,526 iteration 2917 : loss : 0.033316, loss_ce: 0.015158
2022-01-09 01:55:03,939 iteration 2918 : loss : 0.036358, loss_ce: 0.011530
2022-01-09 01:55:05,402 iteration 2919 : loss : 0.049996, loss_ce: 0.015329
2022-01-09 01:55:06,892 iteration 2920 : loss : 0.035524, loss_ce: 0.013002
2022-01-09 01:55:08,336 iteration 2921 : loss : 0.036008, loss_ce: 0.016961
2022-01-09 01:55:09,698 iteration 2922 : loss : 0.043687, loss_ce: 0.016269
2022-01-09 01:55:11,126 iteration 2923 : loss : 0.032742, loss_ce: 0.010612
2022-01-09 01:55:12,592 iteration 2924 : loss : 0.038221, loss_ce: 0.013161
 43%|███████████▌               | 172/400 [1:18:41<1:42:01, 26.85s/it]2022-01-09 01:55:13,985 iteration 2925 : loss : 0.026442, loss_ce: 0.013617
2022-01-09 01:55:15,557 iteration 2926 : loss : 0.050894, loss_ce: 0.024850
2022-01-09 01:55:17,019 iteration 2927 : loss : 0.028137, loss_ce: 0.012004
2022-01-09 01:55:18,316 iteration 2928 : loss : 0.029987, loss_ce: 0.009475
2022-01-09 01:55:19,735 iteration 2929 : loss : 0.026800, loss_ce: 0.011460
2022-01-09 01:55:21,294 iteration 2930 : loss : 0.042200, loss_ce: 0.018003
2022-01-09 01:55:22,717 iteration 2931 : loss : 0.026270, loss_ce: 0.009580
2022-01-09 01:55:24,117 iteration 2932 : loss : 0.034561, loss_ce: 0.013741
2022-01-09 01:55:25,677 iteration 2933 : loss : 0.046890, loss_ce: 0.016523
2022-01-09 01:55:27,136 iteration 2934 : loss : 0.036901, loss_ce: 0.014914
2022-01-09 01:55:28,603 iteration 2935 : loss : 0.031830, loss_ce: 0.011302
2022-01-09 01:55:30,049 iteration 2936 : loss : 0.026280, loss_ce: 0.009785
2022-01-09 01:55:31,549 iteration 2937 : loss : 0.032752, loss_ce: 0.015839
2022-01-09 01:55:33,070 iteration 2938 : loss : 0.032523, loss_ce: 0.010260
2022-01-09 01:55:34,596 iteration 2939 : loss : 0.024506, loss_ce: 0.007229
2022-01-09 01:55:36,199 iteration 2940 : loss : 0.047508, loss_ce: 0.022705
2022-01-09 01:55:37,595 iteration 2941 : loss : 0.030322, loss_ce: 0.010895
 43%|███████████▋               | 173/400 [1:19:06<1:39:29, 26.30s/it]2022-01-09 01:55:39,051 iteration 2942 : loss : 0.029476, loss_ce: 0.008720
2022-01-09 01:55:40,554 iteration 2943 : loss : 0.041479, loss_ce: 0.011764
2022-01-09 01:55:41,971 iteration 2944 : loss : 0.029993, loss_ce: 0.015359
2022-01-09 01:55:43,443 iteration 2945 : loss : 0.030786, loss_ce: 0.008195
2022-01-09 01:55:44,769 iteration 2946 : loss : 0.027048, loss_ce: 0.010704
2022-01-09 01:55:46,184 iteration 2947 : loss : 0.043869, loss_ce: 0.017596
2022-01-09 01:55:47,590 iteration 2948 : loss : 0.033852, loss_ce: 0.009558
2022-01-09 01:55:49,134 iteration 2949 : loss : 0.038273, loss_ce: 0.014232
2022-01-09 01:55:50,619 iteration 2950 : loss : 0.029059, loss_ce: 0.012254
2022-01-09 01:55:52,132 iteration 2951 : loss : 0.028458, loss_ce: 0.012262
2022-01-09 01:55:53,635 iteration 2952 : loss : 0.027468, loss_ce: 0.009275
2022-01-09 01:55:55,175 iteration 2953 : loss : 0.045430, loss_ce: 0.018205
2022-01-09 01:55:56,771 iteration 2954 : loss : 0.054984, loss_ce: 0.013952
2022-01-09 01:55:58,192 iteration 2955 : loss : 0.030112, loss_ce: 0.013783
2022-01-09 01:55:59,628 iteration 2956 : loss : 0.025302, loss_ce: 0.009451
2022-01-09 01:56:01,019 iteration 2957 : loss : 0.036953, loss_ce: 0.013719
2022-01-09 01:56:02,491 iteration 2958 : loss : 0.048331, loss_ce: 0.020902
 44%|███████████▋               | 174/400 [1:19:31<1:37:27, 25.87s/it]2022-01-09 01:56:03,988 iteration 2959 : loss : 0.027691, loss_ce: 0.009094
2022-01-09 01:56:05,429 iteration 2960 : loss : 0.036508, loss_ce: 0.017121
2022-01-09 01:56:06,912 iteration 2961 : loss : 0.046973, loss_ce: 0.018457
2022-01-09 01:56:08,581 iteration 2962 : loss : 0.028559, loss_ce: 0.009574
2022-01-09 01:56:10,080 iteration 2963 : loss : 0.043611, loss_ce: 0.016962
2022-01-09 01:56:11,427 iteration 2964 : loss : 0.039470, loss_ce: 0.010959
2022-01-09 01:56:12,910 iteration 2965 : loss : 0.037418, loss_ce: 0.019509
2022-01-09 01:56:14,448 iteration 2966 : loss : 0.034532, loss_ce: 0.012012
2022-01-09 01:56:15,923 iteration 2967 : loss : 0.040145, loss_ce: 0.014766
2022-01-09 01:56:17,431 iteration 2968 : loss : 0.043155, loss_ce: 0.016376
2022-01-09 01:56:18,910 iteration 2969 : loss : 0.028970, loss_ce: 0.012825
2022-01-09 01:56:20,419 iteration 2970 : loss : 0.034790, loss_ce: 0.013911
2022-01-09 01:56:21,887 iteration 2971 : loss : 0.026144, loss_ce: 0.010306
2022-01-09 01:56:23,353 iteration 2972 : loss : 0.036631, loss_ce: 0.016046
2022-01-09 01:56:24,750 iteration 2973 : loss : 0.035017, loss_ce: 0.008376
2022-01-09 01:56:26,286 iteration 2974 : loss : 0.047410, loss_ce: 0.015162
2022-01-09 01:56:26,286 Training Data Eval:
2022-01-09 01:56:33,617   Average segmentation loss on training set: 0.0212
2022-01-09 01:56:33,617 Validation Data Eval:
2022-01-09 01:56:36,144   Average segmentation loss on validation set: 0.0678
2022-01-09 01:56:37,575 iteration 2975 : loss : 0.023743, loss_ce: 0.009160
 44%|███████████▊               | 175/400 [1:20:06<1:47:23, 28.64s/it]2022-01-09 01:56:38,984 iteration 2976 : loss : 0.025187, loss_ce: 0.010621
2022-01-09 01:56:40,485 iteration 2977 : loss : 0.030216, loss_ce: 0.013750
2022-01-09 01:56:41,966 iteration 2978 : loss : 0.028112, loss_ce: 0.009189
2022-01-09 01:56:43,494 iteration 2979 : loss : 0.024597, loss_ce: 0.007760
2022-01-09 01:56:45,004 iteration 2980 : loss : 0.044283, loss_ce: 0.013908
2022-01-09 01:56:46,609 iteration 2981 : loss : 0.049120, loss_ce: 0.021819
2022-01-09 01:56:48,077 iteration 2982 : loss : 0.025688, loss_ce: 0.010105
2022-01-09 01:56:49,558 iteration 2983 : loss : 0.030044, loss_ce: 0.010424
2022-01-09 01:56:51,040 iteration 2984 : loss : 0.043251, loss_ce: 0.014296
2022-01-09 01:56:52,548 iteration 2985 : loss : 0.041052, loss_ce: 0.015248
2022-01-09 01:56:53,938 iteration 2986 : loss : 0.033908, loss_ce: 0.014165
2022-01-09 01:56:55,410 iteration 2987 : loss : 0.038645, loss_ce: 0.011341
2022-01-09 01:56:56,944 iteration 2988 : loss : 0.038714, loss_ce: 0.015439
2022-01-09 01:56:58,461 iteration 2989 : loss : 0.044157, loss_ce: 0.016508
2022-01-09 01:56:59,850 iteration 2990 : loss : 0.024884, loss_ce: 0.010544
2022-01-09 01:57:01,304 iteration 2991 : loss : 0.024868, loss_ce: 0.009087
2022-01-09 01:57:02,863 iteration 2992 : loss : 0.027831, loss_ce: 0.011838
 44%|███████████▉               | 176/400 [1:20:31<1:43:09, 27.63s/it]2022-01-09 01:57:04,320 iteration 2993 : loss : 0.029667, loss_ce: 0.014334
2022-01-09 01:57:05,803 iteration 2994 : loss : 0.053231, loss_ce: 0.013122
2022-01-09 01:57:07,235 iteration 2995 : loss : 0.028818, loss_ce: 0.010074
2022-01-09 01:57:08,691 iteration 2996 : loss : 0.027726, loss_ce: 0.013247
2022-01-09 01:57:10,145 iteration 2997 : loss : 0.032553, loss_ce: 0.011043
2022-01-09 01:57:11,619 iteration 2998 : loss : 0.028728, loss_ce: 0.013146
2022-01-09 01:57:13,164 iteration 2999 : loss : 0.038146, loss_ce: 0.010510
2022-01-09 01:57:14,732 iteration 3000 : loss : 0.037582, loss_ce: 0.019204
2022-01-09 01:57:16,295 iteration 3001 : loss : 0.036647, loss_ce: 0.010265
2022-01-09 01:57:17,693 iteration 3002 : loss : 0.030080, loss_ce: 0.011764
2022-01-09 01:57:19,119 iteration 3003 : loss : 0.028622, loss_ce: 0.009690
2022-01-09 01:57:20,558 iteration 3004 : loss : 0.027743, loss_ce: 0.011362
2022-01-09 01:57:22,035 iteration 3005 : loss : 0.030630, loss_ce: 0.013181
2022-01-09 01:57:23,545 iteration 3006 : loss : 0.037090, loss_ce: 0.014615
2022-01-09 01:57:25,044 iteration 3007 : loss : 0.031627, loss_ce: 0.012749
2022-01-09 01:57:26,548 iteration 3008 : loss : 0.028259, loss_ce: 0.010015
2022-01-09 01:57:27,918 iteration 3009 : loss : 0.033440, loss_ce: 0.014787
 44%|███████████▉               | 177/400 [1:20:56<1:39:49, 26.86s/it]2022-01-09 01:57:29,604 iteration 3010 : loss : 0.038583, loss_ce: 0.012049
2022-01-09 01:57:31,124 iteration 3011 : loss : 0.029145, loss_ce: 0.013867
2022-01-09 01:57:32,529 iteration 3012 : loss : 0.022813, loss_ce: 0.009859
2022-01-09 01:57:33,903 iteration 3013 : loss : 0.036263, loss_ce: 0.010466
2022-01-09 01:57:35,375 iteration 3014 : loss : 0.033311, loss_ce: 0.011990
2022-01-09 01:57:36,825 iteration 3015 : loss : 0.043462, loss_ce: 0.009934
2022-01-09 01:57:38,276 iteration 3016 : loss : 0.046988, loss_ce: 0.017975
2022-01-09 01:57:39,695 iteration 3017 : loss : 0.029229, loss_ce: 0.015573
2022-01-09 01:57:41,112 iteration 3018 : loss : 0.023027, loss_ce: 0.009533
2022-01-09 01:57:42,476 iteration 3019 : loss : 0.030160, loss_ce: 0.011377
2022-01-09 01:57:43,966 iteration 3020 : loss : 0.033499, loss_ce: 0.011729
2022-01-09 01:57:45,406 iteration 3021 : loss : 0.021126, loss_ce: 0.007485
2022-01-09 01:57:46,834 iteration 3022 : loss : 0.029200, loss_ce: 0.008797
2022-01-09 01:57:48,306 iteration 3023 : loss : 0.044198, loss_ce: 0.015067
2022-01-09 01:57:49,873 iteration 3024 : loss : 0.040053, loss_ce: 0.019059
2022-01-09 01:57:51,415 iteration 3025 : loss : 0.041209, loss_ce: 0.016062
2022-01-09 01:57:52,818 iteration 3026 : loss : 0.039105, loss_ce: 0.019352
 44%|████████████               | 178/400 [1:21:21<1:37:12, 26.27s/it]2022-01-09 01:57:54,417 iteration 3027 : loss : 0.046636, loss_ce: 0.020260
2022-01-09 01:57:55,824 iteration 3028 : loss : 0.028304, loss_ce: 0.011699
2022-01-09 01:57:57,314 iteration 3029 : loss : 0.035307, loss_ce: 0.015686
2022-01-09 01:57:58,738 iteration 3030 : loss : 0.028066, loss_ce: 0.013085
2022-01-09 01:58:00,198 iteration 3031 : loss : 0.048114, loss_ce: 0.018091
2022-01-09 01:58:01,609 iteration 3032 : loss : 0.035433, loss_ce: 0.012266
2022-01-09 01:58:03,112 iteration 3033 : loss : 0.037645, loss_ce: 0.011148
2022-01-09 01:58:04,636 iteration 3034 : loss : 0.033184, loss_ce: 0.012840
2022-01-09 01:58:06,128 iteration 3035 : loss : 0.027723, loss_ce: 0.012462
2022-01-09 01:58:07,562 iteration 3036 : loss : 0.042681, loss_ce: 0.023812
2022-01-09 01:58:09,023 iteration 3037 : loss : 0.035647, loss_ce: 0.015491
2022-01-09 01:58:10,530 iteration 3038 : loss : 0.029678, loss_ce: 0.011699
2022-01-09 01:58:11,905 iteration 3039 : loss : 0.030405, loss_ce: 0.009533
2022-01-09 01:58:13,380 iteration 3040 : loss : 0.038566, loss_ce: 0.011698
2022-01-09 01:58:14,900 iteration 3041 : loss : 0.028514, loss_ce: 0.011172
2022-01-09 01:58:16,355 iteration 3042 : loss : 0.040432, loss_ce: 0.018544
2022-01-09 01:58:18,050 iteration 3043 : loss : 0.061058, loss_ce: 0.021569
 45%|████████████               | 179/400 [1:21:46<1:35:37, 25.96s/it]2022-01-09 01:58:19,600 iteration 3044 : loss : 0.031327, loss_ce: 0.013012
2022-01-09 01:58:20,991 iteration 3045 : loss : 0.028753, loss_ce: 0.010764
2022-01-09 01:58:22,382 iteration 3046 : loss : 0.028197, loss_ce: 0.009638
2022-01-09 01:58:23,987 iteration 3047 : loss : 0.034657, loss_ce: 0.017343
2022-01-09 01:58:25,396 iteration 3048 : loss : 0.033819, loss_ce: 0.011263
2022-01-09 01:58:26,867 iteration 3049 : loss : 0.032579, loss_ce: 0.013736
2022-01-09 01:58:28,371 iteration 3050 : loss : 0.038621, loss_ce: 0.020230
2022-01-09 01:58:29,846 iteration 3051 : loss : 0.029586, loss_ce: 0.007924
2022-01-09 01:58:31,292 iteration 3052 : loss : 0.028498, loss_ce: 0.011050
2022-01-09 01:58:32,868 iteration 3053 : loss : 0.019847, loss_ce: 0.008520
2022-01-09 01:58:34,273 iteration 3054 : loss : 0.030253, loss_ce: 0.008753
2022-01-09 01:58:35,774 iteration 3055 : loss : 0.040481, loss_ce: 0.015911
2022-01-09 01:58:37,205 iteration 3056 : loss : 0.023647, loss_ce: 0.010049
2022-01-09 01:58:38,588 iteration 3057 : loss : 0.027594, loss_ce: 0.010337
2022-01-09 01:58:40,070 iteration 3058 : loss : 0.025236, loss_ce: 0.010345
2022-01-09 01:58:41,580 iteration 3059 : loss : 0.035123, loss_ce: 0.013590
2022-01-09 01:58:41,581 Training Data Eval:
2022-01-09 01:58:48,850   Average segmentation loss on training set: 0.0199
2022-01-09 01:58:48,850 Validation Data Eval:
2022-01-09 01:58:51,414   Average segmentation loss on validation set: 0.0762
2022-01-09 01:58:52,840 iteration 3060 : loss : 0.023905, loss_ce: 0.011400
 45%|████████████▏              | 180/400 [1:22:21<1:44:54, 28.61s/it]2022-01-09 01:58:54,277 iteration 3061 : loss : 0.027635, loss_ce: 0.010833
2022-01-09 01:58:55,737 iteration 3062 : loss : 0.036208, loss_ce: 0.013753
2022-01-09 01:58:57,251 iteration 3063 : loss : 0.047962, loss_ce: 0.018569
2022-01-09 01:58:58,763 iteration 3064 : loss : 0.023988, loss_ce: 0.009371
2022-01-09 01:59:00,105 iteration 3065 : loss : 0.020371, loss_ce: 0.007280
2022-01-09 01:59:01,516 iteration 3066 : loss : 0.026348, loss_ce: 0.012579
2022-01-09 01:59:03,007 iteration 3067 : loss : 0.036614, loss_ce: 0.015867
2022-01-09 01:59:04,483 iteration 3068 : loss : 0.045309, loss_ce: 0.013370
2022-01-09 01:59:05,943 iteration 3069 : loss : 0.049726, loss_ce: 0.016191
2022-01-09 01:59:07,414 iteration 3070 : loss : 0.030944, loss_ce: 0.011181
2022-01-09 01:59:08,802 iteration 3071 : loss : 0.023477, loss_ce: 0.009127
2022-01-09 01:59:10,188 iteration 3072 : loss : 0.025158, loss_ce: 0.009946
2022-01-09 01:59:11,637 iteration 3073 : loss : 0.027317, loss_ce: 0.010038
2022-01-09 01:59:13,206 iteration 3074 : loss : 0.031962, loss_ce: 0.012220
2022-01-09 01:59:14,736 iteration 3075 : loss : 0.031012, loss_ce: 0.010898
2022-01-09 01:59:16,310 iteration 3076 : loss : 0.028293, loss_ce: 0.010690
2022-01-09 01:59:17,741 iteration 3077 : loss : 0.034167, loss_ce: 0.015552
 45%|████████████▏              | 181/400 [1:22:46<1:40:21, 27.49s/it]2022-01-09 01:59:19,229 iteration 3078 : loss : 0.024148, loss_ce: 0.009789
2022-01-09 01:59:20,856 iteration 3079 : loss : 0.023525, loss_ce: 0.008940
2022-01-09 01:59:22,237 iteration 3080 : loss : 0.027940, loss_ce: 0.008955
2022-01-09 01:59:23,747 iteration 3081 : loss : 0.046692, loss_ce: 0.018129
2022-01-09 01:59:25,184 iteration 3082 : loss : 0.037937, loss_ce: 0.010438
2022-01-09 01:59:26,717 iteration 3083 : loss : 0.034076, loss_ce: 0.012061
2022-01-09 01:59:28,025 iteration 3084 : loss : 0.029067, loss_ce: 0.013386
2022-01-09 01:59:29,521 iteration 3085 : loss : 0.028338, loss_ce: 0.013065
2022-01-09 01:59:31,077 iteration 3086 : loss : 0.035506, loss_ce: 0.019303
2022-01-09 01:59:32,509 iteration 3087 : loss : 0.051531, loss_ce: 0.020417
2022-01-09 01:59:33,988 iteration 3088 : loss : 0.022553, loss_ce: 0.007174
2022-01-09 01:59:35,574 iteration 3089 : loss : 0.038591, loss_ce: 0.011991
2022-01-09 01:59:36,913 iteration 3090 : loss : 0.026754, loss_ce: 0.011946
2022-01-09 01:59:38,376 iteration 3091 : loss : 0.038961, loss_ce: 0.017701
2022-01-09 01:59:40,037 iteration 3092 : loss : 0.040110, loss_ce: 0.015013
2022-01-09 01:59:41,488 iteration 3093 : loss : 0.032664, loss_ce: 0.012919
2022-01-09 01:59:42,988 iteration 3094 : loss : 0.031361, loss_ce: 0.012855
 46%|████████████▎              | 182/400 [1:23:11<1:37:27, 26.82s/it]2022-01-09 01:59:44,536 iteration 3095 : loss : 0.027268, loss_ce: 0.010954
2022-01-09 01:59:46,041 iteration 3096 : loss : 0.024515, loss_ce: 0.011465
2022-01-09 01:59:47,497 iteration 3097 : loss : 0.031110, loss_ce: 0.011933
2022-01-09 01:59:48,912 iteration 3098 : loss : 0.028262, loss_ce: 0.012395
2022-01-09 01:59:50,374 iteration 3099 : loss : 0.022405, loss_ce: 0.007386
2022-01-09 01:59:51,764 iteration 3100 : loss : 0.030299, loss_ce: 0.010673
2022-01-09 01:59:53,284 iteration 3101 : loss : 0.027684, loss_ce: 0.010384
2022-01-09 01:59:54,744 iteration 3102 : loss : 0.029663, loss_ce: 0.011134
2022-01-09 01:59:56,224 iteration 3103 : loss : 0.041844, loss_ce: 0.015417
2022-01-09 01:59:57,629 iteration 3104 : loss : 0.027235, loss_ce: 0.012285
2022-01-09 01:59:59,004 iteration 3105 : loss : 0.023533, loss_ce: 0.009640
2022-01-09 02:00:00,442 iteration 3106 : loss : 0.028818, loss_ce: 0.012321
2022-01-09 02:00:01,869 iteration 3107 : loss : 0.030203, loss_ce: 0.009497
2022-01-09 02:00:03,351 iteration 3108 : loss : 0.041821, loss_ce: 0.013895
2022-01-09 02:00:04,857 iteration 3109 : loss : 0.031067, loss_ce: 0.011596
2022-01-09 02:00:06,363 iteration 3110 : loss : 0.046139, loss_ce: 0.022003
2022-01-09 02:00:07,841 iteration 3111 : loss : 0.039819, loss_ce: 0.018110
 46%|████████████▎              | 183/400 [1:23:36<1:34:51, 26.23s/it]2022-01-09 02:00:09,391 iteration 3112 : loss : 0.038536, loss_ce: 0.014117
2022-01-09 02:00:10,799 iteration 3113 : loss : 0.029409, loss_ce: 0.011736
2022-01-09 02:00:12,322 iteration 3114 : loss : 0.034923, loss_ce: 0.011624
2022-01-09 02:00:13,790 iteration 3115 : loss : 0.035026, loss_ce: 0.014454
2022-01-09 02:00:15,237 iteration 3116 : loss : 0.027499, loss_ce: 0.012058
2022-01-09 02:00:16,781 iteration 3117 : loss : 0.025489, loss_ce: 0.010610
2022-01-09 02:00:18,224 iteration 3118 : loss : 0.027710, loss_ce: 0.011077
2022-01-09 02:00:19,818 iteration 3119 : loss : 0.047042, loss_ce: 0.015683
2022-01-09 02:00:21,215 iteration 3120 : loss : 0.027339, loss_ce: 0.010527
2022-01-09 02:00:22,757 iteration 3121 : loss : 0.027270, loss_ce: 0.011989
2022-01-09 02:00:24,191 iteration 3122 : loss : 0.045420, loss_ce: 0.013609
2022-01-09 02:00:25,724 iteration 3123 : loss : 0.031138, loss_ce: 0.013455
2022-01-09 02:00:27,087 iteration 3124 : loss : 0.023425, loss_ce: 0.009612
2022-01-09 02:00:28,536 iteration 3125 : loss : 0.030188, loss_ce: 0.009649
2022-01-09 02:00:30,067 iteration 3126 : loss : 0.023842, loss_ce: 0.010800
2022-01-09 02:00:31,625 iteration 3127 : loss : 0.036476, loss_ce: 0.012858
2022-01-09 02:00:33,170 iteration 3128 : loss : 0.037724, loss_ce: 0.012644
 46%|████████████▍              | 184/400 [1:24:02<1:33:27, 25.96s/it]2022-01-09 02:00:34,666 iteration 3129 : loss : 0.035600, loss_ce: 0.014334
2022-01-09 02:00:36,266 iteration 3130 : loss : 0.032018, loss_ce: 0.013368
2022-01-09 02:00:37,708 iteration 3131 : loss : 0.026057, loss_ce: 0.008254
2022-01-09 02:00:39,172 iteration 3132 : loss : 0.025702, loss_ce: 0.007983
2022-01-09 02:00:40,630 iteration 3133 : loss : 0.046399, loss_ce: 0.021479
2022-01-09 02:00:42,046 iteration 3134 : loss : 0.022800, loss_ce: 0.010277
2022-01-09 02:00:43,505 iteration 3135 : loss : 0.029660, loss_ce: 0.013150
2022-01-09 02:00:44,936 iteration 3136 : loss : 0.026281, loss_ce: 0.011361
2022-01-09 02:00:46,540 iteration 3137 : loss : 0.034223, loss_ce: 0.014184
2022-01-09 02:00:48,089 iteration 3138 : loss : 0.037289, loss_ce: 0.013931
2022-01-09 02:00:49,470 iteration 3139 : loss : 0.027674, loss_ce: 0.010069
2022-01-09 02:00:50,856 iteration 3140 : loss : 0.027294, loss_ce: 0.011678
2022-01-09 02:00:52,344 iteration 3141 : loss : 0.037344, loss_ce: 0.017240
2022-01-09 02:00:53,779 iteration 3142 : loss : 0.038785, loss_ce: 0.012421
2022-01-09 02:00:55,304 iteration 3143 : loss : 0.035942, loss_ce: 0.011476
2022-01-09 02:00:56,747 iteration 3144 : loss : 0.032565, loss_ce: 0.013980
2022-01-09 02:00:56,747 Training Data Eval:
2022-01-09 02:01:04,040   Average segmentation loss on training set: 0.0209
2022-01-09 02:01:04,040 Validation Data Eval:
2022-01-09 02:01:06,534   Average segmentation loss on validation set: 0.0904
2022-01-09 02:01:07,949 iteration 3145 : loss : 0.024998, loss_ce: 0.009695
 46%|████████████▍              | 185/400 [1:24:36<1:42:30, 28.61s/it]2022-01-09 02:01:09,670 iteration 3146 : loss : 0.044852, loss_ce: 0.014432
2022-01-09 02:01:11,194 iteration 3147 : loss : 0.030895, loss_ce: 0.014649
2022-01-09 02:01:12,580 iteration 3148 : loss : 0.023012, loss_ce: 0.008130
2022-01-09 02:01:14,103 iteration 3149 : loss : 0.025010, loss_ce: 0.008288
2022-01-09 02:01:15,619 iteration 3150 : loss : 0.050335, loss_ce: 0.018479
2022-01-09 02:01:17,134 iteration 3151 : loss : 0.032240, loss_ce: 0.013508
2022-01-09 02:01:18,642 iteration 3152 : loss : 0.029967, loss_ce: 0.010313
2022-01-09 02:01:20,124 iteration 3153 : loss : 0.040607, loss_ce: 0.020706
2022-01-09 02:01:21,693 iteration 3154 : loss : 0.038050, loss_ce: 0.014535
2022-01-09 02:01:23,213 iteration 3155 : loss : 0.022398, loss_ce: 0.008071
2022-01-09 02:01:24,597 iteration 3156 : loss : 0.030189, loss_ce: 0.011541
2022-01-09 02:01:26,079 iteration 3157 : loss : 0.025417, loss_ce: 0.010685
2022-01-09 02:01:27,486 iteration 3158 : loss : 0.046295, loss_ce: 0.018731
2022-01-09 02:01:28,982 iteration 3159 : loss : 0.033256, loss_ce: 0.016802
2022-01-09 02:01:30,496 iteration 3160 : loss : 0.028725, loss_ce: 0.011227
2022-01-09 02:01:32,063 iteration 3161 : loss : 0.030807, loss_ce: 0.013668
2022-01-09 02:01:33,546 iteration 3162 : loss : 0.033030, loss_ce: 0.011947
 46%|████████████▌              | 186/400 [1:25:02<1:38:48, 27.70s/it]2022-01-09 02:01:35,029 iteration 3163 : loss : 0.034840, loss_ce: 0.011165
2022-01-09 02:01:36,644 iteration 3164 : loss : 0.039308, loss_ce: 0.015602
2022-01-09 02:01:38,183 iteration 3165 : loss : 0.024491, loss_ce: 0.009693
2022-01-09 02:01:39,610 iteration 3166 : loss : 0.027512, loss_ce: 0.010583
2022-01-09 02:01:41,149 iteration 3167 : loss : 0.028960, loss_ce: 0.009736
2022-01-09 02:01:42,627 iteration 3168 : loss : 0.027658, loss_ce: 0.011616
2022-01-09 02:01:44,139 iteration 3169 : loss : 0.024792, loss_ce: 0.010315
2022-01-09 02:01:45,617 iteration 3170 : loss : 0.029016, loss_ce: 0.012532
2022-01-09 02:01:46,940 iteration 3171 : loss : 0.025653, loss_ce: 0.007634
2022-01-09 02:01:48,359 iteration 3172 : loss : 0.028508, loss_ce: 0.011433
2022-01-09 02:01:49,835 iteration 3173 : loss : 0.037007, loss_ce: 0.014160
2022-01-09 02:01:51,279 iteration 3174 : loss : 0.028437, loss_ce: 0.012904
2022-01-09 02:01:52,693 iteration 3175 : loss : 0.073613, loss_ce: 0.030900
2022-01-09 02:01:54,286 iteration 3176 : loss : 0.051674, loss_ce: 0.013668
2022-01-09 02:01:55,780 iteration 3177 : loss : 0.027539, loss_ce: 0.010132
2022-01-09 02:01:57,383 iteration 3178 : loss : 0.043130, loss_ce: 0.011100
2022-01-09 02:01:58,877 iteration 3179 : loss : 0.030858, loss_ce: 0.014825
 47%|████████████▌              | 187/400 [1:25:27<1:35:49, 26.99s/it]2022-01-09 02:02:00,445 iteration 3180 : loss : 0.054165, loss_ce: 0.021939
2022-01-09 02:02:01,898 iteration 3181 : loss : 0.029442, loss_ce: 0.013832
2022-01-09 02:02:03,317 iteration 3182 : loss : 0.031945, loss_ce: 0.016650
2022-01-09 02:02:04,871 iteration 3183 : loss : 0.045256, loss_ce: 0.018315
2022-01-09 02:02:06,344 iteration 3184 : loss : 0.027342, loss_ce: 0.008921
2022-01-09 02:02:07,827 iteration 3185 : loss : 0.025952, loss_ce: 0.007503
2022-01-09 02:02:09,348 iteration 3186 : loss : 0.028145, loss_ce: 0.011955
2022-01-09 02:02:10,876 iteration 3187 : loss : 0.031266, loss_ce: 0.014438
2022-01-09 02:02:12,416 iteration 3188 : loss : 0.039053, loss_ce: 0.015585
2022-01-09 02:02:13,874 iteration 3189 : loss : 0.029073, loss_ce: 0.010038
2022-01-09 02:02:15,479 iteration 3190 : loss : 0.029726, loss_ce: 0.010739
2022-01-09 02:02:17,021 iteration 3191 : loss : 0.027586, loss_ce: 0.010698
2022-01-09 02:02:18,407 iteration 3192 : loss : 0.052292, loss_ce: 0.012100
2022-01-09 02:02:19,871 iteration 3193 : loss : 0.032637, loss_ce: 0.013328
2022-01-09 02:02:21,511 iteration 3194 : loss : 0.057099, loss_ce: 0.022715
2022-01-09 02:02:22,999 iteration 3195 : loss : 0.032114, loss_ce: 0.014861
2022-01-09 02:02:24,474 iteration 3196 : loss : 0.037165, loss_ce: 0.017858
 47%|████████████▋              | 188/400 [1:25:53<1:33:53, 26.57s/it]2022-01-09 02:02:25,987 iteration 3197 : loss : 0.023036, loss_ce: 0.009513
2022-01-09 02:02:27,513 iteration 3198 : loss : 0.037935, loss_ce: 0.017540
2022-01-09 02:02:28,978 iteration 3199 : loss : 0.038669, loss_ce: 0.014894
2022-01-09 02:02:30,507 iteration 3200 : loss : 0.035883, loss_ce: 0.014796
2022-01-09 02:02:32,004 iteration 3201 : loss : 0.039112, loss_ce: 0.014200
2022-01-09 02:02:33,538 iteration 3202 : loss : 0.033435, loss_ce: 0.010558
2022-01-09 02:02:34,910 iteration 3203 : loss : 0.027903, loss_ce: 0.012145
2022-01-09 02:02:36,269 iteration 3204 : loss : 0.026026, loss_ce: 0.010619
2022-01-09 02:02:37,628 iteration 3205 : loss : 0.037814, loss_ce: 0.011544
2022-01-09 02:02:38,938 iteration 3206 : loss : 0.025236, loss_ce: 0.007680
2022-01-09 02:02:40,440 iteration 3207 : loss : 0.041547, loss_ce: 0.017465
2022-01-09 02:02:41,896 iteration 3208 : loss : 0.027387, loss_ce: 0.010988
2022-01-09 02:02:43,322 iteration 3209 : loss : 0.028747, loss_ce: 0.011170
2022-01-09 02:02:44,827 iteration 3210 : loss : 0.026559, loss_ce: 0.011885
2022-01-09 02:02:46,217 iteration 3211 : loss : 0.030272, loss_ce: 0.008622
2022-01-09 02:02:47,782 iteration 3212 : loss : 0.041975, loss_ce: 0.021375
2022-01-09 02:02:49,361 iteration 3213 : loss : 0.037166, loss_ce: 0.012065
 47%|████████████▊              | 189/400 [1:26:18<1:31:39, 26.07s/it]2022-01-09 02:02:50,823 iteration 3214 : loss : 0.020960, loss_ce: 0.009309
2022-01-09 02:02:52,285 iteration 3215 : loss : 0.032628, loss_ce: 0.013315
2022-01-09 02:02:53,748 iteration 3216 : loss : 0.036851, loss_ce: 0.012603
2022-01-09 02:02:55,259 iteration 3217 : loss : 0.043925, loss_ce: 0.019757
2022-01-09 02:02:56,731 iteration 3218 : loss : 0.048824, loss_ce: 0.017000
2022-01-09 02:02:58,182 iteration 3219 : loss : 0.033993, loss_ce: 0.012815
2022-01-09 02:02:59,583 iteration 3220 : loss : 0.021856, loss_ce: 0.009291
2022-01-09 02:03:01,169 iteration 3221 : loss : 0.043459, loss_ce: 0.017442
2022-01-09 02:03:02,596 iteration 3222 : loss : 0.022342, loss_ce: 0.005576
2022-01-09 02:03:04,102 iteration 3223 : loss : 0.029362, loss_ce: 0.010223
2022-01-09 02:03:05,684 iteration 3224 : loss : 0.041468, loss_ce: 0.012745
2022-01-09 02:03:07,162 iteration 3225 : loss : 0.048955, loss_ce: 0.025479
2022-01-09 02:03:08,631 iteration 3226 : loss : 0.029626, loss_ce: 0.010820
2022-01-09 02:03:10,119 iteration 3227 : loss : 0.030424, loss_ce: 0.012220
2022-01-09 02:03:11,547 iteration 3228 : loss : 0.036213, loss_ce: 0.011434
2022-01-09 02:03:12,931 iteration 3229 : loss : 0.037661, loss_ce: 0.010155
2022-01-09 02:03:12,931 Training Data Eval:
2022-01-09 02:03:20,226   Average segmentation loss on training set: 0.0334
2022-01-09 02:03:20,226 Validation Data Eval:
2022-01-09 02:03:22,809   Average segmentation loss on validation set: 0.0946
2022-01-09 02:03:24,218 iteration 3230 : loss : 0.039081, loss_ce: 0.016896
 48%|████████████▊              | 190/400 [1:26:53<1:40:27, 28.70s/it]2022-01-09 02:03:25,739 iteration 3231 : loss : 0.052337, loss_ce: 0.010949
2022-01-09 02:03:27,220 iteration 3232 : loss : 0.030615, loss_ce: 0.009788
2022-01-09 02:03:28,738 iteration 3233 : loss : 0.035343, loss_ce: 0.015812
2022-01-09 02:03:30,266 iteration 3234 : loss : 0.028537, loss_ce: 0.009522
2022-01-09 02:03:31,787 iteration 3235 : loss : 0.043569, loss_ce: 0.013710
2022-01-09 02:03:33,210 iteration 3236 : loss : 0.022342, loss_ce: 0.010340
2022-01-09 02:03:34,828 iteration 3237 : loss : 0.035400, loss_ce: 0.014683
2022-01-09 02:03:36,318 iteration 3238 : loss : 0.031614, loss_ce: 0.013639
2022-01-09 02:03:37,809 iteration 3239 : loss : 0.028159, loss_ce: 0.011093
2022-01-09 02:03:39,234 iteration 3240 : loss : 0.031152, loss_ce: 0.008190
2022-01-09 02:03:40,634 iteration 3241 : loss : 0.039128, loss_ce: 0.009029
2022-01-09 02:03:42,146 iteration 3242 : loss : 0.030018, loss_ce: 0.011626
2022-01-09 02:03:43,556 iteration 3243 : loss : 0.032978, loss_ce: 0.011850
2022-01-09 02:03:45,072 iteration 3244 : loss : 0.029046, loss_ce: 0.012091
2022-01-09 02:03:46,501 iteration 3245 : loss : 0.023414, loss_ce: 0.010557
2022-01-09 02:03:48,057 iteration 3246 : loss : 0.059374, loss_ce: 0.029779
2022-01-09 02:03:49,477 iteration 3247 : loss : 0.033316, loss_ce: 0.013899
 48%|████████████▉              | 191/400 [1:27:18<1:36:23, 27.67s/it]2022-01-09 02:03:50,953 iteration 3248 : loss : 0.020650, loss_ce: 0.007967
2022-01-09 02:03:52,370 iteration 3249 : loss : 0.031197, loss_ce: 0.012219
2022-01-09 02:03:53,830 iteration 3250 : loss : 0.039873, loss_ce: 0.012612
2022-01-09 02:03:55,304 iteration 3251 : loss : 0.026273, loss_ce: 0.011296
2022-01-09 02:03:56,806 iteration 3252 : loss : 0.031652, loss_ce: 0.013607
2022-01-09 02:03:58,274 iteration 3253 : loss : 0.025604, loss_ce: 0.011774
2022-01-09 02:03:59,716 iteration 3254 : loss : 0.030746, loss_ce: 0.012555
2022-01-09 02:04:01,081 iteration 3255 : loss : 0.031129, loss_ce: 0.012131
2022-01-09 02:04:02,525 iteration 3256 : loss : 0.026745, loss_ce: 0.010306
2022-01-09 02:04:03,931 iteration 3257 : loss : 0.023502, loss_ce: 0.009311
2022-01-09 02:04:05,423 iteration 3258 : loss : 0.027688, loss_ce: 0.012782
2022-01-09 02:04:06,802 iteration 3259 : loss : 0.034797, loss_ce: 0.013468
2022-01-09 02:04:08,266 iteration 3260 : loss : 0.028760, loss_ce: 0.012133
2022-01-09 02:04:09,685 iteration 3261 : loss : 0.035235, loss_ce: 0.009232
2022-01-09 02:04:11,187 iteration 3262 : loss : 0.039875, loss_ce: 0.014527
2022-01-09 02:04:12,787 iteration 3263 : loss : 0.035593, loss_ce: 0.012031
2022-01-09 02:04:14,263 iteration 3264 : loss : 0.052903, loss_ce: 0.021779
 48%|████████████▉              | 192/400 [1:27:43<1:32:56, 26.81s/it]2022-01-09 02:04:15,726 iteration 3265 : loss : 0.023399, loss_ce: 0.010081
2022-01-09 02:04:17,250 iteration 3266 : loss : 0.033406, loss_ce: 0.013857
2022-01-09 02:04:18,732 iteration 3267 : loss : 0.034534, loss_ce: 0.017215
2022-01-09 02:04:20,137 iteration 3268 : loss : 0.028531, loss_ce: 0.010386
2022-01-09 02:04:21,672 iteration 3269 : loss : 0.035465, loss_ce: 0.017554
2022-01-09 02:04:23,249 iteration 3270 : loss : 0.039957, loss_ce: 0.011538
2022-01-09 02:04:24,788 iteration 3271 : loss : 0.030138, loss_ce: 0.010970
2022-01-09 02:04:26,335 iteration 3272 : loss : 0.026415, loss_ce: 0.010617
2022-01-09 02:04:27,721 iteration 3273 : loss : 0.030346, loss_ce: 0.009814
2022-01-09 02:04:29,175 iteration 3274 : loss : 0.022654, loss_ce: 0.011573
2022-01-09 02:04:30,723 iteration 3275 : loss : 0.037360, loss_ce: 0.009587
2022-01-09 02:04:32,150 iteration 3276 : loss : 0.033528, loss_ce: 0.013900
2022-01-09 02:04:33,650 iteration 3277 : loss : 0.031019, loss_ce: 0.015285
2022-01-09 02:04:35,120 iteration 3278 : loss : 0.037967, loss_ce: 0.013806
2022-01-09 02:04:36,531 iteration 3279 : loss : 0.028555, loss_ce: 0.011268
2022-01-09 02:04:37,977 iteration 3280 : loss : 0.040517, loss_ce: 0.015517
2022-01-09 02:04:39,372 iteration 3281 : loss : 0.022587, loss_ce: 0.007691
 48%|█████████████              | 193/400 [1:28:08<1:30:42, 26.29s/it]2022-01-09 02:04:40,966 iteration 3282 : loss : 0.033137, loss_ce: 0.014519
2022-01-09 02:04:42,437 iteration 3283 : loss : 0.024965, loss_ce: 0.007603
2022-01-09 02:04:43,910 iteration 3284 : loss : 0.028327, loss_ce: 0.012110
2022-01-09 02:04:45,377 iteration 3285 : loss : 0.031936, loss_ce: 0.014096
2022-01-09 02:04:46,821 iteration 3286 : loss : 0.019895, loss_ce: 0.007557
2022-01-09 02:04:48,295 iteration 3287 : loss : 0.030737, loss_ce: 0.012489
2022-01-09 02:04:49,694 iteration 3288 : loss : 0.057181, loss_ce: 0.035932
2022-01-09 02:04:51,122 iteration 3289 : loss : 0.033563, loss_ce: 0.009627
2022-01-09 02:04:52,670 iteration 3290 : loss : 0.027796, loss_ce: 0.007968
2022-01-09 02:04:54,218 iteration 3291 : loss : 0.033679, loss_ce: 0.013593
2022-01-09 02:04:55,695 iteration 3292 : loss : 0.040185, loss_ce: 0.011036
2022-01-09 02:04:57,175 iteration 3293 : loss : 0.034514, loss_ce: 0.014272
2022-01-09 02:04:58,624 iteration 3294 : loss : 0.024915, loss_ce: 0.009987
2022-01-09 02:05:00,210 iteration 3295 : loss : 0.047532, loss_ce: 0.020216
2022-01-09 02:05:01,721 iteration 3296 : loss : 0.036255, loss_ce: 0.011769
2022-01-09 02:05:03,091 iteration 3297 : loss : 0.023225, loss_ce: 0.009452
2022-01-09 02:05:04,611 iteration 3298 : loss : 0.037057, loss_ce: 0.014984
 48%|█████████████              | 194/400 [1:28:33<1:29:11, 25.98s/it]2022-01-09 02:05:06,051 iteration 3299 : loss : 0.021678, loss_ce: 0.010020
2022-01-09 02:05:07,478 iteration 3300 : loss : 0.034190, loss_ce: 0.011286
2022-01-09 02:05:09,024 iteration 3301 : loss : 0.029481, loss_ce: 0.013472
2022-01-09 02:05:10,496 iteration 3302 : loss : 0.025369, loss_ce: 0.010225
2022-01-09 02:05:11,949 iteration 3303 : loss : 0.023033, loss_ce: 0.009605
2022-01-09 02:05:13,425 iteration 3304 : loss : 0.024906, loss_ce: 0.009339
2022-01-09 02:05:14,932 iteration 3305 : loss : 0.031637, loss_ce: 0.010561
2022-01-09 02:05:16,467 iteration 3306 : loss : 0.029076, loss_ce: 0.012128
2022-01-09 02:05:17,907 iteration 3307 : loss : 0.023281, loss_ce: 0.009138
2022-01-09 02:05:19,396 iteration 3308 : loss : 0.032785, loss_ce: 0.014344
2022-01-09 02:05:20,858 iteration 3309 : loss : 0.036851, loss_ce: 0.008034
2022-01-09 02:05:22,415 iteration 3310 : loss : 0.053437, loss_ce: 0.022657
2022-01-09 02:05:23,994 iteration 3311 : loss : 0.024510, loss_ce: 0.009144
2022-01-09 02:05:25,376 iteration 3312 : loss : 0.026427, loss_ce: 0.008719
2022-01-09 02:05:26,782 iteration 3313 : loss : 0.023143, loss_ce: 0.008044
2022-01-09 02:05:28,323 iteration 3314 : loss : 0.045495, loss_ce: 0.016958
2022-01-09 02:05:28,324 Training Data Eval:
2022-01-09 02:05:35,596   Average segmentation loss on training set: 0.0189
2022-01-09 02:05:35,596 Validation Data Eval:
2022-01-09 02:05:38,093   Average segmentation loss on validation set: 0.0657
2022-01-09 02:05:39,481 iteration 3315 : loss : 0.018526, loss_ce: 0.008969
 49%|█████████████▏             | 195/400 [1:29:08<1:37:52, 28.65s/it]2022-01-09 02:05:40,937 iteration 3316 : loss : 0.017762, loss_ce: 0.006972
2022-01-09 02:05:42,358 iteration 3317 : loss : 0.035787, loss_ce: 0.014003
2022-01-09 02:05:43,939 iteration 3318 : loss : 0.026710, loss_ce: 0.011596
2022-01-09 02:05:45,373 iteration 3319 : loss : 0.027646, loss_ce: 0.009973
2022-01-09 02:05:46,844 iteration 3320 : loss : 0.021346, loss_ce: 0.008062
2022-01-09 02:05:48,343 iteration 3321 : loss : 0.023874, loss_ce: 0.007792
2022-01-09 02:05:49,844 iteration 3322 : loss : 0.038690, loss_ce: 0.017072
2022-01-09 02:05:51,270 iteration 3323 : loss : 0.028393, loss_ce: 0.008210
2022-01-09 02:05:52,871 iteration 3324 : loss : 0.033157, loss_ce: 0.015970
2022-01-09 02:05:54,306 iteration 3325 : loss : 0.031507, loss_ce: 0.010293
2022-01-09 02:05:55,831 iteration 3326 : loss : 0.026638, loss_ce: 0.010064
2022-01-09 02:05:57,429 iteration 3327 : loss : 0.055694, loss_ce: 0.021543
2022-01-09 02:05:58,898 iteration 3328 : loss : 0.039507, loss_ce: 0.019794
2022-01-09 02:06:00,316 iteration 3329 : loss : 0.029375, loss_ce: 0.013639
2022-01-09 02:06:01,756 iteration 3330 : loss : 0.046914, loss_ce: 0.013785
2022-01-09 02:06:03,232 iteration 3331 : loss : 0.028717, loss_ce: 0.011247
2022-01-09 02:06:04,556 iteration 3332 : loss : 0.019605, loss_ce: 0.008670
 49%|█████████████▏             | 196/400 [1:29:33<1:33:45, 27.58s/it]2022-01-09 02:06:06,053 iteration 3333 : loss : 0.023544, loss_ce: 0.009300
2022-01-09 02:06:07,532 iteration 3334 : loss : 0.019153, loss_ce: 0.008643
2022-01-09 02:06:08,880 iteration 3335 : loss : 0.020292, loss_ce: 0.009208
2022-01-09 02:06:10,337 iteration 3336 : loss : 0.040324, loss_ce: 0.014752
2022-01-09 02:06:11,857 iteration 3337 : loss : 0.031922, loss_ce: 0.012490
2022-01-09 02:06:13,448 iteration 3338 : loss : 0.031670, loss_ce: 0.015452
2022-01-09 02:06:14,899 iteration 3339 : loss : 0.039830, loss_ce: 0.012455
2022-01-09 02:06:16,322 iteration 3340 : loss : 0.023016, loss_ce: 0.009551
2022-01-09 02:06:17,767 iteration 3341 : loss : 0.023085, loss_ce: 0.008620
2022-01-09 02:06:19,289 iteration 3342 : loss : 0.037496, loss_ce: 0.017689
2022-01-09 02:06:20,779 iteration 3343 : loss : 0.040181, loss_ce: 0.014704
2022-01-09 02:06:22,257 iteration 3344 : loss : 0.032881, loss_ce: 0.011090
2022-01-09 02:06:23,688 iteration 3345 : loss : 0.024413, loss_ce: 0.008336
2022-01-09 02:06:25,158 iteration 3346 : loss : 0.024153, loss_ce: 0.011967
2022-01-09 02:06:26,666 iteration 3347 : loss : 0.040294, loss_ce: 0.012494
2022-01-09 02:06:28,247 iteration 3348 : loss : 0.038397, loss_ce: 0.013836
2022-01-09 02:06:29,732 iteration 3349 : loss : 0.039037, loss_ce: 0.014181
 49%|█████████████▎             | 197/400 [1:29:58<1:30:51, 26.86s/it]2022-01-09 02:06:31,206 iteration 3350 : loss : 0.023377, loss_ce: 0.009239
2022-01-09 02:06:32,601 iteration 3351 : loss : 0.029433, loss_ce: 0.013171
2022-01-09 02:06:34,027 iteration 3352 : loss : 0.028289, loss_ce: 0.012292
2022-01-09 02:06:35,517 iteration 3353 : loss : 0.028117, loss_ce: 0.011805
2022-01-09 02:06:37,004 iteration 3354 : loss : 0.025739, loss_ce: 0.008745
2022-01-09 02:06:38,559 iteration 3355 : loss : 0.028603, loss_ce: 0.014900
2022-01-09 02:06:40,083 iteration 3356 : loss : 0.035356, loss_ce: 0.010022
2022-01-09 02:06:41,623 iteration 3357 : loss : 0.028549, loss_ce: 0.010515
2022-01-09 02:06:43,181 iteration 3358 : loss : 0.033541, loss_ce: 0.009534
2022-01-09 02:06:44,685 iteration 3359 : loss : 0.023318, loss_ce: 0.009019
2022-01-09 02:06:46,279 iteration 3360 : loss : 0.034638, loss_ce: 0.014682
2022-01-09 02:06:47,678 iteration 3361 : loss : 0.030128, loss_ce: 0.008174
2022-01-09 02:06:49,093 iteration 3362 : loss : 0.027382, loss_ce: 0.011517
2022-01-09 02:06:50,492 iteration 3363 : loss : 0.026019, loss_ce: 0.012824
2022-01-09 02:06:51,983 iteration 3364 : loss : 0.029562, loss_ce: 0.011059
2022-01-09 02:06:53,384 iteration 3365 : loss : 0.024684, loss_ce: 0.012365
2022-01-09 02:06:54,834 iteration 3366 : loss : 0.028798, loss_ce: 0.011169
 50%|█████████████▎             | 198/400 [1:30:23<1:28:38, 26.33s/it]2022-01-09 02:06:56,355 iteration 3367 : loss : 0.039234, loss_ce: 0.014423
2022-01-09 02:06:57,810 iteration 3368 : loss : 0.038330, loss_ce: 0.019672
2022-01-09 02:06:59,319 iteration 3369 : loss : 0.022621, loss_ce: 0.008149
2022-01-09 02:07:00,838 iteration 3370 : loss : 0.028360, loss_ce: 0.012568
2022-01-09 02:07:02,207 iteration 3371 : loss : 0.020439, loss_ce: 0.010038
2022-01-09 02:07:03,653 iteration 3372 : loss : 0.021202, loss_ce: 0.010646
2022-01-09 02:07:05,095 iteration 3373 : loss : 0.025061, loss_ce: 0.013020
2022-01-09 02:07:06,483 iteration 3374 : loss : 0.022250, loss_ce: 0.009730
2022-01-09 02:07:07,879 iteration 3375 : loss : 0.025059, loss_ce: 0.010643
2022-01-09 02:07:09,292 iteration 3376 : loss : 0.033948, loss_ce: 0.015024
2022-01-09 02:07:10,801 iteration 3377 : loss : 0.038491, loss_ce: 0.013471
2022-01-09 02:07:12,248 iteration 3378 : loss : 0.034855, loss_ce: 0.012323
2022-01-09 02:07:13,588 iteration 3379 : loss : 0.022894, loss_ce: 0.010827
2022-01-09 02:07:15,117 iteration 3380 : loss : 0.036672, loss_ce: 0.015187
2022-01-09 02:07:16,485 iteration 3381 : loss : 0.053222, loss_ce: 0.011062
2022-01-09 02:07:17,950 iteration 3382 : loss : 0.032670, loss_ce: 0.011407
2022-01-09 02:07:19,386 iteration 3383 : loss : 0.032049, loss_ce: 0.011766
 50%|█████████████▍             | 199/400 [1:30:48<1:26:25, 25.80s/it]2022-01-09 02:07:20,835 iteration 3384 : loss : 0.026511, loss_ce: 0.009216
2022-01-09 02:07:22,350 iteration 3385 : loss : 0.031349, loss_ce: 0.014301
2022-01-09 02:07:23,880 iteration 3386 : loss : 0.041897, loss_ce: 0.022831
2022-01-09 02:07:25,379 iteration 3387 : loss : 0.037403, loss_ce: 0.015214
2022-01-09 02:07:26,757 iteration 3388 : loss : 0.027884, loss_ce: 0.010461
2022-01-09 02:07:28,246 iteration 3389 : loss : 0.048255, loss_ce: 0.019315
2022-01-09 02:07:29,617 iteration 3390 : loss : 0.036388, loss_ce: 0.009324
2022-01-09 02:07:31,207 iteration 3391 : loss : 0.035749, loss_ce: 0.010865
2022-01-09 02:07:32,713 iteration 3392 : loss : 0.026749, loss_ce: 0.010931
2022-01-09 02:07:34,209 iteration 3393 : loss : 0.028151, loss_ce: 0.010481
2022-01-09 02:07:35,553 iteration 3394 : loss : 0.026984, loss_ce: 0.009518
2022-01-09 02:07:37,161 iteration 3395 : loss : 0.035654, loss_ce: 0.012949
2022-01-09 02:07:38,648 iteration 3396 : loss : 0.035500, loss_ce: 0.011865
2022-01-09 02:07:40,099 iteration 3397 : loss : 0.025229, loss_ce: 0.010452
2022-01-09 02:07:41,612 iteration 3398 : loss : 0.030868, loss_ce: 0.010559
2022-01-09 02:07:43,106 iteration 3399 : loss : 0.021856, loss_ce: 0.009043
2022-01-09 02:07:43,106 Training Data Eval:
2022-01-09 02:07:50,499   Average segmentation loss on training set: 0.0208
2022-01-09 02:07:50,500 Validation Data Eval:
2022-01-09 02:07:53,014   Average segmentation loss on validation set: 0.0861
2022-01-09 02:07:54,476 iteration 3400 : loss : 0.034266, loss_ce: 0.011574
 50%|█████████████▌             | 200/400 [1:31:23<1:35:17, 28.59s/it]2022-01-09 02:07:56,190 iteration 3401 : loss : 0.029994, loss_ce: 0.012850
2022-01-09 02:07:57,721 iteration 3402 : loss : 0.024229, loss_ce: 0.008214
2022-01-09 02:07:59,148 iteration 3403 : loss : 0.036516, loss_ce: 0.016589
2022-01-09 02:08:00,571 iteration 3404 : loss : 0.019049, loss_ce: 0.006863
2022-01-09 02:08:02,007 iteration 3405 : loss : 0.039741, loss_ce: 0.018278
2022-01-09 02:08:03,541 iteration 3406 : loss : 0.039580, loss_ce: 0.015478
2022-01-09 02:08:05,011 iteration 3407 : loss : 0.040376, loss_ce: 0.017279
2022-01-09 02:08:06,496 iteration 3408 : loss : 0.024022, loss_ce: 0.008783
2022-01-09 02:08:07,997 iteration 3409 : loss : 0.027724, loss_ce: 0.011884
2022-01-09 02:08:09,490 iteration 3410 : loss : 0.029543, loss_ce: 0.010801
2022-01-09 02:08:11,035 iteration 3411 : loss : 0.027988, loss_ce: 0.011519
2022-01-09 02:08:12,432 iteration 3412 : loss : 0.033661, loss_ce: 0.014258
2022-01-09 02:08:13,798 iteration 3413 : loss : 0.020364, loss_ce: 0.009016
2022-01-09 02:08:15,284 iteration 3414 : loss : 0.027478, loss_ce: 0.006519
2022-01-09 02:08:16,799 iteration 3415 : loss : 0.026286, loss_ce: 0.009760
2022-01-09 02:08:18,323 iteration 3416 : loss : 0.022337, loss_ce: 0.006042
2022-01-09 02:08:19,908 iteration 3417 : loss : 0.046598, loss_ce: 0.021792
 50%|█████████████▌             | 201/400 [1:31:48<1:31:39, 27.64s/it]2022-01-09 02:08:21,459 iteration 3418 : loss : 0.024242, loss_ce: 0.010580
2022-01-09 02:08:22,975 iteration 3419 : loss : 0.041747, loss_ce: 0.015365
2022-01-09 02:08:24,568 iteration 3420 : loss : 0.034745, loss_ce: 0.010763
2022-01-09 02:08:26,071 iteration 3421 : loss : 0.033537, loss_ce: 0.012329
2022-01-09 02:08:27,466 iteration 3422 : loss : 0.024789, loss_ce: 0.008084
2022-01-09 02:08:28,934 iteration 3423 : loss : 0.026720, loss_ce: 0.012721
2022-01-09 02:08:30,396 iteration 3424 : loss : 0.033130, loss_ce: 0.014486
2022-01-09 02:08:31,795 iteration 3425 : loss : 0.026895, loss_ce: 0.012193
2022-01-09 02:08:33,272 iteration 3426 : loss : 0.028490, loss_ce: 0.009908
2022-01-09 02:08:34,766 iteration 3427 : loss : 0.027730, loss_ce: 0.008862
2022-01-09 02:08:36,258 iteration 3428 : loss : 0.038967, loss_ce: 0.013036
2022-01-09 02:08:37,787 iteration 3429 : loss : 0.027272, loss_ce: 0.010141
2022-01-09 02:08:39,321 iteration 3430 : loss : 0.035100, loss_ce: 0.013981
2022-01-09 02:08:40,692 iteration 3431 : loss : 0.024449, loss_ce: 0.009834
2022-01-09 02:08:42,101 iteration 3432 : loss : 0.023335, loss_ce: 0.007109
2022-01-09 02:08:43,512 iteration 3433 : loss : 0.026459, loss_ce: 0.008926
2022-01-09 02:08:45,034 iteration 3434 : loss : 0.043918, loss_ce: 0.014834
 50%|█████████████▋             | 202/400 [1:32:13<1:28:43, 26.89s/it]2022-01-09 02:08:46,456 iteration 3435 : loss : 0.029526, loss_ce: 0.008796
2022-01-09 02:08:47,880 iteration 3436 : loss : 0.022153, loss_ce: 0.008535
2022-01-09 02:08:49,478 iteration 3437 : loss : 0.033941, loss_ce: 0.017232
2022-01-09 02:08:51,045 iteration 3438 : loss : 0.023300, loss_ce: 0.010446
2022-01-09 02:08:52,580 iteration 3439 : loss : 0.031103, loss_ce: 0.011776
2022-01-09 02:08:54,004 iteration 3440 : loss : 0.020550, loss_ce: 0.008161
2022-01-09 02:08:55,461 iteration 3441 : loss : 0.030612, loss_ce: 0.012370
2022-01-09 02:08:57,077 iteration 3442 : loss : 0.030302, loss_ce: 0.011378
2022-01-09 02:08:58,603 iteration 3443 : loss : 0.023511, loss_ce: 0.007988
2022-01-09 02:09:00,134 iteration 3444 : loss : 0.030143, loss_ce: 0.011686
2022-01-09 02:09:01,608 iteration 3445 : loss : 0.025976, loss_ce: 0.007507
2022-01-09 02:09:03,069 iteration 3446 : loss : 0.025110, loss_ce: 0.009994
2022-01-09 02:09:04,512 iteration 3447 : loss : 0.027026, loss_ce: 0.009199
2022-01-09 02:09:06,057 iteration 3448 : loss : 0.033087, loss_ce: 0.008401
2022-01-09 02:09:07,559 iteration 3449 : loss : 0.029334, loss_ce: 0.012021
2022-01-09 02:09:09,034 iteration 3450 : loss : 0.034330, loss_ce: 0.015340
2022-01-09 02:09:10,620 iteration 3451 : loss : 0.022696, loss_ce: 0.008474
 51%|█████████████▋             | 203/400 [1:32:39<1:26:59, 26.49s/it]2022-01-09 02:09:12,151 iteration 3452 : loss : 0.026139, loss_ce: 0.009770
2022-01-09 02:09:13,591 iteration 3453 : loss : 0.028331, loss_ce: 0.012130
2022-01-09 02:09:15,010 iteration 3454 : loss : 0.021841, loss_ce: 0.007116
2022-01-09 02:09:16,578 iteration 3455 : loss : 0.031051, loss_ce: 0.013542
2022-01-09 02:09:17,994 iteration 3456 : loss : 0.021966, loss_ce: 0.006721
2022-01-09 02:09:19,599 iteration 3457 : loss : 0.031160, loss_ce: 0.010516
2022-01-09 02:09:21,120 iteration 3458 : loss : 0.024014, loss_ce: 0.009018
2022-01-09 02:09:22,587 iteration 3459 : loss : 0.024874, loss_ce: 0.009807
2022-01-09 02:09:24,040 iteration 3460 : loss : 0.025726, loss_ce: 0.012488
2022-01-09 02:09:25,504 iteration 3461 : loss : 0.023938, loss_ce: 0.008103
2022-01-09 02:09:26,941 iteration 3462 : loss : 0.021931, loss_ce: 0.009957
2022-01-09 02:09:28,454 iteration 3463 : loss : 0.029811, loss_ce: 0.011871
2022-01-09 02:09:29,845 iteration 3464 : loss : 0.023170, loss_ce: 0.008527
2022-01-09 02:09:31,369 iteration 3465 : loss : 0.029479, loss_ce: 0.008894
2022-01-09 02:09:32,859 iteration 3466 : loss : 0.018349, loss_ce: 0.006008
2022-01-09 02:09:34,329 iteration 3467 : loss : 0.026640, loss_ce: 0.013310
2022-01-09 02:09:35,784 iteration 3468 : loss : 0.026519, loss_ce: 0.010560
 51%|█████████████▊             | 204/400 [1:33:04<1:25:14, 26.09s/it]2022-01-09 02:09:37,268 iteration 3469 : loss : 0.023318, loss_ce: 0.005996
2022-01-09 02:09:38,811 iteration 3470 : loss : 0.035896, loss_ce: 0.012314
2022-01-09 02:09:40,204 iteration 3471 : loss : 0.020645, loss_ce: 0.007479
2022-01-09 02:09:41,733 iteration 3472 : loss : 0.023598, loss_ce: 0.010526
2022-01-09 02:09:43,383 iteration 3473 : loss : 0.033883, loss_ce: 0.014041
2022-01-09 02:09:44,881 iteration 3474 : loss : 0.029624, loss_ce: 0.009764
2022-01-09 02:09:46,466 iteration 3475 : loss : 0.048046, loss_ce: 0.011568
2022-01-09 02:09:47,896 iteration 3476 : loss : 0.026373, loss_ce: 0.008057
2022-01-09 02:09:49,404 iteration 3477 : loss : 0.026423, loss_ce: 0.008617
2022-01-09 02:09:50,866 iteration 3478 : loss : 0.028339, loss_ce: 0.010387
2022-01-09 02:09:52,280 iteration 3479 : loss : 0.031408, loss_ce: 0.013364
2022-01-09 02:09:53,636 iteration 3480 : loss : 0.032734, loss_ce: 0.014955
2022-01-09 02:09:55,089 iteration 3481 : loss : 0.027960, loss_ce: 0.011171
2022-01-09 02:09:56,548 iteration 3482 : loss : 0.059711, loss_ce: 0.024863
2022-01-09 02:09:58,089 iteration 3483 : loss : 0.029103, loss_ce: 0.012369
2022-01-09 02:09:59,435 iteration 3484 : loss : 0.026691, loss_ce: 0.011018
2022-01-09 02:09:59,435 Training Data Eval:
2022-01-09 02:10:06,944   Average segmentation loss on training set: 0.0200
2022-01-09 02:10:06,944 Validation Data Eval:
2022-01-09 02:10:09,540   Average segmentation loss on validation set: 0.0617
2022-01-09 02:10:15,372 Found new lowest validation loss at iteration 3484! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 02:10:16,988 iteration 3485 : loss : 0.040043, loss_ce: 0.017924
 51%|█████████████▊             | 205/400 [1:33:45<1:39:33, 30.63s/it]2022-01-09 02:10:18,516 iteration 3486 : loss : 0.033243, loss_ce: 0.014504
2022-01-09 02:10:19,904 iteration 3487 : loss : 0.022948, loss_ce: 0.009829
2022-01-09 02:10:21,335 iteration 3488 : loss : 0.041121, loss_ce: 0.017320
2022-01-09 02:10:22,780 iteration 3489 : loss : 0.036644, loss_ce: 0.013718
2022-01-09 02:10:24,267 iteration 3490 : loss : 0.046833, loss_ce: 0.019375
2022-01-09 02:10:25,802 iteration 3491 : loss : 0.033643, loss_ce: 0.013062
2022-01-09 02:10:27,179 iteration 3492 : loss : 0.030932, loss_ce: 0.011511
2022-01-09 02:10:28,788 iteration 3493 : loss : 0.038979, loss_ce: 0.015667
2022-01-09 02:10:30,296 iteration 3494 : loss : 0.021242, loss_ce: 0.005946
2022-01-09 02:10:31,744 iteration 3495 : loss : 0.030683, loss_ce: 0.009507
2022-01-09 02:10:33,250 iteration 3496 : loss : 0.021500, loss_ce: 0.008501
2022-01-09 02:10:34,746 iteration 3497 : loss : 0.037403, loss_ce: 0.014884
2022-01-09 02:10:36,363 iteration 3498 : loss : 0.029891, loss_ce: 0.011267
2022-01-09 02:10:37,790 iteration 3499 : loss : 0.024334, loss_ce: 0.009484
2022-01-09 02:10:39,144 iteration 3500 : loss : 0.019237, loss_ce: 0.006428
2022-01-09 02:10:40,548 iteration 3501 : loss : 0.027249, loss_ce: 0.009920
2022-01-09 02:10:42,040 iteration 3502 : loss : 0.025245, loss_ce: 0.009672
 52%|█████████████▉             | 206/400 [1:34:10<1:33:37, 28.96s/it]2022-01-09 02:10:43,574 iteration 3503 : loss : 0.032134, loss_ce: 0.012029
2022-01-09 02:10:45,000 iteration 3504 : loss : 0.024310, loss_ce: 0.007442
2022-01-09 02:10:46,325 iteration 3505 : loss : 0.019989, loss_ce: 0.009724
2022-01-09 02:10:47,807 iteration 3506 : loss : 0.032375, loss_ce: 0.010369
2022-01-09 02:10:49,243 iteration 3507 : loss : 0.032510, loss_ce: 0.009031
2022-01-09 02:10:50,861 iteration 3508 : loss : 0.048134, loss_ce: 0.019824
2022-01-09 02:10:52,348 iteration 3509 : loss : 0.023075, loss_ce: 0.009076
2022-01-09 02:10:53,744 iteration 3510 : loss : 0.028560, loss_ce: 0.012428
2022-01-09 02:10:55,131 iteration 3511 : loss : 0.023105, loss_ce: 0.010471
2022-01-09 02:10:56,684 iteration 3512 : loss : 0.030900, loss_ce: 0.009888
2022-01-09 02:10:58,155 iteration 3513 : loss : 0.034504, loss_ce: 0.010417
2022-01-09 02:10:59,784 iteration 3514 : loss : 0.034620, loss_ce: 0.016142
2022-01-09 02:11:01,226 iteration 3515 : loss : 0.033301, loss_ce: 0.011915
2022-01-09 02:11:02,656 iteration 3516 : loss : 0.025243, loss_ce: 0.010080
2022-01-09 02:11:04,092 iteration 3517 : loss : 0.024085, loss_ce: 0.008817
2022-01-09 02:11:05,531 iteration 3518 : loss : 0.024332, loss_ce: 0.012373
2022-01-09 02:11:07,015 iteration 3519 : loss : 0.033431, loss_ce: 0.010936
 52%|█████████████▉             | 207/400 [1:34:35<1:29:17, 27.76s/it]2022-01-09 02:11:08,592 iteration 3520 : loss : 0.027120, loss_ce: 0.010830
2022-01-09 02:11:10,018 iteration 3521 : loss : 0.038484, loss_ce: 0.017435
2022-01-09 02:11:11,389 iteration 3522 : loss : 0.022750, loss_ce: 0.010012
2022-01-09 02:11:12,825 iteration 3523 : loss : 0.021956, loss_ce: 0.007274
2022-01-09 02:11:14,322 iteration 3524 : loss : 0.023456, loss_ce: 0.009360
2022-01-09 02:11:15,910 iteration 3525 : loss : 0.023643, loss_ce: 0.008891
2022-01-09 02:11:17,412 iteration 3526 : loss : 0.029990, loss_ce: 0.010276
2022-01-09 02:11:18,937 iteration 3527 : loss : 0.039960, loss_ce: 0.012270
2022-01-09 02:11:20,577 iteration 3528 : loss : 0.027656, loss_ce: 0.011745
2022-01-09 02:11:22,090 iteration 3529 : loss : 0.037029, loss_ce: 0.010967
2022-01-09 02:11:23,592 iteration 3530 : loss : 0.017857, loss_ce: 0.006831
2022-01-09 02:11:24,992 iteration 3531 : loss : 0.025903, loss_ce: 0.007744
2022-01-09 02:11:26,555 iteration 3532 : loss : 0.034909, loss_ce: 0.015298
2022-01-09 02:11:28,000 iteration 3533 : loss : 0.044589, loss_ce: 0.012507
2022-01-09 02:11:29,499 iteration 3534 : loss : 0.031109, loss_ce: 0.012202
2022-01-09 02:11:31,071 iteration 3535 : loss : 0.030183, loss_ce: 0.012814
2022-01-09 02:11:32,595 iteration 3536 : loss : 0.024345, loss_ce: 0.010111
 52%|██████████████             | 208/400 [1:35:01<1:26:44, 27.10s/it]2022-01-09 02:11:34,013 iteration 3537 : loss : 0.022276, loss_ce: 0.008868
2022-01-09 02:11:35,381 iteration 3538 : loss : 0.024783, loss_ce: 0.010862
2022-01-09 02:11:36,772 iteration 3539 : loss : 0.018716, loss_ce: 0.008975
2022-01-09 02:11:38,086 iteration 3540 : loss : 0.026071, loss_ce: 0.008100
2022-01-09 02:11:39,650 iteration 3541 : loss : 0.033284, loss_ce: 0.012066
2022-01-09 02:11:41,112 iteration 3542 : loss : 0.025421, loss_ce: 0.011001
2022-01-09 02:11:42,732 iteration 3543 : loss : 0.058610, loss_ce: 0.018105
2022-01-09 02:11:44,095 iteration 3544 : loss : 0.019519, loss_ce: 0.007515
2022-01-09 02:11:45,550 iteration 3545 : loss : 0.024370, loss_ce: 0.010333
2022-01-09 02:11:46,922 iteration 3546 : loss : 0.022135, loss_ce: 0.008203
2022-01-09 02:11:48,385 iteration 3547 : loss : 0.029520, loss_ce: 0.015390
2022-01-09 02:11:49,773 iteration 3548 : loss : 0.024509, loss_ce: 0.008670
2022-01-09 02:11:51,292 iteration 3549 : loss : 0.024746, loss_ce: 0.010134
2022-01-09 02:11:52,806 iteration 3550 : loss : 0.023244, loss_ce: 0.010547
2022-01-09 02:11:54,361 iteration 3551 : loss : 0.028328, loss_ce: 0.008855
2022-01-09 02:11:55,903 iteration 3552 : loss : 0.055922, loss_ce: 0.024244
2022-01-09 02:11:57,452 iteration 3553 : loss : 0.034062, loss_ce: 0.015730
 52%|██████████████             | 209/400 [1:35:26<1:24:08, 26.43s/it]2022-01-09 02:11:58,996 iteration 3554 : loss : 0.023604, loss_ce: 0.010464
2022-01-09 02:12:00,423 iteration 3555 : loss : 0.023057, loss_ce: 0.008144
2022-01-09 02:12:01,855 iteration 3556 : loss : 0.023994, loss_ce: 0.010386
2022-01-09 02:12:03,432 iteration 3557 : loss : 0.026910, loss_ce: 0.007230
2022-01-09 02:12:04,826 iteration 3558 : loss : 0.043272, loss_ce: 0.013867
2022-01-09 02:12:06,268 iteration 3559 : loss : 0.020880, loss_ce: 0.006464
2022-01-09 02:12:07,584 iteration 3560 : loss : 0.025128, loss_ce: 0.012774
2022-01-09 02:12:09,147 iteration 3561 : loss : 0.026025, loss_ce: 0.010255
2022-01-09 02:12:10,559 iteration 3562 : loss : 0.021867, loss_ce: 0.008382
2022-01-09 02:12:12,064 iteration 3563 : loss : 0.023151, loss_ce: 0.009111
2022-01-09 02:12:13,539 iteration 3564 : loss : 0.032844, loss_ce: 0.012764
2022-01-09 02:12:15,164 iteration 3565 : loss : 0.036805, loss_ce: 0.013452
2022-01-09 02:12:16,666 iteration 3566 : loss : 0.029654, loss_ce: 0.012027
2022-01-09 02:12:18,260 iteration 3567 : loss : 0.030902, loss_ce: 0.013424
2022-01-09 02:12:19,826 iteration 3568 : loss : 0.036914, loss_ce: 0.013466
2022-01-09 02:12:21,375 iteration 3569 : loss : 0.031381, loss_ce: 0.008745
2022-01-09 02:12:21,375 Training Data Eval:
2022-01-09 02:12:28,826   Average segmentation loss on training set: 0.0205
2022-01-09 02:12:28,827 Validation Data Eval:
2022-01-09 02:12:31,438   Average segmentation loss on validation set: 0.0674
2022-01-09 02:12:32,967 iteration 3570 : loss : 0.037103, loss_ce: 0.013904
 52%|██████████████▏            | 210/400 [1:36:01<1:32:19, 29.16s/it]2022-01-09 02:12:34,524 iteration 3571 : loss : 0.027575, loss_ce: 0.010951
2022-01-09 02:12:36,009 iteration 3572 : loss : 0.025301, loss_ce: 0.007248
2022-01-09 02:12:37,493 iteration 3573 : loss : 0.038776, loss_ce: 0.015227
2022-01-09 02:12:38,932 iteration 3574 : loss : 0.017584, loss_ce: 0.007439
2022-01-09 02:12:40,367 iteration 3575 : loss : 0.022644, loss_ce: 0.008654
2022-01-09 02:12:41,821 iteration 3576 : loss : 0.035009, loss_ce: 0.012815
2022-01-09 02:12:43,320 iteration 3577 : loss : 0.027123, loss_ce: 0.011538
2022-01-09 02:12:44,856 iteration 3578 : loss : 0.034107, loss_ce: 0.015012
2022-01-09 02:12:46,287 iteration 3579 : loss : 0.030986, loss_ce: 0.013696
2022-01-09 02:12:47,791 iteration 3580 : loss : 0.021921, loss_ce: 0.007515
2022-01-09 02:12:49,209 iteration 3581 : loss : 0.024564, loss_ce: 0.005976
2022-01-09 02:12:50,655 iteration 3582 : loss : 0.023748, loss_ce: 0.009765
2022-01-09 02:12:52,166 iteration 3583 : loss : 0.046569, loss_ce: 0.018425
2022-01-09 02:12:53,589 iteration 3584 : loss : 0.027659, loss_ce: 0.011103
2022-01-09 02:12:55,036 iteration 3585 : loss : 0.017351, loss_ce: 0.006417
2022-01-09 02:12:56,417 iteration 3586 : loss : 0.025489, loss_ce: 0.009421
2022-01-09 02:12:57,862 iteration 3587 : loss : 0.027127, loss_ce: 0.011444
 53%|██████████████▏            | 211/400 [1:36:26<1:27:48, 27.88s/it]2022-01-09 02:12:59,291 iteration 3588 : loss : 0.027779, loss_ce: 0.010006
2022-01-09 02:13:00,814 iteration 3589 : loss : 0.034741, loss_ce: 0.011798
2022-01-09 02:13:02,286 iteration 3590 : loss : 0.030435, loss_ce: 0.012723
2022-01-09 02:13:03,749 iteration 3591 : loss : 0.024802, loss_ce: 0.008647
2022-01-09 02:13:05,306 iteration 3592 : loss : 0.022246, loss_ce: 0.009008
2022-01-09 02:13:06,817 iteration 3593 : loss : 0.046524, loss_ce: 0.011623
2022-01-09 02:13:08,324 iteration 3594 : loss : 0.027167, loss_ce: 0.007459
2022-01-09 02:13:09,972 iteration 3595 : loss : 0.051223, loss_ce: 0.011682
2022-01-09 02:13:11,365 iteration 3596 : loss : 0.027928, loss_ce: 0.011881
2022-01-09 02:13:12,965 iteration 3597 : loss : 0.037256, loss_ce: 0.013437
2022-01-09 02:13:14,463 iteration 3598 : loss : 0.037055, loss_ce: 0.017638
2022-01-09 02:13:15,993 iteration 3599 : loss : 0.039723, loss_ce: 0.016277
2022-01-09 02:13:17,479 iteration 3600 : loss : 0.024344, loss_ce: 0.011097
2022-01-09 02:13:18,984 iteration 3601 : loss : 0.033452, loss_ce: 0.013369
2022-01-09 02:13:20,463 iteration 3602 : loss : 0.042247, loss_ce: 0.030655
2022-01-09 02:13:21,912 iteration 3603 : loss : 0.021195, loss_ce: 0.008100
2022-01-09 02:13:23,337 iteration 3604 : loss : 0.026736, loss_ce: 0.010706
 53%|██████████████▎            | 212/400 [1:36:52<1:25:06, 27.16s/it]2022-01-09 02:13:24,875 iteration 3605 : loss : 0.024378, loss_ce: 0.009510
2022-01-09 02:13:26,360 iteration 3606 : loss : 0.037503, loss_ce: 0.014145
2022-01-09 02:13:27,769 iteration 3607 : loss : 0.028081, loss_ce: 0.009878
2022-01-09 02:13:29,194 iteration 3608 : loss : 0.025445, loss_ce: 0.012006
2022-01-09 02:13:30,723 iteration 3609 : loss : 0.032581, loss_ce: 0.012039
2022-01-09 02:13:32,160 iteration 3610 : loss : 0.032948, loss_ce: 0.016181
2022-01-09 02:13:33,686 iteration 3611 : loss : 0.033224, loss_ce: 0.014716
2022-01-09 02:13:35,231 iteration 3612 : loss : 0.032713, loss_ce: 0.012177
2022-01-09 02:13:36,719 iteration 3613 : loss : 0.025173, loss_ce: 0.009303
2022-01-09 02:13:38,259 iteration 3614 : loss : 0.029465, loss_ce: 0.009869
2022-01-09 02:13:39,722 iteration 3615 : loss : 0.030388, loss_ce: 0.010205
2022-01-09 02:13:41,316 iteration 3616 : loss : 0.032195, loss_ce: 0.011378
2022-01-09 02:13:42,794 iteration 3617 : loss : 0.033929, loss_ce: 0.012584
2022-01-09 02:13:44,277 iteration 3618 : loss : 0.027887, loss_ce: 0.012822
2022-01-09 02:13:45,791 iteration 3619 : loss : 0.028753, loss_ce: 0.012071
2022-01-09 02:13:47,328 iteration 3620 : loss : 0.030118, loss_ce: 0.011586
2022-01-09 02:13:48,810 iteration 3621 : loss : 0.028154, loss_ce: 0.012254
 53%|██████████████▍            | 213/400 [1:37:17<1:23:03, 26.65s/it]2022-01-09 02:13:50,438 iteration 3622 : loss : 0.036865, loss_ce: 0.013663
2022-01-09 02:13:51,919 iteration 3623 : loss : 0.024572, loss_ce: 0.009987
2022-01-09 02:13:53,481 iteration 3624 : loss : 0.027312, loss_ce: 0.009140
2022-01-09 02:13:55,037 iteration 3625 : loss : 0.043343, loss_ce: 0.019700
2022-01-09 02:13:56,480 iteration 3626 : loss : 0.026794, loss_ce: 0.010781
2022-01-09 02:13:57,973 iteration 3627 : loss : 0.025118, loss_ce: 0.012862
2022-01-09 02:13:59,468 iteration 3628 : loss : 0.028976, loss_ce: 0.009357
2022-01-09 02:14:01,039 iteration 3629 : loss : 0.028959, loss_ce: 0.012831
2022-01-09 02:14:02,514 iteration 3630 : loss : 0.024378, loss_ce: 0.010903
2022-01-09 02:14:03,894 iteration 3631 : loss : 0.024297, loss_ce: 0.011218
2022-01-09 02:14:05,295 iteration 3632 : loss : 0.020838, loss_ce: 0.009446
2022-01-09 02:14:06,844 iteration 3633 : loss : 0.053230, loss_ce: 0.019759
2022-01-09 02:14:08,299 iteration 3634 : loss : 0.029648, loss_ce: 0.011307
2022-01-09 02:14:09,713 iteration 3635 : loss : 0.027150, loss_ce: 0.010121
2022-01-09 02:14:11,151 iteration 3636 : loss : 0.026279, loss_ce: 0.011052
2022-01-09 02:14:12,711 iteration 3637 : loss : 0.045335, loss_ce: 0.012440
2022-01-09 02:14:14,115 iteration 3638 : loss : 0.027241, loss_ce: 0.009182
 54%|██████████████▍            | 214/400 [1:37:43<1:21:22, 26.25s/it]2022-01-09 02:14:15,620 iteration 3639 : loss : 0.023735, loss_ce: 0.010471
2022-01-09 02:14:17,146 iteration 3640 : loss : 0.021331, loss_ce: 0.007774
2022-01-09 02:14:18,632 iteration 3641 : loss : 0.030745, loss_ce: 0.009802
2022-01-09 02:14:20,185 iteration 3642 : loss : 0.032160, loss_ce: 0.014885
2022-01-09 02:14:21,633 iteration 3643 : loss : 0.030172, loss_ce: 0.013393
2022-01-09 02:14:23,084 iteration 3644 : loss : 0.029389, loss_ce: 0.015038
2022-01-09 02:14:24,507 iteration 3645 : loss : 0.021681, loss_ce: 0.008030
2022-01-09 02:14:25,982 iteration 3646 : loss : 0.023780, loss_ce: 0.007876
2022-01-09 02:14:27,494 iteration 3647 : loss : 0.035196, loss_ce: 0.013696
2022-01-09 02:14:29,105 iteration 3648 : loss : 0.033826, loss_ce: 0.012784
2022-01-09 02:14:30,596 iteration 3649 : loss : 0.037795, loss_ce: 0.011908
2022-01-09 02:14:32,065 iteration 3650 : loss : 0.028013, loss_ce: 0.008833
2022-01-09 02:14:33,543 iteration 3651 : loss : 0.023887, loss_ce: 0.009963
2022-01-09 02:14:34,931 iteration 3652 : loss : 0.023963, loss_ce: 0.010282
2022-01-09 02:14:36,438 iteration 3653 : loss : 0.036580, loss_ce: 0.009457
2022-01-09 02:14:37,913 iteration 3654 : loss : 0.029415, loss_ce: 0.010558
2022-01-09 02:14:37,913 Training Data Eval:
2022-01-09 02:14:45,169   Average segmentation loss on training set: 0.0176
2022-01-09 02:14:45,170 Validation Data Eval:
2022-01-09 02:14:47,651   Average segmentation loss on validation set: 0.0770
2022-01-09 02:14:48,989 iteration 3655 : loss : 0.025114, loss_ce: 0.008770
 54%|██████████████▌            | 215/400 [1:38:17<1:28:54, 28.84s/it]2022-01-09 02:14:50,588 iteration 3656 : loss : 0.059739, loss_ce: 0.016270
2022-01-09 02:14:52,004 iteration 3657 : loss : 0.026978, loss_ce: 0.013953
2022-01-09 02:14:53,469 iteration 3658 : loss : 0.028380, loss_ce: 0.012210
2022-01-09 02:14:54,987 iteration 3659 : loss : 0.023739, loss_ce: 0.008159
2022-01-09 02:14:56,586 iteration 3660 : loss : 0.037043, loss_ce: 0.012790
2022-01-09 02:14:58,196 iteration 3661 : loss : 0.030316, loss_ce: 0.014111
2022-01-09 02:14:59,564 iteration 3662 : loss : 0.025149, loss_ce: 0.009251
2022-01-09 02:15:00,989 iteration 3663 : loss : 0.022624, loss_ce: 0.007348
2022-01-09 02:15:02,436 iteration 3664 : loss : 0.029993, loss_ce: 0.008499
2022-01-09 02:15:03,901 iteration 3665 : loss : 0.020908, loss_ce: 0.008777
2022-01-09 02:15:05,508 iteration 3666 : loss : 0.061271, loss_ce: 0.029106
2022-01-09 02:15:07,030 iteration 3667 : loss : 0.039735, loss_ce: 0.015722
2022-01-09 02:15:08,565 iteration 3668 : loss : 0.027741, loss_ce: 0.010528
2022-01-09 02:15:10,052 iteration 3669 : loss : 0.029265, loss_ce: 0.013018
2022-01-09 02:15:11,490 iteration 3670 : loss : 0.036768, loss_ce: 0.011084
2022-01-09 02:15:13,011 iteration 3671 : loss : 0.037662, loss_ce: 0.014671
2022-01-09 02:15:14,419 iteration 3672 : loss : 0.024915, loss_ce: 0.010282
 54%|██████████████▌            | 216/400 [1:38:43<1:25:17, 27.81s/it]2022-01-09 02:15:15,923 iteration 3673 : loss : 0.028914, loss_ce: 0.013945
2022-01-09 02:15:17,391 iteration 3674 : loss : 0.033479, loss_ce: 0.014237
2022-01-09 02:15:18,827 iteration 3675 : loss : 0.023752, loss_ce: 0.012200
2022-01-09 02:15:20,259 iteration 3676 : loss : 0.027907, loss_ce: 0.009279
2022-01-09 02:15:21,698 iteration 3677 : loss : 0.023151, loss_ce: 0.011483
2022-01-09 02:15:23,265 iteration 3678 : loss : 0.051822, loss_ce: 0.019429
2022-01-09 02:15:24,710 iteration 3679 : loss : 0.025737, loss_ce: 0.010623
2022-01-09 02:15:26,139 iteration 3680 : loss : 0.032326, loss_ce: 0.009997
2022-01-09 02:15:27,669 iteration 3681 : loss : 0.028290, loss_ce: 0.009725
2022-01-09 02:15:29,101 iteration 3682 : loss : 0.035878, loss_ce: 0.015918
2022-01-09 02:15:30,622 iteration 3683 : loss : 0.056543, loss_ce: 0.008270
2022-01-09 02:15:32,048 iteration 3684 : loss : 0.017327, loss_ce: 0.005807
2022-01-09 02:15:33,479 iteration 3685 : loss : 0.038890, loss_ce: 0.015499
2022-01-09 02:15:34,968 iteration 3686 : loss : 0.030951, loss_ce: 0.011504
2022-01-09 02:15:36,437 iteration 3687 : loss : 0.027379, loss_ce: 0.008768
2022-01-09 02:15:37,886 iteration 3688 : loss : 0.050123, loss_ce: 0.023608
2022-01-09 02:15:39,262 iteration 3689 : loss : 0.028107, loss_ce: 0.010213
 54%|██████████████▋            | 217/400 [1:39:08<1:22:07, 26.93s/it]2022-01-09 02:15:40,726 iteration 3690 : loss : 0.025071, loss_ce: 0.006359
2022-01-09 02:15:42,290 iteration 3691 : loss : 0.036953, loss_ce: 0.008133
2022-01-09 02:15:43,762 iteration 3692 : loss : 0.026965, loss_ce: 0.012693
2022-01-09 02:15:45,317 iteration 3693 : loss : 0.043659, loss_ce: 0.016778
2022-01-09 02:15:46,802 iteration 3694 : loss : 0.020850, loss_ce: 0.009247
2022-01-09 02:15:48,207 iteration 3695 : loss : 0.026109, loss_ce: 0.011700
2022-01-09 02:15:49,725 iteration 3696 : loss : 0.041161, loss_ce: 0.019968
2022-01-09 02:15:51,169 iteration 3697 : loss : 0.021441, loss_ce: 0.007034
2022-01-09 02:15:52,636 iteration 3698 : loss : 0.030661, loss_ce: 0.010823
2022-01-09 02:15:54,192 iteration 3699 : loss : 0.030411, loss_ce: 0.011642
2022-01-09 02:15:55,659 iteration 3700 : loss : 0.026480, loss_ce: 0.012442
2022-01-09 02:15:57,092 iteration 3701 : loss : 0.026102, loss_ce: 0.010153
2022-01-09 02:15:58,509 iteration 3702 : loss : 0.052127, loss_ce: 0.021748
2022-01-09 02:16:00,116 iteration 3703 : loss : 0.047861, loss_ce: 0.019156
2022-01-09 02:16:01,532 iteration 3704 : loss : 0.031186, loss_ce: 0.009863
2022-01-09 02:16:02,958 iteration 3705 : loss : 0.023499, loss_ce: 0.008134
2022-01-09 02:16:04,349 iteration 3706 : loss : 0.026812, loss_ce: 0.007221
 55%|██████████████▋            | 218/400 [1:39:33<1:19:59, 26.37s/it]2022-01-09 02:16:05,862 iteration 3707 : loss : 0.036335, loss_ce: 0.014379
2022-01-09 02:16:07,334 iteration 3708 : loss : 0.022100, loss_ce: 0.008838
2022-01-09 02:16:08,794 iteration 3709 : loss : 0.024650, loss_ce: 0.008336
2022-01-09 02:16:10,318 iteration 3710 : loss : 0.040662, loss_ce: 0.014116
2022-01-09 02:16:11,752 iteration 3711 : loss : 0.026048, loss_ce: 0.010401
2022-01-09 02:16:13,234 iteration 3712 : loss : 0.041858, loss_ce: 0.014662
2022-01-09 02:16:14,634 iteration 3713 : loss : 0.029716, loss_ce: 0.011689
2022-01-09 02:16:16,045 iteration 3714 : loss : 0.020655, loss_ce: 0.007392
2022-01-09 02:16:17,510 iteration 3715 : loss : 0.028173, loss_ce: 0.010420
2022-01-09 02:16:18,916 iteration 3716 : loss : 0.037046, loss_ce: 0.014487
2022-01-09 02:16:20,390 iteration 3717 : loss : 0.027505, loss_ce: 0.011872
2022-01-09 02:16:21,927 iteration 3718 : loss : 0.027835, loss_ce: 0.013403
2022-01-09 02:16:23,313 iteration 3719 : loss : 0.022424, loss_ce: 0.006106
2022-01-09 02:16:24,796 iteration 3720 : loss : 0.026078, loss_ce: 0.014828
2022-01-09 02:16:26,263 iteration 3721 : loss : 0.026972, loss_ce: 0.011674
2022-01-09 02:16:27,737 iteration 3722 : loss : 0.021677, loss_ce: 0.007754
2022-01-09 02:16:29,148 iteration 3723 : loss : 0.034849, loss_ce: 0.010677
 55%|██████████████▊            | 219/400 [1:39:58<1:18:07, 25.90s/it]2022-01-09 02:16:30,575 iteration 3724 : loss : 0.019085, loss_ce: 0.008649
2022-01-09 02:16:31,981 iteration 3725 : loss : 0.026278, loss_ce: 0.011364
2022-01-09 02:16:33,521 iteration 3726 : loss : 0.037140, loss_ce: 0.018148
2022-01-09 02:16:34,994 iteration 3727 : loss : 0.026732, loss_ce: 0.011469
2022-01-09 02:16:36,485 iteration 3728 : loss : 0.033542, loss_ce: 0.011010
2022-01-09 02:16:37,888 iteration 3729 : loss : 0.021991, loss_ce: 0.006915
2022-01-09 02:16:39,331 iteration 3730 : loss : 0.025264, loss_ce: 0.010105
2022-01-09 02:16:40,757 iteration 3731 : loss : 0.024926, loss_ce: 0.008459
2022-01-09 02:16:42,261 iteration 3732 : loss : 0.024956, loss_ce: 0.008777
2022-01-09 02:16:43,661 iteration 3733 : loss : 0.026578, loss_ce: 0.010103
2022-01-09 02:16:45,168 iteration 3734 : loss : 0.027715, loss_ce: 0.012557
2022-01-09 02:16:46,628 iteration 3735 : loss : 0.026208, loss_ce: 0.006926
2022-01-09 02:16:48,157 iteration 3736 : loss : 0.038816, loss_ce: 0.018448
2022-01-09 02:16:49,563 iteration 3737 : loss : 0.024195, loss_ce: 0.007963
2022-01-09 02:16:51,011 iteration 3738 : loss : 0.018527, loss_ce: 0.006677
2022-01-09 02:16:52,405 iteration 3739 : loss : 0.022888, loss_ce: 0.009767
2022-01-09 02:16:52,405 Training Data Eval:
2022-01-09 02:16:59,878   Average segmentation loss on training set: 0.0167
2022-01-09 02:16:59,878 Validation Data Eval:
2022-01-09 02:17:02,487   Average segmentation loss on validation set: 0.0736
2022-01-09 02:17:04,177 iteration 3740 : loss : 0.030898, loss_ce: 0.014833
 55%|██████████████▊            | 220/400 [1:40:33<1:25:54, 28.64s/it]2022-01-09 02:17:05,876 iteration 3741 : loss : 0.032856, loss_ce: 0.009249
2022-01-09 02:17:07,333 iteration 3742 : loss : 0.031695, loss_ce: 0.015896
2022-01-09 02:17:08,792 iteration 3743 : loss : 0.033011, loss_ce: 0.012042
2022-01-09 02:17:10,316 iteration 3744 : loss : 0.030556, loss_ce: 0.011849
2022-01-09 02:17:11,868 iteration 3745 : loss : 0.025214, loss_ce: 0.007818
2022-01-09 02:17:13,432 iteration 3746 : loss : 0.032995, loss_ce: 0.011976
2022-01-09 02:17:14,888 iteration 3747 : loss : 0.026946, loss_ce: 0.009343
2022-01-09 02:17:16,321 iteration 3748 : loss : 0.020371, loss_ce: 0.009722
2022-01-09 02:17:17,855 iteration 3749 : loss : 0.033542, loss_ce: 0.010922
2022-01-09 02:17:19,422 iteration 3750 : loss : 0.038554, loss_ce: 0.014327
2022-01-09 02:17:20,922 iteration 3751 : loss : 0.033236, loss_ce: 0.009232
2022-01-09 02:17:22,363 iteration 3752 : loss : 0.030113, loss_ce: 0.008486
2022-01-09 02:17:23,777 iteration 3753 : loss : 0.029245, loss_ce: 0.008751
2022-01-09 02:17:25,262 iteration 3754 : loss : 0.020479, loss_ce: 0.007343
2022-01-09 02:17:26,813 iteration 3755 : loss : 0.025569, loss_ce: 0.011486
2022-01-09 02:17:28,211 iteration 3756 : loss : 0.028636, loss_ce: 0.012856
2022-01-09 02:17:29,650 iteration 3757 : loss : 0.027689, loss_ce: 0.011963
 55%|██████████████▉            | 221/400 [1:40:58<1:22:36, 27.69s/it]2022-01-09 02:17:31,308 iteration 3758 : loss : 0.025920, loss_ce: 0.009156
2022-01-09 02:17:32,743 iteration 3759 : loss : 0.022915, loss_ce: 0.007743
2022-01-09 02:17:34,215 iteration 3760 : loss : 0.021350, loss_ce: 0.008880
2022-01-09 02:17:35,744 iteration 3761 : loss : 0.035463, loss_ce: 0.013546
2022-01-09 02:17:37,299 iteration 3762 : loss : 0.033873, loss_ce: 0.020444
2022-01-09 02:17:38,896 iteration 3763 : loss : 0.025540, loss_ce: 0.008969
2022-01-09 02:17:40,358 iteration 3764 : loss : 0.030559, loss_ce: 0.010536
2022-01-09 02:17:41,766 iteration 3765 : loss : 0.023237, loss_ce: 0.009680
2022-01-09 02:17:43,298 iteration 3766 : loss : 0.033697, loss_ce: 0.011567
2022-01-09 02:17:44,766 iteration 3767 : loss : 0.019787, loss_ce: 0.007345
2022-01-09 02:17:46,170 iteration 3768 : loss : 0.028178, loss_ce: 0.008681
2022-01-09 02:17:47,585 iteration 3769 : loss : 0.019869, loss_ce: 0.008832
2022-01-09 02:17:49,092 iteration 3770 : loss : 0.040416, loss_ce: 0.007809
2022-01-09 02:17:50,727 iteration 3771 : loss : 0.027521, loss_ce: 0.009785
2022-01-09 02:17:52,272 iteration 3772 : loss : 0.027577, loss_ce: 0.007397
2022-01-09 02:17:53,773 iteration 3773 : loss : 0.020787, loss_ce: 0.008180
2022-01-09 02:17:55,327 iteration 3774 : loss : 0.037080, loss_ce: 0.018450
 56%|██████████████▉            | 222/400 [1:41:24<1:20:21, 27.09s/it]2022-01-09 02:17:56,830 iteration 3775 : loss : 0.027176, loss_ce: 0.010858
2022-01-09 02:17:58,183 iteration 3776 : loss : 0.028036, loss_ce: 0.011316
2022-01-09 02:17:59,618 iteration 3777 : loss : 0.021704, loss_ce: 0.006091
2022-01-09 02:18:01,005 iteration 3778 : loss : 0.020647, loss_ce: 0.008120
2022-01-09 02:18:02,410 iteration 3779 : loss : 0.021735, loss_ce: 0.009836
2022-01-09 02:18:03,887 iteration 3780 : loss : 0.030597, loss_ce: 0.011450
2022-01-09 02:18:05,361 iteration 3781 : loss : 0.027073, loss_ce: 0.011404
2022-01-09 02:18:06,874 iteration 3782 : loss : 0.058831, loss_ce: 0.010439
2022-01-09 02:18:08,364 iteration 3783 : loss : 0.022235, loss_ce: 0.009707
2022-01-09 02:18:09,869 iteration 3784 : loss : 0.021393, loss_ce: 0.009702
2022-01-09 02:18:11,298 iteration 3785 : loss : 0.022725, loss_ce: 0.009459
2022-01-09 02:18:12,723 iteration 3786 : loss : 0.022010, loss_ce: 0.008917
2022-01-09 02:18:14,132 iteration 3787 : loss : 0.025639, loss_ce: 0.009325
2022-01-09 02:18:15,681 iteration 3788 : loss : 0.032190, loss_ce: 0.009749
2022-01-09 02:18:17,140 iteration 3789 : loss : 0.023428, loss_ce: 0.006871
2022-01-09 02:18:18,652 iteration 3790 : loss : 0.036124, loss_ce: 0.015311
2022-01-09 02:18:20,164 iteration 3791 : loss : 0.026915, loss_ce: 0.011473
 56%|███████████████            | 223/400 [1:41:49<1:17:55, 26.41s/it]2022-01-09 02:18:21,639 iteration 3792 : loss : 0.020071, loss_ce: 0.006854
2022-01-09 02:18:23,168 iteration 3793 : loss : 0.034865, loss_ce: 0.015166
2022-01-09 02:18:24,671 iteration 3794 : loss : 0.032485, loss_ce: 0.012273
2022-01-09 02:18:26,142 iteration 3795 : loss : 0.018443, loss_ce: 0.006035
2022-01-09 02:18:27,491 iteration 3796 : loss : 0.018572, loss_ce: 0.006633
2022-01-09 02:18:28,925 iteration 3797 : loss : 0.025294, loss_ce: 0.008561
2022-01-09 02:18:30,299 iteration 3798 : loss : 0.020105, loss_ce: 0.007488
2022-01-09 02:18:31,736 iteration 3799 : loss : 0.019519, loss_ce: 0.007332
2022-01-09 02:18:33,168 iteration 3800 : loss : 0.017348, loss_ce: 0.004947
2022-01-09 02:18:34,603 iteration 3801 : loss : 0.037370, loss_ce: 0.008900
2022-01-09 02:18:36,014 iteration 3802 : loss : 0.026069, loss_ce: 0.013518
2022-01-09 02:18:37,461 iteration 3803 : loss : 0.018954, loss_ce: 0.006578
2022-01-09 02:18:38,909 iteration 3804 : loss : 0.028197, loss_ce: 0.012349
2022-01-09 02:18:40,329 iteration 3805 : loss : 0.031561, loss_ce: 0.011607
2022-01-09 02:18:41,752 iteration 3806 : loss : 0.032270, loss_ce: 0.018775
2022-01-09 02:18:43,271 iteration 3807 : loss : 0.028545, loss_ce: 0.010388
2022-01-09 02:18:44,760 iteration 3808 : loss : 0.028918, loss_ce: 0.014427
 56%|███████████████            | 224/400 [1:42:13<1:15:52, 25.87s/it]2022-01-09 02:18:46,224 iteration 3809 : loss : 0.018462, loss_ce: 0.006447
2022-01-09 02:18:47,668 iteration 3810 : loss : 0.023227, loss_ce: 0.009355
2022-01-09 02:18:49,141 iteration 3811 : loss : 0.023550, loss_ce: 0.006516
2022-01-09 02:18:50,637 iteration 3812 : loss : 0.027158, loss_ce: 0.013821
2022-01-09 02:18:52,097 iteration 3813 : loss : 0.021609, loss_ce: 0.009904
2022-01-09 02:18:53,634 iteration 3814 : loss : 0.033291, loss_ce: 0.012800
2022-01-09 02:18:55,136 iteration 3815 : loss : 0.025279, loss_ce: 0.010243
2022-01-09 02:18:56,630 iteration 3816 : loss : 0.021082, loss_ce: 0.007916
2022-01-09 02:18:58,041 iteration 3817 : loss : 0.019424, loss_ce: 0.007535
2022-01-09 02:18:59,565 iteration 3818 : loss : 0.021262, loss_ce: 0.009330
2022-01-09 02:19:01,100 iteration 3819 : loss : 0.023986, loss_ce: 0.010938
2022-01-09 02:19:02,611 iteration 3820 : loss : 0.024380, loss_ce: 0.009497
2022-01-09 02:19:04,114 iteration 3821 : loss : 0.046647, loss_ce: 0.017464
2022-01-09 02:19:05,537 iteration 3822 : loss : 0.036851, loss_ce: 0.012517
2022-01-09 02:19:07,119 iteration 3823 : loss : 0.038325, loss_ce: 0.011764
2022-01-09 02:19:08,599 iteration 3824 : loss : 0.031938, loss_ce: 0.011876
2022-01-09 02:19:08,599 Training Data Eval:
2022-01-09 02:19:16,046   Average segmentation loss on training set: 0.0168
2022-01-09 02:19:16,047 Validation Data Eval:
2022-01-09 02:19:18,649   Average segmentation loss on validation set: 0.0874
2022-01-09 02:19:20,236 iteration 3825 : loss : 0.023777, loss_ce: 0.007947
 56%|███████████████▏           | 225/400 [1:42:49<1:23:50, 28.75s/it]2022-01-09 02:19:21,801 iteration 3826 : loss : 0.020172, loss_ce: 0.008174
2022-01-09 02:19:23,225 iteration 3827 : loss : 0.020917, loss_ce: 0.007810
2022-01-09 02:19:24,734 iteration 3828 : loss : 0.021040, loss_ce: 0.007886
2022-01-09 02:19:26,231 iteration 3829 : loss : 0.027617, loss_ce: 0.007962
2022-01-09 02:19:27,690 iteration 3830 : loss : 0.017899, loss_ce: 0.006485
2022-01-09 02:19:29,141 iteration 3831 : loss : 0.027536, loss_ce: 0.009039
2022-01-09 02:19:30,577 iteration 3832 : loss : 0.021616, loss_ce: 0.005847
2022-01-09 02:19:32,069 iteration 3833 : loss : 0.024066, loss_ce: 0.012088
2022-01-09 02:19:33,592 iteration 3834 : loss : 0.031187, loss_ce: 0.015254
2022-01-09 02:19:35,095 iteration 3835 : loss : 0.039519, loss_ce: 0.014738
2022-01-09 02:19:36,583 iteration 3836 : loss : 0.024910, loss_ce: 0.009192
2022-01-09 02:19:38,102 iteration 3837 : loss : 0.026277, loss_ce: 0.009020
2022-01-09 02:19:39,705 iteration 3838 : loss : 0.034017, loss_ce: 0.015280
2022-01-09 02:19:41,177 iteration 3839 : loss : 0.027614, loss_ce: 0.013238
2022-01-09 02:19:42,671 iteration 3840 : loss : 0.025823, loss_ce: 0.007552
2022-01-09 02:19:44,113 iteration 3841 : loss : 0.025538, loss_ce: 0.010453
2022-01-09 02:19:45,630 iteration 3842 : loss : 0.056064, loss_ce: 0.029852
 56%|███████████████▎           | 226/400 [1:43:14<1:20:27, 27.74s/it]2022-01-09 02:19:47,113 iteration 3843 : loss : 0.027284, loss_ce: 0.011504
2022-01-09 02:19:48,638 iteration 3844 : loss : 0.028132, loss_ce: 0.008114
2022-01-09 02:19:50,157 iteration 3845 : loss : 0.023986, loss_ce: 0.011060
2022-01-09 02:19:51,603 iteration 3846 : loss : 0.019918, loss_ce: 0.007418
2022-01-09 02:19:53,070 iteration 3847 : loss : 0.025305, loss_ce: 0.010169
2022-01-09 02:19:54,626 iteration 3848 : loss : 0.023902, loss_ce: 0.010825
2022-01-09 02:19:56,236 iteration 3849 : loss : 0.033926, loss_ce: 0.014277
2022-01-09 02:19:57,815 iteration 3850 : loss : 0.078562, loss_ce: 0.023616
2022-01-09 02:19:59,302 iteration 3851 : loss : 0.028356, loss_ce: 0.013234
2022-01-09 02:20:00,800 iteration 3852 : loss : 0.027762, loss_ce: 0.008114
2022-01-09 02:20:02,210 iteration 3853 : loss : 0.020430, loss_ce: 0.006644
2022-01-09 02:20:03,716 iteration 3854 : loss : 0.031203, loss_ce: 0.009497
2022-01-09 02:20:05,362 iteration 3855 : loss : 0.027026, loss_ce: 0.010125
2022-01-09 02:20:06,672 iteration 3856 : loss : 0.025784, loss_ce: 0.011753
2022-01-09 02:20:08,105 iteration 3857 : loss : 0.029065, loss_ce: 0.012722
2022-01-09 02:20:09,451 iteration 3858 : loss : 0.027420, loss_ce: 0.011502
2022-01-09 02:20:10,902 iteration 3859 : loss : 0.020976, loss_ce: 0.008835
 57%|███████████████▎           | 227/400 [1:43:39<1:17:51, 27.00s/it]2022-01-09 02:20:12,434 iteration 3860 : loss : 0.022042, loss_ce: 0.008008
2022-01-09 02:20:13,990 iteration 3861 : loss : 0.034993, loss_ce: 0.015596
2022-01-09 02:20:15,489 iteration 3862 : loss : 0.025604, loss_ce: 0.009237
2022-01-09 02:20:16,974 iteration 3863 : loss : 0.043610, loss_ce: 0.016966
2022-01-09 02:20:18,492 iteration 3864 : loss : 0.029623, loss_ce: 0.010360
2022-01-09 02:20:19,953 iteration 3865 : loss : 0.033209, loss_ce: 0.012555
2022-01-09 02:20:21,566 iteration 3866 : loss : 0.027901, loss_ce: 0.009599
2022-01-09 02:20:23,019 iteration 3867 : loss : 0.031630, loss_ce: 0.011662
2022-01-09 02:20:24,556 iteration 3868 : loss : 0.038747, loss_ce: 0.013037
2022-01-09 02:20:26,071 iteration 3869 : loss : 0.028064, loss_ce: 0.014017
2022-01-09 02:20:27,611 iteration 3870 : loss : 0.025390, loss_ce: 0.009591
2022-01-09 02:20:29,116 iteration 3871 : loss : 0.026500, loss_ce: 0.008718
2022-01-09 02:20:30,535 iteration 3872 : loss : 0.024318, loss_ce: 0.010063
2022-01-09 02:20:31,954 iteration 3873 : loss : 0.028034, loss_ce: 0.013540
2022-01-09 02:20:33,452 iteration 3874 : loss : 0.028305, loss_ce: 0.011728
2022-01-09 02:20:34,938 iteration 3875 : loss : 0.035910, loss_ce: 0.014628
2022-01-09 02:20:36,433 iteration 3876 : loss : 0.032315, loss_ce: 0.010121
 57%|███████████████▍           | 228/400 [1:44:05<1:16:08, 26.56s/it]2022-01-09 02:20:37,930 iteration 3877 : loss : 0.041206, loss_ce: 0.016255
2022-01-09 02:20:39,440 iteration 3878 : loss : 0.025145, loss_ce: 0.009414
2022-01-09 02:20:40,873 iteration 3879 : loss : 0.029095, loss_ce: 0.010352
2022-01-09 02:20:42,388 iteration 3880 : loss : 0.024425, loss_ce: 0.010100
2022-01-09 02:20:43,825 iteration 3881 : loss : 0.023660, loss_ce: 0.010378
2022-01-09 02:20:45,426 iteration 3882 : loss : 0.037076, loss_ce: 0.008817
2022-01-09 02:20:46,968 iteration 3883 : loss : 0.052239, loss_ce: 0.020012
2022-01-09 02:20:48,477 iteration 3884 : loss : 0.029197, loss_ce: 0.010772
2022-01-09 02:20:50,013 iteration 3885 : loss : 0.044026, loss_ce: 0.010151
2022-01-09 02:20:51,544 iteration 3886 : loss : 0.032075, loss_ce: 0.013818
2022-01-09 02:20:52,948 iteration 3887 : loss : 0.022858, loss_ce: 0.008393
2022-01-09 02:20:54,469 iteration 3888 : loss : 0.029208, loss_ce: 0.014968
2022-01-09 02:20:55,958 iteration 3889 : loss : 0.030521, loss_ce: 0.014203
2022-01-09 02:20:57,571 iteration 3890 : loss : 0.056057, loss_ce: 0.016958
2022-01-09 02:20:59,012 iteration 3891 : loss : 0.027136, loss_ce: 0.010851
2022-01-09 02:21:00,491 iteration 3892 : loss : 0.026755, loss_ce: 0.011608
2022-01-09 02:21:02,011 iteration 3893 : loss : 0.033913, loss_ce: 0.011927
 57%|███████████████▍           | 229/400 [1:44:30<1:14:51, 26.27s/it]2022-01-09 02:21:03,523 iteration 3894 : loss : 0.024009, loss_ce: 0.007355
2022-01-09 02:21:04,939 iteration 3895 : loss : 0.022522, loss_ce: 0.008034
2022-01-09 02:21:06,414 iteration 3896 : loss : 0.034447, loss_ce: 0.014702
2022-01-09 02:21:07,940 iteration 3897 : loss : 0.027428, loss_ce: 0.011443
2022-01-09 02:21:09,461 iteration 3898 : loss : 0.031651, loss_ce: 0.010349
2022-01-09 02:21:10,848 iteration 3899 : loss : 0.030794, loss_ce: 0.011885
2022-01-09 02:21:12,272 iteration 3900 : loss : 0.026275, loss_ce: 0.010280
2022-01-09 02:21:13,675 iteration 3901 : loss : 0.027342, loss_ce: 0.011118
2022-01-09 02:21:15,088 iteration 3902 : loss : 0.025943, loss_ce: 0.009901
2022-01-09 02:21:16,453 iteration 3903 : loss : 0.024326, loss_ce: 0.007683
2022-01-09 02:21:17,892 iteration 3904 : loss : 0.027213, loss_ce: 0.008418
2022-01-09 02:21:19,335 iteration 3905 : loss : 0.027014, loss_ce: 0.009019
2022-01-09 02:21:20,690 iteration 3906 : loss : 0.018763, loss_ce: 0.007824
2022-01-09 02:21:22,019 iteration 3907 : loss : 0.019654, loss_ce: 0.007359
2022-01-09 02:21:23,378 iteration 3908 : loss : 0.023860, loss_ce: 0.009694
2022-01-09 02:21:24,821 iteration 3909 : loss : 0.029111, loss_ce: 0.015937
2022-01-09 02:21:24,822 Training Data Eval:
2022-01-09 02:21:32,370   Average segmentation loss on training set: 0.0187
2022-01-09 02:21:32,371 Validation Data Eval:
2022-01-09 02:21:35,000   Average segmentation loss on validation set: 0.0874
2022-01-09 02:21:36,596 iteration 3910 : loss : 0.021955, loss_ce: 0.007473
 57%|███████████████▌           | 230/400 [1:45:05<1:21:30, 28.76s/it]2022-01-09 02:21:38,098 iteration 3911 : loss : 0.029638, loss_ce: 0.011739
2022-01-09 02:21:39,629 iteration 3912 : loss : 0.030286, loss_ce: 0.011675
2022-01-09 02:21:41,127 iteration 3913 : loss : 0.018021, loss_ce: 0.007221
2022-01-09 02:21:42,598 iteration 3914 : loss : 0.021734, loss_ce: 0.009521
2022-01-09 02:21:44,078 iteration 3915 : loss : 0.023861, loss_ce: 0.010566
2022-01-09 02:21:45,515 iteration 3916 : loss : 0.023206, loss_ce: 0.008267
2022-01-09 02:21:47,046 iteration 3917 : loss : 0.036380, loss_ce: 0.012849
2022-01-09 02:21:48,565 iteration 3918 : loss : 0.037466, loss_ce: 0.012165
2022-01-09 02:21:50,205 iteration 3919 : loss : 0.035934, loss_ce: 0.010563
2022-01-09 02:21:51,628 iteration 3920 : loss : 0.029501, loss_ce: 0.013338
2022-01-09 02:21:53,117 iteration 3921 : loss : 0.023060, loss_ce: 0.008886
2022-01-09 02:21:54,629 iteration 3922 : loss : 0.033192, loss_ce: 0.015994
2022-01-09 02:21:56,070 iteration 3923 : loss : 0.031142, loss_ce: 0.017084
2022-01-09 02:21:57,468 iteration 3924 : loss : 0.023515, loss_ce: 0.006375
2022-01-09 02:21:59,030 iteration 3925 : loss : 0.033410, loss_ce: 0.013034
2022-01-09 02:22:00,561 iteration 3926 : loss : 0.022912, loss_ce: 0.008188
2022-01-09 02:22:02,018 iteration 3927 : loss : 0.034349, loss_ce: 0.014072
 58%|███████████████▌           | 231/400 [1:45:30<1:18:11, 27.76s/it]2022-01-09 02:22:03,611 iteration 3928 : loss : 0.048699, loss_ce: 0.021539
2022-01-09 02:22:05,090 iteration 3929 : loss : 0.021314, loss_ce: 0.010047
2022-01-09 02:22:06,627 iteration 3930 : loss : 0.023622, loss_ce: 0.009460
2022-01-09 02:22:08,153 iteration 3931 : loss : 0.025447, loss_ce: 0.010310
2022-01-09 02:22:09,668 iteration 3932 : loss : 0.043520, loss_ce: 0.014406
2022-01-09 02:22:11,242 iteration 3933 : loss : 0.023581, loss_ce: 0.009589
2022-01-09 02:22:12,762 iteration 3934 : loss : 0.021626, loss_ce: 0.009483
2022-01-09 02:22:14,145 iteration 3935 : loss : 0.019850, loss_ce: 0.008551
2022-01-09 02:22:15,749 iteration 3936 : loss : 0.026682, loss_ce: 0.011054
2022-01-09 02:22:17,179 iteration 3937 : loss : 0.028106, loss_ce: 0.012245
2022-01-09 02:22:18,688 iteration 3938 : loss : 0.068822, loss_ce: 0.012561
2022-01-09 02:22:20,073 iteration 3939 : loss : 0.033258, loss_ce: 0.012257
2022-01-09 02:22:21,514 iteration 3940 : loss : 0.025402, loss_ce: 0.009704
2022-01-09 02:22:22,985 iteration 3941 : loss : 0.022347, loss_ce: 0.009155
2022-01-09 02:22:24,533 iteration 3942 : loss : 0.041458, loss_ce: 0.018215
2022-01-09 02:22:25,972 iteration 3943 : loss : 0.024910, loss_ce: 0.006244
2022-01-09 02:22:27,540 iteration 3944 : loss : 0.036834, loss_ce: 0.012608
 58%|███████████████▋           | 232/400 [1:45:56<1:15:50, 27.09s/it]2022-01-09 02:22:29,116 iteration 3945 : loss : 0.035488, loss_ce: 0.016830
2022-01-09 02:22:30,498 iteration 3946 : loss : 0.023740, loss_ce: 0.012408
2022-01-09 02:22:31,989 iteration 3947 : loss : 0.025821, loss_ce: 0.007815
2022-01-09 02:22:33,655 iteration 3948 : loss : 0.028799, loss_ce: 0.009046
2022-01-09 02:22:35,213 iteration 3949 : loss : 0.030916, loss_ce: 0.011225
2022-01-09 02:22:36,820 iteration 3950 : loss : 0.026101, loss_ce: 0.008808
2022-01-09 02:22:38,251 iteration 3951 : loss : 0.022804, loss_ce: 0.009501
2022-01-09 02:22:39,811 iteration 3952 : loss : 0.020266, loss_ce: 0.008077
2022-01-09 02:22:41,371 iteration 3953 : loss : 0.041198, loss_ce: 0.014472
2022-01-09 02:22:42,830 iteration 3954 : loss : 0.029203, loss_ce: 0.010482
2022-01-09 02:22:44,283 iteration 3955 : loss : 0.017058, loss_ce: 0.006251
2022-01-09 02:22:45,909 iteration 3956 : loss : 0.030631, loss_ce: 0.012013
2022-01-09 02:22:47,327 iteration 3957 : loss : 0.021159, loss_ce: 0.008888
2022-01-09 02:22:48,761 iteration 3958 : loss : 0.028967, loss_ce: 0.011375
2022-01-09 02:22:50,337 iteration 3959 : loss : 0.029620, loss_ce: 0.008588
2022-01-09 02:22:51,773 iteration 3960 : loss : 0.022536, loss_ce: 0.010635
2022-01-09 02:22:53,249 iteration 3961 : loss : 0.026371, loss_ce: 0.010495
 58%|███████████████▋           | 233/400 [1:46:22<1:14:14, 26.67s/it]2022-01-09 02:22:54,822 iteration 3962 : loss : 0.041117, loss_ce: 0.016747
2022-01-09 02:22:56,291 iteration 3963 : loss : 0.025813, loss_ce: 0.011374
2022-01-09 02:22:57,880 iteration 3964 : loss : 0.025808, loss_ce: 0.011327
2022-01-09 02:22:59,291 iteration 3965 : loss : 0.023005, loss_ce: 0.006523
2022-01-09 02:23:00,818 iteration 3966 : loss : 0.029689, loss_ce: 0.012692
2022-01-09 02:23:02,292 iteration 3967 : loss : 0.024230, loss_ce: 0.008680
2022-01-09 02:23:03,628 iteration 3968 : loss : 0.024832, loss_ce: 0.008633
2022-01-09 02:23:05,119 iteration 3969 : loss : 0.023402, loss_ce: 0.006964
2022-01-09 02:23:06,564 iteration 3970 : loss : 0.024413, loss_ce: 0.008543
2022-01-09 02:23:07,897 iteration 3971 : loss : 0.023593, loss_ce: 0.009457
2022-01-09 02:23:09,406 iteration 3972 : loss : 0.022408, loss_ce: 0.007466
2022-01-09 02:23:10,804 iteration 3973 : loss : 0.034782, loss_ce: 0.018236
2022-01-09 02:23:12,155 iteration 3974 : loss : 0.020092, loss_ce: 0.006169
2022-01-09 02:23:13,647 iteration 3975 : loss : 0.022002, loss_ce: 0.008346
2022-01-09 02:23:15,209 iteration 3976 : loss : 0.038654, loss_ce: 0.016044
2022-01-09 02:23:16,770 iteration 3977 : loss : 0.025209, loss_ce: 0.011166
2022-01-09 02:23:18,356 iteration 3978 : loss : 0.049268, loss_ce: 0.020990
 58%|███████████████▊           | 234/400 [1:46:47<1:12:29, 26.20s/it]2022-01-09 02:23:19,897 iteration 3979 : loss : 0.022686, loss_ce: 0.008091
2022-01-09 02:23:21,411 iteration 3980 : loss : 0.040719, loss_ce: 0.012305
2022-01-09 02:23:22,853 iteration 3981 : loss : 0.022451, loss_ce: 0.008875
2022-01-09 02:23:24,196 iteration 3982 : loss : 0.021498, loss_ce: 0.007402
2022-01-09 02:23:25,548 iteration 3983 : loss : 0.021192, loss_ce: 0.009844
2022-01-09 02:23:26,948 iteration 3984 : loss : 0.019251, loss_ce: 0.008156
2022-01-09 02:23:28,433 iteration 3985 : loss : 0.040325, loss_ce: 0.023121
2022-01-09 02:23:30,006 iteration 3986 : loss : 0.032253, loss_ce: 0.014262
2022-01-09 02:23:31,507 iteration 3987 : loss : 0.037437, loss_ce: 0.015282
2022-01-09 02:23:32,975 iteration 3988 : loss : 0.022189, loss_ce: 0.010251
2022-01-09 02:23:34,400 iteration 3989 : loss : 0.028552, loss_ce: 0.007097
2022-01-09 02:23:35,826 iteration 3990 : loss : 0.051910, loss_ce: 0.018325
2022-01-09 02:23:37,362 iteration 3991 : loss : 0.031814, loss_ce: 0.007165
2022-01-09 02:23:38,850 iteration 3992 : loss : 0.028252, loss_ce: 0.009112
2022-01-09 02:23:40,382 iteration 3993 : loss : 0.035944, loss_ce: 0.011845
2022-01-09 02:23:41,824 iteration 3994 : loss : 0.021232, loss_ce: 0.009727
2022-01-09 02:23:41,824 Training Data Eval:
2022-01-09 02:23:49,119   Average segmentation loss on training set: 0.0176
2022-01-09 02:23:49,119 Validation Data Eval:
2022-01-09 02:23:51,623   Average segmentation loss on validation set: 0.1327
2022-01-09 02:23:53,013 iteration 3995 : loss : 0.021273, loss_ce: 0.008364
 59%|███████████████▊           | 235/400 [1:47:21<1:19:02, 28.74s/it]2022-01-09 02:23:54,622 iteration 3996 : loss : 0.023527, loss_ce: 0.006904
2022-01-09 02:23:56,009 iteration 3997 : loss : 0.034609, loss_ce: 0.012060
2022-01-09 02:23:57,504 iteration 3998 : loss : 0.030822, loss_ce: 0.016495
2022-01-09 02:23:58,974 iteration 3999 : loss : 0.022062, loss_ce: 0.006313
2022-01-09 02:24:00,546 iteration 4000 : loss : 0.030663, loss_ce: 0.017911
2022-01-09 02:24:01,994 iteration 4001 : loss : 0.018799, loss_ce: 0.006996
2022-01-09 02:24:03,513 iteration 4002 : loss : 0.022205, loss_ce: 0.009660
2022-01-09 02:24:05,055 iteration 4003 : loss : 0.021419, loss_ce: 0.009720
2022-01-09 02:24:06,610 iteration 4004 : loss : 0.027208, loss_ce: 0.008160
2022-01-09 02:24:08,119 iteration 4005 : loss : 0.026915, loss_ce: 0.012689
2022-01-09 02:24:09,550 iteration 4006 : loss : 0.022247, loss_ce: 0.009002
2022-01-09 02:24:11,090 iteration 4007 : loss : 0.029556, loss_ce: 0.009417
2022-01-09 02:24:12,540 iteration 4008 : loss : 0.023595, loss_ce: 0.008807
2022-01-09 02:24:14,015 iteration 4009 : loss : 0.039272, loss_ce: 0.015943
2022-01-09 02:24:15,580 iteration 4010 : loss : 0.030339, loss_ce: 0.012312
2022-01-09 02:24:17,015 iteration 4011 : loss : 0.021118, loss_ce: 0.005866
2022-01-09 02:24:18,589 iteration 4012 : loss : 0.024773, loss_ce: 0.010660
 59%|███████████████▉           | 236/400 [1:47:47<1:15:57, 27.79s/it]2022-01-09 02:24:20,223 iteration 4013 : loss : 0.028059, loss_ce: 0.008365
2022-01-09 02:24:21,613 iteration 4014 : loss : 0.020158, loss_ce: 0.006583
2022-01-09 02:24:23,150 iteration 4015 : loss : 0.027420, loss_ce: 0.010643
2022-01-09 02:24:24,665 iteration 4016 : loss : 0.020746, loss_ce: 0.007476
2022-01-09 02:24:26,205 iteration 4017 : loss : 0.031433, loss_ce: 0.011197
2022-01-09 02:24:27,620 iteration 4018 : loss : 0.023247, loss_ce: 0.008193
2022-01-09 02:24:29,073 iteration 4019 : loss : 0.026988, loss_ce: 0.013899
2022-01-09 02:24:30,470 iteration 4020 : loss : 0.032960, loss_ce: 0.018770
2022-01-09 02:24:31,888 iteration 4021 : loss : 0.021545, loss_ce: 0.009738
2022-01-09 02:24:33,392 iteration 4022 : loss : 0.025773, loss_ce: 0.012759
2022-01-09 02:24:34,858 iteration 4023 : loss : 0.018366, loss_ce: 0.006522
2022-01-09 02:24:36,216 iteration 4024 : loss : 0.019905, loss_ce: 0.006727
2022-01-09 02:24:37,627 iteration 4025 : loss : 0.019707, loss_ce: 0.007116
2022-01-09 02:24:39,130 iteration 4026 : loss : 0.024561, loss_ce: 0.010498
2022-01-09 02:24:40,622 iteration 4027 : loss : 0.025993, loss_ce: 0.009598
2022-01-09 02:24:42,156 iteration 4028 : loss : 0.031782, loss_ce: 0.011135
2022-01-09 02:24:43,561 iteration 4029 : loss : 0.030817, loss_ce: 0.014833
 59%|███████████████▉           | 237/400 [1:48:12<1:13:12, 26.95s/it]2022-01-09 02:24:45,187 iteration 4030 : loss : 0.046487, loss_ce: 0.017920
2022-01-09 02:24:46,676 iteration 4031 : loss : 0.035466, loss_ce: 0.012169
2022-01-09 02:24:48,134 iteration 4032 : loss : 0.031020, loss_ce: 0.012602
2022-01-09 02:24:49,631 iteration 4033 : loss : 0.028411, loss_ce: 0.010360
2022-01-09 02:24:51,082 iteration 4034 : loss : 0.026869, loss_ce: 0.008704
2022-01-09 02:24:52,575 iteration 4035 : loss : 0.027148, loss_ce: 0.010778
2022-01-09 02:24:54,134 iteration 4036 : loss : 0.031715, loss_ce: 0.013531
2022-01-09 02:24:55,602 iteration 4037 : loss : 0.017281, loss_ce: 0.008054
2022-01-09 02:24:57,209 iteration 4038 : loss : 0.028899, loss_ce: 0.010536
2022-01-09 02:24:58,831 iteration 4039 : loss : 0.024007, loss_ce: 0.009556
2022-01-09 02:25:00,452 iteration 4040 : loss : 0.041882, loss_ce: 0.022040
2022-01-09 02:25:02,034 iteration 4041 : loss : 0.023591, loss_ce: 0.008039
2022-01-09 02:25:03,493 iteration 4042 : loss : 0.024878, loss_ce: 0.009204
2022-01-09 02:25:05,002 iteration 4043 : loss : 0.182251, loss_ce: 0.010504
2022-01-09 02:25:06,436 iteration 4044 : loss : 0.027221, loss_ce: 0.010019
2022-01-09 02:25:07,885 iteration 4045 : loss : 0.023599, loss_ce: 0.012455
2022-01-09 02:25:09,407 iteration 4046 : loss : 0.024406, loss_ce: 0.011137
 60%|████████████████           | 238/400 [1:48:38<1:11:51, 26.61s/it]2022-01-09 02:25:11,047 iteration 4047 : loss : 0.031093, loss_ce: 0.014445
2022-01-09 02:25:12,576 iteration 4048 : loss : 0.029304, loss_ce: 0.010552
2022-01-09 02:25:13,994 iteration 4049 : loss : 0.020316, loss_ce: 0.007732
2022-01-09 02:25:15,501 iteration 4050 : loss : 0.023800, loss_ce: 0.009754
2022-01-09 02:25:17,114 iteration 4051 : loss : 0.063351, loss_ce: 0.009915
2022-01-09 02:25:18,502 iteration 4052 : loss : 0.031442, loss_ce: 0.011255
2022-01-09 02:25:19,845 iteration 4053 : loss : 0.027536, loss_ce: 0.008975
2022-01-09 02:25:21,242 iteration 4054 : loss : 0.037826, loss_ce: 0.010551
2022-01-09 02:25:22,692 iteration 4055 : loss : 0.029932, loss_ce: 0.015065
2022-01-09 02:25:24,174 iteration 4056 : loss : 0.029215, loss_ce: 0.013690
2022-01-09 02:25:25,579 iteration 4057 : loss : 0.029288, loss_ce: 0.008674
2022-01-09 02:25:27,019 iteration 4058 : loss : 0.024907, loss_ce: 0.009016
2022-01-09 02:25:28,620 iteration 4059 : loss : 0.031165, loss_ce: 0.013703
2022-01-09 02:25:30,103 iteration 4060 : loss : 0.024984, loss_ce: 0.010596
2022-01-09 02:25:31,569 iteration 4061 : loss : 0.037585, loss_ce: 0.018430
2022-01-09 02:25:33,074 iteration 4062 : loss : 0.037377, loss_ce: 0.016813
2022-01-09 02:25:34,560 iteration 4063 : loss : 0.034034, loss_ce: 0.010868
 60%|████████████████▏          | 239/400 [1:49:03<1:10:13, 26.17s/it]2022-01-09 02:25:36,071 iteration 4064 : loss : 0.035140, loss_ce: 0.006956
2022-01-09 02:25:37,492 iteration 4065 : loss : 0.020184, loss_ce: 0.006519
2022-01-09 02:25:39,042 iteration 4066 : loss : 0.033729, loss_ce: 0.014363
2022-01-09 02:25:40,463 iteration 4067 : loss : 0.024050, loss_ce: 0.010169
2022-01-09 02:25:41,925 iteration 4068 : loss : 0.025535, loss_ce: 0.009420
2022-01-09 02:25:43,433 iteration 4069 : loss : 0.021046, loss_ce: 0.008361
2022-01-09 02:25:44,931 iteration 4070 : loss : 0.035000, loss_ce: 0.013706
2022-01-09 02:25:46,476 iteration 4071 : loss : 0.027850, loss_ce: 0.009358
2022-01-09 02:25:47,936 iteration 4072 : loss : 0.023781, loss_ce: 0.007375
2022-01-09 02:25:49,369 iteration 4073 : loss : 0.028951, loss_ce: 0.011498
2022-01-09 02:25:50,834 iteration 4074 : loss : 0.025708, loss_ce: 0.011408
2022-01-09 02:25:52,383 iteration 4075 : loss : 0.024627, loss_ce: 0.009551
2022-01-09 02:25:53,809 iteration 4076 : loss : 0.057196, loss_ce: 0.023389
2022-01-09 02:25:55,274 iteration 4077 : loss : 0.024404, loss_ce: 0.008211
2022-01-09 02:25:56,813 iteration 4078 : loss : 0.042338, loss_ce: 0.018265
2022-01-09 02:25:58,327 iteration 4079 : loss : 0.022327, loss_ce: 0.009428
2022-01-09 02:25:58,327 Training Data Eval:
2022-01-09 02:26:05,800   Average segmentation loss on training set: 0.0195
2022-01-09 02:26:05,800 Validation Data Eval:
2022-01-09 02:26:08,356   Average segmentation loss on validation set: 0.0659
2022-01-09 02:26:09,901 iteration 4080 : loss : 0.026328, loss_ce: 0.010922
 60%|████████████████▏          | 240/400 [1:49:38<1:17:08, 28.93s/it]2022-01-09 02:26:11,483 iteration 4081 : loss : 0.034979, loss_ce: 0.012200
2022-01-09 02:26:12,964 iteration 4082 : loss : 0.024567, loss_ce: 0.009732
2022-01-09 02:26:14,372 iteration 4083 : loss : 0.025092, loss_ce: 0.011852
2022-01-09 02:26:15,848 iteration 4084 : loss : 0.034447, loss_ce: 0.012296
2022-01-09 02:26:17,271 iteration 4085 : loss : 0.038194, loss_ce: 0.013317
2022-01-09 02:26:18,910 iteration 4086 : loss : 0.045633, loss_ce: 0.016060
2022-01-09 02:26:20,404 iteration 4087 : loss : 0.050606, loss_ce: 0.009265
2022-01-09 02:26:21,796 iteration 4088 : loss : 0.043890, loss_ce: 0.026307
2022-01-09 02:26:23,260 iteration 4089 : loss : 0.031409, loss_ce: 0.010326
2022-01-09 02:26:24,613 iteration 4090 : loss : 0.027809, loss_ce: 0.009212
2022-01-09 02:26:26,079 iteration 4091 : loss : 0.083082, loss_ce: 0.037296
2022-01-09 02:26:27,567 iteration 4092 : loss : 0.029503, loss_ce: 0.014861
2022-01-09 02:26:29,065 iteration 4093 : loss : 0.049120, loss_ce: 0.018785
2022-01-09 02:26:30,500 iteration 4094 : loss : 0.030578, loss_ce: 0.011657
2022-01-09 02:26:31,989 iteration 4095 : loss : 0.033086, loss_ce: 0.014235
2022-01-09 02:26:33,411 iteration 4096 : loss : 0.030835, loss_ce: 0.016908
2022-01-09 02:26:34,850 iteration 4097 : loss : 0.022700, loss_ce: 0.008765
 60%|████████████████▎          | 241/400 [1:50:03<1:13:29, 27.73s/it]2022-01-09 02:26:36,356 iteration 4098 : loss : 0.028078, loss_ce: 0.011350
2022-01-09 02:26:37,780 iteration 4099 : loss : 0.044153, loss_ce: 0.012814
2022-01-09 02:26:39,264 iteration 4100 : loss : 0.021976, loss_ce: 0.008117
2022-01-09 02:26:40,803 iteration 4101 : loss : 0.029551, loss_ce: 0.010916
2022-01-09 02:26:42,334 iteration 4102 : loss : 0.026520, loss_ce: 0.011468
2022-01-09 02:26:43,838 iteration 4103 : loss : 0.039238, loss_ce: 0.019466
2022-01-09 02:26:45,315 iteration 4104 : loss : 0.030728, loss_ce: 0.013527
2022-01-09 02:26:46,805 iteration 4105 : loss : 0.021504, loss_ce: 0.011234
2022-01-09 02:26:48,250 iteration 4106 : loss : 0.027596, loss_ce: 0.012191
2022-01-09 02:26:49,689 iteration 4107 : loss : 0.019650, loss_ce: 0.007574
2022-01-09 02:26:51,166 iteration 4108 : loss : 0.024826, loss_ce: 0.010422
2022-01-09 02:26:52,559 iteration 4109 : loss : 0.030835, loss_ce: 0.010987
2022-01-09 02:26:54,070 iteration 4110 : loss : 0.039827, loss_ce: 0.011837
2022-01-09 02:26:55,727 iteration 4111 : loss : 0.035273, loss_ce: 0.017433
2022-01-09 02:26:57,149 iteration 4112 : loss : 0.025761, loss_ce: 0.009257
2022-01-09 02:26:58,668 iteration 4113 : loss : 0.039811, loss_ce: 0.013184
2022-01-09 02:27:00,124 iteration 4114 : loss : 0.028759, loss_ce: 0.012423
 60%|████████████████▎          | 242/400 [1:50:29<1:11:05, 26.99s/it]2022-01-09 02:27:01,605 iteration 4115 : loss : 0.028413, loss_ce: 0.015437
2022-01-09 02:27:03,149 iteration 4116 : loss : 0.030265, loss_ce: 0.011379
2022-01-09 02:27:04,538 iteration 4117 : loss : 0.020737, loss_ce: 0.007909
2022-01-09 02:27:06,076 iteration 4118 : loss : 0.023736, loss_ce: 0.010936
2022-01-09 02:27:07,697 iteration 4119 : loss : 0.061885, loss_ce: 0.016058
2022-01-09 02:27:09,204 iteration 4120 : loss : 0.037491, loss_ce: 0.010514
2022-01-09 02:27:10,622 iteration 4121 : loss : 0.020836, loss_ce: 0.006647
2022-01-09 02:27:12,072 iteration 4122 : loss : 0.035610, loss_ce: 0.010478
2022-01-09 02:27:13,459 iteration 4123 : loss : 0.015694, loss_ce: 0.006672
2022-01-09 02:27:14,977 iteration 4124 : loss : 0.027740, loss_ce: 0.015168
2022-01-09 02:27:16,473 iteration 4125 : loss : 0.032485, loss_ce: 0.016566
2022-01-09 02:27:17,958 iteration 4126 : loss : 0.023973, loss_ce: 0.010145
2022-01-09 02:27:19,365 iteration 4127 : loss : 0.017622, loss_ce: 0.008641
2022-01-09 02:27:20,971 iteration 4128 : loss : 0.054738, loss_ce: 0.014737
2022-01-09 02:27:22,358 iteration 4129 : loss : 0.040346, loss_ce: 0.010994
2022-01-09 02:27:23,743 iteration 4130 : loss : 0.031410, loss_ce: 0.013995
2022-01-09 02:27:25,210 iteration 4131 : loss : 0.019501, loss_ce: 0.008992
 61%|████████████████▍          | 243/400 [1:50:54<1:09:08, 26.42s/it]2022-01-09 02:27:26,848 iteration 4132 : loss : 0.026642, loss_ce: 0.007888
2022-01-09 02:27:28,382 iteration 4133 : loss : 0.024273, loss_ce: 0.007703
2022-01-09 02:27:29,938 iteration 4134 : loss : 0.030326, loss_ce: 0.012850
2022-01-09 02:27:31,332 iteration 4135 : loss : 0.027760, loss_ce: 0.011988
2022-01-09 02:27:32,789 iteration 4136 : loss : 0.022911, loss_ce: 0.010316
2022-01-09 02:27:34,174 iteration 4137 : loss : 0.020200, loss_ce: 0.007604
2022-01-09 02:27:35,687 iteration 4138 : loss : 0.027317, loss_ce: 0.013953
2022-01-09 02:27:37,200 iteration 4139 : loss : 0.023231, loss_ce: 0.009346
2022-01-09 02:27:38,701 iteration 4140 : loss : 0.017426, loss_ce: 0.007778
2022-01-09 02:27:40,204 iteration 4141 : loss : 0.021905, loss_ce: 0.007855
2022-01-09 02:27:41,776 iteration 4142 : loss : 0.045174, loss_ce: 0.017037
2022-01-09 02:27:43,146 iteration 4143 : loss : 0.034406, loss_ce: 0.012542
2022-01-09 02:27:44,554 iteration 4144 : loss : 0.020845, loss_ce: 0.004822
2022-01-09 02:27:46,094 iteration 4145 : loss : 0.025056, loss_ce: 0.010400
2022-01-09 02:27:47,528 iteration 4146 : loss : 0.018493, loss_ce: 0.007820
2022-01-09 02:27:49,019 iteration 4147 : loss : 0.021080, loss_ce: 0.007578
2022-01-09 02:27:50,458 iteration 4148 : loss : 0.024668, loss_ce: 0.010327
 61%|████████████████▍          | 244/400 [1:51:19<1:07:47, 26.07s/it]2022-01-09 02:27:51,910 iteration 4149 : loss : 0.021552, loss_ce: 0.007399
2022-01-09 02:27:53,420 iteration 4150 : loss : 0.021706, loss_ce: 0.007297
2022-01-09 02:27:54,965 iteration 4151 : loss : 0.029011, loss_ce: 0.009692
2022-01-09 02:27:56,455 iteration 4152 : loss : 0.025761, loss_ce: 0.010199
2022-01-09 02:27:57,850 iteration 4153 : loss : 0.021027, loss_ce: 0.009086
2022-01-09 02:27:59,352 iteration 4154 : loss : 0.017256, loss_ce: 0.005968
2022-01-09 02:28:00,818 iteration 4155 : loss : 0.020496, loss_ce: 0.006620
2022-01-09 02:28:02,221 iteration 4156 : loss : 0.019592, loss_ce: 0.006814
2022-01-09 02:28:03,779 iteration 4157 : loss : 0.035432, loss_ce: 0.019319
2022-01-09 02:28:05,218 iteration 4158 : loss : 0.020418, loss_ce: 0.007450
2022-01-09 02:28:06,649 iteration 4159 : loss : 0.023056, loss_ce: 0.009609
2022-01-09 02:28:08,273 iteration 4160 : loss : 0.029529, loss_ce: 0.010889
2022-01-09 02:28:09,847 iteration 4161 : loss : 0.026656, loss_ce: 0.008312
2022-01-09 02:28:11,416 iteration 4162 : loss : 0.034171, loss_ce: 0.011599
2022-01-09 02:28:12,959 iteration 4163 : loss : 0.030897, loss_ce: 0.012155
2022-01-09 02:28:14,438 iteration 4164 : loss : 0.025772, loss_ce: 0.009520
2022-01-09 02:28:14,438 Training Data Eval:
2022-01-09 02:28:21,803   Average segmentation loss on training set: 0.0169
2022-01-09 02:28:21,804 Validation Data Eval:
2022-01-09 02:28:24,371   Average segmentation loss on validation set: 0.0649
2022-01-09 02:28:25,891 iteration 4165 : loss : 0.030518, loss_ce: 0.010049
 61%|████████████████▌          | 245/400 [1:51:54<1:14:35, 28.88s/it]2022-01-09 02:28:27,421 iteration 4166 : loss : 0.020018, loss_ce: 0.007629
2022-01-09 02:28:28,807 iteration 4167 : loss : 0.025487, loss_ce: 0.007282
2022-01-09 02:28:30,247 iteration 4168 : loss : 0.021578, loss_ce: 0.009000
2022-01-09 02:28:31,691 iteration 4169 : loss : 0.020672, loss_ce: 0.010593
2022-01-09 02:28:33,314 iteration 4170 : loss : 0.023201, loss_ce: 0.010148
2022-01-09 02:28:34,709 iteration 4171 : loss : 0.019462, loss_ce: 0.006119
2022-01-09 02:28:36,204 iteration 4172 : loss : 0.022903, loss_ce: 0.009937
2022-01-09 02:28:37,760 iteration 4173 : loss : 0.026887, loss_ce: 0.007665
2022-01-09 02:28:39,218 iteration 4174 : loss : 0.023840, loss_ce: 0.007257
2022-01-09 02:28:40,631 iteration 4175 : loss : 0.018909, loss_ce: 0.007842
2022-01-09 02:28:42,084 iteration 4176 : loss : 0.023609, loss_ce: 0.010343
2022-01-09 02:28:43,606 iteration 4177 : loss : 0.023332, loss_ce: 0.006394
2022-01-09 02:28:45,009 iteration 4178 : loss : 0.024834, loss_ce: 0.009108
2022-01-09 02:28:46,432 iteration 4179 : loss : 0.025918, loss_ce: 0.012482
2022-01-09 02:28:47,846 iteration 4180 : loss : 0.017306, loss_ce: 0.005657
2022-01-09 02:28:49,285 iteration 4181 : loss : 0.022028, loss_ce: 0.009829
2022-01-09 02:28:50,740 iteration 4182 : loss : 0.023646, loss_ce: 0.005856
 62%|████████████████▌          | 246/400 [1:52:19<1:11:01, 27.67s/it]2022-01-09 02:28:52,286 iteration 4183 : loss : 0.021179, loss_ce: 0.007676
2022-01-09 02:28:53,795 iteration 4184 : loss : 0.019092, loss_ce: 0.008415
2022-01-09 02:28:55,253 iteration 4185 : loss : 0.021583, loss_ce: 0.009302
2022-01-09 02:28:56,685 iteration 4186 : loss : 0.019012, loss_ce: 0.005535
2022-01-09 02:28:58,200 iteration 4187 : loss : 0.028094, loss_ce: 0.009239
2022-01-09 02:28:59,618 iteration 4188 : loss : 0.025303, loss_ce: 0.008412
2022-01-09 02:29:01,218 iteration 4189 : loss : 0.026922, loss_ce: 0.012045
2022-01-09 02:29:02,756 iteration 4190 : loss : 0.029644, loss_ce: 0.013167
2022-01-09 02:29:04,313 iteration 4191 : loss : 0.024792, loss_ce: 0.011343
2022-01-09 02:29:05,650 iteration 4192 : loss : 0.015401, loss_ce: 0.004978
2022-01-09 02:29:07,132 iteration 4193 : loss : 0.027786, loss_ce: 0.015866
2022-01-09 02:29:08,647 iteration 4194 : loss : 0.025220, loss_ce: 0.008413
2022-01-09 02:29:10,088 iteration 4195 : loss : 0.021718, loss_ce: 0.006655
2022-01-09 02:29:11,596 iteration 4196 : loss : 0.023922, loss_ce: 0.009713
2022-01-09 02:29:13,040 iteration 4197 : loss : 0.023545, loss_ce: 0.007025
2022-01-09 02:29:14,487 iteration 4198 : loss : 0.018392, loss_ce: 0.007207
2022-01-09 02:29:15,867 iteration 4199 : loss : 0.019870, loss_ce: 0.007779
 62%|████████████████▋          | 247/400 [1:52:44<1:08:37, 26.91s/it]2022-01-09 02:29:17,386 iteration 4200 : loss : 0.019484, loss_ce: 0.007009
2022-01-09 02:29:18,887 iteration 4201 : loss : 0.021348, loss_ce: 0.008843
2022-01-09 02:29:20,376 iteration 4202 : loss : 0.019305, loss_ce: 0.007835
2022-01-09 02:29:21,969 iteration 4203 : loss : 0.027296, loss_ce: 0.012660
2022-01-09 02:29:23,413 iteration 4204 : loss : 0.020069, loss_ce: 0.007886
2022-01-09 02:29:24,862 iteration 4205 : loss : 0.019370, loss_ce: 0.007707
2022-01-09 02:29:26,484 iteration 4206 : loss : 0.037583, loss_ce: 0.012955
2022-01-09 02:29:28,008 iteration 4207 : loss : 0.025252, loss_ce: 0.010546
2022-01-09 02:29:29,358 iteration 4208 : loss : 0.016163, loss_ce: 0.006187
2022-01-09 02:29:30,775 iteration 4209 : loss : 0.019321, loss_ce: 0.008451
2022-01-09 02:29:32,200 iteration 4210 : loss : 0.024107, loss_ce: 0.011440
2022-01-09 02:29:33,687 iteration 4211 : loss : 0.021802, loss_ce: 0.009539
2022-01-09 02:29:35,219 iteration 4212 : loss : 0.026872, loss_ce: 0.011694
2022-01-09 02:29:36,633 iteration 4213 : loss : 0.026210, loss_ce: 0.006795
2022-01-09 02:29:38,152 iteration 4214 : loss : 0.027990, loss_ce: 0.009246
2022-01-09 02:29:39,607 iteration 4215 : loss : 0.019714, loss_ce: 0.006377
2022-01-09 02:29:41,049 iteration 4216 : loss : 0.022185, loss_ce: 0.009739
 62%|████████████████▋          | 248/400 [1:53:09<1:06:51, 26.39s/it]2022-01-09 02:29:42,634 iteration 4217 : loss : 0.029305, loss_ce: 0.011127
2022-01-09 02:29:44,068 iteration 4218 : loss : 0.025087, loss_ce: 0.009492
2022-01-09 02:29:45,474 iteration 4219 : loss : 0.021971, loss_ce: 0.008087
2022-01-09 02:29:46,817 iteration 4220 : loss : 0.020855, loss_ce: 0.005572
2022-01-09 02:29:48,401 iteration 4221 : loss : 0.023320, loss_ce: 0.010300
2022-01-09 02:29:49,989 iteration 4222 : loss : 0.031172, loss_ce: 0.009138
2022-01-09 02:29:51,416 iteration 4223 : loss : 0.022293, loss_ce: 0.007270
2022-01-09 02:29:52,951 iteration 4224 : loss : 0.031379, loss_ce: 0.007644
2022-01-09 02:29:54,440 iteration 4225 : loss : 0.031940, loss_ce: 0.010529
2022-01-09 02:29:55,966 iteration 4226 : loss : 0.022491, loss_ce: 0.009282
2022-01-09 02:29:57,455 iteration 4227 : loss : 0.018756, loss_ce: 0.007704
2022-01-09 02:29:58,855 iteration 4228 : loss : 0.035752, loss_ce: 0.024051
2022-01-09 02:30:00,307 iteration 4229 : loss : 0.028485, loss_ce: 0.010582
2022-01-09 02:30:01,747 iteration 4230 : loss : 0.026359, loss_ce: 0.012805
2022-01-09 02:30:03,269 iteration 4231 : loss : 0.019458, loss_ce: 0.008899
2022-01-09 02:30:04,794 iteration 4232 : loss : 0.021427, loss_ce: 0.006809
2022-01-09 02:30:06,246 iteration 4233 : loss : 0.020559, loss_ce: 0.010861
 62%|████████████████▊          | 249/400 [1:53:35<1:05:30, 26.03s/it]2022-01-09 02:30:07,658 iteration 4234 : loss : 0.029823, loss_ce: 0.006329
2022-01-09 02:30:09,031 iteration 4235 : loss : 0.015065, loss_ce: 0.006041
2022-01-09 02:30:10,523 iteration 4236 : loss : 0.021984, loss_ce: 0.008923
2022-01-09 02:30:11,921 iteration 4237 : loss : 0.023085, loss_ce: 0.009406
2022-01-09 02:30:13,367 iteration 4238 : loss : 0.034320, loss_ce: 0.017364
2022-01-09 02:30:15,003 iteration 4239 : loss : 0.034808, loss_ce: 0.012872
2022-01-09 02:30:16,489 iteration 4240 : loss : 0.020585, loss_ce: 0.006812
2022-01-09 02:30:17,839 iteration 4241 : loss : 0.021643, loss_ce: 0.010009
2022-01-09 02:30:19,311 iteration 4242 : loss : 0.029659, loss_ce: 0.009461
2022-01-09 02:30:20,736 iteration 4243 : loss : 0.025710, loss_ce: 0.011392
2022-01-09 02:30:22,094 iteration 4244 : loss : 0.017023, loss_ce: 0.007744
2022-01-09 02:30:23,623 iteration 4245 : loss : 0.024801, loss_ce: 0.009182
2022-01-09 02:30:25,188 iteration 4246 : loss : 0.031497, loss_ce: 0.007535
2022-01-09 02:30:26,700 iteration 4247 : loss : 0.022389, loss_ce: 0.009544
2022-01-09 02:30:28,088 iteration 4248 : loss : 0.025120, loss_ce: 0.010007
2022-01-09 02:30:29,636 iteration 4249 : loss : 0.028124, loss_ce: 0.007875
2022-01-09 02:30:29,636 Training Data Eval:
2022-01-09 02:30:36,931   Average segmentation loss on training set: 0.0161
2022-01-09 02:30:36,931 Validation Data Eval:
2022-01-09 02:30:39,419   Average segmentation loss on validation set: 0.1035
2022-01-09 02:30:40,869 iteration 4250 : loss : 0.025312, loss_ce: 0.009570
 62%|████████████████▉          | 250/400 [1:54:09<1:11:31, 28.61s/it]2022-01-09 02:30:42,479 iteration 4251 : loss : 0.024711, loss_ce: 0.006124
2022-01-09 02:30:44,005 iteration 4252 : loss : 0.025618, loss_ce: 0.010945
2022-01-09 02:30:45,472 iteration 4253 : loss : 0.056564, loss_ce: 0.022760
2022-01-09 02:30:46,996 iteration 4254 : loss : 0.018943, loss_ce: 0.006732
2022-01-09 02:30:48,525 iteration 4255 : loss : 0.018991, loss_ce: 0.007477
2022-01-09 02:30:49,868 iteration 4256 : loss : 0.020577, loss_ce: 0.009140
2022-01-09 02:30:51,322 iteration 4257 : loss : 0.027825, loss_ce: 0.010627
2022-01-09 02:30:52,690 iteration 4258 : loss : 0.023424, loss_ce: 0.007827
2022-01-09 02:30:54,183 iteration 4259 : loss : 0.023723, loss_ce: 0.010029
2022-01-09 02:30:55,683 iteration 4260 : loss : 0.030349, loss_ce: 0.009337
2022-01-09 02:30:57,202 iteration 4261 : loss : 0.018268, loss_ce: 0.007283
2022-01-09 02:30:58,651 iteration 4262 : loss : 0.020279, loss_ce: 0.009282
2022-01-09 02:31:00,169 iteration 4263 : loss : 0.019039, loss_ce: 0.008045
2022-01-09 02:31:01,730 iteration 4264 : loss : 0.032398, loss_ce: 0.012662
2022-01-09 02:31:03,209 iteration 4265 : loss : 0.030023, loss_ce: 0.008674
2022-01-09 02:31:04,724 iteration 4266 : loss : 0.021280, loss_ce: 0.006748
2022-01-09 02:31:06,270 iteration 4267 : loss : 0.026658, loss_ce: 0.009100
 63%|████████████████▉          | 251/400 [1:54:35<1:08:39, 27.65s/it]2022-01-09 02:31:07,826 iteration 4268 : loss : 0.026733, loss_ce: 0.007855
2022-01-09 02:31:09,252 iteration 4269 : loss : 0.022946, loss_ce: 0.007110
2022-01-09 02:31:10,689 iteration 4270 : loss : 0.026100, loss_ce: 0.015510
2022-01-09 02:31:12,088 iteration 4271 : loss : 0.021798, loss_ce: 0.008521
2022-01-09 02:31:13,574 iteration 4272 : loss : 0.023137, loss_ce: 0.011704
2022-01-09 02:31:14,978 iteration 4273 : loss : 0.019645, loss_ce: 0.008541
2022-01-09 02:31:16,562 iteration 4274 : loss : 0.030149, loss_ce: 0.013163
2022-01-09 02:31:17,995 iteration 4275 : loss : 0.019736, loss_ce: 0.007242
2022-01-09 02:31:19,524 iteration 4276 : loss : 0.021725, loss_ce: 0.006137
2022-01-09 02:31:21,052 iteration 4277 : loss : 0.027651, loss_ce: 0.009234
2022-01-09 02:31:22,583 iteration 4278 : loss : 0.020818, loss_ce: 0.006792
2022-01-09 02:31:24,026 iteration 4279 : loss : 0.025004, loss_ce: 0.012896
2022-01-09 02:31:25,513 iteration 4280 : loss : 0.022631, loss_ce: 0.007189
2022-01-09 02:31:27,000 iteration 4281 : loss : 0.026356, loss_ce: 0.009061
2022-01-09 02:31:28,396 iteration 4282 : loss : 0.020552, loss_ce: 0.007853
2022-01-09 02:31:29,862 iteration 4283 : loss : 0.027465, loss_ce: 0.009000
2022-01-09 02:31:31,299 iteration 4284 : loss : 0.019988, loss_ce: 0.007647
 63%|█████████████████          | 252/400 [1:55:00<1:06:15, 26.86s/it]2022-01-09 02:31:32,771 iteration 4285 : loss : 0.022842, loss_ce: 0.006665
2022-01-09 02:31:34,312 iteration 4286 : loss : 0.025428, loss_ce: 0.011486
2022-01-09 02:31:35,739 iteration 4287 : loss : 0.023023, loss_ce: 0.010089
2022-01-09 02:31:37,167 iteration 4288 : loss : 0.021344, loss_ce: 0.009928
2022-01-09 02:31:38,652 iteration 4289 : loss : 0.020697, loss_ce: 0.008401
2022-01-09 02:31:40,104 iteration 4290 : loss : 0.035763, loss_ce: 0.008595
2022-01-09 02:31:41,606 iteration 4291 : loss : 0.021130, loss_ce: 0.007303
2022-01-09 02:31:43,096 iteration 4292 : loss : 0.018051, loss_ce: 0.008290
2022-01-09 02:31:44,571 iteration 4293 : loss : 0.024856, loss_ce: 0.007189
2022-01-09 02:31:45,974 iteration 4294 : loss : 0.026853, loss_ce: 0.006286
2022-01-09 02:31:47,581 iteration 4295 : loss : 0.022861, loss_ce: 0.007707
2022-01-09 02:31:49,061 iteration 4296 : loss : 0.015342, loss_ce: 0.004510
2022-01-09 02:31:50,561 iteration 4297 : loss : 0.025035, loss_ce: 0.009496
2022-01-09 02:31:51,993 iteration 4298 : loss : 0.021298, loss_ce: 0.010828
2022-01-09 02:31:53,541 iteration 4299 : loss : 0.021226, loss_ce: 0.009857
2022-01-09 02:31:55,119 iteration 4300 : loss : 0.028976, loss_ce: 0.011019
2022-01-09 02:31:56,690 iteration 4301 : loss : 0.021718, loss_ce: 0.008960
 63%|█████████████████          | 253/400 [1:55:25<1:04:44, 26.42s/it]2022-01-09 02:31:58,297 iteration 4302 : loss : 0.025673, loss_ce: 0.010815
2022-01-09 02:31:59,873 iteration 4303 : loss : 0.043616, loss_ce: 0.007255
2022-01-09 02:32:01,312 iteration 4304 : loss : 0.018490, loss_ce: 0.007155
2022-01-09 02:32:02,760 iteration 4305 : loss : 0.021473, loss_ce: 0.008972
2022-01-09 02:32:04,206 iteration 4306 : loss : 0.019766, loss_ce: 0.006814
2022-01-09 02:32:05,605 iteration 4307 : loss : 0.021447, loss_ce: 0.009950
2022-01-09 02:32:07,123 iteration 4308 : loss : 0.034311, loss_ce: 0.011308
2022-01-09 02:32:08,674 iteration 4309 : loss : 0.032261, loss_ce: 0.010384
2022-01-09 02:32:10,141 iteration 4310 : loss : 0.016358, loss_ce: 0.006548
2022-01-09 02:32:11,554 iteration 4311 : loss : 0.020722, loss_ce: 0.007795
2022-01-09 02:32:12,947 iteration 4312 : loss : 0.030238, loss_ce: 0.010596
2022-01-09 02:32:14,367 iteration 4313 : loss : 0.017853, loss_ce: 0.008757
2022-01-09 02:32:15,797 iteration 4314 : loss : 0.026166, loss_ce: 0.008069
2022-01-09 02:32:17,323 iteration 4315 : loss : 0.018994, loss_ce: 0.006318
2022-01-09 02:32:18,768 iteration 4316 : loss : 0.022542, loss_ce: 0.007839
2022-01-09 02:32:20,325 iteration 4317 : loss : 0.022199, loss_ce: 0.009254
2022-01-09 02:32:21,741 iteration 4318 : loss : 0.028548, loss_ce: 0.012482
 64%|█████████████████▏         | 254/400 [1:55:50<1:03:17, 26.01s/it]2022-01-09 02:32:23,352 iteration 4319 : loss : 0.027728, loss_ce: 0.009022
2022-01-09 02:32:24,876 iteration 4320 : loss : 0.026035, loss_ce: 0.009711
2022-01-09 02:32:26,219 iteration 4321 : loss : 0.019114, loss_ce: 0.005797
2022-01-09 02:32:27,687 iteration 4322 : loss : 0.017789, loss_ce: 0.005635
2022-01-09 02:32:29,075 iteration 4323 : loss : 0.019456, loss_ce: 0.009672
2022-01-09 02:32:30,687 iteration 4324 : loss : 0.049966, loss_ce: 0.030682
2022-01-09 02:32:32,154 iteration 4325 : loss : 0.019558, loss_ce: 0.008088
2022-01-09 02:32:33,581 iteration 4326 : loss : 0.027677, loss_ce: 0.013007
2022-01-09 02:32:35,089 iteration 4327 : loss : 0.021462, loss_ce: 0.005951
2022-01-09 02:32:36,504 iteration 4328 : loss : 0.027202, loss_ce: 0.009119
2022-01-09 02:32:38,055 iteration 4329 : loss : 0.044461, loss_ce: 0.011168
2022-01-09 02:32:39,558 iteration 4330 : loss : 0.028959, loss_ce: 0.014607
2022-01-09 02:32:41,136 iteration 4331 : loss : 0.028885, loss_ce: 0.008206
2022-01-09 02:32:42,657 iteration 4332 : loss : 0.025572, loss_ce: 0.007978
2022-01-09 02:32:44,220 iteration 4333 : loss : 0.027504, loss_ce: 0.011918
2022-01-09 02:32:45,702 iteration 4334 : loss : 0.021827, loss_ce: 0.010235
2022-01-09 02:32:45,702 Training Data Eval:
2022-01-09 02:32:52,981   Average segmentation loss on training set: 0.0147
2022-01-09 02:32:52,982 Validation Data Eval:
2022-01-09 02:32:55,473   Average segmentation loss on validation set: 0.0812
2022-01-09 02:32:56,988 iteration 4335 : loss : 0.032540, loss_ce: 0.008112
 64%|█████████████████▏         | 255/400 [1:56:25<1:09:32, 28.78s/it]2022-01-09 02:32:58,369 iteration 4336 : loss : 0.016178, loss_ce: 0.007900
2022-01-09 02:32:59,932 iteration 4337 : loss : 0.026802, loss_ce: 0.011975
2022-01-09 02:33:01,393 iteration 4338 : loss : 0.020732, loss_ce: 0.006992
2022-01-09 02:33:02,800 iteration 4339 : loss : 0.017531, loss_ce: 0.006280
2022-01-09 02:33:04,312 iteration 4340 : loss : 0.031214, loss_ce: 0.016900
2022-01-09 02:33:05,707 iteration 4341 : loss : 0.018327, loss_ce: 0.005919
2022-01-09 02:33:07,221 iteration 4342 : loss : 0.026045, loss_ce: 0.007102
2022-01-09 02:33:08,733 iteration 4343 : loss : 0.019048, loss_ce: 0.005716
2022-01-09 02:33:10,236 iteration 4344 : loss : 0.028447, loss_ce: 0.016256
2022-01-09 02:33:11,823 iteration 4345 : loss : 0.029603, loss_ce: 0.013547
2022-01-09 02:33:13,206 iteration 4346 : loss : 0.020836, loss_ce: 0.005831
2022-01-09 02:33:14,672 iteration 4347 : loss : 0.017441, loss_ce: 0.007933
2022-01-09 02:33:16,068 iteration 4348 : loss : 0.016659, loss_ce: 0.007981
2022-01-09 02:33:17,445 iteration 4349 : loss : 0.017181, loss_ce: 0.004704
2022-01-09 02:33:18,851 iteration 4350 : loss : 0.025419, loss_ce: 0.009142
2022-01-09 02:33:20,413 iteration 4351 : loss : 0.024857, loss_ce: 0.008582
2022-01-09 02:33:21,925 iteration 4352 : loss : 0.023361, loss_ce: 0.009859
 64%|█████████████████▎         | 256/400 [1:56:50<1:06:18, 27.63s/it]2022-01-09 02:33:23,423 iteration 4353 : loss : 0.022831, loss_ce: 0.007182
2022-01-09 02:33:25,021 iteration 4354 : loss : 0.049137, loss_ce: 0.031549
2022-01-09 02:33:26,576 iteration 4355 : loss : 0.028127, loss_ce: 0.015789
2022-01-09 02:33:28,188 iteration 4356 : loss : 0.022292, loss_ce: 0.006597
2022-01-09 02:33:29,747 iteration 4357 : loss : 0.019910, loss_ce: 0.009686
2022-01-09 02:33:31,299 iteration 4358 : loss : 0.037187, loss_ce: 0.016869
2022-01-09 02:33:32,831 iteration 4359 : loss : 0.029573, loss_ce: 0.010922
2022-01-09 02:33:34,324 iteration 4360 : loss : 0.030138, loss_ce: 0.015095
2022-01-09 02:33:35,903 iteration 4361 : loss : 0.028909, loss_ce: 0.009949
2022-01-09 02:33:37,397 iteration 4362 : loss : 0.025195, loss_ce: 0.012238
2022-01-09 02:33:38,881 iteration 4363 : loss : 0.018528, loss_ce: 0.007979
2022-01-09 02:33:40,397 iteration 4364 : loss : 0.021503, loss_ce: 0.008097
2022-01-09 02:33:41,995 iteration 4365 : loss : 0.036963, loss_ce: 0.017888
2022-01-09 02:33:43,393 iteration 4366 : loss : 0.022432, loss_ce: 0.007134
2022-01-09 02:33:44,723 iteration 4367 : loss : 0.019763, loss_ce: 0.008386
2022-01-09 02:33:46,093 iteration 4368 : loss : 0.020894, loss_ce: 0.006925
2022-01-09 02:33:47,567 iteration 4369 : loss : 0.018669, loss_ce: 0.007391
 64%|█████████████████▎         | 257/400 [1:57:16<1:04:25, 27.03s/it]2022-01-09 02:33:49,113 iteration 4370 : loss : 0.025844, loss_ce: 0.007909
2022-01-09 02:33:50,608 iteration 4371 : loss : 0.023878, loss_ce: 0.011462
2022-01-09 02:33:52,113 iteration 4372 : loss : 0.027100, loss_ce: 0.011948
2022-01-09 02:33:53,614 iteration 4373 : loss : 0.025731, loss_ce: 0.013032
2022-01-09 02:33:55,131 iteration 4374 : loss : 0.021992, loss_ce: 0.007676
2022-01-09 02:33:56,644 iteration 4375 : loss : 0.029841, loss_ce: 0.012485
2022-01-09 02:33:58,076 iteration 4376 : loss : 0.024580, loss_ce: 0.009317
2022-01-09 02:33:59,508 iteration 4377 : loss : 0.022420, loss_ce: 0.006020
2022-01-09 02:34:00,884 iteration 4378 : loss : 0.018612, loss_ce: 0.006770
2022-01-09 02:34:02,272 iteration 4379 : loss : 0.021424, loss_ce: 0.006639
2022-01-09 02:34:03,626 iteration 4380 : loss : 0.018184, loss_ce: 0.006209
2022-01-09 02:34:05,119 iteration 4381 : loss : 0.039955, loss_ce: 0.012932
2022-01-09 02:34:06,579 iteration 4382 : loss : 0.036394, loss_ce: 0.007096
2022-01-09 02:34:08,001 iteration 4383 : loss : 0.026388, loss_ce: 0.008057
2022-01-09 02:34:09,540 iteration 4384 : loss : 0.036005, loss_ce: 0.016714
2022-01-09 02:34:11,077 iteration 4385 : loss : 0.023816, loss_ce: 0.008230
2022-01-09 02:34:12,488 iteration 4386 : loss : 0.019551, loss_ce: 0.009359
 64%|█████████████████▍         | 258/400 [1:57:41<1:02:29, 26.40s/it]2022-01-09 02:34:13,932 iteration 4387 : loss : 0.022966, loss_ce: 0.007159
2022-01-09 02:34:15,395 iteration 4388 : loss : 0.025080, loss_ce: 0.012076
2022-01-09 02:34:16,873 iteration 4389 : loss : 0.032241, loss_ce: 0.009574
2022-01-09 02:34:18,260 iteration 4390 : loss : 0.023555, loss_ce: 0.007746
2022-01-09 02:34:19,749 iteration 4391 : loss : 0.031785, loss_ce: 0.009115
2022-01-09 02:34:21,206 iteration 4392 : loss : 0.025804, loss_ce: 0.007021
2022-01-09 02:34:22,775 iteration 4393 : loss : 0.025631, loss_ce: 0.010585
2022-01-09 02:34:24,288 iteration 4394 : loss : 0.028742, loss_ce: 0.010821
2022-01-09 02:34:25,889 iteration 4395 : loss : 0.028674, loss_ce: 0.011091
2022-01-09 02:34:27,317 iteration 4396 : loss : 0.027241, loss_ce: 0.009177
2022-01-09 02:34:28,928 iteration 4397 : loss : 0.029979, loss_ce: 0.018121
2022-01-09 02:34:30,328 iteration 4398 : loss : 0.022626, loss_ce: 0.008960
2022-01-09 02:34:31,857 iteration 4399 : loss : 0.030410, loss_ce: 0.012194
2022-01-09 02:34:33,349 iteration 4400 : loss : 0.037615, loss_ce: 0.011031
2022-01-09 02:34:35,009 iteration 4401 : loss : 0.040281, loss_ce: 0.014216
2022-01-09 02:34:36,512 iteration 4402 : loss : 0.028692, loss_ce: 0.009442
2022-01-09 02:34:38,015 iteration 4403 : loss : 0.020931, loss_ce: 0.007161
 65%|█████████████████▍         | 259/400 [1:58:06<1:01:25, 26.14s/it]2022-01-09 02:34:39,509 iteration 4404 : loss : 0.035263, loss_ce: 0.020968
2022-01-09 02:34:41,031 iteration 4405 : loss : 0.022727, loss_ce: 0.011546
2022-01-09 02:34:42,540 iteration 4406 : loss : 0.020663, loss_ce: 0.009793
2022-01-09 02:34:44,106 iteration 4407 : loss : 0.032308, loss_ce: 0.012779
2022-01-09 02:34:45,536 iteration 4408 : loss : 0.024527, loss_ce: 0.009432
2022-01-09 02:34:47,041 iteration 4409 : loss : 0.028808, loss_ce: 0.009707
2022-01-09 02:34:48,772 iteration 4410 : loss : 0.053316, loss_ce: 0.017908
2022-01-09 02:34:50,312 iteration 4411 : loss : 0.027129, loss_ce: 0.011978
2022-01-09 02:34:51,776 iteration 4412 : loss : 0.037409, loss_ce: 0.012804
2022-01-09 02:34:53,283 iteration 4413 : loss : 0.021955, loss_ce: 0.006525
2022-01-09 02:34:54,668 iteration 4414 : loss : 0.021450, loss_ce: 0.006442
2022-01-09 02:34:56,188 iteration 4415 : loss : 0.018577, loss_ce: 0.006147
2022-01-09 02:34:57,742 iteration 4416 : loss : 0.036011, loss_ce: 0.013501
2022-01-09 02:34:59,215 iteration 4417 : loss : 0.021614, loss_ce: 0.007305
2022-01-09 02:35:00,709 iteration 4418 : loss : 0.026859, loss_ce: 0.012931
2022-01-09 02:35:02,246 iteration 4419 : loss : 0.036470, loss_ce: 0.012454
2022-01-09 02:35:02,247 Training Data Eval:
2022-01-09 02:35:09,542   Average segmentation loss on training set: 0.0174
2022-01-09 02:35:09,543 Validation Data Eval:
2022-01-09 02:35:12,038   Average segmentation loss on validation set: 0.1136
2022-01-09 02:35:13,450 iteration 4420 : loss : 0.018316, loss_ce: 0.008224
 65%|█████████████████▌         | 260/400 [1:58:42<1:07:29, 28.93s/it]2022-01-09 02:35:14,974 iteration 4421 : loss : 0.020881, loss_ce: 0.007898
2022-01-09 02:35:16,475 iteration 4422 : loss : 0.030253, loss_ce: 0.014478
2022-01-09 02:35:18,020 iteration 4423 : loss : 0.030615, loss_ce: 0.012121
2022-01-09 02:35:19,411 iteration 4424 : loss : 0.024901, loss_ce: 0.007835
2022-01-09 02:35:20,789 iteration 4425 : loss : 0.019417, loss_ce: 0.006681
2022-01-09 02:35:22,337 iteration 4426 : loss : 0.019932, loss_ce: 0.008247
2022-01-09 02:35:23,729 iteration 4427 : loss : 0.018316, loss_ce: 0.007384
2022-01-09 02:35:25,063 iteration 4428 : loss : 0.017106, loss_ce: 0.008550
2022-01-09 02:35:26,424 iteration 4429 : loss : 0.024675, loss_ce: 0.008144
2022-01-09 02:35:27,799 iteration 4430 : loss : 0.017119, loss_ce: 0.007904
2022-01-09 02:35:29,281 iteration 4431 : loss : 0.027582, loss_ce: 0.010852
2022-01-09 02:35:30,806 iteration 4432 : loss : 0.029227, loss_ce: 0.012596
2022-01-09 02:35:32,235 iteration 4433 : loss : 0.030150, loss_ce: 0.008565
2022-01-09 02:35:33,622 iteration 4434 : loss : 0.016466, loss_ce: 0.007432
2022-01-09 02:35:35,085 iteration 4435 : loss : 0.025913, loss_ce: 0.005877
2022-01-09 02:35:36,552 iteration 4436 : loss : 0.019008, loss_ce: 0.007673
2022-01-09 02:35:38,101 iteration 4437 : loss : 0.023176, loss_ce: 0.008241
 65%|█████████████████▌         | 261/400 [1:59:07<1:04:02, 27.65s/it]2022-01-09 02:35:39,719 iteration 4438 : loss : 0.062408, loss_ce: 0.021863
2022-01-09 02:35:41,247 iteration 4439 : loss : 0.019892, loss_ce: 0.007529
2022-01-09 02:35:42,734 iteration 4440 : loss : 0.023713, loss_ce: 0.009196
2022-01-09 02:35:44,354 iteration 4441 : loss : 0.031167, loss_ce: 0.011949
2022-01-09 02:35:45,911 iteration 4442 : loss : 0.017876, loss_ce: 0.009250
2022-01-09 02:35:47,415 iteration 4443 : loss : 0.024064, loss_ce: 0.009684
2022-01-09 02:35:48,792 iteration 4444 : loss : 0.022573, loss_ce: 0.007151
2022-01-09 02:35:50,235 iteration 4445 : loss : 0.024708, loss_ce: 0.010318
2022-01-09 02:35:51,745 iteration 4446 : loss : 0.023522, loss_ce: 0.009867
2022-01-09 02:35:53,198 iteration 4447 : loss : 0.018929, loss_ce: 0.006060
2022-01-09 02:35:54,710 iteration 4448 : loss : 0.038646, loss_ce: 0.013770
2022-01-09 02:35:56,205 iteration 4449 : loss : 0.026005, loss_ce: 0.010999
2022-01-09 02:35:57,770 iteration 4450 : loss : 0.030391, loss_ce: 0.010556
2022-01-09 02:35:59,201 iteration 4451 : loss : 0.023861, loss_ce: 0.010530
2022-01-09 02:36:00,699 iteration 4452 : loss : 0.021234, loss_ce: 0.009618
2022-01-09 02:36:02,131 iteration 4453 : loss : 0.022211, loss_ce: 0.007748
2022-01-09 02:36:03,560 iteration 4454 : loss : 0.021513, loss_ce: 0.009456
 66%|█████████████████▋         | 262/400 [1:59:32<1:02:04, 26.99s/it]2022-01-09 02:36:05,128 iteration 4455 : loss : 0.018688, loss_ce: 0.005885
2022-01-09 02:36:06,509 iteration 4456 : loss : 0.016607, loss_ce: 0.006487
2022-01-09 02:36:08,028 iteration 4457 : loss : 0.025914, loss_ce: 0.011714
2022-01-09 02:36:09,644 iteration 4458 : loss : 0.033964, loss_ce: 0.014316
2022-01-09 02:36:11,337 iteration 4459 : loss : 0.031175, loss_ce: 0.013400
2022-01-09 02:36:12,756 iteration 4460 : loss : 0.025400, loss_ce: 0.010345
2022-01-09 02:36:14,231 iteration 4461 : loss : 0.023667, loss_ce: 0.007051
2022-01-09 02:36:15,756 iteration 4462 : loss : 0.028636, loss_ce: 0.011767
2022-01-09 02:36:17,211 iteration 4463 : loss : 0.034859, loss_ce: 0.013630
2022-01-09 02:36:18,641 iteration 4464 : loss : 0.021673, loss_ce: 0.007028
2022-01-09 02:36:20,006 iteration 4465 : loss : 0.023360, loss_ce: 0.005453
2022-01-09 02:36:21,396 iteration 4466 : loss : 0.021700, loss_ce: 0.006914
2022-01-09 02:36:22,854 iteration 4467 : loss : 0.047559, loss_ce: 0.029193
2022-01-09 02:36:24,355 iteration 4468 : loss : 0.023643, loss_ce: 0.010123
2022-01-09 02:36:25,917 iteration 4469 : loss : 0.024677, loss_ce: 0.012175
2022-01-09 02:36:27,459 iteration 4470 : loss : 0.019199, loss_ce: 0.007975
2022-01-09 02:36:29,039 iteration 4471 : loss : 0.029068, loss_ce: 0.010730
 66%|█████████████████▊         | 263/400 [1:59:57<1:00:34, 26.53s/it]2022-01-09 02:36:30,532 iteration 4472 : loss : 0.019575, loss_ce: 0.009300
2022-01-09 02:36:31,942 iteration 4473 : loss : 0.021741, loss_ce: 0.009073
2022-01-09 02:36:33,415 iteration 4474 : loss : 0.023756, loss_ce: 0.011409
2022-01-09 02:36:34,868 iteration 4475 : loss : 0.021649, loss_ce: 0.006407
2022-01-09 02:36:36,282 iteration 4476 : loss : 0.020933, loss_ce: 0.006987
2022-01-09 02:36:37,716 iteration 4477 : loss : 0.021179, loss_ce: 0.009353
2022-01-09 02:36:39,333 iteration 4478 : loss : 0.025790, loss_ce: 0.011913
2022-01-09 02:36:40,793 iteration 4479 : loss : 0.036471, loss_ce: 0.010069
2022-01-09 02:36:42,278 iteration 4480 : loss : 0.017479, loss_ce: 0.006371
2022-01-09 02:36:43,684 iteration 4481 : loss : 0.023157, loss_ce: 0.008527
2022-01-09 02:36:45,077 iteration 4482 : loss : 0.021590, loss_ce: 0.006478
2022-01-09 02:36:46,493 iteration 4483 : loss : 0.026597, loss_ce: 0.008374
2022-01-09 02:36:47,963 iteration 4484 : loss : 0.016252, loss_ce: 0.005662
2022-01-09 02:36:49,335 iteration 4485 : loss : 0.017922, loss_ce: 0.007679
2022-01-09 02:36:50,875 iteration 4486 : loss : 0.016966, loss_ce: 0.005830
2022-01-09 02:36:52,387 iteration 4487 : loss : 0.026563, loss_ce: 0.012818
2022-01-09 02:36:53,790 iteration 4488 : loss : 0.021795, loss_ce: 0.009001
 66%|███████████████████▏         | 264/400 [2:00:22<58:55, 26.00s/it]2022-01-09 02:36:55,361 iteration 4489 : loss : 0.017612, loss_ce: 0.007152
2022-01-09 02:36:56,817 iteration 4490 : loss : 0.028596, loss_ce: 0.011825
2022-01-09 02:36:58,416 iteration 4491 : loss : 0.030228, loss_ce: 0.009301
2022-01-09 02:36:59,849 iteration 4492 : loss : 0.026315, loss_ce: 0.006537
2022-01-09 02:37:01,441 iteration 4493 : loss : 0.024130, loss_ce: 0.013805
2022-01-09 02:37:02,855 iteration 4494 : loss : 0.023669, loss_ce: 0.008745
2022-01-09 02:37:04,331 iteration 4495 : loss : 0.029415, loss_ce: 0.006861
2022-01-09 02:37:05,821 iteration 4496 : loss : 0.024878, loss_ce: 0.007006
2022-01-09 02:37:07,147 iteration 4497 : loss : 0.026040, loss_ce: 0.013676
2022-01-09 02:37:08,609 iteration 4498 : loss : 0.024264, loss_ce: 0.007350
2022-01-09 02:37:10,022 iteration 4499 : loss : 0.016727, loss_ce: 0.006746
2022-01-09 02:37:11,479 iteration 4500 : loss : 0.029322, loss_ce: 0.009573
2022-01-09 02:37:12,900 iteration 4501 : loss : 0.017847, loss_ce: 0.007927
2022-01-09 02:37:14,337 iteration 4502 : loss : 0.025496, loss_ce: 0.009699
2022-01-09 02:37:15,911 iteration 4503 : loss : 0.022400, loss_ce: 0.008466
2022-01-09 02:37:17,255 iteration 4504 : loss : 0.017732, loss_ce: 0.007203
2022-01-09 02:37:17,256 Training Data Eval:
2022-01-09 02:37:24,602   Average segmentation loss on training set: 0.0140
2022-01-09 02:37:24,603 Validation Data Eval:
2022-01-09 02:37:27,229   Average segmentation loss on validation set: 0.0790
2022-01-09 02:37:28,734 iteration 4505 : loss : 0.018723, loss_ce: 0.006662
 66%|█████████████████▉         | 265/400 [2:00:57<1:04:32, 28.69s/it]2022-01-09 02:37:30,310 iteration 4506 : loss : 0.020483, loss_ce: 0.007484
2022-01-09 02:37:31,864 iteration 4507 : loss : 0.036441, loss_ce: 0.013066
2022-01-09 02:37:33,336 iteration 4508 : loss : 0.027921, loss_ce: 0.011404
2022-01-09 02:37:34,705 iteration 4509 : loss : 0.016320, loss_ce: 0.005429
2022-01-09 02:37:36,150 iteration 4510 : loss : 0.016437, loss_ce: 0.005397
2022-01-09 02:37:37,737 iteration 4511 : loss : 0.024445, loss_ce: 0.009432
2022-01-09 02:37:39,082 iteration 4512 : loss : 0.018754, loss_ce: 0.006726
2022-01-09 02:37:40,510 iteration 4513 : loss : 0.022718, loss_ce: 0.008496
2022-01-09 02:37:41,925 iteration 4514 : loss : 0.024041, loss_ce: 0.010133
2022-01-09 02:37:43,371 iteration 4515 : loss : 0.017382, loss_ce: 0.007063
2022-01-09 02:37:44,838 iteration 4516 : loss : 0.019975, loss_ce: 0.008521
2022-01-09 02:37:46,343 iteration 4517 : loss : 0.028573, loss_ce: 0.011831
2022-01-09 02:37:47,769 iteration 4518 : loss : 0.028040, loss_ce: 0.012117
2022-01-09 02:37:49,133 iteration 4519 : loss : 0.022760, loss_ce: 0.007698
2022-01-09 02:37:50,597 iteration 4520 : loss : 0.026195, loss_ce: 0.006926
2022-01-09 02:37:52,076 iteration 4521 : loss : 0.027932, loss_ce: 0.012897
2022-01-09 02:37:53,618 iteration 4522 : loss : 0.029873, loss_ce: 0.009212
 66%|█████████████████▉         | 266/400 [2:01:22<1:01:31, 27.55s/it]2022-01-09 02:37:55,247 iteration 4523 : loss : 0.029363, loss_ce: 0.015281
2022-01-09 02:37:56,741 iteration 4524 : loss : 0.022478, loss_ce: 0.008291
2022-01-09 02:37:58,246 iteration 4525 : loss : 0.026171, loss_ce: 0.011838
2022-01-09 02:37:59,702 iteration 4526 : loss : 0.022553, loss_ce: 0.006747
2022-01-09 02:38:01,061 iteration 4527 : loss : 0.019603, loss_ce: 0.007820
2022-01-09 02:38:02,477 iteration 4528 : loss : 0.021005, loss_ce: 0.006957
2022-01-09 02:38:03,977 iteration 4529 : loss : 0.023940, loss_ce: 0.010592
2022-01-09 02:38:05,412 iteration 4530 : loss : 0.026788, loss_ce: 0.006879
2022-01-09 02:38:06,917 iteration 4531 : loss : 0.021083, loss_ce: 0.007116
2022-01-09 02:38:08,465 iteration 4532 : loss : 0.017229, loss_ce: 0.005448
2022-01-09 02:38:09,923 iteration 4533 : loss : 0.029942, loss_ce: 0.014289
2022-01-09 02:38:11,395 iteration 4534 : loss : 0.026572, loss_ce: 0.008826
2022-01-09 02:38:12,917 iteration 4535 : loss : 0.026126, loss_ce: 0.010135
2022-01-09 02:38:14,427 iteration 4536 : loss : 0.019584, loss_ce: 0.007920
2022-01-09 02:38:15,913 iteration 4537 : loss : 0.023061, loss_ce: 0.008731
2022-01-09 02:38:17,375 iteration 4538 : loss : 0.024030, loss_ce: 0.008809
2022-01-09 02:38:18,833 iteration 4539 : loss : 0.017645, loss_ce: 0.007287
 67%|███████████████████▎         | 267/400 [2:01:47<59:30, 26.85s/it]2022-01-09 02:38:20,327 iteration 4540 : loss : 0.015242, loss_ce: 0.005597
2022-01-09 02:38:21,835 iteration 4541 : loss : 0.025174, loss_ce: 0.011734
2022-01-09 02:38:23,246 iteration 4542 : loss : 0.022151, loss_ce: 0.008396
2022-01-09 02:38:24,683 iteration 4543 : loss : 0.027777, loss_ce: 0.009347
2022-01-09 02:38:26,095 iteration 4544 : loss : 0.020431, loss_ce: 0.007639
2022-01-09 02:38:27,641 iteration 4545 : loss : 0.023740, loss_ce: 0.010216
2022-01-09 02:38:29,257 iteration 4546 : loss : 0.033097, loss_ce: 0.016286
2022-01-09 02:38:30,855 iteration 4547 : loss : 0.022623, loss_ce: 0.009539
2022-01-09 02:38:32,240 iteration 4548 : loss : 0.021667, loss_ce: 0.007980
2022-01-09 02:38:33,628 iteration 4549 : loss : 0.026262, loss_ce: 0.006397
2022-01-09 02:38:35,102 iteration 4550 : loss : 0.024128, loss_ce: 0.009364
2022-01-09 02:38:36,599 iteration 4551 : loss : 0.021378, loss_ce: 0.009100
2022-01-09 02:38:38,186 iteration 4552 : loss : 0.025717, loss_ce: 0.007431
2022-01-09 02:38:39,682 iteration 4553 : loss : 0.021584, loss_ce: 0.008206
2022-01-09 02:38:41,082 iteration 4554 : loss : 0.024068, loss_ce: 0.009382
2022-01-09 02:38:42,531 iteration 4555 : loss : 0.019543, loss_ce: 0.007356
2022-01-09 02:38:43,975 iteration 4556 : loss : 0.029243, loss_ce: 0.008886
 67%|███████████████████▍         | 268/400 [2:02:12<57:55, 26.33s/it]2022-01-09 02:38:45,454 iteration 4557 : loss : 0.022304, loss_ce: 0.009058
2022-01-09 02:38:46,952 iteration 4558 : loss : 0.026502, loss_ce: 0.008972
2022-01-09 02:38:48,501 iteration 4559 : loss : 0.023924, loss_ce: 0.010088
2022-01-09 02:38:50,011 iteration 4560 : loss : 0.018458, loss_ce: 0.009722
2022-01-09 02:38:51,429 iteration 4561 : loss : 0.015305, loss_ce: 0.005091
2022-01-09 02:38:52,906 iteration 4562 : loss : 0.023885, loss_ce: 0.009085
2022-01-09 02:38:54,345 iteration 4563 : loss : 0.026878, loss_ce: 0.008591
2022-01-09 02:38:55,768 iteration 4564 : loss : 0.019200, loss_ce: 0.007983
2022-01-09 02:38:57,216 iteration 4565 : loss : 0.036761, loss_ce: 0.014137
2022-01-09 02:38:58,699 iteration 4566 : loss : 0.020164, loss_ce: 0.008070
2022-01-09 02:39:00,153 iteration 4567 : loss : 0.026318, loss_ce: 0.010363
2022-01-09 02:39:01,505 iteration 4568 : loss : 0.021441, loss_ce: 0.007378
2022-01-09 02:39:02,887 iteration 4569 : loss : 0.022925, loss_ce: 0.008331
2022-01-09 02:39:04,284 iteration 4570 : loss : 0.019321, loss_ce: 0.007154
2022-01-09 02:39:05,796 iteration 4571 : loss : 0.020858, loss_ce: 0.009281
2022-01-09 02:39:07,306 iteration 4572 : loss : 0.024623, loss_ce: 0.008783
2022-01-09 02:39:08,833 iteration 4573 : loss : 0.023508, loss_ce: 0.006495
 67%|███████████████████▌         | 269/400 [2:02:37<56:31, 25.89s/it]2022-01-09 02:39:10,272 iteration 4574 : loss : 0.014978, loss_ce: 0.004748
2022-01-09 02:39:11,762 iteration 4575 : loss : 0.028970, loss_ce: 0.008418
2022-01-09 02:39:13,133 iteration 4576 : loss : 0.020933, loss_ce: 0.009044
2022-01-09 02:39:14,528 iteration 4577 : loss : 0.016699, loss_ce: 0.007829
2022-01-09 02:39:15,978 iteration 4578 : loss : 0.018725, loss_ce: 0.007477
2022-01-09 02:39:17,374 iteration 4579 : loss : 0.019612, loss_ce: 0.009876
2022-01-09 02:39:18,798 iteration 4580 : loss : 0.018767, loss_ce: 0.007181
2022-01-09 02:39:20,258 iteration 4581 : loss : 0.025208, loss_ce: 0.010699
2022-01-09 02:39:21,717 iteration 4582 : loss : 0.029039, loss_ce: 0.014208
2022-01-09 02:39:23,172 iteration 4583 : loss : 0.029274, loss_ce: 0.010501
2022-01-09 02:39:24,624 iteration 4584 : loss : 0.017307, loss_ce: 0.005904
2022-01-09 02:39:26,138 iteration 4585 : loss : 0.024056, loss_ce: 0.009030
2022-01-09 02:39:27,549 iteration 4586 : loss : 0.019561, loss_ce: 0.008063
2022-01-09 02:39:28,998 iteration 4587 : loss : 0.018581, loss_ce: 0.005844
2022-01-09 02:39:30,439 iteration 4588 : loss : 0.021186, loss_ce: 0.007351
2022-01-09 02:39:31,946 iteration 4589 : loss : 0.021963, loss_ce: 0.008141
2022-01-09 02:39:31,947 Training Data Eval:
2022-01-09 02:39:39,555   Average segmentation loss on training set: 0.0137
2022-01-09 02:39:39,555 Validation Data Eval:
2022-01-09 02:39:42,121   Average segmentation loss on validation set: 0.0627
2022-01-09 02:39:43,566 iteration 4590 : loss : 0.018016, loss_ce: 0.007927
 68%|██████████████████▏        | 270/400 [2:03:12<1:01:50, 28.55s/it]2022-01-09 02:39:45,099 iteration 4591 : loss : 0.032057, loss_ce: 0.008294
2022-01-09 02:39:46,627 iteration 4592 : loss : 0.019823, loss_ce: 0.007158
2022-01-09 02:39:48,124 iteration 4593 : loss : 0.024699, loss_ce: 0.011224
2022-01-09 02:39:49,676 iteration 4594 : loss : 0.036452, loss_ce: 0.010724
2022-01-09 02:39:51,129 iteration 4595 : loss : 0.018224, loss_ce: 0.006313
2022-01-09 02:39:52,548 iteration 4596 : loss : 0.021886, loss_ce: 0.006243
2022-01-09 02:39:54,025 iteration 4597 : loss : 0.015275, loss_ce: 0.006868
2022-01-09 02:39:55,563 iteration 4598 : loss : 0.030036, loss_ce: 0.011337
2022-01-09 02:39:57,084 iteration 4599 : loss : 0.016812, loss_ce: 0.008370
2022-01-09 02:39:58,528 iteration 4600 : loss : 0.021320, loss_ce: 0.006936
2022-01-09 02:40:00,021 iteration 4601 : loss : 0.019218, loss_ce: 0.008677
2022-01-09 02:40:01,613 iteration 4602 : loss : 0.026989, loss_ce: 0.007002
2022-01-09 02:40:03,099 iteration 4603 : loss : 0.027541, loss_ce: 0.009779
2022-01-09 02:40:04,624 iteration 4604 : loss : 0.023694, loss_ce: 0.010024
2022-01-09 02:40:06,166 iteration 4605 : loss : 0.052692, loss_ce: 0.009955
2022-01-09 02:40:07,689 iteration 4606 : loss : 0.022729, loss_ce: 0.009009
2022-01-09 02:40:09,224 iteration 4607 : loss : 0.019752, loss_ce: 0.010520
 68%|███████████████████▋         | 271/400 [2:03:38<59:30, 27.68s/it]2022-01-09 02:40:10,703 iteration 4608 : loss : 0.018013, loss_ce: 0.007387
2022-01-09 02:40:12,183 iteration 4609 : loss : 0.023556, loss_ce: 0.009884
2022-01-09 02:40:13,583 iteration 4610 : loss : 0.022553, loss_ce: 0.006709
2022-01-09 02:40:14,976 iteration 4611 : loss : 0.023603, loss_ce: 0.009704
2022-01-09 02:40:16,335 iteration 4612 : loss : 0.018772, loss_ce: 0.005782
2022-01-09 02:40:17,800 iteration 4613 : loss : 0.022134, loss_ce: 0.007429
2022-01-09 02:40:19,266 iteration 4614 : loss : 0.025378, loss_ce: 0.010590
2022-01-09 02:40:20,796 iteration 4615 : loss : 0.020121, loss_ce: 0.006423
2022-01-09 02:40:22,414 iteration 4616 : loss : 0.027471, loss_ce: 0.011786
2022-01-09 02:40:23,906 iteration 4617 : loss : 0.028675, loss_ce: 0.010463
2022-01-09 02:40:25,300 iteration 4618 : loss : 0.020273, loss_ce: 0.007511
2022-01-09 02:40:26,710 iteration 4619 : loss : 0.014385, loss_ce: 0.006293
2022-01-09 02:40:28,246 iteration 4620 : loss : 0.031391, loss_ce: 0.012641
2022-01-09 02:40:29,698 iteration 4621 : loss : 0.037755, loss_ce: 0.024124
2022-01-09 02:40:31,161 iteration 4622 : loss : 0.038929, loss_ce: 0.014883
2022-01-09 02:40:32,616 iteration 4623 : loss : 0.024257, loss_ce: 0.007477
2022-01-09 02:40:34,091 iteration 4624 : loss : 0.020316, loss_ce: 0.009237
 68%|███████████████████▋         | 272/400 [2:04:02<57:14, 26.83s/it]2022-01-09 02:40:35,678 iteration 4625 : loss : 0.024930, loss_ce: 0.010119
2022-01-09 02:40:37,227 iteration 4626 : loss : 0.023284, loss_ce: 0.010323
2022-01-09 02:40:38,835 iteration 4627 : loss : 0.023358, loss_ce: 0.009148
2022-01-09 02:40:40,277 iteration 4628 : loss : 0.021599, loss_ce: 0.007744
2022-01-09 02:40:41,651 iteration 4629 : loss : 0.021544, loss_ce: 0.006560
2022-01-09 02:40:43,099 iteration 4630 : loss : 0.019163, loss_ce: 0.006676
2022-01-09 02:40:44,571 iteration 4631 : loss : 0.022119, loss_ce: 0.007720
2022-01-09 02:40:46,038 iteration 4632 : loss : 0.022321, loss_ce: 0.006987
2022-01-09 02:40:47,478 iteration 4633 : loss : 0.018237, loss_ce: 0.006086
2022-01-09 02:40:48,894 iteration 4634 : loss : 0.017957, loss_ce: 0.008084
2022-01-09 02:40:50,247 iteration 4635 : loss : 0.015646, loss_ce: 0.006114
2022-01-09 02:40:51,816 iteration 4636 : loss : 0.035058, loss_ce: 0.012671
2022-01-09 02:40:53,243 iteration 4637 : loss : 0.024135, loss_ce: 0.006816
2022-01-09 02:40:54,851 iteration 4638 : loss : 0.032320, loss_ce: 0.014315
2022-01-09 02:40:56,382 iteration 4639 : loss : 0.021337, loss_ce: 0.007810
2022-01-09 02:40:58,013 iteration 4640 : loss : 0.029160, loss_ce: 0.011868
2022-01-09 02:40:59,541 iteration 4641 : loss : 0.023472, loss_ce: 0.009070
 68%|███████████████████▊         | 273/400 [2:04:28<55:55, 26.42s/it]2022-01-09 02:41:01,068 iteration 4642 : loss : 0.021315, loss_ce: 0.004300
2022-01-09 02:41:02,535 iteration 4643 : loss : 0.021602, loss_ce: 0.007516
2022-01-09 02:41:04,083 iteration 4644 : loss : 0.024918, loss_ce: 0.008017
2022-01-09 02:41:05,643 iteration 4645 : loss : 0.033946, loss_ce: 0.015408
2022-01-09 02:41:07,147 iteration 4646 : loss : 0.033793, loss_ce: 0.009753
2022-01-09 02:41:08,523 iteration 4647 : loss : 0.019181, loss_ce: 0.006324
2022-01-09 02:41:09,951 iteration 4648 : loss : 0.058959, loss_ce: 0.023162
2022-01-09 02:41:11,440 iteration 4649 : loss : 0.018986, loss_ce: 0.008057
2022-01-09 02:41:12,978 iteration 4650 : loss : 0.029194, loss_ce: 0.010264
2022-01-09 02:41:14,376 iteration 4651 : loss : 0.018678, loss_ce: 0.009017
2022-01-09 02:41:15,951 iteration 4652 : loss : 0.033628, loss_ce: 0.012829
2022-01-09 02:41:17,385 iteration 4653 : loss : 0.022108, loss_ce: 0.009622
2022-01-09 02:41:18,834 iteration 4654 : loss : 0.026783, loss_ce: 0.011004
2022-01-09 02:41:20,400 iteration 4655 : loss : 0.024252, loss_ce: 0.009637
2022-01-09 02:41:21,986 iteration 4656 : loss : 0.026075, loss_ce: 0.011707
2022-01-09 02:41:23,511 iteration 4657 : loss : 0.021473, loss_ce: 0.010353
2022-01-09 02:41:25,036 iteration 4658 : loss : 0.025264, loss_ce: 0.010076
 68%|███████████████████▊         | 274/400 [2:04:53<54:53, 26.14s/it]2022-01-09 02:41:26,627 iteration 4659 : loss : 0.025101, loss_ce: 0.010589
2022-01-09 02:41:28,180 iteration 4660 : loss : 0.033297, loss_ce: 0.017492
2022-01-09 02:41:29,694 iteration 4661 : loss : 0.034793, loss_ce: 0.013112
2022-01-09 02:41:31,107 iteration 4662 : loss : 0.016724, loss_ce: 0.006109
2022-01-09 02:41:32,618 iteration 4663 : loss : 0.022664, loss_ce: 0.009599
2022-01-09 02:41:34,052 iteration 4664 : loss : 0.016083, loss_ce: 0.005872
2022-01-09 02:41:35,506 iteration 4665 : loss : 0.026594, loss_ce: 0.008748
2022-01-09 02:41:36,838 iteration 4666 : loss : 0.016933, loss_ce: 0.006988
2022-01-09 02:41:38,250 iteration 4667 : loss : 0.019988, loss_ce: 0.007323
2022-01-09 02:41:39,889 iteration 4668 : loss : 0.024973, loss_ce: 0.011342
2022-01-09 02:41:41,282 iteration 4669 : loss : 0.016364, loss_ce: 0.007108
2022-01-09 02:41:42,691 iteration 4670 : loss : 0.022427, loss_ce: 0.009373
2022-01-09 02:41:44,070 iteration 4671 : loss : 0.018464, loss_ce: 0.005715
2022-01-09 02:41:45,658 iteration 4672 : loss : 0.028299, loss_ce: 0.011344
2022-01-09 02:41:47,209 iteration 4673 : loss : 0.031123, loss_ce: 0.013433
2022-01-09 02:41:48,588 iteration 4674 : loss : 0.019008, loss_ce: 0.005819
2022-01-09 02:41:48,588 Training Data Eval:
2022-01-09 02:41:55,885   Average segmentation loss on training set: 0.0135
2022-01-09 02:41:55,886 Validation Data Eval:
2022-01-09 02:41:58,407   Average segmentation loss on validation set: 0.0687
2022-01-09 02:41:59,956 iteration 4675 : loss : 0.019452, loss_ce: 0.007248
 69%|███████████████████▉         | 275/400 [2:05:28<59:57, 28.78s/it]2022-01-09 02:42:01,630 iteration 4676 : loss : 0.028434, loss_ce: 0.009934
2022-01-09 02:42:03,087 iteration 4677 : loss : 0.021574, loss_ce: 0.007771
2022-01-09 02:42:04,599 iteration 4678 : loss : 0.031489, loss_ce: 0.010795
2022-01-09 02:42:06,179 iteration 4679 : loss : 0.031142, loss_ce: 0.010791
2022-01-09 02:42:07,641 iteration 4680 : loss : 0.016793, loss_ce: 0.007046
2022-01-09 02:42:09,092 iteration 4681 : loss : 0.027224, loss_ce: 0.007488
2022-01-09 02:42:10,526 iteration 4682 : loss : 0.026783, loss_ce: 0.009948
2022-01-09 02:42:11,847 iteration 4683 : loss : 0.020118, loss_ce: 0.005621
2022-01-09 02:42:13,358 iteration 4684 : loss : 0.020385, loss_ce: 0.009478
2022-01-09 02:42:14,861 iteration 4685 : loss : 0.034102, loss_ce: 0.010456
2022-01-09 02:42:16,326 iteration 4686 : loss : 0.025020, loss_ce: 0.007837
2022-01-09 02:42:17,750 iteration 4687 : loss : 0.022689, loss_ce: 0.010113
2022-01-09 02:42:19,100 iteration 4688 : loss : 0.015430, loss_ce: 0.006996
2022-01-09 02:42:20,475 iteration 4689 : loss : 0.015454, loss_ce: 0.007437
2022-01-09 02:42:21,880 iteration 4690 : loss : 0.024417, loss_ce: 0.009143
2022-01-09 02:42:23,400 iteration 4691 : loss : 0.029169, loss_ce: 0.015737
2022-01-09 02:42:24,834 iteration 4692 : loss : 0.033018, loss_ce: 0.008634
 69%|████████████████████         | 276/400 [2:05:53<57:03, 27.61s/it]2022-01-09 02:42:26,457 iteration 4693 : loss : 0.024643, loss_ce: 0.008403
2022-01-09 02:42:27,961 iteration 4694 : loss : 0.024283, loss_ce: 0.009790
2022-01-09 02:42:29,241 iteration 4695 : loss : 0.016066, loss_ce: 0.004976
2022-01-09 02:42:30,797 iteration 4696 : loss : 0.031845, loss_ce: 0.012561
2022-01-09 02:42:32,219 iteration 4697 : loss : 0.028620, loss_ce: 0.009786
2022-01-09 02:42:33,689 iteration 4698 : loss : 0.022587, loss_ce: 0.010745
2022-01-09 02:42:35,237 iteration 4699 : loss : 0.015646, loss_ce: 0.006747
2022-01-09 02:42:36,640 iteration 4700 : loss : 0.020070, loss_ce: 0.009289
2022-01-09 02:42:38,243 iteration 4701 : loss : 0.024129, loss_ce: 0.008438
2022-01-09 02:42:39,687 iteration 4702 : loss : 0.021039, loss_ce: 0.009031
2022-01-09 02:42:41,100 iteration 4703 : loss : 0.022583, loss_ce: 0.005754
2022-01-09 02:42:42,563 iteration 4704 : loss : 0.018179, loss_ce: 0.006082
2022-01-09 02:42:43,855 iteration 4705 : loss : 0.015264, loss_ce: 0.006508
2022-01-09 02:42:45,311 iteration 4706 : loss : 0.021311, loss_ce: 0.005530
2022-01-09 02:42:46,848 iteration 4707 : loss : 0.016686, loss_ce: 0.005944
2022-01-09 02:42:48,401 iteration 4708 : loss : 0.037175, loss_ce: 0.016162
2022-01-09 02:42:49,915 iteration 4709 : loss : 0.023616, loss_ce: 0.007115
 69%|████████████████████         | 277/400 [2:06:18<55:02, 26.85s/it]2022-01-09 02:42:51,484 iteration 4710 : loss : 0.025402, loss_ce: 0.005617
2022-01-09 02:42:53,149 iteration 4711 : loss : 0.027711, loss_ce: 0.011041
2022-01-09 02:42:54,609 iteration 4712 : loss : 0.021014, loss_ce: 0.008756
2022-01-09 02:42:56,091 iteration 4713 : loss : 0.020470, loss_ce: 0.007539
2022-01-09 02:42:57,606 iteration 4714 : loss : 0.021594, loss_ce: 0.006949
2022-01-09 02:42:59,179 iteration 4715 : loss : 0.027174, loss_ce: 0.011377
2022-01-09 02:43:00,689 iteration 4716 : loss : 0.026302, loss_ce: 0.011909
2022-01-09 02:43:02,153 iteration 4717 : loss : 0.025592, loss_ce: 0.010711
2022-01-09 02:43:03,535 iteration 4718 : loss : 0.018613, loss_ce: 0.006612
2022-01-09 02:43:04,918 iteration 4719 : loss : 0.024767, loss_ce: 0.012884
2022-01-09 02:43:06,327 iteration 4720 : loss : 0.016962, loss_ce: 0.006841
2022-01-09 02:43:07,760 iteration 4721 : loss : 0.027308, loss_ce: 0.012186
2022-01-09 02:43:09,294 iteration 4722 : loss : 0.035427, loss_ce: 0.010829
2022-01-09 02:43:10,753 iteration 4723 : loss : 0.019647, loss_ce: 0.008411
2022-01-09 02:43:12,133 iteration 4724 : loss : 0.019128, loss_ce: 0.007922
2022-01-09 02:43:13,507 iteration 4725 : loss : 0.027412, loss_ce: 0.010501
2022-01-09 02:43:14,964 iteration 4726 : loss : 0.021265, loss_ce: 0.008663
 70%|████████████████████▏        | 278/400 [2:06:43<53:29, 26.31s/it]2022-01-09 02:43:16,576 iteration 4727 : loss : 0.030668, loss_ce: 0.009280
2022-01-09 02:43:17,968 iteration 4728 : loss : 0.017788, loss_ce: 0.007408
2022-01-09 02:43:19,377 iteration 4729 : loss : 0.020177, loss_ce: 0.008499
2022-01-09 02:43:20,783 iteration 4730 : loss : 0.018624, loss_ce: 0.008662
2022-01-09 02:43:22,318 iteration 4731 : loss : 0.023664, loss_ce: 0.008206
2022-01-09 02:43:23,806 iteration 4732 : loss : 0.024423, loss_ce: 0.008751
2022-01-09 02:43:25,281 iteration 4733 : loss : 0.016700, loss_ce: 0.006963
2022-01-09 02:43:26,712 iteration 4734 : loss : 0.020992, loss_ce: 0.008655
2022-01-09 02:43:28,293 iteration 4735 : loss : 0.022915, loss_ce: 0.008461
2022-01-09 02:43:29,667 iteration 4736 : loss : 0.026760, loss_ce: 0.012223
2022-01-09 02:43:31,240 iteration 4737 : loss : 0.028298, loss_ce: 0.009991
2022-01-09 02:43:32,704 iteration 4738 : loss : 0.025460, loss_ce: 0.009193
2022-01-09 02:43:34,163 iteration 4739 : loss : 0.015642, loss_ce: 0.005471
2022-01-09 02:43:35,691 iteration 4740 : loss : 0.019996, loss_ce: 0.008944
2022-01-09 02:43:37,132 iteration 4741 : loss : 0.018783, loss_ce: 0.007733
2022-01-09 02:43:38,588 iteration 4742 : loss : 0.024094, loss_ce: 0.011292
2022-01-09 02:43:40,159 iteration 4743 : loss : 0.033257, loss_ce: 0.011139
 70%|████████████████████▏        | 279/400 [2:07:09<52:23, 25.98s/it]2022-01-09 02:43:41,765 iteration 4744 : loss : 0.020025, loss_ce: 0.007488
2022-01-09 02:43:43,236 iteration 4745 : loss : 0.027324, loss_ce: 0.007137
2022-01-09 02:43:44,650 iteration 4746 : loss : 0.020198, loss_ce: 0.010616
2022-01-09 02:43:46,082 iteration 4747 : loss : 0.032549, loss_ce: 0.011312
2022-01-09 02:43:47,589 iteration 4748 : loss : 0.022606, loss_ce: 0.008376
2022-01-09 02:43:49,125 iteration 4749 : loss : 0.024119, loss_ce: 0.009040
2022-01-09 02:43:50,736 iteration 4750 : loss : 0.030514, loss_ce: 0.013311
2022-01-09 02:43:52,248 iteration 4751 : loss : 0.024520, loss_ce: 0.009882
2022-01-09 02:43:53,810 iteration 4752 : loss : 0.022967, loss_ce: 0.007926
2022-01-09 02:43:55,377 iteration 4753 : loss : 0.030756, loss_ce: 0.011364
2022-01-09 02:43:56,890 iteration 4754 : loss : 0.021415, loss_ce: 0.007767
2022-01-09 02:43:58,510 iteration 4755 : loss : 0.018988, loss_ce: 0.007219
2022-01-09 02:43:59,937 iteration 4756 : loss : 0.018859, loss_ce: 0.008819
2022-01-09 02:44:01,396 iteration 4757 : loss : 0.026612, loss_ce: 0.010866
2022-01-09 02:44:02,813 iteration 4758 : loss : 0.022280, loss_ce: 0.007158
2022-01-09 02:44:04,278 iteration 4759 : loss : 0.016252, loss_ce: 0.005671
2022-01-09 02:44:04,278 Training Data Eval:
2022-01-09 02:44:11,694   Average segmentation loss on training set: 0.0134
2022-01-09 02:44:11,695 Validation Data Eval:
2022-01-09 02:44:14,241   Average segmentation loss on validation set: 0.0733
2022-01-09 02:44:15,683 iteration 4760 : loss : 0.017924, loss_ce: 0.008369
 70%|████████████████████▎        | 280/400 [2:07:44<57:40, 28.84s/it]2022-01-09 02:44:17,197 iteration 4761 : loss : 0.023813, loss_ce: 0.007525
2022-01-09 02:44:18,703 iteration 4762 : loss : 0.028845, loss_ce: 0.009838
2022-01-09 02:44:20,095 iteration 4763 : loss : 0.016432, loss_ce: 0.007313
2022-01-09 02:44:21,603 iteration 4764 : loss : 0.030578, loss_ce: 0.010479
2022-01-09 02:44:23,042 iteration 4765 : loss : 0.030517, loss_ce: 0.012627
2022-01-09 02:44:24,596 iteration 4766 : loss : 0.020533, loss_ce: 0.006803
2022-01-09 02:44:26,093 iteration 4767 : loss : 0.016205, loss_ce: 0.005930
2022-01-09 02:44:27,504 iteration 4768 : loss : 0.022507, loss_ce: 0.006706
2022-01-09 02:44:29,019 iteration 4769 : loss : 0.035819, loss_ce: 0.013712
2022-01-09 02:44:30,540 iteration 4770 : loss : 0.026588, loss_ce: 0.008568
2022-01-09 02:44:31,931 iteration 4771 : loss : 0.017493, loss_ce: 0.005875
2022-01-09 02:44:33,455 iteration 4772 : loss : 0.023654, loss_ce: 0.009685
2022-01-09 02:44:34,957 iteration 4773 : loss : 0.027154, loss_ce: 0.009899
2022-01-09 02:44:36,449 iteration 4774 : loss : 0.016879, loss_ce: 0.006849
2022-01-09 02:44:37,870 iteration 4775 : loss : 0.027110, loss_ce: 0.007844
2022-01-09 02:44:39,323 iteration 4776 : loss : 0.017285, loss_ce: 0.009272
2022-01-09 02:44:40,969 iteration 4777 : loss : 0.030595, loss_ce: 0.015187
 70%|████████████████████▎        | 281/400 [2:08:09<55:04, 27.77s/it]2022-01-09 02:44:42,406 iteration 4778 : loss : 0.014750, loss_ce: 0.004379
2022-01-09 02:44:43,835 iteration 4779 : loss : 0.019989, loss_ce: 0.007348
2022-01-09 02:44:45,233 iteration 4780 : loss : 0.015179, loss_ce: 0.005316
2022-01-09 02:44:46,696 iteration 4781 : loss : 0.021985, loss_ce: 0.006787
2022-01-09 02:44:48,272 iteration 4782 : loss : 0.024057, loss_ce: 0.011494
2022-01-09 02:44:49,781 iteration 4783 : loss : 0.016269, loss_ce: 0.007227
2022-01-09 02:44:51,220 iteration 4784 : loss : 0.021186, loss_ce: 0.008179
2022-01-09 02:44:52,832 iteration 4785 : loss : 0.023973, loss_ce: 0.008069
2022-01-09 02:44:54,372 iteration 4786 : loss : 0.021900, loss_ce: 0.007626
2022-01-09 02:44:55,898 iteration 4787 : loss : 0.022685, loss_ce: 0.007702
2022-01-09 02:44:57,430 iteration 4788 : loss : 0.021339, loss_ce: 0.007284
2022-01-09 02:44:58,835 iteration 4789 : loss : 0.016704, loss_ce: 0.008800
2022-01-09 02:45:00,447 iteration 4790 : loss : 0.031010, loss_ce: 0.011974
2022-01-09 02:45:01,906 iteration 4791 : loss : 0.025359, loss_ce: 0.011969
2022-01-09 02:45:03,459 iteration 4792 : loss : 0.024042, loss_ce: 0.007553
2022-01-09 02:45:04,892 iteration 4793 : loss : 0.021382, loss_ce: 0.009207
2022-01-09 02:45:06,272 iteration 4794 : loss : 0.025567, loss_ce: 0.007913
 70%|████████████████████▍        | 282/400 [2:08:35<53:09, 27.03s/it]2022-01-09 02:45:07,853 iteration 4795 : loss : 0.024522, loss_ce: 0.007760
2022-01-09 02:45:09,357 iteration 4796 : loss : 0.016714, loss_ce: 0.005394
2022-01-09 02:45:10,866 iteration 4797 : loss : 0.026097, loss_ce: 0.010952
2022-01-09 02:45:12,445 iteration 4798 : loss : 0.028577, loss_ce: 0.010292
2022-01-09 02:45:13,929 iteration 4799 : loss : 0.015748, loss_ce: 0.005181
2022-01-09 02:45:15,399 iteration 4800 : loss : 0.025981, loss_ce: 0.008321
2022-01-09 02:45:17,002 iteration 4801 : loss : 0.022695, loss_ce: 0.009037
2022-01-09 02:45:18,408 iteration 4802 : loss : 0.019061, loss_ce: 0.010375
2022-01-09 02:45:19,872 iteration 4803 : loss : 0.017442, loss_ce: 0.008355
2022-01-09 02:45:21,453 iteration 4804 : loss : 0.042802, loss_ce: 0.012617
2022-01-09 02:45:23,001 iteration 4805 : loss : 0.029666, loss_ce: 0.010445
2022-01-09 02:45:24,556 iteration 4806 : loss : 0.028766, loss_ce: 0.011732
2022-01-09 02:45:25,938 iteration 4807 : loss : 0.023205, loss_ce: 0.009259
2022-01-09 02:45:27,278 iteration 4808 : loss : 0.015764, loss_ce: 0.006926
2022-01-09 02:45:28,763 iteration 4809 : loss : 0.025094, loss_ce: 0.011783
2022-01-09 02:45:30,221 iteration 4810 : loss : 0.023459, loss_ce: 0.006323
2022-01-09 02:45:31,713 iteration 4811 : loss : 0.026086, loss_ce: 0.012605
 71%|████████████████████▌        | 283/400 [2:09:00<51:46, 26.55s/it]2022-01-09 02:45:33,174 iteration 4812 : loss : 0.024327, loss_ce: 0.009376
2022-01-09 02:45:34,671 iteration 4813 : loss : 0.019469, loss_ce: 0.005032
2022-01-09 02:45:36,202 iteration 4814 : loss : 0.032682, loss_ce: 0.008719
2022-01-09 02:45:37,579 iteration 4815 : loss : 0.018775, loss_ce: 0.007983
2022-01-09 02:45:39,197 iteration 4816 : loss : 0.024068, loss_ce: 0.010156
2022-01-09 02:45:40,644 iteration 4817 : loss : 0.030888, loss_ce: 0.015749
2022-01-09 02:45:42,078 iteration 4818 : loss : 0.024258, loss_ce: 0.008641
2022-01-09 02:45:43,429 iteration 4819 : loss : 0.023603, loss_ce: 0.009258
2022-01-09 02:45:44,850 iteration 4820 : loss : 0.024840, loss_ce: 0.010016
2022-01-09 02:45:46,436 iteration 4821 : loss : 0.036287, loss_ce: 0.016233
2022-01-09 02:45:48,005 iteration 4822 : loss : 0.031537, loss_ce: 0.011589
2022-01-09 02:45:49,462 iteration 4823 : loss : 0.023100, loss_ce: 0.010065
2022-01-09 02:45:50,965 iteration 4824 : loss : 0.026866, loss_ce: 0.012992
2022-01-09 02:45:52,389 iteration 4825 : loss : 0.023999, loss_ce: 0.009064
2022-01-09 02:45:53,805 iteration 4826 : loss : 0.022675, loss_ce: 0.007612
2022-01-09 02:45:55,242 iteration 4827 : loss : 0.018096, loss_ce: 0.007435
2022-01-09 02:45:56,655 iteration 4828 : loss : 0.027554, loss_ce: 0.010769
 71%|████████████████████▌        | 284/400 [2:09:25<50:24, 26.07s/it]2022-01-09 02:45:58,110 iteration 4829 : loss : 0.027309, loss_ce: 0.008665
2022-01-09 02:45:59,618 iteration 4830 : loss : 0.020247, loss_ce: 0.009348
2022-01-09 02:46:01,105 iteration 4831 : loss : 0.022235, loss_ce: 0.008789
2022-01-09 02:46:02,552 iteration 4832 : loss : 0.023375, loss_ce: 0.007180
2022-01-09 02:46:04,061 iteration 4833 : loss : 0.039397, loss_ce: 0.015176
2022-01-09 02:46:05,446 iteration 4834 : loss : 0.016803, loss_ce: 0.006070
2022-01-09 02:46:07,056 iteration 4835 : loss : 0.021399, loss_ce: 0.009956
2022-01-09 02:46:08,535 iteration 4836 : loss : 0.034889, loss_ce: 0.015065
2022-01-09 02:46:10,070 iteration 4837 : loss : 0.024063, loss_ce: 0.006895
2022-01-09 02:46:11,507 iteration 4838 : loss : 0.016782, loss_ce: 0.005302
2022-01-09 02:46:13,009 iteration 4839 : loss : 0.026448, loss_ce: 0.011887
2022-01-09 02:46:14,482 iteration 4840 : loss : 0.026179, loss_ce: 0.012557
2022-01-09 02:46:15,851 iteration 4841 : loss : 0.014628, loss_ce: 0.004569
2022-01-09 02:46:17,317 iteration 4842 : loss : 0.019933, loss_ce: 0.007457
2022-01-09 02:46:18,905 iteration 4843 : loss : 0.023433, loss_ce: 0.006852
2022-01-09 02:46:20,366 iteration 4844 : loss : 0.038394, loss_ce: 0.013762
2022-01-09 02:46:20,367 Training Data Eval:
2022-01-09 02:46:27,676   Average segmentation loss on training set: 0.0135
2022-01-09 02:46:27,676 Validation Data Eval:
2022-01-09 02:46:30,361   Average segmentation loss on validation set: 0.0707
2022-01-09 02:46:31,961 iteration 4845 : loss : 0.024127, loss_ce: 0.011264
 71%|████████████████████▋        | 285/400 [2:10:00<55:16, 28.84s/it]2022-01-09 02:46:33,495 iteration 4846 : loss : 0.019367, loss_ce: 0.009164
2022-01-09 02:46:34,946 iteration 4847 : loss : 0.021414, loss_ce: 0.008028
2022-01-09 02:46:36,488 iteration 4848 : loss : 0.029012, loss_ce: 0.013436
2022-01-09 02:46:37,997 iteration 4849 : loss : 0.021711, loss_ce: 0.005787
2022-01-09 02:46:39,449 iteration 4850 : loss : 0.028325, loss_ce: 0.009853
2022-01-09 02:46:41,064 iteration 4851 : loss : 0.022423, loss_ce: 0.009505
2022-01-09 02:46:42,511 iteration 4852 : loss : 0.024420, loss_ce: 0.008545
2022-01-09 02:46:44,000 iteration 4853 : loss : 0.020477, loss_ce: 0.008924
2022-01-09 02:46:45,483 iteration 4854 : loss : 0.018482, loss_ce: 0.007744
2022-01-09 02:46:46,913 iteration 4855 : loss : 0.028810, loss_ce: 0.008630
2022-01-09 02:46:48,507 iteration 4856 : loss : 0.030340, loss_ce: 0.013163
2022-01-09 02:46:49,940 iteration 4857 : loss : 0.014783, loss_ce: 0.005703
2022-01-09 02:46:51,520 iteration 4858 : loss : 0.017988, loss_ce: 0.006879
2022-01-09 02:46:52,910 iteration 4859 : loss : 0.015218, loss_ce: 0.006331
2022-01-09 02:46:54,444 iteration 4860 : loss : 0.028891, loss_ce: 0.014323
2022-01-09 02:46:55,909 iteration 4861 : loss : 0.021123, loss_ce: 0.007343
2022-01-09 02:46:57,482 iteration 4862 : loss : 0.025052, loss_ce: 0.013659
 72%|████████████████████▋        | 286/400 [2:10:26<52:54, 27.84s/it]2022-01-09 02:46:59,155 iteration 4863 : loss : 0.036219, loss_ce: 0.015202
2022-01-09 02:47:00,645 iteration 4864 : loss : 0.018118, loss_ce: 0.006666
2022-01-09 02:47:02,121 iteration 4865 : loss : 0.016523, loss_ce: 0.008075
2022-01-09 02:47:03,716 iteration 4866 : loss : 0.024607, loss_ce: 0.007801
2022-01-09 02:47:05,235 iteration 4867 : loss : 0.022166, loss_ce: 0.009006
2022-01-09 02:47:06,611 iteration 4868 : loss : 0.017389, loss_ce: 0.007441
2022-01-09 02:47:08,225 iteration 4869 : loss : 0.031402, loss_ce: 0.015803
2022-01-09 02:47:09,695 iteration 4870 : loss : 0.015891, loss_ce: 0.005771
2022-01-09 02:47:11,312 iteration 4871 : loss : 0.025689, loss_ce: 0.008989
2022-01-09 02:47:12,717 iteration 4872 : loss : 0.018119, loss_ce: 0.005984
2022-01-09 02:47:14,321 iteration 4873 : loss : 0.021958, loss_ce: 0.008262
2022-01-09 02:47:15,829 iteration 4874 : loss : 0.033874, loss_ce: 0.010748
2022-01-09 02:47:17,277 iteration 4875 : loss : 0.023318, loss_ce: 0.009307
2022-01-09 02:47:18,639 iteration 4876 : loss : 0.018979, loss_ce: 0.006106
2022-01-09 02:47:20,182 iteration 4877 : loss : 0.024877, loss_ce: 0.012033
2022-01-09 02:47:21,767 iteration 4878 : loss : 0.037588, loss_ce: 0.015058
2022-01-09 02:47:23,251 iteration 4879 : loss : 0.015837, loss_ce: 0.005565
 72%|████████████████████▊        | 287/400 [2:10:52<51:16, 27.22s/it]2022-01-09 02:47:24,767 iteration 4880 : loss : 0.029417, loss_ce: 0.014699
2022-01-09 02:47:26,173 iteration 4881 : loss : 0.023034, loss_ce: 0.009179
2022-01-09 02:47:27,525 iteration 4882 : loss : 0.019158, loss_ce: 0.009136
2022-01-09 02:47:29,028 iteration 4883 : loss : 0.024948, loss_ce: 0.010804
2022-01-09 02:47:30,552 iteration 4884 : loss : 0.017968, loss_ce: 0.005556
2022-01-09 02:47:31,878 iteration 4885 : loss : 0.014808, loss_ce: 0.005923
2022-01-09 02:47:33,352 iteration 4886 : loss : 0.018468, loss_ce: 0.004888
2022-01-09 02:47:34,756 iteration 4887 : loss : 0.014863, loss_ce: 0.004424
2022-01-09 02:47:36,272 iteration 4888 : loss : 0.033624, loss_ce: 0.010618
2022-01-09 02:47:37,763 iteration 4889 : loss : 0.037332, loss_ce: 0.017244
2022-01-09 02:47:39,184 iteration 4890 : loss : 0.017307, loss_ce: 0.009516
2022-01-09 02:47:40,503 iteration 4891 : loss : 0.014327, loss_ce: 0.005400
2022-01-09 02:47:42,041 iteration 4892 : loss : 0.017132, loss_ce: 0.007072
2022-01-09 02:47:43,545 iteration 4893 : loss : 0.024794, loss_ce: 0.011503
2022-01-09 02:47:45,015 iteration 4894 : loss : 0.024283, loss_ce: 0.014084
2022-01-09 02:47:46,476 iteration 4895 : loss : 0.071126, loss_ce: 0.023428
2022-01-09 02:47:47,925 iteration 4896 : loss : 0.027581, loss_ce: 0.008413
 72%|████████████████████▉        | 288/400 [2:11:16<49:23, 26.46s/it]2022-01-09 02:47:49,452 iteration 4897 : loss : 0.017480, loss_ce: 0.007026
2022-01-09 02:47:51,023 iteration 4898 : loss : 0.042386, loss_ce: 0.026109
2022-01-09 02:47:52,515 iteration 4899 : loss : 0.025313, loss_ce: 0.008463
2022-01-09 02:47:53,951 iteration 4900 : loss : 0.024296, loss_ce: 0.011021
2022-01-09 02:47:55,407 iteration 4901 : loss : 0.053191, loss_ce: 0.015628
2022-01-09 02:47:56,861 iteration 4902 : loss : 0.017465, loss_ce: 0.007110
2022-01-09 02:47:58,381 iteration 4903 : loss : 0.018449, loss_ce: 0.008262
2022-01-09 02:47:59,750 iteration 4904 : loss : 0.023906, loss_ce: 0.006303
2022-01-09 02:48:01,197 iteration 4905 : loss : 0.023637, loss_ce: 0.006479
2022-01-09 02:48:02,734 iteration 4906 : loss : 0.031461, loss_ce: 0.012231
2022-01-09 02:48:04,161 iteration 4907 : loss : 0.018066, loss_ce: 0.006413
2022-01-09 02:48:05,726 iteration 4908 : loss : 0.027888, loss_ce: 0.012534
2022-01-09 02:48:07,231 iteration 4909 : loss : 0.022874, loss_ce: 0.007186
2022-01-09 02:48:08,647 iteration 4910 : loss : 0.020204, loss_ce: 0.009368
2022-01-09 02:48:10,088 iteration 4911 : loss : 0.020443, loss_ce: 0.007616
2022-01-09 02:48:11,562 iteration 4912 : loss : 0.024050, loss_ce: 0.010620
2022-01-09 02:48:13,033 iteration 4913 : loss : 0.035796, loss_ce: 0.014877
 72%|████████████████████▉        | 289/400 [2:11:41<48:11, 26.05s/it]2022-01-09 02:48:14,619 iteration 4914 : loss : 0.025702, loss_ce: 0.010182
2022-01-09 02:48:16,065 iteration 4915 : loss : 0.033487, loss_ce: 0.012226
2022-01-09 02:48:17,530 iteration 4916 : loss : 0.019548, loss_ce: 0.005704
2022-01-09 02:48:19,024 iteration 4917 : loss : 0.023290, loss_ce: 0.008508
2022-01-09 02:48:20,543 iteration 4918 : loss : 0.022095, loss_ce: 0.007439
2022-01-09 02:48:22,215 iteration 4919 : loss : 0.038740, loss_ce: 0.017497
2022-01-09 02:48:23,668 iteration 4920 : loss : 0.017296, loss_ce: 0.006624
2022-01-09 02:48:25,079 iteration 4921 : loss : 0.016954, loss_ce: 0.008057
2022-01-09 02:48:26,456 iteration 4922 : loss : 0.022611, loss_ce: 0.006504
2022-01-09 02:48:27,984 iteration 4923 : loss : 0.028156, loss_ce: 0.010771
2022-01-09 02:48:29,550 iteration 4924 : loss : 0.028355, loss_ce: 0.015303
2022-01-09 02:48:31,058 iteration 4925 : loss : 0.017855, loss_ce: 0.006753
2022-01-09 02:48:32,602 iteration 4926 : loss : 0.029102, loss_ce: 0.013943
2022-01-09 02:48:34,045 iteration 4927 : loss : 0.017005, loss_ce: 0.005594
2022-01-09 02:48:35,593 iteration 4928 : loss : 0.027168, loss_ce: 0.010576
2022-01-09 02:48:37,168 iteration 4929 : loss : 0.020518, loss_ce: 0.009076
2022-01-09 02:48:37,169 Training Data Eval:
2022-01-09 02:48:44,457   Average segmentation loss on training set: 0.0130
2022-01-09 02:48:44,457 Validation Data Eval:
2022-01-09 02:48:46,959   Average segmentation loss on validation set: 0.0633
2022-01-09 02:48:48,430 iteration 4930 : loss : 0.021479, loss_ce: 0.006933
 72%|█████████████████████        | 290/400 [2:12:17<52:54, 28.86s/it]2022-01-09 02:48:49,940 iteration 4931 : loss : 0.026886, loss_ce: 0.011505
2022-01-09 02:48:51,444 iteration 4932 : loss : 0.019774, loss_ce: 0.010228
2022-01-09 02:48:52,829 iteration 4933 : loss : 0.017621, loss_ce: 0.008959
2022-01-09 02:48:54,370 iteration 4934 : loss : 0.028355, loss_ce: 0.010644
2022-01-09 02:48:55,837 iteration 4935 : loss : 0.021540, loss_ce: 0.007249
2022-01-09 02:48:57,332 iteration 4936 : loss : 0.022875, loss_ce: 0.008676
2022-01-09 02:48:58,826 iteration 4937 : loss : 0.018168, loss_ce: 0.008609
2022-01-09 02:49:00,362 iteration 4938 : loss : 0.025776, loss_ce: 0.009175
2022-01-09 02:49:01,896 iteration 4939 : loss : 0.021552, loss_ce: 0.008044
2022-01-09 02:49:03,345 iteration 4940 : loss : 0.029946, loss_ce: 0.014127
2022-01-09 02:49:04,836 iteration 4941 : loss : 0.023781, loss_ce: 0.011277
2022-01-09 02:49:06,332 iteration 4942 : loss : 0.021838, loss_ce: 0.007266
2022-01-09 02:49:07,936 iteration 4943 : loss : 0.026693, loss_ce: 0.007430
2022-01-09 02:49:09,405 iteration 4944 : loss : 0.038398, loss_ce: 0.012504
2022-01-09 02:49:10,934 iteration 4945 : loss : 0.021829, loss_ce: 0.008403
2022-01-09 02:49:12,493 iteration 4946 : loss : 0.032140, loss_ce: 0.009223
2022-01-09 02:49:13,947 iteration 4947 : loss : 0.018553, loss_ce: 0.006535
 73%|█████████████████████        | 291/400 [2:12:42<50:36, 27.85s/it]2022-01-09 02:49:15,514 iteration 4948 : loss : 0.026904, loss_ce: 0.009450
2022-01-09 02:49:16,955 iteration 4949 : loss : 0.020720, loss_ce: 0.008500
2022-01-09 02:49:18,482 iteration 4950 : loss : 0.026888, loss_ce: 0.011049
2022-01-09 02:49:19,995 iteration 4951 : loss : 0.019918, loss_ce: 0.006336
2022-01-09 02:49:21,496 iteration 4952 : loss : 0.016757, loss_ce: 0.009468
2022-01-09 02:49:23,020 iteration 4953 : loss : 0.021077, loss_ce: 0.009867
2022-01-09 02:49:24,456 iteration 4954 : loss : 0.014259, loss_ce: 0.004562
2022-01-09 02:49:25,840 iteration 4955 : loss : 0.027371, loss_ce: 0.007732
2022-01-09 02:49:27,298 iteration 4956 : loss : 0.022753, loss_ce: 0.007079
2022-01-09 02:49:28,641 iteration 4957 : loss : 0.019738, loss_ce: 0.009148
2022-01-09 02:49:30,129 iteration 4958 : loss : 0.035680, loss_ce: 0.022312
2022-01-09 02:49:31,600 iteration 4959 : loss : 0.025670, loss_ce: 0.009271
2022-01-09 02:49:32,967 iteration 4960 : loss : 0.019687, loss_ce: 0.007259
2022-01-09 02:49:34,441 iteration 4961 : loss : 0.020442, loss_ce: 0.007651
2022-01-09 02:49:35,828 iteration 4962 : loss : 0.024498, loss_ce: 0.005757
2022-01-09 02:49:37,255 iteration 4963 : loss : 0.027177, loss_ce: 0.011046
2022-01-09 02:49:38,786 iteration 4964 : loss : 0.024328, loss_ce: 0.008139
 73%|█████████████████████▏       | 292/400 [2:13:07<48:30, 26.95s/it]2022-01-09 02:49:40,418 iteration 4965 : loss : 0.027143, loss_ce: 0.011113
2022-01-09 02:49:41,961 iteration 4966 : loss : 0.029556, loss_ce: 0.010809
2022-01-09 02:49:43,442 iteration 4967 : loss : 0.024233, loss_ce: 0.010663
2022-01-09 02:49:44,955 iteration 4968 : loss : 0.018428, loss_ce: 0.008694
2022-01-09 02:49:46,565 iteration 4969 : loss : 0.050411, loss_ce: 0.017031
2022-01-09 02:49:47,978 iteration 4970 : loss : 0.021060, loss_ce: 0.010055
2022-01-09 02:49:49,413 iteration 4971 : loss : 0.024245, loss_ce: 0.016322
2022-01-09 02:49:50,939 iteration 4972 : loss : 0.026483, loss_ce: 0.010668
2022-01-09 02:49:52,417 iteration 4973 : loss : 0.039372, loss_ce: 0.017740
2022-01-09 02:49:54,057 iteration 4974 : loss : 0.052682, loss_ce: 0.027016
2022-01-09 02:49:55,515 iteration 4975 : loss : 0.018621, loss_ce: 0.005530
2022-01-09 02:49:56,901 iteration 4976 : loss : 0.072444, loss_ce: 0.047061
2022-01-09 02:49:58,412 iteration 4977 : loss : 0.033392, loss_ce: 0.009273
2022-01-09 02:49:59,862 iteration 4978 : loss : 0.023996, loss_ce: 0.006614
2022-01-09 02:50:01,331 iteration 4979 : loss : 0.034176, loss_ce: 0.014534
2022-01-09 02:50:02,813 iteration 4980 : loss : 0.077154, loss_ce: 0.031119
2022-01-09 02:50:04,270 iteration 4981 : loss : 0.034738, loss_ce: 0.013407
 73%|█████████████████████▏       | 293/400 [2:13:33<47:16, 26.51s/it]2022-01-09 02:50:05,777 iteration 4982 : loss : 0.026408, loss_ce: 0.013419
2022-01-09 02:50:07,215 iteration 4983 : loss : 0.023115, loss_ce: 0.010783
2022-01-09 02:50:08,809 iteration 4984 : loss : 0.036998, loss_ce: 0.017130
2022-01-09 02:50:10,183 iteration 4985 : loss : 0.021887, loss_ce: 0.006938
2022-01-09 02:50:11,759 iteration 4986 : loss : 0.029224, loss_ce: 0.012720
2022-01-09 02:50:13,157 iteration 4987 : loss : 0.024690, loss_ce: 0.009210
2022-01-09 02:50:14,649 iteration 4988 : loss : 0.030727, loss_ce: 0.011396
2022-01-09 02:50:16,192 iteration 4989 : loss : 0.053837, loss_ce: 0.017138
2022-01-09 02:50:17,763 iteration 4990 : loss : 0.026885, loss_ce: 0.011812
2022-01-09 02:50:19,255 iteration 4991 : loss : 0.026556, loss_ce: 0.008887
2022-01-09 02:50:20,610 iteration 4992 : loss : 0.018939, loss_ce: 0.009434
2022-01-09 02:50:22,044 iteration 4993 : loss : 0.022751, loss_ce: 0.008619
2022-01-09 02:50:23,440 iteration 4994 : loss : 0.023484, loss_ce: 0.009197
2022-01-09 02:50:24,852 iteration 4995 : loss : 0.024247, loss_ce: 0.010318
2022-01-09 02:50:26,394 iteration 4996 : loss : 0.040361, loss_ce: 0.012571
2022-01-09 02:50:27,764 iteration 4997 : loss : 0.019026, loss_ce: 0.005983
2022-01-09 02:50:29,153 iteration 4998 : loss : 0.020010, loss_ce: 0.008552
 74%|█████████████████████▎       | 294/400 [2:13:58<45:58, 26.02s/it]2022-01-09 02:50:30,639 iteration 4999 : loss : 0.016730, loss_ce: 0.006252
2022-01-09 02:50:32,075 iteration 5000 : loss : 0.020561, loss_ce: 0.008646
2022-01-09 02:50:33,515 iteration 5001 : loss : 0.020070, loss_ce: 0.010051
2022-01-09 02:50:34,950 iteration 5002 : loss : 0.015301, loss_ce: 0.005111
2022-01-09 02:50:36,453 iteration 5003 : loss : 0.040523, loss_ce: 0.007957
2022-01-09 02:50:37,911 iteration 5004 : loss : 0.026554, loss_ce: 0.011881
2022-01-09 02:50:39,364 iteration 5005 : loss : 0.024969, loss_ce: 0.011199
2022-01-09 02:50:40,955 iteration 5006 : loss : 0.025461, loss_ce: 0.010392
2022-01-09 02:50:42,465 iteration 5007 : loss : 0.025563, loss_ce: 0.011163
2022-01-09 02:50:43,996 iteration 5008 : loss : 0.047006, loss_ce: 0.021820
2022-01-09 02:50:45,523 iteration 5009 : loss : 0.024454, loss_ce: 0.006141
2022-01-09 02:50:47,030 iteration 5010 : loss : 0.029006, loss_ce: 0.010897
2022-01-09 02:50:48,478 iteration 5011 : loss : 0.019498, loss_ce: 0.008953
2022-01-09 02:50:49,980 iteration 5012 : loss : 0.022297, loss_ce: 0.009061
2022-01-09 02:50:51,364 iteration 5013 : loss : 0.018244, loss_ce: 0.007536
2022-01-09 02:50:52,813 iteration 5014 : loss : 0.020954, loss_ce: 0.007547
2022-01-09 02:50:52,813 Training Data Eval:
2022-01-09 02:51:00,243   Average segmentation loss on training set: 0.0133
2022-01-09 02:51:00,244 Validation Data Eval:
2022-01-09 02:51:02,930   Average segmentation loss on validation set: 0.0941
2022-01-09 02:51:04,450 iteration 5015 : loss : 0.031965, loss_ce: 0.012836
 74%|█████████████████████▍       | 295/400 [2:14:33<50:24, 28.80s/it]2022-01-09 02:51:06,005 iteration 5016 : loss : 0.019803, loss_ce: 0.009636
2022-01-09 02:51:07,454 iteration 5017 : loss : 0.018881, loss_ce: 0.007222
2022-01-09 02:51:08,922 iteration 5018 : loss : 0.020756, loss_ce: 0.009244
2022-01-09 02:51:10,432 iteration 5019 : loss : 0.023734, loss_ce: 0.009063
2022-01-09 02:51:11,962 iteration 5020 : loss : 0.023822, loss_ce: 0.007248
2022-01-09 02:51:13,511 iteration 5021 : loss : 0.019723, loss_ce: 0.010340
2022-01-09 02:51:14,931 iteration 5022 : loss : 0.023598, loss_ce: 0.009187
2022-01-09 02:51:16,420 iteration 5023 : loss : 0.028415, loss_ce: 0.008204
2022-01-09 02:51:18,025 iteration 5024 : loss : 0.027009, loss_ce: 0.005493
2022-01-09 02:51:19,596 iteration 5025 : loss : 0.026384, loss_ce: 0.005877
2022-01-09 02:51:21,082 iteration 5026 : loss : 0.025343, loss_ce: 0.009556
2022-01-09 02:51:22,503 iteration 5027 : loss : 0.027219, loss_ce: 0.010848
2022-01-09 02:51:23,942 iteration 5028 : loss : 0.026614, loss_ce: 0.010336
2022-01-09 02:51:25,531 iteration 5029 : loss : 0.028984, loss_ce: 0.007378
2022-01-09 02:51:27,006 iteration 5030 : loss : 0.019674, loss_ce: 0.009810
2022-01-09 02:51:28,566 iteration 5031 : loss : 0.025152, loss_ce: 0.011486
2022-01-09 02:51:30,060 iteration 5032 : loss : 0.017017, loss_ce: 0.008370
 74%|█████████████████████▍       | 296/400 [2:14:58<48:16, 27.85s/it]2022-01-09 02:51:31,762 iteration 5033 : loss : 0.027264, loss_ce: 0.009908
2022-01-09 02:51:33,231 iteration 5034 : loss : 0.022215, loss_ce: 0.006828
2022-01-09 02:51:34,694 iteration 5035 : loss : 0.020071, loss_ce: 0.009005
2022-01-09 02:51:36,155 iteration 5036 : loss : 0.021507, loss_ce: 0.008767
2022-01-09 02:51:37,660 iteration 5037 : loss : 0.024946, loss_ce: 0.009546
2022-01-09 02:51:39,119 iteration 5038 : loss : 0.061232, loss_ce: 0.013169
2022-01-09 02:51:40,712 iteration 5039 : loss : 0.027584, loss_ce: 0.013742
2022-01-09 02:51:42,206 iteration 5040 : loss : 0.018152, loss_ce: 0.005665
2022-01-09 02:51:43,784 iteration 5041 : loss : 0.027826, loss_ce: 0.009764
2022-01-09 02:51:45,227 iteration 5042 : loss : 0.021174, loss_ce: 0.009015
2022-01-09 02:51:46,712 iteration 5043 : loss : 0.017243, loss_ce: 0.008094
2022-01-09 02:51:48,197 iteration 5044 : loss : 0.024753, loss_ce: 0.011423
2022-01-09 02:51:49,734 iteration 5045 : loss : 0.024934, loss_ce: 0.010126
2022-01-09 02:51:51,226 iteration 5046 : loss : 0.023988, loss_ce: 0.006815
2022-01-09 02:51:52,674 iteration 5047 : loss : 0.024168, loss_ce: 0.007165
2022-01-09 02:51:54,111 iteration 5048 : loss : 0.023150, loss_ce: 0.009459
2022-01-09 02:51:55,545 iteration 5049 : loss : 0.015998, loss_ce: 0.006450
 74%|█████████████████████▌       | 297/400 [2:15:24<46:35, 27.14s/it]2022-01-09 02:51:57,092 iteration 5050 : loss : 0.028478, loss_ce: 0.009130
2022-01-09 02:51:58,542 iteration 5051 : loss : 0.021602, loss_ce: 0.007859
2022-01-09 02:52:00,002 iteration 5052 : loss : 0.020990, loss_ce: 0.009444
2022-01-09 02:52:01,446 iteration 5053 : loss : 0.016985, loss_ce: 0.006552
2022-01-09 02:52:02,914 iteration 5054 : loss : 0.017667, loss_ce: 0.005895
2022-01-09 02:52:04,352 iteration 5055 : loss : 0.021360, loss_ce: 0.007647
2022-01-09 02:52:05,785 iteration 5056 : loss : 0.024084, loss_ce: 0.005798
2022-01-09 02:52:07,334 iteration 5057 : loss : 0.031801, loss_ce: 0.018367
2022-01-09 02:52:08,904 iteration 5058 : loss : 0.036300, loss_ce: 0.016757
2022-01-09 02:52:10,254 iteration 5059 : loss : 0.017704, loss_ce: 0.004310
2022-01-09 02:52:11,763 iteration 5060 : loss : 0.026887, loss_ce: 0.007141
2022-01-09 02:52:13,295 iteration 5061 : loss : 0.019281, loss_ce: 0.008550
2022-01-09 02:52:14,806 iteration 5062 : loss : 0.041651, loss_ce: 0.015095
2022-01-09 02:52:16,359 iteration 5063 : loss : 0.024787, loss_ce: 0.010394
2022-01-09 02:52:17,787 iteration 5064 : loss : 0.019780, loss_ce: 0.008991
2022-01-09 02:52:19,218 iteration 5065 : loss : 0.026642, loss_ce: 0.009974
2022-01-09 02:52:20,696 iteration 5066 : loss : 0.017292, loss_ce: 0.008427
 74%|█████████████████████▌       | 298/400 [2:15:49<45:07, 26.54s/it]2022-01-09 02:52:22,357 iteration 5067 : loss : 0.026339, loss_ce: 0.009440
2022-01-09 02:52:23,861 iteration 5068 : loss : 0.023624, loss_ce: 0.009811
2022-01-09 02:52:25,362 iteration 5069 : loss : 0.025258, loss_ce: 0.009241
2022-01-09 02:52:26,815 iteration 5070 : loss : 0.020394, loss_ce: 0.006106
2022-01-09 02:52:28,374 iteration 5071 : loss : 0.024574, loss_ce: 0.009606
2022-01-09 02:52:29,691 iteration 5072 : loss : 0.017299, loss_ce: 0.006082
2022-01-09 02:52:31,124 iteration 5073 : loss : 0.018225, loss_ce: 0.008772
2022-01-09 02:52:32,601 iteration 5074 : loss : 0.018319, loss_ce: 0.005984
2022-01-09 02:52:34,076 iteration 5075 : loss : 0.016403, loss_ce: 0.006709
2022-01-09 02:52:35,614 iteration 5076 : loss : 0.025429, loss_ce: 0.011368
2022-01-09 02:52:37,048 iteration 5077 : loss : 0.019935, loss_ce: 0.007172
2022-01-09 02:52:38,535 iteration 5078 : loss : 0.015635, loss_ce: 0.004138
2022-01-09 02:52:39,969 iteration 5079 : loss : 0.017791, loss_ce: 0.007729
2022-01-09 02:52:41,445 iteration 5080 : loss : 0.025014, loss_ce: 0.008048
2022-01-09 02:52:42,957 iteration 5081 : loss : 0.024747, loss_ce: 0.010792
2022-01-09 02:52:44,503 iteration 5082 : loss : 0.035811, loss_ce: 0.018157
2022-01-09 02:52:46,002 iteration 5083 : loss : 0.021313, loss_ce: 0.006629
 75%|█████████████████████▋       | 299/400 [2:16:14<44:03, 26.17s/it]2022-01-09 02:52:47,503 iteration 5084 : loss : 0.019599, loss_ce: 0.006305
2022-01-09 02:52:48,970 iteration 5085 : loss : 0.028466, loss_ce: 0.009323
2022-01-09 02:52:50,394 iteration 5086 : loss : 0.016045, loss_ce: 0.006051
2022-01-09 02:52:51,935 iteration 5087 : loss : 0.024971, loss_ce: 0.010501
2022-01-09 02:52:53,435 iteration 5088 : loss : 0.017071, loss_ce: 0.006291
2022-01-09 02:52:54,846 iteration 5089 : loss : 0.019195, loss_ce: 0.006586
2022-01-09 02:52:56,281 iteration 5090 : loss : 0.017860, loss_ce: 0.005911
2022-01-09 02:52:57,889 iteration 5091 : loss : 0.034632, loss_ce: 0.013063
2022-01-09 02:52:59,360 iteration 5092 : loss : 0.019596, loss_ce: 0.010696
2022-01-09 02:53:00,781 iteration 5093 : loss : 0.020168, loss_ce: 0.009620
2022-01-09 02:53:02,165 iteration 5094 : loss : 0.015652, loss_ce: 0.005711
2022-01-09 02:53:03,683 iteration 5095 : loss : 0.033402, loss_ce: 0.012969
2022-01-09 02:53:05,076 iteration 5096 : loss : 0.013705, loss_ce: 0.005323
2022-01-09 02:53:06,490 iteration 5097 : loss : 0.014661, loss_ce: 0.006627
2022-01-09 02:53:07,978 iteration 5098 : loss : 0.027155, loss_ce: 0.009137
2022-01-09 02:53:09,427 iteration 5099 : loss : 0.021620, loss_ce: 0.007944
2022-01-09 02:53:09,427 Training Data Eval:
2022-01-09 02:53:16,704   Average segmentation loss on training set: 0.0121
2022-01-09 02:53:16,704 Validation Data Eval:
2022-01-09 02:53:19,194   Average segmentation loss on validation set: 0.0728
2022-01-09 02:53:20,662 iteration 5100 : loss : 0.013838, loss_ce: 0.006367
 75%|█████████████████████▊       | 300/400 [2:16:49<47:51, 28.72s/it]2022-01-09 02:53:22,225 iteration 5101 : loss : 0.018835, loss_ce: 0.007842
2022-01-09 02:53:23,650 iteration 5102 : loss : 0.017093, loss_ce: 0.007363
2022-01-09 02:53:25,103 iteration 5103 : loss : 0.021252, loss_ce: 0.008640
2022-01-09 02:53:26,665 iteration 5104 : loss : 0.018967, loss_ce: 0.008163
2022-01-09 02:53:28,063 iteration 5105 : loss : 0.018610, loss_ce: 0.006683
2022-01-09 02:53:29,627 iteration 5106 : loss : 0.027606, loss_ce: 0.010022
2022-01-09 02:53:31,024 iteration 5107 : loss : 0.018132, loss_ce: 0.006673
2022-01-09 02:53:32,526 iteration 5108 : loss : 0.024421, loss_ce: 0.008501
2022-01-09 02:53:34,010 iteration 5109 : loss : 0.028827, loss_ce: 0.008607
2022-01-09 02:53:35,467 iteration 5110 : loss : 0.042679, loss_ce: 0.030186
2022-01-09 02:53:36,978 iteration 5111 : loss : 0.017798, loss_ce: 0.005901
2022-01-09 02:53:38,378 iteration 5112 : loss : 0.026678, loss_ce: 0.008109
2022-01-09 02:53:39,928 iteration 5113 : loss : 0.043181, loss_ce: 0.007977
2022-01-09 02:53:41,371 iteration 5114 : loss : 0.021152, loss_ce: 0.010981
2022-01-09 02:53:42,787 iteration 5115 : loss : 0.019732, loss_ce: 0.005429
2022-01-09 02:53:44,208 iteration 5116 : loss : 0.017230, loss_ce: 0.006674
2022-01-09 02:53:45,780 iteration 5117 : loss : 0.030803, loss_ce: 0.015503
 75%|█████████████████████▊       | 301/400 [2:17:14<45:36, 27.64s/it]2022-01-09 02:53:47,323 iteration 5118 : loss : 0.016160, loss_ce: 0.006288
2022-01-09 02:53:48,835 iteration 5119 : loss : 0.021373, loss_ce: 0.007025
2022-01-09 02:53:50,256 iteration 5120 : loss : 0.022053, loss_ce: 0.005560
2022-01-09 02:53:51,719 iteration 5121 : loss : 0.019824, loss_ce: 0.009487
2022-01-09 02:53:53,131 iteration 5122 : loss : 0.021005, loss_ce: 0.008431
2022-01-09 02:53:54,584 iteration 5123 : loss : 0.018206, loss_ce: 0.005205
2022-01-09 02:53:56,076 iteration 5124 : loss : 0.015859, loss_ce: 0.006115
2022-01-09 02:53:57,428 iteration 5125 : loss : 0.019238, loss_ce: 0.009689
2022-01-09 02:53:58,816 iteration 5126 : loss : 0.021680, loss_ce: 0.006812
2022-01-09 02:54:00,332 iteration 5127 : loss : 0.024404, loss_ce: 0.007849
2022-01-09 02:54:01,762 iteration 5128 : loss : 0.013618, loss_ce: 0.005351
2022-01-09 02:54:03,250 iteration 5129 : loss : 0.019744, loss_ce: 0.006051
2022-01-09 02:54:04,722 iteration 5130 : loss : 0.020598, loss_ce: 0.006590
2022-01-09 02:54:06,081 iteration 5131 : loss : 0.012479, loss_ce: 0.004717
2022-01-09 02:54:07,531 iteration 5132 : loss : 0.018250, loss_ce: 0.008165
2022-01-09 02:54:08,937 iteration 5133 : loss : 0.025707, loss_ce: 0.007073
2022-01-09 02:54:10,323 iteration 5134 : loss : 0.019304, loss_ce: 0.007926
 76%|█████████████████████▉       | 302/400 [2:17:39<43:37, 26.71s/it]2022-01-09 02:54:12,058 iteration 5135 : loss : 0.026766, loss_ce: 0.010452
2022-01-09 02:54:13,514 iteration 5136 : loss : 0.027871, loss_ce: 0.010289
2022-01-09 02:54:15,049 iteration 5137 : loss : 0.019937, loss_ce: 0.007226
2022-01-09 02:54:16,462 iteration 5138 : loss : 0.019711, loss_ce: 0.008065
2022-01-09 02:54:17,930 iteration 5139 : loss : 0.015973, loss_ce: 0.005640
2022-01-09 02:54:19,511 iteration 5140 : loss : 0.015345, loss_ce: 0.006121
2022-01-09 02:54:20,890 iteration 5141 : loss : 0.018257, loss_ce: 0.006369
2022-01-09 02:54:22,369 iteration 5142 : loss : 0.024699, loss_ce: 0.008557
2022-01-09 02:54:23,725 iteration 5143 : loss : 0.017972, loss_ce: 0.007107
2022-01-09 02:54:25,228 iteration 5144 : loss : 0.025128, loss_ce: 0.013087
2022-01-09 02:54:26,580 iteration 5145 : loss : 0.013497, loss_ce: 0.004384
2022-01-09 02:54:28,132 iteration 5146 : loss : 0.023263, loss_ce: 0.010351
2022-01-09 02:54:29,686 iteration 5147 : loss : 0.026940, loss_ce: 0.010936
2022-01-09 02:54:31,115 iteration 5148 : loss : 0.020209, loss_ce: 0.008663
2022-01-09 02:54:32,551 iteration 5149 : loss : 0.037095, loss_ce: 0.010406
2022-01-09 02:54:33,991 iteration 5150 : loss : 0.014473, loss_ce: 0.005865
2022-01-09 02:54:35,461 iteration 5151 : loss : 0.035001, loss_ce: 0.006738
 76%|█████████████████████▉       | 303/400 [2:18:04<42:25, 26.24s/it]2022-01-09 02:54:36,924 iteration 5152 : loss : 0.025232, loss_ce: 0.004697
2022-01-09 02:54:38,261 iteration 5153 : loss : 0.013902, loss_ce: 0.005523
2022-01-09 02:54:39,808 iteration 5154 : loss : 0.026007, loss_ce: 0.009084
2022-01-09 02:54:41,190 iteration 5155 : loss : 0.015488, loss_ce: 0.005288
2022-01-09 02:54:42,626 iteration 5156 : loss : 0.019378, loss_ce: 0.007856
2022-01-09 02:54:43,990 iteration 5157 : loss : 0.017881, loss_ce: 0.006019
2022-01-09 02:54:45,420 iteration 5158 : loss : 0.018139, loss_ce: 0.006004
2022-01-09 02:54:46,951 iteration 5159 : loss : 0.023296, loss_ce: 0.008016
2022-01-09 02:54:48,333 iteration 5160 : loss : 0.015892, loss_ce: 0.005628
2022-01-09 02:54:49,770 iteration 5161 : loss : 0.020425, loss_ce: 0.008045
2022-01-09 02:54:51,150 iteration 5162 : loss : 0.022154, loss_ce: 0.009658
2022-01-09 02:54:52,623 iteration 5163 : loss : 0.022092, loss_ce: 0.008562
2022-01-09 02:54:54,103 iteration 5164 : loss : 0.019097, loss_ce: 0.009975
2022-01-09 02:54:55,472 iteration 5165 : loss : 0.018355, loss_ce: 0.005772
2022-01-09 02:54:56,922 iteration 5166 : loss : 0.019784, loss_ce: 0.008571
2022-01-09 02:54:58,380 iteration 5167 : loss : 0.025091, loss_ce: 0.008185
2022-01-09 02:54:59,916 iteration 5168 : loss : 0.027634, loss_ce: 0.010786
 76%|██████████████████████       | 304/400 [2:18:28<41:07, 25.70s/it]2022-01-09 02:55:01,592 iteration 5169 : loss : 0.029430, loss_ce: 0.013988
2022-01-09 02:55:02,984 iteration 5170 : loss : 0.016219, loss_ce: 0.005147
2022-01-09 02:55:04,527 iteration 5171 : loss : 0.016520, loss_ce: 0.006679
2022-01-09 02:55:05,886 iteration 5172 : loss : 0.016431, loss_ce: 0.005236
2022-01-09 02:55:07,177 iteration 5173 : loss : 0.013468, loss_ce: 0.006031
2022-01-09 02:55:08,729 iteration 5174 : loss : 0.021898, loss_ce: 0.007363
2022-01-09 02:55:10,146 iteration 5175 : loss : 0.034515, loss_ce: 0.020837
2022-01-09 02:55:11,766 iteration 5176 : loss : 0.023076, loss_ce: 0.008122
2022-01-09 02:55:13,170 iteration 5177 : loss : 0.017980, loss_ce: 0.007914
2022-01-09 02:55:14,606 iteration 5178 : loss : 0.016333, loss_ce: 0.005621
2022-01-09 02:55:16,138 iteration 5179 : loss : 0.019418, loss_ce: 0.006278
2022-01-09 02:55:17,592 iteration 5180 : loss : 0.022516, loss_ce: 0.008807
2022-01-09 02:55:18,990 iteration 5181 : loss : 0.014758, loss_ce: 0.003709
2022-01-09 02:55:20,447 iteration 5182 : loss : 0.021224, loss_ce: 0.008014
2022-01-09 02:55:21,872 iteration 5183 : loss : 0.021034, loss_ce: 0.008888
2022-01-09 02:55:23,269 iteration 5184 : loss : 0.024833, loss_ce: 0.008954
2022-01-09 02:55:23,270 Training Data Eval:
2022-01-09 02:55:30,556   Average segmentation loss on training set: 0.0120
2022-01-09 02:55:30,556 Validation Data Eval:
2022-01-09 02:55:33,110   Average segmentation loss on validation set: 0.0638
2022-01-09 02:55:34,579 iteration 5185 : loss : 0.024055, loss_ce: 0.009339
 76%|██████████████████████       | 305/400 [2:19:03<44:57, 28.39s/it]2022-01-09 02:55:36,144 iteration 5186 : loss : 0.020362, loss_ce: 0.010585
2022-01-09 02:55:37,554 iteration 5187 : loss : 0.018935, loss_ce: 0.008094
2022-01-09 02:55:38,989 iteration 5188 : loss : 0.015677, loss_ce: 0.006703
2022-01-09 02:55:40,436 iteration 5189 : loss : 0.015511, loss_ce: 0.007426
2022-01-09 02:55:41,866 iteration 5190 : loss : 0.020274, loss_ce: 0.008801
2022-01-09 02:55:43,415 iteration 5191 : loss : 0.025220, loss_ce: 0.007332
2022-01-09 02:55:44,927 iteration 5192 : loss : 0.018865, loss_ce: 0.005436
2022-01-09 02:55:46,317 iteration 5193 : loss : 0.012181, loss_ce: 0.004380
2022-01-09 02:55:47,820 iteration 5194 : loss : 0.022508, loss_ce: 0.009055
2022-01-09 02:55:49,236 iteration 5195 : loss : 0.017350, loss_ce: 0.005928
2022-01-09 02:55:50,562 iteration 5196 : loss : 0.015374, loss_ce: 0.003864
2022-01-09 02:55:51,937 iteration 5197 : loss : 0.015826, loss_ce: 0.005445
2022-01-09 02:55:53,339 iteration 5198 : loss : 0.018010, loss_ce: 0.009638
2022-01-09 02:55:54,822 iteration 5199 : loss : 0.015669, loss_ce: 0.005726
2022-01-09 02:55:56,263 iteration 5200 : loss : 0.033831, loss_ce: 0.010461
2022-01-09 02:55:57,735 iteration 5201 : loss : 0.025757, loss_ce: 0.008037
2022-01-09 02:55:59,297 iteration 5202 : loss : 0.025630, loss_ce: 0.011982
 76%|██████████████████████▏      | 306/400 [2:19:28<42:45, 27.29s/it]2022-01-09 02:56:00,778 iteration 5203 : loss : 0.015741, loss_ce: 0.005987
2022-01-09 02:56:02,194 iteration 5204 : loss : 0.026130, loss_ce: 0.010253
2022-01-09 02:56:03,831 iteration 5205 : loss : 0.022832, loss_ce: 0.009771
2022-01-09 02:56:05,284 iteration 5206 : loss : 0.020858, loss_ce: 0.007377
2022-01-09 02:56:06,751 iteration 5207 : loss : 0.023833, loss_ce: 0.009468
2022-01-09 02:56:08,311 iteration 5208 : loss : 0.019834, loss_ce: 0.008160
2022-01-09 02:56:09,705 iteration 5209 : loss : 0.015280, loss_ce: 0.006416
2022-01-09 02:56:11,228 iteration 5210 : loss : 0.019526, loss_ce: 0.007319
2022-01-09 02:56:12,711 iteration 5211 : loss : 0.018842, loss_ce: 0.007819
2022-01-09 02:56:14,172 iteration 5212 : loss : 0.020098, loss_ce: 0.007555
2022-01-09 02:56:15,487 iteration 5213 : loss : 0.014516, loss_ce: 0.005463
2022-01-09 02:56:17,068 iteration 5214 : loss : 0.023922, loss_ce: 0.011959
2022-01-09 02:56:18,564 iteration 5215 : loss : 0.024203, loss_ce: 0.007787
2022-01-09 02:56:20,098 iteration 5216 : loss : 0.022936, loss_ce: 0.007206
2022-01-09 02:56:21,666 iteration 5217 : loss : 0.023715, loss_ce: 0.007944
2022-01-09 02:56:23,073 iteration 5218 : loss : 0.030746, loss_ce: 0.005624
2022-01-09 02:56:24,657 iteration 5219 : loss : 0.030563, loss_ce: 0.013741
 77%|██████████████████████▎      | 307/400 [2:19:53<41:24, 26.71s/it]2022-01-09 02:56:26,167 iteration 5220 : loss : 0.021264, loss_ce: 0.006488
2022-01-09 02:56:27,617 iteration 5221 : loss : 0.029934, loss_ce: 0.011022
2022-01-09 02:56:29,001 iteration 5222 : loss : 0.017318, loss_ce: 0.006384
2022-01-09 02:56:30,516 iteration 5223 : loss : 0.022789, loss_ce: 0.008920
2022-01-09 02:56:31,966 iteration 5224 : loss : 0.019411, loss_ce: 0.006467
2022-01-09 02:56:33,498 iteration 5225 : loss : 0.019321, loss_ce: 0.007041
2022-01-09 02:56:34,937 iteration 5226 : loss : 0.022293, loss_ce: 0.008529
2022-01-09 02:56:36,504 iteration 5227 : loss : 0.021840, loss_ce: 0.009266
2022-01-09 02:56:37,922 iteration 5228 : loss : 0.018006, loss_ce: 0.006896
2022-01-09 02:56:39,548 iteration 5229 : loss : 0.021001, loss_ce: 0.008643
2022-01-09 02:56:40,865 iteration 5230 : loss : 0.014571, loss_ce: 0.006298
2022-01-09 02:56:42,346 iteration 5231 : loss : 0.019661, loss_ce: 0.008221
2022-01-09 02:56:43,810 iteration 5232 : loss : 0.019936, loss_ce: 0.005929
2022-01-09 02:56:45,311 iteration 5233 : loss : 0.019518, loss_ce: 0.007508
2022-01-09 02:56:46,730 iteration 5234 : loss : 0.015286, loss_ce: 0.006780
2022-01-09 02:56:48,207 iteration 5235 : loss : 0.021408, loss_ce: 0.007433
2022-01-09 02:56:49,729 iteration 5236 : loss : 0.027279, loss_ce: 0.009504
 77%|██████████████████████▎      | 308/400 [2:20:18<40:12, 26.22s/it]2022-01-09 02:56:51,097 iteration 5237 : loss : 0.014047, loss_ce: 0.005859
2022-01-09 02:56:52,605 iteration 5238 : loss : 0.016821, loss_ce: 0.007104
2022-01-09 02:56:54,100 iteration 5239 : loss : 0.049083, loss_ce: 0.015264
2022-01-09 02:56:55,622 iteration 5240 : loss : 0.176750, loss_ce: 0.007134
2022-01-09 02:56:57,040 iteration 5241 : loss : 0.016045, loss_ce: 0.006286
2022-01-09 02:56:58,505 iteration 5242 : loss : 0.023866, loss_ce: 0.009946
2022-01-09 02:57:00,076 iteration 5243 : loss : 0.033334, loss_ce: 0.011412
2022-01-09 02:57:01,588 iteration 5244 : loss : 0.016414, loss_ce: 0.007184
2022-01-09 02:57:03,065 iteration 5245 : loss : 0.019024, loss_ce: 0.006758
2022-01-09 02:57:04,566 iteration 5246 : loss : 0.021320, loss_ce: 0.007150
2022-01-09 02:57:05,923 iteration 5247 : loss : 0.016450, loss_ce: 0.007808
2022-01-09 02:57:07,442 iteration 5248 : loss : 0.015638, loss_ce: 0.005095
2022-01-09 02:57:09,024 iteration 5249 : loss : 0.022526, loss_ce: 0.008323
2022-01-09 02:57:10,526 iteration 5250 : loss : 0.020762, loss_ce: 0.009528
2022-01-09 02:57:11,888 iteration 5251 : loss : 0.014476, loss_ce: 0.005067
2022-01-09 02:57:13,358 iteration 5252 : loss : 0.014589, loss_ce: 0.005901
2022-01-09 02:57:14,860 iteration 5253 : loss : 0.032809, loss_ce: 0.008164
 77%|██████████████████████▍      | 309/400 [2:20:43<39:16, 25.89s/it]2022-01-09 02:57:16,381 iteration 5254 : loss : 0.013657, loss_ce: 0.005019
2022-01-09 02:57:17,842 iteration 5255 : loss : 0.022822, loss_ce: 0.013108
2022-01-09 02:57:19,225 iteration 5256 : loss : 0.016075, loss_ce: 0.006678
2022-01-09 02:57:20,744 iteration 5257 : loss : 0.019824, loss_ce: 0.008536
2022-01-09 02:57:22,272 iteration 5258 : loss : 0.018668, loss_ce: 0.004917
2022-01-09 02:57:23,725 iteration 5259 : loss : 0.046263, loss_ce: 0.014776
2022-01-09 02:57:25,269 iteration 5260 : loss : 0.015019, loss_ce: 0.005400
2022-01-09 02:57:26,818 iteration 5261 : loss : 0.024906, loss_ce: 0.012424
2022-01-09 02:57:28,321 iteration 5262 : loss : 0.014131, loss_ce: 0.006225
2022-01-09 02:57:29,844 iteration 5263 : loss : 0.018855, loss_ce: 0.007994
2022-01-09 02:57:31,272 iteration 5264 : loss : 0.017216, loss_ce: 0.007545
2022-01-09 02:57:32,833 iteration 5265 : loss : 0.021242, loss_ce: 0.007866
2022-01-09 02:57:34,440 iteration 5266 : loss : 0.019227, loss_ce: 0.006313
2022-01-09 02:57:35,925 iteration 5267 : loss : 0.028569, loss_ce: 0.008480
2022-01-09 02:57:37,430 iteration 5268 : loss : 0.031469, loss_ce: 0.011776
2022-01-09 02:57:39,057 iteration 5269 : loss : 0.040831, loss_ce: 0.010439
2022-01-09 02:57:39,057 Training Data Eval:
2022-01-09 02:57:46,567   Average segmentation loss on training set: 0.0117
2022-01-09 02:57:46,567 Validation Data Eval:
2022-01-09 02:57:49,169   Average segmentation loss on validation set: 0.0599
2022-01-09 02:57:55,176 Found new lowest validation loss at iteration 5269! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 02:57:56,675 iteration 5270 : loss : 0.019168, loss_ce: 0.009311
 78%|██████████████████████▍      | 310/400 [2:21:25<46:00, 30.67s/it]2022-01-09 02:57:58,161 iteration 5271 : loss : 0.019776, loss_ce: 0.007359
2022-01-09 02:57:59,651 iteration 5272 : loss : 0.024661, loss_ce: 0.011157
2022-01-09 02:58:01,064 iteration 5273 : loss : 0.017684, loss_ce: 0.005070
2022-01-09 02:58:02,495 iteration 5274 : loss : 0.018897, loss_ce: 0.007112
2022-01-09 02:58:03,848 iteration 5275 : loss : 0.017461, loss_ce: 0.007675
2022-01-09 02:58:05,334 iteration 5276 : loss : 0.021298, loss_ce: 0.008911
2022-01-09 02:58:06,793 iteration 5277 : loss : 0.017810, loss_ce: 0.006484
2022-01-09 02:58:08,137 iteration 5278 : loss : 0.018409, loss_ce: 0.007578
2022-01-09 02:58:09,591 iteration 5279 : loss : 0.016670, loss_ce: 0.007500
2022-01-09 02:58:11,235 iteration 5280 : loss : 0.030203, loss_ce: 0.011684
2022-01-09 02:58:12,672 iteration 5281 : loss : 0.025847, loss_ce: 0.008506
2022-01-09 02:58:14,043 iteration 5282 : loss : 0.016905, loss_ce: 0.004849
2022-01-09 02:58:15,514 iteration 5283 : loss : 0.024267, loss_ce: 0.008240
2022-01-09 02:58:16,998 iteration 5284 : loss : 0.031035, loss_ce: 0.007436
2022-01-09 02:58:18,450 iteration 5285 : loss : 0.015744, loss_ce: 0.006847
2022-01-09 02:58:19,794 iteration 5286 : loss : 0.015265, loss_ce: 0.006853
2022-01-09 02:58:21,230 iteration 5287 : loss : 0.017253, loss_ce: 0.003456
 78%|██████████████████████▌      | 311/400 [2:21:50<42:46, 28.83s/it]2022-01-09 02:58:22,799 iteration 5288 : loss : 0.016565, loss_ce: 0.006315
2022-01-09 02:58:24,307 iteration 5289 : loss : 0.020109, loss_ce: 0.005776
2022-01-09 02:58:25,749 iteration 5290 : loss : 0.033092, loss_ce: 0.006580
2022-01-09 02:58:27,177 iteration 5291 : loss : 0.023253, loss_ce: 0.008264
2022-01-09 02:58:28,684 iteration 5292 : loss : 0.018013, loss_ce: 0.006203
2022-01-09 02:58:30,161 iteration 5293 : loss : 0.021753, loss_ce: 0.008795
2022-01-09 02:58:31,679 iteration 5294 : loss : 0.027205, loss_ce: 0.010673
2022-01-09 02:58:33,268 iteration 5295 : loss : 0.025025, loss_ce: 0.009264
2022-01-09 02:58:34,792 iteration 5296 : loss : 0.021464, loss_ce: 0.006028
2022-01-09 02:58:36,319 iteration 5297 : loss : 0.015509, loss_ce: 0.004582
2022-01-09 02:58:37,851 iteration 5298 : loss : 0.026777, loss_ce: 0.009217
2022-01-09 02:58:39,372 iteration 5299 : loss : 0.018256, loss_ce: 0.006436
2022-01-09 02:58:40,924 iteration 5300 : loss : 0.022641, loss_ce: 0.007969
2022-01-09 02:58:42,357 iteration 5301 : loss : 0.016109, loss_ce: 0.006098
2022-01-09 02:58:43,813 iteration 5302 : loss : 0.015083, loss_ce: 0.007368
2022-01-09 02:58:45,264 iteration 5303 : loss : 0.016599, loss_ce: 0.006332
2022-01-09 02:58:46,671 iteration 5304 : loss : 0.017444, loss_ce: 0.006270
 78%|██████████████████████▌      | 312/400 [2:22:15<40:48, 27.82s/it]2022-01-09 02:58:48,250 iteration 5305 : loss : 0.022359, loss_ce: 0.007555
2022-01-09 02:58:49,867 iteration 5306 : loss : 0.027524, loss_ce: 0.010853
2022-01-09 02:58:51,502 iteration 5307 : loss : 0.022618, loss_ce: 0.007909
2022-01-09 02:58:53,083 iteration 5308 : loss : 0.021422, loss_ce: 0.006615
2022-01-09 02:58:54,644 iteration 5309 : loss : 0.029898, loss_ce: 0.008375
2022-01-09 02:58:56,082 iteration 5310 : loss : 0.017164, loss_ce: 0.007993
2022-01-09 02:58:57,502 iteration 5311 : loss : 0.016984, loss_ce: 0.005001
2022-01-09 02:58:59,056 iteration 5312 : loss : 0.019258, loss_ce: 0.008864
2022-01-09 02:59:00,666 iteration 5313 : loss : 0.022879, loss_ce: 0.006414
2022-01-09 02:59:02,156 iteration 5314 : loss : 0.019311, loss_ce: 0.006897
2022-01-09 02:59:03,715 iteration 5315 : loss : 0.029325, loss_ce: 0.007204
2022-01-09 02:59:05,255 iteration 5316 : loss : 0.018790, loss_ce: 0.006929
2022-01-09 02:59:06,723 iteration 5317 : loss : 0.024435, loss_ce: 0.009818
2022-01-09 02:59:08,152 iteration 5318 : loss : 0.021551, loss_ce: 0.010027
2022-01-09 02:59:09,697 iteration 5319 : loss : 0.019284, loss_ce: 0.007209
2022-01-09 02:59:11,232 iteration 5320 : loss : 0.035420, loss_ce: 0.014164
2022-01-09 02:59:12,795 iteration 5321 : loss : 0.028134, loss_ce: 0.012059
 78%|██████████████████████▋      | 313/400 [2:22:41<39:35, 27.31s/it]2022-01-09 02:59:14,232 iteration 5322 : loss : 0.015843, loss_ce: 0.006717
2022-01-09 02:59:15,723 iteration 5323 : loss : 0.022862, loss_ce: 0.008753
2022-01-09 02:59:17,104 iteration 5324 : loss : 0.016945, loss_ce: 0.005866
2022-01-09 02:59:18,521 iteration 5325 : loss : 0.022553, loss_ce: 0.005238
2022-01-09 02:59:20,042 iteration 5326 : loss : 0.019802, loss_ce: 0.007102
2022-01-09 02:59:21,460 iteration 5327 : loss : 0.016551, loss_ce: 0.005730
2022-01-09 02:59:22,895 iteration 5328 : loss : 0.018687, loss_ce: 0.009525
2022-01-09 02:59:24,424 iteration 5329 : loss : 0.027492, loss_ce: 0.011206
2022-01-09 02:59:25,929 iteration 5330 : loss : 0.021766, loss_ce: 0.007993
2022-01-09 02:59:27,392 iteration 5331 : loss : 0.017163, loss_ce: 0.005555
2022-01-09 02:59:28,876 iteration 5332 : loss : 0.019220, loss_ce: 0.006121
2022-01-09 02:59:30,395 iteration 5333 : loss : 0.017813, loss_ce: 0.007804
2022-01-09 02:59:31,888 iteration 5334 : loss : 0.051160, loss_ce: 0.021256
2022-01-09 02:59:33,414 iteration 5335 : loss : 0.025046, loss_ce: 0.008435
2022-01-09 02:59:34,912 iteration 5336 : loss : 0.017356, loss_ce: 0.006361
2022-01-09 02:59:36,333 iteration 5337 : loss : 0.015639, loss_ce: 0.006173
2022-01-09 02:59:37,751 iteration 5338 : loss : 0.016304, loss_ce: 0.007104
 78%|██████████████████████▊      | 314/400 [2:23:06<38:07, 26.60s/it]2022-01-09 02:59:39,496 iteration 5339 : loss : 0.028032, loss_ce: 0.011598
2022-01-09 02:59:41,038 iteration 5340 : loss : 0.022855, loss_ce: 0.012755
2022-01-09 02:59:42,542 iteration 5341 : loss : 0.025092, loss_ce: 0.005493
2022-01-09 02:59:44,028 iteration 5342 : loss : 0.017258, loss_ce: 0.009425
2022-01-09 02:59:45,457 iteration 5343 : loss : 0.013858, loss_ce: 0.004415
2022-01-09 02:59:46,960 iteration 5344 : loss : 0.021855, loss_ce: 0.009958
2022-01-09 02:59:48,422 iteration 5345 : loss : 0.021549, loss_ce: 0.007340
2022-01-09 02:59:49,951 iteration 5346 : loss : 0.021781, loss_ce: 0.007568
2022-01-09 02:59:51,479 iteration 5347 : loss : 0.018436, loss_ce: 0.005321
2022-01-09 02:59:53,031 iteration 5348 : loss : 0.023826, loss_ce: 0.006801
2022-01-09 02:59:54,524 iteration 5349 : loss : 0.019123, loss_ce: 0.006711
2022-01-09 02:59:55,913 iteration 5350 : loss : 0.013410, loss_ce: 0.005049
2022-01-09 02:59:57,310 iteration 5351 : loss : 0.025004, loss_ce: 0.009013
2022-01-09 02:59:58,838 iteration 5352 : loss : 0.028388, loss_ce: 0.011251
2022-01-09 03:00:00,371 iteration 5353 : loss : 0.014558, loss_ce: 0.005009
2022-01-09 03:00:01,807 iteration 5354 : loss : 0.016291, loss_ce: 0.008052
2022-01-09 03:00:01,807 Training Data Eval:
2022-01-09 03:00:09,180   Average segmentation loss on training set: 0.0123
2022-01-09 03:00:09,180 Validation Data Eval:
2022-01-09 03:00:11,760   Average segmentation loss on validation set: 0.0666
2022-01-09 03:00:13,188 iteration 5355 : loss : 0.020891, loss_ce: 0.007768
 79%|██████████████████████▊      | 315/400 [2:23:42<41:26, 29.26s/it]2022-01-09 03:00:14,777 iteration 5356 : loss : 0.019505, loss_ce: 0.008882
2022-01-09 03:00:16,198 iteration 5357 : loss : 0.025044, loss_ce: 0.005457
2022-01-09 03:00:17,887 iteration 5358 : loss : 0.031270, loss_ce: 0.012421
2022-01-09 03:00:19,314 iteration 5359 : loss : 0.015272, loss_ce: 0.005561
2022-01-09 03:00:20,842 iteration 5360 : loss : 0.018976, loss_ce: 0.007184
2022-01-09 03:00:22,380 iteration 5361 : loss : 0.028256, loss_ce: 0.008819
2022-01-09 03:00:23,937 iteration 5362 : loss : 0.018286, loss_ce: 0.005491
2022-01-09 03:00:25,458 iteration 5363 : loss : 0.020556, loss_ce: 0.008426
2022-01-09 03:00:26,975 iteration 5364 : loss : 0.022911, loss_ce: 0.004622
2022-01-09 03:00:28,457 iteration 5365 : loss : 0.018934, loss_ce: 0.007473
2022-01-09 03:00:29,992 iteration 5366 : loss : 0.017917, loss_ce: 0.006723
2022-01-09 03:00:31,568 iteration 5367 : loss : 0.016108, loss_ce: 0.006879
2022-01-09 03:00:33,061 iteration 5368 : loss : 0.019395, loss_ce: 0.010814
2022-01-09 03:00:34,599 iteration 5369 : loss : 0.016285, loss_ce: 0.006188
2022-01-09 03:00:36,103 iteration 5370 : loss : 0.013875, loss_ce: 0.005565
2022-01-09 03:00:37,625 iteration 5371 : loss : 0.016140, loss_ce: 0.007665
2022-01-09 03:00:39,088 iteration 5372 : loss : 0.035872, loss_ce: 0.010461
 79%|██████████████████████▉      | 316/400 [2:24:07<39:32, 28.25s/it]2022-01-09 03:00:40,653 iteration 5373 : loss : 0.019151, loss_ce: 0.010374
2022-01-09 03:00:42,099 iteration 5374 : loss : 0.020595, loss_ce: 0.010066
2022-01-09 03:00:43,614 iteration 5375 : loss : 0.012292, loss_ce: 0.004003
2022-01-09 03:00:45,061 iteration 5376 : loss : 0.017215, loss_ce: 0.006813
2022-01-09 03:00:46,495 iteration 5377 : loss : 0.014472, loss_ce: 0.004786
2022-01-09 03:00:47,979 iteration 5378 : loss : 0.018996, loss_ce: 0.006624
2022-01-09 03:00:49,364 iteration 5379 : loss : 0.013543, loss_ce: 0.005061
2022-01-09 03:00:50,848 iteration 5380 : loss : 0.015123, loss_ce: 0.006181
2022-01-09 03:00:52,298 iteration 5381 : loss : 0.020831, loss_ce: 0.008470
2022-01-09 03:00:53,811 iteration 5382 : loss : 0.015381, loss_ce: 0.004501
2022-01-09 03:00:55,371 iteration 5383 : loss : 0.020576, loss_ce: 0.009804
2022-01-09 03:00:57,069 iteration 5384 : loss : 0.031033, loss_ce: 0.013279
2022-01-09 03:00:58,537 iteration 5385 : loss : 0.015702, loss_ce: 0.006124
2022-01-09 03:00:59,943 iteration 5386 : loss : 0.015894, loss_ce: 0.005732
2022-01-09 03:01:01,379 iteration 5387 : loss : 0.020303, loss_ce: 0.009007
2022-01-09 03:01:02,941 iteration 5388 : loss : 0.016668, loss_ce: 0.005842
2022-01-09 03:01:04,583 iteration 5389 : loss : 0.032994, loss_ce: 0.009403
 79%|██████████████████████▉      | 317/400 [2:24:33<37:56, 27.42s/it]2022-01-09 03:01:06,095 iteration 5390 : loss : 0.021460, loss_ce: 0.006641
2022-01-09 03:01:07,624 iteration 5391 : loss : 0.023007, loss_ce: 0.007661
2022-01-09 03:01:09,171 iteration 5392 : loss : 0.019230, loss_ce: 0.010301
2022-01-09 03:01:10,648 iteration 5393 : loss : 0.023536, loss_ce: 0.011016
2022-01-09 03:01:12,091 iteration 5394 : loss : 0.018532, loss_ce: 0.008858
2022-01-09 03:01:13,609 iteration 5395 : loss : 0.033294, loss_ce: 0.015174
2022-01-09 03:01:15,027 iteration 5396 : loss : 0.018511, loss_ce: 0.009810
2022-01-09 03:01:16,582 iteration 5397 : loss : 0.022702, loss_ce: 0.006762
2022-01-09 03:01:18,068 iteration 5398 : loss : 0.016314, loss_ce: 0.005870
2022-01-09 03:01:19,448 iteration 5399 : loss : 0.016144, loss_ce: 0.005219
2022-01-09 03:01:21,037 iteration 5400 : loss : 0.041990, loss_ce: 0.011382
2022-01-09 03:01:22,554 iteration 5401 : loss : 0.023657, loss_ce: 0.010765
2022-01-09 03:01:24,036 iteration 5402 : loss : 0.027713, loss_ce: 0.009078
2022-01-09 03:01:25,467 iteration 5403 : loss : 0.020913, loss_ce: 0.008463
2022-01-09 03:01:26,875 iteration 5404 : loss : 0.017134, loss_ce: 0.004732
2022-01-09 03:01:28,306 iteration 5405 : loss : 0.019099, loss_ce: 0.008625
2022-01-09 03:01:29,841 iteration 5406 : loss : 0.017191, loss_ce: 0.004183
 80%|███████████████████████      | 318/400 [2:24:58<36:35, 26.77s/it]2022-01-09 03:01:31,427 iteration 5407 : loss : 0.022149, loss_ce: 0.008799
2022-01-09 03:01:32,895 iteration 5408 : loss : 0.023484, loss_ce: 0.008096
2022-01-09 03:01:34,468 iteration 5409 : loss : 0.018772, loss_ce: 0.007823
2022-01-09 03:01:35,860 iteration 5410 : loss : 0.020131, loss_ce: 0.010732
2022-01-09 03:01:37,326 iteration 5411 : loss : 0.021253, loss_ce: 0.009159
2022-01-09 03:01:38,818 iteration 5412 : loss : 0.020259, loss_ce: 0.008380
2022-01-09 03:01:40,256 iteration 5413 : loss : 0.021747, loss_ce: 0.011106
2022-01-09 03:01:41,743 iteration 5414 : loss : 0.018150, loss_ce: 0.007635
2022-01-09 03:01:43,325 iteration 5415 : loss : 0.023564, loss_ce: 0.008692
2022-01-09 03:01:44,720 iteration 5416 : loss : 0.013267, loss_ce: 0.004512
2022-01-09 03:01:46,238 iteration 5417 : loss : 0.015755, loss_ce: 0.004482
2022-01-09 03:01:47,694 iteration 5418 : loss : 0.017335, loss_ce: 0.007030
2022-01-09 03:01:49,228 iteration 5419 : loss : 0.021484, loss_ce: 0.005369
2022-01-09 03:01:50,804 iteration 5420 : loss : 0.021106, loss_ce: 0.007806
2022-01-09 03:01:52,315 iteration 5421 : loss : 0.014728, loss_ce: 0.006596
2022-01-09 03:01:53,645 iteration 5422 : loss : 0.011677, loss_ce: 0.005048
2022-01-09 03:01:55,174 iteration 5423 : loss : 0.032442, loss_ce: 0.006079
 80%|███████████████████████▏     | 319/400 [2:25:24<35:33, 26.34s/it]2022-01-09 03:01:56,806 iteration 5424 : loss : 0.020071, loss_ce: 0.006504
2022-01-09 03:01:58,230 iteration 5425 : loss : 0.015278, loss_ce: 0.004413
2022-01-09 03:01:59,689 iteration 5426 : loss : 0.018130, loss_ce: 0.005156
2022-01-09 03:02:01,043 iteration 5427 : loss : 0.021235, loss_ce: 0.008839
2022-01-09 03:02:02,468 iteration 5428 : loss : 0.019961, loss_ce: 0.007988
2022-01-09 03:02:03,951 iteration 5429 : loss : 0.016259, loss_ce: 0.006151
2022-01-09 03:02:05,423 iteration 5430 : loss : 0.020944, loss_ce: 0.009811
2022-01-09 03:02:06,805 iteration 5431 : loss : 0.018645, loss_ce: 0.006615
2022-01-09 03:02:08,361 iteration 5432 : loss : 0.019424, loss_ce: 0.009353
2022-01-09 03:02:09,906 iteration 5433 : loss : 0.021547, loss_ce: 0.008635
2022-01-09 03:02:11,305 iteration 5434 : loss : 0.018119, loss_ce: 0.007896
2022-01-09 03:02:12,868 iteration 5435 : loss : 0.025645, loss_ce: 0.011198
2022-01-09 03:02:14,372 iteration 5436 : loss : 0.014729, loss_ce: 0.005445
2022-01-09 03:02:15,898 iteration 5437 : loss : 0.034572, loss_ce: 0.008690
2022-01-09 03:02:17,437 iteration 5438 : loss : 0.018687, loss_ce: 0.008574
2022-01-09 03:02:18,888 iteration 5439 : loss : 0.017570, loss_ce: 0.008440
2022-01-09 03:02:18,888 Training Data Eval:
2022-01-09 03:02:26,175   Average segmentation loss on training set: 0.0113
2022-01-09 03:02:26,176 Validation Data Eval:
2022-01-09 03:02:28,668   Average segmentation loss on validation set: 0.0652
2022-01-09 03:02:30,058 iteration 5440 : loss : 0.017440, loss_ce: 0.005956
 80%|███████████████████████▏     | 320/400 [2:25:58<38:32, 28.90s/it]2022-01-09 03:02:31,555 iteration 5441 : loss : 0.015161, loss_ce: 0.005239
2022-01-09 03:02:32,948 iteration 5442 : loss : 0.018366, loss_ce: 0.006658
2022-01-09 03:02:34,400 iteration 5443 : loss : 0.019526, loss_ce: 0.006038
2022-01-09 03:02:35,757 iteration 5444 : loss : 0.015990, loss_ce: 0.006111
2022-01-09 03:02:37,232 iteration 5445 : loss : 0.019015, loss_ce: 0.006949
2022-01-09 03:02:38,634 iteration 5446 : loss : 0.017959, loss_ce: 0.008393
2022-01-09 03:02:40,211 iteration 5447 : loss : 0.018609, loss_ce: 0.006128
2022-01-09 03:02:41,608 iteration 5448 : loss : 0.015077, loss_ce: 0.005622
2022-01-09 03:02:43,086 iteration 5449 : loss : 0.028023, loss_ce: 0.015389
2022-01-09 03:02:44,547 iteration 5450 : loss : 0.018341, loss_ce: 0.007498
2022-01-09 03:02:45,927 iteration 5451 : loss : 0.020068, loss_ce: 0.007387
2022-01-09 03:02:47,406 iteration 5452 : loss : 0.019303, loss_ce: 0.007765
2022-01-09 03:02:48,814 iteration 5453 : loss : 0.014109, loss_ce: 0.005649
2022-01-09 03:02:50,349 iteration 5454 : loss : 0.020387, loss_ce: 0.011792
2022-01-09 03:02:51,847 iteration 5455 : loss : 0.038518, loss_ce: 0.011065
2022-01-09 03:02:53,311 iteration 5456 : loss : 0.014492, loss_ce: 0.003382
2022-01-09 03:02:54,718 iteration 5457 : loss : 0.014119, loss_ce: 0.004281
 80%|███████████████████████▎     | 321/400 [2:26:23<36:23, 27.63s/it]2022-01-09 03:02:56,222 iteration 5458 : loss : 0.020907, loss_ce: 0.006574
2022-01-09 03:02:57,605 iteration 5459 : loss : 0.017765, loss_ce: 0.007540
2022-01-09 03:02:59,068 iteration 5460 : loss : 0.013588, loss_ce: 0.004992
2022-01-09 03:03:00,516 iteration 5461 : loss : 0.019219, loss_ce: 0.007121
2022-01-09 03:03:01,962 iteration 5462 : loss : 0.015287, loss_ce: 0.005635
2022-01-09 03:03:03,443 iteration 5463 : loss : 0.019246, loss_ce: 0.005949
2022-01-09 03:03:04,916 iteration 5464 : loss : 0.018676, loss_ce: 0.007925
2022-01-09 03:03:06,472 iteration 5465 : loss : 0.025325, loss_ce: 0.009872
2022-01-09 03:03:08,021 iteration 5466 : loss : 0.016856, loss_ce: 0.006180
2022-01-09 03:03:09,526 iteration 5467 : loss : 0.030423, loss_ce: 0.015612
2022-01-09 03:03:11,027 iteration 5468 : loss : 0.020281, loss_ce: 0.008707
2022-01-09 03:03:12,399 iteration 5469 : loss : 0.017092, loss_ce: 0.006113
2022-01-09 03:03:13,841 iteration 5470 : loss : 0.010687, loss_ce: 0.003715
2022-01-09 03:03:15,291 iteration 5471 : loss : 0.019558, loss_ce: 0.005521
2022-01-09 03:03:16,789 iteration 5472 : loss : 0.024987, loss_ce: 0.009208
2022-01-09 03:03:18,177 iteration 5473 : loss : 0.013561, loss_ce: 0.004981
2022-01-09 03:03:19,571 iteration 5474 : loss : 0.015185, loss_ce: 0.004988
 80%|███████████████████████▎     | 322/400 [2:26:48<34:50, 26.80s/it]2022-01-09 03:03:21,135 iteration 5475 : loss : 0.031336, loss_ce: 0.009257
2022-01-09 03:03:22,668 iteration 5476 : loss : 0.019029, loss_ce: 0.009064
2022-01-09 03:03:24,130 iteration 5477 : loss : 0.062775, loss_ce: 0.009994
2022-01-09 03:03:25,488 iteration 5478 : loss : 0.015990, loss_ce: 0.005361
2022-01-09 03:03:26,940 iteration 5479 : loss : 0.013850, loss_ce: 0.005904
2022-01-09 03:03:28,382 iteration 5480 : loss : 0.016949, loss_ce: 0.005538
2022-01-09 03:03:29,794 iteration 5481 : loss : 0.019874, loss_ce: 0.006791
2022-01-09 03:03:31,392 iteration 5482 : loss : 0.039989, loss_ce: 0.010566
2022-01-09 03:03:32,754 iteration 5483 : loss : 0.014379, loss_ce: 0.005847
2022-01-09 03:03:34,237 iteration 5484 : loss : 0.036304, loss_ce: 0.012391
2022-01-09 03:03:35,691 iteration 5485 : loss : 0.018098, loss_ce: 0.007917
2022-01-09 03:03:37,145 iteration 5486 : loss : 0.020255, loss_ce: 0.009111
2022-01-09 03:03:38,624 iteration 5487 : loss : 0.029147, loss_ce: 0.010442
2022-01-09 03:03:40,033 iteration 5488 : loss : 0.017938, loss_ce: 0.008282
2022-01-09 03:03:41,438 iteration 5489 : loss : 0.017296, loss_ce: 0.009487
2022-01-09 03:03:42,992 iteration 5490 : loss : 0.031096, loss_ce: 0.011425
2022-01-09 03:03:44,418 iteration 5491 : loss : 0.023146, loss_ce: 0.006929
 81%|███████████████████████▍     | 323/400 [2:27:13<33:38, 26.21s/it]2022-01-09 03:03:45,832 iteration 5492 : loss : 0.018053, loss_ce: 0.006709
2022-01-09 03:03:47,321 iteration 5493 : loss : 0.018807, loss_ce: 0.005937
2022-01-09 03:03:48,722 iteration 5494 : loss : 0.020450, loss_ce: 0.006927
2022-01-09 03:03:50,196 iteration 5495 : loss : 0.014251, loss_ce: 0.004148
2022-01-09 03:03:51,675 iteration 5496 : loss : 0.025081, loss_ce: 0.007520
2022-01-09 03:03:53,149 iteration 5497 : loss : 0.016395, loss_ce: 0.006327
2022-01-09 03:03:54,682 iteration 5498 : loss : 0.022637, loss_ce: 0.009565
2022-01-09 03:03:56,177 iteration 5499 : loss : 0.020961, loss_ce: 0.010798
2022-01-09 03:03:57,557 iteration 5500 : loss : 0.018806, loss_ce: 0.008041
2022-01-09 03:03:59,030 iteration 5501 : loss : 0.021769, loss_ce: 0.008603
2022-01-09 03:04:00,431 iteration 5502 : loss : 0.012697, loss_ce: 0.005449
2022-01-09 03:04:01,878 iteration 5503 : loss : 0.021634, loss_ce: 0.005169
2022-01-09 03:04:03,290 iteration 5504 : loss : 0.017302, loss_ce: 0.006541
2022-01-09 03:04:04,724 iteration 5505 : loss : 0.017062, loss_ce: 0.006647
2022-01-09 03:04:06,221 iteration 5506 : loss : 0.015729, loss_ce: 0.005553
2022-01-09 03:04:07,667 iteration 5507 : loss : 0.020953, loss_ce: 0.009579
2022-01-09 03:04:09,135 iteration 5508 : loss : 0.020685, loss_ce: 0.006957
 81%|███████████████████████▍     | 324/400 [2:27:38<32:38, 25.77s/it]2022-01-09 03:04:10,670 iteration 5509 : loss : 0.023862, loss_ce: 0.006596
2022-01-09 03:04:12,243 iteration 5510 : loss : 0.020716, loss_ce: 0.008111
2022-01-09 03:04:13,676 iteration 5511 : loss : 0.015041, loss_ce: 0.005438
2022-01-09 03:04:15,199 iteration 5512 : loss : 0.020617, loss_ce: 0.008112
2022-01-09 03:04:16,631 iteration 5513 : loss : 0.015536, loss_ce: 0.006562
2022-01-09 03:04:18,009 iteration 5514 : loss : 0.012862, loss_ce: 0.004204
2022-01-09 03:04:19,551 iteration 5515 : loss : 0.037043, loss_ce: 0.008707
2022-01-09 03:04:20,982 iteration 5516 : loss : 0.017858, loss_ce: 0.007044
2022-01-09 03:04:22,465 iteration 5517 : loss : 0.015292, loss_ce: 0.006871
2022-01-09 03:04:23,982 iteration 5518 : loss : 0.020597, loss_ce: 0.006916
2022-01-09 03:04:25,495 iteration 5519 : loss : 0.023644, loss_ce: 0.010234
2022-01-09 03:04:26,978 iteration 5520 : loss : 0.017045, loss_ce: 0.007825
2022-01-09 03:04:28,477 iteration 5521 : loss : 0.026888, loss_ce: 0.010422
2022-01-09 03:04:29,933 iteration 5522 : loss : 0.030478, loss_ce: 0.012549
2022-01-09 03:04:31,354 iteration 5523 : loss : 0.011248, loss_ce: 0.004226
2022-01-09 03:04:32,786 iteration 5524 : loss : 0.020896, loss_ce: 0.008112
2022-01-09 03:04:32,786 Training Data Eval:
2022-01-09 03:04:40,076   Average segmentation loss on training set: 0.0113
2022-01-09 03:04:40,077 Validation Data Eval:
2022-01-09 03:04:42,568   Average segmentation loss on validation set: 0.0702
2022-01-09 03:04:43,990 iteration 5525 : loss : 0.021232, loss_ce: 0.010284
 81%|███████████████████████▌     | 325/400 [2:28:12<35:36, 28.49s/it]2022-01-09 03:04:45,575 iteration 5526 : loss : 0.020143, loss_ce: 0.008229
2022-01-09 03:04:47,046 iteration 5527 : loss : 0.019118, loss_ce: 0.008490
2022-01-09 03:04:48,509 iteration 5528 : loss : 0.021130, loss_ce: 0.008003
2022-01-09 03:04:50,015 iteration 5529 : loss : 0.027750, loss_ce: 0.011979
2022-01-09 03:04:51,399 iteration 5530 : loss : 0.017295, loss_ce: 0.008257
2022-01-09 03:04:52,820 iteration 5531 : loss : 0.027565, loss_ce: 0.012144
2022-01-09 03:04:54,221 iteration 5532 : loss : 0.014591, loss_ce: 0.006386
2022-01-09 03:04:55,759 iteration 5533 : loss : 0.031532, loss_ce: 0.010775
2022-01-09 03:04:57,215 iteration 5534 : loss : 0.018175, loss_ce: 0.006339
2022-01-09 03:04:58,764 iteration 5535 : loss : 0.019830, loss_ce: 0.008296
2022-01-09 03:05:00,356 iteration 5536 : loss : 0.015416, loss_ce: 0.006710
2022-01-09 03:05:01,865 iteration 5537 : loss : 0.021697, loss_ce: 0.006144
2022-01-09 03:05:03,346 iteration 5538 : loss : 0.021805, loss_ce: 0.008846
2022-01-09 03:05:04,942 iteration 5539 : loss : 0.023274, loss_ce: 0.006151
2022-01-09 03:05:06,366 iteration 5540 : loss : 0.016149, loss_ce: 0.007543
2022-01-09 03:05:07,845 iteration 5541 : loss : 0.015107, loss_ce: 0.005731
2022-01-09 03:05:09,278 iteration 5542 : loss : 0.017087, loss_ce: 0.006768
 82%|███████████████████████▋     | 326/400 [2:28:38<33:57, 27.53s/it]2022-01-09 03:05:10,845 iteration 5543 : loss : 0.019030, loss_ce: 0.006663
2022-01-09 03:05:12,324 iteration 5544 : loss : 0.019218, loss_ce: 0.009914
2022-01-09 03:05:13,757 iteration 5545 : loss : 0.027657, loss_ce: 0.010488
2022-01-09 03:05:15,159 iteration 5546 : loss : 0.018133, loss_ce: 0.006195
2022-01-09 03:05:16,629 iteration 5547 : loss : 0.015691, loss_ce: 0.005918
2022-01-09 03:05:18,166 iteration 5548 : loss : 0.022224, loss_ce: 0.007744
2022-01-09 03:05:19,550 iteration 5549 : loss : 0.018261, loss_ce: 0.007372
2022-01-09 03:05:20,976 iteration 5550 : loss : 0.014892, loss_ce: 0.005878
2022-01-09 03:05:22,434 iteration 5551 : loss : 0.022525, loss_ce: 0.009574
2022-01-09 03:05:23,817 iteration 5552 : loss : 0.014447, loss_ce: 0.005669
2022-01-09 03:05:25,297 iteration 5553 : loss : 0.027814, loss_ce: 0.011708
2022-01-09 03:05:26,826 iteration 5554 : loss : 0.017344, loss_ce: 0.005818
2022-01-09 03:05:28,349 iteration 5555 : loss : 0.027207, loss_ce: 0.007786
2022-01-09 03:05:29,720 iteration 5556 : loss : 0.020228, loss_ce: 0.007775
2022-01-09 03:05:31,264 iteration 5557 : loss : 0.028233, loss_ce: 0.007322
2022-01-09 03:05:32,739 iteration 5558 : loss : 0.018397, loss_ce: 0.006714
2022-01-09 03:05:34,085 iteration 5559 : loss : 0.017189, loss_ce: 0.008826
 82%|███████████████████████▋     | 327/400 [2:29:02<32:29, 26.71s/it]2022-01-09 03:05:35,854 iteration 5560 : loss : 0.029771, loss_ce: 0.010892
2022-01-09 03:05:37,300 iteration 5561 : loss : 0.018218, loss_ce: 0.006366
2022-01-09 03:05:38,754 iteration 5562 : loss : 0.027624, loss_ce: 0.007877
2022-01-09 03:05:40,207 iteration 5563 : loss : 0.015033, loss_ce: 0.006250
2022-01-09 03:05:41,801 iteration 5564 : loss : 0.028654, loss_ce: 0.008993
2022-01-09 03:05:43,273 iteration 5565 : loss : 0.014377, loss_ce: 0.006023
2022-01-09 03:05:44,799 iteration 5566 : loss : 0.014820, loss_ce: 0.005931
2022-01-09 03:05:46,404 iteration 5567 : loss : 0.018144, loss_ce: 0.008633
2022-01-09 03:05:47,877 iteration 5568 : loss : 0.018184, loss_ce: 0.006736
2022-01-09 03:05:49,407 iteration 5569 : loss : 0.022792, loss_ce: 0.011990
2022-01-09 03:05:50,897 iteration 5570 : loss : 0.012533, loss_ce: 0.004148
2022-01-09 03:05:52,429 iteration 5571 : loss : 0.020297, loss_ce: 0.009571
2022-01-09 03:05:53,902 iteration 5572 : loss : 0.015670, loss_ce: 0.005472
2022-01-09 03:05:55,352 iteration 5573 : loss : 0.015600, loss_ce: 0.005382
2022-01-09 03:05:56,962 iteration 5574 : loss : 0.030545, loss_ce: 0.011022
2022-01-09 03:05:58,290 iteration 5575 : loss : 0.013292, loss_ce: 0.005849
2022-01-09 03:05:59,590 iteration 5576 : loss : 0.013754, loss_ce: 0.004269
 82%|███████████████████████▊     | 328/400 [2:29:28<31:37, 26.35s/it]2022-01-09 03:06:01,107 iteration 5577 : loss : 0.017525, loss_ce: 0.006097
2022-01-09 03:06:02,529 iteration 5578 : loss : 0.014453, loss_ce: 0.004245
2022-01-09 03:06:03,980 iteration 5579 : loss : 0.016787, loss_ce: 0.005690
2022-01-09 03:06:05,507 iteration 5580 : loss : 0.012460, loss_ce: 0.004451
2022-01-09 03:06:06,944 iteration 5581 : loss : 0.018025, loss_ce: 0.007148
2022-01-09 03:06:08,466 iteration 5582 : loss : 0.019519, loss_ce: 0.006618
2022-01-09 03:06:09,904 iteration 5583 : loss : 0.019124, loss_ce: 0.005535
2022-01-09 03:06:11,494 iteration 5584 : loss : 0.024773, loss_ce: 0.010221
2022-01-09 03:06:12,938 iteration 5585 : loss : 0.021691, loss_ce: 0.007975
2022-01-09 03:06:14,410 iteration 5586 : loss : 0.020016, loss_ce: 0.008564
2022-01-09 03:06:15,883 iteration 5587 : loss : 0.018717, loss_ce: 0.009051
2022-01-09 03:06:17,454 iteration 5588 : loss : 0.036777, loss_ce: 0.011966
2022-01-09 03:06:18,996 iteration 5589 : loss : 0.022787, loss_ce: 0.009196
2022-01-09 03:06:20,445 iteration 5590 : loss : 0.023810, loss_ce: 0.010154
2022-01-09 03:06:21,933 iteration 5591 : loss : 0.020334, loss_ce: 0.008670
2022-01-09 03:06:23,412 iteration 5592 : loss : 0.016368, loss_ce: 0.005556
2022-01-09 03:06:24,878 iteration 5593 : loss : 0.014158, loss_ce: 0.007824
 82%|███████████████████████▊     | 329/400 [2:29:53<30:48, 26.03s/it]2022-01-09 03:06:26,474 iteration 5594 : loss : 0.016384, loss_ce: 0.006056
2022-01-09 03:06:27,932 iteration 5595 : loss : 0.018684, loss_ce: 0.008164
2022-01-09 03:06:29,392 iteration 5596 : loss : 0.019693, loss_ce: 0.007826
2022-01-09 03:06:30,899 iteration 5597 : loss : 0.016830, loss_ce: 0.005366
2022-01-09 03:06:32,483 iteration 5598 : loss : 0.023840, loss_ce: 0.008009
2022-01-09 03:06:34,024 iteration 5599 : loss : 0.014458, loss_ce: 0.005054
2022-01-09 03:06:35,559 iteration 5600 : loss : 0.017449, loss_ce: 0.007121
2022-01-09 03:06:37,096 iteration 5601 : loss : 0.014859, loss_ce: 0.004467
2022-01-09 03:06:38,538 iteration 5602 : loss : 0.021105, loss_ce: 0.005638
2022-01-09 03:06:40,092 iteration 5603 : loss : 0.023107, loss_ce: 0.010910
2022-01-09 03:06:41,557 iteration 5604 : loss : 0.017399, loss_ce: 0.007783
2022-01-09 03:06:43,048 iteration 5605 : loss : 0.016297, loss_ce: 0.005759
2022-01-09 03:06:44,651 iteration 5606 : loss : 0.029970, loss_ce: 0.010457
2022-01-09 03:06:46,160 iteration 5607 : loss : 0.023938, loss_ce: 0.009195
2022-01-09 03:06:47,814 iteration 5608 : loss : 0.022355, loss_ce: 0.011334
2022-01-09 03:06:49,257 iteration 5609 : loss : 0.018250, loss_ce: 0.008985
2022-01-09 03:06:49,257 Training Data Eval:
2022-01-09 03:06:56,750   Average segmentation loss on training set: 0.0104
2022-01-09 03:06:56,750 Validation Data Eval:
2022-01-09 03:06:59,320   Average segmentation loss on validation set: 0.0774
2022-01-09 03:07:00,834 iteration 5610 : loss : 0.015484, loss_ce: 0.005470
 82%|███████████████████████▉     | 330/400 [2:30:29<33:50, 29.01s/it]2022-01-09 03:07:02,431 iteration 5611 : loss : 0.016613, loss_ce: 0.007636
2022-01-09 03:07:03,942 iteration 5612 : loss : 0.022108, loss_ce: 0.007212
2022-01-09 03:07:05,400 iteration 5613 : loss : 0.017700, loss_ce: 0.008492
2022-01-09 03:07:06,862 iteration 5614 : loss : 0.031478, loss_ce: 0.010691
2022-01-09 03:07:08,367 iteration 5615 : loss : 0.020409, loss_ce: 0.007678
2022-01-09 03:07:09,889 iteration 5616 : loss : 0.019429, loss_ce: 0.007665
2022-01-09 03:07:11,370 iteration 5617 : loss : 0.015336, loss_ce: 0.005706
2022-01-09 03:07:12,784 iteration 5618 : loss : 0.017229, loss_ce: 0.007824
2022-01-09 03:07:14,318 iteration 5619 : loss : 0.020069, loss_ce: 0.007696
2022-01-09 03:07:15,871 iteration 5620 : loss : 0.019116, loss_ce: 0.005444
2022-01-09 03:07:17,310 iteration 5621 : loss : 0.017606, loss_ce: 0.006370
2022-01-09 03:07:18,897 iteration 5622 : loss : 0.019435, loss_ce: 0.008058
2022-01-09 03:07:20,315 iteration 5623 : loss : 0.024965, loss_ce: 0.007285
2022-01-09 03:07:21,813 iteration 5624 : loss : 0.016630, loss_ce: 0.006759
2022-01-09 03:07:23,335 iteration 5625 : loss : 0.026624, loss_ce: 0.011747
2022-01-09 03:07:24,805 iteration 5626 : loss : 0.023237, loss_ce: 0.009485
2022-01-09 03:07:26,264 iteration 5627 : loss : 0.025448, loss_ce: 0.008856
 83%|███████████████████████▉     | 331/400 [2:30:55<32:07, 27.93s/it]2022-01-09 03:07:27,664 iteration 5628 : loss : 0.015930, loss_ce: 0.005929
2022-01-09 03:07:29,170 iteration 5629 : loss : 0.020161, loss_ce: 0.009112
2022-01-09 03:07:30,631 iteration 5630 : loss : 0.017743, loss_ce: 0.004900
2022-01-09 03:07:32,231 iteration 5631 : loss : 0.026408, loss_ce: 0.013892
2022-01-09 03:07:33,592 iteration 5632 : loss : 0.019422, loss_ce: 0.008927
2022-01-09 03:07:35,119 iteration 5633 : loss : 0.020521, loss_ce: 0.008271
2022-01-09 03:07:36,528 iteration 5634 : loss : 0.016966, loss_ce: 0.006085
2022-01-09 03:07:38,035 iteration 5635 : loss : 0.025724, loss_ce: 0.006782
2022-01-09 03:07:39,426 iteration 5636 : loss : 0.013550, loss_ce: 0.006030
2022-01-09 03:07:40,917 iteration 5637 : loss : 0.017514, loss_ce: 0.006932
2022-01-09 03:07:42,242 iteration 5638 : loss : 0.014491, loss_ce: 0.004404
2022-01-09 03:07:43,632 iteration 5639 : loss : 0.016945, loss_ce: 0.004780
2022-01-09 03:07:45,068 iteration 5640 : loss : 0.014977, loss_ce: 0.006921
2022-01-09 03:07:46,582 iteration 5641 : loss : 0.014000, loss_ce: 0.007012
2022-01-09 03:07:48,028 iteration 5642 : loss : 0.016544, loss_ce: 0.003222
2022-01-09 03:07:49,559 iteration 5643 : loss : 0.016529, loss_ce: 0.004699
2022-01-09 03:07:51,079 iteration 5644 : loss : 0.016252, loss_ce: 0.006295
 83%|████████████████████████     | 332/400 [2:31:19<30:36, 27.00s/it]2022-01-09 03:07:52,769 iteration 5645 : loss : 0.019399, loss_ce: 0.007484
2022-01-09 03:07:54,227 iteration 5646 : loss : 0.018134, loss_ce: 0.006338
2022-01-09 03:07:55,720 iteration 5647 : loss : 0.017663, loss_ce: 0.007837
2022-01-09 03:07:57,260 iteration 5648 : loss : 0.019554, loss_ce: 0.006636
2022-01-09 03:07:58,790 iteration 5649 : loss : 0.016551, loss_ce: 0.007548
2022-01-09 03:08:00,271 iteration 5650 : loss : 0.016731, loss_ce: 0.006337
2022-01-09 03:08:01,738 iteration 5651 : loss : 0.019512, loss_ce: 0.008742
2022-01-09 03:08:03,102 iteration 5652 : loss : 0.014668, loss_ce: 0.005334
2022-01-09 03:08:04,539 iteration 5653 : loss : 0.024382, loss_ce: 0.010712
2022-01-09 03:08:05,919 iteration 5654 : loss : 0.014898, loss_ce: 0.004201
2022-01-09 03:08:07,420 iteration 5655 : loss : 0.017400, loss_ce: 0.007742
2022-01-09 03:08:08,883 iteration 5656 : loss : 0.015345, loss_ce: 0.005456
2022-01-09 03:08:10,349 iteration 5657 : loss : 0.017353, loss_ce: 0.006883
2022-01-09 03:08:11,811 iteration 5658 : loss : 0.018795, loss_ce: 0.010460
2022-01-09 03:08:13,243 iteration 5659 : loss : 0.014835, loss_ce: 0.004184
2022-01-09 03:08:14,631 iteration 5660 : loss : 0.014750, loss_ce: 0.005832
2022-01-09 03:08:16,218 iteration 5661 : loss : 0.018476, loss_ce: 0.006367
 83%|████████████████████████▏    | 333/400 [2:31:45<29:31, 26.44s/it]2022-01-09 03:08:17,746 iteration 5662 : loss : 0.022179, loss_ce: 0.005540
2022-01-09 03:08:19,211 iteration 5663 : loss : 0.023939, loss_ce: 0.011037
2022-01-09 03:08:20,704 iteration 5664 : loss : 0.025621, loss_ce: 0.008220
2022-01-09 03:08:22,059 iteration 5665 : loss : 0.022092, loss_ce: 0.009253
2022-01-09 03:08:23,559 iteration 5666 : loss : 0.016051, loss_ce: 0.005649
2022-01-09 03:08:25,097 iteration 5667 : loss : 0.018069, loss_ce: 0.008014
2022-01-09 03:08:26,560 iteration 5668 : loss : 0.016019, loss_ce: 0.007356
2022-01-09 03:08:28,079 iteration 5669 : loss : 0.027161, loss_ce: 0.014224
2022-01-09 03:08:29,572 iteration 5670 : loss : 0.022440, loss_ce: 0.007723
2022-01-09 03:08:31,061 iteration 5671 : loss : 0.018389, loss_ce: 0.006710
2022-01-09 03:08:32,524 iteration 5672 : loss : 0.014390, loss_ce: 0.004060
2022-01-09 03:08:33,940 iteration 5673 : loss : 0.014370, loss_ce: 0.004723
2022-01-09 03:08:35,492 iteration 5674 : loss : 0.022480, loss_ce: 0.011493
2022-01-09 03:08:36,923 iteration 5675 : loss : 0.022512, loss_ce: 0.011884
2022-01-09 03:08:38,457 iteration 5676 : loss : 0.015153, loss_ce: 0.005487
2022-01-09 03:08:39,984 iteration 5677 : loss : 0.014644, loss_ce: 0.006068
2022-01-09 03:08:41,478 iteration 5678 : loss : 0.027180, loss_ce: 0.008131
 84%|████████████████████████▏    | 334/400 [2:32:10<28:41, 26.08s/it]2022-01-09 03:08:43,031 iteration 5679 : loss : 0.023014, loss_ce: 0.010193
2022-01-09 03:08:44,366 iteration 5680 : loss : 0.013109, loss_ce: 0.003360
2022-01-09 03:08:45,900 iteration 5681 : loss : 0.024374, loss_ce: 0.007783
2022-01-09 03:08:47,430 iteration 5682 : loss : 0.019533, loss_ce: 0.008431
2022-01-09 03:08:48,856 iteration 5683 : loss : 0.029412, loss_ce: 0.014240
2022-01-09 03:08:50,277 iteration 5684 : loss : 0.017504, loss_ce: 0.008966
2022-01-09 03:08:51,765 iteration 5685 : loss : 0.019388, loss_ce: 0.008682
2022-01-09 03:08:53,204 iteration 5686 : loss : 0.017196, loss_ce: 0.006845
2022-01-09 03:08:54,669 iteration 5687 : loss : 0.013680, loss_ce: 0.005768
2022-01-09 03:08:56,265 iteration 5688 : loss : 0.026597, loss_ce: 0.011680
2022-01-09 03:08:57,740 iteration 5689 : loss : 0.032449, loss_ce: 0.007092
2022-01-09 03:08:59,232 iteration 5690 : loss : 0.017216, loss_ce: 0.007562
2022-01-09 03:09:00,751 iteration 5691 : loss : 0.028018, loss_ce: 0.011370
2022-01-09 03:09:02,251 iteration 5692 : loss : 0.018717, loss_ce: 0.008612
2022-01-09 03:09:03,730 iteration 5693 : loss : 0.033502, loss_ce: 0.008270
2022-01-09 03:09:05,180 iteration 5694 : loss : 0.019893, loss_ce: 0.009465
2022-01-09 03:09:05,180 Training Data Eval:
2022-01-09 03:09:12,871   Average segmentation loss on training set: 0.0106
2022-01-09 03:09:12,872 Validation Data Eval:
2022-01-09 03:09:15,554   Average segmentation loss on validation set: 0.0772
2022-01-09 03:09:17,027 iteration 5695 : loss : 0.014151, loss_ce: 0.004636
 84%|████████████████████████▎    | 335/400 [2:32:45<31:20, 28.92s/it]2022-01-09 03:09:18,546 iteration 5696 : loss : 0.013819, loss_ce: 0.007064
2022-01-09 03:09:19,999 iteration 5697 : loss : 0.013975, loss_ce: 0.006382
2022-01-09 03:09:21,577 iteration 5698 : loss : 0.018259, loss_ce: 0.005890
2022-01-09 03:09:22,990 iteration 5699 : loss : 0.014333, loss_ce: 0.006474
2022-01-09 03:09:24,471 iteration 5700 : loss : 0.015940, loss_ce: 0.005512
2022-01-09 03:09:25,910 iteration 5701 : loss : 0.018005, loss_ce: 0.007551
2022-01-09 03:09:27,537 iteration 5702 : loss : 0.024702, loss_ce: 0.009463
2022-01-09 03:09:29,070 iteration 5703 : loss : 0.025144, loss_ce: 0.012196
2022-01-09 03:09:30,594 iteration 5704 : loss : 0.020324, loss_ce: 0.007240
2022-01-09 03:09:32,124 iteration 5705 : loss : 0.021341, loss_ce: 0.007268
2022-01-09 03:09:33,513 iteration 5706 : loss : 0.016773, loss_ce: 0.006786
2022-01-09 03:09:34,978 iteration 5707 : loss : 0.016024, loss_ce: 0.005566
2022-01-09 03:09:36,416 iteration 5708 : loss : 0.019884, loss_ce: 0.007156
2022-01-09 03:09:37,931 iteration 5709 : loss : 0.014683, loss_ce: 0.004989
2022-01-09 03:09:39,420 iteration 5710 : loss : 0.023055, loss_ce: 0.007356
2022-01-09 03:09:40,833 iteration 5711 : loss : 0.015642, loss_ce: 0.005227
2022-01-09 03:09:42,346 iteration 5712 : loss : 0.027597, loss_ce: 0.011401
 84%|████████████████████████▎    | 336/400 [2:33:11<29:42, 27.85s/it]2022-01-09 03:09:43,970 iteration 5713 : loss : 0.018452, loss_ce: 0.006369
2022-01-09 03:09:45,522 iteration 5714 : loss : 0.021240, loss_ce: 0.007501
2022-01-09 03:09:47,081 iteration 5715 : loss : 0.015747, loss_ce: 0.006477
2022-01-09 03:09:48,537 iteration 5716 : loss : 0.020312, loss_ce: 0.008802
2022-01-09 03:09:50,130 iteration 5717 : loss : 0.017514, loss_ce: 0.007568
2022-01-09 03:09:51,548 iteration 5718 : loss : 0.014066, loss_ce: 0.005032
2022-01-09 03:09:52,972 iteration 5719 : loss : 0.014934, loss_ce: 0.006490
2022-01-09 03:09:54,510 iteration 5720 : loss : 0.016058, loss_ce: 0.006455
2022-01-09 03:09:56,009 iteration 5721 : loss : 0.019288, loss_ce: 0.010436
2022-01-09 03:09:57,467 iteration 5722 : loss : 0.017551, loss_ce: 0.007681
2022-01-09 03:09:58,951 iteration 5723 : loss : 0.019665, loss_ce: 0.006667
2022-01-09 03:10:00,470 iteration 5724 : loss : 0.017091, loss_ce: 0.006151
2022-01-09 03:10:01,948 iteration 5725 : loss : 0.017028, loss_ce: 0.006543
2022-01-09 03:10:03,351 iteration 5726 : loss : 0.022636, loss_ce: 0.006211
2022-01-09 03:10:04,835 iteration 5727 : loss : 0.022809, loss_ce: 0.007000
2022-01-09 03:10:06,273 iteration 5728 : loss : 0.017988, loss_ce: 0.005937
2022-01-09 03:10:07,829 iteration 5729 : loss : 0.019406, loss_ce: 0.008502
 84%|████████████████████████▍    | 337/400 [2:33:36<28:29, 27.14s/it]2022-01-09 03:10:09,294 iteration 5730 : loss : 0.014835, loss_ce: 0.006664
2022-01-09 03:10:10,826 iteration 5731 : loss : 0.021380, loss_ce: 0.008924
2022-01-09 03:10:12,231 iteration 5732 : loss : 0.015001, loss_ce: 0.005915
2022-01-09 03:10:13,783 iteration 5733 : loss : 0.017840, loss_ce: 0.004946
2022-01-09 03:10:15,313 iteration 5734 : loss : 0.026898, loss_ce: 0.011607
2022-01-09 03:10:16,745 iteration 5735 : loss : 0.023836, loss_ce: 0.008281
2022-01-09 03:10:18,164 iteration 5736 : loss : 0.021712, loss_ce: 0.008345
2022-01-09 03:10:19,726 iteration 5737 : loss : 0.024834, loss_ce: 0.009480
2022-01-09 03:10:21,155 iteration 5738 : loss : 0.014568, loss_ce: 0.005067
2022-01-09 03:10:22,567 iteration 5739 : loss : 0.015822, loss_ce: 0.006558
2022-01-09 03:10:24,022 iteration 5740 : loss : 0.022960, loss_ce: 0.008496
2022-01-09 03:10:25,485 iteration 5741 : loss : 0.014892, loss_ce: 0.006711
2022-01-09 03:10:26,968 iteration 5742 : loss : 0.016801, loss_ce: 0.003943
2022-01-09 03:10:28,475 iteration 5743 : loss : 0.014893, loss_ce: 0.007020
2022-01-09 03:10:30,023 iteration 5744 : loss : 0.038941, loss_ce: 0.017344
2022-01-09 03:10:31,572 iteration 5745 : loss : 0.021855, loss_ce: 0.008298
2022-01-09 03:10:32,989 iteration 5746 : loss : 0.028965, loss_ce: 0.007892
 84%|████████████████████████▌    | 338/400 [2:34:01<27:25, 26.54s/it]2022-01-09 03:10:34,536 iteration 5747 : loss : 0.019084, loss_ce: 0.006480
2022-01-09 03:10:36,035 iteration 5748 : loss : 0.022254, loss_ce: 0.008627
2022-01-09 03:10:37,506 iteration 5749 : loss : 0.019691, loss_ce: 0.008000
2022-01-09 03:10:38,973 iteration 5750 : loss : 0.016711, loss_ce: 0.006182
2022-01-09 03:10:40,354 iteration 5751 : loss : 0.019683, loss_ce: 0.005989
2022-01-09 03:10:41,789 iteration 5752 : loss : 0.019779, loss_ce: 0.005884
2022-01-09 03:10:43,306 iteration 5753 : loss : 0.015599, loss_ce: 0.005899
2022-01-09 03:10:44,847 iteration 5754 : loss : 0.014291, loss_ce: 0.004342
2022-01-09 03:10:46,317 iteration 5755 : loss : 0.014445, loss_ce: 0.005461
2022-01-09 03:10:47,739 iteration 5756 : loss : 0.016889, loss_ce: 0.005442
2022-01-09 03:10:49,062 iteration 5757 : loss : 0.013349, loss_ce: 0.005528
2022-01-09 03:10:50,592 iteration 5758 : loss : 0.019045, loss_ce: 0.007437
2022-01-09 03:10:52,128 iteration 5759 : loss : 0.015001, loss_ce: 0.006007
2022-01-09 03:10:53,718 iteration 5760 : loss : 0.018638, loss_ce: 0.008776
2022-01-09 03:10:55,315 iteration 5761 : loss : 0.032014, loss_ce: 0.013685
2022-01-09 03:10:56,716 iteration 5762 : loss : 0.013098, loss_ce: 0.005622
2022-01-09 03:10:58,192 iteration 5763 : loss : 0.013043, loss_ce: 0.005367
 85%|████████████████████████▌    | 339/400 [2:34:27<26:34, 26.14s/it]2022-01-09 03:10:59,735 iteration 5764 : loss : 0.016886, loss_ce: 0.006075
2022-01-09 03:11:01,285 iteration 5765 : loss : 0.043324, loss_ce: 0.017380
2022-01-09 03:11:02,793 iteration 5766 : loss : 0.019032, loss_ce: 0.010346
2022-01-09 03:11:04,261 iteration 5767 : loss : 0.013050, loss_ce: 0.006761
2022-01-09 03:11:05,874 iteration 5768 : loss : 0.019833, loss_ce: 0.007450
2022-01-09 03:11:07,413 iteration 5769 : loss : 0.021459, loss_ce: 0.007075
2022-01-09 03:11:08,835 iteration 5770 : loss : 0.015369, loss_ce: 0.005302
2022-01-09 03:11:10,334 iteration 5771 : loss : 0.018883, loss_ce: 0.007461
2022-01-09 03:11:11,790 iteration 5772 : loss : 0.042624, loss_ce: 0.013855
2022-01-09 03:11:13,162 iteration 5773 : loss : 0.032046, loss_ce: 0.008355
2022-01-09 03:11:14,511 iteration 5774 : loss : 0.016916, loss_ce: 0.004485
2022-01-09 03:11:16,004 iteration 5775 : loss : 0.017353, loss_ce: 0.007386
2022-01-09 03:11:17,575 iteration 5776 : loss : 0.025122, loss_ce: 0.011934
2022-01-09 03:11:18,935 iteration 5777 : loss : 0.020357, loss_ce: 0.008744
2022-01-09 03:11:20,361 iteration 5778 : loss : 0.024381, loss_ce: 0.009052
2022-01-09 03:11:21,805 iteration 5779 : loss : 0.013795, loss_ce: 0.004722
2022-01-09 03:11:21,806 Training Data Eval:
2022-01-09 03:11:29,182   Average segmentation loss on training set: 0.0107
2022-01-09 03:11:29,183 Validation Data Eval:
2022-01-09 03:11:31,856   Average segmentation loss on validation set: 0.0707
2022-01-09 03:11:33,382 iteration 5780 : loss : 0.026054, loss_ce: 0.008818
 85%|████████████████████████▋    | 340/400 [2:35:02<28:51, 28.86s/it]2022-01-09 03:11:34,917 iteration 5781 : loss : 0.025625, loss_ce: 0.007922
2022-01-09 03:11:36,480 iteration 5782 : loss : 0.025352, loss_ce: 0.010152
2022-01-09 03:11:37,942 iteration 5783 : loss : 0.024988, loss_ce: 0.009191
2022-01-09 03:11:39,441 iteration 5784 : loss : 0.018789, loss_ce: 0.010230
2022-01-09 03:11:40,856 iteration 5785 : loss : 0.014989, loss_ce: 0.006003
2022-01-09 03:11:42,284 iteration 5786 : loss : 0.016287, loss_ce: 0.006463
2022-01-09 03:11:43,698 iteration 5787 : loss : 0.014058, loss_ce: 0.005210
2022-01-09 03:11:45,336 iteration 5788 : loss : 0.026272, loss_ce: 0.012280
2022-01-09 03:11:46,828 iteration 5789 : loss : 0.015014, loss_ce: 0.005937
2022-01-09 03:11:48,296 iteration 5790 : loss : 0.028151, loss_ce: 0.008556
2022-01-09 03:11:49,703 iteration 5791 : loss : 0.014621, loss_ce: 0.005551
2022-01-09 03:11:51,214 iteration 5792 : loss : 0.023108, loss_ce: 0.009499
2022-01-09 03:11:52,832 iteration 5793 : loss : 0.021781, loss_ce: 0.009687
2022-01-09 03:11:54,324 iteration 5794 : loss : 0.015067, loss_ce: 0.006092
2022-01-09 03:11:55,792 iteration 5795 : loss : 0.016764, loss_ce: 0.006153
2022-01-09 03:11:57,195 iteration 5796 : loss : 0.020642, loss_ce: 0.006091
2022-01-09 03:11:58,696 iteration 5797 : loss : 0.021929, loss_ce: 0.008868
 85%|████████████████████████▋    | 341/400 [2:35:27<27:19, 27.79s/it]2022-01-09 03:12:00,280 iteration 5798 : loss : 0.026803, loss_ce: 0.011555
2022-01-09 03:12:01,767 iteration 5799 : loss : 0.019456, loss_ce: 0.006942
2022-01-09 03:12:03,235 iteration 5800 : loss : 0.015292, loss_ce: 0.006836
2022-01-09 03:12:04,693 iteration 5801 : loss : 0.025180, loss_ce: 0.008955
2022-01-09 03:12:06,075 iteration 5802 : loss : 0.012352, loss_ce: 0.005305
2022-01-09 03:12:07,504 iteration 5803 : loss : 0.015901, loss_ce: 0.008278
2022-01-09 03:12:08,963 iteration 5804 : loss : 0.018173, loss_ce: 0.006892
2022-01-09 03:12:10,548 iteration 5805 : loss : 0.017507, loss_ce: 0.006154
2022-01-09 03:12:12,002 iteration 5806 : loss : 0.024931, loss_ce: 0.009099
2022-01-09 03:12:13,572 iteration 5807 : loss : 0.022549, loss_ce: 0.008347
2022-01-09 03:12:15,081 iteration 5808 : loss : 0.016378, loss_ce: 0.007657
2022-01-09 03:12:16,474 iteration 5809 : loss : 0.011263, loss_ce: 0.002861
2022-01-09 03:12:17,897 iteration 5810 : loss : 0.017337, loss_ce: 0.006696
2022-01-09 03:12:19,314 iteration 5811 : loss : 0.013076, loss_ce: 0.003922
2022-01-09 03:12:20,837 iteration 5812 : loss : 0.019735, loss_ce: 0.006855
2022-01-09 03:12:22,264 iteration 5813 : loss : 0.016853, loss_ce: 0.007824
2022-01-09 03:12:23,791 iteration 5814 : loss : 0.019360, loss_ce: 0.008502
 86%|████████████████████████▊    | 342/400 [2:35:52<26:05, 26.99s/it]2022-01-09 03:12:25,267 iteration 5815 : loss : 0.014257, loss_ce: 0.004854
2022-01-09 03:12:26,748 iteration 5816 : loss : 0.013400, loss_ce: 0.006179
2022-01-09 03:12:28,217 iteration 5817 : loss : 0.013982, loss_ce: 0.005334
2022-01-09 03:12:29,615 iteration 5818 : loss : 0.014564, loss_ce: 0.006368
2022-01-09 03:12:31,189 iteration 5819 : loss : 0.018597, loss_ce: 0.006501
2022-01-09 03:12:32,798 iteration 5820 : loss : 0.029161, loss_ce: 0.016831
2022-01-09 03:12:34,197 iteration 5821 : loss : 0.028620, loss_ce: 0.009902
2022-01-09 03:12:35,642 iteration 5822 : loss : 0.011755, loss_ce: 0.005187
2022-01-09 03:12:37,267 iteration 5823 : loss : 0.018063, loss_ce: 0.007673
2022-01-09 03:12:38,771 iteration 5824 : loss : 0.017263, loss_ce: 0.005182
2022-01-09 03:12:40,327 iteration 5825 : loss : 0.020962, loss_ce: 0.008884
2022-01-09 03:12:41,874 iteration 5826 : loss : 0.019786, loss_ce: 0.004853
2022-01-09 03:12:43,374 iteration 5827 : loss : 0.025741, loss_ce: 0.007411
2022-01-09 03:12:44,994 iteration 5828 : loss : 0.025640, loss_ce: 0.009494
2022-01-09 03:12:46,457 iteration 5829 : loss : 0.015116, loss_ce: 0.006387
2022-01-09 03:12:47,925 iteration 5830 : loss : 0.026869, loss_ce: 0.010516
2022-01-09 03:12:49,405 iteration 5831 : loss : 0.017565, loss_ce: 0.006697
 86%|████████████████████████▊    | 343/400 [2:36:18<25:14, 26.57s/it]2022-01-09 03:12:50,971 iteration 5832 : loss : 0.015227, loss_ce: 0.004544
2022-01-09 03:12:52,471 iteration 5833 : loss : 0.018181, loss_ce: 0.008425
2022-01-09 03:12:53,902 iteration 5834 : loss : 0.012627, loss_ce: 0.004326
2022-01-09 03:12:55,342 iteration 5835 : loss : 0.017846, loss_ce: 0.005324
2022-01-09 03:12:56,900 iteration 5836 : loss : 0.029277, loss_ce: 0.014055
2022-01-09 03:12:58,388 iteration 5837 : loss : 0.016528, loss_ce: 0.003741
2022-01-09 03:12:59,774 iteration 5838 : loss : 0.010338, loss_ce: 0.003177
2022-01-09 03:13:01,256 iteration 5839 : loss : 0.018078, loss_ce: 0.006041
2022-01-09 03:13:02,807 iteration 5840 : loss : 0.020786, loss_ce: 0.006801
2022-01-09 03:13:04,253 iteration 5841 : loss : 0.017874, loss_ce: 0.007764
2022-01-09 03:13:05,687 iteration 5842 : loss : 0.009213, loss_ce: 0.003687
2022-01-09 03:13:07,108 iteration 5843 : loss : 0.015413, loss_ce: 0.006467
2022-01-09 03:13:08,750 iteration 5844 : loss : 0.022435, loss_ce: 0.006518
2022-01-09 03:13:10,222 iteration 5845 : loss : 0.031717, loss_ce: 0.009609
2022-01-09 03:13:11,714 iteration 5846 : loss : 0.027965, loss_ce: 0.010841
2022-01-09 03:13:13,212 iteration 5847 : loss : 0.017086, loss_ce: 0.006854
2022-01-09 03:13:14,686 iteration 5848 : loss : 0.017819, loss_ce: 0.006665
 86%|████████████████████████▉    | 344/400 [2:36:43<24:26, 26.19s/it]2022-01-09 03:13:16,247 iteration 5849 : loss : 0.021642, loss_ce: 0.008180
2022-01-09 03:13:17,631 iteration 5850 : loss : 0.013689, loss_ce: 0.005653
2022-01-09 03:13:19,090 iteration 5851 : loss : 0.018591, loss_ce: 0.008014
2022-01-09 03:13:20,521 iteration 5852 : loss : 0.014355, loss_ce: 0.005320
2022-01-09 03:13:21,887 iteration 5853 : loss : 0.014738, loss_ce: 0.005056
2022-01-09 03:13:23,323 iteration 5854 : loss : 0.021411, loss_ce: 0.007970
2022-01-09 03:13:24,893 iteration 5855 : loss : 0.022685, loss_ce: 0.010197
2022-01-09 03:13:26,372 iteration 5856 : loss : 0.029877, loss_ce: 0.014011
2022-01-09 03:13:27,860 iteration 5857 : loss : 0.015101, loss_ce: 0.005334
2022-01-09 03:13:29,224 iteration 5858 : loss : 0.015163, loss_ce: 0.005816
2022-01-09 03:13:30,745 iteration 5859 : loss : 0.018542, loss_ce: 0.007231
2022-01-09 03:13:32,268 iteration 5860 : loss : 0.017930, loss_ce: 0.005388
2022-01-09 03:13:33,702 iteration 5861 : loss : 0.015096, loss_ce: 0.004747
2022-01-09 03:13:35,194 iteration 5862 : loss : 0.016967, loss_ce: 0.008127
2022-01-09 03:13:36,538 iteration 5863 : loss : 0.012337, loss_ce: 0.004373
2022-01-09 03:13:38,000 iteration 5864 : loss : 0.025189, loss_ce: 0.007442
2022-01-09 03:13:38,001 Training Data Eval:
2022-01-09 03:13:45,297   Average segmentation loss on training set: 0.0103
2022-01-09 03:13:45,297 Validation Data Eval:
2022-01-09 03:13:47,789   Average segmentation loss on validation set: 0.0735
2022-01-09 03:13:49,172 iteration 5865 : loss : 0.021645, loss_ce: 0.006301
 86%|█████████████████████████    | 345/400 [2:37:18<26:17, 28.68s/it]2022-01-09 03:13:50,798 iteration 5866 : loss : 0.021263, loss_ce: 0.009407
2022-01-09 03:13:52,338 iteration 5867 : loss : 0.017909, loss_ce: 0.005859
2022-01-09 03:13:53,850 iteration 5868 : loss : 0.018113, loss_ce: 0.006374
2022-01-09 03:13:55,203 iteration 5869 : loss : 0.015460, loss_ce: 0.006458
2022-01-09 03:13:56,781 iteration 5870 : loss : 0.021364, loss_ce: 0.005535
2022-01-09 03:13:58,283 iteration 5871 : loss : 0.013939, loss_ce: 0.004729
2022-01-09 03:13:59,797 iteration 5872 : loss : 0.028576, loss_ce: 0.010691
2022-01-09 03:14:01,430 iteration 5873 : loss : 0.026942, loss_ce: 0.008757
2022-01-09 03:14:02,842 iteration 5874 : loss : 0.016340, loss_ce: 0.006031
2022-01-09 03:14:04,278 iteration 5875 : loss : 0.015355, loss_ce: 0.007472
2022-01-09 03:14:05,745 iteration 5876 : loss : 0.016159, loss_ce: 0.005782
2022-01-09 03:14:07,199 iteration 5877 : loss : 0.020741, loss_ce: 0.008105
2022-01-09 03:14:08,696 iteration 5878 : loss : 0.015408, loss_ce: 0.006623
2022-01-09 03:14:10,107 iteration 5879 : loss : 0.021091, loss_ce: 0.009600
2022-01-09 03:14:11,700 iteration 5880 : loss : 0.018359, loss_ce: 0.004882
2022-01-09 03:14:13,087 iteration 5881 : loss : 0.014081, loss_ce: 0.005654
2022-01-09 03:14:14,528 iteration 5882 : loss : 0.020351, loss_ce: 0.009684
 86%|█████████████████████████    | 346/400 [2:37:43<24:54, 27.68s/it]2022-01-09 03:14:16,026 iteration 5883 : loss : 0.015898, loss_ce: 0.005518
2022-01-09 03:14:17,450 iteration 5884 : loss : 0.013878, loss_ce: 0.005743
2022-01-09 03:14:19,086 iteration 5885 : loss : 0.023886, loss_ce: 0.009207
2022-01-09 03:14:20,475 iteration 5886 : loss : 0.016032, loss_ce: 0.004705
2022-01-09 03:14:21,897 iteration 5887 : loss : 0.015632, loss_ce: 0.006620
2022-01-09 03:14:23,477 iteration 5888 : loss : 0.019930, loss_ce: 0.006739
2022-01-09 03:14:25,058 iteration 5889 : loss : 0.017468, loss_ce: 0.006351
2022-01-09 03:14:26,551 iteration 5890 : loss : 0.012246, loss_ce: 0.004101
2022-01-09 03:14:28,094 iteration 5891 : loss : 0.026724, loss_ce: 0.009735
2022-01-09 03:14:29,696 iteration 5892 : loss : 0.021441, loss_ce: 0.008890
2022-01-09 03:14:31,133 iteration 5893 : loss : 0.013698, loss_ce: 0.004608
2022-01-09 03:14:32,711 iteration 5894 : loss : 0.014538, loss_ce: 0.007251
2022-01-09 03:14:34,195 iteration 5895 : loss : 0.030184, loss_ce: 0.010937
2022-01-09 03:14:35,649 iteration 5896 : loss : 0.014664, loss_ce: 0.006823
2022-01-09 03:14:37,214 iteration 5897 : loss : 0.027721, loss_ce: 0.010059
2022-01-09 03:14:38,751 iteration 5898 : loss : 0.027633, loss_ce: 0.012247
2022-01-09 03:14:40,187 iteration 5899 : loss : 0.014291, loss_ce: 0.005214
 87%|█████████████████████████▏   | 347/400 [2:38:09<23:54, 27.07s/it]2022-01-09 03:14:41,756 iteration 5900 : loss : 0.014422, loss_ce: 0.004921
2022-01-09 03:14:43,339 iteration 5901 : loss : 0.020842, loss_ce: 0.007255
2022-01-09 03:14:44,814 iteration 5902 : loss : 0.017708, loss_ce: 0.006902
2022-01-09 03:14:46,374 iteration 5903 : loss : 0.012854, loss_ce: 0.005960
2022-01-09 03:14:47,833 iteration 5904 : loss : 0.017282, loss_ce: 0.006864
2022-01-09 03:14:49,365 iteration 5905 : loss : 0.032695, loss_ce: 0.008080
2022-01-09 03:14:50,772 iteration 5906 : loss : 0.017655, loss_ce: 0.006425
2022-01-09 03:14:52,284 iteration 5907 : loss : 0.010759, loss_ce: 0.003703
2022-01-09 03:14:53,771 iteration 5908 : loss : 0.010842, loss_ce: 0.003519
2022-01-09 03:14:55,124 iteration 5909 : loss : 0.030978, loss_ce: 0.007118
2022-01-09 03:14:56,604 iteration 5910 : loss : 0.023440, loss_ce: 0.007747
2022-01-09 03:14:58,143 iteration 5911 : loss : 0.023000, loss_ce: 0.010453
2022-01-09 03:14:59,534 iteration 5912 : loss : 0.016813, loss_ce: 0.005079
2022-01-09 03:15:00,970 iteration 5913 : loss : 0.026759, loss_ce: 0.012980
2022-01-09 03:15:02,448 iteration 5914 : loss : 0.018885, loss_ce: 0.006681
2022-01-09 03:15:03,856 iteration 5915 : loss : 0.018557, loss_ce: 0.006859
2022-01-09 03:15:05,457 iteration 5916 : loss : 0.021793, loss_ce: 0.007418
 87%|█████████████████████████▏   | 348/400 [2:38:34<22:59, 26.53s/it]2022-01-09 03:15:07,022 iteration 5917 : loss : 0.015544, loss_ce: 0.006946
2022-01-09 03:15:08,483 iteration 5918 : loss : 0.018186, loss_ce: 0.007510
2022-01-09 03:15:10,063 iteration 5919 : loss : 0.030959, loss_ce: 0.015869
2022-01-09 03:15:11,658 iteration 5920 : loss : 0.021029, loss_ce: 0.011273
2022-01-09 03:15:13,351 iteration 5921 : loss : 0.020949, loss_ce: 0.006387
2022-01-09 03:15:14,832 iteration 5922 : loss : 0.015890, loss_ce: 0.004506
2022-01-09 03:15:16,370 iteration 5923 : loss : 0.015811, loss_ce: 0.004954
2022-01-09 03:15:18,014 iteration 5924 : loss : 0.025214, loss_ce: 0.005114
2022-01-09 03:15:19,521 iteration 5925 : loss : 0.017522, loss_ce: 0.005876
2022-01-09 03:15:21,020 iteration 5926 : loss : 0.017921, loss_ce: 0.007265
2022-01-09 03:15:22,548 iteration 5927 : loss : 0.015032, loss_ce: 0.005889
2022-01-09 03:15:23,962 iteration 5928 : loss : 0.016254, loss_ce: 0.004118
2022-01-09 03:15:25,382 iteration 5929 : loss : 0.019289, loss_ce: 0.009382
2022-01-09 03:15:26,803 iteration 5930 : loss : 0.016742, loss_ce: 0.007171
2022-01-09 03:15:28,300 iteration 5931 : loss : 0.027645, loss_ce: 0.007024
2022-01-09 03:15:29,844 iteration 5932 : loss : 0.022718, loss_ce: 0.008680
2022-01-09 03:15:31,296 iteration 5933 : loss : 0.016849, loss_ce: 0.006094
 87%|█████████████████████████▎   | 349/400 [2:39:00<22:22, 26.32s/it]2022-01-09 03:15:32,770 iteration 5934 : loss : 0.014390, loss_ce: 0.004715
2022-01-09 03:15:34,408 iteration 5935 : loss : 0.019331, loss_ce: 0.007019
2022-01-09 03:15:35,888 iteration 5936 : loss : 0.012687, loss_ce: 0.004942
2022-01-09 03:15:37,315 iteration 5937 : loss : 0.021843, loss_ce: 0.006983
2022-01-09 03:15:38,674 iteration 5938 : loss : 0.011762, loss_ce: 0.004480
2022-01-09 03:15:40,185 iteration 5939 : loss : 0.016158, loss_ce: 0.006585
2022-01-09 03:15:41,579 iteration 5940 : loss : 0.018117, loss_ce: 0.007490
2022-01-09 03:15:42,994 iteration 5941 : loss : 0.018524, loss_ce: 0.007658
2022-01-09 03:15:44,418 iteration 5942 : loss : 0.014499, loss_ce: 0.006740
2022-01-09 03:15:45,888 iteration 5943 : loss : 0.017930, loss_ce: 0.005074
2022-01-09 03:15:47,367 iteration 5944 : loss : 0.016477, loss_ce: 0.008394
2022-01-09 03:15:48,791 iteration 5945 : loss : 0.018564, loss_ce: 0.003992
2022-01-09 03:15:50,385 iteration 5946 : loss : 0.026170, loss_ce: 0.011681
2022-01-09 03:15:51,901 iteration 5947 : loss : 0.021237, loss_ce: 0.009584
2022-01-09 03:15:53,331 iteration 5948 : loss : 0.021964, loss_ce: 0.007910
2022-01-09 03:15:54,889 iteration 5949 : loss : 0.015868, loss_ce: 0.005249
2022-01-09 03:15:54,889 Training Data Eval:
2022-01-09 03:16:02,177   Average segmentation loss on training set: 0.0097
2022-01-09 03:16:02,178 Validation Data Eval:
2022-01-09 03:16:04,670   Average segmentation loss on validation set: 0.0778
2022-01-09 03:16:06,153 iteration 5950 : loss : 0.013700, loss_ce: 0.004944
 88%|█████████████████████████▍   | 350/400 [2:39:35<24:04, 28.89s/it]2022-01-09 03:16:07,625 iteration 5951 : loss : 0.013588, loss_ce: 0.004931
2022-01-09 03:16:09,089 iteration 5952 : loss : 0.018583, loss_ce: 0.008800
2022-01-09 03:16:10,558 iteration 5953 : loss : 0.028596, loss_ce: 0.012068
2022-01-09 03:16:11,977 iteration 5954 : loss : 0.015527, loss_ce: 0.005697
2022-01-09 03:16:13,439 iteration 5955 : loss : 0.015716, loss_ce: 0.005187
2022-01-09 03:16:14,943 iteration 5956 : loss : 0.016608, loss_ce: 0.008547
2022-01-09 03:16:16,432 iteration 5957 : loss : 0.017114, loss_ce: 0.006270
2022-01-09 03:16:17,839 iteration 5958 : loss : 0.018177, loss_ce: 0.008366
2022-01-09 03:16:19,290 iteration 5959 : loss : 0.018852, loss_ce: 0.007184
2022-01-09 03:16:20,691 iteration 5960 : loss : 0.014857, loss_ce: 0.004757
2022-01-09 03:16:22,080 iteration 5961 : loss : 0.019639, loss_ce: 0.004386
2022-01-09 03:16:23,524 iteration 5962 : loss : 0.025133, loss_ce: 0.005073
2022-01-09 03:16:25,021 iteration 5963 : loss : 0.017671, loss_ce: 0.007311
2022-01-09 03:16:26,447 iteration 5964 : loss : 0.016202, loss_ce: 0.007428
2022-01-09 03:16:27,822 iteration 5965 : loss : 0.013231, loss_ce: 0.006185
2022-01-09 03:16:29,354 iteration 5966 : loss : 0.015790, loss_ce: 0.004850
2022-01-09 03:16:30,834 iteration 5967 : loss : 0.016185, loss_ce: 0.007265
 88%|█████████████████████████▍   | 351/400 [2:39:59<22:33, 27.62s/it]2022-01-09 03:16:32,462 iteration 5968 : loss : 0.027433, loss_ce: 0.009711
2022-01-09 03:16:33,895 iteration 5969 : loss : 0.017972, loss_ce: 0.007671
2022-01-09 03:16:35,412 iteration 5970 : loss : 0.020493, loss_ce: 0.009211
2022-01-09 03:16:36,790 iteration 5971 : loss : 0.013771, loss_ce: 0.004486
2022-01-09 03:16:38,130 iteration 5972 : loss : 0.013112, loss_ce: 0.004227
2022-01-09 03:16:39,595 iteration 5973 : loss : 0.020302, loss_ce: 0.009869
2022-01-09 03:16:41,182 iteration 5974 : loss : 0.030346, loss_ce: 0.014807
2022-01-09 03:16:42,577 iteration 5975 : loss : 0.017282, loss_ce: 0.005668
2022-01-09 03:16:44,000 iteration 5976 : loss : 0.015040, loss_ce: 0.004398
2022-01-09 03:16:45,394 iteration 5977 : loss : 0.018077, loss_ce: 0.004711
2022-01-09 03:16:46,775 iteration 5978 : loss : 0.014531, loss_ce: 0.007186
2022-01-09 03:16:48,217 iteration 5979 : loss : 0.013892, loss_ce: 0.006066
2022-01-09 03:16:49,722 iteration 5980 : loss : 0.014314, loss_ce: 0.004826
2022-01-09 03:16:51,133 iteration 5981 : loss : 0.019268, loss_ce: 0.008084
2022-01-09 03:16:52,588 iteration 5982 : loss : 0.023453, loss_ce: 0.009096
2022-01-09 03:16:54,133 iteration 5983 : loss : 0.015559, loss_ce: 0.004759
2022-01-09 03:16:55,553 iteration 5984 : loss : 0.020008, loss_ce: 0.006929
 88%|█████████████████████████▌   | 352/400 [2:40:24<21:24, 26.75s/it]2022-01-09 03:16:57,082 iteration 5985 : loss : 0.014763, loss_ce: 0.004351
2022-01-09 03:16:58,433 iteration 5986 : loss : 0.014322, loss_ce: 0.004363
2022-01-09 03:16:59,931 iteration 5987 : loss : 0.016337, loss_ce: 0.006829
2022-01-09 03:17:01,458 iteration 5988 : loss : 0.025882, loss_ce: 0.011703
2022-01-09 03:17:02,936 iteration 5989 : loss : 0.012941, loss_ce: 0.004983
2022-01-09 03:17:04,372 iteration 5990 : loss : 0.013474, loss_ce: 0.004817
2022-01-09 03:17:05,872 iteration 5991 : loss : 0.018190, loss_ce: 0.005966
2022-01-09 03:17:07,352 iteration 5992 : loss : 0.016865, loss_ce: 0.005782
2022-01-09 03:17:08,801 iteration 5993 : loss : 0.013999, loss_ce: 0.006216
2022-01-09 03:17:10,324 iteration 5994 : loss : 0.025283, loss_ce: 0.010973
2022-01-09 03:17:11,773 iteration 5995 : loss : 0.012662, loss_ce: 0.005064
2022-01-09 03:17:13,171 iteration 5996 : loss : 0.013875, loss_ce: 0.004192
2022-01-09 03:17:14,656 iteration 5997 : loss : 0.018577, loss_ce: 0.006572
2022-01-09 03:17:16,222 iteration 5998 : loss : 0.014447, loss_ce: 0.004735
2022-01-09 03:17:17,831 iteration 5999 : loss : 0.025737, loss_ce: 0.009508
2022-01-09 03:17:19,323 iteration 6000 : loss : 0.014947, loss_ce: 0.006215
2022-01-09 03:17:20,682 iteration 6001 : loss : 0.015127, loss_ce: 0.007231
 88%|█████████████████████████▌   | 353/400 [2:40:49<20:34, 26.26s/it]2022-01-09 03:17:22,232 iteration 6002 : loss : 0.017230, loss_ce: 0.006319
2022-01-09 03:17:23,651 iteration 6003 : loss : 0.012636, loss_ce: 0.004877
2022-01-09 03:17:25,175 iteration 6004 : loss : 0.015407, loss_ce: 0.006908
2022-01-09 03:17:26,683 iteration 6005 : loss : 0.020381, loss_ce: 0.007060
2022-01-09 03:17:28,299 iteration 6006 : loss : 0.018764, loss_ce: 0.007008
2022-01-09 03:17:29,794 iteration 6007 : loss : 0.011626, loss_ce: 0.004644
2022-01-09 03:17:31,270 iteration 6008 : loss : 0.014237, loss_ce: 0.004827
2022-01-09 03:17:32,627 iteration 6009 : loss : 0.013082, loss_ce: 0.006245
2022-01-09 03:17:34,013 iteration 6010 : loss : 0.012656, loss_ce: 0.005223
2022-01-09 03:17:35,527 iteration 6011 : loss : 0.020474, loss_ce: 0.007962
2022-01-09 03:17:37,079 iteration 6012 : loss : 0.019043, loss_ce: 0.004364
2022-01-09 03:17:38,597 iteration 6013 : loss : 0.029811, loss_ce: 0.009487
2022-01-09 03:17:40,105 iteration 6014 : loss : 0.017739, loss_ce: 0.005243
2022-01-09 03:17:41,528 iteration 6015 : loss : 0.015466, loss_ce: 0.005828
2022-01-09 03:17:42,997 iteration 6016 : loss : 0.013891, loss_ce: 0.005380
2022-01-09 03:17:44,465 iteration 6017 : loss : 0.016327, loss_ce: 0.007329
2022-01-09 03:17:45,967 iteration 6018 : loss : 0.017254, loss_ce: 0.007393
 88%|█████████████████████████▋   | 354/400 [2:41:14<19:54, 25.97s/it]2022-01-09 03:17:47,546 iteration 6019 : loss : 0.019099, loss_ce: 0.010763
2022-01-09 03:17:49,050 iteration 6020 : loss : 0.012107, loss_ce: 0.005749
2022-01-09 03:17:50,598 iteration 6021 : loss : 0.025069, loss_ce: 0.005899
2022-01-09 03:17:52,038 iteration 6022 : loss : 0.013845, loss_ce: 0.005783
2022-01-09 03:17:53,603 iteration 6023 : loss : 0.017636, loss_ce: 0.006778
2022-01-09 03:17:55,085 iteration 6024 : loss : 0.018781, loss_ce: 0.006543
2022-01-09 03:17:56,541 iteration 6025 : loss : 0.015515, loss_ce: 0.005675
2022-01-09 03:17:58,046 iteration 6026 : loss : 0.019258, loss_ce: 0.006829
2022-01-09 03:17:59,538 iteration 6027 : loss : 0.021192, loss_ce: 0.007239
2022-01-09 03:18:01,053 iteration 6028 : loss : 0.016122, loss_ce: 0.006913
2022-01-09 03:18:02,490 iteration 6029 : loss : 0.024041, loss_ce: 0.007453
2022-01-09 03:18:03,863 iteration 6030 : loss : 0.012866, loss_ce: 0.005771
2022-01-09 03:18:05,351 iteration 6031 : loss : 0.013728, loss_ce: 0.006513
2022-01-09 03:18:06,729 iteration 6032 : loss : 0.017965, loss_ce: 0.005176
2022-01-09 03:18:08,256 iteration 6033 : loss : 0.017905, loss_ce: 0.006150
2022-01-09 03:18:09,722 iteration 6034 : loss : 0.015206, loss_ce: 0.006207
2022-01-09 03:18:09,722 Training Data Eval:
2022-01-09 03:18:17,069   Average segmentation loss on training set: 0.0093
2022-01-09 03:18:17,070 Validation Data Eval:
2022-01-09 03:18:19,630   Average segmentation loss on validation set: 0.0674
2022-01-09 03:18:20,962 iteration 6035 : loss : 0.010533, loss_ce: 0.004201
 89%|█████████████████████████▋   | 355/400 [2:41:49<21:30, 28.68s/it]2022-01-09 03:18:22,623 iteration 6036 : loss : 0.023458, loss_ce: 0.005465
2022-01-09 03:18:24,150 iteration 6037 : loss : 0.022212, loss_ce: 0.006355
2022-01-09 03:18:25,598 iteration 6038 : loss : 0.017375, loss_ce: 0.005815
2022-01-09 03:18:27,012 iteration 6039 : loss : 0.015762, loss_ce: 0.005300
2022-01-09 03:18:28,481 iteration 6040 : loss : 0.018739, loss_ce: 0.006823
2022-01-09 03:18:29,897 iteration 6041 : loss : 0.013338, loss_ce: 0.004769
2022-01-09 03:18:31,380 iteration 6042 : loss : 0.012311, loss_ce: 0.005334
2022-01-09 03:18:32,783 iteration 6043 : loss : 0.014060, loss_ce: 0.006633
2022-01-09 03:18:34,177 iteration 6044 : loss : 0.013589, loss_ce: 0.006301
2022-01-09 03:18:35,755 iteration 6045 : loss : 0.023495, loss_ce: 0.009831
2022-01-09 03:18:37,166 iteration 6046 : loss : 0.017885, loss_ce: 0.008082
2022-01-09 03:18:38,572 iteration 6047 : loss : 0.012243, loss_ce: 0.005429
2022-01-09 03:18:40,139 iteration 6048 : loss : 0.018742, loss_ce: 0.005903
2022-01-09 03:18:41,538 iteration 6049 : loss : 0.014020, loss_ce: 0.005157
2022-01-09 03:18:42,935 iteration 6050 : loss : 0.018735, loss_ce: 0.005128
2022-01-09 03:18:44,342 iteration 6051 : loss : 0.012323, loss_ce: 0.005461
2022-01-09 03:18:45,871 iteration 6052 : loss : 0.023226, loss_ce: 0.007337
 89%|█████████████████████████▊   | 356/400 [2:42:14<20:12, 27.55s/it]2022-01-09 03:18:47,386 iteration 6053 : loss : 0.017488, loss_ce: 0.005899
2022-01-09 03:18:48,836 iteration 6054 : loss : 0.015911, loss_ce: 0.004911
2022-01-09 03:18:50,357 iteration 6055 : loss : 0.017575, loss_ce: 0.007401
2022-01-09 03:18:51,830 iteration 6056 : loss : 0.016286, loss_ce: 0.005932
2022-01-09 03:18:53,206 iteration 6057 : loss : 0.015768, loss_ce: 0.006236
2022-01-09 03:18:54,612 iteration 6058 : loss : 0.012542, loss_ce: 0.006295
2022-01-09 03:18:56,164 iteration 6059 : loss : 0.016025, loss_ce: 0.004034
2022-01-09 03:18:57,643 iteration 6060 : loss : 0.016287, loss_ce: 0.004746
2022-01-09 03:18:59,037 iteration 6061 : loss : 0.018970, loss_ce: 0.008961
2022-01-09 03:19:00,665 iteration 6062 : loss : 0.021259, loss_ce: 0.007001
2022-01-09 03:19:02,055 iteration 6063 : loss : 0.018353, loss_ce: 0.008068
2022-01-09 03:19:03,467 iteration 6064 : loss : 0.013671, loss_ce: 0.004934
2022-01-09 03:19:04,961 iteration 6065 : loss : 0.020319, loss_ce: 0.007374
2022-01-09 03:19:06,331 iteration 6066 : loss : 0.015874, loss_ce: 0.006404
2022-01-09 03:19:07,818 iteration 6067 : loss : 0.030973, loss_ce: 0.010496
2022-01-09 03:19:09,295 iteration 6068 : loss : 0.014690, loss_ce: 0.002445
2022-01-09 03:19:10,870 iteration 6069 : loss : 0.024377, loss_ce: 0.011899
 89%|█████████████████████████▉   | 357/400 [2:42:39<19:11, 26.78s/it]2022-01-09 03:19:12,421 iteration 6070 : loss : 0.017983, loss_ce: 0.008784
2022-01-09 03:19:13,892 iteration 6071 : loss : 0.014683, loss_ce: 0.005992
2022-01-09 03:19:15,335 iteration 6072 : loss : 0.017886, loss_ce: 0.005893
2022-01-09 03:19:16,801 iteration 6073 : loss : 0.022458, loss_ce: 0.006373
2022-01-09 03:19:18,151 iteration 6074 : loss : 0.014671, loss_ce: 0.006641
2022-01-09 03:19:19,654 iteration 6075 : loss : 0.016100, loss_ce: 0.007000
2022-01-09 03:19:21,115 iteration 6076 : loss : 0.013921, loss_ce: 0.005968
2022-01-09 03:19:22,429 iteration 6077 : loss : 0.011485, loss_ce: 0.004180
2022-01-09 03:19:23,861 iteration 6078 : loss : 0.017697, loss_ce: 0.008755
2022-01-09 03:19:25,405 iteration 6079 : loss : 0.019458, loss_ce: 0.008420
2022-01-09 03:19:26,774 iteration 6080 : loss : 0.012450, loss_ce: 0.004627
2022-01-09 03:19:28,328 iteration 6081 : loss : 0.019280, loss_ce: 0.007181
2022-01-09 03:19:29,781 iteration 6082 : loss : 0.026274, loss_ce: 0.007876
2022-01-09 03:19:31,306 iteration 6083 : loss : 0.016031, loss_ce: 0.005296
2022-01-09 03:19:32,726 iteration 6084 : loss : 0.017178, loss_ce: 0.004590
2022-01-09 03:19:34,143 iteration 6085 : loss : 0.021555, loss_ce: 0.009607
2022-01-09 03:19:35,814 iteration 6086 : loss : 0.027679, loss_ce: 0.006759
 90%|█████████████████████████▉   | 358/400 [2:43:04<18:21, 26.23s/it]2022-01-09 03:19:37,272 iteration 6087 : loss : 0.013151, loss_ce: 0.005044
2022-01-09 03:19:38,740 iteration 6088 : loss : 0.016929, loss_ce: 0.007089
2022-01-09 03:19:40,175 iteration 6089 : loss : 0.015369, loss_ce: 0.007680
2022-01-09 03:19:41,617 iteration 6090 : loss : 0.016715, loss_ce: 0.004395
2022-01-09 03:19:43,065 iteration 6091 : loss : 0.011359, loss_ce: 0.004463
2022-01-09 03:19:44,525 iteration 6092 : loss : 0.017937, loss_ce: 0.006740
2022-01-09 03:19:45,879 iteration 6093 : loss : 0.015018, loss_ce: 0.005501
2022-01-09 03:19:47,259 iteration 6094 : loss : 0.015437, loss_ce: 0.005589
2022-01-09 03:19:48,746 iteration 6095 : loss : 0.018927, loss_ce: 0.008124
2022-01-09 03:19:50,236 iteration 6096 : loss : 0.017238, loss_ce: 0.005020
2022-01-09 03:19:51,591 iteration 6097 : loss : 0.012859, loss_ce: 0.005768
2022-01-09 03:19:53,056 iteration 6098 : loss : 0.019670, loss_ce: 0.008058
2022-01-09 03:19:54,571 iteration 6099 : loss : 0.031056, loss_ce: 0.007163
2022-01-09 03:19:56,131 iteration 6100 : loss : 0.024927, loss_ce: 0.008892
2022-01-09 03:19:57,653 iteration 6101 : loss : 0.012644, loss_ce: 0.005345
2022-01-09 03:19:59,060 iteration 6102 : loss : 0.023815, loss_ce: 0.009045
2022-01-09 03:20:00,374 iteration 6103 : loss : 0.010520, loss_ce: 0.003873
 90%|██████████████████████████   | 359/400 [2:43:29<17:34, 25.73s/it]2022-01-09 03:20:01,957 iteration 6104 : loss : 0.019539, loss_ce: 0.007404
2022-01-09 03:20:03,343 iteration 6105 : loss : 0.023252, loss_ce: 0.010957
2022-01-09 03:20:04,685 iteration 6106 : loss : 0.014526, loss_ce: 0.004938
2022-01-09 03:20:06,065 iteration 6107 : loss : 0.019821, loss_ce: 0.007762
2022-01-09 03:20:07,586 iteration 6108 : loss : 0.023204, loss_ce: 0.007537
2022-01-09 03:20:09,005 iteration 6109 : loss : 0.012887, loss_ce: 0.004774
2022-01-09 03:20:10,478 iteration 6110 : loss : 0.015500, loss_ce: 0.006389
2022-01-09 03:20:12,000 iteration 6111 : loss : 0.017107, loss_ce: 0.006714
2022-01-09 03:20:13,505 iteration 6112 : loss : 0.015295, loss_ce: 0.005184
2022-01-09 03:20:15,056 iteration 6113 : loss : 0.022050, loss_ce: 0.009627
2022-01-09 03:20:16,496 iteration 6114 : loss : 0.017939, loss_ce: 0.007301
2022-01-09 03:20:18,062 iteration 6115 : loss : 0.021864, loss_ce: 0.007260
2022-01-09 03:20:19,560 iteration 6116 : loss : 0.015145, loss_ce: 0.007010
2022-01-09 03:20:20,975 iteration 6117 : loss : 0.017946, loss_ce: 0.007275
2022-01-09 03:20:22,478 iteration 6118 : loss : 0.015367, loss_ce: 0.006647
2022-01-09 03:20:23,920 iteration 6119 : loss : 0.017673, loss_ce: 0.005663
2022-01-09 03:20:23,921 Training Data Eval:
2022-01-09 03:20:31,435   Average segmentation loss on training set: 0.0099
2022-01-09 03:20:31,436 Validation Data Eval:
2022-01-09 03:20:34,022   Average segmentation loss on validation set: 0.0911
2022-01-09 03:20:35,481 iteration 6120 : loss : 0.014789, loss_ce: 0.006727
 90%|██████████████████████████   | 360/400 [2:44:04<19:01, 28.54s/it]2022-01-09 03:20:37,053 iteration 6121 : loss : 0.015530, loss_ce: 0.006804
2022-01-09 03:20:38,573 iteration 6122 : loss : 0.017529, loss_ce: 0.006764
2022-01-09 03:20:40,136 iteration 6123 : loss : 0.020607, loss_ce: 0.008965
2022-01-09 03:20:41,615 iteration 6124 : loss : 0.018423, loss_ce: 0.006178
2022-01-09 03:20:43,216 iteration 6125 : loss : 0.024200, loss_ce: 0.007780
2022-01-09 03:20:44,684 iteration 6126 : loss : 0.022443, loss_ce: 0.005045
2022-01-09 03:20:46,134 iteration 6127 : loss : 0.015411, loss_ce: 0.005694
2022-01-09 03:20:47,707 iteration 6128 : loss : 0.018723, loss_ce: 0.005602
2022-01-09 03:20:49,188 iteration 6129 : loss : 0.013544, loss_ce: 0.005817
2022-01-09 03:20:50,778 iteration 6130 : loss : 0.020513, loss_ce: 0.008876
2022-01-09 03:20:52,284 iteration 6131 : loss : 0.020891, loss_ce: 0.011818
2022-01-09 03:20:53,787 iteration 6132 : loss : 0.024962, loss_ce: 0.006091
2022-01-09 03:20:55,229 iteration 6133 : loss : 0.012668, loss_ce: 0.006594
2022-01-09 03:20:56,638 iteration 6134 : loss : 0.011155, loss_ce: 0.004898
2022-01-09 03:20:58,073 iteration 6135 : loss : 0.010940, loss_ce: 0.004218
2022-01-09 03:20:59,603 iteration 6136 : loss : 0.015666, loss_ce: 0.005848
2022-01-09 03:21:01,077 iteration 6137 : loss : 0.013545, loss_ce: 0.004998
 90%|██████████████████████████▏  | 361/400 [2:44:29<17:58, 27.66s/it]2022-01-09 03:21:02,580 iteration 6138 : loss : 0.013139, loss_ce: 0.004864
2022-01-09 03:21:04,107 iteration 6139 : loss : 0.015474, loss_ce: 0.006898
2022-01-09 03:21:05,545 iteration 6140 : loss : 0.012239, loss_ce: 0.004377
2022-01-09 03:21:07,068 iteration 6141 : loss : 0.014665, loss_ce: 0.005538
2022-01-09 03:21:08,733 iteration 6142 : loss : 0.022032, loss_ce: 0.007922
2022-01-09 03:21:10,133 iteration 6143 : loss : 0.016607, loss_ce: 0.007595
2022-01-09 03:21:11,640 iteration 6144 : loss : 0.013566, loss_ce: 0.005613
2022-01-09 03:21:13,138 iteration 6145 : loss : 0.021963, loss_ce: 0.008024
2022-01-09 03:21:14,611 iteration 6146 : loss : 0.016334, loss_ce: 0.007140
2022-01-09 03:21:16,064 iteration 6147 : loss : 0.013984, loss_ce: 0.003681
2022-01-09 03:21:17,455 iteration 6148 : loss : 0.012161, loss_ce: 0.003867
2022-01-09 03:21:18,847 iteration 6149 : loss : 0.022011, loss_ce: 0.008187
2022-01-09 03:21:20,273 iteration 6150 : loss : 0.020809, loss_ce: 0.006549
2022-01-09 03:21:21,730 iteration 6151 : loss : 0.016877, loss_ce: 0.003921
2022-01-09 03:21:23,145 iteration 6152 : loss : 0.016280, loss_ce: 0.006460
2022-01-09 03:21:24,689 iteration 6153 : loss : 0.025193, loss_ce: 0.011003
2022-01-09 03:21:26,277 iteration 6154 : loss : 0.040697, loss_ce: 0.019820
 90%|██████████████████████████▏  | 362/400 [2:44:55<17:02, 26.92s/it]2022-01-09 03:21:27,765 iteration 6155 : loss : 0.028475, loss_ce: 0.012845
2022-01-09 03:21:29,211 iteration 6156 : loss : 0.013204, loss_ce: 0.005651
2022-01-09 03:21:30,724 iteration 6157 : loss : 0.016737, loss_ce: 0.005033
2022-01-09 03:21:32,148 iteration 6158 : loss : 0.016070, loss_ce: 0.005522
2022-01-09 03:21:33,487 iteration 6159 : loss : 0.012608, loss_ce: 0.004862
2022-01-09 03:21:35,192 iteration 6160 : loss : 0.024206, loss_ce: 0.012103
2022-01-09 03:21:36,554 iteration 6161 : loss : 0.019638, loss_ce: 0.006144
2022-01-09 03:21:38,040 iteration 6162 : loss : 0.012405, loss_ce: 0.004824
2022-01-09 03:21:39,489 iteration 6163 : loss : 0.015515, loss_ce: 0.005726
2022-01-09 03:21:40,896 iteration 6164 : loss : 0.015635, loss_ce: 0.004630
2022-01-09 03:21:42,364 iteration 6165 : loss : 0.015149, loss_ce: 0.006304
2022-01-09 03:21:43,867 iteration 6166 : loss : 0.017803, loss_ce: 0.007793
2022-01-09 03:21:45,217 iteration 6167 : loss : 0.012616, loss_ce: 0.003962
2022-01-09 03:21:46,808 iteration 6168 : loss : 0.032896, loss_ce: 0.011191
2022-01-09 03:21:48,251 iteration 6169 : loss : 0.015746, loss_ce: 0.007145
2022-01-09 03:21:49,754 iteration 6170 : loss : 0.013699, loss_ce: 0.005691
2022-01-09 03:21:51,216 iteration 6171 : loss : 0.016602, loss_ce: 0.004772
 91%|██████████████████████████▎  | 363/400 [2:45:20<16:14, 26.33s/it]2022-01-09 03:21:52,727 iteration 6172 : loss : 0.015777, loss_ce: 0.005091
2022-01-09 03:21:54,061 iteration 6173 : loss : 0.013362, loss_ce: 0.004847
2022-01-09 03:21:55,484 iteration 6174 : loss : 0.016968, loss_ce: 0.008055
2022-01-09 03:21:56,850 iteration 6175 : loss : 0.011153, loss_ce: 0.003167
2022-01-09 03:21:58,332 iteration 6176 : loss : 0.015289, loss_ce: 0.007623
2022-01-09 03:21:59,916 iteration 6177 : loss : 0.019030, loss_ce: 0.009288
2022-01-09 03:22:01,455 iteration 6178 : loss : 0.017600, loss_ce: 0.005746
2022-01-09 03:22:02,965 iteration 6179 : loss : 0.016800, loss_ce: 0.005651
2022-01-09 03:22:04,466 iteration 6180 : loss : 0.025525, loss_ce: 0.005014
2022-01-09 03:22:05,894 iteration 6181 : loss : 0.017901, loss_ce: 0.006627
2022-01-09 03:22:07,382 iteration 6182 : loss : 0.014243, loss_ce: 0.005627
2022-01-09 03:22:08,783 iteration 6183 : loss : 0.022889, loss_ce: 0.009304
2022-01-09 03:22:10,180 iteration 6184 : loss : 0.013378, loss_ce: 0.005154
2022-01-09 03:22:11,691 iteration 6185 : loss : 0.023679, loss_ce: 0.006697
2022-01-09 03:22:13,066 iteration 6186 : loss : 0.012539, loss_ce: 0.004702
2022-01-09 03:22:14,461 iteration 6187 : loss : 0.018422, loss_ce: 0.005605
2022-01-09 03:22:15,849 iteration 6188 : loss : 0.017210, loss_ce: 0.006003
 91%|██████████████████████████▍  | 364/400 [2:45:44<15:29, 25.82s/it]2022-01-09 03:22:17,475 iteration 6189 : loss : 0.022463, loss_ce: 0.007557
2022-01-09 03:22:18,946 iteration 6190 : loss : 0.013531, loss_ce: 0.005037
2022-01-09 03:22:20,501 iteration 6191 : loss : 0.015104, loss_ce: 0.004258
2022-01-09 03:22:22,016 iteration 6192 : loss : 0.016818, loss_ce: 0.005927
2022-01-09 03:22:23,418 iteration 6193 : loss : 0.012263, loss_ce: 0.003907
2022-01-09 03:22:24,935 iteration 6194 : loss : 0.015101, loss_ce: 0.005953
2022-01-09 03:22:26,489 iteration 6195 : loss : 0.018170, loss_ce: 0.006402
2022-01-09 03:22:27,901 iteration 6196 : loss : 0.018145, loss_ce: 0.005583
2022-01-09 03:22:29,480 iteration 6197 : loss : 0.024650, loss_ce: 0.009831
2022-01-09 03:22:31,066 iteration 6198 : loss : 0.020856, loss_ce: 0.007905
2022-01-09 03:22:32,484 iteration 6199 : loss : 0.012508, loss_ce: 0.004832
2022-01-09 03:22:33,985 iteration 6200 : loss : 0.020283, loss_ce: 0.008938
2022-01-09 03:22:35,456 iteration 6201 : loss : 0.016316, loss_ce: 0.006261
2022-01-09 03:22:36,897 iteration 6202 : loss : 0.015399, loss_ce: 0.004701
2022-01-09 03:22:38,324 iteration 6203 : loss : 0.020909, loss_ce: 0.006896
2022-01-09 03:22:39,846 iteration 6204 : loss : 0.018112, loss_ce: 0.008521
2022-01-09 03:22:39,846 Training Data Eval:
2022-01-09 03:22:47,139   Average segmentation loss on training set: 0.0092
2022-01-09 03:22:47,140 Validation Data Eval:
2022-01-09 03:22:49,629   Average segmentation loss on validation set: 0.0765
2022-01-09 03:22:51,218 iteration 6205 : loss : 0.028768, loss_ce: 0.014647
 91%|██████████████████████████▍  | 365/400 [2:46:20<16:43, 28.68s/it]2022-01-09 03:22:52,777 iteration 6206 : loss : 0.015826, loss_ce: 0.007314
2022-01-09 03:22:54,244 iteration 6207 : loss : 0.017546, loss_ce: 0.009838
2022-01-09 03:22:55,745 iteration 6208 : loss : 0.014931, loss_ce: 0.006257
2022-01-09 03:22:57,200 iteration 6209 : loss : 0.025822, loss_ce: 0.009261
2022-01-09 03:22:58,652 iteration 6210 : loss : 0.020429, loss_ce: 0.007990
2022-01-09 03:23:00,073 iteration 6211 : loss : 0.013646, loss_ce: 0.004958
2022-01-09 03:23:01,542 iteration 6212 : loss : 0.019521, loss_ce: 0.008073
2022-01-09 03:23:03,045 iteration 6213 : loss : 0.012061, loss_ce: 0.004477
2022-01-09 03:23:04,512 iteration 6214 : loss : 0.011016, loss_ce: 0.004284
2022-01-09 03:23:06,057 iteration 6215 : loss : 0.024421, loss_ce: 0.008177
2022-01-09 03:23:07,508 iteration 6216 : loss : 0.016302, loss_ce: 0.007990
2022-01-09 03:23:08,885 iteration 6217 : loss : 0.021525, loss_ce: 0.008804
2022-01-09 03:23:10,356 iteration 6218 : loss : 0.017040, loss_ce: 0.006719
2022-01-09 03:23:11,799 iteration 6219 : loss : 0.013532, loss_ce: 0.004577
2022-01-09 03:23:13,210 iteration 6220 : loss : 0.014923, loss_ce: 0.003979
2022-01-09 03:23:14,693 iteration 6221 : loss : 0.016294, loss_ce: 0.004520
2022-01-09 03:23:16,087 iteration 6222 : loss : 0.011698, loss_ce: 0.005609
 92%|██████████████████████████▌  | 366/400 [2:46:44<15:36, 27.54s/it]2022-01-09 03:23:17,661 iteration 6223 : loss : 0.017077, loss_ce: 0.007836
2022-01-09 03:23:19,106 iteration 6224 : loss : 0.017170, loss_ce: 0.007120
2022-01-09 03:23:20,573 iteration 6225 : loss : 0.023423, loss_ce: 0.007692
2022-01-09 03:23:21,980 iteration 6226 : loss : 0.013476, loss_ce: 0.004707
2022-01-09 03:23:23,359 iteration 6227 : loss : 0.013982, loss_ce: 0.005624
2022-01-09 03:23:24,818 iteration 6228 : loss : 0.015699, loss_ce: 0.005504
2022-01-09 03:23:26,288 iteration 6229 : loss : 0.012322, loss_ce: 0.006172
2022-01-09 03:23:27,786 iteration 6230 : loss : 0.022563, loss_ce: 0.006725
2022-01-09 03:23:29,305 iteration 6231 : loss : 0.017882, loss_ce: 0.006931
2022-01-09 03:23:30,789 iteration 6232 : loss : 0.013139, loss_ce: 0.005405
2022-01-09 03:23:32,254 iteration 6233 : loss : 0.018165, loss_ce: 0.008080
2022-01-09 03:23:33,650 iteration 6234 : loss : 0.015697, loss_ce: 0.003424
2022-01-09 03:23:35,046 iteration 6235 : loss : 0.017219, loss_ce: 0.009720
2022-01-09 03:23:36,505 iteration 6236 : loss : 0.014346, loss_ce: 0.007331
2022-01-09 03:23:37,988 iteration 6237 : loss : 0.021759, loss_ce: 0.009985
2022-01-09 03:23:39,535 iteration 6238 : loss : 0.017319, loss_ce: 0.005400
2022-01-09 03:23:41,007 iteration 6239 : loss : 0.023616, loss_ce: 0.007237
 92%|██████████████████████████▌  | 367/400 [2:47:09<14:42, 26.76s/it]2022-01-09 03:23:42,511 iteration 6240 : loss : 0.011375, loss_ce: 0.003685
2022-01-09 03:23:43,906 iteration 6241 : loss : 0.012301, loss_ce: 0.005818
2022-01-09 03:23:45,464 iteration 6242 : loss : 0.028248, loss_ce: 0.008966
2022-01-09 03:23:46,910 iteration 6243 : loss : 0.012210, loss_ce: 0.004794
2022-01-09 03:23:48,482 iteration 6244 : loss : 0.014788, loss_ce: 0.005879
2022-01-09 03:23:49,936 iteration 6245 : loss : 0.017231, loss_ce: 0.006127
2022-01-09 03:23:51,381 iteration 6246 : loss : 0.015933, loss_ce: 0.006461
2022-01-09 03:23:52,874 iteration 6247 : loss : 0.015741, loss_ce: 0.003812
2022-01-09 03:23:54,312 iteration 6248 : loss : 0.013988, loss_ce: 0.005444
2022-01-09 03:23:55,751 iteration 6249 : loss : 0.016339, loss_ce: 0.004968
2022-01-09 03:23:57,245 iteration 6250 : loss : 0.013224, loss_ce: 0.004855
2022-01-09 03:23:58,829 iteration 6251 : loss : 0.016776, loss_ce: 0.006007
2022-01-09 03:24:00,232 iteration 6252 : loss : 0.015430, loss_ce: 0.007212
2022-01-09 03:24:01,752 iteration 6253 : loss : 0.017422, loss_ce: 0.006434
2022-01-09 03:24:03,184 iteration 6254 : loss : 0.015525, loss_ce: 0.006965
2022-01-09 03:24:04,675 iteration 6255 : loss : 0.015121, loss_ce: 0.006281
2022-01-09 03:24:06,241 iteration 6256 : loss : 0.018544, loss_ce: 0.009018
 92%|██████████████████████████▋  | 368/400 [2:47:35<14:01, 26.30s/it]2022-01-09 03:24:07,823 iteration 6257 : loss : 0.031058, loss_ce: 0.010034
2022-01-09 03:24:09,357 iteration 6258 : loss : 0.020356, loss_ce: 0.007755
2022-01-09 03:24:10,915 iteration 6259 : loss : 0.016624, loss_ce: 0.006322
2022-01-09 03:24:12,359 iteration 6260 : loss : 0.019712, loss_ce: 0.008579
2022-01-09 03:24:13,802 iteration 6261 : loss : 0.016384, loss_ce: 0.005990
2022-01-09 03:24:15,297 iteration 6262 : loss : 0.020038, loss_ce: 0.005304
2022-01-09 03:24:16,784 iteration 6263 : loss : 0.019414, loss_ce: 0.004316
2022-01-09 03:24:18,279 iteration 6264 : loss : 0.014482, loss_ce: 0.005858
2022-01-09 03:24:19,767 iteration 6265 : loss : 0.014278, loss_ce: 0.005386
2022-01-09 03:24:21,204 iteration 6266 : loss : 0.024067, loss_ce: 0.011065
2022-01-09 03:24:22,614 iteration 6267 : loss : 0.014136, loss_ce: 0.006412
2022-01-09 03:24:24,078 iteration 6268 : loss : 0.020686, loss_ce: 0.007792
2022-01-09 03:24:25,373 iteration 6269 : loss : 0.011475, loss_ce: 0.004111
2022-01-09 03:24:26,804 iteration 6270 : loss : 0.013815, loss_ce: 0.006403
2022-01-09 03:24:28,245 iteration 6271 : loss : 0.014603, loss_ce: 0.004272
2022-01-09 03:24:29,764 iteration 6272 : loss : 0.020282, loss_ce: 0.007291
2022-01-09 03:24:31,270 iteration 6273 : loss : 0.016743, loss_ce: 0.007102
 92%|██████████████████████████▊  | 369/400 [2:48:00<13:23, 25.92s/it]2022-01-09 03:24:32,995 iteration 6274 : loss : 0.024304, loss_ce: 0.008239
2022-01-09 03:24:34,514 iteration 6275 : loss : 0.018156, loss_ce: 0.008678
2022-01-09 03:24:36,032 iteration 6276 : loss : 0.016061, loss_ce: 0.005874
2022-01-09 03:24:37,487 iteration 6277 : loss : 0.014045, loss_ce: 0.003886
2022-01-09 03:24:38,987 iteration 6278 : loss : 0.021093, loss_ce: 0.006581
2022-01-09 03:24:40,452 iteration 6279 : loss : 0.019080, loss_ce: 0.009003
2022-01-09 03:24:41,877 iteration 6280 : loss : 0.018597, loss_ce: 0.006900
2022-01-09 03:24:43,493 iteration 6281 : loss : 0.036922, loss_ce: 0.008539
2022-01-09 03:24:44,942 iteration 6282 : loss : 0.014495, loss_ce: 0.005796
2022-01-09 03:24:46,361 iteration 6283 : loss : 0.012674, loss_ce: 0.005471
2022-01-09 03:24:47,847 iteration 6284 : loss : 0.019327, loss_ce: 0.008057
2022-01-09 03:24:49,383 iteration 6285 : loss : 0.021557, loss_ce: 0.007654
2022-01-09 03:24:50,838 iteration 6286 : loss : 0.019395, loss_ce: 0.006940
2022-01-09 03:24:52,294 iteration 6287 : loss : 0.032639, loss_ce: 0.008957
2022-01-09 03:24:53,673 iteration 6288 : loss : 0.013088, loss_ce: 0.004634
2022-01-09 03:24:55,196 iteration 6289 : loss : 0.016534, loss_ce: 0.007705
2022-01-09 03:24:55,196 Training Data Eval:
2022-01-09 03:25:02,738   Average segmentation loss on training set: 0.0096
2022-01-09 03:25:02,739 Validation Data Eval:
2022-01-09 03:25:05,298   Average segmentation loss on validation set: 0.0654
2022-01-09 03:25:06,818 iteration 6290 : loss : 0.020161, loss_ce: 0.009244
 92%|██████████████████████████▊  | 370/400 [2:48:35<14:24, 28.81s/it]2022-01-09 03:25:08,346 iteration 6291 : loss : 0.011982, loss_ce: 0.004857
2022-01-09 03:25:09,811 iteration 6292 : loss : 0.018675, loss_ce: 0.007745
2022-01-09 03:25:11,303 iteration 6293 : loss : 0.029724, loss_ce: 0.017188
2022-01-09 03:25:12,825 iteration 6294 : loss : 0.023751, loss_ce: 0.006403
2022-01-09 03:25:14,281 iteration 6295 : loss : 0.014080, loss_ce: 0.006679
2022-01-09 03:25:15,847 iteration 6296 : loss : 0.020475, loss_ce: 0.006734
2022-01-09 03:25:17,333 iteration 6297 : loss : 0.021662, loss_ce: 0.007639
2022-01-09 03:25:18,861 iteration 6298 : loss : 0.023785, loss_ce: 0.010420
2022-01-09 03:25:20,310 iteration 6299 : loss : 0.019508, loss_ce: 0.007002
2022-01-09 03:25:21,732 iteration 6300 : loss : 0.021615, loss_ce: 0.006243
2022-01-09 03:25:23,151 iteration 6301 : loss : 0.016146, loss_ce: 0.009742
2022-01-09 03:25:24,632 iteration 6302 : loss : 0.022795, loss_ce: 0.008354
2022-01-09 03:25:25,982 iteration 6303 : loss : 0.015808, loss_ce: 0.003482
2022-01-09 03:25:27,357 iteration 6304 : loss : 0.013386, loss_ce: 0.003701
2022-01-09 03:25:28,854 iteration 6305 : loss : 0.019851, loss_ce: 0.008504
2022-01-09 03:25:30,305 iteration 6306 : loss : 0.028030, loss_ce: 0.006546
2022-01-09 03:25:31,761 iteration 6307 : loss : 0.014679, loss_ce: 0.006694
 93%|██████████████████████████▉  | 371/400 [2:49:00<13:21, 27.65s/it]2022-01-09 03:25:33,245 iteration 6308 : loss : 0.020366, loss_ce: 0.004167
2022-01-09 03:25:34,727 iteration 6309 : loss : 0.016996, loss_ce: 0.006072
2022-01-09 03:25:36,183 iteration 6310 : loss : 0.018601, loss_ce: 0.007423
2022-01-09 03:25:37,681 iteration 6311 : loss : 0.014004, loss_ce: 0.006066
2022-01-09 03:25:39,189 iteration 6312 : loss : 0.022022, loss_ce: 0.006590
2022-01-09 03:25:40,770 iteration 6313 : loss : 0.017218, loss_ce: 0.006904
2022-01-09 03:25:42,238 iteration 6314 : loss : 0.011510, loss_ce: 0.003514
2022-01-09 03:25:43,657 iteration 6315 : loss : 0.015767, loss_ce: 0.005264
2022-01-09 03:25:45,117 iteration 6316 : loss : 0.014894, loss_ce: 0.005315
2022-01-09 03:25:46,564 iteration 6317 : loss : 0.021004, loss_ce: 0.005605
2022-01-09 03:25:48,108 iteration 6318 : loss : 0.026866, loss_ce: 0.010639
2022-01-09 03:25:49,598 iteration 6319 : loss : 0.014362, loss_ce: 0.004958
2022-01-09 03:25:51,035 iteration 6320 : loss : 0.013292, loss_ce: 0.004653
2022-01-09 03:25:52,556 iteration 6321 : loss : 0.018535, loss_ce: 0.008975
2022-01-09 03:25:54,094 iteration 6322 : loss : 0.014811, loss_ce: 0.006677
2022-01-09 03:25:55,754 iteration 6323 : loss : 0.017666, loss_ce: 0.005793
2022-01-09 03:25:57,306 iteration 6324 : loss : 0.017832, loss_ce: 0.008260
 93%|██████████████████████████▉  | 372/400 [2:49:26<12:36, 27.02s/it]2022-01-09 03:25:58,888 iteration 6325 : loss : 0.015367, loss_ce: 0.004270
2022-01-09 03:26:00,317 iteration 6326 : loss : 0.016025, loss_ce: 0.006763
2022-01-09 03:26:01,774 iteration 6327 : loss : 0.016789, loss_ce: 0.006187
2022-01-09 03:26:03,303 iteration 6328 : loss : 0.016741, loss_ce: 0.008878
2022-01-09 03:26:04,681 iteration 6329 : loss : 0.014627, loss_ce: 0.006475
2022-01-09 03:26:06,122 iteration 6330 : loss : 0.014831, loss_ce: 0.005826
2022-01-09 03:26:07,664 iteration 6331 : loss : 0.018102, loss_ce: 0.006899
2022-01-09 03:26:09,175 iteration 6332 : loss : 0.012938, loss_ce: 0.003703
2022-01-09 03:26:10,636 iteration 6333 : loss : 0.012824, loss_ce: 0.006342
2022-01-09 03:26:12,101 iteration 6334 : loss : 0.018835, loss_ce: 0.005150
2022-01-09 03:26:13,686 iteration 6335 : loss : 0.015716, loss_ce: 0.006492
2022-01-09 03:26:15,112 iteration 6336 : loss : 0.017159, loss_ce: 0.007537
2022-01-09 03:26:16,603 iteration 6337 : loss : 0.018120, loss_ce: 0.005491
2022-01-09 03:26:18,070 iteration 6338 : loss : 0.017066, loss_ce: 0.004899
2022-01-09 03:26:19,531 iteration 6339 : loss : 0.022683, loss_ce: 0.009828
2022-01-09 03:26:20,991 iteration 6340 : loss : 0.015526, loss_ce: 0.006691
2022-01-09 03:26:22,354 iteration 6341 : loss : 0.012032, loss_ce: 0.004095
 93%|███████████████████████████  | 373/400 [2:49:51<11:53, 26.42s/it]2022-01-09 03:26:23,944 iteration 6342 : loss : 0.023565, loss_ce: 0.006743
2022-01-09 03:26:25,386 iteration 6343 : loss : 0.014911, loss_ce: 0.005973
2022-01-09 03:26:26,818 iteration 6344 : loss : 0.015716, loss_ce: 0.005023
2022-01-09 03:26:28,242 iteration 6345 : loss : 0.024395, loss_ce: 0.003934
2022-01-09 03:26:29,719 iteration 6346 : loss : 0.019772, loss_ce: 0.005781
2022-01-09 03:26:31,195 iteration 6347 : loss : 0.016759, loss_ce: 0.006291
2022-01-09 03:26:32,531 iteration 6348 : loss : 0.010482, loss_ce: 0.004463
2022-01-09 03:26:34,003 iteration 6349 : loss : 0.015441, loss_ce: 0.006795
2022-01-09 03:26:35,619 iteration 6350 : loss : 0.026961, loss_ce: 0.010670
2022-01-09 03:26:37,162 iteration 6351 : loss : 0.016957, loss_ce: 0.008410
2022-01-09 03:26:38,634 iteration 6352 : loss : 0.019738, loss_ce: 0.006218
2022-01-09 03:26:40,111 iteration 6353 : loss : 0.016737, loss_ce: 0.007165
2022-01-09 03:26:41,573 iteration 6354 : loss : 0.015954, loss_ce: 0.007870
2022-01-09 03:26:43,003 iteration 6355 : loss : 0.015794, loss_ce: 0.005342
2022-01-09 03:26:44,548 iteration 6356 : loss : 0.021841, loss_ce: 0.005668
2022-01-09 03:26:45,971 iteration 6357 : loss : 0.017286, loss_ce: 0.006120
2022-01-09 03:26:47,573 iteration 6358 : loss : 0.022821, loss_ce: 0.009939
 94%|███████████████████████████  | 374/400 [2:50:16<11:17, 26.07s/it]2022-01-09 03:26:49,184 iteration 6359 : loss : 0.020580, loss_ce: 0.005442
2022-01-09 03:26:50,586 iteration 6360 : loss : 0.017183, loss_ce: 0.007467
2022-01-09 03:26:52,093 iteration 6361 : loss : 0.016471, loss_ce: 0.005037
2022-01-09 03:26:53,605 iteration 6362 : loss : 0.020219, loss_ce: 0.010123
2022-01-09 03:26:55,158 iteration 6363 : loss : 0.022445, loss_ce: 0.008061
2022-01-09 03:26:56,576 iteration 6364 : loss : 0.027002, loss_ce: 0.005406
2022-01-09 03:26:58,052 iteration 6365 : loss : 0.019061, loss_ce: 0.010346
2022-01-09 03:26:59,647 iteration 6366 : loss : 0.020740, loss_ce: 0.007922
2022-01-09 03:27:01,273 iteration 6367 : loss : 0.021735, loss_ce: 0.007962
2022-01-09 03:27:02,806 iteration 6368 : loss : 0.021381, loss_ce: 0.008582
2022-01-09 03:27:04,280 iteration 6369 : loss : 0.013153, loss_ce: 0.005891
2022-01-09 03:27:05,761 iteration 6370 : loss : 0.015580, loss_ce: 0.004955
2022-01-09 03:27:07,359 iteration 6371 : loss : 0.023109, loss_ce: 0.008235
2022-01-09 03:27:08,776 iteration 6372 : loss : 0.010210, loss_ce: 0.004042
2022-01-09 03:27:10,180 iteration 6373 : loss : 0.014497, loss_ce: 0.005670
2022-01-09 03:27:11,615 iteration 6374 : loss : 0.019228, loss_ce: 0.008160
2022-01-09 03:27:11,615 Training Data Eval:
2022-01-09 03:27:18,941   Average segmentation loss on training set: 0.0090
2022-01-09 03:27:18,941 Validation Data Eval:
2022-01-09 03:27:21,583   Average segmentation loss on validation set: 0.0705
2022-01-09 03:27:23,105 iteration 6375 : loss : 0.019909, loss_ce: 0.006868
 94%|███████████████████████████▏ | 375/400 [2:50:52<12:02, 28.90s/it]2022-01-09 03:27:24,702 iteration 6376 : loss : 0.019606, loss_ce: 0.005823
2022-01-09 03:27:26,194 iteration 6377 : loss : 0.013293, loss_ce: 0.004756
2022-01-09 03:27:27,538 iteration 6378 : loss : 0.013578, loss_ce: 0.006187
2022-01-09 03:27:28,991 iteration 6379 : loss : 0.016550, loss_ce: 0.004747
2022-01-09 03:27:30,409 iteration 6380 : loss : 0.014756, loss_ce: 0.005402
2022-01-09 03:27:31,815 iteration 6381 : loss : 0.010440, loss_ce: 0.003354
2022-01-09 03:27:33,157 iteration 6382 : loss : 0.013141, loss_ce: 0.004444
2022-01-09 03:27:34,677 iteration 6383 : loss : 0.022136, loss_ce: 0.007588
2022-01-09 03:27:36,105 iteration 6384 : loss : 0.017107, loss_ce: 0.008331
2022-01-09 03:27:37,615 iteration 6385 : loss : 0.029706, loss_ce: 0.010863
2022-01-09 03:27:39,147 iteration 6386 : loss : 0.017896, loss_ce: 0.005527
2022-01-09 03:27:40,743 iteration 6387 : loss : 0.020298, loss_ce: 0.009019
2022-01-09 03:27:42,265 iteration 6388 : loss : 0.016891, loss_ce: 0.006974
2022-01-09 03:27:43,595 iteration 6389 : loss : 0.012574, loss_ce: 0.004343
2022-01-09 03:27:45,067 iteration 6390 : loss : 0.016037, loss_ce: 0.007480
2022-01-09 03:27:46,480 iteration 6391 : loss : 0.015030, loss_ce: 0.008681
2022-01-09 03:27:48,041 iteration 6392 : loss : 0.015652, loss_ce: 0.004765
 94%|███████████████████████████▎ | 376/400 [2:51:16<11:05, 27.72s/it]2022-01-09 03:27:49,576 iteration 6393 : loss : 0.019878, loss_ce: 0.007877
2022-01-09 03:27:51,107 iteration 6394 : loss : 0.017332, loss_ce: 0.006576
2022-01-09 03:27:52,561 iteration 6395 : loss : 0.014076, loss_ce: 0.004091
2022-01-09 03:27:54,029 iteration 6396 : loss : 0.013155, loss_ce: 0.004696
2022-01-09 03:27:55,536 iteration 6397 : loss : 0.017523, loss_ce: 0.007325
2022-01-09 03:27:57,048 iteration 6398 : loss : 0.013694, loss_ce: 0.004970
2022-01-09 03:27:58,504 iteration 6399 : loss : 0.016504, loss_ce: 0.005137
2022-01-09 03:27:59,949 iteration 6400 : loss : 0.014702, loss_ce: 0.005944
2022-01-09 03:28:01,445 iteration 6401 : loss : 0.014383, loss_ce: 0.005886
2022-01-09 03:28:02,892 iteration 6402 : loss : 0.016084, loss_ce: 0.008783
2022-01-09 03:28:04,461 iteration 6403 : loss : 0.019163, loss_ce: 0.007707
2022-01-09 03:28:05,906 iteration 6404 : loss : 0.014016, loss_ce: 0.006138
2022-01-09 03:28:07,321 iteration 6405 : loss : 0.017510, loss_ce: 0.005130
2022-01-09 03:28:08,988 iteration 6406 : loss : 0.038830, loss_ce: 0.013282
2022-01-09 03:28:10,496 iteration 6407 : loss : 0.032935, loss_ce: 0.008681
2022-01-09 03:28:11,834 iteration 6408 : loss : 0.011661, loss_ce: 0.004217
2022-01-09 03:28:13,297 iteration 6409 : loss : 0.013595, loss_ce: 0.005906
 94%|███████████████████████████▎ | 377/400 [2:51:42<10:20, 26.98s/it]2022-01-09 03:28:14,800 iteration 6410 : loss : 0.013283, loss_ce: 0.005052
2022-01-09 03:28:16,265 iteration 6411 : loss : 0.020515, loss_ce: 0.009376
2022-01-09 03:28:17,767 iteration 6412 : loss : 0.014446, loss_ce: 0.005113
2022-01-09 03:28:19,215 iteration 6413 : loss : 0.017423, loss_ce: 0.008002
2022-01-09 03:28:20,638 iteration 6414 : loss : 0.014765, loss_ce: 0.006325
2022-01-09 03:28:22,039 iteration 6415 : loss : 0.010361, loss_ce: 0.003594
2022-01-09 03:28:23,515 iteration 6416 : loss : 0.011783, loss_ce: 0.004190
2022-01-09 03:28:24,920 iteration 6417 : loss : 0.017740, loss_ce: 0.008701
2022-01-09 03:28:26,360 iteration 6418 : loss : 0.012172, loss_ce: 0.005479
2022-01-09 03:28:27,948 iteration 6419 : loss : 0.017621, loss_ce: 0.007040
2022-01-09 03:28:29,502 iteration 6420 : loss : 0.015623, loss_ce: 0.003846
2022-01-09 03:28:31,023 iteration 6421 : loss : 0.012974, loss_ce: 0.004964
2022-01-09 03:28:32,505 iteration 6422 : loss : 0.018073, loss_ce: 0.005339
2022-01-09 03:28:34,012 iteration 6423 : loss : 0.012475, loss_ce: 0.005090
2022-01-09 03:28:35,614 iteration 6424 : loss : 0.033702, loss_ce: 0.018876
2022-01-09 03:28:37,069 iteration 6425 : loss : 0.013500, loss_ce: 0.004117
2022-01-09 03:28:38,628 iteration 6426 : loss : 0.022468, loss_ce: 0.007838
 94%|███████████████████████████▍ | 378/400 [2:52:07<09:42, 26.48s/it]2022-01-09 03:28:40,208 iteration 6427 : loss : 0.014765, loss_ce: 0.004783
2022-01-09 03:28:41,684 iteration 6428 : loss : 0.016421, loss_ce: 0.005007
2022-01-09 03:28:43,165 iteration 6429 : loss : 0.022101, loss_ce: 0.006989
2022-01-09 03:28:44,702 iteration 6430 : loss : 0.021364, loss_ce: 0.008339
2022-01-09 03:28:46,093 iteration 6431 : loss : 0.014314, loss_ce: 0.005543
2022-01-09 03:28:47,574 iteration 6432 : loss : 0.014690, loss_ce: 0.006143
2022-01-09 03:28:49,038 iteration 6433 : loss : 0.013126, loss_ce: 0.004406
2022-01-09 03:28:50,452 iteration 6434 : loss : 0.015896, loss_ce: 0.006324
2022-01-09 03:28:51,902 iteration 6435 : loss : 0.015750, loss_ce: 0.005851
2022-01-09 03:28:53,328 iteration 6436 : loss : 0.014683, loss_ce: 0.006874
2022-01-09 03:28:54,790 iteration 6437 : loss : 0.012203, loss_ce: 0.004001
2022-01-09 03:28:56,302 iteration 6438 : loss : 0.018284, loss_ce: 0.005365
2022-01-09 03:28:57,706 iteration 6439 : loss : 0.013663, loss_ce: 0.005967
2022-01-09 03:28:59,221 iteration 6440 : loss : 0.018330, loss_ce: 0.009233
2022-01-09 03:29:00,602 iteration 6441 : loss : 0.011123, loss_ce: 0.004663
2022-01-09 03:29:02,080 iteration 6442 : loss : 0.014147, loss_ce: 0.004456
2022-01-09 03:29:03,438 iteration 6443 : loss : 0.011119, loss_ce: 0.003673
 95%|███████████████████████████▍ | 379/400 [2:52:32<09:05, 25.98s/it]2022-01-09 03:29:04,980 iteration 6444 : loss : 0.013342, loss_ce: 0.006448
2022-01-09 03:29:06,448 iteration 6445 : loss : 0.013061, loss_ce: 0.005647
2022-01-09 03:29:07,928 iteration 6446 : loss : 0.018600, loss_ce: 0.007592
2022-01-09 03:29:09,435 iteration 6447 : loss : 0.023435, loss_ce: 0.004442
2022-01-09 03:29:10,956 iteration 6448 : loss : 0.032264, loss_ce: 0.013668
2022-01-09 03:29:12,391 iteration 6449 : loss : 0.019820, loss_ce: 0.004631
2022-01-09 03:29:13,788 iteration 6450 : loss : 0.016292, loss_ce: 0.007354
2022-01-09 03:29:15,254 iteration 6451 : loss : 0.018399, loss_ce: 0.007811
2022-01-09 03:29:16,812 iteration 6452 : loss : 0.018445, loss_ce: 0.006304
2022-01-09 03:29:18,254 iteration 6453 : loss : 0.017530, loss_ce: 0.005855
2022-01-09 03:29:19,635 iteration 6454 : loss : 0.016109, loss_ce: 0.005571
2022-01-09 03:29:21,157 iteration 6455 : loss : 0.018439, loss_ce: 0.008062
2022-01-09 03:29:22,607 iteration 6456 : loss : 0.015764, loss_ce: 0.005928
2022-01-09 03:29:24,101 iteration 6457 : loss : 0.016491, loss_ce: 0.005854
2022-01-09 03:29:25,515 iteration 6458 : loss : 0.013872, loss_ce: 0.005291
2022-01-09 03:29:26,949 iteration 6459 : loss : 0.009675, loss_ce: 0.002393
2022-01-09 03:29:26,949 Training Data Eval:
2022-01-09 03:29:34,294   Average segmentation loss on training set: 0.0086
2022-01-09 03:29:34,295 Validation Data Eval:
2022-01-09 03:29:36,856   Average segmentation loss on validation set: 0.0779
2022-01-09 03:29:38,388 iteration 6460 : loss : 0.020188, loss_ce: 0.011180
 95%|███████████████████████████▌ | 380/400 [2:53:07<09:33, 28.67s/it]2022-01-09 03:29:40,003 iteration 6461 : loss : 0.017456, loss_ce: 0.007144
2022-01-09 03:29:41,426 iteration 6462 : loss : 0.014213, loss_ce: 0.006076
2022-01-09 03:29:42,963 iteration 6463 : loss : 0.033211, loss_ce: 0.013612
2022-01-09 03:29:44,432 iteration 6464 : loss : 0.015785, loss_ce: 0.005115
2022-01-09 03:29:45,951 iteration 6465 : loss : 0.017631, loss_ce: 0.006820
2022-01-09 03:29:47,351 iteration 6466 : loss : 0.013155, loss_ce: 0.004223
2022-01-09 03:29:48,779 iteration 6467 : loss : 0.013347, loss_ce: 0.005129
2022-01-09 03:29:50,322 iteration 6468 : loss : 0.017337, loss_ce: 0.005884
2022-01-09 03:29:51,767 iteration 6469 : loss : 0.014424, loss_ce: 0.006278
2022-01-09 03:29:53,204 iteration 6470 : loss : 0.012564, loss_ce: 0.003555
2022-01-09 03:29:54,666 iteration 6471 : loss : 0.014737, loss_ce: 0.005454
2022-01-09 03:29:56,141 iteration 6472 : loss : 0.019482, loss_ce: 0.006018
2022-01-09 03:29:57,558 iteration 6473 : loss : 0.014585, loss_ce: 0.005131
2022-01-09 03:29:59,110 iteration 6474 : loss : 0.012411, loss_ce: 0.005621
2022-01-09 03:30:00,599 iteration 6475 : loss : 0.025492, loss_ce: 0.011961
2022-01-09 03:30:01,917 iteration 6476 : loss : 0.014152, loss_ce: 0.005250
2022-01-09 03:30:03,494 iteration 6477 : loss : 0.026749, loss_ce: 0.008555
 95%|███████████████████████████▌ | 381/400 [2:53:32<08:44, 27.60s/it]2022-01-09 03:30:05,129 iteration 6478 : loss : 0.018326, loss_ce: 0.005760
2022-01-09 03:30:06,571 iteration 6479 : loss : 0.015246, loss_ce: 0.005475
2022-01-09 03:30:08,015 iteration 6480 : loss : 0.015709, loss_ce: 0.007503
2022-01-09 03:30:09,513 iteration 6481 : loss : 0.017874, loss_ce: 0.008456
2022-01-09 03:30:10,881 iteration 6482 : loss : 0.013860, loss_ce: 0.004615
2022-01-09 03:30:12,386 iteration 6483 : loss : 0.014295, loss_ce: 0.003635
2022-01-09 03:30:13,794 iteration 6484 : loss : 0.014486, loss_ce: 0.006039
2022-01-09 03:30:15,236 iteration 6485 : loss : 0.016485, loss_ce: 0.006829
2022-01-09 03:30:16,762 iteration 6486 : loss : 0.013715, loss_ce: 0.004624
2022-01-09 03:30:18,374 iteration 6487 : loss : 0.019300, loss_ce: 0.008341
2022-01-09 03:30:19,886 iteration 6488 : loss : 0.019658, loss_ce: 0.006885
2022-01-09 03:30:21,305 iteration 6489 : loss : 0.012529, loss_ce: 0.006395
2022-01-09 03:30:22,789 iteration 6490 : loss : 0.016920, loss_ce: 0.004823
2022-01-09 03:30:24,219 iteration 6491 : loss : 0.012780, loss_ce: 0.004650
2022-01-09 03:30:25,798 iteration 6492 : loss : 0.017785, loss_ce: 0.006535
2022-01-09 03:30:27,298 iteration 6493 : loss : 0.030074, loss_ce: 0.011063
2022-01-09 03:30:28,626 iteration 6494 : loss : 0.009626, loss_ce: 0.003689
 96%|███████████████████████████▋ | 382/400 [2:53:57<08:03, 26.86s/it]2022-01-09 03:30:30,113 iteration 6495 : loss : 0.014215, loss_ce: 0.005479
2022-01-09 03:30:31,575 iteration 6496 : loss : 0.019785, loss_ce: 0.005372
2022-01-09 03:30:33,084 iteration 6497 : loss : 0.013608, loss_ce: 0.005301
2022-01-09 03:30:34,594 iteration 6498 : loss : 0.020067, loss_ce: 0.007170
2022-01-09 03:30:36,112 iteration 6499 : loss : 0.019023, loss_ce: 0.007925
2022-01-09 03:30:37,557 iteration 6500 : loss : 0.018068, loss_ce: 0.010559
2022-01-09 03:30:39,050 iteration 6501 : loss : 0.018602, loss_ce: 0.006833
2022-01-09 03:30:40,580 iteration 6502 : loss : 0.015399, loss_ce: 0.008320
2022-01-09 03:30:42,002 iteration 6503 : loss : 0.011071, loss_ce: 0.002970
2022-01-09 03:30:43,391 iteration 6504 : loss : 0.011533, loss_ce: 0.004276
2022-01-09 03:30:44,742 iteration 6505 : loss : 0.011853, loss_ce: 0.005740
2022-01-09 03:30:46,257 iteration 6506 : loss : 0.016457, loss_ce: 0.004619
2022-01-09 03:30:47,898 iteration 6507 : loss : 0.022477, loss_ce: 0.011734
2022-01-09 03:30:49,472 iteration 6508 : loss : 0.023080, loss_ce: 0.008233
2022-01-09 03:30:50,996 iteration 6509 : loss : 0.026082, loss_ce: 0.009846
2022-01-09 03:30:52,509 iteration 6510 : loss : 0.017775, loss_ce: 0.007963
2022-01-09 03:30:53,885 iteration 6511 : loss : 0.013358, loss_ce: 0.005167
 96%|███████████████████████████▊ | 383/400 [2:54:22<07:28, 26.38s/it]2022-01-09 03:30:55,375 iteration 6512 : loss : 0.016000, loss_ce: 0.005244
2022-01-09 03:30:56,845 iteration 6513 : loss : 0.029571, loss_ce: 0.010810
2022-01-09 03:30:58,447 iteration 6514 : loss : 0.017593, loss_ce: 0.005348
2022-01-09 03:30:59,995 iteration 6515 : loss : 0.017912, loss_ce: 0.006082
2022-01-09 03:31:01,501 iteration 6516 : loss : 0.018430, loss_ce: 0.007753
2022-01-09 03:31:02,954 iteration 6517 : loss : 0.017949, loss_ce: 0.003534
2022-01-09 03:31:04,422 iteration 6518 : loss : 0.013003, loss_ce: 0.004564
2022-01-09 03:31:05,870 iteration 6519 : loss : 0.016373, loss_ce: 0.005122
2022-01-09 03:31:07,400 iteration 6520 : loss : 0.012890, loss_ce: 0.004121
2022-01-09 03:31:08,822 iteration 6521 : loss : 0.019954, loss_ce: 0.010853
2022-01-09 03:31:10,308 iteration 6522 : loss : 0.012452, loss_ce: 0.005098
2022-01-09 03:31:11,679 iteration 6523 : loss : 0.010924, loss_ce: 0.004483
2022-01-09 03:31:13,207 iteration 6524 : loss : 0.019167, loss_ce: 0.008978
2022-01-09 03:31:14,689 iteration 6525 : loss : 0.015939, loss_ce: 0.006013
2022-01-09 03:31:16,157 iteration 6526 : loss : 0.014661, loss_ce: 0.005574
2022-01-09 03:31:17,524 iteration 6527 : loss : 0.013617, loss_ce: 0.006048
2022-01-09 03:31:18,932 iteration 6528 : loss : 0.016279, loss_ce: 0.004707
 96%|███████████████████████████▊ | 384/400 [2:54:47<06:55, 25.98s/it]2022-01-09 03:31:20,523 iteration 6529 : loss : 0.025706, loss_ce: 0.009244
2022-01-09 03:31:22,018 iteration 6530 : loss : 0.016031, loss_ce: 0.005656
2022-01-09 03:31:23,402 iteration 6531 : loss : 0.018248, loss_ce: 0.005891
2022-01-09 03:31:24,905 iteration 6532 : loss : 0.015502, loss_ce: 0.006226
2022-01-09 03:31:26,270 iteration 6533 : loss : 0.009987, loss_ce: 0.003911
2022-01-09 03:31:27,735 iteration 6534 : loss : 0.015266, loss_ce: 0.006755
2022-01-09 03:31:29,211 iteration 6535 : loss : 0.017412, loss_ce: 0.007180
2022-01-09 03:31:30,714 iteration 6536 : loss : 0.014315, loss_ce: 0.004802
2022-01-09 03:31:32,202 iteration 6537 : loss : 0.020864, loss_ce: 0.006368
2022-01-09 03:31:33,697 iteration 6538 : loss : 0.013830, loss_ce: 0.004369
2022-01-09 03:31:35,219 iteration 6539 : loss : 0.019105, loss_ce: 0.009666
2022-01-09 03:31:36,574 iteration 6540 : loss : 0.011545, loss_ce: 0.004911
2022-01-09 03:31:37,994 iteration 6541 : loss : 0.014821, loss_ce: 0.008315
2022-01-09 03:31:39,584 iteration 6542 : loss : 0.013263, loss_ce: 0.004134
2022-01-09 03:31:41,006 iteration 6543 : loss : 0.011417, loss_ce: 0.003684
2022-01-09 03:31:42,516 iteration 6544 : loss : 0.026182, loss_ce: 0.009438
2022-01-09 03:31:42,517 Training Data Eval:
2022-01-09 03:31:49,809   Average segmentation loss on training set: 0.0085
2022-01-09 03:31:49,809 Validation Data Eval:
2022-01-09 03:31:52,316   Average segmentation loss on validation set: 0.0795
2022-01-09 03:31:53,886 iteration 6545 : loss : 0.017429, loss_ce: 0.006933
 96%|███████████████████████████▉ | 385/400 [2:55:22<07:10, 28.67s/it]2022-01-09 03:31:55,620 iteration 6546 : loss : 0.016182, loss_ce: 0.006918
2022-01-09 03:31:57,082 iteration 6547 : loss : 0.025503, loss_ce: 0.011426
2022-01-09 03:31:58,587 iteration 6548 : loss : 0.013387, loss_ce: 0.005114
2022-01-09 03:32:00,007 iteration 6549 : loss : 0.015057, loss_ce: 0.005572
2022-01-09 03:32:01,530 iteration 6550 : loss : 0.019016, loss_ce: 0.005941
2022-01-09 03:32:03,082 iteration 6551 : loss : 0.025664, loss_ce: 0.009889
2022-01-09 03:32:04,689 iteration 6552 : loss : 0.020009, loss_ce: 0.007987
2022-01-09 03:32:06,340 iteration 6553 : loss : 0.024376, loss_ce: 0.008420
2022-01-09 03:32:07,878 iteration 6554 : loss : 0.019466, loss_ce: 0.005905
2022-01-09 03:32:09,383 iteration 6555 : loss : 0.013024, loss_ce: 0.005769
2022-01-09 03:32:10,875 iteration 6556 : loss : 0.021132, loss_ce: 0.005504
2022-01-09 03:32:12,257 iteration 6557 : loss : 0.009832, loss_ce: 0.004313
2022-01-09 03:32:13,723 iteration 6558 : loss : 0.016379, loss_ce: 0.005116
2022-01-09 03:32:15,092 iteration 6559 : loss : 0.011586, loss_ce: 0.004194
2022-01-09 03:32:16,560 iteration 6560 : loss : 0.015398, loss_ce: 0.007673
2022-01-09 03:32:18,073 iteration 6561 : loss : 0.016540, loss_ce: 0.004918
2022-01-09 03:32:19,481 iteration 6562 : loss : 0.012953, loss_ce: 0.004832
 96%|███████████████████████████▉ | 386/400 [2:55:48<06:28, 27.75s/it]2022-01-09 03:32:21,181 iteration 6563 : loss : 0.027391, loss_ce: 0.012508
2022-01-09 03:32:22,624 iteration 6564 : loss : 0.015213, loss_ce: 0.006737
2022-01-09 03:32:24,015 iteration 6565 : loss : 0.021542, loss_ce: 0.004816
2022-01-09 03:32:25,403 iteration 6566 : loss : 0.012918, loss_ce: 0.005359
2022-01-09 03:32:26,751 iteration 6567 : loss : 0.012719, loss_ce: 0.004960
2022-01-09 03:32:28,169 iteration 6568 : loss : 0.018595, loss_ce: 0.007313
2022-01-09 03:32:29,627 iteration 6569 : loss : 0.012023, loss_ce: 0.004938
2022-01-09 03:32:31,023 iteration 6570 : loss : 0.012078, loss_ce: 0.005708
2022-01-09 03:32:32,457 iteration 6571 : loss : 0.015201, loss_ce: 0.004776
2022-01-09 03:32:33,808 iteration 6572 : loss : 0.013923, loss_ce: 0.006245
2022-01-09 03:32:35,356 iteration 6573 : loss : 0.020279, loss_ce: 0.006759
2022-01-09 03:32:36,914 iteration 6574 : loss : 0.012521, loss_ce: 0.005327
2022-01-09 03:32:38,493 iteration 6575 : loss : 0.047805, loss_ce: 0.010377
2022-01-09 03:32:39,907 iteration 6576 : loss : 0.013457, loss_ce: 0.005657
2022-01-09 03:32:41,402 iteration 6577 : loss : 0.021411, loss_ce: 0.005314
2022-01-09 03:32:42,803 iteration 6578 : loss : 0.014557, loss_ce: 0.006197
2022-01-09 03:32:44,274 iteration 6579 : loss : 0.014247, loss_ce: 0.004916
 97%|████████████████████████████ | 387/400 [2:56:13<05:49, 26.86s/it]2022-01-09 03:32:45,817 iteration 6580 : loss : 0.020133, loss_ce: 0.005916
2022-01-09 03:32:47,247 iteration 6581 : loss : 0.017001, loss_ce: 0.005638
2022-01-09 03:32:48,805 iteration 6582 : loss : 0.016924, loss_ce: 0.005492
2022-01-09 03:32:50,281 iteration 6583 : loss : 0.017089, loss_ce: 0.006170
2022-01-09 03:32:51,774 iteration 6584 : loss : 0.016616, loss_ce: 0.006863
2022-01-09 03:32:53,314 iteration 6585 : loss : 0.024520, loss_ce: 0.008546
2022-01-09 03:32:54,819 iteration 6586 : loss : 0.018812, loss_ce: 0.005972
2022-01-09 03:32:56,268 iteration 6587 : loss : 0.017479, loss_ce: 0.007041
2022-01-09 03:32:57,748 iteration 6588 : loss : 0.017392, loss_ce: 0.006953
2022-01-09 03:32:59,129 iteration 6589 : loss : 0.019202, loss_ce: 0.006767
2022-01-09 03:33:00,654 iteration 6590 : loss : 0.018268, loss_ce: 0.008034
2022-01-09 03:33:02,198 iteration 6591 : loss : 0.018109, loss_ce: 0.007005
2022-01-09 03:33:03,623 iteration 6592 : loss : 0.012563, loss_ce: 0.004808
2022-01-09 03:33:05,182 iteration 6593 : loss : 0.033330, loss_ce: 0.006746
2022-01-09 03:33:06,598 iteration 6594 : loss : 0.017728, loss_ce: 0.009854
2022-01-09 03:33:08,028 iteration 6595 : loss : 0.012135, loss_ce: 0.004053
2022-01-09 03:33:09,561 iteration 6596 : loss : 0.018721, loss_ce: 0.007090
 97%|████████████████████████████▏| 388/400 [2:56:38<05:16, 26.39s/it]2022-01-09 03:33:11,089 iteration 6597 : loss : 0.015441, loss_ce: 0.006950
2022-01-09 03:33:12,483 iteration 6598 : loss : 0.033019, loss_ce: 0.004996
2022-01-09 03:33:13,974 iteration 6599 : loss : 0.014708, loss_ce: 0.005493
2022-01-09 03:33:15,290 iteration 6600 : loss : 0.010597, loss_ce: 0.004357
2022-01-09 03:33:16,734 iteration 6601 : loss : 0.013169, loss_ce: 0.004489
2022-01-09 03:33:18,139 iteration 6602 : loss : 0.013587, loss_ce: 0.004711
2022-01-09 03:33:19,594 iteration 6603 : loss : 0.025846, loss_ce: 0.009822
2022-01-09 03:33:20,947 iteration 6604 : loss : 0.009807, loss_ce: 0.004079
2022-01-09 03:33:22,412 iteration 6605 : loss : 0.014148, loss_ce: 0.004757
2022-01-09 03:33:23,913 iteration 6606 : loss : 0.016649, loss_ce: 0.007406
2022-01-09 03:33:25,413 iteration 6607 : loss : 0.016246, loss_ce: 0.006928
2022-01-09 03:33:26,919 iteration 6608 : loss : 0.013579, loss_ce: 0.005730
2022-01-09 03:33:28,318 iteration 6609 : loss : 0.015209, loss_ce: 0.005546
2022-01-09 03:33:29,745 iteration 6610 : loss : 0.021807, loss_ce: 0.008927
2022-01-09 03:33:31,164 iteration 6611 : loss : 0.013081, loss_ce: 0.002957
2022-01-09 03:33:32,683 iteration 6612 : loss : 0.016004, loss_ce: 0.006204
2022-01-09 03:33:33,999 iteration 6613 : loss : 0.012362, loss_ce: 0.004861
 97%|████████████████████████████▏| 389/400 [2:57:02<04:43, 25.80s/it]2022-01-09 03:33:35,531 iteration 6614 : loss : 0.018016, loss_ce: 0.007845
2022-01-09 03:33:37,010 iteration 6615 : loss : 0.015832, loss_ce: 0.006783
2022-01-09 03:33:38,453 iteration 6616 : loss : 0.016822, loss_ce: 0.005541
2022-01-09 03:33:40,098 iteration 6617 : loss : 0.018501, loss_ce: 0.006690
2022-01-09 03:33:41,605 iteration 6618 : loss : 0.013843, loss_ce: 0.004591
2022-01-09 03:33:43,201 iteration 6619 : loss : 0.020685, loss_ce: 0.008490
2022-01-09 03:33:44,736 iteration 6620 : loss : 0.026620, loss_ce: 0.010870
2022-01-09 03:33:46,204 iteration 6621 : loss : 0.022875, loss_ce: 0.008191
2022-01-09 03:33:47,673 iteration 6622 : loss : 0.017295, loss_ce: 0.008731
2022-01-09 03:33:49,150 iteration 6623 : loss : 0.017975, loss_ce: 0.006189
2022-01-09 03:33:50,680 iteration 6624 : loss : 0.032462, loss_ce: 0.013789
2022-01-09 03:33:52,227 iteration 6625 : loss : 0.012101, loss_ce: 0.004290
2022-01-09 03:33:53,686 iteration 6626 : loss : 0.017496, loss_ce: 0.004449
2022-01-09 03:33:55,037 iteration 6627 : loss : 0.014044, loss_ce: 0.004564
2022-01-09 03:33:56,491 iteration 6628 : loss : 0.014344, loss_ce: 0.006441
2022-01-09 03:33:57,936 iteration 6629 : loss : 0.015459, loss_ce: 0.007859
2022-01-09 03:33:57,936 Training Data Eval:
2022-01-09 03:34:05,259   Average segmentation loss on training set: 0.0087
2022-01-09 03:34:05,260 Validation Data Eval:
2022-01-09 03:34:07,801   Average segmentation loss on validation set: 0.0766
2022-01-09 03:34:09,333 iteration 6630 : loss : 0.030229, loss_ce: 0.009693
 98%|████████████████████████████▎| 390/400 [2:57:38<04:46, 28.66s/it]2022-01-09 03:34:10,970 iteration 6631 : loss : 0.019187, loss_ce: 0.006888
2022-01-09 03:34:12,463 iteration 6632 : loss : 0.016938, loss_ce: 0.007047
2022-01-09 03:34:13,946 iteration 6633 : loss : 0.021490, loss_ce: 0.006155
2022-01-09 03:34:15,271 iteration 6634 : loss : 0.013296, loss_ce: 0.004197
2022-01-09 03:34:16,709 iteration 6635 : loss : 0.016469, loss_ce: 0.004544
2022-01-09 03:34:18,269 iteration 6636 : loss : 0.016831, loss_ce: 0.007300
2022-01-09 03:34:19,798 iteration 6637 : loss : 0.011548, loss_ce: 0.005333
2022-01-09 03:34:21,345 iteration 6638 : loss : 0.017960, loss_ce: 0.006886
2022-01-09 03:34:22,939 iteration 6639 : loss : 0.021230, loss_ce: 0.006694
2022-01-09 03:34:24,419 iteration 6640 : loss : 0.025511, loss_ce: 0.006832
2022-01-09 03:34:25,855 iteration 6641 : loss : 0.013741, loss_ce: 0.007149
2022-01-09 03:34:27,432 iteration 6642 : loss : 0.014985, loss_ce: 0.006359
2022-01-09 03:34:28,938 iteration 6643 : loss : 0.023398, loss_ce: 0.009335
2022-01-09 03:34:30,413 iteration 6644 : loss : 0.020057, loss_ce: 0.007235
2022-01-09 03:34:31,806 iteration 6645 : loss : 0.010382, loss_ce: 0.004547
2022-01-09 03:34:33,241 iteration 6646 : loss : 0.018796, loss_ce: 0.011450
2022-01-09 03:34:34,582 iteration 6647 : loss : 0.012604, loss_ce: 0.004207
 98%|████████████████████████████▎| 391/400 [2:58:03<04:08, 27.64s/it]2022-01-09 03:34:36,059 iteration 6648 : loss : 0.016955, loss_ce: 0.005315
2022-01-09 03:34:37,600 iteration 6649 : loss : 0.018400, loss_ce: 0.007659
2022-01-09 03:34:39,008 iteration 6650 : loss : 0.021624, loss_ce: 0.007817
2022-01-09 03:34:40,419 iteration 6651 : loss : 0.011244, loss_ce: 0.003929
2022-01-09 03:34:41,982 iteration 6652 : loss : 0.019560, loss_ce: 0.007004
2022-01-09 03:34:43,521 iteration 6653 : loss : 0.025077, loss_ce: 0.012053
2022-01-09 03:34:44,932 iteration 6654 : loss : 0.012214, loss_ce: 0.004503
2022-01-09 03:34:46,425 iteration 6655 : loss : 0.013331, loss_ce: 0.005183
2022-01-09 03:34:47,932 iteration 6656 : loss : 0.012483, loss_ce: 0.004973
2022-01-09 03:34:49,524 iteration 6657 : loss : 0.018421, loss_ce: 0.007080
2022-01-09 03:34:51,015 iteration 6658 : loss : 0.029414, loss_ce: 0.010669
2022-01-09 03:34:52,551 iteration 6659 : loss : 0.018014, loss_ce: 0.006856
2022-01-09 03:34:53,988 iteration 6660 : loss : 0.020619, loss_ce: 0.006764
2022-01-09 03:34:55,348 iteration 6661 : loss : 0.011098, loss_ce: 0.003987
2022-01-09 03:34:56,827 iteration 6662 : loss : 0.012499, loss_ce: 0.004284
2022-01-09 03:34:58,366 iteration 6663 : loss : 0.015962, loss_ce: 0.005894
2022-01-09 03:34:59,821 iteration 6664 : loss : 0.013657, loss_ce: 0.004435
 98%|████████████████████████████▍| 392/400 [2:58:28<03:35, 26.92s/it]2022-01-09 03:35:01,386 iteration 6665 : loss : 0.024150, loss_ce: 0.008075
2022-01-09 03:35:02,877 iteration 6666 : loss : 0.020303, loss_ce: 0.008008
2022-01-09 03:35:04,302 iteration 6667 : loss : 0.015412, loss_ce: 0.005767
2022-01-09 03:35:05,630 iteration 6668 : loss : 0.009846, loss_ce: 0.004083
2022-01-09 03:35:07,109 iteration 6669 : loss : 0.014396, loss_ce: 0.004722
2022-01-09 03:35:08,486 iteration 6670 : loss : 0.021318, loss_ce: 0.007253
2022-01-09 03:35:10,040 iteration 6671 : loss : 0.013719, loss_ce: 0.005510
2022-01-09 03:35:11,505 iteration 6672 : loss : 0.017364, loss_ce: 0.009505
2022-01-09 03:35:13,070 iteration 6673 : loss : 0.019621, loss_ce: 0.008806
2022-01-09 03:35:14,617 iteration 6674 : loss : 0.018152, loss_ce: 0.007027
2022-01-09 03:35:16,045 iteration 6675 : loss : 0.017197, loss_ce: 0.004948
2022-01-09 03:35:17,411 iteration 6676 : loss : 0.010709, loss_ce: 0.004093
2022-01-09 03:35:18,891 iteration 6677 : loss : 0.012278, loss_ce: 0.002897
2022-01-09 03:35:20,485 iteration 6678 : loss : 0.015101, loss_ce: 0.006897
2022-01-09 03:35:21,984 iteration 6679 : loss : 0.015166, loss_ce: 0.005039
2022-01-09 03:35:23,422 iteration 6680 : loss : 0.020175, loss_ce: 0.008104
2022-01-09 03:35:24,817 iteration 6681 : loss : 0.015010, loss_ce: 0.005604
 98%|████████████████████████████▍| 393/400 [2:58:53<03:04, 26.34s/it]2022-01-09 03:35:26,348 iteration 6682 : loss : 0.014422, loss_ce: 0.004579
2022-01-09 03:35:27,868 iteration 6683 : loss : 0.016351, loss_ce: 0.005721
2022-01-09 03:35:29,368 iteration 6684 : loss : 0.030906, loss_ce: 0.003860
2022-01-09 03:35:30,813 iteration 6685 : loss : 0.017985, loss_ce: 0.006199
2022-01-09 03:35:32,301 iteration 6686 : loss : 0.015251, loss_ce: 0.004746
2022-01-09 03:35:33,783 iteration 6687 : loss : 0.018864, loss_ce: 0.007466
2022-01-09 03:35:35,274 iteration 6688 : loss : 0.018852, loss_ce: 0.006055
2022-01-09 03:35:36,697 iteration 6689 : loss : 0.013791, loss_ce: 0.005359
2022-01-09 03:35:38,249 iteration 6690 : loss : 0.017188, loss_ce: 0.005713
2022-01-09 03:35:39,639 iteration 6691 : loss : 0.013829, loss_ce: 0.005331
2022-01-09 03:35:41,029 iteration 6692 : loss : 0.015734, loss_ce: 0.010652
2022-01-09 03:35:42,512 iteration 6693 : loss : 0.014054, loss_ce: 0.005409
2022-01-09 03:35:43,880 iteration 6694 : loss : 0.011392, loss_ce: 0.004986
2022-01-09 03:35:45,307 iteration 6695 : loss : 0.011937, loss_ce: 0.003442
2022-01-09 03:35:46,749 iteration 6696 : loss : 0.013848, loss_ce: 0.006971
2022-01-09 03:35:48,205 iteration 6697 : loss : 0.018405, loss_ce: 0.006678
2022-01-09 03:35:49,560 iteration 6698 : loss : 0.014249, loss_ce: 0.004769
 98%|████████████████████████████▌| 394/400 [2:59:18<02:35, 25.86s/it]2022-01-09 03:35:51,112 iteration 6699 : loss : 0.014426, loss_ce: 0.003897
2022-01-09 03:35:52,648 iteration 6700 : loss : 0.017210, loss_ce: 0.007900
2022-01-09 03:35:54,227 iteration 6701 : loss : 0.038238, loss_ce: 0.018383
2022-01-09 03:35:55,639 iteration 6702 : loss : 0.013709, loss_ce: 0.004083
2022-01-09 03:35:57,213 iteration 6703 : loss : 0.016714, loss_ce: 0.006438
2022-01-09 03:35:58,697 iteration 6704 : loss : 0.015190, loss_ce: 0.008084
2022-01-09 03:36:00,085 iteration 6705 : loss : 0.012302, loss_ce: 0.003935
2022-01-09 03:36:01,506 iteration 6706 : loss : 0.014361, loss_ce: 0.005308
2022-01-09 03:36:02,949 iteration 6707 : loss : 0.017162, loss_ce: 0.006416
2022-01-09 03:36:04,298 iteration 6708 : loss : 0.011745, loss_ce: 0.003591
2022-01-09 03:36:05,691 iteration 6709 : loss : 0.017531, loss_ce: 0.004765
2022-01-09 03:36:07,164 iteration 6710 : loss : 0.018152, loss_ce: 0.006723
2022-01-09 03:36:08,617 iteration 6711 : loss : 0.016261, loss_ce: 0.007287
2022-01-09 03:36:10,119 iteration 6712 : loss : 0.020371, loss_ce: 0.007895
2022-01-09 03:36:11,534 iteration 6713 : loss : 0.016750, loss_ce: 0.005937
2022-01-09 03:36:12,973 iteration 6714 : loss : 0.022854, loss_ce: 0.009098
2022-01-09 03:36:12,974 Training Data Eval:
2022-01-09 03:36:20,383   Average segmentation loss on training set: 0.0086
2022-01-09 03:36:20,384 Validation Data Eval:
2022-01-09 03:36:22,943   Average segmentation loss on validation set: 0.0732
2022-01-09 03:36:24,364 iteration 6715 : loss : 0.009917, loss_ce: 0.003042
 99%|████████████████████████████▋| 395/400 [2:59:53<02:22, 28.54s/it]2022-01-09 03:36:25,796 iteration 6716 : loss : 0.011831, loss_ce: 0.003856
2022-01-09 03:36:27,304 iteration 6717 : loss : 0.017104, loss_ce: 0.004993
2022-01-09 03:36:28,861 iteration 6718 : loss : 0.023195, loss_ce: 0.010789
2022-01-09 03:36:30,394 iteration 6719 : loss : 0.023175, loss_ce: 0.008243
2022-01-09 03:36:31,890 iteration 6720 : loss : 0.033121, loss_ce: 0.005794
2022-01-09 03:36:33,401 iteration 6721 : loss : 0.020117, loss_ce: 0.009668
2022-01-09 03:36:34,836 iteration 6722 : loss : 0.018569, loss_ce: 0.007329
2022-01-09 03:36:36,325 iteration 6723 : loss : 0.017129, loss_ce: 0.006236
2022-01-09 03:36:37,737 iteration 6724 : loss : 0.014204, loss_ce: 0.006127
2022-01-09 03:36:39,201 iteration 6725 : loss : 0.027328, loss_ce: 0.011862
2022-01-09 03:36:40,563 iteration 6726 : loss : 0.010732, loss_ce: 0.003764
2022-01-09 03:36:42,153 iteration 6727 : loss : 0.020698, loss_ce: 0.007070
2022-01-09 03:36:43,557 iteration 6728 : loss : 0.015013, loss_ce: 0.005463
2022-01-09 03:36:45,086 iteration 6729 : loss : 0.016257, loss_ce: 0.005260
2022-01-09 03:36:46,681 iteration 6730 : loss : 0.029083, loss_ce: 0.010717
2022-01-09 03:36:48,203 iteration 6731 : loss : 0.023119, loss_ce: 0.009469
2022-01-09 03:36:49,658 iteration 6732 : loss : 0.011649, loss_ce: 0.005259
 99%|████████████████████████████▋| 396/400 [3:00:18<01:50, 27.57s/it]2022-01-09 03:36:51,120 iteration 6733 : loss : 0.008926, loss_ce: 0.003701
2022-01-09 03:36:52,562 iteration 6734 : loss : 0.014073, loss_ce: 0.003806
2022-01-09 03:36:54,072 iteration 6735 : loss : 0.017733, loss_ce: 0.006903
2022-01-09 03:36:55,513 iteration 6736 : loss : 0.013141, loss_ce: 0.006369
2022-01-09 03:36:56,899 iteration 6737 : loss : 0.012618, loss_ce: 0.006425
2022-01-09 03:36:58,270 iteration 6738 : loss : 0.013077, loss_ce: 0.004400
2022-01-09 03:36:59,728 iteration 6739 : loss : 0.018246, loss_ce: 0.007993
2022-01-09 03:37:01,225 iteration 6740 : loss : 0.018722, loss_ce: 0.006602
2022-01-09 03:37:02,711 iteration 6741 : loss : 0.020007, loss_ce: 0.008026
2022-01-09 03:37:04,132 iteration 6742 : loss : 0.015273, loss_ce: 0.007458
2022-01-09 03:37:05,511 iteration 6743 : loss : 0.013011, loss_ce: 0.003092
2022-01-09 03:37:07,027 iteration 6744 : loss : 0.030859, loss_ce: 0.014379
2022-01-09 03:37:08,479 iteration 6745 : loss : 0.012709, loss_ce: 0.005709
2022-01-09 03:37:09,842 iteration 6746 : loss : 0.011186, loss_ce: 0.003290
2022-01-09 03:37:11,239 iteration 6747 : loss : 0.010793, loss_ce: 0.003431
2022-01-09 03:37:12,738 iteration 6748 : loss : 0.011310, loss_ce: 0.005085
2022-01-09 03:37:14,209 iteration 6749 : loss : 0.013200, loss_ce: 0.003786
 99%|████████████████████████████▊| 397/400 [3:00:43<01:19, 26.66s/it]2022-01-09 03:37:15,757 iteration 6750 : loss : 0.010957, loss_ce: 0.004846
2022-01-09 03:37:17,318 iteration 6751 : loss : 0.012108, loss_ce: 0.004425
2022-01-09 03:37:18,702 iteration 6752 : loss : 0.017266, loss_ce: 0.005344
2022-01-09 03:37:20,174 iteration 6753 : loss : 0.015369, loss_ce: 0.006067
2022-01-09 03:37:21,676 iteration 6754 : loss : 0.014243, loss_ce: 0.004814
2022-01-09 03:37:23,087 iteration 6755 : loss : 0.023944, loss_ce: 0.008948
2022-01-09 03:37:24,509 iteration 6756 : loss : 0.019325, loss_ce: 0.007339
2022-01-09 03:37:26,014 iteration 6757 : loss : 0.013496, loss_ce: 0.004346
2022-01-09 03:37:27,431 iteration 6758 : loss : 0.020849, loss_ce: 0.011422
2022-01-09 03:37:28,915 iteration 6759 : loss : 0.013000, loss_ce: 0.005939
2022-01-09 03:37:30,482 iteration 6760 : loss : 0.022452, loss_ce: 0.004107
2022-01-09 03:37:31,953 iteration 6761 : loss : 0.015264, loss_ce: 0.006425
2022-01-09 03:37:33,414 iteration 6762 : loss : 0.019045, loss_ce: 0.005817
2022-01-09 03:37:34,969 iteration 6763 : loss : 0.016649, loss_ce: 0.006632
2022-01-09 03:37:36,402 iteration 6764 : loss : 0.017507, loss_ce: 0.007245
2022-01-09 03:37:37,969 iteration 6765 : loss : 0.016646, loss_ce: 0.006554
2022-01-09 03:37:39,422 iteration 6766 : loss : 0.011926, loss_ce: 0.005419
100%|████████████████████████████▊| 398/400 [3:01:08<00:52, 26.23s/it]2022-01-09 03:37:40,966 iteration 6767 : loss : 0.013754, loss_ce: 0.004920
2022-01-09 03:37:42,563 iteration 6768 : loss : 0.019954, loss_ce: 0.007814
2022-01-09 03:37:44,072 iteration 6769 : loss : 0.015162, loss_ce: 0.006775
2022-01-09 03:37:45,511 iteration 6770 : loss : 0.017603, loss_ce: 0.009085
2022-01-09 03:37:47,108 iteration 6771 : loss : 0.023697, loss_ce: 0.004977
2022-01-09 03:37:48,528 iteration 6772 : loss : 0.012939, loss_ce: 0.005045
2022-01-09 03:37:49,823 iteration 6773 : loss : 0.011820, loss_ce: 0.003632
2022-01-09 03:37:51,311 iteration 6774 : loss : 0.026946, loss_ce: 0.009220
2022-01-09 03:37:52,809 iteration 6775 : loss : 0.018926, loss_ce: 0.007370
2022-01-09 03:37:54,260 iteration 6776 : loss : 0.015544, loss_ce: 0.004955
2022-01-09 03:37:55,821 iteration 6777 : loss : 0.014601, loss_ce: 0.006980
2022-01-09 03:37:57,269 iteration 6778 : loss : 0.013469, loss_ce: 0.004286
2022-01-09 03:37:58,847 iteration 6779 : loss : 0.019353, loss_ce: 0.008091
2022-01-09 03:38:00,327 iteration 6780 : loss : 0.014798, loss_ce: 0.006892
2022-01-09 03:38:01,855 iteration 6781 : loss : 0.016365, loss_ce: 0.007309
2022-01-09 03:38:03,385 iteration 6782 : loss : 0.019938, loss_ce: 0.007340
2022-01-09 03:38:04,814 iteration 6783 : loss : 0.014900, loss_ce: 0.005099
100%|████████████████████████████▉| 399/400 [3:01:33<00:25, 25.98s/it]2022-01-09 03:38:06,437 iteration 6784 : loss : 0.015072, loss_ce: 0.005051
2022-01-09 03:38:07,936 iteration 6785 : loss : 0.013937, loss_ce: 0.004403
2022-01-09 03:38:09,343 iteration 6786 : loss : 0.011497, loss_ce: 0.004423
2022-01-09 03:38:10,751 iteration 6787 : loss : 0.011488, loss_ce: 0.003870
2022-01-09 03:38:12,231 iteration 6788 : loss : 0.016694, loss_ce: 0.006330
2022-01-09 03:38:13,696 iteration 6789 : loss : 0.020203, loss_ce: 0.005557
2022-01-09 03:38:15,122 iteration 6790 : loss : 0.010481, loss_ce: 0.004520
2022-01-09 03:38:16,616 iteration 6791 : loss : 0.013065, loss_ce: 0.004772
2022-01-09 03:38:18,002 iteration 6792 : loss : 0.013625, loss_ce: 0.006194
2022-01-09 03:38:19,553 iteration 6793 : loss : 0.027552, loss_ce: 0.008386
2022-01-09 03:38:21,015 iteration 6794 : loss : 0.022199, loss_ce: 0.007402
2022-01-09 03:38:22,525 iteration 6795 : loss : 0.019867, loss_ce: 0.007862
2022-01-09 03:38:24,067 iteration 6796 : loss : 0.022927, loss_ce: 0.011994
2022-01-09 03:38:25,420 iteration 6797 : loss : 0.014972, loss_ce: 0.004446
2022-01-09 03:38:26,929 iteration 6798 : loss : 0.020666, loss_ce: 0.006437
2022-01-09 03:38:28,517 iteration 6799 : loss : 0.017052, loss_ce: 0.007464
2022-01-09 03:38:28,517 Training Data Eval:
2022-01-09 03:38:35,809   Average segmentation loss on training set: 0.0084
2022-01-09 03:38:35,809 Validation Data Eval:
2022-01-09 03:38:38,307   Average segmentation loss on validation set: 0.0685
2022-01-09 03:38:39,749 iteration 6800 : loss : 0.011804, loss_ce: 0.002919
100%|█████████████████████████████| 400/400 [3:02:08<00:00, 28.67s/it]100%|█████████████████████████████| 400/400 [3:02:08<00:00, 27.32s/it]
