2022-01-09 11:27:22,273 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-09 11:27:22,274 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-09 11:27:22,274 ============================================================
2022-01-09 11:27:22,274 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-09 11:27:22,274 ============================================================
2022-01-09 11:27:22,274 Loading data...
2022-01-09 11:27:22,274 Reading NCI - RUNMC images...
2022-01-09 11:27:22,274 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-09 11:27:22,277 Already preprocessed this configuration. Loading now!
2022-01-09 11:27:22,300 Training Images: (256, 256, 286)
2022-01-09 11:27:22,300 Training Labels: (256, 256, 286)
2022-01-09 11:27:22,300 Validation Images: (256, 256, 98)
2022-01-09 11:27:22,300 Validation Labels: (256, 256, 98)
2022-01-09 11:27:22,300 ============================================================
2022-01-09 11:27:22,355 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-09 11:27:25,161 iteration 1 : loss : 0.984521, loss_ce: 1.214093
2022-01-09 11:27:26,487 iteration 2 : loss : 0.919468, loss_ce: 1.108339
2022-01-09 11:27:27,917 iteration 3 : loss : 0.859070, loss_ce: 1.003773
2022-01-09 11:27:29,262 iteration 4 : loss : 0.799864, loss_ce: 0.904421
2022-01-09 11:27:30,603 iteration 5 : loss : 0.749924, loss_ce: 0.834562
2022-01-09 11:27:31,962 iteration 6 : loss : 0.728601, loss_ce: 0.785675
2022-01-09 11:27:33,368 iteration 7 : loss : 0.679076, loss_ce: 0.722630
2022-01-09 11:27:34,730 iteration 8 : loss : 0.665942, loss_ce: 0.670849
2022-01-09 11:27:36,112 iteration 9 : loss : 0.611735, loss_ce: 0.639864
2022-01-09 11:27:37,520 iteration 10 : loss : 0.609625, loss_ce: 0.579692
2022-01-09 11:27:38,982 iteration 11 : loss : 0.568252, loss_ce: 0.547543
2022-01-09 11:27:40,335 iteration 12 : loss : 0.538516, loss_ce: 0.499334
2022-01-09 11:27:41,671 iteration 13 : loss : 0.533734, loss_ce: 0.473486
2022-01-09 11:27:42,982 iteration 14 : loss : 0.501237, loss_ce: 0.435574
2022-01-09 11:27:44,365 iteration 15 : loss : 0.470698, loss_ce: 0.397402
2022-01-09 11:27:45,726 iteration 16 : loss : 0.484728, loss_ce: 0.383352
2022-01-09 11:27:47,083 iteration 17 : loss : 0.417948, loss_ce: 0.340227
  0%|                               | 1/400 [00:24<2:45:02, 24.82s/it]2022-01-09 11:27:48,561 iteration 18 : loss : 0.440760, loss_ce: 0.309940
2022-01-09 11:27:49,855 iteration 19 : loss : 0.374700, loss_ce: 0.274611
2022-01-09 11:27:51,279 iteration 20 : loss : 0.368481, loss_ce: 0.252656
2022-01-09 11:27:52,613 iteration 21 : loss : 0.381841, loss_ce: 0.229036
2022-01-09 11:27:53,969 iteration 22 : loss : 0.359681, loss_ce: 0.241432
2022-01-09 11:27:55,401 iteration 23 : loss : 0.358051, loss_ce: 0.212255
2022-01-09 11:27:56,766 iteration 24 : loss : 0.338948, loss_ce: 0.210180
2022-01-09 11:27:58,182 iteration 25 : loss : 0.358729, loss_ce: 0.240532
2022-01-09 11:27:59,523 iteration 26 : loss : 0.313349, loss_ce: 0.182481
2022-01-09 11:28:00,815 iteration 27 : loss : 0.316223, loss_ce: 0.186867
2022-01-09 11:28:02,119 iteration 28 : loss : 0.283384, loss_ce: 0.155601
2022-01-09 11:28:03,517 iteration 29 : loss : 0.290428, loss_ce: 0.151293
2022-01-09 11:28:04,910 iteration 30 : loss : 0.276616, loss_ce: 0.141581
2022-01-09 11:28:06,218 iteration 31 : loss : 0.275206, loss_ce: 0.144672
2022-01-09 11:28:07,632 iteration 32 : loss : 0.296885, loss_ce: 0.170085
2022-01-09 11:28:09,031 iteration 33 : loss : 0.284477, loss_ce: 0.166126
2022-01-09 11:28:10,431 iteration 34 : loss : 0.285197, loss_ce: 0.171287
  0%|▏                              | 2/400 [00:48<2:38:47, 23.94s/it]2022-01-09 11:28:11,894 iteration 35 : loss : 0.260459, loss_ce: 0.127762
2022-01-09 11:28:13,308 iteration 36 : loss : 0.263689, loss_ce: 0.137644
2022-01-09 11:28:14,728 iteration 37 : loss : 0.274319, loss_ce: 0.118837
2022-01-09 11:28:16,070 iteration 38 : loss : 0.272851, loss_ce: 0.131532
2022-01-09 11:28:17,420 iteration 39 : loss : 0.213821, loss_ce: 0.098883
2022-01-09 11:28:18,844 iteration 40 : loss : 0.270282, loss_ce: 0.136006
2022-01-09 11:28:20,257 iteration 41 : loss : 0.317954, loss_ce: 0.162178
2022-01-09 11:28:21,646 iteration 42 : loss : 0.250136, loss_ce: 0.122308
2022-01-09 11:28:22,947 iteration 43 : loss : 0.258771, loss_ce: 0.123718
2022-01-09 11:28:24,389 iteration 44 : loss : 0.246181, loss_ce: 0.120211
2022-01-09 11:28:25,760 iteration 45 : loss : 0.222399, loss_ce: 0.098991
2022-01-09 11:28:27,145 iteration 46 : loss : 0.262522, loss_ce: 0.104654
2022-01-09 11:28:28,570 iteration 47 : loss : 0.206919, loss_ce: 0.082906
2022-01-09 11:28:29,965 iteration 48 : loss : 0.208827, loss_ce: 0.090585
2022-01-09 11:28:31,399 iteration 49 : loss : 0.284736, loss_ce: 0.132100
2022-01-09 11:28:32,735 iteration 50 : loss : 0.330996, loss_ce: 0.153483
2022-01-09 11:28:34,055 iteration 51 : loss : 0.271561, loss_ce: 0.132652
  1%|▏                              | 3/400 [01:11<2:37:25, 23.79s/it]2022-01-09 11:28:35,539 iteration 52 : loss : 0.294088, loss_ce: 0.149782
2022-01-09 11:28:36,944 iteration 53 : loss : 0.234087, loss_ce: 0.103413
2022-01-09 11:28:38,325 iteration 54 : loss : 0.243999, loss_ce: 0.101388
2022-01-09 11:28:39,725 iteration 55 : loss : 0.281952, loss_ce: 0.143946
2022-01-09 11:28:41,096 iteration 56 : loss : 0.246658, loss_ce: 0.103641
2022-01-09 11:28:42,492 iteration 57 : loss : 0.218820, loss_ce: 0.089043
2022-01-09 11:28:43,894 iteration 58 : loss : 0.283527, loss_ce: 0.118205
2022-01-09 11:28:45,255 iteration 59 : loss : 0.227120, loss_ce: 0.107847
2022-01-09 11:28:46,651 iteration 60 : loss : 0.253546, loss_ce: 0.108062
2022-01-09 11:28:48,037 iteration 61 : loss : 0.248049, loss_ce: 0.116638
2022-01-09 11:28:49,410 iteration 62 : loss : 0.355488, loss_ce: 0.148314
2022-01-09 11:28:50,714 iteration 63 : loss : 0.279949, loss_ce: 0.135495
2022-01-09 11:28:52,073 iteration 64 : loss : 0.300779, loss_ce: 0.115918
2022-01-09 11:28:53,387 iteration 65 : loss : 0.236885, loss_ce: 0.077418
2022-01-09 11:28:54,736 iteration 66 : loss : 0.232801, loss_ce: 0.092483
2022-01-09 11:28:56,161 iteration 67 : loss : 0.275124, loss_ce: 0.094123
2022-01-09 11:28:57,530 iteration 68 : loss : 0.218195, loss_ce: 0.097302
  1%|▎                              | 4/400 [01:35<2:36:11, 23.67s/it]2022-01-09 11:28:58,989 iteration 69 : loss : 0.211600, loss_ce: 0.095226
2022-01-09 11:29:00,445 iteration 70 : loss : 0.222717, loss_ce: 0.099770
2022-01-09 11:29:01,792 iteration 71 : loss : 0.241514, loss_ce: 0.100281
2022-01-09 11:29:03,180 iteration 72 : loss : 0.215353, loss_ce: 0.090191
2022-01-09 11:29:04,516 iteration 73 : loss : 0.240307, loss_ce: 0.120175
2022-01-09 11:29:05,823 iteration 74 : loss : 0.217529, loss_ce: 0.088364
2022-01-09 11:29:07,157 iteration 75 : loss : 0.231073, loss_ce: 0.100966
2022-01-09 11:29:08,489 iteration 76 : loss : 0.222622, loss_ce: 0.095013
2022-01-09 11:29:09,740 iteration 77 : loss : 0.220462, loss_ce: 0.103862
2022-01-09 11:29:11,098 iteration 78 : loss : 0.259069, loss_ce: 0.121702
2022-01-09 11:29:12,428 iteration 79 : loss : 0.263295, loss_ce: 0.099399
2022-01-09 11:29:13,740 iteration 80 : loss : 0.243696, loss_ce: 0.114951
2022-01-09 11:29:15,047 iteration 81 : loss : 0.233432, loss_ce: 0.100714
2022-01-09 11:29:16,381 iteration 82 : loss : 0.239547, loss_ce: 0.097488
2022-01-09 11:29:17,710 iteration 83 : loss : 0.255331, loss_ce: 0.098689
2022-01-09 11:29:19,128 iteration 84 : loss : 0.253114, loss_ce: 0.138414
2022-01-09 11:29:19,128 Training Data Eval:
2022-01-09 11:29:25,956   Average segmentation loss on training set: 0.8856
2022-01-09 11:29:25,956 Validation Data Eval:
2022-01-09 11:29:28,454   Average segmentation loss on validation set: 0.8864
2022-01-09 11:29:34,110 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:29:35,536 iteration 85 : loss : 0.274133, loss_ce: 0.120898
  1%|▍                              | 5/400 [02:13<3:09:49, 28.84s/it]2022-01-09 11:29:36,960 iteration 86 : loss : 0.272800, loss_ce: 0.106293
2022-01-09 11:29:38,447 iteration 87 : loss : 0.233598, loss_ce: 0.106398
2022-01-09 11:29:39,765 iteration 88 : loss : 0.259125, loss_ce: 0.120803
2022-01-09 11:29:41,155 iteration 89 : loss : 0.235487, loss_ce: 0.104821
2022-01-09 11:29:42,535 iteration 90 : loss : 0.239209, loss_ce: 0.103900
2022-01-09 11:29:43,980 iteration 91 : loss : 0.244678, loss_ce: 0.132245
2022-01-09 11:29:45,287 iteration 92 : loss : 0.246468, loss_ce: 0.110557
2022-01-09 11:29:46,641 iteration 93 : loss : 0.251293, loss_ce: 0.090166
2022-01-09 11:29:47,998 iteration 94 : loss : 0.246823, loss_ce: 0.092922
2022-01-09 11:29:49,477 iteration 95 : loss : 0.272159, loss_ce: 0.117481
2022-01-09 11:29:50,817 iteration 96 : loss : 0.255883, loss_ce: 0.111110
2022-01-09 11:29:52,159 iteration 97 : loss : 0.271990, loss_ce: 0.111651
2022-01-09 11:29:53,517 iteration 98 : loss : 0.245184, loss_ce: 0.111926
2022-01-09 11:29:54,951 iteration 99 : loss : 0.221023, loss_ce: 0.101260
2022-01-09 11:29:56,339 iteration 100 : loss : 0.229807, loss_ce: 0.102564
2022-01-09 11:29:57,704 iteration 101 : loss : 0.172114, loss_ce: 0.071941
2022-01-09 11:29:59,006 iteration 102 : loss : 0.231305, loss_ce: 0.096488
  2%|▍                              | 6/400 [02:36<2:57:23, 27.02s/it]2022-01-09 11:30:00,528 iteration 103 : loss : 0.209143, loss_ce: 0.090373
2022-01-09 11:30:01,973 iteration 104 : loss : 0.255251, loss_ce: 0.116266
2022-01-09 11:30:03,468 iteration 105 : loss : 0.256111, loss_ce: 0.104789
2022-01-09 11:30:04,804 iteration 106 : loss : 0.254194, loss_ce: 0.103885
2022-01-09 11:30:06,317 iteration 107 : loss : 0.201087, loss_ce: 0.091338
2022-01-09 11:30:07,632 iteration 108 : loss : 0.249730, loss_ce: 0.102539
2022-01-09 11:30:08,974 iteration 109 : loss : 0.190679, loss_ce: 0.090406
2022-01-09 11:30:10,273 iteration 110 : loss : 0.171219, loss_ce: 0.082850
2022-01-09 11:30:11,657 iteration 111 : loss : 0.254403, loss_ce: 0.132708
2022-01-09 11:30:12,960 iteration 112 : loss : 0.203359, loss_ce: 0.087253
2022-01-09 11:30:14,331 iteration 113 : loss : 0.249776, loss_ce: 0.134664
2022-01-09 11:30:15,706 iteration 114 : loss : 0.285450, loss_ce: 0.108904
2022-01-09 11:30:17,060 iteration 115 : loss : 0.205333, loss_ce: 0.089788
2022-01-09 11:30:18,464 iteration 116 : loss : 0.241582, loss_ce: 0.115917
2022-01-09 11:30:19,867 iteration 117 : loss : 0.212593, loss_ce: 0.093368
2022-01-09 11:30:21,240 iteration 118 : loss : 0.259620, loss_ce: 0.112506
2022-01-09 11:30:22,624 iteration 119 : loss : 0.197428, loss_ce: 0.073040
  2%|▌                              | 7/400 [03:00<2:49:40, 25.91s/it]2022-01-09 11:30:24,014 iteration 120 : loss : 0.305642, loss_ce: 0.143717
2022-01-09 11:30:25,325 iteration 121 : loss : 0.224459, loss_ce: 0.096623
2022-01-09 11:30:26,740 iteration 122 : loss : 0.263136, loss_ce: 0.107928
2022-01-09 11:30:28,123 iteration 123 : loss : 0.243253, loss_ce: 0.099933
2022-01-09 11:30:29,479 iteration 124 : loss : 0.209833, loss_ce: 0.086419
2022-01-09 11:30:30,795 iteration 125 : loss : 0.237610, loss_ce: 0.113624
2022-01-09 11:30:32,173 iteration 126 : loss : 0.220454, loss_ce: 0.078689
2022-01-09 11:30:33,492 iteration 127 : loss : 0.208819, loss_ce: 0.091905
2022-01-09 11:30:34,826 iteration 128 : loss : 0.234106, loss_ce: 0.097609
2022-01-09 11:30:36,232 iteration 129 : loss : 0.221294, loss_ce: 0.083420
2022-01-09 11:30:37,584 iteration 130 : loss : 0.197357, loss_ce: 0.077476
2022-01-09 11:30:39,027 iteration 131 : loss : 0.229082, loss_ce: 0.121339
2022-01-09 11:30:40,491 iteration 132 : loss : 0.203073, loss_ce: 0.061210
2022-01-09 11:30:41,830 iteration 133 : loss : 0.187915, loss_ce: 0.077184
2022-01-09 11:30:43,260 iteration 134 : loss : 0.194059, loss_ce: 0.078747
2022-01-09 11:30:44,706 iteration 135 : loss : 0.210325, loss_ce: 0.103761
2022-01-09 11:30:46,064 iteration 136 : loss : 0.147597, loss_ce: 0.072165
  2%|▌                              | 8/400 [03:23<2:44:05, 25.12s/it]2022-01-09 11:30:47,548 iteration 137 : loss : 0.233565, loss_ce: 0.088147
2022-01-09 11:30:48,884 iteration 138 : loss : 0.227712, loss_ce: 0.128401
2022-01-09 11:30:50,275 iteration 139 : loss : 0.220586, loss_ce: 0.094561
2022-01-09 11:30:51,644 iteration 140 : loss : 0.169080, loss_ce: 0.064634
2022-01-09 11:30:53,052 iteration 141 : loss : 0.201833, loss_ce: 0.100014
2022-01-09 11:30:54,516 iteration 142 : loss : 0.230676, loss_ce: 0.096475
2022-01-09 11:30:55,958 iteration 143 : loss : 0.227786, loss_ce: 0.092315
2022-01-09 11:30:57,371 iteration 144 : loss : 0.215937, loss_ce: 0.089872
2022-01-09 11:30:58,736 iteration 145 : loss : 0.217222, loss_ce: 0.087921
2022-01-09 11:31:00,138 iteration 146 : loss : 0.180258, loss_ce: 0.093046
2022-01-09 11:31:01,549 iteration 147 : loss : 0.207020, loss_ce: 0.090367
2022-01-09 11:31:02,842 iteration 148 : loss : 0.217636, loss_ce: 0.108435
2022-01-09 11:31:04,195 iteration 149 : loss : 0.258476, loss_ce: 0.123198
2022-01-09 11:31:05,525 iteration 150 : loss : 0.262522, loss_ce: 0.098532
2022-01-09 11:31:06,861 iteration 151 : loss : 0.180370, loss_ce: 0.086552
2022-01-09 11:31:08,204 iteration 152 : loss : 0.229412, loss_ce: 0.081700
2022-01-09 11:31:09,594 iteration 153 : loss : 0.270260, loss_ce: 0.122331
  2%|▋                              | 9/400 [03:47<2:40:27, 24.62s/it]2022-01-09 11:31:10,970 iteration 154 : loss : 0.275389, loss_ce: 0.118505
2022-01-09 11:31:12,346 iteration 155 : loss : 0.178412, loss_ce: 0.061903
2022-01-09 11:31:13,794 iteration 156 : loss : 0.200801, loss_ce: 0.075767
2022-01-09 11:31:15,181 iteration 157 : loss : 0.241589, loss_ce: 0.088996
2022-01-09 11:31:16,675 iteration 158 : loss : 0.251390, loss_ce: 0.104226
2022-01-09 11:31:17,941 iteration 159 : loss : 0.171670, loss_ce: 0.069381
2022-01-09 11:31:19,390 iteration 160 : loss : 0.188165, loss_ce: 0.063801
2022-01-09 11:31:20,822 iteration 161 : loss : 0.227022, loss_ce: 0.087125
2022-01-09 11:31:22,264 iteration 162 : loss : 0.237661, loss_ce: 0.102744
2022-01-09 11:31:23,637 iteration 163 : loss : 0.282185, loss_ce: 0.108304
2022-01-09 11:31:25,029 iteration 164 : loss : 0.184655, loss_ce: 0.068457
2022-01-09 11:31:26,391 iteration 165 : loss : 0.216700, loss_ce: 0.089160
2022-01-09 11:31:27,736 iteration 166 : loss : 0.219481, loss_ce: 0.085299
2022-01-09 11:31:29,101 iteration 167 : loss : 0.177238, loss_ce: 0.069852
2022-01-09 11:31:30,502 iteration 168 : loss : 0.240673, loss_ce: 0.115558
2022-01-09 11:31:31,873 iteration 169 : loss : 0.167287, loss_ce: 0.084361
2022-01-09 11:31:31,874 Training Data Eval:
2022-01-09 11:31:38,731   Average segmentation loss on training set: 0.2186
2022-01-09 11:31:38,731 Validation Data Eval:
2022-01-09 11:31:41,105   Average segmentation loss on validation set: 0.2196
2022-01-09 11:31:46,829 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:31:48,344 iteration 170 : loss : 0.169709, loss_ce: 0.078460
  2%|▊                             | 10/400 [04:26<3:08:22, 28.98s/it]2022-01-09 11:31:49,787 iteration 171 : loss : 0.213458, loss_ce: 0.089369
2022-01-09 11:31:51,186 iteration 172 : loss : 0.198863, loss_ce: 0.086532
2022-01-09 11:31:52,604 iteration 173 : loss : 0.145581, loss_ce: 0.068670
2022-01-09 11:31:53,933 iteration 174 : loss : 0.223973, loss_ce: 0.099840
2022-01-09 11:31:55,248 iteration 175 : loss : 0.272223, loss_ce: 0.118908
2022-01-09 11:31:56,637 iteration 176 : loss : 0.213527, loss_ce: 0.103310
2022-01-09 11:31:58,003 iteration 177 : loss : 0.192038, loss_ce: 0.075919
2022-01-09 11:31:59,367 iteration 178 : loss : 0.157654, loss_ce: 0.071030
2022-01-09 11:32:00,732 iteration 179 : loss : 0.228517, loss_ce: 0.091852
2022-01-09 11:32:02,075 iteration 180 : loss : 0.163420, loss_ce: 0.060323
2022-01-09 11:32:03,475 iteration 181 : loss : 0.238819, loss_ce: 0.102290
2022-01-09 11:32:04,809 iteration 182 : loss : 0.149132, loss_ce: 0.062807
2022-01-09 11:32:06,130 iteration 183 : loss : 0.186379, loss_ce: 0.067949
2022-01-09 11:32:07,412 iteration 184 : loss : 0.207874, loss_ce: 0.082999
2022-01-09 11:32:08,810 iteration 185 : loss : 0.258268, loss_ce: 0.103743
2022-01-09 11:32:10,204 iteration 186 : loss : 0.230606, loss_ce: 0.118510
2022-01-09 11:32:11,633 iteration 187 : loss : 0.237657, loss_ce: 0.105027
  3%|▊                             | 11/400 [04:49<2:56:35, 27.24s/it]2022-01-09 11:32:13,086 iteration 188 : loss : 0.326894, loss_ce: 0.155008
2022-01-09 11:32:14,475 iteration 189 : loss : 0.173435, loss_ce: 0.070316
2022-01-09 11:32:15,866 iteration 190 : loss : 0.169610, loss_ce: 0.061784
2022-01-09 11:32:17,219 iteration 191 : loss : 0.158682, loss_ce: 0.061477
2022-01-09 11:32:18,523 iteration 192 : loss : 0.188418, loss_ce: 0.088662
2022-01-09 11:32:19,943 iteration 193 : loss : 0.226289, loss_ce: 0.103917
2022-01-09 11:32:21,300 iteration 194 : loss : 0.225225, loss_ce: 0.067610
2022-01-09 11:32:22,635 iteration 195 : loss : 0.199135, loss_ce: 0.084554
2022-01-09 11:32:23,925 iteration 196 : loss : 0.218491, loss_ce: 0.076506
2022-01-09 11:32:25,301 iteration 197 : loss : 0.246164, loss_ce: 0.103435
2022-01-09 11:32:26,707 iteration 198 : loss : 0.177689, loss_ce: 0.076618
2022-01-09 11:32:28,042 iteration 199 : loss : 0.226025, loss_ce: 0.100557
2022-01-09 11:32:29,395 iteration 200 : loss : 0.187049, loss_ce: 0.080214
2022-01-09 11:32:30,776 iteration 201 : loss : 0.151046, loss_ce: 0.064093
2022-01-09 11:32:32,179 iteration 202 : loss : 0.205170, loss_ce: 0.101279
2022-01-09 11:32:33,506 iteration 203 : loss : 0.188598, loss_ce: 0.080074
2022-01-09 11:32:34,960 iteration 204 : loss : 0.231778, loss_ce: 0.091227
  3%|▉                             | 12/400 [05:12<2:48:26, 26.05s/it]2022-01-09 11:32:36,350 iteration 205 : loss : 0.164968, loss_ce: 0.065850
2022-01-09 11:32:37,710 iteration 206 : loss : 0.171036, loss_ce: 0.061923
2022-01-09 11:32:39,094 iteration 207 : loss : 0.180906, loss_ce: 0.077673
2022-01-09 11:32:40,446 iteration 208 : loss : 0.232876, loss_ce: 0.076005
2022-01-09 11:32:41,892 iteration 209 : loss : 0.259183, loss_ce: 0.116395
2022-01-09 11:32:43,176 iteration 210 : loss : 0.158601, loss_ce: 0.054607
2022-01-09 11:32:44,579 iteration 211 : loss : 0.225229, loss_ce: 0.095266
2022-01-09 11:32:45,962 iteration 212 : loss : 0.208269, loss_ce: 0.077483
2022-01-09 11:32:47,439 iteration 213 : loss : 0.171726, loss_ce: 0.076184
2022-01-09 11:32:48,815 iteration 214 : loss : 0.204513, loss_ce: 0.070234
2022-01-09 11:32:50,193 iteration 215 : loss : 0.223409, loss_ce: 0.091496
2022-01-09 11:32:51,724 iteration 216 : loss : 0.187693, loss_ce: 0.066963
2022-01-09 11:32:53,190 iteration 217 : loss : 0.169602, loss_ce: 0.065786
2022-01-09 11:32:54,583 iteration 218 : loss : 0.169095, loss_ce: 0.078521
2022-01-09 11:32:55,861 iteration 219 : loss : 0.198767, loss_ce: 0.084302
2022-01-09 11:32:57,260 iteration 220 : loss : 0.163119, loss_ce: 0.072052
2022-01-09 11:32:58,640 iteration 221 : loss : 0.245494, loss_ce: 0.094382
  3%|▉                             | 13/400 [05:36<2:43:23, 25.33s/it]2022-01-09 11:33:00,093 iteration 222 : loss : 0.229659, loss_ce: 0.107511
2022-01-09 11:33:01,516 iteration 223 : loss : 0.164055, loss_ce: 0.069394
2022-01-09 11:33:02,897 iteration 224 : loss : 0.286152, loss_ce: 0.153309
2022-01-09 11:33:04,363 iteration 225 : loss : 0.297708, loss_ce: 0.095577
2022-01-09 11:33:05,731 iteration 226 : loss : 0.240969, loss_ce: 0.097594
2022-01-09 11:33:07,148 iteration 227 : loss : 0.297180, loss_ce: 0.147747
2022-01-09 11:33:08,521 iteration 228 : loss : 0.232340, loss_ce: 0.103908
2022-01-09 11:33:09,851 iteration 229 : loss : 0.193399, loss_ce: 0.069438
2022-01-09 11:33:11,233 iteration 230 : loss : 0.196616, loss_ce: 0.083861
2022-01-09 11:33:12,653 iteration 231 : loss : 0.296078, loss_ce: 0.127153
2022-01-09 11:33:14,066 iteration 232 : loss : 0.169604, loss_ce: 0.072638
2022-01-09 11:33:15,404 iteration 233 : loss : 0.240782, loss_ce: 0.110897
2022-01-09 11:33:16,847 iteration 234 : loss : 0.191916, loss_ce: 0.074604
2022-01-09 11:33:18,208 iteration 235 : loss : 0.219549, loss_ce: 0.097762
2022-01-09 11:33:19,612 iteration 236 : loss : 0.216940, loss_ce: 0.089024
2022-01-09 11:33:20,968 iteration 237 : loss : 0.188076, loss_ce: 0.074431
2022-01-09 11:33:22,342 iteration 238 : loss : 0.206643, loss_ce: 0.095695
  4%|█                             | 14/400 [06:00<2:39:48, 24.84s/it]2022-01-09 11:33:23,722 iteration 239 : loss : 0.235331, loss_ce: 0.083067
2022-01-09 11:33:25,129 iteration 240 : loss : 0.198304, loss_ce: 0.088598
2022-01-09 11:33:26,540 iteration 241 : loss : 0.210754, loss_ce: 0.085156
2022-01-09 11:33:27,936 iteration 242 : loss : 0.214851, loss_ce: 0.110465
2022-01-09 11:33:29,458 iteration 243 : loss : 0.215407, loss_ce: 0.087586
2022-01-09 11:33:30,787 iteration 244 : loss : 0.220040, loss_ce: 0.064539
2022-01-09 11:33:32,179 iteration 245 : loss : 0.167081, loss_ce: 0.084169
2022-01-09 11:33:33,612 iteration 246 : loss : 0.157955, loss_ce: 0.068319
2022-01-09 11:33:34,976 iteration 247 : loss : 0.240562, loss_ce: 0.087018
2022-01-09 11:33:36,339 iteration 248 : loss : 0.195984, loss_ce: 0.060906
2022-01-09 11:33:37,688 iteration 249 : loss : 0.244034, loss_ce: 0.121402
2022-01-09 11:33:39,050 iteration 250 : loss : 0.224742, loss_ce: 0.092522
2022-01-09 11:33:40,362 iteration 251 : loss : 0.223367, loss_ce: 0.078445
2022-01-09 11:33:41,725 iteration 252 : loss : 0.140977, loss_ce: 0.055936
2022-01-09 11:33:43,118 iteration 253 : loss : 0.201655, loss_ce: 0.071020
2022-01-09 11:33:44,571 iteration 254 : loss : 0.287305, loss_ce: 0.100145
2022-01-09 11:33:44,571 Training Data Eval:
2022-01-09 11:33:51,463   Average segmentation loss on training set: 0.1689
2022-01-09 11:33:51,464 Validation Data Eval:
2022-01-09 11:33:53,840   Average segmentation loss on validation set: 0.2194
2022-01-09 11:33:59,684 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:34:01,108 iteration 255 : loss : 0.173129, loss_ce: 0.079674
  4%|█▏                            | 15/400 [06:38<3:06:20, 29.04s/it]2022-01-09 11:34:02,569 iteration 256 : loss : 0.204923, loss_ce: 0.089181
2022-01-09 11:34:03,997 iteration 257 : loss : 0.163571, loss_ce: 0.069172
2022-01-09 11:34:05,471 iteration 258 : loss : 0.184382, loss_ce: 0.068732
2022-01-09 11:34:06,773 iteration 259 : loss : 0.149472, loss_ce: 0.065362
2022-01-09 11:34:08,210 iteration 260 : loss : 0.166390, loss_ce: 0.072621
2022-01-09 11:34:09,605 iteration 261 : loss : 0.223956, loss_ce: 0.075817
2022-01-09 11:34:11,081 iteration 262 : loss : 0.193039, loss_ce: 0.074668
2022-01-09 11:34:12,461 iteration 263 : loss : 0.187150, loss_ce: 0.075331
2022-01-09 11:34:13,871 iteration 264 : loss : 0.213518, loss_ce: 0.103819
2022-01-09 11:34:15,212 iteration 265 : loss : 0.145594, loss_ce: 0.060080
2022-01-09 11:34:16,617 iteration 266 : loss : 0.173530, loss_ce: 0.081203
2022-01-09 11:34:17,986 iteration 267 : loss : 0.199235, loss_ce: 0.066236
2022-01-09 11:34:19,371 iteration 268 : loss : 0.216877, loss_ce: 0.104072
2022-01-09 11:34:20,944 iteration 269 : loss : 0.226410, loss_ce: 0.079454
2022-01-09 11:34:22,432 iteration 270 : loss : 0.163704, loss_ce: 0.071339
2022-01-09 11:34:23,697 iteration 271 : loss : 0.224255, loss_ce: 0.071985
2022-01-09 11:34:25,083 iteration 272 : loss : 0.174647, loss_ce: 0.087561
  4%|█▏                            | 16/400 [07:02<2:56:06, 27.52s/it]2022-01-09 11:34:26,476 iteration 273 : loss : 0.176139, loss_ce: 0.066203
2022-01-09 11:34:27,799 iteration 274 : loss : 0.129934, loss_ce: 0.055262
2022-01-09 11:34:29,125 iteration 275 : loss : 0.160696, loss_ce: 0.066168
2022-01-09 11:34:30,509 iteration 276 : loss : 0.147915, loss_ce: 0.068543
2022-01-09 11:34:31,914 iteration 277 : loss : 0.163436, loss_ce: 0.059594
2022-01-09 11:34:33,169 iteration 278 : loss : 0.169354, loss_ce: 0.055597
2022-01-09 11:34:34,489 iteration 279 : loss : 0.203343, loss_ce: 0.073553
2022-01-09 11:34:35,806 iteration 280 : loss : 0.184352, loss_ce: 0.083080
2022-01-09 11:34:37,177 iteration 281 : loss : 0.155288, loss_ce: 0.065422
2022-01-09 11:34:38,531 iteration 282 : loss : 0.231671, loss_ce: 0.101071
2022-01-09 11:34:39,863 iteration 283 : loss : 0.183782, loss_ce: 0.095765
2022-01-09 11:34:41,292 iteration 284 : loss : 0.221192, loss_ce: 0.103368
2022-01-09 11:34:42,606 iteration 285 : loss : 0.187833, loss_ce: 0.066132
2022-01-09 11:34:43,937 iteration 286 : loss : 0.163696, loss_ce: 0.074064
2022-01-09 11:34:45,382 iteration 287 : loss : 0.163951, loss_ce: 0.070816
2022-01-09 11:34:46,810 iteration 288 : loss : 0.220028, loss_ce: 0.084325
2022-01-09 11:34:48,157 iteration 289 : loss : 0.162074, loss_ce: 0.058784
  4%|█▎                            | 17/400 [07:25<2:47:05, 26.18s/it]2022-01-09 11:34:49,592 iteration 290 : loss : 0.169726, loss_ce: 0.063804
2022-01-09 11:34:50,925 iteration 291 : loss : 0.255508, loss_ce: 0.119330
2022-01-09 11:34:52,268 iteration 292 : loss : 0.177003, loss_ce: 0.070934
2022-01-09 11:34:53,615 iteration 293 : loss : 0.224300, loss_ce: 0.095656
2022-01-09 11:34:54,990 iteration 294 : loss : 0.187516, loss_ce: 0.081953
2022-01-09 11:34:56,455 iteration 295 : loss : 0.193461, loss_ce: 0.072183
2022-01-09 11:34:57,800 iteration 296 : loss : 0.182159, loss_ce: 0.071472
2022-01-09 11:34:59,175 iteration 297 : loss : 0.157947, loss_ce: 0.056245
2022-01-09 11:35:00,511 iteration 298 : loss : 0.144507, loss_ce: 0.054968
2022-01-09 11:35:01,877 iteration 299 : loss : 0.173368, loss_ce: 0.063358
2022-01-09 11:35:03,250 iteration 300 : loss : 0.187700, loss_ce: 0.064901
2022-01-09 11:35:04,632 iteration 301 : loss : 0.274466, loss_ce: 0.158433
2022-01-09 11:35:06,062 iteration 302 : loss : 0.164989, loss_ce: 0.063963
2022-01-09 11:35:07,475 iteration 303 : loss : 0.203931, loss_ce: 0.083835
2022-01-09 11:35:08,867 iteration 304 : loss : 0.268017, loss_ce: 0.118866
2022-01-09 11:35:10,295 iteration 305 : loss : 0.178972, loss_ce: 0.075811
2022-01-09 11:35:11,721 iteration 306 : loss : 0.153632, loss_ce: 0.068617
  4%|█▎                            | 18/400 [07:49<2:41:40, 25.39s/it]2022-01-09 11:35:13,170 iteration 307 : loss : 0.186427, loss_ce: 0.070033
2022-01-09 11:35:14,511 iteration 308 : loss : 0.157447, loss_ce: 0.075628
2022-01-09 11:35:15,902 iteration 309 : loss : 0.291778, loss_ce: 0.128225
2022-01-09 11:35:17,315 iteration 310 : loss : 0.191859, loss_ce: 0.089219
2022-01-09 11:35:18,653 iteration 311 : loss : 0.214112, loss_ce: 0.077725
2022-01-09 11:35:20,023 iteration 312 : loss : 0.142187, loss_ce: 0.070065
2022-01-09 11:35:21,404 iteration 313 : loss : 0.193243, loss_ce: 0.107440
2022-01-09 11:35:22,711 iteration 314 : loss : 0.148202, loss_ce: 0.067418
2022-01-09 11:35:24,068 iteration 315 : loss : 0.157703, loss_ce: 0.059531
2022-01-09 11:35:25,458 iteration 316 : loss : 0.184323, loss_ce: 0.068701
2022-01-09 11:35:26,832 iteration 317 : loss : 0.246541, loss_ce: 0.118277
2022-01-09 11:35:28,226 iteration 318 : loss : 0.192391, loss_ce: 0.084943
2022-01-09 11:35:29,543 iteration 319 : loss : 0.138258, loss_ce: 0.062542
2022-01-09 11:35:30,994 iteration 320 : loss : 0.157590, loss_ce: 0.062276
2022-01-09 11:35:32,408 iteration 321 : loss : 0.146450, loss_ce: 0.050113
2022-01-09 11:35:33,779 iteration 322 : loss : 0.264051, loss_ce: 0.134050
2022-01-09 11:35:35,211 iteration 323 : loss : 0.196045, loss_ce: 0.063748
  5%|█▍                            | 19/400 [08:12<2:37:37, 24.82s/it]2022-01-09 11:35:36,636 iteration 324 : loss : 0.177916, loss_ce: 0.082667
2022-01-09 11:35:38,014 iteration 325 : loss : 0.230250, loss_ce: 0.079694
2022-01-09 11:35:39,426 iteration 326 : loss : 0.174564, loss_ce: 0.075188
2022-01-09 11:35:40,900 iteration 327 : loss : 0.205104, loss_ce: 0.081957
2022-01-09 11:35:42,267 iteration 328 : loss : 0.146783, loss_ce: 0.057550
2022-01-09 11:35:43,675 iteration 329 : loss : 0.200280, loss_ce: 0.093587
2022-01-09 11:35:45,104 iteration 330 : loss : 0.192935, loss_ce: 0.077546
2022-01-09 11:35:46,436 iteration 331 : loss : 0.167171, loss_ce: 0.072503
2022-01-09 11:35:47,798 iteration 332 : loss : 0.138300, loss_ce: 0.051911
2022-01-09 11:35:49,202 iteration 333 : loss : 0.142769, loss_ce: 0.064608
2022-01-09 11:35:50,588 iteration 334 : loss : 0.161439, loss_ce: 0.074522
2022-01-09 11:35:51,944 iteration 335 : loss : 0.243148, loss_ce: 0.092472
2022-01-09 11:35:53,370 iteration 336 : loss : 0.152139, loss_ce: 0.061357
2022-01-09 11:35:54,743 iteration 337 : loss : 0.168294, loss_ce: 0.062218
2022-01-09 11:35:56,076 iteration 338 : loss : 0.187021, loss_ce: 0.093045
2022-01-09 11:35:57,419 iteration 339 : loss : 0.206373, loss_ce: 0.079308
2022-01-09 11:35:57,419 Training Data Eval:
2022-01-09 11:36:04,306   Average segmentation loss on training set: 0.3433
2022-01-09 11:36:04,306 Validation Data Eval:
2022-01-09 11:36:06,684   Average segmentation loss on validation set: 0.4556
2022-01-09 11:36:08,107 iteration 340 : loss : 0.191134, loss_ce: 0.089298
  5%|█▌                            | 20/400 [08:45<2:52:32, 27.24s/it]2022-01-09 11:36:09,590 iteration 341 : loss : 0.141165, loss_ce: 0.065671
2022-01-09 11:36:10,998 iteration 342 : loss : 0.158482, loss_ce: 0.059739
2022-01-09 11:36:12,371 iteration 343 : loss : 0.172204, loss_ce: 0.070894
2022-01-09 11:36:13,783 iteration 344 : loss : 0.225351, loss_ce: 0.110524
2022-01-09 11:36:15,153 iteration 345 : loss : 0.145410, loss_ce: 0.047606
2022-01-09 11:36:16,589 iteration 346 : loss : 0.226492, loss_ce: 0.093657
2022-01-09 11:36:17,917 iteration 347 : loss : 0.210636, loss_ce: 0.105279
2022-01-09 11:36:19,320 iteration 348 : loss : 0.198797, loss_ce: 0.085563
2022-01-09 11:36:20,823 iteration 349 : loss : 0.219086, loss_ce: 0.117908
2022-01-09 11:36:22,199 iteration 350 : loss : 0.160487, loss_ce: 0.063133
2022-01-09 11:36:23,633 iteration 351 : loss : 0.165508, loss_ce: 0.074679
2022-01-09 11:36:25,043 iteration 352 : loss : 0.193070, loss_ce: 0.080988
2022-01-09 11:36:26,373 iteration 353 : loss : 0.165183, loss_ce: 0.059821
2022-01-09 11:36:27,759 iteration 354 : loss : 0.196415, loss_ce: 0.071541
2022-01-09 11:36:29,191 iteration 355 : loss : 0.177747, loss_ce: 0.074152
2022-01-09 11:36:30,691 iteration 356 : loss : 0.191419, loss_ce: 0.082340
2022-01-09 11:36:32,009 iteration 357 : loss : 0.171251, loss_ce: 0.065921
  5%|█▌                            | 21/400 [09:09<2:45:45, 26.24s/it]2022-01-09 11:36:33,470 iteration 358 : loss : 0.183969, loss_ce: 0.082804
2022-01-09 11:36:34,925 iteration 359 : loss : 0.173598, loss_ce: 0.066166
2022-01-09 11:36:36,257 iteration 360 : loss : 0.130216, loss_ce: 0.045279
2022-01-09 11:36:37,698 iteration 361 : loss : 0.232818, loss_ce: 0.083819
2022-01-09 11:36:39,186 iteration 362 : loss : 0.153738, loss_ce: 0.046210
2022-01-09 11:36:40,663 iteration 363 : loss : 0.168641, loss_ce: 0.067135
2022-01-09 11:36:42,066 iteration 364 : loss : 0.161520, loss_ce: 0.061679
2022-01-09 11:36:43,357 iteration 365 : loss : 0.180148, loss_ce: 0.088562
2022-01-09 11:36:44,768 iteration 366 : loss : 0.184538, loss_ce: 0.088380
2022-01-09 11:36:46,081 iteration 367 : loss : 0.168599, loss_ce: 0.073592
2022-01-09 11:36:47,503 iteration 368 : loss : 0.190423, loss_ce: 0.070860
2022-01-09 11:36:48,958 iteration 369 : loss : 0.217849, loss_ce: 0.080377
2022-01-09 11:36:50,398 iteration 370 : loss : 0.146899, loss_ce: 0.079135
2022-01-09 11:36:51,796 iteration 371 : loss : 0.218203, loss_ce: 0.072837
2022-01-09 11:36:53,203 iteration 372 : loss : 0.252252, loss_ce: 0.128504
2022-01-09 11:36:54,539 iteration 373 : loss : 0.177461, loss_ce: 0.059332
2022-01-09 11:36:55,921 iteration 374 : loss : 0.165164, loss_ce: 0.064590
  6%|█▋                            | 22/400 [09:33<2:40:55, 25.54s/it]2022-01-09 11:36:57,438 iteration 375 : loss : 0.292317, loss_ce: 0.132139
2022-01-09 11:36:58,786 iteration 376 : loss : 0.099259, loss_ce: 0.035940
2022-01-09 11:37:00,172 iteration 377 : loss : 0.128843, loss_ce: 0.040250
2022-01-09 11:37:01,621 iteration 378 : loss : 0.134535, loss_ce: 0.052416
2022-01-09 11:37:02,921 iteration 379 : loss : 0.121604, loss_ce: 0.050977
2022-01-09 11:37:04,326 iteration 380 : loss : 0.162747, loss_ce: 0.063500
2022-01-09 11:37:05,701 iteration 381 : loss : 0.124032, loss_ce: 0.045976
2022-01-09 11:37:07,000 iteration 382 : loss : 0.176832, loss_ce: 0.078492
2022-01-09 11:37:08,374 iteration 383 : loss : 0.172746, loss_ce: 0.066435
2022-01-09 11:37:09,774 iteration 384 : loss : 0.148109, loss_ce: 0.062885
2022-01-09 11:37:11,149 iteration 385 : loss : 0.109991, loss_ce: 0.045265
2022-01-09 11:37:12,536 iteration 386 : loss : 0.154869, loss_ce: 0.053566
2022-01-09 11:37:13,940 iteration 387 : loss : 0.228938, loss_ce: 0.080447
2022-01-09 11:37:15,354 iteration 388 : loss : 0.154153, loss_ce: 0.060538
2022-01-09 11:37:16,762 iteration 389 : loss : 0.242585, loss_ce: 0.130127
2022-01-09 11:37:18,123 iteration 390 : loss : 0.128679, loss_ce: 0.053572
2022-01-09 11:37:19,483 iteration 391 : loss : 0.219624, loss_ce: 0.132374
  6%|█▋                            | 23/400 [09:57<2:36:44, 24.95s/it]2022-01-09 11:37:20,947 iteration 392 : loss : 0.161618, loss_ce: 0.096428
2022-01-09 11:37:22,287 iteration 393 : loss : 0.139286, loss_ce: 0.046695
2022-01-09 11:37:23,714 iteration 394 : loss : 0.117585, loss_ce: 0.037694
2022-01-09 11:37:25,119 iteration 395 : loss : 0.160440, loss_ce: 0.067270
2022-01-09 11:37:26,472 iteration 396 : loss : 0.145026, loss_ce: 0.051143
2022-01-09 11:37:27,853 iteration 397 : loss : 0.163604, loss_ce: 0.075579
2022-01-09 11:37:29,253 iteration 398 : loss : 0.105477, loss_ce: 0.041189
2022-01-09 11:37:30,536 iteration 399 : loss : 0.170562, loss_ce: 0.064898
2022-01-09 11:37:31,922 iteration 400 : loss : 0.172404, loss_ce: 0.058287
2022-01-09 11:37:33,365 iteration 401 : loss : 0.162393, loss_ce: 0.068398
2022-01-09 11:37:34,725 iteration 402 : loss : 0.112896, loss_ce: 0.048827
2022-01-09 11:37:36,083 iteration 403 : loss : 0.105122, loss_ce: 0.041738
2022-01-09 11:37:37,493 iteration 404 : loss : 0.188927, loss_ce: 0.079724
2022-01-09 11:37:38,940 iteration 405 : loss : 0.165368, loss_ce: 0.075480
2022-01-09 11:37:40,297 iteration 406 : loss : 0.169729, loss_ce: 0.073566
2022-01-09 11:37:41,615 iteration 407 : loss : 0.178166, loss_ce: 0.071084
2022-01-09 11:37:42,978 iteration 408 : loss : 0.160910, loss_ce: 0.063878
  6%|█▊                            | 24/400 [10:20<2:33:36, 24.51s/it]2022-01-09 11:37:44,442 iteration 409 : loss : 0.148128, loss_ce: 0.078602
2022-01-09 11:37:45,848 iteration 410 : loss : 0.177921, loss_ce: 0.078630
2022-01-09 11:37:47,224 iteration 411 : loss : 0.291910, loss_ce: 0.097905
2022-01-09 11:37:48,513 iteration 412 : loss : 0.132095, loss_ce: 0.048960
2022-01-09 11:37:49,884 iteration 413 : loss : 0.143857, loss_ce: 0.052586
2022-01-09 11:37:51,207 iteration 414 : loss : 0.210782, loss_ce: 0.115624
2022-01-09 11:37:52,477 iteration 415 : loss : 0.173531, loss_ce: 0.068891
2022-01-09 11:37:53,830 iteration 416 : loss : 0.173864, loss_ce: 0.088850
2022-01-09 11:37:55,161 iteration 417 : loss : 0.207495, loss_ce: 0.079671
2022-01-09 11:37:56,589 iteration 418 : loss : 0.139226, loss_ce: 0.062528
2022-01-09 11:37:57,939 iteration 419 : loss : 0.189377, loss_ce: 0.108670
2022-01-09 11:37:59,325 iteration 420 : loss : 0.161475, loss_ce: 0.066843
2022-01-09 11:38:00,651 iteration 421 : loss : 0.169387, loss_ce: 0.070770
2022-01-09 11:38:01,976 iteration 422 : loss : 0.178617, loss_ce: 0.069195
2022-01-09 11:38:03,307 iteration 423 : loss : 0.182425, loss_ce: 0.061372
2022-01-09 11:38:04,689 iteration 424 : loss : 0.154893, loss_ce: 0.057709
2022-01-09 11:38:04,689 Training Data Eval:
2022-01-09 11:38:11,593   Average segmentation loss on training set: 0.1484
2022-01-09 11:38:11,593 Validation Data Eval:
2022-01-09 11:38:13,977   Average segmentation loss on validation set: 0.1615
2022-01-09 11:38:19,769 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:38:21,201 iteration 425 : loss : 0.168667, loss_ce: 0.066630
  6%|█▉                            | 25/400 [10:58<2:58:54, 28.63s/it]2022-01-09 11:38:22,584 iteration 426 : loss : 0.160711, loss_ce: 0.054960
2022-01-09 11:38:23,989 iteration 427 : loss : 0.112358, loss_ce: 0.039242
2022-01-09 11:38:25,363 iteration 428 : loss : 0.154905, loss_ce: 0.066485
2022-01-09 11:38:26,687 iteration 429 : loss : 0.223279, loss_ce: 0.114033
2022-01-09 11:38:28,000 iteration 430 : loss : 0.152155, loss_ce: 0.053427
2022-01-09 11:38:29,343 iteration 431 : loss : 0.241582, loss_ce: 0.102531
2022-01-09 11:38:30,743 iteration 432 : loss : 0.124377, loss_ce: 0.050789
2022-01-09 11:38:32,176 iteration 433 : loss : 0.129601, loss_ce: 0.053339
2022-01-09 11:38:33,556 iteration 434 : loss : 0.105500, loss_ce: 0.042369
2022-01-09 11:38:34,872 iteration 435 : loss : 0.139694, loss_ce: 0.058225
2022-01-09 11:38:36,221 iteration 436 : loss : 0.144864, loss_ce: 0.070346
2022-01-09 11:38:37,574 iteration 437 : loss : 0.218132, loss_ce: 0.116750
2022-01-09 11:38:38,878 iteration 438 : loss : 0.160591, loss_ce: 0.051024
2022-01-09 11:38:40,254 iteration 439 : loss : 0.161660, loss_ce: 0.056533
2022-01-09 11:38:41,601 iteration 440 : loss : 0.173776, loss_ce: 0.064699
2022-01-09 11:38:43,011 iteration 441 : loss : 0.170578, loss_ce: 0.065155
2022-01-09 11:38:44,406 iteration 442 : loss : 0.189959, loss_ce: 0.090736
  6%|█▉                            | 26/400 [11:22<2:48:18, 27.00s/it]2022-01-09 11:38:45,813 iteration 443 : loss : 0.150826, loss_ce: 0.069904
2022-01-09 11:38:47,135 iteration 444 : loss : 0.158771, loss_ce: 0.060339
2022-01-09 11:38:48,492 iteration 445 : loss : 0.188758, loss_ce: 0.067899
2022-01-09 11:38:49,984 iteration 446 : loss : 0.153856, loss_ce: 0.060415
2022-01-09 11:38:51,418 iteration 447 : loss : 0.158062, loss_ce: 0.084778
2022-01-09 11:38:52,840 iteration 448 : loss : 0.243615, loss_ce: 0.076052
2022-01-09 11:38:54,220 iteration 449 : loss : 0.174423, loss_ce: 0.084981
2022-01-09 11:38:55,578 iteration 450 : loss : 0.169619, loss_ce: 0.065750
2022-01-09 11:38:57,026 iteration 451 : loss : 0.121839, loss_ce: 0.053694
2022-01-09 11:38:58,467 iteration 452 : loss : 0.152971, loss_ce: 0.063387
2022-01-09 11:38:59,867 iteration 453 : loss : 0.188792, loss_ce: 0.076989
2022-01-09 11:39:01,269 iteration 454 : loss : 0.150465, loss_ce: 0.055146
2022-01-09 11:39:02,705 iteration 455 : loss : 0.121573, loss_ce: 0.046009
2022-01-09 11:39:04,096 iteration 456 : loss : 0.149077, loss_ce: 0.054857
2022-01-09 11:39:05,480 iteration 457 : loss : 0.167411, loss_ce: 0.061996
2022-01-09 11:39:06,931 iteration 458 : loss : 0.170456, loss_ce: 0.074269
2022-01-09 11:39:08,255 iteration 459 : loss : 0.116680, loss_ce: 0.044756
  7%|██                            | 27/400 [11:45<2:41:57, 26.05s/it]2022-01-09 11:39:09,713 iteration 460 : loss : 0.179037, loss_ce: 0.082271
2022-01-09 11:39:11,111 iteration 461 : loss : 0.146764, loss_ce: 0.062111
2022-01-09 11:39:12,477 iteration 462 : loss : 0.155295, loss_ce: 0.062114
2022-01-09 11:39:13,912 iteration 463 : loss : 0.133777, loss_ce: 0.059917
2022-01-09 11:39:15,297 iteration 464 : loss : 0.117903, loss_ce: 0.047222
2022-01-09 11:39:16,632 iteration 465 : loss : 0.159043, loss_ce: 0.052908
2022-01-09 11:39:17,999 iteration 466 : loss : 0.158435, loss_ce: 0.060581
2022-01-09 11:39:19,353 iteration 467 : loss : 0.134224, loss_ce: 0.054729
2022-01-09 11:39:20,826 iteration 468 : loss : 0.157605, loss_ce: 0.068692
2022-01-09 11:39:22,184 iteration 469 : loss : 0.152937, loss_ce: 0.056229
2022-01-09 11:39:23,556 iteration 470 : loss : 0.163597, loss_ce: 0.064105
2022-01-09 11:39:24,927 iteration 471 : loss : 0.178051, loss_ce: 0.061414
2022-01-09 11:39:26,352 iteration 472 : loss : 0.151543, loss_ce: 0.069517
2022-01-09 11:39:27,868 iteration 473 : loss : 0.162352, loss_ce: 0.066395
2022-01-09 11:39:29,336 iteration 474 : loss : 0.179172, loss_ce: 0.078908
2022-01-09 11:39:30,691 iteration 475 : loss : 0.106736, loss_ce: 0.041013
2022-01-09 11:39:32,124 iteration 476 : loss : 0.167515, loss_ce: 0.082319
  7%|██                            | 28/400 [12:09<2:37:28, 25.40s/it]2022-01-09 11:39:33,566 iteration 477 : loss : 0.158913, loss_ce: 0.054987
2022-01-09 11:39:34,989 iteration 478 : loss : 0.096002, loss_ce: 0.036214
2022-01-09 11:39:36,361 iteration 479 : loss : 0.145547, loss_ce: 0.065966
2022-01-09 11:39:37,755 iteration 480 : loss : 0.120661, loss_ce: 0.056981
2022-01-09 11:39:39,056 iteration 481 : loss : 0.165900, loss_ce: 0.070790
2022-01-09 11:39:40,448 iteration 482 : loss : 0.181898, loss_ce: 0.053047
2022-01-09 11:39:41,849 iteration 483 : loss : 0.173669, loss_ce: 0.063528
2022-01-09 11:39:43,273 iteration 484 : loss : 0.188680, loss_ce: 0.085464
2022-01-09 11:39:44,637 iteration 485 : loss : 0.125429, loss_ce: 0.057290
2022-01-09 11:39:45,988 iteration 486 : loss : 0.183605, loss_ce: 0.060459
2022-01-09 11:39:47,419 iteration 487 : loss : 0.110430, loss_ce: 0.044136
2022-01-09 11:39:48,742 iteration 488 : loss : 0.105633, loss_ce: 0.044455
2022-01-09 11:39:50,121 iteration 489 : loss : 0.143984, loss_ce: 0.065741
2022-01-09 11:39:51,398 iteration 490 : loss : 0.160302, loss_ce: 0.065685
2022-01-09 11:39:52,781 iteration 491 : loss : 0.142789, loss_ce: 0.058694
2022-01-09 11:39:54,144 iteration 492 : loss : 0.087423, loss_ce: 0.039738
2022-01-09 11:39:55,547 iteration 493 : loss : 0.165667, loss_ce: 0.082920
  7%|██▏                           | 29/400 [12:33<2:33:23, 24.81s/it]2022-01-09 11:39:56,984 iteration 494 : loss : 0.157405, loss_ce: 0.070526
2022-01-09 11:39:58,393 iteration 495 : loss : 0.117822, loss_ce: 0.042122
2022-01-09 11:39:59,758 iteration 496 : loss : 0.203979, loss_ce: 0.099674
2022-01-09 11:40:01,161 iteration 497 : loss : 0.145682, loss_ce: 0.065565
2022-01-09 11:40:02,599 iteration 498 : loss : 0.155954, loss_ce: 0.071132
2022-01-09 11:40:03,913 iteration 499 : loss : 0.110741, loss_ce: 0.047428
2022-01-09 11:40:05,240 iteration 500 : loss : 0.132585, loss_ce: 0.049425
2022-01-09 11:40:06,569 iteration 501 : loss : 0.147091, loss_ce: 0.057373
2022-01-09 11:40:08,006 iteration 502 : loss : 0.146050, loss_ce: 0.060966
2022-01-09 11:40:09,350 iteration 503 : loss : 0.158744, loss_ce: 0.053268
2022-01-09 11:40:10,751 iteration 504 : loss : 0.124253, loss_ce: 0.040686
2022-01-09 11:40:12,084 iteration 505 : loss : 0.095486, loss_ce: 0.036734
2022-01-09 11:40:13,548 iteration 506 : loss : 0.117238, loss_ce: 0.044135
2022-01-09 11:40:14,937 iteration 507 : loss : 0.211655, loss_ce: 0.075294
2022-01-09 11:40:16,319 iteration 508 : loss : 0.197296, loss_ce: 0.094875
2022-01-09 11:40:17,743 iteration 509 : loss : 0.105313, loss_ce: 0.038290
2022-01-09 11:40:17,744 Training Data Eval:
2022-01-09 11:40:24,640   Average segmentation loss on training set: 0.1141
2022-01-09 11:40:24,640 Validation Data Eval:
2022-01-09 11:40:27,035   Average segmentation loss on validation set: 0.1484
2022-01-09 11:40:32,682 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:40:34,173 iteration 510 : loss : 0.110165, loss_ce: 0.054032
  8%|██▎                           | 30/400 [13:11<2:58:30, 28.95s/it]2022-01-09 11:40:35,617 iteration 511 : loss : 0.186404, loss_ce: 0.085584
2022-01-09 11:40:36,949 iteration 512 : loss : 0.187195, loss_ce: 0.079778
2022-01-09 11:40:38,295 iteration 513 : loss : 0.103200, loss_ce: 0.033088
2022-01-09 11:40:39,661 iteration 514 : loss : 0.101049, loss_ce: 0.041087
2022-01-09 11:40:41,007 iteration 515 : loss : 0.170692, loss_ce: 0.071731
2022-01-09 11:40:42,400 iteration 516 : loss : 0.106144, loss_ce: 0.044001
2022-01-09 11:40:43,823 iteration 517 : loss : 0.139749, loss_ce: 0.068019
2022-01-09 11:40:45,147 iteration 518 : loss : 0.154213, loss_ce: 0.057739
2022-01-09 11:40:46,527 iteration 519 : loss : 0.154637, loss_ce: 0.064728
2022-01-09 11:40:47,849 iteration 520 : loss : 0.112860, loss_ce: 0.044231
2022-01-09 11:40:49,222 iteration 521 : loss : 0.147792, loss_ce: 0.054017
2022-01-09 11:40:50,631 iteration 522 : loss : 0.112032, loss_ce: 0.037196
2022-01-09 11:40:52,082 iteration 523 : loss : 0.131187, loss_ce: 0.052680
2022-01-09 11:40:53,550 iteration 524 : loss : 0.118699, loss_ce: 0.043458
2022-01-09 11:40:54,926 iteration 525 : loss : 0.176410, loss_ce: 0.085528
2022-01-09 11:40:56,292 iteration 526 : loss : 0.248303, loss_ce: 0.125054
2022-01-09 11:40:57,664 iteration 527 : loss : 0.124140, loss_ce: 0.049261
  8%|██▎                           | 31/400 [13:35<2:47:58, 27.31s/it]2022-01-09 11:40:59,073 iteration 528 : loss : 0.137814, loss_ce: 0.061311
2022-01-09 11:41:00,458 iteration 529 : loss : 0.164535, loss_ce: 0.054311
2022-01-09 11:41:01,796 iteration 530 : loss : 0.153580, loss_ce: 0.060315
2022-01-09 11:41:03,122 iteration 531 : loss : 0.135832, loss_ce: 0.048873
2022-01-09 11:41:04,461 iteration 532 : loss : 0.123378, loss_ce: 0.059636
2022-01-09 11:41:05,928 iteration 533 : loss : 0.126645, loss_ce: 0.051877
2022-01-09 11:41:07,252 iteration 534 : loss : 0.127775, loss_ce: 0.061920
2022-01-09 11:41:08,665 iteration 535 : loss : 0.105202, loss_ce: 0.040232
2022-01-09 11:41:10,116 iteration 536 : loss : 0.181241, loss_ce: 0.078782
2022-01-09 11:41:11,578 iteration 537 : loss : 0.159201, loss_ce: 0.050794
2022-01-09 11:41:12,974 iteration 538 : loss : 0.152774, loss_ce: 0.063719
2022-01-09 11:41:14,333 iteration 539 : loss : 0.146319, loss_ce: 0.063010
2022-01-09 11:41:15,730 iteration 540 : loss : 0.126202, loss_ce: 0.042935
2022-01-09 11:41:17,082 iteration 541 : loss : 0.114266, loss_ce: 0.047830
2022-01-09 11:41:18,452 iteration 542 : loss : 0.120676, loss_ce: 0.053897
2022-01-09 11:41:19,774 iteration 543 : loss : 0.101386, loss_ce: 0.043212
2022-01-09 11:41:21,130 iteration 544 : loss : 0.132282, loss_ce: 0.049388
  8%|██▍                           | 32/400 [13:58<2:40:26, 26.16s/it]2022-01-09 11:41:22,578 iteration 545 : loss : 0.100209, loss_ce: 0.048090
2022-01-09 11:41:23,952 iteration 546 : loss : 0.115291, loss_ce: 0.045236
2022-01-09 11:41:25,330 iteration 547 : loss : 0.127143, loss_ce: 0.051590
2022-01-09 11:41:26,695 iteration 548 : loss : 0.202848, loss_ce: 0.075989
2022-01-09 11:41:28,093 iteration 549 : loss : 0.136068, loss_ce: 0.060511
2022-01-09 11:41:29,440 iteration 550 : loss : 0.118186, loss_ce: 0.050094
2022-01-09 11:41:30,807 iteration 551 : loss : 0.139424, loss_ce: 0.058839
2022-01-09 11:41:32,155 iteration 552 : loss : 0.124218, loss_ce: 0.053428
2022-01-09 11:41:33,562 iteration 553 : loss : 0.129482, loss_ce: 0.049697
2022-01-09 11:41:34,882 iteration 554 : loss : 0.112290, loss_ce: 0.054243
2022-01-09 11:41:36,245 iteration 555 : loss : 0.139769, loss_ce: 0.045407
2022-01-09 11:41:37,663 iteration 556 : loss : 0.116790, loss_ce: 0.051284
2022-01-09 11:41:38,964 iteration 557 : loss : 0.156704, loss_ce: 0.057779
2022-01-09 11:41:40,373 iteration 558 : loss : 0.158029, loss_ce: 0.047445
2022-01-09 11:41:41,743 iteration 559 : loss : 0.130615, loss_ce: 0.048924
2022-01-09 11:41:43,134 iteration 560 : loss : 0.119099, loss_ce: 0.032777
2022-01-09 11:41:44,548 iteration 561 : loss : 0.169836, loss_ce: 0.088892
  8%|██▍                           | 33/400 [14:22<2:34:59, 25.34s/it]2022-01-09 11:41:46,089 iteration 562 : loss : 0.173302, loss_ce: 0.084181
2022-01-09 11:41:47,379 iteration 563 : loss : 0.126268, loss_ce: 0.058293
2022-01-09 11:41:48,776 iteration 564 : loss : 0.170265, loss_ce: 0.091928
2022-01-09 11:41:50,175 iteration 565 : loss : 0.137001, loss_ce: 0.061697
2022-01-09 11:41:51,599 iteration 566 : loss : 0.126627, loss_ce: 0.054391
2022-01-09 11:41:52,952 iteration 567 : loss : 0.109015, loss_ce: 0.040905
2022-01-09 11:41:54,297 iteration 568 : loss : 0.170397, loss_ce: 0.056110
2022-01-09 11:41:55,665 iteration 569 : loss : 0.179545, loss_ce: 0.075523
2022-01-09 11:41:57,072 iteration 570 : loss : 0.098566, loss_ce: 0.046633
2022-01-09 11:41:58,427 iteration 571 : loss : 0.205066, loss_ce: 0.081352
2022-01-09 11:41:59,818 iteration 572 : loss : 0.137320, loss_ce: 0.048427
2022-01-09 11:42:01,159 iteration 573 : loss : 0.103180, loss_ce: 0.043551
2022-01-09 11:42:02,534 iteration 574 : loss : 0.138986, loss_ce: 0.049835
2022-01-09 11:42:03,969 iteration 575 : loss : 0.112695, loss_ce: 0.034196
2022-01-09 11:42:05,388 iteration 576 : loss : 0.122547, loss_ce: 0.055421
2022-01-09 11:42:06,766 iteration 577 : loss : 0.106226, loss_ce: 0.044784
2022-01-09 11:42:08,099 iteration 578 : loss : 0.161346, loss_ce: 0.068821
  8%|██▌                           | 34/400 [14:45<2:31:17, 24.80s/it]2022-01-09 11:42:09,696 iteration 579 : loss : 0.131243, loss_ce: 0.060982
2022-01-09 11:42:11,091 iteration 580 : loss : 0.146544, loss_ce: 0.056197
2022-01-09 11:42:12,517 iteration 581 : loss : 0.119843, loss_ce: 0.039681
2022-01-09 11:42:13,887 iteration 582 : loss : 0.121587, loss_ce: 0.054340
2022-01-09 11:42:15,273 iteration 583 : loss : 0.211682, loss_ce: 0.070885
2022-01-09 11:42:16,624 iteration 584 : loss : 0.098129, loss_ce: 0.043387
2022-01-09 11:42:18,052 iteration 585 : loss : 0.098861, loss_ce: 0.037042
2022-01-09 11:42:19,388 iteration 586 : loss : 0.174528, loss_ce: 0.074174
2022-01-09 11:42:20,722 iteration 587 : loss : 0.121592, loss_ce: 0.056850
2022-01-09 11:42:22,122 iteration 588 : loss : 0.122296, loss_ce: 0.047803
2022-01-09 11:42:23,487 iteration 589 : loss : 0.102787, loss_ce: 0.042522
2022-01-09 11:42:24,862 iteration 590 : loss : 0.124827, loss_ce: 0.045476
2022-01-09 11:42:26,244 iteration 591 : loss : 0.122786, loss_ce: 0.048412
2022-01-09 11:42:27,622 iteration 592 : loss : 0.150669, loss_ce: 0.055449
2022-01-09 11:42:29,084 iteration 593 : loss : 0.086690, loss_ce: 0.032146
2022-01-09 11:42:30,475 iteration 594 : loss : 0.124631, loss_ce: 0.049863
2022-01-09 11:42:30,475 Training Data Eval:
2022-01-09 11:42:37,371   Average segmentation loss on training set: 0.1266
2022-01-09 11:42:37,371 Validation Data Eval:
2022-01-09 11:42:39,757   Average segmentation loss on validation set: 0.1381
2022-01-09 11:42:45,520 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:42:47,037 iteration 595 : loss : 0.162619, loss_ce: 0.082056
  9%|██▋                           | 35/400 [15:24<2:56:40, 29.04s/it]2022-01-09 11:42:48,474 iteration 596 : loss : 0.116140, loss_ce: 0.047923
2022-01-09 11:42:49,820 iteration 597 : loss : 0.133626, loss_ce: 0.045811
2022-01-09 11:42:51,155 iteration 598 : loss : 0.103358, loss_ce: 0.041646
2022-01-09 11:42:52,505 iteration 599 : loss : 0.125827, loss_ce: 0.044314
2022-01-09 11:42:53,859 iteration 600 : loss : 0.189710, loss_ce: 0.086159
2022-01-09 11:42:55,237 iteration 601 : loss : 0.134076, loss_ce: 0.050932
2022-01-09 11:42:56,635 iteration 602 : loss : 0.118582, loss_ce: 0.049098
2022-01-09 11:42:57,941 iteration 603 : loss : 0.128992, loss_ce: 0.044852
2022-01-09 11:42:59,265 iteration 604 : loss : 0.095013, loss_ce: 0.036665
2022-01-09 11:43:00,572 iteration 605 : loss : 0.104801, loss_ce: 0.038879
2022-01-09 11:43:02,074 iteration 606 : loss : 0.105197, loss_ce: 0.050064
2022-01-09 11:43:03,436 iteration 607 : loss : 0.190758, loss_ce: 0.072737
2022-01-09 11:43:04,803 iteration 608 : loss : 0.120589, loss_ce: 0.057214
2022-01-09 11:43:06,090 iteration 609 : loss : 0.090807, loss_ce: 0.036833
2022-01-09 11:43:07,504 iteration 610 : loss : 0.100882, loss_ce: 0.049047
2022-01-09 11:43:08,974 iteration 611 : loss : 0.146024, loss_ce: 0.063072
2022-01-09 11:43:10,369 iteration 612 : loss : 0.091613, loss_ce: 0.030428
  9%|██▋                           | 36/400 [15:48<2:45:48, 27.33s/it]2022-01-09 11:43:11,832 iteration 613 : loss : 0.109281, loss_ce: 0.045207
2022-01-09 11:43:13,164 iteration 614 : loss : 0.107738, loss_ce: 0.036778
2022-01-09 11:43:14,494 iteration 615 : loss : 0.181642, loss_ce: 0.087987
2022-01-09 11:43:15,932 iteration 616 : loss : 0.116442, loss_ce: 0.051750
2022-01-09 11:43:17,267 iteration 617 : loss : 0.115013, loss_ce: 0.059551
2022-01-09 11:43:18,645 iteration 618 : loss : 0.169302, loss_ce: 0.043964
2022-01-09 11:43:20,025 iteration 619 : loss : 0.201517, loss_ce: 0.084610
2022-01-09 11:43:21,497 iteration 620 : loss : 0.217821, loss_ce: 0.070023
2022-01-09 11:43:22,877 iteration 621 : loss : 0.088964, loss_ce: 0.034986
2022-01-09 11:43:24,373 iteration 622 : loss : 0.148674, loss_ce: 0.061087
2022-01-09 11:43:25,846 iteration 623 : loss : 0.144534, loss_ce: 0.070869
2022-01-09 11:43:27,210 iteration 624 : loss : 0.132984, loss_ce: 0.043929
2022-01-09 11:43:28,540 iteration 625 : loss : 0.098203, loss_ce: 0.043460
2022-01-09 11:43:29,957 iteration 626 : loss : 0.102686, loss_ce: 0.045837
2022-01-09 11:43:31,386 iteration 627 : loss : 0.166820, loss_ce: 0.069194
2022-01-09 11:43:32,859 iteration 628 : loss : 0.154766, loss_ce: 0.066971
2022-01-09 11:43:34,226 iteration 629 : loss : 0.091572, loss_ce: 0.036837
  9%|██▊                           | 37/400 [16:11<2:39:02, 26.29s/it]2022-01-09 11:43:35,692 iteration 630 : loss : 0.103400, loss_ce: 0.042794
2022-01-09 11:43:37,048 iteration 631 : loss : 0.076559, loss_ce: 0.035587
2022-01-09 11:43:38,401 iteration 632 : loss : 0.135449, loss_ce: 0.056404
2022-01-09 11:43:39,855 iteration 633 : loss : 0.139302, loss_ce: 0.058147
2022-01-09 11:43:41,241 iteration 634 : loss : 0.164724, loss_ce: 0.063265
2022-01-09 11:43:42,715 iteration 635 : loss : 0.123692, loss_ce: 0.054962
2022-01-09 11:43:44,116 iteration 636 : loss : 0.145422, loss_ce: 0.059618
2022-01-09 11:43:45,436 iteration 637 : loss : 0.115800, loss_ce: 0.048271
2022-01-09 11:43:46,886 iteration 638 : loss : 0.139130, loss_ce: 0.042170
2022-01-09 11:43:48,231 iteration 639 : loss : 0.109152, loss_ce: 0.046221
2022-01-09 11:43:49,548 iteration 640 : loss : 0.156433, loss_ce: 0.043244
2022-01-09 11:43:50,866 iteration 641 : loss : 0.112394, loss_ce: 0.047012
2022-01-09 11:43:52,190 iteration 642 : loss : 0.125440, loss_ce: 0.032592
2022-01-09 11:43:53,550 iteration 643 : loss : 0.120892, loss_ce: 0.043415
2022-01-09 11:43:54,918 iteration 644 : loss : 0.115559, loss_ce: 0.048009
2022-01-09 11:43:56,251 iteration 645 : loss : 0.107109, loss_ce: 0.035108
2022-01-09 11:43:57,686 iteration 646 : loss : 0.111528, loss_ce: 0.049717
 10%|██▊                           | 38/400 [16:35<2:33:28, 25.44s/it]2022-01-09 11:43:59,105 iteration 647 : loss : 0.147428, loss_ce: 0.058723
2022-01-09 11:44:00,480 iteration 648 : loss : 0.107454, loss_ce: 0.048385
2022-01-09 11:44:01,882 iteration 649 : loss : 0.105992, loss_ce: 0.048954
2022-01-09 11:44:03,328 iteration 650 : loss : 0.121606, loss_ce: 0.047690
2022-01-09 11:44:04,702 iteration 651 : loss : 0.077683, loss_ce: 0.032917
2022-01-09 11:44:05,979 iteration 652 : loss : 0.082801, loss_ce: 0.029571
2022-01-09 11:44:07,420 iteration 653 : loss : 0.122970, loss_ce: 0.052771
2022-01-09 11:44:08,746 iteration 654 : loss : 0.106953, loss_ce: 0.043504
2022-01-09 11:44:10,120 iteration 655 : loss : 0.158987, loss_ce: 0.055713
2022-01-09 11:44:11,501 iteration 656 : loss : 0.089951, loss_ce: 0.035689
2022-01-09 11:44:12,952 iteration 657 : loss : 0.071475, loss_ce: 0.025331
2022-01-09 11:44:14,352 iteration 658 : loss : 0.168906, loss_ce: 0.054540
2022-01-09 11:44:15,761 iteration 659 : loss : 0.123995, loss_ce: 0.046705
2022-01-09 11:44:17,109 iteration 660 : loss : 0.108631, loss_ce: 0.050406
2022-01-09 11:44:18,509 iteration 661 : loss : 0.112102, loss_ce: 0.044200
2022-01-09 11:44:19,985 iteration 662 : loss : 0.184214, loss_ce: 0.048019
2022-01-09 11:44:21,373 iteration 663 : loss : 0.101627, loss_ce: 0.044031
 10%|██▉                           | 39/400 [16:59<2:29:52, 24.91s/it]2022-01-09 11:44:22,834 iteration 664 : loss : 0.107656, loss_ce: 0.047829
2022-01-09 11:44:24,142 iteration 665 : loss : 0.114006, loss_ce: 0.053000
2022-01-09 11:44:25,506 iteration 666 : loss : 0.089547, loss_ce: 0.037285
2022-01-09 11:44:26,917 iteration 667 : loss : 0.069418, loss_ce: 0.029876
2022-01-09 11:44:28,302 iteration 668 : loss : 0.128037, loss_ce: 0.055527
2022-01-09 11:44:29,654 iteration 669 : loss : 0.096046, loss_ce: 0.036065
2022-01-09 11:44:31,151 iteration 670 : loss : 0.100551, loss_ce: 0.037167
2022-01-09 11:44:32,512 iteration 671 : loss : 0.127683, loss_ce: 0.042567
2022-01-09 11:44:33,956 iteration 672 : loss : 0.097155, loss_ce: 0.035018
2022-01-09 11:44:35,374 iteration 673 : loss : 0.109904, loss_ce: 0.044720
2022-01-09 11:44:36,779 iteration 674 : loss : 0.093647, loss_ce: 0.046101
2022-01-09 11:44:38,213 iteration 675 : loss : 0.187425, loss_ce: 0.075637
2022-01-09 11:44:39,585 iteration 676 : loss : 0.114560, loss_ce: 0.043504
2022-01-09 11:44:40,971 iteration 677 : loss : 0.082495, loss_ce: 0.038081
2022-01-09 11:44:42,380 iteration 678 : loss : 0.121013, loss_ce: 0.041487
2022-01-09 11:44:43,770 iteration 679 : loss : 0.115592, loss_ce: 0.051716
2022-01-09 11:44:43,770 Training Data Eval:
2022-01-09 11:44:50,646   Average segmentation loss on training set: 0.0879
2022-01-09 11:44:50,647 Validation Data Eval:
2022-01-09 11:44:53,031   Average segmentation loss on validation set: 0.1350
2022-01-09 11:44:58,713 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:45:00,171 iteration 680 : loss : 0.079856, loss_ce: 0.029407
 10%|███                           | 40/400 [17:37<2:54:28, 29.08s/it]2022-01-09 11:45:01,621 iteration 681 : loss : 0.084139, loss_ce: 0.033022
2022-01-09 11:45:02,923 iteration 682 : loss : 0.129615, loss_ce: 0.038083
2022-01-09 11:45:04,296 iteration 683 : loss : 0.103448, loss_ce: 0.044555
2022-01-09 11:45:05,629 iteration 684 : loss : 0.088051, loss_ce: 0.028729
2022-01-09 11:45:07,063 iteration 685 : loss : 0.091646, loss_ce: 0.035314
2022-01-09 11:45:08,438 iteration 686 : loss : 0.180548, loss_ce: 0.065883
2022-01-09 11:45:09,809 iteration 687 : loss : 0.105572, loss_ce: 0.045686
2022-01-09 11:45:11,163 iteration 688 : loss : 0.088262, loss_ce: 0.030857
2022-01-09 11:45:12,566 iteration 689 : loss : 0.074520, loss_ce: 0.028635
2022-01-09 11:45:13,933 iteration 690 : loss : 0.089613, loss_ce: 0.030779
2022-01-09 11:45:15,312 iteration 691 : loss : 0.132790, loss_ce: 0.052663
2022-01-09 11:45:16,746 iteration 692 : loss : 0.082606, loss_ce: 0.029827
2022-01-09 11:45:18,063 iteration 693 : loss : 0.116881, loss_ce: 0.054886
2022-01-09 11:45:19,448 iteration 694 : loss : 0.111702, loss_ce: 0.051767
2022-01-09 11:45:20,785 iteration 695 : loss : 0.139611, loss_ce: 0.079090
2022-01-09 11:45:22,219 iteration 696 : loss : 0.095566, loss_ce: 0.043592
2022-01-09 11:45:23,592 iteration 697 : loss : 0.112591, loss_ce: 0.046744
 10%|███                           | 41/400 [18:01<2:43:51, 27.38s/it]2022-01-09 11:45:24,994 iteration 698 : loss : 0.153997, loss_ce: 0.045928
2022-01-09 11:45:26,409 iteration 699 : loss : 0.123686, loss_ce: 0.051281
2022-01-09 11:45:27,762 iteration 700 : loss : 0.108948, loss_ce: 0.050144
2022-01-09 11:45:29,114 iteration 701 : loss : 0.132860, loss_ce: 0.062017
2022-01-09 11:45:30,526 iteration 702 : loss : 0.122886, loss_ce: 0.043288
2022-01-09 11:45:31,985 iteration 703 : loss : 0.060732, loss_ce: 0.026786
2022-01-09 11:45:33,302 iteration 704 : loss : 0.086502, loss_ce: 0.034589
2022-01-09 11:45:34,678 iteration 705 : loss : 0.126021, loss_ce: 0.044867
2022-01-09 11:45:36,066 iteration 706 : loss : 0.094084, loss_ce: 0.035477
2022-01-09 11:45:37,381 iteration 707 : loss : 0.083255, loss_ce: 0.036065
2022-01-09 11:45:38,750 iteration 708 : loss : 0.073415, loss_ce: 0.028231
2022-01-09 11:45:40,119 iteration 709 : loss : 0.120633, loss_ce: 0.055996
2022-01-09 11:45:41,523 iteration 710 : loss : 0.106942, loss_ce: 0.046436
2022-01-09 11:45:42,927 iteration 711 : loss : 0.118038, loss_ce: 0.052022
2022-01-09 11:45:44,388 iteration 712 : loss : 0.072040, loss_ce: 0.027566
2022-01-09 11:45:45,755 iteration 713 : loss : 0.082636, loss_ce: 0.034528
2022-01-09 11:45:47,198 iteration 714 : loss : 0.127387, loss_ce: 0.045576
 10%|███▏                          | 42/400 [18:24<2:36:37, 26.25s/it]2022-01-09 11:45:48,680 iteration 715 : loss : 0.090933, loss_ce: 0.035278
2022-01-09 11:45:50,107 iteration 716 : loss : 0.077177, loss_ce: 0.032888
2022-01-09 11:45:51,482 iteration 717 : loss : 0.124665, loss_ce: 0.046718
2022-01-09 11:45:52,878 iteration 718 : loss : 0.110466, loss_ce: 0.049798
2022-01-09 11:45:54,321 iteration 719 : loss : 0.109779, loss_ce: 0.043986
2022-01-09 11:45:55,700 iteration 720 : loss : 0.118527, loss_ce: 0.044332
2022-01-09 11:45:57,076 iteration 721 : loss : 0.122299, loss_ce: 0.041139
2022-01-09 11:45:58,440 iteration 722 : loss : 0.104825, loss_ce: 0.036230
2022-01-09 11:45:59,861 iteration 723 : loss : 0.110721, loss_ce: 0.053277
2022-01-09 11:46:01,214 iteration 724 : loss : 0.149187, loss_ce: 0.071068
2022-01-09 11:46:02,647 iteration 725 : loss : 0.118914, loss_ce: 0.041452
2022-01-09 11:46:04,015 iteration 726 : loss : 0.116871, loss_ce: 0.048458
2022-01-09 11:46:05,319 iteration 727 : loss : 0.077993, loss_ce: 0.029906
2022-01-09 11:46:06,723 iteration 728 : loss : 0.122052, loss_ce: 0.051483
2022-01-09 11:46:08,147 iteration 729 : loss : 0.091664, loss_ce: 0.034954
2022-01-09 11:46:09,541 iteration 730 : loss : 0.106947, loss_ce: 0.042121
2022-01-09 11:46:10,834 iteration 731 : loss : 0.088195, loss_ce: 0.029454
 11%|███▏                          | 43/400 [18:48<2:31:31, 25.47s/it]2022-01-09 11:46:12,211 iteration 732 : loss : 0.099614, loss_ce: 0.047405
2022-01-09 11:46:13,508 iteration 733 : loss : 0.117499, loss_ce: 0.058973
2022-01-09 11:46:14,921 iteration 734 : loss : 0.098307, loss_ce: 0.039409
2022-01-09 11:46:16,286 iteration 735 : loss : 0.117166, loss_ce: 0.047634
2022-01-09 11:46:17,628 iteration 736 : loss : 0.117634, loss_ce: 0.051098
2022-01-09 11:46:19,112 iteration 737 : loss : 0.151760, loss_ce: 0.063943
2022-01-09 11:46:20,408 iteration 738 : loss : 0.107996, loss_ce: 0.055150
2022-01-09 11:46:21,813 iteration 739 : loss : 0.098289, loss_ce: 0.037603
2022-01-09 11:46:23,171 iteration 740 : loss : 0.088830, loss_ce: 0.037711
2022-01-09 11:46:24,510 iteration 741 : loss : 0.128435, loss_ce: 0.049902
2022-01-09 11:46:25,808 iteration 742 : loss : 0.167208, loss_ce: 0.086612
2022-01-09 11:46:27,186 iteration 743 : loss : 0.088423, loss_ce: 0.032645
2022-01-09 11:46:28,589 iteration 744 : loss : 0.134242, loss_ce: 0.056691
2022-01-09 11:46:29,932 iteration 745 : loss : 0.155140, loss_ce: 0.035690
2022-01-09 11:46:31,273 iteration 746 : loss : 0.128700, loss_ce: 0.053766
2022-01-09 11:46:32,611 iteration 747 : loss : 0.132888, loss_ce: 0.059591
2022-01-09 11:46:33,940 iteration 748 : loss : 0.123032, loss_ce: 0.052449
 11%|███▎                          | 44/400 [19:11<2:26:53, 24.76s/it]2022-01-09 11:46:35,310 iteration 749 : loss : 0.263614, loss_ce: 0.067437
2022-01-09 11:46:36,661 iteration 750 : loss : 0.081480, loss_ce: 0.031245
2022-01-09 11:46:38,047 iteration 751 : loss : 0.098186, loss_ce: 0.038105
2022-01-09 11:46:39,434 iteration 752 : loss : 0.152605, loss_ce: 0.077631
2022-01-09 11:46:40,754 iteration 753 : loss : 0.111481, loss_ce: 0.045979
2022-01-09 11:46:42,081 iteration 754 : loss : 0.116574, loss_ce: 0.041042
2022-01-09 11:46:43,363 iteration 755 : loss : 0.111409, loss_ce: 0.054767
2022-01-09 11:46:44,657 iteration 756 : loss : 0.077041, loss_ce: 0.030428
2022-01-09 11:46:46,014 iteration 757 : loss : 0.130189, loss_ce: 0.062188
2022-01-09 11:46:47,463 iteration 758 : loss : 0.116095, loss_ce: 0.048215
2022-01-09 11:46:48,961 iteration 759 : loss : 0.126390, loss_ce: 0.052282
2022-01-09 11:46:50,314 iteration 760 : loss : 0.101284, loss_ce: 0.049515
2022-01-09 11:46:51,724 iteration 761 : loss : 0.093223, loss_ce: 0.041973
2022-01-09 11:46:53,169 iteration 762 : loss : 0.112863, loss_ce: 0.047102
2022-01-09 11:46:54,615 iteration 763 : loss : 0.133620, loss_ce: 0.053474
2022-01-09 11:46:55,979 iteration 764 : loss : 0.111359, loss_ce: 0.048412
2022-01-09 11:46:55,979 Training Data Eval:
2022-01-09 11:47:02,881   Average segmentation loss on training set: 0.0963
2022-01-09 11:47:02,881 Validation Data Eval:
2022-01-09 11:47:05,267   Average segmentation loss on validation set: 0.1447
2022-01-09 11:47:06,794 iteration 765 : loss : 0.158646, loss_ce: 0.062767
 11%|███▍                          | 45/400 [19:44<2:40:51, 27.19s/it]2022-01-09 11:47:08,280 iteration 766 : loss : 0.132304, loss_ce: 0.053339
2022-01-09 11:47:09,689 iteration 767 : loss : 0.091659, loss_ce: 0.037100
2022-01-09 11:47:11,086 iteration 768 : loss : 0.154837, loss_ce: 0.060388
2022-01-09 11:47:12,507 iteration 769 : loss : 0.127908, loss_ce: 0.057302
2022-01-09 11:47:13,824 iteration 770 : loss : 0.098380, loss_ce: 0.036146
2022-01-09 11:47:15,207 iteration 771 : loss : 0.142735, loss_ce: 0.044985
2022-01-09 11:47:16,639 iteration 772 : loss : 0.100715, loss_ce: 0.039757
2022-01-09 11:47:18,044 iteration 773 : loss : 0.149687, loss_ce: 0.044159
2022-01-09 11:47:19,531 iteration 774 : loss : 0.150451, loss_ce: 0.064011
2022-01-09 11:47:20,889 iteration 775 : loss : 0.117444, loss_ce: 0.052363
2022-01-09 11:47:22,224 iteration 776 : loss : 0.093719, loss_ce: 0.035258
2022-01-09 11:47:23,774 iteration 777 : loss : 0.123214, loss_ce: 0.058424
2022-01-09 11:47:25,087 iteration 778 : loss : 0.129905, loss_ce: 0.041314
2022-01-09 11:47:26,519 iteration 779 : loss : 0.105817, loss_ce: 0.047356
2022-01-09 11:47:27,954 iteration 780 : loss : 0.092798, loss_ce: 0.035649
2022-01-09 11:47:29,291 iteration 781 : loss : 0.085492, loss_ce: 0.042543
2022-01-09 11:47:30,590 iteration 782 : loss : 0.105114, loss_ce: 0.051144
 12%|███▍                          | 46/400 [20:08<2:34:23, 26.17s/it]2022-01-09 11:47:32,008 iteration 783 : loss : 0.105778, loss_ce: 0.054917
2022-01-09 11:47:33,345 iteration 784 : loss : 0.109753, loss_ce: 0.043086
2022-01-09 11:47:34,722 iteration 785 : loss : 0.141618, loss_ce: 0.070952
2022-01-09 11:47:36,075 iteration 786 : loss : 0.117385, loss_ce: 0.046180
2022-01-09 11:47:37,440 iteration 787 : loss : 0.130925, loss_ce: 0.062338
2022-01-09 11:47:38,873 iteration 788 : loss : 0.106491, loss_ce: 0.045150
2022-01-09 11:47:40,168 iteration 789 : loss : 0.101285, loss_ce: 0.034184
2022-01-09 11:47:41,583 iteration 790 : loss : 0.135492, loss_ce: 0.044260
2022-01-09 11:47:42,956 iteration 791 : loss : 0.081253, loss_ce: 0.031534
2022-01-09 11:47:44,276 iteration 792 : loss : 0.088364, loss_ce: 0.039932
2022-01-09 11:47:45,709 iteration 793 : loss : 0.101839, loss_ce: 0.049368
2022-01-09 11:47:46,993 iteration 794 : loss : 0.068343, loss_ce: 0.028561
2022-01-09 11:47:48,304 iteration 795 : loss : 0.100818, loss_ce: 0.038037
2022-01-09 11:47:49,616 iteration 796 : loss : 0.097459, loss_ce: 0.047348
2022-01-09 11:47:51,018 iteration 797 : loss : 0.150536, loss_ce: 0.058005
2022-01-09 11:47:52,367 iteration 798 : loss : 0.099133, loss_ce: 0.033124
2022-01-09 11:47:53,668 iteration 799 : loss : 0.102639, loss_ce: 0.050140
 12%|███▌                          | 47/400 [20:31<2:28:30, 25.24s/it]2022-01-09 11:47:55,194 iteration 800 : loss : 0.105626, loss_ce: 0.040357
2022-01-09 11:47:56,586 iteration 801 : loss : 0.111756, loss_ce: 0.050871
2022-01-09 11:47:57,962 iteration 802 : loss : 0.142388, loss_ce: 0.048233
2022-01-09 11:47:59,354 iteration 803 : loss : 0.083997, loss_ce: 0.045203
2022-01-09 11:48:00,810 iteration 804 : loss : 0.169232, loss_ce: 0.076135
2022-01-09 11:48:02,334 iteration 805 : loss : 0.099454, loss_ce: 0.035268
2022-01-09 11:48:03,691 iteration 806 : loss : 0.129892, loss_ce: 0.062588
2022-01-09 11:48:05,015 iteration 807 : loss : 0.087745, loss_ce: 0.035004
2022-01-09 11:48:06,427 iteration 808 : loss : 0.122560, loss_ce: 0.068120
2022-01-09 11:48:07,808 iteration 809 : loss : 0.083655, loss_ce: 0.030944
2022-01-09 11:48:09,151 iteration 810 : loss : 0.172079, loss_ce: 0.063125
2022-01-09 11:48:10,463 iteration 811 : loss : 0.161266, loss_ce: 0.102616
2022-01-09 11:48:11,803 iteration 812 : loss : 0.115415, loss_ce: 0.053793
2022-01-09 11:48:13,173 iteration 813 : loss : 0.167910, loss_ce: 0.058365
2022-01-09 11:48:14,652 iteration 814 : loss : 0.164537, loss_ce: 0.060487
2022-01-09 11:48:16,087 iteration 815 : loss : 0.131840, loss_ce: 0.057746
2022-01-09 11:48:17,477 iteration 816 : loss : 0.111905, loss_ce: 0.042532
 12%|███▌                          | 48/400 [20:55<2:25:32, 24.81s/it]2022-01-09 11:48:18,896 iteration 817 : loss : 0.104246, loss_ce: 0.045168
2022-01-09 11:48:20,218 iteration 818 : loss : 0.105831, loss_ce: 0.048176
2022-01-09 11:48:21,589 iteration 819 : loss : 0.085300, loss_ce: 0.036098
2022-01-09 11:48:22,891 iteration 820 : loss : 0.086200, loss_ce: 0.037172
2022-01-09 11:48:24,321 iteration 821 : loss : 0.080372, loss_ce: 0.030030
2022-01-09 11:48:25,726 iteration 822 : loss : 0.129295, loss_ce: 0.047282
2022-01-09 11:48:27,047 iteration 823 : loss : 0.076006, loss_ce: 0.028976
2022-01-09 11:48:28,368 iteration 824 : loss : 0.132808, loss_ce: 0.058719
2022-01-09 11:48:29,729 iteration 825 : loss : 0.086427, loss_ce: 0.034296
2022-01-09 11:48:31,079 iteration 826 : loss : 0.123195, loss_ce: 0.037506
2022-01-09 11:48:32,441 iteration 827 : loss : 0.148574, loss_ce: 0.049906
2022-01-09 11:48:33,798 iteration 828 : loss : 0.111856, loss_ce: 0.041277
2022-01-09 11:48:35,180 iteration 829 : loss : 0.112056, loss_ce: 0.045523
2022-01-09 11:48:36,601 iteration 830 : loss : 0.104822, loss_ce: 0.036589
2022-01-09 11:48:37,970 iteration 831 : loss : 0.136708, loss_ce: 0.059255
2022-01-09 11:48:39,319 iteration 832 : loss : 0.083980, loss_ce: 0.029267
2022-01-09 11:48:40,736 iteration 833 : loss : 0.123118, loss_ce: 0.056151
 12%|███▋                          | 49/400 [21:18<2:22:25, 24.35s/it]2022-01-09 11:48:42,166 iteration 834 : loss : 0.106125, loss_ce: 0.042845
2022-01-09 11:48:43,450 iteration 835 : loss : 0.097796, loss_ce: 0.031143
2022-01-09 11:48:44,829 iteration 836 : loss : 0.088203, loss_ce: 0.040425
2022-01-09 11:48:46,146 iteration 837 : loss : 0.097192, loss_ce: 0.050382
2022-01-09 11:48:47,434 iteration 838 : loss : 0.099380, loss_ce: 0.038802
2022-01-09 11:48:48,819 iteration 839 : loss : 0.142482, loss_ce: 0.055899
2022-01-09 11:48:50,170 iteration 840 : loss : 0.118921, loss_ce: 0.041324
2022-01-09 11:48:51,487 iteration 841 : loss : 0.121079, loss_ce: 0.049476
2022-01-09 11:48:52,835 iteration 842 : loss : 0.121597, loss_ce: 0.051655
2022-01-09 11:48:54,285 iteration 843 : loss : 0.112743, loss_ce: 0.042133
2022-01-09 11:48:55,641 iteration 844 : loss : 0.119012, loss_ce: 0.044610
2022-01-09 11:48:57,034 iteration 845 : loss : 0.143359, loss_ce: 0.055046
2022-01-09 11:48:58,517 iteration 846 : loss : 0.119380, loss_ce: 0.059884
2022-01-09 11:48:59,897 iteration 847 : loss : 0.103549, loss_ce: 0.040705
2022-01-09 11:49:01,176 iteration 848 : loss : 0.094103, loss_ce: 0.037721
2022-01-09 11:49:02,555 iteration 849 : loss : 0.096302, loss_ce: 0.039772
2022-01-09 11:49:02,556 Training Data Eval:
2022-01-09 11:49:09,435   Average segmentation loss on training set: 0.0985
2022-01-09 11:49:09,435 Validation Data Eval:
2022-01-09 11:49:11,808   Average segmentation loss on validation set: 0.1235
2022-01-09 11:49:16,876 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:49:18,365 iteration 850 : loss : 0.125169, loss_ce: 0.048969
 12%|███▊                          | 50/400 [21:56<2:45:15, 28.33s/it]2022-01-09 11:49:19,804 iteration 851 : loss : 0.079809, loss_ce: 0.036098
2022-01-09 11:49:21,145 iteration 852 : loss : 0.086691, loss_ce: 0.037418
2022-01-09 11:49:22,584 iteration 853 : loss : 0.102161, loss_ce: 0.041908
2022-01-09 11:49:23,975 iteration 854 : loss : 0.078583, loss_ce: 0.023766
2022-01-09 11:49:25,359 iteration 855 : loss : 0.092294, loss_ce: 0.039549
2022-01-09 11:49:26,769 iteration 856 : loss : 0.079470, loss_ce: 0.031005
2022-01-09 11:49:28,103 iteration 857 : loss : 0.099936, loss_ce: 0.041991
2022-01-09 11:49:29,378 iteration 858 : loss : 0.093539, loss_ce: 0.049343
2022-01-09 11:49:30,731 iteration 859 : loss : 0.083528, loss_ce: 0.029609
2022-01-09 11:49:32,040 iteration 860 : loss : 0.100051, loss_ce: 0.034140
2022-01-09 11:49:33,468 iteration 861 : loss : 0.101972, loss_ce: 0.044486
2022-01-09 11:49:34,762 iteration 862 : loss : 0.093785, loss_ce: 0.030026
2022-01-09 11:49:36,197 iteration 863 : loss : 0.120151, loss_ce: 0.053148
2022-01-09 11:49:37,598 iteration 864 : loss : 0.106833, loss_ce: 0.039583
2022-01-09 11:49:38,889 iteration 865 : loss : 0.106112, loss_ce: 0.042338
2022-01-09 11:49:40,315 iteration 866 : loss : 0.109781, loss_ce: 0.053186
2022-01-09 11:49:41,621 iteration 867 : loss : 0.076945, loss_ce: 0.028598
 13%|███▊                          | 51/400 [22:19<2:35:56, 26.81s/it]2022-01-09 11:49:43,079 iteration 868 : loss : 0.175569, loss_ce: 0.071378
2022-01-09 11:49:44,439 iteration 869 : loss : 0.092427, loss_ce: 0.038195
2022-01-09 11:49:45,717 iteration 870 : loss : 0.083256, loss_ce: 0.035505
2022-01-09 11:49:47,031 iteration 871 : loss : 0.072957, loss_ce: 0.028443
2022-01-09 11:49:48,364 iteration 872 : loss : 0.057242, loss_ce: 0.027444
2022-01-09 11:49:49,800 iteration 873 : loss : 0.098686, loss_ce: 0.036448
2022-01-09 11:49:51,088 iteration 874 : loss : 0.083944, loss_ce: 0.038920
2022-01-09 11:49:52,592 iteration 875 : loss : 0.112595, loss_ce: 0.044576
2022-01-09 11:49:53,955 iteration 876 : loss : 0.050268, loss_ce: 0.023152
2022-01-09 11:49:55,224 iteration 877 : loss : 0.068353, loss_ce: 0.026164
2022-01-09 11:49:56,627 iteration 878 : loss : 0.083196, loss_ce: 0.033510
2022-01-09 11:49:57,964 iteration 879 : loss : 0.133010, loss_ce: 0.066807
2022-01-09 11:49:59,352 iteration 880 : loss : 0.080328, loss_ce: 0.026892
2022-01-09 11:50:00,687 iteration 881 : loss : 0.129663, loss_ce: 0.057543
2022-01-09 11:50:02,092 iteration 882 : loss : 0.119611, loss_ce: 0.044704
2022-01-09 11:50:03,454 iteration 883 : loss : 0.107206, loss_ce: 0.046496
2022-01-09 11:50:04,908 iteration 884 : loss : 0.116628, loss_ce: 0.055224
 13%|███▉                          | 52/400 [22:42<2:29:22, 25.75s/it]2022-01-09 11:50:06,367 iteration 885 : loss : 0.106469, loss_ce: 0.044589
2022-01-09 11:50:07,765 iteration 886 : loss : 0.094333, loss_ce: 0.034677
2022-01-09 11:50:09,178 iteration 887 : loss : 0.164433, loss_ce: 0.080497
2022-01-09 11:50:10,554 iteration 888 : loss : 0.076975, loss_ce: 0.030016
2022-01-09 11:50:11,985 iteration 889 : loss : 0.118562, loss_ce: 0.042842
2022-01-09 11:50:13,463 iteration 890 : loss : 0.117943, loss_ce: 0.041090
2022-01-09 11:50:14,849 iteration 891 : loss : 0.067743, loss_ce: 0.026536
2022-01-09 11:50:16,220 iteration 892 : loss : 0.083002, loss_ce: 0.038437
2022-01-09 11:50:17,576 iteration 893 : loss : 0.097410, loss_ce: 0.036404
2022-01-09 11:50:19,048 iteration 894 : loss : 0.086392, loss_ce: 0.033611
2022-01-09 11:50:20,388 iteration 895 : loss : 0.136065, loss_ce: 0.042770
2022-01-09 11:50:21,815 iteration 896 : loss : 0.118983, loss_ce: 0.044799
2022-01-09 11:50:23,271 iteration 897 : loss : 0.124790, loss_ce: 0.060507
2022-01-09 11:50:24,626 iteration 898 : loss : 0.085476, loss_ce: 0.039458
2022-01-09 11:50:25,982 iteration 899 : loss : 0.101853, loss_ce: 0.046513
2022-01-09 11:50:27,299 iteration 900 : loss : 0.073983, loss_ce: 0.034148
2022-01-09 11:50:28,637 iteration 901 : loss : 0.123706, loss_ce: 0.046201
 13%|███▉                          | 53/400 [23:06<2:25:24, 25.14s/it]2022-01-09 11:50:30,093 iteration 902 : loss : 0.088791, loss_ce: 0.035696
2022-01-09 11:50:31,554 iteration 903 : loss : 0.079321, loss_ce: 0.029330
2022-01-09 11:50:32,960 iteration 904 : loss : 0.130112, loss_ce: 0.043221
2022-01-09 11:50:34,352 iteration 905 : loss : 0.105232, loss_ce: 0.047138
2022-01-09 11:50:35,712 iteration 906 : loss : 0.085104, loss_ce: 0.036052
2022-01-09 11:50:37,143 iteration 907 : loss : 0.075901, loss_ce: 0.021864
2022-01-09 11:50:38,422 iteration 908 : loss : 0.078731, loss_ce: 0.031122
2022-01-09 11:50:39,816 iteration 909 : loss : 0.086537, loss_ce: 0.035957
2022-01-09 11:50:41,120 iteration 910 : loss : 0.116704, loss_ce: 0.045298
2022-01-09 11:50:42,484 iteration 911 : loss : 0.066794, loss_ce: 0.026294
2022-01-09 11:50:43,845 iteration 912 : loss : 0.090007, loss_ce: 0.032585
2022-01-09 11:50:45,188 iteration 913 : loss : 0.100760, loss_ce: 0.035163
2022-01-09 11:50:46,545 iteration 914 : loss : 0.120282, loss_ce: 0.056723
2022-01-09 11:50:47,874 iteration 915 : loss : 0.171056, loss_ce: 0.048383
2022-01-09 11:50:49,206 iteration 916 : loss : 0.092180, loss_ce: 0.032813
2022-01-09 11:50:50,600 iteration 917 : loss : 0.110632, loss_ce: 0.041359
2022-01-09 11:50:52,007 iteration 918 : loss : 0.109012, loss_ce: 0.041858
 14%|████                          | 54/400 [23:29<2:21:54, 24.61s/it]2022-01-09 11:50:53,473 iteration 919 : loss : 0.083124, loss_ce: 0.035314
2022-01-09 11:50:54,864 iteration 920 : loss : 0.080633, loss_ce: 0.034480
2022-01-09 11:50:56,283 iteration 921 : loss : 0.097890, loss_ce: 0.035959
2022-01-09 11:50:57,712 iteration 922 : loss : 0.092149, loss_ce: 0.048664
2022-01-09 11:50:59,061 iteration 923 : loss : 0.088656, loss_ce: 0.041528
2022-01-09 11:51:00,481 iteration 924 : loss : 0.132236, loss_ce: 0.043704
2022-01-09 11:51:01,815 iteration 925 : loss : 0.064366, loss_ce: 0.023103
2022-01-09 11:51:03,235 iteration 926 : loss : 0.085536, loss_ce: 0.040229
2022-01-09 11:51:04,538 iteration 927 : loss : 0.103741, loss_ce: 0.035353
2022-01-09 11:51:05,899 iteration 928 : loss : 0.075757, loss_ce: 0.034810
2022-01-09 11:51:07,325 iteration 929 : loss : 0.112727, loss_ce: 0.041934
2022-01-09 11:51:08,714 iteration 930 : loss : 0.080619, loss_ce: 0.034969
2022-01-09 11:51:10,104 iteration 931 : loss : 0.127862, loss_ce: 0.033125
2022-01-09 11:51:11,479 iteration 932 : loss : 0.093612, loss_ce: 0.030994
2022-01-09 11:51:12,820 iteration 933 : loss : 0.113848, loss_ce: 0.045048
2022-01-09 11:51:14,191 iteration 934 : loss : 0.077897, loss_ce: 0.033385
2022-01-09 11:51:14,192 Training Data Eval:
2022-01-09 11:51:21,075   Average segmentation loss on training set: 0.0700
2022-01-09 11:51:21,075 Validation Data Eval:
2022-01-09 11:51:23,452   Average segmentation loss on validation set: 0.1176
2022-01-09 11:51:29,129 Found new lowest validation loss at iteration 934! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 11:51:30,660 iteration 935 : loss : 0.130251, loss_ce: 0.040023
 14%|████▏                         | 55/400 [24:08<2:45:43, 28.82s/it]2022-01-09 11:51:32,059 iteration 936 : loss : 0.062282, loss_ce: 0.021365
2022-01-09 11:51:33,443 iteration 937 : loss : 0.075518, loss_ce: 0.024709
2022-01-09 11:51:34,856 iteration 938 : loss : 0.102544, loss_ce: 0.039280
2022-01-09 11:51:36,180 iteration 939 : loss : 0.087476, loss_ce: 0.029439
2022-01-09 11:51:37,564 iteration 940 : loss : 0.091309, loss_ce: 0.035428
2022-01-09 11:51:39,072 iteration 941 : loss : 0.107756, loss_ce: 0.039194
2022-01-09 11:51:40,402 iteration 942 : loss : 0.072767, loss_ce: 0.026517
2022-01-09 11:51:41,750 iteration 943 : loss : 0.080468, loss_ce: 0.038074
2022-01-09 11:51:43,078 iteration 944 : loss : 0.087814, loss_ce: 0.034806
2022-01-09 11:51:44,413 iteration 945 : loss : 0.082866, loss_ce: 0.029738
2022-01-09 11:51:45,748 iteration 946 : loss : 0.063045, loss_ce: 0.029325
2022-01-09 11:51:47,075 iteration 947 : loss : 0.092223, loss_ce: 0.033023
2022-01-09 11:51:48,392 iteration 948 : loss : 0.090228, loss_ce: 0.037572
2022-01-09 11:51:49,745 iteration 949 : loss : 0.076603, loss_ce: 0.027781
2022-01-09 11:51:51,091 iteration 950 : loss : 0.088773, loss_ce: 0.039115
2022-01-09 11:51:52,492 iteration 951 : loss : 0.075813, loss_ce: 0.043467
2022-01-09 11:51:53,810 iteration 952 : loss : 0.082243, loss_ce: 0.035472
 14%|████▏                         | 56/400 [24:31<2:35:30, 27.12s/it]2022-01-09 11:51:55,253 iteration 953 : loss : 0.087597, loss_ce: 0.031521
2022-01-09 11:51:56,669 iteration 954 : loss : 0.069917, loss_ce: 0.030707
2022-01-09 11:51:57,970 iteration 955 : loss : 0.046627, loss_ce: 0.019055
2022-01-09 11:51:59,368 iteration 956 : loss : 0.078344, loss_ce: 0.028704
2022-01-09 11:52:00,752 iteration 957 : loss : 0.070538, loss_ce: 0.024353
2022-01-09 11:52:02,149 iteration 958 : loss : 0.114590, loss_ce: 0.037095
2022-01-09 11:52:03,611 iteration 959 : loss : 0.114378, loss_ce: 0.054209
2022-01-09 11:52:05,128 iteration 960 : loss : 0.077595, loss_ce: 0.035570
2022-01-09 11:52:06,470 iteration 961 : loss : 0.129601, loss_ce: 0.034134
2022-01-09 11:52:07,780 iteration 962 : loss : 0.087646, loss_ce: 0.024475
2022-01-09 11:52:09,221 iteration 963 : loss : 0.107035, loss_ce: 0.041736
2022-01-09 11:52:10,625 iteration 964 : loss : 0.080777, loss_ce: 0.039324
2022-01-09 11:52:11,978 iteration 965 : loss : 0.060314, loss_ce: 0.025727
2022-01-09 11:52:13,348 iteration 966 : loss : 0.078656, loss_ce: 0.029955
2022-01-09 11:52:14,733 iteration 967 : loss : 0.052807, loss_ce: 0.022953
2022-01-09 11:52:16,057 iteration 968 : loss : 0.126534, loss_ce: 0.048068
2022-01-09 11:52:17,449 iteration 969 : loss : 0.067077, loss_ce: 0.029233
 14%|████▎                         | 57/400 [24:55<2:29:04, 26.08s/it]2022-01-09 11:52:18,850 iteration 970 : loss : 0.088149, loss_ce: 0.045420
2022-01-09 11:52:20,244 iteration 971 : loss : 0.114672, loss_ce: 0.038980
2022-01-09 11:52:21,554 iteration 972 : loss : 0.073573, loss_ce: 0.033600
2022-01-09 11:52:22,985 iteration 973 : loss : 0.079625, loss_ce: 0.029214
2022-01-09 11:52:24,299 iteration 974 : loss : 0.085345, loss_ce: 0.039903
2022-01-09 11:52:25,713 iteration 975 : loss : 0.130783, loss_ce: 0.044554
2022-01-09 11:52:27,057 iteration 976 : loss : 0.074240, loss_ce: 0.029958
2022-01-09 11:52:28,416 iteration 977 : loss : 0.087718, loss_ce: 0.037062
2022-01-09 11:52:29,800 iteration 978 : loss : 0.062150, loss_ce: 0.021346
2022-01-09 11:52:31,144 iteration 979 : loss : 0.120461, loss_ce: 0.059078
2022-01-09 11:52:32,488 iteration 980 : loss : 0.062059, loss_ce: 0.021308
2022-01-09 11:52:33,949 iteration 981 : loss : 0.076633, loss_ce: 0.035952
2022-01-09 11:52:35,285 iteration 982 : loss : 0.074275, loss_ce: 0.036470
2022-01-09 11:52:36,701 iteration 983 : loss : 0.105811, loss_ce: 0.035066
2022-01-09 11:52:37,998 iteration 984 : loss : 0.079366, loss_ce: 0.035841
2022-01-09 11:52:39,368 iteration 985 : loss : 0.068314, loss_ce: 0.022097
2022-01-09 11:52:40,833 iteration 986 : loss : 0.097207, loss_ce: 0.040816
 14%|████▎                         | 58/400 [25:18<2:24:03, 25.27s/it]2022-01-09 11:52:42,327 iteration 987 : loss : 0.071683, loss_ce: 0.033323
2022-01-09 11:52:43,727 iteration 988 : loss : 0.079979, loss_ce: 0.034898
2022-01-09 11:52:45,109 iteration 989 : loss : 0.092919, loss_ce: 0.036637
2022-01-09 11:52:46,479 iteration 990 : loss : 0.089121, loss_ce: 0.038241
2022-01-09 11:52:47,844 iteration 991 : loss : 0.069134, loss_ce: 0.029611
2022-01-09 11:52:49,295 iteration 992 : loss : 0.072607, loss_ce: 0.031811
2022-01-09 11:52:50,697 iteration 993 : loss : 0.073222, loss_ce: 0.028277
2022-01-09 11:52:52,183 iteration 994 : loss : 0.068528, loss_ce: 0.030638
2022-01-09 11:52:53,575 iteration 995 : loss : 0.127717, loss_ce: 0.044126
2022-01-09 11:52:55,017 iteration 996 : loss : 0.060639, loss_ce: 0.031025
2022-01-09 11:52:56,451 iteration 997 : loss : 0.127169, loss_ce: 0.053881
2022-01-09 11:52:57,785 iteration 998 : loss : 0.098934, loss_ce: 0.036292
2022-01-09 11:52:59,209 iteration 999 : loss : 0.086882, loss_ce: 0.036091
2022-01-09 11:53:00,551 iteration 1000 : loss : 0.085320, loss_ce: 0.027903
2022-01-09 11:53:01,896 iteration 1001 : loss : 0.139413, loss_ce: 0.040881
2022-01-09 11:53:03,367 iteration 1002 : loss : 0.095245, loss_ce: 0.045965
2022-01-09 11:53:04,818 iteration 1003 : loss : 0.097846, loss_ce: 0.039286
 15%|████▍                         | 59/400 [25:42<2:21:25, 24.89s/it]2022-01-09 11:53:06,256 iteration 1004 : loss : 0.131205, loss_ce: 0.054826
2022-01-09 11:53:07,621 iteration 1005 : loss : 0.084561, loss_ce: 0.032897
2022-01-09 11:53:09,166 iteration 1006 : loss : 0.109170, loss_ce: 0.046552
2022-01-09 11:53:10,503 iteration 1007 : loss : 0.096421, loss_ce: 0.050121
2022-01-09 11:53:11,825 iteration 1008 : loss : 0.165896, loss_ce: 0.069464
2022-01-09 11:53:13,161 iteration 1009 : loss : 0.097036, loss_ce: 0.037057
2022-01-09 11:53:14,498 iteration 1010 : loss : 0.090413, loss_ce: 0.034299
2022-01-09 11:53:15,924 iteration 1011 : loss : 0.080270, loss_ce: 0.035896
2022-01-09 11:53:17,287 iteration 1012 : loss : 0.052982, loss_ce: 0.020501
2022-01-09 11:53:18,602 iteration 1013 : loss : 0.060173, loss_ce: 0.020649
2022-01-09 11:53:19,984 iteration 1014 : loss : 0.104335, loss_ce: 0.043999
2022-01-09 11:53:21,334 iteration 1015 : loss : 0.079795, loss_ce: 0.030672
2022-01-09 11:53:22,717 iteration 1016 : loss : 0.088626, loss_ce: 0.035836
2022-01-09 11:53:24,000 iteration 1017 : loss : 0.061765, loss_ce: 0.024071
2022-01-09 11:53:25,282 iteration 1018 : loss : 0.080530, loss_ce: 0.034695
2022-01-09 11:53:26,636 iteration 1019 : loss : 0.072018, loss_ce: 0.029427
2022-01-09 11:53:26,636 Training Data Eval:
2022-01-09 11:53:33,535   Average segmentation loss on training set: 0.0611
2022-01-09 11:53:33,535 Validation Data Eval:
2022-01-09 11:53:35,909   Average segmentation loss on validation set: 0.1242
2022-01-09 11:53:37,222 iteration 1020 : loss : 0.066820, loss_ce: 0.021338
 15%|████▌                         | 60/400 [26:14<2:33:48, 27.14s/it]2022-01-09 11:53:38,723 iteration 1021 : loss : 0.127909, loss_ce: 0.035953
2022-01-09 11:53:40,080 iteration 1022 : loss : 0.085422, loss_ce: 0.039023
2022-01-09 11:53:41,457 iteration 1023 : loss : 0.089872, loss_ce: 0.027222
2022-01-09 11:53:42,883 iteration 1024 : loss : 0.049945, loss_ce: 0.018202
2022-01-09 11:53:44,303 iteration 1025 : loss : 0.071629, loss_ce: 0.031665
2022-01-09 11:53:45,588 iteration 1026 : loss : 0.072146, loss_ce: 0.027211
2022-01-09 11:53:47,001 iteration 1027 : loss : 0.085780, loss_ce: 0.029894
2022-01-09 11:53:48,403 iteration 1028 : loss : 0.103560, loss_ce: 0.044394
2022-01-09 11:53:49,763 iteration 1029 : loss : 0.062088, loss_ce: 0.026949
2022-01-09 11:53:51,130 iteration 1030 : loss : 0.089543, loss_ce: 0.032489
2022-01-09 11:53:52,556 iteration 1031 : loss : 0.114468, loss_ce: 0.052081
2022-01-09 11:53:53,953 iteration 1032 : loss : 0.105456, loss_ce: 0.048249
2022-01-09 11:53:55,295 iteration 1033 : loss : 0.088319, loss_ce: 0.030123
2022-01-09 11:53:56,732 iteration 1034 : loss : 0.096823, loss_ce: 0.041133
2022-01-09 11:53:58,129 iteration 1035 : loss : 0.066622, loss_ce: 0.031368
2022-01-09 11:53:59,520 iteration 1036 : loss : 0.084353, loss_ce: 0.035980
2022-01-09 11:54:00,860 iteration 1037 : loss : 0.085635, loss_ce: 0.030621
 15%|████▌                         | 61/400 [26:38<2:27:22, 26.09s/it]2022-01-09 11:54:02,309 iteration 1038 : loss : 0.063319, loss_ce: 0.027156
2022-01-09 11:54:03,682 iteration 1039 : loss : 0.064530, loss_ce: 0.030542
2022-01-09 11:54:05,093 iteration 1040 : loss : 0.118669, loss_ce: 0.054385
2022-01-09 11:54:06,492 iteration 1041 : loss : 0.070363, loss_ce: 0.028981
2022-01-09 11:54:07,890 iteration 1042 : loss : 0.077397, loss_ce: 0.031059
2022-01-09 11:54:09,273 iteration 1043 : loss : 0.120900, loss_ce: 0.041383
2022-01-09 11:54:10,678 iteration 1044 : loss : 0.063412, loss_ce: 0.023779
2022-01-09 11:54:11,970 iteration 1045 : loss : 0.060101, loss_ce: 0.027764
2022-01-09 11:54:13,291 iteration 1046 : loss : 0.110251, loss_ce: 0.030761
2022-01-09 11:54:14,703 iteration 1047 : loss : 0.142236, loss_ce: 0.060396
2022-01-09 11:54:16,080 iteration 1048 : loss : 0.093342, loss_ce: 0.031391
2022-01-09 11:54:17,462 iteration 1049 : loss : 0.104881, loss_ce: 0.033827
2022-01-09 11:54:18,888 iteration 1050 : loss : 0.104345, loss_ce: 0.035786
2022-01-09 11:54:20,259 iteration 1051 : loss : 0.074911, loss_ce: 0.031101
2022-01-09 11:54:21,633 iteration 1052 : loss : 0.128615, loss_ce: 0.039599
2022-01-09 11:54:22,934 iteration 1053 : loss : 0.092033, loss_ce: 0.028907
2022-01-09 11:54:24,375 iteration 1054 : loss : 0.079623, loss_ce: 0.037169
 16%|████▋                         | 62/400 [27:02<2:22:38, 25.32s/it]2022-01-09 11:54:25,849 iteration 1055 : loss : 0.055089, loss_ce: 0.018660
2022-01-09 11:54:27,193 iteration 1056 : loss : 0.104184, loss_ce: 0.050776
2022-01-09 11:54:28,543 iteration 1057 : loss : 0.097083, loss_ce: 0.046047
2022-01-09 11:54:29,906 iteration 1058 : loss : 0.084355, loss_ce: 0.033094
2022-01-09 11:54:31,195 iteration 1059 : loss : 0.080075, loss_ce: 0.029359
2022-01-09 11:54:32,557 iteration 1060 : loss : 0.059953, loss_ce: 0.022148
2022-01-09 11:54:33,952 iteration 1061 : loss : 0.071743, loss_ce: 0.031917
2022-01-09 11:54:35,339 iteration 1062 : loss : 0.082208, loss_ce: 0.033486
2022-01-09 11:54:36,779 iteration 1063 : loss : 0.145578, loss_ce: 0.053647
2022-01-09 11:54:38,146 iteration 1064 : loss : 0.118493, loss_ce: 0.041953
2022-01-09 11:54:39,458 iteration 1065 : loss : 0.089119, loss_ce: 0.038420
2022-01-09 11:54:40,868 iteration 1066 : loss : 0.086059, loss_ce: 0.036295
2022-01-09 11:54:42,221 iteration 1067 : loss : 0.182181, loss_ce: 0.053429
2022-01-09 11:54:43,641 iteration 1068 : loss : 0.080774, loss_ce: 0.039339
2022-01-09 11:54:45,040 iteration 1069 : loss : 0.107675, loss_ce: 0.040599
2022-01-09 11:54:46,408 iteration 1070 : loss : 0.070470, loss_ce: 0.028899
2022-01-09 11:54:47,770 iteration 1071 : loss : 0.074070, loss_ce: 0.035385
 16%|████▋                         | 63/400 [27:25<2:18:56, 24.74s/it]2022-01-09 11:54:49,294 iteration 1072 : loss : 0.082542, loss_ce: 0.025772
2022-01-09 11:54:50,646 iteration 1073 : loss : 0.123124, loss_ce: 0.031833
2022-01-09 11:54:52,005 iteration 1074 : loss : 0.077428, loss_ce: 0.030933
2022-01-09 11:54:53,407 iteration 1075 : loss : 0.109566, loss_ce: 0.039829
2022-01-09 11:54:54,726 iteration 1076 : loss : 0.077716, loss_ce: 0.028118
2022-01-09 11:54:56,083 iteration 1077 : loss : 0.095457, loss_ce: 0.031118
2022-01-09 11:54:57,518 iteration 1078 : loss : 0.139947, loss_ce: 0.070065
2022-01-09 11:54:58,855 iteration 1079 : loss : 0.080204, loss_ce: 0.029391
2022-01-09 11:55:00,181 iteration 1080 : loss : 0.058479, loss_ce: 0.026751
2022-01-09 11:55:01,582 iteration 1081 : loss : 0.080098, loss_ce: 0.026996
2022-01-09 11:55:02,979 iteration 1082 : loss : 0.078216, loss_ce: 0.038561
2022-01-09 11:55:04,403 iteration 1083 : loss : 0.103141, loss_ce: 0.049622
2022-01-09 11:55:05,755 iteration 1084 : loss : 0.092275, loss_ce: 0.046238
2022-01-09 11:55:07,203 iteration 1085 : loss : 0.080615, loss_ce: 0.030989
2022-01-09 11:55:08,557 iteration 1086 : loss : 0.083953, loss_ce: 0.036514
2022-01-09 11:55:09,928 iteration 1087 : loss : 0.106356, loss_ce: 0.043490
2022-01-09 11:55:11,358 iteration 1088 : loss : 0.084272, loss_ce: 0.036798
 16%|████▊                         | 64/400 [27:49<2:16:36, 24.40s/it]2022-01-09 11:55:12,768 iteration 1089 : loss : 0.109215, loss_ce: 0.039597
2022-01-09 11:55:14,120 iteration 1090 : loss : 0.080366, loss_ce: 0.039266
2022-01-09 11:55:15,492 iteration 1091 : loss : 0.093693, loss_ce: 0.043559
2022-01-09 11:55:16,867 iteration 1092 : loss : 0.072492, loss_ce: 0.031379
2022-01-09 11:55:18,261 iteration 1093 : loss : 0.092730, loss_ce: 0.032155
2022-01-09 11:55:19,700 iteration 1094 : loss : 0.075999, loss_ce: 0.033150
2022-01-09 11:55:21,074 iteration 1095 : loss : 0.071231, loss_ce: 0.031471
2022-01-09 11:55:22,546 iteration 1096 : loss : 0.074337, loss_ce: 0.033330
2022-01-09 11:55:23,939 iteration 1097 : loss : 0.093084, loss_ce: 0.035591
2022-01-09 11:55:25,272 iteration 1098 : loss : 0.058642, loss_ce: 0.028053
2022-01-09 11:55:26,670 iteration 1099 : loss : 0.088099, loss_ce: 0.025573
2022-01-09 11:55:28,161 iteration 1100 : loss : 0.080923, loss_ce: 0.030666
2022-01-09 11:55:29,530 iteration 1101 : loss : 0.089617, loss_ce: 0.036188
2022-01-09 11:55:30,975 iteration 1102 : loss : 0.117051, loss_ce: 0.039393
2022-01-09 11:55:32,332 iteration 1103 : loss : 0.113679, loss_ce: 0.038320
2022-01-09 11:55:33,685 iteration 1104 : loss : 0.140996, loss_ce: 0.065090
2022-01-09 11:55:33,686 Training Data Eval:
2022-01-09 11:55:40,582   Average segmentation loss on training set: 0.2087
2022-01-09 11:55:40,583 Validation Data Eval:
2022-01-09 11:55:42,970   Average segmentation loss on validation set: 0.3301
2022-01-09 11:55:44,396 iteration 1105 : loss : 0.080738, loss_ce: 0.020534
 16%|████▉                         | 65/400 [28:22<2:30:40, 26.99s/it]2022-01-09 11:55:45,760 iteration 1106 : loss : 0.114787, loss_ce: 0.044612
2022-01-09 11:55:47,113 iteration 1107 : loss : 0.112659, loss_ce: 0.057115
2022-01-09 11:55:48,489 iteration 1108 : loss : 0.107808, loss_ce: 0.029901
2022-01-09 11:55:49,799 iteration 1109 : loss : 0.076003, loss_ce: 0.033100
2022-01-09 11:55:51,179 iteration 1110 : loss : 0.111186, loss_ce: 0.051401
2022-01-09 11:55:52,523 iteration 1111 : loss : 0.060095, loss_ce: 0.026804
2022-01-09 11:55:53,875 iteration 1112 : loss : 0.101232, loss_ce: 0.043963
2022-01-09 11:55:55,211 iteration 1113 : loss : 0.092684, loss_ce: 0.031895
2022-01-09 11:55:56,634 iteration 1114 : loss : 0.101433, loss_ce: 0.045049
2022-01-09 11:55:58,137 iteration 1115 : loss : 0.079449, loss_ce: 0.036318
2022-01-09 11:55:59,565 iteration 1116 : loss : 0.122079, loss_ce: 0.041223
2022-01-09 11:56:01,053 iteration 1117 : loss : 0.077345, loss_ce: 0.027485
2022-01-09 11:56:02,480 iteration 1118 : loss : 0.133461, loss_ce: 0.045955
2022-01-09 11:56:03,830 iteration 1119 : loss : 0.063367, loss_ce: 0.031214
2022-01-09 11:56:05,169 iteration 1120 : loss : 0.081162, loss_ce: 0.028414
2022-01-09 11:56:06,561 iteration 1121 : loss : 0.100461, loss_ce: 0.029390
2022-01-09 11:56:07,935 iteration 1122 : loss : 0.104016, loss_ce: 0.048061
 16%|████▉                         | 66/400 [28:45<2:24:28, 25.95s/it]2022-01-09 11:56:09,324 iteration 1123 : loss : 0.120603, loss_ce: 0.043496
2022-01-09 11:56:10,727 iteration 1124 : loss : 0.073771, loss_ce: 0.033255
2022-01-09 11:56:12,095 iteration 1125 : loss : 0.065555, loss_ce: 0.020441
2022-01-09 11:56:13,449 iteration 1126 : loss : 0.079628, loss_ce: 0.026334
2022-01-09 11:56:14,796 iteration 1127 : loss : 0.105858, loss_ce: 0.038383
2022-01-09 11:56:16,220 iteration 1128 : loss : 0.107062, loss_ce: 0.043672
2022-01-09 11:56:17,646 iteration 1129 : loss : 0.087142, loss_ce: 0.033257
2022-01-09 11:56:19,004 iteration 1130 : loss : 0.074658, loss_ce: 0.034198
2022-01-09 11:56:20,361 iteration 1131 : loss : 0.091627, loss_ce: 0.042258
2022-01-09 11:56:21,770 iteration 1132 : loss : 0.071532, loss_ce: 0.026121
2022-01-09 11:56:23,083 iteration 1133 : loss : 0.058299, loss_ce: 0.027621
2022-01-09 11:56:24,449 iteration 1134 : loss : 0.097362, loss_ce: 0.034301
2022-01-09 11:56:25,788 iteration 1135 : loss : 0.093925, loss_ce: 0.039489
2022-01-09 11:56:27,145 iteration 1136 : loss : 0.054693, loss_ce: 0.019247
2022-01-09 11:56:28,490 iteration 1137 : loss : 0.117747, loss_ce: 0.056107
2022-01-09 11:56:29,898 iteration 1138 : loss : 0.089937, loss_ce: 0.039011
2022-01-09 11:56:31,287 iteration 1139 : loss : 0.090675, loss_ce: 0.030113
 17%|█████                         | 67/400 [29:08<2:19:42, 25.17s/it]2022-01-09 11:56:32,713 iteration 1140 : loss : 0.090935, loss_ce: 0.035267
2022-01-09 11:56:34,046 iteration 1141 : loss : 0.085933, loss_ce: 0.036905
2022-01-09 11:56:35,478 iteration 1142 : loss : 0.066342, loss_ce: 0.029009
2022-01-09 11:56:36,845 iteration 1143 : loss : 0.079829, loss_ce: 0.029483
2022-01-09 11:56:38,250 iteration 1144 : loss : 0.138385, loss_ce: 0.079318
2022-01-09 11:56:39,571 iteration 1145 : loss : 0.079066, loss_ce: 0.030494
2022-01-09 11:56:40,958 iteration 1146 : loss : 0.158522, loss_ce: 0.080006
2022-01-09 11:56:42,249 iteration 1147 : loss : 0.064123, loss_ce: 0.024066
2022-01-09 11:56:43,669 iteration 1148 : loss : 0.102398, loss_ce: 0.036568
2022-01-09 11:56:44,989 iteration 1149 : loss : 0.068922, loss_ce: 0.034598
2022-01-09 11:56:46,361 iteration 1150 : loss : 0.096218, loss_ce: 0.043188
2022-01-09 11:56:47,762 iteration 1151 : loss : 0.087054, loss_ce: 0.032208
2022-01-09 11:56:49,252 iteration 1152 : loss : 0.104554, loss_ce: 0.043705
2022-01-09 11:56:50,542 iteration 1153 : loss : 0.082918, loss_ce: 0.029206
2022-01-09 11:56:51,946 iteration 1154 : loss : 0.094785, loss_ce: 0.031165
2022-01-09 11:56:53,279 iteration 1155 : loss : 0.073387, loss_ce: 0.025010
2022-01-09 11:56:54,701 iteration 1156 : loss : 0.111887, loss_ce: 0.062482
 17%|█████                         | 68/400 [29:32<2:16:21, 24.64s/it]2022-01-09 11:56:56,109 iteration 1157 : loss : 0.137460, loss_ce: 0.037159
2022-01-09 11:56:57,500 iteration 1158 : loss : 0.080864, loss_ce: 0.028624
2022-01-09 11:56:58,943 iteration 1159 : loss : 0.054352, loss_ce: 0.017473
2022-01-09 11:57:00,430 iteration 1160 : loss : 0.098846, loss_ce: 0.042256
2022-01-09 11:57:01,864 iteration 1161 : loss : 0.057642, loss_ce: 0.023965
2022-01-09 11:57:03,266 iteration 1162 : loss : 0.069745, loss_ce: 0.025603
2022-01-09 11:57:04,622 iteration 1163 : loss : 0.085926, loss_ce: 0.036943
2022-01-09 11:57:05,984 iteration 1164 : loss : 0.094778, loss_ce: 0.037296
2022-01-09 11:57:07,369 iteration 1165 : loss : 0.102338, loss_ce: 0.050970
2022-01-09 11:57:08,832 iteration 1166 : loss : 0.054553, loss_ce: 0.018021
2022-01-09 11:57:10,188 iteration 1167 : loss : 0.063119, loss_ce: 0.029825
2022-01-09 11:57:11,556 iteration 1168 : loss : 0.057673, loss_ce: 0.023585
2022-01-09 11:57:12,899 iteration 1169 : loss : 0.080241, loss_ce: 0.033107
2022-01-09 11:57:14,240 iteration 1170 : loss : 0.093594, loss_ce: 0.038436
2022-01-09 11:57:15,626 iteration 1171 : loss : 0.097915, loss_ce: 0.040307
2022-01-09 11:57:17,003 iteration 1172 : loss : 0.089919, loss_ce: 0.042571
2022-01-09 11:57:18,402 iteration 1173 : loss : 0.083033, loss_ce: 0.029203
 17%|█████▏                        | 69/400 [29:56<2:14:23, 24.36s/it]2022-01-09 11:57:19,807 iteration 1174 : loss : 0.096538, loss_ce: 0.034645
2022-01-09 11:57:21,181 iteration 1175 : loss : 0.074089, loss_ce: 0.035664
2022-01-09 11:57:22,543 iteration 1176 : loss : 0.079149, loss_ce: 0.033980
2022-01-09 11:57:23,872 iteration 1177 : loss : 0.078270, loss_ce: 0.031173
2022-01-09 11:57:25,272 iteration 1178 : loss : 0.102812, loss_ce: 0.033212
2022-01-09 11:57:26,726 iteration 1179 : loss : 0.088629, loss_ce: 0.040069
2022-01-09 11:57:28,084 iteration 1180 : loss : 0.071878, loss_ce: 0.030870
2022-01-09 11:57:29,462 iteration 1181 : loss : 0.090189, loss_ce: 0.032534
2022-01-09 11:57:30,857 iteration 1182 : loss : 0.085552, loss_ce: 0.034268
2022-01-09 11:57:32,191 iteration 1183 : loss : 0.097270, loss_ce: 0.038577
2022-01-09 11:57:33,526 iteration 1184 : loss : 0.061504, loss_ce: 0.029005
2022-01-09 11:57:34,980 iteration 1185 : loss : 0.075613, loss_ce: 0.029305
2022-01-09 11:57:36,323 iteration 1186 : loss : 0.085017, loss_ce: 0.041939
2022-01-09 11:57:37,755 iteration 1187 : loss : 0.069768, loss_ce: 0.024430
2022-01-09 11:57:39,086 iteration 1188 : loss : 0.119652, loss_ce: 0.048942
2022-01-09 11:57:40,525 iteration 1189 : loss : 0.114573, loss_ce: 0.055001
2022-01-09 11:57:40,525 Training Data Eval:
2022-01-09 11:57:47,413   Average segmentation loss on training set: 0.0676
2022-01-09 11:57:47,414 Validation Data Eval:
2022-01-09 11:57:49,796   Average segmentation loss on validation set: 0.1211
2022-01-09 11:57:51,169 iteration 1190 : loss : 0.089914, loss_ce: 0.041680
 18%|█████▎                        | 70/400 [30:28<2:27:52, 26.89s/it]2022-01-09 11:57:52,670 iteration 1191 : loss : 0.081000, loss_ce: 0.033405
2022-01-09 11:57:53,996 iteration 1192 : loss : 0.056227, loss_ce: 0.026410
2022-01-09 11:57:55,438 iteration 1193 : loss : 0.088158, loss_ce: 0.027998
2022-01-09 11:57:56,795 iteration 1194 : loss : 0.073527, loss_ce: 0.024348
2022-01-09 11:57:58,112 iteration 1195 : loss : 0.076358, loss_ce: 0.034931
2022-01-09 11:57:59,432 iteration 1196 : loss : 0.071919, loss_ce: 0.030168
2022-01-09 11:58:00,719 iteration 1197 : loss : 0.077871, loss_ce: 0.027369
2022-01-09 11:58:02,133 iteration 1198 : loss : 0.060445, loss_ce: 0.028377
2022-01-09 11:58:03,504 iteration 1199 : loss : 0.076171, loss_ce: 0.029380
2022-01-09 11:58:04,851 iteration 1200 : loss : 0.108889, loss_ce: 0.031570
2022-01-09 11:58:06,170 iteration 1201 : loss : 0.061644, loss_ce: 0.029826
2022-01-09 11:58:07,557 iteration 1202 : loss : 0.079159, loss_ce: 0.032457
2022-01-09 11:58:08,937 iteration 1203 : loss : 0.097493, loss_ce: 0.035956
2022-01-09 11:58:10,304 iteration 1204 : loss : 0.083942, loss_ce: 0.030576
2022-01-09 11:58:11,679 iteration 1205 : loss : 0.090141, loss_ce: 0.039263
2022-01-09 11:58:13,061 iteration 1206 : loss : 0.057620, loss_ce: 0.022271
2022-01-09 11:58:14,481 iteration 1207 : loss : 0.056073, loss_ce: 0.020124
 18%|█████▎                        | 71/400 [30:52<2:21:32, 25.81s/it]2022-01-09 11:58:15,941 iteration 1208 : loss : 0.071383, loss_ce: 0.032593
2022-01-09 11:58:17,327 iteration 1209 : loss : 0.076920, loss_ce: 0.029035
2022-01-09 11:58:18,703 iteration 1210 : loss : 0.073978, loss_ce: 0.026916
2022-01-09 11:58:20,027 iteration 1211 : loss : 0.061913, loss_ce: 0.023274
2022-01-09 11:58:21,332 iteration 1212 : loss : 0.087328, loss_ce: 0.023765
2022-01-09 11:58:22,812 iteration 1213 : loss : 0.072103, loss_ce: 0.029720
2022-01-09 11:58:24,181 iteration 1214 : loss : 0.083352, loss_ce: 0.031512
2022-01-09 11:58:25,555 iteration 1215 : loss : 0.128478, loss_ce: 0.063875
2022-01-09 11:58:27,042 iteration 1216 : loss : 0.085828, loss_ce: 0.033358
2022-01-09 11:58:28,426 iteration 1217 : loss : 0.050463, loss_ce: 0.017518
2022-01-09 11:58:29,754 iteration 1218 : loss : 0.055082, loss_ce: 0.019874
2022-01-09 11:58:31,055 iteration 1219 : loss : 0.064793, loss_ce: 0.021486
2022-01-09 11:58:32,416 iteration 1220 : loss : 0.069582, loss_ce: 0.027018
2022-01-09 11:58:33,748 iteration 1221 : loss : 0.084733, loss_ce: 0.033121
2022-01-09 11:58:35,176 iteration 1222 : loss : 0.088103, loss_ce: 0.031310
2022-01-09 11:58:36,522 iteration 1223 : loss : 0.068490, loss_ce: 0.027058
2022-01-09 11:58:37,926 iteration 1224 : loss : 0.105814, loss_ce: 0.041606
 18%|█████▍                        | 72/400 [31:15<2:17:13, 25.10s/it]2022-01-09 11:58:39,331 iteration 1225 : loss : 0.074092, loss_ce: 0.029222
2022-01-09 11:58:40,735 iteration 1226 : loss : 0.113804, loss_ce: 0.051801
2022-01-09 11:58:42,057 iteration 1227 : loss : 0.080882, loss_ce: 0.028141
2022-01-09 11:58:43,410 iteration 1228 : loss : 0.078152, loss_ce: 0.029927
2022-01-09 11:58:44,711 iteration 1229 : loss : 0.065553, loss_ce: 0.022003
2022-01-09 11:58:46,038 iteration 1230 : loss : 0.075643, loss_ce: 0.031428
2022-01-09 11:58:47,435 iteration 1231 : loss : 0.072300, loss_ce: 0.031075
2022-01-09 11:58:48,757 iteration 1232 : loss : 0.097924, loss_ce: 0.043671
2022-01-09 11:58:50,095 iteration 1233 : loss : 0.055425, loss_ce: 0.024134
2022-01-09 11:58:51,494 iteration 1234 : loss : 0.071021, loss_ce: 0.030202
2022-01-09 11:58:52,891 iteration 1235 : loss : 0.125020, loss_ce: 0.039627
2022-01-09 11:58:54,216 iteration 1236 : loss : 0.064174, loss_ce: 0.025553
2022-01-09 11:58:55,498 iteration 1237 : loss : 0.113489, loss_ce: 0.043858
2022-01-09 11:58:56,911 iteration 1238 : loss : 0.077473, loss_ce: 0.025984
2022-01-09 11:58:58,224 iteration 1239 : loss : 0.063232, loss_ce: 0.023356
2022-01-09 11:58:59,543 iteration 1240 : loss : 0.062278, loss_ce: 0.025717
2022-01-09 11:59:00,863 iteration 1241 : loss : 0.093699, loss_ce: 0.043871
 18%|█████▍                        | 73/400 [31:38<2:13:15, 24.45s/it]2022-01-09 11:59:02,339 iteration 1242 : loss : 0.089079, loss_ce: 0.051606
2022-01-09 11:59:03,670 iteration 1243 : loss : 0.088830, loss_ce: 0.029076
2022-01-09 11:59:05,008 iteration 1244 : loss : 0.078921, loss_ce: 0.037423
2022-01-09 11:59:06,389 iteration 1245 : loss : 0.085452, loss_ce: 0.031578
2022-01-09 11:59:07,758 iteration 1246 : loss : 0.066701, loss_ce: 0.021896
2022-01-09 11:59:09,091 iteration 1247 : loss : 0.071067, loss_ce: 0.028889
2022-01-09 11:59:10,474 iteration 1248 : loss : 0.076392, loss_ce: 0.027516
2022-01-09 11:59:11,800 iteration 1249 : loss : 0.055355, loss_ce: 0.019070
2022-01-09 11:59:13,257 iteration 1250 : loss : 0.078797, loss_ce: 0.032550
2022-01-09 11:59:14,740 iteration 1251 : loss : 0.105707, loss_ce: 0.052600
2022-01-09 11:59:16,134 iteration 1252 : loss : 0.070515, loss_ce: 0.029879
2022-01-09 11:59:17,426 iteration 1253 : loss : 0.072687, loss_ce: 0.028040
2022-01-09 11:59:18,789 iteration 1254 : loss : 0.070965, loss_ce: 0.034237
2022-01-09 11:59:20,165 iteration 1255 : loss : 0.084250, loss_ce: 0.030592
2022-01-09 11:59:21,520 iteration 1256 : loss : 0.072694, loss_ce: 0.030460
2022-01-09 11:59:22,880 iteration 1257 : loss : 0.120502, loss_ce: 0.037289
2022-01-09 11:59:24,263 iteration 1258 : loss : 0.065102, loss_ce: 0.029918
 18%|█████▌                        | 74/400 [32:01<2:11:08, 24.14s/it]2022-01-09 11:59:25,643 iteration 1259 : loss : 0.070056, loss_ce: 0.033797
2022-01-09 11:59:27,057 iteration 1260 : loss : 0.060029, loss_ce: 0.027388
2022-01-09 11:59:28,405 iteration 1261 : loss : 0.081577, loss_ce: 0.034221
2022-01-09 11:59:29,855 iteration 1262 : loss : 0.060365, loss_ce: 0.026541
2022-01-09 11:59:31,327 iteration 1263 : loss : 0.143991, loss_ce: 0.065307
2022-01-09 11:59:32,682 iteration 1264 : loss : 0.059557, loss_ce: 0.025763
2022-01-09 11:59:34,106 iteration 1265 : loss : 0.077424, loss_ce: 0.040213
2022-01-09 11:59:35,529 iteration 1266 : loss : 0.073756, loss_ce: 0.025880
2022-01-09 11:59:36,927 iteration 1267 : loss : 0.097216, loss_ce: 0.040310
2022-01-09 11:59:38,268 iteration 1268 : loss : 0.094148, loss_ce: 0.040498
2022-01-09 11:59:39,710 iteration 1269 : loss : 0.130294, loss_ce: 0.032891
2022-01-09 11:59:41,118 iteration 1270 : loss : 0.084012, loss_ce: 0.037849
2022-01-09 11:59:42,486 iteration 1271 : loss : 0.082306, loss_ce: 0.037602
2022-01-09 11:59:43,779 iteration 1272 : loss : 0.073134, loss_ce: 0.024393
2022-01-09 11:59:45,127 iteration 1273 : loss : 0.100011, loss_ce: 0.040151
2022-01-09 11:59:46,496 iteration 1274 : loss : 0.081141, loss_ce: 0.024864
2022-01-09 11:59:46,497 Training Data Eval:
2022-01-09 11:59:53,379   Average segmentation loss on training set: 0.1151
2022-01-09 11:59:53,379 Validation Data Eval:
2022-01-09 11:59:55,756   Average segmentation loss on validation set: 0.2460
2022-01-09 11:59:57,115 iteration 1275 : loss : 0.092855, loss_ce: 0.033017
 19%|█████▋                        | 75/400 [32:34<2:24:53, 26.75s/it]2022-01-09 11:59:58,456 iteration 1276 : loss : 0.076385, loss_ce: 0.030185
2022-01-09 11:59:59,927 iteration 1277 : loss : 0.067091, loss_ce: 0.023549
2022-01-09 12:00:01,260 iteration 1278 : loss : 0.058355, loss_ce: 0.019659
2022-01-09 12:00:02,722 iteration 1279 : loss : 0.087819, loss_ce: 0.029705
2022-01-09 12:00:04,116 iteration 1280 : loss : 0.105148, loss_ce: 0.052330
2022-01-09 12:00:05,497 iteration 1281 : loss : 0.111855, loss_ce: 0.055655
2022-01-09 12:00:06,816 iteration 1282 : loss : 0.052573, loss_ce: 0.024514
2022-01-09 12:00:08,240 iteration 1283 : loss : 0.177226, loss_ce: 0.061374
2022-01-09 12:00:09,567 iteration 1284 : loss : 0.070714, loss_ce: 0.026719
2022-01-09 12:00:10,999 iteration 1285 : loss : 0.075463, loss_ce: 0.029992
2022-01-09 12:00:12,461 iteration 1286 : loss : 0.157169, loss_ce: 0.052311
2022-01-09 12:00:13,811 iteration 1287 : loss : 0.056933, loss_ce: 0.029541
2022-01-09 12:00:15,150 iteration 1288 : loss : 0.097104, loss_ce: 0.047332
2022-01-09 12:00:16,520 iteration 1289 : loss : 0.129705, loss_ce: 0.059922
2022-01-09 12:00:17,926 iteration 1290 : loss : 0.107616, loss_ce: 0.034015
2022-01-09 12:00:19,343 iteration 1291 : loss : 0.106982, loss_ce: 0.044075
2022-01-09 12:00:20,774 iteration 1292 : loss : 0.085669, loss_ce: 0.031835
 19%|█████▋                        | 76/400 [32:58<2:19:26, 25.82s/it]2022-01-09 12:00:22,229 iteration 1293 : loss : 0.088120, loss_ce: 0.034072
2022-01-09 12:00:23,630 iteration 1294 : loss : 0.103813, loss_ce: 0.039937
2022-01-09 12:00:25,076 iteration 1295 : loss : 0.101145, loss_ce: 0.028088
2022-01-09 12:00:26,473 iteration 1296 : loss : 0.110306, loss_ce: 0.039734
2022-01-09 12:00:27,811 iteration 1297 : loss : 0.080458, loss_ce: 0.043826
2022-01-09 12:00:29,116 iteration 1298 : loss : 0.054994, loss_ce: 0.022457
2022-01-09 12:00:30,495 iteration 1299 : loss : 0.087430, loss_ce: 0.041634
2022-01-09 12:00:31,858 iteration 1300 : loss : 0.071889, loss_ce: 0.026596
2022-01-09 12:00:33,257 iteration 1301 : loss : 0.041859, loss_ce: 0.016334
2022-01-09 12:00:34,603 iteration 1302 : loss : 0.078710, loss_ce: 0.034583
2022-01-09 12:00:35,953 iteration 1303 : loss : 0.074415, loss_ce: 0.029349
2022-01-09 12:00:37,331 iteration 1304 : loss : 0.071033, loss_ce: 0.027964
2022-01-09 12:00:38,700 iteration 1305 : loss : 0.091166, loss_ce: 0.033729
2022-01-09 12:00:40,113 iteration 1306 : loss : 0.129262, loss_ce: 0.045631
2022-01-09 12:00:41,521 iteration 1307 : loss : 0.094076, loss_ce: 0.053470
2022-01-09 12:00:42,905 iteration 1308 : loss : 0.103056, loss_ce: 0.036930
2022-01-09 12:00:44,218 iteration 1309 : loss : 0.083585, loss_ce: 0.028164
 19%|█████▊                        | 77/400 [33:21<2:15:11, 25.11s/it]2022-01-09 12:00:45,719 iteration 1310 : loss : 0.097348, loss_ce: 0.041172
2022-01-09 12:00:47,070 iteration 1311 : loss : 0.076285, loss_ce: 0.033832
2022-01-09 12:00:48,411 iteration 1312 : loss : 0.058526, loss_ce: 0.025477
2022-01-09 12:00:49,755 iteration 1313 : loss : 0.100962, loss_ce: 0.031194
2022-01-09 12:00:51,129 iteration 1314 : loss : 0.093834, loss_ce: 0.055509
2022-01-09 12:00:52,526 iteration 1315 : loss : 0.117703, loss_ce: 0.051179
2022-01-09 12:00:54,008 iteration 1316 : loss : 0.089850, loss_ce: 0.037903
2022-01-09 12:00:55,373 iteration 1317 : loss : 0.090107, loss_ce: 0.048281
2022-01-09 12:00:56,830 iteration 1318 : loss : 0.167325, loss_ce: 0.036918
2022-01-09 12:00:58,244 iteration 1319 : loss : 0.089177, loss_ce: 0.041414
2022-01-09 12:00:59,600 iteration 1320 : loss : 0.048887, loss_ce: 0.020317
2022-01-09 12:01:00,995 iteration 1321 : loss : 0.101137, loss_ce: 0.026238
2022-01-09 12:01:02,316 iteration 1322 : loss : 0.084417, loss_ce: 0.038372
2022-01-09 12:01:03,673 iteration 1323 : loss : 0.086530, loss_ce: 0.030359
2022-01-09 12:01:05,066 iteration 1324 : loss : 0.082524, loss_ce: 0.039002
2022-01-09 12:01:06,499 iteration 1325 : loss : 0.082224, loss_ce: 0.030628
2022-01-09 12:01:07,784 iteration 1326 : loss : 0.084377, loss_ce: 0.034587
 20%|█████▊                        | 78/400 [33:45<2:12:16, 24.65s/it]2022-01-09 12:01:09,187 iteration 1327 : loss : 0.077822, loss_ce: 0.027159
2022-01-09 12:01:10,582 iteration 1328 : loss : 0.098809, loss_ce: 0.053266
2022-01-09 12:01:11,955 iteration 1329 : loss : 0.063909, loss_ce: 0.026329
2022-01-09 12:01:13,300 iteration 1330 : loss : 0.129521, loss_ce: 0.038355
2022-01-09 12:01:14,612 iteration 1331 : loss : 0.063256, loss_ce: 0.032435
2022-01-09 12:01:15,937 iteration 1332 : loss : 0.067763, loss_ce: 0.024946
2022-01-09 12:01:17,290 iteration 1333 : loss : 0.088205, loss_ce: 0.030752
2022-01-09 12:01:18,661 iteration 1334 : loss : 0.061648, loss_ce: 0.023934
2022-01-09 12:01:20,002 iteration 1335 : loss : 0.058943, loss_ce: 0.023660
2022-01-09 12:01:21,348 iteration 1336 : loss : 0.074804, loss_ce: 0.031362
2022-01-09 12:01:22,772 iteration 1337 : loss : 0.081663, loss_ce: 0.034346
2022-01-09 12:01:24,174 iteration 1338 : loss : 0.072755, loss_ce: 0.027454
2022-01-09 12:01:25,569 iteration 1339 : loss : 0.070813, loss_ce: 0.030402
2022-01-09 12:01:26,960 iteration 1340 : loss : 0.079653, loss_ce: 0.037548
2022-01-09 12:01:28,340 iteration 1341 : loss : 0.080530, loss_ce: 0.029851
2022-01-09 12:01:29,702 iteration 1342 : loss : 0.095030, loss_ce: 0.037412
2022-01-09 12:01:31,018 iteration 1343 : loss : 0.064604, loss_ce: 0.025788
 20%|█████▉                        | 79/400 [34:08<2:09:35, 24.22s/it]2022-01-09 12:01:32,443 iteration 1344 : loss : 0.068395, loss_ce: 0.029143
2022-01-09 12:01:33,883 iteration 1345 : loss : 0.108321, loss_ce: 0.047518
2022-01-09 12:01:35,233 iteration 1346 : loss : 0.094116, loss_ce: 0.040816
2022-01-09 12:01:36,670 iteration 1347 : loss : 0.081105, loss_ce: 0.036659
2022-01-09 12:01:38,141 iteration 1348 : loss : 0.103005, loss_ce: 0.040749
2022-01-09 12:01:39,515 iteration 1349 : loss : 0.083614, loss_ce: 0.030571
2022-01-09 12:01:40,892 iteration 1350 : loss : 0.086275, loss_ce: 0.040693
2022-01-09 12:01:42,261 iteration 1351 : loss : 0.069402, loss_ce: 0.033880
2022-01-09 12:01:43,682 iteration 1352 : loss : 0.070291, loss_ce: 0.032145
2022-01-09 12:01:45,088 iteration 1353 : loss : 0.070119, loss_ce: 0.025670
2022-01-09 12:01:46,503 iteration 1354 : loss : 0.116909, loss_ce: 0.040684
2022-01-09 12:01:47,981 iteration 1355 : loss : 0.218677, loss_ce: 0.062207
2022-01-09 12:01:49,381 iteration 1356 : loss : 0.059709, loss_ce: 0.024297
2022-01-09 12:01:50,791 iteration 1357 : loss : 0.068508, loss_ce: 0.026597
2022-01-09 12:01:52,219 iteration 1358 : loss : 0.084182, loss_ce: 0.028501
2022-01-09 12:01:53,598 iteration 1359 : loss : 0.078278, loss_ce: 0.031890
2022-01-09 12:01:53,599 Training Data Eval:
2022-01-09 12:02:00,477   Average segmentation loss on training set: 0.0586
2022-01-09 12:02:00,478 Validation Data Eval:
2022-01-09 12:02:02,847   Average segmentation loss on validation set: 0.0906
2022-01-09 12:02:08,537 Found new lowest validation loss at iteration 1359! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 12:02:10,047 iteration 1360 : loss : 0.073259, loss_ce: 0.021687
 20%|██████                        | 80/400 [34:47<2:32:52, 28.66s/it]2022-01-09 12:02:11,565 iteration 1361 : loss : 0.082684, loss_ce: 0.032065
2022-01-09 12:02:12,942 iteration 1362 : loss : 0.078915, loss_ce: 0.027184
2022-01-09 12:02:14,467 iteration 1363 : loss : 0.129124, loss_ce: 0.057356
2022-01-09 12:02:15,722 iteration 1364 : loss : 0.088115, loss_ce: 0.022723
2022-01-09 12:02:17,159 iteration 1365 : loss : 0.096498, loss_ce: 0.040835
2022-01-09 12:02:18,550 iteration 1366 : loss : 0.102163, loss_ce: 0.043766
2022-01-09 12:02:19,914 iteration 1367 : loss : 0.063615, loss_ce: 0.025863
2022-01-09 12:02:21,294 iteration 1368 : loss : 0.048983, loss_ce: 0.024522
2022-01-09 12:02:22,633 iteration 1369 : loss : 0.079871, loss_ce: 0.029355
2022-01-09 12:02:24,040 iteration 1370 : loss : 0.093379, loss_ce: 0.047808
2022-01-09 12:02:25,387 iteration 1371 : loss : 0.055370, loss_ce: 0.019103
2022-01-09 12:02:26,725 iteration 1372 : loss : 0.072652, loss_ce: 0.027308
2022-01-09 12:02:28,055 iteration 1373 : loss : 0.078143, loss_ce: 0.030903
2022-01-09 12:02:29,437 iteration 1374 : loss : 0.068893, loss_ce: 0.024826
2022-01-09 12:02:30,828 iteration 1375 : loss : 0.096302, loss_ce: 0.043942
2022-01-09 12:02:32,210 iteration 1376 : loss : 0.079265, loss_ce: 0.028070
2022-01-09 12:02:33,567 iteration 1377 : loss : 0.046437, loss_ce: 0.019905
 20%|██████                        | 81/400 [35:11<2:24:12, 27.12s/it]2022-01-09 12:02:34,937 iteration 1378 : loss : 0.070931, loss_ce: 0.029574
2022-01-09 12:02:36,348 iteration 1379 : loss : 0.054716, loss_ce: 0.021104
2022-01-09 12:02:37,727 iteration 1380 : loss : 0.081520, loss_ce: 0.036897
2022-01-09 12:02:39,184 iteration 1381 : loss : 0.071692, loss_ce: 0.029560
2022-01-09 12:02:40,483 iteration 1382 : loss : 0.069430, loss_ce: 0.028305
2022-01-09 12:02:41,925 iteration 1383 : loss : 0.086908, loss_ce: 0.033291
2022-01-09 12:02:43,306 iteration 1384 : loss : 0.080705, loss_ce: 0.030591
2022-01-09 12:02:44,619 iteration 1385 : loss : 0.098666, loss_ce: 0.044501
2022-01-09 12:02:45,962 iteration 1386 : loss : 0.080040, loss_ce: 0.038332
2022-01-09 12:02:47,320 iteration 1387 : loss : 0.069750, loss_ce: 0.028495
2022-01-09 12:02:48,689 iteration 1388 : loss : 0.088691, loss_ce: 0.025652
2022-01-09 12:02:50,053 iteration 1389 : loss : 0.084002, loss_ce: 0.031844
2022-01-09 12:02:51,466 iteration 1390 : loss : 0.083159, loss_ce: 0.038477
2022-01-09 12:02:52,908 iteration 1391 : loss : 0.061208, loss_ce: 0.023791
2022-01-09 12:02:54,318 iteration 1392 : loss : 0.074036, loss_ce: 0.038169
2022-01-09 12:02:55,666 iteration 1393 : loss : 0.077528, loss_ce: 0.030178
2022-01-09 12:02:57,096 iteration 1394 : loss : 0.064687, loss_ce: 0.029174
 20%|██████▏                       | 82/400 [35:34<2:18:01, 26.04s/it]2022-01-09 12:02:58,509 iteration 1395 : loss : 0.070317, loss_ce: 0.031571
2022-01-09 12:02:59,857 iteration 1396 : loss : 0.078100, loss_ce: 0.038031
2022-01-09 12:03:01,287 iteration 1397 : loss : 0.096044, loss_ce: 0.035465
2022-01-09 12:03:02,751 iteration 1398 : loss : 0.086761, loss_ce: 0.037806
2022-01-09 12:03:04,052 iteration 1399 : loss : 0.066887, loss_ce: 0.031387
2022-01-09 12:03:05,348 iteration 1400 : loss : 0.070295, loss_ce: 0.021452
2022-01-09 12:03:06,778 iteration 1401 : loss : 0.067679, loss_ce: 0.023762
2022-01-09 12:03:08,163 iteration 1402 : loss : 0.069918, loss_ce: 0.027116
2022-01-09 12:03:09,490 iteration 1403 : loss : 0.067815, loss_ce: 0.027817
2022-01-09 12:03:10,812 iteration 1404 : loss : 0.047477, loss_ce: 0.021715
2022-01-09 12:03:12,198 iteration 1405 : loss : 0.069672, loss_ce: 0.026508
2022-01-09 12:03:13,576 iteration 1406 : loss : 0.083344, loss_ce: 0.035070
2022-01-09 12:03:14,984 iteration 1407 : loss : 0.053276, loss_ce: 0.022188
2022-01-09 12:03:16,354 iteration 1408 : loss : 0.068005, loss_ce: 0.025593
2022-01-09 12:03:17,732 iteration 1409 : loss : 0.098248, loss_ce: 0.034917
2022-01-09 12:03:19,174 iteration 1410 : loss : 0.056306, loss_ce: 0.021534
2022-01-09 12:03:20,523 iteration 1411 : loss : 0.089358, loss_ce: 0.034182
 21%|██████▏                       | 83/400 [35:58<2:13:26, 25.26s/it]2022-01-09 12:03:21,922 iteration 1412 : loss : 0.087850, loss_ce: 0.035256
2022-01-09 12:03:23,359 iteration 1413 : loss : 0.084455, loss_ce: 0.027031
2022-01-09 12:03:24,718 iteration 1414 : loss : 0.107433, loss_ce: 0.044634
2022-01-09 12:03:26,138 iteration 1415 : loss : 0.094074, loss_ce: 0.028674
2022-01-09 12:03:27,497 iteration 1416 : loss : 0.069936, loss_ce: 0.025952
2022-01-09 12:03:28,840 iteration 1417 : loss : 0.079027, loss_ce: 0.034024
2022-01-09 12:03:30,183 iteration 1418 : loss : 0.104155, loss_ce: 0.051191
2022-01-09 12:03:31,502 iteration 1419 : loss : 0.062565, loss_ce: 0.026913
2022-01-09 12:03:32,910 iteration 1420 : loss : 0.095978, loss_ce: 0.036880
2022-01-09 12:03:34,366 iteration 1421 : loss : 0.116958, loss_ce: 0.049515
2022-01-09 12:03:35,766 iteration 1422 : loss : 0.087948, loss_ce: 0.027563
2022-01-09 12:03:37,078 iteration 1423 : loss : 0.079927, loss_ce: 0.033825
2022-01-09 12:03:38,482 iteration 1424 : loss : 0.081945, loss_ce: 0.039490
2022-01-09 12:03:39,877 iteration 1425 : loss : 0.057908, loss_ce: 0.023774
2022-01-09 12:03:41,211 iteration 1426 : loss : 0.084246, loss_ce: 0.034916
2022-01-09 12:03:42,580 iteration 1427 : loss : 0.074676, loss_ce: 0.027140
2022-01-09 12:03:43,860 iteration 1428 : loss : 0.130724, loss_ce: 0.050037
 21%|██████▎                       | 84/400 [36:21<2:10:00, 24.68s/it]2022-01-09 12:03:45,361 iteration 1429 : loss : 0.080796, loss_ce: 0.037186
2022-01-09 12:03:46,698 iteration 1430 : loss : 0.227985, loss_ce: 0.071914
2022-01-09 12:03:48,068 iteration 1431 : loss : 0.121533, loss_ce: 0.061508
2022-01-09 12:03:49,455 iteration 1432 : loss : 0.081570, loss_ce: 0.036155
2022-01-09 12:03:50,883 iteration 1433 : loss : 0.092725, loss_ce: 0.034774
2022-01-09 12:03:52,242 iteration 1434 : loss : 0.076881, loss_ce: 0.032279
2022-01-09 12:03:53,641 iteration 1435 : loss : 0.068212, loss_ce: 0.029681
2022-01-09 12:03:55,049 iteration 1436 : loss : 0.068063, loss_ce: 0.030585
2022-01-09 12:03:56,465 iteration 1437 : loss : 0.079851, loss_ce: 0.026829
2022-01-09 12:03:57,857 iteration 1438 : loss : 0.066377, loss_ce: 0.031178
2022-01-09 12:03:59,282 iteration 1439 : loss : 0.089772, loss_ce: 0.033967
2022-01-09 12:04:00,700 iteration 1440 : loss : 0.073487, loss_ce: 0.034194
2022-01-09 12:04:02,063 iteration 1441 : loss : 0.051909, loss_ce: 0.022777
2022-01-09 12:04:03,448 iteration 1442 : loss : 0.058481, loss_ce: 0.023653
2022-01-09 12:04:04,851 iteration 1443 : loss : 0.093898, loss_ce: 0.036345
2022-01-09 12:04:06,232 iteration 1444 : loss : 0.161969, loss_ce: 0.050422
2022-01-09 12:04:06,233 Training Data Eval:
2022-01-09 12:04:13,128   Average segmentation loss on training set: 0.0729
2022-01-09 12:04:13,128 Validation Data Eval:
2022-01-09 12:04:15,503   Average segmentation loss on validation set: 0.1040
2022-01-09 12:04:16,877 iteration 1445 : loss : 0.082535, loss_ce: 0.029033
 21%|██████▍                       | 85/400 [36:54<2:22:41, 27.18s/it]2022-01-09 12:04:18,395 iteration 1446 : loss : 0.063077, loss_ce: 0.021780
2022-01-09 12:04:19,759 iteration 1447 : loss : 0.078380, loss_ce: 0.024867
2022-01-09 12:04:21,081 iteration 1448 : loss : 0.084151, loss_ce: 0.044068
2022-01-09 12:04:22,484 iteration 1449 : loss : 0.092535, loss_ce: 0.034228
2022-01-09 12:04:23,956 iteration 1450 : loss : 0.062574, loss_ce: 0.028883
2022-01-09 12:04:25,441 iteration 1451 : loss : 0.106527, loss_ce: 0.036943
2022-01-09 12:04:26,795 iteration 1452 : loss : 0.071845, loss_ce: 0.024017
2022-01-09 12:04:28,156 iteration 1453 : loss : 0.081924, loss_ce: 0.038846
2022-01-09 12:04:29,557 iteration 1454 : loss : 0.104196, loss_ce: 0.034708
2022-01-09 12:04:30,903 iteration 1455 : loss : 0.060462, loss_ce: 0.026738
2022-01-09 12:04:32,358 iteration 1456 : loss : 0.151031, loss_ce: 0.048654
2022-01-09 12:04:33,736 iteration 1457 : loss : 0.065633, loss_ce: 0.025679
2022-01-09 12:04:35,102 iteration 1458 : loss : 0.068161, loss_ce: 0.021597
2022-01-09 12:04:36,439 iteration 1459 : loss : 0.069563, loss_ce: 0.019433
2022-01-09 12:04:37,878 iteration 1460 : loss : 0.106750, loss_ce: 0.045019
2022-01-09 12:04:39,265 iteration 1461 : loss : 0.094577, loss_ce: 0.039999
2022-01-09 12:04:40,692 iteration 1462 : loss : 0.050189, loss_ce: 0.019567
 22%|██████▍                       | 86/400 [37:18<2:16:58, 26.17s/it]2022-01-09 12:04:42,128 iteration 1463 : loss : 0.063942, loss_ce: 0.026627
2022-01-09 12:04:43,482 iteration 1464 : loss : 0.069439, loss_ce: 0.028102
2022-01-09 12:04:44,891 iteration 1465 : loss : 0.087234, loss_ce: 0.024647
2022-01-09 12:04:46,305 iteration 1466 : loss : 0.056562, loss_ce: 0.023440
2022-01-09 12:04:47,695 iteration 1467 : loss : 0.064478, loss_ce: 0.029004
2022-01-09 12:04:49,055 iteration 1468 : loss : 0.056277, loss_ce: 0.024973
2022-01-09 12:04:50,433 iteration 1469 : loss : 0.072244, loss_ce: 0.030089
2022-01-09 12:04:51,842 iteration 1470 : loss : 0.103153, loss_ce: 0.033156
2022-01-09 12:04:53,243 iteration 1471 : loss : 0.093745, loss_ce: 0.056271
2022-01-09 12:04:54,531 iteration 1472 : loss : 0.066383, loss_ce: 0.023463
2022-01-09 12:04:55,994 iteration 1473 : loss : 0.058504, loss_ce: 0.022665
2022-01-09 12:04:57,418 iteration 1474 : loss : 0.070572, loss_ce: 0.023846
2022-01-09 12:04:58,795 iteration 1475 : loss : 0.065894, loss_ce: 0.026683
2022-01-09 12:05:00,253 iteration 1476 : loss : 0.143160, loss_ce: 0.052543
2022-01-09 12:05:01,683 iteration 1477 : loss : 0.067755, loss_ce: 0.024102
2022-01-09 12:05:03,137 iteration 1478 : loss : 0.084113, loss_ce: 0.031711
2022-01-09 12:05:04,477 iteration 1479 : loss : 0.077672, loss_ce: 0.029333
 22%|██████▌                       | 87/400 [37:42<2:12:48, 25.46s/it]2022-01-09 12:05:05,959 iteration 1480 : loss : 0.077446, loss_ce: 0.027666
2022-01-09 12:05:07,316 iteration 1481 : loss : 0.054652, loss_ce: 0.022481
2022-01-09 12:05:08,688 iteration 1482 : loss : 0.061568, loss_ce: 0.025943
2022-01-09 12:05:10,138 iteration 1483 : loss : 0.072926, loss_ce: 0.026396
2022-01-09 12:05:11,646 iteration 1484 : loss : 0.087497, loss_ce: 0.031295
2022-01-09 12:05:13,087 iteration 1485 : loss : 0.059505, loss_ce: 0.026093
2022-01-09 12:05:14,450 iteration 1486 : loss : 0.084400, loss_ce: 0.035265
2022-01-09 12:05:15,831 iteration 1487 : loss : 0.070092, loss_ce: 0.029531
2022-01-09 12:05:17,228 iteration 1488 : loss : 0.080510, loss_ce: 0.032811
2022-01-09 12:05:18,628 iteration 1489 : loss : 0.060792, loss_ce: 0.025356
2022-01-09 12:05:20,050 iteration 1490 : loss : 0.077826, loss_ce: 0.032652
2022-01-09 12:05:21,503 iteration 1491 : loss : 0.068809, loss_ce: 0.028737
2022-01-09 12:05:22,842 iteration 1492 : loss : 0.049614, loss_ce: 0.021608
2022-01-09 12:05:24,259 iteration 1493 : loss : 0.076743, loss_ce: 0.027978
2022-01-09 12:05:25,615 iteration 1494 : loss : 0.072641, loss_ce: 0.027285
2022-01-09 12:05:26,972 iteration 1495 : loss : 0.064649, loss_ce: 0.027314
2022-01-09 12:05:28,352 iteration 1496 : loss : 0.107726, loss_ce: 0.027866
 22%|██████▌                       | 88/400 [38:06<2:09:54, 24.98s/it]2022-01-09 12:05:29,745 iteration 1497 : loss : 0.092202, loss_ce: 0.040846
2022-01-09 12:05:31,097 iteration 1498 : loss : 0.058389, loss_ce: 0.018434
2022-01-09 12:05:32,513 iteration 1499 : loss : 0.081727, loss_ce: 0.033385
2022-01-09 12:05:33,826 iteration 1500 : loss : 0.087007, loss_ce: 0.029969
2022-01-09 12:05:35,187 iteration 1501 : loss : 0.068979, loss_ce: 0.026685
2022-01-09 12:05:36,592 iteration 1502 : loss : 0.062421, loss_ce: 0.023715
2022-01-09 12:05:37,949 iteration 1503 : loss : 0.042753, loss_ce: 0.018651
2022-01-09 12:05:39,308 iteration 1504 : loss : 0.067922, loss_ce: 0.024249
2022-01-09 12:05:40,629 iteration 1505 : loss : 0.065969, loss_ce: 0.022058
2022-01-09 12:05:41,995 iteration 1506 : loss : 0.090361, loss_ce: 0.034519
2022-01-09 12:05:43,369 iteration 1507 : loss : 0.089987, loss_ce: 0.029415
2022-01-09 12:05:44,712 iteration 1508 : loss : 0.125724, loss_ce: 0.073568
2022-01-09 12:05:46,073 iteration 1509 : loss : 0.084005, loss_ce: 0.033079
2022-01-09 12:05:47,403 iteration 1510 : loss : 0.044263, loss_ce: 0.017265
2022-01-09 12:05:48,789 iteration 1511 : loss : 0.075766, loss_ce: 0.033713
2022-01-09 12:05:50,128 iteration 1512 : loss : 0.062275, loss_ce: 0.022515
2022-01-09 12:05:51,517 iteration 1513 : loss : 0.064423, loss_ce: 0.030080
 22%|██████▋                       | 89/400 [38:29<2:06:39, 24.44s/it]2022-01-09 12:05:52,965 iteration 1514 : loss : 0.077519, loss_ce: 0.027728
2022-01-09 12:05:54,337 iteration 1515 : loss : 0.074547, loss_ce: 0.031094
2022-01-09 12:05:55,729 iteration 1516 : loss : 0.065957, loss_ce: 0.025161
2022-01-09 12:05:57,092 iteration 1517 : loss : 0.045421, loss_ce: 0.021077
2022-01-09 12:05:58,489 iteration 1518 : loss : 0.068997, loss_ce: 0.020133
2022-01-09 12:05:59,874 iteration 1519 : loss : 0.071986, loss_ce: 0.029341
2022-01-09 12:06:01,215 iteration 1520 : loss : 0.057516, loss_ce: 0.018522
2022-01-09 12:06:02,569 iteration 1521 : loss : 0.069608, loss_ce: 0.023484
2022-01-09 12:06:04,037 iteration 1522 : loss : 0.057421, loss_ce: 0.020198
2022-01-09 12:06:05,362 iteration 1523 : loss : 0.089226, loss_ce: 0.032790
2022-01-09 12:06:06,754 iteration 1524 : loss : 0.098376, loss_ce: 0.036752
2022-01-09 12:06:08,100 iteration 1525 : loss : 0.062628, loss_ce: 0.025521
2022-01-09 12:06:09,529 iteration 1526 : loss : 0.056377, loss_ce: 0.023255
2022-01-09 12:06:10,966 iteration 1527 : loss : 0.086451, loss_ce: 0.044333
2022-01-09 12:06:12,380 iteration 1528 : loss : 0.076959, loss_ce: 0.030771
2022-01-09 12:06:13,727 iteration 1529 : loss : 0.069488, loss_ce: 0.026538
2022-01-09 12:06:13,727 Training Data Eval:
2022-01-09 12:06:20,610   Average segmentation loss on training set: 0.0549
2022-01-09 12:06:20,610 Validation Data Eval:
2022-01-09 12:06:22,985   Average segmentation loss on validation set: 0.1535
2022-01-09 12:06:24,343 iteration 1530 : loss : 0.078020, loss_ce: 0.033433
 22%|██████▊                       | 90/400 [39:02<2:19:16, 26.96s/it]2022-01-09 12:06:25,897 iteration 1531 : loss : 0.055000, loss_ce: 0.022601
2022-01-09 12:06:27,317 iteration 1532 : loss : 0.055384, loss_ce: 0.021026
2022-01-09 12:06:28,655 iteration 1533 : loss : 0.064697, loss_ce: 0.027160
2022-01-09 12:06:30,077 iteration 1534 : loss : 0.080316, loss_ce: 0.027066
2022-01-09 12:06:31,439 iteration 1535 : loss : 0.048252, loss_ce: 0.018678
2022-01-09 12:06:32,781 iteration 1536 : loss : 0.114269, loss_ce: 0.067175
2022-01-09 12:06:34,122 iteration 1537 : loss : 0.069563, loss_ce: 0.020531
2022-01-09 12:06:35,445 iteration 1538 : loss : 0.182370, loss_ce: 0.039070
2022-01-09 12:06:36,813 iteration 1539 : loss : 0.061078, loss_ce: 0.022848
2022-01-09 12:06:38,182 iteration 1540 : loss : 0.078252, loss_ce: 0.035880
2022-01-09 12:06:39,469 iteration 1541 : loss : 0.059184, loss_ce: 0.022313
2022-01-09 12:06:40,848 iteration 1542 : loss : 0.073042, loss_ce: 0.033349
2022-01-09 12:06:42,203 iteration 1543 : loss : 0.072946, loss_ce: 0.025553
2022-01-09 12:06:43,514 iteration 1544 : loss : 0.065355, loss_ce: 0.024239
2022-01-09 12:06:44,875 iteration 1545 : loss : 0.068221, loss_ce: 0.027546
2022-01-09 12:06:46,229 iteration 1546 : loss : 0.059571, loss_ce: 0.026351
2022-01-09 12:06:47,578 iteration 1547 : loss : 0.073713, loss_ce: 0.026537
 23%|██████▊                       | 91/400 [39:25<2:13:03, 25.84s/it]2022-01-09 12:06:49,007 iteration 1548 : loss : 0.056194, loss_ce: 0.020147
2022-01-09 12:06:50,321 iteration 1549 : loss : 0.054876, loss_ce: 0.024118
2022-01-09 12:06:51,783 iteration 1550 : loss : 0.075838, loss_ce: 0.034202
2022-01-09 12:06:53,156 iteration 1551 : loss : 0.069822, loss_ce: 0.028641
2022-01-09 12:06:54,533 iteration 1552 : loss : 0.080536, loss_ce: 0.040219
2022-01-09 12:06:55,824 iteration 1553 : loss : 0.039855, loss_ce: 0.020992
2022-01-09 12:06:57,154 iteration 1554 : loss : 0.073223, loss_ce: 0.028967
2022-01-09 12:06:58,581 iteration 1555 : loss : 0.055451, loss_ce: 0.020460
2022-01-09 12:07:00,076 iteration 1556 : loss : 0.095915, loss_ce: 0.041748
2022-01-09 12:07:01,515 iteration 1557 : loss : 0.097718, loss_ce: 0.028961
2022-01-09 12:07:02,897 iteration 1558 : loss : 0.066398, loss_ce: 0.020315
2022-01-09 12:07:04,365 iteration 1559 : loss : 0.085099, loss_ce: 0.039498
2022-01-09 12:07:05,721 iteration 1560 : loss : 0.077617, loss_ce: 0.022439
2022-01-09 12:07:07,079 iteration 1561 : loss : 0.102433, loss_ce: 0.035411
2022-01-09 12:07:08,457 iteration 1562 : loss : 0.102338, loss_ce: 0.044573
2022-01-09 12:07:09,822 iteration 1563 : loss : 0.078093, loss_ce: 0.036223
2022-01-09 12:07:11,203 iteration 1564 : loss : 0.072640, loss_ce: 0.023585
 23%|██████▉                       | 92/400 [39:48<2:09:13, 25.17s/it]2022-01-09 12:07:12,656 iteration 1565 : loss : 0.056919, loss_ce: 0.021436
2022-01-09 12:07:14,013 iteration 1566 : loss : 0.088048, loss_ce: 0.036889
2022-01-09 12:07:15,422 iteration 1567 : loss : 0.098392, loss_ce: 0.031062
2022-01-09 12:07:16,893 iteration 1568 : loss : 0.076254, loss_ce: 0.035826
2022-01-09 12:07:18,240 iteration 1569 : loss : 0.072309, loss_ce: 0.030161
2022-01-09 12:07:19,627 iteration 1570 : loss : 0.074149, loss_ce: 0.031625
2022-01-09 12:07:20,982 iteration 1571 : loss : 0.084280, loss_ce: 0.033345
2022-01-09 12:07:22,355 iteration 1572 : loss : 0.096860, loss_ce: 0.047876
2022-01-09 12:07:23,750 iteration 1573 : loss : 0.082442, loss_ce: 0.025557
2022-01-09 12:07:25,142 iteration 1574 : loss : 0.068521, loss_ce: 0.035883
2022-01-09 12:07:26,557 iteration 1575 : loss : 0.093261, loss_ce: 0.040280
2022-01-09 12:07:27,917 iteration 1576 : loss : 0.085454, loss_ce: 0.030534
2022-01-09 12:07:29,243 iteration 1577 : loss : 0.137493, loss_ce: 0.057489
2022-01-09 12:07:30,524 iteration 1578 : loss : 0.071267, loss_ce: 0.031515
2022-01-09 12:07:31,882 iteration 1579 : loss : 0.078637, loss_ce: 0.028305
2022-01-09 12:07:33,271 iteration 1580 : loss : 0.087183, loss_ce: 0.026766
2022-01-09 12:07:34,661 iteration 1581 : loss : 0.072273, loss_ce: 0.028559
 23%|██████▉                       | 93/400 [40:12<2:06:10, 24.66s/it]2022-01-09 12:07:36,132 iteration 1582 : loss : 0.086551, loss_ce: 0.037591
2022-01-09 12:07:37,510 iteration 1583 : loss : 0.111219, loss_ce: 0.038407
2022-01-09 12:07:38,841 iteration 1584 : loss : 0.054514, loss_ce: 0.018790
2022-01-09 12:07:40,198 iteration 1585 : loss : 0.071746, loss_ce: 0.032196
2022-01-09 12:07:41,498 iteration 1586 : loss : 0.078146, loss_ce: 0.035437
2022-01-09 12:07:42,850 iteration 1587 : loss : 0.071880, loss_ce: 0.025998
2022-01-09 12:07:44,217 iteration 1588 : loss : 0.072010, loss_ce: 0.025948
2022-01-09 12:07:45,606 iteration 1589 : loss : 0.077737, loss_ce: 0.034236
2022-01-09 12:07:47,053 iteration 1590 : loss : 0.075795, loss_ce: 0.030307
2022-01-09 12:07:48,403 iteration 1591 : loss : 0.069599, loss_ce: 0.034749
2022-01-09 12:07:49,695 iteration 1592 : loss : 0.064355, loss_ce: 0.032931
2022-01-09 12:07:51,069 iteration 1593 : loss : 0.070773, loss_ce: 0.026563
2022-01-09 12:07:52,425 iteration 1594 : loss : 0.063901, loss_ce: 0.021439
2022-01-09 12:07:53,748 iteration 1595 : loss : 0.073518, loss_ce: 0.037373
2022-01-09 12:07:55,041 iteration 1596 : loss : 0.091989, loss_ce: 0.030237
2022-01-09 12:07:56,422 iteration 1597 : loss : 0.111549, loss_ce: 0.030068
2022-01-09 12:07:57,827 iteration 1598 : loss : 0.097242, loss_ce: 0.037678
 24%|███████                       | 94/400 [40:35<2:03:28, 24.21s/it]2022-01-09 12:07:59,293 iteration 1599 : loss : 0.055187, loss_ce: 0.024641
2022-01-09 12:08:00,647 iteration 1600 : loss : 0.064607, loss_ce: 0.029507
2022-01-09 12:08:02,050 iteration 1601 : loss : 0.079183, loss_ce: 0.033078
2022-01-09 12:08:03,435 iteration 1602 : loss : 0.106309, loss_ce: 0.033851
2022-01-09 12:08:04,796 iteration 1603 : loss : 0.066963, loss_ce: 0.023801
2022-01-09 12:08:06,141 iteration 1604 : loss : 0.080182, loss_ce: 0.025160
2022-01-09 12:08:07,491 iteration 1605 : loss : 0.082871, loss_ce: 0.030463
2022-01-09 12:08:08,866 iteration 1606 : loss : 0.064265, loss_ce: 0.026330
2022-01-09 12:08:10,229 iteration 1607 : loss : 0.088804, loss_ce: 0.036017
2022-01-09 12:08:11,672 iteration 1608 : loss : 0.097406, loss_ce: 0.038910
2022-01-09 12:08:13,025 iteration 1609 : loss : 0.057806, loss_ce: 0.021469
2022-01-09 12:08:14,479 iteration 1610 : loss : 0.066630, loss_ce: 0.025042
2022-01-09 12:08:15,932 iteration 1611 : loss : 0.069461, loss_ce: 0.031681
2022-01-09 12:08:17,397 iteration 1612 : loss : 0.048582, loss_ce: 0.020190
2022-01-09 12:08:18,777 iteration 1613 : loss : 0.065112, loss_ce: 0.021854
2022-01-09 12:08:20,209 iteration 1614 : loss : 0.071347, loss_ce: 0.033099
2022-01-09 12:08:20,210 Training Data Eval:
2022-01-09 12:08:27,088   Average segmentation loss on training set: 0.0598
2022-01-09 12:08:27,089 Validation Data Eval:
2022-01-09 12:08:29,459   Average segmentation loss on validation set: 0.0953
2022-01-09 12:08:30,876 iteration 1615 : loss : 0.101307, loss_ce: 0.032485
 24%|███████▏                      | 95/400 [41:08<2:16:33, 26.86s/it]2022-01-09 12:08:32,309 iteration 1616 : loss : 0.056100, loss_ce: 0.022058
2022-01-09 12:08:33,668 iteration 1617 : loss : 0.082875, loss_ce: 0.034346
2022-01-09 12:08:34,987 iteration 1618 : loss : 0.079313, loss_ce: 0.024430
2022-01-09 12:08:36,354 iteration 1619 : loss : 0.071605, loss_ce: 0.040222
2022-01-09 12:08:37,841 iteration 1620 : loss : 0.058377, loss_ce: 0.020092
2022-01-09 12:08:39,328 iteration 1621 : loss : 0.067936, loss_ce: 0.023325
2022-01-09 12:08:40,702 iteration 1622 : loss : 0.063842, loss_ce: 0.024731
2022-01-09 12:08:42,035 iteration 1623 : loss : 0.056451, loss_ce: 0.019369
2022-01-09 12:08:43,431 iteration 1624 : loss : 0.074564, loss_ce: 0.030382
2022-01-09 12:08:44,739 iteration 1625 : loss : 0.057686, loss_ce: 0.019488
2022-01-09 12:08:46,089 iteration 1626 : loss : 0.065271, loss_ce: 0.030825
2022-01-09 12:08:47,570 iteration 1627 : loss : 0.075137, loss_ce: 0.029233
2022-01-09 12:08:49,033 iteration 1628 : loss : 0.069733, loss_ce: 0.031619
2022-01-09 12:08:50,381 iteration 1629 : loss : 0.050591, loss_ce: 0.019271
2022-01-09 12:08:51,709 iteration 1630 : loss : 0.042378, loss_ce: 0.017718
2022-01-09 12:08:53,041 iteration 1631 : loss : 0.097680, loss_ce: 0.033942
2022-01-09 12:08:54,386 iteration 1632 : loss : 0.063298, loss_ce: 0.022863
 24%|███████▏                      | 96/400 [41:32<2:11:00, 25.86s/it]2022-01-09 12:08:55,821 iteration 1633 : loss : 0.077463, loss_ce: 0.026290
2022-01-09 12:08:57,213 iteration 1634 : loss : 0.072290, loss_ce: 0.028383
2022-01-09 12:08:58,563 iteration 1635 : loss : 0.077323, loss_ce: 0.022438
2022-01-09 12:08:59,865 iteration 1636 : loss : 0.069194, loss_ce: 0.033514
2022-01-09 12:09:01,252 iteration 1637 : loss : 0.051229, loss_ce: 0.018229
2022-01-09 12:09:02,622 iteration 1638 : loss : 0.072920, loss_ce: 0.034717
2022-01-09 12:09:04,046 iteration 1639 : loss : 0.052720, loss_ce: 0.023250
2022-01-09 12:09:05,385 iteration 1640 : loss : 0.071311, loss_ce: 0.024982
2022-01-09 12:09:06,831 iteration 1641 : loss : 0.058086, loss_ce: 0.026227
2022-01-09 12:09:08,158 iteration 1642 : loss : 0.050500, loss_ce: 0.019276
2022-01-09 12:09:09,466 iteration 1643 : loss : 0.064450, loss_ce: 0.021786
2022-01-09 12:09:10,886 iteration 1644 : loss : 0.066429, loss_ce: 0.027794
2022-01-09 12:09:12,349 iteration 1645 : loss : 0.076692, loss_ce: 0.035387
2022-01-09 12:09:13,689 iteration 1646 : loss : 0.066563, loss_ce: 0.024945
2022-01-09 12:09:15,045 iteration 1647 : loss : 0.097905, loss_ce: 0.033716
2022-01-09 12:09:16,337 iteration 1648 : loss : 0.051676, loss_ce: 0.022194
2022-01-09 12:09:17,643 iteration 1649 : loss : 0.048996, loss_ce: 0.019797
 24%|███████▎                      | 97/400 [41:55<2:06:38, 25.08s/it]2022-01-09 12:09:19,174 iteration 1650 : loss : 0.084601, loss_ce: 0.044311
2022-01-09 12:09:20,529 iteration 1651 : loss : 0.113504, loss_ce: 0.043680
2022-01-09 12:09:21,911 iteration 1652 : loss : 0.080709, loss_ce: 0.029819
2022-01-09 12:09:23,258 iteration 1653 : loss : 0.064401, loss_ce: 0.020102
2022-01-09 12:09:24,611 iteration 1654 : loss : 0.077792, loss_ce: 0.025959
2022-01-09 12:09:25,940 iteration 1655 : loss : 0.080521, loss_ce: 0.027828
2022-01-09 12:09:27,356 iteration 1656 : loss : 0.074704, loss_ce: 0.034560
2022-01-09 12:09:28,713 iteration 1657 : loss : 0.054632, loss_ce: 0.023495
2022-01-09 12:09:30,132 iteration 1658 : loss : 0.121887, loss_ce: 0.078039
2022-01-09 12:09:31,584 iteration 1659 : loss : 0.062733, loss_ce: 0.028449
2022-01-09 12:09:32,967 iteration 1660 : loss : 0.045836, loss_ce: 0.017746
2022-01-09 12:09:34,283 iteration 1661 : loss : 0.057385, loss_ce: 0.020280
2022-01-09 12:09:35,697 iteration 1662 : loss : 0.060995, loss_ce: 0.023485
2022-01-09 12:09:37,127 iteration 1663 : loss : 0.053814, loss_ce: 0.021882
2022-01-09 12:09:38,506 iteration 1664 : loss : 0.089654, loss_ce: 0.034715
2022-01-09 12:09:39,860 iteration 1665 : loss : 0.068592, loss_ce: 0.030986
2022-01-09 12:09:41,245 iteration 1666 : loss : 0.081893, loss_ce: 0.025997
 24%|███████▎                      | 98/400 [42:18<2:03:58, 24.63s/it]2022-01-09 12:09:42,627 iteration 1667 : loss : 0.057367, loss_ce: 0.028954
2022-01-09 12:09:44,074 iteration 1668 : loss : 0.063905, loss_ce: 0.020399
2022-01-09 12:09:45,483 iteration 1669 : loss : 0.072762, loss_ce: 0.025548
2022-01-09 12:09:46,784 iteration 1670 : loss : 0.048494, loss_ce: 0.016933
2022-01-09 12:09:48,109 iteration 1671 : loss : 0.051496, loss_ce: 0.023257
2022-01-09 12:09:49,542 iteration 1672 : loss : 0.074793, loss_ce: 0.028692
2022-01-09 12:09:50,947 iteration 1673 : loss : 0.087419, loss_ce: 0.030974
2022-01-09 12:09:52,413 iteration 1674 : loss : 0.087364, loss_ce: 0.031406
2022-01-09 12:09:53,761 iteration 1675 : loss : 0.079124, loss_ce: 0.025180
2022-01-09 12:09:55,191 iteration 1676 : loss : 0.062464, loss_ce: 0.029535
2022-01-09 12:09:56,588 iteration 1677 : loss : 0.108905, loss_ce: 0.036596
2022-01-09 12:09:57,998 iteration 1678 : loss : 0.058595, loss_ce: 0.024386
2022-01-09 12:09:59,358 iteration 1679 : loss : 0.105377, loss_ce: 0.051430
2022-01-09 12:10:00,761 iteration 1680 : loss : 0.065831, loss_ce: 0.030068
2022-01-09 12:10:02,124 iteration 1681 : loss : 0.064115, loss_ce: 0.025266
2022-01-09 12:10:03,498 iteration 1682 : loss : 0.068570, loss_ce: 0.027683
2022-01-09 12:10:04,840 iteration 1683 : loss : 0.081020, loss_ce: 0.039453
 25%|███████▍                      | 99/400 [42:42<2:02:01, 24.32s/it]2022-01-09 12:10:06,306 iteration 1684 : loss : 0.091465, loss_ce: 0.031911
2022-01-09 12:10:07,643 iteration 1685 : loss : 0.049375, loss_ce: 0.025633
2022-01-09 12:10:08,950 iteration 1686 : loss : 0.050010, loss_ce: 0.018739
2022-01-09 12:10:10,292 iteration 1687 : loss : 0.052441, loss_ce: 0.019973
2022-01-09 12:10:11,671 iteration 1688 : loss : 0.065512, loss_ce: 0.030377
2022-01-09 12:10:13,002 iteration 1689 : loss : 0.052198, loss_ce: 0.020648
2022-01-09 12:10:14,397 iteration 1690 : loss : 0.089768, loss_ce: 0.039048
2022-01-09 12:10:15,751 iteration 1691 : loss : 0.055790, loss_ce: 0.024003
2022-01-09 12:10:17,162 iteration 1692 : loss : 0.081190, loss_ce: 0.029272
2022-01-09 12:10:18,630 iteration 1693 : loss : 0.063708, loss_ce: 0.030040
2022-01-09 12:10:20,030 iteration 1694 : loss : 0.063358, loss_ce: 0.023331
2022-01-09 12:10:21,332 iteration 1695 : loss : 0.059060, loss_ce: 0.020501
2022-01-09 12:10:22,688 iteration 1696 : loss : 0.078282, loss_ce: 0.027968
2022-01-09 12:10:24,098 iteration 1697 : loss : 0.059037, loss_ce: 0.015293
2022-01-09 12:10:25,450 iteration 1698 : loss : 0.060183, loss_ce: 0.021091
2022-01-09 12:10:26,723 iteration 1699 : loss : 0.060115, loss_ce: 0.028527
2022-01-09 12:10:26,723 Training Data Eval:
2022-01-09 12:10:33,604   Average segmentation loss on training set: 0.0481
2022-01-09 12:10:33,604 Validation Data Eval:
2022-01-09 12:10:35,979   Average segmentation loss on validation set: 0.1596
2022-01-09 12:10:37,363 iteration 1700 : loss : 0.064762, loss_ce: 0.022743
 25%|███████▎                     | 100/400 [43:15<2:13:54, 26.78s/it]2022-01-09 12:10:38,825 iteration 1701 : loss : 0.085747, loss_ce: 0.043461
2022-01-09 12:10:40,203 iteration 1702 : loss : 0.086671, loss_ce: 0.042441
2022-01-09 12:10:41,527 iteration 1703 : loss : 0.053455, loss_ce: 0.020521
2022-01-09 12:10:42,965 iteration 1704 : loss : 0.051743, loss_ce: 0.021018
2022-01-09 12:10:44,336 iteration 1705 : loss : 0.071125, loss_ce: 0.033884
2022-01-09 12:10:45,724 iteration 1706 : loss : 0.052135, loss_ce: 0.023541
2022-01-09 12:10:47,092 iteration 1707 : loss : 0.064639, loss_ce: 0.024272
2022-01-09 12:10:48,493 iteration 1708 : loss : 0.079267, loss_ce: 0.028631
2022-01-09 12:10:49,799 iteration 1709 : loss : 0.053089, loss_ce: 0.022705
2022-01-09 12:10:51,092 iteration 1710 : loss : 0.042988, loss_ce: 0.019347
2022-01-09 12:10:52,467 iteration 1711 : loss : 0.062122, loss_ce: 0.024364
2022-01-09 12:10:53,791 iteration 1712 : loss : 0.056992, loss_ce: 0.019733
2022-01-09 12:10:55,104 iteration 1713 : loss : 0.055479, loss_ce: 0.020456
2022-01-09 12:10:56,553 iteration 1714 : loss : 0.088353, loss_ce: 0.028768
2022-01-09 12:10:57,959 iteration 1715 : loss : 0.107350, loss_ce: 0.030912
2022-01-09 12:10:59,406 iteration 1716 : loss : 0.070626, loss_ce: 0.024963
2022-01-09 12:11:00,776 iteration 1717 : loss : 0.063447, loss_ce: 0.021456
 25%|███████▎                     | 101/400 [43:38<2:08:26, 25.77s/it]2022-01-09 12:11:02,177 iteration 1718 : loss : 0.064201, loss_ce: 0.022879
2022-01-09 12:11:03,565 iteration 1719 : loss : 0.078124, loss_ce: 0.034076
2022-01-09 12:11:04,845 iteration 1720 : loss : 0.043754, loss_ce: 0.015858
2022-01-09 12:11:06,204 iteration 1721 : loss : 0.058488, loss_ce: 0.026466
2022-01-09 12:11:07,620 iteration 1722 : loss : 0.059719, loss_ce: 0.020651
2022-01-09 12:11:08,989 iteration 1723 : loss : 0.097721, loss_ce: 0.038932
2022-01-09 12:11:10,396 iteration 1724 : loss : 0.046622, loss_ce: 0.017206
2022-01-09 12:11:11,881 iteration 1725 : loss : 0.088124, loss_ce: 0.033385
2022-01-09 12:11:13,327 iteration 1726 : loss : 0.069253, loss_ce: 0.019160
2022-01-09 12:11:14,715 iteration 1727 : loss : 0.061982, loss_ce: 0.027988
2022-01-09 12:11:16,072 iteration 1728 : loss : 0.076766, loss_ce: 0.022789
2022-01-09 12:11:17,438 iteration 1729 : loss : 0.052525, loss_ce: 0.022182
2022-01-09 12:11:18,827 iteration 1730 : loss : 0.069425, loss_ce: 0.028594
2022-01-09 12:11:20,268 iteration 1731 : loss : 0.074908, loss_ce: 0.027117
2022-01-09 12:11:21,646 iteration 1732 : loss : 0.048498, loss_ce: 0.025030
2022-01-09 12:11:23,095 iteration 1733 : loss : 0.071939, loss_ce: 0.033259
2022-01-09 12:11:24,425 iteration 1734 : loss : 0.046221, loss_ce: 0.015489
 26%|███████▍                     | 102/400 [44:02<2:04:49, 25.13s/it]2022-01-09 12:11:25,816 iteration 1735 : loss : 0.097559, loss_ce: 0.033352
2022-01-09 12:11:27,177 iteration 1736 : loss : 0.051503, loss_ce: 0.023439
2022-01-09 12:11:28,547 iteration 1737 : loss : 0.056267, loss_ce: 0.024168
2022-01-09 12:11:29,891 iteration 1738 : loss : 0.046730, loss_ce: 0.016995
2022-01-09 12:11:31,351 iteration 1739 : loss : 0.064059, loss_ce: 0.027221
2022-01-09 12:11:32,759 iteration 1740 : loss : 0.076367, loss_ce: 0.028846
2022-01-09 12:11:34,123 iteration 1741 : loss : 0.049287, loss_ce: 0.018954
2022-01-09 12:11:35,594 iteration 1742 : loss : 0.064626, loss_ce: 0.029735
2022-01-09 12:11:36,954 iteration 1743 : loss : 0.068026, loss_ce: 0.018883
2022-01-09 12:11:38,357 iteration 1744 : loss : 0.047556, loss_ce: 0.017569
2022-01-09 12:11:39,869 iteration 1745 : loss : 0.072521, loss_ce: 0.032664
2022-01-09 12:11:41,234 iteration 1746 : loss : 0.043160, loss_ce: 0.016385
2022-01-09 12:11:42,588 iteration 1747 : loss : 0.050475, loss_ce: 0.021851
2022-01-09 12:11:43,966 iteration 1748 : loss : 0.071898, loss_ce: 0.030728
2022-01-09 12:11:45,341 iteration 1749 : loss : 0.047031, loss_ce: 0.016437
2022-01-09 12:11:46,664 iteration 1750 : loss : 0.058023, loss_ce: 0.026639
2022-01-09 12:11:48,124 iteration 1751 : loss : 0.080724, loss_ce: 0.030078
 26%|███████▍                     | 103/400 [44:25<2:02:16, 24.70s/it]2022-01-09 12:11:49,526 iteration 1752 : loss : 0.048813, loss_ce: 0.018105
2022-01-09 12:11:50,860 iteration 1753 : loss : 0.057547, loss_ce: 0.028246
2022-01-09 12:11:52,209 iteration 1754 : loss : 0.054078, loss_ce: 0.020640
2022-01-09 12:11:53,577 iteration 1755 : loss : 0.060821, loss_ce: 0.024394
2022-01-09 12:11:54,960 iteration 1756 : loss : 0.082759, loss_ce: 0.039795
2022-01-09 12:11:56,396 iteration 1757 : loss : 0.073151, loss_ce: 0.040135
2022-01-09 12:11:57,806 iteration 1758 : loss : 0.089474, loss_ce: 0.034342
2022-01-09 12:11:59,113 iteration 1759 : loss : 0.103704, loss_ce: 0.036354
2022-01-09 12:12:00,471 iteration 1760 : loss : 0.062856, loss_ce: 0.024819
2022-01-09 12:12:01,803 iteration 1761 : loss : 0.047349, loss_ce: 0.019463
2022-01-09 12:12:03,169 iteration 1762 : loss : 0.056516, loss_ce: 0.020518
2022-01-09 12:12:04,462 iteration 1763 : loss : 0.053044, loss_ce: 0.022042
2022-01-09 12:12:05,922 iteration 1764 : loss : 0.052562, loss_ce: 0.016428
2022-01-09 12:12:07,303 iteration 1765 : loss : 0.064182, loss_ce: 0.022213
2022-01-09 12:12:08,731 iteration 1766 : loss : 0.040321, loss_ce: 0.014368
2022-01-09 12:12:10,130 iteration 1767 : loss : 0.058889, loss_ce: 0.020468
2022-01-09 12:12:11,495 iteration 1768 : loss : 0.062962, loss_ce: 0.024019
 26%|███████▌                     | 104/400 [44:49<1:59:54, 24.31s/it]2022-01-09 12:12:12,962 iteration 1769 : loss : 0.059847, loss_ce: 0.021801
2022-01-09 12:12:14,322 iteration 1770 : loss : 0.060141, loss_ce: 0.019809
2022-01-09 12:12:15,656 iteration 1771 : loss : 0.038234, loss_ce: 0.011873
2022-01-09 12:12:17,077 iteration 1772 : loss : 0.080926, loss_ce: 0.050086
2022-01-09 12:12:18,472 iteration 1773 : loss : 0.066714, loss_ce: 0.025533
2022-01-09 12:12:19,817 iteration 1774 : loss : 0.060828, loss_ce: 0.019696
2022-01-09 12:12:21,169 iteration 1775 : loss : 0.056329, loss_ce: 0.019939
2022-01-09 12:12:22,511 iteration 1776 : loss : 0.059209, loss_ce: 0.021291
2022-01-09 12:12:23,834 iteration 1777 : loss : 0.060607, loss_ce: 0.024040
2022-01-09 12:12:25,244 iteration 1778 : loss : 0.071152, loss_ce: 0.033095
2022-01-09 12:12:26,622 iteration 1779 : loss : 0.056828, loss_ce: 0.024453
2022-01-09 12:12:28,024 iteration 1780 : loss : 0.112111, loss_ce: 0.040091
2022-01-09 12:12:29,421 iteration 1781 : loss : 0.058015, loss_ce: 0.022007
2022-01-09 12:12:30,730 iteration 1782 : loss : 0.122397, loss_ce: 0.029817
2022-01-09 12:12:32,065 iteration 1783 : loss : 0.059817, loss_ce: 0.026331
2022-01-09 12:12:33,465 iteration 1784 : loss : 0.058409, loss_ce: 0.031109
2022-01-09 12:12:33,466 Training Data Eval:
2022-01-09 12:12:40,345   Average segmentation loss on training set: 0.0514
2022-01-09 12:12:40,345 Validation Data Eval:
2022-01-09 12:12:42,722   Average segmentation loss on validation set: 0.0924
2022-01-09 12:12:44,089 iteration 1785 : loss : 0.064590, loss_ce: 0.025714
 26%|███████▌                     | 105/400 [45:21<2:11:43, 26.79s/it]2022-01-09 12:12:45,556 iteration 1786 : loss : 0.063015, loss_ce: 0.021837
2022-01-09 12:12:46,876 iteration 1787 : loss : 0.069923, loss_ce: 0.017636
2022-01-09 12:12:48,262 iteration 1788 : loss : 0.061054, loss_ce: 0.023921
2022-01-09 12:12:49,649 iteration 1789 : loss : 0.080076, loss_ce: 0.038734
2022-01-09 12:12:51,156 iteration 1790 : loss : 0.040260, loss_ce: 0.017148
2022-01-09 12:12:52,548 iteration 1791 : loss : 0.055303, loss_ce: 0.024244
2022-01-09 12:12:53,947 iteration 1792 : loss : 0.053890, loss_ce: 0.020519
2022-01-09 12:12:55,352 iteration 1793 : loss : 0.099178, loss_ce: 0.047620
2022-01-09 12:12:56,719 iteration 1794 : loss : 0.059977, loss_ce: 0.032197
2022-01-09 12:12:58,001 iteration 1795 : loss : 0.048023, loss_ce: 0.020360
2022-01-09 12:12:59,446 iteration 1796 : loss : 0.060195, loss_ce: 0.029978
2022-01-09 12:13:00,775 iteration 1797 : loss : 0.051284, loss_ce: 0.021907
2022-01-09 12:13:02,152 iteration 1798 : loss : 0.067824, loss_ce: 0.024057
2022-01-09 12:13:03,588 iteration 1799 : loss : 0.060373, loss_ce: 0.022521
2022-01-09 12:13:04,910 iteration 1800 : loss : 0.053209, loss_ce: 0.015972
2022-01-09 12:13:06,281 iteration 1801 : loss : 0.087401, loss_ce: 0.032014
2022-01-09 12:13:07,698 iteration 1802 : loss : 0.048916, loss_ce: 0.016404
 26%|███████▋                     | 106/400 [45:45<2:06:35, 25.83s/it]2022-01-09 12:13:09,177 iteration 1803 : loss : 0.059475, loss_ce: 0.022889
2022-01-09 12:13:10,495 iteration 1804 : loss : 0.058041, loss_ce: 0.018115
2022-01-09 12:13:11,989 iteration 1805 : loss : 0.067400, loss_ce: 0.023715
2022-01-09 12:13:13,446 iteration 1806 : loss : 0.057975, loss_ce: 0.028891
2022-01-09 12:13:14,815 iteration 1807 : loss : 0.059759, loss_ce: 0.025617
2022-01-09 12:13:16,229 iteration 1808 : loss : 0.038233, loss_ce: 0.016021
2022-01-09 12:13:17,583 iteration 1809 : loss : 0.051404, loss_ce: 0.023463
2022-01-09 12:13:18,989 iteration 1810 : loss : 0.068000, loss_ce: 0.034563
2022-01-09 12:13:20,349 iteration 1811 : loss : 0.055178, loss_ce: 0.020849
2022-01-09 12:13:21,698 iteration 1812 : loss : 0.058190, loss_ce: 0.021391
2022-01-09 12:13:23,075 iteration 1813 : loss : 0.056367, loss_ce: 0.027273
2022-01-09 12:13:24,425 iteration 1814 : loss : 0.072982, loss_ce: 0.027808
2022-01-09 12:13:25,874 iteration 1815 : loss : 0.068001, loss_ce: 0.024545
2022-01-09 12:13:27,266 iteration 1816 : loss : 0.034842, loss_ce: 0.014737
2022-01-09 12:13:28,719 iteration 1817 : loss : 0.069907, loss_ce: 0.025935
2022-01-09 12:13:30,044 iteration 1818 : loss : 0.040094, loss_ce: 0.013847
2022-01-09 12:13:31,466 iteration 1819 : loss : 0.078723, loss_ce: 0.031619
 27%|███████▊                     | 107/400 [46:09<2:03:08, 25.22s/it]2022-01-09 12:13:32,802 iteration 1820 : loss : 0.045896, loss_ce: 0.020582
2022-01-09 12:13:34,284 iteration 1821 : loss : 0.050277, loss_ce: 0.018282
2022-01-09 12:13:35,708 iteration 1822 : loss : 0.066367, loss_ce: 0.034350
2022-01-09 12:13:37,157 iteration 1823 : loss : 0.058582, loss_ce: 0.022006
2022-01-09 12:13:38,562 iteration 1824 : loss : 0.090291, loss_ce: 0.028438
2022-01-09 12:13:39,898 iteration 1825 : loss : 0.070966, loss_ce: 0.029295
2022-01-09 12:13:41,338 iteration 1826 : loss : 0.070734, loss_ce: 0.022385
2022-01-09 12:13:42,670 iteration 1827 : loss : 0.064021, loss_ce: 0.027682
2022-01-09 12:13:44,052 iteration 1828 : loss : 0.075400, loss_ce: 0.032005
2022-01-09 12:13:45,397 iteration 1829 : loss : 0.075868, loss_ce: 0.037008
2022-01-09 12:13:46,851 iteration 1830 : loss : 0.065255, loss_ce: 0.021747
2022-01-09 12:13:48,241 iteration 1831 : loss : 0.064789, loss_ce: 0.026366
2022-01-09 12:13:49,595 iteration 1832 : loss : 0.069067, loss_ce: 0.026829
2022-01-09 12:13:50,921 iteration 1833 : loss : 0.060451, loss_ce: 0.024905
2022-01-09 12:13:52,281 iteration 1834 : loss : 0.074262, loss_ce: 0.026301
2022-01-09 12:13:53,657 iteration 1835 : loss : 0.049049, loss_ce: 0.018799
2022-01-09 12:13:54,998 iteration 1836 : loss : 0.070857, loss_ce: 0.027572
 27%|███████▊                     | 108/400 [46:32<2:00:15, 24.71s/it]2022-01-09 12:13:56,468 iteration 1837 : loss : 0.057961, loss_ce: 0.021830
2022-01-09 12:13:57,771 iteration 1838 : loss : 0.055224, loss_ce: 0.025428
2022-01-09 12:13:59,104 iteration 1839 : loss : 0.069193, loss_ce: 0.027843
2022-01-09 12:14:00,489 iteration 1840 : loss : 0.063719, loss_ce: 0.030084
2022-01-09 12:14:01,803 iteration 1841 : loss : 0.063444, loss_ce: 0.025047
2022-01-09 12:14:03,158 iteration 1842 : loss : 0.045436, loss_ce: 0.015139
2022-01-09 12:14:04,546 iteration 1843 : loss : 0.051498, loss_ce: 0.024973
2022-01-09 12:14:05,896 iteration 1844 : loss : 0.063066, loss_ce: 0.019048
2022-01-09 12:14:07,239 iteration 1845 : loss : 0.051931, loss_ce: 0.015781
2022-01-09 12:14:08,678 iteration 1846 : loss : 0.055133, loss_ce: 0.028090
2022-01-09 12:14:10,015 iteration 1847 : loss : 0.056920, loss_ce: 0.021745
2022-01-09 12:14:11,375 iteration 1848 : loss : 0.072220, loss_ce: 0.028172
2022-01-09 12:14:12,715 iteration 1849 : loss : 0.083156, loss_ce: 0.039582
2022-01-09 12:14:14,203 iteration 1850 : loss : 0.094216, loss_ce: 0.037145
2022-01-09 12:14:15,571 iteration 1851 : loss : 0.087811, loss_ce: 0.029784
2022-01-09 12:14:16,958 iteration 1852 : loss : 0.058038, loss_ce: 0.025394
2022-01-09 12:14:18,301 iteration 1853 : loss : 0.062192, loss_ce: 0.026937
 27%|███████▉                     | 109/400 [46:56<1:57:48, 24.29s/it]2022-01-09 12:14:19,707 iteration 1854 : loss : 0.047869, loss_ce: 0.018740
2022-01-09 12:14:21,128 iteration 1855 : loss : 0.053430, loss_ce: 0.022399
2022-01-09 12:14:22,555 iteration 1856 : loss : 0.069985, loss_ce: 0.022601
2022-01-09 12:14:23,998 iteration 1857 : loss : 0.043831, loss_ce: 0.016289
2022-01-09 12:14:25,388 iteration 1858 : loss : 0.075884, loss_ce: 0.036909
2022-01-09 12:14:26,780 iteration 1859 : loss : 0.059128, loss_ce: 0.025980
2022-01-09 12:14:28,118 iteration 1860 : loss : 0.058871, loss_ce: 0.020168
2022-01-09 12:14:29,551 iteration 1861 : loss : 0.059341, loss_ce: 0.024008
2022-01-09 12:14:30,881 iteration 1862 : loss : 0.057556, loss_ce: 0.025044
2022-01-09 12:14:32,254 iteration 1863 : loss : 0.084593, loss_ce: 0.041786
2022-01-09 12:14:33,662 iteration 1864 : loss : 0.119721, loss_ce: 0.030102
2022-01-09 12:14:34,946 iteration 1865 : loss : 0.050268, loss_ce: 0.021637
2022-01-09 12:14:36,378 iteration 1866 : loss : 0.040321, loss_ce: 0.019109
2022-01-09 12:14:37,755 iteration 1867 : loss : 0.042877, loss_ce: 0.019286
2022-01-09 12:14:39,094 iteration 1868 : loss : 0.061204, loss_ce: 0.026705
2022-01-09 12:14:40,424 iteration 1869 : loss : 0.119027, loss_ce: 0.047007
2022-01-09 12:14:40,424 Training Data Eval:
2022-01-09 12:14:47,293   Average segmentation loss on training set: 0.0420
2022-01-09 12:14:47,293 Validation Data Eval:
2022-01-09 12:14:49,665   Average segmentation loss on validation set: 0.0975
2022-01-09 12:14:51,072 iteration 1870 : loss : 0.060629, loss_ce: 0.022388
 28%|███████▉                     | 110/400 [47:28<2:09:42, 26.83s/it]2022-01-09 12:14:52,554 iteration 1871 : loss : 0.052058, loss_ce: 0.022519
2022-01-09 12:14:54,027 iteration 1872 : loss : 0.097100, loss_ce: 0.026729
2022-01-09 12:14:55,352 iteration 1873 : loss : 0.052555, loss_ce: 0.020095
2022-01-09 12:14:56,790 iteration 1874 : loss : 0.084293, loss_ce: 0.039458
2022-01-09 12:14:58,087 iteration 1875 : loss : 0.053993, loss_ce: 0.021665
2022-01-09 12:14:59,531 iteration 1876 : loss : 0.062359, loss_ce: 0.018411
2022-01-09 12:15:00,947 iteration 1877 : loss : 0.092245, loss_ce: 0.040181
2022-01-09 12:15:02,296 iteration 1878 : loss : 0.056550, loss_ce: 0.025572
2022-01-09 12:15:03,637 iteration 1879 : loss : 0.066546, loss_ce: 0.023330
2022-01-09 12:15:04,981 iteration 1880 : loss : 0.068819, loss_ce: 0.029721
2022-01-09 12:15:06,353 iteration 1881 : loss : 0.089694, loss_ce: 0.026234
2022-01-09 12:15:07,678 iteration 1882 : loss : 0.061637, loss_ce: 0.022743
2022-01-09 12:15:08,969 iteration 1883 : loss : 0.070082, loss_ce: 0.023837
2022-01-09 12:15:10,375 iteration 1884 : loss : 0.074732, loss_ce: 0.028100
2022-01-09 12:15:11,661 iteration 1885 : loss : 0.049931, loss_ce: 0.020873
2022-01-09 12:15:13,107 iteration 1886 : loss : 0.042978, loss_ce: 0.018550
2022-01-09 12:15:14,550 iteration 1887 : loss : 0.063988, loss_ce: 0.030367
 28%|████████                     | 111/400 [47:52<2:04:23, 25.83s/it]2022-01-09 12:15:15,966 iteration 1888 : loss : 0.071287, loss_ce: 0.032072
2022-01-09 12:15:17,395 iteration 1889 : loss : 0.055575, loss_ce: 0.024095
2022-01-09 12:15:18,802 iteration 1890 : loss : 0.049491, loss_ce: 0.017793
2022-01-09 12:15:20,221 iteration 1891 : loss : 0.058420, loss_ce: 0.023826
2022-01-09 12:15:21,603 iteration 1892 : loss : 0.054158, loss_ce: 0.024162
2022-01-09 12:15:22,997 iteration 1893 : loss : 0.078492, loss_ce: 0.044407
2022-01-09 12:15:24,361 iteration 1894 : loss : 0.080805, loss_ce: 0.045535
2022-01-09 12:15:25,724 iteration 1895 : loss : 0.072614, loss_ce: 0.022163
2022-01-09 12:15:27,213 iteration 1896 : loss : 0.088671, loss_ce: 0.045862
2022-01-09 12:15:28,573 iteration 1897 : loss : 0.090440, loss_ce: 0.032096
2022-01-09 12:15:30,003 iteration 1898 : loss : 0.061582, loss_ce: 0.024908
2022-01-09 12:15:31,377 iteration 1899 : loss : 0.064263, loss_ce: 0.031329
2022-01-09 12:15:32,678 iteration 1900 : loss : 0.043948, loss_ce: 0.019005
2022-01-09 12:15:34,049 iteration 1901 : loss : 0.041640, loss_ce: 0.015914
2022-01-09 12:15:35,448 iteration 1902 : loss : 0.055878, loss_ce: 0.023426
2022-01-09 12:15:36,871 iteration 1903 : loss : 0.062147, loss_ce: 0.019240
2022-01-09 12:15:38,348 iteration 1904 : loss : 0.057884, loss_ce: 0.022244
 28%|████████                     | 112/400 [48:16<2:01:02, 25.22s/it]2022-01-09 12:15:39,732 iteration 1905 : loss : 0.042032, loss_ce: 0.017129
2022-01-09 12:15:41,162 iteration 1906 : loss : 0.064069, loss_ce: 0.024032
2022-01-09 12:15:42,648 iteration 1907 : loss : 0.065934, loss_ce: 0.022699
2022-01-09 12:15:44,062 iteration 1908 : loss : 0.067183, loss_ce: 0.028900
2022-01-09 12:15:45,505 iteration 1909 : loss : 0.067065, loss_ce: 0.035146
2022-01-09 12:15:46,939 iteration 1910 : loss : 0.054328, loss_ce: 0.023063
2022-01-09 12:15:48,435 iteration 1911 : loss : 0.056519, loss_ce: 0.021394
2022-01-09 12:15:49,856 iteration 1912 : loss : 0.064698, loss_ce: 0.018694
2022-01-09 12:15:51,228 iteration 1913 : loss : 0.046752, loss_ce: 0.017174
2022-01-09 12:15:52,568 iteration 1914 : loss : 0.054235, loss_ce: 0.023115
2022-01-09 12:15:53,951 iteration 1915 : loss : 0.079214, loss_ce: 0.026122
2022-01-09 12:15:55,328 iteration 1916 : loss : 0.056329, loss_ce: 0.024095
2022-01-09 12:15:56,734 iteration 1917 : loss : 0.077316, loss_ce: 0.026870
2022-01-09 12:15:58,076 iteration 1918 : loss : 0.053462, loss_ce: 0.022439
2022-01-09 12:15:59,439 iteration 1919 : loss : 0.053804, loss_ce: 0.026078
2022-01-09 12:16:00,802 iteration 1920 : loss : 0.044288, loss_ce: 0.016162
2022-01-09 12:16:02,215 iteration 1921 : loss : 0.072161, loss_ce: 0.031345
 28%|████████▏                    | 113/400 [48:39<1:58:40, 24.81s/it]2022-01-09 12:16:03,599 iteration 1922 : loss : 0.061564, loss_ce: 0.023730
2022-01-09 12:16:04,907 iteration 1923 : loss : 0.036274, loss_ce: 0.016256
2022-01-09 12:16:06,302 iteration 1924 : loss : 0.089284, loss_ce: 0.049164
2022-01-09 12:16:07,632 iteration 1925 : loss : 0.057069, loss_ce: 0.023268
2022-01-09 12:16:08,974 iteration 1926 : loss : 0.048615, loss_ce: 0.021029
2022-01-09 12:16:10,386 iteration 1927 : loss : 0.075168, loss_ce: 0.027304
2022-01-09 12:16:11,809 iteration 1928 : loss : 0.044620, loss_ce: 0.019726
2022-01-09 12:16:13,160 iteration 1929 : loss : 0.050511, loss_ce: 0.019424
2022-01-09 12:16:14,536 iteration 1930 : loss : 0.077525, loss_ce: 0.026647
2022-01-09 12:16:15,881 iteration 1931 : loss : 0.060618, loss_ce: 0.020318
2022-01-09 12:16:17,254 iteration 1932 : loss : 0.055690, loss_ce: 0.018120
2022-01-09 12:16:18,633 iteration 1933 : loss : 0.051245, loss_ce: 0.020087
2022-01-09 12:16:19,893 iteration 1934 : loss : 0.038598, loss_ce: 0.017683
2022-01-09 12:16:21,240 iteration 1935 : loss : 0.061303, loss_ce: 0.028752
2022-01-09 12:16:22,651 iteration 1936 : loss : 0.034112, loss_ce: 0.012267
2022-01-09 12:16:24,053 iteration 1937 : loss : 0.041029, loss_ce: 0.017670
2022-01-09 12:16:25,460 iteration 1938 : loss : 0.078627, loss_ce: 0.024790
 28%|████████▎                    | 114/400 [49:03<1:56:02, 24.34s/it]2022-01-09 12:16:26,837 iteration 1939 : loss : 0.039672, loss_ce: 0.018605
2022-01-09 12:16:28,186 iteration 1940 : loss : 0.036722, loss_ce: 0.013424
2022-01-09 12:16:29,607 iteration 1941 : loss : 0.048971, loss_ce: 0.021324
2022-01-09 12:16:30,978 iteration 1942 : loss : 0.060151, loss_ce: 0.020045
2022-01-09 12:16:32,281 iteration 1943 : loss : 0.047737, loss_ce: 0.016143
2022-01-09 12:16:33,626 iteration 1944 : loss : 0.045273, loss_ce: 0.016952
2022-01-09 12:16:34,983 iteration 1945 : loss : 0.056828, loss_ce: 0.028835
2022-01-09 12:16:36,342 iteration 1946 : loss : 0.069962, loss_ce: 0.025158
2022-01-09 12:16:37,688 iteration 1947 : loss : 0.062798, loss_ce: 0.022445
2022-01-09 12:16:39,032 iteration 1948 : loss : 0.047403, loss_ce: 0.020062
2022-01-09 12:16:40,329 iteration 1949 : loss : 0.050639, loss_ce: 0.025059
2022-01-09 12:16:41,736 iteration 1950 : loss : 0.064767, loss_ce: 0.025510
2022-01-09 12:16:43,205 iteration 1951 : loss : 0.077274, loss_ce: 0.030440
2022-01-09 12:16:44,528 iteration 1952 : loss : 0.068741, loss_ce: 0.024872
2022-01-09 12:16:45,914 iteration 1953 : loss : 0.055512, loss_ce: 0.026932
2022-01-09 12:16:47,354 iteration 1954 : loss : 0.057383, loss_ce: 0.021995
2022-01-09 12:16:47,355 Training Data Eval:
2022-01-09 12:16:54,223   Average segmentation loss on training set: 0.0380
2022-01-09 12:16:54,223 Validation Data Eval:
2022-01-09 12:16:56,589   Average segmentation loss on validation set: 0.0943
2022-01-09 12:16:57,933 iteration 1955 : loss : 0.039644, loss_ce: 0.014646
 29%|████████▎                    | 115/400 [49:35<2:07:13, 26.78s/it]2022-01-09 12:16:59,405 iteration 1956 : loss : 0.052757, loss_ce: 0.024060
2022-01-09 12:17:00,726 iteration 1957 : loss : 0.046468, loss_ce: 0.020087
2022-01-09 12:17:02,071 iteration 1958 : loss : 0.053760, loss_ce: 0.024482
2022-01-09 12:17:03,512 iteration 1959 : loss : 0.075519, loss_ce: 0.019625
2022-01-09 12:17:04,860 iteration 1960 : loss : 0.061020, loss_ce: 0.020649
2022-01-09 12:17:06,234 iteration 1961 : loss : 0.058326, loss_ce: 0.021227
2022-01-09 12:17:07,592 iteration 1962 : loss : 0.042056, loss_ce: 0.016768
2022-01-09 12:17:08,977 iteration 1963 : loss : 0.055127, loss_ce: 0.027122
2022-01-09 12:17:10,333 iteration 1964 : loss : 0.060823, loss_ce: 0.016803
2022-01-09 12:17:11,666 iteration 1965 : loss : 0.056327, loss_ce: 0.024828
2022-01-09 12:17:12,993 iteration 1966 : loss : 0.058822, loss_ce: 0.028281
2022-01-09 12:17:14,314 iteration 1967 : loss : 0.065573, loss_ce: 0.021760
2022-01-09 12:17:15,611 iteration 1968 : loss : 0.054814, loss_ce: 0.027283
2022-01-09 12:17:17,003 iteration 1969 : loss : 0.048595, loss_ce: 0.019323
2022-01-09 12:17:18,504 iteration 1970 : loss : 0.051380, loss_ce: 0.019383
2022-01-09 12:17:19,792 iteration 1971 : loss : 0.042169, loss_ce: 0.014243
2022-01-09 12:17:21,269 iteration 1972 : loss : 0.081156, loss_ce: 0.034792
 29%|████████▍                    | 116/400 [49:58<2:01:52, 25.75s/it]2022-01-09 12:17:22,627 iteration 1973 : loss : 0.042582, loss_ce: 0.017132
2022-01-09 12:17:23,938 iteration 1974 : loss : 0.056718, loss_ce: 0.025536
2022-01-09 12:17:25,382 iteration 1975 : loss : 0.049769, loss_ce: 0.020813
2022-01-09 12:17:26,677 iteration 1976 : loss : 0.051926, loss_ce: 0.022296
2022-01-09 12:17:28,123 iteration 1977 : loss : 0.071048, loss_ce: 0.030291
2022-01-09 12:17:29,458 iteration 1978 : loss : 0.043186, loss_ce: 0.019892
2022-01-09 12:17:30,803 iteration 1979 : loss : 0.050436, loss_ce: 0.024770
2022-01-09 12:17:32,210 iteration 1980 : loss : 0.085988, loss_ce: 0.028367
2022-01-09 12:17:33,538 iteration 1981 : loss : 0.046861, loss_ce: 0.018189
2022-01-09 12:17:34,960 iteration 1982 : loss : 0.043211, loss_ce: 0.015704
2022-01-09 12:17:36,418 iteration 1983 : loss : 0.048827, loss_ce: 0.018557
2022-01-09 12:17:37,763 iteration 1984 : loss : 0.055863, loss_ce: 0.015949
2022-01-09 12:17:39,132 iteration 1985 : loss : 0.055301, loss_ce: 0.021289
2022-01-09 12:17:40,481 iteration 1986 : loss : 0.054485, loss_ce: 0.020601
2022-01-09 12:17:41,824 iteration 1987 : loss : 0.026730, loss_ce: 0.010932
2022-01-09 12:17:43,272 iteration 1988 : loss : 0.040791, loss_ce: 0.016600
2022-01-09 12:17:44,645 iteration 1989 : loss : 0.043791, loss_ce: 0.019229
 29%|████████▍                    | 117/400 [50:22<1:58:04, 25.03s/it]2022-01-09 12:17:46,068 iteration 1990 : loss : 0.066070, loss_ce: 0.028697
2022-01-09 12:17:47,357 iteration 1991 : loss : 0.044368, loss_ce: 0.019244
2022-01-09 12:17:48,758 iteration 1992 : loss : 0.045718, loss_ce: 0.019441
2022-01-09 12:17:50,112 iteration 1993 : loss : 0.062766, loss_ce: 0.016769
2022-01-09 12:17:51,513 iteration 1994 : loss : 0.048186, loss_ce: 0.021299
2022-01-09 12:17:52,879 iteration 1995 : loss : 0.095353, loss_ce: 0.042687
2022-01-09 12:17:54,264 iteration 1996 : loss : 0.051287, loss_ce: 0.021624
2022-01-09 12:17:55,644 iteration 1997 : loss : 0.055815, loss_ce: 0.025154
2022-01-09 12:17:57,048 iteration 1998 : loss : 0.076107, loss_ce: 0.034590
2022-01-09 12:17:58,459 iteration 1999 : loss : 0.057428, loss_ce: 0.025595
2022-01-09 12:17:59,979 iteration 2000 : loss : 0.095361, loss_ce: 0.025881
2022-01-09 12:18:01,367 iteration 2001 : loss : 0.059278, loss_ce: 0.023799
2022-01-09 12:18:02,668 iteration 2002 : loss : 0.047609, loss_ce: 0.019762
2022-01-09 12:18:03,920 iteration 2003 : loss : 0.038137, loss_ce: 0.019588
2022-01-09 12:18:05,246 iteration 2004 : loss : 0.107578, loss_ce: 0.034622
2022-01-09 12:18:06,653 iteration 2005 : loss : 0.052197, loss_ce: 0.019089
2022-01-09 12:18:08,050 iteration 2006 : loss : 0.058132, loss_ce: 0.021014
 30%|████████▌                    | 118/400 [50:45<1:55:21, 24.55s/it]2022-01-09 12:18:09,536 iteration 2007 : loss : 0.056934, loss_ce: 0.021411
2022-01-09 12:18:11,025 iteration 2008 : loss : 0.067345, loss_ce: 0.021228
2022-01-09 12:18:12,404 iteration 2009 : loss : 0.058347, loss_ce: 0.023775
2022-01-09 12:18:13,771 iteration 2010 : loss : 0.066735, loss_ce: 0.030069
2022-01-09 12:18:15,242 iteration 2011 : loss : 0.089555, loss_ce: 0.035657
2022-01-09 12:18:16,627 iteration 2012 : loss : 0.050228, loss_ce: 0.027028
2022-01-09 12:18:17,933 iteration 2013 : loss : 0.048996, loss_ce: 0.021352
2022-01-09 12:18:19,280 iteration 2014 : loss : 0.058883, loss_ce: 0.017060
2022-01-09 12:18:20,689 iteration 2015 : loss : 0.046595, loss_ce: 0.017874
2022-01-09 12:18:22,055 iteration 2016 : loss : 0.079400, loss_ce: 0.026398
2022-01-09 12:18:23,475 iteration 2017 : loss : 0.044334, loss_ce: 0.015038
2022-01-09 12:18:24,951 iteration 2018 : loss : 0.061285, loss_ce: 0.031626
2022-01-09 12:18:26,310 iteration 2019 : loss : 0.063157, loss_ce: 0.022340
2022-01-09 12:18:27,658 iteration 2020 : loss : 0.051587, loss_ce: 0.021378
2022-01-09 12:18:29,054 iteration 2021 : loss : 0.062152, loss_ce: 0.021762
2022-01-09 12:18:30,458 iteration 2022 : loss : 0.049473, loss_ce: 0.022974
2022-01-09 12:18:31,893 iteration 2023 : loss : 0.060067, loss_ce: 0.023179
 30%|████████▋                    | 119/400 [51:09<1:53:57, 24.33s/it]2022-01-09 12:18:33,281 iteration 2024 : loss : 0.070113, loss_ce: 0.025928
2022-01-09 12:18:34,671 iteration 2025 : loss : 0.054568, loss_ce: 0.023238
2022-01-09 12:18:36,096 iteration 2026 : loss : 0.054942, loss_ce: 0.024180
2022-01-09 12:18:37,452 iteration 2027 : loss : 0.045558, loss_ce: 0.013456
2022-01-09 12:18:38,773 iteration 2028 : loss : 0.042285, loss_ce: 0.020989
2022-01-09 12:18:40,117 iteration 2029 : loss : 0.055577, loss_ce: 0.024306
2022-01-09 12:18:41,451 iteration 2030 : loss : 0.069935, loss_ce: 0.037740
2022-01-09 12:18:42,819 iteration 2031 : loss : 0.050510, loss_ce: 0.019993
2022-01-09 12:18:44,273 iteration 2032 : loss : 0.074432, loss_ce: 0.033177
2022-01-09 12:18:45,757 iteration 2033 : loss : 0.060013, loss_ce: 0.026697
2022-01-09 12:18:47,099 iteration 2034 : loss : 0.048384, loss_ce: 0.019346
2022-01-09 12:18:48,483 iteration 2035 : loss : 0.050167, loss_ce: 0.017908
2022-01-09 12:18:49,900 iteration 2036 : loss : 0.051487, loss_ce: 0.023990
2022-01-09 12:18:51,274 iteration 2037 : loss : 0.080605, loss_ce: 0.020303
2022-01-09 12:18:52,693 iteration 2038 : loss : 0.054582, loss_ce: 0.020482
2022-01-09 12:18:54,159 iteration 2039 : loss : 0.060235, loss_ce: 0.021376
2022-01-09 12:18:54,159 Training Data Eval:
2022-01-09 12:19:01,019   Average segmentation loss on training set: 0.0418
2022-01-09 12:19:01,019 Validation Data Eval:
2022-01-09 12:19:03,380   Average segmentation loss on validation set: 0.1154
2022-01-09 12:19:04,819 iteration 2040 : loss : 0.056836, loss_ce: 0.020985
 30%|████████▋                    | 120/400 [51:42<2:05:36, 26.92s/it]2022-01-09 12:19:06,296 iteration 2041 : loss : 0.062853, loss_ce: 0.016753
2022-01-09 12:19:07,655 iteration 2042 : loss : 0.053432, loss_ce: 0.020278
2022-01-09 12:19:09,005 iteration 2043 : loss : 0.040948, loss_ce: 0.013736
2022-01-09 12:19:10,374 iteration 2044 : loss : 0.061030, loss_ce: 0.022611
2022-01-09 12:19:11,719 iteration 2045 : loss : 0.046866, loss_ce: 0.015504
2022-01-09 12:19:13,098 iteration 2046 : loss : 0.046270, loss_ce: 0.020120
2022-01-09 12:19:14,446 iteration 2047 : loss : 0.044652, loss_ce: 0.018941
2022-01-09 12:19:15,829 iteration 2048 : loss : 0.038660, loss_ce: 0.014521
2022-01-09 12:19:17,176 iteration 2049 : loss : 0.056989, loss_ce: 0.017697
2022-01-09 12:19:18,564 iteration 2050 : loss : 0.061827, loss_ce: 0.024794
2022-01-09 12:19:19,951 iteration 2051 : loss : 0.053210, loss_ce: 0.023377
2022-01-09 12:19:21,273 iteration 2052 : loss : 0.055970, loss_ce: 0.022557
2022-01-09 12:19:22,660 iteration 2053 : loss : 0.063932, loss_ce: 0.020267
2022-01-09 12:19:24,013 iteration 2054 : loss : 0.044689, loss_ce: 0.017379
2022-01-09 12:19:25,375 iteration 2055 : loss : 0.064030, loss_ce: 0.018791
2022-01-09 12:19:26,788 iteration 2056 : loss : 0.060681, loss_ce: 0.025143
2022-01-09 12:19:28,120 iteration 2057 : loss : 0.055426, loss_ce: 0.023751
 30%|████████▊                    | 121/400 [52:05<2:00:05, 25.83s/it]2022-01-09 12:19:29,555 iteration 2058 : loss : 0.042135, loss_ce: 0.017850
2022-01-09 12:19:30,984 iteration 2059 : loss : 0.058679, loss_ce: 0.022219
2022-01-09 12:19:32,278 iteration 2060 : loss : 0.051367, loss_ce: 0.018306
2022-01-09 12:19:33,682 iteration 2061 : loss : 0.076669, loss_ce: 0.029608
2022-01-09 12:19:35,039 iteration 2062 : loss : 0.063389, loss_ce: 0.025037
2022-01-09 12:19:36,401 iteration 2063 : loss : 0.088428, loss_ce: 0.020971
2022-01-09 12:19:37,721 iteration 2064 : loss : 0.039222, loss_ce: 0.013801
2022-01-09 12:19:39,069 iteration 2065 : loss : 0.049388, loss_ce: 0.014856
2022-01-09 12:19:40,439 iteration 2066 : loss : 0.049445, loss_ce: 0.018724
2022-01-09 12:19:41,840 iteration 2067 : loss : 0.042543, loss_ce: 0.015599
2022-01-09 12:19:43,276 iteration 2068 : loss : 0.045715, loss_ce: 0.019720
2022-01-09 12:19:44,625 iteration 2069 : loss : 0.044953, loss_ce: 0.012483
2022-01-09 12:19:46,038 iteration 2070 : loss : 0.048219, loss_ce: 0.020692
2022-01-09 12:19:47,474 iteration 2071 : loss : 0.070108, loss_ce: 0.031301
2022-01-09 12:19:48,861 iteration 2072 : loss : 0.054817, loss_ce: 0.022035
2022-01-09 12:19:50,292 iteration 2073 : loss : 0.057204, loss_ce: 0.020474
2022-01-09 12:19:51,702 iteration 2074 : loss : 0.051127, loss_ce: 0.025174
 30%|████████▊                    | 122/400 [52:29<1:56:33, 25.16s/it]2022-01-09 12:19:53,108 iteration 2075 : loss : 0.040183, loss_ce: 0.013677
2022-01-09 12:19:54,488 iteration 2076 : loss : 0.070133, loss_ce: 0.022162
2022-01-09 12:19:55,892 iteration 2077 : loss : 0.045459, loss_ce: 0.021100
2022-01-09 12:19:57,389 iteration 2078 : loss : 0.053760, loss_ce: 0.017607
2022-01-09 12:19:58,749 iteration 2079 : loss : 0.043353, loss_ce: 0.012203
2022-01-09 12:20:00,220 iteration 2080 : loss : 0.057394, loss_ce: 0.024439
2022-01-09 12:20:01,515 iteration 2081 : loss : 0.049536, loss_ce: 0.018540
2022-01-09 12:20:02,790 iteration 2082 : loss : 0.033046, loss_ce: 0.013184
2022-01-09 12:20:04,145 iteration 2083 : loss : 0.051240, loss_ce: 0.025711
2022-01-09 12:20:05,502 iteration 2084 : loss : 0.055350, loss_ce: 0.026579
2022-01-09 12:20:06,831 iteration 2085 : loss : 0.056531, loss_ce: 0.019720
2022-01-09 12:20:08,190 iteration 2086 : loss : 0.063521, loss_ce: 0.025986
2022-01-09 12:20:09,522 iteration 2087 : loss : 0.057606, loss_ce: 0.023126
2022-01-09 12:20:10,960 iteration 2088 : loss : 0.037620, loss_ce: 0.015959
2022-01-09 12:20:12,376 iteration 2089 : loss : 0.047155, loss_ce: 0.020419
2022-01-09 12:20:13,687 iteration 2090 : loss : 0.042546, loss_ce: 0.017346
2022-01-09 12:20:15,099 iteration 2091 : loss : 0.054222, loss_ce: 0.022109
 31%|████████▉                    | 123/400 [52:52<1:53:41, 24.63s/it]2022-01-09 12:20:16,461 iteration 2092 : loss : 0.043546, loss_ce: 0.017785
2022-01-09 12:20:17,805 iteration 2093 : loss : 0.061961, loss_ce: 0.025573
2022-01-09 12:20:19,173 iteration 2094 : loss : 0.060233, loss_ce: 0.016681
2022-01-09 12:20:20,488 iteration 2095 : loss : 0.032649, loss_ce: 0.014302
2022-01-09 12:20:21,932 iteration 2096 : loss : 0.043108, loss_ce: 0.020531
2022-01-09 12:20:23,323 iteration 2097 : loss : 0.085377, loss_ce: 0.030409
2022-01-09 12:20:24,700 iteration 2098 : loss : 0.063669, loss_ce: 0.023983
2022-01-09 12:20:26,091 iteration 2099 : loss : 0.039012, loss_ce: 0.011552
2022-01-09 12:20:27,502 iteration 2100 : loss : 0.048269, loss_ce: 0.026608
2022-01-09 12:20:28,895 iteration 2101 : loss : 0.044823, loss_ce: 0.014723
2022-01-09 12:20:30,318 iteration 2102 : loss : 0.050929, loss_ce: 0.023229
2022-01-09 12:20:31,618 iteration 2103 : loss : 0.048077, loss_ce: 0.022809
2022-01-09 12:20:32,939 iteration 2104 : loss : 0.054840, loss_ce: 0.016877
2022-01-09 12:20:34,401 iteration 2105 : loss : 0.077424, loss_ce: 0.027779
2022-01-09 12:20:35,750 iteration 2106 : loss : 0.031138, loss_ce: 0.013455
2022-01-09 12:20:37,203 iteration 2107 : loss : 0.039859, loss_ce: 0.014172
2022-01-09 12:20:38,596 iteration 2108 : loss : 0.073421, loss_ce: 0.027344
 31%|████████▉                    | 124/400 [53:16<1:51:43, 24.29s/it]2022-01-09 12:20:40,023 iteration 2109 : loss : 0.052960, loss_ce: 0.022277
2022-01-09 12:20:41,433 iteration 2110 : loss : 0.035205, loss_ce: 0.011902
2022-01-09 12:20:42,898 iteration 2111 : loss : 0.057970, loss_ce: 0.017402
2022-01-09 12:20:44,292 iteration 2112 : loss : 0.061786, loss_ce: 0.028317
2022-01-09 12:20:45,637 iteration 2113 : loss : 0.047237, loss_ce: 0.023849
2022-01-09 12:20:47,048 iteration 2114 : loss : 0.048496, loss_ce: 0.023572
2022-01-09 12:20:48,451 iteration 2115 : loss : 0.044586, loss_ce: 0.015430
2022-01-09 12:20:49,850 iteration 2116 : loss : 0.047533, loss_ce: 0.017816
2022-01-09 12:20:51,241 iteration 2117 : loss : 0.056882, loss_ce: 0.026362
2022-01-09 12:20:52,661 iteration 2118 : loss : 0.064921, loss_ce: 0.019553
2022-01-09 12:20:54,088 iteration 2119 : loss : 0.052518, loss_ce: 0.019139
2022-01-09 12:20:55,473 iteration 2120 : loss : 0.118887, loss_ce: 0.033231
2022-01-09 12:20:56,760 iteration 2121 : loss : 0.047490, loss_ce: 0.020032
2022-01-09 12:20:58,114 iteration 2122 : loss : 0.039850, loss_ce: 0.012972
2022-01-09 12:20:59,547 iteration 2123 : loss : 0.070820, loss_ce: 0.030760
2022-01-09 12:21:00,996 iteration 2124 : loss : 0.045362, loss_ce: 0.018922
2022-01-09 12:21:00,997 Training Data Eval:
2022-01-09 12:21:07,882   Average segmentation loss on training set: 0.0485
2022-01-09 12:21:07,882 Validation Data Eval:
2022-01-09 12:21:10,263   Average segmentation loss on validation set: 0.1628
2022-01-09 12:21:11,660 iteration 2125 : loss : 0.058166, loss_ce: 0.030326
 31%|█████████                    | 125/400 [53:49<2:03:23, 26.92s/it]2022-01-09 12:21:13,144 iteration 2126 : loss : 0.045511, loss_ce: 0.016935
2022-01-09 12:21:14,551 iteration 2127 : loss : 0.075142, loss_ce: 0.040983
2022-01-09 12:21:15,954 iteration 2128 : loss : 0.048714, loss_ce: 0.015529
2022-01-09 12:21:17,296 iteration 2129 : loss : 0.060551, loss_ce: 0.023263
2022-01-09 12:21:18,711 iteration 2130 : loss : 0.089105, loss_ce: 0.032435
2022-01-09 12:21:20,071 iteration 2131 : loss : 0.049203, loss_ce: 0.014629
2022-01-09 12:21:21,451 iteration 2132 : loss : 0.039718, loss_ce: 0.015759
2022-01-09 12:21:22,893 iteration 2133 : loss : 0.065303, loss_ce: 0.023353
2022-01-09 12:21:24,204 iteration 2134 : loss : 0.043043, loss_ce: 0.020008
2022-01-09 12:21:25,579 iteration 2135 : loss : 0.061496, loss_ce: 0.023610
2022-01-09 12:21:26,944 iteration 2136 : loss : 0.049842, loss_ce: 0.020413
2022-01-09 12:21:28,318 iteration 2137 : loss : 0.083287, loss_ce: 0.031715
2022-01-09 12:21:29,650 iteration 2138 : loss : 0.050985, loss_ce: 0.019570
2022-01-09 12:21:31,060 iteration 2139 : loss : 0.044812, loss_ce: 0.022486
2022-01-09 12:21:32,433 iteration 2140 : loss : 0.066026, loss_ce: 0.032491
2022-01-09 12:21:33,870 iteration 2141 : loss : 0.045975, loss_ce: 0.015063
2022-01-09 12:21:35,207 iteration 2142 : loss : 0.053816, loss_ce: 0.017192
 32%|█████████▏                   | 126/400 [54:12<1:58:19, 25.91s/it]2022-01-09 12:21:36,573 iteration 2143 : loss : 0.047654, loss_ce: 0.019566
2022-01-09 12:21:37,967 iteration 2144 : loss : 0.050397, loss_ce: 0.014975
2022-01-09 12:21:39,370 iteration 2145 : loss : 0.070736, loss_ce: 0.025388
2022-01-09 12:21:40,691 iteration 2146 : loss : 0.032519, loss_ce: 0.013702
2022-01-09 12:21:42,080 iteration 2147 : loss : 0.071262, loss_ce: 0.024419
2022-01-09 12:21:43,429 iteration 2148 : loss : 0.050434, loss_ce: 0.020524
2022-01-09 12:21:44,806 iteration 2149 : loss : 0.041619, loss_ce: 0.016301
2022-01-09 12:21:46,155 iteration 2150 : loss : 0.031875, loss_ce: 0.013120
2022-01-09 12:21:47,588 iteration 2151 : loss : 0.058803, loss_ce: 0.023756
2022-01-09 12:21:48,920 iteration 2152 : loss : 0.047910, loss_ce: 0.020618
2022-01-09 12:21:50,245 iteration 2153 : loss : 0.048153, loss_ce: 0.017238
2022-01-09 12:21:51,747 iteration 2154 : loss : 0.073792, loss_ce: 0.030504
2022-01-09 12:21:53,122 iteration 2155 : loss : 0.055801, loss_ce: 0.021476
2022-01-09 12:21:54,453 iteration 2156 : loss : 0.044476, loss_ce: 0.018389
2022-01-09 12:21:55,967 iteration 2157 : loss : 0.084428, loss_ce: 0.030486
2022-01-09 12:21:57,438 iteration 2158 : loss : 0.069504, loss_ce: 0.031672
2022-01-09 12:21:58,843 iteration 2159 : loss : 0.092191, loss_ce: 0.027653
 32%|█████████▏                   | 127/400 [54:36<1:54:46, 25.22s/it]2022-01-09 12:22:00,232 iteration 2160 : loss : 0.048027, loss_ce: 0.019712
2022-01-09 12:22:01,551 iteration 2161 : loss : 0.043568, loss_ce: 0.016077
2022-01-09 12:22:02,902 iteration 2162 : loss : 0.038281, loss_ce: 0.016783
2022-01-09 12:22:04,247 iteration 2163 : loss : 0.044021, loss_ce: 0.019564
2022-01-09 12:22:05,655 iteration 2164 : loss : 0.044068, loss_ce: 0.021873
2022-01-09 12:22:07,050 iteration 2165 : loss : 0.064274, loss_ce: 0.024950
2022-01-09 12:22:08,377 iteration 2166 : loss : 0.046199, loss_ce: 0.020379
2022-01-09 12:22:09,676 iteration 2167 : loss : 0.047645, loss_ce: 0.018123
2022-01-09 12:22:11,019 iteration 2168 : loss : 0.032208, loss_ce: 0.013523
2022-01-09 12:22:12,415 iteration 2169 : loss : 0.061672, loss_ce: 0.022797
2022-01-09 12:22:13,679 iteration 2170 : loss : 0.041535, loss_ce: 0.014353
2022-01-09 12:22:15,025 iteration 2171 : loss : 0.059458, loss_ce: 0.024018
2022-01-09 12:22:16,360 iteration 2172 : loss : 0.089661, loss_ce: 0.034350
2022-01-09 12:22:17,692 iteration 2173 : loss : 0.060404, loss_ce: 0.019272
2022-01-09 12:22:19,064 iteration 2174 : loss : 0.054184, loss_ce: 0.021782
2022-01-09 12:22:20,517 iteration 2175 : loss : 0.041653, loss_ce: 0.020279
2022-01-09 12:22:21,788 iteration 2176 : loss : 0.036782, loss_ce: 0.017644
 32%|█████████▎                   | 128/400 [54:59<1:51:16, 24.54s/it]2022-01-09 12:22:23,150 iteration 2177 : loss : 0.071490, loss_ce: 0.037860
2022-01-09 12:22:24,519 iteration 2178 : loss : 0.044824, loss_ce: 0.014593
2022-01-09 12:22:25,962 iteration 2179 : loss : 0.049118, loss_ce: 0.017629
2022-01-09 12:22:27,376 iteration 2180 : loss : 0.071551, loss_ce: 0.039626
2022-01-09 12:22:28,686 iteration 2181 : loss : 0.040824, loss_ce: 0.015536
2022-01-09 12:22:30,060 iteration 2182 : loss : 0.067945, loss_ce: 0.024808
2022-01-09 12:22:31,480 iteration 2183 : loss : 0.084201, loss_ce: 0.022297
2022-01-09 12:22:32,798 iteration 2184 : loss : 0.050126, loss_ce: 0.023362
2022-01-09 12:22:34,123 iteration 2185 : loss : 0.043860, loss_ce: 0.016143
2022-01-09 12:22:35,567 iteration 2186 : loss : 0.048947, loss_ce: 0.017723
2022-01-09 12:22:37,026 iteration 2187 : loss : 0.051116, loss_ce: 0.017368
2022-01-09 12:22:38,550 iteration 2188 : loss : 0.056327, loss_ce: 0.026479
2022-01-09 12:22:40,063 iteration 2189 : loss : 0.057182, loss_ce: 0.019266
2022-01-09 12:22:41,455 iteration 2190 : loss : 0.057135, loss_ce: 0.025325
2022-01-09 12:22:42,856 iteration 2191 : loss : 0.036242, loss_ce: 0.012919
2022-01-09 12:22:44,202 iteration 2192 : loss : 0.049275, loss_ce: 0.017531
2022-01-09 12:22:45,592 iteration 2193 : loss : 0.039351, loss_ce: 0.013974
 32%|█████████▎                   | 129/400 [55:23<1:49:51, 24.32s/it]2022-01-09 12:22:47,056 iteration 2194 : loss : 0.053661, loss_ce: 0.019200
2022-01-09 12:22:48,451 iteration 2195 : loss : 0.048036, loss_ce: 0.023237
2022-01-09 12:22:49,876 iteration 2196 : loss : 0.049401, loss_ce: 0.021703
2022-01-09 12:22:51,337 iteration 2197 : loss : 0.045486, loss_ce: 0.016874
2022-01-09 12:22:52,716 iteration 2198 : loss : 0.084202, loss_ce: 0.025110
2022-01-09 12:22:54,103 iteration 2199 : loss : 0.068525, loss_ce: 0.034580
2022-01-09 12:22:55,561 iteration 2200 : loss : 0.126656, loss_ce: 0.023028
2022-01-09 12:22:56,891 iteration 2201 : loss : 0.049242, loss_ce: 0.016718
2022-01-09 12:22:58,370 iteration 2202 : loss : 0.048832, loss_ce: 0.020782
2022-01-09 12:22:59,777 iteration 2203 : loss : 0.064072, loss_ce: 0.026754
2022-01-09 12:23:01,095 iteration 2204 : loss : 0.046965, loss_ce: 0.013637
2022-01-09 12:23:02,486 iteration 2205 : loss : 0.081088, loss_ce: 0.038029
2022-01-09 12:23:03,947 iteration 2206 : loss : 0.068636, loss_ce: 0.028030
2022-01-09 12:23:05,363 iteration 2207 : loss : 0.049676, loss_ce: 0.014901
2022-01-09 12:23:06,800 iteration 2208 : loss : 0.075225, loss_ce: 0.020671
2022-01-09 12:23:08,185 iteration 2209 : loss : 0.097520, loss_ce: 0.035473
2022-01-09 12:23:08,185 Training Data Eval:
2022-01-09 12:23:15,066   Average segmentation loss on training set: 0.0408
2022-01-09 12:23:15,066 Validation Data Eval:
2022-01-09 12:23:17,456   Average segmentation loss on validation set: 0.0861
2022-01-09 12:23:23,134 Found new lowest validation loss at iteration 2209! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 12:23:24,640 iteration 2210 : loss : 0.060290, loss_ce: 0.032543
 32%|█████████▍                   | 130/400 [56:02<2:09:19, 28.74s/it]2022-01-09 12:23:26,072 iteration 2211 : loss : 0.052480, loss_ce: 0.015831
2022-01-09 12:23:27,400 iteration 2212 : loss : 0.055129, loss_ce: 0.024930
2022-01-09 12:23:28,775 iteration 2213 : loss : 0.082356, loss_ce: 0.019860
2022-01-09 12:23:30,072 iteration 2214 : loss : 0.049372, loss_ce: 0.015490
2022-01-09 12:23:31,368 iteration 2215 : loss : 0.036578, loss_ce: 0.014212
2022-01-09 12:23:32,702 iteration 2216 : loss : 0.047911, loss_ce: 0.022171
2022-01-09 12:23:34,017 iteration 2217 : loss : 0.047103, loss_ce: 0.022830
2022-01-09 12:23:35,362 iteration 2218 : loss : 0.032768, loss_ce: 0.011785
2022-01-09 12:23:36,756 iteration 2219 : loss : 0.051036, loss_ce: 0.023540
2022-01-09 12:23:38,205 iteration 2220 : loss : 0.056584, loss_ce: 0.024782
2022-01-09 12:23:39,644 iteration 2221 : loss : 0.053372, loss_ce: 0.018309
2022-01-09 12:23:40,940 iteration 2222 : loss : 0.040355, loss_ce: 0.017414
2022-01-09 12:23:42,260 iteration 2223 : loss : 0.059834, loss_ce: 0.019705
2022-01-09 12:23:43,694 iteration 2224 : loss : 0.056429, loss_ce: 0.022401
2022-01-09 12:23:45,089 iteration 2225 : loss : 0.057663, loss_ce: 0.022074
2022-01-09 12:23:46,528 iteration 2226 : loss : 0.047655, loss_ce: 0.023230
2022-01-09 12:23:47,948 iteration 2227 : loss : 0.061642, loss_ce: 0.019065
 33%|█████████▍                   | 131/400 [56:25<2:01:32, 27.11s/it]2022-01-09 12:23:49,449 iteration 2228 : loss : 0.055151, loss_ce: 0.026691
2022-01-09 12:23:50,837 iteration 2229 : loss : 0.063634, loss_ce: 0.021993
2022-01-09 12:23:52,354 iteration 2230 : loss : 0.062457, loss_ce: 0.024992
2022-01-09 12:23:53,724 iteration 2231 : loss : 0.062048, loss_ce: 0.032886
2022-01-09 12:23:55,086 iteration 2232 : loss : 0.049839, loss_ce: 0.024500
2022-01-09 12:23:56,647 iteration 2233 : loss : 0.108447, loss_ce: 0.030539
2022-01-09 12:23:58,061 iteration 2234 : loss : 0.045548, loss_ce: 0.012818
2022-01-09 12:23:59,446 iteration 2235 : loss : 0.043511, loss_ce: 0.017683
2022-01-09 12:24:00,809 iteration 2236 : loss : 0.038350, loss_ce: 0.016956
2022-01-09 12:24:02,203 iteration 2237 : loss : 0.048047, loss_ce: 0.022394
2022-01-09 12:24:03,534 iteration 2238 : loss : 0.050913, loss_ce: 0.015767
2022-01-09 12:24:04,889 iteration 2239 : loss : 0.051746, loss_ce: 0.015428
2022-01-09 12:24:06,192 iteration 2240 : loss : 0.059395, loss_ce: 0.017038
2022-01-09 12:24:07,605 iteration 2241 : loss : 0.039014, loss_ce: 0.014925
2022-01-09 12:24:09,042 iteration 2242 : loss : 0.067286, loss_ce: 0.022664
2022-01-09 12:24:10,453 iteration 2243 : loss : 0.095500, loss_ce: 0.047940
2022-01-09 12:24:11,807 iteration 2244 : loss : 0.031140, loss_ce: 0.011140
 33%|█████████▌                   | 132/400 [56:49<1:56:44, 26.14s/it]2022-01-09 12:24:13,226 iteration 2245 : loss : 0.041153, loss_ce: 0.014889
2022-01-09 12:24:14,516 iteration 2246 : loss : 0.039714, loss_ce: 0.011127
2022-01-09 12:24:15,898 iteration 2247 : loss : 0.061682, loss_ce: 0.032796
2022-01-09 12:24:17,338 iteration 2248 : loss : 0.070097, loss_ce: 0.029685
2022-01-09 12:24:18,712 iteration 2249 : loss : 0.039535, loss_ce: 0.018533
2022-01-09 12:24:20,093 iteration 2250 : loss : 0.049144, loss_ce: 0.018087
2022-01-09 12:24:21,503 iteration 2251 : loss : 0.046175, loss_ce: 0.020233
2022-01-09 12:24:22,883 iteration 2252 : loss : 0.063083, loss_ce: 0.023652
2022-01-09 12:24:24,353 iteration 2253 : loss : 0.082012, loss_ce: 0.038792
2022-01-09 12:24:25,780 iteration 2254 : loss : 0.046351, loss_ce: 0.018638
2022-01-09 12:24:27,158 iteration 2255 : loss : 0.044025, loss_ce: 0.014850
2022-01-09 12:24:28,521 iteration 2256 : loss : 0.037318, loss_ce: 0.014449
2022-01-09 12:24:29,852 iteration 2257 : loss : 0.064316, loss_ce: 0.021199
2022-01-09 12:24:31,175 iteration 2258 : loss : 0.068004, loss_ce: 0.024371
2022-01-09 12:24:32,607 iteration 2259 : loss : 0.047270, loss_ce: 0.020705
2022-01-09 12:24:33,916 iteration 2260 : loss : 0.045996, loss_ce: 0.018440
2022-01-09 12:24:35,307 iteration 2261 : loss : 0.066674, loss_ce: 0.029681
 33%|█████████▋                   | 133/400 [57:13<1:52:47, 25.34s/it]2022-01-09 12:24:36,694 iteration 2262 : loss : 0.046481, loss_ce: 0.023910
2022-01-09 12:24:38,134 iteration 2263 : loss : 0.097139, loss_ce: 0.033912
2022-01-09 12:24:39,603 iteration 2264 : loss : 0.057607, loss_ce: 0.017774
2022-01-09 12:24:41,006 iteration 2265 : loss : 0.038094, loss_ce: 0.015201
2022-01-09 12:24:42,355 iteration 2266 : loss : 0.052492, loss_ce: 0.019133
2022-01-09 12:24:43,708 iteration 2267 : loss : 0.034668, loss_ce: 0.012426
2022-01-09 12:24:45,083 iteration 2268 : loss : 0.053551, loss_ce: 0.022059
2022-01-09 12:24:46,419 iteration 2269 : loss : 0.039551, loss_ce: 0.022017
2022-01-09 12:24:47,792 iteration 2270 : loss : 0.056704, loss_ce: 0.023725
2022-01-09 12:24:49,141 iteration 2271 : loss : 0.042308, loss_ce: 0.015815
2022-01-09 12:24:50,502 iteration 2272 : loss : 0.055260, loss_ce: 0.027179
2022-01-09 12:24:51,903 iteration 2273 : loss : 0.051114, loss_ce: 0.018276
2022-01-09 12:24:53,301 iteration 2274 : loss : 0.047178, loss_ce: 0.016337
2022-01-09 12:24:54,736 iteration 2275 : loss : 0.066207, loss_ce: 0.022122
2022-01-09 12:24:56,057 iteration 2276 : loss : 0.038481, loss_ce: 0.017128
2022-01-09 12:24:57,440 iteration 2277 : loss : 0.056258, loss_ce: 0.020096
2022-01-09 12:24:58,774 iteration 2278 : loss : 0.033028, loss_ce: 0.012907
 34%|█████████▋                   | 134/400 [57:36<1:49:51, 24.78s/it]2022-01-09 12:25:00,135 iteration 2279 : loss : 0.125703, loss_ce: 0.056562
2022-01-09 12:25:01,515 iteration 2280 : loss : 0.048769, loss_ce: 0.020708
2022-01-09 12:25:02,893 iteration 2281 : loss : 0.078097, loss_ce: 0.023927
2022-01-09 12:25:04,296 iteration 2282 : loss : 0.042537, loss_ce: 0.015935
2022-01-09 12:25:05,750 iteration 2283 : loss : 0.048240, loss_ce: 0.020331
2022-01-09 12:25:07,106 iteration 2284 : loss : 0.049011, loss_ce: 0.023504
2022-01-09 12:25:08,467 iteration 2285 : loss : 0.038896, loss_ce: 0.014767
2022-01-09 12:25:09,826 iteration 2286 : loss : 0.041488, loss_ce: 0.016386
2022-01-09 12:25:11,194 iteration 2287 : loss : 0.045938, loss_ce: 0.018004
2022-01-09 12:25:12,606 iteration 2288 : loss : 0.051709, loss_ce: 0.020717
2022-01-09 12:25:13,940 iteration 2289 : loss : 0.042600, loss_ce: 0.011804
2022-01-09 12:25:15,369 iteration 2290 : loss : 0.037182, loss_ce: 0.019339
2022-01-09 12:25:16,738 iteration 2291 : loss : 0.040946, loss_ce: 0.015684
2022-01-09 12:25:18,093 iteration 2292 : loss : 0.050536, loss_ce: 0.021669
2022-01-09 12:25:19,389 iteration 2293 : loss : 0.045749, loss_ce: 0.023101
2022-01-09 12:25:20,795 iteration 2294 : loss : 0.071529, loss_ce: 0.026225
2022-01-09 12:25:20,795 Training Data Eval:
2022-01-09 12:25:27,675   Average segmentation loss on training set: 0.0532
2022-01-09 12:25:27,675 Validation Data Eval:
2022-01-09 12:25:30,049   Average segmentation loss on validation set: 0.0910
2022-01-09 12:25:31,497 iteration 2295 : loss : 0.061854, loss_ce: 0.030370
 34%|█████████▊                   | 135/400 [58:09<1:59:58, 27.16s/it]2022-01-09 12:25:32,891 iteration 2296 : loss : 0.034058, loss_ce: 0.013648
2022-01-09 12:25:34,251 iteration 2297 : loss : 0.052454, loss_ce: 0.022644
2022-01-09 12:25:35,609 iteration 2298 : loss : 0.039237, loss_ce: 0.015598
2022-01-09 12:25:37,049 iteration 2299 : loss : 0.066220, loss_ce: 0.031401
2022-01-09 12:25:38,391 iteration 2300 : loss : 0.056071, loss_ce: 0.023087
2022-01-09 12:25:39,834 iteration 2301 : loss : 0.044554, loss_ce: 0.013496
2022-01-09 12:25:41,247 iteration 2302 : loss : 0.081194, loss_ce: 0.028860
2022-01-09 12:25:42,638 iteration 2303 : loss : 0.043879, loss_ce: 0.015215
2022-01-09 12:25:44,091 iteration 2304 : loss : 0.057013, loss_ce: 0.019704
2022-01-09 12:25:45,507 iteration 2305 : loss : 0.060095, loss_ce: 0.025182
2022-01-09 12:25:46,980 iteration 2306 : loss : 0.047744, loss_ce: 0.022918
2022-01-09 12:25:48,342 iteration 2307 : loss : 0.077923, loss_ce: 0.026305
2022-01-09 12:25:49,674 iteration 2308 : loss : 0.038109, loss_ce: 0.012207
2022-01-09 12:25:51,019 iteration 2309 : loss : 0.047262, loss_ce: 0.022670
2022-01-09 12:25:52,453 iteration 2310 : loss : 0.042227, loss_ce: 0.016820
2022-01-09 12:25:53,756 iteration 2311 : loss : 0.050552, loss_ce: 0.018260
2022-01-09 12:25:55,134 iteration 2312 : loss : 0.069124, loss_ce: 0.024910
 34%|█████████▊                   | 136/400 [58:32<1:54:50, 26.10s/it]2022-01-09 12:25:56,600 iteration 2313 : loss : 0.043983, loss_ce: 0.017198
2022-01-09 12:25:57,879 iteration 2314 : loss : 0.039027, loss_ce: 0.012961
2022-01-09 12:25:59,203 iteration 2315 : loss : 0.036924, loss_ce: 0.013363
2022-01-09 12:26:00,655 iteration 2316 : loss : 0.049765, loss_ce: 0.021767
2022-01-09 12:26:01,968 iteration 2317 : loss : 0.039338, loss_ce: 0.013319
2022-01-09 12:26:03,316 iteration 2318 : loss : 0.041321, loss_ce: 0.020760
2022-01-09 12:26:04,687 iteration 2319 : loss : 0.050462, loss_ce: 0.017682
2022-01-09 12:26:06,128 iteration 2320 : loss : 0.040059, loss_ce: 0.016963
2022-01-09 12:26:07,493 iteration 2321 : loss : 0.054603, loss_ce: 0.021657
2022-01-09 12:26:08,880 iteration 2322 : loss : 0.042473, loss_ce: 0.018199
2022-01-09 12:26:10,170 iteration 2323 : loss : 0.044883, loss_ce: 0.018300
2022-01-09 12:26:11,522 iteration 2324 : loss : 0.032446, loss_ce: 0.014674
2022-01-09 12:26:12,779 iteration 2325 : loss : 0.034934, loss_ce: 0.014886
2022-01-09 12:26:14,128 iteration 2326 : loss : 0.061673, loss_ce: 0.014809
2022-01-09 12:26:15,456 iteration 2327 : loss : 0.040781, loss_ce: 0.016216
2022-01-09 12:26:16,816 iteration 2328 : loss : 0.059505, loss_ce: 0.025101
2022-01-09 12:26:18,133 iteration 2329 : loss : 0.044222, loss_ce: 0.016915
 34%|█████████▉                   | 137/400 [58:55<1:50:21, 25.18s/it]2022-01-09 12:26:19,574 iteration 2330 : loss : 0.044200, loss_ce: 0.016359
2022-01-09 12:26:20,895 iteration 2331 : loss : 0.030471, loss_ce: 0.011568
2022-01-09 12:26:22,339 iteration 2332 : loss : 0.043742, loss_ce: 0.015561
2022-01-09 12:26:23,706 iteration 2333 : loss : 0.051917, loss_ce: 0.024429
2022-01-09 12:26:25,192 iteration 2334 : loss : 0.044861, loss_ce: 0.015852
2022-01-09 12:26:26,676 iteration 2335 : loss : 0.065251, loss_ce: 0.027222
2022-01-09 12:26:28,121 iteration 2336 : loss : 0.035800, loss_ce: 0.015747
2022-01-09 12:26:29,503 iteration 2337 : loss : 0.058613, loss_ce: 0.022773
2022-01-09 12:26:30,910 iteration 2338 : loss : 0.059712, loss_ce: 0.022666
2022-01-09 12:26:32,304 iteration 2339 : loss : 0.075284, loss_ce: 0.035262
2022-01-09 12:26:33,618 iteration 2340 : loss : 0.041543, loss_ce: 0.016712
2022-01-09 12:26:35,005 iteration 2341 : loss : 0.042025, loss_ce: 0.019343
2022-01-09 12:26:36,401 iteration 2342 : loss : 0.054994, loss_ce: 0.022617
2022-01-09 12:26:37,897 iteration 2343 : loss : 0.043304, loss_ce: 0.015343
2022-01-09 12:26:39,228 iteration 2344 : loss : 0.078566, loss_ce: 0.019665
2022-01-09 12:26:40,572 iteration 2345 : loss : 0.066186, loss_ce: 0.035659
2022-01-09 12:26:41,932 iteration 2346 : loss : 0.063980, loss_ce: 0.028758
 34%|██████████                   | 138/400 [59:19<1:48:07, 24.76s/it]2022-01-09 12:26:43,447 iteration 2347 : loss : 0.068935, loss_ce: 0.029500
2022-01-09 12:26:44,840 iteration 2348 : loss : 0.046803, loss_ce: 0.016287
2022-01-09 12:26:46,189 iteration 2349 : loss : 0.042905, loss_ce: 0.014142
2022-01-09 12:26:47,551 iteration 2350 : loss : 0.068129, loss_ce: 0.022036
2022-01-09 12:26:49,000 iteration 2351 : loss : 0.042267, loss_ce: 0.020087
2022-01-09 12:26:50,334 iteration 2352 : loss : 0.039784, loss_ce: 0.019619
2022-01-09 12:26:51,679 iteration 2353 : loss : 0.039463, loss_ce: 0.018285
2022-01-09 12:26:53,002 iteration 2354 : loss : 0.030475, loss_ce: 0.014877
2022-01-09 12:26:54,338 iteration 2355 : loss : 0.048656, loss_ce: 0.016366
2022-01-09 12:26:55,759 iteration 2356 : loss : 0.048475, loss_ce: 0.015665
2022-01-09 12:26:57,147 iteration 2357 : loss : 0.052574, loss_ce: 0.015505
2022-01-09 12:26:58,532 iteration 2358 : loss : 0.039023, loss_ce: 0.016313
2022-01-09 12:26:59,880 iteration 2359 : loss : 0.069204, loss_ce: 0.024752
2022-01-09 12:27:01,202 iteration 2360 : loss : 0.052106, loss_ce: 0.014229
2022-01-09 12:27:02,569 iteration 2361 : loss : 0.052056, loss_ce: 0.017720
2022-01-09 12:27:04,059 iteration 2362 : loss : 0.068100, loss_ce: 0.028073
2022-01-09 12:27:05,440 iteration 2363 : loss : 0.041803, loss_ce: 0.020936
 35%|██████████                   | 139/400 [59:43<1:46:04, 24.39s/it]2022-01-09 12:27:06,947 iteration 2364 : loss : 0.048402, loss_ce: 0.020981
2022-01-09 12:27:08,412 iteration 2365 : loss : 0.064243, loss_ce: 0.023743
2022-01-09 12:27:09,836 iteration 2366 : loss : 0.076122, loss_ce: 0.032759
2022-01-09 12:27:11,159 iteration 2367 : loss : 0.038689, loss_ce: 0.010402
2022-01-09 12:27:12,484 iteration 2368 : loss : 0.051462, loss_ce: 0.024009
2022-01-09 12:27:13,861 iteration 2369 : loss : 0.049379, loss_ce: 0.014055
2022-01-09 12:27:15,217 iteration 2370 : loss : 0.061785, loss_ce: 0.023128
2022-01-09 12:27:16,624 iteration 2371 : loss : 0.031986, loss_ce: 0.012847
2022-01-09 12:27:17,958 iteration 2372 : loss : 0.063614, loss_ce: 0.021104
2022-01-09 12:27:19,315 iteration 2373 : loss : 0.050958, loss_ce: 0.019099
2022-01-09 12:27:20,632 iteration 2374 : loss : 0.051930, loss_ce: 0.023503
2022-01-09 12:27:21,967 iteration 2375 : loss : 0.038667, loss_ce: 0.013577
2022-01-09 12:27:23,359 iteration 2376 : loss : 0.046003, loss_ce: 0.021405
2022-01-09 12:27:24,742 iteration 2377 : loss : 0.060950, loss_ce: 0.024630
2022-01-09 12:27:26,128 iteration 2378 : loss : 0.031461, loss_ce: 0.012381
2022-01-09 12:27:27,540 iteration 2379 : loss : 0.082558, loss_ce: 0.038674
2022-01-09 12:27:27,541 Training Data Eval:
2022-01-09 12:27:34,416   Average segmentation loss on training set: 0.0518
2022-01-09 12:27:34,416 Validation Data Eval:
2022-01-09 12:27:36,793   Average segmentation loss on validation set: 0.1455
2022-01-09 12:27:38,125 iteration 2380 : loss : 0.042239, loss_ce: 0.018343
 35%|█████████▍                 | 140/400 [1:00:15<1:56:26, 26.87s/it]2022-01-09 12:27:39,500 iteration 2381 : loss : 0.054440, loss_ce: 0.023181
2022-01-09 12:27:40,906 iteration 2382 : loss : 0.050573, loss_ce: 0.017022
2022-01-09 12:27:42,239 iteration 2383 : loss : 0.038463, loss_ce: 0.013987
2022-01-09 12:27:43,655 iteration 2384 : loss : 0.061576, loss_ce: 0.021660
2022-01-09 12:27:45,086 iteration 2385 : loss : 0.058543, loss_ce: 0.023564
2022-01-09 12:27:46,450 iteration 2386 : loss : 0.056989, loss_ce: 0.020346
2022-01-09 12:27:47,791 iteration 2387 : loss : 0.046827, loss_ce: 0.015375
2022-01-09 12:27:49,166 iteration 2388 : loss : 0.049695, loss_ce: 0.020901
2022-01-09 12:27:50,479 iteration 2389 : loss : 0.024756, loss_ce: 0.008388
2022-01-09 12:27:51,860 iteration 2390 : loss : 0.045072, loss_ce: 0.020469
2022-01-09 12:27:53,216 iteration 2391 : loss : 0.049617, loss_ce: 0.029217
2022-01-09 12:27:54,612 iteration 2392 : loss : 0.044479, loss_ce: 0.017099
2022-01-09 12:27:56,009 iteration 2393 : loss : 0.058069, loss_ce: 0.019346
2022-01-09 12:27:57,293 iteration 2394 : loss : 0.036178, loss_ce: 0.013250
2022-01-09 12:27:58,714 iteration 2395 : loss : 0.043528, loss_ce: 0.019486
2022-01-09 12:28:00,042 iteration 2396 : loss : 0.033571, loss_ce: 0.015835
2022-01-09 12:28:01,439 iteration 2397 : loss : 0.048913, loss_ce: 0.019173
 35%|█████████▌                 | 141/400 [1:00:39<1:51:24, 25.81s/it]2022-01-09 12:28:02,860 iteration 2398 : loss : 0.058881, loss_ce: 0.022083
2022-01-09 12:28:04,202 iteration 2399 : loss : 0.058805, loss_ce: 0.016524
2022-01-09 12:28:05,632 iteration 2400 : loss : 0.049704, loss_ce: 0.021874
2022-01-09 12:28:07,008 iteration 2401 : loss : 0.038419, loss_ce: 0.015092
2022-01-09 12:28:08,471 iteration 2402 : loss : 0.041413, loss_ce: 0.022087
2022-01-09 12:28:09,807 iteration 2403 : loss : 0.041528, loss_ce: 0.017109
2022-01-09 12:28:11,128 iteration 2404 : loss : 0.036701, loss_ce: 0.018636
2022-01-09 12:28:12,414 iteration 2405 : loss : 0.041778, loss_ce: 0.014117
2022-01-09 12:28:13,909 iteration 2406 : loss : 0.053538, loss_ce: 0.016897
2022-01-09 12:28:15,221 iteration 2407 : loss : 0.048262, loss_ce: 0.023332
2022-01-09 12:28:16,530 iteration 2408 : loss : 0.022972, loss_ce: 0.009298
2022-01-09 12:28:17,941 iteration 2409 : loss : 0.033863, loss_ce: 0.011515
2022-01-09 12:28:19,362 iteration 2410 : loss : 0.040609, loss_ce: 0.018060
2022-01-09 12:28:20,765 iteration 2411 : loss : 0.049415, loss_ce: 0.013958
2022-01-09 12:28:22,095 iteration 2412 : loss : 0.028758, loss_ce: 0.011411
2022-01-09 12:28:23,583 iteration 2413 : loss : 0.079483, loss_ce: 0.037959
2022-01-09 12:28:24,966 iteration 2414 : loss : 0.039747, loss_ce: 0.015102
 36%|█████████▌                 | 142/400 [1:01:02<1:48:01, 25.12s/it]2022-01-09 12:28:26,414 iteration 2415 : loss : 0.048629, loss_ce: 0.016673
2022-01-09 12:28:27,771 iteration 2416 : loss : 0.036662, loss_ce: 0.012723
2022-01-09 12:28:29,139 iteration 2417 : loss : 0.029438, loss_ce: 0.009663
2022-01-09 12:28:30,593 iteration 2418 : loss : 0.040102, loss_ce: 0.015575
2022-01-09 12:28:31,978 iteration 2419 : loss : 0.053692, loss_ce: 0.026707
2022-01-09 12:28:33,451 iteration 2420 : loss : 0.051433, loss_ce: 0.025079
2022-01-09 12:28:34,909 iteration 2421 : loss : 0.068781, loss_ce: 0.021633
2022-01-09 12:28:36,346 iteration 2422 : loss : 0.043038, loss_ce: 0.016032
2022-01-09 12:28:37,715 iteration 2423 : loss : 0.045550, loss_ce: 0.016304
2022-01-09 12:28:39,076 iteration 2424 : loss : 0.047452, loss_ce: 0.020540
2022-01-09 12:28:40,560 iteration 2425 : loss : 0.048369, loss_ce: 0.017378
2022-01-09 12:28:41,915 iteration 2426 : loss : 0.057294, loss_ce: 0.018271
2022-01-09 12:28:43,203 iteration 2427 : loss : 0.036062, loss_ce: 0.016031
2022-01-09 12:28:44,537 iteration 2428 : loss : 0.045269, loss_ce: 0.012514
2022-01-09 12:28:45,871 iteration 2429 : loss : 0.042023, loss_ce: 0.016826
2022-01-09 12:28:47,220 iteration 2430 : loss : 0.026938, loss_ce: 0.010327
2022-01-09 12:28:48,590 iteration 2431 : loss : 0.046531, loss_ce: 0.026290
 36%|█████████▋                 | 143/400 [1:01:26<1:45:40, 24.67s/it]2022-01-09 12:28:49,979 iteration 2432 : loss : 0.046945, loss_ce: 0.017428
2022-01-09 12:28:51,337 iteration 2433 : loss : 0.039300, loss_ce: 0.015283
2022-01-09 12:28:52,707 iteration 2434 : loss : 0.041535, loss_ce: 0.014578
2022-01-09 12:28:54,164 iteration 2435 : loss : 0.040587, loss_ce: 0.020883
2022-01-09 12:28:55,461 iteration 2436 : loss : 0.039648, loss_ce: 0.015554
2022-01-09 12:28:56,878 iteration 2437 : loss : 0.046401, loss_ce: 0.019318
2022-01-09 12:28:58,300 iteration 2438 : loss : 0.049270, loss_ce: 0.014706
2022-01-09 12:28:59,710 iteration 2439 : loss : 0.045227, loss_ce: 0.014332
2022-01-09 12:29:01,129 iteration 2440 : loss : 0.051780, loss_ce: 0.019627
2022-01-09 12:29:02,495 iteration 2441 : loss : 0.038095, loss_ce: 0.013388
2022-01-09 12:29:03,850 iteration 2442 : loss : 0.039726, loss_ce: 0.014400
2022-01-09 12:29:05,157 iteration 2443 : loss : 0.039705, loss_ce: 0.016339
2022-01-09 12:29:06,526 iteration 2444 : loss : 0.042456, loss_ce: 0.016640
2022-01-09 12:29:07,912 iteration 2445 : loss : 0.037830, loss_ce: 0.015439
2022-01-09 12:29:09,374 iteration 2446 : loss : 0.042940, loss_ce: 0.017540
2022-01-09 12:29:10,690 iteration 2447 : loss : 0.034695, loss_ce: 0.012366
2022-01-09 12:29:12,110 iteration 2448 : loss : 0.032192, loss_ce: 0.012205
 36%|█████████▋                 | 144/400 [1:01:49<1:43:47, 24.33s/it]2022-01-09 12:29:13,477 iteration 2449 : loss : 0.049936, loss_ce: 0.018815
2022-01-09 12:29:14,790 iteration 2450 : loss : 0.031568, loss_ce: 0.017378
2022-01-09 12:29:16,136 iteration 2451 : loss : 0.038748, loss_ce: 0.011979
2022-01-09 12:29:17,469 iteration 2452 : loss : 0.037466, loss_ce: 0.012690
2022-01-09 12:29:18,845 iteration 2453 : loss : 0.043686, loss_ce: 0.015123
2022-01-09 12:29:20,197 iteration 2454 : loss : 0.041247, loss_ce: 0.014334
2022-01-09 12:29:21,522 iteration 2455 : loss : 0.041958, loss_ce: 0.019905
2022-01-09 12:29:22,830 iteration 2456 : loss : 0.045705, loss_ce: 0.020921
2022-01-09 12:29:24,175 iteration 2457 : loss : 0.038857, loss_ce: 0.016329
2022-01-09 12:29:25,496 iteration 2458 : loss : 0.042203, loss_ce: 0.018805
2022-01-09 12:29:26,774 iteration 2459 : loss : 0.026689, loss_ce: 0.010335
2022-01-09 12:29:28,105 iteration 2460 : loss : 0.040208, loss_ce: 0.011283
2022-01-09 12:29:29,533 iteration 2461 : loss : 0.037583, loss_ce: 0.012516
2022-01-09 12:29:30,924 iteration 2462 : loss : 0.075624, loss_ce: 0.025343
2022-01-09 12:29:32,271 iteration 2463 : loss : 0.037257, loss_ce: 0.015196
2022-01-09 12:29:33,660 iteration 2464 : loss : 0.039626, loss_ce: 0.015426
2022-01-09 12:29:33,660 Training Data Eval:
2022-01-09 12:29:40,533   Average segmentation loss on training set: 0.0364
2022-01-09 12:29:40,533 Validation Data Eval:
2022-01-09 12:29:42,906   Average segmentation loss on validation set: 0.1348
2022-01-09 12:29:44,256 iteration 2465 : loss : 0.036887, loss_ce: 0.016091
 36%|█████████▊                 | 145/400 [1:02:21<1:53:21, 26.67s/it]2022-01-09 12:29:45,745 iteration 2466 : loss : 0.049257, loss_ce: 0.020496
2022-01-09 12:29:47,194 iteration 2467 : loss : 0.046343, loss_ce: 0.019266
2022-01-09 12:29:48,545 iteration 2468 : loss : 0.052682, loss_ce: 0.025563
2022-01-09 12:29:49,869 iteration 2469 : loss : 0.030306, loss_ce: 0.013179
2022-01-09 12:29:51,222 iteration 2470 : loss : 0.035374, loss_ce: 0.011728
2022-01-09 12:29:52,675 iteration 2471 : loss : 0.063650, loss_ce: 0.023358
2022-01-09 12:29:54,009 iteration 2472 : loss : 0.064755, loss_ce: 0.030270
2022-01-09 12:29:55,433 iteration 2473 : loss : 0.056305, loss_ce: 0.027837
2022-01-09 12:29:56,771 iteration 2474 : loss : 0.041240, loss_ce: 0.014547
2022-01-09 12:29:58,142 iteration 2475 : loss : 0.040317, loss_ce: 0.014615
2022-01-09 12:29:59,457 iteration 2476 : loss : 0.038946, loss_ce: 0.019985
2022-01-09 12:30:00,911 iteration 2477 : loss : 0.053533, loss_ce: 0.027950
2022-01-09 12:30:02,316 iteration 2478 : loss : 0.038428, loss_ce: 0.014398
2022-01-09 12:30:03,611 iteration 2479 : loss : 0.050838, loss_ce: 0.016594
2022-01-09 12:30:04,984 iteration 2480 : loss : 0.040104, loss_ce: 0.015386
2022-01-09 12:30:06,411 iteration 2481 : loss : 0.045054, loss_ce: 0.016815
2022-01-09 12:30:07,758 iteration 2482 : loss : 0.048097, loss_ce: 0.021582
 36%|█████████▊                 | 146/400 [1:02:45<1:48:52, 25.72s/it]2022-01-09 12:30:09,266 iteration 2483 : loss : 0.051711, loss_ce: 0.024682
2022-01-09 12:30:10,668 iteration 2484 : loss : 0.063238, loss_ce: 0.015097
2022-01-09 12:30:12,003 iteration 2485 : loss : 0.033496, loss_ce: 0.015382
2022-01-09 12:30:13,362 iteration 2486 : loss : 0.038399, loss_ce: 0.016098
2022-01-09 12:30:14,715 iteration 2487 : loss : 0.045187, loss_ce: 0.015339
2022-01-09 12:30:16,137 iteration 2488 : loss : 0.039447, loss_ce: 0.016433
2022-01-09 12:30:17,495 iteration 2489 : loss : 0.044090, loss_ce: 0.019259
2022-01-09 12:30:18,911 iteration 2490 : loss : 0.049619, loss_ce: 0.021385
2022-01-09 12:30:20,359 iteration 2491 : loss : 0.049213, loss_ce: 0.019322
2022-01-09 12:30:21,644 iteration 2492 : loss : 0.035660, loss_ce: 0.013790
2022-01-09 12:30:23,030 iteration 2493 : loss : 0.051795, loss_ce: 0.021538
2022-01-09 12:30:24,384 iteration 2494 : loss : 0.053256, loss_ce: 0.015278
2022-01-09 12:30:25,728 iteration 2495 : loss : 0.054755, loss_ce: 0.025398
2022-01-09 12:30:27,272 iteration 2496 : loss : 0.080218, loss_ce: 0.022659
2022-01-09 12:30:28,688 iteration 2497 : loss : 0.043333, loss_ce: 0.014119
2022-01-09 12:30:30,117 iteration 2498 : loss : 0.054618, loss_ce: 0.021244
2022-01-09 12:30:31,434 iteration 2499 : loss : 0.029356, loss_ce: 0.007774
 37%|█████████▉                 | 147/400 [1:03:09<1:45:52, 25.11s/it]2022-01-09 12:30:32,842 iteration 2500 : loss : 0.043301, loss_ce: 0.015090
2022-01-09 12:30:34,205 iteration 2501 : loss : 0.036186, loss_ce: 0.009626
2022-01-09 12:30:35,649 iteration 2502 : loss : 0.058655, loss_ce: 0.026327
2022-01-09 12:30:36,970 iteration 2503 : loss : 0.047502, loss_ce: 0.016313
2022-01-09 12:30:38,309 iteration 2504 : loss : 0.052067, loss_ce: 0.020900
2022-01-09 12:30:39,717 iteration 2505 : loss : 0.059985, loss_ce: 0.022225
2022-01-09 12:30:41,086 iteration 2506 : loss : 0.052254, loss_ce: 0.016964
2022-01-09 12:30:42,405 iteration 2507 : loss : 0.042596, loss_ce: 0.019065
2022-01-09 12:30:43,746 iteration 2508 : loss : 0.040409, loss_ce: 0.013217
2022-01-09 12:30:45,096 iteration 2509 : loss : 0.046594, loss_ce: 0.014523
2022-01-09 12:30:46,427 iteration 2510 : loss : 0.034416, loss_ce: 0.015199
2022-01-09 12:30:47,802 iteration 2511 : loss : 0.040419, loss_ce: 0.018158
2022-01-09 12:30:49,193 iteration 2512 : loss : 0.047474, loss_ce: 0.016476
2022-01-09 12:30:50,621 iteration 2513 : loss : 0.047497, loss_ce: 0.015142
2022-01-09 12:30:52,000 iteration 2514 : loss : 0.057325, loss_ce: 0.017820
2022-01-09 12:30:53,360 iteration 2515 : loss : 0.028076, loss_ce: 0.014641
2022-01-09 12:30:54,714 iteration 2516 : loss : 0.032517, loss_ce: 0.016176
 37%|█████████▉                 | 148/400 [1:03:32<1:43:09, 24.56s/it]2022-01-09 12:30:56,124 iteration 2517 : loss : 0.036019, loss_ce: 0.014216
2022-01-09 12:30:57,492 iteration 2518 : loss : 0.049619, loss_ce: 0.016564
2022-01-09 12:30:58,882 iteration 2519 : loss : 0.044393, loss_ce: 0.018502
2022-01-09 12:31:00,249 iteration 2520 : loss : 0.060387, loss_ce: 0.023783
2022-01-09 12:31:01,603 iteration 2521 : loss : 0.033069, loss_ce: 0.011823
2022-01-09 12:31:02,978 iteration 2522 : loss : 0.046198, loss_ce: 0.022864
2022-01-09 12:31:04,396 iteration 2523 : loss : 0.069036, loss_ce: 0.017902
2022-01-09 12:31:05,749 iteration 2524 : loss : 0.034246, loss_ce: 0.014609
2022-01-09 12:31:07,130 iteration 2525 : loss : 0.057911, loss_ce: 0.022705
2022-01-09 12:31:08,520 iteration 2526 : loss : 0.051229, loss_ce: 0.018905
2022-01-09 12:31:09,897 iteration 2527 : loss : 0.033118, loss_ce: 0.012256
2022-01-09 12:31:11,292 iteration 2528 : loss : 0.048917, loss_ce: 0.017663
2022-01-09 12:31:12,672 iteration 2529 : loss : 0.054395, loss_ce: 0.021134
2022-01-09 12:31:14,053 iteration 2530 : loss : 0.033663, loss_ce: 0.015666
2022-01-09 12:31:15,503 iteration 2531 : loss : 0.038675, loss_ce: 0.015640
2022-01-09 12:31:16,872 iteration 2532 : loss : 0.060276, loss_ce: 0.019962
2022-01-09 12:31:18,254 iteration 2533 : loss : 0.052464, loss_ce: 0.015499
 37%|██████████                 | 149/400 [1:03:55<1:41:26, 24.25s/it]2022-01-09 12:31:19,615 iteration 2534 : loss : 0.041028, loss_ce: 0.018304
2022-01-09 12:31:21,057 iteration 2535 : loss : 0.066272, loss_ce: 0.024900
2022-01-09 12:31:22,397 iteration 2536 : loss : 0.039912, loss_ce: 0.020411
2022-01-09 12:31:23,739 iteration 2537 : loss : 0.035471, loss_ce: 0.015058
2022-01-09 12:31:25,015 iteration 2538 : loss : 0.031743, loss_ce: 0.013029
2022-01-09 12:31:26,406 iteration 2539 : loss : 0.031935, loss_ce: 0.012338
2022-01-09 12:31:27,729 iteration 2540 : loss : 0.029756, loss_ce: 0.009019
2022-01-09 12:31:29,080 iteration 2541 : loss : 0.049362, loss_ce: 0.021276
2022-01-09 12:31:30,388 iteration 2542 : loss : 0.029564, loss_ce: 0.009331
2022-01-09 12:31:31,698 iteration 2543 : loss : 0.045495, loss_ce: 0.015810
2022-01-09 12:31:33,064 iteration 2544 : loss : 0.049415, loss_ce: 0.017919
2022-01-09 12:31:34,543 iteration 2545 : loss : 0.042954, loss_ce: 0.016919
2022-01-09 12:31:36,032 iteration 2546 : loss : 0.054917, loss_ce: 0.024769
2022-01-09 12:31:37,345 iteration 2547 : loss : 0.031686, loss_ce: 0.016807
2022-01-09 12:31:38,743 iteration 2548 : loss : 0.038469, loss_ce: 0.017861
2022-01-09 12:31:40,128 iteration 2549 : loss : 0.053732, loss_ce: 0.018562
2022-01-09 12:31:40,128 Training Data Eval:
2022-01-09 12:31:46,978   Average segmentation loss on training set: 0.0301
2022-01-09 12:31:46,979 Validation Data Eval:
2022-01-09 12:31:49,343   Average segmentation loss on validation set: 0.0932
2022-01-09 12:31:50,680 iteration 2550 : loss : 0.046804, loss_ce: 0.015356
 38%|██████████▏                | 150/400 [1:04:28<1:51:17, 26.71s/it]2022-01-09 12:31:52,093 iteration 2551 : loss : 0.036367, loss_ce: 0.017849
2022-01-09 12:31:53,517 iteration 2552 : loss : 0.047956, loss_ce: 0.021029
2022-01-09 12:31:54,822 iteration 2553 : loss : 0.020039, loss_ce: 0.006774
2022-01-09 12:31:56,191 iteration 2554 : loss : 0.041446, loss_ce: 0.015115
2022-01-09 12:31:57,544 iteration 2555 : loss : 0.041481, loss_ce: 0.010491
2022-01-09 12:31:58,958 iteration 2556 : loss : 0.057572, loss_ce: 0.032100
2022-01-09 12:32:00,317 iteration 2557 : loss : 0.047725, loss_ce: 0.022108
2022-01-09 12:32:01,693 iteration 2558 : loss : 0.041071, loss_ce: 0.021618
2022-01-09 12:32:03,148 iteration 2559 : loss : 0.045207, loss_ce: 0.018327
2022-01-09 12:32:04,521 iteration 2560 : loss : 0.036444, loss_ce: 0.021449
2022-01-09 12:32:05,975 iteration 2561 : loss : 0.045872, loss_ce: 0.016758
2022-01-09 12:32:07,363 iteration 2562 : loss : 0.038561, loss_ce: 0.014961
2022-01-09 12:32:08,765 iteration 2563 : loss : 0.045194, loss_ce: 0.020418
2022-01-09 12:32:10,094 iteration 2564 : loss : 0.042002, loss_ce: 0.018932
2022-01-09 12:32:11,425 iteration 2565 : loss : 0.037373, loss_ce: 0.013943
2022-01-09 12:32:12,777 iteration 2566 : loss : 0.044655, loss_ce: 0.018392
2022-01-09 12:32:14,154 iteration 2567 : loss : 0.052777, loss_ce: 0.013879
 38%|██████████▏                | 151/400 [1:04:51<1:46:47, 25.73s/it]2022-01-09 12:32:15,593 iteration 2568 : loss : 0.036535, loss_ce: 0.016489
2022-01-09 12:32:16,939 iteration 2569 : loss : 0.051625, loss_ce: 0.022666
2022-01-09 12:32:18,290 iteration 2570 : loss : 0.044531, loss_ce: 0.017620
2022-01-09 12:32:19,648 iteration 2571 : loss : 0.030172, loss_ce: 0.011686
2022-01-09 12:32:20,954 iteration 2572 : loss : 0.043649, loss_ce: 0.011836
2022-01-09 12:32:22,271 iteration 2573 : loss : 0.044994, loss_ce: 0.016947
2022-01-09 12:32:23,614 iteration 2574 : loss : 0.050507, loss_ce: 0.014702
2022-01-09 12:32:25,086 iteration 2575 : loss : 0.041713, loss_ce: 0.018442
2022-01-09 12:32:26,474 iteration 2576 : loss : 0.042246, loss_ce: 0.019198
2022-01-09 12:32:27,851 iteration 2577 : loss : 0.055270, loss_ce: 0.022963
2022-01-09 12:32:29,234 iteration 2578 : loss : 0.072700, loss_ce: 0.028660
2022-01-09 12:32:30,547 iteration 2579 : loss : 0.035191, loss_ce: 0.009953
2022-01-09 12:32:31,974 iteration 2580 : loss : 0.054265, loss_ce: 0.011895
2022-01-09 12:32:33,387 iteration 2581 : loss : 0.044839, loss_ce: 0.019258
2022-01-09 12:32:34,835 iteration 2582 : loss : 0.037449, loss_ce: 0.014681
2022-01-09 12:32:36,200 iteration 2583 : loss : 0.027558, loss_ce: 0.010489
2022-01-09 12:32:37,487 iteration 2584 : loss : 0.031803, loss_ce: 0.012668
 38%|██████████▎                | 152/400 [1:05:15<1:43:24, 25.02s/it]2022-01-09 12:32:38,913 iteration 2585 : loss : 0.032067, loss_ce: 0.010872
2022-01-09 12:32:40,296 iteration 2586 : loss : 0.056079, loss_ce: 0.028190
2022-01-09 12:32:41,611 iteration 2587 : loss : 0.045784, loss_ce: 0.015826
2022-01-09 12:32:42,955 iteration 2588 : loss : 0.039500, loss_ce: 0.018509
2022-01-09 12:32:44,365 iteration 2589 : loss : 0.053061, loss_ce: 0.020588
2022-01-09 12:32:45,783 iteration 2590 : loss : 0.037056, loss_ce: 0.015555
2022-01-09 12:32:47,120 iteration 2591 : loss : 0.037804, loss_ce: 0.012442
2022-01-09 12:32:48,459 iteration 2592 : loss : 0.038664, loss_ce: 0.013496
2022-01-09 12:32:49,805 iteration 2593 : loss : 0.036784, loss_ce: 0.016240
2022-01-09 12:32:51,302 iteration 2594 : loss : 0.051247, loss_ce: 0.019384
2022-01-09 12:32:52,716 iteration 2595 : loss : 0.047463, loss_ce: 0.020148
2022-01-09 12:32:54,054 iteration 2596 : loss : 0.033939, loss_ce: 0.015661
2022-01-09 12:32:55,372 iteration 2597 : loss : 0.038296, loss_ce: 0.016743
2022-01-09 12:32:56,717 iteration 2598 : loss : 0.030791, loss_ce: 0.013273
2022-01-09 12:32:58,076 iteration 2599 : loss : 0.027772, loss_ce: 0.006935
2022-01-09 12:32:59,432 iteration 2600 : loss : 0.038644, loss_ce: 0.014719
2022-01-09 12:33:00,742 iteration 2601 : loss : 0.034559, loss_ce: 0.014422
 38%|██████████▎                | 153/400 [1:05:38<1:40:48, 24.49s/it]2022-01-09 12:33:02,240 iteration 2602 : loss : 0.031570, loss_ce: 0.013564
2022-01-09 12:33:03,541 iteration 2603 : loss : 0.028556, loss_ce: 0.011367
2022-01-09 12:33:04,890 iteration 2604 : loss : 0.037382, loss_ce: 0.014041
2022-01-09 12:33:06,376 iteration 2605 : loss : 0.030175, loss_ce: 0.010981
2022-01-09 12:33:07,741 iteration 2606 : loss : 0.039872, loss_ce: 0.017421
2022-01-09 12:33:09,120 iteration 2607 : loss : 0.045765, loss_ce: 0.020082
2022-01-09 12:33:10,439 iteration 2608 : loss : 0.036031, loss_ce: 0.015902
2022-01-09 12:33:11,774 iteration 2609 : loss : 0.048303, loss_ce: 0.014637
2022-01-09 12:33:13,254 iteration 2610 : loss : 0.052910, loss_ce: 0.023877
2022-01-09 12:33:14,599 iteration 2611 : loss : 0.041773, loss_ce: 0.014041
2022-01-09 12:33:15,877 iteration 2612 : loss : 0.041104, loss_ce: 0.018967
2022-01-09 12:33:17,217 iteration 2613 : loss : 0.042897, loss_ce: 0.014526
2022-01-09 12:33:18,636 iteration 2614 : loss : 0.048268, loss_ce: 0.017151
2022-01-09 12:33:19,972 iteration 2615 : loss : 0.036608, loss_ce: 0.015304
2022-01-09 12:33:21,317 iteration 2616 : loss : 0.039411, loss_ce: 0.015941
2022-01-09 12:33:22,761 iteration 2617 : loss : 0.039618, loss_ce: 0.010999
2022-01-09 12:33:24,146 iteration 2618 : loss : 0.055115, loss_ce: 0.024450
 38%|██████████▍                | 154/400 [1:06:01<1:39:03, 24.16s/it]2022-01-09 12:33:25,627 iteration 2619 : loss : 0.088716, loss_ce: 0.034960
2022-01-09 12:33:26,972 iteration 2620 : loss : 0.037971, loss_ce: 0.013694
2022-01-09 12:33:28,362 iteration 2621 : loss : 0.044419, loss_ce: 0.017247
2022-01-09 12:33:29,757 iteration 2622 : loss : 0.043534, loss_ce: 0.021977
2022-01-09 12:33:31,158 iteration 2623 : loss : 0.051395, loss_ce: 0.024102
2022-01-09 12:33:32,489 iteration 2624 : loss : 0.034401, loss_ce: 0.009818
2022-01-09 12:33:33,832 iteration 2625 : loss : 0.030489, loss_ce: 0.012066
2022-01-09 12:33:35,259 iteration 2626 : loss : 0.044210, loss_ce: 0.019851
2022-01-09 12:33:36,652 iteration 2627 : loss : 0.052119, loss_ce: 0.014656
2022-01-09 12:33:37,981 iteration 2628 : loss : 0.037051, loss_ce: 0.013401
2022-01-09 12:33:39,406 iteration 2629 : loss : 0.053140, loss_ce: 0.018042
2022-01-09 12:33:40,722 iteration 2630 : loss : 0.029988, loss_ce: 0.008762
2022-01-09 12:33:42,103 iteration 2631 : loss : 0.047652, loss_ce: 0.021793
2022-01-09 12:33:43,495 iteration 2632 : loss : 0.058232, loss_ce: 0.024043
2022-01-09 12:33:44,828 iteration 2633 : loss : 0.041433, loss_ce: 0.019852
2022-01-09 12:33:46,214 iteration 2634 : loss : 0.041245, loss_ce: 0.013154
2022-01-09 12:33:46,214 Training Data Eval:
2022-01-09 12:33:53,061   Average segmentation loss on training set: 0.0280
2022-01-09 12:33:53,061 Validation Data Eval:
2022-01-09 12:33:55,429   Average segmentation loss on validation set: 0.1030
2022-01-09 12:33:56,867 iteration 2635 : loss : 0.054837, loss_ce: 0.021784
 39%|██████████▍                | 155/400 [1:06:34<1:49:08, 26.73s/it]2022-01-09 12:33:58,304 iteration 2636 : loss : 0.030172, loss_ce: 0.009360
2022-01-09 12:33:59,796 iteration 2637 : loss : 0.052002, loss_ce: 0.018972
2022-01-09 12:34:01,174 iteration 2638 : loss : 0.027669, loss_ce: 0.010367
2022-01-09 12:34:02,437 iteration 2639 : loss : 0.030543, loss_ce: 0.011199
2022-01-09 12:34:03,756 iteration 2640 : loss : 0.039346, loss_ce: 0.015854
2022-01-09 12:34:05,204 iteration 2641 : loss : 0.042197, loss_ce: 0.015959
2022-01-09 12:34:06,621 iteration 2642 : loss : 0.066570, loss_ce: 0.018330
2022-01-09 12:34:08,034 iteration 2643 : loss : 0.047736, loss_ce: 0.015650
2022-01-09 12:34:09,376 iteration 2644 : loss : 0.050302, loss_ce: 0.016573
2022-01-09 12:34:10,712 iteration 2645 : loss : 0.042540, loss_ce: 0.025034
2022-01-09 12:34:12,057 iteration 2646 : loss : 0.047053, loss_ce: 0.015189
2022-01-09 12:34:13,473 iteration 2647 : loss : 0.042310, loss_ce: 0.016981
2022-01-09 12:34:14,769 iteration 2648 : loss : 0.024705, loss_ce: 0.011626
2022-01-09 12:34:16,172 iteration 2649 : loss : 0.040682, loss_ce: 0.017279
2022-01-09 12:34:17,571 iteration 2650 : loss : 0.040431, loss_ce: 0.016934
2022-01-09 12:34:18,912 iteration 2651 : loss : 0.047397, loss_ce: 0.015133
2022-01-09 12:34:20,284 iteration 2652 : loss : 0.033517, loss_ce: 0.015367
 39%|██████████▌                | 156/400 [1:06:57<1:44:39, 25.74s/it]2022-01-09 12:34:21,810 iteration 2653 : loss : 0.069495, loss_ce: 0.022508
2022-01-09 12:34:23,183 iteration 2654 : loss : 0.042464, loss_ce: 0.020859
2022-01-09 12:34:24,499 iteration 2655 : loss : 0.025145, loss_ce: 0.009673
2022-01-09 12:34:25,801 iteration 2656 : loss : 0.030270, loss_ce: 0.014254
2022-01-09 12:34:27,119 iteration 2657 : loss : 0.031478, loss_ce: 0.011209
2022-01-09 12:34:28,451 iteration 2658 : loss : 0.032175, loss_ce: 0.012920
2022-01-09 12:34:29,931 iteration 2659 : loss : 0.043600, loss_ce: 0.019470
2022-01-09 12:34:31,299 iteration 2660 : loss : 0.034400, loss_ce: 0.012754
2022-01-09 12:34:32,590 iteration 2661 : loss : 0.035182, loss_ce: 0.013638
2022-01-09 12:34:34,000 iteration 2662 : loss : 0.048302, loss_ce: 0.020110
2022-01-09 12:34:35,325 iteration 2663 : loss : 0.053218, loss_ce: 0.020909
2022-01-09 12:34:36,742 iteration 2664 : loss : 0.030208, loss_ce: 0.010506
2022-01-09 12:34:38,090 iteration 2665 : loss : 0.037802, loss_ce: 0.013092
2022-01-09 12:34:39,460 iteration 2666 : loss : 0.034427, loss_ce: 0.010804
2022-01-09 12:34:40,859 iteration 2667 : loss : 0.029779, loss_ce: 0.011430
2022-01-09 12:34:42,254 iteration 2668 : loss : 0.048978, loss_ce: 0.019254
2022-01-09 12:34:43,665 iteration 2669 : loss : 0.043439, loss_ce: 0.017206
 39%|██████████▌                | 157/400 [1:07:21<1:41:22, 25.03s/it]2022-01-09 12:34:45,119 iteration 2670 : loss : 0.070242, loss_ce: 0.025500
2022-01-09 12:34:46,508 iteration 2671 : loss : 0.054993, loss_ce: 0.015144
2022-01-09 12:34:47,907 iteration 2672 : loss : 0.029063, loss_ce: 0.011047
2022-01-09 12:34:49,327 iteration 2673 : loss : 0.054596, loss_ce: 0.020763
2022-01-09 12:34:50,663 iteration 2674 : loss : 0.045358, loss_ce: 0.024438
2022-01-09 12:34:52,086 iteration 2675 : loss : 0.050471, loss_ce: 0.020330
2022-01-09 12:34:53,469 iteration 2676 : loss : 0.039383, loss_ce: 0.011846
2022-01-09 12:34:54,786 iteration 2677 : loss : 0.027563, loss_ce: 0.013015
2022-01-09 12:34:56,081 iteration 2678 : loss : 0.036550, loss_ce: 0.011920
2022-01-09 12:34:57,467 iteration 2679 : loss : 0.053276, loss_ce: 0.022994
2022-01-09 12:34:58,892 iteration 2680 : loss : 0.053484, loss_ce: 0.022209
2022-01-09 12:35:00,253 iteration 2681 : loss : 0.038321, loss_ce: 0.018563
2022-01-09 12:35:01,566 iteration 2682 : loss : 0.037231, loss_ce: 0.015949
2022-01-09 12:35:02,941 iteration 2683 : loss : 0.042531, loss_ce: 0.020708
2022-01-09 12:35:04,304 iteration 2684 : loss : 0.047914, loss_ce: 0.016210
2022-01-09 12:35:05,672 iteration 2685 : loss : 0.035765, loss_ce: 0.016860
2022-01-09 12:35:07,033 iteration 2686 : loss : 0.035358, loss_ce: 0.014013
 40%|██████████▋                | 158/400 [1:07:44<1:38:56, 24.53s/it]2022-01-09 12:35:08,434 iteration 2687 : loss : 0.025881, loss_ce: 0.012010
2022-01-09 12:35:09,786 iteration 2688 : loss : 0.038089, loss_ce: 0.017074
2022-01-09 12:35:11,046 iteration 2689 : loss : 0.032442, loss_ce: 0.013291
2022-01-09 12:35:12,395 iteration 2690 : loss : 0.044791, loss_ce: 0.019577
2022-01-09 12:35:13,820 iteration 2691 : loss : 0.033067, loss_ce: 0.011013
2022-01-09 12:35:15,174 iteration 2692 : loss : 0.039700, loss_ce: 0.013716
2022-01-09 12:35:16,503 iteration 2693 : loss : 0.045000, loss_ce: 0.019312
2022-01-09 12:35:17,784 iteration 2694 : loss : 0.040139, loss_ce: 0.014431
2022-01-09 12:35:19,187 iteration 2695 : loss : 0.041299, loss_ce: 0.015739
2022-01-09 12:35:20,644 iteration 2696 : loss : 0.057045, loss_ce: 0.023298
2022-01-09 12:35:22,063 iteration 2697 : loss : 0.043454, loss_ce: 0.023023
2022-01-09 12:35:23,382 iteration 2698 : loss : 0.030733, loss_ce: 0.012269
2022-01-09 12:35:24,710 iteration 2699 : loss : 0.036351, loss_ce: 0.011734
2022-01-09 12:35:26,037 iteration 2700 : loss : 0.046302, loss_ce: 0.013332
2022-01-09 12:35:27,406 iteration 2701 : loss : 0.054370, loss_ce: 0.024787
2022-01-09 12:35:28,804 iteration 2702 : loss : 0.051760, loss_ce: 0.011246
2022-01-09 12:35:30,210 iteration 2703 : loss : 0.045017, loss_ce: 0.017840
 40%|██████████▋                | 159/400 [1:08:07<1:36:54, 24.12s/it]2022-01-09 12:35:31,585 iteration 2704 : loss : 0.031256, loss_ce: 0.008932
2022-01-09 12:35:32,902 iteration 2705 : loss : 0.033091, loss_ce: 0.011185
2022-01-09 12:35:34,300 iteration 2706 : loss : 0.051268, loss_ce: 0.024591
2022-01-09 12:35:35,659 iteration 2707 : loss : 0.039745, loss_ce: 0.015604
2022-01-09 12:35:37,002 iteration 2708 : loss : 0.060914, loss_ce: 0.018906
2022-01-09 12:35:38,429 iteration 2709 : loss : 0.053590, loss_ce: 0.015272
2022-01-09 12:35:39,838 iteration 2710 : loss : 0.050451, loss_ce: 0.019839
2022-01-09 12:35:41,175 iteration 2711 : loss : 0.037863, loss_ce: 0.012770
2022-01-09 12:35:42,524 iteration 2712 : loss : 0.051943, loss_ce: 0.018942
2022-01-09 12:35:43,897 iteration 2713 : loss : 0.036956, loss_ce: 0.012060
2022-01-09 12:35:45,262 iteration 2714 : loss : 0.039790, loss_ce: 0.015822
2022-01-09 12:35:46,619 iteration 2715 : loss : 0.058689, loss_ce: 0.030395
2022-01-09 12:35:47,969 iteration 2716 : loss : 0.055178, loss_ce: 0.014979
2022-01-09 12:35:49,278 iteration 2717 : loss : 0.042378, loss_ce: 0.015016
2022-01-09 12:35:50,667 iteration 2718 : loss : 0.053535, loss_ce: 0.019469
2022-01-09 12:35:52,005 iteration 2719 : loss : 0.037428, loss_ce: 0.017267
2022-01-09 12:35:52,005 Training Data Eval:
2022-01-09 12:35:58,879   Average segmentation loss on training set: 0.0357
2022-01-09 12:35:58,879 Validation Data Eval:
2022-01-09 12:36:01,251   Average segmentation loss on validation set: 0.1213
2022-01-09 12:36:02,598 iteration 2720 : loss : 0.038469, loss_ce: 0.017147
 40%|██████████▊                | 160/400 [1:08:40<1:46:24, 26.60s/it]2022-01-09 12:36:04,082 iteration 2721 : loss : 0.047708, loss_ce: 0.015556
2022-01-09 12:36:05,471 iteration 2722 : loss : 0.038280, loss_ce: 0.016967
2022-01-09 12:36:06,906 iteration 2723 : loss : 0.039477, loss_ce: 0.019164
2022-01-09 12:36:08,221 iteration 2724 : loss : 0.043894, loss_ce: 0.016015
2022-01-09 12:36:09,562 iteration 2725 : loss : 0.046612, loss_ce: 0.013379
2022-01-09 12:36:10,990 iteration 2726 : loss : 0.031703, loss_ce: 0.010929
2022-01-09 12:36:12,351 iteration 2727 : loss : 0.036773, loss_ce: 0.013495
2022-01-09 12:36:13,759 iteration 2728 : loss : 0.033395, loss_ce: 0.012074
2022-01-09 12:36:15,155 iteration 2729 : loss : 0.062170, loss_ce: 0.028985
2022-01-09 12:36:16,591 iteration 2730 : loss : 0.051645, loss_ce: 0.019682
2022-01-09 12:36:17,947 iteration 2731 : loss : 0.062623, loss_ce: 0.023869
2022-01-09 12:36:19,226 iteration 2732 : loss : 0.029254, loss_ce: 0.011337
2022-01-09 12:36:20,581 iteration 2733 : loss : 0.028490, loss_ce: 0.012412
2022-01-09 12:36:21,951 iteration 2734 : loss : 0.045465, loss_ce: 0.011170
2022-01-09 12:36:23,322 iteration 2735 : loss : 0.038477, loss_ce: 0.019219
2022-01-09 12:36:24,667 iteration 2736 : loss : 0.047809, loss_ce: 0.015870
2022-01-09 12:36:26,018 iteration 2737 : loss : 0.060201, loss_ce: 0.017149
 40%|██████████▊                | 161/400 [1:09:03<1:42:09, 25.65s/it]2022-01-09 12:36:27,484 iteration 2738 : loss : 0.040398, loss_ce: 0.019655
2022-01-09 12:36:28,800 iteration 2739 : loss : 0.042536, loss_ce: 0.011381
2022-01-09 12:36:30,242 iteration 2740 : loss : 0.048010, loss_ce: 0.021454
2022-01-09 12:36:31,607 iteration 2741 : loss : 0.044522, loss_ce: 0.021229
2022-01-09 12:36:32,982 iteration 2742 : loss : 0.051340, loss_ce: 0.024830
2022-01-09 12:36:34,295 iteration 2743 : loss : 0.054459, loss_ce: 0.021310
2022-01-09 12:36:35,648 iteration 2744 : loss : 0.033512, loss_ce: 0.011355
2022-01-09 12:36:36,970 iteration 2745 : loss : 0.026084, loss_ce: 0.008222
2022-01-09 12:36:38,425 iteration 2746 : loss : 0.114173, loss_ce: 0.028498
2022-01-09 12:36:39,766 iteration 2747 : loss : 0.037205, loss_ce: 0.014576
2022-01-09 12:36:41,176 iteration 2748 : loss : 0.034425, loss_ce: 0.008458
2022-01-09 12:36:42,612 iteration 2749 : loss : 0.040966, loss_ce: 0.015054
2022-01-09 12:36:43,965 iteration 2750 : loss : 0.074078, loss_ce: 0.035324
2022-01-09 12:36:45,328 iteration 2751 : loss : 0.068442, loss_ce: 0.018896
2022-01-09 12:36:46,791 iteration 2752 : loss : 0.054659, loss_ce: 0.021152
2022-01-09 12:36:48,258 iteration 2753 : loss : 0.039034, loss_ce: 0.017525
2022-01-09 12:36:49,641 iteration 2754 : loss : 0.036064, loss_ce: 0.017879
 40%|██████████▉                | 162/400 [1:09:27<1:39:19, 25.04s/it]2022-01-09 12:36:50,944 iteration 2755 : loss : 0.032827, loss_ce: 0.010941
2022-01-09 12:36:52,321 iteration 2756 : loss : 0.033886, loss_ce: 0.013693
2022-01-09 12:36:53,687 iteration 2757 : loss : 0.039128, loss_ce: 0.017934
2022-01-09 12:36:55,043 iteration 2758 : loss : 0.035533, loss_ce: 0.013551
2022-01-09 12:36:56,406 iteration 2759 : loss : 0.047507, loss_ce: 0.020215
2022-01-09 12:36:57,773 iteration 2760 : loss : 0.040628, loss_ce: 0.017529
2022-01-09 12:36:59,172 iteration 2761 : loss : 0.039800, loss_ce: 0.014081
2022-01-09 12:37:00,530 iteration 2762 : loss : 0.036737, loss_ce: 0.018433
2022-01-09 12:37:01,925 iteration 2763 : loss : 0.052250, loss_ce: 0.017102
2022-01-09 12:37:03,232 iteration 2764 : loss : 0.038519, loss_ce: 0.015073
2022-01-09 12:37:04,701 iteration 2765 : loss : 0.030088, loss_ce: 0.010244
2022-01-09 12:37:06,002 iteration 2766 : loss : 0.039517, loss_ce: 0.014603
2022-01-09 12:37:07,382 iteration 2767 : loss : 0.051150, loss_ce: 0.020573
2022-01-09 12:37:08,725 iteration 2768 : loss : 0.050312, loss_ce: 0.017035
2022-01-09 12:37:10,026 iteration 2769 : loss : 0.030598, loss_ce: 0.012947
2022-01-09 12:37:11,354 iteration 2770 : loss : 0.037841, loss_ce: 0.016789
2022-01-09 12:37:12,713 iteration 2771 : loss : 0.033873, loss_ce: 0.013952
 41%|███████████                | 163/400 [1:09:50<1:36:34, 24.45s/it]2022-01-09 12:37:14,068 iteration 2772 : loss : 0.038893, loss_ce: 0.014577
2022-01-09 12:37:15,440 iteration 2773 : loss : 0.037356, loss_ce: 0.015349
2022-01-09 12:37:16,813 iteration 2774 : loss : 0.066474, loss_ce: 0.013892
2022-01-09 12:37:18,197 iteration 2775 : loss : 0.042424, loss_ce: 0.016216
2022-01-09 12:37:19,545 iteration 2776 : loss : 0.042338, loss_ce: 0.015864
2022-01-09 12:37:20,967 iteration 2777 : loss : 0.033086, loss_ce: 0.011705
2022-01-09 12:37:22,298 iteration 2778 : loss : 0.033640, loss_ce: 0.011008
2022-01-09 12:37:23,633 iteration 2779 : loss : 0.031092, loss_ce: 0.011856
2022-01-09 12:37:24,986 iteration 2780 : loss : 0.038788, loss_ce: 0.017292
2022-01-09 12:37:26,465 iteration 2781 : loss : 0.045252, loss_ce: 0.020521
2022-01-09 12:37:27,906 iteration 2782 : loss : 0.045908, loss_ce: 0.019465
2022-01-09 12:37:29,264 iteration 2783 : loss : 0.038521, loss_ce: 0.012988
2022-01-09 12:37:30,529 iteration 2784 : loss : 0.040892, loss_ce: 0.011653
2022-01-09 12:37:31,844 iteration 2785 : loss : 0.031675, loss_ce: 0.014740
2022-01-09 12:37:33,149 iteration 2786 : loss : 0.033155, loss_ce: 0.014028
2022-01-09 12:37:34,474 iteration 2787 : loss : 0.033593, loss_ce: 0.012511
2022-01-09 12:37:35,966 iteration 2788 : loss : 0.052559, loss_ce: 0.021399
 41%|███████████                | 164/400 [1:10:13<1:34:45, 24.09s/it]2022-01-09 12:37:37,412 iteration 2789 : loss : 0.031747, loss_ce: 0.009915
2022-01-09 12:37:38,862 iteration 2790 : loss : 0.059906, loss_ce: 0.022331
2022-01-09 12:37:40,170 iteration 2791 : loss : 0.028360, loss_ce: 0.008992
2022-01-09 12:37:41,524 iteration 2792 : loss : 0.029221, loss_ce: 0.012090
2022-01-09 12:37:42,886 iteration 2793 : loss : 0.041707, loss_ce: 0.013951
2022-01-09 12:37:44,311 iteration 2794 : loss : 0.039840, loss_ce: 0.020392
2022-01-09 12:37:45,667 iteration 2795 : loss : 0.042461, loss_ce: 0.017474
2022-01-09 12:37:47,009 iteration 2796 : loss : 0.036710, loss_ce: 0.012508
2022-01-09 12:37:48,411 iteration 2797 : loss : 0.031149, loss_ce: 0.013888
2022-01-09 12:37:49,802 iteration 2798 : loss : 0.047552, loss_ce: 0.015220
2022-01-09 12:37:51,251 iteration 2799 : loss : 0.046956, loss_ce: 0.015510
2022-01-09 12:37:52,639 iteration 2800 : loss : 0.035621, loss_ce: 0.017224
2022-01-09 12:37:53,924 iteration 2801 : loss : 0.056064, loss_ce: 0.015058
2022-01-09 12:37:55,248 iteration 2802 : loss : 0.022684, loss_ce: 0.007775
2022-01-09 12:37:56,628 iteration 2803 : loss : 0.037957, loss_ce: 0.014601
2022-01-09 12:37:57,912 iteration 2804 : loss : 0.039351, loss_ce: 0.017118
2022-01-09 12:37:57,912 Training Data Eval:
2022-01-09 12:38:04,780   Average segmentation loss on training set: 0.0241
2022-01-09 12:38:04,781 Validation Data Eval:
2022-01-09 12:38:07,138   Average segmentation loss on validation set: 0.0924
2022-01-09 12:38:08,592 iteration 2805 : loss : 0.045101, loss_ce: 0.017935
 41%|███████████▏               | 165/400 [1:10:46<1:44:23, 26.65s/it]2022-01-09 12:38:10,086 iteration 2806 : loss : 0.052252, loss_ce: 0.021605
2022-01-09 12:38:11,530 iteration 2807 : loss : 0.050986, loss_ce: 0.015168
2022-01-09 12:38:12,945 iteration 2808 : loss : 0.044116, loss_ce: 0.013770
2022-01-09 12:38:14,332 iteration 2809 : loss : 0.041142, loss_ce: 0.017082
2022-01-09 12:38:15,633 iteration 2810 : loss : 0.035213, loss_ce: 0.013523
2022-01-09 12:38:17,048 iteration 2811 : loss : 0.032635, loss_ce: 0.014894
2022-01-09 12:38:18,351 iteration 2812 : loss : 0.024176, loss_ce: 0.010149
2022-01-09 12:38:19,779 iteration 2813 : loss : 0.057634, loss_ce: 0.029924
2022-01-09 12:38:21,163 iteration 2814 : loss : 0.042031, loss_ce: 0.017286
2022-01-09 12:38:22,528 iteration 2815 : loss : 0.062168, loss_ce: 0.026477
2022-01-09 12:38:23,896 iteration 2816 : loss : 0.029382, loss_ce: 0.009807
2022-01-09 12:38:25,340 iteration 2817 : loss : 0.056811, loss_ce: 0.013546
2022-01-09 12:38:26,763 iteration 2818 : loss : 0.067016, loss_ce: 0.038706
2022-01-09 12:38:28,118 iteration 2819 : loss : 0.043954, loss_ce: 0.020703
2022-01-09 12:38:29,589 iteration 2820 : loss : 0.043932, loss_ce: 0.015222
2022-01-09 12:38:31,031 iteration 2821 : loss : 0.057502, loss_ce: 0.021875
2022-01-09 12:38:32,434 iteration 2822 : loss : 0.033587, loss_ce: 0.010841
 42%|███████████▏               | 166/400 [1:11:10<1:40:39, 25.81s/it]2022-01-09 12:38:33,964 iteration 2823 : loss : 0.035712, loss_ce: 0.015620
2022-01-09 12:38:35,308 iteration 2824 : loss : 0.032355, loss_ce: 0.015864
2022-01-09 12:38:36,693 iteration 2825 : loss : 0.062397, loss_ce: 0.025129
2022-01-09 12:38:38,164 iteration 2826 : loss : 0.059336, loss_ce: 0.022076
2022-01-09 12:38:39,656 iteration 2827 : loss : 0.039496, loss_ce: 0.012457
2022-01-09 12:38:41,057 iteration 2828 : loss : 0.042858, loss_ce: 0.015634
2022-01-09 12:38:42,430 iteration 2829 : loss : 0.050696, loss_ce: 0.022414
2022-01-09 12:38:43,789 iteration 2830 : loss : 0.039788, loss_ce: 0.014980
2022-01-09 12:38:45,072 iteration 2831 : loss : 0.030647, loss_ce: 0.012847
2022-01-09 12:38:46,567 iteration 2832 : loss : 0.035273, loss_ce: 0.016210
2022-01-09 12:38:47,925 iteration 2833 : loss : 0.042721, loss_ce: 0.015650
2022-01-09 12:38:49,357 iteration 2834 : loss : 0.040933, loss_ce: 0.014987
2022-01-09 12:38:50,763 iteration 2835 : loss : 0.044257, loss_ce: 0.014147
2022-01-09 12:38:52,135 iteration 2836 : loss : 0.046220, loss_ce: 0.017632
2022-01-09 12:38:53,565 iteration 2837 : loss : 0.040311, loss_ce: 0.014868
2022-01-09 12:38:54,879 iteration 2838 : loss : 0.028896, loss_ce: 0.012571
2022-01-09 12:38:56,285 iteration 2839 : loss : 0.035145, loss_ce: 0.012572
 42%|███████████▎               | 167/400 [1:11:33<1:37:56, 25.22s/it]2022-01-09 12:38:57,772 iteration 2840 : loss : 0.036197, loss_ce: 0.012575
2022-01-09 12:38:59,135 iteration 2841 : loss : 0.043918, loss_ce: 0.014146
2022-01-09 12:39:00,498 iteration 2842 : loss : 0.031222, loss_ce: 0.011180
2022-01-09 12:39:01,915 iteration 2843 : loss : 0.060546, loss_ce: 0.016586
2022-01-09 12:39:03,302 iteration 2844 : loss : 0.049408, loss_ce: 0.021128
2022-01-09 12:39:04,672 iteration 2845 : loss : 0.042525, loss_ce: 0.016119
2022-01-09 12:39:06,055 iteration 2846 : loss : 0.032895, loss_ce: 0.011297
2022-01-09 12:39:07,313 iteration 2847 : loss : 0.025236, loss_ce: 0.013313
2022-01-09 12:39:08,693 iteration 2848 : loss : 0.049588, loss_ce: 0.019354
2022-01-09 12:39:10,113 iteration 2849 : loss : 0.044294, loss_ce: 0.016678
2022-01-09 12:39:11,523 iteration 2850 : loss : 0.039281, loss_ce: 0.015463
2022-01-09 12:39:12,943 iteration 2851 : loss : 0.035567, loss_ce: 0.012592
2022-01-09 12:39:14,331 iteration 2852 : loss : 0.037743, loss_ce: 0.011131
2022-01-09 12:39:15,762 iteration 2853 : loss : 0.032782, loss_ce: 0.011290
2022-01-09 12:39:17,172 iteration 2854 : loss : 0.032274, loss_ce: 0.011963
2022-01-09 12:39:18,514 iteration 2855 : loss : 0.059996, loss_ce: 0.039083
2022-01-09 12:39:19,922 iteration 2856 : loss : 0.038642, loss_ce: 0.018051
 42%|███████████▎               | 168/400 [1:11:57<1:35:40, 24.75s/it]2022-01-09 12:39:21,265 iteration 2857 : loss : 0.031475, loss_ce: 0.011206
2022-01-09 12:39:22,642 iteration 2858 : loss : 0.038729, loss_ce: 0.016284
2022-01-09 12:39:23,980 iteration 2859 : loss : 0.027993, loss_ce: 0.012019
2022-01-09 12:39:25,384 iteration 2860 : loss : 0.039485, loss_ce: 0.014176
2022-01-09 12:39:26,675 iteration 2861 : loss : 0.032609, loss_ce: 0.014431
2022-01-09 12:39:28,128 iteration 2862 : loss : 0.062049, loss_ce: 0.020476
2022-01-09 12:39:29,452 iteration 2863 : loss : 0.034890, loss_ce: 0.011476
2022-01-09 12:39:30,853 iteration 2864 : loss : 0.034014, loss_ce: 0.013586
2022-01-09 12:39:32,263 iteration 2865 : loss : 0.045124, loss_ce: 0.021580
2022-01-09 12:39:33,613 iteration 2866 : loss : 0.034282, loss_ce: 0.012943
2022-01-09 12:39:35,088 iteration 2867 : loss : 0.032986, loss_ce: 0.013309
2022-01-09 12:39:36,425 iteration 2868 : loss : 0.035372, loss_ce: 0.012318
2022-01-09 12:39:37,881 iteration 2869 : loss : 0.036789, loss_ce: 0.016231
2022-01-09 12:39:39,279 iteration 2870 : loss : 0.035648, loss_ce: 0.011418
2022-01-09 12:39:40,623 iteration 2871 : loss : 0.042279, loss_ce: 0.015474
2022-01-09 12:39:41,927 iteration 2872 : loss : 0.040693, loss_ce: 0.016507
2022-01-09 12:39:43,341 iteration 2873 : loss : 0.049902, loss_ce: 0.011601
 42%|███████████▍               | 169/400 [1:12:21<1:33:44, 24.35s/it]2022-01-09 12:39:44,807 iteration 2874 : loss : 0.032571, loss_ce: 0.010275
2022-01-09 12:39:46,198 iteration 2875 : loss : 0.045104, loss_ce: 0.018639
2022-01-09 12:39:47,573 iteration 2876 : loss : 0.041362, loss_ce: 0.017892
2022-01-09 12:39:49,005 iteration 2877 : loss : 0.056277, loss_ce: 0.015705
2022-01-09 12:39:50,376 iteration 2878 : loss : 0.032828, loss_ce: 0.016866
2022-01-09 12:39:51,796 iteration 2879 : loss : 0.032107, loss_ce: 0.011425
2022-01-09 12:39:53,233 iteration 2880 : loss : 0.043961, loss_ce: 0.014464
2022-01-09 12:39:54,619 iteration 2881 : loss : 0.030896, loss_ce: 0.012558
2022-01-09 12:39:56,055 iteration 2882 : loss : 0.030297, loss_ce: 0.015815
2022-01-09 12:39:57,444 iteration 2883 : loss : 0.051804, loss_ce: 0.024044
2022-01-09 12:39:58,803 iteration 2884 : loss : 0.029527, loss_ce: 0.012361
2022-01-09 12:40:00,200 iteration 2885 : loss : 0.041257, loss_ce: 0.019815
2022-01-09 12:40:01,493 iteration 2886 : loss : 0.047243, loss_ce: 0.015551
2022-01-09 12:40:02,936 iteration 2887 : loss : 0.057777, loss_ce: 0.032688
2022-01-09 12:40:04,326 iteration 2888 : loss : 0.063212, loss_ce: 0.015679
2022-01-09 12:40:05,810 iteration 2889 : loss : 0.038039, loss_ce: 0.014798
2022-01-09 12:40:05,810 Training Data Eval:
2022-01-09 12:40:12,688   Average segmentation loss on training set: 0.0252
2022-01-09 12:40:12,688 Validation Data Eval:
2022-01-09 12:40:15,055   Average segmentation loss on validation set: 0.0851
2022-01-09 12:40:20,945 Found new lowest validation loss at iteration 2889! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 12:40:22,478 iteration 2890 : loss : 0.032467, loss_ce: 0.011673
 42%|███████████▍               | 170/400 [1:13:00<1:50:20, 28.78s/it]2022-01-09 12:40:23,973 iteration 2891 : loss : 0.038657, loss_ce: 0.016273
2022-01-09 12:40:25,423 iteration 2892 : loss : 0.036509, loss_ce: 0.010831
2022-01-09 12:40:26,753 iteration 2893 : loss : 0.033200, loss_ce: 0.011635
2022-01-09 12:40:28,198 iteration 2894 : loss : 0.042091, loss_ce: 0.013967
2022-01-09 12:40:29,633 iteration 2895 : loss : 0.055487, loss_ce: 0.028126
2022-01-09 12:40:30,978 iteration 2896 : loss : 0.024166, loss_ce: 0.007835
2022-01-09 12:40:32,355 iteration 2897 : loss : 0.036118, loss_ce: 0.014092
2022-01-09 12:40:33,674 iteration 2898 : loss : 0.033943, loss_ce: 0.016584
2022-01-09 12:40:35,073 iteration 2899 : loss : 0.048145, loss_ce: 0.016503
2022-01-09 12:40:36,437 iteration 2900 : loss : 0.039821, loss_ce: 0.015113
2022-01-09 12:40:37,818 iteration 2901 : loss : 0.039862, loss_ce: 0.012613
2022-01-09 12:40:39,125 iteration 2902 : loss : 0.044459, loss_ce: 0.015084
2022-01-09 12:40:40,455 iteration 2903 : loss : 0.032059, loss_ce: 0.015208
2022-01-09 12:40:41,780 iteration 2904 : loss : 0.029309, loss_ce: 0.011097
2022-01-09 12:40:43,079 iteration 2905 : loss : 0.038978, loss_ce: 0.019349
2022-01-09 12:40:44,482 iteration 2906 : loss : 0.039588, loss_ce: 0.021519
2022-01-09 12:40:45,816 iteration 2907 : loss : 0.035727, loss_ce: 0.012057
 43%|███████████▌               | 171/400 [1:13:23<1:43:37, 27.15s/it]2022-01-09 12:40:47,277 iteration 2908 : loss : 0.069835, loss_ce: 0.036051
2022-01-09 12:40:48,705 iteration 2909 : loss : 0.043083, loss_ce: 0.019793
2022-01-09 12:40:50,079 iteration 2910 : loss : 0.032094, loss_ce: 0.014203
2022-01-09 12:40:51,497 iteration 2911 : loss : 0.035589, loss_ce: 0.011086
2022-01-09 12:40:52,839 iteration 2912 : loss : 0.037765, loss_ce: 0.014624
2022-01-09 12:40:54,275 iteration 2913 : loss : 0.044853, loss_ce: 0.022274
2022-01-09 12:40:55,626 iteration 2914 : loss : 0.061078, loss_ce: 0.031514
2022-01-09 12:40:56,984 iteration 2915 : loss : 0.028555, loss_ce: 0.011872
2022-01-09 12:40:58,365 iteration 2916 : loss : 0.031929, loss_ce: 0.013440
2022-01-09 12:40:59,670 iteration 2917 : loss : 0.035397, loss_ce: 0.015922
2022-01-09 12:41:00,998 iteration 2918 : loss : 0.038940, loss_ce: 0.012063
2022-01-09 12:41:02,364 iteration 2919 : loss : 0.043732, loss_ce: 0.013253
2022-01-09 12:41:03,755 iteration 2920 : loss : 0.038656, loss_ce: 0.013687
2022-01-09 12:41:05,114 iteration 2921 : loss : 0.035296, loss_ce: 0.016526
2022-01-09 12:41:06,423 iteration 2922 : loss : 0.041187, loss_ce: 0.015986
2022-01-09 12:41:07,760 iteration 2923 : loss : 0.035334, loss_ce: 0.012271
2022-01-09 12:41:09,131 iteration 2924 : loss : 0.044842, loss_ce: 0.016705
 43%|███████████▌               | 172/400 [1:13:46<1:38:47, 26.00s/it]2022-01-09 12:41:10,458 iteration 2925 : loss : 0.029890, loss_ce: 0.015020
2022-01-09 12:41:11,907 iteration 2926 : loss : 0.041787, loss_ce: 0.018662
2022-01-09 12:41:13,272 iteration 2927 : loss : 0.046999, loss_ce: 0.018002
2022-01-09 12:41:14,533 iteration 2928 : loss : 0.030956, loss_ce: 0.009808
2022-01-09 12:41:15,875 iteration 2929 : loss : 0.031754, loss_ce: 0.014221
2022-01-09 12:41:17,320 iteration 2930 : loss : 0.044704, loss_ce: 0.019768
2022-01-09 12:41:18,665 iteration 2931 : loss : 0.030364, loss_ce: 0.010736
2022-01-09 12:41:19,985 iteration 2932 : loss : 0.043526, loss_ce: 0.015711
2022-01-09 12:41:21,425 iteration 2933 : loss : 0.047816, loss_ce: 0.015843
2022-01-09 12:41:22,784 iteration 2934 : loss : 0.039989, loss_ce: 0.014134
2022-01-09 12:41:24,160 iteration 2935 : loss : 0.030066, loss_ce: 0.010136
2022-01-09 12:41:25,516 iteration 2936 : loss : 0.025720, loss_ce: 0.008254
2022-01-09 12:41:26,911 iteration 2937 : loss : 0.030568, loss_ce: 0.013587
2022-01-09 12:41:28,319 iteration 2938 : loss : 0.034544, loss_ce: 0.012084
2022-01-09 12:41:29,779 iteration 2939 : loss : 0.034642, loss_ce: 0.012722
2022-01-09 12:41:31,252 iteration 2940 : loss : 0.048966, loss_ce: 0.024474
2022-01-09 12:41:32,578 iteration 2941 : loss : 0.032046, loss_ce: 0.010725
 43%|███████████▋               | 173/400 [1:14:10<1:35:28, 25.24s/it]2022-01-09 12:41:33,956 iteration 2942 : loss : 0.026692, loss_ce: 0.008032
2022-01-09 12:41:35,357 iteration 2943 : loss : 0.049798, loss_ce: 0.014653
2022-01-09 12:41:36,692 iteration 2944 : loss : 0.033769, loss_ce: 0.020030
2022-01-09 12:41:38,068 iteration 2945 : loss : 0.032546, loss_ce: 0.008823
2022-01-09 12:41:39,340 iteration 2946 : loss : 0.028506, loss_ce: 0.011198
2022-01-09 12:41:40,675 iteration 2947 : loss : 0.033888, loss_ce: 0.014534
2022-01-09 12:41:42,008 iteration 2948 : loss : 0.032506, loss_ce: 0.008675
2022-01-09 12:41:43,440 iteration 2949 : loss : 0.043460, loss_ce: 0.016241
2022-01-09 12:41:44,828 iteration 2950 : loss : 0.029312, loss_ce: 0.012425
2022-01-09 12:41:46,234 iteration 2951 : loss : 0.029640, loss_ce: 0.012586
2022-01-09 12:41:47,638 iteration 2952 : loss : 0.031998, loss_ce: 0.012165
2022-01-09 12:41:49,066 iteration 2953 : loss : 0.039651, loss_ce: 0.018438
2022-01-09 12:41:50,535 iteration 2954 : loss : 0.058997, loss_ce: 0.019398
2022-01-09 12:41:51,876 iteration 2955 : loss : 0.029815, loss_ce: 0.013038
2022-01-09 12:41:53,223 iteration 2956 : loss : 0.038119, loss_ce: 0.017137
2022-01-09 12:41:54,543 iteration 2957 : loss : 0.043452, loss_ce: 0.016532
2022-01-09 12:41:55,919 iteration 2958 : loss : 0.041333, loss_ce: 0.018894
 44%|███████████▋               | 174/400 [1:14:33<1:32:54, 24.66s/it]2022-01-09 12:41:57,322 iteration 2959 : loss : 0.025297, loss_ce: 0.008988
2022-01-09 12:41:58,669 iteration 2960 : loss : 0.031124, loss_ce: 0.013907
2022-01-09 12:42:00,052 iteration 2961 : loss : 0.041736, loss_ce: 0.015790
2022-01-09 12:42:01,568 iteration 2962 : loss : 0.028621, loss_ce: 0.010422
2022-01-09 12:42:02,960 iteration 2963 : loss : 0.043760, loss_ce: 0.016799
2022-01-09 12:42:04,244 iteration 2964 : loss : 0.037833, loss_ce: 0.014116
2022-01-09 12:42:05,624 iteration 2965 : loss : 0.030065, loss_ce: 0.015210
2022-01-09 12:42:07,034 iteration 2966 : loss : 0.027544, loss_ce: 0.009912
2022-01-09 12:42:08,393 iteration 2967 : loss : 0.048344, loss_ce: 0.016386
2022-01-09 12:42:09,771 iteration 2968 : loss : 0.043272, loss_ce: 0.017407
2022-01-09 12:42:11,123 iteration 2969 : loss : 0.034430, loss_ce: 0.016367
2022-01-09 12:42:12,502 iteration 2970 : loss : 0.040107, loss_ce: 0.014381
2022-01-09 12:42:13,857 iteration 2971 : loss : 0.030558, loss_ce: 0.011924
2022-01-09 12:42:15,222 iteration 2972 : loss : 0.037634, loss_ce: 0.016733
2022-01-09 12:42:16,540 iteration 2973 : loss : 0.033367, loss_ce: 0.007654
2022-01-09 12:42:17,962 iteration 2974 : loss : 0.065858, loss_ce: 0.021745
2022-01-09 12:42:17,962 Training Data Eval:
2022-01-09 12:42:24,828   Average segmentation loss on training set: 0.0226
2022-01-09 12:42:24,829 Validation Data Eval:
2022-01-09 12:42:27,215   Average segmentation loss on validation set: 0.0924
2022-01-09 12:42:28,561 iteration 2975 : loss : 0.026544, loss_ce: 0.010430
 44%|███████████▊               | 175/400 [1:15:06<1:41:28, 27.06s/it]2022-01-09 12:42:29,911 iteration 2976 : loss : 0.028364, loss_ce: 0.010994
2022-01-09 12:42:31,309 iteration 2977 : loss : 0.041190, loss_ce: 0.021625
2022-01-09 12:42:32,699 iteration 2978 : loss : 0.031340, loss_ce: 0.010057
2022-01-09 12:42:34,117 iteration 2979 : loss : 0.031027, loss_ce: 0.010814
2022-01-09 12:42:35,563 iteration 2980 : loss : 0.043696, loss_ce: 0.011672
2022-01-09 12:42:37,031 iteration 2981 : loss : 0.054197, loss_ce: 0.022752
2022-01-09 12:42:38,408 iteration 2982 : loss : 0.030387, loss_ce: 0.010441
2022-01-09 12:42:39,788 iteration 2983 : loss : 0.036996, loss_ce: 0.014690
2022-01-09 12:42:41,172 iteration 2984 : loss : 0.038323, loss_ce: 0.013392
2022-01-09 12:42:42,583 iteration 2985 : loss : 0.055630, loss_ce: 0.019786
2022-01-09 12:42:43,900 iteration 2986 : loss : 0.032666, loss_ce: 0.014044
2022-01-09 12:42:45,277 iteration 2987 : loss : 0.035688, loss_ce: 0.011436
2022-01-09 12:42:46,699 iteration 2988 : loss : 0.034891, loss_ce: 0.014090
2022-01-09 12:42:48,106 iteration 2989 : loss : 0.052141, loss_ce: 0.017174
2022-01-09 12:42:49,421 iteration 2990 : loss : 0.029374, loss_ce: 0.012188
2022-01-09 12:42:50,787 iteration 2991 : loss : 0.030360, loss_ce: 0.011529
2022-01-09 12:42:52,224 iteration 2992 : loss : 0.033279, loss_ce: 0.012638
 44%|███████████▉               | 176/400 [1:15:29<1:37:12, 26.04s/it]2022-01-09 12:42:53,596 iteration 2993 : loss : 0.035353, loss_ce: 0.016981
2022-01-09 12:42:54,979 iteration 2994 : loss : 0.051127, loss_ce: 0.011006
2022-01-09 12:42:56,331 iteration 2995 : loss : 0.026662, loss_ce: 0.008867
2022-01-09 12:42:57,695 iteration 2996 : loss : 0.029682, loss_ce: 0.014323
2022-01-09 12:42:59,057 iteration 2997 : loss : 0.041981, loss_ce: 0.013956
2022-01-09 12:43:00,437 iteration 2998 : loss : 0.044412, loss_ce: 0.019163
2022-01-09 12:43:01,861 iteration 2999 : loss : 0.083564, loss_ce: 0.017255
2022-01-09 12:43:03,310 iteration 3000 : loss : 0.059965, loss_ce: 0.024700
2022-01-09 12:43:04,746 iteration 3001 : loss : 0.062710, loss_ce: 0.016287
2022-01-09 12:43:06,066 iteration 3002 : loss : 0.028352, loss_ce: 0.010759
2022-01-09 12:43:07,411 iteration 3003 : loss : 0.032155, loss_ce: 0.012145
2022-01-09 12:43:08,763 iteration 3004 : loss : 0.030755, loss_ce: 0.012986
2022-01-09 12:43:10,137 iteration 3005 : loss : 0.038200, loss_ce: 0.016977
2022-01-09 12:43:11,543 iteration 3006 : loss : 0.027629, loss_ce: 0.011625
2022-01-09 12:43:12,940 iteration 3007 : loss : 0.047875, loss_ce: 0.022645
2022-01-09 12:43:14,344 iteration 3008 : loss : 0.055503, loss_ce: 0.022456
2022-01-09 12:43:15,650 iteration 3009 : loss : 0.034808, loss_ce: 0.015313
 44%|███████████▉               | 177/400 [1:15:53<1:33:51, 25.26s/it]2022-01-09 12:43:17,189 iteration 3010 : loss : 0.040983, loss_ce: 0.014149
2022-01-09 12:43:18,601 iteration 3011 : loss : 0.032686, loss_ce: 0.015165
2022-01-09 12:43:19,927 iteration 3012 : loss : 0.027009, loss_ce: 0.011817
2022-01-09 12:43:21,235 iteration 3013 : loss : 0.041956, loss_ce: 0.012637
2022-01-09 12:43:22,613 iteration 3014 : loss : 0.031424, loss_ce: 0.011467
2022-01-09 12:43:23,973 iteration 3015 : loss : 0.055657, loss_ce: 0.014551
2022-01-09 12:43:25,333 iteration 3016 : loss : 0.042128, loss_ce: 0.015740
2022-01-09 12:43:26,674 iteration 3017 : loss : 0.034881, loss_ce: 0.018611
2022-01-09 12:43:28,014 iteration 3018 : loss : 0.022955, loss_ce: 0.010076
2022-01-09 12:43:29,317 iteration 3019 : loss : 0.028123, loss_ce: 0.008826
2022-01-09 12:43:30,709 iteration 3020 : loss : 0.046093, loss_ce: 0.017935
2022-01-09 12:43:32,090 iteration 3021 : loss : 0.025086, loss_ce: 0.008896
2022-01-09 12:43:33,443 iteration 3022 : loss : 0.033120, loss_ce: 0.012908
2022-01-09 12:43:34,822 iteration 3023 : loss : 0.044052, loss_ce: 0.014566
2022-01-09 12:43:36,273 iteration 3024 : loss : 0.041028, loss_ce: 0.017874
2022-01-09 12:43:37,706 iteration 3025 : loss : 0.040230, loss_ce: 0.018249
2022-01-09 12:43:39,035 iteration 3026 : loss : 0.032497, loss_ce: 0.012224
 44%|████████████               | 178/400 [1:16:16<1:31:21, 24.69s/it]2022-01-09 12:43:40,513 iteration 3027 : loss : 0.063329, loss_ce: 0.021747
2022-01-09 12:43:41,843 iteration 3028 : loss : 0.033928, loss_ce: 0.013691
2022-01-09 12:43:43,231 iteration 3029 : loss : 0.036461, loss_ce: 0.017064
2022-01-09 12:43:44,572 iteration 3030 : loss : 0.029359, loss_ce: 0.013400
2022-01-09 12:43:45,944 iteration 3031 : loss : 0.032189, loss_ce: 0.012490
2022-01-09 12:43:47,279 iteration 3032 : loss : 0.046574, loss_ce: 0.017310
2022-01-09 12:43:48,682 iteration 3033 : loss : 0.049312, loss_ce: 0.015494
2022-01-09 12:43:50,104 iteration 3034 : loss : 0.035445, loss_ce: 0.013580
2022-01-09 12:43:51,494 iteration 3035 : loss : 0.027462, loss_ce: 0.011629
2022-01-09 12:43:52,848 iteration 3036 : loss : 0.040337, loss_ce: 0.021787
2022-01-09 12:43:54,217 iteration 3037 : loss : 0.032774, loss_ce: 0.012771
2022-01-09 12:43:55,621 iteration 3038 : loss : 0.031631, loss_ce: 0.010411
2022-01-09 12:43:56,937 iteration 3039 : loss : 0.039956, loss_ce: 0.011997
2022-01-09 12:43:58,320 iteration 3040 : loss : 0.039294, loss_ce: 0.011548
2022-01-09 12:43:59,737 iteration 3041 : loss : 0.043859, loss_ce: 0.018035
2022-01-09 12:44:01,109 iteration 3042 : loss : 0.044604, loss_ce: 0.020430
2022-01-09 12:44:02,652 iteration 3043 : loss : 0.053205, loss_ce: 0.019815
 45%|████████████               | 179/400 [1:16:40<1:29:46, 24.37s/it]2022-01-09 12:44:04,110 iteration 3044 : loss : 0.031565, loss_ce: 0.010962
2022-01-09 12:44:05,437 iteration 3045 : loss : 0.033342, loss_ce: 0.012801
2022-01-09 12:44:06,784 iteration 3046 : loss : 0.033025, loss_ce: 0.012002
2022-01-09 12:44:08,262 iteration 3047 : loss : 0.061547, loss_ce: 0.033114
2022-01-09 12:44:09,602 iteration 3048 : loss : 0.029965, loss_ce: 0.010354
2022-01-09 12:44:10,976 iteration 3049 : loss : 0.038502, loss_ce: 0.016127
2022-01-09 12:44:12,380 iteration 3050 : loss : 0.032655, loss_ce: 0.015512
2022-01-09 12:44:13,759 iteration 3051 : loss : 0.047839, loss_ce: 0.013242
2022-01-09 12:44:15,126 iteration 3052 : loss : 0.039293, loss_ce: 0.012892
2022-01-09 12:44:16,579 iteration 3053 : loss : 0.052283, loss_ce: 0.027536
2022-01-09 12:44:17,912 iteration 3054 : loss : 0.033009, loss_ce: 0.009118
2022-01-09 12:44:19,312 iteration 3055 : loss : 0.048206, loss_ce: 0.018331
2022-01-09 12:44:20,663 iteration 3056 : loss : 0.027964, loss_ce: 0.012164
2022-01-09 12:44:21,980 iteration 3057 : loss : 0.035153, loss_ce: 0.015270
2022-01-09 12:44:23,368 iteration 3058 : loss : 0.033857, loss_ce: 0.014156
2022-01-09 12:44:24,778 iteration 3059 : loss : 0.036575, loss_ce: 0.009987
2022-01-09 12:44:24,778 Training Data Eval:
2022-01-09 12:44:31,657   Average segmentation loss on training set: 0.0493
2022-01-09 12:44:31,657 Validation Data Eval:
2022-01-09 12:44:34,048   Average segmentation loss on validation set: 0.1785
2022-01-09 12:44:35,386 iteration 3060 : loss : 0.027942, loss_ce: 0.012828
 45%|████████████▏              | 180/400 [1:17:13<1:38:33, 26.88s/it]2022-01-09 12:44:36,742 iteration 3061 : loss : 0.033121, loss_ce: 0.012582
2022-01-09 12:44:38,112 iteration 3062 : loss : 0.043294, loss_ce: 0.016769
2022-01-09 12:44:39,516 iteration 3063 : loss : 0.057071, loss_ce: 0.019999
2022-01-09 12:44:40,929 iteration 3064 : loss : 0.040684, loss_ce: 0.019262
2022-01-09 12:44:42,206 iteration 3065 : loss : 0.027287, loss_ce: 0.009167
2022-01-09 12:44:43,545 iteration 3066 : loss : 0.043147, loss_ce: 0.021086
2022-01-09 12:44:44,938 iteration 3067 : loss : 0.045125, loss_ce: 0.021996
2022-01-09 12:44:46,321 iteration 3068 : loss : 0.056630, loss_ce: 0.024649
2022-01-09 12:44:47,691 iteration 3069 : loss : 0.047122, loss_ce: 0.016996
2022-01-09 12:44:49,073 iteration 3070 : loss : 0.029211, loss_ce: 0.011316
2022-01-09 12:44:50,392 iteration 3071 : loss : 0.026519, loss_ce: 0.010748
2022-01-09 12:44:51,712 iteration 3072 : loss : 0.025836, loss_ce: 0.010701
2022-01-09 12:44:53,066 iteration 3073 : loss : 0.030807, loss_ce: 0.011353
2022-01-09 12:44:54,503 iteration 3074 : loss : 0.060776, loss_ce: 0.021877
2022-01-09 12:44:55,905 iteration 3075 : loss : 0.046441, loss_ce: 0.015750
2022-01-09 12:44:57,345 iteration 3076 : loss : 0.048910, loss_ce: 0.017128
2022-01-09 12:44:58,674 iteration 3077 : loss : 0.053051, loss_ce: 0.020988
 45%|████████████▏              | 181/400 [1:17:36<1:34:10, 25.80s/it]2022-01-09 12:45:00,088 iteration 3078 : loss : 0.032989, loss_ce: 0.013400
2022-01-09 12:45:01,565 iteration 3079 : loss : 0.030327, loss_ce: 0.012628
2022-01-09 12:45:02,865 iteration 3080 : loss : 0.039155, loss_ce: 0.013087
2022-01-09 12:45:04,262 iteration 3081 : loss : 0.030137, loss_ce: 0.010911
2022-01-09 12:45:05,614 iteration 3082 : loss : 0.044881, loss_ce: 0.011686
2022-01-09 12:45:07,035 iteration 3083 : loss : 0.034983, loss_ce: 0.014560
2022-01-09 12:45:08,295 iteration 3084 : loss : 0.032534, loss_ce: 0.013266
2022-01-09 12:45:09,686 iteration 3085 : loss : 0.033585, loss_ce: 0.014814
2022-01-09 12:45:11,146 iteration 3086 : loss : 0.031789, loss_ce: 0.016463
2022-01-09 12:45:12,497 iteration 3087 : loss : 0.041533, loss_ce: 0.016018
2022-01-09 12:45:13,879 iteration 3088 : loss : 0.026672, loss_ce: 0.009045
2022-01-09 12:45:15,342 iteration 3089 : loss : 0.048586, loss_ce: 0.018060
2022-01-09 12:45:16,638 iteration 3090 : loss : 0.033817, loss_ce: 0.014314
2022-01-09 12:45:18,010 iteration 3091 : loss : 0.032478, loss_ce: 0.016648
2022-01-09 12:45:19,516 iteration 3092 : loss : 0.044160, loss_ce: 0.015770
2022-01-09 12:45:20,855 iteration 3093 : loss : 0.038382, loss_ce: 0.015354
2022-01-09 12:45:22,236 iteration 3094 : loss : 0.036460, loss_ce: 0.014279
 46%|████████████▎              | 182/400 [1:17:59<1:31:18, 25.13s/it]2022-01-09 12:45:23,659 iteration 3095 : loss : 0.028393, loss_ce: 0.012038
2022-01-09 12:45:25,047 iteration 3096 : loss : 0.030877, loss_ce: 0.013832
2022-01-09 12:45:26,403 iteration 3097 : loss : 0.031906, loss_ce: 0.010616
2022-01-09 12:45:27,732 iteration 3098 : loss : 0.027424, loss_ce: 0.010312
2022-01-09 12:45:29,098 iteration 3099 : loss : 0.029124, loss_ce: 0.010130
2022-01-09 12:45:30,419 iteration 3100 : loss : 0.039710, loss_ce: 0.015575
2022-01-09 12:45:31,834 iteration 3101 : loss : 0.027582, loss_ce: 0.010231
2022-01-09 12:45:33,205 iteration 3102 : loss : 0.034047, loss_ce: 0.012538
2022-01-09 12:45:34,587 iteration 3103 : loss : 0.044656, loss_ce: 0.017685
2022-01-09 12:45:35,916 iteration 3104 : loss : 0.027323, loss_ce: 0.011869
2022-01-09 12:45:37,222 iteration 3105 : loss : 0.028810, loss_ce: 0.013805
2022-01-09 12:45:38,572 iteration 3106 : loss : 0.030217, loss_ce: 0.013308
2022-01-09 12:45:39,918 iteration 3107 : loss : 0.028206, loss_ce: 0.007760
2022-01-09 12:45:41,302 iteration 3108 : loss : 0.047480, loss_ce: 0.015975
2022-01-09 12:45:42,705 iteration 3109 : loss : 0.035447, loss_ce: 0.013074
2022-01-09 12:45:44,113 iteration 3110 : loss : 0.039365, loss_ce: 0.019971
2022-01-09 12:45:45,496 iteration 3111 : loss : 0.027628, loss_ce: 0.012379
 46%|████████████▎              | 183/400 [1:18:23<1:28:51, 24.57s/it]2022-01-09 12:45:46,943 iteration 3112 : loss : 0.032081, loss_ce: 0.012187
2022-01-09 12:45:48,275 iteration 3113 : loss : 0.037180, loss_ce: 0.016927
2022-01-09 12:45:49,685 iteration 3114 : loss : 0.038387, loss_ce: 0.012708
2022-01-09 12:45:51,064 iteration 3115 : loss : 0.029949, loss_ce: 0.012822
2022-01-09 12:45:52,428 iteration 3116 : loss : 0.038765, loss_ce: 0.016667
2022-01-09 12:45:53,861 iteration 3117 : loss : 0.032617, loss_ce: 0.014613
2022-01-09 12:45:55,211 iteration 3118 : loss : 0.068004, loss_ce: 0.029246
2022-01-09 12:45:56,675 iteration 3119 : loss : 0.044465, loss_ce: 0.014597
2022-01-09 12:45:57,996 iteration 3120 : loss : 0.031637, loss_ce: 0.013091
2022-01-09 12:45:59,432 iteration 3121 : loss : 0.026105, loss_ce: 0.011247
2022-01-09 12:46:00,781 iteration 3122 : loss : 0.035038, loss_ce: 0.010262
2022-01-09 12:46:02,204 iteration 3123 : loss : 0.037333, loss_ce: 0.016395
2022-01-09 12:46:03,505 iteration 3124 : loss : 0.026804, loss_ce: 0.010230
2022-01-09 12:46:04,867 iteration 3125 : loss : 0.041280, loss_ce: 0.013340
2022-01-09 12:46:06,292 iteration 3126 : loss : 0.030341, loss_ce: 0.014754
2022-01-09 12:46:07,727 iteration 3127 : loss : 0.029131, loss_ce: 0.009764
2022-01-09 12:46:09,161 iteration 3128 : loss : 0.030656, loss_ce: 0.010921
 46%|████████████▍              | 184/400 [1:18:46<1:27:28, 24.30s/it]2022-01-09 12:46:10,566 iteration 3129 : loss : 0.041084, loss_ce: 0.017905
2022-01-09 12:46:12,030 iteration 3130 : loss : 0.055581, loss_ce: 0.025730
2022-01-09 12:46:13,389 iteration 3131 : loss : 0.039556, loss_ce: 0.013828
2022-01-09 12:46:14,758 iteration 3132 : loss : 0.028088, loss_ce: 0.008556
2022-01-09 12:46:16,132 iteration 3133 : loss : 0.035245, loss_ce: 0.012382
2022-01-09 12:46:17,472 iteration 3134 : loss : 0.026548, loss_ce: 0.011792
2022-01-09 12:46:18,841 iteration 3135 : loss : 0.038685, loss_ce: 0.015083
2022-01-09 12:46:20,188 iteration 3136 : loss : 0.033425, loss_ce: 0.013308
2022-01-09 12:46:21,670 iteration 3137 : loss : 0.028631, loss_ce: 0.012451
2022-01-09 12:46:23,102 iteration 3138 : loss : 0.075368, loss_ce: 0.035792
2022-01-09 12:46:24,416 iteration 3139 : loss : 0.029583, loss_ce: 0.011226
2022-01-09 12:46:25,736 iteration 3140 : loss : 0.032286, loss_ce: 0.014115
2022-01-09 12:46:27,117 iteration 3141 : loss : 0.043129, loss_ce: 0.019308
2022-01-09 12:46:28,466 iteration 3142 : loss : 0.045320, loss_ce: 0.016238
2022-01-09 12:46:29,880 iteration 3143 : loss : 0.052064, loss_ce: 0.021637
2022-01-09 12:46:31,246 iteration 3144 : loss : 0.032261, loss_ce: 0.011846
2022-01-09 12:46:31,246 Training Data Eval:
2022-01-09 12:46:38,107   Average segmentation loss on training set: 0.0215
2022-01-09 12:46:38,107 Validation Data Eval:
2022-01-09 12:46:40,478   Average segmentation loss on validation set: 0.1062
2022-01-09 12:46:41,807 iteration 3145 : loss : 0.030711, loss_ce: 0.015138
 46%|████████████▍              | 185/400 [1:19:19<1:36:02, 26.80s/it]2022-01-09 12:46:43,369 iteration 3146 : loss : 0.069146, loss_ce: 0.024632
2022-01-09 12:46:44,776 iteration 3147 : loss : 0.033001, loss_ce: 0.013681
2022-01-09 12:46:46,090 iteration 3148 : loss : 0.025085, loss_ce: 0.009340
2022-01-09 12:46:47,500 iteration 3149 : loss : 0.030081, loss_ce: 0.010396
2022-01-09 12:46:48,910 iteration 3150 : loss : 0.033270, loss_ce: 0.011192
2022-01-09 12:46:50,314 iteration 3151 : loss : 0.042057, loss_ce: 0.018845
2022-01-09 12:46:51,716 iteration 3152 : loss : 0.037616, loss_ce: 0.013578
2022-01-09 12:46:53,098 iteration 3153 : loss : 0.037666, loss_ce: 0.019689
2022-01-09 12:46:54,549 iteration 3154 : loss : 0.033902, loss_ce: 0.012695
2022-01-09 12:46:55,962 iteration 3155 : loss : 0.028969, loss_ce: 0.009713
2022-01-09 12:46:57,279 iteration 3156 : loss : 0.030334, loss_ce: 0.010885
2022-01-09 12:46:58,668 iteration 3157 : loss : 0.029308, loss_ce: 0.012924
2022-01-09 12:46:59,998 iteration 3158 : loss : 0.052849, loss_ce: 0.011804
2022-01-09 12:47:01,394 iteration 3159 : loss : 0.037496, loss_ce: 0.018226
2022-01-09 12:47:02,794 iteration 3160 : loss : 0.033256, loss_ce: 0.013112
2022-01-09 12:47:04,240 iteration 3161 : loss : 0.028938, loss_ce: 0.010903
2022-01-09 12:47:05,622 iteration 3162 : loss : 0.035924, loss_ce: 0.012514
 46%|████████████▌              | 186/400 [1:19:43<1:32:24, 25.91s/it]2022-01-09 12:47:07,008 iteration 3163 : loss : 0.036040, loss_ce: 0.012025
2022-01-09 12:47:08,480 iteration 3164 : loss : 0.041042, loss_ce: 0.018188
2022-01-09 12:47:09,901 iteration 3165 : loss : 0.031742, loss_ce: 0.013403
2022-01-09 12:47:11,239 iteration 3166 : loss : 0.031448, loss_ce: 0.013256
2022-01-09 12:47:12,663 iteration 3167 : loss : 0.028957, loss_ce: 0.010064
2022-01-09 12:47:14,039 iteration 3168 : loss : 0.032769, loss_ce: 0.013510
2022-01-09 12:47:15,444 iteration 3169 : loss : 0.032714, loss_ce: 0.014000
2022-01-09 12:47:16,821 iteration 3170 : loss : 0.032219, loss_ce: 0.014014
2022-01-09 12:47:18,084 iteration 3171 : loss : 0.034036, loss_ce: 0.009748
2022-01-09 12:47:19,420 iteration 3172 : loss : 0.025260, loss_ce: 0.009653
2022-01-09 12:47:20,793 iteration 3173 : loss : 0.038333, loss_ce: 0.013262
2022-01-09 12:47:22,140 iteration 3174 : loss : 0.029151, loss_ce: 0.012575
2022-01-09 12:47:23,451 iteration 3175 : loss : 0.047412, loss_ce: 0.016220
2022-01-09 12:47:24,897 iteration 3176 : loss : 0.058367, loss_ce: 0.018483
2022-01-09 12:47:26,274 iteration 3177 : loss : 0.039260, loss_ce: 0.016252
2022-01-09 12:47:27,736 iteration 3178 : loss : 0.044638, loss_ce: 0.012520
2022-01-09 12:47:29,123 iteration 3179 : loss : 0.029859, loss_ce: 0.014326
 47%|████████████▌              | 187/400 [1:20:06<1:29:24, 25.18s/it]2022-01-09 12:47:30,563 iteration 3180 : loss : 0.065886, loss_ce: 0.024978
2022-01-09 12:47:31,912 iteration 3181 : loss : 0.031717, loss_ce: 0.014816
2022-01-09 12:47:33,237 iteration 3182 : loss : 0.040678, loss_ce: 0.020766
2022-01-09 12:47:34,668 iteration 3183 : loss : 0.047490, loss_ce: 0.018419
2022-01-09 12:47:36,048 iteration 3184 : loss : 0.031718, loss_ce: 0.010589
2022-01-09 12:47:37,435 iteration 3185 : loss : 0.049773, loss_ce: 0.015854
2022-01-09 12:47:38,851 iteration 3186 : loss : 0.033760, loss_ce: 0.013029
2022-01-09 12:47:40,269 iteration 3187 : loss : 0.025432, loss_ce: 0.010338
2022-01-09 12:47:41,699 iteration 3188 : loss : 0.035234, loss_ce: 0.013533
2022-01-09 12:47:43,072 iteration 3189 : loss : 0.027097, loss_ce: 0.010125
2022-01-09 12:47:44,546 iteration 3190 : loss : 0.042412, loss_ce: 0.015564
2022-01-09 12:47:45,971 iteration 3191 : loss : 0.038406, loss_ce: 0.015981
2022-01-09 12:47:47,282 iteration 3192 : loss : 0.042545, loss_ce: 0.009382
2022-01-09 12:47:48,654 iteration 3193 : loss : 0.025719, loss_ce: 0.009979
2022-01-09 12:47:50,146 iteration 3194 : loss : 0.058244, loss_ce: 0.035075
2022-01-09 12:47:51,534 iteration 3195 : loss : 0.031611, loss_ce: 0.012938
2022-01-09 12:47:52,913 iteration 3196 : loss : 0.031062, loss_ce: 0.015154
 47%|████████████▋              | 188/400 [1:20:30<1:27:30, 24.77s/it]2022-01-09 12:47:54,335 iteration 3197 : loss : 0.030572, loss_ce: 0.013581
2022-01-09 12:47:55,751 iteration 3198 : loss : 0.030762, loss_ce: 0.013920
2022-01-09 12:47:57,154 iteration 3199 : loss : 0.042284, loss_ce: 0.014215
2022-01-09 12:47:58,579 iteration 3200 : loss : 0.033513, loss_ce: 0.010941
2022-01-09 12:47:59,997 iteration 3201 : loss : 0.041034, loss_ce: 0.017710
2022-01-09 12:48:01,419 iteration 3202 : loss : 0.036994, loss_ce: 0.010343
2022-01-09 12:48:02,728 iteration 3203 : loss : 0.031121, loss_ce: 0.014229
2022-01-09 12:48:04,021 iteration 3204 : loss : 0.026998, loss_ce: 0.011217
2022-01-09 12:48:05,316 iteration 3205 : loss : 0.033576, loss_ce: 0.012253
2022-01-09 12:48:06,574 iteration 3206 : loss : 0.026574, loss_ce: 0.008629
2022-01-09 12:48:07,975 iteration 3207 : loss : 0.060879, loss_ce: 0.024060
2022-01-09 12:48:09,340 iteration 3208 : loss : 0.032517, loss_ce: 0.012640
2022-01-09 12:48:10,685 iteration 3209 : loss : 0.029029, loss_ce: 0.011344
2022-01-09 12:48:12,080 iteration 3210 : loss : 0.026396, loss_ce: 0.011857
2022-01-09 12:48:13,397 iteration 3211 : loss : 0.029443, loss_ce: 0.009036
2022-01-09 12:48:14,838 iteration 3212 : loss : 0.052597, loss_ce: 0.028465
2022-01-09 12:48:16,295 iteration 3213 : loss : 0.031502, loss_ce: 0.010967
 47%|████████████▊              | 189/400 [1:20:53<1:25:37, 24.35s/it]2022-01-09 12:48:17,676 iteration 3214 : loss : 0.022494, loss_ce: 0.009917
2022-01-09 12:48:19,048 iteration 3215 : loss : 0.029381, loss_ce: 0.013189
2022-01-09 12:48:20,418 iteration 3216 : loss : 0.045173, loss_ce: 0.016379
2022-01-09 12:48:21,817 iteration 3217 : loss : 0.032594, loss_ce: 0.014401
2022-01-09 12:48:23,196 iteration 3218 : loss : 0.042273, loss_ce: 0.014340
2022-01-09 12:48:24,559 iteration 3219 : loss : 0.049054, loss_ce: 0.021872
2022-01-09 12:48:25,878 iteration 3220 : loss : 0.020840, loss_ce: 0.008903
2022-01-09 12:48:27,340 iteration 3221 : loss : 0.050986, loss_ce: 0.021142
2022-01-09 12:48:28,686 iteration 3222 : loss : 0.022796, loss_ce: 0.006333
2022-01-09 12:48:30,081 iteration 3223 : loss : 0.032045, loss_ce: 0.010941
2022-01-09 12:48:31,538 iteration 3224 : loss : 0.053274, loss_ce: 0.016548
2022-01-09 12:48:32,915 iteration 3225 : loss : 0.051817, loss_ce: 0.026892
2022-01-09 12:48:34,286 iteration 3226 : loss : 0.035768, loss_ce: 0.014095
2022-01-09 12:48:35,673 iteration 3227 : loss : 0.031523, loss_ce: 0.012038
2022-01-09 12:48:37,021 iteration 3228 : loss : 0.026009, loss_ce: 0.008421
2022-01-09 12:48:38,340 iteration 3229 : loss : 0.031867, loss_ce: 0.009657
2022-01-09 12:48:38,341 Training Data Eval:
2022-01-09 12:48:45,209   Average segmentation loss on training set: 0.0221
2022-01-09 12:48:45,210 Validation Data Eval:
2022-01-09 12:48:47,581   Average segmentation loss on validation set: 0.0807
2022-01-09 12:48:53,328 Found new lowest validation loss at iteration 3229! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 12:48:54,714 iteration 3230 : loss : 0.034329, loss_ce: 0.012264
 48%|████████████▊              | 190/400 [1:21:32<1:40:00, 28.57s/it]2022-01-09 12:48:56,095 iteration 3231 : loss : 0.044345, loss_ce: 0.010358
2022-01-09 12:48:57,443 iteration 3232 : loss : 0.030549, loss_ce: 0.009715
2022-01-09 12:48:58,813 iteration 3233 : loss : 0.033603, loss_ce: 0.014890
2022-01-09 12:49:00,205 iteration 3234 : loss : 0.026341, loss_ce: 0.008692
2022-01-09 12:49:01,593 iteration 3235 : loss : 0.052781, loss_ce: 0.017341
2022-01-09 12:49:02,906 iteration 3236 : loss : 0.023488, loss_ce: 0.010632
2022-01-09 12:49:04,372 iteration 3237 : loss : 0.033580, loss_ce: 0.016547
2022-01-09 12:49:05,753 iteration 3238 : loss : 0.033707, loss_ce: 0.016304
2022-01-09 12:49:07,129 iteration 3239 : loss : 0.025870, loss_ce: 0.009915
2022-01-09 12:49:08,454 iteration 3240 : loss : 0.050047, loss_ce: 0.012926
2022-01-09 12:49:09,756 iteration 3241 : loss : 0.039512, loss_ce: 0.010097
2022-01-09 12:49:11,137 iteration 3242 : loss : 0.041156, loss_ce: 0.014164
2022-01-09 12:49:12,446 iteration 3243 : loss : 0.064420, loss_ce: 0.019109
2022-01-09 12:49:13,844 iteration 3244 : loss : 0.037913, loss_ce: 0.014255
2022-01-09 12:49:15,175 iteration 3245 : loss : 0.033928, loss_ce: 0.015413
2022-01-09 12:49:16,601 iteration 3246 : loss : 0.046888, loss_ce: 0.022663
2022-01-09 12:49:17,941 iteration 3247 : loss : 0.039652, loss_ce: 0.013893
 48%|████████████▉              | 191/400 [1:21:55<1:33:56, 26.97s/it]2022-01-09 12:49:19,323 iteration 3248 : loss : 0.024442, loss_ce: 0.009096
2022-01-09 12:49:20,647 iteration 3249 : loss : 0.038926, loss_ce: 0.013599
2022-01-09 12:49:21,998 iteration 3250 : loss : 0.053007, loss_ce: 0.018435
2022-01-09 12:49:23,353 iteration 3251 : loss : 0.032565, loss_ce: 0.014014
2022-01-09 12:49:24,733 iteration 3252 : loss : 0.026804, loss_ce: 0.008121
2022-01-09 12:49:26,082 iteration 3253 : loss : 0.028593, loss_ce: 0.013865
2022-01-09 12:49:27,428 iteration 3254 : loss : 0.041291, loss_ce: 0.014746
2022-01-09 12:49:28,720 iteration 3255 : loss : 0.034442, loss_ce: 0.013765
2022-01-09 12:49:30,066 iteration 3256 : loss : 0.025352, loss_ce: 0.009026
2022-01-09 12:49:31,390 iteration 3257 : loss : 0.029040, loss_ce: 0.011792
2022-01-09 12:49:32,763 iteration 3258 : loss : 0.031868, loss_ce: 0.013298
2022-01-09 12:49:34,074 iteration 3259 : loss : 0.041109, loss_ce: 0.020835
2022-01-09 12:49:35,454 iteration 3260 : loss : 0.035646, loss_ce: 0.015268
2022-01-09 12:49:36,791 iteration 3261 : loss : 0.040695, loss_ce: 0.011566
2022-01-09 12:49:38,194 iteration 3262 : loss : 0.047139, loss_ce: 0.018000
2022-01-09 12:49:39,668 iteration 3263 : loss : 0.056010, loss_ce: 0.014295
2022-01-09 12:49:41,049 iteration 3264 : loss : 0.038389, loss_ce: 0.015759
 48%|████████████▉              | 192/400 [1:22:18<1:29:28, 25.81s/it]2022-01-09 12:49:42,421 iteration 3265 : loss : 0.027538, loss_ce: 0.011514
2022-01-09 12:49:43,840 iteration 3266 : loss : 0.038163, loss_ce: 0.016641
2022-01-09 12:49:45,235 iteration 3267 : loss : 0.027643, loss_ce: 0.010799
2022-01-09 12:49:46,564 iteration 3268 : loss : 0.028366, loss_ce: 0.010473
2022-01-09 12:49:47,984 iteration 3269 : loss : 0.056414, loss_ce: 0.031447
2022-01-09 12:49:49,436 iteration 3270 : loss : 0.049698, loss_ce: 0.016746
2022-01-09 12:49:50,859 iteration 3271 : loss : 0.038869, loss_ce: 0.011994
2022-01-09 12:49:52,288 iteration 3272 : loss : 0.032314, loss_ce: 0.012208
2022-01-09 12:49:53,606 iteration 3273 : loss : 0.043937, loss_ce: 0.013540
2022-01-09 12:49:54,967 iteration 3274 : loss : 0.033238, loss_ce: 0.018175
2022-01-09 12:49:56,398 iteration 3275 : loss : 0.042276, loss_ce: 0.011426
2022-01-09 12:49:57,747 iteration 3276 : loss : 0.045147, loss_ce: 0.018777
2022-01-09 12:49:59,144 iteration 3277 : loss : 0.029161, loss_ce: 0.014295
2022-01-09 12:50:00,521 iteration 3278 : loss : 0.049304, loss_ce: 0.020850
2022-01-09 12:50:01,857 iteration 3279 : loss : 0.028416, loss_ce: 0.011325
2022-01-09 12:50:03,215 iteration 3280 : loss : 0.044316, loss_ce: 0.013792
2022-01-09 12:50:04,531 iteration 3281 : loss : 0.024927, loss_ce: 0.007874
 48%|█████████████              | 193/400 [1:22:42<1:26:37, 25.11s/it]2022-01-09 12:50:05,990 iteration 3282 : loss : 0.044738, loss_ce: 0.016599
2022-01-09 12:50:07,359 iteration 3283 : loss : 0.031741, loss_ce: 0.010579
2022-01-09 12:50:08,728 iteration 3284 : loss : 0.039144, loss_ce: 0.016178
2022-01-09 12:50:10,083 iteration 3285 : loss : 0.037388, loss_ce: 0.013609
2022-01-09 12:50:11,443 iteration 3286 : loss : 0.021579, loss_ce: 0.007913
2022-01-09 12:50:12,820 iteration 3287 : loss : 0.037982, loss_ce: 0.012793
2022-01-09 12:50:14,137 iteration 3288 : loss : 0.040053, loss_ce: 0.023237
2022-01-09 12:50:15,461 iteration 3289 : loss : 0.045714, loss_ce: 0.014545
2022-01-09 12:50:16,879 iteration 3290 : loss : 0.045350, loss_ce: 0.011739
2022-01-09 12:50:18,306 iteration 3291 : loss : 0.036923, loss_ce: 0.012862
2022-01-09 12:50:19,690 iteration 3292 : loss : 0.034371, loss_ce: 0.009591
2022-01-09 12:50:21,057 iteration 3293 : loss : 0.032584, loss_ce: 0.013729
2022-01-09 12:50:22,397 iteration 3294 : loss : 0.028182, loss_ce: 0.011724
2022-01-09 12:50:23,849 iteration 3295 : loss : 0.033142, loss_ce: 0.013210
2022-01-09 12:50:25,241 iteration 3296 : loss : 0.049513, loss_ce: 0.015261
2022-01-09 12:50:26,547 iteration 3297 : loss : 0.026848, loss_ce: 0.011236
2022-01-09 12:50:27,954 iteration 3298 : loss : 0.037460, loss_ce: 0.017832
 48%|█████████████              | 194/400 [1:23:05<1:24:28, 24.60s/it]2022-01-09 12:50:29,309 iteration 3299 : loss : 0.032630, loss_ce: 0.014432
2022-01-09 12:50:30,645 iteration 3300 : loss : 0.040544, loss_ce: 0.014137
2022-01-09 12:50:32,064 iteration 3301 : loss : 0.026425, loss_ce: 0.011358
2022-01-09 12:50:33,432 iteration 3302 : loss : 0.029704, loss_ce: 0.011624
2022-01-09 12:50:34,783 iteration 3303 : loss : 0.028939, loss_ce: 0.013383
2022-01-09 12:50:36,157 iteration 3304 : loss : 0.028257, loss_ce: 0.011024
2022-01-09 12:50:37,550 iteration 3305 : loss : 0.028279, loss_ce: 0.009352
2022-01-09 12:50:38,967 iteration 3306 : loss : 0.026607, loss_ce: 0.010981
2022-01-09 12:50:40,312 iteration 3307 : loss : 0.030722, loss_ce: 0.012102
2022-01-09 12:50:41,689 iteration 3308 : loss : 0.038610, loss_ce: 0.017935
2022-01-09 12:50:43,054 iteration 3309 : loss : 0.050884, loss_ce: 0.011350
2022-01-09 12:50:44,488 iteration 3310 : loss : 0.074250, loss_ce: 0.030595
2022-01-09 12:50:45,952 iteration 3311 : loss : 0.032764, loss_ce: 0.012811
2022-01-09 12:50:47,277 iteration 3312 : loss : 0.035319, loss_ce: 0.011914
2022-01-09 12:50:48,610 iteration 3313 : loss : 0.027885, loss_ce: 0.010081
2022-01-09 12:50:50,047 iteration 3314 : loss : 0.035130, loss_ce: 0.011239
2022-01-09 12:50:50,047 Training Data Eval:
2022-01-09 12:50:56,918   Average segmentation loss on training set: 0.0245
2022-01-09 12:50:56,919 Validation Data Eval:
2022-01-09 12:50:59,295   Average segmentation loss on validation set: 0.1006
2022-01-09 12:51:00,620 iteration 3315 : loss : 0.026045, loss_ce: 0.012225
 49%|█████████████▏             | 195/400 [1:23:38<1:32:19, 27.02s/it]2022-01-09 12:51:02,002 iteration 3316 : loss : 0.023450, loss_ce: 0.009118
2022-01-09 12:51:03,345 iteration 3317 : loss : 0.036013, loss_ce: 0.015207
2022-01-09 12:51:04,807 iteration 3318 : loss : 0.030210, loss_ce: 0.012187
2022-01-09 12:51:06,152 iteration 3319 : loss : 0.033958, loss_ce: 0.011894
2022-01-09 12:51:07,504 iteration 3320 : loss : 0.024765, loss_ce: 0.008968
2022-01-09 12:51:08,881 iteration 3321 : loss : 0.022653, loss_ce: 0.007217
2022-01-09 12:51:10,256 iteration 3322 : loss : 0.054905, loss_ce: 0.025205
2022-01-09 12:51:11,576 iteration 3323 : loss : 0.028974, loss_ce: 0.009094
2022-01-09 12:51:13,018 iteration 3324 : loss : 0.047108, loss_ce: 0.022855
2022-01-09 12:51:14,347 iteration 3325 : loss : 0.044041, loss_ce: 0.012995
2022-01-09 12:51:15,748 iteration 3326 : loss : 0.035326, loss_ce: 0.015298
2022-01-09 12:51:17,212 iteration 3327 : loss : 0.048496, loss_ce: 0.014747
2022-01-09 12:51:18,584 iteration 3328 : loss : 0.034061, loss_ce: 0.012629
2022-01-09 12:51:19,923 iteration 3329 : loss : 0.031194, loss_ce: 0.012330
2022-01-09 12:51:21,278 iteration 3330 : loss : 0.040130, loss_ce: 0.014350
2022-01-09 12:51:22,663 iteration 3331 : loss : 0.023571, loss_ce: 0.009172
2022-01-09 12:51:23,952 iteration 3332 : loss : 0.022663, loss_ce: 0.010503
 49%|█████████████▏             | 196/400 [1:24:01<1:28:07, 25.92s/it]2022-01-09 12:51:25,357 iteration 3333 : loss : 0.027959, loss_ce: 0.011508
2022-01-09 12:51:26,738 iteration 3334 : loss : 0.025876, loss_ce: 0.011487
2022-01-09 12:51:28,034 iteration 3335 : loss : 0.023557, loss_ce: 0.010816
2022-01-09 12:51:29,403 iteration 3336 : loss : 0.047221, loss_ce: 0.019482
2022-01-09 12:51:30,818 iteration 3337 : loss : 0.035726, loss_ce: 0.013896
2022-01-09 12:51:32,274 iteration 3338 : loss : 0.036684, loss_ce: 0.018784
2022-01-09 12:51:33,636 iteration 3339 : loss : 0.037994, loss_ce: 0.012147
2022-01-09 12:51:34,976 iteration 3340 : loss : 0.024492, loss_ce: 0.008480
2022-01-09 12:51:36,336 iteration 3341 : loss : 0.023440, loss_ce: 0.008814
2022-01-09 12:51:37,748 iteration 3342 : loss : 0.063945, loss_ce: 0.031700
2022-01-09 12:51:39,141 iteration 3343 : loss : 0.028259, loss_ce: 0.009544
2022-01-09 12:51:40,522 iteration 3344 : loss : 0.046934, loss_ce: 0.017132
2022-01-09 12:51:41,866 iteration 3345 : loss : 0.033594, loss_ce: 0.015110
2022-01-09 12:51:43,238 iteration 3346 : loss : 0.035765, loss_ce: 0.015795
2022-01-09 12:51:44,640 iteration 3347 : loss : 0.058975, loss_ce: 0.017789
2022-01-09 12:51:46,091 iteration 3348 : loss : 0.046677, loss_ce: 0.018596
2022-01-09 12:51:47,477 iteration 3349 : loss : 0.045810, loss_ce: 0.017445
 49%|█████████████▎             | 197/400 [1:24:25<1:25:15, 25.20s/it]2022-01-09 12:51:48,862 iteration 3350 : loss : 0.026856, loss_ce: 0.010468
2022-01-09 12:51:50,184 iteration 3351 : loss : 0.025858, loss_ce: 0.010470
2022-01-09 12:51:51,530 iteration 3352 : loss : 0.026460, loss_ce: 0.011167
2022-01-09 12:51:52,913 iteration 3353 : loss : 0.034724, loss_ce: 0.014534
2022-01-09 12:51:54,298 iteration 3354 : loss : 0.024283, loss_ce: 0.008463
2022-01-09 12:51:55,738 iteration 3355 : loss : 0.033060, loss_ce: 0.017322
2022-01-09 12:51:57,151 iteration 3356 : loss : 0.030895, loss_ce: 0.009009
2022-01-09 12:51:58,571 iteration 3357 : loss : 0.033604, loss_ce: 0.011002
2022-01-09 12:52:00,010 iteration 3358 : loss : 0.032668, loss_ce: 0.011569
2022-01-09 12:52:01,408 iteration 3359 : loss : 0.041349, loss_ce: 0.017155
2022-01-09 12:52:02,876 iteration 3360 : loss : 0.034281, loss_ce: 0.015654
2022-01-09 12:52:04,200 iteration 3361 : loss : 0.036865, loss_ce: 0.009084
2022-01-09 12:52:05,530 iteration 3362 : loss : 0.041789, loss_ce: 0.016555
2022-01-09 12:52:06,854 iteration 3363 : loss : 0.026770, loss_ce: 0.012799
2022-01-09 12:52:08,242 iteration 3364 : loss : 0.033676, loss_ce: 0.010802
2022-01-09 12:52:09,564 iteration 3365 : loss : 0.028372, loss_ce: 0.014574
2022-01-09 12:52:10,920 iteration 3366 : loss : 0.032829, loss_ce: 0.012595
 50%|█████████████▎             | 198/400 [1:24:48<1:23:03, 24.67s/it]2022-01-09 12:52:12,344 iteration 3367 : loss : 0.037913, loss_ce: 0.014977
2022-01-09 12:52:13,720 iteration 3368 : loss : 0.034455, loss_ce: 0.017321
2022-01-09 12:52:15,107 iteration 3369 : loss : 0.033388, loss_ce: 0.012218
2022-01-09 12:52:16,502 iteration 3370 : loss : 0.032783, loss_ce: 0.013086
2022-01-09 12:52:17,791 iteration 3371 : loss : 0.021971, loss_ce: 0.008725
2022-01-09 12:52:19,146 iteration 3372 : loss : 0.026594, loss_ce: 0.013977
2022-01-09 12:52:20,504 iteration 3373 : loss : 0.026999, loss_ce: 0.013500
2022-01-09 12:52:21,817 iteration 3374 : loss : 0.024205, loss_ce: 0.010205
2022-01-09 12:52:23,145 iteration 3375 : loss : 0.026807, loss_ce: 0.010731
2022-01-09 12:52:24,509 iteration 3376 : loss : 0.044767, loss_ce: 0.017172
2022-01-09 12:52:25,915 iteration 3377 : loss : 0.053864, loss_ce: 0.018804
2022-01-09 12:52:27,270 iteration 3378 : loss : 0.036719, loss_ce: 0.012749
2022-01-09 12:52:28,556 iteration 3379 : loss : 0.023380, loss_ce: 0.010679
2022-01-09 12:52:29,973 iteration 3380 : loss : 0.040435, loss_ce: 0.015518
2022-01-09 12:52:31,277 iteration 3381 : loss : 0.071138, loss_ce: 0.016329
2022-01-09 12:52:32,649 iteration 3382 : loss : 0.046246, loss_ce: 0.018251
2022-01-09 12:52:34,001 iteration 3383 : loss : 0.051948, loss_ce: 0.018540
 50%|█████████████▍             | 199/400 [1:25:11<1:21:03, 24.19s/it]2022-01-09 12:52:35,375 iteration 3384 : loss : 0.043923, loss_ce: 0.016363
2022-01-09 12:52:36,789 iteration 3385 : loss : 0.035995, loss_ce: 0.015687
2022-01-09 12:52:38,207 iteration 3386 : loss : 0.051020, loss_ce: 0.026303
2022-01-09 12:52:39,614 iteration 3387 : loss : 0.049427, loss_ce: 0.019828
2022-01-09 12:52:40,922 iteration 3388 : loss : 0.037028, loss_ce: 0.015833
2022-01-09 12:52:42,310 iteration 3389 : loss : 0.032785, loss_ce: 0.012094
2022-01-09 12:52:43,616 iteration 3390 : loss : 0.044192, loss_ce: 0.015746
2022-01-09 12:52:45,074 iteration 3391 : loss : 0.043981, loss_ce: 0.016368
2022-01-09 12:52:46,474 iteration 3392 : loss : 0.035062, loss_ce: 0.013965
2022-01-09 12:52:47,871 iteration 3393 : loss : 0.038270, loss_ce: 0.015375
2022-01-09 12:52:49,159 iteration 3394 : loss : 0.031403, loss_ce: 0.010609
2022-01-09 12:52:50,628 iteration 3395 : loss : 0.049122, loss_ce: 0.020626
2022-01-09 12:52:52,014 iteration 3396 : loss : 0.038898, loss_ce: 0.015167
2022-01-09 12:52:53,367 iteration 3397 : loss : 0.030577, loss_ce: 0.012433
2022-01-09 12:52:54,774 iteration 3398 : loss : 0.034010, loss_ce: 0.011321
2022-01-09 12:52:56,161 iteration 3399 : loss : 0.029897, loss_ce: 0.012782
2022-01-09 12:52:56,161 Training Data Eval:
2022-01-09 12:53:03,035   Average segmentation loss on training set: 0.0253
2022-01-09 12:53:03,036 Validation Data Eval:
2022-01-09 12:53:05,407   Average segmentation loss on validation set: 0.0989
2022-01-09 12:53:06,780 iteration 3400 : loss : 0.029042, loss_ce: 0.008754
 50%|█████████████▌             | 200/400 [1:25:44<1:29:14, 26.77s/it]2022-01-09 12:53:08,335 iteration 3401 : loss : 0.044166, loss_ce: 0.021752
2022-01-09 12:53:09,760 iteration 3402 : loss : 0.027576, loss_ce: 0.009693
2022-01-09 12:53:11,105 iteration 3403 : loss : 0.035467, loss_ce: 0.017029
2022-01-09 12:53:12,449 iteration 3404 : loss : 0.021673, loss_ce: 0.007673
2022-01-09 12:53:13,800 iteration 3405 : loss : 0.036385, loss_ce: 0.014490
2022-01-09 12:53:15,216 iteration 3406 : loss : 0.038229, loss_ce: 0.012953
2022-01-09 12:53:16,597 iteration 3407 : loss : 0.047843, loss_ce: 0.018928
2022-01-09 12:53:17,986 iteration 3408 : loss : 0.027884, loss_ce: 0.012270
2022-01-09 12:53:19,386 iteration 3409 : loss : 0.042139, loss_ce: 0.021972
2022-01-09 12:53:20,781 iteration 3410 : loss : 0.039865, loss_ce: 0.013905
2022-01-09 12:53:22,213 iteration 3411 : loss : 0.034894, loss_ce: 0.015499
2022-01-09 12:53:23,536 iteration 3412 : loss : 0.034378, loss_ce: 0.011906
2022-01-09 12:53:24,836 iteration 3413 : loss : 0.025632, loss_ce: 0.011710
2022-01-09 12:53:26,223 iteration 3414 : loss : 0.044879, loss_ce: 0.012597
2022-01-09 12:53:27,634 iteration 3415 : loss : 0.029080, loss_ce: 0.010442
2022-01-09 12:53:29,055 iteration 3416 : loss : 0.025582, loss_ce: 0.008521
2022-01-09 12:53:30,511 iteration 3417 : loss : 0.038922, loss_ce: 0.014480
 50%|█████████████▌             | 201/400 [1:26:08<1:25:45, 25.86s/it]2022-01-09 12:53:31,954 iteration 3418 : loss : 0.022441, loss_ce: 0.010440
2022-01-09 12:53:33,360 iteration 3419 : loss : 0.032247, loss_ce: 0.012309
2022-01-09 12:53:34,823 iteration 3420 : loss : 0.038672, loss_ce: 0.011556
2022-01-09 12:53:36,229 iteration 3421 : loss : 0.025882, loss_ce: 0.009569
2022-01-09 12:53:37,550 iteration 3422 : loss : 0.027589, loss_ce: 0.009037
2022-01-09 12:53:38,963 iteration 3423 : loss : 0.032994, loss_ce: 0.013798
2022-01-09 12:53:40,342 iteration 3424 : loss : 0.035874, loss_ce: 0.017529
2022-01-09 12:53:41,669 iteration 3425 : loss : 0.026029, loss_ce: 0.011496
2022-01-09 12:53:43,053 iteration 3426 : loss : 0.028341, loss_ce: 0.010927
2022-01-09 12:53:44,450 iteration 3427 : loss : 0.033461, loss_ce: 0.010579
2022-01-09 12:53:45,846 iteration 3428 : loss : 0.040110, loss_ce: 0.015899
2022-01-09 12:53:47,273 iteration 3429 : loss : 0.029900, loss_ce: 0.011854
2022-01-09 12:53:48,696 iteration 3430 : loss : 0.037714, loss_ce: 0.015026
2022-01-09 12:53:50,002 iteration 3431 : loss : 0.026033, loss_ce: 0.010858
2022-01-09 12:53:51,337 iteration 3432 : loss : 0.029889, loss_ce: 0.009757
2022-01-09 12:53:52,672 iteration 3433 : loss : 0.034073, loss_ce: 0.012750
2022-01-09 12:53:54,091 iteration 3434 : loss : 0.046106, loss_ce: 0.017577
 50%|█████████████▋             | 202/400 [1:26:31<1:23:04, 25.18s/it]2022-01-09 12:53:55,456 iteration 3435 : loss : 0.026575, loss_ce: 0.007833
2022-01-09 12:53:56,794 iteration 3436 : loss : 0.022678, loss_ce: 0.008706
2022-01-09 12:53:58,262 iteration 3437 : loss : 0.028217, loss_ce: 0.013032
2022-01-09 12:53:59,703 iteration 3438 : loss : 0.032819, loss_ce: 0.016685
2022-01-09 12:54:01,129 iteration 3439 : loss : 0.037340, loss_ce: 0.014691
2022-01-09 12:54:02,471 iteration 3440 : loss : 0.028391, loss_ce: 0.010322
2022-01-09 12:54:03,837 iteration 3441 : loss : 0.024205, loss_ce: 0.009440
2022-01-09 12:54:05,316 iteration 3442 : loss : 0.054031, loss_ce: 0.016429
2022-01-09 12:54:06,735 iteration 3443 : loss : 0.027355, loss_ce: 0.009129
2022-01-09 12:54:08,156 iteration 3444 : loss : 0.045043, loss_ce: 0.017421
2022-01-09 12:54:09,538 iteration 3445 : loss : 0.034606, loss_ce: 0.009344
2022-01-09 12:54:10,904 iteration 3446 : loss : 0.032958, loss_ce: 0.014582
2022-01-09 12:54:12,263 iteration 3447 : loss : 0.031620, loss_ce: 0.010691
2022-01-09 12:54:13,689 iteration 3448 : loss : 0.039325, loss_ce: 0.011297
2022-01-09 12:54:15,089 iteration 3449 : loss : 0.031670, loss_ce: 0.013068
2022-01-09 12:54:16,465 iteration 3450 : loss : 0.045137, loss_ce: 0.020918
2022-01-09 12:54:17,919 iteration 3451 : loss : 0.027300, loss_ce: 0.010439
 51%|█████████████▋             | 203/400 [1:26:55<1:21:19, 24.77s/it]2022-01-09 12:54:19,356 iteration 3452 : loss : 0.045593, loss_ce: 0.020161
2022-01-09 12:54:20,714 iteration 3453 : loss : 0.032528, loss_ce: 0.012450
2022-01-09 12:54:22,056 iteration 3454 : loss : 0.031184, loss_ce: 0.010070
2022-01-09 12:54:23,500 iteration 3455 : loss : 0.031981, loss_ce: 0.013392
2022-01-09 12:54:24,839 iteration 3456 : loss : 0.026490, loss_ce: 0.009492
2022-01-09 12:54:26,305 iteration 3457 : loss : 0.038380, loss_ce: 0.014439
2022-01-09 12:54:27,716 iteration 3458 : loss : 0.032727, loss_ce: 0.012934
2022-01-09 12:54:29,091 iteration 3459 : loss : 0.047698, loss_ce: 0.022548
2022-01-09 12:54:30,457 iteration 3460 : loss : 0.027370, loss_ce: 0.010872
2022-01-09 12:54:31,823 iteration 3461 : loss : 0.028959, loss_ce: 0.010446
2022-01-09 12:54:33,178 iteration 3462 : loss : 0.032563, loss_ce: 0.016893
2022-01-09 12:54:34,588 iteration 3463 : loss : 0.030879, loss_ce: 0.013978
2022-01-09 12:54:35,906 iteration 3464 : loss : 0.024287, loss_ce: 0.008743
2022-01-09 12:54:37,324 iteration 3465 : loss : 0.036494, loss_ce: 0.010647
2022-01-09 12:54:38,714 iteration 3466 : loss : 0.024636, loss_ce: 0.009103
2022-01-09 12:54:40,093 iteration 3467 : loss : 0.028670, loss_ce: 0.014467
2022-01-09 12:54:41,456 iteration 3468 : loss : 0.032563, loss_ce: 0.013484
 51%|█████████████▊             | 204/400 [1:27:19<1:19:42, 24.40s/it]2022-01-09 12:54:42,854 iteration 3469 : loss : 0.030243, loss_ce: 0.008635
2022-01-09 12:54:44,274 iteration 3470 : loss : 0.038655, loss_ce: 0.012133
2022-01-09 12:54:45,584 iteration 3471 : loss : 0.025364, loss_ce: 0.009799
2022-01-09 12:54:46,976 iteration 3472 : loss : 0.036538, loss_ce: 0.015343
2022-01-09 12:54:48,465 iteration 3473 : loss : 0.037853, loss_ce: 0.015220
2022-01-09 12:54:49,841 iteration 3474 : loss : 0.028497, loss_ce: 0.008266
2022-01-09 12:54:51,284 iteration 3475 : loss : 0.051571, loss_ce: 0.015207
2022-01-09 12:54:52,625 iteration 3476 : loss : 0.031814, loss_ce: 0.009584
2022-01-09 12:54:54,026 iteration 3477 : loss : 0.033451, loss_ce: 0.012251
2022-01-09 12:54:55,398 iteration 3478 : loss : 0.020963, loss_ce: 0.008081
2022-01-09 12:54:56,736 iteration 3479 : loss : 0.031322, loss_ce: 0.011964
2022-01-09 12:54:58,026 iteration 3480 : loss : 0.031136, loss_ce: 0.012747
2022-01-09 12:54:59,382 iteration 3481 : loss : 0.030223, loss_ce: 0.011238
2022-01-09 12:55:00,749 iteration 3482 : loss : 0.039272, loss_ce: 0.016182
2022-01-09 12:55:02,175 iteration 3483 : loss : 0.031696, loss_ce: 0.014440
2022-01-09 12:55:03,470 iteration 3484 : loss : 0.027361, loss_ce: 0.010319
2022-01-09 12:55:03,470 Training Data Eval:
2022-01-09 12:55:10,339   Average segmentation loss on training set: 0.0205
2022-01-09 12:55:10,339 Validation Data Eval:
2022-01-09 12:55:12,715   Average segmentation loss on validation set: 0.0894
2022-01-09 12:55:14,173 iteration 3485 : loss : 0.049031, loss_ce: 0.024468
 51%|█████████████▊             | 205/400 [1:27:51<1:27:25, 26.90s/it]2022-01-09 12:55:15,601 iteration 3486 : loss : 0.028894, loss_ce: 0.012522
2022-01-09 12:55:16,922 iteration 3487 : loss : 0.025703, loss_ce: 0.011248
2022-01-09 12:55:18,271 iteration 3488 : loss : 0.034559, loss_ce: 0.014311
2022-01-09 12:55:19,635 iteration 3489 : loss : 0.059055, loss_ce: 0.020723
2022-01-09 12:55:21,027 iteration 3490 : loss : 0.031958, loss_ce: 0.011566
2022-01-09 12:55:22,458 iteration 3491 : loss : 0.039767, loss_ce: 0.015197
2022-01-09 12:55:23,771 iteration 3492 : loss : 0.030032, loss_ce: 0.011488
2022-01-09 12:55:25,245 iteration 3493 : loss : 0.027049, loss_ce: 0.009663
2022-01-09 12:55:26,646 iteration 3494 : loss : 0.031492, loss_ce: 0.008627
2022-01-09 12:55:28,011 iteration 3495 : loss : 0.034113, loss_ce: 0.010844
2022-01-09 12:55:29,413 iteration 3496 : loss : 0.029569, loss_ce: 0.013432
2022-01-09 12:55:30,812 iteration 3497 : loss : 0.058644, loss_ce: 0.026022
2022-01-09 12:55:32,296 iteration 3498 : loss : 0.034429, loss_ce: 0.011324
2022-01-09 12:55:33,641 iteration 3499 : loss : 0.031421, loss_ce: 0.012242
2022-01-09 12:55:34,932 iteration 3500 : loss : 0.020279, loss_ce: 0.006626
2022-01-09 12:55:36,260 iteration 3501 : loss : 0.029532, loss_ce: 0.010826
2022-01-09 12:55:37,642 iteration 3502 : loss : 0.054980, loss_ce: 0.023050
 52%|█████████████▉             | 206/400 [1:28:15<1:23:38, 25.87s/it]2022-01-09 12:55:39,083 iteration 3503 : loss : 0.036135, loss_ce: 0.015910
2022-01-09 12:55:40,433 iteration 3504 : loss : 0.025910, loss_ce: 0.008199
2022-01-09 12:55:41,708 iteration 3505 : loss : 0.022803, loss_ce: 0.010882
2022-01-09 12:55:43,097 iteration 3506 : loss : 0.033687, loss_ce: 0.010856
2022-01-09 12:55:44,451 iteration 3507 : loss : 0.041347, loss_ce: 0.012511
2022-01-09 12:55:45,929 iteration 3508 : loss : 0.046005, loss_ce: 0.020220
2022-01-09 12:55:47,322 iteration 3509 : loss : 0.030799, loss_ce: 0.012557
2022-01-09 12:55:48,645 iteration 3510 : loss : 0.033806, loss_ce: 0.014625
2022-01-09 12:55:49,961 iteration 3511 : loss : 0.024661, loss_ce: 0.011656
2022-01-09 12:55:51,396 iteration 3512 : loss : 0.050359, loss_ce: 0.017174
2022-01-09 12:55:52,774 iteration 3513 : loss : 0.027420, loss_ce: 0.008667
2022-01-09 12:55:54,267 iteration 3514 : loss : 0.044431, loss_ce: 0.021354
2022-01-09 12:55:55,623 iteration 3515 : loss : 0.030675, loss_ce: 0.009958
2022-01-09 12:55:56,968 iteration 3516 : loss : 0.022304, loss_ce: 0.009257
2022-01-09 12:55:58,318 iteration 3517 : loss : 0.034465, loss_ce: 0.012428
2022-01-09 12:55:59,673 iteration 3518 : loss : 0.036808, loss_ce: 0.018984
2022-01-09 12:56:01,066 iteration 3519 : loss : 0.022556, loss_ce: 0.008078
 52%|█████████████▉             | 207/400 [1:28:38<1:20:50, 25.13s/it]2022-01-09 12:56:02,508 iteration 3520 : loss : 0.029134, loss_ce: 0.012890
2022-01-09 12:56:03,847 iteration 3521 : loss : 0.036600, loss_ce: 0.017348
2022-01-09 12:56:05,157 iteration 3522 : loss : 0.048562, loss_ce: 0.025466
2022-01-09 12:56:06,507 iteration 3523 : loss : 0.027878, loss_ce: 0.010537
2022-01-09 12:56:07,904 iteration 3524 : loss : 0.030463, loss_ce: 0.012813
2022-01-09 12:56:09,374 iteration 3525 : loss : 0.030373, loss_ce: 0.010708
2022-01-09 12:56:10,768 iteration 3526 : loss : 0.044926, loss_ce: 0.017800
2022-01-09 12:56:12,186 iteration 3527 : loss : 0.051003, loss_ce: 0.016231
2022-01-09 12:56:13,683 iteration 3528 : loss : 0.045726, loss_ce: 0.019484
2022-01-09 12:56:15,086 iteration 3529 : loss : 0.049227, loss_ce: 0.013468
2022-01-09 12:56:16,485 iteration 3530 : loss : 0.022022, loss_ce: 0.008727
2022-01-09 12:56:17,815 iteration 3531 : loss : 0.039634, loss_ce: 0.012078
2022-01-09 12:56:19,264 iteration 3532 : loss : 0.032079, loss_ce: 0.013460
2022-01-09 12:56:20,623 iteration 3533 : loss : 0.027758, loss_ce: 0.008195
2022-01-09 12:56:22,023 iteration 3534 : loss : 0.034821, loss_ce: 0.014288
2022-01-09 12:56:23,471 iteration 3535 : loss : 0.032100, loss_ce: 0.013496
2022-01-09 12:56:24,887 iteration 3536 : loss : 0.047361, loss_ce: 0.026251
 52%|██████████████             | 208/400 [1:29:02<1:19:09, 24.74s/it]2022-01-09 12:56:26,246 iteration 3537 : loss : 0.028686, loss_ce: 0.011289
2022-01-09 12:56:27,554 iteration 3538 : loss : 0.021732, loss_ce: 0.009459
2022-01-09 12:56:28,876 iteration 3539 : loss : 0.023270, loss_ce: 0.011412
2022-01-09 12:56:30,143 iteration 3540 : loss : 0.026057, loss_ce: 0.007680
2022-01-09 12:56:31,586 iteration 3541 : loss : 0.037104, loss_ce: 0.014360
2022-01-09 12:56:32,960 iteration 3542 : loss : 0.031321, loss_ce: 0.015299
2022-01-09 12:56:34,441 iteration 3543 : loss : 0.048096, loss_ce: 0.015228
2022-01-09 12:56:35,741 iteration 3544 : loss : 0.019777, loss_ce: 0.007687
2022-01-09 12:56:37,100 iteration 3545 : loss : 0.027036, loss_ce: 0.012504
2022-01-09 12:56:38,403 iteration 3546 : loss : 0.023599, loss_ce: 0.008682
2022-01-09 12:56:39,775 iteration 3547 : loss : 0.026264, loss_ce: 0.011360
2022-01-09 12:56:41,091 iteration 3548 : loss : 0.032501, loss_ce: 0.011041
2022-01-09 12:56:42,469 iteration 3549 : loss : 0.028309, loss_ce: 0.010975
2022-01-09 12:56:43,877 iteration 3550 : loss : 0.024850, loss_ce: 0.011408
2022-01-09 12:56:45,311 iteration 3551 : loss : 0.042273, loss_ce: 0.013483
2022-01-09 12:56:46,747 iteration 3552 : loss : 0.057097, loss_ce: 0.020591
2022-01-09 12:56:48,181 iteration 3553 : loss : 0.046986, loss_ce: 0.020624
 52%|██████████████             | 209/400 [1:29:25<1:17:22, 24.31s/it]2022-01-09 12:56:49,622 iteration 3554 : loss : 0.028371, loss_ce: 0.012648
2022-01-09 12:56:50,972 iteration 3555 : loss : 0.028452, loss_ce: 0.009527
2022-01-09 12:56:52,320 iteration 3556 : loss : 0.024612, loss_ce: 0.010457
2022-01-09 12:56:53,779 iteration 3557 : loss : 0.048657, loss_ce: 0.013909
2022-01-09 12:56:55,099 iteration 3558 : loss : 0.048892, loss_ce: 0.013312
2022-01-09 12:56:56,449 iteration 3559 : loss : 0.035136, loss_ce: 0.010121
2022-01-09 12:56:57,711 iteration 3560 : loss : 0.024146, loss_ce: 0.012425
2022-01-09 12:56:59,153 iteration 3561 : loss : 0.035803, loss_ce: 0.012754
2022-01-09 12:57:00,488 iteration 3562 : loss : 0.026393, loss_ce: 0.009937
2022-01-09 12:57:01,885 iteration 3563 : loss : 0.026133, loss_ce: 0.009972
2022-01-09 12:57:03,262 iteration 3564 : loss : 0.042735, loss_ce: 0.015424
2022-01-09 12:57:04,739 iteration 3565 : loss : 0.061840, loss_ce: 0.021290
2022-01-09 12:57:06,125 iteration 3566 : loss : 0.045406, loss_ce: 0.016419
2022-01-09 12:57:07,579 iteration 3567 : loss : 0.030078, loss_ce: 0.014223
2022-01-09 12:57:09,019 iteration 3568 : loss : 0.042233, loss_ce: 0.016473
2022-01-09 12:57:10,450 iteration 3569 : loss : 0.048897, loss_ce: 0.011123
2022-01-09 12:57:10,450 Training Data Eval:
2022-01-09 12:57:17,332   Average segmentation loss on training set: 0.0203
2022-01-09 12:57:17,333 Validation Data Eval:
2022-01-09 12:57:19,712   Average segmentation loss on validation set: 0.0828
2022-01-09 12:57:21,108 iteration 3570 : loss : 0.030365, loss_ce: 0.011156
 52%|██████████████▏            | 210/400 [1:29:58<1:25:09, 26.89s/it]2022-01-09 12:57:22,540 iteration 3571 : loss : 0.029815, loss_ce: 0.011314
2022-01-09 12:57:23,922 iteration 3572 : loss : 0.034174, loss_ce: 0.009774
2022-01-09 12:57:25,298 iteration 3573 : loss : 0.036352, loss_ce: 0.014864
2022-01-09 12:57:26,647 iteration 3574 : loss : 0.022781, loss_ce: 0.009776
2022-01-09 12:57:27,997 iteration 3575 : loss : 0.025390, loss_ce: 0.009697
2022-01-09 12:57:29,384 iteration 3576 : loss : 0.031530, loss_ce: 0.010948
2022-01-09 12:57:30,804 iteration 3577 : loss : 0.032183, loss_ce: 0.015796
2022-01-09 12:57:32,228 iteration 3578 : loss : 0.028022, loss_ce: 0.011368
2022-01-09 12:57:33,583 iteration 3579 : loss : 0.025033, loss_ce: 0.009515
2022-01-09 12:57:34,991 iteration 3580 : loss : 0.030249, loss_ce: 0.011284
2022-01-09 12:57:36,330 iteration 3581 : loss : 0.038024, loss_ce: 0.011933
2022-01-09 12:57:37,693 iteration 3582 : loss : 0.027366, loss_ce: 0.009896
2022-01-09 12:57:39,097 iteration 3583 : loss : 0.057066, loss_ce: 0.025901
2022-01-09 12:57:40,441 iteration 3584 : loss : 0.038867, loss_ce: 0.014812
2022-01-09 12:57:41,802 iteration 3585 : loss : 0.024885, loss_ce: 0.009676
2022-01-09 12:57:43,116 iteration 3586 : loss : 0.033912, loss_ce: 0.013622
2022-01-09 12:57:44,477 iteration 3587 : loss : 0.030152, loss_ce: 0.012903
 53%|██████████████▏            | 211/400 [1:30:22<1:21:22, 25.84s/it]2022-01-09 12:57:45,824 iteration 3588 : loss : 0.027626, loss_ce: 0.010324
2022-01-09 12:57:47,231 iteration 3589 : loss : 0.031813, loss_ce: 0.011230
2022-01-09 12:57:48,599 iteration 3590 : loss : 0.037557, loss_ce: 0.014850
2022-01-09 12:57:49,958 iteration 3591 : loss : 0.021811, loss_ce: 0.006800
2022-01-09 12:57:51,387 iteration 3592 : loss : 0.029415, loss_ce: 0.010172
2022-01-09 12:57:52,780 iteration 3593 : loss : 0.061774, loss_ce: 0.014825
2022-01-09 12:57:54,165 iteration 3594 : loss : 0.021832, loss_ce: 0.006527
2022-01-09 12:57:55,665 iteration 3595 : loss : 0.081578, loss_ce: 0.018138
2022-01-09 12:57:56,974 iteration 3596 : loss : 0.026911, loss_ce: 0.011735
2022-01-09 12:57:58,423 iteration 3597 : loss : 0.033110, loss_ce: 0.012489
2022-01-09 12:57:59,794 iteration 3598 : loss : 0.036715, loss_ce: 0.018431
2022-01-09 12:58:01,181 iteration 3599 : loss : 0.032152, loss_ce: 0.012694
2022-01-09 12:58:02,541 iteration 3600 : loss : 0.036760, loss_ce: 0.018472
2022-01-09 12:58:03,911 iteration 3601 : loss : 0.037249, loss_ce: 0.017364
2022-01-09 12:58:05,258 iteration 3602 : loss : 0.044622, loss_ce: 0.018629
2022-01-09 12:58:06,585 iteration 3603 : loss : 0.022976, loss_ce: 0.008316
2022-01-09 12:58:07,901 iteration 3604 : loss : 0.018626, loss_ce: 0.007124
 53%|██████████████▎            | 212/400 [1:30:45<1:18:41, 25.11s/it]2022-01-09 12:58:09,311 iteration 3605 : loss : 0.029291, loss_ce: 0.011838
2022-01-09 12:58:10,680 iteration 3606 : loss : 0.035545, loss_ce: 0.012257
2022-01-09 12:58:11,991 iteration 3607 : loss : 0.025238, loss_ce: 0.009089
2022-01-09 12:58:13,298 iteration 3608 : loss : 0.028691, loss_ce: 0.013602
2022-01-09 12:58:14,692 iteration 3609 : loss : 0.031809, loss_ce: 0.010865
2022-01-09 12:58:16,025 iteration 3610 : loss : 0.035993, loss_ce: 0.016388
2022-01-09 12:58:17,427 iteration 3611 : loss : 0.032848, loss_ce: 0.014976
2022-01-09 12:58:18,844 iteration 3612 : loss : 0.032428, loss_ce: 0.010922
2022-01-09 12:58:20,222 iteration 3613 : loss : 0.038712, loss_ce: 0.015380
2022-01-09 12:58:21,635 iteration 3614 : loss : 0.032603, loss_ce: 0.011802
2022-01-09 12:58:22,977 iteration 3615 : loss : 0.043830, loss_ce: 0.015220
2022-01-09 12:58:24,413 iteration 3616 : loss : 0.050090, loss_ce: 0.013325
2022-01-09 12:58:25,769 iteration 3617 : loss : 0.033053, loss_ce: 0.013156
2022-01-09 12:58:27,127 iteration 3618 : loss : 0.029929, loss_ce: 0.012081
2022-01-09 12:58:28,483 iteration 3619 : loss : 0.024681, loss_ce: 0.010338
2022-01-09 12:58:29,881 iteration 3620 : loss : 0.041328, loss_ce: 0.018278
2022-01-09 12:58:31,235 iteration 3621 : loss : 0.038673, loss_ce: 0.017721
 53%|██████████████▍            | 213/400 [1:31:08<1:16:36, 24.58s/it]2022-01-09 12:58:32,726 iteration 3622 : loss : 0.041192, loss_ce: 0.014473
2022-01-09 12:58:34,099 iteration 3623 : loss : 0.029275, loss_ce: 0.011916
2022-01-09 12:58:35,544 iteration 3624 : loss : 0.033680, loss_ce: 0.010638
2022-01-09 12:58:36,987 iteration 3625 : loss : 0.039862, loss_ce: 0.018001
2022-01-09 12:58:38,362 iteration 3626 : loss : 0.031871, loss_ce: 0.012763
2022-01-09 12:58:39,800 iteration 3627 : loss : 0.027643, loss_ce: 0.013054
2022-01-09 12:58:41,209 iteration 3628 : loss : 0.028351, loss_ce: 0.009971
2022-01-09 12:58:42,654 iteration 3629 : loss : 0.033801, loss_ce: 0.014919
2022-01-09 12:58:44,029 iteration 3630 : loss : 0.025468, loss_ce: 0.010623
2022-01-09 12:58:45,344 iteration 3631 : loss : 0.024923, loss_ce: 0.010127
2022-01-09 12:58:46,657 iteration 3632 : loss : 0.024430, loss_ce: 0.010912
2022-01-09 12:58:48,081 iteration 3633 : loss : 0.039007, loss_ce: 0.014295
2022-01-09 12:58:49,446 iteration 3634 : loss : 0.031954, loss_ce: 0.011555
2022-01-09 12:58:50,778 iteration 3635 : loss : 0.022501, loss_ce: 0.007600
2022-01-09 12:58:52,111 iteration 3636 : loss : 0.030871, loss_ce: 0.012560
2022-01-09 12:58:53,537 iteration 3637 : loss : 0.044940, loss_ce: 0.013521
2022-01-09 12:58:54,853 iteration 3638 : loss : 0.024030, loss_ce: 0.008240
 54%|██████████████▍            | 214/400 [1:31:32<1:15:18, 24.29s/it]2022-01-09 12:58:56,260 iteration 3639 : loss : 0.022659, loss_ce: 0.009942
2022-01-09 12:58:57,674 iteration 3640 : loss : 0.034586, loss_ce: 0.013373
2022-01-09 12:58:59,054 iteration 3641 : loss : 0.025713, loss_ce: 0.008842
2022-01-09 12:59:00,468 iteration 3642 : loss : 0.040882, loss_ce: 0.016741
2022-01-09 12:59:01,835 iteration 3643 : loss : 0.033505, loss_ce: 0.014522
2022-01-09 12:59:03,190 iteration 3644 : loss : 0.031125, loss_ce: 0.014261
2022-01-09 12:59:04,523 iteration 3645 : loss : 0.021782, loss_ce: 0.009136
2022-01-09 12:59:05,902 iteration 3646 : loss : 0.031343, loss_ce: 0.011087
2022-01-09 12:59:07,314 iteration 3647 : loss : 0.039557, loss_ce: 0.017683
2022-01-09 12:59:08,792 iteration 3648 : loss : 0.037890, loss_ce: 0.014603
2022-01-09 12:59:10,184 iteration 3649 : loss : 0.039047, loss_ce: 0.014800
2022-01-09 12:59:11,555 iteration 3650 : loss : 0.029372, loss_ce: 0.008885
2022-01-09 12:59:12,950 iteration 3651 : loss : 0.027453, loss_ce: 0.010265
2022-01-09 12:59:14,296 iteration 3652 : loss : 0.028399, loss_ce: 0.011759
2022-01-09 12:59:15,728 iteration 3653 : loss : 0.033268, loss_ce: 0.007844
2022-01-09 12:59:17,108 iteration 3654 : loss : 0.025032, loss_ce: 0.008048
2022-01-09 12:59:17,108 Training Data Eval:
2022-01-09 12:59:23,966   Average segmentation loss on training set: 0.0188
2022-01-09 12:59:23,967 Validation Data Eval:
2022-01-09 12:59:26,341   Average segmentation loss on validation set: 0.0831
2022-01-09 12:59:27,626 iteration 3655 : loss : 0.022741, loss_ce: 0.008236
 54%|██████████████▌            | 215/400 [1:32:05<1:22:44, 26.84s/it]2022-01-09 12:59:29,111 iteration 3656 : loss : 0.057184, loss_ce: 0.010210
2022-01-09 12:59:30,456 iteration 3657 : loss : 0.029443, loss_ce: 0.014372
2022-01-09 12:59:31,829 iteration 3658 : loss : 0.036845, loss_ce: 0.015026
2022-01-09 12:59:33,237 iteration 3659 : loss : 0.043736, loss_ce: 0.012358
2022-01-09 12:59:34,703 iteration 3660 : loss : 0.041998, loss_ce: 0.013653
2022-01-09 12:59:36,185 iteration 3661 : loss : 0.041346, loss_ce: 0.020906
2022-01-09 12:59:37,483 iteration 3662 : loss : 0.024962, loss_ce: 0.009085
2022-01-09 12:59:38,822 iteration 3663 : loss : 0.022498, loss_ce: 0.007382
2022-01-09 12:59:40,176 iteration 3664 : loss : 0.036402, loss_ce: 0.010724
2022-01-09 12:59:41,537 iteration 3665 : loss : 0.025236, loss_ce: 0.010780
2022-01-09 12:59:43,016 iteration 3666 : loss : 0.038548, loss_ce: 0.017864
2022-01-09 12:59:44,431 iteration 3667 : loss : 0.042627, loss_ce: 0.017471
2022-01-09 12:59:45,848 iteration 3668 : loss : 0.043336, loss_ce: 0.017195
2022-01-09 12:59:47,237 iteration 3669 : loss : 0.029535, loss_ce: 0.013985
2022-01-09 12:59:48,588 iteration 3670 : loss : 0.040215, loss_ce: 0.012728
2022-01-09 12:59:50,002 iteration 3671 : loss : 0.029197, loss_ce: 0.012084
2022-01-09 12:59:51,334 iteration 3672 : loss : 0.027685, loss_ce: 0.010936
 54%|██████████████▌            | 216/400 [1:32:29<1:19:24, 25.90s/it]2022-01-09 12:59:52,748 iteration 3673 : loss : 0.030748, loss_ce: 0.016043
2022-01-09 12:59:54,124 iteration 3674 : loss : 0.028884, loss_ce: 0.012330
2022-01-09 12:59:55,481 iteration 3675 : loss : 0.023827, loss_ce: 0.012477
2022-01-09 12:59:56,834 iteration 3676 : loss : 0.020502, loss_ce: 0.006849
2022-01-09 12:59:58,191 iteration 3677 : loss : 0.031427, loss_ce: 0.014886
2022-01-09 12:59:59,634 iteration 3678 : loss : 0.038497, loss_ce: 0.012670
2022-01-09 13:00:00,994 iteration 3679 : loss : 0.034136, loss_ce: 0.013621
2022-01-09 13:00:02,342 iteration 3680 : loss : 0.036224, loss_ce: 0.012871
2022-01-09 13:00:03,761 iteration 3681 : loss : 0.025322, loss_ce: 0.009011
2022-01-09 13:00:05,116 iteration 3682 : loss : 0.033035, loss_ce: 0.014174
2022-01-09 13:00:06,533 iteration 3683 : loss : 0.051119, loss_ce: 0.009249
2022-01-09 13:00:07,882 iteration 3684 : loss : 0.019382, loss_ce: 0.006551
2022-01-09 13:00:09,234 iteration 3685 : loss : 0.029943, loss_ce: 0.012439
2022-01-09 13:00:10,625 iteration 3686 : loss : 0.034305, loss_ce: 0.013914
2022-01-09 13:00:11,999 iteration 3687 : loss : 0.026868, loss_ce: 0.008614
2022-01-09 13:00:13,358 iteration 3688 : loss : 0.032439, loss_ce: 0.015600
2022-01-09 13:00:14,665 iteration 3689 : loss : 0.029078, loss_ce: 0.009925
 54%|██████████████▋            | 217/400 [1:32:52<1:16:38, 25.13s/it]2022-01-09 13:00:16,047 iteration 3690 : loss : 0.021185, loss_ce: 0.005456
2022-01-09 13:00:17,487 iteration 3691 : loss : 0.048239, loss_ce: 0.009931
2022-01-09 13:00:18,862 iteration 3692 : loss : 0.027806, loss_ce: 0.011835
2022-01-09 13:00:20,297 iteration 3693 : loss : 0.047617, loss_ce: 0.018188
2022-01-09 13:00:21,679 iteration 3694 : loss : 0.020606, loss_ce: 0.009062
2022-01-09 13:00:23,009 iteration 3695 : loss : 0.026473, loss_ce: 0.011298
2022-01-09 13:00:24,417 iteration 3696 : loss : 0.031640, loss_ce: 0.016490
2022-01-09 13:00:25,778 iteration 3697 : loss : 0.032209, loss_ce: 0.010160
2022-01-09 13:00:27,153 iteration 3698 : loss : 0.036908, loss_ce: 0.013543
2022-01-09 13:00:28,594 iteration 3699 : loss : 0.071886, loss_ce: 0.024085
2022-01-09 13:00:29,970 iteration 3700 : loss : 0.028508, loss_ce: 0.012916
2022-01-09 13:00:31,321 iteration 3701 : loss : 0.033661, loss_ce: 0.012532
2022-01-09 13:00:32,659 iteration 3702 : loss : 0.039648, loss_ce: 0.017726
2022-01-09 13:00:34,135 iteration 3703 : loss : 0.045941, loss_ce: 0.023490
2022-01-09 13:00:35,472 iteration 3704 : loss : 0.027321, loss_ce: 0.007969
2022-01-09 13:00:36,803 iteration 3705 : loss : 0.024293, loss_ce: 0.008684
2022-01-09 13:00:38,122 iteration 3706 : loss : 0.025441, loss_ce: 0.006927
 55%|██████████████▋            | 218/400 [1:33:15<1:14:41, 24.62s/it]2022-01-09 13:00:39,538 iteration 3707 : loss : 0.038771, loss_ce: 0.012172
2022-01-09 13:00:40,910 iteration 3708 : loss : 0.033046, loss_ce: 0.012272
2022-01-09 13:00:42,286 iteration 3709 : loss : 0.029116, loss_ce: 0.008689
2022-01-09 13:00:43,702 iteration 3710 : loss : 0.043862, loss_ce: 0.016282
2022-01-09 13:00:45,052 iteration 3711 : loss : 0.025127, loss_ce: 0.009121
2022-01-09 13:00:46,443 iteration 3712 : loss : 0.036309, loss_ce: 0.012404
2022-01-09 13:00:47,766 iteration 3713 : loss : 0.031555, loss_ce: 0.011010
2022-01-09 13:00:49,103 iteration 3714 : loss : 0.025344, loss_ce: 0.008109
2022-01-09 13:00:50,481 iteration 3715 : loss : 0.034480, loss_ce: 0.013311
2022-01-09 13:00:51,814 iteration 3716 : loss : 0.029167, loss_ce: 0.011967
2022-01-09 13:00:53,198 iteration 3717 : loss : 0.033222, loss_ce: 0.014962
2022-01-09 13:00:54,623 iteration 3718 : loss : 0.036969, loss_ce: 0.018353
2022-01-09 13:00:55,949 iteration 3719 : loss : 0.025284, loss_ce: 0.007107
2022-01-09 13:00:57,337 iteration 3720 : loss : 0.022725, loss_ce: 0.011285
2022-01-09 13:00:58,714 iteration 3721 : loss : 0.030422, loss_ce: 0.012372
2022-01-09 13:01:00,075 iteration 3722 : loss : 0.024145, loss_ce: 0.009235
2022-01-09 13:01:01,411 iteration 3723 : loss : 0.049291, loss_ce: 0.012535
 55%|██████████████▊            | 219/400 [1:33:39<1:13:04, 24.22s/it]2022-01-09 13:01:02,770 iteration 3724 : loss : 0.031557, loss_ce: 0.016282
2022-01-09 13:01:04,099 iteration 3725 : loss : 0.035737, loss_ce: 0.013923
2022-01-09 13:01:05,517 iteration 3726 : loss : 0.046190, loss_ce: 0.025542
2022-01-09 13:01:06,896 iteration 3727 : loss : 0.027622, loss_ce: 0.012639
2022-01-09 13:01:08,282 iteration 3728 : loss : 0.042210, loss_ce: 0.012439
2022-01-09 13:01:09,611 iteration 3729 : loss : 0.036216, loss_ce: 0.011571
2022-01-09 13:01:10,966 iteration 3730 : loss : 0.024327, loss_ce: 0.009888
2022-01-09 13:01:12,312 iteration 3731 : loss : 0.040198, loss_ce: 0.012562
2022-01-09 13:01:13,709 iteration 3732 : loss : 0.035267, loss_ce: 0.011709
2022-01-09 13:01:15,035 iteration 3733 : loss : 0.026571, loss_ce: 0.010141
2022-01-09 13:01:16,418 iteration 3734 : loss : 0.032236, loss_ce: 0.013276
2022-01-09 13:01:17,792 iteration 3735 : loss : 0.041964, loss_ce: 0.011728
2022-01-09 13:01:19,208 iteration 3736 : loss : 0.035625, loss_ce: 0.015140
2022-01-09 13:01:20,540 iteration 3737 : loss : 0.030236, loss_ce: 0.010363
2022-01-09 13:01:21,904 iteration 3738 : loss : 0.023733, loss_ce: 0.008772
2022-01-09 13:01:23,228 iteration 3739 : loss : 0.029227, loss_ce: 0.012081
2022-01-09 13:01:23,228 Training Data Eval:
2022-01-09 13:01:30,101   Average segmentation loss on training set: 0.0189
2022-01-09 13:01:30,102 Validation Data Eval:
2022-01-09 13:01:32,487   Average segmentation loss on validation set: 0.0855
2022-01-09 13:01:33,999 iteration 3740 : loss : 0.035535, loss_ce: 0.017499
 55%|██████████████▊            | 220/400 [1:34:11<1:20:11, 26.73s/it]2022-01-09 13:01:35,542 iteration 3741 : loss : 0.034111, loss_ce: 0.010246
2022-01-09 13:01:36,904 iteration 3742 : loss : 0.035935, loss_ce: 0.016472
2022-01-09 13:01:38,276 iteration 3743 : loss : 0.043385, loss_ce: 0.017135
2022-01-09 13:01:39,696 iteration 3744 : loss : 0.035169, loss_ce: 0.013061
2022-01-09 13:01:41,134 iteration 3745 : loss : 0.032137, loss_ce: 0.012010
2022-01-09 13:01:42,578 iteration 3746 : loss : 0.032997, loss_ce: 0.013045
2022-01-09 13:01:43,940 iteration 3747 : loss : 0.031127, loss_ce: 0.011262
2022-01-09 13:01:45,294 iteration 3748 : loss : 0.023597, loss_ce: 0.011557
2022-01-09 13:01:46,717 iteration 3749 : loss : 0.033437, loss_ce: 0.011961
2022-01-09 13:01:48,166 iteration 3750 : loss : 0.038228, loss_ce: 0.013929
2022-01-09 13:01:49,571 iteration 3751 : loss : 0.036248, loss_ce: 0.010896
2022-01-09 13:01:50,926 iteration 3752 : loss : 0.031922, loss_ce: 0.009172
2022-01-09 13:01:52,260 iteration 3753 : loss : 0.036174, loss_ce: 0.010873
2022-01-09 13:01:53,635 iteration 3754 : loss : 0.027003, loss_ce: 0.009862
2022-01-09 13:01:55,057 iteration 3755 : loss : 0.026715, loss_ce: 0.011179
2022-01-09 13:01:56,360 iteration 3756 : loss : 0.028440, loss_ce: 0.012457
2022-01-09 13:01:57,683 iteration 3757 : loss : 0.026127, loss_ce: 0.011976
 55%|██████████████▉            | 221/400 [1:34:35<1:17:02, 25.82s/it]2022-01-09 13:01:59,186 iteration 3758 : loss : 0.027583, loss_ce: 0.009071
2022-01-09 13:02:00,519 iteration 3759 : loss : 0.025726, loss_ce: 0.008390
2022-01-09 13:02:01,874 iteration 3760 : loss : 0.024252, loss_ce: 0.009783
2022-01-09 13:02:03,285 iteration 3761 : loss : 0.031676, loss_ce: 0.013294
2022-01-09 13:02:04,716 iteration 3762 : loss : 0.044746, loss_ce: 0.028485
2022-01-09 13:02:06,184 iteration 3763 : loss : 0.024518, loss_ce: 0.008676
2022-01-09 13:02:07,552 iteration 3764 : loss : 0.041994, loss_ce: 0.016234
2022-01-09 13:02:08,878 iteration 3765 : loss : 0.029455, loss_ce: 0.012029
2022-01-09 13:02:10,298 iteration 3766 : loss : 0.052925, loss_ce: 0.015814
2022-01-09 13:02:11,669 iteration 3767 : loss : 0.019472, loss_ce: 0.007062
2022-01-09 13:02:13,007 iteration 3768 : loss : 0.034618, loss_ce: 0.011223
2022-01-09 13:02:14,352 iteration 3769 : loss : 0.026639, loss_ce: 0.011944
2022-01-09 13:02:15,753 iteration 3770 : loss : 0.035996, loss_ce: 0.007599
2022-01-09 13:02:17,243 iteration 3771 : loss : 0.028653, loss_ce: 0.010061
2022-01-09 13:02:18,677 iteration 3772 : loss : 0.030086, loss_ce: 0.007590
2022-01-09 13:02:20,075 iteration 3773 : loss : 0.019597, loss_ce: 0.007069
2022-01-09 13:02:21,514 iteration 3774 : loss : 0.061682, loss_ce: 0.029867
 56%|██████████████▉            | 222/400 [1:34:59<1:14:49, 25.22s/it]2022-01-09 13:02:22,923 iteration 3775 : loss : 0.033842, loss_ce: 0.012479
2022-01-09 13:02:24,219 iteration 3776 : loss : 0.050696, loss_ce: 0.025395
2022-01-09 13:02:25,574 iteration 3777 : loss : 0.024618, loss_ce: 0.007029
2022-01-09 13:02:26,909 iteration 3778 : loss : 0.024397, loss_ce: 0.009637
2022-01-09 13:02:28,239 iteration 3779 : loss : 0.022016, loss_ce: 0.009474
2022-01-09 13:02:29,623 iteration 3780 : loss : 0.030759, loss_ce: 0.011680
2022-01-09 13:02:30,998 iteration 3781 : loss : 0.034965, loss_ce: 0.014420
2022-01-09 13:02:32,393 iteration 3782 : loss : 0.093424, loss_ce: 0.016228
2022-01-09 13:02:33,786 iteration 3783 : loss : 0.026934, loss_ce: 0.013714
2022-01-09 13:02:35,187 iteration 3784 : loss : 0.021802, loss_ce: 0.009936
2022-01-09 13:02:36,531 iteration 3785 : loss : 0.025867, loss_ce: 0.011351
2022-01-09 13:02:37,876 iteration 3786 : loss : 0.024200, loss_ce: 0.008294
2022-01-09 13:02:39,212 iteration 3787 : loss : 0.028000, loss_ce: 0.009327
2022-01-09 13:02:40,649 iteration 3788 : loss : 0.049074, loss_ce: 0.012620
2022-01-09 13:02:42,016 iteration 3789 : loss : 0.026787, loss_ce: 0.007051
2022-01-09 13:02:43,422 iteration 3790 : loss : 0.033210, loss_ce: 0.013209
2022-01-09 13:02:44,820 iteration 3791 : loss : 0.028641, loss_ce: 0.011458
 56%|███████████████            | 223/400 [1:35:22<1:12:42, 24.65s/it]2022-01-09 13:02:46,226 iteration 3792 : loss : 0.025598, loss_ce: 0.010455
2022-01-09 13:02:47,643 iteration 3793 : loss : 0.027116, loss_ce: 0.008417
2022-01-09 13:02:49,040 iteration 3794 : loss : 0.037403, loss_ce: 0.015395
2022-01-09 13:02:50,416 iteration 3795 : loss : 0.023272, loss_ce: 0.007382
2022-01-09 13:02:51,700 iteration 3796 : loss : 0.022481, loss_ce: 0.007858
2022-01-09 13:02:53,048 iteration 3797 : loss : 0.029704, loss_ce: 0.011853
2022-01-09 13:02:54,359 iteration 3798 : loss : 0.026704, loss_ce: 0.010047
2022-01-09 13:02:55,707 iteration 3799 : loss : 0.028805, loss_ce: 0.009258
2022-01-09 13:02:57,061 iteration 3800 : loss : 0.025584, loss_ce: 0.008982
2022-01-09 13:02:58,413 iteration 3801 : loss : 0.044763, loss_ce: 0.011403
2022-01-09 13:02:59,748 iteration 3802 : loss : 0.039586, loss_ce: 0.020520
2022-01-09 13:03:01,110 iteration 3803 : loss : 0.023286, loss_ce: 0.007864
2022-01-09 13:03:02,475 iteration 3804 : loss : 0.031093, loss_ce: 0.013256
2022-01-09 13:03:03,817 iteration 3805 : loss : 0.027227, loss_ce: 0.010696
2022-01-09 13:03:05,153 iteration 3806 : loss : 0.034849, loss_ce: 0.017393
2022-01-09 13:03:06,553 iteration 3807 : loss : 0.027014, loss_ce: 0.010856
2022-01-09 13:03:07,927 iteration 3808 : loss : 0.036354, loss_ce: 0.014616
 56%|███████████████            | 224/400 [1:35:45<1:10:56, 24.19s/it]2022-01-09 13:03:09,295 iteration 3809 : loss : 0.025558, loss_ce: 0.008321
2022-01-09 13:03:10,638 iteration 3810 : loss : 0.030584, loss_ce: 0.013961
2022-01-09 13:03:12,000 iteration 3811 : loss : 0.031031, loss_ce: 0.008758
2022-01-09 13:03:13,381 iteration 3812 : loss : 0.033265, loss_ce: 0.017072
2022-01-09 13:03:14,736 iteration 3813 : loss : 0.027045, loss_ce: 0.013145
2022-01-09 13:03:16,128 iteration 3814 : loss : 0.056766, loss_ce: 0.022259
2022-01-09 13:03:17,496 iteration 3815 : loss : 0.024081, loss_ce: 0.009174
2022-01-09 13:03:18,881 iteration 3816 : loss : 0.025459, loss_ce: 0.009821
2022-01-09 13:03:20,202 iteration 3817 : loss : 0.027350, loss_ce: 0.010045
2022-01-09 13:03:21,593 iteration 3818 : loss : 0.026109, loss_ce: 0.011465
2022-01-09 13:03:23,007 iteration 3819 : loss : 0.025719, loss_ce: 0.011586
2022-01-09 13:03:24,406 iteration 3820 : loss : 0.037490, loss_ce: 0.013445
2022-01-09 13:03:25,807 iteration 3821 : loss : 0.053772, loss_ce: 0.022114
2022-01-09 13:03:27,140 iteration 3822 : loss : 0.037382, loss_ce: 0.012205
2022-01-09 13:03:28,590 iteration 3823 : loss : 0.039047, loss_ce: 0.011367
2022-01-09 13:03:29,967 iteration 3824 : loss : 0.044362, loss_ce: 0.014462
2022-01-09 13:03:29,967 Training Data Eval:
2022-01-09 13:03:36,831   Average segmentation loss on training set: 0.0189
2022-01-09 13:03:36,832 Validation Data Eval:
2022-01-09 13:03:39,210   Average segmentation loss on validation set: 0.0807
2022-01-09 13:03:40,648 iteration 3825 : loss : 0.033409, loss_ce: 0.011954
 56%|███████████████▏           | 225/400 [1:36:18<1:18:00, 26.74s/it]2022-01-09 13:03:42,091 iteration 3826 : loss : 0.027270, loss_ce: 0.012135
2022-01-09 13:03:43,418 iteration 3827 : loss : 0.028327, loss_ce: 0.009885
2022-01-09 13:03:44,803 iteration 3828 : loss : 0.028254, loss_ce: 0.011286
2022-01-09 13:03:46,180 iteration 3829 : loss : 0.029809, loss_ce: 0.009547
2022-01-09 13:03:47,519 iteration 3830 : loss : 0.022389, loss_ce: 0.008176
2022-01-09 13:03:48,842 iteration 3831 : loss : 0.025079, loss_ce: 0.009255
2022-01-09 13:03:50,164 iteration 3832 : loss : 0.026050, loss_ce: 0.006793
2022-01-09 13:03:51,522 iteration 3833 : loss : 0.023878, loss_ce: 0.011215
2022-01-09 13:03:52,922 iteration 3834 : loss : 0.035608, loss_ce: 0.016285
2022-01-09 13:03:54,305 iteration 3835 : loss : 0.028679, loss_ce: 0.010102
2022-01-09 13:03:55,681 iteration 3836 : loss : 0.028669, loss_ce: 0.010953
2022-01-09 13:03:57,079 iteration 3837 : loss : 0.027143, loss_ce: 0.009488
2022-01-09 13:03:58,536 iteration 3838 : loss : 0.029212, loss_ce: 0.010651
2022-01-09 13:03:59,904 iteration 3839 : loss : 0.025947, loss_ce: 0.011143
2022-01-09 13:04:01,288 iteration 3840 : loss : 0.028847, loss_ce: 0.007276
2022-01-09 13:04:02,627 iteration 3841 : loss : 0.026019, loss_ce: 0.009223
2022-01-09 13:04:04,020 iteration 3842 : loss : 0.033946, loss_ce: 0.016294
 56%|███████████████▎           | 226/400 [1:36:41<1:14:37, 25.73s/it]2022-01-09 13:04:05,410 iteration 3843 : loss : 0.024462, loss_ce: 0.009622
2022-01-09 13:04:06,823 iteration 3844 : loss : 0.033960, loss_ce: 0.010126
2022-01-09 13:04:08,217 iteration 3845 : loss : 0.033572, loss_ce: 0.017967
2022-01-09 13:04:09,547 iteration 3846 : loss : 0.020484, loss_ce: 0.007519
2022-01-09 13:04:10,904 iteration 3847 : loss : 0.023284, loss_ce: 0.008898
2022-01-09 13:04:12,318 iteration 3848 : loss : 0.027055, loss_ce: 0.011941
2022-01-09 13:04:13,787 iteration 3849 : loss : 0.038069, loss_ce: 0.016937
2022-01-09 13:04:15,243 iteration 3850 : loss : 0.055850, loss_ce: 0.014577
2022-01-09 13:04:16,630 iteration 3851 : loss : 0.028492, loss_ce: 0.011680
2022-01-09 13:04:18,028 iteration 3852 : loss : 0.027338, loss_ce: 0.008048
2022-01-09 13:04:19,366 iteration 3853 : loss : 0.020768, loss_ce: 0.006150
2022-01-09 13:04:20,763 iteration 3854 : loss : 0.035305, loss_ce: 0.011463
2022-01-09 13:04:22,269 iteration 3855 : loss : 0.042146, loss_ce: 0.016510
2022-01-09 13:04:23,527 iteration 3856 : loss : 0.024809, loss_ce: 0.011606
2022-01-09 13:04:24,880 iteration 3857 : loss : 0.030062, loss_ce: 0.013445
2022-01-09 13:04:26,167 iteration 3858 : loss : 0.027614, loss_ce: 0.010866
2022-01-09 13:04:27,529 iteration 3859 : loss : 0.034315, loss_ce: 0.017433
 57%|███████████████▎           | 227/400 [1:37:05<1:12:16, 25.07s/it]2022-01-09 13:04:28,951 iteration 3860 : loss : 0.023024, loss_ce: 0.009034
2022-01-09 13:04:30,395 iteration 3861 : loss : 0.038827, loss_ce: 0.017409
2022-01-09 13:04:31,794 iteration 3862 : loss : 0.036991, loss_ce: 0.014005
2022-01-09 13:04:33,188 iteration 3863 : loss : 0.035682, loss_ce: 0.013233
2022-01-09 13:04:34,598 iteration 3864 : loss : 0.028971, loss_ce: 0.011327
2022-01-09 13:04:35,967 iteration 3865 : loss : 0.028436, loss_ce: 0.009123
2022-01-09 13:04:37,449 iteration 3866 : loss : 0.033291, loss_ce: 0.011321
2022-01-09 13:04:38,816 iteration 3867 : loss : 0.028265, loss_ce: 0.010338
2022-01-09 13:04:40,245 iteration 3868 : loss : 0.042152, loss_ce: 0.014996
2022-01-09 13:04:41,649 iteration 3869 : loss : 0.033212, loss_ce: 0.016954
2022-01-09 13:04:43,078 iteration 3870 : loss : 0.027082, loss_ce: 0.010517
2022-01-09 13:04:44,485 iteration 3871 : loss : 0.027068, loss_ce: 0.010754
2022-01-09 13:04:45,830 iteration 3872 : loss : 0.033877, loss_ce: 0.011793
2022-01-09 13:04:47,170 iteration 3873 : loss : 0.021975, loss_ce: 0.011573
2022-01-09 13:04:48,569 iteration 3874 : loss : 0.029440, loss_ce: 0.012235
2022-01-09 13:04:49,963 iteration 3875 : loss : 0.035094, loss_ce: 0.013790
2022-01-09 13:04:51,361 iteration 3876 : loss : 0.033633, loss_ce: 0.011398
 57%|███████████████▍           | 228/400 [1:37:29<1:10:48, 24.70s/it]2022-01-09 13:04:52,769 iteration 3877 : loss : 0.027916, loss_ce: 0.011470
2022-01-09 13:04:54,175 iteration 3878 : loss : 0.030421, loss_ce: 0.012443
2022-01-09 13:04:55,525 iteration 3879 : loss : 0.021662, loss_ce: 0.007648
2022-01-09 13:04:56,936 iteration 3880 : loss : 0.024962, loss_ce: 0.010586
2022-01-09 13:04:58,284 iteration 3881 : loss : 0.047997, loss_ce: 0.024685
2022-01-09 13:04:59,742 iteration 3882 : loss : 0.058018, loss_ce: 0.013210
2022-01-09 13:05:01,171 iteration 3883 : loss : 0.036478, loss_ce: 0.012532
2022-01-09 13:05:02,577 iteration 3884 : loss : 0.033983, loss_ce: 0.016394
2022-01-09 13:05:04,002 iteration 3885 : loss : 0.103847, loss_ce: 0.026133
2022-01-09 13:05:05,424 iteration 3886 : loss : 0.035197, loss_ce: 0.013651
2022-01-09 13:05:06,754 iteration 3887 : loss : 0.026956, loss_ce: 0.010164
2022-01-09 13:05:08,167 iteration 3888 : loss : 0.030808, loss_ce: 0.015455
2022-01-09 13:05:09,554 iteration 3889 : loss : 0.030428, loss_ce: 0.013448
2022-01-09 13:05:11,036 iteration 3890 : loss : 0.081246, loss_ce: 0.031808
2022-01-09 13:05:12,389 iteration 3891 : loss : 0.050363, loss_ce: 0.019858
2022-01-09 13:05:13,777 iteration 3892 : loss : 0.040960, loss_ce: 0.018365
2022-01-09 13:05:15,185 iteration 3893 : loss : 0.072617, loss_ce: 0.029877
 57%|███████████████▍           | 229/400 [1:37:52<1:09:38, 24.43s/it]2022-01-09 13:05:16,600 iteration 3894 : loss : 0.024026, loss_ce: 0.007389
2022-01-09 13:05:17,934 iteration 3895 : loss : 0.031222, loss_ce: 0.012965
2022-01-09 13:05:19,308 iteration 3896 : loss : 0.062902, loss_ce: 0.034475
2022-01-09 13:05:20,724 iteration 3897 : loss : 0.032295, loss_ce: 0.013614
2022-01-09 13:05:22,131 iteration 3898 : loss : 0.046548, loss_ce: 0.013937
2022-01-09 13:05:23,443 iteration 3899 : loss : 0.032714, loss_ce: 0.012871
2022-01-09 13:05:24,787 iteration 3900 : loss : 0.037669, loss_ce: 0.016895
2022-01-09 13:05:26,109 iteration 3901 : loss : 0.044074, loss_ce: 0.019270
2022-01-09 13:05:27,441 iteration 3902 : loss : 0.030246, loss_ce: 0.012213
2022-01-09 13:05:28,740 iteration 3903 : loss : 0.043093, loss_ce: 0.017686
2022-01-09 13:05:30,090 iteration 3904 : loss : 0.032007, loss_ce: 0.010718
2022-01-09 13:05:31,446 iteration 3905 : loss : 0.027867, loss_ce: 0.010276
2022-01-09 13:05:32,734 iteration 3906 : loss : 0.029234, loss_ce: 0.012338
2022-01-09 13:05:34,010 iteration 3907 : loss : 0.023452, loss_ce: 0.008866
2022-01-09 13:05:35,306 iteration 3908 : loss : 0.036728, loss_ce: 0.013613
2022-01-09 13:05:36,662 iteration 3909 : loss : 0.041908, loss_ce: 0.022979
2022-01-09 13:05:36,662 Training Data Eval:
2022-01-09 13:05:43,527   Average segmentation loss on training set: 0.0237
2022-01-09 13:05:43,528 Validation Data Eval:
2022-01-09 13:05:45,904   Average segmentation loss on validation set: 0.1205
2022-01-09 13:05:47,331 iteration 3910 : loss : 0.033919, loss_ce: 0.013908
 57%|███████████████▌           | 230/400 [1:38:25<1:15:47, 26.75s/it]2022-01-09 13:05:48,713 iteration 3911 : loss : 0.031253, loss_ce: 0.014771
2022-01-09 13:05:50,112 iteration 3912 : loss : 0.032745, loss_ce: 0.012773
2022-01-09 13:05:51,478 iteration 3913 : loss : 0.020996, loss_ce: 0.007984
2022-01-09 13:05:52,836 iteration 3914 : loss : 0.022148, loss_ce: 0.010100
2022-01-09 13:05:54,191 iteration 3915 : loss : 0.030173, loss_ce: 0.013582
2022-01-09 13:05:55,514 iteration 3916 : loss : 0.038248, loss_ce: 0.014175
2022-01-09 13:05:56,910 iteration 3917 : loss : 0.056188, loss_ce: 0.020407
2022-01-09 13:05:58,305 iteration 3918 : loss : 0.066693, loss_ce: 0.020932
2022-01-09 13:05:59,801 iteration 3919 : loss : 0.038758, loss_ce: 0.013459
2022-01-09 13:06:01,136 iteration 3920 : loss : 0.027152, loss_ce: 0.010988
2022-01-09 13:06:02,502 iteration 3921 : loss : 0.026421, loss_ce: 0.011124
2022-01-09 13:06:03,912 iteration 3922 : loss : 0.038789, loss_ce: 0.013835
2022-01-09 13:06:05,262 iteration 3923 : loss : 0.031572, loss_ce: 0.019237
2022-01-09 13:06:06,582 iteration 3924 : loss : 0.036961, loss_ce: 0.010512
2022-01-09 13:06:08,022 iteration 3925 : loss : 0.051316, loss_ce: 0.022285
2022-01-09 13:06:09,442 iteration 3926 : loss : 0.021731, loss_ce: 0.007824
2022-01-09 13:06:10,812 iteration 3927 : loss : 0.050117, loss_ce: 0.021517
 58%|███████████████▌           | 231/400 [1:38:48<1:12:34, 25.77s/it]2022-01-09 13:06:12,289 iteration 3928 : loss : 0.046371, loss_ce: 0.020094
2022-01-09 13:06:13,665 iteration 3929 : loss : 0.047283, loss_ce: 0.023019
2022-01-09 13:06:15,090 iteration 3930 : loss : 0.031698, loss_ce: 0.013279
2022-01-09 13:06:16,504 iteration 3931 : loss : 0.029135, loss_ce: 0.012535
2022-01-09 13:06:17,920 iteration 3932 : loss : 0.052020, loss_ce: 0.017955
2022-01-09 13:06:19,373 iteration 3933 : loss : 0.030671, loss_ce: 0.012171
2022-01-09 13:06:20,783 iteration 3934 : loss : 0.028317, loss_ce: 0.013104
2022-01-09 13:06:22,089 iteration 3935 : loss : 0.026158, loss_ce: 0.011196
2022-01-09 13:06:23,585 iteration 3936 : loss : 0.051333, loss_ce: 0.022825
2022-01-09 13:06:24,953 iteration 3937 : loss : 0.034837, loss_ce: 0.016844
2022-01-09 13:06:26,370 iteration 3938 : loss : 0.077993, loss_ce: 0.016037
2022-01-09 13:06:27,677 iteration 3939 : loss : 0.039775, loss_ce: 0.013080
2022-01-09 13:06:29,024 iteration 3940 : loss : 0.029656, loss_ce: 0.010842
2022-01-09 13:06:30,392 iteration 3941 : loss : 0.024139, loss_ce: 0.009666
2022-01-09 13:06:31,815 iteration 3942 : loss : 0.056978, loss_ce: 0.027082
2022-01-09 13:06:33,151 iteration 3943 : loss : 0.039697, loss_ce: 0.009503
2022-01-09 13:06:34,583 iteration 3944 : loss : 0.041185, loss_ce: 0.014888
 58%|███████████████▋           | 232/400 [1:39:12<1:10:28, 25.17s/it]2022-01-09 13:06:36,028 iteration 3945 : loss : 0.060290, loss_ce: 0.028233
2022-01-09 13:06:37,316 iteration 3946 : loss : 0.028964, loss_ce: 0.014945
2022-01-09 13:06:38,676 iteration 3947 : loss : 0.030779, loss_ce: 0.009120
2022-01-09 13:06:40,168 iteration 3948 : loss : 0.042550, loss_ce: 0.014977
2022-01-09 13:06:41,582 iteration 3949 : loss : 0.072322, loss_ce: 0.028910
2022-01-09 13:06:43,023 iteration 3950 : loss : 0.026516, loss_ce: 0.008960
2022-01-09 13:06:44,341 iteration 3951 : loss : 0.023402, loss_ce: 0.009867
2022-01-09 13:06:45,757 iteration 3952 : loss : 0.023506, loss_ce: 0.008873
2022-01-09 13:06:47,192 iteration 3953 : loss : 0.049953, loss_ce: 0.020212
2022-01-09 13:06:48,557 iteration 3954 : loss : 0.029271, loss_ce: 0.010216
2022-01-09 13:06:49,921 iteration 3955 : loss : 0.020460, loss_ce: 0.007016
2022-01-09 13:06:51,413 iteration 3956 : loss : 0.037568, loss_ce: 0.016491
2022-01-09 13:06:52,756 iteration 3957 : loss : 0.033908, loss_ce: 0.015426
2022-01-09 13:06:54,108 iteration 3958 : loss : 0.031593, loss_ce: 0.013183
2022-01-09 13:06:55,561 iteration 3959 : loss : 0.045029, loss_ce: 0.014095
2022-01-09 13:06:56,918 iteration 3960 : loss : 0.032602, loss_ce: 0.017449
2022-01-09 13:06:58,292 iteration 3961 : loss : 0.026986, loss_ce: 0.010583
 58%|███████████████▋           | 233/400 [1:39:35<1:08:49, 24.73s/it]2022-01-09 13:06:59,754 iteration 3962 : loss : 0.039816, loss_ce: 0.014918
2022-01-09 13:07:01,131 iteration 3963 : loss : 0.046141, loss_ce: 0.019931
2022-01-09 13:07:02,596 iteration 3964 : loss : 0.033414, loss_ce: 0.016326
2022-01-09 13:07:03,931 iteration 3965 : loss : 0.032083, loss_ce: 0.010561
2022-01-09 13:07:05,350 iteration 3966 : loss : 0.044470, loss_ce: 0.016801
2022-01-09 13:07:06,729 iteration 3967 : loss : 0.022854, loss_ce: 0.007906
2022-01-09 13:07:08,011 iteration 3968 : loss : 0.028697, loss_ce: 0.009609
2022-01-09 13:07:09,397 iteration 3969 : loss : 0.030049, loss_ce: 0.008989
2022-01-09 13:07:10,751 iteration 3970 : loss : 0.031579, loss_ce: 0.011957
2022-01-09 13:07:12,027 iteration 3971 : loss : 0.023867, loss_ce: 0.009805
2022-01-09 13:07:13,421 iteration 3972 : loss : 0.032216, loss_ce: 0.008979
2022-01-09 13:07:14,742 iteration 3973 : loss : 0.036237, loss_ce: 0.017947
2022-01-09 13:07:16,028 iteration 3974 : loss : 0.027071, loss_ce: 0.009408
2022-01-09 13:07:17,408 iteration 3975 : loss : 0.025050, loss_ce: 0.009417
2022-01-09 13:07:18,843 iteration 3976 : loss : 0.035267, loss_ce: 0.013799
2022-01-09 13:07:20,273 iteration 3977 : loss : 0.042280, loss_ce: 0.015272
2022-01-09 13:07:21,733 iteration 3978 : loss : 0.042035, loss_ce: 0.016871
 58%|███████████████▊           | 234/400 [1:39:59<1:07:20, 24.34s/it]2022-01-09 13:07:23,212 iteration 3979 : loss : 0.027663, loss_ce: 0.010071
2022-01-09 13:07:24,624 iteration 3980 : loss : 0.039480, loss_ce: 0.013360
2022-01-09 13:07:25,985 iteration 3981 : loss : 0.029932, loss_ce: 0.013022
2022-01-09 13:07:27,270 iteration 3982 : loss : 0.020735, loss_ce: 0.007566
2022-01-09 13:07:28,570 iteration 3983 : loss : 0.024857, loss_ce: 0.010949
2022-01-09 13:07:29,904 iteration 3984 : loss : 0.023814, loss_ce: 0.010523
2022-01-09 13:07:31,292 iteration 3985 : loss : 0.055263, loss_ce: 0.039824
2022-01-09 13:07:32,743 iteration 3986 : loss : 0.047989, loss_ce: 0.019413
2022-01-09 13:07:34,139 iteration 3987 : loss : 0.032457, loss_ce: 0.012529
2022-01-09 13:07:35,513 iteration 3988 : loss : 0.031327, loss_ce: 0.012878
2022-01-09 13:07:36,860 iteration 3989 : loss : 0.032175, loss_ce: 0.008016
2022-01-09 13:07:38,209 iteration 3990 : loss : 0.042224, loss_ce: 0.016980
2022-01-09 13:07:39,635 iteration 3991 : loss : 0.026103, loss_ce: 0.006294
2022-01-09 13:07:41,024 iteration 3992 : loss : 0.028527, loss_ce: 0.009132
2022-01-09 13:07:42,450 iteration 3993 : loss : 0.029805, loss_ce: 0.011309
2022-01-09 13:07:43,806 iteration 3994 : loss : 0.031376, loss_ce: 0.016901
2022-01-09 13:07:43,806 Training Data Eval:
2022-01-09 13:07:50,674   Average segmentation loss on training set: 0.0246
2022-01-09 13:07:50,674 Validation Data Eval:
2022-01-09 13:07:53,052   Average segmentation loss on validation set: 0.1327
2022-01-09 13:07:54,371 iteration 3995 : loss : 0.032120, loss_ce: 0.014824
 59%|███████████████▊           | 235/400 [1:40:32<1:13:47, 26.83s/it]2022-01-09 13:07:55,859 iteration 3996 : loss : 0.024382, loss_ce: 0.007371
2022-01-09 13:07:57,179 iteration 3997 : loss : 0.042408, loss_ce: 0.013077
2022-01-09 13:07:58,570 iteration 3998 : loss : 0.052034, loss_ce: 0.028213
2022-01-09 13:07:59,926 iteration 3999 : loss : 0.027244, loss_ce: 0.006943
2022-01-09 13:08:01,346 iteration 4000 : loss : 0.024442, loss_ce: 0.012138
2022-01-09 13:08:02,658 iteration 4001 : loss : 0.023243, loss_ce: 0.009821
2022-01-09 13:08:04,030 iteration 4002 : loss : 0.026046, loss_ce: 0.011374
2022-01-09 13:08:05,412 iteration 4003 : loss : 0.026828, loss_ce: 0.012908
2022-01-09 13:08:06,820 iteration 4004 : loss : 0.038300, loss_ce: 0.012989
2022-01-09 13:08:08,203 iteration 4005 : loss : 0.027511, loss_ce: 0.013438
2022-01-09 13:08:09,523 iteration 4006 : loss : 0.025849, loss_ce: 0.010704
2022-01-09 13:08:10,939 iteration 4007 : loss : 0.041492, loss_ce: 0.016348
2022-01-09 13:08:12,277 iteration 4008 : loss : 0.028968, loss_ce: 0.012574
2022-01-09 13:08:13,644 iteration 4009 : loss : 0.030505, loss_ce: 0.012895
2022-01-09 13:08:15,074 iteration 4010 : loss : 0.032712, loss_ce: 0.013797
2022-01-09 13:08:16,402 iteration 4011 : loss : 0.030797, loss_ce: 0.009154
2022-01-09 13:08:17,849 iteration 4012 : loss : 0.028756, loss_ce: 0.012500
 59%|███████████████▉           | 236/400 [1:40:55<1:10:35, 25.83s/it]2022-01-09 13:08:19,360 iteration 4013 : loss : 0.037506, loss_ce: 0.012453
2022-01-09 13:08:20,686 iteration 4014 : loss : 0.022707, loss_ce: 0.007395
2022-01-09 13:08:22,105 iteration 4015 : loss : 0.039578, loss_ce: 0.017359
2022-01-09 13:08:23,521 iteration 4016 : loss : 0.022422, loss_ce: 0.008078
2022-01-09 13:08:24,951 iteration 4017 : loss : 0.038065, loss_ce: 0.011883
2022-01-09 13:08:26,291 iteration 4018 : loss : 0.034429, loss_ce: 0.012893
2022-01-09 13:08:27,659 iteration 4019 : loss : 0.024230, loss_ce: 0.012222
2022-01-09 13:08:28,987 iteration 4020 : loss : 0.023213, loss_ce: 0.012482
2022-01-09 13:08:30,331 iteration 4021 : loss : 0.026504, loss_ce: 0.011787
2022-01-09 13:08:31,728 iteration 4022 : loss : 0.037819, loss_ce: 0.020638
2022-01-09 13:08:33,102 iteration 4023 : loss : 0.021275, loss_ce: 0.007100
2022-01-09 13:08:34,418 iteration 4024 : loss : 0.023352, loss_ce: 0.007955
2022-01-09 13:08:35,751 iteration 4025 : loss : 0.025390, loss_ce: 0.008388
2022-01-09 13:08:37,150 iteration 4026 : loss : 0.020857, loss_ce: 0.008796
2022-01-09 13:08:38,550 iteration 4027 : loss : 0.028387, loss_ce: 0.009677
2022-01-09 13:08:39,975 iteration 4028 : loss : 0.033719, loss_ce: 0.010656
2022-01-09 13:08:41,309 iteration 4029 : loss : 0.029617, loss_ce: 0.012337
 59%|███████████████▉           | 237/400 [1:41:19<1:08:13, 25.12s/it]2022-01-09 13:08:42,803 iteration 4030 : loss : 0.045854, loss_ce: 0.019301
2022-01-09 13:08:44,197 iteration 4031 : loss : 0.036573, loss_ce: 0.011144
2022-01-09 13:08:45,558 iteration 4032 : loss : 0.031254, loss_ce: 0.014382
2022-01-09 13:08:46,930 iteration 4033 : loss : 0.035761, loss_ce: 0.011696
2022-01-09 13:08:48,245 iteration 4034 : loss : 0.034844, loss_ce: 0.011102
2022-01-09 13:08:49,597 iteration 4035 : loss : 0.024324, loss_ce: 0.008346
2022-01-09 13:08:51,002 iteration 4036 : loss : 0.035846, loss_ce: 0.012801
2022-01-09 13:08:52,330 iteration 4037 : loss : 0.020855, loss_ce: 0.009941
2022-01-09 13:08:53,765 iteration 4038 : loss : 0.037209, loss_ce: 0.011205
2022-01-09 13:08:55,222 iteration 4039 : loss : 0.023547, loss_ce: 0.008057
2022-01-09 13:08:56,673 iteration 4040 : loss : 0.043523, loss_ce: 0.018684
2022-01-09 13:08:58,107 iteration 4041 : loss : 0.041378, loss_ce: 0.017891
2022-01-09 13:08:59,448 iteration 4042 : loss : 0.024566, loss_ce: 0.008902
2022-01-09 13:09:00,836 iteration 4043 : loss : 0.181984, loss_ce: 0.010577
2022-01-09 13:09:02,177 iteration 4044 : loss : 0.030064, loss_ce: 0.012702
2022-01-09 13:09:03,540 iteration 4045 : loss : 0.025156, loss_ce: 0.012438
2022-01-09 13:09:04,961 iteration 4046 : loss : 0.027694, loss_ce: 0.013325
 60%|████████████████           | 238/400 [1:41:42<1:06:37, 24.68s/it]2022-01-09 13:09:06,476 iteration 4047 : loss : 0.022094, loss_ce: 0.010110
2022-01-09 13:09:07,897 iteration 4048 : loss : 0.026428, loss_ce: 0.010195
2022-01-09 13:09:09,236 iteration 4049 : loss : 0.022760, loss_ce: 0.008517
2022-01-09 13:09:10,638 iteration 4050 : loss : 0.037478, loss_ce: 0.016407
2022-01-09 13:09:12,118 iteration 4051 : loss : 0.085018, loss_ce: 0.013714
2022-01-09 13:09:13,436 iteration 4052 : loss : 0.035592, loss_ce: 0.012878
2022-01-09 13:09:14,721 iteration 4053 : loss : 0.029035, loss_ce: 0.009618
2022-01-09 13:09:16,048 iteration 4054 : loss : 0.034293, loss_ce: 0.009784
2022-01-09 13:09:17,402 iteration 4055 : loss : 0.028763, loss_ce: 0.012740
2022-01-09 13:09:18,792 iteration 4056 : loss : 0.033342, loss_ce: 0.014675
2022-01-09 13:09:20,116 iteration 4057 : loss : 0.027888, loss_ce: 0.007006
2022-01-09 13:09:21,473 iteration 4058 : loss : 0.028595, loss_ce: 0.009910
2022-01-09 13:09:22,938 iteration 4059 : loss : 0.047778, loss_ce: 0.024169
2022-01-09 13:09:24,323 iteration 4060 : loss : 0.045586, loss_ce: 0.020521
2022-01-09 13:09:25,697 iteration 4061 : loss : 0.035247, loss_ce: 0.015002
2022-01-09 13:09:27,101 iteration 4062 : loss : 0.034241, loss_ce: 0.013513
2022-01-09 13:09:28,487 iteration 4063 : loss : 0.047501, loss_ce: 0.013837
 60%|████████████████▏          | 239/400 [1:42:06<1:05:16, 24.33s/it]2022-01-09 13:09:29,891 iteration 4064 : loss : 0.050094, loss_ce: 0.014155
2022-01-09 13:09:31,236 iteration 4065 : loss : 0.019563, loss_ce: 0.006861
2022-01-09 13:09:32,671 iteration 4066 : loss : 0.042953, loss_ce: 0.015720
2022-01-09 13:09:34,011 iteration 4067 : loss : 0.027642, loss_ce: 0.011375
2022-01-09 13:09:35,383 iteration 4068 : loss : 0.024542, loss_ce: 0.009518
2022-01-09 13:09:36,785 iteration 4069 : loss : 0.029779, loss_ce: 0.011429
2022-01-09 13:09:38,178 iteration 4070 : loss : 0.027506, loss_ce: 0.010944
2022-01-09 13:09:39,612 iteration 4071 : loss : 0.038759, loss_ce: 0.014580
2022-01-09 13:09:40,981 iteration 4072 : loss : 0.027753, loss_ce: 0.010144
2022-01-09 13:09:42,318 iteration 4073 : loss : 0.033004, loss_ce: 0.011624
2022-01-09 13:09:43,685 iteration 4074 : loss : 0.021872, loss_ce: 0.009414
2022-01-09 13:09:45,098 iteration 4075 : loss : 0.030736, loss_ce: 0.011793
2022-01-09 13:09:46,431 iteration 4076 : loss : 0.035399, loss_ce: 0.015830
2022-01-09 13:09:47,792 iteration 4077 : loss : 0.021912, loss_ce: 0.007616
2022-01-09 13:09:49,217 iteration 4078 : loss : 0.034917, loss_ce: 0.012760
2022-01-09 13:09:50,620 iteration 4079 : loss : 0.027840, loss_ce: 0.012265
2022-01-09 13:09:50,620 Training Data Eval:
2022-01-09 13:09:57,497   Average segmentation loss on training set: 0.0187
2022-01-09 13:09:57,498 Validation Data Eval:
2022-01-09 13:09:59,879   Average segmentation loss on validation set: 0.1181
2022-01-09 13:10:01,302 iteration 4080 : loss : 0.034860, loss_ce: 0.014146
 60%|████████████████▏          | 240/400 [1:42:39<1:11:40, 26.88s/it]2022-01-09 13:10:02,764 iteration 4081 : loss : 0.043196, loss_ce: 0.013917
2022-01-09 13:10:04,150 iteration 4082 : loss : 0.017734, loss_ce: 0.006786
2022-01-09 13:10:05,477 iteration 4083 : loss : 0.021632, loss_ce: 0.008686
2022-01-09 13:10:06,849 iteration 4084 : loss : 0.024243, loss_ce: 0.007647
2022-01-09 13:10:08,175 iteration 4085 : loss : 0.024699, loss_ce: 0.007375
2022-01-09 13:10:09,662 iteration 4086 : loss : 0.038451, loss_ce: 0.014671
2022-01-09 13:10:11,063 iteration 4087 : loss : 0.057471, loss_ce: 0.010585
2022-01-09 13:10:12,372 iteration 4088 : loss : 0.033626, loss_ce: 0.018542
2022-01-09 13:10:13,741 iteration 4089 : loss : 0.032261, loss_ce: 0.011732
2022-01-09 13:10:15,025 iteration 4090 : loss : 0.032243, loss_ce: 0.009107
2022-01-09 13:10:16,390 iteration 4091 : loss : 0.036534, loss_ce: 0.014866
2022-01-09 13:10:17,772 iteration 4092 : loss : 0.031755, loss_ce: 0.010300
2022-01-09 13:10:19,155 iteration 4093 : loss : 0.087867, loss_ce: 0.034513
2022-01-09 13:10:20,505 iteration 4094 : loss : 0.036454, loss_ce: 0.012873
2022-01-09 13:10:21,888 iteration 4095 : loss : 0.062095, loss_ce: 0.031703
2022-01-09 13:10:23,229 iteration 4096 : loss : 0.025682, loss_ce: 0.013059
2022-01-09 13:10:24,578 iteration 4097 : loss : 0.021660, loss_ce: 0.008261
 60%|████████████████▎          | 241/400 [1:43:02<1:08:21, 25.80s/it]2022-01-09 13:10:26,019 iteration 4098 : loss : 0.027662, loss_ce: 0.011309
2022-01-09 13:10:27,359 iteration 4099 : loss : 0.036363, loss_ce: 0.010287
2022-01-09 13:10:28,734 iteration 4100 : loss : 0.032774, loss_ce: 0.013763
2022-01-09 13:10:30,143 iteration 4101 : loss : 0.035298, loss_ce: 0.012772
2022-01-09 13:10:31,554 iteration 4102 : loss : 0.040633, loss_ce: 0.016983
2022-01-09 13:10:32,955 iteration 4103 : loss : 0.043385, loss_ce: 0.019667
2022-01-09 13:10:34,328 iteration 4104 : loss : 0.043237, loss_ce: 0.020534
2022-01-09 13:10:35,717 iteration 4105 : loss : 0.027291, loss_ce: 0.012767
2022-01-09 13:10:37,076 iteration 4106 : loss : 0.032059, loss_ce: 0.013610
2022-01-09 13:10:38,428 iteration 4107 : loss : 0.022321, loss_ce: 0.008690
2022-01-09 13:10:39,795 iteration 4108 : loss : 0.027394, loss_ce: 0.012812
2022-01-09 13:10:41,109 iteration 4109 : loss : 0.024954, loss_ce: 0.008825
2022-01-09 13:10:42,512 iteration 4110 : loss : 0.037442, loss_ce: 0.013332
2022-01-09 13:10:44,026 iteration 4111 : loss : 0.036972, loss_ce: 0.018419
2022-01-09 13:10:45,371 iteration 4112 : loss : 0.026094, loss_ce: 0.009511
2022-01-09 13:10:46,779 iteration 4113 : loss : 0.046956, loss_ce: 0.014590
2022-01-09 13:10:48,143 iteration 4114 : loss : 0.026195, loss_ce: 0.012796
 60%|████████████████▎          | 242/400 [1:43:25<1:06:09, 25.13s/it]2022-01-09 13:10:49,532 iteration 4115 : loss : 0.026848, loss_ce: 0.013359
2022-01-09 13:10:50,996 iteration 4116 : loss : 0.035705, loss_ce: 0.013465
2022-01-09 13:10:52,313 iteration 4117 : loss : 0.020646, loss_ce: 0.007481
2022-01-09 13:10:53,742 iteration 4118 : loss : 0.030653, loss_ce: 0.013427
2022-01-09 13:10:55,228 iteration 4119 : loss : 0.044671, loss_ce: 0.013781
2022-01-09 13:10:56,626 iteration 4120 : loss : 0.035038, loss_ce: 0.008268
2022-01-09 13:10:57,963 iteration 4121 : loss : 0.029497, loss_ce: 0.008662
2022-01-09 13:10:59,327 iteration 4122 : loss : 0.034348, loss_ce: 0.008739
2022-01-09 13:11:00,646 iteration 4123 : loss : 0.017676, loss_ce: 0.007501
2022-01-09 13:11:02,052 iteration 4124 : loss : 0.034180, loss_ce: 0.017398
2022-01-09 13:11:03,449 iteration 4125 : loss : 0.038942, loss_ce: 0.017135
2022-01-09 13:11:04,841 iteration 4126 : loss : 0.027734, loss_ce: 0.011279
2022-01-09 13:11:06,177 iteration 4127 : loss : 0.018262, loss_ce: 0.008164
2022-01-09 13:11:07,645 iteration 4128 : loss : 0.041715, loss_ce: 0.011681
2022-01-09 13:11:08,964 iteration 4129 : loss : 0.031037, loss_ce: 0.007296
2022-01-09 13:11:10,269 iteration 4130 : loss : 0.025628, loss_ce: 0.007735
2022-01-09 13:11:11,622 iteration 4131 : loss : 0.024762, loss_ce: 0.011077
 61%|████████████████▍          | 243/400 [1:43:49<1:04:27, 24.63s/it]2022-01-09 13:11:13,105 iteration 4132 : loss : 0.034103, loss_ce: 0.011334
2022-01-09 13:11:14,500 iteration 4133 : loss : 0.023637, loss_ce: 0.007467
2022-01-09 13:11:15,908 iteration 4134 : loss : 0.031072, loss_ce: 0.013424
2022-01-09 13:11:17,209 iteration 4135 : loss : 0.027786, loss_ce: 0.012975
2022-01-09 13:11:18,567 iteration 4136 : loss : 0.029976, loss_ce: 0.013812
2022-01-09 13:11:19,874 iteration 4137 : loss : 0.028203, loss_ce: 0.009088
2022-01-09 13:11:21,265 iteration 4138 : loss : 0.031306, loss_ce: 0.015180
2022-01-09 13:11:22,662 iteration 4139 : loss : 0.022150, loss_ce: 0.009440
2022-01-09 13:11:24,055 iteration 4140 : loss : 0.026524, loss_ce: 0.010837
2022-01-09 13:11:25,446 iteration 4141 : loss : 0.027767, loss_ce: 0.009321
2022-01-09 13:11:26,888 iteration 4142 : loss : 0.041887, loss_ce: 0.013664
2022-01-09 13:11:28,175 iteration 4143 : loss : 0.021750, loss_ce: 0.007145
2022-01-09 13:11:29,484 iteration 4144 : loss : 0.024408, loss_ce: 0.006447
2022-01-09 13:11:30,903 iteration 4145 : loss : 0.033353, loss_ce: 0.013702
2022-01-09 13:11:32,245 iteration 4146 : loss : 0.020443, loss_ce: 0.008745
2022-01-09 13:11:33,627 iteration 4147 : loss : 0.023784, loss_ce: 0.008896
2022-01-09 13:11:34,970 iteration 4148 : loss : 0.030195, loss_ce: 0.011254
 61%|████████████████▍          | 244/400 [1:44:12<1:03:02, 24.25s/it]2022-01-09 13:11:36,323 iteration 4149 : loss : 0.023364, loss_ce: 0.008349
2022-01-09 13:11:37,708 iteration 4150 : loss : 0.021659, loss_ce: 0.006583
2022-01-09 13:11:39,122 iteration 4151 : loss : 0.037018, loss_ce: 0.012385
2022-01-09 13:11:40,507 iteration 4152 : loss : 0.028063, loss_ce: 0.010158
2022-01-09 13:11:41,814 iteration 4153 : loss : 0.022427, loss_ce: 0.009295
2022-01-09 13:11:43,193 iteration 4154 : loss : 0.020732, loss_ce: 0.007115
2022-01-09 13:11:44,555 iteration 4155 : loss : 0.023668, loss_ce: 0.007298
2022-01-09 13:11:45,875 iteration 4156 : loss : 0.018606, loss_ce: 0.006440
2022-01-09 13:11:47,313 iteration 4157 : loss : 0.026296, loss_ce: 0.012715
2022-01-09 13:11:48,698 iteration 4158 : loss : 0.021742, loss_ce: 0.008144
2022-01-09 13:11:50,059 iteration 4159 : loss : 0.018569, loss_ce: 0.007688
2022-01-09 13:11:51,567 iteration 4160 : loss : 0.036077, loss_ce: 0.014895
2022-01-09 13:11:53,037 iteration 4161 : loss : 0.025287, loss_ce: 0.008211
2022-01-09 13:11:54,509 iteration 4162 : loss : 0.042659, loss_ce: 0.013188
2022-01-09 13:11:55,956 iteration 4163 : loss : 0.026689, loss_ce: 0.012651
2022-01-09 13:11:57,368 iteration 4164 : loss : 0.024488, loss_ce: 0.010259
2022-01-09 13:11:57,368 Training Data Eval:
2022-01-09 13:12:04,233   Average segmentation loss on training set: 0.0169
2022-01-09 13:12:04,234 Validation Data Eval:
2022-01-09 13:12:06,608   Average segmentation loss on validation set: 0.0891
2022-01-09 13:12:08,007 iteration 4165 : loss : 0.031157, loss_ce: 0.009795
 61%|████████████████▌          | 245/400 [1:44:45<1:09:26, 26.88s/it]2022-01-09 13:12:09,424 iteration 4166 : loss : 0.019833, loss_ce: 0.007639
2022-01-09 13:12:10,725 iteration 4167 : loss : 0.023902, loss_ce: 0.006926
2022-01-09 13:12:12,051 iteration 4168 : loss : 0.021093, loss_ce: 0.008538
2022-01-09 13:12:13,372 iteration 4169 : loss : 0.022813, loss_ce: 0.011118
2022-01-09 13:12:14,844 iteration 4170 : loss : 0.032955, loss_ce: 0.015151
2022-01-09 13:12:16,148 iteration 4171 : loss : 0.019549, loss_ce: 0.006238
2022-01-09 13:12:17,507 iteration 4172 : loss : 0.023981, loss_ce: 0.011028
2022-01-09 13:12:18,904 iteration 4173 : loss : 0.023759, loss_ce: 0.007049
2022-01-09 13:12:20,252 iteration 4174 : loss : 0.025921, loss_ce: 0.007672
2022-01-09 13:12:21,568 iteration 4175 : loss : 0.021168, loss_ce: 0.007886
2022-01-09 13:12:22,919 iteration 4176 : loss : 0.024549, loss_ce: 0.011194
2022-01-09 13:12:24,329 iteration 4177 : loss : 0.020111, loss_ce: 0.005604
2022-01-09 13:12:25,652 iteration 4178 : loss : 0.029372, loss_ce: 0.010024
2022-01-09 13:12:26,995 iteration 4179 : loss : 0.024474, loss_ce: 0.010879
2022-01-09 13:12:28,332 iteration 4180 : loss : 0.022818, loss_ce: 0.006834
2022-01-09 13:12:29,690 iteration 4181 : loss : 0.033315, loss_ce: 0.013017
2022-01-09 13:12:31,051 iteration 4182 : loss : 0.031380, loss_ce: 0.011560
 62%|████████████████▌          | 246/400 [1:45:08<1:06:03, 25.73s/it]2022-01-09 13:12:32,491 iteration 4183 : loss : 0.023275, loss_ce: 0.008324
2022-01-09 13:12:33,892 iteration 4184 : loss : 0.028293, loss_ce: 0.011305
2022-01-09 13:12:35,257 iteration 4185 : loss : 0.026352, loss_ce: 0.011870
2022-01-09 13:12:36,609 iteration 4186 : loss : 0.025664, loss_ce: 0.007216
2022-01-09 13:12:38,018 iteration 4187 : loss : 0.031185, loss_ce: 0.009772
2022-01-09 13:12:39,352 iteration 4188 : loss : 0.028108, loss_ce: 0.008886
2022-01-09 13:12:40,822 iteration 4189 : loss : 0.025808, loss_ce: 0.010335
2022-01-09 13:12:42,251 iteration 4190 : loss : 0.036014, loss_ce: 0.016986
2022-01-09 13:12:43,689 iteration 4191 : loss : 0.031977, loss_ce: 0.014436
2022-01-09 13:12:44,971 iteration 4192 : loss : 0.018277, loss_ce: 0.005632
2022-01-09 13:12:46,356 iteration 4193 : loss : 0.024080, loss_ce: 0.013679
2022-01-09 13:12:47,770 iteration 4194 : loss : 0.027132, loss_ce: 0.009333
2022-01-09 13:12:49,129 iteration 4195 : loss : 0.025943, loss_ce: 0.007726
2022-01-09 13:12:50,532 iteration 4196 : loss : 0.021152, loss_ce: 0.008610
2022-01-09 13:12:51,886 iteration 4197 : loss : 0.032395, loss_ce: 0.011083
2022-01-09 13:12:53,243 iteration 4198 : loss : 0.021457, loss_ce: 0.008250
2022-01-09 13:12:54,553 iteration 4199 : loss : 0.019294, loss_ce: 0.007681
 62%|████████████████▋          | 247/400 [1:45:32<1:03:54, 25.06s/it]2022-01-09 13:12:55,975 iteration 4200 : loss : 0.023891, loss_ce: 0.009230
2022-01-09 13:12:57,374 iteration 4201 : loss : 0.027843, loss_ce: 0.012336
2022-01-09 13:12:58,766 iteration 4202 : loss : 0.021252, loss_ce: 0.009038
2022-01-09 13:13:00,236 iteration 4203 : loss : 0.024628, loss_ce: 0.010631
2022-01-09 13:13:01,596 iteration 4204 : loss : 0.021326, loss_ce: 0.008187
2022-01-09 13:13:02,959 iteration 4205 : loss : 0.024702, loss_ce: 0.010540
2022-01-09 13:13:04,449 iteration 4206 : loss : 0.041402, loss_ce: 0.014850
2022-01-09 13:13:05,863 iteration 4207 : loss : 0.029574, loss_ce: 0.012716
2022-01-09 13:13:07,157 iteration 4208 : loss : 0.017581, loss_ce: 0.006866
2022-01-09 13:13:08,498 iteration 4209 : loss : 0.022394, loss_ce: 0.009555
2022-01-09 13:13:09,843 iteration 4210 : loss : 0.022819, loss_ce: 0.009879
2022-01-09 13:13:11,231 iteration 4211 : loss : 0.023159, loss_ce: 0.009078
2022-01-09 13:13:12,646 iteration 4212 : loss : 0.027180, loss_ce: 0.010755
2022-01-09 13:13:13,987 iteration 4213 : loss : 0.035784, loss_ce: 0.013096
2022-01-09 13:13:15,400 iteration 4214 : loss : 0.032374, loss_ce: 0.009120
2022-01-09 13:13:16,759 iteration 4215 : loss : 0.023230, loss_ce: 0.008056
2022-01-09 13:13:18,121 iteration 4216 : loss : 0.026747, loss_ce: 0.008615
 62%|████████████████▋          | 248/400 [1:45:55<1:02:21, 24.61s/it]2022-01-09 13:13:19,592 iteration 4217 : loss : 0.026801, loss_ce: 0.009471
2022-01-09 13:13:20,945 iteration 4218 : loss : 0.024712, loss_ce: 0.007245
2022-01-09 13:13:22,312 iteration 4219 : loss : 0.026276, loss_ce: 0.011264
2022-01-09 13:13:23,597 iteration 4220 : loss : 0.026312, loss_ce: 0.006638
2022-01-09 13:13:25,051 iteration 4221 : loss : 0.033995, loss_ce: 0.014873
2022-01-09 13:13:26,511 iteration 4222 : loss : 0.024967, loss_ce: 0.007552
2022-01-09 13:13:27,858 iteration 4223 : loss : 0.023767, loss_ce: 0.008020
2022-01-09 13:13:29,284 iteration 4224 : loss : 0.035550, loss_ce: 0.009707
2022-01-09 13:13:30,676 iteration 4225 : loss : 0.028767, loss_ce: 0.008835
2022-01-09 13:13:32,099 iteration 4226 : loss : 0.024796, loss_ce: 0.011390
2022-01-09 13:13:33,496 iteration 4227 : loss : 0.021605, loss_ce: 0.008334
2022-01-09 13:13:34,824 iteration 4228 : loss : 0.022701, loss_ce: 0.010802
2022-01-09 13:13:36,189 iteration 4229 : loss : 0.028468, loss_ce: 0.011221
2022-01-09 13:13:37,547 iteration 4230 : loss : 0.028803, loss_ce: 0.012885
2022-01-09 13:13:38,959 iteration 4231 : loss : 0.022792, loss_ce: 0.008980
2022-01-09 13:13:40,371 iteration 4232 : loss : 0.035310, loss_ce: 0.011214
2022-01-09 13:13:41,741 iteration 4233 : loss : 0.025935, loss_ce: 0.013532
 62%|████████████████▊          | 249/400 [1:46:19<1:01:11, 24.32s/it]2022-01-09 13:13:43,089 iteration 4234 : loss : 0.032427, loss_ce: 0.007536
2022-01-09 13:13:44,395 iteration 4235 : loss : 0.018466, loss_ce: 0.006382
2022-01-09 13:13:45,789 iteration 4236 : loss : 0.021805, loss_ce: 0.008317
2022-01-09 13:13:47,116 iteration 4237 : loss : 0.031251, loss_ce: 0.013283
2022-01-09 13:13:48,474 iteration 4238 : loss : 0.028201, loss_ce: 0.014173
2022-01-09 13:13:49,969 iteration 4239 : loss : 0.040970, loss_ce: 0.019088
2022-01-09 13:13:51,354 iteration 4240 : loss : 0.032900, loss_ce: 0.011584
2022-01-09 13:13:52,641 iteration 4241 : loss : 0.019680, loss_ce: 0.008895
2022-01-09 13:13:54,015 iteration 4242 : loss : 0.027963, loss_ce: 0.008237
2022-01-09 13:13:55,358 iteration 4243 : loss : 0.023359, loss_ce: 0.009284
2022-01-09 13:13:56,655 iteration 4244 : loss : 0.022354, loss_ce: 0.010430
2022-01-09 13:13:58,069 iteration 4245 : loss : 0.028155, loss_ce: 0.012162
2022-01-09 13:13:59,515 iteration 4246 : loss : 0.034103, loss_ce: 0.008051
2022-01-09 13:14:00,921 iteration 4247 : loss : 0.027798, loss_ce: 0.012160
2022-01-09 13:14:02,241 iteration 4248 : loss : 0.027238, loss_ce: 0.010459
2022-01-09 13:14:03,672 iteration 4249 : loss : 0.030110, loss_ce: 0.010046
2022-01-09 13:14:03,672 Training Data Eval:
2022-01-09 13:14:10,554   Average segmentation loss on training set: 0.0155
2022-01-09 13:14:10,555 Validation Data Eval:
2022-01-09 13:14:12,933   Average segmentation loss on validation set: 0.0933
2022-01-09 13:14:14,295 iteration 4250 : loss : 0.025616, loss_ce: 0.008450
 62%|████████████████▉          | 250/400 [1:46:51<1:06:57, 26.78s/it]2022-01-09 13:14:15,778 iteration 4251 : loss : 0.044373, loss_ce: 0.012382
2022-01-09 13:14:17,196 iteration 4252 : loss : 0.023081, loss_ce: 0.011030
2022-01-09 13:14:18,563 iteration 4253 : loss : 0.031228, loss_ce: 0.010387
2022-01-09 13:14:19,975 iteration 4254 : loss : 0.020530, loss_ce: 0.007248
2022-01-09 13:14:21,438 iteration 4255 : loss : 0.020703, loss_ce: 0.007677
2022-01-09 13:14:22,725 iteration 4256 : loss : 0.023994, loss_ce: 0.010604
2022-01-09 13:14:24,092 iteration 4257 : loss : 0.025721, loss_ce: 0.009690
2022-01-09 13:14:25,393 iteration 4258 : loss : 0.034955, loss_ce: 0.009920
2022-01-09 13:14:26,782 iteration 4259 : loss : 0.026218, loss_ce: 0.011141
2022-01-09 13:14:28,173 iteration 4260 : loss : 0.026600, loss_ce: 0.008608
2022-01-09 13:14:29,566 iteration 4261 : loss : 0.023946, loss_ce: 0.008697
2022-01-09 13:14:30,902 iteration 4262 : loss : 0.021272, loss_ce: 0.009048
2022-01-09 13:14:32,293 iteration 4263 : loss : 0.021521, loss_ce: 0.009194
2022-01-09 13:14:33,719 iteration 4264 : loss : 0.026162, loss_ce: 0.011299
2022-01-09 13:14:35,082 iteration 4265 : loss : 0.037383, loss_ce: 0.010391
2022-01-09 13:14:36,479 iteration 4266 : loss : 0.030911, loss_ce: 0.009733
2022-01-09 13:14:37,903 iteration 4267 : loss : 0.025503, loss_ce: 0.009787
 63%|████████████████▉          | 251/400 [1:47:15<1:04:09, 25.83s/it]2022-01-09 13:14:39,337 iteration 4268 : loss : 0.018857, loss_ce: 0.004103
2022-01-09 13:14:40,699 iteration 4269 : loss : 0.028588, loss_ce: 0.010172
2022-01-09 13:14:42,036 iteration 4270 : loss : 0.023065, loss_ce: 0.011943
2022-01-09 13:14:43,339 iteration 4271 : loss : 0.021295, loss_ce: 0.008319
2022-01-09 13:14:44,701 iteration 4272 : loss : 0.022141, loss_ce: 0.010480
2022-01-09 13:14:46,013 iteration 4273 : loss : 0.016858, loss_ce: 0.007017
2022-01-09 13:14:47,468 iteration 4274 : loss : 0.031290, loss_ce: 0.014187
2022-01-09 13:14:48,817 iteration 4275 : loss : 0.019397, loss_ce: 0.006646
2022-01-09 13:14:50,235 iteration 4276 : loss : 0.031269, loss_ce: 0.008276
2022-01-09 13:14:51,657 iteration 4277 : loss : 0.037474, loss_ce: 0.011585
2022-01-09 13:14:53,073 iteration 4278 : loss : 0.037121, loss_ce: 0.011641
2022-01-09 13:14:54,431 iteration 4279 : loss : 0.028959, loss_ce: 0.014737
2022-01-09 13:14:55,815 iteration 4280 : loss : 0.037140, loss_ce: 0.010618
2022-01-09 13:14:57,197 iteration 4281 : loss : 0.036524, loss_ce: 0.012406
2022-01-09 13:14:58,512 iteration 4282 : loss : 0.022760, loss_ce: 0.008746
2022-01-09 13:14:59,872 iteration 4283 : loss : 0.033229, loss_ce: 0.011093
2022-01-09 13:15:01,213 iteration 4284 : loss : 0.033893, loss_ce: 0.014493
 63%|█████████████████          | 252/400 [1:47:38<1:01:51, 25.08s/it]2022-01-09 13:15:02,585 iteration 4285 : loss : 0.025028, loss_ce: 0.007522
2022-01-09 13:15:03,998 iteration 4286 : loss : 0.033072, loss_ce: 0.014973
2022-01-09 13:15:05,326 iteration 4287 : loss : 0.021295, loss_ce: 0.009558
2022-01-09 13:15:06,653 iteration 4288 : loss : 0.028423, loss_ce: 0.013817
2022-01-09 13:15:08,029 iteration 4289 : loss : 0.026925, loss_ce: 0.010230
2022-01-09 13:15:09,390 iteration 4290 : loss : 0.033963, loss_ce: 0.009808
2022-01-09 13:15:10,786 iteration 4291 : loss : 0.030580, loss_ce: 0.011390
2022-01-09 13:15:12,179 iteration 4292 : loss : 0.029271, loss_ce: 0.013883
2022-01-09 13:15:13,552 iteration 4293 : loss : 0.037013, loss_ce: 0.011458
2022-01-09 13:15:14,898 iteration 4294 : loss : 0.032964, loss_ce: 0.009341
2022-01-09 13:15:16,367 iteration 4295 : loss : 0.030550, loss_ce: 0.010536
2022-01-09 13:15:17,750 iteration 4296 : loss : 0.025649, loss_ce: 0.008384
2022-01-09 13:15:19,145 iteration 4297 : loss : 0.032179, loss_ce: 0.011828
2022-01-09 13:15:20,490 iteration 4298 : loss : 0.026045, loss_ce: 0.011846
2022-01-09 13:15:21,907 iteration 4299 : loss : 0.022772, loss_ce: 0.010287
2022-01-09 13:15:23,353 iteration 4300 : loss : 0.033939, loss_ce: 0.016549
2022-01-09 13:15:24,793 iteration 4301 : loss : 0.023134, loss_ce: 0.009560
 63%|█████████████████          | 253/400 [1:48:02<1:00:20, 24.63s/it]2022-01-09 13:15:26,267 iteration 4302 : loss : 0.028996, loss_ce: 0.011941
2022-01-09 13:15:27,703 iteration 4303 : loss : 0.056785, loss_ce: 0.013207
2022-01-09 13:15:29,041 iteration 4304 : loss : 0.023177, loss_ce: 0.009083
2022-01-09 13:15:30,393 iteration 4305 : loss : 0.023029, loss_ce: 0.009571
2022-01-09 13:15:31,745 iteration 4306 : loss : 0.021103, loss_ce: 0.006685
2022-01-09 13:15:33,062 iteration 4307 : loss : 0.024288, loss_ce: 0.012472
2022-01-09 13:15:34,462 iteration 4308 : loss : 0.029854, loss_ce: 0.009950
2022-01-09 13:15:35,898 iteration 4309 : loss : 0.040074, loss_ce: 0.017133
2022-01-09 13:15:37,276 iteration 4310 : loss : 0.020113, loss_ce: 0.008709
2022-01-09 13:15:38,615 iteration 4311 : loss : 0.025749, loss_ce: 0.010119
2022-01-09 13:15:39,933 iteration 4312 : loss : 0.031082, loss_ce: 0.010314
2022-01-09 13:15:41,266 iteration 4313 : loss : 0.029115, loss_ce: 0.013425
2022-01-09 13:15:42,604 iteration 4314 : loss : 0.024070, loss_ce: 0.009152
2022-01-09 13:15:44,003 iteration 4315 : loss : 0.022652, loss_ce: 0.007840
2022-01-09 13:15:45,342 iteration 4316 : loss : 0.028607, loss_ce: 0.009067
2022-01-09 13:15:46,766 iteration 4317 : loss : 0.026053, loss_ce: 0.009539
2022-01-09 13:15:48,089 iteration 4318 : loss : 0.022604, loss_ce: 0.008810
 64%|██████████████████▍          | 254/400 [1:48:25<58:57, 24.23s/it]2022-01-09 13:15:49,557 iteration 4319 : loss : 0.024007, loss_ce: 0.007796
2022-01-09 13:15:50,962 iteration 4320 : loss : 0.041404, loss_ce: 0.017712
2022-01-09 13:15:52,245 iteration 4321 : loss : 0.025197, loss_ce: 0.007209
2022-01-09 13:15:53,618 iteration 4322 : loss : 0.025925, loss_ce: 0.008343
2022-01-09 13:15:54,936 iteration 4323 : loss : 0.026867, loss_ce: 0.013908
2022-01-09 13:15:56,415 iteration 4324 : loss : 0.035950, loss_ce: 0.015265
2022-01-09 13:15:57,795 iteration 4325 : loss : 0.025932, loss_ce: 0.010672
2022-01-09 13:15:59,148 iteration 4326 : loss : 0.029915, loss_ce: 0.014204
2022-01-09 13:16:00,555 iteration 4327 : loss : 0.029104, loss_ce: 0.010252
2022-01-09 13:16:01,897 iteration 4328 : loss : 0.029822, loss_ce: 0.010398
2022-01-09 13:16:03,329 iteration 4329 : loss : 0.046000, loss_ce: 0.012469
2022-01-09 13:16:04,739 iteration 4330 : loss : 0.036358, loss_ce: 0.017760
2022-01-09 13:16:06,199 iteration 4331 : loss : 0.040750, loss_ce: 0.011885
2022-01-09 13:16:07,617 iteration 4332 : loss : 0.036750, loss_ce: 0.013171
2022-01-09 13:16:09,062 iteration 4333 : loss : 0.037583, loss_ce: 0.017492
2022-01-09 13:16:10,446 iteration 4334 : loss : 0.023852, loss_ce: 0.011191
2022-01-09 13:16:10,446 Training Data Eval:
2022-01-09 13:16:17,338   Average segmentation loss on training set: 0.0166
2022-01-09 13:16:17,339 Validation Data Eval:
2022-01-09 13:16:19,724   Average segmentation loss on validation set: 0.0840
2022-01-09 13:16:21,138 iteration 4335 : loss : 0.059679, loss_ce: 0.018555
 64%|█████████████████▏         | 255/400 [1:48:58<1:04:56, 26.87s/it]2022-01-09 13:16:22,463 iteration 4336 : loss : 0.017633, loss_ce: 0.008686
2022-01-09 13:16:23,907 iteration 4337 : loss : 0.025563, loss_ce: 0.011244
2022-01-09 13:16:25,279 iteration 4338 : loss : 0.022421, loss_ce: 0.008195
2022-01-09 13:16:26,632 iteration 4339 : loss : 0.022243, loss_ce: 0.007993
2022-01-09 13:16:28,038 iteration 4340 : loss : 0.028752, loss_ce: 0.012172
2022-01-09 13:16:29,359 iteration 4341 : loss : 0.023243, loss_ce: 0.007240
2022-01-09 13:16:30,773 iteration 4342 : loss : 0.034082, loss_ce: 0.009888
2022-01-09 13:16:32,184 iteration 4343 : loss : 0.024985, loss_ce: 0.008632
2022-01-09 13:16:33,585 iteration 4344 : loss : 0.029083, loss_ce: 0.015520
2022-01-09 13:16:35,051 iteration 4345 : loss : 0.041521, loss_ce: 0.020413
2022-01-09 13:16:36,365 iteration 4346 : loss : 0.027607, loss_ce: 0.009467
2022-01-09 13:16:37,741 iteration 4347 : loss : 0.026308, loss_ce: 0.011096
2022-01-09 13:16:39,063 iteration 4348 : loss : 0.019967, loss_ce: 0.010150
2022-01-09 13:16:40,344 iteration 4349 : loss : 0.021720, loss_ce: 0.006451
2022-01-09 13:16:41,625 iteration 4350 : loss : 0.025374, loss_ce: 0.009348
2022-01-09 13:16:43,012 iteration 4351 : loss : 0.027642, loss_ce: 0.007995
2022-01-09 13:16:44,380 iteration 4352 : loss : 0.023599, loss_ce: 0.009988
 64%|█████████████████▎         | 256/400 [1:49:22<1:01:52, 25.78s/it]2022-01-09 13:16:45,760 iteration 4353 : loss : 0.021965, loss_ce: 0.007033
2022-01-09 13:16:47,208 iteration 4354 : loss : 0.030681, loss_ce: 0.015526
2022-01-09 13:16:48,627 iteration 4355 : loss : 0.018522, loss_ce: 0.007736
2022-01-09 13:16:50,088 iteration 4356 : loss : 0.020080, loss_ce: 0.005783
2022-01-09 13:16:51,509 iteration 4357 : loss : 0.022382, loss_ce: 0.010569
2022-01-09 13:16:52,948 iteration 4358 : loss : 0.030392, loss_ce: 0.012002
2022-01-09 13:16:54,346 iteration 4359 : loss : 0.020558, loss_ce: 0.006551
2022-01-09 13:16:55,711 iteration 4360 : loss : 0.031918, loss_ce: 0.016141
2022-01-09 13:16:57,138 iteration 4361 : loss : 0.025103, loss_ce: 0.009102
2022-01-09 13:16:58,513 iteration 4362 : loss : 0.021418, loss_ce: 0.010165
2022-01-09 13:16:59,890 iteration 4363 : loss : 0.018921, loss_ce: 0.008237
2022-01-09 13:17:01,289 iteration 4364 : loss : 0.026894, loss_ce: 0.012473
2022-01-09 13:17:02,757 iteration 4365 : loss : 0.032697, loss_ce: 0.014290
2022-01-09 13:17:04,085 iteration 4366 : loss : 0.019035, loss_ce: 0.005725
2022-01-09 13:17:05,357 iteration 4367 : loss : 0.021987, loss_ce: 0.009454
2022-01-09 13:17:06,663 iteration 4368 : loss : 0.025946, loss_ce: 0.008576
2022-01-09 13:17:08,041 iteration 4369 : loss : 0.021894, loss_ce: 0.008104
 64%|██████████████████▋          | 257/400 [1:49:45<59:56, 25.15s/it]2022-01-09 13:17:09,469 iteration 4370 : loss : 0.020863, loss_ce: 0.006291
2022-01-09 13:17:10,862 iteration 4371 : loss : 0.026751, loss_ce: 0.012868
2022-01-09 13:17:12,265 iteration 4372 : loss : 0.025832, loss_ce: 0.010431
2022-01-09 13:17:13,663 iteration 4373 : loss : 0.020830, loss_ce: 0.010993
2022-01-09 13:17:15,069 iteration 4374 : loss : 0.026105, loss_ce: 0.010218
2022-01-09 13:17:16,470 iteration 4375 : loss : 0.025578, loss_ce: 0.011438
2022-01-09 13:17:17,821 iteration 4376 : loss : 0.025454, loss_ce: 0.010666
2022-01-09 13:17:19,172 iteration 4377 : loss : 0.031668, loss_ce: 0.009072
2022-01-09 13:17:20,482 iteration 4378 : loss : 0.021583, loss_ce: 0.008199
2022-01-09 13:17:21,802 iteration 4379 : loss : 0.024290, loss_ce: 0.008825
2022-01-09 13:17:23,095 iteration 4380 : loss : 0.020948, loss_ce: 0.006569
2022-01-09 13:17:24,488 iteration 4381 : loss : 0.028159, loss_ce: 0.010278
2022-01-09 13:17:25,861 iteration 4382 : loss : 0.033264, loss_ce: 0.006784
2022-01-09 13:17:27,205 iteration 4383 : loss : 0.024180, loss_ce: 0.007068
2022-01-09 13:17:28,636 iteration 4384 : loss : 0.028268, loss_ce: 0.011293
2022-01-09 13:17:30,061 iteration 4385 : loss : 0.026259, loss_ce: 0.008606
2022-01-09 13:17:31,396 iteration 4386 : loss : 0.021836, loss_ce: 0.009999
 64%|██████████████████▋          | 258/400 [1:50:09<58:15, 24.61s/it]2022-01-09 13:17:32,809 iteration 4387 : loss : 0.021155, loss_ce: 0.007259
2022-01-09 13:17:34,181 iteration 4388 : loss : 0.025023, loss_ce: 0.010916
2022-01-09 13:17:35,561 iteration 4389 : loss : 0.039268, loss_ce: 0.015220
2022-01-09 13:17:36,875 iteration 4390 : loss : 0.019514, loss_ce: 0.006686
2022-01-09 13:17:38,261 iteration 4391 : loss : 0.036482, loss_ce: 0.010069
2022-01-09 13:17:39,629 iteration 4392 : loss : 0.025738, loss_ce: 0.007054
2022-01-09 13:17:41,066 iteration 4393 : loss : 0.028464, loss_ce: 0.011885
2022-01-09 13:17:42,476 iteration 4394 : loss : 0.028976, loss_ce: 0.012888
2022-01-09 13:17:43,946 iteration 4395 : loss : 0.029667, loss_ce: 0.010930
2022-01-09 13:17:45,289 iteration 4396 : loss : 0.022100, loss_ce: 0.007085
2022-01-09 13:17:46,765 iteration 4397 : loss : 0.027034, loss_ce: 0.012365
2022-01-09 13:17:48,095 iteration 4398 : loss : 0.021421, loss_ce: 0.008586
2022-01-09 13:17:49,515 iteration 4399 : loss : 0.030530, loss_ce: 0.015124
2022-01-09 13:17:50,904 iteration 4400 : loss : 0.034271, loss_ce: 0.011710
2022-01-09 13:17:52,417 iteration 4401 : loss : 0.034782, loss_ce: 0.013450
2022-01-09 13:17:53,814 iteration 4402 : loss : 0.028687, loss_ce: 0.008676
2022-01-09 13:17:55,210 iteration 4403 : loss : 0.023090, loss_ce: 0.007743
 65%|██████████████████▊          | 259/400 [1:50:32<57:16, 24.37s/it]2022-01-09 13:17:56,610 iteration 4404 : loss : 0.028685, loss_ce: 0.011364
2022-01-09 13:17:58,026 iteration 4405 : loss : 0.041222, loss_ce: 0.016411
2022-01-09 13:17:59,424 iteration 4406 : loss : 0.042711, loss_ce: 0.018620
2022-01-09 13:18:00,865 iteration 4407 : loss : 0.031883, loss_ce: 0.012483
2022-01-09 13:18:02,225 iteration 4408 : loss : 0.018501, loss_ce: 0.007344
2022-01-09 13:18:03,630 iteration 4409 : loss : 0.028846, loss_ce: 0.010182
2022-01-09 13:18:05,148 iteration 4410 : loss : 0.028303, loss_ce: 0.009952
2022-01-09 13:18:06,578 iteration 4411 : loss : 0.030388, loss_ce: 0.012147
2022-01-09 13:18:07,959 iteration 4412 : loss : 0.026609, loss_ce: 0.010957
2022-01-09 13:18:09,364 iteration 4413 : loss : 0.023696, loss_ce: 0.007899
2022-01-09 13:18:10,683 iteration 4414 : loss : 0.025395, loss_ce: 0.007847
2022-01-09 13:18:12,092 iteration 4415 : loss : 0.031589, loss_ce: 0.012637
2022-01-09 13:18:13,528 iteration 4416 : loss : 0.040762, loss_ce: 0.014522
2022-01-09 13:18:14,902 iteration 4417 : loss : 0.027277, loss_ce: 0.009196
2022-01-09 13:18:16,292 iteration 4418 : loss : 0.024531, loss_ce: 0.011400
2022-01-09 13:18:17,723 iteration 4419 : loss : 0.024381, loss_ce: 0.007114
2022-01-09 13:18:17,723 Training Data Eval:
2022-01-09 13:18:24,601   Average segmentation loss on training set: 0.0156
2022-01-09 13:18:24,602 Validation Data Eval:
2022-01-09 13:18:26,970   Average segmentation loss on validation set: 0.0995
2022-01-09 13:18:28,290 iteration 4420 : loss : 0.020699, loss_ce: 0.008936
 65%|█████████████████▌         | 260/400 [1:51:05<1:02:57, 26.98s/it]2022-01-09 13:18:29,692 iteration 4421 : loss : 0.024022, loss_ce: 0.008494
2022-01-09 13:18:31,089 iteration 4422 : loss : 0.034048, loss_ce: 0.016646
2022-01-09 13:18:32,522 iteration 4423 : loss : 0.030902, loss_ce: 0.013419
2022-01-09 13:18:33,844 iteration 4424 : loss : 0.019064, loss_ce: 0.006074
2022-01-09 13:18:35,160 iteration 4425 : loss : 0.020958, loss_ce: 0.006960
2022-01-09 13:18:36,590 iteration 4426 : loss : 0.020736, loss_ce: 0.008374
2022-01-09 13:18:37,909 iteration 4427 : loss : 0.019735, loss_ce: 0.008716
2022-01-09 13:18:39,187 iteration 4428 : loss : 0.017287, loss_ce: 0.007686
2022-01-09 13:18:40,486 iteration 4429 : loss : 0.020514, loss_ce: 0.007120
2022-01-09 13:18:41,792 iteration 4430 : loss : 0.018427, loss_ce: 0.007895
2022-01-09 13:18:43,176 iteration 4431 : loss : 0.028435, loss_ce: 0.012678
2022-01-09 13:18:44,589 iteration 4432 : loss : 0.036972, loss_ce: 0.020028
2022-01-09 13:18:45,935 iteration 4433 : loss : 0.026526, loss_ce: 0.007614
2022-01-09 13:18:47,245 iteration 4434 : loss : 0.015715, loss_ce: 0.006994
2022-01-09 13:18:48,611 iteration 4435 : loss : 0.039476, loss_ce: 0.010168
2022-01-09 13:18:49,971 iteration 4436 : loss : 0.022806, loss_ce: 0.010021
2022-01-09 13:18:51,398 iteration 4437 : loss : 0.025813, loss_ce: 0.009063
 65%|██████████████████▉          | 261/400 [1:51:29<59:49, 25.82s/it]2022-01-09 13:18:52,887 iteration 4438 : loss : 0.043330, loss_ce: 0.011997
2022-01-09 13:18:54,290 iteration 4439 : loss : 0.019605, loss_ce: 0.006886
2022-01-09 13:18:55,671 iteration 4440 : loss : 0.022715, loss_ce: 0.007244
2022-01-09 13:18:57,155 iteration 4441 : loss : 0.027497, loss_ce: 0.010483
2022-01-09 13:18:58,590 iteration 4442 : loss : 0.024030, loss_ce: 0.012554
2022-01-09 13:18:59,995 iteration 4443 : loss : 0.030382, loss_ce: 0.010504
2022-01-09 13:19:01,304 iteration 4444 : loss : 0.029321, loss_ce: 0.008727
2022-01-09 13:19:02,660 iteration 4445 : loss : 0.048959, loss_ce: 0.021620
2022-01-09 13:19:04,066 iteration 4446 : loss : 0.023440, loss_ce: 0.009730
2022-01-09 13:19:05,418 iteration 4447 : loss : 0.021416, loss_ce: 0.007349
2022-01-09 13:19:06,819 iteration 4448 : loss : 0.036856, loss_ce: 0.013123
2022-01-09 13:19:08,201 iteration 4449 : loss : 0.036157, loss_ce: 0.014930
2022-01-09 13:19:09,632 iteration 4450 : loss : 0.028051, loss_ce: 0.008495
2022-01-09 13:19:10,978 iteration 4451 : loss : 0.029689, loss_ce: 0.011831
2022-01-09 13:19:12,368 iteration 4452 : loss : 0.023959, loss_ce: 0.010260
2022-01-09 13:19:13,720 iteration 4453 : loss : 0.020133, loss_ce: 0.007457
2022-01-09 13:19:15,071 iteration 4454 : loss : 0.024746, loss_ce: 0.010043
 66%|██████████████████▉          | 262/400 [1:51:52<57:54, 25.18s/it]2022-01-09 13:19:16,523 iteration 4455 : loss : 0.027599, loss_ce: 0.007808
2022-01-09 13:19:17,821 iteration 4456 : loss : 0.020024, loss_ce: 0.007359
2022-01-09 13:19:19,211 iteration 4457 : loss : 0.030507, loss_ce: 0.014215
2022-01-09 13:19:20,677 iteration 4458 : loss : 0.033171, loss_ce: 0.014302
2022-01-09 13:19:22,212 iteration 4459 : loss : 0.043925, loss_ce: 0.018222
2022-01-09 13:19:23,552 iteration 4460 : loss : 0.022221, loss_ce: 0.008284
2022-01-09 13:19:24,937 iteration 4461 : loss : 0.023721, loss_ce: 0.007197
2022-01-09 13:19:26,359 iteration 4462 : loss : 0.040530, loss_ce: 0.020143
2022-01-09 13:19:27,724 iteration 4463 : loss : 0.039145, loss_ce: 0.015312
2022-01-09 13:19:29,072 iteration 4464 : loss : 0.026897, loss_ce: 0.009157
2022-01-09 13:19:30,372 iteration 4465 : loss : 0.023567, loss_ce: 0.005089
2022-01-09 13:19:31,697 iteration 4466 : loss : 0.024625, loss_ce: 0.007824
2022-01-09 13:19:33,068 iteration 4467 : loss : 0.052528, loss_ce: 0.022992
2022-01-09 13:19:34,474 iteration 4468 : loss : 0.040601, loss_ce: 0.017338
2022-01-09 13:19:35,922 iteration 4469 : loss : 0.035907, loss_ce: 0.017332
2022-01-09 13:19:37,346 iteration 4470 : loss : 0.029906, loss_ce: 0.012962
2022-01-09 13:19:38,799 iteration 4471 : loss : 0.031180, loss_ce: 0.010436
 66%|███████████████████          | 263/400 [1:52:16<56:29, 24.74s/it]2022-01-09 13:19:40,198 iteration 4472 : loss : 0.020304, loss_ce: 0.008389
2022-01-09 13:19:41,532 iteration 4473 : loss : 0.022715, loss_ce: 0.009279
2022-01-09 13:19:42,914 iteration 4474 : loss : 0.027186, loss_ce: 0.012746
2022-01-09 13:19:44,281 iteration 4475 : loss : 0.026610, loss_ce: 0.007846
2022-01-09 13:19:45,623 iteration 4476 : loss : 0.029171, loss_ce: 0.008968
2022-01-09 13:19:46,977 iteration 4477 : loss : 0.032455, loss_ce: 0.012405
2022-01-09 13:19:48,461 iteration 4478 : loss : 0.027490, loss_ce: 0.013401
2022-01-09 13:19:49,837 iteration 4479 : loss : 0.046055, loss_ce: 0.013660
2022-01-09 13:19:51,234 iteration 4480 : loss : 0.023675, loss_ce: 0.008282
2022-01-09 13:19:52,563 iteration 4481 : loss : 0.024113, loss_ce: 0.009006
2022-01-09 13:19:53,882 iteration 4482 : loss : 0.030087, loss_ce: 0.009630
2022-01-09 13:19:55,222 iteration 4483 : loss : 0.029049, loss_ce: 0.010308
2022-01-09 13:19:56,593 iteration 4484 : loss : 0.023852, loss_ce: 0.007912
2022-01-09 13:19:57,895 iteration 4485 : loss : 0.020023, loss_ce: 0.007779
2022-01-09 13:19:59,320 iteration 4486 : loss : 0.023991, loss_ce: 0.008477
2022-01-09 13:20:00,725 iteration 4487 : loss : 0.031536, loss_ce: 0.012452
2022-01-09 13:20:02,051 iteration 4488 : loss : 0.021991, loss_ce: 0.008760
 66%|███████████████████▏         | 264/400 [1:52:39<55:03, 24.29s/it]2022-01-09 13:20:03,518 iteration 4489 : loss : 0.022016, loss_ce: 0.009931
2022-01-09 13:20:04,886 iteration 4490 : loss : 0.027315, loss_ce: 0.011328
2022-01-09 13:20:06,361 iteration 4491 : loss : 0.028550, loss_ce: 0.010430
2022-01-09 13:20:07,711 iteration 4492 : loss : 0.047999, loss_ce: 0.013346
2022-01-09 13:20:09,168 iteration 4493 : loss : 0.032527, loss_ce: 0.019925
2022-01-09 13:20:10,511 iteration 4494 : loss : 0.034769, loss_ce: 0.014471
2022-01-09 13:20:11,894 iteration 4495 : loss : 0.031025, loss_ce: 0.007618
2022-01-09 13:20:13,286 iteration 4496 : loss : 0.029341, loss_ce: 0.009205
2022-01-09 13:20:14,558 iteration 4497 : loss : 0.023665, loss_ce: 0.010615
2022-01-09 13:20:15,929 iteration 4498 : loss : 0.020435, loss_ce: 0.006692
2022-01-09 13:20:17,266 iteration 4499 : loss : 0.028739, loss_ce: 0.012913
2022-01-09 13:20:18,635 iteration 4500 : loss : 0.028181, loss_ce: 0.010157
2022-01-09 13:20:19,975 iteration 4501 : loss : 0.021188, loss_ce: 0.009103
2022-01-09 13:20:21,328 iteration 4502 : loss : 0.032867, loss_ce: 0.012597
2022-01-09 13:20:22,775 iteration 4503 : loss : 0.029454, loss_ce: 0.010528
2022-01-09 13:20:24,062 iteration 4504 : loss : 0.030858, loss_ce: 0.013578
2022-01-09 13:20:24,062 Training Data Eval:
2022-01-09 13:20:30,924   Average segmentation loss on training set: 0.0221
2022-01-09 13:20:30,924 Validation Data Eval:
2022-01-09 13:20:33,305   Average segmentation loss on validation set: 0.1334
2022-01-09 13:20:34,678 iteration 4505 : loss : 0.022630, loss_ce: 0.008597
 66%|█████████████████▉         | 265/400 [1:53:12<1:00:17, 26.80s/it]2022-01-09 13:20:36,138 iteration 4506 : loss : 0.044505, loss_ce: 0.017807
2022-01-09 13:20:37,576 iteration 4507 : loss : 0.023607, loss_ce: 0.009095
2022-01-09 13:20:38,958 iteration 4508 : loss : 0.026836, loss_ce: 0.008311
2022-01-09 13:20:40,271 iteration 4509 : loss : 0.034759, loss_ce: 0.012191
2022-01-09 13:20:41,645 iteration 4510 : loss : 0.021544, loss_ce: 0.007398
2022-01-09 13:20:43,103 iteration 4511 : loss : 0.028907, loss_ce: 0.013280
2022-01-09 13:20:44,393 iteration 4512 : loss : 0.025337, loss_ce: 0.008571
2022-01-09 13:20:45,742 iteration 4513 : loss : 0.026525, loss_ce: 0.009418
2022-01-09 13:20:47,075 iteration 4514 : loss : 0.024599, loss_ce: 0.009559
2022-01-09 13:20:48,435 iteration 4515 : loss : 0.022733, loss_ce: 0.010277
2022-01-09 13:20:49,817 iteration 4516 : loss : 0.023325, loss_ce: 0.010157
2022-01-09 13:20:51,221 iteration 4517 : loss : 0.027564, loss_ce: 0.009796
2022-01-09 13:20:52,570 iteration 4518 : loss : 0.048919, loss_ce: 0.016647
2022-01-09 13:20:53,872 iteration 4519 : loss : 0.044918, loss_ce: 0.012369
2022-01-09 13:20:55,245 iteration 4520 : loss : 0.025931, loss_ce: 0.007667
2022-01-09 13:20:56,636 iteration 4521 : loss : 0.027076, loss_ce: 0.009972
2022-01-09 13:20:58,067 iteration 4522 : loss : 0.034339, loss_ce: 0.009976
 66%|███████████████████▎         | 266/400 [1:53:35<57:33, 25.77s/it]2022-01-09 13:20:59,573 iteration 4523 : loss : 0.049114, loss_ce: 0.023777
2022-01-09 13:21:00,966 iteration 4524 : loss : 0.025291, loss_ce: 0.009332
2022-01-09 13:21:02,373 iteration 4525 : loss : 0.043948, loss_ce: 0.021847
2022-01-09 13:21:03,740 iteration 4526 : loss : 0.031146, loss_ce: 0.010721
2022-01-09 13:21:05,039 iteration 4527 : loss : 0.032555, loss_ce: 0.010579
2022-01-09 13:21:06,370 iteration 4528 : loss : 0.023739, loss_ce: 0.008095
2022-01-09 13:21:07,759 iteration 4529 : loss : 0.036977, loss_ce: 0.018383
2022-01-09 13:21:09,097 iteration 4530 : loss : 0.034087, loss_ce: 0.008052
2022-01-09 13:21:10,475 iteration 4531 : loss : 0.022990, loss_ce: 0.007841
2022-01-09 13:21:11,874 iteration 4532 : loss : 0.017210, loss_ce: 0.005767
2022-01-09 13:21:13,224 iteration 4533 : loss : 0.041705, loss_ce: 0.018223
2022-01-09 13:21:14,579 iteration 4534 : loss : 0.035135, loss_ce: 0.010827
2022-01-09 13:21:15,980 iteration 4535 : loss : 0.028558, loss_ce: 0.010302
2022-01-09 13:21:17,373 iteration 4536 : loss : 0.027074, loss_ce: 0.010737
2022-01-09 13:21:18,751 iteration 4537 : loss : 0.023633, loss_ce: 0.010706
2022-01-09 13:21:20,124 iteration 4538 : loss : 0.026275, loss_ce: 0.010184
2022-01-09 13:21:21,489 iteration 4539 : loss : 0.021952, loss_ce: 0.009066
 67%|███████████████████▎         | 267/400 [1:53:59<55:34, 25.07s/it]2022-01-09 13:21:22,892 iteration 4540 : loss : 0.018556, loss_ce: 0.006767
2022-01-09 13:21:24,295 iteration 4541 : loss : 0.027630, loss_ce: 0.012437
2022-01-09 13:21:25,627 iteration 4542 : loss : 0.027048, loss_ce: 0.009972
2022-01-09 13:21:26,973 iteration 4543 : loss : 0.026507, loss_ce: 0.009885
2022-01-09 13:21:28,304 iteration 4544 : loss : 0.029378, loss_ce: 0.010607
2022-01-09 13:21:29,736 iteration 4545 : loss : 0.033932, loss_ce: 0.016715
2022-01-09 13:21:31,217 iteration 4546 : loss : 0.041292, loss_ce: 0.022196
2022-01-09 13:21:32,684 iteration 4547 : loss : 0.026071, loss_ce: 0.010738
2022-01-09 13:21:33,999 iteration 4548 : loss : 0.020612, loss_ce: 0.008030
2022-01-09 13:21:35,325 iteration 4549 : loss : 0.025797, loss_ce: 0.007902
2022-01-09 13:21:36,703 iteration 4550 : loss : 0.030327, loss_ce: 0.013796
2022-01-09 13:21:38,103 iteration 4551 : loss : 0.025045, loss_ce: 0.009582
2022-01-09 13:21:39,559 iteration 4552 : loss : 0.037311, loss_ce: 0.011305
2022-01-09 13:21:40,960 iteration 4553 : loss : 0.024084, loss_ce: 0.008618
2022-01-09 13:21:42,293 iteration 4554 : loss : 0.022389, loss_ce: 0.009426
2022-01-09 13:21:43,653 iteration 4555 : loss : 0.022604, loss_ce: 0.010002
2022-01-09 13:21:45,013 iteration 4556 : loss : 0.034181, loss_ce: 0.009290
 67%|███████████████████▍         | 268/400 [1:54:22<54:07, 24.60s/it]2022-01-09 13:21:46,407 iteration 4557 : loss : 0.020461, loss_ce: 0.008271
2022-01-09 13:21:47,812 iteration 4558 : loss : 0.030785, loss_ce: 0.011312
2022-01-09 13:21:49,250 iteration 4559 : loss : 0.027487, loss_ce: 0.012027
2022-01-09 13:21:50,662 iteration 4560 : loss : 0.023778, loss_ce: 0.012610
2022-01-09 13:21:52,000 iteration 4561 : loss : 0.018753, loss_ce: 0.006414
2022-01-09 13:21:53,379 iteration 4562 : loss : 0.029278, loss_ce: 0.011099
2022-01-09 13:21:54,738 iteration 4563 : loss : 0.031279, loss_ce: 0.011832
2022-01-09 13:21:56,084 iteration 4564 : loss : 0.021909, loss_ce: 0.009544
2022-01-09 13:21:57,447 iteration 4565 : loss : 0.026352, loss_ce: 0.011516
2022-01-09 13:21:58,830 iteration 4566 : loss : 0.022811, loss_ce: 0.009139
2022-01-09 13:22:00,193 iteration 4567 : loss : 0.021398, loss_ce: 0.009221
2022-01-09 13:22:01,483 iteration 4568 : loss : 0.019845, loss_ce: 0.006974
2022-01-09 13:22:02,795 iteration 4569 : loss : 0.022681, loss_ce: 0.009156
2022-01-09 13:22:04,125 iteration 4570 : loss : 0.016683, loss_ce: 0.005817
2022-01-09 13:22:05,533 iteration 4571 : loss : 0.025255, loss_ce: 0.010991
2022-01-09 13:22:06,932 iteration 4572 : loss : 0.035768, loss_ce: 0.014285
2022-01-09 13:22:08,354 iteration 4573 : loss : 0.034183, loss_ce: 0.011562
 67%|███████████████████▌         | 269/400 [1:54:46<52:53, 24.22s/it]2022-01-09 13:22:09,724 iteration 4574 : loss : 0.015374, loss_ce: 0.004971
2022-01-09 13:22:11,115 iteration 4575 : loss : 0.018288, loss_ce: 0.004458
2022-01-09 13:22:12,417 iteration 4576 : loss : 0.027283, loss_ce: 0.010299
2022-01-09 13:22:13,739 iteration 4577 : loss : 0.024488, loss_ce: 0.012553
2022-01-09 13:22:15,102 iteration 4578 : loss : 0.025710, loss_ce: 0.011771
2022-01-09 13:22:16,430 iteration 4579 : loss : 0.022134, loss_ce: 0.008866
2022-01-09 13:22:17,778 iteration 4580 : loss : 0.019673, loss_ce: 0.007455
2022-01-09 13:22:19,141 iteration 4581 : loss : 0.035722, loss_ce: 0.014536
2022-01-09 13:22:20,507 iteration 4582 : loss : 0.040989, loss_ce: 0.017028
2022-01-09 13:22:21,862 iteration 4583 : loss : 0.026395, loss_ce: 0.009265
2022-01-09 13:22:23,207 iteration 4584 : loss : 0.021571, loss_ce: 0.007732
2022-01-09 13:22:24,589 iteration 4585 : loss : 0.028928, loss_ce: 0.010068
2022-01-09 13:22:25,909 iteration 4586 : loss : 0.021766, loss_ce: 0.008984
2022-01-09 13:22:27,244 iteration 4587 : loss : 0.023833, loss_ce: 0.007571
2022-01-09 13:22:28,579 iteration 4588 : loss : 0.027222, loss_ce: 0.009490
2022-01-09 13:22:29,961 iteration 4589 : loss : 0.026222, loss_ce: 0.008619
2022-01-09 13:22:29,961 Training Data Eval:
2022-01-09 13:22:36,845   Average segmentation loss on training set: 0.0160
2022-01-09 13:22:36,845 Validation Data Eval:
2022-01-09 13:22:39,234   Average segmentation loss on validation set: 0.0821
2022-01-09 13:22:40,588 iteration 4590 : loss : 0.022602, loss_ce: 0.009868
 68%|███████████████████▌         | 270/400 [1:55:18<57:41, 26.63s/it]2022-01-09 13:22:42,058 iteration 4591 : loss : 0.032232, loss_ce: 0.007828
2022-01-09 13:22:43,474 iteration 4592 : loss : 0.019388, loss_ce: 0.006900
2022-01-09 13:22:44,871 iteration 4593 : loss : 0.024192, loss_ce: 0.011538
2022-01-09 13:22:46,310 iteration 4594 : loss : 0.032560, loss_ce: 0.013003
2022-01-09 13:22:47,679 iteration 4595 : loss : 0.023985, loss_ce: 0.008575
2022-01-09 13:22:49,024 iteration 4596 : loss : 0.024086, loss_ce: 0.007055
2022-01-09 13:22:50,404 iteration 4597 : loss : 0.019433, loss_ce: 0.008623
2022-01-09 13:22:51,831 iteration 4598 : loss : 0.032449, loss_ce: 0.010270
2022-01-09 13:22:53,245 iteration 4599 : loss : 0.030269, loss_ce: 0.013878
2022-01-09 13:22:54,602 iteration 4600 : loss : 0.024820, loss_ce: 0.008370
2022-01-09 13:22:55,997 iteration 4601 : loss : 0.022635, loss_ce: 0.010126
2022-01-09 13:22:57,468 iteration 4602 : loss : 0.025557, loss_ce: 0.006396
2022-01-09 13:22:58,857 iteration 4603 : loss : 0.032150, loss_ce: 0.010844
2022-01-09 13:23:00,267 iteration 4604 : loss : 0.026074, loss_ce: 0.012084
2022-01-09 13:23:01,689 iteration 4605 : loss : 0.055268, loss_ce: 0.010722
2022-01-09 13:23:03,107 iteration 4606 : loss : 0.022668, loss_ce: 0.008913
2022-01-09 13:23:04,526 iteration 4607 : loss : 0.030142, loss_ce: 0.017749
 68%|███████████████████▋         | 271/400 [1:55:42<55:30, 25.82s/it]2022-01-09 13:23:05,921 iteration 4608 : loss : 0.022961, loss_ce: 0.008942
2022-01-09 13:23:07,307 iteration 4609 : loss : 0.029739, loss_ce: 0.011875
2022-01-09 13:23:08,632 iteration 4610 : loss : 0.024906, loss_ce: 0.007761
2022-01-09 13:23:09,949 iteration 4611 : loss : 0.025484, loss_ce: 0.010448
2022-01-09 13:23:11,248 iteration 4612 : loss : 0.024840, loss_ce: 0.007538
2022-01-09 13:23:12,622 iteration 4613 : loss : 0.022364, loss_ce: 0.007858
2022-01-09 13:23:13,995 iteration 4614 : loss : 0.022515, loss_ce: 0.009588
2022-01-09 13:23:15,408 iteration 4615 : loss : 0.027428, loss_ce: 0.011645
2022-01-09 13:23:16,890 iteration 4616 : loss : 0.028505, loss_ce: 0.011854
2022-01-09 13:23:18,283 iteration 4617 : loss : 0.021207, loss_ce: 0.007880
2022-01-09 13:23:19,607 iteration 4618 : loss : 0.019746, loss_ce: 0.006050
2022-01-09 13:23:20,946 iteration 4619 : loss : 0.015644, loss_ce: 0.006537
2022-01-09 13:23:22,373 iteration 4620 : loss : 0.027090, loss_ce: 0.007470
2022-01-09 13:23:23,742 iteration 4621 : loss : 0.028408, loss_ce: 0.014082
2022-01-09 13:23:25,120 iteration 4622 : loss : 0.050765, loss_ce: 0.016207
2022-01-09 13:23:26,511 iteration 4623 : loss : 0.019895, loss_ce: 0.006390
2022-01-09 13:23:27,892 iteration 4624 : loss : 0.022225, loss_ce: 0.010001
 68%|███████████████████▋         | 272/400 [1:56:05<53:30, 25.08s/it]2022-01-09 13:23:29,366 iteration 4625 : loss : 0.021305, loss_ce: 0.008085
2022-01-09 13:23:30,802 iteration 4626 : loss : 0.022754, loss_ce: 0.010583
2022-01-09 13:23:32,281 iteration 4627 : loss : 0.027491, loss_ce: 0.011038
2022-01-09 13:23:33,634 iteration 4628 : loss : 0.024231, loss_ce: 0.007965
2022-01-09 13:23:34,949 iteration 4629 : loss : 0.022374, loss_ce: 0.007587
2022-01-09 13:23:36,311 iteration 4630 : loss : 0.022306, loss_ce: 0.008675
2022-01-09 13:23:37,689 iteration 4631 : loss : 0.028810, loss_ce: 0.010884
2022-01-09 13:23:39,066 iteration 4632 : loss : 0.026712, loss_ce: 0.008545
2022-01-09 13:23:40,421 iteration 4633 : loss : 0.020964, loss_ce: 0.006574
2022-01-09 13:23:41,758 iteration 4634 : loss : 0.016973, loss_ce: 0.007635
2022-01-09 13:23:43,051 iteration 4635 : loss : 0.019137, loss_ce: 0.007852
2022-01-09 13:23:44,494 iteration 4636 : loss : 0.030171, loss_ce: 0.010731
2022-01-09 13:23:45,842 iteration 4637 : loss : 0.024768, loss_ce: 0.007522
2022-01-09 13:23:47,320 iteration 4638 : loss : 0.030949, loss_ce: 0.014676
2022-01-09 13:23:48,741 iteration 4639 : loss : 0.030201, loss_ce: 0.011709
2022-01-09 13:23:50,231 iteration 4640 : loss : 0.049534, loss_ce: 0.020490
2022-01-09 13:23:51,650 iteration 4641 : loss : 0.022765, loss_ce: 0.008548
 68%|███████████████████▊         | 273/400 [1:56:29<52:15, 24.69s/it]2022-01-09 13:23:53,078 iteration 4642 : loss : 0.024300, loss_ce: 0.005357
2022-01-09 13:23:54,454 iteration 4643 : loss : 0.026814, loss_ce: 0.008355
2022-01-09 13:23:55,883 iteration 4644 : loss : 0.028334, loss_ce: 0.009464
2022-01-09 13:23:57,424 iteration 4645 : loss : 0.031073, loss_ce: 0.014564
2022-01-09 13:23:58,828 iteration 4646 : loss : 0.047894, loss_ce: 0.012058
2022-01-09 13:24:00,140 iteration 4647 : loss : 0.020996, loss_ce: 0.006659
2022-01-09 13:24:01,526 iteration 4648 : loss : 0.040849, loss_ce: 0.013880
2022-01-09 13:24:02,912 iteration 4649 : loss : 0.023176, loss_ce: 0.009843
2022-01-09 13:24:04,338 iteration 4650 : loss : 0.042966, loss_ce: 0.011281
2022-01-09 13:24:05,666 iteration 4651 : loss : 0.018871, loss_ce: 0.008168
2022-01-09 13:24:07,116 iteration 4652 : loss : 0.022866, loss_ce: 0.008827
2022-01-09 13:24:08,471 iteration 4653 : loss : 0.024600, loss_ce: 0.010175
2022-01-09 13:24:09,830 iteration 4654 : loss : 0.018149, loss_ce: 0.006604
2022-01-09 13:24:11,274 iteration 4655 : loss : 0.043728, loss_ce: 0.016393
2022-01-09 13:24:12,737 iteration 4656 : loss : 0.026530, loss_ce: 0.010102
2022-01-09 13:24:14,155 iteration 4657 : loss : 0.034698, loss_ce: 0.014552
2022-01-09 13:24:15,572 iteration 4658 : loss : 0.030410, loss_ce: 0.011826
 68%|███████████████████▊         | 274/400 [1:56:53<51:21, 24.46s/it]2022-01-09 13:24:17,039 iteration 4659 : loss : 0.027862, loss_ce: 0.012152
2022-01-09 13:24:18,478 iteration 4660 : loss : 0.040519, loss_ce: 0.016060
2022-01-09 13:24:19,889 iteration 4661 : loss : 0.032148, loss_ce: 0.014101
2022-01-09 13:24:21,226 iteration 4662 : loss : 0.019141, loss_ce: 0.007274
2022-01-09 13:24:22,633 iteration 4663 : loss : 0.040708, loss_ce: 0.017771
2022-01-09 13:24:23,987 iteration 4664 : loss : 0.018816, loss_ce: 0.006910
2022-01-09 13:24:25,357 iteration 4665 : loss : 0.029434, loss_ce: 0.011729
2022-01-09 13:24:26,636 iteration 4666 : loss : 0.022122, loss_ce: 0.008453
2022-01-09 13:24:27,972 iteration 4667 : loss : 0.017438, loss_ce: 0.006991
2022-01-09 13:24:29,468 iteration 4668 : loss : 0.033481, loss_ce: 0.016408
2022-01-09 13:24:30,802 iteration 4669 : loss : 0.018456, loss_ce: 0.008190
2022-01-09 13:24:32,142 iteration 4670 : loss : 0.030498, loss_ce: 0.012892
2022-01-09 13:24:33,468 iteration 4671 : loss : 0.027365, loss_ce: 0.008524
2022-01-09 13:24:34,936 iteration 4672 : loss : 0.042034, loss_ce: 0.016109
2022-01-09 13:24:36,374 iteration 4673 : loss : 0.045329, loss_ce: 0.022308
2022-01-09 13:24:37,686 iteration 4674 : loss : 0.017363, loss_ce: 0.004884
2022-01-09 13:24:37,686 Training Data Eval:
2022-01-09 13:24:44,552   Average segmentation loss on training set: 0.0164
2022-01-09 13:24:44,553 Validation Data Eval:
2022-01-09 13:24:46,933   Average segmentation loss on validation set: 0.0664
2022-01-09 13:24:52,705 Found new lowest validation loss at iteration 4674! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed1234.pth
2022-01-09 13:24:54,228 iteration 4675 : loss : 0.024134, loss_ce: 0.010868
 69%|███████████████████▉         | 275/400 [1:57:31<59:49, 28.72s/it]2022-01-09 13:24:55,751 iteration 4676 : loss : 0.037715, loss_ce: 0.013091
2022-01-09 13:24:57,106 iteration 4677 : loss : 0.030342, loss_ce: 0.010698
2022-01-09 13:24:58,501 iteration 4678 : loss : 0.037005, loss_ce: 0.016282
2022-01-09 13:24:59,945 iteration 4679 : loss : 0.060274, loss_ce: 0.015544
2022-01-09 13:25:01,319 iteration 4680 : loss : 0.018837, loss_ce: 0.007601
2022-01-09 13:25:02,676 iteration 4681 : loss : 0.035336, loss_ce: 0.009846
2022-01-09 13:25:04,019 iteration 4682 : loss : 0.025661, loss_ce: 0.009465
2022-01-09 13:25:05,295 iteration 4683 : loss : 0.024350, loss_ce: 0.006616
2022-01-09 13:25:06,722 iteration 4684 : loss : 0.024248, loss_ce: 0.010641
2022-01-09 13:25:08,121 iteration 4685 : loss : 0.027274, loss_ce: 0.009020
2022-01-09 13:25:09,493 iteration 4686 : loss : 0.028240, loss_ce: 0.007966
2022-01-09 13:25:10,844 iteration 4687 : loss : 0.024638, loss_ce: 0.011434
2022-01-09 13:25:12,143 iteration 4688 : loss : 0.020263, loss_ce: 0.008559
2022-01-09 13:25:13,454 iteration 4689 : loss : 0.014824, loss_ce: 0.006515
2022-01-09 13:25:14,780 iteration 4690 : loss : 0.034162, loss_ce: 0.016773
2022-01-09 13:25:16,190 iteration 4691 : loss : 0.042735, loss_ce: 0.026306
2022-01-09 13:25:17,545 iteration 4692 : loss : 0.024619, loss_ce: 0.007357
 69%|████████████████████         | 276/400 [1:57:55<56:00, 27.10s/it]2022-01-09 13:25:19,040 iteration 4693 : loss : 0.039521, loss_ce: 0.013827
2022-01-09 13:25:20,441 iteration 4694 : loss : 0.031299, loss_ce: 0.011593
2022-01-09 13:25:21,684 iteration 4695 : loss : 0.020029, loss_ce: 0.006750
2022-01-09 13:25:23,130 iteration 4696 : loss : 0.027006, loss_ce: 0.010955
2022-01-09 13:25:24,479 iteration 4697 : loss : 0.026912, loss_ce: 0.010617
2022-01-09 13:25:25,859 iteration 4698 : loss : 0.025469, loss_ce: 0.012823
2022-01-09 13:25:27,294 iteration 4699 : loss : 0.025621, loss_ce: 0.014339
2022-01-09 13:25:28,626 iteration 4700 : loss : 0.024652, loss_ce: 0.011799
2022-01-09 13:25:30,091 iteration 4701 : loss : 0.028887, loss_ce: 0.011871
2022-01-09 13:25:31,442 iteration 4702 : loss : 0.022472, loss_ce: 0.009673
2022-01-09 13:25:32,775 iteration 4703 : loss : 0.025113, loss_ce: 0.006865
2022-01-09 13:25:34,148 iteration 4704 : loss : 0.027231, loss_ce: 0.008452
2022-01-09 13:25:35,396 iteration 4705 : loss : 0.015576, loss_ce: 0.006793
2022-01-09 13:25:36,761 iteration 4706 : loss : 0.021071, loss_ce: 0.006400
2022-01-09 13:25:38,183 iteration 4707 : loss : 0.029988, loss_ce: 0.011025
2022-01-09 13:25:39,618 iteration 4708 : loss : 0.022672, loss_ce: 0.007220
2022-01-09 13:25:41,027 iteration 4709 : loss : 0.028605, loss_ce: 0.011191
 69%|████████████████████         | 277/400 [1:58:18<53:19, 26.01s/it]2022-01-09 13:25:42,493 iteration 4710 : loss : 0.032586, loss_ce: 0.007292
2022-01-09 13:25:44,010 iteration 4711 : loss : 0.021751, loss_ce: 0.007438
2022-01-09 13:25:45,377 iteration 4712 : loss : 0.027398, loss_ce: 0.010861
2022-01-09 13:25:46,749 iteration 4713 : loss : 0.018480, loss_ce: 0.007094
2022-01-09 13:25:48,140 iteration 4714 : loss : 0.026426, loss_ce: 0.009683
2022-01-09 13:25:49,575 iteration 4715 : loss : 0.026131, loss_ce: 0.009972
2022-01-09 13:25:50,977 iteration 4716 : loss : 0.021788, loss_ce: 0.008886
2022-01-09 13:25:52,348 iteration 4717 : loss : 0.023522, loss_ce: 0.008919
2022-01-09 13:25:53,661 iteration 4718 : loss : 0.020027, loss_ce: 0.007296
2022-01-09 13:25:54,974 iteration 4719 : loss : 0.018100, loss_ce: 0.007802
2022-01-09 13:25:56,309 iteration 4720 : loss : 0.022812, loss_ce: 0.009889
2022-01-09 13:25:57,662 iteration 4721 : loss : 0.018410, loss_ce: 0.007031
2022-01-09 13:25:59,085 iteration 4722 : loss : 0.026245, loss_ce: 0.008126
2022-01-09 13:26:00,452 iteration 4723 : loss : 0.017954, loss_ce: 0.007029
2022-01-09 13:26:01,760 iteration 4724 : loss : 0.019440, loss_ce: 0.007671
2022-01-09 13:26:03,065 iteration 4725 : loss : 0.018444, loss_ce: 0.007615
2022-01-09 13:26:04,432 iteration 4726 : loss : 0.019814, loss_ce: 0.008126
 70%|████████████████████▏        | 278/400 [1:58:42<51:18, 25.23s/it]2022-01-09 13:26:05,915 iteration 4727 : loss : 0.019519, loss_ce: 0.006651
2022-01-09 13:26:07,237 iteration 4728 : loss : 0.026360, loss_ce: 0.011134
2022-01-09 13:26:08,569 iteration 4729 : loss : 0.020472, loss_ce: 0.007297
2022-01-09 13:26:09,899 iteration 4730 : loss : 0.015213, loss_ce: 0.005048
2022-01-09 13:26:11,320 iteration 4731 : loss : 0.021978, loss_ce: 0.007204
2022-01-09 13:26:12,703 iteration 4732 : loss : 0.022374, loss_ce: 0.007768
2022-01-09 13:26:14,082 iteration 4733 : loss : 0.016215, loss_ce: 0.006247
2022-01-09 13:26:15,434 iteration 4734 : loss : 0.029213, loss_ce: 0.011708
2022-01-09 13:26:16,891 iteration 4735 : loss : 0.023499, loss_ce: 0.009115
2022-01-09 13:26:18,198 iteration 4736 : loss : 0.025194, loss_ce: 0.011081
2022-01-09 13:26:19,637 iteration 4737 : loss : 0.020025, loss_ce: 0.007961
2022-01-09 13:26:21,006 iteration 4738 : loss : 0.042599, loss_ce: 0.019873
2022-01-09 13:26:22,371 iteration 4739 : loss : 0.021726, loss_ce: 0.008136
2022-01-09 13:26:23,784 iteration 4740 : loss : 0.028079, loss_ce: 0.011895
2022-01-09 13:26:25,135 iteration 4741 : loss : 0.018767, loss_ce: 0.006201
2022-01-09 13:26:26,494 iteration 4742 : loss : 0.020269, loss_ce: 0.009894
2022-01-09 13:26:27,945 iteration 4743 : loss : 0.030943, loss_ce: 0.010055
 70%|████████████████████▏        | 279/400 [1:59:05<49:50, 24.72s/it]2022-01-09 13:26:29,468 iteration 4744 : loss : 0.023249, loss_ce: 0.008616
2022-01-09 13:26:30,850 iteration 4745 : loss : 0.034791, loss_ce: 0.010047
2022-01-09 13:26:32,183 iteration 4746 : loss : 0.017507, loss_ce: 0.008746
2022-01-09 13:26:33,531 iteration 4747 : loss : 0.024681, loss_ce: 0.008344
2022-01-09 13:26:34,911 iteration 4748 : loss : 0.040611, loss_ce: 0.018328
2022-01-09 13:26:36,291 iteration 4749 : loss : 0.022848, loss_ce: 0.007848
2022-01-09 13:26:37,714 iteration 4750 : loss : 0.025108, loss_ce: 0.010290
2022-01-09 13:26:39,065 iteration 4751 : loss : 0.026259, loss_ce: 0.009732
2022-01-09 13:26:40,466 iteration 4752 : loss : 0.019332, loss_ce: 0.006682
2022-01-09 13:26:41,884 iteration 4753 : loss : 0.021652, loss_ce: 0.008163
2022-01-09 13:26:43,277 iteration 4754 : loss : 0.019572, loss_ce: 0.006476
2022-01-09 13:26:44,759 iteration 4755 : loss : 0.022240, loss_ce: 0.007399
2022-01-09 13:26:46,105 iteration 4756 : loss : 0.023238, loss_ce: 0.011707
2022-01-09 13:26:47,466 iteration 4757 : loss : 0.024395, loss_ce: 0.009796
2022-01-09 13:26:48,804 iteration 4758 : loss : 0.020771, loss_ce: 0.006518
2022-01-09 13:26:50,170 iteration 4759 : loss : 0.017648, loss_ce: 0.006158
2022-01-09 13:26:50,171 Training Data Eval:
2022-01-09 13:26:57,027   Average segmentation loss on training set: 0.0141
2022-01-09 13:26:57,028 Validation Data Eval:
2022-01-09 13:26:59,392   Average segmentation loss on validation set: 0.0806
2022-01-09 13:27:00,734 iteration 4760 : loss : 0.021543, loss_ce: 0.009573
 70%|████████████████████▎        | 280/400 [1:59:38<54:16, 27.14s/it]2022-01-09 13:27:02,146 iteration 4761 : loss : 0.019806, loss_ce: 0.006317
2022-01-09 13:27:03,549 iteration 4762 : loss : 0.028981, loss_ce: 0.012093
2022-01-09 13:27:04,869 iteration 4763 : loss : 0.018184, loss_ce: 0.007616
2022-01-09 13:27:06,273 iteration 4764 : loss : 0.021009, loss_ce: 0.007609
2022-01-09 13:27:07,628 iteration 4765 : loss : 0.026332, loss_ce: 0.012014
2022-01-09 13:27:09,062 iteration 4766 : loss : 0.025867, loss_ce: 0.009197
2022-01-09 13:27:10,463 iteration 4767 : loss : 0.022591, loss_ce: 0.008967
2022-01-09 13:27:11,797 iteration 4768 : loss : 0.022747, loss_ce: 0.007603
2022-01-09 13:27:13,205 iteration 4769 : loss : 0.023540, loss_ce: 0.009062
2022-01-09 13:27:14,618 iteration 4770 : loss : 0.022455, loss_ce: 0.008137
2022-01-09 13:27:15,938 iteration 4771 : loss : 0.020517, loss_ce: 0.007764
2022-01-09 13:27:17,354 iteration 4772 : loss : 0.027733, loss_ce: 0.011700
2022-01-09 13:27:18,757 iteration 4773 : loss : 0.026100, loss_ce: 0.010079
2022-01-09 13:27:20,158 iteration 4774 : loss : 0.019102, loss_ce: 0.007294
2022-01-09 13:27:21,499 iteration 4775 : loss : 0.032298, loss_ce: 0.009344
2022-01-09 13:27:22,863 iteration 4776 : loss : 0.018257, loss_ce: 0.009706
2022-01-09 13:27:24,368 iteration 4777 : loss : 0.030293, loss_ce: 0.016211
 70%|████████████████████▎        | 281/400 [2:00:02<51:44, 26.09s/it]2022-01-09 13:27:25,735 iteration 4778 : loss : 0.019926, loss_ce: 0.007104
2022-01-09 13:27:27,092 iteration 4779 : loss : 0.030652, loss_ce: 0.011485
2022-01-09 13:27:28,394 iteration 4780 : loss : 0.016477, loss_ce: 0.005498
2022-01-09 13:27:29,740 iteration 4781 : loss : 0.029978, loss_ce: 0.008883
2022-01-09 13:27:31,168 iteration 4782 : loss : 0.025003, loss_ce: 0.012846
2022-01-09 13:27:32,544 iteration 4783 : loss : 0.016661, loss_ce: 0.009484
2022-01-09 13:27:33,861 iteration 4784 : loss : 0.015568, loss_ce: 0.006751
2022-01-09 13:27:35,298 iteration 4785 : loss : 0.023615, loss_ce: 0.008291
2022-01-09 13:27:36,696 iteration 4786 : loss : 0.027280, loss_ce: 0.009547
2022-01-09 13:27:38,096 iteration 4787 : loss : 0.022626, loss_ce: 0.007542
2022-01-09 13:27:39,499 iteration 4788 : loss : 0.022316, loss_ce: 0.007926
2022-01-09 13:27:40,814 iteration 4789 : loss : 0.018351, loss_ce: 0.008793
2022-01-09 13:27:42,280 iteration 4790 : loss : 0.032401, loss_ce: 0.009637
2022-01-09 13:27:43,636 iteration 4791 : loss : 0.042031, loss_ce: 0.021824
2022-01-09 13:27:45,061 iteration 4792 : loss : 0.036768, loss_ce: 0.010224
2022-01-09 13:27:46,396 iteration 4793 : loss : 0.018984, loss_ce: 0.007023
2022-01-09 13:27:47,683 iteration 4794 : loss : 0.024705, loss_ce: 0.007146
 70%|████████████████████▍        | 282/400 [2:00:25<49:39, 25.25s/it]2022-01-09 13:27:49,133 iteration 4795 : loss : 0.034779, loss_ce: 0.011179
2022-01-09 13:27:50,521 iteration 4796 : loss : 0.028329, loss_ce: 0.009360
2022-01-09 13:27:51,918 iteration 4797 : loss : 0.033603, loss_ce: 0.014787
2022-01-09 13:27:53,359 iteration 4798 : loss : 0.039942, loss_ce: 0.014028
2022-01-09 13:27:54,733 iteration 4799 : loss : 0.019574, loss_ce: 0.006062
2022-01-09 13:27:56,113 iteration 4800 : loss : 0.024525, loss_ce: 0.007749
2022-01-09 13:27:57,568 iteration 4801 : loss : 0.025837, loss_ce: 0.011605
2022-01-09 13:27:58,873 iteration 4802 : loss : 0.018035, loss_ce: 0.007912
2022-01-09 13:28:00,220 iteration 4803 : loss : 0.020208, loss_ce: 0.009416
2022-01-09 13:28:01,663 iteration 4804 : loss : 0.072637, loss_ce: 0.019762
2022-01-09 13:28:03,096 iteration 4805 : loss : 0.034719, loss_ce: 0.013133
2022-01-09 13:28:04,537 iteration 4806 : loss : 0.029922, loss_ce: 0.011829
2022-01-09 13:28:05,847 iteration 4807 : loss : 0.023268, loss_ce: 0.009904
2022-01-09 13:28:07,152 iteration 4808 : loss : 0.018025, loss_ce: 0.008723
2022-01-09 13:28:08,537 iteration 4809 : loss : 0.030727, loss_ce: 0.015579
2022-01-09 13:28:09,904 iteration 4810 : loss : 0.028122, loss_ce: 0.007979
2022-01-09 13:28:11,294 iteration 4811 : loss : 0.023720, loss_ce: 0.007995
 71%|████████████████████▌        | 283/400 [2:00:48<48:16, 24.76s/it]2022-01-09 13:28:12,675 iteration 4812 : loss : 0.029101, loss_ce: 0.011235
2022-01-09 13:28:14,066 iteration 4813 : loss : 0.022153, loss_ce: 0.005809
2022-01-09 13:28:15,482 iteration 4814 : loss : 0.037611, loss_ce: 0.009429
2022-01-09 13:28:16,793 iteration 4815 : loss : 0.019131, loss_ce: 0.007689
2022-01-09 13:28:18,271 iteration 4816 : loss : 0.032502, loss_ce: 0.016242
2022-01-09 13:28:19,631 iteration 4817 : loss : 0.029920, loss_ce: 0.011488
2022-01-09 13:28:20,981 iteration 4818 : loss : 0.021136, loss_ce: 0.007789
2022-01-09 13:28:22,270 iteration 4819 : loss : 0.023561, loss_ce: 0.009222
2022-01-09 13:28:23,612 iteration 4820 : loss : 0.038117, loss_ce: 0.017448
2022-01-09 13:28:25,069 iteration 4821 : loss : 0.035711, loss_ce: 0.016180
2022-01-09 13:28:26,520 iteration 4822 : loss : 0.028620, loss_ce: 0.009792
2022-01-09 13:28:27,892 iteration 4823 : loss : 0.027075, loss_ce: 0.012066
2022-01-09 13:28:29,299 iteration 4824 : loss : 0.027663, loss_ce: 0.010439
2022-01-09 13:28:30,654 iteration 4825 : loss : 0.025416, loss_ce: 0.010126
2022-01-09 13:28:31,999 iteration 4826 : loss : 0.029893, loss_ce: 0.010687
2022-01-09 13:28:33,356 iteration 4827 : loss : 0.019833, loss_ce: 0.007421
2022-01-09 13:28:34,724 iteration 4828 : loss : 0.028546, loss_ce: 0.012180
 71%|████████████████████▌        | 284/400 [2:01:12<47:06, 24.36s/it]2022-01-09 13:28:36,100 iteration 4829 : loss : 0.025602, loss_ce: 0.008464
2022-01-09 13:28:37,500 iteration 4830 : loss : 0.019980, loss_ce: 0.009949
2022-01-09 13:28:38,886 iteration 4831 : loss : 0.023387, loss_ce: 0.009583
2022-01-09 13:28:40,252 iteration 4832 : loss : 0.031367, loss_ce: 0.009088
2022-01-09 13:28:41,667 iteration 4833 : loss : 0.031625, loss_ce: 0.011149
2022-01-09 13:28:42,978 iteration 4834 : loss : 0.021147, loss_ce: 0.008069
2022-01-09 13:28:44,457 iteration 4835 : loss : 0.027371, loss_ce: 0.013055
2022-01-09 13:28:45,837 iteration 4836 : loss : 0.026818, loss_ce: 0.008245
2022-01-09 13:28:47,260 iteration 4837 : loss : 0.030750, loss_ce: 0.007624
2022-01-09 13:28:48,612 iteration 4838 : loss : 0.021241, loss_ce: 0.006982
2022-01-09 13:28:50,015 iteration 4839 : loss : 0.025328, loss_ce: 0.009607
2022-01-09 13:28:51,388 iteration 4840 : loss : 0.021837, loss_ce: 0.010141
2022-01-09 13:28:52,694 iteration 4841 : loss : 0.015864, loss_ce: 0.005435
2022-01-09 13:28:54,073 iteration 4842 : loss : 0.028439, loss_ce: 0.012712
2022-01-09 13:28:55,533 iteration 4843 : loss : 0.021019, loss_ce: 0.006447
2022-01-09 13:28:56,904 iteration 4844 : loss : 0.029478, loss_ce: 0.011007
2022-01-09 13:28:56,904 Training Data Eval:
2022-01-09 13:29:03,753   Average segmentation loss on training set: 0.0172
2022-01-09 13:29:03,753 Validation Data Eval:
2022-01-09 13:29:06,129   Average segmentation loss on validation set: 0.1322
2022-01-09 13:29:07,542 iteration 4845 : loss : 0.028567, loss_ce: 0.013398
 71%|████████████████████▋        | 285/400 [2:01:45<51:33, 26.90s/it]2022-01-09 13:29:08,941 iteration 4846 : loss : 0.021294, loss_ce: 0.008852
2022-01-09 13:29:10,282 iteration 4847 : loss : 0.019067, loss_ce: 0.007313
2022-01-09 13:29:11,700 iteration 4848 : loss : 0.027182, loss_ce: 0.013132
2022-01-09 13:29:13,100 iteration 4849 : loss : 0.027571, loss_ce: 0.006912
2022-01-09 13:29:14,442 iteration 4850 : loss : 0.022570, loss_ce: 0.006866
2022-01-09 13:29:15,925 iteration 4851 : loss : 0.017898, loss_ce: 0.007291
2022-01-09 13:29:17,288 iteration 4852 : loss : 0.023238, loss_ce: 0.007323
2022-01-09 13:29:18,672 iteration 4853 : loss : 0.021954, loss_ce: 0.009314
2022-01-09 13:29:20,061 iteration 4854 : loss : 0.019523, loss_ce: 0.007415
2022-01-09 13:29:21,407 iteration 4855 : loss : 0.026006, loss_ce: 0.007933
2022-01-09 13:29:22,870 iteration 4856 : loss : 0.026639, loss_ce: 0.009919
2022-01-09 13:29:24,213 iteration 4857 : loss : 0.014586, loss_ce: 0.005432
2022-01-09 13:29:25,661 iteration 4858 : loss : 0.021938, loss_ce: 0.008645
2022-01-09 13:29:26,980 iteration 4859 : loss : 0.014922, loss_ce: 0.006095
2022-01-09 13:29:28,392 iteration 4860 : loss : 0.028125, loss_ce: 0.011403
2022-01-09 13:29:29,735 iteration 4861 : loss : 0.023744, loss_ce: 0.007954
2022-01-09 13:29:31,149 iteration 4862 : loss : 0.033156, loss_ce: 0.015411
 72%|████████████████████▋        | 286/400 [2:02:08<49:13, 25.91s/it]2022-01-09 13:29:32,664 iteration 4863 : loss : 0.022257, loss_ce: 0.006784
2022-01-09 13:29:34,040 iteration 4864 : loss : 0.016884, loss_ce: 0.006014
2022-01-09 13:29:35,391 iteration 4865 : loss : 0.017593, loss_ce: 0.008266
2022-01-09 13:29:36,825 iteration 4866 : loss : 0.025685, loss_ce: 0.007754
2022-01-09 13:29:38,215 iteration 4867 : loss : 0.024877, loss_ce: 0.008062
2022-01-09 13:29:39,500 iteration 4868 : loss : 0.021535, loss_ce: 0.009060
2022-01-09 13:29:40,963 iteration 4869 : loss : 0.028151, loss_ce: 0.013762
2022-01-09 13:29:42,329 iteration 4870 : loss : 0.015307, loss_ce: 0.005382
2022-01-09 13:29:43,801 iteration 4871 : loss : 0.035141, loss_ce: 0.012678
2022-01-09 13:29:45,110 iteration 4872 : loss : 0.018268, loss_ce: 0.005638
2022-01-09 13:29:46,574 iteration 4873 : loss : 0.031409, loss_ce: 0.015143
2022-01-09 13:29:47,972 iteration 4874 : loss : 0.036120, loss_ce: 0.010565
2022-01-09 13:29:49,331 iteration 4875 : loss : 0.026536, loss_ce: 0.010741
2022-01-09 13:29:50,625 iteration 4876 : loss : 0.021722, loss_ce: 0.007527
2022-01-09 13:29:52,045 iteration 4877 : loss : 0.042161, loss_ce: 0.025991
2022-01-09 13:29:53,495 iteration 4878 : loss : 0.034653, loss_ce: 0.011672
2022-01-09 13:29:54,874 iteration 4879 : loss : 0.014178, loss_ce: 0.005096
 72%|████████████████████▊        | 287/400 [2:02:32<47:33, 25.25s/it]2022-01-09 13:29:56,283 iteration 4880 : loss : 0.020378, loss_ce: 0.008300
2022-01-09 13:29:57,603 iteration 4881 : loss : 0.019240, loss_ce: 0.008159
2022-01-09 13:29:58,889 iteration 4882 : loss : 0.019285, loss_ce: 0.008036
2022-01-09 13:30:00,282 iteration 4883 : loss : 0.015869, loss_ce: 0.006550
2022-01-09 13:30:01,693 iteration 4884 : loss : 0.022429, loss_ce: 0.007144
2022-01-09 13:30:02,966 iteration 4885 : loss : 0.016476, loss_ce: 0.006518
2022-01-09 13:30:04,338 iteration 4886 : loss : 0.025067, loss_ce: 0.006199
2022-01-09 13:30:05,647 iteration 4887 : loss : 0.016278, loss_ce: 0.005085
2022-01-09 13:30:07,036 iteration 4888 : loss : 0.024997, loss_ce: 0.006411
2022-01-09 13:30:08,414 iteration 4889 : loss : 0.028968, loss_ce: 0.013599
2022-01-09 13:30:09,747 iteration 4890 : loss : 0.019139, loss_ce: 0.009975
2022-01-09 13:30:11,006 iteration 4891 : loss : 0.014691, loss_ce: 0.005508
2022-01-09 13:30:12,410 iteration 4892 : loss : 0.027172, loss_ce: 0.013924
2022-01-09 13:30:13,784 iteration 4893 : loss : 0.035702, loss_ce: 0.015721
2022-01-09 13:30:15,142 iteration 4894 : loss : 0.016181, loss_ce: 0.007564
2022-01-09 13:30:16,489 iteration 4895 : loss : 0.023929, loss_ce: 0.005631
2022-01-09 13:30:17,814 iteration 4896 : loss : 0.033619, loss_ce: 0.010976
 72%|████████████████████▉        | 288/400 [2:02:55<45:50, 24.56s/it]2022-01-09 13:30:19,209 iteration 4897 : loss : 0.018391, loss_ce: 0.006995
2022-01-09 13:30:20,638 iteration 4898 : loss : 0.038620, loss_ce: 0.022371
2022-01-09 13:30:22,008 iteration 4899 : loss : 0.019301, loss_ce: 0.005836
2022-01-09 13:30:23,338 iteration 4900 : loss : 0.021389, loss_ce: 0.007527
2022-01-09 13:30:24,684 iteration 4901 : loss : 0.034812, loss_ce: 0.010716
2022-01-09 13:30:26,032 iteration 4902 : loss : 0.013646, loss_ce: 0.004975
2022-01-09 13:30:27,420 iteration 4903 : loss : 0.019002, loss_ce: 0.009050
2022-01-09 13:30:28,691 iteration 4904 : loss : 0.020813, loss_ce: 0.006525
2022-01-09 13:30:30,016 iteration 4905 : loss : 0.021664, loss_ce: 0.006361
2022-01-09 13:30:31,408 iteration 4906 : loss : 0.020615, loss_ce: 0.007558
2022-01-09 13:30:32,735 iteration 4907 : loss : 0.019045, loss_ce: 0.006575
2022-01-09 13:30:34,173 iteration 4908 : loss : 0.024415, loss_ce: 0.011068
2022-01-09 13:30:35,570 iteration 4909 : loss : 0.023915, loss_ce: 0.008072
2022-01-09 13:30:36,896 iteration 4910 : loss : 0.018032, loss_ce: 0.008722
2022-01-09 13:30:38,240 iteration 4911 : loss : 0.013507, loss_ce: 0.005038
2022-01-09 13:30:39,609 iteration 4912 : loss : 0.016850, loss_ce: 0.007762
2022-01-09 13:30:40,969 iteration 4913 : loss : 0.030144, loss_ce: 0.012004
 72%|████████████████████▉        | 289/400 [2:03:18<44:39, 24.14s/it]2022-01-09 13:30:42,439 iteration 4914 : loss : 0.024608, loss_ce: 0.008404
2022-01-09 13:30:43,778 iteration 4915 : loss : 0.019288, loss_ce: 0.007194
2022-01-09 13:30:45,117 iteration 4916 : loss : 0.017889, loss_ce: 0.005339
2022-01-09 13:30:46,490 iteration 4917 : loss : 0.026604, loss_ce: 0.008136
2022-01-09 13:30:47,884 iteration 4918 : loss : 0.021826, loss_ce: 0.008686
2022-01-09 13:30:49,396 iteration 4919 : loss : 0.035625, loss_ce: 0.016183
2022-01-09 13:30:50,752 iteration 4920 : loss : 0.019465, loss_ce: 0.008522
2022-01-09 13:30:52,077 iteration 4921 : loss : 0.019761, loss_ce: 0.010511
2022-01-09 13:30:53,379 iteration 4922 : loss : 0.017106, loss_ce: 0.004512
2022-01-09 13:30:54,786 iteration 4923 : loss : 0.035823, loss_ce: 0.016065
2022-01-09 13:30:56,224 iteration 4924 : loss : 0.030179, loss_ce: 0.019625
2022-01-09 13:30:57,618 iteration 4925 : loss : 0.019993, loss_ce: 0.007275
2022-01-09 13:30:59,040 iteration 4926 : loss : 0.024149, loss_ce: 0.011664
2022-01-09 13:31:00,394 iteration 4927 : loss : 0.019701, loss_ce: 0.006020
2022-01-09 13:31:01,821 iteration 4928 : loss : 0.026829, loss_ce: 0.009252
2022-01-09 13:31:03,279 iteration 4929 : loss : 0.028576, loss_ce: 0.011697
2022-01-09 13:31:03,279 Training Data Eval:
2022-01-09 13:31:10,141   Average segmentation loss on training set: 0.0134
2022-01-09 13:31:10,141 Validation Data Eval:
2022-01-09 13:31:12,518   Average segmentation loss on validation set: 0.0780
2022-01-09 13:31:13,899 iteration 4930 : loss : 0.022897, loss_ce: 0.007404
 72%|█████████████████████        | 290/400 [2:03:51<49:05, 26.77s/it]2022-01-09 13:31:15,309 iteration 4931 : loss : 0.020989, loss_ce: 0.009744
2022-01-09 13:31:16,711 iteration 4932 : loss : 0.022787, loss_ce: 0.011922
2022-01-09 13:31:18,026 iteration 4933 : loss : 0.017447, loss_ce: 0.008484
2022-01-09 13:31:19,459 iteration 4934 : loss : 0.025580, loss_ce: 0.007206
2022-01-09 13:31:20,832 iteration 4935 : loss : 0.024989, loss_ce: 0.007665
2022-01-09 13:31:22,225 iteration 4936 : loss : 0.031593, loss_ce: 0.012654
2022-01-09 13:31:23,610 iteration 4937 : loss : 0.018246, loss_ce: 0.007651
2022-01-09 13:31:25,037 iteration 4938 : loss : 0.046275, loss_ce: 0.016913
2022-01-09 13:31:26,465 iteration 4939 : loss : 0.023233, loss_ce: 0.008813
2022-01-09 13:31:27,819 iteration 4940 : loss : 0.025374, loss_ce: 0.011004
2022-01-09 13:31:29,205 iteration 4941 : loss : 0.020133, loss_ce: 0.009621
2022-01-09 13:31:30,592 iteration 4942 : loss : 0.021584, loss_ce: 0.006976
2022-01-09 13:31:32,065 iteration 4943 : loss : 0.033949, loss_ce: 0.008257
2022-01-09 13:31:33,446 iteration 4944 : loss : 0.028561, loss_ce: 0.008058
2022-01-09 13:31:34,868 iteration 4945 : loss : 0.024269, loss_ce: 0.009272
2022-01-09 13:31:36,305 iteration 4946 : loss : 0.027315, loss_ce: 0.006513
2022-01-09 13:31:37,668 iteration 4947 : loss : 0.020081, loss_ce: 0.006552
 73%|█████████████████████        | 291/400 [2:04:15<47:00, 25.87s/it]2022-01-09 13:31:39,126 iteration 4948 : loss : 0.020616, loss_ce: 0.007448
2022-01-09 13:31:40,480 iteration 4949 : loss : 0.027080, loss_ce: 0.012651
2022-01-09 13:31:41,893 iteration 4950 : loss : 0.030655, loss_ce: 0.013946
2022-01-09 13:31:43,299 iteration 4951 : loss : 0.020348, loss_ce: 0.006213
2022-01-09 13:31:44,697 iteration 4952 : loss : 0.020802, loss_ce: 0.010981
2022-01-09 13:31:46,110 iteration 4953 : loss : 0.023768, loss_ce: 0.009325
2022-01-09 13:31:47,465 iteration 4954 : loss : 0.019246, loss_ce: 0.007084
2022-01-09 13:31:48,781 iteration 4955 : loss : 0.023154, loss_ce: 0.007106
2022-01-09 13:31:50,148 iteration 4956 : loss : 0.025482, loss_ce: 0.008763
2022-01-09 13:31:51,436 iteration 4957 : loss : 0.018197, loss_ce: 0.007333
2022-01-09 13:31:52,817 iteration 4958 : loss : 0.024166, loss_ce: 0.012229
2022-01-09 13:31:54,205 iteration 4959 : loss : 0.023221, loss_ce: 0.008612
2022-01-09 13:31:55,507 iteration 4960 : loss : 0.022205, loss_ce: 0.009461
2022-01-09 13:31:56,888 iteration 4961 : loss : 0.023145, loss_ce: 0.009598
2022-01-09 13:31:58,206 iteration 4962 : loss : 0.025607, loss_ce: 0.005960
2022-01-09 13:31:59,552 iteration 4963 : loss : 0.023776, loss_ce: 0.009616
2022-01-09 13:32:00,967 iteration 4964 : loss : 0.026754, loss_ce: 0.009025
 73%|█████████████████████▏       | 292/400 [2:04:38<45:10, 25.10s/it]2022-01-09 13:32:02,466 iteration 4965 : loss : 0.033889, loss_ce: 0.014431
2022-01-09 13:32:03,903 iteration 4966 : loss : 0.021983, loss_ce: 0.008826
2022-01-09 13:32:05,288 iteration 4967 : loss : 0.023825, loss_ce: 0.010111
2022-01-09 13:32:06,689 iteration 4968 : loss : 0.023503, loss_ce: 0.011050
2022-01-09 13:32:08,175 iteration 4969 : loss : 0.051117, loss_ce: 0.012504
2022-01-09 13:32:09,511 iteration 4970 : loss : 0.018945, loss_ce: 0.007741
2022-01-09 13:32:10,871 iteration 4971 : loss : 0.025979, loss_ce: 0.013488
2022-01-09 13:32:12,284 iteration 4972 : loss : 0.024601, loss_ce: 0.011232
2022-01-09 13:32:13,671 iteration 4973 : loss : 0.035719, loss_ce: 0.013210
2022-01-09 13:32:15,174 iteration 4974 : loss : 0.026889, loss_ce: 0.010097
2022-01-09 13:32:16,545 iteration 4975 : loss : 0.026647, loss_ce: 0.008714
2022-01-09 13:32:17,863 iteration 4976 : loss : 0.026893, loss_ce: 0.010505
2022-01-09 13:32:19,269 iteration 4977 : loss : 0.029876, loss_ce: 0.007085
2022-01-09 13:32:20,631 iteration 4978 : loss : 0.021549, loss_ce: 0.005465
2022-01-09 13:32:22,007 iteration 4979 : loss : 0.027805, loss_ce: 0.009920
2022-01-09 13:32:23,395 iteration 4980 : loss : 0.039779, loss_ce: 0.015007
2022-01-09 13:32:24,763 iteration 4981 : loss : 0.031348, loss_ce: 0.013490
 73%|█████████████████████▏       | 293/400 [2:05:02<44:04, 24.71s/it]2022-01-09 13:32:26,219 iteration 4982 : loss : 0.019313, loss_ce: 0.009313
2022-01-09 13:32:27,575 iteration 4983 : loss : 0.020077, loss_ce: 0.008677
2022-01-09 13:32:29,043 iteration 4984 : loss : 0.023218, loss_ce: 0.007754
2022-01-09 13:32:30,354 iteration 4985 : loss : 0.021370, loss_ce: 0.006385
2022-01-09 13:32:31,808 iteration 4986 : loss : 0.030231, loss_ce: 0.015191
2022-01-09 13:32:33,136 iteration 4987 : loss : 0.024751, loss_ce: 0.008240
2022-01-09 13:32:34,523 iteration 4988 : loss : 0.023024, loss_ce: 0.007944
2022-01-09 13:32:35,955 iteration 4989 : loss : 0.023305, loss_ce: 0.009186
2022-01-09 13:32:37,408 iteration 4990 : loss : 0.022886, loss_ce: 0.010354
2022-01-09 13:32:38,801 iteration 4991 : loss : 0.022336, loss_ce: 0.007454
2022-01-09 13:32:40,092 iteration 4992 : loss : 0.017221, loss_ce: 0.008245
2022-01-09 13:32:41,447 iteration 4993 : loss : 0.024633, loss_ce: 0.008929
2022-01-09 13:32:42,769 iteration 4994 : loss : 0.023087, loss_ce: 0.006835
2022-01-09 13:32:44,107 iteration 4995 : loss : 0.019699, loss_ce: 0.009028
2022-01-09 13:32:45,569 iteration 4996 : loss : 0.018731, loss_ce: 0.005898
2022-01-09 13:32:46,878 iteration 4997 : loss : 0.018160, loss_ce: 0.005533
2022-01-09 13:32:48,196 iteration 4998 : loss : 0.021158, loss_ce: 0.008625
 74%|█████████████████████▎       | 294/400 [2:05:25<42:58, 24.33s/it]2022-01-09 13:32:49,599 iteration 4999 : loss : 0.025071, loss_ce: 0.009037
2022-01-09 13:32:50,955 iteration 5000 : loss : 0.025914, loss_ce: 0.009808
2022-01-09 13:32:52,309 iteration 5001 : loss : 0.022664, loss_ce: 0.011770
2022-01-09 13:32:53,659 iteration 5002 : loss : 0.013460, loss_ce: 0.004505
2022-01-09 13:32:55,051 iteration 5003 : loss : 0.056743, loss_ce: 0.008944
2022-01-09 13:32:56,415 iteration 5004 : loss : 0.022090, loss_ce: 0.009374
2022-01-09 13:32:57,783 iteration 5005 : loss : 0.034161, loss_ce: 0.016313
2022-01-09 13:32:59,210 iteration 5006 : loss : 0.022717, loss_ce: 0.009373
2022-01-09 13:33:00,618 iteration 5007 : loss : 0.038649, loss_ce: 0.016616
2022-01-09 13:33:02,036 iteration 5008 : loss : 0.041548, loss_ce: 0.016603
2022-01-09 13:33:03,454 iteration 5009 : loss : 0.025849, loss_ce: 0.007097
2022-01-09 13:33:04,862 iteration 5010 : loss : 0.029560, loss_ce: 0.010873
2022-01-09 13:33:06,218 iteration 5011 : loss : 0.018248, loss_ce: 0.008814
2022-01-09 13:33:07,609 iteration 5012 : loss : 0.029365, loss_ce: 0.013269
2022-01-09 13:33:08,925 iteration 5013 : loss : 0.021996, loss_ce: 0.008614
2022-01-09 13:33:10,288 iteration 5014 : loss : 0.028646, loss_ce: 0.011127
2022-01-09 13:33:10,288 Training Data Eval:
2022-01-09 13:33:17,158   Average segmentation loss on training set: 0.0140
2022-01-09 13:33:17,159 Validation Data Eval:
2022-01-09 13:33:19,541   Average segmentation loss on validation set: 0.0832
2022-01-09 13:33:20,906 iteration 5015 : loss : 0.030751, loss_ce: 0.012201
 74%|█████████████████████▍       | 295/400 [2:05:58<46:58, 26.84s/it]2022-01-09 13:33:22,327 iteration 5016 : loss : 0.037340, loss_ce: 0.019922
2022-01-09 13:33:23,677 iteration 5017 : loss : 0.018170, loss_ce: 0.007085
2022-01-09 13:33:25,037 iteration 5018 : loss : 0.021854, loss_ce: 0.009580
2022-01-09 13:33:26,420 iteration 5019 : loss : 0.018601, loss_ce: 0.006049
2022-01-09 13:33:27,818 iteration 5020 : loss : 0.024047, loss_ce: 0.007191
2022-01-09 13:33:29,238 iteration 5021 : loss : 0.030765, loss_ce: 0.015920
2022-01-09 13:33:30,563 iteration 5022 : loss : 0.021129, loss_ce: 0.007851
2022-01-09 13:33:31,934 iteration 5023 : loss : 0.032424, loss_ce: 0.008787
2022-01-09 13:33:33,403 iteration 5024 : loss : 0.032174, loss_ce: 0.006991
2022-01-09 13:33:34,844 iteration 5025 : loss : 0.033195, loss_ce: 0.005780
2022-01-09 13:33:36,232 iteration 5026 : loss : 0.023944, loss_ce: 0.009292
2022-01-09 13:33:37,572 iteration 5027 : loss : 0.020553, loss_ce: 0.008701
2022-01-09 13:33:38,926 iteration 5028 : loss : 0.026598, loss_ce: 0.010268
2022-01-09 13:33:40,379 iteration 5029 : loss : 0.030225, loss_ce: 0.007175
2022-01-09 13:33:41,753 iteration 5030 : loss : 0.025938, loss_ce: 0.012419
2022-01-09 13:33:43,190 iteration 5031 : loss : 0.028019, loss_ce: 0.011985
2022-01-09 13:33:44,574 iteration 5032 : loss : 0.026771, loss_ce: 0.013003
 74%|█████████████████████▍       | 296/400 [2:06:22<44:52, 25.89s/it]2022-01-09 13:33:46,115 iteration 5033 : loss : 0.029475, loss_ce: 0.010391
2022-01-09 13:33:47,479 iteration 5034 : loss : 0.027624, loss_ce: 0.008197
2022-01-09 13:33:48,824 iteration 5035 : loss : 0.027614, loss_ce: 0.011855
2022-01-09 13:33:50,176 iteration 5036 : loss : 0.021722, loss_ce: 0.008494
2022-01-09 13:33:51,558 iteration 5037 : loss : 0.024022, loss_ce: 0.009373
2022-01-09 13:33:52,905 iteration 5038 : loss : 0.040688, loss_ce: 0.008772
2022-01-09 13:33:54,357 iteration 5039 : loss : 0.026226, loss_ce: 0.012737
2022-01-09 13:33:55,740 iteration 5040 : loss : 0.023681, loss_ce: 0.007371
2022-01-09 13:33:57,181 iteration 5041 : loss : 0.024798, loss_ce: 0.008929
2022-01-09 13:33:58,518 iteration 5042 : loss : 0.020275, loss_ce: 0.007822
2022-01-09 13:33:59,892 iteration 5043 : loss : 0.023767, loss_ce: 0.010465
2022-01-09 13:34:01,273 iteration 5044 : loss : 0.024061, loss_ce: 0.010804
2022-01-09 13:34:02,696 iteration 5045 : loss : 0.029437, loss_ce: 0.013711
2022-01-09 13:34:04,088 iteration 5046 : loss : 0.043351, loss_ce: 0.010633
2022-01-09 13:34:05,447 iteration 5047 : loss : 0.028578, loss_ce: 0.007366
2022-01-09 13:34:06,795 iteration 5048 : loss : 0.020865, loss_ce: 0.008439
2022-01-09 13:34:08,148 iteration 5049 : loss : 0.024120, loss_ce: 0.011806
 74%|█████████████████████▌       | 297/400 [2:06:45<43:15, 25.20s/it]2022-01-09 13:34:09,586 iteration 5050 : loss : 0.020987, loss_ce: 0.006071
2022-01-09 13:34:10,945 iteration 5051 : loss : 0.023580, loss_ce: 0.009108
2022-01-09 13:34:12,308 iteration 5052 : loss : 0.023471, loss_ce: 0.009928
2022-01-09 13:34:13,661 iteration 5053 : loss : 0.018998, loss_ce: 0.007099
2022-01-09 13:34:15,032 iteration 5054 : loss : 0.016605, loss_ce: 0.005315
2022-01-09 13:34:16,389 iteration 5055 : loss : 0.023330, loss_ce: 0.007579
2022-01-09 13:34:17,728 iteration 5056 : loss : 0.022276, loss_ce: 0.005274
2022-01-09 13:34:19,145 iteration 5057 : loss : 0.026206, loss_ce: 0.012863
2022-01-09 13:34:20,580 iteration 5058 : loss : 0.022941, loss_ce: 0.009747
2022-01-09 13:34:21,846 iteration 5059 : loss : 0.016619, loss_ce: 0.004724
2022-01-09 13:34:23,213 iteration 5060 : loss : 0.026127, loss_ce: 0.008131
2022-01-09 13:34:24,614 iteration 5061 : loss : 0.037292, loss_ce: 0.015518
2022-01-09 13:34:26,011 iteration 5062 : loss : 0.030977, loss_ce: 0.009835
2022-01-09 13:34:27,444 iteration 5063 : loss : 0.043905, loss_ce: 0.020417
2022-01-09 13:34:28,784 iteration 5064 : loss : 0.027723, loss_ce: 0.011984
2022-01-09 13:34:30,129 iteration 5065 : loss : 0.023898, loss_ce: 0.009483
2022-01-09 13:34:31,507 iteration 5066 : loss : 0.019911, loss_ce: 0.011230
 74%|█████████████████████▌       | 298/400 [2:07:09<41:53, 24.64s/it]2022-01-09 13:34:33,024 iteration 5067 : loss : 0.026012, loss_ce: 0.008922
2022-01-09 13:34:34,421 iteration 5068 : loss : 0.028250, loss_ce: 0.011773
2022-01-09 13:34:35,811 iteration 5069 : loss : 0.037100, loss_ce: 0.013755
2022-01-09 13:34:37,173 iteration 5070 : loss : 0.022735, loss_ce: 0.008065
2022-01-09 13:34:38,608 iteration 5071 : loss : 0.028703, loss_ce: 0.011281
2022-01-09 13:34:39,878 iteration 5072 : loss : 0.019956, loss_ce: 0.006516
2022-01-09 13:34:41,237 iteration 5073 : loss : 0.022968, loss_ce: 0.011846
2022-01-09 13:34:42,648 iteration 5074 : loss : 0.026198, loss_ce: 0.007982
2022-01-09 13:34:44,023 iteration 5075 : loss : 0.016109, loss_ce: 0.006502
2022-01-09 13:34:45,448 iteration 5076 : loss : 0.027386, loss_ce: 0.013376
2022-01-09 13:34:46,798 iteration 5077 : loss : 0.020858, loss_ce: 0.008009
2022-01-09 13:34:48,184 iteration 5078 : loss : 0.020640, loss_ce: 0.006188
2022-01-09 13:34:49,534 iteration 5079 : loss : 0.020316, loss_ce: 0.008917
2022-01-09 13:34:50,915 iteration 5080 : loss : 0.019507, loss_ce: 0.006911
2022-01-09 13:34:52,318 iteration 5081 : loss : 0.027679, loss_ce: 0.013527
2022-01-09 13:34:53,748 iteration 5082 : loss : 0.048939, loss_ce: 0.029566
2022-01-09 13:34:55,142 iteration 5083 : loss : 0.034851, loss_ce: 0.012515
 75%|█████████████████████▋       | 299/400 [2:07:32<40:58, 24.34s/it]2022-01-09 13:34:56,546 iteration 5084 : loss : 0.018297, loss_ce: 0.006486
2022-01-09 13:34:57,916 iteration 5085 : loss : 0.025239, loss_ce: 0.008909
2022-01-09 13:34:59,260 iteration 5086 : loss : 0.018089, loss_ce: 0.006547
2022-01-09 13:35:00,684 iteration 5087 : loss : 0.033517, loss_ce: 0.014740
2022-01-09 13:35:02,083 iteration 5088 : loss : 0.020918, loss_ce: 0.007563
2022-01-09 13:35:03,415 iteration 5089 : loss : 0.021216, loss_ce: 0.006659
2022-01-09 13:35:04,758 iteration 5090 : loss : 0.020629, loss_ce: 0.007920
2022-01-09 13:35:06,227 iteration 5091 : loss : 0.070696, loss_ce: 0.032093
2022-01-09 13:35:07,602 iteration 5092 : loss : 0.014766, loss_ce: 0.006940
2022-01-09 13:35:08,945 iteration 5093 : loss : 0.020173, loss_ce: 0.008925
2022-01-09 13:35:10,261 iteration 5094 : loss : 0.018879, loss_ce: 0.006746
2022-01-09 13:35:11,671 iteration 5095 : loss : 0.037179, loss_ce: 0.013316
2022-01-09 13:35:13,021 iteration 5096 : loss : 0.024124, loss_ce: 0.007618
2022-01-09 13:35:14,360 iteration 5097 : loss : 0.016616, loss_ce: 0.007802
2022-01-09 13:35:15,751 iteration 5098 : loss : 0.024984, loss_ce: 0.008461
2022-01-09 13:35:17,114 iteration 5099 : loss : 0.018019, loss_ce: 0.006468
2022-01-09 13:35:17,114 Training Data Eval:
2022-01-09 13:35:23,963   Average segmentation loss on training set: 0.0213
2022-01-09 13:35:23,963 Validation Data Eval:
2022-01-09 13:35:26,325   Average segmentation loss on validation set: 0.0835
2022-01-09 13:35:27,696 iteration 5100 : loss : 0.016307, loss_ce: 0.007866
 75%|█████████████████████▊       | 300/400 [2:08:05<44:40, 26.81s/it]2022-01-09 13:35:29,146 iteration 5101 : loss : 0.030661, loss_ce: 0.014630
2022-01-09 13:35:30,486 iteration 5102 : loss : 0.019024, loss_ce: 0.008406
2022-01-09 13:35:31,844 iteration 5103 : loss : 0.028326, loss_ce: 0.012411
2022-01-09 13:35:33,285 iteration 5104 : loss : 0.018365, loss_ce: 0.007260
2022-01-09 13:35:34,607 iteration 5105 : loss : 0.023302, loss_ce: 0.008229
2022-01-09 13:35:36,044 iteration 5106 : loss : 0.018737, loss_ce: 0.006863
2022-01-09 13:35:37,373 iteration 5107 : loss : 0.016349, loss_ce: 0.006650
2022-01-09 13:35:38,773 iteration 5108 : loss : 0.023259, loss_ce: 0.007828
2022-01-09 13:35:40,156 iteration 5109 : loss : 0.023123, loss_ce: 0.007316
2022-01-09 13:35:41,521 iteration 5110 : loss : 0.030606, loss_ce: 0.016682
2022-01-09 13:35:42,925 iteration 5111 : loss : 0.016931, loss_ce: 0.005741
2022-01-09 13:35:44,252 iteration 5112 : loss : 0.023540, loss_ce: 0.007681
2022-01-09 13:35:45,684 iteration 5113 : loss : 0.063398, loss_ce: 0.009015
2022-01-09 13:35:47,039 iteration 5114 : loss : 0.033015, loss_ce: 0.016507
2022-01-09 13:35:48,379 iteration 5115 : loss : 0.038645, loss_ce: 0.013466
2022-01-09 13:35:49,722 iteration 5116 : loss : 0.017324, loss_ce: 0.006790
2022-01-09 13:35:51,150 iteration 5117 : loss : 0.023663, loss_ce: 0.011295
 75%|█████████████████████▊       | 301/400 [2:08:28<42:34, 25.80s/it]2022-01-09 13:35:52,590 iteration 5118 : loss : 0.021864, loss_ce: 0.008878
2022-01-09 13:35:53,979 iteration 5119 : loss : 0.029227, loss_ce: 0.009624
2022-01-09 13:35:55,318 iteration 5120 : loss : 0.032695, loss_ce: 0.009366
2022-01-09 13:35:56,690 iteration 5121 : loss : 0.028461, loss_ce: 0.012218
2022-01-09 13:35:58,021 iteration 5122 : loss : 0.033024, loss_ce: 0.012344
2022-01-09 13:35:59,384 iteration 5123 : loss : 0.036167, loss_ce: 0.008901
2022-01-09 13:36:00,780 iteration 5124 : loss : 0.018442, loss_ce: 0.007994
2022-01-09 13:36:02,067 iteration 5125 : loss : 0.019639, loss_ce: 0.009836
2022-01-09 13:36:03,385 iteration 5126 : loss : 0.031085, loss_ce: 0.011394
2022-01-09 13:36:04,795 iteration 5127 : loss : 0.039919, loss_ce: 0.013746
2022-01-09 13:36:06,141 iteration 5128 : loss : 0.017889, loss_ce: 0.007412
2022-01-09 13:36:07,532 iteration 5129 : loss : 0.024151, loss_ce: 0.007912
2022-01-09 13:36:08,902 iteration 5130 : loss : 0.023319, loss_ce: 0.007795
2022-01-09 13:36:10,194 iteration 5131 : loss : 0.016939, loss_ce: 0.006518
2022-01-09 13:36:11,552 iteration 5132 : loss : 0.031789, loss_ce: 0.017370
2022-01-09 13:36:12,881 iteration 5133 : loss : 0.029935, loss_ce: 0.010119
2022-01-09 13:36:14,198 iteration 5134 : loss : 0.024317, loss_ce: 0.012142
 76%|█████████████████████▉       | 302/400 [2:08:51<40:47, 24.98s/it]2022-01-09 13:36:15,765 iteration 5135 : loss : 0.031022, loss_ce: 0.013269
2022-01-09 13:36:17,132 iteration 5136 : loss : 0.028024, loss_ce: 0.008422
2022-01-09 13:36:18,554 iteration 5137 : loss : 0.024939, loss_ce: 0.008423
2022-01-09 13:36:19,890 iteration 5138 : loss : 0.020725, loss_ce: 0.008575
2022-01-09 13:36:21,265 iteration 5139 : loss : 0.017062, loss_ce: 0.006631
2022-01-09 13:36:22,716 iteration 5140 : loss : 0.023352, loss_ce: 0.008968
2022-01-09 13:36:24,046 iteration 5141 : loss : 0.026446, loss_ce: 0.009141
2022-01-09 13:36:25,426 iteration 5142 : loss : 0.025453, loss_ce: 0.008710
2022-01-09 13:36:26,721 iteration 5143 : loss : 0.018646, loss_ce: 0.007273
2022-01-09 13:36:28,117 iteration 5144 : loss : 0.027900, loss_ce: 0.013922
2022-01-09 13:36:29,406 iteration 5145 : loss : 0.018514, loss_ce: 0.006279
2022-01-09 13:36:30,840 iteration 5146 : loss : 0.029954, loss_ce: 0.014042
2022-01-09 13:36:32,277 iteration 5147 : loss : 0.022840, loss_ce: 0.008785
2022-01-09 13:36:33,626 iteration 5148 : loss : 0.020604, loss_ce: 0.008291
2022-01-09 13:36:34,979 iteration 5149 : loss : 0.032368, loss_ce: 0.010657
2022-01-09 13:36:36,327 iteration 5150 : loss : 0.015505, loss_ce: 0.006570
2022-01-09 13:36:37,703 iteration 5151 : loss : 0.028027, loss_ce: 0.006139
 76%|█████████████████████▉       | 303/400 [2:09:15<39:39, 24.53s/it]2022-01-09 13:36:39,080 iteration 5152 : loss : 0.025062, loss_ce: 0.005239
2022-01-09 13:36:40,359 iteration 5153 : loss : 0.015697, loss_ce: 0.006202
2022-01-09 13:36:41,791 iteration 5154 : loss : 0.020721, loss_ce: 0.007455
2022-01-09 13:36:43,102 iteration 5155 : loss : 0.015693, loss_ce: 0.005587
2022-01-09 13:36:44,448 iteration 5156 : loss : 0.023717, loss_ce: 0.010005
2022-01-09 13:36:45,749 iteration 5157 : loss : 0.017305, loss_ce: 0.005426
2022-01-09 13:36:47,094 iteration 5158 : loss : 0.023191, loss_ce: 0.008375
2022-01-09 13:36:48,509 iteration 5159 : loss : 0.021679, loss_ce: 0.007771
2022-01-09 13:36:49,819 iteration 5160 : loss : 0.017204, loss_ce: 0.006187
2022-01-09 13:36:51,173 iteration 5161 : loss : 0.029186, loss_ce: 0.011736
2022-01-09 13:36:52,483 iteration 5162 : loss : 0.018797, loss_ce: 0.007664
2022-01-09 13:36:53,862 iteration 5163 : loss : 0.022809, loss_ce: 0.009054
2022-01-09 13:36:55,247 iteration 5164 : loss : 0.022553, loss_ce: 0.011277
2022-01-09 13:36:56,553 iteration 5165 : loss : 0.021076, loss_ce: 0.006920
2022-01-09 13:36:57,913 iteration 5166 : loss : 0.018481, loss_ce: 0.007763
2022-01-09 13:36:59,293 iteration 5167 : loss : 0.025096, loss_ce: 0.008067
2022-01-09 13:37:00,715 iteration 5168 : loss : 0.044392, loss_ce: 0.022999
 76%|██████████████████████       | 304/400 [2:09:38<38:31, 24.08s/it]2022-01-09 13:37:02,255 iteration 5169 : loss : 0.032643, loss_ce: 0.015258
2022-01-09 13:37:03,575 iteration 5170 : loss : 0.022215, loss_ce: 0.006754
2022-01-09 13:37:05,004 iteration 5171 : loss : 0.019357, loss_ce: 0.007106
2022-01-09 13:37:06,301 iteration 5172 : loss : 0.017680, loss_ce: 0.004978
2022-01-09 13:37:07,544 iteration 5173 : loss : 0.015262, loss_ce: 0.006596
2022-01-09 13:37:08,977 iteration 5174 : loss : 0.025654, loss_ce: 0.008669
2022-01-09 13:37:10,306 iteration 5175 : loss : 0.020746, loss_ce: 0.010119
2022-01-09 13:37:11,794 iteration 5176 : loss : 0.034893, loss_ce: 0.014363
2022-01-09 13:37:13,127 iteration 5177 : loss : 0.020735, loss_ce: 0.009330
2022-01-09 13:37:14,482 iteration 5178 : loss : 0.016310, loss_ce: 0.004630
2022-01-09 13:37:15,901 iteration 5179 : loss : 0.018124, loss_ce: 0.005529
2022-01-09 13:37:17,273 iteration 5180 : loss : 0.019804, loss_ce: 0.007261
2022-01-09 13:37:18,601 iteration 5181 : loss : 0.019147, loss_ce: 0.005915
2022-01-09 13:37:19,967 iteration 5182 : loss : 0.018314, loss_ce: 0.006906
2022-01-09 13:37:21,313 iteration 5183 : loss : 0.033724, loss_ce: 0.015192
2022-01-09 13:37:22,641 iteration 5184 : loss : 0.024915, loss_ce: 0.009019
2022-01-09 13:37:22,641 Training Data Eval:
2022-01-09 13:37:29,512   Average segmentation loss on training set: 0.0133
2022-01-09 13:37:29,512 Validation Data Eval:
2022-01-09 13:37:31,891   Average segmentation loss on validation set: 0.0838
2022-01-09 13:37:33,246 iteration 5185 : loss : 0.033748, loss_ce: 0.011447
 76%|██████████████████████       | 305/400 [2:10:10<42:08, 26.61s/it]2022-01-09 13:37:34,686 iteration 5186 : loss : 0.020846, loss_ce: 0.009786
2022-01-09 13:37:36,005 iteration 5187 : loss : 0.020908, loss_ce: 0.008870
2022-01-09 13:37:37,333 iteration 5188 : loss : 0.014299, loss_ce: 0.005913
2022-01-09 13:37:38,669 iteration 5189 : loss : 0.015805, loss_ce: 0.007260
2022-01-09 13:37:39,988 iteration 5190 : loss : 0.019917, loss_ce: 0.008498
2022-01-09 13:37:41,404 iteration 5191 : loss : 0.026078, loss_ce: 0.010749
2022-01-09 13:37:42,801 iteration 5192 : loss : 0.018414, loss_ce: 0.005885
2022-01-09 13:37:44,120 iteration 5193 : loss : 0.014015, loss_ce: 0.004764
2022-01-09 13:37:45,518 iteration 5194 : loss : 0.020736, loss_ce: 0.008275
2022-01-09 13:37:46,858 iteration 5195 : loss : 0.019685, loss_ce: 0.007228
2022-01-09 13:37:48,132 iteration 5196 : loss : 0.018365, loss_ce: 0.004906
2022-01-09 13:37:49,443 iteration 5197 : loss : 0.020131, loss_ce: 0.006871
2022-01-09 13:37:50,769 iteration 5198 : loss : 0.019257, loss_ce: 0.010366
2022-01-09 13:37:52,159 iteration 5199 : loss : 0.014802, loss_ce: 0.005193
2022-01-09 13:37:53,522 iteration 5200 : loss : 0.020971, loss_ce: 0.006504
2022-01-09 13:37:54,901 iteration 5201 : loss : 0.017528, loss_ce: 0.005882
2022-01-09 13:37:56,343 iteration 5202 : loss : 0.025526, loss_ce: 0.012208
 76%|██████████████████████▏      | 306/400 [2:10:34<40:02, 25.56s/it]2022-01-09 13:37:57,740 iteration 5203 : loss : 0.016179, loss_ce: 0.006010
2022-01-09 13:37:59,085 iteration 5204 : loss : 0.021847, loss_ce: 0.009571
2022-01-09 13:38:00,572 iteration 5205 : loss : 0.022538, loss_ce: 0.009549
2022-01-09 13:38:01,939 iteration 5206 : loss : 0.016974, loss_ce: 0.005987
2022-01-09 13:38:03,314 iteration 5207 : loss : 0.026470, loss_ce: 0.011130
2022-01-09 13:38:04,757 iteration 5208 : loss : 0.018941, loss_ce: 0.008712
2022-01-09 13:38:06,080 iteration 5209 : loss : 0.017789, loss_ce: 0.007798
2022-01-09 13:38:07,489 iteration 5210 : loss : 0.018925, loss_ce: 0.006630
2022-01-09 13:38:08,874 iteration 5211 : loss : 0.022991, loss_ce: 0.009821
2022-01-09 13:38:10,245 iteration 5212 : loss : 0.022490, loss_ce: 0.008061
2022-01-09 13:38:11,513 iteration 5213 : loss : 0.016374, loss_ce: 0.006100
2022-01-09 13:38:12,970 iteration 5214 : loss : 0.028809, loss_ce: 0.013295
2022-01-09 13:38:14,371 iteration 5215 : loss : 0.026186, loss_ce: 0.008793
2022-01-09 13:38:15,795 iteration 5216 : loss : 0.023719, loss_ce: 0.008838
2022-01-09 13:38:17,247 iteration 5217 : loss : 0.022191, loss_ce: 0.007642
2022-01-09 13:38:18,583 iteration 5218 : loss : 0.029056, loss_ce: 0.004883
2022-01-09 13:38:20,044 iteration 5219 : loss : 0.020726, loss_ce: 0.006121
 77%|██████████████████████▎      | 307/400 [2:10:57<38:45, 25.00s/it]2022-01-09 13:38:21,463 iteration 5220 : loss : 0.021355, loss_ce: 0.006429
2022-01-09 13:38:22,824 iteration 5221 : loss : 0.019789, loss_ce: 0.006773
2022-01-09 13:38:24,140 iteration 5222 : loss : 0.016173, loss_ce: 0.006330
2022-01-09 13:38:25,548 iteration 5223 : loss : 0.024185, loss_ce: 0.009125
2022-01-09 13:38:26,908 iteration 5224 : loss : 0.033735, loss_ce: 0.012242
2022-01-09 13:38:28,332 iteration 5225 : loss : 0.017498, loss_ce: 0.005297
2022-01-09 13:38:29,689 iteration 5226 : loss : 0.020404, loss_ce: 0.008116
2022-01-09 13:38:31,139 iteration 5227 : loss : 0.025031, loss_ce: 0.011028
2022-01-09 13:38:32,473 iteration 5228 : loss : 0.017700, loss_ce: 0.006267
2022-01-09 13:38:33,958 iteration 5229 : loss : 0.035201, loss_ce: 0.016093
2022-01-09 13:38:35,230 iteration 5230 : loss : 0.015177, loss_ce: 0.006627
2022-01-09 13:38:36,623 iteration 5231 : loss : 0.025276, loss_ce: 0.013060
2022-01-09 13:38:38,004 iteration 5232 : loss : 0.019539, loss_ce: 0.006075
2022-01-09 13:38:39,412 iteration 5233 : loss : 0.020910, loss_ce: 0.007546
2022-01-09 13:38:40,755 iteration 5234 : loss : 0.018201, loss_ce: 0.007633
2022-01-09 13:38:42,138 iteration 5235 : loss : 0.021631, loss_ce: 0.007628
2022-01-09 13:38:43,558 iteration 5236 : loss : 0.022827, loss_ce: 0.008210
 77%|██████████████████████▎      | 308/400 [2:11:21<37:39, 24.56s/it]2022-01-09 13:38:44,871 iteration 5237 : loss : 0.015035, loss_ce: 0.006310
2022-01-09 13:38:46,273 iteration 5238 : loss : 0.016917, loss_ce: 0.007425
2022-01-09 13:38:47,666 iteration 5239 : loss : 0.025492, loss_ce: 0.008038
2022-01-09 13:38:49,079 iteration 5240 : loss : 0.175338, loss_ce: 0.005701
2022-01-09 13:38:50,420 iteration 5241 : loss : 0.017765, loss_ce: 0.007666
2022-01-09 13:38:51,795 iteration 5242 : loss : 0.023745, loss_ce: 0.010256
2022-01-09 13:38:53,252 iteration 5243 : loss : 0.040950, loss_ce: 0.014781
2022-01-09 13:38:54,663 iteration 5244 : loss : 0.017059, loss_ce: 0.007055
2022-01-09 13:38:56,050 iteration 5245 : loss : 0.019402, loss_ce: 0.006606
2022-01-09 13:38:57,447 iteration 5246 : loss : 0.030822, loss_ce: 0.008685
2022-01-09 13:38:58,748 iteration 5247 : loss : 0.014700, loss_ce: 0.005863
2022-01-09 13:39:00,154 iteration 5248 : loss : 0.023405, loss_ce: 0.009246
2022-01-09 13:39:01,612 iteration 5249 : loss : 0.023100, loss_ce: 0.007836
2022-01-09 13:39:03,014 iteration 5250 : loss : 0.025487, loss_ce: 0.011870
2022-01-09 13:39:04,317 iteration 5251 : loss : 0.015976, loss_ce: 0.005572
2022-01-09 13:39:05,695 iteration 5252 : loss : 0.017903, loss_ce: 0.007239
2022-01-09 13:39:07,100 iteration 5253 : loss : 0.038911, loss_ce: 0.009645
 77%|██████████████████████▍      | 309/400 [2:11:44<36:46, 24.25s/it]2022-01-09 13:39:08,524 iteration 5254 : loss : 0.013259, loss_ce: 0.004639
2022-01-09 13:39:09,893 iteration 5255 : loss : 0.026495, loss_ce: 0.014318
2022-01-09 13:39:11,203 iteration 5256 : loss : 0.016791, loss_ce: 0.007202
2022-01-09 13:39:12,586 iteration 5257 : loss : 0.020855, loss_ce: 0.008168
2022-01-09 13:39:13,996 iteration 5258 : loss : 0.014308, loss_ce: 0.004565
2022-01-09 13:39:15,319 iteration 5259 : loss : 0.019138, loss_ce: 0.004682
2022-01-09 13:39:16,708 iteration 5260 : loss : 0.017315, loss_ce: 0.006873
2022-01-09 13:39:18,107 iteration 5261 : loss : 0.029194, loss_ce: 0.013464
2022-01-09 13:39:19,482 iteration 5262 : loss : 0.016480, loss_ce: 0.007535
2022-01-09 13:39:20,865 iteration 5263 : loss : 0.022599, loss_ce: 0.010342
2022-01-09 13:39:22,192 iteration 5264 : loss : 0.019940, loss_ce: 0.010672
2022-01-09 13:39:23,604 iteration 5265 : loss : 0.025389, loss_ce: 0.012650
2022-01-09 13:39:25,059 iteration 5266 : loss : 0.021949, loss_ce: 0.008088
2022-01-09 13:39:26,432 iteration 5267 : loss : 0.037547, loss_ce: 0.010504
2022-01-09 13:39:27,821 iteration 5268 : loss : 0.025041, loss_ce: 0.007534
2022-01-09 13:39:29,304 iteration 5269 : loss : 0.028973, loss_ce: 0.008758
2022-01-09 13:39:29,304 Training Data Eval:
2022-01-09 13:39:36,179   Average segmentation loss on training set: 0.0124
2022-01-09 13:39:36,179 Validation Data Eval:
2022-01-09 13:39:38,551   Average segmentation loss on validation set: 0.0953
2022-01-09 13:39:39,921 iteration 5270 : loss : 0.016614, loss_ce: 0.007191
 78%|██████████████████████▍      | 310/400 [2:12:17<40:13, 26.82s/it]2022-01-09 13:39:41,328 iteration 5271 : loss : 0.016345, loss_ce: 0.006351
2022-01-09 13:39:42,725 iteration 5272 : loss : 0.033826, loss_ce: 0.015210
2022-01-09 13:39:44,067 iteration 5273 : loss : 0.017704, loss_ce: 0.005512
2022-01-09 13:39:45,423 iteration 5274 : loss : 0.018347, loss_ce: 0.007252
2022-01-09 13:39:46,722 iteration 5275 : loss : 0.015922, loss_ce: 0.007384
2022-01-09 13:39:48,114 iteration 5276 : loss : 0.024762, loss_ce: 0.009821
2022-01-09 13:39:49,492 iteration 5277 : loss : 0.020149, loss_ce: 0.007525
2022-01-09 13:39:50,782 iteration 5278 : loss : 0.016189, loss_ce: 0.006207
2022-01-09 13:39:52,151 iteration 5279 : loss : 0.021900, loss_ce: 0.009323
2022-01-09 13:39:53,646 iteration 5280 : loss : 0.032043, loss_ce: 0.013062
2022-01-09 13:39:55,002 iteration 5281 : loss : 0.018441, loss_ce: 0.006402
2022-01-09 13:39:56,310 iteration 5282 : loss : 0.013538, loss_ce: 0.004344
2022-01-09 13:39:57,685 iteration 5283 : loss : 0.033602, loss_ce: 0.010436
2022-01-09 13:39:59,077 iteration 5284 : loss : 0.025097, loss_ce: 0.008486
2022-01-09 13:40:00,443 iteration 5285 : loss : 0.016130, loss_ce: 0.007090
2022-01-09 13:40:01,739 iteration 5286 : loss : 0.015334, loss_ce: 0.006591
2022-01-09 13:40:03,097 iteration 5287 : loss : 0.019248, loss_ce: 0.003970
 78%|██████████████████████▌      | 311/400 [2:12:40<38:09, 25.73s/it]2022-01-09 13:40:04,563 iteration 5288 : loss : 0.021843, loss_ce: 0.008937
2022-01-09 13:40:05,968 iteration 5289 : loss : 0.029427, loss_ce: 0.009916
2022-01-09 13:40:07,330 iteration 5290 : loss : 0.027870, loss_ce: 0.006019
2022-01-09 13:40:08,679 iteration 5291 : loss : 0.028541, loss_ce: 0.010318
2022-01-09 13:40:10,086 iteration 5292 : loss : 0.026277, loss_ce: 0.009874
2022-01-09 13:40:11,472 iteration 5293 : loss : 0.017923, loss_ce: 0.007769
2022-01-09 13:40:12,891 iteration 5294 : loss : 0.022306, loss_ce: 0.008269
2022-01-09 13:40:14,355 iteration 5295 : loss : 0.029375, loss_ce: 0.010883
2022-01-09 13:40:15,769 iteration 5296 : loss : 0.029139, loss_ce: 0.007104
2022-01-09 13:40:17,189 iteration 5297 : loss : 0.017234, loss_ce: 0.005265
2022-01-09 13:40:18,610 iteration 5298 : loss : 0.028101, loss_ce: 0.009846
2022-01-09 13:40:20,026 iteration 5299 : loss : 0.021171, loss_ce: 0.007505
2022-01-09 13:40:21,455 iteration 5300 : loss : 0.021447, loss_ce: 0.007521
2022-01-09 13:40:22,777 iteration 5301 : loss : 0.021768, loss_ce: 0.007495
2022-01-09 13:40:24,096 iteration 5302 : loss : 0.017277, loss_ce: 0.008215
2022-01-09 13:40:25,421 iteration 5303 : loss : 0.018458, loss_ce: 0.006809
2022-01-09 13:40:26,726 iteration 5304 : loss : 0.027673, loss_ce: 0.009204
 78%|██████████████████████▌      | 312/400 [2:13:04<36:48, 25.10s/it]2022-01-09 13:40:28,181 iteration 5305 : loss : 0.030693, loss_ce: 0.014296
2022-01-09 13:40:29,655 iteration 5306 : loss : 0.041337, loss_ce: 0.022078
2022-01-09 13:40:31,145 iteration 5307 : loss : 0.027108, loss_ce: 0.011069
2022-01-09 13:40:32,597 iteration 5308 : loss : 0.056826, loss_ce: 0.024666
2022-01-09 13:40:34,034 iteration 5309 : loss : 0.025452, loss_ce: 0.008754
2022-01-09 13:40:35,392 iteration 5310 : loss : 0.020616, loss_ce: 0.009992
2022-01-09 13:40:36,730 iteration 5311 : loss : 0.030335, loss_ce: 0.007993
2022-01-09 13:40:38,164 iteration 5312 : loss : 0.025024, loss_ce: 0.010429
2022-01-09 13:40:39,649 iteration 5313 : loss : 0.027123, loss_ce: 0.010785
2022-01-09 13:40:41,044 iteration 5314 : loss : 0.020304, loss_ce: 0.006585
2022-01-09 13:40:42,491 iteration 5315 : loss : 0.030991, loss_ce: 0.008452
2022-01-09 13:40:43,917 iteration 5316 : loss : 0.024280, loss_ce: 0.009778
2022-01-09 13:40:45,292 iteration 5317 : loss : 0.021609, loss_ce: 0.008911
2022-01-09 13:40:46,640 iteration 5318 : loss : 0.016240, loss_ce: 0.006350
2022-01-09 13:40:48,074 iteration 5319 : loss : 0.024387, loss_ce: 0.009106
2022-01-09 13:40:49,498 iteration 5320 : loss : 0.024949, loss_ce: 0.010294
2022-01-09 13:40:50,942 iteration 5321 : loss : 0.031732, loss_ce: 0.013056
 78%|██████████████████████▋      | 313/400 [2:13:28<36:00, 24.83s/it]2022-01-09 13:40:52,307 iteration 5322 : loss : 0.015923, loss_ce: 0.006837
2022-01-09 13:40:53,701 iteration 5323 : loss : 0.026573, loss_ce: 0.011114
2022-01-09 13:40:55,023 iteration 5324 : loss : 0.020912, loss_ce: 0.006952
2022-01-09 13:40:56,370 iteration 5325 : loss : 0.022794, loss_ce: 0.005620
2022-01-09 13:40:57,781 iteration 5326 : loss : 0.023768, loss_ce: 0.008589
2022-01-09 13:40:59,121 iteration 5327 : loss : 0.019168, loss_ce: 0.006969
2022-01-09 13:41:00,480 iteration 5328 : loss : 0.020894, loss_ce: 0.011351
2022-01-09 13:41:01,901 iteration 5329 : loss : 0.025986, loss_ce: 0.009569
2022-01-09 13:41:03,301 iteration 5330 : loss : 0.034265, loss_ce: 0.011239
2022-01-09 13:41:04,678 iteration 5331 : loss : 0.021260, loss_ce: 0.006467
2022-01-09 13:41:06,063 iteration 5332 : loss : 0.022483, loss_ce: 0.006979
2022-01-09 13:41:07,480 iteration 5333 : loss : 0.021597, loss_ce: 0.009992
2022-01-09 13:41:08,879 iteration 5334 : loss : 0.030223, loss_ce: 0.011959
2022-01-09 13:41:10,301 iteration 5335 : loss : 0.029303, loss_ce: 0.009790
2022-01-09 13:41:11,693 iteration 5336 : loss : 0.021884, loss_ce: 0.007762
2022-01-09 13:41:13,037 iteration 5337 : loss : 0.018576, loss_ce: 0.006469
2022-01-09 13:41:14,382 iteration 5338 : loss : 0.019143, loss_ce: 0.008619
 78%|██████████████████████▊      | 314/400 [2:13:52<34:59, 24.41s/it]2022-01-09 13:41:15,970 iteration 5339 : loss : 0.024948, loss_ce: 0.010152
2022-01-09 13:41:17,404 iteration 5340 : loss : 0.028308, loss_ce: 0.014243
2022-01-09 13:41:18,808 iteration 5341 : loss : 0.024031, loss_ce: 0.004590
2022-01-09 13:41:20,199 iteration 5342 : loss : 0.017324, loss_ce: 0.008745
2022-01-09 13:41:21,551 iteration 5343 : loss : 0.018642, loss_ce: 0.005689
2022-01-09 13:41:22,950 iteration 5344 : loss : 0.021562, loss_ce: 0.006980
2022-01-09 13:41:24,324 iteration 5345 : loss : 0.026221, loss_ce: 0.008656
2022-01-09 13:41:25,741 iteration 5346 : loss : 0.026046, loss_ce: 0.009631
2022-01-09 13:41:27,161 iteration 5347 : loss : 0.018711, loss_ce: 0.006212
2022-01-09 13:41:28,593 iteration 5348 : loss : 0.022185, loss_ce: 0.005886
2022-01-09 13:41:29,988 iteration 5349 : loss : 0.026297, loss_ce: 0.009554
2022-01-09 13:41:31,304 iteration 5350 : loss : 0.015818, loss_ce: 0.006113
2022-01-09 13:41:32,628 iteration 5351 : loss : 0.026586, loss_ce: 0.009580
2022-01-09 13:41:34,051 iteration 5352 : loss : 0.026430, loss_ce: 0.010915
2022-01-09 13:41:35,476 iteration 5353 : loss : 0.013924, loss_ce: 0.005133
2022-01-09 13:41:36,826 iteration 5354 : loss : 0.018928, loss_ce: 0.008906
2022-01-09 13:41:36,826 Training Data Eval:
2022-01-09 13:41:43,685   Average segmentation loss on training set: 0.0124
2022-01-09 13:41:43,685 Validation Data Eval:
2022-01-09 13:41:46,067   Average segmentation loss on validation set: 0.0972
2022-01-09 13:41:47,386 iteration 5355 : loss : 0.021553, loss_ce: 0.008252
 79%|██████████████████████▊      | 315/400 [2:14:25<38:14, 26.99s/it]2022-01-09 13:41:48,830 iteration 5356 : loss : 0.022772, loss_ce: 0.009626
2022-01-09 13:41:50,144 iteration 5357 : loss : 0.032306, loss_ce: 0.006531
2022-01-09 13:41:51,647 iteration 5358 : loss : 0.029491, loss_ce: 0.015217
2022-01-09 13:41:52,957 iteration 5359 : loss : 0.016301, loss_ce: 0.005789
2022-01-09 13:41:54,355 iteration 5360 : loss : 0.019486, loss_ce: 0.007760
2022-01-09 13:41:55,751 iteration 5361 : loss : 0.029058, loss_ce: 0.009799
2022-01-09 13:41:57,161 iteration 5362 : loss : 0.026543, loss_ce: 0.008304
2022-01-09 13:41:58,548 iteration 5363 : loss : 0.021850, loss_ce: 0.008954
2022-01-09 13:41:59,936 iteration 5364 : loss : 0.029703, loss_ce: 0.006255
2022-01-09 13:42:01,302 iteration 5365 : loss : 0.014646, loss_ce: 0.005704
2022-01-09 13:42:02,699 iteration 5366 : loss : 0.030642, loss_ce: 0.013592
2022-01-09 13:42:04,127 iteration 5367 : loss : 0.023992, loss_ce: 0.011900
2022-01-09 13:42:05,492 iteration 5368 : loss : 0.013771, loss_ce: 0.005966
2022-01-09 13:42:06,887 iteration 5369 : loss : 0.016005, loss_ce: 0.005590
2022-01-09 13:42:08,254 iteration 5370 : loss : 0.014190, loss_ce: 0.005484
2022-01-09 13:42:09,641 iteration 5371 : loss : 0.039152, loss_ce: 0.018320
2022-01-09 13:42:10,994 iteration 5372 : loss : 0.021412, loss_ce: 0.006551
 79%|██████████████████████▉      | 316/400 [2:14:48<36:22, 25.98s/it]2022-01-09 13:42:12,428 iteration 5373 : loss : 0.020418, loss_ce: 0.010008
2022-01-09 13:42:13,770 iteration 5374 : loss : 0.020464, loss_ce: 0.008534
2022-01-09 13:42:15,173 iteration 5375 : loss : 0.015457, loss_ce: 0.005063
2022-01-09 13:42:16,529 iteration 5376 : loss : 0.021686, loss_ce: 0.007678
2022-01-09 13:42:17,875 iteration 5377 : loss : 0.014233, loss_ce: 0.004674
2022-01-09 13:42:19,256 iteration 5378 : loss : 0.035892, loss_ce: 0.012612
2022-01-09 13:42:20,572 iteration 5379 : loss : 0.015730, loss_ce: 0.005592
2022-01-09 13:42:21,946 iteration 5380 : loss : 0.018330, loss_ce: 0.007366
2022-01-09 13:42:23,293 iteration 5381 : loss : 0.041243, loss_ce: 0.019546
2022-01-09 13:42:24,690 iteration 5382 : loss : 0.019620, loss_ce: 0.006222
2022-01-09 13:42:26,151 iteration 5383 : loss : 0.034135, loss_ce: 0.016224
2022-01-09 13:42:27,681 iteration 5384 : loss : 0.033585, loss_ce: 0.013615
2022-01-09 13:42:29,043 iteration 5385 : loss : 0.019560, loss_ce: 0.007471
2022-01-09 13:42:30,345 iteration 5386 : loss : 0.016415, loss_ce: 0.005608
2022-01-09 13:42:31,699 iteration 5387 : loss : 0.025054, loss_ce: 0.010258
2022-01-09 13:42:33,120 iteration 5388 : loss : 0.027451, loss_ce: 0.009667
2022-01-09 13:42:34,612 iteration 5389 : loss : 0.043701, loss_ce: 0.014175
 79%|██████████████████████▉      | 317/400 [2:15:12<34:57, 25.27s/it]2022-01-09 13:42:36,032 iteration 5390 : loss : 0.027696, loss_ce: 0.009974
2022-01-09 13:42:37,456 iteration 5391 : loss : 0.023235, loss_ce: 0.008258
2022-01-09 13:42:38,890 iteration 5392 : loss : 0.019855, loss_ce: 0.010346
2022-01-09 13:42:40,275 iteration 5393 : loss : 0.028834, loss_ce: 0.010586
2022-01-09 13:42:41,625 iteration 5394 : loss : 0.021212, loss_ce: 0.009761
2022-01-09 13:42:43,032 iteration 5395 : loss : 0.024303, loss_ce: 0.008287
2022-01-09 13:42:44,364 iteration 5396 : loss : 0.019269, loss_ce: 0.010635
2022-01-09 13:42:45,799 iteration 5397 : loss : 0.026409, loss_ce: 0.009194
2022-01-09 13:42:47,194 iteration 5398 : loss : 0.026860, loss_ce: 0.014495
2022-01-09 13:42:48,508 iteration 5399 : loss : 0.025110, loss_ce: 0.007687
2022-01-09 13:42:49,969 iteration 5400 : loss : 0.039164, loss_ce: 0.011300
2022-01-09 13:42:51,384 iteration 5401 : loss : 0.019999, loss_ce: 0.009423
2022-01-09 13:42:52,773 iteration 5402 : loss : 0.024430, loss_ce: 0.008851
2022-01-09 13:42:54,124 iteration 5403 : loss : 0.021756, loss_ce: 0.009634
2022-01-09 13:42:55,457 iteration 5404 : loss : 0.021264, loss_ce: 0.005698
2022-01-09 13:42:56,809 iteration 5405 : loss : 0.016769, loss_ce: 0.007238
2022-01-09 13:42:58,232 iteration 5406 : loss : 0.021639, loss_ce: 0.005752
 80%|███████████████████████      | 318/400 [2:15:35<33:51, 24.77s/it]2022-01-09 13:42:59,702 iteration 5407 : loss : 0.020277, loss_ce: 0.007981
2022-01-09 13:43:01,074 iteration 5408 : loss : 0.025631, loss_ce: 0.008347
2022-01-09 13:43:02,526 iteration 5409 : loss : 0.023231, loss_ce: 0.009873
2022-01-09 13:43:03,870 iteration 5410 : loss : 0.022751, loss_ce: 0.010014
2022-01-09 13:43:05,241 iteration 5411 : loss : 0.021097, loss_ce: 0.008277
2022-01-09 13:43:06,633 iteration 5412 : loss : 0.021454, loss_ce: 0.010017
2022-01-09 13:43:07,986 iteration 5413 : loss : 0.025701, loss_ce: 0.012720
2022-01-09 13:43:09,375 iteration 5414 : loss : 0.019245, loss_ce: 0.008082
2022-01-09 13:43:10,834 iteration 5415 : loss : 0.037340, loss_ce: 0.014758
2022-01-09 13:43:12,152 iteration 5416 : loss : 0.012319, loss_ce: 0.004232
2022-01-09 13:43:13,562 iteration 5417 : loss : 0.020144, loss_ce: 0.007152
2022-01-09 13:43:14,921 iteration 5418 : loss : 0.024173, loss_ce: 0.009580
2022-01-09 13:43:16,329 iteration 5419 : loss : 0.022013, loss_ce: 0.005966
2022-01-09 13:43:17,770 iteration 5420 : loss : 0.027420, loss_ce: 0.011758
2022-01-09 13:43:19,169 iteration 5421 : loss : 0.029547, loss_ce: 0.012162
2022-01-09 13:43:20,434 iteration 5422 : loss : 0.015554, loss_ce: 0.006455
2022-01-09 13:43:21,840 iteration 5423 : loss : 0.038234, loss_ce: 0.008089
 80%|███████████████████████▏     | 319/400 [2:15:59<32:58, 24.42s/it]2022-01-09 13:43:23,336 iteration 5424 : loss : 0.018070, loss_ce: 0.006588
2022-01-09 13:43:24,681 iteration 5425 : loss : 0.017369, loss_ce: 0.005528
2022-01-09 13:43:26,049 iteration 5426 : loss : 0.021525, loss_ce: 0.006254
2022-01-09 13:43:27,342 iteration 5427 : loss : 0.020625, loss_ce: 0.009124
2022-01-09 13:43:28,687 iteration 5428 : loss : 0.021381, loss_ce: 0.008286
2022-01-09 13:43:30,075 iteration 5429 : loss : 0.018894, loss_ce: 0.007565
2022-01-09 13:43:31,452 iteration 5430 : loss : 0.038093, loss_ce: 0.022482
2022-01-09 13:43:32,763 iteration 5431 : loss : 0.021569, loss_ce: 0.007700
2022-01-09 13:43:34,206 iteration 5432 : loss : 0.021660, loss_ce: 0.009358
2022-01-09 13:43:35,639 iteration 5433 : loss : 0.024599, loss_ce: 0.010990
2022-01-09 13:43:36,969 iteration 5434 : loss : 0.020279, loss_ce: 0.008744
2022-01-09 13:43:38,409 iteration 5435 : loss : 0.034444, loss_ce: 0.013252
2022-01-09 13:43:39,811 iteration 5436 : loss : 0.019659, loss_ce: 0.007179
2022-01-09 13:43:41,228 iteration 5437 : loss : 0.035307, loss_ce: 0.008524
2022-01-09 13:43:42,654 iteration 5438 : loss : 0.018673, loss_ce: 0.007817
2022-01-09 13:43:44,015 iteration 5439 : loss : 0.020917, loss_ce: 0.009114
2022-01-09 13:43:44,016 Training Data Eval:
2022-01-09 13:43:50,868   Average segmentation loss on training set: 0.0130
2022-01-09 13:43:50,869 Validation Data Eval:
2022-01-09 13:43:53,237   Average segmentation loss on validation set: 0.0674
2022-01-09 13:43:54,554 iteration 5440 : loss : 0.022376, loss_ce: 0.007382
 80%|███████████████████████▏     | 320/400 [2:16:32<35:52, 26.91s/it]2022-01-09 13:43:55,961 iteration 5441 : loss : 0.016680, loss_ce: 0.006503
2022-01-09 13:43:57,288 iteration 5442 : loss : 0.020790, loss_ce: 0.007666
2022-01-09 13:43:58,647 iteration 5443 : loss : 0.023478, loss_ce: 0.008736
2022-01-09 13:43:59,943 iteration 5444 : loss : 0.016062, loss_ce: 0.005047
2022-01-09 13:44:01,324 iteration 5445 : loss : 0.021503, loss_ce: 0.008557
2022-01-09 13:44:02,654 iteration 5446 : loss : 0.019739, loss_ce: 0.009558
2022-01-09 13:44:04,109 iteration 5447 : loss : 0.027729, loss_ce: 0.009390
2022-01-09 13:44:05,437 iteration 5448 : loss : 0.020398, loss_ce: 0.008083
2022-01-09 13:44:06,820 iteration 5449 : loss : 0.022972, loss_ce: 0.011819
2022-01-09 13:44:08,185 iteration 5450 : loss : 0.019658, loss_ce: 0.007848
2022-01-09 13:44:09,500 iteration 5451 : loss : 0.015643, loss_ce: 0.005645
2022-01-09 13:44:10,878 iteration 5452 : loss : 0.030324, loss_ce: 0.011403
2022-01-09 13:44:12,205 iteration 5453 : loss : 0.015951, loss_ce: 0.006259
2022-01-09 13:44:13,629 iteration 5454 : loss : 0.024296, loss_ce: 0.012554
2022-01-09 13:44:15,025 iteration 5455 : loss : 0.034462, loss_ce: 0.009642
2022-01-09 13:44:16,407 iteration 5456 : loss : 0.047481, loss_ce: 0.008352
2022-01-09 13:44:17,738 iteration 5457 : loss : 0.016921, loss_ce: 0.005158
 80%|███████████████████████▎     | 321/400 [2:16:55<33:57, 25.80s/it]2022-01-09 13:44:19,153 iteration 5458 : loss : 0.021856, loss_ce: 0.007703
2022-01-09 13:44:20,468 iteration 5459 : loss : 0.022785, loss_ce: 0.011275
2022-01-09 13:44:21,841 iteration 5460 : loss : 0.014988, loss_ce: 0.005621
2022-01-09 13:44:23,202 iteration 5461 : loss : 0.026177, loss_ce: 0.010233
2022-01-09 13:44:24,558 iteration 5462 : loss : 0.025817, loss_ce: 0.009487
2022-01-09 13:44:25,938 iteration 5463 : loss : 0.032028, loss_ce: 0.010923
2022-01-09 13:44:27,320 iteration 5464 : loss : 0.023334, loss_ce: 0.009814
2022-01-09 13:44:28,763 iteration 5465 : loss : 0.037579, loss_ce: 0.014673
2022-01-09 13:44:30,195 iteration 5466 : loss : 0.024078, loss_ce: 0.007940
2022-01-09 13:44:31,591 iteration 5467 : loss : 0.025038, loss_ce: 0.011407
2022-01-09 13:44:32,984 iteration 5468 : loss : 0.016933, loss_ce: 0.007163
2022-01-09 13:44:34,297 iteration 5469 : loss : 0.020022, loss_ce: 0.007805
2022-01-09 13:44:35,652 iteration 5470 : loss : 0.012700, loss_ce: 0.004243
2022-01-09 13:44:37,010 iteration 5471 : loss : 0.019765, loss_ce: 0.005686
2022-01-09 13:44:38,408 iteration 5472 : loss : 0.023819, loss_ce: 0.007677
2022-01-09 13:44:39,725 iteration 5473 : loss : 0.017263, loss_ce: 0.006387
2022-01-09 13:44:41,048 iteration 5474 : loss : 0.018507, loss_ce: 0.006307
 80%|███████████████████████▎     | 322/400 [2:17:18<32:33, 25.05s/it]2022-01-09 13:44:42,494 iteration 5475 : loss : 0.021617, loss_ce: 0.006390
2022-01-09 13:44:43,926 iteration 5476 : loss : 0.020168, loss_ce: 0.009649
2022-01-09 13:44:45,298 iteration 5477 : loss : 0.052531, loss_ce: 0.008216
2022-01-09 13:44:46,593 iteration 5478 : loss : 0.016110, loss_ce: 0.005252
2022-01-09 13:44:47,953 iteration 5479 : loss : 0.023144, loss_ce: 0.009851
2022-01-09 13:44:49,316 iteration 5480 : loss : 0.026351, loss_ce: 0.009223
2022-01-09 13:44:50,651 iteration 5481 : loss : 0.017412, loss_ce: 0.006764
2022-01-09 13:44:52,113 iteration 5482 : loss : 0.035223, loss_ce: 0.008377
2022-01-09 13:44:53,411 iteration 5483 : loss : 0.015647, loss_ce: 0.006056
2022-01-09 13:44:54,794 iteration 5484 : loss : 0.024824, loss_ce: 0.011557
2022-01-09 13:44:56,159 iteration 5485 : loss : 0.027789, loss_ce: 0.015289
2022-01-09 13:44:57,528 iteration 5486 : loss : 0.033328, loss_ce: 0.017365
2022-01-09 13:44:58,913 iteration 5487 : loss : 0.018538, loss_ce: 0.006939
2022-01-09 13:45:00,250 iteration 5488 : loss : 0.022039, loss_ce: 0.010731
2022-01-09 13:45:01,580 iteration 5489 : loss : 0.018346, loss_ce: 0.009473
2022-01-09 13:45:03,019 iteration 5490 : loss : 0.031965, loss_ce: 0.011822
2022-01-09 13:45:04,368 iteration 5491 : loss : 0.019444, loss_ce: 0.006479
 81%|███████████████████████▍     | 323/400 [2:17:42<31:28, 24.53s/it]2022-01-09 13:45:05,713 iteration 5492 : loss : 0.018736, loss_ce: 0.007590
2022-01-09 13:45:07,109 iteration 5493 : loss : 0.030020, loss_ce: 0.009736
2022-01-09 13:45:08,439 iteration 5494 : loss : 0.026111, loss_ce: 0.008093
2022-01-09 13:45:09,812 iteration 5495 : loss : 0.015340, loss_ce: 0.004869
2022-01-09 13:45:11,193 iteration 5496 : loss : 0.024489, loss_ce: 0.006382
2022-01-09 13:45:12,579 iteration 5497 : loss : 0.030007, loss_ce: 0.012773
2022-01-09 13:45:14,000 iteration 5498 : loss : 0.030080, loss_ce: 0.010719
2022-01-09 13:45:15,399 iteration 5499 : loss : 0.021119, loss_ce: 0.011728
2022-01-09 13:45:16,714 iteration 5500 : loss : 0.018525, loss_ce: 0.008587
2022-01-09 13:45:18,088 iteration 5501 : loss : 0.020519, loss_ce: 0.008256
2022-01-09 13:45:19,420 iteration 5502 : loss : 0.012714, loss_ce: 0.005360
2022-01-09 13:45:20,780 iteration 5503 : loss : 0.017745, loss_ce: 0.004796
2022-01-09 13:45:22,116 iteration 5504 : loss : 0.019488, loss_ce: 0.007178
2022-01-09 13:45:23,479 iteration 5505 : loss : 0.019044, loss_ce: 0.007493
2022-01-09 13:45:24,878 iteration 5506 : loss : 0.017450, loss_ce: 0.006078
2022-01-09 13:45:26,236 iteration 5507 : loss : 0.021015, loss_ce: 0.008800
2022-01-09 13:45:27,616 iteration 5508 : loss : 0.023580, loss_ce: 0.008702
 81%|███████████████████████▍     | 324/400 [2:18:05<30:35, 24.15s/it]2022-01-09 13:45:29,047 iteration 5509 : loss : 0.035051, loss_ce: 0.009661
2022-01-09 13:45:30,504 iteration 5510 : loss : 0.016999, loss_ce: 0.005820
2022-01-09 13:45:31,855 iteration 5511 : loss : 0.017744, loss_ce: 0.006969
2022-01-09 13:45:33,273 iteration 5512 : loss : 0.027850, loss_ce: 0.010403
2022-01-09 13:45:34,624 iteration 5513 : loss : 0.015143, loss_ce: 0.006528
2022-01-09 13:45:35,934 iteration 5514 : loss : 0.011583, loss_ce: 0.003858
2022-01-09 13:45:37,361 iteration 5515 : loss : 0.031783, loss_ce: 0.006919
2022-01-09 13:45:38,709 iteration 5516 : loss : 0.020512, loss_ce: 0.008312
2022-01-09 13:45:40,091 iteration 5517 : loss : 0.022089, loss_ce: 0.009015
2022-01-09 13:45:41,469 iteration 5518 : loss : 0.035509, loss_ce: 0.012415
2022-01-09 13:45:42,861 iteration 5519 : loss : 0.019944, loss_ce: 0.007353
2022-01-09 13:45:44,235 iteration 5520 : loss : 0.017382, loss_ce: 0.006621
2022-01-09 13:45:45,629 iteration 5521 : loss : 0.034216, loss_ce: 0.012280
2022-01-09 13:45:47,006 iteration 5522 : loss : 0.024192, loss_ce: 0.007650
2022-01-09 13:45:48,352 iteration 5523 : loss : 0.013640, loss_ce: 0.004690
2022-01-09 13:45:49,702 iteration 5524 : loss : 0.025998, loss_ce: 0.011999
2022-01-09 13:45:49,702 Training Data Eval:
2022-01-09 13:45:56,582   Average segmentation loss on training set: 0.0127
2022-01-09 13:45:56,582 Validation Data Eval:
2022-01-09 13:45:58,953   Average segmentation loss on validation set: 0.0694
2022-01-09 13:46:00,321 iteration 5525 : loss : 0.029350, loss_ce: 0.013990
 81%|███████████████████████▌     | 325/400 [2:18:38<33:23, 26.71s/it]2022-01-09 13:46:01,793 iteration 5526 : loss : 0.019955, loss_ce: 0.006824
2022-01-09 13:46:03,172 iteration 5527 : loss : 0.022393, loss_ce: 0.010180
2022-01-09 13:46:04,541 iteration 5528 : loss : 0.020094, loss_ce: 0.007042
2022-01-09 13:46:05,946 iteration 5529 : loss : 0.026833, loss_ce: 0.010765
2022-01-09 13:46:07,265 iteration 5530 : loss : 0.016892, loss_ce: 0.007441
2022-01-09 13:46:08,610 iteration 5531 : loss : 0.024328, loss_ce: 0.010752
2022-01-09 13:46:09,934 iteration 5532 : loss : 0.015502, loss_ce: 0.006499
2022-01-09 13:46:11,356 iteration 5533 : loss : 0.033496, loss_ce: 0.010670
2022-01-09 13:46:12,723 iteration 5534 : loss : 0.016404, loss_ce: 0.006050
2022-01-09 13:46:14,154 iteration 5535 : loss : 0.023896, loss_ce: 0.008871
2022-01-09 13:46:15,623 iteration 5536 : loss : 0.019110, loss_ce: 0.007979
2022-01-09 13:46:17,029 iteration 5537 : loss : 0.033904, loss_ce: 0.009223
2022-01-09 13:46:18,412 iteration 5538 : loss : 0.024858, loss_ce: 0.010209
2022-01-09 13:46:19,879 iteration 5539 : loss : 0.020370, loss_ce: 0.004647
2022-01-09 13:46:21,225 iteration 5540 : loss : 0.015867, loss_ce: 0.007281
2022-01-09 13:46:22,611 iteration 5541 : loss : 0.019628, loss_ce: 0.006665
2022-01-09 13:46:23,964 iteration 5542 : loss : 0.017367, loss_ce: 0.006820
 82%|███████████████████████▋     | 326/400 [2:19:01<31:48, 25.79s/it]2022-01-09 13:46:25,442 iteration 5543 : loss : 0.026985, loss_ce: 0.010124
2022-01-09 13:46:26,826 iteration 5544 : loss : 0.019101, loss_ce: 0.009027
2022-01-09 13:46:28,175 iteration 5545 : loss : 0.021791, loss_ce: 0.009163
2022-01-09 13:46:29,503 iteration 5546 : loss : 0.018896, loss_ce: 0.006745
2022-01-09 13:46:30,872 iteration 5547 : loss : 0.018309, loss_ce: 0.007372
2022-01-09 13:46:32,299 iteration 5548 : loss : 0.018133, loss_ce: 0.005643
2022-01-09 13:46:33,613 iteration 5549 : loss : 0.019162, loss_ce: 0.007400
2022-01-09 13:46:34,955 iteration 5550 : loss : 0.017762, loss_ce: 0.006736
2022-01-09 13:46:36,324 iteration 5551 : loss : 0.015538, loss_ce: 0.007266
2022-01-09 13:46:37,635 iteration 5552 : loss : 0.017134, loss_ce: 0.006482
2022-01-09 13:46:39,020 iteration 5553 : loss : 0.028821, loss_ce: 0.012153
2022-01-09 13:46:40,439 iteration 5554 : loss : 0.022143, loss_ce: 0.007813
2022-01-09 13:46:41,857 iteration 5555 : loss : 0.031424, loss_ce: 0.009402
2022-01-09 13:46:43,167 iteration 5556 : loss : 0.023085, loss_ce: 0.009037
2022-01-09 13:46:44,596 iteration 5557 : loss : 0.025331, loss_ce: 0.007298
2022-01-09 13:46:45,978 iteration 5558 : loss : 0.017480, loss_ce: 0.005716
2022-01-09 13:46:47,262 iteration 5559 : loss : 0.014964, loss_ce: 0.006210
 82%|███████████████████████▋     | 327/400 [2:19:24<30:28, 25.04s/it]2022-01-09 13:46:48,852 iteration 5560 : loss : 0.031385, loss_ce: 0.011435
2022-01-09 13:46:50,215 iteration 5561 : loss : 0.017814, loss_ce: 0.006327
2022-01-09 13:46:51,577 iteration 5562 : loss : 0.023709, loss_ce: 0.006652
2022-01-09 13:46:52,945 iteration 5563 : loss : 0.017540, loss_ce: 0.006968
2022-01-09 13:46:54,405 iteration 5564 : loss : 0.032864, loss_ce: 0.013841
2022-01-09 13:46:55,781 iteration 5565 : loss : 0.017689, loss_ce: 0.007232
2022-01-09 13:46:57,198 iteration 5566 : loss : 0.016793, loss_ce: 0.007144
2022-01-09 13:46:58,665 iteration 5567 : loss : 0.029779, loss_ce: 0.016617
2022-01-09 13:47:00,038 iteration 5568 : loss : 0.021788, loss_ce: 0.009778
2022-01-09 13:47:01,459 iteration 5569 : loss : 0.025490, loss_ce: 0.011139
2022-01-09 13:47:02,852 iteration 5570 : loss : 0.018778, loss_ce: 0.006349
2022-01-09 13:47:04,269 iteration 5571 : loss : 0.017282, loss_ce: 0.007543
2022-01-09 13:47:05,650 iteration 5572 : loss : 0.016877, loss_ce: 0.006552
2022-01-09 13:47:07,013 iteration 5573 : loss : 0.015691, loss_ce: 0.005009
2022-01-09 13:47:08,486 iteration 5574 : loss : 0.023776, loss_ce: 0.008265
2022-01-09 13:47:09,762 iteration 5575 : loss : 0.013733, loss_ce: 0.005941
2022-01-09 13:47:11,020 iteration 5576 : loss : 0.014571, loss_ce: 0.004709
 82%|███████████████████████▊     | 328/400 [2:19:48<29:35, 24.66s/it]2022-01-09 13:47:12,439 iteration 5577 : loss : 0.026106, loss_ce: 0.011234
2022-01-09 13:47:13,779 iteration 5578 : loss : 0.027927, loss_ce: 0.007453
2022-01-09 13:47:15,144 iteration 5579 : loss : 0.015626, loss_ce: 0.004862
2022-01-09 13:47:16,554 iteration 5580 : loss : 0.017045, loss_ce: 0.006314
2022-01-09 13:47:17,905 iteration 5581 : loss : 0.015867, loss_ce: 0.005705
2022-01-09 13:47:19,313 iteration 5582 : loss : 0.021604, loss_ce: 0.007863
2022-01-09 13:47:20,669 iteration 5583 : loss : 0.023705, loss_ce: 0.006441
2022-01-09 13:47:22,125 iteration 5584 : loss : 0.062919, loss_ce: 0.027349
2022-01-09 13:47:23,476 iteration 5585 : loss : 0.021760, loss_ce: 0.008033
2022-01-09 13:47:24,846 iteration 5586 : loss : 0.023620, loss_ce: 0.010146
2022-01-09 13:47:26,220 iteration 5587 : loss : 0.019013, loss_ce: 0.008749
2022-01-09 13:47:27,660 iteration 5588 : loss : 0.022458, loss_ce: 0.008418
2022-01-09 13:47:29,085 iteration 5589 : loss : 0.023823, loss_ce: 0.010075
2022-01-09 13:47:30,467 iteration 5590 : loss : 0.021700, loss_ce: 0.008369
2022-01-09 13:47:31,848 iteration 5591 : loss : 0.020023, loss_ce: 0.009550
2022-01-09 13:47:33,212 iteration 5592 : loss : 0.018455, loss_ce: 0.007282
2022-01-09 13:47:34,571 iteration 5593 : loss : 0.019325, loss_ce: 0.011710
 82%|███████████████████████▊     | 329/400 [2:20:12<28:47, 24.33s/it]2022-01-09 13:47:36,028 iteration 5594 : loss : 0.018659, loss_ce: 0.007145
2022-01-09 13:47:37,377 iteration 5595 : loss : 0.018406, loss_ce: 0.007939
2022-01-09 13:47:38,727 iteration 5596 : loss : 0.013831, loss_ce: 0.004044
2022-01-09 13:47:40,122 iteration 5597 : loss : 0.016508, loss_ce: 0.005205
2022-01-09 13:47:41,564 iteration 5598 : loss : 0.024493, loss_ce: 0.010279
2022-01-09 13:47:42,985 iteration 5599 : loss : 0.021795, loss_ce: 0.007882
2022-01-09 13:47:44,396 iteration 5600 : loss : 0.018149, loss_ce: 0.007581
2022-01-09 13:47:45,813 iteration 5601 : loss : 0.017574, loss_ce: 0.005310
2022-01-09 13:47:47,153 iteration 5602 : loss : 0.023862, loss_ce: 0.006437
2022-01-09 13:47:48,575 iteration 5603 : loss : 0.023181, loss_ce: 0.009989
2022-01-09 13:47:49,917 iteration 5604 : loss : 0.020210, loss_ce: 0.009174
2022-01-09 13:47:51,266 iteration 5605 : loss : 0.018922, loss_ce: 0.007106
2022-01-09 13:47:52,703 iteration 5606 : loss : 0.023559, loss_ce: 0.007861
2022-01-09 13:47:54,081 iteration 5607 : loss : 0.026823, loss_ce: 0.011862
2022-01-09 13:47:55,577 iteration 5608 : loss : 0.034558, loss_ce: 0.019238
2022-01-09 13:47:56,924 iteration 5609 : loss : 0.029925, loss_ce: 0.018223
2022-01-09 13:47:56,924 Training Data Eval:
2022-01-09 13:48:03,783   Average segmentation loss on training set: 0.0122
2022-01-09 13:48:03,784 Validation Data Eval:
2022-01-09 13:48:06,145   Average segmentation loss on validation set: 0.0784
2022-01-09 13:48:07,538 iteration 5610 : loss : 0.017305, loss_ce: 0.007361
 82%|███████████████████████▉     | 330/400 [2:20:45<31:24, 26.92s/it]2022-01-09 13:48:09,009 iteration 5611 : loss : 0.020827, loss_ce: 0.009693
2022-01-09 13:48:10,391 iteration 5612 : loss : 0.025601, loss_ce: 0.009264
2022-01-09 13:48:11,756 iteration 5613 : loss : 0.027206, loss_ce: 0.011384
2022-01-09 13:48:13,119 iteration 5614 : loss : 0.023270, loss_ce: 0.008022
2022-01-09 13:48:14,521 iteration 5615 : loss : 0.017949, loss_ce: 0.006285
2022-01-09 13:48:15,934 iteration 5616 : loss : 0.022720, loss_ce: 0.009472
2022-01-09 13:48:17,316 iteration 5617 : loss : 0.014896, loss_ce: 0.005396
2022-01-09 13:48:18,644 iteration 5618 : loss : 0.019590, loss_ce: 0.008860
2022-01-09 13:48:20,058 iteration 5619 : loss : 0.028775, loss_ce: 0.009677
2022-01-09 13:48:21,480 iteration 5620 : loss : 0.020720, loss_ce: 0.005853
2022-01-09 13:48:22,825 iteration 5621 : loss : 0.028385, loss_ce: 0.012936
2022-01-09 13:48:24,282 iteration 5622 : loss : 0.020719, loss_ce: 0.008122
2022-01-09 13:48:25,626 iteration 5623 : loss : 0.022450, loss_ce: 0.006741
2022-01-09 13:48:27,026 iteration 5624 : loss : 0.017448, loss_ce: 0.007203
2022-01-09 13:48:28,437 iteration 5625 : loss : 0.021277, loss_ce: 0.009072
2022-01-09 13:48:29,818 iteration 5626 : loss : 0.021381, loss_ce: 0.008658
2022-01-09 13:48:31,186 iteration 5627 : loss : 0.025713, loss_ce: 0.008887
 83%|███████████████████████▉     | 331/400 [2:21:08<29:49, 25.94s/it]2022-01-09 13:48:32,521 iteration 5628 : loss : 0.015734, loss_ce: 0.005884
2022-01-09 13:48:33,922 iteration 5629 : loss : 0.022213, loss_ce: 0.009668
2022-01-09 13:48:35,292 iteration 5630 : loss : 0.020472, loss_ce: 0.006770
2022-01-09 13:48:36,766 iteration 5631 : loss : 0.027319, loss_ce: 0.012516
2022-01-09 13:48:38,072 iteration 5632 : loss : 0.022150, loss_ce: 0.009869
2022-01-09 13:48:39,490 iteration 5633 : loss : 0.037473, loss_ce: 0.018029
2022-01-09 13:48:40,825 iteration 5634 : loss : 0.016209, loss_ce: 0.005294
2022-01-09 13:48:42,225 iteration 5635 : loss : 0.028690, loss_ce: 0.007879
2022-01-09 13:48:43,541 iteration 5636 : loss : 0.015766, loss_ce: 0.007259
2022-01-09 13:48:44,931 iteration 5637 : loss : 0.023509, loss_ce: 0.009898
2022-01-09 13:48:46,203 iteration 5638 : loss : 0.019525, loss_ce: 0.005827
2022-01-09 13:48:47,520 iteration 5639 : loss : 0.023571, loss_ce: 0.006741
2022-01-09 13:48:48,873 iteration 5640 : loss : 0.019644, loss_ce: 0.009102
2022-01-09 13:48:50,286 iteration 5641 : loss : 0.015845, loss_ce: 0.007483
2022-01-09 13:48:51,638 iteration 5642 : loss : 0.017877, loss_ce: 0.003255
2022-01-09 13:48:53,055 iteration 5643 : loss : 0.018637, loss_ce: 0.005513
2022-01-09 13:48:54,457 iteration 5644 : loss : 0.020495, loss_ce: 0.008113
 83%|████████████████████████     | 332/400 [2:21:32<28:29, 25.14s/it]2022-01-09 13:48:55,994 iteration 5645 : loss : 0.021604, loss_ce: 0.009589
2022-01-09 13:48:57,340 iteration 5646 : loss : 0.023757, loss_ce: 0.009391
2022-01-09 13:48:58,717 iteration 5647 : loss : 0.032242, loss_ce: 0.014820
2022-01-09 13:49:00,138 iteration 5648 : loss : 0.026265, loss_ce: 0.009120
2022-01-09 13:49:01,554 iteration 5649 : loss : 0.023627, loss_ce: 0.012244
2022-01-09 13:49:02,941 iteration 5650 : loss : 0.018507, loss_ce: 0.007499
2022-01-09 13:49:04,311 iteration 5651 : loss : 0.019500, loss_ce: 0.009435
2022-01-09 13:49:05,617 iteration 5652 : loss : 0.015505, loss_ce: 0.006040
2022-01-09 13:49:06,968 iteration 5653 : loss : 0.022557, loss_ce: 0.006955
2022-01-09 13:49:08,283 iteration 5654 : loss : 0.017769, loss_ce: 0.005005
2022-01-09 13:49:09,671 iteration 5655 : loss : 0.031157, loss_ce: 0.013045
2022-01-09 13:49:11,037 iteration 5656 : loss : 0.016902, loss_ce: 0.005473
2022-01-09 13:49:12,410 iteration 5657 : loss : 0.018035, loss_ce: 0.007618
2022-01-09 13:49:13,781 iteration 5658 : loss : 0.020703, loss_ce: 0.010383
2022-01-09 13:49:15,129 iteration 5659 : loss : 0.018001, loss_ce: 0.005309
2022-01-09 13:49:16,440 iteration 5660 : loss : 0.014840, loss_ce: 0.005226
2022-01-09 13:49:17,897 iteration 5661 : loss : 0.019548, loss_ce: 0.006793
 83%|████████████████████████▏    | 333/400 [2:21:55<27:30, 24.63s/it]2022-01-09 13:49:19,333 iteration 5662 : loss : 0.018862, loss_ce: 0.004816
2022-01-09 13:49:20,702 iteration 5663 : loss : 0.028589, loss_ce: 0.011688
2022-01-09 13:49:22,091 iteration 5664 : loss : 0.024442, loss_ce: 0.008007
2022-01-09 13:49:23,384 iteration 5665 : loss : 0.016273, loss_ce: 0.005781
2022-01-09 13:49:24,777 iteration 5666 : loss : 0.018057, loss_ce: 0.006840
2022-01-09 13:49:26,197 iteration 5667 : loss : 0.028346, loss_ce: 0.014420
2022-01-09 13:49:27,562 iteration 5668 : loss : 0.018690, loss_ce: 0.008031
2022-01-09 13:49:28,974 iteration 5669 : loss : 0.020305, loss_ce: 0.009712
2022-01-09 13:49:30,364 iteration 5670 : loss : 0.016411, loss_ce: 0.006007
2022-01-09 13:49:31,754 iteration 5671 : loss : 0.024410, loss_ce: 0.008468
2022-01-09 13:49:33,129 iteration 5672 : loss : 0.030809, loss_ce: 0.008744
2022-01-09 13:49:34,469 iteration 5673 : loss : 0.017317, loss_ce: 0.005350
2022-01-09 13:49:35,912 iteration 5674 : loss : 0.014984, loss_ce: 0.006962
2022-01-09 13:49:37,260 iteration 5675 : loss : 0.016848, loss_ce: 0.007853
2022-01-09 13:49:38,695 iteration 5676 : loss : 0.035326, loss_ce: 0.014813
2022-01-09 13:49:40,109 iteration 5677 : loss : 0.016201, loss_ce: 0.008360
2022-01-09 13:49:41,505 iteration 5678 : loss : 0.026963, loss_ce: 0.009514
 84%|████████████████████████▏    | 334/400 [2:22:19<26:45, 24.32s/it]2022-01-09 13:49:42,936 iteration 5679 : loss : 0.025146, loss_ce: 0.011713
2022-01-09 13:49:44,212 iteration 5680 : loss : 0.014274, loss_ce: 0.003699
2022-01-09 13:49:45,627 iteration 5681 : loss : 0.028790, loss_ce: 0.008433
2022-01-09 13:49:47,050 iteration 5682 : loss : 0.022263, loss_ce: 0.009679
2022-01-09 13:49:48,396 iteration 5683 : loss : 0.015446, loss_ce: 0.005503
2022-01-09 13:49:49,728 iteration 5684 : loss : 0.014289, loss_ce: 0.007063
2022-01-09 13:49:51,120 iteration 5685 : loss : 0.020063, loss_ce: 0.009014
2022-01-09 13:49:52,476 iteration 5686 : loss : 0.016594, loss_ce: 0.005717
2022-01-09 13:49:53,844 iteration 5687 : loss : 0.018253, loss_ce: 0.007484
2022-01-09 13:49:55,307 iteration 5688 : loss : 0.034322, loss_ce: 0.013107
2022-01-09 13:49:56,692 iteration 5689 : loss : 0.026724, loss_ce: 0.006936
2022-01-09 13:49:58,076 iteration 5690 : loss : 0.019980, loss_ce: 0.008887
2022-01-09 13:49:59,479 iteration 5691 : loss : 0.021647, loss_ce: 0.009031
2022-01-09 13:50:00,878 iteration 5692 : loss : 0.021062, loss_ce: 0.009160
2022-01-09 13:50:02,261 iteration 5693 : loss : 0.024155, loss_ce: 0.006998
2022-01-09 13:50:03,622 iteration 5694 : loss : 0.020054, loss_ce: 0.009170
2022-01-09 13:50:03,622 Training Data Eval:
2022-01-09 13:50:10,483   Average segmentation loss on training set: 0.0124
2022-01-09 13:50:10,483 Validation Data Eval:
2022-01-09 13:50:12,855   Average segmentation loss on validation set: 0.0873
2022-01-09 13:50:14,176 iteration 5695 : loss : 0.017305, loss_ce: 0.004792
 84%|████████████████████████▎    | 335/400 [2:22:51<29:03, 26.83s/it]2022-01-09 13:50:15,550 iteration 5696 : loss : 0.016137, loss_ce: 0.007905
2022-01-09 13:50:16,868 iteration 5697 : loss : 0.016203, loss_ce: 0.007463
2022-01-09 13:50:18,284 iteration 5698 : loss : 0.017084, loss_ce: 0.005450
2022-01-09 13:50:19,581 iteration 5699 : loss : 0.015009, loss_ce: 0.006609
2022-01-09 13:50:20,924 iteration 5700 : loss : 0.016780, loss_ce: 0.005712
2022-01-09 13:50:22,237 iteration 5701 : loss : 0.013626, loss_ce: 0.005759
2022-01-09 13:50:23,699 iteration 5702 : loss : 0.018571, loss_ce: 0.006906
2022-01-09 13:50:25,100 iteration 5703 : loss : 0.022909, loss_ce: 0.008692
2022-01-09 13:50:26,479 iteration 5704 : loss : 0.017277, loss_ce: 0.005999
2022-01-09 13:50:27,873 iteration 5705 : loss : 0.030662, loss_ce: 0.010657
2022-01-09 13:50:29,159 iteration 5706 : loss : 0.015424, loss_ce: 0.005739
2022-01-09 13:50:30,507 iteration 5707 : loss : 0.016832, loss_ce: 0.005788
2022-01-09 13:50:31,828 iteration 5708 : loss : 0.022005, loss_ce: 0.007934
2022-01-09 13:50:33,203 iteration 5709 : loss : 0.019054, loss_ce: 0.006832
2022-01-09 13:50:34,564 iteration 5710 : loss : 0.021893, loss_ce: 0.006521
2022-01-09 13:50:35,868 iteration 5711 : loss : 0.020527, loss_ce: 0.006888
2022-01-09 13:50:37,252 iteration 5712 : loss : 0.020191, loss_ce: 0.008602
 84%|████████████████████████▎    | 336/400 [2:23:14<27:25, 25.70s/it]2022-01-09 13:50:38,727 iteration 5713 : loss : 0.021436, loss_ce: 0.007174
2022-01-09 13:50:40,148 iteration 5714 : loss : 0.023086, loss_ce: 0.008751
2022-01-09 13:50:41,581 iteration 5715 : loss : 0.014556, loss_ce: 0.006626
2022-01-09 13:50:42,938 iteration 5716 : loss : 0.017246, loss_ce: 0.006856
2022-01-09 13:50:44,400 iteration 5717 : loss : 0.037765, loss_ce: 0.013116
2022-01-09 13:50:45,729 iteration 5718 : loss : 0.014926, loss_ce: 0.005196
2022-01-09 13:50:47,049 iteration 5719 : loss : 0.022541, loss_ce: 0.011239
2022-01-09 13:50:48,465 iteration 5720 : loss : 0.027756, loss_ce: 0.011930
2022-01-09 13:50:49,849 iteration 5721 : loss : 0.026766, loss_ce: 0.013888
2022-01-09 13:50:51,217 iteration 5722 : loss : 0.018107, loss_ce: 0.008100
2022-01-09 13:50:52,595 iteration 5723 : loss : 0.022996, loss_ce: 0.007754
2022-01-09 13:50:54,012 iteration 5724 : loss : 0.022638, loss_ce: 0.008991
2022-01-09 13:50:55,389 iteration 5725 : loss : 0.021432, loss_ce: 0.008373
2022-01-09 13:50:56,724 iteration 5726 : loss : 0.027069, loss_ce: 0.006029
2022-01-09 13:50:58,113 iteration 5727 : loss : 0.020062, loss_ce: 0.006150
2022-01-09 13:50:59,465 iteration 5728 : loss : 0.019675, loss_ce: 0.006691
2022-01-09 13:51:00,909 iteration 5729 : loss : 0.018560, loss_ce: 0.007062
 84%|████████████████████████▍    | 337/400 [2:23:38<26:20, 25.09s/it]2022-01-09 13:51:02,298 iteration 5730 : loss : 0.016289, loss_ce: 0.007335
2022-01-09 13:51:03,719 iteration 5731 : loss : 0.018514, loss_ce: 0.008083
2022-01-09 13:51:05,081 iteration 5732 : loss : 0.014735, loss_ce: 0.005990
2022-01-09 13:51:06,511 iteration 5733 : loss : 0.025565, loss_ce: 0.006685
2022-01-09 13:51:07,931 iteration 5734 : loss : 0.022686, loss_ce: 0.010438
2022-01-09 13:51:09,278 iteration 5735 : loss : 0.017534, loss_ce: 0.005247
2022-01-09 13:51:10,626 iteration 5736 : loss : 0.030093, loss_ce: 0.011422
2022-01-09 13:51:12,073 iteration 5737 : loss : 0.026798, loss_ce: 0.009459
2022-01-09 13:51:13,414 iteration 5738 : loss : 0.031420, loss_ce: 0.008797
2022-01-09 13:51:14,751 iteration 5739 : loss : 0.022313, loss_ce: 0.009331
2022-01-09 13:51:16,118 iteration 5740 : loss : 0.030092, loss_ce: 0.015718
2022-01-09 13:51:17,491 iteration 5741 : loss : 0.018751, loss_ce: 0.008564
2022-01-09 13:51:18,876 iteration 5742 : loss : 0.023329, loss_ce: 0.005266
2022-01-09 13:51:20,274 iteration 5743 : loss : 0.016226, loss_ce: 0.007397
2022-01-09 13:51:21,701 iteration 5744 : loss : 0.028368, loss_ce: 0.012365
2022-01-09 13:51:23,131 iteration 5745 : loss : 0.028023, loss_ce: 0.011354
2022-01-09 13:51:24,467 iteration 5746 : loss : 0.021027, loss_ce: 0.005597
 84%|████████████████████████▌    | 338/400 [2:24:02<25:27, 24.63s/it]2022-01-09 13:51:25,907 iteration 5747 : loss : 0.020070, loss_ce: 0.007434
2022-01-09 13:51:27,300 iteration 5748 : loss : 0.025307, loss_ce: 0.010645
2022-01-09 13:51:28,677 iteration 5749 : loss : 0.024508, loss_ce: 0.010393
2022-01-09 13:51:30,048 iteration 5750 : loss : 0.021958, loss_ce: 0.006862
2022-01-09 13:51:31,364 iteration 5751 : loss : 0.022464, loss_ce: 0.007779
2022-01-09 13:51:32,721 iteration 5752 : loss : 0.018604, loss_ce: 0.005727
2022-01-09 13:51:34,132 iteration 5753 : loss : 0.022893, loss_ce: 0.009215
2022-01-09 13:51:35,558 iteration 5754 : loss : 0.019907, loss_ce: 0.006192
2022-01-09 13:51:36,932 iteration 5755 : loss : 0.014275, loss_ce: 0.005226
2022-01-09 13:51:38,273 iteration 5756 : loss : 0.018457, loss_ce: 0.006373
2022-01-09 13:51:39,542 iteration 5757 : loss : 0.015486, loss_ce: 0.006161
2022-01-09 13:51:40,966 iteration 5758 : loss : 0.024989, loss_ce: 0.010634
2022-01-09 13:51:42,385 iteration 5759 : loss : 0.018193, loss_ce: 0.007534
2022-01-09 13:51:43,844 iteration 5760 : loss : 0.021781, loss_ce: 0.010322
2022-01-09 13:51:45,315 iteration 5761 : loss : 0.032516, loss_ce: 0.014185
2022-01-09 13:51:46,642 iteration 5762 : loss : 0.013679, loss_ce: 0.005924
2022-01-09 13:51:48,058 iteration 5763 : loss : 0.015462, loss_ce: 0.006346
 85%|████████████████████████▌    | 339/400 [2:24:25<24:43, 24.32s/it]2022-01-09 13:51:49,498 iteration 5764 : loss : 0.019124, loss_ce: 0.007101
2022-01-09 13:51:50,935 iteration 5765 : loss : 0.041047, loss_ce: 0.010966
2022-01-09 13:51:52,338 iteration 5766 : loss : 0.031139, loss_ce: 0.019109
2022-01-09 13:51:53,715 iteration 5767 : loss : 0.015036, loss_ce: 0.007915
2022-01-09 13:51:55,196 iteration 5768 : loss : 0.028010, loss_ce: 0.009746
2022-01-09 13:51:56,625 iteration 5769 : loss : 0.022714, loss_ce: 0.007444
2022-01-09 13:51:57,964 iteration 5770 : loss : 0.018228, loss_ce: 0.006872
2022-01-09 13:51:59,358 iteration 5771 : loss : 0.020610, loss_ce: 0.008325
2022-01-09 13:52:00,731 iteration 5772 : loss : 0.021123, loss_ce: 0.007426
2022-01-09 13:52:02,041 iteration 5773 : loss : 0.018947, loss_ce: 0.007196
2022-01-09 13:52:03,333 iteration 5774 : loss : 0.018078, loss_ce: 0.005180
2022-01-09 13:52:04,723 iteration 5775 : loss : 0.018173, loss_ce: 0.007188
2022-01-09 13:52:06,172 iteration 5776 : loss : 0.020882, loss_ce: 0.009592
2022-01-09 13:52:07,468 iteration 5777 : loss : 0.018207, loss_ce: 0.007938
2022-01-09 13:52:08,815 iteration 5778 : loss : 0.024209, loss_ce: 0.008665
2022-01-09 13:52:10,169 iteration 5779 : loss : 0.016196, loss_ce: 0.005797
2022-01-09 13:52:10,169 Training Data Eval:
2022-01-09 13:52:17,023   Average segmentation loss on training set: 0.0112
2022-01-09 13:52:17,023 Validation Data Eval:
2022-01-09 13:52:19,400   Average segmentation loss on validation set: 0.0750
2022-01-09 13:52:20,763 iteration 5780 : loss : 0.024439, loss_ce: 0.008374
 85%|████████████████████████▋    | 340/400 [2:24:58<26:50, 26.83s/it]2022-01-09 13:52:22,160 iteration 5781 : loss : 0.028898, loss_ce: 0.007771
2022-01-09 13:52:23,590 iteration 5782 : loss : 0.021612, loss_ce: 0.008937
2022-01-09 13:52:24,963 iteration 5783 : loss : 0.025706, loss_ce: 0.009776
2022-01-09 13:52:26,362 iteration 5784 : loss : 0.017113, loss_ce: 0.009051
2022-01-09 13:52:27,694 iteration 5785 : loss : 0.014114, loss_ce: 0.005363
2022-01-09 13:52:29,037 iteration 5786 : loss : 0.017338, loss_ce: 0.007974
2022-01-09 13:52:30,374 iteration 5787 : loss : 0.014217, loss_ce: 0.005499
2022-01-09 13:52:31,866 iteration 5788 : loss : 0.034234, loss_ce: 0.023848
2022-01-09 13:52:33,256 iteration 5789 : loss : 0.015019, loss_ce: 0.005191
2022-01-09 13:52:34,637 iteration 5790 : loss : 0.017435, loss_ce: 0.006171
2022-01-09 13:52:35,974 iteration 5791 : loss : 0.015396, loss_ce: 0.005773
2022-01-09 13:52:37,377 iteration 5792 : loss : 0.021688, loss_ce: 0.009383
2022-01-09 13:52:38,854 iteration 5793 : loss : 0.022801, loss_ce: 0.008899
2022-01-09 13:52:40,243 iteration 5794 : loss : 0.017342, loss_ce: 0.007484
2022-01-09 13:52:41,616 iteration 5795 : loss : 0.019493, loss_ce: 0.006949
2022-01-09 13:52:42,948 iteration 5796 : loss : 0.013539, loss_ce: 0.003668
2022-01-09 13:52:44,345 iteration 5797 : loss : 0.020166, loss_ce: 0.007611
 85%|████████████████████████▋    | 341/400 [2:25:22<25:25, 25.86s/it]2022-01-09 13:52:45,817 iteration 5798 : loss : 0.024834, loss_ce: 0.010868
2022-01-09 13:52:47,210 iteration 5799 : loss : 0.022423, loss_ce: 0.009021
2022-01-09 13:52:48,575 iteration 5800 : loss : 0.014266, loss_ce: 0.005910
2022-01-09 13:52:49,942 iteration 5801 : loss : 0.021445, loss_ce: 0.007479
2022-01-09 13:52:51,255 iteration 5802 : loss : 0.016489, loss_ce: 0.006994
2022-01-09 13:52:52,601 iteration 5803 : loss : 0.016829, loss_ce: 0.008122
2022-01-09 13:52:53,967 iteration 5804 : loss : 0.022464, loss_ce: 0.008008
2022-01-09 13:52:55,430 iteration 5805 : loss : 0.019176, loss_ce: 0.006862
2022-01-09 13:52:56,788 iteration 5806 : loss : 0.020722, loss_ce: 0.006395
2022-01-09 13:52:58,238 iteration 5807 : loss : 0.020554, loss_ce: 0.008551
2022-01-09 13:52:59,636 iteration 5808 : loss : 0.020966, loss_ce: 0.010606
2022-01-09 13:53:00,953 iteration 5809 : loss : 0.014956, loss_ce: 0.004245
2022-01-09 13:53:02,294 iteration 5810 : loss : 0.024183, loss_ce: 0.010915
2022-01-09 13:53:03,599 iteration 5811 : loss : 0.012468, loss_ce: 0.003840
2022-01-09 13:53:04,972 iteration 5812 : loss : 0.018413, loss_ce: 0.006665
2022-01-09 13:53:06,284 iteration 5813 : loss : 0.016850, loss_ce: 0.007316
2022-01-09 13:53:07,664 iteration 5814 : loss : 0.019044, loss_ce: 0.008509
 86%|████████████████████████▊    | 342/400 [2:25:45<24:15, 25.10s/it]2022-01-09 13:53:09,034 iteration 5815 : loss : 0.012607, loss_ce: 0.003821
2022-01-09 13:53:10,386 iteration 5816 : loss : 0.012913, loss_ce: 0.006136
2022-01-09 13:53:11,728 iteration 5817 : loss : 0.014335, loss_ce: 0.005351
2022-01-09 13:53:13,010 iteration 5818 : loss : 0.018687, loss_ce: 0.007794
2022-01-09 13:53:14,433 iteration 5819 : loss : 0.021732, loss_ce: 0.007640
2022-01-09 13:53:15,877 iteration 5820 : loss : 0.029172, loss_ce: 0.014126
2022-01-09 13:53:17,185 iteration 5821 : loss : 0.016756, loss_ce: 0.005454
2022-01-09 13:53:18,521 iteration 5822 : loss : 0.012512, loss_ce: 0.005369
2022-01-09 13:53:19,984 iteration 5823 : loss : 0.017922, loss_ce: 0.007326
2022-01-09 13:53:21,353 iteration 5824 : loss : 0.016780, loss_ce: 0.004959
2022-01-09 13:53:22,769 iteration 5825 : loss : 0.023996, loss_ce: 0.008658
2022-01-09 13:53:24,188 iteration 5826 : loss : 0.017610, loss_ce: 0.004645
2022-01-09 13:53:25,566 iteration 5827 : loss : 0.026971, loss_ce: 0.008355
2022-01-09 13:53:27,039 iteration 5828 : loss : 0.022254, loss_ce: 0.008158
2022-01-09 13:53:28,405 iteration 5829 : loss : 0.014907, loss_ce: 0.005897
2022-01-09 13:53:29,744 iteration 5830 : loss : 0.022092, loss_ce: 0.009066
2022-01-09 13:53:31,095 iteration 5831 : loss : 0.016017, loss_ce: 0.005332
 86%|████████████████████████▊    | 343/400 [2:26:08<23:21, 24.60s/it]2022-01-09 13:53:32,525 iteration 5832 : loss : 0.016789, loss_ce: 0.004948
2022-01-09 13:53:33,932 iteration 5833 : loss : 0.017277, loss_ce: 0.007642
2022-01-09 13:53:35,251 iteration 5834 : loss : 0.015791, loss_ce: 0.005276
2022-01-09 13:53:36,574 iteration 5835 : loss : 0.020909, loss_ce: 0.006115
2022-01-09 13:53:37,986 iteration 5836 : loss : 0.025428, loss_ce: 0.015254
2022-01-09 13:53:39,350 iteration 5837 : loss : 0.019726, loss_ce: 0.004431
2022-01-09 13:53:40,635 iteration 5838 : loss : 0.012026, loss_ce: 0.003526
2022-01-09 13:53:41,993 iteration 5839 : loss : 0.016835, loss_ce: 0.006130
2022-01-09 13:53:43,401 iteration 5840 : loss : 0.020074, loss_ce: 0.006463
2022-01-09 13:53:44,746 iteration 5841 : loss : 0.017286, loss_ce: 0.007181
2022-01-09 13:53:46,089 iteration 5842 : loss : 0.011116, loss_ce: 0.004132
2022-01-09 13:53:47,428 iteration 5843 : loss : 0.015566, loss_ce: 0.006189
2022-01-09 13:53:48,928 iteration 5844 : loss : 0.024429, loss_ce: 0.007571
2022-01-09 13:53:50,299 iteration 5845 : loss : 0.022587, loss_ce: 0.008442
2022-01-09 13:53:51,680 iteration 5846 : loss : 0.021502, loss_ce: 0.009136
2022-01-09 13:53:53,066 iteration 5847 : loss : 0.019202, loss_ce: 0.007120
2022-01-09 13:53:54,438 iteration 5848 : loss : 0.018143, loss_ce: 0.006677
 86%|████████████████████████▉    | 344/400 [2:26:32<22:36, 24.22s/it]2022-01-09 13:53:55,866 iteration 5849 : loss : 0.016084, loss_ce: 0.006491
2022-01-09 13:53:57,175 iteration 5850 : loss : 0.013547, loss_ce: 0.005650
2022-01-09 13:53:58,542 iteration 5851 : loss : 0.018983, loss_ce: 0.008266
2022-01-09 13:53:59,886 iteration 5852 : loss : 0.017607, loss_ce: 0.006538
2022-01-09 13:54:01,172 iteration 5853 : loss : 0.013599, loss_ce: 0.004199
2022-01-09 13:54:02,510 iteration 5854 : loss : 0.021389, loss_ce: 0.007693
2022-01-09 13:54:03,946 iteration 5855 : loss : 0.017271, loss_ce: 0.007249
2022-01-09 13:54:05,321 iteration 5856 : loss : 0.019617, loss_ce: 0.008790
2022-01-09 13:54:06,706 iteration 5857 : loss : 0.022658, loss_ce: 0.006937
2022-01-09 13:54:08,002 iteration 5858 : loss : 0.014928, loss_ce: 0.005606
2022-01-09 13:54:09,415 iteration 5859 : loss : 0.018365, loss_ce: 0.007015
2022-01-09 13:54:10,828 iteration 5860 : loss : 0.019261, loss_ce: 0.005720
2022-01-09 13:54:12,178 iteration 5861 : loss : 0.020791, loss_ce: 0.005849
2022-01-09 13:54:13,568 iteration 5862 : loss : 0.017605, loss_ce: 0.008213
2022-01-09 13:54:14,851 iteration 5863 : loss : 0.012203, loss_ce: 0.004101
2022-01-09 13:54:16,225 iteration 5864 : loss : 0.024229, loss_ce: 0.007281
2022-01-09 13:54:16,225 Training Data Eval:
2022-01-09 13:54:23,102   Average segmentation loss on training set: 0.0106
2022-01-09 13:54:23,102 Validation Data Eval:
2022-01-09 13:54:25,472   Average segmentation loss on validation set: 0.0813
2022-01-09 13:54:26,790 iteration 5865 : loss : 0.021441, loss_ce: 0.006494
 86%|█████████████████████████    | 345/400 [2:27:04<24:26, 26.66s/it]2022-01-09 13:54:28,292 iteration 5866 : loss : 0.016908, loss_ce: 0.006925
2022-01-09 13:54:29,713 iteration 5867 : loss : 0.018620, loss_ce: 0.005789
2022-01-09 13:54:31,127 iteration 5868 : loss : 0.019493, loss_ce: 0.006756
2022-01-09 13:54:32,427 iteration 5869 : loss : 0.017831, loss_ce: 0.006696
2022-01-09 13:54:33,871 iteration 5870 : loss : 0.028859, loss_ce: 0.009135
2022-01-09 13:54:35,272 iteration 5871 : loss : 0.012773, loss_ce: 0.004412
2022-01-09 13:54:36,678 iteration 5872 : loss : 0.029826, loss_ce: 0.010458
2022-01-09 13:54:38,166 iteration 5873 : loss : 0.024035, loss_ce: 0.008030
2022-01-09 13:54:39,508 iteration 5874 : loss : 0.016615, loss_ce: 0.005254
2022-01-09 13:54:40,861 iteration 5875 : loss : 0.015945, loss_ce: 0.007970
2022-01-09 13:54:42,235 iteration 5876 : loss : 0.017171, loss_ce: 0.005992
2022-01-09 13:54:43,609 iteration 5877 : loss : 0.019077, loss_ce: 0.007508
2022-01-09 13:54:45,006 iteration 5878 : loss : 0.018123, loss_ce: 0.008351
2022-01-09 13:54:46,342 iteration 5879 : loss : 0.017208, loss_ce: 0.007531
2022-01-09 13:54:47,810 iteration 5880 : loss : 0.027042, loss_ce: 0.010572
2022-01-09 13:54:49,128 iteration 5881 : loss : 0.014273, loss_ce: 0.005765
2022-01-09 13:54:50,478 iteration 5882 : loss : 0.018443, loss_ce: 0.008795
 86%|█████████████████████████    | 346/400 [2:27:28<23:11, 25.77s/it]2022-01-09 13:54:51,880 iteration 5883 : loss : 0.014882, loss_ce: 0.005196
2022-01-09 13:54:53,227 iteration 5884 : loss : 0.013879, loss_ce: 0.005758
2022-01-09 13:54:54,721 iteration 5885 : loss : 0.042422, loss_ce: 0.016856
2022-01-09 13:54:56,046 iteration 5886 : loss : 0.017205, loss_ce: 0.006470
2022-01-09 13:54:57,389 iteration 5887 : loss : 0.014190, loss_ce: 0.005956
2022-01-09 13:54:58,847 iteration 5888 : loss : 0.022271, loss_ce: 0.008511
2022-01-09 13:55:00,299 iteration 5889 : loss : 0.022725, loss_ce: 0.010312
2022-01-09 13:55:01,698 iteration 5890 : loss : 0.013686, loss_ce: 0.004600
2022-01-09 13:55:03,129 iteration 5891 : loss : 0.036541, loss_ce: 0.011782
2022-01-09 13:55:04,601 iteration 5892 : loss : 0.026124, loss_ce: 0.010675
2022-01-09 13:55:05,951 iteration 5893 : loss : 0.013992, loss_ce: 0.004722
2022-01-09 13:55:07,410 iteration 5894 : loss : 0.019025, loss_ce: 0.007548
2022-01-09 13:55:08,817 iteration 5895 : loss : 0.021819, loss_ce: 0.008263
2022-01-09 13:55:10,180 iteration 5896 : loss : 0.014674, loss_ce: 0.006571
2022-01-09 13:55:11,620 iteration 5897 : loss : 0.040915, loss_ce: 0.021810
2022-01-09 13:55:13,046 iteration 5898 : loss : 0.014007, loss_ce: 0.004988
2022-01-09 13:55:14,397 iteration 5899 : loss : 0.016233, loss_ce: 0.006304
 87%|█████████████████████████▏   | 347/400 [2:27:52<22:16, 25.21s/it]2022-01-09 13:55:15,849 iteration 5900 : loss : 0.018173, loss_ce: 0.006507
2022-01-09 13:55:17,303 iteration 5901 : loss : 0.021677, loss_ce: 0.006993
2022-01-09 13:55:18,681 iteration 5902 : loss : 0.021117, loss_ce: 0.008023
2022-01-09 13:55:20,122 iteration 5903 : loss : 0.015423, loss_ce: 0.006769
2022-01-09 13:55:21,487 iteration 5904 : loss : 0.019444, loss_ce: 0.007291
2022-01-09 13:55:22,911 iteration 5905 : loss : 0.032416, loss_ce: 0.010637
2022-01-09 13:55:24,239 iteration 5906 : loss : 0.019557, loss_ce: 0.007181
2022-01-09 13:55:25,646 iteration 5907 : loss : 0.014303, loss_ce: 0.005581
2022-01-09 13:55:27,034 iteration 5908 : loss : 0.017000, loss_ce: 0.005970
2022-01-09 13:55:28,324 iteration 5909 : loss : 0.025077, loss_ce: 0.006229
2022-01-09 13:55:29,704 iteration 5910 : loss : 0.026369, loss_ce: 0.009596
2022-01-09 13:55:31,122 iteration 5911 : loss : 0.030245, loss_ce: 0.012037
2022-01-09 13:55:32,442 iteration 5912 : loss : 0.020643, loss_ce: 0.007671
2022-01-09 13:55:33,793 iteration 5913 : loss : 0.017646, loss_ce: 0.008501
2022-01-09 13:55:35,178 iteration 5914 : loss : 0.023959, loss_ce: 0.009639
2022-01-09 13:55:36,474 iteration 5915 : loss : 0.017602, loss_ce: 0.006939
2022-01-09 13:55:37,891 iteration 5916 : loss : 0.023647, loss_ce: 0.008462
 87%|█████████████████████████▏   | 348/400 [2:28:15<21:24, 24.70s/it]2022-01-09 13:55:39,292 iteration 5917 : loss : 0.016128, loss_ce: 0.006976
2022-01-09 13:55:40,609 iteration 5918 : loss : 0.015766, loss_ce: 0.005997
2022-01-09 13:55:42,020 iteration 5919 : loss : 0.024604, loss_ce: 0.012047
2022-01-09 13:55:43,449 iteration 5920 : loss : 0.025200, loss_ce: 0.014458
2022-01-09 13:55:44,962 iteration 5921 : loss : 0.043772, loss_ce: 0.011688
2022-01-09 13:55:46,325 iteration 5922 : loss : 0.015323, loss_ce: 0.004031
2022-01-09 13:55:47,722 iteration 5923 : loss : 0.022564, loss_ce: 0.007149
2022-01-09 13:55:49,203 iteration 5924 : loss : 0.024575, loss_ce: 0.005273
2022-01-09 13:55:50,595 iteration 5925 : loss : 0.017756, loss_ce: 0.005860
2022-01-09 13:55:51,966 iteration 5926 : loss : 0.019216, loss_ce: 0.008129
2022-01-09 13:55:53,367 iteration 5927 : loss : 0.014568, loss_ce: 0.005896
2022-01-09 13:55:54,686 iteration 5928 : loss : 0.015442, loss_ce: 0.003881
2022-01-09 13:55:56,003 iteration 5929 : loss : 0.019969, loss_ce: 0.009450
2022-01-09 13:55:57,327 iteration 5930 : loss : 0.019414, loss_ce: 0.008849
2022-01-09 13:55:58,708 iteration 5931 : loss : 0.033287, loss_ce: 0.009936
2022-01-09 13:56:00,133 iteration 5932 : loss : 0.019039, loss_ce: 0.006849
2022-01-09 13:56:01,496 iteration 5933 : loss : 0.017564, loss_ce: 0.006346
 87%|█████████████████████████▎   | 349/400 [2:28:39<20:42, 24.37s/it]2022-01-09 13:56:02,887 iteration 5934 : loss : 0.015338, loss_ce: 0.005160
2022-01-09 13:56:04,389 iteration 5935 : loss : 0.024407, loss_ce: 0.008590
2022-01-09 13:56:05,771 iteration 5936 : loss : 0.014598, loss_ce: 0.006063
2022-01-09 13:56:07,120 iteration 5937 : loss : 0.019405, loss_ce: 0.006345
2022-01-09 13:56:08,418 iteration 5938 : loss : 0.011261, loss_ce: 0.004336
2022-01-09 13:56:09,829 iteration 5939 : loss : 0.022187, loss_ce: 0.011098
2022-01-09 13:56:11,150 iteration 5940 : loss : 0.014663, loss_ce: 0.006198
2022-01-09 13:56:12,484 iteration 5941 : loss : 0.023668, loss_ce: 0.011238
2022-01-09 13:56:13,827 iteration 5942 : loss : 0.015595, loss_ce: 0.006578
2022-01-09 13:56:15,206 iteration 5943 : loss : 0.017977, loss_ce: 0.004811
2022-01-09 13:56:16,589 iteration 5944 : loss : 0.019113, loss_ce: 0.011017
2022-01-09 13:56:17,939 iteration 5945 : loss : 0.021481, loss_ce: 0.005113
2022-01-09 13:56:19,405 iteration 5946 : loss : 0.021145, loss_ce: 0.007559
2022-01-09 13:56:20,814 iteration 5947 : loss : 0.033669, loss_ce: 0.012396
2022-01-09 13:56:22,163 iteration 5948 : loss : 0.022829, loss_ce: 0.008411
2022-01-09 13:56:23,602 iteration 5949 : loss : 0.019806, loss_ce: 0.006699
2022-01-09 13:56:23,602 Training Data Eval:
2022-01-09 13:56:30,468   Average segmentation loss on training set: 0.0105
2022-01-09 13:56:30,469 Validation Data Eval:
2022-01-09 13:56:32,847   Average segmentation loss on validation set: 0.0750
2022-01-09 13:56:34,238 iteration 5950 : loss : 0.018410, loss_ce: 0.005920
 88%|█████████████████████████▍   | 350/400 [2:29:11<22:24, 26.88s/it]2022-01-09 13:56:35,625 iteration 5951 : loss : 0.017810, loss_ce: 0.006536
2022-01-09 13:56:37,001 iteration 5952 : loss : 0.020319, loss_ce: 0.007452
2022-01-09 13:56:38,377 iteration 5953 : loss : 0.027587, loss_ce: 0.013261
2022-01-09 13:56:39,723 iteration 5954 : loss : 0.016345, loss_ce: 0.006244
2022-01-09 13:56:41,091 iteration 5955 : loss : 0.015429, loss_ce: 0.005112
2022-01-09 13:56:42,491 iteration 5956 : loss : 0.020985, loss_ce: 0.010503
2022-01-09 13:56:43,875 iteration 5957 : loss : 0.014207, loss_ce: 0.005155
2022-01-09 13:56:45,206 iteration 5958 : loss : 0.016901, loss_ce: 0.006987
2022-01-09 13:56:46,567 iteration 5959 : loss : 0.018416, loss_ce: 0.007386
2022-01-09 13:56:47,895 iteration 5960 : loss : 0.017593, loss_ce: 0.005439
2022-01-09 13:56:49,214 iteration 5961 : loss : 0.027698, loss_ce: 0.005902
2022-01-09 13:56:50,569 iteration 5962 : loss : 0.022235, loss_ce: 0.004712
2022-01-09 13:56:51,974 iteration 5963 : loss : 0.022920, loss_ce: 0.010327
2022-01-09 13:56:53,325 iteration 5964 : loss : 0.012826, loss_ce: 0.005363
2022-01-09 13:56:54,641 iteration 5965 : loss : 0.015156, loss_ce: 0.007173
2022-01-09 13:56:56,058 iteration 5966 : loss : 0.017866, loss_ce: 0.005358
2022-01-09 13:56:57,443 iteration 5967 : loss : 0.017058, loss_ce: 0.007315
 88%|█████████████████████████▍   | 351/400 [2:29:35<21:03, 25.78s/it]2022-01-09 13:56:58,947 iteration 5968 : loss : 0.023164, loss_ce: 0.009401
2022-01-09 13:57:00,300 iteration 5969 : loss : 0.018056, loss_ce: 0.007406
2022-01-09 13:57:01,705 iteration 5970 : loss : 0.021417, loss_ce: 0.009512
2022-01-09 13:57:03,014 iteration 5971 : loss : 0.013924, loss_ce: 0.004413
2022-01-09 13:57:04,299 iteration 5972 : loss : 0.013286, loss_ce: 0.004483
2022-01-09 13:57:05,678 iteration 5973 : loss : 0.018484, loss_ce: 0.009240
2022-01-09 13:57:07,146 iteration 5974 : loss : 0.036337, loss_ce: 0.014714
2022-01-09 13:57:08,468 iteration 5975 : loss : 0.019866, loss_ce: 0.007141
2022-01-09 13:57:09,815 iteration 5976 : loss : 0.017635, loss_ce: 0.005042
2022-01-09 13:57:11,142 iteration 5977 : loss : 0.023758, loss_ce: 0.005754
2022-01-09 13:57:12,454 iteration 5978 : loss : 0.015592, loss_ce: 0.006961
2022-01-09 13:57:13,816 iteration 5979 : loss : 0.013751, loss_ce: 0.006497
2022-01-09 13:57:15,222 iteration 5980 : loss : 0.019441, loss_ce: 0.007099
2022-01-09 13:57:16,560 iteration 5981 : loss : 0.017478, loss_ce: 0.008029
2022-01-09 13:57:17,930 iteration 5982 : loss : 0.019291, loss_ce: 0.008205
2022-01-09 13:57:19,356 iteration 5983 : loss : 0.020154, loss_ce: 0.006770
2022-01-09 13:57:20,698 iteration 5984 : loss : 0.017972, loss_ce: 0.005902
 88%|█████████████████████████▌   | 352/400 [2:29:58<20:01, 25.02s/it]2022-01-09 13:57:22,132 iteration 5985 : loss : 0.015128, loss_ce: 0.004528
2022-01-09 13:57:23,415 iteration 5986 : loss : 0.016746, loss_ce: 0.004930
2022-01-09 13:57:24,811 iteration 5987 : loss : 0.017603, loss_ce: 0.008188
2022-01-09 13:57:26,231 iteration 5988 : loss : 0.024711, loss_ce: 0.012584
2022-01-09 13:57:27,615 iteration 5989 : loss : 0.018443, loss_ce: 0.007301
2022-01-09 13:57:28,963 iteration 5990 : loss : 0.024246, loss_ce: 0.011282
2022-01-09 13:57:30,385 iteration 5991 : loss : 0.023774, loss_ce: 0.007845
2022-01-09 13:57:31,766 iteration 5992 : loss : 0.021558, loss_ce: 0.008427
2022-01-09 13:57:33,132 iteration 5993 : loss : 0.014075, loss_ce: 0.005900
2022-01-09 13:57:34,544 iteration 5994 : loss : 0.030552, loss_ce: 0.012643
2022-01-09 13:57:35,929 iteration 5995 : loss : 0.016242, loss_ce: 0.007054
2022-01-09 13:57:37,257 iteration 5996 : loss : 0.014306, loss_ce: 0.004261
2022-01-09 13:57:38,643 iteration 5997 : loss : 0.019055, loss_ce: 0.008078
2022-01-09 13:57:40,084 iteration 5998 : loss : 0.019770, loss_ce: 0.006804
2022-01-09 13:57:41,559 iteration 5999 : loss : 0.040637, loss_ce: 0.011260
2022-01-09 13:57:42,946 iteration 6000 : loss : 0.027408, loss_ce: 0.013052
2022-01-09 13:57:44,245 iteration 6001 : loss : 0.015948, loss_ce: 0.007726
 88%|█████████████████████████▌   | 353/400 [2:30:21<19:15, 24.58s/it]2022-01-09 13:57:45,692 iteration 6002 : loss : 0.016369, loss_ce: 0.006149
2022-01-09 13:57:47,032 iteration 6003 : loss : 0.014062, loss_ce: 0.005110
2022-01-09 13:57:48,453 iteration 6004 : loss : 0.021462, loss_ce: 0.010563
2022-01-09 13:57:49,860 iteration 6005 : loss : 0.022496, loss_ce: 0.007526
2022-01-09 13:57:51,332 iteration 6006 : loss : 0.017451, loss_ce: 0.007059
2022-01-09 13:57:52,725 iteration 6007 : loss : 0.012718, loss_ce: 0.004796
2022-01-09 13:57:54,106 iteration 6008 : loss : 0.016138, loss_ce: 0.005446
2022-01-09 13:57:55,405 iteration 6009 : loss : 0.013951, loss_ce: 0.006592
2022-01-09 13:57:56,724 iteration 6010 : loss : 0.017492, loss_ce: 0.007858
2022-01-09 13:57:58,128 iteration 6011 : loss : 0.018842, loss_ce: 0.006962
2022-01-09 13:57:59,564 iteration 6012 : loss : 0.029950, loss_ce: 0.008035
2022-01-09 13:58:00,978 iteration 6013 : loss : 0.019549, loss_ce: 0.005680
2022-01-09 13:58:02,384 iteration 6014 : loss : 0.026344, loss_ce: 0.007141
2022-01-09 13:58:03,727 iteration 6015 : loss : 0.018836, loss_ce: 0.006928
2022-01-09 13:58:05,097 iteration 6016 : loss : 0.016905, loss_ce: 0.005838
2022-01-09 13:58:06,467 iteration 6017 : loss : 0.029534, loss_ce: 0.011577
2022-01-09 13:58:07,861 iteration 6018 : loss : 0.018681, loss_ce: 0.006741
 88%|█████████████████████████▋   | 354/400 [2:30:45<18:37, 24.29s/it]2022-01-09 13:58:09,306 iteration 6019 : loss : 0.021932, loss_ce: 0.012455
2022-01-09 13:58:10,667 iteration 6020 : loss : 0.014480, loss_ce: 0.006553
2022-01-09 13:58:12,059 iteration 6021 : loss : 0.023306, loss_ce: 0.005883
2022-01-09 13:58:13,376 iteration 6022 : loss : 0.016495, loss_ce: 0.006374
2022-01-09 13:58:14,779 iteration 6023 : loss : 0.021064, loss_ce: 0.008659
2022-01-09 13:58:16,132 iteration 6024 : loss : 0.017384, loss_ce: 0.005947
2022-01-09 13:58:17,479 iteration 6025 : loss : 0.015874, loss_ce: 0.005807
2022-01-09 13:58:18,869 iteration 6026 : loss : 0.016894, loss_ce: 0.005809
2022-01-09 13:58:20,257 iteration 6027 : loss : 0.023230, loss_ce: 0.008915
2022-01-09 13:58:21,662 iteration 6028 : loss : 0.023063, loss_ce: 0.008001
2022-01-09 13:58:23,014 iteration 6029 : loss : 0.019390, loss_ce: 0.005169
2022-01-09 13:58:24,321 iteration 6030 : loss : 0.015987, loss_ce: 0.007521
2022-01-09 13:58:25,708 iteration 6031 : loss : 0.017436, loss_ce: 0.008395
2022-01-09 13:58:27,035 iteration 6032 : loss : 0.029055, loss_ce: 0.011222
2022-01-09 13:58:28,452 iteration 6033 : loss : 0.018106, loss_ce: 0.006214
2022-01-09 13:58:29,827 iteration 6034 : loss : 0.017080, loss_ce: 0.006363
2022-01-09 13:58:29,827 Training Data Eval:
2022-01-09 13:58:36,704   Average segmentation loss on training set: 0.0102
2022-01-09 13:58:36,704 Validation Data Eval:
2022-01-09 13:58:39,074   Average segmentation loss on validation set: 0.0725
2022-01-09 13:58:40,333 iteration 6035 : loss : 0.012903, loss_ce: 0.004906
 89%|█████████████████████████▋   | 355/400 [2:31:18<20:03, 26.75s/it]2022-01-09 13:58:41,848 iteration 6036 : loss : 0.022384, loss_ce: 0.006320
2022-01-09 13:58:43,258 iteration 6037 : loss : 0.023209, loss_ce: 0.006730
2022-01-09 13:58:44,614 iteration 6038 : loss : 0.016127, loss_ce: 0.005731
2022-01-09 13:58:45,951 iteration 6039 : loss : 0.014757, loss_ce: 0.004752
2022-01-09 13:58:47,324 iteration 6040 : loss : 0.017681, loss_ce: 0.006480
2022-01-09 13:58:48,657 iteration 6041 : loss : 0.016528, loss_ce: 0.005648
2022-01-09 13:58:50,044 iteration 6042 : loss : 0.014613, loss_ce: 0.006535
2022-01-09 13:58:51,406 iteration 6043 : loss : 0.016080, loss_ce: 0.007522
2022-01-09 13:58:52,712 iteration 6044 : loss : 0.014119, loss_ce: 0.005752
2022-01-09 13:58:54,155 iteration 6045 : loss : 0.022426, loss_ce: 0.008245
2022-01-09 13:58:55,463 iteration 6046 : loss : 0.019893, loss_ce: 0.008974
2022-01-09 13:58:56,773 iteration 6047 : loss : 0.015317, loss_ce: 0.007314
2022-01-09 13:58:58,209 iteration 6048 : loss : 0.017842, loss_ce: 0.006247
2022-01-09 13:58:59,513 iteration 6049 : loss : 0.014803, loss_ce: 0.004965
2022-01-09 13:59:00,812 iteration 6050 : loss : 0.014901, loss_ce: 0.004362
2022-01-09 13:59:02,110 iteration 6051 : loss : 0.014267, loss_ce: 0.006049
2022-01-09 13:59:03,504 iteration 6052 : loss : 0.014529, loss_ce: 0.004887
 89%|█████████████████████████▊   | 356/400 [2:31:41<18:49, 25.67s/it]2022-01-09 13:59:04,897 iteration 6053 : loss : 0.019218, loss_ce: 0.005607
2022-01-09 13:59:06,235 iteration 6054 : loss : 0.016285, loss_ce: 0.005346
2022-01-09 13:59:07,623 iteration 6055 : loss : 0.018229, loss_ce: 0.007686
2022-01-09 13:59:08,972 iteration 6056 : loss : 0.022205, loss_ce: 0.007575
2022-01-09 13:59:10,264 iteration 6057 : loss : 0.017644, loss_ce: 0.006735
2022-01-09 13:59:11,580 iteration 6058 : loss : 0.016478, loss_ce: 0.009794
2022-01-09 13:59:13,003 iteration 6059 : loss : 0.029612, loss_ce: 0.012105
2022-01-09 13:59:14,380 iteration 6060 : loss : 0.021144, loss_ce: 0.005995
2022-01-09 13:59:15,702 iteration 6061 : loss : 0.017383, loss_ce: 0.008454
2022-01-09 13:59:17,189 iteration 6062 : loss : 0.033495, loss_ce: 0.013635
2022-01-09 13:59:18,512 iteration 6063 : loss : 0.019594, loss_ce: 0.008497
2022-01-09 13:59:19,844 iteration 6064 : loss : 0.013264, loss_ce: 0.004734
2022-01-09 13:59:21,279 iteration 6065 : loss : 0.024519, loss_ce: 0.010032
2022-01-09 13:59:22,586 iteration 6066 : loss : 0.016202, loss_ce: 0.006819
2022-01-09 13:59:23,970 iteration 6067 : loss : 0.033327, loss_ce: 0.011655
2022-01-09 13:59:25,338 iteration 6068 : loss : 0.015936, loss_ce: 0.002809
2022-01-09 13:59:26,784 iteration 6069 : loss : 0.020908, loss_ce: 0.009392
 89%|█████████████████████████▉   | 357/400 [2:32:04<17:53, 24.95s/it]2022-01-09 13:59:28,228 iteration 6070 : loss : 0.016324, loss_ce: 0.007562
2022-01-09 13:59:29,601 iteration 6071 : loss : 0.016602, loss_ce: 0.006971
2022-01-09 13:59:30,961 iteration 6072 : loss : 0.016364, loss_ce: 0.006065
2022-01-09 13:59:32,334 iteration 6073 : loss : 0.024780, loss_ce: 0.007452
2022-01-09 13:59:33,623 iteration 6074 : loss : 0.015619, loss_ce: 0.007517
2022-01-09 13:59:35,022 iteration 6075 : loss : 0.032199, loss_ce: 0.013728
2022-01-09 13:59:36,392 iteration 6076 : loss : 0.013287, loss_ce: 0.005254
2022-01-09 13:59:37,654 iteration 6077 : loss : 0.012377, loss_ce: 0.004397
2022-01-09 13:59:39,002 iteration 6078 : loss : 0.016511, loss_ce: 0.008547
2022-01-09 13:59:40,431 iteration 6079 : loss : 0.018320, loss_ce: 0.008747
2022-01-09 13:59:41,731 iteration 6080 : loss : 0.013414, loss_ce: 0.004851
2022-01-09 13:59:43,165 iteration 6081 : loss : 0.026716, loss_ce: 0.011848
2022-01-09 13:59:44,526 iteration 6082 : loss : 0.017761, loss_ce: 0.004730
2022-01-09 13:59:45,934 iteration 6083 : loss : 0.015181, loss_ce: 0.004505
2022-01-09 13:59:47,258 iteration 6084 : loss : 0.016741, loss_ce: 0.004454
2022-01-09 13:59:48,578 iteration 6085 : loss : 0.016457, loss_ce: 0.007330
2022-01-09 13:59:50,090 iteration 6086 : loss : 0.024018, loss_ce: 0.006273
 90%|█████████████████████████▉   | 358/400 [2:32:27<17:07, 24.46s/it]2022-01-09 13:59:51,465 iteration 6087 : loss : 0.014879, loss_ce: 0.006035
2022-01-09 13:59:52,841 iteration 6088 : loss : 0.018141, loss_ce: 0.007784
2022-01-09 13:59:54,192 iteration 6089 : loss : 0.018917, loss_ce: 0.009299
2022-01-09 13:59:55,549 iteration 6090 : loss : 0.014622, loss_ce: 0.004390
2022-01-09 13:59:56,911 iteration 6091 : loss : 0.018640, loss_ce: 0.008413
2022-01-09 13:59:58,283 iteration 6092 : loss : 0.016272, loss_ce: 0.006585
2022-01-09 13:59:59,579 iteration 6093 : loss : 0.015012, loss_ce: 0.005329
2022-01-09 14:00:00,892 iteration 6094 : loss : 0.023587, loss_ce: 0.007130
2022-01-09 14:00:02,279 iteration 6095 : loss : 0.019533, loss_ce: 0.008582
2022-01-09 14:00:03,672 iteration 6096 : loss : 0.018981, loss_ce: 0.006672
2022-01-09 14:00:04,969 iteration 6097 : loss : 0.017581, loss_ce: 0.008085
2022-01-09 14:00:06,340 iteration 6098 : loss : 0.028569, loss_ce: 0.012515
2022-01-09 14:00:07,747 iteration 6099 : loss : 0.027282, loss_ce: 0.007536
2022-01-09 14:00:09,193 iteration 6100 : loss : 0.027339, loss_ce: 0.008967
2022-01-09 14:00:10,608 iteration 6101 : loss : 0.015669, loss_ce: 0.006392
2022-01-09 14:00:11,937 iteration 6102 : loss : 0.017061, loss_ce: 0.005071
2022-01-09 14:00:13,199 iteration 6103 : loss : 0.010954, loss_ce: 0.004181
 90%|██████████████████████████   | 359/400 [2:32:50<16:26, 24.06s/it]2022-01-09 14:00:14,667 iteration 6104 : loss : 0.020072, loss_ce: 0.005754
2022-01-09 14:00:15,983 iteration 6105 : loss : 0.017916, loss_ce: 0.006130
2022-01-09 14:00:17,267 iteration 6106 : loss : 0.015446, loss_ce: 0.005754
2022-01-09 14:00:18,580 iteration 6107 : loss : 0.014834, loss_ce: 0.004339
2022-01-09 14:00:19,991 iteration 6108 : loss : 0.021227, loss_ce: 0.006888
2022-01-09 14:00:21,329 iteration 6109 : loss : 0.014518, loss_ce: 0.005237
2022-01-09 14:00:22,705 iteration 6110 : loss : 0.020950, loss_ce: 0.010761
2022-01-09 14:00:24,127 iteration 6111 : loss : 0.025624, loss_ce: 0.010058
2022-01-09 14:00:25,525 iteration 6112 : loss : 0.017845, loss_ce: 0.006553
2022-01-09 14:00:26,957 iteration 6113 : loss : 0.021691, loss_ce: 0.007709
2022-01-09 14:00:28,308 iteration 6114 : loss : 0.017023, loss_ce: 0.007620
2022-01-09 14:00:29,749 iteration 6115 : loss : 0.032587, loss_ce: 0.012387
2022-01-09 14:00:31,150 iteration 6116 : loss : 0.014651, loss_ce: 0.006367
2022-01-09 14:00:32,486 iteration 6117 : loss : 0.024045, loss_ce: 0.010372
2022-01-09 14:00:33,887 iteration 6118 : loss : 0.018080, loss_ce: 0.007469
2022-01-09 14:00:35,248 iteration 6119 : loss : 0.021921, loss_ce: 0.007073
2022-01-09 14:00:35,248 Training Data Eval:
2022-01-09 14:00:42,126   Average segmentation loss on training set: 0.0102
2022-01-09 14:00:42,126 Validation Data Eval:
2022-01-09 14:00:44,501   Average segmentation loss on validation set: 0.0744
2022-01-09 14:00:45,854 iteration 6120 : loss : 0.014995, loss_ce: 0.006952
 90%|██████████████████████████   | 360/400 [2:33:23<17:45, 26.64s/it]2022-01-09 14:00:47,300 iteration 6121 : loss : 0.015971, loss_ce: 0.007269
2022-01-09 14:00:48,714 iteration 6122 : loss : 0.020689, loss_ce: 0.007185
2022-01-09 14:00:50,155 iteration 6123 : loss : 0.019793, loss_ce: 0.008362
2022-01-09 14:00:51,540 iteration 6124 : loss : 0.017752, loss_ce: 0.005413
2022-01-09 14:00:52,997 iteration 6125 : loss : 0.029822, loss_ce: 0.009447
2022-01-09 14:00:54,355 iteration 6126 : loss : 0.026655, loss_ce: 0.005839
2022-01-09 14:00:55,696 iteration 6127 : loss : 0.016554, loss_ce: 0.006284
2022-01-09 14:00:57,138 iteration 6128 : loss : 0.016747, loss_ce: 0.005551
2022-01-09 14:00:58,514 iteration 6129 : loss : 0.016565, loss_ce: 0.008348
2022-01-09 14:00:59,968 iteration 6130 : loss : 0.019238, loss_ce: 0.007755
2022-01-09 14:01:01,356 iteration 6131 : loss : 0.018738, loss_ce: 0.009336
2022-01-09 14:01:02,744 iteration 6132 : loss : 0.020696, loss_ce: 0.004677
2022-01-09 14:01:04,076 iteration 6133 : loss : 0.014005, loss_ce: 0.006580
2022-01-09 14:01:05,373 iteration 6134 : loss : 0.011662, loss_ce: 0.005036
2022-01-09 14:01:06,675 iteration 6135 : loss : 0.014077, loss_ce: 0.005642
2022-01-09 14:01:08,059 iteration 6136 : loss : 0.022492, loss_ce: 0.008340
2022-01-09 14:01:09,404 iteration 6137 : loss : 0.016922, loss_ce: 0.006693
 90%|██████████████████████████▏  | 361/400 [2:33:47<16:42, 25.71s/it]2022-01-09 14:01:10,781 iteration 6138 : loss : 0.014839, loss_ce: 0.005280
2022-01-09 14:01:12,184 iteration 6139 : loss : 0.017070, loss_ce: 0.006983
2022-01-09 14:01:13,541 iteration 6140 : loss : 0.012988, loss_ce: 0.004868
2022-01-09 14:01:14,945 iteration 6141 : loss : 0.013509, loss_ce: 0.004903
2022-01-09 14:01:16,457 iteration 6142 : loss : 0.022149, loss_ce: 0.008853
2022-01-09 14:01:17,768 iteration 6143 : loss : 0.023618, loss_ce: 0.011823
2022-01-09 14:01:19,154 iteration 6144 : loss : 0.013812, loss_ce: 0.005880
2022-01-09 14:01:20,540 iteration 6145 : loss : 0.035352, loss_ce: 0.010560
2022-01-09 14:01:21,908 iteration 6146 : loss : 0.015771, loss_ce: 0.006902
2022-01-09 14:01:23,272 iteration 6147 : loss : 0.019864, loss_ce: 0.005205
2022-01-09 14:01:24,594 iteration 6148 : loss : 0.011535, loss_ce: 0.003747
2022-01-09 14:01:25,911 iteration 6149 : loss : 0.019722, loss_ce: 0.007264
2022-01-09 14:01:27,255 iteration 6150 : loss : 0.027297, loss_ce: 0.007850
2022-01-09 14:01:28,610 iteration 6151 : loss : 0.027584, loss_ce: 0.005926
2022-01-09 14:01:29,929 iteration 6152 : loss : 0.018908, loss_ce: 0.007742
2022-01-09 14:01:31,351 iteration 6153 : loss : 0.019631, loss_ce: 0.008421
2022-01-09 14:01:32,806 iteration 6154 : loss : 0.019158, loss_ce: 0.007685
 90%|██████████████████████████▏  | 362/400 [2:34:10<15:50, 25.01s/it]2022-01-09 14:01:34,202 iteration 6155 : loss : 0.018033, loss_ce: 0.007761
2022-01-09 14:01:35,563 iteration 6156 : loss : 0.011828, loss_ce: 0.005075
2022-01-09 14:01:36,977 iteration 6157 : loss : 0.020239, loss_ce: 0.006375
2022-01-09 14:01:38,320 iteration 6158 : loss : 0.019067, loss_ce: 0.007377
2022-01-09 14:01:39,597 iteration 6159 : loss : 0.013519, loss_ce: 0.005149
2022-01-09 14:01:41,140 iteration 6160 : loss : 0.027176, loss_ce: 0.012938
2022-01-09 14:01:42,437 iteration 6161 : loss : 0.031357, loss_ce: 0.009913
2022-01-09 14:01:43,825 iteration 6162 : loss : 0.015822, loss_ce: 0.005825
2022-01-09 14:01:45,181 iteration 6163 : loss : 0.016139, loss_ce: 0.005703
2022-01-09 14:01:46,514 iteration 6164 : loss : 0.016047, loss_ce: 0.004701
2022-01-09 14:01:47,888 iteration 6165 : loss : 0.015931, loss_ce: 0.005883
2022-01-09 14:01:49,290 iteration 6166 : loss : 0.021144, loss_ce: 0.008893
2022-01-09 14:01:50,587 iteration 6167 : loss : 0.013281, loss_ce: 0.004439
2022-01-09 14:01:52,055 iteration 6168 : loss : 0.031188, loss_ce: 0.009850
2022-01-09 14:01:53,411 iteration 6169 : loss : 0.014937, loss_ce: 0.006124
2022-01-09 14:01:54,814 iteration 6170 : loss : 0.020369, loss_ce: 0.009838
2022-01-09 14:01:56,173 iteration 6171 : loss : 0.019376, loss_ce: 0.005264
 91%|██████████████████████████▎  | 363/400 [2:34:33<15:07, 24.52s/it]2022-01-09 14:01:57,591 iteration 6172 : loss : 0.016190, loss_ce: 0.005828
2022-01-09 14:01:58,870 iteration 6173 : loss : 0.015591, loss_ce: 0.005571
2022-01-09 14:02:00,212 iteration 6174 : loss : 0.018439, loss_ce: 0.008182
2022-01-09 14:02:01,512 iteration 6175 : loss : 0.011387, loss_ce: 0.003252
2022-01-09 14:02:02,896 iteration 6176 : loss : 0.018586, loss_ce: 0.009572
2022-01-09 14:02:04,358 iteration 6177 : loss : 0.028815, loss_ce: 0.013405
2022-01-09 14:02:05,777 iteration 6178 : loss : 0.026773, loss_ce: 0.011018
2022-01-09 14:02:07,177 iteration 6179 : loss : 0.020222, loss_ce: 0.005961
2022-01-09 14:02:08,572 iteration 6180 : loss : 0.025827, loss_ce: 0.004160
2022-01-09 14:02:09,911 iteration 6181 : loss : 0.018286, loss_ce: 0.006859
2022-01-09 14:02:11,298 iteration 6182 : loss : 0.014386, loss_ce: 0.005692
2022-01-09 14:02:12,626 iteration 6183 : loss : 0.023181, loss_ce: 0.008152
2022-01-09 14:02:13,947 iteration 6184 : loss : 0.015445, loss_ce: 0.006366
2022-01-09 14:02:15,358 iteration 6185 : loss : 0.026151, loss_ce: 0.007706
2022-01-09 14:02:16,668 iteration 6186 : loss : 0.014376, loss_ce: 0.005321
2022-01-09 14:02:17,993 iteration 6187 : loss : 0.016578, loss_ce: 0.005225
2022-01-09 14:02:19,306 iteration 6188 : loss : 0.017711, loss_ce: 0.006465
 91%|██████████████████████████▍  | 364/400 [2:34:57<14:27, 24.11s/it]2022-01-09 14:02:20,810 iteration 6189 : loss : 0.019805, loss_ce: 0.007270
2022-01-09 14:02:22,188 iteration 6190 : loss : 0.020241, loss_ce: 0.008783
2022-01-09 14:02:23,627 iteration 6191 : loss : 0.016198, loss_ce: 0.004697
2022-01-09 14:02:25,030 iteration 6192 : loss : 0.050250, loss_ce: 0.019824
2022-01-09 14:02:26,364 iteration 6193 : loss : 0.013899, loss_ce: 0.004273
2022-01-09 14:02:27,775 iteration 6194 : loss : 0.019133, loss_ce: 0.007906
2022-01-09 14:02:29,213 iteration 6195 : loss : 0.020707, loss_ce: 0.007005
2022-01-09 14:02:30,543 iteration 6196 : loss : 0.021022, loss_ce: 0.008510
2022-01-09 14:02:32,000 iteration 6197 : loss : 0.027342, loss_ce: 0.009042
2022-01-09 14:02:33,463 iteration 6198 : loss : 0.025157, loss_ce: 0.011466
2022-01-09 14:02:34,799 iteration 6199 : loss : 0.012329, loss_ce: 0.004733
2022-01-09 14:02:36,204 iteration 6200 : loss : 0.022307, loss_ce: 0.010354
2022-01-09 14:02:37,579 iteration 6201 : loss : 0.018159, loss_ce: 0.007595
2022-01-09 14:02:38,935 iteration 6202 : loss : 0.016960, loss_ce: 0.004844
2022-01-09 14:02:40,280 iteration 6203 : loss : 0.024328, loss_ce: 0.007513
2022-01-09 14:02:41,692 iteration 6204 : loss : 0.019664, loss_ce: 0.008330
2022-01-09 14:02:41,692 Training Data Eval:
2022-01-09 14:02:48,570   Average segmentation loss on training set: 0.0103
2022-01-09 14:02:48,570 Validation Data Eval:
2022-01-09 14:02:50,955   Average segmentation loss on validation set: 0.0711
2022-01-09 14:02:52,419 iteration 6205 : loss : 0.024923, loss_ce: 0.012342
 91%|██████████████████████████▍  | 365/400 [2:35:30<15:38, 26.81s/it]2022-01-09 14:02:53,870 iteration 6206 : loss : 0.017894, loss_ce: 0.008436
2022-01-09 14:02:55,249 iteration 6207 : loss : 0.017997, loss_ce: 0.009204
2022-01-09 14:02:56,648 iteration 6208 : loss : 0.018028, loss_ce: 0.006996
2022-01-09 14:02:58,039 iteration 6209 : loss : 0.024114, loss_ce: 0.008529
2022-01-09 14:02:59,396 iteration 6210 : loss : 0.021582, loss_ce: 0.008584
2022-01-09 14:03:00,709 iteration 6211 : loss : 0.013894, loss_ce: 0.004893
2022-01-09 14:03:02,056 iteration 6212 : loss : 0.016100, loss_ce: 0.005996
2022-01-09 14:03:03,426 iteration 6213 : loss : 0.017102, loss_ce: 0.006105
2022-01-09 14:03:04,779 iteration 6214 : loss : 0.016647, loss_ce: 0.007213
2022-01-09 14:03:06,204 iteration 6215 : loss : 0.022610, loss_ce: 0.008143
2022-01-09 14:03:07,565 iteration 6216 : loss : 0.014060, loss_ce: 0.005301
2022-01-09 14:03:08,866 iteration 6217 : loss : 0.028412, loss_ce: 0.012833
2022-01-09 14:03:10,240 iteration 6218 : loss : 0.022614, loss_ce: 0.009590
2022-01-09 14:03:11,603 iteration 6219 : loss : 0.013599, loss_ce: 0.004584
2022-01-09 14:03:12,939 iteration 6220 : loss : 0.016927, loss_ce: 0.003864
2022-01-09 14:03:14,323 iteration 6221 : loss : 0.014192, loss_ce: 0.004581
2022-01-09 14:03:15,649 iteration 6222 : loss : 0.013428, loss_ce: 0.006346
 92%|██████████████████████████▌  | 366/400 [2:35:53<14:34, 25.73s/it]2022-01-09 14:03:17,102 iteration 6223 : loss : 0.020754, loss_ce: 0.009791
2022-01-09 14:03:18,461 iteration 6224 : loss : 0.016314, loss_ce: 0.007202
2022-01-09 14:03:19,835 iteration 6225 : loss : 0.026654, loss_ce: 0.007413
2022-01-09 14:03:21,168 iteration 6226 : loss : 0.015757, loss_ce: 0.005565
2022-01-09 14:03:22,481 iteration 6227 : loss : 0.015132, loss_ce: 0.005826
2022-01-09 14:03:23,858 iteration 6228 : loss : 0.015743, loss_ce: 0.005474
2022-01-09 14:03:25,235 iteration 6229 : loss : 0.014275, loss_ce: 0.007100
2022-01-09 14:03:26,629 iteration 6230 : loss : 0.016087, loss_ce: 0.005568
2022-01-09 14:03:28,046 iteration 6231 : loss : 0.023250, loss_ce: 0.008939
2022-01-09 14:03:29,431 iteration 6232 : loss : 0.017279, loss_ce: 0.007708
2022-01-09 14:03:30,795 iteration 6233 : loss : 0.017094, loss_ce: 0.007373
2022-01-09 14:03:32,141 iteration 6234 : loss : 0.017018, loss_ce: 0.003840
2022-01-09 14:03:33,467 iteration 6235 : loss : 0.019091, loss_ce: 0.010627
2022-01-09 14:03:34,836 iteration 6236 : loss : 0.014127, loss_ce: 0.006696
2022-01-09 14:03:36,226 iteration 6237 : loss : 0.019492, loss_ce: 0.009015
2022-01-09 14:03:37,661 iteration 6238 : loss : 0.022010, loss_ce: 0.006398
2022-01-09 14:03:39,043 iteration 6239 : loss : 0.016216, loss_ce: 0.004783
 92%|██████████████████████████▌  | 367/400 [2:36:16<13:46, 25.03s/it]2022-01-09 14:03:40,457 iteration 6240 : loss : 0.012979, loss_ce: 0.004173
2022-01-09 14:03:41,785 iteration 6241 : loss : 0.013197, loss_ce: 0.006482
2022-01-09 14:03:43,222 iteration 6242 : loss : 0.018984, loss_ce: 0.005724
2022-01-09 14:03:44,576 iteration 6243 : loss : 0.014449, loss_ce: 0.005755
2022-01-09 14:03:46,025 iteration 6244 : loss : 0.017974, loss_ce: 0.006941
2022-01-09 14:03:47,386 iteration 6245 : loss : 0.018719, loss_ce: 0.006746
2022-01-09 14:03:48,733 iteration 6246 : loss : 0.016047, loss_ce: 0.007070
2022-01-09 14:03:50,119 iteration 6247 : loss : 0.019871, loss_ce: 0.004598
2022-01-09 14:03:51,477 iteration 6248 : loss : 0.013310, loss_ce: 0.005007
2022-01-09 14:03:52,866 iteration 6249 : loss : 0.018096, loss_ce: 0.005667
2022-01-09 14:03:54,268 iteration 6250 : loss : 0.016067, loss_ce: 0.005938
2022-01-09 14:03:55,726 iteration 6251 : loss : 0.019566, loss_ce: 0.007926
2022-01-09 14:03:57,056 iteration 6252 : loss : 0.014380, loss_ce: 0.006371
2022-01-09 14:03:58,471 iteration 6253 : loss : 0.013985, loss_ce: 0.004678
2022-01-09 14:03:59,816 iteration 6254 : loss : 0.020335, loss_ce: 0.010212
2022-01-09 14:04:01,201 iteration 6255 : loss : 0.022073, loss_ce: 0.012619
2022-01-09 14:04:02,646 iteration 6256 : loss : 0.032625, loss_ce: 0.012514
 92%|██████████████████████████▋  | 368/400 [2:36:40<13:07, 24.60s/it]2022-01-09 14:04:04,117 iteration 6257 : loss : 0.019436, loss_ce: 0.006288
2022-01-09 14:04:05,530 iteration 6258 : loss : 0.024691, loss_ce: 0.008919
2022-01-09 14:04:06,973 iteration 6259 : loss : 0.020081, loss_ce: 0.007783
2022-01-09 14:04:08,330 iteration 6260 : loss : 0.023109, loss_ce: 0.009554
2022-01-09 14:04:09,688 iteration 6261 : loss : 0.016677, loss_ce: 0.005709
2022-01-09 14:04:11,081 iteration 6262 : loss : 0.028061, loss_ce: 0.008056
2022-01-09 14:04:12,473 iteration 6263 : loss : 0.025909, loss_ce: 0.006224
2022-01-09 14:04:13,863 iteration 6264 : loss : 0.015070, loss_ce: 0.005518
2022-01-09 14:04:15,253 iteration 6265 : loss : 0.016396, loss_ce: 0.006194
2022-01-09 14:04:16,602 iteration 6266 : loss : 0.017124, loss_ce: 0.008216
2022-01-09 14:04:17,935 iteration 6267 : loss : 0.018347, loss_ce: 0.007961
2022-01-09 14:04:19,303 iteration 6268 : loss : 0.014334, loss_ce: 0.005580
2022-01-09 14:04:20,549 iteration 6269 : loss : 0.012913, loss_ce: 0.004489
2022-01-09 14:04:21,901 iteration 6270 : loss : 0.012018, loss_ce: 0.005452
2022-01-09 14:04:23,252 iteration 6271 : loss : 0.017682, loss_ce: 0.005082
2022-01-09 14:04:24,664 iteration 6272 : loss : 0.024147, loss_ce: 0.008155
2022-01-09 14:04:26,061 iteration 6273 : loss : 0.022297, loss_ce: 0.008329
 92%|██████████████████████████▊  | 369/400 [2:37:03<12:31, 24.25s/it]2022-01-09 14:04:27,617 iteration 6274 : loss : 0.030311, loss_ce: 0.010104
2022-01-09 14:04:29,007 iteration 6275 : loss : 0.019250, loss_ce: 0.008818
2022-01-09 14:04:30,396 iteration 6276 : loss : 0.015082, loss_ce: 0.005938
2022-01-09 14:04:31,732 iteration 6277 : loss : 0.019236, loss_ce: 0.004304
2022-01-09 14:04:33,115 iteration 6278 : loss : 0.022914, loss_ce: 0.007659
2022-01-09 14:04:34,476 iteration 6279 : loss : 0.032755, loss_ce: 0.012173
2022-01-09 14:04:35,809 iteration 6280 : loss : 0.024719, loss_ce: 0.011350
2022-01-09 14:04:37,291 iteration 6281 : loss : 0.022761, loss_ce: 0.005517
2022-01-09 14:04:38,649 iteration 6282 : loss : 0.014713, loss_ce: 0.005857
2022-01-09 14:04:39,986 iteration 6283 : loss : 0.012314, loss_ce: 0.005188
2022-01-09 14:04:41,361 iteration 6284 : loss : 0.018993, loss_ce: 0.008456
2022-01-09 14:04:42,774 iteration 6285 : loss : 0.013901, loss_ce: 0.004296
2022-01-09 14:04:44,123 iteration 6286 : loss : 0.018627, loss_ce: 0.006303
2022-01-09 14:04:45,467 iteration 6287 : loss : 0.020136, loss_ce: 0.004964
2022-01-09 14:04:46,761 iteration 6288 : loss : 0.014102, loss_ce: 0.004878
2022-01-09 14:04:48,150 iteration 6289 : loss : 0.016010, loss_ce: 0.007157
2022-01-09 14:04:48,150 Training Data Eval:
2022-01-09 14:04:55,030   Average segmentation loss on training set: 0.0097
2022-01-09 14:04:55,031 Validation Data Eval:
2022-01-09 14:04:57,403   Average segmentation loss on validation set: 0.0732
2022-01-09 14:04:58,811 iteration 6290 : loss : 0.025359, loss_ce: 0.010999
 92%|██████████████████████████▊  | 370/400 [2:37:36<13:23, 26.80s/it]2022-01-09 14:05:00,238 iteration 6291 : loss : 0.013385, loss_ce: 0.005629
2022-01-09 14:05:01,608 iteration 6292 : loss : 0.022991, loss_ce: 0.008030
2022-01-09 14:05:03,007 iteration 6293 : loss : 0.023792, loss_ce: 0.011578
2022-01-09 14:05:04,421 iteration 6294 : loss : 0.020724, loss_ce: 0.006915
2022-01-09 14:05:05,792 iteration 6295 : loss : 0.012953, loss_ce: 0.005308
2022-01-09 14:05:07,236 iteration 6296 : loss : 0.035237, loss_ce: 0.015893
2022-01-09 14:05:08,617 iteration 6297 : loss : 0.020485, loss_ce: 0.007357
2022-01-09 14:05:10,032 iteration 6298 : loss : 0.025987, loss_ce: 0.009692
2022-01-09 14:05:11,395 iteration 6299 : loss : 0.018071, loss_ce: 0.006787
2022-01-09 14:05:12,737 iteration 6300 : loss : 0.015159, loss_ce: 0.005232
2022-01-09 14:05:14,075 iteration 6301 : loss : 0.015401, loss_ce: 0.010217
2022-01-09 14:05:15,458 iteration 6302 : loss : 0.018185, loss_ce: 0.006524
2022-01-09 14:05:16,745 iteration 6303 : loss : 0.012659, loss_ce: 0.002938
2022-01-09 14:05:18,043 iteration 6304 : loss : 0.016567, loss_ce: 0.004961
2022-01-09 14:05:19,431 iteration 6305 : loss : 0.017933, loss_ce: 0.007868
2022-01-09 14:05:20,779 iteration 6306 : loss : 0.026515, loss_ce: 0.006352
2022-01-09 14:05:22,140 iteration 6307 : loss : 0.013023, loss_ce: 0.006030
 93%|██████████████████████████▉  | 371/400 [2:37:59<12:26, 25.76s/it]2022-01-09 14:05:23,522 iteration 6308 : loss : 0.025859, loss_ce: 0.004625
2022-01-09 14:05:24,901 iteration 6309 : loss : 0.020975, loss_ce: 0.008253
2022-01-09 14:05:26,258 iteration 6310 : loss : 0.015780, loss_ce: 0.007123
2022-01-09 14:05:27,636 iteration 6311 : loss : 0.013145, loss_ce: 0.005767
2022-01-09 14:05:29,026 iteration 6312 : loss : 0.021709, loss_ce: 0.007587
2022-01-09 14:05:30,465 iteration 6313 : loss : 0.023080, loss_ce: 0.011148
2022-01-09 14:05:31,828 iteration 6314 : loss : 0.013831, loss_ce: 0.004452
2022-01-09 14:05:33,208 iteration 6315 : loss : 0.017022, loss_ce: 0.006122
2022-01-09 14:05:34,569 iteration 6316 : loss : 0.014520, loss_ce: 0.005641
2022-01-09 14:05:35,925 iteration 6317 : loss : 0.015853, loss_ce: 0.004451
2022-01-09 14:05:37,349 iteration 6318 : loss : 0.023597, loss_ce: 0.010184
2022-01-09 14:05:38,743 iteration 6319 : loss : 0.017526, loss_ce: 0.006043
2022-01-09 14:05:40,100 iteration 6320 : loss : 0.019193, loss_ce: 0.006571
2022-01-09 14:05:41,514 iteration 6321 : loss : 0.022329, loss_ce: 0.011806
2022-01-09 14:05:42,942 iteration 6322 : loss : 0.017963, loss_ce: 0.008518
2022-01-09 14:05:44,449 iteration 6323 : loss : 0.024672, loss_ce: 0.008577
2022-01-09 14:05:45,896 iteration 6324 : loss : 0.021569, loss_ce: 0.011368
 93%|██████████████████████████▉  | 372/400 [2:38:23<11:44, 25.16s/it]2022-01-09 14:05:47,365 iteration 6325 : loss : 0.018751, loss_ce: 0.004700
2022-01-09 14:05:48,713 iteration 6326 : loss : 0.014048, loss_ce: 0.005847
2022-01-09 14:05:50,081 iteration 6327 : loss : 0.017800, loss_ce: 0.005885
2022-01-09 14:05:51,507 iteration 6328 : loss : 0.017035, loss_ce: 0.008620
2022-01-09 14:05:52,819 iteration 6329 : loss : 0.014532, loss_ce: 0.007138
2022-01-09 14:05:54,178 iteration 6330 : loss : 0.017427, loss_ce: 0.007175
2022-01-09 14:05:55,604 iteration 6331 : loss : 0.018411, loss_ce: 0.007022
2022-01-09 14:05:57,052 iteration 6332 : loss : 0.012412, loss_ce: 0.003266
2022-01-09 14:05:58,426 iteration 6333 : loss : 0.012853, loss_ce: 0.005761
2022-01-09 14:05:59,800 iteration 6334 : loss : 0.025831, loss_ce: 0.006689
2022-01-09 14:06:01,260 iteration 6335 : loss : 0.015028, loss_ce: 0.006015
2022-01-09 14:06:02,607 iteration 6336 : loss : 0.015134, loss_ce: 0.006029
2022-01-09 14:06:03,997 iteration 6337 : loss : 0.016277, loss_ce: 0.004780
2022-01-09 14:06:05,373 iteration 6338 : loss : 0.018584, loss_ce: 0.005820
2022-01-09 14:06:06,744 iteration 6339 : loss : 0.015568, loss_ce: 0.005344
2022-01-09 14:06:08,118 iteration 6340 : loss : 0.018891, loss_ce: 0.008145
2022-01-09 14:06:09,415 iteration 6341 : loss : 0.018488, loss_ce: 0.007791
 93%|███████████████████████████  | 373/400 [2:38:47<11:05, 24.67s/it]2022-01-09 14:06:10,887 iteration 6342 : loss : 0.024538, loss_ce: 0.008241
2022-01-09 14:06:12,252 iteration 6343 : loss : 0.015849, loss_ce: 0.006412
2022-01-09 14:06:13,605 iteration 6344 : loss : 0.017394, loss_ce: 0.004824
2022-01-09 14:06:14,946 iteration 6345 : loss : 0.040503, loss_ce: 0.008904
2022-01-09 14:06:16,328 iteration 6346 : loss : 0.018012, loss_ce: 0.005977
2022-01-09 14:06:17,708 iteration 6347 : loss : 0.020695, loss_ce: 0.007854
2022-01-09 14:06:18,987 iteration 6348 : loss : 0.011143, loss_ce: 0.004682
2022-01-09 14:06:20,370 iteration 6349 : loss : 0.017047, loss_ce: 0.006985
2022-01-09 14:06:21,849 iteration 6350 : loss : 0.027262, loss_ce: 0.008959
2022-01-09 14:06:23,275 iteration 6351 : loss : 0.017839, loss_ce: 0.008497
2022-01-09 14:06:24,650 iteration 6352 : loss : 0.017925, loss_ce: 0.005690
2022-01-09 14:06:26,030 iteration 6353 : loss : 0.020248, loss_ce: 0.007882
2022-01-09 14:06:27,402 iteration 6354 : loss : 0.021544, loss_ce: 0.009196
2022-01-09 14:06:28,751 iteration 6355 : loss : 0.016715, loss_ce: 0.005844
2022-01-09 14:06:30,181 iteration 6356 : loss : 0.018026, loss_ce: 0.004608
2022-01-09 14:06:31,526 iteration 6357 : loss : 0.018311, loss_ce: 0.006540
2022-01-09 14:06:33,005 iteration 6358 : loss : 0.021454, loss_ce: 0.008982
 94%|███████████████████████████  | 374/400 [2:39:10<10:32, 24.34s/it]2022-01-09 14:06:34,494 iteration 6359 : loss : 0.033547, loss_ce: 0.007296
2022-01-09 14:06:35,817 iteration 6360 : loss : 0.016904, loss_ce: 0.007606
2022-01-09 14:06:37,218 iteration 6361 : loss : 0.015880, loss_ce: 0.005351
2022-01-09 14:06:38,623 iteration 6362 : loss : 0.026660, loss_ce: 0.011524
2022-01-09 14:06:40,058 iteration 6363 : loss : 0.017992, loss_ce: 0.006760
2022-01-09 14:06:41,398 iteration 6364 : loss : 0.013124, loss_ce: 0.003456
2022-01-09 14:06:42,779 iteration 6365 : loss : 0.017183, loss_ce: 0.009250
2022-01-09 14:06:44,237 iteration 6366 : loss : 0.024254, loss_ce: 0.010329
2022-01-09 14:06:45,720 iteration 6367 : loss : 0.026673, loss_ce: 0.010520
2022-01-09 14:06:47,145 iteration 6368 : loss : 0.020683, loss_ce: 0.008244
2022-01-09 14:06:48,521 iteration 6369 : loss : 0.013022, loss_ce: 0.005409
2022-01-09 14:06:49,906 iteration 6370 : loss : 0.017012, loss_ce: 0.006002
2022-01-09 14:06:51,377 iteration 6371 : loss : 0.017205, loss_ce: 0.005350
2022-01-09 14:06:52,716 iteration 6372 : loss : 0.013495, loss_ce: 0.005434
2022-01-09 14:06:54,048 iteration 6373 : loss : 0.013661, loss_ce: 0.005001
2022-01-09 14:06:55,427 iteration 6374 : loss : 0.025825, loss_ce: 0.011311
2022-01-09 14:06:55,427 Training Data Eval:
2022-01-09 14:07:02,300   Average segmentation loss on training set: 0.0097
2022-01-09 14:07:02,301 Validation Data Eval:
2022-01-09 14:07:04,676   Average segmentation loss on validation set: 0.0740
2022-01-09 14:07:06,061 iteration 6375 : loss : 0.027260, loss_ce: 0.008151
 94%|███████████████████████████▏ | 375/400 [2:39:43<11:13, 26.96s/it]2022-01-09 14:07:07,533 iteration 6376 : loss : 0.019032, loss_ce: 0.005925
2022-01-09 14:07:08,923 iteration 6377 : loss : 0.021910, loss_ce: 0.007838
2022-01-09 14:07:10,207 iteration 6378 : loss : 0.013524, loss_ce: 0.005973
2022-01-09 14:07:11,570 iteration 6379 : loss : 0.015537, loss_ce: 0.003771
2022-01-09 14:07:12,913 iteration 6380 : loss : 0.012375, loss_ce: 0.004685
2022-01-09 14:07:14,246 iteration 6381 : loss : 0.011171, loss_ce: 0.003575
2022-01-09 14:07:15,530 iteration 6382 : loss : 0.016564, loss_ce: 0.006098
2022-01-09 14:07:16,937 iteration 6383 : loss : 0.022322, loss_ce: 0.009788
2022-01-09 14:07:18,272 iteration 6384 : loss : 0.016614, loss_ce: 0.005999
2022-01-09 14:07:19,674 iteration 6385 : loss : 0.023747, loss_ce: 0.008240
2022-01-09 14:07:21,091 iteration 6386 : loss : 0.029591, loss_ce: 0.009931
2022-01-09 14:07:22,556 iteration 6387 : loss : 0.018420, loss_ce: 0.008395
2022-01-09 14:07:23,966 iteration 6388 : loss : 0.016980, loss_ce: 0.007444
2022-01-09 14:07:25,234 iteration 6389 : loss : 0.014663, loss_ce: 0.005243
2022-01-09 14:07:26,603 iteration 6390 : loss : 0.018170, loss_ce: 0.007999
2022-01-09 14:07:27,929 iteration 6391 : loss : 0.018333, loss_ce: 0.011277
2022-01-09 14:07:29,364 iteration 6392 : loss : 0.024253, loss_ce: 0.007046
 94%|███████████████████████████▎ | 376/400 [2:40:07<10:20, 25.86s/it]2022-01-09 14:07:30,788 iteration 6393 : loss : 0.020208, loss_ce: 0.007883
2022-01-09 14:07:32,211 iteration 6394 : loss : 0.027122, loss_ce: 0.010525
2022-01-09 14:07:33,571 iteration 6395 : loss : 0.015178, loss_ce: 0.005001
2022-01-09 14:07:34,940 iteration 6396 : loss : 0.013796, loss_ce: 0.004969
2022-01-09 14:07:36,331 iteration 6397 : loss : 0.020431, loss_ce: 0.007904
2022-01-09 14:07:37,734 iteration 6398 : loss : 0.014116, loss_ce: 0.004925
2022-01-09 14:07:39,098 iteration 6399 : loss : 0.020159, loss_ce: 0.006339
2022-01-09 14:07:40,454 iteration 6400 : loss : 0.017900, loss_ce: 0.007732
2022-01-09 14:07:41,854 iteration 6401 : loss : 0.014731, loss_ce: 0.005668
2022-01-09 14:07:43,211 iteration 6402 : loss : 0.017104, loss_ce: 0.009537
2022-01-09 14:07:44,654 iteration 6403 : loss : 0.022066, loss_ce: 0.009109
2022-01-09 14:07:46,010 iteration 6404 : loss : 0.016015, loss_ce: 0.006795
2022-01-09 14:07:47,350 iteration 6405 : loss : 0.023787, loss_ce: 0.007568
2022-01-09 14:07:48,867 iteration 6406 : loss : 0.062117, loss_ce: 0.019525
2022-01-09 14:07:50,269 iteration 6407 : loss : 0.025735, loss_ce: 0.006983
2022-01-09 14:07:51,540 iteration 6408 : loss : 0.012539, loss_ce: 0.004456
2022-01-09 14:07:52,894 iteration 6409 : loss : 0.015415, loss_ce: 0.007000
 94%|███████████████████████████▎ | 377/400 [2:40:30<09:38, 25.16s/it]2022-01-09 14:07:54,290 iteration 6410 : loss : 0.012962, loss_ce: 0.004137
2022-01-09 14:07:55,686 iteration 6411 : loss : 0.019532, loss_ce: 0.008442
2022-01-09 14:07:57,086 iteration 6412 : loss : 0.015008, loss_ce: 0.005278
2022-01-09 14:07:58,446 iteration 6413 : loss : 0.016849, loss_ce: 0.007754
2022-01-09 14:07:59,787 iteration 6414 : loss : 0.018584, loss_ce: 0.008304
2022-01-09 14:08:01,107 iteration 6415 : loss : 0.014384, loss_ce: 0.004724
2022-01-09 14:08:02,483 iteration 6416 : loss : 0.021250, loss_ce: 0.008692
2022-01-09 14:08:03,790 iteration 6417 : loss : 0.015416, loss_ce: 0.005984
2022-01-09 14:08:05,131 iteration 6418 : loss : 0.016085, loss_ce: 0.006158
2022-01-09 14:08:06,586 iteration 6419 : loss : 0.018398, loss_ce: 0.007019
2022-01-09 14:08:08,024 iteration 6420 : loss : 0.022563, loss_ce: 0.005276
2022-01-09 14:08:09,433 iteration 6421 : loss : 0.016893, loss_ce: 0.006702
2022-01-09 14:08:10,809 iteration 6422 : loss : 0.016923, loss_ce: 0.005608
2022-01-09 14:08:12,213 iteration 6423 : loss : 0.015956, loss_ce: 0.006600
2022-01-09 14:08:13,681 iteration 6424 : loss : 0.022954, loss_ce: 0.011354
2022-01-09 14:08:15,039 iteration 6425 : loss : 0.014473, loss_ce: 0.004256
2022-01-09 14:08:16,485 iteration 6426 : loss : 0.024169, loss_ce: 0.008840
 94%|███████████████████████████▍ | 378/400 [2:40:54<09:03, 24.69s/it]2022-01-09 14:08:17,950 iteration 6427 : loss : 0.014822, loss_ce: 0.004727
2022-01-09 14:08:19,333 iteration 6428 : loss : 0.017146, loss_ce: 0.005498
2022-01-09 14:08:20,719 iteration 6429 : loss : 0.021492, loss_ce: 0.008328
2022-01-09 14:08:22,144 iteration 6430 : loss : 0.019137, loss_ce: 0.007695
2022-01-09 14:08:23,459 iteration 6431 : loss : 0.014760, loss_ce: 0.005921
2022-01-09 14:08:24,844 iteration 6432 : loss : 0.033099, loss_ce: 0.016221
2022-01-09 14:08:26,215 iteration 6433 : loss : 0.014468, loss_ce: 0.004857
2022-01-09 14:08:27,554 iteration 6434 : loss : 0.016981, loss_ce: 0.006579
2022-01-09 14:08:28,918 iteration 6435 : loss : 0.019270, loss_ce: 0.007984
2022-01-09 14:08:30,272 iteration 6436 : loss : 0.015665, loss_ce: 0.006333
2022-01-09 14:08:31,650 iteration 6437 : loss : 0.016119, loss_ce: 0.005471
2022-01-09 14:08:33,054 iteration 6438 : loss : 0.016175, loss_ce: 0.004392
2022-01-09 14:08:34,387 iteration 6439 : loss : 0.020428, loss_ce: 0.010689
2022-01-09 14:08:35,794 iteration 6440 : loss : 0.021548, loss_ce: 0.010273
2022-01-09 14:08:37,106 iteration 6441 : loss : 0.012274, loss_ce: 0.005344
2022-01-09 14:08:38,488 iteration 6442 : loss : 0.019826, loss_ce: 0.007395
2022-01-09 14:08:39,787 iteration 6443 : loss : 0.012947, loss_ce: 0.004199
 95%|███████████████████████████▍ | 379/400 [2:41:17<08:29, 24.27s/it]2022-01-09 14:08:41,228 iteration 6444 : loss : 0.015933, loss_ce: 0.008415
2022-01-09 14:08:42,598 iteration 6445 : loss : 0.018012, loss_ce: 0.006558
2022-01-09 14:08:43,983 iteration 6446 : loss : 0.017700, loss_ce: 0.007223
2022-01-09 14:08:45,383 iteration 6447 : loss : 0.049798, loss_ce: 0.009944
2022-01-09 14:08:46,793 iteration 6448 : loss : 0.019346, loss_ce: 0.007312
2022-01-09 14:08:48,142 iteration 6449 : loss : 0.014525, loss_ce: 0.003597
2022-01-09 14:08:49,472 iteration 6450 : loss : 0.015236, loss_ce: 0.006910
2022-01-09 14:08:50,845 iteration 6451 : loss : 0.018072, loss_ce: 0.006110
2022-01-09 14:08:52,286 iteration 6452 : loss : 0.019209, loss_ce: 0.006000
2022-01-09 14:08:53,636 iteration 6453 : loss : 0.025415, loss_ce: 0.009729
2022-01-09 14:08:54,946 iteration 6454 : loss : 0.025405, loss_ce: 0.010344
2022-01-09 14:08:56,353 iteration 6455 : loss : 0.020050, loss_ce: 0.009757
2022-01-09 14:08:57,714 iteration 6456 : loss : 0.017417, loss_ce: 0.006287
2022-01-09 14:08:59,108 iteration 6457 : loss : 0.021570, loss_ce: 0.007619
2022-01-09 14:09:00,428 iteration 6458 : loss : 0.017403, loss_ce: 0.006568
2022-01-09 14:09:01,784 iteration 6459 : loss : 0.013220, loss_ce: 0.003241
2022-01-09 14:09:01,784 Training Data Eval:
2022-01-09 14:09:08,647   Average segmentation loss on training set: 0.0095
2022-01-09 14:09:08,647 Validation Data Eval:
2022-01-09 14:09:11,010   Average segmentation loss on validation set: 0.0743
2022-01-09 14:09:12,419 iteration 6460 : loss : 0.019560, loss_ce: 0.009955
 95%|███████████████████████████▌ | 380/400 [2:41:50<08:55, 26.78s/it]2022-01-09 14:09:13,902 iteration 6461 : loss : 0.018513, loss_ce: 0.007486
2022-01-09 14:09:15,249 iteration 6462 : loss : 0.014803, loss_ce: 0.005561
2022-01-09 14:09:16,673 iteration 6463 : loss : 0.030012, loss_ce: 0.015016
2022-01-09 14:09:18,047 iteration 6464 : loss : 0.032543, loss_ce: 0.007534
2022-01-09 14:09:19,457 iteration 6465 : loss : 0.019150, loss_ce: 0.007023
2022-01-09 14:09:20,783 iteration 6466 : loss : 0.016771, loss_ce: 0.005344
2022-01-09 14:09:22,129 iteration 6467 : loss : 0.015867, loss_ce: 0.005984
2022-01-09 14:09:23,562 iteration 6468 : loss : 0.016250, loss_ce: 0.005725
2022-01-09 14:09:24,915 iteration 6469 : loss : 0.015227, loss_ce: 0.006574
2022-01-09 14:09:26,254 iteration 6470 : loss : 0.016673, loss_ce: 0.005085
2022-01-09 14:09:27,624 iteration 6471 : loss : 0.014454, loss_ce: 0.004962
2022-01-09 14:09:29,009 iteration 6472 : loss : 0.024366, loss_ce: 0.007503
2022-01-09 14:09:30,348 iteration 6473 : loss : 0.017537, loss_ce: 0.006167
2022-01-09 14:09:31,783 iteration 6474 : loss : 0.014969, loss_ce: 0.006468
2022-01-09 14:09:33,174 iteration 6475 : loss : 0.034341, loss_ce: 0.019585
2022-01-09 14:09:34,441 iteration 6476 : loss : 0.014329, loss_ce: 0.005361
2022-01-09 14:09:35,897 iteration 6477 : loss : 0.023358, loss_ce: 0.008317
 95%|███████████████████████████▌ | 381/400 [2:42:13<08:10, 25.79s/it]2022-01-09 14:09:37,387 iteration 6478 : loss : 0.029511, loss_ce: 0.010644
2022-01-09 14:09:38,746 iteration 6479 : loss : 0.023741, loss_ce: 0.008795
2022-01-09 14:09:40,105 iteration 6480 : loss : 0.016950, loss_ce: 0.007688
2022-01-09 14:09:41,504 iteration 6481 : loss : 0.017417, loss_ce: 0.008015
2022-01-09 14:09:42,811 iteration 6482 : loss : 0.013733, loss_ce: 0.004793
2022-01-09 14:09:44,216 iteration 6483 : loss : 0.023926, loss_ce: 0.006382
2022-01-09 14:09:45,549 iteration 6484 : loss : 0.016649, loss_ce: 0.007228
2022-01-09 14:09:46,912 iteration 6485 : loss : 0.017307, loss_ce: 0.007169
2022-01-09 14:09:48,335 iteration 6486 : loss : 0.025605, loss_ce: 0.008946
2022-01-09 14:09:49,813 iteration 6487 : loss : 0.019099, loss_ce: 0.007713
2022-01-09 14:09:51,221 iteration 6488 : loss : 0.023310, loss_ce: 0.010628
2022-01-09 14:09:52,561 iteration 6489 : loss : 0.013383, loss_ce: 0.006176
2022-01-09 14:09:53,953 iteration 6490 : loss : 0.013904, loss_ce: 0.003941
2022-01-09 14:09:55,302 iteration 6491 : loss : 0.021977, loss_ce: 0.007629
2022-01-09 14:09:56,753 iteration 6492 : loss : 0.018604, loss_ce: 0.007224
2022-01-09 14:09:58,155 iteration 6493 : loss : 0.022737, loss_ce: 0.008750
2022-01-09 14:09:59,432 iteration 6494 : loss : 0.011031, loss_ce: 0.004325
 96%|███████████████████████████▋ | 382/400 [2:42:37<07:32, 25.11s/it]2022-01-09 14:10:00,837 iteration 6495 : loss : 0.016804, loss_ce: 0.006881
2022-01-09 14:10:02,206 iteration 6496 : loss : 0.026506, loss_ce: 0.006714
2022-01-09 14:10:03,615 iteration 6497 : loss : 0.014775, loss_ce: 0.006016
2022-01-09 14:10:05,024 iteration 6498 : loss : 0.019893, loss_ce: 0.006551
2022-01-09 14:10:06,431 iteration 6499 : loss : 0.020354, loss_ce: 0.009000
2022-01-09 14:10:07,792 iteration 6500 : loss : 0.014124, loss_ce: 0.005501
2022-01-09 14:10:09,191 iteration 6501 : loss : 0.020956, loss_ce: 0.009138
2022-01-09 14:10:10,611 iteration 6502 : loss : 0.017419, loss_ce: 0.009178
2022-01-09 14:10:11,956 iteration 6503 : loss : 0.012459, loss_ce: 0.003241
2022-01-09 14:10:13,276 iteration 6504 : loss : 0.014082, loss_ce: 0.004693
2022-01-09 14:10:14,524 iteration 6505 : loss : 0.013121, loss_ce: 0.006145
2022-01-09 14:10:15,869 iteration 6506 : loss : 0.014070, loss_ce: 0.004398
2022-01-09 14:10:17,300 iteration 6507 : loss : 0.019121, loss_ce: 0.009135
2022-01-09 14:10:18,705 iteration 6508 : loss : 0.020400, loss_ce: 0.007771
2022-01-09 14:10:20,091 iteration 6509 : loss : 0.019447, loss_ce: 0.007906
2022-01-09 14:10:21,480 iteration 6510 : loss : 0.016854, loss_ce: 0.006053
2022-01-09 14:10:22,783 iteration 6511 : loss : 0.013335, loss_ce: 0.004880
 96%|███████████████████████████▊ | 383/400 [2:43:00<06:57, 24.59s/it]2022-01-09 14:10:24,180 iteration 6512 : loss : 0.015476, loss_ce: 0.005044
2022-01-09 14:10:25,554 iteration 6513 : loss : 0.018415, loss_ce: 0.007194
2022-01-09 14:10:27,021 iteration 6514 : loss : 0.019044, loss_ce: 0.006121
2022-01-09 14:10:28,447 iteration 6515 : loss : 0.018288, loss_ce: 0.006145
2022-01-09 14:10:29,849 iteration 6516 : loss : 0.026090, loss_ce: 0.010910
2022-01-09 14:10:31,209 iteration 6517 : loss : 0.023295, loss_ce: 0.004775
2022-01-09 14:10:32,587 iteration 6518 : loss : 0.013882, loss_ce: 0.004808
2022-01-09 14:10:33,955 iteration 6519 : loss : 0.018183, loss_ce: 0.005394
2022-01-09 14:10:35,374 iteration 6520 : loss : 0.016289, loss_ce: 0.005471
2022-01-09 14:10:36,711 iteration 6521 : loss : 0.021359, loss_ce: 0.009708
2022-01-09 14:10:38,096 iteration 6522 : loss : 0.014753, loss_ce: 0.005858
2022-01-09 14:10:39,400 iteration 6523 : loss : 0.017493, loss_ce: 0.007669
2022-01-09 14:10:40,816 iteration 6524 : loss : 0.016924, loss_ce: 0.007877
2022-01-09 14:10:42,200 iteration 6525 : loss : 0.017490, loss_ce: 0.007082
2022-01-09 14:10:43,575 iteration 6526 : loss : 0.018868, loss_ce: 0.007036
2022-01-09 14:10:44,881 iteration 6527 : loss : 0.013575, loss_ce: 0.006088
2022-01-09 14:10:46,212 iteration 6528 : loss : 0.022051, loss_ce: 0.006922
 96%|███████████████████████████▊ | 384/400 [2:43:23<06:27, 24.24s/it]2022-01-09 14:10:47,693 iteration 6529 : loss : 0.021707, loss_ce: 0.006784
2022-01-09 14:10:49,093 iteration 6530 : loss : 0.015077, loss_ce: 0.005083
2022-01-09 14:10:50,405 iteration 6531 : loss : 0.016248, loss_ce: 0.005266
2022-01-09 14:10:51,796 iteration 6532 : loss : 0.016658, loss_ce: 0.006912
2022-01-09 14:10:53,098 iteration 6533 : loss : 0.010972, loss_ce: 0.004190
2022-01-09 14:10:54,472 iteration 6534 : loss : 0.015707, loss_ce: 0.007406
2022-01-09 14:10:55,859 iteration 6535 : loss : 0.021198, loss_ce: 0.008903
2022-01-09 14:10:57,256 iteration 6536 : loss : 0.016333, loss_ce: 0.004935
2022-01-09 14:10:58,642 iteration 6537 : loss : 0.028903, loss_ce: 0.013512
2022-01-09 14:11:00,042 iteration 6538 : loss : 0.014503, loss_ce: 0.004149
2022-01-09 14:11:01,462 iteration 6539 : loss : 0.027118, loss_ce: 0.012966
2022-01-09 14:11:02,761 iteration 6540 : loss : 0.012825, loss_ce: 0.005297
2022-01-09 14:11:04,097 iteration 6541 : loss : 0.019644, loss_ce: 0.012657
2022-01-09 14:11:05,550 iteration 6542 : loss : 0.012687, loss_ce: 0.004023
2022-01-09 14:11:06,895 iteration 6543 : loss : 0.012739, loss_ce: 0.004020
2022-01-09 14:11:08,304 iteration 6544 : loss : 0.019180, loss_ce: 0.007040
2022-01-09 14:11:08,305 Training Data Eval:
2022-01-09 14:11:15,176   Average segmentation loss on training set: 0.0092
2022-01-09 14:11:15,176 Validation Data Eval:
2022-01-09 14:11:17,575   Average segmentation loss on validation set: 0.0808
2022-01-09 14:11:19,062 iteration 6545 : loss : 0.016950, loss_ce: 0.006773
 96%|███████████████████████████▉ | 385/400 [2:43:56<06:42, 26.82s/it]2022-01-09 14:11:20,624 iteration 6546 : loss : 0.018388, loss_ce: 0.007465
2022-01-09 14:11:21,997 iteration 6547 : loss : 0.015884, loss_ce: 0.006480
2022-01-09 14:11:23,402 iteration 6548 : loss : 0.015426, loss_ce: 0.006123
2022-01-09 14:11:24,743 iteration 6549 : loss : 0.015916, loss_ce: 0.006010
2022-01-09 14:11:26,155 iteration 6550 : loss : 0.020119, loss_ce: 0.006944
2022-01-09 14:11:27,591 iteration 6551 : loss : 0.018531, loss_ce: 0.008057
2022-01-09 14:11:29,069 iteration 6552 : loss : 0.018327, loss_ce: 0.007314
2022-01-09 14:11:30,574 iteration 6553 : loss : 0.018124, loss_ce: 0.005393
2022-01-09 14:11:31,977 iteration 6554 : loss : 0.019461, loss_ce: 0.006630
2022-01-09 14:11:33,355 iteration 6555 : loss : 0.012531, loss_ce: 0.005107
2022-01-09 14:11:34,729 iteration 6556 : loss : 0.018703, loss_ce: 0.004388
2022-01-09 14:11:36,010 iteration 6557 : loss : 0.010465, loss_ce: 0.004487
2022-01-09 14:11:37,373 iteration 6558 : loss : 0.023258, loss_ce: 0.007651
2022-01-09 14:11:38,671 iteration 6559 : loss : 0.012607, loss_ce: 0.004564
2022-01-09 14:11:40,035 iteration 6560 : loss : 0.018353, loss_ce: 0.008170
2022-01-09 14:11:41,440 iteration 6561 : loss : 0.018247, loss_ce: 0.005798
2022-01-09 14:11:42,778 iteration 6562 : loss : 0.014851, loss_ce: 0.005629
 96%|███████████████████████████▉ | 386/400 [2:44:20<06:02, 25.89s/it]2022-01-09 14:11:44,329 iteration 6563 : loss : 0.022709, loss_ce: 0.010466
2022-01-09 14:11:45,685 iteration 6564 : loss : 0.018559, loss_ce: 0.007633
2022-01-09 14:11:47,004 iteration 6565 : loss : 0.022789, loss_ce: 0.005271
2022-01-09 14:11:48,319 iteration 6566 : loss : 0.015122, loss_ce: 0.005881
2022-01-09 14:11:49,615 iteration 6567 : loss : 0.014821, loss_ce: 0.006462
2022-01-09 14:11:50,960 iteration 6568 : loss : 0.013103, loss_ce: 0.005642
2022-01-09 14:11:52,330 iteration 6569 : loss : 0.013983, loss_ce: 0.005417
2022-01-09 14:11:53,651 iteration 6570 : loss : 0.015172, loss_ce: 0.007653
2022-01-09 14:11:55,002 iteration 6571 : loss : 0.015762, loss_ce: 0.004700
2022-01-09 14:11:56,299 iteration 6572 : loss : 0.013405, loss_ce: 0.005609
2022-01-09 14:11:57,725 iteration 6573 : loss : 0.022975, loss_ce: 0.007698
2022-01-09 14:11:59,162 iteration 6574 : loss : 0.012211, loss_ce: 0.004811
2022-01-09 14:12:00,613 iteration 6575 : loss : 0.059503, loss_ce: 0.011711
2022-01-09 14:12:01,952 iteration 6576 : loss : 0.013467, loss_ce: 0.005730
2022-01-09 14:12:03,342 iteration 6577 : loss : 0.027735, loss_ce: 0.006165
2022-01-09 14:12:04,667 iteration 6578 : loss : 0.012747, loss_ce: 0.004818
2022-01-09 14:12:06,042 iteration 6579 : loss : 0.015490, loss_ce: 0.005442
 97%|████████████████████████████ | 387/400 [2:44:43<05:26, 25.10s/it]2022-01-09 14:12:07,481 iteration 6580 : loss : 0.017467, loss_ce: 0.005023
2022-01-09 14:12:08,822 iteration 6581 : loss : 0.016443, loss_ce: 0.005735
2022-01-09 14:12:10,258 iteration 6582 : loss : 0.020841, loss_ce: 0.008149
2022-01-09 14:12:11,630 iteration 6583 : loss : 0.017206, loss_ce: 0.006521
2022-01-09 14:12:13,024 iteration 6584 : loss : 0.014444, loss_ce: 0.006088
2022-01-09 14:12:14,453 iteration 6585 : loss : 0.019135, loss_ce: 0.005483
2022-01-09 14:12:15,852 iteration 6586 : loss : 0.024874, loss_ce: 0.008341
2022-01-09 14:12:17,215 iteration 6587 : loss : 0.012927, loss_ce: 0.005841
2022-01-09 14:12:18,600 iteration 6588 : loss : 0.017540, loss_ce: 0.007122
2022-01-09 14:12:19,914 iteration 6589 : loss : 0.016071, loss_ce: 0.005776
2022-01-09 14:12:21,344 iteration 6590 : loss : 0.016950, loss_ce: 0.007958
2022-01-09 14:12:22,775 iteration 6591 : loss : 0.018219, loss_ce: 0.007473
2022-01-09 14:12:24,123 iteration 6592 : loss : 0.013230, loss_ce: 0.004943
2022-01-09 14:12:25,563 iteration 6593 : loss : 0.035830, loss_ce: 0.007353
2022-01-09 14:12:26,928 iteration 6594 : loss : 0.015342, loss_ce: 0.007624
2022-01-09 14:12:28,278 iteration 6595 : loss : 0.014526, loss_ce: 0.005440
2022-01-09 14:12:29,698 iteration 6596 : loss : 0.028422, loss_ce: 0.009625
 97%|████████████████████████████▏| 388/400 [2:45:07<04:56, 24.67s/it]2022-01-09 14:12:31,121 iteration 6597 : loss : 0.016983, loss_ce: 0.006895
2022-01-09 14:12:32,441 iteration 6598 : loss : 0.029917, loss_ce: 0.005680
2022-01-09 14:12:33,828 iteration 6599 : loss : 0.021954, loss_ce: 0.010171
2022-01-09 14:12:35,094 iteration 6600 : loss : 0.011977, loss_ce: 0.004842
2022-01-09 14:12:36,451 iteration 6601 : loss : 0.014966, loss_ce: 0.005404
2022-01-09 14:12:37,780 iteration 6602 : loss : 0.018222, loss_ce: 0.006714
2022-01-09 14:12:39,141 iteration 6603 : loss : 0.021448, loss_ce: 0.007889
2022-01-09 14:12:40,428 iteration 6604 : loss : 0.009856, loss_ce: 0.004056
2022-01-09 14:12:41,798 iteration 6605 : loss : 0.020931, loss_ce: 0.007134
2022-01-09 14:12:43,193 iteration 6606 : loss : 0.019008, loss_ce: 0.008758
2022-01-09 14:12:44,590 iteration 6607 : loss : 0.022149, loss_ce: 0.009622
2022-01-09 14:12:45,987 iteration 6608 : loss : 0.012077, loss_ce: 0.004770
2022-01-09 14:12:47,310 iteration 6609 : loss : 0.012692, loss_ce: 0.004741
2022-01-09 14:12:48,650 iteration 6610 : loss : 0.020978, loss_ce: 0.010292
2022-01-09 14:12:49,990 iteration 6611 : loss : 0.013779, loss_ce: 0.003212
2022-01-09 14:12:51,397 iteration 6612 : loss : 0.036277, loss_ce: 0.016499
2022-01-09 14:12:52,662 iteration 6613 : loss : 0.012086, loss_ce: 0.004608
 97%|████████████████████████████▏| 389/400 [2:45:30<04:25, 24.16s/it]2022-01-09 14:12:54,084 iteration 6614 : loss : 0.016263, loss_ce: 0.006493
2022-01-09 14:12:55,471 iteration 6615 : loss : 0.016245, loss_ce: 0.007640
2022-01-09 14:12:56,828 iteration 6616 : loss : 0.016738, loss_ce: 0.005226
2022-01-09 14:12:58,324 iteration 6617 : loss : 0.018380, loss_ce: 0.006205
2022-01-09 14:12:59,715 iteration 6618 : loss : 0.019036, loss_ce: 0.005862
2022-01-09 14:13:01,181 iteration 6619 : loss : 0.021733, loss_ce: 0.008071
2022-01-09 14:13:02,600 iteration 6620 : loss : 0.036187, loss_ce: 0.016528
2022-01-09 14:13:03,956 iteration 6621 : loss : 0.019266, loss_ce: 0.006038
2022-01-09 14:13:05,312 iteration 6622 : loss : 0.017476, loss_ce: 0.009063
2022-01-09 14:13:06,680 iteration 6623 : loss : 0.017040, loss_ce: 0.006098
2022-01-09 14:13:08,122 iteration 6624 : loss : 0.024624, loss_ce: 0.010356
2022-01-09 14:13:09,556 iteration 6625 : loss : 0.015894, loss_ce: 0.005726
2022-01-09 14:13:10,917 iteration 6626 : loss : 0.022865, loss_ce: 0.006894
2022-01-09 14:13:12,213 iteration 6627 : loss : 0.015338, loss_ce: 0.005077
2022-01-09 14:13:13,578 iteration 6628 : loss : 0.016142, loss_ce: 0.007818
2022-01-09 14:13:14,935 iteration 6629 : loss : 0.016614, loss_ce: 0.007815
2022-01-09 14:13:14,935 Training Data Eval:
2022-01-09 14:13:21,806   Average segmentation loss on training set: 0.0096
2022-01-09 14:13:21,806 Validation Data Eval:
2022-01-09 14:13:24,186   Average segmentation loss on validation set: 0.0756
2022-01-09 14:13:25,594 iteration 6630 : loss : 0.019372, loss_ce: 0.005624
 98%|████████████████████████████▎| 390/400 [2:46:03<04:27, 26.79s/it]2022-01-09 14:13:27,098 iteration 6631 : loss : 0.017093, loss_ce: 0.005764
2022-01-09 14:13:28,482 iteration 6632 : loss : 0.020913, loss_ce: 0.008953
2022-01-09 14:13:29,900 iteration 6633 : loss : 0.021846, loss_ce: 0.005639
2022-01-09 14:13:31,158 iteration 6634 : loss : 0.012835, loss_ce: 0.004243
2022-01-09 14:13:32,483 iteration 6635 : loss : 0.019459, loss_ce: 0.004867
2022-01-09 14:13:33,899 iteration 6636 : loss : 0.019848, loss_ce: 0.006946
2022-01-09 14:13:35,293 iteration 6637 : loss : 0.032590, loss_ce: 0.013634
2022-01-09 14:13:36,705 iteration 6638 : loss : 0.021758, loss_ce: 0.008489
2022-01-09 14:13:38,152 iteration 6639 : loss : 0.026086, loss_ce: 0.009353
2022-01-09 14:13:39,519 iteration 6640 : loss : 0.015131, loss_ce: 0.004750
2022-01-09 14:13:40,854 iteration 6641 : loss : 0.014860, loss_ce: 0.007815
2022-01-09 14:13:42,299 iteration 6642 : loss : 0.017604, loss_ce: 0.007632
2022-01-09 14:13:43,694 iteration 6643 : loss : 0.018885, loss_ce: 0.009728
2022-01-09 14:13:45,074 iteration 6644 : loss : 0.016303, loss_ce: 0.005514
2022-01-09 14:13:46,390 iteration 6645 : loss : 0.014084, loss_ce: 0.006007
2022-01-09 14:13:47,739 iteration 6646 : loss : 0.013329, loss_ce: 0.005885
2022-01-09 14:13:49,020 iteration 6647 : loss : 0.013582, loss_ce: 0.004340
 98%|████████████████████████████▎| 391/400 [2:46:26<03:52, 25.78s/it]2022-01-09 14:13:50,407 iteration 6648 : loss : 0.020848, loss_ce: 0.007110
2022-01-09 14:13:51,836 iteration 6649 : loss : 0.020562, loss_ce: 0.008586
2022-01-09 14:13:53,148 iteration 6650 : loss : 0.019492, loss_ce: 0.007225
2022-01-09 14:13:54,476 iteration 6651 : loss : 0.013222, loss_ce: 0.004982
2022-01-09 14:13:55,910 iteration 6652 : loss : 0.019109, loss_ce: 0.007822
2022-01-09 14:13:57,340 iteration 6653 : loss : 0.025440, loss_ce: 0.008947
2022-01-09 14:13:58,681 iteration 6654 : loss : 0.013551, loss_ce: 0.004894
2022-01-09 14:14:00,073 iteration 6655 : loss : 0.015379, loss_ce: 0.005858
2022-01-09 14:14:01,480 iteration 6656 : loss : 0.013355, loss_ce: 0.005424
2022-01-09 14:14:02,944 iteration 6657 : loss : 0.020025, loss_ce: 0.007590
2022-01-09 14:14:04,341 iteration 6658 : loss : 0.024555, loss_ce: 0.009892
2022-01-09 14:14:05,760 iteration 6659 : loss : 0.031412, loss_ce: 0.011263
2022-01-09 14:14:07,098 iteration 6660 : loss : 0.014594, loss_ce: 0.004972
2022-01-09 14:14:08,397 iteration 6661 : loss : 0.011292, loss_ce: 0.004228
2022-01-09 14:14:09,816 iteration 6662 : loss : 0.014532, loss_ce: 0.005755
2022-01-09 14:14:11,236 iteration 6663 : loss : 0.017167, loss_ce: 0.006678
2022-01-09 14:14:12,602 iteration 6664 : loss : 0.018105, loss_ce: 0.005776
 98%|████████████████████████████▍| 392/400 [2:46:50<03:20, 25.12s/it]2022-01-09 14:14:14,054 iteration 6665 : loss : 0.017708, loss_ce: 0.006120
2022-01-09 14:14:15,445 iteration 6666 : loss : 0.019556, loss_ce: 0.007125
2022-01-09 14:14:16,786 iteration 6667 : loss : 0.016477, loss_ce: 0.005745
2022-01-09 14:14:18,063 iteration 6668 : loss : 0.010533, loss_ce: 0.004352
2022-01-09 14:14:19,446 iteration 6669 : loss : 0.015456, loss_ce: 0.005094
2022-01-09 14:14:20,756 iteration 6670 : loss : 0.020337, loss_ce: 0.008620
2022-01-09 14:14:22,198 iteration 6671 : loss : 0.015309, loss_ce: 0.005798
2022-01-09 14:14:23,563 iteration 6672 : loss : 0.018705, loss_ce: 0.009860
2022-01-09 14:14:25,005 iteration 6673 : loss : 0.017503, loss_ce: 0.008155
2022-01-09 14:14:26,432 iteration 6674 : loss : 0.018016, loss_ce: 0.007336
2022-01-09 14:14:27,777 iteration 6675 : loss : 0.016967, loss_ce: 0.005052
2022-01-09 14:14:29,082 iteration 6676 : loss : 0.010346, loss_ce: 0.004166
2022-01-09 14:14:30,466 iteration 6677 : loss : 0.013362, loss_ce: 0.003195
2022-01-09 14:14:31,925 iteration 6678 : loss : 0.016217, loss_ce: 0.007353
2022-01-09 14:14:33,324 iteration 6679 : loss : 0.015563, loss_ce: 0.005271
2022-01-09 14:14:34,680 iteration 6680 : loss : 0.027005, loss_ce: 0.012315
2022-01-09 14:14:36,005 iteration 6681 : loss : 0.026662, loss_ce: 0.011281
 98%|████████████████████████████▍| 393/400 [2:47:13<02:52, 24.61s/it]2022-01-09 14:14:37,432 iteration 6682 : loss : 0.016517, loss_ce: 0.005470
2022-01-09 14:14:38,843 iteration 6683 : loss : 0.017729, loss_ce: 0.006038
2022-01-09 14:14:40,236 iteration 6684 : loss : 0.032415, loss_ce: 0.006031
2022-01-09 14:14:41,592 iteration 6685 : loss : 0.014694, loss_ce: 0.005612
2022-01-09 14:14:42,979 iteration 6686 : loss : 0.023289, loss_ce: 0.007790
2022-01-09 14:14:44,363 iteration 6687 : loss : 0.023853, loss_ce: 0.011308
2022-01-09 14:14:45,745 iteration 6688 : loss : 0.025876, loss_ce: 0.008766
2022-01-09 14:14:47,089 iteration 6689 : loss : 0.014758, loss_ce: 0.006000
2022-01-09 14:14:48,519 iteration 6690 : loss : 0.016390, loss_ce: 0.006651
2022-01-09 14:14:49,836 iteration 6691 : loss : 0.012682, loss_ce: 0.004535
2022-01-09 14:14:51,153 iteration 6692 : loss : 0.015090, loss_ce: 0.008153
2022-01-09 14:14:52,537 iteration 6693 : loss : 0.021770, loss_ce: 0.009482
2022-01-09 14:14:53,838 iteration 6694 : loss : 0.010855, loss_ce: 0.004555
2022-01-09 14:14:55,180 iteration 6695 : loss : 0.014680, loss_ce: 0.004174
2022-01-09 14:14:56,537 iteration 6696 : loss : 0.017593, loss_ce: 0.009741
2022-01-09 14:14:57,904 iteration 6697 : loss : 0.023108, loss_ce: 0.008573
2022-01-09 14:14:59,198 iteration 6698 : loss : 0.015949, loss_ce: 0.005117
 98%|████████████████████████████▌| 394/400 [2:47:36<02:25, 24.18s/it]2022-01-09 14:15:00,641 iteration 6699 : loss : 0.018918, loss_ce: 0.005556
2022-01-09 14:15:02,066 iteration 6700 : loss : 0.022480, loss_ce: 0.010027
2022-01-09 14:15:03,515 iteration 6701 : loss : 0.030880, loss_ce: 0.008410
2022-01-09 14:15:04,853 iteration 6702 : loss : 0.014520, loss_ce: 0.004888
2022-01-09 14:15:06,322 iteration 6703 : loss : 0.016428, loss_ce: 0.005279
2022-01-09 14:15:07,704 iteration 6704 : loss : 0.020476, loss_ce: 0.009323
2022-01-09 14:15:09,025 iteration 6705 : loss : 0.014494, loss_ce: 0.004665
2022-01-09 14:15:10,369 iteration 6706 : loss : 0.017093, loss_ce: 0.006010
2022-01-09 14:15:11,733 iteration 6707 : loss : 0.013990, loss_ce: 0.005032
2022-01-09 14:15:13,021 iteration 6708 : loss : 0.014359, loss_ce: 0.004194
2022-01-09 14:15:14,341 iteration 6709 : loss : 0.013567, loss_ce: 0.003857
2022-01-09 14:15:15,719 iteration 6710 : loss : 0.014559, loss_ce: 0.005635
2022-01-09 14:15:17,087 iteration 6711 : loss : 0.017190, loss_ce: 0.007026
2022-01-09 14:15:18,497 iteration 6712 : loss : 0.014174, loss_ce: 0.005618
2022-01-09 14:15:19,836 iteration 6713 : loss : 0.014204, loss_ce: 0.004471
2022-01-09 14:15:21,194 iteration 6714 : loss : 0.016237, loss_ce: 0.006489
2022-01-09 14:15:21,194 Training Data Eval:
2022-01-09 14:15:28,052   Average segmentation loss on training set: 0.0093
2022-01-09 14:15:28,052 Validation Data Eval:
2022-01-09 14:15:30,427   Average segmentation loss on validation set: 0.0726
2022-01-09 14:15:31,755 iteration 6715 : loss : 0.010669, loss_ce: 0.003091
 99%|████████████████████████████▋| 395/400 [2:48:09<02:13, 26.69s/it]2022-01-09 14:15:33,140 iteration 6716 : loss : 0.013767, loss_ce: 0.004404
2022-01-09 14:15:34,534 iteration 6717 : loss : 0.019119, loss_ce: 0.005678
2022-01-09 14:15:35,967 iteration 6718 : loss : 0.027533, loss_ce: 0.014560
2022-01-09 14:15:37,380 iteration 6719 : loss : 0.026568, loss_ce: 0.009348
2022-01-09 14:15:38,774 iteration 6720 : loss : 0.033980, loss_ce: 0.006058
2022-01-09 14:15:40,182 iteration 6721 : loss : 0.017959, loss_ce: 0.008088
2022-01-09 14:15:41,535 iteration 6722 : loss : 0.013279, loss_ce: 0.005740
2022-01-09 14:15:42,913 iteration 6723 : loss : 0.017855, loss_ce: 0.006593
2022-01-09 14:15:44,234 iteration 6724 : loss : 0.015676, loss_ce: 0.006461
2022-01-09 14:15:45,597 iteration 6725 : loss : 0.011660, loss_ce: 0.004068
2022-01-09 14:15:46,891 iteration 6726 : loss : 0.009852, loss_ce: 0.003289
2022-01-09 14:15:48,355 iteration 6727 : loss : 0.023351, loss_ce: 0.008057
2022-01-09 14:15:49,686 iteration 6728 : loss : 0.019675, loss_ce: 0.007858
2022-01-09 14:15:51,141 iteration 6729 : loss : 0.017263, loss_ce: 0.005387
2022-01-09 14:15:52,607 iteration 6730 : loss : 0.019714, loss_ce: 0.008348
2022-01-09 14:15:54,022 iteration 6731 : loss : 0.033743, loss_ce: 0.013377
2022-01-09 14:15:55,390 iteration 6732 : loss : 0.012352, loss_ce: 0.005419
 99%|████████████████████████████▋| 396/400 [2:48:33<01:43, 25.78s/it]2022-01-09 14:15:56,769 iteration 6733 : loss : 0.010904, loss_ce: 0.004679
2022-01-09 14:15:58,128 iteration 6734 : loss : 0.026585, loss_ce: 0.008406
2022-01-09 14:15:59,524 iteration 6735 : loss : 0.016637, loss_ce: 0.005818
2022-01-09 14:16:00,876 iteration 6736 : loss : 0.015849, loss_ce: 0.007996
2022-01-09 14:16:02,192 iteration 6737 : loss : 0.019394, loss_ce: 0.009758
2022-01-09 14:16:03,494 iteration 6738 : loss : 0.012645, loss_ce: 0.003986
2022-01-09 14:16:04,865 iteration 6739 : loss : 0.016914, loss_ce: 0.008488
2022-01-09 14:16:06,264 iteration 6740 : loss : 0.022918, loss_ce: 0.008661
2022-01-09 14:16:07,645 iteration 6741 : loss : 0.017941, loss_ce: 0.007560
2022-01-09 14:16:08,994 iteration 6742 : loss : 0.015366, loss_ce: 0.007285
2022-01-09 14:16:10,310 iteration 6743 : loss : 0.012998, loss_ce: 0.003044
2022-01-09 14:16:11,716 iteration 6744 : loss : 0.023575, loss_ce: 0.010670
2022-01-09 14:16:13,080 iteration 6745 : loss : 0.014745, loss_ce: 0.007038
2022-01-09 14:16:14,389 iteration 6746 : loss : 0.010396, loss_ce: 0.002992
2022-01-09 14:16:15,716 iteration 6747 : loss : 0.014793, loss_ce: 0.004421
2022-01-09 14:16:17,101 iteration 6748 : loss : 0.012065, loss_ce: 0.005148
2022-01-09 14:16:18,453 iteration 6749 : loss : 0.013917, loss_ce: 0.003844
 99%|████████████████████████████▊| 397/400 [2:48:56<01:14, 24.96s/it]2022-01-09 14:16:19,873 iteration 6750 : loss : 0.012588, loss_ce: 0.006053
2022-01-09 14:16:21,303 iteration 6751 : loss : 0.014019, loss_ce: 0.005426
2022-01-09 14:16:22,608 iteration 6752 : loss : 0.013027, loss_ce: 0.004460
2022-01-09 14:16:23,977 iteration 6753 : loss : 0.018435, loss_ce: 0.007768
2022-01-09 14:16:25,373 iteration 6754 : loss : 0.019128, loss_ce: 0.006791
2022-01-09 14:16:26,710 iteration 6755 : loss : 0.014752, loss_ce: 0.004516
2022-01-09 14:16:28,044 iteration 6756 : loss : 0.019678, loss_ce: 0.007495
2022-01-09 14:16:29,479 iteration 6757 : loss : 0.017155, loss_ce: 0.005907
2022-01-09 14:16:30,814 iteration 6758 : loss : 0.018340, loss_ce: 0.009003
2022-01-09 14:16:32,193 iteration 6759 : loss : 0.013830, loss_ce: 0.006223
2022-01-09 14:16:33,633 iteration 6760 : loss : 0.028381, loss_ce: 0.004670
2022-01-09 14:16:35,000 iteration 6761 : loss : 0.017864, loss_ce: 0.007464
2022-01-09 14:16:36,348 iteration 6762 : loss : 0.019538, loss_ce: 0.006173
2022-01-09 14:16:37,768 iteration 6763 : loss : 0.019049, loss_ce: 0.007903
2022-01-09 14:16:39,102 iteration 6764 : loss : 0.021376, loss_ce: 0.008796
2022-01-09 14:16:40,520 iteration 6765 : loss : 0.018796, loss_ce: 0.006943
2022-01-09 14:16:41,858 iteration 6766 : loss : 0.013891, loss_ce: 0.006458
100%|████████████████████████████▊| 398/400 [2:49:19<00:48, 24.49s/it]2022-01-09 14:16:43,280 iteration 6767 : loss : 0.014024, loss_ce: 0.005173
2022-01-09 14:16:44,729 iteration 6768 : loss : 0.014955, loss_ce: 0.005404
2022-01-09 14:16:46,117 iteration 6769 : loss : 0.016465, loss_ce: 0.006865
2022-01-09 14:16:47,466 iteration 6770 : loss : 0.017783, loss_ce: 0.009440
2022-01-09 14:16:48,933 iteration 6771 : loss : 0.026702, loss_ce: 0.005946
2022-01-09 14:16:50,277 iteration 6772 : loss : 0.014569, loss_ce: 0.005859
2022-01-09 14:16:51,526 iteration 6773 : loss : 0.012957, loss_ce: 0.003874
2022-01-09 14:16:52,917 iteration 6774 : loss : 0.023652, loss_ce: 0.007012
2022-01-09 14:16:54,313 iteration 6775 : loss : 0.017989, loss_ce: 0.006354
2022-01-09 14:16:55,667 iteration 6776 : loss : 0.016593, loss_ce: 0.005486
2022-01-09 14:16:57,109 iteration 6777 : loss : 0.018710, loss_ce: 0.009901
2022-01-09 14:16:58,469 iteration 6778 : loss : 0.016323, loss_ce: 0.005201
2022-01-09 14:16:59,916 iteration 6779 : loss : 0.024748, loss_ce: 0.011571
2022-01-09 14:17:01,304 iteration 6780 : loss : 0.014663, loss_ce: 0.006707
2022-01-09 14:17:02,725 iteration 6781 : loss : 0.026447, loss_ce: 0.013357
2022-01-09 14:17:04,141 iteration 6782 : loss : 0.017204, loss_ce: 0.006614
2022-01-09 14:17:05,494 iteration 6783 : loss : 0.013277, loss_ce: 0.004657
100%|████████████████████████████▉| 399/400 [2:49:43<00:24, 24.24s/it]2022-01-09 14:17:06,995 iteration 6784 : loss : 0.016226, loss_ce: 0.005476
2022-01-09 14:17:08,393 iteration 6785 : loss : 0.016728, loss_ce: 0.005825
2022-01-09 14:17:09,719 iteration 6786 : loss : 0.011130, loss_ce: 0.004248
2022-01-09 14:17:11,055 iteration 6787 : loss : 0.013470, loss_ce: 0.004809
2022-01-09 14:17:12,436 iteration 6788 : loss : 0.017931, loss_ce: 0.006740
2022-01-09 14:17:13,812 iteration 6789 : loss : 0.021817, loss_ce: 0.006314
2022-01-09 14:17:15,157 iteration 6790 : loss : 0.012850, loss_ce: 0.005206
2022-01-09 14:17:16,549 iteration 6791 : loss : 0.017519, loss_ce: 0.007011
2022-01-09 14:17:17,868 iteration 6792 : loss : 0.013442, loss_ce: 0.005989
2022-01-09 14:17:19,304 iteration 6793 : loss : 0.028550, loss_ce: 0.009389
2022-01-09 14:17:20,683 iteration 6794 : loss : 0.025793, loss_ce: 0.008993
2022-01-09 14:17:22,088 iteration 6795 : loss : 0.015895, loss_ce: 0.005321
2022-01-09 14:17:23,512 iteration 6796 : loss : 0.016492, loss_ce: 0.008339
2022-01-09 14:17:24,807 iteration 6797 : loss : 0.013750, loss_ce: 0.004727
2022-01-09 14:17:26,211 iteration 6798 : loss : 0.015549, loss_ce: 0.005086
2022-01-09 14:17:27,673 iteration 6799 : loss : 0.015531, loss_ce: 0.007467
2022-01-09 14:17:27,673 Training Data Eval:
2022-01-09 14:17:34,541   Average segmentation loss on training set: 0.0090
2022-01-09 14:17:34,541 Validation Data Eval:
2022-01-09 14:17:36,913   Average segmentation loss on validation set: 0.0689
2022-01-09 14:17:38,232 iteration 6800 : loss : 0.013535, loss_ce: 0.003626
100%|█████████████████████████████| 400/400 [2:50:15<00:00, 26.79s/it]100%|█████████████████████████████| 400/400 [2:50:15<00:00, 25.54s/it]
