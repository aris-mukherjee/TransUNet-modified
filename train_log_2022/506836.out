2022-01-14 11:33:59,233 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-14 11:33:59,234 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-14 11:33:59,234 ============================================================
2022-01-14 11:33:59,234 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-14 11:33:59,234 ============================================================
2022-01-14 11:33:59,234 Loading data...
2022-01-14 11:33:59,234 Reading NCI - RUNMC images...
2022-01-14 11:33:59,234 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-14 11:33:59,236 Already preprocessed this configuration. Loading now!
2022-01-14 11:33:59,249 Training Images: (256, 256, 286)
2022-01-14 11:33:59,249 Training Labels: (256, 256, 286)
2022-01-14 11:33:59,249 Validation Images: (256, 256, 98)
2022-01-14 11:33:59,249 Validation Labels: (256, 256, 98)
2022-01-14 11:33:59,249 ============================================================
2022-01-14 11:33:59,283 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-14 11:34:02,035 iteration 1 : loss : 0.929533, loss_ce: 1.135964
2022-01-14 11:34:02,909 iteration 2 : loss : 0.888882, loss_ce: 1.051740
2022-01-14 11:34:03,750 iteration 3 : loss : 0.838125, loss_ce: 0.946706
2022-01-14 11:34:04,640 iteration 4 : loss : 0.766286, loss_ce: 0.862917
2022-01-14 11:34:05,466 iteration 5 : loss : 0.735533, loss_ce: 0.785442
2022-01-14 11:34:06,451 iteration 6 : loss : 0.683023, loss_ce: 0.717394
2022-01-14 11:34:07,348 iteration 7 : loss : 0.655338, loss_ce: 0.659573
2022-01-14 11:34:08,251 iteration 8 : loss : 0.607150, loss_ce: 0.592284
2022-01-14 11:34:09,040 iteration 9 : loss : 0.545751, loss_ce: 0.535550
2022-01-14 11:34:09,953 iteration 10 : loss : 0.525291, loss_ce: 0.492300
2022-01-14 11:34:10,864 iteration 11 : loss : 0.498305, loss_ce: 0.447760
2022-01-14 11:34:11,811 iteration 12 : loss : 0.478953, loss_ce: 0.416879
2022-01-14 11:34:12,669 iteration 13 : loss : 0.442388, loss_ce: 0.366193
2022-01-14 11:34:13,606 iteration 14 : loss : 0.424683, loss_ce: 0.336172
2022-01-14 11:34:14,485 iteration 15 : loss : 0.413900, loss_ce: 0.310943
2022-01-14 11:34:15,343 iteration 16 : loss : 0.410632, loss_ce: 0.303216
2022-01-14 11:34:16,181 iteration 17 : loss : 0.416430, loss_ce: 0.272958
  0%|                               | 1/400 [00:16<1:52:50, 16.97s/it]2022-01-14 11:34:17,155 iteration 18 : loss : 0.367929, loss_ce: 0.256301
2022-01-14 11:34:18,054 iteration 19 : loss : 0.377349, loss_ce: 0.227978
2022-01-14 11:34:19,043 iteration 20 : loss : 0.339548, loss_ce: 0.221450
2022-01-14 11:34:19,902 iteration 21 : loss : 0.377862, loss_ce: 0.220324
2022-01-14 11:34:20,717 iteration 22 : loss : 0.335956, loss_ce: 0.189631
2022-01-14 11:34:21,677 iteration 23 : loss : 0.337740, loss_ce: 0.182961
2022-01-14 11:34:22,598 iteration 24 : loss : 0.292659, loss_ce: 0.168904
2022-01-14 11:34:23,544 iteration 25 : loss : 0.338515, loss_ce: 0.210993
2022-01-14 11:34:24,420 iteration 26 : loss : 0.297716, loss_ce: 0.171774
2022-01-14 11:34:25,251 iteration 27 : loss : 0.323738, loss_ce: 0.173349
2022-01-14 11:34:26,072 iteration 28 : loss : 0.282380, loss_ce: 0.150571
2022-01-14 11:34:26,974 iteration 29 : loss : 0.327703, loss_ce: 0.176758
2022-01-14 11:34:27,817 iteration 30 : loss : 0.291747, loss_ce: 0.150652
2022-01-14 11:34:28,642 iteration 31 : loss : 0.286088, loss_ce: 0.136561
2022-01-14 11:34:29,505 iteration 32 : loss : 0.286029, loss_ce: 0.167097
2022-01-14 11:34:30,419 iteration 33 : loss : 0.285150, loss_ce: 0.145157
2022-01-14 11:34:31,404 iteration 34 : loss : 0.238108, loss_ce: 0.114414
  0%|▏                              | 2/400 [00:32<1:45:41, 15.93s/it]2022-01-14 11:34:32,383 iteration 35 : loss : 0.247616, loss_ce: 0.122894
2022-01-14 11:34:33,279 iteration 36 : loss : 0.282726, loss_ce: 0.106606
2022-01-14 11:34:34,195 iteration 37 : loss : 0.252187, loss_ce: 0.102335
2022-01-14 11:34:36,688 iteration 38 : loss : 0.264667, loss_ce: 0.109083
2022-01-14 11:34:37,490 iteration 39 : loss : 0.288166, loss_ce: 0.130594
2022-01-14 11:34:38,350 iteration 40 : loss : 0.316081, loss_ce: 0.149260
2022-01-14 11:34:39,133 iteration 41 : loss : 0.238561, loss_ce: 0.109107
2022-01-14 11:34:40,033 iteration 42 : loss : 0.289553, loss_ce: 0.140182
2022-01-14 11:34:40,922 iteration 43 : loss : 0.216547, loss_ce: 0.096710
2022-01-14 11:34:41,727 iteration 44 : loss : 0.277206, loss_ce: 0.144820
2022-01-14 11:34:42,672 iteration 45 : loss : 0.261394, loss_ce: 0.112004
2022-01-14 11:34:43,565 iteration 46 : loss : 0.259521, loss_ce: 0.123435
2022-01-14 11:34:44,455 iteration 47 : loss : 0.254640, loss_ce: 0.107494
2022-01-14 11:34:45,341 iteration 48 : loss : 0.248094, loss_ce: 0.117602
2022-01-14 11:34:46,271 iteration 49 : loss : 0.296155, loss_ce: 0.131734
2022-01-14 11:34:47,209 iteration 50 : loss : 0.254892, loss_ce: 0.107728
2022-01-14 11:34:48,159 iteration 51 : loss : 0.229131, loss_ce: 0.096459
  1%|▏                              | 3/400 [00:48<1:47:54, 16.31s/it]2022-01-14 11:34:49,047 iteration 52 : loss : 0.263769, loss_ce: 0.105880
2022-01-14 11:34:49,854 iteration 53 : loss : 0.243259, loss_ce: 0.110509
2022-01-14 11:34:50,788 iteration 54 : loss : 0.254460, loss_ce: 0.105131
2022-01-14 11:34:51,699 iteration 55 : loss : 0.286429, loss_ce: 0.118555
2022-01-14 11:34:52,618 iteration 56 : loss : 0.271184, loss_ce: 0.134372
2022-01-14 11:34:53,472 iteration 57 : loss : 0.256790, loss_ce: 0.124342
2022-01-14 11:34:54,300 iteration 58 : loss : 0.279663, loss_ce: 0.145072
2022-01-14 11:34:55,129 iteration 59 : loss : 0.256343, loss_ce: 0.121518
2022-01-14 11:34:55,983 iteration 60 : loss : 0.239203, loss_ce: 0.118702
2022-01-14 11:34:56,846 iteration 61 : loss : 0.242936, loss_ce: 0.108933
2022-01-14 11:34:57,784 iteration 62 : loss : 0.272204, loss_ce: 0.102845
2022-01-14 11:34:58,629 iteration 63 : loss : 0.274135, loss_ce: 0.126251
2022-01-14 11:34:59,533 iteration 64 : loss : 0.327662, loss_ce: 0.135067
2022-01-14 11:35:00,422 iteration 65 : loss : 0.243088, loss_ce: 0.091656
2022-01-14 11:35:01,360 iteration 66 : loss : 0.271508, loss_ce: 0.112945
2022-01-14 11:35:02,230 iteration 67 : loss : 0.240718, loss_ce: 0.105401
2022-01-14 11:35:03,041 iteration 68 : loss : 0.247568, loss_ce: 0.110943
  1%|▎                              | 4/400 [01:03<1:43:54, 15.74s/it]2022-01-14 11:35:04,011 iteration 69 : loss : 0.299438, loss_ce: 0.139991
2022-01-14 11:35:04,876 iteration 70 : loss : 0.286044, loss_ce: 0.120791
2022-01-14 11:35:05,754 iteration 71 : loss : 0.291088, loss_ce: 0.155899
2022-01-14 11:35:06,646 iteration 72 : loss : 0.263354, loss_ce: 0.119594
2022-01-14 11:35:07,559 iteration 73 : loss : 0.243950, loss_ce: 0.101041
2022-01-14 11:35:08,458 iteration 74 : loss : 0.234727, loss_ce: 0.099200
2022-01-14 11:35:09,319 iteration 75 : loss : 0.244441, loss_ce: 0.122381
2022-01-14 11:35:10,128 iteration 76 : loss : 0.253913, loss_ce: 0.119042
2022-01-14 11:35:11,024 iteration 77 : loss : 0.300256, loss_ce: 0.136920
2022-01-14 11:35:11,909 iteration 78 : loss : 0.239955, loss_ce: 0.097821
2022-01-14 11:35:12,888 iteration 79 : loss : 0.287125, loss_ce: 0.118688
2022-01-14 11:35:13,684 iteration 80 : loss : 0.253269, loss_ce: 0.098418
2022-01-14 11:35:14,627 iteration 81 : loss : 0.297858, loss_ce: 0.136004
2022-01-14 11:35:15,491 iteration 82 : loss : 0.309085, loss_ce: 0.109365
2022-01-14 11:35:16,322 iteration 83 : loss : 0.320708, loss_ce: 0.099202
2022-01-14 11:35:17,285 iteration 84 : loss : 0.321214, loss_ce: 0.146267
2022-01-14 11:35:17,285 Training Data Eval:
2022-01-14 11:35:21,542   Average segmentation loss on training set: 0.3319
2022-01-14 11:35:21,542 Validation Data Eval:
2022-01-14 11:35:23,211   Average segmentation loss on validation set: 0.3027
2022-01-14 11:35:24,787 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_1BLOCK_best_val_loss_seed100.pth
2022-01-14 11:35:26,845 iteration 85 : loss : 0.292444, loss_ce: 0.135014
  1%|▍                              | 5/400 [01:27<2:02:48, 18.65s/it]2022-01-14 11:35:27,833 iteration 86 : loss : 0.257958, loss_ce: 0.115945
2022-01-14 11:35:28,701 iteration 87 : loss : 0.252042, loss_ce: 0.102734
2022-01-14 11:35:29,536 iteration 88 : loss : 0.228252, loss_ce: 0.112664
2022-01-14 11:35:30,427 iteration 89 : loss : 0.278997, loss_ce: 0.110959
2022-01-14 11:35:31,295 iteration 90 : loss : 0.227587, loss_ce: 0.104593
2022-01-14 11:35:32,221 iteration 91 : loss : 0.240686, loss_ce: 0.116757
2022-01-14 11:35:33,163 iteration 92 : loss : 0.225974, loss_ce: 0.086367
2022-01-14 11:35:33,988 iteration 93 : loss : 0.241238, loss_ce: 0.088867
2022-01-14 11:35:34,905 iteration 94 : loss : 0.219404, loss_ce: 0.091381
2022-01-14 11:35:35,779 iteration 95 : loss : 0.273987, loss_ce: 0.130636
2022-01-14 11:35:36,725 iteration 96 : loss : 0.206148, loss_ce: 0.084709
2022-01-14 11:35:37,612 iteration 97 : loss : 0.256380, loss_ce: 0.094266
2022-01-14 11:35:38,530 iteration 98 : loss : 0.294011, loss_ce: 0.111917
2022-01-14 11:35:39,408 iteration 99 : loss : 0.308690, loss_ce: 0.127111
2022-01-14 11:35:40,332 iteration 100 : loss : 0.263735, loss_ce: 0.112478
2022-01-14 11:35:41,201 iteration 101 : loss : 0.234777, loss_ce: 0.095383
2022-01-14 11:35:42,064 iteration 102 : loss : 0.240824, loss_ce: 0.112412
  2%|▍                              | 6/400 [01:42<1:54:48, 17.48s/it]2022-01-14 11:35:43,036 iteration 103 : loss : 0.286816, loss_ce: 0.107823
2022-01-14 11:35:43,973 iteration 104 : loss : 0.267547, loss_ce: 0.111791
2022-01-14 11:35:44,910 iteration 105 : loss : 0.182521, loss_ce: 0.073836
2022-01-14 11:35:45,918 iteration 106 : loss : 0.242322, loss_ce: 0.096485
2022-01-14 11:35:46,851 iteration 107 : loss : 0.264234, loss_ce: 0.097521
2022-01-14 11:35:47,739 iteration 108 : loss : 0.265134, loss_ce: 0.094132
2022-01-14 11:35:48,564 iteration 109 : loss : 0.284927, loss_ce: 0.127948
2022-01-14 11:35:49,418 iteration 110 : loss : 0.222004, loss_ce: 0.097040
2022-01-14 11:35:50,363 iteration 111 : loss : 0.227412, loss_ce: 0.103118
2022-01-14 11:35:51,241 iteration 112 : loss : 0.233378, loss_ce: 0.087262
2022-01-14 11:35:52,072 iteration 113 : loss : 0.202829, loss_ce: 0.075622
2022-01-14 11:35:52,993 iteration 114 : loss : 0.223204, loss_ce: 0.079793
2022-01-14 11:35:53,864 iteration 115 : loss : 0.225554, loss_ce: 0.101402
2022-01-14 11:35:54,778 iteration 116 : loss : 0.263472, loss_ce: 0.126653
2022-01-14 11:35:55,637 iteration 117 : loss : 0.205342, loss_ce: 0.085140
2022-01-14 11:35:56,506 iteration 118 : loss : 0.283195, loss_ce: 0.145128
2022-01-14 11:35:57,406 iteration 119 : loss : 0.209560, loss_ce: 0.083180
  2%|▌                              | 7/400 [01:58<1:49:56, 16.78s/it]