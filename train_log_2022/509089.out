2022-01-20 19:32:50,154 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-20 19:32:50,155 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-20 19:32:50,155 ============================================================
2022-01-20 19:32:50,155 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-20 19:32:50,155 ============================================================
2022-01-20 19:32:50,155 Loading data...
2022-01-20 19:32:50,155 Reading NCI - RUNMC images...
2022-01-20 19:32:50,155 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-20 19:32:50,157 Already preprocessed this configuration. Loading now!
2022-01-20 19:32:50,172 Training Images: (256, 256, 286)
2022-01-20 19:32:50,172 Training Labels: (256, 256, 286)
2022-01-20 19:32:50,172 Validation Images: (256, 256, 98)
2022-01-20 19:32:50,172 Validation Labels: (256, 256, 98)
2022-01-20 19:32:50,172 ============================================================
2022-01-20 19:32:50,204 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-20 19:32:53,767 iteration 1 : loss : 0.984868, loss_ce: 1.214216
2022-01-20 19:32:54,985 iteration 2 : loss : 0.921685, loss_ce: 1.111337
2022-01-20 19:32:56,319 iteration 3 : loss : 0.870823, loss_ce: 1.019168
2022-01-20 19:32:57,544 iteration 4 : loss : 0.811306, loss_ce: 0.923942
2022-01-20 19:32:58,769 iteration 5 : loss : 0.760632, loss_ce: 0.853519
2022-01-20 19:33:01,614 iteration 6 : loss : 0.723155, loss_ce: 0.783774
2022-01-20 19:33:02,905 iteration 7 : loss : 0.677394, loss_ce: 0.724488
2022-01-20 19:33:04,147 iteration 8 : loss : 0.664742, loss_ce: 0.668613
2022-01-20 19:33:05,406 iteration 9 : loss : 0.600167, loss_ce: 0.629928
2022-01-20 19:33:06,700 iteration 10 : loss : 0.605058, loss_ce: 0.570568
2022-01-20 19:33:08,037 iteration 11 : loss : 0.562284, loss_ce: 0.536474
2022-01-20 19:33:09,260 iteration 12 : loss : 0.537863, loss_ce: 0.488800
2022-01-20 19:33:10,465 iteration 13 : loss : 0.525581, loss_ce: 0.454986
2022-01-20 19:33:11,643 iteration 14 : loss : 0.478635, loss_ce: 0.410043
2022-01-20 19:33:12,900 iteration 15 : loss : 0.452567, loss_ce: 0.373388
2022-01-20 19:33:14,129 iteration 16 : loss : 0.468433, loss_ce: 0.363887
2022-01-20 19:33:15,352 iteration 17 : loss : 0.403509, loss_ce: 0.321442
  0%|                               | 1/400 [00:25<2:47:39, 25.21s/it]2022-01-20 19:33:16,667 iteration 18 : loss : 0.435731, loss_ce: 0.294504
2022-01-20 19:33:17,825 iteration 19 : loss : 0.365090, loss_ce: 0.258454
2022-01-20 19:33:19,118 iteration 20 : loss : 0.348222, loss_ce: 0.236719
2022-01-20 19:33:20,322 iteration 21 : loss : 0.383790, loss_ce: 0.226079
2022-01-20 19:33:21,552 iteration 22 : loss : 0.341835, loss_ce: 0.222463
2022-01-20 19:33:22,855 iteration 23 : loss : 0.325807, loss_ce: 0.190132
2022-01-20 19:33:24,078 iteration 24 : loss : 0.320583, loss_ce: 0.188447
2022-01-20 19:33:25,357 iteration 25 : loss : 0.389903, loss_ce: 0.240049
2022-01-20 19:33:26,561 iteration 26 : loss : 0.302563, loss_ce: 0.170400
2022-01-20 19:33:27,715 iteration 27 : loss : 0.293761, loss_ce: 0.168094
2022-01-20 19:33:28,879 iteration 28 : loss : 0.287285, loss_ce: 0.153826
2022-01-20 19:33:30,145 iteration 29 : loss : 0.289317, loss_ce: 0.149704
2022-01-20 19:33:31,404 iteration 30 : loss : 0.306334, loss_ce: 0.157013
2022-01-20 19:33:32,570 iteration 31 : loss : 0.272514, loss_ce: 0.139299
2022-01-20 19:33:33,851 iteration 32 : loss : 0.290959, loss_ce: 0.164096
2022-01-20 19:33:35,118 iteration 33 : loss : 0.277413, loss_ce: 0.159437
2022-01-20 19:33:36,383 iteration 34 : loss : 0.281665, loss_ce: 0.163948
  0%|▏                              | 2/400 [00:46<2:30:51, 22.74s/it]2022-01-20 19:33:37,682 iteration 35 : loss : 0.259067, loss_ce: 0.122118
2022-01-20 19:33:38,961 iteration 36 : loss : 0.259831, loss_ce: 0.134134
2022-01-20 19:33:40,246 iteration 37 : loss : 0.263521, loss_ce: 0.114912
2022-01-20 19:33:41,448 iteration 38 : loss : 0.248053, loss_ce: 0.119672
2022-01-20 19:33:42,663 iteration 39 : loss : 0.225060, loss_ce: 0.105295
2022-01-20 19:33:43,963 iteration 40 : loss : 0.259180, loss_ce: 0.127459
2022-01-20 19:33:45,252 iteration 41 : loss : 0.347676, loss_ce: 0.185095
2022-01-20 19:33:46,521 iteration 42 : loss : 0.248512, loss_ce: 0.124705
2022-01-20 19:33:47,690 iteration 43 : loss : 0.265135, loss_ce: 0.126268
2022-01-20 19:33:48,995 iteration 44 : loss : 0.231579, loss_ce: 0.110743
2022-01-20 19:33:50,231 iteration 45 : loss : 0.248007, loss_ce: 0.116101
2022-01-20 19:33:51,479 iteration 46 : loss : 0.258941, loss_ce: 0.105860
2022-01-20 19:33:52,762 iteration 47 : loss : 0.224897, loss_ce: 0.091962
2022-01-20 19:33:54,023 iteration 48 : loss : 0.240085, loss_ce: 0.102102
2022-01-20 19:33:55,329 iteration 49 : loss : 0.328828, loss_ce: 0.166162
2022-01-20 19:33:56,531 iteration 50 : loss : 0.332024, loss_ce: 0.156075
2022-01-20 19:33:57,716 iteration 51 : loss : 0.265963, loss_ce: 0.134013
  1%|▏                              | 3/400 [01:07<2:26:12, 22.10s/it]2022-01-20 19:33:59,052 iteration 52 : loss : 0.291752, loss_ce: 0.153353
2022-01-20 19:34:00,334 iteration 53 : loss : 0.265928, loss_ce: 0.129959
2022-01-20 19:34:01,585 iteration 54 : loss : 0.239565, loss_ce: 0.103976
2022-01-20 19:34:02,855 iteration 55 : loss : 0.272740, loss_ce: 0.135001
2022-01-20 19:34:04,104 iteration 56 : loss : 0.264638, loss_ce: 0.114199
2022-01-20 19:34:05,377 iteration 57 : loss : 0.212108, loss_ce: 0.086228
2022-01-20 19:34:06,645 iteration 58 : loss : 0.279385, loss_ce: 0.118520
2022-01-20 19:34:07,876 iteration 59 : loss : 0.211003, loss_ce: 0.097541
2022-01-20 19:34:09,140 iteration 60 : loss : 0.252699, loss_ce: 0.106403
2022-01-20 19:34:10,390 iteration 61 : loss : 0.271681, loss_ce: 0.135562
2022-01-20 19:34:11,631 iteration 62 : loss : 0.343624, loss_ce: 0.134193
2022-01-20 19:34:12,799 iteration 63 : loss : 0.296288, loss_ce: 0.155893
2022-01-20 19:34:14,024 iteration 64 : loss : 0.338584, loss_ce: 0.161565
2022-01-20 19:34:15,218 iteration 65 : loss : 0.241408, loss_ce: 0.085784
2022-01-20 19:34:16,497 iteration 66 : loss : 0.281591, loss_ce: 0.119474
2022-01-20 19:34:17,884 iteration 67 : loss : 0.235913, loss_ce: 0.070221
2022-01-20 19:34:19,221 iteration 68 : loss : 0.236900, loss_ce: 0.098234
  1%|▎                              | 4/400 [01:29<2:24:17, 21.86s/it]2022-01-20 19:34:20,613 iteration 69 : loss : 0.227017, loss_ce: 0.088001
2022-01-20 19:34:22,006 iteration 70 : loss : 0.207884, loss_ce: 0.084412
2022-01-20 19:34:23,289 iteration 71 : loss : 0.229304, loss_ce: 0.095739
2022-01-20 19:34:24,621 iteration 72 : loss : 0.241668, loss_ce: 0.104525
2022-01-20 19:34:25,900 iteration 73 : loss : 0.235986, loss_ce: 0.110187
2022-01-20 19:34:27,145 iteration 74 : loss : 0.236554, loss_ce: 0.100913
2022-01-20 19:34:28,417 iteration 75 : loss : 0.225834, loss_ce: 0.098735
2022-01-20 19:34:29,683 iteration 76 : loss : 0.202232, loss_ce: 0.082641
2022-01-20 19:34:30,874 iteration 77 : loss : 0.227721, loss_ce: 0.101675
2022-01-20 19:34:32,169 iteration 78 : loss : 0.226810, loss_ce: 0.096290
2022-01-20 19:34:33,424 iteration 79 : loss : 0.249962, loss_ce: 0.089631
2022-01-20 19:34:34,679 iteration 80 : loss : 0.221159, loss_ce: 0.098689
2022-01-20 19:34:35,919 iteration 81 : loss : 0.212080, loss_ce: 0.088315
2022-01-20 19:34:37,186 iteration 82 : loss : 0.201052, loss_ce: 0.080044
2022-01-20 19:34:38,446 iteration 83 : loss : 0.218051, loss_ce: 0.078801
2022-01-20 19:34:39,793 iteration 84 : loss : 0.212707, loss_ce: 0.099351
2022-01-20 19:34:39,794 Training Data Eval:
2022-01-20 19:34:46,275   Average segmentation loss on training set: 0.4065
2022-01-20 19:34:46,275 Validation Data Eval:
2022-01-20 19:34:48,720   Average segmentation loss on validation set: 0.4648
2022-01-20 19:34:54,651 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 19:34:55,854 iteration 85 : loss : 0.218239, loss_ce: 0.085568
  1%|▍                              | 5/400 [02:05<2:58:58, 27.19s/it]2022-01-20 19:34:57,128 iteration 86 : loss : 0.205843, loss_ce: 0.067565
2022-01-20 19:34:58,476 iteration 87 : loss : 0.205246, loss_ce: 0.082456
2022-01-20 19:34:59,655 iteration 88 : loss : 0.204965, loss_ce: 0.083114
2022-01-20 19:35:00,923 iteration 89 : loss : 0.219720, loss_ce: 0.087950
2022-01-20 19:35:02,177 iteration 90 : loss : 0.192508, loss_ce: 0.078290
2022-01-20 19:35:03,493 iteration 91 : loss : 0.198671, loss_ce: 0.093215
2022-01-20 19:35:04,659 iteration 92 : loss : 0.199520, loss_ce: 0.077634
2022-01-20 19:35:05,866 iteration 93 : loss : 0.241381, loss_ce: 0.087496
2022-01-20 19:35:07,086 iteration 94 : loss : 0.196831, loss_ce: 0.075371
2022-01-20 19:35:08,433 iteration 95 : loss : 0.216296, loss_ce: 0.092539
2022-01-20 19:35:09,634 iteration 96 : loss : 0.188304, loss_ce: 0.079138
2022-01-20 19:35:10,844 iteration 97 : loss : 0.208344, loss_ce: 0.078511
2022-01-20 19:35:12,066 iteration 98 : loss : 0.204342, loss_ce: 0.079296
2022-01-20 19:35:13,368 iteration 99 : loss : 0.206073, loss_ce: 0.080779
2022-01-20 19:35:14,618 iteration 100 : loss : 0.198324, loss_ce: 0.075756
2022-01-20 19:35:15,844 iteration 101 : loss : 0.147804, loss_ce: 0.054778
2022-01-20 19:35:17,039 iteration 102 : loss : 0.185301, loss_ce: 0.072044
  2%|▍                              | 6/400 [02:26<2:45:09, 25.15s/it]2022-01-20 19:35:18,452 iteration 103 : loss : 0.160571, loss_ce: 0.061692
2022-01-20 19:35:19,836 iteration 104 : loss : 0.218594, loss_ce: 0.090960
2022-01-20 19:35:21,267 iteration 105 : loss : 0.213063, loss_ce: 0.083880
2022-01-20 19:35:22,538 iteration 106 : loss : 0.220462, loss_ce: 0.082796
2022-01-20 19:35:23,977 iteration 107 : loss : 0.204368, loss_ce: 0.088280
2022-01-20 19:35:25,213 iteration 108 : loss : 0.239094, loss_ce: 0.087179
2022-01-20 19:35:26,488 iteration 109 : loss : 0.179619, loss_ce: 0.072877
2022-01-20 19:35:27,729 iteration 110 : loss : 0.194740, loss_ce: 0.079549
2022-01-20 19:35:29,069 iteration 111 : loss : 0.225400, loss_ce: 0.102284
2022-01-20 19:35:30,313 iteration 112 : loss : 0.190739, loss_ce: 0.068387
2022-01-20 19:35:31,621 iteration 113 : loss : 0.241428, loss_ce: 0.118086
2022-01-20 19:35:32,897 iteration 114 : loss : 0.206468, loss_ce: 0.061717
2022-01-20 19:35:34,188 iteration 115 : loss : 0.213097, loss_ce: 0.081308
2022-01-20 19:35:35,520 iteration 116 : loss : 0.392821, loss_ce: 0.192501
2022-01-20 19:35:36,860 iteration 117 : loss : 0.219891, loss_ce: 0.097613
2022-01-20 19:35:38,166 iteration 118 : loss : 0.215379, loss_ce: 0.089917
2022-01-20 19:35:39,499 iteration 119 : loss : 0.192630, loss_ce: 0.070692
  2%|▌                              | 7/400 [02:49<2:38:58, 24.27s/it]2022-01-20 19:35:40,806 iteration 120 : loss : 0.267088, loss_ce: 0.115746
2022-01-20 19:35:42,044 iteration 121 : loss : 0.211687, loss_ce: 0.089397
2022-01-20 19:35:43,390 iteration 122 : loss : 0.230454, loss_ce: 0.094045
2022-01-20 19:35:44,698 iteration 123 : loss : 0.200811, loss_ce: 0.076111
2022-01-20 19:35:45,982 iteration 124 : loss : 0.222613, loss_ce: 0.081782
2022-01-20 19:35:47,236 iteration 125 : loss : 0.192267, loss_ce: 0.084746
2022-01-20 19:35:48,543 iteration 126 : loss : 0.206903, loss_ce: 0.073066
2022-01-20 19:35:49,781 iteration 127 : loss : 0.191048, loss_ce: 0.076594
2022-01-20 19:35:51,056 iteration 128 : loss : 0.198260, loss_ce: 0.077372
2022-01-20 19:35:52,411 iteration 129 : loss : 0.199378, loss_ce: 0.069792
2022-01-20 19:35:53,714 iteration 130 : loss : 0.191441, loss_ce: 0.064777
2022-01-20 19:35:55,099 iteration 131 : loss : 0.217122, loss_ce: 0.098427
2022-01-20 19:35:56,501 iteration 132 : loss : 0.194553, loss_ce: 0.056272
2022-01-20 19:35:57,767 iteration 133 : loss : 0.210574, loss_ce: 0.078074
2022-01-20 19:35:59,136 iteration 134 : loss : 0.185516, loss_ce: 0.075768
2022-01-20 19:36:00,516 iteration 135 : loss : 0.195812, loss_ce: 0.080346
2022-01-20 19:36:01,802 iteration 136 : loss : 0.164511, loss_ce: 0.072292
  2%|▌                              | 8/400 [03:11<2:34:27, 23.64s/it]2022-01-20 19:36:03,205 iteration 137 : loss : 0.200817, loss_ce: 0.067545
2022-01-20 19:36:04,478 iteration 138 : loss : 0.204287, loss_ce: 0.102889
2022-01-20 19:36:05,818 iteration 139 : loss : 0.218924, loss_ce: 0.087414
2022-01-20 19:36:07,129 iteration 140 : loss : 0.199302, loss_ce: 0.071195
2022-01-20 19:36:08,467 iteration 141 : loss : 0.179470, loss_ce: 0.079789
2022-01-20 19:36:09,826 iteration 142 : loss : 0.186761, loss_ce: 0.075995
2022-01-20 19:36:11,207 iteration 143 : loss : 0.188728, loss_ce: 0.071050
2022-01-20 19:36:12,548 iteration 144 : loss : 0.187879, loss_ce: 0.068698
2022-01-20 19:36:13,837 iteration 145 : loss : 0.203755, loss_ce: 0.074217
2022-01-20 19:36:15,170 iteration 146 : loss : 0.156301, loss_ce: 0.068536
2022-01-20 19:36:16,523 iteration 147 : loss : 0.194580, loss_ce: 0.071230
2022-01-20 19:36:17,748 iteration 148 : loss : 0.163358, loss_ce: 0.069463
2022-01-20 19:36:19,049 iteration 149 : loss : 0.244727, loss_ce: 0.105341
2022-01-20 19:36:20,314 iteration 150 : loss : 0.230141, loss_ce: 0.085543
2022-01-20 19:36:21,588 iteration 151 : loss : 0.240830, loss_ce: 0.108755
2022-01-20 19:36:22,872 iteration 152 : loss : 0.213592, loss_ce: 0.069964
2022-01-20 19:36:24,201 iteration 153 : loss : 0.223027, loss_ce: 0.087417
  2%|▋                              | 9/400 [03:34<2:31:32, 23.25s/it]2022-01-20 19:36:25,488 iteration 154 : loss : 0.211501, loss_ce: 0.084441
2022-01-20 19:36:26,806 iteration 155 : loss : 0.206712, loss_ce: 0.070890
2022-01-20 19:36:28,188 iteration 156 : loss : 0.203610, loss_ce: 0.076524
2022-01-20 19:36:29,502 iteration 157 : loss : 0.255948, loss_ce: 0.093112
2022-01-20 19:36:30,908 iteration 158 : loss : 0.202368, loss_ce: 0.081126
2022-01-20 19:36:32,092 iteration 159 : loss : 0.149778, loss_ce: 0.054599
2022-01-20 19:36:33,472 iteration 160 : loss : 0.172476, loss_ce: 0.054222
2022-01-20 19:36:34,830 iteration 161 : loss : 0.246819, loss_ce: 0.098960
2022-01-20 19:36:36,193 iteration 162 : loss : 0.199954, loss_ce: 0.087859
2022-01-20 19:36:37,498 iteration 163 : loss : 0.207660, loss_ce: 0.078815
2022-01-20 19:36:38,825 iteration 164 : loss : 0.170919, loss_ce: 0.060161
2022-01-20 19:36:40,116 iteration 165 : loss : 0.200141, loss_ce: 0.083851
2022-01-20 19:36:41,388 iteration 166 : loss : 0.197398, loss_ce: 0.076478
2022-01-20 19:36:42,674 iteration 167 : loss : 0.145987, loss_ce: 0.057141
2022-01-20 19:36:44,007 iteration 168 : loss : 0.198455, loss_ce: 0.095219
2022-01-20 19:36:45,306 iteration 169 : loss : 0.171843, loss_ce: 0.083300
2022-01-20 19:36:45,307 Training Data Eval:
2022-01-20 19:36:51,779   Average segmentation loss on training set: 0.4926
2022-01-20 19:36:51,780 Validation Data Eval:
2022-01-20 19:36:54,029   Average segmentation loss on validation set: 0.4117
2022-01-20 19:37:00,044 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 19:37:01,319 iteration 170 : loss : 0.166952, loss_ce: 0.070601
  2%|▊                             | 10/400 [04:11<2:58:57, 27.53s/it]2022-01-20 19:37:02,606 iteration 171 : loss : 0.174810, loss_ce: 0.069663
2022-01-20 19:37:03,870 iteration 172 : loss : 0.184952, loss_ce: 0.082855
2022-01-20 19:37:05,151 iteration 173 : loss : 0.129917, loss_ce: 0.056931
2022-01-20 19:37:06,346 iteration 174 : loss : 0.189781, loss_ce: 0.077784
2022-01-20 19:37:07,525 iteration 175 : loss : 0.238669, loss_ce: 0.094541
2022-01-20 19:37:08,784 iteration 176 : loss : 0.190791, loss_ce: 0.079620
2022-01-20 19:37:10,037 iteration 177 : loss : 0.187439, loss_ce: 0.061285
2022-01-20 19:37:11,264 iteration 178 : loss : 0.141238, loss_ce: 0.055542
2022-01-20 19:37:12,499 iteration 179 : loss : 0.194843, loss_ce: 0.079655
2022-01-20 19:37:13,705 iteration 180 : loss : 0.172039, loss_ce: 0.061988
2022-01-20 19:37:14,957 iteration 181 : loss : 0.187953, loss_ce: 0.075723
2022-01-20 19:37:16,160 iteration 182 : loss : 0.152181, loss_ce: 0.062475
2022-01-20 19:37:17,338 iteration 183 : loss : 0.158406, loss_ce: 0.053395
2022-01-20 19:37:18,480 iteration 184 : loss : 0.181715, loss_ce: 0.064694
2022-01-20 19:37:19,805 iteration 185 : loss : 0.192497, loss_ce: 0.066714
2022-01-20 19:37:21,154 iteration 186 : loss : 0.228983, loss_ce: 0.098145
2022-01-20 19:37:22,545 iteration 187 : loss : 0.190983, loss_ce: 0.070056
  3%|▊                             | 11/400 [04:32<2:45:59, 25.60s/it]2022-01-20 19:37:23,934 iteration 188 : loss : 0.187880, loss_ce: 0.073762
2022-01-20 19:37:25,274 iteration 189 : loss : 0.172839, loss_ce: 0.062019
2022-01-20 19:37:26,621 iteration 190 : loss : 0.174660, loss_ce: 0.061512
2022-01-20 19:37:28,036 iteration 191 : loss : 0.167466, loss_ce: 0.059465
2022-01-20 19:37:29,287 iteration 192 : loss : 0.149028, loss_ce: 0.062589
2022-01-20 19:37:30,661 iteration 193 : loss : 0.172206, loss_ce: 0.067069
2022-01-20 19:37:31,958 iteration 194 : loss : 0.195855, loss_ce: 0.054258
2022-01-20 19:37:33,231 iteration 195 : loss : 0.160237, loss_ce: 0.062767
2022-01-20 19:37:34,468 iteration 196 : loss : 0.179525, loss_ce: 0.059791
2022-01-20 19:37:35,802 iteration 197 : loss : 0.192234, loss_ce: 0.074373
2022-01-20 19:37:37,153 iteration 198 : loss : 0.183669, loss_ce: 0.080494
2022-01-20 19:37:38,431 iteration 199 : loss : 0.208267, loss_ce: 0.080747
2022-01-20 19:37:39,729 iteration 200 : loss : 0.168905, loss_ce: 0.076216
2022-01-20 19:37:41,046 iteration 201 : loss : 0.141374, loss_ce: 0.057058
2022-01-20 19:37:42,381 iteration 202 : loss : 0.179697, loss_ce: 0.080763
2022-01-20 19:37:43,641 iteration 203 : loss : 0.158132, loss_ce: 0.063536
2022-01-20 19:37:45,035 iteration 204 : loss : 0.214235, loss_ce: 0.081789
  3%|▉                             | 12/400 [04:54<2:39:26, 24.66s/it]2022-01-20 19:37:46,343 iteration 205 : loss : 0.150314, loss_ce: 0.059107
2022-01-20 19:37:47,643 iteration 206 : loss : 0.149269, loss_ce: 0.046652
2022-01-20 19:37:48,962 iteration 207 : loss : 0.163306, loss_ce: 0.059979
2022-01-20 19:37:50,234 iteration 208 : loss : 0.227725, loss_ce: 0.063576
2022-01-20 19:37:51,607 iteration 209 : loss : 0.252252, loss_ce: 0.118627
2022-01-20 19:37:52,825 iteration 210 : loss : 0.156253, loss_ce: 0.050190
2022-01-20 19:37:54,168 iteration 211 : loss : 0.229820, loss_ce: 0.098874
2022-01-20 19:37:55,496 iteration 212 : loss : 0.207359, loss_ce: 0.080119
2022-01-20 19:37:56,894 iteration 213 : loss : 0.161883, loss_ce: 0.070374
2022-01-20 19:37:58,196 iteration 214 : loss : 0.155625, loss_ce: 0.051972
2022-01-20 19:37:59,495 iteration 215 : loss : 0.202717, loss_ce: 0.086666
2022-01-20 19:38:00,947 iteration 216 : loss : 0.168719, loss_ce: 0.063119
2022-01-20 19:38:02,339 iteration 217 : loss : 0.151288, loss_ce: 0.056762
2022-01-20 19:38:03,665 iteration 218 : loss : 0.163142, loss_ce: 0.070813
2022-01-20 19:38:04,890 iteration 219 : loss : 0.163470, loss_ce: 0.067077
2022-01-20 19:38:06,230 iteration 220 : loss : 0.155600, loss_ce: 0.066387
2022-01-20 19:38:07,545 iteration 221 : loss : 0.199782, loss_ce: 0.068241
  3%|▉                             | 13/400 [05:17<2:34:50, 24.01s/it]2022-01-20 19:38:08,912 iteration 222 : loss : 0.183680, loss_ce: 0.075597
2022-01-20 19:38:10,264 iteration 223 : loss : 0.149327, loss_ce: 0.061919
2022-01-20 19:38:11,563 iteration 224 : loss : 0.243216, loss_ce: 0.121737
2022-01-20 19:38:12,955 iteration 225 : loss : 0.282180, loss_ce: 0.080986
2022-01-20 19:38:14,248 iteration 226 : loss : 0.183897, loss_ce: 0.069281
2022-01-20 19:38:15,589 iteration 227 : loss : 0.248570, loss_ce: 0.112705
2022-01-20 19:38:16,887 iteration 228 : loss : 0.193062, loss_ce: 0.079025
2022-01-20 19:38:18,147 iteration 229 : loss : 0.166863, loss_ce: 0.052110
2022-01-20 19:38:19,465 iteration 230 : loss : 0.161924, loss_ce: 0.059617
2022-01-20 19:38:20,820 iteration 231 : loss : 0.220513, loss_ce: 0.084562
2022-01-20 19:38:22,160 iteration 232 : loss : 0.139511, loss_ce: 0.057192
2022-01-20 19:38:23,417 iteration 233 : loss : 0.187321, loss_ce: 0.076010
2022-01-20 19:38:24,792 iteration 234 : loss : 0.206752, loss_ce: 0.088823
2022-01-20 19:38:26,082 iteration 235 : loss : 0.173701, loss_ce: 0.067029
2022-01-20 19:38:27,411 iteration 236 : loss : 0.171373, loss_ce: 0.067734
2022-01-20 19:38:28,705 iteration 237 : loss : 0.156915, loss_ce: 0.059704
2022-01-20 19:38:30,012 iteration 238 : loss : 0.172224, loss_ce: 0.079673
  4%|█                             | 14/400 [05:39<2:31:27, 23.54s/it]2022-01-20 19:38:31,317 iteration 239 : loss : 0.185253, loss_ce: 0.060153
2022-01-20 19:38:32,663 iteration 240 : loss : 0.159317, loss_ce: 0.073295
2022-01-20 19:38:34,015 iteration 241 : loss : 0.180695, loss_ce: 0.067917
2022-01-20 19:38:35,345 iteration 242 : loss : 0.198145, loss_ce: 0.104280
2022-01-20 19:38:36,794 iteration 243 : loss : 0.169315, loss_ce: 0.067971
2022-01-20 19:38:38,047 iteration 244 : loss : 0.210813, loss_ce: 0.062098
2022-01-20 19:38:39,382 iteration 245 : loss : 0.175746, loss_ce: 0.077080
2022-01-20 19:38:40,754 iteration 246 : loss : 0.177329, loss_ce: 0.067633
2022-01-20 19:38:42,067 iteration 247 : loss : 0.199795, loss_ce: 0.070337
2022-01-20 19:38:43,372 iteration 248 : loss : 0.173304, loss_ce: 0.061325
2022-01-20 19:38:44,660 iteration 249 : loss : 0.182294, loss_ce: 0.095364
2022-01-20 19:38:45,952 iteration 250 : loss : 0.198993, loss_ce: 0.081949
2022-01-20 19:38:47,197 iteration 251 : loss : 0.212353, loss_ce: 0.076200
2022-01-20 19:38:48,502 iteration 252 : loss : 0.115881, loss_ce: 0.045284
2022-01-20 19:38:49,826 iteration 253 : loss : 0.155657, loss_ce: 0.059219
2022-01-20 19:38:51,194 iteration 254 : loss : 0.262196, loss_ce: 0.079940
2022-01-20 19:38:51,194 Training Data Eval:
2022-01-20 19:38:57,706   Average segmentation loss on training set: 0.4534
2022-01-20 19:38:57,706 Validation Data Eval:
2022-01-20 19:38:59,947   Average segmentation loss on validation set: 0.3849
2022-01-20 19:39:05,821 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 19:39:07,027 iteration 255 : loss : 0.191512, loss_ce: 0.073993
  4%|█▏                            | 15/400 [06:16<2:57:07, 27.60s/it]2022-01-20 19:39:08,325 iteration 256 : loss : 0.219566, loss_ce: 0.083413
2022-01-20 19:39:09,617 iteration 257 : loss : 0.171539, loss_ce: 0.063850
2022-01-20 19:39:10,956 iteration 258 : loss : 0.194446, loss_ce: 0.064911
2022-01-20 19:39:12,117 iteration 259 : loss : 0.136766, loss_ce: 0.058442
2022-01-20 19:39:13,413 iteration 260 : loss : 0.157381, loss_ce: 0.065610
2022-01-20 19:39:14,679 iteration 261 : loss : 0.210664, loss_ce: 0.066461
2022-01-20 19:39:16,024 iteration 262 : loss : 0.201721, loss_ce: 0.074634
2022-01-20 19:39:17,273 iteration 263 : loss : 0.168563, loss_ce: 0.068992
2022-01-20 19:39:18,549 iteration 264 : loss : 0.222594, loss_ce: 0.103567
2022-01-20 19:39:19,757 iteration 265 : loss : 0.137829, loss_ce: 0.052887
2022-01-20 19:39:21,028 iteration 266 : loss : 0.162188, loss_ce: 0.067262
2022-01-20 19:39:22,266 iteration 267 : loss : 0.200559, loss_ce: 0.065787
2022-01-20 19:39:23,506 iteration 268 : loss : 0.219826, loss_ce: 0.101508
2022-01-20 19:39:24,984 iteration 269 : loss : 0.207460, loss_ce: 0.074899
2022-01-20 19:39:26,375 iteration 270 : loss : 0.148816, loss_ce: 0.058715
2022-01-20 19:39:27,577 iteration 271 : loss : 0.188361, loss_ce: 0.058571
2022-01-20 19:39:28,921 iteration 272 : loss : 0.156291, loss_ce: 0.073420
  4%|█▏                            | 16/400 [06:38<2:45:40, 25.89s/it]2022-01-20 19:39:30,241 iteration 273 : loss : 0.148354, loss_ce: 0.054446
2022-01-20 19:39:31,514 iteration 274 : loss : 0.133068, loss_ce: 0.049080
2022-01-20 19:39:32,801 iteration 275 : loss : 0.138824, loss_ce: 0.055173
2022-01-20 19:39:34,144 iteration 276 : loss : 0.150131, loss_ce: 0.065979
2022-01-20 19:39:35,498 iteration 277 : loss : 0.140332, loss_ce: 0.052420
2022-01-20 19:39:36,699 iteration 278 : loss : 0.176929, loss_ce: 0.061931
2022-01-20 19:39:37,970 iteration 279 : loss : 0.176755, loss_ce: 0.062426
2022-01-20 19:39:39,220 iteration 280 : loss : 0.160800, loss_ce: 0.068105
2022-01-20 19:39:40,541 iteration 281 : loss : 0.140260, loss_ce: 0.055824
2022-01-20 19:39:41,827 iteration 282 : loss : 0.232186, loss_ce: 0.094282
2022-01-20 19:39:43,096 iteration 283 : loss : 0.182623, loss_ce: 0.090757
2022-01-20 19:39:44,462 iteration 284 : loss : 0.203848, loss_ce: 0.085406
2022-01-20 19:39:45,712 iteration 285 : loss : 0.188541, loss_ce: 0.059694
2022-01-20 19:39:46,983 iteration 286 : loss : 0.147808, loss_ce: 0.061020
2022-01-20 19:39:48,368 iteration 287 : loss : 0.190562, loss_ce: 0.080272
2022-01-20 19:39:49,724 iteration 288 : loss : 0.203571, loss_ce: 0.079560
2022-01-20 19:39:51,023 iteration 289 : loss : 0.164288, loss_ce: 0.059074
  4%|█▎                            | 17/400 [07:00<2:37:57, 24.75s/it]2022-01-20 19:39:52,388 iteration 290 : loss : 0.153592, loss_ce: 0.060246
2022-01-20 19:39:53,670 iteration 291 : loss : 0.172666, loss_ce: 0.070594
2022-01-20 19:39:54,948 iteration 292 : loss : 0.165469, loss_ce: 0.061659
2022-01-20 19:39:56,233 iteration 293 : loss : 0.206727, loss_ce: 0.084538
2022-01-20 19:39:57,553 iteration 294 : loss : 0.165804, loss_ce: 0.064381
2022-01-20 19:39:58,956 iteration 295 : loss : 0.194796, loss_ce: 0.060837
2022-01-20 19:40:00,234 iteration 296 : loss : 0.144451, loss_ce: 0.053447
2022-01-20 19:40:01,543 iteration 297 : loss : 0.181404, loss_ce: 0.055606
2022-01-20 19:40:02,824 iteration 298 : loss : 0.130502, loss_ce: 0.045011
2022-01-20 19:40:04,120 iteration 299 : loss : 0.153783, loss_ce: 0.056628
2022-01-20 19:40:05,423 iteration 300 : loss : 0.163117, loss_ce: 0.056022
2022-01-20 19:40:06,746 iteration 301 : loss : 0.217053, loss_ce: 0.113012
2022-01-20 19:40:08,116 iteration 302 : loss : 0.151843, loss_ce: 0.060624
2022-01-20 19:40:09,451 iteration 303 : loss : 0.172146, loss_ce: 0.065806
2022-01-20 19:40:10,767 iteration 304 : loss : 0.216539, loss_ce: 0.094426
2022-01-20 19:40:12,119 iteration 305 : loss : 0.166128, loss_ce: 0.068548
2022-01-20 19:40:13,457 iteration 306 : loss : 0.127821, loss_ce: 0.057476
  4%|█▎                            | 18/400 [07:23<2:33:08, 24.05s/it]2022-01-20 19:40:14,817 iteration 307 : loss : 0.161785, loss_ce: 0.056455
2022-01-20 19:40:16,094 iteration 308 : loss : 0.154434, loss_ce: 0.070136
2022-01-20 19:40:17,436 iteration 309 : loss : 0.220052, loss_ce: 0.075266
2022-01-20 19:40:18,799 iteration 310 : loss : 0.163441, loss_ce: 0.069955
2022-01-20 19:40:20,066 iteration 311 : loss : 0.206006, loss_ce: 0.061030
2022-01-20 19:40:21,361 iteration 312 : loss : 0.174382, loss_ce: 0.078226
2022-01-20 19:40:22,680 iteration 313 : loss : 0.204701, loss_ce: 0.096152
2022-01-20 19:40:23,926 iteration 314 : loss : 0.131860, loss_ce: 0.058327
2022-01-20 19:40:25,228 iteration 315 : loss : 0.160117, loss_ce: 0.059412
2022-01-20 19:40:26,547 iteration 316 : loss : 0.179173, loss_ce: 0.069567
2022-01-20 19:40:27,856 iteration 317 : loss : 0.212647, loss_ce: 0.089143
2022-01-20 19:40:29,182 iteration 318 : loss : 0.155376, loss_ce: 0.065802
2022-01-20 19:40:30,438 iteration 319 : loss : 0.156570, loss_ce: 0.065698
2022-01-20 19:40:31,828 iteration 320 : loss : 0.176489, loss_ce: 0.075953
2022-01-20 19:40:33,181 iteration 321 : loss : 0.162053, loss_ce: 0.057765
2022-01-20 19:40:34,490 iteration 322 : loss : 0.202254, loss_ce: 0.081006
2022-01-20 19:40:35,860 iteration 323 : loss : 0.177304, loss_ce: 0.056503
  5%|█▍                            | 19/400 [07:45<2:29:35, 23.56s/it]2022-01-20 19:40:37,203 iteration 324 : loss : 0.148011, loss_ce: 0.053115
2022-01-20 19:40:38,522 iteration 325 : loss : 0.188508, loss_ce: 0.066402
2022-01-20 19:40:39,867 iteration 326 : loss : 0.130327, loss_ce: 0.054902
2022-01-20 19:40:41,281 iteration 327 : loss : 0.208455, loss_ce: 0.075821
2022-01-20 19:40:42,592 iteration 328 : loss : 0.144902, loss_ce: 0.059859
2022-01-20 19:40:43,938 iteration 329 : loss : 0.178937, loss_ce: 0.081982
2022-01-20 19:40:45,291 iteration 330 : loss : 0.157092, loss_ce: 0.063213
2022-01-20 19:40:46,552 iteration 331 : loss : 0.152474, loss_ce: 0.069552
2022-01-20 19:40:47,842 iteration 332 : loss : 0.120388, loss_ce: 0.044006
2022-01-20 19:40:49,185 iteration 333 : loss : 0.128195, loss_ce: 0.056659
2022-01-20 19:40:50,512 iteration 334 : loss : 0.142580, loss_ce: 0.061771
2022-01-20 19:40:51,803 iteration 335 : loss : 0.202754, loss_ce: 0.064933
2022-01-20 19:40:53,161 iteration 336 : loss : 0.134504, loss_ce: 0.049294
2022-01-20 19:40:54,474 iteration 337 : loss : 0.158294, loss_ce: 0.048970
2022-01-20 19:40:55,746 iteration 338 : loss : 0.200726, loss_ce: 0.100918
2022-01-20 19:40:57,018 iteration 339 : loss : 0.182791, loss_ce: 0.059286
2022-01-20 19:40:57,018 Training Data Eval:
2022-01-20 19:41:03,532   Average segmentation loss on training set: 0.3214
2022-01-20 19:41:03,532 Validation Data Eval:
2022-01-20 19:41:05,783   Average segmentation loss on validation set: 0.3860
2022-01-20 19:41:07,148 iteration 340 : loss : 0.161902, loss_ce: 0.061258
  5%|█▌                            | 20/400 [08:16<2:43:52, 25.88s/it]2022-01-20 19:41:08,551 iteration 341 : loss : 0.148796, loss_ce: 0.064215
2022-01-20 19:41:09,888 iteration 342 : loss : 0.128352, loss_ce: 0.040822
2022-01-20 19:41:11,200 iteration 343 : loss : 0.180259, loss_ce: 0.071692
2022-01-20 19:41:12,549 iteration 344 : loss : 0.197666, loss_ce: 0.077716
2022-01-20 19:41:13,851 iteration 345 : loss : 0.114455, loss_ce: 0.039326
2022-01-20 19:41:15,213 iteration 346 : loss : 0.163723, loss_ce: 0.061840
2022-01-20 19:41:16,464 iteration 347 : loss : 0.159393, loss_ce: 0.074889
2022-01-20 19:41:17,791 iteration 348 : loss : 0.168982, loss_ce: 0.063849
2022-01-20 19:41:19,229 iteration 349 : loss : 0.156934, loss_ce: 0.079497
2022-01-20 19:41:20,537 iteration 350 : loss : 0.160256, loss_ce: 0.065551
2022-01-20 19:41:21,910 iteration 351 : loss : 0.172715, loss_ce: 0.087650
2022-01-20 19:41:23,266 iteration 352 : loss : 0.144816, loss_ce: 0.061291
2022-01-20 19:41:24,542 iteration 353 : loss : 0.142257, loss_ce: 0.052581
2022-01-20 19:41:25,855 iteration 354 : loss : 0.169478, loss_ce: 0.053632
2022-01-20 19:41:27,211 iteration 355 : loss : 0.185879, loss_ce: 0.073978
2022-01-20 19:41:28,652 iteration 356 : loss : 0.149050, loss_ce: 0.057741
2022-01-20 19:41:29,912 iteration 357 : loss : 0.143827, loss_ce: 0.052703
  5%|█▌                            | 21/400 [08:39<2:37:33, 24.94s/it]2022-01-20 19:41:31,304 iteration 358 : loss : 0.183931, loss_ce: 0.074743
2022-01-20 19:41:32,674 iteration 359 : loss : 0.173186, loss_ce: 0.065708
2022-01-20 19:41:33,936 iteration 360 : loss : 0.132783, loss_ce: 0.046225
2022-01-20 19:41:35,284 iteration 361 : loss : 0.175471, loss_ce: 0.066052
2022-01-20 19:41:36,665 iteration 362 : loss : 0.142100, loss_ce: 0.040544
2022-01-20 19:41:38,045 iteration 363 : loss : 0.168043, loss_ce: 0.058710
2022-01-20 19:41:39,366 iteration 364 : loss : 0.152171, loss_ce: 0.049360
2022-01-20 19:41:40,575 iteration 365 : loss : 0.179585, loss_ce: 0.076487
2022-01-20 19:41:41,916 iteration 366 : loss : 0.186904, loss_ce: 0.079658
2022-01-20 19:41:43,164 iteration 367 : loss : 0.163393, loss_ce: 0.065819
2022-01-20 19:41:44,517 iteration 368 : loss : 0.158188, loss_ce: 0.057521
2022-01-20 19:41:45,899 iteration 369 : loss : 0.205207, loss_ce: 0.064384
2022-01-20 19:41:47,265 iteration 370 : loss : 0.153044, loss_ce: 0.076097
2022-01-20 19:41:48,598 iteration 371 : loss : 0.207080, loss_ce: 0.063059
2022-01-20 19:41:49,937 iteration 372 : loss : 0.221355, loss_ce: 0.108610
2022-01-20 19:41:51,202 iteration 373 : loss : 0.169927, loss_ce: 0.055747
2022-01-20 19:41:52,501 iteration 374 : loss : 0.166370, loss_ce: 0.064785
  6%|█▋                            | 22/400 [09:02<2:32:42, 24.24s/it]2022-01-20 19:41:53,913 iteration 375 : loss : 0.252313, loss_ce: 0.108137
2022-01-20 19:41:55,174 iteration 376 : loss : 0.108430, loss_ce: 0.036965
2022-01-20 19:41:56,457 iteration 377 : loss : 0.150214, loss_ce: 0.049093
2022-01-20 19:41:57,820 iteration 378 : loss : 0.154157, loss_ce: 0.055046
2022-01-20 19:41:59,053 iteration 379 : loss : 0.131918, loss_ce: 0.051742
2022-01-20 19:42:00,405 iteration 380 : loss : 0.165582, loss_ce: 0.057150
2022-01-20 19:42:01,723 iteration 381 : loss : 0.150148, loss_ce: 0.053569
2022-01-20 19:42:02,964 iteration 382 : loss : 0.159516, loss_ce: 0.068061
2022-01-20 19:42:04,269 iteration 383 : loss : 0.179846, loss_ce: 0.063009
2022-01-20 19:42:05,585 iteration 384 : loss : 0.144375, loss_ce: 0.059681
2022-01-20 19:42:06,879 iteration 385 : loss : 0.098614, loss_ce: 0.042356
2022-01-20 19:42:08,193 iteration 386 : loss : 0.148555, loss_ce: 0.051544
2022-01-20 19:42:09,537 iteration 387 : loss : 0.209309, loss_ce: 0.072784
2022-01-20 19:42:10,888 iteration 388 : loss : 0.171516, loss_ce: 0.069152
2022-01-20 19:42:12,229 iteration 389 : loss : 0.240304, loss_ce: 0.125943
2022-01-20 19:42:13,532 iteration 390 : loss : 0.113957, loss_ce: 0.047659
2022-01-20 19:42:14,820 iteration 391 : loss : 0.209925, loss_ce: 0.115957
  6%|█▋                            | 23/400 [09:24<2:28:39, 23.66s/it]2022-01-20 19:42:16,202 iteration 392 : loss : 0.161877, loss_ce: 0.080589
2022-01-20 19:42:17,470 iteration 393 : loss : 0.137457, loss_ce: 0.054113
2022-01-20 19:42:18,833 iteration 394 : loss : 0.137552, loss_ce: 0.047064
2022-01-20 19:42:20,173 iteration 395 : loss : 0.177375, loss_ce: 0.069853
2022-01-20 19:42:21,469 iteration 396 : loss : 0.159446, loss_ce: 0.056737
2022-01-20 19:42:22,772 iteration 397 : loss : 0.177180, loss_ce: 0.074512
2022-01-20 19:42:24,098 iteration 398 : loss : 0.121589, loss_ce: 0.046283
2022-01-20 19:42:25,314 iteration 399 : loss : 0.150160, loss_ce: 0.060376
2022-01-20 19:42:26,644 iteration 400 : loss : 0.174988, loss_ce: 0.056241
2022-01-20 19:42:28,010 iteration 401 : loss : 0.153226, loss_ce: 0.064618
2022-01-20 19:42:29,291 iteration 402 : loss : 0.113134, loss_ce: 0.049269
2022-01-20 19:42:30,580 iteration 403 : loss : 0.118217, loss_ce: 0.048163
2022-01-20 19:42:31,923 iteration 404 : loss : 0.158224, loss_ce: 0.061415
2022-01-20 19:42:33,295 iteration 405 : loss : 0.155122, loss_ce: 0.065005
2022-01-20 19:42:34,584 iteration 406 : loss : 0.160845, loss_ce: 0.068004
2022-01-20 19:42:35,835 iteration 407 : loss : 0.147038, loss_ce: 0.054699
2022-01-20 19:42:37,134 iteration 408 : loss : 0.180172, loss_ce: 0.065513
  6%|█▊                            | 24/400 [09:46<2:25:45, 23.26s/it]2022-01-20 19:42:38,512 iteration 409 : loss : 0.138466, loss_ce: 0.068214
2022-01-20 19:42:39,838 iteration 410 : loss : 0.156611, loss_ce: 0.062895
2022-01-20 19:42:41,138 iteration 411 : loss : 0.276264, loss_ce: 0.089507
2022-01-20 19:42:42,357 iteration 412 : loss : 0.135975, loss_ce: 0.051957
2022-01-20 19:42:43,668 iteration 413 : loss : 0.130571, loss_ce: 0.047343
2022-01-20 19:42:44,927 iteration 414 : loss : 0.186265, loss_ce: 0.085324
2022-01-20 19:42:46,142 iteration 415 : loss : 0.121689, loss_ce: 0.045717
2022-01-20 19:42:47,435 iteration 416 : loss : 0.181033, loss_ce: 0.088409
2022-01-20 19:42:48,709 iteration 417 : loss : 0.195534, loss_ce: 0.068807
2022-01-20 19:42:50,081 iteration 418 : loss : 0.134744, loss_ce: 0.055607
2022-01-20 19:42:51,370 iteration 419 : loss : 0.172267, loss_ce: 0.083821
2022-01-20 19:42:52,688 iteration 420 : loss : 0.128863, loss_ce: 0.047910
2022-01-20 19:42:53,939 iteration 421 : loss : 0.167243, loss_ce: 0.066252
2022-01-20 19:42:55,188 iteration 422 : loss : 0.175079, loss_ce: 0.073935
2022-01-20 19:42:56,457 iteration 423 : loss : 0.186771, loss_ce: 0.063090
2022-01-20 19:42:57,787 iteration 424 : loss : 0.157297, loss_ce: 0.063361
2022-01-20 19:42:57,788 Training Data Eval:
2022-01-20 19:43:04,332   Average segmentation loss on training set: 0.1221
2022-01-20 19:43:04,332 Validation Data Eval:
2022-01-20 19:43:06,574   Average segmentation loss on validation set: 0.1588
2022-01-20 19:43:12,405 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 19:43:13,631 iteration 425 : loss : 0.149800, loss_ce: 0.066831
  6%|█▉                            | 25/400 [10:23<2:50:11, 27.23s/it]2022-01-20 19:43:14,853 iteration 426 : loss : 0.167936, loss_ce: 0.057165
2022-01-20 19:43:16,122 iteration 427 : loss : 0.116159, loss_ce: 0.040244
2022-01-20 19:43:17,362 iteration 428 : loss : 0.153089, loss_ce: 0.063556
2022-01-20 19:43:18,544 iteration 429 : loss : 0.205646, loss_ce: 0.103452
2022-01-20 19:43:19,717 iteration 430 : loss : 0.134152, loss_ce: 0.045783
2022-01-20 19:43:20,922 iteration 431 : loss : 0.265947, loss_ce: 0.106836
2022-01-20 19:43:22,195 iteration 432 : loss : 0.111471, loss_ce: 0.046101
2022-01-20 19:43:23,502 iteration 433 : loss : 0.124400, loss_ce: 0.051148
2022-01-20 19:43:24,753 iteration 434 : loss : 0.127626, loss_ce: 0.049210
2022-01-20 19:43:25,930 iteration 435 : loss : 0.148575, loss_ce: 0.064436
2022-01-20 19:43:27,138 iteration 436 : loss : 0.132674, loss_ce: 0.061710
2022-01-20 19:43:28,361 iteration 437 : loss : 0.195383, loss_ce: 0.098730
2022-01-20 19:43:29,555 iteration 438 : loss : 0.173902, loss_ce: 0.062155
2022-01-20 19:43:30,836 iteration 439 : loss : 0.167225, loss_ce: 0.050065
2022-01-20 19:43:32,133 iteration 440 : loss : 0.131784, loss_ce: 0.046463
2022-01-20 19:43:33,501 iteration 441 : loss : 0.148396, loss_ce: 0.053455
2022-01-20 19:43:34,845 iteration 442 : loss : 0.149987, loss_ce: 0.065225
  6%|█▉                            | 26/400 [10:44<2:38:28, 25.43s/it]2022-01-20 19:43:36,183 iteration 443 : loss : 0.159764, loss_ce: 0.072904
2022-01-20 19:43:37,447 iteration 444 : loss : 0.136734, loss_ce: 0.047325
2022-01-20 19:43:38,745 iteration 445 : loss : 0.180705, loss_ce: 0.063530
2022-01-20 19:43:40,165 iteration 446 : loss : 0.155978, loss_ce: 0.060996
2022-01-20 19:43:41,535 iteration 447 : loss : 0.142462, loss_ce: 0.072499
2022-01-20 19:43:42,880 iteration 448 : loss : 0.237145, loss_ce: 0.072185
2022-01-20 19:43:44,181 iteration 449 : loss : 0.145988, loss_ce: 0.065066
2022-01-20 19:43:45,474 iteration 450 : loss : 0.143676, loss_ce: 0.058456
2022-01-20 19:43:46,868 iteration 451 : loss : 0.124145, loss_ce: 0.052357
2022-01-20 19:43:48,212 iteration 452 : loss : 0.109929, loss_ce: 0.047498
2022-01-20 19:43:49,537 iteration 453 : loss : 0.156703, loss_ce: 0.065537
2022-01-20 19:43:50,863 iteration 454 : loss : 0.169463, loss_ce: 0.053515
2022-01-20 19:43:52,226 iteration 455 : loss : 0.118805, loss_ce: 0.043378
2022-01-20 19:43:53,557 iteration 456 : loss : 0.148834, loss_ce: 0.051359
2022-01-20 19:43:54,874 iteration 457 : loss : 0.146264, loss_ce: 0.053100
2022-01-20 19:43:56,255 iteration 458 : loss : 0.165157, loss_ce: 0.079397
2022-01-20 19:43:57,508 iteration 459 : loss : 0.115055, loss_ce: 0.043808
  7%|██                            | 27/400 [11:07<2:32:54, 24.60s/it]2022-01-20 19:43:58,869 iteration 460 : loss : 0.141089, loss_ce: 0.064271
2022-01-20 19:44:00,196 iteration 461 : loss : 0.130720, loss_ce: 0.062665
2022-01-20 19:44:01,498 iteration 462 : loss : 0.128236, loss_ce: 0.054334
2022-01-20 19:44:02,863 iteration 463 : loss : 0.147243, loss_ce: 0.071998
2022-01-20 19:44:04,173 iteration 464 : loss : 0.115362, loss_ce: 0.041956
2022-01-20 19:44:05,436 iteration 465 : loss : 0.164168, loss_ce: 0.050654
2022-01-20 19:44:06,736 iteration 466 : loss : 0.163865, loss_ce: 0.059075
2022-01-20 19:44:08,028 iteration 467 : loss : 0.147018, loss_ce: 0.053077
2022-01-20 19:44:09,442 iteration 468 : loss : 0.166482, loss_ce: 0.062268
2022-01-20 19:44:10,738 iteration 469 : loss : 0.125105, loss_ce: 0.041852
2022-01-20 19:44:12,050 iteration 470 : loss : 0.153459, loss_ce: 0.056147
2022-01-20 19:44:13,364 iteration 471 : loss : 0.160248, loss_ce: 0.055362
2022-01-20 19:44:14,718 iteration 472 : loss : 0.170829, loss_ce: 0.070483
2022-01-20 19:44:16,161 iteration 473 : loss : 0.165494, loss_ce: 0.065762
2022-01-20 19:44:17,547 iteration 474 : loss : 0.213774, loss_ce: 0.094365
2022-01-20 19:44:18,841 iteration 475 : loss : 0.110854, loss_ce: 0.043113
2022-01-20 19:44:20,211 iteration 476 : loss : 0.163810, loss_ce: 0.076510
  7%|██                            | 28/400 [11:30<2:28:58, 24.03s/it]2022-01-20 19:44:21,556 iteration 477 : loss : 0.163403, loss_ce: 0.054576
2022-01-20 19:44:22,904 iteration 478 : loss : 0.119239, loss_ce: 0.043126
2022-01-20 19:44:24,210 iteration 479 : loss : 0.165263, loss_ce: 0.071225
2022-01-20 19:44:25,542 iteration 480 : loss : 0.126222, loss_ce: 0.060258
2022-01-20 19:44:26,775 iteration 481 : loss : 0.172616, loss_ce: 0.070614
2022-01-20 19:44:28,101 iteration 482 : loss : 0.162297, loss_ce: 0.044641
2022-01-20 19:44:29,436 iteration 483 : loss : 0.191129, loss_ce: 0.066384
2022-01-20 19:44:30,785 iteration 484 : loss : 0.174645, loss_ce: 0.077559
2022-01-20 19:44:32,072 iteration 485 : loss : 0.124654, loss_ce: 0.052434
2022-01-20 19:44:33,354 iteration 486 : loss : 0.179668, loss_ce: 0.058206
2022-01-20 19:44:34,713 iteration 487 : loss : 0.126640, loss_ce: 0.051341
2022-01-20 19:44:35,971 iteration 488 : loss : 0.146801, loss_ce: 0.056685
2022-01-20 19:44:37,298 iteration 489 : loss : 0.131887, loss_ce: 0.054671
2022-01-20 19:44:38,517 iteration 490 : loss : 0.155612, loss_ce: 0.060547
2022-01-20 19:44:39,846 iteration 491 : loss : 0.149580, loss_ce: 0.065555
2022-01-20 19:44:41,153 iteration 492 : loss : 0.126645, loss_ce: 0.061636
2022-01-20 19:44:42,503 iteration 493 : loss : 0.149919, loss_ce: 0.074721
  7%|██▏                           | 29/400 [11:52<2:25:21, 23.51s/it]2022-01-20 19:44:43,857 iteration 494 : loss : 0.155581, loss_ce: 0.072021
2022-01-20 19:44:45,215 iteration 495 : loss : 0.165715, loss_ce: 0.066380
2022-01-20 19:44:46,521 iteration 496 : loss : 0.172067, loss_ce: 0.082629
2022-01-20 19:44:47,865 iteration 497 : loss : 0.155925, loss_ce: 0.074795
2022-01-20 19:44:49,238 iteration 498 : loss : 0.153558, loss_ce: 0.071444
2022-01-20 19:44:50,478 iteration 499 : loss : 0.114947, loss_ce: 0.051148
2022-01-20 19:44:51,732 iteration 500 : loss : 0.136704, loss_ce: 0.054966
2022-01-20 19:44:52,997 iteration 501 : loss : 0.154732, loss_ce: 0.062983
2022-01-20 19:44:54,369 iteration 502 : loss : 0.134641, loss_ce: 0.049032
2022-01-20 19:44:55,648 iteration 503 : loss : 0.169912, loss_ce: 0.052963
2022-01-20 19:44:56,996 iteration 504 : loss : 0.116670, loss_ce: 0.039423
2022-01-20 19:44:58,254 iteration 505 : loss : 0.114359, loss_ce: 0.040233
2022-01-20 19:44:59,638 iteration 506 : loss : 0.117198, loss_ce: 0.041552
2022-01-20 19:45:00,952 iteration 507 : loss : 0.199249, loss_ce: 0.078747
2022-01-20 19:45:02,274 iteration 508 : loss : 0.171890, loss_ce: 0.076825
2022-01-20 19:45:03,704 iteration 509 : loss : 0.121145, loss_ce: 0.044224
2022-01-20 19:45:03,705 Training Data Eval:
2022-01-20 19:45:10,191   Average segmentation loss on training set: 0.1643
2022-01-20 19:45:10,191 Validation Data Eval:
2022-01-20 19:45:12,437   Average segmentation loss on validation set: 0.1589
2022-01-20 19:45:13,802 iteration 510 : loss : 0.134805, loss_ce: 0.057584
  8%|██▎                           | 30/400 [12:23<2:39:21, 25.84s/it]2022-01-20 19:45:15,178 iteration 511 : loss : 0.165369, loss_ce: 0.067366
2022-01-20 19:45:16,464 iteration 512 : loss : 0.154320, loss_ce: 0.062083
2022-01-20 19:45:17,761 iteration 513 : loss : 0.137057, loss_ce: 0.047406
2022-01-20 19:45:19,085 iteration 514 : loss : 0.121210, loss_ce: 0.052652
2022-01-20 19:45:20,381 iteration 515 : loss : 0.214045, loss_ce: 0.098069
2022-01-20 19:45:21,716 iteration 516 : loss : 0.143710, loss_ce: 0.057879
2022-01-20 19:45:23,088 iteration 517 : loss : 0.148863, loss_ce: 0.069533
2022-01-20 19:45:24,351 iteration 518 : loss : 0.142718, loss_ce: 0.050551
2022-01-20 19:45:25,662 iteration 519 : loss : 0.153870, loss_ce: 0.060941
2022-01-20 19:45:26,928 iteration 520 : loss : 0.112866, loss_ce: 0.045477
2022-01-20 19:45:28,248 iteration 521 : loss : 0.139982, loss_ce: 0.050555
2022-01-20 19:45:29,605 iteration 522 : loss : 0.120219, loss_ce: 0.042124
2022-01-20 19:45:30,988 iteration 523 : loss : 0.146231, loss_ce: 0.064252
2022-01-20 19:45:32,388 iteration 524 : loss : 0.134855, loss_ce: 0.054671
2022-01-20 19:45:33,699 iteration 525 : loss : 0.166864, loss_ce: 0.081623
2022-01-20 19:45:34,994 iteration 526 : loss : 0.154264, loss_ce: 0.071180
2022-01-20 19:45:36,303 iteration 527 : loss : 0.133363, loss_ce: 0.043937
  8%|██▎                           | 31/400 [12:46<2:32:46, 24.84s/it]2022-01-20 19:45:37,632 iteration 528 : loss : 0.132014, loss_ce: 0.053649
2022-01-20 19:45:38,961 iteration 529 : loss : 0.142957, loss_ce: 0.040023
2022-01-20 19:45:40,223 iteration 530 : loss : 0.147641, loss_ce: 0.054335
2022-01-20 19:45:41,470 iteration 531 : loss : 0.125422, loss_ce: 0.041116
2022-01-20 19:45:42,745 iteration 532 : loss : 0.129681, loss_ce: 0.055444
2022-01-20 19:45:44,153 iteration 533 : loss : 0.092037, loss_ce: 0.035605
2022-01-20 19:45:45,422 iteration 534 : loss : 0.099957, loss_ce: 0.045157
2022-01-20 19:45:46,777 iteration 535 : loss : 0.110898, loss_ce: 0.043574
2022-01-20 19:45:48,155 iteration 536 : loss : 0.182181, loss_ce: 0.084099
2022-01-20 19:45:49,547 iteration 537 : loss : 0.155713, loss_ce: 0.052363
2022-01-20 19:45:50,870 iteration 538 : loss : 0.128006, loss_ce: 0.060715
2022-01-20 19:45:52,158 iteration 539 : loss : 0.135058, loss_ce: 0.055287
2022-01-20 19:45:53,500 iteration 540 : loss : 0.110604, loss_ce: 0.039301
2022-01-20 19:45:54,796 iteration 541 : loss : 0.109122, loss_ce: 0.055131
2022-01-20 19:45:56,110 iteration 542 : loss : 0.166594, loss_ce: 0.077732
2022-01-20 19:45:57,373 iteration 543 : loss : 0.107232, loss_ce: 0.043258
2022-01-20 19:45:58,661 iteration 544 : loss : 0.125776, loss_ce: 0.048361
  8%|██▍                           | 32/400 [13:08<2:27:47, 24.10s/it]2022-01-20 19:46:00,037 iteration 545 : loss : 0.091415, loss_ce: 0.041577
2022-01-20 19:46:01,345 iteration 546 : loss : 0.111017, loss_ce: 0.039943
2022-01-20 19:46:02,653 iteration 547 : loss : 0.123012, loss_ce: 0.048366
2022-01-20 19:46:03,954 iteration 548 : loss : 0.220369, loss_ce: 0.074905
2022-01-20 19:46:05,294 iteration 549 : loss : 0.097697, loss_ce: 0.040813
2022-01-20 19:46:06,583 iteration 550 : loss : 0.127883, loss_ce: 0.053700
2022-01-20 19:46:07,888 iteration 551 : loss : 0.108267, loss_ce: 0.049677
2022-01-20 19:46:09,159 iteration 552 : loss : 0.118492, loss_ce: 0.044248
2022-01-20 19:46:10,498 iteration 553 : loss : 0.149095, loss_ce: 0.056859
2022-01-20 19:46:11,755 iteration 554 : loss : 0.127265, loss_ce: 0.059354
2022-01-20 19:46:13,043 iteration 555 : loss : 0.122535, loss_ce: 0.037083
2022-01-20 19:46:14,390 iteration 556 : loss : 0.127099, loss_ce: 0.059315
2022-01-20 19:46:15,628 iteration 557 : loss : 0.154182, loss_ce: 0.054825
2022-01-20 19:46:16,969 iteration 558 : loss : 0.153917, loss_ce: 0.044959
2022-01-20 19:46:18,279 iteration 559 : loss : 0.152841, loss_ce: 0.059756
2022-01-20 19:46:19,596 iteration 560 : loss : 0.097336, loss_ce: 0.026025
2022-01-20 19:46:20,943 iteration 561 : loss : 0.170386, loss_ce: 0.080553
  8%|██▍                           | 33/400 [13:30<2:24:04, 23.55s/it]2022-01-20 19:46:22,387 iteration 562 : loss : 0.168825, loss_ce: 0.083913
2022-01-20 19:46:23,600 iteration 563 : loss : 0.133351, loss_ce: 0.054943
2022-01-20 19:46:24,933 iteration 564 : loss : 0.127028, loss_ce: 0.065845
2022-01-20 19:46:26,266 iteration 565 : loss : 0.133794, loss_ce: 0.056417
2022-01-20 19:46:27,621 iteration 566 : loss : 0.108994, loss_ce: 0.052335
2022-01-20 19:46:28,910 iteration 567 : loss : 0.118621, loss_ce: 0.040058
2022-01-20 19:46:30,200 iteration 568 : loss : 0.194611, loss_ce: 0.068332
2022-01-20 19:46:31,511 iteration 569 : loss : 0.150042, loss_ce: 0.062474
2022-01-20 19:46:32,854 iteration 570 : loss : 0.099327, loss_ce: 0.045524
2022-01-20 19:46:34,136 iteration 571 : loss : 0.238142, loss_ce: 0.104212
2022-01-20 19:46:35,461 iteration 572 : loss : 0.131725, loss_ce: 0.048823
2022-01-20 19:46:36,729 iteration 573 : loss : 0.188033, loss_ce: 0.092819
2022-01-20 19:46:38,038 iteration 574 : loss : 0.200258, loss_ce: 0.078304
2022-01-20 19:46:39,412 iteration 575 : loss : 0.125260, loss_ce: 0.042125
2022-01-20 19:46:40,769 iteration 576 : loss : 0.099780, loss_ce: 0.045460
2022-01-20 19:46:42,079 iteration 577 : loss : 0.103112, loss_ce: 0.040304
2022-01-20 19:46:43,339 iteration 578 : loss : 0.135504, loss_ce: 0.050816
  8%|██▌                           | 34/400 [13:53<2:21:32, 23.20s/it]2022-01-20 19:46:44,856 iteration 579 : loss : 0.141526, loss_ce: 0.057374
2022-01-20 19:46:46,182 iteration 580 : loss : 0.141687, loss_ce: 0.055235
2022-01-20 19:46:47,547 iteration 581 : loss : 0.141829, loss_ce: 0.049850
2022-01-20 19:46:48,849 iteration 582 : loss : 0.123860, loss_ce: 0.047944
2022-01-20 19:46:50,162 iteration 583 : loss : 0.212077, loss_ce: 0.064400
2022-01-20 19:46:51,437 iteration 584 : loss : 0.111578, loss_ce: 0.046538
2022-01-20 19:46:52,803 iteration 585 : loss : 0.133355, loss_ce: 0.048100
2022-01-20 19:46:54,079 iteration 586 : loss : 0.170273, loss_ce: 0.061942
2022-01-20 19:46:55,342 iteration 587 : loss : 0.112397, loss_ce: 0.048193
2022-01-20 19:46:56,669 iteration 588 : loss : 0.147726, loss_ce: 0.050317
2022-01-20 19:46:57,972 iteration 589 : loss : 0.114304, loss_ce: 0.041042
2022-01-20 19:46:59,296 iteration 590 : loss : 0.121582, loss_ce: 0.043925
2022-01-20 19:47:00,614 iteration 591 : loss : 0.120026, loss_ce: 0.048472
2022-01-20 19:47:01,921 iteration 592 : loss : 0.113657, loss_ce: 0.045787
2022-01-20 19:47:03,255 iteration 593 : loss : 0.085271, loss_ce: 0.030394
2022-01-20 19:47:04,576 iteration 594 : loss : 0.108695, loss_ce: 0.046400
2022-01-20 19:47:04,577 Training Data Eval:
2022-01-20 19:47:11,097   Average segmentation loss on training set: 0.1738
2022-01-20 19:47:11,097 Validation Data Eval:
2022-01-20 19:47:13,341   Average segmentation loss on validation set: 0.1894
2022-01-20 19:47:14,737 iteration 595 : loss : 0.160165, loss_ce: 0.081891
  9%|██▋                           | 35/400 [14:24<2:36:06, 25.66s/it]2022-01-20 19:47:16,094 iteration 596 : loss : 0.107524, loss_ce: 0.046456
2022-01-20 19:47:17,377 iteration 597 : loss : 0.130540, loss_ce: 0.043869
2022-01-20 19:47:18,658 iteration 598 : loss : 0.101355, loss_ce: 0.040408
2022-01-20 19:47:19,959 iteration 599 : loss : 0.121571, loss_ce: 0.044811
2022-01-20 19:47:21,241 iteration 600 : loss : 0.160231, loss_ce: 0.068760
2022-01-20 19:47:22,562 iteration 601 : loss : 0.112059, loss_ce: 0.042904
2022-01-20 19:47:23,896 iteration 602 : loss : 0.105287, loss_ce: 0.040175
2022-01-20 19:47:25,148 iteration 603 : loss : 0.113989, loss_ce: 0.039528
2022-01-20 19:47:26,421 iteration 604 : loss : 0.122103, loss_ce: 0.043475
2022-01-20 19:47:27,682 iteration 605 : loss : 0.122794, loss_ce: 0.044773
2022-01-20 19:47:29,114 iteration 606 : loss : 0.109014, loss_ce: 0.050510
2022-01-20 19:47:30,411 iteration 607 : loss : 0.178539, loss_ce: 0.071397
2022-01-20 19:47:31,714 iteration 608 : loss : 0.128562, loss_ce: 0.060285
2022-01-20 19:47:32,944 iteration 609 : loss : 0.098191, loss_ce: 0.041219
2022-01-20 19:47:34,302 iteration 610 : loss : 0.100821, loss_ce: 0.042640
2022-01-20 19:47:35,696 iteration 611 : loss : 0.150965, loss_ce: 0.064679
2022-01-20 19:47:37,015 iteration 612 : loss : 0.129475, loss_ce: 0.045968
  9%|██▋                           | 36/400 [14:46<2:29:32, 24.65s/it]2022-01-20 19:47:38,385 iteration 613 : loss : 0.105772, loss_ce: 0.041493
2022-01-20 19:47:39,658 iteration 614 : loss : 0.131004, loss_ce: 0.042849
2022-01-20 19:47:40,927 iteration 615 : loss : 0.158920, loss_ce: 0.074490
2022-01-20 19:47:42,313 iteration 616 : loss : 0.103149, loss_ce: 0.045627
2022-01-20 19:47:43,583 iteration 617 : loss : 0.101222, loss_ce: 0.048947
2022-01-20 19:47:44,912 iteration 618 : loss : 0.175855, loss_ce: 0.049182
2022-01-20 19:47:46,237 iteration 619 : loss : 0.196747, loss_ce: 0.081834
2022-01-20 19:47:47,653 iteration 620 : loss : 0.210967, loss_ce: 0.066834
2022-01-20 19:47:48,959 iteration 621 : loss : 0.088489, loss_ce: 0.031762
2022-01-20 19:47:50,383 iteration 622 : loss : 0.101053, loss_ce: 0.035663
2022-01-20 19:47:51,777 iteration 623 : loss : 0.122972, loss_ce: 0.057227
2022-01-20 19:47:53,067 iteration 624 : loss : 0.109134, loss_ce: 0.035884
2022-01-20 19:47:54,334 iteration 625 : loss : 0.101949, loss_ce: 0.043383
2022-01-20 19:47:55,684 iteration 626 : loss : 0.090261, loss_ce: 0.035150
2022-01-20 19:47:57,040 iteration 627 : loss : 0.119181, loss_ce: 0.046443
2022-01-20 19:47:58,433 iteration 628 : loss : 0.110351, loss_ce: 0.041694
2022-01-20 19:47:59,714 iteration 629 : loss : 0.088657, loss_ce: 0.039574
  9%|██▊                           | 37/400 [15:09<2:25:34, 24.06s/it]2022-01-20 19:48:01,081 iteration 630 : loss : 0.097994, loss_ce: 0.042941
2022-01-20 19:48:02,367 iteration 631 : loss : 0.069182, loss_ce: 0.031850
2022-01-20 19:48:03,650 iteration 632 : loss : 0.169963, loss_ce: 0.071741
2022-01-20 19:48:05,032 iteration 633 : loss : 0.102127, loss_ce: 0.037597
2022-01-20 19:48:06,345 iteration 634 : loss : 0.185816, loss_ce: 0.065609
2022-01-20 19:48:07,751 iteration 635 : loss : 0.130940, loss_ce: 0.052225
2022-01-20 19:48:09,089 iteration 636 : loss : 0.109073, loss_ce: 0.043429
2022-01-20 19:48:10,335 iteration 637 : loss : 0.114515, loss_ce: 0.045505
2022-01-20 19:48:11,708 iteration 638 : loss : 0.186360, loss_ce: 0.077506
2022-01-20 19:48:12,975 iteration 639 : loss : 0.119976, loss_ce: 0.050664
2022-01-20 19:48:14,214 iteration 640 : loss : 0.148908, loss_ce: 0.045836
2022-01-20 19:48:15,469 iteration 641 : loss : 0.119743, loss_ce: 0.044524
2022-01-20 19:48:16,733 iteration 642 : loss : 0.114761, loss_ce: 0.031937
2022-01-20 19:48:18,034 iteration 643 : loss : 0.106197, loss_ce: 0.045553
2022-01-20 19:48:19,338 iteration 644 : loss : 0.102551, loss_ce: 0.041282
2022-01-20 19:48:20,615 iteration 645 : loss : 0.115958, loss_ce: 0.034949
2022-01-20 19:48:21,996 iteration 646 : loss : 0.099244, loss_ce: 0.052868
 10%|██▊                           | 38/400 [15:31<2:21:57, 23.53s/it]2022-01-20 19:48:23,325 iteration 647 : loss : 0.133628, loss_ce: 0.057250
2022-01-20 19:48:24,646 iteration 648 : loss : 0.115102, loss_ce: 0.051727
2022-01-20 19:48:25,989 iteration 649 : loss : 0.106672, loss_ce: 0.046615
2022-01-20 19:48:27,374 iteration 650 : loss : 0.150879, loss_ce: 0.060066
2022-01-20 19:48:28,680 iteration 651 : loss : 0.087460, loss_ce: 0.032904
2022-01-20 19:48:29,894 iteration 652 : loss : 0.078645, loss_ce: 0.029328
2022-01-20 19:48:31,276 iteration 653 : loss : 0.110333, loss_ce: 0.044619
2022-01-20 19:48:32,536 iteration 654 : loss : 0.103252, loss_ce: 0.038680
2022-01-20 19:48:33,842 iteration 655 : loss : 0.146233, loss_ce: 0.046629
2022-01-20 19:48:35,156 iteration 656 : loss : 0.103807, loss_ce: 0.034440
2022-01-20 19:48:36,526 iteration 657 : loss : 0.073923, loss_ce: 0.025188
2022-01-20 19:48:37,854 iteration 658 : loss : 0.121206, loss_ce: 0.035978
2022-01-20 19:48:39,200 iteration 659 : loss : 0.149780, loss_ce: 0.051598
2022-01-20 19:48:40,484 iteration 660 : loss : 0.090943, loss_ce: 0.040988
2022-01-20 19:48:41,816 iteration 661 : loss : 0.106984, loss_ce: 0.043351
2022-01-20 19:48:43,221 iteration 662 : loss : 0.170391, loss_ce: 0.048034
2022-01-20 19:48:44,530 iteration 663 : loss : 0.107240, loss_ce: 0.041810
 10%|██▉                           | 39/400 [15:54<2:19:45, 23.23s/it]2022-01-20 19:48:45,903 iteration 664 : loss : 0.094751, loss_ce: 0.042196
2022-01-20 19:48:47,146 iteration 665 : loss : 0.100505, loss_ce: 0.047655
2022-01-20 19:48:48,443 iteration 666 : loss : 0.079741, loss_ce: 0.027935
2022-01-20 19:48:49,784 iteration 667 : loss : 0.072031, loss_ce: 0.032840
2022-01-20 19:48:51,091 iteration 668 : loss : 0.134901, loss_ce: 0.055598
2022-01-20 19:48:52,375 iteration 669 : loss : 0.122428, loss_ce: 0.030734
2022-01-20 19:48:53,798 iteration 670 : loss : 0.090895, loss_ce: 0.033566
2022-01-20 19:48:55,089 iteration 671 : loss : 0.110967, loss_ce: 0.037392
2022-01-20 19:48:56,459 iteration 672 : loss : 0.107901, loss_ce: 0.039394
2022-01-20 19:48:57,809 iteration 673 : loss : 0.110300, loss_ce: 0.048475
2022-01-20 19:48:59,139 iteration 674 : loss : 0.085683, loss_ce: 0.040198
2022-01-20 19:49:00,497 iteration 675 : loss : 0.142045, loss_ce: 0.059799
2022-01-20 19:49:01,795 iteration 676 : loss : 0.104606, loss_ce: 0.040524
2022-01-20 19:49:03,115 iteration 677 : loss : 0.087552, loss_ce: 0.044283
2022-01-20 19:49:04,453 iteration 678 : loss : 0.158625, loss_ce: 0.060755
2022-01-20 19:49:05,770 iteration 679 : loss : 0.091971, loss_ce: 0.041582
2022-01-20 19:49:05,770 Training Data Eval:
2022-01-20 19:49:12,274   Average segmentation loss on training set: 0.1239
2022-01-20 19:49:12,274 Validation Data Eval:
2022-01-20 19:49:14,515   Average segmentation loss on validation set: 0.1551
2022-01-20 19:49:20,270 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 19:49:21,529 iteration 680 : loss : 0.089117, loss_ce: 0.032691
 10%|███                           | 40/400 [16:31<2:44:10, 27.36s/it]2022-01-20 19:49:22,834 iteration 681 : loss : 0.081487, loss_ce: 0.031499
2022-01-20 19:49:23,997 iteration 682 : loss : 0.141834, loss_ce: 0.047820
2022-01-20 19:49:25,233 iteration 683 : loss : 0.110958, loss_ce: 0.053326
2022-01-20 19:49:26,434 iteration 684 : loss : 0.113559, loss_ce: 0.040913
2022-01-20 19:49:27,745 iteration 685 : loss : 0.094162, loss_ce: 0.033574
2022-01-20 19:49:28,999 iteration 686 : loss : 0.166241, loss_ce: 0.060073
2022-01-20 19:49:30,243 iteration 687 : loss : 0.097663, loss_ce: 0.042669
2022-01-20 19:49:31,458 iteration 688 : loss : 0.117626, loss_ce: 0.037621
2022-01-20 19:49:32,722 iteration 689 : loss : 0.092613, loss_ce: 0.036724
2022-01-20 19:49:33,951 iteration 690 : loss : 0.106210, loss_ce: 0.039107
2022-01-20 19:49:35,196 iteration 691 : loss : 0.102175, loss_ce: 0.043144
2022-01-20 19:49:36,491 iteration 692 : loss : 0.072380, loss_ce: 0.027321
2022-01-20 19:49:37,672 iteration 693 : loss : 0.113899, loss_ce: 0.053143
2022-01-20 19:49:38,923 iteration 694 : loss : 0.116947, loss_ce: 0.048282
2022-01-20 19:49:40,140 iteration 695 : loss : 0.131529, loss_ce: 0.057739
2022-01-20 19:49:41,494 iteration 696 : loss : 0.087202, loss_ce: 0.035785
2022-01-20 19:49:42,810 iteration 697 : loss : 0.099092, loss_ce: 0.039268
 10%|███                           | 41/400 [16:52<2:32:48, 25.54s/it]2022-01-20 19:49:44,154 iteration 698 : loss : 0.140338, loss_ce: 0.046843
2022-01-20 19:49:45,526 iteration 699 : loss : 0.104127, loss_ce: 0.044288
2022-01-20 19:49:46,835 iteration 700 : loss : 0.111764, loss_ce: 0.054628
2022-01-20 19:49:48,127 iteration 701 : loss : 0.119466, loss_ce: 0.060086
2022-01-20 19:49:49,489 iteration 702 : loss : 0.122923, loss_ce: 0.043694
2022-01-20 19:49:50,892 iteration 703 : loss : 0.071176, loss_ce: 0.030229
2022-01-20 19:49:52,146 iteration 704 : loss : 0.072870, loss_ce: 0.028971
2022-01-20 19:49:53,462 iteration 705 : loss : 0.088068, loss_ce: 0.031435
2022-01-20 19:49:54,787 iteration 706 : loss : 0.090050, loss_ce: 0.031996
2022-01-20 19:49:56,033 iteration 707 : loss : 0.069809, loss_ce: 0.028878
2022-01-20 19:49:57,341 iteration 708 : loss : 0.066528, loss_ce: 0.024618
2022-01-20 19:49:58,659 iteration 709 : loss : 0.131125, loss_ce: 0.063021
2022-01-20 19:50:00,010 iteration 710 : loss : 0.104639, loss_ce: 0.045643
2022-01-20 19:50:01,353 iteration 711 : loss : 0.113362, loss_ce: 0.050937
2022-01-20 19:50:02,749 iteration 712 : loss : 0.113338, loss_ce: 0.044953
2022-01-20 19:50:04,044 iteration 713 : loss : 0.078181, loss_ce: 0.035373
2022-01-20 19:50:05,419 iteration 714 : loss : 0.114045, loss_ce: 0.041242
 10%|███▏                          | 42/400 [17:15<2:27:08, 24.66s/it]2022-01-20 19:50:06,815 iteration 715 : loss : 0.093097, loss_ce: 0.034068
2022-01-20 19:50:08,180 iteration 716 : loss : 0.077774, loss_ce: 0.032244
2022-01-20 19:50:09,493 iteration 717 : loss : 0.096776, loss_ce: 0.032066
2022-01-20 19:50:10,813 iteration 718 : loss : 0.109547, loss_ce: 0.052958
2022-01-20 19:50:12,180 iteration 719 : loss : 0.100974, loss_ce: 0.038048
2022-01-20 19:50:13,486 iteration 720 : loss : 0.097397, loss_ce: 0.034956
2022-01-20 19:50:14,792 iteration 721 : loss : 0.111296, loss_ce: 0.037996
2022-01-20 19:50:16,083 iteration 722 : loss : 0.118555, loss_ce: 0.037783
2022-01-20 19:50:17,428 iteration 723 : loss : 0.127307, loss_ce: 0.062749
2022-01-20 19:50:18,719 iteration 724 : loss : 0.138119, loss_ce: 0.064747
2022-01-20 19:50:20,090 iteration 725 : loss : 0.125476, loss_ce: 0.042852
2022-01-20 19:50:21,390 iteration 726 : loss : 0.120005, loss_ce: 0.048751
2022-01-20 19:50:22,640 iteration 727 : loss : 0.084281, loss_ce: 0.034100
2022-01-20 19:50:23,989 iteration 728 : loss : 0.089593, loss_ce: 0.040613
2022-01-20 19:50:25,348 iteration 729 : loss : 0.068770, loss_ce: 0.030967
2022-01-20 19:50:26,687 iteration 730 : loss : 0.093840, loss_ce: 0.035621
2022-01-20 19:50:27,931 iteration 731 : loss : 0.072064, loss_ce: 0.023311
 11%|███▏                          | 43/400 [17:37<2:22:53, 24.02s/it]2022-01-20 19:50:29,236 iteration 732 : loss : 0.091265, loss_ce: 0.040948
2022-01-20 19:50:30,472 iteration 733 : loss : 0.112773, loss_ce: 0.058596
2022-01-20 19:50:31,828 iteration 734 : loss : 0.086049, loss_ce: 0.036258
2022-01-20 19:50:33,127 iteration 735 : loss : 0.101879, loss_ce: 0.038861
2022-01-20 19:50:34,417 iteration 736 : loss : 0.090748, loss_ce: 0.041359
2022-01-20 19:50:35,833 iteration 737 : loss : 0.112994, loss_ce: 0.050233
2022-01-20 19:50:37,065 iteration 738 : loss : 0.082910, loss_ce: 0.038250
2022-01-20 19:50:38,415 iteration 739 : loss : 0.101773, loss_ce: 0.037397
2022-01-20 19:50:39,706 iteration 740 : loss : 0.088599, loss_ce: 0.037187
2022-01-20 19:50:40,981 iteration 741 : loss : 0.089657, loss_ce: 0.035823
2022-01-20 19:50:42,211 iteration 742 : loss : 0.114932, loss_ce: 0.054329
2022-01-20 19:50:43,514 iteration 743 : loss : 0.072183, loss_ce: 0.028807
2022-01-20 19:50:44,847 iteration 744 : loss : 0.130017, loss_ce: 0.057012
2022-01-20 19:50:46,123 iteration 745 : loss : 0.120779, loss_ce: 0.025514
2022-01-20 19:50:47,411 iteration 746 : loss : 0.088100, loss_ce: 0.033683
2022-01-20 19:50:48,681 iteration 747 : loss : 0.092445, loss_ce: 0.039032
2022-01-20 19:50:49,949 iteration 748 : loss : 0.065663, loss_ce: 0.029345
 11%|███▎                          | 44/400 [17:59<2:18:56, 23.42s/it]2022-01-20 19:50:51,251 iteration 749 : loss : 0.214362, loss_ce: 0.050169
2022-01-20 19:50:52,545 iteration 750 : loss : 0.090178, loss_ce: 0.032816
2022-01-20 19:50:53,866 iteration 751 : loss : 0.090457, loss_ce: 0.031464
2022-01-20 19:50:55,155 iteration 752 : loss : 0.145905, loss_ce: 0.069972
2022-01-20 19:50:56,412 iteration 753 : loss : 0.122355, loss_ce: 0.050311
2022-01-20 19:50:57,693 iteration 754 : loss : 0.148723, loss_ce: 0.046574
2022-01-20 19:50:58,925 iteration 755 : loss : 0.136303, loss_ce: 0.070710
2022-01-20 19:51:00,166 iteration 756 : loss : 0.077479, loss_ce: 0.025721
2022-01-20 19:51:01,472 iteration 757 : loss : 0.135385, loss_ce: 0.063416
2022-01-20 19:51:02,873 iteration 758 : loss : 0.108501, loss_ce: 0.044798
2022-01-20 19:51:04,323 iteration 759 : loss : 0.142234, loss_ce: 0.050951
2022-01-20 19:51:05,615 iteration 760 : loss : 0.115299, loss_ce: 0.067845
2022-01-20 19:51:06,948 iteration 761 : loss : 0.108056, loss_ce: 0.043559
2022-01-20 19:51:08,299 iteration 762 : loss : 0.114038, loss_ce: 0.047287
2022-01-20 19:51:09,647 iteration 763 : loss : 0.126280, loss_ce: 0.065115
2022-01-20 19:51:10,924 iteration 764 : loss : 0.119704, loss_ce: 0.055206
2022-01-20 19:51:10,925 Training Data Eval:
2022-01-20 19:51:17,419   Average segmentation loss on training set: 0.2651
2022-01-20 19:51:17,419 Validation Data Eval:
2022-01-20 19:51:19,660   Average segmentation loss on validation set: 0.2426
2022-01-20 19:51:21,136 iteration 765 : loss : 0.121404, loss_ce: 0.049171
 11%|███▍                          | 45/400 [18:30<2:32:20, 25.75s/it]2022-01-20 19:51:22,541 iteration 766 : loss : 0.117168, loss_ce: 0.049582
2022-01-20 19:51:23,887 iteration 767 : loss : 0.102448, loss_ce: 0.038997
2022-01-20 19:51:25,217 iteration 768 : loss : 0.137078, loss_ce: 0.050077
2022-01-20 19:51:26,557 iteration 769 : loss : 0.108888, loss_ce: 0.049213
2022-01-20 19:51:27,790 iteration 770 : loss : 0.090516, loss_ce: 0.032861
2022-01-20 19:51:29,109 iteration 771 : loss : 0.148931, loss_ce: 0.036858
2022-01-20 19:51:30,441 iteration 772 : loss : 0.070460, loss_ce: 0.025899
2022-01-20 19:51:31,778 iteration 773 : loss : 0.098126, loss_ce: 0.023627
2022-01-20 19:51:33,187 iteration 774 : loss : 0.111562, loss_ce: 0.050149
2022-01-20 19:51:34,476 iteration 775 : loss : 0.147231, loss_ce: 0.081908
2022-01-20 19:51:35,744 iteration 776 : loss : 0.129872, loss_ce: 0.054696
2022-01-20 19:51:37,217 iteration 777 : loss : 0.115257, loss_ce: 0.049220
2022-01-20 19:51:38,448 iteration 778 : loss : 0.137607, loss_ce: 0.048047
2022-01-20 19:51:39,815 iteration 779 : loss : 0.108469, loss_ce: 0.050790
2022-01-20 19:51:41,181 iteration 780 : loss : 0.100216, loss_ce: 0.037369
2022-01-20 19:51:42,453 iteration 781 : loss : 0.088605, loss_ce: 0.043145
2022-01-20 19:51:43,682 iteration 782 : loss : 0.082066, loss_ce: 0.042303
 12%|███▍                          | 46/400 [18:53<2:26:14, 24.79s/it]2022-01-20 19:51:45,007 iteration 783 : loss : 0.108091, loss_ce: 0.050371
2022-01-20 19:51:46,279 iteration 784 : loss : 0.101778, loss_ce: 0.037914
2022-01-20 19:51:47,606 iteration 785 : loss : 0.109706, loss_ce: 0.045346
2022-01-20 19:51:48,881 iteration 786 : loss : 0.115851, loss_ce: 0.041240
2022-01-20 19:51:50,167 iteration 787 : loss : 0.085701, loss_ce: 0.038413
2022-01-20 19:51:51,530 iteration 788 : loss : 0.086162, loss_ce: 0.039252
2022-01-20 19:51:52,756 iteration 789 : loss : 0.105416, loss_ce: 0.038394
2022-01-20 19:51:54,111 iteration 790 : loss : 0.098665, loss_ce: 0.034371
2022-01-20 19:51:55,416 iteration 791 : loss : 0.091827, loss_ce: 0.039562
2022-01-20 19:51:56,675 iteration 792 : loss : 0.091466, loss_ce: 0.040805
2022-01-20 19:51:58,049 iteration 793 : loss : 0.099835, loss_ce: 0.050908
2022-01-20 19:51:59,279 iteration 794 : loss : 0.057270, loss_ce: 0.025281
2022-01-20 19:52:00,533 iteration 795 : loss : 0.073488, loss_ce: 0.030704
2022-01-20 19:52:01,785 iteration 796 : loss : 0.093232, loss_ce: 0.044454
2022-01-20 19:52:03,139 iteration 797 : loss : 0.128981, loss_ce: 0.053474
2022-01-20 19:52:04,448 iteration 798 : loss : 0.098588, loss_ce: 0.037746
2022-01-20 19:52:05,696 iteration 799 : loss : 0.075492, loss_ce: 0.038011
 12%|███▌                          | 47/400 [19:15<2:20:56, 23.96s/it]2022-01-20 19:52:07,148 iteration 800 : loss : 0.084924, loss_ce: 0.031998
2022-01-20 19:52:08,487 iteration 801 : loss : 0.122138, loss_ce: 0.055327
2022-01-20 19:52:09,795 iteration 802 : loss : 0.108466, loss_ce: 0.041574
2022-01-20 19:52:11,125 iteration 803 : loss : 0.086971, loss_ce: 0.045881
2022-01-20 19:52:12,525 iteration 804 : loss : 0.128872, loss_ce: 0.058764
2022-01-20 19:52:13,979 iteration 805 : loss : 0.076603, loss_ce: 0.029127
2022-01-20 19:52:15,256 iteration 806 : loss : 0.091851, loss_ce: 0.042326
2022-01-20 19:52:16,501 iteration 807 : loss : 0.087738, loss_ce: 0.035926
2022-01-20 19:52:17,833 iteration 808 : loss : 0.108658, loss_ce: 0.053208
2022-01-20 19:52:19,140 iteration 809 : loss : 0.092851, loss_ce: 0.033237
2022-01-20 19:52:20,412 iteration 810 : loss : 0.155538, loss_ce: 0.058434
2022-01-20 19:52:21,657 iteration 811 : loss : 0.126963, loss_ce: 0.065876
2022-01-20 19:52:22,929 iteration 812 : loss : 0.089896, loss_ce: 0.038350
2022-01-20 19:52:24,228 iteration 813 : loss : 0.130217, loss_ce: 0.047280
2022-01-20 19:52:25,640 iteration 814 : loss : 0.130075, loss_ce: 0.044883
2022-01-20 19:52:26,997 iteration 815 : loss : 0.121548, loss_ce: 0.049179
2022-01-20 19:52:28,297 iteration 816 : loss : 0.076874, loss_ce: 0.026931
 12%|███▌                          | 48/400 [19:38<2:18:08, 23.55s/it]2022-01-20 19:52:29,613 iteration 817 : loss : 0.099027, loss_ce: 0.043061
2022-01-20 19:52:30,858 iteration 818 : loss : 0.088744, loss_ce: 0.035521
2022-01-20 19:52:32,154 iteration 819 : loss : 0.089276, loss_ce: 0.029647
2022-01-20 19:52:33,396 iteration 820 : loss : 0.077568, loss_ce: 0.029107
2022-01-20 19:52:34,765 iteration 821 : loss : 0.068896, loss_ce: 0.024009
2022-01-20 19:52:36,108 iteration 822 : loss : 0.129113, loss_ce: 0.042851
2022-01-20 19:52:37,360 iteration 823 : loss : 0.068568, loss_ce: 0.027300
2022-01-20 19:52:38,617 iteration 824 : loss : 0.089275, loss_ce: 0.040263
2022-01-20 19:52:39,922 iteration 825 : loss : 0.069583, loss_ce: 0.028095
2022-01-20 19:52:41,226 iteration 826 : loss : 0.136112, loss_ce: 0.041540
2022-01-20 19:52:42,517 iteration 827 : loss : 0.154794, loss_ce: 0.057248
2022-01-20 19:52:43,805 iteration 828 : loss : 0.074098, loss_ce: 0.030576
2022-01-20 19:52:45,131 iteration 829 : loss : 0.090034, loss_ce: 0.033852
2022-01-20 19:52:46,490 iteration 830 : loss : 0.084292, loss_ce: 0.034498
2022-01-20 19:52:47,796 iteration 831 : loss : 0.104393, loss_ce: 0.052145
2022-01-20 19:52:49,077 iteration 832 : loss : 0.062573, loss_ce: 0.021789
2022-01-20 19:52:50,422 iteration 833 : loss : 0.083521, loss_ce: 0.043718
 12%|███▋                          | 49/400 [20:00<2:15:15, 23.12s/it]2022-01-20 19:52:51,754 iteration 834 : loss : 0.089586, loss_ce: 0.036253
2022-01-20 19:52:52,970 iteration 835 : loss : 0.111452, loss_ce: 0.031154
2022-01-20 19:52:54,279 iteration 836 : loss : 0.071180, loss_ce: 0.034327
2022-01-20 19:52:55,529 iteration 837 : loss : 0.075110, loss_ce: 0.040988
2022-01-20 19:52:56,761 iteration 838 : loss : 0.080832, loss_ce: 0.035794
2022-01-20 19:52:58,084 iteration 839 : loss : 0.091198, loss_ce: 0.039489
2022-01-20 19:52:59,377 iteration 840 : loss : 0.087151, loss_ce: 0.031947
2022-01-20 19:53:00,634 iteration 841 : loss : 0.100830, loss_ce: 0.045006
2022-01-20 19:53:01,918 iteration 842 : loss : 0.076775, loss_ce: 0.030134
2022-01-20 19:53:03,300 iteration 843 : loss : 0.090977, loss_ce: 0.033516
2022-01-20 19:53:04,588 iteration 844 : loss : 0.090945, loss_ce: 0.030196
2022-01-20 19:53:05,914 iteration 845 : loss : 0.102282, loss_ce: 0.034132
2022-01-20 19:53:07,314 iteration 846 : loss : 0.078004, loss_ce: 0.035158
2022-01-20 19:53:08,613 iteration 847 : loss : 0.077430, loss_ce: 0.030239
2022-01-20 19:53:09,817 iteration 848 : loss : 0.073126, loss_ce: 0.033633
2022-01-20 19:53:11,151 iteration 849 : loss : 0.078382, loss_ce: 0.029102
2022-01-20 19:53:11,151 Training Data Eval:
2022-01-20 19:53:17,677   Average segmentation loss on training set: 0.0647
2022-01-20 19:53:17,677 Validation Data Eval:
2022-01-20 19:53:19,913   Average segmentation loss on validation set: 0.1081
2022-01-20 19:53:25,890 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 19:53:27,184 iteration 850 : loss : 0.117667, loss_ce: 0.041246
 12%|███▊                          | 50/400 [20:37<2:38:44, 27.21s/it]2022-01-20 19:53:28,482 iteration 851 : loss : 0.090612, loss_ce: 0.032751
2022-01-20 19:53:29,688 iteration 852 : loss : 0.064324, loss_ce: 0.026885
2022-01-20 19:53:30,989 iteration 853 : loss : 0.095783, loss_ce: 0.041017
2022-01-20 19:53:32,243 iteration 854 : loss : 0.073377, loss_ce: 0.022464
2022-01-20 19:53:33,499 iteration 855 : loss : 0.090713, loss_ce: 0.036644
2022-01-20 19:53:34,769 iteration 856 : loss : 0.070837, loss_ce: 0.026175
2022-01-20 19:53:35,972 iteration 857 : loss : 0.073660, loss_ce: 0.031469
2022-01-20 19:53:37,114 iteration 858 : loss : 0.080779, loss_ce: 0.041172
2022-01-20 19:53:38,330 iteration 859 : loss : 0.083592, loss_ce: 0.027686
2022-01-20 19:53:39,501 iteration 860 : loss : 0.087829, loss_ce: 0.029218
2022-01-20 19:53:40,793 iteration 861 : loss : 0.113884, loss_ce: 0.046069
2022-01-20 19:53:41,951 iteration 862 : loss : 0.094811, loss_ce: 0.031850
2022-01-20 19:53:43,265 iteration 863 : loss : 0.120156, loss_ce: 0.051463
2022-01-20 19:53:44,556 iteration 864 : loss : 0.107207, loss_ce: 0.039579
2022-01-20 19:53:45,763 iteration 865 : loss : 0.093816, loss_ce: 0.036893
2022-01-20 19:53:47,136 iteration 866 : loss : 0.102929, loss_ce: 0.049542
2022-01-20 19:53:48,397 iteration 867 : loss : 0.055859, loss_ce: 0.021936
 13%|███▊                          | 51/400 [20:58<2:27:49, 25.41s/it]2022-01-20 19:53:49,791 iteration 868 : loss : 0.132315, loss_ce: 0.044413
2022-01-20 19:53:51,114 iteration 869 : loss : 0.095750, loss_ce: 0.043586
2022-01-20 19:53:52,349 iteration 870 : loss : 0.072686, loss_ce: 0.030656
2022-01-20 19:53:53,619 iteration 871 : loss : 0.070609, loss_ce: 0.026757
2022-01-20 19:53:54,908 iteration 872 : loss : 0.064069, loss_ce: 0.031660
2022-01-20 19:53:56,293 iteration 873 : loss : 0.071927, loss_ce: 0.026191
2022-01-20 19:53:57,522 iteration 874 : loss : 0.056619, loss_ce: 0.025582
2022-01-20 19:53:58,970 iteration 875 : loss : 0.110877, loss_ce: 0.041386
2022-01-20 19:54:00,285 iteration 876 : loss : 0.056823, loss_ce: 0.025106
2022-01-20 19:54:01,503 iteration 877 : loss : 0.056531, loss_ce: 0.023111
2022-01-20 19:54:02,864 iteration 878 : loss : 0.082698, loss_ce: 0.036066
2022-01-20 19:54:04,160 iteration 879 : loss : 0.097382, loss_ce: 0.044898
2022-01-20 19:54:05,512 iteration 880 : loss : 0.078631, loss_ce: 0.032994
2022-01-20 19:54:06,791 iteration 881 : loss : 0.109454, loss_ce: 0.053852
2022-01-20 19:54:08,140 iteration 882 : loss : 0.081279, loss_ce: 0.027075
2022-01-20 19:54:09,434 iteration 883 : loss : 0.099419, loss_ce: 0.053598
2022-01-20 19:54:10,831 iteration 884 : loss : 0.088730, loss_ce: 0.040971
 13%|███▉                          | 52/400 [21:20<2:22:13, 24.52s/it]2022-01-20 19:54:12,199 iteration 885 : loss : 0.089160, loss_ce: 0.036029
2022-01-20 19:54:13,545 iteration 886 : loss : 0.076443, loss_ce: 0.028890
2022-01-20 19:54:14,894 iteration 887 : loss : 0.115996, loss_ce: 0.050843
2022-01-20 19:54:16,198 iteration 888 : loss : 0.068461, loss_ce: 0.027312
2022-01-20 19:54:17,560 iteration 889 : loss : 0.088845, loss_ce: 0.032300
2022-01-20 19:54:18,974 iteration 890 : loss : 0.075848, loss_ce: 0.028020
2022-01-20 19:54:20,294 iteration 891 : loss : 0.060680, loss_ce: 0.023270
2022-01-20 19:54:21,615 iteration 892 : loss : 0.068035, loss_ce: 0.031466
2022-01-20 19:54:22,915 iteration 893 : loss : 0.091194, loss_ce: 0.036323
2022-01-20 19:54:24,344 iteration 894 : loss : 0.100669, loss_ce: 0.036617
2022-01-20 19:54:25,623 iteration 895 : loss : 0.128515, loss_ce: 0.037075
2022-01-20 19:54:26,995 iteration 896 : loss : 0.133370, loss_ce: 0.052304
2022-01-20 19:54:28,389 iteration 897 : loss : 0.126767, loss_ce: 0.058738
2022-01-20 19:54:29,675 iteration 898 : loss : 0.090053, loss_ce: 0.043868
2022-01-20 19:54:30,969 iteration 899 : loss : 0.088814, loss_ce: 0.038817
2022-01-20 19:54:32,230 iteration 900 : loss : 0.066953, loss_ce: 0.030895
2022-01-20 19:54:33,512 iteration 901 : loss : 0.102626, loss_ce: 0.036910
 13%|███▉                          | 53/400 [21:43<2:18:36, 23.97s/it]2022-01-20 19:54:34,892 iteration 902 : loss : 0.064141, loss_ce: 0.028010
2022-01-20 19:54:36,294 iteration 903 : loss : 0.093734, loss_ce: 0.036814
2022-01-20 19:54:37,636 iteration 904 : loss : 0.110557, loss_ce: 0.036434
2022-01-20 19:54:38,965 iteration 905 : loss : 0.111076, loss_ce: 0.053514
2022-01-20 19:54:40,278 iteration 906 : loss : 0.079888, loss_ce: 0.036029
2022-01-20 19:54:41,661 iteration 907 : loss : 0.113264, loss_ce: 0.039094
2022-01-20 19:54:42,880 iteration 908 : loss : 0.080235, loss_ce: 0.033951
2022-01-20 19:54:44,212 iteration 909 : loss : 0.076282, loss_ce: 0.030032
2022-01-20 19:54:45,441 iteration 910 : loss : 0.109510, loss_ce: 0.043004
2022-01-20 19:54:46,741 iteration 911 : loss : 0.071592, loss_ce: 0.028507
2022-01-20 19:54:48,030 iteration 912 : loss : 0.068374, loss_ce: 0.027264
2022-01-20 19:54:49,312 iteration 913 : loss : 0.082638, loss_ce: 0.028261
2022-01-20 19:54:50,609 iteration 914 : loss : 0.131593, loss_ce: 0.055695
2022-01-20 19:54:51,882 iteration 915 : loss : 0.105244, loss_ce: 0.032446
2022-01-20 19:54:53,152 iteration 916 : loss : 0.053330, loss_ce: 0.019594
2022-01-20 19:54:54,475 iteration 917 : loss : 0.098816, loss_ce: 0.038978
2022-01-20 19:54:55,814 iteration 918 : loss : 0.083238, loss_ce: 0.030612
 14%|████                          | 54/400 [22:05<2:15:19, 23.47s/it]2022-01-20 19:54:57,193 iteration 919 : loss : 0.079322, loss_ce: 0.033854
2022-01-20 19:54:58,519 iteration 920 : loss : 0.067529, loss_ce: 0.027985
2022-01-20 19:54:59,865 iteration 921 : loss : 0.098014, loss_ce: 0.038487
2022-01-20 19:55:01,216 iteration 922 : loss : 0.100554, loss_ce: 0.048152
2022-01-20 19:55:02,493 iteration 923 : loss : 0.064487, loss_ce: 0.030940
2022-01-20 19:55:03,838 iteration 924 : loss : 0.140355, loss_ce: 0.041944
2022-01-20 19:55:05,102 iteration 925 : loss : 0.048856, loss_ce: 0.018367
2022-01-20 19:55:06,457 iteration 926 : loss : 0.068197, loss_ce: 0.028533
2022-01-20 19:55:07,704 iteration 927 : loss : 0.094557, loss_ce: 0.034894
2022-01-20 19:55:09,005 iteration 928 : loss : 0.070497, loss_ce: 0.032945
2022-01-20 19:55:10,368 iteration 929 : loss : 0.094254, loss_ce: 0.033019
2022-01-20 19:55:11,694 iteration 930 : loss : 0.073837, loss_ce: 0.029561
2022-01-20 19:55:13,032 iteration 931 : loss : 0.128747, loss_ce: 0.026915
2022-01-20 19:55:14,342 iteration 932 : loss : 0.085565, loss_ce: 0.029378
2022-01-20 19:55:15,629 iteration 933 : loss : 0.107525, loss_ce: 0.043176
2022-01-20 19:55:16,934 iteration 934 : loss : 0.055318, loss_ce: 0.022763
2022-01-20 19:55:16,934 Training Data Eval:
2022-01-20 19:55:23,467   Average segmentation loss on training set: 0.0670
2022-01-20 19:55:23,467 Validation Data Eval:
2022-01-20 19:55:25,703   Average segmentation loss on validation set: 0.1395
2022-01-20 19:55:27,105 iteration 935 : loss : 0.150213, loss_ce: 0.054333
 14%|████▏                         | 55/400 [22:36<2:28:25, 25.81s/it]2022-01-20 19:55:28,441 iteration 936 : loss : 0.080838, loss_ce: 0.028227
2022-01-20 19:55:29,784 iteration 937 : loss : 0.069812, loss_ce: 0.021401
2022-01-20 19:55:31,153 iteration 938 : loss : 0.072513, loss_ce: 0.027810
2022-01-20 19:55:32,429 iteration 939 : loss : 0.096770, loss_ce: 0.036680
2022-01-20 19:55:33,747 iteration 940 : loss : 0.094381, loss_ce: 0.041884
2022-01-20 19:55:35,150 iteration 941 : loss : 0.093036, loss_ce: 0.038191
2022-01-20 19:55:36,422 iteration 942 : loss : 0.068833, loss_ce: 0.027178
2022-01-20 19:55:37,717 iteration 943 : loss : 0.072503, loss_ce: 0.035914
2022-01-20 19:55:38,989 iteration 944 : loss : 0.074360, loss_ce: 0.027766
2022-01-20 19:55:40,264 iteration 945 : loss : 0.081530, loss_ce: 0.030363
2022-01-20 19:55:41,547 iteration 946 : loss : 0.067113, loss_ce: 0.027805
2022-01-20 19:55:42,822 iteration 947 : loss : 0.084005, loss_ce: 0.026593
2022-01-20 19:55:44,101 iteration 948 : loss : 0.091767, loss_ce: 0.033301
2022-01-20 19:55:45,409 iteration 949 : loss : 0.078945, loss_ce: 0.030088
2022-01-20 19:55:46,688 iteration 950 : loss : 0.091078, loss_ce: 0.041406
2022-01-20 19:55:48,033 iteration 951 : loss : 0.084191, loss_ce: 0.049513
2022-01-20 19:55:49,304 iteration 952 : loss : 0.074122, loss_ce: 0.033299
 14%|████▏                         | 56/400 [22:59<2:21:47, 24.73s/it]2022-01-20 19:55:50,683 iteration 953 : loss : 0.091872, loss_ce: 0.034166
2022-01-20 19:55:52,037 iteration 954 : loss : 0.070862, loss_ce: 0.029029
2022-01-20 19:55:53,272 iteration 955 : loss : 0.048668, loss_ce: 0.022864
2022-01-20 19:55:54,605 iteration 956 : loss : 0.068910, loss_ce: 0.027282
2022-01-20 19:55:55,910 iteration 957 : loss : 0.065461, loss_ce: 0.023186
2022-01-20 19:55:57,237 iteration 958 : loss : 0.108724, loss_ce: 0.034376
2022-01-20 19:55:58,647 iteration 959 : loss : 0.104481, loss_ce: 0.046580
2022-01-20 19:56:00,106 iteration 960 : loss : 0.089541, loss_ce: 0.044120
2022-01-20 19:56:01,382 iteration 961 : loss : 0.129415, loss_ce: 0.037369
2022-01-20 19:56:02,636 iteration 962 : loss : 0.086628, loss_ce: 0.024063
2022-01-20 19:56:04,040 iteration 963 : loss : 0.092793, loss_ce: 0.036398
2022-01-20 19:56:05,372 iteration 964 : loss : 0.092976, loss_ce: 0.046537
2022-01-20 19:56:06,658 iteration 965 : loss : 0.068275, loss_ce: 0.028497
2022-01-20 19:56:07,966 iteration 966 : loss : 0.084322, loss_ce: 0.034405
2022-01-20 19:56:09,291 iteration 967 : loss : 0.061516, loss_ce: 0.027218
2022-01-20 19:56:10,549 iteration 968 : loss : 0.120314, loss_ce: 0.042104
2022-01-20 19:56:11,874 iteration 969 : loss : 0.054299, loss_ce: 0.021671
 14%|████▎                         | 57/400 [23:21<2:17:40, 24.08s/it]2022-01-20 19:56:13,198 iteration 970 : loss : 0.078690, loss_ce: 0.037305
2022-01-20 19:56:14,527 iteration 971 : loss : 0.081244, loss_ce: 0.023415
2022-01-20 19:56:15,768 iteration 972 : loss : 0.066290, loss_ce: 0.029169
2022-01-20 19:56:17,089 iteration 973 : loss : 0.065710, loss_ce: 0.022618
2022-01-20 19:56:18,346 iteration 974 : loss : 0.084131, loss_ce: 0.038023
2022-01-20 19:56:19,703 iteration 975 : loss : 0.100336, loss_ce: 0.035760
2022-01-20 19:56:20,988 iteration 976 : loss : 0.063397, loss_ce: 0.025892
2022-01-20 19:56:22,289 iteration 977 : loss : 0.064838, loss_ce: 0.022412
2022-01-20 19:56:23,617 iteration 978 : loss : 0.055253, loss_ce: 0.019354
2022-01-20 19:56:24,902 iteration 979 : loss : 0.138475, loss_ce: 0.079078
2022-01-20 19:56:26,183 iteration 980 : loss : 0.057991, loss_ce: 0.021197
2022-01-20 19:56:27,578 iteration 981 : loss : 0.070155, loss_ce: 0.031178
2022-01-20 19:56:28,852 iteration 982 : loss : 0.066298, loss_ce: 0.031303
2022-01-20 19:56:30,199 iteration 983 : loss : 0.108873, loss_ce: 0.039271
2022-01-20 19:56:31,439 iteration 984 : loss : 0.069558, loss_ce: 0.033166
2022-01-20 19:56:32,743 iteration 985 : loss : 0.072718, loss_ce: 0.023757
2022-01-20 19:56:34,110 iteration 986 : loss : 0.087737, loss_ce: 0.033782
 14%|████▎                         | 58/400 [23:43<2:14:07, 23.53s/it]2022-01-20 19:56:35,527 iteration 987 : loss : 0.054621, loss_ce: 0.024694
2022-01-20 19:56:36,861 iteration 988 : loss : 0.081264, loss_ce: 0.036725
2022-01-20 19:56:38,184 iteration 989 : loss : 0.078593, loss_ce: 0.032515
2022-01-20 19:56:39,493 iteration 990 : loss : 0.074257, loss_ce: 0.031431
2022-01-20 19:56:40,796 iteration 991 : loss : 0.068912, loss_ce: 0.030469
2022-01-20 19:56:42,189 iteration 992 : loss : 0.061526, loss_ce: 0.027993
2022-01-20 19:56:43,528 iteration 993 : loss : 0.069675, loss_ce: 0.025877
2022-01-20 19:56:44,957 iteration 994 : loss : 0.078698, loss_ce: 0.034168
2022-01-20 19:56:46,287 iteration 995 : loss : 0.118616, loss_ce: 0.039078
2022-01-20 19:56:47,664 iteration 996 : loss : 0.049733, loss_ce: 0.023332
2022-01-20 19:56:49,035 iteration 997 : loss : 0.114519, loss_ce: 0.045478
2022-01-20 19:56:50,301 iteration 998 : loss : 0.088189, loss_ce: 0.026608
2022-01-20 19:56:51,657 iteration 999 : loss : 0.090651, loss_ce: 0.035317
2022-01-20 19:56:52,919 iteration 1000 : loss : 0.109935, loss_ce: 0.041078
2022-01-20 19:56:54,198 iteration 1001 : loss : 0.122416, loss_ce: 0.038238
2022-01-20 19:56:55,597 iteration 1002 : loss : 0.122679, loss_ce: 0.062642
2022-01-20 19:56:56,984 iteration 1003 : loss : 0.112692, loss_ce: 0.042080
 15%|████▍                         | 59/400 [24:06<2:12:36, 23.33s/it]2022-01-20 19:56:58,329 iteration 1004 : loss : 0.147827, loss_ce: 0.078871
2022-01-20 19:56:59,618 iteration 1005 : loss : 0.074529, loss_ce: 0.029075
2022-01-20 19:57:01,004 iteration 1006 : loss : 0.127545, loss_ce: 0.052759
2022-01-20 19:57:02,259 iteration 1007 : loss : 0.073801, loss_ce: 0.038430
2022-01-20 19:57:03,511 iteration 1008 : loss : 0.140373, loss_ce: 0.056604
2022-01-20 19:57:04,781 iteration 1009 : loss : 0.098793, loss_ce: 0.038895
2022-01-20 19:57:06,054 iteration 1010 : loss : 0.075015, loss_ce: 0.031708
2022-01-20 19:57:07,423 iteration 1011 : loss : 0.086688, loss_ce: 0.036963
2022-01-20 19:57:08,729 iteration 1012 : loss : 0.069029, loss_ce: 0.026673
2022-01-20 19:57:09,981 iteration 1013 : loss : 0.086531, loss_ce: 0.026963
2022-01-20 19:57:11,302 iteration 1014 : loss : 0.106344, loss_ce: 0.042664
2022-01-20 19:57:12,557 iteration 1015 : loss : 0.102248, loss_ce: 0.037695
2022-01-20 19:57:13,888 iteration 1016 : loss : 0.085623, loss_ce: 0.038335
2022-01-20 19:57:15,105 iteration 1017 : loss : 0.060474, loss_ce: 0.021897
2022-01-20 19:57:16,329 iteration 1018 : loss : 0.076445, loss_ce: 0.027214
2022-01-20 19:57:17,610 iteration 1019 : loss : 0.132181, loss_ce: 0.078908
2022-01-20 19:57:17,610 Training Data Eval:
2022-01-20 19:57:24,131   Average segmentation loss on training set: 0.0896
2022-01-20 19:57:24,131 Validation Data Eval:
2022-01-20 19:57:26,361   Average segmentation loss on validation set: 0.1206
2022-01-20 19:57:27,633 iteration 1020 : loss : 0.067107, loss_ce: 0.024763
 15%|████▌                         | 60/400 [24:37<2:24:39, 25.53s/it]2022-01-20 19:57:29,070 iteration 1021 : loss : 0.115138, loss_ce: 0.028409
2022-01-20 19:57:30,371 iteration 1022 : loss : 0.092370, loss_ce: 0.035793
2022-01-20 19:57:31,692 iteration 1023 : loss : 0.138900, loss_ce: 0.038854
2022-01-20 19:57:33,059 iteration 1024 : loss : 0.050558, loss_ce: 0.019193
2022-01-20 19:57:34,405 iteration 1025 : loss : 0.067658, loss_ce: 0.031494
2022-01-20 19:57:35,636 iteration 1026 : loss : 0.060684, loss_ce: 0.022659
2022-01-20 19:57:36,998 iteration 1027 : loss : 0.090130, loss_ce: 0.030459
2022-01-20 19:57:38,333 iteration 1028 : loss : 0.094419, loss_ce: 0.041418
2022-01-20 19:57:39,630 iteration 1029 : loss : 0.066683, loss_ce: 0.028791
2022-01-20 19:57:40,941 iteration 1030 : loss : 0.080951, loss_ce: 0.030195
2022-01-20 19:57:42,300 iteration 1031 : loss : 0.116247, loss_ce: 0.053604
2022-01-20 19:57:43,628 iteration 1032 : loss : 0.087928, loss_ce: 0.041082
2022-01-20 19:57:44,906 iteration 1033 : loss : 0.083909, loss_ce: 0.028257
2022-01-20 19:57:46,265 iteration 1034 : loss : 0.090086, loss_ce: 0.035465
2022-01-20 19:57:47,576 iteration 1035 : loss : 0.063648, loss_ce: 0.025630
2022-01-20 19:57:48,899 iteration 1036 : loss : 0.078608, loss_ce: 0.032293
2022-01-20 19:57:50,169 iteration 1037 : loss : 0.065431, loss_ce: 0.022898
 15%|████▌                         | 61/400 [24:59<2:19:08, 24.63s/it]2022-01-20 19:57:51,528 iteration 1038 : loss : 0.057579, loss_ce: 0.024050
2022-01-20 19:57:52,837 iteration 1039 : loss : 0.088448, loss_ce: 0.040276
2022-01-20 19:57:54,179 iteration 1040 : loss : 0.118046, loss_ce: 0.056766
2022-01-20 19:57:55,504 iteration 1041 : loss : 0.070736, loss_ce: 0.030096
2022-01-20 19:57:56,840 iteration 1042 : loss : 0.091451, loss_ce: 0.038227
2022-01-20 19:57:58,151 iteration 1043 : loss : 0.104379, loss_ce: 0.031388
2022-01-20 19:57:59,482 iteration 1044 : loss : 0.062594, loss_ce: 0.022702
2022-01-20 19:58:00,704 iteration 1045 : loss : 0.064347, loss_ce: 0.028597
2022-01-20 19:58:01,957 iteration 1046 : loss : 0.100383, loss_ce: 0.026523
2022-01-20 19:58:03,300 iteration 1047 : loss : 0.129971, loss_ce: 0.062455
2022-01-20 19:58:04,622 iteration 1048 : loss : 0.090366, loss_ce: 0.032237
2022-01-20 19:58:05,939 iteration 1049 : loss : 0.105570, loss_ce: 0.034933
2022-01-20 19:58:07,304 iteration 1050 : loss : 0.109924, loss_ce: 0.037378
2022-01-20 19:58:08,614 iteration 1051 : loss : 0.079933, loss_ce: 0.031883
2022-01-20 19:58:09,917 iteration 1052 : loss : 0.092010, loss_ce: 0.036420
2022-01-20 19:58:11,161 iteration 1053 : loss : 0.088866, loss_ce: 0.028248
2022-01-20 19:58:12,540 iteration 1054 : loss : 0.084915, loss_ce: 0.039057
 16%|████▋                         | 62/400 [25:22<2:14:56, 23.95s/it]2022-01-20 19:58:13,888 iteration 1055 : loss : 0.076055, loss_ce: 0.027648
2022-01-20 19:58:15,157 iteration 1056 : loss : 0.068869, loss_ce: 0.034785
2022-01-20 19:58:16,434 iteration 1057 : loss : 0.119071, loss_ce: 0.051074
2022-01-20 19:58:17,738 iteration 1058 : loss : 0.079034, loss_ce: 0.033534
2022-01-20 19:58:18,967 iteration 1059 : loss : 0.095615, loss_ce: 0.037266
2022-01-20 19:58:20,264 iteration 1060 : loss : 0.052397, loss_ce: 0.020233
2022-01-20 19:58:21,601 iteration 1061 : loss : 0.068966, loss_ce: 0.027999
2022-01-20 19:58:22,923 iteration 1062 : loss : 0.083317, loss_ce: 0.034168
2022-01-20 19:58:24,294 iteration 1063 : loss : 0.117747, loss_ce: 0.035671
2022-01-20 19:58:25,587 iteration 1064 : loss : 0.089257, loss_ce: 0.026616
2022-01-20 19:58:26,827 iteration 1065 : loss : 0.078151, loss_ce: 0.030893
2022-01-20 19:58:28,165 iteration 1066 : loss : 0.076472, loss_ce: 0.028945
2022-01-20 19:58:29,447 iteration 1067 : loss : 0.149888, loss_ce: 0.038290
2022-01-20 19:58:30,797 iteration 1068 : loss : 0.080199, loss_ce: 0.037741
2022-01-20 19:58:32,124 iteration 1069 : loss : 0.112049, loss_ce: 0.045858
2022-01-20 19:58:33,408 iteration 1070 : loss : 0.078534, loss_ce: 0.033919
2022-01-20 19:58:34,694 iteration 1071 : loss : 0.080426, loss_ce: 0.036328
 16%|████▋                         | 63/400 [25:44<2:11:29, 23.41s/it]2022-01-20 19:58:36,127 iteration 1072 : loss : 0.071503, loss_ce: 0.024182
2022-01-20 19:58:37,411 iteration 1073 : loss : 0.084128, loss_ce: 0.019747
2022-01-20 19:58:38,700 iteration 1074 : loss : 0.077547, loss_ce: 0.031785
2022-01-20 19:58:40,032 iteration 1075 : loss : 0.080825, loss_ce: 0.026193
2022-01-20 19:58:41,295 iteration 1076 : loss : 0.056645, loss_ce: 0.019627
2022-01-20 19:58:42,589 iteration 1077 : loss : 0.062704, loss_ce: 0.022725
2022-01-20 19:58:43,963 iteration 1078 : loss : 0.114144, loss_ce: 0.060450
2022-01-20 19:58:45,243 iteration 1079 : loss : 0.067061, loss_ce: 0.025589
2022-01-20 19:58:46,503 iteration 1080 : loss : 0.057987, loss_ce: 0.024384
2022-01-20 19:58:47,838 iteration 1081 : loss : 0.079848, loss_ce: 0.023885
2022-01-20 19:58:49,159 iteration 1082 : loss : 0.088299, loss_ce: 0.044989
2022-01-20 19:58:50,506 iteration 1083 : loss : 0.085367, loss_ce: 0.038053
2022-01-20 19:58:51,796 iteration 1084 : loss : 0.087662, loss_ce: 0.045386
2022-01-20 19:58:53,176 iteration 1085 : loss : 0.063505, loss_ce: 0.023613
2022-01-20 19:58:54,442 iteration 1086 : loss : 0.076910, loss_ce: 0.032601
2022-01-20 19:58:55,736 iteration 1087 : loss : 0.109241, loss_ce: 0.048996
2022-01-20 19:58:57,094 iteration 1088 : loss : 0.060262, loss_ce: 0.027038
 16%|████▊                         | 64/400 [26:06<2:09:24, 23.11s/it]2022-01-20 19:58:58,406 iteration 1089 : loss : 0.087207, loss_ce: 0.034869
2022-01-20 19:58:59,697 iteration 1090 : loss : 0.065895, loss_ce: 0.031527
2022-01-20 19:59:01,008 iteration 1091 : loss : 0.064152, loss_ce: 0.030055
2022-01-20 19:59:02,313 iteration 1092 : loss : 0.066568, loss_ce: 0.030922
2022-01-20 19:59:03,630 iteration 1093 : loss : 0.078196, loss_ce: 0.030385
2022-01-20 19:59:05,002 iteration 1094 : loss : 0.063592, loss_ce: 0.025845
2022-01-20 19:59:06,261 iteration 1095 : loss : 0.052734, loss_ce: 0.022127
2022-01-20 19:59:07,672 iteration 1096 : loss : 0.089382, loss_ce: 0.035501
2022-01-20 19:59:09,000 iteration 1097 : loss : 0.078615, loss_ce: 0.030508
2022-01-20 19:59:10,269 iteration 1098 : loss : 0.055560, loss_ce: 0.024352
2022-01-20 19:59:11,601 iteration 1099 : loss : 0.091249, loss_ce: 0.025092
2022-01-20 19:59:12,985 iteration 1100 : loss : 0.053496, loss_ce: 0.018086
2022-01-20 19:59:14,285 iteration 1101 : loss : 0.063450, loss_ce: 0.025742
2022-01-20 19:59:15,665 iteration 1102 : loss : 0.132771, loss_ce: 0.048860
2022-01-20 19:59:16,961 iteration 1103 : loss : 0.078950, loss_ce: 0.023805
2022-01-20 19:59:18,244 iteration 1104 : loss : 0.084914, loss_ce: 0.031243
2022-01-20 19:59:18,244 Training Data Eval:
2022-01-20 19:59:24,717   Average segmentation loss on training set: 0.0701
2022-01-20 19:59:24,717 Validation Data Eval:
2022-01-20 19:59:26,966   Average segmentation loss on validation set: 0.1458
2022-01-20 19:59:28,343 iteration 1105 : loss : 0.059108, loss_ce: 0.017996
 16%|████▉                         | 65/400 [26:38<2:22:39, 25.55s/it]2022-01-20 19:59:29,647 iteration 1106 : loss : 0.079017, loss_ce: 0.030129
2022-01-20 19:59:30,945 iteration 1107 : loss : 0.090042, loss_ce: 0.044238
2022-01-20 19:59:32,264 iteration 1108 : loss : 0.136818, loss_ce: 0.043425
2022-01-20 19:59:33,524 iteration 1109 : loss : 0.055719, loss_ce: 0.024488
2022-01-20 19:59:34,832 iteration 1110 : loss : 0.101325, loss_ce: 0.043067
2022-01-20 19:59:36,106 iteration 1111 : loss : 0.056597, loss_ce: 0.022452
2022-01-20 19:59:37,397 iteration 1112 : loss : 0.064944, loss_ce: 0.025511
2022-01-20 19:59:38,684 iteration 1113 : loss : 0.079042, loss_ce: 0.027210
2022-01-20 19:59:40,043 iteration 1114 : loss : 0.068084, loss_ce: 0.031314
2022-01-20 19:59:41,470 iteration 1115 : loss : 0.072942, loss_ce: 0.035533
2022-01-20 19:59:42,818 iteration 1116 : loss : 0.107377, loss_ce: 0.034052
2022-01-20 19:59:44,238 iteration 1117 : loss : 0.062006, loss_ce: 0.021694
2022-01-20 19:59:45,584 iteration 1118 : loss : 0.120241, loss_ce: 0.044657
2022-01-20 19:59:46,857 iteration 1119 : loss : 0.070508, loss_ce: 0.030944
2022-01-20 19:59:48,124 iteration 1120 : loss : 0.090748, loss_ce: 0.030059
2022-01-20 19:59:49,436 iteration 1121 : loss : 0.084025, loss_ce: 0.024565
2022-01-20 19:59:50,743 iteration 1122 : loss : 0.073867, loss_ce: 0.037392
 16%|████▉                         | 66/400 [27:00<2:16:58, 24.60s/it]2022-01-20 19:59:52,051 iteration 1123 : loss : 0.127673, loss_ce: 0.055011
2022-01-20 19:59:53,385 iteration 1124 : loss : 0.073321, loss_ce: 0.034979
2022-01-20 19:59:54,682 iteration 1125 : loss : 0.052756, loss_ce: 0.015078
2022-01-20 19:59:55,963 iteration 1126 : loss : 0.073401, loss_ce: 0.028358
2022-01-20 19:59:57,244 iteration 1127 : loss : 0.085230, loss_ce: 0.030059
2022-01-20 19:59:58,595 iteration 1128 : loss : 0.119009, loss_ce: 0.055819
2022-01-20 19:59:59,948 iteration 1129 : loss : 0.102483, loss_ce: 0.036086
2022-01-20 20:00:01,232 iteration 1130 : loss : 0.064118, loss_ce: 0.029685
2022-01-20 20:00:02,521 iteration 1131 : loss : 0.053660, loss_ce: 0.023695
2022-01-20 20:00:03,857 iteration 1132 : loss : 0.069095, loss_ce: 0.027183
2022-01-20 20:00:05,103 iteration 1133 : loss : 0.048377, loss_ce: 0.022316
2022-01-20 20:00:06,401 iteration 1134 : loss : 0.085402, loss_ce: 0.027104
2022-01-20 20:00:07,672 iteration 1135 : loss : 0.072373, loss_ce: 0.029495
2022-01-20 20:00:08,975 iteration 1136 : loss : 0.041301, loss_ce: 0.015544
2022-01-20 20:00:10,256 iteration 1137 : loss : 0.107021, loss_ce: 0.046021
2022-01-20 20:00:11,608 iteration 1138 : loss : 0.061962, loss_ce: 0.026325
2022-01-20 20:00:12,932 iteration 1139 : loss : 0.068021, loss_ce: 0.028823
 17%|█████                         | 67/400 [27:22<2:12:32, 23.88s/it]2022-01-20 20:00:14,282 iteration 1140 : loss : 0.077336, loss_ce: 0.029205
2022-01-20 20:00:15,552 iteration 1141 : loss : 0.069458, loss_ce: 0.025950
2022-01-20 20:00:16,910 iteration 1142 : loss : 0.080818, loss_ce: 0.031331
2022-01-20 20:00:18,216 iteration 1143 : loss : 0.066343, loss_ce: 0.025582
2022-01-20 20:00:19,558 iteration 1144 : loss : 0.081477, loss_ce: 0.042828
2022-01-20 20:00:20,816 iteration 1145 : loss : 0.061159, loss_ce: 0.023152
2022-01-20 20:00:22,130 iteration 1146 : loss : 0.110454, loss_ce: 0.058110
2022-01-20 20:00:23,352 iteration 1147 : loss : 0.061958, loss_ce: 0.022902
2022-01-20 20:00:24,697 iteration 1148 : loss : 0.080051, loss_ce: 0.031286
2022-01-20 20:00:25,962 iteration 1149 : loss : 0.061003, loss_ce: 0.030125
2022-01-20 20:00:27,276 iteration 1150 : loss : 0.053435, loss_ce: 0.022074
2022-01-20 20:00:28,621 iteration 1151 : loss : 0.056877, loss_ce: 0.019278
2022-01-20 20:00:30,036 iteration 1152 : loss : 0.070152, loss_ce: 0.025298
2022-01-20 20:00:31,253 iteration 1153 : loss : 0.056139, loss_ce: 0.021146
2022-01-20 20:00:32,595 iteration 1154 : loss : 0.089763, loss_ce: 0.031735
2022-01-20 20:00:33,868 iteration 1155 : loss : 0.086807, loss_ce: 0.029401
2022-01-20 20:00:35,226 iteration 1156 : loss : 0.108499, loss_ce: 0.055898
 17%|█████                         | 68/400 [27:45<2:09:30, 23.40s/it]2022-01-20 20:00:36,553 iteration 1157 : loss : 0.125862, loss_ce: 0.029830
2022-01-20 20:00:37,873 iteration 1158 : loss : 0.057356, loss_ce: 0.019766
2022-01-20 20:00:39,247 iteration 1159 : loss : 0.056372, loss_ce: 0.019045
2022-01-20 20:00:40,657 iteration 1160 : loss : 0.094853, loss_ce: 0.037360
2022-01-20 20:00:42,003 iteration 1161 : loss : 0.050013, loss_ce: 0.022654
2022-01-20 20:00:43,322 iteration 1162 : loss : 0.055521, loss_ce: 0.020122
2022-01-20 20:00:44,618 iteration 1163 : loss : 0.059761, loss_ce: 0.025857
2022-01-20 20:00:45,936 iteration 1164 : loss : 0.080519, loss_ce: 0.034409
2022-01-20 20:00:47,262 iteration 1165 : loss : 0.077606, loss_ce: 0.034999
2022-01-20 20:00:48,648 iteration 1166 : loss : 0.047770, loss_ce: 0.017484
2022-01-20 20:00:49,934 iteration 1167 : loss : 0.058498, loss_ce: 0.026866
2022-01-20 20:00:51,223 iteration 1168 : loss : 0.057491, loss_ce: 0.023939
2022-01-20 20:00:52,498 iteration 1169 : loss : 0.066250, loss_ce: 0.026712
2022-01-20 20:00:53,776 iteration 1170 : loss : 0.073935, loss_ce: 0.031491
2022-01-20 20:00:55,087 iteration 1171 : loss : 0.067768, loss_ce: 0.024024
2022-01-20 20:00:56,389 iteration 1172 : loss : 0.081559, loss_ce: 0.036114
2022-01-20 20:00:57,721 iteration 1173 : loss : 0.063231, loss_ce: 0.021639
 17%|█████▏                        | 69/400 [28:07<2:07:35, 23.13s/it]2022-01-20 20:00:59,041 iteration 1174 : loss : 0.082472, loss_ce: 0.027375
2022-01-20 20:01:00,351 iteration 1175 : loss : 0.063059, loss_ce: 0.023156
2022-01-20 20:01:01,642 iteration 1176 : loss : 0.072686, loss_ce: 0.028589
2022-01-20 20:01:02,901 iteration 1177 : loss : 0.055967, loss_ce: 0.021654
2022-01-20 20:01:04,236 iteration 1178 : loss : 0.065429, loss_ce: 0.019533
2022-01-20 20:01:05,629 iteration 1179 : loss : 0.057545, loss_ce: 0.023756
2022-01-20 20:01:06,910 iteration 1180 : loss : 0.056393, loss_ce: 0.024445
2022-01-20 20:01:08,209 iteration 1181 : loss : 0.082258, loss_ce: 0.027889
2022-01-20 20:01:09,523 iteration 1182 : loss : 0.084991, loss_ce: 0.032002
2022-01-20 20:01:10,786 iteration 1183 : loss : 0.065279, loss_ce: 0.022268
2022-01-20 20:01:12,053 iteration 1184 : loss : 0.048208, loss_ce: 0.021242
2022-01-20 20:01:13,392 iteration 1185 : loss : 0.071868, loss_ce: 0.027471
2022-01-20 20:01:14,672 iteration 1186 : loss : 0.056150, loss_ce: 0.025916
2022-01-20 20:01:16,035 iteration 1187 : loss : 0.069109, loss_ce: 0.024070
2022-01-20 20:01:17,301 iteration 1188 : loss : 0.078509, loss_ce: 0.032169
2022-01-20 20:01:18,683 iteration 1189 : loss : 0.062925, loss_ce: 0.023753
2022-01-20 20:01:18,683 Training Data Eval:
2022-01-20 20:01:25,192   Average segmentation loss on training set: 0.0667
2022-01-20 20:01:25,192 Validation Data Eval:
2022-01-20 20:01:27,447   Average segmentation loss on validation set: 0.0875
2022-01-20 20:01:33,238 Found new lowest validation loss at iteration 1189! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 20:01:34,490 iteration 1190 : loss : 0.069229, loss_ce: 0.032393
 18%|█████▎                        | 70/400 [28:44<2:29:44, 27.23s/it]2022-01-20 20:01:35,825 iteration 1191 : loss : 0.082170, loss_ce: 0.034573
2022-01-20 20:01:37,014 iteration 1192 : loss : 0.050779, loss_ce: 0.023119
2022-01-20 20:01:38,321 iteration 1193 : loss : 0.099909, loss_ce: 0.035853
2022-01-20 20:01:39,540 iteration 1194 : loss : 0.070157, loss_ce: 0.023699
2022-01-20 20:01:40,723 iteration 1195 : loss : 0.053379, loss_ce: 0.024026
2022-01-20 20:01:41,905 iteration 1196 : loss : 0.054580, loss_ce: 0.023141
2022-01-20 20:01:43,053 iteration 1197 : loss : 0.052918, loss_ce: 0.018896
2022-01-20 20:01:44,340 iteration 1198 : loss : 0.067503, loss_ce: 0.027439
2022-01-20 20:01:45,587 iteration 1199 : loss : 0.075042, loss_ce: 0.028793
2022-01-20 20:01:46,805 iteration 1200 : loss : 0.100476, loss_ce: 0.028223
2022-01-20 20:01:47,981 iteration 1201 : loss : 0.061052, loss_ce: 0.028347
2022-01-20 20:01:49,231 iteration 1202 : loss : 0.062435, loss_ce: 0.024202
2022-01-20 20:01:50,481 iteration 1203 : loss : 0.066007, loss_ce: 0.022549
2022-01-20 20:01:51,762 iteration 1204 : loss : 0.076467, loss_ce: 0.024974
2022-01-20 20:01:53,067 iteration 1205 : loss : 0.096728, loss_ce: 0.038246
2022-01-20 20:01:54,391 iteration 1206 : loss : 0.052393, loss_ce: 0.020084
2022-01-20 20:01:55,751 iteration 1207 : loss : 0.042404, loss_ce: 0.014744
 18%|█████▎                        | 71/400 [29:05<2:19:28, 25.44s/it]2022-01-20 20:01:57,152 iteration 1208 : loss : 0.072012, loss_ce: 0.033801
2022-01-20 20:01:58,476 iteration 1209 : loss : 0.061473, loss_ce: 0.021545
2022-01-20 20:01:59,791 iteration 1210 : loss : 0.080998, loss_ce: 0.033476
2022-01-20 20:02:01,046 iteration 1211 : loss : 0.050555, loss_ce: 0.020036
2022-01-20 20:02:02,298 iteration 1212 : loss : 0.084453, loss_ce: 0.024721
2022-01-20 20:02:03,727 iteration 1213 : loss : 0.074845, loss_ce: 0.030648
2022-01-20 20:02:05,023 iteration 1214 : loss : 0.067882, loss_ce: 0.026260
2022-01-20 20:02:06,334 iteration 1215 : loss : 0.114604, loss_ce: 0.059441
2022-01-20 20:02:07,750 iteration 1216 : loss : 0.078066, loss_ce: 0.027644
2022-01-20 20:02:09,059 iteration 1217 : loss : 0.045836, loss_ce: 0.016044
2022-01-20 20:02:10,327 iteration 1218 : loss : 0.053844, loss_ce: 0.019792
2022-01-20 20:02:11,576 iteration 1219 : loss : 0.050379, loss_ce: 0.017782
2022-01-20 20:02:12,881 iteration 1220 : loss : 0.077816, loss_ce: 0.030282
2022-01-20 20:02:14,149 iteration 1221 : loss : 0.075775, loss_ce: 0.030948
2022-01-20 20:02:15,521 iteration 1222 : loss : 0.067134, loss_ce: 0.024833
2022-01-20 20:02:16,806 iteration 1223 : loss : 0.075386, loss_ce: 0.034173
2022-01-20 20:02:18,154 iteration 1224 : loss : 0.099041, loss_ce: 0.040607
 18%|█████▍                        | 72/400 [29:27<2:14:04, 24.52s/it]2022-01-20 20:02:19,474 iteration 1225 : loss : 0.067229, loss_ce: 0.026475
2022-01-20 20:02:20,810 iteration 1226 : loss : 0.080145, loss_ce: 0.032533
2022-01-20 20:02:22,066 iteration 1227 : loss : 0.052369, loss_ce: 0.018471
2022-01-20 20:02:23,361 iteration 1228 : loss : 0.055685, loss_ce: 0.021823
2022-01-20 20:02:24,604 iteration 1229 : loss : 0.067355, loss_ce: 0.020056
2022-01-20 20:02:25,870 iteration 1230 : loss : 0.064540, loss_ce: 0.022860
2022-01-20 20:02:27,218 iteration 1231 : loss : 0.065095, loss_ce: 0.030681
2022-01-20 20:02:28,488 iteration 1232 : loss : 0.072160, loss_ce: 0.027678
2022-01-20 20:02:29,755 iteration 1233 : loss : 0.056131, loss_ce: 0.022421
2022-01-20 20:02:31,098 iteration 1234 : loss : 0.069314, loss_ce: 0.027502
2022-01-20 20:02:32,441 iteration 1235 : loss : 0.093551, loss_ce: 0.026743
2022-01-20 20:02:33,715 iteration 1236 : loss : 0.061345, loss_ce: 0.024583
2022-01-20 20:02:34,933 iteration 1237 : loss : 0.076225, loss_ce: 0.029090
2022-01-20 20:02:36,282 iteration 1238 : loss : 0.068660, loss_ce: 0.023501
2022-01-20 20:02:37,538 iteration 1239 : loss : 0.053250, loss_ce: 0.019403
2022-01-20 20:02:38,802 iteration 1240 : loss : 0.051452, loss_ce: 0.020897
2022-01-20 20:02:40,067 iteration 1241 : loss : 0.063545, loss_ce: 0.031756
 18%|█████▍                        | 73/400 [29:49<2:09:23, 23.74s/it]2022-01-20 20:02:41,427 iteration 1242 : loss : 0.070157, loss_ce: 0.037843
2022-01-20 20:02:42,688 iteration 1243 : loss : 0.102472, loss_ce: 0.044316
2022-01-20 20:02:43,962 iteration 1244 : loss : 0.068033, loss_ce: 0.030951
2022-01-20 20:02:45,278 iteration 1245 : loss : 0.086131, loss_ce: 0.033896
2022-01-20 20:02:46,591 iteration 1246 : loss : 0.054115, loss_ce: 0.018401
2022-01-20 20:02:47,870 iteration 1247 : loss : 0.057295, loss_ce: 0.024028
2022-01-20 20:02:49,201 iteration 1248 : loss : 0.076506, loss_ce: 0.023054
2022-01-20 20:02:50,462 iteration 1249 : loss : 0.049363, loss_ce: 0.015355
2022-01-20 20:02:51,863 iteration 1250 : loss : 0.069289, loss_ce: 0.029332
2022-01-20 20:02:53,278 iteration 1251 : loss : 0.064891, loss_ce: 0.033418
2022-01-20 20:02:54,601 iteration 1252 : loss : 0.059372, loss_ce: 0.023244
2022-01-20 20:02:55,827 iteration 1253 : loss : 0.061644, loss_ce: 0.020995
2022-01-20 20:02:57,137 iteration 1254 : loss : 0.064881, loss_ce: 0.034959
2022-01-20 20:02:58,444 iteration 1255 : loss : 0.062953, loss_ce: 0.026151
2022-01-20 20:02:59,736 iteration 1256 : loss : 0.062390, loss_ce: 0.026368
2022-01-20 20:03:01,039 iteration 1257 : loss : 0.076200, loss_ce: 0.022847
2022-01-20 20:03:02,362 iteration 1258 : loss : 0.059964, loss_ce: 0.027641
 18%|█████▌                        | 74/400 [30:12<2:06:37, 23.31s/it]2022-01-20 20:03:03,666 iteration 1259 : loss : 0.046660, loss_ce: 0.021040
2022-01-20 20:03:05,017 iteration 1260 : loss : 0.061110, loss_ce: 0.028808
2022-01-20 20:03:06,300 iteration 1261 : loss : 0.061843, loss_ce: 0.021990
2022-01-20 20:03:07,672 iteration 1262 : loss : 0.072552, loss_ce: 0.029823
2022-01-20 20:03:09,070 iteration 1263 : loss : 0.089640, loss_ce: 0.031750
2022-01-20 20:03:10,347 iteration 1264 : loss : 0.060223, loss_ce: 0.025343
2022-01-20 20:03:11,699 iteration 1265 : loss : 0.067142, loss_ce: 0.034108
2022-01-20 20:03:13,046 iteration 1266 : loss : 0.054227, loss_ce: 0.019734
2022-01-20 20:03:14,372 iteration 1267 : loss : 0.062514, loss_ce: 0.023199
2022-01-20 20:03:15,645 iteration 1268 : loss : 0.073556, loss_ce: 0.030496
2022-01-20 20:03:17,026 iteration 1269 : loss : 0.132799, loss_ce: 0.034868
2022-01-20 20:03:18,372 iteration 1270 : loss : 0.057354, loss_ce: 0.024793
2022-01-20 20:03:19,676 iteration 1271 : loss : 0.066772, loss_ce: 0.030142
2022-01-20 20:03:20,901 iteration 1272 : loss : 0.058796, loss_ce: 0.022231
2022-01-20 20:03:22,185 iteration 1273 : loss : 0.071099, loss_ce: 0.022818
2022-01-20 20:03:23,496 iteration 1274 : loss : 0.070948, loss_ce: 0.021318
2022-01-20 20:03:23,496 Training Data Eval:
2022-01-20 20:03:29,974   Average segmentation loss on training set: 0.0585
2022-01-20 20:03:29,974 Validation Data Eval:
2022-01-20 20:03:32,227   Average segmentation loss on validation set: 0.1352
2022-01-20 20:03:33,537 iteration 1275 : loss : 0.061581, loss_ce: 0.021723
 19%|█████▋                        | 75/400 [30:43<2:19:01, 25.67s/it]2022-01-20 20:03:34,816 iteration 1276 : loss : 0.062622, loss_ce: 0.026176
2022-01-20 20:03:36,232 iteration 1277 : loss : 0.062841, loss_ce: 0.024205
2022-01-20 20:03:37,501 iteration 1278 : loss : 0.039461, loss_ce: 0.014586
2022-01-20 20:03:38,890 iteration 1279 : loss : 0.057456, loss_ce: 0.022520
2022-01-20 20:03:40,212 iteration 1280 : loss : 0.069031, loss_ce: 0.031707
2022-01-20 20:03:41,534 iteration 1281 : loss : 0.066283, loss_ce: 0.032983
2022-01-20 20:03:42,797 iteration 1282 : loss : 0.036173, loss_ce: 0.016721
2022-01-20 20:03:44,152 iteration 1283 : loss : 0.094358, loss_ce: 0.033160
2022-01-20 20:03:45,406 iteration 1284 : loss : 0.060897, loss_ce: 0.022470
2022-01-20 20:03:46,770 iteration 1285 : loss : 0.064825, loss_ce: 0.021712
2022-01-20 20:03:48,157 iteration 1286 : loss : 0.112142, loss_ce: 0.026751
2022-01-20 20:03:49,431 iteration 1287 : loss : 0.046878, loss_ce: 0.024025
2022-01-20 20:03:50,697 iteration 1288 : loss : 0.073884, loss_ce: 0.033053
2022-01-20 20:03:52,008 iteration 1289 : loss : 0.091195, loss_ce: 0.039921
2022-01-20 20:03:53,351 iteration 1290 : loss : 0.069104, loss_ce: 0.026362
2022-01-20 20:03:54,707 iteration 1291 : loss : 0.096773, loss_ce: 0.040030
2022-01-20 20:03:56,081 iteration 1292 : loss : 0.075697, loss_ce: 0.030941
 19%|█████▋                        | 76/400 [31:05<2:13:32, 24.73s/it]2022-01-20 20:03:57,474 iteration 1293 : loss : 0.063538, loss_ce: 0.021580
2022-01-20 20:03:58,819 iteration 1294 : loss : 0.109026, loss_ce: 0.042774
2022-01-20 20:04:00,201 iteration 1295 : loss : 0.161152, loss_ce: 0.063112
2022-01-20 20:04:01,521 iteration 1296 : loss : 0.118870, loss_ce: 0.045192
2022-01-20 20:04:02,784 iteration 1297 : loss : 0.060187, loss_ce: 0.026689
2022-01-20 20:04:04,028 iteration 1298 : loss : 0.045835, loss_ce: 0.018185
2022-01-20 20:04:05,339 iteration 1299 : loss : 0.065617, loss_ce: 0.030779
2022-01-20 20:04:06,630 iteration 1300 : loss : 0.084138, loss_ce: 0.027950
2022-01-20 20:04:07,963 iteration 1301 : loss : 0.057866, loss_ce: 0.025104
2022-01-20 20:04:09,245 iteration 1302 : loss : 0.073918, loss_ce: 0.031034
2022-01-20 20:04:10,534 iteration 1303 : loss : 0.057794, loss_ce: 0.021450
2022-01-20 20:04:11,860 iteration 1304 : loss : 0.053958, loss_ce: 0.021367
2022-01-20 20:04:13,166 iteration 1305 : loss : 0.066877, loss_ce: 0.025388
2022-01-20 20:04:14,509 iteration 1306 : loss : 0.114431, loss_ce: 0.034517
2022-01-20 20:04:15,856 iteration 1307 : loss : 0.067192, loss_ce: 0.036273
2022-01-20 20:04:17,183 iteration 1308 : loss : 0.084452, loss_ce: 0.021369
2022-01-20 20:04:18,438 iteration 1309 : loss : 0.064815, loss_ce: 0.023851
 19%|█████▊                        | 77/400 [31:28<2:09:18, 24.02s/it]2022-01-20 20:04:19,839 iteration 1310 : loss : 0.060868, loss_ce: 0.024616
2022-01-20 20:04:21,110 iteration 1311 : loss : 0.052195, loss_ce: 0.023079
2022-01-20 20:04:22,374 iteration 1312 : loss : 0.042391, loss_ce: 0.017999
2022-01-20 20:04:23,653 iteration 1313 : loss : 0.066239, loss_ce: 0.020662
2022-01-20 20:04:24,968 iteration 1314 : loss : 0.061403, loss_ce: 0.026153
2022-01-20 20:04:26,305 iteration 1315 : loss : 0.066322, loss_ce: 0.023434
2022-01-20 20:04:27,728 iteration 1316 : loss : 0.081526, loss_ce: 0.028677
2022-01-20 20:04:29,022 iteration 1317 : loss : 0.084785, loss_ce: 0.046807
2022-01-20 20:04:30,423 iteration 1318 : loss : 0.172063, loss_ce: 0.031331
2022-01-20 20:04:31,775 iteration 1319 : loss : 0.052127, loss_ce: 0.023039
2022-01-20 20:04:33,063 iteration 1320 : loss : 0.053728, loss_ce: 0.022797
2022-01-20 20:04:34,395 iteration 1321 : loss : 0.123730, loss_ce: 0.041936
2022-01-20 20:04:35,655 iteration 1322 : loss : 0.062638, loss_ce: 0.030590
2022-01-20 20:04:36,945 iteration 1323 : loss : 0.085078, loss_ce: 0.030337
2022-01-20 20:04:38,276 iteration 1324 : loss : 0.089426, loss_ce: 0.043202
2022-01-20 20:04:39,642 iteration 1325 : loss : 0.069796, loss_ce: 0.023938
2022-01-20 20:04:40,860 iteration 1326 : loss : 0.067703, loss_ce: 0.026682
 20%|█████▊                        | 78/400 [31:50<2:06:19, 23.54s/it]2022-01-20 20:04:42,175 iteration 1327 : loss : 0.082850, loss_ce: 0.025714
2022-01-20 20:04:43,504 iteration 1328 : loss : 0.077195, loss_ce: 0.040080
2022-01-20 20:04:44,808 iteration 1329 : loss : 0.054680, loss_ce: 0.022246
2022-01-20 20:04:46,094 iteration 1330 : loss : 0.102360, loss_ce: 0.030146
2022-01-20 20:04:47,337 iteration 1331 : loss : 0.062452, loss_ce: 0.029645
2022-01-20 20:04:48,588 iteration 1332 : loss : 0.057136, loss_ce: 0.017579
2022-01-20 20:04:49,875 iteration 1333 : loss : 0.048562, loss_ce: 0.014476
2022-01-20 20:04:51,193 iteration 1334 : loss : 0.072824, loss_ce: 0.026637
2022-01-20 20:04:52,471 iteration 1335 : loss : 0.071359, loss_ce: 0.019072
2022-01-20 20:04:53,751 iteration 1336 : loss : 0.042259, loss_ce: 0.016097
2022-01-20 20:04:55,102 iteration 1337 : loss : 0.079397, loss_ce: 0.035032
2022-01-20 20:04:56,433 iteration 1338 : loss : 0.072685, loss_ce: 0.021159
2022-01-20 20:04:57,765 iteration 1339 : loss : 0.067652, loss_ce: 0.031330
2022-01-20 20:04:59,100 iteration 1340 : loss : 0.060322, loss_ce: 0.025904
2022-01-20 20:05:00,407 iteration 1341 : loss : 0.082265, loss_ce: 0.030634
2022-01-20 20:05:01,702 iteration 1342 : loss : 0.083274, loss_ce: 0.033920
2022-01-20 20:05:02,954 iteration 1343 : loss : 0.049205, loss_ce: 0.019807
 20%|█████▉                        | 79/400 [32:12<2:03:37, 23.11s/it]2022-01-20 20:05:04,310 iteration 1344 : loss : 0.068018, loss_ce: 0.026145
2022-01-20 20:05:05,690 iteration 1345 : loss : 0.095434, loss_ce: 0.039526
2022-01-20 20:05:06,977 iteration 1346 : loss : 0.073482, loss_ce: 0.031506
2022-01-20 20:05:08,361 iteration 1347 : loss : 0.064116, loss_ce: 0.027175
2022-01-20 20:05:09,780 iteration 1348 : loss : 0.076989, loss_ce: 0.032471
2022-01-20 20:05:11,103 iteration 1349 : loss : 0.055993, loss_ce: 0.023620
2022-01-20 20:05:12,409 iteration 1350 : loss : 0.061786, loss_ce: 0.028318
2022-01-20 20:05:13,722 iteration 1351 : loss : 0.061649, loss_ce: 0.025848
2022-01-20 20:05:15,085 iteration 1352 : loss : 0.057441, loss_ce: 0.026379
2022-01-20 20:05:16,413 iteration 1353 : loss : 0.078272, loss_ce: 0.032030
2022-01-20 20:05:17,759 iteration 1354 : loss : 0.072821, loss_ce: 0.025481
2022-01-20 20:05:19,168 iteration 1355 : loss : 0.179099, loss_ce: 0.041593
2022-01-20 20:05:20,497 iteration 1356 : loss : 0.062931, loss_ce: 0.027558
2022-01-20 20:05:21,835 iteration 1357 : loss : 0.055709, loss_ce: 0.021768
2022-01-20 20:05:23,194 iteration 1358 : loss : 0.074319, loss_ce: 0.030014
2022-01-20 20:05:24,503 iteration 1359 : loss : 0.081357, loss_ce: 0.039920
2022-01-20 20:05:24,503 Training Data Eval:
2022-01-20 20:05:31,009   Average segmentation loss on training set: 0.0636
2022-01-20 20:05:31,010 Validation Data Eval:
2022-01-20 20:05:33,251   Average segmentation loss on validation set: 0.1503
2022-01-20 20:05:34,634 iteration 1360 : loss : 0.062071, loss_ce: 0.017658
 20%|██████                        | 80/400 [32:44<2:16:56, 25.68s/it]2022-01-20 20:05:36,078 iteration 1361 : loss : 0.067327, loss_ce: 0.026703
2022-01-20 20:05:37,399 iteration 1362 : loss : 0.070176, loss_ce: 0.027281
2022-01-20 20:05:38,855 iteration 1363 : loss : 0.103205, loss_ce: 0.040398
2022-01-20 20:05:40,050 iteration 1364 : loss : 0.071290, loss_ce: 0.018317
2022-01-20 20:05:41,420 iteration 1365 : loss : 0.056715, loss_ce: 0.024931
2022-01-20 20:05:42,745 iteration 1366 : loss : 0.071238, loss_ce: 0.027993
2022-01-20 20:05:44,050 iteration 1367 : loss : 0.049772, loss_ce: 0.018571
2022-01-20 20:05:45,364 iteration 1368 : loss : 0.051347, loss_ce: 0.024979
2022-01-20 20:05:46,652 iteration 1369 : loss : 0.077011, loss_ce: 0.028471
2022-01-20 20:05:48,017 iteration 1370 : loss : 0.066403, loss_ce: 0.030297
2022-01-20 20:05:49,316 iteration 1371 : loss : 0.049717, loss_ce: 0.017579
2022-01-20 20:05:50,608 iteration 1372 : loss : 0.055686, loss_ce: 0.021233
2022-01-20 20:05:51,865 iteration 1373 : loss : 0.069701, loss_ce: 0.027586
2022-01-20 20:05:53,173 iteration 1374 : loss : 0.060494, loss_ce: 0.019854
2022-01-20 20:05:54,503 iteration 1375 : loss : 0.072507, loss_ce: 0.032852
2022-01-20 20:05:55,821 iteration 1376 : loss : 0.073792, loss_ce: 0.023944
2022-01-20 20:05:57,122 iteration 1377 : loss : 0.032484, loss_ce: 0.011869
 20%|██████                        | 81/400 [33:06<2:11:26, 24.72s/it]2022-01-20 20:05:58,408 iteration 1378 : loss : 0.092605, loss_ce: 0.038619
2022-01-20 20:05:59,753 iteration 1379 : loss : 0.061399, loss_ce: 0.023393
2022-01-20 20:06:01,052 iteration 1380 : loss : 0.064147, loss_ce: 0.027130
2022-01-20 20:06:02,428 iteration 1381 : loss : 0.058265, loss_ce: 0.025891
2022-01-20 20:06:03,665 iteration 1382 : loss : 0.057297, loss_ce: 0.021965
2022-01-20 20:06:05,041 iteration 1383 : loss : 0.055147, loss_ce: 0.020694
2022-01-20 20:06:06,359 iteration 1384 : loss : 0.070789, loss_ce: 0.026185
2022-01-20 20:06:07,609 iteration 1385 : loss : 0.091113, loss_ce: 0.039056
2022-01-20 20:06:08,889 iteration 1386 : loss : 0.043542, loss_ce: 0.015293
2022-01-20 20:06:10,184 iteration 1387 : loss : 0.057898, loss_ce: 0.023057
2022-01-20 20:06:11,495 iteration 1388 : loss : 0.053911, loss_ce: 0.016518
2022-01-20 20:06:12,796 iteration 1389 : loss : 0.061628, loss_ce: 0.025097
2022-01-20 20:06:14,152 iteration 1390 : loss : 0.055639, loss_ce: 0.022212
2022-01-20 20:06:15,532 iteration 1391 : loss : 0.066212, loss_ce: 0.026930
2022-01-20 20:06:16,878 iteration 1392 : loss : 0.061843, loss_ce: 0.031374
2022-01-20 20:06:18,166 iteration 1393 : loss : 0.063897, loss_ce: 0.021958
2022-01-20 20:06:19,530 iteration 1394 : loss : 0.042181, loss_ce: 0.020544
 20%|██████▏                       | 82/400 [33:29<2:07:20, 24.03s/it]2022-01-20 20:06:20,854 iteration 1395 : loss : 0.065650, loss_ce: 0.024388
2022-01-20 20:06:22,141 iteration 1396 : loss : 0.060376, loss_ce: 0.025939
2022-01-20 20:06:23,502 iteration 1397 : loss : 0.063660, loss_ce: 0.021484
2022-01-20 20:06:24,873 iteration 1398 : loss : 0.057415, loss_ce: 0.023740
2022-01-20 20:06:26,115 iteration 1399 : loss : 0.058139, loss_ce: 0.025041
2022-01-20 20:06:27,346 iteration 1400 : loss : 0.059071, loss_ce: 0.019615
2022-01-20 20:06:28,699 iteration 1401 : loss : 0.059131, loss_ce: 0.020699
2022-01-20 20:06:30,004 iteration 1402 : loss : 0.072924, loss_ce: 0.026359
2022-01-20 20:06:31,252 iteration 1403 : loss : 0.052908, loss_ce: 0.018532
2022-01-20 20:06:32,507 iteration 1404 : loss : 0.035379, loss_ce: 0.014678
2022-01-20 20:06:33,822 iteration 1405 : loss : 0.075742, loss_ce: 0.024707
2022-01-20 20:06:35,135 iteration 1406 : loss : 0.062948, loss_ce: 0.019923
2022-01-20 20:06:36,491 iteration 1407 : loss : 0.043241, loss_ce: 0.016197
2022-01-20 20:06:37,794 iteration 1408 : loss : 0.071234, loss_ce: 0.022491
2022-01-20 20:06:39,114 iteration 1409 : loss : 0.089996, loss_ce: 0.032015
2022-01-20 20:06:40,496 iteration 1410 : loss : 0.047756, loss_ce: 0.019828
2022-01-20 20:06:41,781 iteration 1411 : loss : 0.054874, loss_ce: 0.019441
 21%|██████▏                       | 83/400 [33:51<2:04:07, 23.49s/it]2022-01-20 20:06:43,104 iteration 1412 : loss : 0.084043, loss_ce: 0.029302
2022-01-20 20:06:44,471 iteration 1413 : loss : 0.064638, loss_ce: 0.021017
2022-01-20 20:06:45,759 iteration 1414 : loss : 0.038027, loss_ce: 0.013806
2022-01-20 20:06:47,113 iteration 1415 : loss : 0.050425, loss_ce: 0.014378
2022-01-20 20:06:48,400 iteration 1416 : loss : 0.063480, loss_ce: 0.027224
2022-01-20 20:06:49,672 iteration 1417 : loss : 0.062015, loss_ce: 0.022573
2022-01-20 20:06:50,963 iteration 1418 : loss : 0.075688, loss_ce: 0.033104
2022-01-20 20:06:52,234 iteration 1419 : loss : 0.047445, loss_ce: 0.018022
2022-01-20 20:06:53,590 iteration 1420 : loss : 0.056533, loss_ce: 0.019497
2022-01-20 20:06:54,984 iteration 1421 : loss : 0.060710, loss_ce: 0.024163
2022-01-20 20:06:56,325 iteration 1422 : loss : 0.058098, loss_ce: 0.018888
2022-01-20 20:06:57,570 iteration 1423 : loss : 0.059116, loss_ce: 0.024305
2022-01-20 20:06:58,903 iteration 1424 : loss : 0.056206, loss_ce: 0.022921
2022-01-20 20:07:00,225 iteration 1425 : loss : 0.037660, loss_ce: 0.016238
2022-01-20 20:07:01,479 iteration 1426 : loss : 0.073440, loss_ce: 0.032948
2022-01-20 20:07:02,773 iteration 1427 : loss : 0.061269, loss_ce: 0.022987
2022-01-20 20:07:03,989 iteration 1428 : loss : 0.090308, loss_ce: 0.031775
 21%|██████▎                       | 84/400 [34:13<2:01:42, 23.11s/it]2022-01-20 20:07:05,412 iteration 1429 : loss : 0.048402, loss_ce: 0.021021
2022-01-20 20:07:06,689 iteration 1430 : loss : 0.189650, loss_ce: 0.042859
2022-01-20 20:07:07,993 iteration 1431 : loss : 0.070415, loss_ce: 0.032516
2022-01-20 20:07:09,308 iteration 1432 : loss : 0.063553, loss_ce: 0.029349
2022-01-20 20:07:10,670 iteration 1433 : loss : 0.061952, loss_ce: 0.017873
2022-01-20 20:07:11,949 iteration 1434 : loss : 0.053554, loss_ce: 0.023820
2022-01-20 20:07:13,286 iteration 1435 : loss : 0.057125, loss_ce: 0.021213
2022-01-20 20:07:14,623 iteration 1436 : loss : 0.055395, loss_ce: 0.025127
2022-01-20 20:07:15,978 iteration 1437 : loss : 0.066384, loss_ce: 0.020404
2022-01-20 20:07:17,292 iteration 1438 : loss : 0.045221, loss_ce: 0.021612
2022-01-20 20:07:18,660 iteration 1439 : loss : 0.073161, loss_ce: 0.033919
2022-01-20 20:07:20,004 iteration 1440 : loss : 0.052335, loss_ce: 0.022036
2022-01-20 20:07:21,304 iteration 1441 : loss : 0.041417, loss_ce: 0.019038
2022-01-20 20:07:22,620 iteration 1442 : loss : 0.050297, loss_ce: 0.020533
2022-01-20 20:07:23,955 iteration 1443 : loss : 0.064599, loss_ce: 0.025873
2022-01-20 20:07:25,266 iteration 1444 : loss : 0.136972, loss_ce: 0.029885
2022-01-20 20:07:25,266 Training Data Eval:
2022-01-20 20:07:31,742   Average segmentation loss on training set: 0.0412
2022-01-20 20:07:31,742 Validation Data Eval:
2022-01-20 20:07:33,982   Average segmentation loss on validation set: 0.0908
2022-01-20 20:07:35,343 iteration 1445 : loss : 0.054670, loss_ce: 0.016536
 21%|██████▍                       | 85/400 [34:45<2:14:18, 25.58s/it]2022-01-20 20:07:36,775 iteration 1446 : loss : 0.053215, loss_ce: 0.018486
2022-01-20 20:07:38,072 iteration 1447 : loss : 0.062127, loss_ce: 0.019100
2022-01-20 20:07:39,330 iteration 1448 : loss : 0.093912, loss_ce: 0.052067
2022-01-20 20:07:40,681 iteration 1449 : loss : 0.071874, loss_ce: 0.022926
2022-01-20 20:07:42,096 iteration 1450 : loss : 0.072141, loss_ce: 0.034860
2022-01-20 20:07:43,517 iteration 1451 : loss : 0.063274, loss_ce: 0.026153
2022-01-20 20:07:44,806 iteration 1452 : loss : 0.062688, loss_ce: 0.019066
2022-01-20 20:07:46,095 iteration 1453 : loss : 0.069063, loss_ce: 0.033064
2022-01-20 20:07:47,429 iteration 1454 : loss : 0.093608, loss_ce: 0.036580
2022-01-20 20:07:48,709 iteration 1455 : loss : 0.047413, loss_ce: 0.020996
2022-01-20 20:07:50,098 iteration 1456 : loss : 0.110105, loss_ce: 0.036769
2022-01-20 20:07:51,405 iteration 1457 : loss : 0.074482, loss_ce: 0.035097
2022-01-20 20:07:52,694 iteration 1458 : loss : 0.054960, loss_ce: 0.019375
2022-01-20 20:07:53,954 iteration 1459 : loss : 0.064009, loss_ce: 0.019766
2022-01-20 20:07:55,335 iteration 1460 : loss : 0.103680, loss_ce: 0.039679
2022-01-20 20:07:56,657 iteration 1461 : loss : 0.071032, loss_ce: 0.029221
2022-01-20 20:07:58,009 iteration 1462 : loss : 0.050296, loss_ce: 0.022169
 22%|██████▍                       | 86/400 [35:07<2:09:18, 24.71s/it]2022-01-20 20:07:59,345 iteration 1463 : loss : 0.067183, loss_ce: 0.027622
2022-01-20 20:08:00,632 iteration 1464 : loss : 0.051185, loss_ce: 0.020097
2022-01-20 20:08:01,964 iteration 1465 : loss : 0.070661, loss_ce: 0.018250
2022-01-20 20:08:03,296 iteration 1466 : loss : 0.043007, loss_ce: 0.016787
2022-01-20 20:08:04,605 iteration 1467 : loss : 0.058644, loss_ce: 0.025826
2022-01-20 20:08:05,897 iteration 1468 : loss : 0.053827, loss_ce: 0.023833
2022-01-20 20:08:07,196 iteration 1469 : loss : 0.054271, loss_ce: 0.019239
2022-01-20 20:08:08,531 iteration 1470 : loss : 0.104781, loss_ce: 0.034050
2022-01-20 20:08:09,866 iteration 1471 : loss : 0.068157, loss_ce: 0.036093
2022-01-20 20:08:11,081 iteration 1472 : loss : 0.059960, loss_ce: 0.022236
2022-01-20 20:08:12,468 iteration 1473 : loss : 0.051747, loss_ce: 0.022135
2022-01-20 20:08:13,825 iteration 1474 : loss : 0.074808, loss_ce: 0.026725
2022-01-20 20:08:15,138 iteration 1475 : loss : 0.059180, loss_ce: 0.027466
2022-01-20 20:08:16,533 iteration 1476 : loss : 0.101402, loss_ce: 0.034280
2022-01-20 20:08:17,872 iteration 1477 : loss : 0.061084, loss_ce: 0.020102
2022-01-20 20:08:19,244 iteration 1478 : loss : 0.060544, loss_ce: 0.024451
2022-01-20 20:08:20,521 iteration 1479 : loss : 0.072664, loss_ce: 0.025889
 22%|██████▌                       | 87/400 [35:30<2:05:27, 24.05s/it]2022-01-20 20:08:21,873 iteration 1480 : loss : 0.069312, loss_ce: 0.031108
2022-01-20 20:08:23,164 iteration 1481 : loss : 0.055302, loss_ce: 0.023905
2022-01-20 20:08:24,464 iteration 1482 : loss : 0.048431, loss_ce: 0.020338
2022-01-20 20:08:25,843 iteration 1483 : loss : 0.059118, loss_ce: 0.021813
2022-01-20 20:08:27,277 iteration 1484 : loss : 0.062363, loss_ce: 0.021957
2022-01-20 20:08:28,648 iteration 1485 : loss : 0.063620, loss_ce: 0.030131
2022-01-20 20:08:29,943 iteration 1486 : loss : 0.058784, loss_ce: 0.023221
2022-01-20 20:08:31,255 iteration 1487 : loss : 0.069004, loss_ce: 0.031617
2022-01-20 20:08:32,585 iteration 1488 : loss : 0.052968, loss_ce: 0.023379
2022-01-20 20:08:33,915 iteration 1489 : loss : 0.065394, loss_ce: 0.026793
2022-01-20 20:08:35,261 iteration 1490 : loss : 0.069096, loss_ce: 0.030298
2022-01-20 20:08:36,642 iteration 1491 : loss : 0.066402, loss_ce: 0.027606
2022-01-20 20:08:37,920 iteration 1492 : loss : 0.051072, loss_ce: 0.021136
2022-01-20 20:08:39,257 iteration 1493 : loss : 0.063536, loss_ce: 0.022559
2022-01-20 20:08:40,540 iteration 1494 : loss : 0.055505, loss_ce: 0.023018
2022-01-20 20:08:41,835 iteration 1495 : loss : 0.058954, loss_ce: 0.023023
2022-01-20 20:08:43,143 iteration 1496 : loss : 0.106733, loss_ce: 0.029211
 22%|██████▌                       | 88/400 [35:52<2:02:49, 23.62s/it]2022-01-20 20:08:44,451 iteration 1497 : loss : 0.061460, loss_ce: 0.030566
2022-01-20 20:08:45,740 iteration 1498 : loss : 0.045973, loss_ce: 0.013670
2022-01-20 20:08:47,086 iteration 1499 : loss : 0.077572, loss_ce: 0.035232
2022-01-20 20:08:48,332 iteration 1500 : loss : 0.069995, loss_ce: 0.020980
2022-01-20 20:08:49,631 iteration 1501 : loss : 0.051162, loss_ce: 0.018315
2022-01-20 20:08:50,973 iteration 1502 : loss : 0.056246, loss_ce: 0.019925
2022-01-20 20:08:52,258 iteration 1503 : loss : 0.038673, loss_ce: 0.017162
2022-01-20 20:08:53,542 iteration 1504 : loss : 0.045666, loss_ce: 0.014207
2022-01-20 20:08:54,796 iteration 1505 : loss : 0.049354, loss_ce: 0.017951
2022-01-20 20:08:56,107 iteration 1506 : loss : 0.050656, loss_ce: 0.018088
2022-01-20 20:08:57,422 iteration 1507 : loss : 0.067853, loss_ce: 0.022452
2022-01-20 20:08:58,708 iteration 1508 : loss : 0.060948, loss_ce: 0.025473
2022-01-20 20:09:00,016 iteration 1509 : loss : 0.062279, loss_ce: 0.021660
2022-01-20 20:09:01,284 iteration 1510 : loss : 0.037174, loss_ce: 0.013885
2022-01-20 20:09:02,615 iteration 1511 : loss : 0.053558, loss_ce: 0.026123
2022-01-20 20:09:03,884 iteration 1512 : loss : 0.058991, loss_ce: 0.024753
2022-01-20 20:09:05,197 iteration 1513 : loss : 0.048791, loss_ce: 0.023532
 22%|██████▋                       | 89/400 [36:15<1:59:59, 23.15s/it]2022-01-20 20:09:06,521 iteration 1514 : loss : 0.078274, loss_ce: 0.028772
2022-01-20 20:09:07,821 iteration 1515 : loss : 0.059618, loss_ce: 0.023142
2022-01-20 20:09:09,140 iteration 1516 : loss : 0.054258, loss_ce: 0.020418
2022-01-20 20:09:10,443 iteration 1517 : loss : 0.047930, loss_ce: 0.020345
2022-01-20 20:09:11,788 iteration 1518 : loss : 0.057062, loss_ce: 0.016101
2022-01-20 20:09:13,083 iteration 1519 : loss : 0.045685, loss_ce: 0.017574
2022-01-20 20:09:14,365 iteration 1520 : loss : 0.056999, loss_ce: 0.017970
2022-01-20 20:09:15,643 iteration 1521 : loss : 0.055593, loss_ce: 0.018882
2022-01-20 20:09:17,032 iteration 1522 : loss : 0.053554, loss_ce: 0.017643
2022-01-20 20:09:18,296 iteration 1523 : loss : 0.049758, loss_ce: 0.022741
2022-01-20 20:09:19,643 iteration 1524 : loss : 0.068885, loss_ce: 0.027123
2022-01-20 20:09:20,923 iteration 1525 : loss : 0.046163, loss_ce: 0.020061
2022-01-20 20:09:22,288 iteration 1526 : loss : 0.056779, loss_ce: 0.023515
2022-01-20 20:09:23,651 iteration 1527 : loss : 0.076430, loss_ce: 0.037582
2022-01-20 20:09:24,994 iteration 1528 : loss : 0.062963, loss_ce: 0.023583
2022-01-20 20:09:26,260 iteration 1529 : loss : 0.077804, loss_ce: 0.029396
2022-01-20 20:09:26,261 Training Data Eval:
2022-01-20 20:09:32,760   Average segmentation loss on training set: 0.0462
2022-01-20 20:09:32,761 Validation Data Eval:
2022-01-20 20:09:35,012   Average segmentation loss on validation set: 0.0826
2022-01-20 20:09:40,836 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 20:09:42,064 iteration 1530 : loss : 0.067376, loss_ce: 0.030922
 22%|██████▊                       | 90/400 [36:51<2:20:52, 27.27s/it]2022-01-20 20:09:43,456 iteration 1531 : loss : 0.057762, loss_ce: 0.024078
2022-01-20 20:09:44,734 iteration 1532 : loss : 0.059419, loss_ce: 0.025033
2022-01-20 20:09:45,936 iteration 1533 : loss : 0.043439, loss_ce: 0.020392
2022-01-20 20:09:47,224 iteration 1534 : loss : 0.088571, loss_ce: 0.034919
2022-01-20 20:09:48,448 iteration 1535 : loss : 0.045465, loss_ce: 0.018903
2022-01-20 20:09:49,660 iteration 1536 : loss : 0.076888, loss_ce: 0.041629
2022-01-20 20:09:50,865 iteration 1537 : loss : 0.086484, loss_ce: 0.027071
2022-01-20 20:09:52,050 iteration 1538 : loss : 0.199631, loss_ce: 0.052506
2022-01-20 20:09:53,274 iteration 1539 : loss : 0.056961, loss_ce: 0.022286
2022-01-20 20:09:54,506 iteration 1540 : loss : 0.068588, loss_ce: 0.035568
2022-01-20 20:09:55,658 iteration 1541 : loss : 0.052351, loss_ce: 0.018861
2022-01-20 20:09:56,898 iteration 1542 : loss : 0.057445, loss_ce: 0.027153
2022-01-20 20:09:58,108 iteration 1543 : loss : 0.069092, loss_ce: 0.022563
2022-01-20 20:09:59,309 iteration 1544 : loss : 0.050426, loss_ce: 0.019234
2022-01-20 20:10:00,613 iteration 1545 : loss : 0.061638, loss_ce: 0.025854
2022-01-20 20:10:01,923 iteration 1546 : loss : 0.054580, loss_ce: 0.024667
2022-01-20 20:10:03,224 iteration 1547 : loss : 0.056420, loss_ce: 0.021573
 23%|██████▊                       | 91/400 [37:13<2:10:59, 25.43s/it]2022-01-20 20:10:04,585 iteration 1548 : loss : 0.047772, loss_ce: 0.018888
2022-01-20 20:10:05,843 iteration 1549 : loss : 0.047403, loss_ce: 0.021918
2022-01-20 20:10:07,248 iteration 1550 : loss : 0.076514, loss_ce: 0.028896
2022-01-20 20:10:08,556 iteration 1551 : loss : 0.060506, loss_ce: 0.025909
2022-01-20 20:10:09,877 iteration 1552 : loss : 0.046570, loss_ce: 0.021052
2022-01-20 20:10:11,116 iteration 1553 : loss : 0.039961, loss_ce: 0.021482
2022-01-20 20:10:12,389 iteration 1554 : loss : 0.072502, loss_ce: 0.029289
2022-01-20 20:10:13,765 iteration 1555 : loss : 0.051307, loss_ce: 0.018264
2022-01-20 20:10:15,193 iteration 1556 : loss : 0.070761, loss_ce: 0.024175
2022-01-20 20:10:16,563 iteration 1557 : loss : 0.097454, loss_ce: 0.027037
2022-01-20 20:10:17,884 iteration 1558 : loss : 0.063010, loss_ce: 0.017822
2022-01-20 20:10:19,286 iteration 1559 : loss : 0.066578, loss_ce: 0.028096
2022-01-20 20:10:20,568 iteration 1560 : loss : 0.052391, loss_ce: 0.017940
2022-01-20 20:10:21,853 iteration 1561 : loss : 0.066417, loss_ce: 0.022030
2022-01-20 20:10:23,173 iteration 1562 : loss : 0.061714, loss_ce: 0.025972
2022-01-20 20:10:24,474 iteration 1563 : loss : 0.082351, loss_ce: 0.043415
2022-01-20 20:10:25,789 iteration 1564 : loss : 0.072904, loss_ce: 0.022438
 23%|██████▉                       | 92/400 [37:35<2:06:08, 24.57s/it]2022-01-20 20:10:27,154 iteration 1565 : loss : 0.050351, loss_ce: 0.019030
2022-01-20 20:10:28,445 iteration 1566 : loss : 0.063526, loss_ce: 0.025470
2022-01-20 20:10:29,796 iteration 1567 : loss : 0.066601, loss_ce: 0.019436
2022-01-20 20:10:31,211 iteration 1568 : loss : 0.050744, loss_ce: 0.021534
2022-01-20 20:10:32,490 iteration 1569 : loss : 0.061952, loss_ce: 0.026574
2022-01-20 20:10:33,819 iteration 1570 : loss : 0.044826, loss_ce: 0.017786
2022-01-20 20:10:35,122 iteration 1571 : loss : 0.056160, loss_ce: 0.022763
2022-01-20 20:10:36,427 iteration 1572 : loss : 0.063096, loss_ce: 0.030023
2022-01-20 20:10:37,735 iteration 1573 : loss : 0.070955, loss_ce: 0.020753
2022-01-20 20:10:39,044 iteration 1574 : loss : 0.059169, loss_ce: 0.029773
2022-01-20 20:10:40,384 iteration 1575 : loss : 0.071921, loss_ce: 0.029412
2022-01-20 20:10:41,680 iteration 1576 : loss : 0.064758, loss_ce: 0.018478
2022-01-20 20:10:42,950 iteration 1577 : loss : 0.075520, loss_ce: 0.020979
2022-01-20 20:10:44,176 iteration 1578 : loss : 0.042701, loss_ce: 0.017020
2022-01-20 20:10:45,484 iteration 1579 : loss : 0.046269, loss_ce: 0.016161
2022-01-20 20:10:46,810 iteration 1580 : loss : 0.054243, loss_ce: 0.017281
2022-01-20 20:10:48,138 iteration 1581 : loss : 0.055971, loss_ce: 0.025169
 23%|██████▉                       | 93/400 [37:57<2:02:19, 23.91s/it]2022-01-20 20:10:49,530 iteration 1582 : loss : 0.050622, loss_ce: 0.022749
2022-01-20 20:10:50,838 iteration 1583 : loss : 0.057686, loss_ce: 0.017115
2022-01-20 20:10:52,093 iteration 1584 : loss : 0.042381, loss_ce: 0.013710
2022-01-20 20:10:53,374 iteration 1585 : loss : 0.052085, loss_ce: 0.023879
2022-01-20 20:10:54,605 iteration 1586 : loss : 0.050070, loss_ce: 0.022820
2022-01-20 20:10:55,901 iteration 1587 : loss : 0.050487, loss_ce: 0.018095
2022-01-20 20:10:57,195 iteration 1588 : loss : 0.049232, loss_ce: 0.014841
2022-01-20 20:10:58,491 iteration 1589 : loss : 0.051647, loss_ce: 0.021215
2022-01-20 20:10:59,868 iteration 1590 : loss : 0.054310, loss_ce: 0.019072
2022-01-20 20:11:01,154 iteration 1591 : loss : 0.052000, loss_ce: 0.025004
2022-01-20 20:11:02,385 iteration 1592 : loss : 0.036282, loss_ce: 0.015953
2022-01-20 20:11:03,706 iteration 1593 : loss : 0.047990, loss_ce: 0.016296
2022-01-20 20:11:05,005 iteration 1594 : loss : 0.055744, loss_ce: 0.020365
2022-01-20 20:11:06,263 iteration 1595 : loss : 0.059680, loss_ce: 0.031380
2022-01-20 20:11:07,502 iteration 1596 : loss : 0.058598, loss_ce: 0.020140
2022-01-20 20:11:08,837 iteration 1597 : loss : 0.093386, loss_ce: 0.025106
2022-01-20 20:11:10,185 iteration 1598 : loss : 0.077807, loss_ce: 0.029554
 24%|███████                       | 94/400 [38:20<1:59:04, 23.35s/it]2022-01-20 20:11:11,578 iteration 1599 : loss : 0.055587, loss_ce: 0.024869
2022-01-20 20:11:12,881 iteration 1600 : loss : 0.043888, loss_ce: 0.019654
2022-01-20 20:11:14,227 iteration 1601 : loss : 0.072408, loss_ce: 0.026679
2022-01-20 20:11:15,541 iteration 1602 : loss : 0.101124, loss_ce: 0.032381
2022-01-20 20:11:16,843 iteration 1603 : loss : 0.044874, loss_ce: 0.015814
2022-01-20 20:11:18,122 iteration 1604 : loss : 0.081534, loss_ce: 0.020058
2022-01-20 20:11:19,412 iteration 1605 : loss : 0.053003, loss_ce: 0.017901
2022-01-20 20:11:20,734 iteration 1606 : loss : 0.042288, loss_ce: 0.016862
2022-01-20 20:11:22,031 iteration 1607 : loss : 0.074540, loss_ce: 0.029879
2022-01-20 20:11:23,417 iteration 1608 : loss : 0.090534, loss_ce: 0.046816
2022-01-20 20:11:24,708 iteration 1609 : loss : 0.052649, loss_ce: 0.020453
2022-01-20 20:11:26,099 iteration 1610 : loss : 0.093699, loss_ce: 0.034990
2022-01-20 20:11:27,481 iteration 1611 : loss : 0.089210, loss_ce: 0.040465
2022-01-20 20:11:28,881 iteration 1612 : loss : 0.053536, loss_ce: 0.020822
2022-01-20 20:11:30,192 iteration 1613 : loss : 0.058170, loss_ce: 0.024363
2022-01-20 20:11:31,564 iteration 1614 : loss : 0.060201, loss_ce: 0.028501
2022-01-20 20:11:31,564 Training Data Eval:
2022-01-20 20:11:38,086   Average segmentation loss on training set: 0.0502
2022-01-20 20:11:38,086 Validation Data Eval:
2022-01-20 20:11:40,335   Average segmentation loss on validation set: 0.0757
2022-01-20 20:11:46,202 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 20:11:47,500 iteration 1615 : loss : 0.070519, loss_ce: 0.025787
 24%|███████▏                      | 95/400 [38:57<2:19:59, 27.54s/it]2022-01-20 20:11:48,784 iteration 1616 : loss : 0.058930, loss_ce: 0.023094
2022-01-20 20:11:50,005 iteration 1617 : loss : 0.061104, loss_ce: 0.024492
2022-01-20 20:11:51,188 iteration 1618 : loss : 0.056480, loss_ce: 0.019566
2022-01-20 20:11:52,416 iteration 1619 : loss : 0.056073, loss_ce: 0.030899
2022-01-20 20:11:53,762 iteration 1620 : loss : 0.055254, loss_ce: 0.017929
2022-01-20 20:11:55,112 iteration 1621 : loss : 0.054864, loss_ce: 0.020411
2022-01-20 20:11:56,355 iteration 1622 : loss : 0.058559, loss_ce: 0.023887
2022-01-20 20:11:57,551 iteration 1623 : loss : 0.057345, loss_ce: 0.018558
2022-01-20 20:11:58,809 iteration 1624 : loss : 0.051271, loss_ce: 0.019988
2022-01-20 20:11:59,978 iteration 1625 : loss : 0.043967, loss_ce: 0.015955
2022-01-20 20:12:01,198 iteration 1626 : loss : 0.050496, loss_ce: 0.021842
2022-01-20 20:12:02,565 iteration 1627 : loss : 0.077028, loss_ce: 0.027432
2022-01-20 20:12:03,906 iteration 1628 : loss : 0.065556, loss_ce: 0.032176
2022-01-20 20:12:05,156 iteration 1629 : loss : 0.052086, loss_ce: 0.019595
2022-01-20 20:12:06,424 iteration 1630 : loss : 0.040385, loss_ce: 0.014179
2022-01-20 20:12:07,713 iteration 1631 : loss : 0.065149, loss_ce: 0.020719
2022-01-20 20:12:09,014 iteration 1632 : loss : 0.049520, loss_ce: 0.020133
 24%|███████▏                      | 96/400 [39:18<2:10:21, 25.73s/it]2022-01-20 20:12:10,377 iteration 1633 : loss : 0.058509, loss_ce: 0.017951
2022-01-20 20:12:11,727 iteration 1634 : loss : 0.061593, loss_ce: 0.024784
2022-01-20 20:12:13,025 iteration 1635 : loss : 0.070634, loss_ce: 0.022558
2022-01-20 20:12:14,273 iteration 1636 : loss : 0.057330, loss_ce: 0.026582
2022-01-20 20:12:15,606 iteration 1637 : loss : 0.079283, loss_ce: 0.022479
2022-01-20 20:12:16,910 iteration 1638 : loss : 0.061258, loss_ce: 0.027709
2022-01-20 20:12:18,282 iteration 1639 : loss : 0.061458, loss_ce: 0.027298
2022-01-20 20:12:19,561 iteration 1640 : loss : 0.074533, loss_ce: 0.024733
2022-01-20 20:12:20,942 iteration 1641 : loss : 0.064442, loss_ce: 0.026349
2022-01-20 20:12:22,201 iteration 1642 : loss : 0.053747, loss_ce: 0.020942
2022-01-20 20:12:23,448 iteration 1643 : loss : 0.061365, loss_ce: 0.021146
2022-01-20 20:12:24,812 iteration 1644 : loss : 0.072476, loss_ce: 0.028983
2022-01-20 20:12:26,224 iteration 1645 : loss : 0.071763, loss_ce: 0.034103
2022-01-20 20:12:27,520 iteration 1646 : loss : 0.074014, loss_ce: 0.025722
2022-01-20 20:12:28,816 iteration 1647 : loss : 0.083632, loss_ce: 0.029468
2022-01-20 20:12:30,055 iteration 1648 : loss : 0.046339, loss_ce: 0.018868
2022-01-20 20:12:31,315 iteration 1649 : loss : 0.042970, loss_ce: 0.016595
 24%|███████▎                      | 97/400 [39:41<2:04:45, 24.70s/it]2022-01-20 20:12:32,775 iteration 1650 : loss : 0.077969, loss_ce: 0.039942
2022-01-20 20:12:34,067 iteration 1651 : loss : 0.102647, loss_ce: 0.039411
2022-01-20 20:12:35,391 iteration 1652 : loss : 0.069211, loss_ce: 0.025814
2022-01-20 20:12:36,667 iteration 1653 : loss : 0.055815, loss_ce: 0.017756
2022-01-20 20:12:37,972 iteration 1654 : loss : 0.054873, loss_ce: 0.017363
2022-01-20 20:12:39,236 iteration 1655 : loss : 0.055631, loss_ce: 0.020198
2022-01-20 20:12:40,585 iteration 1656 : loss : 0.080884, loss_ce: 0.038150
2022-01-20 20:12:41,870 iteration 1657 : loss : 0.051230, loss_ce: 0.022642
2022-01-20 20:12:43,220 iteration 1658 : loss : 0.098552, loss_ce: 0.058294
2022-01-20 20:12:44,603 iteration 1659 : loss : 0.053893, loss_ce: 0.023500
2022-01-20 20:12:45,925 iteration 1660 : loss : 0.036629, loss_ce: 0.015023
2022-01-20 20:12:47,183 iteration 1661 : loss : 0.061142, loss_ce: 0.021052
2022-01-20 20:12:48,527 iteration 1662 : loss : 0.068077, loss_ce: 0.022172
2022-01-20 20:12:49,891 iteration 1663 : loss : 0.049241, loss_ce: 0.022200
2022-01-20 20:12:51,204 iteration 1664 : loss : 0.062922, loss_ce: 0.022188
2022-01-20 20:12:52,506 iteration 1665 : loss : 0.057132, loss_ce: 0.025011
2022-01-20 20:12:53,821 iteration 1666 : loss : 0.060900, loss_ce: 0.021574
 24%|███████▎                      | 98/400 [40:03<2:01:00, 24.04s/it]2022-01-20 20:12:55,120 iteration 1667 : loss : 0.061316, loss_ce: 0.028997
2022-01-20 20:12:56,499 iteration 1668 : loss : 0.077194, loss_ce: 0.026476
2022-01-20 20:12:57,836 iteration 1669 : loss : 0.054745, loss_ce: 0.017946
2022-01-20 20:12:59,071 iteration 1670 : loss : 0.038604, loss_ce: 0.011971
2022-01-20 20:13:00,315 iteration 1671 : loss : 0.041904, loss_ce: 0.018374
2022-01-20 20:13:01,677 iteration 1672 : loss : 0.060708, loss_ce: 0.022062
2022-01-20 20:13:03,063 iteration 1673 : loss : 0.058400, loss_ce: 0.017459
2022-01-20 20:13:04,454 iteration 1674 : loss : 0.060883, loss_ce: 0.019840
2022-01-20 20:13:05,744 iteration 1675 : loss : 0.089783, loss_ce: 0.025839
2022-01-20 20:13:07,118 iteration 1676 : loss : 0.065181, loss_ce: 0.029812
2022-01-20 20:13:08,466 iteration 1677 : loss : 0.106584, loss_ce: 0.037445
2022-01-20 20:13:09,813 iteration 1678 : loss : 0.054920, loss_ce: 0.029135
2022-01-20 20:13:11,113 iteration 1679 : loss : 0.098121, loss_ce: 0.052504
2022-01-20 20:13:12,467 iteration 1680 : loss : 0.067493, loss_ce: 0.031317
2022-01-20 20:13:13,777 iteration 1681 : loss : 0.057366, loss_ce: 0.024108
2022-01-20 20:13:15,094 iteration 1682 : loss : 0.058758, loss_ce: 0.028225
2022-01-20 20:13:16,385 iteration 1683 : loss : 0.057122, loss_ce: 0.024967
 25%|███████▍                      | 99/400 [40:26<1:58:23, 23.60s/it]2022-01-20 20:13:17,775 iteration 1684 : loss : 0.087164, loss_ce: 0.031818
2022-01-20 20:13:19,059 iteration 1685 : loss : 0.061442, loss_ce: 0.030023
2022-01-20 20:13:20,308 iteration 1686 : loss : 0.050097, loss_ce: 0.018718
2022-01-20 20:13:21,601 iteration 1687 : loss : 0.049054, loss_ce: 0.020451
2022-01-20 20:13:22,934 iteration 1688 : loss : 0.061368, loss_ce: 0.031227
2022-01-20 20:13:24,208 iteration 1689 : loss : 0.048442, loss_ce: 0.016899
2022-01-20 20:13:25,542 iteration 1690 : loss : 0.085387, loss_ce: 0.036278
2022-01-20 20:13:26,834 iteration 1691 : loss : 0.040002, loss_ce: 0.016332
2022-01-20 20:13:28,181 iteration 1692 : loss : 0.068564, loss_ce: 0.026670
2022-01-20 20:13:29,538 iteration 1693 : loss : 0.053927, loss_ce: 0.022742
2022-01-20 20:13:30,867 iteration 1694 : loss : 0.057378, loss_ce: 0.017468
2022-01-20 20:13:32,102 iteration 1695 : loss : 0.031724, loss_ce: 0.010669
2022-01-20 20:13:33,404 iteration 1696 : loss : 0.063627, loss_ce: 0.023252
2022-01-20 20:13:34,751 iteration 1697 : loss : 0.085195, loss_ce: 0.025026
2022-01-20 20:13:36,039 iteration 1698 : loss : 0.042867, loss_ce: 0.012818
2022-01-20 20:13:37,246 iteration 1699 : loss : 0.047195, loss_ce: 0.021560
2022-01-20 20:13:37,246 Training Data Eval:
2022-01-20 20:13:43,808   Average segmentation loss on training set: 0.0450
2022-01-20 20:13:43,808 Validation Data Eval:
2022-01-20 20:13:46,062   Average segmentation loss on validation set: 0.0904
2022-01-20 20:13:47,402 iteration 1700 : loss : 0.064865, loss_ce: 0.020469
 25%|███████▎                     | 100/400 [40:57<2:09:07, 25.82s/it]2022-01-20 20:13:48,800 iteration 1701 : loss : 0.050432, loss_ce: 0.021600
2022-01-20 20:13:50,127 iteration 1702 : loss : 0.053934, loss_ce: 0.024041
2022-01-20 20:13:51,393 iteration 1703 : loss : 0.059795, loss_ce: 0.021477
2022-01-20 20:13:52,762 iteration 1704 : loss : 0.056991, loss_ce: 0.025900
2022-01-20 20:13:54,077 iteration 1705 : loss : 0.065753, loss_ce: 0.032068
2022-01-20 20:13:55,414 iteration 1706 : loss : 0.056057, loss_ce: 0.024121
2022-01-20 20:13:56,727 iteration 1707 : loss : 0.077598, loss_ce: 0.028414
2022-01-20 20:13:58,072 iteration 1708 : loss : 0.061492, loss_ce: 0.024908
2022-01-20 20:13:59,318 iteration 1709 : loss : 0.052360, loss_ce: 0.026169
2022-01-20 20:14:00,557 iteration 1710 : loss : 0.041096, loss_ce: 0.018922
2022-01-20 20:14:01,884 iteration 1711 : loss : 0.056494, loss_ce: 0.021082
2022-01-20 20:14:03,156 iteration 1712 : loss : 0.048032, loss_ce: 0.017380
2022-01-20 20:14:04,408 iteration 1713 : loss : 0.060859, loss_ce: 0.020790
2022-01-20 20:14:05,789 iteration 1714 : loss : 0.075620, loss_ce: 0.022346
2022-01-20 20:14:07,131 iteration 1715 : loss : 0.086738, loss_ce: 0.024020
2022-01-20 20:14:08,512 iteration 1716 : loss : 0.070518, loss_ce: 0.024687
2022-01-20 20:14:09,810 iteration 1717 : loss : 0.057344, loss_ce: 0.021063
 25%|███████▎                     | 101/400 [41:19<2:03:35, 24.80s/it]2022-01-20 20:14:11,126 iteration 1718 : loss : 0.046809, loss_ce: 0.015333
2022-01-20 20:14:12,450 iteration 1719 : loss : 0.068958, loss_ce: 0.029804
2022-01-20 20:14:13,668 iteration 1720 : loss : 0.045478, loss_ce: 0.016855
2022-01-20 20:14:14,979 iteration 1721 : loss : 0.058756, loss_ce: 0.025762
2022-01-20 20:14:16,334 iteration 1722 : loss : 0.054406, loss_ce: 0.017534
2022-01-20 20:14:17,648 iteration 1723 : loss : 0.096741, loss_ce: 0.040076
2022-01-20 20:14:19,007 iteration 1724 : loss : 0.054119, loss_ce: 0.017950
2022-01-20 20:14:20,440 iteration 1725 : loss : 0.086216, loss_ce: 0.027922
2022-01-20 20:14:21,807 iteration 1726 : loss : 0.093546, loss_ce: 0.025568
2022-01-20 20:14:23,114 iteration 1727 : loss : 0.057509, loss_ce: 0.028116
2022-01-20 20:14:24,405 iteration 1728 : loss : 0.057094, loss_ce: 0.016984
2022-01-20 20:14:25,721 iteration 1729 : loss : 0.052328, loss_ce: 0.022256
2022-01-20 20:14:27,063 iteration 1730 : loss : 0.046052, loss_ce: 0.019060
2022-01-20 20:14:28,451 iteration 1731 : loss : 0.060969, loss_ce: 0.020433
2022-01-20 20:14:29,763 iteration 1732 : loss : 0.041187, loss_ce: 0.020262
2022-01-20 20:14:31,151 iteration 1733 : loss : 0.058535, loss_ce: 0.026676
2022-01-20 20:14:32,426 iteration 1734 : loss : 0.043299, loss_ce: 0.015058
 26%|███████▍                     | 102/400 [41:42<1:59:54, 24.14s/it]2022-01-20 20:14:33,735 iteration 1735 : loss : 0.072833, loss_ce: 0.024239
2022-01-20 20:14:35,039 iteration 1736 : loss : 0.049182, loss_ce: 0.018304
2022-01-20 20:14:36,356 iteration 1737 : loss : 0.057049, loss_ce: 0.024105
2022-01-20 20:14:37,636 iteration 1738 : loss : 0.058115, loss_ce: 0.020720
2022-01-20 20:14:39,027 iteration 1739 : loss : 0.067890, loss_ce: 0.027968
2022-01-20 20:14:40,365 iteration 1740 : loss : 0.074023, loss_ce: 0.028089
2022-01-20 20:14:41,656 iteration 1741 : loss : 0.060392, loss_ce: 0.023212
2022-01-20 20:14:43,063 iteration 1742 : loss : 0.058925, loss_ce: 0.027594
2022-01-20 20:14:44,354 iteration 1743 : loss : 0.059914, loss_ce: 0.017622
2022-01-20 20:14:45,690 iteration 1744 : loss : 0.039999, loss_ce: 0.016149
2022-01-20 20:14:47,136 iteration 1745 : loss : 0.063171, loss_ce: 0.033291
2022-01-20 20:14:48,440 iteration 1746 : loss : 0.051004, loss_ce: 0.019797
2022-01-20 20:14:49,729 iteration 1747 : loss : 0.050542, loss_ce: 0.021773
2022-01-20 20:14:51,053 iteration 1748 : loss : 0.043770, loss_ce: 0.017963
2022-01-20 20:14:52,367 iteration 1749 : loss : 0.057403, loss_ce: 0.018832
2022-01-20 20:14:53,629 iteration 1750 : loss : 0.049125, loss_ce: 0.023148
2022-01-20 20:14:55,035 iteration 1751 : loss : 0.077955, loss_ce: 0.029610
 26%|███████▍                     | 103/400 [42:04<1:57:13, 23.68s/it]2022-01-20 20:14:56,354 iteration 1752 : loss : 0.040023, loss_ce: 0.013702
2022-01-20 20:14:57,619 iteration 1753 : loss : 0.055954, loss_ce: 0.027798
2022-01-20 20:14:58,898 iteration 1754 : loss : 0.045853, loss_ce: 0.018203
2022-01-20 20:15:00,194 iteration 1755 : loss : 0.040823, loss_ce: 0.016637
2022-01-20 20:15:01,518 iteration 1756 : loss : 0.072246, loss_ce: 0.033993
2022-01-20 20:15:02,886 iteration 1757 : loss : 0.058051, loss_ce: 0.025793
2022-01-20 20:15:04,228 iteration 1758 : loss : 0.076164, loss_ce: 0.027784
2022-01-20 20:15:05,468 iteration 1759 : loss : 0.091934, loss_ce: 0.028767
2022-01-20 20:15:06,765 iteration 1760 : loss : 0.058255, loss_ce: 0.022643
2022-01-20 20:15:08,042 iteration 1761 : loss : 0.038554, loss_ce: 0.016143
2022-01-20 20:15:09,348 iteration 1762 : loss : 0.050528, loss_ce: 0.018240
2022-01-20 20:15:10,583 iteration 1763 : loss : 0.058370, loss_ce: 0.025588
2022-01-20 20:15:11,976 iteration 1764 : loss : 0.035674, loss_ce: 0.010642
2022-01-20 20:15:13,284 iteration 1765 : loss : 0.060961, loss_ce: 0.023015
2022-01-20 20:15:14,648 iteration 1766 : loss : 0.044678, loss_ce: 0.018526
2022-01-20 20:15:15,982 iteration 1767 : loss : 0.046882, loss_ce: 0.018742
2022-01-20 20:15:17,296 iteration 1768 : loss : 0.059450, loss_ce: 0.022389
 26%|███████▌                     | 104/400 [42:27<1:54:44, 23.26s/it]2022-01-20 20:15:18,686 iteration 1769 : loss : 0.051667, loss_ce: 0.019752
2022-01-20 20:15:19,989 iteration 1770 : loss : 0.033198, loss_ce: 0.010944
2022-01-20 20:15:21,259 iteration 1771 : loss : 0.034719, loss_ce: 0.010754
2022-01-20 20:15:22,627 iteration 1772 : loss : 0.068811, loss_ce: 0.032076
2022-01-20 20:15:23,968 iteration 1773 : loss : 0.057228, loss_ce: 0.022013
2022-01-20 20:15:25,259 iteration 1774 : loss : 0.061955, loss_ce: 0.020989
2022-01-20 20:15:26,571 iteration 1775 : loss : 0.045334, loss_ce: 0.015934
2022-01-20 20:15:27,869 iteration 1776 : loss : 0.053487, loss_ce: 0.020205
2022-01-20 20:15:29,132 iteration 1777 : loss : 0.042558, loss_ce: 0.017213
2022-01-20 20:15:30,485 iteration 1778 : loss : 0.041218, loss_ce: 0.017501
2022-01-20 20:15:31,801 iteration 1779 : loss : 0.048392, loss_ce: 0.022138
2022-01-20 20:15:33,141 iteration 1780 : loss : 0.085105, loss_ce: 0.024159
2022-01-20 20:15:34,484 iteration 1781 : loss : 0.055108, loss_ce: 0.017656
2022-01-20 20:15:35,719 iteration 1782 : loss : 0.106568, loss_ce: 0.024559
2022-01-20 20:15:36,987 iteration 1783 : loss : 0.067621, loss_ce: 0.032652
2022-01-20 20:15:38,317 iteration 1784 : loss : 0.080602, loss_ce: 0.043639
2022-01-20 20:15:38,318 Training Data Eval:
2022-01-20 20:15:44,831   Average segmentation loss on training set: 0.0385
2022-01-20 20:15:44,832 Validation Data Eval:
2022-01-20 20:15:47,071   Average segmentation loss on validation set: 0.1215
2022-01-20 20:15:48,393 iteration 1785 : loss : 0.055700, loss_ce: 0.028883
 26%|███████▌                     | 105/400 [42:58<2:05:54, 25.61s/it]2022-01-20 20:15:49,788 iteration 1786 : loss : 0.065016, loss_ce: 0.023596
2022-01-20 20:15:51,057 iteration 1787 : loss : 0.047660, loss_ce: 0.010665
2022-01-20 20:15:52,377 iteration 1788 : loss : 0.058906, loss_ce: 0.019264
2022-01-20 20:15:53,710 iteration 1789 : loss : 0.049422, loss_ce: 0.021717
2022-01-20 20:15:55,152 iteration 1790 : loss : 0.038488, loss_ce: 0.015905
2022-01-20 20:15:56,478 iteration 1791 : loss : 0.069529, loss_ce: 0.029708
2022-01-20 20:15:57,816 iteration 1792 : loss : 0.044262, loss_ce: 0.017741
2022-01-20 20:15:59,170 iteration 1793 : loss : 0.092249, loss_ce: 0.045136
2022-01-20 20:16:00,483 iteration 1794 : loss : 0.054035, loss_ce: 0.028498
2022-01-20 20:16:01,711 iteration 1795 : loss : 0.038917, loss_ce: 0.016775
2022-01-20 20:16:03,100 iteration 1796 : loss : 0.051758, loss_ce: 0.024756
2022-01-20 20:16:04,375 iteration 1797 : loss : 0.052593, loss_ce: 0.023041
2022-01-20 20:16:05,687 iteration 1798 : loss : 0.061850, loss_ce: 0.022740
2022-01-20 20:16:07,060 iteration 1799 : loss : 0.045150, loss_ce: 0.016994
2022-01-20 20:16:08,317 iteration 1800 : loss : 0.047261, loss_ce: 0.013765
2022-01-20 20:16:09,617 iteration 1801 : loss : 0.076049, loss_ce: 0.023990
2022-01-20 20:16:10,963 iteration 1802 : loss : 0.044339, loss_ce: 0.015544
 26%|███████▋                     | 106/400 [43:20<2:01:00, 24.70s/it]2022-01-20 20:16:12,349 iteration 1803 : loss : 0.058172, loss_ce: 0.021351
2022-01-20 20:16:13,599 iteration 1804 : loss : 0.045478, loss_ce: 0.015380
2022-01-20 20:16:15,019 iteration 1805 : loss : 0.061738, loss_ce: 0.022673
2022-01-20 20:16:16,409 iteration 1806 : loss : 0.060766, loss_ce: 0.029997
2022-01-20 20:16:17,715 iteration 1807 : loss : 0.043099, loss_ce: 0.018283
2022-01-20 20:16:19,058 iteration 1808 : loss : 0.045459, loss_ce: 0.019882
2022-01-20 20:16:20,346 iteration 1809 : loss : 0.059119, loss_ce: 0.027824
2022-01-20 20:16:21,679 iteration 1810 : loss : 0.091212, loss_ce: 0.044620
2022-01-20 20:16:22,981 iteration 1811 : loss : 0.054167, loss_ce: 0.019389
2022-01-20 20:16:24,277 iteration 1812 : loss : 0.053398, loss_ce: 0.016210
2022-01-20 20:16:25,582 iteration 1813 : loss : 0.051266, loss_ce: 0.024236
2022-01-20 20:16:26,874 iteration 1814 : loss : 0.044466, loss_ce: 0.016845
2022-01-20 20:16:28,249 iteration 1815 : loss : 0.074979, loss_ce: 0.025648
2022-01-20 20:16:29,574 iteration 1816 : loss : 0.042376, loss_ce: 0.016846
2022-01-20 20:16:30,951 iteration 1817 : loss : 0.077416, loss_ce: 0.032699
2022-01-20 20:16:32,212 iteration 1818 : loss : 0.034826, loss_ce: 0.012677
2022-01-20 20:16:33,564 iteration 1819 : loss : 0.084980, loss_ce: 0.033950
 27%|███████▊                     | 107/400 [43:43<1:57:32, 24.07s/it]2022-01-20 20:16:34,818 iteration 1820 : loss : 0.046735, loss_ce: 0.018317
2022-01-20 20:16:36,231 iteration 1821 : loss : 0.050173, loss_ce: 0.018315
2022-01-20 20:16:37,580 iteration 1822 : loss : 0.061251, loss_ce: 0.029563
2022-01-20 20:16:38,965 iteration 1823 : loss : 0.056703, loss_ce: 0.020633
2022-01-20 20:16:40,309 iteration 1824 : loss : 0.070532, loss_ce: 0.020762
2022-01-20 20:16:41,585 iteration 1825 : loss : 0.053605, loss_ce: 0.020318
2022-01-20 20:16:42,955 iteration 1826 : loss : 0.066087, loss_ce: 0.018559
2022-01-20 20:16:44,229 iteration 1827 : loss : 0.038942, loss_ce: 0.013644
2022-01-20 20:16:45,568 iteration 1828 : loss : 0.057207, loss_ce: 0.020680
2022-01-20 20:16:46,858 iteration 1829 : loss : 0.062152, loss_ce: 0.028299
2022-01-20 20:16:48,258 iteration 1830 : loss : 0.056635, loss_ce: 0.020570
2022-01-20 20:16:49,578 iteration 1831 : loss : 0.049366, loss_ce: 0.019453
2022-01-20 20:16:50,868 iteration 1832 : loss : 0.049029, loss_ce: 0.019676
2022-01-20 20:16:52,127 iteration 1833 : loss : 0.055972, loss_ce: 0.024340
2022-01-20 20:16:53,431 iteration 1834 : loss : 0.052082, loss_ce: 0.017561
2022-01-20 20:16:54,748 iteration 1835 : loss : 0.042189, loss_ce: 0.014830
2022-01-20 20:16:56,018 iteration 1836 : loss : 0.067409, loss_ce: 0.021887
 27%|███████▊                     | 108/400 [44:05<1:54:46, 23.58s/it]2022-01-20 20:16:57,404 iteration 1837 : loss : 0.059735, loss_ce: 0.019482
2022-01-20 20:16:58,657 iteration 1838 : loss : 0.040838, loss_ce: 0.017885
2022-01-20 20:16:59,931 iteration 1839 : loss : 0.061996, loss_ce: 0.024379
2022-01-20 20:17:01,262 iteration 1840 : loss : 0.048212, loss_ce: 0.022566
2022-01-20 20:17:02,517 iteration 1841 : loss : 0.073305, loss_ce: 0.034329
2022-01-20 20:17:03,802 iteration 1842 : loss : 0.034055, loss_ce: 0.011165
2022-01-20 20:17:05,132 iteration 1843 : loss : 0.038874, loss_ce: 0.019041
2022-01-20 20:17:06,437 iteration 1844 : loss : 0.051192, loss_ce: 0.016588
2022-01-20 20:17:07,730 iteration 1845 : loss : 0.045872, loss_ce: 0.012971
2022-01-20 20:17:09,119 iteration 1846 : loss : 0.047462, loss_ce: 0.021846
2022-01-20 20:17:10,389 iteration 1847 : loss : 0.048047, loss_ce: 0.017692
2022-01-20 20:17:11,685 iteration 1848 : loss : 0.069970, loss_ce: 0.027178
2022-01-20 20:17:12,962 iteration 1849 : loss : 0.065973, loss_ce: 0.027434
2022-01-20 20:17:14,397 iteration 1850 : loss : 0.084385, loss_ce: 0.037831
2022-01-20 20:17:15,704 iteration 1851 : loss : 0.084590, loss_ce: 0.027489
2022-01-20 20:17:17,044 iteration 1852 : loss : 0.059913, loss_ce: 0.024186
2022-01-20 20:17:18,329 iteration 1853 : loss : 0.048746, loss_ce: 0.019841
 27%|███████▉                     | 109/400 [44:28<1:52:32, 23.20s/it]2022-01-20 20:17:19,659 iteration 1854 : loss : 0.049362, loss_ce: 0.017501
2022-01-20 20:17:21,020 iteration 1855 : loss : 0.045662, loss_ce: 0.016788
2022-01-20 20:17:22,385 iteration 1856 : loss : 0.052314, loss_ce: 0.018347
2022-01-20 20:17:23,767 iteration 1857 : loss : 0.044340, loss_ce: 0.014364
2022-01-20 20:17:25,085 iteration 1858 : loss : 0.048625, loss_ce: 0.020909
2022-01-20 20:17:26,399 iteration 1859 : loss : 0.053192, loss_ce: 0.022396
2022-01-20 20:17:27,665 iteration 1860 : loss : 0.046221, loss_ce: 0.014833
2022-01-20 20:17:29,025 iteration 1861 : loss : 0.041536, loss_ce: 0.014798
2022-01-20 20:17:30,288 iteration 1862 : loss : 0.042637, loss_ce: 0.018314
2022-01-20 20:17:31,597 iteration 1863 : loss : 0.069054, loss_ce: 0.032394
2022-01-20 20:17:32,937 iteration 1864 : loss : 0.075615, loss_ce: 0.019031
2022-01-20 20:17:34,162 iteration 1865 : loss : 0.043040, loss_ce: 0.019496
2022-01-20 20:17:35,541 iteration 1866 : loss : 0.038210, loss_ce: 0.016760
2022-01-20 20:17:36,849 iteration 1867 : loss : 0.033006, loss_ce: 0.013528
2022-01-20 20:17:38,132 iteration 1868 : loss : 0.065197, loss_ce: 0.026764
2022-01-20 20:17:39,449 iteration 1869 : loss : 0.089841, loss_ce: 0.031109
2022-01-20 20:17:39,449 Training Data Eval:
2022-01-20 20:17:45,954   Average segmentation loss on training set: 0.0342
2022-01-20 20:17:45,954 Validation Data Eval:
2022-01-20 20:17:48,211   Average segmentation loss on validation set: 0.0774
2022-01-20 20:17:49,566 iteration 1870 : loss : 0.076571, loss_ce: 0.029157
 28%|███████▉                     | 110/400 [44:59<2:03:47, 25.61s/it]2022-01-20 20:17:50,961 iteration 1871 : loss : 0.031189, loss_ce: 0.013763
2022-01-20 20:17:52,361 iteration 1872 : loss : 0.091179, loss_ce: 0.027134
2022-01-20 20:17:53,616 iteration 1873 : loss : 0.044078, loss_ce: 0.017098
2022-01-20 20:17:54,989 iteration 1874 : loss : 0.057988, loss_ce: 0.025215
2022-01-20 20:17:56,223 iteration 1875 : loss : 0.067810, loss_ce: 0.025461
2022-01-20 20:17:57,605 iteration 1876 : loss : 0.052181, loss_ce: 0.015697
2022-01-20 20:17:58,963 iteration 1877 : loss : 0.077353, loss_ce: 0.033689
2022-01-20 20:18:00,257 iteration 1878 : loss : 0.053865, loss_ce: 0.023559
2022-01-20 20:18:01,549 iteration 1879 : loss : 0.049666, loss_ce: 0.018197
2022-01-20 20:18:02,824 iteration 1880 : loss : 0.061837, loss_ce: 0.027309
2022-01-20 20:18:04,124 iteration 1881 : loss : 0.071555, loss_ce: 0.028566
2022-01-20 20:18:05,393 iteration 1882 : loss : 0.042764, loss_ce: 0.016541
2022-01-20 20:18:06,632 iteration 1883 : loss : 0.058722, loss_ce: 0.019621
2022-01-20 20:18:07,975 iteration 1884 : loss : 0.058723, loss_ce: 0.023528
2022-01-20 20:18:09,208 iteration 1885 : loss : 0.039786, loss_ce: 0.016137
2022-01-20 20:18:10,596 iteration 1886 : loss : 0.041820, loss_ce: 0.018219
2022-01-20 20:18:11,964 iteration 1887 : loss : 0.055668, loss_ce: 0.026110
 28%|████████                     | 111/400 [45:21<1:58:43, 24.65s/it]2022-01-20 20:18:13,292 iteration 1888 : loss : 0.051052, loss_ce: 0.021713
2022-01-20 20:18:14,650 iteration 1889 : loss : 0.064580, loss_ce: 0.031020
2022-01-20 20:18:15,989 iteration 1890 : loss : 0.061730, loss_ce: 0.021849
2022-01-20 20:18:17,341 iteration 1891 : loss : 0.061043, loss_ce: 0.022331
2022-01-20 20:18:18,661 iteration 1892 : loss : 0.041573, loss_ce: 0.017461
2022-01-20 20:18:19,981 iteration 1893 : loss : 0.052517, loss_ce: 0.026302
2022-01-20 20:18:21,270 iteration 1894 : loss : 0.066983, loss_ce: 0.028961
2022-01-20 20:18:22,565 iteration 1895 : loss : 0.058709, loss_ce: 0.020193
2022-01-20 20:18:23,988 iteration 1896 : loss : 0.070050, loss_ce: 0.036945
2022-01-20 20:18:25,277 iteration 1897 : loss : 0.052560, loss_ce: 0.015306
2022-01-20 20:18:26,643 iteration 1898 : loss : 0.064235, loss_ce: 0.023992
2022-01-20 20:18:27,954 iteration 1899 : loss : 0.079711, loss_ce: 0.041853
2022-01-20 20:18:29,192 iteration 1900 : loss : 0.044014, loss_ce: 0.017857
2022-01-20 20:18:30,507 iteration 1901 : loss : 0.054273, loss_ce: 0.021596
2022-01-20 20:18:31,848 iteration 1902 : loss : 0.062061, loss_ce: 0.027455
2022-01-20 20:18:33,203 iteration 1903 : loss : 0.061368, loss_ce: 0.018374
2022-01-20 20:18:34,609 iteration 1904 : loss : 0.077701, loss_ce: 0.029519
 28%|████████                     | 112/400 [45:44<1:55:25, 24.05s/it]2022-01-20 20:18:35,911 iteration 1905 : loss : 0.039065, loss_ce: 0.016272
2022-01-20 20:18:37,265 iteration 1906 : loss : 0.050247, loss_ce: 0.017173
2022-01-20 20:18:38,672 iteration 1907 : loss : 0.056560, loss_ce: 0.021681
2022-01-20 20:18:40,013 iteration 1908 : loss : 0.042525, loss_ce: 0.018196
2022-01-20 20:18:41,383 iteration 1909 : loss : 0.056951, loss_ce: 0.026789
2022-01-20 20:18:42,743 iteration 1910 : loss : 0.059791, loss_ce: 0.031433
2022-01-20 20:18:44,154 iteration 1911 : loss : 0.044494, loss_ce: 0.016688
2022-01-20 20:18:45,502 iteration 1912 : loss : 0.063763, loss_ce: 0.017656
2022-01-20 20:18:46,812 iteration 1913 : loss : 0.043719, loss_ce: 0.015850
2022-01-20 20:18:48,078 iteration 1914 : loss : 0.038720, loss_ce: 0.017150
2022-01-20 20:18:49,396 iteration 1915 : loss : 0.061958, loss_ce: 0.021506
2022-01-20 20:18:50,704 iteration 1916 : loss : 0.048513, loss_ce: 0.019190
2022-01-20 20:18:52,041 iteration 1917 : loss : 0.064039, loss_ce: 0.021775
2022-01-20 20:18:53,316 iteration 1918 : loss : 0.060144, loss_ce: 0.023369
2022-01-20 20:18:54,617 iteration 1919 : loss : 0.049407, loss_ce: 0.021765
2022-01-20 20:18:55,922 iteration 1920 : loss : 0.043307, loss_ce: 0.014042
2022-01-20 20:18:57,266 iteration 1921 : loss : 0.054578, loss_ce: 0.019394
 28%|████████▏                    | 113/400 [46:07<1:53:01, 23.63s/it]2022-01-20 20:18:58,560 iteration 1922 : loss : 0.045492, loss_ce: 0.017111
2022-01-20 20:18:59,805 iteration 1923 : loss : 0.035146, loss_ce: 0.015307
2022-01-20 20:19:01,135 iteration 1924 : loss : 0.064754, loss_ce: 0.032857
2022-01-20 20:19:02,394 iteration 1925 : loss : 0.056772, loss_ce: 0.022754
2022-01-20 20:19:03,675 iteration 1926 : loss : 0.052018, loss_ce: 0.023046
2022-01-20 20:19:05,034 iteration 1927 : loss : 0.063690, loss_ce: 0.024533
2022-01-20 20:19:06,388 iteration 1928 : loss : 0.036328, loss_ce: 0.014092
2022-01-20 20:19:07,673 iteration 1929 : loss : 0.043969, loss_ce: 0.019727
2022-01-20 20:19:08,975 iteration 1930 : loss : 0.058117, loss_ce: 0.018909
2022-01-20 20:19:10,248 iteration 1931 : loss : 0.070491, loss_ce: 0.023387
2022-01-20 20:19:11,557 iteration 1932 : loss : 0.043992, loss_ce: 0.012633
2022-01-20 20:19:12,871 iteration 1933 : loss : 0.051873, loss_ce: 0.018948
2022-01-20 20:19:14,075 iteration 1934 : loss : 0.036411, loss_ce: 0.016052
2022-01-20 20:19:15,371 iteration 1935 : loss : 0.053833, loss_ce: 0.025138
2022-01-20 20:19:16,730 iteration 1936 : loss : 0.037280, loss_ce: 0.013865
2022-01-20 20:19:18,078 iteration 1937 : loss : 0.033178, loss_ce: 0.012971
2022-01-20 20:19:19,420 iteration 1938 : loss : 0.046571, loss_ce: 0.014452
 28%|████████▎                    | 114/400 [46:29<1:50:32, 23.19s/it]2022-01-20 20:19:20,715 iteration 1939 : loss : 0.032842, loss_ce: 0.014437
2022-01-20 20:19:22,011 iteration 1940 : loss : 0.039600, loss_ce: 0.013796
2022-01-20 20:19:23,376 iteration 1941 : loss : 0.041176, loss_ce: 0.019463
2022-01-20 20:19:24,692 iteration 1942 : loss : 0.060256, loss_ce: 0.024352
2022-01-20 20:19:25,929 iteration 1943 : loss : 0.040248, loss_ce: 0.014143
2022-01-20 20:19:27,187 iteration 1944 : loss : 0.040380, loss_ce: 0.016741
2022-01-20 20:19:28,467 iteration 1945 : loss : 0.043220, loss_ce: 0.020996
2022-01-20 20:19:29,767 iteration 1946 : loss : 0.055657, loss_ce: 0.021889
2022-01-20 20:19:31,064 iteration 1947 : loss : 0.057310, loss_ce: 0.020435
2022-01-20 20:19:32,345 iteration 1948 : loss : 0.046918, loss_ce: 0.018019
2022-01-20 20:19:33,580 iteration 1949 : loss : 0.040090, loss_ce: 0.020019
2022-01-20 20:19:34,928 iteration 1950 : loss : 0.046638, loss_ce: 0.018467
2022-01-20 20:19:36,321 iteration 1951 : loss : 0.063391, loss_ce: 0.023254
2022-01-20 20:19:37,580 iteration 1952 : loss : 0.060692, loss_ce: 0.015694
2022-01-20 20:19:38,903 iteration 1953 : loss : 0.048684, loss_ce: 0.023688
2022-01-20 20:19:40,273 iteration 1954 : loss : 0.076461, loss_ce: 0.025590
2022-01-20 20:19:40,274 Training Data Eval:
2022-01-20 20:19:46,762   Average segmentation loss on training set: 0.0412
2022-01-20 20:19:46,762 Validation Data Eval:
2022-01-20 20:19:49,002   Average segmentation loss on validation set: 0.1015
2022-01-20 20:19:50,311 iteration 1955 : loss : 0.035686, loss_ce: 0.013698
 29%|████████▎                    | 115/400 [47:00<2:01:07, 25.50s/it]2022-01-20 20:19:51,713 iteration 1956 : loss : 0.051628, loss_ce: 0.018232
2022-01-20 20:19:52,982 iteration 1957 : loss : 0.049170, loss_ce: 0.019210
2022-01-20 20:19:54,280 iteration 1958 : loss : 0.045669, loss_ce: 0.019685
2022-01-20 20:19:55,665 iteration 1959 : loss : 0.099696, loss_ce: 0.025712
2022-01-20 20:19:56,943 iteration 1960 : loss : 0.059386, loss_ce: 0.020757
2022-01-20 20:19:58,254 iteration 1961 : loss : 0.051901, loss_ce: 0.021220
2022-01-20 20:19:59,541 iteration 1962 : loss : 0.038064, loss_ce: 0.015448
2022-01-20 20:20:00,859 iteration 1963 : loss : 0.061475, loss_ce: 0.031964
2022-01-20 20:20:02,147 iteration 1964 : loss : 0.064412, loss_ce: 0.017986
2022-01-20 20:20:03,425 iteration 1965 : loss : 0.055490, loss_ce: 0.023480
2022-01-20 20:20:04,692 iteration 1966 : loss : 0.045084, loss_ce: 0.019399
2022-01-20 20:20:05,948 iteration 1967 : loss : 0.063462, loss_ce: 0.017279
2022-01-20 20:20:07,199 iteration 1968 : loss : 0.058488, loss_ce: 0.029375
2022-01-20 20:20:08,541 iteration 1969 : loss : 0.050550, loss_ce: 0.018920
2022-01-20 20:20:09,969 iteration 1970 : loss : 0.054399, loss_ce: 0.018523
2022-01-20 20:20:11,186 iteration 1971 : loss : 0.041154, loss_ce: 0.012551
2022-01-20 20:20:12,604 iteration 1972 : loss : 0.062607, loss_ce: 0.023560
 29%|████████▍                    | 116/400 [47:22<1:56:08, 24.54s/it]2022-01-20 20:20:13,875 iteration 1973 : loss : 0.038526, loss_ce: 0.014719
2022-01-20 20:20:15,118 iteration 1974 : loss : 0.046420, loss_ce: 0.020406
2022-01-20 20:20:16,495 iteration 1975 : loss : 0.060482, loss_ce: 0.024015
2022-01-20 20:20:17,725 iteration 1976 : loss : 0.048057, loss_ce: 0.018320
2022-01-20 20:20:19,104 iteration 1977 : loss : 0.064204, loss_ce: 0.024209
2022-01-20 20:20:20,376 iteration 1978 : loss : 0.054020, loss_ce: 0.023091
2022-01-20 20:20:21,672 iteration 1979 : loss : 0.047957, loss_ce: 0.025611
2022-01-20 20:20:23,021 iteration 1980 : loss : 0.041341, loss_ce: 0.014975
2022-01-20 20:20:24,283 iteration 1981 : loss : 0.042316, loss_ce: 0.016757
2022-01-20 20:20:25,642 iteration 1982 : loss : 0.047718, loss_ce: 0.017514
2022-01-20 20:20:27,030 iteration 1983 : loss : 0.044016, loss_ce: 0.018311
2022-01-20 20:20:28,306 iteration 1984 : loss : 0.043799, loss_ce: 0.013224
2022-01-20 20:20:29,587 iteration 1985 : loss : 0.045396, loss_ce: 0.018730
2022-01-20 20:20:30,849 iteration 1986 : loss : 0.048297, loss_ce: 0.019913
2022-01-20 20:20:32,101 iteration 1987 : loss : 0.025566, loss_ce: 0.009615
2022-01-20 20:20:33,477 iteration 1988 : loss : 0.038146, loss_ce: 0.015530
2022-01-20 20:20:34,765 iteration 1989 : loss : 0.041548, loss_ce: 0.018393
 29%|████████▍                    | 117/400 [47:44<1:52:21, 23.82s/it]2022-01-20 20:20:36,088 iteration 1990 : loss : 0.069156, loss_ce: 0.024637
2022-01-20 20:20:37,317 iteration 1991 : loss : 0.036525, loss_ce: 0.016233
2022-01-20 20:20:38,659 iteration 1992 : loss : 0.049020, loss_ce: 0.020358
2022-01-20 20:20:39,942 iteration 1993 : loss : 0.059658, loss_ce: 0.015440
2022-01-20 20:20:41,283 iteration 1994 : loss : 0.033698, loss_ce: 0.013464
2022-01-20 20:20:42,582 iteration 1995 : loss : 0.055129, loss_ce: 0.022272
2022-01-20 20:20:43,906 iteration 1996 : loss : 0.053614, loss_ce: 0.024339
2022-01-20 20:20:45,232 iteration 1997 : loss : 0.046684, loss_ce: 0.018458
2022-01-20 20:20:46,581 iteration 1998 : loss : 0.057111, loss_ce: 0.026136
2022-01-20 20:20:47,919 iteration 1999 : loss : 0.062041, loss_ce: 0.026302
2022-01-20 20:20:49,346 iteration 2000 : loss : 0.064254, loss_ce: 0.017541
2022-01-20 20:20:50,658 iteration 2001 : loss : 0.063212, loss_ce: 0.021500
2022-01-20 20:20:51,890 iteration 2002 : loss : 0.046618, loss_ce: 0.018602
2022-01-20 20:20:53,081 iteration 2003 : loss : 0.032695, loss_ce: 0.015920
2022-01-20 20:20:54,355 iteration 2004 : loss : 0.066274, loss_ce: 0.022688
2022-01-20 20:20:55,690 iteration 2005 : loss : 0.051969, loss_ce: 0.021209
2022-01-20 20:20:57,024 iteration 2006 : loss : 0.052281, loss_ce: 0.017813
 30%|████████▌                    | 118/400 [48:06<1:49:45, 23.35s/it]2022-01-20 20:20:58,417 iteration 2007 : loss : 0.040991, loss_ce: 0.013788
2022-01-20 20:20:59,827 iteration 2008 : loss : 0.048263, loss_ce: 0.016752
2022-01-20 20:21:01,129 iteration 2009 : loss : 0.042687, loss_ce: 0.018384
2022-01-20 20:21:02,426 iteration 2010 : loss : 0.099393, loss_ce: 0.033453
2022-01-20 20:21:03,802 iteration 2011 : loss : 0.052977, loss_ce: 0.018909
2022-01-20 20:21:05,104 iteration 2012 : loss : 0.053970, loss_ce: 0.028603
2022-01-20 20:21:06,330 iteration 2013 : loss : 0.049406, loss_ce: 0.019103
2022-01-20 20:21:07,616 iteration 2014 : loss : 0.062921, loss_ce: 0.022036
2022-01-20 20:21:08,965 iteration 2015 : loss : 0.035555, loss_ce: 0.014290
2022-01-20 20:21:10,261 iteration 2016 : loss : 0.074158, loss_ce: 0.027691
2022-01-20 20:21:11,609 iteration 2017 : loss : 0.046025, loss_ce: 0.014266
2022-01-20 20:21:13,020 iteration 2018 : loss : 0.071345, loss_ce: 0.035239
2022-01-20 20:21:14,307 iteration 2019 : loss : 0.068772, loss_ce: 0.022710
2022-01-20 20:21:15,588 iteration 2020 : loss : 0.043102, loss_ce: 0.016456
2022-01-20 20:21:16,937 iteration 2021 : loss : 0.072985, loss_ce: 0.027835
2022-01-20 20:21:18,290 iteration 2022 : loss : 0.047260, loss_ce: 0.019489
2022-01-20 20:21:19,672 iteration 2023 : loss : 0.057045, loss_ce: 0.022330
 30%|████████▋                    | 119/400 [48:29<1:48:22, 23.14s/it]2022-01-20 20:21:20,973 iteration 2024 : loss : 0.050763, loss_ce: 0.019071
2022-01-20 20:21:22,292 iteration 2025 : loss : 0.041349, loss_ce: 0.019961
2022-01-20 20:21:23,652 iteration 2026 : loss : 0.050247, loss_ce: 0.021256
2022-01-20 20:21:24,949 iteration 2027 : loss : 0.038668, loss_ce: 0.011026
2022-01-20 20:21:26,199 iteration 2028 : loss : 0.039491, loss_ce: 0.020495
2022-01-20 20:21:27,486 iteration 2029 : loss : 0.039181, loss_ce: 0.016062
2022-01-20 20:21:28,759 iteration 2030 : loss : 0.054331, loss_ce: 0.022230
2022-01-20 20:21:30,057 iteration 2031 : loss : 0.047406, loss_ce: 0.017772
2022-01-20 20:21:31,442 iteration 2032 : loss : 0.065980, loss_ce: 0.032617
2022-01-20 20:21:32,859 iteration 2033 : loss : 0.056574, loss_ce: 0.025481
2022-01-20 20:21:34,132 iteration 2034 : loss : 0.036578, loss_ce: 0.014624
2022-01-20 20:21:35,440 iteration 2035 : loss : 0.043098, loss_ce: 0.015004
2022-01-20 20:21:36,788 iteration 2036 : loss : 0.043563, loss_ce: 0.019303
2022-01-20 20:21:38,093 iteration 2037 : loss : 0.085413, loss_ce: 0.025366
2022-01-20 20:21:39,438 iteration 2038 : loss : 0.053461, loss_ce: 0.021243
2022-01-20 20:21:40,823 iteration 2039 : loss : 0.044495, loss_ce: 0.016151
2022-01-20 20:21:40,823 Training Data Eval:
2022-01-20 20:21:47,287   Average segmentation loss on training set: 0.0431
2022-01-20 20:21:47,287 Validation Data Eval:
2022-01-20 20:21:49,519   Average segmentation loss on validation set: 0.1350
2022-01-20 20:21:50,916 iteration 2040 : loss : 0.049762, loss_ce: 0.017957
 30%|████████▋                    | 120/400 [49:00<1:59:21, 25.58s/it]2022-01-20 20:21:52,309 iteration 2041 : loss : 0.055638, loss_ce: 0.013062
2022-01-20 20:21:53,597 iteration 2042 : loss : 0.044899, loss_ce: 0.016145
2022-01-20 20:21:54,884 iteration 2043 : loss : 0.037527, loss_ce: 0.013006
2022-01-20 20:21:56,198 iteration 2044 : loss : 0.052400, loss_ce: 0.020110
2022-01-20 20:21:57,490 iteration 2045 : loss : 0.035624, loss_ce: 0.011149
2022-01-20 20:21:58,815 iteration 2046 : loss : 0.071633, loss_ce: 0.039424
2022-01-20 20:22:00,105 iteration 2047 : loss : 0.062586, loss_ce: 0.028870
2022-01-20 20:22:01,426 iteration 2048 : loss : 0.052221, loss_ce: 0.019583
2022-01-20 20:22:02,708 iteration 2049 : loss : 0.056726, loss_ce: 0.014441
2022-01-20 20:22:04,048 iteration 2050 : loss : 0.054208, loss_ce: 0.021816
2022-01-20 20:22:05,374 iteration 2051 : loss : 0.048556, loss_ce: 0.021676
2022-01-20 20:22:06,622 iteration 2052 : loss : 0.075396, loss_ce: 0.032665
2022-01-20 20:22:07,936 iteration 2053 : loss : 0.062528, loss_ce: 0.019244
2022-01-20 20:22:09,214 iteration 2054 : loss : 0.038766, loss_ce: 0.014500
2022-01-20 20:22:10,501 iteration 2055 : loss : 0.061114, loss_ce: 0.016374
2022-01-20 20:22:11,856 iteration 2056 : loss : 0.042314, loss_ce: 0.019173
2022-01-20 20:22:13,125 iteration 2057 : loss : 0.043683, loss_ce: 0.018231
 30%|████████▊                    | 121/400 [49:22<1:54:12, 24.56s/it]2022-01-20 20:22:14,483 iteration 2058 : loss : 0.044999, loss_ce: 0.022801
2022-01-20 20:22:15,846 iteration 2059 : loss : 0.069037, loss_ce: 0.026836
2022-01-20 20:22:17,074 iteration 2060 : loss : 0.057711, loss_ce: 0.024109
2022-01-20 20:22:18,403 iteration 2061 : loss : 0.097598, loss_ce: 0.036878
2022-01-20 20:22:19,692 iteration 2062 : loss : 0.064218, loss_ce: 0.023912
2022-01-20 20:22:20,992 iteration 2063 : loss : 0.101072, loss_ce: 0.025472
2022-01-20 20:22:22,258 iteration 2064 : loss : 0.031896, loss_ce: 0.010830
2022-01-20 20:22:23,549 iteration 2065 : loss : 0.042860, loss_ce: 0.012228
2022-01-20 20:22:24,865 iteration 2066 : loss : 0.047840, loss_ce: 0.018354
2022-01-20 20:22:26,198 iteration 2067 : loss : 0.044064, loss_ce: 0.016341
2022-01-20 20:22:27,567 iteration 2068 : loss : 0.047956, loss_ce: 0.019938
2022-01-20 20:22:28,844 iteration 2069 : loss : 0.058855, loss_ce: 0.015638
2022-01-20 20:22:30,195 iteration 2070 : loss : 0.044667, loss_ce: 0.018457
2022-01-20 20:22:31,561 iteration 2071 : loss : 0.043834, loss_ce: 0.015623
2022-01-20 20:22:32,886 iteration 2072 : loss : 0.043876, loss_ce: 0.016168
2022-01-20 20:22:34,241 iteration 2073 : loss : 0.059581, loss_ce: 0.019598
2022-01-20 20:22:35,578 iteration 2074 : loss : 0.056412, loss_ce: 0.024586
 30%|████████▊                    | 122/400 [49:45<1:50:53, 23.93s/it]2022-01-20 20:22:36,895 iteration 2075 : loss : 0.036213, loss_ce: 0.013126
2022-01-20 20:22:38,217 iteration 2076 : loss : 0.043137, loss_ce: 0.014341
2022-01-20 20:22:39,554 iteration 2077 : loss : 0.044484, loss_ce: 0.016976
2022-01-20 20:22:40,966 iteration 2078 : loss : 0.046341, loss_ce: 0.014525
2022-01-20 20:22:42,248 iteration 2079 : loss : 0.046801, loss_ce: 0.014635
2022-01-20 20:22:43,593 iteration 2080 : loss : 0.056405, loss_ce: 0.020816
2022-01-20 20:22:44,815 iteration 2081 : loss : 0.045632, loss_ce: 0.015702
2022-01-20 20:22:46,033 iteration 2082 : loss : 0.026247, loss_ce: 0.009452
2022-01-20 20:22:47,329 iteration 2083 : loss : 0.043464, loss_ce: 0.021543
2022-01-20 20:22:48,630 iteration 2084 : loss : 0.061528, loss_ce: 0.029738
2022-01-20 20:22:49,893 iteration 2085 : loss : 0.063151, loss_ce: 0.023529
2022-01-20 20:22:51,193 iteration 2086 : loss : 0.052272, loss_ce: 0.021370
2022-01-20 20:22:52,460 iteration 2087 : loss : 0.051192, loss_ce: 0.019388
2022-01-20 20:22:53,826 iteration 2088 : loss : 0.039250, loss_ce: 0.018124
2022-01-20 20:22:55,168 iteration 2089 : loss : 0.040030, loss_ce: 0.017022
2022-01-20 20:22:56,415 iteration 2090 : loss : 0.038127, loss_ce: 0.015368
2022-01-20 20:22:57,769 iteration 2091 : loss : 0.048815, loss_ce: 0.020460
 31%|████████▉                    | 123/400 [50:07<1:48:04, 23.41s/it]2022-01-20 20:22:59,067 iteration 2092 : loss : 0.026780, loss_ce: 0.011156
2022-01-20 20:23:00,362 iteration 2093 : loss : 0.051751, loss_ce: 0.021343
2022-01-20 20:23:01,669 iteration 2094 : loss : 0.037109, loss_ce: 0.010269
2022-01-20 20:23:02,919 iteration 2095 : loss : 0.030184, loss_ce: 0.012793
2022-01-20 20:23:04,310 iteration 2096 : loss : 0.040380, loss_ce: 0.017078
2022-01-20 20:23:05,649 iteration 2097 : loss : 0.069776, loss_ce: 0.025988
2022-01-20 20:23:06,960 iteration 2098 : loss : 0.057106, loss_ce: 0.020250
2022-01-20 20:23:08,283 iteration 2099 : loss : 0.052121, loss_ce: 0.018104
2022-01-20 20:23:09,626 iteration 2100 : loss : 0.045169, loss_ce: 0.022974
2022-01-20 20:23:10,952 iteration 2101 : loss : 0.038759, loss_ce: 0.012819
2022-01-20 20:23:12,318 iteration 2102 : loss : 0.053369, loss_ce: 0.029779
2022-01-20 20:23:13,555 iteration 2103 : loss : 0.048828, loss_ce: 0.023324
2022-01-20 20:23:14,817 iteration 2104 : loss : 0.067081, loss_ce: 0.019484
2022-01-20 20:23:16,225 iteration 2105 : loss : 0.072849, loss_ce: 0.022696
2022-01-20 20:23:17,508 iteration 2106 : loss : 0.029955, loss_ce: 0.012355
2022-01-20 20:23:18,905 iteration 2107 : loss : 0.034048, loss_ce: 0.011508
2022-01-20 20:23:20,230 iteration 2108 : loss : 0.052535, loss_ce: 0.020462
 31%|████████▉                    | 124/400 [50:30<1:46:22, 23.12s/it]2022-01-20 20:23:21,563 iteration 2109 : loss : 0.045836, loss_ce: 0.019729
2022-01-20 20:23:22,895 iteration 2110 : loss : 0.044352, loss_ce: 0.016869
2022-01-20 20:23:24,281 iteration 2111 : loss : 0.047862, loss_ce: 0.016011
2022-01-20 20:23:25,601 iteration 2112 : loss : 0.072989, loss_ce: 0.033918
2022-01-20 20:23:26,887 iteration 2113 : loss : 0.056731, loss_ce: 0.028733
2022-01-20 20:23:28,226 iteration 2114 : loss : 0.035477, loss_ce: 0.016643
2022-01-20 20:23:29,560 iteration 2115 : loss : 0.049170, loss_ce: 0.017442
2022-01-20 20:23:30,884 iteration 2116 : loss : 0.041436, loss_ce: 0.014829
2022-01-20 20:23:32,198 iteration 2117 : loss : 0.039727, loss_ce: 0.014391
2022-01-20 20:23:33,534 iteration 2118 : loss : 0.057714, loss_ce: 0.025134
2022-01-20 20:23:34,874 iteration 2119 : loss : 0.053448, loss_ce: 0.018060
2022-01-20 20:23:36,183 iteration 2120 : loss : 0.058635, loss_ce: 0.016946
2022-01-20 20:23:37,404 iteration 2121 : loss : 0.042457, loss_ce: 0.017038
2022-01-20 20:23:38,702 iteration 2122 : loss : 0.034771, loss_ce: 0.011306
2022-01-20 20:23:40,069 iteration 2123 : loss : 0.073009, loss_ce: 0.034685
2022-01-20 20:23:41,452 iteration 2124 : loss : 0.042732, loss_ce: 0.016726
2022-01-20 20:23:41,452 Training Data Eval:
2022-01-20 20:23:47,942   Average segmentation loss on training set: 0.0321
2022-01-20 20:23:47,942 Validation Data Eval:
2022-01-20 20:23:50,190   Average segmentation loss on validation set: 0.0796
2022-01-20 20:23:51,548 iteration 2125 : loss : 0.054408, loss_ce: 0.030416
 31%|█████████                    | 125/400 [51:01<1:57:15, 25.58s/it]2022-01-20 20:23:52,969 iteration 2126 : loss : 0.041302, loss_ce: 0.017077
2022-01-20 20:23:54,328 iteration 2127 : loss : 0.053744, loss_ce: 0.024465
2022-01-20 20:23:55,684 iteration 2128 : loss : 0.045179, loss_ce: 0.014633
2022-01-20 20:23:56,981 iteration 2129 : loss : 0.067317, loss_ce: 0.026149
2022-01-20 20:23:58,327 iteration 2130 : loss : 0.076628, loss_ce: 0.028288
2022-01-20 20:23:59,615 iteration 2131 : loss : 0.040740, loss_ce: 0.012261
2022-01-20 20:24:00,926 iteration 2132 : loss : 0.031971, loss_ce: 0.011222
2022-01-20 20:24:02,306 iteration 2133 : loss : 0.066446, loss_ce: 0.020757
2022-01-20 20:24:03,561 iteration 2134 : loss : 0.039767, loss_ce: 0.019848
2022-01-20 20:24:04,876 iteration 2135 : loss : 0.039097, loss_ce: 0.013499
2022-01-20 20:24:06,165 iteration 2136 : loss : 0.046106, loss_ce: 0.019745
2022-01-20 20:24:07,477 iteration 2137 : loss : 0.071258, loss_ce: 0.032079
2022-01-20 20:24:08,748 iteration 2138 : loss : 0.043360, loss_ce: 0.016813
2022-01-20 20:24:10,092 iteration 2139 : loss : 0.040148, loss_ce: 0.020155
2022-01-20 20:24:11,402 iteration 2140 : loss : 0.056141, loss_ce: 0.027054
2022-01-20 20:24:12,779 iteration 2141 : loss : 0.039849, loss_ce: 0.013355
2022-01-20 20:24:14,054 iteration 2142 : loss : 0.044492, loss_ce: 0.014318
 32%|█████████▏                   | 126/400 [51:23<1:52:36, 24.66s/it]2022-01-20 20:24:15,329 iteration 2143 : loss : 0.035343, loss_ce: 0.013579
2022-01-20 20:24:16,654 iteration 2144 : loss : 0.045402, loss_ce: 0.013966
2022-01-20 20:24:17,987 iteration 2145 : loss : 0.067831, loss_ce: 0.025284
2022-01-20 20:24:19,239 iteration 2146 : loss : 0.024427, loss_ce: 0.010796
2022-01-20 20:24:20,559 iteration 2147 : loss : 0.053510, loss_ce: 0.020418
2022-01-20 20:24:21,847 iteration 2148 : loss : 0.042224, loss_ce: 0.015624
2022-01-20 20:24:23,164 iteration 2149 : loss : 0.036110, loss_ce: 0.013652
2022-01-20 20:24:24,453 iteration 2150 : loss : 0.034792, loss_ce: 0.012884
2022-01-20 20:24:25,820 iteration 2151 : loss : 0.059453, loss_ce: 0.028368
2022-01-20 20:24:27,093 iteration 2152 : loss : 0.063216, loss_ce: 0.029075
2022-01-20 20:24:28,353 iteration 2153 : loss : 0.033161, loss_ce: 0.012256
2022-01-20 20:24:29,776 iteration 2154 : loss : 0.060116, loss_ce: 0.018664
2022-01-20 20:24:31,091 iteration 2155 : loss : 0.047185, loss_ce: 0.018908
2022-01-20 20:24:32,358 iteration 2156 : loss : 0.040243, loss_ce: 0.016140
2022-01-20 20:24:33,804 iteration 2157 : loss : 0.055946, loss_ce: 0.019874
2022-01-20 20:24:35,203 iteration 2158 : loss : 0.065884, loss_ce: 0.033103
2022-01-20 20:24:36,532 iteration 2159 : loss : 0.073912, loss_ce: 0.022689
 32%|█████████▏                   | 127/400 [51:46<1:49:13, 24.00s/it]2022-01-20 20:24:37,836 iteration 2160 : loss : 0.038433, loss_ce: 0.013419
2022-01-20 20:24:39,086 iteration 2161 : loss : 0.039096, loss_ce: 0.012610
2022-01-20 20:24:40,367 iteration 2162 : loss : 0.047709, loss_ce: 0.019007
2022-01-20 20:24:41,646 iteration 2163 : loss : 0.038488, loss_ce: 0.014632
2022-01-20 20:24:42,991 iteration 2164 : loss : 0.046682, loss_ce: 0.024455
2022-01-20 20:24:44,327 iteration 2165 : loss : 0.055011, loss_ce: 0.021116
2022-01-20 20:24:45,603 iteration 2166 : loss : 0.041680, loss_ce: 0.017866
2022-01-20 20:24:46,854 iteration 2167 : loss : 0.041584, loss_ce: 0.015330
2022-01-20 20:24:48,132 iteration 2168 : loss : 0.037072, loss_ce: 0.014695
2022-01-20 20:24:49,455 iteration 2169 : loss : 0.063008, loss_ce: 0.024373
2022-01-20 20:24:50,667 iteration 2170 : loss : 0.039565, loss_ce: 0.013733
2022-01-20 20:24:51,958 iteration 2171 : loss : 0.045430, loss_ce: 0.016658
2022-01-20 20:24:53,245 iteration 2172 : loss : 0.049244, loss_ce: 0.016989
2022-01-20 20:24:54,508 iteration 2173 : loss : 0.037150, loss_ce: 0.009976
2022-01-20 20:24:55,813 iteration 2174 : loss : 0.045772, loss_ce: 0.018301
2022-01-20 20:24:57,204 iteration 2175 : loss : 0.047757, loss_ce: 0.021853
2022-01-20 20:24:58,407 iteration 2176 : loss : 0.038090, loss_ce: 0.017633
 32%|█████████▎                   | 128/400 [52:08<1:45:55, 23.37s/it]2022-01-20 20:24:59,689 iteration 2177 : loss : 0.063458, loss_ce: 0.035898
2022-01-20 20:25:00,990 iteration 2178 : loss : 0.036713, loss_ce: 0.011096
2022-01-20 20:25:02,360 iteration 2179 : loss : 0.047561, loss_ce: 0.014855
2022-01-20 20:25:03,694 iteration 2180 : loss : 0.047419, loss_ce: 0.020641
2022-01-20 20:25:04,925 iteration 2181 : loss : 0.039148, loss_ce: 0.014419
2022-01-20 20:25:06,236 iteration 2182 : loss : 0.058590, loss_ce: 0.022790
2022-01-20 20:25:07,563 iteration 2183 : loss : 0.049168, loss_ce: 0.014583
2022-01-20 20:25:08,825 iteration 2184 : loss : 0.042301, loss_ce: 0.019760
2022-01-20 20:25:10,103 iteration 2185 : loss : 0.038123, loss_ce: 0.014553
2022-01-20 20:25:11,497 iteration 2186 : loss : 0.040062, loss_ce: 0.013708
2022-01-20 20:25:12,880 iteration 2187 : loss : 0.042962, loss_ce: 0.013093
2022-01-20 20:25:14,327 iteration 2188 : loss : 0.069005, loss_ce: 0.041249
2022-01-20 20:25:15,743 iteration 2189 : loss : 0.059973, loss_ce: 0.019821
2022-01-20 20:25:17,039 iteration 2190 : loss : 0.046542, loss_ce: 0.020430
2022-01-20 20:25:18,367 iteration 2191 : loss : 0.028886, loss_ce: 0.010456
2022-01-20 20:25:19,638 iteration 2192 : loss : 0.045108, loss_ce: 0.016917
2022-01-20 20:25:20,963 iteration 2193 : loss : 0.036732, loss_ce: 0.011360
 32%|█████████▎                   | 129/400 [52:30<1:44:26, 23.12s/it]2022-01-20 20:25:22,339 iteration 2194 : loss : 0.039982, loss_ce: 0.016454
2022-01-20 20:25:23,665 iteration 2195 : loss : 0.044731, loss_ce: 0.022145
2022-01-20 20:25:25,021 iteration 2196 : loss : 0.049772, loss_ce: 0.023979
2022-01-20 20:25:26,416 iteration 2197 : loss : 0.049850, loss_ce: 0.017803
2022-01-20 20:25:27,728 iteration 2198 : loss : 0.055705, loss_ce: 0.018168
2022-01-20 20:25:29,045 iteration 2199 : loss : 0.041797, loss_ce: 0.017114
2022-01-20 20:25:30,436 iteration 2200 : loss : 0.088871, loss_ce: 0.018622
2022-01-20 20:25:31,706 iteration 2201 : loss : 0.041196, loss_ce: 0.016520
2022-01-20 20:25:33,113 iteration 2202 : loss : 0.054514, loss_ce: 0.021517
2022-01-20 20:25:34,440 iteration 2203 : loss : 0.042298, loss_ce: 0.012466
2022-01-20 20:25:35,682 iteration 2204 : loss : 0.031800, loss_ce: 0.009733
2022-01-20 20:25:37,016 iteration 2205 : loss : 0.048997, loss_ce: 0.022963
2022-01-20 20:25:38,421 iteration 2206 : loss : 0.040879, loss_ce: 0.017201
2022-01-20 20:25:39,760 iteration 2207 : loss : 0.053840, loss_ce: 0.015779
2022-01-20 20:25:41,124 iteration 2208 : loss : 0.061895, loss_ce: 0.018801
2022-01-20 20:25:42,427 iteration 2209 : loss : 0.065001, loss_ce: 0.024350
2022-01-20 20:25:42,427 Training Data Eval:
2022-01-20 20:25:48,903   Average segmentation loss on training set: 0.0418
2022-01-20 20:25:48,903 Validation Data Eval:
2022-01-20 20:25:51,151   Average segmentation loss on validation set: 0.0995
2022-01-20 20:25:52,499 iteration 2210 : loss : 0.051834, loss_ce: 0.029015
 32%|█████████▍                   | 130/400 [53:02<1:55:24, 25.65s/it]2022-01-20 20:25:53,856 iteration 2211 : loss : 0.036245, loss_ce: 0.009200
2022-01-20 20:25:55,127 iteration 2212 : loss : 0.047856, loss_ce: 0.021099
2022-01-20 20:25:56,452 iteration 2213 : loss : 0.056000, loss_ce: 0.013500
2022-01-20 20:25:57,692 iteration 2214 : loss : 0.034722, loss_ce: 0.011569
2022-01-20 20:25:58,937 iteration 2215 : loss : 0.032836, loss_ce: 0.012619
2022-01-20 20:26:00,225 iteration 2216 : loss : 0.054313, loss_ce: 0.024426
2022-01-20 20:26:01,482 iteration 2217 : loss : 0.044060, loss_ce: 0.024137
2022-01-20 20:26:02,780 iteration 2218 : loss : 0.040178, loss_ce: 0.014084
2022-01-20 20:26:04,131 iteration 2219 : loss : 0.047726, loss_ce: 0.021435
2022-01-20 20:26:05,530 iteration 2220 : loss : 0.045476, loss_ce: 0.020327
2022-01-20 20:26:06,899 iteration 2221 : loss : 0.056518, loss_ce: 0.018670
2022-01-20 20:26:08,129 iteration 2222 : loss : 0.037873, loss_ce: 0.015794
2022-01-20 20:26:09,405 iteration 2223 : loss : 0.048197, loss_ce: 0.018541
2022-01-20 20:26:10,787 iteration 2224 : loss : 0.051988, loss_ce: 0.021026
2022-01-20 20:26:12,113 iteration 2225 : loss : 0.042989, loss_ce: 0.016011
2022-01-20 20:26:13,493 iteration 2226 : loss : 0.052022, loss_ce: 0.021141
2022-01-20 20:26:14,850 iteration 2227 : loss : 0.051743, loss_ce: 0.017899
 33%|█████████▍                   | 131/400 [53:24<1:50:33, 24.66s/it]2022-01-20 20:26:16,280 iteration 2228 : loss : 0.050917, loss_ce: 0.023653
2022-01-20 20:26:17,600 iteration 2229 : loss : 0.039717, loss_ce: 0.015220
2022-01-20 20:26:19,034 iteration 2230 : loss : 0.048220, loss_ce: 0.018483
2022-01-20 20:26:20,317 iteration 2231 : loss : 0.057264, loss_ce: 0.029621
2022-01-20 20:26:21,605 iteration 2232 : loss : 0.038707, loss_ce: 0.017419
2022-01-20 20:26:23,102 iteration 2233 : loss : 0.125187, loss_ce: 0.039659
2022-01-20 20:26:24,445 iteration 2234 : loss : 0.055147, loss_ce: 0.015372
2022-01-20 20:26:25,764 iteration 2235 : loss : 0.042577, loss_ce: 0.018074
2022-01-20 20:26:27,057 iteration 2236 : loss : 0.032040, loss_ce: 0.013893
2022-01-20 20:26:28,383 iteration 2237 : loss : 0.040607, loss_ce: 0.017208
2022-01-20 20:26:29,653 iteration 2238 : loss : 0.044080, loss_ce: 0.013099
2022-01-20 20:26:30,937 iteration 2239 : loss : 0.048254, loss_ce: 0.013619
2022-01-20 20:26:32,175 iteration 2240 : loss : 0.059964, loss_ce: 0.017031
2022-01-20 20:26:33,480 iteration 2241 : loss : 0.041610, loss_ce: 0.016447
2022-01-20 20:26:34,851 iteration 2242 : loss : 0.059530, loss_ce: 0.019893
2022-01-20 20:26:36,198 iteration 2243 : loss : 0.086213, loss_ce: 0.039667
2022-01-20 20:26:37,484 iteration 2244 : loss : 0.029153, loss_ce: 0.009543
 33%|█████████▌                   | 132/400 [53:47<1:47:25, 24.05s/it]2022-01-20 20:26:38,812 iteration 2245 : loss : 0.032499, loss_ce: 0.009334
2022-01-20 20:26:40,040 iteration 2246 : loss : 0.035292, loss_ce: 0.010455
2022-01-20 20:26:41,360 iteration 2247 : loss : 0.042227, loss_ce: 0.021408
2022-01-20 20:26:42,727 iteration 2248 : loss : 0.075509, loss_ce: 0.035691
2022-01-20 20:26:44,031 iteration 2249 : loss : 0.035245, loss_ce: 0.015393
2022-01-20 20:26:45,337 iteration 2250 : loss : 0.040868, loss_ce: 0.015209
2022-01-20 20:26:46,670 iteration 2251 : loss : 0.038867, loss_ce: 0.017651
2022-01-20 20:26:47,978 iteration 2252 : loss : 0.044574, loss_ce: 0.016117
2022-01-20 20:26:49,380 iteration 2253 : loss : 0.080829, loss_ce: 0.029959
2022-01-20 20:26:50,735 iteration 2254 : loss : 0.037188, loss_ce: 0.014825
2022-01-20 20:26:52,045 iteration 2255 : loss : 0.040687, loss_ce: 0.015011
2022-01-20 20:26:53,336 iteration 2256 : loss : 0.035741, loss_ce: 0.015813
2022-01-20 20:26:54,604 iteration 2257 : loss : 0.043929, loss_ce: 0.014149
2022-01-20 20:26:55,872 iteration 2258 : loss : 0.058610, loss_ce: 0.022646
2022-01-20 20:26:57,241 iteration 2259 : loss : 0.046183, loss_ce: 0.016637
2022-01-20 20:26:58,493 iteration 2260 : loss : 0.035343, loss_ce: 0.014120
2022-01-20 20:26:59,838 iteration 2261 : loss : 0.046108, loss_ce: 0.019914
 33%|█████████▋                   | 133/400 [54:09<1:44:45, 23.54s/it]2022-01-20 20:27:01,138 iteration 2262 : loss : 0.035854, loss_ce: 0.017917
2022-01-20 20:27:02,506 iteration 2263 : loss : 0.048715, loss_ce: 0.018203
2022-01-20 20:27:03,898 iteration 2264 : loss : 0.052885, loss_ce: 0.013571
2022-01-20 20:27:05,228 iteration 2265 : loss : 0.037436, loss_ce: 0.014115
2022-01-20 20:27:06,515 iteration 2266 : loss : 0.040137, loss_ce: 0.015417
2022-01-20 20:27:07,811 iteration 2267 : loss : 0.031081, loss_ce: 0.010177
2022-01-20 20:27:09,135 iteration 2268 : loss : 0.041942, loss_ce: 0.016372
2022-01-20 20:27:10,417 iteration 2269 : loss : 0.036646, loss_ce: 0.018743
2022-01-20 20:27:11,716 iteration 2270 : loss : 0.053139, loss_ce: 0.018005
2022-01-20 20:27:12,989 iteration 2271 : loss : 0.045258, loss_ce: 0.015803
2022-01-20 20:27:14,289 iteration 2272 : loss : 0.047582, loss_ce: 0.021876
2022-01-20 20:27:15,628 iteration 2273 : loss : 0.030692, loss_ce: 0.009769
2022-01-20 20:27:16,961 iteration 2274 : loss : 0.043482, loss_ce: 0.013024
2022-01-20 20:27:18,331 iteration 2275 : loss : 0.045288, loss_ce: 0.015003
2022-01-20 20:27:19,583 iteration 2276 : loss : 0.032012, loss_ce: 0.012888
2022-01-20 20:27:20,904 iteration 2277 : loss : 0.043726, loss_ce: 0.014650
2022-01-20 20:27:22,174 iteration 2278 : loss : 0.033735, loss_ce: 0.012140
 34%|█████████▋                   | 134/400 [54:32<1:42:45, 23.18s/it]2022-01-20 20:27:23,461 iteration 2279 : loss : 0.087908, loss_ce: 0.038257
2022-01-20 20:27:24,766 iteration 2280 : loss : 0.038692, loss_ce: 0.016153
2022-01-20 20:27:26,066 iteration 2281 : loss : 0.075134, loss_ce: 0.020870
2022-01-20 20:27:27,411 iteration 2282 : loss : 0.040023, loss_ce: 0.014049
2022-01-20 20:27:28,789 iteration 2283 : loss : 0.046274, loss_ce: 0.021959
2022-01-20 20:27:30,078 iteration 2284 : loss : 0.060218, loss_ce: 0.029259
2022-01-20 20:27:31,369 iteration 2285 : loss : 0.054498, loss_ce: 0.024873
2022-01-20 20:27:32,675 iteration 2286 : loss : 0.046927, loss_ce: 0.019322
2022-01-20 20:27:33,997 iteration 2287 : loss : 0.042687, loss_ce: 0.018961
2022-01-20 20:27:35,359 iteration 2288 : loss : 0.056376, loss_ce: 0.018222
2022-01-20 20:27:36,623 iteration 2289 : loss : 0.142024, loss_ce: 0.035698
2022-01-20 20:27:37,990 iteration 2290 : loss : 0.064201, loss_ce: 0.035089
2022-01-20 20:27:39,291 iteration 2291 : loss : 0.034775, loss_ce: 0.013055
2022-01-20 20:27:40,573 iteration 2292 : loss : 0.049951, loss_ce: 0.020641
2022-01-20 20:27:41,808 iteration 2293 : loss : 0.049275, loss_ce: 0.024695
2022-01-20 20:27:43,149 iteration 2294 : loss : 0.090456, loss_ce: 0.032973
2022-01-20 20:27:43,149 Training Data Eval:
2022-01-20 20:27:49,695   Average segmentation loss on training set: 0.0629
2022-01-20 20:27:49,696 Validation Data Eval:
2022-01-20 20:27:51,932   Average segmentation loss on validation set: 0.1204
2022-01-20 20:27:53,330 iteration 2295 : loss : 0.066279, loss_ce: 0.037854
 34%|█████████▊                   | 135/400 [55:03<1:52:56, 25.57s/it]2022-01-20 20:27:54,616 iteration 2296 : loss : 0.041479, loss_ce: 0.017896
2022-01-20 20:27:55,914 iteration 2297 : loss : 0.050326, loss_ce: 0.022384
2022-01-20 20:27:57,214 iteration 2298 : loss : 0.036852, loss_ce: 0.017071
2022-01-20 20:27:58,605 iteration 2299 : loss : 0.071278, loss_ce: 0.033551
2022-01-20 20:27:59,894 iteration 2300 : loss : 0.048638, loss_ce: 0.021251
2022-01-20 20:28:01,281 iteration 2301 : loss : 0.059563, loss_ce: 0.019314
2022-01-20 20:28:02,636 iteration 2302 : loss : 0.086222, loss_ce: 0.029579
2022-01-20 20:28:03,964 iteration 2303 : loss : 0.041959, loss_ce: 0.014700
2022-01-20 20:28:05,356 iteration 2304 : loss : 0.063291, loss_ce: 0.021918
2022-01-20 20:28:06,709 iteration 2305 : loss : 0.071437, loss_ce: 0.027846
2022-01-20 20:28:08,105 iteration 2306 : loss : 0.060428, loss_ce: 0.025545
2022-01-20 20:28:09,396 iteration 2307 : loss : 0.093926, loss_ce: 0.031260
2022-01-20 20:28:10,663 iteration 2308 : loss : 0.057407, loss_ce: 0.017769
2022-01-20 20:28:11,946 iteration 2309 : loss : 0.045472, loss_ce: 0.020543
2022-01-20 20:28:13,312 iteration 2310 : loss : 0.057841, loss_ce: 0.025342
2022-01-20 20:28:14,541 iteration 2311 : loss : 0.054317, loss_ce: 0.018077
2022-01-20 20:28:15,849 iteration 2312 : loss : 0.056016, loss_ce: 0.021155
 34%|█████████▊                   | 136/400 [55:25<1:48:28, 24.65s/it]2022-01-20 20:28:17,233 iteration 2313 : loss : 0.043033, loss_ce: 0.016135
2022-01-20 20:28:18,448 iteration 2314 : loss : 0.045398, loss_ce: 0.015500
2022-01-20 20:28:19,713 iteration 2315 : loss : 0.043748, loss_ce: 0.018632
2022-01-20 20:28:21,107 iteration 2316 : loss : 0.051852, loss_ce: 0.021534
2022-01-20 20:28:22,356 iteration 2317 : loss : 0.036933, loss_ce: 0.012632
2022-01-20 20:28:23,641 iteration 2318 : loss : 0.048130, loss_ce: 0.022806
2022-01-20 20:28:24,949 iteration 2319 : loss : 0.052779, loss_ce: 0.024694
2022-01-20 20:28:26,321 iteration 2320 : loss : 0.062410, loss_ce: 0.027569
2022-01-20 20:28:27,622 iteration 2321 : loss : 0.050175, loss_ce: 0.021327
2022-01-20 20:28:28,953 iteration 2322 : loss : 0.038239, loss_ce: 0.015436
2022-01-20 20:28:30,187 iteration 2323 : loss : 0.038736, loss_ce: 0.015490
2022-01-20 20:28:31,486 iteration 2324 : loss : 0.033908, loss_ce: 0.015512
2022-01-20 20:28:32,679 iteration 2325 : loss : 0.035930, loss_ce: 0.016020
2022-01-20 20:28:33,966 iteration 2326 : loss : 0.095938, loss_ce: 0.019535
2022-01-20 20:28:35,234 iteration 2327 : loss : 0.042221, loss_ce: 0.016137
2022-01-20 20:28:36,538 iteration 2328 : loss : 0.046602, loss_ce: 0.020847
2022-01-20 20:28:37,802 iteration 2329 : loss : 0.056861, loss_ce: 0.022743
 34%|█████████▉                   | 137/400 [55:47<1:44:31, 23.85s/it]2022-01-20 20:28:39,167 iteration 2330 : loss : 0.047454, loss_ce: 0.017843
2022-01-20 20:28:40,437 iteration 2331 : loss : 0.033997, loss_ce: 0.013005
2022-01-20 20:28:41,822 iteration 2332 : loss : 0.048999, loss_ce: 0.019479
2022-01-20 20:28:43,130 iteration 2333 : loss : 0.046591, loss_ce: 0.022638
2022-01-20 20:28:44,552 iteration 2334 : loss : 0.048130, loss_ce: 0.017855
2022-01-20 20:28:45,964 iteration 2335 : loss : 0.039251, loss_ce: 0.016432
2022-01-20 20:28:47,328 iteration 2336 : loss : 0.040099, loss_ce: 0.017191
2022-01-20 20:28:48,630 iteration 2337 : loss : 0.070153, loss_ce: 0.025816
2022-01-20 20:28:49,971 iteration 2338 : loss : 0.064129, loss_ce: 0.026322
2022-01-20 20:28:51,303 iteration 2339 : loss : 0.050482, loss_ce: 0.020524
2022-01-20 20:28:52,558 iteration 2340 : loss : 0.040417, loss_ce: 0.014568
2022-01-20 20:28:53,876 iteration 2341 : loss : 0.041143, loss_ce: 0.016724
2022-01-20 20:28:55,206 iteration 2342 : loss : 0.043503, loss_ce: 0.016417
2022-01-20 20:28:56,640 iteration 2343 : loss : 0.034396, loss_ce: 0.012259
2022-01-20 20:28:57,907 iteration 2344 : loss : 0.096629, loss_ce: 0.024088
2022-01-20 20:28:59,184 iteration 2345 : loss : 0.055923, loss_ce: 0.030179
2022-01-20 20:29:00,475 iteration 2346 : loss : 0.049317, loss_ce: 0.019372
 34%|██████████                   | 138/400 [56:10<1:42:35, 23.49s/it]2022-01-20 20:29:01,908 iteration 2347 : loss : 0.048049, loss_ce: 0.020018
2022-01-20 20:29:03,240 iteration 2348 : loss : 0.039306, loss_ce: 0.014634
2022-01-20 20:29:04,529 iteration 2349 : loss : 0.043415, loss_ce: 0.013905
2022-01-20 20:29:05,833 iteration 2350 : loss : 0.071610, loss_ce: 0.020904
2022-01-20 20:29:07,220 iteration 2351 : loss : 0.074292, loss_ce: 0.034942
2022-01-20 20:29:08,487 iteration 2352 : loss : 0.041721, loss_ce: 0.019807
2022-01-20 20:29:09,757 iteration 2353 : loss : 0.034164, loss_ce: 0.014265
2022-01-20 20:29:11,030 iteration 2354 : loss : 0.027959, loss_ce: 0.013505
2022-01-20 20:29:12,312 iteration 2355 : loss : 0.045560, loss_ce: 0.017202
2022-01-20 20:29:13,668 iteration 2356 : loss : 0.055625, loss_ce: 0.018363
2022-01-20 20:29:14,980 iteration 2357 : loss : 0.072216, loss_ce: 0.018919
2022-01-20 20:29:16,305 iteration 2358 : loss : 0.039689, loss_ce: 0.015978
2022-01-20 20:29:17,589 iteration 2359 : loss : 0.058197, loss_ce: 0.017456
2022-01-20 20:29:18,856 iteration 2360 : loss : 0.063608, loss_ce: 0.018042
2022-01-20 20:29:20,162 iteration 2361 : loss : 0.040434, loss_ce: 0.015787
2022-01-20 20:29:21,582 iteration 2362 : loss : 0.054650, loss_ce: 0.019775
2022-01-20 20:29:22,905 iteration 2363 : loss : 0.048776, loss_ce: 0.020664
 35%|██████████                   | 139/400 [56:32<1:40:48, 23.18s/it]2022-01-20 20:29:24,337 iteration 2364 : loss : 0.030988, loss_ce: 0.010101
2022-01-20 20:29:25,729 iteration 2365 : loss : 0.052620, loss_ce: 0.017335
2022-01-20 20:29:27,075 iteration 2366 : loss : 0.068729, loss_ce: 0.030228
2022-01-20 20:29:28,316 iteration 2367 : loss : 0.044223, loss_ce: 0.009817
2022-01-20 20:29:29,570 iteration 2368 : loss : 0.038305, loss_ce: 0.014901
2022-01-20 20:29:30,891 iteration 2369 : loss : 0.041723, loss_ce: 0.014009
2022-01-20 20:29:32,188 iteration 2370 : loss : 0.049721, loss_ce: 0.018857
2022-01-20 20:29:33,526 iteration 2371 : loss : 0.035835, loss_ce: 0.012769
2022-01-20 20:29:34,798 iteration 2372 : loss : 0.050932, loss_ce: 0.018263
2022-01-20 20:29:36,095 iteration 2373 : loss : 0.054752, loss_ce: 0.021835
2022-01-20 20:29:37,362 iteration 2374 : loss : 0.059557, loss_ce: 0.029831
2022-01-20 20:29:38,644 iteration 2375 : loss : 0.036092, loss_ce: 0.012487
2022-01-20 20:29:39,961 iteration 2376 : loss : 0.049305, loss_ce: 0.023054
2022-01-20 20:29:41,278 iteration 2377 : loss : 0.044472, loss_ce: 0.019781
2022-01-20 20:29:42,590 iteration 2378 : loss : 0.030829, loss_ce: 0.011925
2022-01-20 20:29:43,936 iteration 2379 : loss : 0.047246, loss_ce: 0.019653
2022-01-20 20:29:43,936 Training Data Eval:
2022-01-20 20:29:50,439   Average segmentation loss on training set: 0.0327
2022-01-20 20:29:50,439 Validation Data Eval:
2022-01-20 20:29:52,684   Average segmentation loss on validation set: 0.0808
2022-01-20 20:29:53,976 iteration 2380 : loss : 0.037364, loss_ce: 0.016297
 35%|██████████▏                  | 140/400 [57:03<1:50:40, 25.54s/it]2022-01-20 20:29:55,286 iteration 2381 : loss : 0.041802, loss_ce: 0.015260
2022-01-20 20:29:56,632 iteration 2382 : loss : 0.043368, loss_ce: 0.015846
2022-01-20 20:29:57,907 iteration 2383 : loss : 0.029660, loss_ce: 0.009711
2022-01-20 20:29:59,275 iteration 2384 : loss : 0.054769, loss_ce: 0.018368
2022-01-20 20:30:00,638 iteration 2385 : loss : 0.049141, loss_ce: 0.018354
2022-01-20 20:30:01,934 iteration 2386 : loss : 0.038041, loss_ce: 0.010486
2022-01-20 20:30:03,211 iteration 2387 : loss : 0.046090, loss_ce: 0.015309
2022-01-20 20:30:04,526 iteration 2388 : loss : 0.067688, loss_ce: 0.037273
2022-01-20 20:30:05,771 iteration 2389 : loss : 0.022743, loss_ce: 0.007557
2022-01-20 20:30:07,097 iteration 2390 : loss : 0.041505, loss_ce: 0.020972
2022-01-20 20:30:08,398 iteration 2391 : loss : 0.044468, loss_ce: 0.025217
2022-01-20 20:30:09,736 iteration 2392 : loss : 0.039181, loss_ce: 0.013958
2022-01-20 20:30:11,061 iteration 2393 : loss : 0.049100, loss_ce: 0.019172
2022-01-20 20:30:12,276 iteration 2394 : loss : 0.036195, loss_ce: 0.012365
2022-01-20 20:30:13,635 iteration 2395 : loss : 0.055298, loss_ce: 0.023885
2022-01-20 20:30:14,915 iteration 2396 : loss : 0.028372, loss_ce: 0.012991
2022-01-20 20:30:16,255 iteration 2397 : loss : 0.037435, loss_ce: 0.015325
 35%|██████████▏                  | 141/400 [57:26<1:46:02, 24.57s/it]2022-01-20 20:30:17,600 iteration 2398 : loss : 0.046786, loss_ce: 0.016625
2022-01-20 20:30:18,885 iteration 2399 : loss : 0.048037, loss_ce: 0.014343
2022-01-20 20:30:20,253 iteration 2400 : loss : 0.038269, loss_ce: 0.017726
2022-01-20 20:30:21,567 iteration 2401 : loss : 0.034920, loss_ce: 0.011750
2022-01-20 20:30:22,932 iteration 2402 : loss : 0.042139, loss_ce: 0.022162
2022-01-20 20:30:24,208 iteration 2403 : loss : 0.039294, loss_ce: 0.016317
2022-01-20 20:30:25,472 iteration 2404 : loss : 0.034255, loss_ce: 0.016142
2022-01-20 20:30:26,713 iteration 2405 : loss : 0.041458, loss_ce: 0.014579
2022-01-20 20:30:28,153 iteration 2406 : loss : 0.056930, loss_ce: 0.018292
2022-01-20 20:30:29,390 iteration 2407 : loss : 0.038753, loss_ce: 0.020165
2022-01-20 20:30:30,642 iteration 2408 : loss : 0.025233, loss_ce: 0.010026
2022-01-20 20:30:31,992 iteration 2409 : loss : 0.031834, loss_ce: 0.010002
2022-01-20 20:30:33,348 iteration 2410 : loss : 0.045715, loss_ce: 0.017905
2022-01-20 20:30:34,686 iteration 2411 : loss : 0.044509, loss_ce: 0.013129
2022-01-20 20:30:35,954 iteration 2412 : loss : 0.031601, loss_ce: 0.012217
2022-01-20 20:30:37,330 iteration 2413 : loss : 0.063663, loss_ce: 0.030159
2022-01-20 20:30:38,645 iteration 2414 : loss : 0.028275, loss_ce: 0.010832
 36%|██████████▎                  | 142/400 [57:48<1:42:48, 23.91s/it]2022-01-20 20:30:40,005 iteration 2415 : loss : 0.052871, loss_ce: 0.019156
2022-01-20 20:30:41,292 iteration 2416 : loss : 0.041054, loss_ce: 0.013897
2022-01-20 20:30:42,595 iteration 2417 : loss : 0.047646, loss_ce: 0.016314
2022-01-20 20:30:43,975 iteration 2418 : loss : 0.035846, loss_ce: 0.012360
2022-01-20 20:30:45,289 iteration 2419 : loss : 0.047993, loss_ce: 0.023511
2022-01-20 20:30:46,690 iteration 2420 : loss : 0.055804, loss_ce: 0.026365
2022-01-20 20:30:48,069 iteration 2421 : loss : 0.048340, loss_ce: 0.016012
2022-01-20 20:30:49,427 iteration 2422 : loss : 0.040580, loss_ce: 0.015459
2022-01-20 20:30:50,729 iteration 2423 : loss : 0.041677, loss_ce: 0.014397
2022-01-20 20:30:52,030 iteration 2424 : loss : 0.034586, loss_ce: 0.013592
2022-01-20 20:30:53,438 iteration 2425 : loss : 0.042986, loss_ce: 0.015062
2022-01-20 20:30:54,720 iteration 2426 : loss : 0.040393, loss_ce: 0.011717
2022-01-20 20:30:55,941 iteration 2427 : loss : 0.036254, loss_ce: 0.016713
2022-01-20 20:30:57,217 iteration 2428 : loss : 0.046312, loss_ce: 0.017492
2022-01-20 20:30:58,502 iteration 2429 : loss : 0.046534, loss_ce: 0.016784
2022-01-20 20:30:59,780 iteration 2430 : loss : 0.026043, loss_ce: 0.009777
2022-01-20 20:31:01,094 iteration 2431 : loss : 0.046285, loss_ce: 0.024036
 36%|██████████▎                  | 143/400 [58:10<1:40:32, 23.47s/it]2022-01-20 20:31:02,416 iteration 2432 : loss : 0.037223, loss_ce: 0.011332
2022-01-20 20:31:03,711 iteration 2433 : loss : 0.035624, loss_ce: 0.014459
2022-01-20 20:31:05,027 iteration 2434 : loss : 0.042604, loss_ce: 0.013991
2022-01-20 20:31:06,428 iteration 2435 : loss : 0.045467, loss_ce: 0.025190
2022-01-20 20:31:07,654 iteration 2436 : loss : 0.034629, loss_ce: 0.014004
2022-01-20 20:31:09,014 iteration 2437 : loss : 0.026321, loss_ce: 0.009138
2022-01-20 20:31:10,384 iteration 2438 : loss : 0.054655, loss_ce: 0.018791
2022-01-20 20:31:11,741 iteration 2439 : loss : 0.075058, loss_ce: 0.025576
2022-01-20 20:31:13,100 iteration 2440 : loss : 0.044395, loss_ce: 0.017313
2022-01-20 20:31:14,394 iteration 2441 : loss : 0.034571, loss_ce: 0.012600
2022-01-20 20:31:15,680 iteration 2442 : loss : 0.029933, loss_ce: 0.011842
2022-01-20 20:31:16,923 iteration 2443 : loss : 0.036062, loss_ce: 0.014636
2022-01-20 20:31:18,235 iteration 2444 : loss : 0.046494, loss_ce: 0.017208
2022-01-20 20:31:19,546 iteration 2445 : loss : 0.046281, loss_ce: 0.018398
2022-01-20 20:31:20,944 iteration 2446 : loss : 0.033876, loss_ce: 0.018039
2022-01-20 20:31:22,200 iteration 2447 : loss : 0.033204, loss_ce: 0.010726
2022-01-20 20:31:23,547 iteration 2448 : loss : 0.030583, loss_ce: 0.011628
 36%|██████████▍                  | 144/400 [58:33<1:38:50, 23.17s/it]2022-01-20 20:31:24,825 iteration 2449 : loss : 0.051778, loss_ce: 0.023678
2022-01-20 20:31:26,073 iteration 2450 : loss : 0.030329, loss_ce: 0.015396
2022-01-20 20:31:27,380 iteration 2451 : loss : 0.030443, loss_ce: 0.009617
2022-01-20 20:31:28,667 iteration 2452 : loss : 0.039057, loss_ce: 0.017342
2022-01-20 20:31:29,981 iteration 2453 : loss : 0.041999, loss_ce: 0.014327
2022-01-20 20:31:31,266 iteration 2454 : loss : 0.039336, loss_ce: 0.014145
2022-01-20 20:31:32,534 iteration 2455 : loss : 0.047606, loss_ce: 0.022247
2022-01-20 20:31:33,796 iteration 2456 : loss : 0.040827, loss_ce: 0.019955
2022-01-20 20:31:35,082 iteration 2457 : loss : 0.037770, loss_ce: 0.015604
2022-01-20 20:31:36,351 iteration 2458 : loss : 0.038324, loss_ce: 0.014701
2022-01-20 20:31:37,574 iteration 2459 : loss : 0.026080, loss_ce: 0.010119
2022-01-20 20:31:38,854 iteration 2460 : loss : 0.043747, loss_ce: 0.012748
2022-01-20 20:31:40,214 iteration 2461 : loss : 0.056351, loss_ce: 0.022533
2022-01-20 20:31:41,543 iteration 2462 : loss : 0.055958, loss_ce: 0.022744
2022-01-20 20:31:42,819 iteration 2463 : loss : 0.032737, loss_ce: 0.014030
2022-01-20 20:31:44,146 iteration 2464 : loss : 0.036477, loss_ce: 0.013788
2022-01-20 20:31:44,146 Training Data Eval:
2022-01-20 20:31:50,638   Average segmentation loss on training set: 0.0376
2022-01-20 20:31:50,639 Validation Data Eval:
2022-01-20 20:31:52,875   Average segmentation loss on validation set: 0.1563
2022-01-20 20:31:54,175 iteration 2465 : loss : 0.036566, loss_ce: 0.014183
 36%|██████████▌                  | 145/400 [59:04<1:47:58, 25.41s/it]2022-01-20 20:31:55,588 iteration 2466 : loss : 0.039663, loss_ce: 0.014135
2022-01-20 20:31:56,971 iteration 2467 : loss : 0.046667, loss_ce: 0.021026
2022-01-20 20:31:58,249 iteration 2468 : loss : 0.047265, loss_ce: 0.019258
2022-01-20 20:31:59,504 iteration 2469 : loss : 0.025043, loss_ce: 0.010586
2022-01-20 20:32:00,799 iteration 2470 : loss : 0.046359, loss_ce: 0.014292
2022-01-20 20:32:02,189 iteration 2471 : loss : 0.043909, loss_ce: 0.015648
2022-01-20 20:32:03,468 iteration 2472 : loss : 0.045532, loss_ce: 0.016596
2022-01-20 20:32:04,835 iteration 2473 : loss : 0.034409, loss_ce: 0.014215
2022-01-20 20:32:06,111 iteration 2474 : loss : 0.040352, loss_ce: 0.013839
2022-01-20 20:32:07,423 iteration 2475 : loss : 0.042951, loss_ce: 0.016906
2022-01-20 20:32:08,690 iteration 2476 : loss : 0.054787, loss_ce: 0.026515
2022-01-20 20:32:10,071 iteration 2477 : loss : 0.052723, loss_ce: 0.022704
2022-01-20 20:32:11,402 iteration 2478 : loss : 0.044297, loss_ce: 0.019698
2022-01-20 20:32:12,632 iteration 2479 : loss : 0.033848, loss_ce: 0.012244
2022-01-20 20:32:13,941 iteration 2480 : loss : 0.033186, loss_ce: 0.013229
2022-01-20 20:32:15,304 iteration 2481 : loss : 0.033857, loss_ce: 0.012554
2022-01-20 20:32:16,580 iteration 2482 : loss : 0.029505, loss_ce: 0.012521
 36%|██████████▌                  | 146/400 [59:26<1:43:44, 24.50s/it]2022-01-20 20:32:17,993 iteration 2483 : loss : 0.046053, loss_ce: 0.020721
2022-01-20 20:32:19,316 iteration 2484 : loss : 0.062857, loss_ce: 0.016255
2022-01-20 20:32:20,580 iteration 2485 : loss : 0.035325, loss_ce: 0.015947
2022-01-20 20:32:21,875 iteration 2486 : loss : 0.036098, loss_ce: 0.014134
2022-01-20 20:32:23,157 iteration 2487 : loss : 0.042611, loss_ce: 0.015000
2022-01-20 20:32:24,512 iteration 2488 : loss : 0.037322, loss_ce: 0.015957
2022-01-20 20:32:25,807 iteration 2489 : loss : 0.052408, loss_ce: 0.024979
2022-01-20 20:32:27,151 iteration 2490 : loss : 0.040166, loss_ce: 0.016454
2022-01-20 20:32:28,522 iteration 2491 : loss : 0.040658, loss_ce: 0.020465
2022-01-20 20:32:29,733 iteration 2492 : loss : 0.034120, loss_ce: 0.012599
2022-01-20 20:32:31,056 iteration 2493 : loss : 0.041133, loss_ce: 0.016837
2022-01-20 20:32:32,352 iteration 2494 : loss : 0.052534, loss_ce: 0.015395
2022-01-20 20:32:33,647 iteration 2495 : loss : 0.040335, loss_ce: 0.016385
2022-01-20 20:32:35,138 iteration 2496 : loss : 0.051789, loss_ce: 0.014180
2022-01-20 20:32:36,485 iteration 2497 : loss : 0.048323, loss_ce: 0.017860
2022-01-20 20:32:37,843 iteration 2498 : loss : 0.058027, loss_ce: 0.023738
2022-01-20 20:32:39,096 iteration 2499 : loss : 0.040605, loss_ce: 0.011402
 37%|██████████▋                  | 147/400 [59:48<1:40:48, 23.91s/it]2022-01-20 20:32:40,426 iteration 2500 : loss : 0.033810, loss_ce: 0.013230
2022-01-20 20:32:41,730 iteration 2501 : loss : 0.042601, loss_ce: 0.012343
2022-01-20 20:32:43,115 iteration 2502 : loss : 0.056050, loss_ce: 0.022644
2022-01-20 20:32:44,367 iteration 2503 : loss : 0.036993, loss_ce: 0.012685
2022-01-20 20:32:45,652 iteration 2504 : loss : 0.032370, loss_ce: 0.011632
2022-01-20 20:32:47,006 iteration 2505 : loss : 0.045571, loss_ce: 0.017765
2022-01-20 20:32:48,307 iteration 2506 : loss : 0.051418, loss_ce: 0.016140
2022-01-20 20:32:49,567 iteration 2507 : loss : 0.036346, loss_ce: 0.016476
2022-01-20 20:32:50,841 iteration 2508 : loss : 0.038102, loss_ce: 0.013363
2022-01-20 20:32:52,121 iteration 2509 : loss : 0.051345, loss_ce: 0.017713
2022-01-20 20:32:53,393 iteration 2510 : loss : 0.034185, loss_ce: 0.014905
2022-01-20 20:32:54,696 iteration 2511 : loss : 0.032770, loss_ce: 0.014248
2022-01-20 20:32:56,018 iteration 2512 : loss : 0.037360, loss_ce: 0.013904
2022-01-20 20:32:57,373 iteration 2513 : loss : 0.044443, loss_ce: 0.013674
2022-01-20 20:32:58,687 iteration 2514 : loss : 0.045293, loss_ce: 0.014025
2022-01-20 20:32:59,989 iteration 2515 : loss : 0.036759, loss_ce: 0.027445
2022-01-20 20:33:01,280 iteration 2516 : loss : 0.029669, loss_ce: 0.013739
 37%|█████████▉                 | 148/400 [1:00:11<1:38:14, 23.39s/it]2022-01-20 20:33:02,602 iteration 2517 : loss : 0.029697, loss_ce: 0.011786
2022-01-20 20:33:03,910 iteration 2518 : loss : 0.041827, loss_ce: 0.012538
2022-01-20 20:33:05,234 iteration 2519 : loss : 0.045214, loss_ce: 0.017985
2022-01-20 20:33:06,544 iteration 2520 : loss : 0.045359, loss_ce: 0.020352
2022-01-20 20:33:07,839 iteration 2521 : loss : 0.031132, loss_ce: 0.012156
2022-01-20 20:33:09,153 iteration 2522 : loss : 0.035056, loss_ce: 0.019248
2022-01-20 20:33:10,493 iteration 2523 : loss : 0.052568, loss_ce: 0.015142
2022-01-20 20:33:11,773 iteration 2524 : loss : 0.034282, loss_ce: 0.015634
2022-01-20 20:33:13,090 iteration 2525 : loss : 0.036923, loss_ce: 0.014526
2022-01-20 20:33:14,420 iteration 2526 : loss : 0.042508, loss_ce: 0.016253
2022-01-20 20:33:15,736 iteration 2527 : loss : 0.034830, loss_ce: 0.012694
2022-01-20 20:33:17,065 iteration 2528 : loss : 0.040565, loss_ce: 0.013701
2022-01-20 20:33:18,385 iteration 2529 : loss : 0.042877, loss_ce: 0.016193
2022-01-20 20:33:19,725 iteration 2530 : loss : 0.036773, loss_ce: 0.018089
2022-01-20 20:33:21,107 iteration 2531 : loss : 0.042498, loss_ce: 0.020110
2022-01-20 20:33:22,395 iteration 2532 : loss : 0.055439, loss_ce: 0.019927
2022-01-20 20:33:23,717 iteration 2533 : loss : 0.047920, loss_ce: 0.013337
 37%|██████████                 | 149/400 [1:00:33<1:36:38, 23.10s/it]2022-01-20 20:33:25,005 iteration 2534 : loss : 0.034732, loss_ce: 0.014779
2022-01-20 20:33:26,381 iteration 2535 : loss : 0.069452, loss_ce: 0.027405
2022-01-20 20:33:27,653 iteration 2536 : loss : 0.031736, loss_ce: 0.015600
2022-01-20 20:33:28,924 iteration 2537 : loss : 0.035318, loss_ce: 0.013994
2022-01-20 20:33:30,134 iteration 2538 : loss : 0.029590, loss_ce: 0.012362
2022-01-20 20:33:31,469 iteration 2539 : loss : 0.034147, loss_ce: 0.013528
2022-01-20 20:33:32,740 iteration 2540 : loss : 0.027448, loss_ce: 0.009307
2022-01-20 20:33:34,042 iteration 2541 : loss : 0.036766, loss_ce: 0.015720
2022-01-20 20:33:35,290 iteration 2542 : loss : 0.024687, loss_ce: 0.009547
2022-01-20 20:33:36,540 iteration 2543 : loss : 0.047346, loss_ce: 0.019075
2022-01-20 20:33:37,856 iteration 2544 : loss : 0.042394, loss_ce: 0.015022
2022-01-20 20:33:39,276 iteration 2545 : loss : 0.031367, loss_ce: 0.012724
2022-01-20 20:33:40,700 iteration 2546 : loss : 0.045338, loss_ce: 0.020769
2022-01-20 20:33:41,946 iteration 2547 : loss : 0.046650, loss_ce: 0.021901
2022-01-20 20:33:43,277 iteration 2548 : loss : 0.033086, loss_ce: 0.015766
2022-01-20 20:33:44,596 iteration 2549 : loss : 0.046538, loss_ce: 0.013580
2022-01-20 20:33:44,596 Training Data Eval:
2022-01-20 20:33:51,080   Average segmentation loss on training set: 0.0273
2022-01-20 20:33:51,080 Validation Data Eval:
2022-01-20 20:33:53,324   Average segmentation loss on validation set: 0.0734
2022-01-20 20:33:59,145 Found new lowest validation loss at iteration 2549! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 20:34:00,361 iteration 2550 : loss : 0.035845, loss_ce: 0.010219
 38%|██████████▏                | 150/400 [1:01:10<1:53:12, 27.17s/it]2022-01-20 20:34:01,617 iteration 2551 : loss : 0.033774, loss_ce: 0.017834
2022-01-20 20:34:02,909 iteration 2552 : loss : 0.027240, loss_ce: 0.011254
2022-01-20 20:34:04,076 iteration 2553 : loss : 0.024448, loss_ce: 0.007461
2022-01-20 20:34:05,312 iteration 2554 : loss : 0.042963, loss_ce: 0.017240
2022-01-20 20:34:06,537 iteration 2555 : loss : 0.042374, loss_ce: 0.010769
2022-01-20 20:34:07,828 iteration 2556 : loss : 0.040774, loss_ce: 0.017247
2022-01-20 20:34:09,051 iteration 2557 : loss : 0.040175, loss_ce: 0.019156
2022-01-20 20:34:10,295 iteration 2558 : loss : 0.048614, loss_ce: 0.027341
2022-01-20 20:34:11,615 iteration 2559 : loss : 0.036958, loss_ce: 0.013071
2022-01-20 20:34:12,849 iteration 2560 : loss : 0.043277, loss_ce: 0.022914
2022-01-20 20:34:14,167 iteration 2561 : loss : 0.032858, loss_ce: 0.011771
2022-01-20 20:34:15,419 iteration 2562 : loss : 0.041099, loss_ce: 0.017032
2022-01-20 20:34:16,686 iteration 2563 : loss : 0.039001, loss_ce: 0.017099
2022-01-20 20:34:17,879 iteration 2564 : loss : 0.037581, loss_ce: 0.017343
2022-01-20 20:34:19,084 iteration 2565 : loss : 0.038526, loss_ce: 0.015090
2022-01-20 20:34:20,350 iteration 2566 : loss : 0.035526, loss_ce: 0.013113
2022-01-20 20:34:21,684 iteration 2567 : loss : 0.056518, loss_ce: 0.016253
 38%|██████████▏                | 151/400 [1:01:31<1:45:27, 25.41s/it]2022-01-20 20:34:23,073 iteration 2568 : loss : 0.043425, loss_ce: 0.020213
2022-01-20 20:34:24,385 iteration 2569 : loss : 0.038981, loss_ce: 0.018389
2022-01-20 20:34:25,697 iteration 2570 : loss : 0.046948, loss_ce: 0.020382
2022-01-20 20:34:27,017 iteration 2571 : loss : 0.027953, loss_ce: 0.009744
2022-01-20 20:34:28,284 iteration 2572 : loss : 0.031571, loss_ce: 0.008432
2022-01-20 20:34:29,565 iteration 2573 : loss : 0.042140, loss_ce: 0.016755
2022-01-20 20:34:30,866 iteration 2574 : loss : 0.048146, loss_ce: 0.016719
2022-01-20 20:34:32,283 iteration 2575 : loss : 0.041050, loss_ce: 0.016824
2022-01-20 20:34:33,614 iteration 2576 : loss : 0.029977, loss_ce: 0.011297
2022-01-20 20:34:34,937 iteration 2577 : loss : 0.048661, loss_ce: 0.019766
2022-01-20 20:34:36,278 iteration 2578 : loss : 0.045425, loss_ce: 0.014084
2022-01-20 20:34:37,534 iteration 2579 : loss : 0.039629, loss_ce: 0.010598
2022-01-20 20:34:38,902 iteration 2580 : loss : 0.070285, loss_ce: 0.019589
2022-01-20 20:34:40,261 iteration 2581 : loss : 0.053640, loss_ce: 0.021634
2022-01-20 20:34:41,649 iteration 2582 : loss : 0.044074, loss_ce: 0.018027
2022-01-20 20:34:42,955 iteration 2583 : loss : 0.033160, loss_ce: 0.012729
2022-01-20 20:34:44,172 iteration 2584 : loss : 0.033234, loss_ce: 0.012731
 38%|██████████▎                | 152/400 [1:01:54<1:41:25, 24.54s/it]2022-01-20 20:34:45,519 iteration 2585 : loss : 0.027622, loss_ce: 0.009342
2022-01-20 20:34:46,828 iteration 2586 : loss : 0.038128, loss_ce: 0.015581
2022-01-20 20:34:48,066 iteration 2587 : loss : 0.036219, loss_ce: 0.012403
2022-01-20 20:34:49,366 iteration 2588 : loss : 0.033789, loss_ce: 0.015963
2022-01-20 20:34:50,724 iteration 2589 : loss : 0.052731, loss_ce: 0.020997
2022-01-20 20:34:52,088 iteration 2590 : loss : 0.048662, loss_ce: 0.017303
2022-01-20 20:34:53,365 iteration 2591 : loss : 0.030637, loss_ce: 0.010243
2022-01-20 20:34:54,645 iteration 2592 : loss : 0.038135, loss_ce: 0.013095
2022-01-20 20:34:55,939 iteration 2593 : loss : 0.031157, loss_ce: 0.012942
2022-01-20 20:34:57,372 iteration 2594 : loss : 0.037537, loss_ce: 0.015496
2022-01-20 20:34:58,715 iteration 2595 : loss : 0.038976, loss_ce: 0.016525
2022-01-20 20:34:59,996 iteration 2596 : loss : 0.032935, loss_ce: 0.015420
2022-01-20 20:35:01,268 iteration 2597 : loss : 0.039601, loss_ce: 0.016049
2022-01-20 20:35:02,553 iteration 2598 : loss : 0.032586, loss_ce: 0.011732
2022-01-20 20:35:03,847 iteration 2599 : loss : 0.040879, loss_ce: 0.010938
2022-01-20 20:35:05,141 iteration 2600 : loss : 0.040932, loss_ce: 0.014593
2022-01-20 20:35:06,401 iteration 2601 : loss : 0.036018, loss_ce: 0.015391
 38%|██████████▎                | 153/400 [1:02:16<1:38:09, 23.84s/it]2022-01-20 20:35:07,769 iteration 2602 : loss : 0.034795, loss_ce: 0.014289
2022-01-20 20:35:09,006 iteration 2603 : loss : 0.027014, loss_ce: 0.011803
2022-01-20 20:35:10,291 iteration 2604 : loss : 0.027230, loss_ce: 0.009358
2022-01-20 20:35:11,706 iteration 2605 : loss : 0.032358, loss_ce: 0.010956
2022-01-20 20:35:13,012 iteration 2606 : loss : 0.039775, loss_ce: 0.015550
2022-01-20 20:35:14,330 iteration 2607 : loss : 0.035553, loss_ce: 0.013953
2022-01-20 20:35:15,592 iteration 2608 : loss : 0.040568, loss_ce: 0.020274
2022-01-20 20:35:16,856 iteration 2609 : loss : 0.042454, loss_ce: 0.012311
2022-01-20 20:35:18,266 iteration 2610 : loss : 0.045556, loss_ce: 0.020376
2022-01-20 20:35:19,531 iteration 2611 : loss : 0.032286, loss_ce: 0.010609
2022-01-20 20:35:20,749 iteration 2612 : loss : 0.033987, loss_ce: 0.015691
2022-01-20 20:35:22,040 iteration 2613 : loss : 0.053779, loss_ce: 0.017823
2022-01-20 20:35:23,416 iteration 2614 : loss : 0.063468, loss_ce: 0.022283
2022-01-20 20:35:24,691 iteration 2615 : loss : 0.038601, loss_ce: 0.015297
2022-01-20 20:35:25,980 iteration 2616 : loss : 0.030861, loss_ce: 0.011543
2022-01-20 20:35:27,364 iteration 2617 : loss : 0.040115, loss_ce: 0.010901
2022-01-20 20:35:28,687 iteration 2618 : loss : 0.046431, loss_ce: 0.023658
 38%|██████████▍                | 154/400 [1:02:38<1:35:50, 23.38s/it]2022-01-20 20:35:30,077 iteration 2619 : loss : 0.053733, loss_ce: 0.019438
2022-01-20 20:35:31,350 iteration 2620 : loss : 0.051759, loss_ce: 0.019411
2022-01-20 20:35:32,689 iteration 2621 : loss : 0.039281, loss_ce: 0.013211
2022-01-20 20:35:34,026 iteration 2622 : loss : 0.044200, loss_ce: 0.020028
2022-01-20 20:35:35,368 iteration 2623 : loss : 0.062134, loss_ce: 0.033470
2022-01-20 20:35:36,632 iteration 2624 : loss : 0.036481, loss_ce: 0.011137
2022-01-20 20:35:37,915 iteration 2625 : loss : 0.030173, loss_ce: 0.012143
2022-01-20 20:35:39,277 iteration 2626 : loss : 0.041570, loss_ce: 0.018961
2022-01-20 20:35:40,606 iteration 2627 : loss : 0.060823, loss_ce: 0.021071
2022-01-20 20:35:41,873 iteration 2628 : loss : 0.041642, loss_ce: 0.015876
2022-01-20 20:35:43,234 iteration 2629 : loss : 0.047080, loss_ce: 0.018269
2022-01-20 20:35:44,480 iteration 2630 : loss : 0.025588, loss_ce: 0.008227
2022-01-20 20:35:45,811 iteration 2631 : loss : 0.032931, loss_ce: 0.014570
2022-01-20 20:35:47,158 iteration 2632 : loss : 0.047064, loss_ce: 0.016877
2022-01-20 20:35:48,423 iteration 2633 : loss : 0.032727, loss_ce: 0.014397
2022-01-20 20:35:49,758 iteration 2634 : loss : 0.026109, loss_ce: 0.008602
2022-01-20 20:35:49,758 Training Data Eval:
2022-01-20 20:35:56,249   Average segmentation loss on training set: 0.0257
2022-01-20 20:35:56,250 Validation Data Eval:
2022-01-20 20:35:58,497   Average segmentation loss on validation set: 0.0680
2022-01-20 20:36:03,166 Found new lowest validation loss at iteration 2634! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed1234.pth
2022-01-20 20:36:04,489 iteration 2635 : loss : 0.047030, loss_ce: 0.020813
 39%|██████████▍                | 155/400 [1:03:14<1:50:40, 27.10s/it]2022-01-20 20:36:05,731 iteration 2636 : loss : 0.029743, loss_ce: 0.009256
2022-01-20 20:36:07,105 iteration 2637 : loss : 0.059474, loss_ce: 0.018363
2022-01-20 20:36:08,356 iteration 2638 : loss : 0.037619, loss_ce: 0.015221
2022-01-20 20:36:09,484 iteration 2639 : loss : 0.022600, loss_ce: 0.008313
2022-01-20 20:36:10,667 iteration 2640 : loss : 0.036756, loss_ce: 0.013818
2022-01-20 20:36:11,986 iteration 2641 : loss : 0.035213, loss_ce: 0.014248
2022-01-20 20:36:13,269 iteration 2642 : loss : 0.045619, loss_ce: 0.013732
2022-01-20 20:36:14,543 iteration 2643 : loss : 0.036623, loss_ce: 0.012725
2022-01-20 20:36:15,747 iteration 2644 : loss : 0.037900, loss_ce: 0.011618
2022-01-20 20:36:16,945 iteration 2645 : loss : 0.037521, loss_ce: 0.019985
2022-01-20 20:36:18,155 iteration 2646 : loss : 0.035507, loss_ce: 0.011392
2022-01-20 20:36:19,449 iteration 2647 : loss : 0.045282, loss_ce: 0.023429
2022-01-20 20:36:20,657 iteration 2648 : loss : 0.022107, loss_ce: 0.010577
2022-01-20 20:36:22,003 iteration 2649 : loss : 0.051718, loss_ce: 0.023681
2022-01-20 20:36:23,348 iteration 2650 : loss : 0.038603, loss_ce: 0.017342
2022-01-20 20:36:24,656 iteration 2651 : loss : 0.042126, loss_ce: 0.015030
2022-01-20 20:36:25,993 iteration 2652 : loss : 0.032009, loss_ce: 0.014622
 39%|██████████▌                | 156/400 [1:03:35<1:43:23, 25.43s/it]2022-01-20 20:36:27,459 iteration 2653 : loss : 0.048394, loss_ce: 0.017860
2022-01-20 20:36:28,787 iteration 2654 : loss : 0.042456, loss_ce: 0.017903
2022-01-20 20:36:30,048 iteration 2655 : loss : 0.026074, loss_ce: 0.009990
2022-01-20 20:36:31,302 iteration 2656 : loss : 0.025571, loss_ce: 0.011414
2022-01-20 20:36:32,574 iteration 2657 : loss : 0.030382, loss_ce: 0.011039
2022-01-20 20:36:33,861 iteration 2658 : loss : 0.029640, loss_ce: 0.011487
2022-01-20 20:36:35,283 iteration 2659 : loss : 0.045079, loss_ce: 0.018817
2022-01-20 20:36:36,589 iteration 2660 : loss : 0.034549, loss_ce: 0.013379
2022-01-20 20:36:37,821 iteration 2661 : loss : 0.031336, loss_ce: 0.012454
2022-01-20 20:36:39,176 iteration 2662 : loss : 0.038694, loss_ce: 0.015195
2022-01-20 20:36:40,434 iteration 2663 : loss : 0.035876, loss_ce: 0.014220
2022-01-20 20:36:41,803 iteration 2664 : loss : 0.022199, loss_ce: 0.007711
2022-01-20 20:36:43,093 iteration 2665 : loss : 0.036281, loss_ce: 0.012489
2022-01-20 20:36:44,408 iteration 2666 : loss : 0.039917, loss_ce: 0.014257
2022-01-20 20:36:45,740 iteration 2667 : loss : 0.030081, loss_ce: 0.010373
2022-01-20 20:36:47,060 iteration 2668 : loss : 0.043720, loss_ce: 0.013705
2022-01-20 20:36:48,392 iteration 2669 : loss : 0.032982, loss_ce: 0.013289
 39%|██████████▌                | 157/400 [1:03:58<1:39:17, 24.52s/it]2022-01-20 20:36:49,744 iteration 2670 : loss : 0.036054, loss_ce: 0.011225
2022-01-20 20:36:51,060 iteration 2671 : loss : 0.033215, loss_ce: 0.009725
2022-01-20 20:36:52,389 iteration 2672 : loss : 0.028031, loss_ce: 0.010802
2022-01-20 20:36:53,741 iteration 2673 : loss : 0.053980, loss_ce: 0.019178
2022-01-20 20:36:55,017 iteration 2674 : loss : 0.038864, loss_ce: 0.019137
2022-01-20 20:36:56,384 iteration 2675 : loss : 0.039555, loss_ce: 0.014238
2022-01-20 20:36:57,712 iteration 2676 : loss : 0.034596, loss_ce: 0.009855
2022-01-20 20:36:58,964 iteration 2677 : loss : 0.025806, loss_ce: 0.011815
2022-01-20 20:37:00,209 iteration 2678 : loss : 0.025775, loss_ce: 0.008231
2022-01-20 20:37:01,552 iteration 2679 : loss : 0.028120, loss_ce: 0.013483
2022-01-20 20:37:02,929 iteration 2680 : loss : 0.033722, loss_ce: 0.013903
2022-01-20 20:37:04,230 iteration 2681 : loss : 0.032965, loss_ce: 0.017171
2022-01-20 20:37:05,488 iteration 2682 : loss : 0.028907, loss_ce: 0.012349
2022-01-20 20:37:06,814 iteration 2683 : loss : 0.040209, loss_ce: 0.020199
2022-01-20 20:37:08,125 iteration 2684 : loss : 0.057174, loss_ce: 0.018463
2022-01-20 20:37:09,443 iteration 2685 : loss : 0.030895, loss_ce: 0.013557
2022-01-20 20:37:10,745 iteration 2686 : loss : 0.033880, loss_ce: 0.013095
 40%|██████████▋                | 158/400 [1:04:20<1:36:16, 23.87s/it]2022-01-20 20:37:12,075 iteration 2687 : loss : 0.021092, loss_ce: 0.008959
2022-01-20 20:37:13,380 iteration 2688 : loss : 0.039279, loss_ce: 0.015831
2022-01-20 20:37:14,594 iteration 2689 : loss : 0.026722, loss_ce: 0.010425
2022-01-20 20:37:15,880 iteration 2690 : loss : 0.037138, loss_ce: 0.015269
2022-01-20 20:37:17,195 iteration 2691 : loss : 0.028313, loss_ce: 0.009551
2022-01-20 20:37:18,501 iteration 2692 : loss : 0.032232, loss_ce: 0.010657
2022-01-20 20:37:19,777 iteration 2693 : loss : 0.032548, loss_ce: 0.013052
2022-01-20 20:37:21,010 iteration 2694 : loss : 0.040202, loss_ce: 0.015619
2022-01-20 20:37:22,364 iteration 2695 : loss : 0.049415, loss_ce: 0.019027
2022-01-20 20:37:23,755 iteration 2696 : loss : 0.050699, loss_ce: 0.020311
2022-01-20 20:37:25,109 iteration 2697 : loss : 0.041814, loss_ce: 0.020830
2022-01-20 20:37:26,365 iteration 2698 : loss : 0.025465, loss_ce: 0.010217
2022-01-20 20:37:27,639 iteration 2699 : loss : 0.036105, loss_ce: 0.011513
2022-01-20 20:37:28,902 iteration 2700 : loss : 0.035759, loss_ce: 0.011109
2022-01-20 20:37:30,212 iteration 2701 : loss : 0.039337, loss_ce: 0.019186
2022-01-20 20:37:31,557 iteration 2702 : loss : 0.047772, loss_ce: 0.010652
2022-01-20 20:37:32,906 iteration 2703 : loss : 0.035431, loss_ce: 0.011849
 40%|██████████▋                | 159/400 [1:04:42<1:33:48, 23.36s/it]2022-01-20 20:37:34,211 iteration 2704 : loss : 0.025727, loss_ce: 0.007424
2022-01-20 20:37:35,471 iteration 2705 : loss : 0.033350, loss_ce: 0.012936
2022-01-20 20:37:36,807 iteration 2706 : loss : 0.043598, loss_ce: 0.021950
2022-01-20 20:37:38,105 iteration 2707 : loss : 0.033101, loss_ce: 0.011597
2022-01-20 20:37:39,390 iteration 2708 : loss : 0.040101, loss_ce: 0.015280
2022-01-20 20:37:40,759 iteration 2709 : loss : 0.047699, loss_ce: 0.015522
2022-01-20 20:37:42,100 iteration 2710 : loss : 0.050133, loss_ce: 0.021762
2022-01-20 20:37:43,383 iteration 2711 : loss : 0.031777, loss_ce: 0.011534
2022-01-20 20:37:44,681 iteration 2712 : loss : 0.035434, loss_ce: 0.012299
2022-01-20 20:37:45,988 iteration 2713 : loss : 0.026177, loss_ce: 0.009843
2022-01-20 20:37:47,300 iteration 2714 : loss : 0.034074, loss_ce: 0.013346
2022-01-20 20:37:48,596 iteration 2715 : loss : 0.032496, loss_ce: 0.017688
2022-01-20 20:37:49,887 iteration 2716 : loss : 0.061721, loss_ce: 0.018904
2022-01-20 20:37:51,135 iteration 2717 : loss : 0.033911, loss_ce: 0.013236
2022-01-20 20:37:52,473 iteration 2718 : loss : 0.031390, loss_ce: 0.011347
2022-01-20 20:37:53,741 iteration 2719 : loss : 0.039607, loss_ce: 0.016560
2022-01-20 20:37:53,741 Training Data Eval:
2022-01-20 20:38:00,239   Average segmentation loss on training set: 0.0239
2022-01-20 20:38:00,240 Validation Data Eval:
2022-01-20 20:38:02,492   Average segmentation loss on validation set: 0.0769
2022-01-20 20:38:03,795 iteration 2720 : loss : 0.033950, loss_ce: 0.014793
 40%|██████████▊                | 160/400 [1:05:13<1:42:27, 25.61s/it]2022-01-20 20:38:05,162 iteration 2721 : loss : 0.031192, loss_ce: 0.010066
2022-01-20 20:38:06,502 iteration 2722 : loss : 0.034649, loss_ce: 0.018631
2022-01-20 20:38:07,884 iteration 2723 : loss : 0.035346, loss_ce: 0.015890
2022-01-20 20:38:09,137 iteration 2724 : loss : 0.045796, loss_ce: 0.016208
2022-01-20 20:38:10,428 iteration 2725 : loss : 0.038470, loss_ce: 0.010866
2022-01-20 20:38:11,799 iteration 2726 : loss : 0.028681, loss_ce: 0.010279
2022-01-20 20:38:13,098 iteration 2727 : loss : 0.031055, loss_ce: 0.011773
2022-01-20 20:38:14,433 iteration 2728 : loss : 0.035093, loss_ce: 0.015871
2022-01-20 20:38:15,769 iteration 2729 : loss : 0.044310, loss_ce: 0.021358
2022-01-20 20:38:17,134 iteration 2730 : loss : 0.038699, loss_ce: 0.014354
2022-01-20 20:38:18,432 iteration 2731 : loss : 0.033729, loss_ce: 0.012368
2022-01-20 20:38:19,656 iteration 2732 : loss : 0.025994, loss_ce: 0.010644
2022-01-20 20:38:20,956 iteration 2733 : loss : 0.049341, loss_ce: 0.022252
2022-01-20 20:38:22,262 iteration 2734 : loss : 0.049129, loss_ce: 0.016808
2022-01-20 20:38:23,570 iteration 2735 : loss : 0.035759, loss_ce: 0.016303
2022-01-20 20:38:24,858 iteration 2736 : loss : 0.033659, loss_ce: 0.010597
2022-01-20 20:38:26,158 iteration 2737 : loss : 0.041844, loss_ce: 0.011123
 40%|██████████▊                | 161/400 [1:05:35<1:38:08, 24.64s/it]2022-01-20 20:38:27,562 iteration 2738 : loss : 0.053283, loss_ce: 0.025571
2022-01-20 20:38:28,833 iteration 2739 : loss : 0.038391, loss_ce: 0.011222
2022-01-20 20:38:30,224 iteration 2740 : loss : 0.047011, loss_ce: 0.021375
2022-01-20 20:38:31,533 iteration 2741 : loss : 0.033580, loss_ce: 0.014198
2022-01-20 20:38:32,846 iteration 2742 : loss : 0.039867, loss_ce: 0.019101
2022-01-20 20:38:34,095 iteration 2743 : loss : 0.032684, loss_ce: 0.012094
2022-01-20 20:38:35,384 iteration 2744 : loss : 0.030146, loss_ce: 0.009540
2022-01-20 20:38:36,646 iteration 2745 : loss : 0.029002, loss_ce: 0.008364
2022-01-20 20:38:38,036 iteration 2746 : loss : 0.090810, loss_ce: 0.021752
2022-01-20 20:38:39,313 iteration 2747 : loss : 0.030331, loss_ce: 0.011671
2022-01-20 20:38:40,658 iteration 2748 : loss : 0.038675, loss_ce: 0.010107
2022-01-20 20:38:42,024 iteration 2749 : loss : 0.044195, loss_ce: 0.016899
2022-01-20 20:38:43,305 iteration 2750 : loss : 0.037204, loss_ce: 0.014385
2022-01-20 20:38:44,598 iteration 2751 : loss : 0.047166, loss_ce: 0.012464
2022-01-20 20:38:45,995 iteration 2752 : loss : 0.047047, loss_ce: 0.017022
2022-01-20 20:38:47,393 iteration 2753 : loss : 0.037195, loss_ce: 0.015216
2022-01-20 20:38:48,709 iteration 2754 : loss : 0.031336, loss_ce: 0.015522
 40%|██████████▉                | 162/400 [1:05:58<1:35:14, 24.01s/it]2022-01-20 20:38:49,935 iteration 2755 : loss : 0.029661, loss_ce: 0.010141
2022-01-20 20:38:51,261 iteration 2756 : loss : 0.035499, loss_ce: 0.013294
2022-01-20 20:38:52,564 iteration 2757 : loss : 0.032964, loss_ce: 0.013141
2022-01-20 20:38:53,869 iteration 2758 : loss : 0.033959, loss_ce: 0.012910
2022-01-20 20:38:55,175 iteration 2759 : loss : 0.051131, loss_ce: 0.018205
2022-01-20 20:38:56,475 iteration 2760 : loss : 0.030703, loss_ce: 0.013558
2022-01-20 20:38:57,817 iteration 2761 : loss : 0.043266, loss_ce: 0.013969
2022-01-20 20:38:59,104 iteration 2762 : loss : 0.030439, loss_ce: 0.013044
2022-01-20 20:39:00,436 iteration 2763 : loss : 0.044770, loss_ce: 0.012272
2022-01-20 20:39:01,681 iteration 2764 : loss : 0.023658, loss_ce: 0.008493
2022-01-20 20:39:03,090 iteration 2765 : loss : 0.028762, loss_ce: 0.008290
2022-01-20 20:39:04,327 iteration 2766 : loss : 0.029272, loss_ce: 0.010435
2022-01-20 20:39:05,639 iteration 2767 : loss : 0.047801, loss_ce: 0.021175
2022-01-20 20:39:06,909 iteration 2768 : loss : 0.034899, loss_ce: 0.011393
2022-01-20 20:39:08,145 iteration 2769 : loss : 0.027991, loss_ce: 0.011952
2022-01-20 20:39:09,425 iteration 2770 : loss : 0.034034, loss_ce: 0.015323
2022-01-20 20:39:10,733 iteration 2771 : loss : 0.027089, loss_ce: 0.011929
 41%|███████████                | 163/400 [1:06:20<1:32:29, 23.42s/it]2022-01-20 20:39:12,009 iteration 2772 : loss : 0.031217, loss_ce: 0.011618
2022-01-20 20:39:13,331 iteration 2773 : loss : 0.026960, loss_ce: 0.010149
2022-01-20 20:39:14,644 iteration 2774 : loss : 0.063806, loss_ce: 0.013093
2022-01-20 20:39:15,965 iteration 2775 : loss : 0.042725, loss_ce: 0.016869
2022-01-20 20:39:17,251 iteration 2776 : loss : 0.031767, loss_ce: 0.011277
2022-01-20 20:39:18,609 iteration 2777 : loss : 0.027311, loss_ce: 0.010296
2022-01-20 20:39:19,875 iteration 2778 : loss : 0.024400, loss_ce: 0.008004
2022-01-20 20:39:21,145 iteration 2779 : loss : 0.024588, loss_ce: 0.009463
2022-01-20 20:39:22,447 iteration 2780 : loss : 0.027609, loss_ce: 0.016078
2022-01-20 20:39:23,861 iteration 2781 : loss : 0.044915, loss_ce: 0.017097
2022-01-20 20:39:25,235 iteration 2782 : loss : 0.030706, loss_ce: 0.013856
2022-01-20 20:39:26,525 iteration 2783 : loss : 0.027694, loss_ce: 0.008578
2022-01-20 20:39:27,738 iteration 2784 : loss : 0.034941, loss_ce: 0.010257
2022-01-20 20:39:29,008 iteration 2785 : loss : 0.034391, loss_ce: 0.016258
2022-01-20 20:39:30,268 iteration 2786 : loss : 0.028652, loss_ce: 0.012548
2022-01-20 20:39:31,548 iteration 2787 : loss : 0.025683, loss_ce: 0.009918
2022-01-20 20:39:32,981 iteration 2788 : loss : 0.041416, loss_ce: 0.017376
 41%|███████████                | 164/400 [1:06:42<1:30:43, 23.07s/it]2022-01-20 20:39:34,342 iteration 2789 : loss : 0.026891, loss_ce: 0.008331
2022-01-20 20:39:35,720 iteration 2790 : loss : 0.042294, loss_ce: 0.018207
2022-01-20 20:39:36,955 iteration 2791 : loss : 0.024983, loss_ce: 0.008041
2022-01-20 20:39:38,233 iteration 2792 : loss : 0.044486, loss_ce: 0.020130
2022-01-20 20:39:39,536 iteration 2793 : loss : 0.029995, loss_ce: 0.010697
2022-01-20 20:39:40,891 iteration 2794 : loss : 0.028790, loss_ce: 0.012893
2022-01-20 20:39:42,189 iteration 2795 : loss : 0.033505, loss_ce: 0.013583
2022-01-20 20:39:43,468 iteration 2796 : loss : 0.033609, loss_ce: 0.012531
2022-01-20 20:39:44,810 iteration 2797 : loss : 0.035072, loss_ce: 0.015788
2022-01-20 20:39:46,133 iteration 2798 : loss : 0.040215, loss_ce: 0.012737
2022-01-20 20:39:47,519 iteration 2799 : loss : 0.042337, loss_ce: 0.015447
2022-01-20 20:39:48,832 iteration 2800 : loss : 0.032926, loss_ce: 0.016429
2022-01-20 20:39:50,051 iteration 2801 : loss : 0.040651, loss_ce: 0.011343
2022-01-20 20:39:51,322 iteration 2802 : loss : 0.021561, loss_ce: 0.007551
2022-01-20 20:39:52,654 iteration 2803 : loss : 0.033555, loss_ce: 0.012973
2022-01-20 20:39:53,877 iteration 2804 : loss : 0.032664, loss_ce: 0.013276
2022-01-20 20:39:53,877 Training Data Eval:
2022-01-20 20:40:00,388   Average segmentation loss on training set: 0.0254
2022-01-20 20:40:00,388 Validation Data Eval:
2022-01-20 20:40:02,623   Average segmentation loss on validation set: 0.0807
2022-01-20 20:40:04,028 iteration 2805 : loss : 0.038870, loss_ce: 0.016112
 41%|███████████▏               | 165/400 [1:07:13<1:39:43, 25.46s/it]2022-01-20 20:40:05,433 iteration 2806 : loss : 0.036464, loss_ce: 0.015291
2022-01-20 20:40:06,811 iteration 2807 : loss : 0.051767, loss_ce: 0.014302
2022-01-20 20:40:08,156 iteration 2808 : loss : 0.052481, loss_ce: 0.015193
2022-01-20 20:40:09,484 iteration 2809 : loss : 0.036872, loss_ce: 0.014273
2022-01-20 20:40:10,723 iteration 2810 : loss : 0.036034, loss_ce: 0.012636
2022-01-20 20:40:12,070 iteration 2811 : loss : 0.028188, loss_ce: 0.013148
2022-01-20 20:40:13,310 iteration 2812 : loss : 0.021482, loss_ce: 0.008428
2022-01-20 20:40:14,671 iteration 2813 : loss : 0.047873, loss_ce: 0.021611
2022-01-20 20:40:15,987 iteration 2814 : loss : 0.043271, loss_ce: 0.017625
2022-01-20 20:40:17,281 iteration 2815 : loss : 0.031238, loss_ce: 0.013757
2022-01-20 20:40:18,573 iteration 2816 : loss : 0.034246, loss_ce: 0.011538
2022-01-20 20:40:19,950 iteration 2817 : loss : 0.051185, loss_ce: 0.012390
2022-01-20 20:40:21,308 iteration 2818 : loss : 0.037442, loss_ce: 0.015676
2022-01-20 20:40:22,610 iteration 2819 : loss : 0.044809, loss_ce: 0.019283
2022-01-20 20:40:24,007 iteration 2820 : loss : 0.054134, loss_ce: 0.018464
2022-01-20 20:40:25,379 iteration 2821 : loss : 0.052037, loss_ce: 0.019002
2022-01-20 20:40:26,706 iteration 2822 : loss : 0.050614, loss_ce: 0.017651
 42%|███████████▏               | 166/400 [1:07:36<1:36:02, 24.63s/it]2022-01-20 20:40:28,133 iteration 2823 : loss : 0.056312, loss_ce: 0.027952
2022-01-20 20:40:29,396 iteration 2824 : loss : 0.033707, loss_ce: 0.015548
2022-01-20 20:40:30,709 iteration 2825 : loss : 0.043045, loss_ce: 0.017460
2022-01-20 20:40:32,108 iteration 2826 : loss : 0.055900, loss_ce: 0.020314
2022-01-20 20:40:33,523 iteration 2827 : loss : 0.061310, loss_ce: 0.020891
2022-01-20 20:40:34,855 iteration 2828 : loss : 0.037212, loss_ce: 0.014650
2022-01-20 20:40:36,154 iteration 2829 : loss : 0.048996, loss_ce: 0.024568
2022-01-20 20:40:37,440 iteration 2830 : loss : 0.054809, loss_ce: 0.024176
2022-01-20 20:40:38,645 iteration 2831 : loss : 0.035946, loss_ce: 0.015915
2022-01-20 20:40:40,070 iteration 2832 : loss : 0.037644, loss_ce: 0.016979
2022-01-20 20:40:41,363 iteration 2833 : loss : 0.035530, loss_ce: 0.012616
2022-01-20 20:40:42,722 iteration 2834 : loss : 0.054198, loss_ce: 0.021545
2022-01-20 20:40:44,051 iteration 2835 : loss : 0.047679, loss_ce: 0.015969
2022-01-20 20:40:45,351 iteration 2836 : loss : 0.043505, loss_ce: 0.014912
2022-01-20 20:40:46,707 iteration 2837 : loss : 0.045383, loss_ce: 0.017509
2022-01-20 20:40:47,948 iteration 2838 : loss : 0.025356, loss_ce: 0.010853
2022-01-20 20:40:49,289 iteration 2839 : loss : 0.032630, loss_ce: 0.012233
 42%|███████████▎               | 167/400 [1:07:59<1:33:15, 24.01s/it]2022-01-20 20:40:50,681 iteration 2840 : loss : 0.043960, loss_ce: 0.014972
2022-01-20 20:40:51,962 iteration 2841 : loss : 0.030791, loss_ce: 0.009472
2022-01-20 20:40:53,255 iteration 2842 : loss : 0.041709, loss_ce: 0.014967
2022-01-20 20:40:54,605 iteration 2843 : loss : 0.042070, loss_ce: 0.011205
2022-01-20 20:40:55,932 iteration 2844 : loss : 0.043960, loss_ce: 0.016761
2022-01-20 20:40:57,243 iteration 2845 : loss : 0.032462, loss_ce: 0.011946
2022-01-20 20:40:58,550 iteration 2846 : loss : 0.025359, loss_ce: 0.008978
2022-01-20 20:40:59,744 iteration 2847 : loss : 0.022164, loss_ce: 0.011441
2022-01-20 20:41:01,081 iteration 2848 : loss : 0.049438, loss_ce: 0.015875
2022-01-20 20:41:02,459 iteration 2849 : loss : 0.037134, loss_ce: 0.012983
2022-01-20 20:41:03,810 iteration 2850 : loss : 0.043636, loss_ce: 0.014366
2022-01-20 20:41:05,174 iteration 2851 : loss : 0.039816, loss_ce: 0.015523
2022-01-20 20:41:06,496 iteration 2852 : loss : 0.052234, loss_ce: 0.013281
2022-01-20 20:41:07,867 iteration 2853 : loss : 0.032433, loss_ce: 0.011789
2022-01-20 20:41:09,208 iteration 2854 : loss : 0.033977, loss_ce: 0.011446
2022-01-20 20:41:10,485 iteration 2855 : loss : 0.037780, loss_ce: 0.018634
2022-01-20 20:41:11,826 iteration 2856 : loss : 0.040088, loss_ce: 0.018950
 42%|███████████▎               | 168/400 [1:08:21<1:31:08, 23.57s/it]2022-01-20 20:41:13,084 iteration 2857 : loss : 0.026138, loss_ce: 0.008780
2022-01-20 20:41:14,397 iteration 2858 : loss : 0.031000, loss_ce: 0.013770
2022-01-20 20:41:15,671 iteration 2859 : loss : 0.025136, loss_ce: 0.010633
2022-01-20 20:41:17,011 iteration 2860 : loss : 0.035679, loss_ce: 0.011946
2022-01-20 20:41:18,235 iteration 2861 : loss : 0.034115, loss_ce: 0.014557
2022-01-20 20:41:19,612 iteration 2862 : loss : 0.063436, loss_ce: 0.022681
2022-01-20 20:41:20,868 iteration 2863 : loss : 0.040574, loss_ce: 0.012164
2022-01-20 20:41:22,200 iteration 2864 : loss : 0.030207, loss_ce: 0.012563
2022-01-20 20:41:23,543 iteration 2865 : loss : 0.040827, loss_ce: 0.016158
2022-01-20 20:41:24,831 iteration 2866 : loss : 0.039875, loss_ce: 0.015990
2022-01-20 20:41:26,235 iteration 2867 : loss : 0.047895, loss_ce: 0.021574
2022-01-20 20:41:27,512 iteration 2868 : loss : 0.038030, loss_ce: 0.013294
2022-01-20 20:41:28,898 iteration 2869 : loss : 0.042524, loss_ce: 0.017446
2022-01-20 20:41:30,232 iteration 2870 : loss : 0.033655, loss_ce: 0.012148
2022-01-20 20:41:31,503 iteration 2871 : loss : 0.024235, loss_ce: 0.009812
2022-01-20 20:41:32,742 iteration 2872 : loss : 0.030256, loss_ce: 0.012285
2022-01-20 20:41:34,097 iteration 2873 : loss : 0.044799, loss_ce: 0.014230
 42%|███████████▍               | 169/400 [1:08:43<1:29:15, 23.18s/it]2022-01-20 20:41:35,480 iteration 2874 : loss : 0.036817, loss_ce: 0.012303
2022-01-20 20:41:36,786 iteration 2875 : loss : 0.028605, loss_ce: 0.011898
2022-01-20 20:41:38,104 iteration 2876 : loss : 0.039846, loss_ce: 0.016851
2022-01-20 20:41:39,462 iteration 2877 : loss : 0.039950, loss_ce: 0.011977
2022-01-20 20:41:40,776 iteration 2878 : loss : 0.034880, loss_ce: 0.015219
2022-01-20 20:41:42,131 iteration 2879 : loss : 0.028689, loss_ce: 0.010144
2022-01-20 20:41:43,501 iteration 2880 : loss : 0.040184, loss_ce: 0.012638
2022-01-20 20:41:44,810 iteration 2881 : loss : 0.032676, loss_ce: 0.014456
2022-01-20 20:41:46,163 iteration 2882 : loss : 0.040888, loss_ce: 0.020515
2022-01-20 20:41:47,428 iteration 2883 : loss : 0.040587, loss_ce: 0.016778
2022-01-20 20:41:48,715 iteration 2884 : loss : 0.027071, loss_ce: 0.011477
2022-01-20 20:41:50,040 iteration 2885 : loss : 0.029780, loss_ce: 0.012737
2022-01-20 20:41:51,270 iteration 2886 : loss : 0.051494, loss_ce: 0.014108
2022-01-20 20:41:52,645 iteration 2887 : loss : 0.052530, loss_ce: 0.027428
2022-01-20 20:41:53,975 iteration 2888 : loss : 0.069760, loss_ce: 0.016362
2022-01-20 20:41:55,376 iteration 2889 : loss : 0.032152, loss_ce: 0.011128
2022-01-20 20:41:55,376 Training Data Eval:
2022-01-20 20:42:01,857   Average segmentation loss on training set: 0.0233
2022-01-20 20:42:01,857 Validation Data Eval:
2022-01-20 20:42:04,104   Average segmentation loss on validation set: 0.0887
2022-01-20 20:42:05,489 iteration 2890 : loss : 0.037847, loss_ce: 0.014661
 42%|███████████▍               | 170/400 [1:09:15<1:38:17, 25.64s/it]2022-01-20 20:42:06,882 iteration 2891 : loss : 0.041135, loss_ce: 0.019632
2022-01-20 20:42:08,275 iteration 2892 : loss : 0.040504, loss_ce: 0.011068
2022-01-20 20:42:09,534 iteration 2893 : loss : 0.025557, loss_ce: 0.009260
2022-01-20 20:42:10,923 iteration 2894 : loss : 0.047485, loss_ce: 0.014174
2022-01-20 20:42:12,302 iteration 2895 : loss : 0.045347, loss_ce: 0.024513
2022-01-20 20:42:13,590 iteration 2896 : loss : 0.034593, loss_ce: 0.011219
2022-01-20 20:42:14,909 iteration 2897 : loss : 0.041136, loss_ce: 0.016653
2022-01-20 20:42:16,176 iteration 2898 : loss : 0.031452, loss_ce: 0.015175
2022-01-20 20:42:17,515 iteration 2899 : loss : 0.038013, loss_ce: 0.012500
2022-01-20 20:42:18,817 iteration 2900 : loss : 0.028366, loss_ce: 0.010316
2022-01-20 20:42:20,136 iteration 2901 : loss : 0.027865, loss_ce: 0.009081
2022-01-20 20:42:21,383 iteration 2902 : loss : 0.044529, loss_ce: 0.015815
2022-01-20 20:42:22,661 iteration 2903 : loss : 0.029231, loss_ce: 0.013561
2022-01-20 20:42:23,935 iteration 2904 : loss : 0.039113, loss_ce: 0.017224
2022-01-20 20:42:25,187 iteration 2905 : loss : 0.029629, loss_ce: 0.014379
2022-01-20 20:42:26,533 iteration 2906 : loss : 0.045096, loss_ce: 0.022898
2022-01-20 20:42:27,800 iteration 2907 : loss : 0.037203, loss_ce: 0.012741
 43%|███████████▌               | 171/400 [1:09:37<1:34:03, 24.65s/it]2022-01-20 20:42:29,178 iteration 2908 : loss : 0.058581, loss_ce: 0.027580
2022-01-20 20:42:30,537 iteration 2909 : loss : 0.049711, loss_ce: 0.023777
2022-01-20 20:42:31,851 iteration 2910 : loss : 0.031059, loss_ce: 0.014234
2022-01-20 20:42:33,204 iteration 2911 : loss : 0.039719, loss_ce: 0.012775
2022-01-20 20:42:34,514 iteration 2912 : loss : 0.029602, loss_ce: 0.010950
2022-01-20 20:42:35,883 iteration 2913 : loss : 0.036267, loss_ce: 0.016158
2022-01-20 20:42:37,169 iteration 2914 : loss : 0.045214, loss_ce: 0.015951
slurmstepd: error: *** JOB 509089 ON biwirender06 CANCELLED AT 2022-01-20T20:42:37 ***
