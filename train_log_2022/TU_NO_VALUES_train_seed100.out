2022-01-09 00:38:06,828 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-09 00:38:06,829 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-09 00:38:06,829 ============================================================
2022-01-09 00:38:06,829 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-09 00:38:06,829 ============================================================
2022-01-09 00:38:06,829 Loading data...
2022-01-09 00:38:06,829 Reading NCI - RUNMC images...
2022-01-09 00:38:06,829 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-09 00:38:06,829 Already preprocessed this configuration. Loading now!
2022-01-09 00:38:06,846 Training Images: (256, 256, 286)
2022-01-09 00:38:06,846 Training Labels: (256, 256, 286)
2022-01-09 00:38:06,846 Validation Images: (256, 256, 98)
2022-01-09 00:38:06,846 Validation Labels: (256, 256, 98)
2022-01-09 00:38:06,846 ============================================================
2022-01-09 00:38:06,880 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-09 00:38:09,471 iteration 1 : loss : 0.765833, loss_ce: 0.864111
2022-01-09 00:38:10,841 iteration 2 : loss : 0.723706, loss_ce: 0.774479
2022-01-09 00:38:12,192 iteration 3 : loss : 0.681187, loss_ce: 0.689093
2022-01-09 00:38:13,594 iteration 4 : loss : 0.630940, loss_ce: 0.643906
2022-01-09 00:38:14,920 iteration 5 : loss : 0.602396, loss_ce: 0.562376
2022-01-09 00:38:16,362 iteration 6 : loss : 0.561894, loss_ce: 0.512580
2022-01-09 00:38:17,774 iteration 7 : loss : 0.543727, loss_ce: 0.467686
2022-01-09 00:38:19,195 iteration 8 : loss : 0.501527, loss_ce: 0.418805
2022-01-09 00:38:20,511 iteration 9 : loss : 0.469537, loss_ce: 0.399112
2022-01-09 00:38:22,003 iteration 10 : loss : 0.459256, loss_ce: 0.367194
2022-01-09 00:38:23,518 iteration 11 : loss : 0.432233, loss_ce: 0.322216
2022-01-09 00:38:25,069 iteration 12 : loss : 0.413541, loss_ce: 0.301907
2022-01-09 00:38:26,536 iteration 13 : loss : 0.389195, loss_ce: 0.263824
2022-01-09 00:38:28,082 iteration 14 : loss : 0.385957, loss_ce: 0.254194
2022-01-09 00:38:29,582 iteration 15 : loss : 0.366868, loss_ce: 0.225687
2022-01-09 00:38:31,061 iteration 16 : loss : 0.365115, loss_ce: 0.225379
2022-01-09 00:38:32,517 iteration 17 : loss : 0.362129, loss_ce: 0.197697
  0%|                               | 1/400 [00:25<2:50:58, 25.71s/it]2022-01-09 00:38:34,115 iteration 18 : loss : 0.332338, loss_ce: 0.194789
2022-01-09 00:38:35,641 iteration 19 : loss : 0.332662, loss_ce: 0.164154
2022-01-09 00:38:37,254 iteration 20 : loss : 0.313383, loss_ce: 0.168325
2022-01-09 00:38:38,749 iteration 21 : loss : 0.338319, loss_ce: 0.165092
2022-01-09 00:38:40,186 iteration 22 : loss : 0.344728, loss_ce: 0.158143
2022-01-09 00:38:41,741 iteration 23 : loss : 0.318187, loss_ce: 0.143160
2022-01-09 00:38:43,294 iteration 24 : loss : 0.282250, loss_ce: 0.142516
2022-01-09 00:38:44,879 iteration 25 : loss : 0.324304, loss_ce: 0.183879
2022-01-09 00:38:46,384 iteration 26 : loss : 0.286871, loss_ce: 0.151952
2022-01-09 00:38:47,844 iteration 27 : loss : 0.308428, loss_ce: 0.155464
2022-01-09 00:38:49,290 iteration 28 : loss : 0.290412, loss_ce: 0.144997
2022-01-09 00:38:50,830 iteration 29 : loss : 0.338650, loss_ce: 0.175400
2022-01-09 00:38:52,307 iteration 30 : loss : 0.291625, loss_ce: 0.138539
2022-01-09 00:38:53,762 iteration 31 : loss : 0.263282, loss_ce: 0.107075
2022-01-09 00:38:55,255 iteration 32 : loss : 0.281489, loss_ce: 0.151066
2022-01-09 00:38:56,797 iteration 33 : loss : 0.302443, loss_ce: 0.146757
2022-01-09 00:38:58,401 iteration 34 : loss : 0.227958, loss_ce: 0.096617
  0%|▏                              | 2/400 [00:51<2:51:08, 25.80s/it]2022-01-09 00:38:59,991 iteration 35 : loss : 0.245273, loss_ce: 0.114540
2022-01-09 00:39:01,518 iteration 36 : loss : 0.289418, loss_ce: 0.098869
2022-01-09 00:39:03,070 iteration 37 : loss : 0.254576, loss_ce: 0.094705
2022-01-09 00:39:04,592 iteration 38 : loss : 0.245735, loss_ce: 0.097259
2022-01-09 00:39:06,017 iteration 39 : loss : 0.294261, loss_ce: 0.129365
2022-01-09 00:39:07,497 iteration 40 : loss : 0.303911, loss_ce: 0.146568
2022-01-09 00:39:08,905 iteration 41 : loss : 0.215881, loss_ce: 0.090528
2022-01-09 00:39:10,433 iteration 42 : loss : 0.259999, loss_ce: 0.122639
2022-01-09 00:39:11,947 iteration 43 : loss : 0.222706, loss_ce: 0.093819
2022-01-09 00:39:13,368 iteration 44 : loss : 0.254111, loss_ce: 0.127607
2022-01-09 00:39:14,956 iteration 45 : loss : 0.257594, loss_ce: 0.106292
2022-01-09 00:39:16,497 iteration 46 : loss : 0.232719, loss_ce: 0.102673
2022-01-09 00:39:18,038 iteration 47 : loss : 0.252770, loss_ce: 0.096412
2022-01-09 00:39:19,587 iteration 48 : loss : 0.220100, loss_ce: 0.098248
2022-01-09 00:39:21,183 iteration 49 : loss : 0.291755, loss_ce: 0.124368
2022-01-09 00:39:22,852 iteration 50 : loss : 0.222511, loss_ce: 0.086737
2022-01-09 00:39:24,551 iteration 51 : loss : 0.219859, loss_ce: 0.085523
  1%|▏                              | 3/400 [01:17<2:51:46, 25.96s/it]2022-01-09 00:39:26,173 iteration 52 : loss : 0.240040, loss_ce: 0.091973
2022-01-09 00:39:27,746 iteration 53 : loss : 0.239264, loss_ce: 0.107739
2022-01-09 00:39:29,483 iteration 54 : loss : 0.237652, loss_ce: 0.095843
2022-01-09 00:39:31,222 iteration 55 : loss : 0.305116, loss_ce: 0.118204
2022-01-09 00:39:33,013 iteration 56 : loss : 0.230648, loss_ce: 0.107124
2022-01-09 00:39:34,786 iteration 57 : loss : 0.248983, loss_ce: 0.114907
2022-01-09 00:39:36,646 iteration 58 : loss : 0.242004, loss_ce: 0.116187
2022-01-09 00:39:38,567 iteration 59 : loss : 0.207613, loss_ce: 0.089639
2022-01-09 00:39:40,488 iteration 60 : loss : 0.223619, loss_ce: 0.101745
2022-01-09 00:39:42,510 iteration 61 : loss : 0.242698, loss_ce: 0.117786
2022-01-09 00:39:44,706 iteration 62 : loss : 0.289599, loss_ce: 0.119435
2022-01-09 00:39:46,835 iteration 63 : loss : 0.255079, loss_ce: 0.119925
2022-01-09 00:39:49,094 iteration 64 : loss : 0.271483, loss_ce: 0.110327
2022-01-09 00:39:51,352 iteration 65 : loss : 0.251574, loss_ce: 0.090628
2022-01-09 00:39:53,690 iteration 66 : loss : 0.263481, loss_ce: 0.101538
2022-01-09 00:39:56,119 iteration 67 : loss : 0.221222, loss_ce: 0.086747
2022-01-09 00:39:58,397 iteration 68 : loss : 0.242534, loss_ce: 0.099275
  1%|▎                              | 4/400 [01:51<3:11:53, 29.07s/it]2022-01-09 00:40:00,848 iteration 69 : loss : 0.283113, loss_ce: 0.116283
2022-01-09 00:40:03,302 iteration 70 : loss : 0.268953, loss_ce: 0.103598
2022-01-09 00:40:05,651 iteration 71 : loss : 0.265232, loss_ce: 0.140542
2022-01-09 00:40:08,137 iteration 72 : loss : 0.231853, loss_ce: 0.104959
2022-01-09 00:40:10,652 iteration 73 : loss : 0.218724, loss_ce: 0.085868
2022-01-09 00:40:13,323 iteration 74 : loss : 0.209588, loss_ce: 0.080837
2022-01-09 00:40:15,759 iteration 75 : loss : 0.262744, loss_ce: 0.136205
2022-01-09 00:40:18,270 iteration 76 : loss : 0.226341, loss_ce: 0.095036
2022-01-09 00:40:20,762 iteration 77 : loss : 0.258787, loss_ce: 0.109229
2022-01-09 00:40:23,371 iteration 78 : loss : 0.214814, loss_ce: 0.078313
2022-01-09 00:40:26,048 iteration 79 : loss : 0.238654, loss_ce: 0.088477
2022-01-09 00:40:28,471 iteration 80 : loss : 0.212615, loss_ce: 0.074985
2022-01-09 00:40:31,035 iteration 81 : loss : 0.285591, loss_ce: 0.132711
2022-01-09 00:40:33,574 iteration 82 : loss : 0.299285, loss_ce: 0.101091
2022-01-09 00:40:36,151 iteration 83 : loss : 0.287482, loss_ce: 0.092242
2022-01-09 00:40:38,867 iteration 84 : loss : 0.281470, loss_ce: 0.127757
2022-01-09 00:40:38,867 Training Data Eval:
2022-01-09 00:40:52,922   Average segmentation loss on training set: 0.3274
2022-01-09 00:40:52,923 Validation Data Eval:
2022-01-09 00:40:57,901   Average segmentation loss on validation set: 0.3909
2022-01-09 00:41:03,618 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 00:41:05,340 iteration 85 : loss : 0.278277, loss_ce: 0.126238
  1%|▍                              | 5/400 [02:58<4:41:18, 42.73s/it]2022-01-09 00:41:07,546 iteration 86 : loss : 0.216154, loss_ce: 0.102464
2022-01-09 00:41:09,999 iteration 87 : loss : 0.250518, loss_ce: 0.109017
2022-01-09 00:41:12,538 iteration 88 : loss : 0.206418, loss_ce: 0.100818
2022-01-09 00:41:15,017 iteration 89 : loss : 0.267307, loss_ce: 0.102895
2022-01-09 00:41:17,635 iteration 90 : loss : 0.203287, loss_ce: 0.085092
2022-01-09 00:41:20,174 iteration 91 : loss : 0.219448, loss_ce: 0.099653
2022-01-09 00:41:22,840 iteration 92 : loss : 0.209809, loss_ce: 0.075882
2022-01-09 00:41:25,354 iteration 93 : loss : 0.213699, loss_ce: 0.071155
2022-01-09 00:41:28,210 iteration 94 : loss : 0.208393, loss_ce: 0.095666
2022-01-09 00:41:30,934 iteration 95 : loss : 0.254116, loss_ce: 0.117391
2022-01-09 00:41:33,581 iteration 96 : loss : 0.193106, loss_ce: 0.072937
2022-01-09 00:41:36,210 iteration 97 : loss : 0.221353, loss_ce: 0.087845
2022-01-09 00:41:39,063 iteration 98 : loss : 0.271973, loss_ce: 0.115431
2022-01-09 00:41:41,703 iteration 99 : loss : 0.251999, loss_ce: 0.102730
2022-01-09 00:41:44,507 iteration 100 : loss : 0.224362, loss_ce: 0.097891
2022-01-09 00:41:47,116 iteration 101 : loss : 0.215890, loss_ce: 0.086273
2022-01-09 00:41:49,917 iteration 102 : loss : 0.205390, loss_ce: 0.097057
  2%|▍                              | 6/400 [03:43<4:44:41, 43.35s/it]2022-01-09 00:41:52,575 iteration 103 : loss : 0.216494, loss_ce: 0.074698
2022-01-09 00:41:55,456 iteration 104 : loss : 0.216765, loss_ce: 0.082852
2022-01-09 00:41:58,087 iteration 105 : loss : 0.164083, loss_ce: 0.061695
2022-01-09 00:42:00,836 iteration 106 : loss : 0.192286, loss_ce: 0.073107
2022-01-09 00:42:03,711 iteration 107 : loss : 0.243914, loss_ce: 0.077325
2022-01-09 00:42:06,298 iteration 108 : loss : 0.213627, loss_ce: 0.068258
2022-01-09 00:42:08,911 iteration 109 : loss : 0.272661, loss_ce: 0.113789
2022-01-09 00:42:11,463 iteration 110 : loss : 0.195560, loss_ce: 0.084619
2022-01-09 00:42:14,109 iteration 111 : loss : 0.163041, loss_ce: 0.078553
2022-01-09 00:42:16,917 iteration 112 : loss : 0.213624, loss_ce: 0.084683
2022-01-09 00:42:19,527 iteration 113 : loss : 0.179837, loss_ce: 0.069580
2022-01-09 00:42:22,279 iteration 114 : loss : 0.191860, loss_ce: 0.072032
2022-01-09 00:42:24,939 iteration 115 : loss : 0.159094, loss_ce: 0.077285
2022-01-09 00:42:27,758 iteration 116 : loss : 0.225676, loss_ce: 0.106875
2022-01-09 00:42:30,539 iteration 117 : loss : 0.194835, loss_ce: 0.079076
2022-01-09 00:42:33,149 iteration 118 : loss : 0.242384, loss_ce: 0.117849
2022-01-09 00:42:35,942 iteration 119 : loss : 0.183644, loss_ce: 0.070084
  2%|▌                              | 7/400 [04:29<4:49:41, 44.23s/it]2022-01-09 00:42:38,798 iteration 120 : loss : 0.194792, loss_ce: 0.080465
2022-01-09 00:42:41,498 iteration 121 : loss : 0.248410, loss_ce: 0.101587
2022-01-09 00:42:44,267 iteration 122 : loss : 0.215506, loss_ce: 0.092248
2022-01-09 00:42:47,074 iteration 123 : loss : 0.161885, loss_ce: 0.063051
2022-01-09 00:42:49,917 iteration 124 : loss : 0.226315, loss_ce: 0.088267
2022-01-09 00:42:52,701 iteration 125 : loss : 0.184615, loss_ce: 0.091352
2022-01-09 00:42:55,500 iteration 126 : loss : 0.263430, loss_ce: 0.101483
2022-01-09 00:42:58,240 iteration 127 : loss : 0.160121, loss_ce: 0.068019
2022-01-09 00:43:01,119 iteration 128 : loss : 0.165485, loss_ce: 0.076422
2022-01-09 00:43:03,714 iteration 129 : loss : 0.171745, loss_ce: 0.063732
2022-01-09 00:43:06,593 iteration 130 : loss : 0.177686, loss_ce: 0.080881
2022-01-09 00:43:09,449 iteration 131 : loss : 0.231250, loss_ce: 0.119517
2022-01-09 00:43:12,049 iteration 132 : loss : 0.190748, loss_ce: 0.070546
2022-01-09 00:43:14,940 iteration 133 : loss : 0.184838, loss_ce: 0.062959
2022-01-09 00:43:17,588 iteration 134 : loss : 0.239925, loss_ce: 0.095108
2022-01-09 00:43:20,384 iteration 135 : loss : 0.184637, loss_ce: 0.076313
2022-01-09 00:43:23,208 iteration 136 : loss : 0.199314, loss_ce: 0.090702
  2%|▌                              | 8/400 [05:16<4:55:15, 45.19s/it]2022-01-09 00:43:25,805 iteration 137 : loss : 0.137288, loss_ce: 0.050385
2022-01-09 00:43:28,395 iteration 138 : loss : 0.180717, loss_ce: 0.094000
2022-01-09 00:43:31,230 iteration 139 : loss : 0.197874, loss_ce: 0.080274
2022-01-09 00:43:34,109 iteration 140 : loss : 0.213628, loss_ce: 0.089362
2022-01-09 00:43:36,893 iteration 141 : loss : 0.247951, loss_ce: 0.096682
2022-01-09 00:43:39,690 iteration 142 : loss : 0.204445, loss_ce: 0.078201
2022-01-09 00:43:42,299 iteration 143 : loss : 0.167005, loss_ce: 0.075916
2022-01-09 00:43:45,165 iteration 144 : loss : 0.239876, loss_ce: 0.098667
2022-01-09 00:43:48,124 iteration 145 : loss : 0.171639, loss_ce: 0.079080
2022-01-09 00:43:50,686 iteration 146 : loss : 0.229524, loss_ce: 0.080692
2022-01-09 00:43:53,491 iteration 147 : loss : 0.188658, loss_ce: 0.076024
2022-01-09 00:43:56,241 iteration 148 : loss : 0.188822, loss_ce: 0.089772
2022-01-09 00:43:59,087 iteration 149 : loss : 0.169669, loss_ce: 0.070982
2022-01-09 00:44:01,924 iteration 150 : loss : 0.215847, loss_ce: 0.113088
2022-01-09 00:44:04,749 iteration 151 : loss : 0.150866, loss_ce: 0.073100
2022-01-09 00:44:07,462 iteration 152 : loss : 0.172045, loss_ce: 0.078456
2022-01-09 00:44:10,224 iteration 153 : loss : 0.136822, loss_ce: 0.063975
  2%|▋                              | 9/400 [06:03<4:58:19, 45.78s/it]2022-01-09 00:44:13,044 iteration 154 : loss : 0.141415, loss_ce: 0.055316
2022-01-09 00:44:15,811 iteration 155 : loss : 0.184131, loss_ce: 0.073571
2022-01-09 00:44:18,692 iteration 156 : loss : 0.178420, loss_ce: 0.072777
2022-01-09 00:44:21,394 iteration 157 : loss : 0.208997, loss_ce: 0.074315
2022-01-09 00:44:24,215 iteration 158 : loss : 0.208070, loss_ce: 0.091632
2022-01-09 00:44:26,838 iteration 159 : loss : 0.195487, loss_ce: 0.071287
2022-01-09 00:44:29,619 iteration 160 : loss : 0.269147, loss_ce: 0.098942
2022-01-09 00:44:32,439 iteration 161 : loss : 0.137105, loss_ce: 0.060163
2022-01-09 00:44:35,160 iteration 162 : loss : 0.156490, loss_ce: 0.069774
2022-01-09 00:44:37,918 iteration 163 : loss : 0.136659, loss_ce: 0.060884
2022-01-09 00:44:40,757 iteration 164 : loss : 0.146383, loss_ce: 0.057463
2022-01-09 00:44:43,547 iteration 165 : loss : 0.114018, loss_ce: 0.045993
2022-01-09 00:44:46,391 iteration 166 : loss : 0.164570, loss_ce: 0.063305
2022-01-09 00:44:49,129 iteration 167 : loss : 0.148411, loss_ce: 0.063605
2022-01-09 00:44:51,967 iteration 168 : loss : 0.151413, loss_ce: 0.069558
2022-01-09 00:44:54,778 iteration 169 : loss : 0.165312, loss_ce: 0.073281
2022-01-09 00:44:54,778 Training Data Eval:
2022-01-09 00:45:09,684   Average segmentation loss on training set: 0.2718
2022-01-09 00:45:09,685 Validation Data Eval:
2022-01-09 00:45:15,148   Average segmentation loss on validation set: 0.2227
2022-01-09 00:45:20,935 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 00:45:22,879 iteration 170 : loss : 0.172649, loss_ce: 0.072193
  2%|▊                             | 10/400 [07:16<5:51:23, 54.06s/it]2022-01-09 00:45:25,573 iteration 171 : loss : 0.160925, loss_ce: 0.072169
2022-01-09 00:45:28,232 iteration 172 : loss : 0.256176, loss_ce: 0.097354
2022-01-09 00:45:30,978 iteration 173 : loss : 0.138642, loss_ce: 0.060863
2022-01-09 00:45:33,652 iteration 174 : loss : 0.177302, loss_ce: 0.071072
2022-01-09 00:45:36,261 iteration 175 : loss : 0.177048, loss_ce: 0.066979
2022-01-09 00:45:38,970 iteration 176 : loss : 0.178030, loss_ce: 0.064471
2022-01-09 00:45:41,819 iteration 177 : loss : 0.179417, loss_ce: 0.067523
2022-01-09 00:45:44,601 iteration 178 : loss : 0.189566, loss_ce: 0.067340
2022-01-09 00:45:47,427 iteration 179 : loss : 0.156159, loss_ce: 0.062943
2022-01-09 00:45:50,264 iteration 180 : loss : 0.212542, loss_ce: 0.066913
2022-01-09 00:45:53,108 iteration 181 : loss : 0.120938, loss_ce: 0.046743
2022-01-09 00:45:55,960 iteration 182 : loss : 0.133768, loss_ce: 0.050259
2022-01-09 00:45:58,831 iteration 183 : loss : 0.156301, loss_ce: 0.060043
2022-01-09 00:46:01,627 iteration 184 : loss : 0.155319, loss_ce: 0.065280
2022-01-09 00:46:04,412 iteration 185 : loss : 0.168752, loss_ce: 0.078608
2022-01-09 00:46:07,005 iteration 186 : loss : 0.138867, loss_ce: 0.060088
2022-01-09 00:46:09,774 iteration 187 : loss : 0.196910, loss_ce: 0.098428
  3%|▊                             | 11/400 [08:02<5:36:16, 51.87s/it]2022-01-09 00:46:12,622 iteration 188 : loss : 0.190548, loss_ce: 0.068094
2022-01-09 00:46:15,470 iteration 189 : loss : 0.185142, loss_ce: 0.084983
2022-01-09 00:46:18,347 iteration 190 : loss : 0.169190, loss_ce: 0.057517
2022-01-09 00:46:21,019 iteration 191 : loss : 0.176613, loss_ce: 0.073023
2022-01-09 00:46:23,553 iteration 192 : loss : 0.140027, loss_ce: 0.061836
2022-01-09 00:46:26,298 iteration 193 : loss : 0.251576, loss_ce: 0.101899
2022-01-09 00:46:29,081 iteration 194 : loss : 0.246740, loss_ce: 0.110489
2022-01-09 00:46:31,957 iteration 195 : loss : 0.148335, loss_ce: 0.064028
2022-01-09 00:46:34,860 iteration 196 : loss : 0.136601, loss_ce: 0.048714
2022-01-09 00:46:37,651 iteration 197 : loss : 0.140064, loss_ce: 0.054612
2022-01-09 00:46:40,507 iteration 198 : loss : 0.168240, loss_ce: 0.076058
2022-01-09 00:46:43,185 iteration 199 : loss : 0.140256, loss_ce: 0.071081
2022-01-09 00:46:45,970 iteration 200 : loss : 0.184535, loss_ce: 0.069270
2022-01-09 00:46:48,677 iteration 201 : loss : 0.184311, loss_ce: 0.069356
2022-01-09 00:46:51,454 iteration 202 : loss : 0.155612, loss_ce: 0.054164
2022-01-09 00:46:54,144 iteration 203 : loss : 0.146659, loss_ce: 0.059712
2022-01-09 00:46:56,993 iteration 204 : loss : 0.140876, loss_ce: 0.071164
  3%|▉                             | 12/400 [08:50<5:26:17, 50.46s/it]2022-01-09 00:46:59,899 iteration 205 : loss : 0.169693, loss_ce: 0.064241
2022-01-09 00:47:02,728 iteration 206 : loss : 0.251764, loss_ce: 0.109587
2022-01-09 00:47:05,619 iteration 207 : loss : 0.185140, loss_ce: 0.070437
2022-01-09 00:47:08,476 iteration 208 : loss : 0.136054, loss_ce: 0.062290
2022-01-09 00:47:11,107 iteration 209 : loss : 0.155467, loss_ce: 0.064398
2022-01-09 00:47:13,759 iteration 210 : loss : 0.175219, loss_ce: 0.082807
2022-01-09 00:47:16,493 iteration 211 : loss : 0.125250, loss_ce: 0.053282
2022-01-09 00:47:19,319 iteration 212 : loss : 0.168181, loss_ce: 0.076979
2022-01-09 00:47:22,213 iteration 213 : loss : 0.160811, loss_ce: 0.072997
2022-01-09 00:47:24,831 iteration 214 : loss : 0.162546, loss_ce: 0.071516
2022-01-09 00:47:27,616 iteration 215 : loss : 0.104499, loss_ce: 0.047245
2022-01-09 00:47:30,474 iteration 216 : loss : 0.140955, loss_ce: 0.056615
2022-01-09 00:47:33,334 iteration 217 : loss : 0.150760, loss_ce: 0.060998
2022-01-09 00:47:36,124 iteration 218 : loss : 0.154278, loss_ce: 0.067260
2022-01-09 00:47:38,928 iteration 219 : loss : 0.128940, loss_ce: 0.061462
2022-01-09 00:47:41,581 iteration 220 : loss : 0.120206, loss_ce: 0.046264
2022-01-09 00:47:44,420 iteration 221 : loss : 0.150666, loss_ce: 0.066751
  3%|▉                             | 13/400 [09:37<5:19:30, 49.54s/it]2022-01-09 00:47:47,274 iteration 222 : loss : 0.115419, loss_ce: 0.046693
2022-01-09 00:47:50,059 iteration 223 : loss : 0.234690, loss_ce: 0.113298
2022-01-09 00:47:52,833 iteration 224 : loss : 0.114086, loss_ce: 0.039209
2022-01-09 00:47:55,540 iteration 225 : loss : 0.118030, loss_ce: 0.049367
2022-01-09 00:47:58,217 iteration 226 : loss : 0.133783, loss_ce: 0.050249
2022-01-09 00:48:00,913 iteration 227 : loss : 0.111384, loss_ce: 0.047208
2022-01-09 00:48:03,707 iteration 228 : loss : 0.129670, loss_ce: 0.042844
2022-01-09 00:48:06,729 iteration 229 : loss : 0.152030, loss_ce: 0.054540
2022-01-09 00:48:09,504 iteration 230 : loss : 0.098946, loss_ce: 0.044402
2022-01-09 00:48:12,099 iteration 231 : loss : 0.114699, loss_ce: 0.044279
2022-01-09 00:48:14,922 iteration 232 : loss : 0.120295, loss_ce: 0.054878
2022-01-09 00:48:17,573 iteration 233 : loss : 0.114224, loss_ce: 0.056721
2022-01-09 00:48:20,394 iteration 234 : loss : 0.156084, loss_ce: 0.062734
2022-01-09 00:48:23,256 iteration 235 : loss : 0.141040, loss_ce: 0.056949
2022-01-09 00:48:26,063 iteration 236 : loss : 0.227173, loss_ce: 0.080437
2022-01-09 00:48:28,810 iteration 237 : loss : 0.178224, loss_ce: 0.084631
2022-01-09 00:48:31,638 iteration 238 : loss : 0.150017, loss_ce: 0.049180
  4%|█                             | 14/400 [10:24<5:14:13, 48.84s/it]2022-01-09 00:48:34,481 iteration 239 : loss : 0.144873, loss_ce: 0.053030
2022-01-09 00:48:37,323 iteration 240 : loss : 0.168229, loss_ce: 0.056049
2022-01-09 00:48:40,270 iteration 241 : loss : 0.153273, loss_ce: 0.053769
2022-01-09 00:48:42,878 iteration 242 : loss : 0.096302, loss_ce: 0.036003
2022-01-09 00:48:45,735 iteration 243 : loss : 0.105000, loss_ce: 0.041152
2022-01-09 00:48:48,356 iteration 244 : loss : 0.133081, loss_ce: 0.051953
2022-01-09 00:48:51,140 iteration 245 : loss : 0.110143, loss_ce: 0.035167
2022-01-09 00:48:53,959 iteration 246 : loss : 0.150006, loss_ce: 0.053915
2022-01-09 00:48:56,746 iteration 247 : loss : 0.152043, loss_ce: 0.075874
2022-01-09 00:48:59,567 iteration 248 : loss : 0.128910, loss_ce: 0.060124
2022-01-09 00:49:02,355 iteration 249 : loss : 0.139520, loss_ce: 0.053734
2022-01-09 00:49:04,984 iteration 250 : loss : 0.144271, loss_ce: 0.054645
2022-01-09 00:49:07,755 iteration 251 : loss : 0.121264, loss_ce: 0.055248
2022-01-09 00:49:10,609 iteration 252 : loss : 0.156685, loss_ce: 0.082922
2022-01-09 00:49:13,431 iteration 253 : loss : 0.107232, loss_ce: 0.039965
2022-01-09 00:49:16,255 iteration 254 : loss : 0.112042, loss_ce: 0.051631
2022-01-09 00:49:16,255 Training Data Eval:
2022-01-09 00:49:31,157   Average segmentation loss on training set: 0.1147
2022-01-09 00:49:31,158 Validation Data Eval:
2022-01-09 00:49:36,364   Average segmentation loss on validation set: 0.1863
2022-01-09 00:49:42,182 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 00:49:44,042 iteration 255 : loss : 0.153451, loss_ce: 0.067671
  4%|█▏                            | 15/400 [11:37<5:58:57, 55.94s/it]2022-01-09 00:49:46,518 iteration 256 : loss : 0.088058, loss_ce: 0.042260
2022-01-09 00:49:49,308 iteration 257 : loss : 0.135877, loss_ce: 0.056460
2022-01-09 00:49:52,001 iteration 258 : loss : 0.105564, loss_ce: 0.044044
2022-01-09 00:49:54,819 iteration 259 : loss : 0.156191, loss_ce: 0.066624
2022-01-09 00:49:57,462 iteration 260 : loss : 0.091790, loss_ce: 0.050937
2022-01-09 00:50:00,242 iteration 261 : loss : 0.136950, loss_ce: 0.056936
2022-01-09 00:50:03,151 iteration 262 : loss : 0.218200, loss_ce: 0.100630
2022-01-09 00:50:05,943 iteration 263 : loss : 0.113629, loss_ce: 0.052602
2022-01-09 00:50:08,631 iteration 264 : loss : 0.186455, loss_ce: 0.061530
2022-01-09 00:50:11,388 iteration 265 : loss : 0.163798, loss_ce: 0.067842
2022-01-09 00:50:14,221 iteration 266 : loss : 0.127292, loss_ce: 0.051513
2022-01-09 00:50:17,136 iteration 267 : loss : 0.139524, loss_ce: 0.037516
2022-01-09 00:50:19,841 iteration 268 : loss : 0.134907, loss_ce: 0.049916
2022-01-09 00:50:22,674 iteration 269 : loss : 0.180628, loss_ce: 0.073785
2022-01-09 00:50:25,292 iteration 270 : loss : 0.147500, loss_ce: 0.054331
2022-01-09 00:50:28,064 iteration 271 : loss : 0.107160, loss_ce: 0.045325
2022-01-09 00:50:30,816 iteration 272 : loss : 0.119864, loss_ce: 0.044514
  4%|█▏                            | 16/400 [12:23<5:40:23, 53.19s/it]2022-01-09 00:50:33,736 iteration 273 : loss : 0.126463, loss_ce: 0.045229
2022-01-09 00:50:36,545 iteration 274 : loss : 0.164678, loss_ce: 0.081321
2022-01-09 00:50:39,167 iteration 275 : loss : 0.167487, loss_ce: 0.050170
2022-01-09 00:50:41,968 iteration 276 : loss : 0.113797, loss_ce: 0.045140
2022-01-09 00:50:44,610 iteration 277 : loss : 0.097256, loss_ce: 0.038843
2022-01-09 00:50:47,398 iteration 278 : loss : 0.162432, loss_ce: 0.061283
2022-01-09 00:50:50,122 iteration 279 : loss : 0.110495, loss_ce: 0.044458
2022-01-09 00:50:52,934 iteration 280 : loss : 0.121032, loss_ce: 0.059711
2022-01-09 00:50:55,780 iteration 281 : loss : 0.088897, loss_ce: 0.048699
2022-01-09 00:50:58,377 iteration 282 : loss : 0.092895, loss_ce: 0.035891
2022-01-09 00:51:01,149 iteration 283 : loss : 0.102042, loss_ce: 0.044484
2022-01-09 00:51:03,984 iteration 284 : loss : 0.105427, loss_ce: 0.060161
2022-01-09 00:51:06,839 iteration 285 : loss : 0.142808, loss_ce: 0.044434
2022-01-09 00:51:09,404 iteration 286 : loss : 0.114357, loss_ce: 0.043648
2022-01-09 00:51:12,190 iteration 287 : loss : 0.116143, loss_ce: 0.048914
2022-01-09 00:51:14,930 iteration 288 : loss : 0.113420, loss_ce: 0.044955
2022-01-09 00:51:17,661 iteration 289 : loss : 0.115146, loss_ce: 0.038537
  4%|█▎                            | 17/400 [13:10<5:27:19, 51.28s/it]2022-01-09 00:51:20,459 iteration 290 : loss : 0.105836, loss_ce: 0.044216
2022-01-09 00:51:23,262 iteration 291 : loss : 0.143793, loss_ce: 0.054309
2022-01-09 00:51:25,842 iteration 292 : loss : 0.117309, loss_ce: 0.048994
2022-01-09 00:51:28,681 iteration 293 : loss : 0.093439, loss_ce: 0.037174
2022-01-09 00:51:31,466 iteration 294 : loss : 0.096917, loss_ce: 0.044163
2022-01-09 00:51:34,304 iteration 295 : loss : 0.092601, loss_ce: 0.039447
2022-01-09 00:51:37,177 iteration 296 : loss : 0.147545, loss_ce: 0.065773
2022-01-09 00:51:40,028 iteration 297 : loss : 0.099377, loss_ce: 0.047210
2022-01-09 00:51:42,657 iteration 298 : loss : 0.126092, loss_ce: 0.064472
2022-01-09 00:51:45,486 iteration 299 : loss : 0.117590, loss_ce: 0.043376
2022-01-09 00:51:48,219 iteration 300 : loss : 0.105501, loss_ce: 0.043563
2022-01-09 00:51:50,964 iteration 301 : loss : 0.147595, loss_ce: 0.053866
2022-01-09 00:51:53,823 iteration 302 : loss : 0.118599, loss_ce: 0.045853
2022-01-09 00:51:56,676 iteration 303 : loss : 0.161304, loss_ce: 0.050595
2022-01-09 00:51:59,518 iteration 304 : loss : 0.138252, loss_ce: 0.058558
2022-01-09 00:52:02,150 iteration 305 : loss : 0.077976, loss_ce: 0.030163
2022-01-09 00:52:05,010 iteration 306 : loss : 0.118851, loss_ce: 0.048249
  4%|█▎                            | 18/400 [13:58<5:18:56, 50.09s/it]2022-01-09 00:52:07,834 iteration 307 : loss : 0.198772, loss_ce: 0.076331
2022-01-09 00:52:10,434 iteration 308 : loss : 0.121042, loss_ce: 0.048936
2022-01-09 00:52:13,100 iteration 309 : loss : 0.133865, loss_ce: 0.058303
2022-01-09 00:52:15,903 iteration 310 : loss : 0.131144, loss_ce: 0.049329
2022-01-09 00:52:18,738 iteration 311 : loss : 0.103669, loss_ce: 0.041056
2022-01-09 00:52:21,565 iteration 312 : loss : 0.118642, loss_ce: 0.047460
2022-01-09 00:52:24,195 iteration 313 : loss : 0.113496, loss_ce: 0.046633
2022-01-09 00:52:26,989 iteration 314 : loss : 0.117293, loss_ce: 0.053924
2022-01-09 00:52:29,851 iteration 315 : loss : 0.132930, loss_ce: 0.056895
2022-01-09 00:52:32,600 iteration 316 : loss : 0.125707, loss_ce: 0.047344
2022-01-09 00:52:35,352 iteration 317 : loss : 0.120507, loss_ce: 0.042174
2022-01-09 00:52:37,952 iteration 318 : loss : 0.126475, loss_ce: 0.049286
2022-01-09 00:52:40,640 iteration 319 : loss : 0.127619, loss_ce: 0.059336
2022-01-09 00:52:43,281 iteration 320 : loss : 0.119765, loss_ce: 0.040986
2022-01-09 00:52:46,162 iteration 321 : loss : 0.089203, loss_ce: 0.045681
2022-01-09 00:52:48,868 iteration 322 : loss : 0.100605, loss_ce: 0.039706
2022-01-09 00:52:51,707 iteration 323 : loss : 0.114445, loss_ce: 0.053171
  5%|█▍                            | 19/400 [14:44<5:11:37, 49.07s/it]2022-01-09 00:52:54,445 iteration 324 : loss : 0.142191, loss_ce: 0.061570
2022-01-09 00:52:57,244 iteration 325 : loss : 0.122789, loss_ce: 0.041020
2022-01-09 00:53:00,036 iteration 326 : loss : 0.104043, loss_ce: 0.042318
2022-01-09 00:53:02,901 iteration 327 : loss : 0.123262, loss_ce: 0.047296
2022-01-09 00:53:05,767 iteration 328 : loss : 0.137288, loss_ce: 0.058804
2022-01-09 00:53:08,430 iteration 329 : loss : 0.101531, loss_ce: 0.037335
2022-01-09 00:53:11,178 iteration 330 : loss : 0.099283, loss_ce: 0.037323
2022-01-09 00:53:13,974 iteration 331 : loss : 0.105025, loss_ce: 0.037913
2022-01-09 00:53:16,819 iteration 332 : loss : 0.087801, loss_ce: 0.035246
2022-01-09 00:53:19,676 iteration 333 : loss : 0.090321, loss_ce: 0.034534
2022-01-09 00:53:22,380 iteration 334 : loss : 0.109940, loss_ce: 0.047545
2022-01-09 00:53:25,186 iteration 335 : loss : 0.139679, loss_ce: 0.048701
2022-01-09 00:53:28,023 iteration 336 : loss : 0.093866, loss_ce: 0.036184
2022-01-09 00:53:30,821 iteration 337 : loss : 0.101316, loss_ce: 0.039010
2022-01-09 00:53:33,413 iteration 338 : loss : 0.118208, loss_ce: 0.050918
2022-01-09 00:53:36,295 iteration 339 : loss : 0.098774, loss_ce: 0.036115
2022-01-09 00:53:36,296 Training Data Eval:
2022-01-09 00:53:51,260   Average segmentation loss on training set: 0.1285
2022-01-09 00:53:51,260 Validation Data Eval:
2022-01-09 00:53:56,489   Average segmentation loss on validation set: 0.2197
2022-01-09 00:53:59,309 iteration 340 : loss : 0.108144, loss_ce: 0.038352
  5%|█▌                            | 20/400 [15:52<5:46:01, 54.63s/it]2022-01-09 00:54:02,189 iteration 341 : loss : 0.112220, loss_ce: 0.044148
2022-01-09 00:54:05,002 iteration 342 : loss : 0.088410, loss_ce: 0.034469
2022-01-09 00:54:07,898 iteration 343 : loss : 0.108589, loss_ce: 0.058639
2022-01-09 00:54:10,606 iteration 344 : loss : 0.126819, loss_ce: 0.039300
2022-01-09 00:54:13,466 iteration 345 : loss : 0.112634, loss_ce: 0.047234
2022-01-09 00:54:16,335 iteration 346 : loss : 0.090768, loss_ce: 0.036118
2022-01-09 00:54:19,030 iteration 347 : loss : 0.069564, loss_ce: 0.034027
2022-01-09 00:54:22,014 iteration 348 : loss : 0.113669, loss_ce: 0.048434
2022-01-09 00:54:24,639 iteration 349 : loss : 0.094118, loss_ce: 0.042475
2022-01-09 00:54:27,443 iteration 350 : loss : 0.090398, loss_ce: 0.034008
2022-01-09 00:54:30,350 iteration 351 : loss : 0.109848, loss_ce: 0.037366
2022-01-09 00:54:33,013 iteration 352 : loss : 0.103510, loss_ce: 0.037081
2022-01-09 00:54:35,740 iteration 353 : loss : 0.112878, loss_ce: 0.038080
2022-01-09 00:54:38,558 iteration 354 : loss : 0.114509, loss_ce: 0.047072
2022-01-09 00:54:41,429 iteration 355 : loss : 0.142297, loss_ce: 0.054262
2022-01-09 00:54:44,273 iteration 356 : loss : 0.140111, loss_ce: 0.053313
2022-01-09 00:54:46,926 iteration 357 : loss : 0.117445, loss_ce: 0.047103
  5%|█▌                            | 21/400 [16:40<5:31:50, 52.53s/it]2022-01-09 00:54:49,882 iteration 358 : loss : 0.129429, loss_ce: 0.053190
2022-01-09 00:54:52,745 iteration 359 : loss : 0.077313, loss_ce: 0.035729
2022-01-09 00:54:55,343 iteration 360 : loss : 0.193027, loss_ce: 0.056868
2022-01-09 00:54:58,276 iteration 361 : loss : 0.149315, loss_ce: 0.046120
2022-01-09 00:55:01,082 iteration 362 : loss : 0.118783, loss_ce: 0.041411
2022-01-09 00:55:03,939 iteration 363 : loss : 0.126553, loss_ce: 0.057218
2022-01-09 00:55:06,891 iteration 364 : loss : 0.151501, loss_ce: 0.070014
2022-01-09 00:55:09,735 iteration 365 : loss : 0.091846, loss_ce: 0.038048
2022-01-09 00:55:12,595 iteration 366 : loss : 0.092474, loss_ce: 0.036284
2022-01-09 00:55:15,368 iteration 367 : loss : 0.093398, loss_ce: 0.038566
2022-01-09 00:55:18,111 iteration 368 : loss : 0.122093, loss_ce: 0.045028
2022-01-09 00:55:20,937 iteration 369 : loss : 0.116122, loss_ce: 0.050224
2022-01-09 00:55:23,755 iteration 370 : loss : 0.112602, loss_ce: 0.033291
2022-01-09 00:55:26,507 iteration 371 : loss : 0.105963, loss_ce: 0.049604
2022-01-09 00:55:29,350 iteration 372 : loss : 0.106752, loss_ce: 0.032820
2022-01-09 00:55:32,186 iteration 373 : loss : 0.090688, loss_ce: 0.035110
2022-01-09 00:55:34,959 iteration 374 : loss : 0.089661, loss_ce: 0.038994
  6%|█▋                            | 22/400 [17:28<5:22:26, 51.18s/it]2022-01-09 00:55:37,872 iteration 375 : loss : 0.103093, loss_ce: 0.041653
2022-01-09 00:55:40,568 iteration 376 : loss : 0.113442, loss_ce: 0.041033
2022-01-09 00:55:43,419 iteration 377 : loss : 0.162902, loss_ce: 0.054216
2022-01-09 00:55:46,241 iteration 378 : loss : 0.121087, loss_ce: 0.053018
2022-01-09 00:55:48,854 iteration 379 : loss : 0.141915, loss_ce: 0.051234
2022-01-09 00:55:51,605 iteration 380 : loss : 0.094049, loss_ce: 0.043359
2022-01-09 00:55:54,389 iteration 381 : loss : 0.072708, loss_ce: 0.028088
2022-01-09 00:55:57,271 iteration 382 : loss : 0.089190, loss_ce: 0.035547
2022-01-09 00:56:00,122 iteration 383 : loss : 0.125893, loss_ce: 0.060795
2022-01-09 00:56:02,987 iteration 384 : loss : 0.079135, loss_ce: 0.030157
2022-01-09 00:56:05,673 iteration 385 : loss : 0.080106, loss_ce: 0.031891
2022-01-09 00:56:08,430 iteration 386 : loss : 0.119418, loss_ce: 0.043230
2022-01-09 00:56:11,405 iteration 387 : loss : 0.108774, loss_ce: 0.043465
2022-01-09 00:56:14,142 iteration 388 : loss : 0.083143, loss_ce: 0.034954
2022-01-09 00:56:16,926 iteration 389 : loss : 0.096444, loss_ce: 0.031877
2022-01-09 00:56:19,789 iteration 390 : loss : 0.110120, loss_ce: 0.060874
2022-01-09 00:56:22,392 iteration 391 : loss : 0.114379, loss_ce: 0.042112
  6%|█▋                            | 23/400 [18:15<5:14:31, 50.06s/it]2022-01-09 00:56:25,318 iteration 392 : loss : 0.083870, loss_ce: 0.035697
2022-01-09 00:56:28,134 iteration 393 : loss : 0.098740, loss_ce: 0.050769
2022-01-09 00:56:30,804 iteration 394 : loss : 0.110721, loss_ce: 0.044469
2022-01-09 00:56:33,584 iteration 395 : loss : 0.081354, loss_ce: 0.039490
2022-01-09 00:56:36,451 iteration 396 : loss : 0.108458, loss_ce: 0.049421
2022-01-09 00:56:39,324 iteration 397 : loss : 0.085039, loss_ce: 0.035443
2022-01-09 00:56:42,014 iteration 398 : loss : 0.120151, loss_ce: 0.053646
2022-01-09 00:56:44,804 iteration 399 : loss : 0.079708, loss_ce: 0.030098
2022-01-09 00:56:47,670 iteration 400 : loss : 0.094033, loss_ce: 0.042506
2022-01-09 00:56:50,440 iteration 401 : loss : 0.108805, loss_ce: 0.033180
2022-01-09 00:56:53,300 iteration 402 : loss : 0.104242, loss_ce: 0.038743
2022-01-09 00:56:56,184 iteration 403 : loss : 0.118519, loss_ce: 0.049910
2022-01-09 00:56:59,033 iteration 404 : loss : 0.079133, loss_ce: 0.024076
2022-01-09 00:57:01,862 iteration 405 : loss : 0.108390, loss_ce: 0.036372
2022-01-09 00:57:04,645 iteration 406 : loss : 0.096735, loss_ce: 0.042357
2022-01-09 00:57:07,268 iteration 407 : loss : 0.074238, loss_ce: 0.029346
2022-01-09 00:57:10,064 iteration 408 : loss : 0.070602, loss_ce: 0.024131
  6%|█▊                            | 24/400 [19:03<5:09:11, 49.34s/it]2022-01-09 00:57:12,905 iteration 409 : loss : 0.085690, loss_ce: 0.032428
2022-01-09 00:57:15,675 iteration 410 : loss : 0.075241, loss_ce: 0.029457
2022-01-09 00:57:18,574 iteration 411 : loss : 0.106253, loss_ce: 0.046305
2022-01-09 00:57:21,195 iteration 412 : loss : 0.087990, loss_ce: 0.035812
2022-01-09 00:57:24,007 iteration 413 : loss : 0.090604, loss_ce: 0.040149
2022-01-09 00:57:26,975 iteration 414 : loss : 0.084534, loss_ce: 0.036530
2022-01-09 00:57:29,850 iteration 415 : loss : 0.121153, loss_ce: 0.064425
2022-01-09 00:57:32,568 iteration 416 : loss : 0.120519, loss_ce: 0.047538
2022-01-09 00:57:35,365 iteration 417 : loss : 0.144329, loss_ce: 0.055292
2022-01-09 00:57:38,222 iteration 418 : loss : 0.097640, loss_ce: 0.031313
2022-01-09 00:57:40,833 iteration 419 : loss : 0.172358, loss_ce: 0.068387
2022-01-09 00:57:43,648 iteration 420 : loss : 0.079158, loss_ce: 0.032568
2022-01-09 00:57:46,279 iteration 421 : loss : 0.108550, loss_ce: 0.029410
2022-01-09 00:57:49,111 iteration 422 : loss : 0.104178, loss_ce: 0.035714
2022-01-09 00:57:51,712 iteration 423 : loss : 0.100142, loss_ce: 0.037185
2022-01-09 00:57:54,311 iteration 424 : loss : 0.108313, loss_ce: 0.049327
2022-01-09 00:57:54,311 Training Data Eval:
2022-01-09 00:58:09,721   Average segmentation loss on training set: 0.1019
2022-01-09 00:58:09,721 Validation Data Eval:
2022-01-09 00:58:14,987   Average segmentation loss on validation set: 0.1146
2022-01-09 00:58:20,687 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 00:58:22,725 iteration 425 : loss : 0.117426, loss_ce: 0.050517
  6%|█▉                            | 25/400 [20:15<5:52:06, 56.34s/it]2022-01-09 00:58:25,527 iteration 426 : loss : 0.105503, loss_ce: 0.043285
2022-01-09 00:58:28,226 iteration 427 : loss : 0.098776, loss_ce: 0.038761
2022-01-09 00:58:31,145 iteration 428 : loss : 0.096888, loss_ce: 0.040202
2022-01-09 00:58:33,804 iteration 429 : loss : 0.101373, loss_ce: 0.036386
2022-01-09 00:58:36,462 iteration 430 : loss : 0.090814, loss_ce: 0.043692
2022-01-09 00:58:39,296 iteration 431 : loss : 0.067611, loss_ce: 0.032781
2022-01-09 00:58:42,132 iteration 432 : loss : 0.086634, loss_ce: 0.033163
2022-01-09 00:58:44,813 iteration 433 : loss : 0.082030, loss_ce: 0.031624
2022-01-09 00:58:47,396 iteration 434 : loss : 0.083101, loss_ce: 0.025072
2022-01-09 00:58:50,209 iteration 435 : loss : 0.091754, loss_ce: 0.032895
2022-01-09 00:58:53,079 iteration 436 : loss : 0.142663, loss_ce: 0.035500
2022-01-09 00:58:55,715 iteration 437 : loss : 0.091353, loss_ce: 0.037678
2022-01-09 00:58:58,499 iteration 438 : loss : 0.111864, loss_ce: 0.045226
2022-01-09 00:59:01,120 iteration 439 : loss : 0.108606, loss_ce: 0.043761
2022-01-09 00:59:04,168 iteration 440 : loss : 0.129368, loss_ce: 0.047389
2022-01-09 00:59:06,845 iteration 441 : loss : 0.122800, loss_ce: 0.056481
2022-01-09 00:59:09,772 iteration 442 : loss : 0.075228, loss_ce: 0.027717
  6%|█▉                            | 26/400 [21:02<5:33:48, 53.55s/it]2022-01-09 00:59:12,670 iteration 443 : loss : 0.067796, loss_ce: 0.029547
2022-01-09 00:59:15,500 iteration 444 : loss : 0.082923, loss_ce: 0.029784
2022-01-09 00:59:18,099 iteration 445 : loss : 0.111684, loss_ce: 0.046402
2022-01-09 00:59:20,876 iteration 446 : loss : 0.083114, loss_ce: 0.029732
2022-01-09 00:59:23,676 iteration 447 : loss : 0.070559, loss_ce: 0.028803
2022-01-09 00:59:26,350 iteration 448 : loss : 0.119666, loss_ce: 0.040006
2022-01-09 00:59:29,055 iteration 449 : loss : 0.074034, loss_ce: 0.022670
2022-01-09 00:59:31,815 iteration 450 : loss : 0.096842, loss_ce: 0.047040
2022-01-09 00:59:34,459 iteration 451 : loss : 0.138191, loss_ce: 0.060990
2022-01-09 00:59:37,259 iteration 452 : loss : 0.091999, loss_ce: 0.028608
2022-01-09 00:59:39,888 iteration 453 : loss : 0.053154, loss_ce: 0.022439
2022-01-09 00:59:42,777 iteration 454 : loss : 0.104289, loss_ce: 0.047807
2022-01-09 00:59:45,621 iteration 455 : loss : 0.096341, loss_ce: 0.040316
2022-01-09 00:59:48,337 iteration 456 : loss : 0.059414, loss_ce: 0.018908
2022-01-09 00:59:51,218 iteration 457 : loss : 0.086954, loss_ce: 0.041813
2022-01-09 00:59:53,921 iteration 458 : loss : 0.082285, loss_ce: 0.042564
2022-01-09 00:59:56,759 iteration 459 : loss : 0.115580, loss_ce: 0.048309
  7%|██                            | 27/400 [21:49<5:20:39, 51.58s/it]2022-01-09 00:59:59,520 iteration 460 : loss : 0.081553, loss_ce: 0.035966
2022-01-09 01:00:02,152 iteration 461 : loss : 0.072971, loss_ce: 0.032216
2022-01-09 01:00:04,841 iteration 462 : loss : 0.126231, loss_ce: 0.048715
2022-01-09 01:00:07,597 iteration 463 : loss : 0.107236, loss_ce: 0.041600
2022-01-09 01:00:10,504 iteration 464 : loss : 0.122385, loss_ce: 0.040234
2022-01-09 01:00:13,344 iteration 465 : loss : 0.101922, loss_ce: 0.045406
2022-01-09 01:00:16,134 iteration 466 : loss : 0.079960, loss_ce: 0.032017
2022-01-09 01:00:18,757 iteration 467 : loss : 0.077059, loss_ce: 0.030276
2022-01-09 01:00:21,528 iteration 468 : loss : 0.066229, loss_ce: 0.024627
2022-01-09 01:00:24,357 iteration 469 : loss : 0.076406, loss_ce: 0.032945
2022-01-09 01:00:27,187 iteration 470 : loss : 0.064407, loss_ce: 0.025016
2022-01-09 01:00:29,846 iteration 471 : loss : 0.148001, loss_ce: 0.070466
2022-01-09 01:00:32,719 iteration 472 : loss : 0.056614, loss_ce: 0.025240
2022-01-09 01:00:35,578 iteration 473 : loss : 0.094409, loss_ce: 0.041033
2022-01-09 01:00:38,323 iteration 474 : loss : 0.126695, loss_ce: 0.048325
2022-01-09 01:00:41,131 iteration 475 : loss : 0.139803, loss_ce: 0.051301
2022-01-09 01:00:44,129 iteration 476 : loss : 0.072061, loss_ce: 0.031854
  7%|██                            | 28/400 [22:37<5:11:57, 50.32s/it]2022-01-09 01:00:46,967 iteration 477 : loss : 0.057468, loss_ce: 0.027965
2022-01-09 01:00:49,620 iteration 478 : loss : 0.068099, loss_ce: 0.029886
2022-01-09 01:00:52,464 iteration 479 : loss : 0.105867, loss_ce: 0.043445
2022-01-09 01:00:55,264 iteration 480 : loss : 0.091529, loss_ce: 0.028851
2022-01-09 01:00:57,933 iteration 481 : loss : 0.073233, loss_ce: 0.020548
2022-01-09 01:01:00,800 iteration 482 : loss : 0.091125, loss_ce: 0.038758
2022-01-09 01:01:03,461 iteration 483 : loss : 0.111932, loss_ce: 0.049586
2022-01-09 01:01:06,374 iteration 484 : loss : 0.080399, loss_ce: 0.028511
2022-01-09 01:01:09,012 iteration 485 : loss : 0.107234, loss_ce: 0.046522
2022-01-09 01:01:11,930 iteration 486 : loss : 0.076557, loss_ce: 0.035511
2022-01-09 01:01:14,730 iteration 487 : loss : 0.106357, loss_ce: 0.033289
2022-01-09 01:01:17,270 iteration 488 : loss : 0.090099, loss_ce: 0.037459
2022-01-09 01:01:20,073 iteration 489 : loss : 0.075841, loss_ce: 0.032990
2022-01-09 01:01:22,907 iteration 490 : loss : 0.083764, loss_ce: 0.037151
2022-01-09 01:01:25,915 iteration 491 : loss : 0.153182, loss_ce: 0.067899
2022-01-09 01:01:28,536 iteration 492 : loss : 0.081458, loss_ce: 0.044759
2022-01-09 01:01:31,200 iteration 493 : loss : 0.093112, loss_ce: 0.045067
  7%|██▏                           | 29/400 [23:24<5:05:07, 49.35s/it]2022-01-09 01:01:34,042 iteration 494 : loss : 0.136696, loss_ce: 0.050225
2022-01-09 01:01:36,822 iteration 495 : loss : 0.126016, loss_ce: 0.044355
2022-01-09 01:01:39,679 iteration 496 : loss : 0.079989, loss_ce: 0.030385
2022-01-09 01:01:42,556 iteration 497 : loss : 0.090533, loss_ce: 0.045550
2022-01-09 01:01:45,466 iteration 498 : loss : 0.098735, loss_ce: 0.045822
2022-01-09 01:01:48,354 iteration 499 : loss : 0.105338, loss_ce: 0.034618
2022-01-09 01:01:51,324 iteration 500 : loss : 0.097791, loss_ce: 0.035668
2022-01-09 01:01:53,966 iteration 501 : loss : 0.085539, loss_ce: 0.030798
2022-01-09 01:01:56,714 iteration 502 : loss : 0.082918, loss_ce: 0.036069
2022-01-09 01:01:59,596 iteration 503 : loss : 0.089815, loss_ce: 0.026634
2022-01-09 01:02:02,527 iteration 504 : loss : 0.070376, loss_ce: 0.026417
2022-01-09 01:02:05,296 iteration 505 : loss : 0.080213, loss_ce: 0.032418
2022-01-09 01:02:08,177 iteration 506 : loss : 0.088131, loss_ce: 0.035021
2022-01-09 01:02:10,973 iteration 507 : loss : 0.068988, loss_ce: 0.024180
2022-01-09 01:02:13,626 iteration 508 : loss : 0.081416, loss_ce: 0.030190
2022-01-09 01:02:16,442 iteration 509 : loss : 0.074880, loss_ce: 0.033052
2022-01-09 01:02:16,442 Training Data Eval:
2022-01-09 01:02:31,467   Average segmentation loss on training set: 0.0799
2022-01-09 01:02:31,468 Validation Data Eval:
2022-01-09 01:02:36,647   Average segmentation loss on validation set: 0.0968
2022-01-09 01:02:42,483 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 01:02:44,379 iteration 510 : loss : 0.085918, loss_ce: 0.035344
  8%|██▎                           | 30/400 [24:37<5:48:22, 56.49s/it]2022-01-09 01:02:46,991 iteration 511 : loss : 0.066626, loss_ce: 0.028376
2022-01-09 01:02:49,812 iteration 512 : loss : 0.121020, loss_ce: 0.054517
2022-01-09 01:02:52,572 iteration 513 : loss : 0.110609, loss_ce: 0.060514
2022-01-09 01:02:55,285 iteration 514 : loss : 0.117860, loss_ce: 0.047070
2022-01-09 01:02:57,921 iteration 515 : loss : 0.063638, loss_ce: 0.027599
2022-01-09 01:03:00,792 iteration 516 : loss : 0.064796, loss_ce: 0.033564
2022-01-09 01:03:03,721 iteration 517 : loss : 0.082549, loss_ce: 0.031569
2022-01-09 01:03:06,569 iteration 518 : loss : 0.113634, loss_ce: 0.056329
2022-01-09 01:03:09,333 iteration 519 : loss : 0.069018, loss_ce: 0.027041
2022-01-09 01:03:12,109 iteration 520 : loss : 0.087466, loss_ce: 0.044260
2022-01-09 01:03:14,997 iteration 521 : loss : 0.182131, loss_ce: 0.040808
2022-01-09 01:03:17,795 iteration 522 : loss : 0.055745, loss_ce: 0.019356
2022-01-09 01:03:20,656 iteration 523 : loss : 0.067160, loss_ce: 0.027083
2022-01-09 01:03:23,376 iteration 524 : loss : 0.083016, loss_ce: 0.037458
2022-01-09 01:03:26,198 iteration 525 : loss : 0.054766, loss_ce: 0.022851
2022-01-09 01:03:28,914 iteration 526 : loss : 0.065526, loss_ce: 0.023690
2022-01-09 01:03:31,716 iteration 527 : loss : 0.084655, loss_ce: 0.033032
  8%|██▎                           | 31/400 [25:24<5:30:33, 53.75s/it]2022-01-09 01:03:34,615 iteration 528 : loss : 0.054674, loss_ce: 0.020682
2022-01-09 01:03:37,292 iteration 529 : loss : 0.057962, loss_ce: 0.025355
2022-01-09 01:03:40,108 iteration 530 : loss : 0.051997, loss_ce: 0.021263
2022-01-09 01:03:42,970 iteration 531 : loss : 0.146935, loss_ce: 0.054572
2022-01-09 01:03:45,603 iteration 532 : loss : 0.094134, loss_ce: 0.028223
2022-01-09 01:03:48,439 iteration 533 : loss : 0.060438, loss_ce: 0.027596
2022-01-09 01:03:51,441 iteration 534 : loss : 0.079105, loss_ce: 0.036444
2022-01-09 01:03:54,136 iteration 535 : loss : 0.075124, loss_ce: 0.031649
2022-01-09 01:03:56,854 iteration 536 : loss : 0.076303, loss_ce: 0.037462
2022-01-09 01:03:59,699 iteration 537 : loss : 0.078743, loss_ce: 0.033053
2022-01-09 01:04:02,330 iteration 538 : loss : 0.049924, loss_ce: 0.018421
2022-01-09 01:04:05,101 iteration 539 : loss : 0.113988, loss_ce: 0.041012
2022-01-09 01:04:07,829 iteration 540 : loss : 0.074382, loss_ce: 0.034117
2022-01-09 01:04:10,641 iteration 541 : loss : 0.098005, loss_ce: 0.049155
2022-01-09 01:04:13,367 iteration 542 : loss : 0.082730, loss_ce: 0.031636
2022-01-09 01:04:16,218 iteration 543 : loss : 0.101946, loss_ce: 0.030801
2022-01-09 01:04:18,828 iteration 544 : loss : 0.090819, loss_ce: 0.029570
  8%|██▍                           | 32/400 [26:12<5:17:27, 51.76s/it]2022-01-09 01:04:21,691 iteration 545 : loss : 0.088071, loss_ce: 0.033212
2022-01-09 01:04:24,546 iteration 546 : loss : 0.069054, loss_ce: 0.028609
2022-01-09 01:04:27,245 iteration 547 : loss : 0.124270, loss_ce: 0.054353
2022-01-09 01:04:29,801 iteration 548 : loss : 0.103052, loss_ce: 0.038597
2022-01-09 01:04:32,611 iteration 549 : loss : 0.096653, loss_ce: 0.039510
2022-01-09 01:04:35,503 iteration 550 : loss : 0.106148, loss_ce: 0.047785
2022-01-09 01:04:38,341 iteration 551 : loss : 0.115663, loss_ce: 0.042292
2022-01-09 01:04:41,045 iteration 552 : loss : 0.123859, loss_ce: 0.057472
2022-01-09 01:04:43,943 iteration 553 : loss : 0.086067, loss_ce: 0.039869
2022-01-09 01:04:46,695 iteration 554 : loss : 0.072270, loss_ce: 0.029300
2022-01-09 01:04:49,534 iteration 555 : loss : 0.073816, loss_ce: 0.032683
2022-01-09 01:04:52,345 iteration 556 : loss : 0.080950, loss_ce: 0.031711
2022-01-09 01:04:55,138 iteration 557 : loss : 0.095906, loss_ce: 0.046408
2022-01-09 01:04:57,804 iteration 558 : loss : 0.103621, loss_ce: 0.039005
2022-01-09 01:05:00,677 iteration 559 : loss : 0.103197, loss_ce: 0.043796
2022-01-09 01:05:03,496 iteration 560 : loss : 0.094949, loss_ce: 0.037354
2022-01-09 01:05:06,082 iteration 561 : loss : 0.128544, loss_ce: 0.034496
  8%|██▍                           | 33/400 [26:59<5:08:19, 50.41s/it]2022-01-09 01:05:08,903 iteration 562 : loss : 0.085724, loss_ce: 0.032974
2022-01-09 01:05:11,728 iteration 563 : loss : 0.073350, loss_ce: 0.031520
2022-01-09 01:05:14,446 iteration 564 : loss : 0.070730, loss_ce: 0.036297
2022-01-09 01:05:17,253 iteration 565 : loss : 0.128827, loss_ce: 0.053103
2022-01-09 01:05:19,833 iteration 566 : loss : 0.072447, loss_ce: 0.027942
2022-01-09 01:05:22,691 iteration 567 : loss : 0.090140, loss_ce: 0.027493
2022-01-09 01:05:25,493 iteration 568 : loss : 0.090751, loss_ce: 0.032408
2022-01-09 01:05:28,308 iteration 569 : loss : 0.071603, loss_ce: 0.023948
2022-01-09 01:05:31,217 iteration 570 : loss : 0.087913, loss_ce: 0.036253
2022-01-09 01:05:34,086 iteration 571 : loss : 0.066763, loss_ce: 0.025302
2022-01-09 01:05:36,914 iteration 572 : loss : 0.077635, loss_ce: 0.029488
2022-01-09 01:05:39,739 iteration 573 : loss : 0.069796, loss_ce: 0.026712
2022-01-09 01:05:42,575 iteration 574 : loss : 0.074173, loss_ce: 0.031038
2022-01-09 01:05:45,408 iteration 575 : loss : 0.073908, loss_ce: 0.029358
2022-01-09 01:05:48,248 iteration 576 : loss : 0.084640, loss_ce: 0.043116
2022-01-09 01:05:50,898 iteration 577 : loss : 0.170109, loss_ce: 0.057424
2022-01-09 01:05:53,743 iteration 578 : loss : 0.105074, loss_ce: 0.037905
  8%|██▌                           | 34/400 [27:46<5:02:26, 49.58s/it]2022-01-09 01:05:56,568 iteration 579 : loss : 0.073627, loss_ce: 0.027676
2022-01-09 01:05:59,317 iteration 580 : loss : 0.083782, loss_ce: 0.036077
2022-01-09 01:06:02,117 iteration 581 : loss : 0.078601, loss_ce: 0.030626
2022-01-09 01:06:04,937 iteration 582 : loss : 0.100422, loss_ce: 0.037062
2022-01-09 01:06:07,558 iteration 583 : loss : 0.126673, loss_ce: 0.061922
2022-01-09 01:06:10,203 iteration 584 : loss : 0.066902, loss_ce: 0.026554
2022-01-09 01:06:13,030 iteration 585 : loss : 0.064598, loss_ce: 0.022944
2022-01-09 01:06:15,913 iteration 586 : loss : 0.065643, loss_ce: 0.029022
2022-01-09 01:06:18,600 iteration 587 : loss : 0.068959, loss_ce: 0.026558
2022-01-09 01:06:21,512 iteration 588 : loss : 0.094831, loss_ce: 0.034766
2022-01-09 01:06:24,357 iteration 589 : loss : 0.057479, loss_ce: 0.025232
2022-01-09 01:06:27,159 iteration 590 : loss : 0.110133, loss_ce: 0.063422
2022-01-09 01:06:29,810 iteration 591 : loss : 0.064363, loss_ce: 0.027193
2022-01-09 01:06:32,607 iteration 592 : loss : 0.084616, loss_ce: 0.031719
2022-01-09 01:06:35,446 iteration 593 : loss : 0.082087, loss_ce: 0.031549
2022-01-09 01:06:38,422 iteration 594 : loss : 0.085566, loss_ce: 0.041137
2022-01-09 01:06:38,422 Training Data Eval:
2022-01-09 01:06:53,478   Average segmentation loss on training set: 0.0574
2022-01-09 01:06:53,479 Validation Data Eval:
2022-01-09 01:06:58,883   Average segmentation loss on validation set: 0.0948
2022-01-09 01:07:04,729 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 01:07:06,600 iteration 595 : loss : 0.087676, loss_ce: 0.031901
  9%|██▋                           | 35/400 [28:59<5:44:06, 56.57s/it]2022-01-09 01:07:08,977 iteration 596 : loss : 0.048463, loss_ce: 0.020940
2022-01-09 01:07:11,873 iteration 597 : loss : 0.092198, loss_ce: 0.042369
2022-01-09 01:07:14,486 iteration 598 : loss : 0.076585, loss_ce: 0.030852
2022-01-09 01:07:17,184 iteration 599 : loss : 0.057824, loss_ce: 0.023327
2022-01-09 01:07:19,959 iteration 600 : loss : 0.052574, loss_ce: 0.027028
2022-01-09 01:07:22,634 iteration 601 : loss : 0.084700, loss_ce: 0.036227
2022-01-09 01:07:25,461 iteration 602 : loss : 0.091601, loss_ce: 0.047957
2022-01-09 01:07:28,254 iteration 603 : loss : 0.098343, loss_ce: 0.038281
2022-01-09 01:07:30,895 iteration 604 : loss : 0.077587, loss_ce: 0.031233
2022-01-09 01:07:33,721 iteration 605 : loss : 0.074298, loss_ce: 0.027008
2022-01-09 01:07:36,485 iteration 606 : loss : 0.089842, loss_ce: 0.029444
2022-01-09 01:07:39,315 iteration 607 : loss : 0.084528, loss_ce: 0.030874
2022-01-09 01:07:42,216 iteration 608 : loss : 0.053754, loss_ce: 0.018684
2022-01-09 01:07:44,837 iteration 609 : loss : 0.064477, loss_ce: 0.025493
2022-01-09 01:07:47,742 iteration 610 : loss : 0.064315, loss_ce: 0.024631
2022-01-09 01:07:50,614 iteration 611 : loss : 0.096876, loss_ce: 0.033133
2022-01-09 01:07:53,458 iteration 612 : loss : 0.071956, loss_ce: 0.031518
  9%|██▋                           | 36/400 [29:46<5:25:29, 53.65s/it]2022-01-09 01:07:56,309 iteration 613 : loss : 0.063350, loss_ce: 0.032720
2022-01-09 01:07:59,230 iteration 614 : loss : 0.075386, loss_ce: 0.031880
2022-01-09 01:08:02,071 iteration 615 : loss : 0.072456, loss_ce: 0.026459
2022-01-09 01:08:04,651 iteration 616 : loss : 0.078729, loss_ce: 0.048439
2022-01-09 01:08:07,460 iteration 617 : loss : 0.077973, loss_ce: 0.029314
2022-01-09 01:08:10,033 iteration 618 : loss : 0.096100, loss_ce: 0.036309
2022-01-09 01:08:12,648 iteration 619 : loss : 0.075793, loss_ce: 0.025683
2022-01-09 01:08:15,571 iteration 620 : loss : 0.117059, loss_ce: 0.044041
2022-01-09 01:08:18,379 iteration 621 : loss : 0.059946, loss_ce: 0.022099
2022-01-09 01:08:21,176 iteration 622 : loss : 0.048197, loss_ce: 0.022878
2022-01-09 01:08:23,995 iteration 623 : loss : 0.077532, loss_ce: 0.021827
2022-01-09 01:08:26,728 iteration 624 : loss : 0.076541, loss_ce: 0.027674
2022-01-09 01:08:29,654 iteration 625 : loss : 0.089153, loss_ce: 0.036605
2022-01-09 01:08:32,275 iteration 626 : loss : 0.060811, loss_ce: 0.021923
2022-01-09 01:08:34,943 iteration 627 : loss : 0.063414, loss_ce: 0.020519
2022-01-09 01:08:37,716 iteration 628 : loss : 0.109373, loss_ce: 0.063516
2022-01-09 01:08:40,560 iteration 629 : loss : 0.068875, loss_ce: 0.025383
  9%|██▊                           | 37/400 [30:33<5:12:42, 51.69s/it]2022-01-09 01:08:43,427 iteration 630 : loss : 0.058962, loss_ce: 0.024194
2022-01-09 01:08:46,308 iteration 631 : loss : 0.084054, loss_ce: 0.047665
2022-01-09 01:08:49,156 iteration 632 : loss : 0.080901, loss_ce: 0.031201
2022-01-09 01:08:52,025 iteration 633 : loss : 0.066669, loss_ce: 0.024331
2022-01-09 01:08:54,900 iteration 634 : loss : 0.059578, loss_ce: 0.027995
2022-01-09 01:08:57,753 iteration 635 : loss : 0.116124, loss_ce: 0.039034
2022-01-09 01:09:00,628 iteration 636 : loss : 0.065909, loss_ce: 0.030310
2022-01-09 01:09:03,227 iteration 637 : loss : 0.081700, loss_ce: 0.035334
2022-01-09 01:09:05,970 iteration 638 : loss : 0.119667, loss_ce: 0.052473
2022-01-09 01:09:08,858 iteration 639 : loss : 0.070392, loss_ce: 0.029803
2022-01-09 01:09:11,463 iteration 640 : loss : 0.063893, loss_ce: 0.026072
2022-01-09 01:09:14,211 iteration 641 : loss : 0.070541, loss_ce: 0.027215
2022-01-09 01:09:17,057 iteration 642 : loss : 0.073209, loss_ce: 0.026830
2022-01-09 01:09:19,883 iteration 643 : loss : 0.081736, loss_ce: 0.034952
2022-01-09 01:09:22,543 iteration 644 : loss : 0.055704, loss_ce: 0.020817
2022-01-09 01:09:25,197 iteration 645 : loss : 0.097794, loss_ce: 0.041425
2022-01-09 01:09:28,062 iteration 646 : loss : 0.073234, loss_ce: 0.031349
 10%|██▊                           | 38/400 [31:21<5:04:16, 50.43s/it]2022-01-09 01:09:30,892 iteration 647 : loss : 0.086419, loss_ce: 0.023932
2022-01-09 01:09:33,749 iteration 648 : loss : 0.091436, loss_ce: 0.046536
2022-01-09 01:09:36,606 iteration 649 : loss : 0.065754, loss_ce: 0.030111
2022-01-09 01:09:39,425 iteration 650 : loss : 0.103168, loss_ce: 0.031242
2022-01-09 01:09:42,428 iteration 651 : loss : 0.056975, loss_ce: 0.021267
2022-01-09 01:09:45,172 iteration 652 : loss : 0.069227, loss_ce: 0.029255
2022-01-09 01:09:47,863 iteration 653 : loss : 0.056897, loss_ce: 0.025248
2022-01-09 01:09:50,663 iteration 654 : loss : 0.091701, loss_ce: 0.034402
2022-01-09 01:09:53,322 iteration 655 : loss : 0.084614, loss_ce: 0.033471
2022-01-09 01:09:56,153 iteration 656 : loss : 0.095216, loss_ce: 0.034523
2022-01-09 01:09:58,992 iteration 657 : loss : 0.090205, loss_ce: 0.032437
2022-01-09 01:10:01,854 iteration 658 : loss : 0.066619, loss_ce: 0.025769
2022-01-09 01:10:04,676 iteration 659 : loss : 0.088139, loss_ce: 0.026989
2022-01-09 01:10:07,331 iteration 660 : loss : 0.068181, loss_ce: 0.027948
2022-01-09 01:10:10,155 iteration 661 : loss : 0.064342, loss_ce: 0.034657
2022-01-09 01:10:12,969 iteration 662 : loss : 0.078296, loss_ce: 0.029656
2022-01-09 01:10:15,808 iteration 663 : loss : 0.066309, loss_ce: 0.028441
 10%|██▉                           | 39/400 [32:08<4:58:35, 49.63s/it]2022-01-09 01:10:18,602 iteration 664 : loss : 0.099843, loss_ce: 0.051215
2022-01-09 01:10:21,221 iteration 665 : loss : 0.112029, loss_ce: 0.041065
2022-01-09 01:10:24,039 iteration 666 : loss : 0.079372, loss_ce: 0.027111
2022-01-09 01:10:26,895 iteration 667 : loss : 0.094595, loss_ce: 0.039655
2022-01-09 01:10:29,734 iteration 668 : loss : 0.042260, loss_ce: 0.014863
2022-01-09 01:10:32,595 iteration 669 : loss : 0.073080, loss_ce: 0.033928
2022-01-09 01:10:35,322 iteration 670 : loss : 0.093453, loss_ce: 0.037225
2022-01-09 01:10:38,154 iteration 671 : loss : 0.072565, loss_ce: 0.032369
2022-01-09 01:10:40,746 iteration 672 : loss : 0.072118, loss_ce: 0.032642
2022-01-09 01:10:43,567 iteration 673 : loss : 0.082250, loss_ce: 0.032967
2022-01-09 01:10:46,382 iteration 674 : loss : 0.068397, loss_ce: 0.033758
2022-01-09 01:10:49,185 iteration 675 : loss : 0.073078, loss_ce: 0.026634
2022-01-09 01:10:51,989 iteration 676 : loss : 0.067042, loss_ce: 0.027784
2022-01-09 01:10:55,065 iteration 677 : loss : 0.058476, loss_ce: 0.031186
2022-01-09 01:10:57,958 iteration 678 : loss : 0.087120, loss_ce: 0.022819
2022-01-09 01:11:00,591 iteration 679 : loss : 0.080639, loss_ce: 0.035453
2022-01-09 01:11:00,591 Training Data Eval:
2022-01-09 01:11:15,426   Average segmentation loss on training set: 0.0542
2022-01-09 01:11:15,426 Validation Data Eval:
2022-01-09 01:11:20,601   Average segmentation loss on validation set: 0.0918
2022-01-09 01:11:26,382 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 01:11:28,356 iteration 680 : loss : 0.061699, loss_ce: 0.027039
 10%|███                           | 40/400 [33:21<5:39:02, 56.51s/it]2022-01-09 01:11:31,168 iteration 681 : loss : 0.087769, loss_ce: 0.034443
2022-01-09 01:11:33,896 iteration 682 : loss : 0.065016, loss_ce: 0.023399
2022-01-09 01:11:36,704 iteration 683 : loss : 0.068133, loss_ce: 0.020715
2022-01-09 01:11:39,545 iteration 684 : loss : 0.075309, loss_ce: 0.031658
2022-01-09 01:11:42,306 iteration 685 : loss : 0.065961, loss_ce: 0.022888
2022-01-09 01:11:45,140 iteration 686 : loss : 0.065481, loss_ce: 0.029427
2022-01-09 01:11:47,968 iteration 687 : loss : 0.058269, loss_ce: 0.025514
2022-01-09 01:11:50,848 iteration 688 : loss : 0.077701, loss_ce: 0.030432
2022-01-09 01:11:53,693 iteration 689 : loss : 0.066819, loss_ce: 0.031314
2022-01-09 01:11:56,547 iteration 690 : loss : 0.074714, loss_ce: 0.029032
2022-01-09 01:11:59,379 iteration 691 : loss : 0.076500, loss_ce: 0.029302
2022-01-09 01:12:01,987 iteration 692 : loss : 0.054167, loss_ce: 0.023554
2022-01-09 01:12:04,751 iteration 693 : loss : 0.048076, loss_ce: 0.018887
2022-01-09 01:12:07,480 iteration 694 : loss : 0.103377, loss_ce: 0.035541
2022-01-09 01:12:10,318 iteration 695 : loss : 0.088634, loss_ce: 0.028054
2022-01-09 01:12:12,919 iteration 696 : loss : 0.158778, loss_ce: 0.041675
2022-01-09 01:12:15,587 iteration 697 : loss : 0.052069, loss_ce: 0.023130
 10%|███                           | 41/400 [34:08<5:21:25, 53.72s/it]2022-01-09 01:12:18,624 iteration 698 : loss : 0.053849, loss_ce: 0.023554
2022-01-09 01:12:21,498 iteration 699 : loss : 0.048678, loss_ce: 0.018271
2022-01-09 01:12:24,099 iteration 700 : loss : 0.078800, loss_ce: 0.030913
2022-01-09 01:12:26,972 iteration 701 : loss : 0.073951, loss_ce: 0.026574
2022-01-09 01:12:29,640 iteration 702 : loss : 0.062776, loss_ce: 0.028373
2022-01-09 01:12:32,453 iteration 703 : loss : 0.104430, loss_ce: 0.032343
2022-01-09 01:12:35,327 iteration 704 : loss : 0.074609, loss_ce: 0.025916
2022-01-09 01:12:38,199 iteration 705 : loss : 0.061795, loss_ce: 0.030081
2022-01-09 01:12:40,922 iteration 706 : loss : 0.107373, loss_ce: 0.031757
2022-01-09 01:12:43,693 iteration 707 : loss : 0.076523, loss_ce: 0.035388
2022-01-09 01:12:46,605 iteration 708 : loss : 0.058423, loss_ce: 0.020171
2022-01-09 01:12:49,339 iteration 709 : loss : 0.099922, loss_ce: 0.042321
2022-01-09 01:12:52,048 iteration 710 : loss : 0.060772, loss_ce: 0.025277
2022-01-09 01:12:54,833 iteration 711 : loss : 0.061225, loss_ce: 0.027215
2022-01-09 01:12:57,682 iteration 712 : loss : 0.056195, loss_ce: 0.018573
2022-01-09 01:13:00,395 iteration 713 : loss : 0.058631, loss_ce: 0.025898
2022-01-09 01:13:03,116 iteration 714 : loss : 0.059574, loss_ce: 0.022013
 10%|███▏                          | 42/400 [34:56<5:09:25, 51.86s/it]2022-01-09 01:13:06,022 iteration 715 : loss : 0.063018, loss_ce: 0.030938
2022-01-09 01:13:08,865 iteration 716 : loss : 0.057631, loss_ce: 0.023253
2022-01-09 01:13:11,643 iteration 717 : loss : 0.087806, loss_ce: 0.029530
2022-01-09 01:13:14,435 iteration 718 : loss : 0.101538, loss_ce: 0.045446
2022-01-09 01:13:17,302 iteration 719 : loss : 0.052591, loss_ce: 0.019811
2022-01-09 01:13:19,903 iteration 720 : loss : 0.090222, loss_ce: 0.026383
2022-01-09 01:13:22,789 iteration 721 : loss : 0.063630, loss_ce: 0.023751
2022-01-09 01:13:25,645 iteration 722 : loss : 0.068872, loss_ce: 0.027934
2022-01-09 01:13:28,391 iteration 723 : loss : 0.056633, loss_ce: 0.019800
2022-01-09 01:13:31,473 iteration 724 : loss : 0.080211, loss_ce: 0.034571
2022-01-09 01:13:34,263 iteration 725 : loss : 0.060181, loss_ce: 0.021335
2022-01-09 01:13:37,113 iteration 726 : loss : 0.087766, loss_ce: 0.027177
2022-01-09 01:13:39,914 iteration 727 : loss : 0.079810, loss_ce: 0.029252
2022-01-09 01:13:42,761 iteration 728 : loss : 0.074747, loss_ce: 0.034729
2022-01-09 01:13:45,459 iteration 729 : loss : 0.042474, loss_ce: 0.017443
2022-01-09 01:13:48,198 iteration 730 : loss : 0.052856, loss_ce: 0.024426
2022-01-09 01:13:51,038 iteration 731 : loss : 0.079126, loss_ce: 0.030361
 11%|███▏                          | 43/400 [35:44<5:01:31, 50.68s/it]2022-01-09 01:13:53,901 iteration 732 : loss : 0.067044, loss_ce: 0.029627
2022-01-09 01:13:56,614 iteration 733 : loss : 0.109264, loss_ce: 0.045103
2022-01-09 01:13:59,382 iteration 734 : loss : 0.062563, loss_ce: 0.024308
2022-01-09 01:14:02,161 iteration 735 : loss : 0.058851, loss_ce: 0.021994
2022-01-09 01:14:04,911 iteration 736 : loss : 0.075698, loss_ce: 0.028983
2022-01-09 01:14:07,838 iteration 737 : loss : 0.065340, loss_ce: 0.022808
2022-01-09 01:14:10,486 iteration 738 : loss : 0.044774, loss_ce: 0.020650
2022-01-09 01:14:13,252 iteration 739 : loss : 0.078474, loss_ce: 0.033774
2022-01-09 01:14:16,041 iteration 740 : loss : 0.077691, loss_ce: 0.037635
2022-01-09 01:14:18,684 iteration 741 : loss : 0.065908, loss_ce: 0.026521
2022-01-09 01:14:21,576 iteration 742 : loss : 0.067717, loss_ce: 0.023576
2022-01-09 01:14:24,215 iteration 743 : loss : 0.070778, loss_ce: 0.026603
2022-01-09 01:14:27,033 iteration 744 : loss : 0.067277, loss_ce: 0.026198
2022-01-09 01:14:29,657 iteration 745 : loss : 0.052973, loss_ce: 0.023113
2022-01-09 01:14:32,443 iteration 746 : loss : 0.113814, loss_ce: 0.051710
2022-01-09 01:14:35,276 iteration 747 : loss : 0.077266, loss_ce: 0.022494
2022-01-09 01:14:37,842 iteration 748 : loss : 0.050785, loss_ce: 0.022638
 11%|███▎                          | 44/400 [36:31<4:53:48, 49.52s/it]2022-01-09 01:14:40,687 iteration 749 : loss : 0.084282, loss_ce: 0.039546
2022-01-09 01:14:43,527 iteration 750 : loss : 0.068766, loss_ce: 0.031005
2022-01-09 01:14:46,176 iteration 751 : loss : 0.052198, loss_ce: 0.023126
2022-01-09 01:14:49,030 iteration 752 : loss : 0.062314, loss_ce: 0.025649
2022-01-09 01:14:51,907 iteration 753 : loss : 0.069829, loss_ce: 0.031173
2022-01-09 01:14:54,733 iteration 754 : loss : 0.087171, loss_ce: 0.034438
2022-01-09 01:14:57,596 iteration 755 : loss : 0.052616, loss_ce: 0.018914
2022-01-09 01:15:00,427 iteration 756 : loss : 0.062738, loss_ce: 0.030777
2022-01-09 01:15:03,281 iteration 757 : loss : 0.061960, loss_ce: 0.022447
2022-01-09 01:15:06,184 iteration 758 : loss : 0.074106, loss_ce: 0.030454
2022-01-09 01:15:09,006 iteration 759 : loss : 0.052595, loss_ce: 0.020811
2022-01-09 01:15:11,743 iteration 760 : loss : 0.122470, loss_ce: 0.043421
2022-01-09 01:15:14,616 iteration 761 : loss : 0.075537, loss_ce: 0.025238
2022-01-09 01:15:17,469 iteration 762 : loss : 0.077785, loss_ce: 0.034053
2022-01-09 01:15:20,078 iteration 763 : loss : 0.050390, loss_ce: 0.019829
2022-01-09 01:15:22,853 iteration 764 : loss : 0.073383, loss_ce: 0.034558
2022-01-09 01:15:22,853 Training Data Eval:
2022-01-09 01:15:37,826   Average segmentation loss on training set: 0.1007
2022-01-09 01:15:37,826 Validation Data Eval:
2022-01-09 01:15:43,152   Average segmentation loss on validation set: 0.1297
2022-01-09 01:15:46,008 iteration 765 : loss : 0.074587, loss_ce: 0.037258
 11%|███▍                          | 45/400 [37:39<5:26:05, 55.11s/it]2022-01-09 01:15:48,841 iteration 766 : loss : 0.111524, loss_ce: 0.043634
2022-01-09 01:15:51,663 iteration 767 : loss : 0.105728, loss_ce: 0.038777
2022-01-09 01:15:54,461 iteration 768 : loss : 0.093822, loss_ce: 0.035950
2022-01-09 01:15:57,103 iteration 769 : loss : 0.176729, loss_ce: 0.035025
2022-01-09 01:15:59,886 iteration 770 : loss : 0.057277, loss_ce: 0.021267
2022-01-09 01:16:02,756 iteration 771 : loss : 0.068930, loss_ce: 0.025166
2022-01-09 01:16:05,586 iteration 772 : loss : 0.059285, loss_ce: 0.025335
2022-01-09 01:16:08,454 iteration 773 : loss : 0.072977, loss_ce: 0.027177
2022-01-09 01:16:11,165 iteration 774 : loss : 0.071934, loss_ce: 0.033301
2022-01-09 01:16:13,780 iteration 775 : loss : 0.067023, loss_ce: 0.024820
2022-01-09 01:16:16,572 iteration 776 : loss : 0.074515, loss_ce: 0.030717
2022-01-09 01:16:19,462 iteration 777 : loss : 0.068076, loss_ce: 0.025404
2022-01-09 01:16:22,131 iteration 778 : loss : 0.052914, loss_ce: 0.021123
2022-01-09 01:16:24,956 iteration 779 : loss : 0.063543, loss_ce: 0.030447
2022-01-09 01:16:27,781 iteration 780 : loss : 0.067182, loss_ce: 0.025821
2022-01-09 01:16:30,724 iteration 781 : loss : 0.065589, loss_ce: 0.028111
2022-01-09 01:16:33,535 iteration 782 : loss : 0.081080, loss_ce: 0.034163
 12%|███▍                          | 46/400 [38:26<5:11:42, 52.83s/it]2022-01-09 01:16:36,429 iteration 783 : loss : 0.053298, loss_ce: 0.023951
2022-01-09 01:16:39,239 iteration 784 : loss : 0.078511, loss_ce: 0.029971
2022-01-09 01:16:42,081 iteration 785 : loss : 0.046037, loss_ce: 0.019634
2022-01-09 01:16:44,705 iteration 786 : loss : 0.066753, loss_ce: 0.021358
2022-01-09 01:16:47,344 iteration 787 : loss : 0.071877, loss_ce: 0.032964
2022-01-09 01:16:50,234 iteration 788 : loss : 0.056854, loss_ce: 0.017225
2022-01-09 01:16:53,151 iteration 789 : loss : 0.109038, loss_ce: 0.064077
2022-01-09 01:16:55,992 iteration 790 : loss : 0.048350, loss_ce: 0.015533
2022-01-09 01:16:58,692 iteration 791 : loss : 0.072327, loss_ce: 0.025199
2022-01-09 01:17:01,547 iteration 792 : loss : 0.051052, loss_ce: 0.024064
2022-01-09 01:17:04,205 iteration 793 : loss : 0.083106, loss_ce: 0.034022
2022-01-09 01:17:07,034 iteration 794 : loss : 0.044278, loss_ce: 0.017529
2022-01-09 01:17:09,668 iteration 795 : loss : 0.050357, loss_ce: 0.021092
2022-01-09 01:17:12,516 iteration 796 : loss : 0.069984, loss_ce: 0.026812
2022-01-09 01:17:15,132 iteration 797 : loss : 0.055368, loss_ce: 0.024910
2022-01-09 01:17:17,902 iteration 798 : loss : 0.135227, loss_ce: 0.039793
2022-01-09 01:17:20,757 iteration 799 : loss : 0.051650, loss_ce: 0.019728
 12%|███▌                          | 47/400 [39:13<5:00:56, 51.15s/it]2022-01-09 01:17:23,640 iteration 800 : loss : 0.068819, loss_ce: 0.019741
2022-01-09 01:17:26,378 iteration 801 : loss : 0.064864, loss_ce: 0.021288
2022-01-09 01:17:29,149 iteration 802 : loss : 0.062933, loss_ce: 0.026388
2022-01-09 01:17:32,063 iteration 803 : loss : 0.077686, loss_ce: 0.044147
2022-01-09 01:17:34,846 iteration 804 : loss : 0.079173, loss_ce: 0.037363
2022-01-09 01:17:37,577 iteration 805 : loss : 0.105596, loss_ce: 0.027027
2022-01-09 01:17:40,348 iteration 806 : loss : 0.073236, loss_ce: 0.026928
2022-01-09 01:17:43,216 iteration 807 : loss : 0.061663, loss_ce: 0.027255
2022-01-09 01:17:46,055 iteration 808 : loss : 0.055216, loss_ce: 0.020259
2022-01-09 01:17:48,811 iteration 809 : loss : 0.100181, loss_ce: 0.032651
2022-01-09 01:17:51,477 iteration 810 : loss : 0.074053, loss_ce: 0.026095
2022-01-09 01:17:54,227 iteration 811 : loss : 0.053391, loss_ce: 0.020863
2022-01-09 01:17:57,113 iteration 812 : loss : 0.108061, loss_ce: 0.043950
2022-01-09 01:17:59,919 iteration 813 : loss : 0.070117, loss_ce: 0.028430
2022-01-09 01:18:02,742 iteration 814 : loss : 0.064917, loss_ce: 0.029352
2022-01-09 01:18:05,609 iteration 815 : loss : 0.055108, loss_ce: 0.019772
2022-01-09 01:18:08,161 iteration 816 : loss : 0.060529, loss_ce: 0.021749
 12%|███▌                          | 48/400 [40:01<4:53:31, 50.03s/it]2022-01-09 01:18:10,977 iteration 817 : loss : 0.044786, loss_ce: 0.018360
2022-01-09 01:18:13,801 iteration 818 : loss : 0.075157, loss_ce: 0.027254
2022-01-09 01:18:16,418 iteration 819 : loss : 0.052521, loss_ce: 0.020871
2022-01-09 01:18:19,136 iteration 820 : loss : 0.063214, loss_ce: 0.024804
2022-01-09 01:18:21,881 iteration 821 : loss : 0.077840, loss_ce: 0.028083
2022-01-09 01:18:24,681 iteration 822 : loss : 0.051291, loss_ce: 0.019637
2022-01-09 01:18:27,588 iteration 823 : loss : 0.076350, loss_ce: 0.027704
2022-01-09 01:18:30,393 iteration 824 : loss : 0.073727, loss_ce: 0.033884
2022-01-09 01:18:33,172 iteration 825 : loss : 0.052243, loss_ce: 0.016114
2022-01-09 01:18:35,989 iteration 826 : loss : 0.052260, loss_ce: 0.022864
2022-01-09 01:18:38,806 iteration 827 : loss : 0.094703, loss_ce: 0.025229
2022-01-09 01:18:41,526 iteration 828 : loss : 0.089250, loss_ce: 0.044045
2022-01-09 01:18:44,323 iteration 829 : loss : 0.109394, loss_ce: 0.042376
2022-01-09 01:18:47,207 iteration 830 : loss : 0.088232, loss_ce: 0.030309
2022-01-09 01:18:50,050 iteration 831 : loss : 0.062526, loss_ce: 0.024691
2022-01-09 01:18:52,655 iteration 832 : loss : 0.045786, loss_ce: 0.017612
2022-01-09 01:18:55,445 iteration 833 : loss : 0.055837, loss_ce: 0.027269
 12%|███▋                          | 49/400 [40:48<4:47:51, 49.21s/it]2022-01-09 01:18:58,410 iteration 834 : loss : 0.082878, loss_ce: 0.030008
2022-01-09 01:19:01,148 iteration 835 : loss : 0.060977, loss_ce: 0.023640
2022-01-09 01:19:04,010 iteration 836 : loss : 0.048166, loss_ce: 0.021085
2022-01-09 01:19:06,765 iteration 837 : loss : 0.049432, loss_ce: 0.021594
2022-01-09 01:19:09,556 iteration 838 : loss : 0.076002, loss_ce: 0.028637
2022-01-09 01:19:12,428 iteration 839 : loss : 0.070800, loss_ce: 0.026357
2022-01-09 01:19:15,284 iteration 840 : loss : 0.079962, loss_ce: 0.028841
2022-01-09 01:19:18,127 iteration 841 : loss : 0.063048, loss_ce: 0.033745
2022-01-09 01:19:21,037 iteration 842 : loss : 0.082386, loss_ce: 0.031161
2022-01-09 01:19:23,865 iteration 843 : loss : 0.041951, loss_ce: 0.015240
2022-01-09 01:19:26,680 iteration 844 : loss : 0.119822, loss_ce: 0.057112
2022-01-09 01:19:29,478 iteration 845 : loss : 0.045670, loss_ce: 0.018535
2022-01-09 01:19:32,271 iteration 846 : loss : 0.072931, loss_ce: 0.026634
2022-01-09 01:19:35,074 iteration 847 : loss : 0.106257, loss_ce: 0.043896
2022-01-09 01:19:37,782 iteration 848 : loss : 0.060537, loss_ce: 0.019981
2022-01-09 01:19:40,649 iteration 849 : loss : 0.075538, loss_ce: 0.030845
2022-01-09 01:19:40,650 Training Data Eval:
2022-01-09 01:19:55,507   Average segmentation loss on training set: 0.0512
2022-01-09 01:19:55,508 Validation Data Eval:
2022-01-09 01:20:00,712   Average segmentation loss on validation set: 0.0767
2022-01-09 01:20:06,470 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 01:20:08,465 iteration 850 : loss : 0.057928, loss_ce: 0.019703
 12%|███▊                          | 50/400 [42:01<5:28:42, 56.35s/it]2022-01-09 01:20:11,033 iteration 851 : loss : 0.059578, loss_ce: 0.029375
2022-01-09 01:20:13,857 iteration 852 : loss : 0.068815, loss_ce: 0.023618
2022-01-09 01:20:16,704 iteration 853 : loss : 0.083864, loss_ce: 0.031019
2022-01-09 01:20:19,497 iteration 854 : loss : 0.053090, loss_ce: 0.020187
2022-01-09 01:20:22,075 iteration 855 : loss : 0.042813, loss_ce: 0.016804
2022-01-09 01:20:24,911 iteration 856 : loss : 0.064920, loss_ce: 0.022735
2022-01-09 01:20:27,658 iteration 857 : loss : 0.055258, loss_ce: 0.023241
2022-01-09 01:20:30,450 iteration 858 : loss : 0.062829, loss_ce: 0.023160
2022-01-09 01:20:33,343 iteration 859 : loss : 0.058005, loss_ce: 0.021914
2022-01-09 01:20:36,052 iteration 860 : loss : 0.057413, loss_ce: 0.021763
2022-01-09 01:20:38,841 iteration 861 : loss : 0.067616, loss_ce: 0.025122
2022-01-09 01:20:41,600 iteration 862 : loss : 0.061623, loss_ce: 0.017632
2022-01-09 01:20:44,411 iteration 863 : loss : 0.065606, loss_ce: 0.032966
2022-01-09 01:20:47,104 iteration 864 : loss : 0.042843, loss_ce: 0.017931
2022-01-09 01:20:49,887 iteration 865 : loss : 0.047110, loss_ce: 0.022431
2022-01-09 01:20:52,678 iteration 866 : loss : 0.048002, loss_ce: 0.019311
2022-01-09 01:20:55,308 iteration 867 : loss : 0.048517, loss_ce: 0.020258
 13%|███▊                          | 51/400 [42:48<5:11:11, 53.50s/it]2022-01-09 01:20:58,261 iteration 868 : loss : 0.040559, loss_ce: 0.013292
2022-01-09 01:21:01,076 iteration 869 : loss : 0.069339, loss_ce: 0.031551
2022-01-09 01:21:03,870 iteration 870 : loss : 0.088994, loss_ce: 0.020902
2022-01-09 01:21:06,569 iteration 871 : loss : 0.056198, loss_ce: 0.021117
2022-01-09 01:21:09,402 iteration 872 : loss : 0.041682, loss_ce: 0.017553
2022-01-09 01:21:12,189 iteration 873 : loss : 0.046602, loss_ce: 0.016550
2022-01-09 01:21:14,839 iteration 874 : loss : 0.056817, loss_ce: 0.027043
2022-01-09 01:21:17,710 iteration 875 : loss : 0.068781, loss_ce: 0.031802
2022-01-09 01:21:20,481 iteration 876 : loss : 0.059731, loss_ce: 0.028300
2022-01-09 01:21:23,235 iteration 877 : loss : 0.039378, loss_ce: 0.013625
2022-01-09 01:21:26,071 iteration 878 : loss : 0.042646, loss_ce: 0.014360
2022-01-09 01:21:28,859 iteration 879 : loss : 0.063381, loss_ce: 0.023532
2022-01-09 01:21:31,718 iteration 880 : loss : 0.043838, loss_ce: 0.018030
2022-01-09 01:21:34,552 iteration 881 : loss : 0.058009, loss_ce: 0.024790
2022-01-09 01:21:37,377 iteration 882 : loss : 0.056106, loss_ce: 0.026832
2022-01-09 01:21:40,199 iteration 883 : loss : 0.069405, loss_ce: 0.021569
2022-01-09 01:21:43,052 iteration 884 : loss : 0.060195, loss_ce: 0.021741
 13%|███▉                          | 52/400 [43:36<5:00:16, 51.77s/it]2022-01-09 01:21:45,896 iteration 885 : loss : 0.074100, loss_ce: 0.032291
2022-01-09 01:21:48,816 iteration 886 : loss : 0.056207, loss_ce: 0.019922
2022-01-09 01:21:51,534 iteration 887 : loss : 0.067956, loss_ce: 0.028563
2022-01-09 01:21:54,391 iteration 888 : loss : 0.049199, loss_ce: 0.019206
2022-01-09 01:21:57,065 iteration 889 : loss : 0.062536, loss_ce: 0.017200
2022-01-09 01:21:59,880 iteration 890 : loss : 0.049689, loss_ce: 0.020663
2022-01-09 01:22:02,556 iteration 891 : loss : 0.072949, loss_ce: 0.035622
2022-01-09 01:22:05,435 iteration 892 : loss : 0.081793, loss_ce: 0.022077
2022-01-09 01:22:08,326 iteration 893 : loss : 0.052260, loss_ce: 0.021471
2022-01-09 01:22:11,170 iteration 894 : loss : 0.068295, loss_ce: 0.035814
2022-01-09 01:22:13,982 iteration 895 : loss : 0.061420, loss_ce: 0.020097
2022-01-09 01:22:16,636 iteration 896 : loss : 0.066015, loss_ce: 0.018921
2022-01-09 01:22:19,555 iteration 897 : loss : 0.044553, loss_ce: 0.016550
2022-01-09 01:22:22,461 iteration 898 : loss : 0.053094, loss_ce: 0.019176
2022-01-09 01:22:25,270 iteration 899 : loss : 0.049549, loss_ce: 0.016725
2022-01-09 01:22:28,210 iteration 900 : loss : 0.081190, loss_ce: 0.028369
2022-01-09 01:22:30,907 iteration 901 : loss : 0.060674, loss_ce: 0.034666
 13%|███▉                          | 53/400 [44:24<4:52:37, 50.60s/it]2022-01-09 01:22:33,787 iteration 902 : loss : 0.050562, loss_ce: 0.021339
2022-01-09 01:22:36,606 iteration 903 : loss : 0.050871, loss_ce: 0.016333
2022-01-09 01:22:39,221 iteration 904 : loss : 0.062090, loss_ce: 0.028525
2022-01-09 01:22:42,068 iteration 905 : loss : 0.115532, loss_ce: 0.035581
2022-01-09 01:22:44,886 iteration 906 : loss : 0.046571, loss_ce: 0.016589
2022-01-09 01:22:47,570 iteration 907 : loss : 0.040435, loss_ce: 0.019372
2022-01-09 01:22:50,326 iteration 908 : loss : 0.059259, loss_ce: 0.031210
2022-01-09 01:22:53,157 iteration 909 : loss : 0.068212, loss_ce: 0.032831
2022-01-09 01:22:55,787 iteration 910 : loss : 0.078130, loss_ce: 0.026403
2022-01-09 01:22:58,647 iteration 911 : loss : 0.070115, loss_ce: 0.025231
2022-01-09 01:23:01,238 iteration 912 : loss : 0.065669, loss_ce: 0.023773
2022-01-09 01:23:04,072 iteration 913 : loss : 0.064544, loss_ce: 0.027279
2022-01-09 01:23:06,890 iteration 914 : loss : 0.052411, loss_ce: 0.024064
2022-01-09 01:23:09,743 iteration 915 : loss : 0.064968, loss_ce: 0.026200
2022-01-09 01:23:12,565 iteration 916 : loss : 0.069507, loss_ce: 0.023718
2022-01-09 01:23:15,365 iteration 917 : loss : 0.062100, loss_ce: 0.019863
2022-01-09 01:23:18,159 iteration 918 : loss : 0.054465, loss_ce: 0.018039
 14%|████                          | 54/400 [45:11<4:45:58, 49.59s/it]2022-01-09 01:23:21,083 iteration 919 : loss : 0.056872, loss_ce: 0.025721
2022-01-09 01:23:23,815 iteration 920 : loss : 0.100996, loss_ce: 0.059707
2022-01-09 01:23:26,743 iteration 921 : loss : 0.071489, loss_ce: 0.030639
2022-01-09 01:23:29,319 iteration 922 : loss : 0.048894, loss_ce: 0.021226
2022-01-09 01:23:32,106 iteration 923 : loss : 0.062587, loss_ce: 0.024888
2022-01-09 01:23:34,884 iteration 924 : loss : 0.058399, loss_ce: 0.018632
2022-01-09 01:23:37,491 iteration 925 : loss : 0.052146, loss_ce: 0.020945
2022-01-09 01:23:40,158 iteration 926 : loss : 0.066239, loss_ce: 0.025457
2022-01-09 01:23:42,963 iteration 927 : loss : 0.061404, loss_ce: 0.025493
2022-01-09 01:23:45,798 iteration 928 : loss : 0.053004, loss_ce: 0.019799
2022-01-09 01:23:48,789 iteration 929 : loss : 0.069756, loss_ce: 0.031706
2022-01-09 01:23:51,452 iteration 930 : loss : 0.062429, loss_ce: 0.038630
2022-01-09 01:23:54,268 iteration 931 : loss : 0.067016, loss_ce: 0.024186
2022-01-09 01:23:57,077 iteration 932 : loss : 0.078760, loss_ce: 0.022609
2022-01-09 01:24:00,073 iteration 933 : loss : 0.097870, loss_ce: 0.031544
2022-01-09 01:24:02,715 iteration 934 : loss : 0.057596, loss_ce: 0.021406
2022-01-09 01:24:02,715 Training Data Eval:
2022-01-09 01:24:17,895   Average segmentation loss on training set: 0.0439
2022-01-09 01:24:17,895 Validation Data Eval:
2022-01-09 01:24:23,226   Average segmentation loss on validation set: 0.0775
2022-01-09 01:24:25,875 iteration 935 : loss : 0.037610, loss_ce: 0.013665
 14%|████▏                         | 55/400 [46:19<5:16:24, 55.03s/it]2022-01-09 01:24:28,728 iteration 936 : loss : 0.067534, loss_ce: 0.017615
2022-01-09 01:24:31,489 iteration 937 : loss : 0.061887, loss_ce: 0.025211
2022-01-09 01:24:34,354 iteration 938 : loss : 0.053943, loss_ce: 0.027569
2022-01-09 01:24:37,188 iteration 939 : loss : 0.062791, loss_ce: 0.025331
2022-01-09 01:24:39,797 iteration 940 : loss : 0.071905, loss_ce: 0.033945
2022-01-09 01:24:42,629 iteration 941 : loss : 0.070409, loss_ce: 0.022107
2022-01-09 01:24:45,454 iteration 942 : loss : 0.054665, loss_ce: 0.016855
2022-01-09 01:24:48,101 iteration 943 : loss : 0.093025, loss_ce: 0.031790
2022-01-09 01:24:50,782 iteration 944 : loss : 0.047432, loss_ce: 0.017582
2022-01-09 01:24:53,475 iteration 945 : loss : 0.089207, loss_ce: 0.054526
2022-01-09 01:24:56,284 iteration 946 : loss : 0.047058, loss_ce: 0.015612
2022-01-09 01:24:59,190 iteration 947 : loss : 0.051254, loss_ce: 0.020813
2022-01-09 01:25:01,991 iteration 948 : loss : 0.062757, loss_ce: 0.021975
2022-01-09 01:25:04,702 iteration 949 : loss : 0.055207, loss_ce: 0.020259
2022-01-09 01:25:07,525 iteration 950 : loss : 0.111351, loss_ce: 0.032078
2022-01-09 01:25:10,398 iteration 951 : loss : 0.059580, loss_ce: 0.023691
2022-01-09 01:25:12,978 iteration 952 : loss : 0.059317, loss_ce: 0.015811
 14%|████▏                         | 56/400 [47:06<5:01:54, 52.66s/it]2022-01-09 01:25:15,896 iteration 953 : loss : 0.054724, loss_ce: 0.018902
2022-01-09 01:25:18,804 iteration 954 : loss : 0.063559, loss_ce: 0.023121
2022-01-09 01:25:21,423 iteration 955 : loss : 0.043994, loss_ce: 0.016278
2022-01-09 01:25:24,288 iteration 956 : loss : 0.049956, loss_ce: 0.020510
2022-01-09 01:25:26,981 iteration 957 : loss : 0.045965, loss_ce: 0.016510
2022-01-09 01:25:29,962 iteration 958 : loss : 0.044751, loss_ce: 0.015848
2022-01-09 01:25:32,726 iteration 959 : loss : 0.056822, loss_ce: 0.026105
2022-01-09 01:25:35,499 iteration 960 : loss : 0.058287, loss_ce: 0.019159
2022-01-09 01:25:38,496 iteration 961 : loss : 0.065233, loss_ce: 0.024594
2022-01-09 01:25:41,360 iteration 962 : loss : 0.046470, loss_ce: 0.016572
2022-01-09 01:25:44,066 iteration 963 : loss : 0.058772, loss_ce: 0.024196
2022-01-09 01:25:46,866 iteration 964 : loss : 0.060156, loss_ce: 0.023860
2022-01-09 01:25:49,675 iteration 965 : loss : 0.037355, loss_ce: 0.016120
2022-01-09 01:25:52,424 iteration 966 : loss : 0.063163, loss_ce: 0.030224
2022-01-09 01:25:55,253 iteration 967 : loss : 0.112985, loss_ce: 0.054609
2022-01-09 01:25:58,235 iteration 968 : loss : 0.064857, loss_ce: 0.027561
2022-01-09 01:26:01,104 iteration 969 : loss : 0.050885, loss_ce: 0.019567
 14%|████▎                         | 57/400 [47:54<4:53:13, 51.29s/it]2022-01-09 01:26:03,776 iteration 970 : loss : 0.049252, loss_ce: 0.019472
2022-01-09 01:26:06,692 iteration 971 : loss : 0.068603, loss_ce: 0.024873
2022-01-09 01:26:09,306 iteration 972 : loss : 0.047600, loss_ce: 0.017354
2022-01-09 01:26:12,072 iteration 973 : loss : 0.051861, loss_ce: 0.019352
2022-01-09 01:26:14,886 iteration 974 : loss : 0.064999, loss_ce: 0.027063
2022-01-09 01:26:17,497 iteration 975 : loss : 0.062616, loss_ce: 0.018306
2022-01-09 01:26:20,291 iteration 976 : loss : 0.060095, loss_ce: 0.021107
2022-01-09 01:26:23,140 iteration 977 : loss : 0.062757, loss_ce: 0.020176
2022-01-09 01:26:25,972 iteration 978 : loss : 0.074233, loss_ce: 0.030159
2022-01-09 01:26:28,850 iteration 979 : loss : 0.044396, loss_ce: 0.019310
2022-01-09 01:26:31,515 iteration 980 : loss : 0.049293, loss_ce: 0.017370
2022-01-09 01:26:34,334 iteration 981 : loss : 0.066913, loss_ce: 0.026926
2022-01-09 01:26:37,158 iteration 982 : loss : 0.058334, loss_ce: 0.030010
2022-01-09 01:26:39,960 iteration 983 : loss : 0.055687, loss_ce: 0.022708
2022-01-09 01:26:42,901 iteration 984 : loss : 0.060101, loss_ce: 0.025523
2022-01-09 01:26:45,756 iteration 985 : loss : 0.075563, loss_ce: 0.024492
2022-01-09 01:26:48,539 iteration 986 : loss : 0.085160, loss_ce: 0.027473
 14%|████▎                         | 58/400 [48:41<4:45:45, 50.13s/it]2022-01-09 01:26:51,253 iteration 987 : loss : 0.048586, loss_ce: 0.020256
2022-01-09 01:26:54,119 iteration 988 : loss : 0.066449, loss_ce: 0.027436
2022-01-09 01:26:56,963 iteration 989 : loss : 0.065735, loss_ce: 0.021349
2022-01-09 01:26:59,663 iteration 990 : loss : 0.049643, loss_ce: 0.023913
2022-01-09 01:27:02,484 iteration 991 : loss : 0.074293, loss_ce: 0.041170
2022-01-09 01:27:05,235 iteration 992 : loss : 0.068061, loss_ce: 0.028219
2022-01-09 01:27:08,128 iteration 993 : loss : 0.055815, loss_ce: 0.024666
2022-01-09 01:27:10,906 iteration 994 : loss : 0.057888, loss_ce: 0.020041
2022-01-09 01:27:13,656 iteration 995 : loss : 0.055288, loss_ce: 0.022136
2022-01-09 01:27:16,344 iteration 996 : loss : 0.043864, loss_ce: 0.016221
2022-01-09 01:27:19,213 iteration 997 : loss : 0.064780, loss_ce: 0.029259
2022-01-09 01:27:22,056 iteration 998 : loss : 0.080167, loss_ce: 0.025949
2022-01-09 01:27:24,954 iteration 999 : loss : 0.049916, loss_ce: 0.018708
2022-01-09 01:27:27,788 iteration 1000 : loss : 0.052202, loss_ce: 0.020920
2022-01-09 01:27:30,408 iteration 1001 : loss : 0.062682, loss_ce: 0.025516
2022-01-09 01:27:33,196 iteration 1002 : loss : 0.040317, loss_ce: 0.013066
2022-01-09 01:27:36,005 iteration 1003 : loss : 0.061717, loss_ce: 0.025375
 15%|████▍                         | 59/400 [49:29<4:40:23, 49.34s/it]2022-01-09 01:27:38,804 iteration 1004 : loss : 0.046498, loss_ce: 0.019481
2022-01-09 01:27:41,738 iteration 1005 : loss : 0.112700, loss_ce: 0.040018
2022-01-09 01:27:44,587 iteration 1006 : loss : 0.074058, loss_ce: 0.037650
2022-01-09 01:27:47,450 iteration 1007 : loss : 0.050024, loss_ce: 0.020490
2022-01-09 01:27:50,304 iteration 1008 : loss : 0.075338, loss_ce: 0.023495
2022-01-09 01:27:53,101 iteration 1009 : loss : 0.067234, loss_ce: 0.024246
2022-01-09 01:27:55,749 iteration 1010 : loss : 0.045212, loss_ce: 0.017791
2022-01-09 01:27:58,586 iteration 1011 : loss : 0.047157, loss_ce: 0.019457
2022-01-09 01:28:01,449 iteration 1012 : loss : 0.085194, loss_ce: 0.027505
2022-01-09 01:28:04,244 iteration 1013 : loss : 0.067133, loss_ce: 0.020638
2022-01-09 01:28:07,061 iteration 1014 : loss : 0.036446, loss_ce: 0.012765
2022-01-09 01:28:09,863 iteration 1015 : loss : 0.060039, loss_ce: 0.029456
2022-01-09 01:28:12,562 iteration 1016 : loss : 0.053072, loss_ce: 0.027556
2022-01-09 01:28:15,656 iteration 1017 : loss : 0.071023, loss_ce: 0.022174
2022-01-09 01:28:18,568 iteration 1018 : loss : 0.061613, loss_ce: 0.027299
2022-01-09 01:28:21,323 iteration 1019 : loss : 0.069508, loss_ce: 0.027312
2022-01-09 01:28:21,323 Training Data Eval:
2022-01-09 01:28:36,385   Average segmentation loss on training set: 0.0456
2022-01-09 01:28:36,385 Validation Data Eval:
2022-01-09 01:28:41,712   Average segmentation loss on validation set: 0.0980
2022-01-09 01:28:44,544 iteration 1020 : loss : 0.051555, loss_ce: 0.018017
 15%|████▌                         | 60/400 [50:37<5:12:13, 55.10s/it]2022-01-09 01:28:47,472 iteration 1021 : loss : 0.072135, loss_ce: 0.027829
2022-01-09 01:28:50,466 iteration 1022 : loss : 0.057106, loss_ce: 0.018256
2022-01-09 01:28:53,335 iteration 1023 : loss : 0.082003, loss_ce: 0.027514
2022-01-09 01:28:56,260 iteration 1024 : loss : 0.044908, loss_ce: 0.021119
2022-01-09 01:28:58,957 iteration 1025 : loss : 0.066091, loss_ce: 0.025758
2022-01-09 01:29:01,709 iteration 1026 : loss : 0.064650, loss_ce: 0.023051
2022-01-09 01:29:04,626 iteration 1027 : loss : 0.057146, loss_ce: 0.029990
2022-01-09 01:29:07,393 iteration 1028 : loss : 0.069945, loss_ce: 0.020586
2022-01-09 01:29:10,212 iteration 1029 : loss : 0.078349, loss_ce: 0.022816
2022-01-09 01:29:13,065 iteration 1030 : loss : 0.053146, loss_ce: 0.020734
2022-01-09 01:29:15,917 iteration 1031 : loss : 0.060285, loss_ce: 0.026754
2022-01-09 01:29:18,709 iteration 1032 : loss : 0.044085, loss_ce: 0.016594
2022-01-09 01:29:21,399 iteration 1033 : loss : 0.059451, loss_ce: 0.021862
2022-01-09 01:29:24,319 iteration 1034 : loss : 0.052453, loss_ce: 0.025523
2022-01-09 01:29:27,189 iteration 1035 : loss : 0.069214, loss_ce: 0.023452
2022-01-09 01:29:30,075 iteration 1036 : loss : 0.067298, loss_ce: 0.027289
2022-01-09 01:29:32,801 iteration 1037 : loss : 0.050809, loss_ce: 0.020769
 15%|████▌                         | 61/400 [51:25<4:59:42, 53.05s/it]2022-01-09 01:29:35,598 iteration 1038 : loss : 0.051189, loss_ce: 0.023104
2022-01-09 01:29:38,330 iteration 1039 : loss : 0.054674, loss_ce: 0.020363
2022-01-09 01:29:41,122 iteration 1040 : loss : 0.048196, loss_ce: 0.016251
2022-01-09 01:29:43,952 iteration 1041 : loss : 0.049073, loss_ce: 0.019930
2022-01-09 01:29:46,696 iteration 1042 : loss : 0.064578, loss_ce: 0.023688
2022-01-09 01:29:49,509 iteration 1043 : loss : 0.043342, loss_ce: 0.017653
2022-01-09 01:29:52,347 iteration 1044 : loss : 0.042892, loss_ce: 0.017476
2022-01-09 01:29:55,205 iteration 1045 : loss : 0.051719, loss_ce: 0.029114
2022-01-09 01:29:57,997 iteration 1046 : loss : 0.058552, loss_ce: 0.020736
2022-01-09 01:30:00,869 iteration 1047 : loss : 0.056357, loss_ce: 0.025975
2022-01-09 01:30:03,785 iteration 1048 : loss : 0.054292, loss_ce: 0.023133
2022-01-09 01:30:06,403 iteration 1049 : loss : 0.046186, loss_ce: 0.020176
2022-01-09 01:30:09,206 iteration 1050 : loss : 0.039901, loss_ce: 0.016349
2022-01-09 01:30:12,045 iteration 1051 : loss : 0.050409, loss_ce: 0.016678
2022-01-09 01:30:14,896 iteration 1052 : loss : 0.059277, loss_ce: 0.025529
2022-01-09 01:30:17,806 iteration 1053 : loss : 0.053135, loss_ce: 0.022974
2022-01-09 01:30:20,642 iteration 1054 : loss : 0.044019, loss_ce: 0.015254
 16%|████▋                         | 62/400 [52:13<4:50:00, 51.48s/it]2022-01-09 01:30:23,453 iteration 1055 : loss : 0.048528, loss_ce: 0.017468
2022-01-09 01:30:26,409 iteration 1056 : loss : 0.071520, loss_ce: 0.033378
2022-01-09 01:30:29,258 iteration 1057 : loss : 0.069793, loss_ce: 0.018711
2022-01-09 01:30:31,852 iteration 1058 : loss : 0.039908, loss_ce: 0.017509
2022-01-09 01:30:34,710 iteration 1059 : loss : 0.051449, loss_ce: 0.022126
2022-01-09 01:30:37,480 iteration 1060 : loss : 0.045955, loss_ce: 0.020278
2022-01-09 01:30:40,310 iteration 1061 : loss : 0.043460, loss_ce: 0.015451
2022-01-09 01:30:43,103 iteration 1062 : loss : 0.049697, loss_ce: 0.019212
2022-01-09 01:30:45,967 iteration 1063 : loss : 0.058215, loss_ce: 0.019846
2022-01-09 01:30:48,747 iteration 1064 : loss : 0.053912, loss_ce: 0.028240
2022-01-09 01:30:51,565 iteration 1065 : loss : 0.059354, loss_ce: 0.022055
2022-01-09 01:30:54,359 iteration 1066 : loss : 0.042671, loss_ce: 0.019810
2022-01-09 01:30:57,097 iteration 1067 : loss : 0.067344, loss_ce: 0.024225
2022-01-09 01:31:00,010 iteration 1068 : loss : 0.079640, loss_ce: 0.031682
2022-01-09 01:31:02,862 iteration 1069 : loss : 0.061459, loss_ce: 0.026540
2022-01-09 01:31:05,630 iteration 1070 : loss : 0.047780, loss_ce: 0.020377
2022-01-09 01:31:08,426 iteration 1071 : loss : 0.047288, loss_ce: 0.018501
 16%|████▋                         | 63/400 [53:01<4:42:55, 50.37s/it]2022-01-09 01:31:11,275 iteration 1072 : loss : 0.042771, loss_ce: 0.017090
2022-01-09 01:31:14,175 iteration 1073 : loss : 0.065606, loss_ce: 0.022583
2022-01-09 01:31:17,007 iteration 1074 : loss : 0.033431, loss_ce: 0.015117
2022-01-09 01:31:19,828 iteration 1075 : loss : 0.053160, loss_ce: 0.019663
2022-01-09 01:31:22,648 iteration 1076 : loss : 0.051820, loss_ce: 0.019859
2022-01-09 01:31:25,561 iteration 1077 : loss : 0.069832, loss_ce: 0.021555
2022-01-09 01:31:28,385 iteration 1078 : loss : 0.058840, loss_ce: 0.021612
2022-01-09 01:31:31,179 iteration 1079 : loss : 0.059202, loss_ce: 0.026536
2022-01-09 01:31:34,118 iteration 1080 : loss : 0.065885, loss_ce: 0.027299
2022-01-09 01:31:36,934 iteration 1081 : loss : 0.058349, loss_ce: 0.022282
2022-01-09 01:31:39,732 iteration 1082 : loss : 0.074454, loss_ce: 0.034792
2022-01-09 01:31:42,506 iteration 1083 : loss : 0.065650, loss_ce: 0.023541
2022-01-09 01:31:45,374 iteration 1084 : loss : 0.090225, loss_ce: 0.026342
2022-01-09 01:31:48,150 iteration 1085 : loss : 0.052902, loss_ce: 0.018622
2022-01-09 01:31:51,019 iteration 1086 : loss : 0.049458, loss_ce: 0.017049
2022-01-09 01:31:53,657 iteration 1087 : loss : 0.051912, loss_ce: 0.022490
2022-01-09 01:31:56,486 iteration 1088 : loss : 0.055933, loss_ce: 0.028367
 16%|████▊                         | 64/400 [53:49<4:38:11, 49.68s/it]2022-01-09 01:31:59,086 iteration 1089 : loss : 0.044162, loss_ce: 0.018831
2022-01-09 01:32:01,891 iteration 1090 : loss : 0.063068, loss_ce: 0.023697
2022-01-09 01:32:04,728 iteration 1091 : loss : 0.049215, loss_ce: 0.025662
2022-01-09 01:32:07,558 iteration 1092 : loss : 0.059181, loss_ce: 0.016364
2022-01-09 01:32:10,177 iteration 1093 : loss : 0.072243, loss_ce: 0.021552
2022-01-09 01:32:13,139 iteration 1094 : loss : 0.107238, loss_ce: 0.024506
2022-01-09 01:32:15,716 iteration 1095 : loss : 0.046892, loss_ce: 0.015909
2022-01-09 01:32:18,311 iteration 1096 : loss : 0.044894, loss_ce: 0.018652
2022-01-09 01:32:20,976 iteration 1097 : loss : 0.075449, loss_ce: 0.031750
2022-01-09 01:32:23,824 iteration 1098 : loss : 0.074890, loss_ce: 0.042594
2022-01-09 01:32:26,626 iteration 1099 : loss : 0.050621, loss_ce: 0.017343
2022-01-09 01:32:29,489 iteration 1100 : loss : 0.069469, loss_ce: 0.031742
2022-01-09 01:32:32,228 iteration 1101 : loss : 0.060691, loss_ce: 0.027723
2022-01-09 01:32:35,028 iteration 1102 : loss : 0.073233, loss_ce: 0.029489
2022-01-09 01:32:37,867 iteration 1103 : loss : 0.045390, loss_ce: 0.020581
2022-01-09 01:32:40,725 iteration 1104 : loss : 0.052635, loss_ce: 0.022141
2022-01-09 01:32:40,725 Training Data Eval:
2022-01-09 01:32:55,746   Average segmentation loss on training set: 0.0973
2022-01-09 01:32:55,747 Validation Data Eval:
2022-01-09 01:33:01,082   Average segmentation loss on validation set: 0.1030
2022-01-09 01:33:03,956 iteration 1105 : loss : 0.052367, loss_ce: 0.025115
 16%|████▉                         | 65/400 [54:57<5:07:11, 55.02s/it]2022-01-09 01:33:06,715 iteration 1106 : loss : 0.064458, loss_ce: 0.029640
2022-01-09 01:33:09,605 iteration 1107 : loss : 0.063153, loss_ce: 0.028600
2022-01-09 01:33:12,406 iteration 1108 : loss : 0.061737, loss_ce: 0.022406
2022-01-09 01:33:15,239 iteration 1109 : loss : 0.062054, loss_ce: 0.023931
2022-01-09 01:33:18,000 iteration 1110 : loss : 0.049352, loss_ce: 0.017952
2022-01-09 01:33:20,910 iteration 1111 : loss : 0.095762, loss_ce: 0.030110
2022-01-09 01:33:23,670 iteration 1112 : loss : 0.043861, loss_ce: 0.018789
2022-01-09 01:33:26,466 iteration 1113 : loss : 0.042135, loss_ce: 0.016330
2022-01-09 01:33:29,224 iteration 1114 : loss : 0.065468, loss_ce: 0.022495
2022-01-09 01:33:32,051 iteration 1115 : loss : 0.038219, loss_ce: 0.015288
2022-01-09 01:33:34,820 iteration 1116 : loss : 0.063077, loss_ce: 0.030670
2022-01-09 01:33:37,605 iteration 1117 : loss : 0.066698, loss_ce: 0.027375
2022-01-09 01:33:40,264 iteration 1118 : loss : 0.058726, loss_ce: 0.025000
2022-01-09 01:33:43,089 iteration 1119 : loss : 0.046732, loss_ce: 0.020024
2022-01-09 01:33:45,899 iteration 1120 : loss : 0.052486, loss_ce: 0.013887
2022-01-09 01:33:48,513 iteration 1121 : loss : 0.054303, loss_ce: 0.025137
2022-01-09 01:33:51,327 iteration 1122 : loss : 0.058889, loss_ce: 0.016980
 16%|████▉                         | 66/400 [55:44<4:53:29, 52.72s/it]2022-01-09 01:33:54,219 iteration 1123 : loss : 0.036242, loss_ce: 0.013493
2022-01-09 01:33:56,874 iteration 1124 : loss : 0.035599, loss_ce: 0.016612
2022-01-09 01:33:59,667 iteration 1125 : loss : 0.058209, loss_ce: 0.015753
2022-01-09 01:34:02,458 iteration 1126 : loss : 0.060854, loss_ce: 0.018442
2022-01-09 01:34:05,447 iteration 1127 : loss : 0.051871, loss_ce: 0.018924
2022-01-09 01:34:08,518 iteration 1128 : loss : 0.043020, loss_ce: 0.018891
2022-01-09 01:34:11,396 iteration 1129 : loss : 0.052078, loss_ce: 0.022676
2022-01-09 01:34:14,200 iteration 1130 : loss : 0.042369, loss_ce: 0.016767
2022-01-09 01:34:17,065 iteration 1131 : loss : 0.060521, loss_ce: 0.024438
2022-01-09 01:34:19,796 iteration 1132 : loss : 0.071209, loss_ce: 0.026121
2022-01-09 01:34:22,652 iteration 1133 : loss : 0.047302, loss_ce: 0.021317
2022-01-09 01:34:25,494 iteration 1134 : loss : 0.050614, loss_ce: 0.014806
2022-01-09 01:34:28,321 iteration 1135 : loss : 0.078590, loss_ce: 0.028807
2022-01-09 01:34:30,979 iteration 1136 : loss : 0.037810, loss_ce: 0.017976
2022-01-09 01:34:33,800 iteration 1137 : loss : 0.058210, loss_ce: 0.027624
2022-01-09 01:34:36,586 iteration 1138 : loss : 0.066746, loss_ce: 0.027118
2022-01-09 01:34:39,376 iteration 1139 : loss : 0.050334, loss_ce: 0.022334
 17%|█████                         | 67/400 [56:32<4:44:50, 51.32s/it]2022-01-09 01:34:42,340 iteration 1140 : loss : 0.056192, loss_ce: 0.025104
2022-01-09 01:34:45,203 iteration 1141 : loss : 0.092489, loss_ce: 0.044462
2022-01-09 01:34:48,108 iteration 1142 : loss : 0.049177, loss_ce: 0.020644
2022-01-09 01:34:50,772 iteration 1143 : loss : 0.059029, loss_ce: 0.023162
2022-01-09 01:34:53,705 iteration 1144 : loss : 0.048328, loss_ce: 0.014588
2022-01-09 01:34:56,533 iteration 1145 : loss : 0.055517, loss_ce: 0.021551
2022-01-09 01:34:59,308 iteration 1146 : loss : 0.042996, loss_ce: 0.018630
2022-01-09 01:35:02,250 iteration 1147 : loss : 0.036764, loss_ce: 0.014218
2022-01-09 01:35:05,065 iteration 1148 : loss : 0.041656, loss_ce: 0.016978
2022-01-09 01:35:07,699 iteration 1149 : loss : 0.032418, loss_ce: 0.015651
2022-01-09 01:35:10,492 iteration 1150 : loss : 0.052261, loss_ce: 0.019979
2022-01-09 01:35:13,198 iteration 1151 : loss : 0.032360, loss_ce: 0.014277
2022-01-09 01:35:15,996 iteration 1152 : loss : 0.053495, loss_ce: 0.022559
2022-01-09 01:35:18,878 iteration 1153 : loss : 0.069343, loss_ce: 0.023368
2022-01-09 01:35:21,639 iteration 1154 : loss : 0.038835, loss_ce: 0.014020
2022-01-09 01:35:24,480 iteration 1155 : loss : 0.048370, loss_ce: 0.016062
2022-01-09 01:35:27,454 iteration 1156 : loss : 0.079379, loss_ce: 0.024050
 17%|█████                         | 68/400 [57:20<4:38:34, 50.35s/it]2022-01-09 01:35:30,199 iteration 1157 : loss : 0.061546, loss_ce: 0.024283
2022-01-09 01:35:32,995 iteration 1158 : loss : 0.043902, loss_ce: 0.015060
2022-01-09 01:35:35,531 iteration 1159 : loss : 0.048126, loss_ce: 0.017779
2022-01-09 01:35:38,390 iteration 1160 : loss : 0.052397, loss_ce: 0.026361
2022-01-09 01:35:41,167 iteration 1161 : loss : 0.056745, loss_ce: 0.019844
2022-01-09 01:35:44,026 iteration 1162 : loss : 0.053085, loss_ce: 0.022392
2022-01-09 01:35:46,715 iteration 1163 : loss : 0.041146, loss_ce: 0.019765
2022-01-09 01:35:49,553 iteration 1164 : loss : 0.063579, loss_ce: 0.027244
2022-01-09 01:35:52,148 iteration 1165 : loss : 0.042574, loss_ce: 0.019450
2022-01-09 01:35:54,932 iteration 1166 : loss : 0.065846, loss_ce: 0.022606
2022-01-09 01:35:57,711 iteration 1167 : loss : 0.051400, loss_ce: 0.023356
2022-01-09 01:36:00,495 iteration 1168 : loss : 0.115091, loss_ce: 0.030506
2022-01-09 01:36:03,152 iteration 1169 : loss : 0.084543, loss_ce: 0.026729
2022-01-09 01:36:05,963 iteration 1170 : loss : 0.054726, loss_ce: 0.015419
2022-01-09 01:36:08,950 iteration 1171 : loss : 0.109456, loss_ce: 0.036514
2022-01-09 01:36:11,525 iteration 1172 : loss : 0.041515, loss_ce: 0.016082
2022-01-09 01:36:14,338 iteration 1173 : loss : 0.046372, loss_ce: 0.016760
 17%|█████▏                        | 69/400 [58:07<4:32:01, 49.31s/it]2022-01-09 01:36:17,181 iteration 1174 : loss : 0.061603, loss_ce: 0.020281
2022-01-09 01:36:19,938 iteration 1175 : loss : 0.078448, loss_ce: 0.031896
2022-01-09 01:36:22,757 iteration 1176 : loss : 0.073284, loss_ce: 0.025456
2022-01-09 01:36:25,596 iteration 1177 : loss : 0.082783, loss_ce: 0.035851
2022-01-09 01:36:28,195 iteration 1178 : loss : 0.071145, loss_ce: 0.024863
2022-01-09 01:36:31,035 iteration 1179 : loss : 0.049943, loss_ce: 0.020512
2022-01-09 01:36:33,868 iteration 1180 : loss : 0.047986, loss_ce: 0.020913
2022-01-09 01:36:36,527 iteration 1181 : loss : 0.054186, loss_ce: 0.017821
2022-01-09 01:36:39,344 iteration 1182 : loss : 0.038645, loss_ce: 0.017539
2022-01-09 01:36:42,104 iteration 1183 : loss : 0.048199, loss_ce: 0.021925
2022-01-09 01:36:44,942 iteration 1184 : loss : 0.081079, loss_ce: 0.031185
2022-01-09 01:36:47,741 iteration 1185 : loss : 0.051667, loss_ce: 0.028683
2022-01-09 01:36:50,551 iteration 1186 : loss : 0.049847, loss_ce: 0.016872
2022-01-09 01:36:53,458 iteration 1187 : loss : 0.033589, loss_ce: 0.015267
2022-01-09 01:36:56,394 iteration 1188 : loss : 0.092695, loss_ce: 0.034392
2022-01-09 01:36:59,145 iteration 1189 : loss : 0.098367, loss_ce: 0.022645
2022-01-09 01:36:59,146 Training Data Eval:
2022-01-09 01:37:14,235   Average segmentation loss on training set: 0.0406
2022-01-09 01:37:14,236 Validation Data Eval:
2022-01-09 01:37:19,511   Average segmentation loss on validation set: 0.0727
2022-01-09 01:37:24,718 Found new lowest validation loss at iteration 1189! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 01:37:26,638 iteration 1190 : loss : 0.063573, loss_ce: 0.021634
 18%|█████▎                        | 70/400 [59:19<5:09:06, 56.20s/it]2022-01-09 01:37:29,165 iteration 1191 : loss : 0.049332, loss_ce: 0.020487
2022-01-09 01:37:31,980 iteration 1192 : loss : 0.065885, loss_ce: 0.035975
2022-01-09 01:37:34,560 iteration 1193 : loss : 0.054327, loss_ce: 0.019779
2022-01-09 01:37:37,355 iteration 1194 : loss : 0.057982, loss_ce: 0.020040
2022-01-09 01:37:40,021 iteration 1195 : loss : 0.064905, loss_ce: 0.025033
2022-01-09 01:37:42,807 iteration 1196 : loss : 0.040261, loss_ce: 0.017511
2022-01-09 01:37:45,668 iteration 1197 : loss : 0.080933, loss_ce: 0.025915
2022-01-09 01:37:48,275 iteration 1198 : loss : 0.034673, loss_ce: 0.012445
2022-01-09 01:37:51,049 iteration 1199 : loss : 0.061654, loss_ce: 0.029210
2022-01-09 01:37:53,824 iteration 1200 : loss : 0.037531, loss_ce: 0.015092
2022-01-09 01:37:56,620 iteration 1201 : loss : 0.052655, loss_ce: 0.016237
2022-01-09 01:37:59,335 iteration 1202 : loss : 0.055371, loss_ce: 0.022265
2022-01-09 01:38:02,170 iteration 1203 : loss : 0.049506, loss_ce: 0.021203
2022-01-09 01:38:04,928 iteration 1204 : loss : 0.063704, loss_ce: 0.017275
2022-01-09 01:38:07,765 iteration 1205 : loss : 0.046296, loss_ce: 0.021240
2022-01-09 01:38:10,530 iteration 1206 : loss : 0.057859, loss_ce: 0.020395
2022-01-09 01:38:13,539 iteration 1207 : loss : 0.042796, loss_ce: 0.018277
 18%|████▉                       | 71/400 [1:00:06<4:52:52, 53.41s/it]2022-01-09 01:38:16,335 iteration 1208 : loss : 0.051557, loss_ce: 0.022451
2022-01-09 01:38:19,138 iteration 1209 : loss : 0.045487, loss_ce: 0.016788
2022-01-09 01:38:21,791 iteration 1210 : loss : 0.045881, loss_ce: 0.017591
2022-01-09 01:38:24,637 iteration 1211 : loss : 0.054592, loss_ce: 0.024741
2022-01-09 01:38:27,391 iteration 1212 : loss : 0.044929, loss_ce: 0.016642
2022-01-09 01:38:30,013 iteration 1213 : loss : 0.035636, loss_ce: 0.012068
2022-01-09 01:38:33,057 iteration 1214 : loss : 0.078424, loss_ce: 0.035584
2022-01-09 01:38:35,752 iteration 1215 : loss : 0.040037, loss_ce: 0.016339
2022-01-09 01:38:38,566 iteration 1216 : loss : 0.049898, loss_ce: 0.016593
2022-01-09 01:38:41,465 iteration 1217 : loss : 0.086726, loss_ce: 0.035085
2022-01-09 01:38:44,257 iteration 1218 : loss : 0.055637, loss_ce: 0.024868
2022-01-09 01:38:47,191 iteration 1219 : loss : 0.053349, loss_ce: 0.027620
2022-01-09 01:38:49,935 iteration 1220 : loss : 0.048277, loss_ce: 0.018652
2022-01-09 01:38:52,857 iteration 1221 : loss : 0.046555, loss_ce: 0.015531
2022-01-09 01:38:55,468 iteration 1222 : loss : 0.062203, loss_ce: 0.019636
2022-01-09 01:38:58,225 iteration 1223 : loss : 0.055309, loss_ce: 0.020828
2022-01-09 01:39:00,838 iteration 1224 : loss : 0.086515, loss_ce: 0.033110
 18%|█████                       | 72/400 [1:00:54<4:41:58, 51.58s/it]2022-01-09 01:39:03,719 iteration 1225 : loss : 0.054947, loss_ce: 0.016709
2022-01-09 01:39:06,517 iteration 1226 : loss : 0.041622, loss_ce: 0.014259
2022-01-09 01:39:09,281 iteration 1227 : loss : 0.058253, loss_ce: 0.021914
2022-01-09 01:39:11,904 iteration 1228 : loss : 0.099250, loss_ce: 0.035857
2022-01-09 01:39:14,806 iteration 1229 : loss : 0.079953, loss_ce: 0.043118
2022-01-09 01:39:17,629 iteration 1230 : loss : 0.051190, loss_ce: 0.020869
2022-01-09 01:39:20,343 iteration 1231 : loss : 0.041422, loss_ce: 0.016144
2022-01-09 01:39:23,157 iteration 1232 : loss : 0.063623, loss_ce: 0.024290
2022-01-09 01:39:25,993 iteration 1233 : loss : 0.048762, loss_ce: 0.016474
2022-01-09 01:39:28,894 iteration 1234 : loss : 0.060945, loss_ce: 0.021479
2022-01-09 01:39:31,663 iteration 1235 : loss : 0.052590, loss_ce: 0.021424
2022-01-09 01:39:34,487 iteration 1236 : loss : 0.050679, loss_ce: 0.022883
2022-01-09 01:39:37,303 iteration 1237 : loss : 0.059455, loss_ce: 0.020218
2022-01-09 01:39:40,070 iteration 1238 : loss : 0.052347, loss_ce: 0.020713
2022-01-09 01:39:42,934 iteration 1239 : loss : 0.038640, loss_ce: 0.017826
2022-01-09 01:39:45,853 iteration 1240 : loss : 0.064856, loss_ce: 0.024185
2022-01-09 01:39:48,585 iteration 1241 : loss : 0.040415, loss_ce: 0.017839
 18%|█████                       | 73/400 [1:01:41<4:34:51, 50.43s/it]2022-01-09 01:39:51,562 iteration 1242 : loss : 0.060267, loss_ce: 0.033056
2022-01-09 01:39:54,390 iteration 1243 : loss : 0.052746, loss_ce: 0.022205
2022-01-09 01:39:57,118 iteration 1244 : loss : 0.060641, loss_ce: 0.022115
2022-01-09 01:40:00,034 iteration 1245 : loss : 0.047852, loss_ce: 0.018166
2022-01-09 01:40:02,890 iteration 1246 : loss : 0.058619, loss_ce: 0.025112
2022-01-09 01:40:05,702 iteration 1247 : loss : 0.065737, loss_ce: 0.027818
2022-01-09 01:40:08,462 iteration 1248 : loss : 0.039886, loss_ce: 0.017722
2022-01-09 01:40:11,282 iteration 1249 : loss : 0.043573, loss_ce: 0.019279
2022-01-09 01:40:14,063 iteration 1250 : loss : 0.070176, loss_ce: 0.014392
2022-01-09 01:40:17,099 iteration 1251 : loss : 0.051652, loss_ce: 0.022220
2022-01-09 01:40:19,795 iteration 1252 : loss : 0.077496, loss_ce: 0.022472
2022-01-09 01:40:22,605 iteration 1253 : loss : 0.054535, loss_ce: 0.024147
2022-01-09 01:40:25,442 iteration 1254 : loss : 0.047222, loss_ce: 0.015109
2022-01-09 01:40:28,338 iteration 1255 : loss : 0.062516, loss_ce: 0.028205
2022-01-09 01:40:31,236 iteration 1256 : loss : 0.049390, loss_ce: 0.014824
2022-01-09 01:40:33,927 iteration 1257 : loss : 0.042449, loss_ce: 0.015030
2022-01-09 01:40:36,697 iteration 1258 : loss : 0.046486, loss_ce: 0.020545
 18%|█████▏                      | 74/400 [1:02:29<4:30:13, 49.73s/it]2022-01-09 01:40:39,479 iteration 1259 : loss : 0.047261, loss_ce: 0.018259
2022-01-09 01:40:42,364 iteration 1260 : loss : 0.041870, loss_ce: 0.016422
2022-01-09 01:40:45,141 iteration 1261 : loss : 0.045736, loss_ce: 0.014700
2022-01-09 01:40:47,919 iteration 1262 : loss : 0.035887, loss_ce: 0.012305
2022-01-09 01:40:50,554 iteration 1263 : loss : 0.035439, loss_ce: 0.012901
2022-01-09 01:40:53,360 iteration 1264 : loss : 0.049893, loss_ce: 0.019215
2022-01-09 01:40:56,165 iteration 1265 : loss : 0.048790, loss_ce: 0.016760
2022-01-09 01:40:58,784 iteration 1266 : loss : 0.039590, loss_ce: 0.017371
2022-01-09 01:41:01,437 iteration 1267 : loss : 0.061372, loss_ce: 0.027979
2022-01-09 01:41:04,228 iteration 1268 : loss : 0.039128, loss_ce: 0.013045
2022-01-09 01:41:07,086 iteration 1269 : loss : 0.048458, loss_ce: 0.022217
2022-01-09 01:41:09,778 iteration 1270 : loss : 0.031077, loss_ce: 0.012665
2022-01-09 01:41:12,644 iteration 1271 : loss : 0.054533, loss_ce: 0.026596
2022-01-09 01:41:15,228 iteration 1272 : loss : 0.046980, loss_ce: 0.013748
2022-01-09 01:41:18,061 iteration 1273 : loss : 0.048016, loss_ce: 0.015954
2022-01-09 01:41:20,861 iteration 1274 : loss : 0.068083, loss_ce: 0.029229
2022-01-09 01:41:20,861 Training Data Eval:
2022-01-09 01:41:35,764   Average segmentation loss on training set: 0.0368
2022-01-09 01:41:35,764 Validation Data Eval:
2022-01-09 01:41:41,058   Average segmentation loss on validation set: 0.0907
2022-01-09 01:41:43,785 iteration 1275 : loss : 0.043745, loss_ce: 0.017462
 19%|█████▎                      | 75/400 [1:03:36<4:57:37, 54.94s/it]2022-01-09 01:41:46,628 iteration 1276 : loss : 0.056520, loss_ce: 0.027309
2022-01-09 01:41:49,599 iteration 1277 : loss : 0.071406, loss_ce: 0.027580
2022-01-09 01:41:52,422 iteration 1278 : loss : 0.079489, loss_ce: 0.033621
2022-01-09 01:41:55,070 iteration 1279 : loss : 0.049170, loss_ce: 0.017405
2022-01-09 01:41:57,891 iteration 1280 : loss : 0.038006, loss_ce: 0.017078
2022-01-09 01:42:00,719 iteration 1281 : loss : 0.061421, loss_ce: 0.032770
2022-01-09 01:42:03,389 iteration 1282 : loss : 0.035251, loss_ce: 0.014757
2022-01-09 01:42:06,029 iteration 1283 : loss : 0.052182, loss_ce: 0.014959
2022-01-09 01:42:08,814 iteration 1284 : loss : 0.041812, loss_ce: 0.017058
2022-01-09 01:42:11,560 iteration 1285 : loss : 0.034259, loss_ce: 0.011864
2022-01-09 01:42:14,377 iteration 1286 : loss : 0.058274, loss_ce: 0.021376
2022-01-09 01:42:17,142 iteration 1287 : loss : 0.031433, loss_ce: 0.012205
2022-01-09 01:42:19,926 iteration 1288 : loss : 0.059673, loss_ce: 0.023257
2022-01-09 01:42:22,690 iteration 1289 : loss : 0.045856, loss_ce: 0.018865
2022-01-09 01:42:25,452 iteration 1290 : loss : 0.067087, loss_ce: 0.022450
2022-01-09 01:42:28,239 iteration 1291 : loss : 0.046444, loss_ce: 0.018321
2022-01-09 01:42:31,052 iteration 1292 : loss : 0.057719, loss_ce: 0.032671
 19%|█████▎                      | 76/400 [1:04:24<4:44:14, 52.64s/it]2022-01-09 01:42:33,992 iteration 1293 : loss : 0.051600, loss_ce: 0.022496
2022-01-09 01:42:36,579 iteration 1294 : loss : 0.036949, loss_ce: 0.014972
2022-01-09 01:42:39,410 iteration 1295 : loss : 0.055962, loss_ce: 0.022811
2022-01-09 01:42:42,188 iteration 1296 : loss : 0.082153, loss_ce: 0.024329
2022-01-09 01:42:44,795 iteration 1297 : loss : 0.038080, loss_ce: 0.015504
2022-01-09 01:42:47,661 iteration 1298 : loss : 0.060182, loss_ce: 0.020752
2022-01-09 01:42:50,435 iteration 1299 : loss : 0.037621, loss_ce: 0.015148
2022-01-09 01:42:53,277 iteration 1300 : loss : 0.055846, loss_ce: 0.016970
2022-01-09 01:42:56,031 iteration 1301 : loss : 0.061677, loss_ce: 0.018585
2022-01-09 01:42:58,687 iteration 1302 : loss : 0.065662, loss_ce: 0.023636
2022-01-09 01:43:01,480 iteration 1303 : loss : 0.061672, loss_ce: 0.020739
2022-01-09 01:43:04,219 iteration 1304 : loss : 0.047478, loss_ce: 0.022459
2022-01-09 01:43:07,064 iteration 1305 : loss : 0.069598, loss_ce: 0.032234
2022-01-09 01:43:09,883 iteration 1306 : loss : 0.042047, loss_ce: 0.016801
2022-01-09 01:43:12,491 iteration 1307 : loss : 0.038668, loss_ce: 0.014262
2022-01-09 01:43:15,297 iteration 1308 : loss : 0.051497, loss_ce: 0.024369
2022-01-09 01:43:18,094 iteration 1309 : loss : 0.044343, loss_ce: 0.016812
 19%|█████▍                      | 77/400 [1:05:11<4:34:19, 50.96s/it]2022-01-09 01:43:20,745 iteration 1310 : loss : 0.048859, loss_ce: 0.017894
2022-01-09 01:43:23,643 iteration 1311 : loss : 0.059081, loss_ce: 0.019990
2022-01-09 01:43:26,478 iteration 1312 : loss : 0.040943, loss_ce: 0.013382
2022-01-09 01:43:29,341 iteration 1313 : loss : 0.044965, loss_ce: 0.016528
2022-01-09 01:43:32,178 iteration 1314 : loss : 0.048979, loss_ce: 0.024314
2022-01-09 01:43:35,085 iteration 1315 : loss : 0.054432, loss_ce: 0.029365
2022-01-09 01:43:37,713 iteration 1316 : loss : 0.037793, loss_ce: 0.017384
2022-01-09 01:43:40,538 iteration 1317 : loss : 0.035873, loss_ce: 0.015238
2022-01-09 01:43:43,152 iteration 1318 : loss : 0.043416, loss_ce: 0.017196
2022-01-09 01:43:45,977 iteration 1319 : loss : 0.053531, loss_ce: 0.020251
2022-01-09 01:43:48,752 iteration 1320 : loss : 0.038597, loss_ce: 0.014511
2022-01-09 01:43:51,540 iteration 1321 : loss : 0.054395, loss_ce: 0.025021
2022-01-09 01:43:54,319 iteration 1322 : loss : 0.043494, loss_ce: 0.014397
2022-01-09 01:43:57,142 iteration 1323 : loss : 0.053555, loss_ce: 0.020482
2022-01-09 01:44:00,000 iteration 1324 : loss : 0.039464, loss_ce: 0.017001
2022-01-09 01:44:02,792 iteration 1325 : loss : 0.037200, loss_ce: 0.015469
2022-01-09 01:44:05,603 iteration 1326 : loss : 0.048583, loss_ce: 0.017803
 20%|█████▍                      | 78/400 [1:05:58<4:27:55, 49.92s/it]2022-01-09 01:44:08,411 iteration 1327 : loss : 0.035236, loss_ce: 0.013272
2022-01-09 01:44:11,052 iteration 1328 : loss : 0.056549, loss_ce: 0.019230
2022-01-09 01:44:13,833 iteration 1329 : loss : 0.056515, loss_ce: 0.022804
2022-01-09 01:44:16,680 iteration 1330 : loss : 0.044723, loss_ce: 0.013684
2022-01-09 01:44:19,525 iteration 1331 : loss : 0.042345, loss_ce: 0.017820
2022-01-09 01:44:22,265 iteration 1332 : loss : 0.044684, loss_ce: 0.016608
2022-01-09 01:44:25,127 iteration 1333 : loss : 0.038063, loss_ce: 0.017527
2022-01-09 01:44:27,844 iteration 1334 : loss : 0.032830, loss_ce: 0.012987
2022-01-09 01:44:30,688 iteration 1335 : loss : 0.045284, loss_ce: 0.018395
2022-01-09 01:44:33,485 iteration 1336 : loss : 0.045077, loss_ce: 0.020698
2022-01-09 01:44:36,328 iteration 1337 : loss : 0.069284, loss_ce: 0.025152
2022-01-09 01:44:39,139 iteration 1338 : loss : 0.033886, loss_ce: 0.013837
2022-01-09 01:44:41,889 iteration 1339 : loss : 0.047235, loss_ce: 0.017828
2022-01-09 01:44:44,737 iteration 1340 : loss : 0.034693, loss_ce: 0.012677
2022-01-09 01:44:47,468 iteration 1341 : loss : 0.045282, loss_ce: 0.017875
2022-01-09 01:44:50,417 iteration 1342 : loss : 0.045897, loss_ce: 0.015629
2022-01-09 01:44:52,996 iteration 1343 : loss : 0.042448, loss_ce: 0.013744
 20%|█████▌                      | 79/400 [1:06:46<4:23:01, 49.16s/it]2022-01-09 01:44:55,926 iteration 1344 : loss : 0.034921, loss_ce: 0.011153
2022-01-09 01:44:58,548 iteration 1345 : loss : 0.037866, loss_ce: 0.009953
2022-01-09 01:45:01,375 iteration 1346 : loss : 0.042539, loss_ce: 0.013928
2022-01-09 01:45:04,156 iteration 1347 : loss : 0.044765, loss_ce: 0.019529
2022-01-09 01:45:06,942 iteration 1348 : loss : 0.052678, loss_ce: 0.020935
2022-01-09 01:45:09,737 iteration 1349 : loss : 0.062529, loss_ce: 0.027631
2022-01-09 01:45:12,590 iteration 1350 : loss : 0.053251, loss_ce: 0.018929
2022-01-09 01:45:15,333 iteration 1351 : loss : 0.055633, loss_ce: 0.023538
2022-01-09 01:45:18,113 iteration 1352 : loss : 0.047751, loss_ce: 0.020210
2022-01-09 01:45:20,882 iteration 1353 : loss : 0.049827, loss_ce: 0.023553
2022-01-09 01:45:23,675 iteration 1354 : loss : 0.053151, loss_ce: 0.019282
2022-01-09 01:45:26,420 iteration 1355 : loss : 0.050462, loss_ce: 0.016009
2022-01-09 01:45:29,258 iteration 1356 : loss : 0.050322, loss_ce: 0.017278
2022-01-09 01:45:32,034 iteration 1357 : loss : 0.034919, loss_ce: 0.016901
2022-01-09 01:45:34,843 iteration 1358 : loss : 0.049641, loss_ce: 0.023670
2022-01-09 01:45:37,689 iteration 1359 : loss : 0.059204, loss_ce: 0.032450
2022-01-09 01:45:37,690 Training Data Eval:
2022-01-09 01:45:52,675   Average segmentation loss on training set: 0.0347
2022-01-09 01:45:52,676 Validation Data Eval:
2022-01-09 01:45:57,977   Average segmentation loss on validation set: 0.0874
2022-01-09 01:46:00,837 iteration 1360 : loss : 0.032097, loss_ce: 0.012856
 20%|█████▌                      | 80/400 [1:07:53<4:52:04, 54.76s/it]2022-01-09 01:46:03,695 iteration 1361 : loss : 0.060893, loss_ce: 0.017802
2022-01-09 01:46:06,467 iteration 1362 : loss : 0.042737, loss_ce: 0.013288
2022-01-09 01:46:09,232 iteration 1363 : loss : 0.036613, loss_ce: 0.015334
2022-01-09 01:46:11,995 iteration 1364 : loss : 0.045579, loss_ce: 0.023340
2022-01-09 01:46:14,808 iteration 1365 : loss : 0.048007, loss_ce: 0.015936
2022-01-09 01:46:17,622 iteration 1366 : loss : 0.067293, loss_ce: 0.026520
2022-01-09 01:46:20,452 iteration 1367 : loss : 0.063430, loss_ce: 0.033925
2022-01-09 01:46:23,440 iteration 1368 : loss : 0.046331, loss_ce: 0.018008
2022-01-09 01:46:26,128 iteration 1369 : loss : 0.058707, loss_ce: 0.019741
2022-01-09 01:46:28,721 iteration 1370 : loss : 0.062108, loss_ce: 0.030546
2022-01-09 01:46:31,487 iteration 1371 : loss : 0.048273, loss_ce: 0.019459
2022-01-09 01:46:34,410 iteration 1372 : loss : 0.054936, loss_ce: 0.022234
2022-01-09 01:46:37,174 iteration 1373 : loss : 0.046700, loss_ce: 0.019176
2022-01-09 01:46:40,150 iteration 1374 : loss : 0.047419, loss_ce: 0.017167
2022-01-09 01:46:43,020 iteration 1375 : loss : 0.037342, loss_ce: 0.014016
2022-01-09 01:46:45,800 iteration 1376 : loss : 0.033329, loss_ce: 0.009865
2022-01-09 01:46:48,821 iteration 1377 : loss : 0.039633, loss_ce: 0.021089
 20%|█████▋                      | 81/400 [1:08:41<4:40:21, 52.73s/it]2022-01-09 01:46:51,754 iteration 1378 : loss : 0.044038, loss_ce: 0.022787
2022-01-09 01:46:54,625 iteration 1379 : loss : 0.051276, loss_ce: 0.020668
2022-01-09 01:46:57,426 iteration 1380 : loss : 0.070971, loss_ce: 0.023164
2022-01-09 01:47:00,270 iteration 1381 : loss : 0.033459, loss_ce: 0.013078
2022-01-09 01:47:03,091 iteration 1382 : loss : 0.059448, loss_ce: 0.024594
2022-01-09 01:47:05,655 iteration 1383 : loss : 0.058822, loss_ce: 0.016474
2022-01-09 01:47:08,493 iteration 1384 : loss : 0.040676, loss_ce: 0.017802
2022-01-09 01:47:11,246 iteration 1385 : loss : 0.032384, loss_ce: 0.013405
2022-01-09 01:47:13,936 iteration 1386 : loss : 0.054192, loss_ce: 0.019077
2022-01-09 01:47:16,674 iteration 1387 : loss : 0.052901, loss_ce: 0.019353
2022-01-09 01:47:19,531 iteration 1388 : loss : 0.046337, loss_ce: 0.015535
2022-01-09 01:47:22,364 iteration 1389 : loss : 0.053274, loss_ce: 0.020903
2022-01-09 01:47:25,223 iteration 1390 : loss : 0.043215, loss_ce: 0.016797
2022-01-09 01:47:28,020 iteration 1391 : loss : 0.041227, loss_ce: 0.009098
2022-01-09 01:47:30,837 iteration 1392 : loss : 0.065192, loss_ce: 0.021364
2022-01-09 01:47:33,621 iteration 1393 : loss : 0.039160, loss_ce: 0.011102
2022-01-09 01:47:36,495 iteration 1394 : loss : 0.037820, loss_ce: 0.018060
 20%|█████▋                      | 82/400 [1:09:29<4:31:26, 51.21s/it]2022-01-09 01:47:39,346 iteration 1395 : loss : 0.040132, loss_ce: 0.014263
2022-01-09 01:47:42,136 iteration 1396 : loss : 0.037651, loss_ce: 0.014428
2022-01-09 01:47:44,958 iteration 1397 : loss : 0.057667, loss_ce: 0.017818
2022-01-09 01:47:47,862 iteration 1398 : loss : 0.031689, loss_ce: 0.013016
2022-01-09 01:47:50,682 iteration 1399 : loss : 0.061948, loss_ce: 0.031721
2022-01-09 01:47:53,326 iteration 1400 : loss : 0.064836, loss_ce: 0.036340
2022-01-09 01:47:55,940 iteration 1401 : loss : 0.054168, loss_ce: 0.018078
2022-01-09 01:47:58,700 iteration 1402 : loss : 0.037784, loss_ce: 0.014771
2022-01-09 01:48:01,591 iteration 1403 : loss : 0.044130, loss_ce: 0.017020
2022-01-09 01:48:04,435 iteration 1404 : loss : 0.059510, loss_ce: 0.025734
2022-01-09 01:48:07,319 iteration 1405 : loss : 0.044412, loss_ce: 0.018765
2022-01-09 01:48:10,167 iteration 1406 : loss : 0.044370, loss_ce: 0.014643
2022-01-09 01:48:12,984 iteration 1407 : loss : 0.038189, loss_ce: 0.019228
2022-01-09 01:48:15,730 iteration 1408 : loss : 0.060753, loss_ce: 0.019181
2022-01-09 01:48:18,600 iteration 1409 : loss : 0.091794, loss_ce: 0.025756
2022-01-09 01:48:21,416 iteration 1410 : loss : 0.035513, loss_ce: 0.012250
2022-01-09 01:48:24,116 iteration 1411 : loss : 0.057951, loss_ce: 0.033262
 21%|█████▊                      | 83/400 [1:10:17<4:24:53, 50.14s/it]2022-01-09 01:48:26,938 iteration 1412 : loss : 0.054724, loss_ce: 0.019771
2022-01-09 01:48:29,794 iteration 1413 : loss : 0.038075, loss_ce: 0.013174
2022-01-09 01:48:32,431 iteration 1414 : loss : 0.038966, loss_ce: 0.017839
2022-01-09 01:48:35,192 iteration 1415 : loss : 0.050284, loss_ce: 0.016302
2022-01-09 01:48:37,989 iteration 1416 : loss : 0.042988, loss_ce: 0.012656
2022-01-09 01:48:40,758 iteration 1417 : loss : 0.040927, loss_ce: 0.016438
2022-01-09 01:48:43,414 iteration 1418 : loss : 0.033972, loss_ce: 0.010933
2022-01-09 01:48:46,167 iteration 1419 : loss : 0.054317, loss_ce: 0.018055
2022-01-09 01:48:49,042 iteration 1420 : loss : 0.046989, loss_ce: 0.015355
2022-01-09 01:48:51,642 iteration 1421 : loss : 0.042556, loss_ce: 0.018229
2022-01-09 01:48:54,454 iteration 1422 : loss : 0.061494, loss_ce: 0.025782
2022-01-09 01:48:57,259 iteration 1423 : loss : 0.079579, loss_ce: 0.037541
2022-01-09 01:49:00,096 iteration 1424 : loss : 0.045490, loss_ce: 0.014589
2022-01-09 01:49:02,964 iteration 1425 : loss : 0.063045, loss_ce: 0.025877
2022-01-09 01:49:05,671 iteration 1426 : loss : 0.040747, loss_ce: 0.016712
2022-01-09 01:49:08,491 iteration 1427 : loss : 0.046779, loss_ce: 0.020097
2022-01-09 01:49:11,343 iteration 1428 : loss : 0.050413, loss_ce: 0.021486
 21%|█████▉                      | 84/400 [1:11:04<4:19:26, 49.26s/it]2022-01-09 01:49:14,273 iteration 1429 : loss : 0.087225, loss_ce: 0.041511
2022-01-09 01:49:17,088 iteration 1430 : loss : 0.061855, loss_ce: 0.018123
2022-01-09 01:49:19,919 iteration 1431 : loss : 0.038327, loss_ce: 0.019207
2022-01-09 01:49:22,777 iteration 1432 : loss : 0.049938, loss_ce: 0.020653
2022-01-09 01:49:25,633 iteration 1433 : loss : 0.049126, loss_ce: 0.018522
2022-01-09 01:49:28,502 iteration 1434 : loss : 0.038094, loss_ce: 0.014749
2022-01-09 01:49:31,242 iteration 1435 : loss : 0.043446, loss_ce: 0.012636
2022-01-09 01:49:34,113 iteration 1436 : loss : 0.055406, loss_ce: 0.024568
2022-01-09 01:49:36,662 iteration 1437 : loss : 0.076505, loss_ce: 0.019833
2022-01-09 01:49:39,229 iteration 1438 : loss : 0.037416, loss_ce: 0.018532
2022-01-09 01:49:41,942 iteration 1439 : loss : 0.034151, loss_ce: 0.010713
2022-01-09 01:49:44,823 iteration 1440 : loss : 0.057015, loss_ce: 0.022363
2022-01-09 01:49:47,637 iteration 1441 : loss : 0.036587, loss_ce: 0.015215
2022-01-09 01:49:50,246 iteration 1442 : loss : 0.050232, loss_ce: 0.017432
2022-01-09 01:49:52,906 iteration 1443 : loss : 0.056255, loss_ce: 0.025158
2022-01-09 01:49:55,736 iteration 1444 : loss : 0.033326, loss_ce: 0.014701
2022-01-09 01:49:55,736 Training Data Eval:
2022-01-09 01:50:10,894   Average segmentation loss on training set: 0.0341
2022-01-09 01:50:10,894 Validation Data Eval:
2022-01-09 01:50:16,173   Average segmentation loss on validation set: 0.0722
2022-01-09 01:50:21,923 Found new lowest validation loss at iteration 1444! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 01:50:23,845 iteration 1445 : loss : 0.050298, loss_ce: 0.014638
 21%|█████▉                      | 85/400 [1:12:17<4:55:15, 56.24s/it]2022-01-09 01:50:26,439 iteration 1446 : loss : 0.051525, loss_ce: 0.018932
2022-01-09 01:50:29,129 iteration 1447 : loss : 0.070648, loss_ce: 0.028213
2022-01-09 01:50:31,892 iteration 1448 : loss : 0.050403, loss_ce: 0.024766
2022-01-09 01:50:34,708 iteration 1449 : loss : 0.063004, loss_ce: 0.023402
2022-01-09 01:50:37,379 iteration 1450 : loss : 0.054166, loss_ce: 0.021470
2022-01-09 01:50:40,103 iteration 1451 : loss : 0.039621, loss_ce: 0.014742
2022-01-09 01:50:42,879 iteration 1452 : loss : 0.071383, loss_ce: 0.017889
2022-01-09 01:50:45,669 iteration 1453 : loss : 0.049522, loss_ce: 0.015918
2022-01-09 01:50:48,340 iteration 1454 : loss : 0.042477, loss_ce: 0.017747
2022-01-09 01:50:51,014 iteration 1455 : loss : 0.041201, loss_ce: 0.017367
2022-01-09 01:50:53,829 iteration 1456 : loss : 0.062391, loss_ce: 0.022674
2022-01-09 01:50:56,649 iteration 1457 : loss : 0.044303, loss_ce: 0.017812
2022-01-09 01:50:59,343 iteration 1458 : loss : 0.034412, loss_ce: 0.011330
2022-01-09 01:51:02,122 iteration 1459 : loss : 0.037841, loss_ce: 0.015548
2022-01-09 01:51:04,966 iteration 1460 : loss : 0.065342, loss_ce: 0.038786
2022-01-09 01:51:07,595 iteration 1461 : loss : 0.044073, loss_ce: 0.013219
2022-01-09 01:51:10,395 iteration 1462 : loss : 0.036580, loss_ce: 0.013346
 22%|██████                      | 86/400 [1:13:03<4:39:05, 53.33s/it]2022-01-09 01:51:13,207 iteration 1463 : loss : 0.044363, loss_ce: 0.019106
2022-01-09 01:51:16,090 iteration 1464 : loss : 0.039087, loss_ce: 0.012534
2022-01-09 01:51:18,954 iteration 1465 : loss : 0.052008, loss_ce: 0.021914
2022-01-09 01:51:21,743 iteration 1466 : loss : 0.051370, loss_ce: 0.018730
2022-01-09 01:51:24,505 iteration 1467 : loss : 0.046211, loss_ce: 0.018538
2022-01-09 01:51:27,416 iteration 1468 : loss : 0.037672, loss_ce: 0.012981
2022-01-09 01:51:30,346 iteration 1469 : loss : 0.034093, loss_ce: 0.013194
2022-01-09 01:51:33,014 iteration 1470 : loss : 0.043936, loss_ce: 0.016158
2022-01-09 01:51:35,788 iteration 1471 : loss : 0.040738, loss_ce: 0.013751
2022-01-09 01:51:38,677 iteration 1472 : loss : 0.046710, loss_ce: 0.019346
2022-01-09 01:51:41,632 iteration 1473 : loss : 0.042397, loss_ce: 0.018420
2022-01-09 01:51:44,485 iteration 1474 : loss : 0.039703, loss_ce: 0.015565
2022-01-09 01:51:47,293 iteration 1475 : loss : 0.044446, loss_ce: 0.014138
2022-01-09 01:51:50,024 iteration 1476 : loss : 0.073721, loss_ce: 0.043308
2022-01-09 01:51:52,812 iteration 1477 : loss : 0.041765, loss_ce: 0.014238
2022-01-09 01:51:55,666 iteration 1478 : loss : 0.042290, loss_ce: 0.015634
2022-01-09 01:51:58,522 iteration 1479 : loss : 0.046884, loss_ce: 0.017085
 22%|██████                      | 87/400 [1:13:51<4:30:03, 51.77s/it]2022-01-09 01:52:01,371 iteration 1480 : loss : 0.037675, loss_ce: 0.022042
2022-01-09 01:52:04,109 iteration 1481 : loss : 0.037633, loss_ce: 0.015880
2022-01-09 01:52:06,803 iteration 1482 : loss : 0.048546, loss_ce: 0.022551
2022-01-09 01:52:09,583 iteration 1483 : loss : 0.037997, loss_ce: 0.015828
2022-01-09 01:52:12,373 iteration 1484 : loss : 0.039817, loss_ce: 0.017501
2022-01-09 01:52:14,970 iteration 1485 : loss : 0.033991, loss_ce: 0.012852
2022-01-09 01:52:17,651 iteration 1486 : loss : 0.045593, loss_ce: 0.015449
2022-01-09 01:52:20,534 iteration 1487 : loss : 0.062714, loss_ce: 0.023653
2022-01-09 01:52:23,338 iteration 1488 : loss : 0.033197, loss_ce: 0.011719
2022-01-09 01:52:26,129 iteration 1489 : loss : 0.037196, loss_ce: 0.014181
2022-01-09 01:52:28,921 iteration 1490 : loss : 0.036160, loss_ce: 0.014374
2022-01-09 01:52:31,703 iteration 1491 : loss : 0.053760, loss_ce: 0.017546
2022-01-09 01:52:34,566 iteration 1492 : loss : 0.053717, loss_ce: 0.027188
2022-01-09 01:52:37,469 iteration 1493 : loss : 0.052737, loss_ce: 0.019517
2022-01-09 01:52:40,290 iteration 1494 : loss : 0.045848, loss_ce: 0.014225
2022-01-09 01:52:43,127 iteration 1495 : loss : 0.047391, loss_ce: 0.015676
2022-01-09 01:52:45,962 iteration 1496 : loss : 0.053699, loss_ce: 0.019977
 22%|██████▏                     | 88/400 [1:14:39<4:22:27, 50.47s/it]2022-01-09 01:52:48,853 iteration 1497 : loss : 0.036552, loss_ce: 0.015035
2022-01-09 01:52:51,548 iteration 1498 : loss : 0.035836, loss_ce: 0.013139
2022-01-09 01:52:54,384 iteration 1499 : loss : 0.049089, loss_ce: 0.020044
2022-01-09 01:52:57,324 iteration 1500 : loss : 0.078819, loss_ce: 0.029185
2022-01-09 01:53:00,067 iteration 1501 : loss : 0.028128, loss_ce: 0.008889
2022-01-09 01:53:02,894 iteration 1502 : loss : 0.042375, loss_ce: 0.016225
2022-01-09 01:53:05,751 iteration 1503 : loss : 0.042140, loss_ce: 0.016093
2022-01-09 01:53:08,559 iteration 1504 : loss : 0.044729, loss_ce: 0.017698
2022-01-09 01:53:11,146 iteration 1505 : loss : 0.045727, loss_ce: 0.017675
2022-01-09 01:53:13,981 iteration 1506 : loss : 0.042947, loss_ce: 0.015861
2022-01-09 01:53:16,733 iteration 1507 : loss : 0.040547, loss_ce: 0.017738
2022-01-09 01:53:19,585 iteration 1508 : loss : 0.137598, loss_ce: 0.029555
2022-01-09 01:53:22,196 iteration 1509 : loss : 0.050436, loss_ce: 0.022731
2022-01-09 01:53:24,964 iteration 1510 : loss : 0.046768, loss_ce: 0.017005
2022-01-09 01:53:27,844 iteration 1511 : loss : 0.058531, loss_ce: 0.030807
2022-01-09 01:53:30,647 iteration 1512 : loss : 0.044996, loss_ce: 0.015140
2022-01-09 01:53:33,508 iteration 1513 : loss : 0.043507, loss_ce: 0.016205
 22%|██████▏                     | 89/400 [1:15:26<4:17:03, 49.59s/it]2022-01-09 01:53:36,377 iteration 1514 : loss : 0.061690, loss_ce: 0.022797
2022-01-09 01:53:39,139 iteration 1515 : loss : 0.033926, loss_ce: 0.014408
2022-01-09 01:53:41,854 iteration 1516 : loss : 0.058606, loss_ce: 0.035112
2022-01-09 01:53:44,718 iteration 1517 : loss : 0.044350, loss_ce: 0.020167
2022-01-09 01:53:47,481 iteration 1518 : loss : 0.035408, loss_ce: 0.015312
2022-01-09 01:53:50,319 iteration 1519 : loss : 0.037523, loss_ce: 0.013661
2022-01-09 01:53:53,263 iteration 1520 : loss : 0.069340, loss_ce: 0.029389
2022-01-09 01:53:56,085 iteration 1521 : loss : 0.032006, loss_ce: 0.010942
2022-01-09 01:53:58,945 iteration 1522 : loss : 0.039150, loss_ce: 0.013061
2022-01-09 01:54:01,771 iteration 1523 : loss : 0.037814, loss_ce: 0.014370
2022-01-09 01:54:04,364 iteration 1524 : loss : 0.030485, loss_ce: 0.011125
2022-01-09 01:54:07,108 iteration 1525 : loss : 0.044298, loss_ce: 0.018674
2022-01-09 01:54:09,918 iteration 1526 : loss : 0.067120, loss_ce: 0.025609
2022-01-09 01:54:12,832 iteration 1527 : loss : 0.029026, loss_ce: 0.011192
2022-01-09 01:54:15,629 iteration 1528 : loss : 0.034355, loss_ce: 0.013409
2022-01-09 01:54:18,427 iteration 1529 : loss : 0.052825, loss_ce: 0.014097
2022-01-09 01:54:18,427 Training Data Eval:
2022-01-09 01:54:33,462   Average segmentation loss on training set: 0.0290
2022-01-09 01:54:33,462 Validation Data Eval:
2022-01-09 01:54:38,934   Average segmentation loss on validation set: 0.0963
2022-01-09 01:54:41,784 iteration 1530 : loss : 0.038011, loss_ce: 0.016711
 22%|██████▎                     | 90/400 [1:16:34<4:45:12, 55.20s/it]2022-01-09 01:54:44,860 iteration 1531 : loss : 0.043079, loss_ce: 0.018303
2022-01-09 01:54:47,690 iteration 1532 : loss : 0.066236, loss_ce: 0.020951
2022-01-09 01:54:50,342 iteration 1533 : loss : 0.033547, loss_ce: 0.012831
2022-01-09 01:54:53,183 iteration 1534 : loss : 0.040300, loss_ce: 0.021517
2022-01-09 01:54:55,855 iteration 1535 : loss : 0.041292, loss_ce: 0.016135
2022-01-09 01:54:58,598 iteration 1536 : loss : 0.035228, loss_ce: 0.010815
2022-01-09 01:55:01,399 iteration 1537 : loss : 0.057169, loss_ce: 0.028840
2022-01-09 01:55:04,099 iteration 1538 : loss : 0.059442, loss_ce: 0.015318
2022-01-09 01:55:06,688 iteration 1539 : loss : 0.034494, loss_ce: 0.015365
2022-01-09 01:55:09,415 iteration 1540 : loss : 0.040989, loss_ce: 0.018344
2022-01-09 01:55:12,277 iteration 1541 : loss : 0.040675, loss_ce: 0.014907
2022-01-09 01:55:15,135 iteration 1542 : loss : 0.039078, loss_ce: 0.017149
2022-01-09 01:55:17,949 iteration 1543 : loss : 0.060583, loss_ce: 0.014718
2022-01-09 01:55:20,817 iteration 1544 : loss : 0.037537, loss_ce: 0.016093
2022-01-09 01:55:23,430 iteration 1545 : loss : 0.043911, loss_ce: 0.015825
2022-01-09 01:55:26,216 iteration 1546 : loss : 0.048844, loss_ce: 0.015185
2022-01-09 01:55:29,082 iteration 1547 : loss : 0.045147, loss_ce: 0.019697
 23%|██████▎                     | 91/400 [1:17:22<4:32:04, 52.83s/it]2022-01-09 01:55:31,927 iteration 1548 : loss : 0.046187, loss_ce: 0.013833
2022-01-09 01:55:34,488 iteration 1549 : loss : 0.051201, loss_ce: 0.019997
2022-01-09 01:55:37,366 iteration 1550 : loss : 0.058006, loss_ce: 0.018945
2022-01-09 01:55:40,017 iteration 1551 : loss : 0.047910, loss_ce: 0.015106
2022-01-09 01:55:42,784 iteration 1552 : loss : 0.035369, loss_ce: 0.008182
2022-01-09 01:55:45,648 iteration 1553 : loss : 0.043958, loss_ce: 0.019921
2022-01-09 01:55:48,201 iteration 1554 : loss : 0.064750, loss_ce: 0.032778
2022-01-09 01:55:50,978 iteration 1555 : loss : 0.042597, loss_ce: 0.013469
2022-01-09 01:55:53,868 iteration 1556 : loss : 0.052677, loss_ce: 0.020040
2022-01-09 01:55:56,558 iteration 1557 : loss : 0.054707, loss_ce: 0.018314
2022-01-09 01:55:59,159 iteration 1558 : loss : 0.048011, loss_ce: 0.016809
2022-01-09 01:56:02,022 iteration 1559 : loss : 0.066286, loss_ce: 0.023322
2022-01-09 01:56:04,616 iteration 1560 : loss : 0.044664, loss_ce: 0.021139
2022-01-09 01:56:07,384 iteration 1561 : loss : 0.046858, loss_ce: 0.018043
2022-01-09 01:56:10,147 iteration 1562 : loss : 0.041097, loss_ce: 0.021440
2022-01-09 01:56:12,968 iteration 1563 : loss : 0.043418, loss_ce: 0.016189
2022-01-09 01:56:15,739 iteration 1564 : loss : 0.035300, loss_ce: 0.012437
 23%|██████▍                     | 92/400 [1:18:08<4:21:41, 50.98s/it]2022-01-09 01:56:18,375 iteration 1565 : loss : 0.030331, loss_ce: 0.014720
2022-01-09 01:56:21,229 iteration 1566 : loss : 0.067601, loss_ce: 0.018656
2022-01-09 01:56:23,875 iteration 1567 : loss : 0.044356, loss_ce: 0.016433
2022-01-09 01:56:26,932 iteration 1568 : loss : 0.039366, loss_ce: 0.012686
2022-01-09 01:56:29,562 iteration 1569 : loss : 0.049860, loss_ce: 0.021036
2022-01-09 01:56:32,464 iteration 1570 : loss : 0.041863, loss_ce: 0.012952
2022-01-09 01:56:35,115 iteration 1571 : loss : 0.048553, loss_ce: 0.016050
2022-01-09 01:56:37,977 iteration 1572 : loss : 0.052119, loss_ce: 0.018382
2022-01-09 01:56:40,729 iteration 1573 : loss : 0.035171, loss_ce: 0.016394
2022-01-09 01:56:43,562 iteration 1574 : loss : 0.044086, loss_ce: 0.017376
2022-01-09 01:56:46,434 iteration 1575 : loss : 0.041164, loss_ce: 0.016797
2022-01-09 01:56:49,267 iteration 1576 : loss : 0.032110, loss_ce: 0.013031
2022-01-09 01:56:51,970 iteration 1577 : loss : 0.034134, loss_ce: 0.011149
2022-01-09 01:56:54,671 iteration 1578 : loss : 0.039627, loss_ce: 0.014105
2022-01-09 01:56:57,516 iteration 1579 : loss : 0.073073, loss_ce: 0.029611
2022-01-09 01:57:00,345 iteration 1580 : loss : 0.051660, loss_ce: 0.016582
2022-01-09 01:57:03,030 iteration 1581 : loss : 0.044321, loss_ce: 0.018797
 23%|██████▌                     | 93/400 [1:18:56<4:15:10, 49.87s/it]2022-01-09 01:57:05,918 iteration 1582 : loss : 0.037271, loss_ce: 0.014690
2022-01-09 01:57:08,704 iteration 1583 : loss : 0.033834, loss_ce: 0.013894
2022-01-09 01:57:11,454 iteration 1584 : loss : 0.045091, loss_ce: 0.015740
2022-01-09 01:57:14,151 iteration 1585 : loss : 0.079326, loss_ce: 0.021646
2022-01-09 01:57:17,016 iteration 1586 : loss : 0.049981, loss_ce: 0.013497
2022-01-09 01:57:19,700 iteration 1587 : loss : 0.043363, loss_ce: 0.015285
2022-01-09 01:57:22,524 iteration 1588 : loss : 0.063351, loss_ce: 0.030467
2022-01-09 01:57:25,296 iteration 1589 : loss : 0.043135, loss_ce: 0.011936
2022-01-09 01:57:28,114 iteration 1590 : loss : 0.059958, loss_ce: 0.029894
2022-01-09 01:57:30,907 iteration 1591 : loss : 0.080749, loss_ce: 0.044760
2022-01-09 01:57:33,499 iteration 1592 : loss : 0.064585, loss_ce: 0.025083
2022-01-09 01:57:36,403 iteration 1593 : loss : 0.039767, loss_ce: 0.014138
2022-01-09 01:57:39,201 iteration 1594 : loss : 0.041165, loss_ce: 0.021669
2022-01-09 01:57:42,013 iteration 1595 : loss : 0.060460, loss_ce: 0.024392
2022-01-09 01:57:44,824 iteration 1596 : loss : 0.036043, loss_ce: 0.015056
2022-01-09 01:57:47,414 iteration 1597 : loss : 0.046678, loss_ce: 0.015486
2022-01-09 01:57:50,233 iteration 1598 : loss : 0.041751, loss_ce: 0.019746
 24%|██████▌                     | 94/400 [1:19:43<4:10:15, 49.07s/it]2022-01-09 01:57:53,160 iteration 1599 : loss : 0.027286, loss_ce: 0.010719
2022-01-09 01:57:55,914 iteration 1600 : loss : 0.053708, loss_ce: 0.020528
2022-01-09 01:57:58,754 iteration 1601 : loss : 0.071670, loss_ce: 0.027794
2022-01-09 01:58:01,598 iteration 1602 : loss : 0.057222, loss_ce: 0.027079
2022-01-09 01:58:04,457 iteration 1603 : loss : 0.064132, loss_ce: 0.027782
2022-01-09 01:58:07,260 iteration 1604 : loss : 0.046761, loss_ce: 0.020133
2022-01-09 01:58:10,120 iteration 1605 : loss : 0.056348, loss_ce: 0.021249
2022-01-09 01:58:12,944 iteration 1606 : loss : 0.044717, loss_ce: 0.013761
2022-01-09 01:58:15,749 iteration 1607 : loss : 0.046613, loss_ce: 0.016984
2022-01-09 01:58:18,384 iteration 1608 : loss : 0.031625, loss_ce: 0.011593
2022-01-09 01:58:21,145 iteration 1609 : loss : 0.034096, loss_ce: 0.011945
2022-01-09 01:58:24,024 iteration 1610 : loss : 0.053413, loss_ce: 0.023237
2022-01-09 01:58:26,693 iteration 1611 : loss : 0.060090, loss_ce: 0.024091
2022-01-09 01:58:29,491 iteration 1612 : loss : 0.037074, loss_ce: 0.018083
2022-01-09 01:58:32,315 iteration 1613 : loss : 0.045091, loss_ce: 0.019386
2022-01-09 01:58:34,973 iteration 1614 : loss : 0.060466, loss_ce: 0.020741
2022-01-09 01:58:34,973 Training Data Eval:
2022-01-09 01:58:49,962   Average segmentation loss on training set: 0.0370
2022-01-09 01:58:49,962 Validation Data Eval:
2022-01-09 01:58:55,313   Average segmentation loss on validation set: 0.1100
2022-01-09 01:58:58,163 iteration 1615 : loss : 0.061597, loss_ce: 0.029395
 24%|██████▋                     | 95/400 [1:20:51<4:38:12, 54.73s/it]2022-01-09 01:59:01,085 iteration 1616 : loss : 0.038873, loss_ce: 0.011649
2022-01-09 01:59:03,876 iteration 1617 : loss : 0.031286, loss_ce: 0.014341
2022-01-09 01:59:06,720 iteration 1618 : loss : 0.025866, loss_ce: 0.009016
2022-01-09 01:59:09,534 iteration 1619 : loss : 0.031576, loss_ce: 0.016596
2022-01-09 01:59:12,098 iteration 1620 : loss : 0.039601, loss_ce: 0.016304
2022-01-09 01:59:14,998 iteration 1621 : loss : 0.041880, loss_ce: 0.016189
2022-01-09 01:59:17,799 iteration 1622 : loss : 0.045837, loss_ce: 0.015953
2022-01-09 01:59:20,612 iteration 1623 : loss : 0.029552, loss_ce: 0.014627
2022-01-09 01:59:23,510 iteration 1624 : loss : 0.044458, loss_ce: 0.018010
2022-01-09 01:59:26,437 iteration 1625 : loss : 0.034062, loss_ce: 0.017539
2022-01-09 01:59:29,333 iteration 1626 : loss : 0.037451, loss_ce: 0.015595
2022-01-09 01:59:32,074 iteration 1627 : loss : 0.044915, loss_ce: 0.016410
2022-01-09 01:59:35,061 iteration 1628 : loss : 0.045161, loss_ce: 0.012985
2022-01-09 01:59:37,788 iteration 1629 : loss : 0.046031, loss_ce: 0.018888
2022-01-09 01:59:40,431 iteration 1630 : loss : 0.040555, loss_ce: 0.015581
2022-01-09 01:59:43,225 iteration 1631 : loss : 0.039311, loss_ce: 0.016658
2022-01-09 01:59:45,861 iteration 1632 : loss : 0.032651, loss_ce: 0.013153
 24%|██████▋                     | 96/400 [1:21:39<4:26:36, 52.62s/it]2022-01-09 01:59:48,785 iteration 1633 : loss : 0.043704, loss_ce: 0.016102
2022-01-09 01:59:51,600 iteration 1634 : loss : 0.051288, loss_ce: 0.023963
2022-01-09 01:59:54,305 iteration 1635 : loss : 0.038100, loss_ce: 0.016094
2022-01-09 01:59:57,034 iteration 1636 : loss : 0.036939, loss_ce: 0.014876
2022-01-09 01:59:59,885 iteration 1637 : loss : 0.048711, loss_ce: 0.016553
2022-01-09 02:00:02,796 iteration 1638 : loss : 0.047284, loss_ce: 0.018556
2022-01-09 02:00:05,668 iteration 1639 : loss : 0.032700, loss_ce: 0.014074
2022-01-09 02:00:08,301 iteration 1640 : loss : 0.044949, loss_ce: 0.017255
2022-01-09 02:00:11,107 iteration 1641 : loss : 0.046129, loss_ce: 0.017029
2022-01-09 02:00:13,988 iteration 1642 : loss : 0.066691, loss_ce: 0.023492
2022-01-09 02:00:16,807 iteration 1643 : loss : 0.025231, loss_ce: 0.010806
2022-01-09 02:00:19,662 iteration 1644 : loss : 0.064008, loss_ce: 0.024564
2022-01-09 02:00:22,192 iteration 1645 : loss : 0.038452, loss_ce: 0.012810
2022-01-09 02:00:25,000 iteration 1646 : loss : 0.040346, loss_ce: 0.015557
2022-01-09 02:00:27,848 iteration 1647 : loss : 0.046431, loss_ce: 0.026860
2022-01-09 02:00:30,553 iteration 1648 : loss : 0.049378, loss_ce: 0.023138
2022-01-09 02:00:33,491 iteration 1649 : loss : 0.058024, loss_ce: 0.019635
 24%|██████▊                     | 97/400 [1:22:26<4:18:09, 51.12s/it]2022-01-09 02:00:36,311 iteration 1650 : loss : 0.030010, loss_ce: 0.009366
2022-01-09 02:00:38,958 iteration 1651 : loss : 0.055569, loss_ce: 0.023572
2022-01-09 02:00:41,867 iteration 1652 : loss : 0.054431, loss_ce: 0.019981
2022-01-09 02:00:44,555 iteration 1653 : loss : 0.063163, loss_ce: 0.032362
2022-01-09 02:00:47,337 iteration 1654 : loss : 0.033971, loss_ce: 0.011002
2022-01-09 02:00:49,936 iteration 1655 : loss : 0.044648, loss_ce: 0.014329
2022-01-09 02:00:52,732 iteration 1656 : loss : 0.030765, loss_ce: 0.013211
2022-01-09 02:00:55,616 iteration 1657 : loss : 0.049925, loss_ce: 0.022576
2022-01-09 02:00:58,357 iteration 1658 : loss : 0.032555, loss_ce: 0.012977
2022-01-09 02:01:01,088 iteration 1659 : loss : 0.038882, loss_ce: 0.015238
2022-01-09 02:01:04,069 iteration 1660 : loss : 0.051777, loss_ce: 0.021608
2022-01-09 02:01:06,928 iteration 1661 : loss : 0.045923, loss_ce: 0.015416
2022-01-09 02:01:09,547 iteration 1662 : loss : 0.033081, loss_ce: 0.011984
2022-01-09 02:01:12,459 iteration 1663 : loss : 0.047376, loss_ce: 0.016931
2022-01-09 02:01:15,336 iteration 1664 : loss : 0.050725, loss_ce: 0.017277
2022-01-09 02:01:18,151 iteration 1665 : loss : 0.025895, loss_ce: 0.010822
2022-01-09 02:01:20,789 iteration 1666 : loss : 0.037816, loss_ce: 0.013826
 24%|██████▊                     | 98/400 [1:23:13<4:11:32, 49.98s/it]2022-01-09 02:01:23,696 iteration 1667 : loss : 0.035873, loss_ce: 0.016188
2022-01-09 02:01:26,599 iteration 1668 : loss : 0.052703, loss_ce: 0.021265
2022-01-09 02:01:29,425 iteration 1669 : loss : 0.028489, loss_ce: 0.012168
2022-01-09 02:01:32,237 iteration 1670 : loss : 0.043442, loss_ce: 0.018333
2022-01-09 02:01:34,941 iteration 1671 : loss : 0.034541, loss_ce: 0.013982
2022-01-09 02:01:37,716 iteration 1672 : loss : 0.030512, loss_ce: 0.009429
2022-01-09 02:01:40,540 iteration 1673 : loss : 0.033760, loss_ce: 0.015070
2022-01-09 02:01:43,396 iteration 1674 : loss : 0.038684, loss_ce: 0.017330
2022-01-09 02:01:46,146 iteration 1675 : loss : 0.033839, loss_ce: 0.012330
2022-01-09 02:01:48,992 iteration 1676 : loss : 0.033442, loss_ce: 0.014782
2022-01-09 02:01:51,838 iteration 1677 : loss : 0.037649, loss_ce: 0.016292
2022-01-09 02:01:54,679 iteration 1678 : loss : 0.077080, loss_ce: 0.020392
2022-01-09 02:01:57,524 iteration 1679 : loss : 0.027136, loss_ce: 0.006881
2022-01-09 02:02:00,141 iteration 1680 : loss : 0.048472, loss_ce: 0.030677
2022-01-09 02:02:02,952 iteration 1681 : loss : 0.058870, loss_ce: 0.015559
2022-01-09 02:02:05,610 iteration 1682 : loss : 0.039848, loss_ce: 0.016845
2022-01-09 02:02:08,449 iteration 1683 : loss : 0.052202, loss_ce: 0.023145
 25%|██████▉                     | 99/400 [1:24:01<4:07:13, 49.28s/it]2022-01-09 02:02:11,327 iteration 1684 : loss : 0.034772, loss_ce: 0.011780
2022-01-09 02:02:14,116 iteration 1685 : loss : 0.040141, loss_ce: 0.014003
2022-01-09 02:02:16,774 iteration 1686 : loss : 0.035957, loss_ce: 0.013349
2022-01-09 02:02:19,551 iteration 1687 : loss : 0.030673, loss_ce: 0.009393
2022-01-09 02:02:22,379 iteration 1688 : loss : 0.035471, loss_ce: 0.012555
2022-01-09 02:02:25,172 iteration 1689 : loss : 0.040246, loss_ce: 0.016402
2022-01-09 02:02:27,939 iteration 1690 : loss : 0.036570, loss_ce: 0.011197
2022-01-09 02:02:30,716 iteration 1691 : loss : 0.056039, loss_ce: 0.019479
2022-01-09 02:02:33,576 iteration 1692 : loss : 0.029434, loss_ce: 0.012156
2022-01-09 02:02:36,592 iteration 1693 : loss : 0.036772, loss_ce: 0.012929
2022-01-09 02:02:39,329 iteration 1694 : loss : 0.037764, loss_ce: 0.010172
2022-01-09 02:02:42,175 iteration 1695 : loss : 0.040719, loss_ce: 0.011625
2022-01-09 02:02:45,036 iteration 1696 : loss : 0.041020, loss_ce: 0.019327
2022-01-09 02:02:47,911 iteration 1697 : loss : 0.048314, loss_ce: 0.020451
2022-01-09 02:02:50,723 iteration 1698 : loss : 0.027440, loss_ce: 0.010225
2022-01-09 02:02:53,655 iteration 1699 : loss : 0.038146, loss_ce: 0.016408
2022-01-09 02:02:53,655 Training Data Eval:
2022-01-09 02:03:08,715   Average segmentation loss on training set: 0.0296
2022-01-09 02:03:08,715 Validation Data Eval:
2022-01-09 02:03:14,067   Average segmentation loss on validation set: 0.0911
2022-01-09 02:03:16,874 iteration 1700 : loss : 0.032451, loss_ce: 0.011586
 25%|██████▊                    | 100/400 [1:25:10<4:35:07, 55.02s/it]2022-01-09 02:03:19,722 iteration 1701 : loss : 0.030319, loss_ce: 0.012365
2022-01-09 02:03:22,475 iteration 1702 : loss : 0.035633, loss_ce: 0.013632
2022-01-09 02:03:25,382 iteration 1703 : loss : 0.052239, loss_ce: 0.022211
2022-01-09 02:03:27,999 iteration 1704 : loss : 0.034543, loss_ce: 0.016259
2022-01-09 02:03:30,816 iteration 1705 : loss : 0.041553, loss_ce: 0.014673
2022-01-09 02:03:33,680 iteration 1706 : loss : 0.045880, loss_ce: 0.017843
2022-01-09 02:03:36,364 iteration 1707 : loss : 0.034010, loss_ce: 0.014343
2022-01-09 02:03:39,253 iteration 1708 : loss : 0.055120, loss_ce: 0.016145
2022-01-09 02:03:42,112 iteration 1709 : loss : 0.037203, loss_ce: 0.016126
2022-01-09 02:03:44,828 iteration 1710 : loss : 0.056525, loss_ce: 0.019353
2022-01-09 02:03:47,550 iteration 1711 : loss : 0.048873, loss_ce: 0.018022
2022-01-09 02:03:50,327 iteration 1712 : loss : 0.026871, loss_ce: 0.011372
2022-01-09 02:03:53,008 iteration 1713 : loss : 0.059580, loss_ce: 0.017621
2022-01-09 02:03:55,982 iteration 1714 : loss : 0.038686, loss_ce: 0.010103
2022-01-09 02:03:58,852 iteration 1715 : loss : 0.045757, loss_ce: 0.022782
2022-01-09 02:04:01,637 iteration 1716 : loss : 0.041245, loss_ce: 0.013724
2022-01-09 02:04:04,437 iteration 1717 : loss : 0.063599, loss_ce: 0.021004
 25%|██████▊                    | 101/400 [1:25:57<4:23:01, 52.78s/it]2022-01-09 02:04:07,278 iteration 1718 : loss : 0.048400, loss_ce: 0.026725
2022-01-09 02:04:10,064 iteration 1719 : loss : 0.055630, loss_ce: 0.012148
2022-01-09 02:04:12,924 iteration 1720 : loss : 0.038608, loss_ce: 0.013790
2022-01-09 02:04:15,619 iteration 1721 : loss : 0.038242, loss_ce: 0.013608
2022-01-09 02:04:18,465 iteration 1722 : loss : 0.046515, loss_ce: 0.016047
2022-01-09 02:04:21,074 iteration 1723 : loss : 0.046460, loss_ce: 0.015577
2022-01-09 02:04:23,755 iteration 1724 : loss : 0.032667, loss_ce: 0.011698
2022-01-09 02:04:26,487 iteration 1725 : loss : 0.051780, loss_ce: 0.017510
2022-01-09 02:04:29,327 iteration 1726 : loss : 0.034224, loss_ce: 0.012366
2022-01-09 02:04:32,183 iteration 1727 : loss : 0.046529, loss_ce: 0.020030
2022-01-09 02:04:34,939 iteration 1728 : loss : 0.031239, loss_ce: 0.010678
2022-01-09 02:04:37,757 iteration 1729 : loss : 0.047944, loss_ce: 0.017370
2022-01-09 02:04:40,642 iteration 1730 : loss : 0.040309, loss_ce: 0.018179
2022-01-09 02:04:43,290 iteration 1731 : loss : 0.053084, loss_ce: 0.029222
2022-01-09 02:04:46,110 iteration 1732 : loss : 0.045735, loss_ce: 0.019397
2022-01-09 02:04:48,840 iteration 1733 : loss : 0.040775, loss_ce: 0.019413
2022-01-09 02:04:51,619 iteration 1734 : loss : 0.032674, loss_ce: 0.013655
 26%|██████▉                    | 102/400 [1:26:44<4:13:47, 51.10s/it]2022-01-09 02:04:54,576 iteration 1735 : loss : 0.038222, loss_ce: 0.015024
2022-01-09 02:04:57,367 iteration 1736 : loss : 0.039817, loss_ce: 0.016239
2022-01-09 02:05:00,304 iteration 1737 : loss : 0.050468, loss_ce: 0.022915
2022-01-09 02:05:02,975 iteration 1738 : loss : 0.036106, loss_ce: 0.020048
2022-01-09 02:05:05,766 iteration 1739 : loss : 0.029391, loss_ce: 0.010923
2022-01-09 02:05:08,615 iteration 1740 : loss : 0.031720, loss_ce: 0.013564
2022-01-09 02:05:11,408 iteration 1741 : loss : 0.027253, loss_ce: 0.009528
2022-01-09 02:05:14,295 iteration 1742 : loss : 0.036875, loss_ce: 0.014305
2022-01-09 02:05:16,903 iteration 1743 : loss : 0.035906, loss_ce: 0.011221
2022-01-09 02:05:19,803 iteration 1744 : loss : 0.042040, loss_ce: 0.018151
2022-01-09 02:05:22,696 iteration 1745 : loss : 0.045119, loss_ce: 0.017145
2022-01-09 02:05:25,539 iteration 1746 : loss : 0.045269, loss_ce: 0.021256
2022-01-09 02:05:28,400 iteration 1747 : loss : 0.036783, loss_ce: 0.015801
2022-01-09 02:05:31,199 iteration 1748 : loss : 0.035104, loss_ce: 0.013271
2022-01-09 02:05:33,925 iteration 1749 : loss : 0.027556, loss_ce: 0.008302
2022-01-09 02:05:36,760 iteration 1750 : loss : 0.072392, loss_ce: 0.015237
2022-01-09 02:05:39,825 iteration 1751 : loss : 0.035728, loss_ce: 0.012876
 26%|██████▉                    | 103/400 [1:27:32<4:08:39, 50.23s/it]2022-01-09 02:05:42,812 iteration 1752 : loss : 0.074869, loss_ce: 0.018416
2022-01-09 02:05:45,542 iteration 1753 : loss : 0.043207, loss_ce: 0.012901
2022-01-09 02:05:48,391 iteration 1754 : loss : 0.044503, loss_ce: 0.016630
2022-01-09 02:05:51,209 iteration 1755 : loss : 0.035979, loss_ce: 0.014493
2022-01-09 02:05:54,006 iteration 1756 : loss : 0.048940, loss_ce: 0.018646
2022-01-09 02:05:56,774 iteration 1757 : loss : 0.029316, loss_ce: 0.011715
2022-01-09 02:05:59,606 iteration 1758 : loss : 0.036171, loss_ce: 0.019669
2022-01-09 02:06:02,202 iteration 1759 : loss : 0.037468, loss_ce: 0.010355
2022-01-09 02:06:04,962 iteration 1760 : loss : 0.036628, loss_ce: 0.012405
2022-01-09 02:06:07,816 iteration 1761 : loss : 0.042934, loss_ce: 0.013802
2022-01-09 02:06:10,441 iteration 1762 : loss : 0.027304, loss_ce: 0.011039
2022-01-09 02:06:13,198 iteration 1763 : loss : 0.038014, loss_ce: 0.014176
2022-01-09 02:06:15,733 iteration 1764 : loss : 0.027833, loss_ce: 0.008767
2022-01-09 02:06:18,580 iteration 1765 : loss : 0.028956, loss_ce: 0.015324
2022-01-09 02:06:21,268 iteration 1766 : loss : 0.039699, loss_ce: 0.020017
2022-01-09 02:06:24,032 iteration 1767 : loss : 0.041678, loss_ce: 0.020450
2022-01-09 02:06:26,890 iteration 1768 : loss : 0.043514, loss_ce: 0.016701
 26%|███████                    | 104/400 [1:28:20<4:03:07, 49.28s/it]2022-01-09 02:06:29,728 iteration 1769 : loss : 0.039256, loss_ce: 0.015083
2022-01-09 02:06:32,521 iteration 1770 : loss : 0.028453, loss_ce: 0.010985
2022-01-09 02:06:35,280 iteration 1771 : loss : 0.044352, loss_ce: 0.018013
2022-01-09 02:06:37,958 iteration 1772 : loss : 0.031186, loss_ce: 0.016015
2022-01-09 02:06:40,824 iteration 1773 : loss : 0.068392, loss_ce: 0.025207
2022-01-09 02:06:43,590 iteration 1774 : loss : 0.029163, loss_ce: 0.009193
2022-01-09 02:06:46,417 iteration 1775 : loss : 0.050020, loss_ce: 0.019161
2022-01-09 02:06:49,083 iteration 1776 : loss : 0.028726, loss_ce: 0.013380
2022-01-09 02:06:51,874 iteration 1777 : loss : 0.027557, loss_ce: 0.010389
2022-01-09 02:06:54,648 iteration 1778 : loss : 0.036354, loss_ce: 0.013341
2022-01-09 02:06:57,522 iteration 1779 : loss : 0.032267, loss_ce: 0.011827
2022-01-09 02:07:00,167 iteration 1780 : loss : 0.042635, loss_ce: 0.019808
2022-01-09 02:07:02,895 iteration 1781 : loss : 0.028796, loss_ce: 0.012256
2022-01-09 02:07:05,645 iteration 1782 : loss : 0.037747, loss_ce: 0.014102
2022-01-09 02:07:08,461 iteration 1783 : loss : 0.038592, loss_ce: 0.014658
2022-01-09 02:07:11,072 iteration 1784 : loss : 0.039342, loss_ce: 0.015021
2022-01-09 02:07:11,072 Training Data Eval:
2022-01-09 02:07:26,104   Average segmentation loss on training set: 0.0248
2022-01-09 02:07:26,105 Validation Data Eval:
2022-01-09 02:07:31,527   Average segmentation loss on validation set: 0.0697
2022-01-09 02:07:37,269 Found new lowest validation loss at iteration 1784! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 02:07:39,218 iteration 1785 : loss : 0.031130, loss_ce: 0.011637
 26%|███████                    | 105/400 [1:29:32<4:36:17, 56.20s/it]2022-01-09 02:07:41,740 iteration 1786 : loss : 0.036512, loss_ce: 0.018453
2022-01-09 02:07:44,496 iteration 1787 : loss : 0.035048, loss_ce: 0.013433
2022-01-09 02:07:47,332 iteration 1788 : loss : 0.045775, loss_ce: 0.026024
2022-01-09 02:07:50,052 iteration 1789 : loss : 0.032845, loss_ce: 0.013503
2022-01-09 02:07:52,816 iteration 1790 : loss : 0.036119, loss_ce: 0.016874
2022-01-09 02:07:55,661 iteration 1791 : loss : 0.053349, loss_ce: 0.019849
2022-01-09 02:07:58,400 iteration 1792 : loss : 0.034310, loss_ce: 0.013847
2022-01-09 02:08:01,121 iteration 1793 : loss : 0.040918, loss_ce: 0.014327
2022-01-09 02:08:03,976 iteration 1794 : loss : 0.027261, loss_ce: 0.010890
2022-01-09 02:08:06,686 iteration 1795 : loss : 0.042199, loss_ce: 0.015659
2022-01-09 02:08:09,480 iteration 1796 : loss : 0.053364, loss_ce: 0.026664
2022-01-09 02:08:12,358 iteration 1797 : loss : 0.053507, loss_ce: 0.018676
2022-01-09 02:08:15,248 iteration 1798 : loss : 0.029459, loss_ce: 0.010772
2022-01-09 02:08:18,116 iteration 1799 : loss : 0.034756, loss_ce: 0.014508
2022-01-09 02:08:20,793 iteration 1800 : loss : 0.036241, loss_ce: 0.013631
2022-01-09 02:08:23,475 iteration 1801 : loss : 0.033579, loss_ce: 0.011033
2022-01-09 02:08:26,293 iteration 1802 : loss : 0.042131, loss_ce: 0.014577
 26%|███████▏                   | 106/400 [1:30:19<4:21:56, 53.46s/it]2022-01-09 02:08:29,196 iteration 1803 : loss : 0.036391, loss_ce: 0.014801
2022-01-09 02:08:32,112 iteration 1804 : loss : 0.034044, loss_ce: 0.009183
2022-01-09 02:08:34,743 iteration 1805 : loss : 0.032082, loss_ce: 0.009716
2022-01-09 02:08:37,504 iteration 1806 : loss : 0.035375, loss_ce: 0.015558
2022-01-09 02:08:40,302 iteration 1807 : loss : 0.050734, loss_ce: 0.016529
2022-01-09 02:08:43,079 iteration 1808 : loss : 0.037031, loss_ce: 0.013391
2022-01-09 02:08:45,901 iteration 1809 : loss : 0.025927, loss_ce: 0.010007
2022-01-09 02:08:48,701 iteration 1810 : loss : 0.026500, loss_ce: 0.009481
2022-01-09 02:08:51,594 iteration 1811 : loss : 0.033151, loss_ce: 0.014668
2022-01-09 02:08:54,490 iteration 1812 : loss : 0.031154, loss_ce: 0.011202
2022-01-09 02:08:57,278 iteration 1813 : loss : 0.037393, loss_ce: 0.012859
2022-01-09 02:09:00,113 iteration 1814 : loss : 0.027841, loss_ce: 0.013247
2022-01-09 02:09:02,908 iteration 1815 : loss : 0.035274, loss_ce: 0.009328
2022-01-09 02:09:05,619 iteration 1816 : loss : 0.039087, loss_ce: 0.015542
2022-01-09 02:09:08,349 iteration 1817 : loss : 0.046224, loss_ce: 0.016561
2022-01-09 02:09:11,149 iteration 1818 : loss : 0.063651, loss_ce: 0.023452
2022-01-09 02:09:13,980 iteration 1819 : loss : 0.044196, loss_ce: 0.019761
 27%|███████▏                   | 107/400 [1:31:07<4:12:36, 51.73s/it]2022-01-09 02:09:16,886 iteration 1820 : loss : 0.027373, loss_ce: 0.011023
2022-01-09 02:09:19,488 iteration 1821 : loss : 0.027328, loss_ce: 0.012698
2022-01-09 02:09:22,171 iteration 1822 : loss : 0.031741, loss_ce: 0.011596
2022-01-09 02:09:24,909 iteration 1823 : loss : 0.028159, loss_ce: 0.008614
2022-01-09 02:09:27,622 iteration 1824 : loss : 0.031766, loss_ce: 0.011995
2022-01-09 02:09:30,664 iteration 1825 : loss : 0.038321, loss_ce: 0.015008
2022-01-09 02:09:33,381 iteration 1826 : loss : 0.031808, loss_ce: 0.014540
2022-01-09 02:09:36,151 iteration 1827 : loss : 0.095558, loss_ce: 0.028433
2022-01-09 02:09:38,960 iteration 1828 : loss : 0.032887, loss_ce: 0.011193
2022-01-09 02:09:41,768 iteration 1829 : loss : 0.038723, loss_ce: 0.010903
2022-01-09 02:09:44,488 iteration 1830 : loss : 0.036255, loss_ce: 0.018163
2022-01-09 02:09:47,360 iteration 1831 : loss : 0.071573, loss_ce: 0.051886
2022-01-09 02:09:50,118 iteration 1832 : loss : 0.036544, loss_ce: 0.011747
2022-01-09 02:09:53,049 iteration 1833 : loss : 0.038387, loss_ce: 0.019523
2022-01-09 02:09:55,816 iteration 1834 : loss : 0.047092, loss_ce: 0.020003
2022-01-09 02:09:58,616 iteration 1835 : loss : 0.057259, loss_ce: 0.028086
2022-01-09 02:10:01,464 iteration 1836 : loss : 0.048491, loss_ce: 0.017912
 27%|███████▎                   | 108/400 [1:31:54<4:05:34, 50.46s/it]2022-01-09 02:10:04,382 iteration 1837 : loss : 0.030331, loss_ce: 0.012425
2022-01-09 02:10:07,110 iteration 1838 : loss : 0.049361, loss_ce: 0.021270
2022-01-09 02:10:09,919 iteration 1839 : loss : 0.040662, loss_ce: 0.017236
2022-01-09 02:10:12,630 iteration 1840 : loss : 0.036075, loss_ce: 0.017021
2022-01-09 02:10:15,457 iteration 1841 : loss : 0.055014, loss_ce: 0.022435
2022-01-09 02:10:18,301 iteration 1842 : loss : 0.050890, loss_ce: 0.018435
2022-01-09 02:10:21,131 iteration 1843 : loss : 0.026911, loss_ce: 0.012287
2022-01-09 02:10:23,832 iteration 1844 : loss : 0.040712, loss_ce: 0.014755
2022-01-09 02:10:26,557 iteration 1845 : loss : 0.031078, loss_ce: 0.012541
2022-01-09 02:10:29,437 iteration 1846 : loss : 0.040113, loss_ce: 0.017061
2022-01-09 02:10:32,292 iteration 1847 : loss : 0.043093, loss_ce: 0.014142
2022-01-09 02:10:35,110 iteration 1848 : loss : 0.025611, loss_ce: 0.011222
2022-01-09 02:10:37,861 iteration 1849 : loss : 0.024374, loss_ce: 0.011652
2022-01-09 02:10:40,524 iteration 1850 : loss : 0.047223, loss_ce: 0.013908
2022-01-09 02:10:43,349 iteration 1851 : loss : 0.035050, loss_ce: 0.013057
2022-01-09 02:10:46,156 iteration 1852 : loss : 0.032915, loss_ce: 0.012691
2022-01-09 02:10:48,851 iteration 1853 : loss : 0.040775, loss_ce: 0.008938
 27%|███████▎                   | 109/400 [1:32:42<4:00:15, 49.54s/it]2022-01-09 02:10:51,695 iteration 1854 : loss : 0.031362, loss_ce: 0.013963
2022-01-09 02:10:54,452 iteration 1855 : loss : 0.038682, loss_ce: 0.013336
2022-01-09 02:10:57,248 iteration 1856 : loss : 0.029977, loss_ce: 0.007803
2022-01-09 02:11:00,051 iteration 1857 : loss : 0.055896, loss_ce: 0.020155
2022-01-09 02:11:02,893 iteration 1858 : loss : 0.038737, loss_ce: 0.018965
2022-01-09 02:11:05,624 iteration 1859 : loss : 0.038121, loss_ce: 0.019452
2022-01-09 02:11:08,546 iteration 1860 : loss : 0.050909, loss_ce: 0.022224
2022-01-09 02:11:11,382 iteration 1861 : loss : 0.034791, loss_ce: 0.014405
2022-01-09 02:11:14,220 iteration 1862 : loss : 0.033486, loss_ce: 0.016936
2022-01-09 02:11:17,049 iteration 1863 : loss : 0.041961, loss_ce: 0.018522
2022-01-09 02:11:19,913 iteration 1864 : loss : 0.032308, loss_ce: 0.011882
2022-01-09 02:11:22,794 iteration 1865 : loss : 0.032328, loss_ce: 0.015602
2022-01-09 02:11:25,659 iteration 1866 : loss : 0.029238, loss_ce: 0.013417
2022-01-09 02:11:28,514 iteration 1867 : loss : 0.094217, loss_ce: 0.034474
2022-01-09 02:11:31,122 iteration 1868 : loss : 0.038144, loss_ce: 0.018122
2022-01-09 02:11:33,981 iteration 1869 : loss : 0.037773, loss_ce: 0.014345
2022-01-09 02:11:33,981 Training Data Eval:
2022-01-09 02:11:49,056   Average segmentation loss on training set: 0.0255
2022-01-09 02:11:49,057 Validation Data Eval:
2022-01-09 02:11:54,242   Average segmentation loss on validation set: 0.0720
2022-01-09 02:11:57,024 iteration 1870 : loss : 0.033238, loss_ce: 0.010114
 28%|███████▍                   | 110/400 [1:33:50<4:26:26, 55.13s/it]2022-01-09 02:11:59,850 iteration 1871 : loss : 0.031445, loss_ce: 0.014962
2022-01-09 02:12:02,624 iteration 1872 : loss : 0.037701, loss_ce: 0.012977
2022-01-09 02:12:05,425 iteration 1873 : loss : 0.036997, loss_ce: 0.015665
2022-01-09 02:12:08,254 iteration 1874 : loss : 0.052998, loss_ce: 0.021597
2022-01-09 02:12:11,074 iteration 1875 : loss : 0.035693, loss_ce: 0.020561
2022-01-09 02:12:13,845 iteration 1876 : loss : 0.037571, loss_ce: 0.010949
2022-01-09 02:12:16,661 iteration 1877 : loss : 0.038162, loss_ce: 0.011631
2022-01-09 02:12:19,441 iteration 1878 : loss : 0.038610, loss_ce: 0.013172
2022-01-09 02:12:22,209 iteration 1879 : loss : 0.039327, loss_ce: 0.018614
2022-01-09 02:12:24,883 iteration 1880 : loss : 0.033546, loss_ce: 0.011201
2022-01-09 02:12:27,561 iteration 1881 : loss : 0.023713, loss_ce: 0.007593
2022-01-09 02:12:30,360 iteration 1882 : loss : 0.049717, loss_ce: 0.012877
2022-01-09 02:12:33,162 iteration 1883 : loss : 0.036563, loss_ce: 0.014701
2022-01-09 02:12:36,065 iteration 1884 : loss : 0.027985, loss_ce: 0.010282
2022-01-09 02:12:38,672 iteration 1885 : loss : 0.030610, loss_ce: 0.011760
2022-01-09 02:12:41,489 iteration 1886 : loss : 0.047615, loss_ce: 0.021672
2022-01-09 02:12:44,164 iteration 1887 : loss : 0.043050, loss_ce: 0.016653
 28%|███████▍                   | 111/400 [1:34:37<4:13:59, 52.73s/it]2022-01-09 02:12:47,088 iteration 1888 : loss : 0.033220, loss_ce: 0.011104
2022-01-09 02:12:50,024 iteration 1889 : loss : 0.044247, loss_ce: 0.018172
2022-01-09 02:12:52,774 iteration 1890 : loss : 0.026716, loss_ce: 0.009087
2022-01-09 02:12:55,583 iteration 1891 : loss : 0.044704, loss_ce: 0.024360
2022-01-09 02:12:58,417 iteration 1892 : loss : 0.039288, loss_ce: 0.013702
2022-01-09 02:13:01,292 iteration 1893 : loss : 0.059688, loss_ce: 0.021328
2022-01-09 02:13:04,049 iteration 1894 : loss : 0.039415, loss_ce: 0.016143
2022-01-09 02:13:06,854 iteration 1895 : loss : 0.054882, loss_ce: 0.021813
2022-01-09 02:13:09,714 iteration 1896 : loss : 0.032560, loss_ce: 0.014299
2022-01-09 02:13:12,621 iteration 1897 : loss : 0.047773, loss_ce: 0.017442
2022-01-09 02:13:15,487 iteration 1898 : loss : 0.038791, loss_ce: 0.014462
2022-01-09 02:13:18,244 iteration 1899 : loss : 0.032358, loss_ce: 0.012702
2022-01-09 02:13:21,047 iteration 1900 : loss : 0.041070, loss_ce: 0.018610
2022-01-09 02:13:23,919 iteration 1901 : loss : 0.032485, loss_ce: 0.011411
2022-01-09 02:13:26,737 iteration 1902 : loss : 0.031476, loss_ce: 0.010373
2022-01-09 02:13:29,581 iteration 1903 : loss : 0.036170, loss_ce: 0.015518
2022-01-09 02:13:32,373 iteration 1904 : loss : 0.041995, loss_ce: 0.012880
 28%|███████▌                   | 112/400 [1:35:25<4:06:36, 51.38s/it]2022-01-09 02:13:35,156 iteration 1905 : loss : 0.046494, loss_ce: 0.020021
2022-01-09 02:13:37,966 iteration 1906 : loss : 0.043777, loss_ce: 0.012941
2022-01-09 02:13:40,815 iteration 1907 : loss : 0.030518, loss_ce: 0.012113
2022-01-09 02:13:43,729 iteration 1908 : loss : 0.039765, loss_ce: 0.011486
2022-01-09 02:13:46,575 iteration 1909 : loss : 0.045944, loss_ce: 0.016451
2022-01-09 02:13:49,427 iteration 1910 : loss : 0.082266, loss_ce: 0.018475
2022-01-09 02:13:52,085 iteration 1911 : loss : 0.031626, loss_ce: 0.010756
2022-01-09 02:13:54,900 iteration 1912 : loss : 0.032440, loss_ce: 0.011032
2022-01-09 02:13:57,715 iteration 1913 : loss : 0.044994, loss_ce: 0.019163
2022-01-09 02:14:00,564 iteration 1914 : loss : 0.043369, loss_ce: 0.017201
2022-01-09 02:14:03,350 iteration 1915 : loss : 0.029955, loss_ce: 0.012313
2022-01-09 02:14:05,979 iteration 1916 : loss : 0.048065, loss_ce: 0.019109
2022-01-09 02:14:08,788 iteration 1917 : loss : 0.034121, loss_ce: 0.015533
2022-01-09 02:14:11,618 iteration 1918 : loss : 0.029449, loss_ce: 0.010276
2022-01-09 02:14:14,409 iteration 1919 : loss : 0.030011, loss_ce: 0.011192
2022-01-09 02:14:17,011 iteration 1920 : loss : 0.048481, loss_ce: 0.018833
2022-01-09 02:14:19,934 iteration 1921 : loss : 0.061835, loss_ce: 0.023420
 28%|███████▋                   | 113/400 [1:36:13<4:00:16, 50.23s/it]2022-01-09 02:14:22,755 iteration 1922 : loss : 0.036079, loss_ce: 0.015359
2022-01-09 02:14:25,600 iteration 1923 : loss : 0.027988, loss_ce: 0.012810
2022-01-09 02:14:28,282 iteration 1924 : loss : 0.024700, loss_ce: 0.010664
2022-01-09 02:14:31,102 iteration 1925 : loss : 0.043048, loss_ce: 0.016675
2022-01-09 02:14:33,874 iteration 1926 : loss : 0.027789, loss_ce: 0.011374
2022-01-09 02:14:36,685 iteration 1927 : loss : 0.043626, loss_ce: 0.018891
2022-01-09 02:14:39,590 iteration 1928 : loss : 0.040523, loss_ce: 0.018696
2022-01-09 02:14:42,196 iteration 1929 : loss : 0.035287, loss_ce: 0.013325
2022-01-09 02:14:44,990 iteration 1930 : loss : 0.031527, loss_ce: 0.014895
2022-01-09 02:14:47,834 iteration 1931 : loss : 0.067393, loss_ce: 0.015274
2022-01-09 02:14:50,692 iteration 1932 : loss : 0.051120, loss_ce: 0.016820
2022-01-09 02:14:53,568 iteration 1933 : loss : 0.050207, loss_ce: 0.026241
2022-01-09 02:14:56,364 iteration 1934 : loss : 0.028955, loss_ce: 0.013741
2022-01-09 02:14:58,961 iteration 1935 : loss : 0.054121, loss_ce: 0.019284
2022-01-09 02:15:01,876 iteration 1936 : loss : 0.040070, loss_ce: 0.016777
2022-01-09 02:15:04,499 iteration 1937 : loss : 0.037778, loss_ce: 0.009616
2022-01-09 02:15:07,146 iteration 1938 : loss : 0.033218, loss_ce: 0.012151
 28%|███████▋                   | 114/400 [1:37:00<3:55:06, 49.32s/it]2022-01-09 02:15:09,960 iteration 1939 : loss : 0.027445, loss_ce: 0.008730
2022-01-09 02:15:12,876 iteration 1940 : loss : 0.034845, loss_ce: 0.015154
2022-01-09 02:15:15,740 iteration 1941 : loss : 0.049538, loss_ce: 0.021720
2022-01-09 02:15:18,620 iteration 1942 : loss : 0.041054, loss_ce: 0.014555
2022-01-09 02:15:21,310 iteration 1943 : loss : 0.035557, loss_ce: 0.015623
2022-01-09 02:15:24,149 iteration 1944 : loss : 0.027586, loss_ce: 0.010264
2022-01-09 02:15:26,828 iteration 1945 : loss : 0.037911, loss_ce: 0.013301
2022-01-09 02:15:29,727 iteration 1946 : loss : 0.038228, loss_ce: 0.014868
2022-01-09 02:15:32,570 iteration 1947 : loss : 0.049005, loss_ce: 0.022307
2022-01-09 02:15:35,234 iteration 1948 : loss : 0.053036, loss_ce: 0.016384
2022-01-09 02:15:37,998 iteration 1949 : loss : 0.033925, loss_ce: 0.015772
2022-01-09 02:15:40,718 iteration 1950 : loss : 0.025584, loss_ce: 0.010780
2022-01-09 02:15:43,577 iteration 1951 : loss : 0.049517, loss_ce: 0.017400
2022-01-09 02:15:46,419 iteration 1952 : loss : 0.041843, loss_ce: 0.012431
2022-01-09 02:15:49,287 iteration 1953 : loss : 0.044766, loss_ce: 0.021839
2022-01-09 02:15:52,122 iteration 1954 : loss : 0.023887, loss_ce: 0.009949
2022-01-09 02:15:52,122 Training Data Eval:
2022-01-09 02:16:07,249   Average segmentation loss on training set: 0.0239
2022-01-09 02:16:07,250 Validation Data Eval:
2022-01-09 02:16:12,586   Average segmentation loss on validation set: 0.0835
2022-01-09 02:16:15,329 iteration 1955 : loss : 0.039450, loss_ce: 0.010601
 29%|███████▊                   | 115/400 [1:38:08<4:21:10, 54.98s/it]2022-01-09 02:16:18,150 iteration 1956 : loss : 0.036200, loss_ce: 0.008977
2022-01-09 02:16:21,026 iteration 1957 : loss : 0.061362, loss_ce: 0.027410
2022-01-09 02:16:23,944 iteration 1958 : loss : 0.047363, loss_ce: 0.018714
2022-01-09 02:16:26,621 iteration 1959 : loss : 0.041184, loss_ce: 0.014478
2022-01-09 02:16:29,389 iteration 1960 : loss : 0.038448, loss_ce: 0.020787
2022-01-09 02:16:32,205 iteration 1961 : loss : 0.043378, loss_ce: 0.019073
2022-01-09 02:16:35,108 iteration 1962 : loss : 0.037007, loss_ce: 0.016450
2022-01-09 02:16:37,785 iteration 1963 : loss : 0.030589, loss_ce: 0.011549
2022-01-09 02:16:40,638 iteration 1964 : loss : 0.029036, loss_ce: 0.012051
2022-01-09 02:16:43,376 iteration 1965 : loss : 0.036476, loss_ce: 0.014970
2022-01-09 02:16:46,227 iteration 1966 : loss : 0.047504, loss_ce: 0.019626
2022-01-09 02:16:49,124 iteration 1967 : loss : 0.036346, loss_ce: 0.013159
2022-01-09 02:16:51,984 iteration 1968 : loss : 0.038435, loss_ce: 0.020068
2022-01-09 02:16:54,854 iteration 1969 : loss : 0.042845, loss_ce: 0.016807
2022-01-09 02:16:57,666 iteration 1970 : loss : 0.044317, loss_ce: 0.014349
2022-01-09 02:17:00,391 iteration 1971 : loss : 0.051881, loss_ce: 0.023008
2022-01-09 02:17:03,209 iteration 1972 : loss : 0.061148, loss_ce: 0.022496
 29%|███████▊                   | 116/400 [1:38:56<4:10:09, 52.85s/it]2022-01-09 02:17:05,966 iteration 1973 : loss : 0.025650, loss_ce: 0.010012
2022-01-09 02:17:08,790 iteration 1974 : loss : 0.039040, loss_ce: 0.015536
2022-01-09 02:17:11,617 iteration 1975 : loss : 0.049672, loss_ce: 0.016551
2022-01-09 02:17:14,495 iteration 1976 : loss : 0.046844, loss_ce: 0.023111
2022-01-09 02:17:17,344 iteration 1977 : loss : 0.046614, loss_ce: 0.015022
2022-01-09 02:17:19,903 iteration 1978 : loss : 0.026350, loss_ce: 0.011648
2022-01-09 02:17:22,666 iteration 1979 : loss : 0.034195, loss_ce: 0.013285
2022-01-09 02:17:25,430 iteration 1980 : loss : 0.037337, loss_ce: 0.015780
2022-01-09 02:17:28,179 iteration 1981 : loss : 0.042198, loss_ce: 0.016576
2022-01-09 02:17:30,924 iteration 1982 : loss : 0.056955, loss_ce: 0.016640
2022-01-09 02:17:33,833 iteration 1983 : loss : 0.044073, loss_ce: 0.020605
2022-01-09 02:17:36,443 iteration 1984 : loss : 0.041855, loss_ce: 0.010717
2022-01-09 02:17:39,418 iteration 1985 : loss : 0.027581, loss_ce: 0.010453
2022-01-09 02:17:42,070 iteration 1986 : loss : 0.034276, loss_ce: 0.016863
2022-01-09 02:17:44,972 iteration 1987 : loss : 0.026486, loss_ce: 0.009817
2022-01-09 02:17:47,757 iteration 1988 : loss : 0.042424, loss_ce: 0.011843
2022-01-09 02:17:50,526 iteration 1989 : loss : 0.045332, loss_ce: 0.021227
 29%|███████▉                   | 117/400 [1:39:43<4:01:27, 51.19s/it]2022-01-09 02:17:53,130 iteration 1990 : loss : 0.026851, loss_ce: 0.011313
2022-01-09 02:17:55,913 iteration 1991 : loss : 0.026130, loss_ce: 0.009552
2022-01-09 02:17:58,736 iteration 1992 : loss : 0.037540, loss_ce: 0.013818
2022-01-09 02:18:01,692 iteration 1993 : loss : 0.038302, loss_ce: 0.017994
2022-01-09 02:18:04,309 iteration 1994 : loss : 0.025725, loss_ce: 0.011153
2022-01-09 02:18:07,077 iteration 1995 : loss : 0.054211, loss_ce: 0.017399
2022-01-09 02:18:09,821 iteration 1996 : loss : 0.044549, loss_ce: 0.012918
2022-01-09 02:18:12,690 iteration 1997 : loss : 0.030372, loss_ce: 0.010030
2022-01-09 02:18:15,459 iteration 1998 : loss : 0.042393, loss_ce: 0.014337
2022-01-09 02:18:18,075 iteration 1999 : loss : 0.035247, loss_ce: 0.015051
2022-01-09 02:18:20,923 iteration 2000 : loss : 0.038368, loss_ce: 0.009888
2022-01-09 02:18:23,682 iteration 2001 : loss : 0.034612, loss_ce: 0.014190
2022-01-09 02:18:26,533 iteration 2002 : loss : 0.037455, loss_ce: 0.019206
2022-01-09 02:18:29,318 iteration 2003 : loss : 0.035824, loss_ce: 0.015314
2022-01-09 02:18:32,133 iteration 2004 : loss : 0.041336, loss_ce: 0.013933
2022-01-09 02:18:34,994 iteration 2005 : loss : 0.043653, loss_ce: 0.017793
2022-01-09 02:18:37,693 iteration 2006 : loss : 0.041460, loss_ce: 0.014616
 30%|███████▉                   | 118/400 [1:40:30<3:54:56, 49.99s/it]2022-01-09 02:18:40,558 iteration 2007 : loss : 0.030235, loss_ce: 0.012009
2022-01-09 02:18:43,331 iteration 2008 : loss : 0.043775, loss_ce: 0.017381
2022-01-09 02:18:46,191 iteration 2009 : loss : 0.047692, loss_ce: 0.014644
2022-01-09 02:18:48,920 iteration 2010 : loss : 0.044763, loss_ce: 0.017437
2022-01-09 02:18:51,718 iteration 2011 : loss : 0.038428, loss_ce: 0.016101
2022-01-09 02:18:54,574 iteration 2012 : loss : 0.075332, loss_ce: 0.023851
2022-01-09 02:18:57,232 iteration 2013 : loss : 0.043141, loss_ce: 0.014955
2022-01-09 02:19:00,109 iteration 2014 : loss : 0.032411, loss_ce: 0.012020
2022-01-09 02:19:02,822 iteration 2015 : loss : 0.034347, loss_ce: 0.012781
2022-01-09 02:19:05,661 iteration 2016 : loss : 0.036881, loss_ce: 0.011228
2022-01-09 02:19:08,453 iteration 2017 : loss : 0.027884, loss_ce: 0.010744
2022-01-09 02:19:11,076 iteration 2018 : loss : 0.042122, loss_ce: 0.016540
2022-01-09 02:19:13,742 iteration 2019 : loss : 0.031160, loss_ce: 0.012794
2022-01-09 02:19:16,548 iteration 2020 : loss : 0.038693, loss_ce: 0.016137
2022-01-09 02:19:19,383 iteration 2021 : loss : 0.039265, loss_ce: 0.013592
2022-01-09 02:19:22,229 iteration 2022 : loss : 0.040394, loss_ce: 0.018237
2022-01-09 02:19:24,886 iteration 2023 : loss : 0.043174, loss_ce: 0.014066
 30%|████████                   | 119/400 [1:41:18<3:50:09, 49.14s/it]2022-01-09 02:19:27,761 iteration 2024 : loss : 0.046028, loss_ce: 0.017928
2022-01-09 02:19:30,538 iteration 2025 : loss : 0.030361, loss_ce: 0.010445
2022-01-09 02:19:33,302 iteration 2026 : loss : 0.037077, loss_ce: 0.014459
2022-01-09 02:19:36,160 iteration 2027 : loss : 0.039244, loss_ce: 0.012710
2022-01-09 02:19:39,018 iteration 2028 : loss : 0.040651, loss_ce: 0.014937
2022-01-09 02:19:41,607 iteration 2029 : loss : 0.028185, loss_ce: 0.013402
2022-01-09 02:19:44,427 iteration 2030 : loss : 0.030400, loss_ce: 0.010782
2022-01-09 02:19:47,257 iteration 2031 : loss : 0.031018, loss_ce: 0.010647
2022-01-09 02:19:50,097 iteration 2032 : loss : 0.036517, loss_ce: 0.013397
2022-01-09 02:19:52,708 iteration 2033 : loss : 0.042778, loss_ce: 0.013989
2022-01-09 02:19:55,415 iteration 2034 : loss : 0.025082, loss_ce: 0.008575
2022-01-09 02:19:58,178 iteration 2035 : loss : 0.024310, loss_ce: 0.011426
2022-01-09 02:20:01,035 iteration 2036 : loss : 0.033344, loss_ce: 0.015318
2022-01-09 02:20:03,803 iteration 2037 : loss : 0.036873, loss_ce: 0.011303
2022-01-09 02:20:06,434 iteration 2038 : loss : 0.027010, loss_ce: 0.009278
2022-01-09 02:20:09,226 iteration 2039 : loss : 0.034818, loss_ce: 0.015058
2022-01-09 02:20:09,226 Training Data Eval:
2022-01-09 02:20:24,165   Average segmentation loss on training set: 0.0246
2022-01-09 02:20:24,166 Validation Data Eval:
2022-01-09 02:20:29,381   Average segmentation loss on validation set: 0.0843
2022-01-09 02:20:32,196 iteration 2040 : loss : 0.030309, loss_ce: 0.009458
 30%|████████                   | 120/400 [1:42:25<4:14:46, 54.59s/it]2022-01-09 02:20:34,876 iteration 2041 : loss : 0.033450, loss_ce: 0.015220
2022-01-09 02:20:37,652 iteration 2042 : loss : 0.025940, loss_ce: 0.010997
2022-01-09 02:20:40,494 iteration 2043 : loss : 0.025572, loss_ce: 0.011019
2022-01-09 02:20:43,339 iteration 2044 : loss : 0.025060, loss_ce: 0.010233
2022-01-09 02:20:46,210 iteration 2045 : loss : 0.028131, loss_ce: 0.011157
2022-01-09 02:20:49,091 iteration 2046 : loss : 0.041343, loss_ce: 0.017839
2022-01-09 02:20:51,931 iteration 2047 : loss : 0.031565, loss_ce: 0.010524
2022-01-09 02:20:54,564 iteration 2048 : loss : 0.025668, loss_ce: 0.008744
2022-01-09 02:20:57,538 iteration 2049 : loss : 0.024779, loss_ce: 0.008789
2022-01-09 02:21:00,278 iteration 2050 : loss : 0.048149, loss_ce: 0.015685
2022-01-09 02:21:03,148 iteration 2051 : loss : 0.032877, loss_ce: 0.011146
2022-01-09 02:21:06,066 iteration 2052 : loss : 0.024867, loss_ce: 0.011689
2022-01-09 02:21:08,923 iteration 2053 : loss : 0.027390, loss_ce: 0.008703
2022-01-09 02:21:11,669 iteration 2054 : loss : 0.039770, loss_ce: 0.014158
2022-01-09 02:21:14,488 iteration 2055 : loss : 0.033036, loss_ce: 0.014646
2022-01-09 02:21:17,235 iteration 2056 : loss : 0.031809, loss_ce: 0.013884
2022-01-09 02:21:20,081 iteration 2057 : loss : 0.026335, loss_ce: 0.010762
 30%|████████▏                  | 121/400 [1:43:13<4:04:31, 52.58s/it]2022-01-09 02:21:22,995 iteration 2058 : loss : 0.041593, loss_ce: 0.019457
2022-01-09 02:21:25,826 iteration 2059 : loss : 0.030322, loss_ce: 0.010234
2022-01-09 02:21:28,730 iteration 2060 : loss : 0.044080, loss_ce: 0.015348
2022-01-09 02:21:31,363 iteration 2061 : loss : 0.024987, loss_ce: 0.011648
2022-01-09 02:21:34,143 iteration 2062 : loss : 0.035084, loss_ce: 0.012605
2022-01-09 02:21:36,952 iteration 2063 : loss : 0.021943, loss_ce: 0.009693
2022-01-09 02:21:39,814 iteration 2064 : loss : 0.024823, loss_ce: 0.013447
2022-01-09 02:21:42,675 iteration 2065 : loss : 0.033743, loss_ce: 0.010881
2022-01-09 02:21:45,467 iteration 2066 : loss : 0.038369, loss_ce: 0.012865
2022-01-09 02:21:48,206 iteration 2067 : loss : 0.028014, loss_ce: 0.010119
2022-01-09 02:21:51,255 iteration 2068 : loss : 0.035249, loss_ce: 0.016723
2022-01-09 02:21:54,182 iteration 2069 : loss : 0.045242, loss_ce: 0.011838
2022-01-09 02:21:56,800 iteration 2070 : loss : 0.025376, loss_ce: 0.010304
2022-01-09 02:21:59,563 iteration 2071 : loss : 0.039107, loss_ce: 0.016932
2022-01-09 02:22:02,174 iteration 2072 : loss : 0.030093, loss_ce: 0.013069
2022-01-09 02:22:05,054 iteration 2073 : loss : 0.028055, loss_ce: 0.008887
2022-01-09 02:22:07,909 iteration 2074 : loss : 0.040267, loss_ce: 0.010333
 30%|████████▏                  | 122/400 [1:44:01<3:57:01, 51.16s/it]2022-01-09 02:22:10,732 iteration 2075 : loss : 0.037228, loss_ce: 0.013412
2022-01-09 02:22:13,519 iteration 2076 : loss : 0.061564, loss_ce: 0.025819
2022-01-09 02:22:16,351 iteration 2077 : loss : 0.025999, loss_ce: 0.009058
2022-01-09 02:22:19,022 iteration 2078 : loss : 0.027236, loss_ce: 0.010789
2022-01-09 02:22:21,836 iteration 2079 : loss : 0.026804, loss_ce: 0.009315
2022-01-09 02:22:24,699 iteration 2080 : loss : 0.039855, loss_ce: 0.014524
2022-01-09 02:22:27,576 iteration 2081 : loss : 0.035998, loss_ce: 0.014976
2022-01-09 02:22:30,437 iteration 2082 : loss : 0.029519, loss_ce: 0.011656
2022-01-09 02:22:33,205 iteration 2083 : loss : 0.025061, loss_ce: 0.008238
2022-01-09 02:22:36,137 iteration 2084 : loss : 0.053212, loss_ce: 0.023228
2022-01-09 02:22:38,908 iteration 2085 : loss : 0.030777, loss_ce: 0.012916
2022-01-09 02:22:41,722 iteration 2086 : loss : 0.023069, loss_ce: 0.009779
2022-01-09 02:22:44,336 iteration 2087 : loss : 0.035318, loss_ce: 0.011375
2022-01-09 02:22:47,245 iteration 2088 : loss : 0.050454, loss_ce: 0.024586
2022-01-09 02:22:50,103 iteration 2089 : loss : 0.028946, loss_ce: 0.009949
2022-01-09 02:22:52,970 iteration 2090 : loss : 0.034019, loss_ce: 0.011269
2022-01-09 02:22:55,583 iteration 2091 : loss : 0.032332, loss_ce: 0.013895
 31%|████████▎                  | 123/400 [1:44:48<3:51:20, 50.11s/it]2022-01-09 02:22:58,454 iteration 2092 : loss : 0.036350, loss_ce: 0.012316
2022-01-09 02:23:01,265 iteration 2093 : loss : 0.040987, loss_ce: 0.018744
2022-01-09 02:23:04,108 iteration 2094 : loss : 0.028070, loss_ce: 0.012441
2022-01-09 02:23:06,939 iteration 2095 : loss : 0.032108, loss_ce: 0.012269
2022-01-09 02:23:09,783 iteration 2096 : loss : 0.045561, loss_ce: 0.015443
2022-01-09 02:23:12,375 iteration 2097 : loss : 0.032529, loss_ce: 0.014248
2022-01-09 02:23:15,212 iteration 2098 : loss : 0.039524, loss_ce: 0.013396
2022-01-09 02:23:18,114 iteration 2099 : loss : 0.042996, loss_ce: 0.015836
2022-01-09 02:23:20,959 iteration 2100 : loss : 0.031562, loss_ce: 0.013328
2022-01-09 02:23:23,787 iteration 2101 : loss : 0.049634, loss_ce: 0.013288
2022-01-09 02:23:26,599 iteration 2102 : loss : 0.030355, loss_ce: 0.011089
2022-01-09 02:23:29,499 iteration 2103 : loss : 0.044016, loss_ce: 0.015681
2022-01-09 02:23:32,336 iteration 2104 : loss : 0.026829, loss_ce: 0.010279
2022-01-09 02:23:35,170 iteration 2105 : loss : 0.036048, loss_ce: 0.010629
2022-01-09 02:23:38,042 iteration 2106 : loss : 0.051701, loss_ce: 0.011020
2022-01-09 02:23:40,864 iteration 2107 : loss : 0.032637, loss_ce: 0.012145
2022-01-09 02:23:43,662 iteration 2108 : loss : 0.037489, loss_ce: 0.013869
 31%|████████▎                  | 124/400 [1:45:36<3:47:42, 49.50s/it]2022-01-09 02:23:46,510 iteration 2109 : loss : 0.030512, loss_ce: 0.012445
2022-01-09 02:23:49,286 iteration 2110 : loss : 0.029851, loss_ce: 0.008462
2022-01-09 02:23:52,216 iteration 2111 : loss : 0.038637, loss_ce: 0.012797
2022-01-09 02:23:55,135 iteration 2112 : loss : 0.041227, loss_ce: 0.014178
2022-01-09 02:23:57,920 iteration 2113 : loss : 0.043963, loss_ce: 0.016329
2022-01-09 02:24:00,758 iteration 2114 : loss : 0.041483, loss_ce: 0.013359
2022-01-09 02:24:03,481 iteration 2115 : loss : 0.043800, loss_ce: 0.013041
2022-01-09 02:24:06,305 iteration 2116 : loss : 0.026450, loss_ce: 0.009203
2022-01-09 02:24:09,173 iteration 2117 : loss : 0.031457, loss_ce: 0.011043
2022-01-09 02:24:11,918 iteration 2118 : loss : 0.033140, loss_ce: 0.013369
2022-01-09 02:24:14,760 iteration 2119 : loss : 0.031910, loss_ce: 0.012020
2022-01-09 02:24:17,661 iteration 2120 : loss : 0.040539, loss_ce: 0.015121
2022-01-09 02:24:20,553 iteration 2121 : loss : 0.038523, loss_ce: 0.016262
2022-01-09 02:24:23,370 iteration 2122 : loss : 0.035430, loss_ce: 0.013972
2022-01-09 02:24:26,170 iteration 2123 : loss : 0.035982, loss_ce: 0.018855
2022-01-09 02:24:28,834 iteration 2124 : loss : 0.041918, loss_ce: 0.015190
2022-01-09 02:24:28,834 Training Data Eval:
2022-01-09 02:24:43,653   Average segmentation loss on training set: 0.0255
2022-01-09 02:24:43,653 Validation Data Eval:
2022-01-09 02:24:48,780   Average segmentation loss on validation set: 0.0966
2022-01-09 02:24:51,587 iteration 2125 : loss : 0.023767, loss_ce: 0.007741
 31%|████████▍                  | 125/400 [1:46:44<4:12:13, 55.03s/it]2022-01-09 02:24:54,455 iteration 2126 : loss : 0.025926, loss_ce: 0.008709
2022-01-09 02:24:57,306 iteration 2127 : loss : 0.029348, loss_ce: 0.011430
2022-01-09 02:24:59,890 iteration 2128 : loss : 0.025011, loss_ce: 0.011339
2022-01-09 02:25:02,694 iteration 2129 : loss : 0.041417, loss_ce: 0.015423
2022-01-09 02:25:05,471 iteration 2130 : loss : 0.026728, loss_ce: 0.009100
2022-01-09 02:25:08,213 iteration 2131 : loss : 0.049423, loss_ce: 0.020167
2022-01-09 02:25:11,138 iteration 2132 : loss : 0.040584, loss_ce: 0.014767
2022-01-09 02:25:13,816 iteration 2133 : loss : 0.039544, loss_ce: 0.016608
2022-01-09 02:25:16,670 iteration 2134 : loss : 0.059749, loss_ce: 0.018424
2022-01-09 02:25:19,475 iteration 2135 : loss : 0.043141, loss_ce: 0.013507
2022-01-09 02:25:22,293 iteration 2136 : loss : 0.031041, loss_ce: 0.011723
2022-01-09 02:25:25,137 iteration 2137 : loss : 0.044764, loss_ce: 0.014354
2022-01-09 02:25:27,743 iteration 2138 : loss : 0.031592, loss_ce: 0.010359
2022-01-09 02:25:30,489 iteration 2139 : loss : 0.032182, loss_ce: 0.014701
2022-01-09 02:25:33,095 iteration 2140 : loss : 0.048335, loss_ce: 0.021422
2022-01-09 02:25:35,890 iteration 2141 : loss : 0.039557, loss_ce: 0.015141
2022-01-09 02:25:38,689 iteration 2142 : loss : 0.058360, loss_ce: 0.030333
 32%|████████▌                  | 126/400 [1:47:31<4:00:26, 52.65s/it]2022-01-09 02:25:41,418 iteration 2143 : loss : 0.038787, loss_ce: 0.010547
2022-01-09 02:25:44,212 iteration 2144 : loss : 0.041599, loss_ce: 0.013274
2022-01-09 02:25:46,778 iteration 2145 : loss : 0.028243, loss_ce: 0.015781
2022-01-09 02:25:49,647 iteration 2146 : loss : 0.044364, loss_ce: 0.016988
2022-01-09 02:25:52,286 iteration 2147 : loss : 0.033805, loss_ce: 0.014514
2022-01-09 02:25:55,138 iteration 2148 : loss : 0.052820, loss_ce: 0.020375
2022-01-09 02:25:58,113 iteration 2149 : loss : 0.044898, loss_ce: 0.016850
2022-01-09 02:26:00,981 iteration 2150 : loss : 0.037262, loss_ce: 0.013581
2022-01-09 02:26:03,838 iteration 2151 : loss : 0.038939, loss_ce: 0.015596
2022-01-09 02:26:06,437 iteration 2152 : loss : 0.032776, loss_ce: 0.012235
2022-01-09 02:26:09,179 iteration 2153 : loss : 0.051027, loss_ce: 0.014603
2022-01-09 02:26:11,941 iteration 2154 : loss : 0.027010, loss_ce: 0.012245
2022-01-09 02:26:14,793 iteration 2155 : loss : 0.029716, loss_ce: 0.011898
2022-01-09 02:26:17,611 iteration 2156 : loss : 0.040234, loss_ce: 0.014333
2022-01-09 02:26:20,447 iteration 2157 : loss : 0.033910, loss_ce: 0.012482
2022-01-09 02:26:23,133 iteration 2158 : loss : 0.034755, loss_ce: 0.016803
2022-01-09 02:26:26,019 iteration 2159 : loss : 0.046914, loss_ce: 0.015061
 32%|████████▌                  | 127/400 [1:48:19<3:52:18, 51.06s/it]2022-01-09 02:26:28,899 iteration 2160 : loss : 0.025045, loss_ce: 0.007815
2022-01-09 02:26:31,827 iteration 2161 : loss : 0.032557, loss_ce: 0.013252
2022-01-09 02:26:34,622 iteration 2162 : loss : 0.032968, loss_ce: 0.014651
2022-01-09 02:26:37,362 iteration 2163 : loss : 0.035689, loss_ce: 0.011852
2022-01-09 02:26:40,366 iteration 2164 : loss : 0.029953, loss_ce: 0.012301
2022-01-09 02:26:43,185 iteration 2165 : loss : 0.028025, loss_ce: 0.008773
2022-01-09 02:26:46,028 iteration 2166 : loss : 0.034973, loss_ce: 0.017473
2022-01-09 02:26:48,738 iteration 2167 : loss : 0.029874, loss_ce: 0.016066
2022-01-09 02:26:51,578 iteration 2168 : loss : 0.036377, loss_ce: 0.015272
2022-01-09 02:26:54,391 iteration 2169 : loss : 0.027010, loss_ce: 0.009965
2022-01-09 02:26:57,022 iteration 2170 : loss : 0.031396, loss_ce: 0.011783
2022-01-09 02:26:59,818 iteration 2171 : loss : 0.019779, loss_ce: 0.006098
2022-01-09 02:27:02,361 iteration 2172 : loss : 0.025827, loss_ce: 0.011808
2022-01-09 02:27:05,153 iteration 2173 : loss : 0.040607, loss_ce: 0.015230
2022-01-09 02:27:07,991 iteration 2174 : loss : 0.036634, loss_ce: 0.014488
2022-01-09 02:27:10,574 iteration 2175 : loss : 0.046653, loss_ce: 0.013388
2022-01-09 02:27:13,319 iteration 2176 : loss : 0.035803, loss_ce: 0.017017
 32%|████████▋                  | 128/400 [1:49:06<3:46:19, 49.93s/it]2022-01-09 02:27:16,291 iteration 2177 : loss : 0.049331, loss_ce: 0.014277
2022-01-09 02:27:19,005 iteration 2178 : loss : 0.028680, loss_ce: 0.012744
2022-01-09 02:27:21,623 iteration 2179 : loss : 0.031954, loss_ce: 0.012534
2022-01-09 02:27:24,490 iteration 2180 : loss : 0.027297, loss_ce: 0.007944
2022-01-09 02:27:27,280 iteration 2181 : loss : 0.027241, loss_ce: 0.009527
2022-01-09 02:27:30,129 iteration 2182 : loss : 0.028811, loss_ce: 0.011437
2022-01-09 02:27:32,908 iteration 2183 : loss : 0.044752, loss_ce: 0.012125
2022-01-09 02:27:35,661 iteration 2184 : loss : 0.030182, loss_ce: 0.013851
2022-01-09 02:27:38,414 iteration 2185 : loss : 0.030492, loss_ce: 0.011817
2022-01-09 02:27:41,259 iteration 2186 : loss : 0.046739, loss_ce: 0.018756
2022-01-09 02:27:44,001 iteration 2187 : loss : 0.037832, loss_ce: 0.018208
2022-01-09 02:27:46,935 iteration 2188 : loss : 0.033340, loss_ce: 0.013458
2022-01-09 02:27:49,728 iteration 2189 : loss : 0.035242, loss_ce: 0.013432
2022-01-09 02:27:52,595 iteration 2190 : loss : 0.040141, loss_ce: 0.014233
2022-01-09 02:27:55,184 iteration 2191 : loss : 0.029025, loss_ce: 0.012078
2022-01-09 02:27:57,987 iteration 2192 : loss : 0.054565, loss_ce: 0.016177
2022-01-09 02:28:00,832 iteration 2193 : loss : 0.039724, loss_ce: 0.015398
 32%|████████▋                  | 129/400 [1:49:54<3:42:14, 49.21s/it]2022-01-09 02:28:03,598 iteration 2194 : loss : 0.037745, loss_ce: 0.013564
2022-01-09 02:28:06,248 iteration 2195 : loss : 0.030771, loss_ce: 0.011732
2022-01-09 02:28:09,028 iteration 2196 : loss : 0.036129, loss_ce: 0.015017
2022-01-09 02:28:11,850 iteration 2197 : loss : 0.041815, loss_ce: 0.014790
2022-01-09 02:28:14,692 iteration 2198 : loss : 0.026804, loss_ce: 0.013293
2022-01-09 02:28:17,659 iteration 2199 : loss : 0.047876, loss_ce: 0.020342
2022-01-09 02:28:20,337 iteration 2200 : loss : 0.037794, loss_ce: 0.013959
2022-01-09 02:28:23,154 iteration 2201 : loss : 0.040582, loss_ce: 0.011953
2022-01-09 02:28:25,981 iteration 2202 : loss : 0.096412, loss_ce: 0.035225
2022-01-09 02:28:28,705 iteration 2203 : loss : 0.026998, loss_ce: 0.009102
2022-01-09 02:28:31,550 iteration 2204 : loss : 0.030095, loss_ce: 0.009864
2022-01-09 02:28:34,118 iteration 2205 : loss : 0.027039, loss_ce: 0.011305
2022-01-09 02:28:37,016 iteration 2206 : loss : 0.052154, loss_ce: 0.030836
2022-01-09 02:28:39,791 iteration 2207 : loss : 0.085573, loss_ce: 0.044304
2022-01-09 02:28:42,764 iteration 2208 : loss : 0.053698, loss_ce: 0.017149
2022-01-09 02:28:45,643 iteration 2209 : loss : 0.052763, loss_ce: 0.017021
2022-01-09 02:28:45,643 Training Data Eval:
2022-01-09 02:29:00,610   Average segmentation loss on training set: 0.0752
2022-01-09 02:29:00,610 Validation Data Eval:
2022-01-09 02:29:05,890   Average segmentation loss on validation set: 0.1061
2022-01-09 02:29:08,574 iteration 2210 : loss : 0.039792, loss_ce: 0.015431
 32%|████████▊                  | 130/400 [1:51:01<4:06:26, 54.76s/it]2022-01-09 02:29:11,477 iteration 2211 : loss : 0.032591, loss_ce: 0.015632
2022-01-09 02:29:14,306 iteration 2212 : loss : 0.030152, loss_ce: 0.010791
2022-01-09 02:29:17,056 iteration 2213 : loss : 0.033377, loss_ce: 0.012741
2022-01-09 02:29:19,742 iteration 2214 : loss : 0.032119, loss_ce: 0.014403
2022-01-09 02:29:22,527 iteration 2215 : loss : 0.033578, loss_ce: 0.012288
2022-01-09 02:29:25,146 iteration 2216 : loss : 0.053796, loss_ce: 0.022801
2022-01-09 02:29:27,819 iteration 2217 : loss : 0.037479, loss_ce: 0.016703
2022-01-09 02:29:30,691 iteration 2218 : loss : 0.040885, loss_ce: 0.016683
2022-01-09 02:29:33,309 iteration 2219 : loss : 0.045939, loss_ce: 0.016483
2022-01-09 02:29:35,963 iteration 2220 : loss : 0.027850, loss_ce: 0.014307
2022-01-09 02:29:38,880 iteration 2221 : loss : 0.038182, loss_ce: 0.016337
2022-01-09 02:29:41,485 iteration 2222 : loss : 0.038347, loss_ce: 0.012455
2022-01-09 02:29:44,216 iteration 2223 : loss : 0.060163, loss_ce: 0.020012
2022-01-09 02:29:47,007 iteration 2224 : loss : 0.065970, loss_ce: 0.015489
2022-01-09 02:29:49,787 iteration 2225 : loss : 0.028699, loss_ce: 0.011344
2022-01-09 02:29:52,687 iteration 2226 : loss : 0.039115, loss_ce: 0.015805
2022-01-09 02:29:55,532 iteration 2227 : loss : 0.037198, loss_ce: 0.010716
 33%|████████▊                  | 131/400 [1:51:48<3:55:01, 52.42s/it]2022-01-09 02:29:58,422 iteration 2228 : loss : 0.077431, loss_ce: 0.036329
2022-01-09 02:30:01,263 iteration 2229 : loss : 0.045781, loss_ce: 0.020126
2022-01-09 02:30:04,024 iteration 2230 : loss : 0.038810, loss_ce: 0.015329
2022-01-09 02:30:06,808 iteration 2231 : loss : 0.024604, loss_ce: 0.010717
2022-01-09 02:30:09,754 iteration 2232 : loss : 0.028941, loss_ce: 0.009940
2022-01-09 02:30:12,611 iteration 2233 : loss : 0.035354, loss_ce: 0.013187
2022-01-09 02:30:15,354 iteration 2234 : loss : 0.040119, loss_ce: 0.014727
2022-01-09 02:30:18,203 iteration 2235 : loss : 0.041575, loss_ce: 0.018389
2022-01-09 02:30:20,968 iteration 2236 : loss : 0.024294, loss_ce: 0.009303
2022-01-09 02:30:23,634 iteration 2237 : loss : 0.033623, loss_ce: 0.014066
2022-01-09 02:30:26,444 iteration 2238 : loss : 0.045154, loss_ce: 0.016426
2022-01-09 02:30:29,277 iteration 2239 : loss : 0.036319, loss_ce: 0.012592
2022-01-09 02:30:32,091 iteration 2240 : loss : 0.027352, loss_ce: 0.010811
2022-01-09 02:30:34,905 iteration 2241 : loss : 0.031613, loss_ce: 0.012401
2022-01-09 02:30:37,670 iteration 2242 : loss : 0.031069, loss_ce: 0.013640
2022-01-09 02:30:40,284 iteration 2243 : loss : 0.033276, loss_ce: 0.010752
2022-01-09 02:30:43,139 iteration 2244 : loss : 0.025768, loss_ce: 0.009855
 33%|████████▉                  | 132/400 [1:52:36<3:47:40, 50.97s/it]2022-01-09 02:30:45,791 iteration 2245 : loss : 0.028587, loss_ce: 0.010870
2022-01-09 02:30:48,591 iteration 2246 : loss : 0.063405, loss_ce: 0.026440
2022-01-09 02:30:51,299 iteration 2247 : loss : 0.039260, loss_ce: 0.014843
2022-01-09 02:30:54,056 iteration 2248 : loss : 0.022366, loss_ce: 0.009683
2022-01-09 02:30:56,918 iteration 2249 : loss : 0.036885, loss_ce: 0.014071
2022-01-09 02:30:59,817 iteration 2250 : loss : 0.029959, loss_ce: 0.013691
2022-01-09 02:31:02,647 iteration 2251 : loss : 0.028937, loss_ce: 0.011491
2022-01-09 02:31:05,449 iteration 2252 : loss : 0.037931, loss_ce: 0.013754
2022-01-09 02:31:08,205 iteration 2253 : loss : 0.035742, loss_ce: 0.010857
2022-01-09 02:31:11,057 iteration 2254 : loss : 0.037561, loss_ce: 0.016155
2022-01-09 02:31:13,703 iteration 2255 : loss : 0.031882, loss_ce: 0.015441
2022-01-09 02:31:16,608 iteration 2256 : loss : 0.042694, loss_ce: 0.018308
2022-01-09 02:31:19,343 iteration 2257 : loss : 0.034595, loss_ce: 0.011472
2022-01-09 02:31:22,169 iteration 2258 : loss : 0.043640, loss_ce: 0.016512
2022-01-09 02:31:25,006 iteration 2259 : loss : 0.046711, loss_ce: 0.016321
2022-01-09 02:31:27,847 iteration 2260 : loss : 0.051146, loss_ce: 0.017032
2022-01-09 02:31:30,529 iteration 2261 : loss : 0.036775, loss_ce: 0.008325
 33%|████████▉                  | 133/400 [1:53:23<3:42:03, 49.90s/it]2022-01-09 02:31:33,482 iteration 2262 : loss : 0.036756, loss_ce: 0.015569
2022-01-09 02:31:36,296 iteration 2263 : loss : 0.028557, loss_ce: 0.011056
2022-01-09 02:31:39,140 iteration 2264 : loss : 0.034598, loss_ce: 0.010802
2022-01-09 02:31:41,966 iteration 2265 : loss : 0.026979, loss_ce: 0.010386
2022-01-09 02:31:44,662 iteration 2266 : loss : 0.028722, loss_ce: 0.010654
2022-01-09 02:31:47,502 iteration 2267 : loss : 0.030192, loss_ce: 0.010434
2022-01-09 02:31:50,108 iteration 2268 : loss : 0.027531, loss_ce: 0.010995
2022-01-09 02:31:52,942 iteration 2269 : loss : 0.033168, loss_ce: 0.013737
2022-01-09 02:31:55,610 iteration 2270 : loss : 0.032472, loss_ce: 0.015123
2022-01-09 02:31:58,405 iteration 2271 : loss : 0.046798, loss_ce: 0.029456
2022-01-09 02:32:00,983 iteration 2272 : loss : 0.027988, loss_ce: 0.008230
2022-01-09 02:32:03,886 iteration 2273 : loss : 0.037752, loss_ce: 0.010614
2022-01-09 02:32:06,634 iteration 2274 : loss : 0.030320, loss_ce: 0.010858
2022-01-09 02:32:09,476 iteration 2275 : loss : 0.035457, loss_ce: 0.016891
2022-01-09 02:32:12,104 iteration 2276 : loss : 0.051882, loss_ce: 0.017830
2022-01-09 02:32:14,880 iteration 2277 : loss : 0.031403, loss_ce: 0.013354
2022-01-09 02:32:17,660 iteration 2278 : loss : 0.028258, loss_ce: 0.012168
 34%|█████████                  | 134/400 [1:54:10<3:37:32, 49.07s/it]2022-01-09 02:32:20,652 iteration 2279 : loss : 0.035148, loss_ce: 0.016453
2022-01-09 02:32:23,537 iteration 2280 : loss : 0.046769, loss_ce: 0.024348
2022-01-09 02:32:26,119 iteration 2281 : loss : 0.026933, loss_ce: 0.010112
2022-01-09 02:32:29,077 iteration 2282 : loss : 0.037091, loss_ce: 0.012744
2022-01-09 02:32:31,749 iteration 2283 : loss : 0.040753, loss_ce: 0.014092
2022-01-09 02:32:34,651 iteration 2284 : loss : 0.055122, loss_ce: 0.020918
2022-01-09 02:32:37,471 iteration 2285 : loss : 0.029974, loss_ce: 0.010528
2022-01-09 02:32:40,298 iteration 2286 : loss : 0.021445, loss_ce: 0.007312
2022-01-09 02:32:43,062 iteration 2287 : loss : 0.045713, loss_ce: 0.014546
2022-01-09 02:32:45,871 iteration 2288 : loss : 0.054024, loss_ce: 0.019898
2022-01-09 02:32:48,682 iteration 2289 : loss : 0.031478, loss_ce: 0.010139
2022-01-09 02:32:51,477 iteration 2290 : loss : 0.023578, loss_ce: 0.009211
2022-01-09 02:32:54,349 iteration 2291 : loss : 0.032950, loss_ce: 0.011015
2022-01-09 02:32:57,196 iteration 2292 : loss : 0.027636, loss_ce: 0.009787
2022-01-09 02:32:59,797 iteration 2293 : loss : 0.043110, loss_ce: 0.018895
2022-01-09 02:33:02,557 iteration 2294 : loss : 0.035500, loss_ce: 0.014840
2022-01-09 02:33:02,558 Training Data Eval:
2022-01-09 02:33:17,655   Average segmentation loss on training set: 0.0235
2022-01-09 02:33:17,656 Validation Data Eval:
2022-01-09 02:33:22,947   Average segmentation loss on validation set: 0.0683
2022-01-09 02:33:29,094 Found new lowest validation loss at iteration 2294! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 02:33:31,027 iteration 2295 : loss : 0.032094, loss_ce: 0.011121
 34%|█████████                  | 135/400 [1:55:24<4:08:55, 56.36s/it]2022-01-09 02:33:33,592 iteration 2296 : loss : 0.033698, loss_ce: 0.014579
2022-01-09 02:33:36,362 iteration 2297 : loss : 0.042313, loss_ce: 0.015732
2022-01-09 02:33:39,320 iteration 2298 : loss : 0.030887, loss_ce: 0.011202
2022-01-09 02:33:41,930 iteration 2299 : loss : 0.032480, loss_ce: 0.008583
2022-01-09 02:33:44,854 iteration 2300 : loss : 0.039338, loss_ce: 0.013324
2022-01-09 02:33:47,674 iteration 2301 : loss : 0.041285, loss_ce: 0.018293
2022-01-09 02:33:50,284 iteration 2302 : loss : 0.031918, loss_ce: 0.015419
2022-01-09 02:33:53,078 iteration 2303 : loss : 0.033868, loss_ce: 0.010464
2022-01-09 02:33:55,701 iteration 2304 : loss : 0.041310, loss_ce: 0.016786
2022-01-09 02:33:58,622 iteration 2305 : loss : 0.027510, loss_ce: 0.009844
2022-01-09 02:34:01,338 iteration 2306 : loss : 0.032471, loss_ce: 0.011126
2022-01-09 02:34:03,979 iteration 2307 : loss : 0.024020, loss_ce: 0.008685
2022-01-09 02:34:06,789 iteration 2308 : loss : 0.033142, loss_ce: 0.014964
2022-01-09 02:34:09,569 iteration 2309 : loss : 0.031822, loss_ce: 0.012685
2022-01-09 02:34:12,374 iteration 2310 : loss : 0.034471, loss_ce: 0.015498
2022-01-09 02:34:15,313 iteration 2311 : loss : 0.037080, loss_ce: 0.013703
2022-01-09 02:34:17,958 iteration 2312 : loss : 0.032748, loss_ce: 0.014717
 34%|█████████▏                 | 136/400 [1:56:11<3:55:32, 53.53s/it]2022-01-09 02:34:20,806 iteration 2313 : loss : 0.030560, loss_ce: 0.013752
2022-01-09 02:34:23,569 iteration 2314 : loss : 0.036233, loss_ce: 0.013810
2022-01-09 02:34:26,423 iteration 2315 : loss : 0.022262, loss_ce: 0.007928
2022-01-09 02:34:29,232 iteration 2316 : loss : 0.030239, loss_ce: 0.014816
2022-01-09 02:34:31,973 iteration 2317 : loss : 0.045856, loss_ce: 0.021026
2022-01-09 02:34:34,775 iteration 2318 : loss : 0.024353, loss_ce: 0.011289
2022-01-09 02:34:37,702 iteration 2319 : loss : 0.037849, loss_ce: 0.013705
2022-01-09 02:34:40,395 iteration 2320 : loss : 0.035803, loss_ce: 0.017478
2022-01-09 02:34:43,012 iteration 2321 : loss : 0.030421, loss_ce: 0.009120
2022-01-09 02:34:45,683 iteration 2322 : loss : 0.022575, loss_ce: 0.008817
2022-01-09 02:34:48,620 iteration 2323 : loss : 0.035361, loss_ce: 0.010341
2022-01-09 02:34:51,326 iteration 2324 : loss : 0.026437, loss_ce: 0.008406
2022-01-09 02:34:54,283 iteration 2325 : loss : 0.033390, loss_ce: 0.015070
2022-01-09 02:34:57,093 iteration 2326 : loss : 0.036806, loss_ce: 0.019132
2022-01-09 02:34:59,723 iteration 2327 : loss : 0.024584, loss_ce: 0.008318
2022-01-09 02:35:02,638 iteration 2328 : loss : 0.024933, loss_ce: 0.009007
2022-01-09 02:35:05,278 iteration 2329 : loss : 0.038993, loss_ce: 0.013097
 34%|█████████▏                 | 137/400 [1:56:58<3:46:28, 51.67s/it]2022-01-09 02:35:08,116 iteration 2330 : loss : 0.027318, loss_ce: 0.010496
2022-01-09 02:35:10,783 iteration 2331 : loss : 0.032851, loss_ce: 0.008109
2022-01-09 02:35:13,663 iteration 2332 : loss : 0.054708, loss_ce: 0.022849
2022-01-09 02:35:16,445 iteration 2333 : loss : 0.025258, loss_ce: 0.013237
2022-01-09 02:35:19,062 iteration 2334 : loss : 0.033599, loss_ce: 0.007880
2022-01-09 02:35:21,969 iteration 2335 : loss : 0.035833, loss_ce: 0.015961
2022-01-09 02:35:24,793 iteration 2336 : loss : 0.030513, loss_ce: 0.014432
2022-01-09 02:35:27,641 iteration 2337 : loss : 0.030701, loss_ce: 0.010648
2022-01-09 02:35:30,401 iteration 2338 : loss : 0.029639, loss_ce: 0.010651
2022-01-09 02:35:33,235 iteration 2339 : loss : 0.050819, loss_ce: 0.020529
2022-01-09 02:35:36,068 iteration 2340 : loss : 0.048049, loss_ce: 0.020623
2022-01-09 02:35:38,808 iteration 2341 : loss : 0.039515, loss_ce: 0.015078
2022-01-09 02:35:41,650 iteration 2342 : loss : 0.020554, loss_ce: 0.006576
2022-01-09 02:35:44,552 iteration 2343 : loss : 0.024009, loss_ce: 0.009096
2022-01-09 02:35:47,241 iteration 2344 : loss : 0.023620, loss_ce: 0.009240
2022-01-09 02:35:49,908 iteration 2345 : loss : 0.026038, loss_ce: 0.010348
2022-01-09 02:35:52,734 iteration 2346 : loss : 0.033112, loss_ce: 0.011051
 34%|█████████▎                 | 138/400 [1:57:45<3:40:05, 50.40s/it]2022-01-09 02:35:55,657 iteration 2347 : loss : 0.028460, loss_ce: 0.009579
2022-01-09 02:35:58,292 iteration 2348 : loss : 0.034440, loss_ce: 0.012352
2022-01-09 02:36:01,093 iteration 2349 : loss : 0.034284, loss_ce: 0.016810
2022-01-09 02:36:03,843 iteration 2350 : loss : 0.028825, loss_ce: 0.014608
2022-01-09 02:36:06,687 iteration 2351 : loss : 0.029828, loss_ce: 0.010906
2022-01-09 02:36:09,559 iteration 2352 : loss : 0.037673, loss_ce: 0.017569
2022-01-09 02:36:12,377 iteration 2353 : loss : 0.024684, loss_ce: 0.010508
2022-01-09 02:36:15,206 iteration 2354 : loss : 0.037685, loss_ce: 0.010541
2022-01-09 02:36:17,788 iteration 2355 : loss : 0.025028, loss_ce: 0.012504
2022-01-09 02:36:20,643 iteration 2356 : loss : 0.050994, loss_ce: 0.017812
2022-01-09 02:36:23,573 iteration 2357 : loss : 0.026539, loss_ce: 0.012258
2022-01-09 02:36:26,454 iteration 2358 : loss : 0.062342, loss_ce: 0.027326
2022-01-09 02:36:29,262 iteration 2359 : loss : 0.039106, loss_ce: 0.012390
2022-01-09 02:36:32,004 iteration 2360 : loss : 0.045744, loss_ce: 0.009679
2022-01-09 02:36:34,713 iteration 2361 : loss : 0.030879, loss_ce: 0.011595
2022-01-09 02:36:37,658 iteration 2362 : loss : 0.029867, loss_ce: 0.012716
2022-01-09 02:36:40,273 iteration 2363 : loss : 0.026212, loss_ce: 0.009538
 35%|█████████▍                 | 139/400 [1:58:33<3:35:31, 49.55s/it]2022-01-09 02:36:43,139 iteration 2364 : loss : 0.041002, loss_ce: 0.018260
2022-01-09 02:36:45,920 iteration 2365 : loss : 0.027208, loss_ce: 0.009626
2022-01-09 02:36:48,516 iteration 2366 : loss : 0.029114, loss_ce: 0.013289
2022-01-09 02:36:51,316 iteration 2367 : loss : 0.043061, loss_ce: 0.011589
2022-01-09 02:36:54,200 iteration 2368 : loss : 0.051155, loss_ce: 0.015269
2022-01-09 02:36:56,972 iteration 2369 : loss : 0.038670, loss_ce: 0.014157
2022-01-09 02:36:59,730 iteration 2370 : loss : 0.021676, loss_ce: 0.007107
2022-01-09 02:37:02,408 iteration 2371 : loss : 0.026514, loss_ce: 0.011354
2022-01-09 02:37:05,177 iteration 2372 : loss : 0.029976, loss_ce: 0.015756
2022-01-09 02:37:07,990 iteration 2373 : loss : 0.022911, loss_ce: 0.008531
2022-01-09 02:37:10,727 iteration 2374 : loss : 0.028078, loss_ce: 0.011115
2022-01-09 02:37:13,542 iteration 2375 : loss : 0.029498, loss_ce: 0.009171
2022-01-09 02:37:16,354 iteration 2376 : loss : 0.041916, loss_ce: 0.023302
2022-01-09 02:37:19,296 iteration 2377 : loss : 0.026165, loss_ce: 0.006161
2022-01-09 02:37:22,185 iteration 2378 : loss : 0.035087, loss_ce: 0.018721
2022-01-09 02:37:24,986 iteration 2379 : loss : 0.033885, loss_ce: 0.011027
2022-01-09 02:37:24,986 Training Data Eval:
2022-01-09 02:37:39,988   Average segmentation loss on training set: 0.0240
2022-01-09 02:37:39,988 Validation Data Eval:
2022-01-09 02:37:45,305   Average segmentation loss on validation set: 0.0695
2022-01-09 02:37:48,237 iteration 2380 : loss : 0.025269, loss_ce: 0.008595
 35%|█████████▍                 | 140/400 [1:59:41<3:58:37, 55.07s/it]2022-01-09 02:37:50,949 iteration 2381 : loss : 0.035100, loss_ce: 0.012662
2022-01-09 02:37:53,892 iteration 2382 : loss : 0.031377, loss_ce: 0.014121
2022-01-09 02:37:56,683 iteration 2383 : loss : 0.027707, loss_ce: 0.011252
2022-01-09 02:37:59,489 iteration 2384 : loss : 0.032416, loss_ce: 0.010684
2022-01-09 02:38:02,337 iteration 2385 : loss : 0.032522, loss_ce: 0.015114
2022-01-09 02:38:05,181 iteration 2386 : loss : 0.034866, loss_ce: 0.012394
2022-01-09 02:38:07,974 iteration 2387 : loss : 0.046297, loss_ce: 0.015594
2022-01-09 02:38:10,845 iteration 2388 : loss : 0.023931, loss_ce: 0.010417
2022-01-09 02:38:13,494 iteration 2389 : loss : 0.028914, loss_ce: 0.011076
2022-01-09 02:38:16,491 iteration 2390 : loss : 0.021879, loss_ce: 0.008967
2022-01-09 02:38:19,231 iteration 2391 : loss : 0.029177, loss_ce: 0.010830
2022-01-09 02:38:21,997 iteration 2392 : loss : 0.028244, loss_ce: 0.011709
2022-01-09 02:38:24,786 iteration 2393 : loss : 0.028448, loss_ce: 0.013172
2022-01-09 02:38:27,454 iteration 2394 : loss : 0.044554, loss_ce: 0.010129
2022-01-09 02:38:30,394 iteration 2395 : loss : 0.038514, loss_ce: 0.011976
2022-01-09 02:38:33,260 iteration 2396 : loss : 0.034371, loss_ce: 0.015730
2022-01-09 02:38:36,085 iteration 2397 : loss : 0.039931, loss_ce: 0.012790
 35%|█████████▌                 | 141/400 [2:00:29<3:48:22, 52.90s/it]2022-01-09 02:38:38,891 iteration 2398 : loss : 0.042465, loss_ce: 0.027332
2022-01-09 02:38:41,611 iteration 2399 : loss : 0.025469, loss_ce: 0.009583
2022-01-09 02:38:44,479 iteration 2400 : loss : 0.047120, loss_ce: 0.014921
2022-01-09 02:38:47,107 iteration 2401 : loss : 0.034146, loss_ce: 0.013182
2022-01-09 02:38:49,865 iteration 2402 : loss : 0.035238, loss_ce: 0.016426
2022-01-09 02:38:52,583 iteration 2403 : loss : 0.023088, loss_ce: 0.008217
2022-01-09 02:38:55,498 iteration 2404 : loss : 0.031922, loss_ce: 0.015589
2022-01-09 02:38:58,109 iteration 2405 : loss : 0.029764, loss_ce: 0.010969
2022-01-09 02:39:00,994 iteration 2406 : loss : 0.036903, loss_ce: 0.019054
2022-01-09 02:39:03,778 iteration 2407 : loss : 0.028638, loss_ce: 0.009999
2022-01-09 02:39:06,601 iteration 2408 : loss : 0.028345, loss_ce: 0.012313
2022-01-09 02:39:09,243 iteration 2409 : loss : 0.037852, loss_ce: 0.012982
2022-01-09 02:39:12,148 iteration 2410 : loss : 0.039201, loss_ce: 0.009524
2022-01-09 02:39:14,871 iteration 2411 : loss : 0.029575, loss_ce: 0.008787
2022-01-09 02:39:17,896 iteration 2412 : loss : 0.039737, loss_ce: 0.014570
2022-01-09 02:39:20,837 iteration 2413 : loss : 0.030783, loss_ce: 0.014057
2022-01-09 02:39:23,696 iteration 2414 : loss : 0.048518, loss_ce: 0.022595
 36%|█████████▌                 | 142/400 [2:01:16<3:40:39, 51.31s/it]2022-01-09 02:39:26,407 iteration 2415 : loss : 0.024219, loss_ce: 0.007482
2022-01-09 02:39:29,197 iteration 2416 : loss : 0.028903, loss_ce: 0.012196
2022-01-09 02:39:32,020 iteration 2417 : loss : 0.027136, loss_ce: 0.008630
2022-01-09 02:39:34,870 iteration 2418 : loss : 0.037297, loss_ce: 0.012565
2022-01-09 02:39:37,590 iteration 2419 : loss : 0.027292, loss_ce: 0.013195
2022-01-09 02:39:40,399 iteration 2420 : loss : 0.024926, loss_ce: 0.009583
2022-01-09 02:39:43,329 iteration 2421 : loss : 0.035505, loss_ce: 0.009194
2022-01-09 02:39:46,057 iteration 2422 : loss : 0.044868, loss_ce: 0.016567
2022-01-09 02:39:49,042 iteration 2423 : loss : 0.029145, loss_ce: 0.012744
2022-01-09 02:39:51,760 iteration 2424 : loss : 0.033636, loss_ce: 0.012176
2022-01-09 02:39:54,632 iteration 2425 : loss : 0.036133, loss_ce: 0.020618
2022-01-09 02:39:57,428 iteration 2426 : loss : 0.031073, loss_ce: 0.010751
2022-01-09 02:40:00,336 iteration 2427 : loss : 0.065610, loss_ce: 0.014929
2022-01-09 02:40:03,018 iteration 2428 : loss : 0.035608, loss_ce: 0.014242
2022-01-09 02:40:05,964 iteration 2429 : loss : 0.043315, loss_ce: 0.021687
2022-01-09 02:40:08,812 iteration 2430 : loss : 0.044223, loss_ce: 0.016038
2022-01-09 02:40:11,609 iteration 2431 : loss : 0.025357, loss_ce: 0.012640
 36%|█████████▋                 | 143/400 [2:02:04<3:35:26, 50.30s/it]2022-01-09 02:40:14,377 iteration 2432 : loss : 0.030525, loss_ce: 0.013612
2022-01-09 02:40:17,120 iteration 2433 : loss : 0.044403, loss_ce: 0.017142
2022-01-09 02:40:19,967 iteration 2434 : loss : 0.027925, loss_ce: 0.008784
2022-01-09 02:40:22,878 iteration 2435 : loss : 0.028506, loss_ce: 0.014601
2022-01-09 02:40:25,691 iteration 2436 : loss : 0.026836, loss_ce: 0.010578
2022-01-09 02:40:28,417 iteration 2437 : loss : 0.023245, loss_ce: 0.009673
2022-01-09 02:40:31,322 iteration 2438 : loss : 0.033770, loss_ce: 0.012808
2022-01-09 02:40:34,142 iteration 2439 : loss : 0.038569, loss_ce: 0.014542
2022-01-09 02:40:36,894 iteration 2440 : loss : 0.031347, loss_ce: 0.010055
2022-01-09 02:40:39,758 iteration 2441 : loss : 0.040557, loss_ce: 0.015695
2022-01-09 02:40:42,390 iteration 2442 : loss : 0.030186, loss_ce: 0.010201
2022-01-09 02:40:45,155 iteration 2443 : loss : 0.025624, loss_ce: 0.010640
2022-01-09 02:40:47,944 iteration 2444 : loss : 0.025574, loss_ce: 0.009541
2022-01-09 02:40:50,823 iteration 2445 : loss : 0.038403, loss_ce: 0.008804
2022-01-09 02:40:53,686 iteration 2446 : loss : 0.034946, loss_ce: 0.012158
2022-01-09 02:40:56,661 iteration 2447 : loss : 0.032800, loss_ce: 0.014542
2022-01-09 02:40:59,474 iteration 2448 : loss : 0.024990, loss_ce: 0.008942
 36%|█████████▋                 | 144/400 [2:02:52<3:31:29, 49.57s/it]2022-01-09 02:41:02,332 iteration 2449 : loss : 0.029381, loss_ce: 0.013176
2022-01-09 02:41:05,204 iteration 2450 : loss : 0.032691, loss_ce: 0.010283
2022-01-09 02:41:07,943 iteration 2451 : loss : 0.059978, loss_ce: 0.022915
2022-01-09 02:41:10,693 iteration 2452 : loss : 0.020976, loss_ce: 0.009573
2022-01-09 02:41:13,532 iteration 2453 : loss : 0.045638, loss_ce: 0.013057
2022-01-09 02:41:16,440 iteration 2454 : loss : 0.029656, loss_ce: 0.013126
2022-01-09 02:41:19,317 iteration 2455 : loss : 0.038332, loss_ce: 0.018195
2022-01-09 02:41:22,251 iteration 2456 : loss : 0.041559, loss_ce: 0.018060
2022-01-09 02:41:25,067 iteration 2457 : loss : 0.038627, loss_ce: 0.015721
2022-01-09 02:41:27,976 iteration 2458 : loss : 0.047815, loss_ce: 0.016549
2022-01-09 02:41:30,628 iteration 2459 : loss : 0.050974, loss_ce: 0.014310
2022-01-09 02:41:33,415 iteration 2460 : loss : 0.024782, loss_ce: 0.007373
2022-01-09 02:41:36,229 iteration 2461 : loss : 0.033038, loss_ce: 0.012134
2022-01-09 02:41:39,000 iteration 2462 : loss : 0.022827, loss_ce: 0.009710
2022-01-09 02:41:41,743 iteration 2463 : loss : 0.026068, loss_ce: 0.009007
2022-01-09 02:41:44,611 iteration 2464 : loss : 0.042925, loss_ce: 0.018115
2022-01-09 02:41:44,611 Training Data Eval:
2022-01-09 02:41:59,517   Average segmentation loss on training set: 0.0247
2022-01-09 02:41:59,518 Validation Data Eval:
2022-01-09 02:42:04,689   Average segmentation loss on validation set: 0.1033
2022-01-09 02:42:07,405 iteration 2465 : loss : 0.029023, loss_ce: 0.010664
 36%|█████████▊                 | 145/400 [2:04:00<3:54:03, 55.07s/it]2022-01-09 02:42:10,216 iteration 2466 : loss : 0.024852, loss_ce: 0.009626
2022-01-09 02:42:13,056 iteration 2467 : loss : 0.048359, loss_ce: 0.026300
2022-01-09 02:42:15,857 iteration 2468 : loss : 0.025778, loss_ce: 0.011582
2022-01-09 02:42:18,408 iteration 2469 : loss : 0.017897, loss_ce: 0.008730
2022-01-09 02:42:21,236 iteration 2470 : loss : 0.048396, loss_ce: 0.018638
2022-01-09 02:42:24,075 iteration 2471 : loss : 0.028919, loss_ce: 0.011112
2022-01-09 02:42:27,129 iteration 2472 : loss : 0.037589, loss_ce: 0.012975
2022-01-09 02:42:29,922 iteration 2473 : loss : 0.033595, loss_ce: 0.012200
2022-01-09 02:42:32,791 iteration 2474 : loss : 0.036290, loss_ce: 0.013034
2022-01-09 02:42:35,411 iteration 2475 : loss : 0.021191, loss_ce: 0.009198
2022-01-09 02:42:38,283 iteration 2476 : loss : 0.040786, loss_ce: 0.010043
2022-01-09 02:42:41,075 iteration 2477 : loss : 0.030211, loss_ce: 0.012129
2022-01-09 02:42:43,893 iteration 2478 : loss : 0.026419, loss_ce: 0.007854
2022-01-09 02:42:46,829 iteration 2479 : loss : 0.040500, loss_ce: 0.014590
2022-01-09 02:42:49,556 iteration 2480 : loss : 0.050273, loss_ce: 0.025572
2022-01-09 02:42:52,308 iteration 2481 : loss : 0.030806, loss_ce: 0.010532
2022-01-09 02:42:55,030 iteration 2482 : loss : 0.035564, loss_ce: 0.012361
 36%|█████████▊                 | 146/400 [2:04:48<3:43:41, 52.84s/it]2022-01-09 02:42:57,904 iteration 2483 : loss : 0.024100, loss_ce: 0.008902
2022-01-09 02:43:00,788 iteration 2484 : loss : 0.037977, loss_ce: 0.012677
2022-01-09 02:43:03,468 iteration 2485 : loss : 0.039126, loss_ce: 0.013893
2022-01-09 02:43:06,289 iteration 2486 : loss : 0.023291, loss_ce: 0.010324
2022-01-09 02:43:09,099 iteration 2487 : loss : 0.030602, loss_ce: 0.010535
2022-01-09 02:43:11,961 iteration 2488 : loss : 0.038460, loss_ce: 0.013548
2022-01-09 02:43:14,815 iteration 2489 : loss : 0.029775, loss_ce: 0.012470
2022-01-09 02:43:17,613 iteration 2490 : loss : 0.029012, loss_ce: 0.011734
2022-01-09 02:43:20,438 iteration 2491 : loss : 0.045978, loss_ce: 0.019950
2022-01-09 02:43:23,222 iteration 2492 : loss : 0.036183, loss_ce: 0.012123
2022-01-09 02:43:25,828 iteration 2493 : loss : 0.023719, loss_ce: 0.007606
2022-01-09 02:43:28,584 iteration 2494 : loss : 0.029890, loss_ce: 0.012385
2022-01-09 02:43:31,397 iteration 2495 : loss : 0.030352, loss_ce: 0.009245
2022-01-09 02:43:34,117 iteration 2496 : loss : 0.033233, loss_ce: 0.009598
2022-01-09 02:43:36,940 iteration 2497 : loss : 0.044032, loss_ce: 0.018392
2022-01-09 02:43:39,552 iteration 2498 : loss : 0.037770, loss_ce: 0.019688
2022-01-09 02:43:42,470 iteration 2499 : loss : 0.034981, loss_ce: 0.015129
 37%|█████████▉                 | 147/400 [2:05:35<3:35:57, 51.22s/it]2022-01-09 02:43:45,346 iteration 2500 : loss : 0.035390, loss_ce: 0.011294
2022-01-09 02:43:47,939 iteration 2501 : loss : 0.025300, loss_ce: 0.008942
2022-01-09 02:43:50,751 iteration 2502 : loss : 0.035709, loss_ce: 0.012437
2022-01-09 02:43:53,553 iteration 2503 : loss : 0.028417, loss_ce: 0.013659
2022-01-09 02:43:56,192 iteration 2504 : loss : 0.028763, loss_ce: 0.010995
2022-01-09 02:43:59,025 iteration 2505 : loss : 0.023695, loss_ce: 0.010319
2022-01-09 02:44:01,878 iteration 2506 : loss : 0.048906, loss_ce: 0.012935
2022-01-09 02:44:04,699 iteration 2507 : loss : 0.022902, loss_ce: 0.008485
2022-01-09 02:44:07,572 iteration 2508 : loss : 0.022340, loss_ce: 0.010444
2022-01-09 02:44:10,479 iteration 2509 : loss : 0.026966, loss_ce: 0.011137
2022-01-09 02:44:13,115 iteration 2510 : loss : 0.031383, loss_ce: 0.011659
2022-01-09 02:44:15,763 iteration 2511 : loss : 0.027322, loss_ce: 0.013595
2022-01-09 02:44:18,631 iteration 2512 : loss : 0.039392, loss_ce: 0.013762
2022-01-09 02:44:21,292 iteration 2513 : loss : 0.025831, loss_ce: 0.009652
2022-01-09 02:44:24,131 iteration 2514 : loss : 0.032367, loss_ce: 0.010172
2022-01-09 02:44:26,921 iteration 2515 : loss : 0.056633, loss_ce: 0.030341
2022-01-09 02:44:29,912 iteration 2516 : loss : 0.031896, loss_ce: 0.011964
 37%|█████████▉                 | 148/400 [2:06:23<3:30:21, 50.09s/it]2022-01-09 02:44:32,586 iteration 2517 : loss : 0.031250, loss_ce: 0.010794
2022-01-09 02:44:35,500 iteration 2518 : loss : 0.041995, loss_ce: 0.011606
2022-01-09 02:44:38,074 iteration 2519 : loss : 0.020154, loss_ce: 0.006437
2022-01-09 02:44:40,915 iteration 2520 : loss : 0.036733, loss_ce: 0.016029
2022-01-09 02:44:43,726 iteration 2521 : loss : 0.026271, loss_ce: 0.011612
2022-01-09 02:44:46,336 iteration 2522 : loss : 0.042054, loss_ce: 0.017561
2022-01-09 02:44:49,063 iteration 2523 : loss : 0.035076, loss_ce: 0.010427
2022-01-09 02:44:51,942 iteration 2524 : loss : 0.027395, loss_ce: 0.010570
2022-01-09 02:44:54,820 iteration 2525 : loss : 0.026944, loss_ce: 0.008145
2022-01-09 02:44:57,626 iteration 2526 : loss : 0.028241, loss_ce: 0.011001
2022-01-09 02:45:00,449 iteration 2527 : loss : 0.034223, loss_ce: 0.015804
2022-01-09 02:45:03,114 iteration 2528 : loss : 0.029678, loss_ce: 0.013772
2022-01-09 02:45:06,046 iteration 2529 : loss : 0.056651, loss_ce: 0.021879
2022-01-09 02:45:08,706 iteration 2530 : loss : 0.025289, loss_ce: 0.009239
2022-01-09 02:45:11,582 iteration 2531 : loss : 0.035182, loss_ce: 0.011002
2022-01-09 02:45:14,462 iteration 2532 : loss : 0.036663, loss_ce: 0.016342
2022-01-09 02:45:17,144 iteration 2533 : loss : 0.036214, loss_ce: 0.008373
 37%|██████████                 | 149/400 [2:07:10<3:25:56, 49.23s/it]2022-01-09 02:45:20,023 iteration 2534 : loss : 0.032419, loss_ce: 0.010245
2022-01-09 02:45:22,819 iteration 2535 : loss : 0.037303, loss_ce: 0.014656
2022-01-09 02:45:25,510 iteration 2536 : loss : 0.023454, loss_ce: 0.009061
2022-01-09 02:45:28,397 iteration 2537 : loss : 0.043208, loss_ce: 0.021041
2022-01-09 02:45:31,103 iteration 2538 : loss : 0.030484, loss_ce: 0.010184
2022-01-09 02:45:33,748 iteration 2539 : loss : 0.025586, loss_ce: 0.010252
2022-01-09 02:45:36,569 iteration 2540 : loss : 0.031612, loss_ce: 0.014440
2022-01-09 02:45:39,434 iteration 2541 : loss : 0.040417, loss_ce: 0.014811
2022-01-09 02:45:42,100 iteration 2542 : loss : 0.036521, loss_ce: 0.010735
2022-01-09 02:45:44,992 iteration 2543 : loss : 0.030821, loss_ce: 0.008425
2022-01-09 02:45:47,890 iteration 2544 : loss : 0.059854, loss_ce: 0.025608
2022-01-09 02:45:50,568 iteration 2545 : loss : 0.028815, loss_ce: 0.014199
2022-01-09 02:45:53,439 iteration 2546 : loss : 0.058626, loss_ce: 0.021279
2022-01-09 02:45:56,303 iteration 2547 : loss : 0.052761, loss_ce: 0.016974
2022-01-09 02:45:59,232 iteration 2548 : loss : 0.032491, loss_ce: 0.014685
2022-01-09 02:46:02,084 iteration 2549 : loss : 0.045668, loss_ce: 0.017685
2022-01-09 02:46:02,085 Training Data Eval:
2022-01-09 02:46:17,075   Average segmentation loss on training set: 0.0226
2022-01-09 02:46:17,076 Validation Data Eval:
2022-01-09 02:46:22,396   Average segmentation loss on validation set: 0.0918
2022-01-09 02:46:25,079 iteration 2550 : loss : 0.037670, loss_ce: 0.009452
 38%|██████████▏                | 150/400 [2:08:18<3:48:30, 54.84s/it]2022-01-09 02:46:27,861 iteration 2551 : loss : 0.040609, loss_ce: 0.013596
2022-01-09 02:46:30,675 iteration 2552 : loss : 0.021366, loss_ce: 0.008750
2022-01-09 02:46:33,503 iteration 2553 : loss : 0.034601, loss_ce: 0.013107
2022-01-09 02:46:36,208 iteration 2554 : loss : 0.032621, loss_ce: 0.013463
2022-01-09 02:46:39,029 iteration 2555 : loss : 0.025183, loss_ce: 0.009686
2022-01-09 02:46:41,878 iteration 2556 : loss : 0.041260, loss_ce: 0.016377
2022-01-09 02:46:44,737 iteration 2557 : loss : 0.024752, loss_ce: 0.009550
2022-01-09 02:46:47,333 iteration 2558 : loss : 0.025370, loss_ce: 0.008995
2022-01-09 02:46:50,227 iteration 2559 : loss : 0.037414, loss_ce: 0.016372
2022-01-09 02:46:53,080 iteration 2560 : loss : 0.036881, loss_ce: 0.012686
2022-01-09 02:46:56,072 iteration 2561 : loss : 0.072596, loss_ce: 0.029356
2022-01-09 02:46:58,951 iteration 2562 : loss : 0.025815, loss_ce: 0.010241
2022-01-09 02:47:01,525 iteration 2563 : loss : 0.030685, loss_ce: 0.013147
2022-01-09 02:47:04,294 iteration 2564 : loss : 0.029373, loss_ce: 0.012168
2022-01-09 02:47:07,067 iteration 2565 : loss : 0.020541, loss_ce: 0.007934
2022-01-09 02:47:09,794 iteration 2566 : loss : 0.024559, loss_ce: 0.008915
2022-01-09 02:47:12,663 iteration 2567 : loss : 0.064549, loss_ce: 0.015932
 38%|██████████▏                | 151/400 [2:09:05<3:38:34, 52.67s/it]2022-01-09 02:47:15,401 iteration 2568 : loss : 0.025298, loss_ce: 0.010507
2022-01-09 02:47:18,174 iteration 2569 : loss : 0.036615, loss_ce: 0.013599
2022-01-09 02:47:21,059 iteration 2570 : loss : 0.028911, loss_ce: 0.010361
2022-01-09 02:47:23,867 iteration 2571 : loss : 0.032994, loss_ce: 0.009270
2022-01-09 02:47:26,455 iteration 2572 : loss : 0.027764, loss_ce: 0.010937
2022-01-09 02:47:29,267 iteration 2573 : loss : 0.029206, loss_ce: 0.011478
2022-01-09 02:47:32,056 iteration 2574 : loss : 0.028482, loss_ce: 0.011012
2022-01-09 02:47:34,809 iteration 2575 : loss : 0.025512, loss_ce: 0.008699
2022-01-09 02:47:37,533 iteration 2576 : loss : 0.027506, loss_ce: 0.008197
2022-01-09 02:47:40,531 iteration 2577 : loss : 0.040492, loss_ce: 0.020508
2022-01-09 02:47:43,346 iteration 2578 : loss : 0.027395, loss_ce: 0.009968
2022-01-09 02:47:46,055 iteration 2579 : loss : 0.035761, loss_ce: 0.012203
2022-01-09 02:47:48,711 iteration 2580 : loss : 0.041732, loss_ce: 0.012773
2022-01-09 02:47:51,517 iteration 2581 : loss : 0.030430, loss_ce: 0.013546
2022-01-09 02:47:54,370 iteration 2582 : loss : 0.027575, loss_ce: 0.011480
2022-01-09 02:47:56,975 iteration 2583 : loss : 0.027476, loss_ce: 0.012364
2022-01-09 02:47:59,732 iteration 2584 : loss : 0.024076, loss_ce: 0.007691
 38%|██████████▎                | 152/400 [2:09:52<3:30:44, 50.99s/it]2022-01-09 02:48:02,638 iteration 2585 : loss : 0.037166, loss_ce: 0.021111
2022-01-09 02:48:05,577 iteration 2586 : loss : 0.044050, loss_ce: 0.014010
2022-01-09 02:48:08,442 iteration 2587 : loss : 0.038382, loss_ce: 0.011547
2022-01-09 02:48:11,269 iteration 2588 : loss : 0.031887, loss_ce: 0.011799
2022-01-09 02:48:14,172 iteration 2589 : loss : 0.036066, loss_ce: 0.009808
2022-01-09 02:48:16,956 iteration 2590 : loss : 0.043209, loss_ce: 0.015894
2022-01-09 02:48:19,674 iteration 2591 : loss : 0.021310, loss_ce: 0.008886
2022-01-09 02:48:22,590 iteration 2592 : loss : 0.025034, loss_ce: 0.011916
2022-01-09 02:48:25,322 iteration 2593 : loss : 0.030135, loss_ce: 0.016024
2022-01-09 02:48:27,958 iteration 2594 : loss : 0.063935, loss_ce: 0.029064
2022-01-09 02:48:30,732 iteration 2595 : loss : 0.026672, loss_ce: 0.008465
2022-01-09 02:48:33,537 iteration 2596 : loss : 0.033625, loss_ce: 0.009670
2022-01-09 02:48:36,340 iteration 2597 : loss : 0.024053, loss_ce: 0.008328
2022-01-09 02:48:39,031 iteration 2598 : loss : 0.025908, loss_ce: 0.010806
2022-01-09 02:48:41,874 iteration 2599 : loss : 0.024164, loss_ce: 0.009542
2022-01-09 02:48:44,643 iteration 2600 : loss : 0.028028, loss_ce: 0.011894
2022-01-09 02:48:47,308 iteration 2601 : loss : 0.025547, loss_ce: 0.009665
 38%|██████████▎                | 153/400 [2:10:40<3:25:41, 49.97s/it]2022-01-09 02:48:50,153 iteration 2602 : loss : 0.033310, loss_ce: 0.012642
2022-01-09 02:48:52,883 iteration 2603 : loss : 0.053107, loss_ce: 0.011314
2022-01-09 02:48:55,685 iteration 2604 : loss : 0.021593, loss_ce: 0.008160
2022-01-09 02:48:58,486 iteration 2605 : loss : 0.030289, loss_ce: 0.010494
2022-01-09 02:49:01,114 iteration 2606 : loss : 0.028833, loss_ce: 0.012132
2022-01-09 02:49:03,980 iteration 2607 : loss : 0.031747, loss_ce: 0.013613
2022-01-09 02:49:06,590 iteration 2608 : loss : 0.031958, loss_ce: 0.010497
2022-01-09 02:49:09,459 iteration 2609 : loss : 0.037476, loss_ce: 0.017215
2022-01-09 02:49:12,295 iteration 2610 : loss : 0.024383, loss_ce: 0.008415
2022-01-09 02:49:15,350 iteration 2611 : loss : 0.030843, loss_ce: 0.013085
2022-01-09 02:49:18,238 iteration 2612 : loss : 0.030590, loss_ce: 0.010055
2022-01-09 02:49:20,954 iteration 2613 : loss : 0.033511, loss_ce: 0.013962
2022-01-09 02:49:23,870 iteration 2614 : loss : 0.027894, loss_ce: 0.011838
2022-01-09 02:49:26,740 iteration 2615 : loss : 0.029284, loss_ce: 0.013152
2022-01-09 02:49:29,558 iteration 2616 : loss : 0.029715, loss_ce: 0.011124
2022-01-09 02:49:32,302 iteration 2617 : loss : 0.027447, loss_ce: 0.010229
2022-01-09 02:49:35,127 iteration 2618 : loss : 0.029994, loss_ce: 0.009320
 38%|██████████▍                | 154/400 [2:11:28<3:22:12, 49.32s/it]2022-01-09 02:49:38,131 iteration 2619 : loss : 0.038158, loss_ce: 0.015901
2022-01-09 02:49:40,952 iteration 2620 : loss : 0.025818, loss_ce: 0.008620
2022-01-09 02:49:43,742 iteration 2621 : loss : 0.027421, loss_ce: 0.011457
2022-01-09 02:49:46,601 iteration 2622 : loss : 0.046566, loss_ce: 0.014971
2022-01-09 02:49:49,372 iteration 2623 : loss : 0.022204, loss_ce: 0.008642
2022-01-09 02:49:52,412 iteration 2624 : loss : 0.036818, loss_ce: 0.016019
2022-01-09 02:49:55,268 iteration 2625 : loss : 0.032903, loss_ce: 0.014791
2022-01-09 02:49:58,094 iteration 2626 : loss : 0.025909, loss_ce: 0.010288
2022-01-09 02:50:00,690 iteration 2627 : loss : 0.025077, loss_ce: 0.014677
2022-01-09 02:50:03,377 iteration 2628 : loss : 0.023987, loss_ce: 0.008825
2022-01-09 02:50:06,041 iteration 2629 : loss : 0.029025, loss_ce: 0.007245
2022-01-09 02:50:08,941 iteration 2630 : loss : 0.051083, loss_ce: 0.008349
2022-01-09 02:50:11,752 iteration 2631 : loss : 0.033530, loss_ce: 0.017201
2022-01-09 02:50:14,408 iteration 2632 : loss : 0.025897, loss_ce: 0.008773
2022-01-09 02:50:17,230 iteration 2633 : loss : 0.018199, loss_ce: 0.005936
2022-01-09 02:50:19,912 iteration 2634 : loss : 0.027267, loss_ce: 0.008526
2022-01-09 02:50:19,913 Training Data Eval:
2022-01-09 02:50:35,193   Average segmentation loss on training set: 0.0220
2022-01-09 02:50:35,193 Validation Data Eval:
2022-01-09 02:50:40,362   Average segmentation loss on validation set: 0.0647
2022-01-09 02:50:46,133 Found new lowest validation loss at iteration 2634! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 02:50:48,054 iteration 2635 : loss : 0.027285, loss_ce: 0.010252
 39%|██████████▍                | 155/400 [2:12:41<3:50:17, 56.40s/it]2022-01-09 02:50:50,712 iteration 2636 : loss : 0.024909, loss_ce: 0.007755
2022-01-09 02:50:53,476 iteration 2637 : loss : 0.025206, loss_ce: 0.010790
2022-01-09 02:50:56,394 iteration 2638 : loss : 0.030690, loss_ce: 0.010083
2022-01-09 02:50:59,041 iteration 2639 : loss : 0.021616, loss_ce: 0.008146
2022-01-09 02:51:01,846 iteration 2640 : loss : 0.039580, loss_ce: 0.014231
2022-01-09 02:51:04,645 iteration 2641 : loss : 0.031746, loss_ce: 0.012644
2022-01-09 02:51:07,453 iteration 2642 : loss : 0.027302, loss_ce: 0.008014
2022-01-09 02:51:10,091 iteration 2643 : loss : 0.030107, loss_ce: 0.012527
2022-01-09 02:51:12,909 iteration 2644 : loss : 0.037981, loss_ce: 0.021654
2022-01-09 02:51:15,519 iteration 2645 : loss : 0.025245, loss_ce: 0.010182
2022-01-09 02:51:18,357 iteration 2646 : loss : 0.027880, loss_ce: 0.007806
2022-01-09 02:51:21,231 iteration 2647 : loss : 0.031462, loss_ce: 0.014571
2022-01-09 02:51:23,857 iteration 2648 : loss : 0.031333, loss_ce: 0.010779
2022-01-09 02:51:26,687 iteration 2649 : loss : 0.039725, loss_ce: 0.013009
2022-01-09 02:51:29,563 iteration 2650 : loss : 0.036546, loss_ce: 0.014059
2022-01-09 02:51:32,496 iteration 2651 : loss : 0.029979, loss_ce: 0.011470
2022-01-09 02:51:35,319 iteration 2652 : loss : 0.027268, loss_ce: 0.010999
 39%|██████████▌                | 156/400 [2:13:28<3:38:12, 53.66s/it]2022-01-09 02:51:38,248 iteration 2653 : loss : 0.044449, loss_ce: 0.017050
2022-01-09 02:51:40,890 iteration 2654 : loss : 0.023965, loss_ce: 0.010441
2022-01-09 02:51:43,750 iteration 2655 : loss : 0.026495, loss_ce: 0.011629
2022-01-09 02:51:46,504 iteration 2656 : loss : 0.027468, loss_ce: 0.014379
2022-01-09 02:51:49,283 iteration 2657 : loss : 0.023144, loss_ce: 0.009651
2022-01-09 02:51:52,178 iteration 2658 : loss : 0.047455, loss_ce: 0.016504
2022-01-09 02:51:55,022 iteration 2659 : loss : 0.030382, loss_ce: 0.012239
2022-01-09 02:51:57,906 iteration 2660 : loss : 0.032655, loss_ce: 0.014297
2022-01-09 02:52:00,478 iteration 2661 : loss : 0.023038, loss_ce: 0.007224
2022-01-09 02:52:03,413 iteration 2662 : loss : 0.026000, loss_ce: 0.009802
2022-01-09 02:52:06,246 iteration 2663 : loss : 0.039085, loss_ce: 0.015275
2022-01-09 02:52:08,841 iteration 2664 : loss : 0.034009, loss_ce: 0.012047
2022-01-09 02:52:11,640 iteration 2665 : loss : 0.017030, loss_ce: 0.006671
2022-01-09 02:52:14,501 iteration 2666 : loss : 0.029467, loss_ce: 0.009928
2022-01-09 02:52:17,308 iteration 2667 : loss : 0.031985, loss_ce: 0.011678
2022-01-09 02:52:20,001 iteration 2668 : loss : 0.031015, loss_ce: 0.012590
2022-01-09 02:52:22,817 iteration 2669 : loss : 0.043991, loss_ce: 0.012902
 39%|██████████▌                | 157/400 [2:14:15<3:29:50, 51.81s/it]2022-01-09 02:52:25,588 iteration 2670 : loss : 0.022111, loss_ce: 0.008494
2022-01-09 02:52:28,287 iteration 2671 : loss : 0.022246, loss_ce: 0.008443
2022-01-09 02:52:30,971 iteration 2672 : loss : 0.029884, loss_ce: 0.009168
2022-01-09 02:52:33,768 iteration 2673 : loss : 0.024780, loss_ce: 0.007381
2022-01-09 02:52:36,499 iteration 2674 : loss : 0.028722, loss_ce: 0.009901
2022-01-09 02:52:39,335 iteration 2675 : loss : 0.025451, loss_ce: 0.008380
2022-01-09 02:52:42,132 iteration 2676 : loss : 0.022526, loss_ce: 0.007498
2022-01-09 02:52:44,739 iteration 2677 : loss : 0.021917, loss_ce: 0.008492
2022-01-09 02:52:47,300 iteration 2678 : loss : 0.024207, loss_ce: 0.010732
2022-01-09 02:52:50,008 iteration 2679 : loss : 0.030702, loss_ce: 0.012494
2022-01-09 02:52:53,043 iteration 2680 : loss : 0.023786, loss_ce: 0.009020
2022-01-09 02:52:55,717 iteration 2681 : loss : 0.033192, loss_ce: 0.010406
2022-01-09 02:52:58,541 iteration 2682 : loss : 0.028150, loss_ce: 0.011672
2022-01-09 02:53:01,140 iteration 2683 : loss : 0.024286, loss_ce: 0.010578
2022-01-09 02:53:03,990 iteration 2684 : loss : 0.026871, loss_ce: 0.012171
2022-01-09 02:53:06,811 iteration 2685 : loss : 0.031050, loss_ce: 0.012653
2022-01-09 02:53:09,653 iteration 2686 : loss : 0.053583, loss_ce: 0.011234
 40%|██████████▋                | 158/400 [2:15:02<3:22:57, 50.32s/it]2022-01-09 02:53:12,488 iteration 2687 : loss : 0.031184, loss_ce: 0.013437
2022-01-09 02:53:15,182 iteration 2688 : loss : 0.026637, loss_ce: 0.009412
2022-01-09 02:53:17,851 iteration 2689 : loss : 0.029478, loss_ce: 0.009371
2022-01-09 02:53:20,692 iteration 2690 : loss : 0.054518, loss_ce: 0.011438
2022-01-09 02:53:23,513 iteration 2691 : loss : 0.019797, loss_ce: 0.008015
2022-01-09 02:53:26,102 iteration 2692 : loss : 0.048176, loss_ce: 0.019034
2022-01-09 02:53:28,854 iteration 2693 : loss : 0.021934, loss_ce: 0.008273
2022-01-09 02:53:31,705 iteration 2694 : loss : 0.031074, loss_ce: 0.012626
2022-01-09 02:53:34,364 iteration 2695 : loss : 0.027730, loss_ce: 0.013291
2022-01-09 02:53:36,983 iteration 2696 : loss : 0.029613, loss_ce: 0.013454
2022-01-09 02:53:39,734 iteration 2697 : loss : 0.033059, loss_ce: 0.015725
2022-01-09 02:53:42,414 iteration 2698 : loss : 0.029196, loss_ce: 0.009513
2022-01-09 02:53:45,229 iteration 2699 : loss : 0.032010, loss_ce: 0.013304
2022-01-09 02:53:47,812 iteration 2700 : loss : 0.030265, loss_ce: 0.009286
2022-01-09 02:53:50,728 iteration 2701 : loss : 0.033665, loss_ce: 0.010956
2022-01-09 02:53:53,550 iteration 2702 : loss : 0.026422, loss_ce: 0.013141
2022-01-09 02:53:56,207 iteration 2703 : loss : 0.031217, loss_ce: 0.012461
 40%|██████████▋                | 159/400 [2:15:49<3:17:34, 49.19s/it]2022-01-09 02:53:59,148 iteration 2704 : loss : 0.060931, loss_ce: 0.021284
2022-01-09 02:54:01,927 iteration 2705 : loss : 0.023937, loss_ce: 0.010353
2022-01-09 02:54:04,633 iteration 2706 : loss : 0.052223, loss_ce: 0.019101
2022-01-09 02:54:07,494 iteration 2707 : loss : 0.024971, loss_ce: 0.012839
2022-01-09 02:54:10,388 iteration 2708 : loss : 0.021752, loss_ce: 0.008603
2022-01-09 02:54:13,126 iteration 2709 : loss : 0.020686, loss_ce: 0.008612
2022-01-09 02:54:15,869 iteration 2710 : loss : 0.023623, loss_ce: 0.010213
2022-01-09 02:54:18,691 iteration 2711 : loss : 0.027255, loss_ce: 0.009915
2022-01-09 02:54:21,504 iteration 2712 : loss : 0.033202, loss_ce: 0.008876
2022-01-09 02:54:24,180 iteration 2713 : loss : 0.022486, loss_ce: 0.011332
2022-01-09 02:54:27,053 iteration 2714 : loss : 0.024040, loss_ce: 0.008739
2022-01-09 02:54:29,941 iteration 2715 : loss : 0.042621, loss_ce: 0.015098
2022-01-09 02:54:32,651 iteration 2716 : loss : 0.031232, loss_ce: 0.011510
2022-01-09 02:54:35,532 iteration 2717 : loss : 0.030063, loss_ce: 0.012594
2022-01-09 02:54:38,445 iteration 2718 : loss : 0.037315, loss_ce: 0.012912
2022-01-09 02:54:41,195 iteration 2719 : loss : 0.032069, loss_ce: 0.011473
2022-01-09 02:54:41,195 Training Data Eval:
2022-01-09 02:54:56,236   Average segmentation loss on training set: 0.0243
2022-01-09 02:54:56,236 Validation Data Eval:
2022-01-09 02:55:01,456   Average segmentation loss on validation set: 0.0893
2022-01-09 02:55:04,054 iteration 2720 : loss : 0.025926, loss_ce: 0.009100
 40%|██████████▊                | 160/400 [2:16:57<3:39:08, 54.78s/it]2022-01-09 02:55:06,931 iteration 2721 : loss : 0.033079, loss_ce: 0.013637
2022-01-09 02:55:09,783 iteration 2722 : loss : 0.045084, loss_ce: 0.021626
2022-01-09 02:55:12,639 iteration 2723 : loss : 0.030142, loss_ce: 0.012103
2022-01-09 02:55:15,349 iteration 2724 : loss : 0.037681, loss_ce: 0.011829
2022-01-09 02:55:18,179 iteration 2725 : loss : 0.031546, loss_ce: 0.009602
2022-01-09 02:55:21,097 iteration 2726 : loss : 0.033243, loss_ce: 0.011649
2022-01-09 02:55:23,935 iteration 2727 : loss : 0.032174, loss_ce: 0.014915
2022-01-09 02:55:26,805 iteration 2728 : loss : 0.020965, loss_ce: 0.007715
2022-01-09 02:55:29,441 iteration 2729 : loss : 0.029557, loss_ce: 0.013952
2022-01-09 02:55:32,207 iteration 2730 : loss : 0.024671, loss_ce: 0.009131
2022-01-09 02:55:35,039 iteration 2731 : loss : 0.027442, loss_ce: 0.009869
2022-01-09 02:55:37,776 iteration 2732 : loss : 0.031833, loss_ce: 0.013307
2022-01-09 02:55:40,596 iteration 2733 : loss : 0.030310, loss_ce: 0.010498
2022-01-09 02:55:43,408 iteration 2734 : loss : 0.037423, loss_ce: 0.013488
2022-01-09 02:55:46,289 iteration 2735 : loss : 0.023555, loss_ce: 0.008055
2022-01-09 02:55:49,109 iteration 2736 : loss : 0.022022, loss_ce: 0.010367
2022-01-09 02:55:51,926 iteration 2737 : loss : 0.029109, loss_ce: 0.010675
 40%|██████████▊                | 161/400 [2:17:45<3:29:58, 52.71s/it]2022-01-09 02:55:54,710 iteration 2738 : loss : 0.019176, loss_ce: 0.007193
2022-01-09 02:55:57,574 iteration 2739 : loss : 0.032885, loss_ce: 0.013226
2022-01-09 02:56:00,454 iteration 2740 : loss : 0.038358, loss_ce: 0.014018
2022-01-09 02:56:03,259 iteration 2741 : loss : 0.022916, loss_ce: 0.008386
2022-01-09 02:56:05,838 iteration 2742 : loss : 0.029882, loss_ce: 0.008407
2022-01-09 02:56:08,716 iteration 2743 : loss : 0.034586, loss_ce: 0.010615
2022-01-09 02:56:11,657 iteration 2744 : loss : 0.040019, loss_ce: 0.016013
2022-01-09 02:56:14,275 iteration 2745 : loss : 0.027909, loss_ce: 0.008402
2022-01-09 02:56:17,041 iteration 2746 : loss : 0.023292, loss_ce: 0.008293
2022-01-09 02:56:19,938 iteration 2747 : loss : 0.043651, loss_ce: 0.020076
2022-01-09 02:56:22,724 iteration 2748 : loss : 0.029704, loss_ce: 0.011907
2022-01-09 02:56:25,570 iteration 2749 : loss : 0.029377, loss_ce: 0.013311
2022-01-09 02:56:28,259 iteration 2750 : loss : 0.031176, loss_ce: 0.010135
2022-01-09 02:56:31,025 iteration 2751 : loss : 0.028060, loss_ce: 0.007355
2022-01-09 02:56:33,674 iteration 2752 : loss : 0.029474, loss_ce: 0.012228
2022-01-09 02:56:36,464 iteration 2753 : loss : 0.029943, loss_ce: 0.011691
2022-01-09 02:56:39,292 iteration 2754 : loss : 0.034537, loss_ce: 0.010781
 40%|██████████▉                | 162/400 [2:18:32<3:22:43, 51.11s/it]2022-01-09 02:56:42,154 iteration 2755 : loss : 0.026154, loss_ce: 0.009468
2022-01-09 02:56:44,956 iteration 2756 : loss : 0.027384, loss_ce: 0.012887
2022-01-09 02:56:47,582 iteration 2757 : loss : 0.022952, loss_ce: 0.008350
2022-01-09 02:56:50,354 iteration 2758 : loss : 0.034431, loss_ce: 0.015567
2022-01-09 02:56:53,169 iteration 2759 : loss : 0.025094, loss_ce: 0.009150
2022-01-09 02:56:55,871 iteration 2760 : loss : 0.063516, loss_ce: 0.019587
2022-01-09 02:56:58,745 iteration 2761 : loss : 0.025671, loss_ce: 0.008804
2022-01-09 02:57:01,574 iteration 2762 : loss : 0.030309, loss_ce: 0.009815
2022-01-09 02:57:04,335 iteration 2763 : loss : 0.029684, loss_ce: 0.011887
2022-01-09 02:57:07,049 iteration 2764 : loss : 0.027398, loss_ce: 0.012494
2022-01-09 02:57:09,859 iteration 2765 : loss : 0.038600, loss_ce: 0.018229
2022-01-09 02:57:12,876 iteration 2766 : loss : 0.032934, loss_ce: 0.011992
2022-01-09 02:57:15,694 iteration 2767 : loss : 0.031497, loss_ce: 0.007612
2022-01-09 02:57:18,469 iteration 2768 : loss : 0.030938, loss_ce: 0.012379
2022-01-09 02:57:21,300 iteration 2769 : loss : 0.031384, loss_ce: 0.010316
2022-01-09 02:57:24,088 iteration 2770 : loss : 0.023307, loss_ce: 0.012094
2022-01-09 02:57:26,742 iteration 2771 : loss : 0.027585, loss_ce: 0.012749
 41%|███████████                | 163/400 [2:19:19<3:17:33, 50.01s/it]2022-01-09 02:57:29,754 iteration 2772 : loss : 0.032777, loss_ce: 0.011861
2022-01-09 02:57:32,463 iteration 2773 : loss : 0.023755, loss_ce: 0.007389
2022-01-09 02:57:35,230 iteration 2774 : loss : 0.022841, loss_ce: 0.009923
2022-01-09 02:57:37,855 iteration 2775 : loss : 0.033040, loss_ce: 0.014018
2022-01-09 02:57:40,583 iteration 2776 : loss : 0.034540, loss_ce: 0.018289
2022-01-09 02:57:43,411 iteration 2777 : loss : 0.034217, loss_ce: 0.013261
2022-01-09 02:57:46,117 iteration 2778 : loss : 0.021979, loss_ce: 0.010380
2022-01-09 02:57:48,933 iteration 2779 : loss : 0.028657, loss_ce: 0.010814
2022-01-09 02:57:51,803 iteration 2780 : loss : 0.033161, loss_ce: 0.012937
2022-01-09 02:57:54,543 iteration 2781 : loss : 0.026334, loss_ce: 0.010590
2022-01-09 02:57:57,379 iteration 2782 : loss : 0.027179, loss_ce: 0.010896
2022-01-09 02:58:00,102 iteration 2783 : loss : 0.030540, loss_ce: 0.009824
2022-01-09 02:58:02,990 iteration 2784 : loss : 0.030645, loss_ce: 0.015099
2022-01-09 02:58:05,755 iteration 2785 : loss : 0.023618, loss_ce: 0.008307
2022-01-09 02:58:08,575 iteration 2786 : loss : 0.026913, loss_ce: 0.009682
2022-01-09 02:58:11,453 iteration 2787 : loss : 0.039711, loss_ce: 0.012157
2022-01-09 02:58:14,318 iteration 2788 : loss : 0.028170, loss_ce: 0.008729
 41%|███████████                | 164/400 [2:20:07<3:13:49, 49.28s/it]2022-01-09 02:58:17,145 iteration 2789 : loss : 0.021847, loss_ce: 0.010582
2022-01-09 02:58:19,957 iteration 2790 : loss : 0.026231, loss_ce: 0.006630
2022-01-09 02:58:22,826 iteration 2791 : loss : 0.026963, loss_ce: 0.009104
2022-01-09 02:58:25,523 iteration 2792 : loss : 0.044895, loss_ce: 0.012617
2022-01-09 02:58:28,317 iteration 2793 : loss : 0.034106, loss_ce: 0.016579
2022-01-09 02:58:31,151 iteration 2794 : loss : 0.030924, loss_ce: 0.012666
2022-01-09 02:58:34,043 iteration 2795 : loss : 0.039735, loss_ce: 0.020417
2022-01-09 02:58:36,742 iteration 2796 : loss : 0.037216, loss_ce: 0.014664
2022-01-09 02:58:39,584 iteration 2797 : loss : 0.029027, loss_ce: 0.011931
2022-01-09 02:58:42,461 iteration 2798 : loss : 0.030378, loss_ce: 0.010159
2022-01-09 02:58:45,374 iteration 2799 : loss : 0.031335, loss_ce: 0.011785
2022-01-09 02:58:48,051 iteration 2800 : loss : 0.028225, loss_ce: 0.014803
2022-01-09 02:58:50,836 iteration 2801 : loss : 0.037070, loss_ce: 0.015765
2022-01-09 02:58:53,530 iteration 2802 : loss : 0.021152, loss_ce: 0.008534
2022-01-09 02:58:56,406 iteration 2803 : loss : 0.046328, loss_ce: 0.016483
2022-01-09 02:58:59,233 iteration 2804 : loss : 0.036873, loss_ce: 0.015242
2022-01-09 02:58:59,233 Training Data Eval:
2022-01-09 02:59:14,256   Average segmentation loss on training set: 0.0302
2022-01-09 02:59:14,256 Validation Data Eval:
2022-01-09 02:59:19,734   Average segmentation loss on validation set: 0.0740
2022-01-09 02:59:22,428 iteration 2805 : loss : 0.062559, loss_ce: 0.017529
 41%|███████████▏               | 165/400 [2:21:15<3:35:08, 54.93s/it]2022-01-09 02:59:25,316 iteration 2806 : loss : 0.066645, loss_ce: 0.022304
2022-01-09 02:59:28,147 iteration 2807 : loss : 0.032288, loss_ce: 0.011988
2022-01-09 02:59:30,914 iteration 2808 : loss : 0.023671, loss_ce: 0.012475
2022-01-09 02:59:33,544 iteration 2809 : loss : 0.045228, loss_ce: 0.018439
2022-01-09 02:59:36,435 iteration 2810 : loss : 0.029162, loss_ce: 0.011470
2022-01-09 02:59:39,258 iteration 2811 : loss : 0.025614, loss_ce: 0.009963
2022-01-09 02:59:41,930 iteration 2812 : loss : 0.029827, loss_ce: 0.011671
2022-01-09 02:59:44,840 iteration 2813 : loss : 0.028128, loss_ce: 0.007619
2022-01-09 02:59:47,477 iteration 2814 : loss : 0.029399, loss_ce: 0.011061
2022-01-09 02:59:50,269 iteration 2815 : loss : 0.020906, loss_ce: 0.008989
2022-01-09 02:59:53,071 iteration 2816 : loss : 0.025272, loss_ce: 0.010645
2022-01-09 02:59:55,928 iteration 2817 : loss : 0.023373, loss_ce: 0.007072
2022-01-09 02:59:58,665 iteration 2818 : loss : 0.023236, loss_ce: 0.007214
2022-01-09 03:00:01,487 iteration 2819 : loss : 0.037058, loss_ce: 0.018330
2022-01-09 03:00:04,454 iteration 2820 : loss : 0.027999, loss_ce: 0.011932
2022-01-09 03:00:07,313 iteration 2821 : loss : 0.028611, loss_ce: 0.013193
2022-01-09 03:00:09,938 iteration 2822 : loss : 0.023604, loss_ce: 0.007793
 42%|███████████▏               | 166/400 [2:22:03<3:25:32, 52.70s/it]2022-01-09 03:00:12,827 iteration 2823 : loss : 0.040576, loss_ce: 0.015506
2022-01-09 03:00:15,592 iteration 2824 : loss : 0.020159, loss_ce: 0.007301
2022-01-09 03:00:18,293 iteration 2825 : loss : 0.028783, loss_ce: 0.012106
2022-01-09 03:00:20,880 iteration 2826 : loss : 0.019515, loss_ce: 0.007898
2022-01-09 03:00:23,639 iteration 2827 : loss : 0.030971, loss_ce: 0.013914
2022-01-09 03:00:26,482 iteration 2828 : loss : 0.030438, loss_ce: 0.012692
2022-01-09 03:00:29,439 iteration 2829 : loss : 0.037145, loss_ce: 0.015251
2022-01-09 03:00:32,079 iteration 2830 : loss : 0.025691, loss_ce: 0.010921
2022-01-09 03:00:34,875 iteration 2831 : loss : 0.024637, loss_ce: 0.010319
2022-01-09 03:00:37,932 iteration 2832 : loss : 0.030115, loss_ce: 0.008967
2022-01-09 03:00:40,885 iteration 2833 : loss : 0.028210, loss_ce: 0.012020
2022-01-09 03:00:43,664 iteration 2834 : loss : 0.048060, loss_ce: 0.013927
2022-01-09 03:00:46,498 iteration 2835 : loss : 0.047972, loss_ce: 0.013115
2022-01-09 03:00:49,402 iteration 2836 : loss : 0.047194, loss_ce: 0.019295
2022-01-09 03:00:52,223 iteration 2837 : loss : 0.035201, loss_ce: 0.008136
2022-01-09 03:00:54,890 iteration 2838 : loss : 0.019921, loss_ce: 0.006184
2022-01-09 03:00:57,647 iteration 2839 : loss : 0.047911, loss_ce: 0.020747
 42%|███████████▎               | 167/400 [2:22:50<3:18:50, 51.21s/it]2022-01-09 03:01:00,486 iteration 2840 : loss : 0.033705, loss_ce: 0.016267
2022-01-09 03:01:03,301 iteration 2841 : loss : 0.027044, loss_ce: 0.010949
2022-01-09 03:01:06,099 iteration 2842 : loss : 0.032374, loss_ce: 0.010495
2022-01-09 03:01:08,658 iteration 2843 : loss : 0.025174, loss_ce: 0.010620
2022-01-09 03:01:11,466 iteration 2844 : loss : 0.018628, loss_ce: 0.006567
2022-01-09 03:01:14,218 iteration 2845 : loss : 0.024715, loss_ce: 0.010037
2022-01-09 03:01:17,116 iteration 2846 : loss : 0.057025, loss_ce: 0.014886
2022-01-09 03:01:19,989 iteration 2847 : loss : 0.028148, loss_ce: 0.009302
2022-01-09 03:01:22,820 iteration 2848 : loss : 0.032334, loss_ce: 0.009141
2022-01-09 03:01:25,645 iteration 2849 : loss : 0.031991, loss_ce: 0.013087
2022-01-09 03:01:28,471 iteration 2850 : loss : 0.024716, loss_ce: 0.011147
2022-01-09 03:01:31,374 iteration 2851 : loss : 0.026318, loss_ce: 0.011329
2022-01-09 03:01:34,062 iteration 2852 : loss : 0.030960, loss_ce: 0.010401
2022-01-09 03:01:36,937 iteration 2853 : loss : 0.038629, loss_ce: 0.012773
2022-01-09 03:01:39,517 iteration 2854 : loss : 0.025804, loss_ce: 0.007614
2022-01-09 03:01:42,298 iteration 2855 : loss : 0.030419, loss_ce: 0.012964
2022-01-09 03:01:44,949 iteration 2856 : loss : 0.020836, loss_ce: 0.010137
 42%|███████████▎               | 168/400 [2:23:38<3:13:28, 50.04s/it]2022-01-09 03:01:47,822 iteration 2857 : loss : 0.027765, loss_ce: 0.012906
2022-01-09 03:01:50,722 iteration 2858 : loss : 0.032854, loss_ce: 0.011152
2022-01-09 03:01:53,607 iteration 2859 : loss : 0.025260, loss_ce: 0.009877
2022-01-09 03:01:56,242 iteration 2860 : loss : 0.022418, loss_ce: 0.008173
2022-01-09 03:01:59,074 iteration 2861 : loss : 0.022487, loss_ce: 0.006202
2022-01-09 03:02:01,993 iteration 2862 : loss : 0.042495, loss_ce: 0.014051
2022-01-09 03:02:04,628 iteration 2863 : loss : 0.021633, loss_ce: 0.007894
2022-01-09 03:02:07,413 iteration 2864 : loss : 0.025146, loss_ce: 0.008230
2022-01-09 03:02:10,038 iteration 2865 : loss : 0.023806, loss_ce: 0.008998
2022-01-09 03:02:12,903 iteration 2866 : loss : 0.021489, loss_ce: 0.007414
2022-01-09 03:02:15,796 iteration 2867 : loss : 0.031576, loss_ce: 0.011846
2022-01-09 03:02:18,651 iteration 2868 : loss : 0.031425, loss_ce: 0.013486
2022-01-09 03:02:21,511 iteration 2869 : loss : 0.026396, loss_ce: 0.009175
2022-01-09 03:02:24,422 iteration 2870 : loss : 0.029456, loss_ce: 0.010681
2022-01-09 03:02:27,182 iteration 2871 : loss : 0.028080, loss_ce: 0.013943
2022-01-09 03:02:29,826 iteration 2872 : loss : 0.024715, loss_ce: 0.013287
2022-01-09 03:02:32,616 iteration 2873 : loss : 0.029743, loss_ce: 0.012257
 42%|███████████▍               | 169/400 [2:24:25<3:09:54, 49.33s/it]2022-01-09 03:02:35,465 iteration 2874 : loss : 0.019388, loss_ce: 0.006975
2022-01-09 03:02:38,317 iteration 2875 : loss : 0.028512, loss_ce: 0.012072
2022-01-09 03:02:41,136 iteration 2876 : loss : 0.033957, loss_ce: 0.011056
2022-01-09 03:02:43,768 iteration 2877 : loss : 0.024821, loss_ce: 0.008278
2022-01-09 03:02:46,561 iteration 2878 : loss : 0.021817, loss_ce: 0.006422
2022-01-09 03:02:49,365 iteration 2879 : loss : 0.023105, loss_ce: 0.009754
2022-01-09 03:02:52,166 iteration 2880 : loss : 0.021596, loss_ce: 0.011258
2022-01-09 03:02:54,960 iteration 2881 : loss : 0.022486, loss_ce: 0.010767
2022-01-09 03:02:57,551 iteration 2882 : loss : 0.039236, loss_ce: 0.012271
2022-01-09 03:03:00,427 iteration 2883 : loss : 0.031322, loss_ce: 0.009026
2022-01-09 03:03:03,298 iteration 2884 : loss : 0.029251, loss_ce: 0.012128
2022-01-09 03:03:06,177 iteration 2885 : loss : 0.025594, loss_ce: 0.009854
2022-01-09 03:03:08,794 iteration 2886 : loss : 0.022148, loss_ce: 0.007284
2022-01-09 03:03:11,647 iteration 2887 : loss : 0.021616, loss_ce: 0.007049
2022-01-09 03:03:14,550 iteration 2888 : loss : 0.020222, loss_ce: 0.008480
2022-01-09 03:03:17,401 iteration 2889 : loss : 0.022950, loss_ce: 0.009221
2022-01-09 03:03:17,401 Training Data Eval:
2022-01-09 03:03:32,476   Average segmentation loss on training set: 0.0189
2022-01-09 03:03:32,477 Validation Data Eval:
2022-01-09 03:03:37,756   Average segmentation loss on validation set: 0.0757
2022-01-09 03:03:40,593 iteration 2890 : loss : 0.030093, loss_ce: 0.011826
 42%|███████████▍               | 170/400 [2:25:33<3:30:31, 54.92s/it]2022-01-09 03:03:43,475 iteration 2891 : loss : 0.023344, loss_ce: 0.010530
2022-01-09 03:03:46,314 iteration 2892 : loss : 0.040058, loss_ce: 0.018832
2022-01-09 03:03:48,930 iteration 2893 : loss : 0.022975, loss_ce: 0.008688
2022-01-09 03:03:51,565 iteration 2894 : loss : 0.025998, loss_ce: 0.010198
2022-01-09 03:03:54,372 iteration 2895 : loss : 0.023492, loss_ce: 0.008008
2022-01-09 03:03:57,037 iteration 2896 : loss : 0.035479, loss_ce: 0.013367
2022-01-09 03:03:59,944 iteration 2897 : loss : 0.055238, loss_ce: 0.015995
2022-01-09 03:04:02,763 iteration 2898 : loss : 0.028494, loss_ce: 0.009284
2022-01-09 03:04:05,576 iteration 2899 : loss : 0.029366, loss_ce: 0.013919
2022-01-09 03:04:08,521 iteration 2900 : loss : 0.028883, loss_ce: 0.010935
2022-01-09 03:04:11,394 iteration 2901 : loss : 0.024395, loss_ce: 0.008876
2022-01-09 03:04:14,046 iteration 2902 : loss : 0.029487, loss_ce: 0.013612
2022-01-09 03:04:16,862 iteration 2903 : loss : 0.038962, loss_ce: 0.009847
2022-01-09 03:04:19,573 iteration 2904 : loss : 0.026610, loss_ce: 0.011682
2022-01-09 03:04:22,417 iteration 2905 : loss : 0.029432, loss_ce: 0.014440
2022-01-09 03:04:25,085 iteration 2906 : loss : 0.048701, loss_ce: 0.024518
2022-01-09 03:04:27,895 iteration 2907 : loss : 0.022528, loss_ce: 0.008860
 43%|███████████▌               | 171/400 [2:26:21<3:20:52, 52.63s/it]2022-01-09 03:04:30,768 iteration 2908 : loss : 0.032444, loss_ce: 0.012904
2022-01-09 03:04:33,579 iteration 2909 : loss : 0.042461, loss_ce: 0.015133
2022-01-09 03:04:36,317 iteration 2910 : loss : 0.023267, loss_ce: 0.009197
2022-01-09 03:04:39,156 iteration 2911 : loss : 0.054859, loss_ce: 0.019506
2022-01-09 03:04:41,965 iteration 2912 : loss : 0.021298, loss_ce: 0.007971
2022-01-09 03:04:44,608 iteration 2913 : loss : 0.025760, loss_ce: 0.008833
2022-01-09 03:04:47,453 iteration 2914 : loss : 0.041699, loss_ce: 0.016315
2022-01-09 03:04:50,255 iteration 2915 : loss : 0.034478, loss_ce: 0.014921
2022-01-09 03:04:52,889 iteration 2916 : loss : 0.030165, loss_ce: 0.012797
2022-01-09 03:04:55,746 iteration 2917 : loss : 0.034048, loss_ce: 0.012998
2022-01-09 03:04:58,455 iteration 2918 : loss : 0.027606, loss_ce: 0.012375
2022-01-09 03:05:01,250 iteration 2919 : loss : 0.039471, loss_ce: 0.011237
2022-01-09 03:05:04,126 iteration 2920 : loss : 0.024285, loss_ce: 0.009431
2022-01-09 03:05:06,894 iteration 2921 : loss : 0.024817, loss_ce: 0.010975
2022-01-09 03:05:09,684 iteration 2922 : loss : 0.031099, loss_ce: 0.008074
2022-01-09 03:05:12,584 iteration 2923 : loss : 0.045256, loss_ce: 0.012949
2022-01-09 03:05:15,294 iteration 2924 : loss : 0.024246, loss_ce: 0.010402
 43%|███████████▌               | 172/400 [2:27:08<3:14:02, 51.07s/it]2022-01-09 03:05:18,162 iteration 2925 : loss : 0.062051, loss_ce: 0.032414
2022-01-09 03:05:20,925 iteration 2926 : loss : 0.020529, loss_ce: 0.007206
2022-01-09 03:05:23,575 iteration 2927 : loss : 0.025334, loss_ce: 0.008891
2022-01-09 03:05:26,470 iteration 2928 : loss : 0.038175, loss_ce: 0.015665
2022-01-09 03:05:29,060 iteration 2929 : loss : 0.030358, loss_ce: 0.007493
2022-01-09 03:05:31,911 iteration 2930 : loss : 0.024854, loss_ce: 0.010561
2022-01-09 03:05:34,798 iteration 2931 : loss : 0.072414, loss_ce: 0.010212
2022-01-09 03:05:37,659 iteration 2932 : loss : 0.029161, loss_ce: 0.013562
2022-01-09 03:05:40,475 iteration 2933 : loss : 0.061354, loss_ce: 0.027598
2022-01-09 03:05:43,339 iteration 2934 : loss : 0.032587, loss_ce: 0.015788
2022-01-09 03:05:46,155 iteration 2935 : loss : 0.051594, loss_ce: 0.022333
2022-01-09 03:05:48,977 iteration 2936 : loss : 0.029696, loss_ce: 0.011603
2022-01-09 03:05:51,843 iteration 2937 : loss : 0.033901, loss_ce: 0.014401
2022-01-09 03:05:54,666 iteration 2938 : loss : 0.040817, loss_ce: 0.014077
2022-01-09 03:05:57,389 iteration 2939 : loss : 0.043422, loss_ce: 0.011050
2022-01-09 03:06:00,016 iteration 2940 : loss : 0.039429, loss_ce: 0.018948
2022-01-09 03:06:02,836 iteration 2941 : loss : 0.045860, loss_ce: 0.018222
 43%|███████████▋               | 173/400 [2:27:56<3:09:11, 50.01s/it]2022-01-09 03:06:05,790 iteration 2942 : loss : 0.033153, loss_ce: 0.015692
2022-01-09 03:06:08,644 iteration 2943 : loss : 0.034615, loss_ce: 0.015764
2022-01-09 03:06:11,439 iteration 2944 : loss : 0.066162, loss_ce: 0.023485
2022-01-09 03:06:14,101 iteration 2945 : loss : 0.041116, loss_ce: 0.017566
2022-01-09 03:06:17,000 iteration 2946 : loss : 0.034521, loss_ce: 0.012357
2022-01-09 03:06:19,650 iteration 2947 : loss : 0.034466, loss_ce: 0.011867
2022-01-09 03:06:22,492 iteration 2948 : loss : 0.028223, loss_ce: 0.010452
2022-01-09 03:06:25,362 iteration 2949 : loss : 0.036374, loss_ce: 0.011991
2022-01-09 03:06:27,958 iteration 2950 : loss : 0.028977, loss_ce: 0.010239
2022-01-09 03:06:30,815 iteration 2951 : loss : 0.025481, loss_ce: 0.007369
2022-01-09 03:06:33,365 iteration 2952 : loss : 0.099418, loss_ce: 0.019564
2022-01-09 03:06:36,210 iteration 2953 : loss : 0.029779, loss_ce: 0.010606
2022-01-09 03:06:38,993 iteration 2954 : loss : 0.037381, loss_ce: 0.018899
2022-01-09 03:06:41,817 iteration 2955 : loss : 0.047006, loss_ce: 0.009040
2022-01-09 03:06:44,612 iteration 2956 : loss : 0.058119, loss_ce: 0.036907
2022-01-09 03:06:47,364 iteration 2957 : loss : 0.035043, loss_ce: 0.011765
2022-01-09 03:06:50,208 iteration 2958 : loss : 0.042381, loss_ce: 0.014908
 44%|███████████▋               | 174/400 [2:28:43<3:05:22, 49.21s/it]2022-01-09 03:06:53,044 iteration 2959 : loss : 0.031935, loss_ce: 0.015271
2022-01-09 03:06:55,873 iteration 2960 : loss : 0.032685, loss_ce: 0.013520
2022-01-09 03:06:58,712 iteration 2961 : loss : 0.031624, loss_ce: 0.013197
2022-01-09 03:07:01,547 iteration 2962 : loss : 0.035627, loss_ce: 0.012436
2022-01-09 03:07:04,367 iteration 2963 : loss : 0.034558, loss_ce: 0.013823
2022-01-09 03:07:07,036 iteration 2964 : loss : 0.040195, loss_ce: 0.015664
2022-01-09 03:07:09,778 iteration 2965 : loss : 0.050905, loss_ce: 0.018253
2022-01-09 03:07:12,537 iteration 2966 : loss : 0.027197, loss_ce: 0.010237
2022-01-09 03:07:15,402 iteration 2967 : loss : 0.031351, loss_ce: 0.013224
2022-01-09 03:07:18,279 iteration 2968 : loss : 0.047098, loss_ce: 0.013103
2022-01-09 03:07:21,165 iteration 2969 : loss : 0.024718, loss_ce: 0.010900
2022-01-09 03:07:23,809 iteration 2970 : loss : 0.036173, loss_ce: 0.014351
2022-01-09 03:07:26,569 iteration 2971 : loss : 0.031300, loss_ce: 0.013334
2022-01-09 03:07:29,592 iteration 2972 : loss : 0.038025, loss_ce: 0.013073
2022-01-09 03:07:32,191 iteration 2973 : loss : 0.028606, loss_ce: 0.011737
2022-01-09 03:07:35,088 iteration 2974 : loss : 0.025878, loss_ce: 0.009421
2022-01-09 03:07:35,088 Training Data Eval:
2022-01-09 03:07:50,147   Average segmentation loss on training set: 0.0467
2022-01-09 03:07:50,148 Validation Data Eval:
2022-01-09 03:07:55,579   Average segmentation loss on validation set: 0.2041
2022-01-09 03:07:58,429 iteration 2975 : loss : 0.029033, loss_ce: 0.007607
 44%|███████████▊               | 175/400 [2:29:51<3:25:56, 54.92s/it]2022-01-09 03:08:01,349 iteration 2976 : loss : 0.036445, loss_ce: 0.017507
2022-01-09 03:08:03,944 iteration 2977 : loss : 0.026128, loss_ce: 0.009949
2022-01-09 03:08:06,759 iteration 2978 : loss : 0.027186, loss_ce: 0.009902
2022-01-09 03:08:09,420 iteration 2979 : loss : 0.031220, loss_ce: 0.012998
2022-01-09 03:08:12,254 iteration 2980 : loss : 0.042838, loss_ce: 0.018530
2022-01-09 03:08:15,108 iteration 2981 : loss : 0.030780, loss_ce: 0.010152
2022-01-09 03:08:18,030 iteration 2982 : loss : 0.036864, loss_ce: 0.015872
2022-01-09 03:08:20,829 iteration 2983 : loss : 0.043418, loss_ce: 0.019365
2022-01-09 03:08:23,674 iteration 2984 : loss : 0.024513, loss_ce: 0.009463
2022-01-09 03:08:26,494 iteration 2985 : loss : 0.033002, loss_ce: 0.010656
2022-01-09 03:08:29,493 iteration 2986 : loss : 0.035953, loss_ce: 0.014049
2022-01-09 03:08:32,348 iteration 2987 : loss : 0.024476, loss_ce: 0.011447
2022-01-09 03:08:34,990 iteration 2988 : loss : 0.024941, loss_ce: 0.008559
2022-01-09 03:08:37,894 iteration 2989 : loss : 0.029753, loss_ce: 0.007503
2022-01-09 03:08:40,548 iteration 2990 : loss : 0.035636, loss_ce: 0.015825
2022-01-09 03:08:43,468 iteration 2991 : loss : 0.024952, loss_ce: 0.008180
2022-01-09 03:08:46,382 iteration 2992 : loss : 0.032390, loss_ce: 0.012916
 44%|███████████▉               | 176/400 [2:30:39<3:17:12, 52.83s/it]2022-01-09 03:08:49,144 iteration 2993 : loss : 0.025179, loss_ce: 0.007511
2022-01-09 03:08:51,961 iteration 2994 : loss : 0.025602, loss_ce: 0.009032
2022-01-09 03:08:54,893 iteration 2995 : loss : 0.045803, loss_ce: 0.022445
2022-01-09 03:08:57,483 iteration 2996 : loss : 0.022841, loss_ce: 0.007965
2022-01-09 03:09:00,288 iteration 2997 : loss : 0.021198, loss_ce: 0.007073
2022-01-09 03:09:03,054 iteration 2998 : loss : 0.020935, loss_ce: 0.007049
2022-01-09 03:09:05,819 iteration 2999 : loss : 0.028992, loss_ce: 0.010218
2022-01-09 03:09:08,657 iteration 3000 : loss : 0.034259, loss_ce: 0.010441
2022-01-09 03:09:11,520 iteration 3001 : loss : 0.023071, loss_ce: 0.009080
2022-01-09 03:09:14,255 iteration 3002 : loss : 0.028810, loss_ce: 0.012045
2022-01-09 03:09:17,093 iteration 3003 : loss : 0.023048, loss_ce: 0.009083
2022-01-09 03:09:19,920 iteration 3004 : loss : 0.021037, loss_ce: 0.008423
2022-01-09 03:09:22,793 iteration 3005 : loss : 0.051752, loss_ce: 0.018485
2022-01-09 03:09:25,657 iteration 3006 : loss : 0.042387, loss_ce: 0.016237
2022-01-09 03:09:28,475 iteration 3007 : loss : 0.019975, loss_ce: 0.007175
2022-01-09 03:09:31,398 iteration 3008 : loss : 0.034360, loss_ce: 0.014434
2022-01-09 03:09:34,124 iteration 3009 : loss : 0.027003, loss_ce: 0.014814
 44%|███████████▉               | 177/400 [2:31:27<3:10:40, 51.30s/it]2022-01-09 03:09:36,967 iteration 3010 : loss : 0.022427, loss_ce: 0.008607
2022-01-09 03:09:39,596 iteration 3011 : loss : 0.027837, loss_ce: 0.008898
2022-01-09 03:09:42,221 iteration 3012 : loss : 0.023063, loss_ce: 0.007500
2022-01-09 03:09:45,068 iteration 3013 : loss : 0.016684, loss_ce: 0.006660
2022-01-09 03:09:47,916 iteration 3014 : loss : 0.026517, loss_ce: 0.010837
2022-01-09 03:09:50,779 iteration 3015 : loss : 0.025183, loss_ce: 0.008580
2022-01-09 03:09:53,442 iteration 3016 : loss : 0.024587, loss_ce: 0.008244
2022-01-09 03:09:56,326 iteration 3017 : loss : 0.031323, loss_ce: 0.009147
2022-01-09 03:09:59,110 iteration 3018 : loss : 0.021257, loss_ce: 0.005627
2022-01-09 03:10:01,977 iteration 3019 : loss : 0.036266, loss_ce: 0.014769
2022-01-09 03:10:04,652 iteration 3020 : loss : 0.024237, loss_ce: 0.012683
2022-01-09 03:10:07,374 iteration 3021 : loss : 0.019103, loss_ce: 0.006194
2022-01-09 03:10:10,133 iteration 3022 : loss : 0.024318, loss_ce: 0.006973
2022-01-09 03:10:12,966 iteration 3023 : loss : 0.022236, loss_ce: 0.011236
2022-01-09 03:10:15,602 iteration 3024 : loss : 0.027110, loss_ce: 0.009744
2022-01-09 03:10:18,236 iteration 3025 : loss : 0.023043, loss_ce: 0.009186
2022-01-09 03:10:21,054 iteration 3026 : loss : 0.019575, loss_ce: 0.009314
 44%|████████████               | 178/400 [2:32:14<3:04:58, 49.99s/it]2022-01-09 03:10:23,947 iteration 3027 : loss : 0.028354, loss_ce: 0.013275
2022-01-09 03:10:26,591 iteration 3028 : loss : 0.029296, loss_ce: 0.012784
2022-01-09 03:10:29,412 iteration 3029 : loss : 0.023793, loss_ce: 0.010428
2022-01-09 03:10:32,083 iteration 3030 : loss : 0.032577, loss_ce: 0.010090
2022-01-09 03:10:34,619 iteration 3031 : loss : 0.019961, loss_ce: 0.007933
2022-01-09 03:10:37,418 iteration 3032 : loss : 0.024174, loss_ce: 0.007851
2022-01-09 03:10:40,100 iteration 3033 : loss : 0.023472, loss_ce: 0.010911
2022-01-09 03:10:42,876 iteration 3034 : loss : 0.024465, loss_ce: 0.008095
2022-01-09 03:10:45,745 iteration 3035 : loss : 0.029875, loss_ce: 0.009972
2022-01-09 03:10:48,388 iteration 3036 : loss : 0.022229, loss_ce: 0.009150
2022-01-09 03:10:51,337 iteration 3037 : loss : 0.018337, loss_ce: 0.005964
2022-01-09 03:10:54,033 iteration 3038 : loss : 0.020572, loss_ce: 0.004715
2022-01-09 03:10:57,017 iteration 3039 : loss : 0.044916, loss_ce: 0.015170
2022-01-09 03:10:59,605 iteration 3040 : loss : 0.028406, loss_ce: 0.012934
2022-01-09 03:11:02,351 iteration 3041 : loss : 0.027264, loss_ce: 0.010368
2022-01-09 03:11:05,127 iteration 3042 : loss : 0.023009, loss_ce: 0.009735
2022-01-09 03:11:08,060 iteration 3043 : loss : 0.023825, loss_ce: 0.008951
 45%|████████████               | 179/400 [2:33:01<3:00:50, 49.10s/it]2022-01-09 03:11:10,945 iteration 3044 : loss : 0.031120, loss_ce: 0.011702
2022-01-09 03:11:13,592 iteration 3045 : loss : 0.022610, loss_ce: 0.009530
2022-01-09 03:11:16,456 iteration 3046 : loss : 0.030956, loss_ce: 0.011112
2022-01-09 03:11:19,371 iteration 3047 : loss : 0.028579, loss_ce: 0.011974
2022-01-09 03:11:22,236 iteration 3048 : loss : 0.028165, loss_ce: 0.015770
2022-01-09 03:11:24,845 iteration 3049 : loss : 0.030589, loss_ce: 0.011506
2022-01-09 03:11:27,750 iteration 3050 : loss : 0.029898, loss_ce: 0.012821
2022-01-09 03:11:30,591 iteration 3051 : loss : 0.037314, loss_ce: 0.011524
2022-01-09 03:11:33,463 iteration 3052 : loss : 0.037043, loss_ce: 0.019032
2022-01-09 03:11:36,264 iteration 3053 : loss : 0.026659, loss_ce: 0.012759
2022-01-09 03:11:39,111 iteration 3054 : loss : 0.037479, loss_ce: 0.013781
2022-01-09 03:11:42,057 iteration 3055 : loss : 0.022638, loss_ce: 0.008338
2022-01-09 03:11:44,768 iteration 3056 : loss : 0.024895, loss_ce: 0.008926
2022-01-09 03:11:47,592 iteration 3057 : loss : 0.019092, loss_ce: 0.006221
2022-01-09 03:11:50,466 iteration 3058 : loss : 0.061089, loss_ce: 0.019077
2022-01-09 03:11:53,298 iteration 3059 : loss : 0.034036, loss_ce: 0.011759
2022-01-09 03:11:53,298 Training Data Eval:
2022-01-09 03:12:08,282   Average segmentation loss on training set: 0.0187
2022-01-09 03:12:08,282 Validation Data Eval:
2022-01-09 03:12:13,402   Average segmentation loss on validation set: 0.0685
2022-01-09 03:12:16,111 iteration 3060 : loss : 0.023691, loss_ce: 0.009196
 45%|████████████▏              | 180/400 [2:34:09<3:20:52, 54.78s/it]2022-01-09 03:12:18,984 iteration 3061 : loss : 0.027899, loss_ce: 0.009893
2022-01-09 03:12:21,987 iteration 3062 : loss : 0.029263, loss_ce: 0.008509
2022-01-09 03:12:24,786 iteration 3063 : loss : 0.032689, loss_ce: 0.011043
2022-01-09 03:12:27,473 iteration 3064 : loss : 0.027232, loss_ce: 0.015035
2022-01-09 03:12:30,276 iteration 3065 : loss : 0.034659, loss_ce: 0.009480
2022-01-09 03:12:33,071 iteration 3066 : loss : 0.023703, loss_ce: 0.006042
2022-01-09 03:12:35,919 iteration 3067 : loss : 0.029905, loss_ce: 0.014908
2022-01-09 03:12:38,778 iteration 3068 : loss : 0.019464, loss_ce: 0.006486
2022-01-09 03:12:41,652 iteration 3069 : loss : 0.053017, loss_ce: 0.011481
2022-01-09 03:12:44,259 iteration 3070 : loss : 0.024775, loss_ce: 0.009349
2022-01-09 03:12:47,120 iteration 3071 : loss : 0.029085, loss_ce: 0.011479
2022-01-09 03:12:49,939 iteration 3072 : loss : 0.020199, loss_ce: 0.008371
2022-01-09 03:12:52,744 iteration 3073 : loss : 0.027494, loss_ce: 0.009445
2022-01-09 03:12:55,663 iteration 3074 : loss : 0.043100, loss_ce: 0.019248
2022-01-09 03:12:58,379 iteration 3075 : loss : 0.027238, loss_ce: 0.011355
2022-01-09 03:13:01,179 iteration 3076 : loss : 0.027681, loss_ce: 0.013353
2022-01-09 03:13:04,060 iteration 3077 : loss : 0.031330, loss_ce: 0.013790
 45%|████████████▏              | 181/400 [2:34:57<3:12:28, 52.73s/it]2022-01-09 03:13:06,868 iteration 3078 : loss : 0.017605, loss_ce: 0.007095
2022-01-09 03:13:09,591 iteration 3079 : loss : 0.039262, loss_ce: 0.013387
2022-01-09 03:13:12,399 iteration 3080 : loss : 0.024118, loss_ce: 0.010162
2022-01-09 03:13:15,023 iteration 3081 : loss : 0.027090, loss_ce: 0.008046
2022-01-09 03:13:17,858 iteration 3082 : loss : 0.022808, loss_ce: 0.010112
2022-01-09 03:13:20,532 iteration 3083 : loss : 0.034590, loss_ce: 0.014748
2022-01-09 03:13:23,357 iteration 3084 : loss : 0.037675, loss_ce: 0.014422
2022-01-09 03:13:26,295 iteration 3085 : loss : 0.066009, loss_ce: 0.012465
2022-01-09 03:13:29,065 iteration 3086 : loss : 0.020592, loss_ce: 0.008094
2022-01-09 03:13:31,889 iteration 3087 : loss : 0.028190, loss_ce: 0.007666
2022-01-09 03:13:34,693 iteration 3088 : loss : 0.020971, loss_ce: 0.007832
2022-01-09 03:13:37,352 iteration 3089 : loss : 0.032431, loss_ce: 0.013475
2022-01-09 03:13:40,243 iteration 3090 : loss : 0.028668, loss_ce: 0.009458
2022-01-09 03:13:42,920 iteration 3091 : loss : 0.035183, loss_ce: 0.014490
2022-01-09 03:13:45,672 iteration 3092 : loss : 0.029039, loss_ce: 0.014028
2022-01-09 03:13:48,451 iteration 3093 : loss : 0.023020, loss_ce: 0.007260
2022-01-09 03:13:51,250 iteration 3094 : loss : 0.046547, loss_ce: 0.025255
 46%|████████████▎              | 182/400 [2:35:44<3:05:32, 51.07s/it]2022-01-09 03:13:54,086 iteration 3095 : loss : 0.022431, loss_ce: 0.006719
2022-01-09 03:13:56,872 iteration 3096 : loss : 0.020898, loss_ce: 0.008182
2022-01-09 03:13:59,708 iteration 3097 : loss : 0.032550, loss_ce: 0.013828
2022-01-09 03:14:02,430 iteration 3098 : loss : 0.032490, loss_ce: 0.010530
2022-01-09 03:14:05,154 iteration 3099 : loss : 0.037255, loss_ce: 0.012134
2022-01-09 03:14:08,011 iteration 3100 : loss : 0.032106, loss_ce: 0.011169
2022-01-09 03:14:10,875 iteration 3101 : loss : 0.026867, loss_ce: 0.011616
2022-01-09 03:14:13,771 iteration 3102 : loss : 0.027612, loss_ce: 0.009388
2022-01-09 03:14:16,379 iteration 3103 : loss : 0.031662, loss_ce: 0.008420
2022-01-09 03:14:19,245 iteration 3104 : loss : 0.030527, loss_ce: 0.009076
2022-01-09 03:14:22,016 iteration 3105 : loss : 0.019909, loss_ce: 0.006622
2022-01-09 03:14:24,996 iteration 3106 : loss : 0.050400, loss_ce: 0.021522
2022-01-09 03:14:27,839 iteration 3107 : loss : 0.028669, loss_ce: 0.011474
2022-01-09 03:14:30,554 iteration 3108 : loss : 0.027122, loss_ce: 0.010034
2022-01-09 03:14:33,246 iteration 3109 : loss : 0.059061, loss_ce: 0.033104
2022-01-09 03:14:36,157 iteration 3110 : loss : 0.043383, loss_ce: 0.018219
2022-01-09 03:14:38,962 iteration 3111 : loss : 0.046614, loss_ce: 0.018037
 46%|████████████▎              | 183/400 [2:36:32<3:01:03, 50.06s/it]2022-01-09 03:14:41,832 iteration 3112 : loss : 0.019983, loss_ce: 0.007594
2022-01-09 03:14:44,675 iteration 3113 : loss : 0.034880, loss_ce: 0.013653
2022-01-09 03:14:47,608 iteration 3114 : loss : 0.054862, loss_ce: 0.015374
2022-01-09 03:14:50,477 iteration 3115 : loss : 0.038120, loss_ce: 0.018703
2022-01-09 03:14:53,274 iteration 3116 : loss : 0.023170, loss_ce: 0.008629
2022-01-09 03:14:56,069 iteration 3117 : loss : 0.042111, loss_ce: 0.019101
2022-01-09 03:14:58,924 iteration 3118 : loss : 0.032613, loss_ce: 0.013946
2022-01-09 03:15:01,758 iteration 3119 : loss : 0.032505, loss_ce: 0.012269
2022-01-09 03:15:04,411 iteration 3120 : loss : 0.020863, loss_ce: 0.007123
2022-01-09 03:15:07,290 iteration 3121 : loss : 0.033897, loss_ce: 0.009607
2022-01-09 03:15:10,034 iteration 3122 : loss : 0.034229, loss_ce: 0.009632
2022-01-09 03:15:12,842 iteration 3123 : loss : 0.021738, loss_ce: 0.008431
2022-01-09 03:15:15,714 iteration 3124 : loss : 0.027662, loss_ce: 0.011995
2022-01-09 03:15:18,552 iteration 3125 : loss : 0.027863, loss_ce: 0.012887
2022-01-09 03:15:21,332 iteration 3126 : loss : 0.026583, loss_ce: 0.009691
2022-01-09 03:15:24,062 iteration 3127 : loss : 0.043027, loss_ce: 0.018869
2022-01-09 03:15:26,882 iteration 3128 : loss : 0.020622, loss_ce: 0.006804
 46%|████████████▍              | 184/400 [2:37:20<2:57:53, 49.41s/it]2022-01-09 03:15:29,749 iteration 3129 : loss : 0.027514, loss_ce: 0.011251
2022-01-09 03:15:32,743 iteration 3130 : loss : 0.029114, loss_ce: 0.009396
2022-01-09 03:15:35,620 iteration 3131 : loss : 0.040211, loss_ce: 0.015225
2022-01-09 03:15:38,422 iteration 3132 : loss : 0.027114, loss_ce: 0.011256
2022-01-09 03:15:41,008 iteration 3133 : loss : 0.024252, loss_ce: 0.008274
2022-01-09 03:15:43,792 iteration 3134 : loss : 0.024161, loss_ce: 0.007473
2022-01-09 03:15:46,689 iteration 3135 : loss : 0.025410, loss_ce: 0.008708
2022-01-09 03:15:49,604 iteration 3136 : loss : 0.042956, loss_ce: 0.014465
2022-01-09 03:15:52,265 iteration 3137 : loss : 0.042208, loss_ce: 0.017239
2022-01-09 03:15:55,095 iteration 3138 : loss : 0.025685, loss_ce: 0.012153
2022-01-09 03:15:58,015 iteration 3139 : loss : 0.031418, loss_ce: 0.010405
2022-01-09 03:16:00,757 iteration 3140 : loss : 0.039314, loss_ce: 0.014967
2022-01-09 03:16:03,617 iteration 3141 : loss : 0.036528, loss_ce: 0.012169
2022-01-09 03:16:06,594 iteration 3142 : loss : 0.025472, loss_ce: 0.008615
2022-01-09 03:16:09,432 iteration 3143 : loss : 0.023707, loss_ce: 0.010213
2022-01-09 03:16:12,101 iteration 3144 : loss : 0.027559, loss_ce: 0.012825
2022-01-09 03:16:12,102 Training Data Eval:
2022-01-09 03:16:27,202   Average segmentation loss on training set: 0.0198
2022-01-09 03:16:27,202 Validation Data Eval:
2022-01-09 03:16:32,394   Average segmentation loss on validation set: 0.0955
2022-01-09 03:16:35,002 iteration 3145 : loss : 0.033755, loss_ce: 0.014698
 46%|████████████▍              | 185/400 [2:38:28<3:17:11, 55.03s/it]2022-01-09 03:16:37,737 iteration 3146 : loss : 0.035243, loss_ce: 0.013386
2022-01-09 03:16:40,668 iteration 3147 : loss : 0.029972, loss_ce: 0.012897
2022-01-09 03:16:43,355 iteration 3148 : loss : 0.027018, loss_ce: 0.008562
2022-01-09 03:16:46,193 iteration 3149 : loss : 0.017834, loss_ce: 0.005785
2022-01-09 03:16:49,013 iteration 3150 : loss : 0.056224, loss_ce: 0.032253
2022-01-09 03:16:51,762 iteration 3151 : loss : 0.032670, loss_ce: 0.008208
2022-01-09 03:16:54,597 iteration 3152 : loss : 0.024521, loss_ce: 0.010027
2022-01-09 03:16:57,336 iteration 3153 : loss : 0.031919, loss_ce: 0.010381
2022-01-09 03:17:00,199 iteration 3154 : loss : 0.025215, loss_ce: 0.011177
2022-01-09 03:17:02,877 iteration 3155 : loss : 0.032619, loss_ce: 0.011915
2022-01-09 03:17:05,795 iteration 3156 : loss : 0.037123, loss_ce: 0.015956
2022-01-09 03:17:08,458 iteration 3157 : loss : 0.024932, loss_ce: 0.010144
2022-01-09 03:17:11,228 iteration 3158 : loss : 0.032071, loss_ce: 0.014056
2022-01-09 03:17:14,045 iteration 3159 : loss : 0.023688, loss_ce: 0.009538
2022-01-09 03:17:16,719 iteration 3160 : loss : 0.026528, loss_ce: 0.011269
2022-01-09 03:17:19,506 iteration 3161 : loss : 0.025821, loss_ce: 0.010387
2022-01-09 03:17:22,568 iteration 3162 : loss : 0.025479, loss_ce: 0.009764
 46%|████████████▌              | 186/400 [2:39:15<3:08:17, 52.79s/it]2022-01-09 03:17:25,476 iteration 3163 : loss : 0.036148, loss_ce: 0.016283
2022-01-09 03:17:28,145 iteration 3164 : loss : 0.026825, loss_ce: 0.010746
2022-01-09 03:17:30,881 iteration 3165 : loss : 0.020572, loss_ce: 0.007333
2022-01-09 03:17:33,719 iteration 3166 : loss : 0.023914, loss_ce: 0.008613
2022-01-09 03:17:36,436 iteration 3167 : loss : 0.015243, loss_ce: 0.006370
2022-01-09 03:17:39,403 iteration 3168 : loss : 0.033705, loss_ce: 0.016110
2022-01-09 03:17:42,303 iteration 3169 : loss : 0.034172, loss_ce: 0.017821
2022-01-09 03:17:45,146 iteration 3170 : loss : 0.023498, loss_ce: 0.010323
2022-01-09 03:17:47,979 iteration 3171 : loss : 0.019791, loss_ce: 0.009086
2022-01-09 03:17:50,786 iteration 3172 : loss : 0.027802, loss_ce: 0.010041
2022-01-09 03:17:53,782 iteration 3173 : loss : 0.019919, loss_ce: 0.006913
2022-01-09 03:17:56,639 iteration 3174 : loss : 0.031220, loss_ce: 0.014370
2022-01-09 03:17:59,565 iteration 3175 : loss : 0.053580, loss_ce: 0.014682
2022-01-09 03:18:02,186 iteration 3176 : loss : 0.025800, loss_ce: 0.008575
2022-01-09 03:18:05,205 iteration 3177 : loss : 0.043621, loss_ce: 0.017195
2022-01-09 03:18:07,806 iteration 3178 : loss : 0.022230, loss_ce: 0.006919
2022-01-09 03:18:10,549 iteration 3179 : loss : 0.038925, loss_ce: 0.011460
 47%|████████████▌              | 187/400 [2:40:03<3:02:16, 51.35s/it]2022-01-09 03:18:13,419 iteration 3180 : loss : 0.035597, loss_ce: 0.014428
2022-01-09 03:18:16,196 iteration 3181 : loss : 0.021636, loss_ce: 0.007876
2022-01-09 03:18:19,112 iteration 3182 : loss : 0.032393, loss_ce: 0.011796
2022-01-09 03:18:21,884 iteration 3183 : loss : 0.038178, loss_ce: 0.011772
2022-01-09 03:18:24,740 iteration 3184 : loss : 0.030286, loss_ce: 0.010873
2022-01-09 03:18:27,614 iteration 3185 : loss : 0.027524, loss_ce: 0.010802
2022-01-09 03:18:30,279 iteration 3186 : loss : 0.021165, loss_ce: 0.010006
2022-01-09 03:18:32,997 iteration 3187 : loss : 0.020619, loss_ce: 0.008057
2022-01-09 03:18:35,789 iteration 3188 : loss : 0.024912, loss_ce: 0.010885
2022-01-09 03:18:38,631 iteration 3189 : loss : 0.032058, loss_ce: 0.014560
2022-01-09 03:18:41,324 iteration 3190 : loss : 0.024685, loss_ce: 0.008655
2022-01-09 03:18:44,255 iteration 3191 : loss : 0.023504, loss_ce: 0.006204
2022-01-09 03:18:46,843 iteration 3192 : loss : 0.035126, loss_ce: 0.011534
2022-01-09 03:18:49,717 iteration 3193 : loss : 0.028528, loss_ce: 0.013674
2022-01-09 03:18:52,637 iteration 3194 : loss : 0.033375, loss_ce: 0.010394
2022-01-09 03:18:55,352 iteration 3195 : loss : 0.025180, loss_ce: 0.010846
2022-01-09 03:18:58,152 iteration 3196 : loss : 0.030883, loss_ce: 0.015845
 47%|████████████▋              | 188/400 [2:40:51<2:57:27, 50.22s/it]2022-01-09 03:19:00,975 iteration 3197 : loss : 0.025864, loss_ce: 0.010675
2022-01-09 03:19:03,812 iteration 3198 : loss : 0.030835, loss_ce: 0.011494
2022-01-09 03:19:06,678 iteration 3199 : loss : 0.032246, loss_ce: 0.013557
2022-01-09 03:19:09,570 iteration 3200 : loss : 0.033175, loss_ce: 0.015152
2022-01-09 03:19:12,324 iteration 3201 : loss : 0.038508, loss_ce: 0.011614
2022-01-09 03:19:15,109 iteration 3202 : loss : 0.029551, loss_ce: 0.010741
2022-01-09 03:19:18,050 iteration 3203 : loss : 0.027556, loss_ce: 0.010521
2022-01-09 03:19:20,713 iteration 3204 : loss : 0.026937, loss_ce: 0.010420
2022-01-09 03:19:23,564 iteration 3205 : loss : 0.031532, loss_ce: 0.009959
2022-01-09 03:19:26,359 iteration 3206 : loss : 0.024794, loss_ce: 0.010570
2022-01-09 03:19:29,238 iteration 3207 : loss : 0.029744, loss_ce: 0.012346
2022-01-09 03:19:31,870 iteration 3208 : loss : 0.029754, loss_ce: 0.008620
2022-01-09 03:19:34,649 iteration 3209 : loss : 0.025469, loss_ce: 0.013271
2022-01-09 03:19:37,191 iteration 3210 : loss : 0.023481, loss_ce: 0.011191
2022-01-09 03:19:40,014 iteration 3211 : loss : 0.028539, loss_ce: 0.008863
2022-01-09 03:19:42,687 iteration 3212 : loss : 0.023298, loss_ce: 0.008634
2022-01-09 03:19:45,590 iteration 3213 : loss : 0.026367, loss_ce: 0.007182
 47%|████████████▊              | 189/400 [2:41:38<2:53:41, 49.39s/it]2022-01-09 03:19:48,469 iteration 3214 : loss : 0.027638, loss_ce: 0.012296
2022-01-09 03:19:51,370 iteration 3215 : loss : 0.036810, loss_ce: 0.014090
2022-01-09 03:19:54,236 iteration 3216 : loss : 0.032258, loss_ce: 0.011372
2022-01-09 03:19:57,098 iteration 3217 : loss : 0.048428, loss_ce: 0.009292
2022-01-09 03:19:59,938 iteration 3218 : loss : 0.023448, loss_ce: 0.008362
2022-01-09 03:20:02,791 iteration 3219 : loss : 0.021705, loss_ce: 0.008315
2022-01-09 03:20:05,626 iteration 3220 : loss : 0.029489, loss_ce: 0.010064
2022-01-09 03:20:08,486 iteration 3221 : loss : 0.029231, loss_ce: 0.011318
2022-01-09 03:20:11,277 iteration 3222 : loss : 0.029128, loss_ce: 0.016618
2022-01-09 03:20:14,268 iteration 3223 : loss : 0.027545, loss_ce: 0.006717
2022-01-09 03:20:17,165 iteration 3224 : loss : 0.023027, loss_ce: 0.007831
2022-01-09 03:20:19,878 iteration 3225 : loss : 0.030784, loss_ce: 0.011026
2022-01-09 03:20:22,789 iteration 3226 : loss : 0.025939, loss_ce: 0.008173
2022-01-09 03:20:25,640 iteration 3227 : loss : 0.026354, loss_ce: 0.011167
2022-01-09 03:20:28,258 iteration 3228 : loss : 0.023214, loss_ce: 0.009887
2022-01-09 03:20:31,195 iteration 3229 : loss : 0.025485, loss_ce: 0.010221
2022-01-09 03:20:31,195 Training Data Eval:
2022-01-09 03:20:46,469   Average segmentation loss on training set: 0.0189
2022-01-09 03:20:46,469 Validation Data Eval:
2022-01-09 03:20:51,641   Average segmentation loss on validation set: 0.1058
2022-01-09 03:20:54,515 iteration 3230 : loss : 0.031206, loss_ce: 0.011020
 48%|████████████▊              | 190/400 [2:42:47<3:13:22, 55.25s/it]2022-01-09 03:20:57,351 iteration 3231 : loss : 0.022725, loss_ce: 0.008088
2022-01-09 03:21:00,061 iteration 3232 : loss : 0.022172, loss_ce: 0.010240
2022-01-09 03:21:02,883 iteration 3233 : loss : 0.025846, loss_ce: 0.007949
2022-01-09 03:21:05,462 iteration 3234 : loss : 0.037743, loss_ce: 0.014261
2022-01-09 03:21:08,056 iteration 3235 : loss : 0.017346, loss_ce: 0.004452
2022-01-09 03:21:10,923 iteration 3236 : loss : 0.024284, loss_ce: 0.009880
2022-01-09 03:21:13,776 iteration 3237 : loss : 0.019789, loss_ce: 0.008674
2022-01-09 03:21:16,560 iteration 3238 : loss : 0.022865, loss_ce: 0.005415
2022-01-09 03:21:19,269 iteration 3239 : loss : 0.020566, loss_ce: 0.008033
2022-01-09 03:21:22,090 iteration 3240 : loss : 0.022148, loss_ce: 0.007327
2022-01-09 03:21:24,943 iteration 3241 : loss : 0.025774, loss_ce: 0.010324
2022-01-09 03:21:27,843 iteration 3242 : loss : 0.025522, loss_ce: 0.010519
2022-01-09 03:21:30,661 iteration 3243 : loss : 0.024363, loss_ce: 0.011197
2022-01-09 03:21:33,513 iteration 3244 : loss : 0.050220, loss_ce: 0.010680
2022-01-09 03:21:36,127 iteration 3245 : loss : 0.021688, loss_ce: 0.008788
2022-01-09 03:21:38,964 iteration 3246 : loss : 0.040902, loss_ce: 0.017952
2022-01-09 03:21:41,785 iteration 3247 : loss : 0.025527, loss_ce: 0.011687
 48%|████████████▉              | 191/400 [2:43:34<3:04:07, 52.86s/it]2022-01-09 03:21:44,815 iteration 3248 : loss : 0.027542, loss_ce: 0.011589
2022-01-09 03:21:47,409 iteration 3249 : loss : 0.017859, loss_ce: 0.005021
2022-01-09 03:21:50,281 iteration 3250 : loss : 0.026546, loss_ce: 0.009945
2022-01-09 03:21:52,936 iteration 3251 : loss : 0.043109, loss_ce: 0.009548
2022-01-09 03:21:55,681 iteration 3252 : loss : 0.024629, loss_ce: 0.010172
2022-01-09 03:21:58,485 iteration 3253 : loss : 0.034649, loss_ce: 0.013918
2022-01-09 03:22:01,334 iteration 3254 : loss : 0.022895, loss_ce: 0.007048
2022-01-09 03:22:04,176 iteration 3255 : loss : 0.022971, loss_ce: 0.009569
2022-01-09 03:22:06,842 iteration 3256 : loss : 0.027317, loss_ce: 0.017502
2022-01-09 03:22:09,620 iteration 3257 : loss : 0.036543, loss_ce: 0.012990
2022-01-09 03:22:12,510 iteration 3258 : loss : 0.023176, loss_ce: 0.009273
2022-01-09 03:22:15,187 iteration 3259 : loss : 0.033342, loss_ce: 0.013516
2022-01-09 03:22:17,976 iteration 3260 : loss : 0.031410, loss_ce: 0.012255
2022-01-09 03:22:20,769 iteration 3261 : loss : 0.033165, loss_ce: 0.012724
2022-01-09 03:22:23,554 iteration 3262 : loss : 0.031850, loss_ce: 0.013606
2022-01-09 03:22:26,393 iteration 3263 : loss : 0.033730, loss_ce: 0.010160
2022-01-09 03:22:29,241 iteration 3264 : loss : 0.029383, loss_ce: 0.009292
 48%|████████████▉              | 192/400 [2:44:22<2:57:37, 51.24s/it]2022-01-09 03:22:32,094 iteration 3265 : loss : 0.017782, loss_ce: 0.006948
2022-01-09 03:22:35,034 iteration 3266 : loss : 0.022211, loss_ce: 0.009189
2022-01-09 03:22:37,941 iteration 3267 : loss : 0.020748, loss_ce: 0.009020
2022-01-09 03:22:40,851 iteration 3268 : loss : 0.029718, loss_ce: 0.013442
2022-01-09 03:22:43,492 iteration 3269 : loss : 0.022788, loss_ce: 0.008936
2022-01-09 03:22:46,278 iteration 3270 : loss : 0.019978, loss_ce: 0.007967
2022-01-09 03:22:49,151 iteration 3271 : loss : 0.023552, loss_ce: 0.007813
2022-01-09 03:22:51,959 iteration 3272 : loss : 0.020519, loss_ce: 0.008092
2022-01-09 03:22:54,571 iteration 3273 : loss : 0.029710, loss_ce: 0.011431
2022-01-09 03:22:57,385 iteration 3274 : loss : 0.020819, loss_ce: 0.006797
2022-01-09 03:23:00,183 iteration 3275 : loss : 0.019345, loss_ce: 0.007938
2022-01-09 03:23:03,028 iteration 3276 : loss : 0.022928, loss_ce: 0.008142
2022-01-09 03:23:05,850 iteration 3277 : loss : 0.026491, loss_ce: 0.011327
2022-01-09 03:23:08,686 iteration 3278 : loss : 0.020575, loss_ce: 0.007482
2022-01-09 03:23:11,583 iteration 3279 : loss : 0.035055, loss_ce: 0.013251
2022-01-09 03:23:14,232 iteration 3280 : loss : 0.023491, loss_ce: 0.010184
2022-01-09 03:23:17,088 iteration 3281 : loss : 0.020317, loss_ce: 0.006509
 48%|█████████████              | 193/400 [2:45:10<2:53:15, 50.22s/it]2022-01-09 03:23:19,756 iteration 3282 : loss : 0.025624, loss_ce: 0.013219
2022-01-09 03:23:22,692 iteration 3283 : loss : 0.032190, loss_ce: 0.011289
2022-01-09 03:23:25,402 iteration 3284 : loss : 0.023699, loss_ce: 0.010943
2022-01-09 03:23:28,215 iteration 3285 : loss : 0.018009, loss_ce: 0.006399
2022-01-09 03:23:31,074 iteration 3286 : loss : 0.028646, loss_ce: 0.007556
2022-01-09 03:23:33,921 iteration 3287 : loss : 0.025744, loss_ce: 0.012037
2022-01-09 03:23:36,561 iteration 3288 : loss : 0.021418, loss_ce: 0.008237
2022-01-09 03:23:39,427 iteration 3289 : loss : 0.023549, loss_ce: 0.006883
2022-01-09 03:23:42,044 iteration 3290 : loss : 0.019953, loss_ce: 0.005231
2022-01-09 03:23:44,938 iteration 3291 : loss : 0.025065, loss_ce: 0.008566
2022-01-09 03:23:47,579 iteration 3292 : loss : 0.036326, loss_ce: 0.016788
2022-01-09 03:23:50,353 iteration 3293 : loss : 0.017294, loss_ce: 0.006422
2022-01-09 03:23:53,020 iteration 3294 : loss : 0.016523, loss_ce: 0.006700
2022-01-09 03:23:55,882 iteration 3295 : loss : 0.024467, loss_ce: 0.010903
2022-01-09 03:23:58,702 iteration 3296 : loss : 0.025554, loss_ce: 0.010547
2022-01-09 03:24:01,508 iteration 3297 : loss : 0.026452, loss_ce: 0.010851
2022-01-09 03:24:04,339 iteration 3298 : loss : 0.028424, loss_ce: 0.010596
 48%|█████████████              | 194/400 [2:45:57<2:49:21, 49.33s/it]2022-01-09 03:24:07,328 iteration 3299 : loss : 0.015679, loss_ce: 0.006098
2022-01-09 03:24:09,988 iteration 3300 : loss : 0.021096, loss_ce: 0.008441
2022-01-09 03:24:12,885 iteration 3301 : loss : 0.027182, loss_ce: 0.009143
2022-01-09 03:24:15,618 iteration 3302 : loss : 0.023106, loss_ce: 0.007513
2022-01-09 03:24:18,490 iteration 3303 : loss : 0.022557, loss_ce: 0.007796
2022-01-09 03:24:21,328 iteration 3304 : loss : 0.022500, loss_ce: 0.010305
2022-01-09 03:24:24,173 iteration 3305 : loss : 0.022182, loss_ce: 0.008307
2022-01-09 03:24:26,780 iteration 3306 : loss : 0.016139, loss_ce: 0.006658
2022-01-09 03:24:29,660 iteration 3307 : loss : 0.025424, loss_ce: 0.009427
2022-01-09 03:24:32,535 iteration 3308 : loss : 0.031565, loss_ce: 0.010514
2022-01-09 03:24:35,313 iteration 3309 : loss : 0.017234, loss_ce: 0.006692
2022-01-09 03:24:38,167 iteration 3310 : loss : 0.036257, loss_ce: 0.009544
2022-01-09 03:24:40,954 iteration 3311 : loss : 0.029620, loss_ce: 0.009337
2022-01-09 03:24:43,534 iteration 3312 : loss : 0.016962, loss_ce: 0.007600
2022-01-09 03:24:46,373 iteration 3313 : loss : 0.038864, loss_ce: 0.016555
2022-01-09 03:24:49,011 iteration 3314 : loss : 0.020706, loss_ce: 0.008731
2022-01-09 03:24:49,012 Training Data Eval:
2022-01-09 03:25:04,073   Average segmentation loss on training set: 0.0209
2022-01-09 03:25:04,073 Validation Data Eval:
2022-01-09 03:25:09,365   Average segmentation loss on validation set: 0.1303
2022-01-09 03:25:12,145 iteration 3315 : loss : 0.028127, loss_ce: 0.010314
 49%|█████████████▏             | 195/400 [2:47:05<3:07:27, 54.87s/it]2022-01-09 03:25:15,174 iteration 3316 : loss : 0.020508, loss_ce: 0.005048
2022-01-09 03:25:18,012 iteration 3317 : loss : 0.029760, loss_ce: 0.010828
2022-01-09 03:25:20,709 iteration 3318 : loss : 0.032667, loss_ce: 0.014557
2022-01-09 03:25:23,376 iteration 3319 : loss : 0.023608, loss_ce: 0.010244
2022-01-09 03:25:26,274 iteration 3320 : loss : 0.027106, loss_ce: 0.009024
2022-01-09 03:25:29,113 iteration 3321 : loss : 0.020454, loss_ce: 0.010807
2022-01-09 03:25:31,869 iteration 3322 : loss : 0.019676, loss_ce: 0.007470
2022-01-09 03:25:34,520 iteration 3323 : loss : 0.026148, loss_ce: 0.011478
2022-01-09 03:25:37,292 iteration 3324 : loss : 0.020929, loss_ce: 0.006776
2022-01-09 03:25:40,155 iteration 3325 : loss : 0.023556, loss_ce: 0.007951
2022-01-09 03:25:42,989 iteration 3326 : loss : 0.024701, loss_ce: 0.009015
2022-01-09 03:25:45,992 iteration 3327 : loss : 0.036966, loss_ce: 0.010433
2022-01-09 03:25:48,618 iteration 3328 : loss : 0.023855, loss_ce: 0.013063
2022-01-09 03:25:51,388 iteration 3329 : loss : 0.021849, loss_ce: 0.008205
2022-01-09 03:25:54,173 iteration 3330 : loss : 0.029810, loss_ce: 0.014161
2022-01-09 03:25:56,974 iteration 3331 : loss : 0.022348, loss_ce: 0.008539
2022-01-09 03:25:59,664 iteration 3332 : loss : 0.022328, loss_ce: 0.007500
 49%|█████████████▏             | 196/400 [2:47:52<2:59:03, 52.66s/it]2022-01-09 03:26:02,647 iteration 3333 : loss : 0.027307, loss_ce: 0.012307
2022-01-09 03:26:05,504 iteration 3334 : loss : 0.017893, loss_ce: 0.007333
2022-01-09 03:26:08,378 iteration 3335 : loss : 0.031466, loss_ce: 0.013968
2022-01-09 03:26:11,188 iteration 3336 : loss : 0.021625, loss_ce: 0.007582
2022-01-09 03:26:14,032 iteration 3337 : loss : 0.021931, loss_ce: 0.007801
2022-01-09 03:26:16,847 iteration 3338 : loss : 0.023434, loss_ce: 0.008114
2022-01-09 03:26:19,704 iteration 3339 : loss : 0.029377, loss_ce: 0.008462
2022-01-09 03:26:22,423 iteration 3340 : loss : 0.027343, loss_ce: 0.012188
2022-01-09 03:26:25,161 iteration 3341 : loss : 0.020417, loss_ce: 0.008614
2022-01-09 03:26:28,027 iteration 3342 : loss : 0.027063, loss_ce: 0.014074
2022-01-09 03:26:30,668 iteration 3343 : loss : 0.025471, loss_ce: 0.010132
2022-01-09 03:26:33,566 iteration 3344 : loss : 0.027425, loss_ce: 0.009543
2022-01-09 03:26:36,390 iteration 3345 : loss : 0.024856, loss_ce: 0.012646
2022-01-09 03:26:39,187 iteration 3346 : loss : 0.035182, loss_ce: 0.011228
2022-01-09 03:26:41,878 iteration 3347 : loss : 0.024261, loss_ce: 0.009616
2022-01-09 03:26:44,797 iteration 3348 : loss : 0.022300, loss_ce: 0.009184
2022-01-09 03:26:47,420 iteration 3349 : loss : 0.029348, loss_ce: 0.007614
 49%|█████████████▎             | 197/400 [2:48:40<2:53:11, 51.19s/it]2022-01-09 03:26:50,205 iteration 3350 : loss : 0.018703, loss_ce: 0.008385
2022-01-09 03:26:53,092 iteration 3351 : loss : 0.017131, loss_ce: 0.006094
2022-01-09 03:26:55,910 iteration 3352 : loss : 0.025159, loss_ce: 0.008031
2022-01-09 03:26:58,543 iteration 3353 : loss : 0.037992, loss_ce: 0.013869
2022-01-09 03:27:01,319 iteration 3354 : loss : 0.016878, loss_ce: 0.004925
2022-01-09 03:27:04,147 iteration 3355 : loss : 0.017818, loss_ce: 0.006711
2022-01-09 03:27:06,880 iteration 3356 : loss : 0.033074, loss_ce: 0.014457
2022-01-09 03:27:09,662 iteration 3357 : loss : 0.018786, loss_ce: 0.007996
2022-01-09 03:27:12,481 iteration 3358 : loss : 0.016405, loss_ce: 0.005232
2022-01-09 03:27:15,118 iteration 3359 : loss : 0.046079, loss_ce: 0.020066
2022-01-09 03:27:17,976 iteration 3360 : loss : 0.023634, loss_ce: 0.008984
2022-01-09 03:27:20,679 iteration 3361 : loss : 0.022132, loss_ce: 0.009188
2022-01-09 03:27:23,375 iteration 3362 : loss : 0.022314, loss_ce: 0.007582
2022-01-09 03:27:26,125 iteration 3363 : loss : 0.024768, loss_ce: 0.008153
2022-01-09 03:27:28,996 iteration 3364 : loss : 0.026974, loss_ce: 0.010589
2022-01-09 03:27:31,633 iteration 3365 : loss : 0.027271, loss_ce: 0.011547
2022-01-09 03:27:34,379 iteration 3366 : loss : 0.023517, loss_ce: 0.009491
 50%|█████████████▎             | 198/400 [2:49:27<2:48:04, 49.93s/it]2022-01-09 03:27:37,344 iteration 3367 : loss : 0.032424, loss_ce: 0.013674
2022-01-09 03:27:40,167 iteration 3368 : loss : 0.027589, loss_ce: 0.013480
2022-01-09 03:27:42,995 iteration 3369 : loss : 0.028326, loss_ce: 0.012112
2022-01-09 03:27:45,800 iteration 3370 : loss : 0.032663, loss_ce: 0.009112
2022-01-09 03:27:48,439 iteration 3371 : loss : 0.018927, loss_ce: 0.007418
2022-01-09 03:27:51,286 iteration 3372 : loss : 0.035069, loss_ce: 0.010759
2022-01-09 03:27:54,088 iteration 3373 : loss : 0.021316, loss_ce: 0.008168
2022-01-09 03:27:56,823 iteration 3374 : loss : 0.023730, loss_ce: 0.010932
2022-01-09 03:27:59,772 iteration 3375 : loss : 0.041105, loss_ce: 0.017226
2022-01-09 03:28:02,533 iteration 3376 : loss : 0.016155, loss_ce: 0.005219
2022-01-09 03:28:05,232 iteration 3377 : loss : 0.020529, loss_ce: 0.008071
2022-01-09 03:28:08,022 iteration 3378 : loss : 0.024589, loss_ce: 0.007817
2022-01-09 03:28:10,782 iteration 3379 : loss : 0.028888, loss_ce: 0.010811
2022-01-09 03:28:13,505 iteration 3380 : loss : 0.021581, loss_ce: 0.006388
2022-01-09 03:28:16,182 iteration 3381 : loss : 0.034989, loss_ce: 0.020567
2022-01-09 03:28:19,055 iteration 3382 : loss : 0.048260, loss_ce: 0.017766
2022-01-09 03:28:21,899 iteration 3383 : loss : 0.028328, loss_ce: 0.013843
 50%|█████████████▍             | 199/400 [2:50:15<2:44:50, 49.20s/it]2022-01-09 03:28:24,706 iteration 3384 : loss : 0.024567, loss_ce: 0.008699
2022-01-09 03:28:27,576 iteration 3385 : loss : 0.021472, loss_ce: 0.008506
2022-01-09 03:28:30,361 iteration 3386 : loss : 0.021164, loss_ce: 0.006751
2022-01-09 03:28:32,935 iteration 3387 : loss : 0.028930, loss_ce: 0.008739
2022-01-09 03:28:35,766 iteration 3388 : loss : 0.025979, loss_ce: 0.010440
2022-01-09 03:28:38,517 iteration 3389 : loss : 0.025121, loss_ce: 0.009894
2022-01-09 03:28:41,151 iteration 3390 : loss : 0.028173, loss_ce: 0.013828
2022-01-09 03:28:44,041 iteration 3391 : loss : 0.019159, loss_ce: 0.009340
2022-01-09 03:28:46,897 iteration 3392 : loss : 0.024697, loss_ce: 0.009597
2022-01-09 03:28:49,723 iteration 3393 : loss : 0.024343, loss_ce: 0.009937
2022-01-09 03:28:52,414 iteration 3394 : loss : 0.027527, loss_ce: 0.009386
2022-01-09 03:28:55,428 iteration 3395 : loss : 0.023634, loss_ce: 0.009872
2022-01-09 03:28:58,315 iteration 3396 : loss : 0.024032, loss_ce: 0.009861
2022-01-09 03:29:01,254 iteration 3397 : loss : 0.021122, loss_ce: 0.008348
2022-01-09 03:29:04,170 iteration 3398 : loss : 0.025877, loss_ce: 0.009519
2022-01-09 03:29:07,017 iteration 3399 : loss : 0.029353, loss_ce: 0.014522
2022-01-09 03:29:07,018 Training Data Eval:
2022-01-09 03:29:22,065   Average segmentation loss on training set: 0.0180
2022-01-09 03:29:22,066 Validation Data Eval:
2022-01-09 03:29:27,280   Average segmentation loss on validation set: 0.0621
2022-01-09 03:29:33,070 Found new lowest validation loss at iteration 3399! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 03:29:34,977 iteration 3400 : loss : 0.028395, loss_ce: 0.008972
 50%|█████████████▌             | 200/400 [2:51:28<3:07:52, 56.36s/it]2022-01-09 03:29:37,705 iteration 3401 : loss : 0.031483, loss_ce: 0.013202
2022-01-09 03:29:40,411 iteration 3402 : loss : 0.027870, loss_ce: 0.009823
2022-01-09 03:29:43,342 iteration 3403 : loss : 0.031207, loss_ce: 0.013571
2022-01-09 03:29:45,968 iteration 3404 : loss : 0.021722, loss_ce: 0.006454
2022-01-09 03:29:48,752 iteration 3405 : loss : 0.028692, loss_ce: 0.008560
2022-01-09 03:29:51,560 iteration 3406 : loss : 0.020515, loss_ce: 0.009619
2022-01-09 03:29:54,354 iteration 3407 : loss : 0.026509, loss_ce: 0.009821
2022-01-09 03:29:56,976 iteration 3408 : loss : 0.018398, loss_ce: 0.006256
2022-01-09 03:29:59,672 iteration 3409 : loss : 0.032933, loss_ce: 0.006041
2022-01-09 03:30:02,495 iteration 3410 : loss : 0.026771, loss_ce: 0.011642
2022-01-09 03:30:05,265 iteration 3411 : loss : 0.034490, loss_ce: 0.009723
2022-01-09 03:30:08,330 iteration 3412 : loss : 0.025079, loss_ce: 0.012482
2022-01-09 03:30:11,262 iteration 3413 : loss : 0.027707, loss_ce: 0.011783
2022-01-09 03:30:14,123 iteration 3414 : loss : 0.033932, loss_ce: 0.010189
2022-01-09 03:30:16,704 iteration 3415 : loss : 0.021532, loss_ce: 0.008924
2022-01-09 03:30:19,670 iteration 3416 : loss : 0.028298, loss_ce: 0.012744
2022-01-09 03:30:22,310 iteration 3417 : loss : 0.019569, loss_ce: 0.007987
 50%|█████████████▌             | 201/400 [2:52:15<2:57:57, 53.65s/it]2022-01-09 03:30:25,237 iteration 3418 : loss : 0.030700, loss_ce: 0.009134
2022-01-09 03:30:28,059 iteration 3419 : loss : 0.033740, loss_ce: 0.018173
2022-01-09 03:30:30,653 iteration 3420 : loss : 0.021784, loss_ce: 0.010827
2022-01-09 03:30:33,495 iteration 3421 : loss : 0.032876, loss_ce: 0.008801
2022-01-09 03:30:36,255 iteration 3422 : loss : 0.023024, loss_ce: 0.007441
2022-01-09 03:30:39,117 iteration 3423 : loss : 0.026543, loss_ce: 0.011515
2022-01-09 03:30:41,868 iteration 3424 : loss : 0.018049, loss_ce: 0.005794
2022-01-09 03:30:44,730 iteration 3425 : loss : 0.027621, loss_ce: 0.013854
2022-01-09 03:30:47,538 iteration 3426 : loss : 0.018127, loss_ce: 0.005284
2022-01-09 03:30:50,312 iteration 3427 : loss : 0.032603, loss_ce: 0.012726
2022-01-09 03:30:53,124 iteration 3428 : loss : 0.027536, loss_ce: 0.011010
2022-01-09 03:30:55,946 iteration 3429 : loss : 0.031211, loss_ce: 0.015538
2022-01-09 03:30:58,787 iteration 3430 : loss : 0.017794, loss_ce: 0.005916
2022-01-09 03:31:01,649 iteration 3431 : loss : 0.026363, loss_ce: 0.010299
2022-01-09 03:31:04,511 iteration 3432 : loss : 0.021396, loss_ce: 0.006367
2022-01-09 03:31:07,414 iteration 3433 : loss : 0.025595, loss_ce: 0.006666
2022-01-09 03:31:10,118 iteration 3434 : loss : 0.019516, loss_ce: 0.005707
 50%|█████████████▋             | 202/400 [2:53:03<2:51:17, 51.90s/it]2022-01-09 03:31:12,951 iteration 3435 : loss : 0.019215, loss_ce: 0.008718
2022-01-09 03:31:15,779 iteration 3436 : loss : 0.029257, loss_ce: 0.013716
2022-01-09 03:31:18,507 iteration 3437 : loss : 0.018005, loss_ce: 0.005690
2022-01-09 03:31:21,302 iteration 3438 : loss : 0.028993, loss_ce: 0.013436
2022-01-09 03:31:24,262 iteration 3439 : loss : 0.029246, loss_ce: 0.014483
2022-01-09 03:31:27,196 iteration 3440 : loss : 0.027604, loss_ce: 0.008766
2022-01-09 03:31:29,715 iteration 3441 : loss : 0.020075, loss_ce: 0.010369
2022-01-09 03:31:32,505 iteration 3442 : loss : 0.016845, loss_ce: 0.004849
2022-01-09 03:31:35,345 iteration 3443 : loss : 0.024037, loss_ce: 0.010004
2022-01-09 03:31:38,026 iteration 3444 : loss : 0.019699, loss_ce: 0.006449
2022-01-09 03:31:40,844 iteration 3445 : loss : 0.019180, loss_ce: 0.006960
2022-01-09 03:31:43,642 iteration 3446 : loss : 0.042662, loss_ce: 0.017166
2022-01-09 03:31:46,474 iteration 3447 : loss : 0.021475, loss_ce: 0.010500
2022-01-09 03:31:49,072 iteration 3448 : loss : 0.021906, loss_ce: 0.007813
2022-01-09 03:31:51,871 iteration 3449 : loss : 0.032032, loss_ce: 0.009137
2022-01-09 03:31:54,738 iteration 3450 : loss : 0.022834, loss_ce: 0.007437
2022-01-09 03:31:57,327 iteration 3451 : loss : 0.017412, loss_ce: 0.007359
 51%|█████████████▋             | 203/400 [2:53:50<2:45:47, 50.50s/it]2022-01-09 03:32:00,220 iteration 3452 : loss : 0.017063, loss_ce: 0.006242
2022-01-09 03:32:03,083 iteration 3453 : loss : 0.027507, loss_ce: 0.008941
2022-01-09 03:32:05,975 iteration 3454 : loss : 0.048576, loss_ce: 0.015734
2022-01-09 03:32:08,823 iteration 3455 : loss : 0.023552, loss_ce: 0.009428
2022-01-09 03:32:11,637 iteration 3456 : loss : 0.025203, loss_ce: 0.010142
2022-01-09 03:32:14,306 iteration 3457 : loss : 0.023659, loss_ce: 0.008096
2022-01-09 03:32:17,219 iteration 3458 : loss : 0.017414, loss_ce: 0.009147
2022-01-09 03:32:20,095 iteration 3459 : loss : 0.034943, loss_ce: 0.020403
2022-01-09 03:32:22,902 iteration 3460 : loss : 0.023383, loss_ce: 0.008228
2022-01-09 03:32:25,691 iteration 3461 : loss : 0.033429, loss_ce: 0.013304
2022-01-09 03:32:28,360 iteration 3462 : loss : 0.018441, loss_ce: 0.007241
2022-01-09 03:32:31,154 iteration 3463 : loss : 0.015763, loss_ce: 0.004894
2022-01-09 03:32:33,966 iteration 3464 : loss : 0.020415, loss_ce: 0.007587
2022-01-09 03:32:36,592 iteration 3465 : loss : 0.023666, loss_ce: 0.008072
2022-01-09 03:32:39,394 iteration 3466 : loss : 0.019147, loss_ce: 0.007777
2022-01-09 03:32:42,328 iteration 3467 : loss : 0.031509, loss_ce: 0.012602
2022-01-09 03:32:45,049 iteration 3468 : loss : 0.023170, loss_ce: 0.007481
 51%|█████████████▊             | 204/400 [2:54:38<2:42:13, 49.66s/it]2022-01-09 03:32:47,831 iteration 3469 : loss : 0.019780, loss_ce: 0.007620
2022-01-09 03:32:50,560 iteration 3470 : loss : 0.027016, loss_ce: 0.009774
2022-01-09 03:32:53,339 iteration 3471 : loss : 0.022397, loss_ce: 0.008489
2022-01-09 03:32:56,210 iteration 3472 : loss : 0.023638, loss_ce: 0.009260
2022-01-09 03:32:59,074 iteration 3473 : loss : 0.018770, loss_ce: 0.009050
2022-01-09 03:33:01,875 iteration 3474 : loss : 0.026656, loss_ce: 0.011089
2022-01-09 03:33:04,765 iteration 3475 : loss : 0.045279, loss_ce: 0.019305
2022-01-09 03:33:07,392 iteration 3476 : loss : 0.020352, loss_ce: 0.007345
2022-01-09 03:33:10,270 iteration 3477 : loss : 0.027042, loss_ce: 0.008842
2022-01-09 03:33:12,972 iteration 3478 : loss : 0.031170, loss_ce: 0.009957
2022-01-09 03:33:15,863 iteration 3479 : loss : 0.027558, loss_ce: 0.011701
2022-01-09 03:33:18,667 iteration 3480 : loss : 0.017744, loss_ce: 0.006885
2022-01-09 03:33:21,327 iteration 3481 : loss : 0.021151, loss_ce: 0.011177
2022-01-09 03:33:24,202 iteration 3482 : loss : 0.024074, loss_ce: 0.008894
2022-01-09 03:33:27,063 iteration 3483 : loss : 0.022771, loss_ce: 0.008501
2022-01-09 03:33:29,790 iteration 3484 : loss : 0.020601, loss_ce: 0.005383
2022-01-09 03:33:29,790 Training Data Eval:
2022-01-09 03:33:44,790   Average segmentation loss on training set: 0.0152
2022-01-09 03:33:44,791 Validation Data Eval:
2022-01-09 03:33:50,015   Average segmentation loss on validation set: 0.0696
2022-01-09 03:33:52,862 iteration 3485 : loss : 0.022005, loss_ce: 0.008237
 51%|█████████████▊             | 205/400 [2:55:46<2:59:06, 55.11s/it]2022-01-09 03:33:55,700 iteration 3486 : loss : 0.023550, loss_ce: 0.007659
2022-01-09 03:33:58,420 iteration 3487 : loss : 0.019497, loss_ce: 0.007248
2022-01-09 03:34:01,224 iteration 3488 : loss : 0.029405, loss_ce: 0.017075
2022-01-09 03:34:03,972 iteration 3489 : loss : 0.025608, loss_ce: 0.008169
2022-01-09 03:34:06,823 iteration 3490 : loss : 0.023835, loss_ce: 0.008968
2022-01-09 03:34:09,660 iteration 3491 : loss : 0.026616, loss_ce: 0.014410
2022-01-09 03:34:12,407 iteration 3492 : loss : 0.017721, loss_ce: 0.005063
2022-01-09 03:34:15,292 iteration 3493 : loss : 0.039429, loss_ce: 0.010924
2022-01-09 03:34:18,127 iteration 3494 : loss : 0.026086, loss_ce: 0.013282
2022-01-09 03:34:20,713 iteration 3495 : loss : 0.015784, loss_ce: 0.006794
2022-01-09 03:34:23,591 iteration 3496 : loss : 0.044809, loss_ce: 0.012322
2022-01-09 03:34:26,454 iteration 3497 : loss : 0.023810, loss_ce: 0.011399
2022-01-09 03:34:29,318 iteration 3498 : loss : 0.021730, loss_ce: 0.008575
2022-01-09 03:34:32,183 iteration 3499 : loss : 0.046913, loss_ce: 0.011316
2022-01-09 03:34:35,014 iteration 3500 : loss : 0.022435, loss_ce: 0.006353
2022-01-09 03:34:37,759 iteration 3501 : loss : 0.023540, loss_ce: 0.009136
2022-01-09 03:34:40,667 iteration 3502 : loss : 0.023934, loss_ce: 0.008412
 52%|█████████████▉             | 206/400 [2:56:33<2:51:05, 52.91s/it]2022-01-09 03:34:43,472 iteration 3503 : loss : 0.019541, loss_ce: 0.009424
2022-01-09 03:34:46,110 iteration 3504 : loss : 0.020254, loss_ce: 0.006777
2022-01-09 03:34:48,745 iteration 3505 : loss : 0.024056, loss_ce: 0.012577
2022-01-09 03:34:51,675 iteration 3506 : loss : 0.035354, loss_ce: 0.013045
2022-01-09 03:34:54,506 iteration 3507 : loss : 0.019627, loss_ce: 0.008241
2022-01-09 03:34:57,226 iteration 3508 : loss : 0.019198, loss_ce: 0.006394
2022-01-09 03:35:00,111 iteration 3509 : loss : 0.020534, loss_ce: 0.008641
2022-01-09 03:35:03,051 iteration 3510 : loss : 0.032971, loss_ce: 0.012036
2022-01-09 03:35:05,670 iteration 3511 : loss : 0.023072, loss_ce: 0.010417
2022-01-09 03:35:08,460 iteration 3512 : loss : 0.024474, loss_ce: 0.007913
2022-01-09 03:35:11,233 iteration 3513 : loss : 0.023345, loss_ce: 0.008057
2022-01-09 03:35:13,877 iteration 3514 : loss : 0.015190, loss_ce: 0.006510
2022-01-09 03:35:16,826 iteration 3515 : loss : 0.017785, loss_ce: 0.006052
2022-01-09 03:35:19,696 iteration 3516 : loss : 0.027570, loss_ce: 0.009288
2022-01-09 03:35:22,365 iteration 3517 : loss : 0.019960, loss_ce: 0.008170
2022-01-09 03:35:25,131 iteration 3518 : loss : 0.015805, loss_ce: 0.006673
2022-01-09 03:35:27,961 iteration 3519 : loss : 0.027061, loss_ce: 0.009200
 52%|█████████████▉             | 207/400 [2:57:21<2:44:47, 51.23s/it]2022-01-09 03:35:30,888 iteration 3520 : loss : 0.023229, loss_ce: 0.007730
2022-01-09 03:35:33,693 iteration 3521 : loss : 0.021987, loss_ce: 0.008926
2022-01-09 03:35:36,514 iteration 3522 : loss : 0.034380, loss_ce: 0.012822
2022-01-09 03:35:39,430 iteration 3523 : loss : 0.032218, loss_ce: 0.011014
2022-01-09 03:35:42,254 iteration 3524 : loss : 0.019216, loss_ce: 0.005405
2022-01-09 03:35:45,101 iteration 3525 : loss : 0.023444, loss_ce: 0.010253
2022-01-09 03:35:47,850 iteration 3526 : loss : 0.022467, loss_ce: 0.011089
2022-01-09 03:35:50,643 iteration 3527 : loss : 0.029015, loss_ce: 0.007660
2022-01-09 03:35:53,341 iteration 3528 : loss : 0.017947, loss_ce: 0.008743
2022-01-09 03:35:56,189 iteration 3529 : loss : 0.027413, loss_ce: 0.010200
2022-01-09 03:35:58,976 iteration 3530 : loss : 0.030593, loss_ce: 0.010831
2022-01-09 03:36:01,714 iteration 3531 : loss : 0.020443, loss_ce: 0.008237
2022-01-09 03:36:04,318 iteration 3532 : loss : 0.016254, loss_ce: 0.005729
2022-01-09 03:36:07,081 iteration 3533 : loss : 0.022581, loss_ce: 0.010126
2022-01-09 03:36:09,706 iteration 3534 : loss : 0.021029, loss_ce: 0.010946
2022-01-09 03:36:12,533 iteration 3535 : loss : 0.022030, loss_ce: 0.007617
2022-01-09 03:36:15,100 iteration 3536 : loss : 0.020153, loss_ce: 0.007035
 52%|██████████████             | 208/400 [2:58:08<2:40:00, 50.00s/it]2022-01-09 03:36:17,935 iteration 3537 : loss : 0.019890, loss_ce: 0.007157
2022-01-09 03:36:20,800 iteration 3538 : loss : 0.027720, loss_ce: 0.008578
2022-01-09 03:36:23,494 iteration 3539 : loss : 0.037779, loss_ce: 0.016744
2022-01-09 03:36:26,264 iteration 3540 : loss : 0.024012, loss_ce: 0.005876
2022-01-09 03:36:28,978 iteration 3541 : loss : 0.018435, loss_ce: 0.007664
2022-01-09 03:36:31,815 iteration 3542 : loss : 0.025759, loss_ce: 0.006537
2022-01-09 03:36:34,683 iteration 3543 : loss : 0.022368, loss_ce: 0.009551
2022-01-09 03:36:37,475 iteration 3544 : loss : 0.023874, loss_ce: 0.008985
2022-01-09 03:36:40,255 iteration 3545 : loss : 0.019156, loss_ce: 0.006505
2022-01-09 03:36:43,015 iteration 3546 : loss : 0.019684, loss_ce: 0.009269
2022-01-09 03:36:45,720 iteration 3547 : loss : 0.022217, loss_ce: 0.007948
2022-01-09 03:36:48,609 iteration 3548 : loss : 0.020285, loss_ce: 0.006376
2022-01-09 03:36:51,305 iteration 3549 : loss : 0.020876, loss_ce: 0.009103
2022-01-09 03:36:54,011 iteration 3550 : loss : 0.019363, loss_ce: 0.006351
2022-01-09 03:36:56,934 iteration 3551 : loss : 0.045875, loss_ce: 0.012094
2022-01-09 03:36:59,850 iteration 3552 : loss : 0.024535, loss_ce: 0.010077
2022-01-09 03:37:02,700 iteration 3553 : loss : 0.031802, loss_ce: 0.009686
 52%|██████████████             | 209/400 [2:58:55<2:36:52, 49.28s/it]2022-01-09 03:37:05,605 iteration 3554 : loss : 0.022631, loss_ce: 0.008763
2022-01-09 03:37:08,323 iteration 3555 : loss : 0.023323, loss_ce: 0.010276
2022-01-09 03:37:11,052 iteration 3556 : loss : 0.028315, loss_ce: 0.009120
2022-01-09 03:37:13,836 iteration 3557 : loss : 0.021439, loss_ce: 0.008792
2022-01-09 03:37:16,504 iteration 3558 : loss : 0.017762, loss_ce: 0.005765
2022-01-09 03:37:19,356 iteration 3559 : loss : 0.023112, loss_ce: 0.008503
2022-01-09 03:37:22,173 iteration 3560 : loss : 0.039712, loss_ce: 0.009385
2022-01-09 03:37:24,795 iteration 3561 : loss : 0.023938, loss_ce: 0.011111
2022-01-09 03:37:27,691 iteration 3562 : loss : 0.024922, loss_ce: 0.008758
2022-01-09 03:37:30,534 iteration 3563 : loss : 0.019922, loss_ce: 0.006833
2022-01-09 03:37:33,430 iteration 3564 : loss : 0.038449, loss_ce: 0.011201
2022-01-09 03:37:36,158 iteration 3565 : loss : 0.018243, loss_ce: 0.007426
2022-01-09 03:37:38,870 iteration 3566 : loss : 0.016456, loss_ce: 0.006051
2022-01-09 03:37:41,653 iteration 3567 : loss : 0.025964, loss_ce: 0.009060
2022-01-09 03:37:44,509 iteration 3568 : loss : 0.025196, loss_ce: 0.008358
2022-01-09 03:37:47,312 iteration 3569 : loss : 0.024493, loss_ce: 0.010811
2022-01-09 03:37:47,312 Training Data Eval:
2022-01-09 03:38:02,565   Average segmentation loss on training set: 0.0172
2022-01-09 03:38:02,566 Validation Data Eval:
2022-01-09 03:38:07,741   Average segmentation loss on validation set: 0.0659
2022-01-09 03:38:10,423 iteration 3570 : loss : 0.017670, loss_ce: 0.005298
 52%|██████████████▏            | 210/400 [3:00:03<2:53:34, 54.82s/it]2022-01-09 03:38:13,172 iteration 3571 : loss : 0.033719, loss_ce: 0.014188
2022-01-09 03:38:16,079 iteration 3572 : loss : 0.022086, loss_ce: 0.008732
2022-01-09 03:38:18,900 iteration 3573 : loss : 0.027887, loss_ce: 0.009487
2022-01-09 03:38:21,505 iteration 3574 : loss : 0.020176, loss_ce: 0.010084
2022-01-09 03:38:24,254 iteration 3575 : loss : 0.021192, loss_ce: 0.008540
2022-01-09 03:38:26,957 iteration 3576 : loss : 0.038245, loss_ce: 0.014118
2022-01-09 03:38:29,799 iteration 3577 : loss : 0.021162, loss_ce: 0.008354
2022-01-09 03:38:32,632 iteration 3578 : loss : 0.022219, loss_ce: 0.008780
2022-01-09 03:38:35,298 iteration 3579 : loss : 0.019807, loss_ce: 0.007906
2022-01-09 03:38:38,177 iteration 3580 : loss : 0.025820, loss_ce: 0.014035
2022-01-09 03:38:40,892 iteration 3581 : loss : 0.024715, loss_ce: 0.009718
2022-01-09 03:38:43,638 iteration 3582 : loss : 0.033096, loss_ce: 0.011086
2022-01-09 03:38:46,417 iteration 3583 : loss : 0.043939, loss_ce: 0.016551
2022-01-09 03:38:49,217 iteration 3584 : loss : 0.019286, loss_ce: 0.006511
2022-01-09 03:38:51,914 iteration 3585 : loss : 0.016897, loss_ce: 0.006825
2022-01-09 03:38:54,485 iteration 3586 : loss : 0.017444, loss_ce: 0.005916
2022-01-09 03:38:57,279 iteration 3587 : loss : 0.016674, loss_ce: 0.007181
 53%|██████████████▏            | 211/400 [3:00:50<2:45:08, 52.43s/it]2022-01-09 03:39:00,216 iteration 3588 : loss : 0.019707, loss_ce: 0.008007
2022-01-09 03:39:03,065 iteration 3589 : loss : 0.026677, loss_ce: 0.007351
2022-01-09 03:39:05,737 iteration 3590 : loss : 0.022632, loss_ce: 0.006256
2022-01-09 03:39:08,532 iteration 3591 : loss : 0.022684, loss_ce: 0.009247
2022-01-09 03:39:11,372 iteration 3592 : loss : 0.017430, loss_ce: 0.006967
2022-01-09 03:39:14,191 iteration 3593 : loss : 0.028576, loss_ce: 0.015020
2022-01-09 03:39:16,889 iteration 3594 : loss : 0.021503, loss_ce: 0.006195
2022-01-09 03:39:19,746 iteration 3595 : loss : 0.033335, loss_ce: 0.010960
2022-01-09 03:39:22,337 iteration 3596 : loss : 0.018966, loss_ce: 0.007942
2022-01-09 03:39:25,170 iteration 3597 : loss : 0.038804, loss_ce: 0.017853
2022-01-09 03:39:28,003 iteration 3598 : loss : 0.022251, loss_ce: 0.009257
2022-01-09 03:39:30,824 iteration 3599 : loss : 0.035852, loss_ce: 0.010804
2022-01-09 03:39:33,397 iteration 3600 : loss : 0.025683, loss_ce: 0.012601
2022-01-09 03:39:36,192 iteration 3601 : loss : 0.025634, loss_ce: 0.011817
2022-01-09 03:39:39,037 iteration 3602 : loss : 0.048948, loss_ce: 0.018534
2022-01-09 03:39:41,867 iteration 3603 : loss : 0.022296, loss_ce: 0.011176
2022-01-09 03:39:44,709 iteration 3604 : loss : 0.047233, loss_ce: 0.014411
 53%|██████████████▎            | 212/400 [3:01:37<2:39:34, 50.93s/it]2022-01-09 03:39:47,650 iteration 3605 : loss : 0.025214, loss_ce: 0.010285
2022-01-09 03:39:50,313 iteration 3606 : loss : 0.023934, loss_ce: 0.005489
2022-01-09 03:39:53,112 iteration 3607 : loss : 0.013092, loss_ce: 0.004394
2022-01-09 03:39:56,131 iteration 3608 : loss : 0.022474, loss_ce: 0.007623
2022-01-09 03:39:58,839 iteration 3609 : loss : 0.040439, loss_ce: 0.017941
2022-01-09 03:40:01,526 iteration 3610 : loss : 0.014663, loss_ce: 0.005525
2022-01-09 03:40:04,192 iteration 3611 : loss : 0.024641, loss_ce: 0.010246
2022-01-09 03:40:06,967 iteration 3612 : loss : 0.020629, loss_ce: 0.008455
2022-01-09 03:40:09,756 iteration 3613 : loss : 0.021765, loss_ce: 0.008760
2022-01-09 03:40:12,452 iteration 3614 : loss : 0.023096, loss_ce: 0.008852
2022-01-09 03:40:15,264 iteration 3615 : loss : 0.027664, loss_ce: 0.009833
2022-01-09 03:40:18,095 iteration 3616 : loss : 0.019341, loss_ce: 0.008997
2022-01-09 03:40:20,961 iteration 3617 : loss : 0.024187, loss_ce: 0.011334
2022-01-09 03:40:23,838 iteration 3618 : loss : 0.021196, loss_ce: 0.007242
2022-01-09 03:40:26,703 iteration 3619 : loss : 0.017236, loss_ce: 0.005891
2022-01-09 03:40:29,497 iteration 3620 : loss : 0.023395, loss_ce: 0.007171
2022-01-09 03:40:32,438 iteration 3621 : loss : 0.020607, loss_ce: 0.010175
 53%|██████████████▍            | 213/400 [3:02:25<2:35:43, 49.97s/it]2022-01-09 03:40:35,324 iteration 3622 : loss : 0.027346, loss_ce: 0.008392
2022-01-09 03:40:38,179 iteration 3623 : loss : 0.034973, loss_ce: 0.014260
2022-01-09 03:40:40,889 iteration 3624 : loss : 0.026774, loss_ce: 0.010623
2022-01-09 03:40:43,537 iteration 3625 : loss : 0.019123, loss_ce: 0.007027
2022-01-09 03:40:46,307 iteration 3626 : loss : 0.031868, loss_ce: 0.012723
2022-01-09 03:40:49,187 iteration 3627 : loss : 0.032094, loss_ce: 0.013790
2022-01-09 03:40:51,854 iteration 3628 : loss : 0.026650, loss_ce: 0.008306
2022-01-09 03:40:54,551 iteration 3629 : loss : 0.028368, loss_ce: 0.012665
2022-01-09 03:40:57,341 iteration 3630 : loss : 0.034876, loss_ce: 0.014046
2022-01-09 03:41:00,159 iteration 3631 : loss : 0.019163, loss_ce: 0.006593
2022-01-09 03:41:02,970 iteration 3632 : loss : 0.027291, loss_ce: 0.014998
2022-01-09 03:41:05,862 iteration 3633 : loss : 0.022604, loss_ce: 0.008075
2022-01-09 03:41:08,452 iteration 3634 : loss : 0.018104, loss_ce: 0.005322
2022-01-09 03:41:11,350 iteration 3635 : loss : 0.027494, loss_ce: 0.013671
2022-01-09 03:41:14,163 iteration 3636 : loss : 0.016092, loss_ce: 0.007222
2022-01-09 03:41:16,827 iteration 3637 : loss : 0.019179, loss_ce: 0.007119
2022-01-09 03:41:19,690 iteration 3638 : loss : 0.021094, loss_ce: 0.007263
 54%|██████████████▍            | 214/400 [3:03:12<2:32:22, 49.15s/it]2022-01-09 03:41:22,542 iteration 3639 : loss : 0.047516, loss_ce: 0.013283
2022-01-09 03:41:25,244 iteration 3640 : loss : 0.024232, loss_ce: 0.006798
2022-01-09 03:41:28,146 iteration 3641 : loss : 0.027849, loss_ce: 0.009143
2022-01-09 03:41:30,895 iteration 3642 : loss : 0.027901, loss_ce: 0.011437
2022-01-09 03:41:33,894 iteration 3643 : loss : 0.029348, loss_ce: 0.008376
2022-01-09 03:41:36,748 iteration 3644 : loss : 0.017555, loss_ce: 0.006503
2022-01-09 03:41:39,628 iteration 3645 : loss : 0.036595, loss_ce: 0.008543
2022-01-09 03:41:42,479 iteration 3646 : loss : 0.024590, loss_ce: 0.011403
2022-01-09 03:41:45,287 iteration 3647 : loss : 0.020281, loss_ce: 0.008827
2022-01-09 03:41:48,106 iteration 3648 : loss : 0.029128, loss_ce: 0.011830
2022-01-09 03:41:50,870 iteration 3649 : loss : 0.021732, loss_ce: 0.008484
2022-01-09 03:41:53,501 iteration 3650 : loss : 0.019206, loss_ce: 0.008049
2022-01-09 03:41:56,244 iteration 3651 : loss : 0.020342, loss_ce: 0.007390
2022-01-09 03:41:59,024 iteration 3652 : loss : 0.034686, loss_ce: 0.014192
2022-01-09 03:42:01,815 iteration 3653 : loss : 0.025946, loss_ce: 0.010306
2022-01-09 03:42:04,647 iteration 3654 : loss : 0.016853, loss_ce: 0.006452
2022-01-09 03:42:04,648 Training Data Eval:
2022-01-09 03:42:19,477   Average segmentation loss on training set: 0.0148
2022-01-09 03:42:19,478 Validation Data Eval:
2022-01-09 03:42:24,734   Average segmentation loss on validation set: 0.0790
2022-01-09 03:42:27,578 iteration 3655 : loss : 0.031855, loss_ce: 0.011203
 54%|██████████████▌            | 215/400 [3:04:20<2:48:52, 54.77s/it]2022-01-09 03:42:30,435 iteration 3656 : loss : 0.020443, loss_ce: 0.006293
2022-01-09 03:42:33,232 iteration 3657 : loss : 0.024010, loss_ce: 0.010143
2022-01-09 03:42:35,954 iteration 3658 : loss : 0.033457, loss_ce: 0.014451
2022-01-09 03:42:38,740 iteration 3659 : loss : 0.027969, loss_ce: 0.010249
2022-01-09 03:42:41,543 iteration 3660 : loss : 0.024569, loss_ce: 0.008858
2022-01-09 03:42:44,408 iteration 3661 : loss : 0.029235, loss_ce: 0.011632
2022-01-09 03:42:47,195 iteration 3662 : loss : 0.024002, loss_ce: 0.012323
2022-01-09 03:42:49,843 iteration 3663 : loss : 0.018884, loss_ce: 0.007700
2022-01-09 03:42:52,663 iteration 3664 : loss : 0.019906, loss_ce: 0.005376
2022-01-09 03:42:55,608 iteration 3665 : loss : 0.023102, loss_ce: 0.006559
2022-01-09 03:42:58,436 iteration 3666 : loss : 0.021662, loss_ce: 0.009435
2022-01-09 03:43:01,246 iteration 3667 : loss : 0.023788, loss_ce: 0.009070
2022-01-09 03:43:04,022 iteration 3668 : loss : 0.031351, loss_ce: 0.010191
2022-01-09 03:43:06,810 iteration 3669 : loss : 0.031397, loss_ce: 0.010304
2022-01-09 03:43:09,585 iteration 3670 : loss : 0.019058, loss_ce: 0.008386
2022-01-09 03:43:12,391 iteration 3671 : loss : 0.035797, loss_ce: 0.008181
2022-01-09 03:43:15,030 iteration 3672 : loss : 0.019543, loss_ce: 0.005579
 54%|██████████████▌            | 216/400 [3:05:08<2:41:13, 52.57s/it]2022-01-09 03:43:17,776 iteration 3673 : loss : 0.019901, loss_ce: 0.008980
2022-01-09 03:43:20,540 iteration 3674 : loss : 0.017795, loss_ce: 0.008162
2022-01-09 03:43:23,507 iteration 3675 : loss : 0.028418, loss_ce: 0.010788
2022-01-09 03:43:26,398 iteration 3676 : loss : 0.022016, loss_ce: 0.006562
2022-01-09 03:43:29,079 iteration 3677 : loss : 0.024854, loss_ce: 0.011595
2022-01-09 03:43:31,943 iteration 3678 : loss : 0.029148, loss_ce: 0.009515
2022-01-09 03:43:34,713 iteration 3679 : loss : 0.020674, loss_ce: 0.006637
2022-01-09 03:43:37,449 iteration 3680 : loss : 0.025557, loss_ce: 0.010350
2022-01-09 03:43:40,235 iteration 3681 : loss : 0.024965, loss_ce: 0.009334
2022-01-09 03:43:43,074 iteration 3682 : loss : 0.018032, loss_ce: 0.005538
2022-01-09 03:43:45,734 iteration 3683 : loss : 0.018649, loss_ce: 0.008176
2022-01-09 03:43:48,630 iteration 3684 : loss : 0.019502, loss_ce: 0.006387
2022-01-09 03:43:51,385 iteration 3685 : loss : 0.022270, loss_ce: 0.008147
2022-01-09 03:43:54,251 iteration 3686 : loss : 0.024163, loss_ce: 0.009054
2022-01-09 03:43:57,130 iteration 3687 : loss : 0.023173, loss_ce: 0.009211
2022-01-09 03:43:59,950 iteration 3688 : loss : 0.023289, loss_ce: 0.009235
2022-01-09 03:44:02,764 iteration 3689 : loss : 0.022883, loss_ce: 0.007062
 54%|██████████████▋            | 217/400 [3:05:55<2:35:55, 51.12s/it]2022-01-09 03:44:05,771 iteration 3690 : loss : 0.028406, loss_ce: 0.013587
2022-01-09 03:44:08,403 iteration 3691 : loss : 0.027768, loss_ce: 0.007729
2022-01-09 03:44:11,235 iteration 3692 : loss : 0.028390, loss_ce: 0.010414
2022-01-09 03:44:13,817 iteration 3693 : loss : 0.015881, loss_ce: 0.004098
2022-01-09 03:44:16,557 iteration 3694 : loss : 0.024552, loss_ce: 0.007544
2022-01-09 03:44:19,352 iteration 3695 : loss : 0.016454, loss_ce: 0.005179
2022-01-09 03:44:22,217 iteration 3696 : loss : 0.016012, loss_ce: 0.006464
2022-01-09 03:44:24,794 iteration 3697 : loss : 0.029407, loss_ce: 0.010702
2022-01-09 03:44:27,717 iteration 3698 : loss : 0.021339, loss_ce: 0.006569
2022-01-09 03:44:30,342 iteration 3699 : loss : 0.020047, loss_ce: 0.010134
2022-01-09 03:44:33,123 iteration 3700 : loss : 0.018963, loss_ce: 0.005638
2022-01-09 03:44:35,833 iteration 3701 : loss : 0.028753, loss_ce: 0.015368
2022-01-09 03:44:38,652 iteration 3702 : loss : 0.026830, loss_ce: 0.010880
2022-01-09 03:44:41,448 iteration 3703 : loss : 0.018543, loss_ce: 0.006403
2022-01-09 03:44:44,239 iteration 3704 : loss : 0.023176, loss_ce: 0.008953
2022-01-09 03:44:47,048 iteration 3705 : loss : 0.037602, loss_ce: 0.018701
2022-01-09 03:44:49,612 iteration 3706 : loss : 0.022946, loss_ce: 0.007825
 55%|██████████████▋            | 218/400 [3:06:42<2:31:11, 49.84s/it]2022-01-09 03:44:52,533 iteration 3707 : loss : 0.057524, loss_ce: 0.010122
2022-01-09 03:44:55,466 iteration 3708 : loss : 0.032885, loss_ce: 0.011360
2022-01-09 03:44:58,139 iteration 3709 : loss : 0.024211, loss_ce: 0.007510
2022-01-09 03:45:00,883 iteration 3710 : loss : 0.022327, loss_ce: 0.008060
2022-01-09 03:45:03,745 iteration 3711 : loss : 0.026014, loss_ce: 0.010736
2022-01-09 03:45:06,597 iteration 3712 : loss : 0.031918, loss_ce: 0.010665
2022-01-09 03:45:09,351 iteration 3713 : loss : 0.017719, loss_ce: 0.006368
2022-01-09 03:45:12,157 iteration 3714 : loss : 0.028635, loss_ce: 0.012092
2022-01-09 03:45:14,946 iteration 3715 : loss : 0.022478, loss_ce: 0.007526
2022-01-09 03:45:17,745 iteration 3716 : loss : 0.028546, loss_ce: 0.013861
2022-01-09 03:45:20,575 iteration 3717 : loss : 0.022694, loss_ce: 0.010633
2022-01-09 03:45:23,462 iteration 3718 : loss : 0.034721, loss_ce: 0.012497
2022-01-09 03:45:26,350 iteration 3719 : loss : 0.031905, loss_ce: 0.010911
2022-01-09 03:45:29,236 iteration 3720 : loss : 0.026443, loss_ce: 0.010644
2022-01-09 03:45:32,093 iteration 3721 : loss : 0.028389, loss_ce: 0.010821
2022-01-09 03:45:34,958 iteration 3722 : loss : 0.024850, loss_ce: 0.007240
2022-01-09 03:45:37,790 iteration 3723 : loss : 0.022439, loss_ce: 0.009719
 55%|██████████████▊            | 219/400 [3:07:30<2:28:50, 49.34s/it]2022-01-09 03:45:40,668 iteration 3724 : loss : 0.025952, loss_ce: 0.013852
2022-01-09 03:45:43,534 iteration 3725 : loss : 0.031991, loss_ce: 0.013287
2022-01-09 03:45:46,388 iteration 3726 : loss : 0.017600, loss_ce: 0.007248
2022-01-09 03:45:49,258 iteration 3727 : loss : 0.017048, loss_ce: 0.005764
2022-01-09 03:45:51,859 iteration 3728 : loss : 0.019843, loss_ce: 0.009209
2022-01-09 03:45:54,690 iteration 3729 : loss : 0.023581, loss_ce: 0.008071
2022-01-09 03:45:57,463 iteration 3730 : loss : 0.062318, loss_ce: 0.026162
2022-01-09 03:46:00,318 iteration 3731 : loss : 0.019801, loss_ce: 0.007823
2022-01-09 03:46:02,948 iteration 3732 : loss : 0.020552, loss_ce: 0.009146
2022-01-09 03:46:05,681 iteration 3733 : loss : 0.019689, loss_ce: 0.006073
2022-01-09 03:46:08,513 iteration 3734 : loss : 0.030832, loss_ce: 0.012513
2022-01-09 03:46:11,324 iteration 3735 : loss : 0.029458, loss_ce: 0.009733
2022-01-09 03:46:14,135 iteration 3736 : loss : 0.029852, loss_ce: 0.012919
2022-01-09 03:46:16,987 iteration 3737 : loss : 0.033731, loss_ce: 0.013959
2022-01-09 03:46:19,616 iteration 3738 : loss : 0.039191, loss_ce: 0.011390
2022-01-09 03:46:22,462 iteration 3739 : loss : 0.028678, loss_ce: 0.008047
2022-01-09 03:46:22,462 Training Data Eval:
2022-01-09 03:46:37,610   Average segmentation loss on training set: 0.0177
2022-01-09 03:46:37,611 Validation Data Eval:
2022-01-09 03:46:42,924   Average segmentation loss on validation set: 0.0686
2022-01-09 03:46:45,680 iteration 3740 : loss : 0.024901, loss_ce: 0.008341
 55%|██████████████▊            | 220/400 [3:08:38<2:44:43, 54.91s/it]2022-01-09 03:46:48,394 iteration 3741 : loss : 0.032553, loss_ce: 0.015111
2022-01-09 03:46:51,161 iteration 3742 : loss : 0.023164, loss_ce: 0.007545
2022-01-09 03:46:53,986 iteration 3743 : loss : 0.026357, loss_ce: 0.007917
2022-01-09 03:46:56,792 iteration 3744 : loss : 0.016652, loss_ce: 0.005922
2022-01-09 03:46:59,465 iteration 3745 : loss : 0.032932, loss_ce: 0.020967
2022-01-09 03:47:02,332 iteration 3746 : loss : 0.020377, loss_ce: 0.005030
2022-01-09 03:47:05,144 iteration 3747 : loss : 0.034789, loss_ce: 0.020966
2022-01-09 03:47:08,185 iteration 3748 : loss : 0.028766, loss_ce: 0.009406
2022-01-09 03:47:10,860 iteration 3749 : loss : 0.024171, loss_ce: 0.006887
2022-01-09 03:47:13,753 iteration 3750 : loss : 0.026580, loss_ce: 0.009278
2022-01-09 03:47:16,535 iteration 3751 : loss : 0.020540, loss_ce: 0.008676
2022-01-09 03:47:19,145 iteration 3752 : loss : 0.018486, loss_ce: 0.006647
2022-01-09 03:47:21,920 iteration 3753 : loss : 0.022074, loss_ce: 0.006446
2022-01-09 03:47:24,588 iteration 3754 : loss : 0.016182, loss_ce: 0.007802
2022-01-09 03:47:27,441 iteration 3755 : loss : 0.026971, loss_ce: 0.011106
2022-01-09 03:47:30,098 iteration 3756 : loss : 0.024241, loss_ce: 0.007368
2022-01-09 03:47:32,962 iteration 3757 : loss : 0.025694, loss_ce: 0.009517
 55%|██████████████▉            | 221/400 [3:09:26<2:36:59, 52.62s/it]2022-01-09 03:47:35,910 iteration 3758 : loss : 0.032317, loss_ce: 0.011914
2022-01-09 03:47:38,716 iteration 3759 : loss : 0.026208, loss_ce: 0.011838
2022-01-09 03:47:41,610 iteration 3760 : loss : 0.049715, loss_ce: 0.024201
2022-01-09 03:47:44,307 iteration 3761 : loss : 0.023301, loss_ce: 0.010502
2022-01-09 03:47:47,362 iteration 3762 : loss : 0.029980, loss_ce: 0.012029
2022-01-09 03:47:50,071 iteration 3763 : loss : 0.035979, loss_ce: 0.021056
2022-01-09 03:47:52,930 iteration 3764 : loss : 0.024579, loss_ce: 0.005787
2022-01-09 03:47:55,686 iteration 3765 : loss : 0.026521, loss_ce: 0.009087
2022-01-09 03:47:58,510 iteration 3766 : loss : 0.022569, loss_ce: 0.007078
2022-01-09 03:48:01,368 iteration 3767 : loss : 0.019107, loss_ce: 0.006512
2022-01-09 03:48:04,251 iteration 3768 : loss : 0.023734, loss_ce: 0.009053
2022-01-09 03:48:07,101 iteration 3769 : loss : 0.024236, loss_ce: 0.008927
2022-01-09 03:48:09,749 iteration 3770 : loss : 0.019757, loss_ce: 0.008546
2022-01-09 03:48:12,496 iteration 3771 : loss : 0.021035, loss_ce: 0.008784
2022-01-09 03:48:15,333 iteration 3772 : loss : 0.038779, loss_ce: 0.009624
2022-01-09 03:48:18,119 iteration 3773 : loss : 0.022667, loss_ce: 0.007103
2022-01-09 03:48:21,021 iteration 3774 : loss : 0.026851, loss_ce: 0.013309
 56%|██████████████▉            | 222/400 [3:10:14<2:32:03, 51.25s/it]2022-01-09 03:48:23,922 iteration 3775 : loss : 0.022026, loss_ce: 0.008980
2022-01-09 03:48:26,691 iteration 3776 : loss : 0.015985, loss_ce: 0.006223
2022-01-09 03:48:29,485 iteration 3777 : loss : 0.019965, loss_ce: 0.006680
2022-01-09 03:48:32,356 iteration 3778 : loss : 0.031021, loss_ce: 0.011556
2022-01-09 03:48:35,177 iteration 3779 : loss : 0.022932, loss_ce: 0.008216
2022-01-09 03:48:38,006 iteration 3780 : loss : 0.023575, loss_ce: 0.011018
2022-01-09 03:48:40,730 iteration 3781 : loss : 0.026408, loss_ce: 0.011222
2022-01-09 03:48:43,594 iteration 3782 : loss : 0.039669, loss_ce: 0.017635
2022-01-09 03:48:46,579 iteration 3783 : loss : 0.029242, loss_ce: 0.017454
2022-01-09 03:48:49,372 iteration 3784 : loss : 0.020178, loss_ce: 0.005139
2022-01-09 03:48:52,278 iteration 3785 : loss : 0.037794, loss_ce: 0.009735
2022-01-09 03:48:54,937 iteration 3786 : loss : 0.023964, loss_ce: 0.009722
2022-01-09 03:48:57,639 iteration 3787 : loss : 0.039870, loss_ce: 0.021980
2022-01-09 03:49:00,468 iteration 3788 : loss : 0.022869, loss_ce: 0.007881
2022-01-09 03:49:03,258 iteration 3789 : loss : 0.030134, loss_ce: 0.011568
2022-01-09 03:49:05,878 iteration 3790 : loss : 0.025994, loss_ce: 0.011367
2022-01-09 03:49:08,644 iteration 3791 : loss : 0.025228, loss_ce: 0.009106
 56%|███████████████            | 223/400 [3:11:01<2:27:58, 50.16s/it]2022-01-09 03:49:11,623 iteration 3792 : loss : 0.029524, loss_ce: 0.011367
2022-01-09 03:49:14,348 iteration 3793 : loss : 0.014224, loss_ce: 0.003936
2022-01-09 03:49:17,164 iteration 3794 : loss : 0.018706, loss_ce: 0.008199
2022-01-09 03:49:19,795 iteration 3795 : loss : 0.019099, loss_ce: 0.006167
2022-01-09 03:49:22,701 iteration 3796 : loss : 0.025941, loss_ce: 0.010235
2022-01-09 03:49:25,694 iteration 3797 : loss : 0.017329, loss_ce: 0.005248
2022-01-09 03:49:28,334 iteration 3798 : loss : 0.017288, loss_ce: 0.006755
2022-01-09 03:49:31,271 iteration 3799 : loss : 0.029936, loss_ce: 0.010292
2022-01-09 03:49:34,241 iteration 3800 : loss : 0.051480, loss_ce: 0.025145
2022-01-09 03:49:36,942 iteration 3801 : loss : 0.034377, loss_ce: 0.011327
2022-01-09 03:49:39,800 iteration 3802 : loss : 0.030550, loss_ce: 0.011264
2022-01-09 03:49:42,598 iteration 3803 : loss : 0.023420, loss_ce: 0.009935
2022-01-09 03:49:45,444 iteration 3804 : loss : 0.023013, loss_ce: 0.008563
2022-01-09 03:49:48,270 iteration 3805 : loss : 0.023041, loss_ce: 0.008137
2022-01-09 03:49:50,847 iteration 3806 : loss : 0.019060, loss_ce: 0.008558
2022-01-09 03:49:53,663 iteration 3807 : loss : 0.019044, loss_ce: 0.006716
2022-01-09 03:49:56,372 iteration 3808 : loss : 0.021868, loss_ce: 0.010016
 56%|███████████████            | 224/400 [3:11:49<2:24:59, 49.43s/it]2022-01-09 03:49:59,235 iteration 3809 : loss : 0.022632, loss_ce: 0.010029
2022-01-09 03:50:02,068 iteration 3810 : loss : 0.039234, loss_ce: 0.018081
2022-01-09 03:50:04,719 iteration 3811 : loss : 0.019869, loss_ce: 0.007194
2022-01-09 03:50:07,515 iteration 3812 : loss : 0.025766, loss_ce: 0.011648
2022-01-09 03:50:10,251 iteration 3813 : loss : 0.018929, loss_ce: 0.008520
2022-01-09 03:50:13,096 iteration 3814 : loss : 0.024851, loss_ce: 0.013108
2022-01-09 03:50:16,026 iteration 3815 : loss : 0.021418, loss_ce: 0.008459
2022-01-09 03:50:18,607 iteration 3816 : loss : 0.016691, loss_ce: 0.006156
2022-01-09 03:50:21,200 iteration 3817 : loss : 0.016848, loss_ce: 0.005586
2022-01-09 03:50:24,115 iteration 3818 : loss : 0.020155, loss_ce: 0.006851
2022-01-09 03:50:26,986 iteration 3819 : loss : 0.024053, loss_ce: 0.009001
2022-01-09 03:50:29,797 iteration 3820 : loss : 0.015668, loss_ce: 0.007204
2022-01-09 03:50:32,479 iteration 3821 : loss : 0.020169, loss_ce: 0.008561
2022-01-09 03:50:35,301 iteration 3822 : loss : 0.027144, loss_ce: 0.007870
2022-01-09 03:50:38,087 iteration 3823 : loss : 0.023106, loss_ce: 0.007660
2022-01-09 03:50:40,905 iteration 3824 : loss : 0.023727, loss_ce: 0.008933
2022-01-09 03:50:40,905 Training Data Eval:
2022-01-09 03:50:55,850   Average segmentation loss on training set: 0.0143
2022-01-09 03:50:55,850 Validation Data Eval:
2022-01-09 03:51:01,143   Average segmentation loss on validation set: 0.0744
2022-01-09 03:51:04,039 iteration 3825 : loss : 0.020197, loss_ce: 0.007066
 56%|███████████████▏           | 225/400 [3:12:57<2:40:08, 54.90s/it]2022-01-09 03:51:06,955 iteration 3826 : loss : 0.023172, loss_ce: 0.010057
2022-01-09 03:51:09,822 iteration 3827 : loss : 0.024813, loss_ce: 0.011190
2022-01-09 03:51:12,444 iteration 3828 : loss : 0.025118, loss_ce: 0.018893
2022-01-09 03:51:15,303 iteration 3829 : loss : 0.016037, loss_ce: 0.005729
2022-01-09 03:51:18,131 iteration 3830 : loss : 0.022304, loss_ce: 0.006197
2022-01-09 03:51:20,977 iteration 3831 : loss : 0.032218, loss_ce: 0.013338
2022-01-09 03:51:23,788 iteration 3832 : loss : 0.020520, loss_ce: 0.009958
2022-01-09 03:51:26,599 iteration 3833 : loss : 0.033157, loss_ce: 0.010550
2022-01-09 03:51:29,373 iteration 3834 : loss : 0.022432, loss_ce: 0.008303
2022-01-09 03:51:32,035 iteration 3835 : loss : 0.049181, loss_ce: 0.013152
2022-01-09 03:51:34,836 iteration 3836 : loss : 0.018151, loss_ce: 0.008329
2022-01-09 03:51:37,529 iteration 3837 : loss : 0.018743, loss_ce: 0.008416
2022-01-09 03:51:40,399 iteration 3838 : loss : 0.020602, loss_ce: 0.008546
2022-01-09 03:51:43,191 iteration 3839 : loss : 0.020154, loss_ce: 0.005434
2022-01-09 03:51:46,033 iteration 3840 : loss : 0.024636, loss_ce: 0.010257
2022-01-09 03:51:48,630 iteration 3841 : loss : 0.018442, loss_ce: 0.007231
2022-01-09 03:51:51,498 iteration 3842 : loss : 0.049546, loss_ce: 0.014476
 56%|███████████████▎           | 226/400 [3:13:44<2:32:44, 52.67s/it]2022-01-09 03:51:54,390 iteration 3843 : loss : 0.019517, loss_ce: 0.007280
2022-01-09 03:51:57,225 iteration 3844 : loss : 0.046731, loss_ce: 0.021177
2022-01-09 03:51:59,852 iteration 3845 : loss : 0.031509, loss_ce: 0.010015
2022-01-09 03:52:02,660 iteration 3846 : loss : 0.028530, loss_ce: 0.011599
2022-01-09 03:52:05,747 iteration 3847 : loss : 0.031452, loss_ce: 0.011949
2022-01-09 03:52:08,552 iteration 3848 : loss : 0.027096, loss_ce: 0.009098
2022-01-09 03:52:11,163 iteration 3849 : loss : 0.020604, loss_ce: 0.008153
2022-01-09 03:52:13,997 iteration 3850 : loss : 0.052128, loss_ce: 0.019692
2022-01-09 03:52:16,864 iteration 3851 : loss : 0.039845, loss_ce: 0.014446
2022-01-09 03:52:19,697 iteration 3852 : loss : 0.031049, loss_ce: 0.011589
2022-01-09 03:52:22,492 iteration 3853 : loss : 0.033337, loss_ce: 0.012766
2022-01-09 03:52:25,055 iteration 3854 : loss : 0.018901, loss_ce: 0.007170
2022-01-09 03:52:27,828 iteration 3855 : loss : 0.022238, loss_ce: 0.005526
2022-01-09 03:52:30,721 iteration 3856 : loss : 0.024312, loss_ce: 0.011484
2022-01-09 03:52:33,548 iteration 3857 : loss : 0.032155, loss_ce: 0.008639
2022-01-09 03:52:36,306 iteration 3858 : loss : 0.033618, loss_ce: 0.015288
2022-01-09 03:52:38,974 iteration 3859 : loss : 0.021342, loss_ce: 0.009773
 57%|███████████████▎           | 227/400 [3:14:32<2:27:21, 51.11s/it]2022-01-09 03:52:41,794 iteration 3860 : loss : 0.019512, loss_ce: 0.008933
2022-01-09 03:52:44,477 iteration 3861 : loss : 0.023577, loss_ce: 0.009601
2022-01-09 03:52:47,266 iteration 3862 : loss : 0.021507, loss_ce: 0.006035
2022-01-09 03:52:49,978 iteration 3863 : loss : 0.026202, loss_ce: 0.011805
2022-01-09 03:52:52,730 iteration 3864 : loss : 0.019854, loss_ce: 0.008040
2022-01-09 03:52:55,536 iteration 3865 : loss : 0.019400, loss_ce: 0.007946
2022-01-09 03:52:58,355 iteration 3866 : loss : 0.020548, loss_ce: 0.009122
2022-01-09 03:53:01,195 iteration 3867 : loss : 0.033254, loss_ce: 0.013162
2022-01-09 03:53:03,777 iteration 3868 : loss : 0.021332, loss_ce: 0.008107
2022-01-09 03:53:06,652 iteration 3869 : loss : 0.024007, loss_ce: 0.006846
2022-01-09 03:53:09,418 iteration 3870 : loss : 0.019955, loss_ce: 0.007609
2022-01-09 03:53:12,262 iteration 3871 : loss : 0.023230, loss_ce: 0.008330
2022-01-09 03:53:14,890 iteration 3872 : loss : 0.022817, loss_ce: 0.007654
2022-01-09 03:53:17,575 iteration 3873 : loss : 0.019303, loss_ce: 0.006275
2022-01-09 03:53:20,465 iteration 3874 : loss : 0.024335, loss_ce: 0.007723
2022-01-09 03:53:23,314 iteration 3875 : loss : 0.024495, loss_ce: 0.007768
2022-01-09 03:53:26,196 iteration 3876 : loss : 0.024564, loss_ce: 0.010873
 57%|███████████████▍           | 228/400 [3:15:19<2:23:10, 49.94s/it]2022-01-09 03:53:29,047 iteration 3877 : loss : 0.018710, loss_ce: 0.007644
2022-01-09 03:53:31,854 iteration 3878 : loss : 0.020374, loss_ce: 0.006921
2022-01-09 03:53:34,868 iteration 3879 : loss : 0.025308, loss_ce: 0.014417
2022-01-09 03:53:37,540 iteration 3880 : loss : 0.018703, loss_ce: 0.006570
2022-01-09 03:53:40,356 iteration 3881 : loss : 0.025280, loss_ce: 0.007964
2022-01-09 03:53:43,209 iteration 3882 : loss : 0.020481, loss_ce: 0.010016
2022-01-09 03:53:46,062 iteration 3883 : loss : 0.018983, loss_ce: 0.006500
2022-01-09 03:53:48,892 iteration 3884 : loss : 0.016970, loss_ce: 0.005846
2022-01-09 03:53:51,516 iteration 3885 : loss : 0.016187, loss_ce: 0.004785
2022-01-09 03:53:54,366 iteration 3886 : loss : 0.024969, loss_ce: 0.010454
2022-01-09 03:53:57,187 iteration 3887 : loss : 0.022096, loss_ce: 0.007715
2022-01-09 03:54:00,049 iteration 3888 : loss : 0.027418, loss_ce: 0.018340
2022-01-09 03:54:02,843 iteration 3889 : loss : 0.019669, loss_ce: 0.005944
2022-01-09 03:54:05,449 iteration 3890 : loss : 0.018321, loss_ce: 0.006663
2022-01-09 03:54:08,296 iteration 3891 : loss : 0.025537, loss_ce: 0.012356
2022-01-09 03:54:11,098 iteration 3892 : loss : 0.026151, loss_ce: 0.008706
2022-01-09 03:54:13,888 iteration 3893 : loss : 0.027854, loss_ce: 0.007195
 57%|███████████████▍           | 229/400 [3:16:07<2:20:24, 49.27s/it]2022-01-09 03:54:16,621 iteration 3894 : loss : 0.027425, loss_ce: 0.010345
2022-01-09 03:54:19,453 iteration 3895 : loss : 0.019592, loss_ce: 0.009055
2022-01-09 03:54:22,169 iteration 3896 : loss : 0.015818, loss_ce: 0.005612
2022-01-09 03:54:25,181 iteration 3897 : loss : 0.019837, loss_ce: 0.006136
2022-01-09 03:54:28,039 iteration 3898 : loss : 0.024909, loss_ce: 0.008565
2022-01-09 03:54:30,792 iteration 3899 : loss : 0.026048, loss_ce: 0.007843
2022-01-09 03:54:33,637 iteration 3900 : loss : 0.015880, loss_ce: 0.007034
2022-01-09 03:54:36,419 iteration 3901 : loss : 0.016389, loss_ce: 0.008343
2022-01-09 03:54:39,041 iteration 3902 : loss : 0.015222, loss_ce: 0.004902
2022-01-09 03:54:41,864 iteration 3903 : loss : 0.017867, loss_ce: 0.005430
2022-01-09 03:54:44,536 iteration 3904 : loss : 0.015749, loss_ce: 0.006887
2022-01-09 03:54:47,373 iteration 3905 : loss : 0.021507, loss_ce: 0.011045
2022-01-09 03:54:50,175 iteration 3906 : loss : 0.024069, loss_ce: 0.008523
2022-01-09 03:54:52,805 iteration 3907 : loss : 0.023066, loss_ce: 0.006579
2022-01-09 03:54:55,466 iteration 3908 : loss : 0.018795, loss_ce: 0.006453
2022-01-09 03:54:58,221 iteration 3909 : loss : 0.020677, loss_ce: 0.008208
2022-01-09 03:54:58,222 Training Data Eval:
2022-01-09 03:55:13,306   Average segmentation loss on training set: 0.0141
2022-01-09 03:55:13,306 Validation Data Eval:
2022-01-09 03:55:18,467   Average segmentation loss on validation set: 0.0724
2022-01-09 03:55:21,173 iteration 3910 : loss : 0.021017, loss_ce: 0.008954
 57%|███████████████▌           | 230/400 [3:17:14<2:34:54, 54.68s/it]2022-01-09 03:55:23,967 iteration 3911 : loss : 0.015905, loss_ce: 0.007243
2022-01-09 03:55:26,565 iteration 3912 : loss : 0.026894, loss_ce: 0.008163
2022-01-09 03:55:29,402 iteration 3913 : loss : 0.024117, loss_ce: 0.011219
2022-01-09 03:55:32,201 iteration 3914 : loss : 0.014003, loss_ce: 0.005921
2022-01-09 03:55:35,025 iteration 3915 : loss : 0.026125, loss_ce: 0.008128
2022-01-09 03:55:37,803 iteration 3916 : loss : 0.020611, loss_ce: 0.007485
2022-01-09 03:55:40,630 iteration 3917 : loss : 0.017877, loss_ce: 0.005883
2022-01-09 03:55:43,435 iteration 3918 : loss : 0.023282, loss_ce: 0.008072
2022-01-09 03:55:46,267 iteration 3919 : loss : 0.018165, loss_ce: 0.005937
2022-01-09 03:55:49,130 iteration 3920 : loss : 0.037506, loss_ce: 0.010268
2022-01-09 03:55:51,757 iteration 3921 : loss : 0.024793, loss_ce: 0.007331
2022-01-09 03:55:54,560 iteration 3922 : loss : 0.025453, loss_ce: 0.007452
2022-01-09 03:55:57,440 iteration 3923 : loss : 0.027644, loss_ce: 0.011672
2022-01-09 03:56:00,237 iteration 3924 : loss : 0.017011, loss_ce: 0.005981
2022-01-09 03:56:02,888 iteration 3925 : loss : 0.019340, loss_ce: 0.008749
2022-01-09 03:56:05,857 iteration 3926 : loss : 0.028650, loss_ce: 0.010416
2022-01-09 03:56:08,530 iteration 3927 : loss : 0.025271, loss_ce: 0.012769
 58%|███████████████▌           | 231/400 [3:18:01<2:27:49, 52.48s/it]2022-01-09 03:56:11,371 iteration 3928 : loss : 0.019897, loss_ce: 0.007372
2022-01-09 03:56:14,041 iteration 3929 : loss : 0.019799, loss_ce: 0.011587
2022-01-09 03:56:16,868 iteration 3930 : loss : 0.024007, loss_ce: 0.006708
2022-01-09 03:56:19,508 iteration 3931 : loss : 0.019793, loss_ce: 0.004519
2022-01-09 03:56:22,329 iteration 3932 : loss : 0.020616, loss_ce: 0.006948
2022-01-09 03:56:25,067 iteration 3933 : loss : 0.020159, loss_ce: 0.009009
2022-01-09 03:56:27,870 iteration 3934 : loss : 0.030507, loss_ce: 0.009656
2022-01-09 03:56:30,701 iteration 3935 : loss : 0.024139, loss_ce: 0.009373
2022-01-09 03:56:33,508 iteration 3936 : loss : 0.020977, loss_ce: 0.007426
2022-01-09 03:56:36,204 iteration 3937 : loss : 0.017678, loss_ce: 0.005272
2022-01-09 03:56:39,091 iteration 3938 : loss : 0.022416, loss_ce: 0.005577
2022-01-09 03:56:41,972 iteration 3939 : loss : 0.027593, loss_ce: 0.006052
2022-01-09 03:56:44,625 iteration 3940 : loss : 0.021359, loss_ce: 0.008778
2022-01-09 03:56:47,352 iteration 3941 : loss : 0.015408, loss_ce: 0.004512
2022-01-09 03:56:50,168 iteration 3942 : loss : 0.019300, loss_ce: 0.009874
2022-01-09 03:56:52,976 iteration 3943 : loss : 0.026254, loss_ce: 0.010265
2022-01-09 03:56:55,601 iteration 3944 : loss : 0.026202, loss_ce: 0.007238
 58%|███████████████▋           | 232/400 [3:18:48<2:22:24, 50.86s/it]2022-01-09 03:56:58,497 iteration 3945 : loss : 0.019447, loss_ce: 0.006302
2022-01-09 03:57:01,278 iteration 3946 : loss : 0.018760, loss_ce: 0.007669
2022-01-09 03:57:04,106 iteration 3947 : loss : 0.019385, loss_ce: 0.007992
2022-01-09 03:57:06,820 iteration 3948 : loss : 0.026307, loss_ce: 0.011370
2022-01-09 03:57:09,678 iteration 3949 : loss : 0.022535, loss_ce: 0.007210
2022-01-09 03:57:12,465 iteration 3950 : loss : 0.017473, loss_ce: 0.008314
2022-01-09 03:57:15,096 iteration 3951 : loss : 0.016922, loss_ce: 0.003650
2022-01-09 03:57:18,035 iteration 3952 : loss : 0.025321, loss_ce: 0.010763
2022-01-09 03:57:20,885 iteration 3953 : loss : 0.015122, loss_ce: 0.006389
2022-01-09 03:57:23,715 iteration 3954 : loss : 0.016838, loss_ce: 0.005994
2022-01-09 03:57:26,611 iteration 3955 : loss : 0.017394, loss_ce: 0.007047
2022-01-09 03:57:29,458 iteration 3956 : loss : 0.027424, loss_ce: 0.014129
2022-01-09 03:57:32,278 iteration 3957 : loss : 0.052329, loss_ce: 0.013730
2022-01-09 03:57:35,124 iteration 3958 : loss : 0.021130, loss_ce: 0.009814
2022-01-09 03:57:37,757 iteration 3959 : loss : 0.020640, loss_ce: 0.005139
2022-01-09 03:57:40,589 iteration 3960 : loss : 0.022900, loss_ce: 0.008545
2022-01-09 03:57:43,344 iteration 3961 : loss : 0.020426, loss_ce: 0.005474
 58%|███████████████▋           | 233/400 [3:19:36<2:18:57, 49.92s/it]2022-01-09 03:57:46,127 iteration 3962 : loss : 0.027939, loss_ce: 0.010037
2022-01-09 03:57:48,842 iteration 3963 : loss : 0.025142, loss_ce: 0.010074
2022-01-09 03:57:51,580 iteration 3964 : loss : 0.016842, loss_ce: 0.004922
2022-01-09 03:57:54,371 iteration 3965 : loss : 0.017287, loss_ce: 0.007200
2022-01-09 03:57:57,404 iteration 3966 : loss : 0.026729, loss_ce: 0.010943
2022-01-09 03:58:00,282 iteration 3967 : loss : 0.032972, loss_ce: 0.008960
2022-01-09 03:58:03,098 iteration 3968 : loss : 0.015156, loss_ce: 0.005018
2022-01-09 03:58:05,949 iteration 3969 : loss : 0.019966, loss_ce: 0.006358
2022-01-09 03:58:08,815 iteration 3970 : loss : 0.027664, loss_ce: 0.006750
2022-01-09 03:58:11,458 iteration 3971 : loss : 0.017851, loss_ce: 0.005701
2022-01-09 03:58:14,246 iteration 3972 : loss : 0.024840, loss_ce: 0.008553
2022-01-09 03:58:17,083 iteration 3973 : loss : 0.024623, loss_ce: 0.010723
2022-01-09 03:58:19,965 iteration 3974 : loss : 0.018167, loss_ce: 0.009228
2022-01-09 03:58:22,644 iteration 3975 : loss : 0.021415, loss_ce: 0.006019
2022-01-09 03:58:25,411 iteration 3976 : loss : 0.021851, loss_ce: 0.007411
2022-01-09 03:58:28,283 iteration 3977 : loss : 0.020785, loss_ce: 0.009279
2022-01-09 03:58:31,130 iteration 3978 : loss : 0.024729, loss_ce: 0.013029
 58%|███████████████▊           | 234/400 [3:20:24<2:16:20, 49.28s/it]2022-01-09 03:58:34,042 iteration 3979 : loss : 0.035831, loss_ce: 0.018236
2022-01-09 03:58:36,747 iteration 3980 : loss : 0.019822, loss_ce: 0.007860
2022-01-09 03:58:39,506 iteration 3981 : loss : 0.025081, loss_ce: 0.007470
2022-01-09 03:58:42,398 iteration 3982 : loss : 0.023387, loss_ce: 0.010465
2022-01-09 03:58:45,267 iteration 3983 : loss : 0.055676, loss_ce: 0.019116
2022-01-09 03:58:48,103 iteration 3984 : loss : 0.031342, loss_ce: 0.014109
2022-01-09 03:58:50,932 iteration 3985 : loss : 0.017556, loss_ce: 0.005984
2022-01-09 03:58:53,696 iteration 3986 : loss : 0.026759, loss_ce: 0.008102
2022-01-09 03:58:56,382 iteration 3987 : loss : 0.024562, loss_ce: 0.009693
2022-01-09 03:58:59,206 iteration 3988 : loss : 0.035642, loss_ce: 0.010186
2022-01-09 03:59:01,871 iteration 3989 : loss : 0.021812, loss_ce: 0.008591
2022-01-09 03:59:04,737 iteration 3990 : loss : 0.027925, loss_ce: 0.012104
2022-01-09 03:59:07,542 iteration 3991 : loss : 0.035976, loss_ce: 0.019036
2022-01-09 03:59:10,445 iteration 3992 : loss : 0.016990, loss_ce: 0.006643
2022-01-09 03:59:13,313 iteration 3993 : loss : 0.036676, loss_ce: 0.018167
2022-01-09 03:59:16,123 iteration 3994 : loss : 0.037290, loss_ce: 0.011771
2022-01-09 03:59:16,123 Training Data Eval:
2022-01-09 03:59:31,019   Average segmentation loss on training set: 0.0190
2022-01-09 03:59:31,019 Validation Data Eval:
2022-01-09 03:59:36,223   Average segmentation loss on validation set: 0.0710
2022-01-09 03:59:39,131 iteration 3995 : loss : 0.028413, loss_ce: 0.009584
 59%|███████████████▊           | 235/400 [3:21:32<2:30:57, 54.90s/it]2022-01-09 03:59:41,985 iteration 3996 : loss : 0.023059, loss_ce: 0.010413
2022-01-09 03:59:44,724 iteration 3997 : loss : 0.028522, loss_ce: 0.012690
2022-01-09 03:59:47,516 iteration 3998 : loss : 0.026668, loss_ce: 0.009007
2022-01-09 03:59:50,336 iteration 3999 : loss : 0.045544, loss_ce: 0.014303
2022-01-09 03:59:53,141 iteration 4000 : loss : 0.019080, loss_ce: 0.007895
2022-01-09 03:59:55,967 iteration 4001 : loss : 0.023010, loss_ce: 0.012725
2022-01-09 03:59:58,511 iteration 4002 : loss : 0.025870, loss_ce: 0.008072
2022-01-09 04:00:01,408 iteration 4003 : loss : 0.028588, loss_ce: 0.008876
2022-01-09 04:00:04,043 iteration 4004 : loss : 0.038857, loss_ce: 0.020708
2022-01-09 04:00:06,883 iteration 4005 : loss : 0.018752, loss_ce: 0.007299
2022-01-09 04:00:09,503 iteration 4006 : loss : 0.030116, loss_ce: 0.009017
2022-01-09 04:00:12,334 iteration 4007 : loss : 0.025866, loss_ce: 0.009361
2022-01-09 04:00:15,170 iteration 4008 : loss : 0.017836, loss_ce: 0.007943
2022-01-09 04:00:17,932 iteration 4009 : loss : 0.015156, loss_ce: 0.004528
2022-01-09 04:00:20,713 iteration 4010 : loss : 0.021055, loss_ce: 0.008764
2022-01-09 04:00:23,708 iteration 4011 : loss : 0.030234, loss_ce: 0.010561
2022-01-09 04:00:26,424 iteration 4012 : loss : 0.027463, loss_ce: 0.011743
 59%|███████████████▉           | 236/400 [3:22:19<2:23:48, 52.61s/it]2022-01-09 04:00:29,324 iteration 4013 : loss : 0.016248, loss_ce: 0.007667
2022-01-09 04:00:32,181 iteration 4014 : loss : 0.041581, loss_ce: 0.020694
2022-01-09 04:00:34,767 iteration 4015 : loss : 0.016981, loss_ce: 0.007750
2022-01-09 04:00:37,658 iteration 4016 : loss : 0.027903, loss_ce: 0.010178
2022-01-09 04:00:40,310 iteration 4017 : loss : 0.029847, loss_ce: 0.008943
2022-01-09 04:00:43,037 iteration 4018 : loss : 0.024407, loss_ce: 0.008685
2022-01-09 04:00:45,887 iteration 4019 : loss : 0.046266, loss_ce: 0.012865
2022-01-09 04:00:48,707 iteration 4020 : loss : 0.017692, loss_ce: 0.005825
2022-01-09 04:00:51,574 iteration 4021 : loss : 0.030595, loss_ce: 0.008146
2022-01-09 04:00:54,430 iteration 4022 : loss : 0.025324, loss_ce: 0.008886
2022-01-09 04:00:57,200 iteration 4023 : loss : 0.019134, loss_ce: 0.008626
2022-01-09 04:01:00,160 iteration 4024 : loss : 0.024876, loss_ce: 0.009191
2022-01-09 04:01:02,953 iteration 4025 : loss : 0.029864, loss_ce: 0.008256
2022-01-09 04:01:05,799 iteration 4026 : loss : 0.019618, loss_ce: 0.005578
2022-01-09 04:01:08,572 iteration 4027 : loss : 0.020087, loss_ce: 0.007114
2022-01-09 04:01:11,492 iteration 4028 : loss : 0.030727, loss_ce: 0.014260
2022-01-09 04:01:14,377 iteration 4029 : loss : 0.031983, loss_ce: 0.012703
 59%|███████████████▉           | 237/400 [3:23:07<2:19:08, 51.22s/it]2022-01-09 04:01:17,263 iteration 4030 : loss : 0.020339, loss_ce: 0.008100
2022-01-09 04:01:19,883 iteration 4031 : loss : 0.027504, loss_ce: 0.012438
2022-01-09 04:01:22,689 iteration 4032 : loss : 0.017583, loss_ce: 0.004861
2022-01-09 04:01:25,441 iteration 4033 : loss : 0.029564, loss_ce: 0.010945
2022-01-09 04:01:28,237 iteration 4034 : loss : 0.022717, loss_ce: 0.009280
2022-01-09 04:01:31,061 iteration 4035 : loss : 0.022978, loss_ce: 0.008008
2022-01-09 04:01:33,865 iteration 4036 : loss : 0.018671, loss_ce: 0.007918
2022-01-09 04:01:36,458 iteration 4037 : loss : 0.016975, loss_ce: 0.008878
2022-01-09 04:01:39,084 iteration 4038 : loss : 0.027349, loss_ce: 0.011251
2022-01-09 04:01:41,944 iteration 4039 : loss : 0.024982, loss_ce: 0.008246
2022-01-09 04:01:44,582 iteration 4040 : loss : 0.025176, loss_ce: 0.007592
2022-01-09 04:01:47,471 iteration 4041 : loss : 0.030117, loss_ce: 0.011743
2022-01-09 04:01:50,346 iteration 4042 : loss : 0.023882, loss_ce: 0.008803
2022-01-09 04:01:53,031 iteration 4043 : loss : 0.018994, loss_ce: 0.007830
2022-01-09 04:01:55,779 iteration 4044 : loss : 0.024987, loss_ce: 0.009316
2022-01-09 04:01:58,602 iteration 4045 : loss : 0.023893, loss_ce: 0.006842
2022-01-09 04:02:01,337 iteration 4046 : loss : 0.025967, loss_ce: 0.008762
 60%|████████████████           | 238/400 [3:23:54<2:14:49, 49.94s/it]2022-01-09 04:02:04,136 iteration 4047 : loss : 0.020173, loss_ce: 0.007813
2022-01-09 04:02:06,761 iteration 4048 : loss : 0.022913, loss_ce: 0.006792
2022-01-09 04:02:09,544 iteration 4049 : loss : 0.016470, loss_ce: 0.004618
2022-01-09 04:02:12,327 iteration 4050 : loss : 0.044908, loss_ce: 0.027382
2022-01-09 04:02:14,951 iteration 4051 : loss : 0.023542, loss_ce: 0.011128
2022-01-09 04:02:17,730 iteration 4052 : loss : 0.016694, loss_ce: 0.006321
2022-01-09 04:02:20,548 iteration 4053 : loss : 0.034024, loss_ce: 0.006837
2022-01-09 04:02:23,420 iteration 4054 : loss : 0.018769, loss_ce: 0.007747
2022-01-09 04:02:26,250 iteration 4055 : loss : 0.016026, loss_ce: 0.007067
2022-01-09 04:02:28,921 iteration 4056 : loss : 0.030659, loss_ce: 0.018167
2022-01-09 04:02:31,775 iteration 4057 : loss : 0.018626, loss_ce: 0.005293
2022-01-09 04:02:34,538 iteration 4058 : loss : 0.015887, loss_ce: 0.006644
2022-01-09 04:02:37,224 iteration 4059 : loss : 0.024552, loss_ce: 0.009574
2022-01-09 04:02:39,992 iteration 4060 : loss : 0.017754, loss_ce: 0.007066
2022-01-09 04:02:42,525 iteration 4061 : loss : 0.018257, loss_ce: 0.007997
2022-01-09 04:02:45,348 iteration 4062 : loss : 0.016397, loss_ce: 0.007211
2022-01-09 04:02:48,055 iteration 4063 : loss : 0.023676, loss_ce: 0.008458
 60%|████████████████▏          | 239/400 [3:24:41<2:11:24, 48.97s/it]2022-01-09 04:02:50,907 iteration 4064 : loss : 0.020853, loss_ce: 0.008813
2022-01-09 04:02:53,707 iteration 4065 : loss : 0.021667, loss_ce: 0.005113
2022-01-09 04:02:56,340 iteration 4066 : loss : 0.017156, loss_ce: 0.005674
2022-01-09 04:02:59,104 iteration 4067 : loss : 0.018560, loss_ce: 0.006627
2022-01-09 04:03:01,924 iteration 4068 : loss : 0.022302, loss_ce: 0.008547
2022-01-09 04:03:04,764 iteration 4069 : loss : 0.026460, loss_ce: 0.011528
2022-01-09 04:03:07,350 iteration 4070 : loss : 0.020682, loss_ce: 0.005930
2022-01-09 04:03:10,150 iteration 4071 : loss : 0.022258, loss_ce: 0.007894
2022-01-09 04:03:12,928 iteration 4072 : loss : 0.016976, loss_ce: 0.006343
2022-01-09 04:03:15,647 iteration 4073 : loss : 0.020356, loss_ce: 0.007836
2022-01-09 04:03:18,472 iteration 4074 : loss : 0.017570, loss_ce: 0.007040
2022-01-09 04:03:21,361 iteration 4075 : loss : 0.021247, loss_ce: 0.006401
2022-01-09 04:03:24,156 iteration 4076 : loss : 0.014834, loss_ce: 0.006234
2022-01-09 04:03:27,010 iteration 4077 : loss : 0.024571, loss_ce: 0.009433
2022-01-09 04:03:29,934 iteration 4078 : loss : 0.026619, loss_ce: 0.010523
2022-01-09 04:03:32,541 iteration 4079 : loss : 0.017450, loss_ce: 0.006130
2022-01-09 04:03:32,542 Training Data Eval:
2022-01-09 04:03:47,644   Average segmentation loss on training set: 0.0139
2022-01-09 04:03:47,644 Validation Data Eval:
2022-01-09 04:03:53,003   Average segmentation loss on validation set: 0.0642
2022-01-09 04:03:55,864 iteration 4080 : loss : 0.030590, loss_ce: 0.011474
 60%|████████████████▏          | 240/400 [3:25:49<2:25:40, 54.63s/it]2022-01-09 04:03:58,680 iteration 4081 : loss : 0.020431, loss_ce: 0.009126
2022-01-09 04:04:01,336 iteration 4082 : loss : 0.021897, loss_ce: 0.007988
2022-01-09 04:04:04,145 iteration 4083 : loss : 0.014659, loss_ce: 0.006008
2022-01-09 04:04:06,799 iteration 4084 : loss : 0.022935, loss_ce: 0.006868
2022-01-09 04:04:09,867 iteration 4085 : loss : 0.027646, loss_ce: 0.008302
2022-01-09 04:04:12,745 iteration 4086 : loss : 0.021964, loss_ce: 0.009729
2022-01-09 04:04:15,357 iteration 4087 : loss : 0.018300, loss_ce: 0.005349
2022-01-09 04:04:18,132 iteration 4088 : loss : 0.018002, loss_ce: 0.007730
2022-01-09 04:04:20,953 iteration 4089 : loss : 0.031100, loss_ce: 0.011634
2022-01-09 04:04:23,703 iteration 4090 : loss : 0.018078, loss_ce: 0.007904
2022-01-09 04:04:26,508 iteration 4091 : loss : 0.018219, loss_ce: 0.005849
2022-01-09 04:04:29,335 iteration 4092 : loss : 0.017530, loss_ce: 0.007548
2022-01-09 04:04:32,173 iteration 4093 : loss : 0.016061, loss_ce: 0.006422
2022-01-09 04:04:35,028 iteration 4094 : loss : 0.015803, loss_ce: 0.005615
2022-01-09 04:04:37,843 iteration 4095 : loss : 0.029642, loss_ce: 0.011546
2022-01-09 04:04:40,651 iteration 4096 : loss : 0.019528, loss_ce: 0.007766
2022-01-09 04:04:43,495 iteration 4097 : loss : 0.022807, loss_ce: 0.010676
 60%|████████████████▎          | 241/400 [3:26:36<2:19:11, 52.53s/it]2022-01-09 04:04:46,371 iteration 4098 : loss : 0.019131, loss_ce: 0.007264
2022-01-09 04:04:49,197 iteration 4099 : loss : 0.041576, loss_ce: 0.009653
2022-01-09 04:04:52,021 iteration 4100 : loss : 0.017117, loss_ce: 0.007250
2022-01-09 04:04:54,644 iteration 4101 : loss : 0.019940, loss_ce: 0.007394
2022-01-09 04:04:57,433 iteration 4102 : loss : 0.026533, loss_ce: 0.010056
2022-01-09 04:05:00,188 iteration 4103 : loss : 0.015871, loss_ce: 0.005905
2022-01-09 04:05:02,945 iteration 4104 : loss : 0.017095, loss_ce: 0.007105
2022-01-09 04:05:05,804 iteration 4105 : loss : 0.021494, loss_ce: 0.008577
2022-01-09 04:05:08,647 iteration 4106 : loss : 0.025413, loss_ce: 0.005320
2022-01-09 04:05:11,517 iteration 4107 : loss : 0.025311, loss_ce: 0.012461
2022-01-09 04:05:14,291 iteration 4108 : loss : 0.036597, loss_ce: 0.013610
2022-01-09 04:05:17,208 iteration 4109 : loss : 0.019611, loss_ce: 0.004503
2022-01-09 04:05:19,888 iteration 4110 : loss : 0.022354, loss_ce: 0.010823
2022-01-09 04:05:22,585 iteration 4111 : loss : 0.024518, loss_ce: 0.009006
2022-01-09 04:05:25,338 iteration 4112 : loss : 0.030953, loss_ce: 0.011222
2022-01-09 04:05:28,153 iteration 4113 : loss : 0.022968, loss_ce: 0.008714
2022-01-09 04:05:31,026 iteration 4114 : loss : 0.029419, loss_ce: 0.009803
 60%|████████████████▎          | 242/400 [3:27:24<2:14:22, 51.03s/it]2022-01-09 04:05:33,848 iteration 4115 : loss : 0.020005, loss_ce: 0.005363
2022-01-09 04:05:36,439 iteration 4116 : loss : 0.016796, loss_ce: 0.007034
2022-01-09 04:05:39,190 iteration 4117 : loss : 0.030359, loss_ce: 0.009607
2022-01-09 04:05:42,108 iteration 4118 : loss : 0.024554, loss_ce: 0.011092
2022-01-09 04:05:44,809 iteration 4119 : loss : 0.021170, loss_ce: 0.008269
2022-01-09 04:05:47,715 iteration 4120 : loss : 0.028219, loss_ce: 0.005803
2022-01-09 04:05:50,514 iteration 4121 : loss : 0.020140, loss_ce: 0.009278
2022-01-09 04:05:53,329 iteration 4122 : loss : 0.021560, loss_ce: 0.006773
2022-01-09 04:05:56,175 iteration 4123 : loss : 0.025105, loss_ce: 0.008254
2022-01-09 04:05:58,904 iteration 4124 : loss : 0.019296, loss_ce: 0.006687
2022-01-09 04:06:01,755 iteration 4125 : loss : 0.037122, loss_ce: 0.009519
2022-01-09 04:06:04,537 iteration 4126 : loss : 0.016833, loss_ce: 0.007357
2022-01-09 04:06:07,348 iteration 4127 : loss : 0.023432, loss_ce: 0.008937
2022-01-09 04:06:10,139 iteration 4128 : loss : 0.030602, loss_ce: 0.016151
2022-01-09 04:06:12,988 iteration 4129 : loss : 0.037798, loss_ce: 0.014842
2022-01-09 04:06:15,830 iteration 4130 : loss : 0.016775, loss_ce: 0.008400
2022-01-09 04:06:18,448 iteration 4131 : loss : 0.013973, loss_ce: 0.004019
 61%|████████████████▍          | 243/400 [3:28:11<2:10:41, 49.95s/it]2022-01-09 04:06:21,369 iteration 4132 : loss : 0.021990, loss_ce: 0.007757
2022-01-09 04:06:24,310 iteration 4133 : loss : 0.025991, loss_ce: 0.011682
2022-01-09 04:06:27,011 iteration 4134 : loss : 0.022372, loss_ce: 0.009165
2022-01-09 04:06:29,853 iteration 4135 : loss : 0.023767, loss_ce: 0.007839
2022-01-09 04:06:32,653 iteration 4136 : loss : 0.019436, loss_ce: 0.009323
2022-01-09 04:06:35,288 iteration 4137 : loss : 0.018875, loss_ce: 0.008624
2022-01-09 04:06:38,145 iteration 4138 : loss : 0.015474, loss_ce: 0.006371
2022-01-09 04:06:40,906 iteration 4139 : loss : 0.022258, loss_ce: 0.007171
2022-01-09 04:06:43,747 iteration 4140 : loss : 0.028374, loss_ce: 0.009410
2022-01-09 04:06:46,592 iteration 4141 : loss : 0.028906, loss_ce: 0.008256
2022-01-09 04:06:49,329 iteration 4142 : loss : 0.029014, loss_ce: 0.010147
2022-01-09 04:06:52,158 iteration 4143 : loss : 0.025239, loss_ce: 0.006147
2022-01-09 04:06:54,971 iteration 4144 : loss : 0.022152, loss_ce: 0.008107
2022-01-09 04:06:57,679 iteration 4145 : loss : 0.018048, loss_ce: 0.005116
2022-01-09 04:07:00,488 iteration 4146 : loss : 0.022244, loss_ce: 0.007102
2022-01-09 04:07:03,273 iteration 4147 : loss : 0.035873, loss_ce: 0.010760
2022-01-09 04:07:06,164 iteration 4148 : loss : 0.014274, loss_ce: 0.004749
 61%|████████████████▍          | 244/400 [3:28:59<2:08:07, 49.28s/it]2022-01-09 04:07:09,130 iteration 4149 : loss : 0.033839, loss_ce: 0.013494
2022-01-09 04:07:11,838 iteration 4150 : loss : 0.020663, loss_ce: 0.005364
2022-01-09 04:07:14,646 iteration 4151 : loss : 0.019532, loss_ce: 0.008563
2022-01-09 04:07:17,379 iteration 4152 : loss : 0.021161, loss_ce: 0.007371
2022-01-09 04:07:20,183 iteration 4153 : loss : 0.038069, loss_ce: 0.010956
2022-01-09 04:07:22,961 iteration 4154 : loss : 0.029704, loss_ce: 0.012799
2022-01-09 04:07:25,802 iteration 4155 : loss : 0.021909, loss_ce: 0.007531
2022-01-09 04:07:28,437 iteration 4156 : loss : 0.013133, loss_ce: 0.004922
2022-01-09 04:07:31,264 iteration 4157 : loss : 0.019550, loss_ce: 0.007048
2022-01-09 04:07:34,069 iteration 4158 : loss : 0.038100, loss_ce: 0.023507
2022-01-09 04:07:36,909 iteration 4159 : loss : 0.017278, loss_ce: 0.007367
2022-01-09 04:07:39,625 iteration 4160 : loss : 0.026480, loss_ce: 0.011917
2022-01-09 04:07:42,478 iteration 4161 : loss : 0.017899, loss_ce: 0.006851
2022-01-09 04:07:45,084 iteration 4162 : loss : 0.017488, loss_ce: 0.006024
2022-01-09 04:07:47,918 iteration 4163 : loss : 0.029188, loss_ce: 0.007704
2022-01-09 04:07:50,798 iteration 4164 : loss : 0.027918, loss_ce: 0.014331
2022-01-09 04:07:50,798 Training Data Eval:
2022-01-09 04:08:05,917   Average segmentation loss on training set: 0.0150
2022-01-09 04:08:05,918 Validation Data Eval:
2022-01-09 04:08:11,222   Average segmentation loss on validation set: 0.0618
2022-01-09 04:08:17,172 Found new lowest validation loss at iteration 4164! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 04:08:19,060 iteration 4165 : loss : 0.023474, loss_ce: 0.009497
 61%|████████████████▌          | 245/400 [3:30:12<2:25:35, 56.36s/it]2022-01-09 04:08:21,518 iteration 4166 : loss : 0.018893, loss_ce: 0.007289
2022-01-09 04:08:24,339 iteration 4167 : loss : 0.024110, loss_ce: 0.006257
2022-01-09 04:08:27,184 iteration 4168 : loss : 0.023305, loss_ce: 0.009799
2022-01-09 04:08:29,869 iteration 4169 : loss : 0.027663, loss_ce: 0.009516
2022-01-09 04:08:32,707 iteration 4170 : loss : 0.027840, loss_ce: 0.011031
2022-01-09 04:08:35,507 iteration 4171 : loss : 0.034175, loss_ce: 0.015313
2022-01-09 04:08:38,326 iteration 4172 : loss : 0.019452, loss_ce: 0.007194
2022-01-09 04:08:41,077 iteration 4173 : loss : 0.017477, loss_ce: 0.005526
2022-01-09 04:08:43,930 iteration 4174 : loss : 0.031550, loss_ce: 0.009924
2022-01-09 04:08:46,578 iteration 4175 : loss : 0.019381, loss_ce: 0.004933
2022-01-09 04:08:49,325 iteration 4176 : loss : 0.022075, loss_ce: 0.006977
2022-01-09 04:08:52,083 iteration 4177 : loss : 0.018721, loss_ce: 0.008838
2022-01-09 04:08:54,782 iteration 4178 : loss : 0.014775, loss_ce: 0.005760
2022-01-09 04:08:57,560 iteration 4179 : loss : 0.023146, loss_ce: 0.008687
2022-01-09 04:09:00,423 iteration 4180 : loss : 0.029330, loss_ce: 0.010930
2022-01-09 04:09:03,255 iteration 4181 : loss : 0.026920, loss_ce: 0.010397
2022-01-09 04:09:06,098 iteration 4182 : loss : 0.018934, loss_ce: 0.007513
 62%|████████████████▌          | 246/400 [3:30:59<2:17:28, 53.56s/it]2022-01-09 04:09:08,922 iteration 4183 : loss : 0.028641, loss_ce: 0.007442
2022-01-09 04:09:11,660 iteration 4184 : loss : 0.021783, loss_ce: 0.007999
2022-01-09 04:09:14,421 iteration 4185 : loss : 0.030365, loss_ce: 0.010489
2022-01-09 04:09:17,022 iteration 4186 : loss : 0.018442, loss_ce: 0.006589
2022-01-09 04:09:19,849 iteration 4187 : loss : 0.018628, loss_ce: 0.007313
2022-01-09 04:09:22,634 iteration 4188 : loss : 0.024357, loss_ce: 0.012337
2022-01-09 04:09:25,427 iteration 4189 : loss : 0.019468, loss_ce: 0.006743
2022-01-09 04:09:28,366 iteration 4190 : loss : 0.032485, loss_ce: 0.015943
2022-01-09 04:09:31,366 iteration 4191 : loss : 0.020016, loss_ce: 0.004753
2022-01-09 04:09:33,985 iteration 4192 : loss : 0.016111, loss_ce: 0.008432
2022-01-09 04:09:36,751 iteration 4193 : loss : 0.016748, loss_ce: 0.005096
2022-01-09 04:09:39,401 iteration 4194 : loss : 0.019040, loss_ce: 0.006478
2022-01-09 04:09:42,247 iteration 4195 : loss : 0.018120, loss_ce: 0.008058
2022-01-09 04:09:44,984 iteration 4196 : loss : 0.015703, loss_ce: 0.006019
2022-01-09 04:09:47,794 iteration 4197 : loss : 0.015908, loss_ce: 0.006721
2022-01-09 04:09:50,712 iteration 4198 : loss : 0.040974, loss_ce: 0.007304
2022-01-09 04:09:53,430 iteration 4199 : loss : 0.020147, loss_ce: 0.008711
 62%|████████████████▋          | 247/400 [3:31:46<2:11:49, 51.70s/it]2022-01-09 04:09:56,272 iteration 4200 : loss : 0.025353, loss_ce: 0.008308
2022-01-09 04:09:59,101 iteration 4201 : loss : 0.037206, loss_ce: 0.019335
2022-01-09 04:10:01,986 iteration 4202 : loss : 0.017743, loss_ce: 0.008753
2022-01-09 04:10:04,728 iteration 4203 : loss : 0.023836, loss_ce: 0.010302
2022-01-09 04:10:07,556 iteration 4204 : loss : 0.016259, loss_ce: 0.006226
2022-01-09 04:10:10,243 iteration 4205 : loss : 0.013683, loss_ce: 0.004698
2022-01-09 04:10:13,031 iteration 4206 : loss : 0.044664, loss_ce: 0.012120
2022-01-09 04:10:15,674 iteration 4207 : loss : 0.014236, loss_ce: 0.006611
2022-01-09 04:10:18,549 iteration 4208 : loss : 0.022349, loss_ce: 0.007625
2022-01-09 04:10:21,150 iteration 4209 : loss : 0.018648, loss_ce: 0.005949
2022-01-09 04:10:23,906 iteration 4210 : loss : 0.015413, loss_ce: 0.005367
2022-01-09 04:10:26,655 iteration 4211 : loss : 0.017964, loss_ce: 0.006785
2022-01-09 04:10:29,414 iteration 4212 : loss : 0.016851, loss_ce: 0.007205
2022-01-09 04:10:32,273 iteration 4213 : loss : 0.018090, loss_ce: 0.005817
2022-01-09 04:10:35,124 iteration 4214 : loss : 0.026192, loss_ce: 0.012294
2022-01-09 04:10:37,969 iteration 4215 : loss : 0.036297, loss_ce: 0.008351
2022-01-09 04:10:40,610 iteration 4216 : loss : 0.027525, loss_ce: 0.010173
 62%|████████████████▋          | 248/400 [3:32:33<2:07:32, 50.34s/it]2022-01-09 04:10:43,517 iteration 4217 : loss : 0.037443, loss_ce: 0.009867
2022-01-09 04:10:46,567 iteration 4218 : loss : 0.022276, loss_ce: 0.011086
2022-01-09 04:10:49,446 iteration 4219 : loss : 0.017740, loss_ce: 0.006553
2022-01-09 04:10:52,334 iteration 4220 : loss : 0.018140, loss_ce: 0.006195
2022-01-09 04:10:55,203 iteration 4221 : loss : 0.025921, loss_ce: 0.008629
2022-01-09 04:10:58,071 iteration 4222 : loss : 0.019847, loss_ce: 0.008296
2022-01-09 04:11:01,012 iteration 4223 : loss : 0.033224, loss_ce: 0.009312
2022-01-09 04:11:03,782 iteration 4224 : loss : 0.028400, loss_ce: 0.007710
2022-01-09 04:11:06,669 iteration 4225 : loss : 0.022809, loss_ce: 0.009661
2022-01-09 04:11:09,582 iteration 4226 : loss : 0.026858, loss_ce: 0.013410
2022-01-09 04:11:12,249 iteration 4227 : loss : 0.020626, loss_ce: 0.006680
2022-01-09 04:11:15,069 iteration 4228 : loss : 0.027856, loss_ce: 0.012710
2022-01-09 04:11:17,908 iteration 4229 : loss : 0.022740, loss_ce: 0.008689
2022-01-09 04:11:20,773 iteration 4230 : loss : 0.034499, loss_ce: 0.016191
2022-01-09 04:11:23,645 iteration 4231 : loss : 0.044834, loss_ce: 0.014332
2022-01-09 04:11:26,516 iteration 4232 : loss : 0.043910, loss_ce: 0.019315
2022-01-09 04:11:29,086 iteration 4233 : loss : 0.020155, loss_ce: 0.007393
 62%|████████████████▊          | 249/400 [3:33:22<2:05:17, 49.78s/it]2022-01-09 04:11:31,971 iteration 4234 : loss : 0.026341, loss_ce: 0.010456
2022-01-09 04:11:34,729 iteration 4235 : loss : 0.020722, loss_ce: 0.007620
2022-01-09 04:11:37,567 iteration 4236 : loss : 0.032703, loss_ce: 0.016440
2022-01-09 04:11:40,274 iteration 4237 : loss : 0.018089, loss_ce: 0.009017
2022-01-09 04:11:43,139 iteration 4238 : loss : 0.035352, loss_ce: 0.011434
2022-01-09 04:11:45,895 iteration 4239 : loss : 0.027791, loss_ce: 0.010256
2022-01-09 04:11:48,764 iteration 4240 : loss : 0.032724, loss_ce: 0.010184
2022-01-09 04:11:51,414 iteration 4241 : loss : 0.062092, loss_ce: 0.027477
2022-01-09 04:11:54,212 iteration 4242 : loss : 0.027601, loss_ce: 0.007966
2022-01-09 04:11:57,098 iteration 4243 : loss : 0.022406, loss_ce: 0.008106
2022-01-09 04:11:59,976 iteration 4244 : loss : 0.032368, loss_ce: 0.011603
2022-01-09 04:12:02,703 iteration 4245 : loss : 0.034970, loss_ce: 0.009982
2022-01-09 04:12:05,471 iteration 4246 : loss : 0.025612, loss_ce: 0.013088
2022-01-09 04:12:08,297 iteration 4247 : loss : 0.026716, loss_ce: 0.005759
2022-01-09 04:12:11,152 iteration 4248 : loss : 0.028182, loss_ce: 0.011428
2022-01-09 04:12:14,034 iteration 4249 : loss : 0.041419, loss_ce: 0.023384
2022-01-09 04:12:14,034 Training Data Eval:
2022-01-09 04:12:29,043   Average segmentation loss on training set: 0.0172
2022-01-09 04:12:29,044 Validation Data Eval:
2022-01-09 04:12:34,234   Average segmentation loss on validation set: 0.1065
2022-01-09 04:12:37,103 iteration 4250 : loss : 0.043528, loss_ce: 0.014900
 62%|████████████████▉          | 250/400 [3:34:30<2:18:07, 55.25s/it]2022-01-09 04:12:40,030 iteration 4251 : loss : 0.029557, loss_ce: 0.014807
2022-01-09 04:12:42,839 iteration 4252 : loss : 0.020653, loss_ce: 0.008669
2022-01-09 04:12:45,669 iteration 4253 : loss : 0.028613, loss_ce: 0.011111
2022-01-09 04:12:48,602 iteration 4254 : loss : 0.026554, loss_ce: 0.011871
2022-01-09 04:12:51,193 iteration 4255 : loss : 0.016686, loss_ce: 0.006533
2022-01-09 04:12:53,995 iteration 4256 : loss : 0.024218, loss_ce: 0.006927
2022-01-09 04:12:56,860 iteration 4257 : loss : 0.027774, loss_ce: 0.009926
2022-01-09 04:12:59,531 iteration 4258 : loss : 0.019867, loss_ce: 0.006806
2022-01-09 04:13:02,363 iteration 4259 : loss : 0.018958, loss_ce: 0.006745
2022-01-09 04:13:05,035 iteration 4260 : loss : 0.030852, loss_ce: 0.009067
2022-01-09 04:13:07,889 iteration 4261 : loss : 0.017774, loss_ce: 0.006212
2022-01-09 04:13:10,656 iteration 4262 : loss : 0.023198, loss_ce: 0.010258
2022-01-09 04:13:13,419 iteration 4263 : loss : 0.032644, loss_ce: 0.016396
2022-01-09 04:13:16,194 iteration 4264 : loss : 0.024208, loss_ce: 0.007794
2022-01-09 04:13:19,055 iteration 4265 : loss : 0.029650, loss_ce: 0.009843
2022-01-09 04:13:21,954 iteration 4266 : loss : 0.022795, loss_ce: 0.007969
2022-01-09 04:13:24,617 iteration 4267 : loss : 0.019480, loss_ce: 0.008819
 63%|████████████████▉          | 251/400 [3:35:17<2:11:26, 52.93s/it]2022-01-09 04:13:27,467 iteration 4268 : loss : 0.024752, loss_ce: 0.010439
2022-01-09 04:13:30,222 iteration 4269 : loss : 0.017201, loss_ce: 0.006015
2022-01-09 04:13:32,821 iteration 4270 : loss : 0.051603, loss_ce: 0.008832
2022-01-09 04:13:35,607 iteration 4271 : loss : 0.018426, loss_ce: 0.008427
2022-01-09 04:13:38,441 iteration 4272 : loss : 0.023196, loss_ce: 0.008199
2022-01-09 04:13:41,299 iteration 4273 : loss : 0.030413, loss_ce: 0.016016
2022-01-09 04:13:44,080 iteration 4274 : loss : 0.026145, loss_ce: 0.012965
2022-01-09 04:13:46,811 iteration 4275 : loss : 0.020765, loss_ce: 0.008692
2022-01-09 04:13:49,385 iteration 4276 : loss : 0.033396, loss_ce: 0.010640
2022-01-09 04:13:52,198 iteration 4277 : loss : 0.015419, loss_ce: 0.006093
2022-01-09 04:13:54,786 iteration 4278 : loss : 0.030455, loss_ce: 0.010274
2022-01-09 04:13:57,552 iteration 4279 : loss : 0.018053, loss_ce: 0.005214
2022-01-09 04:14:00,219 iteration 4280 : loss : 0.021177, loss_ce: 0.006454
2022-01-09 04:14:03,284 iteration 4281 : loss : 0.027445, loss_ce: 0.013166
2022-01-09 04:14:06,056 iteration 4282 : loss : 0.024680, loss_ce: 0.012705
2022-01-09 04:14:08,639 iteration 4283 : loss : 0.018246, loss_ce: 0.007738
2022-01-09 04:14:11,413 iteration 4284 : loss : 0.026291, loss_ce: 0.012811
 63%|█████████████████          | 252/400 [3:36:04<2:06:01, 51.09s/it]2022-01-09 04:14:14,153 iteration 4285 : loss : 0.030846, loss_ce: 0.013842
2022-01-09 04:14:16,967 iteration 4286 : loss : 0.017459, loss_ce: 0.005060
2022-01-09 04:14:19,818 iteration 4287 : loss : 0.051524, loss_ce: 0.030911
2022-01-09 04:14:22,652 iteration 4288 : loss : 0.022550, loss_ce: 0.006885
2022-01-09 04:14:25,521 iteration 4289 : loss : 0.025239, loss_ce: 0.008302
2022-01-09 04:14:28,388 iteration 4290 : loss : 0.033726, loss_ce: 0.012144
2022-01-09 04:14:30,974 iteration 4291 : loss : 0.021037, loss_ce: 0.007318
2022-01-09 04:14:33,893 iteration 4292 : loss : 0.023038, loss_ce: 0.008506
2022-01-09 04:14:36,490 iteration 4293 : loss : 0.032276, loss_ce: 0.013751
2022-01-09 04:14:39,338 iteration 4294 : loss : 0.034002, loss_ce: 0.016499
2022-01-09 04:14:42,191 iteration 4295 : loss : 0.019638, loss_ce: 0.008843
2022-01-09 04:14:44,837 iteration 4296 : loss : 0.017344, loss_ce: 0.007140
2022-01-09 04:14:47,607 iteration 4297 : loss : 0.029933, loss_ce: 0.008341
2022-01-09 04:14:50,357 iteration 4298 : loss : 0.021112, loss_ce: 0.006975
2022-01-09 04:14:53,257 iteration 4299 : loss : 0.030407, loss_ce: 0.012811
2022-01-09 04:14:56,094 iteration 4300 : loss : 0.022987, loss_ce: 0.006576
2022-01-09 04:14:58,927 iteration 4301 : loss : 0.019856, loss_ce: 0.007535
 63%|█████████████████          | 253/400 [3:36:52<2:02:32, 50.02s/it]2022-01-09 04:15:01,826 iteration 4302 : loss : 0.015789, loss_ce: 0.006261
2022-01-09 04:15:04,535 iteration 4303 : loss : 0.016998, loss_ce: 0.005592
2022-01-09 04:15:07,185 iteration 4304 : loss : 0.018014, loss_ce: 0.006369
2022-01-09 04:15:10,188 iteration 4305 : loss : 0.020497, loss_ce: 0.005654
2022-01-09 04:15:13,019 iteration 4306 : loss : 0.031574, loss_ce: 0.009832
2022-01-09 04:15:15,630 iteration 4307 : loss : 0.018891, loss_ce: 0.006867
2022-01-09 04:15:18,548 iteration 4308 : loss : 0.024369, loss_ce: 0.008309
2022-01-09 04:15:21,228 iteration 4309 : loss : 0.022436, loss_ce: 0.006160
2022-01-09 04:15:23,908 iteration 4310 : loss : 0.027391, loss_ce: 0.014563
2022-01-09 04:15:26,770 iteration 4311 : loss : 0.018666, loss_ce: 0.008944
2022-01-09 04:15:29,660 iteration 4312 : loss : 0.032775, loss_ce: 0.014161
2022-01-09 04:15:32,512 iteration 4313 : loss : 0.026849, loss_ce: 0.009927
2022-01-09 04:15:35,352 iteration 4314 : loss : 0.025534, loss_ce: 0.008293
2022-01-09 04:15:38,116 iteration 4315 : loss : 0.027294, loss_ce: 0.014855
2022-01-09 04:15:40,819 iteration 4316 : loss : 0.022519, loss_ce: 0.008651
2022-01-09 04:15:43,568 iteration 4317 : loss : 0.021599, loss_ce: 0.007817
2022-01-09 04:15:46,414 iteration 4318 : loss : 0.019437, loss_ce: 0.009176
 64%|█████████████████▏         | 254/400 [3:37:39<1:59:51, 49.26s/it]2022-01-09 04:15:49,289 iteration 4319 : loss : 0.024465, loss_ce: 0.007479
2022-01-09 04:15:52,064 iteration 4320 : loss : 0.024600, loss_ce: 0.010421
2022-01-09 04:15:54,957 iteration 4321 : loss : 0.019917, loss_ce: 0.008204
2022-01-09 04:15:57,622 iteration 4322 : loss : 0.019841, loss_ce: 0.006532
2022-01-09 04:16:00,404 iteration 4323 : loss : 0.013394, loss_ce: 0.004051
2022-01-09 04:16:03,270 iteration 4324 : loss : 0.021925, loss_ce: 0.008503
2022-01-09 04:16:06,226 iteration 4325 : loss : 0.032698, loss_ce: 0.007045
2022-01-09 04:16:08,879 iteration 4326 : loss : 0.022587, loss_ce: 0.012741
2022-01-09 04:16:11,644 iteration 4327 : loss : 0.018650, loss_ce: 0.007372
2022-01-09 04:16:14,341 iteration 4328 : loss : 0.045067, loss_ce: 0.013895
2022-01-09 04:16:17,173 iteration 4329 : loss : 0.023499, loss_ce: 0.010120
2022-01-09 04:16:19,783 iteration 4330 : loss : 0.014204, loss_ce: 0.005032
2022-01-09 04:16:22,703 iteration 4331 : loss : 0.024710, loss_ce: 0.006647
2022-01-09 04:16:25,554 iteration 4332 : loss : 0.027360, loss_ce: 0.011370
2022-01-09 04:16:28,131 iteration 4333 : loss : 0.018905, loss_ce: 0.005323
2022-01-09 04:16:30,868 iteration 4334 : loss : 0.019151, loss_ce: 0.007415
2022-01-09 04:16:30,868 Training Data Eval:
2022-01-09 04:16:45,775   Average segmentation loss on training set: 0.0139
2022-01-09 04:16:45,775 Validation Data Eval:
2022-01-09 04:16:51,139   Average segmentation loss on validation set: 0.0753
2022-01-09 04:16:54,013 iteration 4335 : loss : 0.017883, loss_ce: 0.009011
 64%|█████████████████▏         | 255/400 [3:38:47<2:12:19, 54.76s/it]2022-01-09 04:16:56,677 iteration 4336 : loss : 0.017861, loss_ce: 0.006751
2022-01-09 04:16:59,548 iteration 4337 : loss : 0.022688, loss_ce: 0.009663
2022-01-09 04:17:02,437 iteration 4338 : loss : 0.027391, loss_ce: 0.009066
2022-01-09 04:17:05,120 iteration 4339 : loss : 0.024168, loss_ce: 0.008050
2022-01-09 04:17:07,913 iteration 4340 : loss : 0.016042, loss_ce: 0.006456
2022-01-09 04:17:10,578 iteration 4341 : loss : 0.027404, loss_ce: 0.015103
2022-01-09 04:17:13,225 iteration 4342 : loss : 0.036972, loss_ce: 0.009886
2022-01-09 04:17:15,970 iteration 4343 : loss : 0.023180, loss_ce: 0.008138
2022-01-09 04:17:18,778 iteration 4344 : loss : 0.020188, loss_ce: 0.006666
2022-01-09 04:17:21,438 iteration 4345 : loss : 0.021340, loss_ce: 0.005983
2022-01-09 04:17:24,229 iteration 4346 : loss : 0.014407, loss_ce: 0.006136
2022-01-09 04:17:27,060 iteration 4347 : loss : 0.025838, loss_ce: 0.008653
2022-01-09 04:17:29,880 iteration 4348 : loss : 0.024125, loss_ce: 0.013055
2022-01-09 04:17:32,522 iteration 4349 : loss : 0.019491, loss_ce: 0.005581
2022-01-09 04:17:35,221 iteration 4350 : loss : 0.015885, loss_ce: 0.006239
2022-01-09 04:17:37,883 iteration 4351 : loss : 0.014484, loss_ce: 0.006337
2022-01-09 04:17:40,567 iteration 4352 : loss : 0.014609, loss_ce: 0.005671
 64%|█████████████████▎         | 256/400 [3:39:33<2:05:31, 52.30s/it]2022-01-09 04:17:43,448 iteration 4353 : loss : 0.015428, loss_ce: 0.005037
2022-01-09 04:17:46,323 iteration 4354 : loss : 0.020208, loss_ce: 0.005647
2022-01-09 04:17:49,109 iteration 4355 : loss : 0.017022, loss_ce: 0.005903
2022-01-09 04:17:51,947 iteration 4356 : loss : 0.027085, loss_ce: 0.014200
2022-01-09 04:17:54,674 iteration 4357 : loss : 0.018712, loss_ce: 0.007451
2022-01-09 04:17:57,455 iteration 4358 : loss : 0.021979, loss_ce: 0.010658
2022-01-09 04:18:00,275 iteration 4359 : loss : 0.013240, loss_ce: 0.004703
2022-01-09 04:18:03,221 iteration 4360 : loss : 0.021900, loss_ce: 0.007922
2022-01-09 04:18:06,041 iteration 4361 : loss : 0.030240, loss_ce: 0.015530
2022-01-09 04:18:08,846 iteration 4362 : loss : 0.016874, loss_ce: 0.006358
2022-01-09 04:18:11,673 iteration 4363 : loss : 0.017832, loss_ce: 0.008861
2022-01-09 04:18:14,550 iteration 4364 : loss : 0.027358, loss_ce: 0.009950
2022-01-09 04:18:17,209 iteration 4365 : loss : 0.017400, loss_ce: 0.007018
2022-01-09 04:18:19,933 iteration 4366 : loss : 0.013681, loss_ce: 0.005765
2022-01-09 04:18:22,688 iteration 4367 : loss : 0.032417, loss_ce: 0.010472
2022-01-09 04:18:25,586 iteration 4368 : loss : 0.032414, loss_ce: 0.013961
2022-01-09 04:18:28,402 iteration 4369 : loss : 0.023133, loss_ce: 0.005872
 64%|█████████████████▎         | 257/400 [3:40:21<2:01:26, 50.96s/it]2022-01-09 04:18:31,144 iteration 4370 : loss : 0.018608, loss_ce: 0.004068
2022-01-09 04:18:33,982 iteration 4371 : loss : 0.018856, loss_ce: 0.009406
2022-01-09 04:18:36,570 iteration 4372 : loss : 0.015314, loss_ce: 0.006627
2022-01-09 04:18:39,328 iteration 4373 : loss : 0.033888, loss_ce: 0.010852
2022-01-09 04:18:42,159 iteration 4374 : loss : 0.016350, loss_ce: 0.007627
2022-01-09 04:18:44,990 iteration 4375 : loss : 0.017727, loss_ce: 0.007557
2022-01-09 04:18:47,600 iteration 4376 : loss : 0.017448, loss_ce: 0.005923
2022-01-09 04:18:50,528 iteration 4377 : loss : 0.031215, loss_ce: 0.009590
2022-01-09 04:18:53,202 iteration 4378 : loss : 0.024130, loss_ce: 0.010853
2022-01-09 04:18:56,101 iteration 4379 : loss : 0.035851, loss_ce: 0.019088
2022-01-09 04:18:58,858 iteration 4380 : loss : 0.014452, loss_ce: 0.005685
2022-01-09 04:19:01,614 iteration 4381 : loss : 0.029054, loss_ce: 0.012745
2022-01-09 04:19:04,448 iteration 4382 : loss : 0.028909, loss_ce: 0.010054
2022-01-09 04:19:07,125 iteration 4383 : loss : 0.023247, loss_ce: 0.006089
2022-01-09 04:19:10,032 iteration 4384 : loss : 0.022818, loss_ce: 0.007290
2022-01-09 04:19:12,730 iteration 4385 : loss : 0.017904, loss_ce: 0.008239
2022-01-09 04:19:15,526 iteration 4386 : loss : 0.022938, loss_ce: 0.008671
 64%|█████████████████▍         | 258/400 [3:41:08<1:57:52, 49.81s/it]2022-01-09 04:19:18,282 iteration 4387 : loss : 0.017971, loss_ce: 0.007453
2022-01-09 04:19:21,114 iteration 4388 : loss : 0.022359, loss_ce: 0.009576
2022-01-09 04:19:23,717 iteration 4389 : loss : 0.015102, loss_ce: 0.006379
2022-01-09 04:19:26,482 iteration 4390 : loss : 0.018623, loss_ce: 0.006869
2022-01-09 04:19:29,260 iteration 4391 : loss : 0.017579, loss_ce: 0.007141
2022-01-09 04:19:32,046 iteration 4392 : loss : 0.019688, loss_ce: 0.009698
2022-01-09 04:19:34,833 iteration 4393 : loss : 0.018825, loss_ce: 0.006441
2022-01-09 04:19:37,558 iteration 4394 : loss : 0.021960, loss_ce: 0.009532
2022-01-09 04:19:40,380 iteration 4395 : loss : 0.019645, loss_ce: 0.006830
2022-01-09 04:19:43,253 iteration 4396 : loss : 0.020580, loss_ce: 0.007183
2022-01-09 04:19:46,072 iteration 4397 : loss : 0.020788, loss_ce: 0.007019
2022-01-09 04:19:48,674 iteration 4398 : loss : 0.024124, loss_ce: 0.006822
2022-01-09 04:19:51,481 iteration 4399 : loss : 0.025621, loss_ce: 0.007885
2022-01-09 04:19:54,232 iteration 4400 : loss : 0.015614, loss_ce: 0.006274
2022-01-09 04:19:57,019 iteration 4401 : loss : 0.020361, loss_ce: 0.009652
2022-01-09 04:19:59,819 iteration 4402 : loss : 0.016229, loss_ce: 0.004923
2022-01-09 04:20:02,636 iteration 4403 : loss : 0.022047, loss_ce: 0.007030
 65%|█████████████████▍         | 259/400 [3:41:55<1:55:08, 49.00s/it]2022-01-09 04:20:05,478 iteration 4404 : loss : 0.013737, loss_ce: 0.006763
2022-01-09 04:20:08,354 iteration 4405 : loss : 0.018012, loss_ce: 0.005191
2022-01-09 04:20:11,271 iteration 4406 : loss : 0.018975, loss_ce: 0.007336
2022-01-09 04:20:13,998 iteration 4407 : loss : 0.021411, loss_ce: 0.008142
2022-01-09 04:20:16,780 iteration 4408 : loss : 0.019744, loss_ce: 0.004900
2022-01-09 04:20:19,620 iteration 4409 : loss : 0.016886, loss_ce: 0.006537
2022-01-09 04:20:22,512 iteration 4410 : loss : 0.024765, loss_ce: 0.007246
2022-01-09 04:20:25,351 iteration 4411 : loss : 0.019424, loss_ce: 0.009768
2022-01-09 04:20:28,098 iteration 4412 : loss : 0.017144, loss_ce: 0.007110
2022-01-09 04:20:30,810 iteration 4413 : loss : 0.020796, loss_ce: 0.006076
2022-01-09 04:20:33,562 iteration 4414 : loss : 0.014305, loss_ce: 0.005582
2022-01-09 04:20:36,414 iteration 4415 : loss : 0.025450, loss_ce: 0.011347
2022-01-09 04:20:39,085 iteration 4416 : loss : 0.033898, loss_ce: 0.007126
2022-01-09 04:20:41,883 iteration 4417 : loss : 0.020193, loss_ce: 0.006671
2022-01-09 04:20:44,826 iteration 4418 : loss : 0.020873, loss_ce: 0.009355
2022-01-09 04:20:47,611 iteration 4419 : loss : 0.029122, loss_ce: 0.010287
2022-01-09 04:20:47,611 Training Data Eval:
2022-01-09 04:21:02,797   Average segmentation loss on training set: 0.0281
2022-01-09 04:21:02,798 Validation Data Eval:
2022-01-09 04:21:08,094   Average segmentation loss on validation set: 0.0768
2022-01-09 04:21:10,865 iteration 4420 : loss : 0.014702, loss_ce: 0.007027
 65%|█████████████████▌         | 260/400 [3:43:04<2:07:47, 54.77s/it]2022-01-09 04:21:13,722 iteration 4421 : loss : 0.041864, loss_ce: 0.012924
2022-01-09 04:21:16,593 iteration 4422 : loss : 0.025979, loss_ce: 0.010171
2022-01-09 04:21:19,503 iteration 4423 : loss : 0.032126, loss_ce: 0.014088
2022-01-09 04:21:22,357 iteration 4424 : loss : 0.029423, loss_ce: 0.010293
2022-01-09 04:21:24,935 iteration 4425 : loss : 0.017090, loss_ce: 0.005962
2022-01-09 04:21:27,607 iteration 4426 : loss : 0.026669, loss_ce: 0.014154
2022-01-09 04:21:30,485 iteration 4427 : loss : 0.024220, loss_ce: 0.005901
2022-01-09 04:21:33,353 iteration 4428 : loss : 0.024305, loss_ce: 0.009029
2022-01-09 04:21:36,214 iteration 4429 : loss : 0.030828, loss_ce: 0.011311
2022-01-09 04:21:38,827 iteration 4430 : loss : 0.027663, loss_ce: 0.008819
2022-01-09 04:21:41,582 iteration 4431 : loss : 0.020360, loss_ce: 0.009739
2022-01-09 04:21:44,409 iteration 4432 : loss : 0.016657, loss_ce: 0.006286
2022-01-09 04:21:47,119 iteration 4433 : loss : 0.018734, loss_ce: 0.006771
2022-01-09 04:21:49,954 iteration 4434 : loss : 0.018782, loss_ce: 0.007660
2022-01-09 04:21:52,954 iteration 4435 : loss : 0.029003, loss_ce: 0.010046
2022-01-09 04:21:55,879 iteration 4436 : loss : 0.033703, loss_ce: 0.017155
2022-01-09 04:21:58,633 iteration 4437 : loss : 0.035175, loss_ce: 0.017902
 65%|█████████████████▌         | 261/400 [3:43:51<2:02:00, 52.67s/it]2022-01-09 04:22:01,511 iteration 4438 : loss : 0.025310, loss_ce: 0.009127
2022-01-09 04:22:04,288 iteration 4439 : loss : 0.022277, loss_ce: 0.007528
2022-01-09 04:22:07,220 iteration 4440 : loss : 0.035864, loss_ce: 0.014776
2022-01-09 04:22:09,951 iteration 4441 : loss : 0.021245, loss_ce: 0.009372
2022-01-09 04:22:12,742 iteration 4442 : loss : 0.018009, loss_ce: 0.008949
2022-01-09 04:22:15,553 iteration 4443 : loss : 0.016037, loss_ce: 0.006944
2022-01-09 04:22:18,177 iteration 4444 : loss : 0.031714, loss_ce: 0.010474
2022-01-09 04:22:21,066 iteration 4445 : loss : 0.023004, loss_ce: 0.008729
2022-01-09 04:22:23,934 iteration 4446 : loss : 0.019032, loss_ce: 0.007232
2022-01-09 04:22:26,731 iteration 4447 : loss : 0.029350, loss_ce: 0.012022
2022-01-09 04:22:29,507 iteration 4448 : loss : 0.030770, loss_ce: 0.008629
2022-01-09 04:22:32,333 iteration 4449 : loss : 0.023845, loss_ce: 0.010317
2022-01-09 04:22:35,201 iteration 4450 : loss : 0.019553, loss_ce: 0.006254
2022-01-09 04:22:37,757 iteration 4451 : loss : 0.018815, loss_ce: 0.005749
2022-01-09 04:22:40,598 iteration 4452 : loss : 0.023487, loss_ce: 0.010027
2022-01-09 04:22:43,626 iteration 4453 : loss : 0.024449, loss_ce: 0.009737
2022-01-09 04:22:46,348 iteration 4454 : loss : 0.021087, loss_ce: 0.009058
 66%|█████████████████▋         | 262/400 [3:44:39<1:57:43, 51.19s/it]2022-01-09 04:22:49,325 iteration 4455 : loss : 0.025568, loss_ce: 0.009211
2022-01-09 04:22:52,098 iteration 4456 : loss : 0.022330, loss_ce: 0.009559
2022-01-09 04:22:54,868 iteration 4457 : loss : 0.013989, loss_ce: 0.006637
2022-01-09 04:22:57,720 iteration 4458 : loss : 0.022500, loss_ce: 0.009947
2022-01-09 04:23:00,444 iteration 4459 : loss : 0.015825, loss_ce: 0.006247
2022-01-09 04:23:03,196 iteration 4460 : loss : 0.014219, loss_ce: 0.004990
2022-01-09 04:23:06,047 iteration 4461 : loss : 0.018753, loss_ce: 0.008580
2022-01-09 04:23:08,672 iteration 4462 : loss : 0.015479, loss_ce: 0.005851
2022-01-09 04:23:11,341 iteration 4463 : loss : 0.018804, loss_ce: 0.006303
2022-01-09 04:23:14,008 iteration 4464 : loss : 0.020305, loss_ce: 0.007818
2022-01-09 04:23:16,992 iteration 4465 : loss : 0.038093, loss_ce: 0.020382
2022-01-09 04:23:19,835 iteration 4466 : loss : 0.030109, loss_ce: 0.010146
2022-01-09 04:23:22,499 iteration 4467 : loss : 0.016698, loss_ce: 0.006139
2022-01-09 04:23:25,373 iteration 4468 : loss : 0.017055, loss_ce: 0.005478
2022-01-09 04:23:28,025 iteration 4469 : loss : 0.018623, loss_ce: 0.005645
2022-01-09 04:23:30,774 iteration 4470 : loss : 0.035621, loss_ce: 0.018347
2022-01-09 04:23:33,607 iteration 4471 : loss : 0.019905, loss_ce: 0.006630
 66%|█████████████████▊         | 263/400 [3:45:26<1:54:10, 50.01s/it]2022-01-09 04:23:36,438 iteration 4472 : loss : 0.017796, loss_ce: 0.007413
2022-01-09 04:23:39,290 iteration 4473 : loss : 0.017036, loss_ce: 0.006951
2022-01-09 04:23:42,112 iteration 4474 : loss : 0.018479, loss_ce: 0.005493
2022-01-09 04:23:44,890 iteration 4475 : loss : 0.017241, loss_ce: 0.007475
2022-01-09 04:23:47,626 iteration 4476 : loss : 0.014719, loss_ce: 0.004730
2022-01-09 04:23:50,405 iteration 4477 : loss : 0.018626, loss_ce: 0.005954
2022-01-09 04:23:53,242 iteration 4478 : loss : 0.037551, loss_ce: 0.010892
2022-01-09 04:23:56,097 iteration 4479 : loss : 0.064986, loss_ce: 0.008755
2022-01-09 04:23:58,834 iteration 4480 : loss : 0.020905, loss_ce: 0.008483
2022-01-09 04:24:01,633 iteration 4481 : loss : 0.013378, loss_ce: 0.005117
2022-01-09 04:24:04,444 iteration 4482 : loss : 0.024179, loss_ce: 0.008304
2022-01-09 04:24:07,381 iteration 4483 : loss : 0.020236, loss_ce: 0.008059
2022-01-09 04:24:10,097 iteration 4484 : loss : 0.022626, loss_ce: 0.007844
2022-01-09 04:24:12,906 iteration 4485 : loss : 0.018040, loss_ce: 0.005305
2022-01-09 04:24:15,796 iteration 4486 : loss : 0.032467, loss_ce: 0.013564
2022-01-09 04:24:18,626 iteration 4487 : loss : 0.023072, loss_ce: 0.009312
2022-01-09 04:24:21,302 iteration 4488 : loss : 0.025048, loss_ce: 0.009998
 66%|█████████████████▊         | 264/400 [3:46:14<1:51:47, 49.32s/it]2022-01-09 04:24:24,359 iteration 4489 : loss : 0.023420, loss_ce: 0.009480
2022-01-09 04:24:27,216 iteration 4490 : loss : 0.013780, loss_ce: 0.005558
2022-01-09 04:24:30,091 iteration 4491 : loss : 0.025583, loss_ce: 0.009835
2022-01-09 04:24:32,704 iteration 4492 : loss : 0.018239, loss_ce: 0.006831
2022-01-09 04:24:35,408 iteration 4493 : loss : 0.039328, loss_ce: 0.013551
2022-01-09 04:24:38,072 iteration 4494 : loss : 0.017919, loss_ce: 0.006803
2022-01-09 04:24:40,831 iteration 4495 : loss : 0.020001, loss_ce: 0.009156
2022-01-09 04:24:43,677 iteration 4496 : loss : 0.019251, loss_ce: 0.005928
2022-01-09 04:24:46,479 iteration 4497 : loss : 0.018077, loss_ce: 0.005090
2022-01-09 04:24:49,334 iteration 4498 : loss : 0.025603, loss_ce: 0.011242
2022-01-09 04:24:52,128 iteration 4499 : loss : 0.022935, loss_ce: 0.010320
2022-01-09 04:24:54,884 iteration 4500 : loss : 0.017470, loss_ce: 0.007301
2022-01-09 04:24:57,666 iteration 4501 : loss : 0.019034, loss_ce: 0.006698
2022-01-09 04:25:00,496 iteration 4502 : loss : 0.021476, loss_ce: 0.007808
2022-01-09 04:25:03,403 iteration 4503 : loss : 0.023684, loss_ce: 0.009732
2022-01-09 04:25:06,081 iteration 4504 : loss : 0.020144, loss_ce: 0.005754
2022-01-09 04:25:06,081 Training Data Eval:
2022-01-09 04:25:21,122   Average segmentation loss on training set: 0.0121
2022-01-09 04:25:21,122 Validation Data Eval:
2022-01-09 04:25:26,332   Average segmentation loss on validation set: 0.0995
2022-01-09 04:25:28,966 iteration 4505 : loss : 0.012811, loss_ce: 0.005038
 66%|█████████████████▉         | 265/400 [3:47:22<2:03:20, 54.82s/it]2022-01-09 04:25:31,859 iteration 4506 : loss : 0.025490, loss_ce: 0.008045
2022-01-09 04:25:34,677 iteration 4507 : loss : 0.015061, loss_ce: 0.004433
2022-01-09 04:25:37,395 iteration 4508 : loss : 0.029730, loss_ce: 0.006504
2022-01-09 04:25:40,293 iteration 4509 : loss : 0.021475, loss_ce: 0.006879
2022-01-09 04:25:42,956 iteration 4510 : loss : 0.017799, loss_ce: 0.007405
2022-01-09 04:25:45,707 iteration 4511 : loss : 0.019717, loss_ce: 0.007361
2022-01-09 04:25:48,524 iteration 4512 : loss : 0.012215, loss_ce: 0.005039
2022-01-09 04:25:51,392 iteration 4513 : loss : 0.021837, loss_ce: 0.013411
2022-01-09 04:25:54,157 iteration 4514 : loss : 0.016318, loss_ce: 0.005989
2022-01-09 04:25:56,969 iteration 4515 : loss : 0.018886, loss_ce: 0.006652
2022-01-09 04:25:59,862 iteration 4516 : loss : 0.022151, loss_ce: 0.008973
2022-01-09 04:26:02,668 iteration 4517 : loss : 0.018867, loss_ce: 0.006241
2022-01-09 04:26:05,479 iteration 4518 : loss : 0.021124, loss_ce: 0.006720
2022-01-09 04:26:08,129 iteration 4519 : loss : 0.021305, loss_ce: 0.009352
2022-01-09 04:26:10,879 iteration 4520 : loss : 0.015876, loss_ce: 0.005514
2022-01-09 04:26:13,722 iteration 4521 : loss : 0.022738, loss_ce: 0.007794
2022-01-09 04:26:16,333 iteration 4522 : loss : 0.017818, loss_ce: 0.008467
 66%|█████████████████▉         | 266/400 [3:48:09<1:57:26, 52.58s/it]2022-01-09 04:26:19,120 iteration 4523 : loss : 0.016020, loss_ce: 0.005602
2022-01-09 04:26:21,679 iteration 4524 : loss : 0.014814, loss_ce: 0.006095
2022-01-09 04:26:24,564 iteration 4525 : loss : 0.027507, loss_ce: 0.007778
2022-01-09 04:26:27,276 iteration 4526 : loss : 0.014897, loss_ce: 0.006351
2022-01-09 04:26:30,123 iteration 4527 : loss : 0.016852, loss_ce: 0.007312
2022-01-09 04:26:32,969 iteration 4528 : loss : 0.014516, loss_ce: 0.005241
2022-01-09 04:26:35,584 iteration 4529 : loss : 0.022676, loss_ce: 0.008767
2022-01-09 04:26:38,394 iteration 4530 : loss : 0.023243, loss_ce: 0.010488
2022-01-09 04:26:40,976 iteration 4531 : loss : 0.018062, loss_ce: 0.007751
2022-01-09 04:26:43,646 iteration 4532 : loss : 0.017569, loss_ce: 0.007823
2022-01-09 04:26:46,520 iteration 4533 : loss : 0.030885, loss_ce: 0.011580
2022-01-09 04:26:49,459 iteration 4534 : loss : 0.021141, loss_ce: 0.007762
2022-01-09 04:26:52,013 iteration 4535 : loss : 0.016356, loss_ce: 0.005090
2022-01-09 04:26:54,682 iteration 4536 : loss : 0.020264, loss_ce: 0.011203
2022-01-09 04:26:57,592 iteration 4537 : loss : 0.021589, loss_ce: 0.007335
2022-01-09 04:27:00,411 iteration 4538 : loss : 0.017414, loss_ce: 0.008270
2022-01-09 04:27:03,278 iteration 4539 : loss : 0.049948, loss_ce: 0.022941
 67%|██████████████████         | 267/400 [3:48:56<1:52:48, 50.89s/it]2022-01-09 04:27:06,174 iteration 4540 : loss : 0.018360, loss_ce: 0.005292
2022-01-09 04:27:08,889 iteration 4541 : loss : 0.016392, loss_ce: 0.005836
2022-01-09 04:27:11,850 iteration 4542 : loss : 0.019477, loss_ce: 0.004914
2022-01-09 04:27:14,384 iteration 4543 : loss : 0.014938, loss_ce: 0.005725
2022-01-09 04:27:17,278 iteration 4544 : loss : 0.024759, loss_ce: 0.012243
2022-01-09 04:27:19,975 iteration 4545 : loss : 0.023709, loss_ce: 0.011328
2022-01-09 04:27:22,810 iteration 4546 : loss : 0.027194, loss_ce: 0.009259
2022-01-09 04:27:25,666 iteration 4547 : loss : 0.015428, loss_ce: 0.007495
2022-01-09 04:27:28,478 iteration 4548 : loss : 0.031410, loss_ce: 0.012493
2022-01-09 04:27:31,358 iteration 4549 : loss : 0.021080, loss_ce: 0.007122
2022-01-09 04:27:34,105 iteration 4550 : loss : 0.035243, loss_ce: 0.010579
2022-01-09 04:27:36,973 iteration 4551 : loss : 0.022621, loss_ce: 0.005961
2022-01-09 04:27:39,688 iteration 4552 : loss : 0.013358, loss_ce: 0.005209
2022-01-09 04:27:42,442 iteration 4553 : loss : 0.017896, loss_ce: 0.006714
2022-01-09 04:27:45,054 iteration 4554 : loss : 0.015450, loss_ce: 0.005626
2022-01-09 04:27:47,949 iteration 4555 : loss : 0.023160, loss_ce: 0.008169
2022-01-09 04:27:50,634 iteration 4556 : loss : 0.019970, loss_ce: 0.009342
 67%|██████████████████         | 268/400 [3:49:43<1:49:37, 49.83s/it]2022-01-09 04:27:53,500 iteration 4557 : loss : 0.027718, loss_ce: 0.012234
2022-01-09 04:27:56,334 iteration 4558 : loss : 0.023853, loss_ce: 0.011362
2022-01-09 04:27:59,196 iteration 4559 : loss : 0.036155, loss_ce: 0.011178
2022-01-09 04:28:01,948 iteration 4560 : loss : 0.015994, loss_ce: 0.004457
2022-01-09 04:28:04,846 iteration 4561 : loss : 0.018007, loss_ce: 0.005388
2022-01-09 04:28:07,656 iteration 4562 : loss : 0.019289, loss_ce: 0.007403
2022-01-09 04:28:10,459 iteration 4563 : loss : 0.015595, loss_ce: 0.005567
2022-01-09 04:28:13,092 iteration 4564 : loss : 0.029437, loss_ce: 0.008884
2022-01-09 04:28:15,733 iteration 4565 : loss : 0.019643, loss_ce: 0.007527
2022-01-09 04:28:18,641 iteration 4566 : loss : 0.022339, loss_ce: 0.011488
2022-01-09 04:28:21,307 iteration 4567 : loss : 0.022056, loss_ce: 0.008825
2022-01-09 04:28:24,175 iteration 4568 : loss : 0.017019, loss_ce: 0.005308
2022-01-09 04:28:26,842 iteration 4569 : loss : 0.017233, loss_ce: 0.007036
2022-01-09 04:28:29,617 iteration 4570 : loss : 0.018751, loss_ce: 0.007802
2022-01-09 04:28:32,291 iteration 4571 : loss : 0.015670, loss_ce: 0.008136
2022-01-09 04:28:35,090 iteration 4572 : loss : 0.014010, loss_ce: 0.004575
2022-01-09 04:28:37,905 iteration 4573 : loss : 0.019011, loss_ce: 0.007918
 67%|██████████████████▏        | 269/400 [3:50:31<1:47:07, 49.06s/it]2022-01-09 04:28:40,769 iteration 4574 : loss : 0.019581, loss_ce: 0.006404
2022-01-09 04:28:43,560 iteration 4575 : loss : 0.014640, loss_ce: 0.006156
2022-01-09 04:28:46,230 iteration 4576 : loss : 0.017768, loss_ce: 0.009540
2022-01-09 04:28:49,065 iteration 4577 : loss : 0.022793, loss_ce: 0.008322
2022-01-09 04:28:51,860 iteration 4578 : loss : 0.016124, loss_ce: 0.006496
2022-01-09 04:28:54,694 iteration 4579 : loss : 0.016546, loss_ce: 0.007665
2022-01-09 04:28:57,446 iteration 4580 : loss : 0.018235, loss_ce: 0.007658
2022-01-09 04:29:00,344 iteration 4581 : loss : 0.020847, loss_ce: 0.006347
2022-01-09 04:29:03,249 iteration 4582 : loss : 0.016992, loss_ce: 0.007548
2022-01-09 04:29:05,914 iteration 4583 : loss : 0.016941, loss_ce: 0.006590
2022-01-09 04:29:08,764 iteration 4584 : loss : 0.024463, loss_ce: 0.009433
2022-01-09 04:29:11,399 iteration 4585 : loss : 0.026318, loss_ce: 0.008112
2022-01-09 04:29:14,213 iteration 4586 : loss : 0.014098, loss_ce: 0.005874
2022-01-09 04:29:17,090 iteration 4587 : loss : 0.026708, loss_ce: 0.009837
2022-01-09 04:29:19,909 iteration 4588 : loss : 0.023059, loss_ce: 0.007257
2022-01-09 04:29:22,593 iteration 4589 : loss : 0.016750, loss_ce: 0.006912
2022-01-09 04:29:22,593 Training Data Eval:
2022-01-09 04:29:37,489   Average segmentation loss on training set: 0.0115
2022-01-09 04:29:37,489 Validation Data Eval:
2022-01-09 04:29:42,685   Average segmentation loss on validation set: 0.0714
2022-01-09 04:29:45,638 iteration 4590 : loss : 0.019742, loss_ce: 0.007118
 68%|██████████████████▏        | 270/400 [3:51:38<1:58:26, 54.66s/it]2022-01-09 04:29:48,282 iteration 4591 : loss : 0.014804, loss_ce: 0.004143
2022-01-09 04:29:51,059 iteration 4592 : loss : 0.018085, loss_ce: 0.006956
2022-01-09 04:29:53,755 iteration 4593 : loss : 0.015603, loss_ce: 0.004653
2022-01-09 04:29:56,571 iteration 4594 : loss : 0.017708, loss_ce: 0.007702
2022-01-09 04:29:59,432 iteration 4595 : loss : 0.026394, loss_ce: 0.005048
2022-01-09 04:30:02,301 iteration 4596 : loss : 0.023783, loss_ce: 0.008086
2022-01-09 04:30:05,080 iteration 4597 : loss : 0.016416, loss_ce: 0.006713
2022-01-09 04:30:07,833 iteration 4598 : loss : 0.017654, loss_ce: 0.008214
2022-01-09 04:30:10,420 iteration 4599 : loss : 0.020380, loss_ce: 0.004724
2022-01-09 04:30:13,245 iteration 4600 : loss : 0.024416, loss_ce: 0.011873
2022-01-09 04:30:16,137 iteration 4601 : loss : 0.018574, loss_ce: 0.009328
2022-01-09 04:30:18,892 iteration 4602 : loss : 0.013832, loss_ce: 0.005110
2022-01-09 04:30:21,467 iteration 4603 : loss : 0.019611, loss_ce: 0.009326
2022-01-09 04:30:24,281 iteration 4604 : loss : 0.022452, loss_ce: 0.007104
2022-01-09 04:30:27,086 iteration 4605 : loss : 0.023081, loss_ce: 0.006009
2022-01-09 04:30:29,938 iteration 4606 : loss : 0.020126, loss_ce: 0.006878
2022-01-09 04:30:32,516 iteration 4607 : loss : 0.016193, loss_ce: 0.008966
 68%|██████████████████▎        | 271/400 [3:52:25<1:52:30, 52.33s/it]2022-01-09 04:30:35,242 iteration 4608 : loss : 0.031966, loss_ce: 0.009963
2022-01-09 04:30:38,115 iteration 4609 : loss : 0.027911, loss_ce: 0.014287
2022-01-09 04:30:40,776 iteration 4610 : loss : 0.027194, loss_ce: 0.006769
2022-01-09 04:30:43,652 iteration 4611 : loss : 0.020650, loss_ce: 0.008503
2022-01-09 04:30:46,392 iteration 4612 : loss : 0.015238, loss_ce: 0.005456
2022-01-09 04:30:49,082 iteration 4613 : loss : 0.018961, loss_ce: 0.008265
2022-01-09 04:30:51,886 iteration 4614 : loss : 0.024957, loss_ce: 0.008595
2022-01-09 04:30:54,712 iteration 4615 : loss : 0.037219, loss_ce: 0.015292
2022-01-09 04:30:57,445 iteration 4616 : loss : 0.022324, loss_ce: 0.007952
2022-01-09 04:31:00,236 iteration 4617 : loss : 0.019885, loss_ce: 0.008708
2022-01-09 04:31:03,040 iteration 4618 : loss : 0.029059, loss_ce: 0.008755
2022-01-09 04:31:05,889 iteration 4619 : loss : 0.021700, loss_ce: 0.008533
2022-01-09 04:31:08,736 iteration 4620 : loss : 0.016421, loss_ce: 0.005171
2022-01-09 04:31:11,582 iteration 4621 : loss : 0.024021, loss_ce: 0.008935
2022-01-09 04:31:14,347 iteration 4622 : loss : 0.018933, loss_ce: 0.006639
2022-01-09 04:31:17,161 iteration 4623 : loss : 0.021820, loss_ce: 0.008161
2022-01-09 04:31:19,815 iteration 4624 : loss : 0.017619, loss_ce: 0.006911
 68%|██████████████████▎        | 272/400 [3:53:12<1:48:24, 50.82s/it]2022-01-09 04:31:22,697 iteration 4625 : loss : 0.014240, loss_ce: 0.005989
2022-01-09 04:31:25,330 iteration 4626 : loss : 0.014873, loss_ce: 0.004662
2022-01-09 04:31:28,132 iteration 4627 : loss : 0.016733, loss_ce: 0.005303
2022-01-09 04:31:30,809 iteration 4628 : loss : 0.023164, loss_ce: 0.008143
2022-01-09 04:31:33,623 iteration 4629 : loss : 0.017323, loss_ce: 0.007367
2022-01-09 04:31:36,373 iteration 4630 : loss : 0.026354, loss_ce: 0.008581
2022-01-09 04:31:39,239 iteration 4631 : loss : 0.020627, loss_ce: 0.008019
2022-01-09 04:31:42,030 iteration 4632 : loss : 0.014400, loss_ce: 0.004506
2022-01-09 04:31:44,885 iteration 4633 : loss : 0.021126, loss_ce: 0.008448
2022-01-09 04:31:47,448 iteration 4634 : loss : 0.014612, loss_ce: 0.006117
2022-01-09 04:31:50,275 iteration 4635 : loss : 0.039189, loss_ce: 0.011066
2022-01-09 04:31:53,086 iteration 4636 : loss : 0.019454, loss_ce: 0.006465
2022-01-09 04:31:55,711 iteration 4637 : loss : 0.015529, loss_ce: 0.005211
2022-01-09 04:31:58,452 iteration 4638 : loss : 0.027372, loss_ce: 0.013170
2022-01-09 04:32:01,223 iteration 4639 : loss : 0.023905, loss_ce: 0.007260
2022-01-09 04:32:03,922 iteration 4640 : loss : 0.022565, loss_ce: 0.009537
2022-01-09 04:32:06,793 iteration 4641 : loss : 0.022488, loss_ce: 0.011040
 68%|██████████████████▍        | 273/400 [3:53:59<1:45:07, 49.66s/it]2022-01-09 04:32:09,662 iteration 4642 : loss : 0.026885, loss_ce: 0.008338
2022-01-09 04:32:12,435 iteration 4643 : loss : 0.041058, loss_ce: 0.021418
2022-01-09 04:32:15,266 iteration 4644 : loss : 0.018779, loss_ce: 0.007553
2022-01-09 04:32:18,193 iteration 4645 : loss : 0.019295, loss_ce: 0.007725
2022-01-09 04:32:20,963 iteration 4646 : loss : 0.016050, loss_ce: 0.006668
2022-01-09 04:32:23,666 iteration 4647 : loss : 0.016174, loss_ce: 0.006720
2022-01-09 04:32:26,462 iteration 4648 : loss : 0.013620, loss_ce: 0.005361
2022-01-09 04:32:29,328 iteration 4649 : loss : 0.019408, loss_ce: 0.007199
2022-01-09 04:32:31,971 iteration 4650 : loss : 0.017597, loss_ce: 0.006894
2022-01-09 04:32:34,855 iteration 4651 : loss : 0.019388, loss_ce: 0.007832
2022-01-09 04:32:37,513 iteration 4652 : loss : 0.022496, loss_ce: 0.009056
2022-01-09 04:32:40,351 iteration 4653 : loss : 0.022213, loss_ce: 0.008241
2022-01-09 04:32:43,196 iteration 4654 : loss : 0.023208, loss_ce: 0.012422
2022-01-09 04:32:45,994 iteration 4655 : loss : 0.020420, loss_ce: 0.007969
2022-01-09 04:32:48,880 iteration 4656 : loss : 0.027053, loss_ce: 0.006840
2022-01-09 04:32:51,702 iteration 4657 : loss : 0.036753, loss_ce: 0.005072
2022-01-09 04:32:54,413 iteration 4658 : loss : 0.035188, loss_ce: 0.007316
 68%|██████████████████▍        | 274/400 [3:54:47<1:43:00, 49.05s/it]2022-01-09 04:32:57,273 iteration 4659 : loss : 0.018849, loss_ce: 0.007130
2022-01-09 04:33:00,020 iteration 4660 : loss : 0.018817, loss_ce: 0.004742
2022-01-09 04:33:02,849 iteration 4661 : loss : 0.021611, loss_ce: 0.011217
2022-01-09 04:33:05,719 iteration 4662 : loss : 0.028904, loss_ce: 0.015428
2022-01-09 04:33:08,522 iteration 4663 : loss : 0.020221, loss_ce: 0.006691
2022-01-09 04:33:11,201 iteration 4664 : loss : 0.020864, loss_ce: 0.006073
2022-01-09 04:33:14,184 iteration 4665 : loss : 0.025110, loss_ce: 0.011246
2022-01-09 04:33:16,886 iteration 4666 : loss : 0.015423, loss_ce: 0.005768
2022-01-09 04:33:19,600 iteration 4667 : loss : 0.021823, loss_ce: 0.008480
2022-01-09 04:33:22,291 iteration 4668 : loss : 0.023685, loss_ce: 0.006022
2022-01-09 04:33:25,168 iteration 4669 : loss : 0.018651, loss_ce: 0.008299
2022-01-09 04:33:28,024 iteration 4670 : loss : 0.020020, loss_ce: 0.008295
2022-01-09 04:33:30,776 iteration 4671 : loss : 0.014683, loss_ce: 0.004487
2022-01-09 04:33:33,703 iteration 4672 : loss : 0.022182, loss_ce: 0.008107
2022-01-09 04:33:36,404 iteration 4673 : loss : 0.016391, loss_ce: 0.005313
2022-01-09 04:33:39,273 iteration 4674 : loss : 0.021210, loss_ce: 0.012342
2022-01-09 04:33:39,273 Training Data Eval:
2022-01-09 04:33:54,420   Average segmentation loss on training set: 0.0117
2022-01-09 04:33:54,420 Validation Data Eval:
2022-01-09 04:33:59,738   Average segmentation loss on validation set: 0.0745
2022-01-09 04:34:02,430 iteration 4675 : loss : 0.021658, loss_ce: 0.009555
 69%|██████████████████▌        | 275/400 [3:55:55<1:54:03, 54.75s/it]2022-01-09 04:34:05,313 iteration 4676 : loss : 0.021214, loss_ce: 0.006928
2022-01-09 04:34:08,087 iteration 4677 : loss : 0.017615, loss_ce: 0.007463
2022-01-09 04:34:10,928 iteration 4678 : loss : 0.028590, loss_ce: 0.009292
2022-01-09 04:34:13,777 iteration 4679 : loss : 0.018335, loss_ce: 0.009657
2022-01-09 04:34:16,572 iteration 4680 : loss : 0.018070, loss_ce: 0.006288
2022-01-09 04:34:19,416 iteration 4681 : loss : 0.028459, loss_ce: 0.009327
2022-01-09 04:34:22,035 iteration 4682 : loss : 0.018763, loss_ce: 0.004580
2022-01-09 04:34:24,895 iteration 4683 : loss : 0.020904, loss_ce: 0.008868
2022-01-09 04:34:27,601 iteration 4684 : loss : 0.016951, loss_ce: 0.007379
2022-01-09 04:34:30,414 iteration 4685 : loss : 0.014402, loss_ce: 0.005925
2022-01-09 04:34:33,166 iteration 4686 : loss : 0.019151, loss_ce: 0.006184
2022-01-09 04:34:35,927 iteration 4687 : loss : 0.021852, loss_ce: 0.008910
2022-01-09 04:34:38,788 iteration 4688 : loss : 0.020119, loss_ce: 0.006557
2022-01-09 04:34:41,695 iteration 4689 : loss : 0.024688, loss_ce: 0.008971
2022-01-09 04:34:44,363 iteration 4690 : loss : 0.027245, loss_ce: 0.007710
2022-01-09 04:34:47,165 iteration 4691 : loss : 0.014273, loss_ce: 0.005051
2022-01-09 04:34:49,934 iteration 4692 : loss : 0.023066, loss_ce: 0.009655
 69%|██████████████████▋        | 276/400 [3:56:43<1:48:38, 52.57s/it]2022-01-09 04:34:52,860 iteration 4693 : loss : 0.022936, loss_ce: 0.010830
2022-01-09 04:34:55,452 iteration 4694 : loss : 0.018737, loss_ce: 0.005540
2022-01-09 04:34:58,301 iteration 4695 : loss : 0.019404, loss_ce: 0.006833
2022-01-09 04:35:01,126 iteration 4696 : loss : 0.021938, loss_ce: 0.009545
2022-01-09 04:35:03,861 iteration 4697 : loss : 0.026586, loss_ce: 0.011220
2022-01-09 04:35:06,481 iteration 4698 : loss : 0.028957, loss_ce: 0.014400
2022-01-09 04:35:09,359 iteration 4699 : loss : 0.019635, loss_ce: 0.006759
2022-01-09 04:35:12,069 iteration 4700 : loss : 0.040527, loss_ce: 0.012811
2022-01-09 04:35:14,952 iteration 4701 : loss : 0.024526, loss_ce: 0.010173
2022-01-09 04:35:17,698 iteration 4702 : loss : 0.019101, loss_ce: 0.006527
2022-01-09 04:35:20,409 iteration 4703 : loss : 0.017282, loss_ce: 0.005391
2022-01-09 04:35:23,233 iteration 4704 : loss : 0.023927, loss_ce: 0.005747
2022-01-09 04:35:25,812 iteration 4705 : loss : 0.013740, loss_ce: 0.005147
2022-01-09 04:35:28,572 iteration 4706 : loss : 0.016282, loss_ce: 0.007030
2022-01-09 04:35:31,423 iteration 4707 : loss : 0.018751, loss_ce: 0.008065
2022-01-09 04:35:34,259 iteration 4708 : loss : 0.022506, loss_ce: 0.004630
2022-01-09 04:35:36,871 iteration 4709 : loss : 0.016975, loss_ce: 0.006792
 69%|██████████████████▋        | 277/400 [3:57:30<1:44:17, 50.88s/it]2022-01-09 04:35:39,637 iteration 4710 : loss : 0.029105, loss_ce: 0.010050
2022-01-09 04:35:42,409 iteration 4711 : loss : 0.022556, loss_ce: 0.010699
2022-01-09 04:35:45,205 iteration 4712 : loss : 0.015878, loss_ce: 0.006698
2022-01-09 04:35:47,795 iteration 4713 : loss : 0.013624, loss_ce: 0.005418
2022-01-09 04:35:50,397 iteration 4714 : loss : 0.013319, loss_ce: 0.005870
2022-01-09 04:35:53,319 iteration 4715 : loss : 0.027751, loss_ce: 0.008187
2022-01-09 04:35:56,186 iteration 4716 : loss : 0.013505, loss_ce: 0.005098
2022-01-09 04:35:58,954 iteration 4717 : loss : 0.020456, loss_ce: 0.005779
2022-01-09 04:36:01,739 iteration 4718 : loss : 0.014835, loss_ce: 0.005205
2022-01-09 04:36:04,416 iteration 4719 : loss : 0.013979, loss_ce: 0.005100
2022-01-09 04:36:07,296 iteration 4720 : loss : 0.016704, loss_ce: 0.006870
2022-01-09 04:36:10,140 iteration 4721 : loss : 0.022374, loss_ce: 0.007444
2022-01-09 04:36:12,977 iteration 4722 : loss : 0.024728, loss_ce: 0.010551
2022-01-09 04:36:15,781 iteration 4723 : loss : 0.026542, loss_ce: 0.008943
2022-01-09 04:36:18,549 iteration 4724 : loss : 0.017955, loss_ce: 0.009055
2022-01-09 04:36:21,161 iteration 4725 : loss : 0.016012, loss_ce: 0.006270
2022-01-09 04:36:23,934 iteration 4726 : loss : 0.019222, loss_ce: 0.006267
 70%|██████████████████▊        | 278/400 [3:58:17<1:41:07, 49.74s/it]2022-01-09 04:36:26,777 iteration 4727 : loss : 0.024465, loss_ce: 0.008621
2022-01-09 04:36:29,587 iteration 4728 : loss : 0.025813, loss_ce: 0.010811
2022-01-09 04:36:32,333 iteration 4729 : loss : 0.015288, loss_ce: 0.004872
2022-01-09 04:36:35,133 iteration 4730 : loss : 0.018732, loss_ce: 0.007474
2022-01-09 04:36:37,957 iteration 4731 : loss : 0.028004, loss_ce: 0.006862
2022-01-09 04:36:40,550 iteration 4732 : loss : 0.015474, loss_ce: 0.006558
2022-01-09 04:36:43,420 iteration 4733 : loss : 0.018847, loss_ce: 0.008784
2022-01-09 04:36:46,208 iteration 4734 : loss : 0.015971, loss_ce: 0.006070
2022-01-09 04:36:49,019 iteration 4735 : loss : 0.014814, loss_ce: 0.005961
2022-01-09 04:36:51,857 iteration 4736 : loss : 0.031297, loss_ce: 0.010653
2022-01-09 04:36:54,625 iteration 4737 : loss : 0.019216, loss_ce: 0.009368
2022-01-09 04:36:57,545 iteration 4738 : loss : 0.032962, loss_ce: 0.009644
2022-01-09 04:37:00,390 iteration 4739 : loss : 0.014410, loss_ce: 0.005882
2022-01-09 04:37:03,149 iteration 4740 : loss : 0.013906, loss_ce: 0.003807
2022-01-09 04:37:05,786 iteration 4741 : loss : 0.023119, loss_ce: 0.009910
2022-01-09 04:37:08,538 iteration 4742 : loss : 0.031915, loss_ce: 0.015489
2022-01-09 04:37:11,418 iteration 4743 : loss : 0.032055, loss_ce: 0.016028
 70%|██████████████████▊        | 279/400 [3:59:04<1:38:55, 49.06s/it]2022-01-09 04:37:14,311 iteration 4744 : loss : 0.021412, loss_ce: 0.006930
2022-01-09 04:37:17,089 iteration 4745 : loss : 0.019020, loss_ce: 0.008129
2022-01-09 04:37:19,958 iteration 4746 : loss : 0.036435, loss_ce: 0.013898
2022-01-09 04:37:22,695 iteration 4747 : loss : 0.018409, loss_ce: 0.010048
2022-01-09 04:37:25,576 iteration 4748 : loss : 0.018543, loss_ce: 0.007471
2022-01-09 04:37:28,332 iteration 4749 : loss : 0.039561, loss_ce: 0.025199
2022-01-09 04:37:31,216 iteration 4750 : loss : 0.021051, loss_ce: 0.005902
2022-01-09 04:37:33,975 iteration 4751 : loss : 0.031176, loss_ce: 0.015311
2022-01-09 04:37:36,942 iteration 4752 : loss : 0.025337, loss_ce: 0.007135
2022-01-09 04:37:39,694 iteration 4753 : loss : 0.013547, loss_ce: 0.004450
2022-01-09 04:37:42,510 iteration 4754 : loss : 0.024255, loss_ce: 0.012535
2022-01-09 04:37:45,407 iteration 4755 : loss : 0.024440, loss_ce: 0.007196
2022-01-09 04:37:48,106 iteration 4756 : loss : 0.026991, loss_ce: 0.008234
2022-01-09 04:37:50,912 iteration 4757 : loss : 0.016433, loss_ce: 0.008232
2022-01-09 04:37:53,804 iteration 4758 : loss : 0.024793, loss_ce: 0.008042
2022-01-09 04:37:56,521 iteration 4759 : loss : 0.019824, loss_ce: 0.010647
2022-01-09 04:37:56,522 Training Data Eval:
2022-01-09 04:38:11,787   Average segmentation loss on training set: 0.0112
2022-01-09 04:38:11,787 Validation Data Eval:
2022-01-09 04:38:17,134   Average segmentation loss on validation set: 0.0698
2022-01-09 04:38:19,982 iteration 4760 : loss : 0.018799, loss_ce: 0.005820
 70%|██████████████████▉        | 280/400 [4:00:13<1:49:49, 54.91s/it]2022-01-09 04:38:22,801 iteration 4761 : loss : 0.019315, loss_ce: 0.006556
2022-01-09 04:38:25,557 iteration 4762 : loss : 0.032688, loss_ce: 0.009854
2022-01-09 04:38:28,388 iteration 4763 : loss : 0.019015, loss_ce: 0.009291
2022-01-09 04:38:31,171 iteration 4764 : loss : 0.017676, loss_ce: 0.006737
2022-01-09 04:38:33,808 iteration 4765 : loss : 0.019156, loss_ce: 0.008174
2022-01-09 04:38:36,566 iteration 4766 : loss : 0.016945, loss_ce: 0.005498
2022-01-09 04:38:39,413 iteration 4767 : loss : 0.024238, loss_ce: 0.008505
2022-01-09 04:38:42,305 iteration 4768 : loss : 0.025755, loss_ce: 0.012251
2022-01-09 04:38:45,174 iteration 4769 : loss : 0.031418, loss_ce: 0.007081
2022-01-09 04:38:47,941 iteration 4770 : loss : 0.015092, loss_ce: 0.007531
2022-01-09 04:38:50,683 iteration 4771 : loss : 0.020384, loss_ce: 0.008916
2022-01-09 04:38:53,336 iteration 4772 : loss : 0.011954, loss_ce: 0.003679
2022-01-09 04:38:56,197 iteration 4773 : loss : 0.022672, loss_ce: 0.009501
2022-01-09 04:38:58,798 iteration 4774 : loss : 0.021039, loss_ce: 0.008369
2022-01-09 04:39:01,623 iteration 4775 : loss : 0.023857, loss_ce: 0.009604
2022-01-09 04:39:04,404 iteration 4776 : loss : 0.017764, loss_ce: 0.007378
2022-01-09 04:39:07,120 iteration 4777 : loss : 0.016885, loss_ce: 0.005701
 70%|██████████████████▉        | 281/400 [4:01:00<1:44:17, 52.58s/it]2022-01-09 04:39:10,028 iteration 4778 : loss : 0.020600, loss_ce: 0.007083
2022-01-09 04:39:12,747 iteration 4779 : loss : 0.014385, loss_ce: 0.005202
2022-01-09 04:39:15,382 iteration 4780 : loss : 0.022771, loss_ce: 0.006535
2022-01-09 04:39:18,141 iteration 4781 : loss : 0.013307, loss_ce: 0.006005
2022-01-09 04:39:20,976 iteration 4782 : loss : 0.024531, loss_ce: 0.008491
2022-01-09 04:39:23,802 iteration 4783 : loss : 0.015154, loss_ce: 0.004227
2022-01-09 04:39:26,705 iteration 4784 : loss : 0.015577, loss_ce: 0.005666
2022-01-09 04:39:29,584 iteration 4785 : loss : 0.028139, loss_ce: 0.013982
2022-01-09 04:39:32,363 iteration 4786 : loss : 0.017303, loss_ce: 0.006461
2022-01-09 04:39:35,379 iteration 4787 : loss : 0.017649, loss_ce: 0.004494
2022-01-09 04:39:38,078 iteration 4788 : loss : 0.016248, loss_ce: 0.007236
2022-01-09 04:39:40,890 iteration 4789 : loss : 0.015083, loss_ce: 0.005464
2022-01-09 04:39:43,734 iteration 4790 : loss : 0.023726, loss_ce: 0.012198
2022-01-09 04:39:46,407 iteration 4791 : loss : 0.015262, loss_ce: 0.005821
2022-01-09 04:39:49,177 iteration 4792 : loss : 0.024382, loss_ce: 0.010743
2022-01-09 04:39:51,860 iteration 4793 : loss : 0.015838, loss_ce: 0.006896
2022-01-09 04:39:54,784 iteration 4794 : loss : 0.024234, loss_ce: 0.005819
 70%|███████████████████        | 282/400 [4:01:47<1:40:30, 51.11s/it]2022-01-09 04:39:57,420 iteration 4795 : loss : 0.020074, loss_ce: 0.005794
2022-01-09 04:40:00,188 iteration 4796 : loss : 0.015419, loss_ce: 0.006281
2022-01-09 04:40:02,984 iteration 4797 : loss : 0.016552, loss_ce: 0.006256
2022-01-09 04:40:05,778 iteration 4798 : loss : 0.021947, loss_ce: 0.010661
2022-01-09 04:40:08,590 iteration 4799 : loss : 0.026180, loss_ce: 0.006096
2022-01-09 04:40:11,394 iteration 4800 : loss : 0.034933, loss_ce: 0.014851
2022-01-09 04:40:14,231 iteration 4801 : loss : 0.014416, loss_ce: 0.006097
2022-01-09 04:40:16,815 iteration 4802 : loss : 0.012497, loss_ce: 0.003912
2022-01-09 04:40:19,626 iteration 4803 : loss : 0.018078, loss_ce: 0.007795
2022-01-09 04:40:22,440 iteration 4804 : loss : 0.015470, loss_ce: 0.004544
2022-01-09 04:40:25,060 iteration 4805 : loss : 0.016452, loss_ce: 0.005327
2022-01-09 04:40:27,770 iteration 4806 : loss : 0.027747, loss_ce: 0.007078
2022-01-09 04:40:30,646 iteration 4807 : loss : 0.015018, loss_ce: 0.005630
2022-01-09 04:40:33,525 iteration 4808 : loss : 0.024148, loss_ce: 0.008148
2022-01-09 04:40:36,269 iteration 4809 : loss : 0.016684, loss_ce: 0.006953
2022-01-09 04:40:39,110 iteration 4810 : loss : 0.019882, loss_ce: 0.007786
2022-01-09 04:40:41,895 iteration 4811 : loss : 0.025657, loss_ce: 0.010345
 71%|███████████████████        | 283/400 [4:02:35<1:37:18, 49.90s/it]2022-01-09 04:40:44,537 iteration 4812 : loss : 0.022889, loss_ce: 0.007535
2022-01-09 04:40:47,313 iteration 4813 : loss : 0.022843, loss_ce: 0.008908
2022-01-09 04:40:50,084 iteration 4814 : loss : 0.014181, loss_ce: 0.007029
2022-01-09 04:40:52,954 iteration 4815 : loss : 0.019496, loss_ce: 0.007265
2022-01-09 04:40:55,805 iteration 4816 : loss : 0.014696, loss_ce: 0.005318
2022-01-09 04:40:58,502 iteration 4817 : loss : 0.017212, loss_ce: 0.005611
2022-01-09 04:41:01,419 iteration 4818 : loss : 0.022651, loss_ce: 0.009935
2022-01-09 04:41:04,137 iteration 4819 : loss : 0.017139, loss_ce: 0.005197
2022-01-09 04:41:06,997 iteration 4820 : loss : 0.015289, loss_ce: 0.006571
2022-01-09 04:41:09,775 iteration 4821 : loss : 0.017156, loss_ce: 0.007542
2022-01-09 04:41:12,599 iteration 4822 : loss : 0.027350, loss_ce: 0.008941
2022-01-09 04:41:15,233 iteration 4823 : loss : 0.034913, loss_ce: 0.011197
2022-01-09 04:41:18,132 iteration 4824 : loss : 0.016270, loss_ce: 0.005594
2022-01-09 04:41:20,991 iteration 4825 : loss : 0.011534, loss_ce: 0.003455
2022-01-09 04:41:23,672 iteration 4826 : loss : 0.020980, loss_ce: 0.006351
2022-01-09 04:41:26,576 iteration 4827 : loss : 0.021633, loss_ce: 0.008151
2022-01-09 04:41:29,426 iteration 4828 : loss : 0.021745, loss_ce: 0.008177
 71%|███████████████████▏       | 284/400 [4:03:22<1:35:06, 49.19s/it]2022-01-09 04:41:32,307 iteration 4829 : loss : 0.024428, loss_ce: 0.007641
2022-01-09 04:41:34,955 iteration 4830 : loss : 0.016131, loss_ce: 0.005285
2022-01-09 04:41:37,857 iteration 4831 : loss : 0.030549, loss_ce: 0.010588
2022-01-09 04:41:40,476 iteration 4832 : loss : 0.017386, loss_ce: 0.007025
2022-01-09 04:41:43,344 iteration 4833 : loss : 0.019461, loss_ce: 0.005987
2022-01-09 04:41:46,264 iteration 4834 : loss : 0.030113, loss_ce: 0.010503
2022-01-09 04:41:49,072 iteration 4835 : loss : 0.022984, loss_ce: 0.007731
2022-01-09 04:41:51,926 iteration 4836 : loss : 0.025894, loss_ce: 0.009389
2022-01-09 04:41:54,662 iteration 4837 : loss : 0.015634, loss_ce: 0.003396
2022-01-09 04:41:57,494 iteration 4838 : loss : 0.020880, loss_ce: 0.010612
2022-01-09 04:42:00,398 iteration 4839 : loss : 0.015645, loss_ce: 0.007660
2022-01-09 04:42:03,086 iteration 4840 : loss : 0.022223, loss_ce: 0.008525
2022-01-09 04:42:05,964 iteration 4841 : loss : 0.017641, loss_ce: 0.006243
2022-01-09 04:42:08,795 iteration 4842 : loss : 0.020778, loss_ce: 0.009319
2022-01-09 04:42:11,604 iteration 4843 : loss : 0.020798, loss_ce: 0.006387
2022-01-09 04:42:14,265 iteration 4844 : loss : 0.015613, loss_ce: 0.007237
2022-01-09 04:42:14,265 Training Data Eval:
2022-01-09 04:42:29,295   Average segmentation loss on training set: 0.0110
2022-01-09 04:42:29,295 Validation Data Eval:
2022-01-09 04:42:34,602   Average segmentation loss on validation set: 0.0702
2022-01-09 04:42:37,406 iteration 4845 : loss : 0.020203, loss_ce: 0.007337
 71%|███████████████████▏       | 285/400 [4:04:30<1:45:05, 54.83s/it]2022-01-09 04:42:40,285 iteration 4846 : loss : 0.018960, loss_ce: 0.006467
2022-01-09 04:42:42,987 iteration 4847 : loss : 0.015459, loss_ce: 0.004365
2022-01-09 04:42:45,935 iteration 4848 : loss : 0.023733, loss_ce: 0.009754
2022-01-09 04:42:48,670 iteration 4849 : loss : 0.016621, loss_ce: 0.007566
2022-01-09 04:42:51,465 iteration 4850 : loss : 0.036226, loss_ce: 0.009350
2022-01-09 04:42:54,301 iteration 4851 : loss : 0.014418, loss_ce: 0.006399
2022-01-09 04:42:57,130 iteration 4852 : loss : 0.026657, loss_ce: 0.012343
2022-01-09 04:42:59,727 iteration 4853 : loss : 0.018045, loss_ce: 0.006237
2022-01-09 04:43:02,480 iteration 4854 : loss : 0.015300, loss_ce: 0.006146
2022-01-09 04:43:05,272 iteration 4855 : loss : 0.015535, loss_ce: 0.005657
2022-01-09 04:43:08,014 iteration 4856 : loss : 0.021097, loss_ce: 0.007860
2022-01-09 04:43:10,860 iteration 4857 : loss : 0.017196, loss_ce: 0.008454
2022-01-09 04:43:13,669 iteration 4858 : loss : 0.017452, loss_ce: 0.007482
2022-01-09 04:43:16,244 iteration 4859 : loss : 0.014972, loss_ce: 0.005423
2022-01-09 04:43:19,006 iteration 4860 : loss : 0.016473, loss_ce: 0.004502
2022-01-09 04:43:21,642 iteration 4861 : loss : 0.023863, loss_ce: 0.006038
2022-01-09 04:43:24,500 iteration 4862 : loss : 0.019625, loss_ce: 0.006828
 72%|███████████████████▎       | 286/400 [4:05:17<1:39:46, 52.51s/it]2022-01-09 04:43:27,391 iteration 4863 : loss : 0.016470, loss_ce: 0.005233
2022-01-09 04:43:30,232 iteration 4864 : loss : 0.023328, loss_ce: 0.008977
2022-01-09 04:43:33,038 iteration 4865 : loss : 0.019185, loss_ce: 0.007412
2022-01-09 04:43:35,693 iteration 4866 : loss : 0.022642, loss_ce: 0.009798
2022-01-09 04:43:38,518 iteration 4867 : loss : 0.017857, loss_ce: 0.006422
2022-01-09 04:43:41,334 iteration 4868 : loss : 0.017191, loss_ce: 0.008431
2022-01-09 04:43:44,017 iteration 4869 : loss : 0.018339, loss_ce: 0.007339
2022-01-09 04:43:46,754 iteration 4870 : loss : 0.015084, loss_ce: 0.005958
2022-01-09 04:43:49,380 iteration 4871 : loss : 0.018739, loss_ce: 0.006133
2022-01-09 04:43:52,178 iteration 4872 : loss : 0.015127, loss_ce: 0.006285
2022-01-09 04:43:54,899 iteration 4873 : loss : 0.020956, loss_ce: 0.007065
2022-01-09 04:43:57,800 iteration 4874 : loss : 0.017229, loss_ce: 0.005785
2022-01-09 04:44:00,541 iteration 4875 : loss : 0.016659, loss_ce: 0.004638
2022-01-09 04:44:03,287 iteration 4876 : loss : 0.014661, loss_ce: 0.006161
2022-01-09 04:44:06,074 iteration 4877 : loss : 0.012266, loss_ce: 0.004280
2022-01-09 04:44:08,719 iteration 4878 : loss : 0.022883, loss_ce: 0.008812
2022-01-09 04:44:11,615 iteration 4879 : loss : 0.019144, loss_ce: 0.007341
 72%|███████████████████▎       | 287/400 [4:06:04<1:35:50, 50.89s/it]2022-01-09 04:44:14,354 iteration 4880 : loss : 0.016672, loss_ce: 0.005153
2022-01-09 04:44:17,131 iteration 4881 : loss : 0.013854, loss_ce: 0.004082
2022-01-09 04:44:19,945 iteration 4882 : loss : 0.014502, loss_ce: 0.003436
2022-01-09 04:44:22,745 iteration 4883 : loss : 0.023728, loss_ce: 0.010098
2022-01-09 04:44:25,591 iteration 4884 : loss : 0.016501, loss_ce: 0.006957
2022-01-09 04:44:28,308 iteration 4885 : loss : 0.016911, loss_ce: 0.005021
2022-01-09 04:44:31,125 iteration 4886 : loss : 0.017369, loss_ce: 0.005695
2022-01-09 04:44:33,916 iteration 4887 : loss : 0.016944, loss_ce: 0.007037
2022-01-09 04:44:36,616 iteration 4888 : loss : 0.017516, loss_ce: 0.004498
2022-01-09 04:44:39,344 iteration 4889 : loss : 0.016295, loss_ce: 0.006591
2022-01-09 04:44:42,044 iteration 4890 : loss : 0.016716, loss_ce: 0.006142
2022-01-09 04:44:44,843 iteration 4891 : loss : 0.019639, loss_ce: 0.009103
2022-01-09 04:44:47,685 iteration 4892 : loss : 0.020382, loss_ce: 0.010038
2022-01-09 04:44:50,678 iteration 4893 : loss : 0.017418, loss_ce: 0.006342
2022-01-09 04:44:53,320 iteration 4894 : loss : 0.016618, loss_ce: 0.006543
2022-01-09 04:44:56,088 iteration 4895 : loss : 0.021028, loss_ce: 0.008945
2022-01-09 04:44:58,842 iteration 4896 : loss : 0.017739, loss_ce: 0.006441
 72%|███████████████████▍       | 288/400 [4:06:52<1:32:56, 49.79s/it]2022-01-09 04:45:01,592 iteration 4897 : loss : 0.012517, loss_ce: 0.006115
2022-01-09 04:45:04,362 iteration 4898 : loss : 0.012908, loss_ce: 0.004452
2022-01-09 04:45:07,141 iteration 4899 : loss : 0.014484, loss_ce: 0.006357
2022-01-09 04:45:10,169 iteration 4900 : loss : 0.038726, loss_ce: 0.016042
2022-01-09 04:45:12,887 iteration 4901 : loss : 0.023009, loss_ce: 0.009885
2022-01-09 04:45:15,650 iteration 4902 : loss : 0.014685, loss_ce: 0.003141
2022-01-09 04:45:18,458 iteration 4903 : loss : 0.014442, loss_ce: 0.006062
2022-01-09 04:45:21,260 iteration 4904 : loss : 0.022368, loss_ce: 0.010071
2022-01-09 04:45:23,911 iteration 4905 : loss : 0.012882, loss_ce: 0.005304
2022-01-09 04:45:26,616 iteration 4906 : loss : 0.015100, loss_ce: 0.004781
2022-01-09 04:45:29,375 iteration 4907 : loss : 0.018068, loss_ce: 0.006171
2022-01-09 04:45:32,149 iteration 4908 : loss : 0.017315, loss_ce: 0.004387
2022-01-09 04:45:34,818 iteration 4909 : loss : 0.026465, loss_ce: 0.007399
2022-01-09 04:45:37,632 iteration 4910 : loss : 0.015541, loss_ce: 0.003681
2022-01-09 04:45:40,470 iteration 4911 : loss : 0.021749, loss_ce: 0.009010
2022-01-09 04:45:43,378 iteration 4912 : loss : 0.021554, loss_ce: 0.007438
2022-01-09 04:45:45,981 iteration 4913 : loss : 0.014922, loss_ce: 0.004708
 72%|███████████████████▌       | 289/400 [4:07:39<1:30:38, 48.99s/it]2022-01-09 04:45:48,786 iteration 4914 : loss : 0.013647, loss_ce: 0.004101
2022-01-09 04:45:51,618 iteration 4915 : loss : 0.016327, loss_ce: 0.006572
2022-01-09 04:45:54,446 iteration 4916 : loss : 0.017622, loss_ce: 0.005519
2022-01-09 04:45:57,292 iteration 4917 : loss : 0.016217, loss_ce: 0.006052
2022-01-09 04:45:59,879 iteration 4918 : loss : 0.014142, loss_ce: 0.006153
2022-01-09 04:46:02,637 iteration 4919 : loss : 0.015750, loss_ce: 0.005726
2022-01-09 04:46:05,472 iteration 4920 : loss : 0.015232, loss_ce: 0.007167
2022-01-09 04:46:08,078 iteration 4921 : loss : 0.015335, loss_ce: 0.003190
2022-01-09 04:46:10,838 iteration 4922 : loss : 0.024766, loss_ce: 0.004541
2022-01-09 04:46:13,562 iteration 4923 : loss : 0.019058, loss_ce: 0.007474
2022-01-09 04:46:16,543 iteration 4924 : loss : 0.027303, loss_ce: 0.011112
2022-01-09 04:46:19,470 iteration 4925 : loss : 0.019351, loss_ce: 0.007995
2022-01-09 04:46:22,271 iteration 4926 : loss : 0.016249, loss_ce: 0.005975
2022-01-09 04:46:25,136 iteration 4927 : loss : 0.022134, loss_ce: 0.008109
2022-01-09 04:46:27,996 iteration 4928 : loss : 0.022076, loss_ce: 0.009928
2022-01-09 04:46:30,794 iteration 4929 : loss : 0.044510, loss_ce: 0.014829
2022-01-09 04:46:30,794 Training Data Eval:
2022-01-09 04:46:45,991   Average segmentation loss on training set: 0.0103
2022-01-09 04:46:45,992 Validation Data Eval:
2022-01-09 04:46:51,385   Average segmentation loss on validation set: 0.0645
2022-01-09 04:46:54,249 iteration 4930 : loss : 0.011782, loss_ce: 0.004571
 72%|███████████████████▌       | 290/400 [4:08:47<1:40:25, 54.78s/it]2022-01-09 04:46:57,144 iteration 4931 : loss : 0.016321, loss_ce: 0.006707
2022-01-09 04:46:59,962 iteration 4932 : loss : 0.013879, loss_ce: 0.005334
2022-01-09 04:47:02,549 iteration 4933 : loss : 0.015861, loss_ce: 0.005448
2022-01-09 04:47:05,355 iteration 4934 : loss : 0.016230, loss_ce: 0.007012
2022-01-09 04:47:08,181 iteration 4935 : loss : 0.022692, loss_ce: 0.007242
2022-01-09 04:47:10,997 iteration 4936 : loss : 0.015895, loss_ce: 0.006411
2022-01-09 04:47:13,813 iteration 4937 : loss : 0.016580, loss_ce: 0.005502
2022-01-09 04:47:16,669 iteration 4938 : loss : 0.021056, loss_ce: 0.007146
2022-01-09 04:47:19,593 iteration 4939 : loss : 0.025056, loss_ce: 0.009115
2022-01-09 04:47:22,341 iteration 4940 : loss : 0.026373, loss_ce: 0.009464
2022-01-09 04:47:25,215 iteration 4941 : loss : 0.014828, loss_ce: 0.005635
2022-01-09 04:47:28,082 iteration 4942 : loss : 0.019147, loss_ce: 0.006843
2022-01-09 04:47:30,887 iteration 4943 : loss : 0.011134, loss_ce: 0.003502
2022-01-09 04:47:33,664 iteration 4944 : loss : 0.037243, loss_ce: 0.015814
2022-01-09 04:47:36,536 iteration 4945 : loss : 0.015961, loss_ce: 0.004767
2022-01-09 04:47:39,389 iteration 4946 : loss : 0.023298, loss_ce: 0.009632
2022-01-09 04:47:42,339 iteration 4947 : loss : 0.032019, loss_ce: 0.011667
 73%|███████████████████▋       | 291/400 [4:09:35<1:35:52, 52.77s/it]2022-01-09 04:47:45,263 iteration 4948 : loss : 0.034415, loss_ce: 0.013268
2022-01-09 04:47:47,928 iteration 4949 : loss : 0.017135, loss_ce: 0.007381
2022-01-09 04:47:50,522 iteration 4950 : loss : 0.015951, loss_ce: 0.005531
2022-01-09 04:47:53,136 iteration 4951 : loss : 0.023728, loss_ce: 0.008219
2022-01-09 04:47:55,919 iteration 4952 : loss : 0.018924, loss_ce: 0.007416
2022-01-09 04:47:58,634 iteration 4953 : loss : 0.014145, loss_ce: 0.005610
2022-01-09 04:48:01,304 iteration 4954 : loss : 0.021114, loss_ce: 0.008578
2022-01-09 04:48:04,087 iteration 4955 : loss : 0.014998, loss_ce: 0.006128
2022-01-09 04:48:06,913 iteration 4956 : loss : 0.019012, loss_ce: 0.006606
2022-01-09 04:48:09,562 iteration 4957 : loss : 0.018729, loss_ce: 0.008202
2022-01-09 04:48:12,437 iteration 4958 : loss : 0.017790, loss_ce: 0.007684
2022-01-09 04:48:15,194 iteration 4959 : loss : 0.015839, loss_ce: 0.005084
2022-01-09 04:48:17,980 iteration 4960 : loss : 0.019527, loss_ce: 0.006256
2022-01-09 04:48:20,758 iteration 4961 : loss : 0.019371, loss_ce: 0.007999
2022-01-09 04:48:23,586 iteration 4962 : loss : 0.014740, loss_ce: 0.006579
2022-01-09 04:48:26,242 iteration 4963 : loss : 0.019360, loss_ce: 0.007700
2022-01-09 04:48:29,124 iteration 4964 : loss : 0.020167, loss_ce: 0.006121
 73%|███████████████████▋       | 292/400 [4:10:22<1:31:45, 50.97s/it]2022-01-09 04:48:32,039 iteration 4965 : loss : 0.021287, loss_ce: 0.006554
2022-01-09 04:48:34,977 iteration 4966 : loss : 0.039884, loss_ce: 0.010773
2022-01-09 04:48:37,574 iteration 4967 : loss : 0.011593, loss_ce: 0.004730
2022-01-09 04:48:40,364 iteration 4968 : loss : 0.016036, loss_ce: 0.006443
2022-01-09 04:48:43,097 iteration 4969 : loss : 0.016071, loss_ce: 0.003339
2022-01-09 04:48:45,841 iteration 4970 : loss : 0.011711, loss_ce: 0.004919
2022-01-09 04:48:48,725 iteration 4971 : loss : 0.027921, loss_ce: 0.010220
2022-01-09 04:48:51,593 iteration 4972 : loss : 0.019548, loss_ce: 0.005915
2022-01-09 04:48:54,427 iteration 4973 : loss : 0.015698, loss_ce: 0.006026
2022-01-09 04:48:57,238 iteration 4974 : loss : 0.016548, loss_ce: 0.005879
2022-01-09 04:49:00,009 iteration 4975 : loss : 0.024564, loss_ce: 0.008362
2022-01-09 04:49:02,830 iteration 4976 : loss : 0.022387, loss_ce: 0.008601
2022-01-09 04:49:05,709 iteration 4977 : loss : 0.018429, loss_ce: 0.009608
2022-01-09 04:49:08,558 iteration 4978 : loss : 0.030842, loss_ce: 0.008070
2022-01-09 04:49:11,350 iteration 4979 : loss : 0.023437, loss_ce: 0.015068
2022-01-09 04:49:13,984 iteration 4980 : loss : 0.012606, loss_ce: 0.004429
2022-01-09 04:49:16,826 iteration 4981 : loss : 0.022098, loss_ce: 0.007143
 73%|███████████████████▊       | 293/400 [4:11:09<1:29:08, 49.99s/it]2022-01-09 04:49:19,647 iteration 4982 : loss : 0.016720, loss_ce: 0.007143
2022-01-09 04:49:22,400 iteration 4983 : loss : 0.016968, loss_ce: 0.006741
2022-01-09 04:49:25,264 iteration 4984 : loss : 0.025222, loss_ce: 0.010825
2022-01-09 04:49:28,083 iteration 4985 : loss : 0.016282, loss_ce: 0.005234
2022-01-09 04:49:30,979 iteration 4986 : loss : 0.032736, loss_ce: 0.015736
2022-01-09 04:49:33,762 iteration 4987 : loss : 0.015096, loss_ce: 0.006838
2022-01-09 04:49:36,383 iteration 4988 : loss : 0.015917, loss_ce: 0.006916
2022-01-09 04:49:39,209 iteration 4989 : loss : 0.017969, loss_ce: 0.006163
2022-01-09 04:49:42,079 iteration 4990 : loss : 0.017497, loss_ce: 0.005904
2022-01-09 04:49:45,081 iteration 4991 : loss : 0.023247, loss_ce: 0.005370
2022-01-09 04:49:47,776 iteration 4992 : loss : 0.016694, loss_ce: 0.007323
2022-01-09 04:49:50,619 iteration 4993 : loss : 0.020514, loss_ce: 0.009042
2022-01-09 04:49:53,490 iteration 4994 : loss : 0.026025, loss_ce: 0.006322
2022-01-09 04:49:56,299 iteration 4995 : loss : 0.023031, loss_ce: 0.010052
2022-01-09 04:49:59,110 iteration 4996 : loss : 0.019784, loss_ce: 0.007141
2022-01-09 04:50:01,874 iteration 4997 : loss : 0.022046, loss_ce: 0.009203
2022-01-09 04:50:04,617 iteration 4998 : loss : 0.018148, loss_ce: 0.007140
 74%|███████████████████▊       | 294/400 [4:11:57<1:27:09, 49.33s/it]2022-01-09 04:50:07,546 iteration 4999 : loss : 0.017508, loss_ce: 0.007222
2022-01-09 04:50:10,270 iteration 5000 : loss : 0.018322, loss_ce: 0.006692
2022-01-09 04:50:12,966 iteration 5001 : loss : 0.019823, loss_ce: 0.006972
2022-01-09 04:50:15,894 iteration 5002 : loss : 0.017255, loss_ce: 0.007053
2022-01-09 04:50:18,547 iteration 5003 : loss : 0.027337, loss_ce: 0.008523
2022-01-09 04:50:21,398 iteration 5004 : loss : 0.028645, loss_ce: 0.009699
2022-01-09 04:50:24,175 iteration 5005 : loss : 0.021573, loss_ce: 0.008109
2022-01-09 04:50:26,977 iteration 5006 : loss : 0.015782, loss_ce: 0.005235
2022-01-09 04:50:29,758 iteration 5007 : loss : 0.018548, loss_ce: 0.007111
2022-01-09 04:50:32,514 iteration 5008 : loss : 0.019729, loss_ce: 0.006488
2022-01-09 04:50:35,386 iteration 5009 : loss : 0.027609, loss_ce: 0.007998
2022-01-09 04:50:38,272 iteration 5010 : loss : 0.029648, loss_ce: 0.010665
2022-01-09 04:50:41,138 iteration 5011 : loss : 0.027329, loss_ce: 0.016112
2022-01-09 04:50:44,004 iteration 5012 : loss : 0.023227, loss_ce: 0.008492
2022-01-09 04:50:46,999 iteration 5013 : loss : 0.016995, loss_ce: 0.006894
2022-01-09 04:50:49,540 iteration 5014 : loss : 0.017813, loss_ce: 0.006712
2022-01-09 04:50:49,540 Training Data Eval:
2022-01-09 04:51:04,248   Average segmentation loss on training set: 0.0113
2022-01-09 04:51:04,248 Validation Data Eval:
2022-01-09 04:51:09,628   Average segmentation loss on validation set: 0.0683
2022-01-09 04:51:12,402 iteration 5015 : loss : 0.014147, loss_ce: 0.002878
 74%|███████████████████▉       | 295/400 [4:13:05<1:36:00, 54.87s/it]2022-01-09 04:51:15,198 iteration 5016 : loss : 0.018812, loss_ce: 0.007442
2022-01-09 04:51:17,883 iteration 5017 : loss : 0.018156, loss_ce: 0.005053
2022-01-09 04:51:20,653 iteration 5018 : loss : 0.020102, loss_ce: 0.006267
2022-01-09 04:51:23,390 iteration 5019 : loss : 0.013272, loss_ce: 0.004208
2022-01-09 04:51:26,183 iteration 5020 : loss : 0.012946, loss_ce: 0.005037
2022-01-09 04:51:29,136 iteration 5021 : loss : 0.017280, loss_ce: 0.006440
2022-01-09 04:51:32,076 iteration 5022 : loss : 0.023828, loss_ce: 0.010095
2022-01-09 04:51:34,883 iteration 5023 : loss : 0.015102, loss_ce: 0.005904
2022-01-09 04:51:37,698 iteration 5024 : loss : 0.011018, loss_ce: 0.003920
2022-01-09 04:51:40,531 iteration 5025 : loss : 0.012446, loss_ce: 0.004624
2022-01-09 04:51:43,448 iteration 5026 : loss : 0.022242, loss_ce: 0.009818
2022-01-09 04:51:46,270 iteration 5027 : loss : 0.014438, loss_ce: 0.005145
2022-01-09 04:51:48,875 iteration 5028 : loss : 0.019764, loss_ce: 0.007155
2022-01-09 04:51:51,669 iteration 5029 : loss : 0.020244, loss_ce: 0.009669
2022-01-09 04:51:54,522 iteration 5030 : loss : 0.018293, loss_ce: 0.005707
2022-01-09 04:51:57,395 iteration 5031 : loss : 0.026685, loss_ce: 0.010685
2022-01-09 04:52:00,318 iteration 5032 : loss : 0.022088, loss_ce: 0.007767
 74%|███████████████████▉       | 296/400 [4:13:53<1:31:29, 52.79s/it]2022-01-09 04:52:03,224 iteration 5033 : loss : 0.025301, loss_ce: 0.012540
2022-01-09 04:52:05,862 iteration 5034 : loss : 0.021070, loss_ce: 0.005490
2022-01-09 04:52:08,647 iteration 5035 : loss : 0.017409, loss_ce: 0.006580
2022-01-09 04:52:11,242 iteration 5036 : loss : 0.014026, loss_ce: 0.005045
2022-01-09 04:52:14,106 iteration 5037 : loss : 0.021753, loss_ce: 0.006567
2022-01-09 04:52:17,022 iteration 5038 : loss : 0.020386, loss_ce: 0.008883
2022-01-09 04:52:19,846 iteration 5039 : loss : 0.015345, loss_ce: 0.005367
2022-01-09 04:52:22,667 iteration 5040 : loss : 0.021063, loss_ce: 0.008832
2022-01-09 04:52:25,494 iteration 5041 : loss : 0.016679, loss_ce: 0.005950
2022-01-09 04:52:28,282 iteration 5042 : loss : 0.013456, loss_ce: 0.005206
2022-01-09 04:52:31,106 iteration 5043 : loss : 0.016957, loss_ce: 0.005395
2022-01-09 04:52:33,973 iteration 5044 : loss : 0.014263, loss_ce: 0.004784
2022-01-09 04:52:36,573 iteration 5045 : loss : 0.012626, loss_ce: 0.003854
2022-01-09 04:52:39,564 iteration 5046 : loss : 0.019532, loss_ce: 0.005708
2022-01-09 04:52:42,358 iteration 5047 : loss : 0.018458, loss_ce: 0.007172
2022-01-09 04:52:44,968 iteration 5048 : loss : 0.016907, loss_ce: 0.005938
2022-01-09 04:52:47,811 iteration 5049 : loss : 0.024244, loss_ce: 0.010633
 74%|████████████████████       | 297/400 [4:14:40<1:27:53, 51.19s/it]2022-01-09 04:52:50,656 iteration 5050 : loss : 0.018332, loss_ce: 0.004894
2022-01-09 04:52:53,324 iteration 5051 : loss : 0.014624, loss_ce: 0.006257
2022-01-09 04:52:56,155 iteration 5052 : loss : 0.017216, loss_ce: 0.008247
2022-01-09 04:52:58,732 iteration 5053 : loss : 0.015449, loss_ce: 0.005331
2022-01-09 04:53:01,377 iteration 5054 : loss : 0.014833, loss_ce: 0.007016
2022-01-09 04:53:04,132 iteration 5055 : loss : 0.017464, loss_ce: 0.007323
2022-01-09 04:53:06,955 iteration 5056 : loss : 0.015253, loss_ce: 0.004452
2022-01-09 04:53:09,655 iteration 5057 : loss : 0.014535, loss_ce: 0.006442
2022-01-09 04:53:12,486 iteration 5058 : loss : 0.016550, loss_ce: 0.005040
2022-01-09 04:53:15,337 iteration 5059 : loss : 0.017134, loss_ce: 0.007534
2022-01-09 04:53:18,168 iteration 5060 : loss : 0.034424, loss_ce: 0.008193
2022-01-09 04:53:20,973 iteration 5061 : loss : 0.019448, loss_ce: 0.010341
2022-01-09 04:53:23,892 iteration 5062 : loss : 0.017749, loss_ce: 0.006347
2022-01-09 04:53:26,546 iteration 5063 : loss : 0.013302, loss_ce: 0.005180
2022-01-09 04:53:29,498 iteration 5064 : loss : 0.020298, loss_ce: 0.005406
2022-01-09 04:53:32,321 iteration 5065 : loss : 0.024974, loss_ce: 0.007895
2022-01-09 04:53:34,954 iteration 5066 : loss : 0.013821, loss_ce: 0.006066
 74%|████████████████████       | 298/400 [4:15:28<1:24:57, 49.98s/it]2022-01-09 04:53:37,781 iteration 5067 : loss : 0.022756, loss_ce: 0.007383
2022-01-09 04:53:40,634 iteration 5068 : loss : 0.018238, loss_ce: 0.006209
2022-01-09 04:53:43,424 iteration 5069 : loss : 0.017285, loss_ce: 0.005898
2022-01-09 04:53:46,107 iteration 5070 : loss : 0.018091, loss_ce: 0.009093
2022-01-09 04:53:48,960 iteration 5071 : loss : 0.018203, loss_ce: 0.008338
2022-01-09 04:53:51,802 iteration 5072 : loss : 0.015627, loss_ce: 0.004838
2022-01-09 04:53:54,663 iteration 5073 : loss : 0.025470, loss_ce: 0.007413
2022-01-09 04:53:57,492 iteration 5074 : loss : 0.022674, loss_ce: 0.009064
2022-01-09 04:54:00,098 iteration 5075 : loss : 0.018745, loss_ce: 0.008990
2022-01-09 04:54:02,822 iteration 5076 : loss : 0.014305, loss_ce: 0.005446
2022-01-09 04:54:05,729 iteration 5077 : loss : 0.031721, loss_ce: 0.009764
2022-01-09 04:54:08,368 iteration 5078 : loss : 0.020545, loss_ce: 0.006752
2022-01-09 04:54:11,138 iteration 5079 : loss : 0.012139, loss_ce: 0.004167
2022-01-09 04:54:13,954 iteration 5080 : loss : 0.017146, loss_ce: 0.006652
2022-01-09 04:54:16,655 iteration 5081 : loss : 0.029087, loss_ce: 0.015871
2022-01-09 04:54:19,447 iteration 5082 : loss : 0.020005, loss_ce: 0.005544
2022-01-09 04:54:22,057 iteration 5083 : loss : 0.021170, loss_ce: 0.008329
 75%|████████████████████▏      | 299/400 [4:16:15<1:22:40, 49.12s/it]2022-01-09 04:54:25,108 iteration 5084 : loss : 0.022652, loss_ce: 0.012291
2022-01-09 04:54:27,938 iteration 5085 : loss : 0.020550, loss_ce: 0.007369
2022-01-09 04:54:30,694 iteration 5086 : loss : 0.012882, loss_ce: 0.004089
2022-01-09 04:54:33,393 iteration 5087 : loss : 0.019730, loss_ce: 0.006068
2022-01-09 04:54:36,062 iteration 5088 : loss : 0.011384, loss_ce: 0.005334
2022-01-09 04:54:38,710 iteration 5089 : loss : 0.016265, loss_ce: 0.006300
2022-01-09 04:54:41,564 iteration 5090 : loss : 0.024296, loss_ce: 0.010305
2022-01-09 04:54:44,285 iteration 5091 : loss : 0.020503, loss_ce: 0.007904
2022-01-09 04:54:47,117 iteration 5092 : loss : 0.028997, loss_ce: 0.007364
2022-01-09 04:54:49,968 iteration 5093 : loss : 0.016714, loss_ce: 0.005720
2022-01-09 04:54:52,966 iteration 5094 : loss : 0.017727, loss_ce: 0.006358
2022-01-09 04:54:55,650 iteration 5095 : loss : 0.017066, loss_ce: 0.006977
2022-01-09 04:54:58,574 iteration 5096 : loss : 0.018104, loss_ce: 0.004673
2022-01-09 04:55:01,429 iteration 5097 : loss : 0.022593, loss_ce: 0.011081
2022-01-09 04:55:04,298 iteration 5098 : loss : 0.016221, loss_ce: 0.006454
2022-01-09 04:55:07,220 iteration 5099 : loss : 0.016723, loss_ce: 0.007072
2022-01-09 04:55:07,220 Training Data Eval:
2022-01-09 04:55:22,690   Average segmentation loss on training set: 0.0105
2022-01-09 04:55:22,690 Validation Data Eval:
2022-01-09 04:55:27,883   Average segmentation loss on validation set: 0.0815
2022-01-09 04:55:30,663 iteration 5100 : loss : 0.019665, loss_ce: 0.006527
 75%|████████████████████▎      | 300/400 [4:17:23<1:31:36, 54.96s/it]2022-01-09 04:55:33,530 iteration 5101 : loss : 0.017894, loss_ce: 0.005159
2022-01-09 04:55:36,395 iteration 5102 : loss : 0.016688, loss_ce: 0.007824
2022-01-09 04:55:39,163 iteration 5103 : loss : 0.017259, loss_ce: 0.006226
2022-01-09 04:55:41,753 iteration 5104 : loss : 0.018877, loss_ce: 0.005845
2022-01-09 04:55:44,612 iteration 5105 : loss : 0.020274, loss_ce: 0.006308
2022-01-09 04:55:47,249 iteration 5106 : loss : 0.022118, loss_ce: 0.008467
2022-01-09 04:55:50,111 iteration 5107 : loss : 0.019376, loss_ce: 0.007503
2022-01-09 04:55:52,987 iteration 5108 : loss : 0.016029, loss_ce: 0.006522
2022-01-09 04:55:55,779 iteration 5109 : loss : 0.014783, loss_ce: 0.006141
2022-01-09 04:55:58,472 iteration 5110 : loss : 0.013191, loss_ce: 0.004923
2022-01-09 04:56:01,307 iteration 5111 : loss : 0.017148, loss_ce: 0.005377
2022-01-09 04:56:04,123 iteration 5112 : loss : 0.015102, loss_ce: 0.007417
2022-01-09 04:56:06,979 iteration 5113 : loss : 0.018042, loss_ce: 0.006470
2022-01-09 04:56:09,563 iteration 5114 : loss : 0.017635, loss_ce: 0.007263
2022-01-09 04:56:12,277 iteration 5115 : loss : 0.015273, loss_ce: 0.007429
2022-01-09 04:56:15,103 iteration 5116 : loss : 0.022811, loss_ce: 0.009772
2022-01-09 04:56:17,894 iteration 5117 : loss : 0.016197, loss_ce: 0.005596
 75%|████████████████████▎      | 301/400 [4:18:11<1:26:51, 52.64s/it]2022-01-09 04:56:20,737 iteration 5118 : loss : 0.019194, loss_ce: 0.009610
2022-01-09 04:56:23,497 iteration 5119 : loss : 0.021965, loss_ce: 0.007156
2022-01-09 04:56:26,362 iteration 5120 : loss : 0.012536, loss_ce: 0.003303
2022-01-09 04:56:29,161 iteration 5121 : loss : 0.015353, loss_ce: 0.005633
2022-01-09 04:56:31,972 iteration 5122 : loss : 0.029238, loss_ce: 0.007752
2022-01-09 04:56:34,667 iteration 5123 : loss : 0.024372, loss_ce: 0.012029
2022-01-09 04:56:37,503 iteration 5124 : loss : 0.014542, loss_ce: 0.003306
2022-01-09 04:56:40,203 iteration 5125 : loss : 0.020996, loss_ce: 0.008548
2022-01-09 04:56:42,809 iteration 5126 : loss : 0.013294, loss_ce: 0.005166
2022-01-09 04:56:45,683 iteration 5127 : loss : 0.024302, loss_ce: 0.010068
2022-01-09 04:56:48,617 iteration 5128 : loss : 0.020438, loss_ce: 0.006742
2022-01-09 04:56:51,491 iteration 5129 : loss : 0.018750, loss_ce: 0.006610
2022-01-09 04:56:54,064 iteration 5130 : loss : 0.014013, loss_ce: 0.005479
2022-01-09 04:56:56,890 iteration 5131 : loss : 0.013889, loss_ce: 0.004045
2022-01-09 04:56:59,633 iteration 5132 : loss : 0.020475, loss_ce: 0.008353
2022-01-09 04:57:02,328 iteration 5133 : loss : 0.014477, loss_ce: 0.006189
2022-01-09 04:57:04,908 iteration 5134 : loss : 0.010540, loss_ce: 0.003521
 76%|████████████████████▍      | 302/400 [4:18:58<1:23:14, 50.96s/it]2022-01-09 04:57:07,589 iteration 5135 : loss : 0.013264, loss_ce: 0.004197
2022-01-09 04:57:10,478 iteration 5136 : loss : 0.015592, loss_ce: 0.005550
2022-01-09 04:57:13,346 iteration 5137 : loss : 0.016564, loss_ce: 0.004909
2022-01-09 04:57:16,239 iteration 5138 : loss : 0.020794, loss_ce: 0.009563
2022-01-09 04:57:18,838 iteration 5139 : loss : 0.011141, loss_ce: 0.003194
2022-01-09 04:57:21,749 iteration 5140 : loss : 0.018676, loss_ce: 0.008877
2022-01-09 04:57:24,522 iteration 5141 : loss : 0.014689, loss_ce: 0.007122
2022-01-09 04:57:27,456 iteration 5142 : loss : 0.026035, loss_ce: 0.008875
2022-01-09 04:57:30,287 iteration 5143 : loss : 0.012591, loss_ce: 0.005094
2022-01-09 04:57:32,905 iteration 5144 : loss : 0.012465, loss_ce: 0.005281
2022-01-09 04:57:35,793 iteration 5145 : loss : 0.019883, loss_ce: 0.007158
2022-01-09 04:57:38,668 iteration 5146 : loss : 0.030568, loss_ce: 0.007771
2022-01-09 04:57:41,406 iteration 5147 : loss : 0.017269, loss_ce: 0.008627
2022-01-09 04:57:44,244 iteration 5148 : loss : 0.020964, loss_ce: 0.008995
2022-01-09 04:57:47,000 iteration 5149 : loss : 0.016407, loss_ce: 0.006387
2022-01-09 04:57:49,813 iteration 5150 : loss : 0.013269, loss_ce: 0.005067
2022-01-09 04:57:52,643 iteration 5151 : loss : 0.013262, loss_ce: 0.004236
 76%|████████████████████▍      | 303/400 [4:19:45<1:20:48, 49.99s/it]2022-01-09 04:57:55,423 iteration 5152 : loss : 0.016845, loss_ce: 0.004405
2022-01-09 04:57:58,071 iteration 5153 : loss : 0.012180, loss_ce: 0.005759
2022-01-09 04:58:00,921 iteration 5154 : loss : 0.021356, loss_ce: 0.007644
2022-01-09 04:58:03,499 iteration 5155 : loss : 0.010529, loss_ce: 0.003460
2022-01-09 04:58:06,108 iteration 5156 : loss : 0.019397, loss_ce: 0.007422
2022-01-09 04:58:09,027 iteration 5157 : loss : 0.015177, loss_ce: 0.005131
2022-01-09 04:58:11,847 iteration 5158 : loss : 0.014575, loss_ce: 0.005484
2022-01-09 04:58:14,492 iteration 5159 : loss : 0.023612, loss_ce: 0.007348
2022-01-09 04:58:17,348 iteration 5160 : loss : 0.017514, loss_ce: 0.007562
2022-01-09 04:58:20,243 iteration 5161 : loss : 0.024163, loss_ce: 0.008974
2022-01-09 04:58:22,869 iteration 5162 : loss : 0.012817, loss_ce: 0.003632
2022-01-09 04:58:25,657 iteration 5163 : loss : 0.016296, loss_ce: 0.005252
2022-01-09 04:58:28,227 iteration 5164 : loss : 0.014172, loss_ce: 0.007449
2022-01-09 04:58:31,087 iteration 5165 : loss : 0.013216, loss_ce: 0.003973
2022-01-09 04:58:33,748 iteration 5166 : loss : 0.016149, loss_ce: 0.007250
2022-01-09 04:58:36,502 iteration 5167 : loss : 0.019711, loss_ce: 0.008009
2022-01-09 04:58:39,345 iteration 5168 : loss : 0.023557, loss_ce: 0.008906
 76%|████████████████████▌      | 304/400 [4:20:32<1:18:24, 49.00s/it]2022-01-09 04:58:42,134 iteration 5169 : loss : 0.016777, loss_ce: 0.006095
2022-01-09 04:58:45,082 iteration 5170 : loss : 0.013753, loss_ce: 0.005707
2022-01-09 04:58:47,835 iteration 5171 : loss : 0.018670, loss_ce: 0.004822
2022-01-09 04:58:50,830 iteration 5172 : loss : 0.019510, loss_ce: 0.005998
2022-01-09 04:58:53,641 iteration 5173 : loss : 0.014908, loss_ce: 0.006872
2022-01-09 04:58:56,464 iteration 5174 : loss : 0.017312, loss_ce: 0.006909
2022-01-09 04:58:59,317 iteration 5175 : loss : 0.025000, loss_ce: 0.008274
2022-01-09 04:59:02,103 iteration 5176 : loss : 0.018266, loss_ce: 0.005995
2022-01-09 04:59:05,022 iteration 5177 : loss : 0.027428, loss_ce: 0.012892
2022-01-09 04:59:07,844 iteration 5178 : loss : 0.016212, loss_ce: 0.003202
2022-01-09 04:59:10,702 iteration 5179 : loss : 0.013717, loss_ce: 0.005725
2022-01-09 04:59:13,406 iteration 5180 : loss : 0.015081, loss_ce: 0.005872
2022-01-09 04:59:16,256 iteration 5181 : loss : 0.019705, loss_ce: 0.009055
2022-01-09 04:59:19,105 iteration 5182 : loss : 0.025679, loss_ce: 0.010757
2022-01-09 04:59:21,967 iteration 5183 : loss : 0.019783, loss_ce: 0.005424
2022-01-09 04:59:24,808 iteration 5184 : loss : 0.016960, loss_ce: 0.008151
2022-01-09 04:59:24,809 Training Data Eval:
2022-01-09 04:59:39,852   Average segmentation loss on training set: 0.0095
2022-01-09 04:59:39,852 Validation Data Eval:
2022-01-09 04:59:45,189   Average segmentation loss on validation set: 0.0621
2022-01-09 04:59:48,163 iteration 5185 : loss : 0.016448, loss_ce: 0.007422
 76%|████████████████████▌      | 305/400 [4:21:41<1:26:59, 54.95s/it]2022-01-09 04:59:51,031 iteration 5186 : loss : 0.015031, loss_ce: 0.007422
2022-01-09 04:59:53,889 iteration 5187 : loss : 0.014685, loss_ce: 0.005688
2022-01-09 04:59:56,499 iteration 5188 : loss : 0.015831, loss_ce: 0.004150
2022-01-09 04:59:59,309 iteration 5189 : loss : 0.012615, loss_ce: 0.005306
2022-01-09 05:00:02,033 iteration 5190 : loss : 0.013806, loss_ce: 0.005914
2022-01-09 05:00:04,677 iteration 5191 : loss : 0.016578, loss_ce: 0.006039
2022-01-09 05:00:07,516 iteration 5192 : loss : 0.018416, loss_ce: 0.006337
2022-01-09 05:00:10,322 iteration 5193 : loss : 0.024483, loss_ce: 0.012772
2022-01-09 05:00:13,098 iteration 5194 : loss : 0.017464, loss_ce: 0.007541
2022-01-09 05:00:15,860 iteration 5195 : loss : 0.018064, loss_ce: 0.005757
2022-01-09 05:00:18,780 iteration 5196 : loss : 0.042285, loss_ce: 0.011163
2022-01-09 05:00:21,601 iteration 5197 : loss : 0.015546, loss_ce: 0.004308
2022-01-09 05:00:24,418 iteration 5198 : loss : 0.022710, loss_ce: 0.008518
2022-01-09 05:00:27,284 iteration 5199 : loss : 0.016502, loss_ce: 0.006343
2022-01-09 05:00:30,145 iteration 5200 : loss : 0.013776, loss_ce: 0.004493
2022-01-09 05:00:32,750 iteration 5201 : loss : 0.016623, loss_ce: 0.005206
2022-01-09 05:00:35,536 iteration 5202 : loss : 0.019213, loss_ce: 0.007970
 76%|████████████████████▋      | 306/400 [4:22:28<1:22:31, 52.68s/it]2022-01-09 05:00:38,338 iteration 5203 : loss : 0.016264, loss_ce: 0.004657
2022-01-09 05:00:41,141 iteration 5204 : loss : 0.020633, loss_ce: 0.007147
2022-01-09 05:00:43,952 iteration 5205 : loss : 0.017366, loss_ce: 0.006768
2022-01-09 05:00:46,724 iteration 5206 : loss : 0.012135, loss_ce: 0.004837
2022-01-09 05:00:49,421 iteration 5207 : loss : 0.017725, loss_ce: 0.008002
2022-01-09 05:00:52,281 iteration 5208 : loss : 0.018851, loss_ce: 0.007183
2022-01-09 05:00:55,069 iteration 5209 : loss : 0.015413, loss_ce: 0.005692
2022-01-09 05:00:57,855 iteration 5210 : loss : 0.011893, loss_ce: 0.003604
2022-01-09 05:01:00,612 iteration 5211 : loss : 0.013064, loss_ce: 0.003896
2022-01-09 05:01:03,416 iteration 5212 : loss : 0.021228, loss_ce: 0.005612
2022-01-09 05:01:06,255 iteration 5213 : loss : 0.022886, loss_ce: 0.007248
2022-01-09 05:01:09,089 iteration 5214 : loss : 0.018832, loss_ce: 0.011004
2022-01-09 05:01:11,826 iteration 5215 : loss : 0.019326, loss_ce: 0.007589
2022-01-09 05:01:14,631 iteration 5216 : loss : 0.013147, loss_ce: 0.005463
2022-01-09 05:01:17,251 iteration 5217 : loss : 0.018408, loss_ce: 0.006247
2022-01-09 05:01:20,056 iteration 5218 : loss : 0.020689, loss_ce: 0.006265
2022-01-09 05:01:22,899 iteration 5219 : loss : 0.018709, loss_ce: 0.007673
 77%|████████████████████▋      | 307/400 [4:23:16<1:19:10, 51.08s/it]2022-01-09 05:01:25,853 iteration 5220 : loss : 0.021133, loss_ce: 0.007885
2022-01-09 05:01:28,541 iteration 5221 : loss : 0.013157, loss_ce: 0.005130
2022-01-09 05:01:31,165 iteration 5222 : loss : 0.013254, loss_ce: 0.003333
2022-01-09 05:01:33,974 iteration 5223 : loss : 0.017053, loss_ce: 0.006807
2022-01-09 05:01:37,042 iteration 5224 : loss : 0.021963, loss_ce: 0.008103
2022-01-09 05:01:39,796 iteration 5225 : loss : 0.017686, loss_ce: 0.004243
2022-01-09 05:01:42,709 iteration 5226 : loss : 0.033183, loss_ce: 0.014163
2022-01-09 05:01:45,445 iteration 5227 : loss : 0.012483, loss_ce: 0.004413
2022-01-09 05:01:48,205 iteration 5228 : loss : 0.011094, loss_ce: 0.004760
2022-01-09 05:01:51,043 iteration 5229 : loss : 0.026205, loss_ce: 0.010703
2022-01-09 05:01:53,839 iteration 5230 : loss : 0.023440, loss_ce: 0.012393
2022-01-09 05:01:56,860 iteration 5231 : loss : 0.028827, loss_ce: 0.008387
2022-01-09 05:01:59,664 iteration 5232 : loss : 0.015410, loss_ce: 0.006787
2022-01-09 05:02:02,516 iteration 5233 : loss : 0.015969, loss_ce: 0.006656
2022-01-09 05:02:05,090 iteration 5234 : loss : 0.011060, loss_ce: 0.003861
2022-01-09 05:02:08,032 iteration 5235 : loss : 0.019857, loss_ce: 0.007089
2022-01-09 05:02:10,648 iteration 5236 : loss : 0.017082, loss_ce: 0.006488
 77%|████████████████████▊      | 308/400 [4:24:03<1:16:47, 50.08s/it]2022-01-09 05:02:13,570 iteration 5237 : loss : 0.018448, loss_ce: 0.006533
2022-01-09 05:02:16,268 iteration 5238 : loss : 0.019067, loss_ce: 0.008397
2022-01-09 05:02:19,099 iteration 5239 : loss : 0.023626, loss_ce: 0.005989
2022-01-09 05:02:21,985 iteration 5240 : loss : 0.014549, loss_ce: 0.004557
2022-01-09 05:02:24,841 iteration 5241 : loss : 0.016308, loss_ce: 0.007195
2022-01-09 05:02:27,726 iteration 5242 : loss : 0.021066, loss_ce: 0.008520
2022-01-09 05:02:30,567 iteration 5243 : loss : 0.015935, loss_ce: 0.005265
2022-01-09 05:02:33,433 iteration 5244 : loss : 0.018671, loss_ce: 0.006521
2022-01-09 05:02:36,296 iteration 5245 : loss : 0.014587, loss_ce: 0.004843
2022-01-09 05:02:38,896 iteration 5246 : loss : 0.010779, loss_ce: 0.004462
2022-01-09 05:02:41,703 iteration 5247 : loss : 0.013644, loss_ce: 0.004727
2022-01-09 05:02:44,543 iteration 5248 : loss : 0.016003, loss_ce: 0.006012
2022-01-09 05:02:47,313 iteration 5249 : loss : 0.017750, loss_ce: 0.006524
2022-01-09 05:02:49,965 iteration 5250 : loss : 0.019478, loss_ce: 0.008575
2022-01-09 05:02:52,863 iteration 5251 : loss : 0.013701, loss_ce: 0.004345
2022-01-09 05:02:55,660 iteration 5252 : loss : 0.015692, loss_ce: 0.004663
2022-01-09 05:02:58,470 iteration 5253 : loss : 0.015433, loss_ce: 0.005698
 77%|████████████████████▊      | 309/400 [4:24:51<1:14:55, 49.40s/it]2022-01-09 05:03:01,306 iteration 5254 : loss : 0.012554, loss_ce: 0.004505
2022-01-09 05:03:04,151 iteration 5255 : loss : 0.015178, loss_ce: 0.004825
2022-01-09 05:03:07,023 iteration 5256 : loss : 0.014942, loss_ce: 0.006411
2022-01-09 05:03:09,898 iteration 5257 : loss : 0.017116, loss_ce: 0.004309
2022-01-09 05:03:12,519 iteration 5258 : loss : 0.017844, loss_ce: 0.006635
2022-01-09 05:03:15,413 iteration 5259 : loss : 0.022653, loss_ce: 0.006442
2022-01-09 05:03:18,185 iteration 5260 : loss : 0.014388, loss_ce: 0.006046
2022-01-09 05:03:21,184 iteration 5261 : loss : 0.014490, loss_ce: 0.006242
2022-01-09 05:03:23,915 iteration 5262 : loss : 0.010352, loss_ce: 0.004639
2022-01-09 05:03:26,746 iteration 5263 : loss : 0.016422, loss_ce: 0.005856
2022-01-09 05:03:29,573 iteration 5264 : loss : 0.015053, loss_ce: 0.004884
2022-01-09 05:03:32,495 iteration 5265 : loss : 0.013720, loss_ce: 0.004900
2022-01-09 05:03:35,483 iteration 5266 : loss : 0.017173, loss_ce: 0.007059
2022-01-09 05:03:38,417 iteration 5267 : loss : 0.015327, loss_ce: 0.005714
2022-01-09 05:03:41,163 iteration 5268 : loss : 0.018649, loss_ce: 0.007865
2022-01-09 05:03:44,024 iteration 5269 : loss : 0.016870, loss_ce: 0.006872
2022-01-09 05:03:44,025 Training Data Eval:
2022-01-09 05:03:58,822   Average segmentation loss on training set: 0.0095
2022-01-09 05:03:58,822 Validation Data Eval:
2022-01-09 05:04:04,164   Average segmentation loss on validation set: 0.0692
2022-01-09 05:04:06,926 iteration 5270 : loss : 0.019594, loss_ce: 0.007540
 78%|████████████████████▉      | 310/400 [4:26:00<1:22:40, 55.11s/it]2022-01-09 05:04:09,805 iteration 5271 : loss : 0.015077, loss_ce: 0.006460
2022-01-09 05:04:12,603 iteration 5272 : loss : 0.017192, loss_ce: 0.006221
2022-01-09 05:04:15,428 iteration 5273 : loss : 0.014839, loss_ce: 0.006427
2022-01-09 05:04:18,043 iteration 5274 : loss : 0.013590, loss_ce: 0.004366
2022-01-09 05:04:20,924 iteration 5275 : loss : 0.029810, loss_ce: 0.006909
2022-01-09 05:04:23,817 iteration 5276 : loss : 0.027503, loss_ce: 0.008744
2022-01-09 05:04:26,408 iteration 5277 : loss : 0.020600, loss_ce: 0.006762
2022-01-09 05:04:29,370 iteration 5278 : loss : 0.014582, loss_ce: 0.005762
2022-01-09 05:04:32,003 iteration 5279 : loss : 0.016763, loss_ce: 0.004446
2022-01-09 05:04:34,903 iteration 5280 : loss : 0.017218, loss_ce: 0.006255
2022-01-09 05:04:37,542 iteration 5281 : loss : 0.016621, loss_ce: 0.008462
2022-01-09 05:04:40,476 iteration 5282 : loss : 0.016260, loss_ce: 0.006137
2022-01-09 05:04:43,163 iteration 5283 : loss : 0.017067, loss_ce: 0.005441
2022-01-09 05:04:45,999 iteration 5284 : loss : 0.017204, loss_ce: 0.006960
2022-01-09 05:04:48,884 iteration 5285 : loss : 0.020941, loss_ce: 0.009187
2022-01-09 05:04:51,751 iteration 5286 : loss : 0.016210, loss_ce: 0.005748
2022-01-09 05:04:54,445 iteration 5287 : loss : 0.026383, loss_ce: 0.010780
 78%|████████████████████▉      | 311/400 [4:26:47<1:18:22, 52.84s/it]2022-01-09 05:04:57,271 iteration 5288 : loss : 0.016470, loss_ce: 0.005805
2022-01-09 05:05:00,119 iteration 5289 : loss : 0.018891, loss_ce: 0.007079
2022-01-09 05:05:02,973 iteration 5290 : loss : 0.018577, loss_ce: 0.006535
2022-01-09 05:05:05,820 iteration 5291 : loss : 0.019591, loss_ce: 0.007982
2022-01-09 05:05:08,654 iteration 5292 : loss : 0.012226, loss_ce: 0.004285
2022-01-09 05:05:11,304 iteration 5293 : loss : 0.018497, loss_ce: 0.005609
2022-01-09 05:05:14,170 iteration 5294 : loss : 0.026190, loss_ce: 0.010897
2022-01-09 05:05:16,875 iteration 5295 : loss : 0.013065, loss_ce: 0.005078
2022-01-09 05:05:19,752 iteration 5296 : loss : 0.018627, loss_ce: 0.005573
2022-01-09 05:05:22,625 iteration 5297 : loss : 0.018124, loss_ce: 0.006470
2022-01-09 05:05:25,429 iteration 5298 : loss : 0.013058, loss_ce: 0.005524
2022-01-09 05:05:28,331 iteration 5299 : loss : 0.015764, loss_ce: 0.006202
2022-01-09 05:05:30,929 iteration 5300 : loss : 0.015332, loss_ce: 0.005816
2022-01-09 05:05:33,814 iteration 5301 : loss : 0.031895, loss_ce: 0.009675
2022-01-09 05:05:36,433 iteration 5302 : loss : 0.015421, loss_ce: 0.006250
2022-01-09 05:05:39,042 iteration 5303 : loss : 0.012470, loss_ce: 0.004308
2022-01-09 05:05:41,920 iteration 5304 : loss : 0.023185, loss_ce: 0.008824
 78%|█████████████████████      | 312/400 [4:27:35<1:15:08, 51.23s/it]2022-01-09 05:05:44,797 iteration 5305 : loss : 0.026991, loss_ce: 0.013135
2022-01-09 05:05:47,602 iteration 5306 : loss : 0.020885, loss_ce: 0.007589
2022-01-09 05:05:50,467 iteration 5307 : loss : 0.014986, loss_ce: 0.005631
2022-01-09 05:05:53,178 iteration 5308 : loss : 0.018237, loss_ce: 0.007655
2022-01-09 05:05:55,930 iteration 5309 : loss : 0.013865, loss_ce: 0.004677
2022-01-09 05:05:58,744 iteration 5310 : loss : 0.016694, loss_ce: 0.007192
2022-01-09 05:06:01,541 iteration 5311 : loss : 0.021954, loss_ce: 0.007719
2022-01-09 05:06:04,385 iteration 5312 : loss : 0.016059, loss_ce: 0.005026
2022-01-09 05:06:07,241 iteration 5313 : loss : 0.025158, loss_ce: 0.007489
2022-01-09 05:06:10,040 iteration 5314 : loss : 0.013997, loss_ce: 0.005706
2022-01-09 05:06:12,857 iteration 5315 : loss : 0.016104, loss_ce: 0.006008
2022-01-09 05:06:15,641 iteration 5316 : loss : 0.011715, loss_ce: 0.005400
2022-01-09 05:06:18,546 iteration 5317 : loss : 0.016000, loss_ce: 0.005948
2022-01-09 05:06:21,400 iteration 5318 : loss : 0.023112, loss_ce: 0.006768
2022-01-09 05:06:24,248 iteration 5319 : loss : 0.018944, loss_ce: 0.005908
2022-01-09 05:06:27,118 iteration 5320 : loss : 0.017987, loss_ce: 0.008661
2022-01-09 05:06:29,732 iteration 5321 : loss : 0.016581, loss_ce: 0.003307
 78%|█████████████████████▏     | 313/400 [4:28:22<1:12:47, 50.21s/it]2022-01-09 05:06:32,594 iteration 5322 : loss : 0.026576, loss_ce: 0.014098
2022-01-09 05:06:35,445 iteration 5323 : loss : 0.021850, loss_ce: 0.006092
2022-01-09 05:06:38,302 iteration 5324 : loss : 0.012805, loss_ce: 0.005496
2022-01-09 05:06:40,970 iteration 5325 : loss : 0.016487, loss_ce: 0.005595
2022-01-09 05:06:43,910 iteration 5326 : loss : 0.019981, loss_ce: 0.005246
2022-01-09 05:06:46,709 iteration 5327 : loss : 0.017496, loss_ce: 0.007199
2022-01-09 05:06:49,507 iteration 5328 : loss : 0.017178, loss_ce: 0.007449
2022-01-09 05:06:52,311 iteration 5329 : loss : 0.016077, loss_ce: 0.005840
2022-01-09 05:06:54,939 iteration 5330 : loss : 0.023994, loss_ce: 0.008375
2022-01-09 05:06:57,674 iteration 5331 : loss : 0.018694, loss_ce: 0.007271
2022-01-09 05:07:00,545 iteration 5332 : loss : 0.058932, loss_ce: 0.033486
2022-01-09 05:07:03,370 iteration 5333 : loss : 0.017721, loss_ce: 0.006500
2022-01-09 05:07:05,927 iteration 5334 : loss : 0.014787, loss_ce: 0.004237
2022-01-09 05:07:08,722 iteration 5335 : loss : 0.022810, loss_ce: 0.008644
2022-01-09 05:07:11,438 iteration 5336 : loss : 0.016502, loss_ce: 0.006048
2022-01-09 05:07:14,264 iteration 5337 : loss : 0.015052, loss_ce: 0.005777
2022-01-09 05:07:17,080 iteration 5338 : loss : 0.025898, loss_ce: 0.009193
 78%|█████████████████████▏     | 314/400 [4:29:10<1:10:44, 49.35s/it]2022-01-09 05:07:19,857 iteration 5339 : loss : 0.015192, loss_ce: 0.005601
2022-01-09 05:07:22,661 iteration 5340 : loss : 0.014192, loss_ce: 0.005531
2022-01-09 05:07:25,519 iteration 5341 : loss : 0.026405, loss_ce: 0.006196
2022-01-09 05:07:28,374 iteration 5342 : loss : 0.016334, loss_ce: 0.005467
2022-01-09 05:07:31,235 iteration 5343 : loss : 0.021831, loss_ce: 0.007891
2022-01-09 05:07:34,030 iteration 5344 : loss : 0.018831, loss_ce: 0.010111
2022-01-09 05:07:36,667 iteration 5345 : loss : 0.014631, loss_ce: 0.005769
2022-01-09 05:07:39,516 iteration 5346 : loss : 0.017613, loss_ce: 0.006463
2022-01-09 05:07:42,387 iteration 5347 : loss : 0.012862, loss_ce: 0.005463
2022-01-09 05:07:45,165 iteration 5348 : loss : 0.020402, loss_ce: 0.006684
2022-01-09 05:07:47,973 iteration 5349 : loss : 0.022027, loss_ce: 0.007354
2022-01-09 05:07:50,762 iteration 5350 : loss : 0.014029, loss_ce: 0.004394
2022-01-09 05:07:53,542 iteration 5351 : loss : 0.012659, loss_ce: 0.004565
2022-01-09 05:07:56,359 iteration 5352 : loss : 0.018815, loss_ce: 0.007434
2022-01-09 05:07:59,340 iteration 5353 : loss : 0.015111, loss_ce: 0.006037
2022-01-09 05:08:02,225 iteration 5354 : loss : 0.014621, loss_ce: 0.006331
2022-01-09 05:08:02,225 Training Data Eval:
2022-01-09 05:08:17,270   Average segmentation loss on training set: 0.0102
2022-01-09 05:08:17,271 Validation Data Eval:
2022-01-09 05:08:22,459   Average segmentation loss on validation set: 0.0651
2022-01-09 05:08:25,163 iteration 5355 : loss : 0.016727, loss_ce: 0.007436
 79%|█████████████████████▎     | 315/400 [4:30:18<1:17:52, 54.97s/it]2022-01-09 05:08:28,095 iteration 5356 : loss : 0.028045, loss_ce: 0.012017
2022-01-09 05:08:30,731 iteration 5357 : loss : 0.013045, loss_ce: 0.005071
2022-01-09 05:08:33,392 iteration 5358 : loss : 0.014866, loss_ce: 0.006265
2022-01-09 05:08:36,210 iteration 5359 : loss : 0.013305, loss_ce: 0.004465
2022-01-09 05:08:38,910 iteration 5360 : loss : 0.019019, loss_ce: 0.006903
2022-01-09 05:08:41,688 iteration 5361 : loss : 0.015367, loss_ce: 0.006703
2022-01-09 05:08:44,299 iteration 5362 : loss : 0.013236, loss_ce: 0.004617
2022-01-09 05:08:47,257 iteration 5363 : loss : 0.018416, loss_ce: 0.006930
2022-01-09 05:08:49,931 iteration 5364 : loss : 0.020559, loss_ce: 0.005762
2022-01-09 05:08:52,720 iteration 5365 : loss : 0.025018, loss_ce: 0.009161
2022-01-09 05:08:55,378 iteration 5366 : loss : 0.026965, loss_ce: 0.012550
2022-01-09 05:08:58,252 iteration 5367 : loss : 0.014699, loss_ce: 0.004631
2022-01-09 05:09:01,143 iteration 5368 : loss : 0.024395, loss_ce: 0.008098
2022-01-09 05:09:03,930 iteration 5369 : loss : 0.010818, loss_ce: 0.004115
2022-01-09 05:09:06,671 iteration 5370 : loss : 0.014891, loss_ce: 0.007019
2022-01-09 05:09:09,443 iteration 5371 : loss : 0.016312, loss_ce: 0.004776
2022-01-09 05:09:12,195 iteration 5372 : loss : 0.013067, loss_ce: 0.004879
 79%|█████████████████████▎     | 316/400 [4:31:05<1:13:37, 52.59s/it]2022-01-09 05:09:15,022 iteration 5373 : loss : 0.022006, loss_ce: 0.012046
2022-01-09 05:09:17,872 iteration 5374 : loss : 0.015533, loss_ce: 0.003974
2022-01-09 05:09:20,652 iteration 5375 : loss : 0.013477, loss_ce: 0.004830
2022-01-09 05:09:23,535 iteration 5376 : loss : 0.021084, loss_ce: 0.006586
2022-01-09 05:09:26,459 iteration 5377 : loss : 0.019902, loss_ce: 0.005463
2022-01-09 05:09:29,318 iteration 5378 : loss : 0.025852, loss_ce: 0.007795
2022-01-09 05:09:31,964 iteration 5379 : loss : 0.014176, loss_ce: 0.005820
2022-01-09 05:09:34,735 iteration 5380 : loss : 0.014237, loss_ce: 0.006065
2022-01-09 05:09:37,466 iteration 5381 : loss : 0.019995, loss_ce: 0.007895
2022-01-09 05:09:40,260 iteration 5382 : loss : 0.018807, loss_ce: 0.007282
2022-01-09 05:09:43,016 iteration 5383 : loss : 0.021693, loss_ce: 0.011502
2022-01-09 05:09:45,782 iteration 5384 : loss : 0.013014, loss_ce: 0.006875
2022-01-09 05:09:48,645 iteration 5385 : loss : 0.020296, loss_ce: 0.007939
2022-01-09 05:09:51,240 iteration 5386 : loss : 0.016329, loss_ce: 0.006160
2022-01-09 05:09:54,166 iteration 5387 : loss : 0.016256, loss_ce: 0.005473
2022-01-09 05:09:56,971 iteration 5388 : loss : 0.012635, loss_ce: 0.003652
2022-01-09 05:09:59,836 iteration 5389 : loss : 0.023346, loss_ce: 0.009653
 79%|█████████████████████▍     | 317/400 [4:31:53<1:10:41, 51.11s/it]2022-01-09 05:10:02,639 iteration 5390 : loss : 0.013582, loss_ce: 0.004532
2022-01-09 05:10:05,506 iteration 5391 : loss : 0.029136, loss_ce: 0.013228
2022-01-09 05:10:08,306 iteration 5392 : loss : 0.017195, loss_ce: 0.005305
2022-01-09 05:10:11,041 iteration 5393 : loss : 0.014459, loss_ce: 0.004963
2022-01-09 05:10:13,983 iteration 5394 : loss : 0.011989, loss_ce: 0.003638
2022-01-09 05:10:16,638 iteration 5395 : loss : 0.018607, loss_ce: 0.006178
2022-01-09 05:10:19,393 iteration 5396 : loss : 0.013852, loss_ce: 0.004997
2022-01-09 05:10:22,228 iteration 5397 : loss : 0.017888, loss_ce: 0.007312
2022-01-09 05:10:25,030 iteration 5398 : loss : 0.026431, loss_ce: 0.009160
2022-01-09 05:10:27,950 iteration 5399 : loss : 0.019133, loss_ce: 0.006444
2022-01-09 05:10:30,563 iteration 5400 : loss : 0.017642, loss_ce: 0.006901
2022-01-09 05:10:33,436 iteration 5401 : loss : 0.015985, loss_ce: 0.008523
2022-01-09 05:10:36,067 iteration 5402 : loss : 0.017427, loss_ce: 0.009039
2022-01-09 05:10:38,847 iteration 5403 : loss : 0.014046, loss_ce: 0.005725
2022-01-09 05:10:41,683 iteration 5404 : loss : 0.014149, loss_ce: 0.006904
2022-01-09 05:10:44,584 iteration 5405 : loss : 0.018323, loss_ce: 0.006836
2022-01-09 05:10:47,377 iteration 5406 : loss : 0.011786, loss_ce: 0.005043
 80%|█████████████████████▍     | 318/400 [4:32:40<1:08:22, 50.03s/it]2022-01-09 05:10:50,285 iteration 5407 : loss : 0.015899, loss_ce: 0.007474
2022-01-09 05:10:53,039 iteration 5408 : loss : 0.019734, loss_ce: 0.006941
2022-01-09 05:10:55,774 iteration 5409 : loss : 0.011480, loss_ce: 0.003934
2022-01-09 05:10:58,751 iteration 5410 : loss : 0.018477, loss_ce: 0.007814
2022-01-09 05:11:01,547 iteration 5411 : loss : 0.010895, loss_ce: 0.004227
2022-01-09 05:11:04,408 iteration 5412 : loss : 0.017304, loss_ce: 0.006630
2022-01-09 05:11:07,043 iteration 5413 : loss : 0.012712, loss_ce: 0.005310
2022-01-09 05:11:09,877 iteration 5414 : loss : 0.012595, loss_ce: 0.004760
2022-01-09 05:11:12,767 iteration 5415 : loss : 0.025701, loss_ce: 0.009250
2022-01-09 05:11:15,684 iteration 5416 : loss : 0.013796, loss_ce: 0.004542
2022-01-09 05:11:18,514 iteration 5417 : loss : 0.011130, loss_ce: 0.003646
2022-01-09 05:11:21,379 iteration 5418 : loss : 0.017300, loss_ce: 0.008394
2022-01-09 05:11:24,115 iteration 5419 : loss : 0.017412, loss_ce: 0.007918
2022-01-09 05:11:26,689 iteration 5420 : loss : 0.013409, loss_ce: 0.005484
2022-01-09 05:11:29,569 iteration 5421 : loss : 0.018341, loss_ce: 0.007634
2022-01-09 05:11:32,420 iteration 5422 : loss : 0.018073, loss_ce: 0.003948
2022-01-09 05:11:35,066 iteration 5423 : loss : 0.018201, loss_ce: 0.007768
 80%|█████████████████████▌     | 319/400 [4:33:28<1:06:35, 49.33s/it]2022-01-09 05:11:37,892 iteration 5424 : loss : 0.019125, loss_ce: 0.006029
2022-01-09 05:11:40,701 iteration 5425 : loss : 0.015063, loss_ce: 0.003634
2022-01-09 05:11:43,573 iteration 5426 : loss : 0.023215, loss_ce: 0.010264
2022-01-09 05:11:46,328 iteration 5427 : loss : 0.013372, loss_ce: 0.003867
2022-01-09 05:11:48,992 iteration 5428 : loss : 0.011376, loss_ce: 0.004716
2022-01-09 05:11:51,770 iteration 5429 : loss : 0.016567, loss_ce: 0.006290
2022-01-09 05:11:54,699 iteration 5430 : loss : 0.013678, loss_ce: 0.004245
2022-01-09 05:11:57,355 iteration 5431 : loss : 0.015232, loss_ce: 0.006448
2022-01-09 05:12:00,150 iteration 5432 : loss : 0.019238, loss_ce: 0.008988
2022-01-09 05:12:02,999 iteration 5433 : loss : 0.013350, loss_ce: 0.005221
2022-01-09 05:12:05,607 iteration 5434 : loss : 0.012610, loss_ce: 0.004588
2022-01-09 05:12:08,382 iteration 5435 : loss : 0.023338, loss_ce: 0.011253
2022-01-09 05:12:11,209 iteration 5436 : loss : 0.013800, loss_ce: 0.005858
2022-01-09 05:12:13,908 iteration 5437 : loss : 0.010684, loss_ce: 0.004183
2022-01-09 05:12:16,820 iteration 5438 : loss : 0.024255, loss_ce: 0.010222
2022-01-09 05:12:19,597 iteration 5439 : loss : 0.010191, loss_ce: 0.004002
2022-01-09 05:12:19,597 Training Data Eval:
2022-01-09 05:12:34,756   Average segmentation loss on training set: 0.0096
2022-01-09 05:12:34,756 Validation Data Eval:
2022-01-09 05:12:40,053   Average segmentation loss on validation set: 0.0698
2022-01-09 05:12:42,932 iteration 5440 : loss : 0.019950, loss_ce: 0.005283
 80%|█████████████████████▌     | 320/400 [4:34:36<1:13:11, 54.89s/it]2022-01-09 05:12:45,688 iteration 5441 : loss : 0.012131, loss_ce: 0.003641
2022-01-09 05:12:48,362 iteration 5442 : loss : 0.014201, loss_ce: 0.004742
2022-01-09 05:12:51,164 iteration 5443 : loss : 0.013901, loss_ce: 0.004941
2022-01-09 05:12:53,985 iteration 5444 : loss : 0.013447, loss_ce: 0.004807
2022-01-09 05:12:56,874 iteration 5445 : loss : 0.023123, loss_ce: 0.009579
2022-01-09 05:12:59,486 iteration 5446 : loss : 0.015796, loss_ce: 0.005150
2022-01-09 05:13:02,297 iteration 5447 : loss : 0.017603, loss_ce: 0.008362
2022-01-09 05:13:05,134 iteration 5448 : loss : 0.020465, loss_ce: 0.006126
2022-01-09 05:13:07,944 iteration 5449 : loss : 0.017344, loss_ce: 0.005792
2022-01-09 05:13:10,826 iteration 5450 : loss : 0.020150, loss_ce: 0.011057
2022-01-09 05:13:13,672 iteration 5451 : loss : 0.015973, loss_ce: 0.006659
2022-01-09 05:13:16,642 iteration 5452 : loss : 0.013512, loss_ce: 0.005333
2022-01-09 05:13:19,456 iteration 5453 : loss : 0.019647, loss_ce: 0.008373
2022-01-09 05:13:22,251 iteration 5454 : loss : 0.010022, loss_ce: 0.004579
2022-01-09 05:13:25,146 iteration 5455 : loss : 0.018428, loss_ce: 0.006621
2022-01-09 05:13:28,008 iteration 5456 : loss : 0.013463, loss_ce: 0.005241
2022-01-09 05:13:30,643 iteration 5457 : loss : 0.018385, loss_ce: 0.006716
 80%|█████████████████████▋     | 321/400 [4:35:23<1:09:26, 52.74s/it]2022-01-09 05:13:33,517 iteration 5458 : loss : 0.014602, loss_ce: 0.005455
2022-01-09 05:13:36,202 iteration 5459 : loss : 0.012826, loss_ce: 0.004538
2022-01-09 05:13:39,038 iteration 5460 : loss : 0.015828, loss_ce: 0.006179
2022-01-09 05:13:41,914 iteration 5461 : loss : 0.015928, loss_ce: 0.006743
2022-01-09 05:13:44,706 iteration 5462 : loss : 0.016239, loss_ce: 0.003708
2022-01-09 05:13:47,456 iteration 5463 : loss : 0.015781, loss_ce: 0.005389
2022-01-09 05:13:50,316 iteration 5464 : loss : 0.015956, loss_ce: 0.006061
2022-01-09 05:13:53,057 iteration 5465 : loss : 0.016197, loss_ce: 0.005721
2022-01-09 05:13:55,722 iteration 5466 : loss : 0.018195, loss_ce: 0.007135
2022-01-09 05:13:58,547 iteration 5467 : loss : 0.015920, loss_ce: 0.008698
2022-01-09 05:14:01,406 iteration 5468 : loss : 0.019764, loss_ce: 0.008501
2022-01-09 05:14:04,235 iteration 5469 : loss : 0.010733, loss_ce: 0.003606
2022-01-09 05:14:07,103 iteration 5470 : loss : 0.016108, loss_ce: 0.006190
2022-01-09 05:14:09,966 iteration 5471 : loss : 0.015471, loss_ce: 0.005072
2022-01-09 05:14:12,758 iteration 5472 : loss : 0.013690, loss_ce: 0.005057
2022-01-09 05:14:15,601 iteration 5473 : loss : 0.015544, loss_ce: 0.005678
2022-01-09 05:14:18,459 iteration 5474 : loss : 0.016396, loss_ce: 0.006936
 80%|█████████████████████▋     | 322/400 [4:36:11<1:06:38, 51.26s/it]2022-01-09 05:14:21,278 iteration 5475 : loss : 0.017834, loss_ce: 0.005077
2022-01-09 05:14:24,085 iteration 5476 : loss : 0.018546, loss_ce: 0.005277
2022-01-09 05:14:26,726 iteration 5477 : loss : 0.011872, loss_ce: 0.004835
2022-01-09 05:14:29,552 iteration 5478 : loss : 0.020810, loss_ce: 0.011423
2022-01-09 05:14:32,438 iteration 5479 : loss : 0.014605, loss_ce: 0.006487
2022-01-09 05:14:35,306 iteration 5480 : loss : 0.016374, loss_ce: 0.006411
2022-01-09 05:14:38,112 iteration 5481 : loss : 0.012981, loss_ce: 0.003890
2022-01-09 05:14:40,850 iteration 5482 : loss : 0.011959, loss_ce: 0.003960
2022-01-09 05:14:43,714 iteration 5483 : loss : 0.012714, loss_ce: 0.005089
2022-01-09 05:14:46,431 iteration 5484 : loss : 0.013484, loss_ce: 0.005743
2022-01-09 05:14:48,993 iteration 5485 : loss : 0.011225, loss_ce: 0.003992
2022-01-09 05:14:51,845 iteration 5486 : loss : 0.025539, loss_ce: 0.012851
2022-01-09 05:14:54,824 iteration 5487 : loss : 0.021829, loss_ce: 0.008711
2022-01-09 05:14:57,617 iteration 5488 : loss : 0.023167, loss_ce: 0.008307
2022-01-09 05:15:00,549 iteration 5489 : loss : 0.015395, loss_ce: 0.005992
2022-01-09 05:15:03,317 iteration 5490 : loss : 0.026367, loss_ce: 0.008623
2022-01-09 05:15:06,094 iteration 5491 : loss : 0.013793, loss_ce: 0.005297
 81%|█████████████████████▊     | 323/400 [4:36:59<1:04:23, 50.17s/it]2022-01-09 05:15:08,970 iteration 5492 : loss : 0.015184, loss_ce: 0.006595
2022-01-09 05:15:11,597 iteration 5493 : loss : 0.019774, loss_ce: 0.008715
2022-01-09 05:15:14,365 iteration 5494 : loss : 0.015458, loss_ce: 0.004294
2022-01-09 05:15:17,079 iteration 5495 : loss : 0.023879, loss_ce: 0.004095
2022-01-09 05:15:19,820 iteration 5496 : loss : 0.014147, loss_ce: 0.004264
2022-01-09 05:15:22,674 iteration 5497 : loss : 0.012164, loss_ce: 0.003874
2022-01-09 05:15:25,490 iteration 5498 : loss : 0.017698, loss_ce: 0.010056
2022-01-09 05:15:28,138 iteration 5499 : loss : 0.020869, loss_ce: 0.007314
2022-01-09 05:15:30,974 iteration 5500 : loss : 0.024427, loss_ce: 0.008874
2022-01-09 05:15:33,866 iteration 5501 : loss : 0.019347, loss_ce: 0.008289
2022-01-09 05:15:36,690 iteration 5502 : loss : 0.019623, loss_ce: 0.007857
2022-01-09 05:15:39,327 iteration 5503 : loss : 0.020683, loss_ce: 0.006443
2022-01-09 05:15:42,175 iteration 5504 : loss : 0.014867, loss_ce: 0.006526
2022-01-09 05:15:44,951 iteration 5505 : loss : 0.014898, loss_ce: 0.006998
2022-01-09 05:15:47,718 iteration 5506 : loss : 0.016220, loss_ce: 0.005827
2022-01-09 05:15:50,517 iteration 5507 : loss : 0.022219, loss_ce: 0.007758
2022-01-09 05:15:53,240 iteration 5508 : loss : 0.015331, loss_ce: 0.007310
 81%|█████████████████████▊     | 324/400 [4:37:46<1:02:24, 49.27s/it]2022-01-09 05:15:55,986 iteration 5509 : loss : 0.018739, loss_ce: 0.007267
2022-01-09 05:15:58,936 iteration 5510 : loss : 0.022833, loss_ce: 0.008717
2022-01-09 05:16:01,722 iteration 5511 : loss : 0.021252, loss_ce: 0.010822
2022-01-09 05:16:04,590 iteration 5512 : loss : 0.013612, loss_ce: 0.002092
2022-01-09 05:16:07,372 iteration 5513 : loss : 0.015288, loss_ce: 0.003401
2022-01-09 05:16:10,231 iteration 5514 : loss : 0.017399, loss_ce: 0.007231
2022-01-09 05:16:13,100 iteration 5515 : loss : 0.034793, loss_ce: 0.009539
2022-01-09 05:16:15,661 iteration 5516 : loss : 0.011405, loss_ce: 0.004175
2022-01-09 05:16:18,545 iteration 5517 : loss : 0.014234, loss_ce: 0.004545
2022-01-09 05:16:21,185 iteration 5518 : loss : 0.015959, loss_ce: 0.006996
2022-01-09 05:16:23,953 iteration 5519 : loss : 0.017882, loss_ce: 0.005879
2022-01-09 05:16:26,770 iteration 5520 : loss : 0.022722, loss_ce: 0.007999
2022-01-09 05:16:29,711 iteration 5521 : loss : 0.019932, loss_ce: 0.006928
2022-01-09 05:16:32,339 iteration 5522 : loss : 0.011075, loss_ce: 0.004650
2022-01-09 05:16:35,126 iteration 5523 : loss : 0.014893, loss_ce: 0.006114
2022-01-09 05:16:37,941 iteration 5524 : loss : 0.016249, loss_ce: 0.007457
2022-01-09 05:16:37,941 Training Data Eval:
2022-01-09 05:16:52,464   Average segmentation loss on training set: 0.0091
2022-01-09 05:16:52,465 Validation Data Eval:
2022-01-09 05:16:57,610   Average segmentation loss on validation set: 0.0653
2022-01-09 05:17:00,506 iteration 5525 : loss : 0.030901, loss_ce: 0.012440
 81%|█████████████████████▉     | 325/400 [4:38:53<1:08:20, 54.67s/it]2022-01-09 05:17:03,449 iteration 5526 : loss : 0.017870, loss_ce: 0.006535
2022-01-09 05:17:06,133 iteration 5527 : loss : 0.016159, loss_ce: 0.005716
2022-01-09 05:17:09,032 iteration 5528 : loss : 0.015575, loss_ce: 0.006290
2022-01-09 05:17:11,685 iteration 5529 : loss : 0.012216, loss_ce: 0.004269
2022-01-09 05:17:14,491 iteration 5530 : loss : 0.013175, loss_ce: 0.003702
2022-01-09 05:17:17,357 iteration 5531 : loss : 0.014733, loss_ce: 0.005687
2022-01-09 05:17:20,168 iteration 5532 : loss : 0.018999, loss_ce: 0.006229
2022-01-09 05:17:22,904 iteration 5533 : loss : 0.013826, loss_ce: 0.005729
2022-01-09 05:17:25,784 iteration 5534 : loss : 0.018109, loss_ce: 0.006429
2022-01-09 05:17:28,653 iteration 5535 : loss : 0.015381, loss_ce: 0.006068
2022-01-09 05:17:31,423 iteration 5536 : loss : 0.017621, loss_ce: 0.005220
2022-01-09 05:17:34,228 iteration 5537 : loss : 0.014785, loss_ce: 0.004932
2022-01-09 05:17:36,844 iteration 5538 : loss : 0.013038, loss_ce: 0.006544
2022-01-09 05:17:39,580 iteration 5539 : loss : 0.015613, loss_ce: 0.007007
2022-01-09 05:17:42,557 iteration 5540 : loss : 0.012911, loss_ce: 0.004196
2022-01-09 05:17:45,296 iteration 5541 : loss : 0.015595, loss_ce: 0.005778
2022-01-09 05:17:48,102 iteration 5542 : loss : 0.013963, loss_ce: 0.005765
 82%|██████████████████████     | 326/400 [4:39:41<1:04:48, 52.54s/it]2022-01-09 05:17:50,878 iteration 5543 : loss : 0.031840, loss_ce: 0.015596
2022-01-09 05:17:53,761 iteration 5544 : loss : 0.017857, loss_ce: 0.005178
2022-01-09 05:17:56,618 iteration 5545 : loss : 0.022120, loss_ce: 0.007939
2022-01-09 05:17:59,415 iteration 5546 : loss : 0.010915, loss_ce: 0.005028
2022-01-09 05:18:02,112 iteration 5547 : loss : 0.014335, loss_ce: 0.006107
2022-01-09 05:18:04,911 iteration 5548 : loss : 0.016853, loss_ce: 0.005972
2022-01-09 05:18:07,745 iteration 5549 : loss : 0.012784, loss_ce: 0.004740
2022-01-09 05:18:10,612 iteration 5550 : loss : 0.018882, loss_ce: 0.007577
2022-01-09 05:18:13,411 iteration 5551 : loss : 0.016547, loss_ce: 0.004324
2022-01-09 05:18:16,485 iteration 5552 : loss : 0.027167, loss_ce: 0.007571
2022-01-09 05:18:19,351 iteration 5553 : loss : 0.014966, loss_ce: 0.005296
2022-01-09 05:18:22,283 iteration 5554 : loss : 0.018213, loss_ce: 0.004800
2022-01-09 05:18:25,172 iteration 5555 : loss : 0.017813, loss_ce: 0.007601
2022-01-09 05:18:28,029 iteration 5556 : loss : 0.013224, loss_ce: 0.005927
2022-01-09 05:18:30,895 iteration 5557 : loss : 0.023004, loss_ce: 0.007066
2022-01-09 05:18:33,562 iteration 5558 : loss : 0.016401, loss_ce: 0.006127
2022-01-09 05:18:36,139 iteration 5559 : loss : 0.012596, loss_ce: 0.004186
 82%|██████████████████████     | 327/400 [4:40:29<1:02:17, 51.19s/it]2022-01-09 05:18:38,971 iteration 5560 : loss : 0.015998, loss_ce: 0.006398
2022-01-09 05:18:41,849 iteration 5561 : loss : 0.017217, loss_ce: 0.004942
2022-01-09 05:18:44,650 iteration 5562 : loss : 0.015401, loss_ce: 0.004244
2022-01-09 05:18:47,509 iteration 5563 : loss : 0.018500, loss_ce: 0.007102
2022-01-09 05:18:50,170 iteration 5564 : loss : 0.015121, loss_ce: 0.004585
2022-01-09 05:18:53,010 iteration 5565 : loss : 0.019201, loss_ce: 0.005648
2022-01-09 05:18:55,610 iteration 5566 : loss : 0.014240, loss_ce: 0.004854
2022-01-09 05:18:58,441 iteration 5567 : loss : 0.012984, loss_ce: 0.006109
2022-01-09 05:19:01,273 iteration 5568 : loss : 0.021272, loss_ce: 0.009853
2022-01-09 05:19:04,128 iteration 5569 : loss : 0.022139, loss_ce: 0.010708
2022-01-09 05:19:06,929 iteration 5570 : loss : 0.013641, loss_ce: 0.006586
2022-01-09 05:19:09,759 iteration 5571 : loss : 0.027723, loss_ce: 0.010114
2022-01-09 05:19:12,352 iteration 5572 : loss : 0.010357, loss_ce: 0.003622
2022-01-09 05:19:15,170 iteration 5573 : loss : 0.019353, loss_ce: 0.007610
2022-01-09 05:19:17,789 iteration 5574 : loss : 0.012101, loss_ce: 0.004644
2022-01-09 05:19:20,552 iteration 5575 : loss : 0.013083, loss_ce: 0.005094
2022-01-09 05:19:23,174 iteration 5576 : loss : 0.016186, loss_ce: 0.004866
 82%|███████████████████████▊     | 328/400 [4:41:16<59:56, 49.94s/it]2022-01-09 05:19:26,074 iteration 5577 : loss : 0.020497, loss_ce: 0.006354
2022-01-09 05:19:28,844 iteration 5578 : loss : 0.011845, loss_ce: 0.004007
2022-01-09 05:19:31,730 iteration 5579 : loss : 0.015637, loss_ce: 0.004057
2022-01-09 05:19:34,578 iteration 5580 : loss : 0.017755, loss_ce: 0.005423
2022-01-09 05:19:37,448 iteration 5581 : loss : 0.018576, loss_ce: 0.008842
2022-01-09 05:19:40,307 iteration 5582 : loss : 0.015918, loss_ce: 0.006656
2022-01-09 05:19:42,908 iteration 5583 : loss : 0.012859, loss_ce: 0.003893
2022-01-09 05:19:45,691 iteration 5584 : loss : 0.018606, loss_ce: 0.008012
2022-01-09 05:19:48,328 iteration 5585 : loss : 0.012004, loss_ce: 0.003973
2022-01-09 05:19:51,123 iteration 5586 : loss : 0.011435, loss_ce: 0.004651
2022-01-09 05:19:53,782 iteration 5587 : loss : 0.013633, loss_ce: 0.005486
2022-01-09 05:19:56,426 iteration 5588 : loss : 0.015377, loss_ce: 0.006408
2022-01-09 05:19:59,317 iteration 5589 : loss : 0.015126, loss_ce: 0.004651
2022-01-09 05:20:01,920 iteration 5590 : loss : 0.012476, loss_ce: 0.005331
2022-01-09 05:20:04,756 iteration 5591 : loss : 0.021858, loss_ce: 0.007495
2022-01-09 05:20:07,573 iteration 5592 : loss : 0.014326, loss_ce: 0.005782
2022-01-09 05:20:10,140 iteration 5593 : loss : 0.011924, loss_ce: 0.006032
 82%|███████████████████████▊     | 329/400 [4:42:03<58:02, 49.05s/it]2022-01-09 05:20:13,027 iteration 5594 : loss : 0.010563, loss_ce: 0.003406
2022-01-09 05:20:15,640 iteration 5595 : loss : 0.011112, loss_ce: 0.004217
2022-01-09 05:20:18,486 iteration 5596 : loss : 0.031937, loss_ce: 0.009243
2022-01-09 05:20:21,301 iteration 5597 : loss : 0.014997, loss_ce: 0.006914
2022-01-09 05:20:24,211 iteration 5598 : loss : 0.019077, loss_ce: 0.008170
2022-01-09 05:20:27,151 iteration 5599 : loss : 0.021139, loss_ce: 0.008069
2022-01-09 05:20:29,765 iteration 5600 : loss : 0.013215, loss_ce: 0.005602
2022-01-09 05:20:32,566 iteration 5601 : loss : 0.015131, loss_ce: 0.005480
2022-01-09 05:20:35,402 iteration 5602 : loss : 0.018131, loss_ce: 0.006316
2022-01-09 05:20:38,093 iteration 5603 : loss : 0.014439, loss_ce: 0.006896
2022-01-09 05:20:40,679 iteration 5604 : loss : 0.017363, loss_ce: 0.005077
2022-01-09 05:20:43,453 iteration 5605 : loss : 0.022137, loss_ce: 0.010696
2022-01-09 05:20:46,151 iteration 5606 : loss : 0.027496, loss_ce: 0.009458
2022-01-09 05:20:48,926 iteration 5607 : loss : 0.019716, loss_ce: 0.007250
2022-01-09 05:20:51,728 iteration 5608 : loss : 0.014079, loss_ce: 0.007001
2022-01-09 05:20:54,349 iteration 5609 : loss : 0.012988, loss_ce: 0.004515
2022-01-09 05:20:54,349 Training Data Eval:
2022-01-09 05:21:09,233   Average segmentation loss on training set: 0.0088
2022-01-09 05:21:09,234 Validation Data Eval:
2022-01-09 05:21:14,419   Average segmentation loss on validation set: 0.0699
2022-01-09 05:21:17,256 iteration 5610 : loss : 0.019166, loss_ce: 0.007304
 82%|██████████████████████▎    | 330/400 [4:43:10<1:03:33, 54.47s/it]2022-01-09 05:21:19,969 iteration 5611 : loss : 0.008908, loss_ce: 0.003457
2022-01-09 05:21:22,708 iteration 5612 : loss : 0.021549, loss_ce: 0.006940
2022-01-09 05:21:25,343 iteration 5613 : loss : 0.019501, loss_ce: 0.004993
2022-01-09 05:21:27,988 iteration 5614 : loss : 0.015234, loss_ce: 0.005627
2022-01-09 05:21:30,556 iteration 5615 : loss : 0.015186, loss_ce: 0.006814
2022-01-09 05:21:33,127 iteration 5616 : loss : 0.011720, loss_ce: 0.003445
2022-01-09 05:21:35,974 iteration 5617 : loss : 0.035049, loss_ce: 0.010243
2022-01-09 05:21:38,625 iteration 5618 : loss : 0.016057, loss_ce: 0.005301
2022-01-09 05:21:41,439 iteration 5619 : loss : 0.012207, loss_ce: 0.005421
2022-01-09 05:21:44,075 iteration 5620 : loss : 0.011780, loss_ce: 0.004519
2022-01-09 05:21:46,711 iteration 5621 : loss : 0.015069, loss_ce: 0.005330
2022-01-09 05:21:49,262 iteration 5622 : loss : 0.011258, loss_ce: 0.004931
2022-01-09 05:21:52,143 iteration 5623 : loss : 0.012750, loss_ce: 0.004740
2022-01-09 05:21:54,824 iteration 5624 : loss : 0.011213, loss_ce: 0.004100
2022-01-09 05:21:57,534 iteration 5625 : loss : 0.017541, loss_ce: 0.008222
2022-01-09 05:22:00,204 iteration 5626 : loss : 0.015566, loss_ce: 0.005772
2022-01-09 05:22:02,776 iteration 5627 : loss : 0.012134, loss_ce: 0.005171
 83%|███████████████████████▉     | 331/400 [4:43:55<59:33, 51.79s/it]2022-01-09 05:22:05,478 iteration 5628 : loss : 0.012038, loss_ce: 0.004738
2022-01-09 05:22:08,124 iteration 5629 : loss : 0.021336, loss_ce: 0.008406
2022-01-09 05:22:10,784 iteration 5630 : loss : 0.012227, loss_ce: 0.004366
2022-01-09 05:22:13,645 iteration 5631 : loss : 0.014698, loss_ce: 0.007444
2022-01-09 05:22:16,382 iteration 5632 : loss : 0.014993, loss_ce: 0.005862
2022-01-09 05:22:18,834 iteration 5633 : loss : 0.017402, loss_ce: 0.004426
2022-01-09 05:22:21,462 iteration 5634 : loss : 0.018420, loss_ce: 0.004576
2022-01-09 05:22:24,088 iteration 5635 : loss : 0.012772, loss_ce: 0.004955
2022-01-09 05:22:26,745 iteration 5636 : loss : 0.018622, loss_ce: 0.005238
2022-01-09 05:22:29,488 iteration 5637 : loss : 0.014793, loss_ce: 0.006104
2022-01-09 05:22:32,137 iteration 5638 : loss : 0.015021, loss_ce: 0.006965
2022-01-09 05:22:34,656 iteration 5639 : loss : 0.016802, loss_ce: 0.005132
2022-01-09 05:22:37,308 iteration 5640 : loss : 0.020137, loss_ce: 0.006685
2022-01-09 05:22:39,861 iteration 5641 : loss : 0.012297, loss_ce: 0.004530
2022-01-09 05:22:42,459 iteration 5642 : loss : 0.018167, loss_ce: 0.008500
2022-01-09 05:22:45,054 iteration 5643 : loss : 0.015413, loss_ce: 0.006882
2022-01-09 05:22:47,609 iteration 5644 : loss : 0.017541, loss_ce: 0.008165
 83%|████████████████████████     | 332/400 [4:44:40<56:19, 49.70s/it]2022-01-09 05:22:50,208 iteration 5645 : loss : 0.011509, loss_ce: 0.003128
2022-01-09 05:22:52,833 iteration 5646 : loss : 0.013634, loss_ce: 0.003821
2022-01-09 05:22:55,428 iteration 5647 : loss : 0.010323, loss_ce: 0.003594
2022-01-09 05:22:57,944 iteration 5648 : loss : 0.014842, loss_ce: 0.007515
2022-01-09 05:23:00,609 iteration 5649 : loss : 0.015225, loss_ce: 0.003374
2022-01-09 05:23:03,305 iteration 5650 : loss : 0.015244, loss_ce: 0.006383
2022-01-09 05:23:05,945 iteration 5651 : loss : 0.016300, loss_ce: 0.006967
2022-01-09 05:23:08,607 iteration 5652 : loss : 0.014561, loss_ce: 0.004197
2022-01-09 05:23:11,259 iteration 5653 : loss : 0.016494, loss_ce: 0.004906
2022-01-09 05:23:13,737 iteration 5654 : loss : 0.014822, loss_ce: 0.005508
2022-01-09 05:23:16,254 iteration 5655 : loss : 0.011384, loss_ce: 0.005190
2022-01-09 05:23:18,950 iteration 5656 : loss : 0.024054, loss_ce: 0.007817
2022-01-09 05:23:21,492 iteration 5657 : loss : 0.021885, loss_ce: 0.008481
2022-01-09 05:23:24,036 iteration 5658 : loss : 0.017134, loss_ce: 0.007199
2022-01-09 05:23:26,642 iteration 5659 : loss : 0.014427, loss_ce: 0.005218
2022-01-09 05:23:29,151 iteration 5660 : loss : 0.017765, loss_ce: 0.005907
2022-01-09 05:23:31,713 iteration 5661 : loss : 0.015351, loss_ce: 0.006482
 83%|████████████████████████▏    | 333/400 [4:45:24<53:37, 48.02s/it]2022-01-09 05:23:34,255 iteration 5662 : loss : 0.015568, loss_ce: 0.005617
2022-01-09 05:23:36,736 iteration 5663 : loss : 0.018986, loss_ce: 0.005400
2022-01-09 05:23:39,346 iteration 5664 : loss : 0.014189, loss_ce: 0.006057
2022-01-09 05:23:41,976 iteration 5665 : loss : 0.010529, loss_ce: 0.003951
2022-01-09 05:23:44,472 iteration 5666 : loss : 0.019341, loss_ce: 0.004499
2022-01-09 05:23:47,125 iteration 5667 : loss : 0.019274, loss_ce: 0.007503
2022-01-09 05:23:49,620 iteration 5668 : loss : 0.016848, loss_ce: 0.005900
2022-01-09 05:23:52,283 iteration 5669 : loss : 0.021476, loss_ce: 0.007549
2022-01-09 05:23:54,780 iteration 5670 : loss : 0.012615, loss_ce: 0.004230
2022-01-09 05:23:57,213 iteration 5671 : loss : 0.014418, loss_ce: 0.005200
2022-01-09 05:23:59,968 iteration 5672 : loss : 0.020120, loss_ce: 0.009810
2022-01-09 05:24:02,620 iteration 5673 : loss : 0.014211, loss_ce: 0.005120
2022-01-09 05:24:05,103 iteration 5674 : loss : 0.015768, loss_ce: 0.004568
2022-01-09 05:24:07,613 iteration 5675 : loss : 0.012115, loss_ce: 0.005893
2022-01-09 05:24:10,275 iteration 5676 : loss : 0.018753, loss_ce: 0.007495
2022-01-09 05:24:12,914 iteration 5677 : loss : 0.019806, loss_ce: 0.005797
2022-01-09 05:24:15,564 iteration 5678 : loss : 0.025102, loss_ce: 0.010289
 84%|████████████████████████▏    | 334/400 [4:46:08<51:26, 46.77s/it]2022-01-09 05:24:18,218 iteration 5679 : loss : 0.015276, loss_ce: 0.005172
2022-01-09 05:24:20,662 iteration 5680 : loss : 0.016691, loss_ce: 0.009549
2022-01-09 05:24:23,349 iteration 5681 : loss : 0.012622, loss_ce: 0.004948
2022-01-09 05:24:25,814 iteration 5682 : loss : 0.013910, loss_ce: 0.005162
2022-01-09 05:24:28,491 iteration 5683 : loss : 0.024011, loss_ce: 0.008582
2022-01-09 05:24:31,125 iteration 5684 : loss : 0.019820, loss_ce: 0.006914
2022-01-09 05:24:33,652 iteration 5685 : loss : 0.010945, loss_ce: 0.002545
2022-01-09 05:24:36,134 iteration 5686 : loss : 0.016991, loss_ce: 0.007565
2022-01-09 05:24:38,724 iteration 5687 : loss : 0.012808, loss_ce: 0.003512
2022-01-09 05:24:41,206 iteration 5688 : loss : 0.023382, loss_ce: 0.007380
2022-01-09 05:24:43,762 iteration 5689 : loss : 0.016977, loss_ce: 0.007562
2022-01-09 05:24:46,309 iteration 5690 : loss : 0.018099, loss_ce: 0.009168
2022-01-09 05:24:48,770 iteration 5691 : loss : 0.013990, loss_ce: 0.004102
2022-01-09 05:24:51,264 iteration 5692 : loss : 0.013109, loss_ce: 0.004304
2022-01-09 05:24:53,865 iteration 5693 : loss : 0.025282, loss_ce: 0.013187
2022-01-09 05:24:56,399 iteration 5694 : loss : 0.019101, loss_ce: 0.006713
2022-01-09 05:24:56,399 Training Data Eval:
2022-01-09 05:25:10,108   Average segmentation loss on training set: 0.0093
2022-01-09 05:25:10,109 Validation Data Eval:
2022-01-09 05:25:14,816   Average segmentation loss on validation set: 0.0619
2022-01-09 05:25:17,322 iteration 5695 : loss : 0.017762, loss_ce: 0.006603
 84%|████████████████████████▎    | 335/400 [4:47:10<55:32, 51.27s/it]2022-01-09 05:25:19,976 iteration 5696 : loss : 0.014289, loss_ce: 0.004809
2022-01-09 05:25:22,530 iteration 5697 : loss : 0.025785, loss_ce: 0.010625
2022-01-09 05:25:25,041 iteration 5698 : loss : 0.011767, loss_ce: 0.004330
2022-01-09 05:25:27,598 iteration 5699 : loss : 0.017750, loss_ce: 0.006454
2022-01-09 05:25:30,083 iteration 5700 : loss : 0.013538, loss_ce: 0.004899
2022-01-09 05:25:32,682 iteration 5701 : loss : 0.014796, loss_ce: 0.007479
2022-01-09 05:25:35,366 iteration 5702 : loss : 0.011190, loss_ce: 0.004011
2022-01-09 05:25:37,838 iteration 5703 : loss : 0.010534, loss_ce: 0.004364
2022-01-09 05:25:40,557 iteration 5704 : loss : 0.017499, loss_ce: 0.007863
2022-01-09 05:25:43,065 iteration 5705 : loss : 0.014156, loss_ce: 0.005453
2022-01-09 05:25:45,685 iteration 5706 : loss : 0.015392, loss_ce: 0.003518
2022-01-09 05:25:48,347 iteration 5707 : loss : 0.024179, loss_ce: 0.008090
2022-01-09 05:25:50,812 iteration 5708 : loss : 0.020053, loss_ce: 0.008024
2022-01-09 05:25:53,471 iteration 5709 : loss : 0.018373, loss_ce: 0.008359
2022-01-09 05:25:56,119 iteration 5710 : loss : 0.015768, loss_ce: 0.006180
2022-01-09 05:25:58,581 iteration 5711 : loss : 0.018481, loss_ce: 0.009531
2022-01-09 05:26:01,052 iteration 5712 : loss : 0.013971, loss_ce: 0.005662
 84%|████████████████████████▎    | 336/400 [4:47:54<52:16, 49.01s/it]2022-01-09 05:26:03,711 iteration 5713 : loss : 0.014652, loss_ce: 0.003769
2022-01-09 05:26:06,157 iteration 5714 : loss : 0.012892, loss_ce: 0.004728
2022-01-09 05:26:08,631 iteration 5715 : loss : 0.012340, loss_ce: 0.004830
2022-01-09 05:26:11,143 iteration 5716 : loss : 0.014235, loss_ce: 0.006314
2022-01-09 05:26:13,738 iteration 5717 : loss : 0.019533, loss_ce: 0.006841
2022-01-09 05:26:16,160 iteration 5718 : loss : 0.016210, loss_ce: 0.004693
2022-01-09 05:26:18,857 iteration 5719 : loss : 0.012217, loss_ce: 0.004568
2022-01-09 05:26:21,314 iteration 5720 : loss : 0.016180, loss_ce: 0.006549
2022-01-09 05:26:23,811 iteration 5721 : loss : 0.011530, loss_ce: 0.003247
2022-01-09 05:26:26,287 iteration 5722 : loss : 0.012971, loss_ce: 0.004371
2022-01-09 05:26:28,750 iteration 5723 : loss : 0.013475, loss_ce: 0.004721
2022-01-09 05:26:31,204 iteration 5724 : loss : 0.009072, loss_ce: 0.003384
2022-01-09 05:26:33,838 iteration 5725 : loss : 0.012110, loss_ce: 0.005240
2022-01-09 05:26:36,260 iteration 5726 : loss : 0.013211, loss_ce: 0.005739
2022-01-09 05:26:38,742 iteration 5727 : loss : 0.015456, loss_ce: 0.004900
2022-01-09 05:26:41,196 iteration 5728 : loss : 0.013722, loss_ce: 0.006860
2022-01-09 05:26:43,704 iteration 5729 : loss : 0.012591, loss_ce: 0.004217
 84%|████████████████████████▍    | 337/400 [4:48:36<49:27, 47.10s/it]2022-01-09 05:26:46,265 iteration 5730 : loss : 0.019115, loss_ce: 0.005134
2022-01-09 05:26:48,804 iteration 5731 : loss : 0.013415, loss_ce: 0.006562
2022-01-09 05:26:51,339 iteration 5732 : loss : 0.017515, loss_ce: 0.005487
2022-01-09 05:26:53,877 iteration 5733 : loss : 0.010924, loss_ce: 0.003093
2022-01-09 05:26:56,384 iteration 5734 : loss : 0.021616, loss_ce: 0.007385
2022-01-09 05:26:58,871 iteration 5735 : loss : 0.014054, loss_ce: 0.006519
2022-01-09 05:27:01,477 iteration 5736 : loss : 0.020618, loss_ce: 0.013196
2022-01-09 05:27:03,937 iteration 5737 : loss : 0.018576, loss_ce: 0.006644
2022-01-09 05:27:06,447 iteration 5738 : loss : 0.010163, loss_ce: 0.004273
2022-01-09 05:27:08,934 iteration 5739 : loss : 0.014050, loss_ce: 0.005808
2022-01-09 05:27:11,632 iteration 5740 : loss : 0.016908, loss_ce: 0.005914
2022-01-09 05:27:14,129 iteration 5741 : loss : 0.014184, loss_ce: 0.006777
2022-01-09 05:27:16,814 iteration 5742 : loss : 0.027629, loss_ce: 0.013497
2022-01-09 05:27:19,271 iteration 5743 : loss : 0.013025, loss_ce: 0.005164
2022-01-09 05:27:21,800 iteration 5744 : loss : 0.013356, loss_ce: 0.005112
2022-01-09 05:27:24,283 iteration 5745 : loss : 0.023519, loss_ce: 0.005969
2022-01-09 05:27:26,757 iteration 5746 : loss : 0.010217, loss_ce: 0.002661
 84%|████████████████████████▌    | 338/400 [4:49:19<47:24, 45.89s/it]2022-01-09 05:27:29,322 iteration 5747 : loss : 0.013459, loss_ce: 0.004798
2022-01-09 05:27:31,846 iteration 5748 : loss : 0.019606, loss_ce: 0.005429
2022-01-09 05:27:34,432 iteration 5749 : loss : 0.020653, loss_ce: 0.009419
2022-01-09 05:27:37,055 iteration 5750 : loss : 0.023814, loss_ce: 0.009366
2022-01-09 05:27:39,481 iteration 5751 : loss : 0.015854, loss_ce: 0.005792
2022-01-09 05:27:41,986 iteration 5752 : loss : 0.018373, loss_ce: 0.006572
2022-01-09 05:27:44,513 iteration 5753 : loss : 0.025964, loss_ce: 0.009122
2022-01-09 05:27:47,020 iteration 5754 : loss : 0.013531, loss_ce: 0.004984
2022-01-09 05:27:49,497 iteration 5755 : loss : 0.016334, loss_ce: 0.007887
2022-01-09 05:27:52,021 iteration 5756 : loss : 0.014921, loss_ce: 0.004530
2022-01-09 05:27:54,540 iteration 5757 : loss : 0.013509, loss_ce: 0.006751
2022-01-09 05:27:57,172 iteration 5758 : loss : 0.018273, loss_ce: 0.004013
2022-01-09 05:27:59,662 iteration 5759 : loss : 0.023436, loss_ce: 0.012580
2022-01-09 05:28:02,219 iteration 5760 : loss : 0.019497, loss_ce: 0.007307
2022-01-09 05:28:04,629 iteration 5761 : loss : 0.014245, loss_ce: 0.006312
2022-01-09 05:28:07,310 iteration 5762 : loss : 0.033088, loss_ce: 0.012757
2022-01-09 05:28:09,784 iteration 5763 : loss : 0.013687, loss_ce: 0.005373
 85%|████████████████████████▌    | 339/400 [4:50:02<45:46, 45.03s/it]2022-01-09 05:28:12,343 iteration 5764 : loss : 0.028429, loss_ce: 0.012284
2022-01-09 05:28:14,870 iteration 5765 : loss : 0.020619, loss_ce: 0.006845
2022-01-09 05:28:17,566 iteration 5766 : loss : 0.014534, loss_ce: 0.005916
2022-01-09 05:28:20,057 iteration 5767 : loss : 0.015970, loss_ce: 0.006220
2022-01-09 05:28:22,713 iteration 5768 : loss : 0.024608, loss_ce: 0.007950
2022-01-09 05:28:25,180 iteration 5769 : loss : 0.015864, loss_ce: 0.005846
2022-01-09 05:28:27,710 iteration 5770 : loss : 0.015792, loss_ce: 0.006259
2022-01-09 05:28:30,195 iteration 5771 : loss : 0.015416, loss_ce: 0.007199
2022-01-09 05:28:32,825 iteration 5772 : loss : 0.021926, loss_ce: 0.008692
2022-01-09 05:28:35,459 iteration 5773 : loss : 0.017185, loss_ce: 0.007237
2022-01-09 05:28:38,080 iteration 5774 : loss : 0.018435, loss_ce: 0.007244
2022-01-09 05:28:40,719 iteration 5775 : loss : 0.016318, loss_ce: 0.005293
2022-01-09 05:28:43,167 iteration 5776 : loss : 0.016899, loss_ce: 0.006161
2022-01-09 05:28:45,617 iteration 5777 : loss : 0.016427, loss_ce: 0.006447
2022-01-09 05:28:48,139 iteration 5778 : loss : 0.019104, loss_ce: 0.005534
2022-01-09 05:28:50,618 iteration 5779 : loss : 0.015211, loss_ce: 0.006195
2022-01-09 05:28:50,618 Training Data Eval:
2022-01-09 05:29:04,067   Average segmentation loss on training set: 0.0100
2022-01-09 05:29:04,067 Validation Data Eval:
2022-01-09 05:29:08,882   Average segmentation loss on validation set: 0.0722
2022-01-09 05:29:11,414 iteration 5780 : loss : 0.014281, loss_ce: 0.005452
 85%|████████████████████████▋    | 340/400 [4:51:04<50:00, 50.01s/it]2022-01-09 05:29:14,137 iteration 5781 : loss : 0.033472, loss_ce: 0.011570
2022-01-09 05:29:16,533 iteration 5782 : loss : 0.012951, loss_ce: 0.006070
2022-01-09 05:29:18,960 iteration 5783 : loss : 0.017953, loss_ce: 0.006785
2022-01-09 05:29:21,417 iteration 5784 : loss : 0.012621, loss_ce: 0.004375
2022-01-09 05:29:23,851 iteration 5785 : loss : 0.011964, loss_ce: 0.005097
2022-01-09 05:29:26,360 iteration 5786 : loss : 0.023889, loss_ce: 0.005559
2022-01-09 05:29:28,822 iteration 5787 : loss : 0.016103, loss_ce: 0.006792
2022-01-09 05:29:31,241 iteration 5788 : loss : 0.012265, loss_ce: 0.004515
2022-01-09 05:29:33,705 iteration 5789 : loss : 0.010568, loss_ce: 0.004081
2022-01-09 05:29:36,202 iteration 5790 : loss : 0.011355, loss_ce: 0.004040
2022-01-09 05:29:38,782 iteration 5791 : loss : 0.013946, loss_ce: 0.006291
2022-01-09 05:29:41,210 iteration 5792 : loss : 0.012116, loss_ce: 0.004515
2022-01-09 05:29:43,730 iteration 5793 : loss : 0.015978, loss_ce: 0.005249
2022-01-09 05:29:46,466 iteration 5794 : loss : 0.014621, loss_ce: 0.005987
2022-01-09 05:29:48,902 iteration 5795 : loss : 0.010119, loss_ce: 0.004086
2022-01-09 05:29:51,393 iteration 5796 : loss : 0.014621, loss_ce: 0.005984
2022-01-09 05:29:53,931 iteration 5797 : loss : 0.019026, loss_ce: 0.006736
 85%|████████████████████████▋    | 341/400 [4:51:47<46:57, 47.76s/it]2022-01-09 05:29:56,592 iteration 5798 : loss : 0.016485, loss_ce: 0.004576
2022-01-09 05:29:59,070 iteration 5799 : loss : 0.014289, loss_ce: 0.004222
2022-01-09 05:30:01,540 iteration 5800 : loss : 0.015134, loss_ce: 0.004015
2022-01-09 05:30:03,983 iteration 5801 : loss : 0.010492, loss_ce: 0.003307
2022-01-09 05:30:06,490 iteration 5802 : loss : 0.012956, loss_ce: 0.006325
2022-01-09 05:30:08,897 iteration 5803 : loss : 0.012166, loss_ce: 0.004111
2022-01-09 05:30:11,657 iteration 5804 : loss : 0.032144, loss_ce: 0.010854
2022-01-09 05:30:14,106 iteration 5805 : loss : 0.017062, loss_ce: 0.007938
2022-01-09 05:30:16,538 iteration 5806 : loss : 0.014491, loss_ce: 0.005374
2022-01-09 05:30:19,029 iteration 5807 : loss : 0.011022, loss_ce: 0.003983
2022-01-09 05:30:21,622 iteration 5808 : loss : 0.031793, loss_ce: 0.013168
2022-01-09 05:30:24,155 iteration 5809 : loss : 0.025425, loss_ce: 0.008916
2022-01-09 05:30:26,575 iteration 5810 : loss : 0.016676, loss_ce: 0.004605
2022-01-09 05:30:29,157 iteration 5811 : loss : 0.014614, loss_ce: 0.006160
2022-01-09 05:30:31,659 iteration 5812 : loss : 0.015807, loss_ce: 0.006715
2022-01-09 05:30:34,098 iteration 5813 : loss : 0.025156, loss_ce: 0.007382
2022-01-09 05:30:36,717 iteration 5814 : loss : 0.018511, loss_ce: 0.006933
 86%|████████████████████████▊    | 342/400 [4:52:29<44:43, 46.27s/it]2022-01-09 05:30:39,393 iteration 5815 : loss : 0.020511, loss_ce: 0.010200
2022-01-09 05:30:41,814 iteration 5816 : loss : 0.013872, loss_ce: 0.004537
2022-01-09 05:30:44,333 iteration 5817 : loss : 0.016274, loss_ce: 0.004629
2022-01-09 05:30:46,795 iteration 5818 : loss : 0.018269, loss_ce: 0.005765
2022-01-09 05:30:49,363 iteration 5819 : loss : 0.016367, loss_ce: 0.007176
2022-01-09 05:30:51,838 iteration 5820 : loss : 0.012460, loss_ce: 0.005484
2022-01-09 05:30:54,444 iteration 5821 : loss : 0.014052, loss_ce: 0.005086
2022-01-09 05:30:57,013 iteration 5822 : loss : 0.032284, loss_ce: 0.011706
2022-01-09 05:30:59,506 iteration 5823 : loss : 0.010910, loss_ce: 0.003348
2022-01-09 05:31:01,978 iteration 5824 : loss : 0.013093, loss_ce: 0.004535
2022-01-09 05:31:04,585 iteration 5825 : loss : 0.019057, loss_ce: 0.008270
2022-01-09 05:31:07,022 iteration 5826 : loss : 0.014171, loss_ce: 0.004456
2022-01-09 05:31:09,502 iteration 5827 : loss : 0.017391, loss_ce: 0.004501
2022-01-09 05:31:11,892 iteration 5828 : loss : 0.009658, loss_ce: 0.004099
2022-01-09 05:31:14,361 iteration 5829 : loss : 0.012842, loss_ce: 0.004943
2022-01-09 05:31:17,046 iteration 5830 : loss : 0.022091, loss_ce: 0.008346
2022-01-09 05:31:19,640 iteration 5831 : loss : 0.013398, loss_ce: 0.005200
 86%|████████████████████████▊    | 343/400 [4:53:12<43:00, 45.26s/it]2022-01-09 05:31:22,323 iteration 5832 : loss : 0.017502, loss_ce: 0.006699
2022-01-09 05:31:24,781 iteration 5833 : loss : 0.017105, loss_ce: 0.006198
2022-01-09 05:31:27,304 iteration 5834 : loss : 0.015620, loss_ce: 0.005800
2022-01-09 05:31:29,977 iteration 5835 : loss : 0.026795, loss_ce: 0.011311
2022-01-09 05:31:32,451 iteration 5836 : loss : 0.016349, loss_ce: 0.007347
2022-01-09 05:31:35,074 iteration 5837 : loss : 0.015928, loss_ce: 0.007018
2022-01-09 05:31:37,693 iteration 5838 : loss : 0.017637, loss_ce: 0.004874
2022-01-09 05:31:40,108 iteration 5839 : loss : 0.014586, loss_ce: 0.004987
2022-01-09 05:31:42,710 iteration 5840 : loss : 0.019605, loss_ce: 0.009419
2022-01-09 05:31:45,246 iteration 5841 : loss : 0.009402, loss_ce: 0.003674
2022-01-09 05:31:47,794 iteration 5842 : loss : 0.016987, loss_ce: 0.006486
2022-01-09 05:31:50,426 iteration 5843 : loss : 0.018705, loss_ce: 0.005869
2022-01-09 05:31:52,948 iteration 5844 : loss : 0.016487, loss_ce: 0.004562
2022-01-09 05:31:55,475 iteration 5845 : loss : 0.023711, loss_ce: 0.008539
2022-01-09 05:31:57,965 iteration 5846 : loss : 0.011195, loss_ce: 0.004692
2022-01-09 05:32:00,481 iteration 5847 : loss : 0.012488, loss_ce: 0.004404
2022-01-09 05:32:02,886 iteration 5848 : loss : 0.011933, loss_ce: 0.003631
 86%|████████████████████████▉    | 344/400 [4:53:56<41:40, 44.66s/it]2022-01-09 05:32:05,398 iteration 5849 : loss : 0.013491, loss_ce: 0.004760
2022-01-09 05:32:08,001 iteration 5850 : loss : 0.018797, loss_ce: 0.005485
2022-01-09 05:32:10,447 iteration 5851 : loss : 0.014301, loss_ce: 0.006314
2022-01-09 05:32:12,900 iteration 5852 : loss : 0.014624, loss_ce: 0.004764
2022-01-09 05:32:15,340 iteration 5853 : loss : 0.016766, loss_ce: 0.004787
2022-01-09 05:32:17,818 iteration 5854 : loss : 0.025360, loss_ce: 0.010084
2022-01-09 05:32:20,272 iteration 5855 : loss : 0.014619, loss_ce: 0.003728
2022-01-09 05:32:22,741 iteration 5856 : loss : 0.012879, loss_ce: 0.005435
2022-01-09 05:32:25,299 iteration 5857 : loss : 0.015406, loss_ce: 0.006722
2022-01-09 05:32:27,754 iteration 5858 : loss : 0.019118, loss_ce: 0.005243
2022-01-09 05:32:30,261 iteration 5859 : loss : 0.015255, loss_ce: 0.007228
2022-01-09 05:32:32,769 iteration 5860 : loss : 0.011277, loss_ce: 0.004310
2022-01-09 05:32:35,314 iteration 5861 : loss : 0.015067, loss_ce: 0.006455
2022-01-09 05:32:37,783 iteration 5862 : loss : 0.011239, loss_ce: 0.004311
2022-01-09 05:32:40,250 iteration 5863 : loss : 0.015207, loss_ce: 0.005899
2022-01-09 05:32:42,729 iteration 5864 : loss : 0.015336, loss_ce: 0.005412
2022-01-09 05:32:42,730 Training Data Eval:
2022-01-09 05:32:56,240   Average segmentation loss on training set: 0.0084
2022-01-09 05:32:56,240 Validation Data Eval:
2022-01-09 05:33:00,905   Average segmentation loss on validation set: 0.0700
2022-01-09 05:33:03,377 iteration 5865 : loss : 0.011463, loss_ce: 0.003926
 86%|█████████████████████████    | 345/400 [4:54:56<45:17, 49.41s/it]2022-01-09 05:33:05,922 iteration 5866 : loss : 0.013045, loss_ce: 0.004206
2022-01-09 05:33:08,456 iteration 5867 : loss : 0.013346, loss_ce: 0.005708
2022-01-09 05:33:10,989 iteration 5868 : loss : 0.009965, loss_ce: 0.003778
2022-01-09 05:33:13,437 iteration 5869 : loss : 0.010034, loss_ce: 0.003441
2022-01-09 05:33:15,938 iteration 5870 : loss : 0.012805, loss_ce: 0.004670
2022-01-09 05:33:18,402 iteration 5871 : loss : 0.011581, loss_ce: 0.004540
2022-01-09 05:33:20,858 iteration 5872 : loss : 0.013076, loss_ce: 0.005338
2022-01-09 05:33:23,326 iteration 5873 : loss : 0.011912, loss_ce: 0.005361
2022-01-09 05:33:25,774 iteration 5874 : loss : 0.015713, loss_ce: 0.003912
2022-01-09 05:33:28,350 iteration 5875 : loss : 0.013547, loss_ce: 0.005589
2022-01-09 05:33:30,984 iteration 5876 : loss : 0.016967, loss_ce: 0.007941
2022-01-09 05:33:33,449 iteration 5877 : loss : 0.018434, loss_ce: 0.006940
2022-01-09 05:33:35,981 iteration 5878 : loss : 0.018286, loss_ce: 0.004901
2022-01-09 05:33:38,428 iteration 5879 : loss : 0.016980, loss_ce: 0.005427
2022-01-09 05:33:40,888 iteration 5880 : loss : 0.013250, loss_ce: 0.004822
2022-01-09 05:33:43,414 iteration 5881 : loss : 0.020681, loss_ce: 0.006341
2022-01-09 05:33:45,883 iteration 5882 : loss : 0.014427, loss_ce: 0.005838
 86%|█████████████████████████    | 346/400 [4:55:39<42:36, 47.34s/it]2022-01-09 05:33:48,460 iteration 5883 : loss : 0.019321, loss_ce: 0.006760
2022-01-09 05:33:50,911 iteration 5884 : loss : 0.014081, loss_ce: 0.005887
2022-01-09 05:33:53,516 iteration 5885 : loss : 0.020425, loss_ce: 0.004450
2022-01-09 05:33:55,940 iteration 5886 : loss : 0.010664, loss_ce: 0.003601
2022-01-09 05:33:58,390 iteration 5887 : loss : 0.017695, loss_ce: 0.005219
2022-01-09 05:34:00,786 iteration 5888 : loss : 0.011247, loss_ce: 0.004784
2022-01-09 05:34:03,240 iteration 5889 : loss : 0.013682, loss_ce: 0.006268
2022-01-09 05:34:05,811 iteration 5890 : loss : 0.015744, loss_ce: 0.007151
2022-01-09 05:34:08,299 iteration 5891 : loss : 0.019351, loss_ce: 0.005860
2022-01-09 05:34:10,783 iteration 5892 : loss : 0.013212, loss_ce: 0.006357
2022-01-09 05:34:13,214 iteration 5893 : loss : 0.011528, loss_ce: 0.002876
2022-01-09 05:34:15,639 iteration 5894 : loss : 0.011745, loss_ce: 0.003514
2022-01-09 05:34:18,198 iteration 5895 : loss : 0.016943, loss_ce: 0.007058
2022-01-09 05:34:20,494 iteration 5896 : loss : 0.011700, loss_ce: 0.003847
2022-01-09 05:34:22,974 iteration 5897 : loss : 0.015396, loss_ce: 0.008071
2022-01-09 05:34:25,377 iteration 5898 : loss : 0.019257, loss_ce: 0.007624
2022-01-09 05:34:27,960 iteration 5899 : loss : 0.014353, loss_ce: 0.004752
 87%|█████████████████████████▏   | 347/400 [4:56:21<40:25, 45.76s/it]2022-01-09 05:34:30,359 iteration 5900 : loss : 0.010182, loss_ce: 0.003974
2022-01-09 05:34:32,792 iteration 5901 : loss : 0.010590, loss_ce: 0.004438
2022-01-09 05:34:35,193 iteration 5902 : loss : 0.011452, loss_ce: 0.004452
2022-01-09 05:34:37,639 iteration 5903 : loss : 0.015943, loss_ce: 0.006851
2022-01-09 05:34:40,138 iteration 5904 : loss : 0.014813, loss_ce: 0.004776
2022-01-09 05:34:42,649 iteration 5905 : loss : 0.019328, loss_ce: 0.009129
2022-01-09 05:34:45,188 iteration 5906 : loss : 0.015455, loss_ce: 0.003704
2022-01-09 05:34:47,591 iteration 5907 : loss : 0.011177, loss_ce: 0.004802
2022-01-09 05:34:50,091 iteration 5908 : loss : 0.015490, loss_ce: 0.004898
2022-01-09 05:34:52,661 iteration 5909 : loss : 0.020507, loss_ce: 0.011234
2022-01-09 05:34:55,123 iteration 5910 : loss : 0.013876, loss_ce: 0.004252
2022-01-09 05:34:57,504 iteration 5911 : loss : 0.013767, loss_ce: 0.003338
2022-01-09 05:35:00,113 iteration 5912 : loss : 0.018547, loss_ce: 0.004080
2022-01-09 05:35:02,538 iteration 5913 : loss : 0.015419, loss_ce: 0.007173
2022-01-09 05:35:05,027 iteration 5914 : loss : 0.013276, loss_ce: 0.006241
2022-01-09 05:35:07,530 iteration 5915 : loss : 0.016402, loss_ce: 0.007874
2022-01-09 05:35:09,958 iteration 5916 : loss : 0.013468, loss_ce: 0.004796
 87%|█████████████████████████▏   | 348/400 [4:57:03<38:40, 44.63s/it]2022-01-09 05:35:12,446 iteration 5917 : loss : 0.014122, loss_ce: 0.005653
2022-01-09 05:35:14,872 iteration 5918 : loss : 0.017385, loss_ce: 0.007959
2022-01-09 05:35:17,424 iteration 5919 : loss : 0.020038, loss_ce: 0.007797
2022-01-09 05:35:19,853 iteration 5920 : loss : 0.015483, loss_ce: 0.004877
2022-01-09 05:35:22,450 iteration 5921 : loss : 0.015346, loss_ce: 0.005501
2022-01-09 05:35:24,973 iteration 5922 : loss : 0.015452, loss_ce: 0.008005
2022-01-09 05:35:27,471 iteration 5923 : loss : 0.017627, loss_ce: 0.006951
2022-01-09 05:35:29,943 iteration 5924 : loss : 0.015509, loss_ce: 0.005869
2022-01-09 05:35:32,391 iteration 5925 : loss : 0.014791, loss_ce: 0.004920
2022-01-09 05:35:34,845 iteration 5926 : loss : 0.016389, loss_ce: 0.005389
2022-01-09 05:35:37,287 iteration 5927 : loss : 0.009784, loss_ce: 0.004019
2022-01-09 05:35:39,772 iteration 5928 : loss : 0.013791, loss_ce: 0.006069
2022-01-09 05:35:42,155 iteration 5929 : loss : 0.015194, loss_ce: 0.006474
2022-01-09 05:35:44,706 iteration 5930 : loss : 0.013232, loss_ce: 0.005028
2022-01-09 05:35:47,137 iteration 5931 : loss : 0.012432, loss_ce: 0.005377
2022-01-09 05:35:49,668 iteration 5932 : loss : 0.014017, loss_ce: 0.004419
2022-01-09 05:35:52,245 iteration 5933 : loss : 0.027186, loss_ce: 0.010569
 87%|█████████████████████████▎   | 349/400 [4:57:45<37:20, 43.93s/it]2022-01-09 05:35:54,660 iteration 5934 : loss : 0.010400, loss_ce: 0.003388
2022-01-09 05:35:57,154 iteration 5935 : loss : 0.012877, loss_ce: 0.003712
2022-01-09 05:35:59,620 iteration 5936 : loss : 0.021975, loss_ce: 0.008254
2022-01-09 05:36:02,052 iteration 5937 : loss : 0.023470, loss_ce: 0.010282
2022-01-09 05:36:04,565 iteration 5938 : loss : 0.015669, loss_ce: 0.005180
2022-01-09 05:36:07,026 iteration 5939 : loss : 0.013058, loss_ce: 0.005138
2022-01-09 05:36:09,553 iteration 5940 : loss : 0.016424, loss_ce: 0.007130
2022-01-09 05:36:11,949 iteration 5941 : loss : 0.012235, loss_ce: 0.005448
2022-01-09 05:36:14,339 iteration 5942 : loss : 0.022998, loss_ce: 0.008244
2022-01-09 05:36:16,808 iteration 5943 : loss : 0.010107, loss_ce: 0.003719
2022-01-09 05:36:19,345 iteration 5944 : loss : 0.019432, loss_ce: 0.006381
2022-01-09 05:36:21,796 iteration 5945 : loss : 0.011855, loss_ce: 0.004013
2022-01-09 05:36:24,219 iteration 5946 : loss : 0.016640, loss_ce: 0.006135
2022-01-09 05:36:26,632 iteration 5947 : loss : 0.011781, loss_ce: 0.004735
2022-01-09 05:36:29,198 iteration 5948 : loss : 0.015431, loss_ce: 0.006589
2022-01-09 05:36:31,624 iteration 5949 : loss : 0.010914, loss_ce: 0.004645
2022-01-09 05:36:31,624 Training Data Eval:
2022-01-09 05:36:44,757   Average segmentation loss on training set: 0.0080
2022-01-09 05:36:44,757 Validation Data Eval:
2022-01-09 05:36:49,415   Average segmentation loss on validation set: 0.0670
2022-01-09 05:36:51,960 iteration 5950 : loss : 0.022943, loss_ce: 0.006281
 88%|█████████████████████████▍   | 350/400 [4:58:45<40:33, 48.66s/it]2022-01-09 05:36:54,484 iteration 5951 : loss : 0.014957, loss_ce: 0.004365
2022-01-09 05:36:56,971 iteration 5952 : loss : 0.012037, loss_ce: 0.006092
2022-01-09 05:36:59,502 iteration 5953 : loss : 0.014303, loss_ce: 0.006350
2022-01-09 05:37:01,910 iteration 5954 : loss : 0.022455, loss_ce: 0.008199
2022-01-09 05:37:04,394 iteration 5955 : loss : 0.011379, loss_ce: 0.004014
2022-01-09 05:37:06,810 iteration 5956 : loss : 0.024705, loss_ce: 0.006746
2022-01-09 05:37:09,300 iteration 5957 : loss : 0.020563, loss_ce: 0.005847
2022-01-09 05:37:11,734 iteration 5958 : loss : 0.011504, loss_ce: 0.005048
2022-01-09 05:37:14,074 iteration 5959 : loss : 0.013199, loss_ce: 0.004208
2022-01-09 05:37:16,534 iteration 5960 : loss : 0.008716, loss_ce: 0.002676
2022-01-09 05:37:19,080 iteration 5961 : loss : 0.017173, loss_ce: 0.006510
2022-01-09 05:37:21,604 iteration 5962 : loss : 0.016329, loss_ce: 0.006665
2022-01-09 05:37:24,074 iteration 5963 : loss : 0.012885, loss_ce: 0.005081
2022-01-09 05:37:26,560 iteration 5964 : loss : 0.021934, loss_ce: 0.008848
2022-01-09 05:37:29,006 iteration 5965 : loss : 0.014079, loss_ce: 0.005890
2022-01-09 05:37:31,584 iteration 5966 : loss : 0.019261, loss_ce: 0.006984
2022-01-09 05:37:34,039 iteration 5967 : loss : 0.018798, loss_ce: 0.005757
 88%|█████████████████████████▍   | 351/400 [4:59:27<38:07, 46.69s/it]2022-01-09 05:37:36,582 iteration 5968 : loss : 0.014366, loss_ce: 0.007499
2022-01-09 05:37:38,953 iteration 5969 : loss : 0.028047, loss_ce: 0.007722
2022-01-09 05:37:41,442 iteration 5970 : loss : 0.012558, loss_ce: 0.004311
2022-01-09 05:37:43,844 iteration 5971 : loss : 0.011270, loss_ce: 0.004461
2022-01-09 05:37:46,283 iteration 5972 : loss : 0.012963, loss_ce: 0.004733
2022-01-09 05:37:48,759 iteration 5973 : loss : 0.009568, loss_ce: 0.003989
2022-01-09 05:37:51,184 iteration 5974 : loss : 0.016380, loss_ce: 0.005482
2022-01-09 05:37:53,691 iteration 5975 : loss : 0.015997, loss_ce: 0.005301
2022-01-09 05:37:56,053 iteration 5976 : loss : 0.017843, loss_ce: 0.004106
2022-01-09 05:37:58,554 iteration 5977 : loss : 0.013608, loss_ce: 0.005175
2022-01-09 05:38:01,043 iteration 5978 : loss : 0.012983, loss_ce: 0.005221
2022-01-09 05:38:03,498 iteration 5979 : loss : 0.010779, loss_ce: 0.004071
2022-01-09 05:38:06,055 iteration 5980 : loss : 0.009306, loss_ce: 0.003317
2022-01-09 05:38:08,408 iteration 5981 : loss : 0.018599, loss_ce: 0.007343
2022-01-09 05:38:10,950 iteration 5982 : loss : 0.021093, loss_ce: 0.008448
2022-01-09 05:38:13,382 iteration 5983 : loss : 0.011599, loss_ce: 0.005451
2022-01-09 05:38:15,873 iteration 5984 : loss : 0.014383, loss_ce: 0.004171
 88%|█████████████████████████▌   | 352/400 [5:00:09<36:11, 45.23s/it]2022-01-09 05:38:18,352 iteration 5985 : loss : 0.027114, loss_ce: 0.013054
2022-01-09 05:38:20,794 iteration 5986 : loss : 0.013683, loss_ce: 0.005348
2022-01-09 05:38:23,320 iteration 5987 : loss : 0.033121, loss_ce: 0.006112
2022-01-09 05:38:25,895 iteration 5988 : loss : 0.019731, loss_ce: 0.007714
2022-01-09 05:38:28,394 iteration 5989 : loss : 0.013523, loss_ce: 0.005427
2022-01-09 05:38:30,907 iteration 5990 : loss : 0.014831, loss_ce: 0.008178
2022-01-09 05:38:33,294 iteration 5991 : loss : 0.010532, loss_ce: 0.003745
2022-01-09 05:38:35,677 iteration 5992 : loss : 0.016435, loss_ce: 0.004339
2022-01-09 05:38:38,226 iteration 5993 : loss : 0.008292, loss_ce: 0.002922
2022-01-09 05:38:40,688 iteration 5994 : loss : 0.023564, loss_ce: 0.011263
2022-01-09 05:38:43,247 iteration 5995 : loss : 0.016120, loss_ce: 0.006333
2022-01-09 05:38:45,677 iteration 5996 : loss : 0.017943, loss_ce: 0.007610
2022-01-09 05:38:48,221 iteration 5997 : loss : 0.014405, loss_ce: 0.006405
2022-01-09 05:38:50,687 iteration 5998 : loss : 0.012469, loss_ce: 0.004421
2022-01-09 05:38:53,290 iteration 5999 : loss : 0.021421, loss_ce: 0.005924
2022-01-09 05:38:55,793 iteration 6000 : loss : 0.015378, loss_ce: 0.006516
2022-01-09 05:38:58,258 iteration 6001 : loss : 0.017045, loss_ce: 0.005597
 88%|█████████████████████████▌   | 353/400 [5:00:51<34:45, 44.38s/it]2022-01-09 05:39:00,686 iteration 6002 : loss : 0.014900, loss_ce: 0.006293
2022-01-09 05:39:03,234 iteration 6003 : loss : 0.024807, loss_ce: 0.008388
2022-01-09 05:39:05,719 iteration 6004 : loss : 0.015738, loss_ce: 0.006292
2022-01-09 05:39:08,304 iteration 6005 : loss : 0.016230, loss_ce: 0.006422
2022-01-09 05:39:10,762 iteration 6006 : loss : 0.013191, loss_ce: 0.006758
2022-01-09 05:39:13,121 iteration 6007 : loss : 0.013206, loss_ce: 0.004548
2022-01-09 05:39:15,596 iteration 6008 : loss : 0.014314, loss_ce: 0.005401
2022-01-09 05:39:18,117 iteration 6009 : loss : 0.022734, loss_ce: 0.007394
2022-01-09 05:39:20,476 iteration 6010 : loss : 0.010809, loss_ce: 0.004499
2022-01-09 05:39:23,051 iteration 6011 : loss : 0.017367, loss_ce: 0.005880
2022-01-09 05:39:25,528 iteration 6012 : loss : 0.025729, loss_ce: 0.007946
2022-01-09 05:39:28,169 iteration 6013 : loss : 0.019166, loss_ce: 0.007622
2022-01-09 05:39:30,636 iteration 6014 : loss : 0.013843, loss_ce: 0.005161
2022-01-09 05:39:33,114 iteration 6015 : loss : 0.016202, loss_ce: 0.005137
2022-01-09 05:39:35,483 iteration 6016 : loss : 0.010939, loss_ce: 0.003076
2022-01-09 05:39:38,087 iteration 6017 : loss : 0.015345, loss_ce: 0.004243
2022-01-09 05:39:40,535 iteration 6018 : loss : 0.016603, loss_ce: 0.006958
 88%|█████████████████████████▋   | 354/400 [5:01:33<33:32, 43.75s/it]2022-01-09 05:39:43,153 iteration 6019 : loss : 0.016838, loss_ce: 0.006791
2022-01-09 05:39:45,636 iteration 6020 : loss : 0.016050, loss_ce: 0.007196
2022-01-09 05:39:47,984 iteration 6021 : loss : 0.015761, loss_ce: 0.005385
2022-01-09 05:39:50,481 iteration 6022 : loss : 0.013067, loss_ce: 0.005411
2022-01-09 05:39:52,884 iteration 6023 : loss : 0.013039, loss_ce: 0.004913
2022-01-09 05:39:55,442 iteration 6024 : loss : 0.016749, loss_ce: 0.006140
2022-01-09 05:39:58,035 iteration 6025 : loss : 0.024297, loss_ce: 0.008657
2022-01-09 05:40:00,558 iteration 6026 : loss : 0.017197, loss_ce: 0.005751
2022-01-09 05:40:03,009 iteration 6027 : loss : 0.012645, loss_ce: 0.005126
2022-01-09 05:40:05,462 iteration 6028 : loss : 0.015972, loss_ce: 0.006336
2022-01-09 05:40:07,996 iteration 6029 : loss : 0.031275, loss_ce: 0.012582
2022-01-09 05:40:10,430 iteration 6030 : loss : 0.012526, loss_ce: 0.004264
2022-01-09 05:40:13,036 iteration 6031 : loss : 0.025907, loss_ce: 0.007613
2022-01-09 05:40:15,628 iteration 6032 : loss : 0.013334, loss_ce: 0.003857
2022-01-09 05:40:18,107 iteration 6033 : loss : 0.018851, loss_ce: 0.008409
2022-01-09 05:40:20,541 iteration 6034 : loss : 0.012154, loss_ce: 0.003741
2022-01-09 05:40:20,542 Training Data Eval:
2022-01-09 05:40:33,936   Average segmentation loss on training set: 0.0085
2022-01-09 05:40:33,936 Validation Data Eval:
2022-01-09 05:40:38,605   Average segmentation loss on validation set: 0.0611
2022-01-09 05:40:44,440 Found new lowest validation loss at iteration 6034! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 05:40:45,986 iteration 6035 : loss : 0.016702, loss_ce: 0.004344
 89%|█████████████████████████▋   | 355/400 [5:02:39<37:41, 50.25s/it]2022-01-09 05:40:47,846 iteration 6036 : loss : 0.017814, loss_ce: 0.006343
2022-01-09 05:40:49,921 iteration 6037 : loss : 0.015627, loss_ce: 0.005329
2022-01-09 05:40:52,098 iteration 6038 : loss : 0.021787, loss_ce: 0.008571
2022-01-09 05:40:54,345 iteration 6039 : loss : 0.015693, loss_ce: 0.007377
2022-01-09 05:40:56,541 iteration 6040 : loss : 0.013817, loss_ce: 0.006235
2022-01-09 05:40:58,853 iteration 6041 : loss : 0.025574, loss_ce: 0.008508
2022-01-09 05:41:01,196 iteration 6042 : loss : 0.016150, loss_ce: 0.006243
2022-01-09 05:41:03,655 iteration 6043 : loss : 0.010957, loss_ce: 0.004746
2022-01-09 05:41:05,931 iteration 6044 : loss : 0.014492, loss_ce: 0.005368
2022-01-09 05:41:08,326 iteration 6045 : loss : 0.022816, loss_ce: 0.007189
2022-01-09 05:41:10,834 iteration 6046 : loss : 0.013749, loss_ce: 0.003985
2022-01-09 05:41:13,335 iteration 6047 : loss : 0.012660, loss_ce: 0.003036
2022-01-09 05:41:15,795 iteration 6048 : loss : 0.021778, loss_ce: 0.004519
2022-01-09 05:41:18,321 iteration 6049 : loss : 0.015143, loss_ce: 0.006364
2022-01-09 05:41:20,773 iteration 6050 : loss : 0.013631, loss_ce: 0.005379
2022-01-09 05:41:23,183 iteration 6051 : loss : 0.014394, loss_ce: 0.006248
2022-01-09 05:41:25,755 iteration 6052 : loss : 0.016031, loss_ce: 0.004125
 89%|█████████████████████████▊   | 356/400 [5:03:18<34:32, 47.11s/it]2022-01-09 05:41:28,240 iteration 6053 : loss : 0.013198, loss_ce: 0.006277
2022-01-09 05:41:30,855 iteration 6054 : loss : 0.021073, loss_ce: 0.007657
2022-01-09 05:41:33,322 iteration 6055 : loss : 0.011733, loss_ce: 0.004147
2022-01-09 05:41:35,826 iteration 6056 : loss : 0.012092, loss_ce: 0.004526
2022-01-09 05:41:38,286 iteration 6057 : loss : 0.014418, loss_ce: 0.004948
2022-01-09 05:41:40,816 iteration 6058 : loss : 0.014505, loss_ce: 0.006790
2022-01-09 05:41:43,366 iteration 6059 : loss : 0.010884, loss_ce: 0.004194
2022-01-09 05:41:45,961 iteration 6060 : loss : 0.014305, loss_ce: 0.005991
2022-01-09 05:41:48,384 iteration 6061 : loss : 0.028077, loss_ce: 0.014270
2022-01-09 05:41:50,948 iteration 6062 : loss : 0.013507, loss_ce: 0.006886
2022-01-09 05:41:53,512 iteration 6063 : loss : 0.015890, loss_ce: 0.005572
2022-01-09 05:41:56,007 iteration 6064 : loss : 0.029539, loss_ce: 0.006962
2022-01-09 05:41:58,633 iteration 6065 : loss : 0.018639, loss_ce: 0.009640
2022-01-09 05:42:01,107 iteration 6066 : loss : 0.012763, loss_ce: 0.005051
2022-01-09 05:42:03,667 iteration 6067 : loss : 0.028669, loss_ce: 0.004980
2022-01-09 05:42:06,286 iteration 6068 : loss : 0.021408, loss_ce: 0.006818
2022-01-09 05:42:08,740 iteration 6069 : loss : 0.018246, loss_ce: 0.002641
 89%|█████████████████████████▉   | 357/400 [5:04:01<32:52, 45.87s/it]2022-01-09 05:42:11,320 iteration 6070 : loss : 0.013472, loss_ce: 0.006286
2022-01-09 05:42:13,848 iteration 6071 : loss : 0.015061, loss_ce: 0.004156
2022-01-09 05:42:16,406 iteration 6072 : loss : 0.018071, loss_ce: 0.005365
2022-01-09 05:42:18,919 iteration 6073 : loss : 0.022393, loss_ce: 0.008791
2022-01-09 05:42:21,426 iteration 6074 : loss : 0.011257, loss_ce: 0.003290
2022-01-09 05:42:23,994 iteration 6075 : loss : 0.014444, loss_ce: 0.006121
2022-01-09 05:42:26,458 iteration 6076 : loss : 0.015319, loss_ce: 0.006157
2022-01-09 05:42:29,004 iteration 6077 : loss : 0.015953, loss_ce: 0.007022
2022-01-09 05:42:31,498 iteration 6078 : loss : 0.015487, loss_ce: 0.005168
2022-01-09 05:42:33,994 iteration 6079 : loss : 0.019441, loss_ce: 0.004499
2022-01-09 05:42:36,483 iteration 6080 : loss : 0.015786, loss_ce: 0.004946
2022-01-09 05:42:38,907 iteration 6081 : loss : 0.015597, loss_ce: 0.004516
2022-01-09 05:42:41,639 iteration 6082 : loss : 0.020857, loss_ce: 0.008337
2022-01-09 05:42:44,072 iteration 6083 : loss : 0.040329, loss_ce: 0.019286
2022-01-09 05:42:46,528 iteration 6084 : loss : 0.023117, loss_ce: 0.006241
2022-01-09 05:42:49,017 iteration 6085 : loss : 0.015660, loss_ce: 0.007597
2022-01-09 05:42:51,645 iteration 6086 : loss : 0.027713, loss_ce: 0.011002
 90%|█████████████████████████▉   | 358/400 [5:04:44<31:29, 44.98s/it]2022-01-09 05:42:54,223 iteration 6087 : loss : 0.024744, loss_ce: 0.007265
2022-01-09 05:42:56,710 iteration 6088 : loss : 0.015020, loss_ce: 0.004979
2022-01-09 05:42:59,254 iteration 6089 : loss : 0.019620, loss_ce: 0.008985
2022-01-09 05:43:01,848 iteration 6090 : loss : 0.011852, loss_ce: 0.003672
2022-01-09 05:43:04,299 iteration 6091 : loss : 0.011376, loss_ce: 0.003502
2022-01-09 05:43:06,902 iteration 6092 : loss : 0.021320, loss_ce: 0.008611
2022-01-09 05:43:09,558 iteration 6093 : loss : 0.023673, loss_ce: 0.009969
2022-01-09 05:43:12,165 iteration 6094 : loss : 0.016649, loss_ce: 0.007934
2022-01-09 05:43:14,585 iteration 6095 : loss : 0.014823, loss_ce: 0.004094
2022-01-09 05:43:17,141 iteration 6096 : loss : 0.020516, loss_ce: 0.004612
2022-01-09 05:43:19,632 iteration 6097 : loss : 0.013929, loss_ce: 0.006771
2022-01-09 05:43:22,133 iteration 6098 : loss : 0.014459, loss_ce: 0.007465
2022-01-09 05:43:24,622 iteration 6099 : loss : 0.014336, loss_ce: 0.006041
2022-01-09 05:43:27,133 iteration 6100 : loss : 0.015944, loss_ce: 0.004994
2022-01-09 05:43:29,708 iteration 6101 : loss : 0.017523, loss_ce: 0.007132
2022-01-09 05:43:32,145 iteration 6102 : loss : 0.016952, loss_ce: 0.006285
2022-01-09 05:43:34,780 iteration 6103 : loss : 0.015369, loss_ce: 0.004662
 90%|██████████████████████████   | 359/400 [5:05:27<30:21, 44.43s/it]2022-01-09 05:43:37,420 iteration 6104 : loss : 0.014925, loss_ce: 0.006146
2022-01-09 05:43:39,870 iteration 6105 : loss : 0.018353, loss_ce: 0.005091
2022-01-09 05:43:42,356 iteration 6106 : loss : 0.010110, loss_ce: 0.004082
2022-01-09 05:43:44,786 iteration 6107 : loss : 0.011275, loss_ce: 0.004492
2022-01-09 05:43:47,288 iteration 6108 : loss : 0.017531, loss_ce: 0.005004
2022-01-09 05:43:49,811 iteration 6109 : loss : 0.013956, loss_ce: 0.005494
2022-01-09 05:43:52,392 iteration 6110 : loss : 0.013561, loss_ce: 0.006117
2022-01-09 05:43:54,898 iteration 6111 : loss : 0.017981, loss_ce: 0.006086
2022-01-09 05:43:57,487 iteration 6112 : loss : 0.014569, loss_ce: 0.004462
2022-01-09 05:43:59,955 iteration 6113 : loss : 0.018417, loss_ce: 0.008228
2022-01-09 05:44:02,461 iteration 6114 : loss : 0.018038, loss_ce: 0.008859
2022-01-09 05:44:05,064 iteration 6115 : loss : 0.016889, loss_ce: 0.004542
2022-01-09 05:44:07,542 iteration 6116 : loss : 0.014683, loss_ce: 0.005106
2022-01-09 05:44:10,023 iteration 6117 : loss : 0.015185, loss_ce: 0.005761
2022-01-09 05:44:12,531 iteration 6118 : loss : 0.014041, loss_ce: 0.007230
2022-01-09 05:44:15,016 iteration 6119 : loss : 0.012297, loss_ce: 0.005008
2022-01-09 05:44:15,016 Training Data Eval:
2022-01-09 05:44:28,505   Average segmentation loss on training set: 0.0081
2022-01-09 05:44:28,505 Validation Data Eval:
2022-01-09 05:44:33,203   Average segmentation loss on validation set: 0.0669
2022-01-09 05:44:35,917 iteration 6120 : loss : 0.019284, loss_ce: 0.008608
 90%|██████████████████████████   | 360/400 [5:06:29<32:57, 49.45s/it]2022-01-09 05:44:38,342 iteration 6121 : loss : 0.015997, loss_ce: 0.005320
2022-01-09 05:44:41,026 iteration 6122 : loss : 0.014917, loss_ce: 0.006214
2022-01-09 05:44:43,497 iteration 6123 : loss : 0.016466, loss_ce: 0.005357
2022-01-09 05:44:46,010 iteration 6124 : loss : 0.028390, loss_ce: 0.011183
2022-01-09 05:44:48,530 iteration 6125 : loss : 0.015777, loss_ce: 0.004875
2022-01-09 05:44:51,071 iteration 6126 : loss : 0.023995, loss_ce: 0.009607
2022-01-09 05:44:53,708 iteration 6127 : loss : 0.020980, loss_ce: 0.011238
2022-01-09 05:44:56,191 iteration 6128 : loss : 0.012415, loss_ce: 0.004309
2022-01-09 05:44:58,648 iteration 6129 : loss : 0.013268, loss_ce: 0.004356
2022-01-09 05:45:01,188 iteration 6130 : loss : 0.014873, loss_ce: 0.006149
2022-01-09 05:45:03,630 iteration 6131 : loss : 0.015674, loss_ce: 0.004464
2022-01-09 05:45:06,177 iteration 6132 : loss : 0.019614, loss_ce: 0.006481
2022-01-09 05:45:08,760 iteration 6133 : loss : 0.017704, loss_ce: 0.006778
2022-01-09 05:45:11,213 iteration 6134 : loss : 0.012965, loss_ce: 0.004925
2022-01-09 05:45:13,636 iteration 6135 : loss : 0.014929, loss_ce: 0.007377
2022-01-09 05:45:16,267 iteration 6136 : loss : 0.011121, loss_ce: 0.003015
2022-01-09 05:45:18,689 iteration 6137 : loss : 0.022825, loss_ce: 0.006696
 90%|██████████████████████████▏  | 361/400 [5:07:11<30:50, 47.44s/it]2022-01-09 05:45:21,245 iteration 6138 : loss : 0.013838, loss_ce: 0.005103
2022-01-09 05:45:23,678 iteration 6139 : loss : 0.012345, loss_ce: 0.002955
2022-01-09 05:45:26,115 iteration 6140 : loss : 0.018464, loss_ce: 0.005955
2022-01-09 05:45:28,650 iteration 6141 : loss : 0.013380, loss_ce: 0.005997
2022-01-09 05:45:31,032 iteration 6142 : loss : 0.014255, loss_ce: 0.005349
2022-01-09 05:45:33,639 iteration 6143 : loss : 0.015032, loss_ce: 0.006690
2022-01-09 05:45:36,048 iteration 6144 : loss : 0.020528, loss_ce: 0.010328
2022-01-09 05:45:38,564 iteration 6145 : loss : 0.015159, loss_ce: 0.006064
2022-01-09 05:45:41,060 iteration 6146 : loss : 0.012420, loss_ce: 0.003708
2022-01-09 05:45:43,499 iteration 6147 : loss : 0.013164, loss_ce: 0.005046
2022-01-09 05:45:46,023 iteration 6148 : loss : 0.013569, loss_ce: 0.005714
2022-01-09 05:45:48,453 iteration 6149 : loss : 0.011541, loss_ce: 0.004113
2022-01-09 05:45:50,982 iteration 6150 : loss : 0.016259, loss_ce: 0.005708
2022-01-09 05:45:53,468 iteration 6151 : loss : 0.013616, loss_ce: 0.003536
2022-01-09 05:45:55,953 iteration 6152 : loss : 0.013251, loss_ce: 0.005701
2022-01-09 05:45:58,455 iteration 6153 : loss : 0.013799, loss_ce: 0.005719
2022-01-09 05:46:01,027 iteration 6154 : loss : 0.020757, loss_ce: 0.008635
 90%|██████████████████████████▏  | 362/400 [5:07:54<29:04, 45.91s/it]2022-01-09 05:46:03,510 iteration 6155 : loss : 0.022277, loss_ce: 0.010090
2022-01-09 05:46:05,963 iteration 6156 : loss : 0.009953, loss_ce: 0.004268
2022-01-09 05:46:08,435 iteration 6157 : loss : 0.010380, loss_ce: 0.003188
2022-01-09 05:46:11,010 iteration 6158 : loss : 0.014616, loss_ce: 0.004871
2022-01-09 05:46:13,448 iteration 6159 : loss : 0.013747, loss_ce: 0.004518
2022-01-09 05:46:15,920 iteration 6160 : loss : 0.015163, loss_ce: 0.005765
2022-01-09 05:46:18,414 iteration 6161 : loss : 0.012136, loss_ce: 0.003683
2022-01-09 05:46:21,010 iteration 6162 : loss : 0.017635, loss_ce: 0.003926
2022-01-09 05:46:23,498 iteration 6163 : loss : 0.013522, loss_ce: 0.005402
2022-01-09 05:46:26,081 iteration 6164 : loss : 0.021204, loss_ce: 0.010012
2022-01-09 05:46:28,543 iteration 6165 : loss : 0.020253, loss_ce: 0.007883
2022-01-09 05:46:31,079 iteration 6166 : loss : 0.011677, loss_ce: 0.005199
2022-01-09 05:46:33,517 iteration 6167 : loss : 0.013816, loss_ce: 0.005157
2022-01-09 05:46:35,914 iteration 6168 : loss : 0.018640, loss_ce: 0.006803
2022-01-09 05:46:38,480 iteration 6169 : loss : 0.017486, loss_ce: 0.005603
2022-01-09 05:46:40,932 iteration 6170 : loss : 0.012274, loss_ce: 0.004204
2022-01-09 05:46:43,372 iteration 6171 : loss : 0.011314, loss_ce: 0.004222
 91%|██████████████████████████▎  | 363/400 [5:08:36<27:39, 44.84s/it]2022-01-09 05:46:45,881 iteration 6172 : loss : 0.011660, loss_ce: 0.004119
2022-01-09 05:46:48,461 iteration 6173 : loss : 0.016660, loss_ce: 0.007754
2022-01-09 05:46:50,996 iteration 6174 : loss : 0.015579, loss_ce: 0.006107
2022-01-09 05:46:53,400 iteration 6175 : loss : 0.022115, loss_ce: 0.008283
2022-01-09 05:46:55,940 iteration 6176 : loss : 0.013366, loss_ce: 0.004675
2022-01-09 05:46:58,377 iteration 6177 : loss : 0.019154, loss_ce: 0.007378
2022-01-09 05:47:00,874 iteration 6178 : loss : 0.012425, loss_ce: 0.005258
2022-01-09 05:47:03,345 iteration 6179 : loss : 0.012535, loss_ce: 0.005900
2022-01-09 05:47:05,897 iteration 6180 : loss : 0.011178, loss_ce: 0.003661
2022-01-09 05:47:08,286 iteration 6181 : loss : 0.009893, loss_ce: 0.003198
2022-01-09 05:47:10,823 iteration 6182 : loss : 0.017169, loss_ce: 0.004710
2022-01-09 05:47:13,239 iteration 6183 : loss : 0.013761, loss_ce: 0.005417
2022-01-09 05:47:15,739 iteration 6184 : loss : 0.020418, loss_ce: 0.006884
2022-01-09 05:47:18,213 iteration 6185 : loss : 0.014135, loss_ce: 0.004748
2022-01-09 05:47:20,700 iteration 6186 : loss : 0.014071, loss_ce: 0.004498
2022-01-09 05:47:23,241 iteration 6187 : loss : 0.014352, loss_ce: 0.005830
2022-01-09 05:47:25,696 iteration 6188 : loss : 0.012736, loss_ce: 0.004118
 91%|██████████████████████████▍  | 364/400 [5:09:18<26:27, 44.09s/it]2022-01-09 05:47:28,143 iteration 6189 : loss : 0.010649, loss_ce: 0.003148
2022-01-09 05:47:30,545 iteration 6190 : loss : 0.012427, loss_ce: 0.004453
2022-01-09 05:47:33,027 iteration 6191 : loss : 0.021589, loss_ce: 0.007882
2022-01-09 05:47:35,622 iteration 6192 : loss : 0.013048, loss_ce: 0.004592
2022-01-09 05:47:38,065 iteration 6193 : loss : 0.009056, loss_ce: 0.003413
2022-01-09 05:47:40,531 iteration 6194 : loss : 0.012421, loss_ce: 0.004392
2022-01-09 05:47:43,111 iteration 6195 : loss : 0.012591, loss_ce: 0.005165
2022-01-09 05:47:45,484 iteration 6196 : loss : 0.011602, loss_ce: 0.004816
2022-01-09 05:47:47,992 iteration 6197 : loss : 0.015913, loss_ce: 0.007401
2022-01-09 05:47:50,471 iteration 6198 : loss : 0.011015, loss_ce: 0.004165
2022-01-09 05:47:53,010 iteration 6199 : loss : 0.021186, loss_ce: 0.008527
2022-01-09 05:47:55,412 iteration 6200 : loss : 0.008716, loss_ce: 0.003944
2022-01-09 05:47:57,932 iteration 6201 : loss : 0.029338, loss_ce: 0.008890
2022-01-09 05:48:00,351 iteration 6202 : loss : 0.011807, loss_ce: 0.004854
2022-01-09 05:48:02,794 iteration 6203 : loss : 0.012353, loss_ce: 0.003040
2022-01-09 05:48:05,327 iteration 6204 : loss : 0.011797, loss_ce: 0.005206
2022-01-09 05:48:05,327 Training Data Eval:
2022-01-09 05:48:18,589   Average segmentation loss on training set: 0.0078
2022-01-09 05:48:18,589 Validation Data Eval:
2022-01-09 05:48:23,368   Average segmentation loss on validation set: 0.0677
2022-01-09 05:48:25,734 iteration 6205 : loss : 0.018074, loss_ce: 0.006183
 91%|██████████████████████████▍  | 365/400 [5:10:18<28:30, 48.88s/it]2022-01-09 05:48:28,307 iteration 6206 : loss : 0.015051, loss_ce: 0.007327
2022-01-09 05:48:30,757 iteration 6207 : loss : 0.011686, loss_ce: 0.003236
2022-01-09 05:48:33,314 iteration 6208 : loss : 0.022987, loss_ce: 0.008006
2022-01-09 05:48:35,835 iteration 6209 : loss : 0.014944, loss_ce: 0.004014
2022-01-09 05:48:38,271 iteration 6210 : loss : 0.014800, loss_ce: 0.005586
2022-01-09 05:48:40,785 iteration 6211 : loss : 0.016209, loss_ce: 0.007301
2022-01-09 05:48:43,295 iteration 6212 : loss : 0.011393, loss_ce: 0.003528
2022-01-09 05:48:45,742 iteration 6213 : loss : 0.015341, loss_ce: 0.003813
2022-01-09 05:48:48,264 iteration 6214 : loss : 0.013662, loss_ce: 0.002669
2022-01-09 05:48:50,736 iteration 6215 : loss : 0.016541, loss_ce: 0.007970
2022-01-09 05:48:53,091 iteration 6216 : loss : 0.018399, loss_ce: 0.005827
2022-01-09 05:48:55,519 iteration 6217 : loss : 0.012093, loss_ce: 0.004021
2022-01-09 05:48:58,065 iteration 6218 : loss : 0.012270, loss_ce: 0.006079
2022-01-09 05:49:00,493 iteration 6219 : loss : 0.011996, loss_ce: 0.004563
2022-01-09 05:49:03,013 iteration 6220 : loss : 0.014147, loss_ce: 0.004096
2022-01-09 05:49:05,426 iteration 6221 : loss : 0.017926, loss_ce: 0.007275
2022-01-09 05:49:07,917 iteration 6222 : loss : 0.017055, loss_ce: 0.008210
 92%|██████████████████████████▌  | 366/400 [5:11:01<26:33, 46.87s/it]2022-01-09 05:49:10,355 iteration 6223 : loss : 0.050777, loss_ce: 0.023043
2022-01-09 05:49:12,836 iteration 6224 : loss : 0.015393, loss_ce: 0.004493
2022-01-09 05:49:15,300 iteration 6225 : loss : 0.015944, loss_ce: 0.006671
2022-01-09 05:49:17,852 iteration 6226 : loss : 0.015302, loss_ce: 0.006024
2022-01-09 05:49:20,343 iteration 6227 : loss : 0.016555, loss_ce: 0.003864
2022-01-09 05:49:22,841 iteration 6228 : loss : 0.015812, loss_ce: 0.007583
2022-01-09 05:49:25,278 iteration 6229 : loss : 0.011005, loss_ce: 0.004409
2022-01-09 05:49:27,710 iteration 6230 : loss : 0.016035, loss_ce: 0.006212
2022-01-09 05:49:30,193 iteration 6231 : loss : 0.012287, loss_ce: 0.005061
2022-01-09 05:49:32,680 iteration 6232 : loss : 0.014873, loss_ce: 0.006185
2022-01-09 05:49:35,204 iteration 6233 : loss : 0.012200, loss_ce: 0.005158
2022-01-09 05:49:37,694 iteration 6234 : loss : 0.011696, loss_ce: 0.004289
2022-01-09 05:49:40,188 iteration 6235 : loss : 0.039213, loss_ce: 0.009201
2022-01-09 05:49:42,583 iteration 6236 : loss : 0.012412, loss_ce: 0.004741
2022-01-09 05:49:45,044 iteration 6237 : loss : 0.012451, loss_ce: 0.004600
2022-01-09 05:49:47,482 iteration 6238 : loss : 0.007288, loss_ce: 0.002744
2022-01-09 05:49:49,919 iteration 6239 : loss : 0.009814, loss_ce: 0.003934
 92%|██████████████████████████▌  | 367/400 [5:11:43<24:58, 45.41s/it]2022-01-09 05:49:52,565 iteration 6240 : loss : 0.012003, loss_ce: 0.004612
2022-01-09 05:49:54,965 iteration 6241 : loss : 0.021092, loss_ce: 0.010618
2022-01-09 05:49:57,482 iteration 6242 : loss : 0.014709, loss_ce: 0.005388
2022-01-09 05:49:59,978 iteration 6243 : loss : 0.010458, loss_ce: 0.003762
2022-01-09 05:50:02,398 iteration 6244 : loss : 0.011726, loss_ce: 0.003402
2022-01-09 05:50:04,908 iteration 6245 : loss : 0.017171, loss_ce: 0.006188
2022-01-09 05:50:07,353 iteration 6246 : loss : 0.014624, loss_ce: 0.005882
2022-01-09 05:50:09,843 iteration 6247 : loss : 0.014553, loss_ce: 0.003511
2022-01-09 05:50:12,426 iteration 6248 : loss : 0.017873, loss_ce: 0.006745
2022-01-09 05:50:14,866 iteration 6249 : loss : 0.012628, loss_ce: 0.004432
2022-01-09 05:50:17,310 iteration 6250 : loss : 0.016976, loss_ce: 0.006829
2022-01-09 05:50:19,751 iteration 6251 : loss : 0.010944, loss_ce: 0.004023
2022-01-09 05:50:22,269 iteration 6252 : loss : 0.027250, loss_ce: 0.007580
2022-01-09 05:50:24,778 iteration 6253 : loss : 0.014992, loss_ce: 0.005755
2022-01-09 05:50:27,175 iteration 6254 : loss : 0.011970, loss_ce: 0.004492
2022-01-09 05:50:29,752 iteration 6255 : loss : 0.011420, loss_ce: 0.004092
2022-01-09 05:50:32,209 iteration 6256 : loss : 0.011053, loss_ce: 0.003844
 92%|██████████████████████████▋  | 368/400 [5:12:25<23:43, 44.47s/it]2022-01-09 05:50:34,709 iteration 6257 : loss : 0.011697, loss_ce: 0.003762
2022-01-09 05:50:37,136 iteration 6258 : loss : 0.010762, loss_ce: 0.003484
2022-01-09 05:50:39,541 iteration 6259 : loss : 0.018616, loss_ce: 0.006503
2022-01-09 05:50:42,182 iteration 6260 : loss : 0.022405, loss_ce: 0.006117
2022-01-09 05:50:44,682 iteration 6261 : loss : 0.016619, loss_ce: 0.007659
2022-01-09 05:50:47,208 iteration 6262 : loss : 0.018964, loss_ce: 0.007921
2022-01-09 05:50:49,657 iteration 6263 : loss : 0.013388, loss_ce: 0.004441
2022-01-09 05:50:52,089 iteration 6264 : loss : 0.012286, loss_ce: 0.004911
2022-01-09 05:50:54,568 iteration 6265 : loss : 0.013280, loss_ce: 0.005107
2022-01-09 05:50:57,070 iteration 6266 : loss : 0.010394, loss_ce: 0.003554
2022-01-09 05:50:59,582 iteration 6267 : loss : 0.017480, loss_ce: 0.007029
2022-01-09 05:51:02,055 iteration 6268 : loss : 0.024846, loss_ce: 0.013374
2022-01-09 05:51:04,542 iteration 6269 : loss : 0.013874, loss_ce: 0.005184
2022-01-09 05:51:07,023 iteration 6270 : loss : 0.018255, loss_ce: 0.008180
2022-01-09 05:51:09,512 iteration 6271 : loss : 0.015361, loss_ce: 0.006126
2022-01-09 05:51:11,945 iteration 6272 : loss : 0.015965, loss_ce: 0.005269
2022-01-09 05:51:14,555 iteration 6273 : loss : 0.022237, loss_ce: 0.007794
 92%|██████████████████████████▊  | 369/400 [5:13:07<22:38, 43.83s/it]2022-01-09 05:51:17,062 iteration 6274 : loss : 0.017942, loss_ce: 0.006445
2022-01-09 05:51:19,531 iteration 6275 : loss : 0.014452, loss_ce: 0.003827
2022-01-09 05:51:21,935 iteration 6276 : loss : 0.014162, loss_ce: 0.005970
2022-01-09 05:51:24,445 iteration 6277 : loss : 0.013765, loss_ce: 0.005859
2022-01-09 05:51:27,034 iteration 6278 : loss : 0.015880, loss_ce: 0.006382
2022-01-09 05:51:29,428 iteration 6279 : loss : 0.018288, loss_ce: 0.006338
2022-01-09 05:51:31,863 iteration 6280 : loss : 0.014564, loss_ce: 0.005162
2022-01-09 05:51:34,319 iteration 6281 : loss : 0.019457, loss_ce: 0.005710
2022-01-09 05:51:36,792 iteration 6282 : loss : 0.016632, loss_ce: 0.008672
2022-01-09 05:51:39,363 iteration 6283 : loss : 0.016599, loss_ce: 0.006940
2022-01-09 05:51:41,930 iteration 6284 : loss : 0.012959, loss_ce: 0.003891
2022-01-09 05:51:44,388 iteration 6285 : loss : 0.014129, loss_ce: 0.006163
2022-01-09 05:51:46,795 iteration 6286 : loss : 0.013975, loss_ce: 0.007262
2022-01-09 05:51:49,299 iteration 6287 : loss : 0.012854, loss_ce: 0.005165
2022-01-09 05:51:51,706 iteration 6288 : loss : 0.010908, loss_ce: 0.003812
2022-01-09 05:51:54,169 iteration 6289 : loss : 0.014852, loss_ce: 0.006916
2022-01-09 05:51:54,169 Training Data Eval:
2022-01-09 05:52:07,553   Average segmentation loss on training set: 0.0077
2022-01-09 05:52:07,554 Validation Data Eval:
2022-01-09 05:52:12,234   Average segmentation loss on validation set: 0.0625
2022-01-09 05:52:14,748 iteration 6290 : loss : 0.012960, loss_ce: 0.004914
 92%|██████████████████████████▊  | 370/400 [5:14:07<24:22, 48.74s/it]2022-01-09 05:52:17,250 iteration 6291 : loss : 0.016387, loss_ce: 0.006011
2022-01-09 05:52:19,708 iteration 6292 : loss : 0.011582, loss_ce: 0.003517
2022-01-09 05:52:22,140 iteration 6293 : loss : 0.015234, loss_ce: 0.003762
2022-01-09 05:52:24,698 iteration 6294 : loss : 0.019196, loss_ce: 0.006173
2022-01-09 05:52:27,045 iteration 6295 : loss : 0.011192, loss_ce: 0.004755
2022-01-09 05:52:29,497 iteration 6296 : loss : 0.011369, loss_ce: 0.004473
2022-01-09 05:52:32,096 iteration 6297 : loss : 0.015490, loss_ce: 0.007214
2022-01-09 05:52:34,570 iteration 6298 : loss : 0.015444, loss_ce: 0.007284
2022-01-09 05:52:37,082 iteration 6299 : loss : 0.021462, loss_ce: 0.009033
2022-01-09 05:52:39,598 iteration 6300 : loss : 0.023049, loss_ce: 0.006722
2022-01-09 05:52:42,093 iteration 6301 : loss : 0.013153, loss_ce: 0.006246
2022-01-09 05:52:44,669 iteration 6302 : loss : 0.012853, loss_ce: 0.004831
2022-01-09 05:52:47,118 iteration 6303 : loss : 0.012748, loss_ce: 0.004890
2022-01-09 05:52:49,618 iteration 6304 : loss : 0.015838, loss_ce: 0.006246
2022-01-09 05:52:52,079 iteration 6305 : loss : 0.010817, loss_ce: 0.003746
2022-01-09 05:52:54,555 iteration 6306 : loss : 0.016792, loss_ce: 0.008123
2022-01-09 05:52:57,195 iteration 6307 : loss : 0.011872, loss_ce: 0.004085
 93%|██████████████████████████▉  | 371/400 [5:14:50<22:38, 46.86s/it]2022-01-09 05:52:59,757 iteration 6308 : loss : 0.014181, loss_ce: 0.005230
2022-01-09 05:53:02,213 iteration 6309 : loss : 0.021841, loss_ce: 0.005385
2022-01-09 05:53:04,734 iteration 6310 : loss : 0.014783, loss_ce: 0.005023
2022-01-09 05:53:07,130 iteration 6311 : loss : 0.021367, loss_ce: 0.006912
2022-01-09 05:53:09,586 iteration 6312 : loss : 0.017083, loss_ce: 0.006973
2022-01-09 05:53:12,081 iteration 6313 : loss : 0.015017, loss_ce: 0.004043
2022-01-09 05:53:14,609 iteration 6314 : loss : 0.011207, loss_ce: 0.003986
2022-01-09 05:53:17,110 iteration 6315 : loss : 0.013218, loss_ce: 0.005688
2022-01-09 05:53:19,523 iteration 6316 : loss : 0.011744, loss_ce: 0.003433
2022-01-09 05:53:21,951 iteration 6317 : loss : 0.015435, loss_ce: 0.005236
2022-01-09 05:53:24,478 iteration 6318 : loss : 0.016114, loss_ce: 0.006073
2022-01-09 05:53:26,939 iteration 6319 : loss : 0.016223, loss_ce: 0.006179
2022-01-09 05:53:29,532 iteration 6320 : loss : 0.011170, loss_ce: 0.004935
2022-01-09 05:53:32,005 iteration 6321 : loss : 0.014407, loss_ce: 0.005643
2022-01-09 05:53:34,566 iteration 6322 : loss : 0.020084, loss_ce: 0.007698
2022-01-09 05:53:37,152 iteration 6323 : loss : 0.026904, loss_ce: 0.007496
2022-01-09 05:53:39,605 iteration 6324 : loss : 0.015886, loss_ce: 0.006134
 93%|██████████████████████████▉  | 372/400 [5:15:32<21:14, 45.52s/it]2022-01-09 05:53:42,103 iteration 6325 : loss : 0.011625, loss_ce: 0.004887
2022-01-09 05:53:44,534 iteration 6326 : loss : 0.009702, loss_ce: 0.003920
2022-01-09 05:53:47,127 iteration 6327 : loss : 0.014009, loss_ce: 0.005381
2022-01-09 05:53:49,616 iteration 6328 : loss : 0.010329, loss_ce: 0.003484
2022-01-09 05:53:52,037 iteration 6329 : loss : 0.013619, loss_ce: 0.003708
2022-01-09 05:53:54,509 iteration 6330 : loss : 0.011081, loss_ce: 0.003561
2022-01-09 05:53:57,046 iteration 6331 : loss : 0.030625, loss_ce: 0.011076
2022-01-09 05:53:59,479 iteration 6332 : loss : 0.014212, loss_ce: 0.006066
2022-01-09 05:54:01,983 iteration 6333 : loss : 0.012999, loss_ce: 0.003703
2022-01-09 05:54:04,655 iteration 6334 : loss : 0.013712, loss_ce: 0.006073
2022-01-09 05:54:07,237 iteration 6335 : loss : 0.021707, loss_ce: 0.008571
2022-01-09 05:54:09,666 iteration 6336 : loss : 0.008090, loss_ce: 0.002993
2022-01-09 05:54:12,225 iteration 6337 : loss : 0.018641, loss_ce: 0.006524
2022-01-09 05:54:14,805 iteration 6338 : loss : 0.031356, loss_ce: 0.003608
2022-01-09 05:54:17,264 iteration 6339 : loss : 0.015692, loss_ce: 0.006440
2022-01-09 05:54:19,730 iteration 6340 : loss : 0.010870, loss_ce: 0.005541
2022-01-09 05:54:22,222 iteration 6341 : loss : 0.015164, loss_ce: 0.006104
 93%|███████████████████████████  | 373/400 [5:16:15<20:05, 44.65s/it]2022-01-09 05:54:24,802 iteration 6342 : loss : 0.017556, loss_ce: 0.007037
2022-01-09 05:54:27,257 iteration 6343 : loss : 0.023848, loss_ce: 0.006489
2022-01-09 05:54:29,822 iteration 6344 : loss : 0.019759, loss_ce: 0.008501
2022-01-09 05:54:32,300 iteration 6345 : loss : 0.019149, loss_ce: 0.004369
2022-01-09 05:54:34,909 iteration 6346 : loss : 0.011728, loss_ce: 0.004165
2022-01-09 05:54:37,408 iteration 6347 : loss : 0.016940, loss_ce: 0.008740
2022-01-09 05:54:39,902 iteration 6348 : loss : 0.012564, loss_ce: 0.005327
2022-01-09 05:54:42,373 iteration 6349 : loss : 0.018388, loss_ce: 0.006975
2022-01-09 05:54:44,842 iteration 6350 : loss : 0.011567, loss_ce: 0.003874
2022-01-09 05:54:47,335 iteration 6351 : loss : 0.014757, loss_ce: 0.007019
2022-01-09 05:54:50,047 iteration 6352 : loss : 0.014161, loss_ce: 0.005093
2022-01-09 05:54:52,638 iteration 6353 : loss : 0.036399, loss_ce: 0.010918
2022-01-09 05:54:55,121 iteration 6354 : loss : 0.014011, loss_ce: 0.003455
2022-01-09 05:54:57,756 iteration 6355 : loss : 0.013662, loss_ce: 0.004677
2022-01-09 05:55:00,184 iteration 6356 : loss : 0.017378, loss_ce: 0.006150
2022-01-09 05:55:02,693 iteration 6357 : loss : 0.014488, loss_ce: 0.007503
2022-01-09 05:55:05,291 iteration 6358 : loss : 0.019545, loss_ce: 0.006021
 94%|███████████████████████████  | 374/400 [5:16:58<19:08, 44.18s/it]2022-01-09 05:55:07,826 iteration 6359 : loss : 0.012567, loss_ce: 0.003802
2022-01-09 05:55:10,393 iteration 6360 : loss : 0.008677, loss_ce: 0.003035
2022-01-09 05:55:12,826 iteration 6361 : loss : 0.013717, loss_ce: 0.004679
2022-01-09 05:55:15,323 iteration 6362 : loss : 0.013766, loss_ce: 0.005491
2022-01-09 05:55:17,913 iteration 6363 : loss : 0.022736, loss_ce: 0.008052
2022-01-09 05:55:20,502 iteration 6364 : loss : 0.009522, loss_ce: 0.002577
2022-01-09 05:55:22,863 iteration 6365 : loss : 0.018212, loss_ce: 0.007562
2022-01-09 05:55:25,342 iteration 6366 : loss : 0.013438, loss_ce: 0.004423
2022-01-09 05:55:28,074 iteration 6367 : loss : 0.014453, loss_ce: 0.006273
2022-01-09 05:55:30,644 iteration 6368 : loss : 0.018251, loss_ce: 0.004932
2022-01-09 05:55:33,134 iteration 6369 : loss : 0.013972, loss_ce: 0.003026
2022-01-09 05:55:35,649 iteration 6370 : loss : 0.013603, loss_ce: 0.006180
2022-01-09 05:55:38,140 iteration 6371 : loss : 0.010999, loss_ce: 0.004343
2022-01-09 05:55:40,636 iteration 6372 : loss : 0.014845, loss_ce: 0.005240
2022-01-09 05:55:43,090 iteration 6373 : loss : 0.011030, loss_ce: 0.004989
2022-01-09 05:55:45,656 iteration 6374 : loss : 0.010078, loss_ce: 0.003679
2022-01-09 05:55:45,656 Training Data Eval:
2022-01-09 05:55:59,090   Average segmentation loss on training set: 0.0077
2022-01-09 05:55:59,091 Validation Data Eval:
2022-01-09 05:56:03,790   Average segmentation loss on validation set: 0.0646
2022-01-09 05:56:06,410 iteration 6375 : loss : 0.013687, loss_ce: 0.006869
 94%|███████████████████████████▏ | 375/400 [5:17:59<20:31, 49.26s/it]2022-01-09 05:56:09,059 iteration 6376 : loss : 0.023141, loss_ce: 0.008510
2022-01-09 05:56:11,562 iteration 6377 : loss : 0.016112, loss_ce: 0.004291
2022-01-09 05:56:14,025 iteration 6378 : loss : 0.018296, loss_ce: 0.009544
2022-01-09 05:56:16,530 iteration 6379 : loss : 0.012778, loss_ce: 0.005424
2022-01-09 05:56:19,102 iteration 6380 : loss : 0.020139, loss_ce: 0.007271
2022-01-09 05:56:21,530 iteration 6381 : loss : 0.016807, loss_ce: 0.006926
2022-01-09 05:56:24,087 iteration 6382 : loss : 0.018025, loss_ce: 0.009568
2022-01-09 05:56:26,657 iteration 6383 : loss : 0.020899, loss_ce: 0.006761
2022-01-09 05:56:29,264 iteration 6384 : loss : 0.020832, loss_ce: 0.008876
2022-01-09 05:56:31,654 iteration 6385 : loss : 0.008667, loss_ce: 0.003450
2022-01-09 05:56:34,234 iteration 6386 : loss : 0.015715, loss_ce: 0.004904
2022-01-09 05:56:36,753 iteration 6387 : loss : 0.015557, loss_ce: 0.007544
2022-01-09 05:56:39,197 iteration 6388 : loss : 0.010169, loss_ce: 0.004853
2022-01-09 05:56:41,723 iteration 6389 : loss : 0.013147, loss_ce: 0.006501
2022-01-09 05:56:44,419 iteration 6390 : loss : 0.014925, loss_ce: 0.004380
2022-01-09 05:56:46,820 iteration 6391 : loss : 0.018302, loss_ce: 0.007828
2022-01-09 05:56:49,359 iteration 6392 : loss : 0.016632, loss_ce: 0.003616
 94%|███████████████████████████▎ | 376/400 [5:18:42<18:56, 47.36s/it]2022-01-09 05:56:51,888 iteration 6393 : loss : 0.011645, loss_ce: 0.005917
2022-01-09 05:56:54,367 iteration 6394 : loss : 0.011693, loss_ce: 0.005222
2022-01-09 05:56:56,807 iteration 6395 : loss : 0.008643, loss_ce: 0.002563
2022-01-09 05:56:59,314 iteration 6396 : loss : 0.012132, loss_ce: 0.004564
2022-01-09 05:57:01,934 iteration 6397 : loss : 0.017133, loss_ce: 0.005110
2022-01-09 05:57:04,539 iteration 6398 : loss : 0.013952, loss_ce: 0.004474
2022-01-09 05:57:06,953 iteration 6399 : loss : 0.014126, loss_ce: 0.004054
2022-01-09 05:57:09,415 iteration 6400 : loss : 0.009769, loss_ce: 0.003406
2022-01-09 05:57:11,943 iteration 6401 : loss : 0.008651, loss_ce: 0.003052
2022-01-09 05:57:14,521 iteration 6402 : loss : 0.019052, loss_ce: 0.008014
2022-01-09 05:57:16,962 iteration 6403 : loss : 0.014194, loss_ce: 0.005591
2022-01-09 05:57:19,388 iteration 6404 : loss : 0.010999, loss_ce: 0.005538
2022-01-09 05:57:21,987 iteration 6405 : loss : 0.021728, loss_ce: 0.008725
2022-01-09 05:57:24,429 iteration 6406 : loss : 0.010260, loss_ce: 0.003982
2022-01-09 05:57:26,937 iteration 6407 : loss : 0.018986, loss_ce: 0.006526
2022-01-09 05:57:29,461 iteration 6408 : loss : 0.014594, loss_ce: 0.004853
2022-01-09 05:57:31,886 iteration 6409 : loss : 0.009066, loss_ce: 0.002991
 94%|███████████████████████████▎ | 377/400 [5:19:25<17:36, 45.92s/it]2022-01-09 05:57:34,371 iteration 6410 : loss : 0.018052, loss_ce: 0.006046
2022-01-09 05:57:36,945 iteration 6411 : loss : 0.011272, loss_ce: 0.004237
2022-01-09 05:57:39,557 iteration 6412 : loss : 0.012652, loss_ce: 0.004159
2022-01-09 05:57:42,037 iteration 6413 : loss : 0.030485, loss_ce: 0.012515
2022-01-09 05:57:44,638 iteration 6414 : loss : 0.014862, loss_ce: 0.005692
2022-01-09 05:57:47,149 iteration 6415 : loss : 0.012941, loss_ce: 0.004717
2022-01-09 05:57:49,593 iteration 6416 : loss : 0.016356, loss_ce: 0.005916
2022-01-09 05:57:52,162 iteration 6417 : loss : 0.015303, loss_ce: 0.004697
2022-01-09 05:57:54,646 iteration 6418 : loss : 0.012476, loss_ce: 0.005137
2022-01-09 05:57:57,122 iteration 6419 : loss : 0.016099, loss_ce: 0.006922
2022-01-09 05:57:59,811 iteration 6420 : loss : 0.015472, loss_ce: 0.006601
2022-01-09 05:58:02,293 iteration 6421 : loss : 0.014675, loss_ce: 0.005654
2022-01-09 05:58:04,783 iteration 6422 : loss : 0.011738, loss_ce: 0.004657
2022-01-09 05:58:07,340 iteration 6423 : loss : 0.013689, loss_ce: 0.004953
2022-01-09 05:58:09,776 iteration 6424 : loss : 0.014579, loss_ce: 0.004608
2022-01-09 05:58:12,336 iteration 6425 : loss : 0.014260, loss_ce: 0.004615
2022-01-09 05:58:14,753 iteration 6426 : loss : 0.015262, loss_ce: 0.005011
 94%|███████████████████████████▍ | 378/400 [5:20:07<16:30, 45.00s/it]2022-01-09 05:58:17,322 iteration 6427 : loss : 0.013356, loss_ce: 0.004914
2022-01-09 05:58:19,909 iteration 6428 : loss : 0.018382, loss_ce: 0.005991
2022-01-09 05:58:22,375 iteration 6429 : loss : 0.011136, loss_ce: 0.004467
2022-01-09 05:58:24,927 iteration 6430 : loss : 0.013260, loss_ce: 0.005269
2022-01-09 05:58:27,411 iteration 6431 : loss : 0.013761, loss_ce: 0.004149
2022-01-09 05:58:29,894 iteration 6432 : loss : 0.014165, loss_ce: 0.005163
2022-01-09 05:58:32,412 iteration 6433 : loss : 0.011045, loss_ce: 0.002892
2022-01-09 05:58:34,899 iteration 6434 : loss : 0.019751, loss_ce: 0.007601
2022-01-09 05:58:37,313 iteration 6435 : loss : 0.009043, loss_ce: 0.002958
2022-01-09 05:58:39,801 iteration 6436 : loss : 0.011406, loss_ce: 0.006214
2022-01-09 05:58:42,264 iteration 6437 : loss : 0.012504, loss_ce: 0.004526
2022-01-09 05:58:44,659 iteration 6438 : loss : 0.009200, loss_ce: 0.004196
2022-01-09 05:58:47,233 iteration 6439 : loss : 0.016969, loss_ce: 0.005253
2022-01-09 05:58:49,726 iteration 6440 : loss : 0.010870, loss_ce: 0.003705
2022-01-09 05:58:52,216 iteration 6441 : loss : 0.010135, loss_ce: 0.004128
2022-01-09 05:58:54,737 iteration 6442 : loss : 0.014023, loss_ce: 0.007668
2022-01-09 05:58:57,189 iteration 6443 : loss : 0.011959, loss_ce: 0.004079
 95%|███████████████████████████▍ | 379/400 [5:20:50<15:28, 44.23s/it]2022-01-09 05:58:59,695 iteration 6444 : loss : 0.012796, loss_ce: 0.005453
2022-01-09 05:59:02,223 iteration 6445 : loss : 0.009847, loss_ce: 0.003483
2022-01-09 05:59:04,570 iteration 6446 : loss : 0.018479, loss_ce: 0.008357
2022-01-09 05:59:07,065 iteration 6447 : loss : 0.011055, loss_ce: 0.004848
2022-01-09 05:59:09,545 iteration 6448 : loss : 0.020211, loss_ce: 0.003545
2022-01-09 05:59:12,038 iteration 6449 : loss : 0.015273, loss_ce: 0.005042
2022-01-09 05:59:14,610 iteration 6450 : loss : 0.018140, loss_ce: 0.004965
2022-01-09 05:59:17,080 iteration 6451 : loss : 0.014713, loss_ce: 0.005427
2022-01-09 05:59:19,658 iteration 6452 : loss : 0.015519, loss_ce: 0.003440
2022-01-09 05:59:22,118 iteration 6453 : loss : 0.011196, loss_ce: 0.004760
2022-01-09 05:59:24,597 iteration 6454 : loss : 0.013747, loss_ce: 0.004939
2022-01-09 05:59:27,039 iteration 6455 : loss : 0.015495, loss_ce: 0.005606
2022-01-09 05:59:29,534 iteration 6456 : loss : 0.014745, loss_ce: 0.007269
2022-01-09 05:59:32,032 iteration 6457 : loss : 0.010642, loss_ce: 0.004610
2022-01-09 05:59:34,464 iteration 6458 : loss : 0.011221, loss_ce: 0.005942
2022-01-09 05:59:36,994 iteration 6459 : loss : 0.022541, loss_ce: 0.009466
2022-01-09 05:59:36,994 Training Data Eval:
2022-01-09 05:59:50,313   Average segmentation loss on training set: 0.0073
2022-01-09 05:59:50,313 Validation Data Eval:
2022-01-09 05:59:54,912   Average segmentation loss on validation set: 0.0597
2022-01-09 06:00:00,715 Found new lowest validation loss at iteration 6459! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_VALUES_best_val_loss_seed100.pth
2022-01-09 06:00:02,399 iteration 6460 : loss : 0.012738, loss_ce: 0.004329
 95%|███████████████████████████▌ | 380/400 [5:21:55<16:50, 50.52s/it]2022-01-09 06:00:04,211 iteration 6461 : loss : 0.009164, loss_ce: 0.002974
2022-01-09 06:00:06,247 iteration 6462 : loss : 0.010981, loss_ce: 0.004257
2022-01-09 06:00:08,393 iteration 6463 : loss : 0.011394, loss_ce: 0.005103
2022-01-09 06:00:10,508 iteration 6464 : loss : 0.014980, loss_ce: 0.005417
2022-01-09 06:00:12,616 iteration 6465 : loss : 0.010197, loss_ce: 0.003277
2022-01-09 06:00:14,759 iteration 6466 : loss : 0.018903, loss_ce: 0.008020
2022-01-09 06:00:17,089 iteration 6467 : loss : 0.013099, loss_ce: 0.005338
2022-01-09 06:00:19,339 iteration 6468 : loss : 0.020216, loss_ce: 0.005596
2022-01-09 06:00:21,796 iteration 6469 : loss : 0.020894, loss_ce: 0.005551
2022-01-09 06:00:24,253 iteration 6470 : loss : 0.011443, loss_ce: 0.004519
2022-01-09 06:00:26,591 iteration 6471 : loss : 0.013737, loss_ce: 0.005367
2022-01-09 06:00:28,993 iteration 6472 : loss : 0.012759, loss_ce: 0.004461
2022-01-09 06:00:31,576 iteration 6473 : loss : 0.013646, loss_ce: 0.005510
2022-01-09 06:00:33,928 iteration 6474 : loss : 0.011145, loss_ce: 0.004266
2022-01-09 06:00:36,457 iteration 6475 : loss : 0.026269, loss_ce: 0.009302
2022-01-09 06:00:38,994 iteration 6476 : loss : 0.013632, loss_ce: 0.004557
2022-01-09 06:00:41,347 iteration 6477 : loss : 0.014197, loss_ce: 0.006319
 95%|███████████████████████████▌ | 381/400 [5:22:34<14:53, 47.05s/it]2022-01-09 06:00:43,865 iteration 6478 : loss : 0.014232, loss_ce: 0.006366
2022-01-09 06:00:46,401 iteration 6479 : loss : 0.014433, loss_ce: 0.004669
2022-01-09 06:00:48,815 iteration 6480 : loss : 0.011700, loss_ce: 0.004772
2022-01-09 06:00:51,372 iteration 6481 : loss : 0.020892, loss_ce: 0.006722
2022-01-09 06:00:53,881 iteration 6482 : loss : 0.015441, loss_ce: 0.006684
2022-01-09 06:00:56,265 iteration 6483 : loss : 0.014600, loss_ce: 0.005586
2022-01-09 06:00:58,681 iteration 6484 : loss : 0.013497, loss_ce: 0.004355
2022-01-09 06:01:01,162 iteration 6485 : loss : 0.011026, loss_ce: 0.003300
2022-01-09 06:01:03,656 iteration 6486 : loss : 0.012987, loss_ce: 0.004446
2022-01-09 06:01:06,215 iteration 6487 : loss : 0.012517, loss_ce: 0.005486
2022-01-09 06:01:08,619 iteration 6488 : loss : 0.011659, loss_ce: 0.003854
2022-01-09 06:01:11,099 iteration 6489 : loss : 0.015504, loss_ce: 0.003994
2022-01-09 06:01:13,569 iteration 6490 : loss : 0.020451, loss_ce: 0.006786
2022-01-09 06:01:16,031 iteration 6491 : loss : 0.013205, loss_ce: 0.006508
2022-01-09 06:01:18,465 iteration 6492 : loss : 0.011827, loss_ce: 0.007565
2022-01-09 06:01:20,970 iteration 6493 : loss : 0.014357, loss_ce: 0.005276
2022-01-09 06:01:23,473 iteration 6494 : loss : 0.014255, loss_ce: 0.005805
 96%|███████████████████████████▋ | 382/400 [5:23:16<13:40, 45.57s/it]2022-01-09 06:01:25,948 iteration 6495 : loss : 0.012168, loss_ce: 0.005331
2022-01-09 06:01:28,437 iteration 6496 : loss : 0.012730, loss_ce: 0.004163
2022-01-09 06:01:30,995 iteration 6497 : loss : 0.008832, loss_ce: 0.004256
2022-01-09 06:01:33,395 iteration 6498 : loss : 0.012629, loss_ce: 0.003334
2022-01-09 06:01:35,910 iteration 6499 : loss : 0.022615, loss_ce: 0.004856
2022-01-09 06:01:38,356 iteration 6500 : loss : 0.022112, loss_ce: 0.007125
2022-01-09 06:01:40,918 iteration 6501 : loss : 0.014320, loss_ce: 0.006654
2022-01-09 06:01:43,421 iteration 6502 : loss : 0.011553, loss_ce: 0.003979
2022-01-09 06:01:45,917 iteration 6503 : loss : 0.011273, loss_ce: 0.004095
2022-01-09 06:01:48,450 iteration 6504 : loss : 0.013958, loss_ce: 0.006258
2022-01-09 06:01:50,865 iteration 6505 : loss : 0.009014, loss_ce: 0.003428
2022-01-09 06:01:53,366 iteration 6506 : loss : 0.019352, loss_ce: 0.005541
2022-01-09 06:01:55,820 iteration 6507 : loss : 0.012062, loss_ce: 0.004894
2022-01-09 06:01:58,210 iteration 6508 : loss : 0.014547, loss_ce: 0.004739
2022-01-09 06:02:00,739 iteration 6509 : loss : 0.010775, loss_ce: 0.003647
2022-01-09 06:02:03,154 iteration 6510 : loss : 0.008473, loss_ce: 0.003166
2022-01-09 06:02:05,657 iteration 6511 : loss : 0.013635, loss_ce: 0.004949
 96%|███████████████████████████▊ | 383/400 [5:23:58<12:37, 44.56s/it]2022-01-09 06:02:08,271 iteration 6512 : loss : 0.015429, loss_ce: 0.005130
2022-01-09 06:02:10,685 iteration 6513 : loss : 0.013285, loss_ce: 0.005191
2022-01-09 06:02:13,373 iteration 6514 : loss : 0.014751, loss_ce: 0.004594
2022-01-09 06:02:15,979 iteration 6515 : loss : 0.016788, loss_ce: 0.006143
2022-01-09 06:02:18,425 iteration 6516 : loss : 0.011758, loss_ce: 0.004565
2022-01-09 06:02:20,722 iteration 6517 : loss : 0.012595, loss_ce: 0.005622
2022-01-09 06:02:23,259 iteration 6518 : loss : 0.013664, loss_ce: 0.005166
2022-01-09 06:02:25,876 iteration 6519 : loss : 0.030678, loss_ce: 0.015807
2022-01-09 06:02:28,295 iteration 6520 : loss : 0.012013, loss_ce: 0.004685
2022-01-09 06:02:30,792 iteration 6521 : loss : 0.013216, loss_ce: 0.005821
2022-01-09 06:02:33,288 iteration 6522 : loss : 0.018777, loss_ce: 0.006874
2022-01-09 06:02:35,857 iteration 6523 : loss : 0.014019, loss_ce: 0.005594
2022-01-09 06:02:38,368 iteration 6524 : loss : 0.011633, loss_ce: 0.004500
2022-01-09 06:02:40,956 iteration 6525 : loss : 0.025484, loss_ce: 0.009862
2022-01-09 06:02:43,429 iteration 6526 : loss : 0.012350, loss_ce: 0.004654
2022-01-09 06:02:45,940 iteration 6527 : loss : 0.012583, loss_ce: 0.004198
2022-01-09 06:02:48,365 iteration 6528 : loss : 0.009721, loss_ce: 0.003899
 96%|███████████████████████████▊ | 384/400 [5:24:41<11:44, 44.00s/it]2022-01-09 06:02:50,993 iteration 6529 : loss : 0.016654, loss_ce: 0.006553
2022-01-09 06:02:53,341 iteration 6530 : loss : 0.015367, loss_ce: 0.006974
2022-01-09 06:02:55,924 iteration 6531 : loss : 0.019344, loss_ce: 0.006386
2022-01-09 06:02:58,455 iteration 6532 : loss : 0.015826, loss_ce: 0.006881
2022-01-09 06:03:00,872 iteration 6533 : loss : 0.017603, loss_ce: 0.004413
2022-01-09 06:03:03,561 iteration 6534 : loss : 0.022707, loss_ce: 0.005749
2022-01-09 06:03:05,970 iteration 6535 : loss : 0.020126, loss_ce: 0.003557
2022-01-09 06:03:08,492 iteration 6536 : loss : 0.016876, loss_ce: 0.006340
2022-01-09 06:03:10,950 iteration 6537 : loss : 0.015787, loss_ce: 0.006176
2022-01-09 06:03:13,507 iteration 6538 : loss : 0.019936, loss_ce: 0.010077
2022-01-09 06:03:15,936 iteration 6539 : loss : 0.018285, loss_ce: 0.007033
2022-01-09 06:03:18,434 iteration 6540 : loss : 0.013109, loss_ce: 0.005850
2022-01-09 06:03:20,897 iteration 6541 : loss : 0.012011, loss_ce: 0.005035
2022-01-09 06:03:23,436 iteration 6542 : loss : 0.019640, loss_ce: 0.008311
2022-01-09 06:03:25,952 iteration 6543 : loss : 0.018937, loss_ce: 0.006843
2022-01-09 06:03:28,354 iteration 6544 : loss : 0.009393, loss_ce: 0.003156
2022-01-09 06:03:28,354 Training Data Eval:
2022-01-09 06:03:41,712   Average segmentation loss on training set: 0.0072
2022-01-09 06:03:41,712 Validation Data Eval:
2022-01-09 06:03:46,372   Average segmentation loss on validation set: 0.0641
2022-01-09 06:03:48,640 iteration 6545 : loss : 0.008605, loss_ce: 0.001891
 96%|███████████████████████████▉ | 385/400 [5:25:41<12:13, 48.88s/it]2022-01-09 06:03:51,385 iteration 6546 : loss : 0.017743, loss_ce: 0.006055
2022-01-09 06:03:53,908 iteration 6547 : loss : 0.022194, loss_ce: 0.007802
2022-01-09 06:03:56,320 iteration 6548 : loss : 0.014218, loss_ce: 0.009283
2022-01-09 06:03:58,870 iteration 6549 : loss : 0.007401, loss_ce: 0.002496
2022-01-09 06:04:01,310 iteration 6550 : loss : 0.009738, loss_ce: 0.003924
2022-01-09 06:04:03,846 iteration 6551 : loss : 0.013319, loss_ce: 0.005782
2022-01-09 06:04:06,232 iteration 6552 : loss : 0.010793, loss_ce: 0.005162
2022-01-09 06:04:08,841 iteration 6553 : loss : 0.017871, loss_ce: 0.006305
2022-01-09 06:04:11,305 iteration 6554 : loss : 0.014435, loss_ce: 0.004535
2022-01-09 06:04:13,815 iteration 6555 : loss : 0.015046, loss_ce: 0.005766
2022-01-09 06:04:16,331 iteration 6556 : loss : 0.023999, loss_ce: 0.005603
2022-01-09 06:04:18,787 iteration 6557 : loss : 0.012194, loss_ce: 0.003732
2022-01-09 06:04:21,221 iteration 6558 : loss : 0.012695, loss_ce: 0.003566
2022-01-09 06:04:23,788 iteration 6559 : loss : 0.014614, loss_ce: 0.006623
2022-01-09 06:04:26,413 iteration 6560 : loss : 0.033341, loss_ce: 0.007045
2022-01-09 06:04:28,831 iteration 6561 : loss : 0.013113, loss_ce: 0.005175
2022-01-09 06:04:31,215 iteration 6562 : loss : 0.008113, loss_ce: 0.002532
 96%|███████████████████████████▉ | 386/400 [5:26:24<10:57, 46.99s/it]2022-01-09 06:04:33,740 iteration 6563 : loss : 0.011371, loss_ce: 0.004940
2022-01-09 06:04:36,197 iteration 6564 : loss : 0.016271, loss_ce: 0.004349
2022-01-09 06:04:38,641 iteration 6565 : loss : 0.012313, loss_ce: 0.003982
2022-01-09 06:04:41,213 iteration 6566 : loss : 0.011252, loss_ce: 0.004241
2022-01-09 06:04:43,759 iteration 6567 : loss : 0.014290, loss_ce: 0.005404
2022-01-09 06:04:46,367 iteration 6568 : loss : 0.019223, loss_ce: 0.007011
2022-01-09 06:04:48,770 iteration 6569 : loss : 0.020790, loss_ce: 0.005856
2022-01-09 06:04:51,249 iteration 6570 : loss : 0.015808, loss_ce: 0.003855
2022-01-09 06:04:53,864 iteration 6571 : loss : 0.017830, loss_ce: 0.008069
2022-01-09 06:04:56,424 iteration 6572 : loss : 0.009654, loss_ce: 0.003959
2022-01-09 06:04:58,834 iteration 6573 : loss : 0.024427, loss_ce: 0.007469
2022-01-09 06:05:01,288 iteration 6574 : loss : 0.009000, loss_ce: 0.002546
2022-01-09 06:05:03,843 iteration 6575 : loss : 0.019396, loss_ce: 0.006435
2022-01-09 06:05:06,341 iteration 6576 : loss : 0.012921, loss_ce: 0.005460
2022-01-09 06:05:08,801 iteration 6577 : loss : 0.014437, loss_ce: 0.004818
2022-01-09 06:05:11,314 iteration 6578 : loss : 0.009920, loss_ce: 0.003620
2022-01-09 06:05:13,794 iteration 6579 : loss : 0.013899, loss_ce: 0.005919
 97%|████████████████████████████ | 387/400 [5:27:06<09:53, 45.66s/it]2022-01-09 06:05:16,363 iteration 6580 : loss : 0.014909, loss_ce: 0.006417
2022-01-09 06:05:18,801 iteration 6581 : loss : 0.010664, loss_ce: 0.003084
2022-01-09 06:05:21,377 iteration 6582 : loss : 0.017381, loss_ce: 0.005574
2022-01-09 06:05:23,876 iteration 6583 : loss : 0.025485, loss_ce: 0.009237
2022-01-09 06:05:26,379 iteration 6584 : loss : 0.016260, loss_ce: 0.005941
2022-01-09 06:05:28,810 iteration 6585 : loss : 0.009815, loss_ce: 0.002957
2022-01-09 06:05:31,249 iteration 6586 : loss : 0.011528, loss_ce: 0.002460
2022-01-09 06:05:33,711 iteration 6587 : loss : 0.010521, loss_ce: 0.004800
2022-01-09 06:05:36,212 iteration 6588 : loss : 0.010098, loss_ce: 0.004570
2022-01-09 06:05:38,707 iteration 6589 : loss : 0.014423, loss_ce: 0.005606
2022-01-09 06:05:41,158 iteration 6590 : loss : 0.010688, loss_ce: 0.004687
2022-01-09 06:05:43,635 iteration 6591 : loss : 0.009932, loss_ce: 0.003478
2022-01-09 06:05:46,237 iteration 6592 : loss : 0.012951, loss_ce: 0.006765
2022-01-09 06:05:48,676 iteration 6593 : loss : 0.009684, loss_ce: 0.004985
2022-01-09 06:05:51,124 iteration 6594 : loss : 0.012211, loss_ce: 0.004636
2022-01-09 06:05:53,701 iteration 6595 : loss : 0.012898, loss_ce: 0.003593
2022-01-09 06:05:56,122 iteration 6596 : loss : 0.009997, loss_ce: 0.002825
 97%|████████████████████████████▏| 388/400 [5:27:49<08:56, 44.67s/it]2022-01-09 06:05:58,686 iteration 6597 : loss : 0.012339, loss_ce: 0.007449
2022-01-09 06:06:01,213 iteration 6598 : loss : 0.013599, loss_ce: 0.002532
2022-01-09 06:06:03,759 iteration 6599 : loss : 0.014538, loss_ce: 0.005403
2022-01-09 06:06:06,239 iteration 6600 : loss : 0.011501, loss_ce: 0.004907
2022-01-09 06:06:08,833 iteration 6601 : loss : 0.012670, loss_ce: 0.004912
2022-01-09 06:06:11,360 iteration 6602 : loss : 0.014337, loss_ce: 0.005406
2022-01-09 06:06:13,846 iteration 6603 : loss : 0.011367, loss_ce: 0.004916
2022-01-09 06:06:16,429 iteration 6604 : loss : 0.010904, loss_ce: 0.005093
2022-01-09 06:06:18,893 iteration 6605 : loss : 0.013441, loss_ce: 0.005877
2022-01-09 06:06:21,403 iteration 6606 : loss : 0.012793, loss_ce: 0.004278
2022-01-09 06:06:23,962 iteration 6607 : loss : 0.018028, loss_ce: 0.007195
2022-01-09 06:06:26,446 iteration 6608 : loss : 0.012405, loss_ce: 0.005137
2022-01-09 06:06:29,004 iteration 6609 : loss : 0.014310, loss_ce: 0.006429
2022-01-09 06:06:31,474 iteration 6610 : loss : 0.011303, loss_ce: 0.004167
2022-01-09 06:06:33,934 iteration 6611 : loss : 0.009091, loss_ce: 0.002406
2022-01-09 06:06:36,469 iteration 6612 : loss : 0.011606, loss_ce: 0.003121
2022-01-09 06:06:39,041 iteration 6613 : loss : 0.011684, loss_ce: 0.004035
 97%|████████████████████████████▏| 389/400 [5:28:32<08:05, 44.14s/it]2022-01-09 06:06:41,541 iteration 6614 : loss : 0.009071, loss_ce: 0.003397
2022-01-09 06:06:44,018 iteration 6615 : loss : 0.007514, loss_ce: 0.002798
2022-01-09 06:06:46,531 iteration 6616 : loss : 0.010659, loss_ce: 0.003534
2022-01-09 06:06:48,993 iteration 6617 : loss : 0.022695, loss_ce: 0.007794
2022-01-09 06:06:51,575 iteration 6618 : loss : 0.014976, loss_ce: 0.005409
2022-01-09 06:06:54,072 iteration 6619 : loss : 0.015153, loss_ce: 0.005245
2022-01-09 06:06:56,600 iteration 6620 : loss : 0.013044, loss_ce: 0.004315
2022-01-09 06:06:59,156 iteration 6621 : loss : 0.014094, loss_ce: 0.004688
2022-01-09 06:07:01,612 iteration 6622 : loss : 0.012783, loss_ce: 0.005684
2022-01-09 06:07:04,099 iteration 6623 : loss : 0.011084, loss_ce: 0.003357
2022-01-09 06:07:06,582 iteration 6624 : loss : 0.012270, loss_ce: 0.004626
2022-01-09 06:07:09,093 iteration 6625 : loss : 0.015226, loss_ce: 0.005921
2022-01-09 06:07:11,573 iteration 6626 : loss : 0.014900, loss_ce: 0.005982
2022-01-09 06:07:14,154 iteration 6627 : loss : 0.018997, loss_ce: 0.006836
2022-01-09 06:07:16,663 iteration 6628 : loss : 0.020068, loss_ce: 0.007116
2022-01-09 06:07:19,271 iteration 6629 : loss : 0.019489, loss_ce: 0.004955
2022-01-09 06:07:19,271 Training Data Eval:
2022-01-09 06:07:33,008   Average segmentation loss on training set: 0.0070
2022-01-09 06:07:33,008 Validation Data Eval:
2022-01-09 06:07:37,731   Average segmentation loss on validation set: 0.0718
2022-01-09 06:07:40,229 iteration 6630 : loss : 0.015089, loss_ce: 0.005201
 98%|████████████████████████████▎| 390/400 [5:29:33<08:12, 49.25s/it]2022-01-09 06:07:42,763 iteration 6631 : loss : 0.017054, loss_ce: 0.005411
2022-01-09 06:07:45,248 iteration 6632 : loss : 0.010660, loss_ce: 0.005225
2022-01-09 06:07:47,961 iteration 6633 : loss : 0.014906, loss_ce: 0.003738
2022-01-09 06:07:50,428 iteration 6634 : loss : 0.012096, loss_ce: 0.004575
2022-01-09 06:07:52,906 iteration 6635 : loss : 0.012624, loss_ce: 0.004920
2022-01-09 06:07:55,453 iteration 6636 : loss : 0.011733, loss_ce: 0.005152
2022-01-09 06:07:57,916 iteration 6637 : loss : 0.016362, loss_ce: 0.005423
2022-01-09 06:08:00,520 iteration 6638 : loss : 0.016412, loss_ce: 0.002935
2022-01-09 06:08:03,054 iteration 6639 : loss : 0.014706, loss_ce: 0.005604
2022-01-09 06:08:05,530 iteration 6640 : loss : 0.012351, loss_ce: 0.003747
2022-01-09 06:08:08,235 iteration 6641 : loss : 0.012139, loss_ce: 0.003788
2022-01-09 06:08:10,723 iteration 6642 : loss : 0.020101, loss_ce: 0.009178
2022-01-09 06:08:13,263 iteration 6643 : loss : 0.013008, loss_ce: 0.004683
2022-01-09 06:08:15,896 iteration 6644 : loss : 0.018457, loss_ce: 0.007569
2022-01-09 06:08:18,361 iteration 6645 : loss : 0.010613, loss_ce: 0.004437
2022-01-09 06:08:20,959 iteration 6646 : loss : 0.024296, loss_ce: 0.007758
2022-01-09 06:08:23,566 iteration 6647 : loss : 0.014273, loss_ce: 0.005410
 98%|████████████████████████████▎| 391/400 [5:30:16<07:07, 47.48s/it]2022-01-09 06:08:26,147 iteration 6648 : loss : 0.013323, loss_ce: 0.003238
2022-01-09 06:08:28,732 iteration 6649 : loss : 0.014987, loss_ce: 0.004181
2022-01-09 06:08:31,361 iteration 6650 : loss : 0.014788, loss_ce: 0.005285
2022-01-09 06:08:34,042 iteration 6651 : loss : 0.016437, loss_ce: 0.007271
2022-01-09 06:08:36,513 iteration 6652 : loss : 0.012684, loss_ce: 0.005916
2022-01-09 06:08:38,927 iteration 6653 : loss : 0.009293, loss_ce: 0.003726
2022-01-09 06:08:41,512 iteration 6654 : loss : 0.014156, loss_ce: 0.004821
2022-01-09 06:08:44,040 iteration 6655 : loss : 0.014206, loss_ce: 0.004638
2022-01-09 06:08:46,554 iteration 6656 : loss : 0.021446, loss_ce: 0.008418
2022-01-09 06:08:49,234 iteration 6657 : loss : 0.011382, loss_ce: 0.003225
2022-01-09 06:08:51,632 iteration 6658 : loss : 0.014426, loss_ce: 0.005497
2022-01-09 06:08:54,321 iteration 6659 : loss : 0.015338, loss_ce: 0.007397
2022-01-09 06:08:56,865 iteration 6660 : loss : 0.014900, loss_ce: 0.006441
2022-01-09 06:08:59,387 iteration 6661 : loss : 0.014544, loss_ce: 0.005971
2022-01-09 06:09:01,967 iteration 6662 : loss : 0.027636, loss_ce: 0.007556
2022-01-09 06:09:04,468 iteration 6663 : loss : 0.015431, loss_ce: 0.005663
2022-01-09 06:09:06,998 iteration 6664 : loss : 0.021556, loss_ce: 0.011124
 98%|████████████████████████████▍| 392/400 [5:31:00<06:10, 46.26s/it]2022-01-09 06:09:09,576 iteration 6665 : loss : 0.013848, loss_ce: 0.004254
2022-01-09 06:09:12,215 iteration 6666 : loss : 0.026863, loss_ce: 0.007252
2022-01-09 06:09:14,808 iteration 6667 : loss : 0.033260, loss_ce: 0.005418
2022-01-09 06:09:17,225 iteration 6668 : loss : 0.013112, loss_ce: 0.006322
2022-01-09 06:09:19,707 iteration 6669 : loss : 0.012459, loss_ce: 0.004623
2022-01-09 06:09:22,189 iteration 6670 : loss : 0.012043, loss_ce: 0.005519
2022-01-09 06:09:24,697 iteration 6671 : loss : 0.015034, loss_ce: 0.005995
2022-01-09 06:09:27,234 iteration 6672 : loss : 0.012057, loss_ce: 0.003390
2022-01-09 06:09:29,844 iteration 6673 : loss : 0.027967, loss_ce: 0.006125
2022-01-09 06:09:32,333 iteration 6674 : loss : 0.012369, loss_ce: 0.005754
2022-01-09 06:09:34,897 iteration 6675 : loss : 0.026527, loss_ce: 0.015162
2022-01-09 06:09:37,354 iteration 6676 : loss : 0.012893, loss_ce: 0.005171
2022-01-09 06:09:39,842 iteration 6677 : loss : 0.012029, loss_ce: 0.004219
2022-01-09 06:09:42,322 iteration 6678 : loss : 0.012029, loss_ce: 0.005432
2022-01-09 06:09:45,015 iteration 6679 : loss : 0.014746, loss_ce: 0.006524
2022-01-09 06:09:47,436 iteration 6680 : loss : 0.010029, loss_ce: 0.003679
2022-01-09 06:09:49,878 iteration 6681 : loss : 0.009289, loss_ce: 0.003054
 98%|████████████████████████████▍| 393/400 [5:31:43<05:16, 45.25s/it]2022-01-09 06:09:52,424 iteration 6682 : loss : 0.012776, loss_ce: 0.005626
2022-01-09 06:09:54,865 iteration 6683 : loss : 0.015209, loss_ce: 0.005676
2022-01-09 06:09:57,355 iteration 6684 : loss : 0.010438, loss_ce: 0.004596
2022-01-09 06:09:59,849 iteration 6685 : loss : 0.013922, loss_ce: 0.004800
2022-01-09 06:10:02,385 iteration 6686 : loss : 0.015077, loss_ce: 0.006705
2022-01-09 06:10:04,890 iteration 6687 : loss : 0.013886, loss_ce: 0.005124
2022-01-09 06:10:07,380 iteration 6688 : loss : 0.014544, loss_ce: 0.005008
2022-01-09 06:10:09,852 iteration 6689 : loss : 0.007498, loss_ce: 0.001872
2022-01-09 06:10:12,336 iteration 6690 : loss : 0.008017, loss_ce: 0.002241
2022-01-09 06:10:14,899 iteration 6691 : loss : 0.015327, loss_ce: 0.004710
2022-01-09 06:10:17,349 iteration 6692 : loss : 0.014820, loss_ce: 0.005175
2022-01-09 06:10:19,845 iteration 6693 : loss : 0.018717, loss_ce: 0.006062
2022-01-09 06:10:22,346 iteration 6694 : loss : 0.014961, loss_ce: 0.005619
2022-01-09 06:10:24,799 iteration 6695 : loss : 0.012533, loss_ce: 0.004138
2022-01-09 06:10:27,273 iteration 6696 : loss : 0.009855, loss_ce: 0.003851
2022-01-09 06:10:29,827 iteration 6697 : loss : 0.011309, loss_ce: 0.003944
2022-01-09 06:10:32,442 iteration 6698 : loss : 0.014646, loss_ce: 0.004618
 98%|████████████████████████████▌| 394/400 [5:32:25<04:26, 44.44s/it]2022-01-09 06:10:34,932 iteration 6699 : loss : 0.013241, loss_ce: 0.004836
2022-01-09 06:10:37,539 iteration 6700 : loss : 0.016328, loss_ce: 0.005413
2022-01-09 06:10:40,018 iteration 6701 : loss : 0.010115, loss_ce: 0.003532
2022-01-09 06:10:42,433 iteration 6702 : loss : 0.009695, loss_ce: 0.004612
2022-01-09 06:10:44,989 iteration 6703 : loss : 0.011141, loss_ce: 0.003718
2022-01-09 06:10:47,504 iteration 6704 : loss : 0.016719, loss_ce: 0.008476
2022-01-09 06:10:49,952 iteration 6705 : loss : 0.013535, loss_ce: 0.006172
2022-01-09 06:10:52,446 iteration 6706 : loss : 0.013988, loss_ce: 0.004483
2022-01-09 06:10:54,869 iteration 6707 : loss : 0.009665, loss_ce: 0.003877
2022-01-09 06:10:57,363 iteration 6708 : loss : 0.013593, loss_ce: 0.005046
2022-01-09 06:10:59,973 iteration 6709 : loss : 0.016529, loss_ce: 0.007234
2022-01-09 06:11:02,554 iteration 6710 : loss : 0.017108, loss_ce: 0.003799
2022-01-09 06:11:04,948 iteration 6711 : loss : 0.017537, loss_ce: 0.005716
2022-01-09 06:11:07,595 iteration 6712 : loss : 0.015386, loss_ce: 0.006114
2022-01-09 06:11:10,025 iteration 6713 : loss : 0.008914, loss_ce: 0.003195
2022-01-09 06:11:12,514 iteration 6714 : loss : 0.014174, loss_ce: 0.005578
2022-01-09 06:11:12,514 Training Data Eval:
2022-01-09 06:11:25,987   Average segmentation loss on training set: 0.0072
2022-01-09 06:11:25,987 Validation Data Eval:
2022-01-09 06:11:30,671   Average segmentation loss on validation set: 0.0643
2022-01-09 06:11:33,136 iteration 6715 : loss : 0.012809, loss_ce: 0.004842
 99%|████████████████████████████▋| 395/400 [5:33:26<04:06, 49.32s/it]2022-01-09 06:11:35,666 iteration 6716 : loss : 0.013049, loss_ce: 0.005719
2022-01-09 06:11:38,160 iteration 6717 : loss : 0.011927, loss_ce: 0.004790
2022-01-09 06:11:40,514 iteration 6718 : loss : 0.014482, loss_ce: 0.005788
2022-01-09 06:11:42,985 iteration 6719 : loss : 0.015172, loss_ce: 0.007058
2022-01-09 06:11:45,443 iteration 6720 : loss : 0.011499, loss_ce: 0.004208
2022-01-09 06:11:47,868 iteration 6721 : loss : 0.009249, loss_ce: 0.002947
2022-01-09 06:11:50,498 iteration 6722 : loss : 0.018825, loss_ce: 0.007622
2022-01-09 06:11:52,889 iteration 6723 : loss : 0.009500, loss_ce: 0.002868
2022-01-09 06:11:55,363 iteration 6724 : loss : 0.015586, loss_ce: 0.005069
2022-01-09 06:11:57,877 iteration 6725 : loss : 0.011058, loss_ce: 0.003468
2022-01-09 06:12:00,456 iteration 6726 : loss : 0.019264, loss_ce: 0.008165
2022-01-09 06:12:02,882 iteration 6727 : loss : 0.026386, loss_ce: 0.010325
2022-01-09 06:12:05,366 iteration 6728 : loss : 0.013604, loss_ce: 0.005066
2022-01-09 06:12:07,859 iteration 6729 : loss : 0.017730, loss_ce: 0.004840
2022-01-09 06:12:10,294 iteration 6730 : loss : 0.008672, loss_ce: 0.003056
2022-01-09 06:12:12,836 iteration 6731 : loss : 0.012284, loss_ce: 0.004591
2022-01-09 06:12:15,215 iteration 6732 : loss : 0.013016, loss_ce: 0.004812
 99%|████████████████████████████▋| 396/400 [5:34:08<03:08, 47.15s/it]2022-01-09 06:12:17,721 iteration 6733 : loss : 0.010981, loss_ce: 0.003969
2022-01-09 06:12:20,231 iteration 6734 : loss : 0.017774, loss_ce: 0.006428
2022-01-09 06:12:22,701 iteration 6735 : loss : 0.009351, loss_ce: 0.003754
2022-01-09 06:12:25,275 iteration 6736 : loss : 0.015980, loss_ce: 0.007153
2022-01-09 06:12:27,644 iteration 6737 : loss : 0.014645, loss_ce: 0.005349
2022-01-09 06:12:30,101 iteration 6738 : loss : 0.013372, loss_ce: 0.004045
2022-01-09 06:12:32,548 iteration 6739 : loss : 0.013695, loss_ce: 0.006087
2022-01-09 06:12:35,115 iteration 6740 : loss : 0.009729, loss_ce: 0.004225
2022-01-09 06:12:37,565 iteration 6741 : loss : 0.009001, loss_ce: 0.003131
2022-01-09 06:12:40,006 iteration 6742 : loss : 0.013935, loss_ce: 0.004889
2022-01-09 06:12:42,549 iteration 6743 : loss : 0.016332, loss_ce: 0.007097
2022-01-09 06:12:44,910 iteration 6744 : loss : 0.015471, loss_ce: 0.006460
2022-01-09 06:12:47,345 iteration 6745 : loss : 0.015248, loss_ce: 0.005582
2022-01-09 06:12:49,836 iteration 6746 : loss : 0.010574, loss_ce: 0.004227
2022-01-09 06:12:52,324 iteration 6747 : loss : 0.015239, loss_ce: 0.004514
2022-01-09 06:12:54,800 iteration 6748 : loss : 0.014921, loss_ce: 0.004834
2022-01-09 06:12:57,321 iteration 6749 : loss : 0.011559, loss_ce: 0.004820
 99%|████████████████████████████▊| 397/400 [5:34:50<02:16, 45.63s/it]2022-01-09 06:12:59,754 iteration 6750 : loss : 0.010666, loss_ce: 0.003725
2022-01-09 06:13:02,319 iteration 6751 : loss : 0.021622, loss_ce: 0.007700
2022-01-09 06:13:04,822 iteration 6752 : loss : 0.016571, loss_ce: 0.005769
2022-01-09 06:13:07,331 iteration 6753 : loss : 0.016550, loss_ce: 0.006384
2022-01-09 06:13:09,749 iteration 6754 : loss : 0.012051, loss_ce: 0.005268
2022-01-09 06:13:12,225 iteration 6755 : loss : 0.009110, loss_ce: 0.002324
2022-01-09 06:13:14,612 iteration 6756 : loss : 0.008449, loss_ce: 0.003903
2022-01-09 06:13:17,070 iteration 6757 : loss : 0.009940, loss_ce: 0.004180
2022-01-09 06:13:19,521 iteration 6758 : loss : 0.009554, loss_ce: 0.004716
2022-01-09 06:13:22,040 iteration 6759 : loss : 0.017246, loss_ce: 0.005145
2022-01-09 06:13:24,550 iteration 6760 : loss : 0.010313, loss_ce: 0.003902
2022-01-09 06:13:27,007 iteration 6761 : loss : 0.020494, loss_ce: 0.009338
2022-01-09 06:13:29,373 iteration 6762 : loss : 0.015890, loss_ce: 0.004312
2022-01-09 06:13:31,851 iteration 6763 : loss : 0.013107, loss_ce: 0.005862
2022-01-09 06:13:34,276 iteration 6764 : loss : 0.013897, loss_ce: 0.004657
2022-01-09 06:13:36,784 iteration 6765 : loss : 0.014494, loss_ce: 0.003739
2022-01-09 06:13:39,294 iteration 6766 : loss : 0.010911, loss_ce: 0.004754
100%|████████████████████████████▊| 398/400 [5:35:32<01:29, 44.53s/it]2022-01-09 06:13:41,756 iteration 6767 : loss : 0.017461, loss_ce: 0.007130
2022-01-09 06:13:44,239 iteration 6768 : loss : 0.011295, loss_ce: 0.003299
2022-01-09 06:13:46,698 iteration 6769 : loss : 0.010103, loss_ce: 0.003271
2022-01-09 06:13:49,210 iteration 6770 : loss : 0.021092, loss_ce: 0.006977
2022-01-09 06:13:51,665 iteration 6771 : loss : 0.011813, loss_ce: 0.005074
2022-01-09 06:13:54,160 iteration 6772 : loss : 0.017653, loss_ce: 0.006853
2022-01-09 06:13:56,586 iteration 6773 : loss : 0.008800, loss_ce: 0.003308
2022-01-09 06:13:59,046 iteration 6774 : loss : 0.020392, loss_ce: 0.006194
2022-01-09 06:14:01,535 iteration 6775 : loss : 0.013364, loss_ce: 0.002637
2022-01-09 06:14:03,995 iteration 6776 : loss : 0.015947, loss_ce: 0.006809
2022-01-09 06:14:06,480 iteration 6777 : loss : 0.018631, loss_ce: 0.007538
2022-01-09 06:14:08,909 iteration 6778 : loss : 0.011144, loss_ce: 0.004133
2022-01-09 06:14:11,383 iteration 6779 : loss : 0.012814, loss_ce: 0.006012
2022-01-09 06:14:13,931 iteration 6780 : loss : 0.010512, loss_ce: 0.003706
2022-01-09 06:14:16,290 iteration 6781 : loss : 0.012894, loss_ce: 0.004892
2022-01-09 06:14:18,791 iteration 6782 : loss : 0.013066, loss_ce: 0.005182
2022-01-09 06:14:21,319 iteration 6783 : loss : 0.018180, loss_ce: 0.005984
100%|████████████████████████████▉| 399/400 [5:36:14<00:43, 43.78s/it]2022-01-09 06:14:23,768 iteration 6784 : loss : 0.011545, loss_ce: 0.006136
2022-01-09 06:14:26,228 iteration 6785 : loss : 0.012476, loss_ce: 0.004144
2022-01-09 06:14:28,743 iteration 6786 : loss : 0.011942, loss_ce: 0.004458
2022-01-09 06:14:31,216 iteration 6787 : loss : 0.012476, loss_ce: 0.003290
2022-01-09 06:14:33,692 iteration 6788 : loss : 0.010522, loss_ce: 0.004809
2022-01-09 06:14:36,260 iteration 6789 : loss : 0.020014, loss_ce: 0.006249
2022-01-09 06:14:38,734 iteration 6790 : loss : 0.011806, loss_ce: 0.004647
2022-01-09 06:14:41,330 iteration 6791 : loss : 0.018885, loss_ce: 0.005536
2022-01-09 06:14:43,793 iteration 6792 : loss : 0.011719, loss_ce: 0.004829
2022-01-09 06:14:46,191 iteration 6793 : loss : 0.009706, loss_ce: 0.003680
2022-01-09 06:14:48,694 iteration 6794 : loss : 0.018802, loss_ce: 0.007536
2022-01-09 06:14:51,182 iteration 6795 : loss : 0.011794, loss_ce: 0.005146
2022-01-09 06:14:53,647 iteration 6796 : loss : 0.012964, loss_ce: 0.003316
2022-01-09 06:14:56,273 iteration 6797 : loss : 0.013878, loss_ce: 0.004355
2022-01-09 06:14:58,865 iteration 6798 : loss : 0.014037, loss_ce: 0.004506
2022-01-09 06:15:01,342 iteration 6799 : loss : 0.012368, loss_ce: 0.003772
2022-01-09 06:15:01,342 Training Data Eval:
2022-01-09 06:15:14,787   Average segmentation loss on training set: 0.0068
2022-01-09 06:15:14,788 Validation Data Eval:
2022-01-09 06:15:19,495   Average segmentation loss on validation set: 0.0676
2022-01-09 06:15:22,050 iteration 6800 : loss : 0.011966, loss_ce: 0.004772
100%|█████████████████████████████| 400/400 [5:37:15<00:00, 48.87s/it]100%|█████████████████████████████| 400/400 [5:37:15<00:00, 50.59s/it]
