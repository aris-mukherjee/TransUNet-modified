2022-01-10 02:11:53,485 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-10 02:11:53,486 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-10 02:11:53,486 ============================================================
2022-01-10 02:11:53,486 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-10 02:11:53,486 ============================================================
2022-01-10 02:11:53,486 Loading data...
2022-01-10 02:11:53,486 Reading NCI - RUNMC images...
2022-01-10 02:11:53,486 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-10 02:11:53,489 Already preprocessed this configuration. Loading now!
2022-01-10 02:11:53,511 Training Images: (256, 256, 286)
2022-01-10 02:11:53,511 Training Labels: (256, 256, 286)
2022-01-10 02:11:53,511 Validation Images: (256, 256, 98)
2022-01-10 02:11:53,511 Validation Labels: (256, 256, 98)
2022-01-10 02:11:53,511 ============================================================
2022-01-10 02:11:53,547 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-10 02:11:56,369 iteration 1 : loss : 0.926168, loss_ce: 1.121727
2022-01-10 02:11:57,759 iteration 2 : loss : 0.859492, loss_ce: 1.026776
2022-01-10 02:11:59,227 iteration 3 : loss : 0.801728, loss_ce: 0.938222
2022-01-10 02:12:00,637 iteration 4 : loss : 0.768535, loss_ce: 0.848103
2022-01-10 02:12:01,979 iteration 5 : loss : 0.724192, loss_ce: 0.766431
2022-01-10 02:12:03,369 iteration 6 : loss : 0.677085, loss_ce: 0.700715
2022-01-10 02:12:04,799 iteration 7 : loss : 0.634990, loss_ce: 0.642744
2022-01-10 02:12:06,283 iteration 8 : loss : 0.606047, loss_ce: 0.588892
2022-01-10 02:12:07,636 iteration 9 : loss : 0.591611, loss_ce: 0.539814
2022-01-10 02:12:08,998 iteration 10 : loss : 0.544008, loss_ce: 0.488676
2022-01-10 02:12:10,343 iteration 11 : loss : 0.524911, loss_ce: 0.445799
2022-01-10 02:12:11,735 iteration 12 : loss : 0.502564, loss_ce: 0.419846
2022-01-10 02:12:13,182 iteration 13 : loss : 0.476616, loss_ce: 0.399748
2022-01-10 02:12:14,531 iteration 14 : loss : 0.440659, loss_ce: 0.347959
2022-01-10 02:12:15,972 iteration 15 : loss : 0.424488, loss_ce: 0.316550
2022-01-10 02:12:17,355 iteration 16 : loss : 0.439778, loss_ce: 0.310112
2022-01-10 02:12:18,734 iteration 17 : loss : 0.406934, loss_ce: 0.297880
  0%|                               | 1/400 [00:25<2:48:02, 25.27s/it]2022-01-10 02:12:20,184 iteration 18 : loss : 0.383254, loss_ce: 0.253610
2022-01-10 02:12:21,598 iteration 19 : loss : 0.374818, loss_ce: 0.241392
2022-01-10 02:12:22,992 iteration 20 : loss : 0.344448, loss_ce: 0.220198
2022-01-10 02:12:24,463 iteration 21 : loss : 0.354506, loss_ce: 0.217225
2022-01-10 02:12:25,848 iteration 22 : loss : 0.339965, loss_ce: 0.199733
2022-01-10 02:12:27,260 iteration 23 : loss : 0.306777, loss_ce: 0.173410
2022-01-10 02:12:28,605 iteration 24 : loss : 0.324744, loss_ce: 0.192064
2022-01-10 02:12:29,915 iteration 25 : loss : 0.306610, loss_ce: 0.165629
2022-01-10 02:12:31,260 iteration 26 : loss : 0.322584, loss_ce: 0.167085
2022-01-10 02:12:32,660 iteration 27 : loss : 0.276865, loss_ce: 0.153062
2022-01-10 02:12:34,039 iteration 28 : loss : 0.296693, loss_ce: 0.148685
2022-01-10 02:12:35,557 iteration 29 : loss : 0.295002, loss_ce: 0.155980
2022-01-10 02:12:36,996 iteration 30 : loss : 0.276450, loss_ce: 0.141511
2022-01-10 02:12:38,346 iteration 31 : loss : 0.264271, loss_ce: 0.138862
2022-01-10 02:12:39,719 iteration 32 : loss : 0.281321, loss_ce: 0.143161
2022-01-10 02:12:41,184 iteration 33 : loss : 0.279253, loss_ce: 0.147417
2022-01-10 02:12:42,549 iteration 34 : loss : 0.271644, loss_ce: 0.125475
  0%|▏                              | 2/400 [00:49<2:41:49, 24.39s/it]2022-01-10 02:12:44,042 iteration 35 : loss : 0.261452, loss_ce: 0.138289
2022-01-10 02:12:45,445 iteration 36 : loss : 0.256805, loss_ce: 0.127032
2022-01-10 02:12:46,879 iteration 37 : loss : 0.252636, loss_ce: 0.128926
2022-01-10 02:12:48,380 iteration 38 : loss : 0.236177, loss_ce: 0.108407
2022-01-10 02:12:49,794 iteration 39 : loss : 0.324831, loss_ce: 0.142704
2022-01-10 02:12:51,234 iteration 40 : loss : 0.274649, loss_ce: 0.153270
2022-01-10 02:12:52,587 iteration 41 : loss : 0.249098, loss_ce: 0.107078
2022-01-10 02:12:54,110 iteration 42 : loss : 0.239731, loss_ce: 0.119540
2022-01-10 02:12:55,457 iteration 43 : loss : 0.268932, loss_ce: 0.113373
2022-01-10 02:12:56,880 iteration 44 : loss : 0.220134, loss_ce: 0.096801
2022-01-10 02:12:58,327 iteration 45 : loss : 0.368358, loss_ce: 0.164153
2022-01-10 02:12:59,647 iteration 46 : loss : 0.221861, loss_ce: 0.093641
2022-01-10 02:13:01,035 iteration 47 : loss : 0.278077, loss_ce: 0.105628
2022-01-10 02:13:02,463 iteration 48 : loss : 0.203572, loss_ce: 0.091869
2022-01-10 02:13:03,870 iteration 49 : loss : 0.259974, loss_ce: 0.105276
2022-01-10 02:13:05,317 iteration 50 : loss : 0.254115, loss_ce: 0.109789
2022-01-10 02:13:06,737 iteration 51 : loss : 0.244668, loss_ce: 0.121163
  1%|▏                              | 3/400 [01:13<2:40:48, 24.30s/it]2022-01-10 02:13:08,125 iteration 52 : loss : 0.265673, loss_ce: 0.123946
2022-01-10 02:13:09,449 iteration 53 : loss : 0.224166, loss_ce: 0.098560
2022-01-10 02:13:10,876 iteration 54 : loss : 0.198703, loss_ce: 0.090133
2022-01-10 02:13:12,291 iteration 55 : loss : 0.327431, loss_ce: 0.142604
2022-01-10 02:13:13,738 iteration 56 : loss : 0.207797, loss_ce: 0.089009
2022-01-10 02:13:15,178 iteration 57 : loss : 0.274767, loss_ce: 0.113025
2022-01-10 02:13:16,592 iteration 58 : loss : 0.258539, loss_ce: 0.124806
2022-01-10 02:13:17,989 iteration 59 : loss : 0.239671, loss_ce: 0.097393
2022-01-10 02:13:19,455 iteration 60 : loss : 0.256715, loss_ce: 0.106231
2022-01-10 02:13:20,933 iteration 61 : loss : 0.209657, loss_ce: 0.089972
2022-01-10 02:13:22,344 iteration 62 : loss : 0.256967, loss_ce: 0.104260
2022-01-10 02:13:23,690 iteration 63 : loss : 0.214409, loss_ce: 0.101274
2022-01-10 02:13:25,079 iteration 64 : loss : 0.256936, loss_ce: 0.133024
2022-01-10 02:13:26,471 iteration 65 : loss : 0.248071, loss_ce: 0.104545
2022-01-10 02:13:27,922 iteration 66 : loss : 0.263906, loss_ce: 0.117914
2022-01-10 02:13:29,285 iteration 67 : loss : 0.250305, loss_ce: 0.115368
2022-01-10 02:13:30,681 iteration 68 : loss : 0.283715, loss_ce: 0.110881
  1%|▎                              | 4/400 [01:37<2:39:28, 24.16s/it]2022-01-10 02:13:32,140 iteration 69 : loss : 0.282346, loss_ce: 0.103950
2022-01-10 02:13:33,566 iteration 70 : loss : 0.282167, loss_ce: 0.121791
2022-01-10 02:13:34,932 iteration 71 : loss : 0.250246, loss_ce: 0.098083
2022-01-10 02:13:36,388 iteration 72 : loss : 0.242564, loss_ce: 0.084523
2022-01-10 02:13:37,786 iteration 73 : loss : 0.240396, loss_ce: 0.117567
2022-01-10 02:13:39,264 iteration 74 : loss : 0.223935, loss_ce: 0.097067
2022-01-10 02:13:40,684 iteration 75 : loss : 0.254852, loss_ce: 0.133134
2022-01-10 02:13:42,147 iteration 76 : loss : 0.242283, loss_ce: 0.109239
2022-01-10 02:13:43,634 iteration 77 : loss : 0.230096, loss_ce: 0.092549
2022-01-10 02:13:45,097 iteration 78 : loss : 0.205232, loss_ce: 0.093882
2022-01-10 02:13:46,463 iteration 79 : loss : 0.201725, loss_ce: 0.074711
2022-01-10 02:13:47,918 iteration 80 : loss : 0.278811, loss_ce: 0.105935
2022-01-10 02:13:49,313 iteration 81 : loss : 0.221520, loss_ce: 0.084526
2022-01-10 02:13:50,716 iteration 82 : loss : 0.288140, loss_ce: 0.139654
2022-01-10 02:13:52,185 iteration 83 : loss : 0.207705, loss_ce: 0.081920
2022-01-10 02:13:53,565 iteration 84 : loss : 0.242979, loss_ce: 0.113637
2022-01-10 02:13:53,565 Training Data Eval:
2022-01-10 02:14:00,590   Average segmentation loss on training set: 0.3954
2022-01-10 02:14:00,590 Validation Data Eval:
2022-01-10 02:14:03,149   Average segmentation loss on validation set: 0.4661
2022-01-10 02:14:08,776 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 02:14:10,262 iteration 85 : loss : 0.182098, loss_ce: 0.076467
  1%|▍                              | 5/400 [02:16<3:15:39, 29.72s/it]2022-01-10 02:14:11,687 iteration 86 : loss : 0.302262, loss_ce: 0.127039
2022-01-10 02:14:13,004 iteration 87 : loss : 0.215306, loss_ce: 0.088653
2022-01-10 02:14:14,370 iteration 88 : loss : 0.211440, loss_ce: 0.076796
2022-01-10 02:14:15,820 iteration 89 : loss : 0.217658, loss_ce: 0.093779
2022-01-10 02:14:17,182 iteration 90 : loss : 0.190077, loss_ce: 0.080164
2022-01-10 02:14:18,593 iteration 91 : loss : 0.273842, loss_ce: 0.122813
2022-01-10 02:14:19,979 iteration 92 : loss : 0.220175, loss_ce: 0.089547
2022-01-10 02:14:21,442 iteration 93 : loss : 0.251841, loss_ce: 0.102639
2022-01-10 02:14:22,904 iteration 94 : loss : 0.243722, loss_ce: 0.125771
2022-01-10 02:14:24,294 iteration 95 : loss : 0.243517, loss_ce: 0.092355
2022-01-10 02:14:25,644 iteration 96 : loss : 0.210678, loss_ce: 0.076564
2022-01-10 02:14:26,997 iteration 97 : loss : 0.178739, loss_ce: 0.072358
2022-01-10 02:14:28,383 iteration 98 : loss : 0.268818, loss_ce: 0.104918
2022-01-10 02:14:29,897 iteration 99 : loss : 0.224785, loss_ce: 0.095492
2022-01-10 02:14:31,428 iteration 100 : loss : 0.257763, loss_ce: 0.106533
2022-01-10 02:14:32,809 iteration 101 : loss : 0.270618, loss_ce: 0.127930
2022-01-10 02:14:34,211 iteration 102 : loss : 0.217593, loss_ce: 0.080439
  2%|▍                              | 6/400 [02:40<3:02:18, 27.76s/it]2022-01-10 02:14:35,693 iteration 103 : loss : 0.238167, loss_ce: 0.094495
2022-01-10 02:14:37,144 iteration 104 : loss : 0.233050, loss_ce: 0.088730
2022-01-10 02:14:38,567 iteration 105 : loss : 0.214288, loss_ce: 0.091468
2022-01-10 02:14:39,996 iteration 106 : loss : 0.215030, loss_ce: 0.087125
2022-01-10 02:14:41,484 iteration 107 : loss : 0.205972, loss_ce: 0.085494
2022-01-10 02:14:42,888 iteration 108 : loss : 0.232274, loss_ce: 0.108345
2022-01-10 02:14:44,308 iteration 109 : loss : 0.180413, loss_ce: 0.071120
2022-01-10 02:14:45,762 iteration 110 : loss : 0.243266, loss_ce: 0.108531
2022-01-10 02:14:47,346 iteration 111 : loss : 0.194603, loss_ce: 0.074665
2022-01-10 02:14:48,733 iteration 112 : loss : 0.262684, loss_ce: 0.096186
2022-01-10 02:14:50,245 iteration 113 : loss : 0.183917, loss_ce: 0.065029
2022-01-10 02:14:51,667 iteration 114 : loss : 0.205181, loss_ce: 0.068246
2022-01-10 02:14:53,013 iteration 115 : loss : 0.175684, loss_ce: 0.064321
2022-01-10 02:14:54,427 iteration 116 : loss : 0.242745, loss_ce: 0.104096
2022-01-10 02:14:55,812 iteration 117 : loss : 0.173332, loss_ce: 0.068616
2022-01-10 02:14:57,280 iteration 118 : loss : 0.222115, loss_ce: 0.084551
2022-01-10 02:14:58,662 iteration 119 : loss : 0.267932, loss_ce: 0.118930
  2%|▌                              | 7/400 [03:05<2:54:44, 26.68s/it]2022-01-10 02:15:00,188 iteration 120 : loss : 0.236844, loss_ce: 0.110579
2022-01-10 02:15:01,640 iteration 121 : loss : 0.269416, loss_ce: 0.114479
2022-01-10 02:15:02,967 iteration 122 : loss : 0.219279, loss_ce: 0.091657
2022-01-10 02:15:04,390 iteration 123 : loss : 0.207081, loss_ce: 0.085353
2022-01-10 02:15:05,776 iteration 124 : loss : 0.214082, loss_ce: 0.087287
2022-01-10 02:15:07,167 iteration 125 : loss : 0.226701, loss_ce: 0.084971
2022-01-10 02:15:08,555 iteration 126 : loss : 0.182229, loss_ce: 0.080421
2022-01-10 02:15:09,961 iteration 127 : loss : 0.239111, loss_ce: 0.105027
2022-01-10 02:15:11,341 iteration 128 : loss : 0.191555, loss_ce: 0.067012
2022-01-10 02:15:12,813 iteration 129 : loss : 0.280874, loss_ce: 0.127438
2022-01-10 02:15:14,126 iteration 130 : loss : 0.191026, loss_ce: 0.077441
2022-01-10 02:15:15,514 iteration 131 : loss : 0.206859, loss_ce: 0.098837
2022-01-10 02:15:16,932 iteration 132 : loss : 0.245009, loss_ce: 0.103386
2022-01-10 02:15:18,350 iteration 133 : loss : 0.287452, loss_ce: 0.145835
2022-01-10 02:15:19,747 iteration 134 : loss : 0.174080, loss_ce: 0.061972
2022-01-10 02:15:21,137 iteration 135 : loss : 0.185615, loss_ce: 0.060457
2022-01-10 02:15:22,652 iteration 136 : loss : 0.203134, loss_ce: 0.088618
  2%|▌                              | 8/400 [03:29<2:48:41, 25.82s/it]2022-01-10 02:15:24,218 iteration 137 : loss : 0.216303, loss_ce: 0.080168
2022-01-10 02:15:25,591 iteration 138 : loss : 0.204855, loss_ce: 0.076812
2022-01-10 02:15:27,096 iteration 139 : loss : 0.231580, loss_ce: 0.067062
2022-01-10 02:15:28,502 iteration 140 : loss : 0.275681, loss_ce: 0.120904
2022-01-10 02:15:29,864 iteration 141 : loss : 0.230897, loss_ce: 0.094146
2022-01-10 02:15:31,260 iteration 142 : loss : 0.189670, loss_ce: 0.071695
2022-01-10 02:15:32,677 iteration 143 : loss : 0.229165, loss_ce: 0.090132
2022-01-10 02:15:34,068 iteration 144 : loss : 0.183875, loss_ce: 0.058238
2022-01-10 02:15:35,486 iteration 145 : loss : 0.206668, loss_ce: 0.099638
2022-01-10 02:15:36,933 iteration 146 : loss : 0.185788, loss_ce: 0.073745
2022-01-10 02:15:38,359 iteration 147 : loss : 0.265857, loss_ce: 0.130870
2022-01-10 02:15:39,863 iteration 148 : loss : 0.203981, loss_ce: 0.082014
2022-01-10 02:15:41,348 iteration 149 : loss : 0.230784, loss_ce: 0.106519
2022-01-10 02:15:42,832 iteration 150 : loss : 0.201086, loss_ce: 0.087295
2022-01-10 02:15:44,207 iteration 151 : loss : 0.188900, loss_ce: 0.104955
2022-01-10 02:15:45,536 iteration 152 : loss : 0.169687, loss_ce: 0.079506
2022-01-10 02:15:46,954 iteration 153 : loss : 0.213772, loss_ce: 0.092024
  2%|▋                              | 9/400 [03:53<2:45:09, 25.35s/it]2022-01-10 02:15:48,505 iteration 154 : loss : 0.218053, loss_ce: 0.103010
2022-01-10 02:15:49,966 iteration 155 : loss : 0.194755, loss_ce: 0.094759
2022-01-10 02:15:51,391 iteration 156 : loss : 0.207050, loss_ce: 0.099040
2022-01-10 02:15:52,802 iteration 157 : loss : 0.199820, loss_ce: 0.089982
2022-01-10 02:15:54,287 iteration 158 : loss : 0.229588, loss_ce: 0.095786
2022-01-10 02:15:55,765 iteration 159 : loss : 0.160670, loss_ce: 0.062941
2022-01-10 02:15:57,159 iteration 160 : loss : 0.166454, loss_ce: 0.063137
2022-01-10 02:15:58,650 iteration 161 : loss : 0.156400, loss_ce: 0.073036
2022-01-10 02:16:00,068 iteration 162 : loss : 0.196620, loss_ce: 0.070271
2022-01-10 02:16:01,483 iteration 163 : loss : 0.229593, loss_ce: 0.108210
2022-01-10 02:16:02,884 iteration 164 : loss : 0.219534, loss_ce: 0.080521
2022-01-10 02:16:04,230 iteration 165 : loss : 0.172822, loss_ce: 0.071463
2022-01-10 02:16:05,671 iteration 166 : loss : 0.258204, loss_ce: 0.157811
2022-01-10 02:16:07,182 iteration 167 : loss : 0.262784, loss_ce: 0.124914
2022-01-10 02:16:08,583 iteration 168 : loss : 0.254694, loss_ce: 0.090678
2022-01-10 02:16:10,013 iteration 169 : loss : 0.183594, loss_ce: 0.073955
2022-01-10 02:16:10,013 Training Data Eval:
2022-01-10 02:16:17,044   Average segmentation loss on training set: 0.2251
2022-01-10 02:16:17,045 Validation Data Eval:
2022-01-10 02:16:19,480   Average segmentation loss on validation set: 0.2244
2022-01-10 02:16:25,182 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 02:16:26,717 iteration 170 : loss : 0.184553, loss_ce: 0.074076
  2%|▊                             | 10/400 [04:33<3:13:39, 29.79s/it]2022-01-10 02:16:28,268 iteration 171 : loss : 0.141794, loss_ce: 0.065354
2022-01-10 02:16:29,626 iteration 172 : loss : 0.184836, loss_ce: 0.085376
2022-01-10 02:16:31,089 iteration 173 : loss : 0.177447, loss_ce: 0.067020
2022-01-10 02:16:32,421 iteration 174 : loss : 0.204085, loss_ce: 0.107737
2022-01-10 02:16:33,798 iteration 175 : loss : 0.221318, loss_ce: 0.074060
2022-01-10 02:16:35,172 iteration 176 : loss : 0.190566, loss_ce: 0.079036
2022-01-10 02:16:36,608 iteration 177 : loss : 0.171673, loss_ce: 0.075687
2022-01-10 02:16:38,005 iteration 178 : loss : 0.174207, loss_ce: 0.068950
2022-01-10 02:16:39,341 iteration 179 : loss : 0.178534, loss_ce: 0.058733
2022-01-10 02:16:40,827 iteration 180 : loss : 0.243724, loss_ce: 0.125470
2022-01-10 02:16:42,268 iteration 181 : loss : 0.201858, loss_ce: 0.097798
2022-01-10 02:16:43,669 iteration 182 : loss : 0.193163, loss_ce: 0.088464
2022-01-10 02:16:45,104 iteration 183 : loss : 0.212587, loss_ce: 0.073019
2022-01-10 02:16:46,655 iteration 184 : loss : 0.212986, loss_ce: 0.075869
2022-01-10 02:16:48,193 iteration 185 : loss : 0.207828, loss_ce: 0.095323
2022-01-10 02:16:49,592 iteration 186 : loss : 0.201207, loss_ce: 0.071348
2022-01-10 02:16:51,021 iteration 187 : loss : 0.158363, loss_ce: 0.070891
  3%|▊                             | 11/400 [04:57<3:02:16, 28.11s/it]2022-01-10 02:16:52,451 iteration 188 : loss : 0.168628, loss_ce: 0.069468
2022-01-10 02:16:53,951 iteration 189 : loss : 0.182962, loss_ce: 0.080629
2022-01-10 02:16:55,394 iteration 190 : loss : 0.218050, loss_ce: 0.100640
2022-01-10 02:16:56,826 iteration 191 : loss : 0.147464, loss_ce: 0.059236
2022-01-10 02:16:58,234 iteration 192 : loss : 0.169669, loss_ce: 0.061890
2022-01-10 02:16:59,576 iteration 193 : loss : 0.197336, loss_ce: 0.080188
2022-01-10 02:17:01,021 iteration 194 : loss : 0.137768, loss_ce: 0.058971
2022-01-10 02:17:02,476 iteration 195 : loss : 0.139242, loss_ce: 0.051840
2022-01-10 02:17:03,871 iteration 196 : loss : 0.139152, loss_ce: 0.061289
2022-01-10 02:17:05,316 iteration 197 : loss : 0.160936, loss_ce: 0.055709
2022-01-10 02:17:06,743 iteration 198 : loss : 0.162320, loss_ce: 0.057458
2022-01-10 02:17:08,066 iteration 199 : loss : 0.210380, loss_ce: 0.067516
2022-01-10 02:17:09,462 iteration 200 : loss : 0.206797, loss_ce: 0.099532
2022-01-10 02:17:10,987 iteration 201 : loss : 0.193072, loss_ce: 0.084451
2022-01-10 02:17:12,370 iteration 202 : loss : 0.154214, loss_ce: 0.068221
2022-01-10 02:17:13,734 iteration 203 : loss : 0.197169, loss_ce: 0.104382
2022-01-10 02:17:15,068 iteration 204 : loss : 0.133777, loss_ce: 0.062968
  3%|▉                             | 12/400 [05:21<2:53:50, 26.88s/it]2022-01-10 02:17:16,472 iteration 205 : loss : 0.206989, loss_ce: 0.093271
2022-01-10 02:17:17,900 iteration 206 : loss : 0.266024, loss_ce: 0.122625
2022-01-10 02:17:19,400 iteration 207 : loss : 0.188585, loss_ce: 0.099064
2022-01-10 02:17:20,904 iteration 208 : loss : 0.133225, loss_ce: 0.051911
2022-01-10 02:17:22,270 iteration 209 : loss : 0.152591, loss_ce: 0.059453
2022-01-10 02:17:23,621 iteration 210 : loss : 0.150152, loss_ce: 0.055477
2022-01-10 02:17:25,025 iteration 211 : loss : 0.172140, loss_ce: 0.067799
2022-01-10 02:17:26,344 iteration 212 : loss : 0.163261, loss_ce: 0.060922
2022-01-10 02:17:27,791 iteration 213 : loss : 0.206476, loss_ce: 0.097392
2022-01-10 02:17:29,219 iteration 214 : loss : 0.180338, loss_ce: 0.060257
2022-01-10 02:17:30,563 iteration 215 : loss : 0.177821, loss_ce: 0.076680
2022-01-10 02:17:32,021 iteration 216 : loss : 0.189327, loss_ce: 0.080745
2022-01-10 02:17:33,400 iteration 217 : loss : 0.135685, loss_ce: 0.058374
2022-01-10 02:17:34,782 iteration 218 : loss : 0.136913, loss_ce: 0.056871
2022-01-10 02:17:36,172 iteration 219 : loss : 0.163753, loss_ce: 0.059005
2022-01-10 02:17:37,522 iteration 220 : loss : 0.169391, loss_ce: 0.088647
2022-01-10 02:17:38,910 iteration 221 : loss : 0.129947, loss_ce: 0.064972
  3%|▉                             | 13/400 [05:45<2:47:26, 25.96s/it]2022-01-10 02:17:40,491 iteration 222 : loss : 0.176297, loss_ce: 0.069479
2022-01-10 02:17:41,948 iteration 223 : loss : 0.133794, loss_ce: 0.052586
2022-01-10 02:17:43,269 iteration 224 : loss : 0.165979, loss_ce: 0.076501
2022-01-10 02:17:44,716 iteration 225 : loss : 0.128875, loss_ce: 0.053486
2022-01-10 02:17:46,170 iteration 226 : loss : 0.244486, loss_ce: 0.094884
2022-01-10 02:17:47,617 iteration 227 : loss : 0.163947, loss_ce: 0.061838
2022-01-10 02:17:49,042 iteration 228 : loss : 0.157535, loss_ce: 0.064030
2022-01-10 02:17:50,467 iteration 229 : loss : 0.164929, loss_ce: 0.083608
2022-01-10 02:17:51,869 iteration 230 : loss : 0.206998, loss_ce: 0.094901
2022-01-10 02:17:53,246 iteration 231 : loss : 0.207692, loss_ce: 0.077359
2022-01-10 02:17:54,725 iteration 232 : loss : 0.161931, loss_ce: 0.082053
2022-01-10 02:17:56,191 iteration 233 : loss : 0.151755, loss_ce: 0.080160
2022-01-10 02:17:57,637 iteration 234 : loss : 0.170106, loss_ce: 0.052735
2022-01-10 02:17:59,040 iteration 235 : loss : 0.131037, loss_ce: 0.047116
2022-01-10 02:18:00,428 iteration 236 : loss : 0.187734, loss_ce: 0.079018
2022-01-10 02:18:01,806 iteration 237 : loss : 0.122947, loss_ce: 0.049755
2022-01-10 02:18:03,115 iteration 238 : loss : 0.158229, loss_ce: 0.081314
  4%|█                             | 14/400 [06:09<2:43:34, 25.43s/it]2022-01-10 02:18:04,537 iteration 239 : loss : 0.144096, loss_ce: 0.059287
2022-01-10 02:18:06,009 iteration 240 : loss : 0.187518, loss_ce: 0.094630
2022-01-10 02:18:07,377 iteration 241 : loss : 0.208172, loss_ce: 0.066360
2022-01-10 02:18:08,837 iteration 242 : loss : 0.151106, loss_ce: 0.060215
2022-01-10 02:18:10,317 iteration 243 : loss : 0.203261, loss_ce: 0.080653
2022-01-10 02:18:11,779 iteration 244 : loss : 0.156147, loss_ce: 0.069563
2022-01-10 02:18:13,115 iteration 245 : loss : 0.136250, loss_ce: 0.049981
2022-01-10 02:18:14,533 iteration 246 : loss : 0.165393, loss_ce: 0.069324
2022-01-10 02:18:15,886 iteration 247 : loss : 0.165635, loss_ce: 0.069287
2022-01-10 02:18:17,214 iteration 248 : loss : 0.138440, loss_ce: 0.056926
2022-01-10 02:18:18,710 iteration 249 : loss : 0.184020, loss_ce: 0.065113
2022-01-10 02:18:20,167 iteration 250 : loss : 0.183912, loss_ce: 0.084494
2022-01-10 02:18:21,560 iteration 251 : loss : 0.146418, loss_ce: 0.058722
2022-01-10 02:18:22,968 iteration 252 : loss : 0.171858, loss_ce: 0.085192
2022-01-10 02:18:24,325 iteration 253 : loss : 0.199444, loss_ce: 0.072296
2022-01-10 02:18:25,681 iteration 254 : loss : 0.166511, loss_ce: 0.070208
2022-01-10 02:18:25,681 Training Data Eval:
2022-01-10 02:18:32,714   Average segmentation loss on training set: 0.2368
2022-01-10 02:18:32,714 Validation Data Eval:
2022-01-10 02:18:35,141   Average segmentation loss on validation set: 0.3131
2022-01-10 02:18:36,536 iteration 255 : loss : 0.245520, loss_ce: 0.103613
  4%|█▏                            | 15/400 [06:43<2:58:36, 27.84s/it]2022-01-10 02:18:37,993 iteration 256 : loss : 0.136678, loss_ce: 0.052152
2022-01-10 02:18:39,376 iteration 257 : loss : 0.148945, loss_ce: 0.073713
2022-01-10 02:18:40,893 iteration 258 : loss : 0.156469, loss_ce: 0.057415
2022-01-10 02:18:42,243 iteration 259 : loss : 0.199983, loss_ce: 0.081184
2022-01-10 02:18:43,670 iteration 260 : loss : 0.153718, loss_ce: 0.067296
2022-01-10 02:18:45,106 iteration 261 : loss : 0.174543, loss_ce: 0.077427
2022-01-10 02:18:46,535 iteration 262 : loss : 0.196919, loss_ce: 0.111627
2022-01-10 02:18:47,898 iteration 263 : loss : 0.212569, loss_ce: 0.121051
2022-01-10 02:18:49,255 iteration 264 : loss : 0.195127, loss_ce: 0.081133
2022-01-10 02:18:50,704 iteration 265 : loss : 0.152597, loss_ce: 0.071241
2022-01-10 02:18:52,130 iteration 266 : loss : 0.213908, loss_ce: 0.076553
2022-01-10 02:18:53,526 iteration 267 : loss : 0.153192, loss_ce: 0.067476
2022-01-10 02:18:54,931 iteration 268 : loss : 0.127278, loss_ce: 0.063473
2022-01-10 02:18:56,316 iteration 269 : loss : 0.135017, loss_ce: 0.058612
2022-01-10 02:18:57,645 iteration 270 : loss : 0.171216, loss_ce: 0.061111
2022-01-10 02:18:59,069 iteration 271 : loss : 0.266256, loss_ce: 0.103847
2022-01-10 02:19:00,544 iteration 272 : loss : 0.176323, loss_ce: 0.073309
  4%|█▏                            | 16/400 [07:07<2:50:47, 26.69s/it]2022-01-10 02:19:01,995 iteration 273 : loss : 0.143215, loss_ce: 0.051870
2022-01-10 02:19:03,421 iteration 274 : loss : 0.185905, loss_ce: 0.077254
2022-01-10 02:19:04,833 iteration 275 : loss : 0.147657, loss_ce: 0.072670
2022-01-10 02:19:06,182 iteration 276 : loss : 0.146959, loss_ce: 0.071078
2022-01-10 02:19:07,658 iteration 277 : loss : 0.168966, loss_ce: 0.066466
2022-01-10 02:19:08,998 iteration 278 : loss : 0.314414, loss_ce: 0.119427
2022-01-10 02:19:10,405 iteration 279 : loss : 0.168934, loss_ce: 0.067342
2022-01-10 02:19:11,727 iteration 280 : loss : 0.166768, loss_ce: 0.086855
2022-01-10 02:19:13,142 iteration 281 : loss : 0.151317, loss_ce: 0.060883
2022-01-10 02:19:14,585 iteration 282 : loss : 0.190303, loss_ce: 0.092157
2022-01-10 02:19:16,001 iteration 283 : loss : 0.144479, loss_ce: 0.069661
2022-01-10 02:19:17,358 iteration 284 : loss : 0.076455, loss_ce: 0.035606
2022-01-10 02:19:18,770 iteration 285 : loss : 0.130248, loss_ce: 0.053443
2022-01-10 02:19:20,203 iteration 286 : loss : 0.108803, loss_ce: 0.043964
2022-01-10 02:19:21,564 iteration 287 : loss : 0.153133, loss_ce: 0.058590
2022-01-10 02:19:22,956 iteration 288 : loss : 0.107958, loss_ce: 0.050087
2022-01-10 02:19:24,352 iteration 289 : loss : 0.134292, loss_ce: 0.067133
  4%|█▎                            | 17/400 [07:30<2:44:49, 25.82s/it]2022-01-10 02:19:25,830 iteration 290 : loss : 0.114916, loss_ce: 0.055277
2022-01-10 02:19:27,235 iteration 291 : loss : 0.100307, loss_ce: 0.042431
2022-01-10 02:19:28,640 iteration 292 : loss : 0.098715, loss_ce: 0.043274
2022-01-10 02:19:30,175 iteration 293 : loss : 0.161830, loss_ce: 0.077390
2022-01-10 02:19:31,544 iteration 294 : loss : 0.135324, loss_ce: 0.061705
2022-01-10 02:19:32,918 iteration 295 : loss : 0.190885, loss_ce: 0.071117
2022-01-10 02:19:34,307 iteration 296 : loss : 0.137485, loss_ce: 0.061824
2022-01-10 02:19:35,769 iteration 297 : loss : 0.158302, loss_ce: 0.078653
2022-01-10 02:19:37,132 iteration 298 : loss : 0.149444, loss_ce: 0.060069
2022-01-10 02:19:38,563 iteration 299 : loss : 0.203806, loss_ce: 0.070408
2022-01-10 02:19:39,936 iteration 300 : loss : 0.160990, loss_ce: 0.069336
2022-01-10 02:19:41,337 iteration 301 : loss : 0.156376, loss_ce: 0.061061
2022-01-10 02:19:42,836 iteration 302 : loss : 0.200958, loss_ce: 0.108988
2022-01-10 02:19:44,280 iteration 303 : loss : 0.185397, loss_ce: 0.092926
2022-01-10 02:19:45,672 iteration 304 : loss : 0.096971, loss_ce: 0.044155
2022-01-10 02:19:47,078 iteration 305 : loss : 0.101819, loss_ce: 0.042237
2022-01-10 02:19:48,480 iteration 306 : loss : 0.156080, loss_ce: 0.063721
  4%|█▎                            | 18/400 [07:54<2:41:08, 25.31s/it]2022-01-10 02:19:49,914 iteration 307 : loss : 0.131853, loss_ce: 0.060281
2022-01-10 02:19:51,366 iteration 308 : loss : 0.137789, loss_ce: 0.052417
2022-01-10 02:19:52,777 iteration 309 : loss : 0.150709, loss_ce: 0.072326
2022-01-10 02:19:54,158 iteration 310 : loss : 0.107768, loss_ce: 0.040690
2022-01-10 02:19:55,528 iteration 311 : loss : 0.128189, loss_ce: 0.046298
2022-01-10 02:19:56,943 iteration 312 : loss : 0.197542, loss_ce: 0.055751
2022-01-10 02:19:58,395 iteration 313 : loss : 0.121948, loss_ce: 0.048539
2022-01-10 02:19:59,710 iteration 314 : loss : 0.158521, loss_ce: 0.064021
2022-01-10 02:20:01,083 iteration 315 : loss : 0.156819, loss_ce: 0.067717
2022-01-10 02:20:02,555 iteration 316 : loss : 0.140438, loss_ce: 0.058672
2022-01-10 02:20:04,037 iteration 317 : loss : 0.142095, loss_ce: 0.068975
2022-01-10 02:20:05,437 iteration 318 : loss : 0.158349, loss_ce: 0.086834
2022-01-10 02:20:06,860 iteration 319 : loss : 0.110897, loss_ce: 0.053398
2022-01-10 02:20:08,275 iteration 320 : loss : 0.142412, loss_ce: 0.055131
2022-01-10 02:20:09,651 iteration 321 : loss : 0.140866, loss_ce: 0.055772
2022-01-10 02:20:11,005 iteration 322 : loss : 0.095354, loss_ce: 0.046547
2022-01-10 02:20:12,384 iteration 323 : loss : 0.127400, loss_ce: 0.061535
  5%|█▍                            | 19/400 [08:18<2:38:02, 24.89s/it]2022-01-10 02:20:13,841 iteration 324 : loss : 0.095207, loss_ce: 0.043843
2022-01-10 02:20:15,212 iteration 325 : loss : 0.155545, loss_ce: 0.052553
2022-01-10 02:20:16,579 iteration 326 : loss : 0.192699, loss_ce: 0.090911
2022-01-10 02:20:17,923 iteration 327 : loss : 0.156787, loss_ce: 0.074585
2022-01-10 02:20:19,337 iteration 328 : loss : 0.077170, loss_ce: 0.027758
2022-01-10 02:20:20,801 iteration 329 : loss : 0.161323, loss_ce: 0.055785
2022-01-10 02:20:22,181 iteration 330 : loss : 0.125011, loss_ce: 0.060076
2022-01-10 02:20:23,576 iteration 331 : loss : 0.169090, loss_ce: 0.071907
2022-01-10 02:20:24,893 iteration 332 : loss : 0.090814, loss_ce: 0.042806
2022-01-10 02:20:26,338 iteration 333 : loss : 0.182219, loss_ce: 0.092869
2022-01-10 02:20:27,743 iteration 334 : loss : 0.110242, loss_ce: 0.048263
2022-01-10 02:20:29,209 iteration 335 : loss : 0.176782, loss_ce: 0.099321
2022-01-10 02:20:30,600 iteration 336 : loss : 0.144049, loss_ce: 0.070278
2022-01-10 02:20:31,979 iteration 337 : loss : 0.151875, loss_ce: 0.055988
2022-01-10 02:20:33,385 iteration 338 : loss : 0.150381, loss_ce: 0.073267
2022-01-10 02:20:34,761 iteration 339 : loss : 0.143721, loss_ce: 0.067318
2022-01-10 02:20:34,761 Training Data Eval:
2022-01-10 02:20:41,885   Average segmentation loss on training set: 0.2479
2022-01-10 02:20:41,886 Validation Data Eval:
2022-01-10 02:20:44,341   Average segmentation loss on validation set: 0.3870
2022-01-10 02:20:45,786 iteration 340 : loss : 0.157316, loss_ce: 0.081649
  5%|█▌                            | 20/400 [08:52<2:53:48, 27.44s/it]2022-01-10 02:20:47,222 iteration 341 : loss : 0.080073, loss_ce: 0.036883
2022-01-10 02:20:48,680 iteration 342 : loss : 0.117036, loss_ce: 0.058123
2022-01-10 02:20:50,125 iteration 343 : loss : 0.132124, loss_ce: 0.060528
2022-01-10 02:20:51,509 iteration 344 : loss : 0.108089, loss_ce: 0.053354
2022-01-10 02:20:52,891 iteration 345 : loss : 0.107798, loss_ce: 0.045796
2022-01-10 02:20:54,344 iteration 346 : loss : 0.090209, loss_ce: 0.040730
2022-01-10 02:20:55,792 iteration 347 : loss : 0.144515, loss_ce: 0.064252
2022-01-10 02:20:57,182 iteration 348 : loss : 0.092390, loss_ce: 0.038624
2022-01-10 02:20:58,581 iteration 349 : loss : 0.133133, loss_ce: 0.049215
2022-01-10 02:21:00,018 iteration 350 : loss : 0.132988, loss_ce: 0.060610
2022-01-10 02:21:01,414 iteration 351 : loss : 0.115570, loss_ce: 0.046167
2022-01-10 02:21:02,918 iteration 352 : loss : 0.115852, loss_ce: 0.072084
2022-01-10 02:21:04,410 iteration 353 : loss : 0.134692, loss_ce: 0.060428
2022-01-10 02:21:05,802 iteration 354 : loss : 0.146308, loss_ce: 0.056124
2022-01-10 02:21:07,239 iteration 355 : loss : 0.109762, loss_ce: 0.039517
2022-01-10 02:21:08,654 iteration 356 : loss : 0.107178, loss_ce: 0.043531
2022-01-10 02:21:10,081 iteration 357 : loss : 0.088780, loss_ce: 0.035306
  5%|█▌                            | 21/400 [09:16<2:47:23, 26.50s/it]2022-01-10 02:21:11,647 iteration 358 : loss : 0.154675, loss_ce: 0.072047
2022-01-10 02:21:13,130 iteration 359 : loss : 0.125708, loss_ce: 0.049718
2022-01-10 02:21:14,515 iteration 360 : loss : 0.087870, loss_ce: 0.031204
2022-01-10 02:21:15,950 iteration 361 : loss : 0.194213, loss_ce: 0.069140
2022-01-10 02:21:17,388 iteration 362 : loss : 0.123712, loss_ce: 0.072689
2022-01-10 02:21:18,729 iteration 363 : loss : 0.116450, loss_ce: 0.051349
2022-01-10 02:21:20,145 iteration 364 : loss : 0.235345, loss_ce: 0.096217
2022-01-10 02:21:21,511 iteration 365 : loss : 0.110167, loss_ce: 0.049697
2022-01-10 02:21:22,898 iteration 366 : loss : 0.135596, loss_ce: 0.043560
2022-01-10 02:21:24,313 iteration 367 : loss : 0.141269, loss_ce: 0.060605
2022-01-10 02:21:25,705 iteration 368 : loss : 0.146415, loss_ce: 0.064944
2022-01-10 02:21:27,077 iteration 369 : loss : 0.113395, loss_ce: 0.036903
2022-01-10 02:21:28,481 iteration 370 : loss : 0.194874, loss_ce: 0.102292
2022-01-10 02:21:29,886 iteration 371 : loss : 0.149961, loss_ce: 0.056732
2022-01-10 02:21:31,250 iteration 372 : loss : 0.123470, loss_ce: 0.052047
2022-01-10 02:21:32,644 iteration 373 : loss : 0.103563, loss_ce: 0.043519
2022-01-10 02:21:34,079 iteration 374 : loss : 0.131292, loss_ce: 0.056748
  6%|█▋                            | 22/400 [09:40<2:42:12, 25.75s/it]2022-01-10 02:21:35,539 iteration 375 : loss : 0.115695, loss_ce: 0.052310
2022-01-10 02:21:36,927 iteration 376 : loss : 0.166712, loss_ce: 0.054898
2022-01-10 02:21:38,335 iteration 377 : loss : 0.132411, loss_ce: 0.052339
2022-01-10 02:21:39,814 iteration 378 : loss : 0.126967, loss_ce: 0.046780
2022-01-10 02:21:41,243 iteration 379 : loss : 0.102226, loss_ce: 0.044716
2022-01-10 02:21:42,624 iteration 380 : loss : 0.127318, loss_ce: 0.067407
2022-01-10 02:21:44,045 iteration 381 : loss : 0.166381, loss_ce: 0.077700
2022-01-10 02:21:45,387 iteration 382 : loss : 0.098793, loss_ce: 0.043504
2022-01-10 02:21:46,807 iteration 383 : loss : 0.230079, loss_ce: 0.055290
2022-01-10 02:21:48,215 iteration 384 : loss : 0.129739, loss_ce: 0.051013
2022-01-10 02:21:49,595 iteration 385 : loss : 0.094823, loss_ce: 0.043254
2022-01-10 02:21:51,008 iteration 386 : loss : 0.122178, loss_ce: 0.048602
2022-01-10 02:21:52,371 iteration 387 : loss : 0.101735, loss_ce: 0.043176
2022-01-10 02:21:53,812 iteration 388 : loss : 0.125411, loss_ce: 0.059448
2022-01-10 02:21:55,203 iteration 389 : loss : 0.156613, loss_ce: 0.056266
2022-01-10 02:21:56,615 iteration 390 : loss : 0.159117, loss_ce: 0.089807
2022-01-10 02:21:58,041 iteration 391 : loss : 0.084039, loss_ce: 0.034084
  6%|█▋                            | 23/400 [10:04<2:38:26, 25.21s/it]2022-01-10 02:21:59,559 iteration 392 : loss : 0.129564, loss_ce: 0.057784
2022-01-10 02:22:00,966 iteration 393 : loss : 0.128780, loss_ce: 0.060525
2022-01-10 02:22:02,381 iteration 394 : loss : 0.121931, loss_ce: 0.048911
2022-01-10 02:22:03,743 iteration 395 : loss : 0.089326, loss_ce: 0.034506
2022-01-10 02:22:05,137 iteration 396 : loss : 0.138260, loss_ce: 0.047451
2022-01-10 02:22:06,619 iteration 397 : loss : 0.120552, loss_ce: 0.060253
2022-01-10 02:22:08,011 iteration 398 : loss : 0.121104, loss_ce: 0.047377
2022-01-10 02:22:09,423 iteration 399 : loss : 0.127971, loss_ce: 0.055951
2022-01-10 02:22:10,799 iteration 400 : loss : 0.094927, loss_ce: 0.039644
2022-01-10 02:22:12,220 iteration 401 : loss : 0.084942, loss_ce: 0.041871
2022-01-10 02:22:13,724 iteration 402 : loss : 0.098260, loss_ce: 0.045844
2022-01-10 02:22:15,177 iteration 403 : loss : 0.114201, loss_ce: 0.049322
2022-01-10 02:22:16,566 iteration 404 : loss : 0.072831, loss_ce: 0.028865
2022-01-10 02:22:17,962 iteration 405 : loss : 0.151369, loss_ce: 0.079783
2022-01-10 02:22:19,402 iteration 406 : loss : 0.124395, loss_ce: 0.051076
2022-01-10 02:22:20,846 iteration 407 : loss : 0.096647, loss_ce: 0.040813
2022-01-10 02:22:22,173 iteration 408 : loss : 0.086999, loss_ce: 0.041717
  6%|█▊                            | 24/400 [10:28<2:35:58, 24.89s/it]2022-01-10 02:22:23,701 iteration 409 : loss : 0.157212, loss_ce: 0.050054
2022-01-10 02:22:25,134 iteration 410 : loss : 0.104694, loss_ce: 0.039452
2022-01-10 02:22:26,567 iteration 411 : loss : 0.145657, loss_ce: 0.049099
2022-01-10 02:22:27,953 iteration 412 : loss : 0.110194, loss_ce: 0.047119
2022-01-10 02:22:29,354 iteration 413 : loss : 0.112102, loss_ce: 0.036241
2022-01-10 02:22:30,712 iteration 414 : loss : 0.107220, loss_ce: 0.042222
2022-01-10 02:22:32,177 iteration 415 : loss : 0.137837, loss_ce: 0.055200
2022-01-10 02:22:33,552 iteration 416 : loss : 0.109779, loss_ce: 0.052805
2022-01-10 02:22:34,983 iteration 417 : loss : 0.126652, loss_ce: 0.044767
2022-01-10 02:22:36,319 iteration 418 : loss : 0.105037, loss_ce: 0.064551
2022-01-10 02:22:37,709 iteration 419 : loss : 0.152886, loss_ce: 0.054868
2022-01-10 02:22:39,108 iteration 420 : loss : 0.093471, loss_ce: 0.045696
2022-01-10 02:22:40,580 iteration 421 : loss : 0.161857, loss_ce: 0.083779
2022-01-10 02:22:41,962 iteration 422 : loss : 0.117699, loss_ce: 0.042430
2022-01-10 02:22:43,371 iteration 423 : loss : 0.156218, loss_ce: 0.064087
2022-01-10 02:22:44,814 iteration 424 : loss : 0.119167, loss_ce: 0.054099
2022-01-10 02:22:44,814 Training Data Eval:
2022-01-10 02:22:51,858   Average segmentation loss on training set: 0.1231
2022-01-10 02:22:51,858 Validation Data Eval:
2022-01-10 02:22:54,292   Average segmentation loss on validation set: 0.1976
2022-01-10 02:22:59,966 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 02:23:01,478 iteration 425 : loss : 0.125889, loss_ce: 0.052504
  6%|█▉                            | 25/400 [11:07<3:02:34, 29.21s/it]2022-01-10 02:23:03,039 iteration 426 : loss : 0.124373, loss_ce: 0.050315
2022-01-10 02:23:04,449 iteration 427 : loss : 0.119334, loss_ce: 0.046665
2022-01-10 02:23:05,899 iteration 428 : loss : 0.113857, loss_ce: 0.048974
2022-01-10 02:23:07,273 iteration 429 : loss : 0.092411, loss_ce: 0.038408
2022-01-10 02:23:08,702 iteration 430 : loss : 0.150072, loss_ce: 0.062645
2022-01-10 02:23:10,052 iteration 431 : loss : 0.117898, loss_ce: 0.049021
2022-01-10 02:23:11,431 iteration 432 : loss : 0.071798, loss_ce: 0.035209
2022-01-10 02:23:12,848 iteration 433 : loss : 0.123493, loss_ce: 0.044091
2022-01-10 02:23:14,240 iteration 434 : loss : 0.076348, loss_ce: 0.027810
2022-01-10 02:23:15,631 iteration 435 : loss : 0.138101, loss_ce: 0.070258
2022-01-10 02:23:17,075 iteration 436 : loss : 0.148036, loss_ce: 0.063780
2022-01-10 02:23:18,488 iteration 437 : loss : 0.114179, loss_ce: 0.046413
2022-01-10 02:23:19,895 iteration 438 : loss : 0.165140, loss_ce: 0.100110
2022-01-10 02:23:21,294 iteration 439 : loss : 0.135515, loss_ce: 0.065197
2022-01-10 02:23:22,657 iteration 440 : loss : 0.090991, loss_ce: 0.039200
2022-01-10 02:23:24,046 iteration 441 : loss : 0.136049, loss_ce: 0.065375
2022-01-10 02:23:25,424 iteration 442 : loss : 0.174886, loss_ce: 0.064804
  6%|█▉                            | 26/400 [11:31<2:52:14, 27.63s/it]2022-01-10 02:23:26,893 iteration 443 : loss : 0.084172, loss_ce: 0.043005
2022-01-10 02:23:28,287 iteration 444 : loss : 0.178900, loss_ce: 0.067743
2022-01-10 02:23:29,868 iteration 445 : loss : 0.136593, loss_ce: 0.067770
2022-01-10 02:23:31,224 iteration 446 : loss : 0.142564, loss_ce: 0.055608
2022-01-10 02:23:32,707 iteration 447 : loss : 0.159045, loss_ce: 0.075788
2022-01-10 02:23:34,155 iteration 448 : loss : 0.088882, loss_ce: 0.043726
2022-01-10 02:23:35,548 iteration 449 : loss : 0.104696, loss_ce: 0.048171
2022-01-10 02:23:36,882 iteration 450 : loss : 0.101754, loss_ce: 0.044727
2022-01-10 02:23:38,279 iteration 451 : loss : 0.145300, loss_ce: 0.057674
2022-01-10 02:23:39,702 iteration 452 : loss : 0.090409, loss_ce: 0.043750
2022-01-10 02:23:41,116 iteration 453 : loss : 0.102913, loss_ce: 0.032732
2022-01-10 02:23:42,558 iteration 454 : loss : 0.184695, loss_ce: 0.059597
2022-01-10 02:23:44,037 iteration 455 : loss : 0.102009, loss_ce: 0.030534
2022-01-10 02:23:45,416 iteration 456 : loss : 0.148989, loss_ce: 0.051625
2022-01-10 02:23:46,799 iteration 457 : loss : 0.134558, loss_ce: 0.055692
2022-01-10 02:23:48,197 iteration 458 : loss : 0.151103, loss_ce: 0.070020
2022-01-10 02:23:49,613 iteration 459 : loss : 0.089486, loss_ce: 0.035498
  7%|██                            | 27/400 [11:56<2:45:22, 26.60s/it]2022-01-10 02:23:51,006 iteration 460 : loss : 0.107967, loss_ce: 0.038615
2022-01-10 02:23:52,433 iteration 461 : loss : 0.131250, loss_ce: 0.058087
2022-01-10 02:23:53,852 iteration 462 : loss : 0.134638, loss_ce: 0.036920
2022-01-10 02:23:55,184 iteration 463 : loss : 0.089775, loss_ce: 0.036704
2022-01-10 02:23:56,655 iteration 464 : loss : 0.126769, loss_ce: 0.060813
2022-01-10 02:23:58,045 iteration 465 : loss : 0.102373, loss_ce: 0.052381
2022-01-10 02:23:59,494 iteration 466 : loss : 0.118023, loss_ce: 0.057052
2022-01-10 02:24:00,874 iteration 467 : loss : 0.108625, loss_ce: 0.034899
2022-01-10 02:24:02,275 iteration 468 : loss : 0.110388, loss_ce: 0.044536
2022-01-10 02:24:03,644 iteration 469 : loss : 0.134847, loss_ce: 0.050228
2022-01-10 02:24:05,119 iteration 470 : loss : 0.136563, loss_ce: 0.067817
2022-01-10 02:24:06,490 iteration 471 : loss : 0.093629, loss_ce: 0.039665
2022-01-10 02:24:07,943 iteration 472 : loss : 0.167131, loss_ce: 0.075336
2022-01-10 02:24:09,389 iteration 473 : loss : 0.084303, loss_ce: 0.033957
2022-01-10 02:24:10,855 iteration 474 : loss : 0.104244, loss_ce: 0.043438
2022-01-10 02:24:12,274 iteration 475 : loss : 0.151490, loss_ce: 0.044907
2022-01-10 02:24:13,714 iteration 476 : loss : 0.087290, loss_ce: 0.043598
  7%|██                            | 28/400 [12:20<2:40:15, 25.85s/it]2022-01-10 02:24:15,166 iteration 477 : loss : 0.100659, loss_ce: 0.047133
2022-01-10 02:24:16,621 iteration 478 : loss : 0.104565, loss_ce: 0.041381
2022-01-10 02:24:17,983 iteration 479 : loss : 0.088126, loss_ce: 0.033272
2022-01-10 02:24:19,312 iteration 480 : loss : 0.114572, loss_ce: 0.054423
2022-01-10 02:24:20,709 iteration 481 : loss : 0.094231, loss_ce: 0.043768
2022-01-10 02:24:22,178 iteration 482 : loss : 0.094141, loss_ce: 0.038856
2022-01-10 02:24:23,608 iteration 483 : loss : 0.083405, loss_ce: 0.029286
2022-01-10 02:24:25,016 iteration 484 : loss : 0.103372, loss_ce: 0.048291
2022-01-10 02:24:26,365 iteration 485 : loss : 0.074201, loss_ce: 0.031828
2022-01-10 02:24:27,768 iteration 486 : loss : 0.251604, loss_ce: 0.085763
2022-01-10 02:24:29,207 iteration 487 : loss : 0.145621, loss_ce: 0.088664
2022-01-10 02:24:30,706 iteration 488 : loss : 0.187131, loss_ce: 0.074344
2022-01-10 02:24:32,142 iteration 489 : loss : 0.072349, loss_ce: 0.029438
2022-01-10 02:24:33,509 iteration 490 : loss : 0.112095, loss_ce: 0.046421
2022-01-10 02:24:34,971 iteration 491 : loss : 0.120962, loss_ce: 0.052228
2022-01-10 02:24:36,339 iteration 492 : loss : 0.124154, loss_ce: 0.055099
2022-01-10 02:24:37,736 iteration 493 : loss : 0.166322, loss_ce: 0.072028
  7%|██▏                           | 29/400 [12:44<2:36:26, 25.30s/it]2022-01-10 02:24:39,165 iteration 494 : loss : 0.091515, loss_ce: 0.034199
2022-01-10 02:24:40,565 iteration 495 : loss : 0.080234, loss_ce: 0.033747
2022-01-10 02:24:41,893 iteration 496 : loss : 0.121687, loss_ce: 0.054414
2022-01-10 02:24:43,337 iteration 497 : loss : 0.093975, loss_ce: 0.032023
2022-01-10 02:24:44,736 iteration 498 : loss : 0.143962, loss_ce: 0.060962
2022-01-10 02:24:46,216 iteration 499 : loss : 0.122078, loss_ce: 0.060216
2022-01-10 02:24:47,550 iteration 500 : loss : 0.087999, loss_ce: 0.038107
2022-01-10 02:24:48,972 iteration 501 : loss : 0.100674, loss_ce: 0.035934
2022-01-10 02:24:50,354 iteration 502 : loss : 0.114982, loss_ce: 0.042183
2022-01-10 02:24:51,763 iteration 503 : loss : 0.121876, loss_ce: 0.060278
2022-01-10 02:24:53,243 iteration 504 : loss : 0.073471, loss_ce: 0.030658
2022-01-10 02:24:54,731 iteration 505 : loss : 0.065637, loss_ce: 0.029043
2022-01-10 02:24:56,164 iteration 506 : loss : 0.128233, loss_ce: 0.065491
2022-01-10 02:24:57,565 iteration 507 : loss : 0.077180, loss_ce: 0.029674
2022-01-10 02:24:59,006 iteration 508 : loss : 0.118643, loss_ce: 0.041139
2022-01-10 02:25:00,467 iteration 509 : loss : 0.088975, loss_ce: 0.042062
2022-01-10 02:25:00,467 Training Data Eval:
2022-01-10 02:25:07,510   Average segmentation loss on training set: 0.5074
2022-01-10 02:25:07,510 Validation Data Eval:
2022-01-10 02:25:09,958   Average segmentation loss on validation set: 0.4909
2022-01-10 02:25:11,344 iteration 510 : loss : 0.086019, loss_ce: 0.035431
  8%|██▎                           | 30/400 [13:17<2:51:24, 27.80s/it]2022-01-10 02:25:12,778 iteration 511 : loss : 0.091441, loss_ce: 0.032052
2022-01-10 02:25:14,178 iteration 512 : loss : 0.124786, loss_ce: 0.066523
2022-01-10 02:25:15,587 iteration 513 : loss : 0.088639, loss_ce: 0.036739
2022-01-10 02:25:17,023 iteration 514 : loss : 0.115012, loss_ce: 0.049815
2022-01-10 02:25:18,365 iteration 515 : loss : 0.109952, loss_ce: 0.048164
2022-01-10 02:25:19,678 iteration 516 : loss : 0.065647, loss_ce: 0.025892
2022-01-10 02:25:21,154 iteration 517 : loss : 0.093991, loss_ce: 0.046613
2022-01-10 02:25:22,648 iteration 518 : loss : 0.143888, loss_ce: 0.056484
2022-01-10 02:25:24,131 iteration 519 : loss : 0.110810, loss_ce: 0.057019
2022-01-10 02:25:25,554 iteration 520 : loss : 0.102491, loss_ce: 0.046663
2022-01-10 02:25:26,910 iteration 521 : loss : 0.065165, loss_ce: 0.026857
2022-01-10 02:25:28,364 iteration 522 : loss : 0.166348, loss_ce: 0.072762
2022-01-10 02:25:29,763 iteration 523 : loss : 0.099782, loss_ce: 0.032942
2022-01-10 02:25:31,149 iteration 524 : loss : 0.104829, loss_ce: 0.053710
2022-01-10 02:25:32,543 iteration 525 : loss : 0.087424, loss_ce: 0.038954
2022-01-10 02:25:33,989 iteration 526 : loss : 0.079480, loss_ce: 0.032870
2022-01-10 02:25:35,429 iteration 527 : loss : 0.113350, loss_ce: 0.043722
  8%|██▎                           | 31/400 [13:41<2:44:05, 26.68s/it]2022-01-10 02:25:36,878 iteration 528 : loss : 0.075988, loss_ce: 0.034800
2022-01-10 02:25:38,397 iteration 529 : loss : 0.117238, loss_ce: 0.037940
2022-01-10 02:25:39,780 iteration 530 : loss : 0.076142, loss_ce: 0.030737
2022-01-10 02:25:41,148 iteration 531 : loss : 0.078732, loss_ce: 0.031552
2022-01-10 02:25:42,580 iteration 532 : loss : 0.095789, loss_ce: 0.042444
2022-01-10 02:25:43,996 iteration 533 : loss : 0.102768, loss_ce: 0.040478
2022-01-10 02:25:45,398 iteration 534 : loss : 0.104824, loss_ce: 0.037804
2022-01-10 02:25:46,846 iteration 535 : loss : 0.087054, loss_ce: 0.032171
2022-01-10 02:25:48,267 iteration 536 : loss : 0.098082, loss_ce: 0.044913
2022-01-10 02:25:49,661 iteration 537 : loss : 0.102227, loss_ce: 0.044721
2022-01-10 02:25:51,082 iteration 538 : loss : 0.096166, loss_ce: 0.051616
2022-01-10 02:25:52,422 iteration 539 : loss : 0.081046, loss_ce: 0.034375
2022-01-10 02:25:53,950 iteration 540 : loss : 0.112211, loss_ce: 0.056577
2022-01-10 02:25:55,461 iteration 541 : loss : 0.155875, loss_ce: 0.074163
2022-01-10 02:25:56,822 iteration 542 : loss : 0.101082, loss_ce: 0.042782
2022-01-10 02:25:58,241 iteration 543 : loss : 0.085637, loss_ce: 0.036179
2022-01-10 02:25:59,635 iteration 544 : loss : 0.207523, loss_ce: 0.073616
  8%|██▍                           | 32/400 [14:06<2:39:05, 25.94s/it]2022-01-10 02:26:01,166 iteration 545 : loss : 0.093467, loss_ce: 0.039787
2022-01-10 02:26:02,647 iteration 546 : loss : 0.088370, loss_ce: 0.040640
2022-01-10 02:26:04,109 iteration 547 : loss : 0.091831, loss_ce: 0.042525
2022-01-10 02:26:05,465 iteration 548 : loss : 0.159673, loss_ce: 0.063124
2022-01-10 02:26:06,828 iteration 549 : loss : 0.106111, loss_ce: 0.037999
2022-01-10 02:26:08,191 iteration 550 : loss : 0.080028, loss_ce: 0.032192
2022-01-10 02:26:09,559 iteration 551 : loss : 0.083352, loss_ce: 0.024267
2022-01-10 02:26:11,008 iteration 552 : loss : 0.155049, loss_ce: 0.063938
2022-01-10 02:26:12,439 iteration 553 : loss : 0.082509, loss_ce: 0.033478
2022-01-10 02:26:13,864 iteration 554 : loss : 0.152239, loss_ce: 0.062291
2022-01-10 02:26:15,314 iteration 555 : loss : 0.060169, loss_ce: 0.017889
2022-01-10 02:26:16,798 iteration 556 : loss : 0.122988, loss_ce: 0.058406
2022-01-10 02:26:18,176 iteration 557 : loss : 0.092539, loss_ce: 0.044717
2022-01-10 02:26:19,532 iteration 558 : loss : 0.072899, loss_ce: 0.028103
2022-01-10 02:26:20,914 iteration 559 : loss : 0.097391, loss_ce: 0.039736
2022-01-10 02:26:22,383 iteration 560 : loss : 0.082515, loss_ce: 0.041034
2022-01-10 02:26:23,831 iteration 561 : loss : 0.064284, loss_ce: 0.029640
  8%|██▍                           | 33/400 [14:30<2:35:27, 25.42s/it]2022-01-10 02:26:25,334 iteration 562 : loss : 0.142471, loss_ce: 0.050162
2022-01-10 02:26:26,695 iteration 563 : loss : 0.064032, loss_ce: 0.033928
2022-01-10 02:26:28,063 iteration 564 : loss : 0.094626, loss_ce: 0.046623
2022-01-10 02:26:29,499 iteration 565 : loss : 0.076884, loss_ce: 0.038289
2022-01-10 02:26:30,921 iteration 566 : loss : 0.129815, loss_ce: 0.040048
2022-01-10 02:26:32,392 iteration 567 : loss : 0.143711, loss_ce: 0.065444
2022-01-10 02:26:33,825 iteration 568 : loss : 0.145335, loss_ce: 0.047892
2022-01-10 02:26:35,310 iteration 569 : loss : 0.166624, loss_ce: 0.058354
2022-01-10 02:26:36,733 iteration 570 : loss : 0.079593, loss_ce: 0.030945
2022-01-10 02:26:38,186 iteration 571 : loss : 0.068553, loss_ce: 0.028646
2022-01-10 02:26:39,654 iteration 572 : loss : 0.114040, loss_ce: 0.057655
2022-01-10 02:26:41,031 iteration 573 : loss : 0.096262, loss_ce: 0.039871
2022-01-10 02:26:42,514 iteration 574 : loss : 0.099412, loss_ce: 0.038857
2022-01-10 02:26:43,912 iteration 575 : loss : 0.168665, loss_ce: 0.043447
2022-01-10 02:26:45,266 iteration 576 : loss : 0.098355, loss_ce: 0.047344
2022-01-10 02:26:46,792 iteration 577 : loss : 0.117189, loss_ce: 0.052113
2022-01-10 02:26:48,246 iteration 578 : loss : 0.114947, loss_ce: 0.056544
  8%|██▌                           | 34/400 [14:54<2:33:11, 25.11s/it]2022-01-10 02:26:49,647 iteration 579 : loss : 0.064967, loss_ce: 0.031267
2022-01-10 02:26:51,101 iteration 580 : loss : 0.074221, loss_ce: 0.029603
2022-01-10 02:26:52,647 iteration 581 : loss : 0.065760, loss_ce: 0.029778
2022-01-10 02:26:54,009 iteration 582 : loss : 0.071824, loss_ce: 0.031111
2022-01-10 02:26:55,432 iteration 583 : loss : 0.090753, loss_ce: 0.046663
2022-01-10 02:26:56,878 iteration 584 : loss : 0.115699, loss_ce: 0.048234
2022-01-10 02:26:58,261 iteration 585 : loss : 0.139295, loss_ce: 0.046143
2022-01-10 02:26:59,705 iteration 586 : loss : 0.088846, loss_ce: 0.034921
2022-01-10 02:27:01,131 iteration 587 : loss : 0.086374, loss_ce: 0.038906
2022-01-10 02:27:02,503 iteration 588 : loss : 0.103884, loss_ce: 0.038204
2022-01-10 02:27:03,973 iteration 589 : loss : 0.123703, loss_ce: 0.057716
2022-01-10 02:27:05,367 iteration 590 : loss : 0.082599, loss_ce: 0.034485
2022-01-10 02:27:06,762 iteration 591 : loss : 0.122869, loss_ce: 0.047821
2022-01-10 02:27:08,206 iteration 592 : loss : 0.077015, loss_ce: 0.037934
2022-01-10 02:27:09,561 iteration 593 : loss : 0.141160, loss_ce: 0.053850
2022-01-10 02:27:11,018 iteration 594 : loss : 0.083435, loss_ce: 0.029127
2022-01-10 02:27:11,018 Training Data Eval:
2022-01-10 02:27:18,062   Average segmentation loss on training set: 0.0779
2022-01-10 02:27:18,063 Validation Data Eval:
2022-01-10 02:27:20,494   Average segmentation loss on validation set: 0.1251
2022-01-10 02:27:26,504 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 02:27:27,951 iteration 595 : loss : 0.135354, loss_ce: 0.054872
  9%|██▋                           | 35/400 [15:34<2:59:24, 29.49s/it]2022-01-10 02:27:29,489 iteration 596 : loss : 0.121550, loss_ce: 0.052992
2022-01-10 02:27:30,895 iteration 597 : loss : 0.066444, loss_ce: 0.025525
2022-01-10 02:27:32,301 iteration 598 : loss : 0.110186, loss_ce: 0.039610
2022-01-10 02:27:33,738 iteration 599 : loss : 0.097468, loss_ce: 0.037102
2022-01-10 02:27:35,192 iteration 600 : loss : 0.094831, loss_ce: 0.034604
2022-01-10 02:27:36,638 iteration 601 : loss : 0.122809, loss_ce: 0.053825
2022-01-10 02:27:37,966 iteration 602 : loss : 0.104641, loss_ce: 0.047959
2022-01-10 02:27:39,371 iteration 603 : loss : 0.078679, loss_ce: 0.033835
2022-01-10 02:27:40,738 iteration 604 : loss : 0.119118, loss_ce: 0.036687
2022-01-10 02:27:42,245 iteration 605 : loss : 0.079654, loss_ce: 0.032083
2022-01-10 02:27:43,616 iteration 606 : loss : 0.068749, loss_ce: 0.026723
2022-01-10 02:27:44,987 iteration 607 : loss : 0.071849, loss_ce: 0.025825
2022-01-10 02:27:46,351 iteration 608 : loss : 0.055832, loss_ce: 0.028112
2022-01-10 02:27:47,715 iteration 609 : loss : 0.097336, loss_ce: 0.037979
2022-01-10 02:27:49,213 iteration 610 : loss : 0.137548, loss_ce: 0.074212
2022-01-10 02:27:50,613 iteration 611 : loss : 0.072884, loss_ce: 0.034914
2022-01-10 02:27:51,958 iteration 612 : loss : 0.057665, loss_ce: 0.026242
  9%|██▋                           | 36/400 [15:58<2:48:55, 27.85s/it]2022-01-10 02:27:53,418 iteration 613 : loss : 0.112500, loss_ce: 0.047124
2022-01-10 02:27:54,882 iteration 614 : loss : 0.087617, loss_ce: 0.040771
2022-01-10 02:27:56,300 iteration 615 : loss : 0.098427, loss_ce: 0.052529
2022-01-10 02:27:57,689 iteration 616 : loss : 0.106055, loss_ce: 0.036684
2022-01-10 02:27:59,171 iteration 617 : loss : 0.133310, loss_ce: 0.047245
2022-01-10 02:28:00,652 iteration 618 : loss : 0.059149, loss_ce: 0.030756
2022-01-10 02:28:02,081 iteration 619 : loss : 0.067189, loss_ce: 0.027350
2022-01-10 02:28:03,543 iteration 620 : loss : 0.103285, loss_ce: 0.048963
2022-01-10 02:28:04,872 iteration 621 : loss : 0.098130, loss_ce: 0.039376
2022-01-10 02:28:06,341 iteration 622 : loss : 0.095600, loss_ce: 0.038989
2022-01-10 02:28:07,811 iteration 623 : loss : 0.133356, loss_ce: 0.057843
2022-01-10 02:28:09,154 iteration 624 : loss : 0.076509, loss_ce: 0.032394
2022-01-10 02:28:10,682 iteration 625 : loss : 0.087012, loss_ce: 0.040477
2022-01-10 02:28:12,079 iteration 626 : loss : 0.080531, loss_ce: 0.032560
2022-01-10 02:28:13,425 iteration 627 : loss : 0.101631, loss_ce: 0.037310
2022-01-10 02:28:14,754 iteration 628 : loss : 0.123409, loss_ce: 0.044208
2022-01-10 02:28:16,178 iteration 629 : loss : 0.058798, loss_ce: 0.025709
  9%|██▊                           | 37/400 [16:22<2:41:53, 26.76s/it]2022-01-10 02:28:17,550 iteration 630 : loss : 0.080366, loss_ce: 0.033857
2022-01-10 02:28:18,953 iteration 631 : loss : 0.094485, loss_ce: 0.035403
2022-01-10 02:28:20,295 iteration 632 : loss : 0.060861, loss_ce: 0.024431
2022-01-10 02:28:21,729 iteration 633 : loss : 0.096912, loss_ce: 0.049280
2022-01-10 02:28:23,152 iteration 634 : loss : 0.072717, loss_ce: 0.032399
2022-01-10 02:28:24,593 iteration 635 : loss : 0.098789, loss_ce: 0.045357
2022-01-10 02:28:25,979 iteration 636 : loss : 0.145182, loss_ce: 0.047029
2022-01-10 02:28:27,351 iteration 637 : loss : 0.085277, loss_ce: 0.040158
2022-01-10 02:28:28,717 iteration 638 : loss : 0.056933, loss_ce: 0.026920
2022-01-10 02:28:30,167 iteration 639 : loss : 0.081561, loss_ce: 0.044279
2022-01-10 02:28:31,620 iteration 640 : loss : 0.086955, loss_ce: 0.029374
2022-01-10 02:28:32,980 iteration 641 : loss : 0.078744, loss_ce: 0.029239
2022-01-10 02:28:34,396 iteration 642 : loss : 0.067492, loss_ce: 0.024954
2022-01-10 02:28:35,780 iteration 643 : loss : 0.088295, loss_ce: 0.031107
2022-01-10 02:28:37,179 iteration 644 : loss : 0.061510, loss_ce: 0.022633
2022-01-10 02:28:38,554 iteration 645 : loss : 0.120807, loss_ce: 0.051440
2022-01-10 02:28:39,995 iteration 646 : loss : 0.078462, loss_ce: 0.032454
 10%|██▊                           | 38/400 [16:46<2:36:08, 25.88s/it]2022-01-10 02:28:41,431 iteration 647 : loss : 0.092831, loss_ce: 0.036755
2022-01-10 02:28:42,880 iteration 648 : loss : 0.113032, loss_ce: 0.047443
2022-01-10 02:28:44,310 iteration 649 : loss : 0.092621, loss_ce: 0.039657
2022-01-10 02:28:45,654 iteration 650 : loss : 0.070522, loss_ce: 0.034500
2022-01-10 02:28:47,063 iteration 651 : loss : 0.095420, loss_ce: 0.037810
2022-01-10 02:28:48,410 iteration 652 : loss : 0.098782, loss_ce: 0.032690
2022-01-10 02:28:49,823 iteration 653 : loss : 0.125320, loss_ce: 0.038667
2022-01-10 02:28:51,290 iteration 654 : loss : 0.089011, loss_ce: 0.030014
2022-01-10 02:28:52,727 iteration 655 : loss : 0.054454, loss_ce: 0.023101
2022-01-10 02:28:54,075 iteration 656 : loss : 0.059753, loss_ce: 0.027486
2022-01-10 02:28:55,484 iteration 657 : loss : 0.073597, loss_ce: 0.031911
2022-01-10 02:28:56,940 iteration 658 : loss : 0.141962, loss_ce: 0.051041
2022-01-10 02:28:58,401 iteration 659 : loss : 0.082395, loss_ce: 0.036664
2022-01-10 02:28:59,827 iteration 660 : loss : 0.064055, loss_ce: 0.023141
2022-01-10 02:29:01,244 iteration 661 : loss : 0.090668, loss_ce: 0.046648
2022-01-10 02:29:02,619 iteration 662 : loss : 0.058416, loss_ce: 0.028397
2022-01-10 02:29:04,021 iteration 663 : loss : 0.105235, loss_ce: 0.035226
 10%|██▉                           | 39/400 [17:10<2:32:21, 25.32s/it]2022-01-10 02:29:05,459 iteration 664 : loss : 0.091431, loss_ce: 0.049936
2022-01-10 02:29:06,852 iteration 665 : loss : 0.079101, loss_ce: 0.038088
2022-01-10 02:29:08,271 iteration 666 : loss : 0.080921, loss_ce: 0.031091
2022-01-10 02:29:09,635 iteration 667 : loss : 0.046877, loss_ce: 0.021600
2022-01-10 02:29:11,095 iteration 668 : loss : 0.070316, loss_ce: 0.028276
2022-01-10 02:29:12,470 iteration 669 : loss : 0.070036, loss_ce: 0.033454
2022-01-10 02:29:13,847 iteration 670 : loss : 0.064164, loss_ce: 0.031831
2022-01-10 02:29:15,295 iteration 671 : loss : 0.099794, loss_ce: 0.032766
2022-01-10 02:29:16,753 iteration 672 : loss : 0.094861, loss_ce: 0.036889
2022-01-10 02:29:18,116 iteration 673 : loss : 0.068067, loss_ce: 0.030908
2022-01-10 02:29:19,554 iteration 674 : loss : 0.106667, loss_ce: 0.049824
2022-01-10 02:29:21,005 iteration 675 : loss : 0.092817, loss_ce: 0.038147
2022-01-10 02:29:22,395 iteration 676 : loss : 0.091722, loss_ce: 0.030990
2022-01-10 02:29:23,816 iteration 677 : loss : 0.087519, loss_ce: 0.037259
2022-01-10 02:29:25,217 iteration 678 : loss : 0.065214, loss_ce: 0.024279
2022-01-10 02:29:26,577 iteration 679 : loss : 0.119796, loss_ce: 0.057627
2022-01-10 02:29:26,577 Training Data Eval:
2022-01-10 02:29:33,631   Average segmentation loss on training set: 0.0637
2022-01-10 02:29:33,631 Validation Data Eval:
2022-01-10 02:29:36,121   Average segmentation loss on validation set: 0.1001
2022-01-10 02:29:42,058 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 02:29:43,524 iteration 680 : loss : 0.142519, loss_ce: 0.032797
 10%|███                           | 40/400 [17:50<2:57:26, 29.57s/it]2022-01-10 02:29:45,034 iteration 681 : loss : 0.082745, loss_ce: 0.034067
2022-01-10 02:29:46,402 iteration 682 : loss : 0.049568, loss_ce: 0.018989
2022-01-10 02:29:47,827 iteration 683 : loss : 0.115862, loss_ce: 0.058645
2022-01-10 02:29:49,262 iteration 684 : loss : 0.078353, loss_ce: 0.032202
2022-01-10 02:29:50,723 iteration 685 : loss : 0.077253, loss_ce: 0.031989
2022-01-10 02:29:52,047 iteration 686 : loss : 0.090579, loss_ce: 0.033645
2022-01-10 02:29:53,412 iteration 687 : loss : 0.057321, loss_ce: 0.021837
2022-01-10 02:29:54,879 iteration 688 : loss : 0.094760, loss_ce: 0.040231
2022-01-10 02:29:56,295 iteration 689 : loss : 0.072186, loss_ce: 0.033717
2022-01-10 02:29:57,658 iteration 690 : loss : 0.103928, loss_ce: 0.051573
2022-01-10 02:29:59,151 iteration 691 : loss : 0.090034, loss_ce: 0.038713
2022-01-10 02:30:00,614 iteration 692 : loss : 0.105169, loss_ce: 0.039887
2022-01-10 02:30:02,068 iteration 693 : loss : 0.101692, loss_ce: 0.040664
2022-01-10 02:30:03,486 iteration 694 : loss : 0.097906, loss_ce: 0.045642
2022-01-10 02:30:04,897 iteration 695 : loss : 0.092065, loss_ce: 0.031206
2022-01-10 02:30:06,296 iteration 696 : loss : 0.092636, loss_ce: 0.040877
2022-01-10 02:30:07,714 iteration 697 : loss : 0.100015, loss_ce: 0.037682
 10%|███                           | 41/400 [18:14<2:47:17, 27.96s/it]2022-01-10 02:30:09,166 iteration 698 : loss : 0.094158, loss_ce: 0.039362
2022-01-10 02:30:10,574 iteration 699 : loss : 0.064864, loss_ce: 0.022194
2022-01-10 02:30:11,988 iteration 700 : loss : 0.096138, loss_ce: 0.037702
2022-01-10 02:30:13,382 iteration 701 : loss : 0.100253, loss_ce: 0.040622
2022-01-10 02:30:14,829 iteration 702 : loss : 0.092547, loss_ce: 0.031602
2022-01-10 02:30:16,233 iteration 703 : loss : 0.078936, loss_ce: 0.029915
2022-01-10 02:30:17,626 iteration 704 : loss : 0.089031, loss_ce: 0.030624
2022-01-10 02:30:18,989 iteration 705 : loss : 0.113061, loss_ce: 0.057507
2022-01-10 02:30:20,441 iteration 706 : loss : 0.071836, loss_ce: 0.031182
2022-01-10 02:30:21,873 iteration 707 : loss : 0.109087, loss_ce: 0.059314
2022-01-10 02:30:23,296 iteration 708 : loss : 0.085712, loss_ce: 0.035803
2022-01-10 02:30:24,729 iteration 709 : loss : 0.067201, loss_ce: 0.033118
2022-01-10 02:30:26,151 iteration 710 : loss : 0.082265, loss_ce: 0.030321
2022-01-10 02:30:27,604 iteration 711 : loss : 0.155749, loss_ce: 0.053861
2022-01-10 02:30:29,071 iteration 712 : loss : 0.075809, loss_ce: 0.037885
2022-01-10 02:30:30,449 iteration 713 : loss : 0.079382, loss_ce: 0.036056
2022-01-10 02:30:31,853 iteration 714 : loss : 0.081405, loss_ce: 0.038645
 10%|███▏                          | 42/400 [18:38<2:39:59, 26.81s/it]2022-01-10 02:30:33,305 iteration 715 : loss : 0.056610, loss_ce: 0.026238
2022-01-10 02:30:34,704 iteration 716 : loss : 0.111852, loss_ce: 0.032721
2022-01-10 02:30:36,096 iteration 717 : loss : 0.065999, loss_ce: 0.024868
2022-01-10 02:30:37,415 iteration 718 : loss : 0.071196, loss_ce: 0.025531
2022-01-10 02:30:38,813 iteration 719 : loss : 0.070884, loss_ce: 0.037268
2022-01-10 02:30:40,138 iteration 720 : loss : 0.066613, loss_ce: 0.028709
2022-01-10 02:30:41,662 iteration 721 : loss : 0.121836, loss_ce: 0.043858
2022-01-10 02:30:43,070 iteration 722 : loss : 0.062295, loss_ce: 0.030478
2022-01-10 02:30:44,491 iteration 723 : loss : 0.086730, loss_ce: 0.035369
2022-01-10 02:30:45,970 iteration 724 : loss : 0.081906, loss_ce: 0.032361
2022-01-10 02:30:47,333 iteration 725 : loss : 0.062547, loss_ce: 0.029948
2022-01-10 02:30:48,750 iteration 726 : loss : 0.092992, loss_ce: 0.048453
2022-01-10 02:30:50,126 iteration 727 : loss : 0.050890, loss_ce: 0.021230
2022-01-10 02:30:51,513 iteration 728 : loss : 0.076766, loss_ce: 0.022806
2022-01-10 02:30:52,898 iteration 729 : loss : 0.090375, loss_ce: 0.040161
2022-01-10 02:30:54,321 iteration 730 : loss : 0.086683, loss_ce: 0.034739
2022-01-10 02:30:55,692 iteration 731 : loss : 0.100315, loss_ce: 0.038038
 11%|███▏                          | 43/400 [19:02<2:34:14, 25.92s/it]2022-01-10 02:30:57,188 iteration 732 : loss : 0.068738, loss_ce: 0.026861
2022-01-10 02:30:58,615 iteration 733 : loss : 0.063137, loss_ce: 0.027523
2022-01-10 02:31:00,037 iteration 734 : loss : 0.115995, loss_ce: 0.053365
2022-01-10 02:31:01,473 iteration 735 : loss : 0.088588, loss_ce: 0.030988
2022-01-10 02:31:02,909 iteration 736 : loss : 0.060736, loss_ce: 0.022704
2022-01-10 02:31:04,264 iteration 737 : loss : 0.051981, loss_ce: 0.023951
2022-01-10 02:31:05,733 iteration 738 : loss : 0.071554, loss_ce: 0.027393
2022-01-10 02:31:07,163 iteration 739 : loss : 0.094119, loss_ce: 0.049102
2022-01-10 02:31:08,573 iteration 740 : loss : 0.054910, loss_ce: 0.016768
2022-01-10 02:31:10,010 iteration 741 : loss : 0.074369, loss_ce: 0.029772
2022-01-10 02:31:11,453 iteration 742 : loss : 0.064368, loss_ce: 0.024538
2022-01-10 02:31:12,848 iteration 743 : loss : 0.057636, loss_ce: 0.020089
2022-01-10 02:31:14,272 iteration 744 : loss : 0.059454, loss_ce: 0.026188
2022-01-10 02:31:15,679 iteration 745 : loss : 0.094542, loss_ce: 0.034044
2022-01-10 02:31:17,065 iteration 746 : loss : 0.134757, loss_ce: 0.047135
2022-01-10 02:31:18,458 iteration 747 : loss : 0.068298, loss_ce: 0.030289
2022-01-10 02:31:19,887 iteration 748 : loss : 0.106232, loss_ce: 0.043597
 11%|███▎                          | 44/400 [19:26<2:30:42, 25.40s/it]2022-01-10 02:31:21,321 iteration 749 : loss : 0.096654, loss_ce: 0.034801
2022-01-10 02:31:22,707 iteration 750 : loss : 0.047303, loss_ce: 0.015862
2022-01-10 02:31:24,151 iteration 751 : loss : 0.088551, loss_ce: 0.035983
2022-01-10 02:31:25,529 iteration 752 : loss : 0.074077, loss_ce: 0.028840
2022-01-10 02:31:26,968 iteration 753 : loss : 0.058357, loss_ce: 0.023655
2022-01-10 02:31:28,371 iteration 754 : loss : 0.046179, loss_ce: 0.016186
2022-01-10 02:31:29,791 iteration 755 : loss : 0.053238, loss_ce: 0.023199
2022-01-10 02:31:31,116 iteration 756 : loss : 0.058904, loss_ce: 0.026310
2022-01-10 02:31:32,463 iteration 757 : loss : 0.059925, loss_ce: 0.023641
2022-01-10 02:31:33,872 iteration 758 : loss : 0.087663, loss_ce: 0.036172
2022-01-10 02:31:35,320 iteration 759 : loss : 0.102370, loss_ce: 0.040410
2022-01-10 02:31:36,741 iteration 760 : loss : 0.067851, loss_ce: 0.021317
2022-01-10 02:31:38,178 iteration 761 : loss : 0.083948, loss_ce: 0.032981
2022-01-10 02:31:39,512 iteration 762 : loss : 0.058301, loss_ce: 0.024931
2022-01-10 02:31:40,912 iteration 763 : loss : 0.066002, loss_ce: 0.028399
2022-01-10 02:31:42,239 iteration 764 : loss : 0.065493, loss_ce: 0.023802
2022-01-10 02:31:42,239 Training Data Eval:
2022-01-10 02:31:49,293   Average segmentation loss on training set: 0.0523
2022-01-10 02:31:49,293 Validation Data Eval:
2022-01-10 02:31:51,725   Average segmentation loss on validation set: 0.1015
2022-01-10 02:31:53,052 iteration 765 : loss : 0.066140, loss_ce: 0.025630
 11%|███▍                          | 45/400 [19:59<2:44:04, 27.73s/it]2022-01-10 02:31:54,579 iteration 766 : loss : 0.062555, loss_ce: 0.022155
2022-01-10 02:31:56,121 iteration 767 : loss : 0.074304, loss_ce: 0.034087
2022-01-10 02:31:57,559 iteration 768 : loss : 0.085976, loss_ce: 0.032677
2022-01-10 02:31:59,007 iteration 769 : loss : 0.078521, loss_ce: 0.035608
2022-01-10 02:32:00,484 iteration 770 : loss : 0.069644, loss_ce: 0.038185
2022-01-10 02:32:01,916 iteration 771 : loss : 0.061557, loss_ce: 0.031241
2022-01-10 02:32:03,270 iteration 772 : loss : 0.051743, loss_ce: 0.022713
2022-01-10 02:32:04,673 iteration 773 : loss : 0.119020, loss_ce: 0.044174
2022-01-10 02:32:06,149 iteration 774 : loss : 0.065773, loss_ce: 0.031112
2022-01-10 02:32:07,496 iteration 775 : loss : 0.060649, loss_ce: 0.025441
2022-01-10 02:32:08,875 iteration 776 : loss : 0.103056, loss_ce: 0.025900
2022-01-10 02:32:10,385 iteration 777 : loss : 0.049671, loss_ce: 0.021479
2022-01-10 02:32:11,805 iteration 778 : loss : 0.058617, loss_ce: 0.019846
2022-01-10 02:32:13,208 iteration 779 : loss : 0.095104, loss_ce: 0.033031
2022-01-10 02:32:14,713 iteration 780 : loss : 0.097457, loss_ce: 0.038073
2022-01-10 02:32:16,165 iteration 781 : loss : 0.061227, loss_ce: 0.023181
2022-01-10 02:32:17,554 iteration 782 : loss : 0.069571, loss_ce: 0.030934
 12%|███▍                          | 46/400 [20:24<2:37:54, 26.76s/it]2022-01-10 02:32:19,013 iteration 783 : loss : 0.057229, loss_ce: 0.027079
2022-01-10 02:32:20,367 iteration 784 : loss : 0.062687, loss_ce: 0.020220
2022-01-10 02:32:21,833 iteration 785 : loss : 0.089816, loss_ce: 0.031262
2022-01-10 02:32:23,224 iteration 786 : loss : 0.071341, loss_ce: 0.026778
2022-01-10 02:32:24,673 iteration 787 : loss : 0.099111, loss_ce: 0.037671
2022-01-10 02:32:26,036 iteration 788 : loss : 0.083343, loss_ce: 0.036251
2022-01-10 02:32:27,483 iteration 789 : loss : 0.113004, loss_ce: 0.048119
2022-01-10 02:32:28,953 iteration 790 : loss : 0.063510, loss_ce: 0.027381
2022-01-10 02:32:30,307 iteration 791 : loss : 0.059194, loss_ce: 0.029513
2022-01-10 02:32:31,779 iteration 792 : loss : 0.084182, loss_ce: 0.034358
2022-01-10 02:32:33,219 iteration 793 : loss : 0.100958, loss_ce: 0.041177
2022-01-10 02:32:34,672 iteration 794 : loss : 0.055570, loss_ce: 0.023138
2022-01-10 02:32:36,115 iteration 795 : loss : 0.082076, loss_ce: 0.032792
2022-01-10 02:32:37,615 iteration 796 : loss : 0.061076, loss_ce: 0.026167
2022-01-10 02:32:39,025 iteration 797 : loss : 0.053049, loss_ce: 0.025701
2022-01-10 02:32:40,427 iteration 798 : loss : 0.074375, loss_ce: 0.030837
2022-01-10 02:32:41,948 iteration 799 : loss : 0.148629, loss_ce: 0.051009
 12%|███▌                          | 47/400 [20:48<2:33:16, 26.05s/it]2022-01-10 02:32:43,437 iteration 800 : loss : 0.079319, loss_ce: 0.041042
2022-01-10 02:32:44,830 iteration 801 : loss : 0.099180, loss_ce: 0.029130
2022-01-10 02:32:46,279 iteration 802 : loss : 0.097180, loss_ce: 0.046010
2022-01-10 02:32:47,623 iteration 803 : loss : 0.050780, loss_ce: 0.020042
2022-01-10 02:32:49,114 iteration 804 : loss : 0.096599, loss_ce: 0.031943
2022-01-10 02:32:50,503 iteration 805 : loss : 0.064478, loss_ce: 0.027105
2022-01-10 02:32:51,846 iteration 806 : loss : 0.071504, loss_ce: 0.027573
2022-01-10 02:32:53,249 iteration 807 : loss : 0.082665, loss_ce: 0.041784
2022-01-10 02:32:54,656 iteration 808 : loss : 0.072687, loss_ce: 0.022645
2022-01-10 02:32:56,116 iteration 809 : loss : 0.067707, loss_ce: 0.029088
2022-01-10 02:32:57,480 iteration 810 : loss : 0.079710, loss_ce: 0.032332
2022-01-10 02:32:58,916 iteration 811 : loss : 0.076207, loss_ce: 0.037537
2022-01-10 02:33:00,277 iteration 812 : loss : 0.074922, loss_ce: 0.033485
2022-01-10 02:33:01,745 iteration 813 : loss : 0.082270, loss_ce: 0.035865
2022-01-10 02:33:03,244 iteration 814 : loss : 0.154650, loss_ce: 0.042519
2022-01-10 02:33:04,531 iteration 815 : loss : 0.059265, loss_ce: 0.027067
2022-01-10 02:33:06,001 iteration 816 : loss : 0.089785, loss_ce: 0.035297
 12%|███▌                          | 48/400 [21:12<2:29:19, 25.45s/it]2022-01-10 02:33:07,430 iteration 817 : loss : 0.065486, loss_ce: 0.028956
2022-01-10 02:33:08,819 iteration 818 : loss : 0.067793, loss_ce: 0.031202
2022-01-10 02:33:10,354 iteration 819 : loss : 0.077666, loss_ce: 0.032853
2022-01-10 02:33:11,822 iteration 820 : loss : 0.086222, loss_ce: 0.038346
2022-01-10 02:33:13,265 iteration 821 : loss : 0.099911, loss_ce: 0.040468
2022-01-10 02:33:14,681 iteration 822 : loss : 0.074389, loss_ce: 0.032316
2022-01-10 02:33:16,120 iteration 823 : loss : 0.072868, loss_ce: 0.029528
2022-01-10 02:33:17,472 iteration 824 : loss : 0.070958, loss_ce: 0.025612
2022-01-10 02:33:18,874 iteration 825 : loss : 0.117556, loss_ce: 0.035855
2022-01-10 02:33:20,223 iteration 826 : loss : 0.081606, loss_ce: 0.035048
2022-01-10 02:33:21,628 iteration 827 : loss : 0.068137, loss_ce: 0.023865
2022-01-10 02:33:22,962 iteration 828 : loss : 0.067349, loss_ce: 0.025930
2022-01-10 02:33:24,436 iteration 829 : loss : 0.095595, loss_ce: 0.047971
2022-01-10 02:33:25,828 iteration 830 : loss : 0.078976, loss_ce: 0.035826
2022-01-10 02:33:27,268 iteration 831 : loss : 0.115904, loss_ce: 0.051092
2022-01-10 02:33:28,690 iteration 832 : loss : 0.075671, loss_ce: 0.025854
2022-01-10 02:33:30,112 iteration 833 : loss : 0.071001, loss_ce: 0.029791
 12%|███▋                          | 49/400 [21:36<2:26:32, 25.05s/it]2022-01-10 02:33:31,505 iteration 834 : loss : 0.081597, loss_ce: 0.031437
2022-01-10 02:33:32,828 iteration 835 : loss : 0.047084, loss_ce: 0.017146
2022-01-10 02:33:34,295 iteration 836 : loss : 0.067669, loss_ce: 0.023951
2022-01-10 02:33:35,676 iteration 837 : loss : 0.046976, loss_ce: 0.018654
2022-01-10 02:33:37,112 iteration 838 : loss : 0.091855, loss_ce: 0.041837
2022-01-10 02:33:38,443 iteration 839 : loss : 0.057572, loss_ce: 0.021899
2022-01-10 02:33:39,784 iteration 840 : loss : 0.048189, loss_ce: 0.022533
2022-01-10 02:33:41,144 iteration 841 : loss : 0.070069, loss_ce: 0.029557
2022-01-10 02:33:42,623 iteration 842 : loss : 0.093778, loss_ce: 0.035286
2022-01-10 02:33:43,989 iteration 843 : loss : 0.082321, loss_ce: 0.028079
2022-01-10 02:33:45,437 iteration 844 : loss : 0.046235, loss_ce: 0.017526
2022-01-10 02:33:46,822 iteration 845 : loss : 0.061056, loss_ce: 0.029828
2022-01-10 02:33:48,211 iteration 846 : loss : 0.046541, loss_ce: 0.022242
2022-01-10 02:33:49,683 iteration 847 : loss : 0.085531, loss_ce: 0.031913
2022-01-10 02:33:51,080 iteration 848 : loss : 0.078774, loss_ce: 0.040518
2022-01-10 02:33:52,461 iteration 849 : loss : 0.064139, loss_ce: 0.025146
2022-01-10 02:33:52,461 Training Data Eval:
2022-01-10 02:33:59,503   Average segmentation loss on training set: 0.0835
2022-01-10 02:33:59,503 Validation Data Eval:
2022-01-10 02:34:01,937   Average segmentation loss on validation set: 0.2032
2022-01-10 02:34:03,289 iteration 850 : loss : 0.080005, loss_ce: 0.041190
 12%|███▊                          | 50/400 [22:09<2:40:20, 27.49s/it]2022-01-10 02:34:04,740 iteration 851 : loss : 0.066583, loss_ce: 0.027772
2022-01-10 02:34:06,133 iteration 852 : loss : 0.067351, loss_ce: 0.022545
2022-01-10 02:34:07,544 iteration 853 : loss : 0.073494, loss_ce: 0.026942
2022-01-10 02:34:08,964 iteration 854 : loss : 0.083496, loss_ce: 0.046455
2022-01-10 02:34:10,335 iteration 855 : loss : 0.074349, loss_ce: 0.036794
2022-01-10 02:34:11,777 iteration 856 : loss : 0.067747, loss_ce: 0.032938
2022-01-10 02:34:13,164 iteration 857 : loss : 0.080490, loss_ce: 0.031353
2022-01-10 02:34:14,651 iteration 858 : loss : 0.047105, loss_ce: 0.018612
2022-01-10 02:34:15,935 iteration 859 : loss : 0.046750, loss_ce: 0.017296
2022-01-10 02:34:17,324 iteration 860 : loss : 0.053417, loss_ce: 0.020586
2022-01-10 02:34:18,725 iteration 861 : loss : 0.063938, loss_ce: 0.019007
2022-01-10 02:34:20,145 iteration 862 : loss : 0.070012, loss_ce: 0.033904
2022-01-10 02:34:21,543 iteration 863 : loss : 0.099126, loss_ce: 0.038716
2022-01-10 02:34:22,930 iteration 864 : loss : 0.056941, loss_ce: 0.025045
2022-01-10 02:34:24,343 iteration 865 : loss : 0.070404, loss_ce: 0.035150
2022-01-10 02:34:25,715 iteration 866 : loss : 0.055619, loss_ce: 0.025817
2022-01-10 02:34:27,151 iteration 867 : loss : 0.089911, loss_ce: 0.043142
 13%|███▊                          | 51/400 [22:33<2:33:34, 26.40s/it]2022-01-10 02:34:28,599 iteration 868 : loss : 0.066739, loss_ce: 0.028293
2022-01-10 02:34:30,045 iteration 869 : loss : 0.062090, loss_ce: 0.027189
2022-01-10 02:34:31,374 iteration 870 : loss : 0.061545, loss_ce: 0.027730
2022-01-10 02:34:32,756 iteration 871 : loss : 0.083452, loss_ce: 0.041821
2022-01-10 02:34:34,141 iteration 872 : loss : 0.073823, loss_ce: 0.032018
2022-01-10 02:34:35,564 iteration 873 : loss : 0.081074, loss_ce: 0.041669
2022-01-10 02:34:36,976 iteration 874 : loss : 0.055893, loss_ce: 0.023369
2022-01-10 02:34:38,387 iteration 875 : loss : 0.077525, loss_ce: 0.027377
2022-01-10 02:34:39,797 iteration 876 : loss : 0.051636, loss_ce: 0.019663
2022-01-10 02:34:41,269 iteration 877 : loss : 0.054340, loss_ce: 0.022220
2022-01-10 02:34:42,642 iteration 878 : loss : 0.135664, loss_ce: 0.034099
2022-01-10 02:34:44,037 iteration 879 : loss : 0.097389, loss_ce: 0.031557
2022-01-10 02:34:45,401 iteration 880 : loss : 0.057012, loss_ce: 0.026139
2022-01-10 02:34:46,868 iteration 881 : loss : 0.068032, loss_ce: 0.023021
2022-01-10 02:34:48,297 iteration 882 : loss : 0.086380, loss_ce: 0.031035
2022-01-10 02:34:49,671 iteration 883 : loss : 0.053098, loss_ce: 0.018879
2022-01-10 02:34:51,112 iteration 884 : loss : 0.094866, loss_ce: 0.042996
 13%|███▉                          | 52/400 [22:57<2:28:52, 25.67s/it]2022-01-10 02:34:52,589 iteration 885 : loss : 0.063471, loss_ce: 0.029582
2022-01-10 02:34:53,999 iteration 886 : loss : 0.102994, loss_ce: 0.051271
2022-01-10 02:34:55,405 iteration 887 : loss : 0.084013, loss_ce: 0.035402
2022-01-10 02:34:56,828 iteration 888 : loss : 0.098031, loss_ce: 0.043658
2022-01-10 02:34:58,243 iteration 889 : loss : 0.133071, loss_ce: 0.041324
2022-01-10 02:34:59,627 iteration 890 : loss : 0.057260, loss_ce: 0.021806
2022-01-10 02:35:01,041 iteration 891 : loss : 0.064264, loss_ce: 0.042570
2022-01-10 02:35:02,447 iteration 892 : loss : 0.054727, loss_ce: 0.022059
2022-01-10 02:35:03,842 iteration 893 : loss : 0.059006, loss_ce: 0.028080
2022-01-10 02:35:05,226 iteration 894 : loss : 0.047721, loss_ce: 0.015934
2022-01-10 02:35:06,612 iteration 895 : loss : 0.072642, loss_ce: 0.025399
2022-01-10 02:35:07,983 iteration 896 : loss : 0.067399, loss_ce: 0.031242
2022-01-10 02:35:09,524 iteration 897 : loss : 0.068169, loss_ce: 0.026136
2022-01-10 02:35:10,911 iteration 898 : loss : 0.070751, loss_ce: 0.027298
2022-01-10 02:35:12,368 iteration 899 : loss : 0.079097, loss_ce: 0.030145
2022-01-10 02:35:13,802 iteration 900 : loss : 0.059303, loss_ce: 0.021993
2022-01-10 02:35:15,180 iteration 901 : loss : 0.061798, loss_ce: 0.026260
 13%|███▉                          | 53/400 [23:21<2:25:41, 25.19s/it]2022-01-10 02:35:16,719 iteration 902 : loss : 0.072704, loss_ce: 0.036488
2022-01-10 02:35:18,153 iteration 903 : loss : 0.073795, loss_ce: 0.025852
2022-01-10 02:35:19,551 iteration 904 : loss : 0.057578, loss_ce: 0.024258
2022-01-10 02:35:20,939 iteration 905 : loss : 0.053659, loss_ce: 0.023711
2022-01-10 02:35:22,435 iteration 906 : loss : 0.082768, loss_ce: 0.034883
2022-01-10 02:35:23,803 iteration 907 : loss : 0.044057, loss_ce: 0.017922
2022-01-10 02:35:25,183 iteration 908 : loss : 0.061161, loss_ce: 0.032836
2022-01-10 02:35:26,624 iteration 909 : loss : 0.067250, loss_ce: 0.030275
2022-01-10 02:35:28,112 iteration 910 : loss : 0.060478, loss_ce: 0.026005
2022-01-10 02:35:29,519 iteration 911 : loss : 0.062602, loss_ce: 0.023865
2022-01-10 02:35:30,964 iteration 912 : loss : 0.075274, loss_ce: 0.028437
2022-01-10 02:35:32,375 iteration 913 : loss : 0.095201, loss_ce: 0.035768
2022-01-10 02:35:33,723 iteration 914 : loss : 0.090409, loss_ce: 0.031929
2022-01-10 02:35:35,141 iteration 915 : loss : 0.072260, loss_ce: 0.027041
2022-01-10 02:35:36,612 iteration 916 : loss : 0.106537, loss_ce: 0.039854
2022-01-10 02:35:38,034 iteration 917 : loss : 0.072512, loss_ce: 0.030247
2022-01-10 02:35:39,515 iteration 918 : loss : 0.070797, loss_ce: 0.020615
 14%|████                          | 54/400 [23:46<2:23:47, 24.93s/it]2022-01-10 02:35:40,960 iteration 919 : loss : 0.050162, loss_ce: 0.016142
2022-01-10 02:35:42,426 iteration 920 : loss : 0.065170, loss_ce: 0.029167
2022-01-10 02:35:43,783 iteration 921 : loss : 0.053808, loss_ce: 0.021512
2022-01-10 02:35:45,216 iteration 922 : loss : 0.069243, loss_ce: 0.023791
2022-01-10 02:35:46,566 iteration 923 : loss : 0.049894, loss_ce: 0.017254
2022-01-10 02:35:47,998 iteration 924 : loss : 0.066558, loss_ce: 0.020558
2022-01-10 02:35:49,438 iteration 925 : loss : 0.067241, loss_ce: 0.018856
2022-01-10 02:35:50,863 iteration 926 : loss : 0.050205, loss_ce: 0.020861
2022-01-10 02:35:52,206 iteration 927 : loss : 0.097676, loss_ce: 0.028335
2022-01-10 02:35:53,677 iteration 928 : loss : 0.090643, loss_ce: 0.040673
2022-01-10 02:35:55,066 iteration 929 : loss : 0.047099, loss_ce: 0.018420
2022-01-10 02:35:56,420 iteration 930 : loss : 0.069454, loss_ce: 0.027122
2022-01-10 02:35:57,793 iteration 931 : loss : 0.100552, loss_ce: 0.057663
2022-01-10 02:35:59,199 iteration 932 : loss : 0.060300, loss_ce: 0.030231
2022-01-10 02:36:00,637 iteration 933 : loss : 0.065048, loss_ce: 0.035175
2022-01-10 02:36:02,006 iteration 934 : loss : 0.076344, loss_ce: 0.027452
2022-01-10 02:36:02,006 Training Data Eval:
2022-01-10 02:36:09,067   Average segmentation loss on training set: 0.0780
2022-01-10 02:36:09,068 Validation Data Eval:
2022-01-10 02:36:11,522   Average segmentation loss on validation set: 0.0878
2022-01-10 02:36:17,259 Found new lowest validation loss at iteration 934! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 02:36:18,722 iteration 935 : loss : 0.063028, loss_ce: 0.029354
 14%|████▏                         | 55/400 [24:25<2:47:58, 29.21s/it]2022-01-10 02:36:20,189 iteration 936 : loss : 0.062724, loss_ce: 0.030235
2022-01-10 02:36:21,577 iteration 937 : loss : 0.105526, loss_ce: 0.038217
2022-01-10 02:36:22,996 iteration 938 : loss : 0.048812, loss_ce: 0.022335
2022-01-10 02:36:24,496 iteration 939 : loss : 0.100239, loss_ce: 0.033611
2022-01-10 02:36:25,842 iteration 940 : loss : 0.077055, loss_ce: 0.030263
2022-01-10 02:36:27,217 iteration 941 : loss : 0.076942, loss_ce: 0.031848
2022-01-10 02:36:28,615 iteration 942 : loss : 0.069243, loss_ce: 0.027971
2022-01-10 02:36:30,042 iteration 943 : loss : 0.087023, loss_ce: 0.029122
2022-01-10 02:36:31,447 iteration 944 : loss : 0.095144, loss_ce: 0.036478
2022-01-10 02:36:32,880 iteration 945 : loss : 0.083863, loss_ce: 0.042276
2022-01-10 02:36:34,281 iteration 946 : loss : 0.053294, loss_ce: 0.021288
2022-01-10 02:36:35,690 iteration 947 : loss : 0.071691, loss_ce: 0.024509
2022-01-10 02:36:37,161 iteration 948 : loss : 0.101755, loss_ce: 0.045040
2022-01-10 02:36:38,631 iteration 949 : loss : 0.084146, loss_ce: 0.036618
2022-01-10 02:36:40,115 iteration 950 : loss : 0.065950, loss_ce: 0.027714
2022-01-10 02:36:41,508 iteration 951 : loss : 0.044525, loss_ce: 0.017709
2022-01-10 02:36:42,956 iteration 952 : loss : 0.076724, loss_ce: 0.025482
 14%|████▏                         | 56/400 [24:49<2:38:56, 27.72s/it]2022-01-10 02:36:44,397 iteration 953 : loss : 0.077680, loss_ce: 0.029727
2022-01-10 02:36:45,777 iteration 954 : loss : 0.047788, loss_ce: 0.017485
2022-01-10 02:36:47,208 iteration 955 : loss : 0.103841, loss_ce: 0.043057
2022-01-10 02:36:48,639 iteration 956 : loss : 0.050398, loss_ce: 0.017034
2022-01-10 02:36:50,051 iteration 957 : loss : 0.081604, loss_ce: 0.031318
2022-01-10 02:36:51,482 iteration 958 : loss : 0.076478, loss_ce: 0.030406
2022-01-10 02:36:52,945 iteration 959 : loss : 0.071609, loss_ce: 0.032108
2022-01-10 02:36:54,383 iteration 960 : loss : 0.063433, loss_ce: 0.029436
2022-01-10 02:36:55,807 iteration 961 : loss : 0.070156, loss_ce: 0.028148
2022-01-10 02:36:57,250 iteration 962 : loss : 0.098089, loss_ce: 0.028348
2022-01-10 02:36:58,604 iteration 963 : loss : 0.062015, loss_ce: 0.027396
2022-01-10 02:37:00,035 iteration 964 : loss : 0.076786, loss_ce: 0.021129
2022-01-10 02:37:01,414 iteration 965 : loss : 0.068733, loss_ce: 0.024294
2022-01-10 02:37:02,904 iteration 966 : loss : 0.080563, loss_ce: 0.026874
2022-01-10 02:37:04,330 iteration 967 : loss : 0.094001, loss_ce: 0.042009
2022-01-10 02:37:05,672 iteration 968 : loss : 0.062070, loss_ce: 0.024065
2022-01-10 02:37:07,088 iteration 969 : loss : 0.056971, loss_ce: 0.028297
 14%|████▎                         | 57/400 [25:13<2:32:17, 26.64s/it]2022-01-10 02:37:08,512 iteration 970 : loss : 0.070205, loss_ce: 0.032680
2022-01-10 02:37:09,944 iteration 971 : loss : 0.065207, loss_ce: 0.030185
2022-01-10 02:37:11,346 iteration 972 : loss : 0.061450, loss_ce: 0.024035
2022-01-10 02:37:12,777 iteration 973 : loss : 0.050000, loss_ce: 0.020639
2022-01-10 02:37:14,180 iteration 974 : loss : 0.087279, loss_ce: 0.038636
2022-01-10 02:37:15,550 iteration 975 : loss : 0.078866, loss_ce: 0.027071
2022-01-10 02:37:17,025 iteration 976 : loss : 0.076752, loss_ce: 0.036320
2022-01-10 02:37:18,428 iteration 977 : loss : 0.051991, loss_ce: 0.020684
2022-01-10 02:37:19,860 iteration 978 : loss : 0.052371, loss_ce: 0.020553
2022-01-10 02:37:21,241 iteration 979 : loss : 0.049642, loss_ce: 0.019310
2022-01-10 02:37:22,625 iteration 980 : loss : 0.069732, loss_ce: 0.025263
2022-01-10 02:37:23,998 iteration 981 : loss : 0.036345, loss_ce: 0.014605
2022-01-10 02:37:25,432 iteration 982 : loss : 0.079461, loss_ce: 0.025310
2022-01-10 02:37:26,903 iteration 983 : loss : 0.084718, loss_ce: 0.034500
2022-01-10 02:37:28,378 iteration 984 : loss : 0.057879, loss_ce: 0.028929
2022-01-10 02:37:29,820 iteration 985 : loss : 0.062079, loss_ce: 0.026764
2022-01-10 02:37:31,193 iteration 986 : loss : 0.054598, loss_ce: 0.016558
 14%|████▎                         | 58/400 [25:37<2:27:31, 25.88s/it]2022-01-10 02:37:32,578 iteration 987 : loss : 0.044903, loss_ce: 0.015751
2022-01-10 02:37:33,982 iteration 988 : loss : 0.059441, loss_ce: 0.023002
2022-01-10 02:37:35,374 iteration 989 : loss : 0.045876, loss_ce: 0.016897
2022-01-10 02:37:36,771 iteration 990 : loss : 0.063405, loss_ce: 0.022508
2022-01-10 02:37:38,178 iteration 991 : loss : 0.058509, loss_ce: 0.021444
2022-01-10 02:37:39,551 iteration 992 : loss : 0.046214, loss_ce: 0.017078
2022-01-10 02:37:40,887 iteration 993 : loss : 0.050431, loss_ce: 0.017843
2022-01-10 02:37:42,175 iteration 994 : loss : 0.054291, loss_ce: 0.018955
2022-01-10 02:37:43,644 iteration 995 : loss : 0.074857, loss_ce: 0.035163
2022-01-10 02:37:45,077 iteration 996 : loss : 0.052034, loss_ce: 0.025039
2022-01-10 02:37:46,524 iteration 997 : loss : 0.069151, loss_ce: 0.028026
2022-01-10 02:37:47,961 iteration 998 : loss : 0.045797, loss_ce: 0.017148
2022-01-10 02:37:49,334 iteration 999 : loss : 0.066275, loss_ce: 0.030070
2022-01-10 02:37:50,796 iteration 1000 : loss : 0.060423, loss_ce: 0.029668
2022-01-10 02:37:52,185 iteration 1001 : loss : 0.042736, loss_ce: 0.018586
2022-01-10 02:37:53,650 iteration 1002 : loss : 0.090583, loss_ce: 0.036529
2022-01-10 02:37:55,000 iteration 1003 : loss : 0.122589, loss_ce: 0.038244
 15%|████▍                         | 59/400 [26:01<2:23:32, 25.26s/it]2022-01-10 02:37:56,450 iteration 1004 : loss : 0.062276, loss_ce: 0.021498
2022-01-10 02:37:57,854 iteration 1005 : loss : 0.079997, loss_ce: 0.039893
2022-01-10 02:37:59,347 iteration 1006 : loss : 0.075071, loss_ce: 0.032615
2022-01-10 02:38:00,804 iteration 1007 : loss : 0.075429, loss_ce: 0.031008
2022-01-10 02:38:02,273 iteration 1008 : loss : 0.070042, loss_ce: 0.037915
2022-01-10 02:38:03,652 iteration 1009 : loss : 0.051508, loss_ce: 0.019406
2022-01-10 02:38:04,995 iteration 1010 : loss : 0.052320, loss_ce: 0.015143
2022-01-10 02:38:06,386 iteration 1011 : loss : 0.069807, loss_ce: 0.026181
2022-01-10 02:38:07,844 iteration 1012 : loss : 0.075495, loss_ce: 0.028381
2022-01-10 02:38:09,246 iteration 1013 : loss : 0.050790, loss_ce: 0.019804
2022-01-10 02:38:10,636 iteration 1014 : loss : 0.035641, loss_ce: 0.014400
2022-01-10 02:38:12,167 iteration 1015 : loss : 0.078638, loss_ce: 0.031731
2022-01-10 02:38:13,569 iteration 1016 : loss : 0.051064, loss_ce: 0.019076
2022-01-10 02:38:14,946 iteration 1017 : loss : 0.058831, loss_ce: 0.022486
2022-01-10 02:38:16,414 iteration 1018 : loss : 0.082629, loss_ce: 0.026472
2022-01-10 02:38:17,857 iteration 1019 : loss : 0.063546, loss_ce: 0.022758
2022-01-10 02:38:17,858 Training Data Eval:
2022-01-10 02:38:24,928   Average segmentation loss on training set: 0.0419
2022-01-10 02:38:24,928 Validation Data Eval:
2022-01-10 02:38:27,368   Average segmentation loss on validation set: 0.0820
2022-01-10 02:38:31,954 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 02:38:33,468 iteration 1020 : loss : 0.059731, loss_ce: 0.025707
 15%|████▌                         | 60/400 [26:39<2:45:35, 29.22s/it]2022-01-10 02:38:34,910 iteration 1021 : loss : 0.051665, loss_ce: 0.018536
2022-01-10 02:38:36,398 iteration 1022 : loss : 0.055274, loss_ce: 0.024733
2022-01-10 02:38:37,764 iteration 1023 : loss : 0.071426, loss_ce: 0.027774
2022-01-10 02:38:39,141 iteration 1024 : loss : 0.042508, loss_ce: 0.014903
2022-01-10 02:38:40,591 iteration 1025 : loss : 0.062884, loss_ce: 0.031220
2022-01-10 02:38:42,016 iteration 1026 : loss : 0.099565, loss_ce: 0.028333
2022-01-10 02:38:43,408 iteration 1027 : loss : 0.054511, loss_ce: 0.018490
2022-01-10 02:38:44,893 iteration 1028 : loss : 0.088079, loss_ce: 0.028749
2022-01-10 02:38:46,350 iteration 1029 : loss : 0.068644, loss_ce: 0.021983
2022-01-10 02:38:47,764 iteration 1030 : loss : 0.086857, loss_ce: 0.039147
2022-01-10 02:38:49,273 iteration 1031 : loss : 0.071312, loss_ce: 0.030586
2022-01-10 02:38:50,706 iteration 1032 : loss : 0.074171, loss_ce: 0.036683
2022-01-10 02:38:52,099 iteration 1033 : loss : 0.072384, loss_ce: 0.034432
2022-01-10 02:38:53,465 iteration 1034 : loss : 0.047949, loss_ce: 0.018371
2022-01-10 02:38:54,913 iteration 1035 : loss : 0.058694, loss_ce: 0.025029
2022-01-10 02:38:56,389 iteration 1036 : loss : 0.065211, loss_ce: 0.028220
2022-01-10 02:38:57,726 iteration 1037 : loss : 0.055263, loss_ce: 0.023975
 15%|████▌                         | 61/400 [27:04<2:36:42, 27.73s/it]2022-01-10 02:38:59,190 iteration 1038 : loss : 0.057859, loss_ce: 0.024667
2022-01-10 02:39:00,574 iteration 1039 : loss : 0.054133, loss_ce: 0.022342
2022-01-10 02:39:01,962 iteration 1040 : loss : 0.065327, loss_ce: 0.022324
2022-01-10 02:39:03,364 iteration 1041 : loss : 0.072390, loss_ce: 0.033999
2022-01-10 02:39:04,747 iteration 1042 : loss : 0.055022, loss_ce: 0.029314
2022-01-10 02:39:06,143 iteration 1043 : loss : 0.060024, loss_ce: 0.022668
2022-01-10 02:39:07,605 iteration 1044 : loss : 0.060428, loss_ce: 0.028954
2022-01-10 02:39:09,023 iteration 1045 : loss : 0.079163, loss_ce: 0.034304
2022-01-10 02:39:10,394 iteration 1046 : loss : 0.047482, loss_ce: 0.023934
2022-01-10 02:39:11,697 iteration 1047 : loss : 0.049110, loss_ce: 0.019109
2022-01-10 02:39:13,111 iteration 1048 : loss : 0.045203, loss_ce: 0.021190
2022-01-10 02:39:14,568 iteration 1049 : loss : 0.060474, loss_ce: 0.024487
2022-01-10 02:39:15,968 iteration 1050 : loss : 0.137482, loss_ce: 0.032649
2022-01-10 02:39:17,350 iteration 1051 : loss : 0.079703, loss_ce: 0.037412
2022-01-10 02:39:18,804 iteration 1052 : loss : 0.072676, loss_ce: 0.021323
2022-01-10 02:39:20,243 iteration 1053 : loss : 0.096460, loss_ce: 0.028261
2022-01-10 02:39:21,657 iteration 1054 : loss : 0.059781, loss_ce: 0.022273
 16%|████▋                         | 62/400 [27:28<2:29:47, 26.59s/it]2022-01-10 02:39:23,120 iteration 1055 : loss : 0.054989, loss_ce: 0.015758
2022-01-10 02:39:24,453 iteration 1056 : loss : 0.051968, loss_ce: 0.023277
2022-01-10 02:39:25,890 iteration 1057 : loss : 0.095234, loss_ce: 0.036020
2022-01-10 02:39:27,303 iteration 1058 : loss : 0.092028, loss_ce: 0.033403
2022-01-10 02:39:28,723 iteration 1059 : loss : 0.092547, loss_ce: 0.040385
2022-01-10 02:39:30,061 iteration 1060 : loss : 0.048814, loss_ce: 0.015826
2022-01-10 02:39:31,556 iteration 1061 : loss : 0.097582, loss_ce: 0.050122
2022-01-10 02:39:32,985 iteration 1062 : loss : 0.113362, loss_ce: 0.042763
2022-01-10 02:39:34,525 iteration 1063 : loss : 0.062458, loss_ce: 0.029932
2022-01-10 02:39:35,961 iteration 1064 : loss : 0.096388, loss_ce: 0.035241
2022-01-10 02:39:37,458 iteration 1065 : loss : 0.068629, loss_ce: 0.035486
2022-01-10 02:39:38,837 iteration 1066 : loss : 0.059443, loss_ce: 0.023551
2022-01-10 02:39:40,217 iteration 1067 : loss : 0.052709, loss_ce: 0.023425
2022-01-10 02:39:41,666 iteration 1068 : loss : 0.066350, loss_ce: 0.028302
2022-01-10 02:39:43,146 iteration 1069 : loss : 0.054621, loss_ce: 0.026099
2022-01-10 02:39:44,666 iteration 1070 : loss : 0.148158, loss_ce: 0.053035
2022-01-10 02:39:46,085 iteration 1071 : loss : 0.059149, loss_ce: 0.028658
 16%|████▋                         | 63/400 [27:52<2:25:43, 25.94s/it]2022-01-10 02:39:47,643 iteration 1072 : loss : 0.073860, loss_ce: 0.024673
2022-01-10 02:39:49,076 iteration 1073 : loss : 0.089940, loss_ce: 0.042419
2022-01-10 02:39:50,525 iteration 1074 : loss : 0.067178, loss_ce: 0.026576
2022-01-10 02:39:51,964 iteration 1075 : loss : 0.089100, loss_ce: 0.029728
2022-01-10 02:39:53,347 iteration 1076 : loss : 0.047857, loss_ce: 0.019133
2022-01-10 02:39:54,838 iteration 1077 : loss : 0.070655, loss_ce: 0.022934
2022-01-10 02:39:56,226 iteration 1078 : loss : 0.082977, loss_ce: 0.041479
2022-01-10 02:39:57,695 iteration 1079 : loss : 0.082873, loss_ce: 0.038836
2022-01-10 02:39:59,039 iteration 1080 : loss : 0.042679, loss_ce: 0.014576
2022-01-10 02:40:00,437 iteration 1081 : loss : 0.067959, loss_ce: 0.034410
2022-01-10 02:40:01,884 iteration 1082 : loss : 0.067989, loss_ce: 0.032118
2022-01-10 02:40:03,262 iteration 1083 : loss : 0.068443, loss_ce: 0.026129
2022-01-10 02:40:04,654 iteration 1084 : loss : 0.067972, loss_ce: 0.019710
2022-01-10 02:40:06,079 iteration 1085 : loss : 0.080263, loss_ce: 0.036302
2022-01-10 02:40:07,482 iteration 1086 : loss : 0.071661, loss_ce: 0.034334
2022-01-10 02:40:08,964 iteration 1087 : loss : 0.046173, loss_ce: 0.019670
2022-01-10 02:40:10,379 iteration 1088 : loss : 0.047540, loss_ce: 0.020600
 16%|████▊                         | 64/400 [28:16<2:22:29, 25.44s/it]2022-01-10 02:40:11,811 iteration 1089 : loss : 0.087176, loss_ce: 0.032544
2022-01-10 02:40:13,195 iteration 1090 : loss : 0.070819, loss_ce: 0.024242
2022-01-10 02:40:14,655 iteration 1091 : loss : 0.046304, loss_ce: 0.022830
2022-01-10 02:40:16,077 iteration 1092 : loss : 0.069272, loss_ce: 0.028357
2022-01-10 02:40:17,499 iteration 1093 : loss : 0.057690, loss_ce: 0.023365
2022-01-10 02:40:18,934 iteration 1094 : loss : 0.054513, loss_ce: 0.024966
2022-01-10 02:40:20,379 iteration 1095 : loss : 0.096156, loss_ce: 0.029503
2022-01-10 02:40:21,797 iteration 1096 : loss : 0.043108, loss_ce: 0.019545
2022-01-10 02:40:23,166 iteration 1097 : loss : 0.082756, loss_ce: 0.027305
2022-01-10 02:40:24,544 iteration 1098 : loss : 0.045023, loss_ce: 0.019728
2022-01-10 02:40:25,971 iteration 1099 : loss : 0.059032, loss_ce: 0.023499
2022-01-10 02:40:27,360 iteration 1100 : loss : 0.077400, loss_ce: 0.033139
2022-01-10 02:40:28,745 iteration 1101 : loss : 0.052805, loss_ce: 0.019935
2022-01-10 02:40:30,227 iteration 1102 : loss : 0.070062, loss_ce: 0.023242
2022-01-10 02:40:31,626 iteration 1103 : loss : 0.044162, loss_ce: 0.016504
2022-01-10 02:40:32,984 iteration 1104 : loss : 0.062056, loss_ce: 0.025248
2022-01-10 02:40:32,984 Training Data Eval:
2022-01-10 02:40:40,032   Average segmentation loss on training set: 0.0674
2022-01-10 02:40:40,032 Validation Data Eval:
2022-01-10 02:40:42,453   Average segmentation loss on validation set: 0.1963
2022-01-10 02:40:43,866 iteration 1105 : loss : 0.046141, loss_ce: 0.020448
 16%|████▉                         | 65/400 [28:50<2:35:33, 27.86s/it]2022-01-10 02:40:45,330 iteration 1106 : loss : 0.041241, loss_ce: 0.014102
2022-01-10 02:40:46,797 iteration 1107 : loss : 0.070515, loss_ce: 0.029858
2022-01-10 02:40:48,164 iteration 1108 : loss : 0.040572, loss_ce: 0.015383
2022-01-10 02:40:49,517 iteration 1109 : loss : 0.067283, loss_ce: 0.028211
2022-01-10 02:40:50,916 iteration 1110 : loss : 0.050815, loss_ce: 0.022007
2022-01-10 02:40:52,334 iteration 1111 : loss : 0.049190, loss_ce: 0.024660
2022-01-10 02:40:53,763 iteration 1112 : loss : 0.062105, loss_ce: 0.029245
2022-01-10 02:40:55,147 iteration 1113 : loss : 0.049232, loss_ce: 0.020948
2022-01-10 02:40:56,511 iteration 1114 : loss : 0.052654, loss_ce: 0.018709
2022-01-10 02:40:57,840 iteration 1115 : loss : 0.046830, loss_ce: 0.014215
2022-01-10 02:40:59,316 iteration 1116 : loss : 0.072545, loss_ce: 0.029794
2022-01-10 02:41:00,805 iteration 1117 : loss : 0.050464, loss_ce: 0.019399
2022-01-10 02:41:02,181 iteration 1118 : loss : 0.040583, loss_ce: 0.016570
2022-01-10 02:41:03,554 iteration 1119 : loss : 0.034805, loss_ce: 0.013055
2022-01-10 02:41:04,911 iteration 1120 : loss : 0.051341, loss_ce: 0.019181
2022-01-10 02:41:06,271 iteration 1121 : loss : 0.049890, loss_ce: 0.016779
2022-01-10 02:41:07,624 iteration 1122 : loss : 0.041139, loss_ce: 0.017001
 16%|████▉                         | 66/400 [29:14<2:28:15, 26.63s/it]2022-01-10 02:41:09,098 iteration 1123 : loss : 0.042828, loss_ce: 0.019962
2022-01-10 02:41:10,471 iteration 1124 : loss : 0.067417, loss_ce: 0.028889
2022-01-10 02:41:11,920 iteration 1125 : loss : 0.063485, loss_ce: 0.024739
2022-01-10 02:41:13,440 iteration 1126 : loss : 0.100039, loss_ce: 0.050481
2022-01-10 02:41:14,879 iteration 1127 : loss : 0.063392, loss_ce: 0.020791
2022-01-10 02:41:16,313 iteration 1128 : loss : 0.058348, loss_ce: 0.017060
2022-01-10 02:41:17,717 iteration 1129 : loss : 0.033795, loss_ce: 0.014308
2022-01-10 02:41:19,182 iteration 1130 : loss : 0.056335, loss_ce: 0.031491
2022-01-10 02:41:20,664 iteration 1131 : loss : 0.112216, loss_ce: 0.034475
2022-01-10 02:41:22,105 iteration 1132 : loss : 0.068764, loss_ce: 0.027708
2022-01-10 02:41:23,535 iteration 1133 : loss : 0.059641, loss_ce: 0.027512
2022-01-10 02:41:24,958 iteration 1134 : loss : 0.072102, loss_ce: 0.029603
2022-01-10 02:41:26,325 iteration 1135 : loss : 0.041913, loss_ce: 0.014136
2022-01-10 02:41:27,742 iteration 1136 : loss : 0.048861, loss_ce: 0.019272
2022-01-10 02:41:29,133 iteration 1137 : loss : 0.046087, loss_ce: 0.016458
2022-01-10 02:41:30,616 iteration 1138 : loss : 0.079470, loss_ce: 0.034612
2022-01-10 02:41:32,097 iteration 1139 : loss : 0.049570, loss_ce: 0.020705
 17%|█████                         | 67/400 [29:38<2:24:12, 25.98s/it]2022-01-10 02:41:33,559 iteration 1140 : loss : 0.045814, loss_ce: 0.019659
2022-01-10 02:41:34,981 iteration 1141 : loss : 0.043832, loss_ce: 0.021066
2022-01-10 02:41:36,349 iteration 1142 : loss : 0.041866, loss_ce: 0.015644
2022-01-10 02:41:37,773 iteration 1143 : loss : 0.076683, loss_ce: 0.028026
2022-01-10 02:41:39,237 iteration 1144 : loss : 0.047569, loss_ce: 0.020503
2022-01-10 02:41:40,646 iteration 1145 : loss : 0.041277, loss_ce: 0.013593
2022-01-10 02:41:42,083 iteration 1146 : loss : 0.045957, loss_ce: 0.018888
2022-01-10 02:41:43,438 iteration 1147 : loss : 0.037758, loss_ce: 0.013234
2022-01-10 02:41:44,844 iteration 1148 : loss : 0.053450, loss_ce: 0.023305
2022-01-10 02:41:46,296 iteration 1149 : loss : 0.050896, loss_ce: 0.024479
2022-01-10 02:41:47,711 iteration 1150 : loss : 0.051796, loss_ce: 0.022115
2022-01-10 02:41:49,136 iteration 1151 : loss : 0.056913, loss_ce: 0.018319
2022-01-10 02:41:50,594 iteration 1152 : loss : 0.133032, loss_ce: 0.024034
2022-01-10 02:41:52,039 iteration 1153 : loss : 0.042094, loss_ce: 0.015658
2022-01-10 02:41:53,431 iteration 1154 : loss : 0.049492, loss_ce: 0.022739
2022-01-10 02:41:54,946 iteration 1155 : loss : 0.084723, loss_ce: 0.030313
2022-01-10 02:41:56,404 iteration 1156 : loss : 0.087674, loss_ce: 0.027876
 17%|█████                         | 68/400 [30:02<2:20:58, 25.48s/it]2022-01-10 02:41:57,837 iteration 1157 : loss : 0.048410, loss_ce: 0.021804
2022-01-10 02:41:59,362 iteration 1158 : loss : 0.068923, loss_ce: 0.019631
2022-01-10 02:42:00,832 iteration 1159 : loss : 0.093831, loss_ce: 0.042201
2022-01-10 02:42:02,172 iteration 1160 : loss : 0.052106, loss_ce: 0.025806
2022-01-10 02:42:03,575 iteration 1161 : loss : 0.048802, loss_ce: 0.015621
2022-01-10 02:42:04,976 iteration 1162 : loss : 0.051849, loss_ce: 0.013327
2022-01-10 02:42:06,474 iteration 1163 : loss : 0.090717, loss_ce: 0.029047
2022-01-10 02:42:07,911 iteration 1164 : loss : 0.046661, loss_ce: 0.017281
2022-01-10 02:42:09,272 iteration 1165 : loss : 0.051159, loss_ce: 0.027685
2022-01-10 02:42:10,669 iteration 1166 : loss : 0.049449, loss_ce: 0.022550
2022-01-10 02:42:12,104 iteration 1167 : loss : 0.078466, loss_ce: 0.035355
2022-01-10 02:42:13,444 iteration 1168 : loss : 0.044705, loss_ce: 0.016314
2022-01-10 02:42:14,861 iteration 1169 : loss : 0.058476, loss_ce: 0.019212
2022-01-10 02:42:16,280 iteration 1170 : loss : 0.038219, loss_ce: 0.013561
2022-01-10 02:42:17,624 iteration 1171 : loss : 0.047643, loss_ce: 0.016956
2022-01-10 02:42:19,029 iteration 1172 : loss : 0.051670, loss_ce: 0.022974
2022-01-10 02:42:20,462 iteration 1173 : loss : 0.067304, loss_ce: 0.043406
 17%|█████▏                        | 69/400 [30:26<2:18:13, 25.06s/it]2022-01-10 02:42:21,963 iteration 1174 : loss : 0.049128, loss_ce: 0.017410
2022-01-10 02:42:23,357 iteration 1175 : loss : 0.037761, loss_ce: 0.017012
2022-01-10 02:42:24,820 iteration 1176 : loss : 0.082121, loss_ce: 0.042401
2022-01-10 02:42:26,324 iteration 1177 : loss : 0.063372, loss_ce: 0.028148
2022-01-10 02:42:27,757 iteration 1178 : loss : 0.069989, loss_ce: 0.032990
2022-01-10 02:42:29,094 iteration 1179 : loss : 0.041151, loss_ce: 0.018158
2022-01-10 02:42:30,549 iteration 1180 : loss : 0.056873, loss_ce: 0.023282
2022-01-10 02:42:31,891 iteration 1181 : loss : 0.037158, loss_ce: 0.015930
2022-01-10 02:42:33,392 iteration 1182 : loss : 0.051427, loss_ce: 0.022110
2022-01-10 02:42:34,801 iteration 1183 : loss : 0.057708, loss_ce: 0.027339
2022-01-10 02:42:36,150 iteration 1184 : loss : 0.047037, loss_ce: 0.019645
2022-01-10 02:42:37,577 iteration 1185 : loss : 0.083733, loss_ce: 0.027490
2022-01-10 02:42:38,949 iteration 1186 : loss : 0.054362, loss_ce: 0.020217
2022-01-10 02:42:40,374 iteration 1187 : loss : 0.053740, loss_ce: 0.021627
2022-01-10 02:42:41,731 iteration 1188 : loss : 0.036913, loss_ce: 0.013774
2022-01-10 02:42:43,135 iteration 1189 : loss : 0.068675, loss_ce: 0.029094
2022-01-10 02:42:43,135 Training Data Eval:
2022-01-10 02:42:50,168   Average segmentation loss on training set: 0.0384
2022-01-10 02:42:50,169 Validation Data Eval:
2022-01-10 02:42:52,604   Average segmentation loss on validation set: 0.0886
2022-01-10 02:42:54,031 iteration 1190 : loss : 0.043663, loss_ce: 0.016754
 18%|█████▎                        | 70/400 [31:00<2:31:50, 27.61s/it]2022-01-10 02:42:55,467 iteration 1191 : loss : 0.042322, loss_ce: 0.020465
2022-01-10 02:42:56,832 iteration 1192 : loss : 0.041306, loss_ce: 0.018092
2022-01-10 02:42:58,237 iteration 1193 : loss : 0.055618, loss_ce: 0.018339
2022-01-10 02:42:59,609 iteration 1194 : loss : 0.068974, loss_ce: 0.023074
2022-01-10 02:43:01,055 iteration 1195 : loss : 0.052950, loss_ce: 0.016623
2022-01-10 02:43:02,402 iteration 1196 : loss : 0.043469, loss_ce: 0.015020
2022-01-10 02:43:03,837 iteration 1197 : loss : 0.064592, loss_ce: 0.033143
2022-01-10 02:43:05,250 iteration 1198 : loss : 0.055373, loss_ce: 0.029740
2022-01-10 02:43:06,672 iteration 1199 : loss : 0.059163, loss_ce: 0.022033
2022-01-10 02:43:08,113 iteration 1200 : loss : 0.061872, loss_ce: 0.022476
2022-01-10 02:43:09,515 iteration 1201 : loss : 0.039137, loss_ce: 0.018321
2022-01-10 02:43:10,960 iteration 1202 : loss : 0.041513, loss_ce: 0.018216
2022-01-10 02:43:12,360 iteration 1203 : loss : 0.053442, loss_ce: 0.020452
2022-01-10 02:43:13,757 iteration 1204 : loss : 0.045719, loss_ce: 0.019993
2022-01-10 02:43:15,204 iteration 1205 : loss : 0.069371, loss_ce: 0.039975
2022-01-10 02:43:16,664 iteration 1206 : loss : 0.092120, loss_ce: 0.029968
2022-01-10 02:43:18,043 iteration 1207 : loss : 0.062635, loss_ce: 0.020615
 18%|█████▎                        | 71/400 [31:24<2:25:28, 26.53s/it]2022-01-10 02:43:19,454 iteration 1208 : loss : 0.044409, loss_ce: 0.021081
2022-01-10 02:43:20,863 iteration 1209 : loss : 0.064162, loss_ce: 0.021398
2022-01-10 02:43:22,201 iteration 1210 : loss : 0.054653, loss_ce: 0.017859
2022-01-10 02:43:23,633 iteration 1211 : loss : 0.060637, loss_ce: 0.020226
2022-01-10 02:43:24,999 iteration 1212 : loss : 0.041314, loss_ce: 0.017609
2022-01-10 02:43:26,411 iteration 1213 : loss : 0.053264, loss_ce: 0.017120
2022-01-10 02:43:27,840 iteration 1214 : loss : 0.052111, loss_ce: 0.020068
2022-01-10 02:43:29,241 iteration 1215 : loss : 0.058969, loss_ce: 0.027103
2022-01-10 02:43:30,624 iteration 1216 : loss : 0.047610, loss_ce: 0.018918
2022-01-10 02:43:32,067 iteration 1217 : loss : 0.064699, loss_ce: 0.026085
2022-01-10 02:43:33,382 iteration 1218 : loss : 0.045456, loss_ce: 0.019501
2022-01-10 02:43:34,819 iteration 1219 : loss : 0.052124, loss_ce: 0.020184
2022-01-10 02:43:36,207 iteration 1220 : loss : 0.048462, loss_ce: 0.017625
2022-01-10 02:43:37,725 iteration 1221 : loss : 0.072345, loss_ce: 0.028642
2022-01-10 02:43:39,169 iteration 1222 : loss : 0.070524, loss_ce: 0.023534
2022-01-10 02:43:40,561 iteration 1223 : loss : 0.042242, loss_ce: 0.016803
2022-01-10 02:43:41,998 iteration 1224 : loss : 0.050585, loss_ce: 0.022478
 18%|█████▍                        | 72/400 [31:48<2:20:47, 25.76s/it]2022-01-10 02:43:43,501 iteration 1225 : loss : 0.045339, loss_ce: 0.016141
2022-01-10 02:43:44,922 iteration 1226 : loss : 0.067925, loss_ce: 0.028953
2022-01-10 02:43:46,291 iteration 1227 : loss : 0.056319, loss_ce: 0.018084
2022-01-10 02:43:47,793 iteration 1228 : loss : 0.092688, loss_ce: 0.026957
2022-01-10 02:43:49,262 iteration 1229 : loss : 0.072373, loss_ce: 0.032704
2022-01-10 02:43:50,713 iteration 1230 : loss : 0.059356, loss_ce: 0.032337
2022-01-10 02:43:52,116 iteration 1231 : loss : 0.071115, loss_ce: 0.026282
2022-01-10 02:43:53,576 iteration 1232 : loss : 0.051260, loss_ce: 0.021366
2022-01-10 02:43:55,096 iteration 1233 : loss : 0.065869, loss_ce: 0.028895
2022-01-10 02:43:56,524 iteration 1234 : loss : 0.072923, loss_ce: 0.031646
2022-01-10 02:43:57,922 iteration 1235 : loss : 0.062623, loss_ce: 0.027064
2022-01-10 02:43:59,296 iteration 1236 : loss : 0.033446, loss_ce: 0.012613
2022-01-10 02:44:00,628 iteration 1237 : loss : 0.041524, loss_ce: 0.013861
2022-01-10 02:44:02,031 iteration 1238 : loss : 0.054343, loss_ce: 0.020327
2022-01-10 02:44:03,480 iteration 1239 : loss : 0.051242, loss_ce: 0.018808
2022-01-10 02:44:04,817 iteration 1240 : loss : 0.047095, loss_ce: 0.017006
2022-01-10 02:44:06,301 iteration 1241 : loss : 0.048763, loss_ce: 0.020604
 18%|█████▍                        | 73/400 [32:12<2:18:00, 25.32s/it]2022-01-10 02:44:07,692 iteration 1242 : loss : 0.043940, loss_ce: 0.018162
2022-01-10 02:44:09,030 iteration 1243 : loss : 0.031178, loss_ce: 0.014630
2022-01-10 02:44:10,382 iteration 1244 : loss : 0.051422, loss_ce: 0.018128
2022-01-10 02:44:11,772 iteration 1245 : loss : 0.053459, loss_ce: 0.022204
2022-01-10 02:44:13,186 iteration 1246 : loss : 0.052319, loss_ce: 0.025002
2022-01-10 02:44:14,622 iteration 1247 : loss : 0.065719, loss_ce: 0.023283
2022-01-10 02:44:15,990 iteration 1248 : loss : 0.033974, loss_ce: 0.013658
2022-01-10 02:44:17,410 iteration 1249 : loss : 0.048758, loss_ce: 0.016092
2022-01-10 02:44:18,784 iteration 1250 : loss : 0.049506, loss_ce: 0.017352
2022-01-10 02:44:20,212 iteration 1251 : loss : 0.036244, loss_ce: 0.014724
2022-01-10 02:44:21,635 iteration 1252 : loss : 0.084975, loss_ce: 0.024863
2022-01-10 02:44:23,057 iteration 1253 : loss : 0.053284, loss_ce: 0.020839
2022-01-10 02:44:24,446 iteration 1254 : loss : 0.069557, loss_ce: 0.031966
2022-01-10 02:44:25,895 iteration 1255 : loss : 0.042553, loss_ce: 0.017328
2022-01-10 02:44:27,367 iteration 1256 : loss : 0.067663, loss_ce: 0.025461
2022-01-10 02:44:28,779 iteration 1257 : loss : 0.046151, loss_ce: 0.021381
2022-01-10 02:44:30,265 iteration 1258 : loss : 0.073677, loss_ce: 0.033850
 18%|█████▌                        | 74/400 [32:36<2:15:21, 24.91s/it]2022-01-10 02:44:31,732 iteration 1259 : loss : 0.041042, loss_ce: 0.013752
2022-01-10 02:44:33,183 iteration 1260 : loss : 0.059624, loss_ce: 0.019694
2022-01-10 02:44:34,613 iteration 1261 : loss : 0.054611, loss_ce: 0.024871
2022-01-10 02:44:35,984 iteration 1262 : loss : 0.033685, loss_ce: 0.014804
2022-01-10 02:44:37,448 iteration 1263 : loss : 0.040261, loss_ce: 0.014941
2022-01-10 02:44:38,798 iteration 1264 : loss : 0.041607, loss_ce: 0.021308
2022-01-10 02:44:40,160 iteration 1265 : loss : 0.049999, loss_ce: 0.020529
2022-01-10 02:44:41,499 iteration 1266 : loss : 0.037146, loss_ce: 0.016954
2022-01-10 02:44:42,990 iteration 1267 : loss : 0.072709, loss_ce: 0.025970
2022-01-10 02:44:44,389 iteration 1268 : loss : 0.128889, loss_ce: 0.034025
2022-01-10 02:44:45,830 iteration 1269 : loss : 0.067708, loss_ce: 0.021637
2022-01-10 02:44:47,190 iteration 1270 : loss : 0.045593, loss_ce: 0.014905
2022-01-10 02:44:48,590 iteration 1271 : loss : 0.047791, loss_ce: 0.020332
2022-01-10 02:44:49,982 iteration 1272 : loss : 0.062467, loss_ce: 0.026630
2022-01-10 02:44:51,428 iteration 1273 : loss : 0.058187, loss_ce: 0.026354
2022-01-10 02:44:52,848 iteration 1274 : loss : 0.047991, loss_ce: 0.016155
2022-01-10 02:44:52,849 Training Data Eval:
2022-01-10 02:44:59,896   Average segmentation loss on training set: 0.0389
2022-01-10 02:44:59,896 Validation Data Eval:
2022-01-10 02:45:02,346   Average segmentation loss on validation set: 0.1380
2022-01-10 02:45:03,777 iteration 1275 : loss : 0.049618, loss_ce: 0.017545
 19%|█████▋                        | 75/400 [33:10<2:28:55, 27.49s/it]2022-01-10 02:45:05,333 iteration 1276 : loss : 0.081138, loss_ce: 0.022095
2022-01-10 02:45:06,717 iteration 1277 : loss : 0.037598, loss_ce: 0.013803
2022-01-10 02:45:08,185 iteration 1278 : loss : 0.064711, loss_ce: 0.027520
2022-01-10 02:45:09,520 iteration 1279 : loss : 0.051014, loss_ce: 0.028058
2022-01-10 02:45:10,854 iteration 1280 : loss : 0.049233, loss_ce: 0.024916
2022-01-10 02:45:12,319 iteration 1281 : loss : 0.053872, loss_ce: 0.022931
2022-01-10 02:45:13,681 iteration 1282 : loss : 0.051008, loss_ce: 0.026251
2022-01-10 02:45:15,065 iteration 1283 : loss : 0.042281, loss_ce: 0.017332
2022-01-10 02:45:16,474 iteration 1284 : loss : 0.046663, loss_ce: 0.022218
2022-01-10 02:45:18,019 iteration 1285 : loss : 0.069357, loss_ce: 0.023875
2022-01-10 02:45:19,470 iteration 1286 : loss : 0.046183, loss_ce: 0.017582
2022-01-10 02:45:20,864 iteration 1287 : loss : 0.066837, loss_ce: 0.024334
2022-01-10 02:45:22,295 iteration 1288 : loss : 0.040907, loss_ce: 0.016320
2022-01-10 02:45:23,778 iteration 1289 : loss : 0.050103, loss_ce: 0.018944
2022-01-10 02:45:25,166 iteration 1290 : loss : 0.055086, loss_ce: 0.017388
2022-01-10 02:45:26,588 iteration 1291 : loss : 0.036846, loss_ce: 0.013636
2022-01-10 02:45:27,948 iteration 1292 : loss : 0.034601, loss_ce: 0.014167
 19%|█████▋                        | 76/400 [33:34<2:23:04, 26.50s/it]2022-01-10 02:45:29,411 iteration 1293 : loss : 0.067455, loss_ce: 0.032441
2022-01-10 02:45:30,785 iteration 1294 : loss : 0.040698, loss_ce: 0.015492
2022-01-10 02:45:32,176 iteration 1295 : loss : 0.050115, loss_ce: 0.023532
2022-01-10 02:45:33,574 iteration 1296 : loss : 0.041016, loss_ce: 0.013823
2022-01-10 02:45:34,967 iteration 1297 : loss : 0.043749, loss_ce: 0.016846
2022-01-10 02:45:36,384 iteration 1298 : loss : 0.029293, loss_ce: 0.011708
2022-01-10 02:45:37,753 iteration 1299 : loss : 0.047072, loss_ce: 0.016733
2022-01-10 02:45:39,216 iteration 1300 : loss : 0.077750, loss_ce: 0.018669
2022-01-10 02:45:40,637 iteration 1301 : loss : 0.056501, loss_ce: 0.028999
2022-01-10 02:45:42,099 iteration 1302 : loss : 0.057470, loss_ce: 0.018367
2022-01-10 02:45:43,472 iteration 1303 : loss : 0.041880, loss_ce: 0.017062
2022-01-10 02:45:44,921 iteration 1304 : loss : 0.069528, loss_ce: 0.019214
2022-01-10 02:45:46,336 iteration 1305 : loss : 0.046446, loss_ce: 0.020193
2022-01-10 02:45:47,796 iteration 1306 : loss : 0.053276, loss_ce: 0.019752
2022-01-10 02:45:49,270 iteration 1307 : loss : 0.060293, loss_ce: 0.023696
2022-01-10 02:45:50,693 iteration 1308 : loss : 0.068095, loss_ce: 0.039192
2022-01-10 02:45:52,109 iteration 1309 : loss : 0.058206, loss_ce: 0.019862
 19%|█████▊                        | 77/400 [33:58<2:18:51, 25.79s/it]2022-01-10 02:45:53,592 iteration 1310 : loss : 0.080354, loss_ce: 0.033164
2022-01-10 02:45:55,078 iteration 1311 : loss : 0.051399, loss_ce: 0.021329
2022-01-10 02:45:56,485 iteration 1312 : loss : 0.062360, loss_ce: 0.030214
2022-01-10 02:45:57,836 iteration 1313 : loss : 0.036781, loss_ce: 0.016056
2022-01-10 02:45:59,255 iteration 1314 : loss : 0.040065, loss_ce: 0.016340
2022-01-10 02:46:00,680 iteration 1315 : loss : 0.060828, loss_ce: 0.024728
2022-01-10 02:46:02,067 iteration 1316 : loss : 0.044757, loss_ce: 0.020087
2022-01-10 02:46:03,507 iteration 1317 : loss : 0.048490, loss_ce: 0.021940
2022-01-10 02:46:04,962 iteration 1318 : loss : 0.050697, loss_ce: 0.021630
2022-01-10 02:46:06,350 iteration 1319 : loss : 0.048372, loss_ce: 0.017113
2022-01-10 02:46:07,753 iteration 1320 : loss : 0.044000, loss_ce: 0.017352
2022-01-10 02:46:09,171 iteration 1321 : loss : 0.079599, loss_ce: 0.038778
2022-01-10 02:46:10,681 iteration 1322 : loss : 0.046747, loss_ce: 0.018369
2022-01-10 02:46:12,129 iteration 1323 : loss : 0.032770, loss_ce: 0.012542
2022-01-10 02:46:13,575 iteration 1324 : loss : 0.038034, loss_ce: 0.014176
2022-01-10 02:46:15,033 iteration 1325 : loss : 0.051427, loss_ce: 0.019005
2022-01-10 02:46:16,394 iteration 1326 : loss : 0.045338, loss_ce: 0.015736
 20%|█████▊                        | 78/400 [34:22<2:16:00, 25.34s/it]2022-01-10 02:46:17,888 iteration 1327 : loss : 0.064734, loss_ce: 0.037904
2022-01-10 02:46:19,391 iteration 1328 : loss : 0.076834, loss_ce: 0.025109
2022-01-10 02:46:20,741 iteration 1329 : loss : 0.043524, loss_ce: 0.021349
2022-01-10 02:46:22,145 iteration 1330 : loss : 0.050532, loss_ce: 0.021554
2022-01-10 02:46:23,551 iteration 1331 : loss : 0.041312, loss_ce: 0.014494
2022-01-10 02:46:24,948 iteration 1332 : loss : 0.030890, loss_ce: 0.014119
2022-01-10 02:46:26,386 iteration 1333 : loss : 0.085475, loss_ce: 0.019947
2022-01-10 02:46:27,789 iteration 1334 : loss : 0.031179, loss_ce: 0.013373
2022-01-10 02:46:29,304 iteration 1335 : loss : 0.048649, loss_ce: 0.017216
2022-01-10 02:46:30,750 iteration 1336 : loss : 0.060341, loss_ce: 0.029386
2022-01-10 02:46:32,223 iteration 1337 : loss : 0.042472, loss_ce: 0.019633
2022-01-10 02:46:33,708 iteration 1338 : loss : 0.058085, loss_ce: 0.023596
2022-01-10 02:46:35,165 iteration 1339 : loss : 0.042677, loss_ce: 0.016046
2022-01-10 02:46:36,542 iteration 1340 : loss : 0.045318, loss_ce: 0.014707
2022-01-10 02:46:37,945 iteration 1341 : loss : 0.049463, loss_ce: 0.019630
2022-01-10 02:46:39,383 iteration 1342 : loss : 0.062606, loss_ce: 0.024998
2022-01-10 02:46:40,813 iteration 1343 : loss : 0.037153, loss_ce: 0.014820
 20%|█████▉                        | 79/400 [34:47<2:14:05, 25.07s/it]2022-01-10 02:46:42,286 iteration 1344 : loss : 0.033314, loss_ce: 0.012057
2022-01-10 02:46:43,672 iteration 1345 : loss : 0.043289, loss_ce: 0.018480
2022-01-10 02:46:45,045 iteration 1346 : loss : 0.035506, loss_ce: 0.015303
2022-01-10 02:46:46,524 iteration 1347 : loss : 0.108729, loss_ce: 0.024392
2022-01-10 02:46:48,007 iteration 1348 : loss : 0.044634, loss_ce: 0.019174
2022-01-10 02:46:49,439 iteration 1349 : loss : 0.102971, loss_ce: 0.052332
2022-01-10 02:46:50,898 iteration 1350 : loss : 0.099865, loss_ce: 0.048665
2022-01-10 02:46:52,268 iteration 1351 : loss : 0.041404, loss_ce: 0.014480
2022-01-10 02:46:53,756 iteration 1352 : loss : 0.039990, loss_ce: 0.015631
2022-01-10 02:46:55,105 iteration 1353 : loss : 0.053739, loss_ce: 0.023272
2022-01-10 02:46:56,549 iteration 1354 : loss : 0.065975, loss_ce: 0.026885
2022-01-10 02:46:57,907 iteration 1355 : loss : 0.092089, loss_ce: 0.033525
2022-01-10 02:46:59,359 iteration 1356 : loss : 0.069495, loss_ce: 0.024142
2022-01-10 02:47:00,723 iteration 1357 : loss : 0.055099, loss_ce: 0.030569
2022-01-10 02:47:02,131 iteration 1358 : loss : 0.053174, loss_ce: 0.020302
2022-01-10 02:47:03,484 iteration 1359 : loss : 0.055016, loss_ce: 0.031395
2022-01-10 02:47:03,484 Training Data Eval:
2022-01-10 02:47:10,722   Average segmentation loss on training set: 0.0539
2022-01-10 02:47:10,723 Validation Data Eval:
2022-01-10 02:47:13,198   Average segmentation loss on validation set: 0.1002
2022-01-10 02:47:14,595 iteration 1360 : loss : 0.060967, loss_ce: 0.023995
 20%|██████                        | 80/400 [35:21<2:27:37, 27.68s/it]2022-01-10 02:47:16,104 iteration 1361 : loss : 0.121288, loss_ce: 0.040931
2022-01-10 02:47:17,534 iteration 1362 : loss : 0.050410, loss_ce: 0.022862
2022-01-10 02:47:18,973 iteration 1363 : loss : 0.038903, loss_ce: 0.015475
2022-01-10 02:47:20,441 iteration 1364 : loss : 0.069685, loss_ce: 0.021979
2022-01-10 02:47:21,944 iteration 1365 : loss : 0.063077, loss_ce: 0.024675
2022-01-10 02:47:23,361 iteration 1366 : loss : 0.063814, loss_ce: 0.027979
2022-01-10 02:47:24,734 iteration 1367 : loss : 0.039856, loss_ce: 0.017250
2022-01-10 02:47:26,155 iteration 1368 : loss : 0.069566, loss_ce: 0.032126
2022-01-10 02:47:27,546 iteration 1369 : loss : 0.067383, loss_ce: 0.018154
2022-01-10 02:47:28,959 iteration 1370 : loss : 0.068624, loss_ce: 0.023733
2022-01-10 02:47:30,308 iteration 1371 : loss : 0.042915, loss_ce: 0.016946
2022-01-10 02:47:31,741 iteration 1372 : loss : 0.071879, loss_ce: 0.021208
2022-01-10 02:47:33,247 iteration 1373 : loss : 0.064740, loss_ce: 0.037152
2022-01-10 02:47:34,651 iteration 1374 : loss : 0.079505, loss_ce: 0.026630
2022-01-10 02:47:36,119 iteration 1375 : loss : 0.046901, loss_ce: 0.017153
2022-01-10 02:47:37,456 iteration 1376 : loss : 0.054276, loss_ce: 0.024164
2022-01-10 02:47:38,904 iteration 1377 : loss : 0.081675, loss_ce: 0.026862
 20%|██████                        | 81/400 [35:45<2:21:47, 26.67s/it]2022-01-10 02:47:40,438 iteration 1378 : loss : 0.064126, loss_ce: 0.030837
2022-01-10 02:47:41,910 iteration 1379 : loss : 0.080773, loss_ce: 0.024423
2022-01-10 02:47:43,329 iteration 1380 : loss : 0.049007, loss_ce: 0.024273
2022-01-10 02:47:44,758 iteration 1381 : loss : 0.045832, loss_ce: 0.019347
2022-01-10 02:47:46,157 iteration 1382 : loss : 0.049352, loss_ce: 0.020630
2022-01-10 02:47:47,568 iteration 1383 : loss : 0.064777, loss_ce: 0.022271
2022-01-10 02:47:49,065 iteration 1384 : loss : 0.071374, loss_ce: 0.028571
2022-01-10 02:47:50,407 iteration 1385 : loss : 0.038684, loss_ce: 0.015361
2022-01-10 02:47:51,905 iteration 1386 : loss : 0.073073, loss_ce: 0.032405
2022-01-10 02:47:53,287 iteration 1387 : loss : 0.090807, loss_ce: 0.027324
2022-01-10 02:47:54,741 iteration 1388 : loss : 0.093389, loss_ce: 0.030616
2022-01-10 02:47:56,185 iteration 1389 : loss : 0.073358, loss_ce: 0.036399
2022-01-10 02:47:57,542 iteration 1390 : loss : 0.051864, loss_ce: 0.022563
2022-01-10 02:47:58,975 iteration 1391 : loss : 0.095708, loss_ce: 0.028031
2022-01-10 02:48:00,380 iteration 1392 : loss : 0.057405, loss_ce: 0.026721
2022-01-10 02:48:01,802 iteration 1393 : loss : 0.062164, loss_ce: 0.020407
2022-01-10 02:48:03,212 iteration 1394 : loss : 0.060083, loss_ce: 0.019035
 20%|██████▏                       | 82/400 [36:09<2:17:35, 25.96s/it]2022-01-10 02:48:04,731 iteration 1395 : loss : 0.048628, loss_ce: 0.023541
2022-01-10 02:48:06,084 iteration 1396 : loss : 0.048283, loss_ce: 0.023845
2022-01-10 02:48:07,549 iteration 1397 : loss : 0.049462, loss_ce: 0.018041
2022-01-10 02:48:08,883 iteration 1398 : loss : 0.032419, loss_ce: 0.012677
2022-01-10 02:48:10,313 iteration 1399 : loss : 0.043372, loss_ce: 0.019541
2022-01-10 02:48:11,787 iteration 1400 : loss : 0.057027, loss_ce: 0.021298
2022-01-10 02:48:13,173 iteration 1401 : loss : 0.047013, loss_ce: 0.014956
2022-01-10 02:48:14,657 iteration 1402 : loss : 0.062106, loss_ce: 0.023702
2022-01-10 02:48:16,082 iteration 1403 : loss : 0.075295, loss_ce: 0.024705
2022-01-10 02:48:17,578 iteration 1404 : loss : 0.064095, loss_ce: 0.025379
2022-01-10 02:48:18,848 iteration 1405 : loss : 0.036967, loss_ce: 0.013860
2022-01-10 02:48:20,285 iteration 1406 : loss : 0.053030, loss_ce: 0.023259
2022-01-10 02:48:21,668 iteration 1407 : loss : 0.044303, loss_ce: 0.018561
2022-01-10 02:48:23,027 iteration 1408 : loss : 0.060753, loss_ce: 0.034738
2022-01-10 02:48:24,510 iteration 1409 : loss : 0.067033, loss_ce: 0.018191
2022-01-10 02:48:25,877 iteration 1410 : loss : 0.073205, loss_ce: 0.033199
2022-01-10 02:48:27,215 iteration 1411 : loss : 0.035979, loss_ce: 0.014575
 21%|██████▏                       | 83/400 [36:33<2:14:02, 25.37s/it]2022-01-10 02:48:28,706 iteration 1412 : loss : 0.049914, loss_ce: 0.024124
2022-01-10 02:48:30,050 iteration 1413 : loss : 0.048814, loss_ce: 0.018690
2022-01-10 02:48:31,490 iteration 1414 : loss : 0.042693, loss_ce: 0.018610
2022-01-10 02:48:32,842 iteration 1415 : loss : 0.052533, loss_ce: 0.020122
2022-01-10 02:48:34,237 iteration 1416 : loss : 0.048558, loss_ce: 0.016869
2022-01-10 02:48:35,657 iteration 1417 : loss : 0.067024, loss_ce: 0.026925
2022-01-10 02:48:37,076 iteration 1418 : loss : 0.039424, loss_ce: 0.013921
2022-01-10 02:48:38,417 iteration 1419 : loss : 0.040592, loss_ce: 0.016839
2022-01-10 02:48:39,804 iteration 1420 : loss : 0.047392, loss_ce: 0.013852
2022-01-10 02:48:41,200 iteration 1421 : loss : 0.073683, loss_ce: 0.046870
2022-01-10 02:48:42,618 iteration 1422 : loss : 0.052765, loss_ce: 0.022435
2022-01-10 02:48:43,991 iteration 1423 : loss : 0.051153, loss_ce: 0.016296
2022-01-10 02:48:45,320 iteration 1424 : loss : 0.026199, loss_ce: 0.010605
2022-01-10 02:48:46,745 iteration 1425 : loss : 0.061227, loss_ce: 0.021417
2022-01-10 02:48:48,181 iteration 1426 : loss : 0.056362, loss_ce: 0.031653
2022-01-10 02:48:49,593 iteration 1427 : loss : 0.083674, loss_ce: 0.015635
2022-01-10 02:48:51,103 iteration 1428 : loss : 0.074834, loss_ce: 0.028001
 21%|██████▎                       | 84/400 [36:57<2:11:17, 24.93s/it]2022-01-10 02:48:52,602 iteration 1429 : loss : 0.072766, loss_ce: 0.037300
2022-01-10 02:48:53,996 iteration 1430 : loss : 0.040611, loss_ce: 0.017057
2022-01-10 02:48:55,357 iteration 1431 : loss : 0.077404, loss_ce: 0.025806
2022-01-10 02:48:56,771 iteration 1432 : loss : 0.041858, loss_ce: 0.018931
2022-01-10 02:48:58,115 iteration 1433 : loss : 0.044407, loss_ce: 0.018314
2022-01-10 02:48:59,603 iteration 1434 : loss : 0.047511, loss_ce: 0.018858
2022-01-10 02:49:01,054 iteration 1435 : loss : 0.053797, loss_ce: 0.018022
2022-01-10 02:49:02,554 iteration 1436 : loss : 0.053359, loss_ce: 0.027858
2022-01-10 02:49:03,970 iteration 1437 : loss : 0.037533, loss_ce: 0.017840
2022-01-10 02:49:05,371 iteration 1438 : loss : 0.050130, loss_ce: 0.020144
2022-01-10 02:49:06,796 iteration 1439 : loss : 0.100911, loss_ce: 0.030119
2022-01-10 02:49:08,151 iteration 1440 : loss : 0.051416, loss_ce: 0.017165
2022-01-10 02:49:09,580 iteration 1441 : loss : 0.059475, loss_ce: 0.023466
2022-01-10 02:49:10,987 iteration 1442 : loss : 0.059928, loss_ce: 0.017985
2022-01-10 02:49:12,455 iteration 1443 : loss : 0.046182, loss_ce: 0.016482
2022-01-10 02:49:13,861 iteration 1444 : loss : 0.054331, loss_ce: 0.014008
2022-01-10 02:49:13,861 Training Data Eval:
2022-01-10 02:49:20,903   Average segmentation loss on training set: 0.0366
2022-01-10 02:49:20,903 Validation Data Eval:
2022-01-10 02:49:23,340   Average segmentation loss on validation set: 0.0908
2022-01-10 02:49:24,788 iteration 1445 : loss : 0.047815, loss_ce: 0.021514
 21%|██████▍                       | 85/400 [37:31<2:24:40, 27.56s/it]2022-01-10 02:49:26,185 iteration 1446 : loss : 0.038807, loss_ce: 0.015431
2022-01-10 02:49:27,517 iteration 1447 : loss : 0.034067, loss_ce: 0.014706
2022-01-10 02:49:28,971 iteration 1448 : loss : 0.065882, loss_ce: 0.016744
2022-01-10 02:49:30,422 iteration 1449 : loss : 0.052613, loss_ce: 0.021527
2022-01-10 02:49:31,815 iteration 1450 : loss : 0.047360, loss_ce: 0.017579
2022-01-10 02:49:33,243 iteration 1451 : loss : 0.031975, loss_ce: 0.008865
2022-01-10 02:49:34,649 iteration 1452 : loss : 0.049170, loss_ce: 0.028309
2022-01-10 02:49:36,082 iteration 1453 : loss : 0.045424, loss_ce: 0.020733
2022-01-10 02:49:37,504 iteration 1454 : loss : 0.048227, loss_ce: 0.015834
2022-01-10 02:49:38,935 iteration 1455 : loss : 0.047943, loss_ce: 0.018065
2022-01-10 02:49:40,310 iteration 1456 : loss : 0.042136, loss_ce: 0.011992
2022-01-10 02:49:41,775 iteration 1457 : loss : 0.046142, loss_ce: 0.020078
2022-01-10 02:49:43,209 iteration 1458 : loss : 0.040233, loss_ce: 0.018089
2022-01-10 02:49:44,596 iteration 1459 : loss : 0.040905, loss_ce: 0.014015
2022-01-10 02:49:45,995 iteration 1460 : loss : 0.042417, loss_ce: 0.016129
2022-01-10 02:49:47,354 iteration 1461 : loss : 0.074301, loss_ce: 0.028170
2022-01-10 02:49:48,823 iteration 1462 : loss : 0.044040, loss_ce: 0.019216
 22%|██████▍                       | 86/400 [37:55<2:18:41, 26.50s/it]2022-01-10 02:49:50,402 iteration 1463 : loss : 0.111616, loss_ce: 0.039538
2022-01-10 02:49:51,775 iteration 1464 : loss : 0.042501, loss_ce: 0.022650
2022-01-10 02:49:53,305 iteration 1465 : loss : 0.054715, loss_ce: 0.024656
2022-01-10 02:49:54,730 iteration 1466 : loss : 0.055540, loss_ce: 0.022973
2022-01-10 02:49:56,106 iteration 1467 : loss : 0.049428, loss_ce: 0.019939
2022-01-10 02:49:57,522 iteration 1468 : loss : 0.046380, loss_ce: 0.021958
2022-01-10 02:49:58,923 iteration 1469 : loss : 0.057864, loss_ce: 0.025702
2022-01-10 02:50:00,279 iteration 1470 : loss : 0.069181, loss_ce: 0.015104
2022-01-10 02:50:01,689 iteration 1471 : loss : 0.045140, loss_ce: 0.021898
2022-01-10 02:50:03,121 iteration 1472 : loss : 0.058185, loss_ce: 0.022717
2022-01-10 02:50:04,494 iteration 1473 : loss : 0.039288, loss_ce: 0.019138
2022-01-10 02:50:05,944 iteration 1474 : loss : 0.055356, loss_ce: 0.016827
2022-01-10 02:50:07,323 iteration 1475 : loss : 0.043911, loss_ce: 0.021589
2022-01-10 02:50:08,706 iteration 1476 : loss : 0.040969, loss_ce: 0.016623
2022-01-10 02:50:10,191 iteration 1477 : loss : 0.044843, loss_ce: 0.021374
2022-01-10 02:50:11,561 iteration 1478 : loss : 0.074194, loss_ce: 0.022828
2022-01-10 02:50:12,970 iteration 1479 : loss : 0.055505, loss_ce: 0.016296
 22%|██████▌                       | 87/400 [38:19<2:14:33, 25.79s/it]2022-01-10 02:50:14,323 iteration 1480 : loss : 0.029868, loss_ce: 0.010633
2022-01-10 02:50:15,818 iteration 1481 : loss : 0.053170, loss_ce: 0.024343
2022-01-10 02:50:17,219 iteration 1482 : loss : 0.034991, loss_ce: 0.015744
2022-01-10 02:50:18,681 iteration 1483 : loss : 0.054338, loss_ce: 0.019583
2022-01-10 02:50:20,122 iteration 1484 : loss : 0.039090, loss_ce: 0.011723
2022-01-10 02:50:21,575 iteration 1485 : loss : 0.041573, loss_ce: 0.021073
2022-01-10 02:50:22,975 iteration 1486 : loss : 0.050780, loss_ce: 0.016880
2022-01-10 02:50:24,382 iteration 1487 : loss : 0.038716, loss_ce: 0.015184
2022-01-10 02:50:25,739 iteration 1488 : loss : 0.037328, loss_ce: 0.016464
2022-01-10 02:50:27,116 iteration 1489 : loss : 0.042642, loss_ce: 0.018932
2022-01-10 02:50:28,546 iteration 1490 : loss : 0.061676, loss_ce: 0.020272
2022-01-10 02:50:29,870 iteration 1491 : loss : 0.061245, loss_ce: 0.019639
2022-01-10 02:50:31,217 iteration 1492 : loss : 0.051733, loss_ce: 0.022075
2022-01-10 02:50:32,657 iteration 1493 : loss : 0.037030, loss_ce: 0.015631
2022-01-10 02:50:34,051 iteration 1494 : loss : 0.049532, loss_ce: 0.015807
2022-01-10 02:50:35,397 iteration 1495 : loss : 0.040567, loss_ce: 0.010943
2022-01-10 02:50:36,841 iteration 1496 : loss : 0.047322, loss_ce: 0.021816
 22%|██████▌                       | 88/400 [38:43<2:11:07, 25.22s/it]2022-01-10 02:50:38,230 iteration 1497 : loss : 0.029454, loss_ce: 0.012491
2022-01-10 02:50:39,672 iteration 1498 : loss : 0.050900, loss_ce: 0.021518
2022-01-10 02:50:41,017 iteration 1499 : loss : 0.045527, loss_ce: 0.016052
2022-01-10 02:50:42,427 iteration 1500 : loss : 0.039147, loss_ce: 0.018379
2022-01-10 02:50:43,814 iteration 1501 : loss : 0.037414, loss_ce: 0.016266
2022-01-10 02:50:45,292 iteration 1502 : loss : 0.100320, loss_ce: 0.020189
2022-01-10 02:50:46,820 iteration 1503 : loss : 0.099080, loss_ce: 0.028841
2022-01-10 02:50:48,214 iteration 1504 : loss : 0.031553, loss_ce: 0.013862
2022-01-10 02:50:49,656 iteration 1505 : loss : 0.051760, loss_ce: 0.025004
2022-01-10 02:50:51,094 iteration 1506 : loss : 0.041196, loss_ce: 0.016505
2022-01-10 02:50:52,539 iteration 1507 : loss : 0.065866, loss_ce: 0.019536
2022-01-10 02:50:53,973 iteration 1508 : loss : 0.054417, loss_ce: 0.029817
2022-01-10 02:50:55,388 iteration 1509 : loss : 0.042888, loss_ce: 0.015132
2022-01-10 02:50:56,879 iteration 1510 : loss : 0.041329, loss_ce: 0.013561
2022-01-10 02:50:58,355 iteration 1511 : loss : 0.052867, loss_ce: 0.023975
2022-01-10 02:50:59,778 iteration 1512 : loss : 0.055657, loss_ce: 0.014709
2022-01-10 02:51:01,203 iteration 1513 : loss : 0.039651, loss_ce: 0.014218
 22%|██████▋                       | 89/400 [39:07<2:09:22, 24.96s/it]2022-01-10 02:51:02,717 iteration 1514 : loss : 0.053380, loss_ce: 0.020738
2022-01-10 02:51:04,083 iteration 1515 : loss : 0.032006, loss_ce: 0.012527
2022-01-10 02:51:05,498 iteration 1516 : loss : 0.050111, loss_ce: 0.021790
2022-01-10 02:51:06,829 iteration 1517 : loss : 0.045704, loss_ce: 0.018498
2022-01-10 02:51:08,265 iteration 1518 : loss : 0.040439, loss_ce: 0.013913
2022-01-10 02:51:09,747 iteration 1519 : loss : 0.052890, loss_ce: 0.020330
2022-01-10 02:51:11,204 iteration 1520 : loss : 0.047583, loss_ce: 0.016722
2022-01-10 02:51:12,623 iteration 1521 : loss : 0.051972, loss_ce: 0.017738
2022-01-10 02:51:13,998 iteration 1522 : loss : 0.047216, loss_ce: 0.019514
2022-01-10 02:51:15,381 iteration 1523 : loss : 0.043187, loss_ce: 0.013409
2022-01-10 02:51:16,801 iteration 1524 : loss : 0.040247, loss_ce: 0.016479
2022-01-10 02:51:18,146 iteration 1525 : loss : 0.043752, loss_ce: 0.018642
2022-01-10 02:51:19,585 iteration 1526 : loss : 0.053179, loss_ce: 0.025834
2022-01-10 02:51:21,031 iteration 1527 : loss : 0.037118, loss_ce: 0.017799
2022-01-10 02:51:22,401 iteration 1528 : loss : 0.036894, loss_ce: 0.015127
2022-01-10 02:51:23,763 iteration 1529 : loss : 0.053785, loss_ce: 0.017439
2022-01-10 02:51:23,764 Training Data Eval:
2022-01-10 02:51:30,813   Average segmentation loss on training set: 0.0301
2022-01-10 02:51:30,813 Validation Data Eval:
2022-01-10 02:51:33,276   Average segmentation loss on validation set: 0.0994
2022-01-10 02:51:34,719 iteration 1530 : loss : 0.051498, loss_ce: 0.021756
 22%|██████▊                       | 90/400 [39:41<2:22:13, 27.53s/it]2022-01-10 02:51:36,197 iteration 1531 : loss : 0.034533, loss_ce: 0.013887
2022-01-10 02:51:37,604 iteration 1532 : loss : 0.046313, loss_ce: 0.016686
2022-01-10 02:51:38,959 iteration 1533 : loss : 0.030642, loss_ce: 0.011383
2022-01-10 02:51:40,345 iteration 1534 : loss : 0.041146, loss_ce: 0.020825
2022-01-10 02:51:41,807 iteration 1535 : loss : 0.044887, loss_ce: 0.017836
2022-01-10 02:51:43,265 iteration 1536 : loss : 0.049400, loss_ce: 0.024771
2022-01-10 02:51:44,667 iteration 1537 : loss : 0.049368, loss_ce: 0.017231
2022-01-10 02:51:46,111 iteration 1538 : loss : 0.041928, loss_ce: 0.017436
2022-01-10 02:51:47,522 iteration 1539 : loss : 0.034489, loss_ce: 0.015474
2022-01-10 02:51:48,922 iteration 1540 : loss : 0.063427, loss_ce: 0.027450
2022-01-10 02:51:50,301 iteration 1541 : loss : 0.027675, loss_ce: 0.011478
2022-01-10 02:51:51,659 iteration 1542 : loss : 0.028638, loss_ce: 0.011560
2022-01-10 02:51:53,043 iteration 1543 : loss : 0.067008, loss_ce: 0.022756
2022-01-10 02:51:54,384 iteration 1544 : loss : 0.031173, loss_ce: 0.012083
2022-01-10 02:51:55,894 iteration 1545 : loss : 0.048902, loss_ce: 0.021051
2022-01-10 02:51:57,252 iteration 1546 : loss : 0.083158, loss_ce: 0.020166
2022-01-10 02:51:58,737 iteration 1547 : loss : 0.041099, loss_ce: 0.016507
 23%|██████▊                       | 91/400 [40:05<2:16:20, 26.48s/it]2022-01-10 02:52:00,224 iteration 1548 : loss : 0.042291, loss_ce: 0.019568
2022-01-10 02:52:01,551 iteration 1549 : loss : 0.035613, loss_ce: 0.015478
2022-01-10 02:52:02,914 iteration 1550 : loss : 0.045281, loss_ce: 0.017983
2022-01-10 02:52:04,214 iteration 1551 : loss : 0.036508, loss_ce: 0.012568
2022-01-10 02:52:05,632 iteration 1552 : loss : 0.038167, loss_ce: 0.012559
2022-01-10 02:52:07,080 iteration 1553 : loss : 0.057812, loss_ce: 0.024795
2022-01-10 02:52:08,511 iteration 1554 : loss : 0.039135, loss_ce: 0.016121
2022-01-10 02:52:09,899 iteration 1555 : loss : 0.047293, loss_ce: 0.020086
2022-01-10 02:52:11,331 iteration 1556 : loss : 0.037561, loss_ce: 0.012904
2022-01-10 02:52:12,708 iteration 1557 : loss : 0.055763, loss_ce: 0.023634
2022-01-10 02:52:14,139 iteration 1558 : loss : 0.048811, loss_ce: 0.024597
2022-01-10 02:52:15,555 iteration 1559 : loss : 0.046354, loss_ce: 0.016690
2022-01-10 02:52:16,910 iteration 1560 : loss : 0.030596, loss_ce: 0.014639
2022-01-10 02:52:18,342 iteration 1561 : loss : 0.051915, loss_ce: 0.019498
2022-01-10 02:52:19,704 iteration 1562 : loss : 0.057174, loss_ce: 0.016010
2022-01-10 02:52:21,155 iteration 1563 : loss : 0.034625, loss_ce: 0.015487
2022-01-10 02:52:22,641 iteration 1564 : loss : 0.038025, loss_ce: 0.017757
 23%|██████▉                       | 92/400 [40:29<2:11:56, 25.70s/it]2022-01-10 02:52:24,076 iteration 1565 : loss : 0.045843, loss_ce: 0.023943
2022-01-10 02:52:25,480 iteration 1566 : loss : 0.036125, loss_ce: 0.013347
2022-01-10 02:52:26,923 iteration 1567 : loss : 0.043764, loss_ce: 0.013190
2022-01-10 02:52:28,303 iteration 1568 : loss : 0.069420, loss_ce: 0.020806
2022-01-10 02:52:29,733 iteration 1569 : loss : 0.044231, loss_ce: 0.021254
2022-01-10 02:52:31,143 iteration 1570 : loss : 0.033295, loss_ce: 0.015650
2022-01-10 02:52:32,514 iteration 1571 : loss : 0.050000, loss_ce: 0.010857
2022-01-10 02:52:33,975 iteration 1572 : loss : 0.061353, loss_ce: 0.021733
2022-01-10 02:52:35,401 iteration 1573 : loss : 0.045833, loss_ce: 0.022090
2022-01-10 02:52:36,765 iteration 1574 : loss : 0.023043, loss_ce: 0.010166
2022-01-10 02:52:38,078 iteration 1575 : loss : 0.055264, loss_ce: 0.022918
2022-01-10 02:52:39,471 iteration 1576 : loss : 0.049682, loss_ce: 0.026624
2022-01-10 02:52:40,867 iteration 1577 : loss : 0.028019, loss_ce: 0.011380
2022-01-10 02:52:42,266 iteration 1578 : loss : 0.061147, loss_ce: 0.021394
2022-01-10 02:52:43,702 iteration 1579 : loss : 0.051162, loss_ce: 0.019669
2022-01-10 02:52:45,063 iteration 1580 : loss : 0.043919, loss_ce: 0.016557
2022-01-10 02:52:46,456 iteration 1581 : loss : 0.046528, loss_ce: 0.016412
 23%|██████▉                       | 93/400 [40:52<2:08:36, 25.14s/it]2022-01-10 02:52:47,971 iteration 1582 : loss : 0.048955, loss_ce: 0.015758
2022-01-10 02:52:49,366 iteration 1583 : loss : 0.053574, loss_ce: 0.014022
2022-01-10 02:52:50,790 iteration 1584 : loss : 0.029179, loss_ce: 0.016393
2022-01-10 02:52:52,191 iteration 1585 : loss : 0.034242, loss_ce: 0.011021
2022-01-10 02:52:53,578 iteration 1586 : loss : 0.040073, loss_ce: 0.017948
2022-01-10 02:52:54,886 iteration 1587 : loss : 0.030650, loss_ce: 0.012074
2022-01-10 02:52:56,311 iteration 1588 : loss : 0.038330, loss_ce: 0.016784
2022-01-10 02:52:57,716 iteration 1589 : loss : 0.032526, loss_ce: 0.012824
2022-01-10 02:52:59,241 iteration 1590 : loss : 0.073612, loss_ce: 0.028292
2022-01-10 02:53:00,606 iteration 1591 : loss : 0.035970, loss_ce: 0.012454
2022-01-10 02:53:02,009 iteration 1592 : loss : 0.044073, loss_ce: 0.015549
2022-01-10 02:53:03,505 iteration 1593 : loss : 0.052405, loss_ce: 0.023077
2022-01-10 02:53:04,993 iteration 1594 : loss : 0.050270, loss_ce: 0.017581
2022-01-10 02:53:06,391 iteration 1595 : loss : 0.031034, loss_ce: 0.010792
2022-01-10 02:53:07,805 iteration 1596 : loss : 0.047973, loss_ce: 0.023841
2022-01-10 02:53:09,207 iteration 1597 : loss : 0.048843, loss_ce: 0.023674
2022-01-10 02:53:10,633 iteration 1598 : loss : 0.032785, loss_ce: 0.012132
 24%|███████                       | 94/400 [41:17<2:06:44, 24.85s/it]2022-01-10 02:53:12,074 iteration 1599 : loss : 0.037848, loss_ce: 0.014550
2022-01-10 02:53:13,479 iteration 1600 : loss : 0.034641, loss_ce: 0.012166
2022-01-10 02:53:14,948 iteration 1601 : loss : 0.042720, loss_ce: 0.015969
2022-01-10 02:53:16,477 iteration 1602 : loss : 0.052183, loss_ce: 0.021135
2022-01-10 02:53:17,883 iteration 1603 : loss : 0.040912, loss_ce: 0.016453
2022-01-10 02:53:19,258 iteration 1604 : loss : 0.039937, loss_ce: 0.014387
2022-01-10 02:53:20,709 iteration 1605 : loss : 0.041759, loss_ce: 0.015302
2022-01-10 02:53:22,100 iteration 1606 : loss : 0.066062, loss_ce: 0.041820
2022-01-10 02:53:23,476 iteration 1607 : loss : 0.029148, loss_ce: 0.011772
2022-01-10 02:53:24,956 iteration 1608 : loss : 0.033602, loss_ce: 0.011938
2022-01-10 02:53:26,358 iteration 1609 : loss : 0.088303, loss_ce: 0.042507
2022-01-10 02:53:27,775 iteration 1610 : loss : 0.037355, loss_ce: 0.014826
2022-01-10 02:53:29,271 iteration 1611 : loss : 0.052890, loss_ce: 0.018287
2022-01-10 02:53:30,677 iteration 1612 : loss : 0.031616, loss_ce: 0.016241
2022-01-10 02:53:31,959 iteration 1613 : loss : 0.044938, loss_ce: 0.020099
2022-01-10 02:53:33,391 iteration 1614 : loss : 0.041693, loss_ce: 0.012277
2022-01-10 02:53:33,392 Training Data Eval:
2022-01-10 02:53:40,438   Average segmentation loss on training set: 0.0336
2022-01-10 02:53:40,439 Validation Data Eval:
2022-01-10 02:53:42,881   Average segmentation loss on validation set: 0.1206
2022-01-10 02:53:44,312 iteration 1615 : loss : 0.033583, loss_ce: 0.012892
 24%|███████▏                      | 95/400 [41:50<2:19:47, 27.50s/it]2022-01-10 02:53:45,816 iteration 1616 : loss : 0.042985, loss_ce: 0.016684
2022-01-10 02:53:47,244 iteration 1617 : loss : 0.068241, loss_ce: 0.019292
2022-01-10 02:53:48,584 iteration 1618 : loss : 0.034075, loss_ce: 0.014971
2022-01-10 02:53:49,929 iteration 1619 : loss : 0.030699, loss_ce: 0.011463
2022-01-10 02:53:51,254 iteration 1620 : loss : 0.048366, loss_ce: 0.014176
2022-01-10 02:53:52,672 iteration 1621 : loss : 0.061086, loss_ce: 0.033256
2022-01-10 02:53:54,204 iteration 1622 : loss : 0.072890, loss_ce: 0.019280
2022-01-10 02:53:55,611 iteration 1623 : loss : 0.052718, loss_ce: 0.024279
2022-01-10 02:53:56,983 iteration 1624 : loss : 0.036863, loss_ce: 0.013507
2022-01-10 02:53:58,373 iteration 1625 : loss : 0.036368, loss_ce: 0.014867
2022-01-10 02:53:59,758 iteration 1626 : loss : 0.030664, loss_ce: 0.013231
2022-01-10 02:54:01,150 iteration 1627 : loss : 0.049992, loss_ce: 0.018693
2022-01-10 02:54:02,635 iteration 1628 : loss : 0.034755, loss_ce: 0.017328
2022-01-10 02:54:04,031 iteration 1629 : loss : 0.029337, loss_ce: 0.010442
2022-01-10 02:54:05,378 iteration 1630 : loss : 0.039620, loss_ce: 0.015176
2022-01-10 02:54:06,900 iteration 1631 : loss : 0.039608, loss_ce: 0.016893
2022-01-10 02:54:08,382 iteration 1632 : loss : 0.043581, loss_ce: 0.017080
 24%|███████▏                      | 96/400 [42:14<2:14:05, 26.47s/it]2022-01-10 02:54:09,907 iteration 1633 : loss : 0.038836, loss_ce: 0.014818
2022-01-10 02:54:11,345 iteration 1634 : loss : 0.055531, loss_ce: 0.020247
2022-01-10 02:54:12,755 iteration 1635 : loss : 0.035138, loss_ce: 0.017970
2022-01-10 02:54:14,236 iteration 1636 : loss : 0.042047, loss_ce: 0.020935
2022-01-10 02:54:15,661 iteration 1637 : loss : 0.049392, loss_ce: 0.017794
2022-01-10 02:54:17,048 iteration 1638 : loss : 0.041106, loss_ce: 0.018014
2022-01-10 02:54:18,551 iteration 1639 : loss : 0.052067, loss_ce: 0.012180
2022-01-10 02:54:19,993 iteration 1640 : loss : 0.066437, loss_ce: 0.018185
2022-01-10 02:54:21,339 iteration 1641 : loss : 0.029918, loss_ce: 0.013301
2022-01-10 02:54:22,730 iteration 1642 : loss : 0.025914, loss_ce: 0.009576
2022-01-10 02:54:24,165 iteration 1643 : loss : 0.027776, loss_ce: 0.007792
2022-01-10 02:54:25,546 iteration 1644 : loss : 0.050279, loss_ce: 0.018982
2022-01-10 02:54:26,942 iteration 1645 : loss : 0.040619, loss_ce: 0.014812
2022-01-10 02:54:28,331 iteration 1646 : loss : 0.037868, loss_ce: 0.018481
2022-01-10 02:54:29,716 iteration 1647 : loss : 0.039355, loss_ce: 0.013907
2022-01-10 02:54:31,131 iteration 1648 : loss : 0.059845, loss_ce: 0.029219
2022-01-10 02:54:32,485 iteration 1649 : loss : 0.039806, loss_ce: 0.017058
 24%|███████▎                      | 97/400 [42:38<2:10:05, 25.76s/it]2022-01-10 02:54:33,982 iteration 1650 : loss : 0.047192, loss_ce: 0.019988
2022-01-10 02:54:35,511 iteration 1651 : loss : 0.063003, loss_ce: 0.022336
2022-01-10 02:54:36,875 iteration 1652 : loss : 0.049512, loss_ce: 0.014735
2022-01-10 02:54:38,234 iteration 1653 : loss : 0.037781, loss_ce: 0.013752
2022-01-10 02:54:39,679 iteration 1654 : loss : 0.062711, loss_ce: 0.018158
2022-01-10 02:54:41,142 iteration 1655 : loss : 0.049844, loss_ce: 0.018774
2022-01-10 02:54:42,538 iteration 1656 : loss : 0.052897, loss_ce: 0.021098
2022-01-10 02:54:44,058 iteration 1657 : loss : 0.060586, loss_ce: 0.020539
2022-01-10 02:54:45,523 iteration 1658 : loss : 0.064119, loss_ce: 0.035962
2022-01-10 02:54:46,887 iteration 1659 : loss : 0.028614, loss_ce: 0.010246
2022-01-10 02:54:48,402 iteration 1660 : loss : 0.056328, loss_ce: 0.021059
2022-01-10 02:54:49,898 iteration 1661 : loss : 0.038866, loss_ce: 0.012811
2022-01-10 02:54:51,313 iteration 1662 : loss : 0.035623, loss_ce: 0.013107
2022-01-10 02:54:52,615 iteration 1663 : loss : 0.029607, loss_ce: 0.015292
2022-01-10 02:54:54,101 iteration 1664 : loss : 0.049273, loss_ce: 0.019704
2022-01-10 02:54:55,561 iteration 1665 : loss : 0.040713, loss_ce: 0.014045
2022-01-10 02:54:57,040 iteration 1666 : loss : 0.040429, loss_ce: 0.016451
 24%|███████▎                      | 98/400 [43:03<2:07:50, 25.40s/it]2022-01-10 02:54:58,544 iteration 1667 : loss : 0.047311, loss_ce: 0.015306
2022-01-10 02:54:59,972 iteration 1668 : loss : 0.040180, loss_ce: 0.012867
2022-01-10 02:55:01,339 iteration 1669 : loss : 0.034315, loss_ce: 0.013352
2022-01-10 02:55:02,767 iteration 1670 : loss : 0.043199, loss_ce: 0.019444
2022-01-10 02:55:04,173 iteration 1671 : loss : 0.032753, loss_ce: 0.011856
2022-01-10 02:55:05,642 iteration 1672 : loss : 0.038981, loss_ce: 0.015357
2022-01-10 02:55:07,079 iteration 1673 : loss : 0.053734, loss_ce: 0.021828
2022-01-10 02:55:08,563 iteration 1674 : loss : 0.049914, loss_ce: 0.025896
2022-01-10 02:55:09,948 iteration 1675 : loss : 0.038919, loss_ce: 0.011183
2022-01-10 02:55:11,345 iteration 1676 : loss : 0.069070, loss_ce: 0.022959
2022-01-10 02:55:12,830 iteration 1677 : loss : 0.048390, loss_ce: 0.023294
2022-01-10 02:55:14,133 iteration 1678 : loss : 0.036745, loss_ce: 0.015499
2022-01-10 02:55:15,518 iteration 1679 : loss : 0.039603, loss_ce: 0.010808
2022-01-10 02:55:16,965 iteration 1680 : loss : 0.034219, loss_ce: 0.016874
2022-01-10 02:55:18,411 iteration 1681 : loss : 0.044396, loss_ce: 0.026213
2022-01-10 02:55:19,858 iteration 1682 : loss : 0.051309, loss_ce: 0.016945
2022-01-10 02:55:21,223 iteration 1683 : loss : 0.029003, loss_ce: 0.010152
 25%|███████▍                      | 99/400 [43:27<2:05:35, 25.04s/it]2022-01-10 02:55:22,723 iteration 1684 : loss : 0.046714, loss_ce: 0.016037
2022-01-10 02:55:24,139 iteration 1685 : loss : 0.042734, loss_ce: 0.012803
2022-01-10 02:55:25,536 iteration 1686 : loss : 0.023689, loss_ce: 0.009273
2022-01-10 02:55:26,886 iteration 1687 : loss : 0.034333, loss_ce: 0.015449
2022-01-10 02:55:28,417 iteration 1688 : loss : 0.045877, loss_ce: 0.021134
2022-01-10 02:55:29,849 iteration 1689 : loss : 0.061303, loss_ce: 0.026482
2022-01-10 02:55:31,307 iteration 1690 : loss : 0.055587, loss_ce: 0.025437
2022-01-10 02:55:32,650 iteration 1691 : loss : 0.079094, loss_ce: 0.020279
2022-01-10 02:55:34,110 iteration 1692 : loss : 0.108085, loss_ce: 0.043398
2022-01-10 02:55:35,405 iteration 1693 : loss : 0.031898, loss_ce: 0.013073
2022-01-10 02:55:36,840 iteration 1694 : loss : 0.028238, loss_ce: 0.012158
2022-01-10 02:55:38,317 iteration 1695 : loss : 0.046015, loss_ce: 0.020139
2022-01-10 02:55:39,796 iteration 1696 : loss : 0.043538, loss_ce: 0.016572
2022-01-10 02:55:41,201 iteration 1697 : loss : 0.036211, loss_ce: 0.013796
2022-01-10 02:55:42,660 iteration 1698 : loss : 0.054758, loss_ce: 0.020653
2022-01-10 02:55:44,082 iteration 1699 : loss : 0.053530, loss_ce: 0.029716
2022-01-10 02:55:44,082 Training Data Eval:
2022-01-10 02:55:51,133   Average segmentation loss on training set: 0.0385
2022-01-10 02:55:51,133 Validation Data Eval:
2022-01-10 02:55:53,568   Average segmentation loss on validation set: 0.1501
2022-01-10 02:55:54,949 iteration 1700 : loss : 0.042136, loss_ce: 0.013910
 25%|███████▎                     | 100/400 [44:01<2:18:12, 27.64s/it]2022-01-10 02:55:56,426 iteration 1701 : loss : 0.033282, loss_ce: 0.015255
2022-01-10 02:55:57,817 iteration 1702 : loss : 0.055361, loss_ce: 0.020995
2022-01-10 02:55:59,306 iteration 1703 : loss : 0.044251, loss_ce: 0.019348
2022-01-10 02:56:00,774 iteration 1704 : loss : 0.049322, loss_ce: 0.020224
2022-01-10 02:56:02,177 iteration 1705 : loss : 0.044649, loss_ce: 0.015760
2022-01-10 02:56:03,633 iteration 1706 : loss : 0.065544, loss_ce: 0.019343
2022-01-10 02:56:05,027 iteration 1707 : loss : 0.038540, loss_ce: 0.016469
2022-01-10 02:56:06,427 iteration 1708 : loss : 0.040840, loss_ce: 0.016841
2022-01-10 02:56:07,986 iteration 1709 : loss : 0.061303, loss_ce: 0.026606
2022-01-10 02:56:09,366 iteration 1710 : loss : 0.046253, loss_ce: 0.022375
2022-01-10 02:56:10,793 iteration 1711 : loss : 0.044406, loss_ce: 0.016388
2022-01-10 02:56:12,217 iteration 1712 : loss : 0.041650, loss_ce: 0.013242
2022-01-10 02:56:13,632 iteration 1713 : loss : 0.046037, loss_ce: 0.016855
2022-01-10 02:56:15,023 iteration 1714 : loss : 0.033080, loss_ce: 0.013096
2022-01-10 02:56:16,495 iteration 1715 : loss : 0.059374, loss_ce: 0.020530
2022-01-10 02:56:17,893 iteration 1716 : loss : 0.077965, loss_ce: 0.024984
2022-01-10 02:56:19,323 iteration 1717 : loss : 0.044394, loss_ce: 0.021032
 25%|███████▎                     | 101/400 [44:25<2:12:52, 26.66s/it]2022-01-10 02:56:20,830 iteration 1718 : loss : 0.040168, loss_ce: 0.016301
2022-01-10 02:56:22,319 iteration 1719 : loss : 0.036230, loss_ce: 0.011270
2022-01-10 02:56:23,770 iteration 1720 : loss : 0.053642, loss_ce: 0.020824
2022-01-10 02:56:25,200 iteration 1721 : loss : 0.063415, loss_ce: 0.030978
2022-01-10 02:56:26,574 iteration 1722 : loss : 0.036184, loss_ce: 0.015161
2022-01-10 02:56:27,957 iteration 1723 : loss : 0.043846, loss_ce: 0.024553
2022-01-10 02:56:29,296 iteration 1724 : loss : 0.033004, loss_ce: 0.014790
2022-01-10 02:56:30,651 iteration 1725 : loss : 0.033195, loss_ce: 0.014431
2022-01-10 02:56:32,167 iteration 1726 : loss : 0.052842, loss_ce: 0.021787
2022-01-10 02:56:33,625 iteration 1727 : loss : 0.054244, loss_ce: 0.018001
2022-01-10 02:56:35,127 iteration 1728 : loss : 0.057348, loss_ce: 0.029219
2022-01-10 02:56:36,510 iteration 1729 : loss : 0.038519, loss_ce: 0.014565
2022-01-10 02:56:37,935 iteration 1730 : loss : 0.043310, loss_ce: 0.017958
2022-01-10 02:56:39,308 iteration 1731 : loss : 0.043063, loss_ce: 0.018528
2022-01-10 02:56:40,732 iteration 1732 : loss : 0.054033, loss_ce: 0.023962
2022-01-10 02:56:42,146 iteration 1733 : loss : 0.034469, loss_ce: 0.012136
2022-01-10 02:56:43,535 iteration 1734 : loss : 0.061078, loss_ce: 0.018434
 26%|███████▍                     | 102/400 [44:50<2:08:44, 25.92s/it]2022-01-10 02:56:45,067 iteration 1735 : loss : 0.042013, loss_ce: 0.013411
2022-01-10 02:56:46,484 iteration 1736 : loss : 0.045120, loss_ce: 0.016786
2022-01-10 02:56:47,923 iteration 1737 : loss : 0.051047, loss_ce: 0.021843
2022-01-10 02:56:49,330 iteration 1738 : loss : 0.048557, loss_ce: 0.018036
2022-01-10 02:56:50,731 iteration 1739 : loss : 0.059260, loss_ce: 0.032638
2022-01-10 02:56:52,121 iteration 1740 : loss : 0.067733, loss_ce: 0.015656
2022-01-10 02:56:53,612 iteration 1741 : loss : 0.036270, loss_ce: 0.011656
2022-01-10 02:56:55,030 iteration 1742 : loss : 0.053918, loss_ce: 0.019869
2022-01-10 02:56:56,436 iteration 1743 : loss : 0.050120, loss_ce: 0.030954
2022-01-10 02:56:57,799 iteration 1744 : loss : 0.042692, loss_ce: 0.015307
2022-01-10 02:56:59,269 iteration 1745 : loss : 0.047813, loss_ce: 0.019818
2022-01-10 02:57:00,654 iteration 1746 : loss : 0.058654, loss_ce: 0.024401
2022-01-10 02:57:02,049 iteration 1747 : loss : 0.044958, loss_ce: 0.016593
2022-01-10 02:57:03,407 iteration 1748 : loss : 0.036494, loss_ce: 0.017104
2022-01-10 02:57:04,837 iteration 1749 : loss : 0.052112, loss_ce: 0.020413
2022-01-10 02:57:06,201 iteration 1750 : loss : 0.043607, loss_ce: 0.022586
2022-01-10 02:57:07,571 iteration 1751 : loss : 0.041616, loss_ce: 0.015395
 26%|███████▍                     | 103/400 [45:14<2:05:31, 25.36s/it]2022-01-10 02:57:09,035 iteration 1752 : loss : 0.032687, loss_ce: 0.013435
2022-01-10 02:57:10,525 iteration 1753 : loss : 0.085057, loss_ce: 0.048135
2022-01-10 02:57:11,962 iteration 1754 : loss : 0.033772, loss_ce: 0.014660
2022-01-10 02:57:13,392 iteration 1755 : loss : 0.054381, loss_ce: 0.014992
2022-01-10 02:57:14,852 iteration 1756 : loss : 0.059464, loss_ce: 0.031570
2022-01-10 02:57:16,273 iteration 1757 : loss : 0.052378, loss_ce: 0.014110
2022-01-10 02:57:17,701 iteration 1758 : loss : 0.051788, loss_ce: 0.022025
2022-01-10 02:57:19,099 iteration 1759 : loss : 0.047848, loss_ce: 0.015910
2022-01-10 02:57:20,560 iteration 1760 : loss : 0.050882, loss_ce: 0.023765
2022-01-10 02:57:21,920 iteration 1761 : loss : 0.030813, loss_ce: 0.014299
2022-01-10 02:57:23,386 iteration 1762 : loss : 0.065937, loss_ce: 0.029262
2022-01-10 02:57:24,760 iteration 1763 : loss : 0.053512, loss_ce: 0.019265
2022-01-10 02:57:26,201 iteration 1764 : loss : 0.109715, loss_ce: 0.031211
2022-01-10 02:57:27,653 iteration 1765 : loss : 0.054000, loss_ce: 0.016990
2022-01-10 02:57:29,016 iteration 1766 : loss : 0.037657, loss_ce: 0.017278
2022-01-10 02:57:30,424 iteration 1767 : loss : 0.065857, loss_ce: 0.028917
2022-01-10 02:57:31,903 iteration 1768 : loss : 0.043944, loss_ce: 0.014384
 26%|███████▌                     | 104/400 [45:38<2:03:34, 25.05s/it]2022-01-10 02:57:33,363 iteration 1769 : loss : 0.035257, loss_ce: 0.010200
2022-01-10 02:57:34,946 iteration 1770 : loss : 0.084985, loss_ce: 0.038863
2022-01-10 02:57:36,355 iteration 1771 : loss : 0.062034, loss_ce: 0.026541
2022-01-10 02:57:37,714 iteration 1772 : loss : 0.028591, loss_ce: 0.009007
2022-01-10 02:57:39,053 iteration 1773 : loss : 0.037822, loss_ce: 0.011456
2022-01-10 02:57:40,468 iteration 1774 : loss : 0.034113, loss_ce: 0.013966
2022-01-10 02:57:41,933 iteration 1775 : loss : 0.040404, loss_ce: 0.018352
2022-01-10 02:57:43,343 iteration 1776 : loss : 0.059418, loss_ce: 0.014888
2022-01-10 02:57:44,725 iteration 1777 : loss : 0.044085, loss_ce: 0.018176
2022-01-10 02:57:46,124 iteration 1778 : loss : 0.068312, loss_ce: 0.023916
2022-01-10 02:57:47,543 iteration 1779 : loss : 0.042553, loss_ce: 0.019835
2022-01-10 02:57:48,932 iteration 1780 : loss : 0.043637, loss_ce: 0.016102
2022-01-10 02:57:50,325 iteration 1781 : loss : 0.048484, loss_ce: 0.025206
2022-01-10 02:57:51,759 iteration 1782 : loss : 0.046112, loss_ce: 0.016698
2022-01-10 02:57:53,128 iteration 1783 : loss : 0.035997, loss_ce: 0.017450
2022-01-10 02:57:54,627 iteration 1784 : loss : 0.043925, loss_ce: 0.018344
2022-01-10 02:57:54,627 Training Data Eval:
2022-01-10 02:58:01,690   Average segmentation loss on training set: 0.0436
2022-01-10 02:58:01,690 Validation Data Eval:
2022-01-10 02:58:04,135   Average segmentation loss on validation set: 0.1602
2022-01-10 02:58:05,556 iteration 1785 : loss : 0.047737, loss_ce: 0.018193
 26%|███████▌                     | 105/400 [46:12<2:15:51, 27.63s/it]2022-01-10 02:58:07,141 iteration 1786 : loss : 0.051290, loss_ce: 0.027806
2022-01-10 02:58:08,648 iteration 1787 : loss : 0.037718, loss_ce: 0.015454
2022-01-10 02:58:10,093 iteration 1788 : loss : 0.042862, loss_ce: 0.018380
2022-01-10 02:58:11,514 iteration 1789 : loss : 0.044030, loss_ce: 0.014604
2022-01-10 02:58:12,915 iteration 1790 : loss : 0.043691, loss_ce: 0.019744
2022-01-10 02:58:14,271 iteration 1791 : loss : 0.026836, loss_ce: 0.012865
2022-01-10 02:58:15,710 iteration 1792 : loss : 0.056875, loss_ce: 0.018767
2022-01-10 02:58:17,165 iteration 1793 : loss : 0.041266, loss_ce: 0.019395
2022-01-10 02:58:18,543 iteration 1794 : loss : 0.034047, loss_ce: 0.016355
2022-01-10 02:58:19,968 iteration 1795 : loss : 0.053474, loss_ce: 0.022626
2022-01-10 02:58:21,389 iteration 1796 : loss : 0.058408, loss_ce: 0.029939
2022-01-10 02:58:22,895 iteration 1797 : loss : 0.030497, loss_ce: 0.011443
2022-01-10 02:58:24,315 iteration 1798 : loss : 0.045012, loss_ce: 0.016560
2022-01-10 02:58:25,759 iteration 1799 : loss : 0.050923, loss_ce: 0.020331
2022-01-10 02:58:27,193 iteration 1800 : loss : 0.058873, loss_ce: 0.027002
2022-01-10 02:58:28,643 iteration 1801 : loss : 0.028639, loss_ce: 0.011482
2022-01-10 02:58:29,983 iteration 1802 : loss : 0.057637, loss_ce: 0.017125
 26%|███████▋                     | 106/400 [46:36<2:10:40, 26.67s/it]2022-01-10 02:58:31,474 iteration 1803 : loss : 0.045026, loss_ce: 0.014491
2022-01-10 02:58:32,868 iteration 1804 : loss : 0.056820, loss_ce: 0.012179
2022-01-10 02:58:34,224 iteration 1805 : loss : 0.034892, loss_ce: 0.016389
2022-01-10 02:58:35,594 iteration 1806 : loss : 0.040359, loss_ce: 0.013971
2022-01-10 02:58:37,016 iteration 1807 : loss : 0.048297, loss_ce: 0.015203
2022-01-10 02:58:38,437 iteration 1808 : loss : 0.067021, loss_ce: 0.024659
2022-01-10 02:58:39,832 iteration 1809 : loss : 0.043784, loss_ce: 0.019765
2022-01-10 02:58:41,255 iteration 1810 : loss : 0.044100, loss_ce: 0.018663
2022-01-10 02:58:42,695 iteration 1811 : loss : 0.055513, loss_ce: 0.021136
2022-01-10 02:58:44,177 iteration 1812 : loss : 0.051158, loss_ce: 0.023838
2022-01-10 02:58:45,627 iteration 1813 : loss : 0.093917, loss_ce: 0.033898
2022-01-10 02:58:47,122 iteration 1814 : loss : 0.075887, loss_ce: 0.026902
2022-01-10 02:58:48,559 iteration 1815 : loss : 0.029649, loss_ce: 0.013894
2022-01-10 02:58:50,031 iteration 1816 : loss : 0.054173, loss_ce: 0.021535
2022-01-10 02:58:51,454 iteration 1817 : loss : 0.054230, loss_ce: 0.018781
2022-01-10 02:58:52,963 iteration 1818 : loss : 0.050349, loss_ce: 0.026627
2022-01-10 02:58:54,359 iteration 1819 : loss : 0.040134, loss_ce: 0.018840
 27%|███████▊                     | 107/400 [47:00<2:06:52, 25.98s/it]2022-01-10 02:58:55,881 iteration 1820 : loss : 0.084419, loss_ce: 0.024016
2022-01-10 02:58:57,265 iteration 1821 : loss : 0.034845, loss_ce: 0.012074
2022-01-10 02:58:58,644 iteration 1822 : loss : 0.039739, loss_ce: 0.021202
2022-01-10 02:59:00,092 iteration 1823 : loss : 0.040021, loss_ce: 0.017084
2022-01-10 02:59:01,499 iteration 1824 : loss : 0.035967, loss_ce: 0.013727
2022-01-10 02:59:02,923 iteration 1825 : loss : 0.037881, loss_ce: 0.013146
2022-01-10 02:59:04,259 iteration 1826 : loss : 0.064602, loss_ce: 0.028356
2022-01-10 02:59:05,630 iteration 1827 : loss : 0.054531, loss_ce: 0.018428
2022-01-10 02:59:07,076 iteration 1828 : loss : 0.037412, loss_ce: 0.016321
2022-01-10 02:59:08,518 iteration 1829 : loss : 0.051238, loss_ce: 0.014741
2022-01-10 02:59:09,928 iteration 1830 : loss : 0.032454, loss_ce: 0.013790
2022-01-10 02:59:11,321 iteration 1831 : loss : 0.035202, loss_ce: 0.013966
2022-01-10 02:59:12,713 iteration 1832 : loss : 0.037732, loss_ce: 0.013683
2022-01-10 02:59:14,092 iteration 1833 : loss : 0.046860, loss_ce: 0.028916
2022-01-10 02:59:15,564 iteration 1834 : loss : 0.042088, loss_ce: 0.014782
2022-01-10 02:59:16,893 iteration 1835 : loss : 0.041285, loss_ce: 0.013517
2022-01-10 02:59:18,265 iteration 1836 : loss : 0.043843, loss_ce: 0.017360
 27%|███████▊                     | 108/400 [47:24<2:03:25, 25.36s/it]2022-01-10 02:59:19,681 iteration 1837 : loss : 0.045960, loss_ce: 0.009910
2022-01-10 02:59:21,155 iteration 1838 : loss : 0.071422, loss_ce: 0.039252
2022-01-10 02:59:22,503 iteration 1839 : loss : 0.033665, loss_ce: 0.014220
2022-01-10 02:59:23,916 iteration 1840 : loss : 0.054527, loss_ce: 0.015132
2022-01-10 02:59:25,335 iteration 1841 : loss : 0.039691, loss_ce: 0.014706
2022-01-10 02:59:26,821 iteration 1842 : loss : 0.047382, loss_ce: 0.018495
2022-01-10 02:59:28,220 iteration 1843 : loss : 0.049283, loss_ce: 0.022282
2022-01-10 02:59:29,632 iteration 1844 : loss : 0.043561, loss_ce: 0.018389
2022-01-10 02:59:31,036 iteration 1845 : loss : 0.037606, loss_ce: 0.014162
2022-01-10 02:59:32,456 iteration 1846 : loss : 0.051434, loss_ce: 0.016425
2022-01-10 02:59:33,784 iteration 1847 : loss : 0.029567, loss_ce: 0.011147
2022-01-10 02:59:35,199 iteration 1848 : loss : 0.035535, loss_ce: 0.012636
2022-01-10 02:59:36,609 iteration 1849 : loss : 0.040317, loss_ce: 0.016352
2022-01-10 02:59:38,031 iteration 1850 : loss : 0.044015, loss_ce: 0.016378
2022-01-10 02:59:39,481 iteration 1851 : loss : 0.030952, loss_ce: 0.012899
2022-01-10 02:59:40,807 iteration 1852 : loss : 0.038886, loss_ce: 0.020577
2022-01-10 02:59:42,245 iteration 1853 : loss : 0.038379, loss_ce: 0.017269
 27%|███████▉                     | 109/400 [47:48<2:00:59, 24.95s/it]2022-01-10 02:59:43,738 iteration 1854 : loss : 0.046850, loss_ce: 0.014082
2022-01-10 02:59:45,200 iteration 1855 : loss : 0.046776, loss_ce: 0.019710
2022-01-10 02:59:46,523 iteration 1856 : loss : 0.035786, loss_ce: 0.016972
2022-01-10 02:59:47,851 iteration 1857 : loss : 0.029566, loss_ce: 0.013409
2022-01-10 02:59:49,205 iteration 1858 : loss : 0.022764, loss_ce: 0.010706
2022-01-10 02:59:50,620 iteration 1859 : loss : 0.046543, loss_ce: 0.019101
2022-01-10 02:59:52,062 iteration 1860 : loss : 0.048237, loss_ce: 0.018735
2022-01-10 02:59:53,478 iteration 1861 : loss : 0.032526, loss_ce: 0.015824
2022-01-10 02:59:54,910 iteration 1862 : loss : 0.032825, loss_ce: 0.016105
2022-01-10 02:59:56,354 iteration 1863 : loss : 0.063555, loss_ce: 0.024648
2022-01-10 02:59:57,789 iteration 1864 : loss : 0.035169, loss_ce: 0.013542
2022-01-10 02:59:59,206 iteration 1865 : loss : 0.055862, loss_ce: 0.016690
2022-01-10 03:00:00,625 iteration 1866 : loss : 0.026577, loss_ce: 0.009806
2022-01-10 03:00:01,939 iteration 1867 : loss : 0.031137, loss_ce: 0.011471
2022-01-10 03:00:03,314 iteration 1868 : loss : 0.036916, loss_ce: 0.013194
2022-01-10 03:00:04,700 iteration 1869 : loss : 0.034036, loss_ce: 0.017124
2022-01-10 03:00:04,700 Training Data Eval:
2022-01-10 03:00:11,757   Average segmentation loss on training set: 0.0310
2022-01-10 03:00:11,758 Validation Data Eval:
2022-01-10 03:00:14,209   Average segmentation loss on validation set: 0.1313
2022-01-10 03:00:15,640 iteration 1870 : loss : 0.046611, loss_ce: 0.019145
 28%|███████▉                     | 110/400 [48:22<2:12:49, 27.48s/it]2022-01-10 03:00:17,158 iteration 1871 : loss : 0.039763, loss_ce: 0.011989
2022-01-10 03:00:18,608 iteration 1872 : loss : 0.051661, loss_ce: 0.018404
2022-01-10 03:00:20,073 iteration 1873 : loss : 0.034536, loss_ce: 0.012075
2022-01-10 03:00:21,518 iteration 1874 : loss : 0.042278, loss_ce: 0.017805
2022-01-10 03:00:22,867 iteration 1875 : loss : 0.027055, loss_ce: 0.011181
2022-01-10 03:00:24,195 iteration 1876 : loss : 0.027108, loss_ce: 0.012230
2022-01-10 03:00:25,645 iteration 1877 : loss : 0.036642, loss_ce: 0.014782
2022-01-10 03:00:27,076 iteration 1878 : loss : 0.040657, loss_ce: 0.021201
2022-01-10 03:00:28,494 iteration 1879 : loss : 0.038991, loss_ce: 0.013789
2022-01-10 03:00:29,934 iteration 1880 : loss : 0.045936, loss_ce: 0.020116
2022-01-10 03:00:31,345 iteration 1881 : loss : 0.043774, loss_ce: 0.018517
2022-01-10 03:00:32,819 iteration 1882 : loss : 0.053635, loss_ce: 0.016986
2022-01-10 03:00:34,249 iteration 1883 : loss : 0.029746, loss_ce: 0.010838
2022-01-10 03:00:35,628 iteration 1884 : loss : 0.031391, loss_ce: 0.011183
2022-01-10 03:00:36,969 iteration 1885 : loss : 0.024944, loss_ce: 0.009927
2022-01-10 03:00:38,395 iteration 1886 : loss : 0.047061, loss_ce: 0.017366
2022-01-10 03:00:39,817 iteration 1887 : loss : 0.055226, loss_ce: 0.023952
 28%|████████                     | 111/400 [48:46<2:07:34, 26.49s/it]2022-01-10 03:00:41,390 iteration 1888 : loss : 0.042445, loss_ce: 0.016910
2022-01-10 03:00:42,754 iteration 1889 : loss : 0.062703, loss_ce: 0.016457
2022-01-10 03:00:44,100 iteration 1890 : loss : 0.032455, loss_ce: 0.014716
2022-01-10 03:00:45,461 iteration 1891 : loss : 0.027551, loss_ce: 0.009833
2022-01-10 03:00:46,859 iteration 1892 : loss : 0.039331, loss_ce: 0.019112
2022-01-10 03:00:48,287 iteration 1893 : loss : 0.034953, loss_ce: 0.010663
2022-01-10 03:00:49,693 iteration 1894 : loss : 0.057743, loss_ce: 0.030887
2022-01-10 03:00:51,040 iteration 1895 : loss : 0.032058, loss_ce: 0.012248
2022-01-10 03:00:52,513 iteration 1896 : loss : 0.044993, loss_ce: 0.022657
2022-01-10 03:00:53,952 iteration 1897 : loss : 0.072254, loss_ce: 0.026747
2022-01-10 03:00:55,445 iteration 1898 : loss : 0.048033, loss_ce: 0.021055
2022-01-10 03:00:56,860 iteration 1899 : loss : 0.049858, loss_ce: 0.016123
2022-01-10 03:00:58,228 iteration 1900 : loss : 0.044569, loss_ce: 0.017286
2022-01-10 03:00:59,572 iteration 1901 : loss : 0.028226, loss_ce: 0.009712
2022-01-10 03:01:00,975 iteration 1902 : loss : 0.037532, loss_ce: 0.014957
2022-01-10 03:01:02,438 iteration 1903 : loss : 0.033664, loss_ce: 0.012926
2022-01-10 03:01:03,908 iteration 1904 : loss : 0.030838, loss_ce: 0.012494
 28%|████████                     | 112/400 [49:10<2:03:42, 25.77s/it]2022-01-10 03:01:05,317 iteration 1905 : loss : 0.029341, loss_ce: 0.013350
2022-01-10 03:01:06,765 iteration 1906 : loss : 0.029914, loss_ce: 0.010392
2022-01-10 03:01:08,222 iteration 1907 : loss : 0.040497, loss_ce: 0.018794
2022-01-10 03:01:09,773 iteration 1908 : loss : 0.068142, loss_ce: 0.031871
2022-01-10 03:01:11,110 iteration 1909 : loss : 0.026275, loss_ce: 0.011289
2022-01-10 03:01:12,508 iteration 1910 : loss : 0.032664, loss_ce: 0.011894
2022-01-10 03:01:13,961 iteration 1911 : loss : 0.033032, loss_ce: 0.015239
2022-01-10 03:01:15,478 iteration 1912 : loss : 0.035240, loss_ce: 0.013770
2022-01-10 03:01:16,924 iteration 1913 : loss : 0.040253, loss_ce: 0.014546
2022-01-10 03:01:18,426 iteration 1914 : loss : 0.038663, loss_ce: 0.015523
2022-01-10 03:01:19,849 iteration 1915 : loss : 0.029307, loss_ce: 0.010071
2022-01-10 03:01:21,244 iteration 1916 : loss : 0.053172, loss_ce: 0.019248
2022-01-10 03:01:22,641 iteration 1917 : loss : 0.030004, loss_ce: 0.012793
2022-01-10 03:01:24,102 iteration 1918 : loss : 0.047620, loss_ce: 0.025346
2022-01-10 03:01:25,480 iteration 1919 : loss : 0.045732, loss_ce: 0.014113
2022-01-10 03:01:26,818 iteration 1920 : loss : 0.038483, loss_ce: 0.016284
2022-01-10 03:01:28,307 iteration 1921 : loss : 0.043441, loss_ce: 0.015356
 28%|████████▏                    | 113/400 [49:34<2:01:18, 25.36s/it]2022-01-10 03:01:29,838 iteration 1922 : loss : 0.058538, loss_ce: 0.024449
2022-01-10 03:01:31,217 iteration 1923 : loss : 0.027641, loss_ce: 0.010698
2022-01-10 03:01:32,697 iteration 1924 : loss : 0.060808, loss_ce: 0.020618
2022-01-10 03:01:34,061 iteration 1925 : loss : 0.046933, loss_ce: 0.015300
2022-01-10 03:01:35,533 iteration 1926 : loss : 0.044056, loss_ce: 0.017524
2022-01-10 03:01:37,021 iteration 1927 : loss : 0.033137, loss_ce: 0.014904
2022-01-10 03:01:38,395 iteration 1928 : loss : 0.039354, loss_ce: 0.012300
2022-01-10 03:01:39,950 iteration 1929 : loss : 0.059390, loss_ce: 0.027930
2022-01-10 03:01:41,368 iteration 1930 : loss : 0.031888, loss_ce: 0.013858
2022-01-10 03:01:42,839 iteration 1931 : loss : 0.064269, loss_ce: 0.023741
2022-01-10 03:01:44,283 iteration 1932 : loss : 0.089335, loss_ce: 0.024893
2022-01-10 03:01:45,729 iteration 1933 : loss : 0.052413, loss_ce: 0.018451
2022-01-10 03:01:47,137 iteration 1934 : loss : 0.069370, loss_ce: 0.024168
2022-01-10 03:01:48,559 iteration 1935 : loss : 0.031230, loss_ce: 0.012678
2022-01-10 03:01:49,963 iteration 1936 : loss : 0.032363, loss_ce: 0.013768
2022-01-10 03:01:51,325 iteration 1937 : loss : 0.039866, loss_ce: 0.015116
2022-01-10 03:01:52,761 iteration 1938 : loss : 0.051203, loss_ce: 0.021471
 28%|████████▎                    | 114/400 [49:59<1:59:35, 25.09s/it]2022-01-10 03:01:54,266 iteration 1939 : loss : 0.045738, loss_ce: 0.017397
2022-01-10 03:01:55,632 iteration 1940 : loss : 0.044327, loss_ce: 0.024612
2022-01-10 03:01:57,047 iteration 1941 : loss : 0.028886, loss_ce: 0.010773
2022-01-10 03:01:58,476 iteration 1942 : loss : 0.040021, loss_ce: 0.012781
2022-01-10 03:01:59,881 iteration 1943 : loss : 0.059218, loss_ce: 0.018665
2022-01-10 03:02:01,299 iteration 1944 : loss : 0.046691, loss_ce: 0.018876
2022-01-10 03:02:02,732 iteration 1945 : loss : 0.065725, loss_ce: 0.029183
2022-01-10 03:02:04,121 iteration 1946 : loss : 0.059771, loss_ce: 0.016685
2022-01-10 03:02:05,525 iteration 1947 : loss : 0.043713, loss_ce: 0.011418
2022-01-10 03:02:06,961 iteration 1948 : loss : 0.039942, loss_ce: 0.016430
2022-01-10 03:02:08,337 iteration 1949 : loss : 0.033312, loss_ce: 0.011703
2022-01-10 03:02:09,790 iteration 1950 : loss : 0.068250, loss_ce: 0.026294
2022-01-10 03:02:11,234 iteration 1951 : loss : 0.040108, loss_ce: 0.015796
2022-01-10 03:02:12,658 iteration 1952 : loss : 0.034384, loss_ce: 0.014615
2022-01-10 03:02:14,001 iteration 1953 : loss : 0.048343, loss_ce: 0.028254
2022-01-10 03:02:15,437 iteration 1954 : loss : 0.033407, loss_ce: 0.013542
2022-01-10 03:02:15,437 Training Data Eval:
2022-01-10 03:02:22,495   Average segmentation loss on training set: 0.0293
2022-01-10 03:02:22,495 Validation Data Eval:
2022-01-10 03:02:24,930   Average segmentation loss on validation set: 0.1186
2022-01-10 03:02:26,378 iteration 1955 : loss : 0.038237, loss_ce: 0.014455
 29%|████████▎                    | 115/400 [50:32<2:11:18, 27.65s/it]2022-01-10 03:02:27,836 iteration 1956 : loss : 0.068616, loss_ce: 0.025374
2022-01-10 03:02:29,298 iteration 1957 : loss : 0.055928, loss_ce: 0.031271
2022-01-10 03:02:30,700 iteration 1958 : loss : 0.069175, loss_ce: 0.015655
2022-01-10 03:02:32,094 iteration 1959 : loss : 0.038121, loss_ce: 0.011112
2022-01-10 03:02:33,414 iteration 1960 : loss : 0.038588, loss_ce: 0.016430
2022-01-10 03:02:34,713 iteration 1961 : loss : 0.032364, loss_ce: 0.011084
2022-01-10 03:02:36,165 iteration 1962 : loss : 0.042554, loss_ce: 0.017127
2022-01-10 03:02:37,530 iteration 1963 : loss : 0.034637, loss_ce: 0.012792
2022-01-10 03:02:38,977 iteration 1964 : loss : 0.051058, loss_ce: 0.013999
2022-01-10 03:02:40,306 iteration 1965 : loss : 0.031264, loss_ce: 0.013828
2022-01-10 03:02:41,708 iteration 1966 : loss : 0.028928, loss_ce: 0.011698
2022-01-10 03:02:43,224 iteration 1967 : loss : 0.049144, loss_ce: 0.021929
2022-01-10 03:02:44,796 iteration 1968 : loss : 0.085353, loss_ce: 0.034912
2022-01-10 03:02:46,256 iteration 1969 : loss : 0.047777, loss_ce: 0.016878
2022-01-10 03:02:47,602 iteration 1970 : loss : 0.027284, loss_ce: 0.010942
2022-01-10 03:02:49,023 iteration 1971 : loss : 0.039257, loss_ce: 0.015291
2022-01-10 03:02:50,444 iteration 1972 : loss : 0.057021, loss_ce: 0.015272
 29%|████████▍                    | 116/400 [50:56<2:05:46, 26.57s/it]2022-01-10 03:02:51,861 iteration 1973 : loss : 0.037494, loss_ce: 0.016252
2022-01-10 03:02:53,233 iteration 1974 : loss : 0.035233, loss_ce: 0.012923
2022-01-10 03:02:54,671 iteration 1975 : loss : 0.049792, loss_ce: 0.019025
2022-01-10 03:02:56,047 iteration 1976 : loss : 0.040218, loss_ce: 0.017612
2022-01-10 03:02:57,516 iteration 1977 : loss : 0.034454, loss_ce: 0.011854
2022-01-10 03:02:59,009 iteration 1978 : loss : 0.049436, loss_ce: 0.018175
2022-01-10 03:03:00,364 iteration 1979 : loss : 0.033142, loss_ce: 0.013566
2022-01-10 03:03:01,725 iteration 1980 : loss : 0.031956, loss_ce: 0.011214
2022-01-10 03:03:03,110 iteration 1981 : loss : 0.032553, loss_ce: 0.014681
2022-01-10 03:03:04,565 iteration 1982 : loss : 0.044034, loss_ce: 0.016994
2022-01-10 03:03:05,911 iteration 1983 : loss : 0.061245, loss_ce: 0.022878
2022-01-10 03:03:07,376 iteration 1984 : loss : 0.043140, loss_ce: 0.016950
2022-01-10 03:03:08,782 iteration 1985 : loss : 0.059506, loss_ce: 0.022937
2022-01-10 03:03:10,244 iteration 1986 : loss : 0.047619, loss_ce: 0.016835
2022-01-10 03:03:11,631 iteration 1987 : loss : 0.026604, loss_ce: 0.009185
2022-01-10 03:03:12,965 iteration 1988 : loss : 0.029295, loss_ce: 0.013188
2022-01-10 03:03:14,366 iteration 1989 : loss : 0.051097, loss_ce: 0.016093
 29%|████████▍                    | 117/400 [51:20<2:01:34, 25.78s/it]2022-01-10 03:03:15,833 iteration 1990 : loss : 0.069210, loss_ce: 0.021604
2022-01-10 03:03:17,173 iteration 1991 : loss : 0.031781, loss_ce: 0.013160
2022-01-10 03:03:18,604 iteration 1992 : loss : 0.031103, loss_ce: 0.010402
2022-01-10 03:03:20,037 iteration 1993 : loss : 0.041965, loss_ce: 0.016198
2022-01-10 03:03:21,508 iteration 1994 : loss : 0.050552, loss_ce: 0.024814
2022-01-10 03:03:22,895 iteration 1995 : loss : 0.033028, loss_ce: 0.012829
2022-01-10 03:03:24,335 iteration 1996 : loss : 0.048605, loss_ce: 0.022111
2022-01-10 03:03:25,696 iteration 1997 : loss : 0.036445, loss_ce: 0.015342
2022-01-10 03:03:27,109 iteration 1998 : loss : 0.051593, loss_ce: 0.020143
2022-01-10 03:03:28,545 iteration 1999 : loss : 0.032638, loss_ce: 0.011874
2022-01-10 03:03:29,951 iteration 2000 : loss : 0.039111, loss_ce: 0.017296
2022-01-10 03:03:31,313 iteration 2001 : loss : 0.038332, loss_ce: 0.013417
2022-01-10 03:03:32,770 iteration 2002 : loss : 0.031576, loss_ce: 0.012132
2022-01-10 03:03:34,130 iteration 2003 : loss : 0.049786, loss_ce: 0.016713
2022-01-10 03:03:35,567 iteration 2004 : loss : 0.039289, loss_ce: 0.019523
2022-01-10 03:03:36,934 iteration 2005 : loss : 0.035529, loss_ce: 0.017135
2022-01-10 03:03:38,332 iteration 2006 : loss : 0.036260, loss_ce: 0.012472
 30%|████████▌                    | 118/400 [51:44<1:58:35, 25.23s/it]2022-01-10 03:03:39,870 iteration 2007 : loss : 0.054317, loss_ce: 0.020279
2022-01-10 03:03:41,311 iteration 2008 : loss : 0.033612, loss_ce: 0.014334
2022-01-10 03:03:42,683 iteration 2009 : loss : 0.030690, loss_ce: 0.013907
2022-01-10 03:03:44,103 iteration 2010 : loss : 0.050384, loss_ce: 0.021262
2022-01-10 03:03:45,451 iteration 2011 : loss : 0.058321, loss_ce: 0.023382
2022-01-10 03:03:46,788 iteration 2012 : loss : 0.031719, loss_ce: 0.013181
2022-01-10 03:03:48,156 iteration 2013 : loss : 0.035194, loss_ce: 0.018529
2022-01-10 03:03:49,580 iteration 2014 : loss : 0.032742, loss_ce: 0.013989
2022-01-10 03:03:51,022 iteration 2015 : loss : 0.042967, loss_ce: 0.015508
2022-01-10 03:03:52,497 iteration 2016 : loss : 0.042759, loss_ce: 0.018426
2022-01-10 03:03:53,932 iteration 2017 : loss : 0.047431, loss_ce: 0.017739
2022-01-10 03:03:55,272 iteration 2018 : loss : 0.037487, loss_ce: 0.012061
2022-01-10 03:03:56,682 iteration 2019 : loss : 0.029846, loss_ce: 0.012161
2022-01-10 03:03:58,083 iteration 2020 : loss : 0.034877, loss_ce: 0.013364
2022-01-10 03:03:59,470 iteration 2021 : loss : 0.040329, loss_ce: 0.016768
2022-01-10 03:04:00,877 iteration 2022 : loss : 0.031862, loss_ce: 0.010809
2022-01-10 03:04:02,300 iteration 2023 : loss : 0.030882, loss_ce: 0.011235
 30%|████████▋                    | 119/400 [52:08<1:56:23, 24.85s/it]2022-01-10 03:04:03,786 iteration 2024 : loss : 0.062443, loss_ce: 0.031577
2022-01-10 03:04:05,345 iteration 2025 : loss : 0.072517, loss_ce: 0.024633
2022-01-10 03:04:06,724 iteration 2026 : loss : 0.028745, loss_ce: 0.009178
2022-01-10 03:04:08,164 iteration 2027 : loss : 0.030163, loss_ce: 0.010599
2022-01-10 03:04:09,483 iteration 2028 : loss : 0.028493, loss_ce: 0.012160
2022-01-10 03:04:10,824 iteration 2029 : loss : 0.041191, loss_ce: 0.013590
2022-01-10 03:04:12,251 iteration 2030 : loss : 0.036286, loss_ce: 0.012676
2022-01-10 03:04:13,683 iteration 2031 : loss : 0.041644, loss_ce: 0.014424
2022-01-10 03:04:15,102 iteration 2032 : loss : 0.033882, loss_ce: 0.015282
2022-01-10 03:04:16,416 iteration 2033 : loss : 0.025369, loss_ce: 0.011560
2022-01-10 03:04:17,785 iteration 2034 : loss : 0.037689, loss_ce: 0.013854
2022-01-10 03:04:19,220 iteration 2035 : loss : 0.039681, loss_ce: 0.015339
2022-01-10 03:04:20,602 iteration 2036 : loss : 0.027416, loss_ce: 0.012536
2022-01-10 03:04:21,960 iteration 2037 : loss : 0.033660, loss_ce: 0.013547
2022-01-10 03:04:23,459 iteration 2038 : loss : 0.063052, loss_ce: 0.019177
2022-01-10 03:04:24,900 iteration 2039 : loss : 0.039335, loss_ce: 0.014216
2022-01-10 03:04:24,900 Training Data Eval:
2022-01-10 03:04:31,958   Average segmentation loss on training set: 0.0242
2022-01-10 03:04:31,958 Validation Data Eval:
2022-01-10 03:04:34,400   Average segmentation loss on validation set: 0.0827
2022-01-10 03:04:35,864 iteration 2040 : loss : 0.032005, loss_ce: 0.015151
 30%|████████▋                    | 120/400 [52:42<2:08:11, 27.47s/it]2022-01-10 03:04:37,298 iteration 2041 : loss : 0.032518, loss_ce: 0.015203
2022-01-10 03:04:38,734 iteration 2042 : loss : 0.037133, loss_ce: 0.013514
2022-01-10 03:04:40,157 iteration 2043 : loss : 0.042712, loss_ce: 0.014302
2022-01-10 03:04:41,546 iteration 2044 : loss : 0.036513, loss_ce: 0.016890
2022-01-10 03:04:43,052 iteration 2045 : loss : 0.041914, loss_ce: 0.016636
2022-01-10 03:04:44,473 iteration 2046 : loss : 0.035994, loss_ce: 0.010435
2022-01-10 03:04:45,858 iteration 2047 : loss : 0.026313, loss_ce: 0.010984
2022-01-10 03:04:47,204 iteration 2048 : loss : 0.036902, loss_ce: 0.014304
2022-01-10 03:04:48,663 iteration 2049 : loss : 0.042743, loss_ce: 0.020780
2022-01-10 03:04:50,114 iteration 2050 : loss : 0.030606, loss_ce: 0.009339
2022-01-10 03:04:51,502 iteration 2051 : loss : 0.038882, loss_ce: 0.015715
2022-01-10 03:04:52,963 iteration 2052 : loss : 0.040898, loss_ce: 0.015211
2022-01-10 03:04:54,385 iteration 2053 : loss : 0.032760, loss_ce: 0.013791
2022-01-10 03:04:55,787 iteration 2054 : loss : 0.043675, loss_ce: 0.016351
2022-01-10 03:04:57,220 iteration 2055 : loss : 0.042856, loss_ce: 0.016328
2022-01-10 03:04:58,599 iteration 2056 : loss : 0.028546, loss_ce: 0.010148
2022-01-10 03:04:59,976 iteration 2057 : loss : 0.027612, loss_ce: 0.011739
 30%|████████▊                    | 121/400 [53:06<2:03:02, 26.46s/it]2022-01-10 03:05:01,438 iteration 2058 : loss : 0.034837, loss_ce: 0.018139
2022-01-10 03:05:02,893 iteration 2059 : loss : 0.046270, loss_ce: 0.018888
2022-01-10 03:05:04,373 iteration 2060 : loss : 0.039719, loss_ce: 0.014507
2022-01-10 03:05:05,781 iteration 2061 : loss : 0.027760, loss_ce: 0.013451
2022-01-10 03:05:07,142 iteration 2062 : loss : 0.039266, loss_ce: 0.015338
2022-01-10 03:05:08,705 iteration 2063 : loss : 0.039490, loss_ce: 0.014704
2022-01-10 03:05:10,127 iteration 2064 : loss : 0.044185, loss_ce: 0.016565
2022-01-10 03:05:11,612 iteration 2065 : loss : 0.038136, loss_ce: 0.013383
2022-01-10 03:05:13,017 iteration 2066 : loss : 0.040475, loss_ce: 0.015316
2022-01-10 03:05:14,471 iteration 2067 : loss : 0.044260, loss_ce: 0.016156
2022-01-10 03:05:15,836 iteration 2068 : loss : 0.023282, loss_ce: 0.008155
2022-01-10 03:05:17,257 iteration 2069 : loss : 0.054046, loss_ce: 0.026720
2022-01-10 03:05:18,718 iteration 2070 : loss : 0.035230, loss_ce: 0.016479
2022-01-10 03:05:20,083 iteration 2071 : loss : 0.027054, loss_ce: 0.011251
2022-01-10 03:05:21,451 iteration 2072 : loss : 0.032795, loss_ce: 0.016718
2022-01-10 03:05:22,913 iteration 2073 : loss : 0.042165, loss_ce: 0.017712
2022-01-10 03:05:24,258 iteration 2074 : loss : 0.035874, loss_ce: 0.011677
 30%|████████▊                    | 122/400 [53:30<1:59:34, 25.81s/it]2022-01-10 03:05:25,717 iteration 2075 : loss : 0.053129, loss_ce: 0.025848
2022-01-10 03:05:27,167 iteration 2076 : loss : 0.035246, loss_ce: 0.014539
2022-01-10 03:05:28,612 iteration 2077 : loss : 0.045389, loss_ce: 0.021581
2022-01-10 03:05:30,075 iteration 2078 : loss : 0.041231, loss_ce: 0.015441
2022-01-10 03:05:31,523 iteration 2079 : loss : 0.049377, loss_ce: 0.021244
2022-01-10 03:05:32,902 iteration 2080 : loss : 0.071679, loss_ce: 0.033722
2022-01-10 03:05:34,254 iteration 2081 : loss : 0.034349, loss_ce: 0.011920
2022-01-10 03:05:35,656 iteration 2082 : loss : 0.037356, loss_ce: 0.010372
2022-01-10 03:05:37,047 iteration 2083 : loss : 0.042818, loss_ce: 0.015794
2022-01-10 03:05:38,395 iteration 2084 : loss : 0.039141, loss_ce: 0.018847
2022-01-10 03:05:39,827 iteration 2085 : loss : 0.049462, loss_ce: 0.011739
2022-01-10 03:05:41,336 iteration 2086 : loss : 0.042898, loss_ce: 0.017081
2022-01-10 03:05:42,678 iteration 2087 : loss : 0.034927, loss_ce: 0.011708
2022-01-10 03:05:44,070 iteration 2088 : loss : 0.038084, loss_ce: 0.014415
2022-01-10 03:05:45,496 iteration 2089 : loss : 0.036373, loss_ce: 0.016090
2022-01-10 03:05:46,908 iteration 2090 : loss : 0.041364, loss_ce: 0.019465
2022-01-10 03:05:48,413 iteration 2091 : loss : 0.047916, loss_ce: 0.022120
 31%|████████▉                    | 123/400 [53:54<1:56:51, 25.31s/it]2022-01-10 03:05:49,867 iteration 2092 : loss : 0.033997, loss_ce: 0.011345
2022-01-10 03:05:51,218 iteration 2093 : loss : 0.022281, loss_ce: 0.008903
2022-01-10 03:05:52,662 iteration 2094 : loss : 0.033911, loss_ce: 0.012531
2022-01-10 03:05:54,240 iteration 2095 : loss : 0.049980, loss_ce: 0.019950
2022-01-10 03:05:55,638 iteration 2096 : loss : 0.035688, loss_ce: 0.013525
2022-01-10 03:05:57,034 iteration 2097 : loss : 0.059122, loss_ce: 0.016324
2022-01-10 03:05:58,389 iteration 2098 : loss : 0.032023, loss_ce: 0.011916
2022-01-10 03:05:59,780 iteration 2099 : loss : 0.031045, loss_ce: 0.013718
2022-01-10 03:06:01,235 iteration 2100 : loss : 0.038337, loss_ce: 0.013734
2022-01-10 03:06:02,562 iteration 2101 : loss : 0.050528, loss_ce: 0.014311
2022-01-10 03:06:04,003 iteration 2102 : loss : 0.044632, loss_ce: 0.020021
2022-01-10 03:06:05,368 iteration 2103 : loss : 0.034388, loss_ce: 0.016336
2022-01-10 03:06:06,695 iteration 2104 : loss : 0.032986, loss_ce: 0.012117
2022-01-10 03:06:08,110 iteration 2105 : loss : 0.044767, loss_ce: 0.018864
2022-01-10 03:06:09,532 iteration 2106 : loss : 0.042492, loss_ce: 0.015197
2022-01-10 03:06:10,900 iteration 2107 : loss : 0.032533, loss_ce: 0.011499
2022-01-10 03:06:12,247 iteration 2108 : loss : 0.028484, loss_ce: 0.011382
 31%|████████▉                    | 124/400 [54:18<1:54:24, 24.87s/it]2022-01-10 03:06:13,727 iteration 2109 : loss : 0.070394, loss_ce: 0.018674
2022-01-10 03:06:15,157 iteration 2110 : loss : 0.049293, loss_ce: 0.021967
2022-01-10 03:06:16,624 iteration 2111 : loss : 0.030440, loss_ce: 0.010971
2022-01-10 03:06:18,136 iteration 2112 : loss : 0.048154, loss_ce: 0.022562
2022-01-10 03:06:19,603 iteration 2113 : loss : 0.045201, loss_ce: 0.016170
2022-01-10 03:06:21,046 iteration 2114 : loss : 0.077270, loss_ce: 0.021989
2022-01-10 03:06:22,417 iteration 2115 : loss : 0.040702, loss_ce: 0.011797
2022-01-10 03:06:23,786 iteration 2116 : loss : 0.041784, loss_ce: 0.013622
2022-01-10 03:06:25,288 iteration 2117 : loss : 0.070574, loss_ce: 0.022071
2022-01-10 03:06:26,720 iteration 2118 : loss : 0.043802, loss_ce: 0.020135
2022-01-10 03:06:28,247 iteration 2119 : loss : 0.055841, loss_ce: 0.020920
2022-01-10 03:06:29,725 iteration 2120 : loss : 0.033369, loss_ce: 0.016707
2022-01-10 03:06:31,200 iteration 2121 : loss : 0.050336, loss_ce: 0.023417
2022-01-10 03:06:32,665 iteration 2122 : loss : 0.084247, loss_ce: 0.053147
2022-01-10 03:06:34,017 iteration 2123 : loss : 0.037956, loss_ce: 0.011846
2022-01-10 03:06:35,494 iteration 2124 : loss : 0.043044, loss_ce: 0.018183
2022-01-10 03:06:35,494 Training Data Eval:
2022-01-10 03:06:42,553   Average segmentation loss on training set: 0.0376
2022-01-10 03:06:42,554 Validation Data Eval:
2022-01-10 03:06:44,995   Average segmentation loss on validation set: 0.0845
2022-01-10 03:06:46,386 iteration 2125 : loss : 0.040586, loss_ce: 0.017998
 31%|█████████                    | 125/400 [54:52<2:06:44, 27.65s/it]2022-01-10 03:06:47,899 iteration 2126 : loss : 0.068371, loss_ce: 0.024805
2022-01-10 03:06:49,246 iteration 2127 : loss : 0.031677, loss_ce: 0.013808
2022-01-10 03:06:50,583 iteration 2128 : loss : 0.031627, loss_ce: 0.012957
2022-01-10 03:06:52,069 iteration 2129 : loss : 0.073382, loss_ce: 0.031596
2022-01-10 03:06:53,466 iteration 2130 : loss : 0.041591, loss_ce: 0.016915
2022-01-10 03:06:54,824 iteration 2131 : loss : 0.024839, loss_ce: 0.010288
2022-01-10 03:06:56,258 iteration 2132 : loss : 0.054012, loss_ce: 0.015912
2022-01-10 03:06:57,663 iteration 2133 : loss : 0.039235, loss_ce: 0.017583
2022-01-10 03:06:59,141 iteration 2134 : loss : 0.054092, loss_ce: 0.019547
2022-01-10 03:07:00,485 iteration 2135 : loss : 0.041142, loss_ce: 0.012952
2022-01-10 03:07:01,950 iteration 2136 : loss : 0.049175, loss_ce: 0.023360
2022-01-10 03:07:03,353 iteration 2137 : loss : 0.035293, loss_ce: 0.014626
2022-01-10 03:07:04,838 iteration 2138 : loss : 0.042524, loss_ce: 0.013399
2022-01-10 03:07:06,231 iteration 2139 : loss : 0.032399, loss_ce: 0.015424
2022-01-10 03:07:07,690 iteration 2140 : loss : 0.058872, loss_ce: 0.039454
2022-01-10 03:07:09,123 iteration 2141 : loss : 0.038480, loss_ce: 0.011870
2022-01-10 03:07:10,567 iteration 2142 : loss : 0.067824, loss_ce: 0.023067
 32%|█████████▏                   | 126/400 [55:17<2:01:30, 26.61s/it]2022-01-10 03:07:12,052 iteration 2143 : loss : 0.048113, loss_ce: 0.027501
2022-01-10 03:07:13,480 iteration 2144 : loss : 0.045474, loss_ce: 0.018791
2022-01-10 03:07:14,895 iteration 2145 : loss : 0.041015, loss_ce: 0.016462
2022-01-10 03:07:16,277 iteration 2146 : loss : 0.039517, loss_ce: 0.015132
2022-01-10 03:07:17,635 iteration 2147 : loss : 0.026454, loss_ce: 0.009906
2022-01-10 03:07:19,028 iteration 2148 : loss : 0.034645, loss_ce: 0.013196
2022-01-10 03:07:20,497 iteration 2149 : loss : 0.044296, loss_ce: 0.017549
2022-01-10 03:07:21,873 iteration 2150 : loss : 0.029506, loss_ce: 0.012936
2022-01-10 03:07:23,279 iteration 2151 : loss : 0.032932, loss_ce: 0.012259
2022-01-10 03:07:24,638 iteration 2152 : loss : 0.035379, loss_ce: 0.016955
2022-01-10 03:07:26,040 iteration 2153 : loss : 0.043269, loss_ce: 0.016470
2022-01-10 03:07:27,446 iteration 2154 : loss : 0.027632, loss_ce: 0.010865
2022-01-10 03:07:28,884 iteration 2155 : loss : 0.039743, loss_ce: 0.021994
2022-01-10 03:07:30,235 iteration 2156 : loss : 0.035927, loss_ce: 0.015150
2022-01-10 03:07:31,739 iteration 2157 : loss : 0.049394, loss_ce: 0.021042
2022-01-10 03:07:33,213 iteration 2158 : loss : 0.037203, loss_ce: 0.014771
2022-01-10 03:07:34,653 iteration 2159 : loss : 0.046190, loss_ce: 0.016827
 32%|█████████▏                   | 127/400 [55:41<1:57:36, 25.85s/it]2022-01-10 03:07:36,133 iteration 2160 : loss : 0.038879, loss_ce: 0.013899
2022-01-10 03:07:37,634 iteration 2161 : loss : 0.060542, loss_ce: 0.031729
2022-01-10 03:07:39,113 iteration 2162 : loss : 0.036792, loss_ce: 0.014123
2022-01-10 03:07:40,469 iteration 2163 : loss : 0.037133, loss_ce: 0.014469
2022-01-10 03:07:41,933 iteration 2164 : loss : 0.042706, loss_ce: 0.018037
2022-01-10 03:07:43,383 iteration 2165 : loss : 0.030883, loss_ce: 0.013222
2022-01-10 03:07:44,824 iteration 2166 : loss : 0.049161, loss_ce: 0.014345
2022-01-10 03:07:46,299 iteration 2167 : loss : 0.029140, loss_ce: 0.009130
2022-01-10 03:07:47,728 iteration 2168 : loss : 0.038289, loss_ce: 0.016490
2022-01-10 03:07:49,144 iteration 2169 : loss : 0.038600, loss_ce: 0.014547
2022-01-10 03:07:50,515 iteration 2170 : loss : 0.056040, loss_ce: 0.025471
2022-01-10 03:07:51,971 iteration 2171 : loss : 0.054473, loss_ce: 0.019382
2022-01-10 03:07:53,493 iteration 2172 : loss : 0.051494, loss_ce: 0.016666
2022-01-10 03:07:54,866 iteration 2173 : loss : 0.042660, loss_ce: 0.013819
2022-01-10 03:07:56,196 iteration 2174 : loss : 0.030454, loss_ce: 0.014402
2022-01-10 03:07:57,559 iteration 2175 : loss : 0.031049, loss_ce: 0.011979
2022-01-10 03:07:59,071 iteration 2176 : loss : 0.061859, loss_ce: 0.025024
 32%|█████████▎                   | 128/400 [56:05<1:55:15, 25.42s/it]2022-01-10 03:08:00,572 iteration 2177 : loss : 0.048675, loss_ce: 0.017505
2022-01-10 03:08:02,045 iteration 2178 : loss : 0.034038, loss_ce: 0.014268
2022-01-10 03:08:03,446 iteration 2179 : loss : 0.049195, loss_ce: 0.026418
2022-01-10 03:08:04,846 iteration 2180 : loss : 0.033951, loss_ce: 0.013053
2022-01-10 03:08:06,301 iteration 2181 : loss : 0.043714, loss_ce: 0.020982
2022-01-10 03:08:07,696 iteration 2182 : loss : 0.060672, loss_ce: 0.015478
2022-01-10 03:08:09,046 iteration 2183 : loss : 0.027682, loss_ce: 0.012238
2022-01-10 03:08:10,525 iteration 2184 : loss : 0.049479, loss_ce: 0.013324
2022-01-10 03:08:11,895 iteration 2185 : loss : 0.031141, loss_ce: 0.010681
2022-01-10 03:08:13,288 iteration 2186 : loss : 0.039390, loss_ce: 0.014740
2022-01-10 03:08:14,698 iteration 2187 : loss : 0.025569, loss_ce: 0.009405
2022-01-10 03:08:16,096 iteration 2188 : loss : 0.048320, loss_ce: 0.012298
2022-01-10 03:08:17,573 iteration 2189 : loss : 0.051996, loss_ce: 0.019105
2022-01-10 03:08:18,984 iteration 2190 : loss : 0.047560, loss_ce: 0.014378
2022-01-10 03:08:20,426 iteration 2191 : loss : 0.064514, loss_ce: 0.018296
2022-01-10 03:08:21,799 iteration 2192 : loss : 0.031736, loss_ce: 0.010577
2022-01-10 03:08:23,218 iteration 2193 : loss : 0.038981, loss_ce: 0.015568
 32%|█████████▎                   | 129/400 [56:29<1:53:05, 25.04s/it]2022-01-10 03:08:24,737 iteration 2194 : loss : 0.041311, loss_ce: 0.020193
2022-01-10 03:08:26,130 iteration 2195 : loss : 0.033019, loss_ce: 0.011311
2022-01-10 03:08:27,534 iteration 2196 : loss : 0.034154, loss_ce: 0.012537
2022-01-10 03:08:28,974 iteration 2197 : loss : 0.033680, loss_ce: 0.017209
2022-01-10 03:08:30,412 iteration 2198 : loss : 0.030475, loss_ce: 0.014529
2022-01-10 03:08:31,833 iteration 2199 : loss : 0.041413, loss_ce: 0.018881
2022-01-10 03:08:33,223 iteration 2200 : loss : 0.046335, loss_ce: 0.015966
2022-01-10 03:08:34,595 iteration 2201 : loss : 0.025059, loss_ce: 0.009877
2022-01-10 03:08:36,059 iteration 2202 : loss : 0.063631, loss_ce: 0.014668
2022-01-10 03:08:37,525 iteration 2203 : loss : 0.029422, loss_ce: 0.014822
2022-01-10 03:08:38,904 iteration 2204 : loss : 0.031084, loss_ce: 0.014195
2022-01-10 03:08:40,238 iteration 2205 : loss : 0.024376, loss_ce: 0.010332
2022-01-10 03:08:41,568 iteration 2206 : loss : 0.041124, loss_ce: 0.010559
2022-01-10 03:08:42,940 iteration 2207 : loss : 0.039712, loss_ce: 0.013082
2022-01-10 03:08:44,369 iteration 2208 : loss : 0.030441, loss_ce: 0.008477
2022-01-10 03:08:45,762 iteration 2209 : loss : 0.041780, loss_ce: 0.013959
2022-01-10 03:08:45,763 Training Data Eval:
2022-01-10 03:08:52,829   Average segmentation loss on training set: 0.0221
2022-01-10 03:08:52,829 Validation Data Eval:
2022-01-10 03:08:55,268   Average segmentation loss on validation set: 0.0953
2022-01-10 03:08:56,683 iteration 2210 : loss : 0.045472, loss_ce: 0.015892
 32%|█████████▍                   | 130/400 [57:03<2:04:03, 27.57s/it]2022-01-10 03:08:58,183 iteration 2211 : loss : 0.057794, loss_ce: 0.019591
2022-01-10 03:08:59,622 iteration 2212 : loss : 0.026821, loss_ce: 0.010789
2022-01-10 03:09:01,071 iteration 2213 : loss : 0.040765, loss_ce: 0.013661
2022-01-10 03:09:02,512 iteration 2214 : loss : 0.039358, loss_ce: 0.014848
2022-01-10 03:09:03,973 iteration 2215 : loss : 0.043226, loss_ce: 0.017800
2022-01-10 03:09:05,322 iteration 2216 : loss : 0.033630, loss_ce: 0.011217
2022-01-10 03:09:06,789 iteration 2217 : loss : 0.044177, loss_ce: 0.020031
2022-01-10 03:09:08,155 iteration 2218 : loss : 0.039241, loss_ce: 0.014649
2022-01-10 03:09:09,615 iteration 2219 : loss : 0.033424, loss_ce: 0.012818
2022-01-10 03:09:10,938 iteration 2220 : loss : 0.040682, loss_ce: 0.015461
2022-01-10 03:09:12,309 iteration 2221 : loss : 0.028043, loss_ce: 0.009301
2022-01-10 03:09:13,762 iteration 2222 : loss : 0.035480, loss_ce: 0.014223
2022-01-10 03:09:15,185 iteration 2223 : loss : 0.045084, loss_ce: 0.015021
2022-01-10 03:09:16,667 iteration 2224 : loss : 0.037345, loss_ce: 0.018180
2022-01-10 03:09:18,025 iteration 2225 : loss : 0.022265, loss_ce: 0.007964
2022-01-10 03:09:19,519 iteration 2226 : loss : 0.056250, loss_ce: 0.026177
2022-01-10 03:09:20,917 iteration 2227 : loss : 0.032307, loss_ce: 0.011740
 33%|█████████▍                   | 131/400 [57:27<1:59:06, 26.57s/it]2022-01-10 03:09:22,442 iteration 2228 : loss : 0.030698, loss_ce: 0.015397
2022-01-10 03:09:23,866 iteration 2229 : loss : 0.041252, loss_ce: 0.017591
2022-01-10 03:09:25,333 iteration 2230 : loss : 0.035280, loss_ce: 0.012295
2022-01-10 03:09:26,655 iteration 2231 : loss : 0.024313, loss_ce: 0.010478
2022-01-10 03:09:28,026 iteration 2232 : loss : 0.033541, loss_ce: 0.014078
2022-01-10 03:09:29,411 iteration 2233 : loss : 0.043841, loss_ce: 0.018172
2022-01-10 03:09:30,802 iteration 2234 : loss : 0.032188, loss_ce: 0.011221
2022-01-10 03:09:32,191 iteration 2235 : loss : 0.078907, loss_ce: 0.016942
2022-01-10 03:09:33,731 iteration 2236 : loss : 0.054981, loss_ce: 0.020896
2022-01-10 03:09:35,148 iteration 2237 : loss : 0.028405, loss_ce: 0.012194
2022-01-10 03:09:36,574 iteration 2238 : loss : 0.039101, loss_ce: 0.020323
2022-01-10 03:09:37,991 iteration 2239 : loss : 0.030631, loss_ce: 0.012286
2022-01-10 03:09:39,446 iteration 2240 : loss : 0.036132, loss_ce: 0.012271
2022-01-10 03:09:40,921 iteration 2241 : loss : 0.042315, loss_ce: 0.011520
2022-01-10 03:09:42,389 iteration 2242 : loss : 0.028309, loss_ce: 0.010411
2022-01-10 03:09:43,909 iteration 2243 : loss : 0.031247, loss_ce: 0.009472
2022-01-10 03:09:45,325 iteration 2244 : loss : 0.040329, loss_ce: 0.021100
 33%|█████████▌                   | 132/400 [57:51<1:55:45, 25.92s/it]2022-01-10 03:09:46,887 iteration 2245 : loss : 0.044194, loss_ce: 0.016465
2022-01-10 03:09:48,274 iteration 2246 : loss : 0.030995, loss_ce: 0.013252
2022-01-10 03:09:49,782 iteration 2247 : loss : 0.068923, loss_ce: 0.021722
2022-01-10 03:09:51,227 iteration 2248 : loss : 0.041482, loss_ce: 0.018480
2022-01-10 03:09:52,655 iteration 2249 : loss : 0.032386, loss_ce: 0.011523
2022-01-10 03:09:54,066 iteration 2250 : loss : 0.042086, loss_ce: 0.018509
2022-01-10 03:09:55,409 iteration 2251 : loss : 0.032385, loss_ce: 0.010680
2022-01-10 03:09:56,929 iteration 2252 : loss : 0.057215, loss_ce: 0.018487
2022-01-10 03:09:58,428 iteration 2253 : loss : 0.047697, loss_ce: 0.022002
2022-01-10 03:09:59,842 iteration 2254 : loss : 0.032017, loss_ce: 0.015659
2022-01-10 03:10:01,229 iteration 2255 : loss : 0.030091, loss_ce: 0.009678
2022-01-10 03:10:02,661 iteration 2256 : loss : 0.030002, loss_ce: 0.011268
2022-01-10 03:10:04,062 iteration 2257 : loss : 0.028737, loss_ce: 0.010247
2022-01-10 03:10:05,414 iteration 2258 : loss : 0.029785, loss_ce: 0.012696
2022-01-10 03:10:06,909 iteration 2259 : loss : 0.026888, loss_ce: 0.010071
2022-01-10 03:10:08,294 iteration 2260 : loss : 0.030038, loss_ce: 0.012909
2022-01-10 03:10:09,736 iteration 2261 : loss : 0.036937, loss_ce: 0.014122
 33%|█████████▋                   | 133/400 [58:16<1:53:19, 25.47s/it]2022-01-10 03:10:11,234 iteration 2262 : loss : 0.037200, loss_ce: 0.018118
2022-01-10 03:10:12,656 iteration 2263 : loss : 0.030731, loss_ce: 0.012986
2022-01-10 03:10:14,116 iteration 2264 : loss : 0.050700, loss_ce: 0.023911
2022-01-10 03:10:15,529 iteration 2265 : loss : 0.029984, loss_ce: 0.011924
2022-01-10 03:10:16,934 iteration 2266 : loss : 0.027929, loss_ce: 0.007905
2022-01-10 03:10:18,277 iteration 2267 : loss : 0.022605, loss_ce: 0.009004
2022-01-10 03:10:19,674 iteration 2268 : loss : 0.036755, loss_ce: 0.015787
2022-01-10 03:10:21,044 iteration 2269 : loss : 0.027258, loss_ce: 0.012558
2022-01-10 03:10:22,397 iteration 2270 : loss : 0.022350, loss_ce: 0.007746
2022-01-10 03:10:23,796 iteration 2271 : loss : 0.048483, loss_ce: 0.019670
2022-01-10 03:10:25,263 iteration 2272 : loss : 0.030140, loss_ce: 0.011268
2022-01-10 03:10:26,727 iteration 2273 : loss : 0.036823, loss_ce: 0.012571
2022-01-10 03:10:28,141 iteration 2274 : loss : 0.038453, loss_ce: 0.016325
2022-01-10 03:10:29,545 iteration 2275 : loss : 0.048776, loss_ce: 0.016666
2022-01-10 03:10:30,903 iteration 2276 : loss : 0.036044, loss_ce: 0.014037
2022-01-10 03:10:32,298 iteration 2277 : loss : 0.021950, loss_ce: 0.006420
2022-01-10 03:10:33,731 iteration 2278 : loss : 0.032062, loss_ce: 0.011417
 34%|█████████▋                   | 134/400 [58:40<1:50:56, 25.03s/it]2022-01-10 03:10:35,138 iteration 2279 : loss : 0.019919, loss_ce: 0.007234
2022-01-10 03:10:36,489 iteration 2280 : loss : 0.028891, loss_ce: 0.013558
2022-01-10 03:10:37,892 iteration 2281 : loss : 0.033946, loss_ce: 0.010881
2022-01-10 03:10:39,237 iteration 2282 : loss : 0.031386, loss_ce: 0.010417
2022-01-10 03:10:40,657 iteration 2283 : loss : 0.037453, loss_ce: 0.011234
2022-01-10 03:10:41,995 iteration 2284 : loss : 0.042212, loss_ce: 0.018263
2022-01-10 03:10:43,439 iteration 2285 : loss : 0.046117, loss_ce: 0.019570
2022-01-10 03:10:44,835 iteration 2286 : loss : 0.042507, loss_ce: 0.018756
2022-01-10 03:10:46,250 iteration 2287 : loss : 0.030062, loss_ce: 0.007749
2022-01-10 03:10:47,662 iteration 2288 : loss : 0.047273, loss_ce: 0.011990
2022-01-10 03:10:49,097 iteration 2289 : loss : 0.035427, loss_ce: 0.011248
2022-01-10 03:10:50,513 iteration 2290 : loss : 0.032089, loss_ce: 0.013281
2022-01-10 03:10:52,022 iteration 2291 : loss : 0.036272, loss_ce: 0.014098
2022-01-10 03:10:53,471 iteration 2292 : loss : 0.044657, loss_ce: 0.019330
2022-01-10 03:10:54,896 iteration 2293 : loss : 0.039122, loss_ce: 0.013549
2022-01-10 03:10:56,289 iteration 2294 : loss : 0.033966, loss_ce: 0.015382
2022-01-10 03:10:56,289 Training Data Eval:
2022-01-10 03:11:03,458   Average segmentation loss on training set: 0.0222
2022-01-10 03:11:03,459 Validation Data Eval:
2022-01-10 03:11:05,974   Average segmentation loss on validation set: 0.0747
2022-01-10 03:11:11,707 Found new lowest validation loss at iteration 2294! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 03:11:13,153 iteration 2295 : loss : 0.040457, loss_ce: 0.013596
 34%|█████████▊                   | 135/400 [59:19<2:09:35, 29.34s/it]2022-01-10 03:11:14,564 iteration 2296 : loss : 0.029672, loss_ce: 0.011112
2022-01-10 03:11:16,018 iteration 2297 : loss : 0.027578, loss_ce: 0.010099
2022-01-10 03:11:17,435 iteration 2298 : loss : 0.029207, loss_ce: 0.009629
2022-01-10 03:11:18,883 iteration 2299 : loss : 0.040398, loss_ce: 0.012680
2022-01-10 03:11:20,269 iteration 2300 : loss : 0.024852, loss_ce: 0.010861
2022-01-10 03:11:21,686 iteration 2301 : loss : 0.030406, loss_ce: 0.010828
2022-01-10 03:11:23,139 iteration 2302 : loss : 0.036661, loss_ce: 0.014085
2022-01-10 03:11:24,580 iteration 2303 : loss : 0.033566, loss_ce: 0.010638
2022-01-10 03:11:26,020 iteration 2304 : loss : 0.030579, loss_ce: 0.011461
2022-01-10 03:11:27,419 iteration 2305 : loss : 0.038454, loss_ce: 0.019543
2022-01-10 03:11:28,803 iteration 2306 : loss : 0.041595, loss_ce: 0.017074
2022-01-10 03:11:30,284 iteration 2307 : loss : 0.041695, loss_ce: 0.015441
2022-01-10 03:11:31,660 iteration 2308 : loss : 0.030066, loss_ce: 0.012194
2022-01-10 03:11:33,113 iteration 2309 : loss : 0.033772, loss_ce: 0.014232
2022-01-10 03:11:34,553 iteration 2310 : loss : 0.032875, loss_ce: 0.014714
2022-01-10 03:11:35,931 iteration 2311 : loss : 0.032394, loss_ce: 0.010979
2022-01-10 03:11:37,278 iteration 2312 : loss : 0.028668, loss_ce: 0.009622
 34%|█████████▊                   | 136/400 [59:43<2:02:14, 27.78s/it]2022-01-10 03:11:38,738 iteration 2313 : loss : 0.036370, loss_ce: 0.014749
2022-01-10 03:11:40,165 iteration 2314 : loss : 0.025906, loss_ce: 0.008256
2022-01-10 03:11:41,661 iteration 2315 : loss : 0.044557, loss_ce: 0.009904
2022-01-10 03:11:43,004 iteration 2316 : loss : 0.029158, loss_ce: 0.011398
2022-01-10 03:11:44,481 iteration 2317 : loss : 0.053604, loss_ce: 0.018844
2022-01-10 03:11:45,922 iteration 2318 : loss : 0.030390, loss_ce: 0.013031
2022-01-10 03:11:47,349 iteration 2319 : loss : 0.027063, loss_ce: 0.010793
2022-01-10 03:11:48,817 iteration 2320 : loss : 0.029784, loss_ce: 0.014264
2022-01-10 03:11:50,183 iteration 2321 : loss : 0.034906, loss_ce: 0.011637
2022-01-10 03:11:51,587 iteration 2322 : loss : 0.036197, loss_ce: 0.015989
2022-01-10 03:11:52,991 iteration 2323 : loss : 0.030744, loss_ce: 0.012904
2022-01-10 03:11:54,432 iteration 2324 : loss : 0.036910, loss_ce: 0.011642
2022-01-10 03:11:55,821 iteration 2325 : loss : 0.024898, loss_ce: 0.009820
2022-01-10 03:11:57,248 iteration 2326 : loss : 0.038207, loss_ce: 0.017610
2022-01-10 03:11:58,647 iteration 2327 : loss : 0.026993, loss_ce: 0.010364
2022-01-10 03:12:00,092 iteration 2328 : loss : 0.046456, loss_ce: 0.024196
2022-01-10 03:12:01,474 iteration 2329 : loss : 0.021241, loss_ce: 0.008188
 34%|█████████▏                 | 137/400 [1:00:07<1:57:02, 26.70s/it]2022-01-10 03:12:02,918 iteration 2330 : loss : 0.029220, loss_ce: 0.010666
2022-01-10 03:12:04,324 iteration 2331 : loss : 0.039486, loss_ce: 0.018914
2022-01-10 03:12:05,790 iteration 2332 : loss : 0.048549, loss_ce: 0.020669
2022-01-10 03:12:07,265 iteration 2333 : loss : 0.038393, loss_ce: 0.018900
2022-01-10 03:12:08,660 iteration 2334 : loss : 0.024172, loss_ce: 0.010679
2022-01-10 03:12:10,063 iteration 2335 : loss : 0.053954, loss_ce: 0.018988
2022-01-10 03:12:11,523 iteration 2336 : loss : 0.056417, loss_ce: 0.016206
2022-01-10 03:12:12,977 iteration 2337 : loss : 0.022602, loss_ce: 0.009468
2022-01-10 03:12:14,373 iteration 2338 : loss : 0.030649, loss_ce: 0.010580
2022-01-10 03:12:15,810 iteration 2339 : loss : 0.030191, loss_ce: 0.011857
2022-01-10 03:12:17,282 iteration 2340 : loss : 0.038049, loss_ce: 0.015859
2022-01-10 03:12:18,625 iteration 2341 : loss : 0.022930, loss_ce: 0.005633
2022-01-10 03:12:20,037 iteration 2342 : loss : 0.047030, loss_ce: 0.023010
2022-01-10 03:12:21,518 iteration 2343 : loss : 0.057977, loss_ce: 0.023661
2022-01-10 03:12:22,967 iteration 2344 : loss : 0.048594, loss_ce: 0.019451
2022-01-10 03:12:24,440 iteration 2345 : loss : 0.035492, loss_ce: 0.010809
2022-01-10 03:12:25,869 iteration 2346 : loss : 0.033301, loss_ce: 0.014719
 34%|█████████▎                 | 138/400 [1:00:32<1:53:34, 26.01s/it]2022-01-10 03:12:27,348 iteration 2347 : loss : 0.031637, loss_ce: 0.011634
2022-01-10 03:12:28,750 iteration 2348 : loss : 0.032745, loss_ce: 0.013157
2022-01-10 03:12:30,225 iteration 2349 : loss : 0.034841, loss_ce: 0.015582
2022-01-10 03:12:31,628 iteration 2350 : loss : 0.039084, loss_ce: 0.015616
2022-01-10 03:12:33,054 iteration 2351 : loss : 0.162689, loss_ce: 0.035097
2022-01-10 03:12:34,483 iteration 2352 : loss : 0.030372, loss_ce: 0.010640
2022-01-10 03:12:35,948 iteration 2353 : loss : 0.034532, loss_ce: 0.014177
2022-01-10 03:12:37,425 iteration 2354 : loss : 0.031410, loss_ce: 0.013564
2022-01-10 03:12:38,770 iteration 2355 : loss : 0.029962, loss_ce: 0.010905
2022-01-10 03:12:40,186 iteration 2356 : loss : 0.036286, loss_ce: 0.013706
2022-01-10 03:12:41,595 iteration 2357 : loss : 0.049212, loss_ce: 0.016442
2022-01-10 03:12:43,036 iteration 2358 : loss : 0.037280, loss_ce: 0.014741
2022-01-10 03:12:44,565 iteration 2359 : loss : 0.038641, loss_ce: 0.015713
2022-01-10 03:12:45,955 iteration 2360 : loss : 0.029100, loss_ce: 0.012901
2022-01-10 03:12:47,404 iteration 2361 : loss : 0.034888, loss_ce: 0.012421
2022-01-10 03:12:48,928 iteration 2362 : loss : 0.075109, loss_ce: 0.036400
2022-01-10 03:12:50,291 iteration 2363 : loss : 0.027972, loss_ce: 0.012550
 35%|█████████▍                 | 139/400 [1:00:56<1:51:04, 25.54s/it]2022-01-10 03:12:51,691 iteration 2364 : loss : 0.074411, loss_ce: 0.015572
2022-01-10 03:12:53,061 iteration 2365 : loss : 0.070365, loss_ce: 0.018112
2022-01-10 03:12:54,494 iteration 2366 : loss : 0.038437, loss_ce: 0.015232
2022-01-10 03:12:55,890 iteration 2367 : loss : 0.040095, loss_ce: 0.019971
2022-01-10 03:12:57,274 iteration 2368 : loss : 0.046673, loss_ce: 0.017873
2022-01-10 03:12:58,706 iteration 2369 : loss : 0.057656, loss_ce: 0.023092
2022-01-10 03:13:00,040 iteration 2370 : loss : 0.057298, loss_ce: 0.023344
2022-01-10 03:13:01,493 iteration 2371 : loss : 0.038802, loss_ce: 0.015244
2022-01-10 03:13:02,906 iteration 2372 : loss : 0.040232, loss_ce: 0.017368
2022-01-10 03:13:04,271 iteration 2373 : loss : 0.029825, loss_ce: 0.012363
2022-01-10 03:13:05,655 iteration 2374 : loss : 0.035488, loss_ce: 0.012459
2022-01-10 03:13:07,098 iteration 2375 : loss : 0.039573, loss_ce: 0.015037
2022-01-10 03:13:08,553 iteration 2376 : loss : 0.026753, loss_ce: 0.009834
2022-01-10 03:13:09,951 iteration 2377 : loss : 0.023922, loss_ce: 0.008449
2022-01-10 03:13:11,350 iteration 2378 : loss : 0.022195, loss_ce: 0.008053
2022-01-10 03:13:12,767 iteration 2379 : loss : 0.055115, loss_ce: 0.031371
2022-01-10 03:13:12,767 Training Data Eval:
2022-01-10 03:13:19,837   Average segmentation loss on training set: 0.0298
2022-01-10 03:13:19,838 Validation Data Eval:
2022-01-10 03:13:22,290   Average segmentation loss on validation set: 0.0883
2022-01-10 03:13:23,661 iteration 2380 : loss : 0.027008, loss_ce: 0.015104
 35%|█████████▍                 | 140/400 [1:01:30<2:00:50, 27.89s/it]2022-01-10 03:13:25,229 iteration 2381 : loss : 0.050270, loss_ce: 0.016194
2022-01-10 03:13:26,661 iteration 2382 : loss : 0.042469, loss_ce: 0.015915
2022-01-10 03:13:28,065 iteration 2383 : loss : 0.035814, loss_ce: 0.014073
2022-01-10 03:13:29,425 iteration 2384 : loss : 0.044812, loss_ce: 0.016515
2022-01-10 03:13:30,853 iteration 2385 : loss : 0.030888, loss_ce: 0.011379
2022-01-10 03:13:32,316 iteration 2386 : loss : 0.042033, loss_ce: 0.012376
2022-01-10 03:13:33,788 iteration 2387 : loss : 0.028274, loss_ce: 0.010538
2022-01-10 03:13:35,216 iteration 2388 : loss : 0.031797, loss_ce: 0.014477
2022-01-10 03:13:36,636 iteration 2389 : loss : 0.027096, loss_ce: 0.009387
2022-01-10 03:13:38,010 iteration 2390 : loss : 0.039772, loss_ce: 0.014967
2022-01-10 03:13:39,355 iteration 2391 : loss : 0.027838, loss_ce: 0.010602
2022-01-10 03:13:40,897 iteration 2392 : loss : 0.034119, loss_ce: 0.009675
2022-01-10 03:13:42,373 iteration 2393 : loss : 0.037081, loss_ce: 0.013560
2022-01-10 03:13:43,795 iteration 2394 : loss : 0.058802, loss_ce: 0.026469
2022-01-10 03:13:45,271 iteration 2395 : loss : 0.032025, loss_ce: 0.015187
2022-01-10 03:13:46,640 iteration 2396 : loss : 0.032488, loss_ce: 0.014050
2022-01-10 03:13:48,112 iteration 2397 : loss : 0.029566, loss_ce: 0.015918
 35%|█████████▌                 | 141/400 [1:01:54<1:55:55, 26.86s/it]2022-01-10 03:13:49,555 iteration 2398 : loss : 0.028496, loss_ce: 0.013180
2022-01-10 03:13:51,054 iteration 2399 : loss : 0.044959, loss_ce: 0.018695
2022-01-10 03:13:52,455 iteration 2400 : loss : 0.048256, loss_ce: 0.018260
2022-01-10 03:13:53,998 iteration 2401 : loss : 0.072464, loss_ce: 0.028230
2022-01-10 03:13:55,409 iteration 2402 : loss : 0.050247, loss_ce: 0.021435
2022-01-10 03:13:56,801 iteration 2403 : loss : 0.033692, loss_ce: 0.012399
2022-01-10 03:13:58,175 iteration 2404 : loss : 0.025111, loss_ce: 0.008754
2022-01-10 03:13:59,641 iteration 2405 : loss : 0.041051, loss_ce: 0.015274
2022-01-10 03:14:00,995 iteration 2406 : loss : 0.035711, loss_ce: 0.015739
2022-01-10 03:14:02,391 iteration 2407 : loss : 0.025554, loss_ce: 0.008970
2022-01-10 03:14:03,953 iteration 2408 : loss : 0.037467, loss_ce: 0.013671
2022-01-10 03:14:05,331 iteration 2409 : loss : 0.054447, loss_ce: 0.024441
2022-01-10 03:14:06,708 iteration 2410 : loss : 0.028293, loss_ce: 0.010854
2022-01-10 03:14:08,076 iteration 2411 : loss : 0.028765, loss_ce: 0.010775
2022-01-10 03:14:09,460 iteration 2412 : loss : 0.025147, loss_ce: 0.009703
2022-01-10 03:14:10,821 iteration 2413 : loss : 0.041771, loss_ce: 0.017188
2022-01-10 03:14:12,176 iteration 2414 : loss : 0.027283, loss_ce: 0.011287
 36%|█████████▌                 | 142/400 [1:02:18<1:51:52, 26.02s/it]2022-01-10 03:14:13,691 iteration 2415 : loss : 0.026793, loss_ce: 0.008882
2022-01-10 03:14:15,074 iteration 2416 : loss : 0.029247, loss_ce: 0.012403
2022-01-10 03:14:16,484 iteration 2417 : loss : 0.024571, loss_ce: 0.012723
2022-01-10 03:14:17,841 iteration 2418 : loss : 0.035355, loss_ce: 0.010525
2022-01-10 03:14:19,246 iteration 2419 : loss : 0.045878, loss_ce: 0.016106
2022-01-10 03:14:20,721 iteration 2420 : loss : 0.024748, loss_ce: 0.011003
2022-01-10 03:14:22,128 iteration 2421 : loss : 0.027990, loss_ce: 0.012828
2022-01-10 03:14:23,532 iteration 2422 : loss : 0.026770, loss_ce: 0.012084
2022-01-10 03:14:24,869 iteration 2423 : loss : 0.030309, loss_ce: 0.014310
2022-01-10 03:14:26,277 iteration 2424 : loss : 0.029551, loss_ce: 0.011610
2022-01-10 03:14:27,710 iteration 2425 : loss : 0.023911, loss_ce: 0.008011
2022-01-10 03:14:29,112 iteration 2426 : loss : 0.028038, loss_ce: 0.011187
2022-01-10 03:14:30,465 iteration 2427 : loss : 0.029248, loss_ce: 0.012548
2022-01-10 03:14:31,854 iteration 2428 : loss : 0.023717, loss_ce: 0.007285
2022-01-10 03:14:33,217 iteration 2429 : loss : 0.034185, loss_ce: 0.009615
2022-01-10 03:14:34,614 iteration 2430 : loss : 0.042142, loss_ce: 0.016991
2022-01-10 03:14:36,044 iteration 2431 : loss : 0.034210, loss_ce: 0.010138
 36%|█████████▋                 | 143/400 [1:02:42<1:48:40, 25.37s/it]2022-01-10 03:14:37,446 iteration 2432 : loss : 0.025333, loss_ce: 0.008233
2022-01-10 03:14:38,826 iteration 2433 : loss : 0.026020, loss_ce: 0.011933
2022-01-10 03:14:40,262 iteration 2434 : loss : 0.041366, loss_ce: 0.018263
2022-01-10 03:14:41,665 iteration 2435 : loss : 0.022759, loss_ce: 0.009312
2022-01-10 03:14:43,143 iteration 2436 : loss : 0.034711, loss_ce: 0.013791
2022-01-10 03:14:44,550 iteration 2437 : loss : 0.031064, loss_ce: 0.011838
2022-01-10 03:14:45,897 iteration 2438 : loss : 0.021936, loss_ce: 0.008715
2022-01-10 03:14:47,292 iteration 2439 : loss : 0.023396, loss_ce: 0.008699
2022-01-10 03:14:48,696 iteration 2440 : loss : 0.032979, loss_ce: 0.012345
2022-01-10 03:14:50,206 iteration 2441 : loss : 0.042271, loss_ce: 0.012098
2022-01-10 03:14:51,655 iteration 2442 : loss : 0.032319, loss_ce: 0.010739
2022-01-10 03:14:53,104 iteration 2443 : loss : 0.040416, loss_ce: 0.015411
2022-01-10 03:14:54,550 iteration 2444 : loss : 0.032278, loss_ce: 0.011672
2022-01-10 03:14:56,060 iteration 2445 : loss : 0.032530, loss_ce: 0.012540
2022-01-10 03:14:57,451 iteration 2446 : loss : 0.033862, loss_ce: 0.012334
2022-01-10 03:14:58,921 iteration 2447 : loss : 0.037292, loss_ce: 0.012245
2022-01-10 03:15:00,286 iteration 2448 : loss : 0.037041, loss_ce: 0.018468
 36%|█████████▋                 | 144/400 [1:03:06<1:46:49, 25.04s/it]2022-01-10 03:15:01,757 iteration 2449 : loss : 0.022834, loss_ce: 0.006680
2022-01-10 03:15:03,113 iteration 2450 : loss : 0.028264, loss_ce: 0.011315
2022-01-10 03:15:04,467 iteration 2451 : loss : 0.035018, loss_ce: 0.014479
2022-01-10 03:15:05,850 iteration 2452 : loss : 0.052082, loss_ce: 0.022538
2022-01-10 03:15:07,242 iteration 2453 : loss : 0.036517, loss_ce: 0.017687
2022-01-10 03:15:08,623 iteration 2454 : loss : 0.053958, loss_ce: 0.018805
2022-01-10 03:15:10,108 iteration 2455 : loss : 0.037794, loss_ce: 0.013532
2022-01-10 03:15:11,530 iteration 2456 : loss : 0.026387, loss_ce: 0.011884
2022-01-10 03:15:12,908 iteration 2457 : loss : 0.048548, loss_ce: 0.023424
2022-01-10 03:15:14,348 iteration 2458 : loss : 0.028891, loss_ce: 0.009709
2022-01-10 03:15:15,747 iteration 2459 : loss : 0.024896, loss_ce: 0.008624
2022-01-10 03:15:17,205 iteration 2460 : loss : 0.059718, loss_ce: 0.015970
2022-01-10 03:15:18,647 iteration 2461 : loss : 0.048767, loss_ce: 0.014615
2022-01-10 03:15:20,154 iteration 2462 : loss : 0.038740, loss_ce: 0.016257
2022-01-10 03:15:21,539 iteration 2463 : loss : 0.034428, loss_ce: 0.012679
2022-01-10 03:15:22,957 iteration 2464 : loss : 0.033587, loss_ce: 0.017337
2022-01-10 03:15:22,957 Training Data Eval:
2022-01-10 03:15:30,016   Average segmentation loss on training set: 0.0237
2022-01-10 03:15:30,016 Validation Data Eval:
2022-01-10 03:15:32,461   Average segmentation loss on validation set: 0.0859
2022-01-10 03:15:33,872 iteration 2465 : loss : 0.029057, loss_ce: 0.013386
 36%|█████████▊                 | 145/400 [1:03:40<1:57:17, 27.60s/it]2022-01-10 03:15:35,365 iteration 2466 : loss : 0.035027, loss_ce: 0.012649
2022-01-10 03:15:36,758 iteration 2467 : loss : 0.024081, loss_ce: 0.009740
2022-01-10 03:15:38,239 iteration 2468 : loss : 0.092947, loss_ce: 0.040941
2022-01-10 03:15:39,601 iteration 2469 : loss : 0.030694, loss_ce: 0.013368
2022-01-10 03:15:41,045 iteration 2470 : loss : 0.034356, loss_ce: 0.013523
2022-01-10 03:15:42,476 iteration 2471 : loss : 0.036087, loss_ce: 0.017410
2022-01-10 03:15:43,911 iteration 2472 : loss : 0.029748, loss_ce: 0.011175
2022-01-10 03:15:45,317 iteration 2473 : loss : 0.023544, loss_ce: 0.009528
2022-01-10 03:15:46,637 iteration 2474 : loss : 0.029951, loss_ce: 0.013383
2022-01-10 03:15:48,077 iteration 2475 : loss : 0.042437, loss_ce: 0.026623
2022-01-10 03:15:49,510 iteration 2476 : loss : 0.031635, loss_ce: 0.010005
2022-01-10 03:15:50,925 iteration 2477 : loss : 0.039191, loss_ce: 0.013032
2022-01-10 03:15:52,316 iteration 2478 : loss : 0.035434, loss_ce: 0.011375
2022-01-10 03:15:53,766 iteration 2479 : loss : 0.032406, loss_ce: 0.010795
2022-01-10 03:15:55,162 iteration 2480 : loss : 0.023394, loss_ce: 0.008605
2022-01-10 03:15:56,537 iteration 2481 : loss : 0.028536, loss_ce: 0.011775
2022-01-10 03:15:58,041 iteration 2482 : loss : 0.063550, loss_ce: 0.025219
 36%|█████████▊                 | 146/400 [1:04:04<1:52:28, 26.57s/it]2022-01-10 03:15:59,490 iteration 2483 : loss : 0.042781, loss_ce: 0.019723
2022-01-10 03:16:00,925 iteration 2484 : loss : 0.081338, loss_ce: 0.015201
2022-01-10 03:16:02,353 iteration 2485 : loss : 0.042381, loss_ce: 0.016758
2022-01-10 03:16:03,777 iteration 2486 : loss : 0.043651, loss_ce: 0.020448
2022-01-10 03:16:05,144 iteration 2487 : loss : 0.028163, loss_ce: 0.011272
2022-01-10 03:16:06,609 iteration 2488 : loss : 0.037035, loss_ce: 0.015407
2022-01-10 03:16:08,055 iteration 2489 : loss : 0.045885, loss_ce: 0.020379
2022-01-10 03:16:09,529 iteration 2490 : loss : 0.040574, loss_ce: 0.019957
2022-01-10 03:16:10,962 iteration 2491 : loss : 0.043357, loss_ce: 0.014745
2022-01-10 03:16:12,399 iteration 2492 : loss : 0.042263, loss_ce: 0.015798
2022-01-10 03:16:13,765 iteration 2493 : loss : 0.030736, loss_ce: 0.011973
2022-01-10 03:16:15,174 iteration 2494 : loss : 0.037372, loss_ce: 0.012653
2022-01-10 03:16:16,662 iteration 2495 : loss : 0.028744, loss_ce: 0.012249
2022-01-10 03:16:17,969 iteration 2496 : loss : 0.023147, loss_ce: 0.008452
2022-01-10 03:16:19,333 iteration 2497 : loss : 0.031270, loss_ce: 0.012154
2022-01-10 03:16:20,872 iteration 2498 : loss : 0.043773, loss_ce: 0.017856
2022-01-10 03:16:22,288 iteration 2499 : loss : 0.031073, loss_ce: 0.014992
 37%|█████████▉                 | 147/400 [1:04:28<1:49:06, 25.87s/it]2022-01-10 03:16:23,772 iteration 2500 : loss : 0.040258, loss_ce: 0.013671
2022-01-10 03:16:25,207 iteration 2501 : loss : 0.028888, loss_ce: 0.011606
2022-01-10 03:16:26,639 iteration 2502 : loss : 0.034194, loss_ce: 0.016395
2022-01-10 03:16:28,034 iteration 2503 : loss : 0.028883, loss_ce: 0.014547
2022-01-10 03:16:29,476 iteration 2504 : loss : 0.049665, loss_ce: 0.018748
2022-01-10 03:16:30,902 iteration 2505 : loss : 0.040451, loss_ce: 0.014822
2022-01-10 03:16:32,331 iteration 2506 : loss : 0.036545, loss_ce: 0.017235
2022-01-10 03:16:33,696 iteration 2507 : loss : 0.019486, loss_ce: 0.007135
2022-01-10 03:16:35,026 iteration 2508 : loss : 0.027332, loss_ce: 0.011695
2022-01-10 03:16:36,411 iteration 2509 : loss : 0.024694, loss_ce: 0.010937
2022-01-10 03:16:37,766 iteration 2510 : loss : 0.043853, loss_ce: 0.013991
2022-01-10 03:16:39,162 iteration 2511 : loss : 0.026569, loss_ce: 0.012264
2022-01-10 03:16:40,552 iteration 2512 : loss : 0.026566, loss_ce: 0.009522
2022-01-10 03:16:41,951 iteration 2513 : loss : 0.035687, loss_ce: 0.014256
2022-01-10 03:16:43,367 iteration 2514 : loss : 0.032636, loss_ce: 0.012536
2022-01-10 03:16:44,753 iteration 2515 : loss : 0.030203, loss_ce: 0.008969
2022-01-10 03:16:46,117 iteration 2516 : loss : 0.040251, loss_ce: 0.017319
 37%|█████████▉                 | 148/400 [1:04:52<1:46:05, 25.26s/it]2022-01-10 03:16:47,564 iteration 2517 : loss : 0.033369, loss_ce: 0.013058
2022-01-10 03:16:49,064 iteration 2518 : loss : 0.041106, loss_ce: 0.018946
2022-01-10 03:16:50,469 iteration 2519 : loss : 0.029957, loss_ce: 0.012542
2022-01-10 03:16:51,852 iteration 2520 : loss : 0.022845, loss_ce: 0.007376
2022-01-10 03:16:53,281 iteration 2521 : loss : 0.025103, loss_ce: 0.011943
2022-01-10 03:16:54,628 iteration 2522 : loss : 0.023293, loss_ce: 0.008874
2022-01-10 03:16:55,924 iteration 2523 : loss : 0.029759, loss_ce: 0.007354
2022-01-10 03:16:57,368 iteration 2524 : loss : 0.029339, loss_ce: 0.014000
2022-01-10 03:16:58,737 iteration 2525 : loss : 0.022340, loss_ce: 0.008194
2022-01-10 03:17:00,106 iteration 2526 : loss : 0.028818, loss_ce: 0.011730
2022-01-10 03:17:01,553 iteration 2527 : loss : 0.029779, loss_ce: 0.012890
2022-01-10 03:17:02,929 iteration 2528 : loss : 0.039875, loss_ce: 0.015336
2022-01-10 03:17:04,357 iteration 2529 : loss : 0.033159, loss_ce: 0.013418
2022-01-10 03:17:05,766 iteration 2530 : loss : 0.026338, loss_ce: 0.007882
2022-01-10 03:17:07,158 iteration 2531 : loss : 0.029506, loss_ce: 0.010501
2022-01-10 03:17:08,590 iteration 2532 : loss : 0.032466, loss_ce: 0.011849
2022-01-10 03:17:10,007 iteration 2533 : loss : 0.028794, loss_ce: 0.016165
 37%|██████████                 | 149/400 [1:05:16<1:43:57, 24.85s/it]2022-01-10 03:17:11,420 iteration 2534 : loss : 0.021124, loss_ce: 0.008914
2022-01-10 03:17:12,854 iteration 2535 : loss : 0.039201, loss_ce: 0.011505
2022-01-10 03:17:14,306 iteration 2536 : loss : 0.032269, loss_ce: 0.014148
2022-01-10 03:17:15,765 iteration 2537 : loss : 0.042336, loss_ce: 0.014253
2022-01-10 03:17:17,235 iteration 2538 : loss : 0.036131, loss_ce: 0.013473
2022-01-10 03:17:18,588 iteration 2539 : loss : 0.037825, loss_ce: 0.016424
2022-01-10 03:17:19,986 iteration 2540 : loss : 0.043148, loss_ce: 0.016340
2022-01-10 03:17:21,510 iteration 2541 : loss : 0.049589, loss_ce: 0.014145
2022-01-10 03:17:22,864 iteration 2542 : loss : 0.025268, loss_ce: 0.009255
2022-01-10 03:17:24,269 iteration 2543 : loss : 0.056183, loss_ce: 0.020521
2022-01-10 03:17:25,742 iteration 2544 : loss : 0.051842, loss_ce: 0.022936
2022-01-10 03:17:27,196 iteration 2545 : loss : 0.059483, loss_ce: 0.035951
2022-01-10 03:17:28,559 iteration 2546 : loss : 0.025986, loss_ce: 0.010028
2022-01-10 03:17:29,963 iteration 2547 : loss : 0.054039, loss_ce: 0.012676
2022-01-10 03:17:31,360 iteration 2548 : loss : 0.034918, loss_ce: 0.014989
2022-01-10 03:17:32,820 iteration 2549 : loss : 0.043141, loss_ce: 0.012393
2022-01-10 03:17:32,820 Training Data Eval:
2022-01-10 03:17:39,880   Average segmentation loss on training set: 0.0223
2022-01-10 03:17:39,881 Validation Data Eval:
2022-01-10 03:17:42,315   Average segmentation loss on validation set: 0.0781
2022-01-10 03:17:43,705 iteration 2550 : loss : 0.054538, loss_ce: 0.035868
 38%|██████████▏                | 150/400 [1:05:50<1:54:35, 27.50s/it]2022-01-10 03:17:45,137 iteration 2551 : loss : 0.027109, loss_ce: 0.009919
2022-01-10 03:17:46,539 iteration 2552 : loss : 0.026794, loss_ce: 0.010537
2022-01-10 03:17:48,013 iteration 2553 : loss : 0.044722, loss_ce: 0.015716
2022-01-10 03:17:49,462 iteration 2554 : loss : 0.053659, loss_ce: 0.013013
2022-01-10 03:17:50,855 iteration 2555 : loss : 0.048435, loss_ce: 0.016285
2022-01-10 03:17:52,195 iteration 2556 : loss : 0.029254, loss_ce: 0.012454
2022-01-10 03:17:53,646 iteration 2557 : loss : 0.037360, loss_ce: 0.016235
2022-01-10 03:17:55,091 iteration 2558 : loss : 0.029177, loss_ce: 0.013361
2022-01-10 03:17:56,558 iteration 2559 : loss : 0.084192, loss_ce: 0.022732
2022-01-10 03:17:58,017 iteration 2560 : loss : 0.032184, loss_ce: 0.012894
2022-01-10 03:17:59,419 iteration 2561 : loss : 0.045744, loss_ce: 0.020620
2022-01-10 03:18:00,785 iteration 2562 : loss : 0.029797, loss_ce: 0.015199
2022-01-10 03:18:02,177 iteration 2563 : loss : 0.034551, loss_ce: 0.011972
2022-01-10 03:18:03,581 iteration 2564 : loss : 0.036831, loss_ce: 0.017430
2022-01-10 03:18:04,984 iteration 2565 : loss : 0.041858, loss_ce: 0.011323
2022-01-10 03:18:06,365 iteration 2566 : loss : 0.046051, loss_ce: 0.020244
2022-01-10 03:18:07,863 iteration 2567 : loss : 0.041580, loss_ce: 0.017389
 38%|██████████▏                | 151/400 [1:06:14<1:49:59, 26.50s/it]2022-01-10 03:18:09,324 iteration 2568 : loss : 0.025446, loss_ce: 0.011847
2022-01-10 03:18:10,732 iteration 2569 : loss : 0.041632, loss_ce: 0.015883
2022-01-10 03:18:12,112 iteration 2570 : loss : 0.055745, loss_ce: 0.017280
2022-01-10 03:18:13,528 iteration 2571 : loss : 0.049854, loss_ce: 0.019301
2022-01-10 03:18:14,871 iteration 2572 : loss : 0.033568, loss_ce: 0.010986
2022-01-10 03:18:16,238 iteration 2573 : loss : 0.032140, loss_ce: 0.013636
2022-01-10 03:18:17,627 iteration 2574 : loss : 0.035798, loss_ce: 0.013508
2022-01-10 03:18:19,066 iteration 2575 : loss : 0.035683, loss_ce: 0.012408
2022-01-10 03:18:20,572 iteration 2576 : loss : 0.032252, loss_ce: 0.013003
2022-01-10 03:18:21,959 iteration 2577 : loss : 0.031623, loss_ce: 0.010914
2022-01-10 03:18:23,395 iteration 2578 : loss : 0.032616, loss_ce: 0.016775
2022-01-10 03:18:24,781 iteration 2579 : loss : 0.036968, loss_ce: 0.010969
2022-01-10 03:18:26,224 iteration 2580 : loss : 0.034588, loss_ce: 0.010166
2022-01-10 03:18:27,738 iteration 2581 : loss : 0.041789, loss_ce: 0.014107
2022-01-10 03:18:29,050 iteration 2582 : loss : 0.028337, loss_ce: 0.012055
2022-01-10 03:18:30,488 iteration 2583 : loss : 0.023123, loss_ce: 0.007715
2022-01-10 03:18:31,872 iteration 2584 : loss : 0.027769, loss_ce: 0.010730
 38%|██████████▎                | 152/400 [1:06:38<1:46:26, 25.75s/it]2022-01-10 03:18:33,383 iteration 2585 : loss : 0.036777, loss_ce: 0.015095
2022-01-10 03:18:34,796 iteration 2586 : loss : 0.058363, loss_ce: 0.028843
2022-01-10 03:18:36,150 iteration 2587 : loss : 0.022427, loss_ce: 0.007846
2022-01-10 03:18:37,531 iteration 2588 : loss : 0.035275, loss_ce: 0.015498
2022-01-10 03:18:38,867 iteration 2589 : loss : 0.028806, loss_ce: 0.011795
2022-01-10 03:18:40,307 iteration 2590 : loss : 0.037421, loss_ce: 0.014707
2022-01-10 03:18:41,709 iteration 2591 : loss : 0.030717, loss_ce: 0.012943
2022-01-10 03:18:43,095 iteration 2592 : loss : 0.028914, loss_ce: 0.011669
2022-01-10 03:18:44,470 iteration 2593 : loss : 0.049588, loss_ce: 0.022684
2022-01-10 03:18:45,906 iteration 2594 : loss : 0.052661, loss_ce: 0.019615
2022-01-10 03:18:47,408 iteration 2595 : loss : 0.043114, loss_ce: 0.013518
2022-01-10 03:18:48,753 iteration 2596 : loss : 0.031070, loss_ce: 0.011051
2022-01-10 03:18:50,168 iteration 2597 : loss : 0.053055, loss_ce: 0.016212
2022-01-10 03:18:51,545 iteration 2598 : loss : 0.028295, loss_ce: 0.013836
2022-01-10 03:18:52,990 iteration 2599 : loss : 0.024271, loss_ce: 0.010376
2022-01-10 03:18:54,393 iteration 2600 : loss : 0.045501, loss_ce: 0.013402
2022-01-10 03:18:55,814 iteration 2601 : loss : 0.030971, loss_ce: 0.013543
 38%|██████████▎                | 153/400 [1:07:02<1:43:47, 25.21s/it]2022-01-10 03:18:57,302 iteration 2602 : loss : 0.029250, loss_ce: 0.009112
2022-01-10 03:18:58,797 iteration 2603 : loss : 0.039638, loss_ce: 0.012667
2022-01-10 03:19:00,211 iteration 2604 : loss : 0.030230, loss_ce: 0.014583
2022-01-10 03:19:01,504 iteration 2605 : loss : 0.022825, loss_ce: 0.006626
2022-01-10 03:19:03,041 iteration 2606 : loss : 0.039387, loss_ce: 0.013220
2022-01-10 03:19:04,488 iteration 2607 : loss : 0.032495, loss_ce: 0.011215
2022-01-10 03:19:05,900 iteration 2608 : loss : 0.027128, loss_ce: 0.012880
2022-01-10 03:19:07,398 iteration 2609 : loss : 0.037126, loss_ce: 0.015053
2022-01-10 03:19:08,860 iteration 2610 : loss : 0.035379, loss_ce: 0.014612
2022-01-10 03:19:10,300 iteration 2611 : loss : 0.040751, loss_ce: 0.015796
2022-01-10 03:19:11,759 iteration 2612 : loss : 0.036644, loss_ce: 0.016327
2022-01-10 03:19:13,121 iteration 2613 : loss : 0.027296, loss_ce: 0.011786
2022-01-10 03:19:14,593 iteration 2614 : loss : 0.038302, loss_ce: 0.012327
2022-01-10 03:19:15,976 iteration 2615 : loss : 0.059831, loss_ce: 0.025296
2022-01-10 03:19:17,418 iteration 2616 : loss : 0.033388, loss_ce: 0.018263
2022-01-10 03:19:18,821 iteration 2617 : loss : 0.035563, loss_ce: 0.012403
2022-01-10 03:19:20,185 iteration 2618 : loss : 0.021611, loss_ce: 0.009676
 38%|██████████▍                | 154/400 [1:07:26<1:42:19, 24.96s/it]2022-01-10 03:19:21,623 iteration 2619 : loss : 0.034631, loss_ce: 0.011447
2022-01-10 03:19:23,004 iteration 2620 : loss : 0.025598, loss_ce: 0.011121
2022-01-10 03:19:24,468 iteration 2621 : loss : 0.032413, loss_ce: 0.011848
2022-01-10 03:19:25,865 iteration 2622 : loss : 0.044459, loss_ce: 0.011077
2022-01-10 03:19:27,296 iteration 2623 : loss : 0.046071, loss_ce: 0.021159
2022-01-10 03:19:28,665 iteration 2624 : loss : 0.021032, loss_ce: 0.006987
2022-01-10 03:19:30,098 iteration 2625 : loss : 0.055083, loss_ce: 0.011210
2022-01-10 03:19:31,528 iteration 2626 : loss : 0.051579, loss_ce: 0.024685
2022-01-10 03:19:33,078 iteration 2627 : loss : 0.035662, loss_ce: 0.013511
2022-01-10 03:19:34,430 iteration 2628 : loss : 0.026874, loss_ce: 0.011679
2022-01-10 03:19:35,898 iteration 2629 : loss : 0.032161, loss_ce: 0.013093
2022-01-10 03:19:37,331 iteration 2630 : loss : 0.039393, loss_ce: 0.017944
2022-01-10 03:19:38,743 iteration 2631 : loss : 0.034467, loss_ce: 0.015796
2022-01-10 03:19:40,147 iteration 2632 : loss : 0.023890, loss_ce: 0.009227
2022-01-10 03:19:41,589 iteration 2633 : loss : 0.042830, loss_ce: 0.022613
2022-01-10 03:19:43,021 iteration 2634 : loss : 0.032881, loss_ce: 0.010225
2022-01-10 03:19:43,021 Training Data Eval:
2022-01-10 03:19:50,112   Average segmentation loss on training set: 0.0245
2022-01-10 03:19:50,112 Validation Data Eval:
2022-01-10 03:19:52,592   Average segmentation loss on validation set: 0.0670
2022-01-10 03:19:58,311 Found new lowest validation loss at iteration 2634! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed2.pth
2022-01-10 03:19:59,746 iteration 2635 : loss : 0.023304, loss_ce: 0.008627
 39%|██████████▍                | 155/400 [1:08:06<1:59:48, 29.34s/it]2022-01-10 03:20:01,210 iteration 2636 : loss : 0.038611, loss_ce: 0.018446
2022-01-10 03:20:02,688 iteration 2637 : loss : 0.031745, loss_ce: 0.012397
2022-01-10 03:20:04,071 iteration 2638 : loss : 0.031114, loss_ce: 0.009930
2022-01-10 03:20:05,429 iteration 2639 : loss : 0.028560, loss_ce: 0.012545
2022-01-10 03:20:06,812 iteration 2640 : loss : 0.033762, loss_ce: 0.011968
2022-01-10 03:20:08,208 iteration 2641 : loss : 0.035483, loss_ce: 0.016321
2022-01-10 03:20:09,573 iteration 2642 : loss : 0.024517, loss_ce: 0.008787
2022-01-10 03:20:11,083 iteration 2643 : loss : 0.028166, loss_ce: 0.010456
2022-01-10 03:20:12,472 iteration 2644 : loss : 0.024860, loss_ce: 0.008931
2022-01-10 03:20:13,833 iteration 2645 : loss : 0.022449, loss_ce: 0.007725
2022-01-10 03:20:15,402 iteration 2646 : loss : 0.032230, loss_ce: 0.011426
2022-01-10 03:20:16,869 iteration 2647 : loss : 0.034115, loss_ce: 0.015222
2022-01-10 03:20:18,269 iteration 2648 : loss : 0.043548, loss_ce: 0.018762
2022-01-10 03:20:19,691 iteration 2649 : loss : 0.046294, loss_ce: 0.018037
2022-01-10 03:20:21,149 iteration 2650 : loss : 0.042555, loss_ce: 0.012074
2022-01-10 03:20:22,578 iteration 2651 : loss : 0.037695, loss_ce: 0.015967
2022-01-10 03:20:23,932 iteration 2652 : loss : 0.023824, loss_ce: 0.008873
 39%|██████████▌                | 156/400 [1:08:30<1:53:01, 27.79s/it]2022-01-10 03:20:25,447 iteration 2653 : loss : 0.025479, loss_ce: 0.011755
2022-01-10 03:20:26,868 iteration 2654 : loss : 0.031998, loss_ce: 0.010786
2022-01-10 03:20:28,366 iteration 2655 : loss : 0.035548, loss_ce: 0.018075
2022-01-10 03:20:29,762 iteration 2656 : loss : 0.050165, loss_ce: 0.013910
2022-01-10 03:20:31,161 iteration 2657 : loss : 0.039124, loss_ce: 0.014401
2022-01-10 03:20:32,457 iteration 2658 : loss : 0.023954, loss_ce: 0.011591
2022-01-10 03:20:33,806 iteration 2659 : loss : 0.046351, loss_ce: 0.012184
2022-01-10 03:20:35,249 iteration 2660 : loss : 0.036384, loss_ce: 0.011375
2022-01-10 03:20:36,624 iteration 2661 : loss : 0.021718, loss_ce: 0.009858
2022-01-10 03:20:38,019 iteration 2662 : loss : 0.034883, loss_ce: 0.017035
2022-01-10 03:20:39,398 iteration 2663 : loss : 0.030874, loss_ce: 0.008590
2022-01-10 03:20:40,894 iteration 2664 : loss : 0.044160, loss_ce: 0.019205
2022-01-10 03:20:42,310 iteration 2665 : loss : 0.052834, loss_ce: 0.019462
2022-01-10 03:20:43,681 iteration 2666 : loss : 0.022714, loss_ce: 0.008151
2022-01-10 03:20:45,144 iteration 2667 : loss : 0.039988, loss_ce: 0.015225
2022-01-10 03:20:46,521 iteration 2668 : loss : 0.022952, loss_ce: 0.007639
2022-01-10 03:20:47,899 iteration 2669 : loss : 0.032648, loss_ce: 0.010628
 39%|██████████▌                | 157/400 [1:08:54<1:47:54, 26.64s/it]2022-01-10 03:20:49,396 iteration 2670 : loss : 0.039011, loss_ce: 0.016431
2022-01-10 03:20:50,914 iteration 2671 : loss : 0.041659, loss_ce: 0.014127
2022-01-10 03:20:52,356 iteration 2672 : loss : 0.034104, loss_ce: 0.013105
2022-01-10 03:20:53,714 iteration 2673 : loss : 0.023391, loss_ce: 0.011398
2022-01-10 03:20:55,152 iteration 2674 : loss : 0.039709, loss_ce: 0.018973
2022-01-10 03:20:56,453 iteration 2675 : loss : 0.022747, loss_ce: 0.009805
2022-01-10 03:20:57,967 iteration 2676 : loss : 0.075196, loss_ce: 0.018045
2022-01-10 03:20:59,410 iteration 2677 : loss : 0.030586, loss_ce: 0.012235
2022-01-10 03:21:00,816 iteration 2678 : loss : 0.027259, loss_ce: 0.010952
2022-01-10 03:21:02,235 iteration 2679 : loss : 0.040599, loss_ce: 0.013982
2022-01-10 03:21:03,634 iteration 2680 : loss : 0.020534, loss_ce: 0.007944
2022-01-10 03:21:05,055 iteration 2681 : loss : 0.026655, loss_ce: 0.008527
2022-01-10 03:21:06,477 iteration 2682 : loss : 0.037293, loss_ce: 0.014464
2022-01-10 03:21:07,881 iteration 2683 : loss : 0.048189, loss_ce: 0.011579
2022-01-10 03:21:09,241 iteration 2684 : loss : 0.030314, loss_ce: 0.014867
2022-01-10 03:21:10,660 iteration 2685 : loss : 0.056936, loss_ce: 0.024026
2022-01-10 03:21:12,019 iteration 2686 : loss : 0.026998, loss_ce: 0.011569
 40%|██████████▋                | 158/400 [1:09:18<1:44:24, 25.89s/it]2022-01-10 03:21:13,395 iteration 2687 : loss : 0.032040, loss_ce: 0.008744
2022-01-10 03:21:14,802 iteration 2688 : loss : 0.028820, loss_ce: 0.010399
2022-01-10 03:21:16,184 iteration 2689 : loss : 0.038424, loss_ce: 0.014321
2022-01-10 03:21:17,688 iteration 2690 : loss : 0.045198, loss_ce: 0.021111
2022-01-10 03:21:19,036 iteration 2691 : loss : 0.022655, loss_ce: 0.009251
2022-01-10 03:21:20,399 iteration 2692 : loss : 0.031961, loss_ce: 0.009394
2022-01-10 03:21:21,811 iteration 2693 : loss : 0.033943, loss_ce: 0.012434
2022-01-10 03:21:23,292 iteration 2694 : loss : 0.041164, loss_ce: 0.014034
2022-01-10 03:21:24,678 iteration 2695 : loss : 0.026908, loss_ce: 0.010779
2022-01-10 03:21:26,103 iteration 2696 : loss : 0.024676, loss_ce: 0.010995
2022-01-10 03:21:27,457 iteration 2697 : loss : 0.024254, loss_ce: 0.010583
2022-01-10 03:21:28,739 iteration 2698 : loss : 0.021371, loss_ce: 0.009924
2022-01-10 03:21:30,147 iteration 2699 : loss : 0.034973, loss_ce: 0.013658
2022-01-10 03:21:31,608 iteration 2700 : loss : 0.039391, loss_ce: 0.020471
2022-01-10 03:21:33,017 iteration 2701 : loss : 0.030874, loss_ce: 0.012398
2022-01-10 03:21:34,460 iteration 2702 : loss : 0.059783, loss_ce: 0.009621
2022-01-10 03:21:35,858 iteration 2703 : loss : 0.032895, loss_ce: 0.014286
 40%|██████████▋                | 159/400 [1:09:42<1:41:31, 25.27s/it]2022-01-10 03:21:37,425 iteration 2704 : loss : 0.033600, loss_ce: 0.012850
2022-01-10 03:21:38,790 iteration 2705 : loss : 0.039320, loss_ce: 0.014209
2022-01-10 03:21:40,154 iteration 2706 : loss : 0.042731, loss_ce: 0.023981
2022-01-10 03:21:41,632 iteration 2707 : loss : 0.045434, loss_ce: 0.013468
2022-01-10 03:21:42,968 iteration 2708 : loss : 0.023143, loss_ce: 0.008594
2022-01-10 03:21:44,371 iteration 2709 : loss : 0.035122, loss_ce: 0.011379
2022-01-10 03:21:45,728 iteration 2710 : loss : 0.023170, loss_ce: 0.007737
2022-01-10 03:21:47,229 iteration 2711 : loss : 0.041411, loss_ce: 0.015205
2022-01-10 03:21:48,584 iteration 2712 : loss : 0.023971, loss_ce: 0.009498
2022-01-10 03:21:50,088 iteration 2713 : loss : 0.043948, loss_ce: 0.012466
2022-01-10 03:21:51,438 iteration 2714 : loss : 0.034768, loss_ce: 0.010796
2022-01-10 03:21:52,858 iteration 2715 : loss : 0.033299, loss_ce: 0.017970
2022-01-10 03:21:54,236 iteration 2716 : loss : 0.042936, loss_ce: 0.016468
2022-01-10 03:21:55,585 iteration 2717 : loss : 0.033308, loss_ce: 0.016648
2022-01-10 03:21:56,941 iteration 2718 : loss : 0.031612, loss_ce: 0.011348
2022-01-10 03:21:58,400 iteration 2719 : loss : 0.045943, loss_ce: 0.013182
2022-01-10 03:21:58,400 Training Data Eval:
2022-01-10 03:22:05,456   Average segmentation loss on training set: 0.0219
2022-01-10 03:22:05,456 Validation Data Eval:
2022-01-10 03:22:07,893   Average segmentation loss on validation set: 0.1121
2022-01-10 03:22:09,240 iteration 2720 : loss : 0.027677, loss_ce: 0.012647
 40%|██████████▊                | 160/400 [1:10:15<1:50:49, 27.71s/it]2022-01-10 03:22:10,719 iteration 2721 : loss : 0.028768, loss_ce: 0.011383
2022-01-10 03:22:12,153 iteration 2722 : loss : 0.026963, loss_ce: 0.012174
2022-01-10 03:22:13,594 iteration 2723 : loss : 0.024661, loss_ce: 0.008922
2022-01-10 03:22:14,943 iteration 2724 : loss : 0.028108, loss_ce: 0.008612
2022-01-10 03:22:16,263 iteration 2725 : loss : 0.026950, loss_ce: 0.010097
2022-01-10 03:22:17,635 iteration 2726 : loss : 0.034688, loss_ce: 0.011473
2022-01-10 03:22:18,967 iteration 2727 : loss : 0.022574, loss_ce: 0.006965
2022-01-10 03:22:20,480 iteration 2728 : loss : 0.035573, loss_ce: 0.014134
2022-01-10 03:22:21,891 iteration 2729 : loss : 0.036251, loss_ce: 0.012832
2022-01-10 03:22:23,337 iteration 2730 : loss : 0.022243, loss_ce: 0.008786
2022-01-10 03:22:24,775 iteration 2731 : loss : 0.024522, loss_ce: 0.010615
2022-01-10 03:22:26,276 iteration 2732 : loss : 0.034406, loss_ce: 0.015332
2022-01-10 03:22:27,714 iteration 2733 : loss : 0.029916, loss_ce: 0.011804
2022-01-10 03:22:29,144 iteration 2734 : loss : 0.023243, loss_ce: 0.006863
2022-01-10 03:22:30,470 iteration 2735 : loss : 0.030916, loss_ce: 0.012816
2022-01-10 03:22:31,931 iteration 2736 : loss : 0.030711, loss_ce: 0.015160
2022-01-10 03:22:33,324 iteration 2737 : loss : 0.029210, loss_ce: 0.010168
 40%|██████████▊                | 161/400 [1:10:39<1:46:01, 26.62s/it]2022-01-10 03:22:34,779 iteration 2738 : loss : 0.029353, loss_ce: 0.014184
2022-01-10 03:22:36,161 iteration 2739 : loss : 0.023431, loss_ce: 0.010983
2022-01-10 03:22:37,587 iteration 2740 : loss : 0.024129, loss_ce: 0.009364
2022-01-10 03:22:39,016 iteration 2741 : loss : 0.025660, loss_ce: 0.007728
2022-01-10 03:22:40,354 iteration 2742 : loss : 0.019489, loss_ce: 0.007493
2022-01-10 03:22:41,761 iteration 2743 : loss : 0.035060, loss_ce: 0.016960
2022-01-10 03:22:43,195 iteration 2744 : loss : 0.028172, loss_ce: 0.011403
2022-01-10 03:22:44,610 iteration 2745 : loss : 0.024551, loss_ce: 0.009220
2022-01-10 03:22:45,964 iteration 2746 : loss : 0.044957, loss_ce: 0.010699
2022-01-10 03:22:47,377 iteration 2747 : loss : 0.025761, loss_ce: 0.007995
2022-01-10 03:22:48,818 iteration 2748 : loss : 0.033662, loss_ce: 0.015808
2022-01-10 03:22:50,160 iteration 2749 : loss : 0.024540, loss_ce: 0.010279
2022-01-10 03:22:51,594 iteration 2750 : loss : 0.038769, loss_ce: 0.011562
2022-01-10 03:22:52,968 iteration 2751 : loss : 0.023339, loss_ce: 0.011011
2022-01-10 03:22:54,294 iteration 2752 : loss : 0.032729, loss_ce: 0.007553
2022-01-10 03:22:55,777 iteration 2753 : loss : 0.046323, loss_ce: 0.022326
2022-01-10 03:22:57,225 iteration 2754 : loss : 0.026620, loss_ce: 0.012101
 40%|██████████▉                | 162/400 [1:11:03<1:42:21, 25.80s/it]2022-01-10 03:22:58,688 iteration 2755 : loss : 0.020695, loss_ce: 0.008354
2022-01-10 03:23:00,106 iteration 2756 : loss : 0.047668, loss_ce: 0.029419
2022-01-10 03:23:01,511 iteration 2757 : loss : 0.028083, loss_ce: 0.011331
2022-01-10 03:23:02,985 iteration 2758 : loss : 0.036001, loss_ce: 0.011963
2022-01-10 03:23:04,400 iteration 2759 : loss : 0.030686, loss_ce: 0.010345
2022-01-10 03:23:05,812 iteration 2760 : loss : 0.028651, loss_ce: 0.011049
2022-01-10 03:23:07,144 iteration 2761 : loss : 0.036446, loss_ce: 0.014204
2022-01-10 03:23:08,489 iteration 2762 : loss : 0.023181, loss_ce: 0.009331
2022-01-10 03:23:09,936 iteration 2763 : loss : 0.047504, loss_ce: 0.015691
2022-01-10 03:23:11,335 iteration 2764 : loss : 0.028999, loss_ce: 0.010678
2022-01-10 03:23:12,751 iteration 2765 : loss : 0.031792, loss_ce: 0.012250
2022-01-10 03:23:14,130 iteration 2766 : loss : 0.025200, loss_ce: 0.010755
2022-01-10 03:23:15,554 iteration 2767 : loss : 0.023911, loss_ce: 0.009085
2022-01-10 03:23:16,921 iteration 2768 : loss : 0.020725, loss_ce: 0.005653
2022-01-10 03:23:18,323 iteration 2769 : loss : 0.021100, loss_ce: 0.008936
2022-01-10 03:23:19,726 iteration 2770 : loss : 0.025351, loss_ce: 0.012820
2022-01-10 03:23:21,112 iteration 2771 : loss : 0.021794, loss_ce: 0.006879
 41%|███████████                | 163/400 [1:11:27<1:39:39, 25.23s/it]2022-01-10 03:23:22,618 iteration 2772 : loss : 0.030345, loss_ce: 0.013045
2022-01-10 03:23:23,969 iteration 2773 : loss : 0.021602, loss_ce: 0.008535
2022-01-10 03:23:25,307 iteration 2774 : loss : 0.030378, loss_ce: 0.007537
2022-01-10 03:23:26,682 iteration 2775 : loss : 0.029531, loss_ce: 0.009993
2022-01-10 03:23:28,046 iteration 2776 : loss : 0.024342, loss_ce: 0.009053
2022-01-10 03:23:29,443 iteration 2777 : loss : 0.018999, loss_ce: 0.006266
2022-01-10 03:23:30,918 iteration 2778 : loss : 0.033274, loss_ce: 0.012497
2022-01-10 03:23:32,426 iteration 2779 : loss : 0.027740, loss_ce: 0.010812
2022-01-10 03:23:33,789 iteration 2780 : loss : 0.026798, loss_ce: 0.008938
2022-01-10 03:23:35,237 iteration 2781 : loss : 0.036471, loss_ce: 0.013073
2022-01-10 03:23:36,629 iteration 2782 : loss : 0.024729, loss_ce: 0.008323
2022-01-10 03:23:37,995 iteration 2783 : loss : 0.027646, loss_ce: 0.011991
2022-01-10 03:23:39,500 iteration 2784 : loss : 0.028544, loss_ce: 0.010013
2022-01-10 03:23:40,968 iteration 2785 : loss : 0.038822, loss_ce: 0.010514
2022-01-10 03:23:42,407 iteration 2786 : loss : 0.031992, loss_ce: 0.014580
2022-01-10 03:23:43,856 iteration 2787 : loss : 0.020340, loss_ce: 0.008653
2022-01-10 03:23:45,228 iteration 2788 : loss : 0.022772, loss_ce: 0.011851
 41%|███████████                | 164/400 [1:11:51<1:37:55, 24.89s/it]2022-01-10 03:23:46,662 iteration 2789 : loss : 0.030838, loss_ce: 0.011361
2022-01-10 03:23:48,094 iteration 2790 : loss : 0.026106, loss_ce: 0.008175
2022-01-10 03:23:49,469 iteration 2791 : loss : 0.044482, loss_ce: 0.018918
2022-01-10 03:23:50,934 iteration 2792 : loss : 0.032100, loss_ce: 0.012500
2022-01-10 03:23:52,300 iteration 2793 : loss : 0.022145, loss_ce: 0.009235
2022-01-10 03:23:53,738 iteration 2794 : loss : 0.026793, loss_ce: 0.009963
2022-01-10 03:23:55,101 iteration 2795 : loss : 0.021748, loss_ce: 0.006780
2022-01-10 03:23:56,535 iteration 2796 : loss : 0.025654, loss_ce: 0.010243
2022-01-10 03:23:57,969 iteration 2797 : loss : 0.035504, loss_ce: 0.014058
2022-01-10 03:23:59,433 iteration 2798 : loss : 0.059557, loss_ce: 0.032558
2022-01-10 03:24:00,803 iteration 2799 : loss : 0.020821, loss_ce: 0.009086
2022-01-10 03:24:02,152 iteration 2800 : loss : 0.040565, loss_ce: 0.014299
2022-01-10 03:24:03,496 iteration 2801 : loss : 0.017129, loss_ce: 0.005448
2022-01-10 03:24:04,950 iteration 2802 : loss : 0.023018, loss_ce: 0.006862
2022-01-10 03:24:06,346 iteration 2803 : loss : 0.022838, loss_ce: 0.009490
2022-01-10 03:24:07,723 iteration 2804 : loss : 0.021611, loss_ce: 0.006999
2022-01-10 03:24:07,723 Training Data Eval:
2022-01-10 03:24:14,778   Average segmentation loss on training set: 0.0194
2022-01-10 03:24:14,779 Validation Data Eval:
2022-01-10 03:24:17,226   Average segmentation loss on validation set: 0.0982
2022-01-10 03:24:18,596 iteration 2805 : loss : 0.026123, loss_ce: 0.011833
 41%|███████████▏               | 165/400 [1:12:25<1:47:27, 27.44s/it]2022-01-10 03:24:20,169 iteration 2806 : loss : 0.034113, loss_ce: 0.013433
2022-01-10 03:24:21,507 iteration 2807 : loss : 0.029615, loss_ce: 0.012435
2022-01-10 03:24:22,863 iteration 2808 : loss : 0.020821, loss_ce: 0.008183
2022-01-10 03:24:24,289 iteration 2809 : loss : 0.020400, loss_ce: 0.006781
2022-01-10 03:24:25,675 iteration 2810 : loss : 0.028771, loss_ce: 0.013479
2022-01-10 03:24:27,160 iteration 2811 : loss : 0.046489, loss_ce: 0.017488
2022-01-10 03:24:28,664 iteration 2812 : loss : 0.028761, loss_ce: 0.010338
2022-01-10 03:24:29,996 iteration 2813 : loss : 0.024995, loss_ce: 0.009592
2022-01-10 03:24:31,418 iteration 2814 : loss : 0.026772, loss_ce: 0.012022
2022-01-10 03:24:32,861 iteration 2815 : loss : 0.033168, loss_ce: 0.013767
2022-01-10 03:24:34,249 iteration 2816 : loss : 0.032448, loss_ce: 0.012950
2022-01-10 03:24:35,641 iteration 2817 : loss : 0.028552, loss_ce: 0.011927
2022-01-10 03:24:37,139 iteration 2818 : loss : 0.032132, loss_ce: 0.010534
2022-01-10 03:24:38,603 iteration 2819 : loss : 0.027603, loss_ce: 0.012212
2022-01-10 03:24:39,991 iteration 2820 : loss : 0.026864, loss_ce: 0.009668
2022-01-10 03:24:41,407 iteration 2821 : loss : 0.018796, loss_ce: 0.007528
2022-01-10 03:24:42,858 iteration 2822 : loss : 0.042146, loss_ce: 0.011044
 42%|███████████▏               | 166/400 [1:12:49<1:43:16, 26.48s/it]2022-01-10 03:24:44,353 iteration 2823 : loss : 0.023676, loss_ce: 0.009032
2022-01-10 03:24:45,798 iteration 2824 : loss : 0.033533, loss_ce: 0.008805
2022-01-10 03:24:47,229 iteration 2825 : loss : 0.040864, loss_ce: 0.014812
2022-01-10 03:24:48,598 iteration 2826 : loss : 0.035250, loss_ce: 0.010996
2022-01-10 03:24:50,051 iteration 2827 : loss : 0.046777, loss_ce: 0.023413
2022-01-10 03:24:51,440 iteration 2828 : loss : 0.029113, loss_ce: 0.011229
2022-01-10 03:24:52,823 iteration 2829 : loss : 0.040214, loss_ce: 0.012976
2022-01-10 03:24:54,251 iteration 2830 : loss : 0.027955, loss_ce: 0.012186
2022-01-10 03:24:55,647 iteration 2831 : loss : 0.039101, loss_ce: 0.011739
2022-01-10 03:24:56,998 iteration 2832 : loss : 0.022815, loss_ce: 0.008897
2022-01-10 03:24:58,472 iteration 2833 : loss : 0.020251, loss_ce: 0.008057
2022-01-10 03:24:59,895 iteration 2834 : loss : 0.030086, loss_ce: 0.012852
2022-01-10 03:25:01,322 iteration 2835 : loss : 0.031759, loss_ce: 0.013060
2022-01-10 03:25:02,758 iteration 2836 : loss : 0.029174, loss_ce: 0.010330
2022-01-10 03:25:04,216 iteration 2837 : loss : 0.020869, loss_ce: 0.007089
2022-01-10 03:25:05,662 iteration 2838 : loss : 0.033513, loss_ce: 0.010434
2022-01-10 03:25:07,010 iteration 2839 : loss : 0.028205, loss_ce: 0.010886
 42%|███████████▎               | 167/400 [1:13:13<1:40:07, 25.78s/it]2022-01-10 03:25:08,461 iteration 2840 : loss : 0.031275, loss_ce: 0.010121
2022-01-10 03:25:09,830 iteration 2841 : loss : 0.021325, loss_ce: 0.009242
2022-01-10 03:25:11,161 iteration 2842 : loss : 0.019616, loss_ce: 0.007731
2022-01-10 03:25:12,553 iteration 2843 : loss : 0.025385, loss_ce: 0.008554
2022-01-10 03:25:13,997 iteration 2844 : loss : 0.029680, loss_ce: 0.012081
2022-01-10 03:25:15,467 iteration 2845 : loss : 0.025233, loss_ce: 0.008111
2022-01-10 03:25:16,862 iteration 2846 : loss : 0.027390, loss_ce: 0.009087
2022-01-10 03:25:18,284 iteration 2847 : loss : 0.029700, loss_ce: 0.013134
2022-01-10 03:25:19,793 iteration 2848 : loss : 0.059486, loss_ce: 0.028119
2022-01-10 03:25:21,184 iteration 2849 : loss : 0.032009, loss_ce: 0.010867
2022-01-10 03:25:22,552 iteration 2850 : loss : 0.024647, loss_ce: 0.008204
2022-01-10 03:25:23,962 iteration 2851 : loss : 0.031731, loss_ce: 0.015100
2022-01-10 03:25:25,376 iteration 2852 : loss : 0.038902, loss_ce: 0.015901
2022-01-10 03:25:26,751 iteration 2853 : loss : 0.026777, loss_ce: 0.011831
2022-01-10 03:25:28,193 iteration 2854 : loss : 0.041176, loss_ce: 0.019535
2022-01-10 03:25:29,601 iteration 2855 : loss : 0.039628, loss_ce: 0.018008
2022-01-10 03:25:31,062 iteration 2856 : loss : 0.027912, loss_ce: 0.011520
 42%|███████████▎               | 168/400 [1:13:37<1:37:41, 25.26s/it]2022-01-10 03:25:32,582 iteration 2857 : loss : 0.045374, loss_ce: 0.011288
2022-01-10 03:25:34,011 iteration 2858 : loss : 0.045993, loss_ce: 0.013889
2022-01-10 03:25:35,369 iteration 2859 : loss : 0.019657, loss_ce: 0.007050
2022-01-10 03:25:36,832 iteration 2860 : loss : 0.033058, loss_ce: 0.013671
2022-01-10 03:25:38,205 iteration 2861 : loss : 0.029256, loss_ce: 0.009025
2022-01-10 03:25:39,646 iteration 2862 : loss : 0.022377, loss_ce: 0.007864
2022-01-10 03:25:41,121 iteration 2863 : loss : 0.050616, loss_ce: 0.019272
2022-01-10 03:25:42,543 iteration 2864 : loss : 0.026311, loss_ce: 0.012191
2022-01-10 03:25:43,881 iteration 2865 : loss : 0.025361, loss_ce: 0.008881
2022-01-10 03:25:45,272 iteration 2866 : loss : 0.030484, loss_ce: 0.008688
2022-01-10 03:25:46,731 iteration 2867 : loss : 0.035153, loss_ce: 0.013835
2022-01-10 03:25:48,177 iteration 2868 : loss : 0.022275, loss_ce: 0.008714
2022-01-10 03:25:49,547 iteration 2869 : loss : 0.028316, loss_ce: 0.011678
2022-01-10 03:25:50,962 iteration 2870 : loss : 0.037360, loss_ce: 0.017220
2022-01-10 03:25:52,287 iteration 2871 : loss : 0.020693, loss_ce: 0.009456
2022-01-10 03:25:53,689 iteration 2872 : loss : 0.054431, loss_ce: 0.020190
2022-01-10 03:25:55,139 iteration 2873 : loss : 0.035832, loss_ce: 0.018551
 42%|███████████▍               | 169/400 [1:14:01<1:35:54, 24.91s/it]2022-01-10 03:25:56,576 iteration 2874 : loss : 0.028878, loss_ce: 0.008849
2022-01-10 03:25:57,902 iteration 2875 : loss : 0.023166, loss_ce: 0.009053
2022-01-10 03:25:59,276 iteration 2876 : loss : 0.023609, loss_ce: 0.009842
2022-01-10 03:26:00,689 iteration 2877 : loss : 0.027037, loss_ce: 0.009147
2022-01-10 03:26:02,118 iteration 2878 : loss : 0.032193, loss_ce: 0.011921
2022-01-10 03:26:03,469 iteration 2879 : loss : 0.021977, loss_ce: 0.010063
2022-01-10 03:26:04,938 iteration 2880 : loss : 0.030488, loss_ce: 0.012854
2022-01-10 03:26:06,333 iteration 2881 : loss : 0.026080, loss_ce: 0.010790
2022-01-10 03:26:07,827 iteration 2882 : loss : 0.046482, loss_ce: 0.013790
2022-01-10 03:26:09,317 iteration 2883 : loss : 0.029647, loss_ce: 0.012936
2022-01-10 03:26:10,805 iteration 2884 : loss : 0.027488, loss_ce: 0.008530
2022-01-10 03:26:12,222 iteration 2885 : loss : 0.033683, loss_ce: 0.016503
2022-01-10 03:26:13,593 iteration 2886 : loss : 0.028707, loss_ce: 0.006863
2022-01-10 03:26:14,994 iteration 2887 : loss : 0.024867, loss_ce: 0.008313
2022-01-10 03:26:16,490 iteration 2888 : loss : 0.026906, loss_ce: 0.008870
2022-01-10 03:26:17,911 iteration 2889 : loss : 0.040015, loss_ce: 0.018709
2022-01-10 03:26:17,911 Training Data Eval:
2022-01-10 03:26:24,957   Average segmentation loss on training set: 0.0202
2022-01-10 03:26:24,957 Validation Data Eval:
2022-01-10 03:26:27,396   Average segmentation loss on validation set: 0.1204
2022-01-10 03:26:28,847 iteration 2890 : loss : 0.032858, loss_ce: 0.012657
 42%|███████████▍               | 170/400 [1:14:35<1:45:35, 27.55s/it]2022-01-10 03:26:30,369 iteration 2891 : loss : 0.030889, loss_ce: 0.015536
2022-01-10 03:26:31,722 iteration 2892 : loss : 0.022567, loss_ce: 0.010728
2022-01-10 03:26:33,105 iteration 2893 : loss : 0.036479, loss_ce: 0.009955
2022-01-10 03:26:34,537 iteration 2894 : loss : 0.019869, loss_ce: 0.007917
2022-01-10 03:26:35,922 iteration 2895 : loss : 0.024916, loss_ce: 0.007833
2022-01-10 03:26:37,287 iteration 2896 : loss : 0.022723, loss_ce: 0.008833
2022-01-10 03:26:38,680 iteration 2897 : loss : 0.026077, loss_ce: 0.009821
2022-01-10 03:26:40,088 iteration 2898 : loss : 0.033655, loss_ce: 0.019939
2022-01-10 03:26:41,538 iteration 2899 : loss : 0.032732, loss_ce: 0.012973
2022-01-10 03:26:42,891 iteration 2900 : loss : 0.032848, loss_ce: 0.011661
2022-01-10 03:26:44,386 iteration 2901 : loss : 0.028259, loss_ce: 0.009502
2022-01-10 03:26:45,698 iteration 2902 : loss : 0.020551, loss_ce: 0.006677
2022-01-10 03:26:47,141 iteration 2903 : loss : 0.024524, loss_ce: 0.010337
2022-01-10 03:26:48,624 iteration 2904 : loss : 0.051594, loss_ce: 0.016415
2022-01-10 03:26:50,038 iteration 2905 : loss : 0.027517, loss_ce: 0.009200
2022-01-10 03:26:51,509 iteration 2906 : loss : 0.023472, loss_ce: 0.008103
2022-01-10 03:26:52,881 iteration 2907 : loss : 0.018911, loss_ce: 0.007475
 43%|███████████▌               | 171/400 [1:14:59<1:41:07, 26.49s/it]2022-01-10 03:26:54,348 iteration 2908 : loss : 0.023474, loss_ce: 0.008359
2022-01-10 03:26:55,691 iteration 2909 : loss : 0.023501, loss_ce: 0.005297
2022-01-10 03:26:57,135 iteration 2910 : loss : 0.033620, loss_ce: 0.014020
2022-01-10 03:26:58,553 iteration 2911 : loss : 0.037883, loss_ce: 0.018359
2022-01-10 03:26:59,955 iteration 2912 : loss : 0.026116, loss_ce: 0.009807
2022-01-10 03:27:01,407 iteration 2913 : loss : 0.023929, loss_ce: 0.009774
2022-01-10 03:27:02,828 iteration 2914 : loss : 0.027886, loss_ce: 0.011662
2022-01-10 03:27:04,288 iteration 2915 : loss : 0.025722, loss_ce: 0.008757
2022-01-10 03:27:05,646 iteration 2916 : loss : 0.023802, loss_ce: 0.012106
2022-01-10 03:27:07,111 iteration 2917 : loss : 0.040028, loss_ce: 0.016088
2022-01-10 03:27:08,545 iteration 2918 : loss : 0.030567, loss_ce: 0.010617
2022-01-10 03:27:09,994 iteration 2919 : loss : 0.037397, loss_ce: 0.018791
2022-01-10 03:27:11,428 iteration 2920 : loss : 0.041215, loss_ce: 0.010234
2022-01-10 03:27:12,909 iteration 2921 : loss : 0.031819, loss_ce: 0.009697
2022-01-10 03:27:14,309 iteration 2922 : loss : 0.030205, loss_ce: 0.012611
2022-01-10 03:27:15,756 iteration 2923 : loss : 0.030372, loss_ce: 0.015054
2022-01-10 03:27:17,148 iteration 2924 : loss : 0.021023, loss_ce: 0.006321
 43%|███████████▌               | 172/400 [1:15:23<1:38:09, 25.83s/it]2022-01-10 03:27:18,716 iteration 2925 : loss : 0.036832, loss_ce: 0.015428
2022-01-10 03:27:20,105 iteration 2926 : loss : 0.021343, loss_ce: 0.006276
2022-01-10 03:27:21,578 iteration 2927 : loss : 0.028823, loss_ce: 0.012867
2022-01-10 03:27:23,114 iteration 2928 : loss : 0.039744, loss_ce: 0.010612
2022-01-10 03:27:24,503 iteration 2929 : loss : 0.031962, loss_ce: 0.012880
2022-01-10 03:27:25,886 iteration 2930 : loss : 0.038395, loss_ce: 0.016722
2022-01-10 03:27:27,228 iteration 2931 : loss : 0.030447, loss_ce: 0.012114
2022-01-10 03:27:28,703 iteration 2932 : loss : 0.023140, loss_ce: 0.008681
2022-01-10 03:27:30,096 iteration 2933 : loss : 0.027574, loss_ce: 0.010770
2022-01-10 03:27:31,561 iteration 2934 : loss : 0.033348, loss_ce: 0.013572
2022-01-10 03:27:32,986 iteration 2935 : loss : 0.049557, loss_ce: 0.014598
2022-01-10 03:27:34,460 iteration 2936 : loss : 0.028022, loss_ce: 0.011898
2022-01-10 03:27:35,859 iteration 2937 : loss : 0.024262, loss_ce: 0.010763
2022-01-10 03:27:37,270 iteration 2938 : loss : 0.019774, loss_ce: 0.007625
2022-01-10 03:27:38,666 iteration 2939 : loss : 0.040074, loss_ce: 0.018215
2022-01-10 03:27:40,107 iteration 2940 : loss : 0.031885, loss_ce: 0.009816
2022-01-10 03:27:41,515 iteration 2941 : loss : 0.034392, loss_ce: 0.010516
 43%|███████████▋               | 173/400 [1:15:48<1:36:03, 25.39s/it]2022-01-10 03:27:43,003 iteration 2942 : loss : 0.031150, loss_ce: 0.012704
2022-01-10 03:27:44,332 iteration 2943 : loss : 0.020513, loss_ce: 0.008207
2022-01-10 03:27:45,679 iteration 2944 : loss : 0.035489, loss_ce: 0.012665
2022-01-10 03:27:47,138 iteration 2945 : loss : 0.037149, loss_ce: 0.016548
2022-01-10 03:27:48,514 iteration 2946 : loss : 0.055833, loss_ce: 0.024660
2022-01-10 03:27:49,873 iteration 2947 : loss : 0.020449, loss_ce: 0.007193
2022-01-10 03:27:51,276 iteration 2948 : loss : 0.028184, loss_ce: 0.010339
2022-01-10 03:27:52,720 iteration 2949 : loss : 0.027460, loss_ce: 0.010029
2022-01-10 03:27:54,124 iteration 2950 : loss : 0.027907, loss_ce: 0.011902
2022-01-10 03:27:55,482 iteration 2951 : loss : 0.028877, loss_ce: 0.010626
2022-01-10 03:27:56,889 iteration 2952 : loss : 0.030694, loss_ce: 0.007719
2022-01-10 03:27:58,292 iteration 2953 : loss : 0.024421, loss_ce: 0.009663
2022-01-10 03:27:59,732 iteration 2954 : loss : 0.036241, loss_ce: 0.012298
2022-01-10 03:28:01,105 iteration 2955 : loss : 0.021497, loss_ce: 0.009330
2022-01-10 03:28:02,503 iteration 2956 : loss : 0.021778, loss_ce: 0.009634
2022-01-10 03:28:03,909 iteration 2957 : loss : 0.040850, loss_ce: 0.009401
2022-01-10 03:28:05,313 iteration 2958 : loss : 0.038035, loss_ce: 0.012128
 44%|███████████▋               | 174/400 [1:16:11<1:33:49, 24.91s/it]2022-01-10 03:28:06,858 iteration 2959 : loss : 0.025661, loss_ce: 0.011313
2022-01-10 03:28:08,299 iteration 2960 : loss : 0.026537, loss_ce: 0.008862
2022-01-10 03:28:09,760 iteration 2961 : loss : 0.028270, loss_ce: 0.013398
2022-01-10 03:28:11,130 iteration 2962 : loss : 0.026236, loss_ce: 0.009089
2022-01-10 03:28:12,529 iteration 2963 : loss : 0.034997, loss_ce: 0.013358
2022-01-10 03:28:13,991 iteration 2964 : loss : 0.032452, loss_ce: 0.011433
2022-01-10 03:28:15,411 iteration 2965 : loss : 0.024862, loss_ce: 0.011520
2022-01-10 03:28:16,806 iteration 2966 : loss : 0.028518, loss_ce: 0.009128
2022-01-10 03:28:18,287 iteration 2967 : loss : 0.038990, loss_ce: 0.014312
2022-01-10 03:28:19,694 iteration 2968 : loss : 0.037848, loss_ce: 0.014792
2022-01-10 03:28:21,054 iteration 2969 : loss : 0.039569, loss_ce: 0.010250
2022-01-10 03:28:22,438 iteration 2970 : loss : 0.039341, loss_ce: 0.012848
2022-01-10 03:28:23,865 iteration 2971 : loss : 0.037053, loss_ce: 0.013791
2022-01-10 03:28:25,307 iteration 2972 : loss : 0.024246, loss_ce: 0.008632
2022-01-10 03:28:26,713 iteration 2973 : loss : 0.035719, loss_ce: 0.017284
2022-01-10 03:28:28,232 iteration 2974 : loss : 0.028862, loss_ce: 0.011553
2022-01-10 03:28:28,232 Training Data Eval:
2022-01-10 03:28:35,284   Average segmentation loss on training set: 0.0196
2022-01-10 03:28:35,284 Validation Data Eval:
2022-01-10 03:28:37,726   Average segmentation loss on validation set: 0.0799
2022-01-10 03:28:39,109 iteration 2975 : loss : 0.046293, loss_ce: 0.015681
 44%|███████████▊               | 175/400 [1:16:45<1:43:25, 27.58s/it]2022-01-10 03:28:40,610 iteration 2976 : loss : 0.034422, loss_ce: 0.013747
2022-01-10 03:28:42,142 iteration 2977 : loss : 0.033646, loss_ce: 0.012266
2022-01-10 03:28:43,562 iteration 2978 : loss : 0.031371, loss_ce: 0.011512
2022-01-10 03:28:45,045 iteration 2979 : loss : 0.043360, loss_ce: 0.010954
2022-01-10 03:28:46,606 iteration 2980 : loss : 0.039987, loss_ce: 0.017919
2022-01-10 03:28:48,020 iteration 2981 : loss : 0.025130, loss_ce: 0.010358
2022-01-10 03:28:49,430 iteration 2982 : loss : 0.032578, loss_ce: 0.012590
2022-01-10 03:28:50,875 iteration 2983 : loss : 0.041186, loss_ce: 0.009341
2022-01-10 03:28:52,308 iteration 2984 : loss : 0.037051, loss_ce: 0.019069
2022-01-10 03:28:53,727 iteration 2985 : loss : 0.030949, loss_ce: 0.015774
2022-01-10 03:28:55,133 iteration 2986 : loss : 0.037550, loss_ce: 0.017231
2022-01-10 03:28:56,517 iteration 2987 : loss : 0.038676, loss_ce: 0.014381
2022-01-10 03:28:57,949 iteration 2988 : loss : 0.028048, loss_ce: 0.011323
2022-01-10 03:28:59,330 iteration 2989 : loss : 0.025436, loss_ce: 0.009673
2022-01-10 03:29:00,745 iteration 2990 : loss : 0.049432, loss_ce: 0.019559
2022-01-10 03:29:02,125 iteration 2991 : loss : 0.026471, loss_ce: 0.008405
2022-01-10 03:29:03,557 iteration 2992 : loss : 0.033816, loss_ce: 0.015568
 44%|███████████▉               | 176/400 [1:17:10<1:39:27, 26.64s/it]2022-01-10 03:29:05,048 iteration 2993 : loss : 0.045808, loss_ce: 0.021038
2022-01-10 03:29:06,452 iteration 2994 : loss : 0.020434, loss_ce: 0.007798
2022-01-10 03:29:07,905 iteration 2995 : loss : 0.043012, loss_ce: 0.015538
2022-01-10 03:29:09,277 iteration 2996 : loss : 0.035861, loss_ce: 0.012984
2022-01-10 03:29:10,684 iteration 2997 : loss : 0.024873, loss_ce: 0.008070
2022-01-10 03:29:12,125 iteration 2998 : loss : 0.024371, loss_ce: 0.011372
2022-01-10 03:29:13,535 iteration 2999 : loss : 0.031911, loss_ce: 0.011134
2022-01-10 03:29:14,938 iteration 3000 : loss : 0.027184, loss_ce: 0.010363
2022-01-10 03:29:16,431 iteration 3001 : loss : 0.027582, loss_ce: 0.010684
2022-01-10 03:29:17,887 iteration 3002 : loss : 0.045104, loss_ce: 0.020609
2022-01-10 03:29:19,235 iteration 3003 : loss : 0.024189, loss_ce: 0.010240
2022-01-10 03:29:20,639 iteration 3004 : loss : 0.031532, loss_ce: 0.016416
2022-01-10 03:29:22,024 iteration 3005 : loss : 0.026326, loss_ce: 0.011206
2022-01-10 03:29:23,411 iteration 3006 : loss : 0.037952, loss_ce: 0.016757
2022-01-10 03:29:24,829 iteration 3007 : loss : 0.025184, loss_ce: 0.008624
2022-01-10 03:29:26,189 iteration 3008 : loss : 0.025400, loss_ce: 0.009853
2022-01-10 03:29:27,548 iteration 3009 : loss : 0.031314, loss_ce: 0.008195
 44%|███████████▉               | 177/400 [1:17:34<1:36:03, 25.84s/it]2022-01-10 03:29:29,092 iteration 3010 : loss : 0.043293, loss_ce: 0.014237
2022-01-10 03:29:30,483 iteration 3011 : loss : 0.035700, loss_ce: 0.010791
2022-01-10 03:29:31,858 iteration 3012 : loss : 0.039037, loss_ce: 0.013439
2022-01-10 03:29:33,343 iteration 3013 : loss : 0.035283, loss_ce: 0.013981
2022-01-10 03:29:34,785 iteration 3014 : loss : 0.035126, loss_ce: 0.014246
2022-01-10 03:29:36,213 iteration 3015 : loss : 0.025872, loss_ce: 0.007969
2022-01-10 03:29:37,554 iteration 3016 : loss : 0.025164, loss_ce: 0.008043
2022-01-10 03:29:39,018 iteration 3017 : loss : 0.037353, loss_ce: 0.018693
2022-01-10 03:29:40,515 iteration 3018 : loss : 0.033672, loss_ce: 0.018342
2022-01-10 03:29:42,073 iteration 3019 : loss : 0.041422, loss_ce: 0.015311
2022-01-10 03:29:43,465 iteration 3020 : loss : 0.024924, loss_ce: 0.008675
2022-01-10 03:29:44,871 iteration 3021 : loss : 0.025517, loss_ce: 0.009220
2022-01-10 03:29:46,216 iteration 3022 : loss : 0.027219, loss_ce: 0.015222
2022-01-10 03:29:47,591 iteration 3023 : loss : 0.022463, loss_ce: 0.008360
2022-01-10 03:29:49,005 iteration 3024 : loss : 0.035000, loss_ce: 0.015401
2022-01-10 03:29:50,421 iteration 3025 : loss : 0.027537, loss_ce: 0.011578
2022-01-10 03:29:51,786 iteration 3026 : loss : 0.028392, loss_ce: 0.008512
 44%|████████████               | 178/400 [1:17:58<1:33:50, 25.36s/it]2022-01-10 03:29:53,243 iteration 3027 : loss : 0.041220, loss_ce: 0.018409
2022-01-10 03:29:54,743 iteration 3028 : loss : 0.036511, loss_ce: 0.016863
2022-01-10 03:29:56,130 iteration 3029 : loss : 0.024172, loss_ce: 0.008779
2022-01-10 03:29:57,602 iteration 3030 : loss : 0.028701, loss_ce: 0.009436
2022-01-10 03:29:59,078 iteration 3031 : loss : 0.038605, loss_ce: 0.011745
2022-01-10 03:30:00,550 iteration 3032 : loss : 0.020857, loss_ce: 0.008740
2022-01-10 03:30:01,984 iteration 3033 : loss : 0.029766, loss_ce: 0.010071
2022-01-10 03:30:03,371 iteration 3034 : loss : 0.030359, loss_ce: 0.009548
2022-01-10 03:30:04,729 iteration 3035 : loss : 0.024123, loss_ce: 0.010780
2022-01-10 03:30:06,177 iteration 3036 : loss : 0.027531, loss_ce: 0.012043
2022-01-10 03:30:07,540 iteration 3037 : loss : 0.028142, loss_ce: 0.013296
2022-01-10 03:30:08,881 iteration 3038 : loss : 0.026662, loss_ce: 0.013081
2022-01-10 03:30:10,246 iteration 3039 : loss : 0.035012, loss_ce: 0.012396
2022-01-10 03:30:11,727 iteration 3040 : loss : 0.029951, loss_ce: 0.008574
2022-01-10 03:30:13,038 iteration 3041 : loss : 0.021033, loss_ce: 0.008214
2022-01-10 03:30:14,404 iteration 3042 : loss : 0.020520, loss_ce: 0.009916
2022-01-10 03:30:15,771 iteration 3043 : loss : 0.029406, loss_ce: 0.012300
 45%|████████████               | 179/400 [1:18:22<1:31:53, 24.95s/it]2022-01-10 03:30:17,270 iteration 3044 : loss : 0.028935, loss_ce: 0.011805
2022-01-10 03:30:18,607 iteration 3045 : loss : 0.024900, loss_ce: 0.008951
2022-01-10 03:30:20,103 iteration 3046 : loss : 0.028437, loss_ce: 0.012454
2022-01-10 03:30:21,608 iteration 3047 : loss : 0.024222, loss_ce: 0.007897
2022-01-10 03:30:23,055 iteration 3048 : loss : 0.032405, loss_ce: 0.012073
2022-01-10 03:30:24,501 iteration 3049 : loss : 0.020007, loss_ce: 0.006393
2022-01-10 03:30:25,954 iteration 3050 : loss : 0.033744, loss_ce: 0.008174
2022-01-10 03:30:27,445 iteration 3051 : loss : 0.029309, loss_ce: 0.010916
2022-01-10 03:30:28,896 iteration 3052 : loss : 0.026029, loss_ce: 0.012529
2022-01-10 03:30:30,236 iteration 3053 : loss : 0.020020, loss_ce: 0.007034
2022-01-10 03:30:31,610 iteration 3054 : loss : 0.025490, loss_ce: 0.010986
2022-01-10 03:30:32,955 iteration 3055 : loss : 0.030904, loss_ce: 0.011379
2022-01-10 03:30:34,288 iteration 3056 : loss : 0.021274, loss_ce: 0.007870
2022-01-10 03:30:35,808 iteration 3057 : loss : 0.031158, loss_ce: 0.011323
2022-01-10 03:30:37,182 iteration 3058 : loss : 0.021316, loss_ce: 0.007439
2022-01-10 03:30:38,555 iteration 3059 : loss : 0.023330, loss_ce: 0.012870
2022-01-10 03:30:38,555 Training Data Eval:
2022-01-10 03:30:45,626   Average segmentation loss on training set: 0.0182
2022-01-10 03:30:45,626 Validation Data Eval:
2022-01-10 03:30:48,064   Average segmentation loss on validation set: 0.1058
2022-01-10 03:30:49,508 iteration 3060 : loss : 0.027923, loss_ce: 0.010132
 45%|████████████▏              | 180/400 [1:18:56<1:41:08, 27.58s/it]2022-01-10 03:30:50,990 iteration 3061 : loss : 0.021507, loss_ce: 0.005895
2022-01-10 03:30:52,432 iteration 3062 : loss : 0.027731, loss_ce: 0.013264
2022-01-10 03:30:53,826 iteration 3063 : loss : 0.025362, loss_ce: 0.010629
2022-01-10 03:30:55,242 iteration 3064 : loss : 0.034464, loss_ce: 0.019551
2022-01-10 03:30:56,693 iteration 3065 : loss : 0.026456, loss_ce: 0.007159
2022-01-10 03:30:58,068 iteration 3066 : loss : 0.023215, loss_ce: 0.007036
2022-01-10 03:30:59,540 iteration 3067 : loss : 0.042465, loss_ce: 0.017001
2022-01-10 03:31:01,030 iteration 3068 : loss : 0.035412, loss_ce: 0.015629
2022-01-10 03:31:02,392 iteration 3069 : loss : 0.014082, loss_ce: 0.005429
2022-01-10 03:31:03,804 iteration 3070 : loss : 0.032715, loss_ce: 0.012354
2022-01-10 03:31:05,350 iteration 3071 : loss : 0.041856, loss_ce: 0.015328
2022-01-10 03:31:06,752 iteration 3072 : loss : 0.025120, loss_ce: 0.009967
2022-01-10 03:31:08,143 iteration 3073 : loss : 0.025101, loss_ce: 0.007938
2022-01-10 03:31:09,546 iteration 3074 : loss : 0.023601, loss_ce: 0.009685
2022-01-10 03:31:10,976 iteration 3075 : loss : 0.038159, loss_ce: 0.013602
2022-01-10 03:31:12,341 iteration 3076 : loss : 0.021783, loss_ce: 0.007634
2022-01-10 03:31:13,827 iteration 3077 : loss : 0.047900, loss_ce: 0.014583
 45%|████████████▏              | 181/400 [1:19:20<1:37:06, 26.60s/it]2022-01-10 03:31:15,340 iteration 3078 : loss : 0.056685, loss_ce: 0.009911
2022-01-10 03:31:16,736 iteration 3079 : loss : 0.021279, loss_ce: 0.006145
2022-01-10 03:31:18,135 iteration 3080 : loss : 0.021221, loss_ce: 0.006241
2022-01-10 03:31:19,553 iteration 3081 : loss : 0.031661, loss_ce: 0.011827
2022-01-10 03:31:20,969 iteration 3082 : loss : 0.042960, loss_ce: 0.014465
2022-01-10 03:31:22,384 iteration 3083 : loss : 0.031684, loss_ce: 0.009533
2022-01-10 03:31:23,826 iteration 3084 : loss : 0.042617, loss_ce: 0.013908
2022-01-10 03:31:25,175 iteration 3085 : loss : 0.025494, loss_ce: 0.009560
2022-01-10 03:31:26,568 iteration 3086 : loss : 0.031794, loss_ce: 0.016484
2022-01-10 03:31:28,020 iteration 3087 : loss : 0.028257, loss_ce: 0.012868
2022-01-10 03:31:29,428 iteration 3088 : loss : 0.041216, loss_ce: 0.019496
2022-01-10 03:31:30,917 iteration 3089 : loss : 0.059584, loss_ce: 0.023390
2022-01-10 03:31:32,273 iteration 3090 : loss : 0.025366, loss_ce: 0.010324
2022-01-10 03:31:33,728 iteration 3091 : loss : 0.036817, loss_ce: 0.014961
2022-01-10 03:31:35,100 iteration 3092 : loss : 0.036553, loss_ce: 0.016051
2022-01-10 03:31:36,466 iteration 3093 : loss : 0.024502, loss_ce: 0.010235
2022-01-10 03:31:37,860 iteration 3094 : loss : 0.081538, loss_ce: 0.024518
 46%|████████████▎              | 182/400 [1:19:44<1:33:51, 25.83s/it]2022-01-10 03:31:39,356 iteration 3095 : loss : 0.028973, loss_ce: 0.011619
2022-01-10 03:31:40,775 iteration 3096 : loss : 0.029918, loss_ce: 0.010024
2022-01-10 03:31:42,209 iteration 3097 : loss : 0.029702, loss_ce: 0.014620
2022-01-10 03:31:43,652 iteration 3098 : loss : 0.044932, loss_ce: 0.016562
2022-01-10 03:31:45,097 iteration 3099 : loss : 0.051026, loss_ce: 0.022566
2022-01-10 03:31:46,514 iteration 3100 : loss : 0.031648, loss_ce: 0.010763
2022-01-10 03:31:47,889 iteration 3101 : loss : 0.022960, loss_ce: 0.008733
2022-01-10 03:31:49,227 iteration 3102 : loss : 0.026663, loss_ce: 0.010898
2022-01-10 03:31:50,669 iteration 3103 : loss : 0.029224, loss_ce: 0.013671
2022-01-10 03:31:52,070 iteration 3104 : loss : 0.032600, loss_ce: 0.008883
2022-01-10 03:31:53,525 iteration 3105 : loss : 0.028746, loss_ce: 0.011855
2022-01-10 03:31:54,945 iteration 3106 : loss : 0.044331, loss_ce: 0.018966
2022-01-10 03:31:56,421 iteration 3107 : loss : 0.027180, loss_ce: 0.011554
2022-01-10 03:31:57,862 iteration 3108 : loss : 0.033075, loss_ce: 0.011979
2022-01-10 03:31:59,206 iteration 3109 : loss : 0.026582, loss_ce: 0.014740
2022-01-10 03:32:00,592 iteration 3110 : loss : 0.030651, loss_ce: 0.012335
2022-01-10 03:32:02,009 iteration 3111 : loss : 0.029579, loss_ce: 0.007931
 46%|████████████▎              | 183/400 [1:20:08<1:31:36, 25.33s/it]2022-01-10 03:32:03,434 iteration 3112 : loss : 0.024306, loss_ce: 0.009460
2022-01-10 03:32:04,868 iteration 3113 : loss : 0.050421, loss_ce: 0.020654
2022-01-10 03:32:06,240 iteration 3114 : loss : 0.023794, loss_ce: 0.011128
2022-01-10 03:32:07,677 iteration 3115 : loss : 0.028766, loss_ce: 0.009770
2022-01-10 03:32:09,029 iteration 3116 : loss : 0.024427, loss_ce: 0.009669
2022-01-10 03:32:10,507 iteration 3117 : loss : 0.037202, loss_ce: 0.010576
2022-01-10 03:32:11,940 iteration 3118 : loss : 0.025350, loss_ce: 0.011782
2022-01-10 03:32:13,363 iteration 3119 : loss : 0.023479, loss_ce: 0.009273
2022-01-10 03:32:14,719 iteration 3120 : loss : 0.025514, loss_ce: 0.007888
2022-01-10 03:32:16,154 iteration 3121 : loss : 0.050913, loss_ce: 0.013557
2022-01-10 03:32:17,557 iteration 3122 : loss : 0.036243, loss_ce: 0.015335
2022-01-10 03:32:19,077 iteration 3123 : loss : 0.041180, loss_ce: 0.015662
2022-01-10 03:32:20,452 iteration 3124 : loss : 0.027892, loss_ce: 0.011916
2022-01-10 03:32:21,831 iteration 3125 : loss : 0.017443, loss_ce: 0.007109
2022-01-10 03:32:23,210 iteration 3126 : loss : 0.023737, loss_ce: 0.008935
2022-01-10 03:32:24,598 iteration 3127 : loss : 0.023948, loss_ce: 0.010642
2022-01-10 03:32:25,993 iteration 3128 : loss : 0.041258, loss_ce: 0.010111
 46%|████████████▍              | 184/400 [1:20:32<1:29:43, 24.93s/it]2022-01-10 03:32:27,432 iteration 3129 : loss : 0.040550, loss_ce: 0.011682
2022-01-10 03:32:28,805 iteration 3130 : loss : 0.023785, loss_ce: 0.008683
2022-01-10 03:32:30,257 iteration 3131 : loss : 0.039152, loss_ce: 0.020133
2022-01-10 03:32:31,688 iteration 3132 : loss : 0.023466, loss_ce: 0.008414
2022-01-10 03:32:33,060 iteration 3133 : loss : 0.020388, loss_ce: 0.008680
2022-01-10 03:32:34,503 iteration 3134 : loss : 0.026749, loss_ce: 0.011408
2022-01-10 03:32:35,954 iteration 3135 : loss : 0.037839, loss_ce: 0.008039
2022-01-10 03:32:37,319 iteration 3136 : loss : 0.023781, loss_ce: 0.012077
2022-01-10 03:32:38,684 iteration 3137 : loss : 0.025125, loss_ce: 0.010492
2022-01-10 03:32:40,073 iteration 3138 : loss : 0.023556, loss_ce: 0.009228
2022-01-10 03:32:41,606 iteration 3139 : loss : 0.052458, loss_ce: 0.020157
2022-01-10 03:32:43,049 iteration 3140 : loss : 0.027292, loss_ce: 0.009939
2022-01-10 03:32:44,447 iteration 3141 : loss : 0.029149, loss_ce: 0.013496
2022-01-10 03:32:45,794 iteration 3142 : loss : 0.022951, loss_ce: 0.007971
2022-01-10 03:32:47,221 iteration 3143 : loss : 0.044418, loss_ce: 0.012775
2022-01-10 03:32:48,685 iteration 3144 : loss : 0.024051, loss_ce: 0.005517
2022-01-10 03:32:48,685 Training Data Eval:
2022-01-10 03:32:55,731   Average segmentation loss on training set: 0.0184
2022-01-10 03:32:55,732 Validation Data Eval:
2022-01-10 03:32:58,172   Average segmentation loss on validation set: 0.1095
2022-01-10 03:32:59,518 iteration 3145 : loss : 0.026250, loss_ce: 0.012093
 46%|████████████▍              | 185/400 [1:21:06<1:38:33, 27.50s/it]2022-01-10 03:33:01,065 iteration 3146 : loss : 0.025114, loss_ce: 0.009469
2022-01-10 03:33:02,541 iteration 3147 : loss : 0.052471, loss_ce: 0.021983
2022-01-10 03:33:03,894 iteration 3148 : loss : 0.027759, loss_ce: 0.010756
2022-01-10 03:33:05,306 iteration 3149 : loss : 0.040674, loss_ce: 0.012002
2022-01-10 03:33:06,719 iteration 3150 : loss : 0.032961, loss_ce: 0.009942
2022-01-10 03:33:08,049 iteration 3151 : loss : 0.021487, loss_ce: 0.009633
2022-01-10 03:33:09,501 iteration 3152 : loss : 0.025274, loss_ce: 0.013437
2022-01-10 03:33:10,889 iteration 3153 : loss : 0.035163, loss_ce: 0.017471
2022-01-10 03:33:12,311 iteration 3154 : loss : 0.029520, loss_ce: 0.007821
2022-01-10 03:33:13,732 iteration 3155 : loss : 0.023463, loss_ce: 0.008656
2022-01-10 03:33:15,167 iteration 3156 : loss : 0.021435, loss_ce: 0.008284
2022-01-10 03:33:16,509 iteration 3157 : loss : 0.029440, loss_ce: 0.009671
2022-01-10 03:33:17,988 iteration 3158 : loss : 0.024630, loss_ce: 0.009302
2022-01-10 03:33:19,398 iteration 3159 : loss : 0.026852, loss_ce: 0.015031
2022-01-10 03:33:20,797 iteration 3160 : loss : 0.021369, loss_ce: 0.008966
2022-01-10 03:33:22,289 iteration 3161 : loss : 0.031775, loss_ce: 0.009720
2022-01-10 03:33:23,799 iteration 3162 : loss : 0.026047, loss_ce: 0.011664
 46%|████████████▌              | 186/400 [1:21:30<1:34:39, 26.54s/it]2022-01-10 03:33:25,227 iteration 3163 : loss : 0.043456, loss_ce: 0.011775
2022-01-10 03:33:26,591 iteration 3164 : loss : 0.024243, loss_ce: 0.007422
2022-01-10 03:33:27,978 iteration 3165 : loss : 0.015625, loss_ce: 0.006364
2022-01-10 03:33:29,387 iteration 3166 : loss : 0.023460, loss_ce: 0.011090
2022-01-10 03:33:30,812 iteration 3167 : loss : 0.035208, loss_ce: 0.011897
2022-01-10 03:33:32,174 iteration 3168 : loss : 0.019201, loss_ce: 0.009188
2022-01-10 03:33:33,588 iteration 3169 : loss : 0.020005, loss_ce: 0.007720
2022-01-10 03:33:35,023 iteration 3170 : loss : 0.033549, loss_ce: 0.012730
2022-01-10 03:33:36,493 iteration 3171 : loss : 0.031596, loss_ce: 0.008300
2022-01-10 03:33:37,943 iteration 3172 : loss : 0.032982, loss_ce: 0.013939
2022-01-10 03:33:39,387 iteration 3173 : loss : 0.029329, loss_ce: 0.011524
2022-01-10 03:33:40,854 iteration 3174 : loss : 0.031637, loss_ce: 0.011006
2022-01-10 03:33:42,266 iteration 3175 : loss : 0.022225, loss_ce: 0.008373
2022-01-10 03:33:43,698 iteration 3176 : loss : 0.029081, loss_ce: 0.014116
2022-01-10 03:33:45,077 iteration 3177 : loss : 0.020454, loss_ce: 0.008715
2022-01-10 03:33:46,495 iteration 3178 : loss : 0.022523, loss_ce: 0.006320
2022-01-10 03:33:47,941 iteration 3179 : loss : 0.033884, loss_ce: 0.011361
 47%|████████████▌              | 187/400 [1:21:54<1:31:39, 25.82s/it]2022-01-10 03:33:49,476 iteration 3180 : loss : 0.023437, loss_ce: 0.010322
2022-01-10 03:33:50,903 iteration 3181 : loss : 0.024985, loss_ce: 0.009748
2022-01-10 03:33:52,358 iteration 3182 : loss : 0.026141, loss_ce: 0.010146
2022-01-10 03:33:53,820 iteration 3183 : loss : 0.025954, loss_ce: 0.011347
2022-01-10 03:33:55,155 iteration 3184 : loss : 0.017995, loss_ce: 0.007819
2022-01-10 03:33:56,511 iteration 3185 : loss : 0.023052, loss_ce: 0.009217
2022-01-10 03:33:57,886 iteration 3186 : loss : 0.024386, loss_ce: 0.011629
2022-01-10 03:33:59,245 iteration 3187 : loss : 0.017970, loss_ce: 0.005560
2022-01-10 03:34:00,647 iteration 3188 : loss : 0.045334, loss_ce: 0.016257
2022-01-10 03:34:01,998 iteration 3189 : loss : 0.019994, loss_ce: 0.010299
2022-01-10 03:34:03,454 iteration 3190 : loss : 0.032722, loss_ce: 0.012869
2022-01-10 03:34:04,892 iteration 3191 : loss : 0.031083, loss_ce: 0.011167
2022-01-10 03:34:06,293 iteration 3192 : loss : 0.018753, loss_ce: 0.007570
2022-01-10 03:34:07,654 iteration 3193 : loss : 0.022561, loss_ce: 0.009885
2022-01-10 03:34:08,997 iteration 3194 : loss : 0.024864, loss_ce: 0.007833
2022-01-10 03:34:10,385 iteration 3195 : loss : 0.026725, loss_ce: 0.007965
2022-01-10 03:34:11,827 iteration 3196 : loss : 0.024823, loss_ce: 0.007168
 47%|████████████▋              | 188/400 [1:22:18<1:29:10, 25.24s/it]2022-01-10 03:34:13,277 iteration 3197 : loss : 0.020772, loss_ce: 0.006490
2022-01-10 03:34:14,640 iteration 3198 : loss : 0.026343, loss_ce: 0.008266
2022-01-10 03:34:15,988 iteration 3199 : loss : 0.020500, loss_ce: 0.007753
2022-01-10 03:34:17,356 iteration 3200 : loss : 0.020305, loss_ce: 0.008552
2022-01-10 03:34:18,792 iteration 3201 : loss : 0.058266, loss_ce: 0.034455
2022-01-10 03:34:20,222 iteration 3202 : loss : 0.046951, loss_ce: 0.024080
2022-01-10 03:34:21,567 iteration 3203 : loss : 0.033435, loss_ce: 0.005408
2022-01-10 03:34:22,989 iteration 3204 : loss : 0.024693, loss_ce: 0.012517
2022-01-10 03:34:24,419 iteration 3205 : loss : 0.023058, loss_ce: 0.007366
2022-01-10 03:34:25,789 iteration 3206 : loss : 0.027672, loss_ce: 0.012418
2022-01-10 03:34:27,145 iteration 3207 : loss : 0.022271, loss_ce: 0.007100
2022-01-10 03:34:28,510 iteration 3208 : loss : 0.028103, loss_ce: 0.009411
2022-01-10 03:34:29,949 iteration 3209 : loss : 0.023969, loss_ce: 0.008875
2022-01-10 03:34:31,366 iteration 3210 : loss : 0.040520, loss_ce: 0.015293
2022-01-10 03:34:32,820 iteration 3211 : loss : 0.032196, loss_ce: 0.015802
2022-01-10 03:34:34,258 iteration 3212 : loss : 0.025116, loss_ce: 0.012203
2022-01-10 03:34:35,656 iteration 3213 : loss : 0.024769, loss_ce: 0.011685
 47%|████████████▊              | 189/400 [1:22:42<1:27:16, 24.82s/it]2022-01-10 03:34:37,105 iteration 3214 : loss : 0.023480, loss_ce: 0.009193
2022-01-10 03:34:38,482 iteration 3215 : loss : 0.025341, loss_ce: 0.012936
2022-01-10 03:34:39,871 iteration 3216 : loss : 0.028915, loss_ce: 0.013291
2022-01-10 03:34:41,304 iteration 3217 : loss : 0.031990, loss_ce: 0.009932
2022-01-10 03:34:42,670 iteration 3218 : loss : 0.023355, loss_ce: 0.009819
2022-01-10 03:34:44,040 iteration 3219 : loss : 0.020060, loss_ce: 0.006125
2022-01-10 03:34:45,459 iteration 3220 : loss : 0.035809, loss_ce: 0.018201
2022-01-10 03:34:46,890 iteration 3221 : loss : 0.033531, loss_ce: 0.012129
2022-01-10 03:34:48,250 iteration 3222 : loss : 0.019612, loss_ce: 0.007047
2022-01-10 03:34:49,670 iteration 3223 : loss : 0.027680, loss_ce: 0.007063
2022-01-10 03:34:51,070 iteration 3224 : loss : 0.017213, loss_ce: 0.005696
2022-01-10 03:34:52,414 iteration 3225 : loss : 0.022928, loss_ce: 0.008978
2022-01-10 03:34:53,810 iteration 3226 : loss : 0.023430, loss_ce: 0.010123
2022-01-10 03:34:55,222 iteration 3227 : loss : 0.025482, loss_ce: 0.009137
2022-01-10 03:34:56,622 iteration 3228 : loss : 0.037689, loss_ce: 0.017609
2022-01-10 03:34:58,047 iteration 3229 : loss : 0.021487, loss_ce: 0.009145
2022-01-10 03:34:58,047 Training Data Eval:
2022-01-10 03:35:05,099   Average segmentation loss on training set: 0.0163
2022-01-10 03:35:05,099 Validation Data Eval:
2022-01-10 03:35:07,533   Average segmentation loss on validation set: 0.0978
2022-01-10 03:35:08,910 iteration 3230 : loss : 0.029451, loss_ce: 0.014646
 48%|████████████▊              | 190/400 [1:23:15<1:35:42, 27.35s/it]2022-01-10 03:35:10,272 iteration 3231 : loss : 0.021884, loss_ce: 0.008282
2022-01-10 03:35:11,684 iteration 3232 : loss : 0.020059, loss_ce: 0.007933
2022-01-10 03:35:13,095 iteration 3233 : loss : 0.031033, loss_ce: 0.008706
2022-01-10 03:35:14,462 iteration 3234 : loss : 0.018229, loss_ce: 0.007469
2022-01-10 03:35:15,876 iteration 3235 : loss : 0.021395, loss_ce: 0.008113
2022-01-10 03:35:17,176 iteration 3236 : loss : 0.019636, loss_ce: 0.010267
2022-01-10 03:35:18,555 iteration 3237 : loss : 0.023371, loss_ce: 0.009006
2022-01-10 03:35:19,961 iteration 3238 : loss : 0.023386, loss_ce: 0.009409
2022-01-10 03:35:21,285 iteration 3239 : loss : 0.018346, loss_ce: 0.007860
2022-01-10 03:35:22,718 iteration 3240 : loss : 0.031182, loss_ce: 0.011764
2022-01-10 03:35:24,207 iteration 3241 : loss : 0.038266, loss_ce: 0.014369
2022-01-10 03:35:25,627 iteration 3242 : loss : 0.028387, loss_ce: 0.009937
2022-01-10 03:35:27,074 iteration 3243 : loss : 0.025162, loss_ce: 0.011805
2022-01-10 03:35:28,439 iteration 3244 : loss : 0.024050, loss_ce: 0.007599
2022-01-10 03:35:29,860 iteration 3245 : loss : 0.034300, loss_ce: 0.011075
2022-01-10 03:35:31,298 iteration 3246 : loss : 0.030795, loss_ce: 0.010167
2022-01-10 03:35:32,806 iteration 3247 : loss : 0.038732, loss_ce: 0.013652
 48%|████████████▉              | 191/400 [1:23:39<1:31:39, 26.31s/it]2022-01-10 03:35:34,319 iteration 3248 : loss : 0.041264, loss_ce: 0.014351
2022-01-10 03:35:35,767 iteration 3249 : loss : 0.031803, loss_ce: 0.010224
2022-01-10 03:35:37,184 iteration 3250 : loss : 0.029943, loss_ce: 0.011765
2022-01-10 03:35:38,609 iteration 3251 : loss : 0.019009, loss_ce: 0.006738
2022-01-10 03:35:39,925 iteration 3252 : loss : 0.016730, loss_ce: 0.007811
2022-01-10 03:35:41,314 iteration 3253 : loss : 0.023321, loss_ce: 0.008106
2022-01-10 03:35:42,758 iteration 3254 : loss : 0.038567, loss_ce: 0.017418
2022-01-10 03:35:44,173 iteration 3255 : loss : 0.026326, loss_ce: 0.010328
2022-01-10 03:35:45,616 iteration 3256 : loss : 0.026227, loss_ce: 0.010588
2022-01-10 03:35:47,040 iteration 3257 : loss : 0.031941, loss_ce: 0.013209
2022-01-10 03:35:48,396 iteration 3258 : loss : 0.020559, loss_ce: 0.005650
2022-01-10 03:35:49,810 iteration 3259 : loss : 0.037450, loss_ce: 0.014502
2022-01-10 03:35:51,245 iteration 3260 : loss : 0.029669, loss_ce: 0.011809
2022-01-10 03:35:52,646 iteration 3261 : loss : 0.022870, loss_ce: 0.008942
2022-01-10 03:35:54,136 iteration 3262 : loss : 0.023943, loss_ce: 0.008327
2022-01-10 03:35:55,524 iteration 3263 : loss : 0.028278, loss_ce: 0.009954
2022-01-10 03:35:56,996 iteration 3264 : loss : 0.036929, loss_ce: 0.015689
 48%|████████████▉              | 192/400 [1:24:03<1:29:00, 25.68s/it]2022-01-10 03:35:58,435 iteration 3265 : loss : 0.034321, loss_ce: 0.013336
2022-01-10 03:35:59,874 iteration 3266 : loss : 0.025682, loss_ce: 0.008321
2022-01-10 03:36:01,296 iteration 3267 : loss : 0.018429, loss_ce: 0.005937
2022-01-10 03:36:02,718 iteration 3268 : loss : 0.028513, loss_ce: 0.010636
2022-01-10 03:36:04,080 iteration 3269 : loss : 0.023305, loss_ce: 0.011906
2022-01-10 03:36:05,492 iteration 3270 : loss : 0.025728, loss_ce: 0.009684
2022-01-10 03:36:06,819 iteration 3271 : loss : 0.023128, loss_ce: 0.007064
2022-01-10 03:36:08,295 iteration 3272 : loss : 0.031540, loss_ce: 0.012356
2022-01-10 03:36:09,698 iteration 3273 : loss : 0.025414, loss_ce: 0.011234
2022-01-10 03:36:11,035 iteration 3274 : loss : 0.027200, loss_ce: 0.009474
2022-01-10 03:36:12,414 iteration 3275 : loss : 0.026759, loss_ce: 0.010176
2022-01-10 03:36:13,797 iteration 3276 : loss : 0.023579, loss_ce: 0.006948
2022-01-10 03:36:15,141 iteration 3277 : loss : 0.018589, loss_ce: 0.006661
2022-01-10 03:36:16,533 iteration 3278 : loss : 0.019881, loss_ce: 0.008029
2022-01-10 03:36:17,840 iteration 3279 : loss : 0.023450, loss_ce: 0.013572
2022-01-10 03:36:19,306 iteration 3280 : loss : 0.025577, loss_ce: 0.010213
2022-01-10 03:36:20,705 iteration 3281 : loss : 0.016891, loss_ce: 0.004741
 48%|█████████████              | 193/400 [1:24:27<1:26:32, 25.08s/it]2022-01-10 03:36:22,199 iteration 3282 : loss : 0.022788, loss_ce: 0.006516
2022-01-10 03:36:23,647 iteration 3283 : loss : 0.023861, loss_ce: 0.009005
2022-01-10 03:36:25,065 iteration 3284 : loss : 0.028861, loss_ce: 0.010078
2022-01-10 03:36:26,500 iteration 3285 : loss : 0.031541, loss_ce: 0.010147
2022-01-10 03:36:27,906 iteration 3286 : loss : 0.026439, loss_ce: 0.007669
2022-01-10 03:36:29,391 iteration 3287 : loss : 0.028112, loss_ce: 0.011903
2022-01-10 03:36:30,744 iteration 3288 : loss : 0.022533, loss_ce: 0.009584
2022-01-10 03:36:32,250 iteration 3289 : loss : 0.018104, loss_ce: 0.006972
2022-01-10 03:36:33,717 iteration 3290 : loss : 0.024167, loss_ce: 0.008396
2022-01-10 03:36:35,194 iteration 3291 : loss : 0.027953, loss_ce: 0.013625
2022-01-10 03:36:36,602 iteration 3292 : loss : 0.034133, loss_ce: 0.011151
2022-01-10 03:36:38,001 iteration 3293 : loss : 0.051816, loss_ce: 0.015248
2022-01-10 03:36:39,503 iteration 3294 : loss : 0.030553, loss_ce: 0.012673
2022-01-10 03:36:40,953 iteration 3295 : loss : 0.020120, loss_ce: 0.008452
2022-01-10 03:36:42,307 iteration 3296 : loss : 0.025308, loss_ce: 0.009242
2022-01-10 03:36:43,684 iteration 3297 : loss : 0.021091, loss_ce: 0.008314
2022-01-10 03:36:45,182 iteration 3298 : loss : 0.018676, loss_ce: 0.008153
 48%|█████████████              | 194/400 [1:24:51<1:25:30, 24.90s/it]2022-01-10 03:36:46,578 iteration 3299 : loss : 0.019791, loss_ce: 0.005008
2022-01-10 03:36:48,024 iteration 3300 : loss : 0.029877, loss_ce: 0.010385
2022-01-10 03:36:49,436 iteration 3301 : loss : 0.020968, loss_ce: 0.009098
2022-01-10 03:36:50,813 iteration 3302 : loss : 0.029105, loss_ce: 0.008251
2022-01-10 03:36:52,278 iteration 3303 : loss : 0.028104, loss_ce: 0.013661
2022-01-10 03:36:53,726 iteration 3304 : loss : 0.025515, loss_ce: 0.009133
2022-01-10 03:36:55,123 iteration 3305 : loss : 0.022069, loss_ce: 0.007036
2022-01-10 03:36:56,525 iteration 3306 : loss : 0.026638, loss_ce: 0.006640
2022-01-10 03:36:58,055 iteration 3307 : loss : 0.032528, loss_ce: 0.014751
2022-01-10 03:36:59,530 iteration 3308 : loss : 0.025578, loss_ce: 0.011789
2022-01-10 03:37:00,987 iteration 3309 : loss : 0.024786, loss_ce: 0.010740
2022-01-10 03:37:02,358 iteration 3310 : loss : 0.020163, loss_ce: 0.010626
2022-01-10 03:37:03,838 iteration 3311 : loss : 0.034623, loss_ce: 0.010757
2022-01-10 03:37:05,247 iteration 3312 : loss : 0.035319, loss_ce: 0.009327
2022-01-10 03:37:06,730 iteration 3313 : loss : 0.030622, loss_ce: 0.012646
2022-01-10 03:37:08,146 iteration 3314 : loss : 0.030395, loss_ce: 0.009961
2022-01-10 03:37:08,147 Training Data Eval:
2022-01-10 03:37:15,205   Average segmentation loss on training set: 0.0159
2022-01-10 03:37:15,205 Validation Data Eval:
2022-01-10 03:37:17,639   Average segmentation loss on validation set: 0.1012
2022-01-10 03:37:18,996 iteration 3315 : loss : 0.025535, loss_ce: 0.012198
 49%|█████████████▏             | 195/400 [1:25:25<1:34:13, 27.58s/it]2022-01-10 03:37:20,416 iteration 3316 : loss : 0.019038, loss_ce: 0.008172
2022-01-10 03:37:21,836 iteration 3317 : loss : 0.021906, loss_ce: 0.009960
2022-01-10 03:37:23,228 iteration 3318 : loss : 0.033986, loss_ce: 0.014235
2022-01-10 03:37:24,583 iteration 3319 : loss : 0.023124, loss_ce: 0.007971
2022-01-10 03:37:26,073 iteration 3320 : loss : 0.031746, loss_ce: 0.013643
2022-01-10 03:37:27,458 iteration 3321 : loss : 0.022012, loss_ce: 0.007173
2022-01-10 03:37:28,918 iteration 3322 : loss : 0.044233, loss_ce: 0.015525
2022-01-10 03:37:30,274 iteration 3323 : loss : 0.019956, loss_ce: 0.006398
2022-01-10 03:37:31,677 iteration 3324 : loss : 0.023370, loss_ce: 0.011149
2022-01-10 03:37:33,115 iteration 3325 : loss : 0.026831, loss_ce: 0.011078
2022-01-10 03:37:34,483 iteration 3326 : loss : 0.023970, loss_ce: 0.008337
2022-01-10 03:37:35,883 iteration 3327 : loss : 0.030126, loss_ce: 0.010050
2022-01-10 03:37:37,349 iteration 3328 : loss : 0.025063, loss_ce: 0.009634
2022-01-10 03:37:38,760 iteration 3329 : loss : 0.032381, loss_ce: 0.011923
2022-01-10 03:37:40,140 iteration 3330 : loss : 0.021131, loss_ce: 0.011028
2022-01-10 03:37:41,476 iteration 3331 : loss : 0.020966, loss_ce: 0.009053
2022-01-10 03:37:42,908 iteration 3332 : loss : 0.030841, loss_ce: 0.010232
 49%|█████████████▏             | 196/400 [1:25:49<1:30:00, 26.48s/it]2022-01-10 03:37:44,405 iteration 3333 : loss : 0.026714, loss_ce: 0.009830
2022-01-10 03:37:45,838 iteration 3334 : loss : 0.029150, loss_ce: 0.014417
2022-01-10 03:37:47,331 iteration 3335 : loss : 0.034640, loss_ce: 0.013972
2022-01-10 03:37:48,749 iteration 3336 : loss : 0.032019, loss_ce: 0.009351
2022-01-10 03:37:50,205 iteration 3337 : loss : 0.034780, loss_ce: 0.014939
2022-01-10 03:37:51,718 iteration 3338 : loss : 0.024803, loss_ce: 0.011172
2022-01-10 03:37:53,080 iteration 3339 : loss : 0.020290, loss_ce: 0.009258
2022-01-10 03:37:54,423 iteration 3340 : loss : 0.024681, loss_ce: 0.006927
2022-01-10 03:37:55,821 iteration 3341 : loss : 0.027680, loss_ce: 0.010177
2022-01-10 03:37:57,206 iteration 3342 : loss : 0.019735, loss_ce: 0.008968
2022-01-10 03:37:58,649 iteration 3343 : loss : 0.019480, loss_ce: 0.008135
2022-01-10 03:38:00,060 iteration 3344 : loss : 0.027248, loss_ce: 0.010190
2022-01-10 03:38:01,482 iteration 3345 : loss : 0.025672, loss_ce: 0.010202
2022-01-10 03:38:02,860 iteration 3346 : loss : 0.026850, loss_ce: 0.014112
2022-01-10 03:38:04,227 iteration 3347 : loss : 0.019069, loss_ce: 0.006942
2022-01-10 03:38:05,561 iteration 3348 : loss : 0.019288, loss_ce: 0.007717
2022-01-10 03:38:07,028 iteration 3349 : loss : 0.040385, loss_ce: 0.010961
 49%|█████████████▎             | 197/400 [1:26:13<1:27:11, 25.77s/it]2022-01-10 03:38:08,505 iteration 3350 : loss : 0.027298, loss_ce: 0.009507
2022-01-10 03:38:09,966 iteration 3351 : loss : 0.031174, loss_ce: 0.011242
2022-01-10 03:38:11,416 iteration 3352 : loss : 0.036060, loss_ce: 0.014724
2022-01-10 03:38:12,911 iteration 3353 : loss : 0.021210, loss_ce: 0.006346
2022-01-10 03:38:14,301 iteration 3354 : loss : 0.023295, loss_ce: 0.010435
2022-01-10 03:38:15,777 iteration 3355 : loss : 0.027915, loss_ce: 0.012915
2022-01-10 03:38:17,241 iteration 3356 : loss : 0.047456, loss_ce: 0.021466
2022-01-10 03:38:18,642 iteration 3357 : loss : 0.025337, loss_ce: 0.007826
2022-01-10 03:38:20,094 iteration 3358 : loss : 0.022636, loss_ce: 0.010403
2022-01-10 03:38:21,440 iteration 3359 : loss : 0.020862, loss_ce: 0.007380
2022-01-10 03:38:22,826 iteration 3360 : loss : 0.024658, loss_ce: 0.009287
2022-01-10 03:38:24,231 iteration 3361 : loss : 0.024151, loss_ce: 0.008326
2022-01-10 03:38:25,628 iteration 3362 : loss : 0.023260, loss_ce: 0.008628
2022-01-10 03:38:27,108 iteration 3363 : loss : 0.054020, loss_ce: 0.017472
2022-01-10 03:38:28,464 iteration 3364 : loss : 0.020932, loss_ce: 0.008764
2022-01-10 03:38:29,904 iteration 3365 : loss : 0.028362, loss_ce: 0.012464
2022-01-10 03:38:31,239 iteration 3366 : loss : 0.019407, loss_ce: 0.007975
 50%|█████████████▎             | 198/400 [1:26:37<1:25:10, 25.30s/it]2022-01-10 03:38:32,704 iteration 3367 : loss : 0.018544, loss_ce: 0.006858
2022-01-10 03:38:34,089 iteration 3368 : loss : 0.017576, loss_ce: 0.007821
2022-01-10 03:38:35,423 iteration 3369 : loss : 0.020240, loss_ce: 0.006575
2022-01-10 03:38:36,885 iteration 3370 : loss : 0.029905, loss_ce: 0.012825
2022-01-10 03:38:38,407 iteration 3371 : loss : 0.035959, loss_ce: 0.013320
2022-01-10 03:38:39,779 iteration 3372 : loss : 0.016201, loss_ce: 0.006375
2022-01-10 03:38:41,200 iteration 3373 : loss : 0.019571, loss_ce: 0.006619
2022-01-10 03:38:42,558 iteration 3374 : loss : 0.018465, loss_ce: 0.006160
2022-01-10 03:38:43,923 iteration 3375 : loss : 0.027996, loss_ce: 0.010947
2022-01-10 03:38:45,332 iteration 3376 : loss : 0.020489, loss_ce: 0.005587
2022-01-10 03:38:46,647 iteration 3377 : loss : 0.015199, loss_ce: 0.005935
2022-01-10 03:38:48,076 iteration 3378 : loss : 0.035634, loss_ce: 0.006905
2022-01-10 03:38:49,575 iteration 3379 : loss : 0.040308, loss_ce: 0.020624
2022-01-10 03:38:51,032 iteration 3380 : loss : 0.027679, loss_ce: 0.009127
2022-01-10 03:38:52,477 iteration 3381 : loss : 0.036546, loss_ce: 0.013397
2022-01-10 03:38:53,902 iteration 3382 : loss : 0.026194, loss_ce: 0.009533
2022-01-10 03:38:55,280 iteration 3383 : loss : 0.024138, loss_ce: 0.008073
 50%|█████████████▍             | 199/400 [1:27:01<1:23:29, 24.92s/it]2022-01-10 03:38:56,813 iteration 3384 : loss : 0.036631, loss_ce: 0.013413
2022-01-10 03:38:58,261 iteration 3385 : loss : 0.026813, loss_ce: 0.010179
2022-01-10 03:38:59,624 iteration 3386 : loss : 0.017747, loss_ce: 0.005391
2022-01-10 03:39:01,101 iteration 3387 : loss : 0.025049, loss_ce: 0.009440
2022-01-10 03:39:02,539 iteration 3388 : loss : 0.037726, loss_ce: 0.017031
2022-01-10 03:39:03,923 iteration 3389 : loss : 0.020209, loss_ce: 0.008134
2022-01-10 03:39:05,222 iteration 3390 : loss : 0.015197, loss_ce: 0.006184
2022-01-10 03:39:06,669 iteration 3391 : loss : 0.039110, loss_ce: 0.014556
2022-01-10 03:39:08,109 iteration 3392 : loss : 0.035258, loss_ce: 0.010174
2022-01-10 03:39:09,532 iteration 3393 : loss : 0.026354, loss_ce: 0.010095
2022-01-10 03:39:10,972 iteration 3394 : loss : 0.021217, loss_ce: 0.008537
2022-01-10 03:39:12,318 iteration 3395 : loss : 0.017032, loss_ce: 0.006913
2022-01-10 03:39:13,778 iteration 3396 : loss : 0.027366, loss_ce: 0.008565
2022-01-10 03:39:15,212 iteration 3397 : loss : 0.022603, loss_ce: 0.008888
2022-01-10 03:39:16,655 iteration 3398 : loss : 0.033193, loss_ce: 0.014040
2022-01-10 03:39:18,053 iteration 3399 : loss : 0.030088, loss_ce: 0.010763
2022-01-10 03:39:18,053 Training Data Eval:
2022-01-10 03:39:25,099   Average segmentation loss on training set: 0.0165
2022-01-10 03:39:25,100 Validation Data Eval:
2022-01-10 03:39:27,521   Average segmentation loss on validation set: 0.0766
2022-01-10 03:39:28,862 iteration 3400 : loss : 0.026524, loss_ce: 0.007961
 50%|█████████████▌             | 200/400 [1:27:35<1:31:44, 27.52s/it]2022-01-10 03:39:30,342 iteration 3401 : loss : 0.035691, loss_ce: 0.022790
2022-01-10 03:39:31,684 iteration 3402 : loss : 0.022809, loss_ce: 0.008757
2022-01-10 03:39:33,176 iteration 3403 : loss : 0.033139, loss_ce: 0.011994
2022-01-10 03:39:34,639 iteration 3404 : loss : 0.022951, loss_ce: 0.008508
2022-01-10 03:39:35,994 iteration 3405 : loss : 0.024389, loss_ce: 0.008847
2022-01-10 03:39:37,470 iteration 3406 : loss : 0.020649, loss_ce: 0.007740
2022-01-10 03:39:38,863 iteration 3407 : loss : 0.023996, loss_ce: 0.009223
2022-01-10 03:39:40,200 iteration 3408 : loss : 0.025126, loss_ce: 0.009831
2022-01-10 03:39:41,608 iteration 3409 : loss : 0.019418, loss_ce: 0.006336
2022-01-10 03:39:43,046 iteration 3410 : loss : 0.024745, loss_ce: 0.012267
2022-01-10 03:39:44,502 iteration 3411 : loss : 0.041038, loss_ce: 0.018052
2022-01-10 03:39:45,938 iteration 3412 : loss : 0.021445, loss_ce: 0.009113
2022-01-10 03:39:47,355 iteration 3413 : loss : 0.034308, loss_ce: 0.008683
2022-01-10 03:39:48,829 iteration 3414 : loss : 0.025833, loss_ce: 0.011012
2022-01-10 03:39:50,257 iteration 3415 : loss : 0.022449, loss_ce: 0.010263
2022-01-10 03:39:51,747 iteration 3416 : loss : 0.021096, loss_ce: 0.006752
2022-01-10 03:39:53,225 iteration 3417 : loss : 0.036279, loss_ce: 0.013925
 50%|█████████████▌             | 201/400 [1:27:59<1:28:07, 26.57s/it]2022-01-10 03:39:54,636 iteration 3418 : loss : 0.014262, loss_ce: 0.004843
2022-01-10 03:39:55,955 iteration 3419 : loss : 0.020269, loss_ce: 0.009018
2022-01-10 03:39:57,322 iteration 3420 : loss : 0.019997, loss_ce: 0.006699
2022-01-10 03:39:58,671 iteration 3421 : loss : 0.019176, loss_ce: 0.007420
2022-01-10 03:40:00,009 iteration 3422 : loss : 0.024000, loss_ce: 0.007955
2022-01-10 03:40:01,434 iteration 3423 : loss : 0.044006, loss_ce: 0.026518
2022-01-10 03:40:02,850 iteration 3424 : loss : 0.036474, loss_ce: 0.016081
2022-01-10 03:40:04,279 iteration 3425 : loss : 0.022464, loss_ce: 0.008176
2022-01-10 03:40:05,593 iteration 3426 : loss : 0.023502, loss_ce: 0.008535
2022-01-10 03:40:07,002 iteration 3427 : loss : 0.030030, loss_ce: 0.009641
2022-01-10 03:40:08,431 iteration 3428 : loss : 0.023970, loss_ce: 0.009394
2022-01-10 03:40:09,803 iteration 3429 : loss : 0.020945, loss_ce: 0.007002
2022-01-10 03:40:11,243 iteration 3430 : loss : 0.020591, loss_ce: 0.007752
2022-01-10 03:40:12,665 iteration 3431 : loss : 0.028341, loss_ce: 0.009684
2022-01-10 03:40:14,148 iteration 3432 : loss : 0.036083, loss_ce: 0.013524
2022-01-10 03:40:15,614 iteration 3433 : loss : 0.036299, loss_ce: 0.011704
2022-01-10 03:40:17,028 iteration 3434 : loss : 0.026116, loss_ce: 0.007302
 50%|█████████████▋             | 202/400 [1:28:23<1:24:56, 25.74s/it]2022-01-10 03:40:18,483 iteration 3435 : loss : 0.019158, loss_ce: 0.006690
2022-01-10 03:40:19,949 iteration 3436 : loss : 0.035723, loss_ce: 0.014517
2022-01-10 03:40:21,429 iteration 3437 : loss : 0.026565, loss_ce: 0.009525
2022-01-10 03:40:22,910 iteration 3438 : loss : 0.033387, loss_ce: 0.015729
2022-01-10 03:40:24,275 iteration 3439 : loss : 0.032755, loss_ce: 0.011685
2022-01-10 03:40:25,711 iteration 3440 : loss : 0.021019, loss_ce: 0.007814
2022-01-10 03:40:27,097 iteration 3441 : loss : 0.031462, loss_ce: 0.010078
2022-01-10 03:40:28,517 iteration 3442 : loss : 0.020881, loss_ce: 0.008563
2022-01-10 03:40:30,018 iteration 3443 : loss : 0.035664, loss_ce: 0.015030
2022-01-10 03:40:31,409 iteration 3444 : loss : 0.027724, loss_ce: 0.008973
2022-01-10 03:40:32,843 iteration 3445 : loss : 0.033022, loss_ce: 0.012361
2022-01-10 03:40:34,308 iteration 3446 : loss : 0.025167, loss_ce: 0.011303
2022-01-10 03:40:35,768 iteration 3447 : loss : 0.030942, loss_ce: 0.008964
2022-01-10 03:40:37,120 iteration 3448 : loss : 0.024260, loss_ce: 0.008455
2022-01-10 03:40:38,487 iteration 3449 : loss : 0.023462, loss_ce: 0.009458
2022-01-10 03:40:39,876 iteration 3450 : loss : 0.022781, loss_ce: 0.007206
2022-01-10 03:40:41,266 iteration 3451 : loss : 0.018304, loss_ce: 0.009502
 51%|█████████████▋             | 203/400 [1:28:47<1:23:02, 25.29s/it]2022-01-10 03:40:42,644 iteration 3452 : loss : 0.018106, loss_ce: 0.007267
2022-01-10 03:40:44,094 iteration 3453 : loss : 0.032230, loss_ce: 0.012836
2022-01-10 03:40:45,642 iteration 3454 : loss : 0.035705, loss_ce: 0.013693
2022-01-10 03:40:47,034 iteration 3455 : loss : 0.025916, loss_ce: 0.010827
2022-01-10 03:40:48,470 iteration 3456 : loss : 0.043743, loss_ce: 0.011600
2022-01-10 03:40:49,835 iteration 3457 : loss : 0.023950, loss_ce: 0.010529
2022-01-10 03:40:51,181 iteration 3458 : loss : 0.031704, loss_ce: 0.008031
2022-01-10 03:40:52,643 iteration 3459 : loss : 0.025339, loss_ce: 0.011777
2022-01-10 03:40:53,996 iteration 3460 : loss : 0.018681, loss_ce: 0.007365
2022-01-10 03:40:55,355 iteration 3461 : loss : 0.022348, loss_ce: 0.008823
2022-01-10 03:40:56,651 iteration 3462 : loss : 0.020010, loss_ce: 0.009072
2022-01-10 03:40:58,088 iteration 3463 : loss : 0.027220, loss_ce: 0.011404
2022-01-10 03:40:59,513 iteration 3464 : loss : 0.027002, loss_ce: 0.009021
2022-01-10 03:41:00,886 iteration 3465 : loss : 0.022587, loss_ce: 0.008503
2022-01-10 03:41:02,378 iteration 3466 : loss : 0.037557, loss_ce: 0.014581
2022-01-10 03:41:03,717 iteration 3467 : loss : 0.017206, loss_ce: 0.006706
2022-01-10 03:41:05,143 iteration 3468 : loss : 0.018315, loss_ce: 0.005038
 51%|█████████████▊             | 204/400 [1:29:11<1:21:13, 24.87s/it]2022-01-10 03:41:06,580 iteration 3469 : loss : 0.023981, loss_ce: 0.010699
2022-01-10 03:41:07,973 iteration 3470 : loss : 0.035874, loss_ce: 0.014799
2022-01-10 03:41:09,442 iteration 3471 : loss : 0.027108, loss_ce: 0.011366
2022-01-10 03:41:10,933 iteration 3472 : loss : 0.023852, loss_ce: 0.007436
2022-01-10 03:41:12,297 iteration 3473 : loss : 0.018609, loss_ce: 0.007706
2022-01-10 03:41:13,760 iteration 3474 : loss : 0.029299, loss_ce: 0.013185
2022-01-10 03:41:15,179 iteration 3475 : loss : 0.017888, loss_ce: 0.007745
2022-01-10 03:41:16,561 iteration 3476 : loss : 0.026804, loss_ce: 0.008606
2022-01-10 03:41:17,955 iteration 3477 : loss : 0.017618, loss_ce: 0.005951
2022-01-10 03:41:19,371 iteration 3478 : loss : 0.022762, loss_ce: 0.010817
2022-01-10 03:41:20,771 iteration 3479 : loss : 0.017625, loss_ce: 0.007339
2022-01-10 03:41:22,131 iteration 3480 : loss : 0.020335, loss_ce: 0.009644
2022-01-10 03:41:23,502 iteration 3481 : loss : 0.025558, loss_ce: 0.007965
2022-01-10 03:41:24,893 iteration 3482 : loss : 0.022176, loss_ce: 0.008757
2022-01-10 03:41:26,365 iteration 3483 : loss : 0.026293, loss_ce: 0.012050
2022-01-10 03:41:27,796 iteration 3484 : loss : 0.023579, loss_ce: 0.007880
2022-01-10 03:41:27,796 Training Data Eval:
2022-01-10 03:41:35,003   Average segmentation loss on training set: 0.0148
2022-01-10 03:41:35,003 Validation Data Eval:
2022-01-10 03:41:37,485   Average segmentation loss on validation set: 0.0839
2022-01-10 03:41:38,888 iteration 3485 : loss : 0.022449, loss_ce: 0.009110
 51%|█████████████▊             | 205/400 [1:29:45<1:29:28, 27.53s/it]2022-01-10 03:41:40,379 iteration 3486 : loss : 0.024562, loss_ce: 0.006793
2022-01-10 03:41:41,782 iteration 3487 : loss : 0.019864, loss_ce: 0.008867
2022-01-10 03:41:43,186 iteration 3488 : loss : 0.027981, loss_ce: 0.008059
2022-01-10 03:41:44,486 iteration 3489 : loss : 0.020201, loss_ce: 0.008864
2022-01-10 03:41:45,926 iteration 3490 : loss : 0.041726, loss_ce: 0.016178
2022-01-10 03:41:47,438 iteration 3491 : loss : 0.030217, loss_ce: 0.014288
2022-01-10 03:41:48,863 iteration 3492 : loss : 0.027040, loss_ce: 0.007795
2022-01-10 03:41:50,253 iteration 3493 : loss : 0.033684, loss_ce: 0.012078
2022-01-10 03:41:51,671 iteration 3494 : loss : 0.023590, loss_ce: 0.007977
2022-01-10 03:41:53,114 iteration 3495 : loss : 0.021448, loss_ce: 0.006064
2022-01-10 03:41:54,634 iteration 3496 : loss : 0.030747, loss_ce: 0.011548
2022-01-10 03:41:56,044 iteration 3497 : loss : 0.023291, loss_ce: 0.010157
2022-01-10 03:41:57,499 iteration 3498 : loss : 0.026646, loss_ce: 0.012041
2022-01-10 03:41:58,886 iteration 3499 : loss : 0.023215, loss_ce: 0.012834
2022-01-10 03:42:00,300 iteration 3500 : loss : 0.022150, loss_ce: 0.009375
2022-01-10 03:42:01,797 iteration 3501 : loss : 0.022724, loss_ce: 0.008932
2022-01-10 03:42:03,205 iteration 3502 : loss : 0.020011, loss_ce: 0.010408
 52%|█████████████▉             | 206/400 [1:30:09<1:25:53, 26.57s/it]2022-01-10 03:42:04,698 iteration 3503 : loss : 0.031678, loss_ce: 0.008547
2022-01-10 03:42:06,103 iteration 3504 : loss : 0.017918, loss_ce: 0.006137
2022-01-10 03:42:07,509 iteration 3505 : loss : 0.019346, loss_ce: 0.009220
2022-01-10 03:42:08,968 iteration 3506 : loss : 0.029498, loss_ce: 0.011156
2022-01-10 03:42:10,326 iteration 3507 : loss : 0.016829, loss_ce: 0.006444
2022-01-10 03:42:11,728 iteration 3508 : loss : 0.025675, loss_ce: 0.006958
2022-01-10 03:42:13,182 iteration 3509 : loss : 0.028181, loss_ce: 0.011000
2022-01-10 03:42:14,587 iteration 3510 : loss : 0.025426, loss_ce: 0.010345
2022-01-10 03:42:16,189 iteration 3511 : loss : 0.042520, loss_ce: 0.019377
2022-01-10 03:42:17,551 iteration 3512 : loss : 0.017620, loss_ce: 0.006218
2022-01-10 03:42:18,887 iteration 3513 : loss : 0.019136, loss_ce: 0.008341
2022-01-10 03:42:20,286 iteration 3514 : loss : 0.023252, loss_ce: 0.011002
2022-01-10 03:42:21,677 iteration 3515 : loss : 0.025527, loss_ce: 0.012048
2022-01-10 03:42:23,061 iteration 3516 : loss : 0.016848, loss_ce: 0.007566
2022-01-10 03:42:24,463 iteration 3517 : loss : 0.022359, loss_ce: 0.009792
2022-01-10 03:42:25,952 iteration 3518 : loss : 0.025286, loss_ce: 0.007498
2022-01-10 03:42:27,406 iteration 3519 : loss : 0.031608, loss_ce: 0.014034
 52%|█████████████▉             | 207/400 [1:30:33<1:23:10, 25.86s/it]2022-01-10 03:42:28,806 iteration 3520 : loss : 0.018901, loss_ce: 0.007358
2022-01-10 03:42:30,194 iteration 3521 : loss : 0.033216, loss_ce: 0.007891
2022-01-10 03:42:31,549 iteration 3522 : loss : 0.022395, loss_ce: 0.006435
2022-01-10 03:42:32,983 iteration 3523 : loss : 0.019394, loss_ce: 0.009217
2022-01-10 03:42:34,469 iteration 3524 : loss : 0.021024, loss_ce: 0.008694
2022-01-10 03:42:35,847 iteration 3525 : loss : 0.024222, loss_ce: 0.007389
2022-01-10 03:42:37,274 iteration 3526 : loss : 0.018391, loss_ce: 0.008856
2022-01-10 03:42:38,770 iteration 3527 : loss : 0.028363, loss_ce: 0.011801
2022-01-10 03:42:40,159 iteration 3528 : loss : 0.038245, loss_ce: 0.010666
2022-01-10 03:42:41,559 iteration 3529 : loss : 0.022781, loss_ce: 0.007727
2022-01-10 03:42:42,959 iteration 3530 : loss : 0.019993, loss_ce: 0.007087
2022-01-10 03:42:44,366 iteration 3531 : loss : 0.026140, loss_ce: 0.009628
2022-01-10 03:42:45,789 iteration 3532 : loss : 0.025003, loss_ce: 0.013011
2022-01-10 03:42:47,238 iteration 3533 : loss : 0.031588, loss_ce: 0.012190
2022-01-10 03:42:48,586 iteration 3534 : loss : 0.027571, loss_ce: 0.010059
2022-01-10 03:42:50,007 iteration 3535 : loss : 0.025316, loss_ce: 0.009769
2022-01-10 03:42:51,397 iteration 3536 : loss : 0.024903, loss_ce: 0.010260
 52%|██████████████             | 208/400 [1:30:57<1:20:56, 25.29s/it]2022-01-10 03:42:52,939 iteration 3537 : loss : 0.071328, loss_ce: 0.038753
2022-01-10 03:42:54,324 iteration 3538 : loss : 0.020391, loss_ce: 0.006869
2022-01-10 03:42:55,701 iteration 3539 : loss : 0.031569, loss_ce: 0.009637
2022-01-10 03:42:57,138 iteration 3540 : loss : 0.035260, loss_ce: 0.014656
2022-01-10 03:42:58,595 iteration 3541 : loss : 0.036153, loss_ce: 0.014306
2022-01-10 03:43:00,033 iteration 3542 : loss : 0.026284, loss_ce: 0.010425
2022-01-10 03:43:01,518 iteration 3543 : loss : 0.033865, loss_ce: 0.014410
2022-01-10 03:43:02,985 iteration 3544 : loss : 0.067596, loss_ce: 0.014246
2022-01-10 03:43:04,467 iteration 3545 : loss : 0.024238, loss_ce: 0.007354
2022-01-10 03:43:05,930 iteration 3546 : loss : 0.022804, loss_ce: 0.008293
2022-01-10 03:43:07,413 iteration 3547 : loss : 0.037737, loss_ce: 0.017432
2022-01-10 03:43:08,784 iteration 3548 : loss : 0.014458, loss_ce: 0.004217
2022-01-10 03:43:10,286 iteration 3549 : loss : 0.045861, loss_ce: 0.025055
2022-01-10 03:43:11,680 iteration 3550 : loss : 0.024818, loss_ce: 0.007826
2022-01-10 03:43:13,055 iteration 3551 : loss : 0.024825, loss_ce: 0.010064
2022-01-10 03:43:14,417 iteration 3552 : loss : 0.030967, loss_ce: 0.014942
2022-01-10 03:43:15,794 iteration 3553 : loss : 0.020627, loss_ce: 0.007231
 52%|██████████████             | 209/400 [1:31:22<1:19:40, 25.03s/it]2022-01-10 03:43:17,256 iteration 3554 : loss : 0.023907, loss_ce: 0.009763
2022-01-10 03:43:18,652 iteration 3555 : loss : 0.025532, loss_ce: 0.011164
2022-01-10 03:43:20,125 iteration 3556 : loss : 0.033936, loss_ce: 0.018358
2022-01-10 03:43:21,616 iteration 3557 : loss : 0.030428, loss_ce: 0.011870
2022-01-10 03:43:23,018 iteration 3558 : loss : 0.025740, loss_ce: 0.009147
2022-01-10 03:43:24,441 iteration 3559 : loss : 0.029862, loss_ce: 0.007933
2022-01-10 03:43:25,841 iteration 3560 : loss : 0.033473, loss_ce: 0.013502
2022-01-10 03:43:27,174 iteration 3561 : loss : 0.015927, loss_ce: 0.007707
2022-01-10 03:43:28,598 iteration 3562 : loss : 0.034492, loss_ce: 0.013004
2022-01-10 03:43:30,010 iteration 3563 : loss : 0.022907, loss_ce: 0.008208
2022-01-10 03:43:31,466 iteration 3564 : loss : 0.042561, loss_ce: 0.013344
2022-01-10 03:43:32,827 iteration 3565 : loss : 0.032525, loss_ce: 0.008436
2022-01-10 03:43:34,188 iteration 3566 : loss : 0.027058, loss_ce: 0.010933
2022-01-10 03:43:35,670 iteration 3567 : loss : 0.038736, loss_ce: 0.022988
2022-01-10 03:43:37,073 iteration 3568 : loss : 0.032589, loss_ce: 0.012465
2022-01-10 03:43:38,439 iteration 3569 : loss : 0.029041, loss_ce: 0.014252
2022-01-10 03:43:38,439 Training Data Eval:
2022-01-10 03:43:45,494   Average segmentation loss on training set: 0.0206
2022-01-10 03:43:45,494 Validation Data Eval:
2022-01-10 03:43:47,932   Average segmentation loss on validation set: 0.1309
2022-01-10 03:43:49,376 iteration 3570 : loss : 0.026881, loss_ce: 0.007411
 52%|██████████████▏            | 210/400 [1:31:55<1:27:23, 27.60s/it]2022-01-10 03:43:50,869 iteration 3571 : loss : 0.030753, loss_ce: 0.016706
2022-01-10 03:43:52,219 iteration 3572 : loss : 0.018724, loss_ce: 0.006878
2022-01-10 03:43:53,645 iteration 3573 : loss : 0.026130, loss_ce: 0.008337
2022-01-10 03:43:55,050 iteration 3574 : loss : 0.022207, loss_ce: 0.006389
2022-01-10 03:43:56,404 iteration 3575 : loss : 0.017833, loss_ce: 0.006127
2022-01-10 03:43:57,856 iteration 3576 : loss : 0.021024, loss_ce: 0.005610
2022-01-10 03:43:59,280 iteration 3577 : loss : 0.022582, loss_ce: 0.009610
2022-01-10 03:44:00,597 iteration 3578 : loss : 0.023197, loss_ce: 0.008362
2022-01-10 03:44:01,972 iteration 3579 : loss : 0.026205, loss_ce: 0.011533
2022-01-10 03:44:03,332 iteration 3580 : loss : 0.022576, loss_ce: 0.008908
2022-01-10 03:44:04,682 iteration 3581 : loss : 0.030628, loss_ce: 0.016981
2022-01-10 03:44:06,074 iteration 3582 : loss : 0.020251, loss_ce: 0.007296
2022-01-10 03:44:07,510 iteration 3583 : loss : 0.027782, loss_ce: 0.010539
2022-01-10 03:44:08,970 iteration 3584 : loss : 0.021382, loss_ce: 0.007978
2022-01-10 03:44:10,330 iteration 3585 : loss : 0.020968, loss_ce: 0.007093
2022-01-10 03:44:11,798 iteration 3586 : loss : 0.030524, loss_ce: 0.012911
2022-01-10 03:44:13,235 iteration 3587 : loss : 0.017001, loss_ce: 0.007044
 53%|██████████████▏            | 211/400 [1:32:19<1:23:23, 26.48s/it]2022-01-10 03:44:14,762 iteration 3588 : loss : 0.030309, loss_ce: 0.013123
2022-01-10 03:44:16,103 iteration 3589 : loss : 0.017739, loss_ce: 0.006613
2022-01-10 03:44:17,476 iteration 3590 : loss : 0.020303, loss_ce: 0.007967
2022-01-10 03:44:18,880 iteration 3591 : loss : 0.021318, loss_ce: 0.008160
2022-01-10 03:44:20,446 iteration 3592 : loss : 0.027483, loss_ce: 0.010071
2022-01-10 03:44:21,834 iteration 3593 : loss : 0.020683, loss_ce: 0.007100
2022-01-10 03:44:23,232 iteration 3594 : loss : 0.024324, loss_ce: 0.010226
2022-01-10 03:44:24,663 iteration 3595 : loss : 0.030547, loss_ce: 0.011914
2022-01-10 03:44:26,071 iteration 3596 : loss : 0.017473, loss_ce: 0.006146
2022-01-10 03:44:27,516 iteration 3597 : loss : 0.035345, loss_ce: 0.017117
2022-01-10 03:44:28,868 iteration 3598 : loss : 0.014508, loss_ce: 0.005922
2022-01-10 03:44:30,317 iteration 3599 : loss : 0.023805, loss_ce: 0.007949
2022-01-10 03:44:31,617 iteration 3600 : loss : 0.017430, loss_ce: 0.006975
2022-01-10 03:44:33,019 iteration 3601 : loss : 0.024929, loss_ce: 0.011076
2022-01-10 03:44:34,367 iteration 3602 : loss : 0.017953, loss_ce: 0.005883
2022-01-10 03:44:35,792 iteration 3603 : loss : 0.026926, loss_ce: 0.012737
2022-01-10 03:44:37,132 iteration 3604 : loss : 0.020505, loss_ce: 0.009395
 53%|██████████████▎            | 212/400 [1:32:43<1:20:32, 25.70s/it]2022-01-10 03:44:38,601 iteration 3605 : loss : 0.020423, loss_ce: 0.007705
2022-01-10 03:44:39,984 iteration 3606 : loss : 0.029034, loss_ce: 0.014871
2022-01-10 03:44:41,412 iteration 3607 : loss : 0.028749, loss_ce: 0.011480
2022-01-10 03:44:42,795 iteration 3608 : loss : 0.020279, loss_ce: 0.007842
2022-01-10 03:44:44,169 iteration 3609 : loss : 0.020670, loss_ce: 0.009337
2022-01-10 03:44:45,580 iteration 3610 : loss : 0.020484, loss_ce: 0.008460
2022-01-10 03:44:46,915 iteration 3611 : loss : 0.025728, loss_ce: 0.007769
2022-01-10 03:44:48,257 iteration 3612 : loss : 0.018151, loss_ce: 0.006968
2022-01-10 03:44:49,750 iteration 3613 : loss : 0.030153, loss_ce: 0.011910
2022-01-10 03:44:51,105 iteration 3614 : loss : 0.017885, loss_ce: 0.007091
2022-01-10 03:44:52,417 iteration 3615 : loss : 0.017361, loss_ce: 0.006008
2022-01-10 03:44:53,833 iteration 3616 : loss : 0.026930, loss_ce: 0.011105
2022-01-10 03:44:55,270 iteration 3617 : loss : 0.034568, loss_ce: 0.013492
2022-01-10 03:44:56,687 iteration 3618 : loss : 0.020460, loss_ce: 0.009059
2022-01-10 03:44:58,092 iteration 3619 : loss : 0.027217, loss_ce: 0.007686
2022-01-10 03:44:59,454 iteration 3620 : loss : 0.027255, loss_ce: 0.008870
2022-01-10 03:45:00,868 iteration 3621 : loss : 0.021020, loss_ce: 0.007415
 53%|██████████████▍            | 213/400 [1:33:07<1:18:15, 25.11s/it]2022-01-10 03:45:02,336 iteration 3622 : loss : 0.019280, loss_ce: 0.008536
2022-01-10 03:45:03,794 iteration 3623 : loss : 0.022212, loss_ce: 0.005595
2022-01-10 03:45:05,195 iteration 3624 : loss : 0.014481, loss_ce: 0.005826
2022-01-10 03:45:06,585 iteration 3625 : loss : 0.025085, loss_ce: 0.008344
2022-01-10 03:45:07,942 iteration 3626 : loss : 0.019294, loss_ce: 0.008968
2022-01-10 03:45:09,286 iteration 3627 : loss : 0.024233, loss_ce: 0.008479
2022-01-10 03:45:10,738 iteration 3628 : loss : 0.036443, loss_ce: 0.011391
2022-01-10 03:45:12,289 iteration 3629 : loss : 0.025211, loss_ce: 0.010495
2022-01-10 03:45:13,611 iteration 3630 : loss : 0.023887, loss_ce: 0.009164
2022-01-10 03:45:15,017 iteration 3631 : loss : 0.033800, loss_ce: 0.011292
2022-01-10 03:45:16,538 iteration 3632 : loss : 0.026838, loss_ce: 0.008594
2022-01-10 03:45:17,940 iteration 3633 : loss : 0.017298, loss_ce: 0.006487
2022-01-10 03:45:19,327 iteration 3634 : loss : 0.020365, loss_ce: 0.008157
2022-01-10 03:45:20,818 iteration 3635 : loss : 0.039463, loss_ce: 0.019257
2022-01-10 03:45:22,281 iteration 3636 : loss : 0.030006, loss_ce: 0.008873
2022-01-10 03:45:23,691 iteration 3637 : loss : 0.033928, loss_ce: 0.012585
2022-01-10 03:45:25,090 iteration 3638 : loss : 0.027573, loss_ce: 0.014305
 54%|██████████████▍            | 214/400 [1:33:31<1:17:00, 24.84s/it]2022-01-10 03:45:26,498 iteration 3639 : loss : 0.019160, loss_ce: 0.008848
2022-01-10 03:45:27,847 iteration 3640 : loss : 0.019859, loss_ce: 0.007552
2022-01-10 03:45:29,189 iteration 3641 : loss : 0.018039, loss_ce: 0.005873
2022-01-10 03:45:30,595 iteration 3642 : loss : 0.023353, loss_ce: 0.006549
2022-01-10 03:45:31,940 iteration 3643 : loss : 0.019164, loss_ce: 0.008055
2022-01-10 03:45:33,306 iteration 3644 : loss : 0.013397, loss_ce: 0.004895
2022-01-10 03:45:34,705 iteration 3645 : loss : 0.021639, loss_ce: 0.006649
2022-01-10 03:45:36,105 iteration 3646 : loss : 0.022742, loss_ce: 0.009188
2022-01-10 03:45:37,511 iteration 3647 : loss : 0.019730, loss_ce: 0.005611
2022-01-10 03:45:38,939 iteration 3648 : loss : 0.035021, loss_ce: 0.013157
2022-01-10 03:45:40,325 iteration 3649 : loss : 0.018889, loss_ce: 0.005353
2022-01-10 03:45:41,737 iteration 3650 : loss : 0.021661, loss_ce: 0.009554
2022-01-10 03:45:43,163 iteration 3651 : loss : 0.030117, loss_ce: 0.010035
2022-01-10 03:45:44,694 iteration 3652 : loss : 0.041945, loss_ce: 0.011586
2022-01-10 03:45:46,056 iteration 3653 : loss : 0.024652, loss_ce: 0.014582
2022-01-10 03:45:47,395 iteration 3654 : loss : 0.017689, loss_ce: 0.008854
2022-01-10 03:45:47,395 Training Data Eval:
2022-01-10 03:45:54,435   Average segmentation loss on training set: 0.0151
2022-01-10 03:45:54,436 Validation Data Eval:
2022-01-10 03:45:56,869   Average segmentation loss on validation set: 0.0993
2022-01-10 03:45:58,290 iteration 3655 : loss : 0.018907, loss_ce: 0.007212
 54%|██████████████▌            | 215/400 [1:34:04<1:24:20, 27.35s/it]2022-01-10 03:45:59,799 iteration 3656 : loss : 0.024952, loss_ce: 0.008798
2022-01-10 03:46:01,160 iteration 3657 : loss : 0.020344, loss_ce: 0.010497
2022-01-10 03:46:02,620 iteration 3658 : loss : 0.023675, loss_ce: 0.011148
2022-01-10 03:46:04,133 iteration 3659 : loss : 0.029732, loss_ce: 0.010251
2022-01-10 03:46:05,546 iteration 3660 : loss : 0.022250, loss_ce: 0.005316
2022-01-10 03:46:06,939 iteration 3661 : loss : 0.018074, loss_ce: 0.008154
2022-01-10 03:46:08,348 iteration 3662 : loss : 0.020641, loss_ce: 0.007383
2022-01-10 03:46:09,736 iteration 3663 : loss : 0.024842, loss_ce: 0.008246
2022-01-10 03:46:11,172 iteration 3664 : loss : 0.017338, loss_ce: 0.006114
2022-01-10 03:46:12,571 iteration 3665 : loss : 0.025386, loss_ce: 0.013432
2022-01-10 03:46:13,963 iteration 3666 : loss : 0.024750, loss_ce: 0.007653
2022-01-10 03:46:15,344 iteration 3667 : loss : 0.025550, loss_ce: 0.012244
2022-01-10 03:46:16,666 iteration 3668 : loss : 0.019874, loss_ce: 0.004884
2022-01-10 03:46:18,119 iteration 3669 : loss : 0.022417, loss_ce: 0.009462
2022-01-10 03:46:19,467 iteration 3670 : loss : 0.020526, loss_ce: 0.007268
2022-01-10 03:46:20,849 iteration 3671 : loss : 0.017504, loss_ce: 0.008070
2022-01-10 03:46:22,391 iteration 3672 : loss : 0.052734, loss_ce: 0.013675
 54%|██████████████▌            | 216/400 [1:34:28<1:20:53, 26.38s/it]2022-01-10 03:46:23,884 iteration 3673 : loss : 0.027507, loss_ce: 0.011097
2022-01-10 03:46:25,348 iteration 3674 : loss : 0.028774, loss_ce: 0.007733
2022-01-10 03:46:26,843 iteration 3675 : loss : 0.025770, loss_ce: 0.009108
2022-01-10 03:46:28,177 iteration 3676 : loss : 0.016393, loss_ce: 0.006161
2022-01-10 03:46:29,592 iteration 3677 : loss : 0.034053, loss_ce: 0.009380
2022-01-10 03:46:31,064 iteration 3678 : loss : 0.052547, loss_ce: 0.017538
2022-01-10 03:46:32,461 iteration 3679 : loss : 0.018609, loss_ce: 0.006743
2022-01-10 03:46:33,871 iteration 3680 : loss : 0.028258, loss_ce: 0.009883
2022-01-10 03:46:35,264 iteration 3681 : loss : 0.019031, loss_ce: 0.007605
2022-01-10 03:46:36,588 iteration 3682 : loss : 0.018898, loss_ce: 0.007183
2022-01-10 03:46:37,909 iteration 3683 : loss : 0.022467, loss_ce: 0.007177
2022-01-10 03:46:39,361 iteration 3684 : loss : 0.019938, loss_ce: 0.007256
2022-01-10 03:46:40,726 iteration 3685 : loss : 0.023228, loss_ce: 0.006466
2022-01-10 03:46:42,144 iteration 3686 : loss : 0.024592, loss_ce: 0.009645
2022-01-10 03:46:43,515 iteration 3687 : loss : 0.019039, loss_ce: 0.006310
2022-01-10 03:46:44,909 iteration 3688 : loss : 0.022116, loss_ce: 0.010608
2022-01-10 03:46:46,325 iteration 3689 : loss : 0.030394, loss_ce: 0.008374
 54%|██████████████▋            | 217/400 [1:34:52<1:18:12, 25.64s/it]2022-01-10 03:46:47,826 iteration 3690 : loss : 0.026521, loss_ce: 0.006044
2022-01-10 03:46:49,258 iteration 3691 : loss : 0.035069, loss_ce: 0.009319
2022-01-10 03:46:50,682 iteration 3692 : loss : 0.025669, loss_ce: 0.011588
2022-01-10 03:46:52,033 iteration 3693 : loss : 0.021004, loss_ce: 0.008183
2022-01-10 03:46:53,434 iteration 3694 : loss : 0.042951, loss_ce: 0.017428
2022-01-10 03:46:54,848 iteration 3695 : loss : 0.022978, loss_ce: 0.011705
2022-01-10 03:46:56,237 iteration 3696 : loss : 0.038669, loss_ce: 0.013101
2022-01-10 03:46:57,670 iteration 3697 : loss : 0.050595, loss_ce: 0.013615
2022-01-10 03:46:59,159 iteration 3698 : loss : 0.024077, loss_ce: 0.011369
2022-01-10 03:47:00,518 iteration 3699 : loss : 0.018257, loss_ce: 0.007258
2022-01-10 03:47:01,930 iteration 3700 : loss : 0.024177, loss_ce: 0.007697
2022-01-10 03:47:03,248 iteration 3701 : loss : 0.019005, loss_ce: 0.008598
2022-01-10 03:47:04,666 iteration 3702 : loss : 0.027186, loss_ce: 0.009841
2022-01-10 03:47:06,077 iteration 3703 : loss : 0.025378, loss_ce: 0.010976
2022-01-10 03:47:07,397 iteration 3704 : loss : 0.029597, loss_ce: 0.010554
2022-01-10 03:47:08,864 iteration 3705 : loss : 0.028579, loss_ce: 0.009737
2022-01-10 03:47:10,190 iteration 3706 : loss : 0.026542, loss_ce: 0.008197
 55%|██████████████▋            | 218/400 [1:35:16<1:16:10, 25.11s/it]2022-01-10 03:47:11,700 iteration 3707 : loss : 0.029422, loss_ce: 0.013127
2022-01-10 03:47:13,027 iteration 3708 : loss : 0.027062, loss_ce: 0.010501
2022-01-10 03:47:14,523 iteration 3709 : loss : 0.031615, loss_ce: 0.008733
2022-01-10 03:47:15,926 iteration 3710 : loss : 0.029812, loss_ce: 0.009932
2022-01-10 03:47:17,293 iteration 3711 : loss : 0.019223, loss_ce: 0.006444
2022-01-10 03:47:18,636 iteration 3712 : loss : 0.017993, loss_ce: 0.008017
2022-01-10 03:47:20,120 iteration 3713 : loss : 0.022525, loss_ce: 0.008269
2022-01-10 03:47:21,529 iteration 3714 : loss : 0.024280, loss_ce: 0.009560
2022-01-10 03:47:22,907 iteration 3715 : loss : 0.016249, loss_ce: 0.006677
2022-01-10 03:47:24,302 iteration 3716 : loss : 0.022327, loss_ce: 0.011134
2022-01-10 03:47:25,746 iteration 3717 : loss : 0.021456, loss_ce: 0.008748
2022-01-10 03:47:27,125 iteration 3718 : loss : 0.019502, loss_ce: 0.007899
2022-01-10 03:47:28,536 iteration 3719 : loss : 0.025560, loss_ce: 0.009632
2022-01-10 03:47:30,028 iteration 3720 : loss : 0.032008, loss_ce: 0.011017
2022-01-10 03:47:31,407 iteration 3721 : loss : 0.018192, loss_ce: 0.006578
2022-01-10 03:47:32,791 iteration 3722 : loss : 0.031088, loss_ce: 0.013466
2022-01-10 03:47:34,245 iteration 3723 : loss : 0.057080, loss_ce: 0.019078
 55%|██████████████▊            | 219/400 [1:35:40<1:14:47, 24.80s/it]2022-01-10 03:47:35,757 iteration 3724 : loss : 0.027316, loss_ce: 0.011524
2022-01-10 03:47:37,159 iteration 3725 : loss : 0.033520, loss_ce: 0.013777
2022-01-10 03:47:38,683 iteration 3726 : loss : 0.031749, loss_ce: 0.009122
2022-01-10 03:47:40,043 iteration 3727 : loss : 0.013948, loss_ce: 0.004539
2022-01-10 03:47:41,443 iteration 3728 : loss : 0.033309, loss_ce: 0.017445
2022-01-10 03:47:42,899 iteration 3729 : loss : 0.033017, loss_ce: 0.010630
2022-01-10 03:47:44,278 iteration 3730 : loss : 0.029162, loss_ce: 0.007860
2022-01-10 03:47:45,622 iteration 3731 : loss : 0.022258, loss_ce: 0.007527
2022-01-10 03:47:47,025 iteration 3732 : loss : 0.022064, loss_ce: 0.008941
2022-01-10 03:47:48,409 iteration 3733 : loss : 0.026982, loss_ce: 0.008528
2022-01-10 03:47:49,822 iteration 3734 : loss : 0.025790, loss_ce: 0.009386
2022-01-10 03:47:51,251 iteration 3735 : loss : 0.023223, loss_ce: 0.011528
2022-01-10 03:47:52,700 iteration 3736 : loss : 0.044959, loss_ce: 0.012108
2022-01-10 03:47:54,115 iteration 3737 : loss : 0.033146, loss_ce: 0.013819
2022-01-10 03:47:55,517 iteration 3738 : loss : 0.020231, loss_ce: 0.008523
2022-01-10 03:47:56,918 iteration 3739 : loss : 0.027592, loss_ce: 0.011471
2022-01-10 03:47:56,918 Training Data Eval:
2022-01-10 03:48:03,959   Average segmentation loss on training set: 0.0555
2022-01-10 03:48:03,959 Validation Data Eval:
2022-01-10 03:48:06,402   Average segmentation loss on validation set: 0.2486
2022-01-10 03:48:07,850 iteration 3740 : loss : 0.020314, loss_ce: 0.007172
 55%|██████████████▊            | 220/400 [1:36:14<1:22:18, 27.44s/it]2022-01-10 03:48:09,327 iteration 3741 : loss : 0.032554, loss_ce: 0.011511
2022-01-10 03:48:10,698 iteration 3742 : loss : 0.025712, loss_ce: 0.011782
2022-01-10 03:48:12,083 iteration 3743 : loss : 0.027042, loss_ce: 0.009066
2022-01-10 03:48:13,497 iteration 3744 : loss : 0.025338, loss_ce: 0.011190
2022-01-10 03:48:14,854 iteration 3745 : loss : 0.031594, loss_ce: 0.012829
2022-01-10 03:48:16,295 iteration 3746 : loss : 0.021459, loss_ce: 0.008891
2022-01-10 03:48:17,740 iteration 3747 : loss : 0.072969, loss_ce: 0.012097
2022-01-10 03:48:19,151 iteration 3748 : loss : 0.058474, loss_ce: 0.035439
2022-01-10 03:48:20,546 iteration 3749 : loss : 0.050361, loss_ce: 0.030250
2022-01-10 03:48:21,964 iteration 3750 : loss : 0.028584, loss_ce: 0.011847
2022-01-10 03:48:23,395 iteration 3751 : loss : 0.048770, loss_ce: 0.013409
2022-01-10 03:48:24,803 iteration 3752 : loss : 0.032104, loss_ce: 0.013000
2022-01-10 03:48:26,159 iteration 3753 : loss : 0.031343, loss_ce: 0.008112
2022-01-10 03:48:27,501 iteration 3754 : loss : 0.018752, loss_ce: 0.006672
2022-01-10 03:48:28,872 iteration 3755 : loss : 0.024954, loss_ce: 0.011650
2022-01-10 03:48:30,274 iteration 3756 : loss : 0.040920, loss_ce: 0.016323
2022-01-10 03:48:31,679 iteration 3757 : loss : 0.029327, loss_ce: 0.010658
 55%|██████████████▉            | 221/400 [1:36:38<1:18:37, 26.35s/it]2022-01-10 03:48:33,165 iteration 3758 : loss : 0.028486, loss_ce: 0.010871
2022-01-10 03:48:34,567 iteration 3759 : loss : 0.026298, loss_ce: 0.009553
2022-01-10 03:48:35,881 iteration 3760 : loss : 0.019469, loss_ce: 0.006804
2022-01-10 03:48:37,330 iteration 3761 : loss : 0.040066, loss_ce: 0.019023
2022-01-10 03:48:38,735 iteration 3762 : loss : 0.032510, loss_ce: 0.008838
2022-01-10 03:48:40,195 iteration 3763 : loss : 0.032430, loss_ce: 0.012642
2022-01-10 03:48:41,643 iteration 3764 : loss : 0.025266, loss_ce: 0.010020
2022-01-10 03:48:43,051 iteration 3765 : loss : 0.023891, loss_ce: 0.009897
2022-01-10 03:48:44,448 iteration 3766 : loss : 0.036300, loss_ce: 0.011504
2022-01-10 03:48:45,919 iteration 3767 : loss : 0.031182, loss_ce: 0.009946
2022-01-10 03:48:47,335 iteration 3768 : loss : 0.028175, loss_ce: 0.012258
2022-01-10 03:48:48,713 iteration 3769 : loss : 0.023066, loss_ce: 0.010274
2022-01-10 03:48:50,060 iteration 3770 : loss : 0.020453, loss_ce: 0.007151
2022-01-10 03:48:51,425 iteration 3771 : loss : 0.021706, loss_ce: 0.008331
2022-01-10 03:48:52,813 iteration 3772 : loss : 0.026034, loss_ce: 0.009118
2022-01-10 03:48:54,148 iteration 3773 : loss : 0.017927, loss_ce: 0.006309
2022-01-10 03:48:55,500 iteration 3774 : loss : 0.037919, loss_ce: 0.017800
 56%|██████████████▉            | 222/400 [1:37:02<1:15:55, 25.59s/it]2022-01-10 03:48:56,923 iteration 3775 : loss : 0.027587, loss_ce: 0.006711
2022-01-10 03:48:58,363 iteration 3776 : loss : 0.027878, loss_ce: 0.012838
2022-01-10 03:48:59,884 iteration 3777 : loss : 0.033223, loss_ce: 0.011412
2022-01-10 03:49:01,264 iteration 3778 : loss : 0.019562, loss_ce: 0.010382
2022-01-10 03:49:02,700 iteration 3779 : loss : 0.019976, loss_ce: 0.006479
2022-01-10 03:49:04,083 iteration 3780 : loss : 0.019477, loss_ce: 0.009393
2022-01-10 03:49:05,476 iteration 3781 : loss : 0.021515, loss_ce: 0.008514
2022-01-10 03:49:06,901 iteration 3782 : loss : 0.040525, loss_ce: 0.014481
2022-01-10 03:49:08,347 iteration 3783 : loss : 0.018028, loss_ce: 0.005411
2022-01-10 03:49:09,803 iteration 3784 : loss : 0.032800, loss_ce: 0.016915
2022-01-10 03:49:11,172 iteration 3785 : loss : 0.022153, loss_ce: 0.008050
2022-01-10 03:49:12,584 iteration 3786 : loss : 0.025978, loss_ce: 0.009850
2022-01-10 03:49:14,055 iteration 3787 : loss : 0.040504, loss_ce: 0.017485
2022-01-10 03:49:15,531 iteration 3788 : loss : 0.033342, loss_ce: 0.013472
2022-01-10 03:49:16,908 iteration 3789 : loss : 0.024933, loss_ce: 0.007317
2022-01-10 03:49:18,335 iteration 3790 : loss : 0.033325, loss_ce: 0.014559
2022-01-10 03:49:19,653 iteration 3791 : loss : 0.016348, loss_ce: 0.006348
 56%|███████████████            | 223/400 [1:37:26<1:14:13, 25.16s/it]2022-01-10 03:49:21,103 iteration 3792 : loss : 0.017379, loss_ce: 0.007102
2022-01-10 03:49:22,514 iteration 3793 : loss : 0.021585, loss_ce: 0.009121
2022-01-10 03:49:23,868 iteration 3794 : loss : 0.019088, loss_ce: 0.007234
2022-01-10 03:49:25,372 iteration 3795 : loss : 0.032379, loss_ce: 0.013408
2022-01-10 03:49:26,812 iteration 3796 : loss : 0.047437, loss_ce: 0.013187
2022-01-10 03:49:28,193 iteration 3797 : loss : 0.023653, loss_ce: 0.008200
2022-01-10 03:49:29,686 iteration 3798 : loss : 0.027077, loss_ce: 0.011694
2022-01-10 03:49:31,051 iteration 3799 : loss : 0.020935, loss_ce: 0.012160
2022-01-10 03:49:32,501 iteration 3800 : loss : 0.022153, loss_ce: 0.007558
2022-01-10 03:49:33,854 iteration 3801 : loss : 0.018895, loss_ce: 0.006912
2022-01-10 03:49:35,258 iteration 3802 : loss : 0.019138, loss_ce: 0.005647
2022-01-10 03:49:36,654 iteration 3803 : loss : 0.020895, loss_ce: 0.007985
2022-01-10 03:49:37,965 iteration 3804 : loss : 0.019349, loss_ce: 0.007313
2022-01-10 03:49:39,397 iteration 3805 : loss : 0.024694, loss_ce: 0.010990
2022-01-10 03:49:40,765 iteration 3806 : loss : 0.022186, loss_ce: 0.006209
2022-01-10 03:49:42,148 iteration 3807 : loss : 0.021503, loss_ce: 0.009464
2022-01-10 03:49:43,569 iteration 3808 : loss : 0.026119, loss_ce: 0.009955
 56%|███████████████            | 224/400 [1:37:50<1:12:43, 24.79s/it]2022-01-10 03:49:45,037 iteration 3809 : loss : 0.023416, loss_ce: 0.008599
2022-01-10 03:49:46,436 iteration 3810 : loss : 0.019320, loss_ce: 0.006537
2022-01-10 03:49:47,949 iteration 3811 : loss : 0.022200, loss_ce: 0.006944
2022-01-10 03:49:49,328 iteration 3812 : loss : 0.020476, loss_ce: 0.007677
2022-01-10 03:49:50,747 iteration 3813 : loss : 0.050267, loss_ce: 0.015549
2022-01-10 03:49:52,075 iteration 3814 : loss : 0.016899, loss_ce: 0.005580
2022-01-10 03:49:53,533 iteration 3815 : loss : 0.023218, loss_ce: 0.007368
2022-01-10 03:49:55,046 iteration 3816 : loss : 0.028039, loss_ce: 0.008647
2022-01-10 03:49:56,469 iteration 3817 : loss : 0.019460, loss_ce: 0.008531
2022-01-10 03:49:57,983 iteration 3818 : loss : 0.026580, loss_ce: 0.011552
2022-01-10 03:49:59,346 iteration 3819 : loss : 0.022537, loss_ce: 0.011469
2022-01-10 03:50:00,795 iteration 3820 : loss : 0.025605, loss_ce: 0.010814
2022-01-10 03:50:02,199 iteration 3821 : loss : 0.022805, loss_ce: 0.011031
2022-01-10 03:50:03,512 iteration 3822 : loss : 0.017658, loss_ce: 0.008123
2022-01-10 03:50:04,871 iteration 3823 : loss : 0.022332, loss_ce: 0.007795
2022-01-10 03:50:06,257 iteration 3824 : loss : 0.028864, loss_ce: 0.010215
2022-01-10 03:50:06,258 Training Data Eval:
2022-01-10 03:50:13,337   Average segmentation loss on training set: 0.0144
2022-01-10 03:50:13,338 Validation Data Eval:
2022-01-10 03:50:15,922   Average segmentation loss on validation set: 0.0813
2022-01-10 03:50:17,335 iteration 3825 : loss : 0.033173, loss_ce: 0.012391
 56%|███████████████▏           | 225/400 [1:38:23<1:20:09, 27.48s/it]2022-01-10 03:50:18,734 iteration 3826 : loss : 0.018897, loss_ce: 0.008005
2022-01-10 03:50:20,157 iteration 3827 : loss : 0.020313, loss_ce: 0.008008
2022-01-10 03:50:21,595 iteration 3828 : loss : 0.025568, loss_ce: 0.009512
2022-01-10 03:50:22,995 iteration 3829 : loss : 0.023042, loss_ce: 0.010103
2022-01-10 03:50:24,401 iteration 3830 : loss : 0.020330, loss_ce: 0.008289
2022-01-10 03:50:25,872 iteration 3831 : loss : 0.036063, loss_ce: 0.008717
2022-01-10 03:50:27,201 iteration 3832 : loss : 0.015068, loss_ce: 0.005249
2022-01-10 03:50:28,616 iteration 3833 : loss : 0.018594, loss_ce: 0.006326
2022-01-10 03:50:29,997 iteration 3834 : loss : 0.018191, loss_ce: 0.008782
2022-01-10 03:50:31,448 iteration 3835 : loss : 0.020811, loss_ce: 0.010470
2022-01-10 03:50:32,913 iteration 3836 : loss : 0.025219, loss_ce: 0.007781
2022-01-10 03:50:34,246 iteration 3837 : loss : 0.017735, loss_ce: 0.006873
2022-01-10 03:50:35,609 iteration 3838 : loss : 0.019134, loss_ce: 0.006225
2022-01-10 03:50:36,975 iteration 3839 : loss : 0.025290, loss_ce: 0.012051
2022-01-10 03:50:38,287 iteration 3840 : loss : 0.020383, loss_ce: 0.005805
2022-01-10 03:50:39,651 iteration 3841 : loss : 0.022123, loss_ce: 0.007746
2022-01-10 03:50:41,022 iteration 3842 : loss : 0.022222, loss_ce: 0.005084
 56%|███████████████▎           | 226/400 [1:38:47<1:16:23, 26.34s/it]2022-01-10 03:50:42,505 iteration 3843 : loss : 0.016710, loss_ce: 0.006522
2022-01-10 03:50:43,916 iteration 3844 : loss : 0.024301, loss_ce: 0.012061
2022-01-10 03:50:45,289 iteration 3845 : loss : 0.028740, loss_ce: 0.009524
2022-01-10 03:50:46,677 iteration 3846 : loss : 0.018456, loss_ce: 0.004518
2022-01-10 03:50:48,120 iteration 3847 : loss : 0.016579, loss_ce: 0.006502
2022-01-10 03:50:49,501 iteration 3848 : loss : 0.016876, loss_ce: 0.006687
2022-01-10 03:50:50,834 iteration 3849 : loss : 0.016311, loss_ce: 0.006243
2022-01-10 03:50:52,262 iteration 3850 : loss : 0.038201, loss_ce: 0.008215
2022-01-10 03:50:53,710 iteration 3851 : loss : 0.054088, loss_ce: 0.033687
2022-01-10 03:50:55,085 iteration 3852 : loss : 0.021518, loss_ce: 0.008120
2022-01-10 03:50:56,451 iteration 3853 : loss : 0.017803, loss_ce: 0.009455
2022-01-10 03:50:57,891 iteration 3854 : loss : 0.031669, loss_ce: 0.011665
2022-01-10 03:50:59,299 iteration 3855 : loss : 0.025277, loss_ce: 0.010706
2022-01-10 03:51:00,717 iteration 3856 : loss : 0.048512, loss_ce: 0.022305
2022-01-10 03:51:02,087 iteration 3857 : loss : 0.019199, loss_ce: 0.008892
2022-01-10 03:51:03,515 iteration 3858 : loss : 0.024631, loss_ce: 0.011054
2022-01-10 03:51:04,934 iteration 3859 : loss : 0.024972, loss_ce: 0.007216
 57%|███████████████▎           | 227/400 [1:39:11<1:13:51, 25.61s/it]2022-01-10 03:51:06,410 iteration 3860 : loss : 0.019662, loss_ce: 0.008518
2022-01-10 03:51:07,875 iteration 3861 : loss : 0.037698, loss_ce: 0.015864
2022-01-10 03:51:09,218 iteration 3862 : loss : 0.021899, loss_ce: 0.010530
2022-01-10 03:51:10,657 iteration 3863 : loss : 0.028260, loss_ce: 0.013100
2022-01-10 03:51:12,115 iteration 3864 : loss : 0.020214, loss_ce: 0.006870
2022-01-10 03:51:13,655 iteration 3865 : loss : 0.033863, loss_ce: 0.015433
2022-01-10 03:51:15,177 iteration 3866 : loss : 0.033548, loss_ce: 0.017519
2022-01-10 03:51:16,571 iteration 3867 : loss : 0.016661, loss_ce: 0.006102
2022-01-10 03:51:18,010 iteration 3868 : loss : 0.021568, loss_ce: 0.008733
2022-01-10 03:51:19,450 iteration 3869 : loss : 0.032143, loss_ce: 0.011984
2022-01-10 03:51:20,921 iteration 3870 : loss : 0.023814, loss_ce: 0.007095
2022-01-10 03:51:22,331 iteration 3871 : loss : 0.026603, loss_ce: 0.007083
2022-01-10 03:51:23,721 iteration 3872 : loss : 0.019820, loss_ce: 0.008306
2022-01-10 03:51:25,116 iteration 3873 : loss : 0.023637, loss_ce: 0.008966
2022-01-10 03:51:26,574 iteration 3874 : loss : 0.019208, loss_ce: 0.006390
2022-01-10 03:51:28,019 iteration 3875 : loss : 0.023131, loss_ce: 0.008328
2022-01-10 03:51:29,418 iteration 3876 : loss : 0.027252, loss_ce: 0.005193
 57%|███████████████▍           | 228/400 [1:39:35<1:12:27, 25.28s/it]2022-01-10 03:51:30,854 iteration 3877 : loss : 0.019196, loss_ce: 0.007398
2022-01-10 03:51:32,324 iteration 3878 : loss : 0.029598, loss_ce: 0.014092
2022-01-10 03:51:33,755 iteration 3879 : loss : 0.029399, loss_ce: 0.013946
2022-01-10 03:51:35,156 iteration 3880 : loss : 0.019348, loss_ce: 0.007783
2022-01-10 03:51:36,524 iteration 3881 : loss : 0.020861, loss_ce: 0.010299
2022-01-10 03:51:37,992 iteration 3882 : loss : 0.036093, loss_ce: 0.013310
2022-01-10 03:51:39,429 iteration 3883 : loss : 0.024239, loss_ce: 0.009790
2022-01-10 03:51:40,792 iteration 3884 : loss : 0.055879, loss_ce: 0.010045
2022-01-10 03:51:42,144 iteration 3885 : loss : 0.016937, loss_ce: 0.005762
2022-01-10 03:51:43,512 iteration 3886 : loss : 0.025888, loss_ce: 0.009244
2022-01-10 03:51:44,971 iteration 3887 : loss : 0.024577, loss_ce: 0.009426
2022-01-10 03:51:46,382 iteration 3888 : loss : 0.023293, loss_ce: 0.009054
2022-01-10 03:51:47,795 iteration 3889 : loss : 0.025886, loss_ce: 0.013591
2022-01-10 03:51:49,118 iteration 3890 : loss : 0.026404, loss_ce: 0.009514
2022-01-10 03:51:50,535 iteration 3891 : loss : 0.018253, loss_ce: 0.008567
2022-01-10 03:51:51,954 iteration 3892 : loss : 0.030910, loss_ce: 0.006487
2022-01-10 03:51:53,453 iteration 3893 : loss : 0.030933, loss_ce: 0.014982
 57%|███████████████▍           | 229/400 [1:39:59<1:10:58, 24.90s/it]2022-01-10 03:51:54,861 iteration 3894 : loss : 0.018901, loss_ce: 0.008247
2022-01-10 03:51:56,321 iteration 3895 : loss : 0.026081, loss_ce: 0.011605
2022-01-10 03:51:57,710 iteration 3896 : loss : 0.020879, loss_ce: 0.005374
2022-01-10 03:51:59,138 iteration 3897 : loss : 0.029622, loss_ce: 0.013460
2022-01-10 03:52:00,519 iteration 3898 : loss : 0.020804, loss_ce: 0.007964
2022-01-10 03:52:01,962 iteration 3899 : loss : 0.025088, loss_ce: 0.010334
2022-01-10 03:52:03,442 iteration 3900 : loss : 0.024018, loss_ce: 0.007148
2022-01-10 03:52:04,802 iteration 3901 : loss : 0.017643, loss_ce: 0.005801
2022-01-10 03:52:06,175 iteration 3902 : loss : 0.014080, loss_ce: 0.005195
2022-01-10 03:52:07,578 iteration 3903 : loss : 0.018868, loss_ce: 0.006774
2022-01-10 03:52:09,054 iteration 3904 : loss : 0.020446, loss_ce: 0.007447
2022-01-10 03:52:10,460 iteration 3905 : loss : 0.019759, loss_ce: 0.008760
2022-01-10 03:52:11,834 iteration 3906 : loss : 0.019947, loss_ce: 0.009941
2022-01-10 03:52:13,251 iteration 3907 : loss : 0.025600, loss_ce: 0.007437
2022-01-10 03:52:14,734 iteration 3908 : loss : 0.032779, loss_ce: 0.011488
2022-01-10 03:52:16,177 iteration 3909 : loss : 0.018138, loss_ce: 0.007025
2022-01-10 03:52:16,177 Training Data Eval:
2022-01-10 03:52:23,228   Average segmentation loss on training set: 0.0138
2022-01-10 03:52:23,229 Validation Data Eval:
2022-01-10 03:52:25,674   Average segmentation loss on validation set: 0.1033
2022-01-10 03:52:27,011 iteration 3910 : loss : 0.017676, loss_ce: 0.005873
 57%|███████████████▌           | 230/400 [1:40:33<1:17:54, 27.50s/it]2022-01-10 03:52:28,423 iteration 3911 : loss : 0.017271, loss_ce: 0.005749
2022-01-10 03:52:29,879 iteration 3912 : loss : 0.025744, loss_ce: 0.012797
2022-01-10 03:52:31,214 iteration 3913 : loss : 0.016691, loss_ce: 0.005412
2022-01-10 03:52:32,536 iteration 3914 : loss : 0.017265, loss_ce: 0.006342
2022-01-10 03:52:33,853 iteration 3915 : loss : 0.018660, loss_ce: 0.006555
2022-01-10 03:52:35,310 iteration 3916 : loss : 0.027108, loss_ce: 0.011872
2022-01-10 03:52:36,771 iteration 3917 : loss : 0.042171, loss_ce: 0.010383
2022-01-10 03:52:38,236 iteration 3918 : loss : 0.027530, loss_ce: 0.012712
2022-01-10 03:52:39,645 iteration 3919 : loss : 0.024919, loss_ce: 0.009663
2022-01-10 03:52:41,073 iteration 3920 : loss : 0.021267, loss_ce: 0.008913
2022-01-10 03:52:42,499 iteration 3921 : loss : 0.027055, loss_ce: 0.008559
2022-01-10 03:52:43,872 iteration 3922 : loss : 0.018551, loss_ce: 0.009677
2022-01-10 03:52:45,271 iteration 3923 : loss : 0.025068, loss_ce: 0.010043
2022-01-10 03:52:46,672 iteration 3924 : loss : 0.025307, loss_ce: 0.010542
2022-01-10 03:52:48,218 iteration 3925 : loss : 0.024995, loss_ce: 0.010294
2022-01-10 03:52:49,631 iteration 3926 : loss : 0.022298, loss_ce: 0.008504
2022-01-10 03:52:51,053 iteration 3927 : loss : 0.027202, loss_ce: 0.011157
 58%|███████████████▌           | 231/400 [1:40:57<1:14:32, 26.46s/it]2022-01-10 03:52:52,537 iteration 3928 : loss : 0.020114, loss_ce: 0.008200
2022-01-10 03:52:54,028 iteration 3929 : loss : 0.027042, loss_ce: 0.010294
2022-01-10 03:52:55,579 iteration 3930 : loss : 0.037752, loss_ce: 0.012458
2022-01-10 03:52:57,009 iteration 3931 : loss : 0.019615, loss_ce: 0.008738
2022-01-10 03:52:58,470 iteration 3932 : loss : 0.024790, loss_ce: 0.006576
2022-01-10 03:52:59,851 iteration 3933 : loss : 0.027890, loss_ce: 0.006534
2022-01-10 03:53:01,258 iteration 3934 : loss : 0.024657, loss_ce: 0.007813
2022-01-10 03:53:02,721 iteration 3935 : loss : 0.024490, loss_ce: 0.009444
2022-01-10 03:53:04,115 iteration 3936 : loss : 0.041386, loss_ce: 0.022518
2022-01-10 03:53:05,519 iteration 3937 : loss : 0.016002, loss_ce: 0.004653
2022-01-10 03:53:06,895 iteration 3938 : loss : 0.020465, loss_ce: 0.009428
2022-01-10 03:53:08,388 iteration 3939 : loss : 0.045175, loss_ce: 0.015235
2022-01-10 03:53:09,772 iteration 3940 : loss : 0.027697, loss_ce: 0.006605
2022-01-10 03:53:11,173 iteration 3941 : loss : 0.018422, loss_ce: 0.008117
2022-01-10 03:53:12,563 iteration 3942 : loss : 0.024855, loss_ce: 0.010919
2022-01-10 03:53:14,007 iteration 3943 : loss : 0.024388, loss_ce: 0.012301
2022-01-10 03:53:15,421 iteration 3944 : loss : 0.025931, loss_ce: 0.008178
 58%|███████████████▋           | 232/400 [1:41:21<1:12:19, 25.83s/it]2022-01-10 03:53:16,868 iteration 3945 : loss : 0.018140, loss_ce: 0.008588
2022-01-10 03:53:18,198 iteration 3946 : loss : 0.013620, loss_ce: 0.005559
2022-01-10 03:53:19,594 iteration 3947 : loss : 0.019473, loss_ce: 0.006519
2022-01-10 03:53:20,981 iteration 3948 : loss : 0.018441, loss_ce: 0.007482
2022-01-10 03:53:22,399 iteration 3949 : loss : 0.026754, loss_ce: 0.013460
2022-01-10 03:53:23,804 iteration 3950 : loss : 0.017178, loss_ce: 0.006498
2022-01-10 03:53:25,243 iteration 3951 : loss : 0.033109, loss_ce: 0.012383
2022-01-10 03:53:26,646 iteration 3952 : loss : 0.018538, loss_ce: 0.008590
2022-01-10 03:53:28,047 iteration 3953 : loss : 0.017801, loss_ce: 0.005672
2022-01-10 03:53:29,496 iteration 3954 : loss : 0.021601, loss_ce: 0.008523
2022-01-10 03:53:30,923 iteration 3955 : loss : 0.024165, loss_ce: 0.009689
2022-01-10 03:53:32,305 iteration 3956 : loss : 0.022941, loss_ce: 0.009230
2022-01-10 03:53:33,693 iteration 3957 : loss : 0.024719, loss_ce: 0.010476
2022-01-10 03:53:35,181 iteration 3958 : loss : 0.036436, loss_ce: 0.014962
2022-01-10 03:53:36,560 iteration 3959 : loss : 0.019754, loss_ce: 0.005599
2022-01-10 03:53:37,974 iteration 3960 : loss : 0.028036, loss_ce: 0.006116
2022-01-10 03:53:39,421 iteration 3961 : loss : 0.017687, loss_ce: 0.006068
 58%|███████████████▋           | 233/400 [1:41:45<1:10:22, 25.28s/it]2022-01-10 03:53:41,020 iteration 3962 : loss : 0.031620, loss_ce: 0.012841
2022-01-10 03:53:42,372 iteration 3963 : loss : 0.021108, loss_ce: 0.009224
2022-01-10 03:53:43,819 iteration 3964 : loss : 0.021426, loss_ce: 0.009521
2022-01-10 03:53:45,315 iteration 3965 : loss : 0.034353, loss_ce: 0.011258
2022-01-10 03:53:46,700 iteration 3966 : loss : 0.019048, loss_ce: 0.007050
2022-01-10 03:53:48,077 iteration 3967 : loss : 0.032610, loss_ce: 0.008828
2022-01-10 03:53:49,559 iteration 3968 : loss : 0.024730, loss_ce: 0.010273
2022-01-10 03:53:51,026 iteration 3969 : loss : 0.021600, loss_ce: 0.008874
2022-01-10 03:53:52,522 iteration 3970 : loss : 0.032563, loss_ce: 0.016332
2022-01-10 03:53:53,897 iteration 3971 : loss : 0.022068, loss_ce: 0.008028
2022-01-10 03:53:55,273 iteration 3972 : loss : 0.018935, loss_ce: 0.006652
2022-01-10 03:53:56,699 iteration 3973 : loss : 0.040282, loss_ce: 0.015468
2022-01-10 03:53:58,185 iteration 3974 : loss : 0.037856, loss_ce: 0.016247
2022-01-10 03:53:59,669 iteration 3975 : loss : 0.021876, loss_ce: 0.007634
2022-01-10 03:54:01,173 iteration 3976 : loss : 0.020609, loss_ce: 0.009541
2022-01-10 03:54:02,583 iteration 3977 : loss : 0.023663, loss_ce: 0.008397
2022-01-10 03:54:03,984 iteration 3978 : loss : 0.020379, loss_ce: 0.007885
 58%|███████████████▊           | 234/400 [1:42:10<1:09:21, 25.07s/it]2022-01-10 03:54:05,463 iteration 3979 : loss : 0.030629, loss_ce: 0.010227
2022-01-10 03:54:06,904 iteration 3980 : loss : 0.019423, loss_ce: 0.008088
2022-01-10 03:54:08,280 iteration 3981 : loss : 0.022306, loss_ce: 0.009227
2022-01-10 03:54:09,823 iteration 3982 : loss : 0.022335, loss_ce: 0.007442
2022-01-10 03:54:11,234 iteration 3983 : loss : 0.019386, loss_ce: 0.005514
2022-01-10 03:54:12,689 iteration 3984 : loss : 0.030995, loss_ce: 0.011261
2022-01-10 03:54:14,107 iteration 3985 : loss : 0.025182, loss_ce: 0.009410
2022-01-10 03:54:15,480 iteration 3986 : loss : 0.021528, loss_ce: 0.008825
2022-01-10 03:54:16,863 iteration 3987 : loss : 0.020590, loss_ce: 0.009164
2022-01-10 03:54:18,326 iteration 3988 : loss : 0.025538, loss_ce: 0.010577
2022-01-10 03:54:19,747 iteration 3989 : loss : 0.028158, loss_ce: 0.009948
2022-01-10 03:54:21,150 iteration 3990 : loss : 0.021002, loss_ce: 0.008852
2022-01-10 03:54:22,557 iteration 3991 : loss : 0.034017, loss_ce: 0.014205
2022-01-10 03:54:24,090 iteration 3992 : loss : 0.027717, loss_ce: 0.009565
2022-01-10 03:54:25,497 iteration 3993 : loss : 0.023026, loss_ce: 0.012217
2022-01-10 03:54:26,898 iteration 3994 : loss : 0.020717, loss_ce: 0.008211
2022-01-10 03:54:26,898 Training Data Eval:
2022-01-10 03:54:33,960   Average segmentation loss on training set: 0.0139
2022-01-10 03:54:33,960 Validation Data Eval:
2022-01-10 03:54:36,395   Average segmentation loss on validation set: 0.0918
2022-01-10 03:54:37,786 iteration 3995 : loss : 0.028982, loss_ce: 0.007497
 59%|███████████████▊           | 235/400 [1:42:44<1:16:08, 27.69s/it]2022-01-10 03:54:39,321 iteration 3996 : loss : 0.027401, loss_ce: 0.014136
2022-01-10 03:54:40,766 iteration 3997 : loss : 0.026202, loss_ce: 0.008106
2022-01-10 03:54:42,171 iteration 3998 : loss : 0.020632, loss_ce: 0.007364
2022-01-10 03:54:43,631 iteration 3999 : loss : 0.019914, loss_ce: 0.006269
2022-01-10 03:54:45,054 iteration 4000 : loss : 0.021106, loss_ce: 0.008436
2022-01-10 03:54:46,579 iteration 4001 : loss : 0.024526, loss_ce: 0.007376
2022-01-10 03:54:47,993 iteration 4002 : loss : 0.022093, loss_ce: 0.011173
2022-01-10 03:54:49,403 iteration 4003 : loss : 0.020191, loss_ce: 0.007831
2022-01-10 03:54:50,806 iteration 4004 : loss : 0.017628, loss_ce: 0.006147
2022-01-10 03:54:52,231 iteration 4005 : loss : 0.034031, loss_ce: 0.011527
2022-01-10 03:54:53,740 iteration 4006 : loss : 0.028498, loss_ce: 0.009303
2022-01-10 03:54:55,134 iteration 4007 : loss : 0.019828, loss_ce: 0.007380
2022-01-10 03:54:56,557 iteration 4008 : loss : 0.039734, loss_ce: 0.015781
2022-01-10 03:54:57,962 iteration 4009 : loss : 0.018462, loss_ce: 0.006896
2022-01-10 03:54:59,308 iteration 4010 : loss : 0.014402, loss_ce: 0.004971
2022-01-10 03:55:00,762 iteration 4011 : loss : 0.035826, loss_ce: 0.008372
2022-01-10 03:55:02,217 iteration 4012 : loss : 0.025557, loss_ce: 0.011427
 59%|███████████████▉           | 236/400 [1:43:08<1:13:00, 26.71s/it]2022-01-10 03:55:03,672 iteration 4013 : loss : 0.023276, loss_ce: 0.005845
2022-01-10 03:55:05,132 iteration 4014 : loss : 0.019469, loss_ce: 0.006403
2022-01-10 03:55:06,518 iteration 4015 : loss : 0.020162, loss_ce: 0.010909
2022-01-10 03:55:07,961 iteration 4016 : loss : 0.022246, loss_ce: 0.006584
2022-01-10 03:55:09,338 iteration 4017 : loss : 0.030205, loss_ce: 0.011063
2022-01-10 03:55:10,827 iteration 4018 : loss : 0.035902, loss_ce: 0.013789
2022-01-10 03:55:12,303 iteration 4019 : loss : 0.026791, loss_ce: 0.012456
2022-01-10 03:55:13,703 iteration 4020 : loss : 0.020035, loss_ce: 0.007163
2022-01-10 03:55:15,124 iteration 4021 : loss : 0.027530, loss_ce: 0.009247
2022-01-10 03:55:16,534 iteration 4022 : loss : 0.023238, loss_ce: 0.006576
2022-01-10 03:55:17,960 iteration 4023 : loss : 0.026869, loss_ce: 0.011947
2022-01-10 03:55:19,388 iteration 4024 : loss : 0.041467, loss_ce: 0.013789
2022-01-10 03:55:20,780 iteration 4025 : loss : 0.027117, loss_ce: 0.009978
2022-01-10 03:55:22,172 iteration 4026 : loss : 0.016906, loss_ce: 0.006833
2022-01-10 03:55:23,563 iteration 4027 : loss : 0.019530, loss_ce: 0.006944
2022-01-10 03:55:24,968 iteration 4028 : loss : 0.034658, loss_ce: 0.010098
2022-01-10 03:55:26,397 iteration 4029 : loss : 0.019826, loss_ce: 0.008097
 59%|███████████████▉           | 237/400 [1:43:32<1:10:29, 25.95s/it]2022-01-10 03:55:27,880 iteration 4030 : loss : 0.022625, loss_ce: 0.009051
2022-01-10 03:55:29,279 iteration 4031 : loss : 0.020032, loss_ce: 0.007823
2022-01-10 03:55:30,736 iteration 4032 : loss : 0.021104, loss_ce: 0.007748
2022-01-10 03:55:32,236 iteration 4033 : loss : 0.036609, loss_ce: 0.012027
2022-01-10 03:55:33,623 iteration 4034 : loss : 0.023562, loss_ce: 0.007744
2022-01-10 03:55:35,045 iteration 4035 : loss : 0.020907, loss_ce: 0.007073
2022-01-10 03:55:36,453 iteration 4036 : loss : 0.014492, loss_ce: 0.004678
2022-01-10 03:55:37,909 iteration 4037 : loss : 0.027445, loss_ce: 0.009360
2022-01-10 03:55:39,332 iteration 4038 : loss : 0.020475, loss_ce: 0.011481
2022-01-10 03:55:40,648 iteration 4039 : loss : 0.016492, loss_ce: 0.007557
2022-01-10 03:55:42,050 iteration 4040 : loss : 0.028147, loss_ce: 0.005882
2022-01-10 03:55:43,574 iteration 4041 : loss : 0.034976, loss_ce: 0.015739
2022-01-10 03:55:44,922 iteration 4042 : loss : 0.019317, loss_ce: 0.008615
2022-01-10 03:55:46,352 iteration 4043 : loss : 0.018128, loss_ce: 0.006901
2022-01-10 03:55:47,780 iteration 4044 : loss : 0.024068, loss_ce: 0.009496
2022-01-10 03:55:49,165 iteration 4045 : loss : 0.017580, loss_ce: 0.006760
2022-01-10 03:55:50,616 iteration 4046 : loss : 0.022363, loss_ce: 0.007687
 60%|████████████████           | 238/400 [1:43:57<1:08:39, 25.43s/it]2022-01-10 03:55:52,028 iteration 4047 : loss : 0.015780, loss_ce: 0.006755
2022-01-10 03:55:53,429 iteration 4048 : loss : 0.018412, loss_ce: 0.004960
2022-01-10 03:55:54,913 iteration 4049 : loss : 0.029766, loss_ce: 0.010511
2022-01-10 03:55:56,327 iteration 4050 : loss : 0.018662, loss_ce: 0.005408
2022-01-10 03:55:57,748 iteration 4051 : loss : 0.015877, loss_ce: 0.005848
2022-01-10 03:55:59,158 iteration 4052 : loss : 0.024353, loss_ce: 0.007016
2022-01-10 03:56:00,552 iteration 4053 : loss : 0.021641, loss_ce: 0.009885
2022-01-10 03:56:01,930 iteration 4054 : loss : 0.021763, loss_ce: 0.008243
2022-01-10 03:56:03,446 iteration 4055 : loss : 0.043700, loss_ce: 0.015255
2022-01-10 03:56:04,904 iteration 4056 : loss : 0.020268, loss_ce: 0.006409
2022-01-10 03:56:06,381 iteration 4057 : loss : 0.022688, loss_ce: 0.010387
2022-01-10 03:56:07,723 iteration 4058 : loss : 0.020061, loss_ce: 0.009572
2022-01-10 03:56:09,192 iteration 4059 : loss : 0.024480, loss_ce: 0.010961
2022-01-10 03:56:10,632 iteration 4060 : loss : 0.025810, loss_ce: 0.011028
2022-01-10 03:56:12,035 iteration 4061 : loss : 0.029017, loss_ce: 0.007118
2022-01-10 03:56:13,425 iteration 4062 : loss : 0.023776, loss_ce: 0.010717
2022-01-10 03:56:14,950 iteration 4063 : loss : 0.045732, loss_ce: 0.017057
 60%|████████████████▏          | 239/400 [1:44:21<1:07:21, 25.11s/it]2022-01-10 03:56:16,427 iteration 4064 : loss : 0.021104, loss_ce: 0.006968
2022-01-10 03:56:17,839 iteration 4065 : loss : 0.021993, loss_ce: 0.007438
2022-01-10 03:56:19,169 iteration 4066 : loss : 0.014888, loss_ce: 0.004932
2022-01-10 03:56:20,535 iteration 4067 : loss : 0.023208, loss_ce: 0.006250
2022-01-10 03:56:21,957 iteration 4068 : loss : 0.017471, loss_ce: 0.006914
2022-01-10 03:56:23,416 iteration 4069 : loss : 0.021988, loss_ce: 0.007197
2022-01-10 03:56:24,847 iteration 4070 : loss : 0.020307, loss_ce: 0.009992
2022-01-10 03:56:26,201 iteration 4071 : loss : 0.027497, loss_ce: 0.012296
2022-01-10 03:56:27,602 iteration 4072 : loss : 0.021764, loss_ce: 0.009891
2022-01-10 03:56:29,032 iteration 4073 : loss : 0.025589, loss_ce: 0.012322
2022-01-10 03:56:30,399 iteration 4074 : loss : 0.024837, loss_ce: 0.008588
2022-01-10 03:56:31,853 iteration 4075 : loss : 0.038423, loss_ce: 0.015235
2022-01-10 03:56:33,289 iteration 4076 : loss : 0.017031, loss_ce: 0.005405
2022-01-10 03:56:34,733 iteration 4077 : loss : 0.021905, loss_ce: 0.010347
2022-01-10 03:56:36,216 iteration 4078 : loss : 0.031775, loss_ce: 0.008893
2022-01-10 03:56:37,641 iteration 4079 : loss : 0.033018, loss_ce: 0.011547
2022-01-10 03:56:37,641 Training Data Eval:
2022-01-10 03:56:44,685   Average segmentation loss on training set: 0.0133
2022-01-10 03:56:44,685 Validation Data Eval:
2022-01-10 03:56:47,138   Average segmentation loss on validation set: 0.0822
2022-01-10 03:56:48,558 iteration 4080 : loss : 0.019730, loss_ce: 0.005674
 60%|████████████████▏          | 240/400 [1:44:55<1:13:44, 27.65s/it]2022-01-10 03:56:50,017 iteration 4081 : loss : 0.017831, loss_ce: 0.008512
2022-01-10 03:56:51,505 iteration 4082 : loss : 0.024460, loss_ce: 0.008577
2022-01-10 03:56:52,856 iteration 4083 : loss : 0.019654, loss_ce: 0.007652
2022-01-10 03:56:54,235 iteration 4084 : loss : 0.019946, loss_ce: 0.006819
2022-01-10 03:56:55,646 iteration 4085 : loss : 0.025686, loss_ce: 0.006150
2022-01-10 03:56:57,033 iteration 4086 : loss : 0.017677, loss_ce: 0.006052
2022-01-10 03:56:58,444 iteration 4087 : loss : 0.030235, loss_ce: 0.008573
2022-01-10 03:56:59,868 iteration 4088 : loss : 0.024771, loss_ce: 0.012403
2022-01-10 03:57:01,307 iteration 4089 : loss : 0.034384, loss_ce: 0.012598
2022-01-10 03:57:02,729 iteration 4090 : loss : 0.024722, loss_ce: 0.009821
2022-01-10 03:57:04,190 iteration 4091 : loss : 0.028550, loss_ce: 0.009351
2022-01-10 03:57:05,559 iteration 4092 : loss : 0.027379, loss_ce: 0.007458
2022-01-10 03:57:06,944 iteration 4093 : loss : 0.017451, loss_ce: 0.007593
2022-01-10 03:57:08,424 iteration 4094 : loss : 0.023088, loss_ce: 0.007122
2022-01-10 03:57:09,851 iteration 4095 : loss : 0.031838, loss_ce: 0.009376
2022-01-10 03:57:11,299 iteration 4096 : loss : 0.025020, loss_ce: 0.010313
2022-01-10 03:57:12,759 iteration 4097 : loss : 0.023286, loss_ce: 0.010241
 60%|████████████████▎          | 241/400 [1:45:19<1:10:32, 26.62s/it]2022-01-10 03:57:14,246 iteration 4098 : loss : 0.021090, loss_ce: 0.007951
2022-01-10 03:57:15,663 iteration 4099 : loss : 0.027474, loss_ce: 0.011547
2022-01-10 03:57:17,042 iteration 4100 : loss : 0.031376, loss_ce: 0.011118
2022-01-10 03:57:18,474 iteration 4101 : loss : 0.017672, loss_ce: 0.007355
2022-01-10 03:57:19,964 iteration 4102 : loss : 0.026302, loss_ce: 0.009387
2022-01-10 03:57:21,303 iteration 4103 : loss : 0.016996, loss_ce: 0.006244
2022-01-10 03:57:22,735 iteration 4104 : loss : 0.029132, loss_ce: 0.010370
2022-01-10 03:57:24,193 iteration 4105 : loss : 0.025539, loss_ce: 0.010452
2022-01-10 03:57:25,697 iteration 4106 : loss : 0.036070, loss_ce: 0.011563
2022-01-10 03:57:27,144 iteration 4107 : loss : 0.026229, loss_ce: 0.010690
2022-01-10 03:57:28,601 iteration 4108 : loss : 0.027747, loss_ce: 0.010958
2022-01-10 03:57:30,041 iteration 4109 : loss : 0.039633, loss_ce: 0.010617
2022-01-10 03:57:31,508 iteration 4110 : loss : 0.020196, loss_ce: 0.006745
2022-01-10 03:57:32,933 iteration 4111 : loss : 0.025262, loss_ce: 0.006886
2022-01-10 03:57:34,255 iteration 4112 : loss : 0.015903, loss_ce: 0.006658
2022-01-10 03:57:35,733 iteration 4113 : loss : 0.033770, loss_ce: 0.010797
2022-01-10 03:57:37,155 iteration 4114 : loss : 0.025489, loss_ce: 0.009450
 60%|████████████████▎          | 242/400 [1:45:43<1:08:20, 25.95s/it]2022-01-10 03:57:38,624 iteration 4115 : loss : 0.020938, loss_ce: 0.005984
2022-01-10 03:57:40,008 iteration 4116 : loss : 0.022288, loss_ce: 0.010126
2022-01-10 03:57:41,369 iteration 4117 : loss : 0.016874, loss_ce: 0.008240
2022-01-10 03:57:42,765 iteration 4118 : loss : 0.028574, loss_ce: 0.007850
2022-01-10 03:57:44,081 iteration 4119 : loss : 0.017381, loss_ce: 0.006540
2022-01-10 03:57:45,600 iteration 4120 : loss : 0.023704, loss_ce: 0.007479
2022-01-10 03:57:47,015 iteration 4121 : loss : 0.017934, loss_ce: 0.006957
2022-01-10 03:57:48,458 iteration 4122 : loss : 0.030296, loss_ce: 0.011226
2022-01-10 03:57:49,900 iteration 4123 : loss : 0.027818, loss_ce: 0.010524
2022-01-10 03:57:51,245 iteration 4124 : loss : 0.017719, loss_ce: 0.007160
2022-01-10 03:57:52,681 iteration 4125 : loss : 0.030159, loss_ce: 0.008924
2022-01-10 03:57:54,211 iteration 4126 : loss : 0.021648, loss_ce: 0.008608
2022-01-10 03:57:55,684 iteration 4127 : loss : 0.039206, loss_ce: 0.019451
2022-01-10 03:57:57,074 iteration 4128 : loss : 0.019341, loss_ce: 0.008162
2022-01-10 03:57:58,601 iteration 4129 : loss : 0.028140, loss_ce: 0.006061
2022-01-10 03:58:00,067 iteration 4130 : loss : 0.027277, loss_ce: 0.008528
2022-01-10 03:58:01,477 iteration 4131 : loss : 0.022512, loss_ce: 0.009559
 61%|████████████████▍          | 243/400 [1:46:07<1:06:37, 25.46s/it]2022-01-10 03:58:02,994 iteration 4132 : loss : 0.018880, loss_ce: 0.006036
2022-01-10 03:58:04,360 iteration 4133 : loss : 0.021009, loss_ce: 0.010000
2022-01-10 03:58:05,828 iteration 4134 : loss : 0.017525, loss_ce: 0.008053
2022-01-10 03:58:07,252 iteration 4135 : loss : 0.025903, loss_ce: 0.009255
2022-01-10 03:58:08,658 iteration 4136 : loss : 0.016379, loss_ce: 0.005079
2022-01-10 03:58:10,089 iteration 4137 : loss : 0.020084, loss_ce: 0.010287
2022-01-10 03:58:11,528 iteration 4138 : loss : 0.018738, loss_ce: 0.009835
2022-01-10 03:58:12,973 iteration 4139 : loss : 0.036509, loss_ce: 0.009597
2022-01-10 03:58:14,437 iteration 4140 : loss : 0.025905, loss_ce: 0.009583
2022-01-10 03:58:15,852 iteration 4141 : loss : 0.020468, loss_ce: 0.005523
2022-01-10 03:58:17,243 iteration 4142 : loss : 0.020672, loss_ce: 0.009887
2022-01-10 03:58:18,619 iteration 4143 : loss : 0.019841, loss_ce: 0.006970
2022-01-10 03:58:20,051 iteration 4144 : loss : 0.019084, loss_ce: 0.006969
2022-01-10 03:58:21,451 iteration 4145 : loss : 0.030222, loss_ce: 0.013869
2022-01-10 03:58:22,766 iteration 4146 : loss : 0.019479, loss_ce: 0.005366
2022-01-10 03:58:24,155 iteration 4147 : loss : 0.018351, loss_ce: 0.007120
2022-01-10 03:58:25,564 iteration 4148 : loss : 0.031506, loss_ce: 0.010522
 61%|████████████████▍          | 244/400 [1:46:32<1:05:08, 25.05s/it]2022-01-10 03:58:27,047 iteration 4149 : loss : 0.027656, loss_ce: 0.010088
2022-01-10 03:58:28,442 iteration 4150 : loss : 0.023521, loss_ce: 0.008170
2022-01-10 03:58:29,845 iteration 4151 : loss : 0.018991, loss_ce: 0.008011
2022-01-10 03:58:31,330 iteration 4152 : loss : 0.045080, loss_ce: 0.019060
2022-01-10 03:58:32,777 iteration 4153 : loss : 0.024771, loss_ce: 0.011847
2022-01-10 03:58:34,130 iteration 4154 : loss : 0.018829, loss_ce: 0.007770
2022-01-10 03:58:35,502 iteration 4155 : loss : 0.020151, loss_ce: 0.008408
2022-01-10 03:58:36,913 iteration 4156 : loss : 0.024354, loss_ce: 0.008019
2022-01-10 03:58:38,332 iteration 4157 : loss : 0.015046, loss_ce: 0.005810
2022-01-10 03:58:39,722 iteration 4158 : loss : 0.025619, loss_ce: 0.009460
2022-01-10 03:58:41,184 iteration 4159 : loss : 0.032965, loss_ce: 0.017572
2022-01-10 03:58:42,611 iteration 4160 : loss : 0.024049, loss_ce: 0.008618
2022-01-10 03:58:43,998 iteration 4161 : loss : 0.025947, loss_ce: 0.008421
2022-01-10 03:58:45,434 iteration 4162 : loss : 0.026303, loss_ce: 0.009066
2022-01-10 03:58:46,867 iteration 4163 : loss : 0.025043, loss_ce: 0.008575
2022-01-10 03:58:48,262 iteration 4164 : loss : 0.019326, loss_ce: 0.005228
2022-01-10 03:58:48,262 Training Data Eval:
2022-01-10 03:58:55,316   Average segmentation loss on training set: 0.0127
2022-01-10 03:58:55,316 Validation Data Eval:
2022-01-10 03:58:57,763   Average segmentation loss on validation set: 0.1010
2022-01-10 03:58:59,196 iteration 4165 : loss : 0.020117, loss_ce: 0.006156
 61%|████████████████▌          | 245/400 [1:47:05<1:11:22, 27.63s/it]2022-01-10 03:59:00,644 iteration 4166 : loss : 0.031395, loss_ce: 0.007754
2022-01-10 03:59:02,025 iteration 4167 : loss : 0.015147, loss_ce: 0.006268
2022-01-10 03:59:03,362 iteration 4168 : loss : 0.016518, loss_ce: 0.005412
2022-01-10 03:59:04,724 iteration 4169 : loss : 0.023507, loss_ce: 0.008993
2022-01-10 03:59:06,212 iteration 4170 : loss : 0.030921, loss_ce: 0.009691
2022-01-10 03:59:07,721 iteration 4171 : loss : 0.027552, loss_ce: 0.010486
2022-01-10 03:59:09,075 iteration 4172 : loss : 0.018539, loss_ce: 0.004415
2022-01-10 03:59:10,491 iteration 4173 : loss : 0.021749, loss_ce: 0.007875
2022-01-10 03:59:11,911 iteration 4174 : loss : 0.048883, loss_ce: 0.025250
2022-01-10 03:59:13,361 iteration 4175 : loss : 0.023458, loss_ce: 0.008923
2022-01-10 03:59:14,781 iteration 4176 : loss : 0.024837, loss_ce: 0.011385
2022-01-10 03:59:16,217 iteration 4177 : loss : 0.032148, loss_ce: 0.010259
2022-01-10 03:59:17,625 iteration 4178 : loss : 0.022754, loss_ce: 0.011646
2022-01-10 03:59:19,116 iteration 4179 : loss : 0.023531, loss_ce: 0.009575
2022-01-10 03:59:20,533 iteration 4180 : loss : 0.031899, loss_ce: 0.011147
2022-01-10 03:59:21,853 iteration 4181 : loss : 0.027738, loss_ce: 0.008690
2022-01-10 03:59:23,298 iteration 4182 : loss : 0.024453, loss_ce: 0.010434
 62%|████████████████▌          | 246/400 [1:47:29<1:08:11, 26.57s/it]2022-01-10 03:59:24,772 iteration 4183 : loss : 0.022982, loss_ce: 0.007803
2022-01-10 03:59:26,199 iteration 4184 : loss : 0.020489, loss_ce: 0.008254
2022-01-10 03:59:27,610 iteration 4185 : loss : 0.024930, loss_ce: 0.009011
2022-01-10 03:59:28,988 iteration 4186 : loss : 0.018441, loss_ce: 0.007386
2022-01-10 03:59:30,464 iteration 4187 : loss : 0.037192, loss_ce: 0.012915
2022-01-10 03:59:31,924 iteration 4188 : loss : 0.028811, loss_ce: 0.013413
2022-01-10 03:59:33,353 iteration 4189 : loss : 0.018242, loss_ce: 0.004941
2022-01-10 03:59:34,828 iteration 4190 : loss : 0.035959, loss_ce: 0.013626
2022-01-10 03:59:36,214 iteration 4191 : loss : 0.030316, loss_ce: 0.009423
2022-01-10 03:59:37,630 iteration 4192 : loss : 0.016330, loss_ce: 0.004648
2022-01-10 03:59:39,047 iteration 4193 : loss : 0.018761, loss_ce: 0.005838
2022-01-10 03:59:40,485 iteration 4194 : loss : 0.020145, loss_ce: 0.007505
2022-01-10 03:59:42,040 iteration 4195 : loss : 0.037818, loss_ce: 0.014348
2022-01-10 03:59:43,416 iteration 4196 : loss : 0.016873, loss_ce: 0.008535
2022-01-10 03:59:44,885 iteration 4197 : loss : 0.020526, loss_ce: 0.006406
2022-01-10 03:59:46,311 iteration 4198 : loss : 0.022825, loss_ce: 0.009264
2022-01-10 03:59:47,659 iteration 4199 : loss : 0.021382, loss_ce: 0.007470
 62%|████████████████▋          | 247/400 [1:47:54<1:06:03, 25.91s/it]2022-01-10 03:59:49,094 iteration 4200 : loss : 0.020362, loss_ce: 0.006070
2022-01-10 03:59:50,534 iteration 4201 : loss : 0.023881, loss_ce: 0.009975
2022-01-10 03:59:51,881 iteration 4202 : loss : 0.015295, loss_ce: 0.006640
2022-01-10 03:59:53,256 iteration 4203 : loss : 0.019271, loss_ce: 0.007965
2022-01-10 03:59:54,661 iteration 4204 : loss : 0.019110, loss_ce: 0.006910
2022-01-10 03:59:56,039 iteration 4205 : loss : 0.015477, loss_ce: 0.005356
2022-01-10 03:59:57,370 iteration 4206 : loss : 0.018429, loss_ce: 0.005811
2022-01-10 03:59:58,736 iteration 4207 : loss : 0.017843, loss_ce: 0.007997
2022-01-10 04:00:00,118 iteration 4208 : loss : 0.022288, loss_ce: 0.010560
2022-01-10 04:00:01,611 iteration 4209 : loss : 0.031028, loss_ce: 0.008191
2022-01-10 04:00:03,085 iteration 4210 : loss : 0.025833, loss_ce: 0.009777
2022-01-10 04:00:04,368 iteration 4211 : loss : 0.013848, loss_ce: 0.005406
2022-01-10 04:00:05,771 iteration 4212 : loss : 0.021784, loss_ce: 0.009124
2022-01-10 04:00:07,174 iteration 4213 : loss : 0.027021, loss_ce: 0.011539
2022-01-10 04:00:08,633 iteration 4214 : loss : 0.022196, loss_ce: 0.007213
2022-01-10 04:00:10,018 iteration 4215 : loss : 0.019591, loss_ce: 0.006512
2022-01-10 04:00:11,513 iteration 4216 : loss : 0.029247, loss_ce: 0.010228
 62%|████████████████▋          | 248/400 [1:48:18<1:04:04, 25.29s/it]2022-01-10 04:00:13,069 iteration 4217 : loss : 0.021593, loss_ce: 0.008515
2022-01-10 04:00:14,416 iteration 4218 : loss : 0.015864, loss_ce: 0.007859
2022-01-10 04:00:15,780 iteration 4219 : loss : 0.016735, loss_ce: 0.006668
2022-01-10 04:00:17,306 iteration 4220 : loss : 0.027949, loss_ce: 0.010725
2022-01-10 04:00:18,755 iteration 4221 : loss : 0.028773, loss_ce: 0.007374
2022-01-10 04:00:20,215 iteration 4222 : loss : 0.023905, loss_ce: 0.010389
2022-01-10 04:00:21,681 iteration 4223 : loss : 0.023184, loss_ce: 0.008248
2022-01-10 04:00:23,134 iteration 4224 : loss : 0.024860, loss_ce: 0.007130
2022-01-10 04:00:24,551 iteration 4225 : loss : 0.023468, loss_ce: 0.007549
2022-01-10 04:00:25,955 iteration 4226 : loss : 0.015221, loss_ce: 0.004752
2022-01-10 04:00:27,470 iteration 4227 : loss : 0.037832, loss_ce: 0.013162
2022-01-10 04:00:28,879 iteration 4228 : loss : 0.021523, loss_ce: 0.007654
2022-01-10 04:00:30,328 iteration 4229 : loss : 0.020293, loss_ce: 0.007085
2022-01-10 04:00:31,750 iteration 4230 : loss : 0.020285, loss_ce: 0.009240
2022-01-10 04:00:33,130 iteration 4231 : loss : 0.017823, loss_ce: 0.005394
2022-01-10 04:00:34,520 iteration 4232 : loss : 0.031514, loss_ce: 0.014072
2022-01-10 04:00:35,925 iteration 4233 : loss : 0.020031, loss_ce: 0.009315
 62%|████████████████▊          | 249/400 [1:48:42<1:02:58, 25.03s/it]2022-01-10 04:00:37,455 iteration 4234 : loss : 0.029275, loss_ce: 0.012923
2022-01-10 04:00:38,859 iteration 4235 : loss : 0.016874, loss_ce: 0.004593
2022-01-10 04:00:40,212 iteration 4236 : loss : 0.017193, loss_ce: 0.006886
2022-01-10 04:00:41,572 iteration 4237 : loss : 0.018070, loss_ce: 0.007492
2022-01-10 04:00:42,966 iteration 4238 : loss : 0.019335, loss_ce: 0.006218
2022-01-10 04:00:44,416 iteration 4239 : loss : 0.019890, loss_ce: 0.008557
2022-01-10 04:00:45,846 iteration 4240 : loss : 0.024776, loss_ce: 0.008209
2022-01-10 04:00:47,258 iteration 4241 : loss : 0.017467, loss_ce: 0.007050
2022-01-10 04:00:48,562 iteration 4242 : loss : 0.015534, loss_ce: 0.005936
2022-01-10 04:00:49,988 iteration 4243 : loss : 0.029510, loss_ce: 0.010466
2022-01-10 04:00:51,348 iteration 4244 : loss : 0.020877, loss_ce: 0.006417
2022-01-10 04:00:52,738 iteration 4245 : loss : 0.022638, loss_ce: 0.008192
2022-01-10 04:00:54,110 iteration 4246 : loss : 0.018909, loss_ce: 0.006230
2022-01-10 04:00:55,500 iteration 4247 : loss : 0.020363, loss_ce: 0.005921
2022-01-10 04:00:56,830 iteration 4248 : loss : 0.022872, loss_ce: 0.010245
2022-01-10 04:00:58,296 iteration 4249 : loss : 0.019619, loss_ce: 0.007616
2022-01-10 04:00:58,296 Training Data Eval:
2022-01-10 04:01:05,355   Average segmentation loss on training set: 0.0128
2022-01-10 04:01:05,355 Validation Data Eval:
2022-01-10 04:01:07,797   Average segmentation loss on validation set: 0.0819
2022-01-10 04:01:09,208 iteration 4250 : loss : 0.017690, loss_ce: 0.006884
 62%|████████████████▉          | 250/400 [1:49:15<1:08:45, 27.51s/it]2022-01-10 04:01:10,694 iteration 4251 : loss : 0.023884, loss_ce: 0.008227
2022-01-10 04:01:12,107 iteration 4252 : loss : 0.022825, loss_ce: 0.006192
2022-01-10 04:01:13,540 iteration 4253 : loss : 0.017389, loss_ce: 0.008036
2022-01-10 04:01:14,978 iteration 4254 : loss : 0.026207, loss_ce: 0.010154
2022-01-10 04:01:16,412 iteration 4255 : loss : 0.027605, loss_ce: 0.008006
2022-01-10 04:01:17,793 iteration 4256 : loss : 0.042260, loss_ce: 0.022568
2022-01-10 04:01:19,204 iteration 4257 : loss : 0.018171, loss_ce: 0.007589
2022-01-10 04:01:20,683 iteration 4258 : loss : 0.028749, loss_ce: 0.012303
2022-01-10 04:01:22,003 iteration 4259 : loss : 0.015082, loss_ce: 0.005143
2022-01-10 04:01:23,427 iteration 4260 : loss : 0.017763, loss_ce: 0.005560
2022-01-10 04:01:24,775 iteration 4261 : loss : 0.014594, loss_ce: 0.005059
2022-01-10 04:01:26,163 iteration 4262 : loss : 0.018597, loss_ce: 0.008639
2022-01-10 04:01:27,609 iteration 4263 : loss : 0.028599, loss_ce: 0.015802
2022-01-10 04:01:29,006 iteration 4264 : loss : 0.019495, loss_ce: 0.008562
2022-01-10 04:01:30,298 iteration 4265 : loss : 0.017218, loss_ce: 0.007750
2022-01-10 04:01:31,726 iteration 4266 : loss : 0.025137, loss_ce: 0.008465
2022-01-10 04:01:33,168 iteration 4267 : loss : 0.026179, loss_ce: 0.008986
 63%|████████████████▉          | 251/400 [1:49:39<1:05:39, 26.44s/it]2022-01-10 04:01:34,565 iteration 4268 : loss : 0.021326, loss_ce: 0.008568
2022-01-10 04:01:35,910 iteration 4269 : loss : 0.015338, loss_ce: 0.004575
2022-01-10 04:01:37,295 iteration 4270 : loss : 0.032138, loss_ce: 0.012109
2022-01-10 04:01:38,680 iteration 4271 : loss : 0.021770, loss_ce: 0.007442
2022-01-10 04:01:40,070 iteration 4272 : loss : 0.021619, loss_ce: 0.009211
2022-01-10 04:01:41,441 iteration 4273 : loss : 0.018353, loss_ce: 0.007981
2022-01-10 04:01:42,842 iteration 4274 : loss : 0.021138, loss_ce: 0.009580
2022-01-10 04:01:44,223 iteration 4275 : loss : 0.017021, loss_ce: 0.006794
2022-01-10 04:01:45,597 iteration 4276 : loss : 0.018280, loss_ce: 0.006927
2022-01-10 04:01:47,004 iteration 4277 : loss : 0.023822, loss_ce: 0.010411
2022-01-10 04:01:48,393 iteration 4278 : loss : 0.021309, loss_ce: 0.005680
2022-01-10 04:01:49,753 iteration 4279 : loss : 0.018811, loss_ce: 0.008085
2022-01-10 04:01:51,164 iteration 4280 : loss : 0.024347, loss_ce: 0.006526
2022-01-10 04:01:52,575 iteration 4281 : loss : 0.026130, loss_ce: 0.010226
2022-01-10 04:01:54,012 iteration 4282 : loss : 0.030258, loss_ce: 0.011812
2022-01-10 04:01:55,494 iteration 4283 : loss : 0.023891, loss_ce: 0.012584
2022-01-10 04:01:56,975 iteration 4284 : loss : 0.027791, loss_ce: 0.010621
 63%|█████████████████          | 252/400 [1:50:03<1:03:16, 25.65s/it]2022-01-10 04:01:58,385 iteration 4285 : loss : 0.016088, loss_ce: 0.007967
2022-01-10 04:01:59,749 iteration 4286 : loss : 0.020510, loss_ce: 0.008043
2022-01-10 04:02:01,160 iteration 4287 : loss : 0.020953, loss_ce: 0.008796
2022-01-10 04:02:02,519 iteration 4288 : loss : 0.015990, loss_ce: 0.004589
2022-01-10 04:02:03,879 iteration 4289 : loss : 0.017735, loss_ce: 0.007354
2022-01-10 04:02:05,279 iteration 4290 : loss : 0.019324, loss_ce: 0.007454
2022-01-10 04:02:06,743 iteration 4291 : loss : 0.018288, loss_ce: 0.007954
2022-01-10 04:02:08,182 iteration 4292 : loss : 0.023282, loss_ce: 0.011071
2022-01-10 04:02:09,499 iteration 4293 : loss : 0.020165, loss_ce: 0.007584
2022-01-10 04:02:10,866 iteration 4294 : loss : 0.015824, loss_ce: 0.007165
2022-01-10 04:02:12,399 iteration 4295 : loss : 0.028977, loss_ce: 0.010930
2022-01-10 04:02:13,781 iteration 4296 : loss : 0.027907, loss_ce: 0.009121
2022-01-10 04:02:15,149 iteration 4297 : loss : 0.015468, loss_ce: 0.004737
2022-01-10 04:02:16,613 iteration 4298 : loss : 0.019816, loss_ce: 0.005717
2022-01-10 04:02:18,048 iteration 4299 : loss : 0.020873, loss_ce: 0.006156
2022-01-10 04:02:19,551 iteration 4300 : loss : 0.029183, loss_ce: 0.011368
2022-01-10 04:02:20,964 iteration 4301 : loss : 0.029224, loss_ce: 0.011529
 63%|█████████████████          | 253/400 [1:50:27<1:01:37, 25.15s/it]2022-01-10 04:02:22,495 iteration 4302 : loss : 0.024021, loss_ce: 0.008937
2022-01-10 04:02:23,839 iteration 4303 : loss : 0.019312, loss_ce: 0.009658
2022-01-10 04:02:25,243 iteration 4304 : loss : 0.023861, loss_ce: 0.009403
2022-01-10 04:02:26,665 iteration 4305 : loss : 0.017145, loss_ce: 0.004740
2022-01-10 04:02:28,069 iteration 4306 : loss : 0.028621, loss_ce: 0.011706
2022-01-10 04:02:29,544 iteration 4307 : loss : 0.018549, loss_ce: 0.005854
2022-01-10 04:02:30,937 iteration 4308 : loss : 0.020211, loss_ce: 0.007363
2022-01-10 04:02:32,386 iteration 4309 : loss : 0.021985, loss_ce: 0.007857
2022-01-10 04:02:33,737 iteration 4310 : loss : 0.015287, loss_ce: 0.005239
2022-01-10 04:02:35,229 iteration 4311 : loss : 0.024253, loss_ce: 0.010726
2022-01-10 04:02:36,617 iteration 4312 : loss : 0.021779, loss_ce: 0.009754
2022-01-10 04:02:38,073 iteration 4313 : loss : 0.025326, loss_ce: 0.013275
2022-01-10 04:02:39,498 iteration 4314 : loss : 0.022556, loss_ce: 0.008462
2022-01-10 04:02:40,879 iteration 4315 : loss : 0.015502, loss_ce: 0.005438
2022-01-10 04:02:42,260 iteration 4316 : loss : 0.025811, loss_ce: 0.009334
2022-01-10 04:02:43,677 iteration 4317 : loss : 0.016445, loss_ce: 0.006728
2022-01-10 04:02:45,183 iteration 4318 : loss : 0.025696, loss_ce: 0.010296
 64%|█████████████████▏         | 254/400 [1:50:51<1:00:31, 24.87s/it]2022-01-10 04:02:46,632 iteration 4319 : loss : 0.020393, loss_ce: 0.007587
2022-01-10 04:02:47,946 iteration 4320 : loss : 0.016709, loss_ce: 0.006976
2022-01-10 04:02:49,488 iteration 4321 : loss : 0.030491, loss_ce: 0.015223
2022-01-10 04:02:50,890 iteration 4322 : loss : 0.038867, loss_ce: 0.014288
2022-01-10 04:02:52,296 iteration 4323 : loss : 0.026571, loss_ce: 0.008444
2022-01-10 04:02:53,679 iteration 4324 : loss : 0.015222, loss_ce: 0.006813
2022-01-10 04:02:55,058 iteration 4325 : loss : 0.045655, loss_ce: 0.017559
2022-01-10 04:02:56,504 iteration 4326 : loss : 0.029937, loss_ce: 0.009025
2022-01-10 04:02:57,987 iteration 4327 : loss : 0.018957, loss_ce: 0.009169
2022-01-10 04:02:59,430 iteration 4328 : loss : 0.020718, loss_ce: 0.010004
2022-01-10 04:03:00,816 iteration 4329 : loss : 0.017696, loss_ce: 0.007259
2022-01-10 04:03:02,168 iteration 4330 : loss : 0.018669, loss_ce: 0.008065
2022-01-10 04:03:03,545 iteration 4331 : loss : 0.021395, loss_ce: 0.009239
2022-01-10 04:03:04,891 iteration 4332 : loss : 0.018363, loss_ce: 0.006050
2022-01-10 04:03:06,203 iteration 4333 : loss : 0.017763, loss_ce: 0.006973
2022-01-10 04:03:07,620 iteration 4334 : loss : 0.031434, loss_ce: 0.009675
2022-01-10 04:03:07,620 Training Data Eval:
2022-01-10 04:03:14,666   Average segmentation loss on training set: 0.0129
2022-01-10 04:03:14,666 Validation Data Eval:
2022-01-10 04:03:17,108   Average segmentation loss on validation set: 0.0756
2022-01-10 04:03:18,540 iteration 4335 : loss : 0.049154, loss_ce: 0.011342
 64%|█████████████████▏         | 255/400 [1:51:25<1:06:15, 27.42s/it]2022-01-10 04:03:19,925 iteration 4336 : loss : 0.017710, loss_ce: 0.007521
2022-01-10 04:03:21,351 iteration 4337 : loss : 0.015613, loss_ce: 0.005965
2022-01-10 04:03:22,779 iteration 4338 : loss : 0.018834, loss_ce: 0.007813
2022-01-10 04:03:24,109 iteration 4339 : loss : 0.026635, loss_ce: 0.007079
2022-01-10 04:03:25,528 iteration 4340 : loss : 0.029896, loss_ce: 0.007429
2022-01-10 04:03:26,923 iteration 4341 : loss : 0.021193, loss_ce: 0.007800
2022-01-10 04:03:28,305 iteration 4342 : loss : 0.018402, loss_ce: 0.006950
2022-01-10 04:03:29,775 iteration 4343 : loss : 0.036410, loss_ce: 0.009954
2022-01-10 04:03:31,141 iteration 4344 : loss : 0.022149, loss_ce: 0.008456
2022-01-10 04:03:32,508 iteration 4345 : loss : 0.015435, loss_ce: 0.006369
2022-01-10 04:03:33,803 iteration 4346 : loss : 0.012729, loss_ce: 0.004193
2022-01-10 04:03:35,256 iteration 4347 : loss : 0.026868, loss_ce: 0.010483
2022-01-10 04:03:36,704 iteration 4348 : loss : 0.020916, loss_ce: 0.010258
2022-01-10 04:03:38,047 iteration 4349 : loss : 0.019168, loss_ce: 0.007878
2022-01-10 04:03:39,388 iteration 4350 : loss : 0.017417, loss_ce: 0.006226
2022-01-10 04:03:40,850 iteration 4351 : loss : 0.019538, loss_ce: 0.008477
2022-01-10 04:03:42,285 iteration 4352 : loss : 0.021097, loss_ce: 0.008639
 64%|█████████████████▎         | 256/400 [1:51:48<1:03:09, 26.32s/it]2022-01-10 04:03:43,714 iteration 4353 : loss : 0.015761, loss_ce: 0.006034
2022-01-10 04:03:45,183 iteration 4354 : loss : 0.021990, loss_ce: 0.007347
2022-01-10 04:03:46,615 iteration 4355 : loss : 0.019786, loss_ce: 0.006921
2022-01-10 04:03:47,981 iteration 4356 : loss : 0.018893, loss_ce: 0.007451
2022-01-10 04:03:49,453 iteration 4357 : loss : 0.025432, loss_ce: 0.009513
2022-01-10 04:03:50,828 iteration 4358 : loss : 0.017201, loss_ce: 0.006836
2022-01-10 04:03:52,263 iteration 4359 : loss : 0.026445, loss_ce: 0.007560
2022-01-10 04:03:53,607 iteration 4360 : loss : 0.015501, loss_ce: 0.006317
2022-01-10 04:03:55,062 iteration 4361 : loss : 0.030257, loss_ce: 0.011484
2022-01-10 04:03:56,413 iteration 4362 : loss : 0.019186, loss_ce: 0.006029
2022-01-10 04:03:57,847 iteration 4363 : loss : 0.017514, loss_ce: 0.006702
2022-01-10 04:03:59,247 iteration 4364 : loss : 0.020985, loss_ce: 0.010194
2022-01-10 04:04:00,694 iteration 4365 : loss : 0.045030, loss_ce: 0.014213
2022-01-10 04:04:02,065 iteration 4366 : loss : 0.016238, loss_ce: 0.006367
2022-01-10 04:04:03,404 iteration 4367 : loss : 0.021819, loss_ce: 0.010080
2022-01-10 04:04:04,777 iteration 4368 : loss : 0.013498, loss_ce: 0.004815
2022-01-10 04:04:06,228 iteration 4369 : loss : 0.024685, loss_ce: 0.010169
 64%|█████████████████▎         | 257/400 [1:52:12<1:01:01, 25.60s/it]2022-01-10 04:04:07,646 iteration 4370 : loss : 0.023616, loss_ce: 0.008658
2022-01-10 04:04:09,205 iteration 4371 : loss : 0.025483, loss_ce: 0.009761
2022-01-10 04:04:10,653 iteration 4372 : loss : 0.031283, loss_ce: 0.009458
2022-01-10 04:04:12,102 iteration 4373 : loss : 0.030548, loss_ce: 0.011966
2022-01-10 04:04:13,485 iteration 4374 : loss : 0.015622, loss_ce: 0.006217
2022-01-10 04:04:14,895 iteration 4375 : loss : 0.015344, loss_ce: 0.006403
2022-01-10 04:04:16,327 iteration 4376 : loss : 0.025019, loss_ce: 0.009499
2022-01-10 04:04:17,694 iteration 4377 : loss : 0.015264, loss_ce: 0.005551
2022-01-10 04:04:19,143 iteration 4378 : loss : 0.025997, loss_ce: 0.011358
2022-01-10 04:04:20,524 iteration 4379 : loss : 0.023971, loss_ce: 0.009335
2022-01-10 04:04:21,937 iteration 4380 : loss : 0.031346, loss_ce: 0.010154
2022-01-10 04:04:23,369 iteration 4381 : loss : 0.027234, loss_ce: 0.007015
2022-01-10 04:04:24,873 iteration 4382 : loss : 0.043403, loss_ce: 0.017977
2022-01-10 04:04:26,285 iteration 4383 : loss : 0.028092, loss_ce: 0.009612
2022-01-10 04:04:27,745 iteration 4384 : loss : 0.018038, loss_ce: 0.006353
2022-01-10 04:04:29,212 iteration 4385 : loss : 0.022779, loss_ce: 0.010753
2022-01-10 04:04:30,599 iteration 4386 : loss : 0.022378, loss_ce: 0.006075
 64%|██████████████████▋          | 258/400 [1:52:37<59:43, 25.23s/it]2022-01-10 04:04:32,135 iteration 4387 : loss : 0.036685, loss_ce: 0.009887
2022-01-10 04:04:33,562 iteration 4388 : loss : 0.021301, loss_ce: 0.010289
2022-01-10 04:04:34,987 iteration 4389 : loss : 0.020236, loss_ce: 0.008510
2022-01-10 04:04:36,438 iteration 4390 : loss : 0.025922, loss_ce: 0.011700
2022-01-10 04:04:37,921 iteration 4391 : loss : 0.027591, loss_ce: 0.008943
2022-01-10 04:04:39,362 iteration 4392 : loss : 0.024761, loss_ce: 0.009722
2022-01-10 04:04:40,872 iteration 4393 : loss : 0.018435, loss_ce: 0.007127
2022-01-10 04:04:42,218 iteration 4394 : loss : 0.014408, loss_ce: 0.005562
2022-01-10 04:04:43,632 iteration 4395 : loss : 0.016125, loss_ce: 0.006628
2022-01-10 04:04:45,052 iteration 4396 : loss : 0.024270, loss_ce: 0.009228
2022-01-10 04:04:46,401 iteration 4397 : loss : 0.015759, loss_ce: 0.005735
2022-01-10 04:04:47,807 iteration 4398 : loss : 0.015399, loss_ce: 0.005935
2022-01-10 04:04:49,222 iteration 4399 : loss : 0.028736, loss_ce: 0.007468
2022-01-10 04:04:50,578 iteration 4400 : loss : 0.016897, loss_ce: 0.005429
2022-01-10 04:04:52,024 iteration 4401 : loss : 0.020504, loss_ce: 0.010189
2022-01-10 04:04:53,361 iteration 4402 : loss : 0.020885, loss_ce: 0.006729
2022-01-10 04:04:54,781 iteration 4403 : loss : 0.025856, loss_ce: 0.007968
 65%|██████████████████▊          | 259/400 [1:53:01<58:33, 24.92s/it]2022-01-10 04:04:56,316 iteration 4404 : loss : 0.030232, loss_ce: 0.014532
2022-01-10 04:04:57,728 iteration 4405 : loss : 0.022698, loss_ce: 0.009014
2022-01-10 04:04:59,171 iteration 4406 : loss : 0.018758, loss_ce: 0.008494
2022-01-10 04:05:00,600 iteration 4407 : loss : 0.033782, loss_ce: 0.008861
2022-01-10 04:05:01,976 iteration 4408 : loss : 0.022662, loss_ce: 0.012248
2022-01-10 04:05:03,334 iteration 4409 : loss : 0.023917, loss_ce: 0.008208
2022-01-10 04:05:04,831 iteration 4410 : loss : 0.024427, loss_ce: 0.008313
2022-01-10 04:05:06,255 iteration 4411 : loss : 0.027584, loss_ce: 0.010074
2022-01-10 04:05:07,673 iteration 4412 : loss : 0.018698, loss_ce: 0.006663
2022-01-10 04:05:09,102 iteration 4413 : loss : 0.015435, loss_ce: 0.006492
2022-01-10 04:05:10,484 iteration 4414 : loss : 0.031992, loss_ce: 0.009451
2022-01-10 04:05:11,840 iteration 4415 : loss : 0.018070, loss_ce: 0.008374
2022-01-10 04:05:13,271 iteration 4416 : loss : 0.021813, loss_ce: 0.007410
2022-01-10 04:05:14,698 iteration 4417 : loss : 0.021864, loss_ce: 0.010547
2022-01-10 04:05:16,122 iteration 4418 : loss : 0.016315, loss_ce: 0.006393
2022-01-10 04:05:17,515 iteration 4419 : loss : 0.019237, loss_ce: 0.006492
2022-01-10 04:05:17,516 Training Data Eval:
2022-01-10 04:05:24,565   Average segmentation loss on training set: 0.0139
2022-01-10 04:05:24,565 Validation Data Eval:
2022-01-10 04:05:27,011   Average segmentation loss on validation set: 0.0836
2022-01-10 04:05:28,394 iteration 4420 : loss : 0.016236, loss_ce: 0.005143
 65%|█████████████████▌         | 260/400 [1:53:34<1:04:13, 27.53s/it]2022-01-10 04:05:29,855 iteration 4421 : loss : 0.026066, loss_ce: 0.007621
2022-01-10 04:05:31,244 iteration 4422 : loss : 0.015137, loss_ce: 0.005376
2022-01-10 04:05:32,625 iteration 4423 : loss : 0.016619, loss_ce: 0.005904
2022-01-10 04:05:34,073 iteration 4424 : loss : 0.020537, loss_ce: 0.007479
2022-01-10 04:05:35,474 iteration 4425 : loss : 0.027649, loss_ce: 0.011497
2022-01-10 04:05:36,862 iteration 4426 : loss : 0.015116, loss_ce: 0.004104
2022-01-10 04:05:38,279 iteration 4427 : loss : 0.019284, loss_ce: 0.008364
2022-01-10 04:05:39,709 iteration 4428 : loss : 0.025421, loss_ce: 0.013230
2022-01-10 04:05:41,018 iteration 4429 : loss : 0.024185, loss_ce: 0.011482
2022-01-10 04:05:42,431 iteration 4430 : loss : 0.034654, loss_ce: 0.015349
2022-01-10 04:05:43,877 iteration 4431 : loss : 0.023766, loss_ce: 0.007398
2022-01-10 04:05:45,345 iteration 4432 : loss : 0.025797, loss_ce: 0.011666
2022-01-10 04:05:46,776 iteration 4433 : loss : 0.023108, loss_ce: 0.005578
2022-01-10 04:05:48,221 iteration 4434 : loss : 0.022351, loss_ce: 0.008978
2022-01-10 04:05:49,587 iteration 4435 : loss : 0.015418, loss_ce: 0.004837
2022-01-10 04:05:50,973 iteration 4436 : loss : 0.017237, loss_ce: 0.008225
2022-01-10 04:05:52,364 iteration 4437 : loss : 0.017793, loss_ce: 0.006820
 65%|█████████████████▌         | 261/400 [1:53:58<1:01:17, 26.46s/it]2022-01-10 04:05:53,917 iteration 4438 : loss : 0.029357, loss_ce: 0.011750
2022-01-10 04:05:55,254 iteration 4439 : loss : 0.014861, loss_ce: 0.006566
2022-01-10 04:05:56,720 iteration 4440 : loss : 0.023432, loss_ce: 0.010704
2022-01-10 04:05:58,135 iteration 4441 : loss : 0.019323, loss_ce: 0.007325
2022-01-10 04:05:59,637 iteration 4442 : loss : 0.023086, loss_ce: 0.010026
2022-01-10 04:06:01,119 iteration 4443 : loss : 0.025324, loss_ce: 0.009868
2022-01-10 04:06:02,558 iteration 4444 : loss : 0.021439, loss_ce: 0.007857
2022-01-10 04:06:03,942 iteration 4445 : loss : 0.022232, loss_ce: 0.008772
2022-01-10 04:06:05,410 iteration 4446 : loss : 0.043179, loss_ce: 0.008267
2022-01-10 04:06:06,907 iteration 4447 : loss : 0.019205, loss_ce: 0.005612
2022-01-10 04:06:08,304 iteration 4448 : loss : 0.017557, loss_ce: 0.005894
2022-01-10 04:06:09,806 iteration 4449 : loss : 0.027122, loss_ce: 0.013171
2022-01-10 04:06:11,208 iteration 4450 : loss : 0.019172, loss_ce: 0.011276
2022-01-10 04:06:12,653 iteration 4451 : loss : 0.014638, loss_ce: 0.005566
2022-01-10 04:06:14,027 iteration 4452 : loss : 0.030552, loss_ce: 0.008413
2022-01-10 04:06:15,389 iteration 4453 : loss : 0.017720, loss_ce: 0.007742
2022-01-10 04:06:16,772 iteration 4454 : loss : 0.021151, loss_ce: 0.007562
 66%|██████████████████▉          | 262/400 [1:54:23<59:26, 25.84s/it]2022-01-10 04:06:18,206 iteration 4455 : loss : 0.024592, loss_ce: 0.009469
2022-01-10 04:06:19,544 iteration 4456 : loss : 0.024924, loss_ce: 0.008872
2022-01-10 04:06:20,938 iteration 4457 : loss : 0.021343, loss_ce: 0.009478
2022-01-10 04:06:22,305 iteration 4458 : loss : 0.020133, loss_ce: 0.007743
2022-01-10 04:06:23,745 iteration 4459 : loss : 0.046489, loss_ce: 0.013704
2022-01-10 04:06:25,106 iteration 4460 : loss : 0.014327, loss_ce: 0.005791
2022-01-10 04:06:26,490 iteration 4461 : loss : 0.017769, loss_ce: 0.007720
2022-01-10 04:06:27,970 iteration 4462 : loss : 0.027347, loss_ce: 0.007948
2022-01-10 04:06:29,318 iteration 4463 : loss : 0.019365, loss_ce: 0.007150
2022-01-10 04:06:30,699 iteration 4464 : loss : 0.016682, loss_ce: 0.007866
2022-01-10 04:06:32,206 iteration 4465 : loss : 0.036307, loss_ce: 0.006471
2022-01-10 04:06:33,542 iteration 4466 : loss : 0.014455, loss_ce: 0.005242
2022-01-10 04:06:34,965 iteration 4467 : loss : 0.021585, loss_ce: 0.009443
2022-01-10 04:06:36,365 iteration 4468 : loss : 0.026937, loss_ce: 0.009094
2022-01-10 04:06:37,763 iteration 4469 : loss : 0.016574, loss_ce: 0.004732
2022-01-10 04:06:39,157 iteration 4470 : loss : 0.029508, loss_ce: 0.014636
2022-01-10 04:06:40,607 iteration 4471 : loss : 0.027357, loss_ce: 0.007310
 66%|███████████████████          | 263/400 [1:54:47<57:37, 25.24s/it]2022-01-10 04:06:42,029 iteration 4472 : loss : 0.021764, loss_ce: 0.010009
2022-01-10 04:06:43,473 iteration 4473 : loss : 0.025488, loss_ce: 0.010036
2022-01-10 04:06:44,975 iteration 4474 : loss : 0.029147, loss_ce: 0.011874
2022-01-10 04:06:46,413 iteration 4475 : loss : 0.022738, loss_ce: 0.009242
2022-01-10 04:06:47,805 iteration 4476 : loss : 0.022012, loss_ce: 0.007865
2022-01-10 04:06:49,190 iteration 4477 : loss : 0.021312, loss_ce: 0.008359
2022-01-10 04:06:50,685 iteration 4478 : loss : 0.024237, loss_ce: 0.011126
2022-01-10 04:06:52,081 iteration 4479 : loss : 0.028987, loss_ce: 0.014108
2022-01-10 04:06:53,425 iteration 4480 : loss : 0.017641, loss_ce: 0.006740
2022-01-10 04:06:54,887 iteration 4481 : loss : 0.025366, loss_ce: 0.005123
2022-01-10 04:06:56,295 iteration 4482 : loss : 0.021762, loss_ce: 0.007099
2022-01-10 04:06:57,660 iteration 4483 : loss : 0.023564, loss_ce: 0.007032
2022-01-10 04:06:59,137 iteration 4484 : loss : 0.026662, loss_ce: 0.009995
2022-01-10 04:07:00,603 iteration 4485 : loss : 0.025556, loss_ce: 0.011721
2022-01-10 04:07:02,075 iteration 4486 : loss : 0.024878, loss_ce: 0.008343
2022-01-10 04:07:03,442 iteration 4487 : loss : 0.015838, loss_ce: 0.004584
2022-01-10 04:07:04,826 iteration 4488 : loss : 0.026726, loss_ce: 0.010218
 66%|███████████████████▏         | 264/400 [1:55:11<56:31, 24.93s/it]2022-01-10 04:07:06,328 iteration 4489 : loss : 0.031007, loss_ce: 0.019561
2022-01-10 04:07:07,692 iteration 4490 : loss : 0.014855, loss_ce: 0.007968
2022-01-10 04:07:09,139 iteration 4491 : loss : 0.045487, loss_ce: 0.018164
2022-01-10 04:07:10,639 iteration 4492 : loss : 0.029238, loss_ce: 0.014557
2022-01-10 04:07:12,028 iteration 4493 : loss : 0.022501, loss_ce: 0.008186
2022-01-10 04:07:13,431 iteration 4494 : loss : 0.028380, loss_ce: 0.007694
2022-01-10 04:07:14,853 iteration 4495 : loss : 0.034452, loss_ce: 0.012966
2022-01-10 04:07:16,231 iteration 4496 : loss : 0.019113, loss_ce: 0.008304
2022-01-10 04:07:17,702 iteration 4497 : loss : 0.022437, loss_ce: 0.010295
2022-01-10 04:07:19,106 iteration 4498 : loss : 0.038684, loss_ce: 0.012697
2022-01-10 04:07:20,543 iteration 4499 : loss : 0.029902, loss_ce: 0.010557
2022-01-10 04:07:21,893 iteration 4500 : loss : 0.015130, loss_ce: 0.005218
2022-01-10 04:07:23,261 iteration 4501 : loss : 0.022471, loss_ce: 0.007440
2022-01-10 04:07:24,637 iteration 4502 : loss : 0.021406, loss_ce: 0.006507
2022-01-10 04:07:26,121 iteration 4503 : loss : 0.018947, loss_ce: 0.007463
2022-01-10 04:07:27,522 iteration 4504 : loss : 0.018756, loss_ce: 0.006940
2022-01-10 04:07:27,522 Training Data Eval:
2022-01-10 04:07:34,581   Average segmentation loss on training set: 0.0143
2022-01-10 04:07:34,581 Validation Data Eval:
2022-01-10 04:07:37,040   Average segmentation loss on validation set: 0.1153
2022-01-10 04:07:38,486 iteration 4505 : loss : 0.020512, loss_ce: 0.008171
 66%|█████████████████▉         | 265/400 [1:55:44<1:01:59, 27.55s/it]2022-01-10 04:07:39,933 iteration 4506 : loss : 0.027246, loss_ce: 0.010639
2022-01-10 04:07:41,359 iteration 4507 : loss : 0.019538, loss_ce: 0.008432
2022-01-10 04:07:42,743 iteration 4508 : loss : 0.016117, loss_ce: 0.005458
2022-01-10 04:07:44,111 iteration 4509 : loss : 0.037665, loss_ce: 0.008840
2022-01-10 04:07:45,589 iteration 4510 : loss : 0.029799, loss_ce: 0.010296
2022-01-10 04:07:46,984 iteration 4511 : loss : 0.017421, loss_ce: 0.004695
2022-01-10 04:07:48,455 iteration 4512 : loss : 0.020624, loss_ce: 0.010806
2022-01-10 04:07:49,957 iteration 4513 : loss : 0.026217, loss_ce: 0.010080
2022-01-10 04:07:51,293 iteration 4514 : loss : 0.020606, loss_ce: 0.005698
2022-01-10 04:07:52,695 iteration 4515 : loss : 0.026272, loss_ce: 0.008684
2022-01-10 04:07:54,055 iteration 4516 : loss : 0.017389, loss_ce: 0.006745
2022-01-10 04:07:55,422 iteration 4517 : loss : 0.016700, loss_ce: 0.006131
2022-01-10 04:07:56,855 iteration 4518 : loss : 0.020572, loss_ce: 0.007639
2022-01-10 04:07:58,188 iteration 4519 : loss : 0.017572, loss_ce: 0.006437
2022-01-10 04:07:59,627 iteration 4520 : loss : 0.026029, loss_ce: 0.010192
2022-01-10 04:08:01,002 iteration 4521 : loss : 0.028776, loss_ce: 0.013356
2022-01-10 04:08:02,415 iteration 4522 : loss : 0.022999, loss_ce: 0.007754
 66%|███████████████████▎         | 266/400 [1:56:08<59:06, 26.47s/it]2022-01-10 04:08:03,958 iteration 4523 : loss : 0.025895, loss_ce: 0.013045
2022-01-10 04:08:05,335 iteration 4524 : loss : 0.019366, loss_ce: 0.007123
2022-01-10 04:08:06,665 iteration 4525 : loss : 0.019316, loss_ce: 0.008980
2022-01-10 04:08:08,038 iteration 4526 : loss : 0.022431, loss_ce: 0.007597
2022-01-10 04:08:09,413 iteration 4527 : loss : 0.012608, loss_ce: 0.005156
2022-01-10 04:08:10,821 iteration 4528 : loss : 0.013768, loss_ce: 0.005148
2022-01-10 04:08:12,157 iteration 4529 : loss : 0.019791, loss_ce: 0.007792
2022-01-10 04:08:13,559 iteration 4530 : loss : 0.019987, loss_ce: 0.008551
2022-01-10 04:08:15,060 iteration 4531 : loss : 0.047168, loss_ce: 0.013906
2022-01-10 04:08:16,399 iteration 4532 : loss : 0.015370, loss_ce: 0.005468
2022-01-10 04:08:17,944 iteration 4533 : loss : 0.025855, loss_ce: 0.011133
2022-01-10 04:08:19,317 iteration 4534 : loss : 0.015841, loss_ce: 0.007357
2022-01-10 04:08:20,754 iteration 4535 : loss : 0.028289, loss_ce: 0.013643
2022-01-10 04:08:22,173 iteration 4536 : loss : 0.019467, loss_ce: 0.005751
2022-01-10 04:08:23,548 iteration 4537 : loss : 0.020666, loss_ce: 0.005913
2022-01-10 04:08:24,937 iteration 4538 : loss : 0.018420, loss_ce: 0.008507
2022-01-10 04:08:26,248 iteration 4539 : loss : 0.013067, loss_ce: 0.004713
 67%|███████████████████▎         | 267/400 [1:56:32<56:55, 25.68s/it]2022-01-10 04:08:27,662 iteration 4540 : loss : 0.016492, loss_ce: 0.007407
2022-01-10 04:08:29,061 iteration 4541 : loss : 0.030589, loss_ce: 0.011192
2022-01-10 04:08:30,467 iteration 4542 : loss : 0.023349, loss_ce: 0.007527
2022-01-10 04:08:32,024 iteration 4543 : loss : 0.023510, loss_ce: 0.009368
2022-01-10 04:08:33,380 iteration 4544 : loss : 0.022737, loss_ce: 0.009743
2022-01-10 04:08:34,733 iteration 4545 : loss : 0.016299, loss_ce: 0.006687
2022-01-10 04:08:36,121 iteration 4546 : loss : 0.020641, loss_ce: 0.007640
2022-01-10 04:08:37,538 iteration 4547 : loss : 0.016188, loss_ce: 0.006011
2022-01-10 04:08:38,921 iteration 4548 : loss : 0.022489, loss_ce: 0.009964
2022-01-10 04:08:40,252 iteration 4549 : loss : 0.020880, loss_ce: 0.006164
2022-01-10 04:08:41,729 iteration 4550 : loss : 0.027905, loss_ce: 0.010460
2022-01-10 04:08:43,207 iteration 4551 : loss : 0.028239, loss_ce: 0.011350
2022-01-10 04:08:44,614 iteration 4552 : loss : 0.018919, loss_ce: 0.006377
2022-01-10 04:08:46,003 iteration 4553 : loss : 0.013088, loss_ce: 0.004213
2022-01-10 04:08:47,454 iteration 4554 : loss : 0.027214, loss_ce: 0.007334
2022-01-10 04:08:48,816 iteration 4555 : loss : 0.024146, loss_ce: 0.008911
2022-01-10 04:08:50,218 iteration 4556 : loss : 0.016250, loss_ce: 0.006694
 67%|███████████████████▍         | 268/400 [1:56:56<55:21, 25.16s/it]2022-01-10 04:08:51,664 iteration 4557 : loss : 0.014853, loss_ce: 0.007596
2022-01-10 04:08:53,084 iteration 4558 : loss : 0.022582, loss_ce: 0.010907
2022-01-10 04:08:54,527 iteration 4559 : loss : 0.015942, loss_ce: 0.004706
2022-01-10 04:08:55,939 iteration 4560 : loss : 0.025387, loss_ce: 0.009017
2022-01-10 04:08:57,381 iteration 4561 : loss : 0.020000, loss_ce: 0.005138
2022-01-10 04:08:58,756 iteration 4562 : loss : 0.020565, loss_ce: 0.006586
2022-01-10 04:09:00,186 iteration 4563 : loss : 0.029370, loss_ce: 0.015491
2022-01-10 04:09:01,640 iteration 4564 : loss : 0.026468, loss_ce: 0.009919
2022-01-10 04:09:03,131 iteration 4565 : loss : 0.023352, loss_ce: 0.008125
2022-01-10 04:09:04,589 iteration 4566 : loss : 0.022223, loss_ce: 0.011800
2022-01-10 04:09:05,985 iteration 4567 : loss : 0.017912, loss_ce: 0.006462
2022-01-10 04:09:07,383 iteration 4568 : loss : 0.019793, loss_ce: 0.006311
2022-01-10 04:09:08,876 iteration 4569 : loss : 0.069291, loss_ce: 0.012657
2022-01-10 04:09:10,381 iteration 4570 : loss : 0.051002, loss_ce: 0.028754
2022-01-10 04:09:11,781 iteration 4571 : loss : 0.019623, loss_ce: 0.007671
2022-01-10 04:09:13,131 iteration 4572 : loss : 0.019860, loss_ce: 0.007226
2022-01-10 04:09:14,566 iteration 4573 : loss : 0.029482, loss_ce: 0.012819
 67%|███████████████████▌         | 269/400 [1:57:21<54:24, 24.92s/it]2022-01-10 04:09:15,957 iteration 4574 : loss : 0.017867, loss_ce: 0.007790
2022-01-10 04:09:17,414 iteration 4575 : loss : 0.024372, loss_ce: 0.006777
2022-01-10 04:09:18,948 iteration 4576 : loss : 0.028543, loss_ce: 0.010939
2022-01-10 04:09:20,381 iteration 4577 : loss : 0.023524, loss_ce: 0.008014
2022-01-10 04:09:21,718 iteration 4578 : loss : 0.020089, loss_ce: 0.008468
2022-01-10 04:09:23,190 iteration 4579 : loss : 0.018941, loss_ce: 0.006814
2022-01-10 04:09:24,602 iteration 4580 : loss : 0.019752, loss_ce: 0.009072
2022-01-10 04:09:26,020 iteration 4581 : loss : 0.029903, loss_ce: 0.013291
2022-01-10 04:09:27,419 iteration 4582 : loss : 0.021701, loss_ce: 0.008718
2022-01-10 04:09:28,924 iteration 4583 : loss : 0.039494, loss_ce: 0.009408
2022-01-10 04:09:30,333 iteration 4584 : loss : 0.044316, loss_ce: 0.010951
2022-01-10 04:09:31,693 iteration 4585 : loss : 0.014560, loss_ce: 0.005266
2022-01-10 04:09:33,084 iteration 4586 : loss : 0.019078, loss_ce: 0.008339
2022-01-10 04:09:34,492 iteration 4587 : loss : 0.018573, loss_ce: 0.007255
2022-01-10 04:09:35,967 iteration 4588 : loss : 0.022570, loss_ce: 0.008678
2022-01-10 04:09:37,377 iteration 4589 : loss : 0.021740, loss_ce: 0.009612
2022-01-10 04:09:37,377 Training Data Eval:
2022-01-10 04:09:44,538   Average segmentation loss on training set: 0.0154
2022-01-10 04:09:44,538 Validation Data Eval:
2022-01-10 04:09:47,017   Average segmentation loss on validation set: 0.0995
2022-01-10 04:09:48,469 iteration 4590 : loss : 0.038887, loss_ce: 0.026180
 68%|███████████████████▌         | 270/400 [1:57:54<59:49, 27.61s/it]2022-01-10 04:09:50,082 iteration 4591 : loss : 0.048665, loss_ce: 0.021247
2022-01-10 04:09:51,494 iteration 4592 : loss : 0.020479, loss_ce: 0.009237
2022-01-10 04:09:52,911 iteration 4593 : loss : 0.029623, loss_ce: 0.010364
2022-01-10 04:09:54,331 iteration 4594 : loss : 0.021277, loss_ce: 0.008650
2022-01-10 04:09:55,742 iteration 4595 : loss : 0.015000, loss_ce: 0.006000
2022-01-10 04:09:57,255 iteration 4596 : loss : 0.023150, loss_ce: 0.010672
2022-01-10 04:09:58,619 iteration 4597 : loss : 0.025937, loss_ce: 0.007578
2022-01-10 04:09:59,945 iteration 4598 : loss : 0.019498, loss_ce: 0.005536
2022-01-10 04:10:01,324 iteration 4599 : loss : 0.020957, loss_ce: 0.008489
2022-01-10 04:10:02,751 iteration 4600 : loss : 0.017202, loss_ce: 0.006315
2022-01-10 04:10:04,189 iteration 4601 : loss : 0.023494, loss_ce: 0.011861
2022-01-10 04:10:05,687 iteration 4602 : loss : 0.027853, loss_ce: 0.010242
2022-01-10 04:10:07,177 iteration 4603 : loss : 0.023913, loss_ce: 0.008132
2022-01-10 04:10:08,546 iteration 4604 : loss : 0.015009, loss_ce: 0.005299
2022-01-10 04:10:09,952 iteration 4605 : loss : 0.017085, loss_ce: 0.005009
2022-01-10 04:10:11,349 iteration 4606 : loss : 0.021337, loss_ce: 0.007803
2022-01-10 04:10:12,739 iteration 4607 : loss : 0.017559, loss_ce: 0.006572
 68%|███████████████████▋         | 271/400 [1:58:19<57:12, 26.61s/it]2022-01-10 04:10:14,229 iteration 4608 : loss : 0.017258, loss_ce: 0.006838
2022-01-10 04:10:15,662 iteration 4609 : loss : 0.016786, loss_ce: 0.005625
2022-01-10 04:10:17,097 iteration 4610 : loss : 0.032294, loss_ce: 0.007782
2022-01-10 04:10:18,559 iteration 4611 : loss : 0.028755, loss_ce: 0.011483
2022-01-10 04:10:19,983 iteration 4612 : loss : 0.026818, loss_ce: 0.011952
2022-01-10 04:10:21,409 iteration 4613 : loss : 0.018394, loss_ce: 0.006007
2022-01-10 04:10:22,827 iteration 4614 : loss : 0.016200, loss_ce: 0.004909
2022-01-10 04:10:24,360 iteration 4615 : loss : 0.029949, loss_ce: 0.013859
2022-01-10 04:10:25,719 iteration 4616 : loss : 0.021493, loss_ce: 0.009269
2022-01-10 04:10:27,151 iteration 4617 : loss : 0.027643, loss_ce: 0.009214
2022-01-10 04:10:28,534 iteration 4618 : loss : 0.014807, loss_ce: 0.006024
2022-01-10 04:10:29,963 iteration 4619 : loss : 0.019805, loss_ce: 0.008397
2022-01-10 04:10:31,349 iteration 4620 : loss : 0.017143, loss_ce: 0.006739
2022-01-10 04:10:32,735 iteration 4621 : loss : 0.019129, loss_ce: 0.006861
2022-01-10 04:10:34,164 iteration 4622 : loss : 0.025287, loss_ce: 0.009860
2022-01-10 04:10:35,583 iteration 4623 : loss : 0.015821, loss_ce: 0.006946
2022-01-10 04:10:36,963 iteration 4624 : loss : 0.016417, loss_ce: 0.005602
 68%|███████████████████▋         | 272/400 [1:58:43<55:14, 25.90s/it]2022-01-10 04:10:38,496 iteration 4625 : loss : 0.021012, loss_ce: 0.007754
2022-01-10 04:10:39,945 iteration 4626 : loss : 0.027988, loss_ce: 0.008404
2022-01-10 04:10:41,341 iteration 4627 : loss : 0.022534, loss_ce: 0.010145
2022-01-10 04:10:42,753 iteration 4628 : loss : 0.024227, loss_ce: 0.007847
2022-01-10 04:10:44,115 iteration 4629 : loss : 0.032513, loss_ce: 0.005772
2022-01-10 04:10:45,532 iteration 4630 : loss : 0.024195, loss_ce: 0.015702
2022-01-10 04:10:46,913 iteration 4631 : loss : 0.025088, loss_ce: 0.009285
2022-01-10 04:10:48,334 iteration 4632 : loss : 0.021071, loss_ce: 0.006134
2022-01-10 04:10:49,767 iteration 4633 : loss : 0.014954, loss_ce: 0.005140
2022-01-10 04:10:51,254 iteration 4634 : loss : 0.028406, loss_ce: 0.011234
2022-01-10 04:10:52,555 iteration 4635 : loss : 0.013580, loss_ce: 0.004467
2022-01-10 04:10:53,897 iteration 4636 : loss : 0.012783, loss_ce: 0.004883
2022-01-10 04:10:55,295 iteration 4637 : loss : 0.027869, loss_ce: 0.009500
2022-01-10 04:10:56,696 iteration 4638 : loss : 0.020051, loss_ce: 0.008611
2022-01-10 04:10:58,135 iteration 4639 : loss : 0.024757, loss_ce: 0.008792
2022-01-10 04:10:59,583 iteration 4640 : loss : 0.024368, loss_ce: 0.010524
2022-01-10 04:11:01,002 iteration 4641 : loss : 0.017947, loss_ce: 0.006521
 68%|███████████████████▊         | 273/400 [1:59:07<53:37, 25.34s/it]2022-01-10 04:11:02,468 iteration 4642 : loss : 0.016823, loss_ce: 0.006907
2022-01-10 04:11:03,928 iteration 4643 : loss : 0.026510, loss_ce: 0.009472
2022-01-10 04:11:05,357 iteration 4644 : loss : 0.023497, loss_ce: 0.010161
2022-01-10 04:11:06,733 iteration 4645 : loss : 0.028202, loss_ce: 0.010874
2022-01-10 04:11:08,135 iteration 4646 : loss : 0.017402, loss_ce: 0.007038
2022-01-10 04:11:09,550 iteration 4647 : loss : 0.016870, loss_ce: 0.007203
2022-01-10 04:11:10,924 iteration 4648 : loss : 0.016772, loss_ce: 0.007770
2022-01-10 04:11:12,459 iteration 4649 : loss : 0.022648, loss_ce: 0.007898
2022-01-10 04:11:13,761 iteration 4650 : loss : 0.015148, loss_ce: 0.006137
2022-01-10 04:11:15,180 iteration 4651 : loss : 0.023154, loss_ce: 0.008222
2022-01-10 04:11:16,578 iteration 4652 : loss : 0.017761, loss_ce: 0.006538
2022-01-10 04:11:17,909 iteration 4653 : loss : 0.018232, loss_ce: 0.006603
2022-01-10 04:11:19,257 iteration 4654 : loss : 0.014275, loss_ce: 0.004311
2022-01-10 04:11:20,719 iteration 4655 : loss : 0.023571, loss_ce: 0.007192
2022-01-10 04:11:22,122 iteration 4656 : loss : 0.018459, loss_ce: 0.005850
2022-01-10 04:11:23,532 iteration 4657 : loss : 0.030186, loss_ce: 0.017042
2022-01-10 04:11:24,922 iteration 4658 : loss : 0.023771, loss_ce: 0.007188
 68%|███████████████████▊         | 274/400 [1:59:31<52:18, 24.91s/it]2022-01-10 04:11:26,398 iteration 4659 : loss : 0.020370, loss_ce: 0.007217
2022-01-10 04:11:27,768 iteration 4660 : loss : 0.013419, loss_ce: 0.005198
2022-01-10 04:11:29,226 iteration 4661 : loss : 0.015415, loss_ce: 0.006405
2022-01-10 04:11:30,536 iteration 4662 : loss : 0.011943, loss_ce: 0.005410
2022-01-10 04:11:32,028 iteration 4663 : loss : 0.021167, loss_ce: 0.008606
2022-01-10 04:11:33,435 iteration 4664 : loss : 0.019460, loss_ce: 0.006310
2022-01-10 04:11:34,837 iteration 4665 : loss : 0.027700, loss_ce: 0.011013
2022-01-10 04:11:36,303 iteration 4666 : loss : 0.017471, loss_ce: 0.006819
2022-01-10 04:11:37,769 iteration 4667 : loss : 0.023710, loss_ce: 0.008907
2022-01-10 04:11:39,162 iteration 4668 : loss : 0.013806, loss_ce: 0.005596
2022-01-10 04:11:40,625 iteration 4669 : loss : 0.022700, loss_ce: 0.008119
2022-01-10 04:11:42,048 iteration 4670 : loss : 0.024946, loss_ce: 0.006668
2022-01-10 04:11:43,430 iteration 4671 : loss : 0.014870, loss_ce: 0.005238
2022-01-10 04:11:44,806 iteration 4672 : loss : 0.012582, loss_ce: 0.004540
2022-01-10 04:11:46,125 iteration 4673 : loss : 0.012575, loss_ce: 0.004225
2022-01-10 04:11:47,555 iteration 4674 : loss : 0.022521, loss_ce: 0.005911
2022-01-10 04:11:47,555 Training Data Eval:
2022-01-10 04:11:54,591   Average segmentation loss on training set: 0.0107
2022-01-10 04:11:54,591 Validation Data Eval:
2022-01-10 04:11:57,031   Average segmentation loss on validation set: 0.0934
2022-01-10 04:11:58,365 iteration 4675 : loss : 0.012752, loss_ce: 0.004907
 69%|███████████████████▉         | 275/400 [2:00:04<57:14, 27.47s/it]2022-01-10 04:11:59,836 iteration 4676 : loss : 0.012558, loss_ce: 0.004869
2022-01-10 04:12:01,201 iteration 4677 : loss : 0.015136, loss_ce: 0.006222
2022-01-10 04:12:02,648 iteration 4678 : loss : 0.017808, loss_ce: 0.006702
2022-01-10 04:12:04,139 iteration 4679 : loss : 0.020050, loss_ce: 0.008172
2022-01-10 04:12:05,479 iteration 4680 : loss : 0.017068, loss_ce: 0.006632
2022-01-10 04:12:06,876 iteration 4681 : loss : 0.017519, loss_ce: 0.007510
2022-01-10 04:12:08,263 iteration 4682 : loss : 0.016997, loss_ce: 0.006219
2022-01-10 04:12:09,633 iteration 4683 : loss : 0.014287, loss_ce: 0.004458
2022-01-10 04:12:11,035 iteration 4684 : loss : 0.026776, loss_ce: 0.008743
2022-01-10 04:12:12,485 iteration 4685 : loss : 0.015016, loss_ce: 0.006389
2022-01-10 04:12:13,842 iteration 4686 : loss : 0.015793, loss_ce: 0.005503
2022-01-10 04:12:15,252 iteration 4687 : loss : 0.023011, loss_ce: 0.006546
2022-01-10 04:12:16,570 iteration 4688 : loss : 0.011536, loss_ce: 0.004870
2022-01-10 04:12:18,000 iteration 4689 : loss : 0.024471, loss_ce: 0.008516
2022-01-10 04:12:19,322 iteration 4690 : loss : 0.014680, loss_ce: 0.004524
2022-01-10 04:12:20,720 iteration 4691 : loss : 0.023733, loss_ce: 0.007536
2022-01-10 04:12:22,228 iteration 4692 : loss : 0.023156, loss_ce: 0.008460
 69%|████████████████████         | 276/400 [2:00:28<54:32, 26.39s/it]2022-01-10 04:12:23,676 iteration 4693 : loss : 0.016397, loss_ce: 0.006200
2022-01-10 04:12:25,069 iteration 4694 : loss : 0.018152, loss_ce: 0.005899
2022-01-10 04:12:26,497 iteration 4695 : loss : 0.026716, loss_ce: 0.009568
2022-01-10 04:12:27,926 iteration 4696 : loss : 0.025012, loss_ce: 0.008590
2022-01-10 04:12:29,285 iteration 4697 : loss : 0.018107, loss_ce: 0.007355
2022-01-10 04:12:30,653 iteration 4698 : loss : 0.017738, loss_ce: 0.008318
2022-01-10 04:12:32,043 iteration 4699 : loss : 0.018210, loss_ce: 0.005131
2022-01-10 04:12:33,445 iteration 4700 : loss : 0.018617, loss_ce: 0.006623
2022-01-10 04:12:34,831 iteration 4701 : loss : 0.019887, loss_ce: 0.006391
2022-01-10 04:12:36,289 iteration 4702 : loss : 0.024684, loss_ce: 0.009047
2022-01-10 04:12:37,712 iteration 4703 : loss : 0.032692, loss_ce: 0.015961
2022-01-10 04:12:39,144 iteration 4704 : loss : 0.021030, loss_ce: 0.005694
2022-01-10 04:12:40,575 iteration 4705 : loss : 0.037693, loss_ce: 0.009791
2022-01-10 04:12:41,906 iteration 4706 : loss : 0.013287, loss_ce: 0.005099
2022-01-10 04:12:43,286 iteration 4707 : loss : 0.018823, loss_ce: 0.005678
2022-01-10 04:12:44,685 iteration 4708 : loss : 0.023882, loss_ce: 0.010105
2022-01-10 04:12:46,063 iteration 4709 : loss : 0.020665, loss_ce: 0.008834
 69%|████████████████████         | 277/400 [2:00:52<52:31, 25.62s/it]2022-01-10 04:12:47,487 iteration 4710 : loss : 0.020291, loss_ce: 0.009516
2022-01-10 04:12:48,839 iteration 4711 : loss : 0.026161, loss_ce: 0.007988
2022-01-10 04:12:50,166 iteration 4712 : loss : 0.016685, loss_ce: 0.005787
2022-01-10 04:12:51,573 iteration 4713 : loss : 0.019737, loss_ce: 0.007326
2022-01-10 04:12:52,910 iteration 4714 : loss : 0.024731, loss_ce: 0.008245
2022-01-10 04:12:54,327 iteration 4715 : loss : 0.020135, loss_ce: 0.010076
2022-01-10 04:12:55,773 iteration 4716 : loss : 0.029265, loss_ce: 0.009894
2022-01-10 04:12:57,161 iteration 4717 : loss : 0.020379, loss_ce: 0.007025
2022-01-10 04:12:58,579 iteration 4718 : loss : 0.018609, loss_ce: 0.006843
2022-01-10 04:12:59,959 iteration 4719 : loss : 0.011796, loss_ce: 0.003852
2022-01-10 04:13:01,357 iteration 4720 : loss : 0.016354, loss_ce: 0.007650
2022-01-10 04:13:02,744 iteration 4721 : loss : 0.015866, loss_ce: 0.005137
2022-01-10 04:13:04,213 iteration 4722 : loss : 0.018204, loss_ce: 0.007771
2022-01-10 04:13:05,700 iteration 4723 : loss : 0.020808, loss_ce: 0.006261
2022-01-10 04:13:07,198 iteration 4724 : loss : 0.022118, loss_ce: 0.012480
2022-01-10 04:13:08,571 iteration 4725 : loss : 0.025711, loss_ce: 0.008859
2022-01-10 04:13:10,015 iteration 4726 : loss : 0.020601, loss_ce: 0.005705
 70%|████████████████████▏        | 278/400 [2:01:16<51:05, 25.12s/it]2022-01-10 04:13:11,496 iteration 4727 : loss : 0.021631, loss_ce: 0.007216
2022-01-10 04:13:12,924 iteration 4728 : loss : 0.016780, loss_ce: 0.006027
2022-01-10 04:13:14,296 iteration 4729 : loss : 0.017673, loss_ce: 0.005481
2022-01-10 04:13:15,712 iteration 4730 : loss : 0.017495, loss_ce: 0.007333
2022-01-10 04:13:17,125 iteration 4731 : loss : 0.018156, loss_ce: 0.005225
2022-01-10 04:13:18,545 iteration 4732 : loss : 0.020727, loss_ce: 0.008886
2022-01-10 04:13:19,900 iteration 4733 : loss : 0.014674, loss_ce: 0.004834
2022-01-10 04:13:21,250 iteration 4734 : loss : 0.014069, loss_ce: 0.004848
2022-01-10 04:13:22,721 iteration 4735 : loss : 0.022444, loss_ce: 0.010461
2022-01-10 04:13:24,154 iteration 4736 : loss : 0.016824, loss_ce: 0.006725
2022-01-10 04:13:25,651 iteration 4737 : loss : 0.034832, loss_ce: 0.011047
2022-01-10 04:13:27,122 iteration 4738 : loss : 0.046833, loss_ce: 0.006256
2022-01-10 04:13:28,511 iteration 4739 : loss : 0.016443, loss_ce: 0.006066
2022-01-10 04:13:29,853 iteration 4740 : loss : 0.012274, loss_ce: 0.005335
2022-01-10 04:13:31,248 iteration 4741 : loss : 0.023353, loss_ce: 0.014148
2022-01-10 04:13:32,572 iteration 4742 : loss : 0.013097, loss_ce: 0.004875
2022-01-10 04:13:33,980 iteration 4743 : loss : 0.013309, loss_ce: 0.004808
 70%|████████████████████▏        | 279/400 [2:01:40<49:57, 24.78s/it]2022-01-10 04:13:35,460 iteration 4744 : loss : 0.025264, loss_ce: 0.010765
2022-01-10 04:13:36,878 iteration 4745 : loss : 0.017200, loss_ce: 0.007354
2022-01-10 04:13:38,277 iteration 4746 : loss : 0.016105, loss_ce: 0.007301
2022-01-10 04:13:39,730 iteration 4747 : loss : 0.022036, loss_ce: 0.010961
2022-01-10 04:13:41,234 iteration 4748 : loss : 0.020488, loss_ce: 0.008822
2022-01-10 04:13:42,701 iteration 4749 : loss : 0.017336, loss_ce: 0.004407
2022-01-10 04:13:44,098 iteration 4750 : loss : 0.016431, loss_ce: 0.006070
2022-01-10 04:13:45,447 iteration 4751 : loss : 0.022103, loss_ce: 0.007730
2022-01-10 04:13:46,854 iteration 4752 : loss : 0.017356, loss_ce: 0.004162
2022-01-10 04:13:48,290 iteration 4753 : loss : 0.017716, loss_ce: 0.008340
2022-01-10 04:13:49,712 iteration 4754 : loss : 0.020983, loss_ce: 0.006816
2022-01-10 04:13:51,148 iteration 4755 : loss : 0.016214, loss_ce: 0.005662
2022-01-10 04:13:52,567 iteration 4756 : loss : 0.016049, loss_ce: 0.006454
2022-01-10 04:13:53,979 iteration 4757 : loss : 0.019364, loss_ce: 0.007118
2022-01-10 04:13:55,484 iteration 4758 : loss : 0.026511, loss_ce: 0.008304
2022-01-10 04:13:56,837 iteration 4759 : loss : 0.012913, loss_ce: 0.004821
2022-01-10 04:13:56,837 Training Data Eval:
2022-01-10 04:14:03,882   Average segmentation loss on training set: 0.0127
2022-01-10 04:14:03,882 Validation Data Eval:
2022-01-10 04:14:06,323   Average segmentation loss on validation set: 0.0731
2022-01-10 04:14:07,731 iteration 4760 : loss : 0.023709, loss_ce: 0.008073
 70%|████████████████████▎        | 280/400 [2:02:14<54:56, 27.47s/it]2022-01-10 04:14:09,209 iteration 4761 : loss : 0.016147, loss_ce: 0.006171
2022-01-10 04:14:10,656 iteration 4762 : loss : 0.018833, loss_ce: 0.005803
2022-01-10 04:14:12,082 iteration 4763 : loss : 0.019337, loss_ce: 0.007008
2022-01-10 04:14:13,556 iteration 4764 : loss : 0.016451, loss_ce: 0.005207
2022-01-10 04:14:15,006 iteration 4765 : loss : 0.017945, loss_ce: 0.006894
2022-01-10 04:14:16,528 iteration 4766 : loss : 0.018906, loss_ce: 0.005039
2022-01-10 04:14:18,053 iteration 4767 : loss : 0.022542, loss_ce: 0.010593
2022-01-10 04:14:19,493 iteration 4768 : loss : 0.017245, loss_ce: 0.006849
2022-01-10 04:14:20,847 iteration 4769 : loss : 0.013922, loss_ce: 0.004727
2022-01-10 04:14:22,303 iteration 4770 : loss : 0.021683, loss_ce: 0.011298
2022-01-10 04:14:23,705 iteration 4771 : loss : 0.022149, loss_ce: 0.007123
2022-01-10 04:14:25,080 iteration 4772 : loss : 0.022052, loss_ce: 0.008133
2022-01-10 04:14:26,422 iteration 4773 : loss : 0.012956, loss_ce: 0.005564
2022-01-10 04:14:27,865 iteration 4774 : loss : 0.020901, loss_ce: 0.007416
2022-01-10 04:14:29,216 iteration 4775 : loss : 0.013784, loss_ce: 0.005038
2022-01-10 04:14:30,618 iteration 4776 : loss : 0.019952, loss_ce: 0.008076
2022-01-10 04:14:32,010 iteration 4777 : loss : 0.016552, loss_ce: 0.005239
 70%|████████████████████▎        | 281/400 [2:02:38<52:34, 26.51s/it]2022-01-10 04:14:33,496 iteration 4778 : loss : 0.019218, loss_ce: 0.008828
2022-01-10 04:14:34,868 iteration 4779 : loss : 0.013732, loss_ce: 0.005704
2022-01-10 04:14:36,240 iteration 4780 : loss : 0.015990, loss_ce: 0.006682
2022-01-10 04:14:37,680 iteration 4781 : loss : 0.021646, loss_ce: 0.007050
2022-01-10 04:14:39,133 iteration 4782 : loss : 0.019212, loss_ce: 0.007251
2022-01-10 04:14:40,529 iteration 4783 : loss : 0.018974, loss_ce: 0.009301
2022-01-10 04:14:41,993 iteration 4784 : loss : 0.021180, loss_ce: 0.007303
2022-01-10 04:14:43,399 iteration 4785 : loss : 0.017930, loss_ce: 0.006381
2022-01-10 04:14:44,798 iteration 4786 : loss : 0.015613, loss_ce: 0.005664
2022-01-10 04:14:46,192 iteration 4787 : loss : 0.028528, loss_ce: 0.009062
2022-01-10 04:14:47,626 iteration 4788 : loss : 0.025327, loss_ce: 0.007702
2022-01-10 04:14:48,940 iteration 4789 : loss : 0.013672, loss_ce: 0.005309
2022-01-10 04:14:50,249 iteration 4790 : loss : 0.012549, loss_ce: 0.004599
2022-01-10 04:14:51,673 iteration 4791 : loss : 0.038165, loss_ce: 0.019105
2022-01-10 04:14:53,090 iteration 4792 : loss : 0.017483, loss_ce: 0.006345
2022-01-10 04:14:54,563 iteration 4793 : loss : 0.023466, loss_ce: 0.008610
2022-01-10 04:14:55,956 iteration 4794 : loss : 0.014836, loss_ce: 0.004290
 70%|████████████████████▍        | 282/400 [2:03:02<50:37, 25.74s/it]2022-01-10 04:14:57,454 iteration 4795 : loss : 0.025802, loss_ce: 0.009970
2022-01-10 04:14:58,894 iteration 4796 : loss : 0.022260, loss_ce: 0.007788
2022-01-10 04:15:00,314 iteration 4797 : loss : 0.014669, loss_ce: 0.005483
2022-01-10 04:15:01,752 iteration 4798 : loss : 0.017687, loss_ce: 0.006218
2022-01-10 04:15:03,144 iteration 4799 : loss : 0.017544, loss_ce: 0.004125
2022-01-10 04:15:04,571 iteration 4800 : loss : 0.020003, loss_ce: 0.007142
2022-01-10 04:15:05,958 iteration 4801 : loss : 0.017893, loss_ce: 0.007551
2022-01-10 04:15:07,409 iteration 4802 : loss : 0.015240, loss_ce: 0.006185
2022-01-10 04:15:08,814 iteration 4803 : loss : 0.025063, loss_ce: 0.010430
2022-01-10 04:15:10,250 iteration 4804 : loss : 0.016203, loss_ce: 0.006265
2022-01-10 04:15:11,742 iteration 4805 : loss : 0.021370, loss_ce: 0.008662
2022-01-10 04:15:13,189 iteration 4806 : loss : 0.043900, loss_ce: 0.008339
2022-01-10 04:15:14,545 iteration 4807 : loss : 0.015778, loss_ce: 0.004793
2022-01-10 04:15:16,002 iteration 4808 : loss : 0.019586, loss_ce: 0.006765
2022-01-10 04:15:17,369 iteration 4809 : loss : 0.016142, loss_ce: 0.007786
2022-01-10 04:15:18,775 iteration 4810 : loss : 0.024376, loss_ce: 0.008708
2022-01-10 04:15:20,218 iteration 4811 : loss : 0.023129, loss_ce: 0.008440
 71%|████████████████████▌        | 283/400 [2:03:26<49:19, 25.30s/it]2022-01-10 04:15:21,680 iteration 4812 : loss : 0.026996, loss_ce: 0.006930
2022-01-10 04:15:23,049 iteration 4813 : loss : 0.014619, loss_ce: 0.005576
2022-01-10 04:15:24,589 iteration 4814 : loss : 0.038787, loss_ce: 0.015124
2022-01-10 04:15:25,985 iteration 4815 : loss : 0.020908, loss_ce: 0.007683
2022-01-10 04:15:27,349 iteration 4816 : loss : 0.018132, loss_ce: 0.006604
2022-01-10 04:15:28,702 iteration 4817 : loss : 0.014940, loss_ce: 0.005928
2022-01-10 04:15:30,074 iteration 4818 : loss : 0.018350, loss_ce: 0.007014
2022-01-10 04:15:31,500 iteration 4819 : loss : 0.020369, loss_ce: 0.006457
2022-01-10 04:15:32,871 iteration 4820 : loss : 0.014808, loss_ce: 0.006479
2022-01-10 04:15:34,279 iteration 4821 : loss : 0.014991, loss_ce: 0.005132
2022-01-10 04:15:35,744 iteration 4822 : loss : 0.031265, loss_ce: 0.010710
2022-01-10 04:15:37,165 iteration 4823 : loss : 0.022442, loss_ce: 0.005822
2022-01-10 04:15:38,547 iteration 4824 : loss : 0.022949, loss_ce: 0.006382
2022-01-10 04:15:40,009 iteration 4825 : loss : 0.026236, loss_ce: 0.014974
2022-01-10 04:15:41,586 iteration 4826 : loss : 0.025402, loss_ce: 0.011852
2022-01-10 04:15:42,968 iteration 4827 : loss : 0.017286, loss_ce: 0.008876
2022-01-10 04:15:44,338 iteration 4828 : loss : 0.015405, loss_ce: 0.005339
 71%|████████████████████▌        | 284/400 [2:03:50<48:13, 24.94s/it]2022-01-10 04:15:45,778 iteration 4829 : loss : 0.018269, loss_ce: 0.008865
2022-01-10 04:15:47,235 iteration 4830 : loss : 0.017623, loss_ce: 0.006883
2022-01-10 04:15:48,673 iteration 4831 : loss : 0.020017, loss_ce: 0.010318
2022-01-10 04:15:50,117 iteration 4832 : loss : 0.015753, loss_ce: 0.005575
2022-01-10 04:15:51,522 iteration 4833 : loss : 0.016113, loss_ce: 0.007485
2022-01-10 04:15:53,008 iteration 4834 : loss : 0.030592, loss_ce: 0.010042
2022-01-10 04:15:54,406 iteration 4835 : loss : 0.020141, loss_ce: 0.008684
2022-01-10 04:15:55,784 iteration 4836 : loss : 0.015336, loss_ce: 0.004433
2022-01-10 04:15:57,207 iteration 4837 : loss : 0.021960, loss_ce: 0.005723
2022-01-10 04:15:58,638 iteration 4838 : loss : 0.018644, loss_ce: 0.007829
2022-01-10 04:15:59,950 iteration 4839 : loss : 0.016563, loss_ce: 0.005959
2022-01-10 04:16:01,332 iteration 4840 : loss : 0.018244, loss_ce: 0.005743
2022-01-10 04:16:02,776 iteration 4841 : loss : 0.024740, loss_ce: 0.009259
2022-01-10 04:16:04,202 iteration 4842 : loss : 0.025236, loss_ce: 0.009690
2022-01-10 04:16:05,598 iteration 4843 : loss : 0.014976, loss_ce: 0.004535
2022-01-10 04:16:07,063 iteration 4844 : loss : 0.034989, loss_ce: 0.010908
2022-01-10 04:16:07,063 Training Data Eval:
2022-01-10 04:16:14,125   Average segmentation loss on training set: 0.0115
2022-01-10 04:16:14,125 Validation Data Eval:
2022-01-10 04:16:16,570   Average segmentation loss on validation set: 0.0948
2022-01-10 04:16:17,938 iteration 4845 : loss : 0.016341, loss_ce: 0.005413
 71%|████████████████████▋        | 285/400 [2:04:24<52:47, 27.54s/it]2022-01-10 04:16:19,407 iteration 4846 : loss : 0.026552, loss_ce: 0.011538
2022-01-10 04:16:20,761 iteration 4847 : loss : 0.012878, loss_ce: 0.004388
2022-01-10 04:16:22,259 iteration 4848 : loss : 0.025456, loss_ce: 0.010359
2022-01-10 04:16:23,645 iteration 4849 : loss : 0.015024, loss_ce: 0.005131
2022-01-10 04:16:24,979 iteration 4850 : loss : 0.015152, loss_ce: 0.007670
2022-01-10 04:16:26,481 iteration 4851 : loss : 0.027105, loss_ce: 0.008596
2022-01-10 04:16:27,878 iteration 4852 : loss : 0.017116, loss_ce: 0.009306
2022-01-10 04:16:29,316 iteration 4853 : loss : 0.022127, loss_ce: 0.004464
2022-01-10 04:16:30,861 iteration 4854 : loss : 0.028962, loss_ce: 0.008872
2022-01-10 04:16:32,290 iteration 4855 : loss : 0.024576, loss_ce: 0.008475
2022-01-10 04:16:33,639 iteration 4856 : loss : 0.028545, loss_ce: 0.010912
2022-01-10 04:16:35,102 iteration 4857 : loss : 0.015076, loss_ce: 0.006918
2022-01-10 04:16:36,546 iteration 4858 : loss : 0.017256, loss_ce: 0.006100
2022-01-10 04:16:37,963 iteration 4859 : loss : 0.016694, loss_ce: 0.007569
2022-01-10 04:16:39,406 iteration 4860 : loss : 0.027799, loss_ce: 0.010725
2022-01-10 04:16:40,819 iteration 4861 : loss : 0.022949, loss_ce: 0.008865
2022-01-10 04:16:42,178 iteration 4862 : loss : 0.014606, loss_ce: 0.005832
 72%|████████████████████▋        | 286/400 [2:04:48<50:26, 26.55s/it]2022-01-10 04:16:43,595 iteration 4863 : loss : 0.016584, loss_ce: 0.005696
2022-01-10 04:16:45,054 iteration 4864 : loss : 0.020724, loss_ce: 0.007497
2022-01-10 04:16:46,555 iteration 4865 : loss : 0.027103, loss_ce: 0.005991
2022-01-10 04:16:47,954 iteration 4866 : loss : 0.012696, loss_ce: 0.004877
2022-01-10 04:16:49,376 iteration 4867 : loss : 0.027963, loss_ce: 0.013984
2022-01-10 04:16:50,751 iteration 4868 : loss : 0.014549, loss_ce: 0.007182
2022-01-10 04:16:52,134 iteration 4869 : loss : 0.016193, loss_ce: 0.006671
2022-01-10 04:16:53,627 iteration 4870 : loss : 0.025280, loss_ce: 0.007750
2022-01-10 04:16:55,063 iteration 4871 : loss : 0.013825, loss_ce: 0.005071
2022-01-10 04:16:56,412 iteration 4872 : loss : 0.017874, loss_ce: 0.007385
2022-01-10 04:16:57,744 iteration 4873 : loss : 0.015774, loss_ce: 0.005073
2022-01-10 04:16:59,063 iteration 4874 : loss : 0.011981, loss_ce: 0.005039
2022-01-10 04:17:00,463 iteration 4875 : loss : 0.013132, loss_ce: 0.003933
2022-01-10 04:17:01,812 iteration 4876 : loss : 0.017883, loss_ce: 0.005960
2022-01-10 04:17:03,324 iteration 4877 : loss : 0.027192, loss_ce: 0.010724
2022-01-10 04:17:04,731 iteration 4878 : loss : 0.020250, loss_ce: 0.007939
2022-01-10 04:17:06,128 iteration 4879 : loss : 0.016333, loss_ce: 0.005745
 72%|████████████████████▊        | 287/400 [2:05:12<48:31, 25.77s/it]2022-01-10 04:17:07,654 iteration 4880 : loss : 0.023220, loss_ce: 0.012223
2022-01-10 04:17:09,043 iteration 4881 : loss : 0.024679, loss_ce: 0.007511
2022-01-10 04:17:10,447 iteration 4882 : loss : 0.013607, loss_ce: 0.003878
2022-01-10 04:17:11,862 iteration 4883 : loss : 0.019356, loss_ce: 0.005062
2022-01-10 04:17:13,283 iteration 4884 : loss : 0.011054, loss_ce: 0.005912
2022-01-10 04:17:14,828 iteration 4885 : loss : 0.020027, loss_ce: 0.006169
2022-01-10 04:17:16,293 iteration 4886 : loss : 0.022270, loss_ce: 0.007981
2022-01-10 04:17:17,778 iteration 4887 : loss : 0.019337, loss_ce: 0.007661
2022-01-10 04:17:19,177 iteration 4888 : loss : 0.019037, loss_ce: 0.005445
2022-01-10 04:17:20,575 iteration 4889 : loss : 0.015841, loss_ce: 0.005778
2022-01-10 04:17:22,048 iteration 4890 : loss : 0.026085, loss_ce: 0.009427
2022-01-10 04:17:23,528 iteration 4891 : loss : 0.016856, loss_ce: 0.009143
2022-01-10 04:17:24,879 iteration 4892 : loss : 0.015130, loss_ce: 0.005937
2022-01-10 04:17:26,260 iteration 4893 : loss : 0.015197, loss_ce: 0.007350
2022-01-10 04:17:27,664 iteration 4894 : loss : 0.019447, loss_ce: 0.004365
2022-01-10 04:17:29,037 iteration 4895 : loss : 0.017675, loss_ce: 0.006959
2022-01-10 04:17:30,455 iteration 4896 : loss : 0.019001, loss_ce: 0.006114
 72%|████████████████████▉        | 288/400 [2:05:36<47:17, 25.34s/it]2022-01-10 04:17:31,902 iteration 4897 : loss : 0.020241, loss_ce: 0.007015
2022-01-10 04:17:33,344 iteration 4898 : loss : 0.036535, loss_ce: 0.014000
2022-01-10 04:17:34,622 iteration 4899 : loss : 0.016108, loss_ce: 0.006700
2022-01-10 04:17:36,115 iteration 4900 : loss : 0.018440, loss_ce: 0.005952
2022-01-10 04:17:37,499 iteration 4901 : loss : 0.012070, loss_ce: 0.002995
2022-01-10 04:17:38,965 iteration 4902 : loss : 0.024742, loss_ce: 0.009437
2022-01-10 04:17:40,464 iteration 4903 : loss : 0.019246, loss_ce: 0.009214
2022-01-10 04:17:41,943 iteration 4904 : loss : 0.025186, loss_ce: 0.011653
2022-01-10 04:17:43,384 iteration 4905 : loss : 0.033603, loss_ce: 0.010847
2022-01-10 04:17:44,821 iteration 4906 : loss : 0.023011, loss_ce: 0.008661
2022-01-10 04:17:46,143 iteration 4907 : loss : 0.013150, loss_ce: 0.005100
2022-01-10 04:17:47,502 iteration 4908 : loss : 0.016703, loss_ce: 0.006840
2022-01-10 04:17:48,904 iteration 4909 : loss : 0.020997, loss_ce: 0.007765
2022-01-10 04:17:50,304 iteration 4910 : loss : 0.019224, loss_ce: 0.007441
2022-01-10 04:17:51,669 iteration 4911 : loss : 0.018485, loss_ce: 0.005047
2022-01-10 04:17:53,111 iteration 4912 : loss : 0.019026, loss_ce: 0.005821
2022-01-10 04:17:54,518 iteration 4913 : loss : 0.023366, loss_ce: 0.006407
 72%|████████████████████▉        | 289/400 [2:06:01<46:10, 24.96s/it]2022-01-10 04:17:55,926 iteration 4914 : loss : 0.020091, loss_ce: 0.008099
2022-01-10 04:17:57,339 iteration 4915 : loss : 0.014614, loss_ce: 0.006640
2022-01-10 04:17:58,799 iteration 4916 : loss : 0.028397, loss_ce: 0.008585
2022-01-10 04:18:00,203 iteration 4917 : loss : 0.014401, loss_ce: 0.004802
2022-01-10 04:18:01,619 iteration 4918 : loss : 0.017389, loss_ce: 0.007587
2022-01-10 04:18:03,019 iteration 4919 : loss : 0.013138, loss_ce: 0.005444
2022-01-10 04:18:04,492 iteration 4920 : loss : 0.020117, loss_ce: 0.007807
2022-01-10 04:18:05,844 iteration 4921 : loss : 0.038115, loss_ce: 0.012470
2022-01-10 04:18:07,289 iteration 4922 : loss : 0.016491, loss_ce: 0.005444
2022-01-10 04:18:08,708 iteration 4923 : loss : 0.020717, loss_ce: 0.008120
2022-01-10 04:18:10,141 iteration 4924 : loss : 0.018686, loss_ce: 0.008292
2022-01-10 04:18:11,536 iteration 4925 : loss : 0.023547, loss_ce: 0.008882
2022-01-10 04:18:12,852 iteration 4926 : loss : 0.012944, loss_ce: 0.005956
2022-01-10 04:18:14,260 iteration 4927 : loss : 0.012446, loss_ce: 0.004443
2022-01-10 04:18:15,694 iteration 4928 : loss : 0.019161, loss_ce: 0.009389
2022-01-10 04:18:17,057 iteration 4929 : loss : 0.018173, loss_ce: 0.004902
2022-01-10 04:18:17,057 Training Data Eval:
2022-01-10 04:18:24,121   Average segmentation loss on training set: 0.0115
2022-01-10 04:18:24,121 Validation Data Eval:
2022-01-10 04:18:26,567   Average segmentation loss on validation set: 0.0836
2022-01-10 04:18:27,986 iteration 4930 : loss : 0.020939, loss_ce: 0.006150
 72%|█████████████████████        | 290/400 [2:06:34<50:26, 27.51s/it]2022-01-10 04:18:29,495 iteration 4931 : loss : 0.022934, loss_ce: 0.010773
2022-01-10 04:18:30,941 iteration 4932 : loss : 0.019801, loss_ce: 0.005948
2022-01-10 04:18:32,293 iteration 4933 : loss : 0.018914, loss_ce: 0.007268
2022-01-10 04:18:33,704 iteration 4934 : loss : 0.016834, loss_ce: 0.005854
2022-01-10 04:18:35,095 iteration 4935 : loss : 0.012060, loss_ce: 0.003152
2022-01-10 04:18:36,527 iteration 4936 : loss : 0.014418, loss_ce: 0.005008
2022-01-10 04:18:37,903 iteration 4937 : loss : 0.020093, loss_ce: 0.005578
2022-01-10 04:18:39,260 iteration 4938 : loss : 0.014632, loss_ce: 0.005761
2022-01-10 04:18:40,766 iteration 4939 : loss : 0.028012, loss_ce: 0.011410
2022-01-10 04:18:42,133 iteration 4940 : loss : 0.015337, loss_ce: 0.005627
2022-01-10 04:18:43,545 iteration 4941 : loss : 0.020252, loss_ce: 0.008502
2022-01-10 04:18:45,126 iteration 4942 : loss : 0.017307, loss_ce: 0.006165
2022-01-10 04:18:46,515 iteration 4943 : loss : 0.028508, loss_ce: 0.012101
2022-01-10 04:18:47,889 iteration 4944 : loss : 0.020268, loss_ce: 0.009702
2022-01-10 04:18:49,292 iteration 4945 : loss : 0.012002, loss_ce: 0.004079
2022-01-10 04:18:50,634 iteration 4946 : loss : 0.018010, loss_ce: 0.006902
2022-01-10 04:18:52,099 iteration 4947 : loss : 0.021863, loss_ce: 0.007320
 73%|█████████████████████        | 291/400 [2:06:58<48:07, 26.49s/it]2022-01-10 04:18:53,601 iteration 4948 : loss : 0.021293, loss_ce: 0.007242
2022-01-10 04:18:55,119 iteration 4949 : loss : 0.020392, loss_ce: 0.008790
2022-01-10 04:18:56,489 iteration 4950 : loss : 0.033283, loss_ce: 0.019212
2022-01-10 04:18:57,890 iteration 4951 : loss : 0.019987, loss_ce: 0.007474
2022-01-10 04:18:59,297 iteration 4952 : loss : 0.018977, loss_ce: 0.006650
2022-01-10 04:19:00,778 iteration 4953 : loss : 0.020145, loss_ce: 0.005325
2022-01-10 04:19:02,108 iteration 4954 : loss : 0.023793, loss_ce: 0.006783
2022-01-10 04:19:03,472 iteration 4955 : loss : 0.015289, loss_ce: 0.007185
2022-01-10 04:19:04,915 iteration 4956 : loss : 0.016316, loss_ce: 0.004801
2022-01-10 04:19:06,333 iteration 4957 : loss : 0.017037, loss_ce: 0.007120
2022-01-10 04:19:07,893 iteration 4958 : loss : 0.021043, loss_ce: 0.008399
2022-01-10 04:19:09,388 iteration 4959 : loss : 0.024598, loss_ce: 0.010788
2022-01-10 04:19:10,922 iteration 4960 : loss : 0.023969, loss_ce: 0.011237
2022-01-10 04:19:12,319 iteration 4961 : loss : 0.020650, loss_ce: 0.004227
2022-01-10 04:19:13,732 iteration 4962 : loss : 0.015041, loss_ce: 0.006259
2022-01-10 04:19:15,188 iteration 4963 : loss : 0.019067, loss_ce: 0.006326
2022-01-10 04:19:16,640 iteration 4964 : loss : 0.013188, loss_ce: 0.004027
 73%|█████████████████████▏       | 292/400 [2:07:23<46:37, 25.90s/it]2022-01-10 04:19:18,089 iteration 4965 : loss : 0.023485, loss_ce: 0.006302
2022-01-10 04:19:19,508 iteration 4966 : loss : 0.016511, loss_ce: 0.006142
2022-01-10 04:19:20,923 iteration 4967 : loss : 0.020533, loss_ce: 0.005325
2022-01-10 04:19:22,322 iteration 4968 : loss : 0.028302, loss_ce: 0.006129
2022-01-10 04:19:23,750 iteration 4969 : loss : 0.018892, loss_ce: 0.006612
2022-01-10 04:19:25,197 iteration 4970 : loss : 0.023384, loss_ce: 0.012327
2022-01-10 04:19:26,587 iteration 4971 : loss : 0.017716, loss_ce: 0.007141
2022-01-10 04:19:27,953 iteration 4972 : loss : 0.021806, loss_ce: 0.007817
2022-01-10 04:19:29,341 iteration 4973 : loss : 0.015478, loss_ce: 0.005069
2022-01-10 04:19:30,773 iteration 4974 : loss : 0.014978, loss_ce: 0.007117
2022-01-10 04:19:32,267 iteration 4975 : loss : 0.015023, loss_ce: 0.005096
2022-01-10 04:19:33,651 iteration 4976 : loss : 0.014085, loss_ce: 0.005099
2022-01-10 04:19:35,052 iteration 4977 : loss : 0.015314, loss_ce: 0.007003
2022-01-10 04:19:36,470 iteration 4978 : loss : 0.020104, loss_ce: 0.007953
2022-01-10 04:19:37,840 iteration 4979 : loss : 0.016343, loss_ce: 0.005949
2022-01-10 04:19:39,183 iteration 4980 : loss : 0.012588, loss_ce: 0.004236
2022-01-10 04:19:40,578 iteration 4981 : loss : 0.016362, loss_ce: 0.005662
 73%|█████████████████████▏       | 293/400 [2:07:47<45:08, 25.32s/it]2022-01-10 04:19:42,070 iteration 4982 : loss : 0.020592, loss_ce: 0.005071
2022-01-10 04:19:43,405 iteration 4983 : loss : 0.012536, loss_ce: 0.003844
2022-01-10 04:19:44,823 iteration 4984 : loss : 0.021553, loss_ce: 0.006989
2022-01-10 04:19:46,160 iteration 4985 : loss : 0.019901, loss_ce: 0.005398
2022-01-10 04:19:47,523 iteration 4986 : loss : 0.018324, loss_ce: 0.005606
2022-01-10 04:19:48,931 iteration 4987 : loss : 0.014509, loss_ce: 0.006632
2022-01-10 04:19:50,356 iteration 4988 : loss : 0.020583, loss_ce: 0.010147
2022-01-10 04:19:51,746 iteration 4989 : loss : 0.021587, loss_ce: 0.008429
2022-01-10 04:19:53,179 iteration 4990 : loss : 0.016424, loss_ce: 0.007033
2022-01-10 04:19:54,566 iteration 4991 : loss : 0.017113, loss_ce: 0.005449
2022-01-10 04:19:55,981 iteration 4992 : loss : 0.016642, loss_ce: 0.005081
2022-01-10 04:19:57,391 iteration 4993 : loss : 0.021032, loss_ce: 0.009866
2022-01-10 04:19:58,920 iteration 4994 : loss : 0.024677, loss_ce: 0.009705
2022-01-10 04:20:00,383 iteration 4995 : loss : 0.020203, loss_ce: 0.008840
2022-01-10 04:20:01,754 iteration 4996 : loss : 0.015721, loss_ce: 0.004455
2022-01-10 04:20:03,183 iteration 4997 : loss : 0.015388, loss_ce: 0.005827
2022-01-10 04:20:04,605 iteration 4998 : loss : 0.017947, loss_ce: 0.006950
 74%|█████████████████████▎       | 294/400 [2:08:11<44:02, 24.93s/it]2022-01-10 04:20:06,070 iteration 4999 : loss : 0.013507, loss_ce: 0.005399
2022-01-10 04:20:07,449 iteration 5000 : loss : 0.014458, loss_ce: 0.004424
2022-01-10 04:20:08,886 iteration 5001 : loss : 0.019220, loss_ce: 0.005066
2022-01-10 04:20:10,373 iteration 5002 : loss : 0.019756, loss_ce: 0.005768
2022-01-10 04:20:11,883 iteration 5003 : loss : 0.022747, loss_ce: 0.010857
2022-01-10 04:20:13,287 iteration 5004 : loss : 0.013725, loss_ce: 0.005883
2022-01-10 04:20:14,720 iteration 5005 : loss : 0.019284, loss_ce: 0.007935
2022-01-10 04:20:16,077 iteration 5006 : loss : 0.016569, loss_ce: 0.005913
2022-01-10 04:20:17,511 iteration 5007 : loss : 0.024368, loss_ce: 0.006215
2022-01-10 04:20:18,866 iteration 5008 : loss : 0.013200, loss_ce: 0.005127
2022-01-10 04:20:20,222 iteration 5009 : loss : 0.016572, loss_ce: 0.008752
2022-01-10 04:20:21,667 iteration 5010 : loss : 0.019193, loss_ce: 0.008897
2022-01-10 04:20:23,026 iteration 5011 : loss : 0.013039, loss_ce: 0.004881
2022-01-10 04:20:24,405 iteration 5012 : loss : 0.024894, loss_ce: 0.007126
2022-01-10 04:20:25,891 iteration 5013 : loss : 0.017449, loss_ce: 0.007611
2022-01-10 04:20:27,236 iteration 5014 : loss : 0.016348, loss_ce: 0.006043
2022-01-10 04:20:27,236 Training Data Eval:
2022-01-10 04:20:34,281   Average segmentation loss on training set: 0.0101
2022-01-10 04:20:34,282 Validation Data Eval:
2022-01-10 04:20:36,713   Average segmentation loss on validation set: 0.0812
2022-01-10 04:20:38,097 iteration 5015 : loss : 0.022629, loss_ce: 0.007369
 74%|█████████████████████▍       | 295/400 [2:08:44<48:07, 27.50s/it]2022-01-10 04:20:39,586 iteration 5016 : loss : 0.021158, loss_ce: 0.009148
2022-01-10 04:20:40,900 iteration 5017 : loss : 0.012727, loss_ce: 0.005016
2022-01-10 04:20:42,404 iteration 5018 : loss : 0.024362, loss_ce: 0.010874
2022-01-10 04:20:43,750 iteration 5019 : loss : 0.014745, loss_ce: 0.005178
2022-01-10 04:20:45,205 iteration 5020 : loss : 0.024043, loss_ce: 0.007161
2022-01-10 04:20:46,606 iteration 5021 : loss : 0.028943, loss_ce: 0.010323
2022-01-10 04:20:48,004 iteration 5022 : loss : 0.016498, loss_ce: 0.005732
2022-01-10 04:20:49,405 iteration 5023 : loss : 0.016684, loss_ce: 0.007566
2022-01-10 04:20:50,872 iteration 5024 : loss : 0.025872, loss_ce: 0.010005
2022-01-10 04:20:52,291 iteration 5025 : loss : 0.015896, loss_ce: 0.005261
2022-01-10 04:20:53,760 iteration 5026 : loss : 0.018912, loss_ce: 0.006513
2022-01-10 04:20:55,128 iteration 5027 : loss : 0.016963, loss_ce: 0.006207
2022-01-10 04:20:56,471 iteration 5028 : loss : 0.013179, loss_ce: 0.005351
2022-01-10 04:20:57,932 iteration 5029 : loss : 0.021982, loss_ce: 0.009044
2022-01-10 04:20:59,313 iteration 5030 : loss : 0.012873, loss_ce: 0.005505
2022-01-10 04:21:00,769 iteration 5031 : loss : 0.027176, loss_ce: 0.008457
2022-01-10 04:21:02,111 iteration 5032 : loss : 0.013378, loss_ce: 0.005189
 74%|█████████████████████▍       | 296/400 [2:09:08<45:50, 26.45s/it]2022-01-10 04:21:03,510 iteration 5033 : loss : 0.017399, loss_ce: 0.006151
2022-01-10 04:21:04,869 iteration 5034 : loss : 0.020194, loss_ce: 0.005464
2022-01-10 04:21:06,264 iteration 5035 : loss : 0.013962, loss_ce: 0.004645
2022-01-10 04:21:07,625 iteration 5036 : loss : 0.023554, loss_ce: 0.009444
2022-01-10 04:21:09,013 iteration 5037 : loss : 0.015892, loss_ce: 0.005128
2022-01-10 04:21:10,384 iteration 5038 : loss : 0.017719, loss_ce: 0.005368
2022-01-10 04:21:11,801 iteration 5039 : loss : 0.023332, loss_ce: 0.009246
2022-01-10 04:21:13,249 iteration 5040 : loss : 0.013025, loss_ce: 0.004542
2022-01-10 04:21:14,643 iteration 5041 : loss : 0.015908, loss_ce: 0.006568
2022-01-10 04:21:16,137 iteration 5042 : loss : 0.019033, loss_ce: 0.006525
2022-01-10 04:21:17,455 iteration 5043 : loss : 0.016354, loss_ce: 0.005625
2022-01-10 04:21:18,895 iteration 5044 : loss : 0.013821, loss_ce: 0.004558
2022-01-10 04:21:20,321 iteration 5045 : loss : 0.017194, loss_ce: 0.007475
2022-01-10 04:21:21,729 iteration 5046 : loss : 0.021880, loss_ce: 0.005694
2022-01-10 04:21:23,076 iteration 5047 : loss : 0.011418, loss_ce: 0.003310
2022-01-10 04:21:24,409 iteration 5048 : loss : 0.015302, loss_ce: 0.007088
2022-01-10 04:21:25,729 iteration 5049 : loss : 0.011635, loss_ce: 0.004900
 74%|█████████████████████▌       | 297/400 [2:09:32<43:56, 25.60s/it]2022-01-10 04:21:27,205 iteration 5050 : loss : 0.015681, loss_ce: 0.005184
2022-01-10 04:21:28,674 iteration 5051 : loss : 0.022996, loss_ce: 0.007454
2022-01-10 04:21:30,167 iteration 5052 : loss : 0.032397, loss_ce: 0.014401
2022-01-10 04:21:31,653 iteration 5053 : loss : 0.024579, loss_ce: 0.007728
2022-01-10 04:21:33,035 iteration 5054 : loss : 0.018231, loss_ce: 0.004595
2022-01-10 04:21:34,459 iteration 5055 : loss : 0.017224, loss_ce: 0.005783
2022-01-10 04:21:35,883 iteration 5056 : loss : 0.017992, loss_ce: 0.006952
2022-01-10 04:21:37,373 iteration 5057 : loss : 0.027050, loss_ce: 0.012786
2022-01-10 04:21:38,864 iteration 5058 : loss : 0.019254, loss_ce: 0.006936
2022-01-10 04:21:40,265 iteration 5059 : loss : 0.014695, loss_ce: 0.005592
2022-01-10 04:21:41,719 iteration 5060 : loss : 0.017318, loss_ce: 0.008011
2022-01-10 04:21:43,190 iteration 5061 : loss : 0.021725, loss_ce: 0.007673
2022-01-10 04:21:44,695 iteration 5062 : loss : 0.018872, loss_ce: 0.006100
2022-01-10 04:21:46,189 iteration 5063 : loss : 0.029589, loss_ce: 0.010270
2022-01-10 04:21:47,589 iteration 5064 : loss : 0.016796, loss_ce: 0.005473
2022-01-10 04:21:49,056 iteration 5065 : loss : 0.017907, loss_ce: 0.009337
2022-01-10 04:21:50,412 iteration 5066 : loss : 0.014716, loss_ce: 0.005953
 74%|█████████████████████▌       | 298/400 [2:09:56<43:03, 25.32s/it]2022-01-10 04:21:51,760 iteration 5067 : loss : 0.012724, loss_ce: 0.004651
2022-01-10 04:21:53,207 iteration 5068 : loss : 0.015567, loss_ce: 0.003997
2022-01-10 04:21:54,630 iteration 5069 : loss : 0.022450, loss_ce: 0.011182
2022-01-10 04:21:55,984 iteration 5070 : loss : 0.014950, loss_ce: 0.004940
2022-01-10 04:21:57,424 iteration 5071 : loss : 0.023387, loss_ce: 0.010791
2022-01-10 04:21:58,761 iteration 5072 : loss : 0.013547, loss_ce: 0.005010
2022-01-10 04:22:00,227 iteration 5073 : loss : 0.017897, loss_ce: 0.007366
2022-01-10 04:22:01,607 iteration 5074 : loss : 0.013502, loss_ce: 0.005571
2022-01-10 04:22:02,991 iteration 5075 : loss : 0.015588, loss_ce: 0.006079
2022-01-10 04:22:04,417 iteration 5076 : loss : 0.045352, loss_ce: 0.007525
2022-01-10 04:22:05,831 iteration 5077 : loss : 0.025784, loss_ce: 0.008208
2022-01-10 04:22:07,195 iteration 5078 : loss : 0.016970, loss_ce: 0.008030
2022-01-10 04:22:08,542 iteration 5079 : loss : 0.013024, loss_ce: 0.005513
2022-01-10 04:22:09,908 iteration 5080 : loss : 0.020079, loss_ce: 0.007213
2022-01-10 04:22:11,343 iteration 5081 : loss : 0.017680, loss_ce: 0.004984
2022-01-10 04:22:12,727 iteration 5082 : loss : 0.013861, loss_ce: 0.005089
2022-01-10 04:22:14,143 iteration 5083 : loss : 0.028499, loss_ce: 0.006050
 75%|█████████████████████▋       | 299/400 [2:10:20<41:49, 24.85s/it]2022-01-10 04:22:15,640 iteration 5084 : loss : 0.016522, loss_ce: 0.007973
2022-01-10 04:22:17,132 iteration 5085 : loss : 0.028741, loss_ce: 0.011038
2022-01-10 04:22:18,582 iteration 5086 : loss : 0.017302, loss_ce: 0.005671
2022-01-10 04:22:19,936 iteration 5087 : loss : 0.018062, loss_ce: 0.007779
2022-01-10 04:22:21,288 iteration 5088 : loss : 0.018335, loss_ce: 0.006999
2022-01-10 04:22:22,716 iteration 5089 : loss : 0.025057, loss_ce: 0.007461
2022-01-10 04:22:24,127 iteration 5090 : loss : 0.019073, loss_ce: 0.004066
2022-01-10 04:22:25,597 iteration 5091 : loss : 0.025968, loss_ce: 0.009448
2022-01-10 04:22:27,020 iteration 5092 : loss : 0.021244, loss_ce: 0.007742
2022-01-10 04:22:28,377 iteration 5093 : loss : 0.023259, loss_ce: 0.006657
2022-01-10 04:22:29,817 iteration 5094 : loss : 0.029061, loss_ce: 0.010862
2022-01-10 04:22:31,174 iteration 5095 : loss : 0.014521, loss_ce: 0.005443
2022-01-10 04:22:32,569 iteration 5096 : loss : 0.019650, loss_ce: 0.008788
2022-01-10 04:22:33,903 iteration 5097 : loss : 0.016250, loss_ce: 0.007197
2022-01-10 04:22:35,379 iteration 5098 : loss : 0.021039, loss_ce: 0.005518
2022-01-10 04:22:36,812 iteration 5099 : loss : 0.021410, loss_ce: 0.006453
2022-01-10 04:22:36,812 Training Data Eval:
2022-01-10 04:22:43,847   Average segmentation loss on training set: 0.0127
2022-01-10 04:22:43,848 Validation Data Eval:
2022-01-10 04:22:46,287   Average segmentation loss on validation set: 0.1322
2022-01-10 04:22:47,755 iteration 5100 : loss : 0.025528, loss_ce: 0.006590
 75%|█████████████████████▊       | 300/400 [2:10:54<45:47, 27.48s/it]2022-01-10 04:22:49,315 iteration 5101 : loss : 0.022653, loss_ce: 0.009437
2022-01-10 04:22:50,714 iteration 5102 : loss : 0.022445, loss_ce: 0.010578
2022-01-10 04:22:52,123 iteration 5103 : loss : 0.020085, loss_ce: 0.010275
2022-01-10 04:22:53,595 iteration 5104 : loss : 0.021772, loss_ce: 0.007772
2022-01-10 04:22:54,976 iteration 5105 : loss : 0.021618, loss_ce: 0.008300
2022-01-10 04:22:56,357 iteration 5106 : loss : 0.018684, loss_ce: 0.007456
2022-01-10 04:22:57,827 iteration 5107 : loss : 0.023204, loss_ce: 0.006199
2022-01-10 04:22:59,261 iteration 5108 : loss : 0.019201, loss_ce: 0.009693
2022-01-10 04:23:00,678 iteration 5109 : loss : 0.031497, loss_ce: 0.011246
2022-01-10 04:23:02,046 iteration 5110 : loss : 0.013871, loss_ce: 0.004656
2022-01-10 04:23:03,477 iteration 5111 : loss : 0.037947, loss_ce: 0.022619
2022-01-10 04:23:04,981 iteration 5112 : loss : 0.020915, loss_ce: 0.006476
2022-01-10 04:23:06,413 iteration 5113 : loss : 0.021421, loss_ce: 0.007444
2022-01-10 04:23:07,893 iteration 5114 : loss : 0.020213, loss_ce: 0.007398
2022-01-10 04:23:09,299 iteration 5115 : loss : 0.042716, loss_ce: 0.015579
2022-01-10 04:23:10,682 iteration 5116 : loss : 0.015978, loss_ce: 0.005944
2022-01-10 04:23:12,120 iteration 5117 : loss : 0.023481, loss_ce: 0.009503
 75%|█████████████████████▊       | 301/400 [2:11:18<43:47, 26.55s/it]2022-01-10 04:23:13,621 iteration 5118 : loss : 0.019219, loss_ce: 0.005133
2022-01-10 04:23:15,017 iteration 5119 : loss : 0.017482, loss_ce: 0.007794
2022-01-10 04:23:16,373 iteration 5120 : loss : 0.013509, loss_ce: 0.005396
2022-01-10 04:23:17,832 iteration 5121 : loss : 0.017086, loss_ce: 0.005022
2022-01-10 04:23:19,325 iteration 5122 : loss : 0.017027, loss_ce: 0.006997
2022-01-10 04:23:20,809 iteration 5123 : loss : 0.024320, loss_ce: 0.008081
2022-01-10 04:23:22,170 iteration 5124 : loss : 0.013354, loss_ce: 0.006364
2022-01-10 04:23:23,524 iteration 5125 : loss : 0.018838, loss_ce: 0.006579
2022-01-10 04:23:24,945 iteration 5126 : loss : 0.026001, loss_ce: 0.009245
2022-01-10 04:23:26,342 iteration 5127 : loss : 0.022769, loss_ce: 0.010165
2022-01-10 04:23:27,710 iteration 5128 : loss : 0.014335, loss_ce: 0.003960
2022-01-10 04:23:29,125 iteration 5129 : loss : 0.014728, loss_ce: 0.006294
2022-01-10 04:23:30,527 iteration 5130 : loss : 0.014810, loss_ce: 0.005950
2022-01-10 04:23:31,952 iteration 5131 : loss : 0.014997, loss_ce: 0.005662
2022-01-10 04:23:33,359 iteration 5132 : loss : 0.019122, loss_ce: 0.006641
2022-01-10 04:23:34,831 iteration 5133 : loss : 0.025321, loss_ce: 0.010233
2022-01-10 04:23:36,232 iteration 5134 : loss : 0.019479, loss_ce: 0.005010
 76%|█████████████████████▉       | 302/400 [2:11:42<42:09, 25.81s/it]2022-01-10 04:23:37,680 iteration 5135 : loss : 0.019205, loss_ce: 0.010975
2022-01-10 04:23:39,083 iteration 5136 : loss : 0.010998, loss_ce: 0.003620
2022-01-10 04:23:40,437 iteration 5137 : loss : 0.019338, loss_ce: 0.006725
2022-01-10 04:23:41,875 iteration 5138 : loss : 0.016637, loss_ce: 0.007052
2022-01-10 04:23:43,381 iteration 5139 : loss : 0.021504, loss_ce: 0.008438
2022-01-10 04:23:44,860 iteration 5140 : loss : 0.020820, loss_ce: 0.007435
2022-01-10 04:23:46,226 iteration 5141 : loss : 0.017750, loss_ce: 0.005734
2022-01-10 04:23:47,656 iteration 5142 : loss : 0.016184, loss_ce: 0.006410
2022-01-10 04:23:49,057 iteration 5143 : loss : 0.013065, loss_ce: 0.006337
2022-01-10 04:23:50,541 iteration 5144 : loss : 0.012485, loss_ce: 0.004878
2022-01-10 04:23:52,079 iteration 5145 : loss : 0.017696, loss_ce: 0.005912
2022-01-10 04:23:53,529 iteration 5146 : loss : 0.019297, loss_ce: 0.006397
2022-01-10 04:23:54,881 iteration 5147 : loss : 0.014528, loss_ce: 0.003390
2022-01-10 04:23:56,282 iteration 5148 : loss : 0.015330, loss_ce: 0.005284
2022-01-10 04:23:57,672 iteration 5149 : loss : 0.017258, loss_ce: 0.006091
2022-01-10 04:23:59,064 iteration 5150 : loss : 0.018086, loss_ce: 0.009569
2022-01-10 04:24:00,513 iteration 5151 : loss : 0.017314, loss_ce: 0.007691
 76%|█████████████████████▉       | 303/400 [2:12:07<40:59, 25.36s/it]2022-01-10 04:24:01,963 iteration 5152 : loss : 0.015961, loss_ce: 0.006269
2022-01-10 04:24:03,382 iteration 5153 : loss : 0.019432, loss_ce: 0.008027
2022-01-10 04:24:04,724 iteration 5154 : loss : 0.013242, loss_ce: 0.004393
2022-01-10 04:24:06,082 iteration 5155 : loss : 0.014081, loss_ce: 0.005835
2022-01-10 04:24:07,472 iteration 5156 : loss : 0.019602, loss_ce: 0.004457
2022-01-10 04:24:08,824 iteration 5157 : loss : 0.013612, loss_ce: 0.005983
2022-01-10 04:24:10,189 iteration 5158 : loss : 0.015863, loss_ce: 0.006066
2022-01-10 04:24:11,611 iteration 5159 : loss : 0.017195, loss_ce: 0.006042
2022-01-10 04:24:13,025 iteration 5160 : loss : 0.016238, loss_ce: 0.007346
2022-01-10 04:24:14,460 iteration 5161 : loss : 0.017652, loss_ce: 0.006828
2022-01-10 04:24:15,936 iteration 5162 : loss : 0.013817, loss_ce: 0.003759
2022-01-10 04:24:17,371 iteration 5163 : loss : 0.016141, loss_ce: 0.008522
2022-01-10 04:24:18,732 iteration 5164 : loss : 0.013247, loss_ce: 0.004656
2022-01-10 04:24:20,212 iteration 5165 : loss : 0.015694, loss_ce: 0.005183
2022-01-10 04:24:21,641 iteration 5166 : loss : 0.021322, loss_ce: 0.007758
2022-01-10 04:24:23,098 iteration 5167 : loss : 0.019170, loss_ce: 0.004667
2022-01-10 04:24:24,423 iteration 5168 : loss : 0.012457, loss_ce: 0.005786
 76%|██████████████████████       | 304/400 [2:12:30<39:52, 24.92s/it]2022-01-10 04:24:25,936 iteration 5169 : loss : 0.017436, loss_ce: 0.004866
2022-01-10 04:24:27,324 iteration 5170 : loss : 0.012586, loss_ce: 0.006040
2022-01-10 04:24:28,733 iteration 5171 : loss : 0.015734, loss_ce: 0.004869
2022-01-10 04:24:30,159 iteration 5172 : loss : 0.032376, loss_ce: 0.013730
2022-01-10 04:24:31,646 iteration 5173 : loss : 0.021236, loss_ce: 0.007820
2022-01-10 04:24:33,060 iteration 5174 : loss : 0.013204, loss_ce: 0.003854
2022-01-10 04:24:34,505 iteration 5175 : loss : 0.017815, loss_ce: 0.006710
2022-01-10 04:24:35,998 iteration 5176 : loss : 0.016041, loss_ce: 0.005214
2022-01-10 04:24:37,421 iteration 5177 : loss : 0.014740, loss_ce: 0.005449
2022-01-10 04:24:38,854 iteration 5178 : loss : 0.018074, loss_ce: 0.009194
2022-01-10 04:24:40,238 iteration 5179 : loss : 0.019205, loss_ce: 0.007741
2022-01-10 04:24:41,579 iteration 5180 : loss : 0.013875, loss_ce: 0.005685
2022-01-10 04:24:42,947 iteration 5181 : loss : 0.016344, loss_ce: 0.005143
2022-01-10 04:24:44,335 iteration 5182 : loss : 0.015265, loss_ce: 0.006661
2022-01-10 04:24:45,718 iteration 5183 : loss : 0.012999, loss_ce: 0.004820
2022-01-10 04:24:47,073 iteration 5184 : loss : 0.015758, loss_ce: 0.007013
2022-01-10 04:24:47,073 Training Data Eval:
2022-01-10 04:24:54,110   Average segmentation loss on training set: 0.0094
2022-01-10 04:24:54,110 Validation Data Eval:
2022-01-10 04:24:56,552   Average segmentation loss on validation set: 0.0780
2022-01-10 04:24:57,936 iteration 5185 : loss : 0.011102, loss_ce: 0.004276
 76%|██████████████████████       | 305/400 [2:13:04<43:32, 27.50s/it]2022-01-10 04:24:59,474 iteration 5186 : loss : 0.019399, loss_ce: 0.006043
2022-01-10 04:25:00,825 iteration 5187 : loss : 0.012028, loss_ce: 0.004961
2022-01-10 04:25:02,228 iteration 5188 : loss : 0.014437, loss_ce: 0.004994
2022-01-10 04:25:03,643 iteration 5189 : loss : 0.034554, loss_ce: 0.012532
2022-01-10 04:25:05,145 iteration 5190 : loss : 0.018883, loss_ce: 0.006234
2022-01-10 04:25:06,529 iteration 5191 : loss : 0.015276, loss_ce: 0.005406
2022-01-10 04:25:07,928 iteration 5192 : loss : 0.022285, loss_ce: 0.009988
2022-01-10 04:25:09,378 iteration 5193 : loss : 0.017822, loss_ce: 0.008282
2022-01-10 04:25:10,812 iteration 5194 : loss : 0.020233, loss_ce: 0.005595
2022-01-10 04:25:12,213 iteration 5195 : loss : 0.024475, loss_ce: 0.008786
2022-01-10 04:25:13,721 iteration 5196 : loss : 0.025108, loss_ce: 0.007612
2022-01-10 04:25:15,125 iteration 5197 : loss : 0.017837, loss_ce: 0.005820
2022-01-10 04:25:16,490 iteration 5198 : loss : 0.013755, loss_ce: 0.007640
2022-01-10 04:25:18,012 iteration 5199 : loss : 0.014312, loss_ce: 0.004888
2022-01-10 04:25:19,346 iteration 5200 : loss : 0.015252, loss_ce: 0.006320
2022-01-10 04:25:20,690 iteration 5201 : loss : 0.014508, loss_ce: 0.006006
2022-01-10 04:25:22,079 iteration 5202 : loss : 0.016343, loss_ce: 0.006625
 76%|██████████████████████▏      | 306/400 [2:13:28<41:30, 26.49s/it]2022-01-10 04:25:23,529 iteration 5203 : loss : 0.018691, loss_ce: 0.007183
2022-01-10 04:25:24,993 iteration 5204 : loss : 0.017692, loss_ce: 0.007452
2022-01-10 04:25:26,444 iteration 5205 : loss : 0.023273, loss_ce: 0.005847
2022-01-10 04:25:27,985 iteration 5206 : loss : 0.026641, loss_ce: 0.009047
2022-01-10 04:25:29,454 iteration 5207 : loss : 0.036782, loss_ce: 0.010792
2022-01-10 04:25:30,880 iteration 5208 : loss : 0.013251, loss_ce: 0.005363
2022-01-10 04:25:32,345 iteration 5209 : loss : 0.025030, loss_ce: 0.012006
2022-01-10 04:25:33,807 iteration 5210 : loss : 0.018734, loss_ce: 0.008475
2022-01-10 04:25:35,215 iteration 5211 : loss : 0.019459, loss_ce: 0.006420
2022-01-10 04:25:36,680 iteration 5212 : loss : 0.028753, loss_ce: 0.007472
2022-01-10 04:25:38,078 iteration 5213 : loss : 0.019340, loss_ce: 0.006723
2022-01-10 04:25:39,485 iteration 5214 : loss : 0.014988, loss_ce: 0.006961
2022-01-10 04:25:40,815 iteration 5215 : loss : 0.013054, loss_ce: 0.004513
2022-01-10 04:25:42,189 iteration 5216 : loss : 0.013928, loss_ce: 0.004167
2022-01-10 04:25:43,660 iteration 5217 : loss : 0.025983, loss_ce: 0.011632
2022-01-10 04:25:45,015 iteration 5218 : loss : 0.016665, loss_ce: 0.005444
2022-01-10 04:25:46,427 iteration 5219 : loss : 0.017395, loss_ce: 0.006402
 77%|██████████████████████▎      | 307/400 [2:13:52<40:04, 25.85s/it]2022-01-10 04:25:47,814 iteration 5220 : loss : 0.013206, loss_ce: 0.005774
2022-01-10 04:25:49,154 iteration 5221 : loss : 0.020757, loss_ce: 0.006018
2022-01-10 04:25:50,599 iteration 5222 : loss : 0.018508, loss_ce: 0.005595
2022-01-10 04:25:51,995 iteration 5223 : loss : 0.013594, loss_ce: 0.005946
2022-01-10 04:25:53,477 iteration 5224 : loss : 0.038767, loss_ce: 0.008270
2022-01-10 04:25:54,860 iteration 5225 : loss : 0.012159, loss_ce: 0.005325
2022-01-10 04:25:56,279 iteration 5226 : loss : 0.012546, loss_ce: 0.005169
2022-01-10 04:25:57,713 iteration 5227 : loss : 0.021823, loss_ce: 0.007724
2022-01-10 04:25:59,049 iteration 5228 : loss : 0.010615, loss_ce: 0.003404
2022-01-10 04:26:00,491 iteration 5229 : loss : 0.019099, loss_ce: 0.006854
2022-01-10 04:26:01,841 iteration 5230 : loss : 0.011859, loss_ce: 0.004218
2022-01-10 04:26:03,264 iteration 5231 : loss : 0.013800, loss_ce: 0.003894
2022-01-10 04:26:04,763 iteration 5232 : loss : 0.019620, loss_ce: 0.006211
2022-01-10 04:26:06,060 iteration 5233 : loss : 0.012972, loss_ce: 0.005441
2022-01-10 04:26:07,462 iteration 5234 : loss : 0.016598, loss_ce: 0.006120
2022-01-10 04:26:08,902 iteration 5235 : loss : 0.022746, loss_ce: 0.011447
2022-01-10 04:26:10,392 iteration 5236 : loss : 0.041683, loss_ce: 0.018343
 77%|██████████████████████▎      | 308/400 [2:14:16<38:45, 25.28s/it]2022-01-10 04:26:11,785 iteration 5237 : loss : 0.011404, loss_ce: 0.005181
2022-01-10 04:26:13,182 iteration 5238 : loss : 0.016408, loss_ce: 0.006966
2022-01-10 04:26:14,568 iteration 5239 : loss : 0.015340, loss_ce: 0.005970
2022-01-10 04:26:16,028 iteration 5240 : loss : 0.065102, loss_ce: 0.009201
2022-01-10 04:26:17,452 iteration 5241 : loss : 0.017344, loss_ce: 0.006661
2022-01-10 04:26:18,969 iteration 5242 : loss : 0.023206, loss_ce: 0.011252
2022-01-10 04:26:20,416 iteration 5243 : loss : 0.022695, loss_ce: 0.011332
2022-01-10 04:26:21,809 iteration 5244 : loss : 0.017513, loss_ce: 0.005822
2022-01-10 04:26:23,278 iteration 5245 : loss : 0.022894, loss_ce: 0.007349
2022-01-10 04:26:24,715 iteration 5246 : loss : 0.025453, loss_ce: 0.009178
2022-01-10 04:26:26,110 iteration 5247 : loss : 0.018318, loss_ce: 0.005900
2022-01-10 04:26:27,585 iteration 5248 : loss : 0.025451, loss_ce: 0.011658
2022-01-10 04:26:28,973 iteration 5249 : loss : 0.017606, loss_ce: 0.006221
2022-01-10 04:26:30,387 iteration 5250 : loss : 0.020099, loss_ce: 0.007322
2022-01-10 04:26:31,849 iteration 5251 : loss : 0.019291, loss_ce: 0.008210
2022-01-10 04:26:33,243 iteration 5252 : loss : 0.015362, loss_ce: 0.007431
2022-01-10 04:26:34,629 iteration 5253 : loss : 0.015169, loss_ce: 0.005638
 77%|██████████████████████▍      | 309/400 [2:14:41<37:52, 24.97s/it]2022-01-10 04:26:36,082 iteration 5254 : loss : 0.015660, loss_ce: 0.007156
2022-01-10 04:26:37,541 iteration 5255 : loss : 0.018467, loss_ce: 0.004983
2022-01-10 04:26:38,966 iteration 5256 : loss : 0.021289, loss_ce: 0.008119
2022-01-10 04:26:40,403 iteration 5257 : loss : 0.016718, loss_ce: 0.006304
2022-01-10 04:26:41,737 iteration 5258 : loss : 0.013306, loss_ce: 0.005373
2022-01-10 04:26:43,117 iteration 5259 : loss : 0.021973, loss_ce: 0.008900
2022-01-10 04:26:44,613 iteration 5260 : loss : 0.024718, loss_ce: 0.007458
2022-01-10 04:26:46,013 iteration 5261 : loss : 0.018168, loss_ce: 0.008358
2022-01-10 04:26:47,390 iteration 5262 : loss : 0.021384, loss_ce: 0.009025
2022-01-10 04:26:48,828 iteration 5263 : loss : 0.020119, loss_ce: 0.007694
2022-01-10 04:26:50,273 iteration 5264 : loss : 0.019255, loss_ce: 0.007665
2022-01-10 04:26:51,727 iteration 5265 : loss : 0.018079, loss_ce: 0.008089
2022-01-10 04:26:53,114 iteration 5266 : loss : 0.020173, loss_ce: 0.006946
2022-01-10 04:26:54,601 iteration 5267 : loss : 0.022065, loss_ce: 0.009753
2022-01-10 04:26:55,889 iteration 5268 : loss : 0.012759, loss_ce: 0.002770
2022-01-10 04:26:57,197 iteration 5269 : loss : 0.013953, loss_ce: 0.005114
2022-01-10 04:26:57,197 Training Data Eval:
2022-01-10 04:27:04,249   Average segmentation loss on training set: 0.0101
2022-01-10 04:27:04,250 Validation Data Eval:
2022-01-10 04:27:06,696   Average segmentation loss on validation set: 0.0685
2022-01-10 04:27:08,083 iteration 5270 : loss : 0.024093, loss_ce: 0.003212
 78%|██████████████████████▍      | 310/400 [2:15:14<41:16, 27.52s/it]2022-01-10 04:27:09,658 iteration 5271 : loss : 0.027469, loss_ce: 0.007796
2022-01-10 04:27:11,145 iteration 5272 : loss : 0.016984, loss_ce: 0.005942
2022-01-10 04:27:12,562 iteration 5273 : loss : 0.022201, loss_ce: 0.010424
2022-01-10 04:27:13,983 iteration 5274 : loss : 0.031537, loss_ce: 0.016479
2022-01-10 04:27:15,360 iteration 5275 : loss : 0.015019, loss_ce: 0.006164
2022-01-10 04:27:16,738 iteration 5276 : loss : 0.017942, loss_ce: 0.005720
2022-01-10 04:27:18,249 iteration 5277 : loss : 0.031010, loss_ce: 0.012006
2022-01-10 04:27:19,652 iteration 5278 : loss : 0.017370, loss_ce: 0.004032
2022-01-10 04:27:21,105 iteration 5279 : loss : 0.026008, loss_ce: 0.008548
2022-01-10 04:27:22,459 iteration 5280 : loss : 0.015095, loss_ce: 0.004185
2022-01-10 04:27:23,839 iteration 5281 : loss : 0.013517, loss_ce: 0.006157
2022-01-10 04:27:25,211 iteration 5282 : loss : 0.016574, loss_ce: 0.006732
2022-01-10 04:27:26,583 iteration 5283 : loss : 0.018423, loss_ce: 0.008252
2022-01-10 04:27:27,999 iteration 5284 : loss : 0.014489, loss_ce: 0.005554
2022-01-10 04:27:29,384 iteration 5285 : loss : 0.016725, loss_ce: 0.006611
2022-01-10 04:27:30,743 iteration 5286 : loss : 0.015775, loss_ce: 0.007225
2022-01-10 04:27:32,165 iteration 5287 : loss : 0.012077, loss_ce: 0.004002
 78%|██████████████████████▌      | 311/400 [2:15:38<39:17, 26.49s/it]2022-01-10 04:27:33,644 iteration 5288 : loss : 0.018728, loss_ce: 0.007922
2022-01-10 04:27:35,006 iteration 5289 : loss : 0.022783, loss_ce: 0.007746
2022-01-10 04:27:36,536 iteration 5290 : loss : 0.025268, loss_ce: 0.006983
2022-01-10 04:27:37,948 iteration 5291 : loss : 0.014231, loss_ce: 0.005796
2022-01-10 04:27:39,386 iteration 5292 : loss : 0.018445, loss_ce: 0.005522
2022-01-10 04:27:40,743 iteration 5293 : loss : 0.020030, loss_ce: 0.006512
2022-01-10 04:27:42,192 iteration 5294 : loss : 0.022388, loss_ce: 0.007208
2022-01-10 04:27:43,609 iteration 5295 : loss : 0.016178, loss_ce: 0.004884
2022-01-10 04:27:44,960 iteration 5296 : loss : 0.018669, loss_ce: 0.004958
2022-01-10 04:27:46,341 iteration 5297 : loss : 0.019593, loss_ce: 0.005594
2022-01-10 04:27:47,708 iteration 5298 : loss : 0.013902, loss_ce: 0.005721
2022-01-10 04:27:49,081 iteration 5299 : loss : 0.017017, loss_ce: 0.003232
2022-01-10 04:27:50,449 iteration 5300 : loss : 0.014232, loss_ce: 0.005463
2022-01-10 04:27:51,834 iteration 5301 : loss : 0.015534, loss_ce: 0.003639
2022-01-10 04:27:53,309 iteration 5302 : loss : 0.029074, loss_ce: 0.011799
2022-01-10 04:27:54,752 iteration 5303 : loss : 0.015471, loss_ce: 0.006966
2022-01-10 04:27:56,149 iteration 5304 : loss : 0.025354, loss_ce: 0.013755
 78%|██████████████████████▌      | 312/400 [2:16:02<37:44, 25.73s/it]2022-01-10 04:27:57,547 iteration 5305 : loss : 0.013844, loss_ce: 0.005174
2022-01-10 04:27:58,998 iteration 5306 : loss : 0.016762, loss_ce: 0.006967
2022-01-10 04:28:00,427 iteration 5307 : loss : 0.019153, loss_ce: 0.007603
2022-01-10 04:28:01,814 iteration 5308 : loss : 0.022720, loss_ce: 0.007171
2022-01-10 04:28:03,276 iteration 5309 : loss : 0.030198, loss_ce: 0.017659
2022-01-10 04:28:04,793 iteration 5310 : loss : 0.015322, loss_ce: 0.005246
2022-01-10 04:28:06,196 iteration 5311 : loss : 0.027668, loss_ce: 0.012271
2022-01-10 04:28:07,673 iteration 5312 : loss : 0.020520, loss_ce: 0.007793
2022-01-10 04:28:09,060 iteration 5313 : loss : 0.017708, loss_ce: 0.009033
2022-01-10 04:28:10,468 iteration 5314 : loss : 0.016360, loss_ce: 0.005138
2022-01-10 04:28:11,843 iteration 5315 : loss : 0.021174, loss_ce: 0.007053
2022-01-10 04:28:13,242 iteration 5316 : loss : 0.017444, loss_ce: 0.004739
2022-01-10 04:28:14,628 iteration 5317 : loss : 0.014869, loss_ce: 0.004368
2022-01-10 04:28:16,082 iteration 5318 : loss : 0.027610, loss_ce: 0.012419
2022-01-10 04:28:17,482 iteration 5319 : loss : 0.021875, loss_ce: 0.007543
2022-01-10 04:28:18,844 iteration 5320 : loss : 0.014284, loss_ce: 0.004736
2022-01-10 04:28:20,220 iteration 5321 : loss : 0.017199, loss_ce: 0.006264
 78%|██████████████████████▋      | 313/400 [2:16:26<36:35, 25.24s/it]2022-01-10 04:28:21,628 iteration 5322 : loss : 0.020869, loss_ce: 0.007284
2022-01-10 04:28:23,064 iteration 5323 : loss : 0.019499, loss_ce: 0.005155
2022-01-10 04:28:24,449 iteration 5324 : loss : 0.017656, loss_ce: 0.005685
2022-01-10 04:28:25,913 iteration 5325 : loss : 0.021864, loss_ce: 0.009867
2022-01-10 04:28:27,366 iteration 5326 : loss : 0.016913, loss_ce: 0.006142
2022-01-10 04:28:28,780 iteration 5327 : loss : 0.019817, loss_ce: 0.007621
2022-01-10 04:28:30,220 iteration 5328 : loss : 0.011460, loss_ce: 0.004008
2022-01-10 04:28:31,660 iteration 5329 : loss : 0.020882, loss_ce: 0.012086
2022-01-10 04:28:33,116 iteration 5330 : loss : 0.013556, loss_ce: 0.004470
2022-01-10 04:28:34,470 iteration 5331 : loss : 0.017105, loss_ce: 0.006577
2022-01-10 04:28:35,838 iteration 5332 : loss : 0.018899, loss_ce: 0.008307
2022-01-10 04:28:37,229 iteration 5333 : loss : 0.018584, loss_ce: 0.005787
2022-01-10 04:28:38,611 iteration 5334 : loss : 0.014477, loss_ce: 0.004435
2022-01-10 04:28:40,112 iteration 5335 : loss : 0.024916, loss_ce: 0.009208
2022-01-10 04:28:41,537 iteration 5336 : loss : 0.025303, loss_ce: 0.008199
2022-01-10 04:28:42,945 iteration 5337 : loss : 0.019576, loss_ce: 0.006766
2022-01-10 04:28:44,263 iteration 5338 : loss : 0.012918, loss_ce: 0.004938
 78%|██████████████████████▊      | 314/400 [2:16:50<35:39, 24.88s/it]2022-01-10 04:28:45,716 iteration 5339 : loss : 0.018161, loss_ce: 0.005395
2022-01-10 04:28:47,124 iteration 5340 : loss : 0.018638, loss_ce: 0.012106
2022-01-10 04:28:48,556 iteration 5341 : loss : 0.017171, loss_ce: 0.007454
2022-01-10 04:28:50,005 iteration 5342 : loss : 0.019372, loss_ce: 0.004946
2022-01-10 04:28:51,385 iteration 5343 : loss : 0.019890, loss_ce: 0.006798
2022-01-10 04:28:52,763 iteration 5344 : loss : 0.016634, loss_ce: 0.007612
2022-01-10 04:28:54,116 iteration 5345 : loss : 0.016440, loss_ce: 0.004856
2022-01-10 04:28:55,503 iteration 5346 : loss : 0.017748, loss_ce: 0.006271
2022-01-10 04:28:56,998 iteration 5347 : loss : 0.018295, loss_ce: 0.007381
2022-01-10 04:28:58,438 iteration 5348 : loss : 0.022785, loss_ce: 0.009995
2022-01-10 04:28:59,846 iteration 5349 : loss : 0.018215, loss_ce: 0.008882
2022-01-10 04:29:01,197 iteration 5350 : loss : 0.011404, loss_ce: 0.004217
2022-01-10 04:29:02,650 iteration 5351 : loss : 0.016996, loss_ce: 0.002888
2022-01-10 04:29:04,061 iteration 5352 : loss : 0.012020, loss_ce: 0.004320
2022-01-10 04:29:05,515 iteration 5353 : loss : 0.019562, loss_ce: 0.008874
2022-01-10 04:29:06,877 iteration 5354 : loss : 0.013728, loss_ce: 0.005591
2022-01-10 04:29:06,878 Training Data Eval:
2022-01-10 04:29:13,937   Average segmentation loss on training set: 0.0096
2022-01-10 04:29:13,937 Validation Data Eval:
2022-01-10 04:29:16,369   Average segmentation loss on validation set: 0.0792
2022-01-10 04:29:17,718 iteration 5355 : loss : 0.012109, loss_ce: 0.004348
 79%|██████████████████████▊      | 315/400 [2:17:24<38:53, 27.45s/it]2022-01-10 04:29:19,235 iteration 5356 : loss : 0.021048, loss_ce: 0.008812
2022-01-10 04:29:20,559 iteration 5357 : loss : 0.009973, loss_ce: 0.002807
2022-01-10 04:29:21,960 iteration 5358 : loss : 0.016172, loss_ce: 0.006390
2022-01-10 04:29:23,331 iteration 5359 : loss : 0.014631, loss_ce: 0.008311
2022-01-10 04:29:24,725 iteration 5360 : loss : 0.013436, loss_ce: 0.004673
2022-01-10 04:29:26,196 iteration 5361 : loss : 0.020176, loss_ce: 0.006294
2022-01-10 04:29:27,655 iteration 5362 : loss : 0.022963, loss_ce: 0.013286
2022-01-10 04:29:29,062 iteration 5363 : loss : 0.013298, loss_ce: 0.005589
2022-01-10 04:29:30,379 iteration 5364 : loss : 0.013639, loss_ce: 0.001923
2022-01-10 04:29:31,832 iteration 5365 : loss : 0.014772, loss_ce: 0.004528
2022-01-10 04:29:33,183 iteration 5366 : loss : 0.019173, loss_ce: 0.005355
2022-01-10 04:29:34,613 iteration 5367 : loss : 0.022843, loss_ce: 0.006010
2022-01-10 04:29:36,106 iteration 5368 : loss : 0.013904, loss_ce: 0.005154
2022-01-10 04:29:37,528 iteration 5369 : loss : 0.021353, loss_ce: 0.010334
2022-01-10 04:29:38,951 iteration 5370 : loss : 0.020514, loss_ce: 0.005922
2022-01-10 04:29:40,429 iteration 5371 : loss : 0.018073, loss_ce: 0.005450
2022-01-10 04:29:41,817 iteration 5372 : loss : 0.025548, loss_ce: 0.009888
 79%|██████████████████████▉      | 316/400 [2:17:48<37:01, 26.45s/it]2022-01-10 04:29:43,203 iteration 5373 : loss : 0.012714, loss_ce: 0.005463
2022-01-10 04:29:44,630 iteration 5374 : loss : 0.020964, loss_ce: 0.005666
2022-01-10 04:29:45,990 iteration 5375 : loss : 0.013256, loss_ce: 0.004474
2022-01-10 04:29:47,336 iteration 5376 : loss : 0.013199, loss_ce: 0.004081
2022-01-10 04:29:48,636 iteration 5377 : loss : 0.013517, loss_ce: 0.002218
2022-01-10 04:29:49,932 iteration 5378 : loss : 0.010884, loss_ce: 0.004075
2022-01-10 04:29:51,352 iteration 5379 : loss : 0.014056, loss_ce: 0.004737
2022-01-10 04:29:52,717 iteration 5380 : loss : 0.013932, loss_ce: 0.005941
2022-01-10 04:29:54,122 iteration 5381 : loss : 0.024760, loss_ce: 0.008762
2022-01-10 04:29:55,564 iteration 5382 : loss : 0.018387, loss_ce: 0.004638
2022-01-10 04:29:57,017 iteration 5383 : loss : 0.027440, loss_ce: 0.014926
2022-01-10 04:29:58,413 iteration 5384 : loss : 0.013206, loss_ce: 0.005808
2022-01-10 04:29:59,814 iteration 5385 : loss : 0.017114, loss_ce: 0.007107
2022-01-10 04:30:01,261 iteration 5386 : loss : 0.017265, loss_ce: 0.007218
2022-01-10 04:30:02,675 iteration 5387 : loss : 0.015355, loss_ce: 0.006582
2022-01-10 04:30:03,999 iteration 5388 : loss : 0.012478, loss_ce: 0.004854
2022-01-10 04:30:05,380 iteration 5389 : loss : 0.011856, loss_ce: 0.004172
 79%|██████████████████████▉      | 317/400 [2:18:11<35:22, 25.58s/it]2022-01-10 04:30:06,914 iteration 5390 : loss : 0.025306, loss_ce: 0.008212
2022-01-10 04:30:08,227 iteration 5391 : loss : 0.014265, loss_ce: 0.004557
2022-01-10 04:30:09,658 iteration 5392 : loss : 0.013328, loss_ce: 0.005361
2022-01-10 04:30:11,100 iteration 5393 : loss : 0.028745, loss_ce: 0.008545
2022-01-10 04:30:12,495 iteration 5394 : loss : 0.013527, loss_ce: 0.005354
2022-01-10 04:30:13,845 iteration 5395 : loss : 0.013429, loss_ce: 0.005799
2022-01-10 04:30:15,291 iteration 5396 : loss : 0.023364, loss_ce: 0.007006
2022-01-10 04:30:16,705 iteration 5397 : loss : 0.017990, loss_ce: 0.007075
2022-01-10 04:30:18,010 iteration 5398 : loss : 0.011412, loss_ce: 0.005791
2022-01-10 04:30:19,384 iteration 5399 : loss : 0.013146, loss_ce: 0.005753
2022-01-10 04:30:20,859 iteration 5400 : loss : 0.020348, loss_ce: 0.006393
2022-01-10 04:30:22,305 iteration 5401 : loss : 0.014716, loss_ce: 0.004566
2022-01-10 04:30:23,702 iteration 5402 : loss : 0.014781, loss_ce: 0.005528
2022-01-10 04:30:25,144 iteration 5403 : loss : 0.020361, loss_ce: 0.005997
2022-01-10 04:30:26,516 iteration 5404 : loss : 0.014472, loss_ce: 0.006974
2022-01-10 04:30:27,861 iteration 5405 : loss : 0.015109, loss_ce: 0.005139
2022-01-10 04:30:29,172 iteration 5406 : loss : 0.017891, loss_ce: 0.004151
 80%|███████████████████████      | 318/400 [2:18:35<34:13, 25.05s/it]2022-01-10 04:30:30,699 iteration 5407 : loss : 0.017837, loss_ce: 0.005554
2022-01-10 04:30:32,081 iteration 5408 : loss : 0.016303, loss_ce: 0.004743
2022-01-10 04:30:33,432 iteration 5409 : loss : 0.011081, loss_ce: 0.004942
2022-01-10 04:30:34,799 iteration 5410 : loss : 0.014384, loss_ce: 0.004798
2022-01-10 04:30:36,226 iteration 5411 : loss : 0.020504, loss_ce: 0.007382
2022-01-10 04:30:37,668 iteration 5412 : loss : 0.019050, loss_ce: 0.008777
2022-01-10 04:30:39,021 iteration 5413 : loss : 0.014001, loss_ce: 0.005416
2022-01-10 04:30:40,475 iteration 5414 : loss : 0.021645, loss_ce: 0.008231
2022-01-10 04:30:41,869 iteration 5415 : loss : 0.014327, loss_ce: 0.004783
2022-01-10 04:30:43,324 iteration 5416 : loss : 0.016702, loss_ce: 0.007096
2022-01-10 04:30:44,697 iteration 5417 : loss : 0.017841, loss_ce: 0.006169
2022-01-10 04:30:46,091 iteration 5418 : loss : 0.014694, loss_ce: 0.005022
2022-01-10 04:30:47,463 iteration 5419 : loss : 0.012813, loss_ce: 0.005849
2022-01-10 04:30:48,873 iteration 5420 : loss : 0.013464, loss_ce: 0.005825
2022-01-10 04:30:50,310 iteration 5421 : loss : 0.020949, loss_ce: 0.006282
2022-01-10 04:30:51,729 iteration 5422 : loss : 0.018451, loss_ce: 0.005864
2022-01-10 04:30:53,188 iteration 5423 : loss : 0.027725, loss_ce: 0.011731
 80%|███████████████████████▏     | 319/400 [2:18:59<33:23, 24.73s/it]2022-01-10 04:30:54,680 iteration 5424 : loss : 0.016685, loss_ce: 0.006371
2022-01-10 04:30:56,103 iteration 5425 : loss : 0.020148, loss_ce: 0.007688
2022-01-10 04:30:57,535 iteration 5426 : loss : 0.013955, loss_ce: 0.003700
2022-01-10 04:30:59,054 iteration 5427 : loss : 0.025705, loss_ce: 0.011271
2022-01-10 04:31:00,533 iteration 5428 : loss : 0.035104, loss_ce: 0.014624
2022-01-10 04:31:01,907 iteration 5429 : loss : 0.013749, loss_ce: 0.005860
2022-01-10 04:31:03,241 iteration 5430 : loss : 0.013198, loss_ce: 0.005534
2022-01-10 04:31:04,713 iteration 5431 : loss : 0.026885, loss_ce: 0.009684
2022-01-10 04:31:06,094 iteration 5432 : loss : 0.019091, loss_ce: 0.006305
2022-01-10 04:31:07,622 iteration 5433 : loss : 0.034603, loss_ce: 0.012854
2022-01-10 04:31:09,116 iteration 5434 : loss : 0.031872, loss_ce: 0.010709
2022-01-10 04:31:10,556 iteration 5435 : loss : 0.014841, loss_ce: 0.006216
2022-01-10 04:31:12,016 iteration 5436 : loss : 0.023329, loss_ce: 0.011856
2022-01-10 04:31:13,394 iteration 5437 : loss : 0.014938, loss_ce: 0.003782
2022-01-10 04:31:14,807 iteration 5438 : loss : 0.017637, loss_ce: 0.007679
2022-01-10 04:31:16,245 iteration 5439 : loss : 0.019635, loss_ce: 0.006785
2022-01-10 04:31:16,245 Training Data Eval:
2022-01-10 04:31:23,390   Average segmentation loss on training set: 0.0103
2022-01-10 04:31:23,390 Validation Data Eval:
2022-01-10 04:31:25,982   Average segmentation loss on validation set: 0.0882
2022-01-10 04:31:27,455 iteration 5440 : loss : 0.015972, loss_ce: 0.006349
 80%|███████████████████████▏     | 320/400 [2:19:33<36:47, 27.60s/it]2022-01-10 04:31:28,887 iteration 5441 : loss : 0.012892, loss_ce: 0.005542
2022-01-10 04:31:30,420 iteration 5442 : loss : 0.027709, loss_ce: 0.011000
2022-01-10 04:31:31,872 iteration 5443 : loss : 0.022287, loss_ce: 0.008736
2022-01-10 04:31:33,270 iteration 5444 : loss : 0.014827, loss_ce: 0.006903
2022-01-10 04:31:34,653 iteration 5445 : loss : 0.024448, loss_ce: 0.011261
2022-01-10 04:31:36,060 iteration 5446 : loss : 0.017884, loss_ce: 0.007112
2022-01-10 04:31:37,398 iteration 5447 : loss : 0.017006, loss_ce: 0.002938
2022-01-10 04:31:38,826 iteration 5448 : loss : 0.015539, loss_ce: 0.004719
2022-01-10 04:31:40,214 iteration 5449 : loss : 0.015290, loss_ce: 0.004592
2022-01-10 04:31:41,583 iteration 5450 : loss : 0.018943, loss_ce: 0.006326
2022-01-10 04:31:42,997 iteration 5451 : loss : 0.013022, loss_ce: 0.003656
2022-01-10 04:31:44,415 iteration 5452 : loss : 0.012172, loss_ce: 0.004691
2022-01-10 04:31:45,866 iteration 5453 : loss : 0.017112, loss_ce: 0.006452
2022-01-10 04:31:47,293 iteration 5454 : loss : 0.019616, loss_ce: 0.008275
2022-01-10 04:31:48,704 iteration 5455 : loss : 0.032749, loss_ce: 0.015703
2022-01-10 04:31:50,140 iteration 5456 : loss : 0.015338, loss_ce: 0.004542
2022-01-10 04:31:51,568 iteration 5457 : loss : 0.017115, loss_ce: 0.006891
 80%|███████████████████████▎     | 321/400 [2:19:58<34:57, 26.55s/it]2022-01-10 04:31:53,106 iteration 5458 : loss : 0.022440, loss_ce: 0.006070
2022-01-10 04:31:54,469 iteration 5459 : loss : 0.016992, loss_ce: 0.006637
2022-01-10 04:31:55,882 iteration 5460 : loss : 0.021620, loss_ce: 0.009461
2022-01-10 04:31:57,316 iteration 5461 : loss : 0.017760, loss_ce: 0.006481
2022-01-10 04:31:58,767 iteration 5462 : loss : 0.017657, loss_ce: 0.007614
2022-01-10 04:32:00,144 iteration 5463 : loss : 0.011421, loss_ce: 0.003808
2022-01-10 04:32:01,684 iteration 5464 : loss : 0.024933, loss_ce: 0.009980
2022-01-10 04:32:03,063 iteration 5465 : loss : 0.016927, loss_ce: 0.006142
2022-01-10 04:32:04,441 iteration 5466 : loss : 0.016003, loss_ce: 0.006613
2022-01-10 04:32:05,915 iteration 5467 : loss : 0.014983, loss_ce: 0.004855
2022-01-10 04:32:07,380 iteration 5468 : loss : 0.015344, loss_ce: 0.007014
2022-01-10 04:32:08,848 iteration 5469 : loss : 0.033563, loss_ce: 0.012931
2022-01-10 04:32:10,238 iteration 5470 : loss : 0.015066, loss_ce: 0.008164
2022-01-10 04:32:11,622 iteration 5471 : loss : 0.013795, loss_ce: 0.006984
2022-01-10 04:32:13,097 iteration 5472 : loss : 0.017379, loss_ce: 0.007571
2022-01-10 04:32:14,479 iteration 5473 : loss : 0.014762, loss_ce: 0.003945
2022-01-10 04:32:15,940 iteration 5474 : loss : 0.019861, loss_ce: 0.005313
 80%|███████████████████████▎     | 322/400 [2:20:22<33:39, 25.90s/it]2022-01-10 04:32:17,373 iteration 5475 : loss : 0.018079, loss_ce: 0.006130
2022-01-10 04:32:18,765 iteration 5476 : loss : 0.012309, loss_ce: 0.005698
2022-01-10 04:32:20,214 iteration 5477 : loss : 0.011807, loss_ce: 0.003953
2022-01-10 04:32:21,681 iteration 5478 : loss : 0.015595, loss_ce: 0.006853
2022-01-10 04:32:23,057 iteration 5479 : loss : 0.016778, loss_ce: 0.004182
2022-01-10 04:32:24,465 iteration 5480 : loss : 0.014299, loss_ce: 0.003592
2022-01-10 04:32:25,889 iteration 5481 : loss : 0.017900, loss_ce: 0.007160
2022-01-10 04:32:27,309 iteration 5482 : loss : 0.015623, loss_ce: 0.005617
2022-01-10 04:32:28,665 iteration 5483 : loss : 0.018272, loss_ce: 0.006567
2022-01-10 04:32:30,132 iteration 5484 : loss : 0.023038, loss_ce: 0.009153
2022-01-10 04:32:31,476 iteration 5485 : loss : 0.012043, loss_ce: 0.005612
2022-01-10 04:32:33,004 iteration 5486 : loss : 0.014518, loss_ce: 0.004153
2022-01-10 04:32:34,363 iteration 5487 : loss : 0.016142, loss_ce: 0.009192
2022-01-10 04:32:35,803 iteration 5488 : loss : 0.035349, loss_ce: 0.012181
2022-01-10 04:32:37,164 iteration 5489 : loss : 0.020190, loss_ce: 0.004928
2022-01-10 04:32:38,608 iteration 5490 : loss : 0.015198, loss_ce: 0.005847
2022-01-10 04:32:39,981 iteration 5491 : loss : 0.014718, loss_ce: 0.005190
 81%|███████████████████████▍     | 323/400 [2:20:46<32:31, 25.34s/it]2022-01-10 04:32:41,471 iteration 5492 : loss : 0.018647, loss_ce: 0.008038
2022-01-10 04:32:42,824 iteration 5493 : loss : 0.015449, loss_ce: 0.005520
2022-01-10 04:32:44,202 iteration 5494 : loss : 0.015080, loss_ce: 0.006978
2022-01-10 04:32:45,674 iteration 5495 : loss : 0.021548, loss_ce: 0.004412
2022-01-10 04:32:47,033 iteration 5496 : loss : 0.015132, loss_ce: 0.004767
2022-01-10 04:32:48,454 iteration 5497 : loss : 0.016302, loss_ce: 0.008297
2022-01-10 04:32:49,912 iteration 5498 : loss : 0.018060, loss_ce: 0.007831
2022-01-10 04:32:51,327 iteration 5499 : loss : 0.015384, loss_ce: 0.005042
2022-01-10 04:32:52,737 iteration 5500 : loss : 0.014056, loss_ce: 0.004305
2022-01-10 04:32:54,121 iteration 5501 : loss : 0.011323, loss_ce: 0.004000
2022-01-10 04:32:55,499 iteration 5502 : loss : 0.016945, loss_ce: 0.009128
2022-01-10 04:32:57,035 iteration 5503 : loss : 0.021279, loss_ce: 0.006127
2022-01-10 04:32:58,465 iteration 5504 : loss : 0.016516, loss_ce: 0.006691
2022-01-10 04:32:59,810 iteration 5505 : loss : 0.011977, loss_ce: 0.004278
2022-01-10 04:33:01,179 iteration 5506 : loss : 0.014047, loss_ce: 0.005395
2022-01-10 04:33:02,664 iteration 5507 : loss : 0.019086, loss_ce: 0.007041
2022-01-10 04:33:04,115 iteration 5508 : loss : 0.019580, loss_ce: 0.008668
 81%|███████████████████████▍     | 324/400 [2:21:10<31:38, 24.98s/it]2022-01-10 04:33:05,620 iteration 5509 : loss : 0.017571, loss_ce: 0.008385
2022-01-10 04:33:07,010 iteration 5510 : loss : 0.013796, loss_ce: 0.005007
2022-01-10 04:33:08,438 iteration 5511 : loss : 0.025745, loss_ce: 0.008122
2022-01-10 04:33:09,839 iteration 5512 : loss : 0.016715, loss_ce: 0.008301
2022-01-10 04:33:11,238 iteration 5513 : loss : 0.018268, loss_ce: 0.011492
2022-01-10 04:33:12,701 iteration 5514 : loss : 0.022691, loss_ce: 0.009832
2022-01-10 04:33:14,103 iteration 5515 : loss : 0.013826, loss_ce: 0.005239
2022-01-10 04:33:15,506 iteration 5516 : loss : 0.013479, loss_ce: 0.006315
2022-01-10 04:33:16,937 iteration 5517 : loss : 0.040290, loss_ce: 0.016955
2022-01-10 04:33:18,274 iteration 5518 : loss : 0.012023, loss_ce: 0.004067
2022-01-10 04:33:19,760 iteration 5519 : loss : 0.025877, loss_ce: 0.007102
2022-01-10 04:33:21,160 iteration 5520 : loss : 0.011539, loss_ce: 0.001912
2022-01-10 04:33:22,656 iteration 5521 : loss : 0.017503, loss_ce: 0.006072
2022-01-10 04:33:24,064 iteration 5522 : loss : 0.017049, loss_ce: 0.006486
2022-01-10 04:33:25,520 iteration 5523 : loss : 0.023845, loss_ce: 0.008296
2022-01-10 04:33:26,998 iteration 5524 : loss : 0.019106, loss_ce: 0.006950
2022-01-10 04:33:26,998 Training Data Eval:
2022-01-10 04:33:34,048   Average segmentation loss on training set: 0.0095
2022-01-10 04:33:34,048 Validation Data Eval:
2022-01-10 04:33:36,485   Average segmentation loss on validation set: 0.0741
2022-01-10 04:33:37,982 iteration 5525 : loss : 0.026379, loss_ce: 0.011791
 81%|███████████████████████▌     | 325/400 [2:21:44<34:33, 27.65s/it]2022-01-10 04:33:39,444 iteration 5526 : loss : 0.017964, loss_ce: 0.005588
2022-01-10 04:33:40,785 iteration 5527 : loss : 0.013668, loss_ce: 0.006146
2022-01-10 04:33:42,261 iteration 5528 : loss : 0.036356, loss_ce: 0.012918
2022-01-10 04:33:43,640 iteration 5529 : loss : 0.012505, loss_ce: 0.004452
2022-01-10 04:33:44,999 iteration 5530 : loss : 0.012905, loss_ce: 0.005218
2022-01-10 04:33:46,321 iteration 5531 : loss : 0.011032, loss_ce: 0.003417
2022-01-10 04:33:47,669 iteration 5532 : loss : 0.020988, loss_ce: 0.007942
2022-01-10 04:33:49,107 iteration 5533 : loss : 0.017145, loss_ce: 0.007559
2022-01-10 04:33:50,446 iteration 5534 : loss : 0.012645, loss_ce: 0.004243
2022-01-10 04:33:51,825 iteration 5535 : loss : 0.018098, loss_ce: 0.007126
2022-01-10 04:33:53,273 iteration 5536 : loss : 0.023105, loss_ce: 0.007289
2022-01-10 04:33:54,725 iteration 5537 : loss : 0.021286, loss_ce: 0.010936
2022-01-10 04:33:56,216 iteration 5538 : loss : 0.018659, loss_ce: 0.005249
2022-01-10 04:33:57,633 iteration 5539 : loss : 0.012884, loss_ce: 0.004668
2022-01-10 04:33:58,972 iteration 5540 : loss : 0.017504, loss_ce: 0.006830
2022-01-10 04:34:00,484 iteration 5541 : loss : 0.028327, loss_ce: 0.011516
2022-01-10 04:34:01,906 iteration 5542 : loss : 0.026304, loss_ce: 0.013832
 82%|███████████████████████▋     | 326/400 [2:22:08<32:43, 26.53s/it]2022-01-10 04:34:03,427 iteration 5543 : loss : 0.015258, loss_ce: 0.007119
2022-01-10 04:34:04,803 iteration 5544 : loss : 0.013714, loss_ce: 0.004968
2022-01-10 04:34:06,251 iteration 5545 : loss : 0.014764, loss_ce: 0.007544
2022-01-10 04:34:07,754 iteration 5546 : loss : 0.039873, loss_ce: 0.008779
2022-01-10 04:34:09,213 iteration 5547 : loss : 0.017072, loss_ce: 0.007429
2022-01-10 04:34:10,604 iteration 5548 : loss : 0.015074, loss_ce: 0.003484
2022-01-10 04:34:11,993 iteration 5549 : loss : 0.015992, loss_ce: 0.006389
2022-01-10 04:34:13,423 iteration 5550 : loss : 0.016595, loss_ce: 0.005851
2022-01-10 04:34:14,804 iteration 5551 : loss : 0.022961, loss_ce: 0.010411
2022-01-10 04:34:16,097 iteration 5552 : loss : 0.012596, loss_ce: 0.003989
2022-01-10 04:34:17,511 iteration 5553 : loss : 0.015292, loss_ce: 0.006180
2022-01-10 04:34:18,902 iteration 5554 : loss : 0.014217, loss_ce: 0.006264
2022-01-10 04:34:20,230 iteration 5555 : loss : 0.014621, loss_ce: 0.006394
2022-01-10 04:34:21,689 iteration 5556 : loss : 0.024451, loss_ce: 0.008267
2022-01-10 04:34:23,124 iteration 5557 : loss : 0.014724, loss_ce: 0.007118
2022-01-10 04:34:24,534 iteration 5558 : loss : 0.020500, loss_ce: 0.007363
2022-01-10 04:34:25,930 iteration 5559 : loss : 0.025787, loss_ce: 0.006715
 82%|███████████████████████▋     | 327/400 [2:22:32<31:21, 25.78s/it]2022-01-10 04:34:27,409 iteration 5560 : loss : 0.022628, loss_ce: 0.007769
2022-01-10 04:34:28,786 iteration 5561 : loss : 0.017663, loss_ce: 0.006063
2022-01-10 04:34:30,180 iteration 5562 : loss : 0.013692, loss_ce: 0.005945
2022-01-10 04:34:31,567 iteration 5563 : loss : 0.015694, loss_ce: 0.006619
2022-01-10 04:34:33,009 iteration 5564 : loss : 0.023583, loss_ce: 0.012007
2022-01-10 04:34:34,460 iteration 5565 : loss : 0.015900, loss_ce: 0.007353
2022-01-10 04:34:35,874 iteration 5566 : loss : 0.014669, loss_ce: 0.005885
2022-01-10 04:34:37,320 iteration 5567 : loss : 0.017514, loss_ce: 0.008891
2022-01-10 04:34:38,713 iteration 5568 : loss : 0.019808, loss_ce: 0.007495
2022-01-10 04:34:40,129 iteration 5569 : loss : 0.021540, loss_ce: 0.009652
2022-01-10 04:34:41,491 iteration 5570 : loss : 0.010962, loss_ce: 0.003270
2022-01-10 04:34:42,800 iteration 5571 : loss : 0.011158, loss_ce: 0.004200
2022-01-10 04:34:44,161 iteration 5572 : loss : 0.019749, loss_ce: 0.006212
2022-01-10 04:34:45,495 iteration 5573 : loss : 0.010140, loss_ce: 0.002956
2022-01-10 04:34:46,891 iteration 5574 : loss : 0.013358, loss_ce: 0.004191
2022-01-10 04:34:48,386 iteration 5575 : loss : 0.027807, loss_ce: 0.010329
2022-01-10 04:34:49,821 iteration 5576 : loss : 0.018187, loss_ce: 0.006589
 82%|███████████████████████▊     | 328/400 [2:22:56<30:15, 25.21s/it]2022-01-10 04:34:51,299 iteration 5577 : loss : 0.018552, loss_ce: 0.007518
2022-01-10 04:34:52,738 iteration 5578 : loss : 0.021725, loss_ce: 0.007689
2022-01-10 04:34:54,213 iteration 5579 : loss : 0.018908, loss_ce: 0.008396
2022-01-10 04:34:55,600 iteration 5580 : loss : 0.020066, loss_ce: 0.007028
2022-01-10 04:34:56,956 iteration 5581 : loss : 0.012977, loss_ce: 0.005119
2022-01-10 04:34:58,366 iteration 5582 : loss : 0.022337, loss_ce: 0.006978
2022-01-10 04:34:59,763 iteration 5583 : loss : 0.010267, loss_ce: 0.003723
2022-01-10 04:35:01,217 iteration 5584 : loss : 0.028808, loss_ce: 0.011921
2022-01-10 04:35:02,607 iteration 5585 : loss : 0.013210, loss_ce: 0.003854
2022-01-10 04:35:04,076 iteration 5586 : loss : 0.017557, loss_ce: 0.005718
2022-01-10 04:35:05,432 iteration 5587 : loss : 0.016069, loss_ce: 0.005479
2022-01-10 04:35:06,822 iteration 5588 : loss : 0.015150, loss_ce: 0.005246
2022-01-10 04:35:08,165 iteration 5589 : loss : 0.011268, loss_ce: 0.003138
2022-01-10 04:35:09,531 iteration 5590 : loss : 0.010335, loss_ce: 0.002728
2022-01-10 04:35:10,908 iteration 5591 : loss : 0.013767, loss_ce: 0.005606
2022-01-10 04:35:12,378 iteration 5592 : loss : 0.019217, loss_ce: 0.006315
2022-01-10 04:35:13,816 iteration 5593 : loss : 0.018739, loss_ce: 0.008282
 82%|███████████████████████▊     | 329/400 [2:23:20<29:24, 24.85s/it]2022-01-10 04:35:15,235 iteration 5594 : loss : 0.016375, loss_ce: 0.004717
2022-01-10 04:35:16,680 iteration 5595 : loss : 0.021976, loss_ce: 0.010371
2022-01-10 04:35:18,063 iteration 5596 : loss : 0.016627, loss_ce: 0.006693
2022-01-10 04:35:19,529 iteration 5597 : loss : 0.014812, loss_ce: 0.005085
2022-01-10 04:35:20,880 iteration 5598 : loss : 0.011257, loss_ce: 0.005079
2022-01-10 04:35:22,282 iteration 5599 : loss : 0.020682, loss_ce: 0.008279
2022-01-10 04:35:23,672 iteration 5600 : loss : 0.013508, loss_ce: 0.005632
2022-01-10 04:35:25,045 iteration 5601 : loss : 0.010247, loss_ce: 0.003819
2022-01-10 04:35:26,467 iteration 5602 : loss : 0.017907, loss_ce: 0.007203
2022-01-10 04:35:27,822 iteration 5603 : loss : 0.011293, loss_ce: 0.004452
2022-01-10 04:35:29,244 iteration 5604 : loss : 0.018656, loss_ce: 0.004300
2022-01-10 04:35:30,679 iteration 5605 : loss : 0.019782, loss_ce: 0.008833
2022-01-10 04:35:32,099 iteration 5606 : loss : 0.014522, loss_ce: 0.004597
2022-01-10 04:35:33,510 iteration 5607 : loss : 0.015684, loss_ce: 0.005075
2022-01-10 04:35:34,914 iteration 5608 : loss : 0.014125, loss_ce: 0.004652
2022-01-10 04:35:36,354 iteration 5609 : loss : 0.013215, loss_ce: 0.005254
2022-01-10 04:35:36,354 Training Data Eval:
2022-01-10 04:35:43,406   Average segmentation loss on training set: 0.0090
2022-01-10 04:35:43,406 Validation Data Eval:
2022-01-10 04:35:45,851   Average segmentation loss on validation set: 0.0830
2022-01-10 04:35:47,211 iteration 5610 : loss : 0.012715, loss_ce: 0.004480
 82%|███████████████████████▉     | 330/400 [2:23:53<31:58, 27.41s/it]2022-01-10 04:35:48,655 iteration 5611 : loss : 0.024837, loss_ce: 0.009026
2022-01-10 04:35:50,076 iteration 5612 : loss : 0.024795, loss_ce: 0.005347
2022-01-10 04:35:51,481 iteration 5613 : loss : 0.015847, loss_ce: 0.005959
2022-01-10 04:35:52,883 iteration 5614 : loss : 0.016288, loss_ce: 0.006449
2022-01-10 04:35:54,208 iteration 5615 : loss : 0.010680, loss_ce: 0.004564
2022-01-10 04:35:55,641 iteration 5616 : loss : 0.013485, loss_ce: 0.005841
2022-01-10 04:35:57,102 iteration 5617 : loss : 0.013448, loss_ce: 0.005252
2022-01-10 04:35:58,534 iteration 5618 : loss : 0.013051, loss_ce: 0.005146
2022-01-10 04:35:59,918 iteration 5619 : loss : 0.020386, loss_ce: 0.005813
2022-01-10 04:36:01,385 iteration 5620 : loss : 0.014364, loss_ce: 0.005201
2022-01-10 04:36:02,727 iteration 5621 : loss : 0.013728, loss_ce: 0.004721
2022-01-10 04:36:04,139 iteration 5622 : loss : 0.016593, loss_ce: 0.006768
2022-01-10 04:36:05,626 iteration 5623 : loss : 0.029719, loss_ce: 0.012146
2022-01-10 04:36:06,982 iteration 5624 : loss : 0.024289, loss_ce: 0.005931
2022-01-10 04:36:08,334 iteration 5625 : loss : 0.013463, loss_ce: 0.006060
2022-01-10 04:36:09,710 iteration 5626 : loss : 0.015242, loss_ce: 0.003822
2022-01-10 04:36:11,142 iteration 5627 : loss : 0.014109, loss_ce: 0.005909
 83%|███████████████████████▉     | 331/400 [2:24:17<30:19, 26.37s/it]2022-01-10 04:36:12,702 iteration 5628 : loss : 0.027578, loss_ce: 0.013294
2022-01-10 04:36:14,071 iteration 5629 : loss : 0.012678, loss_ce: 0.005210
2022-01-10 04:36:15,510 iteration 5630 : loss : 0.018644, loss_ce: 0.008517
2022-01-10 04:36:16,908 iteration 5631 : loss : 0.016107, loss_ce: 0.005659
2022-01-10 04:36:18,351 iteration 5632 : loss : 0.015538, loss_ce: 0.006819
2022-01-10 04:36:19,818 iteration 5633 : loss : 0.018538, loss_ce: 0.007264
2022-01-10 04:36:21,234 iteration 5634 : loss : 0.010056, loss_ce: 0.004007
2022-01-10 04:36:22,629 iteration 5635 : loss : 0.017570, loss_ce: 0.005899
2022-01-10 04:36:24,047 iteration 5636 : loss : 0.016424, loss_ce: 0.005556
2022-01-10 04:36:25,447 iteration 5637 : loss : 0.024356, loss_ce: 0.009707
2022-01-10 04:36:26,861 iteration 5638 : loss : 0.013347, loss_ce: 0.005612
2022-01-10 04:36:28,290 iteration 5639 : loss : 0.012296, loss_ce: 0.003959
2022-01-10 04:36:29,717 iteration 5640 : loss : 0.016547, loss_ce: 0.007209
2022-01-10 04:36:31,061 iteration 5641 : loss : 0.025894, loss_ce: 0.006285
2022-01-10 04:36:32,421 iteration 5642 : loss : 0.013413, loss_ce: 0.005281
2022-01-10 04:36:33,818 iteration 5643 : loss : 0.013665, loss_ce: 0.004398
2022-01-10 04:36:35,199 iteration 5644 : loss : 0.020061, loss_ce: 0.009742
 83%|████████████████████████     | 332/400 [2:24:41<29:06, 25.68s/it]2022-01-10 04:36:36,634 iteration 5645 : loss : 0.014119, loss_ce: 0.004816
2022-01-10 04:36:38,030 iteration 5646 : loss : 0.014872, loss_ce: 0.005404
2022-01-10 04:36:39,472 iteration 5647 : loss : 0.015194, loss_ce: 0.004849
2022-01-10 04:36:40,890 iteration 5648 : loss : 0.015773, loss_ce: 0.004374
2022-01-10 04:36:42,292 iteration 5649 : loss : 0.015005, loss_ce: 0.005521
2022-01-10 04:36:43,744 iteration 5650 : loss : 0.018270, loss_ce: 0.005003
2022-01-10 04:36:45,141 iteration 5651 : loss : 0.013795, loss_ce: 0.007149
2022-01-10 04:36:46,514 iteration 5652 : loss : 0.019336, loss_ce: 0.008626
2022-01-10 04:36:47,894 iteration 5653 : loss : 0.016460, loss_ce: 0.005875
2022-01-10 04:36:49,373 iteration 5654 : loss : 0.014773, loss_ce: 0.005569
2022-01-10 04:36:50,766 iteration 5655 : loss : 0.022680, loss_ce: 0.007639
2022-01-10 04:36:52,199 iteration 5656 : loss : 0.018219, loss_ce: 0.008558
2022-01-10 04:36:53,585 iteration 5657 : loss : 0.015057, loss_ce: 0.007524
2022-01-10 04:36:55,053 iteration 5658 : loss : 0.027070, loss_ce: 0.007480
2022-01-10 04:36:56,550 iteration 5659 : loss : 0.013922, loss_ce: 0.006128
2022-01-10 04:36:57,875 iteration 5660 : loss : 0.010838, loss_ce: 0.004150
2022-01-10 04:36:59,311 iteration 5661 : loss : 0.018162, loss_ce: 0.006803
 83%|████████████████████████▏    | 333/400 [2:25:05<28:08, 25.20s/it]2022-01-10 04:37:00,738 iteration 5662 : loss : 0.013313, loss_ce: 0.005482
2022-01-10 04:37:02,188 iteration 5663 : loss : 0.011955, loss_ce: 0.005189
2022-01-10 04:37:03,651 iteration 5664 : loss : 0.017095, loss_ce: 0.005713
2022-01-10 04:37:05,042 iteration 5665 : loss : 0.010481, loss_ce: 0.002434
2022-01-10 04:37:06,513 iteration 5666 : loss : 0.012648, loss_ce: 0.004470
2022-01-10 04:37:07,944 iteration 5667 : loss : 0.016410, loss_ce: 0.005369
2022-01-10 04:37:09,376 iteration 5668 : loss : 0.021278, loss_ce: 0.007352
2022-01-10 04:37:10,859 iteration 5669 : loss : 0.020102, loss_ce: 0.009868
2022-01-10 04:37:12,287 iteration 5670 : loss : 0.020728, loss_ce: 0.007115
2022-01-10 04:37:13,766 iteration 5671 : loss : 0.033847, loss_ce: 0.006762
2022-01-10 04:37:15,097 iteration 5672 : loss : 0.012949, loss_ce: 0.003802
2022-01-10 04:37:16,490 iteration 5673 : loss : 0.016161, loss_ce: 0.007885
2022-01-10 04:37:17,845 iteration 5674 : loss : 0.017587, loss_ce: 0.005939
2022-01-10 04:37:19,360 iteration 5675 : loss : 0.020643, loss_ce: 0.007306
2022-01-10 04:37:20,768 iteration 5676 : loss : 0.016877, loss_ce: 0.008708
2022-01-10 04:37:22,144 iteration 5677 : loss : 0.015991, loss_ce: 0.005805
2022-01-10 04:37:23,542 iteration 5678 : loss : 0.026592, loss_ce: 0.010460
 84%|████████████████████████▏    | 334/400 [2:25:30<27:24, 24.91s/it]2022-01-10 04:37:25,037 iteration 5679 : loss : 0.021430, loss_ce: 0.007574
2022-01-10 04:37:26,439 iteration 5680 : loss : 0.016461, loss_ce: 0.007288
2022-01-10 04:37:27,883 iteration 5681 : loss : 0.020233, loss_ce: 0.006927
2022-01-10 04:37:29,219 iteration 5682 : loss : 0.012268, loss_ce: 0.004216
2022-01-10 04:37:30,548 iteration 5683 : loss : 0.009940, loss_ce: 0.004411
2022-01-10 04:37:32,013 iteration 5684 : loss : 0.026663, loss_ce: 0.016707
2022-01-10 04:37:33,443 iteration 5685 : loss : 0.017669, loss_ce: 0.005538
2022-01-10 04:37:34,852 iteration 5686 : loss : 0.019684, loss_ce: 0.006525
2022-01-10 04:37:36,289 iteration 5687 : loss : 0.017390, loss_ce: 0.004745
2022-01-10 04:37:37,763 iteration 5688 : loss : 0.033758, loss_ce: 0.012216
2022-01-10 04:37:39,150 iteration 5689 : loss : 0.013685, loss_ce: 0.005739
2022-01-10 04:37:40,507 iteration 5690 : loss : 0.013854, loss_ce: 0.005010
2022-01-10 04:37:41,931 iteration 5691 : loss : 0.014955, loss_ce: 0.006156
2022-01-10 04:37:43,332 iteration 5692 : loss : 0.011662, loss_ce: 0.003272
2022-01-10 04:37:44,622 iteration 5693 : loss : 0.012158, loss_ce: 0.004939
2022-01-10 04:37:46,036 iteration 5694 : loss : 0.016261, loss_ce: 0.006530
2022-01-10 04:37:46,037 Training Data Eval:
2022-01-10 04:37:53,078   Average segmentation loss on training set: 0.0089
2022-01-10 04:37:53,079 Validation Data Eval:
2022-01-10 04:37:55,503   Average segmentation loss on validation set: 0.0812
2022-01-10 04:37:56,893 iteration 5695 : loss : 0.015925, loss_ce: 0.006758
 84%|████████████████████████▎    | 335/400 [2:26:03<29:43, 27.44s/it]2022-01-10 04:37:58,373 iteration 5696 : loss : 0.018499, loss_ce: 0.005911
2022-01-10 04:37:59,737 iteration 5697 : loss : 0.011682, loss_ce: 0.003307
2022-01-10 04:38:01,201 iteration 5698 : loss : 0.020915, loss_ce: 0.007314
2022-01-10 04:38:02,680 iteration 5699 : loss : 0.014197, loss_ce: 0.003734
2022-01-10 04:38:04,003 iteration 5700 : loss : 0.011063, loss_ce: 0.004800
2022-01-10 04:38:05,500 iteration 5701 : loss : 0.025073, loss_ce: 0.004244
2022-01-10 04:38:06,932 iteration 5702 : loss : 0.014834, loss_ce: 0.005773
2022-01-10 04:38:08,410 iteration 5703 : loss : 0.019399, loss_ce: 0.008621
2022-01-10 04:38:09,788 iteration 5704 : loss : 0.016622, loss_ce: 0.009158
2022-01-10 04:38:11,164 iteration 5705 : loss : 0.014037, loss_ce: 0.006171
2022-01-10 04:38:12,526 iteration 5706 : loss : 0.013635, loss_ce: 0.005029
2022-01-10 04:38:13,956 iteration 5707 : loss : 0.021815, loss_ce: 0.005674
2022-01-10 04:38:15,360 iteration 5708 : loss : 0.014038, loss_ce: 0.006539
2022-01-10 04:38:16,718 iteration 5709 : loss : 0.036699, loss_ce: 0.011377
2022-01-10 04:38:18,098 iteration 5710 : loss : 0.015089, loss_ce: 0.006963
2022-01-10 04:38:19,510 iteration 5711 : loss : 0.011293, loss_ce: 0.004988
2022-01-10 04:38:20,918 iteration 5712 : loss : 0.016154, loss_ce: 0.006449
 84%|████████████████████████▎    | 336/400 [2:26:27<28:10, 26.42s/it]2022-01-10 04:38:22,396 iteration 5713 : loss : 0.017048, loss_ce: 0.008167
2022-01-10 04:38:23,829 iteration 5714 : loss : 0.021393, loss_ce: 0.007156
2022-01-10 04:38:25,185 iteration 5715 : loss : 0.021901, loss_ce: 0.007779
2022-01-10 04:38:26,629 iteration 5716 : loss : 0.017631, loss_ce: 0.006581
2022-01-10 04:38:27,975 iteration 5717 : loss : 0.011035, loss_ce: 0.003972
2022-01-10 04:38:29,338 iteration 5718 : loss : 0.019466, loss_ce: 0.008891
2022-01-10 04:38:30,782 iteration 5719 : loss : 0.028623, loss_ce: 0.011137
2022-01-10 04:38:32,243 iteration 5720 : loss : 0.030449, loss_ce: 0.011648
2022-01-10 04:38:33,595 iteration 5721 : loss : 0.015358, loss_ce: 0.004480
2022-01-10 04:38:35,034 iteration 5722 : loss : 0.019965, loss_ce: 0.007445
2022-01-10 04:38:36,502 iteration 5723 : loss : 0.021745, loss_ce: 0.009351
2022-01-10 04:38:37,935 iteration 5724 : loss : 0.017679, loss_ce: 0.006706
2022-01-10 04:38:39,323 iteration 5725 : loss : 0.014230, loss_ce: 0.005384
2022-01-10 04:38:40,771 iteration 5726 : loss : 0.011237, loss_ce: 0.004627
2022-01-10 04:38:42,195 iteration 5727 : loss : 0.018536, loss_ce: 0.010160
2022-01-10 04:38:43,568 iteration 5728 : loss : 0.013346, loss_ce: 0.004163
2022-01-10 04:38:44,902 iteration 5729 : loss : 0.012229, loss_ce: 0.003685
 84%|████████████████████████▍    | 337/400 [2:26:51<26:58, 25.69s/it]2022-01-10 04:38:46,452 iteration 5730 : loss : 0.018895, loss_ce: 0.007272
2022-01-10 04:38:47,928 iteration 5731 : loss : 0.021502, loss_ce: 0.007928
2022-01-10 04:38:49,375 iteration 5732 : loss : 0.021025, loss_ce: 0.004823
2022-01-10 04:38:50,784 iteration 5733 : loss : 0.017663, loss_ce: 0.006887
2022-01-10 04:38:52,160 iteration 5734 : loss : 0.012605, loss_ce: 0.004443
2022-01-10 04:38:53,663 iteration 5735 : loss : 0.026228, loss_ce: 0.008203
2022-01-10 04:38:55,137 iteration 5736 : loss : 0.018805, loss_ce: 0.007457
2022-01-10 04:38:56,544 iteration 5737 : loss : 0.015829, loss_ce: 0.006142
2022-01-10 04:38:57,923 iteration 5738 : loss : 0.017365, loss_ce: 0.008072
2022-01-10 04:38:59,346 iteration 5739 : loss : 0.012831, loss_ce: 0.004100
2022-01-10 04:39:00,812 iteration 5740 : loss : 0.018155, loss_ce: 0.007640
2022-01-10 04:39:02,231 iteration 5741 : loss : 0.012596, loss_ce: 0.005258
2022-01-10 04:39:03,616 iteration 5742 : loss : 0.026145, loss_ce: 0.004286
2022-01-10 04:39:05,031 iteration 5743 : loss : 0.015528, loss_ce: 0.005248
2022-01-10 04:39:06,402 iteration 5744 : loss : 0.019354, loss_ce: 0.008526
2022-01-10 04:39:07,824 iteration 5745 : loss : 0.011206, loss_ce: 0.003778
2022-01-10 04:39:09,195 iteration 5746 : loss : 0.016617, loss_ce: 0.007741
 84%|████████████████████████▌    | 338/400 [2:27:15<26:06, 25.27s/it]2022-01-10 04:39:10,700 iteration 5747 : loss : 0.019849, loss_ce: 0.007195
2022-01-10 04:39:12,083 iteration 5748 : loss : 0.020203, loss_ce: 0.008464
2022-01-10 04:39:13,537 iteration 5749 : loss : 0.022755, loss_ce: 0.010600
2022-01-10 04:39:14,960 iteration 5750 : loss : 0.015548, loss_ce: 0.003848
2022-01-10 04:39:16,428 iteration 5751 : loss : 0.026010, loss_ce: 0.010028
2022-01-10 04:39:17,853 iteration 5752 : loss : 0.014788, loss_ce: 0.004892
2022-01-10 04:39:19,302 iteration 5753 : loss : 0.017912, loss_ce: 0.007820
2022-01-10 04:39:20,697 iteration 5754 : loss : 0.013536, loss_ce: 0.004372
2022-01-10 04:39:22,080 iteration 5755 : loss : 0.016071, loss_ce: 0.004679
2022-01-10 04:39:23,454 iteration 5756 : loss : 0.015308, loss_ce: 0.007608
2022-01-10 04:39:24,879 iteration 5757 : loss : 0.020982, loss_ce: 0.007290
2022-01-10 04:39:26,232 iteration 5758 : loss : 0.007674, loss_ce: 0.002295
2022-01-10 04:39:27,693 iteration 5759 : loss : 0.016006, loss_ce: 0.004008
2022-01-10 04:39:29,097 iteration 5760 : loss : 0.017843, loss_ce: 0.006013
2022-01-10 04:39:30,533 iteration 5761 : loss : 0.018376, loss_ce: 0.007148
2022-01-10 04:39:32,009 iteration 5762 : loss : 0.020814, loss_ce: 0.009247
2022-01-10 04:39:33,448 iteration 5763 : loss : 0.017346, loss_ce: 0.007430
 85%|████████████████████████▌    | 339/400 [2:27:39<25:22, 24.96s/it]2022-01-10 04:39:34,981 iteration 5764 : loss : 0.015953, loss_ce: 0.006526
2022-01-10 04:39:36,411 iteration 5765 : loss : 0.050078, loss_ce: 0.019246
2022-01-10 04:39:37,789 iteration 5766 : loss : 0.015750, loss_ce: 0.006172
2022-01-10 04:39:39,243 iteration 5767 : loss : 0.018107, loss_ce: 0.007140
2022-01-10 04:39:40,693 iteration 5768 : loss : 0.025161, loss_ce: 0.007088
2022-01-10 04:39:42,128 iteration 5769 : loss : 0.013605, loss_ce: 0.003710
2022-01-10 04:39:43,568 iteration 5770 : loss : 0.020192, loss_ce: 0.007949
2022-01-10 04:39:44,903 iteration 5771 : loss : 0.010962, loss_ce: 0.004570
2022-01-10 04:39:46,320 iteration 5772 : loss : 0.012814, loss_ce: 0.004449
2022-01-10 04:39:47,657 iteration 5773 : loss : 0.011465, loss_ce: 0.003979
2022-01-10 04:39:49,026 iteration 5774 : loss : 0.012202, loss_ce: 0.004725
2022-01-10 04:39:50,447 iteration 5775 : loss : 0.023496, loss_ce: 0.009367
2022-01-10 04:39:51,755 iteration 5776 : loss : 0.013919, loss_ce: 0.005276
2022-01-10 04:39:53,141 iteration 5777 : loss : 0.012615, loss_ce: 0.004190
2022-01-10 04:39:54,567 iteration 5778 : loss : 0.014867, loss_ce: 0.005570
2022-01-10 04:39:55,977 iteration 5779 : loss : 0.011257, loss_ce: 0.004524
2022-01-10 04:39:55,977 Training Data Eval:
2022-01-10 04:40:03,021   Average segmentation loss on training set: 0.0085
2022-01-10 04:40:03,021 Validation Data Eval:
2022-01-10 04:40:05,463   Average segmentation loss on validation set: 0.0910
2022-01-10 04:40:06,851 iteration 5780 : loss : 0.016178, loss_ce: 0.006275
 85%|████████████████████████▋    | 340/400 [2:28:13<27:29, 27.50s/it]2022-01-10 04:40:08,319 iteration 5781 : loss : 0.014470, loss_ce: 0.004769
2022-01-10 04:40:09,646 iteration 5782 : loss : 0.009706, loss_ce: 0.003384
2022-01-10 04:40:11,067 iteration 5783 : loss : 0.012506, loss_ce: 0.003849
2022-01-10 04:40:12,500 iteration 5784 : loss : 0.013120, loss_ce: 0.003965
2022-01-10 04:40:13,921 iteration 5785 : loss : 0.011194, loss_ce: 0.005042
2022-01-10 04:40:15,380 iteration 5786 : loss : 0.028413, loss_ce: 0.014476
2022-01-10 04:40:16,783 iteration 5787 : loss : 0.016701, loss_ce: 0.005255
2022-01-10 04:40:18,199 iteration 5788 : loss : 0.015303, loss_ce: 0.006194
2022-01-10 04:40:19,606 iteration 5789 : loss : 0.016105, loss_ce: 0.004911
2022-01-10 04:40:21,040 iteration 5790 : loss : 0.019315, loss_ce: 0.007914
2022-01-10 04:40:22,485 iteration 5791 : loss : 0.011729, loss_ce: 0.004925
2022-01-10 04:40:23,964 iteration 5792 : loss : 0.018176, loss_ce: 0.005642
2022-01-10 04:40:25,337 iteration 5793 : loss : 0.012848, loss_ce: 0.004907
2022-01-10 04:40:26,701 iteration 5794 : loss : 0.014462, loss_ce: 0.004608
2022-01-10 04:40:28,089 iteration 5795 : loss : 0.015319, loss_ce: 0.007112
2022-01-10 04:40:29,479 iteration 5796 : loss : 0.015133, loss_ce: 0.006101
2022-01-10 04:40:30,853 iteration 5797 : loss : 0.016815, loss_ce: 0.005257
 85%|████████████████████████▋    | 341/400 [2:28:37<26:00, 26.45s/it]2022-01-10 04:40:32,272 iteration 5798 : loss : 0.015977, loss_ce: 0.004966
2022-01-10 04:40:33,640 iteration 5799 : loss : 0.018481, loss_ce: 0.006847
2022-01-10 04:40:34,945 iteration 5800 : loss : 0.008993, loss_ce: 0.003607
2022-01-10 04:40:36,344 iteration 5801 : loss : 0.013034, loss_ce: 0.005514
2022-01-10 04:40:37,693 iteration 5802 : loss : 0.014199, loss_ce: 0.006416
2022-01-10 04:40:39,125 iteration 5803 : loss : 0.013190, loss_ce: 0.004462
2022-01-10 04:40:40,482 iteration 5804 : loss : 0.015070, loss_ce: 0.004790
2022-01-10 04:40:41,834 iteration 5805 : loss : 0.013585, loss_ce: 0.004293
2022-01-10 04:40:43,171 iteration 5806 : loss : 0.011144, loss_ce: 0.005059
2022-01-10 04:40:44,593 iteration 5807 : loss : 0.017337, loss_ce: 0.005758
2022-01-10 04:40:46,040 iteration 5808 : loss : 0.020753, loss_ce: 0.007390
2022-01-10 04:40:47,483 iteration 5809 : loss : 0.015800, loss_ce: 0.005792
2022-01-10 04:40:48,968 iteration 5810 : loss : 0.019279, loss_ce: 0.006123
2022-01-10 04:40:50,419 iteration 5811 : loss : 0.019500, loss_ce: 0.007978
2022-01-10 04:40:51,802 iteration 5812 : loss : 0.014972, loss_ce: 0.006693
2022-01-10 04:40:53,165 iteration 5813 : loss : 0.012417, loss_ce: 0.004057
2022-01-10 04:40:54,543 iteration 5814 : loss : 0.021918, loss_ce: 0.007102
 86%|████████████████████████▊    | 342/400 [2:29:01<24:46, 25.62s/it]2022-01-10 04:40:56,056 iteration 5815 : loss : 0.022540, loss_ce: 0.009312
2022-01-10 04:40:57,446 iteration 5816 : loss : 0.015546, loss_ce: 0.005758
2022-01-10 04:40:58,925 iteration 5817 : loss : 0.023034, loss_ce: 0.010956
2022-01-10 04:41:00,369 iteration 5818 : loss : 0.017861, loss_ce: 0.007222
2022-01-10 04:41:01,755 iteration 5819 : loss : 0.013030, loss_ce: 0.003541
2022-01-10 04:41:03,126 iteration 5820 : loss : 0.017893, loss_ce: 0.006045
2022-01-10 04:41:04,549 iteration 5821 : loss : 0.011733, loss_ce: 0.003981
2022-01-10 04:41:06,025 iteration 5822 : loss : 0.017206, loss_ce: 0.005278
2022-01-10 04:41:07,376 iteration 5823 : loss : 0.012471, loss_ce: 0.005737
2022-01-10 04:41:08,863 iteration 5824 : loss : 0.024847, loss_ce: 0.005450
2022-01-10 04:41:10,283 iteration 5825 : loss : 0.012689, loss_ce: 0.004266
2022-01-10 04:41:11,700 iteration 5826 : loss : 0.014373, loss_ce: 0.006933
2022-01-10 04:41:13,050 iteration 5827 : loss : 0.016795, loss_ce: 0.007218
2022-01-10 04:41:14,529 iteration 5828 : loss : 0.029197, loss_ce: 0.006082
2022-01-10 04:41:15,929 iteration 5829 : loss : 0.015848, loss_ce: 0.005337
2022-01-10 04:41:17,286 iteration 5830 : loss : 0.015642, loss_ce: 0.004895
2022-01-10 04:41:18,757 iteration 5831 : loss : 0.020700, loss_ce: 0.007822
 86%|████████████████████████▊    | 343/400 [2:29:25<23:56, 25.20s/it]2022-01-10 04:41:20,262 iteration 5832 : loss : 0.019536, loss_ce: 0.007573
2022-01-10 04:41:21,734 iteration 5833 : loss : 0.023055, loss_ce: 0.007772
2022-01-10 04:41:23,138 iteration 5834 : loss : 0.016466, loss_ce: 0.008111
2022-01-10 04:41:24,533 iteration 5835 : loss : 0.015227, loss_ce: 0.006519
2022-01-10 04:41:25,921 iteration 5836 : loss : 0.019760, loss_ce: 0.006499
2022-01-10 04:41:27,395 iteration 5837 : loss : 0.022292, loss_ce: 0.007777
2022-01-10 04:41:28,806 iteration 5838 : loss : 0.016655, loss_ce: 0.004434
2022-01-10 04:41:30,218 iteration 5839 : loss : 0.016618, loss_ce: 0.005879
2022-01-10 04:41:31,625 iteration 5840 : loss : 0.012178, loss_ce: 0.004159
2022-01-10 04:41:33,079 iteration 5841 : loss : 0.018719, loss_ce: 0.007063
2022-01-10 04:41:34,436 iteration 5842 : loss : 0.015808, loss_ce: 0.005015
2022-01-10 04:41:35,788 iteration 5843 : loss : 0.014015, loss_ce: 0.005191
2022-01-10 04:41:37,194 iteration 5844 : loss : 0.017484, loss_ce: 0.007139
2022-01-10 04:41:38,549 iteration 5845 : loss : 0.011108, loss_ce: 0.003832
2022-01-10 04:41:39,905 iteration 5846 : loss : 0.011140, loss_ce: 0.004314
2022-01-10 04:41:41,273 iteration 5847 : loss : 0.013387, loss_ce: 0.005471
2022-01-10 04:41:42,643 iteration 5848 : loss : 0.013730, loss_ce: 0.004540
 86%|████████████████████████▉    | 344/400 [2:29:49<23:09, 24.81s/it]2022-01-10 04:41:44,107 iteration 5849 : loss : 0.019810, loss_ce: 0.005689
2022-01-10 04:41:45,571 iteration 5850 : loss : 0.027026, loss_ce: 0.010782
2022-01-10 04:41:47,018 iteration 5851 : loss : 0.012557, loss_ce: 0.003619
2022-01-10 04:41:48,441 iteration 5852 : loss : 0.023401, loss_ce: 0.012962
2022-01-10 04:41:49,806 iteration 5853 : loss : 0.020917, loss_ce: 0.006661
2022-01-10 04:41:51,255 iteration 5854 : loss : 0.019169, loss_ce: 0.006620
2022-01-10 04:41:52,684 iteration 5855 : loss : 0.013722, loss_ce: 0.006243
2022-01-10 04:41:54,093 iteration 5856 : loss : 0.015570, loss_ce: 0.006264
2022-01-10 04:41:55,524 iteration 5857 : loss : 0.016147, loss_ce: 0.005854
2022-01-10 04:41:56,969 iteration 5858 : loss : 0.012388, loss_ce: 0.003762
2022-01-10 04:41:58,462 iteration 5859 : loss : 0.021471, loss_ce: 0.007164
2022-01-10 04:41:59,883 iteration 5860 : loss : 0.012787, loss_ce: 0.004283
2022-01-10 04:42:01,267 iteration 5861 : loss : 0.015793, loss_ce: 0.006752
2022-01-10 04:42:02,697 iteration 5862 : loss : 0.015361, loss_ce: 0.006633
2022-01-10 04:42:04,109 iteration 5863 : loss : 0.026562, loss_ce: 0.007498
2022-01-10 04:42:05,608 iteration 5864 : loss : 0.014555, loss_ce: 0.006008
2022-01-10 04:42:05,608 Training Data Eval:
2022-01-10 04:42:12,667   Average segmentation loss on training set: 0.0081
2022-01-10 04:42:12,668 Validation Data Eval:
2022-01-10 04:42:15,105   Average segmentation loss on validation set: 0.0850
2022-01-10 04:42:16,597 iteration 5865 : loss : 0.044898, loss_ce: 0.015180
 86%|█████████████████████████    | 345/400 [2:30:23<25:15, 27.55s/it]2022-01-10 04:42:18,008 iteration 5866 : loss : 0.017072, loss_ce: 0.005087
2022-01-10 04:42:19,389 iteration 5867 : loss : 0.011268, loss_ce: 0.004598
2022-01-10 04:42:20,812 iteration 5868 : loss : 0.015744, loss_ce: 0.005738
2022-01-10 04:42:22,191 iteration 5869 : loss : 0.013785, loss_ce: 0.004175
2022-01-10 04:42:23,697 iteration 5870 : loss : 0.018122, loss_ce: 0.007035
2022-01-10 04:42:25,094 iteration 5871 : loss : 0.010923, loss_ce: 0.003831
2022-01-10 04:42:26,470 iteration 5872 : loss : 0.013756, loss_ce: 0.004193
2022-01-10 04:42:27,892 iteration 5873 : loss : 0.018645, loss_ce: 0.010967
2022-01-10 04:42:29,315 iteration 5874 : loss : 0.014016, loss_ce: 0.005101
2022-01-10 04:42:30,736 iteration 5875 : loss : 0.012329, loss_ce: 0.005889
2022-01-10 04:42:32,225 iteration 5876 : loss : 0.014355, loss_ce: 0.005733
2022-01-10 04:42:33,683 iteration 5877 : loss : 0.018923, loss_ce: 0.006843
2022-01-10 04:42:35,043 iteration 5878 : loss : 0.016310, loss_ce: 0.005656
2022-01-10 04:42:36,442 iteration 5879 : loss : 0.014839, loss_ce: 0.005012
2022-01-10 04:42:37,870 iteration 5880 : loss : 0.016608, loss_ce: 0.004643
2022-01-10 04:42:39,239 iteration 5881 : loss : 0.013479, loss_ce: 0.005288
2022-01-10 04:42:40,585 iteration 5882 : loss : 0.013825, loss_ce: 0.004699
 86%|█████████████████████████    | 346/400 [2:30:47<23:49, 26.48s/it]2022-01-10 04:42:42,006 iteration 5883 : loss : 0.012408, loss_ce: 0.003398
2022-01-10 04:42:43,329 iteration 5884 : loss : 0.010370, loss_ce: 0.003843
2022-01-10 04:42:44,792 iteration 5885 : loss : 0.020658, loss_ce: 0.005500
2022-01-10 04:42:46,184 iteration 5886 : loss : 0.017502, loss_ce: 0.005407
2022-01-10 04:42:47,630 iteration 5887 : loss : 0.019605, loss_ce: 0.010152
2022-01-10 04:42:49,033 iteration 5888 : loss : 0.009098, loss_ce: 0.003537
2022-01-10 04:42:50,506 iteration 5889 : loss : 0.012530, loss_ce: 0.004170
2022-01-10 04:42:51,899 iteration 5890 : loss : 0.014452, loss_ce: 0.006040
2022-01-10 04:42:53,300 iteration 5891 : loss : 0.022592, loss_ce: 0.007226
2022-01-10 04:42:54,731 iteration 5892 : loss : 0.017609, loss_ce: 0.004417
2022-01-10 04:42:56,114 iteration 5893 : loss : 0.015221, loss_ce: 0.006775
2022-01-10 04:42:57,464 iteration 5894 : loss : 0.016715, loss_ce: 0.006636
2022-01-10 04:42:58,813 iteration 5895 : loss : 0.015278, loss_ce: 0.006152
2022-01-10 04:43:00,168 iteration 5896 : loss : 0.016309, loss_ce: 0.006450
2022-01-10 04:43:01,572 iteration 5897 : loss : 0.014407, loss_ce: 0.006465
2022-01-10 04:43:02,936 iteration 5898 : loss : 0.029693, loss_ce: 0.013730
2022-01-10 04:43:04,375 iteration 5899 : loss : 0.019730, loss_ce: 0.008571
 87%|█████████████████████████▏   | 347/400 [2:31:10<22:40, 25.67s/it]2022-01-10 04:43:05,767 iteration 5900 : loss : 0.020080, loss_ce: 0.006659
2022-01-10 04:43:07,265 iteration 5901 : loss : 0.020900, loss_ce: 0.008347
2022-01-10 04:43:08,756 iteration 5902 : loss : 0.023974, loss_ce: 0.011762
2022-01-10 04:43:10,154 iteration 5903 : loss : 0.013997, loss_ce: 0.004352
2022-01-10 04:43:11,572 iteration 5904 : loss : 0.011491, loss_ce: 0.004865
2022-01-10 04:43:12,922 iteration 5905 : loss : 0.012690, loss_ce: 0.004488
2022-01-10 04:43:14,315 iteration 5906 : loss : 0.022656, loss_ce: 0.011038
2022-01-10 04:43:15,732 iteration 5907 : loss : 0.022424, loss_ce: 0.007045
2022-01-10 04:43:17,129 iteration 5908 : loss : 0.020169, loss_ce: 0.006543
2022-01-10 04:43:18,488 iteration 5909 : loss : 0.011280, loss_ce: 0.004456
2022-01-10 04:43:19,862 iteration 5910 : loss : 0.015466, loss_ce: 0.007038
2022-01-10 04:43:21,251 iteration 5911 : loss : 0.013245, loss_ce: 0.005332
2022-01-10 04:43:22,770 iteration 5912 : loss : 0.028586, loss_ce: 0.004934
2022-01-10 04:43:24,111 iteration 5913 : loss : 0.016327, loss_ce: 0.005405
2022-01-10 04:43:25,589 iteration 5914 : loss : 0.014826, loss_ce: 0.004935
2022-01-10 04:43:27,042 iteration 5915 : loss : 0.018538, loss_ce: 0.007375
2022-01-10 04:43:28,449 iteration 5916 : loss : 0.016667, loss_ce: 0.006988
 87%|█████████████████████████▏   | 348/400 [2:31:34<21:49, 25.19s/it]2022-01-10 04:43:29,902 iteration 5917 : loss : 0.023791, loss_ce: 0.004833
2022-01-10 04:43:31,395 iteration 5918 : loss : 0.022673, loss_ce: 0.010653
2022-01-10 04:43:32,764 iteration 5919 : loss : 0.012360, loss_ce: 0.007177
2022-01-10 04:43:34,271 iteration 5920 : loss : 0.017230, loss_ce: 0.007541
2022-01-10 04:43:35,671 iteration 5921 : loss : 0.013079, loss_ce: 0.003951
2022-01-10 04:43:37,113 iteration 5922 : loss : 0.015938, loss_ce: 0.007393
2022-01-10 04:43:38,501 iteration 5923 : loss : 0.016179, loss_ce: 0.004044
2022-01-10 04:43:39,862 iteration 5924 : loss : 0.013046, loss_ce: 0.005393
2022-01-10 04:43:41,247 iteration 5925 : loss : 0.013694, loss_ce: 0.004173
2022-01-10 04:43:42,641 iteration 5926 : loss : 0.012204, loss_ce: 0.004516
2022-01-10 04:43:44,113 iteration 5927 : loss : 0.016107, loss_ce: 0.005898
2022-01-10 04:43:45,434 iteration 5928 : loss : 0.012496, loss_ce: 0.005326
2022-01-10 04:43:46,792 iteration 5929 : loss : 0.017451, loss_ce: 0.004761
2022-01-10 04:43:48,206 iteration 5930 : loss : 0.014809, loss_ce: 0.005227
2022-01-10 04:43:49,620 iteration 5931 : loss : 0.013261, loss_ce: 0.005356
2022-01-10 04:43:51,010 iteration 5932 : loss : 0.019019, loss_ce: 0.006670
2022-01-10 04:43:52,376 iteration 5933 : loss : 0.011448, loss_ce: 0.004870
 87%|█████████████████████████▎   | 349/400 [2:31:58<21:05, 24.82s/it]2022-01-10 04:43:53,774 iteration 5934 : loss : 0.012331, loss_ce: 0.004421
2022-01-10 04:43:55,118 iteration 5935 : loss : 0.014661, loss_ce: 0.006969
2022-01-10 04:43:56,456 iteration 5936 : loss : 0.015472, loss_ce: 0.005992
2022-01-10 04:43:57,885 iteration 5937 : loss : 0.016786, loss_ce: 0.007073
2022-01-10 04:43:59,312 iteration 5938 : loss : 0.013325, loss_ce: 0.005856
2022-01-10 04:44:00,778 iteration 5939 : loss : 0.022660, loss_ce: 0.008261
2022-01-10 04:44:02,154 iteration 5940 : loss : 0.010079, loss_ce: 0.003906
2022-01-10 04:44:03,598 iteration 5941 : loss : 0.018449, loss_ce: 0.006218
2022-01-10 04:44:05,014 iteration 5942 : loss : 0.011964, loss_ce: 0.004822
2022-01-10 04:44:06,464 iteration 5943 : loss : 0.015614, loss_ce: 0.005682
2022-01-10 04:44:07,783 iteration 5944 : loss : 0.012738, loss_ce: 0.005156
2022-01-10 04:44:09,295 iteration 5945 : loss : 0.032629, loss_ce: 0.009988
2022-01-10 04:44:10,718 iteration 5946 : loss : 0.012889, loss_ce: 0.005611
2022-01-10 04:44:12,232 iteration 5947 : loss : 0.019211, loss_ce: 0.005612
2022-01-10 04:44:13,704 iteration 5948 : loss : 0.018146, loss_ce: 0.006010
2022-01-10 04:44:15,102 iteration 5949 : loss : 0.022614, loss_ce: 0.006561
2022-01-10 04:44:15,102 Training Data Eval:
2022-01-10 04:44:22,159   Average segmentation loss on training set: 0.0086
2022-01-10 04:44:22,160 Validation Data Eval:
2022-01-10 04:44:24,612   Average segmentation loss on validation set: 0.1106
2022-01-10 04:44:25,962 iteration 5950 : loss : 0.014492, loss_ce: 0.004684
 88%|█████████████████████████▍   | 350/400 [2:32:32<22:52, 27.45s/it]2022-01-10 04:44:27,359 iteration 5951 : loss : 0.018202, loss_ce: 0.005224
2022-01-10 04:44:28,778 iteration 5952 : loss : 0.018363, loss_ce: 0.008669
2022-01-10 04:44:30,283 iteration 5953 : loss : 0.017959, loss_ce: 0.007527
2022-01-10 04:44:31,663 iteration 5954 : loss : 0.014981, loss_ce: 0.003549
2022-01-10 04:44:33,173 iteration 5955 : loss : 0.025258, loss_ce: 0.008432
2022-01-10 04:44:34,559 iteration 5956 : loss : 0.017114, loss_ce: 0.004238
2022-01-10 04:44:36,004 iteration 5957 : loss : 0.026492, loss_ce: 0.011510
2022-01-10 04:44:37,457 iteration 5958 : loss : 0.017451, loss_ce: 0.007150
2022-01-10 04:44:38,894 iteration 5959 : loss : 0.016995, loss_ce: 0.004825
2022-01-10 04:44:40,278 iteration 5960 : loss : 0.018872, loss_ce: 0.003985
2022-01-10 04:44:41,729 iteration 5961 : loss : 0.025349, loss_ce: 0.012373
2022-01-10 04:44:43,172 iteration 5962 : loss : 0.017244, loss_ce: 0.006640
2022-01-10 04:44:44,545 iteration 5963 : loss : 0.014053, loss_ce: 0.005263
2022-01-10 04:44:46,005 iteration 5964 : loss : 0.020518, loss_ce: 0.008669
2022-01-10 04:44:47,434 iteration 5965 : loss : 0.013368, loss_ce: 0.006875
2022-01-10 04:44:48,820 iteration 5966 : loss : 0.010547, loss_ce: 0.003622
2022-01-10 04:44:50,301 iteration 5967 : loss : 0.014774, loss_ce: 0.004957
 88%|█████████████████████████▍   | 351/400 [2:32:56<21:39, 26.51s/it]2022-01-10 04:44:51,749 iteration 5968 : loss : 0.013178, loss_ce: 0.005372
2022-01-10 04:44:53,139 iteration 5969 : loss : 0.014524, loss_ce: 0.006785
2022-01-10 04:44:54,573 iteration 5970 : loss : 0.016683, loss_ce: 0.006968
2022-01-10 04:44:55,937 iteration 5971 : loss : 0.011602, loss_ce: 0.004860
2022-01-10 04:44:57,292 iteration 5972 : loss : 0.015211, loss_ce: 0.004142
2022-01-10 04:44:58,651 iteration 5973 : loss : 0.012283, loss_ce: 0.004595
2022-01-10 04:45:00,046 iteration 5974 : loss : 0.019103, loss_ce: 0.005897
2022-01-10 04:45:01,456 iteration 5975 : loss : 0.017354, loss_ce: 0.010403
2022-01-10 04:45:02,893 iteration 5976 : loss : 0.012289, loss_ce: 0.003624
2022-01-10 04:45:04,254 iteration 5977 : loss : 0.013467, loss_ce: 0.004445
2022-01-10 04:45:05,637 iteration 5978 : loss : 0.014705, loss_ce: 0.004765
2022-01-10 04:45:07,037 iteration 5979 : loss : 0.013647, loss_ce: 0.003678
2022-01-10 04:45:08,499 iteration 5980 : loss : 0.021198, loss_ce: 0.004567
2022-01-10 04:45:09,978 iteration 5981 : loss : 0.017331, loss_ce: 0.008830
2022-01-10 04:45:11,397 iteration 5982 : loss : 0.026152, loss_ce: 0.012612
2022-01-10 04:45:12,816 iteration 5983 : loss : 0.037050, loss_ce: 0.009293
2022-01-10 04:45:14,294 iteration 5984 : loss : 0.016603, loss_ce: 0.005405
 88%|█████████████████████████▌   | 352/400 [2:33:20<20:36, 25.76s/it]2022-01-10 04:45:15,794 iteration 5985 : loss : 0.018082, loss_ce: 0.007039
2022-01-10 04:45:17,110 iteration 5986 : loss : 0.010676, loss_ce: 0.004705
2022-01-10 04:45:18,566 iteration 5987 : loss : 0.021434, loss_ce: 0.004758
2022-01-10 04:45:19,900 iteration 5988 : loss : 0.012483, loss_ce: 0.004976
2022-01-10 04:45:21,313 iteration 5989 : loss : 0.014141, loss_ce: 0.005432
2022-01-10 04:45:22,702 iteration 5990 : loss : 0.029646, loss_ce: 0.008898
2022-01-10 04:45:24,177 iteration 5991 : loss : 0.017451, loss_ce: 0.004584
2022-01-10 04:45:25,643 iteration 5992 : loss : 0.021254, loss_ce: 0.005300
2022-01-10 04:45:27,036 iteration 5993 : loss : 0.017251, loss_ce: 0.006224
2022-01-10 04:45:28,370 iteration 5994 : loss : 0.010331, loss_ce: 0.004450
2022-01-10 04:45:29,864 iteration 5995 : loss : 0.015858, loss_ce: 0.004872
2022-01-10 04:45:31,309 iteration 5996 : loss : 0.013612, loss_ce: 0.007127
2022-01-10 04:45:32,679 iteration 5997 : loss : 0.013165, loss_ce: 0.003603
2022-01-10 04:45:34,197 iteration 5998 : loss : 0.033949, loss_ce: 0.013281
2022-01-10 04:45:35,585 iteration 5999 : loss : 0.019285, loss_ce: 0.007851
2022-01-10 04:45:37,101 iteration 6000 : loss : 0.026020, loss_ce: 0.010309
2022-01-10 04:45:38,455 iteration 6001 : loss : 0.009807, loss_ce: 0.003645
 88%|█████████████████████████▌   | 353/400 [2:33:44<19:48, 25.28s/it]2022-01-10 04:45:39,859 iteration 6002 : loss : 0.015469, loss_ce: 0.005860
2022-01-10 04:45:41,287 iteration 6003 : loss : 0.011284, loss_ce: 0.004955
2022-01-10 04:45:42,672 iteration 6004 : loss : 0.012881, loss_ce: 0.004860
2022-01-10 04:45:44,043 iteration 6005 : loss : 0.020967, loss_ce: 0.008846
2022-01-10 04:45:45,496 iteration 6006 : loss : 0.025593, loss_ce: 0.007264
2022-01-10 04:45:46,861 iteration 6007 : loss : 0.011914, loss_ce: 0.004427
2022-01-10 04:45:48,255 iteration 6008 : loss : 0.016609, loss_ce: 0.005733
2022-01-10 04:45:49,698 iteration 6009 : loss : 0.015171, loss_ce: 0.004497
2022-01-10 04:45:51,055 iteration 6010 : loss : 0.013461, loss_ce: 0.003433
2022-01-10 04:45:52,491 iteration 6011 : loss : 0.018894, loss_ce: 0.005590
2022-01-10 04:45:53,959 iteration 6012 : loss : 0.019266, loss_ce: 0.008357
2022-01-10 04:45:55,348 iteration 6013 : loss : 0.019050, loss_ce: 0.006548
2022-01-10 04:45:56,707 iteration 6014 : loss : 0.013159, loss_ce: 0.005410
2022-01-10 04:45:58,030 iteration 6015 : loss : 0.011007, loss_ce: 0.005237
2022-01-10 04:45:59,408 iteration 6016 : loss : 0.010586, loss_ce: 0.003114
2022-01-10 04:46:00,816 iteration 6017 : loss : 0.011818, loss_ce: 0.004520
2022-01-10 04:46:02,139 iteration 6018 : loss : 0.013342, loss_ce: 0.005291
 88%|█████████████████████████▋   | 354/400 [2:34:08<19:00, 24.80s/it]2022-01-10 04:46:03,646 iteration 6019 : loss : 0.014217, loss_ce: 0.004744
2022-01-10 04:46:05,064 iteration 6020 : loss : 0.016177, loss_ce: 0.005415
2022-01-10 04:46:06,426 iteration 6021 : loss : 0.013674, loss_ce: 0.005192
2022-01-10 04:46:07,861 iteration 6022 : loss : 0.013991, loss_ce: 0.005857
2022-01-10 04:46:09,302 iteration 6023 : loss : 0.017737, loss_ce: 0.007702
2022-01-10 04:46:10,714 iteration 6024 : loss : 0.012959, loss_ce: 0.005372
2022-01-10 04:46:12,070 iteration 6025 : loss : 0.020144, loss_ce: 0.003969
2022-01-10 04:46:13,446 iteration 6026 : loss : 0.017004, loss_ce: 0.006687
2022-01-10 04:46:14,896 iteration 6027 : loss : 0.013206, loss_ce: 0.005870
2022-01-10 04:46:16,262 iteration 6028 : loss : 0.010425, loss_ce: 0.004390
2022-01-10 04:46:17,676 iteration 6029 : loss : 0.017377, loss_ce: 0.007497
2022-01-10 04:46:19,134 iteration 6030 : loss : 0.016720, loss_ce: 0.006215
2022-01-10 04:46:20,580 iteration 6031 : loss : 0.022765, loss_ce: 0.007694
2022-01-10 04:46:21,991 iteration 6032 : loss : 0.025186, loss_ce: 0.007772
2022-01-10 04:46:23,392 iteration 6033 : loss : 0.019950, loss_ce: 0.006651
2022-01-10 04:46:24,749 iteration 6034 : loss : 0.010931, loss_ce: 0.004277
2022-01-10 04:46:24,750 Training Data Eval:
2022-01-10 04:46:31,809   Average segmentation loss on training set: 0.0084
2022-01-10 04:46:31,809 Validation Data Eval:
2022-01-10 04:46:34,254   Average segmentation loss on validation set: 0.0813
2022-01-10 04:46:35,610 iteration 6035 : loss : 0.015736, loss_ce: 0.006573
 89%|█████████████████████████▋   | 355/400 [2:34:42<20:33, 27.40s/it]2022-01-10 04:46:37,084 iteration 6036 : loss : 0.012013, loss_ce: 0.005432
2022-01-10 04:46:38,437 iteration 6037 : loss : 0.010054, loss_ce: 0.003707
2022-01-10 04:46:39,800 iteration 6038 : loss : 0.012599, loss_ce: 0.004527
2022-01-10 04:46:41,106 iteration 6039 : loss : 0.011868, loss_ce: 0.003742
2022-01-10 04:46:42,473 iteration 6040 : loss : 0.015791, loss_ce: 0.004391
2022-01-10 04:46:43,914 iteration 6041 : loss : 0.016679, loss_ce: 0.006525
2022-01-10 04:46:45,263 iteration 6042 : loss : 0.018104, loss_ce: 0.004767
2022-01-10 04:46:46,585 iteration 6043 : loss : 0.009914, loss_ce: 0.002475
2022-01-10 04:46:48,013 iteration 6044 : loss : 0.016411, loss_ce: 0.004374
2022-01-10 04:46:49,383 iteration 6045 : loss : 0.013372, loss_ce: 0.004939
2022-01-10 04:46:50,780 iteration 6046 : loss : 0.019824, loss_ce: 0.009454
2022-01-10 04:46:52,150 iteration 6047 : loss : 0.011674, loss_ce: 0.004450
2022-01-10 04:46:53,582 iteration 6048 : loss : 0.015575, loss_ce: 0.007984
2022-01-10 04:46:54,967 iteration 6049 : loss : 0.013702, loss_ce: 0.006428
2022-01-10 04:46:56,363 iteration 6050 : loss : 0.012444, loss_ce: 0.004221
2022-01-10 04:46:57,866 iteration 6051 : loss : 0.013928, loss_ce: 0.004674
2022-01-10 04:46:59,249 iteration 6052 : loss : 0.013754, loss_ce: 0.006069
 89%|█████████████████████████▊   | 356/400 [2:35:05<19:15, 26.27s/it]2022-01-10 04:47:00,664 iteration 6053 : loss : 0.018906, loss_ce: 0.005883
2022-01-10 04:47:01,997 iteration 6054 : loss : 0.011988, loss_ce: 0.005768
2022-01-10 04:47:03,440 iteration 6055 : loss : 0.015788, loss_ce: 0.005040
2022-01-10 04:47:04,827 iteration 6056 : loss : 0.014808, loss_ce: 0.005285
2022-01-10 04:47:06,188 iteration 6057 : loss : 0.013708, loss_ce: 0.004231
2022-01-10 04:47:07,601 iteration 6058 : loss : 0.018525, loss_ce: 0.006528
2022-01-10 04:47:09,048 iteration 6059 : loss : 0.014843, loss_ce: 0.007625
2022-01-10 04:47:10,590 iteration 6060 : loss : 0.052418, loss_ce: 0.018671
2022-01-10 04:47:11,997 iteration 6061 : loss : 0.011537, loss_ce: 0.004863
2022-01-10 04:47:13,478 iteration 6062 : loss : 0.015922, loss_ce: 0.004871
2022-01-10 04:47:14,840 iteration 6063 : loss : 0.015502, loss_ce: 0.005056
2022-01-10 04:47:16,308 iteration 6064 : loss : 0.019166, loss_ce: 0.008396
2022-01-10 04:47:17,657 iteration 6065 : loss : 0.009512, loss_ce: 0.003565
2022-01-10 04:47:19,014 iteration 6066 : loss : 0.008570, loss_ce: 0.002495
2022-01-10 04:47:20,407 iteration 6067 : loss : 0.011917, loss_ce: 0.005033
2022-01-10 04:47:21,770 iteration 6068 : loss : 0.010372, loss_ce: 0.002793
2022-01-10 04:47:23,152 iteration 6069 : loss : 0.016239, loss_ce: 0.006523
 89%|█████████████████████████▉   | 357/400 [2:35:29<18:19, 25.57s/it]2022-01-10 04:47:24,609 iteration 6070 : loss : 0.009712, loss_ce: 0.004826
2022-01-10 04:47:25,963 iteration 6071 : loss : 0.023480, loss_ce: 0.006739
2022-01-10 04:47:27,330 iteration 6072 : loss : 0.011096, loss_ce: 0.005405
2022-01-10 04:47:28,732 iteration 6073 : loss : 0.013913, loss_ce: 0.004789
2022-01-10 04:47:30,162 iteration 6074 : loss : 0.015487, loss_ce: 0.005504
2022-01-10 04:47:31,480 iteration 6075 : loss : 0.010056, loss_ce: 0.004410
2022-01-10 04:47:32,854 iteration 6076 : loss : 0.012970, loss_ce: 0.002533
2022-01-10 04:47:34,262 iteration 6077 : loss : 0.011628, loss_ce: 0.004276
2022-01-10 04:47:35,719 iteration 6078 : loss : 0.015465, loss_ce: 0.004955
2022-01-10 04:47:37,231 iteration 6079 : loss : 0.019926, loss_ce: 0.007018
2022-01-10 04:47:38,646 iteration 6080 : loss : 0.016528, loss_ce: 0.005568
2022-01-10 04:47:40,091 iteration 6081 : loss : 0.019371, loss_ce: 0.007466
2022-01-10 04:47:41,418 iteration 6082 : loss : 0.014440, loss_ce: 0.003994
2022-01-10 04:47:42,781 iteration 6083 : loss : 0.013681, loss_ce: 0.003686
2022-01-10 04:47:44,186 iteration 6084 : loss : 0.017090, loss_ce: 0.003880
2022-01-10 04:47:45,574 iteration 6085 : loss : 0.015859, loss_ce: 0.005370
2022-01-10 04:47:46,951 iteration 6086 : loss : 0.012245, loss_ce: 0.003864
 90%|█████████████████████████▉   | 358/400 [2:35:53<17:31, 25.03s/it]2022-01-10 04:47:48,351 iteration 6087 : loss : 0.013008, loss_ce: 0.004031
2022-01-10 04:47:49,786 iteration 6088 : loss : 0.012752, loss_ce: 0.005082
2022-01-10 04:47:51,173 iteration 6089 : loss : 0.013998, loss_ce: 0.004272
2022-01-10 04:47:52,473 iteration 6090 : loss : 0.010660, loss_ce: 0.004357
2022-01-10 04:47:53,891 iteration 6091 : loss : 0.016841, loss_ce: 0.005997
2022-01-10 04:47:55,307 iteration 6092 : loss : 0.011910, loss_ce: 0.003943
2022-01-10 04:47:56,703 iteration 6093 : loss : 0.017493, loss_ce: 0.008475
2022-01-10 04:47:58,056 iteration 6094 : loss : 0.012592, loss_ce: 0.004303
2022-01-10 04:47:59,421 iteration 6095 : loss : 0.016803, loss_ce: 0.004943
2022-01-10 04:48:00,838 iteration 6096 : loss : 0.012890, loss_ce: 0.003574
2022-01-10 04:48:02,258 iteration 6097 : loss : 0.008893, loss_ce: 0.003147
2022-01-10 04:48:03,744 iteration 6098 : loss : 0.019151, loss_ce: 0.008161
2022-01-10 04:48:05,192 iteration 6099 : loss : 0.014924, loss_ce: 0.006695
2022-01-10 04:48:06,627 iteration 6100 : loss : 0.013969, loss_ce: 0.006970
2022-01-10 04:48:08,041 iteration 6101 : loss : 0.015332, loss_ce: 0.005931
2022-01-10 04:48:09,485 iteration 6102 : loss : 0.019701, loss_ce: 0.009872
2022-01-10 04:48:10,937 iteration 6103 : loss : 0.022193, loss_ce: 0.008298
 90%|██████████████████████████   | 359/400 [2:36:17<16:53, 24.72s/it]2022-01-10 04:48:12,459 iteration 6104 : loss : 0.020795, loss_ce: 0.008942
2022-01-10 04:48:13,919 iteration 6105 : loss : 0.018035, loss_ce: 0.008991
2022-01-10 04:48:15,348 iteration 6106 : loss : 0.024894, loss_ce: 0.011341
2022-01-10 04:48:16,756 iteration 6107 : loss : 0.020003, loss_ce: 0.009958
2022-01-10 04:48:18,137 iteration 6108 : loss : 0.013346, loss_ce: 0.005709
2022-01-10 04:48:19,524 iteration 6109 : loss : 0.014413, loss_ce: 0.003361
2022-01-10 04:48:20,898 iteration 6110 : loss : 0.012209, loss_ce: 0.004719
2022-01-10 04:48:22,285 iteration 6111 : loss : 0.013143, loss_ce: 0.004965
2022-01-10 04:48:23,680 iteration 6112 : loss : 0.018618, loss_ce: 0.005323
2022-01-10 04:48:25,078 iteration 6113 : loss : 0.013588, loss_ce: 0.005927
2022-01-10 04:48:26,519 iteration 6114 : loss : 0.025145, loss_ce: 0.007895
2022-01-10 04:48:27,914 iteration 6115 : loss : 0.017871, loss_ce: 0.006963
2022-01-10 04:48:29,333 iteration 6116 : loss : 0.023302, loss_ce: 0.007682
2022-01-10 04:48:30,699 iteration 6117 : loss : 0.014732, loss_ce: 0.006268
2022-01-10 04:48:32,048 iteration 6118 : loss : 0.010341, loss_ce: 0.002985
2022-01-10 04:48:33,537 iteration 6119 : loss : 0.021127, loss_ce: 0.009367
2022-01-10 04:48:33,537 Training Data Eval:
2022-01-10 04:48:40,591   Average segmentation loss on training set: 0.0080
2022-01-10 04:48:40,591 Validation Data Eval:
2022-01-10 04:48:43,023   Average segmentation loss on validation set: 0.0756
2022-01-10 04:48:44,413 iteration 6120 : loss : 0.015751, loss_ce: 0.005135
 90%|██████████████████████████   | 360/400 [2:36:50<18:13, 27.35s/it]2022-01-10 04:48:45,853 iteration 6121 : loss : 0.017267, loss_ce: 0.004945
2022-01-10 04:48:47,249 iteration 6122 : loss : 0.013249, loss_ce: 0.005144
2022-01-10 04:48:48,649 iteration 6123 : loss : 0.020392, loss_ce: 0.008360
2022-01-10 04:48:49,997 iteration 6124 : loss : 0.010441, loss_ce: 0.003783
2022-01-10 04:48:51,399 iteration 6125 : loss : 0.020770, loss_ce: 0.008406
2022-01-10 04:48:52,760 iteration 6126 : loss : 0.013201, loss_ce: 0.007392
2022-01-10 04:48:54,194 iteration 6127 : loss : 0.011903, loss_ce: 0.003750
2022-01-10 04:48:55,575 iteration 6128 : loss : 0.013517, loss_ce: 0.004640
2022-01-10 04:48:57,062 iteration 6129 : loss : 0.014816, loss_ce: 0.005899
2022-01-10 04:48:58,435 iteration 6130 : loss : 0.014484, loss_ce: 0.003463
2022-01-10 04:48:59,918 iteration 6131 : loss : 0.014325, loss_ce: 0.006692
2022-01-10 04:49:01,303 iteration 6132 : loss : 0.020225, loss_ce: 0.009980
2022-01-10 04:49:02,735 iteration 6133 : loss : 0.015315, loss_ce: 0.004184
2022-01-10 04:49:04,149 iteration 6134 : loss : 0.023323, loss_ce: 0.008337
2022-01-10 04:49:05,648 iteration 6135 : loss : 0.031027, loss_ce: 0.012383
2022-01-10 04:49:07,048 iteration 6136 : loss : 0.029220, loss_ce: 0.007760
2022-01-10 04:49:08,415 iteration 6137 : loss : 0.015762, loss_ce: 0.006116
 90%|██████████████████████████▏  | 361/400 [2:37:14<17:07, 26.34s/it]2022-01-10 04:49:09,841 iteration 6138 : loss : 0.017851, loss_ce: 0.006136
2022-01-10 04:49:11,235 iteration 6139 : loss : 0.014757, loss_ce: 0.005941
2022-01-10 04:49:12,610 iteration 6140 : loss : 0.016834, loss_ce: 0.007895
2022-01-10 04:49:13,938 iteration 6141 : loss : 0.010703, loss_ce: 0.004138
2022-01-10 04:49:15,254 iteration 6142 : loss : 0.010375, loss_ce: 0.004214
2022-01-10 04:49:16,699 iteration 6143 : loss : 0.014287, loss_ce: 0.003451
2022-01-10 04:49:18,084 iteration 6144 : loss : 0.015771, loss_ce: 0.006037
2022-01-10 04:49:19,458 iteration 6145 : loss : 0.012933, loss_ce: 0.005700
2022-01-10 04:49:20,816 iteration 6146 : loss : 0.022924, loss_ce: 0.005808
2022-01-10 04:49:22,202 iteration 6147 : loss : 0.014492, loss_ce: 0.005901
2022-01-10 04:49:23,642 iteration 6148 : loss : 0.021743, loss_ce: 0.008946
2022-01-10 04:49:25,076 iteration 6149 : loss : 0.015458, loss_ce: 0.005592
2022-01-10 04:49:26,549 iteration 6150 : loss : 0.018516, loss_ce: 0.005912
2022-01-10 04:49:28,020 iteration 6151 : loss : 0.016589, loss_ce: 0.004395
2022-01-10 04:49:29,380 iteration 6152 : loss : 0.013024, loss_ce: 0.005311
2022-01-10 04:49:30,794 iteration 6153 : loss : 0.012664, loss_ce: 0.003786
2022-01-10 04:49:32,226 iteration 6154 : loss : 0.019669, loss_ce: 0.008046
 90%|██████████████████████████▏  | 362/400 [2:37:38<16:12, 25.58s/it]2022-01-10 04:49:33,730 iteration 6155 : loss : 0.017634, loss_ce: 0.006307
2022-01-10 04:49:35,164 iteration 6156 : loss : 0.017708, loss_ce: 0.006870
2022-01-10 04:49:36,586 iteration 6157 : loss : 0.016132, loss_ce: 0.006175
2022-01-10 04:49:38,060 iteration 6158 : loss : 0.022676, loss_ce: 0.005490
2022-01-10 04:49:39,435 iteration 6159 : loss : 0.010463, loss_ce: 0.003667
2022-01-10 04:49:40,818 iteration 6160 : loss : 0.013271, loss_ce: 0.004646
2022-01-10 04:49:42,254 iteration 6161 : loss : 0.016781, loss_ce: 0.005073
2022-01-10 04:49:43,726 iteration 6162 : loss : 0.026418, loss_ce: 0.007131
2022-01-10 04:49:45,138 iteration 6163 : loss : 0.017034, loss_ce: 0.007009
2022-01-10 04:49:46,469 iteration 6164 : loss : 0.011262, loss_ce: 0.003113
2022-01-10 04:49:47,924 iteration 6165 : loss : 0.020283, loss_ce: 0.006893
2022-01-10 04:49:49,297 iteration 6166 : loss : 0.016855, loss_ce: 0.005195
2022-01-10 04:49:50,762 iteration 6167 : loss : 0.017087, loss_ce: 0.007930
2022-01-10 04:49:52,178 iteration 6168 : loss : 0.022335, loss_ce: 0.008629
2022-01-10 04:49:53,570 iteration 6169 : loss : 0.016543, loss_ce: 0.004467
2022-01-10 04:49:54,929 iteration 6170 : loss : 0.011806, loss_ce: 0.005693
2022-01-10 04:49:56,301 iteration 6171 : loss : 0.016001, loss_ce: 0.007556
 91%|██████████████████████████▎  | 363/400 [2:38:02<15:29, 25.13s/it]2022-01-10 04:49:57,740 iteration 6172 : loss : 0.018073, loss_ce: 0.006801
2022-01-10 04:49:59,218 iteration 6173 : loss : 0.022697, loss_ce: 0.005597
2022-01-10 04:50:00,643 iteration 6174 : loss : 0.016156, loss_ce: 0.007566
2022-01-10 04:50:02,149 iteration 6175 : loss : 0.017895, loss_ce: 0.005070
2022-01-10 04:50:03,577 iteration 6176 : loss : 0.013602, loss_ce: 0.004870
2022-01-10 04:50:04,979 iteration 6177 : loss : 0.012736, loss_ce: 0.005825
2022-01-10 04:50:06,392 iteration 6178 : loss : 0.015308, loss_ce: 0.005259
2022-01-10 04:50:07,756 iteration 6179 : loss : 0.016522, loss_ce: 0.005424
2022-01-10 04:50:09,207 iteration 6180 : loss : 0.018327, loss_ce: 0.007967
2022-01-10 04:50:10,577 iteration 6181 : loss : 0.010256, loss_ce: 0.003155
2022-01-10 04:50:11,953 iteration 6182 : loss : 0.012883, loss_ce: 0.003570
2022-01-10 04:50:13,408 iteration 6183 : loss : 0.016016, loss_ce: 0.006267
2022-01-10 04:50:14,830 iteration 6184 : loss : 0.013077, loss_ce: 0.005355
2022-01-10 04:50:16,292 iteration 6185 : loss : 0.017442, loss_ce: 0.004850
2022-01-10 04:50:17,688 iteration 6186 : loss : 0.011404, loss_ce: 0.004155
2022-01-10 04:50:19,106 iteration 6187 : loss : 0.022310, loss_ce: 0.012439
2022-01-10 04:50:20,596 iteration 6188 : loss : 0.017435, loss_ce: 0.008086
 91%|██████████████████████████▍  | 364/400 [2:38:27<14:55, 24.88s/it]2022-01-10 04:50:22,122 iteration 6189 : loss : 0.014288, loss_ce: 0.003141
2022-01-10 04:50:23,594 iteration 6190 : loss : 0.014391, loss_ce: 0.004459
2022-01-10 04:50:24,974 iteration 6191 : loss : 0.016897, loss_ce: 0.010299
2022-01-10 04:50:26,324 iteration 6192 : loss : 0.010293, loss_ce: 0.004131
2022-01-10 04:50:27,718 iteration 6193 : loss : 0.015958, loss_ce: 0.005733
2022-01-10 04:50:29,068 iteration 6194 : loss : 0.014800, loss_ce: 0.005976
2022-01-10 04:50:30,548 iteration 6195 : loss : 0.027384, loss_ce: 0.008863
2022-01-10 04:50:31,941 iteration 6196 : loss : 0.016626, loss_ce: 0.005852
2022-01-10 04:50:33,296 iteration 6197 : loss : 0.012390, loss_ce: 0.005844
2022-01-10 04:50:34,817 iteration 6198 : loss : 0.022825, loss_ce: 0.007515
2022-01-10 04:50:36,214 iteration 6199 : loss : 0.014754, loss_ce: 0.005926
2022-01-10 04:50:37,596 iteration 6200 : loss : 0.011487, loss_ce: 0.004871
2022-01-10 04:50:39,014 iteration 6201 : loss : 0.015447, loss_ce: 0.007093
2022-01-10 04:50:40,359 iteration 6202 : loss : 0.013135, loss_ce: 0.004446
2022-01-10 04:50:41,910 iteration 6203 : loss : 0.034345, loss_ce: 0.014722
2022-01-10 04:50:43,260 iteration 6204 : loss : 0.012971, loss_ce: 0.005698
2022-01-10 04:50:43,260 Training Data Eval:
2022-01-10 04:50:50,327   Average segmentation loss on training set: 0.0083
2022-01-10 04:50:50,327 Validation Data Eval:
2022-01-10 04:50:52,769   Average segmentation loss on validation set: 0.0705
2022-01-10 04:50:54,224 iteration 6205 : loss : 0.029378, loss_ce: 0.009101
 91%|██████████████████████████▍  | 365/400 [2:39:00<16:02, 27.51s/it]2022-01-10 04:50:55,708 iteration 6206 : loss : 0.019105, loss_ce: 0.008909
2022-01-10 04:50:57,139 iteration 6207 : loss : 0.013086, loss_ce: 0.005233
2022-01-10 04:50:58,591 iteration 6208 : loss : 0.028848, loss_ce: 0.012749
2022-01-10 04:51:00,002 iteration 6209 : loss : 0.012281, loss_ce: 0.004550
2022-01-10 04:51:01,422 iteration 6210 : loss : 0.010760, loss_ce: 0.004438
2022-01-10 04:51:02,815 iteration 6211 : loss : 0.016113, loss_ce: 0.008174
2022-01-10 04:51:04,191 iteration 6212 : loss : 0.010843, loss_ce: 0.004234
2022-01-10 04:51:05,508 iteration 6213 : loss : 0.011468, loss_ce: 0.003895
2022-01-10 04:51:06,986 iteration 6214 : loss : 0.016034, loss_ce: 0.005746
2022-01-10 04:51:08,386 iteration 6215 : loss : 0.013516, loss_ce: 0.003834
2022-01-10 04:51:09,802 iteration 6216 : loss : 0.019890, loss_ce: 0.006953
2022-01-10 04:51:11,240 iteration 6217 : loss : 0.027249, loss_ce: 0.010323
2022-01-10 04:51:12,600 iteration 6218 : loss : 0.014920, loss_ce: 0.005775
2022-01-10 04:51:14,019 iteration 6219 : loss : 0.018984, loss_ce: 0.006596
2022-01-10 04:51:15,343 iteration 6220 : loss : 0.011505, loss_ce: 0.002981
2022-01-10 04:51:16,823 iteration 6221 : loss : 0.015943, loss_ce: 0.004885
2022-01-10 04:51:18,238 iteration 6222 : loss : 0.014975, loss_ce: 0.008438
 92%|██████████████████████████▌  | 366/400 [2:39:24<14:59, 26.46s/it]2022-01-10 04:51:19,722 iteration 6223 : loss : 0.014252, loss_ce: 0.005582
2022-01-10 04:51:21,169 iteration 6224 : loss : 0.015064, loss_ce: 0.007906
2022-01-10 04:51:22,599 iteration 6225 : loss : 0.011339, loss_ce: 0.005216
2022-01-10 04:51:24,006 iteration 6226 : loss : 0.011743, loss_ce: 0.004287
2022-01-10 04:51:25,494 iteration 6227 : loss : 0.010944, loss_ce: 0.003674
2022-01-10 04:51:26,891 iteration 6228 : loss : 0.016070, loss_ce: 0.006066
2022-01-10 04:51:28,328 iteration 6229 : loss : 0.015375, loss_ce: 0.005175
2022-01-10 04:51:29,783 iteration 6230 : loss : 0.023796, loss_ce: 0.004982
2022-01-10 04:51:31,140 iteration 6231 : loss : 0.014045, loss_ce: 0.006645
2022-01-10 04:51:32,521 iteration 6232 : loss : 0.021471, loss_ce: 0.007492
2022-01-10 04:51:34,003 iteration 6233 : loss : 0.027682, loss_ce: 0.010519
2022-01-10 04:51:35,398 iteration 6234 : loss : 0.016351, loss_ce: 0.006782
2022-01-10 04:51:36,803 iteration 6235 : loss : 0.015057, loss_ce: 0.004427
2022-01-10 04:51:38,252 iteration 6236 : loss : 0.019602, loss_ce: 0.006870
2022-01-10 04:51:39,683 iteration 6237 : loss : 0.019767, loss_ce: 0.007034
2022-01-10 04:51:41,145 iteration 6238 : loss : 0.013070, loss_ce: 0.005041
2022-01-10 04:51:42,624 iteration 6239 : loss : 0.026120, loss_ce: 0.011559
 92%|██████████████████████████▌  | 367/400 [2:39:49<14:12, 25.84s/it]2022-01-10 04:51:44,048 iteration 6240 : loss : 0.015995, loss_ce: 0.005527
2022-01-10 04:51:45,404 iteration 6241 : loss : 0.011949, loss_ce: 0.004876
2022-01-10 04:51:46,835 iteration 6242 : loss : 0.013727, loss_ce: 0.006006
2022-01-10 04:51:48,238 iteration 6243 : loss : 0.014098, loss_ce: 0.005866
2022-01-10 04:51:49,652 iteration 6244 : loss : 0.017181, loss_ce: 0.006285
2022-01-10 04:51:51,083 iteration 6245 : loss : 0.013318, loss_ce: 0.006281
2022-01-10 04:51:52,468 iteration 6246 : loss : 0.011539, loss_ce: 0.005054
2022-01-10 04:51:53,834 iteration 6247 : loss : 0.012366, loss_ce: 0.004231
2022-01-10 04:51:55,271 iteration 6248 : loss : 0.040692, loss_ce: 0.007623
2022-01-10 04:51:56,716 iteration 6249 : loss : 0.014119, loss_ce: 0.004622
2022-01-10 04:51:58,026 iteration 6250 : loss : 0.019053, loss_ce: 0.004360
2022-01-10 04:51:59,473 iteration 6251 : loss : 0.024427, loss_ce: 0.013249
2022-01-10 04:52:00,812 iteration 6252 : loss : 0.016962, loss_ce: 0.006424
2022-01-10 04:52:02,306 iteration 6253 : loss : 0.015456, loss_ce: 0.006183
2022-01-10 04:52:03,633 iteration 6254 : loss : 0.008132, loss_ce: 0.003429
2022-01-10 04:52:05,185 iteration 6255 : loss : 0.025375, loss_ce: 0.008437
2022-01-10 04:52:06,564 iteration 6256 : loss : 0.014473, loss_ce: 0.005602
 92%|██████████████████████████▋  | 368/400 [2:40:13<13:28, 25.27s/it]2022-01-10 04:52:08,071 iteration 6257 : loss : 0.022088, loss_ce: 0.009233
2022-01-10 04:52:09,458 iteration 6258 : loss : 0.009904, loss_ce: 0.004734
2022-01-10 04:52:10,889 iteration 6259 : loss : 0.015635, loss_ce: 0.007281
2022-01-10 04:52:12,290 iteration 6260 : loss : 0.017542, loss_ce: 0.004886
2022-01-10 04:52:13,657 iteration 6261 : loss : 0.013499, loss_ce: 0.005520
2022-01-10 04:52:15,094 iteration 6262 : loss : 0.018281, loss_ce: 0.007694
2022-01-10 04:52:16,546 iteration 6263 : loss : 0.016023, loss_ce: 0.003925
2022-01-10 04:52:17,938 iteration 6264 : loss : 0.013609, loss_ce: 0.006643
2022-01-10 04:52:19,303 iteration 6265 : loss : 0.013226, loss_ce: 0.005496
2022-01-10 04:52:20,710 iteration 6266 : loss : 0.013665, loss_ce: 0.005538
2022-01-10 04:52:22,118 iteration 6267 : loss : 0.017093, loss_ce: 0.005768
2022-01-10 04:52:23,518 iteration 6268 : loss : 0.022898, loss_ce: 0.008323
2022-01-10 04:52:24,896 iteration 6269 : loss : 0.022239, loss_ce: 0.006575
2022-01-10 04:52:26,323 iteration 6270 : loss : 0.016961, loss_ce: 0.005135
2022-01-10 04:52:27,725 iteration 6271 : loss : 0.016346, loss_ce: 0.003785
2022-01-10 04:52:29,114 iteration 6272 : loss : 0.010919, loss_ce: 0.002837
2022-01-10 04:52:30,442 iteration 6273 : loss : 0.009391, loss_ce: 0.003777
 92%|██████████████████████████▊  | 369/400 [2:40:36<12:50, 24.85s/it]2022-01-10 04:52:31,881 iteration 6274 : loss : 0.022527, loss_ce: 0.007142
2022-01-10 04:52:33,282 iteration 6275 : loss : 0.012350, loss_ce: 0.005127
2022-01-10 04:52:34,668 iteration 6276 : loss : 0.021397, loss_ce: 0.009886
2022-01-10 04:52:36,119 iteration 6277 : loss : 0.022058, loss_ce: 0.006669
2022-01-10 04:52:37,592 iteration 6278 : loss : 0.023403, loss_ce: 0.011533
2022-01-10 04:52:39,112 iteration 6279 : loss : 0.022529, loss_ce: 0.006294
2022-01-10 04:52:40,533 iteration 6280 : loss : 0.015214, loss_ce: 0.007047
2022-01-10 04:52:41,925 iteration 6281 : loss : 0.013589, loss_ce: 0.004224
2022-01-10 04:52:43,301 iteration 6282 : loss : 0.016291, loss_ce: 0.005660
2022-01-10 04:52:44,758 iteration 6283 : loss : 0.025324, loss_ce: 0.006785
2022-01-10 04:52:46,237 iteration 6284 : loss : 0.020580, loss_ce: 0.010902
2022-01-10 04:52:47,648 iteration 6285 : loss : 0.016755, loss_ce: 0.005311
2022-01-10 04:52:49,053 iteration 6286 : loss : 0.019456, loss_ce: 0.009685
2022-01-10 04:52:50,505 iteration 6287 : loss : 0.012143, loss_ce: 0.004614
2022-01-10 04:52:51,982 iteration 6288 : loss : 0.021802, loss_ce: 0.006628
2022-01-10 04:52:53,365 iteration 6289 : loss : 0.013239, loss_ce: 0.004316
2022-01-10 04:52:53,365 Training Data Eval:
2022-01-10 04:53:00,422   Average segmentation loss on training set: 0.0084
2022-01-10 04:53:00,422 Validation Data Eval:
2022-01-10 04:53:02,874   Average segmentation loss on validation set: 0.0831
2022-01-10 04:53:04,244 iteration 6290 : loss : 0.012456, loss_ce: 0.004592
 92%|██████████████████████████▊  | 370/400 [2:41:10<13:46, 27.54s/it]2022-01-10 04:53:05,696 iteration 6291 : loss : 0.011592, loss_ce: 0.002596
2022-01-10 04:53:07,191 iteration 6292 : loss : 0.013828, loss_ce: 0.004805
2022-01-10 04:53:08,582 iteration 6293 : loss : 0.013675, loss_ce: 0.006136
2022-01-10 04:53:10,011 iteration 6294 : loss : 0.014428, loss_ce: 0.005852
2022-01-10 04:53:11,443 iteration 6295 : loss : 0.018282, loss_ce: 0.006828
2022-01-10 04:53:12,828 iteration 6296 : loss : 0.015645, loss_ce: 0.005255
2022-01-10 04:53:14,230 iteration 6297 : loss : 0.016435, loss_ce: 0.004193
2022-01-10 04:53:15,610 iteration 6298 : loss : 0.014070, loss_ce: 0.004473
2022-01-10 04:53:17,058 iteration 6299 : loss : 0.014513, loss_ce: 0.005722
2022-01-10 04:53:18,464 iteration 6300 : loss : 0.015818, loss_ce: 0.005729
2022-01-10 04:53:19,839 iteration 6301 : loss : 0.019031, loss_ce: 0.007886
2022-01-10 04:53:21,203 iteration 6302 : loss : 0.013773, loss_ce: 0.005693
2022-01-10 04:53:22,622 iteration 6303 : loss : 0.016688, loss_ce: 0.006833
2022-01-10 04:53:24,003 iteration 6304 : loss : 0.013596, loss_ce: 0.005027
2022-01-10 04:53:25,344 iteration 6305 : loss : 0.010811, loss_ce: 0.004597
2022-01-10 04:53:26,689 iteration 6306 : loss : 0.011659, loss_ce: 0.004080
2022-01-10 04:53:28,160 iteration 6307 : loss : 0.018278, loss_ce: 0.007372
 93%|██████████████████████████▉  | 371/400 [2:41:34<12:47, 26.45s/it]2022-01-10 04:53:29,592 iteration 6308 : loss : 0.010921, loss_ce: 0.004378
2022-01-10 04:53:31,015 iteration 6309 : loss : 0.032007, loss_ce: 0.014529
2022-01-10 04:53:32,439 iteration 6310 : loss : 0.017329, loss_ce: 0.008056
2022-01-10 04:53:33,844 iteration 6311 : loss : 0.013505, loss_ce: 0.003544
2022-01-10 04:53:35,281 iteration 6312 : loss : 0.018157, loss_ce: 0.006171
2022-01-10 04:53:36,663 iteration 6313 : loss : 0.010509, loss_ce: 0.002612
2022-01-10 04:53:38,108 iteration 6314 : loss : 0.017890, loss_ce: 0.007513
2022-01-10 04:53:39,573 iteration 6315 : loss : 0.019700, loss_ce: 0.007398
2022-01-10 04:53:40,972 iteration 6316 : loss : 0.014553, loss_ce: 0.007186
2022-01-10 04:53:42,406 iteration 6317 : loss : 0.010984, loss_ce: 0.004262
2022-01-10 04:53:43,881 iteration 6318 : loss : 0.017089, loss_ce: 0.007723
2022-01-10 04:53:45,386 iteration 6319 : loss : 0.015319, loss_ce: 0.005309
2022-01-10 04:53:46,894 iteration 6320 : loss : 0.017004, loss_ce: 0.007205
2022-01-10 04:53:48,353 iteration 6321 : loss : 0.015973, loss_ce: 0.006878
2022-01-10 04:53:49,838 iteration 6322 : loss : 0.015930, loss_ce: 0.006148
2022-01-10 04:53:51,128 iteration 6323 : loss : 0.010342, loss_ce: 0.004679
2022-01-10 04:53:52,543 iteration 6324 : loss : 0.016288, loss_ce: 0.004975
 93%|██████████████████████████▉  | 372/400 [2:41:59<12:03, 25.83s/it]2022-01-10 04:53:53,955 iteration 6325 : loss : 0.018506, loss_ce: 0.006024
2022-01-10 04:53:55,289 iteration 6326 : loss : 0.009384, loss_ce: 0.004229
2022-01-10 04:53:56,613 iteration 6327 : loss : 0.009772, loss_ce: 0.003639
2022-01-10 04:53:58,081 iteration 6328 : loss : 0.015385, loss_ce: 0.006627
2022-01-10 04:53:59,587 iteration 6329 : loss : 0.015836, loss_ce: 0.006112
2022-01-10 04:54:01,019 iteration 6330 : loss : 0.011720, loss_ce: 0.004672
2022-01-10 04:54:02,482 iteration 6331 : loss : 0.020470, loss_ce: 0.009961
2022-01-10 04:54:03,869 iteration 6332 : loss : 0.012189, loss_ce: 0.004421
2022-01-10 04:54:05,262 iteration 6333 : loss : 0.016532, loss_ce: 0.006628
2022-01-10 04:54:06,691 iteration 6334 : loss : 0.011184, loss_ce: 0.004642
2022-01-10 04:54:08,240 iteration 6335 : loss : 0.018741, loss_ce: 0.008228
2022-01-10 04:54:09,679 iteration 6336 : loss : 0.014227, loss_ce: 0.005354
2022-01-10 04:54:11,014 iteration 6337 : loss : 0.012872, loss_ce: 0.004844
2022-01-10 04:54:12,490 iteration 6338 : loss : 0.014142, loss_ce: 0.004528
2022-01-10 04:54:13,868 iteration 6339 : loss : 0.013853, loss_ce: 0.004356
2022-01-10 04:54:15,283 iteration 6340 : loss : 0.016202, loss_ce: 0.006939
2022-01-10 04:54:16,721 iteration 6341 : loss : 0.022055, loss_ce: 0.005669
 93%|███████████████████████████  | 373/400 [2:42:23<11:24, 25.33s/it]2022-01-10 04:54:18,294 iteration 6342 : loss : 0.022434, loss_ce: 0.009346
2022-01-10 04:54:19,699 iteration 6343 : loss : 0.013387, loss_ce: 0.004160
2022-01-10 04:54:21,117 iteration 6344 : loss : 0.016286, loss_ce: 0.007260
2022-01-10 04:54:22,537 iteration 6345 : loss : 0.019636, loss_ce: 0.009034
2022-01-10 04:54:23,909 iteration 6346 : loss : 0.012579, loss_ce: 0.003290
2022-01-10 04:54:25,314 iteration 6347 : loss : 0.011553, loss_ce: 0.003951
2022-01-10 04:54:26,817 iteration 6348 : loss : 0.019824, loss_ce: 0.007009
2022-01-10 04:54:28,177 iteration 6349 : loss : 0.010724, loss_ce: 0.004484
2022-01-10 04:54:29,559 iteration 6350 : loss : 0.013319, loss_ce: 0.004142
2022-01-10 04:54:30,935 iteration 6351 : loss : 0.011689, loss_ce: 0.004236
2022-01-10 04:54:32,389 iteration 6352 : loss : 0.016034, loss_ce: 0.006709
2022-01-10 04:54:33,850 iteration 6353 : loss : 0.014338, loss_ce: 0.006252
2022-01-10 04:54:35,356 iteration 6354 : loss : 0.022060, loss_ce: 0.008719
2022-01-10 04:54:36,732 iteration 6355 : loss : 0.012853, loss_ce: 0.006143
2022-01-10 04:54:38,164 iteration 6356 : loss : 0.016072, loss_ce: 0.007108
2022-01-10 04:54:39,553 iteration 6357 : loss : 0.016182, loss_ce: 0.005723
2022-01-10 04:54:41,029 iteration 6358 : loss : 0.019206, loss_ce: 0.006360
 94%|███████████████████████████  | 374/400 [2:42:47<10:50, 25.03s/it]2022-01-10 04:54:42,500 iteration 6359 : loss : 0.013493, loss_ce: 0.004270
2022-01-10 04:54:43,871 iteration 6360 : loss : 0.023976, loss_ce: 0.006141
2022-01-10 04:54:45,294 iteration 6361 : loss : 0.016740, loss_ce: 0.003982
2022-01-10 04:54:46,736 iteration 6362 : loss : 0.012849, loss_ce: 0.006297
2022-01-10 04:54:48,225 iteration 6363 : loss : 0.019261, loss_ce: 0.008333
2022-01-10 04:54:49,584 iteration 6364 : loss : 0.008404, loss_ce: 0.003085
2022-01-10 04:54:50,931 iteration 6365 : loss : 0.013433, loss_ce: 0.005052
2022-01-10 04:54:52,378 iteration 6366 : loss : 0.017248, loss_ce: 0.006508
2022-01-10 04:54:53,770 iteration 6367 : loss : 0.012316, loss_ce: 0.005501
2022-01-10 04:54:55,149 iteration 6368 : loss : 0.014681, loss_ce: 0.005333
2022-01-10 04:54:56,577 iteration 6369 : loss : 0.021012, loss_ce: 0.008856
2022-01-10 04:54:58,021 iteration 6370 : loss : 0.022357, loss_ce: 0.006068
2022-01-10 04:54:59,470 iteration 6371 : loss : 0.018745, loss_ce: 0.006594
2022-01-10 04:55:00,889 iteration 6372 : loss : 0.027556, loss_ce: 0.012169
2022-01-10 04:55:02,180 iteration 6373 : loss : 0.008801, loss_ce: 0.002231
2022-01-10 04:55:03,547 iteration 6374 : loss : 0.010155, loss_ce: 0.004090
2022-01-10 04:55:03,547 Training Data Eval:
2022-01-10 04:55:10,606   Average segmentation loss on training set: 0.0077
2022-01-10 04:55:10,606 Validation Data Eval:
2022-01-10 04:55:13,085   Average segmentation loss on validation set: 0.0771
2022-01-10 04:55:14,535 iteration 6375 : loss : 0.013116, loss_ce: 0.004638
 94%|███████████████████████████▏ | 375/400 [2:43:21<11:29, 27.57s/it]2022-01-10 04:55:16,217 iteration 6376 : loss : 0.021175, loss_ce: 0.007827
2022-01-10 04:55:17,638 iteration 6377 : loss : 0.015108, loss_ce: 0.005679
2022-01-10 04:55:19,021 iteration 6378 : loss : 0.016367, loss_ce: 0.005782
2022-01-10 04:55:20,457 iteration 6379 : loss : 0.016163, loss_ce: 0.004613
2022-01-10 04:55:21,788 iteration 6380 : loss : 0.009629, loss_ce: 0.004399
2022-01-10 04:55:23,123 iteration 6381 : loss : 0.013820, loss_ce: 0.004577
2022-01-10 04:55:24,549 iteration 6382 : loss : 0.018182, loss_ce: 0.008103
2022-01-10 04:55:25,988 iteration 6383 : loss : 0.014783, loss_ce: 0.008001
2022-01-10 04:55:27,375 iteration 6384 : loss : 0.010643, loss_ce: 0.003944
2022-01-10 04:55:28,861 iteration 6385 : loss : 0.022718, loss_ce: 0.010073
2022-01-10 04:55:30,283 iteration 6386 : loss : 0.015482, loss_ce: 0.005957
2022-01-10 04:55:31,822 iteration 6387 : loss : 0.032026, loss_ce: 0.016778
2022-01-10 04:55:33,217 iteration 6388 : loss : 0.010391, loss_ce: 0.003827
2022-01-10 04:55:34,538 iteration 6389 : loss : 0.025959, loss_ce: 0.007742
2022-01-10 04:55:35,933 iteration 6390 : loss : 0.015648, loss_ce: 0.006082
2022-01-10 04:55:37,441 iteration 6391 : loss : 0.053156, loss_ce: 0.010310
2022-01-10 04:55:38,791 iteration 6392 : loss : 0.008676, loss_ce: 0.003570
 94%|███████████████████████████▎ | 376/400 [2:43:45<10:37, 26.58s/it]2022-01-10 04:55:40,261 iteration 6393 : loss : 0.012331, loss_ce: 0.003910
2022-01-10 04:55:41,690 iteration 6394 : loss : 0.020487, loss_ce: 0.007059
2022-01-10 04:55:43,064 iteration 6395 : loss : 0.011422, loss_ce: 0.005099
2022-01-10 04:55:44,558 iteration 6396 : loss : 0.025844, loss_ce: 0.009951
2022-01-10 04:55:45,937 iteration 6397 : loss : 0.019581, loss_ce: 0.009723
2022-01-10 04:55:47,310 iteration 6398 : loss : 0.010365, loss_ce: 0.003021
2022-01-10 04:55:48,767 iteration 6399 : loss : 0.012421, loss_ce: 0.003833
2022-01-10 04:55:50,222 iteration 6400 : loss : 0.015155, loss_ce: 0.006839
2022-01-10 04:55:51,602 iteration 6401 : loss : 0.011105, loss_ce: 0.004215
2022-01-10 04:55:53,040 iteration 6402 : loss : 0.018469, loss_ce: 0.008759
2022-01-10 04:55:54,576 iteration 6403 : loss : 0.019169, loss_ce: 0.007076
2022-01-10 04:55:55,982 iteration 6404 : loss : 0.016750, loss_ce: 0.004832
2022-01-10 04:55:57,386 iteration 6405 : loss : 0.019596, loss_ce: 0.007506
2022-01-10 04:55:58,947 iteration 6406 : loss : 0.014675, loss_ce: 0.006333
2022-01-10 04:56:00,300 iteration 6407 : loss : 0.010515, loss_ce: 0.002948
2022-01-10 04:56:01,710 iteration 6408 : loss : 0.013793, loss_ce: 0.003279
2022-01-10 04:56:03,086 iteration 6409 : loss : 0.017835, loss_ce: 0.007113
 94%|███████████████████████████▎ | 377/400 [2:44:09<09:55, 25.89s/it]2022-01-10 04:56:04,678 iteration 6410 : loss : 0.032574, loss_ce: 0.011196
2022-01-10 04:56:06,181 iteration 6411 : loss : 0.018793, loss_ce: 0.007238
2022-01-10 04:56:07,616 iteration 6412 : loss : 0.014699, loss_ce: 0.006874
2022-01-10 04:56:09,024 iteration 6413 : loss : 0.015320, loss_ce: 0.006283
2022-01-10 04:56:10,470 iteration 6414 : loss : 0.017708, loss_ce: 0.006088
2022-01-10 04:56:11,859 iteration 6415 : loss : 0.015144, loss_ce: 0.005256
2022-01-10 04:56:13,257 iteration 6416 : loss : 0.022090, loss_ce: 0.006026
2022-01-10 04:56:14,702 iteration 6417 : loss : 0.022930, loss_ce: 0.011304
2022-01-10 04:56:16,145 iteration 6418 : loss : 0.013646, loss_ce: 0.004978
2022-01-10 04:56:17,553 iteration 6419 : loss : 0.014518, loss_ce: 0.004756
2022-01-10 04:56:19,005 iteration 6420 : loss : 0.019039, loss_ce: 0.005972
2022-01-10 04:56:20,426 iteration 6421 : loss : 0.010156, loss_ce: 0.003641
2022-01-10 04:56:21,768 iteration 6422 : loss : 0.012456, loss_ce: 0.004976
2022-01-10 04:56:23,208 iteration 6423 : loss : 0.020844, loss_ce: 0.006358
2022-01-10 04:56:24,662 iteration 6424 : loss : 0.012402, loss_ce: 0.005689
2022-01-10 04:56:26,000 iteration 6425 : loss : 0.014540, loss_ce: 0.005503
2022-01-10 04:56:27,445 iteration 6426 : loss : 0.018554, loss_ce: 0.008414
 94%|███████████████████████████▍ | 378/400 [2:44:33<09:19, 25.43s/it]2022-01-10 04:56:28,928 iteration 6427 : loss : 0.014275, loss_ce: 0.005798
2022-01-10 04:56:30,259 iteration 6428 : loss : 0.010919, loss_ce: 0.005212
2022-01-10 04:56:31,702 iteration 6429 : loss : 0.021168, loss_ce: 0.006106
2022-01-10 04:56:33,052 iteration 6430 : loss : 0.014501, loss_ce: 0.004961
2022-01-10 04:56:34,465 iteration 6431 : loss : 0.015388, loss_ce: 0.005194
2022-01-10 04:56:35,883 iteration 6432 : loss : 0.011831, loss_ce: 0.004383
2022-01-10 04:56:37,359 iteration 6433 : loss : 0.016015, loss_ce: 0.005954
2022-01-10 04:56:38,686 iteration 6434 : loss : 0.009377, loss_ce: 0.003212
2022-01-10 04:56:40,092 iteration 6435 : loss : 0.014555, loss_ce: 0.008140
2022-01-10 04:56:41,482 iteration 6436 : loss : 0.017275, loss_ce: 0.008475
2022-01-10 04:56:42,916 iteration 6437 : loss : 0.018396, loss_ce: 0.005515
2022-01-10 04:56:44,365 iteration 6438 : loss : 0.014152, loss_ce: 0.005815
2022-01-10 04:56:45,808 iteration 6439 : loss : 0.019951, loss_ce: 0.009056
2022-01-10 04:56:47,286 iteration 6440 : loss : 0.016851, loss_ce: 0.007258
2022-01-10 04:56:48,714 iteration 6441 : loss : 0.011503, loss_ce: 0.002976
2022-01-10 04:56:50,121 iteration 6442 : loss : 0.016265, loss_ce: 0.007058
2022-01-10 04:56:51,558 iteration 6443 : loss : 0.023222, loss_ce: 0.008910
 95%|███████████████████████████▍ | 379/400 [2:44:58<08:45, 25.03s/it]2022-01-10 04:56:53,079 iteration 6444 : loss : 0.015148, loss_ce: 0.006746
2022-01-10 04:56:54,564 iteration 6445 : loss : 0.014408, loss_ce: 0.005480
2022-01-10 04:56:55,991 iteration 6446 : loss : 0.016204, loss_ce: 0.004347
2022-01-10 04:56:57,398 iteration 6447 : loss : 0.019872, loss_ce: 0.010635
2022-01-10 04:56:58,814 iteration 6448 : loss : 0.015835, loss_ce: 0.007690
2022-01-10 04:57:00,278 iteration 6449 : loss : 0.015265, loss_ce: 0.005139
2022-01-10 04:57:01,645 iteration 6450 : loss : 0.012859, loss_ce: 0.005788
2022-01-10 04:57:02,992 iteration 6451 : loss : 0.012022, loss_ce: 0.004304
2022-01-10 04:57:04,428 iteration 6452 : loss : 0.013568, loss_ce: 0.006391
2022-01-10 04:57:05,847 iteration 6453 : loss : 0.014216, loss_ce: 0.005819
2022-01-10 04:57:07,235 iteration 6454 : loss : 0.013955, loss_ce: 0.004632
2022-01-10 04:57:08,629 iteration 6455 : loss : 0.018224, loss_ce: 0.005911
2022-01-10 04:57:10,140 iteration 6456 : loss : 0.021484, loss_ce: 0.009568
2022-01-10 04:57:11,560 iteration 6457 : loss : 0.015972, loss_ce: 0.004885
2022-01-10 04:57:12,939 iteration 6458 : loss : 0.012440, loss_ce: 0.004014
2022-01-10 04:57:14,290 iteration 6459 : loss : 0.012616, loss_ce: 0.004151
2022-01-10 04:57:14,290 Training Data Eval:
2022-01-10 04:57:21,349   Average segmentation loss on training set: 0.0073
2022-01-10 04:57:21,350 Validation Data Eval:
2022-01-10 04:57:23,783   Average segmentation loss on validation set: 0.0898
2022-01-10 04:57:25,226 iteration 6460 : loss : 0.024227, loss_ce: 0.006488
 95%|███████████████████████████▌ | 380/400 [2:45:31<09:12, 27.63s/it]2022-01-10 04:57:26,710 iteration 6461 : loss : 0.017481, loss_ce: 0.004561
2022-01-10 04:57:28,130 iteration 6462 : loss : 0.019589, loss_ce: 0.007662
2022-01-10 04:57:29,492 iteration 6463 : loss : 0.009062, loss_ce: 0.002441
2022-01-10 04:57:30,852 iteration 6464 : loss : 0.011151, loss_ce: 0.003541
2022-01-10 04:57:32,245 iteration 6465 : loss : 0.017744, loss_ce: 0.004962
2022-01-10 04:57:33,681 iteration 6466 : loss : 0.012782, loss_ce: 0.004595
2022-01-10 04:57:35,144 iteration 6467 : loss : 0.013140, loss_ce: 0.005677
2022-01-10 04:57:36,625 iteration 6468 : loss : 0.023433, loss_ce: 0.011597
2022-01-10 04:57:38,035 iteration 6469 : loss : 0.015159, loss_ce: 0.006577
2022-01-10 04:57:39,428 iteration 6470 : loss : 0.027543, loss_ce: 0.007509
2022-01-10 04:57:40,817 iteration 6471 : loss : 0.027643, loss_ce: 0.010927
2022-01-10 04:57:42,213 iteration 6472 : loss : 0.014183, loss_ce: 0.004143
2022-01-10 04:57:43,562 iteration 6473 : loss : 0.013426, loss_ce: 0.006928
2022-01-10 04:57:45,029 iteration 6474 : loss : 0.014776, loss_ce: 0.007822
2022-01-10 04:57:46,373 iteration 6475 : loss : 0.011795, loss_ce: 0.004380
2022-01-10 04:57:47,748 iteration 6476 : loss : 0.013163, loss_ce: 0.004870
2022-01-10 04:57:49,081 iteration 6477 : loss : 0.011679, loss_ce: 0.004417
 95%|███████████████████████████▌ | 381/400 [2:45:55<08:23, 26.50s/it]2022-01-10 04:57:50,577 iteration 6478 : loss : 0.014893, loss_ce: 0.006299
2022-01-10 04:57:52,136 iteration 6479 : loss : 0.020241, loss_ce: 0.012439
2022-01-10 04:57:53,581 iteration 6480 : loss : 0.017141, loss_ce: 0.003907
2022-01-10 04:57:55,014 iteration 6481 : loss : 0.019770, loss_ce: 0.007834
2022-01-10 04:57:56,444 iteration 6482 : loss : 0.019062, loss_ce: 0.007400
2022-01-10 04:57:57,829 iteration 6483 : loss : 0.015581, loss_ce: 0.005759
2022-01-10 04:57:59,232 iteration 6484 : loss : 0.020324, loss_ce: 0.006494
2022-01-10 04:58:00,665 iteration 6485 : loss : 0.015961, loss_ce: 0.006745
2022-01-10 04:58:02,157 iteration 6486 : loss : 0.018923, loss_ce: 0.005826
2022-01-10 04:58:03,596 iteration 6487 : loss : 0.014855, loss_ce: 0.004837
2022-01-10 04:58:04,920 iteration 6488 : loss : 0.009784, loss_ce: 0.004190
2022-01-10 04:58:06,382 iteration 6489 : loss : 0.022883, loss_ce: 0.005530
2022-01-10 04:58:07,707 iteration 6490 : loss : 0.010993, loss_ce: 0.004175
2022-01-10 04:58:09,146 iteration 6491 : loss : 0.017635, loss_ce: 0.005420
2022-01-10 04:58:10,528 iteration 6492 : loss : 0.012029, loss_ce: 0.004622
2022-01-10 04:58:11,968 iteration 6493 : loss : 0.011984, loss_ce: 0.003517
2022-01-10 04:58:13,372 iteration 6494 : loss : 0.014899, loss_ce: 0.005556
 96%|███████████████████████████▋ | 382/400 [2:46:19<07:44, 25.83s/it]2022-01-10 04:58:14,881 iteration 6495 : loss : 0.012915, loss_ce: 0.004144
2022-01-10 04:58:16,279 iteration 6496 : loss : 0.016982, loss_ce: 0.005875
2022-01-10 04:58:17,720 iteration 6497 : loss : 0.009585, loss_ce: 0.003392
2022-01-10 04:58:19,144 iteration 6498 : loss : 0.019560, loss_ce: 0.010354
2022-01-10 04:58:20,660 iteration 6499 : loss : 0.017797, loss_ce: 0.005927
2022-01-10 04:58:22,118 iteration 6500 : loss : 0.025259, loss_ce: 0.011184
2022-01-10 04:58:23,594 iteration 6501 : loss : 0.014555, loss_ce: 0.003609
2022-01-10 04:58:25,059 iteration 6502 : loss : 0.023372, loss_ce: 0.008997
2022-01-10 04:58:26,568 iteration 6503 : loss : 0.019725, loss_ce: 0.006782
2022-01-10 04:58:27,933 iteration 6504 : loss : 0.010189, loss_ce: 0.005016
2022-01-10 04:58:29,322 iteration 6505 : loss : 0.010771, loss_ce: 0.002898
2022-01-10 04:58:30,718 iteration 6506 : loss : 0.012241, loss_ce: 0.003751
2022-01-10 04:58:32,115 iteration 6507 : loss : 0.015330, loss_ce: 0.006076
2022-01-10 04:58:33,532 iteration 6508 : loss : 0.011952, loss_ce: 0.003956
2022-01-10 04:58:34,934 iteration 6509 : loss : 0.012215, loss_ce: 0.005496
2022-01-10 04:58:36,313 iteration 6510 : loss : 0.021826, loss_ce: 0.006456
2022-01-10 04:58:37,721 iteration 6511 : loss : 0.018386, loss_ce: 0.008125
 96%|███████████████████████████▊ | 383/400 [2:46:44<07:11, 25.39s/it]2022-01-10 04:58:39,187 iteration 6512 : loss : 0.011385, loss_ce: 0.004382
2022-01-10 04:58:40,539 iteration 6513 : loss : 0.012626, loss_ce: 0.004503
2022-01-10 04:58:41,939 iteration 6514 : loss : 0.010000, loss_ce: 0.002272
2022-01-10 04:58:43,343 iteration 6515 : loss : 0.017239, loss_ce: 0.005535
2022-01-10 04:58:44,749 iteration 6516 : loss : 0.024293, loss_ce: 0.006686
2022-01-10 04:58:46,233 iteration 6517 : loss : 0.015803, loss_ce: 0.008059
2022-01-10 04:58:47,588 iteration 6518 : loss : 0.015248, loss_ce: 0.006054
2022-01-10 04:58:49,072 iteration 6519 : loss : 0.020096, loss_ce: 0.006504
2022-01-10 04:58:50,416 iteration 6520 : loss : 0.011811, loss_ce: 0.005144
2022-01-10 04:58:51,865 iteration 6521 : loss : 0.012843, loss_ce: 0.005323
2022-01-10 04:58:53,265 iteration 6522 : loss : 0.014018, loss_ce: 0.006620
2022-01-10 04:58:54,623 iteration 6523 : loss : 0.018756, loss_ce: 0.006229
2022-01-10 04:58:56,037 iteration 6524 : loss : 0.019321, loss_ce: 0.006358
2022-01-10 04:58:57,423 iteration 6525 : loss : 0.013013, loss_ce: 0.004034
2022-01-10 04:58:58,855 iteration 6526 : loss : 0.011296, loss_ce: 0.004007
2022-01-10 04:59:00,321 iteration 6527 : loss : 0.011422, loss_ce: 0.003529
2022-01-10 04:59:01,755 iteration 6528 : loss : 0.014991, loss_ce: 0.005946
 96%|███████████████████████████▊ | 384/400 [2:47:08<06:39, 24.98s/it]2022-01-10 04:59:03,173 iteration 6529 : loss : 0.014985, loss_ce: 0.006362
2022-01-10 04:59:04,507 iteration 6530 : loss : 0.008904, loss_ce: 0.003817
2022-01-10 04:59:05,920 iteration 6531 : loss : 0.018611, loss_ce: 0.005441
2022-01-10 04:59:07,382 iteration 6532 : loss : 0.013072, loss_ce: 0.004209
2022-01-10 04:59:08,759 iteration 6533 : loss : 0.013924, loss_ce: 0.005970
2022-01-10 04:59:10,206 iteration 6534 : loss : 0.018137, loss_ce: 0.005047
2022-01-10 04:59:11,626 iteration 6535 : loss : 0.013986, loss_ce: 0.005357
2022-01-10 04:59:13,074 iteration 6536 : loss : 0.014790, loss_ce: 0.005526
2022-01-10 04:59:14,592 iteration 6537 : loss : 0.022371, loss_ce: 0.007167
2022-01-10 04:59:15,965 iteration 6538 : loss : 0.013604, loss_ce: 0.004485
2022-01-10 04:59:17,500 iteration 6539 : loss : 0.017029, loss_ce: 0.006502
2022-01-10 04:59:18,872 iteration 6540 : loss : 0.016176, loss_ce: 0.007341
2022-01-10 04:59:20,214 iteration 6541 : loss : 0.011909, loss_ce: 0.005197
2022-01-10 04:59:21,613 iteration 6542 : loss : 0.016947, loss_ce: 0.006265
2022-01-10 04:59:23,091 iteration 6543 : loss : 0.014688, loss_ce: 0.006025
2022-01-10 04:59:24,495 iteration 6544 : loss : 0.018051, loss_ce: 0.006576
2022-01-10 04:59:24,495 Training Data Eval:
2022-01-10 04:59:31,557   Average segmentation loss on training set: 0.0073
2022-01-10 04:59:31,557 Validation Data Eval:
2022-01-10 04:59:33,991   Average segmentation loss on validation set: 0.0848
2022-01-10 04:59:35,395 iteration 6545 : loss : 0.010351, loss_ce: 0.002760
 96%|███████████████████████████▉ | 385/400 [2:47:41<06:53, 27.58s/it]2022-01-10 04:59:36,924 iteration 6546 : loss : 0.015018, loss_ce: 0.005453
2022-01-10 04:59:38,281 iteration 6547 : loss : 0.012479, loss_ce: 0.002622
2022-01-10 04:59:39,706 iteration 6548 : loss : 0.040042, loss_ce: 0.013016
2022-01-10 04:59:41,204 iteration 6549 : loss : 0.030469, loss_ce: 0.007628
2022-01-10 04:59:42,611 iteration 6550 : loss : 0.013423, loss_ce: 0.006159
2022-01-10 04:59:44,084 iteration 6551 : loss : 0.018546, loss_ce: 0.007254
2022-01-10 04:59:45,600 iteration 6552 : loss : 0.015800, loss_ce: 0.005945
2022-01-10 04:59:47,005 iteration 6553 : loss : 0.013418, loss_ce: 0.005114
2022-01-10 04:59:48,544 iteration 6554 : loss : 0.021510, loss_ce: 0.007394
2022-01-10 04:59:49,927 iteration 6555 : loss : 0.012454, loss_ce: 0.004378
2022-01-10 04:59:51,357 iteration 6556 : loss : 0.019801, loss_ce: 0.007757
2022-01-10 04:59:52,694 iteration 6557 : loss : 0.008789, loss_ce: 0.003035
2022-01-10 04:59:54,154 iteration 6558 : loss : 0.013354, loss_ce: 0.005305
2022-01-10 04:59:55,497 iteration 6559 : loss : 0.009698, loss_ce: 0.003200
2022-01-10 04:59:56,858 iteration 6560 : loss : 0.011141, loss_ce: 0.005117
2022-01-10 04:59:58,285 iteration 6561 : loss : 0.013576, loss_ce: 0.004556
2022-01-10 04:59:59,696 iteration 6562 : loss : 0.013526, loss_ce: 0.005740
 96%|███████████████████████████▉ | 386/400 [2:48:06<06:12, 26.59s/it]2022-01-10 05:00:01,180 iteration 6563 : loss : 0.017242, loss_ce: 0.009220
2022-01-10 05:00:02,657 iteration 6564 : loss : 0.026303, loss_ce: 0.008233
2022-01-10 05:00:04,033 iteration 6565 : loss : 0.013864, loss_ce: 0.003868
2022-01-10 05:00:05,517 iteration 6566 : loss : 0.021810, loss_ce: 0.007417
2022-01-10 05:00:06,937 iteration 6567 : loss : 0.019767, loss_ce: 0.006977
2022-01-10 05:00:08,346 iteration 6568 : loss : 0.010417, loss_ce: 0.003780
2022-01-10 05:00:09,731 iteration 6569 : loss : 0.014890, loss_ce: 0.004707
2022-01-10 05:00:11,240 iteration 6570 : loss : 0.014782, loss_ce: 0.005896
2022-01-10 05:00:12,583 iteration 6571 : loss : 0.011668, loss_ce: 0.005054
2022-01-10 05:00:13,958 iteration 6572 : loss : 0.013278, loss_ce: 0.004397
2022-01-10 05:00:15,321 iteration 6573 : loss : 0.011964, loss_ce: 0.003915
2022-01-10 05:00:16,741 iteration 6574 : loss : 0.014206, loss_ce: 0.005954
2022-01-10 05:00:18,057 iteration 6575 : loss : 0.009086, loss_ce: 0.003747
2022-01-10 05:00:19,472 iteration 6576 : loss : 0.024283, loss_ce: 0.009527
2022-01-10 05:00:20,916 iteration 6577 : loss : 0.015840, loss_ce: 0.006582
2022-01-10 05:00:22,348 iteration 6578 : loss : 0.013832, loss_ce: 0.006807
2022-01-10 05:00:23,752 iteration 6579 : loss : 0.014874, loss_ce: 0.003603
 97%|████████████████████████████ | 387/400 [2:48:30<05:35, 25.84s/it]2022-01-10 05:00:25,266 iteration 6580 : loss : 0.020651, loss_ce: 0.007711
2022-01-10 05:00:26,720 iteration 6581 : loss : 0.017288, loss_ce: 0.005024
2022-01-10 05:00:28,187 iteration 6582 : loss : 0.029589, loss_ce: 0.011813
2022-01-10 05:00:29,604 iteration 6583 : loss : 0.010847, loss_ce: 0.004510
2022-01-10 05:00:31,034 iteration 6584 : loss : 0.013276, loss_ce: 0.005147
2022-01-10 05:00:32,407 iteration 6585 : loss : 0.014546, loss_ce: 0.005132
2022-01-10 05:00:33,833 iteration 6586 : loss : 0.016062, loss_ce: 0.005795
2022-01-10 05:00:35,184 iteration 6587 : loss : 0.012725, loss_ce: 0.005323
2022-01-10 05:00:36,644 iteration 6588 : loss : 0.019352, loss_ce: 0.007034
2022-01-10 05:00:38,051 iteration 6589 : loss : 0.013486, loss_ce: 0.004435
2022-01-10 05:00:39,441 iteration 6590 : loss : 0.012724, loss_ce: 0.004978
2022-01-10 05:00:40,807 iteration 6591 : loss : 0.014121, loss_ce: 0.006125
2022-01-10 05:00:42,218 iteration 6592 : loss : 0.016703, loss_ce: 0.006848
2022-01-10 05:00:43,681 iteration 6593 : loss : 0.017157, loss_ce: 0.007443
2022-01-10 05:00:45,072 iteration 6594 : loss : 0.014567, loss_ce: 0.005350
2022-01-10 05:00:46,420 iteration 6595 : loss : 0.012374, loss_ce: 0.003898
2022-01-10 05:00:47,790 iteration 6596 : loss : 0.008127, loss_ce: 0.003243
 97%|████████████████████████████▏| 388/400 [2:48:54<05:03, 25.30s/it]2022-01-10 05:00:49,315 iteration 6597 : loss : 0.016088, loss_ce: 0.007788
2022-01-10 05:00:50,785 iteration 6598 : loss : 0.028828, loss_ce: 0.007720
2022-01-10 05:00:52,144 iteration 6599 : loss : 0.016639, loss_ce: 0.005541
2022-01-10 05:00:53,592 iteration 6600 : loss : 0.014697, loss_ce: 0.005240
2022-01-10 05:00:54,951 iteration 6601 : loss : 0.011925, loss_ce: 0.004812
2022-01-10 05:00:56,263 iteration 6602 : loss : 0.008370, loss_ce: 0.002974
2022-01-10 05:00:57,701 iteration 6603 : loss : 0.018325, loss_ce: 0.004830
2022-01-10 05:00:59,188 iteration 6604 : loss : 0.014956, loss_ce: 0.005524
2022-01-10 05:01:00,627 iteration 6605 : loss : 0.017807, loss_ce: 0.006402
2022-01-10 05:01:02,057 iteration 6606 : loss : 0.014273, loss_ce: 0.007112
2022-01-10 05:01:03,437 iteration 6607 : loss : 0.015851, loss_ce: 0.004881
2022-01-10 05:01:04,883 iteration 6608 : loss : 0.013264, loss_ce: 0.004657
2022-01-10 05:01:06,357 iteration 6609 : loss : 0.013383, loss_ce: 0.005364
2022-01-10 05:01:07,755 iteration 6610 : loss : 0.014437, loss_ce: 0.003987
2022-01-10 05:01:09,075 iteration 6611 : loss : 0.011287, loss_ce: 0.004966
2022-01-10 05:01:10,492 iteration 6612 : loss : 0.013488, loss_ce: 0.004877
2022-01-10 05:01:11,949 iteration 6613 : loss : 0.019596, loss_ce: 0.005608
 97%|████████████████████████████▏| 389/400 [2:49:18<04:34, 24.96s/it]2022-01-10 05:01:13,327 iteration 6614 : loss : 0.008871, loss_ce: 0.003489
2022-01-10 05:01:14,760 iteration 6615 : loss : 0.013828, loss_ce: 0.005151
2022-01-10 05:01:16,201 iteration 6616 : loss : 0.010833, loss_ce: 0.004534
2022-01-10 05:01:17,576 iteration 6617 : loss : 0.010861, loss_ce: 0.002015
2022-01-10 05:01:18,990 iteration 6618 : loss : 0.016283, loss_ce: 0.005291
2022-01-10 05:01:20,440 iteration 6619 : loss : 0.013417, loss_ce: 0.004684
2022-01-10 05:01:21,853 iteration 6620 : loss : 0.021460, loss_ce: 0.006090
2022-01-10 05:01:23,192 iteration 6621 : loss : 0.012272, loss_ce: 0.005474
2022-01-10 05:01:24,494 iteration 6622 : loss : 0.009934, loss_ce: 0.002989
2022-01-10 05:01:25,907 iteration 6623 : loss : 0.018857, loss_ce: 0.008836
2022-01-10 05:01:27,202 iteration 6624 : loss : 0.010632, loss_ce: 0.003850
2022-01-10 05:01:28,505 iteration 6625 : loss : 0.009172, loss_ce: 0.003683
2022-01-10 05:01:29,903 iteration 6626 : loss : 0.014252, loss_ce: 0.005572
2022-01-10 05:01:31,250 iteration 6627 : loss : 0.010329, loss_ce: 0.003295
2022-01-10 05:01:32,687 iteration 6628 : loss : 0.014811, loss_ce: 0.004902
2022-01-10 05:01:34,053 iteration 6629 : loss : 0.011690, loss_ce: 0.005084
2022-01-10 05:01:34,053 Training Data Eval:
2022-01-10 05:01:41,113   Average segmentation loss on training set: 0.0072
2022-01-10 05:01:41,113 Validation Data Eval:
2022-01-10 05:01:43,617   Average segmentation loss on validation set: 0.0794
2022-01-10 05:01:45,073 iteration 6630 : loss : 0.017356, loss_ce: 0.006792
 98%|████████████████████████████▎| 390/400 [2:49:51<04:34, 27.40s/it]2022-01-10 05:01:46,458 iteration 6631 : loss : 0.009981, loss_ce: 0.003723
2022-01-10 05:01:47,891 iteration 6632 : loss : 0.014152, loss_ce: 0.003013
2022-01-10 05:01:49,283 iteration 6633 : loss : 0.010290, loss_ce: 0.004069
2022-01-10 05:01:50,642 iteration 6634 : loss : 0.012927, loss_ce: 0.004898
2022-01-10 05:01:52,088 iteration 6635 : loss : 0.012134, loss_ce: 0.004653
2022-01-10 05:01:53,455 iteration 6636 : loss : 0.010313, loss_ce: 0.005102
2022-01-10 05:01:54,885 iteration 6637 : loss : 0.018695, loss_ce: 0.006527
2022-01-10 05:01:56,310 iteration 6638 : loss : 0.011831, loss_ce: 0.003353
2022-01-10 05:01:57,730 iteration 6639 : loss : 0.014615, loss_ce: 0.004562
2022-01-10 05:01:59,162 iteration 6640 : loss : 0.021767, loss_ce: 0.008029
2022-01-10 05:02:00,555 iteration 6641 : loss : 0.011951, loss_ce: 0.004645
2022-01-10 05:02:01,958 iteration 6642 : loss : 0.013848, loss_ce: 0.004008
2022-01-10 05:02:03,432 iteration 6643 : loss : 0.017040, loss_ce: 0.008222
2022-01-10 05:02:04,916 iteration 6644 : loss : 0.015756, loss_ce: 0.006977
2022-01-10 05:02:06,318 iteration 6645 : loss : 0.015813, loss_ce: 0.006542
2022-01-10 05:02:07,724 iteration 6646 : loss : 0.018402, loss_ce: 0.008279
2022-01-10 05:02:09,139 iteration 6647 : loss : 0.014536, loss_ce: 0.002816
 98%|████████████████████████████▎| 391/400 [2:50:15<03:57, 26.40s/it]2022-01-10 05:02:10,635 iteration 6648 : loss : 0.015049, loss_ce: 0.006312
2022-01-10 05:02:12,084 iteration 6649 : loss : 0.019927, loss_ce: 0.005834
2022-01-10 05:02:13,445 iteration 6650 : loss : 0.011469, loss_ce: 0.004186
2022-01-10 05:02:14,835 iteration 6651 : loss : 0.015728, loss_ce: 0.007932
2022-01-10 05:02:16,275 iteration 6652 : loss : 0.014619, loss_ce: 0.007448
2022-01-10 05:02:17,745 iteration 6653 : loss : 0.020707, loss_ce: 0.006611
2022-01-10 05:02:19,281 iteration 6654 : loss : 0.016138, loss_ce: 0.005217
2022-01-10 05:02:20,681 iteration 6655 : loss : 0.013786, loss_ce: 0.006525
2022-01-10 05:02:22,171 iteration 6656 : loss : 0.026019, loss_ce: 0.008097
2022-01-10 05:02:23,601 iteration 6657 : loss : 0.021646, loss_ce: 0.006772
2022-01-10 05:02:25,073 iteration 6658 : loss : 0.015999, loss_ce: 0.006100
2022-01-10 05:02:26,497 iteration 6659 : loss : 0.017559, loss_ce: 0.005313
2022-01-10 05:02:27,896 iteration 6660 : loss : 0.011122, loss_ce: 0.004296
2022-01-10 05:02:29,265 iteration 6661 : loss : 0.013495, loss_ce: 0.005620
2022-01-10 05:02:30,711 iteration 6662 : loss : 0.016777, loss_ce: 0.005847
2022-01-10 05:02:32,094 iteration 6663 : loss : 0.015534, loss_ce: 0.006848
2022-01-10 05:02:33,543 iteration 6664 : loss : 0.013314, loss_ce: 0.007128
 98%|████████████████████████████▍| 392/400 [2:50:40<03:26, 25.81s/it]2022-01-10 05:02:35,026 iteration 6665 : loss : 0.026241, loss_ce: 0.010312
2022-01-10 05:02:36,476 iteration 6666 : loss : 0.010280, loss_ce: 0.003041
2022-01-10 05:02:37,892 iteration 6667 : loss : 0.016135, loss_ce: 0.007052
2022-01-10 05:02:39,428 iteration 6668 : loss : 0.022111, loss_ce: 0.011612
2022-01-10 05:02:40,789 iteration 6669 : loss : 0.013341, loss_ce: 0.004883
2022-01-10 05:02:42,157 iteration 6670 : loss : 0.010004, loss_ce: 0.003910
2022-01-10 05:02:43,542 iteration 6671 : loss : 0.014683, loss_ce: 0.005645
2022-01-10 05:02:44,870 iteration 6672 : loss : 0.013289, loss_ce: 0.003186
2022-01-10 05:02:46,310 iteration 6673 : loss : 0.018024, loss_ce: 0.006195
2022-01-10 05:02:47,714 iteration 6674 : loss : 0.015627, loss_ce: 0.006079
2022-01-10 05:02:49,165 iteration 6675 : loss : 0.013487, loss_ce: 0.003876
2022-01-10 05:02:50,525 iteration 6676 : loss : 0.010326, loss_ce: 0.003019
2022-01-10 05:02:51,914 iteration 6677 : loss : 0.017047, loss_ce: 0.004173
2022-01-10 05:02:53,208 iteration 6678 : loss : 0.009027, loss_ce: 0.003846
2022-01-10 05:02:54,728 iteration 6679 : loss : 0.018712, loss_ce: 0.008535
2022-01-10 05:02:56,138 iteration 6680 : loss : 0.013891, loss_ce: 0.004693
2022-01-10 05:02:57,593 iteration 6681 : loss : 0.013111, loss_ce: 0.006146
 98%|████████████████████████████▍| 393/400 [2:51:04<02:56, 25.28s/it]2022-01-10 05:02:59,073 iteration 6682 : loss : 0.011727, loss_ce: 0.005724
2022-01-10 05:03:00,572 iteration 6683 : loss : 0.014216, loss_ce: 0.004546
2022-01-10 05:03:01,940 iteration 6684 : loss : 0.011294, loss_ce: 0.003471
2022-01-10 05:03:03,402 iteration 6685 : loss : 0.021540, loss_ce: 0.009451
2022-01-10 05:03:04,759 iteration 6686 : loss : 0.013544, loss_ce: 0.007458
2022-01-10 05:03:06,161 iteration 6687 : loss : 0.036573, loss_ce: 0.009283
2022-01-10 05:03:07,623 iteration 6688 : loss : 0.016322, loss_ce: 0.006369
2022-01-10 05:03:09,005 iteration 6689 : loss : 0.009466, loss_ce: 0.003105
2022-01-10 05:03:10,389 iteration 6690 : loss : 0.014739, loss_ce: 0.005557
2022-01-10 05:03:11,856 iteration 6691 : loss : 0.020297, loss_ce: 0.007780
2022-01-10 05:03:13,292 iteration 6692 : loss : 0.020915, loss_ce: 0.005733
2022-01-10 05:03:14,755 iteration 6693 : loss : 0.011906, loss_ce: 0.004032
2022-01-10 05:03:16,196 iteration 6694 : loss : 0.015988, loss_ce: 0.007722
2022-01-10 05:03:17,662 iteration 6695 : loss : 0.015118, loss_ce: 0.006448
2022-01-10 05:03:19,023 iteration 6696 : loss : 0.011984, loss_ce: 0.005542
2022-01-10 05:03:20,485 iteration 6697 : loss : 0.016117, loss_ce: 0.005761
2022-01-10 05:03:21,899 iteration 6698 : loss : 0.010945, loss_ce: 0.003529
 98%|████████████████████████████▌| 394/400 [2:51:28<02:29, 24.99s/it]2022-01-10 05:03:23,340 iteration 6699 : loss : 0.016044, loss_ce: 0.008296
2022-01-10 05:03:24,821 iteration 6700 : loss : 0.026153, loss_ce: 0.011621
2022-01-10 05:03:26,219 iteration 6701 : loss : 0.012308, loss_ce: 0.005686
2022-01-10 05:03:27,599 iteration 6702 : loss : 0.029435, loss_ce: 0.012024
2022-01-10 05:03:28,997 iteration 6703 : loss : 0.011521, loss_ce: 0.003588
2022-01-10 05:03:30,447 iteration 6704 : loss : 0.016520, loss_ce: 0.005948
2022-01-10 05:03:31,822 iteration 6705 : loss : 0.015642, loss_ce: 0.005283
2022-01-10 05:03:33,191 iteration 6706 : loss : 0.010078, loss_ce: 0.004917
2022-01-10 05:03:34,562 iteration 6707 : loss : 0.013629, loss_ce: 0.003759
2022-01-10 05:03:35,896 iteration 6708 : loss : 0.010827, loss_ce: 0.003771
2022-01-10 05:03:37,260 iteration 6709 : loss : 0.014344, loss_ce: 0.004611
2022-01-10 05:03:38,633 iteration 6710 : loss : 0.011039, loss_ce: 0.004205
2022-01-10 05:03:39,998 iteration 6711 : loss : 0.017900, loss_ce: 0.002978
2022-01-10 05:03:41,428 iteration 6712 : loss : 0.019014, loss_ce: 0.006658
2022-01-10 05:03:42,911 iteration 6713 : loss : 0.020518, loss_ce: 0.007558
2022-01-10 05:03:44,345 iteration 6714 : loss : 0.015140, loss_ce: 0.006823
2022-01-10 05:03:44,346 Training Data Eval:
2022-01-10 05:03:51,405   Average segmentation loss on training set: 0.0069
2022-01-10 05:03:51,406 Validation Data Eval:
2022-01-10 05:03:53,856   Average segmentation loss on validation set: 0.0790
2022-01-10 05:03:55,307 iteration 6715 : loss : 0.016623, loss_ce: 0.006157
 99%|████████████████████████████▋| 395/400 [2:52:01<02:17, 27.51s/it]2022-01-10 05:03:56,790 iteration 6716 : loss : 0.010412, loss_ce: 0.004311
2022-01-10 05:03:58,201 iteration 6717 : loss : 0.017118, loss_ce: 0.006616
2022-01-10 05:03:59,548 iteration 6718 : loss : 0.010689, loss_ce: 0.003521
2022-01-10 05:04:00,935 iteration 6719 : loss : 0.011955, loss_ce: 0.003934
2022-01-10 05:04:02,342 iteration 6720 : loss : 0.010001, loss_ce: 0.003717
2022-01-10 05:04:03,787 iteration 6721 : loss : 0.016430, loss_ce: 0.005878
2022-01-10 05:04:05,244 iteration 6722 : loss : 0.019286, loss_ce: 0.005673
2022-01-10 05:04:06,644 iteration 6723 : loss : 0.014734, loss_ce: 0.005180
2022-01-10 05:04:08,019 iteration 6724 : loss : 0.015102, loss_ce: 0.007212
2022-01-10 05:04:09,459 iteration 6725 : loss : 0.013016, loss_ce: 0.006411
2022-01-10 05:04:10,877 iteration 6726 : loss : 0.014636, loss_ce: 0.004004
2022-01-10 05:04:12,315 iteration 6727 : loss : 0.015095, loss_ce: 0.006804
2022-01-10 05:04:13,757 iteration 6728 : loss : 0.016677, loss_ce: 0.007495
2022-01-10 05:04:15,114 iteration 6729 : loss : 0.011390, loss_ce: 0.003905
2022-01-10 05:04:16,495 iteration 6730 : loss : 0.020683, loss_ce: 0.007135
2022-01-10 05:04:17,872 iteration 6731 : loss : 0.015494, loss_ce: 0.006593
2022-01-10 05:04:19,279 iteration 6732 : loss : 0.011588, loss_ce: 0.005531
 99%|████████████████████████████▋| 396/400 [2:52:25<01:45, 26.45s/it]2022-01-10 05:04:20,789 iteration 6733 : loss : 0.015552, loss_ce: 0.007760
2022-01-10 05:04:22,158 iteration 6734 : loss : 0.007365, loss_ce: 0.002175
2022-01-10 05:04:23,578 iteration 6735 : loss : 0.012153, loss_ce: 0.005990
2022-01-10 05:04:24,987 iteration 6736 : loss : 0.010650, loss_ce: 0.003256
2022-01-10 05:04:26,360 iteration 6737 : loss : 0.011105, loss_ce: 0.005432
2022-01-10 05:04:27,761 iteration 6738 : loss : 0.010536, loss_ce: 0.003693
2022-01-10 05:04:29,201 iteration 6739 : loss : 0.015945, loss_ce: 0.006218
2022-01-10 05:04:30,687 iteration 6740 : loss : 0.012698, loss_ce: 0.004460
2022-01-10 05:04:32,044 iteration 6741 : loss : 0.011731, loss_ce: 0.005552
2022-01-10 05:04:33,496 iteration 6742 : loss : 0.016854, loss_ce: 0.006603
2022-01-10 05:04:34,931 iteration 6743 : loss : 0.015363, loss_ce: 0.004623
2022-01-10 05:04:36,417 iteration 6744 : loss : 0.018979, loss_ce: 0.005885
2022-01-10 05:04:37,795 iteration 6745 : loss : 0.017420, loss_ce: 0.005254
2022-01-10 05:04:39,179 iteration 6746 : loss : 0.017977, loss_ce: 0.008294
2022-01-10 05:04:40,579 iteration 6747 : loss : 0.018274, loss_ce: 0.006599
2022-01-10 05:04:41,999 iteration 6748 : loss : 0.017948, loss_ce: 0.006122
2022-01-10 05:04:43,444 iteration 6749 : loss : 0.017091, loss_ce: 0.005562
 99%|████████████████████████████▊| 397/400 [2:52:49<01:17, 25.76s/it]2022-01-10 05:04:44,886 iteration 6750 : loss : 0.016499, loss_ce: 0.006267
2022-01-10 05:04:46,342 iteration 6751 : loss : 0.017791, loss_ce: 0.008149
2022-01-10 05:04:47,792 iteration 6752 : loss : 0.021403, loss_ce: 0.010752
2022-01-10 05:04:49,213 iteration 6753 : loss : 0.011531, loss_ce: 0.004847
2022-01-10 05:04:50,675 iteration 6754 : loss : 0.020367, loss_ce: 0.006461
2022-01-10 05:04:52,083 iteration 6755 : loss : 0.017582, loss_ce: 0.007434
2022-01-10 05:04:53,503 iteration 6756 : loss : 0.011650, loss_ce: 0.004306
2022-01-10 05:04:54,924 iteration 6757 : loss : 0.027280, loss_ce: 0.010991
2022-01-10 05:04:56,293 iteration 6758 : loss : 0.009551, loss_ce: 0.004328
2022-01-10 05:04:57,734 iteration 6759 : loss : 0.013704, loss_ce: 0.004945
2022-01-10 05:04:59,136 iteration 6760 : loss : 0.014184, loss_ce: 0.005779
2022-01-10 05:05:00,582 iteration 6761 : loss : 0.017226, loss_ce: 0.007541
2022-01-10 05:05:01,897 iteration 6762 : loss : 0.009255, loss_ce: 0.003363
2022-01-10 05:05:03,310 iteration 6763 : loss : 0.013737, loss_ce: 0.003338
2022-01-10 05:05:04,695 iteration 6764 : loss : 0.019127, loss_ce: 0.007338
2022-01-10 05:05:06,063 iteration 6765 : loss : 0.012615, loss_ce: 0.004515
2022-01-10 05:05:07,448 iteration 6766 : loss : 0.015207, loss_ce: 0.005388
100%|████████████████████████████▊| 398/400 [2:53:13<00:50, 25.24s/it]2022-01-10 05:05:09,023 iteration 6767 : loss : 0.012898, loss_ce: 0.004119
2022-01-10 05:05:10,373 iteration 6768 : loss : 0.016114, loss_ce: 0.011234
2022-01-10 05:05:11,782 iteration 6769 : loss : 0.012177, loss_ce: 0.003878
2022-01-10 05:05:13,130 iteration 6770 : loss : 0.010870, loss_ce: 0.004450
2022-01-10 05:05:14,522 iteration 6771 : loss : 0.012831, loss_ce: 0.004060
2022-01-10 05:05:15,999 iteration 6772 : loss : 0.020001, loss_ce: 0.005665
2022-01-10 05:05:17,423 iteration 6773 : loss : 0.011325, loss_ce: 0.004560
2022-01-10 05:05:18,827 iteration 6774 : loss : 0.012098, loss_ce: 0.005002
2022-01-10 05:05:20,221 iteration 6775 : loss : 0.013711, loss_ce: 0.004798
2022-01-10 05:05:21,681 iteration 6776 : loss : 0.025186, loss_ce: 0.010026
2022-01-10 05:05:23,092 iteration 6777 : loss : 0.011998, loss_ce: 0.004454
2022-01-10 05:05:24,465 iteration 6778 : loss : 0.011638, loss_ce: 0.003508
2022-01-10 05:05:25,855 iteration 6779 : loss : 0.011232, loss_ce: 0.003592
2022-01-10 05:05:27,187 iteration 6780 : loss : 0.009784, loss_ce: 0.003373
2022-01-10 05:05:28,544 iteration 6781 : loss : 0.011541, loss_ce: 0.005198
2022-01-10 05:05:29,948 iteration 6782 : loss : 0.009100, loss_ce: 0.003917
2022-01-10 05:05:31,352 iteration 6783 : loss : 0.010483, loss_ce: 0.004124
100%|████████████████████████████▉| 399/400 [2:53:37<00:24, 24.84s/it]2022-01-10 05:05:32,910 iteration 6784 : loss : 0.018165, loss_ce: 0.004998
2022-01-10 05:05:34,256 iteration 6785 : loss : 0.014167, loss_ce: 0.006287
2022-01-10 05:05:35,569 iteration 6786 : loss : 0.011124, loss_ce: 0.005126
2022-01-10 05:05:36,998 iteration 6787 : loss : 0.021158, loss_ce: 0.007983
2022-01-10 05:05:38,340 iteration 6788 : loss : 0.016563, loss_ce: 0.003873
2022-01-10 05:05:39,762 iteration 6789 : loss : 0.009137, loss_ce: 0.003812
2022-01-10 05:05:41,195 iteration 6790 : loss : 0.010383, loss_ce: 0.003309
2022-01-10 05:05:42,653 iteration 6791 : loss : 0.027322, loss_ce: 0.008185
2022-01-10 05:05:44,100 iteration 6792 : loss : 0.012774, loss_ce: 0.005274
2022-01-10 05:05:45,509 iteration 6793 : loss : 0.011862, loss_ce: 0.004352
2022-01-10 05:05:46,934 iteration 6794 : loss : 0.013380, loss_ce: 0.004805
2022-01-10 05:05:48,327 iteration 6795 : loss : 0.010201, loss_ce: 0.003695
2022-01-10 05:05:49,734 iteration 6796 : loss : 0.017123, loss_ce: 0.006117
2022-01-10 05:05:51,118 iteration 6797 : loss : 0.015165, loss_ce: 0.005515
2022-01-10 05:05:52,447 iteration 6798 : loss : 0.011080, loss_ce: 0.003962
2022-01-10 05:05:53,845 iteration 6799 : loss : 0.015899, loss_ce: 0.007259
2022-01-10 05:05:53,846 Training Data Eval:
2022-01-10 05:06:00,905   Average segmentation loss on training set: 0.0070
2022-01-10 05:06:00,906 Validation Data Eval:
2022-01-10 05:06:03,349   Average segmentation loss on validation set: 0.0768
2022-01-10 05:06:04,742 iteration 6800 : loss : 0.013094, loss_ce: 0.005667
100%|█████████████████████████████| 400/400 [2:54:11<00:00, 27.40s/it]100%|█████████████████████████████| 400/400 [2:54:11<00:00, 26.13s/it]
