2022-01-06 21:41:13,259 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-06 21:41:13,260 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-06 21:41:13,260 ============================================================
2022-01-06 21:41:13,260 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-06 21:41:13,260 ============================================================
2022-01-06 21:41:13,260 Loading data...
2022-01-06 21:41:13,260 Reading NCI - RUNMC images...
2022-01-06 21:41:13,260 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-06 21:41:13,261 Already preprocessed this configuration. Loading now!
2022-01-06 21:41:13,277 Training Images: (256, 256, 286)
2022-01-06 21:41:13,277 Training Labels: (256, 256, 286)
2022-01-06 21:41:13,277 Validation Images: (256, 256, 98)
2022-01-06 21:41:13,277 Validation Labels: (256, 256, 98)
2022-01-06 21:41:13,277 ============================================================
2022-01-06 21:41:13,322 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-06 21:41:15,729 iteration 1 : loss : 0.983210, loss_ce: 1.213091
2022-01-06 21:41:17,102 iteration 2 : loss : 0.921211, loss_ce: 1.112888
2022-01-06 21:41:18,584 iteration 3 : loss : 0.865048, loss_ce: 1.008295
2022-01-06 21:41:19,975 iteration 4 : loss : 0.807049, loss_ce: 0.916679
2022-01-06 21:41:21,370 iteration 5 : loss : 0.757797, loss_ce: 0.842589
2022-01-06 21:41:22,782 iteration 6 : loss : 0.728870, loss_ce: 0.786269
2022-01-06 21:41:24,239 iteration 7 : loss : 0.683782, loss_ce: 0.728127
2022-01-06 21:41:25,650 iteration 8 : loss : 0.673857, loss_ce: 0.676472
2022-01-06 21:41:27,083 iteration 9 : loss : 0.613608, loss_ce: 0.642219
2022-01-06 21:41:28,545 iteration 10 : loss : 0.614814, loss_ce: 0.585661
2022-01-06 21:41:30,060 iteration 11 : loss : 0.579448, loss_ce: 0.555457
2022-01-06 21:41:31,462 iteration 12 : loss : 0.555729, loss_ce: 0.510279
2022-01-06 21:41:32,848 iteration 13 : loss : 0.545473, loss_ce: 0.482466
2022-01-06 21:41:34,207 iteration 14 : loss : 0.516615, loss_ce: 0.446103
2022-01-06 21:41:35,641 iteration 15 : loss : 0.485973, loss_ce: 0.409355
2022-01-06 21:41:37,053 iteration 16 : loss : 0.490111, loss_ce: 0.397720
2022-01-06 21:41:38,462 iteration 17 : loss : 0.447193, loss_ce: 0.367578
  0%|                               | 1/400 [00:25<2:47:39, 25.21s/it]2022-01-06 21:41:39,975 iteration 18 : loss : 0.465289, loss_ce: 0.330085
2022-01-06 21:41:41,320 iteration 19 : loss : 0.419186, loss_ce: 0.303253
2022-01-06 21:41:42,797 iteration 20 : loss : 0.406014, loss_ce: 0.280407
2022-01-06 21:41:44,187 iteration 21 : loss : 0.407031, loss_ce: 0.254317
2022-01-06 21:41:45,599 iteration 22 : loss : 0.381209, loss_ce: 0.258693
2022-01-06 21:41:47,093 iteration 23 : loss : 0.369223, loss_ce: 0.217236
2022-01-06 21:41:48,503 iteration 24 : loss : 0.353782, loss_ce: 0.215091
2022-01-06 21:41:49,973 iteration 25 : loss : 0.378547, loss_ce: 0.241953
2022-01-06 21:41:51,372 iteration 26 : loss : 0.332734, loss_ce: 0.190177
2022-01-06 21:41:52,715 iteration 27 : loss : 0.324977, loss_ce: 0.188553
2022-01-06 21:41:54,073 iteration 28 : loss : 0.311493, loss_ce: 0.170879
2022-01-06 21:41:55,528 iteration 29 : loss : 0.309132, loss_ce: 0.160288
2022-01-06 21:41:56,972 iteration 30 : loss : 0.316746, loss_ce: 0.158341
2022-01-06 21:41:58,334 iteration 31 : loss : 0.297550, loss_ce: 0.157975
2022-01-06 21:41:59,802 iteration 32 : loss : 0.308294, loss_ce: 0.176133
2022-01-06 21:42:01,261 iteration 33 : loss : 0.320961, loss_ce: 0.181079
2022-01-06 21:42:02,719 iteration 34 : loss : 0.305589, loss_ce: 0.174670
  0%|▏                              | 2/400 [00:49<2:43:29, 24.65s/it]2022-01-06 21:42:04,236 iteration 35 : loss : 0.292645, loss_ce: 0.140232
2022-01-06 21:42:05,711 iteration 36 : loss : 0.284185, loss_ce: 0.141570
2022-01-06 21:42:07,191 iteration 37 : loss : 0.282763, loss_ce: 0.121132
2022-01-06 21:42:08,588 iteration 38 : loss : 0.287659, loss_ce: 0.137500
2022-01-06 21:42:09,996 iteration 39 : loss : 0.254991, loss_ce: 0.120049
2022-01-06 21:42:11,499 iteration 40 : loss : 0.282570, loss_ce: 0.138402
2022-01-06 21:42:12,975 iteration 41 : loss : 0.322805, loss_ce: 0.155396
2022-01-06 21:42:14,422 iteration 42 : loss : 0.279908, loss_ce: 0.134460
2022-01-06 21:42:15,779 iteration 43 : loss : 0.269275, loss_ce: 0.120377
2022-01-06 21:42:17,276 iteration 44 : loss : 0.262450, loss_ce: 0.125182
2022-01-06 21:42:18,709 iteration 45 : loss : 0.246807, loss_ce: 0.111601
2022-01-06 21:42:20,153 iteration 46 : loss : 0.258906, loss_ce: 0.103181
2022-01-06 21:42:21,640 iteration 47 : loss : 0.225539, loss_ce: 0.092106
2022-01-06 21:42:23,101 iteration 48 : loss : 0.226662, loss_ce: 0.098438
2022-01-06 21:42:24,604 iteration 49 : loss : 0.301999, loss_ce: 0.145413
2022-01-06 21:42:26,006 iteration 50 : loss : 0.314703, loss_ce: 0.139132
2022-01-06 21:42:27,386 iteration 51 : loss : 0.284815, loss_ce: 0.134135
  1%|▏                              | 3/400 [01:14<2:43:07, 24.65s/it]2022-01-06 21:42:28,932 iteration 52 : loss : 0.292763, loss_ce: 0.152807
2022-01-06 21:42:30,400 iteration 53 : loss : 0.252730, loss_ce: 0.119300
2022-01-06 21:42:31,840 iteration 54 : loss : 0.255674, loss_ce: 0.112385
2022-01-06 21:42:33,291 iteration 55 : loss : 0.328974, loss_ce: 0.172770
2022-01-06 21:42:34,727 iteration 56 : loss : 0.268144, loss_ce: 0.117787
2022-01-06 21:42:36,185 iteration 57 : loss : 0.226622, loss_ce: 0.096033
2022-01-06 21:42:37,645 iteration 58 : loss : 0.309546, loss_ce: 0.130524
2022-01-06 21:42:39,067 iteration 59 : loss : 0.241708, loss_ce: 0.108314
2022-01-06 21:42:40,529 iteration 60 : loss : 0.252453, loss_ce: 0.107472
2022-01-06 21:42:41,978 iteration 61 : loss : 0.255462, loss_ce: 0.121993
2022-01-06 21:42:43,442 iteration 62 : loss : 0.352346, loss_ce: 0.141796
2022-01-06 21:42:44,825 iteration 63 : loss : 0.302096, loss_ce: 0.148650
2022-01-06 21:42:46,293 iteration 64 : loss : 0.299058, loss_ce: 0.129179
2022-01-06 21:42:47,721 iteration 65 : loss : 0.254199, loss_ce: 0.092578
2022-01-06 21:42:49,182 iteration 66 : loss : 0.243003, loss_ce: 0.101975
2022-01-06 21:42:50,715 iteration 67 : loss : 0.290780, loss_ce: 0.108713
2022-01-06 21:42:52,187 iteration 68 : loss : 0.220604, loss_ce: 0.096389
  1%|▎                              | 4/400 [01:38<2:43:04, 24.71s/it]2022-01-06 21:42:53,737 iteration 69 : loss : 0.228247, loss_ce: 0.101593
2022-01-06 21:42:55,283 iteration 70 : loss : 0.250705, loss_ce: 0.114584
2022-01-06 21:42:56,734 iteration 71 : loss : 0.253398, loss_ce: 0.111858
2022-01-06 21:42:58,232 iteration 72 : loss : 0.238092, loss_ce: 0.101968
2022-01-06 21:42:59,672 iteration 73 : loss : 0.244351, loss_ce: 0.112513
2022-01-06 21:43:01,103 iteration 74 : loss : 0.219626, loss_ce: 0.086545
2022-01-06 21:43:02,583 iteration 75 : loss : 0.207766, loss_ce: 0.083348
2022-01-06 21:43:04,072 iteration 76 : loss : 0.253310, loss_ce: 0.103330
2022-01-06 21:43:05,495 iteration 77 : loss : 0.232697, loss_ce: 0.105751
2022-01-06 21:43:07,019 iteration 78 : loss : 0.268937, loss_ce: 0.124014
2022-01-06 21:43:08,514 iteration 79 : loss : 0.281463, loss_ce: 0.109887
2022-01-06 21:43:09,997 iteration 80 : loss : 0.234627, loss_ce: 0.106742
2022-01-06 21:43:11,460 iteration 81 : loss : 0.263716, loss_ce: 0.117695
2022-01-06 21:43:12,969 iteration 82 : loss : 0.227021, loss_ce: 0.087700
2022-01-06 21:43:14,433 iteration 83 : loss : 0.262194, loss_ce: 0.092403
2022-01-06 21:43:15,986 iteration 84 : loss : 0.258027, loss_ce: 0.128093
2022-01-06 21:43:15,986 Training Data Eval:
2022-01-06 21:43:23,673   Average segmentation loss on training set: 1.4794
2022-01-06 21:43:23,673 Validation Data Eval:
2022-01-06 21:43:26,333   Average segmentation loss on validation set: 1.4754
2022-01-06 21:43:27,799 iteration 85 : loss : 0.281592, loss_ce: 0.114246
  1%|▍                              | 5/400 [02:14<3:08:32, 28.64s/it]2022-01-06 21:43:29,351 iteration 86 : loss : 0.271280, loss_ce: 0.096961
2022-01-06 21:43:30,952 iteration 87 : loss : 0.244320, loss_ce: 0.102250
2022-01-06 21:43:32,385 iteration 88 : loss : 0.240325, loss_ce: 0.103318
2022-01-06 21:43:33,897 iteration 89 : loss : 0.273703, loss_ce: 0.112739
2022-01-06 21:43:35,391 iteration 90 : loss : 0.235152, loss_ce: 0.098628
2022-01-06 21:43:36,955 iteration 91 : loss : 0.232123, loss_ce: 0.108850
2022-01-06 21:43:38,395 iteration 92 : loss : 0.231021, loss_ce: 0.100722
2022-01-06 21:43:39,858 iteration 93 : loss : 0.261945, loss_ce: 0.100614
2022-01-06 21:43:41,326 iteration 94 : loss : 0.260662, loss_ce: 0.106793
2022-01-06 21:43:42,918 iteration 95 : loss : 0.236372, loss_ce: 0.105578
2022-01-06 21:43:44,378 iteration 96 : loss : 0.231311, loss_ce: 0.101403
2022-01-06 21:43:45,838 iteration 97 : loss : 0.250572, loss_ce: 0.105153
2022-01-06 21:43:47,313 iteration 98 : loss : 0.248622, loss_ce: 0.108642
2022-01-06 21:43:48,860 iteration 99 : loss : 0.224005, loss_ce: 0.102101
2022-01-06 21:43:50,363 iteration 100 : loss : 0.238057, loss_ce: 0.101656
2022-01-06 21:43:51,844 iteration 101 : loss : 0.194102, loss_ce: 0.078002
2022-01-06 21:43:53,274 iteration 102 : loss : 0.241049, loss_ce: 0.098962
  2%|▍                              | 6/400 [02:40<3:01:01, 27.57s/it]2022-01-06 21:43:54,929 iteration 103 : loss : 0.224202, loss_ce: 0.094791
2022-01-06 21:43:56,508 iteration 104 : loss : 0.262274, loss_ce: 0.113259
2022-01-06 21:43:58,130 iteration 105 : loss : 0.258932, loss_ce: 0.106061
2022-01-06 21:43:59,604 iteration 106 : loss : 0.248567, loss_ce: 0.095590
2022-01-06 21:44:01,266 iteration 107 : loss : 0.218451, loss_ce: 0.093700
2022-01-06 21:44:02,746 iteration 108 : loss : 0.252783, loss_ce: 0.100494
2022-01-06 21:44:04,264 iteration 109 : loss : 0.218506, loss_ce: 0.095881
2022-01-06 21:44:05,742 iteration 110 : loss : 0.192714, loss_ce: 0.078139
2022-01-06 21:44:07,302 iteration 111 : loss : 0.241765, loss_ce: 0.115135
2022-01-06 21:44:08,788 iteration 112 : loss : 0.219120, loss_ce: 0.084497
2022-01-06 21:44:10,316 iteration 113 : loss : 0.238271, loss_ce: 0.112732
2022-01-06 21:44:11,799 iteration 114 : loss : 0.411758, loss_ce: 0.196931
2022-01-06 21:44:13,307 iteration 115 : loss : 0.213325, loss_ce: 0.087573
2022-01-06 21:44:14,872 iteration 116 : loss : 0.355075, loss_ce: 0.169715
2022-01-06 21:44:16,435 iteration 117 : loss : 0.352335, loss_ce: 0.180468
2022-01-06 21:44:17,944 iteration 118 : loss : 0.446402, loss_ce: 0.247082
2022-01-06 21:44:19,465 iteration 119 : loss : 0.321166, loss_ce: 0.127789
  2%|▌                              | 7/400 [03:06<2:57:37, 27.12s/it]2022-01-06 21:44:20,995 iteration 120 : loss : 0.370853, loss_ce: 0.177328
2022-01-06 21:44:22,457 iteration 121 : loss : 0.377848, loss_ce: 0.170406
2022-01-06 21:44:24,046 iteration 122 : loss : 0.335819, loss_ce: 0.137255
2022-01-06 21:44:25,611 iteration 123 : loss : 0.322160, loss_ce: 0.128246
2022-01-06 21:44:27,157 iteration 124 : loss : 0.367175, loss_ce: 0.159966
2022-01-06 21:44:28,665 iteration 125 : loss : 0.361585, loss_ce: 0.190383
2022-01-06 21:44:30,228 iteration 126 : loss : 0.326538, loss_ce: 0.144833
2022-01-06 21:44:31,730 iteration 127 : loss : 0.304424, loss_ce: 0.151814
2022-01-06 21:44:33,248 iteration 128 : loss : 0.286917, loss_ce: 0.137424
2022-01-06 21:44:34,823 iteration 129 : loss : 0.303978, loss_ce: 0.132609
2022-01-06 21:44:36,339 iteration 130 : loss : 0.296413, loss_ce: 0.123597
2022-01-06 21:44:37,940 iteration 131 : loss : 0.325411, loss_ce: 0.155492
2022-01-06 21:44:39,545 iteration 132 : loss : 0.281050, loss_ce: 0.082685
2022-01-06 21:44:41,027 iteration 133 : loss : 0.274693, loss_ce: 0.103302
2022-01-06 21:44:42,609 iteration 134 : loss : 0.316600, loss_ce: 0.144606
2022-01-06 21:44:44,201 iteration 135 : loss : 0.289771, loss_ce: 0.144326
2022-01-06 21:44:45,701 iteration 136 : loss : 0.302238, loss_ce: 0.167563
  2%|▌                              | 8/400 [03:32<2:55:19, 26.84s/it]2022-01-06 21:44:47,381 iteration 137 : loss : 0.293596, loss_ce: 0.129113
2022-01-06 21:44:48,870 iteration 138 : loss : 0.277526, loss_ce: 0.144720
2022-01-06 21:44:50,418 iteration 139 : loss : 0.306124, loss_ce: 0.123085
2022-01-06 21:44:51,968 iteration 140 : loss : 0.285995, loss_ce: 0.113325
2022-01-06 21:44:53,567 iteration 141 : loss : 0.307805, loss_ce: 0.147127
2022-01-06 21:44:55,181 iteration 142 : loss : 0.261689, loss_ce: 0.101604
2022-01-06 21:44:56,824 iteration 143 : loss : 0.264696, loss_ce: 0.113265
2022-01-06 21:44:58,434 iteration 144 : loss : 0.282021, loss_ce: 0.115168
2022-01-06 21:44:59,987 iteration 145 : loss : 0.271608, loss_ce: 0.114635
2022-01-06 21:45:01,584 iteration 146 : loss : 0.234145, loss_ce: 0.112439
2022-01-06 21:45:03,189 iteration 147 : loss : 0.261854, loss_ce: 0.109355
2022-01-06 21:45:04,666 iteration 148 : loss : 0.260478, loss_ce: 0.115285
2022-01-06 21:45:06,209 iteration 149 : loss : 0.308985, loss_ce: 0.140873
2022-01-06 21:45:07,722 iteration 150 : loss : 0.274492, loss_ce: 0.098979
2022-01-06 21:45:09,257 iteration 151 : loss : 0.262938, loss_ce: 0.119796
2022-01-06 21:45:10,767 iteration 152 : loss : 0.248094, loss_ce: 0.091087
2022-01-06 21:45:12,328 iteration 153 : loss : 0.265043, loss_ce: 0.115199
  2%|▋                              | 9/400 [03:59<2:54:26, 26.77s/it]2022-01-06 21:45:13,870 iteration 154 : loss : 0.284926, loss_ce: 0.128084
2022-01-06 21:45:15,416 iteration 155 : loss : 0.242176, loss_ce: 0.089279
2022-01-06 21:45:17,031 iteration 156 : loss : 0.240584, loss_ce: 0.089948
2022-01-06 21:45:18,597 iteration 157 : loss : 0.287055, loss_ce: 0.108737
2022-01-06 21:45:20,271 iteration 158 : loss : 0.266534, loss_ce: 0.120439
2022-01-06 21:45:21,721 iteration 159 : loss : 0.223335, loss_ce: 0.087865
2022-01-06 21:45:23,356 iteration 160 : loss : 0.198243, loss_ce: 0.062235
2022-01-06 21:45:24,984 iteration 161 : loss : 0.264067, loss_ce: 0.098183
2022-01-06 21:45:26,604 iteration 162 : loss : 0.239600, loss_ce: 0.111280
2022-01-06 21:45:28,164 iteration 163 : loss : 0.250422, loss_ce: 0.097795
2022-01-06 21:45:29,751 iteration 164 : loss : 0.221239, loss_ce: 0.081381
2022-01-06 21:45:31,304 iteration 165 : loss : 0.261273, loss_ce: 0.107028
2022-01-06 21:45:32,844 iteration 166 : loss : 0.231688, loss_ce: 0.090323
2022-01-06 21:45:34,402 iteration 167 : loss : 0.216037, loss_ce: 0.085079
2022-01-06 21:45:36,002 iteration 168 : loss : 0.250759, loss_ce: 0.121878
2022-01-06 21:45:37,570 iteration 169 : loss : 0.211644, loss_ce: 0.102794
2022-01-06 21:45:37,570 Training Data Eval:
2022-01-06 21:45:45,616   Average segmentation loss on training set: 0.4390
2022-01-06 21:45:45,617 Validation Data Eval:
2022-01-06 21:45:48,388   Average segmentation loss on validation set: 0.5162
2022-01-06 21:45:55,619 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 21:45:57,142 iteration 170 : loss : 0.216622, loss_ce: 0.092926
  2%|▊                             | 10/400 [04:43<3:30:11, 32.34s/it]2022-01-06 21:45:58,648 iteration 171 : loss : 0.241936, loss_ce: 0.097812
2022-01-06 21:46:00,113 iteration 172 : loss : 0.257115, loss_ce: 0.108893
2022-01-06 21:46:01,596 iteration 173 : loss : 0.210162, loss_ce: 0.092657
2022-01-06 21:46:02,986 iteration 174 : loss : 0.244361, loss_ce: 0.107377
2022-01-06 21:46:04,367 iteration 175 : loss : 0.277180, loss_ce: 0.111111
2022-01-06 21:46:05,823 iteration 176 : loss : 0.207972, loss_ce: 0.095693
2022-01-06 21:46:07,252 iteration 177 : loss : 0.235366, loss_ce: 0.087189
2022-01-06 21:46:08,692 iteration 178 : loss : 0.187532, loss_ce: 0.083523
2022-01-06 21:46:10,185 iteration 179 : loss : 0.248090, loss_ce: 0.101566
2022-01-06 21:46:11,692 iteration 180 : loss : 0.205685, loss_ce: 0.078764
2022-01-06 21:46:13,253 iteration 181 : loss : 0.195616, loss_ce: 0.082674
2022-01-06 21:46:14,754 iteration 182 : loss : 0.173584, loss_ce: 0.069847
2022-01-06 21:46:16,234 iteration 183 : loss : 0.210544, loss_ce: 0.075905
2022-01-06 21:46:17,672 iteration 184 : loss : 0.211995, loss_ce: 0.076082
2022-01-06 21:46:19,216 iteration 185 : loss : 0.271261, loss_ce: 0.096417
2022-01-06 21:46:20,760 iteration 186 : loss : 0.268700, loss_ce: 0.125433
2022-01-06 21:46:22,328 iteration 187 : loss : 0.200112, loss_ce: 0.081936
  3%|▊                             | 11/400 [05:09<3:15:28, 30.15s/it]2022-01-06 21:46:23,916 iteration 188 : loss : 0.242532, loss_ce: 0.108390
2022-01-06 21:46:25,429 iteration 189 : loss : 0.205312, loss_ce: 0.084518
2022-01-06 21:46:26,958 iteration 190 : loss : 0.230205, loss_ce: 0.083038
2022-01-06 21:46:28,443 iteration 191 : loss : 0.173317, loss_ce: 0.066778
2022-01-06 21:46:29,888 iteration 192 : loss : 0.207591, loss_ce: 0.093841
2022-01-06 21:46:31,449 iteration 193 : loss : 0.218495, loss_ce: 0.095622
2022-01-06 21:46:32,931 iteration 194 : loss : 0.260927, loss_ce: 0.083457
2022-01-06 21:46:34,398 iteration 195 : loss : 0.249377, loss_ce: 0.110441
2022-01-06 21:46:35,824 iteration 196 : loss : 0.221393, loss_ce: 0.074861
2022-01-06 21:46:37,347 iteration 197 : loss : 0.240540, loss_ce: 0.098047
2022-01-06 21:46:38,910 iteration 198 : loss : 0.205618, loss_ce: 0.087966
2022-01-06 21:46:40,409 iteration 199 : loss : 0.223724, loss_ce: 0.096291
2022-01-06 21:46:41,936 iteration 200 : loss : 0.218354, loss_ce: 0.092290
2022-01-06 21:46:43,502 iteration 201 : loss : 0.156171, loss_ce: 0.063099
2022-01-06 21:46:45,100 iteration 202 : loss : 0.192491, loss_ce: 0.091774
2022-01-06 21:46:46,613 iteration 203 : loss : 0.182310, loss_ce: 0.074873
2022-01-06 21:46:48,249 iteration 204 : loss : 0.233314, loss_ce: 0.098370
  3%|▉                             | 12/400 [05:34<3:06:38, 28.86s/it]2022-01-06 21:46:49,817 iteration 205 : loss : 0.185019, loss_ce: 0.074401
2022-01-06 21:46:51,357 iteration 206 : loss : 0.226457, loss_ce: 0.080642
2022-01-06 21:46:52,916 iteration 207 : loss : 0.194237, loss_ce: 0.081987
2022-01-06 21:46:54,423 iteration 208 : loss : 0.221385, loss_ce: 0.074093
2022-01-06 21:46:56,012 iteration 209 : loss : 0.274380, loss_ce: 0.135299
2022-01-06 21:46:57,445 iteration 210 : loss : 0.174098, loss_ce: 0.059643
2022-01-06 21:46:59,005 iteration 211 : loss : 0.225540, loss_ce: 0.102575
2022-01-06 21:47:00,541 iteration 212 : loss : 0.209769, loss_ce: 0.078217
2022-01-06 21:47:02,163 iteration 213 : loss : 0.177817, loss_ce: 0.076093
2022-01-06 21:47:03,665 iteration 214 : loss : 0.193051, loss_ce: 0.064836
2022-01-06 21:47:05,168 iteration 215 : loss : 0.245695, loss_ce: 0.093772
2022-01-06 21:47:06,823 iteration 216 : loss : 0.193836, loss_ce: 0.072141
2022-01-06 21:47:08,412 iteration 217 : loss : 0.196949, loss_ce: 0.074693
2022-01-06 21:47:09,940 iteration 218 : loss : 0.183313, loss_ce: 0.085958
2022-01-06 21:47:11,356 iteration 219 : loss : 0.183617, loss_ce: 0.077887
2022-01-06 21:47:12,891 iteration 220 : loss : 0.166505, loss_ce: 0.079996
2022-01-06 21:47:14,401 iteration 221 : loss : 0.241543, loss_ce: 0.093997
  3%|▉                             | 13/400 [06:01<3:00:52, 28.04s/it]2022-01-06 21:47:15,972 iteration 222 : loss : 0.185327, loss_ce: 0.091148
2022-01-06 21:47:17,521 iteration 223 : loss : 0.171625, loss_ce: 0.073148
2022-01-06 21:47:19,037 iteration 224 : loss : 0.315144, loss_ce: 0.161625
2022-01-06 21:47:20,631 iteration 225 : loss : 0.289849, loss_ce: 0.081186
2022-01-06 21:47:22,136 iteration 226 : loss : 0.162474, loss_ce: 0.053613
2022-01-06 21:47:23,698 iteration 227 : loss : 0.281067, loss_ce: 0.126931
2022-01-06 21:47:25,207 iteration 228 : loss : 0.207870, loss_ce: 0.080277
2022-01-06 21:47:26,683 iteration 229 : loss : 0.201285, loss_ce: 0.071428
2022-01-06 21:47:28,212 iteration 230 : loss : 0.178883, loss_ce: 0.068764
2022-01-06 21:47:29,779 iteration 231 : loss : 0.260005, loss_ce: 0.102976
2022-01-06 21:47:31,338 iteration 232 : loss : 0.170865, loss_ce: 0.071842
2022-01-06 21:47:32,810 iteration 233 : loss : 0.232773, loss_ce: 0.110739
2022-01-06 21:47:34,388 iteration 234 : loss : 0.195429, loss_ce: 0.075207
2022-01-06 21:47:35,888 iteration 235 : loss : 0.162490, loss_ce: 0.068164
2022-01-06 21:47:37,429 iteration 236 : loss : 0.193069, loss_ce: 0.071588
2022-01-06 21:47:38,922 iteration 237 : loss : 0.168660, loss_ce: 0.065425
2022-01-06 21:47:40,429 iteration 238 : loss : 0.177107, loss_ce: 0.077605
  4%|█                             | 14/400 [06:27<2:56:29, 27.43s/it]2022-01-06 21:47:41,943 iteration 239 : loss : 0.229038, loss_ce: 0.074042
2022-01-06 21:47:43,475 iteration 240 : loss : 0.179842, loss_ce: 0.081391
2022-01-06 21:47:45,020 iteration 241 : loss : 0.222267, loss_ce: 0.092081
2022-01-06 21:47:46,544 iteration 242 : loss : 0.209380, loss_ce: 0.102334
2022-01-06 21:47:48,188 iteration 243 : loss : 0.197296, loss_ce: 0.085505
2022-01-06 21:47:49,644 iteration 244 : loss : 0.244638, loss_ce: 0.084220
2022-01-06 21:47:51,183 iteration 245 : loss : 0.166201, loss_ce: 0.080784
2022-01-06 21:47:52,761 iteration 246 : loss : 0.173879, loss_ce: 0.078490
2022-01-06 21:47:54,274 iteration 247 : loss : 0.207650, loss_ce: 0.076218
2022-01-06 21:47:55,803 iteration 248 : loss : 0.175485, loss_ce: 0.061709
2022-01-06 21:47:57,316 iteration 249 : loss : 0.213925, loss_ce: 0.107442
2022-01-06 21:47:58,830 iteration 250 : loss : 0.205365, loss_ce: 0.079333
2022-01-06 21:48:00,289 iteration 251 : loss : 0.227746, loss_ce: 0.073622
2022-01-06 21:48:01,799 iteration 252 : loss : 0.145471, loss_ce: 0.053999
2022-01-06 21:48:03,340 iteration 253 : loss : 0.199709, loss_ce: 0.070874
2022-01-06 21:48:04,930 iteration 254 : loss : 0.270386, loss_ce: 0.081535
2022-01-06 21:48:04,930 Training Data Eval:
2022-01-06 21:48:12,720   Average segmentation loss on training set: 0.5009
2022-01-06 21:48:12,720 Validation Data Eval:
2022-01-06 21:48:15,409   Average segmentation loss on validation set: 0.4442
2022-01-06 21:48:21,270 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 21:48:22,699 iteration 255 : loss : 0.193905, loss_ce: 0.080127
  4%|█▏                            | 15/400 [07:09<3:24:44, 31.91s/it]2022-01-06 21:48:24,222 iteration 256 : loss : 0.189764, loss_ce: 0.076675
2022-01-06 21:48:25,720 iteration 257 : loss : 0.181073, loss_ce: 0.069280
2022-01-06 21:48:27,262 iteration 258 : loss : 0.180333, loss_ce: 0.068533
2022-01-06 21:48:28,628 iteration 259 : loss : 0.150041, loss_ce: 0.063705
2022-01-06 21:48:30,134 iteration 260 : loss : 0.169933, loss_ce: 0.068475
2022-01-06 21:48:31,593 iteration 261 : loss : 0.201659, loss_ce: 0.069286
2022-01-06 21:48:33,142 iteration 262 : loss : 0.202604, loss_ce: 0.080351
2022-01-06 21:48:34,588 iteration 263 : loss : 0.154965, loss_ce: 0.065224
2022-01-06 21:48:36,072 iteration 264 : loss : 0.211968, loss_ce: 0.100779
2022-01-06 21:48:37,482 iteration 265 : loss : 0.141542, loss_ce: 0.058052
2022-01-06 21:48:38,981 iteration 266 : loss : 0.183956, loss_ce: 0.087280
2022-01-06 21:48:40,478 iteration 267 : loss : 0.190081, loss_ce: 0.066895
2022-01-06 21:48:42,035 iteration 268 : loss : 0.229836, loss_ce: 0.105656
2022-01-06 21:48:43,783 iteration 269 : loss : 0.193497, loss_ce: 0.066916
2022-01-06 21:48:45,451 iteration 270 : loss : 0.150807, loss_ce: 0.054480
2022-01-06 21:48:46,899 iteration 271 : loss : 0.171374, loss_ce: 0.049157
2022-01-06 21:48:48,459 iteration 272 : loss : 0.172818, loss_ce: 0.074285
  4%|█▏                            | 16/400 [07:35<3:12:22, 30.06s/it]2022-01-06 21:48:50,024 iteration 273 : loss : 0.150157, loss_ce: 0.048392
2022-01-06 21:48:51,509 iteration 274 : loss : 0.147161, loss_ce: 0.052794
2022-01-06 21:48:52,996 iteration 275 : loss : 0.153595, loss_ce: 0.061186
2022-01-06 21:48:54,541 iteration 276 : loss : 0.157406, loss_ce: 0.070669
2022-01-06 21:48:56,105 iteration 277 : loss : 0.149397, loss_ce: 0.055795
2022-01-06 21:48:57,513 iteration 278 : loss : 0.189044, loss_ce: 0.063787
2022-01-06 21:48:58,983 iteration 279 : loss : 0.183576, loss_ce: 0.067318
2022-01-06 21:49:00,453 iteration 280 : loss : 0.183557, loss_ce: 0.076990
2022-01-06 21:49:01,970 iteration 281 : loss : 0.153469, loss_ce: 0.060834
2022-01-06 21:49:03,458 iteration 282 : loss : 0.218303, loss_ce: 0.097311
2022-01-06 21:49:04,931 iteration 283 : loss : 0.178402, loss_ce: 0.083842
2022-01-06 21:49:06,512 iteration 284 : loss : 0.226468, loss_ce: 0.097804
2022-01-06 21:49:07,975 iteration 285 : loss : 0.179977, loss_ce: 0.059374
2022-01-06 21:49:09,448 iteration 286 : loss : 0.162302, loss_ce: 0.068799
2022-01-06 21:49:11,021 iteration 287 : loss : 0.192604, loss_ce: 0.077710
2022-01-06 21:49:12,559 iteration 288 : loss : 0.184297, loss_ce: 0.073272
2022-01-06 21:49:14,035 iteration 289 : loss : 0.153927, loss_ce: 0.058683
  4%|█▎                            | 17/400 [08:00<3:03:14, 28.71s/it]2022-01-06 21:49:15,595 iteration 290 : loss : 0.146694, loss_ce: 0.055825
2022-01-06 21:49:17,063 iteration 291 : loss : 0.166688, loss_ce: 0.068112
2022-01-06 21:49:18,535 iteration 292 : loss : 0.155325, loss_ce: 0.060463
2022-01-06 21:49:20,011 iteration 293 : loss : 0.203964, loss_ce: 0.084805
2022-01-06 21:49:21,530 iteration 294 : loss : 0.182323, loss_ce: 0.071143
2022-01-06 21:49:23,140 iteration 295 : loss : 0.202224, loss_ce: 0.072470
2022-01-06 21:49:24,629 iteration 296 : loss : 0.142549, loss_ce: 0.045805
2022-01-06 21:49:26,148 iteration 297 : loss : 0.164673, loss_ce: 0.055168
2022-01-06 21:49:27,628 iteration 298 : loss : 0.144792, loss_ce: 0.050009
2022-01-06 21:49:29,136 iteration 299 : loss : 0.177228, loss_ce: 0.070329
2022-01-06 21:49:30,640 iteration 300 : loss : 0.185879, loss_ce: 0.062558
2022-01-06 21:49:32,145 iteration 301 : loss : 0.210090, loss_ce: 0.103633
2022-01-06 21:49:33,706 iteration 302 : loss : 0.147250, loss_ce: 0.057448
2022-01-06 21:49:35,245 iteration 303 : loss : 0.202499, loss_ce: 0.080385
2022-01-06 21:49:36,779 iteration 304 : loss : 0.190054, loss_ce: 0.080823
2022-01-06 21:49:38,344 iteration 305 : loss : 0.161759, loss_ce: 0.063176
2022-01-06 21:49:39,891 iteration 306 : loss : 0.154654, loss_ce: 0.067073
  4%|█▎                            | 18/400 [08:26<2:57:20, 27.85s/it]2022-01-06 21:49:41,461 iteration 307 : loss : 0.185342, loss_ce: 0.067825
2022-01-06 21:49:42,930 iteration 308 : loss : 0.147852, loss_ce: 0.070760
2022-01-06 21:49:44,451 iteration 309 : loss : 0.274026, loss_ce: 0.106555
2022-01-06 21:49:45,982 iteration 310 : loss : 0.178405, loss_ce: 0.076688
2022-01-06 21:49:47,440 iteration 311 : loss : 0.199235, loss_ce: 0.062000
2022-01-06 21:49:48,924 iteration 312 : loss : 0.134249, loss_ce: 0.063950
2022-01-06 21:49:50,424 iteration 313 : loss : 0.205630, loss_ce: 0.098434
2022-01-06 21:49:51,868 iteration 314 : loss : 0.138399, loss_ce: 0.062249
2022-01-06 21:49:53,363 iteration 315 : loss : 0.169732, loss_ce: 0.064112
2022-01-06 21:49:54,877 iteration 316 : loss : 0.182031, loss_ce: 0.062974
2022-01-06 21:49:56,388 iteration 317 : loss : 0.217478, loss_ce: 0.088578
2022-01-06 21:49:57,932 iteration 318 : loss : 0.146226, loss_ce: 0.059327
2022-01-06 21:49:59,422 iteration 319 : loss : 0.187919, loss_ce: 0.080298
2022-01-06 21:50:01,050 iteration 320 : loss : 0.189656, loss_ce: 0.074385
2022-01-06 21:50:02,640 iteration 321 : loss : 0.152177, loss_ce: 0.052414
2022-01-06 21:50:04,200 iteration 322 : loss : 0.216506, loss_ce: 0.096326
2022-01-06 21:50:05,812 iteration 323 : loss : 0.179961, loss_ce: 0.059575
  5%|█▍                            | 19/400 [08:52<2:53:11, 27.27s/it]2022-01-06 21:50:07,419 iteration 324 : loss : 0.147062, loss_ce: 0.054088
2022-01-06 21:50:08,973 iteration 325 : loss : 0.202854, loss_ce: 0.074556
2022-01-06 21:50:10,555 iteration 326 : loss : 0.138223, loss_ce: 0.059980
2022-01-06 21:50:12,190 iteration 327 : loss : 0.190159, loss_ce: 0.076838
2022-01-06 21:50:13,710 iteration 328 : loss : 0.172773, loss_ce: 0.067132
2022-01-06 21:50:15,272 iteration 329 : loss : 0.187666, loss_ce: 0.086124
2022-01-06 21:50:16,834 iteration 330 : loss : 0.167024, loss_ce: 0.070623
2022-01-06 21:50:18,313 iteration 331 : loss : 0.173252, loss_ce: 0.075387
2022-01-06 21:50:19,819 iteration 332 : loss : 0.146728, loss_ce: 0.048616
2022-01-06 21:50:21,362 iteration 333 : loss : 0.130174, loss_ce: 0.056592
2022-01-06 21:50:22,882 iteration 334 : loss : 0.147343, loss_ce: 0.061079
2022-01-06 21:50:24,378 iteration 335 : loss : 0.219719, loss_ce: 0.080620
2022-01-06 21:50:25,950 iteration 336 : loss : 0.127765, loss_ce: 0.046155
2022-01-06 21:50:27,500 iteration 337 : loss : 0.139868, loss_ce: 0.045503
2022-01-06 21:50:28,979 iteration 338 : loss : 0.209507, loss_ce: 0.102871
2022-01-06 21:50:30,472 iteration 339 : loss : 0.181325, loss_ce: 0.060834
2022-01-06 21:50:30,472 Training Data Eval:
2022-01-06 21:50:38,249   Average segmentation loss on training set: 0.4480
2022-01-06 21:50:38,249 Validation Data Eval:
2022-01-06 21:50:40,926   Average segmentation loss on validation set: 0.5308
2022-01-06 21:50:42,486 iteration 340 : loss : 0.170569, loss_ce: 0.070062
  5%|█▌                            | 20/400 [09:29<3:10:34, 30.09s/it]2022-01-06 21:50:44,103 iteration 341 : loss : 0.142131, loss_ce: 0.060359
2022-01-06 21:50:45,646 iteration 342 : loss : 0.146454, loss_ce: 0.054342
2022-01-06 21:50:47,158 iteration 343 : loss : 0.210601, loss_ce: 0.086684
2022-01-06 21:50:48,708 iteration 344 : loss : 0.199048, loss_ce: 0.086223
2022-01-06 21:50:50,212 iteration 345 : loss : 0.156429, loss_ce: 0.047843
2022-01-06 21:50:51,770 iteration 346 : loss : 0.180102, loss_ce: 0.070256
2022-01-06 21:50:53,222 iteration 347 : loss : 0.180015, loss_ce: 0.079880
2022-01-06 21:50:54,754 iteration 348 : loss : 0.192256, loss_ce: 0.071849
2022-01-06 21:50:56,377 iteration 349 : loss : 0.170357, loss_ce: 0.087785
2022-01-06 21:50:57,874 iteration 350 : loss : 0.174670, loss_ce: 0.069358
2022-01-06 21:50:59,430 iteration 351 : loss : 0.160008, loss_ce: 0.071462
2022-01-06 21:51:00,968 iteration 352 : loss : 0.174823, loss_ce: 0.068174
2022-01-06 21:51:02,442 iteration 353 : loss : 0.163176, loss_ce: 0.059123
2022-01-06 21:51:03,981 iteration 354 : loss : 0.168947, loss_ce: 0.053904
2022-01-06 21:51:05,573 iteration 355 : loss : 0.169661, loss_ce: 0.066494
2022-01-06 21:51:07,217 iteration 356 : loss : 0.181074, loss_ce: 0.073527
2022-01-06 21:51:08,693 iteration 357 : loss : 0.149835, loss_ce: 0.056035
  5%|█▌                            | 21/400 [09:55<3:02:42, 28.93s/it]2022-01-06 21:51:10,316 iteration 358 : loss : 0.201154, loss_ce: 0.084688
2022-01-06 21:51:11,912 iteration 359 : loss : 0.182336, loss_ce: 0.068886
2022-01-06 21:51:13,372 iteration 360 : loss : 0.143798, loss_ce: 0.048996
2022-01-06 21:51:14,925 iteration 361 : loss : 0.196923, loss_ce: 0.076551
2022-01-06 21:51:16,526 iteration 362 : loss : 0.155981, loss_ce: 0.048110
2022-01-06 21:51:18,107 iteration 363 : loss : 0.178700, loss_ce: 0.069414
2022-01-06 21:51:19,636 iteration 364 : loss : 0.160198, loss_ce: 0.060695
2022-01-06 21:51:21,059 iteration 365 : loss : 0.151519, loss_ce: 0.065892
2022-01-06 21:51:22,599 iteration 366 : loss : 0.195861, loss_ce: 0.086446
2022-01-06 21:51:24,046 iteration 367 : loss : 0.147582, loss_ce: 0.057074
2022-01-06 21:51:25,595 iteration 368 : loss : 0.184714, loss_ce: 0.063240
2022-01-06 21:51:27,173 iteration 369 : loss : 0.212066, loss_ce: 0.074527
2022-01-06 21:51:28,736 iteration 370 : loss : 0.147865, loss_ce: 0.069015
2022-01-06 21:51:30,265 iteration 371 : loss : 0.201318, loss_ce: 0.064549
2022-01-06 21:51:31,795 iteration 372 : loss : 0.247951, loss_ce: 0.129010
2022-01-06 21:51:33,256 iteration 373 : loss : 0.163626, loss_ce: 0.055887
2022-01-06 21:51:34,752 iteration 374 : loss : 0.148703, loss_ce: 0.058968
  6%|█▋                            | 22/400 [10:21<2:56:50, 28.07s/it]2022-01-06 21:51:36,365 iteration 375 : loss : 0.232593, loss_ce: 0.086621
2022-01-06 21:51:37,814 iteration 376 : loss : 0.098287, loss_ce: 0.037076
2022-01-06 21:51:39,301 iteration 377 : loss : 0.138112, loss_ce: 0.041924
2022-01-06 21:51:40,862 iteration 378 : loss : 0.152761, loss_ce: 0.060227
2022-01-06 21:51:42,273 iteration 379 : loss : 0.134284, loss_ce: 0.049881
2022-01-06 21:51:43,832 iteration 380 : loss : 0.117610, loss_ce: 0.041893
2022-01-06 21:51:45,330 iteration 381 : loss : 0.111143, loss_ce: 0.040196
2022-01-06 21:51:46,772 iteration 382 : loss : 0.177106, loss_ce: 0.075882
2022-01-06 21:51:48,296 iteration 383 : loss : 0.209036, loss_ce: 0.096071
2022-01-06 21:51:49,829 iteration 384 : loss : 0.150231, loss_ce: 0.064948
2022-01-06 21:51:51,351 iteration 385 : loss : 0.124188, loss_ce: 0.054938
2022-01-06 21:51:52,879 iteration 386 : loss : 0.173642, loss_ce: 0.061029
2022-01-06 21:51:54,400 iteration 387 : loss : 0.202077, loss_ce: 0.066270
2022-01-06 21:51:55,931 iteration 388 : loss : 0.184850, loss_ce: 0.075864
2022-01-06 21:51:57,472 iteration 389 : loss : 0.225500, loss_ce: 0.096185
2022-01-06 21:51:58,971 iteration 390 : loss : 0.132545, loss_ce: 0.050439
2022-01-06 21:52:00,466 iteration 391 : loss : 0.183902, loss_ce: 0.094188
  6%|█▋                            | 23/400 [10:47<2:51:53, 27.36s/it]2022-01-06 21:52:02,052 iteration 392 : loss : 0.152223, loss_ce: 0.072385
2022-01-06 21:52:03,529 iteration 393 : loss : 0.147408, loss_ce: 0.053793
2022-01-06 21:52:05,098 iteration 394 : loss : 0.132918, loss_ce: 0.045488
2022-01-06 21:52:06,643 iteration 395 : loss : 0.167269, loss_ce: 0.066007
2022-01-06 21:52:08,147 iteration 396 : loss : 0.143924, loss_ce: 0.050507
2022-01-06 21:52:09,682 iteration 397 : loss : 0.155009, loss_ce: 0.058857
2022-01-06 21:52:11,238 iteration 398 : loss : 0.114962, loss_ce: 0.044106
2022-01-06 21:52:12,674 iteration 399 : loss : 0.145805, loss_ce: 0.057711
2022-01-06 21:52:14,206 iteration 400 : loss : 0.145279, loss_ce: 0.049921
2022-01-06 21:52:15,784 iteration 401 : loss : 0.167936, loss_ce: 0.064994
2022-01-06 21:52:17,276 iteration 402 : loss : 0.109055, loss_ce: 0.047390
2022-01-06 21:52:18,768 iteration 403 : loss : 0.092717, loss_ce: 0.034729
2022-01-06 21:52:20,303 iteration 404 : loss : 0.193481, loss_ce: 0.084922
2022-01-06 21:52:21,881 iteration 405 : loss : 0.166197, loss_ce: 0.067688
2022-01-06 21:52:23,374 iteration 406 : loss : 0.144684, loss_ce: 0.062978
2022-01-06 21:52:24,824 iteration 407 : loss : 0.145041, loss_ce: 0.053874
2022-01-06 21:52:26,323 iteration 408 : loss : 0.159327, loss_ce: 0.057609
  6%|█▊                            | 24/400 [11:13<2:48:38, 26.91s/it]2022-01-06 21:52:27,905 iteration 409 : loss : 0.126879, loss_ce: 0.061508
2022-01-06 21:52:29,428 iteration 410 : loss : 0.139754, loss_ce: 0.053349
2022-01-06 21:52:30,920 iteration 411 : loss : 0.238818, loss_ce: 0.071240
2022-01-06 21:52:32,341 iteration 412 : loss : 0.131518, loss_ce: 0.049700
2022-01-06 21:52:33,842 iteration 413 : loss : 0.131885, loss_ce: 0.047235
2022-01-06 21:52:35,300 iteration 414 : loss : 0.193901, loss_ce: 0.094605
2022-01-06 21:52:36,713 iteration 415 : loss : 0.132812, loss_ce: 0.046912
2022-01-06 21:52:38,217 iteration 416 : loss : 0.190493, loss_ce: 0.093849
2022-01-06 21:52:39,690 iteration 417 : loss : 0.255622, loss_ce: 0.099806
2022-01-06 21:52:41,260 iteration 418 : loss : 0.140679, loss_ce: 0.056025
2022-01-06 21:52:42,756 iteration 419 : loss : 0.140306, loss_ce: 0.072650
2022-01-06 21:52:44,314 iteration 420 : loss : 0.136793, loss_ce: 0.054359
2022-01-06 21:52:45,807 iteration 421 : loss : 0.165439, loss_ce: 0.067906
2022-01-06 21:52:47,292 iteration 422 : loss : 0.162939, loss_ce: 0.062003
2022-01-06 21:52:48,793 iteration 423 : loss : 0.157201, loss_ce: 0.055018
2022-01-06 21:52:50,345 iteration 424 : loss : 0.179851, loss_ce: 0.078983
2022-01-06 21:52:50,346 Training Data Eval:
2022-01-06 21:52:58,208   Average segmentation loss on training set: 0.2173
2022-01-06 21:52:58,208 Validation Data Eval:
2022-01-06 21:53:00,904   Average segmentation loss on validation set: 0.2354
2022-01-06 21:53:06,605 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 21:53:08,063 iteration 425 : loss : 0.171597, loss_ce: 0.067035
  6%|█▉                            | 25/400 [11:54<3:15:59, 31.36s/it]2022-01-06 21:53:09,507 iteration 426 : loss : 0.152649, loss_ce: 0.053414
2022-01-06 21:53:10,984 iteration 427 : loss : 0.127989, loss_ce: 0.045720
2022-01-06 21:53:12,429 iteration 428 : loss : 0.162507, loss_ce: 0.058556
2022-01-06 21:53:13,813 iteration 429 : loss : 0.201219, loss_ce: 0.093583
2022-01-06 21:53:15,194 iteration 430 : loss : 0.147277, loss_ce: 0.050448
2022-01-06 21:53:16,598 iteration 431 : loss : 0.211193, loss_ce: 0.080791
2022-01-06 21:53:18,057 iteration 432 : loss : 0.098370, loss_ce: 0.040847
2022-01-06 21:53:19,554 iteration 433 : loss : 0.114052, loss_ce: 0.045320
2022-01-06 21:53:20,996 iteration 434 : loss : 0.121440, loss_ce: 0.046981
2022-01-06 21:53:22,406 iteration 435 : loss : 0.135207, loss_ce: 0.057196
2022-01-06 21:53:23,892 iteration 436 : loss : 0.113366, loss_ce: 0.052371
2022-01-06 21:53:25,396 iteration 437 : loss : 0.174372, loss_ce: 0.091838
2022-01-06 21:53:26,857 iteration 438 : loss : 0.139389, loss_ce: 0.043867
2022-01-06 21:53:28,374 iteration 439 : loss : 0.185470, loss_ce: 0.062521
2022-01-06 21:53:29,866 iteration 440 : loss : 0.142985, loss_ce: 0.055452
2022-01-06 21:53:31,420 iteration 441 : loss : 0.139139, loss_ce: 0.049338
2022-01-06 21:53:32,944 iteration 442 : loss : 0.144192, loss_ce: 0.067343
  6%|█▉                            | 26/400 [12:19<3:03:21, 29.42s/it]2022-01-06 21:53:34,481 iteration 443 : loss : 0.145937, loss_ce: 0.065980
2022-01-06 21:53:35,938 iteration 444 : loss : 0.160749, loss_ce: 0.060060
2022-01-06 21:53:37,435 iteration 445 : loss : 0.191128, loss_ce: 0.067669
2022-01-06 21:53:39,063 iteration 446 : loss : 0.126891, loss_ce: 0.050424
2022-01-06 21:53:40,623 iteration 447 : loss : 0.117069, loss_ce: 0.058626
2022-01-06 21:53:42,186 iteration 448 : loss : 0.255348, loss_ce: 0.079894
2022-01-06 21:53:43,719 iteration 449 : loss : 0.132462, loss_ce: 0.059133
2022-01-06 21:53:45,229 iteration 450 : loss : 0.140235, loss_ce: 0.053071
2022-01-06 21:53:46,835 iteration 451 : loss : 0.122267, loss_ce: 0.054247
2022-01-06 21:53:48,385 iteration 452 : loss : 0.125885, loss_ce: 0.050411
2022-01-06 21:53:49,915 iteration 453 : loss : 0.161902, loss_ce: 0.069005
2022-01-06 21:53:51,465 iteration 454 : loss : 0.156661, loss_ce: 0.057107
2022-01-06 21:53:53,056 iteration 455 : loss : 0.125755, loss_ce: 0.050610
2022-01-06 21:53:54,611 iteration 456 : loss : 0.118800, loss_ce: 0.038587
2022-01-06 21:53:56,159 iteration 457 : loss : 0.145437, loss_ce: 0.049732
2022-01-06 21:53:57,785 iteration 458 : loss : 0.146963, loss_ce: 0.064325
2022-01-06 21:53:59,272 iteration 459 : loss : 0.103912, loss_ce: 0.035429
  7%|██                            | 27/400 [12:46<2:57:05, 28.49s/it]2022-01-06 21:54:00,879 iteration 460 : loss : 0.135447, loss_ce: 0.060199
2022-01-06 21:54:02,426 iteration 461 : loss : 0.110326, loss_ce: 0.046875
2022-01-06 21:54:03,947 iteration 462 : loss : 0.109627, loss_ce: 0.046837
2022-01-06 21:54:05,540 iteration 463 : loss : 0.143758, loss_ce: 0.072057
2022-01-06 21:54:07,082 iteration 464 : loss : 0.118146, loss_ce: 0.045322
2022-01-06 21:54:08,582 iteration 465 : loss : 0.134191, loss_ce: 0.040149
2022-01-06 21:54:10,122 iteration 466 : loss : 0.152868, loss_ce: 0.059851
2022-01-06 21:54:11,652 iteration 467 : loss : 0.125684, loss_ce: 0.046775
2022-01-06 21:54:13,298 iteration 468 : loss : 0.132979, loss_ce: 0.051333
2022-01-06 21:54:14,819 iteration 469 : loss : 0.114751, loss_ce: 0.039295
2022-01-06 21:54:16,358 iteration 470 : loss : 0.138428, loss_ce: 0.051762
2022-01-06 21:54:17,886 iteration 471 : loss : 0.129606, loss_ce: 0.040873
2022-01-06 21:54:19,465 iteration 472 : loss : 0.153257, loss_ce: 0.060998
2022-01-06 21:54:21,109 iteration 473 : loss : 0.194689, loss_ce: 0.084185
2022-01-06 21:54:22,706 iteration 474 : loss : 0.197146, loss_ce: 0.082990
2022-01-06 21:54:24,207 iteration 475 : loss : 0.111938, loss_ce: 0.047983
2022-01-06 21:54:25,776 iteration 476 : loss : 0.138734, loss_ce: 0.066801
  7%|██                            | 28/400 [13:12<2:52:56, 27.89s/it]2022-01-06 21:54:27,342 iteration 477 : loss : 0.171881, loss_ce: 0.060686
2022-01-06 21:54:28,894 iteration 478 : loss : 0.142449, loss_ce: 0.054906
2022-01-06 21:54:30,399 iteration 479 : loss : 0.179910, loss_ce: 0.077147
2022-01-06 21:54:31,939 iteration 480 : loss : 0.126952, loss_ce: 0.059783
2022-01-06 21:54:33,400 iteration 481 : loss : 0.178586, loss_ce: 0.081837
2022-01-06 21:54:34,946 iteration 482 : loss : 0.194740, loss_ce: 0.061340
2022-01-06 21:54:36,515 iteration 483 : loss : 0.176682, loss_ce: 0.063069
2022-01-06 21:54:38,098 iteration 484 : loss : 0.177923, loss_ce: 0.079782
2022-01-06 21:54:39,623 iteration 485 : loss : 0.133020, loss_ce: 0.056213
2022-01-06 21:54:41,137 iteration 486 : loss : 0.176951, loss_ce: 0.058187
2022-01-06 21:54:42,734 iteration 487 : loss : 0.111562, loss_ce: 0.044509
2022-01-06 21:54:44,226 iteration 488 : loss : 0.125229, loss_ce: 0.047902
2022-01-06 21:54:45,772 iteration 489 : loss : 0.141343, loss_ce: 0.059903
2022-01-06 21:54:47,212 iteration 490 : loss : 0.154443, loss_ce: 0.057330
2022-01-06 21:54:48,763 iteration 491 : loss : 0.140534, loss_ce: 0.058231
2022-01-06 21:54:50,277 iteration 492 : loss : 0.106146, loss_ce: 0.050543
2022-01-06 21:54:51,831 iteration 493 : loss : 0.159102, loss_ce: 0.071443
  7%|██▏                           | 29/400 [13:38<2:49:04, 27.34s/it]2022-01-06 21:54:53,402 iteration 494 : loss : 0.140281, loss_ce: 0.062280
2022-01-06 21:54:54,941 iteration 495 : loss : 0.170006, loss_ce: 0.063236
2022-01-06 21:54:56,445 iteration 496 : loss : 0.158284, loss_ce: 0.069612
2022-01-06 21:54:57,987 iteration 497 : loss : 0.149852, loss_ce: 0.070506
2022-01-06 21:54:59,560 iteration 498 : loss : 0.121035, loss_ce: 0.056376
2022-01-06 21:55:01,012 iteration 499 : loss : 0.098994, loss_ce: 0.040689
2022-01-06 21:55:02,487 iteration 500 : loss : 0.143540, loss_ce: 0.053913
2022-01-06 21:55:03,962 iteration 501 : loss : 0.140737, loss_ce: 0.056550
2022-01-06 21:55:05,532 iteration 502 : loss : 0.131893, loss_ce: 0.050784
2022-01-06 21:55:07,017 iteration 503 : loss : 0.176191, loss_ce: 0.055641
2022-01-06 21:55:08,553 iteration 504 : loss : 0.115792, loss_ce: 0.035340
2022-01-06 21:55:10,045 iteration 505 : loss : 0.113270, loss_ce: 0.038392
2022-01-06 21:55:11,632 iteration 506 : loss : 0.126166, loss_ce: 0.048482
2022-01-06 21:55:13,141 iteration 507 : loss : 0.187758, loss_ce: 0.076380
2022-01-06 21:55:14,639 iteration 508 : loss : 0.172014, loss_ce: 0.077784
2022-01-06 21:55:16,192 iteration 509 : loss : 0.114249, loss_ce: 0.043320
2022-01-06 21:55:16,192 Training Data Eval:
2022-01-06 21:55:23,983   Average segmentation loss on training set: 0.1151
2022-01-06 21:55:23,983 Validation Data Eval:
2022-01-06 21:55:26,683   Average segmentation loss on validation set: 0.1741
2022-01-06 21:55:32,394 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 21:55:33,905 iteration 510 : loss : 0.128723, loss_ce: 0.056306
  8%|██▎                           | 30/400 [14:20<3:15:50, 31.76s/it]2022-01-06 21:55:35,411 iteration 511 : loss : 0.191068, loss_ce: 0.091668
2022-01-06 21:55:36,805 iteration 512 : loss : 0.187717, loss_ce: 0.084191
2022-01-06 21:55:38,215 iteration 513 : loss : 0.107736, loss_ce: 0.035776
2022-01-06 21:55:39,646 iteration 514 : loss : 0.121805, loss_ce: 0.046391
2022-01-06 21:55:41,060 iteration 515 : loss : 0.157345, loss_ce: 0.063317
2022-01-06 21:55:42,520 iteration 516 : loss : 0.126367, loss_ce: 0.048595
2022-01-06 21:55:44,006 iteration 517 : loss : 0.126500, loss_ce: 0.055437
2022-01-06 21:55:45,398 iteration 518 : loss : 0.144654, loss_ce: 0.051622
2022-01-06 21:55:46,837 iteration 519 : loss : 0.132907, loss_ce: 0.053033
2022-01-06 21:55:48,223 iteration 520 : loss : 0.119493, loss_ce: 0.045088
2022-01-06 21:55:49,669 iteration 521 : loss : 0.094987, loss_ce: 0.032222
2022-01-06 21:55:51,172 iteration 522 : loss : 0.106592, loss_ce: 0.037273
2022-01-06 21:55:52,719 iteration 523 : loss : 0.136761, loss_ce: 0.056014
2022-01-06 21:55:54,298 iteration 524 : loss : 0.124814, loss_ce: 0.049656
2022-01-06 21:55:55,798 iteration 525 : loss : 0.205400, loss_ce: 0.102962
2022-01-06 21:55:57,296 iteration 526 : loss : 0.151965, loss_ce: 0.071776
2022-01-06 21:55:58,805 iteration 527 : loss : 0.127268, loss_ce: 0.049066
  8%|██▎                           | 31/400 [14:45<3:02:40, 29.70s/it]2022-01-06 21:56:00,340 iteration 528 : loss : 0.126557, loss_ce: 0.052609
2022-01-06 21:56:01,862 iteration 529 : loss : 0.112278, loss_ce: 0.037800
2022-01-06 21:56:03,351 iteration 530 : loss : 0.164732, loss_ce: 0.067517
2022-01-06 21:56:04,830 iteration 531 : loss : 0.150206, loss_ce: 0.045489
2022-01-06 21:56:06,336 iteration 532 : loss : 0.118816, loss_ce: 0.058113
2022-01-06 21:56:07,965 iteration 533 : loss : 0.077808, loss_ce: 0.032838
2022-01-06 21:56:09,448 iteration 534 : loss : 0.119498, loss_ce: 0.052187
2022-01-06 21:56:11,019 iteration 535 : loss : 0.102317, loss_ce: 0.039880
2022-01-06 21:56:12,626 iteration 536 : loss : 0.165963, loss_ce: 0.077931
2022-01-06 21:56:14,234 iteration 537 : loss : 0.142825, loss_ce: 0.045298
2022-01-06 21:56:15,784 iteration 538 : loss : 0.119505, loss_ce: 0.051307
2022-01-06 21:56:17,294 iteration 539 : loss : 0.118475, loss_ce: 0.045017
2022-01-06 21:56:18,835 iteration 540 : loss : 0.121404, loss_ce: 0.044898
2022-01-06 21:56:20,338 iteration 541 : loss : 0.096864, loss_ce: 0.043807
2022-01-06 21:56:21,857 iteration 542 : loss : 0.127197, loss_ce: 0.053620
2022-01-06 21:56:23,325 iteration 543 : loss : 0.094390, loss_ce: 0.036436
2022-01-06 21:56:24,819 iteration 544 : loss : 0.108686, loss_ce: 0.038112
  8%|██▍                           | 32/400 [15:11<2:55:23, 28.60s/it]2022-01-06 21:56:26,393 iteration 545 : loss : 0.088823, loss_ce: 0.040755
2022-01-06 21:56:27,888 iteration 546 : loss : 0.103440, loss_ce: 0.035198
2022-01-06 21:56:29,400 iteration 547 : loss : 0.116848, loss_ce: 0.043920
2022-01-06 21:56:30,897 iteration 548 : loss : 0.172617, loss_ce: 0.059287
2022-01-06 21:56:32,415 iteration 549 : loss : 0.107031, loss_ce: 0.043319
2022-01-06 21:56:33,916 iteration 550 : loss : 0.121065, loss_ce: 0.057367
2022-01-06 21:56:35,415 iteration 551 : loss : 0.099934, loss_ce: 0.049784
2022-01-06 21:56:36,929 iteration 552 : loss : 0.097863, loss_ce: 0.037047
2022-01-06 21:56:38,471 iteration 553 : loss : 0.128658, loss_ce: 0.047917
2022-01-06 21:56:39,914 iteration 554 : loss : 0.110166, loss_ce: 0.052365
2022-01-06 21:56:41,410 iteration 555 : loss : 0.128377, loss_ce: 0.039679
2022-01-06 21:56:42,951 iteration 556 : loss : 0.114506, loss_ce: 0.048572
2022-01-06 21:56:44,385 iteration 557 : loss : 0.140557, loss_ce: 0.048531
2022-01-06 21:56:45,921 iteration 558 : loss : 0.101211, loss_ce: 0.031138
2022-01-06 21:56:47,409 iteration 559 : loss : 0.136064, loss_ce: 0.055728
2022-01-06 21:56:48,927 iteration 560 : loss : 0.132759, loss_ce: 0.030336
2022-01-06 21:56:50,470 iteration 561 : loss : 0.186798, loss_ce: 0.091038
  8%|██▍                           | 33/400 [15:37<2:49:31, 27.71s/it]2022-01-06 21:56:52,141 iteration 562 : loss : 0.154565, loss_ce: 0.074493
2022-01-06 21:56:53,561 iteration 563 : loss : 0.107169, loss_ce: 0.041978
2022-01-06 21:56:55,088 iteration 564 : loss : 0.143853, loss_ce: 0.067525
2022-01-06 21:56:56,612 iteration 565 : loss : 0.142415, loss_ce: 0.063451
2022-01-06 21:56:58,169 iteration 566 : loss : 0.091627, loss_ce: 0.041454
2022-01-06 21:56:59,653 iteration 567 : loss : 0.134357, loss_ce: 0.046143
2022-01-06 21:57:01,123 iteration 568 : loss : 0.179855, loss_ce: 0.060880
2022-01-06 21:57:02,631 iteration 569 : loss : 0.135069, loss_ce: 0.050676
2022-01-06 21:57:04,162 iteration 570 : loss : 0.077205, loss_ce: 0.034672
2022-01-06 21:57:05,635 iteration 571 : loss : 0.217829, loss_ce: 0.094020
2022-01-06 21:57:07,146 iteration 572 : loss : 0.118892, loss_ce: 0.039522
2022-01-06 21:57:08,613 iteration 573 : loss : 0.094555, loss_ce: 0.034070
2022-01-06 21:57:10,116 iteration 574 : loss : 0.137349, loss_ce: 0.046571
2022-01-06 21:57:11,666 iteration 575 : loss : 0.130694, loss_ce: 0.037743
2022-01-06 21:57:13,199 iteration 576 : loss : 0.138886, loss_ce: 0.058118
2022-01-06 21:57:14,694 iteration 577 : loss : 0.109177, loss_ce: 0.044720
2022-01-06 21:57:16,140 iteration 578 : loss : 0.133211, loss_ce: 0.048851
  8%|██▌                           | 34/400 [16:02<2:45:18, 27.10s/it]2022-01-06 21:57:17,848 iteration 579 : loss : 0.115327, loss_ce: 0.051023
2022-01-06 21:57:19,360 iteration 580 : loss : 0.136016, loss_ce: 0.048643
2022-01-06 21:57:20,905 iteration 581 : loss : 0.109066, loss_ce: 0.035584
2022-01-06 21:57:22,387 iteration 582 : loss : 0.115450, loss_ce: 0.043442
2022-01-06 21:57:23,886 iteration 583 : loss : 0.206500, loss_ce: 0.075444
2022-01-06 21:57:25,357 iteration 584 : loss : 0.078822, loss_ce: 0.034725
2022-01-06 21:57:26,893 iteration 585 : loss : 0.113686, loss_ce: 0.045344
2022-01-06 21:57:28,351 iteration 586 : loss : 0.149615, loss_ce: 0.056888
2022-01-06 21:57:29,812 iteration 587 : loss : 0.105057, loss_ce: 0.045027
2022-01-06 21:57:31,347 iteration 588 : loss : 0.111470, loss_ce: 0.040073
2022-01-06 21:57:32,846 iteration 589 : loss : 0.110537, loss_ce: 0.041594
2022-01-06 21:57:34,364 iteration 590 : loss : 0.113304, loss_ce: 0.038899
2022-01-06 21:57:35,893 iteration 591 : loss : 0.124743, loss_ce: 0.047694
2022-01-06 21:57:37,416 iteration 592 : loss : 0.108485, loss_ce: 0.042070
2022-01-06 21:57:38,965 iteration 593 : loss : 0.085220, loss_ce: 0.029500
2022-01-06 21:57:40,497 iteration 594 : loss : 0.106188, loss_ce: 0.044313
2022-01-06 21:57:40,497 Training Data Eval:
2022-01-06 21:57:48,285   Average segmentation loss on training set: 0.3717
2022-01-06 21:57:48,285 Validation Data Eval:
2022-01-06 21:57:50,968   Average segmentation loss on validation set: 0.4757
2022-01-06 21:57:52,541 iteration 595 : loss : 0.139146, loss_ce: 0.064244
  9%|██▋                           | 35/400 [16:39<3:01:49, 29.89s/it]2022-01-06 21:57:54,114 iteration 596 : loss : 0.096274, loss_ce: 0.037941
2022-01-06 21:57:55,590 iteration 597 : loss : 0.127270, loss_ce: 0.042481
2022-01-06 21:57:57,064 iteration 598 : loss : 0.090650, loss_ce: 0.032450
2022-01-06 21:57:58,560 iteration 599 : loss : 0.101630, loss_ce: 0.038304
2022-01-06 21:58:00,052 iteration 600 : loss : 0.106975, loss_ce: 0.048773
2022-01-06 21:58:01,559 iteration 601 : loss : 0.121273, loss_ce: 0.046378
2022-01-06 21:58:03,084 iteration 602 : loss : 0.093759, loss_ce: 0.037210
2022-01-06 21:58:04,526 iteration 603 : loss : 0.136020, loss_ce: 0.046792
2022-01-06 21:58:05,983 iteration 604 : loss : 0.110119, loss_ce: 0.042272
2022-01-06 21:58:07,421 iteration 605 : loss : 0.101064, loss_ce: 0.034776
2022-01-06 21:58:09,036 iteration 606 : loss : 0.104852, loss_ce: 0.042777
2022-01-06 21:58:10,524 iteration 607 : loss : 0.163697, loss_ce: 0.065959
2022-01-06 21:58:12,013 iteration 608 : loss : 0.121873, loss_ce: 0.053192
2022-01-06 21:58:13,436 iteration 609 : loss : 0.093011, loss_ce: 0.038651
2022-01-06 21:58:14,967 iteration 610 : loss : 0.102254, loss_ce: 0.042192
2022-01-06 21:58:16,556 iteration 611 : loss : 0.117959, loss_ce: 0.054515
2022-01-06 21:58:18,071 iteration 612 : loss : 0.107014, loss_ce: 0.037230
  9%|██▋                           | 36/400 [17:04<2:53:24, 28.58s/it]2022-01-06 21:58:19,655 iteration 613 : loss : 0.090211, loss_ce: 0.034655
2022-01-06 21:58:21,123 iteration 614 : loss : 0.103317, loss_ce: 0.035464
2022-01-06 21:58:22,581 iteration 615 : loss : 0.168288, loss_ce: 0.080164
2022-01-06 21:58:24,151 iteration 616 : loss : 0.101640, loss_ce: 0.043075
2022-01-06 21:58:25,606 iteration 617 : loss : 0.114495, loss_ce: 0.050592
2022-01-06 21:58:27,111 iteration 618 : loss : 0.172383, loss_ce: 0.048050
2022-01-06 21:58:28,608 iteration 619 : loss : 0.206611, loss_ce: 0.090123
2022-01-06 21:58:30,205 iteration 620 : loss : 0.197418, loss_ce: 0.065739
2022-01-06 21:58:31,734 iteration 621 : loss : 0.087361, loss_ce: 0.035997
2022-01-06 21:58:33,391 iteration 622 : loss : 0.124077, loss_ce: 0.047461
2022-01-06 21:58:35,038 iteration 623 : loss : 0.140308, loss_ce: 0.079437
2022-01-06 21:58:36,597 iteration 624 : loss : 0.105408, loss_ce: 0.034915
2022-01-06 21:58:38,112 iteration 625 : loss : 0.106507, loss_ce: 0.043583
2022-01-06 21:58:39,715 iteration 626 : loss : 0.115214, loss_ce: 0.053018
2022-01-06 21:58:41,312 iteration 627 : loss : 0.149871, loss_ce: 0.050008
2022-01-06 21:58:42,946 iteration 628 : loss : 0.103163, loss_ce: 0.040256
2022-01-06 21:58:44,473 iteration 629 : loss : 0.088985, loss_ce: 0.035188
  9%|██▊                           | 37/400 [17:31<2:48:57, 27.93s/it]2022-01-06 21:58:46,082 iteration 630 : loss : 0.077190, loss_ce: 0.032748
2022-01-06 21:58:47,587 iteration 631 : loss : 0.063565, loss_ce: 0.027265
2022-01-06 21:58:49,097 iteration 632 : loss : 0.117180, loss_ce: 0.046194
2022-01-06 21:58:50,703 iteration 633 : loss : 0.080918, loss_ce: 0.031470
2022-01-06 21:58:52,256 iteration 634 : loss : 0.141937, loss_ce: 0.049348
2022-01-06 21:58:53,903 iteration 635 : loss : 0.107358, loss_ce: 0.044685
2022-01-06 21:58:55,483 iteration 636 : loss : 0.103581, loss_ce: 0.042553
2022-01-06 21:58:56,980 iteration 637 : loss : 0.120505, loss_ce: 0.051148
2022-01-06 21:58:58,615 iteration 638 : loss : 0.159708, loss_ce: 0.053125
2022-01-06 21:59:00,142 iteration 639 : loss : 0.096375, loss_ce: 0.040888
2022-01-06 21:59:01,643 iteration 640 : loss : 0.136384, loss_ce: 0.038175
2022-01-06 21:59:03,154 iteration 641 : loss : 0.096663, loss_ce: 0.038618
2022-01-06 21:59:04,657 iteration 642 : loss : 0.122498, loss_ce: 0.038488
2022-01-06 21:59:06,186 iteration 643 : loss : 0.125633, loss_ce: 0.047073
2022-01-06 21:59:07,729 iteration 644 : loss : 0.119275, loss_ce: 0.052291
2022-01-06 21:59:09,224 iteration 645 : loss : 0.103017, loss_ce: 0.035756
2022-01-06 21:59:10,814 iteration 646 : loss : 0.112207, loss_ce: 0.052970
 10%|██▊                           | 38/400 [17:57<2:45:37, 27.45s/it]2022-01-06 21:59:12,379 iteration 647 : loss : 0.127197, loss_ce: 0.048964
2022-01-06 21:59:13,903 iteration 648 : loss : 0.101888, loss_ce: 0.044501
2022-01-06 21:59:15,449 iteration 649 : loss : 0.094255, loss_ce: 0.043410
2022-01-06 21:59:17,037 iteration 650 : loss : 0.140741, loss_ce: 0.058969
2022-01-06 21:59:18,553 iteration 651 : loss : 0.092873, loss_ce: 0.038680
2022-01-06 21:59:19,995 iteration 652 : loss : 0.092495, loss_ce: 0.034830
2022-01-06 21:59:21,605 iteration 653 : loss : 0.103274, loss_ce: 0.041570
2022-01-06 21:59:23,108 iteration 654 : loss : 0.102380, loss_ce: 0.040259
2022-01-06 21:59:24,642 iteration 655 : loss : 0.158928, loss_ce: 0.053363
2022-01-06 21:59:26,180 iteration 656 : loss : 0.117511, loss_ce: 0.039032
2022-01-06 21:59:27,778 iteration 657 : loss : 0.081613, loss_ce: 0.026770
2022-01-06 21:59:29,329 iteration 658 : loss : 0.129757, loss_ce: 0.041130
2022-01-06 21:59:30,880 iteration 659 : loss : 0.123504, loss_ce: 0.041636
2022-01-06 21:59:32,379 iteration 660 : loss : 0.099694, loss_ce: 0.045839
2022-01-06 21:59:33,941 iteration 661 : loss : 0.113956, loss_ce: 0.047974
2022-01-06 21:59:35,559 iteration 662 : loss : 0.142297, loss_ce: 0.035132
2022-01-06 21:59:37,096 iteration 663 : loss : 0.115409, loss_ce: 0.045330
 10%|██▉                           | 39/400 [18:23<2:43:02, 27.10s/it]2022-01-06 21:59:38,706 iteration 664 : loss : 0.098142, loss_ce: 0.040708
2022-01-06 21:59:40,177 iteration 665 : loss : 0.083353, loss_ce: 0.038457
2022-01-06 21:59:41,709 iteration 666 : loss : 0.081427, loss_ce: 0.027650
2022-01-06 21:59:43,275 iteration 667 : loss : 0.059387, loss_ce: 0.024894
2022-01-06 21:59:44,800 iteration 668 : loss : 0.133497, loss_ce: 0.052543
2022-01-06 21:59:46,309 iteration 669 : loss : 0.121504, loss_ce: 0.038878
2022-01-06 21:59:47,964 iteration 670 : loss : 0.100468, loss_ce: 0.041655
2022-01-06 21:59:49,476 iteration 671 : loss : 0.124970, loss_ce: 0.040019
2022-01-06 21:59:51,067 iteration 672 : loss : 0.089644, loss_ce: 0.032455
2022-01-06 21:59:52,621 iteration 673 : loss : 0.104925, loss_ce: 0.045205
2022-01-06 21:59:54,161 iteration 674 : loss : 0.075835, loss_ce: 0.033582
2022-01-06 21:59:55,733 iteration 675 : loss : 0.144803, loss_ce: 0.063426
2022-01-06 21:59:57,251 iteration 676 : loss : 0.104267, loss_ce: 0.038627
2022-01-06 21:59:58,786 iteration 677 : loss : 0.067347, loss_ce: 0.030386
2022-01-06 22:00:00,349 iteration 678 : loss : 0.134908, loss_ce: 0.046123
2022-01-06 22:00:01,893 iteration 679 : loss : 0.111431, loss_ce: 0.045230
2022-01-06 22:00:01,893 Training Data Eval:
2022-01-06 22:00:09,765   Average segmentation loss on training set: 0.1922
2022-01-06 22:00:09,765 Validation Data Eval:
2022-01-06 22:00:12,498   Average segmentation loss on validation set: 0.2342
2022-01-06 22:00:14,043 iteration 680 : loss : 0.098822, loss_ce: 0.033686
 10%|███                           | 40/400 [19:00<3:00:19, 30.05s/it]2022-01-06 22:00:15,645 iteration 681 : loss : 0.080603, loss_ce: 0.026717
2022-01-06 22:00:17,102 iteration 682 : loss : 0.133591, loss_ce: 0.044979
2022-01-06 22:00:18,630 iteration 683 : loss : 0.116202, loss_ce: 0.049655
2022-01-06 22:00:20,110 iteration 684 : loss : 0.093614, loss_ce: 0.032609
2022-01-06 22:00:21,684 iteration 685 : loss : 0.094025, loss_ce: 0.037503
2022-01-06 22:00:23,205 iteration 686 : loss : 0.147464, loss_ce: 0.052590
2022-01-06 22:00:24,728 iteration 687 : loss : 0.090471, loss_ce: 0.039338
2022-01-06 22:00:26,226 iteration 688 : loss : 0.107612, loss_ce: 0.037734
2022-01-06 22:00:27,768 iteration 689 : loss : 0.088023, loss_ce: 0.037600
2022-01-06 22:00:29,285 iteration 690 : loss : 0.094955, loss_ce: 0.033667
2022-01-06 22:00:30,796 iteration 691 : loss : 0.092367, loss_ce: 0.035685
2022-01-06 22:00:32,359 iteration 692 : loss : 0.088873, loss_ce: 0.030582
2022-01-06 22:00:33,833 iteration 693 : loss : 0.136687, loss_ce: 0.062394
2022-01-06 22:00:35,348 iteration 694 : loss : 0.120415, loss_ce: 0.044708
2022-01-06 22:00:36,816 iteration 695 : loss : 0.135861, loss_ce: 0.065831
2022-01-06 22:00:38,365 iteration 696 : loss : 0.081484, loss_ce: 0.043885
2022-01-06 22:00:39,874 iteration 697 : loss : 0.088275, loss_ce: 0.035227
 10%|███                           | 41/400 [19:26<2:52:15, 28.79s/it]2022-01-06 22:00:41,401 iteration 698 : loss : 0.152693, loss_ce: 0.046546
2022-01-06 22:00:42,942 iteration 699 : loss : 0.096970, loss_ce: 0.041128
2022-01-06 22:00:44,422 iteration 700 : loss : 0.094019, loss_ce: 0.045766
2022-01-06 22:00:45,943 iteration 701 : loss : 0.115397, loss_ce: 0.053878
2022-01-06 22:00:47,478 iteration 702 : loss : 0.132481, loss_ce: 0.050908
2022-01-06 22:00:49,057 iteration 703 : loss : 0.055030, loss_ce: 0.024137
2022-01-06 22:00:50,501 iteration 704 : loss : 0.083074, loss_ce: 0.032658
2022-01-06 22:00:52,003 iteration 705 : loss : 0.115102, loss_ce: 0.043400
2022-01-06 22:00:53,520 iteration 706 : loss : 0.104179, loss_ce: 0.038950
2022-01-06 22:00:54,969 iteration 707 : loss : 0.068371, loss_ce: 0.028870
2022-01-06 22:00:56,482 iteration 708 : loss : 0.071406, loss_ce: 0.026083
2022-01-06 22:00:58,010 iteration 709 : loss : 0.114431, loss_ce: 0.043357
2022-01-06 22:00:59,581 iteration 710 : loss : 0.108520, loss_ce: 0.042298
2022-01-06 22:01:01,139 iteration 711 : loss : 0.112421, loss_ce: 0.048114
2022-01-06 22:01:02,756 iteration 712 : loss : 0.085081, loss_ce: 0.034321
2022-01-06 22:01:04,283 iteration 713 : loss : 0.092567, loss_ce: 0.036160
2022-01-06 22:01:05,872 iteration 714 : loss : 0.102175, loss_ce: 0.035004
 10%|███▏                          | 42/400 [19:52<2:46:46, 27.95s/it]2022-01-06 22:01:07,503 iteration 715 : loss : 0.088586, loss_ce: 0.032658
2022-01-06 22:01:09,055 iteration 716 : loss : 0.084176, loss_ce: 0.034108
2022-01-06 22:01:10,559 iteration 717 : loss : 0.134281, loss_ce: 0.049082
2022-01-06 22:01:12,082 iteration 718 : loss : 0.100890, loss_ce: 0.047210
2022-01-06 22:01:13,655 iteration 719 : loss : 0.105863, loss_ce: 0.037450
2022-01-06 22:01:15,176 iteration 720 : loss : 0.089709, loss_ce: 0.030943
2022-01-06 22:01:16,717 iteration 721 : loss : 0.099256, loss_ce: 0.031797
2022-01-06 22:01:18,258 iteration 722 : loss : 0.105935, loss_ce: 0.039164
2022-01-06 22:01:19,835 iteration 723 : loss : 0.116127, loss_ce: 0.051134
2022-01-06 22:01:21,347 iteration 724 : loss : 0.138411, loss_ce: 0.063484
2022-01-06 22:01:22,945 iteration 725 : loss : 0.104355, loss_ce: 0.040919
2022-01-06 22:01:24,476 iteration 726 : loss : 0.106166, loss_ce: 0.045238
2022-01-06 22:01:25,945 iteration 727 : loss : 0.080952, loss_ce: 0.031298
2022-01-06 22:01:27,497 iteration 728 : loss : 0.113623, loss_ce: 0.057525
2022-01-06 22:01:29,066 iteration 729 : loss : 0.060060, loss_ce: 0.024403
2022-01-06 22:01:30,611 iteration 730 : loss : 0.090218, loss_ce: 0.029540
2022-01-06 22:01:32,066 iteration 731 : loss : 0.078865, loss_ce: 0.028139
 11%|███▏                          | 43/400 [20:18<2:43:10, 27.42s/it]2022-01-06 22:01:33,584 iteration 732 : loss : 0.106758, loss_ce: 0.048445
2022-01-06 22:01:35,026 iteration 733 : loss : 0.114343, loss_ce: 0.054225
2022-01-06 22:01:36,577 iteration 734 : loss : 0.099879, loss_ce: 0.036216
2022-01-06 22:01:38,090 iteration 735 : loss : 0.120950, loss_ce: 0.046627
2022-01-06 22:01:39,605 iteration 736 : loss : 0.111217, loss_ce: 0.049093
2022-01-06 22:01:41,270 iteration 737 : loss : 0.120102, loss_ce: 0.051362
2022-01-06 22:01:42,756 iteration 738 : loss : 0.118784, loss_ce: 0.070812
2022-01-06 22:01:44,347 iteration 739 : loss : 0.094984, loss_ce: 0.035504
2022-01-06 22:01:45,885 iteration 740 : loss : 0.092290, loss_ce: 0.036596
2022-01-06 22:01:47,410 iteration 741 : loss : 0.097788, loss_ce: 0.037409
2022-01-06 22:01:48,891 iteration 742 : loss : 0.111084, loss_ce: 0.050619
2022-01-06 22:01:50,447 iteration 743 : loss : 0.079766, loss_ce: 0.028015
2022-01-06 22:01:52,018 iteration 744 : loss : 0.104510, loss_ce: 0.039176
2022-01-06 22:01:53,525 iteration 745 : loss : 0.135819, loss_ce: 0.027799
2022-01-06 22:01:55,035 iteration 746 : loss : 0.109348, loss_ce: 0.041264
2022-01-06 22:01:56,538 iteration 747 : loss : 0.124180, loss_ce: 0.050677
2022-01-06 22:01:58,017 iteration 748 : loss : 0.099265, loss_ce: 0.043729
 11%|███▎                          | 44/400 [20:44<2:40:06, 26.98s/it]2022-01-06 22:01:59,560 iteration 749 : loss : 0.191616, loss_ce: 0.042357
2022-01-06 22:02:01,067 iteration 750 : loss : 0.069747, loss_ce: 0.026128
2022-01-06 22:02:02,611 iteration 751 : loss : 0.092883, loss_ce: 0.031035
2022-01-06 22:02:04,121 iteration 752 : loss : 0.134452, loss_ce: 0.068814
2022-01-06 22:02:05,602 iteration 753 : loss : 0.113227, loss_ce: 0.046918
2022-01-06 22:02:07,085 iteration 754 : loss : 0.112504, loss_ce: 0.038600
2022-01-06 22:02:08,528 iteration 755 : loss : 0.108589, loss_ce: 0.051632
2022-01-06 22:02:09,977 iteration 756 : loss : 0.061121, loss_ce: 0.023300
2022-01-06 22:02:11,484 iteration 757 : loss : 0.118817, loss_ce: 0.055887
2022-01-06 22:02:13,071 iteration 758 : loss : 0.092845, loss_ce: 0.043124
2022-01-06 22:02:14,689 iteration 759 : loss : 0.121443, loss_ce: 0.045434
2022-01-06 22:02:16,169 iteration 760 : loss : 0.119741, loss_ce: 0.071091
2022-01-06 22:02:17,690 iteration 761 : loss : 0.097087, loss_ce: 0.041871
2022-01-06 22:02:19,226 iteration 762 : loss : 0.102557, loss_ce: 0.039908
2022-01-06 22:02:20,771 iteration 763 : loss : 0.102205, loss_ce: 0.045575
2022-01-06 22:02:22,259 iteration 764 : loss : 0.100112, loss_ce: 0.042450
2022-01-06 22:02:22,259 Training Data Eval:
2022-01-06 22:02:30,101   Average segmentation loss on training set: 0.3335
2022-01-06 22:02:30,101 Validation Data Eval:
2022-01-06 22:02:32,812   Average segmentation loss on validation set: 0.2859
2022-01-06 22:02:34,486 iteration 765 : loss : 0.131707, loss_ce: 0.054003
 11%|███▍                          | 45/400 [21:21<2:56:29, 29.83s/it]2022-01-06 22:02:36,107 iteration 766 : loss : 0.139078, loss_ce: 0.067792
2022-01-06 22:02:37,654 iteration 767 : loss : 0.110450, loss_ce: 0.040500
2022-01-06 22:02:39,181 iteration 768 : loss : 0.128440, loss_ce: 0.045388
2022-01-06 22:02:40,723 iteration 769 : loss : 0.115180, loss_ce: 0.050373
2022-01-06 22:02:42,152 iteration 770 : loss : 0.081399, loss_ce: 0.026466
2022-01-06 22:02:43,677 iteration 771 : loss : 0.104572, loss_ce: 0.028915
2022-01-06 22:02:45,213 iteration 772 : loss : 0.090320, loss_ce: 0.035956
2022-01-06 22:02:46,752 iteration 773 : loss : 0.102735, loss_ce: 0.029266
2022-01-06 22:02:48,349 iteration 774 : loss : 0.093248, loss_ce: 0.036584
2022-01-06 22:02:49,827 iteration 775 : loss : 0.138220, loss_ce: 0.070434
2022-01-06 22:02:51,286 iteration 776 : loss : 0.100726, loss_ce: 0.038946
2022-01-06 22:02:52,954 iteration 777 : loss : 0.096622, loss_ce: 0.041390
2022-01-06 22:02:54,390 iteration 778 : loss : 0.143012, loss_ce: 0.044752
2022-01-06 22:02:55,949 iteration 779 : loss : 0.110845, loss_ce: 0.050219
2022-01-06 22:02:57,505 iteration 780 : loss : 0.102218, loss_ce: 0.038117
2022-01-06 22:02:58,969 iteration 781 : loss : 0.083568, loss_ce: 0.041793
2022-01-06 22:03:00,408 iteration 782 : loss : 0.088483, loss_ce: 0.043701
 12%|███▍                          | 46/400 [21:47<2:49:04, 28.66s/it]2022-01-06 22:03:01,968 iteration 783 : loss : 0.114652, loss_ce: 0.053951
2022-01-06 22:03:03,467 iteration 784 : loss : 0.108686, loss_ce: 0.041235
2022-01-06 22:03:05,018 iteration 785 : loss : 0.109265, loss_ce: 0.052137
2022-01-06 22:03:06,560 iteration 786 : loss : 0.123559, loss_ce: 0.047826
2022-01-06 22:03:08,112 iteration 787 : loss : 0.131389, loss_ce: 0.047926
2022-01-06 22:03:09,729 iteration 788 : loss : 0.126744, loss_ce: 0.046538
2022-01-06 22:03:11,208 iteration 789 : loss : 0.101662, loss_ce: 0.035761
2022-01-06 22:03:12,801 iteration 790 : loss : 0.121020, loss_ce: 0.042162
2022-01-06 22:03:14,342 iteration 791 : loss : 0.067444, loss_ce: 0.028342
2022-01-06 22:03:15,838 iteration 792 : loss : 0.090472, loss_ce: 0.039222
2022-01-06 22:03:17,448 iteration 793 : loss : 0.105150, loss_ce: 0.054885
2022-01-06 22:03:18,905 iteration 794 : loss : 0.071254, loss_ce: 0.028711
2022-01-06 22:03:20,391 iteration 795 : loss : 0.119264, loss_ce: 0.052747
2022-01-06 22:03:21,879 iteration 796 : loss : 0.100703, loss_ce: 0.047335
2022-01-06 22:03:23,442 iteration 797 : loss : 0.176016, loss_ce: 0.090669
2022-01-06 22:03:24,952 iteration 798 : loss : 0.137790, loss_ce: 0.045805
2022-01-06 22:03:26,416 iteration 799 : loss : 0.069748, loss_ce: 0.031151
 12%|███▌                          | 47/400 [22:13<2:43:55, 27.86s/it]2022-01-06 22:03:28,098 iteration 800 : loss : 0.064090, loss_ce: 0.024379
2022-01-06 22:03:29,641 iteration 801 : loss : 0.077635, loss_ce: 0.030172
2022-01-06 22:03:31,165 iteration 802 : loss : 0.106787, loss_ce: 0.037268
2022-01-06 22:03:32,719 iteration 803 : loss : 0.093180, loss_ce: 0.046134
2022-01-06 22:03:34,338 iteration 804 : loss : 0.148156, loss_ce: 0.066930
2022-01-06 22:03:36,031 iteration 805 : loss : 0.080782, loss_ce: 0.030931
2022-01-06 22:03:37,568 iteration 806 : loss : 0.080694, loss_ce: 0.033277
2022-01-06 22:03:39,078 iteration 807 : loss : 0.100761, loss_ce: 0.036942
2022-01-06 22:03:40,670 iteration 808 : loss : 0.128125, loss_ce: 0.066304
2022-01-06 22:03:42,243 iteration 809 : loss : 0.081819, loss_ce: 0.028908
2022-01-06 22:03:43,776 iteration 810 : loss : 0.114031, loss_ce: 0.038107
2022-01-06 22:03:45,276 iteration 811 : loss : 0.126104, loss_ce: 0.068211
2022-01-06 22:03:46,805 iteration 812 : loss : 0.097435, loss_ce: 0.045472
2022-01-06 22:03:48,354 iteration 813 : loss : 0.137380, loss_ce: 0.046323
2022-01-06 22:03:50,013 iteration 814 : loss : 0.127234, loss_ce: 0.041380
2022-01-06 22:03:51,629 iteration 815 : loss : 0.109104, loss_ce: 0.045353
2022-01-06 22:03:53,204 iteration 816 : loss : 0.072568, loss_ce: 0.027822
 12%|███▌                          | 48/400 [22:39<2:41:33, 27.54s/it]2022-01-06 22:03:54,807 iteration 817 : loss : 0.089730, loss_ce: 0.038583
2022-01-06 22:03:56,305 iteration 818 : loss : 0.085276, loss_ce: 0.037679
2022-01-06 22:03:57,853 iteration 819 : loss : 0.082326, loss_ce: 0.029987
2022-01-06 22:03:59,337 iteration 820 : loss : 0.067827, loss_ce: 0.026690
2022-01-06 22:04:00,942 iteration 821 : loss : 0.096429, loss_ce: 0.032989
2022-01-06 22:04:02,525 iteration 822 : loss : 0.111069, loss_ce: 0.045884
2022-01-06 22:04:04,019 iteration 823 : loss : 0.059833, loss_ce: 0.023000
2022-01-06 22:04:05,521 iteration 824 : loss : 0.094433, loss_ce: 0.037773
2022-01-06 22:04:07,056 iteration 825 : loss : 0.079469, loss_ce: 0.030950
2022-01-06 22:04:08,579 iteration 826 : loss : 0.117028, loss_ce: 0.035815
2022-01-06 22:04:10,099 iteration 827 : loss : 0.126653, loss_ce: 0.038244
2022-01-06 22:04:11,628 iteration 828 : loss : 0.077546, loss_ce: 0.027230
2022-01-06 22:04:13,191 iteration 829 : loss : 0.102569, loss_ce: 0.044444
2022-01-06 22:04:14,791 iteration 830 : loss : 0.085229, loss_ce: 0.029996
2022-01-06 22:04:16,330 iteration 831 : loss : 0.103163, loss_ce: 0.042934
2022-01-06 22:04:17,850 iteration 832 : loss : 0.076084, loss_ce: 0.026151
2022-01-06 22:04:19,428 iteration 833 : loss : 0.097057, loss_ce: 0.044273
 12%|███▋                          | 49/400 [23:06<2:38:47, 27.15s/it]2022-01-06 22:04:21,005 iteration 834 : loss : 0.087901, loss_ce: 0.036141
2022-01-06 22:04:22,443 iteration 835 : loss : 0.085957, loss_ce: 0.025074
2022-01-06 22:04:23,973 iteration 836 : loss : 0.069323, loss_ce: 0.029616
2022-01-06 22:04:25,444 iteration 837 : loss : 0.065886, loss_ce: 0.033526
2022-01-06 22:04:26,930 iteration 838 : loss : 0.081660, loss_ce: 0.032616
2022-01-06 22:04:28,463 iteration 839 : loss : 0.090351, loss_ce: 0.037884
2022-01-06 22:04:29,964 iteration 840 : loss : 0.094219, loss_ce: 0.033370
2022-01-06 22:04:31,446 iteration 841 : loss : 0.103883, loss_ce: 0.045273
2022-01-06 22:04:32,940 iteration 842 : loss : 0.077908, loss_ce: 0.033419
2022-01-06 22:04:34,525 iteration 843 : loss : 0.076322, loss_ce: 0.025944
2022-01-06 22:04:36,016 iteration 844 : loss : 0.086231, loss_ce: 0.026992
2022-01-06 22:04:37,546 iteration 845 : loss : 0.090921, loss_ce: 0.031105
2022-01-06 22:04:39,162 iteration 846 : loss : 0.096836, loss_ce: 0.043079
2022-01-06 22:04:40,676 iteration 847 : loss : 0.088954, loss_ce: 0.034345
2022-01-06 22:04:42,101 iteration 848 : loss : 0.084309, loss_ce: 0.033446
2022-01-06 22:04:43,628 iteration 849 : loss : 0.084954, loss_ce: 0.030736
2022-01-06 22:04:43,628 Training Data Eval:
2022-01-06 22:04:51,408   Average segmentation loss on training set: 0.0857
2022-01-06 22:04:51,409 Validation Data Eval:
2022-01-06 22:04:54,096   Average segmentation loss on validation set: 0.1479
2022-01-06 22:05:01,168 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 22:05:02,682 iteration 850 : loss : 0.100127, loss_ce: 0.042404
 12%|███▊                          | 50/400 [23:49<3:06:32, 31.98s/it]2022-01-06 22:05:04,194 iteration 851 : loss : 0.061320, loss_ce: 0.025258
2022-01-06 22:05:05,601 iteration 852 : loss : 0.064250, loss_ce: 0.024727
2022-01-06 22:05:07,109 iteration 853 : loss : 0.087971, loss_ce: 0.036542
2022-01-06 22:05:08,574 iteration 854 : loss : 0.067792, loss_ce: 0.019999
2022-01-06 22:05:10,031 iteration 855 : loss : 0.069018, loss_ce: 0.029150
2022-01-06 22:05:11,505 iteration 856 : loss : 0.076356, loss_ce: 0.029538
2022-01-06 22:05:12,911 iteration 857 : loss : 0.067880, loss_ce: 0.029205
2022-01-06 22:05:14,254 iteration 858 : loss : 0.088269, loss_ce: 0.050177
2022-01-06 22:05:15,665 iteration 859 : loss : 0.080479, loss_ce: 0.028666
2022-01-06 22:05:17,090 iteration 860 : loss : 0.087034, loss_ce: 0.027439
2022-01-06 22:05:18,681 iteration 861 : loss : 0.089479, loss_ce: 0.035320
2022-01-06 22:05:20,171 iteration 862 : loss : 0.081348, loss_ce: 0.025609
2022-01-06 22:05:21,796 iteration 863 : loss : 0.107549, loss_ce: 0.050319
2022-01-06 22:05:23,380 iteration 864 : loss : 0.086414, loss_ce: 0.039266
2022-01-06 22:05:24,861 iteration 865 : loss : 0.090183, loss_ce: 0.035951
2022-01-06 22:05:26,461 iteration 866 : loss : 0.098330, loss_ce: 0.047933
2022-01-06 22:05:27,933 iteration 867 : loss : 0.051107, loss_ce: 0.017602
 13%|███▊                          | 51/400 [24:14<2:54:15, 29.96s/it]2022-01-06 22:05:29,541 iteration 868 : loss : 0.124356, loss_ce: 0.039912
2022-01-06 22:05:31,062 iteration 869 : loss : 0.090371, loss_ce: 0.038692
2022-01-06 22:05:32,509 iteration 870 : loss : 0.077293, loss_ce: 0.028263
2022-01-06 22:05:33,986 iteration 871 : loss : 0.065227, loss_ce: 0.024985
2022-01-06 22:05:35,479 iteration 872 : loss : 0.065825, loss_ce: 0.033878
2022-01-06 22:05:37,057 iteration 873 : loss : 0.077930, loss_ce: 0.026221
2022-01-06 22:05:38,491 iteration 874 : loss : 0.048379, loss_ce: 0.021184
2022-01-06 22:05:40,138 iteration 875 : loss : 0.102823, loss_ce: 0.040967
2022-01-06 22:05:41,663 iteration 876 : loss : 0.045109, loss_ce: 0.019617
2022-01-06 22:05:43,109 iteration 877 : loss : 0.065771, loss_ce: 0.023805
2022-01-06 22:05:44,678 iteration 878 : loss : 0.072966, loss_ce: 0.029010
2022-01-06 22:05:46,176 iteration 879 : loss : 0.095576, loss_ce: 0.043874
2022-01-06 22:05:47,722 iteration 880 : loss : 0.061855, loss_ce: 0.022041
2022-01-06 22:05:49,202 iteration 881 : loss : 0.115620, loss_ce: 0.067233
2022-01-06 22:05:50,755 iteration 882 : loss : 0.083725, loss_ce: 0.031375
2022-01-06 22:05:52,265 iteration 883 : loss : 0.068874, loss_ce: 0.028817
2022-01-06 22:05:53,883 iteration 884 : loss : 0.095819, loss_ce: 0.047011
 13%|███▉                          | 52/400 [24:40<2:46:47, 28.76s/it]2022-01-06 22:05:55,471 iteration 885 : loss : 0.090319, loss_ce: 0.036099
2022-01-06 22:05:57,009 iteration 886 : loss : 0.064971, loss_ce: 0.025053
2022-01-06 22:05:58,547 iteration 887 : loss : 0.127999, loss_ce: 0.056726
2022-01-06 22:06:00,055 iteration 888 : loss : 0.074184, loss_ce: 0.028531
2022-01-06 22:06:01,620 iteration 889 : loss : 0.108881, loss_ce: 0.035965
2022-01-06 22:06:03,241 iteration 890 : loss : 0.114722, loss_ce: 0.044556
2022-01-06 22:06:04,756 iteration 891 : loss : 0.055763, loss_ce: 0.021612
2022-01-06 22:06:06,270 iteration 892 : loss : 0.062420, loss_ce: 0.029567
2022-01-06 22:06:07,758 iteration 893 : loss : 0.102041, loss_ce: 0.038989
2022-01-06 22:06:09,362 iteration 894 : loss : 0.104399, loss_ce: 0.040782
2022-01-06 22:06:10,847 iteration 895 : loss : 0.101367, loss_ce: 0.027078
2022-01-06 22:06:12,415 iteration 896 : loss : 0.100152, loss_ce: 0.038351
2022-01-06 22:06:13,990 iteration 897 : loss : 0.109615, loss_ce: 0.046193
2022-01-06 22:06:15,470 iteration 898 : loss : 0.069409, loss_ce: 0.034104
2022-01-06 22:06:16,951 iteration 899 : loss : 0.072871, loss_ce: 0.029219
2022-01-06 22:06:18,404 iteration 900 : loss : 0.062495, loss_ce: 0.027623
2022-01-06 22:06:19,882 iteration 901 : loss : 0.092987, loss_ce: 0.031884
 13%|███▉                          | 53/400 [25:06<2:41:31, 27.93s/it]2022-01-06 22:06:21,503 iteration 902 : loss : 0.077692, loss_ce: 0.031041
2022-01-06 22:06:23,096 iteration 903 : loss : 0.092245, loss_ce: 0.037469
2022-01-06 22:06:24,639 iteration 904 : loss : 0.091242, loss_ce: 0.031869
2022-01-06 22:06:26,176 iteration 905 : loss : 0.104195, loss_ce: 0.047136
2022-01-06 22:06:27,690 iteration 906 : loss : 0.058160, loss_ce: 0.028253
2022-01-06 22:06:29,271 iteration 907 : loss : 0.102609, loss_ce: 0.035786
2022-01-06 22:06:30,690 iteration 908 : loss : 0.064241, loss_ce: 0.025359
2022-01-06 22:06:32,224 iteration 909 : loss : 0.075773, loss_ce: 0.032196
2022-01-06 22:06:33,664 iteration 910 : loss : 0.118605, loss_ce: 0.050197
2022-01-06 22:06:35,164 iteration 911 : loss : 0.067024, loss_ce: 0.025651
2022-01-06 22:06:36,656 iteration 912 : loss : 0.076655, loss_ce: 0.031371
2022-01-06 22:06:38,130 iteration 913 : loss : 0.081753, loss_ce: 0.027806
2022-01-06 22:06:39,632 iteration 914 : loss : 0.126464, loss_ce: 0.060904
2022-01-06 22:06:41,110 iteration 915 : loss : 0.115096, loss_ce: 0.031752
2022-01-06 22:06:42,595 iteration 916 : loss : 0.066305, loss_ce: 0.023506
2022-01-06 22:06:44,123 iteration 917 : loss : 0.100015, loss_ce: 0.035811
2022-01-06 22:06:45,654 iteration 918 : loss : 0.078437, loss_ce: 0.026926
 14%|████                          | 54/400 [25:32<2:37:18, 27.28s/it]2022-01-06 22:06:47,245 iteration 919 : loss : 0.073744, loss_ce: 0.031329
2022-01-06 22:06:48,776 iteration 920 : loss : 0.065701, loss_ce: 0.027007
2022-01-06 22:06:50,323 iteration 921 : loss : 0.092254, loss_ce: 0.032939
2022-01-06 22:06:51,888 iteration 922 : loss : 0.095866, loss_ce: 0.044913
2022-01-06 22:06:53,390 iteration 923 : loss : 0.060086, loss_ce: 0.026115
2022-01-06 22:06:54,971 iteration 924 : loss : 0.121867, loss_ce: 0.033749
2022-01-06 22:06:56,468 iteration 925 : loss : 0.048515, loss_ce: 0.018168
2022-01-06 22:06:58,046 iteration 926 : loss : 0.071685, loss_ce: 0.029979
2022-01-06 22:06:59,509 iteration 927 : loss : 0.093674, loss_ce: 0.034213
2022-01-06 22:07:01,022 iteration 928 : loss : 0.085225, loss_ce: 0.039085
2022-01-06 22:07:02,599 iteration 929 : loss : 0.081400, loss_ce: 0.031987
2022-01-06 22:07:04,136 iteration 930 : loss : 0.085544, loss_ce: 0.036328
2022-01-06 22:07:05,658 iteration 931 : loss : 0.127434, loss_ce: 0.033661
2022-01-06 22:07:07,171 iteration 932 : loss : 0.082480, loss_ce: 0.031504
2022-01-06 22:07:08,655 iteration 933 : loss : 0.110102, loss_ce: 0.040619
2022-01-06 22:07:10,169 iteration 934 : loss : 0.071933, loss_ce: 0.031264
2022-01-06 22:07:10,169 Training Data Eval:
2022-01-06 22:07:17,921   Average segmentation loss on training set: 1.5425
2022-01-06 22:07:17,922 Validation Data Eval:
2022-01-06 22:07:20,604   Average segmentation loss on validation set: 1.4566
2022-01-06 22:07:22,195 iteration 935 : loss : 0.128462, loss_ce: 0.041853
 14%|████▏                         | 55/400 [26:08<2:52:49, 30.06s/it]2022-01-06 22:07:23,726 iteration 936 : loss : 0.079240, loss_ce: 0.025449
2022-01-06 22:07:25,255 iteration 937 : loss : 0.078715, loss_ce: 0.028289
2022-01-06 22:07:26,816 iteration 938 : loss : 0.108643, loss_ce: 0.040899
2022-01-06 22:07:28,305 iteration 939 : loss : 0.096353, loss_ce: 0.036227
2022-01-06 22:07:29,849 iteration 940 : loss : 0.095876, loss_ce: 0.038284
2022-01-06 22:07:31,456 iteration 941 : loss : 0.095326, loss_ce: 0.041828
2022-01-06 22:07:32,925 iteration 942 : loss : 0.078145, loss_ce: 0.028632
2022-01-06 22:07:34,417 iteration 943 : loss : 0.076675, loss_ce: 0.033256
2022-01-06 22:07:35,882 iteration 944 : loss : 0.089674, loss_ce: 0.038158
2022-01-06 22:07:37,368 iteration 945 : loss : 0.073419, loss_ce: 0.026754
2022-01-06 22:07:38,844 iteration 946 : loss : 0.070227, loss_ce: 0.031766
2022-01-06 22:07:40,307 iteration 947 : loss : 0.078831, loss_ce: 0.026256
2022-01-06 22:07:41,764 iteration 948 : loss : 0.099006, loss_ce: 0.037704
2022-01-06 22:07:43,251 iteration 949 : loss : 0.065939, loss_ce: 0.026633
2022-01-06 22:07:44,729 iteration 950 : loss : 0.100396, loss_ce: 0.049240
2022-01-06 22:07:46,271 iteration 951 : loss : 0.073834, loss_ce: 0.044500
2022-01-06 22:07:47,720 iteration 952 : loss : 0.080659, loss_ce: 0.033791
 14%|████▏                         | 56/400 [26:34<2:44:32, 28.70s/it]2022-01-06 22:07:49,292 iteration 953 : loss : 0.067809, loss_ce: 0.023667
2022-01-06 22:07:50,820 iteration 954 : loss : 0.058064, loss_ce: 0.023018
2022-01-06 22:07:52,236 iteration 955 : loss : 0.049414, loss_ce: 0.019915
2022-01-06 22:07:53,764 iteration 956 : loss : 0.065452, loss_ce: 0.026648
2022-01-06 22:07:55,275 iteration 957 : loss : 0.068341, loss_ce: 0.023266
2022-01-06 22:07:56,796 iteration 958 : loss : 0.088254, loss_ce: 0.030642
2022-01-06 22:07:58,387 iteration 959 : loss : 0.103019, loss_ce: 0.041841
2022-01-06 22:08:00,024 iteration 960 : loss : 0.076939, loss_ce: 0.032131
2022-01-06 22:08:01,486 iteration 961 : loss : 0.104588, loss_ce: 0.031603
2022-01-06 22:08:02,928 iteration 962 : loss : 0.082966, loss_ce: 0.025471
2022-01-06 22:08:04,512 iteration 963 : loss : 0.095755, loss_ce: 0.042192
2022-01-06 22:08:06,048 iteration 964 : loss : 0.077956, loss_ce: 0.039555
2022-01-06 22:08:07,535 iteration 965 : loss : 0.054173, loss_ce: 0.023232
2022-01-06 22:08:09,051 iteration 966 : loss : 0.064320, loss_ce: 0.026241
2022-01-06 22:08:10,586 iteration 967 : loss : 0.066917, loss_ce: 0.025109
2022-01-06 22:08:12,082 iteration 968 : loss : 0.127432, loss_ce: 0.053980
2022-01-06 22:08:13,645 iteration 969 : loss : 0.066725, loss_ce: 0.029209
 14%|████▎                         | 57/400 [27:00<2:39:18, 27.87s/it]2022-01-06 22:08:15,228 iteration 970 : loss : 0.087681, loss_ce: 0.037157
2022-01-06 22:08:16,804 iteration 971 : loss : 0.083576, loss_ce: 0.023023
2022-01-06 22:08:18,297 iteration 972 : loss : 0.059687, loss_ce: 0.026204
2022-01-06 22:08:19,861 iteration 973 : loss : 0.080267, loss_ce: 0.024060
2022-01-06 22:08:21,334 iteration 974 : loss : 0.066152, loss_ce: 0.027717
2022-01-06 22:08:22,912 iteration 975 : loss : 0.110395, loss_ce: 0.043484
2022-01-06 22:08:24,427 iteration 976 : loss : 0.059084, loss_ce: 0.024401
2022-01-06 22:08:25,943 iteration 977 : loss : 0.058072, loss_ce: 0.019600
2022-01-06 22:08:27,484 iteration 978 : loss : 0.051257, loss_ce: 0.017633
2022-01-06 22:08:28,993 iteration 979 : loss : 0.121684, loss_ce: 0.066283
2022-01-06 22:08:30,497 iteration 980 : loss : 0.044263, loss_ce: 0.015254
2022-01-06 22:08:32,097 iteration 981 : loss : 0.069888, loss_ce: 0.030715
2022-01-06 22:08:33,586 iteration 982 : loss : 0.078710, loss_ce: 0.041460
2022-01-06 22:08:35,148 iteration 983 : loss : 0.092561, loss_ce: 0.030777
2022-01-06 22:08:36,606 iteration 984 : loss : 0.063164, loss_ce: 0.027567
2022-01-06 22:08:38,116 iteration 985 : loss : 0.059796, loss_ce: 0.019935
2022-01-06 22:08:39,670 iteration 986 : loss : 0.102565, loss_ce: 0.036121
 14%|████▎                         | 58/400 [27:26<2:35:42, 27.32s/it]2022-01-06 22:08:41,303 iteration 987 : loss : 0.061771, loss_ce: 0.027985
2022-01-06 22:08:42,833 iteration 988 : loss : 0.091853, loss_ce: 0.045292
2022-01-06 22:08:44,343 iteration 989 : loss : 0.078497, loss_ce: 0.034216
2022-01-06 22:08:45,838 iteration 990 : loss : 0.074483, loss_ce: 0.033485
2022-01-06 22:08:47,328 iteration 991 : loss : 0.081833, loss_ce: 0.036557
2022-01-06 22:08:48,897 iteration 992 : loss : 0.066122, loss_ce: 0.030390
2022-01-06 22:08:50,430 iteration 993 : loss : 0.061028, loss_ce: 0.024109
2022-01-06 22:08:52,040 iteration 994 : loss : 0.074786, loss_ce: 0.030988
2022-01-06 22:08:53,558 iteration 995 : loss : 0.128634, loss_ce: 0.044165
2022-01-06 22:08:55,117 iteration 996 : loss : 0.052187, loss_ce: 0.024049
2022-01-06 22:08:56,657 iteration 997 : loss : 0.082135, loss_ce: 0.031554
2022-01-06 22:08:58,101 iteration 998 : loss : 0.088311, loss_ce: 0.027434
2022-01-06 22:08:59,652 iteration 999 : loss : 0.077359, loss_ce: 0.029893
2022-01-06 22:09:01,113 iteration 1000 : loss : 0.072910, loss_ce: 0.022055
2022-01-06 22:09:02,595 iteration 1001 : loss : 0.101741, loss_ce: 0.021050
2022-01-06 22:09:04,184 iteration 1002 : loss : 0.080404, loss_ce: 0.031037
2022-01-06 22:09:05,753 iteration 1003 : loss : 0.100769, loss_ce: 0.037813
 15%|████▍                         | 59/400 [27:52<2:33:08, 26.95s/it]2022-01-06 22:09:07,312 iteration 1004 : loss : 0.128385, loss_ce: 0.056281
2022-01-06 22:09:08,840 iteration 1005 : loss : 0.078510, loss_ce: 0.029258
2022-01-06 22:09:10,415 iteration 1006 : loss : 0.110206, loss_ce: 0.043664
2022-01-06 22:09:11,870 iteration 1007 : loss : 0.084731, loss_ce: 0.042075
2022-01-06 22:09:13,319 iteration 1008 : loss : 0.128960, loss_ce: 0.047989
2022-01-06 22:09:14,777 iteration 1009 : loss : 0.085399, loss_ce: 0.032637
2022-01-06 22:09:16,238 iteration 1010 : loss : 0.085691, loss_ce: 0.029186
2022-01-06 22:09:17,805 iteration 1011 : loss : 0.081288, loss_ce: 0.033016
2022-01-06 22:09:19,300 iteration 1012 : loss : 0.063811, loss_ce: 0.024651
2022-01-06 22:09:20,747 iteration 1013 : loss : 0.072975, loss_ce: 0.023532
2022-01-06 22:09:22,255 iteration 1014 : loss : 0.124991, loss_ce: 0.050654
2022-01-06 22:09:23,700 iteration 1015 : loss : 0.089758, loss_ce: 0.034153
2022-01-06 22:09:25,229 iteration 1016 : loss : 0.070740, loss_ce: 0.029881
2022-01-06 22:09:26,664 iteration 1017 : loss : 0.095697, loss_ce: 0.040945
2022-01-06 22:09:28,109 iteration 1018 : loss : 0.122118, loss_ce: 0.044357
2022-01-06 22:09:29,628 iteration 1019 : loss : 0.141131, loss_ce: 0.069564
2022-01-06 22:09:29,628 Training Data Eval:
2022-01-06 22:09:37,453   Average segmentation loss on training set: 0.3430
2022-01-06 22:09:37,453 Validation Data Eval:
2022-01-06 22:09:40,162   Average segmentation loss on validation set: 0.4031
2022-01-06 22:09:41,651 iteration 1020 : loss : 0.083779, loss_ce: 0.027556
 15%|████▌                         | 60/400 [28:28<2:47:54, 29.63s/it]2022-01-06 22:09:43,314 iteration 1021 : loss : 0.134849, loss_ce: 0.037022
2022-01-06 22:09:44,862 iteration 1022 : loss : 0.101452, loss_ce: 0.045288
2022-01-06 22:09:46,386 iteration 1023 : loss : 0.153426, loss_ce: 0.038022
2022-01-06 22:09:47,970 iteration 1024 : loss : 0.083054, loss_ce: 0.032254
2022-01-06 22:09:49,529 iteration 1025 : loss : 0.101369, loss_ce: 0.049310
2022-01-06 22:09:50,975 iteration 1026 : loss : 0.067046, loss_ce: 0.026011
2022-01-06 22:09:52,552 iteration 1027 : loss : 0.085922, loss_ce: 0.028248
2022-01-06 22:09:54,095 iteration 1028 : loss : 0.091518, loss_ce: 0.040982
2022-01-06 22:09:55,592 iteration 1029 : loss : 0.057147, loss_ce: 0.023483
2022-01-06 22:09:57,103 iteration 1030 : loss : 0.089947, loss_ce: 0.035417
2022-01-06 22:09:58,676 iteration 1031 : loss : 0.120798, loss_ce: 0.055788
2022-01-06 22:10:00,217 iteration 1032 : loss : 0.083848, loss_ce: 0.038471
2022-01-06 22:10:01,707 iteration 1033 : loss : 0.094981, loss_ce: 0.034860
2022-01-06 22:10:03,292 iteration 1034 : loss : 0.088438, loss_ce: 0.030624
2022-01-06 22:10:04,822 iteration 1035 : loss : 0.078329, loss_ce: 0.032737
2022-01-06 22:10:06,359 iteration 1036 : loss : 0.077830, loss_ce: 0.032215
2022-01-06 22:10:07,845 iteration 1037 : loss : 0.084446, loss_ce: 0.028416
 15%|████▌                         | 61/400 [28:54<2:41:34, 28.60s/it]2022-01-06 22:10:09,415 iteration 1038 : loss : 0.060617, loss_ce: 0.024128
2022-01-06 22:10:10,914 iteration 1039 : loss : 0.066535, loss_ce: 0.035365
2022-01-06 22:10:12,455 iteration 1040 : loss : 0.118634, loss_ce: 0.055743
2022-01-06 22:10:13,989 iteration 1041 : loss : 0.091871, loss_ce: 0.037230
2022-01-06 22:10:15,527 iteration 1042 : loss : 0.098901, loss_ce: 0.046962
2022-01-06 22:10:17,046 iteration 1043 : loss : 0.098148, loss_ce: 0.032990
2022-01-06 22:10:18,566 iteration 1044 : loss : 0.059957, loss_ce: 0.023003
2022-01-06 22:10:19,997 iteration 1045 : loss : 0.069786, loss_ce: 0.029582
2022-01-06 22:10:21,454 iteration 1046 : loss : 0.114953, loss_ce: 0.028287
2022-01-06 22:10:22,995 iteration 1047 : loss : 0.114672, loss_ce: 0.041771
2022-01-06 22:10:24,497 iteration 1048 : loss : 0.083541, loss_ce: 0.031517
2022-01-06 22:10:25,996 iteration 1049 : loss : 0.121699, loss_ce: 0.043568
2022-01-06 22:10:27,539 iteration 1050 : loss : 0.112031, loss_ce: 0.032726
2022-01-06 22:10:29,033 iteration 1051 : loss : 0.094184, loss_ce: 0.035243
2022-01-06 22:10:30,529 iteration 1052 : loss : 0.123816, loss_ce: 0.052777
2022-01-06 22:10:31,959 iteration 1053 : loss : 0.107439, loss_ce: 0.031565
2022-01-06 22:10:33,526 iteration 1054 : loss : 0.101505, loss_ce: 0.040464
 16%|████▋                         | 62/400 [29:20<2:36:11, 27.73s/it]2022-01-06 22:10:35,083 iteration 1055 : loss : 0.088274, loss_ce: 0.028429
2022-01-06 22:10:36,564 iteration 1056 : loss : 0.087256, loss_ce: 0.042065
2022-01-06 22:10:38,081 iteration 1057 : loss : 0.112917, loss_ce: 0.064081
2022-01-06 22:10:39,577 iteration 1058 : loss : 0.098075, loss_ce: 0.042474
2022-01-06 22:10:41,006 iteration 1059 : loss : 0.082727, loss_ce: 0.030165
2022-01-06 22:10:42,506 iteration 1060 : loss : 0.059155, loss_ce: 0.022274
2022-01-06 22:10:44,039 iteration 1061 : loss : 0.058759, loss_ce: 0.024867
2022-01-06 22:10:45,551 iteration 1062 : loss : 0.078980, loss_ce: 0.033208
2022-01-06 22:10:47,112 iteration 1063 : loss : 0.125063, loss_ce: 0.045483
2022-01-06 22:10:48,603 iteration 1064 : loss : 0.114278, loss_ce: 0.042302
2022-01-06 22:10:50,056 iteration 1065 : loss : 0.073210, loss_ce: 0.034211
2022-01-06 22:10:51,593 iteration 1066 : loss : 0.081851, loss_ce: 0.033652
2022-01-06 22:10:53,075 iteration 1067 : loss : 0.164272, loss_ce: 0.041248
2022-01-06 22:10:54,612 iteration 1068 : loss : 0.096549, loss_ce: 0.047704
2022-01-06 22:10:56,126 iteration 1069 : loss : 0.109481, loss_ce: 0.048148
2022-01-06 22:10:57,625 iteration 1070 : loss : 0.081162, loss_ce: 0.031213
2022-01-06 22:10:59,133 iteration 1071 : loss : 0.067410, loss_ce: 0.029049
 16%|████▋                         | 63/400 [29:45<2:32:08, 27.09s/it]2022-01-06 22:11:00,816 iteration 1072 : loss : 0.094756, loss_ce: 0.035572
2022-01-06 22:11:02,323 iteration 1073 : loss : 0.101460, loss_ce: 0.027299
2022-01-06 22:11:03,831 iteration 1074 : loss : 0.085800, loss_ce: 0.035638
2022-01-06 22:11:05,386 iteration 1075 : loss : 0.071106, loss_ce: 0.023667
2022-01-06 22:11:06,855 iteration 1076 : loss : 0.055266, loss_ce: 0.020480
2022-01-06 22:11:08,367 iteration 1077 : loss : 0.082499, loss_ce: 0.027630
2022-01-06 22:11:09,958 iteration 1078 : loss : 0.115629, loss_ce: 0.056546
2022-01-06 22:11:11,455 iteration 1079 : loss : 0.064337, loss_ce: 0.025476
2022-01-06 22:11:12,957 iteration 1080 : loss : 0.065690, loss_ce: 0.026939
2022-01-06 22:11:14,531 iteration 1081 : loss : 0.067290, loss_ce: 0.023218
2022-01-06 22:11:16,111 iteration 1082 : loss : 0.087050, loss_ce: 0.040112
2022-01-06 22:11:17,717 iteration 1083 : loss : 0.079156, loss_ce: 0.032621
2022-01-06 22:11:19,258 iteration 1084 : loss : 0.089932, loss_ce: 0.050793
2022-01-06 22:11:20,884 iteration 1085 : loss : 0.055853, loss_ce: 0.022319
2022-01-06 22:11:22,415 iteration 1086 : loss : 0.065239, loss_ce: 0.024984
2022-01-06 22:11:23,967 iteration 1087 : loss : 0.067949, loss_ce: 0.028878
2022-01-06 22:11:25,576 iteration 1088 : loss : 0.069199, loss_ce: 0.028905
 16%|████▊                         | 64/400 [30:12<2:30:37, 26.90s/it]2022-01-06 22:11:27,155 iteration 1089 : loss : 0.082090, loss_ce: 0.029752
2022-01-06 22:11:28,677 iteration 1090 : loss : 0.053035, loss_ce: 0.025360
2022-01-06 22:11:30,230 iteration 1091 : loss : 0.075878, loss_ce: 0.039383
2022-01-06 22:11:31,781 iteration 1092 : loss : 0.077406, loss_ce: 0.035367
2022-01-06 22:11:33,350 iteration 1093 : loss : 0.084739, loss_ce: 0.030733
2022-01-06 22:11:34,972 iteration 1094 : loss : 0.057060, loss_ce: 0.024261
2022-01-06 22:11:36,493 iteration 1095 : loss : 0.050746, loss_ce: 0.021113
2022-01-06 22:11:38,146 iteration 1096 : loss : 0.089883, loss_ce: 0.035203
2022-01-06 22:11:39,735 iteration 1097 : loss : 0.078546, loss_ce: 0.032956
2022-01-06 22:11:41,267 iteration 1098 : loss : 0.049799, loss_ce: 0.022390
2022-01-06 22:11:42,861 iteration 1099 : loss : 0.092503, loss_ce: 0.030088
2022-01-06 22:11:44,510 iteration 1100 : loss : 0.071548, loss_ce: 0.025256
2022-01-06 22:11:46,077 iteration 1101 : loss : 0.067997, loss_ce: 0.028773
2022-01-06 22:11:47,719 iteration 1102 : loss : 0.141935, loss_ce: 0.049022
2022-01-06 22:11:49,262 iteration 1103 : loss : 0.091097, loss_ce: 0.028866
2022-01-06 22:11:50,813 iteration 1104 : loss : 0.111521, loss_ce: 0.049173
2022-01-06 22:11:50,813 Training Data Eval:
2022-01-06 22:11:58,855   Average segmentation loss on training set: 0.1091
2022-01-06 22:11:58,856 Validation Data Eval:
2022-01-06 22:12:01,620   Average segmentation loss on validation set: 0.1977
2022-01-06 22:12:03,212 iteration 1105 : loss : 0.074682, loss_ce: 0.020396
 16%|████▉                         | 65/400 [30:49<2:48:09, 30.12s/it]2022-01-06 22:12:04,752 iteration 1106 : loss : 0.072597, loss_ce: 0.027621
2022-01-06 22:12:06,271 iteration 1107 : loss : 0.076236, loss_ce: 0.038752
2022-01-06 22:12:07,805 iteration 1108 : loss : 0.099962, loss_ce: 0.026835
2022-01-06 22:12:09,274 iteration 1109 : loss : 0.056039, loss_ce: 0.023688
2022-01-06 22:12:10,813 iteration 1110 : loss : 0.092594, loss_ce: 0.046593
2022-01-06 22:12:12,320 iteration 1111 : loss : 0.047079, loss_ce: 0.021043
2022-01-06 22:12:13,833 iteration 1112 : loss : 0.066793, loss_ce: 0.025631
2022-01-06 22:12:15,323 iteration 1113 : loss : 0.063347, loss_ce: 0.020554
2022-01-06 22:12:16,898 iteration 1114 : loss : 0.065413, loss_ce: 0.028562
2022-01-06 22:12:18,529 iteration 1115 : loss : 0.069633, loss_ce: 0.031634
2022-01-06 22:12:20,093 iteration 1116 : loss : 0.107221, loss_ce: 0.037492
2022-01-06 22:12:21,709 iteration 1117 : loss : 0.070360, loss_ce: 0.025451
2022-01-06 22:12:23,263 iteration 1118 : loss : 0.092116, loss_ce: 0.034113
2022-01-06 22:12:24,755 iteration 1119 : loss : 0.065180, loss_ce: 0.027780
2022-01-06 22:12:26,245 iteration 1120 : loss : 0.091936, loss_ce: 0.034974
2022-01-06 22:12:27,771 iteration 1121 : loss : 0.083037, loss_ce: 0.025641
2022-01-06 22:12:29,270 iteration 1122 : loss : 0.095083, loss_ce: 0.038260
 16%|████▉                         | 66/400 [31:16<2:40:52, 28.90s/it]2022-01-06 22:12:30,790 iteration 1123 : loss : 0.135713, loss_ce: 0.067777
2022-01-06 22:12:32,318 iteration 1124 : loss : 0.073255, loss_ce: 0.031705
2022-01-06 22:12:33,817 iteration 1125 : loss : 0.063501, loss_ce: 0.019558
2022-01-06 22:12:35,306 iteration 1126 : loss : 0.087184, loss_ce: 0.035464
2022-01-06 22:12:36,794 iteration 1127 : loss : 0.093855, loss_ce: 0.031894
2022-01-06 22:12:38,349 iteration 1128 : loss : 0.080698, loss_ce: 0.034639
2022-01-06 22:12:39,913 iteration 1129 : loss : 0.090485, loss_ce: 0.030477
2022-01-06 22:12:41,412 iteration 1130 : loss : 0.066651, loss_ce: 0.030608
2022-01-06 22:12:42,911 iteration 1131 : loss : 0.061227, loss_ce: 0.025414
2022-01-06 22:12:44,461 iteration 1132 : loss : 0.072428, loss_ce: 0.028112
2022-01-06 22:12:45,917 iteration 1133 : loss : 0.055889, loss_ce: 0.027116
2022-01-06 22:12:47,429 iteration 1134 : loss : 0.081660, loss_ce: 0.026820
2022-01-06 22:12:48,912 iteration 1135 : loss : 0.087256, loss_ce: 0.037188
2022-01-06 22:12:50,411 iteration 1136 : loss : 0.052864, loss_ce: 0.019529
2022-01-06 22:12:51,889 iteration 1137 : loss : 0.091235, loss_ce: 0.036180
2022-01-06 22:12:53,426 iteration 1138 : loss : 0.075540, loss_ce: 0.034619
2022-01-06 22:12:54,947 iteration 1139 : loss : 0.085054, loss_ce: 0.027498
 17%|█████                         | 67/400 [31:41<2:35:01, 27.93s/it]2022-01-06 22:12:56,499 iteration 1140 : loss : 0.061512, loss_ce: 0.022958
2022-01-06 22:12:57,964 iteration 1141 : loss : 0.085484, loss_ce: 0.039605
2022-01-06 22:12:59,535 iteration 1142 : loss : 0.070827, loss_ce: 0.031055
2022-01-06 22:13:01,036 iteration 1143 : loss : 0.066314, loss_ce: 0.024118
2022-01-06 22:13:02,606 iteration 1144 : loss : 0.129963, loss_ce: 0.071306
2022-01-06 22:13:04,115 iteration 1145 : loss : 0.052041, loss_ce: 0.019849
2022-01-06 22:13:05,687 iteration 1146 : loss : 0.106488, loss_ce: 0.048726
2022-01-06 22:13:07,167 iteration 1147 : loss : 0.057621, loss_ce: 0.022585
2022-01-06 22:13:08,767 iteration 1148 : loss : 0.109872, loss_ce: 0.035582
2022-01-06 22:13:10,265 iteration 1149 : loss : 0.063659, loss_ce: 0.030769
2022-01-06 22:13:11,808 iteration 1150 : loss : 0.063481, loss_ce: 0.027376
2022-01-06 22:13:13,368 iteration 1151 : loss : 0.097451, loss_ce: 0.034688
2022-01-06 22:13:15,011 iteration 1152 : loss : 0.075126, loss_ce: 0.032964
2022-01-06 22:13:16,459 iteration 1153 : loss : 0.073491, loss_ce: 0.028589
2022-01-06 22:13:18,011 iteration 1154 : loss : 0.110922, loss_ce: 0.042348
2022-01-06 22:13:19,496 iteration 1155 : loss : 0.080845, loss_ce: 0.025446
2022-01-06 22:13:21,058 iteration 1156 : loss : 0.106903, loss_ce: 0.049819
 17%|█████                         | 68/400 [32:07<2:31:31, 27.38s/it]2022-01-06 22:13:22,609 iteration 1157 : loss : 0.144663, loss_ce: 0.040093
2022-01-06 22:13:24,140 iteration 1158 : loss : 0.074551, loss_ce: 0.022807
2022-01-06 22:13:25,720 iteration 1159 : loss : 0.068761, loss_ce: 0.023576
2022-01-06 22:13:27,334 iteration 1160 : loss : 0.108664, loss_ce: 0.046881
2022-01-06 22:13:28,903 iteration 1161 : loss : 0.057877, loss_ce: 0.024505
2022-01-06 22:13:30,441 iteration 1162 : loss : 0.081090, loss_ce: 0.026147
2022-01-06 22:13:31,929 iteration 1163 : loss : 0.100020, loss_ce: 0.044144
2022-01-06 22:13:33,442 iteration 1164 : loss : 0.086315, loss_ce: 0.031135
2022-01-06 22:13:34,962 iteration 1165 : loss : 0.063655, loss_ce: 0.031905
2022-01-06 22:13:36,547 iteration 1166 : loss : 0.053133, loss_ce: 0.020138
2022-01-06 22:13:38,033 iteration 1167 : loss : 0.071762, loss_ce: 0.029855
2022-01-06 22:13:39,530 iteration 1168 : loss : 0.058802, loss_ce: 0.024642
2022-01-06 22:13:41,017 iteration 1169 : loss : 0.058843, loss_ce: 0.024108
2022-01-06 22:13:42,496 iteration 1170 : loss : 0.070692, loss_ce: 0.028101
2022-01-06 22:13:44,021 iteration 1171 : loss : 0.078661, loss_ce: 0.026470
2022-01-06 22:13:45,536 iteration 1172 : loss : 0.094226, loss_ce: 0.046094
2022-01-06 22:13:47,070 iteration 1173 : loss : 0.076327, loss_ce: 0.026452
 17%|█████▏                        | 69/400 [32:33<2:28:47, 26.97s/it]2022-01-06 22:13:48,605 iteration 1174 : loss : 0.075779, loss_ce: 0.024327
2022-01-06 22:13:50,102 iteration 1175 : loss : 0.049766, loss_ce: 0.020659
2022-01-06 22:13:51,595 iteration 1176 : loss : 0.070821, loss_ce: 0.030632
2022-01-06 22:13:53,068 iteration 1177 : loss : 0.071910, loss_ce: 0.027126
2022-01-06 22:13:54,608 iteration 1178 : loss : 0.061444, loss_ce: 0.021010
2022-01-06 22:13:56,188 iteration 1179 : loss : 0.064984, loss_ce: 0.026862
2022-01-06 22:13:57,658 iteration 1180 : loss : 0.053565, loss_ce: 0.021813
2022-01-06 22:13:59,150 iteration 1181 : loss : 0.064878, loss_ce: 0.025442
2022-01-06 22:14:00,661 iteration 1182 : loss : 0.080286, loss_ce: 0.032034
2022-01-06 22:14:02,119 iteration 1183 : loss : 0.071125, loss_ce: 0.025482
2022-01-06 22:14:03,583 iteration 1184 : loss : 0.056641, loss_ce: 0.025163
2022-01-06 22:14:05,126 iteration 1185 : loss : 0.070668, loss_ce: 0.026726
2022-01-06 22:14:06,591 iteration 1186 : loss : 0.058979, loss_ce: 0.028188
2022-01-06 22:14:08,149 iteration 1187 : loss : 0.069786, loss_ce: 0.022806
2022-01-06 22:14:09,612 iteration 1188 : loss : 0.100517, loss_ce: 0.036345
2022-01-06 22:14:11,185 iteration 1189 : loss : 0.076405, loss_ce: 0.034379
2022-01-06 22:14:11,185 Training Data Eval:
2022-01-06 22:14:18,932   Average segmentation loss on training set: 0.1997
2022-01-06 22:14:18,932 Validation Data Eval:
2022-01-06 22:14:21,616   Average segmentation loss on validation set: 0.1886
2022-01-06 22:14:23,131 iteration 1190 : loss : 0.067934, loss_ce: 0.030604
 18%|█████▎                        | 70/400 [33:09<2:43:21, 29.70s/it]2022-01-06 22:14:24,768 iteration 1191 : loss : 0.078179, loss_ce: 0.032147
2022-01-06 22:14:26,245 iteration 1192 : loss : 0.052859, loss_ce: 0.025791
2022-01-06 22:14:27,837 iteration 1193 : loss : 0.082817, loss_ce: 0.027691
2022-01-06 22:14:29,336 iteration 1194 : loss : 0.065175, loss_ce: 0.022552
2022-01-06 22:14:30,799 iteration 1195 : loss : 0.048848, loss_ce: 0.021830
2022-01-06 22:14:32,270 iteration 1196 : loss : 0.080924, loss_ce: 0.036344
2022-01-06 22:14:33,706 iteration 1197 : loss : 0.062018, loss_ce: 0.024312
2022-01-06 22:14:35,252 iteration 1198 : loss : 0.054522, loss_ce: 0.026279
2022-01-06 22:14:36,766 iteration 1199 : loss : 0.054429, loss_ce: 0.020545
2022-01-06 22:14:38,259 iteration 1200 : loss : 0.083704, loss_ce: 0.026959
2022-01-06 22:14:39,712 iteration 1201 : loss : 0.050517, loss_ce: 0.022399
2022-01-06 22:14:41,245 iteration 1202 : loss : 0.088604, loss_ce: 0.036435
2022-01-06 22:14:42,748 iteration 1203 : loss : 0.082248, loss_ce: 0.026860
2022-01-06 22:14:44,248 iteration 1204 : loss : 0.065609, loss_ce: 0.023708
2022-01-06 22:14:45,769 iteration 1205 : loss : 0.109793, loss_ce: 0.054343
2022-01-06 22:14:47,281 iteration 1206 : loss : 0.057850, loss_ce: 0.022713
2022-01-06 22:14:48,834 iteration 1207 : loss : 0.049851, loss_ce: 0.015553
 18%|█████▎                        | 71/400 [33:35<2:36:17, 28.50s/it]2022-01-06 22:14:50,422 iteration 1208 : loss : 0.069944, loss_ce: 0.030924
2022-01-06 22:14:51,926 iteration 1209 : loss : 0.057888, loss_ce: 0.023457
2022-01-06 22:14:53,432 iteration 1210 : loss : 0.057026, loss_ce: 0.022735
2022-01-06 22:14:54,890 iteration 1211 : loss : 0.054776, loss_ce: 0.023510
2022-01-06 22:14:56,328 iteration 1212 : loss : 0.075942, loss_ce: 0.023963
2022-01-06 22:14:57,935 iteration 1213 : loss : 0.061292, loss_ce: 0.027248
2022-01-06 22:14:59,433 iteration 1214 : loss : 0.063181, loss_ce: 0.024955
2022-01-06 22:15:00,931 iteration 1215 : loss : 0.082629, loss_ce: 0.033621
2022-01-06 22:15:02,537 iteration 1216 : loss : 0.091293, loss_ce: 0.038352
2022-01-06 22:15:04,039 iteration 1217 : loss : 0.043430, loss_ce: 0.014668
2022-01-06 22:15:05,492 iteration 1218 : loss : 0.056320, loss_ce: 0.021587
2022-01-06 22:15:06,929 iteration 1219 : loss : 0.046166, loss_ce: 0.015251
2022-01-06 22:15:08,413 iteration 1220 : loss : 0.062861, loss_ce: 0.021231
2022-01-06 22:15:09,873 iteration 1221 : loss : 0.075204, loss_ce: 0.030029
2022-01-06 22:15:11,427 iteration 1222 : loss : 0.103326, loss_ce: 0.036947
2022-01-06 22:15:12,906 iteration 1223 : loss : 0.075864, loss_ce: 0.034341
2022-01-06 22:15:14,463 iteration 1224 : loss : 0.102269, loss_ce: 0.042119
 18%|█████▍                        | 72/400 [34:01<2:31:05, 27.64s/it]2022-01-06 22:15:16,005 iteration 1225 : loss : 0.066111, loss_ce: 0.024445
2022-01-06 22:15:17,542 iteration 1226 : loss : 0.094169, loss_ce: 0.043354
2022-01-06 22:15:19,002 iteration 1227 : loss : 0.066805, loss_ce: 0.025168
2022-01-06 22:15:20,494 iteration 1228 : loss : 0.064824, loss_ce: 0.023573
2022-01-06 22:15:21,934 iteration 1229 : loss : 0.075178, loss_ce: 0.022586
2022-01-06 22:15:23,397 iteration 1230 : loss : 0.072009, loss_ce: 0.028706
2022-01-06 22:15:24,930 iteration 1231 : loss : 0.065786, loss_ce: 0.028560
2022-01-06 22:15:26,403 iteration 1232 : loss : 0.062040, loss_ce: 0.024384
2022-01-06 22:15:27,901 iteration 1233 : loss : 0.048770, loss_ce: 0.020111
2022-01-06 22:15:29,456 iteration 1234 : loss : 0.066850, loss_ce: 0.028429
2022-01-06 22:15:31,006 iteration 1235 : loss : 0.094759, loss_ce: 0.033342
2022-01-06 22:15:32,487 iteration 1236 : loss : 0.059800, loss_ce: 0.024621
2022-01-06 22:15:33,927 iteration 1237 : loss : 0.083497, loss_ce: 0.033374
2022-01-06 22:15:35,486 iteration 1238 : loss : 0.056078, loss_ce: 0.018176
2022-01-06 22:15:36,951 iteration 1239 : loss : 0.044465, loss_ce: 0.016573
2022-01-06 22:15:38,424 iteration 1240 : loss : 0.053628, loss_ce: 0.022395
2022-01-06 22:15:39,895 iteration 1241 : loss : 0.064523, loss_ce: 0.027917
 18%|█████▍                        | 73/400 [34:26<2:27:01, 26.98s/it]2022-01-06 22:15:41,480 iteration 1242 : loss : 0.069319, loss_ce: 0.037179
2022-01-06 22:15:42,958 iteration 1243 : loss : 0.071970, loss_ce: 0.025353
2022-01-06 22:15:44,440 iteration 1244 : loss : 0.060718, loss_ce: 0.023947
2022-01-06 22:15:45,973 iteration 1245 : loss : 0.072586, loss_ce: 0.022488
2022-01-06 22:15:47,495 iteration 1246 : loss : 0.070312, loss_ce: 0.021079
2022-01-06 22:15:48,970 iteration 1247 : loss : 0.071827, loss_ce: 0.028932
2022-01-06 22:15:50,493 iteration 1248 : loss : 0.053380, loss_ce: 0.016557
2022-01-06 22:15:51,962 iteration 1249 : loss : 0.053430, loss_ce: 0.015709
2022-01-06 22:15:53,555 iteration 1250 : loss : 0.065734, loss_ce: 0.024922
2022-01-06 22:15:55,185 iteration 1251 : loss : 0.097049, loss_ce: 0.051038
2022-01-06 22:15:56,748 iteration 1252 : loss : 0.050555, loss_ce: 0.020970
2022-01-06 22:15:58,185 iteration 1253 : loss : 0.049902, loss_ce: 0.017441
2022-01-06 22:15:59,702 iteration 1254 : loss : 0.070452, loss_ce: 0.035398
2022-01-06 22:16:01,221 iteration 1255 : loss : 0.061957, loss_ce: 0.026989
2022-01-06 22:16:02,722 iteration 1256 : loss : 0.070172, loss_ce: 0.029840
2022-01-06 22:16:04,239 iteration 1257 : loss : 0.081383, loss_ce: 0.024615
2022-01-06 22:16:05,770 iteration 1258 : loss : 0.056301, loss_ce: 0.026390
 18%|█████▌                        | 74/400 [34:52<2:24:46, 26.65s/it]2022-01-06 22:16:07,292 iteration 1259 : loss : 0.057143, loss_ce: 0.025062
2022-01-06 22:16:08,848 iteration 1260 : loss : 0.064818, loss_ce: 0.030332
2022-01-06 22:16:10,338 iteration 1261 : loss : 0.061477, loss_ce: 0.024923
2022-01-06 22:16:11,929 iteration 1262 : loss : 0.066306, loss_ce: 0.027672
2022-01-06 22:16:13,548 iteration 1263 : loss : 0.094795, loss_ce: 0.033446
2022-01-06 22:16:15,051 iteration 1264 : loss : 0.060082, loss_ce: 0.026477
2022-01-06 22:16:16,622 iteration 1265 : loss : 0.058718, loss_ce: 0.029693
2022-01-06 22:16:18,186 iteration 1266 : loss : 0.070461, loss_ce: 0.025163
2022-01-06 22:16:19,754 iteration 1267 : loss : 0.084681, loss_ce: 0.032016
2022-01-06 22:16:21,250 iteration 1268 : loss : 0.077521, loss_ce: 0.027957
2022-01-06 22:16:22,839 iteration 1269 : loss : 0.106435, loss_ce: 0.030311
2022-01-06 22:16:24,378 iteration 1270 : loss : 0.082384, loss_ce: 0.036304
2022-01-06 22:16:25,889 iteration 1271 : loss : 0.067472, loss_ce: 0.030587
2022-01-06 22:16:27,315 iteration 1272 : loss : 0.070744, loss_ce: 0.023266
2022-01-06 22:16:28,792 iteration 1273 : loss : 0.086397, loss_ce: 0.026863
2022-01-06 22:16:30,308 iteration 1274 : loss : 0.059043, loss_ce: 0.019198
2022-01-06 22:16:30,309 Training Data Eval:
2022-01-06 22:16:38,116   Average segmentation loss on training set: 0.1059
2022-01-06 22:16:38,117 Validation Data Eval:
2022-01-06 22:16:40,832   Average segmentation loss on validation set: 0.2412
2022-01-06 22:16:42,334 iteration 1275 : loss : 0.063342, loss_ce: 0.021415
 19%|█████▋                        | 75/400 [35:29<2:40:26, 29.62s/it]2022-01-06 22:16:43,826 iteration 1276 : loss : 0.066157, loss_ce: 0.026235
2022-01-06 22:16:45,440 iteration 1277 : loss : 0.058464, loss_ce: 0.021674
2022-01-06 22:16:46,902 iteration 1278 : loss : 0.042986, loss_ce: 0.014897
2022-01-06 22:16:48,493 iteration 1279 : loss : 0.047651, loss_ce: 0.018177
2022-01-06 22:16:50,017 iteration 1280 : loss : 0.109573, loss_ce: 0.058855
2022-01-06 22:16:51,550 iteration 1281 : loss : 0.107121, loss_ce: 0.044869
2022-01-06 22:16:53,028 iteration 1282 : loss : 0.039682, loss_ce: 0.018509
2022-01-06 22:16:54,620 iteration 1283 : loss : 0.080491, loss_ce: 0.027965
2022-01-06 22:16:56,138 iteration 1284 : loss : 0.069734, loss_ce: 0.027162
2022-01-06 22:16:57,758 iteration 1285 : loss : 0.059893, loss_ce: 0.022673
2022-01-06 22:16:59,404 iteration 1286 : loss : 0.133470, loss_ce: 0.038532
2022-01-06 22:17:00,954 iteration 1287 : loss : 0.048569, loss_ce: 0.024967
2022-01-06 22:17:02,490 iteration 1288 : loss : 0.085621, loss_ce: 0.041321
2022-01-06 22:17:04,058 iteration 1289 : loss : 0.074388, loss_ce: 0.033989
2022-01-06 22:17:05,656 iteration 1290 : loss : 0.089411, loss_ce: 0.031410
2022-01-06 22:17:07,267 iteration 1291 : loss : 0.083551, loss_ce: 0.040025
2022-01-06 22:17:08,895 iteration 1292 : loss : 0.074177, loss_ce: 0.028590
 19%|█████▋                        | 76/400 [35:55<2:34:59, 28.70s/it]2022-01-06 22:17:10,548 iteration 1293 : loss : 0.051689, loss_ce: 0.017685
2022-01-06 22:17:12,147 iteration 1294 : loss : 0.077493, loss_ce: 0.028533
2022-01-06 22:17:13,798 iteration 1295 : loss : 0.078954, loss_ce: 0.021741
2022-01-06 22:17:15,403 iteration 1296 : loss : 0.084164, loss_ce: 0.030020
2022-01-06 22:17:16,940 iteration 1297 : loss : 0.067053, loss_ce: 0.027792
2022-01-06 22:17:18,444 iteration 1298 : loss : 0.057255, loss_ce: 0.021536
2022-01-06 22:17:20,012 iteration 1299 : loss : 0.100560, loss_ce: 0.056841
2022-01-06 22:17:21,572 iteration 1300 : loss : 0.075500, loss_ce: 0.028333
2022-01-06 22:17:23,159 iteration 1301 : loss : 0.054219, loss_ce: 0.020432
2022-01-06 22:17:24,703 iteration 1302 : loss : 0.074494, loss_ce: 0.031426
2022-01-06 22:17:26,247 iteration 1303 : loss : 0.065369, loss_ce: 0.026834
2022-01-06 22:17:27,813 iteration 1304 : loss : 0.083623, loss_ce: 0.033262
2022-01-06 22:17:29,370 iteration 1305 : loss : 0.069483, loss_ce: 0.029885
2022-01-06 22:17:30,959 iteration 1306 : loss : 0.115566, loss_ce: 0.039150
2022-01-06 22:17:32,553 iteration 1307 : loss : 0.056109, loss_ce: 0.027344
2022-01-06 22:17:34,117 iteration 1308 : loss : 0.098968, loss_ce: 0.028790
2022-01-06 22:17:35,614 iteration 1309 : loss : 0.060245, loss_ce: 0.020968
 19%|█████▊                        | 77/400 [36:22<2:31:20, 28.11s/it]2022-01-06 22:17:37,330 iteration 1310 : loss : 0.070564, loss_ce: 0.026153
2022-01-06 22:17:38,837 iteration 1311 : loss : 0.063460, loss_ce: 0.027231
2022-01-06 22:17:40,344 iteration 1312 : loss : 0.040281, loss_ce: 0.014544
2022-01-06 22:17:41,843 iteration 1313 : loss : 0.104859, loss_ce: 0.028508
2022-01-06 22:17:43,375 iteration 1314 : loss : 0.076802, loss_ce: 0.032237
2022-01-06 22:17:44,915 iteration 1315 : loss : 0.077047, loss_ce: 0.031794
2022-01-06 22:17:46,537 iteration 1316 : loss : 0.080051, loss_ce: 0.033684
2022-01-06 22:17:48,041 iteration 1317 : loss : 0.090545, loss_ce: 0.048566
2022-01-06 22:17:49,630 iteration 1318 : loss : 0.174133, loss_ce: 0.033024
2022-01-06 22:17:51,171 iteration 1319 : loss : 0.057432, loss_ce: 0.026417
2022-01-06 22:17:52,670 iteration 1320 : loss : 0.053735, loss_ce: 0.020065
2022-01-06 22:17:54,198 iteration 1321 : loss : 0.078381, loss_ce: 0.021892
2022-01-06 22:17:55,666 iteration 1322 : loss : 0.067757, loss_ce: 0.032287
2022-01-06 22:17:57,164 iteration 1323 : loss : 0.064064, loss_ce: 0.021950
2022-01-06 22:17:58,699 iteration 1324 : loss : 0.057390, loss_ce: 0.022456
2022-01-06 22:18:00,274 iteration 1325 : loss : 0.104981, loss_ce: 0.032433
2022-01-06 22:18:01,698 iteration 1326 : loss : 0.058886, loss_ce: 0.021610
 20%|█████▊                        | 78/400 [36:48<2:27:35, 27.50s/it]2022-01-06 22:18:03,236 iteration 1327 : loss : 0.071839, loss_ce: 0.023369
2022-01-06 22:18:04,777 iteration 1328 : loss : 0.090254, loss_ce: 0.041326
2022-01-06 22:18:06,276 iteration 1329 : loss : 0.046926, loss_ce: 0.017933
2022-01-06 22:18:07,760 iteration 1330 : loss : 0.106454, loss_ce: 0.030324
2022-01-06 22:18:09,213 iteration 1331 : loss : 0.074613, loss_ce: 0.036680
2022-01-06 22:18:10,686 iteration 1332 : loss : 0.080228, loss_ce: 0.032071
2022-01-06 22:18:12,217 iteration 1333 : loss : 0.068645, loss_ce: 0.019804
2022-01-06 22:18:13,745 iteration 1334 : loss : 0.059629, loss_ce: 0.022050
2022-01-06 22:18:15,241 iteration 1335 : loss : 0.048647, loss_ce: 0.018718
2022-01-06 22:18:16,759 iteration 1336 : loss : 0.050952, loss_ce: 0.020030
2022-01-06 22:18:18,342 iteration 1337 : loss : 0.082039, loss_ce: 0.031118
2022-01-06 22:18:19,906 iteration 1338 : loss : 0.062074, loss_ce: 0.021618
2022-01-06 22:18:21,460 iteration 1339 : loss : 0.061628, loss_ce: 0.027545
2022-01-06 22:18:23,005 iteration 1340 : loss : 0.052938, loss_ce: 0.023184
2022-01-06 22:18:24,529 iteration 1341 : loss : 0.065207, loss_ce: 0.025062
2022-01-06 22:18:26,034 iteration 1342 : loss : 0.087752, loss_ce: 0.036108
2022-01-06 22:18:27,507 iteration 1343 : loss : 0.047428, loss_ce: 0.016694
 20%|█████▉                        | 79/400 [37:14<2:24:24, 26.99s/it]2022-01-06 22:18:29,087 iteration 1344 : loss : 0.053373, loss_ce: 0.022185
2022-01-06 22:18:30,665 iteration 1345 : loss : 0.078401, loss_ce: 0.028359
2022-01-06 22:18:32,159 iteration 1346 : loss : 0.082809, loss_ce: 0.037423
2022-01-06 22:18:33,738 iteration 1347 : loss : 0.045468, loss_ce: 0.018288
2022-01-06 22:18:35,341 iteration 1348 : loss : 0.071619, loss_ce: 0.029026
2022-01-06 22:18:36,852 iteration 1349 : loss : 0.057446, loss_ce: 0.022896
2022-01-06 22:18:38,365 iteration 1350 : loss : 0.079307, loss_ce: 0.033858
2022-01-06 22:18:39,869 iteration 1351 : loss : 0.053895, loss_ce: 0.024464
2022-01-06 22:18:41,429 iteration 1352 : loss : 0.054751, loss_ce: 0.024158
2022-01-06 22:18:42,960 iteration 1353 : loss : 0.065357, loss_ce: 0.026388
2022-01-06 22:18:44,517 iteration 1354 : loss : 0.079804, loss_ce: 0.026942
2022-01-06 22:18:46,122 iteration 1355 : loss : 0.140949, loss_ce: 0.044096
2022-01-06 22:18:47,650 iteration 1356 : loss : 0.062659, loss_ce: 0.027760
2022-01-06 22:18:49,194 iteration 1357 : loss : 0.047214, loss_ce: 0.018958
2022-01-06 22:18:50,751 iteration 1358 : loss : 0.060648, loss_ce: 0.023337
2022-01-06 22:18:52,249 iteration 1359 : loss : 0.057672, loss_ce: 0.023582
2022-01-06 22:18:52,250 Training Data Eval:
2022-01-06 22:19:00,029   Average segmentation loss on training set: 0.0778
2022-01-06 22:19:00,029 Validation Data Eval:
2022-01-06 22:19:02,729   Average segmentation loss on validation set: 0.1527
2022-01-06 22:19:04,308 iteration 1360 : loss : 0.052857, loss_ce: 0.015039
 20%|██████                        | 80/400 [37:51<2:39:39, 29.94s/it]2022-01-06 22:19:05,977 iteration 1361 : loss : 0.064848, loss_ce: 0.029421
2022-01-06 22:19:07,518 iteration 1362 : loss : 0.064108, loss_ce: 0.024076
2022-01-06 22:19:09,196 iteration 1363 : loss : 0.084203, loss_ce: 0.033379
2022-01-06 22:19:10,613 iteration 1364 : loss : 0.063297, loss_ce: 0.016573
2022-01-06 22:19:12,206 iteration 1365 : loss : 0.093994, loss_ce: 0.039848
2022-01-06 22:19:13,734 iteration 1366 : loss : 0.073126, loss_ce: 0.026638
2022-01-06 22:19:15,242 iteration 1367 : loss : 0.077577, loss_ce: 0.029123
2022-01-06 22:19:16,762 iteration 1368 : loss : 0.057506, loss_ce: 0.027085
2022-01-06 22:19:18,253 iteration 1369 : loss : 0.070381, loss_ce: 0.023417
2022-01-06 22:19:19,814 iteration 1370 : loss : 0.061657, loss_ce: 0.026953
2022-01-06 22:19:21,314 iteration 1371 : loss : 0.056582, loss_ce: 0.022608
2022-01-06 22:19:22,804 iteration 1372 : loss : 0.080207, loss_ce: 0.032194
2022-01-06 22:19:24,294 iteration 1373 : loss : 0.076785, loss_ce: 0.030201
2022-01-06 22:19:25,840 iteration 1374 : loss : 0.077287, loss_ce: 0.027049
2022-01-06 22:19:27,379 iteration 1375 : loss : 0.070544, loss_ce: 0.035434
2022-01-06 22:19:28,906 iteration 1376 : loss : 0.092306, loss_ce: 0.032992
2022-01-06 22:19:30,420 iteration 1377 : loss : 0.048241, loss_ce: 0.019517
 20%|██████                        | 81/400 [38:17<2:33:03, 28.79s/it]2022-01-06 22:19:31,932 iteration 1378 : loss : 0.092869, loss_ce: 0.039939
2022-01-06 22:19:33,493 iteration 1379 : loss : 0.074777, loss_ce: 0.027267
2022-01-06 22:19:35,039 iteration 1380 : loss : 0.065719, loss_ce: 0.028024
2022-01-06 22:19:36,642 iteration 1381 : loss : 0.060952, loss_ce: 0.025854
2022-01-06 22:19:38,106 iteration 1382 : loss : 0.061734, loss_ce: 0.021833
2022-01-06 22:19:39,706 iteration 1383 : loss : 0.058309, loss_ce: 0.022052
2022-01-06 22:19:41,236 iteration 1384 : loss : 0.069083, loss_ce: 0.026719
2022-01-06 22:19:42,702 iteration 1385 : loss : 0.080686, loss_ce: 0.031785
2022-01-06 22:19:44,205 iteration 1386 : loss : 0.046244, loss_ce: 0.017141
2022-01-06 22:19:45,710 iteration 1387 : loss : 0.056932, loss_ce: 0.022514
2022-01-06 22:19:47,234 iteration 1388 : loss : 0.083671, loss_ce: 0.026136
2022-01-06 22:19:48,760 iteration 1389 : loss : 0.062655, loss_ce: 0.022607
2022-01-06 22:19:50,332 iteration 1390 : loss : 0.052743, loss_ce: 0.021871
2022-01-06 22:19:51,916 iteration 1391 : loss : 0.061626, loss_ce: 0.020251
2022-01-06 22:19:53,463 iteration 1392 : loss : 0.075207, loss_ce: 0.038018
2022-01-06 22:19:54,961 iteration 1393 : loss : 0.056912, loss_ce: 0.021595
2022-01-06 22:19:56,555 iteration 1394 : loss : 0.057849, loss_ce: 0.024526
 20%|██████▏                       | 82/400 [38:43<2:28:21, 27.99s/it]2022-01-06 22:19:58,159 iteration 1395 : loss : 0.058218, loss_ce: 0.023625
2022-01-06 22:19:59,709 iteration 1396 : loss : 0.067029, loss_ce: 0.031776
2022-01-06 22:20:01,336 iteration 1397 : loss : 0.062368, loss_ce: 0.022712
2022-01-06 22:20:02,973 iteration 1398 : loss : 0.060791, loss_ce: 0.025971
2022-01-06 22:20:04,475 iteration 1399 : loss : 0.052975, loss_ce: 0.023296
2022-01-06 22:20:05,970 iteration 1400 : loss : 0.055456, loss_ce: 0.018761
2022-01-06 22:20:07,589 iteration 1401 : loss : 0.090234, loss_ce: 0.031623
2022-01-06 22:20:09,168 iteration 1402 : loss : 0.080056, loss_ce: 0.030809
2022-01-06 22:20:10,686 iteration 1403 : loss : 0.067324, loss_ce: 0.027086
2022-01-06 22:20:12,204 iteration 1404 : loss : 0.033286, loss_ce: 0.015336
2022-01-06 22:20:13,765 iteration 1405 : loss : 0.063549, loss_ce: 0.024380
2022-01-06 22:20:15,335 iteration 1406 : loss : 0.083305, loss_ce: 0.031499
2022-01-06 22:20:16,941 iteration 1407 : loss : 0.043386, loss_ce: 0.017316
2022-01-06 22:20:18,500 iteration 1408 : loss : 0.062357, loss_ce: 0.020710
2022-01-06 22:20:20,061 iteration 1409 : loss : 0.089636, loss_ce: 0.032849
2022-01-06 22:20:21,678 iteration 1410 : loss : 0.053979, loss_ce: 0.021521
2022-01-06 22:20:23,198 iteration 1411 : loss : 0.055656, loss_ce: 0.019280
 21%|██████▏                       | 83/400 [39:09<2:25:45, 27.59s/it]2022-01-06 22:20:24,767 iteration 1412 : loss : 0.055001, loss_ce: 0.021680
2022-01-06 22:20:26,366 iteration 1413 : loss : 0.092233, loss_ce: 0.032853
2022-01-06 22:20:27,876 iteration 1414 : loss : 0.055654, loss_ce: 0.020357
2022-01-06 22:20:29,441 iteration 1415 : loss : 0.058136, loss_ce: 0.015935
2022-01-06 22:20:30,945 iteration 1416 : loss : 0.058953, loss_ce: 0.022741
2022-01-06 22:20:32,443 iteration 1417 : loss : 0.083911, loss_ce: 0.041922
2022-01-06 22:20:33,943 iteration 1418 : loss : 0.066952, loss_ce: 0.029198
2022-01-06 22:20:35,416 iteration 1419 : loss : 0.053923, loss_ce: 0.020898
2022-01-06 22:20:36,980 iteration 1420 : loss : 0.076781, loss_ce: 0.032044
2022-01-06 22:20:38,584 iteration 1421 : loss : 0.070852, loss_ce: 0.030553
2022-01-06 22:20:40,138 iteration 1422 : loss : 0.047526, loss_ce: 0.015853
2022-01-06 22:20:41,618 iteration 1423 : loss : 0.058655, loss_ce: 0.023249
2022-01-06 22:20:43,189 iteration 1424 : loss : 0.056744, loss_ce: 0.024228
2022-01-06 22:20:44,753 iteration 1425 : loss : 0.029512, loss_ce: 0.012103
2022-01-06 22:20:46,251 iteration 1426 : loss : 0.062951, loss_ce: 0.027233
2022-01-06 22:20:47,783 iteration 1427 : loss : 0.083542, loss_ce: 0.033450
2022-01-06 22:20:49,233 iteration 1428 : loss : 0.076544, loss_ce: 0.029044
 21%|██████▎                       | 84/400 [39:35<2:22:51, 27.12s/it]2022-01-06 22:20:50,886 iteration 1429 : loss : 0.079806, loss_ce: 0.036362
2022-01-06 22:20:52,371 iteration 1430 : loss : 0.142457, loss_ce: 0.038864
2022-01-06 22:20:53,902 iteration 1431 : loss : 0.081444, loss_ce: 0.035979
2022-01-06 22:20:55,445 iteration 1432 : loss : 0.057252, loss_ce: 0.024820
2022-01-06 22:20:57,018 iteration 1433 : loss : 0.071011, loss_ce: 0.027318
2022-01-06 22:20:58,516 iteration 1434 : loss : 0.055096, loss_ce: 0.021455
2022-01-06 22:21:00,056 iteration 1435 : loss : 0.057523, loss_ce: 0.024851
2022-01-06 22:21:01,617 iteration 1436 : loss : 0.078673, loss_ce: 0.030832
2022-01-06 22:21:03,171 iteration 1437 : loss : 0.067617, loss_ce: 0.026865
2022-01-06 22:21:04,695 iteration 1438 : loss : 0.047700, loss_ce: 0.021160
2022-01-06 22:21:06,255 iteration 1439 : loss : 0.065137, loss_ce: 0.023842
2022-01-06 22:21:07,794 iteration 1440 : loss : 0.055312, loss_ce: 0.023807
2022-01-06 22:21:09,287 iteration 1441 : loss : 0.050249, loss_ce: 0.021968
2022-01-06 22:21:10,809 iteration 1442 : loss : 0.050118, loss_ce: 0.020405
2022-01-06 22:21:12,355 iteration 1443 : loss : 0.078705, loss_ce: 0.029438
2022-01-06 22:21:13,877 iteration 1444 : loss : 0.113531, loss_ce: 0.038455
2022-01-06 22:21:13,878 Training Data Eval:
2022-01-06 22:21:21,688   Average segmentation loss on training set: 0.0558
2022-01-06 22:21:21,688 Validation Data Eval:
2022-01-06 22:21:24,378   Average segmentation loss on validation set: 0.1256
2022-01-06 22:21:30,112 Found new lowest validation loss at iteration 1444! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 22:21:31,616 iteration 1445 : loss : 0.077932, loss_ce: 0.025104
 21%|██████▍                       | 85/400 [40:18<2:46:25, 31.70s/it]2022-01-06 22:21:33,199 iteration 1446 : loss : 0.053640, loss_ce: 0.017686
2022-01-06 22:21:34,629 iteration 1447 : loss : 0.068881, loss_ce: 0.022679
2022-01-06 22:21:36,011 iteration 1448 : loss : 0.066258, loss_ce: 0.036772
2022-01-06 22:21:37,478 iteration 1449 : loss : 0.068953, loss_ce: 0.023430
2022-01-06 22:21:39,021 iteration 1450 : loss : 0.079664, loss_ce: 0.040955
2022-01-06 22:21:40,575 iteration 1451 : loss : 0.094180, loss_ce: 0.035568
2022-01-06 22:21:41,985 iteration 1452 : loss : 0.065146, loss_ce: 0.019409
2022-01-06 22:21:43,406 iteration 1453 : loss : 0.088685, loss_ce: 0.036413
2022-01-06 22:21:44,870 iteration 1454 : loss : 0.113955, loss_ce: 0.042088
2022-01-06 22:21:46,320 iteration 1455 : loss : 0.048318, loss_ce: 0.020473
2022-01-06 22:21:47,900 iteration 1456 : loss : 0.116022, loss_ce: 0.033740
2022-01-06 22:21:49,407 iteration 1457 : loss : 0.076243, loss_ce: 0.034877
2022-01-06 22:21:50,916 iteration 1458 : loss : 0.055093, loss_ce: 0.017980
2022-01-06 22:21:52,409 iteration 1459 : loss : 0.069761, loss_ce: 0.020039
2022-01-06 22:21:53,986 iteration 1460 : loss : 0.105966, loss_ce: 0.050262
2022-01-06 22:21:55,507 iteration 1461 : loss : 0.088576, loss_ce: 0.034609
2022-01-06 22:21:57,072 iteration 1462 : loss : 0.048931, loss_ce: 0.021013
 22%|██████▍                       | 86/400 [40:43<2:36:05, 29.83s/it]2022-01-06 22:21:58,639 iteration 1463 : loss : 0.062128, loss_ce: 0.025188
2022-01-06 22:22:00,145 iteration 1464 : loss : 0.066824, loss_ce: 0.026997
2022-01-06 22:22:01,700 iteration 1465 : loss : 0.078168, loss_ce: 0.022905
2022-01-06 22:22:03,255 iteration 1466 : loss : 0.057443, loss_ce: 0.022814
2022-01-06 22:22:04,795 iteration 1467 : loss : 0.058945, loss_ce: 0.025047
2022-01-06 22:22:06,304 iteration 1468 : loss : 0.048137, loss_ce: 0.021106
2022-01-06 22:22:07,830 iteration 1469 : loss : 0.073323, loss_ce: 0.030953
2022-01-06 22:22:09,425 iteration 1470 : loss : 0.104820, loss_ce: 0.039817
2022-01-06 22:22:10,950 iteration 1471 : loss : 0.074533, loss_ce: 0.039978
2022-01-06 22:22:12,364 iteration 1472 : loss : 0.058053, loss_ce: 0.020551
2022-01-06 22:22:13,959 iteration 1473 : loss : 0.072345, loss_ce: 0.026712
2022-01-06 22:22:15,509 iteration 1474 : loss : 0.071848, loss_ce: 0.023899
2022-01-06 22:22:17,020 iteration 1475 : loss : 0.066983, loss_ce: 0.025674
2022-01-06 22:22:18,609 iteration 1476 : loss : 0.112919, loss_ce: 0.039455
2022-01-06 22:22:20,157 iteration 1477 : loss : 0.067768, loss_ce: 0.022735
2022-01-06 22:22:21,721 iteration 1478 : loss : 0.063869, loss_ce: 0.024262
2022-01-06 22:22:23,187 iteration 1479 : loss : 0.080599, loss_ce: 0.025155
 22%|██████▌                       | 87/400 [41:09<2:29:47, 28.71s/it]2022-01-06 22:22:24,747 iteration 1480 : loss : 0.073791, loss_ce: 0.028162
2022-01-06 22:22:26,219 iteration 1481 : loss : 0.056117, loss_ce: 0.022408
2022-01-06 22:22:27,720 iteration 1482 : loss : 0.045075, loss_ce: 0.018118
2022-01-06 22:22:29,304 iteration 1483 : loss : 0.049674, loss_ce: 0.017511
2022-01-06 22:22:30,942 iteration 1484 : loss : 0.078949, loss_ce: 0.028874
2022-01-06 22:22:32,519 iteration 1485 : loss : 0.065739, loss_ce: 0.028264
2022-01-06 22:22:34,019 iteration 1486 : loss : 0.062682, loss_ce: 0.024349
2022-01-06 22:22:35,541 iteration 1487 : loss : 0.059984, loss_ce: 0.026271
2022-01-06 22:22:37,078 iteration 1488 : loss : 0.061134, loss_ce: 0.025886
2022-01-06 22:22:38,627 iteration 1489 : loss : 0.044485, loss_ce: 0.017048
2022-01-06 22:22:40,189 iteration 1490 : loss : 0.063546, loss_ce: 0.026841
2022-01-06 22:22:41,802 iteration 1491 : loss : 0.067608, loss_ce: 0.027020
2022-01-06 22:22:43,337 iteration 1492 : loss : 0.050312, loss_ce: 0.019267
2022-01-06 22:22:44,953 iteration 1493 : loss : 0.066288, loss_ce: 0.023369
2022-01-06 22:22:46,498 iteration 1494 : loss : 0.056539, loss_ce: 0.023091
2022-01-06 22:22:48,054 iteration 1495 : loss : 0.072203, loss_ce: 0.027919
2022-01-06 22:22:49,631 iteration 1496 : loss : 0.114015, loss_ce: 0.034331
 22%|██████▌                       | 88/400 [41:36<2:25:46, 28.03s/it]2022-01-06 22:22:51,212 iteration 1497 : loss : 0.073297, loss_ce: 0.037204
2022-01-06 22:22:52,759 iteration 1498 : loss : 0.046733, loss_ce: 0.013635
2022-01-06 22:22:54,363 iteration 1499 : loss : 0.064982, loss_ce: 0.029770
2022-01-06 22:22:55,865 iteration 1500 : loss : 0.076203, loss_ce: 0.024016
2022-01-06 22:22:57,419 iteration 1501 : loss : 0.075148, loss_ce: 0.020635
2022-01-06 22:22:59,010 iteration 1502 : loss : 0.068217, loss_ce: 0.023021
2022-01-06 22:23:00,559 iteration 1503 : loss : 0.045606, loss_ce: 0.019892
2022-01-06 22:23:02,114 iteration 1504 : loss : 0.053496, loss_ce: 0.017920
2022-01-06 22:23:03,616 iteration 1505 : loss : 0.048853, loss_ce: 0.015831
2022-01-06 22:23:05,151 iteration 1506 : loss : 0.056029, loss_ce: 0.020758
2022-01-06 22:23:06,684 iteration 1507 : loss : 0.064299, loss_ce: 0.022304
2022-01-06 22:23:08,193 iteration 1508 : loss : 0.069853, loss_ce: 0.031851
2022-01-06 22:23:09,711 iteration 1509 : loss : 0.076083, loss_ce: 0.030125
2022-01-06 22:23:11,190 iteration 1510 : loss : 0.039468, loss_ce: 0.015195
2022-01-06 22:23:12,735 iteration 1511 : loss : 0.047560, loss_ce: 0.020677
2022-01-06 22:23:14,233 iteration 1512 : loss : 0.063577, loss_ce: 0.026184
2022-01-06 22:23:15,774 iteration 1513 : loss : 0.053649, loss_ce: 0.024456
 22%|██████▋                       | 89/400 [42:02<2:22:21, 27.46s/it]2022-01-06 22:23:17,322 iteration 1514 : loss : 0.062397, loss_ce: 0.025511
2022-01-06 22:23:18,828 iteration 1515 : loss : 0.062381, loss_ce: 0.024292
2022-01-06 22:23:20,349 iteration 1516 : loss : 0.057563, loss_ce: 0.025479
2022-01-06 22:23:21,858 iteration 1517 : loss : 0.043453, loss_ce: 0.018699
2022-01-06 22:23:23,413 iteration 1518 : loss : 0.051994, loss_ce: 0.017193
2022-01-06 22:23:24,935 iteration 1519 : loss : 0.053470, loss_ce: 0.020482
2022-01-06 22:23:26,452 iteration 1520 : loss : 0.047832, loss_ce: 0.016771
2022-01-06 22:23:27,974 iteration 1521 : loss : 0.052878, loss_ce: 0.019132
2022-01-06 22:23:29,606 iteration 1522 : loss : 0.052483, loss_ce: 0.016690
2022-01-06 22:23:31,105 iteration 1523 : loss : 0.074535, loss_ce: 0.037301
2022-01-06 22:23:32,692 iteration 1524 : loss : 0.072034, loss_ce: 0.027199
2022-01-06 22:23:34,235 iteration 1525 : loss : 0.043111, loss_ce: 0.018531
2022-01-06 22:23:35,862 iteration 1526 : loss : 0.040616, loss_ce: 0.016232
2022-01-06 22:23:37,490 iteration 1527 : loss : 0.067745, loss_ce: 0.029699
2022-01-06 22:23:39,098 iteration 1528 : loss : 0.053358, loss_ce: 0.024097
2022-01-06 22:23:40,643 iteration 1529 : loss : 0.060714, loss_ce: 0.024482
2022-01-06 22:23:40,643 Training Data Eval:
2022-01-06 22:23:48,711   Average segmentation loss on training set: 0.0611
2022-01-06 22:23:48,712 Validation Data Eval:
2022-01-06 22:23:51,482   Average segmentation loss on validation set: 0.1119
2022-01-06 22:23:57,201 Found new lowest validation loss at iteration 1529! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 22:23:58,669 iteration 1530 : loss : 0.058398, loss_ce: 0.029103
 22%|██████▊                       | 90/400 [42:45<2:45:49, 32.10s/it]2022-01-06 22:24:00,278 iteration 1531 : loss : 0.053472, loss_ce: 0.021317
2022-01-06 22:24:01,761 iteration 1532 : loss : 0.059862, loss_ce: 0.021465
2022-01-06 22:24:03,165 iteration 1533 : loss : 0.038127, loss_ce: 0.017261
2022-01-06 22:24:04,654 iteration 1534 : loss : 0.072064, loss_ce: 0.026359
2022-01-06 22:24:06,112 iteration 1535 : loss : 0.047015, loss_ce: 0.018283
2022-01-06 22:24:07,519 iteration 1536 : loss : 0.069753, loss_ce: 0.036617
2022-01-06 22:24:08,919 iteration 1537 : loss : 0.061861, loss_ce: 0.020475
2022-01-06 22:24:10,339 iteration 1538 : loss : 0.170951, loss_ce: 0.040973
2022-01-06 22:24:11,824 iteration 1539 : loss : 0.062859, loss_ce: 0.022671
2022-01-06 22:24:13,327 iteration 1540 : loss : 0.070964, loss_ce: 0.033689
2022-01-06 22:24:14,749 iteration 1541 : loss : 0.048778, loss_ce: 0.017315
2022-01-06 22:24:16,257 iteration 1542 : loss : 0.062830, loss_ce: 0.029829
2022-01-06 22:24:17,745 iteration 1543 : loss : 0.060523, loss_ce: 0.018540
2022-01-06 22:24:19,198 iteration 1544 : loss : 0.060548, loss_ce: 0.022598
2022-01-06 22:24:20,718 iteration 1545 : loss : 0.064269, loss_ce: 0.023805
2022-01-06 22:24:22,232 iteration 1546 : loss : 0.056653, loss_ce: 0.024342
2022-01-06 22:24:23,734 iteration 1547 : loss : 0.073409, loss_ce: 0.027602
 23%|██████▊                       | 91/400 [43:10<2:34:25, 29.99s/it]2022-01-06 22:24:25,311 iteration 1548 : loss : 0.047333, loss_ce: 0.017156
2022-01-06 22:24:26,780 iteration 1549 : loss : 0.048931, loss_ce: 0.020709
2022-01-06 22:24:28,391 iteration 1550 : loss : 0.057882, loss_ce: 0.025190
2022-01-06 22:24:29,910 iteration 1551 : loss : 0.060028, loss_ce: 0.026156
2022-01-06 22:24:31,440 iteration 1552 : loss : 0.050005, loss_ce: 0.021979
2022-01-06 22:24:32,898 iteration 1553 : loss : 0.035456, loss_ce: 0.019050
2022-01-06 22:24:34,385 iteration 1554 : loss : 0.063838, loss_ce: 0.025112
2022-01-06 22:24:35,961 iteration 1555 : loss : 0.045697, loss_ce: 0.017167
2022-01-06 22:24:37,597 iteration 1556 : loss : 0.071682, loss_ce: 0.026133
2022-01-06 22:24:39,178 iteration 1557 : loss : 0.105243, loss_ce: 0.029058
2022-01-06 22:24:40,713 iteration 1558 : loss : 0.065432, loss_ce: 0.018396
2022-01-06 22:24:42,328 iteration 1559 : loss : 0.046327, loss_ce: 0.020108
2022-01-06 22:24:43,819 iteration 1560 : loss : 0.058465, loss_ce: 0.016799
2022-01-06 22:24:45,320 iteration 1561 : loss : 0.071529, loss_ce: 0.022650
2022-01-06 22:24:46,847 iteration 1562 : loss : 0.050571, loss_ce: 0.021096
2022-01-06 22:24:48,354 iteration 1563 : loss : 0.057232, loss_ce: 0.030037
2022-01-06 22:24:49,876 iteration 1564 : loss : 0.067242, loss_ce: 0.026741
 23%|██████▉                       | 92/400 [43:36<2:28:00, 28.83s/it]2022-01-06 22:24:51,475 iteration 1565 : loss : 0.067314, loss_ce: 0.022595
2022-01-06 22:24:52,983 iteration 1566 : loss : 0.051935, loss_ce: 0.022839
2022-01-06 22:24:54,557 iteration 1567 : loss : 0.066821, loss_ce: 0.019826
2022-01-06 22:24:56,201 iteration 1568 : loss : 0.041997, loss_ce: 0.017931
2022-01-06 22:24:57,717 iteration 1569 : loss : 0.062019, loss_ce: 0.025107
2022-01-06 22:24:59,266 iteration 1570 : loss : 0.065555, loss_ce: 0.026450
2022-01-06 22:25:00,776 iteration 1571 : loss : 0.065472, loss_ce: 0.026276
2022-01-06 22:25:02,309 iteration 1572 : loss : 0.069906, loss_ce: 0.032590
2022-01-06 22:25:03,854 iteration 1573 : loss : 0.079518, loss_ce: 0.025032
2022-01-06 22:25:05,388 iteration 1574 : loss : 0.060841, loss_ce: 0.034668
2022-01-06 22:25:06,944 iteration 1575 : loss : 0.079519, loss_ce: 0.034971
2022-01-06 22:25:08,456 iteration 1576 : loss : 0.056863, loss_ce: 0.018342
2022-01-06 22:25:09,934 iteration 1577 : loss : 0.106754, loss_ce: 0.044162
2022-01-06 22:25:11,363 iteration 1578 : loss : 0.043873, loss_ce: 0.017853
2022-01-06 22:25:12,877 iteration 1579 : loss : 0.060118, loss_ce: 0.027404
2022-01-06 22:25:14,423 iteration 1580 : loss : 0.057806, loss_ce: 0.020574
2022-01-06 22:25:15,975 iteration 1581 : loss : 0.051697, loss_ce: 0.019905
 23%|██████▉                       | 93/400 [44:02<2:23:20, 28.01s/it]2022-01-06 22:25:17,603 iteration 1582 : loss : 0.072505, loss_ce: 0.043836
2022-01-06 22:25:19,144 iteration 1583 : loss : 0.083092, loss_ce: 0.039039
2022-01-06 22:25:20,628 iteration 1584 : loss : 0.040833, loss_ce: 0.014134
2022-01-06 22:25:22,150 iteration 1585 : loss : 0.060243, loss_ce: 0.027413
2022-01-06 22:25:23,614 iteration 1586 : loss : 0.048285, loss_ce: 0.021816
2022-01-06 22:25:25,125 iteration 1587 : loss : 0.062485, loss_ce: 0.022186
2022-01-06 22:25:26,647 iteration 1588 : loss : 0.053808, loss_ce: 0.019331
2022-01-06 22:25:28,187 iteration 1589 : loss : 0.057775, loss_ce: 0.026851
2022-01-06 22:25:29,793 iteration 1590 : loss : 0.069451, loss_ce: 0.023909
2022-01-06 22:25:31,299 iteration 1591 : loss : 0.049620, loss_ce: 0.024665
2022-01-06 22:25:32,752 iteration 1592 : loss : 0.043189, loss_ce: 0.019278
2022-01-06 22:25:34,282 iteration 1593 : loss : 0.073974, loss_ce: 0.026525
2022-01-06 22:25:35,796 iteration 1594 : loss : 0.061168, loss_ce: 0.021915
2022-01-06 22:25:37,272 iteration 1595 : loss : 0.049712, loss_ce: 0.025505
2022-01-06 22:25:38,711 iteration 1596 : loss : 0.078446, loss_ce: 0.023969
2022-01-06 22:25:40,241 iteration 1597 : loss : 0.080878, loss_ce: 0.024449
2022-01-06 22:25:41,797 iteration 1598 : loss : 0.066636, loss_ce: 0.023580
 24%|███████                       | 94/400 [44:28<2:19:30, 27.36s/it]2022-01-06 22:25:43,404 iteration 1599 : loss : 0.060873, loss_ce: 0.025783
2022-01-06 22:25:44,899 iteration 1600 : loss : 0.050334, loss_ce: 0.022601
2022-01-06 22:25:46,447 iteration 1601 : loss : 0.058200, loss_ce: 0.022125
2022-01-06 22:25:47,973 iteration 1602 : loss : 0.082092, loss_ce: 0.025565
2022-01-06 22:25:49,483 iteration 1603 : loss : 0.057215, loss_ce: 0.022161
2022-01-06 22:25:50,989 iteration 1604 : loss : 0.077119, loss_ce: 0.022652
2022-01-06 22:25:52,521 iteration 1605 : loss : 0.062898, loss_ce: 0.025086
2022-01-06 22:25:54,084 iteration 1606 : loss : 0.060354, loss_ce: 0.023695
2022-01-06 22:25:55,639 iteration 1607 : loss : 0.086757, loss_ce: 0.030803
2022-01-06 22:25:57,266 iteration 1608 : loss : 0.101333, loss_ce: 0.048797
2022-01-06 22:25:58,795 iteration 1609 : loss : 0.048075, loss_ce: 0.018654
2022-01-06 22:26:00,432 iteration 1610 : loss : 0.074388, loss_ce: 0.030200
2022-01-06 22:26:02,055 iteration 1611 : loss : 0.069741, loss_ce: 0.029126
2022-01-06 22:26:03,689 iteration 1612 : loss : 0.061868, loss_ce: 0.025478
2022-01-06 22:26:05,229 iteration 1613 : loss : 0.052818, loss_ce: 0.019310
2022-01-06 22:26:06,820 iteration 1614 : loss : 0.052406, loss_ce: 0.022660
2022-01-06 22:26:06,820 Training Data Eval:
2022-01-06 22:26:14,825   Average segmentation loss on training set: 0.0605
2022-01-06 22:26:14,825 Validation Data Eval:
2022-01-06 22:26:17,603   Average segmentation loss on validation set: 0.1095
2022-01-06 22:26:23,353 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 22:26:24,893 iteration 1615 : loss : 0.074405, loss_ce: 0.026948
 24%|███████▏                      | 95/400 [45:11<2:43:03, 32.08s/it]2022-01-06 22:26:26,392 iteration 1616 : loss : 0.051419, loss_ce: 0.021380
2022-01-06 22:26:27,813 iteration 1617 : loss : 0.070594, loss_ce: 0.027652
2022-01-06 22:26:29,195 iteration 1618 : loss : 0.056606, loss_ce: 0.020095
2022-01-06 22:26:30,624 iteration 1619 : loss : 0.057864, loss_ce: 0.034543
2022-01-06 22:26:32,172 iteration 1620 : loss : 0.069761, loss_ce: 0.023642
2022-01-06 22:26:33,722 iteration 1621 : loss : 0.053432, loss_ce: 0.019830
2022-01-06 22:26:35,160 iteration 1622 : loss : 0.058731, loss_ce: 0.024267
2022-01-06 22:26:36,587 iteration 1623 : loss : 0.054950, loss_ce: 0.017211
2022-01-06 22:26:38,101 iteration 1624 : loss : 0.064649, loss_ce: 0.025039
2022-01-06 22:26:39,554 iteration 1625 : loss : 0.043717, loss_ce: 0.015059
2022-01-06 22:26:41,046 iteration 1626 : loss : 0.043856, loss_ce: 0.017862
2022-01-06 22:26:42,661 iteration 1627 : loss : 0.061093, loss_ce: 0.023584
2022-01-06 22:26:44,256 iteration 1628 : loss : 0.053443, loss_ce: 0.022652
2022-01-06 22:26:45,755 iteration 1629 : loss : 0.048713, loss_ce: 0.021964
2022-01-06 22:26:47,216 iteration 1630 : loss : 0.038002, loss_ce: 0.015126
2022-01-06 22:26:48,688 iteration 1631 : loss : 0.069057, loss_ce: 0.022676
2022-01-06 22:26:50,199 iteration 1632 : loss : 0.051155, loss_ce: 0.021419
 24%|███████▏                      | 96/400 [45:36<2:32:13, 30.05s/it]2022-01-06 22:26:51,756 iteration 1633 : loss : 0.061927, loss_ce: 0.017907
2022-01-06 22:26:53,295 iteration 1634 : loss : 0.059909, loss_ce: 0.022439
2022-01-06 22:26:54,813 iteration 1635 : loss : 0.073920, loss_ce: 0.023544
2022-01-06 22:26:56,306 iteration 1636 : loss : 0.053924, loss_ce: 0.025492
2022-01-06 22:26:57,883 iteration 1637 : loss : 0.060747, loss_ce: 0.019373
2022-01-06 22:26:59,434 iteration 1638 : loss : 0.055862, loss_ce: 0.027970
2022-01-06 22:27:01,044 iteration 1639 : loss : 0.044261, loss_ce: 0.019178
2022-01-06 22:27:02,578 iteration 1640 : loss : 0.063382, loss_ce: 0.023382
2022-01-06 22:27:04,198 iteration 1641 : loss : 0.056243, loss_ce: 0.023174
2022-01-06 22:27:05,703 iteration 1642 : loss : 0.041389, loss_ce: 0.014605
2022-01-06 22:27:07,189 iteration 1643 : loss : 0.048947, loss_ce: 0.016266
2022-01-06 22:27:08,796 iteration 1644 : loss : 0.054992, loss_ce: 0.023927
2022-01-06 22:27:10,444 iteration 1645 : loss : 0.044370, loss_ce: 0.020235
2022-01-06 22:27:11,960 iteration 1646 : loss : 0.062546, loss_ce: 0.024223
2022-01-06 22:27:13,497 iteration 1647 : loss : 0.103335, loss_ce: 0.036870
2022-01-06 22:27:14,980 iteration 1648 : loss : 0.051388, loss_ce: 0.022447
2022-01-06 22:27:16,500 iteration 1649 : loss : 0.039611, loss_ce: 0.015191
 24%|███████▎                      | 97/400 [46:03<2:26:03, 28.92s/it]2022-01-06 22:27:18,209 iteration 1650 : loss : 0.073508, loss_ce: 0.041110
2022-01-06 22:27:19,757 iteration 1651 : loss : 0.081085, loss_ce: 0.028650
2022-01-06 22:27:21,325 iteration 1652 : loss : 0.073725, loss_ce: 0.023413
2022-01-06 22:27:22,850 iteration 1653 : loss : 0.051324, loss_ce: 0.016704
2022-01-06 22:27:24,381 iteration 1654 : loss : 0.056438, loss_ce: 0.018779
2022-01-06 22:27:25,886 iteration 1655 : loss : 0.046983, loss_ce: 0.017460
2022-01-06 22:27:27,468 iteration 1656 : loss : 0.056402, loss_ce: 0.020810
2022-01-06 22:27:28,982 iteration 1657 : loss : 0.045097, loss_ce: 0.019706
2022-01-06 22:27:30,579 iteration 1658 : loss : 0.099865, loss_ce: 0.070307
2022-01-06 22:27:32,212 iteration 1659 : loss : 0.052961, loss_ce: 0.023881
2022-01-06 22:27:33,771 iteration 1660 : loss : 0.041753, loss_ce: 0.016761
2022-01-06 22:27:35,273 iteration 1661 : loss : 0.060980, loss_ce: 0.021391
2022-01-06 22:27:36,872 iteration 1662 : loss : 0.047603, loss_ce: 0.017481
2022-01-06 22:27:38,486 iteration 1663 : loss : 0.045084, loss_ce: 0.018811
2022-01-06 22:27:40,052 iteration 1664 : loss : 0.076792, loss_ce: 0.024891
2022-01-06 22:27:41,580 iteration 1665 : loss : 0.057012, loss_ce: 0.026823
2022-01-06 22:27:43,156 iteration 1666 : loss : 0.057114, loss_ce: 0.019496
 24%|███████▎                      | 98/400 [46:29<2:22:08, 28.24s/it]2022-01-06 22:27:44,712 iteration 1667 : loss : 0.048151, loss_ce: 0.023314
2022-01-06 22:27:46,323 iteration 1668 : loss : 0.061780, loss_ce: 0.019923
2022-01-06 22:27:47,899 iteration 1669 : loss : 0.065234, loss_ce: 0.023510
2022-01-06 22:27:49,368 iteration 1670 : loss : 0.036151, loss_ce: 0.012301
2022-01-06 22:27:50,855 iteration 1671 : loss : 0.038717, loss_ce: 0.017354
2022-01-06 22:27:52,446 iteration 1672 : loss : 0.079002, loss_ce: 0.033016
2022-01-06 22:27:54,002 iteration 1673 : loss : 0.054549, loss_ce: 0.016658
2022-01-06 22:27:55,614 iteration 1674 : loss : 0.051325, loss_ce: 0.019859
2022-01-06 22:27:57,117 iteration 1675 : loss : 0.066358, loss_ce: 0.022746
2022-01-06 22:27:58,691 iteration 1676 : loss : 0.062164, loss_ce: 0.029865
2022-01-06 22:28:00,224 iteration 1677 : loss : 0.090239, loss_ce: 0.030291
2022-01-06 22:28:01,778 iteration 1678 : loss : 0.053391, loss_ce: 0.023317
2022-01-06 22:28:03,282 iteration 1679 : loss : 0.087870, loss_ce: 0.045081
2022-01-06 22:28:04,835 iteration 1680 : loss : 0.066846, loss_ce: 0.032214
2022-01-06 22:28:06,348 iteration 1681 : loss : 0.064209, loss_ce: 0.027593
2022-01-06 22:28:07,873 iteration 1682 : loss : 0.061973, loss_ce: 0.031643
2022-01-06 22:28:09,361 iteration 1683 : loss : 0.063924, loss_ce: 0.032563
 25%|███████▍                      | 99/400 [46:56<2:18:37, 27.63s/it]2022-01-06 22:28:10,970 iteration 1684 : loss : 0.061694, loss_ce: 0.022749
2022-01-06 22:28:12,459 iteration 1685 : loss : 0.054830, loss_ce: 0.027000
2022-01-06 22:28:13,918 iteration 1686 : loss : 0.046623, loss_ce: 0.017560
2022-01-06 22:28:15,410 iteration 1687 : loss : 0.053585, loss_ce: 0.019844
2022-01-06 22:28:16,934 iteration 1688 : loss : 0.067862, loss_ce: 0.034555
2022-01-06 22:28:18,422 iteration 1689 : loss : 0.051243, loss_ce: 0.018029
2022-01-06 22:28:19,982 iteration 1690 : loss : 0.058018, loss_ce: 0.026908
2022-01-06 22:28:21,500 iteration 1691 : loss : 0.040099, loss_ce: 0.017245
2022-01-06 22:28:23,070 iteration 1692 : loss : 0.097495, loss_ce: 0.035599
2022-01-06 22:28:24,649 iteration 1693 : loss : 0.040832, loss_ce: 0.018433
2022-01-06 22:28:26,206 iteration 1694 : loss : 0.064987, loss_ce: 0.020630
2022-01-06 22:28:27,665 iteration 1695 : loss : 0.045293, loss_ce: 0.015931
2022-01-06 22:28:29,169 iteration 1696 : loss : 0.060526, loss_ce: 0.023127
2022-01-06 22:28:30,721 iteration 1697 : loss : 0.059707, loss_ce: 0.019480
2022-01-06 22:28:32,216 iteration 1698 : loss : 0.048242, loss_ce: 0.015115
2022-01-06 22:28:33,629 iteration 1699 : loss : 0.051767, loss_ce: 0.024266
2022-01-06 22:28:33,629 Training Data Eval:
2022-01-06 22:28:41,403   Average segmentation loss on training set: 0.0463
2022-01-06 22:28:41,403 Validation Data Eval:
2022-01-06 22:28:44,085   Average segmentation loss on validation set: 0.1396
2022-01-06 22:28:45,613 iteration 1700 : loss : 0.062939, loss_ce: 0.022248
 25%|███████▎                     | 100/400 [47:32<2:31:04, 30.22s/it]2022-01-06 22:28:47,216 iteration 1701 : loss : 0.049730, loss_ce: 0.020651
2022-01-06 22:28:48,741 iteration 1702 : loss : 0.045456, loss_ce: 0.018353
2022-01-06 22:28:50,201 iteration 1703 : loss : 0.049550, loss_ce: 0.021949
2022-01-06 22:28:51,779 iteration 1704 : loss : 0.063176, loss_ce: 0.022165
2022-01-06 22:28:53,280 iteration 1705 : loss : 0.056577, loss_ce: 0.025099
2022-01-06 22:28:54,803 iteration 1706 : loss : 0.054998, loss_ce: 0.023455
2022-01-06 22:28:56,297 iteration 1707 : loss : 0.054657, loss_ce: 0.021630
2022-01-06 22:28:57,827 iteration 1708 : loss : 0.049825, loss_ce: 0.016736
2022-01-06 22:28:59,274 iteration 1709 : loss : 0.052891, loss_ce: 0.022404
2022-01-06 22:29:00,712 iteration 1710 : loss : 0.039683, loss_ce: 0.016951
2022-01-06 22:29:02,216 iteration 1711 : loss : 0.049186, loss_ce: 0.018453
2022-01-06 22:29:03,662 iteration 1712 : loss : 0.048051, loss_ce: 0.016470
2022-01-06 22:29:05,111 iteration 1713 : loss : 0.042638, loss_ce: 0.014500
2022-01-06 22:29:06,696 iteration 1714 : loss : 0.094966, loss_ce: 0.033201
2022-01-06 22:29:08,217 iteration 1715 : loss : 0.095522, loss_ce: 0.023521
2022-01-06 22:29:09,782 iteration 1716 : loss : 0.059267, loss_ce: 0.019965
2022-01-06 22:29:11,277 iteration 1717 : loss : 0.059615, loss_ce: 0.020894
 25%|███████▎                     | 101/400 [47:58<2:23:46, 28.85s/it]2022-01-06 22:29:12,798 iteration 1718 : loss : 0.054072, loss_ce: 0.018240
2022-01-06 22:29:14,311 iteration 1719 : loss : 0.067987, loss_ce: 0.027543
2022-01-06 22:29:15,721 iteration 1720 : loss : 0.039061, loss_ce: 0.013184
2022-01-06 22:29:17,214 iteration 1721 : loss : 0.065136, loss_ce: 0.027358
2022-01-06 22:29:18,753 iteration 1722 : loss : 0.047705, loss_ce: 0.017489
2022-01-06 22:29:20,236 iteration 1723 : loss : 0.063339, loss_ce: 0.027321
2022-01-06 22:29:21,767 iteration 1724 : loss : 0.050724, loss_ce: 0.018164
2022-01-06 22:29:23,376 iteration 1725 : loss : 0.077656, loss_ce: 0.025514
2022-01-06 22:29:24,931 iteration 1726 : loss : 0.064593, loss_ce: 0.020768
2022-01-06 22:29:26,437 iteration 1727 : loss : 0.049635, loss_ce: 0.026705
2022-01-06 22:29:27,920 iteration 1728 : loss : 0.053177, loss_ce: 0.016679
2022-01-06 22:29:29,419 iteration 1729 : loss : 0.041681, loss_ce: 0.019256
2022-01-06 22:29:30,943 iteration 1730 : loss : 0.054072, loss_ce: 0.020952
2022-01-06 22:29:32,508 iteration 1731 : loss : 0.056917, loss_ce: 0.020708
2022-01-06 22:29:34,005 iteration 1732 : loss : 0.036900, loss_ce: 0.018579
2022-01-06 22:29:35,567 iteration 1733 : loss : 0.074519, loss_ce: 0.036265
2022-01-06 22:29:37,020 iteration 1734 : loss : 0.039788, loss_ce: 0.015522
 26%|███████▍                     | 102/400 [48:23<2:18:39, 27.92s/it]2022-01-06 22:29:38,531 iteration 1735 : loss : 0.073898, loss_ce: 0.024327
2022-01-06 22:29:40,006 iteration 1736 : loss : 0.036695, loss_ce: 0.016530
2022-01-06 22:29:41,497 iteration 1737 : loss : 0.047934, loss_ce: 0.018574
2022-01-06 22:29:42,967 iteration 1738 : loss : 0.058195, loss_ce: 0.020707
2022-01-06 22:29:44,539 iteration 1739 : loss : 0.059002, loss_ce: 0.024741
2022-01-06 22:29:46,071 iteration 1740 : loss : 0.094458, loss_ce: 0.035949
2022-01-06 22:29:47,577 iteration 1741 : loss : 0.053578, loss_ce: 0.017391
2022-01-06 22:29:49,192 iteration 1742 : loss : 0.058452, loss_ce: 0.025789
2022-01-06 22:29:50,703 iteration 1743 : loss : 0.055359, loss_ce: 0.018594
2022-01-06 22:29:52,265 iteration 1744 : loss : 0.051710, loss_ce: 0.019489
2022-01-06 22:29:53,938 iteration 1745 : loss : 0.080725, loss_ce: 0.048743
2022-01-06 22:29:55,458 iteration 1746 : loss : 0.040207, loss_ce: 0.015138
2022-01-06 22:29:56,978 iteration 1747 : loss : 0.047210, loss_ce: 0.020372
2022-01-06 22:29:58,506 iteration 1748 : loss : 0.046080, loss_ce: 0.019595
2022-01-06 22:30:00,027 iteration 1749 : loss : 0.084631, loss_ce: 0.029014
2022-01-06 22:30:01,509 iteration 1750 : loss : 0.042514, loss_ce: 0.019415
2022-01-06 22:30:03,107 iteration 1751 : loss : 0.061589, loss_ce: 0.024031
 26%|███████▍                     | 103/400 [48:49<2:15:28, 27.37s/it]2022-01-06 22:30:04,649 iteration 1752 : loss : 0.047161, loss_ce: 0.018948
2022-01-06 22:30:06,130 iteration 1753 : loss : 0.047762, loss_ce: 0.021631
2022-01-06 22:30:07,626 iteration 1754 : loss : 0.051796, loss_ce: 0.022031
2022-01-06 22:30:09,135 iteration 1755 : loss : 0.056869, loss_ce: 0.021896
2022-01-06 22:30:10,662 iteration 1756 : loss : 0.047604, loss_ce: 0.022046
2022-01-06 22:30:12,228 iteration 1757 : loss : 0.048743, loss_ce: 0.023828
2022-01-06 22:30:13,771 iteration 1758 : loss : 0.080367, loss_ce: 0.028146
2022-01-06 22:30:15,231 iteration 1759 : loss : 0.098394, loss_ce: 0.034591
2022-01-06 22:30:16,734 iteration 1760 : loss : 0.068242, loss_ce: 0.029968
2022-01-06 22:30:18,194 iteration 1761 : loss : 0.037838, loss_ce: 0.015919
2022-01-06 22:30:19,689 iteration 1762 : loss : 0.053131, loss_ce: 0.021534
2022-01-06 22:30:21,122 iteration 1763 : loss : 0.052461, loss_ce: 0.022966
2022-01-06 22:30:22,706 iteration 1764 : loss : 0.073551, loss_ce: 0.025291
2022-01-06 22:30:24,212 iteration 1765 : loss : 0.062017, loss_ce: 0.020651
2022-01-06 22:30:25,769 iteration 1766 : loss : 0.038279, loss_ce: 0.014341
2022-01-06 22:30:27,282 iteration 1767 : loss : 0.046113, loss_ce: 0.018378
2022-01-06 22:30:28,763 iteration 1768 : loss : 0.053320, loss_ce: 0.019567
 26%|███████▌                     | 104/400 [49:15<2:12:29, 26.86s/it]2022-01-06 22:30:30,346 iteration 1769 : loss : 0.041337, loss_ce: 0.016456
2022-01-06 22:30:31,824 iteration 1770 : loss : 0.037999, loss_ce: 0.012782
2022-01-06 22:30:33,286 iteration 1771 : loss : 0.033572, loss_ce: 0.010514
2022-01-06 22:30:34,850 iteration 1772 : loss : 0.069681, loss_ce: 0.031790
2022-01-06 22:30:36,374 iteration 1773 : loss : 0.060402, loss_ce: 0.024711
2022-01-06 22:30:37,848 iteration 1774 : loss : 0.059471, loss_ce: 0.020350
2022-01-06 22:30:39,340 iteration 1775 : loss : 0.050963, loss_ce: 0.017338
2022-01-06 22:30:40,827 iteration 1776 : loss : 0.060837, loss_ce: 0.024278
2022-01-06 22:30:42,295 iteration 1777 : loss : 0.042012, loss_ce: 0.015802
2022-01-06 22:30:43,838 iteration 1778 : loss : 0.053595, loss_ce: 0.025684
2022-01-06 22:30:45,349 iteration 1779 : loss : 0.047733, loss_ce: 0.018731
2022-01-06 22:30:46,890 iteration 1780 : loss : 0.069152, loss_ce: 0.019825
2022-01-06 22:30:48,433 iteration 1781 : loss : 0.063903, loss_ce: 0.023220
2022-01-06 22:30:49,881 iteration 1782 : loss : 0.077105, loss_ce: 0.020909
2022-01-06 22:30:51,369 iteration 1783 : loss : 0.053713, loss_ce: 0.022117
2022-01-06 22:30:52,912 iteration 1784 : loss : 0.068626, loss_ce: 0.032714
2022-01-06 22:30:52,912 Training Data Eval:
2022-01-06 22:31:00,666   Average segmentation loss on training set: 0.0434
2022-01-06 22:31:00,666 Validation Data Eval:
2022-01-06 22:31:03,358   Average segmentation loss on validation set: 0.1388
2022-01-06 22:31:04,874 iteration 1785 : loss : 0.052376, loss_ce: 0.025489
 26%|███████▌                     | 105/400 [49:51<2:25:41, 29.63s/it]2022-01-06 22:31:06,499 iteration 1786 : loss : 0.063610, loss_ce: 0.021979
2022-01-06 22:31:07,947 iteration 1787 : loss : 0.064854, loss_ce: 0.016288
2022-01-06 22:31:09,462 iteration 1788 : loss : 0.061824, loss_ce: 0.020588
2022-01-06 22:31:10,988 iteration 1789 : loss : 0.051085, loss_ce: 0.024205
2022-01-06 22:31:12,623 iteration 1790 : loss : 0.050984, loss_ce: 0.018626
2022-01-06 22:31:14,143 iteration 1791 : loss : 0.049912, loss_ce: 0.019116
2022-01-06 22:31:15,666 iteration 1792 : loss : 0.049995, loss_ce: 0.019080
2022-01-06 22:31:17,210 iteration 1793 : loss : 0.079123, loss_ce: 0.034110
2022-01-06 22:31:18,709 iteration 1794 : loss : 0.053438, loss_ce: 0.027758
2022-01-06 22:31:20,129 iteration 1795 : loss : 0.043505, loss_ce: 0.017897
2022-01-06 22:31:21,701 iteration 1796 : loss : 0.055741, loss_ce: 0.024613
2022-01-06 22:31:23,177 iteration 1797 : loss : 0.053059, loss_ce: 0.023253
2022-01-06 22:31:24,712 iteration 1798 : loss : 0.046277, loss_ce: 0.017573
2022-01-06 22:31:26,300 iteration 1799 : loss : 0.062959, loss_ce: 0.024582
2022-01-06 22:31:27,793 iteration 1800 : loss : 0.044597, loss_ce: 0.014348
2022-01-06 22:31:29,337 iteration 1801 : loss : 0.072572, loss_ce: 0.022133
2022-01-06 22:31:30,913 iteration 1802 : loss : 0.051283, loss_ce: 0.017026
 26%|███████▋                     | 106/400 [50:17<2:19:54, 28.55s/it]2022-01-06 22:31:32,536 iteration 1803 : loss : 0.048663, loss_ce: 0.017972
2022-01-06 22:31:34,000 iteration 1804 : loss : 0.049104, loss_ce: 0.018642
2022-01-06 22:31:35,640 iteration 1805 : loss : 0.078108, loss_ce: 0.030944
2022-01-06 22:31:37,237 iteration 1806 : loss : 0.055900, loss_ce: 0.025858
2022-01-06 22:31:38,780 iteration 1807 : loss : 0.060703, loss_ce: 0.026763
2022-01-06 22:31:40,342 iteration 1808 : loss : 0.041970, loss_ce: 0.018169
2022-01-06 22:31:41,850 iteration 1809 : loss : 0.041366, loss_ce: 0.017668
2022-01-06 22:31:43,405 iteration 1810 : loss : 0.058109, loss_ce: 0.029194
2022-01-06 22:31:44,905 iteration 1811 : loss : 0.067787, loss_ce: 0.026293
2022-01-06 22:31:46,394 iteration 1812 : loss : 0.055701, loss_ce: 0.019033
2022-01-06 22:31:47,914 iteration 1813 : loss : 0.052923, loss_ce: 0.024412
2022-01-06 22:31:49,417 iteration 1814 : loss : 0.079681, loss_ce: 0.034013
2022-01-06 22:31:51,004 iteration 1815 : loss : 0.087141, loss_ce: 0.031210
2022-01-06 22:31:52,532 iteration 1816 : loss : 0.042617, loss_ce: 0.019377
2022-01-06 22:31:54,119 iteration 1817 : loss : 0.060738, loss_ce: 0.026291
2022-01-06 22:31:55,578 iteration 1818 : loss : 0.034455, loss_ce: 0.013014
2022-01-06 22:31:57,130 iteration 1819 : loss : 0.069573, loss_ce: 0.026339
 27%|███████▊                     | 107/400 [50:43<2:16:01, 27.85s/it]2022-01-06 22:31:58,610 iteration 1820 : loss : 0.044336, loss_ce: 0.020049
2022-01-06 22:32:00,219 iteration 1821 : loss : 0.067975, loss_ce: 0.023512
2022-01-06 22:32:01,767 iteration 1822 : loss : 0.067343, loss_ce: 0.030552
2022-01-06 22:32:03,337 iteration 1823 : loss : 0.068482, loss_ce: 0.022388
2022-01-06 22:32:04,877 iteration 1824 : loss : 0.092207, loss_ce: 0.025929
2022-01-06 22:32:06,340 iteration 1825 : loss : 0.049263, loss_ce: 0.017607
2022-01-06 22:32:07,906 iteration 1826 : loss : 0.043124, loss_ce: 0.014342
2022-01-06 22:32:09,357 iteration 1827 : loss : 0.050340, loss_ce: 0.020176
2022-01-06 22:32:10,876 iteration 1828 : loss : 0.065602, loss_ce: 0.025228
2022-01-06 22:32:12,358 iteration 1829 : loss : 0.060517, loss_ce: 0.026798
2022-01-06 22:32:13,934 iteration 1830 : loss : 0.071615, loss_ce: 0.030859
2022-01-06 22:32:15,443 iteration 1831 : loss : 0.056679, loss_ce: 0.019685
2022-01-06 22:32:16,926 iteration 1832 : loss : 0.055932, loss_ce: 0.022699
2022-01-06 22:32:18,379 iteration 1833 : loss : 0.058882, loss_ce: 0.027866
2022-01-06 22:32:19,881 iteration 1834 : loss : 0.051755, loss_ce: 0.019731
2022-01-06 22:32:21,391 iteration 1835 : loss : 0.047918, loss_ce: 0.020019
2022-01-06 22:32:22,863 iteration 1836 : loss : 0.072570, loss_ce: 0.027944
 27%|███████▊                     | 108/400 [51:09<2:12:27, 27.22s/it]2022-01-06 22:32:24,458 iteration 1837 : loss : 0.065047, loss_ce: 0.025600
2022-01-06 22:32:25,886 iteration 1838 : loss : 0.052570, loss_ce: 0.021404
2022-01-06 22:32:27,353 iteration 1839 : loss : 0.060299, loss_ce: 0.024860
2022-01-06 22:32:28,876 iteration 1840 : loss : 0.062530, loss_ce: 0.026243
2022-01-06 22:32:30,322 iteration 1841 : loss : 0.055164, loss_ce: 0.021503
2022-01-06 22:32:31,818 iteration 1842 : loss : 0.039195, loss_ce: 0.012283
2022-01-06 22:32:33,339 iteration 1843 : loss : 0.034504, loss_ce: 0.016886
2022-01-06 22:32:34,823 iteration 1844 : loss : 0.051340, loss_ce: 0.016329
2022-01-06 22:32:36,312 iteration 1845 : loss : 0.039975, loss_ce: 0.012569
2022-01-06 22:32:37,894 iteration 1846 : loss : 0.058244, loss_ce: 0.025824
2022-01-06 22:32:39,380 iteration 1847 : loss : 0.060301, loss_ce: 0.019111
2022-01-06 22:32:40,894 iteration 1848 : loss : 0.071053, loss_ce: 0.029391
2022-01-06 22:32:42,388 iteration 1849 : loss : 0.064736, loss_ce: 0.030137
2022-01-06 22:32:44,005 iteration 1850 : loss : 0.073453, loss_ce: 0.029434
2022-01-06 22:32:45,499 iteration 1851 : loss : 0.098848, loss_ce: 0.031830
2022-01-06 22:32:47,021 iteration 1852 : loss : 0.057882, loss_ce: 0.025067
2022-01-06 22:32:48,508 iteration 1853 : loss : 0.043352, loss_ce: 0.017632
 27%|███████▉                     | 109/400 [51:35<2:09:43, 26.75s/it]2022-01-06 22:32:50,092 iteration 1854 : loss : 0.044654, loss_ce: 0.017108
2022-01-06 22:32:51,665 iteration 1855 : loss : 0.051375, loss_ce: 0.022985
2022-01-06 22:32:53,261 iteration 1856 : loss : 0.067138, loss_ce: 0.024099
2022-01-06 22:32:54,861 iteration 1857 : loss : 0.065043, loss_ce: 0.021382
2022-01-06 22:32:56,406 iteration 1858 : loss : 0.062388, loss_ce: 0.028286
2022-01-06 22:32:57,955 iteration 1859 : loss : 0.055727, loss_ce: 0.025384
2022-01-06 22:32:59,453 iteration 1860 : loss : 0.048639, loss_ce: 0.017060
2022-01-06 22:33:01,038 iteration 1861 : loss : 0.045386, loss_ce: 0.016573
2022-01-06 22:33:02,531 iteration 1862 : loss : 0.043754, loss_ce: 0.020440
2022-01-06 22:33:04,065 iteration 1863 : loss : 0.042207, loss_ce: 0.019077
2022-01-06 22:33:05,632 iteration 1864 : loss : 0.075812, loss_ce: 0.019119
2022-01-06 22:33:07,092 iteration 1865 : loss : 0.045834, loss_ce: 0.020347
2022-01-06 22:33:08,691 iteration 1866 : loss : 0.037023, loss_ce: 0.017247
2022-01-06 22:33:10,227 iteration 1867 : loss : 0.037258, loss_ce: 0.016489
2022-01-06 22:33:11,721 iteration 1868 : loss : 0.059095, loss_ce: 0.023960
2022-01-06 22:33:13,213 iteration 1869 : loss : 0.073069, loss_ce: 0.024018
2022-01-06 22:33:13,213 Training Data Eval:
2022-01-06 22:33:21,067   Average segmentation loss on training set: 0.0384
2022-01-06 22:33:21,068 Validation Data Eval:
2022-01-06 22:33:23,824   Average segmentation loss on validation set: 0.1099
2022-01-06 22:33:25,408 iteration 1870 : loss : 0.051225, loss_ce: 0.021002
 28%|███████▉                     | 110/400 [52:12<2:24:00, 29.79s/it]2022-01-06 22:33:27,050 iteration 1871 : loss : 0.034532, loss_ce: 0.013053
2022-01-06 22:33:28,678 iteration 1872 : loss : 0.083757, loss_ce: 0.020513
2022-01-06 22:33:30,162 iteration 1873 : loss : 0.047010, loss_ce: 0.017058
2022-01-06 22:33:31,747 iteration 1874 : loss : 0.065832, loss_ce: 0.029277
2022-01-06 22:33:33,197 iteration 1875 : loss : 0.047922, loss_ce: 0.017714
2022-01-06 22:33:34,809 iteration 1876 : loss : 0.059492, loss_ce: 0.018681
2022-01-06 22:33:36,373 iteration 1877 : loss : 0.069524, loss_ce: 0.029115
2022-01-06 22:33:37,877 iteration 1878 : loss : 0.048330, loss_ce: 0.021031
2022-01-06 22:33:39,377 iteration 1879 : loss : 0.067463, loss_ce: 0.024176
2022-01-06 22:33:40,874 iteration 1880 : loss : 0.051563, loss_ce: 0.022937
2022-01-06 22:33:42,415 iteration 1881 : loss : 0.051885, loss_ce: 0.018967
2022-01-06 22:33:43,914 iteration 1882 : loss : 0.050012, loss_ce: 0.018314
2022-01-06 22:33:45,380 iteration 1883 : loss : 0.050278, loss_ce: 0.017387
2022-01-06 22:33:46,949 iteration 1884 : loss : 0.053876, loss_ce: 0.022319
2022-01-06 22:33:48,399 iteration 1885 : loss : 0.040205, loss_ce: 0.016145
2022-01-06 22:33:50,000 iteration 1886 : loss : 0.047765, loss_ce: 0.020878
2022-01-06 22:33:51,590 iteration 1887 : loss : 0.052293, loss_ce: 0.026068
 28%|████████                     | 111/400 [52:38<2:18:16, 28.71s/it]2022-01-06 22:33:53,136 iteration 1888 : loss : 0.046452, loss_ce: 0.019186
2022-01-06 22:33:54,707 iteration 1889 : loss : 0.044687, loss_ce: 0.019822
2022-01-06 22:33:56,261 iteration 1890 : loss : 0.056069, loss_ce: 0.019517
2022-01-06 22:33:57,816 iteration 1891 : loss : 0.053096, loss_ce: 0.017882
2022-01-06 22:33:59,337 iteration 1892 : loss : 0.042221, loss_ce: 0.018692
2022-01-06 22:34:00,872 iteration 1893 : loss : 0.053258, loss_ce: 0.027206
2022-01-06 22:34:02,387 iteration 1894 : loss : 0.053647, loss_ce: 0.023831
2022-01-06 22:34:03,899 iteration 1895 : loss : 0.064115, loss_ce: 0.020028
2022-01-06 22:34:05,541 iteration 1896 : loss : 0.077166, loss_ce: 0.037579
2022-01-06 22:34:07,049 iteration 1897 : loss : 0.055142, loss_ce: 0.018486
2022-01-06 22:34:08,626 iteration 1898 : loss : 0.052440, loss_ce: 0.019689
2022-01-06 22:34:10,146 iteration 1899 : loss : 0.056613, loss_ce: 0.028685
2022-01-06 22:34:11,599 iteration 1900 : loss : 0.040908, loss_ce: 0.017302
2022-01-06 22:34:13,119 iteration 1901 : loss : 0.040133, loss_ce: 0.014404
2022-01-06 22:34:14,667 iteration 1902 : loss : 0.057385, loss_ce: 0.022806
2022-01-06 22:34:16,235 iteration 1903 : loss : 0.070143, loss_ce: 0.018639
2022-01-06 22:34:17,845 iteration 1904 : loss : 0.062935, loss_ce: 0.020990
 28%|████████                     | 112/400 [53:04<2:14:15, 27.97s/it]2022-01-06 22:34:19,367 iteration 1905 : loss : 0.047347, loss_ce: 0.020485
2022-01-06 22:34:20,939 iteration 1906 : loss : 0.043908, loss_ce: 0.015079
2022-01-06 22:34:22,565 iteration 1907 : loss : 0.051495, loss_ce: 0.019581
2022-01-06 22:34:24,129 iteration 1908 : loss : 0.054733, loss_ce: 0.022454
2022-01-06 22:34:25,717 iteration 1909 : loss : 0.061894, loss_ce: 0.032736
2022-01-06 22:34:27,309 iteration 1910 : loss : 0.040190, loss_ce: 0.017795
2022-01-06 22:34:28,954 iteration 1911 : loss : 0.047715, loss_ce: 0.019998
2022-01-06 22:34:30,527 iteration 1912 : loss : 0.053816, loss_ce: 0.014223
2022-01-06 22:34:32,052 iteration 1913 : loss : 0.049240, loss_ce: 0.016871
2022-01-06 22:34:33,550 iteration 1914 : loss : 0.041171, loss_ce: 0.016862
2022-01-06 22:34:35,094 iteration 1915 : loss : 0.053911, loss_ce: 0.017676
2022-01-06 22:34:36,628 iteration 1916 : loss : 0.052128, loss_ce: 0.021314
2022-01-06 22:34:38,198 iteration 1917 : loss : 0.066665, loss_ce: 0.022993
2022-01-06 22:34:39,706 iteration 1918 : loss : 0.055565, loss_ce: 0.023619
2022-01-06 22:34:41,239 iteration 1919 : loss : 0.045292, loss_ce: 0.020282
2022-01-06 22:34:42,766 iteration 1920 : loss : 0.046774, loss_ce: 0.017135
2022-01-06 22:34:44,346 iteration 1921 : loss : 0.052851, loss_ce: 0.019730
 28%|████████▏                    | 113/400 [53:31<2:11:41, 27.53s/it]2022-01-06 22:34:45,898 iteration 1922 : loss : 0.042490, loss_ce: 0.016127
2022-01-06 22:34:47,372 iteration 1923 : loss : 0.031831, loss_ce: 0.013348
2022-01-06 22:34:48,927 iteration 1924 : loss : 0.073085, loss_ce: 0.039829
2022-01-06 22:34:50,418 iteration 1925 : loss : 0.054914, loss_ce: 0.023501
2022-01-06 22:34:51,930 iteration 1926 : loss : 0.042291, loss_ce: 0.017737
2022-01-06 22:34:53,503 iteration 1927 : loss : 0.071420, loss_ce: 0.026004
2022-01-06 22:34:55,071 iteration 1928 : loss : 0.032390, loss_ce: 0.011900
2022-01-06 22:34:56,572 iteration 1929 : loss : 0.048247, loss_ce: 0.020918
2022-01-06 22:34:58,100 iteration 1930 : loss : 0.078211, loss_ce: 0.027780
2022-01-06 22:34:59,592 iteration 1931 : loss : 0.062458, loss_ce: 0.018001
2022-01-06 22:35:01,121 iteration 1932 : loss : 0.056973, loss_ce: 0.019158
2022-01-06 22:35:02,650 iteration 1933 : loss : 0.047759, loss_ce: 0.018268
2022-01-06 22:35:04,068 iteration 1934 : loss : 0.030840, loss_ce: 0.014017
2022-01-06 22:35:05,568 iteration 1935 : loss : 0.069175, loss_ce: 0.035354
2022-01-06 22:35:07,141 iteration 1936 : loss : 0.046981, loss_ce: 0.019524
2022-01-06 22:35:08,692 iteration 1937 : loss : 0.040684, loss_ce: 0.016862
2022-01-06 22:35:10,245 iteration 1938 : loss : 0.073247, loss_ce: 0.022818
 28%|████████▎                    | 114/400 [53:56<2:08:54, 27.04s/it]2022-01-06 22:35:11,752 iteration 1939 : loss : 0.040533, loss_ce: 0.017950
2022-01-06 22:35:13,250 iteration 1940 : loss : 0.032009, loss_ce: 0.011972
2022-01-06 22:35:14,826 iteration 1941 : loss : 0.051425, loss_ce: 0.021729
2022-01-06 22:35:16,339 iteration 1942 : loss : 0.060509, loss_ce: 0.018714
2022-01-06 22:35:17,796 iteration 1943 : loss : 0.039214, loss_ce: 0.014573
2022-01-06 22:35:19,297 iteration 1944 : loss : 0.039435, loss_ce: 0.014285
2022-01-06 22:35:20,816 iteration 1945 : loss : 0.048783, loss_ce: 0.024577
2022-01-06 22:35:22,334 iteration 1946 : loss : 0.074635, loss_ce: 0.035963
2022-01-06 22:35:23,829 iteration 1947 : loss : 0.057197, loss_ce: 0.022405
2022-01-06 22:35:25,327 iteration 1948 : loss : 0.040295, loss_ce: 0.016644
2022-01-06 22:35:26,788 iteration 1949 : loss : 0.041514, loss_ce: 0.021358
2022-01-06 22:35:28,340 iteration 1950 : loss : 0.060464, loss_ce: 0.024621
2022-01-06 22:35:29,946 iteration 1951 : loss : 0.069244, loss_ce: 0.024462
2022-01-06 22:35:31,409 iteration 1952 : loss : 0.054363, loss_ce: 0.017738
2022-01-06 22:35:32,951 iteration 1953 : loss : 0.058327, loss_ce: 0.029375
2022-01-06 22:35:34,536 iteration 1954 : loss : 0.044717, loss_ce: 0.015913
2022-01-06 22:35:34,537 Training Data Eval:
2022-01-06 22:35:42,350   Average segmentation loss on training set: 0.0740
2022-01-06 22:35:42,351 Validation Data Eval:
2022-01-06 22:35:45,050   Average segmentation loss on validation set: 0.1698
2022-01-06 22:35:46,556 iteration 1955 : loss : 0.040748, loss_ce: 0.015880
 29%|████████▎                    | 115/400 [54:33<2:21:40, 29.82s/it]2022-01-06 22:35:48,165 iteration 1956 : loss : 0.062939, loss_ce: 0.027307
2022-01-06 22:35:49,626 iteration 1957 : loss : 0.038199, loss_ce: 0.016089
2022-01-06 22:35:51,110 iteration 1958 : loss : 0.062355, loss_ce: 0.031384
2022-01-06 22:35:52,690 iteration 1959 : loss : 0.069317, loss_ce: 0.019341
2022-01-06 22:35:54,166 iteration 1960 : loss : 0.057133, loss_ce: 0.019011
2022-01-06 22:35:55,682 iteration 1961 : loss : 0.061441, loss_ce: 0.024859
2022-01-06 22:35:57,171 iteration 1962 : loss : 0.037883, loss_ce: 0.016119
2022-01-06 22:35:58,690 iteration 1963 : loss : 0.044458, loss_ce: 0.019292
2022-01-06 22:36:00,182 iteration 1964 : loss : 0.044278, loss_ce: 0.012718
2022-01-06 22:36:01,649 iteration 1965 : loss : 0.047486, loss_ce: 0.022041
2022-01-06 22:36:03,116 iteration 1966 : loss : 0.039212, loss_ce: 0.018990
2022-01-06 22:36:04,576 iteration 1967 : loss : 0.045349, loss_ce: 0.013051
2022-01-06 22:36:06,009 iteration 1968 : loss : 0.038976, loss_ce: 0.021041
2022-01-06 22:36:07,531 iteration 1969 : loss : 0.041323, loss_ce: 0.015663
2022-01-06 22:36:09,149 iteration 1970 : loss : 0.045302, loss_ce: 0.017041
2022-01-06 22:36:10,571 iteration 1971 : loss : 0.047106, loss_ce: 0.014873
2022-01-06 22:36:12,175 iteration 1972 : loss : 0.056310, loss_ce: 0.021073
 29%|████████▍                    | 116/400 [54:58<2:15:11, 28.56s/it]2022-01-06 22:36:13,645 iteration 1973 : loss : 0.037062, loss_ce: 0.015297
2022-01-06 22:36:15,080 iteration 1974 : loss : 0.038604, loss_ce: 0.015846
2022-01-06 22:36:16,655 iteration 1975 : loss : 0.040562, loss_ce: 0.015555
2022-01-06 22:36:18,089 iteration 1976 : loss : 0.037290, loss_ce: 0.015960
2022-01-06 22:36:19,662 iteration 1977 : loss : 0.062475, loss_ce: 0.026959
2022-01-06 22:36:21,116 iteration 1978 : loss : 0.053403, loss_ce: 0.022869
2022-01-06 22:36:22,600 iteration 1979 : loss : 0.041028, loss_ce: 0.021159
2022-01-06 22:36:24,127 iteration 1980 : loss : 0.070839, loss_ce: 0.025208
2022-01-06 22:36:25,574 iteration 1981 : loss : 0.040879, loss_ce: 0.016652
2022-01-06 22:36:27,127 iteration 1982 : loss : 0.047980, loss_ce: 0.016640
2022-01-06 22:36:28,709 iteration 1983 : loss : 0.050389, loss_ce: 0.020336
2022-01-06 22:36:30,184 iteration 1984 : loss : 0.042684, loss_ce: 0.013703
2022-01-06 22:36:31,659 iteration 1985 : loss : 0.041506, loss_ce: 0.015788
2022-01-06 22:36:33,120 iteration 1986 : loss : 0.052922, loss_ce: 0.016834
2022-01-06 22:36:34,583 iteration 1987 : loss : 0.032291, loss_ce: 0.013429
2022-01-06 22:36:36,159 iteration 1988 : loss : 0.047728, loss_ce: 0.019474
2022-01-06 22:36:37,661 iteration 1989 : loss : 0.037305, loss_ce: 0.016160
 29%|████████▍                    | 117/400 [55:24<2:10:20, 27.64s/it]2022-01-06 22:36:39,212 iteration 1990 : loss : 0.067414, loss_ce: 0.029776
2022-01-06 22:36:40,634 iteration 1991 : loss : 0.038482, loss_ce: 0.017018
2022-01-06 22:36:42,162 iteration 1992 : loss : 0.043989, loss_ce: 0.019162
2022-01-06 22:36:43,636 iteration 1993 : loss : 0.067785, loss_ce: 0.017772
2022-01-06 22:36:45,159 iteration 1994 : loss : 0.056433, loss_ce: 0.021039
2022-01-06 22:36:46,654 iteration 1995 : loss : 0.067556, loss_ce: 0.022005
2022-01-06 22:36:48,160 iteration 1996 : loss : 0.045936, loss_ce: 0.019718
2022-01-06 22:36:49,670 iteration 1997 : loss : 0.040919, loss_ce: 0.017783
2022-01-06 22:36:51,208 iteration 1998 : loss : 0.052532, loss_ce: 0.023180
2022-01-06 22:36:52,720 iteration 1999 : loss : 0.050747, loss_ce: 0.019684
2022-01-06 22:36:54,335 iteration 2000 : loss : 0.068141, loss_ce: 0.017318
2022-01-06 22:36:55,850 iteration 2001 : loss : 0.041781, loss_ce: 0.017115
2022-01-06 22:36:57,296 iteration 2002 : loss : 0.046253, loss_ce: 0.017656
2022-01-06 22:36:58,706 iteration 2003 : loss : 0.029589, loss_ce: 0.014328
2022-01-06 22:37:00,182 iteration 2004 : loss : 0.090069, loss_ce: 0.025030
2022-01-06 22:37:01,730 iteration 2005 : loss : 0.033249, loss_ce: 0.012490
2022-01-06 22:37:03,273 iteration 2006 : loss : 0.044010, loss_ce: 0.017362
 30%|████████▌                    | 118/400 [55:50<2:07:02, 27.03s/it]2022-01-06 22:37:04,894 iteration 2007 : loss : 0.048752, loss_ce: 0.018222
2022-01-06 22:37:06,511 iteration 2008 : loss : 0.050567, loss_ce: 0.016179
2022-01-06 22:37:08,019 iteration 2009 : loss : 0.045367, loss_ce: 0.016944
2022-01-06 22:37:09,538 iteration 2010 : loss : 0.045647, loss_ce: 0.019519
2022-01-06 22:37:11,163 iteration 2011 : loss : 0.068254, loss_ce: 0.026188
2022-01-06 22:37:12,722 iteration 2012 : loss : 0.049191, loss_ce: 0.024598
2022-01-06 22:37:14,217 iteration 2013 : loss : 0.035131, loss_ce: 0.014985
2022-01-06 22:37:15,757 iteration 2014 : loss : 0.054094, loss_ce: 0.018392
2022-01-06 22:37:17,352 iteration 2015 : loss : 0.037985, loss_ce: 0.015654
2022-01-06 22:37:18,898 iteration 2016 : loss : 0.063418, loss_ce: 0.026680
2022-01-06 22:37:20,493 iteration 2017 : loss : 0.045693, loss_ce: 0.015864
2022-01-06 22:37:22,151 iteration 2018 : loss : 0.045867, loss_ce: 0.018396
2022-01-06 22:37:23,696 iteration 2019 : loss : 0.051403, loss_ce: 0.018273
2022-01-06 22:37:25,224 iteration 2020 : loss : 0.034138, loss_ce: 0.012232
2022-01-06 22:37:26,803 iteration 2021 : loss : 0.054144, loss_ce: 0.018965
2022-01-06 22:37:28,381 iteration 2022 : loss : 0.039541, loss_ce: 0.018382
2022-01-06 22:37:30,000 iteration 2023 : loss : 0.049141, loss_ce: 0.018307
 30%|████████▋                    | 119/400 [56:16<2:06:09, 26.94s/it]2022-01-06 22:37:31,574 iteration 2024 : loss : 0.051641, loss_ce: 0.016849
2022-01-06 22:37:33,158 iteration 2025 : loss : 0.044403, loss_ce: 0.021078
2022-01-06 22:37:34,758 iteration 2026 : loss : 0.052108, loss_ce: 0.021942
2022-01-06 22:37:36,301 iteration 2027 : loss : 0.031453, loss_ce: 0.009742
2022-01-06 22:37:37,800 iteration 2028 : loss : 0.043986, loss_ce: 0.024242
2022-01-06 22:37:39,327 iteration 2029 : loss : 0.037744, loss_ce: 0.015349
2022-01-06 22:37:40,834 iteration 2030 : loss : 0.044569, loss_ce: 0.019852
2022-01-06 22:37:42,373 iteration 2031 : loss : 0.044987, loss_ce: 0.016613
2022-01-06 22:37:43,989 iteration 2032 : loss : 0.059638, loss_ce: 0.026609
2022-01-06 22:37:45,624 iteration 2033 : loss : 0.067357, loss_ce: 0.026773
2022-01-06 22:37:47,114 iteration 2034 : loss : 0.040642, loss_ce: 0.016470
2022-01-06 22:37:48,645 iteration 2035 : loss : 0.057949, loss_ce: 0.021111
2022-01-06 22:37:50,215 iteration 2036 : loss : 0.045679, loss_ce: 0.022846
2022-01-06 22:37:51,730 iteration 2037 : loss : 0.080188, loss_ce: 0.020452
2022-01-06 22:37:53,292 iteration 2038 : loss : 0.059290, loss_ce: 0.020923
2022-01-06 22:37:54,915 iteration 2039 : loss : 0.060806, loss_ce: 0.020925
2022-01-06 22:37:54,915 Training Data Eval:
2022-01-06 22:38:02,864   Average segmentation loss on training set: 0.0416
2022-01-06 22:38:02,864 Validation Data Eval:
2022-01-06 22:38:05,629   Average segmentation loss on validation set: 0.0943
2022-01-06 22:38:11,596 Found new lowest validation loss at iteration 2039! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 22:38:13,147 iteration 2040 : loss : 0.061112, loss_ce: 0.027204
 30%|████████▋                    | 120/400 [56:59<2:28:25, 31.80s/it]2022-01-06 22:38:14,688 iteration 2041 : loss : 0.071177, loss_ce: 0.019549
2022-01-06 22:38:16,103 iteration 2042 : loss : 0.058677, loss_ce: 0.022625
2022-01-06 22:38:17,520 iteration 2043 : loss : 0.035263, loss_ce: 0.012506
2022-01-06 22:38:18,955 iteration 2044 : loss : 0.054968, loss_ce: 0.021105
2022-01-06 22:38:20,367 iteration 2045 : loss : 0.050672, loss_ce: 0.015797
2022-01-06 22:38:21,884 iteration 2046 : loss : 0.050234, loss_ce: 0.022587
2022-01-06 22:38:23,401 iteration 2047 : loss : 0.041967, loss_ce: 0.016173
2022-01-06 22:38:24,966 iteration 2048 : loss : 0.050368, loss_ce: 0.017991
2022-01-06 22:38:26,495 iteration 2049 : loss : 0.053501, loss_ce: 0.014314
2022-01-06 22:38:28,062 iteration 2050 : loss : 0.060002, loss_ce: 0.025610
2022-01-06 22:38:29,608 iteration 2051 : loss : 0.054216, loss_ce: 0.024290
2022-01-06 22:38:31,090 iteration 2052 : loss : 0.065766, loss_ce: 0.032282
2022-01-06 22:38:32,633 iteration 2053 : loss : 0.065291, loss_ce: 0.024565
2022-01-06 22:38:34,151 iteration 2054 : loss : 0.041268, loss_ce: 0.016798
2022-01-06 22:38:35,677 iteration 2055 : loss : 0.070125, loss_ce: 0.020050
2022-01-06 22:38:37,256 iteration 2056 : loss : 0.051203, loss_ce: 0.020338
2022-01-06 22:38:38,747 iteration 2057 : loss : 0.044504, loss_ce: 0.019617
 30%|████████▊                    | 121/400 [57:25<2:19:12, 29.94s/it]2022-01-06 22:38:40,360 iteration 2058 : loss : 0.049706, loss_ce: 0.025326
2022-01-06 22:38:41,976 iteration 2059 : loss : 0.067177, loss_ce: 0.031166
2022-01-06 22:38:43,457 iteration 2060 : loss : 0.048046, loss_ce: 0.018559
2022-01-06 22:38:45,045 iteration 2061 : loss : 0.058744, loss_ce: 0.022671
2022-01-06 22:38:46,583 iteration 2062 : loss : 0.059582, loss_ce: 0.023849
2022-01-06 22:38:48,134 iteration 2063 : loss : 0.112428, loss_ce: 0.029536
2022-01-06 22:38:49,639 iteration 2064 : loss : 0.034220, loss_ce: 0.012617
2022-01-06 22:38:51,167 iteration 2065 : loss : 0.053405, loss_ce: 0.016415
2022-01-06 22:38:52,710 iteration 2066 : loss : 0.064110, loss_ce: 0.035618
2022-01-06 22:38:54,286 iteration 2067 : loss : 0.042940, loss_ce: 0.015180
2022-01-06 22:38:55,873 iteration 2068 : loss : 0.034769, loss_ce: 0.015761
2022-01-06 22:38:57,377 iteration 2069 : loss : 0.047495, loss_ce: 0.013651
2022-01-06 22:38:58,951 iteration 2070 : loss : 0.060574, loss_ce: 0.023659
2022-01-06 22:39:00,538 iteration 2071 : loss : 0.060319, loss_ce: 0.021258
2022-01-06 22:39:02,070 iteration 2072 : loss : 0.059717, loss_ce: 0.022597
2022-01-06 22:39:03,629 iteration 2073 : loss : 0.057664, loss_ce: 0.017588
2022-01-06 22:39:05,180 iteration 2074 : loss : 0.048428, loss_ce: 0.024330
 30%|████████▊                    | 122/400 [57:51<2:13:51, 28.89s/it]2022-01-06 22:39:06,735 iteration 2075 : loss : 0.035753, loss_ce: 0.013786
2022-01-06 22:39:08,278 iteration 2076 : loss : 0.045063, loss_ce: 0.017395
2022-01-06 22:39:09,849 iteration 2077 : loss : 0.047099, loss_ce: 0.020571
2022-01-06 22:39:11,505 iteration 2078 : loss : 0.065524, loss_ce: 0.021300
2022-01-06 22:39:13,039 iteration 2079 : loss : 0.060958, loss_ce: 0.021311
2022-01-06 22:39:14,632 iteration 2080 : loss : 0.058332, loss_ce: 0.022218
2022-01-06 22:39:16,087 iteration 2081 : loss : 0.037464, loss_ce: 0.012996
2022-01-06 22:39:17,526 iteration 2082 : loss : 0.029475, loss_ce: 0.010538
2022-01-06 22:39:19,049 iteration 2083 : loss : 0.056712, loss_ce: 0.031338
2022-01-06 22:39:20,574 iteration 2084 : loss : 0.058926, loss_ce: 0.028072
2022-01-06 22:39:22,077 iteration 2085 : loss : 0.051292, loss_ce: 0.015442
2022-01-06 22:39:23,620 iteration 2086 : loss : 0.081229, loss_ce: 0.037441
2022-01-06 22:39:25,138 iteration 2087 : loss : 0.055066, loss_ce: 0.021267
2022-01-06 22:39:26,770 iteration 2088 : loss : 0.039701, loss_ce: 0.019075
2022-01-06 22:39:28,376 iteration 2089 : loss : 0.064369, loss_ce: 0.035029
2022-01-06 22:39:29,884 iteration 2090 : loss : 0.048694, loss_ce: 0.022064
2022-01-06 22:39:31,487 iteration 2091 : loss : 0.048998, loss_ce: 0.019366
 31%|████████▉                    | 123/400 [58:18<2:09:47, 28.11s/it]2022-01-06 22:39:33,044 iteration 2092 : loss : 0.042707, loss_ce: 0.016214
2022-01-06 22:39:34,588 iteration 2093 : loss : 0.064750, loss_ce: 0.031081
2022-01-06 22:39:36,156 iteration 2094 : loss : 0.067012, loss_ce: 0.022535
2022-01-06 22:39:37,671 iteration 2095 : loss : 0.030361, loss_ce: 0.013435
2022-01-06 22:39:39,316 iteration 2096 : loss : 0.043287, loss_ce: 0.019288
2022-01-06 22:39:40,910 iteration 2097 : loss : 0.077552, loss_ce: 0.025845
2022-01-06 22:39:42,482 iteration 2098 : loss : 0.059252, loss_ce: 0.023261
2022-01-06 22:39:44,105 iteration 2099 : loss : 0.043136, loss_ce: 0.014225
2022-01-06 22:39:45,711 iteration 2100 : loss : 0.049360, loss_ce: 0.026271
2022-01-06 22:39:47,301 iteration 2101 : loss : 0.035160, loss_ce: 0.011765
2022-01-06 22:39:48,905 iteration 2102 : loss : 0.055050, loss_ce: 0.027037
2022-01-06 22:39:50,399 iteration 2103 : loss : 0.038888, loss_ce: 0.018801
2022-01-06 22:39:51,906 iteration 2104 : loss : 0.065158, loss_ce: 0.019650
2022-01-06 22:39:53,545 iteration 2105 : loss : 0.070538, loss_ce: 0.021254
2022-01-06 22:39:55,072 iteration 2106 : loss : 0.035973, loss_ce: 0.016417
2022-01-06 22:39:56,707 iteration 2107 : loss : 0.044482, loss_ce: 0.016858
2022-01-06 22:39:58,273 iteration 2108 : loss : 0.056693, loss_ce: 0.022128
 31%|████████▉                    | 124/400 [58:45<2:07:29, 27.72s/it]2022-01-06 22:39:59,864 iteration 2109 : loss : 0.057514, loss_ce: 0.025284
2022-01-06 22:40:01,449 iteration 2110 : loss : 0.042011, loss_ce: 0.014883
2022-01-06 22:40:03,088 iteration 2111 : loss : 0.068265, loss_ce: 0.022906
2022-01-06 22:40:04,677 iteration 2112 : loss : 0.080885, loss_ce: 0.034687
2022-01-06 22:40:06,213 iteration 2113 : loss : 0.051454, loss_ce: 0.025111
2022-01-06 22:40:07,817 iteration 2114 : loss : 0.044293, loss_ce: 0.020796
2022-01-06 22:40:09,415 iteration 2115 : loss : 0.050480, loss_ce: 0.015358
2022-01-06 22:40:11,001 iteration 2116 : loss : 0.047113, loss_ce: 0.017859
2022-01-06 22:40:12,572 iteration 2117 : loss : 0.041275, loss_ce: 0.015057
2022-01-06 22:40:14,181 iteration 2118 : loss : 0.059606, loss_ce: 0.018017
2022-01-06 22:40:15,793 iteration 2119 : loss : 0.062085, loss_ce: 0.021150
2022-01-06 22:40:17,370 iteration 2120 : loss : 0.049634, loss_ce: 0.014099
2022-01-06 22:40:18,847 iteration 2121 : loss : 0.059848, loss_ce: 0.023098
2022-01-06 22:40:20,387 iteration 2122 : loss : 0.048118, loss_ce: 0.015091
2022-01-06 22:40:21,992 iteration 2123 : loss : 0.059249, loss_ce: 0.026386
2022-01-06 22:40:23,620 iteration 2124 : loss : 0.048243, loss_ce: 0.019299
2022-01-06 22:40:23,620 Training Data Eval:
2022-01-06 22:40:31,528   Average segmentation loss on training set: 0.2613
2022-01-06 22:40:31,529 Validation Data Eval:
2022-01-06 22:40:34,250   Average segmentation loss on validation set: 0.2361
2022-01-06 22:40:35,804 iteration 2125 : loss : 0.078474, loss_ce: 0.043793
 31%|█████████                    | 125/400 [59:22<2:20:32, 30.66s/it]2022-01-06 22:40:37,484 iteration 2126 : loss : 0.037869, loss_ce: 0.014802
2022-01-06 22:40:39,048 iteration 2127 : loss : 0.088582, loss_ce: 0.047629
2022-01-06 22:40:40,615 iteration 2128 : loss : 0.050463, loss_ce: 0.017323
2022-01-06 22:40:42,125 iteration 2129 : loss : 0.059128, loss_ce: 0.021475
2022-01-06 22:40:43,691 iteration 2130 : loss : 0.113164, loss_ce: 0.041518
2022-01-06 22:40:45,201 iteration 2131 : loss : 0.052543, loss_ce: 0.018117
2022-01-06 22:40:46,727 iteration 2132 : loss : 0.056411, loss_ce: 0.024263
2022-01-06 22:40:48,314 iteration 2133 : loss : 0.052787, loss_ce: 0.017027
2022-01-06 22:40:49,776 iteration 2134 : loss : 0.045618, loss_ce: 0.022532
2022-01-06 22:40:51,297 iteration 2135 : loss : 0.040686, loss_ce: 0.014825
2022-01-06 22:40:52,800 iteration 2136 : loss : 0.043837, loss_ce: 0.018302
2022-01-06 22:40:54,316 iteration 2137 : loss : 0.061875, loss_ce: 0.024677
2022-01-06 22:40:55,796 iteration 2138 : loss : 0.073108, loss_ce: 0.025087
2022-01-06 22:40:57,365 iteration 2139 : loss : 0.042411, loss_ce: 0.021223
2022-01-06 22:40:58,895 iteration 2140 : loss : 0.046559, loss_ce: 0.020718
2022-01-06 22:41:00,488 iteration 2141 : loss : 0.055663, loss_ce: 0.016682
2022-01-06 22:41:01,992 iteration 2142 : loss : 0.043703, loss_ce: 0.013406
 32%|█████████▏                   | 126/400 [59:48<2:13:53, 29.32s/it]2022-01-06 22:41:03,525 iteration 2143 : loss : 0.046989, loss_ce: 0.016682
2022-01-06 22:41:05,081 iteration 2144 : loss : 0.035544, loss_ce: 0.011090
2022-01-06 22:41:06,641 iteration 2145 : loss : 0.040550, loss_ce: 0.012581
2022-01-06 22:41:08,128 iteration 2146 : loss : 0.026744, loss_ce: 0.011532
2022-01-06 22:41:09,671 iteration 2147 : loss : 0.048097, loss_ce: 0.016800
2022-01-06 22:41:11,182 iteration 2148 : loss : 0.043148, loss_ce: 0.017936
2022-01-06 22:41:12,711 iteration 2149 : loss : 0.043232, loss_ce: 0.017993
2022-01-06 22:41:14,214 iteration 2150 : loss : 0.028837, loss_ce: 0.011518
2022-01-06 22:41:15,796 iteration 2151 : loss : 0.066414, loss_ce: 0.029021
2022-01-06 22:41:17,268 iteration 2152 : loss : 0.043704, loss_ce: 0.018960
2022-01-06 22:41:18,738 iteration 2153 : loss : 0.032688, loss_ce: 0.013188
2022-01-06 22:41:20,363 iteration 2154 : loss : 0.048311, loss_ce: 0.017099
2022-01-06 22:41:21,870 iteration 2155 : loss : 0.047008, loss_ce: 0.017797
2022-01-06 22:41:23,339 iteration 2156 : loss : 0.044321, loss_ce: 0.018020
2022-01-06 22:41:24,974 iteration 2157 : loss : 0.089064, loss_ce: 0.032560
2022-01-06 22:41:26,561 iteration 2158 : loss : 0.058715, loss_ce: 0.030559
2022-01-06 22:41:28,090 iteration 2159 : loss : 0.072168, loss_ce: 0.025418
 32%|████████▌                  | 127/400 [1:00:14<2:08:59, 28.35s/it]2022-01-06 22:41:29,601 iteration 2160 : loss : 0.037736, loss_ce: 0.014437
2022-01-06 22:41:31,057 iteration 2161 : loss : 0.036722, loss_ce: 0.013392
2022-01-06 22:41:32,549 iteration 2162 : loss : 0.042292, loss_ce: 0.018533
2022-01-06 22:41:34,027 iteration 2163 : loss : 0.031247, loss_ce: 0.013212
2022-01-06 22:41:35,551 iteration 2164 : loss : 0.050209, loss_ce: 0.026624
2022-01-06 22:41:37,069 iteration 2165 : loss : 0.058416, loss_ce: 0.019968
2022-01-06 22:41:38,529 iteration 2166 : loss : 0.048694, loss_ce: 0.021689
2022-01-06 22:41:39,972 iteration 2167 : loss : 0.043334, loss_ce: 0.015902
2022-01-06 22:41:41,453 iteration 2168 : loss : 0.034838, loss_ce: 0.014975
2022-01-06 22:41:42,980 iteration 2169 : loss : 0.053799, loss_ce: 0.019344
2022-01-06 22:41:44,389 iteration 2170 : loss : 0.037181, loss_ce: 0.011962
2022-01-06 22:41:45,873 iteration 2171 : loss : 0.056792, loss_ce: 0.021803
2022-01-06 22:41:47,350 iteration 2172 : loss : 0.071708, loss_ce: 0.022309
2022-01-06 22:41:48,814 iteration 2173 : loss : 0.048894, loss_ce: 0.013857
2022-01-06 22:41:50,309 iteration 2174 : loss : 0.042523, loss_ce: 0.017931
2022-01-06 22:41:51,886 iteration 2175 : loss : 0.039666, loss_ce: 0.018152
2022-01-06 22:41:53,287 iteration 2176 : loss : 0.036286, loss_ce: 0.017552
 32%|████████▋                  | 128/400 [1:00:40<2:04:14, 27.41s/it]2022-01-06 22:41:54,808 iteration 2177 : loss : 0.073081, loss_ce: 0.039033
2022-01-06 22:41:56,305 iteration 2178 : loss : 0.037529, loss_ce: 0.012346
2022-01-06 22:41:57,869 iteration 2179 : loss : 0.053072, loss_ce: 0.020194
2022-01-06 22:41:59,404 iteration 2180 : loss : 0.036308, loss_ce: 0.014898
2022-01-06 22:42:00,828 iteration 2181 : loss : 0.039988, loss_ce: 0.014031
2022-01-06 22:42:02,331 iteration 2182 : loss : 0.070813, loss_ce: 0.024719
2022-01-06 22:42:03,851 iteration 2183 : loss : 0.066247, loss_ce: 0.020369
2022-01-06 22:42:05,299 iteration 2184 : loss : 0.059549, loss_ce: 0.031204
2022-01-06 22:42:06,756 iteration 2185 : loss : 0.033381, loss_ce: 0.011902
2022-01-06 22:42:08,324 iteration 2186 : loss : 0.052988, loss_ce: 0.022051
2022-01-06 22:42:09,903 iteration 2187 : loss : 0.048182, loss_ce: 0.016336
2022-01-06 22:42:11,540 iteration 2188 : loss : 0.070743, loss_ce: 0.036861
2022-01-06 22:42:13,160 iteration 2189 : loss : 0.057445, loss_ce: 0.020409
2022-01-06 22:42:14,663 iteration 2190 : loss : 0.049314, loss_ce: 0.023112
2022-01-06 22:42:16,183 iteration 2191 : loss : 0.031426, loss_ce: 0.011759
2022-01-06 22:42:17,653 iteration 2192 : loss : 0.039383, loss_ce: 0.015459
2022-01-06 22:42:19,170 iteration 2193 : loss : 0.040270, loss_ce: 0.013631
 32%|████████▋                  | 129/400 [1:01:05<2:01:43, 26.95s/it]2022-01-06 22:42:20,752 iteration 2194 : loss : 0.043545, loss_ce: 0.016308
2022-01-06 22:42:22,272 iteration 2195 : loss : 0.044604, loss_ce: 0.021471
2022-01-06 22:42:23,821 iteration 2196 : loss : 0.039144, loss_ce: 0.015926
2022-01-06 22:42:25,408 iteration 2197 : loss : 0.044245, loss_ce: 0.015120
2022-01-06 22:42:26,921 iteration 2198 : loss : 0.064083, loss_ce: 0.020223
2022-01-06 22:42:28,463 iteration 2199 : loss : 0.050650, loss_ce: 0.020726
2022-01-06 22:42:30,078 iteration 2200 : loss : 0.140584, loss_ce: 0.030753
2022-01-06 22:42:31,571 iteration 2201 : loss : 0.037206, loss_ce: 0.013854
2022-01-06 22:42:33,201 iteration 2202 : loss : 0.064676, loss_ce: 0.026054
2022-01-06 22:42:34,752 iteration 2203 : loss : 0.044182, loss_ce: 0.015547
2022-01-06 22:42:36,235 iteration 2204 : loss : 0.031937, loss_ce: 0.011006
2022-01-06 22:42:37,784 iteration 2205 : loss : 0.042566, loss_ce: 0.022525
2022-01-06 22:42:39,393 iteration 2206 : loss : 0.049975, loss_ce: 0.019161
2022-01-06 22:42:40,954 iteration 2207 : loss : 0.043687, loss_ce: 0.014993
2022-01-06 22:42:42,540 iteration 2208 : loss : 0.044663, loss_ce: 0.012035
2022-01-06 22:42:44,070 iteration 2209 : loss : 0.063078, loss_ce: 0.023698
2022-01-06 22:42:44,070 Training Data Eval:
2022-01-06 22:42:51,944   Average segmentation loss on training set: 0.0852
2022-01-06 22:42:51,944 Validation Data Eval:
2022-01-06 22:42:54,724   Average segmentation loss on validation set: 0.1251
2022-01-06 22:42:56,321 iteration 2210 : loss : 0.045229, loss_ce: 0.022037
 32%|████████▊                  | 130/400 [1:01:43<2:15:02, 30.01s/it]2022-01-06 22:42:57,946 iteration 2211 : loss : 0.037676, loss_ce: 0.011224
2022-01-06 22:42:59,466 iteration 2212 : loss : 0.045740, loss_ce: 0.019890
2022-01-06 22:43:01,034 iteration 2213 : loss : 0.064412, loss_ce: 0.016263
2022-01-06 22:43:02,518 iteration 2214 : loss : 0.040480, loss_ce: 0.013726
2022-01-06 22:43:04,004 iteration 2215 : loss : 0.029552, loss_ce: 0.011078
2022-01-06 22:43:05,530 iteration 2216 : loss : 0.050314, loss_ce: 0.025381
2022-01-06 22:43:07,025 iteration 2217 : loss : 0.040737, loss_ce: 0.019372
2022-01-06 22:43:08,536 iteration 2218 : loss : 0.029415, loss_ce: 0.010053
2022-01-06 22:43:10,100 iteration 2219 : loss : 0.038241, loss_ce: 0.018296
2022-01-06 22:43:11,718 iteration 2220 : loss : 0.040996, loss_ce: 0.018117
2022-01-06 22:43:13,310 iteration 2221 : loss : 0.048194, loss_ce: 0.016560
2022-01-06 22:43:14,763 iteration 2222 : loss : 0.034855, loss_ce: 0.013807
2022-01-06 22:43:16,242 iteration 2223 : loss : 0.038037, loss_ce: 0.014052
2022-01-06 22:43:17,844 iteration 2224 : loss : 0.046967, loss_ce: 0.018172
2022-01-06 22:43:19,388 iteration 2225 : loss : 0.043246, loss_ce: 0.015429
2022-01-06 22:43:20,966 iteration 2226 : loss : 0.049563, loss_ce: 0.021114
2022-01-06 22:43:22,523 iteration 2227 : loss : 0.067077, loss_ce: 0.019605
 33%|████████▊                  | 131/400 [1:02:09<2:09:25, 28.87s/it]2022-01-06 22:43:24,157 iteration 2228 : loss : 0.060381, loss_ce: 0.031870
2022-01-06 22:43:25,688 iteration 2229 : loss : 0.042422, loss_ce: 0.017222
2022-01-06 22:43:27,341 iteration 2230 : loss : 0.063799, loss_ce: 0.028951
2022-01-06 22:43:28,841 iteration 2231 : loss : 0.057919, loss_ce: 0.031263
2022-01-06 22:43:30,332 iteration 2232 : loss : 0.037499, loss_ce: 0.017102
2022-01-06 22:43:32,005 iteration 2233 : loss : 0.078834, loss_ce: 0.022552
2022-01-06 22:43:33,533 iteration 2234 : loss : 0.050800, loss_ce: 0.013814
2022-01-06 22:43:35,039 iteration 2235 : loss : 0.037968, loss_ce: 0.015941
2022-01-06 22:43:36,530 iteration 2236 : loss : 0.037378, loss_ce: 0.016534
2022-01-06 22:43:38,045 iteration 2237 : loss : 0.040836, loss_ce: 0.019504
2022-01-06 22:43:39,504 iteration 2238 : loss : 0.043609, loss_ce: 0.013236
2022-01-06 22:43:40,997 iteration 2239 : loss : 0.036916, loss_ce: 0.012396
2022-01-06 22:43:42,432 iteration 2240 : loss : 0.047206, loss_ce: 0.014100
2022-01-06 22:43:43,932 iteration 2241 : loss : 0.039482, loss_ce: 0.014992
2022-01-06 22:43:45,494 iteration 2242 : loss : 0.074949, loss_ce: 0.030803
2022-01-06 22:43:47,025 iteration 2243 : loss : 0.049760, loss_ce: 0.024742
2022-01-06 22:43:48,511 iteration 2244 : loss : 0.028711, loss_ce: 0.010219
 33%|████████▉                  | 132/400 [1:02:35<2:05:04, 28.00s/it]2022-01-06 22:43:50,063 iteration 2245 : loss : 0.035340, loss_ce: 0.011920
2022-01-06 22:43:51,488 iteration 2246 : loss : 0.032670, loss_ce: 0.009409
2022-01-06 22:43:53,006 iteration 2247 : loss : 0.044077, loss_ce: 0.020700
2022-01-06 22:43:54,580 iteration 2248 : loss : 0.074421, loss_ce: 0.029179
2022-01-06 22:43:56,095 iteration 2249 : loss : 0.033589, loss_ce: 0.015663
2022-01-06 22:43:57,627 iteration 2250 : loss : 0.035159, loss_ce: 0.011358
2022-01-06 22:43:59,187 iteration 2251 : loss : 0.038588, loss_ce: 0.017960
2022-01-06 22:44:00,719 iteration 2252 : loss : 0.044498, loss_ce: 0.015399
2022-01-06 22:44:02,332 iteration 2253 : loss : 0.061472, loss_ce: 0.023444
2022-01-06 22:44:03,908 iteration 2254 : loss : 0.036398, loss_ce: 0.013938
2022-01-06 22:44:05,436 iteration 2255 : loss : 0.042131, loss_ce: 0.014506
2022-01-06 22:44:06,944 iteration 2256 : loss : 0.034451, loss_ce: 0.015135
2022-01-06 22:44:08,427 iteration 2257 : loss : 0.060570, loss_ce: 0.020352
2022-01-06 22:44:09,903 iteration 2258 : loss : 0.062183, loss_ce: 0.019490
2022-01-06 22:44:11,485 iteration 2259 : loss : 0.032506, loss_ce: 0.012245
2022-01-06 22:44:12,936 iteration 2260 : loss : 0.038632, loss_ce: 0.015389
2022-01-06 22:44:14,460 iteration 2261 : loss : 0.056926, loss_ce: 0.023944
 33%|████████▉                  | 133/400 [1:03:01<2:01:52, 27.39s/it]2022-01-06 22:44:15,975 iteration 2262 : loss : 0.035430, loss_ce: 0.018048
2022-01-06 22:44:17,540 iteration 2263 : loss : 0.085677, loss_ce: 0.037237
2022-01-06 22:44:19,134 iteration 2264 : loss : 0.052007, loss_ce: 0.014885
2022-01-06 22:44:20,674 iteration 2265 : loss : 0.027993, loss_ce: 0.010932
2022-01-06 22:44:22,160 iteration 2266 : loss : 0.045193, loss_ce: 0.017094
2022-01-06 22:44:23,656 iteration 2267 : loss : 0.033816, loss_ce: 0.012963
2022-01-06 22:44:25,189 iteration 2268 : loss : 0.044309, loss_ce: 0.017511
2022-01-06 22:44:26,701 iteration 2269 : loss : 0.038650, loss_ce: 0.020995
2022-01-06 22:44:28,264 iteration 2270 : loss : 0.057511, loss_ce: 0.020598
2022-01-06 22:44:29,794 iteration 2271 : loss : 0.042710, loss_ce: 0.017121
2022-01-06 22:44:31,351 iteration 2272 : loss : 0.050521, loss_ce: 0.024592
2022-01-06 22:44:32,928 iteration 2273 : loss : 0.037345, loss_ce: 0.013186
2022-01-06 22:44:34,505 iteration 2274 : loss : 0.037787, loss_ce: 0.013235
2022-01-06 22:44:36,116 iteration 2275 : loss : 0.056428, loss_ce: 0.016860
2022-01-06 22:44:37,624 iteration 2276 : loss : 0.029937, loss_ce: 0.012163
2022-01-06 22:44:39,179 iteration 2277 : loss : 0.047287, loss_ce: 0.016052
2022-01-06 22:44:40,694 iteration 2278 : loss : 0.038844, loss_ce: 0.015525
 34%|█████████                  | 134/400 [1:03:27<1:59:52, 27.04s/it]2022-01-06 22:44:42,225 iteration 2279 : loss : 0.055046, loss_ce: 0.020799
2022-01-06 22:44:43,758 iteration 2280 : loss : 0.051822, loss_ce: 0.023440
2022-01-06 22:44:45,275 iteration 2281 : loss : 0.067856, loss_ce: 0.016406
2022-01-06 22:44:46,831 iteration 2282 : loss : 0.038008, loss_ce: 0.014860
2022-01-06 22:44:48,432 iteration 2283 : loss : 0.040337, loss_ce: 0.018274
2022-01-06 22:44:49,945 iteration 2284 : loss : 0.045585, loss_ce: 0.022354
2022-01-06 22:44:51,467 iteration 2285 : loss : 0.032768, loss_ce: 0.011934
2022-01-06 22:44:52,990 iteration 2286 : loss : 0.044367, loss_ce: 0.014995
2022-01-06 22:44:54,517 iteration 2287 : loss : 0.037419, loss_ce: 0.015145
2022-01-06 22:44:56,088 iteration 2288 : loss : 0.043762, loss_ce: 0.016787
2022-01-06 22:44:57,592 iteration 2289 : loss : 0.036478, loss_ce: 0.009706
2022-01-06 22:44:59,182 iteration 2290 : loss : 0.071775, loss_ce: 0.033645
2022-01-06 22:45:00,700 iteration 2291 : loss : 0.033183, loss_ce: 0.012942
2022-01-06 22:45:02,214 iteration 2292 : loss : 0.042179, loss_ce: 0.018879
2022-01-06 22:45:03,669 iteration 2293 : loss : 0.042770, loss_ce: 0.021380
2022-01-06 22:45:05,218 iteration 2294 : loss : 0.060666, loss_ce: 0.020064
2022-01-06 22:45:05,219 Training Data Eval:
2022-01-06 22:45:13,049   Average segmentation loss on training set: 0.0295
2022-01-06 22:45:13,049 Validation Data Eval:
2022-01-06 22:45:15,757   Average segmentation loss on validation set: 0.0725
2022-01-06 22:45:21,746 Found new lowest validation loss at iteration 2294! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 22:45:23,294 iteration 2295 : loss : 0.066126, loss_ce: 0.026328
 34%|█████████                  | 135/400 [1:04:10<2:20:02, 31.71s/it]2022-01-06 22:45:24,714 iteration 2296 : loss : 0.031922, loss_ce: 0.012986
2022-01-06 22:45:26,136 iteration 2297 : loss : 0.041838, loss_ce: 0.018895
2022-01-06 22:45:27,602 iteration 2298 : loss : 0.030897, loss_ce: 0.013355
2022-01-06 22:45:29,112 iteration 2299 : loss : 0.061777, loss_ce: 0.028639
2022-01-06 22:45:30,521 iteration 2300 : loss : 0.035552, loss_ce: 0.015921
2022-01-06 22:45:32,031 iteration 2301 : loss : 0.045618, loss_ce: 0.014040
2022-01-06 22:45:33,518 iteration 2302 : loss : 0.053785, loss_ce: 0.019207
2022-01-06 22:45:34,974 iteration 2303 : loss : 0.040235, loss_ce: 0.012982
2022-01-06 22:45:36,493 iteration 2304 : loss : 0.051462, loss_ce: 0.016352
2022-01-06 22:45:37,970 iteration 2305 : loss : 0.041279, loss_ce: 0.017348
2022-01-06 22:45:39,517 iteration 2306 : loss : 0.042402, loss_ce: 0.022404
2022-01-06 22:45:40,970 iteration 2307 : loss : 0.099420, loss_ce: 0.041457
2022-01-06 22:45:42,422 iteration 2308 : loss : 0.045491, loss_ce: 0.015063
2022-01-06 22:45:43,903 iteration 2309 : loss : 0.035909, loss_ce: 0.016862
2022-01-06 22:45:45,480 iteration 2310 : loss : 0.035943, loss_ce: 0.014828
2022-01-06 22:45:46,909 iteration 2311 : loss : 0.046979, loss_ce: 0.017033
2022-01-06 22:45:48,418 iteration 2312 : loss : 0.051915, loss_ce: 0.018080
 34%|█████████▏                 | 136/400 [1:04:35<2:10:48, 29.73s/it]2022-01-06 22:45:50,017 iteration 2313 : loss : 0.037290, loss_ce: 0.013878
2022-01-06 22:45:51,421 iteration 2314 : loss : 0.031335, loss_ce: 0.010533
2022-01-06 22:45:52,876 iteration 2315 : loss : 0.030901, loss_ce: 0.011356
2022-01-06 22:45:54,446 iteration 2316 : loss : 0.068920, loss_ce: 0.032005
2022-01-06 22:45:55,882 iteration 2317 : loss : 0.031463, loss_ce: 0.010494
2022-01-06 22:45:57,347 iteration 2318 : loss : 0.040182, loss_ce: 0.018103
2022-01-06 22:45:58,846 iteration 2319 : loss : 0.055265, loss_ce: 0.024613
2022-01-06 22:46:00,416 iteration 2320 : loss : 0.049289, loss_ce: 0.022570
2022-01-06 22:46:01,912 iteration 2321 : loss : 0.048510, loss_ce: 0.020761
2022-01-06 22:46:03,422 iteration 2322 : loss : 0.040324, loss_ce: 0.016965
2022-01-06 22:46:04,840 iteration 2323 : loss : 0.043793, loss_ce: 0.019688
2022-01-06 22:46:06,326 iteration 2324 : loss : 0.027165, loss_ce: 0.012459
2022-01-06 22:46:07,730 iteration 2325 : loss : 0.032099, loss_ce: 0.013737
2022-01-06 22:46:09,232 iteration 2326 : loss : 0.076606, loss_ce: 0.016412
2022-01-06 22:46:10,715 iteration 2327 : loss : 0.049625, loss_ce: 0.019385
2022-01-06 22:46:12,228 iteration 2328 : loss : 0.046038, loss_ce: 0.018568
2022-01-06 22:46:13,696 iteration 2329 : loss : 0.033180, loss_ce: 0.013626
 34%|█████████▏                 | 137/400 [1:05:00<2:04:28, 28.40s/it]2022-01-06 22:46:15,281 iteration 2330 : loss : 0.047344, loss_ce: 0.018691
2022-01-06 22:46:16,753 iteration 2331 : loss : 0.039471, loss_ce: 0.014282
2022-01-06 22:46:18,335 iteration 2332 : loss : 0.040290, loss_ce: 0.015608
2022-01-06 22:46:19,833 iteration 2333 : loss : 0.059304, loss_ce: 0.024062
2022-01-06 22:46:21,456 iteration 2334 : loss : 0.036241, loss_ce: 0.012216
2022-01-06 22:46:23,063 iteration 2335 : loss : 0.045676, loss_ce: 0.018648
2022-01-06 22:46:24,624 iteration 2336 : loss : 0.033819, loss_ce: 0.013603
2022-01-06 22:46:26,132 iteration 2337 : loss : 0.060061, loss_ce: 0.021165
2022-01-06 22:46:27,673 iteration 2338 : loss : 0.057137, loss_ce: 0.021080
2022-01-06 22:46:29,200 iteration 2339 : loss : 0.054021, loss_ce: 0.018749
2022-01-06 22:46:30,656 iteration 2340 : loss : 0.041240, loss_ce: 0.014877
2022-01-06 22:46:32,177 iteration 2341 : loss : 0.041627, loss_ce: 0.016863
2022-01-06 22:46:33,708 iteration 2342 : loss : 0.029846, loss_ce: 0.010938
2022-01-06 22:46:35,326 iteration 2343 : loss : 0.036144, loss_ce: 0.013602
2022-01-06 22:46:36,798 iteration 2344 : loss : 0.069095, loss_ce: 0.018514
2022-01-06 22:46:38,287 iteration 2345 : loss : 0.054162, loss_ce: 0.028182
2022-01-06 22:46:39,805 iteration 2346 : loss : 0.055506, loss_ce: 0.022347
 34%|█████████▎                 | 138/400 [1:05:26<2:01:00, 27.71s/it]2022-01-06 22:46:41,451 iteration 2347 : loss : 0.070350, loss_ce: 0.032352
2022-01-06 22:46:42,968 iteration 2348 : loss : 0.033429, loss_ce: 0.012118
2022-01-06 22:46:44,450 iteration 2349 : loss : 0.041445, loss_ce: 0.015135
2022-01-06 22:46:45,954 iteration 2350 : loss : 0.065402, loss_ce: 0.018349
2022-01-06 22:46:47,529 iteration 2351 : loss : 0.040842, loss_ce: 0.018220
2022-01-06 22:46:48,990 iteration 2352 : loss : 0.034204, loss_ce: 0.016023
2022-01-06 22:46:50,457 iteration 2353 : loss : 0.036481, loss_ce: 0.015478
2022-01-06 22:46:51,916 iteration 2354 : loss : 0.027686, loss_ce: 0.013826
2022-01-06 22:46:53,393 iteration 2355 : loss : 0.055001, loss_ce: 0.020526
2022-01-06 22:46:54,963 iteration 2356 : loss : 0.042852, loss_ce: 0.015884
2022-01-06 22:46:56,493 iteration 2357 : loss : 0.057489, loss_ce: 0.016768
2022-01-06 22:46:58,082 iteration 2358 : loss : 0.042091, loss_ce: 0.016913
2022-01-06 22:46:59,572 iteration 2359 : loss : 0.057031, loss_ce: 0.019143
2022-01-06 22:47:01,037 iteration 2360 : loss : 0.053426, loss_ce: 0.013684
2022-01-06 22:47:02,538 iteration 2361 : loss : 0.036127, loss_ce: 0.013131
2022-01-06 22:47:04,181 iteration 2362 : loss : 0.063273, loss_ce: 0.026327
2022-01-06 22:47:05,692 iteration 2363 : loss : 0.046029, loss_ce: 0.024160
 35%|█████████▍                 | 139/400 [1:05:52<1:58:10, 27.16s/it]2022-01-06 22:47:07,331 iteration 2364 : loss : 0.033234, loss_ce: 0.012220
2022-01-06 22:47:08,922 iteration 2365 : loss : 0.057796, loss_ce: 0.020118
2022-01-06 22:47:10,471 iteration 2366 : loss : 0.050987, loss_ce: 0.019628
2022-01-06 22:47:11,927 iteration 2367 : loss : 0.037032, loss_ce: 0.011035
2022-01-06 22:47:13,394 iteration 2368 : loss : 0.044980, loss_ce: 0.020836
2022-01-06 22:47:14,929 iteration 2369 : loss : 0.052528, loss_ce: 0.013849
2022-01-06 22:47:16,465 iteration 2370 : loss : 0.049272, loss_ce: 0.017762
2022-01-06 22:47:18,057 iteration 2371 : loss : 0.036110, loss_ce: 0.015037
2022-01-06 22:47:19,578 iteration 2372 : loss : 0.056994, loss_ce: 0.021417
2022-01-06 22:47:21,126 iteration 2373 : loss : 0.054485, loss_ce: 0.019082
2022-01-06 22:47:22,633 iteration 2374 : loss : 0.052509, loss_ce: 0.021888
2022-01-06 22:47:24,161 iteration 2375 : loss : 0.036449, loss_ce: 0.011681
2022-01-06 22:47:25,733 iteration 2376 : loss : 0.053980, loss_ce: 0.027304
2022-01-06 22:47:27,293 iteration 2377 : loss : 0.042322, loss_ce: 0.016496
2022-01-06 22:47:28,850 iteration 2378 : loss : 0.030907, loss_ce: 0.012155
2022-01-06 22:47:30,435 iteration 2379 : loss : 0.050314, loss_ce: 0.020138
2022-01-06 22:47:30,435 Training Data Eval:
2022-01-06 22:47:38,373   Average segmentation loss on training set: 0.0985
2022-01-06 22:47:38,374 Validation Data Eval:
2022-01-06 22:47:41,145   Average segmentation loss on validation set: 0.1195
2022-01-06 22:47:42,680 iteration 2380 : loss : 0.025902, loss_ce: 0.012134
 35%|█████████▍                 | 140/400 [1:06:29<2:10:27, 30.11s/it]2022-01-06 22:47:44,250 iteration 2381 : loss : 0.040957, loss_ce: 0.014732
2022-01-06 22:47:45,842 iteration 2382 : loss : 0.039070, loss_ce: 0.011777
2022-01-06 22:47:47,361 iteration 2383 : loss : 0.030375, loss_ce: 0.010607
2022-01-06 22:47:48,965 iteration 2384 : loss : 0.051621, loss_ce: 0.020622
2022-01-06 22:47:50,583 iteration 2385 : loss : 0.045392, loss_ce: 0.019227
2022-01-06 22:47:52,128 iteration 2386 : loss : 0.040730, loss_ce: 0.014075
2022-01-06 22:47:53,654 iteration 2387 : loss : 0.035465, loss_ce: 0.012198
2022-01-06 22:47:55,205 iteration 2388 : loss : 0.036962, loss_ce: 0.016091
2022-01-06 22:47:56,701 iteration 2389 : loss : 0.021445, loss_ce: 0.006603
2022-01-06 22:47:58,260 iteration 2390 : loss : 0.047126, loss_ce: 0.031585
2022-01-06 22:47:59,783 iteration 2391 : loss : 0.054586, loss_ce: 0.038786
2022-01-06 22:48:01,342 iteration 2392 : loss : 0.033666, loss_ce: 0.012535
2022-01-06 22:48:02,901 iteration 2393 : loss : 0.057756, loss_ce: 0.016854
2022-01-06 22:48:04,347 iteration 2394 : loss : 0.034540, loss_ce: 0.011825
2022-01-06 22:48:05,915 iteration 2395 : loss : 0.025947, loss_ce: 0.011707
2022-01-06 22:48:07,386 iteration 2396 : loss : 0.027386, loss_ce: 0.012729
2022-01-06 22:48:08,974 iteration 2397 : loss : 0.038599, loss_ce: 0.015472
 35%|█████████▌                 | 141/400 [1:06:55<2:05:02, 28.97s/it]2022-01-06 22:48:10,554 iteration 2398 : loss : 0.055942, loss_ce: 0.019287
2022-01-06 22:48:12,051 iteration 2399 : loss : 0.038963, loss_ce: 0.013800
2022-01-06 22:48:13,624 iteration 2400 : loss : 0.040982, loss_ce: 0.019946
2022-01-06 22:48:15,150 iteration 2401 : loss : 0.036408, loss_ce: 0.012336
2022-01-06 22:48:16,723 iteration 2402 : loss : 0.037421, loss_ce: 0.019973
2022-01-06 22:48:18,222 iteration 2403 : loss : 0.035863, loss_ce: 0.014798
2022-01-06 22:48:19,702 iteration 2404 : loss : 0.032421, loss_ce: 0.015728
2022-01-06 22:48:21,153 iteration 2405 : loss : 0.033508, loss_ce: 0.012036
2022-01-06 22:48:22,813 iteration 2406 : loss : 0.048324, loss_ce: 0.014734
2022-01-06 22:48:24,305 iteration 2407 : loss : 0.040525, loss_ce: 0.021525
2022-01-06 22:48:25,803 iteration 2408 : loss : 0.031519, loss_ce: 0.013933
2022-01-06 22:48:27,393 iteration 2409 : loss : 0.033912, loss_ce: 0.011388
2022-01-06 22:48:28,991 iteration 2410 : loss : 0.037637, loss_ce: 0.014989
2022-01-06 22:48:30,567 iteration 2411 : loss : 0.049166, loss_ce: 0.013855
2022-01-06 22:48:32,074 iteration 2412 : loss : 0.027763, loss_ce: 0.011992
2022-01-06 22:48:33,687 iteration 2413 : loss : 0.051973, loss_ce: 0.024334
2022-01-06 22:48:35,244 iteration 2414 : loss : 0.031274, loss_ce: 0.012567
 36%|█████████▌                 | 142/400 [1:07:21<2:01:04, 28.16s/it]2022-01-06 22:48:36,876 iteration 2415 : loss : 0.049087, loss_ce: 0.017204
2022-01-06 22:48:38,415 iteration 2416 : loss : 0.030514, loss_ce: 0.012076
2022-01-06 22:48:39,958 iteration 2417 : loss : 0.047259, loss_ce: 0.017307
2022-01-06 22:48:41,579 iteration 2418 : loss : 0.034206, loss_ce: 0.012667
2022-01-06 22:48:43,128 iteration 2419 : loss : 0.046191, loss_ce: 0.021682
2022-01-06 22:48:44,761 iteration 2420 : loss : 0.046540, loss_ce: 0.021481
2022-01-06 22:48:46,371 iteration 2421 : loss : 0.043670, loss_ce: 0.017523
2022-01-06 22:48:47,959 iteration 2422 : loss : 0.043321, loss_ce: 0.015802
2022-01-06 22:48:49,486 iteration 2423 : loss : 0.042406, loss_ce: 0.015491
2022-01-06 22:48:50,990 iteration 2424 : loss : 0.031917, loss_ce: 0.013337
2022-01-06 22:48:52,615 iteration 2425 : loss : 0.042860, loss_ce: 0.013664
2022-01-06 22:48:54,136 iteration 2426 : loss : 0.053181, loss_ce: 0.016311
2022-01-06 22:48:55,581 iteration 2427 : loss : 0.030495, loss_ce: 0.013389
2022-01-06 22:48:57,078 iteration 2428 : loss : 0.038194, loss_ce: 0.011254
2022-01-06 22:48:58,569 iteration 2429 : loss : 0.039347, loss_ce: 0.016185
2022-01-06 22:49:00,062 iteration 2430 : loss : 0.024781, loss_ce: 0.009481
2022-01-06 22:49:01,588 iteration 2431 : loss : 0.043490, loss_ce: 0.021778
 36%|█████████▋                 | 143/400 [1:07:48<1:58:15, 27.61s/it]2022-01-06 22:49:03,131 iteration 2432 : loss : 0.033452, loss_ce: 0.010984
2022-01-06 22:49:04,638 iteration 2433 : loss : 0.039356, loss_ce: 0.016111
2022-01-06 22:49:06,161 iteration 2434 : loss : 0.040376, loss_ce: 0.013241
2022-01-06 22:49:07,762 iteration 2435 : loss : 0.050979, loss_ce: 0.026963
2022-01-06 22:49:09,202 iteration 2436 : loss : 0.036717, loss_ce: 0.016749
2022-01-06 22:49:10,766 iteration 2437 : loss : 0.035374, loss_ce: 0.012666
2022-01-06 22:49:12,330 iteration 2438 : loss : 0.036898, loss_ce: 0.012104
2022-01-06 22:49:13,880 iteration 2439 : loss : 0.035119, loss_ce: 0.011332
2022-01-06 22:49:15,446 iteration 2440 : loss : 0.044931, loss_ce: 0.018079
2022-01-06 22:49:16,966 iteration 2441 : loss : 0.030995, loss_ce: 0.010796
2022-01-06 22:49:18,471 iteration 2442 : loss : 0.034871, loss_ce: 0.014141
2022-01-06 22:49:19,933 iteration 2443 : loss : 0.043589, loss_ce: 0.015428
2022-01-06 22:49:21,450 iteration 2444 : loss : 0.042640, loss_ce: 0.016589
2022-01-06 22:49:22,969 iteration 2445 : loss : 0.032327, loss_ce: 0.012816
2022-01-06 22:49:24,563 iteration 2446 : loss : 0.039152, loss_ce: 0.019814
2022-01-06 22:49:26,024 iteration 2447 : loss : 0.030332, loss_ce: 0.010068
2022-01-06 22:49:27,577 iteration 2448 : loss : 0.035705, loss_ce: 0.011890
 36%|█████████▋                 | 144/400 [1:08:14<1:55:43, 27.12s/it]2022-01-06 22:49:29,085 iteration 2449 : loss : 0.046210, loss_ce: 0.015232
2022-01-06 22:49:30,548 iteration 2450 : loss : 0.030631, loss_ce: 0.015613
2022-01-06 22:49:32,047 iteration 2451 : loss : 0.028230, loss_ce: 0.008390
2022-01-06 22:49:33,535 iteration 2452 : loss : 0.040511, loss_ce: 0.015135
2022-01-06 22:49:35,060 iteration 2453 : loss : 0.031866, loss_ce: 0.011745
2022-01-06 22:49:36,569 iteration 2454 : loss : 0.040442, loss_ce: 0.014367
2022-01-06 22:49:38,065 iteration 2455 : loss : 0.040926, loss_ce: 0.020646
2022-01-06 22:49:39,544 iteration 2456 : loss : 0.031497, loss_ce: 0.014437
2022-01-06 22:49:41,065 iteration 2457 : loss : 0.033966, loss_ce: 0.014702
2022-01-06 22:49:42,560 iteration 2458 : loss : 0.035048, loss_ce: 0.013819
2022-01-06 22:49:44,003 iteration 2459 : loss : 0.024170, loss_ce: 0.009336
2022-01-06 22:49:45,521 iteration 2460 : loss : 0.048709, loss_ce: 0.012454
2022-01-06 22:49:47,124 iteration 2461 : loss : 0.033248, loss_ce: 0.011904
2022-01-06 22:49:48,687 iteration 2462 : loss : 0.056420, loss_ce: 0.019793
2022-01-06 22:49:50,204 iteration 2463 : loss : 0.031747, loss_ce: 0.012804
2022-01-06 22:49:51,758 iteration 2464 : loss : 0.028370, loss_ce: 0.009992
2022-01-06 22:49:51,793 Training Data Eval:
2022-01-06 22:49:59,711   Average segmentation loss on training set: 0.1491
2022-01-06 22:49:59,712 Validation Data Eval:
2022-01-06 22:50:02,447   Average segmentation loss on validation set: 0.2168
2022-01-06 22:50:03,965 iteration 2465 : loss : 0.035453, loss_ce: 0.014389
 36%|█████████▊                 | 145/400 [1:08:50<2:07:05, 29.90s/it]2022-01-06 22:50:05,612 iteration 2466 : loss : 0.043368, loss_ce: 0.014825
2022-01-06 22:50:07,205 iteration 2467 : loss : 0.053396, loss_ce: 0.025687
2022-01-06 22:50:08,714 iteration 2468 : loss : 0.046713, loss_ce: 0.020641
2022-01-06 22:50:10,205 iteration 2469 : loss : 0.033279, loss_ce: 0.014737
2022-01-06 22:50:11,737 iteration 2470 : loss : 0.040774, loss_ce: 0.013985
2022-01-06 22:50:13,369 iteration 2471 : loss : 0.070502, loss_ce: 0.028481
2022-01-06 22:50:14,886 iteration 2472 : loss : 0.044995, loss_ce: 0.018099
2022-01-06 22:50:16,484 iteration 2473 : loss : 0.045899, loss_ce: 0.018496
2022-01-06 22:50:17,988 iteration 2474 : loss : 0.035584, loss_ce: 0.013336
2022-01-06 22:50:19,534 iteration 2475 : loss : 0.043032, loss_ce: 0.014787
2022-01-06 22:50:21,035 iteration 2476 : loss : 0.033967, loss_ce: 0.016929
2022-01-06 22:50:22,656 iteration 2477 : loss : 0.051637, loss_ce: 0.020316
2022-01-06 22:50:24,233 iteration 2478 : loss : 0.041792, loss_ce: 0.016513
2022-01-06 22:50:25,708 iteration 2479 : loss : 0.033958, loss_ce: 0.011782
2022-01-06 22:50:27,258 iteration 2480 : loss : 0.037367, loss_ce: 0.014230
2022-01-06 22:50:28,867 iteration 2481 : loss : 0.035861, loss_ce: 0.011483
2022-01-06 22:50:30,397 iteration 2482 : loss : 0.033188, loss_ce: 0.014240
 36%|█████████▊                 | 146/400 [1:09:17<2:02:10, 28.86s/it]2022-01-06 22:50:32,092 iteration 2483 : loss : 0.070043, loss_ce: 0.034665
2022-01-06 22:50:33,688 iteration 2484 : loss : 0.051829, loss_ce: 0.009697
2022-01-06 22:50:35,213 iteration 2485 : loss : 0.032884, loss_ce: 0.014702
2022-01-06 22:50:36,753 iteration 2486 : loss : 0.037156, loss_ce: 0.013218
2022-01-06 22:50:38,275 iteration 2487 : loss : 0.043297, loss_ce: 0.013849
2022-01-06 22:50:39,867 iteration 2488 : loss : 0.046709, loss_ce: 0.021866
2022-01-06 22:50:41,397 iteration 2489 : loss : 0.044526, loss_ce: 0.020528
2022-01-06 22:50:42,984 iteration 2490 : loss : 0.049731, loss_ce: 0.020482
2022-01-06 22:50:44,618 iteration 2491 : loss : 0.046960, loss_ce: 0.020649
2022-01-06 22:50:46,057 iteration 2492 : loss : 0.028693, loss_ce: 0.011210
2022-01-06 22:50:47,606 iteration 2493 : loss : 0.039062, loss_ce: 0.016946
2022-01-06 22:50:49,130 iteration 2494 : loss : 0.063703, loss_ce: 0.019888
2022-01-06 22:50:50,643 iteration 2495 : loss : 0.046470, loss_ce: 0.028037
2022-01-06 22:50:52,343 iteration 2496 : loss : 0.073118, loss_ce: 0.026764
2022-01-06 22:50:53,923 iteration 2497 : loss : 0.043639, loss_ce: 0.013904
2022-01-06 22:50:55,496 iteration 2498 : loss : 0.045307, loss_ce: 0.019524
2022-01-06 22:50:56,968 iteration 2499 : loss : 0.033376, loss_ce: 0.010034
 37%|█████████▉                 | 147/400 [1:09:43<1:58:48, 28.18s/it]2022-01-06 22:50:58,549 iteration 2500 : loss : 0.038348, loss_ce: 0.013383
2022-01-06 22:51:00,079 iteration 2501 : loss : 0.034361, loss_ce: 0.011331
2022-01-06 22:51:01,682 iteration 2502 : loss : 0.054907, loss_ce: 0.020414
2022-01-06 22:51:03,171 iteration 2503 : loss : 0.033372, loss_ce: 0.013977
2022-01-06 22:51:04,688 iteration 2504 : loss : 0.036750, loss_ce: 0.013127
2022-01-06 22:51:06,311 iteration 2505 : loss : 0.054429, loss_ce: 0.021555
2022-01-06 22:51:07,861 iteration 2506 : loss : 0.044172, loss_ce: 0.014787
2022-01-06 22:51:09,365 iteration 2507 : loss : 0.039281, loss_ce: 0.016916
2022-01-06 22:51:10,900 iteration 2508 : loss : 0.032337, loss_ce: 0.011001
2022-01-06 22:51:12,427 iteration 2509 : loss : 0.036115, loss_ce: 0.011993
2022-01-06 22:51:13,945 iteration 2510 : loss : 0.030068, loss_ce: 0.012642
2022-01-06 22:51:15,508 iteration 2511 : loss : 0.034233, loss_ce: 0.014722
2022-01-06 22:51:17,070 iteration 2512 : loss : 0.038872, loss_ce: 0.016320
2022-01-06 22:51:18,658 iteration 2513 : loss : 0.041280, loss_ce: 0.012082
2022-01-06 22:51:20,203 iteration 2514 : loss : 0.047059, loss_ce: 0.017278
2022-01-06 22:51:21,729 iteration 2515 : loss : 0.030713, loss_ce: 0.018762
2022-01-06 22:51:23,237 iteration 2516 : loss : 0.025883, loss_ce: 0.012217
 37%|█████████▉                 | 148/400 [1:10:09<1:55:55, 27.60s/it]2022-01-06 22:51:24,799 iteration 2517 : loss : 0.031376, loss_ce: 0.012888
2022-01-06 22:51:26,340 iteration 2518 : loss : 0.042647, loss_ce: 0.015405
2022-01-06 22:51:27,900 iteration 2519 : loss : 0.032806, loss_ce: 0.013109
2022-01-06 22:51:29,429 iteration 2520 : loss : 0.037127, loss_ce: 0.014833
2022-01-06 22:51:30,957 iteration 2521 : loss : 0.032770, loss_ce: 0.012335
2022-01-06 22:51:32,500 iteration 2522 : loss : 0.039110, loss_ce: 0.020376
2022-01-06 22:51:34,079 iteration 2523 : loss : 0.074366, loss_ce: 0.018851
2022-01-06 22:51:35,589 iteration 2524 : loss : 0.033690, loss_ce: 0.013720
2022-01-06 22:51:37,115 iteration 2525 : loss : 0.037531, loss_ce: 0.017835
2022-01-06 22:51:38,655 iteration 2526 : loss : 0.047829, loss_ce: 0.016632
2022-01-06 22:51:40,175 iteration 2527 : loss : 0.039440, loss_ce: 0.013878
2022-01-06 22:51:41,725 iteration 2528 : loss : 0.036287, loss_ce: 0.014556
2022-01-06 22:51:43,261 iteration 2529 : loss : 0.041090, loss_ce: 0.015713
2022-01-06 22:51:44,787 iteration 2530 : loss : 0.038113, loss_ce: 0.017613
2022-01-06 22:51:46,369 iteration 2531 : loss : 0.034492, loss_ce: 0.014064
2022-01-06 22:51:47,874 iteration 2532 : loss : 0.060228, loss_ce: 0.022069
2022-01-06 22:51:49,389 iteration 2533 : loss : 0.042365, loss_ce: 0.012364
 37%|██████████                 | 149/400 [1:10:36<1:53:38, 27.17s/it]2022-01-06 22:51:50,936 iteration 2534 : loss : 0.035821, loss_ce: 0.016942
2022-01-06 22:51:52,531 iteration 2535 : loss : 0.048672, loss_ce: 0.016324
2022-01-06 22:51:54,023 iteration 2536 : loss : 0.042322, loss_ce: 0.021191
2022-01-06 22:51:55,534 iteration 2537 : loss : 0.032052, loss_ce: 0.012381
2022-01-06 22:51:56,981 iteration 2538 : loss : 0.028947, loss_ce: 0.012224
2022-01-06 22:51:58,527 iteration 2539 : loss : 0.030221, loss_ce: 0.011839
2022-01-06 22:52:00,014 iteration 2540 : loss : 0.031727, loss_ce: 0.009882
2022-01-06 22:52:01,523 iteration 2541 : loss : 0.037913, loss_ce: 0.017734
2022-01-06 22:52:02,981 iteration 2542 : loss : 0.023144, loss_ce: 0.008080
2022-01-06 22:52:04,447 iteration 2543 : loss : 0.038642, loss_ce: 0.014428
2022-01-06 22:52:05,967 iteration 2544 : loss : 0.039067, loss_ce: 0.012744
2022-01-06 22:52:07,590 iteration 2545 : loss : 0.031721, loss_ce: 0.012017
2022-01-06 22:52:09,252 iteration 2546 : loss : 0.045443, loss_ce: 0.018807
2022-01-06 22:52:10,714 iteration 2547 : loss : 0.038986, loss_ce: 0.018737
2022-01-06 22:52:12,271 iteration 2548 : loss : 0.031959, loss_ce: 0.015187
2022-01-06 22:52:13,810 iteration 2549 : loss : 0.041879, loss_ce: 0.010256
2022-01-06 22:52:13,810 Training Data Eval:
2022-01-06 22:52:21,670   Average segmentation loss on training set: 0.0280
2022-01-06 22:52:21,670 Validation Data Eval:
2022-01-06 22:52:24,412   Average segmentation loss on validation set: 0.0827
2022-01-06 22:52:25,920 iteration 2550 : loss : 0.038641, loss_ce: 0.011788
 38%|██████████▏                | 150/400 [1:11:12<2:04:54, 29.98s/it]2022-01-06 22:52:27,509 iteration 2551 : loss : 0.029821, loss_ce: 0.013953
2022-01-06 22:52:29,090 iteration 2552 : loss : 0.024564, loss_ce: 0.009959
2022-01-06 22:52:30,554 iteration 2553 : loss : 0.022364, loss_ce: 0.007928
2022-01-06 22:52:32,079 iteration 2554 : loss : 0.038727, loss_ce: 0.016011
2022-01-06 22:52:33,584 iteration 2555 : loss : 0.033338, loss_ce: 0.008984
2022-01-06 22:52:35,154 iteration 2556 : loss : 0.033183, loss_ce: 0.013716
2022-01-06 22:52:36,667 iteration 2557 : loss : 0.038476, loss_ce: 0.017550
2022-01-06 22:52:38,205 iteration 2558 : loss : 0.038517, loss_ce: 0.020376
2022-01-06 22:52:39,813 iteration 2559 : loss : 0.036623, loss_ce: 0.013768
2022-01-06 22:52:41,343 iteration 2560 : loss : 0.031023, loss_ce: 0.015729
2022-01-06 22:52:42,971 iteration 2561 : loss : 0.038219, loss_ce: 0.013606
2022-01-06 22:52:44,540 iteration 2562 : loss : 0.044731, loss_ce: 0.017094
2022-01-06 22:52:46,115 iteration 2563 : loss : 0.042561, loss_ce: 0.018473
2022-01-06 22:52:47,641 iteration 2564 : loss : 0.032658, loss_ce: 0.013699
2022-01-06 22:52:49,148 iteration 2565 : loss : 0.035444, loss_ce: 0.012320
2022-01-06 22:52:50,680 iteration 2566 : loss : 0.032838, loss_ce: 0.012266
2022-01-06 22:52:52,235 iteration 2567 : loss : 0.048847, loss_ce: 0.012535
 38%|██████████▏                | 151/400 [1:11:38<1:59:50, 28.88s/it]2022-01-06 22:52:53,853 iteration 2568 : loss : 0.035720, loss_ce: 0.016703
2022-01-06 22:52:55,372 iteration 2569 : loss : 0.040450, loss_ce: 0.019048
2022-01-06 22:52:56,897 iteration 2570 : loss : 0.039743, loss_ce: 0.014966
2022-01-06 22:52:58,428 iteration 2571 : loss : 0.024240, loss_ce: 0.009228
2022-01-06 22:52:59,910 iteration 2572 : loss : 0.031146, loss_ce: 0.009584
2022-01-06 22:53:01,417 iteration 2573 : loss : 0.040291, loss_ce: 0.016441
2022-01-06 22:53:02,946 iteration 2574 : loss : 0.048136, loss_ce: 0.017822
2022-01-06 22:53:04,601 iteration 2575 : loss : 0.048997, loss_ce: 0.019276
2022-01-06 22:53:06,173 iteration 2576 : loss : 0.039656, loss_ce: 0.016863
2022-01-06 22:53:07,718 iteration 2577 : loss : 0.051431, loss_ce: 0.020977
2022-01-06 22:53:09,264 iteration 2578 : loss : 0.051669, loss_ce: 0.018353
2022-01-06 22:53:10,753 iteration 2579 : loss : 0.030305, loss_ce: 0.009197
2022-01-06 22:53:12,337 iteration 2580 : loss : 0.032408, loss_ce: 0.009923
2022-01-06 22:53:13,906 iteration 2581 : loss : 0.036320, loss_ce: 0.014536
2022-01-06 22:53:15,503 iteration 2582 : loss : 0.033577, loss_ce: 0.011935
2022-01-06 22:53:17,027 iteration 2583 : loss : 0.039653, loss_ce: 0.015121
2022-01-06 22:53:18,475 iteration 2584 : loss : 0.037475, loss_ce: 0.015632
 38%|██████████▎                | 152/400 [1:12:05<1:56:06, 28.09s/it]2022-01-06 22:53:20,052 iteration 2585 : loss : 0.026489, loss_ce: 0.007649
2022-01-06 22:53:21,577 iteration 2586 : loss : 0.037782, loss_ce: 0.015973
2022-01-06 22:53:23,041 iteration 2587 : loss : 0.032829, loss_ce: 0.012629
2022-01-06 22:53:24,547 iteration 2588 : loss : 0.035890, loss_ce: 0.017490
2022-01-06 22:53:26,112 iteration 2589 : loss : 0.034735, loss_ce: 0.012903
2022-01-06 22:53:27,688 iteration 2590 : loss : 0.041237, loss_ce: 0.015958
2022-01-06 22:53:29,205 iteration 2591 : loss : 0.031686, loss_ce: 0.011087
2022-01-06 22:53:30,726 iteration 2592 : loss : 0.037192, loss_ce: 0.013205
2022-01-06 22:53:32,253 iteration 2593 : loss : 0.031621, loss_ce: 0.014370
2022-01-06 22:53:33,912 iteration 2594 : loss : 0.038875, loss_ce: 0.015807
2022-01-06 22:53:35,486 iteration 2595 : loss : 0.041586, loss_ce: 0.016752
2022-01-06 22:53:36,982 iteration 2596 : loss : 0.029001, loss_ce: 0.013282
2022-01-06 22:53:38,472 iteration 2597 : loss : 0.032899, loss_ce: 0.011751
2022-01-06 22:53:39,981 iteration 2598 : loss : 0.030676, loss_ce: 0.012725
2022-01-06 22:53:41,499 iteration 2599 : loss : 0.038454, loss_ce: 0.009402
2022-01-06 22:53:43,016 iteration 2600 : loss : 0.036365, loss_ce: 0.012912
2022-01-06 22:53:44,478 iteration 2601 : loss : 0.033077, loss_ce: 0.014864
 38%|██████████▎                | 153/400 [1:12:31<1:53:03, 27.46s/it]2022-01-06 22:53:46,080 iteration 2602 : loss : 0.032717, loss_ce: 0.012656
2022-01-06 22:53:47,531 iteration 2603 : loss : 0.023476, loss_ce: 0.009812
2022-01-06 22:53:49,103 iteration 2604 : loss : 0.036651, loss_ce: 0.013896
2022-01-06 22:53:50,738 iteration 2605 : loss : 0.031246, loss_ce: 0.011329
2022-01-06 22:53:52,252 iteration 2606 : loss : 0.033739, loss_ce: 0.015051
2022-01-06 22:53:53,787 iteration 2607 : loss : 0.048262, loss_ce: 0.018079
2022-01-06 22:53:55,265 iteration 2608 : loss : 0.037728, loss_ce: 0.017316
2022-01-06 22:53:56,764 iteration 2609 : loss : 0.038571, loss_ce: 0.011328
2022-01-06 22:53:58,420 iteration 2610 : loss : 0.040195, loss_ce: 0.016110
2022-01-06 22:53:59,934 iteration 2611 : loss : 0.037671, loss_ce: 0.014007
2022-01-06 22:54:01,392 iteration 2612 : loss : 0.036899, loss_ce: 0.017952
2022-01-06 22:54:02,910 iteration 2613 : loss : 0.036147, loss_ce: 0.010999
2022-01-06 22:54:04,489 iteration 2614 : loss : 0.060224, loss_ce: 0.017623
2022-01-06 22:54:05,986 iteration 2615 : loss : 0.044908, loss_ce: 0.017484
2022-01-06 22:54:07,498 iteration 2616 : loss : 0.033093, loss_ce: 0.012608
2022-01-06 22:54:09,100 iteration 2617 : loss : 0.045955, loss_ce: 0.013711
2022-01-06 22:54:10,645 iteration 2618 : loss : 0.045063, loss_ce: 0.022226
 38%|██████████▍                | 154/400 [1:12:57<1:50:59, 27.07s/it]2022-01-06 22:54:12,267 iteration 2619 : loss : 0.038048, loss_ce: 0.012049
2022-01-06 22:54:13,754 iteration 2620 : loss : 0.037003, loss_ce: 0.014097
2022-01-06 22:54:15,303 iteration 2621 : loss : 0.046250, loss_ce: 0.016711
2022-01-06 22:54:16,845 iteration 2622 : loss : 0.054127, loss_ce: 0.027677
2022-01-06 22:54:18,387 iteration 2623 : loss : 0.062878, loss_ce: 0.035814
2022-01-06 22:54:19,854 iteration 2624 : loss : 0.041500, loss_ce: 0.014533
2022-01-06 22:54:21,347 iteration 2625 : loss : 0.032780, loss_ce: 0.012371
2022-01-06 22:54:22,916 iteration 2626 : loss : 0.047458, loss_ce: 0.020194
2022-01-06 22:54:24,446 iteration 2627 : loss : 0.049420, loss_ce: 0.012941
2022-01-06 22:54:25,915 iteration 2628 : loss : 0.037340, loss_ce: 0.013883
2022-01-06 22:54:27,479 iteration 2629 : loss : 0.045215, loss_ce: 0.016127
2022-01-06 22:54:28,933 iteration 2630 : loss : 0.041420, loss_ce: 0.014328
2022-01-06 22:54:30,457 iteration 2631 : loss : 0.033968, loss_ce: 0.015644
2022-01-06 22:54:31,990 iteration 2632 : loss : 0.049050, loss_ce: 0.022744
2022-01-06 22:54:33,473 iteration 2633 : loss : 0.034301, loss_ce: 0.016186
2022-01-06 22:54:34,997 iteration 2634 : loss : 0.033756, loss_ce: 0.011826
2022-01-06 22:54:34,998 Training Data Eval:
2022-01-06 22:54:42,812   Average segmentation loss on training set: 0.0564
2022-01-06 22:54:42,812 Validation Data Eval:
2022-01-06 22:54:45,552   Average segmentation loss on validation set: 0.0951
2022-01-06 22:54:47,147 iteration 2635 : loss : 0.044056, loss_ce: 0.018746
 39%|██████████▍                | 155/400 [1:13:33<2:02:05, 29.90s/it]2022-01-06 22:54:48,700 iteration 2636 : loss : 0.037268, loss_ce: 0.011928
2022-01-06 22:54:50,345 iteration 2637 : loss : 0.057699, loss_ce: 0.021441
2022-01-06 22:54:51,881 iteration 2638 : loss : 0.032867, loss_ce: 0.013685
2022-01-06 22:54:53,305 iteration 2639 : loss : 0.026922, loss_ce: 0.009623
2022-01-06 22:54:54,782 iteration 2640 : loss : 0.045966, loss_ce: 0.018913
2022-01-06 22:54:56,360 iteration 2641 : loss : 0.036443, loss_ce: 0.015703
2022-01-06 22:54:57,921 iteration 2642 : loss : 0.050648, loss_ce: 0.013922
2022-01-06 22:54:59,471 iteration 2643 : loss : 0.042495, loss_ce: 0.014549
2022-01-06 22:55:00,957 iteration 2644 : loss : 0.032321, loss_ce: 0.009371
2022-01-06 22:55:02,446 iteration 2645 : loss : 0.032852, loss_ce: 0.016460
2022-01-06 22:55:03,933 iteration 2646 : loss : 0.036032, loss_ce: 0.010981
2022-01-06 22:55:05,497 iteration 2647 : loss : 0.039014, loss_ce: 0.016446
2022-01-06 22:55:06,939 iteration 2648 : loss : 0.026210, loss_ce: 0.013695
2022-01-06 22:55:08,487 iteration 2649 : loss : 0.044626, loss_ce: 0.019121
2022-01-06 22:55:10,029 iteration 2650 : loss : 0.036338, loss_ce: 0.015940
2022-01-06 22:55:11,507 iteration 2651 : loss : 0.045357, loss_ce: 0.015363
2022-01-06 22:55:13,010 iteration 2652 : loss : 0.032873, loss_ce: 0.015883
 39%|██████████▌                | 156/400 [1:13:59<1:56:40, 28.69s/it]2022-01-06 22:55:14,647 iteration 2653 : loss : 0.042641, loss_ce: 0.015666
2022-01-06 22:55:16,144 iteration 2654 : loss : 0.027645, loss_ce: 0.010121
2022-01-06 22:55:17,605 iteration 2655 : loss : 0.026862, loss_ce: 0.009335
2022-01-06 22:55:19,049 iteration 2656 : loss : 0.027342, loss_ce: 0.013292
2022-01-06 22:55:20,502 iteration 2657 : loss : 0.025147, loss_ce: 0.008898
2022-01-06 22:55:21,972 iteration 2658 : loss : 0.028621, loss_ce: 0.011573
2022-01-06 22:55:23,572 iteration 2659 : loss : 0.044328, loss_ce: 0.020348
2022-01-06 22:55:25,055 iteration 2660 : loss : 0.031044, loss_ce: 0.011686
2022-01-06 22:55:26,484 iteration 2661 : loss : 0.036523, loss_ce: 0.013271
2022-01-06 22:55:28,039 iteration 2662 : loss : 0.045529, loss_ce: 0.018933
2022-01-06 22:55:29,499 iteration 2663 : loss : 0.044638, loss_ce: 0.015742
2022-01-06 22:55:31,063 iteration 2664 : loss : 0.025375, loss_ce: 0.008887
2022-01-06 22:55:32,564 iteration 2665 : loss : 0.031053, loss_ce: 0.010992
2022-01-06 22:55:34,097 iteration 2666 : loss : 0.029703, loss_ce: 0.010099
2022-01-06 22:55:35,656 iteration 2667 : loss : 0.030343, loss_ce: 0.012065
2022-01-06 22:55:37,221 iteration 2668 : loss : 0.039024, loss_ce: 0.014928
2022-01-06 22:55:38,823 iteration 2669 : loss : 0.038245, loss_ce: 0.016449
 39%|██████████▌                | 157/400 [1:14:25<1:52:42, 27.83s/it]2022-01-06 22:55:40,467 iteration 2670 : loss : 0.055851, loss_ce: 0.014962
2022-01-06 22:55:42,048 iteration 2671 : loss : 0.037152, loss_ce: 0.010222
2022-01-06 22:55:43,635 iteration 2672 : loss : 0.024345, loss_ce: 0.009139
2022-01-06 22:55:45,245 iteration 2673 : loss : 0.066680, loss_ce: 0.028902
2022-01-06 22:55:46,765 iteration 2674 : loss : 0.032946, loss_ce: 0.016163
2022-01-06 22:55:48,357 iteration 2675 : loss : 0.040633, loss_ce: 0.014421
2022-01-06 22:55:49,909 iteration 2676 : loss : 0.034313, loss_ce: 0.009371
2022-01-06 22:55:51,393 iteration 2677 : loss : 0.031689, loss_ce: 0.015197
2022-01-06 22:55:52,863 iteration 2678 : loss : 0.034345, loss_ce: 0.011181
2022-01-06 22:55:54,422 iteration 2679 : loss : 0.031806, loss_ce: 0.014416
2022-01-06 22:55:56,011 iteration 2680 : loss : 0.044953, loss_ce: 0.020102
2022-01-06 22:55:57,523 iteration 2681 : loss : 0.052209, loss_ce: 0.026682
2022-01-06 22:55:58,995 iteration 2682 : loss : 0.032283, loss_ce: 0.013945
2022-01-06 22:56:00,524 iteration 2683 : loss : 0.038071, loss_ce: 0.018642
2022-01-06 22:56:02,032 iteration 2684 : loss : 0.071676, loss_ce: 0.020575
2022-01-06 22:56:03,553 iteration 2685 : loss : 0.031399, loss_ce: 0.015004
2022-01-06 22:56:05,071 iteration 2686 : loss : 0.039655, loss_ce: 0.015073
 40%|██████████▋                | 158/400 [1:14:51<1:50:19, 27.35s/it]2022-01-06 22:56:06,637 iteration 2687 : loss : 0.023307, loss_ce: 0.010323
2022-01-06 22:56:08,162 iteration 2688 : loss : 0.039144, loss_ce: 0.014665
2022-01-06 22:56:09,611 iteration 2689 : loss : 0.031452, loss_ce: 0.011980
2022-01-06 22:56:11,141 iteration 2690 : loss : 0.031268, loss_ce: 0.013520
2022-01-06 22:56:12,696 iteration 2691 : loss : 0.027489, loss_ce: 0.008704
2022-01-06 22:56:14,224 iteration 2692 : loss : 0.045564, loss_ce: 0.016818
2022-01-06 22:56:15,727 iteration 2693 : loss : 0.030801, loss_ce: 0.010678
2022-01-06 22:56:17,190 iteration 2694 : loss : 0.030523, loss_ce: 0.010792
2022-01-06 22:56:18,782 iteration 2695 : loss : 0.057255, loss_ce: 0.021481
2022-01-06 22:56:20,437 iteration 2696 : loss : 0.048524, loss_ce: 0.018419
2022-01-06 22:56:22,048 iteration 2697 : loss : 0.039647, loss_ce: 0.020965
2022-01-06 22:56:23,554 iteration 2698 : loss : 0.027307, loss_ce: 0.010434
2022-01-06 22:56:25,071 iteration 2699 : loss : 0.032053, loss_ce: 0.011101
2022-01-06 22:56:26,585 iteration 2700 : loss : 0.037742, loss_ce: 0.011055
2022-01-06 22:56:28,141 iteration 2701 : loss : 0.052041, loss_ce: 0.021936
2022-01-06 22:56:29,723 iteration 2702 : loss : 0.052061, loss_ce: 0.012485
2022-01-06 22:56:31,305 iteration 2703 : loss : 0.039757, loss_ce: 0.013490
 40%|██████████▋                | 159/400 [1:15:18<1:48:31, 27.02s/it]2022-01-06 22:56:32,850 iteration 2704 : loss : 0.028321, loss_ce: 0.008558
2022-01-06 22:56:34,334 iteration 2705 : loss : 0.043048, loss_ce: 0.016981
2022-01-06 22:56:35,895 iteration 2706 : loss : 0.051296, loss_ce: 0.023519
2022-01-06 22:56:37,419 iteration 2707 : loss : 0.026890, loss_ce: 0.009537
2022-01-06 22:56:38,926 iteration 2708 : loss : 0.045909, loss_ce: 0.015107
2022-01-06 22:56:40,508 iteration 2709 : loss : 0.040654, loss_ce: 0.011755
2022-01-06 22:56:42,064 iteration 2710 : loss : 0.044346, loss_ce: 0.018465
2022-01-06 22:56:43,557 iteration 2711 : loss : 0.030199, loss_ce: 0.011066
2022-01-06 22:56:45,056 iteration 2712 : loss : 0.087116, loss_ce: 0.033730
2022-01-06 22:56:46,575 iteration 2713 : loss : 0.034116, loss_ce: 0.012127
2022-01-06 22:56:48,097 iteration 2714 : loss : 0.036372, loss_ce: 0.014319
2022-01-06 22:56:49,605 iteration 2715 : loss : 0.034801, loss_ce: 0.018687
2022-01-06 22:56:51,113 iteration 2716 : loss : 0.041990, loss_ce: 0.010929
2022-01-06 22:56:52,561 iteration 2717 : loss : 0.032315, loss_ce: 0.011964
2022-01-06 22:56:54,099 iteration 2718 : loss : 0.036319, loss_ce: 0.012983
2022-01-06 22:56:55,590 iteration 2719 : loss : 0.027220, loss_ce: 0.011229
2022-01-06 22:56:55,590 Training Data Eval:
2022-01-06 22:57:03,407   Average segmentation loss on training set: 0.0931
2022-01-06 22:57:03,407 Validation Data Eval:
2022-01-06 22:57:06,105   Average segmentation loss on validation set: 0.2093
2022-01-06 22:57:07,612 iteration 2720 : loss : 0.030385, loss_ce: 0.013173
 40%|██████████▊                | 160/400 [1:15:54<1:59:12, 29.80s/it]2022-01-06 22:57:09,182 iteration 2721 : loss : 0.033715, loss_ce: 0.010964
2022-01-06 22:57:10,714 iteration 2722 : loss : 0.035628, loss_ce: 0.017976
2022-01-06 22:57:12,283 iteration 2723 : loss : 0.048357, loss_ce: 0.024997
2022-01-06 22:57:13,741 iteration 2724 : loss : 0.039241, loss_ce: 0.015890
2022-01-06 22:57:15,238 iteration 2725 : loss : 0.051827, loss_ce: 0.014495
2022-01-06 22:57:16,820 iteration 2726 : loss : 0.027509, loss_ce: 0.010306
2022-01-06 22:57:18,334 iteration 2727 : loss : 0.033702, loss_ce: 0.012521
2022-01-06 22:57:19,882 iteration 2728 : loss : 0.039129, loss_ce: 0.015077
2022-01-06 22:57:21,426 iteration 2729 : loss : 0.057632, loss_ce: 0.030855
2022-01-06 22:57:23,000 iteration 2730 : loss : 0.057529, loss_ce: 0.024105
2022-01-06 22:57:24,494 iteration 2731 : loss : 0.039335, loss_ce: 0.013766
2022-01-06 22:57:25,918 iteration 2732 : loss : 0.026422, loss_ce: 0.010904
2022-01-06 22:57:27,422 iteration 2733 : loss : 0.029401, loss_ce: 0.012797
2022-01-06 22:57:28,952 iteration 2734 : loss : 0.054934, loss_ce: 0.017373
2022-01-06 22:57:30,468 iteration 2735 : loss : 0.046250, loss_ce: 0.021099
2022-01-06 22:57:31,964 iteration 2736 : loss : 0.031928, loss_ce: 0.010849
2022-01-06 22:57:33,454 iteration 2737 : loss : 0.042853, loss_ce: 0.012012
 40%|██████████▊                | 161/400 [1:16:20<1:53:58, 28.61s/it]2022-01-06 22:57:35,058 iteration 2738 : loss : 0.046303, loss_ce: 0.021438
2022-01-06 22:57:36,522 iteration 2739 : loss : 0.036293, loss_ce: 0.009138
2022-01-06 22:57:38,108 iteration 2740 : loss : 0.061474, loss_ce: 0.026722
2022-01-06 22:57:39,596 iteration 2741 : loss : 0.057430, loss_ce: 0.026727
2022-01-06 22:57:41,097 iteration 2742 : loss : 0.050363, loss_ce: 0.021990
2022-01-06 22:57:42,547 iteration 2743 : loss : 0.034618, loss_ce: 0.014121
2022-01-06 22:57:44,049 iteration 2744 : loss : 0.026252, loss_ce: 0.009132
2022-01-06 22:57:45,515 iteration 2745 : loss : 0.037386, loss_ce: 0.013130
2022-01-06 22:57:47,108 iteration 2746 : loss : 0.099232, loss_ce: 0.030538
2022-01-06 22:57:48,588 iteration 2747 : loss : 0.030618, loss_ce: 0.012055
2022-01-06 22:57:50,138 iteration 2748 : loss : 0.040721, loss_ce: 0.011453
2022-01-06 22:57:51,711 iteration 2749 : loss : 0.038901, loss_ce: 0.014360
2022-01-06 22:57:53,210 iteration 2750 : loss : 0.044051, loss_ce: 0.017108
2022-01-06 22:57:54,738 iteration 2751 : loss : 0.053005, loss_ce: 0.014305
2022-01-06 22:57:56,356 iteration 2752 : loss : 0.036088, loss_ce: 0.013358
2022-01-06 22:57:57,981 iteration 2753 : loss : 0.039451, loss_ce: 0.015989
2022-01-06 22:57:59,523 iteration 2754 : loss : 0.035058, loss_ce: 0.017572
 40%|██████████▉                | 162/400 [1:16:46<1:50:28, 27.85s/it]2022-01-06 22:58:00,987 iteration 2755 : loss : 0.026577, loss_ce: 0.009015
2022-01-06 22:58:02,539 iteration 2756 : loss : 0.036484, loss_ce: 0.014651
2022-01-06 22:58:04,085 iteration 2757 : loss : 0.031278, loss_ce: 0.012418
2022-01-06 22:58:05,621 iteration 2758 : loss : 0.029429, loss_ce: 0.011260
2022-01-06 22:58:07,152 iteration 2759 : loss : 0.037449, loss_ce: 0.015221
2022-01-06 22:58:08,687 iteration 2760 : loss : 0.040566, loss_ce: 0.019386
2022-01-06 22:58:10,247 iteration 2761 : loss : 0.043870, loss_ce: 0.016359
2022-01-06 22:58:11,768 iteration 2762 : loss : 0.026838, loss_ce: 0.010659
2022-01-06 22:58:13,324 iteration 2763 : loss : 0.052069, loss_ce: 0.017090
2022-01-06 22:58:14,796 iteration 2764 : loss : 0.023152, loss_ce: 0.008927
2022-01-06 22:58:16,421 iteration 2765 : loss : 0.026434, loss_ce: 0.008862
2022-01-06 22:58:17,885 iteration 2766 : loss : 0.042369, loss_ce: 0.014173
2022-01-06 22:58:19,429 iteration 2767 : loss : 0.063125, loss_ce: 0.028816
2022-01-06 22:58:20,938 iteration 2768 : loss : 0.038040, loss_ce: 0.011655
2022-01-06 22:58:22,401 iteration 2769 : loss : 0.026974, loss_ce: 0.012004
2022-01-06 22:58:23,889 iteration 2770 : loss : 0.038046, loss_ce: 0.016189
2022-01-06 22:58:25,408 iteration 2771 : loss : 0.031327, loss_ce: 0.014481
 41%|███████████                | 163/400 [1:17:12<1:47:40, 27.26s/it]2022-01-06 22:58:26,925 iteration 2772 : loss : 0.034652, loss_ce: 0.011724
2022-01-06 22:58:28,465 iteration 2773 : loss : 0.045624, loss_ce: 0.024745
2022-01-06 22:58:30,009 iteration 2774 : loss : 0.056395, loss_ce: 0.012959
2022-01-06 22:58:31,600 iteration 2775 : loss : 0.080116, loss_ce: 0.041421
2022-01-06 22:58:33,123 iteration 2776 : loss : 0.039651, loss_ce: 0.017624
2022-01-06 22:58:34,713 iteration 2777 : loss : 0.028376, loss_ce: 0.011502
2022-01-06 22:58:36,214 iteration 2778 : loss : 0.027899, loss_ce: 0.009211
2022-01-06 22:58:37,726 iteration 2779 : loss : 0.026407, loss_ce: 0.010592
2022-01-06 22:58:39,248 iteration 2780 : loss : 0.031111, loss_ce: 0.015525
2022-01-06 22:58:40,880 iteration 2781 : loss : 0.046882, loss_ce: 0.017755
2022-01-06 22:58:42,466 iteration 2782 : loss : 0.044099, loss_ce: 0.023863
2022-01-06 22:58:43,971 iteration 2783 : loss : 0.040453, loss_ce: 0.014145
2022-01-06 22:58:45,386 iteration 2784 : loss : 0.050079, loss_ce: 0.016048
2022-01-06 22:58:46,870 iteration 2785 : loss : 0.028133, loss_ce: 0.012745
2022-01-06 22:58:48,338 iteration 2786 : loss : 0.034409, loss_ce: 0.014670
2022-01-06 22:58:49,833 iteration 2787 : loss : 0.031053, loss_ce: 0.011561
2022-01-06 22:58:51,490 iteration 2788 : loss : 0.039644, loss_ce: 0.015654
 41%|███████████                | 164/400 [1:17:38<1:45:50, 26.91s/it]2022-01-06 22:58:53,100 iteration 2789 : loss : 0.026198, loss_ce: 0.008431
2022-01-06 22:58:54,721 iteration 2790 : loss : 0.049475, loss_ce: 0.022894
2022-01-06 22:58:56,216 iteration 2791 : loss : 0.023946, loss_ce: 0.007216
2022-01-06 22:58:57,744 iteration 2792 : loss : 0.036823, loss_ce: 0.015549
2022-01-06 22:58:59,280 iteration 2793 : loss : 0.032785, loss_ce: 0.011886
2022-01-06 22:59:00,874 iteration 2794 : loss : 0.029719, loss_ce: 0.013817
2022-01-06 22:59:02,443 iteration 2795 : loss : 0.034406, loss_ce: 0.014168
2022-01-06 22:59:03,954 iteration 2796 : loss : 0.034390, loss_ce: 0.012784
2022-01-06 22:59:05,524 iteration 2797 : loss : 0.035175, loss_ce: 0.016279
2022-01-06 22:59:07,070 iteration 2798 : loss : 0.043899, loss_ce: 0.012937
2022-01-06 22:59:08,683 iteration 2799 : loss : 0.032122, loss_ce: 0.012395
2022-01-06 22:59:10,228 iteration 2800 : loss : 0.032216, loss_ce: 0.016125
2022-01-06 22:59:11,666 iteration 2801 : loss : 0.051107, loss_ce: 0.014901
2022-01-06 22:59:13,146 iteration 2802 : loss : 0.026524, loss_ce: 0.010273
2022-01-06 22:59:14,672 iteration 2803 : loss : 0.047235, loss_ce: 0.018708
2022-01-06 22:59:16,105 iteration 2804 : loss : 0.041683, loss_ce: 0.017521
2022-01-06 22:59:16,105 Training Data Eval:
2022-01-06 22:59:23,952   Average segmentation loss on training set: 0.0277
2022-01-06 22:59:23,953 Validation Data Eval:
2022-01-06 22:59:26,653   Average segmentation loss on validation set: 0.0800
2022-01-06 22:59:28,259 iteration 2805 : loss : 0.053398, loss_ce: 0.024011
 41%|███████████▏               | 165/400 [1:18:14<1:56:59, 29.87s/it]2022-01-06 22:59:29,900 iteration 2806 : loss : 0.041839, loss_ce: 0.016778
2022-01-06 22:59:31,500 iteration 2807 : loss : 0.049005, loss_ce: 0.013961
2022-01-06 22:59:33,049 iteration 2808 : loss : 0.035153, loss_ce: 0.011494
2022-01-06 22:59:34,579 iteration 2809 : loss : 0.032308, loss_ce: 0.012723
2022-01-06 22:59:36,043 iteration 2810 : loss : 0.044368, loss_ce: 0.016129
2022-01-06 22:59:37,617 iteration 2811 : loss : 0.028316, loss_ce: 0.012665
2022-01-06 22:59:39,080 iteration 2812 : loss : 0.021261, loss_ce: 0.009077
2022-01-06 22:59:40,693 iteration 2813 : loss : 0.036545, loss_ce: 0.015773
2022-01-06 22:59:42,267 iteration 2814 : loss : 0.036195, loss_ce: 0.015413
2022-01-06 22:59:43,816 iteration 2815 : loss : 0.030707, loss_ce: 0.014673
2022-01-06 22:59:45,358 iteration 2816 : loss : 0.030100, loss_ce: 0.011213
2022-01-06 22:59:46,964 iteration 2817 : loss : 0.052572, loss_ce: 0.012448
2022-01-06 22:59:48,549 iteration 2818 : loss : 0.050984, loss_ce: 0.022349
2022-01-06 22:59:50,058 iteration 2819 : loss : 0.038946, loss_ce: 0.015765
2022-01-06 22:59:51,682 iteration 2820 : loss : 0.043783, loss_ce: 0.012920
2022-01-06 22:59:53,273 iteration 2821 : loss : 0.049235, loss_ce: 0.017167
2022-01-06 22:59:54,828 iteration 2822 : loss : 0.035933, loss_ce: 0.013266
 42%|███████████▏               | 166/400 [1:18:41<1:52:37, 28.88s/it]2022-01-06 22:59:56,506 iteration 2823 : loss : 0.050110, loss_ce: 0.022447
2022-01-06 22:59:58,002 iteration 2824 : loss : 0.032695, loss_ce: 0.015284
2022-01-06 22:59:59,564 iteration 2825 : loss : 0.060559, loss_ce: 0.026442
2022-01-06 23:00:01,222 iteration 2826 : loss : 0.051239, loss_ce: 0.020141
2022-01-06 23:00:02,904 iteration 2827 : loss : 0.047905, loss_ce: 0.018124
2022-01-06 23:00:04,489 iteration 2828 : loss : 0.038008, loss_ce: 0.012765
2022-01-06 23:00:06,056 iteration 2829 : loss : 0.034175, loss_ce: 0.016403
2022-01-06 23:00:07,601 iteration 2830 : loss : 0.046409, loss_ce: 0.017153
2022-01-06 23:00:09,076 iteration 2831 : loss : 0.029992, loss_ce: 0.014340
2022-01-06 23:00:10,756 iteration 2832 : loss : 0.043916, loss_ce: 0.018334
2022-01-06 23:00:12,300 iteration 2833 : loss : 0.033390, loss_ce: 0.012606
2022-01-06 23:00:13,916 iteration 2834 : loss : 0.062390, loss_ce: 0.028624
2022-01-06 23:00:15,502 iteration 2835 : loss : 0.043199, loss_ce: 0.014548
2022-01-06 23:00:17,086 iteration 2836 : loss : 0.040795, loss_ce: 0.018386
2022-01-06 23:00:18,690 iteration 2837 : loss : 0.036049, loss_ce: 0.013782
2022-01-06 23:00:20,161 iteration 2838 : loss : 0.026144, loss_ce: 0.011404
2022-01-06 23:00:21,727 iteration 2839 : loss : 0.031168, loss_ce: 0.011506
 42%|███████████▎               | 167/400 [1:19:08<1:49:50, 28.28s/it]2022-01-06 23:00:23,372 iteration 2840 : loss : 0.041313, loss_ce: 0.014546
2022-01-06 23:00:24,894 iteration 2841 : loss : 0.031102, loss_ce: 0.009188
2022-01-06 23:00:26,408 iteration 2842 : loss : 0.025761, loss_ce: 0.008890
2022-01-06 23:00:27,981 iteration 2843 : loss : 0.039852, loss_ce: 0.010462
2022-01-06 23:00:29,532 iteration 2844 : loss : 0.033057, loss_ce: 0.012739
2022-01-06 23:00:31,054 iteration 2845 : loss : 0.034470, loss_ce: 0.011570
2022-01-06 23:00:32,583 iteration 2846 : loss : 0.023997, loss_ce: 0.008624
2022-01-06 23:00:33,995 iteration 2847 : loss : 0.024414, loss_ce: 0.012867
2022-01-06 23:00:35,544 iteration 2848 : loss : 0.043308, loss_ce: 0.015118
2022-01-06 23:00:37,117 iteration 2849 : loss : 0.032477, loss_ce: 0.011253
2022-01-06 23:00:38,673 iteration 2850 : loss : 0.030603, loss_ce: 0.011488
2022-01-06 23:00:40,237 iteration 2851 : loss : 0.036378, loss_ce: 0.015542
2022-01-06 23:00:41,769 iteration 2852 : loss : 0.039911, loss_ce: 0.011344
2022-01-06 23:00:43,327 iteration 2853 : loss : 0.028612, loss_ce: 0.010456
2022-01-06 23:00:44,865 iteration 2854 : loss : 0.035123, loss_ce: 0.011354
2022-01-06 23:00:46,335 iteration 2855 : loss : 0.029035, loss_ce: 0.014044
2022-01-06 23:00:47,881 iteration 2856 : loss : 0.036158, loss_ce: 0.016123
 42%|███████████▎               | 168/400 [1:19:34<1:46:53, 27.64s/it]2022-01-06 23:00:49,368 iteration 2857 : loss : 0.031131, loss_ce: 0.011546
2022-01-06 23:00:50,890 iteration 2858 : loss : 0.038534, loss_ce: 0.016551
2022-01-06 23:00:52,367 iteration 2859 : loss : 0.025027, loss_ce: 0.010922
2022-01-06 23:00:53,902 iteration 2860 : loss : 0.033698, loss_ce: 0.010833
2022-01-06 23:00:55,334 iteration 2861 : loss : 0.032429, loss_ce: 0.013936
2022-01-06 23:00:56,926 iteration 2862 : loss : 0.044920, loss_ce: 0.015238
2022-01-06 23:00:58,393 iteration 2863 : loss : 0.038474, loss_ce: 0.009966
2022-01-06 23:00:59,940 iteration 2864 : loss : 0.028247, loss_ce: 0.011694
2022-01-06 23:01:01,492 iteration 2865 : loss : 0.038154, loss_ce: 0.014519
2022-01-06 23:01:02,989 iteration 2866 : loss : 0.040190, loss_ce: 0.015925
2022-01-06 23:01:04,582 iteration 2867 : loss : 0.031370, loss_ce: 0.013287
2022-01-06 23:01:06,050 iteration 2868 : loss : 0.039967, loss_ce: 0.012977
2022-01-06 23:01:07,630 iteration 2869 : loss : 0.034475, loss_ce: 0.014827
2022-01-06 23:01:09,164 iteration 2870 : loss : 0.026437, loss_ce: 0.008628
2022-01-06 23:01:10,645 iteration 2871 : loss : 0.025328, loss_ce: 0.010024
2022-01-06 23:01:12,082 iteration 2872 : loss : 0.035887, loss_ce: 0.015138
2022-01-06 23:01:13,628 iteration 2873 : loss : 0.026902, loss_ce: 0.008739
 42%|███████████▍               | 169/400 [1:20:00<1:44:14, 27.08s/it]2022-01-06 23:01:15,219 iteration 2874 : loss : 0.037133, loss_ce: 0.012139
2022-01-06 23:01:16,741 iteration 2875 : loss : 0.024986, loss_ce: 0.010519
2022-01-06 23:01:18,252 iteration 2876 : loss : 0.036929, loss_ce: 0.015114
2022-01-06 23:01:19,812 iteration 2877 : loss : 0.046111, loss_ce: 0.013144
2022-01-06 23:01:21,322 iteration 2878 : loss : 0.045451, loss_ce: 0.022443
2022-01-06 23:01:22,886 iteration 2879 : loss : 0.039691, loss_ce: 0.013373
2022-01-06 23:01:24,449 iteration 2880 : loss : 0.034016, loss_ce: 0.011884
2022-01-06 23:01:25,969 iteration 2881 : loss : 0.030082, loss_ce: 0.012142
2022-01-06 23:01:27,548 iteration 2882 : loss : 0.032906, loss_ce: 0.016765
2022-01-06 23:01:29,047 iteration 2883 : loss : 0.039728, loss_ce: 0.015971
2022-01-06 23:01:30,561 iteration 2884 : loss : 0.028464, loss_ce: 0.011820
2022-01-06 23:01:32,095 iteration 2885 : loss : 0.041931, loss_ce: 0.020330
2022-01-06 23:01:33,536 iteration 2886 : loss : 0.057377, loss_ce: 0.017451
2022-01-06 23:01:35,127 iteration 2887 : loss : 0.043638, loss_ce: 0.021554
2022-01-06 23:01:36,658 iteration 2888 : loss : 0.054065, loss_ce: 0.017297
2022-01-06 23:01:38,273 iteration 2889 : loss : 0.043187, loss_ce: 0.014886
2022-01-06 23:01:38,273 Training Data Eval:
2022-01-06 23:01:46,059   Average segmentation loss on training set: 0.0499
2022-01-06 23:01:46,059 Validation Data Eval:
2022-01-06 23:01:48,745   Average segmentation loss on validation set: 0.0941
2022-01-06 23:01:50,316 iteration 2890 : loss : 0.030224, loss_ce: 0.009508
 42%|███████████▍               | 170/400 [1:20:37<1:54:50, 29.96s/it]2022-01-06 23:01:51,925 iteration 2891 : loss : 0.032337, loss_ce: 0.013184
2022-01-06 23:01:53,511 iteration 2892 : loss : 0.050249, loss_ce: 0.013554
2022-01-06 23:01:54,991 iteration 2893 : loss : 0.027032, loss_ce: 0.008835
2022-01-06 23:01:56,579 iteration 2894 : loss : 0.045624, loss_ce: 0.012877
2022-01-06 23:01:58,146 iteration 2895 : loss : 0.049297, loss_ce: 0.024079
2022-01-06 23:01:59,630 iteration 2896 : loss : 0.020275, loss_ce: 0.006664
2022-01-06 23:02:01,142 iteration 2897 : loss : 0.034540, loss_ce: 0.015771
2022-01-06 23:02:02,608 iteration 2898 : loss : 0.029207, loss_ce: 0.012885
2022-01-06 23:02:04,141 iteration 2899 : loss : 0.039182, loss_ce: 0.012602
2022-01-06 23:02:05,627 iteration 2900 : loss : 0.030598, loss_ce: 0.011613
2022-01-06 23:02:07,144 iteration 2901 : loss : 0.025844, loss_ce: 0.007807
2022-01-06 23:02:08,601 iteration 2902 : loss : 0.041189, loss_ce: 0.015703
2022-01-06 23:02:10,085 iteration 2903 : loss : 0.028702, loss_ce: 0.013520
2022-01-06 23:02:11,559 iteration 2904 : loss : 0.053543, loss_ce: 0.024538
2022-01-06 23:02:13,008 iteration 2905 : loss : 0.040202, loss_ce: 0.024141
2022-01-06 23:02:14,544 iteration 2906 : loss : 0.038645, loss_ce: 0.019995
2022-01-06 23:02:16,015 iteration 2907 : loss : 0.032197, loss_ce: 0.011724
 43%|███████████▌               | 171/400 [1:21:02<1:49:28, 28.68s/it]2022-01-06 23:02:17,600 iteration 2908 : loss : 0.054916, loss_ce: 0.024555
2022-01-06 23:02:19,149 iteration 2909 : loss : 0.041440, loss_ce: 0.018255
2022-01-06 23:02:20,665 iteration 2910 : loss : 0.043484, loss_ce: 0.016520
2022-01-06 23:02:22,221 iteration 2911 : loss : 0.039079, loss_ce: 0.012902
2022-01-06 23:02:23,700 iteration 2912 : loss : 0.036951, loss_ce: 0.014539
2022-01-06 23:02:25,267 iteration 2913 : loss : 0.034375, loss_ce: 0.016408
2022-01-06 23:02:26,742 iteration 2914 : loss : 0.045329, loss_ce: 0.016216
2022-01-06 23:02:28,232 iteration 2915 : loss : 0.025437, loss_ce: 0.010840
2022-01-06 23:02:29,742 iteration 2916 : loss : 0.037183, loss_ce: 0.014843
2022-01-06 23:02:31,180 iteration 2917 : loss : 0.032243, loss_ce: 0.014524
2022-01-06 23:02:32,648 iteration 2918 : loss : 0.033901, loss_ce: 0.009359
2022-01-06 23:02:34,158 iteration 2919 : loss : 0.037579, loss_ce: 0.010941
2022-01-06 23:02:35,670 iteration 2920 : loss : 0.036926, loss_ce: 0.014001
2022-01-06 23:02:37,151 iteration 2921 : loss : 0.029425, loss_ce: 0.012729
2022-01-06 23:02:38,594 iteration 2922 : loss : 0.040030, loss_ce: 0.014511
2022-01-06 23:02:40,077 iteration 2923 : loss : 0.030429, loss_ce: 0.009668
2022-01-06 23:02:41,588 iteration 2924 : loss : 0.038029, loss_ce: 0.012817
 43%|███████████▌               | 172/400 [1:21:28<1:45:26, 27.75s/it]2022-01-06 23:02:43,056 iteration 2925 : loss : 0.022391, loss_ce: 0.011208
2022-01-06 23:02:44,632 iteration 2926 : loss : 0.031644, loss_ce: 0.014286
2022-01-06 23:02:46,126 iteration 2927 : loss : 0.030205, loss_ce: 0.012072
2022-01-06 23:02:47,516 iteration 2928 : loss : 0.034706, loss_ce: 0.009977
2022-01-06 23:02:49,001 iteration 2929 : loss : 0.026918, loss_ce: 0.012085
2022-01-06 23:02:50,579 iteration 2930 : loss : 0.038446, loss_ce: 0.018264
2022-01-06 23:02:52,063 iteration 2931 : loss : 0.032844, loss_ce: 0.012800
2022-01-06 23:02:53,519 iteration 2932 : loss : 0.031003, loss_ce: 0.012076
2022-01-06 23:02:55,080 iteration 2933 : loss : 0.033978, loss_ce: 0.012016
2022-01-06 23:02:56,570 iteration 2934 : loss : 0.048427, loss_ce: 0.018152
2022-01-06 23:02:58,079 iteration 2935 : loss : 0.024895, loss_ce: 0.008633
2022-01-06 23:02:59,571 iteration 2936 : loss : 0.026083, loss_ce: 0.007940
2022-01-06 23:03:01,099 iteration 2937 : loss : 0.030123, loss_ce: 0.013575
2022-01-06 23:03:02,654 iteration 2938 : loss : 0.041608, loss_ce: 0.013914
2022-01-06 23:03:04,222 iteration 2939 : loss : 0.035773, loss_ce: 0.010998
2022-01-06 23:03:05,838 iteration 2940 : loss : 0.035213, loss_ce: 0.016745
2022-01-06 23:03:07,345 iteration 2941 : loss : 0.029946, loss_ce: 0.009976
 43%|███████████▋               | 173/400 [1:21:54<1:42:43, 27.15s/it]2022-01-06 23:03:08,865 iteration 2942 : loss : 0.027567, loss_ce: 0.008619
2022-01-06 23:03:10,411 iteration 2943 : loss : 0.036334, loss_ce: 0.010327
2022-01-06 23:03:11,891 iteration 2944 : loss : 0.033328, loss_ce: 0.016461
2022-01-06 23:03:13,410 iteration 2945 : loss : 0.023718, loss_ce: 0.006350
2022-01-06 23:03:14,825 iteration 2946 : loss : 0.030474, loss_ce: 0.012660
2022-01-06 23:03:16,309 iteration 2947 : loss : 0.034563, loss_ce: 0.013937
2022-01-06 23:03:17,777 iteration 2948 : loss : 0.027787, loss_ce: 0.007933
2022-01-06 23:03:19,335 iteration 2949 : loss : 0.035329, loss_ce: 0.015390
2022-01-06 23:03:20,860 iteration 2950 : loss : 0.028596, loss_ce: 0.011771
2022-01-06 23:03:22,399 iteration 2951 : loss : 0.027787, loss_ce: 0.012552
2022-01-06 23:03:23,936 iteration 2952 : loss : 0.026722, loss_ce: 0.009675
2022-01-06 23:03:25,496 iteration 2953 : loss : 0.036130, loss_ce: 0.018554
2022-01-06 23:03:27,087 iteration 2954 : loss : 0.065556, loss_ce: 0.016718
2022-01-06 23:03:28,571 iteration 2955 : loss : 0.033421, loss_ce: 0.015554
2022-01-06 23:03:30,086 iteration 2956 : loss : 0.025039, loss_ce: 0.010979
2022-01-06 23:03:31,595 iteration 2957 : loss : 0.031619, loss_ce: 0.013102
2022-01-06 23:03:33,168 iteration 2958 : loss : 0.029171, loss_ce: 0.013457
 44%|███████████▋               | 174/400 [1:22:19<1:40:45, 26.75s/it]2022-01-06 23:03:34,806 iteration 2959 : loss : 0.024163, loss_ce: 0.008066
2022-01-06 23:03:36,357 iteration 2960 : loss : 0.035354, loss_ce: 0.015472
2022-01-06 23:03:37,935 iteration 2961 : loss : 0.037221, loss_ce: 0.013860
2022-01-06 23:03:39,648 iteration 2962 : loss : 0.028601, loss_ce: 0.009650
2022-01-06 23:03:41,243 iteration 2963 : loss : 0.043078, loss_ce: 0.014445
2022-01-06 23:03:42,731 iteration 2964 : loss : 0.027084, loss_ce: 0.008785
2022-01-06 23:03:44,306 iteration 2965 : loss : 0.025921, loss_ce: 0.012933
2022-01-06 23:03:45,906 iteration 2966 : loss : 0.032954, loss_ce: 0.012174
2022-01-06 23:03:47,464 iteration 2967 : loss : 0.033428, loss_ce: 0.012965
2022-01-06 23:03:49,039 iteration 2968 : loss : 0.047207, loss_ce: 0.017761
2022-01-06 23:03:50,577 iteration 2969 : loss : 0.028264, loss_ce: 0.012098
2022-01-06 23:03:52,161 iteration 2970 : loss : 0.039924, loss_ce: 0.014884
2022-01-06 23:03:53,705 iteration 2971 : loss : 0.024041, loss_ce: 0.008477
2022-01-06 23:03:55,307 iteration 2972 : loss : 0.028940, loss_ce: 0.012743
2022-01-06 23:03:56,825 iteration 2973 : loss : 0.025229, loss_ce: 0.006191
2022-01-06 23:03:58,440 iteration 2974 : loss : 0.038510, loss_ce: 0.010856
2022-01-06 23:03:58,440 Training Data Eval:
2022-01-06 23:04:06,496   Average segmentation loss on training set: 0.0526
2022-01-06 23:04:06,496 Validation Data Eval:
2022-01-06 23:04:09,274   Average segmentation loss on validation set: 0.1002
2022-01-06 23:04:10,806 iteration 2975 : loss : 0.023857, loss_ce: 0.009036
 44%|███████████▊               | 175/400 [1:22:57<1:52:33, 30.02s/it]2022-01-06 23:04:12,326 iteration 2976 : loss : 0.024661, loss_ce: 0.009852
2022-01-06 23:04:13,898 iteration 2977 : loss : 0.032106, loss_ce: 0.015665
2022-01-06 23:04:15,457 iteration 2978 : loss : 0.030478, loss_ce: 0.009927
2022-01-06 23:04:17,038 iteration 2979 : loss : 0.024550, loss_ce: 0.008702
2022-01-06 23:04:18,604 iteration 2980 : loss : 0.037626, loss_ce: 0.010560
2022-01-06 23:04:20,240 iteration 2981 : loss : 0.048242, loss_ce: 0.024001
2022-01-06 23:04:21,766 iteration 2982 : loss : 0.019828, loss_ce: 0.008063
2022-01-06 23:04:23,290 iteration 2983 : loss : 0.026117, loss_ce: 0.009604
2022-01-06 23:04:24,823 iteration 2984 : loss : 0.034813, loss_ce: 0.011517
2022-01-06 23:04:26,380 iteration 2985 : loss : 0.034652, loss_ce: 0.013399
2022-01-06 23:04:27,839 iteration 2986 : loss : 0.032072, loss_ce: 0.013016
2022-01-06 23:04:29,375 iteration 2987 : loss : 0.041247, loss_ce: 0.013354
2022-01-06 23:04:30,935 iteration 2988 : loss : 0.029023, loss_ce: 0.012196
2022-01-06 23:04:32,479 iteration 2989 : loss : 0.040388, loss_ce: 0.014608
2022-01-06 23:04:33,936 iteration 2990 : loss : 0.027347, loss_ce: 0.011169
2022-01-06 23:04:35,450 iteration 2991 : loss : 0.028771, loss_ce: 0.010547
2022-01-06 23:04:37,039 iteration 2992 : loss : 0.032846, loss_ce: 0.014393
 44%|███████████▉               | 176/400 [1:23:23<1:47:49, 28.88s/it]2022-01-06 23:04:38,538 iteration 2993 : loss : 0.043645, loss_ce: 0.020231
2022-01-06 23:04:40,051 iteration 2994 : loss : 0.047159, loss_ce: 0.013762
2022-01-06 23:04:41,541 iteration 2995 : loss : 0.029260, loss_ce: 0.010510
2022-01-06 23:04:43,036 iteration 2996 : loss : 0.020623, loss_ce: 0.009507
2022-01-06 23:04:44,526 iteration 2997 : loss : 0.037524, loss_ce: 0.013471
2022-01-06 23:04:46,044 iteration 2998 : loss : 0.029483, loss_ce: 0.012473
2022-01-06 23:04:47,599 iteration 2999 : loss : 0.072436, loss_ce: 0.013984
2022-01-06 23:04:49,169 iteration 3000 : loss : 0.032198, loss_ce: 0.012938
2022-01-06 23:04:50,735 iteration 3001 : loss : 0.062288, loss_ce: 0.015358
2022-01-06 23:04:52,190 iteration 3002 : loss : 0.024495, loss_ce: 0.009315
2022-01-06 23:04:53,676 iteration 3003 : loss : 0.027032, loss_ce: 0.010217
2022-01-06 23:04:55,173 iteration 3004 : loss : 0.022643, loss_ce: 0.009523
2022-01-06 23:04:56,691 iteration 3005 : loss : 0.027123, loss_ce: 0.011478
2022-01-06 23:04:58,232 iteration 3006 : loss : 0.033327, loss_ce: 0.014740
2022-01-06 23:04:59,769 iteration 3007 : loss : 0.042091, loss_ce: 0.016722
2022-01-06 23:05:01,300 iteration 3008 : loss : 0.054665, loss_ce: 0.023253
2022-01-06 23:05:02,746 iteration 3009 : loss : 0.039230, loss_ce: 0.016938
 44%|███████████▉               | 177/400 [1:23:49<1:43:48, 27.93s/it]2022-01-06 23:05:04,417 iteration 3010 : loss : 0.054581, loss_ce: 0.017521
2022-01-06 23:05:05,966 iteration 3011 : loss : 0.031922, loss_ce: 0.017324
2022-01-06 23:05:07,447 iteration 3012 : loss : 0.028837, loss_ce: 0.011880
2022-01-06 23:05:08,923 iteration 3013 : loss : 0.036934, loss_ce: 0.010899
2022-01-06 23:05:10,453 iteration 3014 : loss : 0.027012, loss_ce: 0.010145
2022-01-06 23:05:11,959 iteration 3015 : loss : 0.045552, loss_ce: 0.011654
2022-01-06 23:05:13,458 iteration 3016 : loss : 0.035759, loss_ce: 0.013548
2022-01-06 23:05:14,943 iteration 3017 : loss : 0.036693, loss_ce: 0.020582
2022-01-06 23:05:16,439 iteration 3018 : loss : 0.023843, loss_ce: 0.010321
2022-01-06 23:05:17,908 iteration 3019 : loss : 0.028945, loss_ce: 0.010463
2022-01-06 23:05:19,474 iteration 3020 : loss : 0.042772, loss_ce: 0.014511
2022-01-06 23:05:21,018 iteration 3021 : loss : 0.022303, loss_ce: 0.007686
2022-01-06 23:05:22,525 iteration 3022 : loss : 0.030456, loss_ce: 0.010973
2022-01-06 23:05:24,066 iteration 3023 : loss : 0.048583, loss_ce: 0.017653
2022-01-06 23:05:25,672 iteration 3024 : loss : 0.052960, loss_ce: 0.028031
2022-01-06 23:05:27,254 iteration 3025 : loss : 0.044327, loss_ce: 0.018370
2022-01-06 23:05:28,737 iteration 3026 : loss : 0.025523, loss_ce: 0.010349
 44%|████████████               | 178/400 [1:24:15<1:41:11, 27.35s/it]2022-01-06 23:05:30,370 iteration 3027 : loss : 0.040205, loss_ce: 0.016295
2022-01-06 23:05:31,867 iteration 3028 : loss : 0.028287, loss_ce: 0.012326
2022-01-06 23:05:33,415 iteration 3029 : loss : 0.051541, loss_ce: 0.020390
2022-01-06 23:05:34,926 iteration 3030 : loss : 0.027225, loss_ce: 0.012838
2022-01-06 23:05:36,477 iteration 3031 : loss : 0.037060, loss_ce: 0.015585
2022-01-06 23:05:37,989 iteration 3032 : loss : 0.033476, loss_ce: 0.011404
2022-01-06 23:05:39,567 iteration 3033 : loss : 0.040997, loss_ce: 0.012930
2022-01-06 23:05:41,148 iteration 3034 : loss : 0.040918, loss_ce: 0.014192
2022-01-06 23:05:42,705 iteration 3035 : loss : 0.032979, loss_ce: 0.015633
2022-01-06 23:05:44,223 iteration 3036 : loss : 0.039181, loss_ce: 0.021420
2022-01-06 23:05:45,753 iteration 3037 : loss : 0.039268, loss_ce: 0.015468
2022-01-06 23:05:47,312 iteration 3038 : loss : 0.031563, loss_ce: 0.011080
2022-01-06 23:05:48,781 iteration 3039 : loss : 0.037512, loss_ce: 0.011949
2022-01-06 23:05:50,306 iteration 3040 : loss : 0.049726, loss_ce: 0.013491
2022-01-06 23:05:51,870 iteration 3041 : loss : 0.033213, loss_ce: 0.013604
2022-01-06 23:05:53,385 iteration 3042 : loss : 0.038730, loss_ce: 0.018787
2022-01-06 23:05:55,073 iteration 3043 : loss : 0.044813, loss_ce: 0.012323
 45%|████████████               | 179/400 [1:24:41<1:39:37, 27.05s/it]2022-01-06 23:05:56,657 iteration 3044 : loss : 0.029239, loss_ce: 0.011333
2022-01-06 23:05:58,122 iteration 3045 : loss : 0.026836, loss_ce: 0.009876
2022-01-06 23:05:59,608 iteration 3046 : loss : 0.028570, loss_ce: 0.008507
2022-01-06 23:06:01,218 iteration 3047 : loss : 0.045345, loss_ce: 0.024196
2022-01-06 23:06:02,694 iteration 3048 : loss : 0.031394, loss_ce: 0.010788
2022-01-06 23:06:04,204 iteration 3049 : loss : 0.042423, loss_ce: 0.017177
2022-01-06 23:06:05,737 iteration 3050 : loss : 0.026381, loss_ce: 0.012317
2022-01-06 23:06:07,250 iteration 3051 : loss : 0.026725, loss_ce: 0.007814
2022-01-06 23:06:08,754 iteration 3052 : loss : 0.026277, loss_ce: 0.009517
2022-01-06 23:06:10,338 iteration 3053 : loss : 0.026531, loss_ce: 0.011481
2022-01-06 23:06:11,793 iteration 3054 : loss : 0.033102, loss_ce: 0.010737
2022-01-06 23:06:13,318 iteration 3055 : loss : 0.051467, loss_ce: 0.020881
2022-01-06 23:06:14,804 iteration 3056 : loss : 0.024098, loss_ce: 0.010137
2022-01-06 23:06:16,259 iteration 3057 : loss : 0.025782, loss_ce: 0.010024
2022-01-06 23:06:17,786 iteration 3058 : loss : 0.027398, loss_ce: 0.011660
2022-01-06 23:06:19,311 iteration 3059 : loss : 0.029245, loss_ce: 0.008849
2022-01-06 23:06:19,311 Training Data Eval:
2022-01-06 23:06:27,044   Average segmentation loss on training set: 0.0380
2022-01-06 23:06:27,045 Validation Data Eval:
2022-01-06 23:06:29,720   Average segmentation loss on validation set: 0.0887
2022-01-06 23:06:31,193 iteration 3060 : loss : 0.027179, loss_ce: 0.012834
 45%|████████████▏              | 180/400 [1:25:17<1:49:09, 29.77s/it]2022-01-06 23:06:32,689 iteration 3061 : loss : 0.031425, loss_ce: 0.012858
2022-01-06 23:06:34,205 iteration 3062 : loss : 0.047337, loss_ce: 0.017896
2022-01-06 23:06:35,757 iteration 3063 : loss : 0.031059, loss_ce: 0.011962
2022-01-06 23:06:37,302 iteration 3064 : loss : 0.039029, loss_ce: 0.017823
2022-01-06 23:06:38,724 iteration 3065 : loss : 0.023400, loss_ce: 0.008317
2022-01-06 23:06:40,201 iteration 3066 : loss : 0.030435, loss_ce: 0.013590
2022-01-06 23:06:41,724 iteration 3067 : loss : 0.043453, loss_ce: 0.017408
2022-01-06 23:06:43,243 iteration 3068 : loss : 0.059066, loss_ce: 0.017523
2022-01-06 23:06:44,741 iteration 3069 : loss : 0.044305, loss_ce: 0.015941
2022-01-06 23:06:46,252 iteration 3070 : loss : 0.029069, loss_ce: 0.012305
2022-01-06 23:06:47,710 iteration 3071 : loss : 0.027722, loss_ce: 0.011045
2022-01-06 23:06:49,162 iteration 3072 : loss : 0.048139, loss_ce: 0.017428
2022-01-06 23:06:50,648 iteration 3073 : loss : 0.036177, loss_ce: 0.015649
2022-01-06 23:06:52,210 iteration 3074 : loss : 0.027799, loss_ce: 0.012089
2022-01-06 23:06:53,737 iteration 3075 : loss : 0.029166, loss_ce: 0.010260
2022-01-06 23:06:55,306 iteration 3076 : loss : 0.050183, loss_ce: 0.025680
2022-01-06 23:06:56,776 iteration 3077 : loss : 0.036666, loss_ce: 0.013375
 45%|████████████▏              | 181/400 [1:25:43<1:44:03, 28.51s/it]2022-01-06 23:06:58,283 iteration 3078 : loss : 0.029045, loss_ce: 0.012105
2022-01-06 23:06:59,890 iteration 3079 : loss : 0.031510, loss_ce: 0.010584
2022-01-06 23:07:01,329 iteration 3080 : loss : 0.033782, loss_ce: 0.009884
2022-01-06 23:07:02,864 iteration 3081 : loss : 0.045428, loss_ce: 0.014936
2022-01-06 23:07:04,354 iteration 3082 : loss : 0.041319, loss_ce: 0.011162
2022-01-06 23:07:05,908 iteration 3083 : loss : 0.034599, loss_ce: 0.015183
2022-01-06 23:07:07,307 iteration 3084 : loss : 0.027337, loss_ce: 0.012070
2022-01-06 23:07:08,836 iteration 3085 : loss : 0.037347, loss_ce: 0.017552
2022-01-06 23:07:10,430 iteration 3086 : loss : 0.033589, loss_ce: 0.017534
2022-01-06 23:07:11,908 iteration 3087 : loss : 0.042442, loss_ce: 0.015336
2022-01-06 23:07:13,413 iteration 3088 : loss : 0.025861, loss_ce: 0.007502
2022-01-06 23:07:14,994 iteration 3089 : loss : 0.043326, loss_ce: 0.012495
2022-01-06 23:07:16,429 iteration 3090 : loss : 0.030884, loss_ce: 0.014992
2022-01-06 23:07:17,925 iteration 3091 : loss : 0.034077, loss_ce: 0.015837
2022-01-06 23:07:19,552 iteration 3092 : loss : 0.042214, loss_ce: 0.015615
2022-01-06 23:07:21,017 iteration 3093 : loss : 0.036112, loss_ce: 0.017176
2022-01-06 23:07:22,532 iteration 3094 : loss : 0.032652, loss_ce: 0.015659
 46%|████████████▎              | 182/400 [1:26:09<1:40:35, 27.69s/it]2022-01-06 23:07:24,091 iteration 3095 : loss : 0.035381, loss_ce: 0.015145
2022-01-06 23:07:25,606 iteration 3096 : loss : 0.025639, loss_ce: 0.012846
2022-01-06 23:07:27,097 iteration 3097 : loss : 0.032097, loss_ce: 0.012522
2022-01-06 23:07:28,565 iteration 3098 : loss : 0.026017, loss_ce: 0.010633
2022-01-06 23:07:30,074 iteration 3099 : loss : 0.023797, loss_ce: 0.008010
2022-01-06 23:07:31,530 iteration 3100 : loss : 0.027145, loss_ce: 0.008578
2022-01-06 23:07:33,090 iteration 3101 : loss : 0.028123, loss_ce: 0.010650
2022-01-06 23:07:34,609 iteration 3102 : loss : 0.033642, loss_ce: 0.012494
2022-01-06 23:07:36,128 iteration 3103 : loss : 0.040299, loss_ce: 0.016279
2022-01-06 23:07:37,592 iteration 3104 : loss : 0.027095, loss_ce: 0.011889
2022-01-06 23:07:39,043 iteration 3105 : loss : 0.026492, loss_ce: 0.012503
2022-01-06 23:07:40,533 iteration 3106 : loss : 0.033353, loss_ce: 0.015833
2022-01-06 23:07:42,022 iteration 3107 : loss : 0.021148, loss_ce: 0.006543
2022-01-06 23:07:43,543 iteration 3108 : loss : 0.042060, loss_ce: 0.015653
2022-01-06 23:07:45,074 iteration 3109 : loss : 0.033591, loss_ce: 0.011534
2022-01-06 23:07:46,606 iteration 3110 : loss : 0.040043, loss_ce: 0.018214
2022-01-06 23:07:48,120 iteration 3111 : loss : 0.035357, loss_ce: 0.015096
 46%|████████████▎              | 183/400 [1:26:34<1:37:50, 27.06s/it]2022-01-06 23:07:49,690 iteration 3112 : loss : 0.029544, loss_ce: 0.011473
2022-01-06 23:07:51,153 iteration 3113 : loss : 0.026247, loss_ce: 0.010387
2022-01-06 23:07:52,685 iteration 3114 : loss : 0.035936, loss_ce: 0.013060
2022-01-06 23:07:54,183 iteration 3115 : loss : 0.029766, loss_ce: 0.013544
2022-01-06 23:07:55,678 iteration 3116 : loss : 0.026802, loss_ce: 0.011945
2022-01-06 23:07:57,240 iteration 3117 : loss : 0.024564, loss_ce: 0.008920
2022-01-06 23:07:58,718 iteration 3118 : loss : 0.028599, loss_ce: 0.011773
2022-01-06 23:08:00,311 iteration 3119 : loss : 0.033034, loss_ce: 0.011260
2022-01-06 23:08:01,770 iteration 3120 : loss : 0.023597, loss_ce: 0.009558
2022-01-06 23:08:03,338 iteration 3121 : loss : 0.036928, loss_ce: 0.016321
2022-01-06 23:08:04,822 iteration 3122 : loss : 0.036264, loss_ce: 0.009939
2022-01-06 23:08:06,394 iteration 3123 : loss : 0.031966, loss_ce: 0.014771
2022-01-06 23:08:07,866 iteration 3124 : loss : 0.021935, loss_ce: 0.007991
2022-01-06 23:08:09,416 iteration 3125 : loss : 0.033723, loss_ce: 0.010063
2022-01-06 23:08:11,029 iteration 3126 : loss : 0.024365, loss_ce: 0.011300
2022-01-06 23:08:12,648 iteration 3127 : loss : 0.022418, loss_ce: 0.007108
2022-01-06 23:08:14,254 iteration 3128 : loss : 0.035518, loss_ce: 0.012087
 46%|████████████▍              | 184/400 [1:27:00<1:36:24, 26.78s/it]2022-01-06 23:08:15,839 iteration 3129 : loss : 0.031988, loss_ce: 0.011226
2022-01-06 23:08:17,479 iteration 3130 : loss : 0.038343, loss_ce: 0.014528
2022-01-06 23:08:19,011 iteration 3131 : loss : 0.032299, loss_ce: 0.011782
2022-01-06 23:08:20,556 iteration 3132 : loss : 0.028565, loss_ce: 0.008547
2022-01-06 23:08:22,095 iteration 3133 : loss : 0.041374, loss_ce: 0.015581
2022-01-06 23:08:23,604 iteration 3134 : loss : 0.023283, loss_ce: 0.012391
2022-01-06 23:08:25,135 iteration 3135 : loss : 0.028501, loss_ce: 0.012776
2022-01-06 23:08:26,647 iteration 3136 : loss : 0.036742, loss_ce: 0.015658
2022-01-06 23:08:28,283 iteration 3137 : loss : 0.024992, loss_ce: 0.011170
2022-01-06 23:08:29,858 iteration 3138 : loss : 0.065276, loss_ce: 0.028200
2022-01-06 23:08:31,318 iteration 3139 : loss : 0.028337, loss_ce: 0.010213
2022-01-06 23:08:32,783 iteration 3140 : loss : 0.034533, loss_ce: 0.015847
2022-01-06 23:08:34,313 iteration 3141 : loss : 0.036023, loss_ce: 0.017162
2022-01-06 23:08:35,809 iteration 3142 : loss : 0.051901, loss_ce: 0.020603
2022-01-06 23:08:37,364 iteration 3143 : loss : 0.051288, loss_ce: 0.019155
2022-01-06 23:08:38,867 iteration 3144 : loss : 0.026448, loss_ce: 0.009615
2022-01-06 23:08:38,867 Training Data Eval:
2022-01-06 23:08:46,640   Average segmentation loss on training set: 0.0273
2022-01-06 23:08:46,640 Validation Data Eval:
2022-01-06 23:08:49,323   Average segmentation loss on validation set: 0.1604
2022-01-06 23:08:50,796 iteration 3145 : loss : 0.023642, loss_ce: 0.008852
 46%|████████████▍              | 185/400 [1:27:37<1:46:27, 29.71s/it]2022-01-06 23:08:52,506 iteration 3146 : loss : 0.035388, loss_ce: 0.010764
2022-01-06 23:08:54,049 iteration 3147 : loss : 0.041423, loss_ce: 0.016154
2022-01-06 23:08:55,496 iteration 3148 : loss : 0.035050, loss_ce: 0.015681
2022-01-06 23:08:57,037 iteration 3149 : loss : 0.036984, loss_ce: 0.012537
2022-01-06 23:08:58,587 iteration 3150 : loss : 0.036075, loss_ce: 0.010854
2022-01-06 23:09:00,131 iteration 3151 : loss : 0.041427, loss_ce: 0.017106
2022-01-06 23:09:01,669 iteration 3152 : loss : 0.043722, loss_ce: 0.014776
2022-01-06 23:09:03,185 iteration 3153 : loss : 0.038747, loss_ce: 0.019701
2022-01-06 23:09:04,765 iteration 3154 : loss : 0.043913, loss_ce: 0.016897
2022-01-06 23:09:06,303 iteration 3155 : loss : 0.021369, loss_ce: 0.007921
2022-01-06 23:09:07,752 iteration 3156 : loss : 0.028811, loss_ce: 0.010969
2022-01-06 23:09:09,285 iteration 3157 : loss : 0.028699, loss_ce: 0.011023
2022-01-06 23:09:10,763 iteration 3158 : loss : 0.054737, loss_ce: 0.016631
2022-01-06 23:09:12,312 iteration 3159 : loss : 0.037983, loss_ce: 0.017462
2022-01-06 23:09:13,874 iteration 3160 : loss : 0.030873, loss_ce: 0.012421
2022-01-06 23:09:15,490 iteration 3161 : loss : 0.031045, loss_ce: 0.012944
2022-01-06 23:09:17,045 iteration 3162 : loss : 0.045761, loss_ce: 0.021944
 46%|████████████▌              | 186/400 [1:28:03<1:42:15, 28.67s/it]2022-01-06 23:09:18,601 iteration 3163 : loss : 0.033183, loss_ce: 0.011307
2022-01-06 23:09:20,233 iteration 3164 : loss : 0.032557, loss_ce: 0.013375
2022-01-06 23:09:21,819 iteration 3165 : loss : 0.031391, loss_ce: 0.011133
2022-01-06 23:09:23,322 iteration 3166 : loss : 0.033841, loss_ce: 0.015352
2022-01-06 23:09:24,890 iteration 3167 : loss : 0.030499, loss_ce: 0.011004
2022-01-06 23:09:26,425 iteration 3168 : loss : 0.035120, loss_ce: 0.014396
2022-01-06 23:09:27,988 iteration 3169 : loss : 0.036935, loss_ce: 0.015037
2022-01-06 23:09:29,523 iteration 3170 : loss : 0.044474, loss_ce: 0.021729
2022-01-06 23:09:30,943 iteration 3171 : loss : 0.026247, loss_ce: 0.008447
2022-01-06 23:09:32,441 iteration 3172 : loss : 0.031149, loss_ce: 0.013889
2022-01-06 23:09:33,975 iteration 3173 : loss : 0.049326, loss_ce: 0.017289
2022-01-06 23:09:35,479 iteration 3174 : loss : 0.030027, loss_ce: 0.012703
2022-01-06 23:09:36,953 iteration 3175 : loss : 0.086458, loss_ce: 0.041726
2022-01-06 23:09:38,544 iteration 3176 : loss : 0.065735, loss_ce: 0.023196
2022-01-06 23:09:40,072 iteration 3177 : loss : 0.028253, loss_ce: 0.010639
2022-01-06 23:09:41,687 iteration 3178 : loss : 0.034035, loss_ce: 0.009449
2022-01-06 23:09:43,214 iteration 3179 : loss : 0.040151, loss_ce: 0.020656
 47%|████████████▌              | 187/400 [1:28:29<1:39:07, 27.92s/it]2022-01-06 23:09:44,841 iteration 3180 : loss : 0.098880, loss_ce: 0.057876
2022-01-06 23:09:46,318 iteration 3181 : loss : 0.032895, loss_ce: 0.014015
2022-01-06 23:09:47,779 iteration 3182 : loss : 0.027647, loss_ce: 0.011699
2022-01-06 23:09:49,345 iteration 3183 : loss : 0.052020, loss_ce: 0.018344
2022-01-06 23:09:50,857 iteration 3184 : loss : 0.032868, loss_ce: 0.012459
2022-01-06 23:09:52,374 iteration 3185 : loss : 0.046934, loss_ce: 0.017364
2022-01-06 23:09:53,927 iteration 3186 : loss : 0.046998, loss_ce: 0.024359
2022-01-06 23:09:55,481 iteration 3187 : loss : 0.029906, loss_ce: 0.013168
2022-01-06 23:09:57,030 iteration 3188 : loss : 0.044687, loss_ce: 0.018481
2022-01-06 23:09:58,531 iteration 3189 : loss : 0.040058, loss_ce: 0.015246
2022-01-06 23:10:00,137 iteration 3190 : loss : 0.038963, loss_ce: 0.015989
2022-01-06 23:10:01,690 iteration 3191 : loss : 0.036062, loss_ce: 0.014869
2022-01-06 23:10:03,141 iteration 3192 : loss : 0.047811, loss_ce: 0.010379
2022-01-06 23:10:04,668 iteration 3193 : loss : 0.030904, loss_ce: 0.012627
2022-01-06 23:10:06,293 iteration 3194 : loss : 0.047082, loss_ce: 0.019601
2022-01-06 23:10:07,812 iteration 3195 : loss : 0.035881, loss_ce: 0.017052
2022-01-06 23:10:09,323 iteration 3196 : loss : 0.036422, loss_ce: 0.018380
 47%|████████████▋              | 188/400 [1:28:56<1:36:43, 27.38s/it]2022-01-06 23:10:10,879 iteration 3197 : loss : 0.029216, loss_ce: 0.011432
2022-01-06 23:10:12,424 iteration 3198 : loss : 0.051622, loss_ce: 0.026601
2022-01-06 23:10:13,952 iteration 3199 : loss : 0.033700, loss_ce: 0.010675
2022-01-06 23:10:15,495 iteration 3200 : loss : 0.043657, loss_ce: 0.016106
2022-01-06 23:10:17,037 iteration 3201 : loss : 0.030867, loss_ce: 0.011081
2022-01-06 23:10:18,582 iteration 3202 : loss : 0.037049, loss_ce: 0.010647
2022-01-06 23:10:20,014 iteration 3203 : loss : 0.031424, loss_ce: 0.015590
2022-01-06 23:10:21,438 iteration 3204 : loss : 0.024923, loss_ce: 0.010919
2022-01-06 23:10:22,875 iteration 3205 : loss : 0.038079, loss_ce: 0.014907
2022-01-06 23:10:24,281 iteration 3206 : loss : 0.025923, loss_ce: 0.008747
2022-01-06 23:10:25,816 iteration 3207 : loss : 0.033799, loss_ce: 0.012959
2022-01-06 23:10:27,312 iteration 3208 : loss : 0.027412, loss_ce: 0.010971
2022-01-06 23:10:28,782 iteration 3209 : loss : 0.030521, loss_ce: 0.010911
2022-01-06 23:10:30,315 iteration 3210 : loss : 0.029262, loss_ce: 0.013941
2022-01-06 23:10:31,769 iteration 3211 : loss : 0.033473, loss_ce: 0.014430
2022-01-06 23:10:33,341 iteration 3212 : loss : 0.039717, loss_ce: 0.016697
2022-01-06 23:10:34,911 iteration 3213 : loss : 0.040619, loss_ce: 0.014399
 47%|████████████▊              | 189/400 [1:29:21<1:34:23, 26.84s/it]2022-01-06 23:10:36,419 iteration 3214 : loss : 0.021390, loss_ce: 0.009274
2022-01-06 23:10:37,921 iteration 3215 : loss : 0.026797, loss_ce: 0.011672
2022-01-06 23:10:39,430 iteration 3216 : loss : 0.044570, loss_ce: 0.019360
2022-01-06 23:10:40,974 iteration 3217 : loss : 0.038342, loss_ce: 0.018125
2022-01-06 23:10:42,481 iteration 3218 : loss : 0.060449, loss_ce: 0.022747
2022-01-06 23:10:43,967 iteration 3219 : loss : 0.049141, loss_ce: 0.018551
2022-01-06 23:10:45,424 iteration 3220 : loss : 0.022188, loss_ce: 0.009937
2022-01-06 23:10:47,008 iteration 3221 : loss : 0.047161, loss_ce: 0.020946
2022-01-06 23:10:48,487 iteration 3222 : loss : 0.026755, loss_ce: 0.007495
2022-01-06 23:10:50,015 iteration 3223 : loss : 0.034811, loss_ce: 0.012296
2022-01-06 23:10:51,596 iteration 3224 : loss : 0.049903, loss_ce: 0.020180
2022-01-06 23:10:53,095 iteration 3225 : loss : 0.046998, loss_ce: 0.023217
2022-01-06 23:10:54,602 iteration 3226 : loss : 0.030268, loss_ce: 0.012087
2022-01-06 23:10:56,128 iteration 3227 : loss : 0.034252, loss_ce: 0.013290
2022-01-06 23:10:57,611 iteration 3228 : loss : 0.029892, loss_ce: 0.009495
2022-01-06 23:10:59,062 iteration 3229 : loss : 0.035243, loss_ce: 0.009839
2022-01-06 23:10:59,062 Training Data Eval:
2022-01-06 23:11:06,828   Average segmentation loss on training set: 0.0805
2022-01-06 23:11:06,828 Validation Data Eval:
2022-01-06 23:11:09,513   Average segmentation loss on validation set: 0.1950
2022-01-06 23:11:10,976 iteration 3230 : loss : 0.032420, loss_ce: 0.012934
 48%|████████████▊              | 190/400 [1:29:57<1:43:37, 29.61s/it]2022-01-06 23:11:12,502 iteration 3231 : loss : 0.059339, loss_ce: 0.011599
2022-01-06 23:11:13,990 iteration 3232 : loss : 0.036153, loss_ce: 0.013765
2022-01-06 23:11:15,499 iteration 3233 : loss : 0.042504, loss_ce: 0.019884
2022-01-06 23:11:17,034 iteration 3234 : loss : 0.027045, loss_ce: 0.009714
2022-01-06 23:11:18,554 iteration 3235 : loss : 0.070197, loss_ce: 0.022253
2022-01-06 23:11:20,001 iteration 3236 : loss : 0.022128, loss_ce: 0.010485
2022-01-06 23:11:21,593 iteration 3237 : loss : 0.048909, loss_ce: 0.020297
2022-01-06 23:11:23,097 iteration 3238 : loss : 0.028623, loss_ce: 0.013514
2022-01-06 23:11:24,608 iteration 3239 : loss : 0.035163, loss_ce: 0.015063
2022-01-06 23:11:26,071 iteration 3240 : loss : 0.064986, loss_ce: 0.020340
2022-01-06 23:11:27,513 iteration 3241 : loss : 0.040642, loss_ce: 0.010208
2022-01-06 23:11:29,025 iteration 3242 : loss : 0.035413, loss_ce: 0.014618
2022-01-06 23:11:30,467 iteration 3243 : loss : 0.030648, loss_ce: 0.011380
2022-01-06 23:11:31,986 iteration 3244 : loss : 0.040180, loss_ce: 0.016067
2022-01-06 23:11:33,450 iteration 3245 : loss : 0.027429, loss_ce: 0.012452
2022-01-06 23:11:35,008 iteration 3246 : loss : 0.044291, loss_ce: 0.024324
2022-01-06 23:11:36,469 iteration 3247 : loss : 0.048902, loss_ce: 0.020435
 48%|████████████▉              | 191/400 [1:30:23<1:38:50, 28.37s/it]2022-01-06 23:11:37,981 iteration 3248 : loss : 0.022318, loss_ce: 0.008024
2022-01-06 23:11:39,448 iteration 3249 : loss : 0.033550, loss_ce: 0.014612
2022-01-06 23:11:40,951 iteration 3250 : loss : 0.034647, loss_ce: 0.011783
2022-01-06 23:11:42,456 iteration 3251 : loss : 0.032790, loss_ce: 0.013097
2022-01-06 23:11:43,983 iteration 3252 : loss : 0.032883, loss_ce: 0.012135
2022-01-06 23:11:45,483 iteration 3253 : loss : 0.035910, loss_ce: 0.016446
2022-01-06 23:11:46,984 iteration 3254 : loss : 0.033850, loss_ce: 0.012428
2022-01-06 23:11:48,425 iteration 3255 : loss : 0.035761, loss_ce: 0.014084
2022-01-06 23:11:49,936 iteration 3256 : loss : 0.023042, loss_ce: 0.008592
2022-01-06 23:11:51,405 iteration 3257 : loss : 0.025567, loss_ce: 0.010850
2022-01-06 23:11:52,929 iteration 3258 : loss : 0.038296, loss_ce: 0.014923
2022-01-06 23:11:54,390 iteration 3259 : loss : 0.035967, loss_ce: 0.013112
2022-01-06 23:11:55,909 iteration 3260 : loss : 0.027259, loss_ce: 0.011571
2022-01-06 23:11:57,390 iteration 3261 : loss : 0.056026, loss_ce: 0.014176
2022-01-06 23:11:58,950 iteration 3262 : loss : 0.057183, loss_ce: 0.021949
2022-01-06 23:12:00,575 iteration 3263 : loss : 0.031543, loss_ce: 0.008826
2022-01-06 23:12:02,105 iteration 3264 : loss : 0.045033, loss_ce: 0.018565
 48%|████████████▉              | 192/400 [1:30:48<1:35:31, 27.55s/it]2022-01-06 23:12:03,637 iteration 3265 : loss : 0.029835, loss_ce: 0.013379
2022-01-06 23:12:05,202 iteration 3266 : loss : 0.030759, loss_ce: 0.012471
2022-01-06 23:12:06,738 iteration 3267 : loss : 0.034270, loss_ce: 0.013583
2022-01-06 23:12:08,211 iteration 3268 : loss : 0.028105, loss_ce: 0.012010
2022-01-06 23:12:09,764 iteration 3269 : loss : 0.034310, loss_ce: 0.014754
2022-01-06 23:12:11,344 iteration 3270 : loss : 0.044041, loss_ce: 0.012461
2022-01-06 23:12:12,907 iteration 3271 : loss : 0.035678, loss_ce: 0.012510
2022-01-06 23:12:14,475 iteration 3272 : loss : 0.032872, loss_ce: 0.014071
2022-01-06 23:12:15,950 iteration 3273 : loss : 0.032571, loss_ce: 0.010880
2022-01-06 23:12:17,501 iteration 3274 : loss : 0.027886, loss_ce: 0.013657
2022-01-06 23:12:19,092 iteration 3275 : loss : 0.051119, loss_ce: 0.013526
2022-01-06 23:12:20,602 iteration 3276 : loss : 0.037182, loss_ce: 0.016076
2022-01-06 23:12:22,143 iteration 3277 : loss : 0.041004, loss_ce: 0.020765
2022-01-06 23:12:23,660 iteration 3278 : loss : 0.029845, loss_ce: 0.010495
2022-01-06 23:12:25,141 iteration 3279 : loss : 0.029719, loss_ce: 0.011170
2022-01-06 23:12:26,646 iteration 3280 : loss : 0.026457, loss_ce: 0.009032
2022-01-06 23:12:28,104 iteration 3281 : loss : 0.023914, loss_ce: 0.007904
 48%|█████████████              | 193/400 [1:31:14<1:33:26, 27.08s/it]2022-01-06 23:12:29,697 iteration 3282 : loss : 0.027574, loss_ce: 0.010692
2022-01-06 23:12:31,193 iteration 3283 : loss : 0.028925, loss_ce: 0.008864
2022-01-06 23:12:32,692 iteration 3284 : loss : 0.038058, loss_ce: 0.015093
2022-01-06 23:12:34,192 iteration 3285 : loss : 0.034841, loss_ce: 0.013188
2022-01-06 23:12:35,694 iteration 3286 : loss : 0.022092, loss_ce: 0.008582
2022-01-06 23:12:37,211 iteration 3287 : loss : 0.047303, loss_ce: 0.017650
2022-01-06 23:12:38,662 iteration 3288 : loss : 0.031607, loss_ce: 0.017950
2022-01-06 23:12:40,129 iteration 3289 : loss : 0.044980, loss_ce: 0.013702
2022-01-06 23:12:41,664 iteration 3290 : loss : 0.036205, loss_ce: 0.011285
2022-01-06 23:12:43,211 iteration 3291 : loss : 0.031543, loss_ce: 0.012937
2022-01-06 23:12:44,728 iteration 3292 : loss : 0.027646, loss_ce: 0.009604
2022-01-06 23:12:46,222 iteration 3293 : loss : 0.044300, loss_ce: 0.022308
2022-01-06 23:12:47,699 iteration 3294 : loss : 0.034553, loss_ce: 0.015607
2022-01-06 23:12:49,293 iteration 3295 : loss : 0.046902, loss_ce: 0.018424
2022-01-06 23:12:50,829 iteration 3296 : loss : 0.030227, loss_ce: 0.008910
2022-01-06 23:12:52,288 iteration 3297 : loss : 0.019930, loss_ce: 0.008751
2022-01-06 23:12:53,852 iteration 3298 : loss : 0.034543, loss_ce: 0.014327
 48%|█████████████              | 194/400 [1:31:40<1:31:36, 26.68s/it]2022-01-06 23:12:55,367 iteration 3299 : loss : 0.026588, loss_ce: 0.010845
2022-01-06 23:12:56,856 iteration 3300 : loss : 0.053270, loss_ce: 0.023919
2022-01-06 23:12:58,433 iteration 3301 : loss : 0.025412, loss_ce: 0.010962
2022-01-06 23:12:59,954 iteration 3302 : loss : 0.025405, loss_ce: 0.010467
2022-01-06 23:13:01,456 iteration 3303 : loss : 0.025728, loss_ce: 0.010777
2022-01-06 23:13:02,974 iteration 3304 : loss : 0.026611, loss_ce: 0.010013
2022-01-06 23:13:04,508 iteration 3305 : loss : 0.030629, loss_ce: 0.011787
2022-01-06 23:13:06,056 iteration 3306 : loss : 0.033017, loss_ce: 0.013892
2022-01-06 23:13:07,544 iteration 3307 : loss : 0.033409, loss_ce: 0.011833
2022-01-06 23:13:09,070 iteration 3308 : loss : 0.036379, loss_ce: 0.015901
2022-01-06 23:13:10,571 iteration 3309 : loss : 0.061709, loss_ce: 0.021305
2022-01-06 23:13:12,144 iteration 3310 : loss : 0.061607, loss_ce: 0.024809
2022-01-06 23:13:13,738 iteration 3311 : loss : 0.030732, loss_ce: 0.011402
2022-01-06 23:13:15,179 iteration 3312 : loss : 0.031049, loss_ce: 0.012906
2022-01-06 23:13:16,647 iteration 3313 : loss : 0.026068, loss_ce: 0.008741
2022-01-06 23:13:18,219 iteration 3314 : loss : 0.034435, loss_ce: 0.011493
2022-01-06 23:13:18,220 Training Data Eval:
2022-01-06 23:13:25,995   Average segmentation loss on training set: 0.1015
2022-01-06 23:13:25,996 Validation Data Eval:
2022-01-06 23:13:28,696   Average segmentation loss on validation set: 0.1355
2022-01-06 23:13:30,170 iteration 3315 : loss : 0.021092, loss_ce: 0.010186
 49%|█████████████▏             | 195/400 [1:32:16<1:41:02, 29.57s/it]2022-01-06 23:13:31,688 iteration 3316 : loss : 0.019788, loss_ce: 0.007677
2022-01-06 23:13:33,165 iteration 3317 : loss : 0.037840, loss_ce: 0.013803
2022-01-06 23:13:34,750 iteration 3318 : loss : 0.034789, loss_ce: 0.015427
2022-01-06 23:13:36,220 iteration 3319 : loss : 0.024474, loss_ce: 0.009191
2022-01-06 23:13:37,701 iteration 3320 : loss : 0.022163, loss_ce: 0.008228
2022-01-06 23:13:39,207 iteration 3321 : loss : 0.026188, loss_ce: 0.007613
2022-01-06 23:13:40,721 iteration 3322 : loss : 0.045925, loss_ce: 0.020854
2022-01-06 23:13:42,192 iteration 3323 : loss : 0.028957, loss_ce: 0.009324
2022-01-06 23:13:43,795 iteration 3324 : loss : 0.040647, loss_ce: 0.018766
2022-01-06 23:13:45,268 iteration 3325 : loss : 0.030513, loss_ce: 0.011390
2022-01-06 23:13:46,815 iteration 3326 : loss : 0.036365, loss_ce: 0.014840
2022-01-06 23:13:48,418 iteration 3327 : loss : 0.041313, loss_ce: 0.016206
2022-01-06 23:13:49,932 iteration 3328 : loss : 0.031187, loss_ce: 0.012668
2022-01-06 23:13:51,412 iteration 3329 : loss : 0.024887, loss_ce: 0.010785
2022-01-06 23:13:52,914 iteration 3330 : loss : 0.042194, loss_ce: 0.013613
2022-01-06 23:13:54,440 iteration 3331 : loss : 0.022885, loss_ce: 0.008917
2022-01-06 23:13:55,875 iteration 3332 : loss : 0.020387, loss_ce: 0.009141
 49%|█████████████▏             | 196/400 [1:32:42<1:36:36, 28.42s/it]2022-01-06 23:13:57,431 iteration 3333 : loss : 0.023367, loss_ce: 0.008929
2022-01-06 23:13:58,961 iteration 3334 : loss : 0.025290, loss_ce: 0.012597
2022-01-06 23:14:00,406 iteration 3335 : loss : 0.021424, loss_ce: 0.009118
2022-01-06 23:14:01,920 iteration 3336 : loss : 0.046009, loss_ce: 0.014875
2022-01-06 23:14:03,472 iteration 3337 : loss : 0.045359, loss_ce: 0.021271
2022-01-06 23:14:05,054 iteration 3338 : loss : 0.039176, loss_ce: 0.022593
2022-01-06 23:14:06,546 iteration 3339 : loss : 0.036517, loss_ce: 0.012524
2022-01-06 23:14:08,021 iteration 3340 : loss : 0.025297, loss_ce: 0.010381
2022-01-06 23:14:09,521 iteration 3341 : loss : 0.022929, loss_ce: 0.008583
2022-01-06 23:14:11,075 iteration 3342 : loss : 0.049010, loss_ce: 0.022850
2022-01-06 23:14:12,614 iteration 3343 : loss : 0.024153, loss_ce: 0.008386
2022-01-06 23:14:14,143 iteration 3344 : loss : 0.033706, loss_ce: 0.012033
2022-01-06 23:14:15,637 iteration 3345 : loss : 0.027683, loss_ce: 0.010327
2022-01-06 23:14:17,160 iteration 3346 : loss : 0.021556, loss_ce: 0.010905
2022-01-06 23:14:18,723 iteration 3347 : loss : 0.042400, loss_ce: 0.012219
2022-01-06 23:14:20,360 iteration 3348 : loss : 0.035234, loss_ce: 0.012277
2022-01-06 23:14:21,941 iteration 3349 : loss : 0.042351, loss_ce: 0.018482
 49%|█████████████▎             | 197/400 [1:33:08<1:33:44, 27.71s/it]2022-01-06 23:14:23,539 iteration 3350 : loss : 0.021395, loss_ce: 0.008838
2022-01-06 23:14:25,061 iteration 3351 : loss : 0.031658, loss_ce: 0.013511
2022-01-06 23:14:26,608 iteration 3352 : loss : 0.044347, loss_ce: 0.014307
2022-01-06 23:14:28,185 iteration 3353 : loss : 0.024711, loss_ce: 0.011098
2022-01-06 23:14:29,761 iteration 3354 : loss : 0.036213, loss_ce: 0.016122
2022-01-06 23:14:31,390 iteration 3355 : loss : 0.026971, loss_ce: 0.013544
2022-01-06 23:14:33,001 iteration 3356 : loss : 0.064205, loss_ce: 0.016724
2022-01-06 23:14:34,619 iteration 3357 : loss : 0.035681, loss_ce: 0.013250
2022-01-06 23:14:36,238 iteration 3358 : loss : 0.031440, loss_ce: 0.011139
2022-01-06 23:14:37,823 iteration 3359 : loss : 0.028241, loss_ce: 0.011390
2022-01-06 23:14:39,460 iteration 3360 : loss : 0.032673, loss_ce: 0.014693
2022-01-06 23:14:40,956 iteration 3361 : loss : 0.032488, loss_ce: 0.009369
2022-01-06 23:14:42,468 iteration 3362 : loss : 0.027274, loss_ce: 0.011371
2022-01-06 23:14:43,966 iteration 3363 : loss : 0.026233, loss_ce: 0.011733
2022-01-06 23:14:45,533 iteration 3364 : loss : 0.032140, loss_ce: 0.011209
2022-01-06 23:14:47,023 iteration 3365 : loss : 0.027809, loss_ce: 0.013652
2022-01-06 23:14:48,538 iteration 3366 : loss : 0.028857, loss_ce: 0.010998
 50%|█████████████▎             | 198/400 [1:33:35<1:32:10, 27.38s/it]2022-01-06 23:14:50,109 iteration 3367 : loss : 0.033737, loss_ce: 0.011700
2022-01-06 23:14:51,624 iteration 3368 : loss : 0.027704, loss_ce: 0.013251
2022-01-06 23:14:53,171 iteration 3369 : loss : 0.025786, loss_ce: 0.009505
2022-01-06 23:14:54,721 iteration 3370 : loss : 0.031986, loss_ce: 0.015053
2022-01-06 23:14:56,160 iteration 3371 : loss : 0.020277, loss_ce: 0.009009
2022-01-06 23:14:57,703 iteration 3372 : loss : 0.031872, loss_ce: 0.016794
2022-01-06 23:14:59,225 iteration 3373 : loss : 0.027006, loss_ce: 0.013375
2022-01-06 23:15:00,715 iteration 3374 : loss : 0.021316, loss_ce: 0.009703
2022-01-06 23:15:02,205 iteration 3375 : loss : 0.022551, loss_ce: 0.009055
2022-01-06 23:15:03,710 iteration 3376 : loss : 0.030541, loss_ce: 0.013161
2022-01-06 23:15:05,283 iteration 3377 : loss : 0.045557, loss_ce: 0.013566
2022-01-06 23:15:06,815 iteration 3378 : loss : 0.040669, loss_ce: 0.014405
2022-01-06 23:15:08,311 iteration 3379 : loss : 0.021967, loss_ce: 0.010101
2022-01-06 23:15:09,890 iteration 3380 : loss : 0.047294, loss_ce: 0.019886
2022-01-06 23:15:11,362 iteration 3381 : loss : 0.053098, loss_ce: 0.012180
2022-01-06 23:15:12,900 iteration 3382 : loss : 0.033964, loss_ce: 0.012570
2022-01-06 23:15:14,404 iteration 3383 : loss : 0.028103, loss_ce: 0.010348
 50%|█████████████▍             | 199/400 [1:34:01<1:30:11, 26.92s/it]2022-01-06 23:15:15,939 iteration 3384 : loss : 0.028720, loss_ce: 0.009912
2022-01-06 23:15:17,506 iteration 3385 : loss : 0.025023, loss_ce: 0.010852
2022-01-06 23:15:19,088 iteration 3386 : loss : 0.030329, loss_ce: 0.014665
2022-01-06 23:15:20,641 iteration 3387 : loss : 0.024653, loss_ce: 0.009562
2022-01-06 23:15:22,099 iteration 3388 : loss : 0.027353, loss_ce: 0.010370
2022-01-06 23:15:23,645 iteration 3389 : loss : 0.040863, loss_ce: 0.018496
2022-01-06 23:15:25,098 iteration 3390 : loss : 0.029230, loss_ce: 0.009277
2022-01-06 23:15:26,702 iteration 3391 : loss : 0.047153, loss_ce: 0.015537
2022-01-06 23:15:28,249 iteration 3392 : loss : 0.042412, loss_ce: 0.017153
2022-01-06 23:15:29,786 iteration 3393 : loss : 0.033459, loss_ce: 0.011858
2022-01-06 23:15:31,212 iteration 3394 : loss : 0.031389, loss_ce: 0.010839
2022-01-06 23:15:32,813 iteration 3395 : loss : 0.044075, loss_ce: 0.016269
2022-01-06 23:15:34,336 iteration 3396 : loss : 0.032608, loss_ce: 0.013731
2022-01-06 23:15:35,827 iteration 3397 : loss : 0.037103, loss_ce: 0.014899
2022-01-06 23:15:37,367 iteration 3398 : loss : 0.026851, loss_ce: 0.009766
2022-01-06 23:15:38,885 iteration 3399 : loss : 0.022431, loss_ce: 0.009201
2022-01-06 23:15:38,885 Training Data Eval:
2022-01-06 23:15:46,693   Average segmentation loss on training set: 0.0425
2022-01-06 23:15:46,693 Validation Data Eval:
2022-01-06 23:15:49,413   Average segmentation loss on validation set: 0.1541
2022-01-06 23:15:50,930 iteration 3400 : loss : 0.029668, loss_ce: 0.009544
 50%|█████████████▌             | 200/400 [1:34:37<1:39:20, 29.80s/it]2022-01-06 23:15:52,628 iteration 3401 : loss : 0.046029, loss_ce: 0.023191
2022-01-06 23:15:54,182 iteration 3402 : loss : 0.025623, loss_ce: 0.009651
2022-01-06 23:15:55,672 iteration 3403 : loss : 0.029689, loss_ce: 0.014868
2022-01-06 23:15:57,168 iteration 3404 : loss : 0.023886, loss_ce: 0.008306
2022-01-06 23:15:58,660 iteration 3405 : loss : 0.030053, loss_ce: 0.012297
2022-01-06 23:16:00,216 iteration 3406 : loss : 0.043218, loss_ce: 0.015036
2022-01-06 23:16:01,735 iteration 3407 : loss : 0.034414, loss_ce: 0.015471
2022-01-06 23:16:03,267 iteration 3408 : loss : 0.030214, loss_ce: 0.011097
2022-01-06 23:16:04,799 iteration 3409 : loss : 0.059101, loss_ce: 0.023394
2022-01-06 23:16:06,316 iteration 3410 : loss : 0.029573, loss_ce: 0.010875
2022-01-06 23:16:07,866 iteration 3411 : loss : 0.038487, loss_ce: 0.013262
2022-01-06 23:16:09,321 iteration 3412 : loss : 0.035580, loss_ce: 0.012228
2022-01-06 23:16:10,764 iteration 3413 : loss : 0.023639, loss_ce: 0.010717
2022-01-06 23:16:12,298 iteration 3414 : loss : 0.062116, loss_ce: 0.015080
2022-01-06 23:16:13,856 iteration 3415 : loss : 0.036409, loss_ce: 0.014257
2022-01-06 23:16:15,405 iteration 3416 : loss : 0.029852, loss_ce: 0.009192
2022-01-06 23:16:16,985 iteration 3417 : loss : 0.035907, loss_ce: 0.014055
 50%|█████████████▌             | 201/400 [1:35:03<1:35:06, 28.68s/it]2022-01-06 23:16:18,556 iteration 3418 : loss : 0.026773, loss_ce: 0.012285
2022-01-06 23:16:20,113 iteration 3419 : loss : 0.035242, loss_ce: 0.011189
2022-01-06 23:16:21,745 iteration 3420 : loss : 0.047252, loss_ce: 0.015488
2022-01-06 23:16:23,323 iteration 3421 : loss : 0.036988, loss_ce: 0.013295
2022-01-06 23:16:24,840 iteration 3422 : loss : 0.023514, loss_ce: 0.008227
2022-01-06 23:16:26,409 iteration 3423 : loss : 0.028409, loss_ce: 0.012438
2022-01-06 23:16:27,972 iteration 3424 : loss : 0.031748, loss_ce: 0.014810
2022-01-06 23:16:29,492 iteration 3425 : loss : 0.035004, loss_ce: 0.016809
2022-01-06 23:16:31,055 iteration 3426 : loss : 0.030316, loss_ce: 0.010249
2022-01-06 23:16:32,629 iteration 3427 : loss : 0.046673, loss_ce: 0.014647
2022-01-06 23:16:34,211 iteration 3428 : loss : 0.047052, loss_ce: 0.017389
2022-01-06 23:16:35,798 iteration 3429 : loss : 0.033677, loss_ce: 0.012255
2022-01-06 23:16:37,391 iteration 3430 : loss : 0.039495, loss_ce: 0.015619
2022-01-06 23:16:38,862 iteration 3431 : loss : 0.024120, loss_ce: 0.010134
2022-01-06 23:16:40,369 iteration 3432 : loss : 0.028118, loss_ce: 0.009076
2022-01-06 23:16:41,868 iteration 3433 : loss : 0.029948, loss_ce: 0.010942
2022-01-06 23:16:43,443 iteration 3434 : loss : 0.047610, loss_ce: 0.016001
 50%|█████████████▋             | 202/400 [1:35:30<1:32:26, 28.01s/it]2022-01-06 23:16:44,949 iteration 3435 : loss : 0.028965, loss_ce: 0.009370
2022-01-06 23:16:46,447 iteration 3436 : loss : 0.023247, loss_ce: 0.008452
2022-01-06 23:16:48,062 iteration 3437 : loss : 0.035661, loss_ce: 0.018030
2022-01-06 23:16:49,647 iteration 3438 : loss : 0.025133, loss_ce: 0.011058
2022-01-06 23:16:51,217 iteration 3439 : loss : 0.031246, loss_ce: 0.012464
2022-01-06 23:16:52,714 iteration 3440 : loss : 0.027460, loss_ce: 0.010255
2022-01-06 23:16:54,255 iteration 3441 : loss : 0.025085, loss_ce: 0.009783
2022-01-06 23:16:55,923 iteration 3442 : loss : 0.038650, loss_ce: 0.015091
2022-01-06 23:16:57,527 iteration 3443 : loss : 0.034206, loss_ce: 0.011106
2022-01-06 23:16:59,138 iteration 3444 : loss : 0.037202, loss_ce: 0.013659
2022-01-06 23:17:00,714 iteration 3445 : loss : 0.034579, loss_ce: 0.010219
2022-01-06 23:17:02,275 iteration 3446 : loss : 0.027807, loss_ce: 0.012336
2022-01-06 23:17:03,822 iteration 3447 : loss : 0.029414, loss_ce: 0.011074
2022-01-06 23:17:05,428 iteration 3448 : loss : 0.034254, loss_ce: 0.010460
2022-01-06 23:17:07,011 iteration 3449 : loss : 0.029443, loss_ce: 0.012161
2022-01-06 23:17:08,568 iteration 3450 : loss : 0.037578, loss_ce: 0.016654
2022-01-06 23:17:10,237 iteration 3451 : loss : 0.027934, loss_ce: 0.010437
 51%|█████████████▋             | 203/400 [1:35:56<1:30:46, 27.65s/it]2022-01-06 23:17:11,826 iteration 3452 : loss : 0.032482, loss_ce: 0.012582
2022-01-06 23:17:13,345 iteration 3453 : loss : 0.022958, loss_ce: 0.009575
2022-01-06 23:17:14,846 iteration 3454 : loss : 0.033842, loss_ce: 0.011806
2022-01-06 23:17:16,448 iteration 3455 : loss : 0.038572, loss_ce: 0.016960
2022-01-06 23:17:17,948 iteration 3456 : loss : 0.023776, loss_ce: 0.007425
2022-01-06 23:17:19,565 iteration 3457 : loss : 0.027235, loss_ce: 0.010435
2022-01-06 23:17:21,129 iteration 3458 : loss : 0.036498, loss_ce: 0.013811
2022-01-06 23:17:22,661 iteration 3459 : loss : 0.026259, loss_ce: 0.011311
2022-01-06 23:17:24,185 iteration 3460 : loss : 0.028310, loss_ce: 0.012075
2022-01-06 23:17:25,726 iteration 3461 : loss : 0.026671, loss_ce: 0.009719
2022-01-06 23:17:27,258 iteration 3462 : loss : 0.028432, loss_ce: 0.014366
2022-01-06 23:17:28,827 iteration 3463 : loss : 0.023363, loss_ce: 0.008744
2022-01-06 23:17:30,306 iteration 3464 : loss : 0.020981, loss_ce: 0.008073
2022-01-06 23:17:31,882 iteration 3465 : loss : 0.028371, loss_ce: 0.009374
2022-01-06 23:17:33,416 iteration 3466 : loss : 0.024062, loss_ce: 0.008337
2022-01-06 23:17:34,932 iteration 3467 : loss : 0.030856, loss_ce: 0.015860
2022-01-06 23:17:36,441 iteration 3468 : loss : 0.028046, loss_ce: 0.011650
 51%|█████████████▊             | 204/400 [1:36:23<1:28:53, 27.21s/it]2022-01-06 23:17:37,981 iteration 3469 : loss : 0.030927, loss_ce: 0.008937
2022-01-06 23:17:39,551 iteration 3470 : loss : 0.039553, loss_ce: 0.015410
2022-01-06 23:17:41,002 iteration 3471 : loss : 0.020814, loss_ce: 0.008282
2022-01-06 23:17:42,551 iteration 3472 : loss : 0.033375, loss_ce: 0.014219
2022-01-06 23:17:44,194 iteration 3473 : loss : 0.053406, loss_ce: 0.020327
2022-01-06 23:17:45,729 iteration 3474 : loss : 0.027314, loss_ce: 0.008608
2022-01-06 23:17:47,349 iteration 3475 : loss : 0.064701, loss_ce: 0.014856
2022-01-06 23:17:48,874 iteration 3476 : loss : 0.026217, loss_ce: 0.007664
2022-01-06 23:17:50,469 iteration 3477 : loss : 0.034555, loss_ce: 0.009785
2022-01-06 23:17:52,035 iteration 3478 : loss : 0.023719, loss_ce: 0.008618
2022-01-06 23:17:53,560 iteration 3479 : loss : 0.026734, loss_ce: 0.010851
2022-01-06 23:17:55,050 iteration 3480 : loss : 0.023621, loss_ce: 0.009446
2022-01-06 23:17:56,606 iteration 3481 : loss : 0.036300, loss_ce: 0.013669
2022-01-06 23:17:58,166 iteration 3482 : loss : 0.036473, loss_ce: 0.012754
2022-01-06 23:17:59,785 iteration 3483 : loss : 0.035704, loss_ce: 0.018287
2022-01-06 23:18:01,270 iteration 3484 : loss : 0.025041, loss_ce: 0.010204
2022-01-06 23:18:01,270 Training Data Eval:
2022-01-06 23:18:09,329   Average segmentation loss on training set: 0.0205
2022-01-06 23:18:09,330 Validation Data Eval:
2022-01-06 23:18:12,112   Average segmentation loss on validation set: 0.0945
2022-01-06 23:18:13,766 iteration 3485 : loss : 0.044384, loss_ce: 0.021760
 51%|█████████████▊             | 205/400 [1:37:00<1:38:18, 30.25s/it]2022-01-06 23:18:15,390 iteration 3486 : loss : 0.036912, loss_ce: 0.016144
2022-01-06 23:18:16,900 iteration 3487 : loss : 0.026419, loss_ce: 0.011134
2022-01-06 23:18:18,438 iteration 3488 : loss : 0.034494, loss_ce: 0.014348
2022-01-06 23:18:19,985 iteration 3489 : loss : 0.041469, loss_ce: 0.017503
2022-01-06 23:18:21,565 iteration 3490 : loss : 0.034711, loss_ce: 0.011709
2022-01-06 23:18:23,174 iteration 3491 : loss : 0.039145, loss_ce: 0.015903
2022-01-06 23:18:24,667 iteration 3492 : loss : 0.036884, loss_ce: 0.014675
2022-01-06 23:18:26,316 iteration 3493 : loss : 0.030287, loss_ce: 0.011522
2022-01-06 23:18:27,888 iteration 3494 : loss : 0.025115, loss_ce: 0.008193
2022-01-06 23:18:29,426 iteration 3495 : loss : 0.034137, loss_ce: 0.010485
2022-01-06 23:18:30,998 iteration 3496 : loss : 0.030058, loss_ce: 0.013329
2022-01-06 23:18:32,561 iteration 3497 : loss : 0.036848, loss_ce: 0.015737
2022-01-06 23:18:34,208 iteration 3498 : loss : 0.036284, loss_ce: 0.013460
2022-01-06 23:18:35,716 iteration 3499 : loss : 0.027013, loss_ce: 0.010108
2022-01-06 23:18:37,177 iteration 3500 : loss : 0.019566, loss_ce: 0.006426
2022-01-06 23:18:38,667 iteration 3501 : loss : 0.027181, loss_ce: 0.009784
2022-01-06 23:18:40,225 iteration 3502 : loss : 0.039651, loss_ce: 0.017333
 52%|█████████████▉             | 206/400 [1:37:26<1:34:07, 29.11s/it]2022-01-06 23:18:41,858 iteration 3503 : loss : 0.027281, loss_ce: 0.010533
2022-01-06 23:18:43,403 iteration 3504 : loss : 0.026600, loss_ce: 0.007566
2022-01-06 23:18:44,878 iteration 3505 : loss : 0.025230, loss_ce: 0.011172
2022-01-06 23:18:46,458 iteration 3506 : loss : 0.041335, loss_ce: 0.012022
2022-01-06 23:18:48,003 iteration 3507 : loss : 0.037520, loss_ce: 0.011212
2022-01-06 23:18:49,670 iteration 3508 : loss : 0.035538, loss_ce: 0.016793
2022-01-06 23:18:51,247 iteration 3509 : loss : 0.030993, loss_ce: 0.012024
2022-01-06 23:18:52,766 iteration 3510 : loss : 0.034148, loss_ce: 0.013603
2022-01-06 23:18:54,273 iteration 3511 : loss : 0.028623, loss_ce: 0.012634
2022-01-06 23:18:55,889 iteration 3512 : loss : 0.040254, loss_ce: 0.013558
2022-01-06 23:18:57,439 iteration 3513 : loss : 0.029961, loss_ce: 0.009480
2022-01-06 23:18:59,142 iteration 3514 : loss : 0.041813, loss_ce: 0.023723
2022-01-06 23:19:00,682 iteration 3515 : loss : 0.026453, loss_ce: 0.009366
2022-01-06 23:19:02,200 iteration 3516 : loss : 0.024175, loss_ce: 0.010424
2022-01-06 23:19:03,711 iteration 3517 : loss : 0.028249, loss_ce: 0.010870
2022-01-06 23:19:05,236 iteration 3518 : loss : 0.028437, loss_ce: 0.015048
2022-01-06 23:19:06,799 iteration 3519 : loss : 0.037230, loss_ce: 0.013319
 52%|█████████████▉             | 207/400 [1:37:53<1:31:11, 28.35s/it]2022-01-06 23:19:08,409 iteration 3520 : loss : 0.033381, loss_ce: 0.013421
2022-01-06 23:19:09,906 iteration 3521 : loss : 0.042574, loss_ce: 0.019122
2022-01-06 23:19:11,386 iteration 3522 : loss : 0.022329, loss_ce: 0.010450
2022-01-06 23:19:12,902 iteration 3523 : loss : 0.018828, loss_ce: 0.006813
2022-01-06 23:19:14,456 iteration 3524 : loss : 0.036746, loss_ce: 0.015220
2022-01-06 23:19:16,063 iteration 3525 : loss : 0.023519, loss_ce: 0.009246
2022-01-06 23:19:17,604 iteration 3526 : loss : 0.024155, loss_ce: 0.008939
2022-01-06 23:19:19,184 iteration 3527 : loss : 0.050450, loss_ce: 0.014691
2022-01-06 23:19:20,837 iteration 3528 : loss : 0.031119, loss_ce: 0.012626
2022-01-06 23:19:22,387 iteration 3529 : loss : 0.037687, loss_ce: 0.010871
2022-01-06 23:19:23,922 iteration 3530 : loss : 0.021992, loss_ce: 0.009564
2022-01-06 23:19:25,395 iteration 3531 : loss : 0.025807, loss_ce: 0.007286
2022-01-06 23:19:26,985 iteration 3532 : loss : 0.035460, loss_ce: 0.014816
2022-01-06 23:19:28,490 iteration 3533 : loss : 0.027655, loss_ce: 0.008599
2022-01-06 23:19:30,032 iteration 3534 : loss : 0.034682, loss_ce: 0.013195
2022-01-06 23:19:31,620 iteration 3535 : loss : 0.032301, loss_ce: 0.013456
2022-01-06 23:19:33,179 iteration 3536 : loss : 0.029602, loss_ce: 0.012913
 52%|██████████████             | 208/400 [1:38:19<1:28:49, 27.76s/it]2022-01-06 23:19:34,692 iteration 3537 : loss : 0.028010, loss_ce: 0.012174
2022-01-06 23:19:36,171 iteration 3538 : loss : 0.024709, loss_ce: 0.010302
2022-01-06 23:19:37,672 iteration 3539 : loss : 0.021125, loss_ce: 0.010893
2022-01-06 23:19:39,117 iteration 3540 : loss : 0.021974, loss_ce: 0.006891
2022-01-06 23:19:40,733 iteration 3541 : loss : 0.026654, loss_ce: 0.010104
2022-01-06 23:19:42,275 iteration 3542 : loss : 0.024144, loss_ce: 0.010295
2022-01-06 23:19:43,926 iteration 3543 : loss : 0.045435, loss_ce: 0.014550
2022-01-06 23:19:45,391 iteration 3544 : loss : 0.018098, loss_ce: 0.007476
2022-01-06 23:19:46,921 iteration 3545 : loss : 0.021217, loss_ce: 0.009765
2022-01-06 23:19:48,384 iteration 3546 : loss : 0.022297, loss_ce: 0.008093
2022-01-06 23:19:49,908 iteration 3547 : loss : 0.025164, loss_ce: 0.011407
2022-01-06 23:19:51,376 iteration 3548 : loss : 0.031370, loss_ce: 0.010626
2022-01-06 23:19:52,910 iteration 3549 : loss : 0.023281, loss_ce: 0.009024
2022-01-06 23:19:54,470 iteration 3550 : loss : 0.024363, loss_ce: 0.010308
2022-01-06 23:19:56,045 iteration 3551 : loss : 0.037888, loss_ce: 0.011746
2022-01-06 23:19:57,608 iteration 3552 : loss : 0.044877, loss_ce: 0.016931
2022-01-06 23:19:59,164 iteration 3553 : loss : 0.036421, loss_ce: 0.016229
 52%|██████████████             | 209/400 [1:38:45<1:26:40, 27.23s/it]2022-01-06 23:20:00,743 iteration 3554 : loss : 0.030725, loss_ce: 0.013742
2022-01-06 23:20:02,237 iteration 3555 : loss : 0.022305, loss_ce: 0.007391
2022-01-06 23:20:03,742 iteration 3556 : loss : 0.024810, loss_ce: 0.010537
2022-01-06 23:20:05,334 iteration 3557 : loss : 0.022773, loss_ce: 0.006969
2022-01-06 23:20:06,793 iteration 3558 : loss : 0.052159, loss_ce: 0.013004
2022-01-06 23:20:08,288 iteration 3559 : loss : 0.031627, loss_ce: 0.009548
2022-01-06 23:20:09,705 iteration 3560 : loss : 0.018999, loss_ce: 0.009148
2022-01-06 23:20:11,294 iteration 3561 : loss : 0.029083, loss_ce: 0.011531
2022-01-06 23:20:12,770 iteration 3562 : loss : 0.021764, loss_ce: 0.008716
2022-01-06 23:20:14,310 iteration 3563 : loss : 0.024633, loss_ce: 0.008383
2022-01-06 23:20:15,837 iteration 3564 : loss : 0.040563, loss_ce: 0.015758
2022-01-06 23:20:17,455 iteration 3565 : loss : 0.037669, loss_ce: 0.013000
2022-01-06 23:20:18,968 iteration 3566 : loss : 0.034038, loss_ce: 0.013098
2022-01-06 23:20:20,585 iteration 3567 : loss : 0.031055, loss_ce: 0.013444
2022-01-06 23:20:22,156 iteration 3568 : loss : 0.032230, loss_ce: 0.012373
2022-01-06 23:20:23,716 iteration 3569 : loss : 0.031709, loss_ce: 0.008057
2022-01-06 23:20:23,716 Training Data Eval:
2022-01-06 23:20:31,467   Average segmentation loss on training set: 0.0203
2022-01-06 23:20:31,468 Validation Data Eval:
2022-01-06 23:20:34,187   Average segmentation loss on validation set: 0.0953
2022-01-06 23:20:35,742 iteration 3570 : loss : 0.030518, loss_ce: 0.011020
 52%|██████████████▏            | 210/400 [1:39:22<1:35:05, 30.03s/it]2022-01-06 23:20:37,323 iteration 3571 : loss : 0.034749, loss_ce: 0.014458
2022-01-06 23:20:38,856 iteration 3572 : loss : 0.018946, loss_ce: 0.005052
2022-01-06 23:20:40,388 iteration 3573 : loss : 0.045277, loss_ce: 0.022048
2022-01-06 23:20:41,890 iteration 3574 : loss : 0.020574, loss_ce: 0.008996
2022-01-06 23:20:43,396 iteration 3575 : loss : 0.022710, loss_ce: 0.009028
2022-01-06 23:20:44,938 iteration 3576 : loss : 0.039203, loss_ce: 0.013160
2022-01-06 23:20:46,513 iteration 3577 : loss : 0.025474, loss_ce: 0.011559
2022-01-06 23:20:48,076 iteration 3578 : loss : 0.026642, loss_ce: 0.010744
2022-01-06 23:20:49,558 iteration 3579 : loss : 0.025040, loss_ce: 0.009666
2022-01-06 23:20:51,104 iteration 3580 : loss : 0.027659, loss_ce: 0.008308
2022-01-06 23:20:52,590 iteration 3581 : loss : 0.042887, loss_ce: 0.009798
2022-01-06 23:20:54,099 iteration 3582 : loss : 0.023765, loss_ce: 0.009234
2022-01-06 23:20:55,645 iteration 3583 : loss : 0.036638, loss_ce: 0.015223
2022-01-06 23:20:57,115 iteration 3584 : loss : 0.032041, loss_ce: 0.014393
2022-01-06 23:20:58,619 iteration 3585 : loss : 0.021309, loss_ce: 0.007605
2022-01-06 23:21:00,088 iteration 3586 : loss : 0.029322, loss_ce: 0.010291
2022-01-06 23:21:01,609 iteration 3587 : loss : 0.029766, loss_ce: 0.014193
 53%|██████████████▏            | 211/400 [1:39:48<1:30:39, 28.78s/it]2022-01-06 23:21:03,122 iteration 3588 : loss : 0.026146, loss_ce: 0.009646
2022-01-06 23:21:04,680 iteration 3589 : loss : 0.032284, loss_ce: 0.012018
2022-01-06 23:21:06,214 iteration 3590 : loss : 0.029751, loss_ce: 0.012612
2022-01-06 23:21:07,735 iteration 3591 : loss : 0.023546, loss_ce: 0.007900
2022-01-06 23:21:09,314 iteration 3592 : loss : 0.024576, loss_ce: 0.009434
2022-01-06 23:21:10,866 iteration 3593 : loss : 0.031020, loss_ce: 0.009583
2022-01-06 23:21:12,403 iteration 3594 : loss : 0.021334, loss_ce: 0.006036
2022-01-06 23:21:14,046 iteration 3595 : loss : 0.062372, loss_ce: 0.014389
2022-01-06 23:21:15,501 iteration 3596 : loss : 0.023240, loss_ce: 0.009683
2022-01-06 23:21:17,091 iteration 3597 : loss : 0.033487, loss_ce: 0.011047
2022-01-06 23:21:18,601 iteration 3598 : loss : 0.032267, loss_ce: 0.015512
2022-01-06 23:21:20,121 iteration 3599 : loss : 0.032589, loss_ce: 0.012348
2022-01-06 23:21:21,620 iteration 3600 : loss : 0.022099, loss_ce: 0.009867
2022-01-06 23:21:23,136 iteration 3601 : loss : 0.030000, loss_ce: 0.012798
2022-01-06 23:21:24,633 iteration 3602 : loss : 0.028035, loss_ce: 0.013978
2022-01-06 23:21:26,104 iteration 3603 : loss : 0.018319, loss_ce: 0.006689
2022-01-06 23:21:27,564 iteration 3604 : loss : 0.019991, loss_ce: 0.007114
 53%|██████████████▎            | 212/400 [1:40:14<1:27:32, 27.94s/it]2022-01-06 23:21:29,123 iteration 3605 : loss : 0.020687, loss_ce: 0.008243
2022-01-06 23:21:30,625 iteration 3606 : loss : 0.031059, loss_ce: 0.010722
2022-01-06 23:21:32,074 iteration 3607 : loss : 0.021289, loss_ce: 0.007774
2022-01-06 23:21:33,534 iteration 3608 : loss : 0.022928, loss_ce: 0.010363
2022-01-06 23:21:35,073 iteration 3609 : loss : 0.024209, loss_ce: 0.007293
2022-01-06 23:21:36,552 iteration 3610 : loss : 0.032431, loss_ce: 0.014684
2022-01-06 23:21:38,099 iteration 3611 : loss : 0.028658, loss_ce: 0.015486
2022-01-06 23:21:39,649 iteration 3612 : loss : 0.023257, loss_ce: 0.007788
2022-01-06 23:21:41,144 iteration 3613 : loss : 0.031649, loss_ce: 0.010928
2022-01-06 23:21:42,691 iteration 3614 : loss : 0.024185, loss_ce: 0.009094
2022-01-06 23:21:44,176 iteration 3615 : loss : 0.034134, loss_ce: 0.014652
2022-01-06 23:21:45,741 iteration 3616 : loss : 0.028164, loss_ce: 0.008825
2022-01-06 23:21:47,226 iteration 3617 : loss : 0.032691, loss_ce: 0.013983
2022-01-06 23:21:48,729 iteration 3618 : loss : 0.025119, loss_ce: 0.011245
2022-01-06 23:21:50,235 iteration 3619 : loss : 0.029885, loss_ce: 0.015357
2022-01-06 23:21:51,778 iteration 3620 : loss : 0.027037, loss_ce: 0.010909
2022-01-06 23:21:53,288 iteration 3621 : loss : 0.027797, loss_ce: 0.012653
 53%|██████████████▍            | 213/400 [1:40:40<1:24:59, 27.27s/it]2022-01-06 23:21:54,919 iteration 3622 : loss : 0.043473, loss_ce: 0.014050
2022-01-06 23:21:56,444 iteration 3623 : loss : 0.027376, loss_ce: 0.010528
2022-01-06 23:21:58,040 iteration 3624 : loss : 0.022632, loss_ce: 0.007563
2022-01-06 23:21:59,622 iteration 3625 : loss : 0.039339, loss_ce: 0.017598
2022-01-06 23:22:01,142 iteration 3626 : loss : 0.023072, loss_ce: 0.009510
2022-01-06 23:22:02,705 iteration 3627 : loss : 0.022744, loss_ce: 0.011471
2022-01-06 23:22:04,227 iteration 3628 : loss : 0.028329, loss_ce: 0.009029
2022-01-06 23:22:05,807 iteration 3629 : loss : 0.026705, loss_ce: 0.011285
2022-01-06 23:22:07,324 iteration 3630 : loss : 0.021432, loss_ce: 0.009593
2022-01-06 23:22:08,780 iteration 3631 : loss : 0.022200, loss_ce: 0.009185
2022-01-06 23:22:10,229 iteration 3632 : loss : 0.020406, loss_ce: 0.008615
2022-01-06 23:22:11,807 iteration 3633 : loss : 0.039125, loss_ce: 0.016114
2022-01-06 23:22:13,316 iteration 3634 : loss : 0.029065, loss_ce: 0.009961
2022-01-06 23:22:14,795 iteration 3635 : loss : 0.026960, loss_ce: 0.009450
2022-01-06 23:22:16,264 iteration 3636 : loss : 0.023929, loss_ce: 0.010190
2022-01-06 23:22:17,822 iteration 3637 : loss : 0.042482, loss_ce: 0.014328
2022-01-06 23:22:19,294 iteration 3638 : loss : 0.024538, loss_ce: 0.007701
 54%|██████████████▍            | 214/400 [1:41:06<1:23:21, 26.89s/it]2022-01-06 23:22:20,862 iteration 3639 : loss : 0.022166, loss_ce: 0.009672
2022-01-06 23:22:22,436 iteration 3640 : loss : 0.025073, loss_ce: 0.008629
2022-01-06 23:22:23,965 iteration 3641 : loss : 0.035123, loss_ce: 0.010624
2022-01-06 23:22:25,531 iteration 3642 : loss : 0.025149, loss_ce: 0.010608
2022-01-06 23:22:27,048 iteration 3643 : loss : 0.034843, loss_ce: 0.015845
2022-01-06 23:22:28,549 iteration 3644 : loss : 0.021893, loss_ce: 0.010176
2022-01-06 23:22:30,031 iteration 3645 : loss : 0.018843, loss_ce: 0.007465
2022-01-06 23:22:31,563 iteration 3646 : loss : 0.023844, loss_ce: 0.009086
2022-01-06 23:22:33,110 iteration 3647 : loss : 0.029414, loss_ce: 0.012772
2022-01-06 23:22:34,718 iteration 3648 : loss : 0.029231, loss_ce: 0.011598
2022-01-06 23:22:36,244 iteration 3649 : loss : 0.045960, loss_ce: 0.016971
2022-01-06 23:22:37,761 iteration 3650 : loss : 0.026261, loss_ce: 0.008437
2022-01-06 23:22:39,296 iteration 3651 : loss : 0.025862, loss_ce: 0.010171
2022-01-06 23:22:40,786 iteration 3652 : loss : 0.021768, loss_ce: 0.009297
2022-01-06 23:22:42,362 iteration 3653 : loss : 0.059179, loss_ce: 0.014221
2022-01-06 23:22:43,890 iteration 3654 : loss : 0.022760, loss_ce: 0.006693
2022-01-06 23:22:43,890 Training Data Eval:
2022-01-06 23:22:51,698   Average segmentation loss on training set: 0.0177
2022-01-06 23:22:51,698 Validation Data Eval:
2022-01-06 23:22:54,393   Average segmentation loss on validation set: 0.0752
2022-01-06 23:22:55,838 iteration 3655 : loss : 0.020446, loss_ce: 0.007147
 54%|██████████████▌            | 215/400 [1:41:42<1:31:50, 29.79s/it]2022-01-06 23:22:57,461 iteration 3656 : loss : 0.065744, loss_ce: 0.020559
2022-01-06 23:22:58,939 iteration 3657 : loss : 0.027773, loss_ce: 0.012934
2022-01-06 23:23:00,456 iteration 3658 : loss : 0.028155, loss_ce: 0.012858
2022-01-06 23:23:02,000 iteration 3659 : loss : 0.026708, loss_ce: 0.009355
2022-01-06 23:23:03,590 iteration 3660 : loss : 0.040669, loss_ce: 0.013602
2022-01-06 23:23:05,189 iteration 3661 : loss : 0.035070, loss_ce: 0.018173
2022-01-06 23:23:06,620 iteration 3662 : loss : 0.028458, loss_ce: 0.009473
2022-01-06 23:23:08,099 iteration 3663 : loss : 0.018440, loss_ce: 0.005906
2022-01-06 23:23:09,592 iteration 3664 : loss : 0.025729, loss_ce: 0.008471
2022-01-06 23:23:11,086 iteration 3665 : loss : 0.025503, loss_ce: 0.011506
2022-01-06 23:23:12,697 iteration 3666 : loss : 0.048473, loss_ce: 0.023112
2022-01-06 23:23:14,264 iteration 3667 : loss : 0.036971, loss_ce: 0.013896
2022-01-06 23:23:15,843 iteration 3668 : loss : 0.030923, loss_ce: 0.012372
2022-01-06 23:23:17,400 iteration 3669 : loss : 0.026305, loss_ce: 0.012268
2022-01-06 23:23:18,919 iteration 3670 : loss : 0.042512, loss_ce: 0.013690
2022-01-06 23:23:20,495 iteration 3671 : loss : 0.032740, loss_ce: 0.012015
2022-01-06 23:23:22,004 iteration 3672 : loss : 0.028882, loss_ce: 0.011954
 54%|██████████████▌            | 216/400 [1:42:08<1:28:00, 28.70s/it]2022-01-06 23:23:23,592 iteration 3673 : loss : 0.035447, loss_ce: 0.016926
2022-01-06 23:23:25,135 iteration 3674 : loss : 0.028133, loss_ce: 0.012007
2022-01-06 23:23:26,665 iteration 3675 : loss : 0.025643, loss_ce: 0.013084
2022-01-06 23:23:28,188 iteration 3676 : loss : 0.031389, loss_ce: 0.009677
2022-01-06 23:23:29,710 iteration 3677 : loss : 0.028827, loss_ce: 0.014099
2022-01-06 23:23:31,312 iteration 3678 : loss : 0.035129, loss_ce: 0.011245
2022-01-06 23:23:32,837 iteration 3679 : loss : 0.028510, loss_ce: 0.011198
2022-01-06 23:23:34,337 iteration 3680 : loss : 0.033462, loss_ce: 0.012259
2022-01-06 23:23:35,908 iteration 3681 : loss : 0.026236, loss_ce: 0.009443
2022-01-06 23:23:37,412 iteration 3682 : loss : 0.031633, loss_ce: 0.011580
2022-01-06 23:23:38,976 iteration 3683 : loss : 0.080134, loss_ce: 0.010538
2022-01-06 23:23:40,455 iteration 3684 : loss : 0.019419, loss_ce: 0.005014
2022-01-06 23:23:41,956 iteration 3685 : loss : 0.035483, loss_ce: 0.016563
2022-01-06 23:23:43,486 iteration 3686 : loss : 0.027306, loss_ce: 0.008941
2022-01-06 23:23:45,004 iteration 3687 : loss : 0.025676, loss_ce: 0.008309
2022-01-06 23:23:46,507 iteration 3688 : loss : 0.041205, loss_ce: 0.021272
2022-01-06 23:23:47,955 iteration 3689 : loss : 0.023326, loss_ce: 0.008200
 54%|██████████████▋            | 217/400 [1:42:34<1:25:01, 27.88s/it]2022-01-06 23:23:49,519 iteration 3690 : loss : 0.023768, loss_ce: 0.006420
2022-01-06 23:23:51,089 iteration 3691 : loss : 0.053309, loss_ce: 0.012847
2022-01-06 23:23:52,593 iteration 3692 : loss : 0.025927, loss_ce: 0.012537
2022-01-06 23:23:54,170 iteration 3693 : loss : 0.039351, loss_ce: 0.016057
2022-01-06 23:23:55,687 iteration 3694 : loss : 0.020615, loss_ce: 0.009019
2022-01-06 23:23:57,158 iteration 3695 : loss : 0.025173, loss_ce: 0.010887
2022-01-06 23:23:58,712 iteration 3696 : loss : 0.032311, loss_ce: 0.017236
2022-01-06 23:24:00,200 iteration 3697 : loss : 0.024285, loss_ce: 0.007270
2022-01-06 23:24:01,707 iteration 3698 : loss : 0.033199, loss_ce: 0.013030
2022-01-06 23:24:03,285 iteration 3699 : loss : 0.029672, loss_ce: 0.011193
2022-01-06 23:24:04,808 iteration 3700 : loss : 0.025941, loss_ce: 0.011498
2022-01-06 23:24:06,299 iteration 3701 : loss : 0.024669, loss_ce: 0.009717
2022-01-06 23:24:07,782 iteration 3702 : loss : 0.039555, loss_ce: 0.015921
2022-01-06 23:24:09,396 iteration 3703 : loss : 0.036943, loss_ce: 0.018276
2022-01-06 23:24:10,871 iteration 3704 : loss : 0.027659, loss_ce: 0.009122
2022-01-06 23:24:12,340 iteration 3705 : loss : 0.022194, loss_ce: 0.007428
2022-01-06 23:24:13,801 iteration 3706 : loss : 0.023713, loss_ce: 0.008199
 55%|██████████████▋            | 218/400 [1:43:00<1:22:42, 27.27s/it]2022-01-06 23:24:15,353 iteration 3707 : loss : 0.036349, loss_ce: 0.015372
2022-01-06 23:24:16,861 iteration 3708 : loss : 0.029506, loss_ce: 0.011268
2022-01-06 23:24:18,367 iteration 3709 : loss : 0.028371, loss_ce: 0.009109
2022-01-06 23:24:19,911 iteration 3710 : loss : 0.032640, loss_ce: 0.011315
2022-01-06 23:24:21,394 iteration 3711 : loss : 0.023290, loss_ce: 0.008959
2022-01-06 23:24:22,921 iteration 3712 : loss : 0.033276, loss_ce: 0.011152
2022-01-06 23:24:24,390 iteration 3713 : loss : 0.027137, loss_ce: 0.009668
2022-01-06 23:24:25,874 iteration 3714 : loss : 0.019252, loss_ce: 0.007130
2022-01-06 23:24:27,396 iteration 3715 : loss : 0.033189, loss_ce: 0.011632
2022-01-06 23:24:28,866 iteration 3716 : loss : 0.034397, loss_ce: 0.011590
2022-01-06 23:24:30,387 iteration 3717 : loss : 0.045528, loss_ce: 0.020168
2022-01-06 23:24:31,948 iteration 3718 : loss : 0.024968, loss_ce: 0.010763
2022-01-06 23:24:33,415 iteration 3719 : loss : 0.023912, loss_ce: 0.006960
2022-01-06 23:24:34,948 iteration 3720 : loss : 0.025188, loss_ce: 0.012847
2022-01-06 23:24:36,500 iteration 3721 : loss : 0.024828, loss_ce: 0.010372
2022-01-06 23:24:37,988 iteration 3722 : loss : 0.021146, loss_ce: 0.008184
2022-01-06 23:24:39,460 iteration 3723 : loss : 0.026812, loss_ce: 0.009123
 55%|██████████████▊            | 219/400 [1:43:26<1:20:48, 26.78s/it]2022-01-06 23:24:40,955 iteration 3724 : loss : 0.022800, loss_ce: 0.012042
2022-01-06 23:24:42,408 iteration 3725 : loss : 0.029541, loss_ce: 0.014772
2022-01-06 23:24:43,956 iteration 3726 : loss : 0.035258, loss_ce: 0.016901
2022-01-06 23:24:45,460 iteration 3727 : loss : 0.020281, loss_ce: 0.008218
2022-01-06 23:24:46,972 iteration 3728 : loss : 0.034974, loss_ce: 0.009660
2022-01-06 23:24:48,435 iteration 3729 : loss : 0.021914, loss_ce: 0.006180
2022-01-06 23:24:49,937 iteration 3730 : loss : 0.028705, loss_ce: 0.012697
2022-01-06 23:24:51,430 iteration 3731 : loss : 0.021974, loss_ce: 0.007475
2022-01-06 23:24:52,969 iteration 3732 : loss : 0.023133, loss_ce: 0.007541
2022-01-06 23:24:54,445 iteration 3733 : loss : 0.022988, loss_ce: 0.008508
2022-01-06 23:24:55,968 iteration 3734 : loss : 0.026925, loss_ce: 0.012548
2022-01-06 23:24:57,482 iteration 3735 : loss : 0.027631, loss_ce: 0.007449
2022-01-06 23:24:59,036 iteration 3736 : loss : 0.037635, loss_ce: 0.016294
2022-01-06 23:25:00,488 iteration 3737 : loss : 0.027527, loss_ce: 0.008542
2022-01-06 23:25:01,983 iteration 3738 : loss : 0.024382, loss_ce: 0.008403
2022-01-06 23:25:03,455 iteration 3739 : loss : 0.028755, loss_ce: 0.011551
2022-01-06 23:25:03,456 Training Data Eval:
2022-01-06 23:25:11,220   Average segmentation loss on training set: 0.0261
2022-01-06 23:25:11,221 Validation Data Eval:
2022-01-06 23:25:13,926   Average segmentation loss on validation set: 0.0702
2022-01-06 23:25:20,089 Found new lowest validation loss at iteration 3739! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 23:25:21,720 iteration 3740 : loss : 0.031541, loss_ce: 0.014958
 55%|██████████████▊            | 220/400 [1:44:08<1:34:16, 31.42s/it]2022-01-06 23:25:23,324 iteration 3741 : loss : 0.045590, loss_ce: 0.012951
2022-01-06 23:25:24,750 iteration 3742 : loss : 0.028170, loss_ce: 0.013371
2022-01-06 23:25:26,180 iteration 3743 : loss : 0.029336, loss_ce: 0.014065
2022-01-06 23:25:27,658 iteration 3744 : loss : 0.040420, loss_ce: 0.015427
2022-01-06 23:25:29,158 iteration 3745 : loss : 0.038555, loss_ce: 0.015616
2022-01-06 23:25:30,664 iteration 3746 : loss : 0.031528, loss_ce: 0.012039
2022-01-06 23:25:32,091 iteration 3747 : loss : 0.026474, loss_ce: 0.008833
2022-01-06 23:25:33,509 iteration 3748 : loss : 0.024216, loss_ce: 0.012276
2022-01-06 23:25:34,991 iteration 3749 : loss : 0.030764, loss_ce: 0.012420
2022-01-06 23:25:36,506 iteration 3750 : loss : 0.039781, loss_ce: 0.013725
2022-01-06 23:25:37,969 iteration 3751 : loss : 0.043961, loss_ce: 0.012570
2022-01-06 23:25:39,407 iteration 3752 : loss : 0.036658, loss_ce: 0.010087
2022-01-06 23:25:40,856 iteration 3753 : loss : 0.029682, loss_ce: 0.008201
2022-01-06 23:25:42,358 iteration 3754 : loss : 0.027852, loss_ce: 0.009507
2022-01-06 23:25:43,899 iteration 3755 : loss : 0.022173, loss_ce: 0.010186
2022-01-06 23:25:45,327 iteration 3756 : loss : 0.025112, loss_ce: 0.010635
2022-01-06 23:25:46,771 iteration 3757 : loss : 0.025697, loss_ce: 0.011269
 55%|██████████████▉            | 221/400 [1:44:33<1:28:03, 29.52s/it]2022-01-06 23:25:48,403 iteration 3758 : loss : 0.025331, loss_ce: 0.007955
2022-01-06 23:25:49,863 iteration 3759 : loss : 0.028206, loss_ce: 0.009169
2022-01-06 23:25:51,344 iteration 3760 : loss : 0.023410, loss_ce: 0.009411
2022-01-06 23:25:52,876 iteration 3761 : loss : 0.034032, loss_ce: 0.013040
2022-01-06 23:25:54,430 iteration 3762 : loss : 0.022673, loss_ce: 0.010782
2022-01-06 23:25:56,020 iteration 3763 : loss : 0.054114, loss_ce: 0.019972
2022-01-06 23:25:57,530 iteration 3764 : loss : 0.030787, loss_ce: 0.010412
2022-01-06 23:25:59,009 iteration 3765 : loss : 0.027208, loss_ce: 0.011588
2022-01-06 23:26:00,573 iteration 3766 : loss : 0.043477, loss_ce: 0.012577
2022-01-06 23:26:02,077 iteration 3767 : loss : 0.022614, loss_ce: 0.007385
2022-01-06 23:26:03,546 iteration 3768 : loss : 0.027903, loss_ce: 0.009510
2022-01-06 23:26:05,018 iteration 3769 : loss : 0.025573, loss_ce: 0.010346
2022-01-06 23:26:06,557 iteration 3770 : loss : 0.049580, loss_ce: 0.008895
2022-01-06 23:26:08,168 iteration 3771 : loss : 0.031886, loss_ce: 0.011144
2022-01-06 23:26:09,731 iteration 3772 : loss : 0.037558, loss_ce: 0.009609
2022-01-06 23:26:11,250 iteration 3773 : loss : 0.027209, loss_ce: 0.011165
2022-01-06 23:26:12,819 iteration 3774 : loss : 0.067151, loss_ce: 0.038268
 56%|██████████████▉            | 222/400 [1:44:59<1:24:28, 28.47s/it]2022-01-06 23:26:14,354 iteration 3775 : loss : 0.035017, loss_ce: 0.012616
2022-01-06 23:26:15,791 iteration 3776 : loss : 0.030343, loss_ce: 0.012027
2022-01-06 23:26:17,275 iteration 3777 : loss : 0.021638, loss_ce: 0.005772
2022-01-06 23:26:18,736 iteration 3778 : loss : 0.020770, loss_ce: 0.008395
2022-01-06 23:26:20,204 iteration 3779 : loss : 0.036043, loss_ce: 0.019611
2022-01-06 23:26:21,717 iteration 3780 : loss : 0.034699, loss_ce: 0.013349
2022-01-06 23:26:23,223 iteration 3781 : loss : 0.025974, loss_ce: 0.010677
2022-01-06 23:26:24,735 iteration 3782 : loss : 0.087839, loss_ce: 0.023504
2022-01-06 23:26:26,248 iteration 3783 : loss : 0.047225, loss_ce: 0.020126
2022-01-06 23:26:27,779 iteration 3784 : loss : 0.026362, loss_ce: 0.012229
2022-01-06 23:26:29,258 iteration 3785 : loss : 0.026323, loss_ce: 0.010490
2022-01-06 23:26:30,724 iteration 3786 : loss : 0.019495, loss_ce: 0.007527
2022-01-06 23:26:32,177 iteration 3787 : loss : 0.028093, loss_ce: 0.009776
2022-01-06 23:26:33,725 iteration 3788 : loss : 0.047996, loss_ce: 0.017145
2022-01-06 23:26:35,218 iteration 3789 : loss : 0.040200, loss_ce: 0.012902
2022-01-06 23:26:36,762 iteration 3790 : loss : 0.030967, loss_ce: 0.011606
2022-01-06 23:26:38,290 iteration 3791 : loss : 0.027675, loss_ce: 0.011397
 56%|███████████████            | 223/400 [1:45:25<1:21:20, 27.57s/it]2022-01-06 23:26:39,829 iteration 3792 : loss : 0.020917, loss_ce: 0.006913
2022-01-06 23:26:41,375 iteration 3793 : loss : 0.030005, loss_ce: 0.010462
2022-01-06 23:26:42,899 iteration 3794 : loss : 0.029310, loss_ce: 0.010723
2022-01-06 23:26:44,412 iteration 3795 : loss : 0.020261, loss_ce: 0.006633
2022-01-06 23:26:45,834 iteration 3796 : loss : 0.028354, loss_ce: 0.009063
2022-01-06 23:26:47,320 iteration 3797 : loss : 0.022393, loss_ce: 0.009693
2022-01-06 23:26:48,770 iteration 3798 : loss : 0.022164, loss_ce: 0.008400
2022-01-06 23:26:50,265 iteration 3799 : loss : 0.036778, loss_ce: 0.013389
2022-01-06 23:26:51,752 iteration 3800 : loss : 0.024491, loss_ce: 0.007396
2022-01-06 23:26:53,234 iteration 3801 : loss : 0.051204, loss_ce: 0.012799
2022-01-06 23:26:54,704 iteration 3802 : loss : 0.040580, loss_ce: 0.021288
2022-01-06 23:26:56,207 iteration 3803 : loss : 0.024892, loss_ce: 0.007811
2022-01-06 23:26:57,700 iteration 3804 : loss : 0.040141, loss_ce: 0.013922
2022-01-06 23:26:59,170 iteration 3805 : loss : 0.029639, loss_ce: 0.011504
2022-01-06 23:27:00,647 iteration 3806 : loss : 0.037987, loss_ce: 0.020199
2022-01-06 23:27:02,183 iteration 3807 : loss : 0.031080, loss_ce: 0.010741
2022-01-06 23:27:03,688 iteration 3808 : loss : 0.026214, loss_ce: 0.011455
 56%|███████████████            | 224/400 [1:45:50<1:18:57, 26.92s/it]2022-01-06 23:27:05,182 iteration 3809 : loss : 0.026969, loss_ce: 0.007947
2022-01-06 23:27:06,663 iteration 3810 : loss : 0.026787, loss_ce: 0.011246
2022-01-06 23:27:08,159 iteration 3811 : loss : 0.022791, loss_ce: 0.006413
2022-01-06 23:27:09,662 iteration 3812 : loss : 0.036291, loss_ce: 0.017721
2022-01-06 23:27:11,145 iteration 3813 : loss : 0.027548, loss_ce: 0.012965
2022-01-06 23:27:12,663 iteration 3814 : loss : 0.030248, loss_ce: 0.009810
2022-01-06 23:27:14,164 iteration 3815 : loss : 0.027263, loss_ce: 0.009642
2022-01-06 23:27:15,685 iteration 3816 : loss : 0.026566, loss_ce: 0.010627
2022-01-06 23:27:17,148 iteration 3817 : loss : 0.036083, loss_ce: 0.014275
2022-01-06 23:27:18,683 iteration 3818 : loss : 0.024123, loss_ce: 0.010809
2022-01-06 23:27:20,227 iteration 3819 : loss : 0.023701, loss_ce: 0.011012
2022-01-06 23:27:21,751 iteration 3820 : loss : 0.026928, loss_ce: 0.009889
2022-01-06 23:27:23,284 iteration 3821 : loss : 0.026684, loss_ce: 0.011245
2022-01-06 23:27:24,756 iteration 3822 : loss : 0.040615, loss_ce: 0.014400
2022-01-06 23:27:26,328 iteration 3823 : loss : 0.026927, loss_ce: 0.008557
2022-01-06 23:27:27,847 iteration 3824 : loss : 0.028881, loss_ce: 0.011120
2022-01-06 23:27:27,847 Training Data Eval:
2022-01-06 23:27:35,656   Average segmentation loss on training set: 0.0176
2022-01-06 23:27:35,656 Validation Data Eval:
2022-01-06 23:27:38,389   Average segmentation loss on validation set: 0.0735
2022-01-06 23:27:39,987 iteration 3825 : loss : 0.042544, loss_ce: 0.013667
 56%|███████████████▏           | 225/400 [1:46:26<1:26:43, 29.73s/it]2022-01-06 23:27:41,586 iteration 3826 : loss : 0.025684, loss_ce: 0.011453
2022-01-06 23:27:43,097 iteration 3827 : loss : 0.022039, loss_ce: 0.008422
2022-01-06 23:27:44,664 iteration 3828 : loss : 0.025893, loss_ce: 0.010364
2022-01-06 23:27:46,223 iteration 3829 : loss : 0.031433, loss_ce: 0.008416
2022-01-06 23:27:47,753 iteration 3830 : loss : 0.018484, loss_ce: 0.006636
2022-01-06 23:27:49,270 iteration 3831 : loss : 0.031433, loss_ce: 0.010930
2022-01-06 23:27:50,776 iteration 3832 : loss : 0.025576, loss_ce: 0.006628
2022-01-06 23:27:52,323 iteration 3833 : loss : 0.022174, loss_ce: 0.009806
2022-01-06 23:27:53,903 iteration 3834 : loss : 0.035357, loss_ce: 0.016529
2022-01-06 23:27:55,463 iteration 3835 : loss : 0.037661, loss_ce: 0.012437
2022-01-06 23:27:56,994 iteration 3836 : loss : 0.024216, loss_ce: 0.009315
2022-01-06 23:27:58,550 iteration 3837 : loss : 0.034661, loss_ce: 0.012864
2022-01-06 23:28:00,154 iteration 3838 : loss : 0.031383, loss_ce: 0.011806
2022-01-06 23:28:01,719 iteration 3839 : loss : 0.021175, loss_ce: 0.008921
2022-01-06 23:28:03,260 iteration 3840 : loss : 0.030780, loss_ce: 0.008398
2022-01-06 23:28:04,752 iteration 3841 : loss : 0.026847, loss_ce: 0.009441
2022-01-06 23:28:06,298 iteration 3842 : loss : 0.037513, loss_ce: 0.017842
 56%|███████████████▎           | 226/400 [1:46:53<1:23:15, 28.71s/it]2022-01-06 23:28:07,847 iteration 3843 : loss : 0.024557, loss_ce: 0.009376
2022-01-06 23:28:09,403 iteration 3844 : loss : 0.029245, loss_ce: 0.008179
2022-01-06 23:28:10,948 iteration 3845 : loss : 0.023549, loss_ce: 0.011044
2022-01-06 23:28:12,432 iteration 3846 : loss : 0.018919, loss_ce: 0.007395
2022-01-06 23:28:13,946 iteration 3847 : loss : 0.026380, loss_ce: 0.010175
2022-01-06 23:28:15,508 iteration 3848 : loss : 0.036374, loss_ce: 0.019729
2022-01-06 23:28:17,116 iteration 3849 : loss : 0.027186, loss_ce: 0.012606
2022-01-06 23:28:18,698 iteration 3850 : loss : 0.050028, loss_ce: 0.011620
2022-01-06 23:28:20,219 iteration 3851 : loss : 0.028125, loss_ce: 0.012971
2022-01-06 23:28:21,778 iteration 3852 : loss : 0.022034, loss_ce: 0.006485
2022-01-06 23:28:23,284 iteration 3853 : loss : 0.019167, loss_ce: 0.006149
2022-01-06 23:28:24,875 iteration 3854 : loss : 0.027619, loss_ce: 0.009350
2022-01-06 23:28:26,566 iteration 3855 : loss : 0.029484, loss_ce: 0.012425
2022-01-06 23:28:28,024 iteration 3856 : loss : 0.022798, loss_ce: 0.010242
2022-01-06 23:28:29,569 iteration 3857 : loss : 0.024139, loss_ce: 0.010020
2022-01-06 23:28:31,045 iteration 3858 : loss : 0.020863, loss_ce: 0.009106
2022-01-06 23:28:32,585 iteration 3859 : loss : 0.030558, loss_ce: 0.016005
 57%|███████████████▎           | 227/400 [1:47:19<1:20:40, 27.98s/it]2022-01-06 23:28:34,193 iteration 3860 : loss : 0.022940, loss_ce: 0.009376
2022-01-06 23:28:35,810 iteration 3861 : loss : 0.034060, loss_ce: 0.015014
2022-01-06 23:28:37,372 iteration 3862 : loss : 0.029473, loss_ce: 0.010639
2022-01-06 23:28:38,930 iteration 3863 : loss : 0.022465, loss_ce: 0.007674
2022-01-06 23:28:40,512 iteration 3864 : loss : 0.028802, loss_ce: 0.012677
2022-01-06 23:28:42,038 iteration 3865 : loss : 0.035214, loss_ce: 0.011189
2022-01-06 23:28:43,674 iteration 3866 : loss : 0.027679, loss_ce: 0.008714
2022-01-06 23:28:45,191 iteration 3867 : loss : 0.029827, loss_ce: 0.009923
2022-01-06 23:28:46,770 iteration 3868 : loss : 0.087968, loss_ce: 0.025004
2022-01-06 23:28:48,345 iteration 3869 : loss : 0.023731, loss_ce: 0.010387
2022-01-06 23:28:49,908 iteration 3870 : loss : 0.023979, loss_ce: 0.009757
2022-01-06 23:28:51,447 iteration 3871 : loss : 0.029760, loss_ce: 0.008377
2022-01-06 23:28:52,928 iteration 3872 : loss : 0.022904, loss_ce: 0.009442
2022-01-06 23:28:54,409 iteration 3873 : loss : 0.026414, loss_ce: 0.013240
2022-01-06 23:28:55,947 iteration 3874 : loss : 0.023789, loss_ce: 0.010033
2022-01-06 23:28:57,474 iteration 3875 : loss : 0.026864, loss_ce: 0.010443
2022-01-06 23:28:58,998 iteration 3876 : loss : 0.036285, loss_ce: 0.011706
 57%|███████████████▍           | 228/400 [1:47:45<1:18:52, 27.51s/it]2022-01-06 23:29:00,537 iteration 3877 : loss : 0.025780, loss_ce: 0.009100
2022-01-06 23:29:02,071 iteration 3878 : loss : 0.028941, loss_ce: 0.010066
2022-01-06 23:29:03,565 iteration 3879 : loss : 0.020704, loss_ce: 0.008120
2022-01-06 23:29:05,107 iteration 3880 : loss : 0.035598, loss_ce: 0.015452
2022-01-06 23:29:06,573 iteration 3881 : loss : 0.032164, loss_ce: 0.015074
2022-01-06 23:29:08,159 iteration 3882 : loss : 0.049138, loss_ce: 0.011505
2022-01-06 23:29:09,722 iteration 3883 : loss : 0.044879, loss_ce: 0.016618
2022-01-06 23:29:11,252 iteration 3884 : loss : 0.030510, loss_ce: 0.014371
2022-01-06 23:29:12,808 iteration 3885 : loss : 0.096901, loss_ce: 0.017971
2022-01-06 23:29:14,448 iteration 3886 : loss : 0.026180, loss_ce: 0.009580
2022-01-06 23:29:15,900 iteration 3887 : loss : 0.022667, loss_ce: 0.007924
2022-01-06 23:29:17,440 iteration 3888 : loss : 0.025014, loss_ce: 0.011329
2022-01-06 23:29:18,956 iteration 3889 : loss : 0.019547, loss_ce: 0.007275
2022-01-06 23:29:20,562 iteration 3890 : loss : 0.032043, loss_ce: 0.010805
2022-01-06 23:29:22,039 iteration 3891 : loss : 0.032566, loss_ce: 0.012302
2022-01-06 23:29:23,561 iteration 3892 : loss : 0.023722, loss_ce: 0.010166
2022-01-06 23:29:25,097 iteration 3893 : loss : 0.055264, loss_ce: 0.018653
 57%|███████████████▍           | 229/400 [1:48:11<1:17:11, 27.09s/it]2022-01-06 23:29:26,629 iteration 3894 : loss : 0.029452, loss_ce: 0.009029
2022-01-06 23:29:28,085 iteration 3895 : loss : 0.023310, loss_ce: 0.008866
2022-01-06 23:29:29,598 iteration 3896 : loss : 0.034982, loss_ce: 0.014700
2022-01-06 23:29:31,145 iteration 3897 : loss : 0.034223, loss_ce: 0.014580
2022-01-06 23:29:32,679 iteration 3898 : loss : 0.029829, loss_ce: 0.010712
2022-01-06 23:29:34,129 iteration 3899 : loss : 0.024988, loss_ce: 0.009334
2022-01-06 23:29:35,609 iteration 3900 : loss : 0.025119, loss_ce: 0.009796
2022-01-06 23:29:37,079 iteration 3901 : loss : 0.029924, loss_ce: 0.012370
2022-01-06 23:29:38,556 iteration 3902 : loss : 0.020694, loss_ce: 0.007343
2022-01-06 23:29:40,015 iteration 3903 : loss : 0.030499, loss_ce: 0.009302
2022-01-06 23:29:41,487 iteration 3904 : loss : 0.028097, loss_ce: 0.009465
2022-01-06 23:29:42,975 iteration 3905 : loss : 0.029233, loss_ce: 0.010517
2022-01-06 23:29:44,407 iteration 3906 : loss : 0.017538, loss_ce: 0.007321
2022-01-06 23:29:45,829 iteration 3907 : loss : 0.020407, loss_ce: 0.007167
2022-01-06 23:29:47,273 iteration 3908 : loss : 0.021095, loss_ce: 0.007751
2022-01-06 23:29:48,768 iteration 3909 : loss : 0.034047, loss_ce: 0.020715
2022-01-06 23:29:48,768 Training Data Eval:
2022-01-06 23:29:56,518   Average segmentation loss on training set: 0.0214
2022-01-06 23:29:56,518 Validation Data Eval:
2022-01-06 23:29:59,190   Average segmentation loss on validation set: 0.0707
2022-01-06 23:30:00,766 iteration 3910 : loss : 0.031059, loss_ce: 0.011170
 57%|███████████████▌           | 230/400 [1:48:47<1:24:02, 29.66s/it]2022-01-06 23:30:02,298 iteration 3911 : loss : 0.025641, loss_ce: 0.010462
2022-01-06 23:30:03,836 iteration 3912 : loss : 0.029939, loss_ce: 0.011422
2022-01-06 23:30:05,339 iteration 3913 : loss : 0.021257, loss_ce: 0.009416
2022-01-06 23:30:06,826 iteration 3914 : loss : 0.016929, loss_ce: 0.008008
2022-01-06 23:30:08,311 iteration 3915 : loss : 0.027091, loss_ce: 0.011423
2022-01-06 23:30:09,775 iteration 3916 : loss : 0.018812, loss_ce: 0.006585
2022-01-06 23:30:11,309 iteration 3917 : loss : 0.026667, loss_ce: 0.009777
2022-01-06 23:30:12,834 iteration 3918 : loss : 0.030678, loss_ce: 0.010544
2022-01-06 23:30:14,447 iteration 3919 : loss : 0.038060, loss_ce: 0.012153
2022-01-06 23:30:15,899 iteration 3920 : loss : 0.022186, loss_ce: 0.008877
2022-01-06 23:30:17,397 iteration 3921 : loss : 0.021830, loss_ce: 0.008533
2022-01-06 23:30:18,923 iteration 3922 : loss : 0.024256, loss_ce: 0.009085
2022-01-06 23:30:20,401 iteration 3923 : loss : 0.024809, loss_ce: 0.014547
2022-01-06 23:30:21,858 iteration 3924 : loss : 0.023204, loss_ce: 0.006107
2022-01-06 23:30:23,427 iteration 3925 : loss : 0.029026, loss_ce: 0.011772
2022-01-06 23:30:24,964 iteration 3926 : loss : 0.025221, loss_ce: 0.009828
2022-01-06 23:30:26,449 iteration 3927 : loss : 0.023500, loss_ce: 0.010155
 58%|███████████████▌           | 231/400 [1:49:13<1:20:11, 28.47s/it]2022-01-06 23:30:28,056 iteration 3928 : loss : 0.037092, loss_ce: 0.017132
2022-01-06 23:30:29,578 iteration 3929 : loss : 0.040114, loss_ce: 0.020593
2022-01-06 23:30:31,126 iteration 3930 : loss : 0.031994, loss_ce: 0.013842
2022-01-06 23:30:32,672 iteration 3931 : loss : 0.024568, loss_ce: 0.011395
2022-01-06 23:30:34,205 iteration 3932 : loss : 0.038109, loss_ce: 0.011763
2022-01-06 23:30:35,776 iteration 3933 : loss : 0.020067, loss_ce: 0.007830
2022-01-06 23:30:37,302 iteration 3934 : loss : 0.031756, loss_ce: 0.017668
2022-01-06 23:30:38,743 iteration 3935 : loss : 0.022788, loss_ce: 0.009237
2022-01-06 23:30:40,363 iteration 3936 : loss : 0.040506, loss_ce: 0.015540
2022-01-06 23:30:41,831 iteration 3937 : loss : 0.026530, loss_ce: 0.011029
2022-01-06 23:30:43,379 iteration 3938 : loss : 0.062716, loss_ce: 0.011812
2022-01-06 23:30:44,814 iteration 3939 : loss : 0.029746, loss_ce: 0.008699
2022-01-06 23:30:46,296 iteration 3940 : loss : 0.027065, loss_ce: 0.010761
2022-01-06 23:30:47,796 iteration 3941 : loss : 0.035972, loss_ce: 0.015398
2022-01-06 23:30:49,343 iteration 3942 : loss : 0.038101, loss_ce: 0.015359
2022-01-06 23:30:50,803 iteration 3943 : loss : 0.037276, loss_ce: 0.014251
2022-01-06 23:30:52,358 iteration 3944 : loss : 0.037135, loss_ce: 0.014335
 58%|███████████████▋           | 232/400 [1:49:39<1:17:33, 27.70s/it]2022-01-06 23:30:53,924 iteration 3945 : loss : 0.031683, loss_ce: 0.013391
2022-01-06 23:30:55,340 iteration 3946 : loss : 0.018706, loss_ce: 0.009032
2022-01-06 23:30:56,838 iteration 3947 : loss : 0.019128, loss_ce: 0.005718
2022-01-06 23:30:58,452 iteration 3948 : loss : 0.028124, loss_ce: 0.008987
2022-01-06 23:30:59,990 iteration 3949 : loss : 0.039967, loss_ce: 0.015160
2022-01-06 23:31:01,561 iteration 3950 : loss : 0.029230, loss_ce: 0.008822
2022-01-06 23:31:03,012 iteration 3951 : loss : 0.022508, loss_ce: 0.010070
2022-01-06 23:31:04,569 iteration 3952 : loss : 0.025086, loss_ce: 0.009244
2022-01-06 23:31:06,144 iteration 3953 : loss : 0.038012, loss_ce: 0.012905
2022-01-06 23:31:07,653 iteration 3954 : loss : 0.025846, loss_ce: 0.008762
2022-01-06 23:31:09,166 iteration 3955 : loss : 0.023745, loss_ce: 0.009019
2022-01-06 23:31:10,805 iteration 3956 : loss : 0.033463, loss_ce: 0.011985
2022-01-06 23:31:12,294 iteration 3957 : loss : 0.027031, loss_ce: 0.012880
2022-01-06 23:31:13,796 iteration 3958 : loss : 0.025000, loss_ce: 0.010216
2022-01-06 23:31:15,396 iteration 3959 : loss : 0.031310, loss_ce: 0.009898
2022-01-06 23:31:16,894 iteration 3960 : loss : 0.021961, loss_ce: 0.011214
2022-01-06 23:31:18,406 iteration 3961 : loss : 0.025398, loss_ce: 0.010186
 58%|███████████████▋           | 233/400 [1:50:05<1:15:42, 27.20s/it]2022-01-06 23:31:20,001 iteration 3962 : loss : 0.026422, loss_ce: 0.009163
2022-01-06 23:31:21,514 iteration 3963 : loss : 0.022569, loss_ce: 0.010758
2022-01-06 23:31:23,140 iteration 3964 : loss : 0.025371, loss_ce: 0.010835
2022-01-06 23:31:24,650 iteration 3965 : loss : 0.025143, loss_ce: 0.007850
2022-01-06 23:31:26,260 iteration 3966 : loss : 0.031750, loss_ce: 0.012136
2022-01-06 23:31:27,828 iteration 3967 : loss : 0.023754, loss_ce: 0.008234
2022-01-06 23:31:29,303 iteration 3968 : loss : 0.021502, loss_ce: 0.007759
2022-01-06 23:31:30,877 iteration 3969 : loss : 0.029988, loss_ce: 0.009167
2022-01-06 23:31:32,432 iteration 3970 : loss : 0.029326, loss_ce: 0.009989
2022-01-06 23:31:33,900 iteration 3971 : loss : 0.022761, loss_ce: 0.009396
2022-01-06 23:31:35,489 iteration 3972 : loss : 0.030291, loss_ce: 0.008172
2022-01-06 23:31:37,011 iteration 3973 : loss : 0.053484, loss_ce: 0.029269
2022-01-06 23:31:38,498 iteration 3974 : loss : 0.018405, loss_ce: 0.005736
2022-01-06 23:31:40,075 iteration 3975 : loss : 0.021818, loss_ce: 0.009238
2022-01-06 23:31:41,699 iteration 3976 : loss : 0.026262, loss_ce: 0.009800
2022-01-06 23:31:43,319 iteration 3977 : loss : 0.026553, loss_ce: 0.012859
2022-01-06 23:31:44,947 iteration 3978 : loss : 0.039798, loss_ce: 0.014435
 58%|███████████████▊           | 234/400 [1:50:31<1:14:43, 27.01s/it]2022-01-06 23:31:46,561 iteration 3979 : loss : 0.031452, loss_ce: 0.012584
2022-01-06 23:31:48,156 iteration 3980 : loss : 0.040960, loss_ce: 0.012113
2022-01-06 23:31:49,695 iteration 3981 : loss : 0.032118, loss_ce: 0.013376
2022-01-06 23:31:51,157 iteration 3982 : loss : 0.022640, loss_ce: 0.008198
2022-01-06 23:31:52,623 iteration 3983 : loss : 0.019894, loss_ce: 0.008981
2022-01-06 23:31:54,124 iteration 3984 : loss : 0.020111, loss_ce: 0.008473
2022-01-06 23:31:55,677 iteration 3985 : loss : 0.042899, loss_ce: 0.026643
2022-01-06 23:31:57,292 iteration 3986 : loss : 0.025038, loss_ce: 0.010593
2022-01-06 23:31:58,861 iteration 3987 : loss : 0.032687, loss_ce: 0.013222
2022-01-06 23:32:00,400 iteration 3988 : loss : 0.029886, loss_ce: 0.014584
2022-01-06 23:32:01,914 iteration 3989 : loss : 0.068803, loss_ce: 0.026786
2022-01-06 23:32:03,422 iteration 3990 : loss : 0.031362, loss_ce: 0.012455
2022-01-06 23:32:05,009 iteration 3991 : loss : 0.035793, loss_ce: 0.009197
2022-01-06 23:32:06,538 iteration 3992 : loss : 0.034967, loss_ce: 0.011765
2022-01-06 23:32:08,107 iteration 3993 : loss : 0.046730, loss_ce: 0.019949
2022-01-06 23:32:09,611 iteration 3994 : loss : 0.037135, loss_ce: 0.019874
2022-01-06 23:32:09,611 Training Data Eval:
2022-01-06 23:32:17,453   Average segmentation loss on training set: 0.3314
2022-01-06 23:32:17,453 Validation Data Eval:
2022-01-06 23:32:20,158   Average segmentation loss on validation set: 0.5349
2022-01-06 23:32:21,631 iteration 3995 : loss : 0.046604, loss_ce: 0.026606
 59%|███████████████▊           | 235/400 [1:51:08<1:22:15, 29.91s/it]2022-01-06 23:32:23,262 iteration 3996 : loss : 0.028334, loss_ce: 0.010139
2022-01-06 23:32:24,724 iteration 3997 : loss : 0.037880, loss_ce: 0.014777
2022-01-06 23:32:26,268 iteration 3998 : loss : 0.039516, loss_ce: 0.020868
2022-01-06 23:32:27,774 iteration 3999 : loss : 0.038654, loss_ce: 0.014531
2022-01-06 23:32:29,328 iteration 4000 : loss : 0.029000, loss_ce: 0.015549
2022-01-06 23:32:30,780 iteration 4001 : loss : 0.025345, loss_ce: 0.011531
2022-01-06 23:32:32,305 iteration 4002 : loss : 0.023213, loss_ce: 0.010334
2022-01-06 23:32:33,821 iteration 4003 : loss : 0.028572, loss_ce: 0.012110
2022-01-06 23:32:35,352 iteration 4004 : loss : 0.050728, loss_ce: 0.013072
2022-01-06 23:32:36,857 iteration 4005 : loss : 0.040237, loss_ce: 0.019237
2022-01-06 23:32:38,315 iteration 4006 : loss : 0.032674, loss_ce: 0.015304
2022-01-06 23:32:39,860 iteration 4007 : loss : 0.028301, loss_ce: 0.010065
2022-01-06 23:32:41,363 iteration 4008 : loss : 0.023365, loss_ce: 0.010268
2022-01-06 23:32:42,869 iteration 4009 : loss : 0.032424, loss_ce: 0.014550
2022-01-06 23:32:44,446 iteration 4010 : loss : 0.043630, loss_ce: 0.019751
2022-01-06 23:32:45,917 iteration 4011 : loss : 0.025131, loss_ce: 0.007760
2022-01-06 23:32:47,502 iteration 4012 : loss : 0.033462, loss_ce: 0.015889
 59%|███████████████▉           | 236/400 [1:51:34<1:18:26, 28.70s/it]2022-01-06 23:32:49,131 iteration 4013 : loss : 0.050273, loss_ce: 0.018867
2022-01-06 23:32:50,589 iteration 4014 : loss : 0.021801, loss_ce: 0.007391
2022-01-06 23:32:52,143 iteration 4015 : loss : 0.039707, loss_ce: 0.016043
2022-01-06 23:32:53,673 iteration 4016 : loss : 0.025123, loss_ce: 0.009857
2022-01-06 23:32:55,234 iteration 4017 : loss : 0.035925, loss_ce: 0.010946
2022-01-06 23:32:56,711 iteration 4018 : loss : 0.024842, loss_ce: 0.009334
2022-01-06 23:32:58,214 iteration 4019 : loss : 0.024388, loss_ce: 0.012258
2022-01-06 23:32:59,682 iteration 4020 : loss : 0.023248, loss_ce: 0.013563
2022-01-06 23:33:01,168 iteration 4021 : loss : 0.030409, loss_ce: 0.014208
2022-01-06 23:33:02,713 iteration 4022 : loss : 0.033951, loss_ce: 0.016204
2022-01-06 23:33:04,217 iteration 4023 : loss : 0.019978, loss_ce: 0.007056
2022-01-06 23:33:05,667 iteration 4024 : loss : 0.021504, loss_ce: 0.007265
2022-01-06 23:33:07,141 iteration 4025 : loss : 0.019270, loss_ce: 0.006826
2022-01-06 23:33:08,683 iteration 4026 : loss : 0.027065, loss_ce: 0.011889
2022-01-06 23:33:10,222 iteration 4027 : loss : 0.030703, loss_ce: 0.010384
2022-01-06 23:33:11,782 iteration 4028 : loss : 0.046252, loss_ce: 0.012213
2022-01-06 23:33:13,243 iteration 4029 : loss : 0.020830, loss_ce: 0.009752
 59%|███████████████▉           | 237/400 [1:52:00<1:15:36, 27.83s/it]2022-01-06 23:33:14,919 iteration 4030 : loss : 0.027173, loss_ce: 0.011378
2022-01-06 23:33:16,446 iteration 4031 : loss : 0.036854, loss_ce: 0.012835
2022-01-06 23:33:17,941 iteration 4032 : loss : 0.030484, loss_ce: 0.014681
2022-01-06 23:33:19,439 iteration 4033 : loss : 0.035177, loss_ce: 0.012659
2022-01-06 23:33:20,885 iteration 4034 : loss : 0.028977, loss_ce: 0.010101
2022-01-06 23:33:22,365 iteration 4035 : loss : 0.023041, loss_ce: 0.007165
2022-01-06 23:33:23,896 iteration 4036 : loss : 0.035606, loss_ce: 0.015042
2022-01-06 23:33:25,359 iteration 4037 : loss : 0.019870, loss_ce: 0.009193
2022-01-06 23:33:26,926 iteration 4038 : loss : 0.028120, loss_ce: 0.010599
2022-01-06 23:33:28,504 iteration 4039 : loss : 0.042231, loss_ce: 0.014174
2022-01-06 23:33:30,082 iteration 4040 : loss : 0.042631, loss_ce: 0.018145
2022-01-06 23:33:31,635 iteration 4041 : loss : 0.024618, loss_ce: 0.008050
2022-01-06 23:33:33,108 iteration 4042 : loss : 0.032599, loss_ce: 0.010203
2022-01-06 23:33:34,610 iteration 4043 : loss : 0.182568, loss_ce: 0.010571
2022-01-06 23:33:36,068 iteration 4044 : loss : 0.025826, loss_ce: 0.010275
2022-01-06 23:33:37,549 iteration 4045 : loss : 0.029375, loss_ce: 0.015116
2022-01-06 23:33:39,102 iteration 4046 : loss : 0.036043, loss_ce: 0.017843
 60%|████████████████           | 238/400 [1:52:25<1:13:29, 27.22s/it]2022-01-06 23:33:40,745 iteration 4047 : loss : 0.021753, loss_ce: 0.009930
2022-01-06 23:33:42,305 iteration 4048 : loss : 0.029584, loss_ce: 0.011643
2022-01-06 23:33:43,808 iteration 4049 : loss : 0.020336, loss_ce: 0.007951
2022-01-06 23:33:45,375 iteration 4050 : loss : 0.026454, loss_ce: 0.010419
2022-01-06 23:33:46,996 iteration 4051 : loss : 0.055457, loss_ce: 0.012114
2022-01-06 23:33:48,461 iteration 4052 : loss : 0.028123, loss_ce: 0.010461
2022-01-06 23:33:49,904 iteration 4053 : loss : 0.022906, loss_ce: 0.008165
2022-01-06 23:33:51,391 iteration 4054 : loss : 0.035450, loss_ce: 0.010034
2022-01-06 23:33:52,923 iteration 4055 : loss : 0.022038, loss_ce: 0.009370
2022-01-06 23:33:54,480 iteration 4056 : loss : 0.037728, loss_ce: 0.020959
2022-01-06 23:33:55,995 iteration 4057 : loss : 0.024469, loss_ce: 0.007202
2022-01-06 23:33:57,530 iteration 4058 : loss : 0.029928, loss_ce: 0.011129
2022-01-06 23:33:59,169 iteration 4059 : loss : 0.037754, loss_ce: 0.018415
2022-01-06 23:34:00,720 iteration 4060 : loss : 0.023224, loss_ce: 0.010372
2022-01-06 23:34:02,260 iteration 4061 : loss : 0.030475, loss_ce: 0.013245
2022-01-06 23:34:03,831 iteration 4062 : loss : 0.034512, loss_ce: 0.013975
2022-01-06 23:34:05,383 iteration 4063 : loss : 0.037339, loss_ce: 0.013754
 60%|████████████████▏          | 239/400 [1:52:52<1:12:16, 26.93s/it]2022-01-06 23:34:06,962 iteration 4064 : loss : 0.041272, loss_ce: 0.013161
2022-01-06 23:34:08,486 iteration 4065 : loss : 0.018034, loss_ce: 0.006162
2022-01-06 23:34:10,109 iteration 4066 : loss : 0.031230, loss_ce: 0.012923
2022-01-06 23:34:11,644 iteration 4067 : loss : 0.019773, loss_ce: 0.007936
2022-01-06 23:34:13,206 iteration 4068 : loss : 0.033708, loss_ce: 0.013649
2022-01-06 23:34:14,792 iteration 4069 : loss : 0.023050, loss_ce: 0.009573
2022-01-06 23:34:16,373 iteration 4070 : loss : 0.024705, loss_ce: 0.010084
2022-01-06 23:34:17,991 iteration 4071 : loss : 0.041432, loss_ce: 0.015124
2022-01-06 23:34:19,556 iteration 4072 : loss : 0.020053, loss_ce: 0.006463
2022-01-06 23:34:21,094 iteration 4073 : loss : 0.024164, loss_ce: 0.008317
2022-01-06 23:34:22,660 iteration 4074 : loss : 0.021981, loss_ce: 0.009826
2022-01-06 23:34:24,271 iteration 4075 : loss : 0.032058, loss_ce: 0.013289
2022-01-06 23:34:25,800 iteration 4076 : loss : 0.027261, loss_ce: 0.010924
2022-01-06 23:34:27,361 iteration 4077 : loss : 0.019149, loss_ce: 0.007064
2022-01-06 23:34:28,983 iteration 4078 : loss : 0.053280, loss_ce: 0.018985
2022-01-06 23:34:30,586 iteration 4079 : loss : 0.027111, loss_ce: 0.012733
2022-01-06 23:34:30,587 Training Data Eval:
2022-01-06 23:34:38,643   Average segmentation loss on training set: 0.0267
2022-01-06 23:34:38,644 Validation Data Eval:
2022-01-06 23:34:41,418   Average segmentation loss on validation set: 0.1179
2022-01-06 23:34:43,024 iteration 4080 : loss : 0.021164, loss_ce: 0.009571
 60%|████████████████▏          | 240/400 [1:53:29<1:20:24, 30.15s/it]2022-01-06 23:34:44,667 iteration 4081 : loss : 0.032238, loss_ce: 0.011036
2022-01-06 23:34:46,232 iteration 4082 : loss : 0.021357, loss_ce: 0.009005
2022-01-06 23:34:47,746 iteration 4083 : loss : 0.017825, loss_ce: 0.007181
2022-01-06 23:34:49,289 iteration 4084 : loss : 0.030450, loss_ce: 0.010243
2022-01-06 23:34:50,795 iteration 4085 : loss : 0.031627, loss_ce: 0.009836
2022-01-06 23:34:52,464 iteration 4086 : loss : 0.030697, loss_ce: 0.010509
2022-01-06 23:34:54,030 iteration 4087 : loss : 0.032008, loss_ce: 0.007210
2022-01-06 23:34:55,514 iteration 4088 : loss : 0.024454, loss_ce: 0.012055
2022-01-06 23:34:57,091 iteration 4089 : loss : 0.029644, loss_ce: 0.009984
2022-01-06 23:34:58,539 iteration 4090 : loss : 0.018316, loss_ce: 0.005782
2022-01-06 23:35:00,069 iteration 4091 : loss : 0.033908, loss_ce: 0.013610
2022-01-06 23:35:01,602 iteration 4092 : loss : 0.021885, loss_ce: 0.008083
2022-01-06 23:35:03,148 iteration 4093 : loss : 0.040056, loss_ce: 0.017351
2022-01-06 23:35:04,647 iteration 4094 : loss : 0.020057, loss_ce: 0.007586
2022-01-06 23:35:06,185 iteration 4095 : loss : 0.028656, loss_ce: 0.012889
2022-01-06 23:35:07,675 iteration 4096 : loss : 0.021211, loss_ce: 0.011717
2022-01-06 23:35:09,178 iteration 4097 : loss : 0.027293, loss_ce: 0.008779
 60%|████████████████▎          | 241/400 [1:53:55<1:16:43, 28.95s/it]2022-01-06 23:35:10,729 iteration 4098 : loss : 0.023832, loss_ce: 0.008715
2022-01-06 23:35:12,211 iteration 4099 : loss : 0.051071, loss_ce: 0.020403
2022-01-06 23:35:13,735 iteration 4100 : loss : 0.019082, loss_ce: 0.006168
2022-01-06 23:35:15,295 iteration 4101 : loss : 0.025892, loss_ce: 0.009502
2022-01-06 23:35:16,896 iteration 4102 : loss : 0.040171, loss_ce: 0.017842
2022-01-06 23:35:18,425 iteration 4103 : loss : 0.029703, loss_ce: 0.013706
2022-01-06 23:35:19,926 iteration 4104 : loss : 0.026285, loss_ce: 0.012247
2022-01-06 23:35:21,441 iteration 4105 : loss : 0.021626, loss_ce: 0.009617
2022-01-06 23:35:22,930 iteration 4106 : loss : 0.033279, loss_ce: 0.016713
2022-01-06 23:35:24,425 iteration 4107 : loss : 0.016186, loss_ce: 0.006275
2022-01-06 23:35:25,934 iteration 4108 : loss : 0.020371, loss_ce: 0.008120
2022-01-06 23:35:27,390 iteration 4109 : loss : 0.019017, loss_ce: 0.006941
2022-01-06 23:35:28,927 iteration 4110 : loss : 0.025012, loss_ce: 0.010479
2022-01-06 23:35:30,568 iteration 4111 : loss : 0.034845, loss_ce: 0.019075
2022-01-06 23:35:32,042 iteration 4112 : loss : 0.029280, loss_ce: 0.010223
2022-01-06 23:35:33,588 iteration 4113 : loss : 0.040959, loss_ce: 0.013406
2022-01-06 23:35:35,086 iteration 4114 : loss : 0.022801, loss_ce: 0.009279
 60%|████████████████▎          | 242/400 [1:54:21<1:13:49, 28.04s/it]2022-01-06 23:35:36,614 iteration 4115 : loss : 0.022441, loss_ce: 0.011878
2022-01-06 23:35:38,164 iteration 4116 : loss : 0.020640, loss_ce: 0.006930
2022-01-06 23:35:39,623 iteration 4117 : loss : 0.024506, loss_ce: 0.011345
2022-01-06 23:35:41,193 iteration 4118 : loss : 0.023123, loss_ce: 0.009868
2022-01-06 23:35:42,818 iteration 4119 : loss : 0.035301, loss_ce: 0.010468
2022-01-06 23:35:44,363 iteration 4120 : loss : 0.022527, loss_ce: 0.006664
2022-01-06 23:35:45,849 iteration 4121 : loss : 0.024547, loss_ce: 0.008136
2022-01-06 23:35:47,407 iteration 4122 : loss : 0.028513, loss_ce: 0.008757
2022-01-06 23:35:48,902 iteration 4123 : loss : 0.014382, loss_ce: 0.006387
2022-01-06 23:35:50,486 iteration 4124 : loss : 0.029397, loss_ce: 0.013355
2022-01-06 23:35:52,053 iteration 4125 : loss : 0.026522, loss_ce: 0.014017
2022-01-06 23:35:53,613 iteration 4126 : loss : 0.023346, loss_ce: 0.010194
2022-01-06 23:35:55,122 iteration 4127 : loss : 0.018160, loss_ce: 0.009349
2022-01-06 23:35:56,773 iteration 4128 : loss : 0.044044, loss_ce: 0.012739
2022-01-06 23:35:58,271 iteration 4129 : loss : 0.028965, loss_ce: 0.007049
2022-01-06 23:35:59,761 iteration 4130 : loss : 0.023492, loss_ce: 0.007933
2022-01-06 23:36:01,299 iteration 4131 : loss : 0.018910, loss_ce: 0.008286
 61%|████████████████▍          | 243/400 [1:54:48<1:11:56, 27.49s/it]2022-01-06 23:36:02,949 iteration 4132 : loss : 0.027449, loss_ce: 0.007647
2022-01-06 23:36:04,515 iteration 4133 : loss : 0.021989, loss_ce: 0.006871
2022-01-06 23:36:06,093 iteration 4134 : loss : 0.028690, loss_ce: 0.011554
2022-01-06 23:36:07,556 iteration 4135 : loss : 0.024004, loss_ce: 0.010584
2022-01-06 23:36:09,075 iteration 4136 : loss : 0.028637, loss_ce: 0.013090
2022-01-06 23:36:10,541 iteration 4137 : loss : 0.024238, loss_ce: 0.008627
2022-01-06 23:36:12,091 iteration 4138 : loss : 0.029552, loss_ce: 0.010924
2022-01-06 23:36:13,641 iteration 4139 : loss : 0.021426, loss_ce: 0.009284
2022-01-06 23:36:15,171 iteration 4140 : loss : 0.023810, loss_ce: 0.010947
2022-01-06 23:36:16,702 iteration 4141 : loss : 0.023376, loss_ce: 0.007612
2022-01-06 23:36:18,277 iteration 4142 : loss : 0.034346, loss_ce: 0.011389
2022-01-06 23:36:19,712 iteration 4143 : loss : 0.028596, loss_ce: 0.008843
2022-01-06 23:36:21,163 iteration 4144 : loss : 0.023537, loss_ce: 0.004990
2022-01-06 23:36:22,721 iteration 4145 : loss : 0.025554, loss_ce: 0.010386
2022-01-06 23:36:24,207 iteration 4146 : loss : 0.020426, loss_ce: 0.009712
2022-01-06 23:36:25,736 iteration 4147 : loss : 0.021805, loss_ce: 0.008370
2022-01-06 23:36:27,219 iteration 4148 : loss : 0.027331, loss_ce: 0.010426
 61%|████████████████▍          | 244/400 [1:55:13<1:10:15, 27.02s/it]2022-01-06 23:36:28,705 iteration 4149 : loss : 0.020419, loss_ce: 0.006753
2022-01-06 23:36:30,224 iteration 4150 : loss : 0.022617, loss_ce: 0.007133
2022-01-06 23:36:31,773 iteration 4151 : loss : 0.030710, loss_ce: 0.009907
2022-01-06 23:36:33,286 iteration 4152 : loss : 0.021659, loss_ce: 0.008523
2022-01-06 23:36:34,725 iteration 4153 : loss : 0.017222, loss_ce: 0.006083
2022-01-06 23:36:36,241 iteration 4154 : loss : 0.019492, loss_ce: 0.006754
2022-01-06 23:36:37,725 iteration 4155 : loss : 0.022161, loss_ce: 0.007136
2022-01-06 23:36:39,163 iteration 4156 : loss : 0.016046, loss_ce: 0.006046
2022-01-06 23:36:40,712 iteration 4157 : loss : 0.031446, loss_ce: 0.015567
2022-01-06 23:36:42,187 iteration 4158 : loss : 0.020173, loss_ce: 0.007855
2022-01-06 23:36:43,688 iteration 4159 : loss : 0.021640, loss_ce: 0.009057
2022-01-06 23:36:45,301 iteration 4160 : loss : 0.030025, loss_ce: 0.010585
2022-01-06 23:36:46,898 iteration 4161 : loss : 0.022754, loss_ce: 0.006400
2022-01-06 23:36:48,493 iteration 4162 : loss : 0.036619, loss_ce: 0.012406
2022-01-06 23:36:50,054 iteration 4163 : loss : 0.036155, loss_ce: 0.015834
2022-01-06 23:36:51,571 iteration 4164 : loss : 0.024432, loss_ce: 0.009900
2022-01-06 23:36:51,572 Training Data Eval:
2022-01-06 23:36:59,386   Average segmentation loss on training set: 0.0168
2022-01-06 23:36:59,387 Validation Data Eval:
2022-01-06 23:37:02,083   Average segmentation loss on validation set: 0.0757
2022-01-06 23:37:03,624 iteration 4165 : loss : 0.025884, loss_ce: 0.008088
 61%|████████████████▌          | 245/400 [1:55:50<1:17:04, 29.83s/it]2022-01-06 23:37:05,190 iteration 4166 : loss : 0.020920, loss_ce: 0.008803
2022-01-06 23:37:06,648 iteration 4167 : loss : 0.029120, loss_ce: 0.008579
2022-01-06 23:37:08,124 iteration 4168 : loss : 0.019237, loss_ce: 0.007941
2022-01-06 23:37:09,600 iteration 4169 : loss : 0.017587, loss_ce: 0.008592
2022-01-06 23:37:11,211 iteration 4170 : loss : 0.026282, loss_ce: 0.010829
2022-01-06 23:37:12,643 iteration 4171 : loss : 0.021019, loss_ce: 0.006482
2022-01-06 23:37:14,149 iteration 4172 : loss : 0.020108, loss_ce: 0.008738
2022-01-06 23:37:15,691 iteration 4173 : loss : 0.019885, loss_ce: 0.005527
2022-01-06 23:37:17,169 iteration 4174 : loss : 0.020598, loss_ce: 0.006372
2022-01-06 23:37:18,660 iteration 4175 : loss : 0.020921, loss_ce: 0.008557
2022-01-06 23:37:20,147 iteration 4176 : loss : 0.019450, loss_ce: 0.008411
2022-01-06 23:37:21,698 iteration 4177 : loss : 0.021568, loss_ce: 0.006273
2022-01-06 23:37:23,163 iteration 4178 : loss : 0.024903, loss_ce: 0.010189
2022-01-06 23:37:24,645 iteration 4179 : loss : 0.028963, loss_ce: 0.012464
2022-01-06 23:37:26,128 iteration 4180 : loss : 0.020025, loss_ce: 0.006047
2022-01-06 23:37:27,628 iteration 4181 : loss : 0.019458, loss_ce: 0.008341
2022-01-06 23:37:29,134 iteration 4182 : loss : 0.019216, loss_ce: 0.004936
 62%|████████████████▌          | 246/400 [1:56:15<1:13:14, 28.54s/it]2022-01-06 23:37:30,710 iteration 4183 : loss : 0.019386, loss_ce: 0.006356
2022-01-06 23:37:32,241 iteration 4184 : loss : 0.020629, loss_ce: 0.008874
2022-01-06 23:37:33,746 iteration 4185 : loss : 0.022029, loss_ce: 0.010325
2022-01-06 23:37:35,238 iteration 4186 : loss : 0.021871, loss_ce: 0.005960
2022-01-06 23:37:36,788 iteration 4187 : loss : 0.034059, loss_ce: 0.012451
2022-01-06 23:37:38,263 iteration 4188 : loss : 0.024355, loss_ce: 0.008258
2022-01-06 23:37:39,867 iteration 4189 : loss : 0.021145, loss_ce: 0.008435
2022-01-06 23:37:41,419 iteration 4190 : loss : 0.025575, loss_ce: 0.011623
2022-01-06 23:37:42,989 iteration 4191 : loss : 0.028908, loss_ce: 0.012885
2022-01-06 23:37:44,411 iteration 4192 : loss : 0.022191, loss_ce: 0.006611
2022-01-06 23:37:45,929 iteration 4193 : loss : 0.033670, loss_ce: 0.018648
2022-01-06 23:37:47,473 iteration 4194 : loss : 0.024098, loss_ce: 0.008688
2022-01-06 23:37:48,970 iteration 4195 : loss : 0.021019, loss_ce: 0.006319
2022-01-06 23:37:50,517 iteration 4196 : loss : 0.026286, loss_ce: 0.010511
2022-01-06 23:37:52,010 iteration 4197 : loss : 0.026062, loss_ce: 0.008949
2022-01-06 23:37:53,514 iteration 4198 : loss : 0.018598, loss_ce: 0.007395
2022-01-06 23:37:54,965 iteration 4199 : loss : 0.018370, loss_ce: 0.007417
 62%|████████████████▋          | 247/400 [1:56:41<1:10:42, 27.73s/it]2022-01-06 23:37:56,531 iteration 4200 : loss : 0.020017, loss_ce: 0.007534
2022-01-06 23:37:58,060 iteration 4201 : loss : 0.028152, loss_ce: 0.012441
2022-01-06 23:37:59,587 iteration 4202 : loss : 0.020778, loss_ce: 0.008959
2022-01-06 23:38:01,196 iteration 4203 : loss : 0.023523, loss_ce: 0.010447
2022-01-06 23:38:02,699 iteration 4204 : loss : 0.018341, loss_ce: 0.007366
2022-01-06 23:38:04,208 iteration 4205 : loss : 0.021197, loss_ce: 0.009285
2022-01-06 23:38:05,836 iteration 4206 : loss : 0.036180, loss_ce: 0.010817
2022-01-06 23:38:07,379 iteration 4207 : loss : 0.026572, loss_ce: 0.011651
2022-01-06 23:38:08,815 iteration 4208 : loss : 0.015882, loss_ce: 0.006384
2022-01-06 23:38:10,305 iteration 4209 : loss : 0.031512, loss_ce: 0.011620
2022-01-06 23:38:11,788 iteration 4210 : loss : 0.024179, loss_ce: 0.011168
2022-01-06 23:38:13,319 iteration 4211 : loss : 0.025821, loss_ce: 0.010860
2022-01-06 23:38:14,881 iteration 4212 : loss : 0.024940, loss_ce: 0.010002
2022-01-06 23:38:16,356 iteration 4213 : loss : 0.021545, loss_ce: 0.006488
2022-01-06 23:38:17,907 iteration 4214 : loss : 0.032334, loss_ce: 0.008943
2022-01-06 23:38:19,412 iteration 4215 : loss : 0.019802, loss_ce: 0.006890
2022-01-06 23:38:20,918 iteration 4216 : loss : 0.019682, loss_ce: 0.006541
 62%|████████████████▋          | 248/400 [1:57:07<1:08:53, 27.19s/it]2022-01-06 23:38:22,528 iteration 4217 : loss : 0.041904, loss_ce: 0.012941
2022-01-06 23:38:24,023 iteration 4218 : loss : 0.026200, loss_ce: 0.009745
2022-01-06 23:38:25,505 iteration 4219 : loss : 0.018324, loss_ce: 0.007703
2022-01-06 23:38:26,947 iteration 4220 : loss : 0.019856, loss_ce: 0.005147
2022-01-06 23:38:28,538 iteration 4221 : loss : 0.025255, loss_ce: 0.011129
2022-01-06 23:38:30,128 iteration 4222 : loss : 0.033587, loss_ce: 0.009656
2022-01-06 23:38:31,603 iteration 4223 : loss : 0.024752, loss_ce: 0.008208
2022-01-06 23:38:33,160 iteration 4224 : loss : 0.057968, loss_ce: 0.011982
2022-01-06 23:38:34,687 iteration 4225 : loss : 0.025572, loss_ce: 0.006572
2022-01-06 23:38:36,241 iteration 4226 : loss : 0.021940, loss_ce: 0.009971
2022-01-06 23:38:37,768 iteration 4227 : loss : 0.017130, loss_ce: 0.006765
2022-01-06 23:38:39,229 iteration 4228 : loss : 0.023442, loss_ce: 0.011873
2022-01-06 23:38:40,738 iteration 4229 : loss : 0.027822, loss_ce: 0.010613
2022-01-06 23:38:42,238 iteration 4230 : loss : 0.032769, loss_ce: 0.016720
2022-01-06 23:38:43,784 iteration 4231 : loss : 0.032912, loss_ce: 0.012208
2022-01-06 23:38:45,317 iteration 4232 : loss : 0.032882, loss_ce: 0.009245
2022-01-06 23:38:46,822 iteration 4233 : loss : 0.033288, loss_ce: 0.016136
 62%|████████████████▊          | 249/400 [1:57:33<1:07:27, 26.81s/it]2022-01-06 23:38:48,309 iteration 4234 : loss : 0.029046, loss_ce: 0.006681
2022-01-06 23:38:49,758 iteration 4235 : loss : 0.015985, loss_ce: 0.006389
2022-01-06 23:38:51,293 iteration 4236 : loss : 0.024680, loss_ce: 0.009593
2022-01-06 23:38:52,763 iteration 4237 : loss : 0.027560, loss_ce: 0.012098
2022-01-06 23:38:54,255 iteration 4238 : loss : 0.028618, loss_ce: 0.013897
2022-01-06 23:38:55,884 iteration 4239 : loss : 0.033136, loss_ce: 0.013792
2022-01-06 23:38:57,396 iteration 4240 : loss : 0.036951, loss_ce: 0.014705
2022-01-06 23:38:58,815 iteration 4241 : loss : 0.023591, loss_ce: 0.011144
2022-01-06 23:39:00,330 iteration 4242 : loss : 0.026036, loss_ce: 0.007908
2022-01-06 23:39:01,810 iteration 4243 : loss : 0.035598, loss_ce: 0.014537
2022-01-06 23:39:03,246 iteration 4244 : loss : 0.021991, loss_ce: 0.011245
2022-01-06 23:39:04,787 iteration 4245 : loss : 0.034257, loss_ce: 0.015528
2022-01-06 23:39:06,363 iteration 4246 : loss : 0.038112, loss_ce: 0.008390
2022-01-06 23:39:07,900 iteration 4247 : loss : 0.021861, loss_ce: 0.009149
2022-01-06 23:39:09,349 iteration 4248 : loss : 0.025782, loss_ce: 0.011135
2022-01-06 23:39:10,907 iteration 4249 : loss : 0.029154, loss_ce: 0.008949
2022-01-06 23:39:10,908 Training Data Eval:
2022-01-06 23:39:18,663   Average segmentation loss on training set: 0.0459
2022-01-06 23:39:18,663 Validation Data Eval:
2022-01-06 23:39:21,353   Average segmentation loss on validation set: 0.2100
2022-01-06 23:39:22,853 iteration 4250 : loss : 0.034943, loss_ce: 0.013013
 62%|████████████████▉          | 250/400 [1:58:09<1:13:55, 29.57s/it]2022-01-06 23:39:24,473 iteration 4251 : loss : 0.037893, loss_ce: 0.010143
2022-01-06 23:39:26,027 iteration 4252 : loss : 0.027577, loss_ce: 0.013293
2022-01-06 23:39:27,543 iteration 4253 : loss : 0.027763, loss_ce: 0.010123
2022-01-06 23:39:29,094 iteration 4254 : loss : 0.027597, loss_ce: 0.010966
2022-01-06 23:39:30,643 iteration 4255 : loss : 0.021387, loss_ce: 0.007514
2022-01-06 23:39:32,072 iteration 4256 : loss : 0.026494, loss_ce: 0.011330
2022-01-06 23:39:33,590 iteration 4257 : loss : 0.024150, loss_ce: 0.009518
2022-01-06 23:39:35,041 iteration 4258 : loss : 0.047444, loss_ce: 0.017088
2022-01-06 23:39:36,570 iteration 4259 : loss : 0.024658, loss_ce: 0.011034
2022-01-06 23:39:38,094 iteration 4260 : loss : 0.028567, loss_ce: 0.009376
2022-01-06 23:39:39,623 iteration 4261 : loss : 0.026227, loss_ce: 0.009858
2022-01-06 23:39:41,095 iteration 4262 : loss : 0.020851, loss_ce: 0.009461
2022-01-06 23:39:42,627 iteration 4263 : loss : 0.020200, loss_ce: 0.008788
2022-01-06 23:39:44,181 iteration 4264 : loss : 0.038101, loss_ce: 0.016052
2022-01-06 23:39:45,676 iteration 4265 : loss : 0.030057, loss_ce: 0.008678
2022-01-06 23:39:47,206 iteration 4266 : loss : 0.029491, loss_ce: 0.009047
2022-01-06 23:39:48,747 iteration 4267 : loss : 0.023046, loss_ce: 0.008605
 63%|████████████████▉          | 251/400 [1:58:35<1:10:42, 28.47s/it]2022-01-06 23:39:50,307 iteration 4268 : loss : 0.020408, loss_ce: 0.004885
2022-01-06 23:39:51,791 iteration 4269 : loss : 0.029794, loss_ce: 0.010802
2022-01-06 23:39:53,259 iteration 4270 : loss : 0.022768, loss_ce: 0.012377
2022-01-06 23:39:54,699 iteration 4271 : loss : 0.018784, loss_ce: 0.006975
2022-01-06 23:39:56,199 iteration 4272 : loss : 0.020649, loss_ce: 0.009973
2022-01-06 23:39:57,644 iteration 4273 : loss : 0.015236, loss_ce: 0.006442
2022-01-06 23:39:59,229 iteration 4274 : loss : 0.022708, loss_ce: 0.011712
2022-01-06 23:40:00,715 iteration 4275 : loss : 0.019205, loss_ce: 0.006822
2022-01-06 23:40:02,269 iteration 4276 : loss : 0.023865, loss_ce: 0.006629
2022-01-06 23:40:03,819 iteration 4277 : loss : 0.025349, loss_ce: 0.007919
2022-01-06 23:40:05,360 iteration 4278 : loss : 0.025548, loss_ce: 0.008444
2022-01-06 23:40:06,844 iteration 4279 : loss : 0.020000, loss_ce: 0.009688
2022-01-06 23:40:08,371 iteration 4280 : loss : 0.027828, loss_ce: 0.008831
2022-01-06 23:40:09,886 iteration 4281 : loss : 0.034290, loss_ce: 0.010345
2022-01-06 23:40:11,333 iteration 4282 : loss : 0.025584, loss_ce: 0.009153
2022-01-06 23:40:12,829 iteration 4283 : loss : 0.023894, loss_ce: 0.007312
2022-01-06 23:40:14,301 iteration 4284 : loss : 0.021062, loss_ce: 0.007832
 63%|█████████████████          | 252/400 [1:59:01<1:08:03, 27.59s/it]2022-01-06 23:40:15,813 iteration 4285 : loss : 0.024223, loss_ce: 0.007396
2022-01-06 23:40:17,368 iteration 4286 : loss : 0.021016, loss_ce: 0.008071
2022-01-06 23:40:18,834 iteration 4287 : loss : 0.023374, loss_ce: 0.011130
2022-01-06 23:40:20,313 iteration 4288 : loss : 0.020246, loss_ce: 0.009131
2022-01-06 23:40:21,833 iteration 4289 : loss : 0.023644, loss_ce: 0.008918
2022-01-06 23:40:23,333 iteration 4290 : loss : 0.023983, loss_ce: 0.007310
2022-01-06 23:40:24,854 iteration 4291 : loss : 0.025023, loss_ce: 0.008392
2022-01-06 23:40:26,370 iteration 4292 : loss : 0.023450, loss_ce: 0.012085
2022-01-06 23:40:27,873 iteration 4293 : loss : 0.030946, loss_ce: 0.008480
2022-01-06 23:40:29,350 iteration 4294 : loss : 0.022827, loss_ce: 0.005834
2022-01-06 23:40:30,949 iteration 4295 : loss : 0.027703, loss_ce: 0.009611
2022-01-06 23:40:32,452 iteration 4296 : loss : 0.033105, loss_ce: 0.011672
2022-01-06 23:40:33,968 iteration 4297 : loss : 0.037695, loss_ce: 0.012519
2022-01-06 23:40:35,446 iteration 4298 : loss : 0.024372, loss_ce: 0.014442
2022-01-06 23:40:37,001 iteration 4299 : loss : 0.022298, loss_ce: 0.010011
2022-01-06 23:40:38,573 iteration 4300 : loss : 0.029455, loss_ce: 0.011257
2022-01-06 23:40:40,142 iteration 4301 : loss : 0.023482, loss_ce: 0.010800
 63%|█████████████████          | 253/400 [1:59:26<1:06:19, 27.07s/it]2022-01-06 23:40:41,742 iteration 4302 : loss : 0.030079, loss_ce: 0.012683
2022-01-06 23:40:43,281 iteration 4303 : loss : 0.038527, loss_ce: 0.008488
2022-01-06 23:40:44,732 iteration 4304 : loss : 0.020201, loss_ce: 0.007936
2022-01-06 23:40:46,211 iteration 4305 : loss : 0.022582, loss_ce: 0.010342
2022-01-06 23:40:47,708 iteration 4306 : loss : 0.019017, loss_ce: 0.005934
2022-01-06 23:40:49,180 iteration 4307 : loss : 0.022105, loss_ce: 0.010476
2022-01-06 23:40:50,720 iteration 4308 : loss : 0.032578, loss_ce: 0.011545
2022-01-06 23:40:52,314 iteration 4309 : loss : 0.031376, loss_ce: 0.011220
2022-01-06 23:40:53,867 iteration 4310 : loss : 0.019327, loss_ce: 0.007782
2022-01-06 23:40:55,364 iteration 4311 : loss : 0.025345, loss_ce: 0.008694
2022-01-06 23:40:56,842 iteration 4312 : loss : 0.029436, loss_ce: 0.010471
2022-01-06 23:40:58,337 iteration 4313 : loss : 0.021943, loss_ce: 0.010931
2022-01-06 23:40:59,829 iteration 4314 : loss : 0.025260, loss_ce: 0.008236
2022-01-06 23:41:01,379 iteration 4315 : loss : 0.024591, loss_ce: 0.008156
2022-01-06 23:41:02,862 iteration 4316 : loss : 0.033347, loss_ce: 0.011173
2022-01-06 23:41:04,439 iteration 4317 : loss : 0.031411, loss_ce: 0.012492
2022-01-06 23:41:05,917 iteration 4318 : loss : 0.022940, loss_ce: 0.008601
 64%|█████████████████▏         | 254/400 [1:59:52<1:04:55, 26.68s/it]2022-01-06 23:41:07,545 iteration 4319 : loss : 0.030705, loss_ce: 0.009455
2022-01-06 23:41:09,099 iteration 4320 : loss : 0.033312, loss_ce: 0.012261
2022-01-06 23:41:10,539 iteration 4321 : loss : 0.024039, loss_ce: 0.007311
2022-01-06 23:41:12,070 iteration 4322 : loss : 0.022676, loss_ce: 0.007041
2022-01-06 23:41:13,546 iteration 4323 : loss : 0.023493, loss_ce: 0.011450
2022-01-06 23:41:15,174 iteration 4324 : loss : 0.039997, loss_ce: 0.018984
2022-01-06 23:41:16,698 iteration 4325 : loss : 0.018707, loss_ce: 0.007704
2022-01-06 23:41:18,198 iteration 4326 : loss : 0.034076, loss_ce: 0.017243
2022-01-06 23:41:19,754 iteration 4327 : loss : 0.042671, loss_ce: 0.011861
2022-01-06 23:41:21,255 iteration 4328 : loss : 0.032211, loss_ce: 0.010321
2022-01-06 23:41:22,842 iteration 4329 : loss : 0.046140, loss_ce: 0.009953
2022-01-06 23:41:24,410 iteration 4330 : loss : 0.024659, loss_ce: 0.010192
2022-01-06 23:41:26,018 iteration 4331 : loss : 0.069553, loss_ce: 0.018990
2022-01-06 23:41:27,577 iteration 4332 : loss : 0.035450, loss_ce: 0.012363
2022-01-06 23:41:29,165 iteration 4333 : loss : 0.027439, loss_ce: 0.012331
2022-01-06 23:41:30,700 iteration 4334 : loss : 0.021216, loss_ce: 0.010991
2022-01-06 23:41:30,700 Training Data Eval:
2022-01-06 23:41:38,488   Average segmentation loss on training set: 0.0233
2022-01-06 23:41:38,488 Validation Data Eval:
2022-01-06 23:41:41,185   Average segmentation loss on validation set: 0.0765
2022-01-06 23:41:42,747 iteration 4335 : loss : 0.034347, loss_ce: 0.008824
 64%|█████████████████▏         | 255/400 [2:00:29<1:11:49, 29.72s/it]2022-01-06 23:41:44,210 iteration 4336 : loss : 0.016088, loss_ce: 0.008179
2022-01-06 23:41:45,801 iteration 4337 : loss : 0.031325, loss_ce: 0.013224
2022-01-06 23:41:47,317 iteration 4338 : loss : 0.023716, loss_ce: 0.007518
2022-01-06 23:41:48,803 iteration 4339 : loss : 0.017963, loss_ce: 0.006307
2022-01-06 23:41:50,348 iteration 4340 : loss : 0.026195, loss_ce: 0.010329
2022-01-06 23:41:51,808 iteration 4341 : loss : 0.019870, loss_ce: 0.006656
2022-01-06 23:41:53,353 iteration 4342 : loss : 0.028682, loss_ce: 0.008306
2022-01-06 23:41:54,903 iteration 4343 : loss : 0.028301, loss_ce: 0.007487
2022-01-06 23:41:56,443 iteration 4344 : loss : 0.022171, loss_ce: 0.010393
2022-01-06 23:41:58,031 iteration 4345 : loss : 0.029421, loss_ce: 0.012981
2022-01-06 23:41:59,468 iteration 4346 : loss : 0.029319, loss_ce: 0.007815
2022-01-06 23:42:00,974 iteration 4347 : loss : 0.018589, loss_ce: 0.008174
2022-01-06 23:42:02,436 iteration 4348 : loss : 0.018324, loss_ce: 0.008814
2022-01-06 23:42:03,859 iteration 4349 : loss : 0.019516, loss_ce: 0.005330
2022-01-06 23:42:05,286 iteration 4350 : loss : 0.024286, loss_ce: 0.009021
2022-01-06 23:42:06,816 iteration 4351 : loss : 0.030582, loss_ce: 0.009158
2022-01-06 23:42:08,331 iteration 4352 : loss : 0.022359, loss_ce: 0.009408
 64%|█████████████████▎         | 256/400 [2:00:55<1:08:21, 28.48s/it]2022-01-06 23:42:09,891 iteration 4353 : loss : 0.020047, loss_ce: 0.006532
2022-01-06 23:42:11,488 iteration 4354 : loss : 0.029130, loss_ce: 0.014619
2022-01-06 23:42:13,059 iteration 4355 : loss : 0.031653, loss_ce: 0.017770
2022-01-06 23:42:14,661 iteration 4356 : loss : 0.030169, loss_ce: 0.008475
2022-01-06 23:42:16,234 iteration 4357 : loss : 0.018830, loss_ce: 0.008542
2022-01-06 23:42:17,827 iteration 4358 : loss : 0.034325, loss_ce: 0.016474
2022-01-06 23:42:19,377 iteration 4359 : loss : 0.023364, loss_ce: 0.008292
2022-01-06 23:42:20,894 iteration 4360 : loss : 0.027255, loss_ce: 0.012544
2022-01-06 23:42:22,511 iteration 4361 : loss : 0.030797, loss_ce: 0.010645
2022-01-06 23:42:24,046 iteration 4362 : loss : 0.017764, loss_ce: 0.007779
2022-01-06 23:42:25,590 iteration 4363 : loss : 0.020637, loss_ce: 0.009281
2022-01-06 23:42:27,156 iteration 4364 : loss : 0.021943, loss_ce: 0.008387
2022-01-06 23:42:28,783 iteration 4365 : loss : 0.024723, loss_ce: 0.012046
2022-01-06 23:42:30,268 iteration 4366 : loss : 0.021338, loss_ce: 0.006037
2022-01-06 23:42:31,713 iteration 4367 : loss : 0.022940, loss_ce: 0.010797
2022-01-06 23:42:33,181 iteration 4368 : loss : 0.025533, loss_ce: 0.007740
2022-01-06 23:42:34,734 iteration 4369 : loss : 0.020298, loss_ce: 0.008072
 64%|█████████████████▎         | 257/400 [2:01:21<1:06:23, 27.86s/it]2022-01-06 23:42:36,341 iteration 4370 : loss : 0.020486, loss_ce: 0.007271
2022-01-06 23:42:37,920 iteration 4371 : loss : 0.026789, loss_ce: 0.014917
2022-01-06 23:42:39,505 iteration 4372 : loss : 0.020531, loss_ce: 0.008717
2022-01-06 23:42:41,093 iteration 4373 : loss : 0.023475, loss_ce: 0.011784
2022-01-06 23:42:42,686 iteration 4374 : loss : 0.021618, loss_ce: 0.007109
2022-01-06 23:42:44,279 iteration 4375 : loss : 0.024016, loss_ce: 0.011000
2022-01-06 23:42:45,822 iteration 4376 : loss : 0.033330, loss_ce: 0.012146
2022-01-06 23:42:47,356 iteration 4377 : loss : 0.025373, loss_ce: 0.007574
2022-01-06 23:42:48,853 iteration 4378 : loss : 0.018339, loss_ce: 0.006572
2022-01-06 23:42:50,362 iteration 4379 : loss : 0.023794, loss_ce: 0.008460
2022-01-06 23:42:51,839 iteration 4380 : loss : 0.018109, loss_ce: 0.005878
2022-01-06 23:42:53,423 iteration 4381 : loss : 0.034601, loss_ce: 0.012257
2022-01-06 23:42:54,978 iteration 4382 : loss : 0.054388, loss_ce: 0.011090
2022-01-06 23:42:56,507 iteration 4383 : loss : 0.028159, loss_ce: 0.007574
2022-01-06 23:42:58,125 iteration 4384 : loss : 0.022579, loss_ce: 0.010292
2022-01-06 23:42:59,732 iteration 4385 : loss : 0.050666, loss_ce: 0.020842
2022-01-06 23:43:01,247 iteration 4386 : loss : 0.024937, loss_ce: 0.011920
 64%|█████████████████▍         | 258/400 [2:01:47<1:04:58, 27.46s/it]2022-01-06 23:43:02,822 iteration 4387 : loss : 0.032948, loss_ce: 0.009658
2022-01-06 23:43:04,376 iteration 4388 : loss : 0.044148, loss_ce: 0.021195
2022-01-06 23:43:05,927 iteration 4389 : loss : 0.027464, loss_ce: 0.009738
2022-01-06 23:43:07,426 iteration 4390 : loss : 0.021326, loss_ce: 0.007538
2022-01-06 23:43:08,998 iteration 4391 : loss : 0.030503, loss_ce: 0.013903
2022-01-06 23:43:10,553 iteration 4392 : loss : 0.022911, loss_ce: 0.007110
2022-01-06 23:43:12,180 iteration 4393 : loss : 0.021920, loss_ce: 0.009228
2022-01-06 23:43:13,771 iteration 4394 : loss : 0.025502, loss_ce: 0.011094
2022-01-06 23:43:15,426 iteration 4395 : loss : 0.032917, loss_ce: 0.012630
2022-01-06 23:43:16,959 iteration 4396 : loss : 0.049995, loss_ce: 0.023268
2022-01-06 23:43:18,604 iteration 4397 : loss : 0.025629, loss_ce: 0.012436
2022-01-06 23:43:20,103 iteration 4398 : loss : 0.023171, loss_ce: 0.009240
2022-01-06 23:43:21,697 iteration 4399 : loss : 0.033810, loss_ce: 0.015502
2022-01-06 23:43:23,256 iteration 4400 : loss : 0.038741, loss_ce: 0.012571
2022-01-06 23:43:24,940 iteration 4401 : loss : 0.042148, loss_ce: 0.018133
2022-01-06 23:43:26,510 iteration 4402 : loss : 0.026875, loss_ce: 0.009119
2022-01-06 23:43:28,078 iteration 4403 : loss : 0.024853, loss_ce: 0.008740
 65%|█████████████████▍         | 259/400 [2:02:14<1:04:05, 27.27s/it]2022-01-06 23:43:29,653 iteration 4404 : loss : 0.023395, loss_ce: 0.010615
2022-01-06 23:43:31,231 iteration 4405 : loss : 0.020287, loss_ce: 0.009373
2022-01-06 23:43:32,795 iteration 4406 : loss : 0.020988, loss_ce: 0.009843
2022-01-06 23:43:34,393 iteration 4407 : loss : 0.030809, loss_ce: 0.010845
2022-01-06 23:43:35,903 iteration 4408 : loss : 0.019736, loss_ce: 0.008316
2022-01-06 23:43:37,451 iteration 4409 : loss : 0.024795, loss_ce: 0.009275
2022-01-06 23:43:39,123 iteration 4410 : loss : 0.028164, loss_ce: 0.009587
2022-01-06 23:43:40,690 iteration 4411 : loss : 0.026922, loss_ce: 0.011426
2022-01-06 23:43:42,217 iteration 4412 : loss : 0.027744, loss_ce: 0.010664
2022-01-06 23:43:43,767 iteration 4413 : loss : 0.021545, loss_ce: 0.007494
2022-01-06 23:43:45,225 iteration 4414 : loss : 0.020955, loss_ce: 0.006891
2022-01-06 23:43:46,775 iteration 4415 : loss : 0.021254, loss_ce: 0.006965
2022-01-06 23:43:48,349 iteration 4416 : loss : 0.038491, loss_ce: 0.012105
2022-01-06 23:43:49,868 iteration 4417 : loss : 0.018497, loss_ce: 0.005740
2022-01-06 23:43:51,410 iteration 4418 : loss : 0.023680, loss_ce: 0.011606
2022-01-06 23:43:52,983 iteration 4419 : loss : 0.022150, loss_ce: 0.007439
2022-01-06 23:43:52,984 Training Data Eval:
2022-01-06 23:44:00,839   Average segmentation loss on training set: 0.0382
2022-01-06 23:44:00,840 Validation Data Eval:
2022-01-06 23:44:03,538   Average segmentation loss on validation set: 0.2060
2022-01-06 23:44:05,020 iteration 4420 : loss : 0.016303, loss_ce: 0.007638
 65%|█████████████████▌         | 260/400 [2:02:51<1:10:24, 30.17s/it]2022-01-06 23:44:06,578 iteration 4421 : loss : 0.020050, loss_ce: 0.006824
2022-01-06 23:44:08,118 iteration 4422 : loss : 0.034143, loss_ce: 0.014356
2022-01-06 23:44:09,700 iteration 4423 : loss : 0.038241, loss_ce: 0.015617
2022-01-06 23:44:11,172 iteration 4424 : loss : 0.020753, loss_ce: 0.006599
2022-01-06 23:44:12,655 iteration 4425 : loss : 0.018834, loss_ce: 0.006176
2022-01-06 23:44:14,261 iteration 4426 : loss : 0.017364, loss_ce: 0.007133
2022-01-06 23:44:15,774 iteration 4427 : loss : 0.018586, loss_ce: 0.008493
2022-01-06 23:44:17,250 iteration 4428 : loss : 0.016976, loss_ce: 0.008461
2022-01-06 23:44:18,738 iteration 4429 : loss : 0.020340, loss_ce: 0.006969
2022-01-06 23:44:20,235 iteration 4430 : loss : 0.017565, loss_ce: 0.007706
2022-01-06 23:44:21,806 iteration 4431 : loss : 0.024392, loss_ce: 0.010286
2022-01-06 23:44:23,401 iteration 4432 : loss : 0.024550, loss_ce: 0.009089
2022-01-06 23:44:24,935 iteration 4433 : loss : 0.029760, loss_ce: 0.008240
2022-01-06 23:44:26,432 iteration 4434 : loss : 0.015540, loss_ce: 0.006934
2022-01-06 23:44:27,980 iteration 4435 : loss : 0.025626, loss_ce: 0.006577
2022-01-06 23:44:29,518 iteration 4436 : loss : 0.018316, loss_ce: 0.007862
2022-01-06 23:44:31,119 iteration 4437 : loss : 0.026649, loss_ce: 0.008882
 65%|█████████████████▌         | 261/400 [2:03:17<1:07:03, 28.95s/it]2022-01-06 23:44:32,780 iteration 4438 : loss : 0.042078, loss_ce: 0.012912
2022-01-06 23:44:34,341 iteration 4439 : loss : 0.022304, loss_ce: 0.007813
2022-01-06 23:44:35,895 iteration 4440 : loss : 0.026781, loss_ce: 0.009817
2022-01-06 23:44:37,546 iteration 4441 : loss : 0.031651, loss_ce: 0.010753
2022-01-06 23:44:39,148 iteration 4442 : loss : 0.021340, loss_ce: 0.011187
2022-01-06 23:44:40,708 iteration 4443 : loss : 0.023122, loss_ce: 0.008558
2022-01-06 23:44:42,178 iteration 4444 : loss : 0.018753, loss_ce: 0.006361
2022-01-06 23:44:43,699 iteration 4445 : loss : 0.032583, loss_ce: 0.012811
2022-01-06 23:44:45,256 iteration 4446 : loss : 0.023553, loss_ce: 0.009424
2022-01-06 23:44:46,759 iteration 4447 : loss : 0.019507, loss_ce: 0.006897
2022-01-06 23:44:48,305 iteration 4448 : loss : 0.023784, loss_ce: 0.009809
2022-01-06 23:44:49,836 iteration 4449 : loss : 0.023276, loss_ce: 0.009554
2022-01-06 23:44:51,434 iteration 4450 : loss : 0.039622, loss_ce: 0.014731
2022-01-06 23:44:52,944 iteration 4451 : loss : 0.018316, loss_ce: 0.007128
2022-01-06 23:44:54,489 iteration 4452 : loss : 0.020445, loss_ce: 0.008658
2022-01-06 23:44:55,999 iteration 4453 : loss : 0.019084, loss_ce: 0.006716
2022-01-06 23:44:57,502 iteration 4454 : loss : 0.020602, loss_ce: 0.008181
 66%|█████████████████▋         | 262/400 [2:03:44<1:04:48, 28.18s/it]2022-01-06 23:44:59,113 iteration 4455 : loss : 0.029098, loss_ce: 0.009440
2022-01-06 23:45:00,578 iteration 4456 : loss : 0.018128, loss_ce: 0.006833
2022-01-06 23:45:02,129 iteration 4457 : loss : 0.020983, loss_ce: 0.008915
2022-01-06 23:45:03,756 iteration 4458 : loss : 0.049858, loss_ce: 0.021221
2022-01-06 23:45:05,439 iteration 4459 : loss : 0.028511, loss_ce: 0.014531
2022-01-06 23:45:06,941 iteration 4460 : loss : 0.019288, loss_ce: 0.006804
2022-01-06 23:45:08,485 iteration 4461 : loss : 0.025500, loss_ce: 0.008478
2022-01-06 23:45:10,057 iteration 4462 : loss : 0.026221, loss_ce: 0.011742
2022-01-06 23:45:11,579 iteration 4463 : loss : 0.031609, loss_ce: 0.012984
2022-01-06 23:45:13,081 iteration 4464 : loss : 0.018459, loss_ce: 0.006871
2022-01-06 23:45:14,536 iteration 4465 : loss : 0.026719, loss_ce: 0.006703
2022-01-06 23:45:16,015 iteration 4466 : loss : 0.019128, loss_ce: 0.005600
2022-01-06 23:45:17,536 iteration 4467 : loss : 0.059927, loss_ce: 0.026414
2022-01-06 23:45:19,082 iteration 4468 : loss : 0.023176, loss_ce: 0.010271
2022-01-06 23:45:20,679 iteration 4469 : loss : 0.035506, loss_ce: 0.018254
2022-01-06 23:45:22,237 iteration 4470 : loss : 0.021910, loss_ce: 0.008914
2022-01-06 23:45:23,822 iteration 4471 : loss : 0.039381, loss_ce: 0.012633
 66%|█████████████████▊         | 263/400 [2:04:10<1:03:03, 27.62s/it]2022-01-06 23:45:25,360 iteration 4472 : loss : 0.019634, loss_ce: 0.009398
2022-01-06 23:45:26,838 iteration 4473 : loss : 0.023119, loss_ce: 0.009412
2022-01-06 23:45:28,361 iteration 4474 : loss : 0.018958, loss_ce: 0.009059
2022-01-06 23:45:29,860 iteration 4475 : loss : 0.019487, loss_ce: 0.006365
2022-01-06 23:45:31,339 iteration 4476 : loss : 0.023337, loss_ce: 0.008320
2022-01-06 23:45:32,826 iteration 4477 : loss : 0.027209, loss_ce: 0.010464
2022-01-06 23:45:34,433 iteration 4478 : loss : 0.026162, loss_ce: 0.012791
2022-01-06 23:45:35,942 iteration 4479 : loss : 0.025941, loss_ce: 0.008270
2022-01-06 23:45:37,458 iteration 4480 : loss : 0.025387, loss_ce: 0.009121
2022-01-06 23:45:38,922 iteration 4481 : loss : 0.024435, loss_ce: 0.009498
2022-01-06 23:45:40,385 iteration 4482 : loss : 0.024929, loss_ce: 0.007717
2022-01-06 23:45:41,870 iteration 4483 : loss : 0.029233, loss_ce: 0.008994
2022-01-06 23:45:43,390 iteration 4484 : loss : 0.016136, loss_ce: 0.005271
2022-01-06 23:45:44,832 iteration 4485 : loss : 0.015590, loss_ce: 0.006873
2022-01-06 23:45:46,394 iteration 4486 : loss : 0.021357, loss_ce: 0.008758
2022-01-06 23:45:47,926 iteration 4487 : loss : 0.027535, loss_ce: 0.011589
2022-01-06 23:45:49,382 iteration 4488 : loss : 0.024973, loss_ce: 0.008418
 66%|█████████████████▊         | 264/400 [2:04:36<1:01:12, 27.00s/it]2022-01-06 23:45:50,980 iteration 4489 : loss : 0.027516, loss_ce: 0.015982
2022-01-06 23:45:52,480 iteration 4490 : loss : 0.019294, loss_ce: 0.008979
2022-01-06 23:45:54,073 iteration 4491 : loss : 0.025103, loss_ce: 0.008648
2022-01-06 23:45:55,549 iteration 4492 : loss : 0.038182, loss_ce: 0.012499
2022-01-06 23:45:57,126 iteration 4493 : loss : 0.030701, loss_ce: 0.018582
2022-01-06 23:45:58,588 iteration 4494 : loss : 0.022430, loss_ce: 0.007652
2022-01-06 23:46:00,104 iteration 4495 : loss : 0.030591, loss_ce: 0.007283
2022-01-06 23:46:01,621 iteration 4496 : loss : 0.018279, loss_ce: 0.005194
2022-01-06 23:46:03,037 iteration 4497 : loss : 0.026586, loss_ce: 0.012240
2022-01-06 23:46:04,548 iteration 4498 : loss : 0.024411, loss_ce: 0.008005
2022-01-06 23:46:06,020 iteration 4499 : loss : 0.017364, loss_ce: 0.007213
2022-01-06 23:46:07,523 iteration 4500 : loss : 0.024418, loss_ce: 0.009464
2022-01-06 23:46:09,007 iteration 4501 : loss : 0.025977, loss_ce: 0.012250
2022-01-06 23:46:10,494 iteration 4502 : loss : 0.026524, loss_ce: 0.011132
2022-01-06 23:46:12,077 iteration 4503 : loss : 0.036019, loss_ce: 0.015141
2022-01-06 23:46:13,507 iteration 4504 : loss : 0.016364, loss_ce: 0.006795
2022-01-06 23:46:13,507 Training Data Eval:
2022-01-06 23:46:21,284   Average segmentation loss on training set: 0.0172
2022-01-06 23:46:21,284 Validation Data Eval:
2022-01-06 23:46:23,985   Average segmentation loss on validation set: 0.1024
2022-01-06 23:46:25,504 iteration 4505 : loss : 0.020385, loss_ce: 0.007466
 66%|█████████████████▉         | 265/400 [2:05:12<1:06:55, 29.74s/it]2022-01-06 23:46:27,103 iteration 4506 : loss : 0.017993, loss_ce: 0.006609
2022-01-06 23:46:28,681 iteration 4507 : loss : 0.027008, loss_ce: 0.010548
2022-01-06 23:46:30,192 iteration 4508 : loss : 0.040933, loss_ce: 0.011091
2022-01-06 23:46:31,647 iteration 4509 : loss : 0.018526, loss_ce: 0.006266
2022-01-06 23:46:33,163 iteration 4510 : loss : 0.016033, loss_ce: 0.005232
2022-01-06 23:46:34,762 iteration 4511 : loss : 0.025832, loss_ce: 0.009963
2022-01-06 23:46:36,194 iteration 4512 : loss : 0.019021, loss_ce: 0.006518
2022-01-06 23:46:37,667 iteration 4513 : loss : 0.020457, loss_ce: 0.007474
2022-01-06 23:46:39,138 iteration 4514 : loss : 0.023510, loss_ce: 0.009780
2022-01-06 23:46:40,640 iteration 4515 : loss : 0.020461, loss_ce: 0.008715
2022-01-06 23:46:42,164 iteration 4516 : loss : 0.021617, loss_ce: 0.010049
2022-01-06 23:46:43,714 iteration 4517 : loss : 0.048847, loss_ce: 0.020338
2022-01-06 23:46:45,209 iteration 4518 : loss : 0.024709, loss_ce: 0.010406
2022-01-06 23:46:46,663 iteration 4519 : loss : 0.034228, loss_ce: 0.010431
2022-01-06 23:46:48,187 iteration 4520 : loss : 0.026347, loss_ce: 0.007755
2022-01-06 23:46:49,724 iteration 4521 : loss : 0.021357, loss_ce: 0.008697
2022-01-06 23:46:51,282 iteration 4522 : loss : 0.050085, loss_ce: 0.019527
 66%|█████████████████▉         | 266/400 [2:05:38<1:03:45, 28.55s/it]2022-01-06 23:46:52,915 iteration 4523 : loss : 0.029530, loss_ce: 0.015058
2022-01-06 23:46:54,441 iteration 4524 : loss : 0.025495, loss_ce: 0.009548
2022-01-06 23:46:55,993 iteration 4525 : loss : 0.023679, loss_ce: 0.010813
2022-01-06 23:46:57,495 iteration 4526 : loss : 0.041367, loss_ce: 0.015193
2022-01-06 23:46:58,938 iteration 4527 : loss : 0.025593, loss_ce: 0.010097
2022-01-06 23:47:00,421 iteration 4528 : loss : 0.026656, loss_ce: 0.009274
2022-01-06 23:47:01,953 iteration 4529 : loss : 0.029899, loss_ce: 0.012106
2022-01-06 23:47:03,450 iteration 4530 : loss : 0.038443, loss_ce: 0.008991
2022-01-06 23:47:04,997 iteration 4531 : loss : 0.022231, loss_ce: 0.008079
2022-01-06 23:47:06,571 iteration 4532 : loss : 0.019578, loss_ce: 0.006337
2022-01-06 23:47:08,098 iteration 4533 : loss : 0.033133, loss_ce: 0.014809
2022-01-06 23:47:09,640 iteration 4534 : loss : 0.032107, loss_ce: 0.011154
2022-01-06 23:47:11,218 iteration 4535 : loss : 0.025202, loss_ce: 0.009281
2022-01-06 23:47:12,791 iteration 4536 : loss : 0.034392, loss_ce: 0.012307
2022-01-06 23:47:14,345 iteration 4537 : loss : 0.034713, loss_ce: 0.015486
2022-01-06 23:47:15,891 iteration 4538 : loss : 0.022613, loss_ce: 0.008457
2022-01-06 23:47:17,423 iteration 4539 : loss : 0.019133, loss_ce: 0.007808
 67%|██████████████████         | 267/400 [2:06:04<1:01:41, 27.83s/it]2022-01-06 23:47:18,991 iteration 4540 : loss : 0.016553, loss_ce: 0.006002
2022-01-06 23:47:20,554 iteration 4541 : loss : 0.026424, loss_ce: 0.013332
2022-01-06 23:47:22,049 iteration 4542 : loss : 0.022762, loss_ce: 0.008164
2022-01-06 23:47:23,569 iteration 4543 : loss : 0.023066, loss_ce: 0.008248
2022-01-06 23:47:25,068 iteration 4544 : loss : 0.023403, loss_ce: 0.008501
2022-01-06 23:47:26,676 iteration 4545 : loss : 0.025437, loss_ce: 0.011964
2022-01-06 23:47:28,319 iteration 4546 : loss : 0.039069, loss_ce: 0.018105
2022-01-06 23:47:29,939 iteration 4547 : loss : 0.022537, loss_ce: 0.008841
2022-01-06 23:47:31,418 iteration 4548 : loss : 0.020072, loss_ce: 0.007278
2022-01-06 23:47:32,902 iteration 4549 : loss : 0.019489, loss_ce: 0.005407
2022-01-06 23:47:34,433 iteration 4550 : loss : 0.037510, loss_ce: 0.021204
2022-01-06 23:47:35,974 iteration 4551 : loss : 0.028339, loss_ce: 0.011252
2022-01-06 23:47:37,570 iteration 4552 : loss : 0.034058, loss_ce: 0.010599
2022-01-06 23:47:39,100 iteration 4553 : loss : 0.028649, loss_ce: 0.008451
2022-01-06 23:47:40,575 iteration 4554 : loss : 0.022013, loss_ce: 0.009775
2022-01-06 23:47:42,097 iteration 4555 : loss : 0.017625, loss_ce: 0.007295
2022-01-06 23:47:43,611 iteration 4556 : loss : 0.027351, loss_ce: 0.008085
 67%|██████████████████         | 268/400 [2:06:30<1:00:07, 27.33s/it]2022-01-06 23:47:45,159 iteration 4557 : loss : 0.026511, loss_ce: 0.010406
2022-01-06 23:47:46,717 iteration 4558 : loss : 0.032459, loss_ce: 0.015704
2022-01-06 23:47:48,292 iteration 4559 : loss : 0.024869, loss_ce: 0.009852
2022-01-06 23:47:49,850 iteration 4560 : loss : 0.019314, loss_ce: 0.011019
2022-01-06 23:47:51,342 iteration 4561 : loss : 0.016894, loss_ce: 0.005676
2022-01-06 23:47:52,876 iteration 4562 : loss : 0.025293, loss_ce: 0.011220
2022-01-06 23:47:54,383 iteration 4563 : loss : 0.025155, loss_ce: 0.008714
2022-01-06 23:47:55,868 iteration 4564 : loss : 0.017739, loss_ce: 0.007637
2022-01-06 23:47:57,374 iteration 4565 : loss : 0.033874, loss_ce: 0.013149
2022-01-06 23:47:58,898 iteration 4566 : loss : 0.020727, loss_ce: 0.007640
2022-01-06 23:48:00,411 iteration 4567 : loss : 0.021042, loss_ce: 0.008611
2022-01-06 23:48:01,861 iteration 4568 : loss : 0.018259, loss_ce: 0.006116
2022-01-06 23:48:03,331 iteration 4569 : loss : 0.018397, loss_ce: 0.008100
2022-01-06 23:48:04,818 iteration 4570 : loss : 0.019615, loss_ce: 0.006969
2022-01-06 23:48:06,363 iteration 4571 : loss : 0.019109, loss_ce: 0.008366
2022-01-06 23:48:07,892 iteration 4572 : loss : 0.022700, loss_ce: 0.008934
2022-01-06 23:48:09,442 iteration 4573 : loss : 0.028545, loss_ce: 0.009690
 67%|███████████████████▌         | 269/400 [2:06:56<58:41, 26.88s/it]2022-01-06 23:48:10,944 iteration 4574 : loss : 0.015669, loss_ce: 0.005184
2022-01-06 23:48:12,465 iteration 4575 : loss : 0.031186, loss_ce: 0.009219
2022-01-06 23:48:13,909 iteration 4576 : loss : 0.019777, loss_ce: 0.007798
2022-01-06 23:48:15,374 iteration 4577 : loss : 0.018815, loss_ce: 0.009355
2022-01-06 23:48:16,886 iteration 4578 : loss : 0.019308, loss_ce: 0.007410
2022-01-06 23:48:18,361 iteration 4579 : loss : 0.021985, loss_ce: 0.008873
2022-01-06 23:48:19,869 iteration 4580 : loss : 0.022523, loss_ce: 0.009548
2022-01-06 23:48:21,395 iteration 4581 : loss : 0.022233, loss_ce: 0.008984
2022-01-06 23:48:22,910 iteration 4582 : loss : 0.033540, loss_ce: 0.013556
2022-01-06 23:48:24,426 iteration 4583 : loss : 0.024087, loss_ce: 0.008771
2022-01-06 23:48:25,933 iteration 4584 : loss : 0.021209, loss_ce: 0.007623
2022-01-06 23:48:27,464 iteration 4585 : loss : 0.024766, loss_ce: 0.008774
2022-01-06 23:48:28,912 iteration 4586 : loss : 0.020090, loss_ce: 0.008305
2022-01-06 23:48:30,392 iteration 4587 : loss : 0.017376, loss_ce: 0.005699
2022-01-06 23:48:31,877 iteration 4588 : loss : 0.023336, loss_ce: 0.008479
2022-01-06 23:48:33,398 iteration 4589 : loss : 0.020057, loss_ce: 0.006983
2022-01-06 23:48:33,398 Training Data Eval:
2022-01-06 23:48:41,207   Average segmentation loss on training set: 0.0146
2022-01-06 23:48:41,208 Validation Data Eval:
2022-01-06 23:48:43,911   Average segmentation loss on validation set: 0.0705
2022-01-06 23:48:45,419 iteration 4590 : loss : 0.020509, loss_ce: 0.008779
 68%|██████████████████▏        | 270/400 [2:07:32<1:04:09, 29.61s/it]2022-01-06 23:48:46,992 iteration 4591 : loss : 0.034746, loss_ce: 0.011271
2022-01-06 23:48:48,551 iteration 4592 : loss : 0.031061, loss_ce: 0.010220
2022-01-06 23:48:50,075 iteration 4593 : loss : 0.022360, loss_ce: 0.009898
2022-01-06 23:48:51,641 iteration 4594 : loss : 0.041213, loss_ce: 0.013865
2022-01-06 23:48:53,146 iteration 4595 : loss : 0.020352, loss_ce: 0.006898
2022-01-06 23:48:54,644 iteration 4596 : loss : 0.018976, loss_ce: 0.005286
2022-01-06 23:48:56,201 iteration 4597 : loss : 0.018284, loss_ce: 0.007758
2022-01-06 23:48:57,815 iteration 4598 : loss : 0.033005, loss_ce: 0.012134
2022-01-06 23:48:59,416 iteration 4599 : loss : 0.028642, loss_ce: 0.013557
2022-01-06 23:49:00,961 iteration 4600 : loss : 0.024363, loss_ce: 0.010912
2022-01-06 23:49:02,541 iteration 4601 : loss : 0.026646, loss_ce: 0.011755
2022-01-06 23:49:04,191 iteration 4602 : loss : 0.023783, loss_ce: 0.006367
2022-01-06 23:49:05,766 iteration 4603 : loss : 0.019339, loss_ce: 0.007022
2022-01-06 23:49:07,356 iteration 4604 : loss : 0.020713, loss_ce: 0.009173
2022-01-06 23:49:08,948 iteration 4605 : loss : 0.061957, loss_ce: 0.017014
2022-01-06 23:49:10,538 iteration 4606 : loss : 0.020772, loss_ce: 0.008446
2022-01-06 23:49:12,127 iteration 4607 : loss : 0.016591, loss_ce: 0.008606
 68%|██████████████████▎        | 271/400 [2:07:58<1:01:47, 28.74s/it]2022-01-06 23:49:13,687 iteration 4608 : loss : 0.014675, loss_ce: 0.005558
2022-01-06 23:49:15,238 iteration 4609 : loss : 0.023375, loss_ce: 0.010421
2022-01-06 23:49:16,743 iteration 4610 : loss : 0.020967, loss_ce: 0.006680
2022-01-06 23:49:18,247 iteration 4611 : loss : 0.023449, loss_ce: 0.008975
2022-01-06 23:49:19,732 iteration 4612 : loss : 0.017996, loss_ce: 0.005748
2022-01-06 23:49:21,291 iteration 4613 : loss : 0.021353, loss_ce: 0.007344
2022-01-06 23:49:22,844 iteration 4614 : loss : 0.021387, loss_ce: 0.008753
2022-01-06 23:49:24,430 iteration 4615 : loss : 0.015920, loss_ce: 0.005661
2022-01-06 23:49:26,076 iteration 4616 : loss : 0.025331, loss_ce: 0.010700
2022-01-06 23:49:27,629 iteration 4617 : loss : 0.025923, loss_ce: 0.012785
2022-01-06 23:49:29,119 iteration 4618 : loss : 0.017858, loss_ce: 0.005541
2022-01-06 23:49:30,622 iteration 4619 : loss : 0.014429, loss_ce: 0.006395
2022-01-06 23:49:32,195 iteration 4620 : loss : 0.030844, loss_ce: 0.009556
2022-01-06 23:49:33,705 iteration 4621 : loss : 0.025242, loss_ce: 0.013051
2022-01-06 23:49:35,239 iteration 4622 : loss : 0.050518, loss_ce: 0.023812
2022-01-06 23:49:36,759 iteration 4623 : loss : 0.024282, loss_ce: 0.007840
2022-01-06 23:49:38,302 iteration 4624 : loss : 0.020646, loss_ce: 0.008752
 68%|███████████████████▋         | 272/400 [2:08:25<59:40, 27.97s/it]2022-01-06 23:49:39,927 iteration 4625 : loss : 0.027059, loss_ce: 0.010571
2022-01-06 23:49:41,510 iteration 4626 : loss : 0.019116, loss_ce: 0.007770
2022-01-06 23:49:43,135 iteration 4627 : loss : 0.025475, loss_ce: 0.010227
2022-01-06 23:49:44,641 iteration 4628 : loss : 0.021838, loss_ce: 0.007557
2022-01-06 23:49:46,092 iteration 4629 : loss : 0.019530, loss_ce: 0.006496
2022-01-06 23:49:47,597 iteration 4630 : loss : 0.017433, loss_ce: 0.006356
2022-01-06 23:49:49,122 iteration 4631 : loss : 0.026239, loss_ce: 0.009686
2022-01-06 23:49:50,652 iteration 4632 : loss : 0.018957, loss_ce: 0.006060
2022-01-06 23:49:52,159 iteration 4633 : loss : 0.020634, loss_ce: 0.006256
2022-01-06 23:49:53,644 iteration 4634 : loss : 0.017178, loss_ce: 0.007746
2022-01-06 23:49:55,096 iteration 4635 : loss : 0.013167, loss_ce: 0.005592
2022-01-06 23:49:56,689 iteration 4636 : loss : 0.024665, loss_ce: 0.010131
2022-01-06 23:49:58,183 iteration 4637 : loss : 0.025425, loss_ce: 0.007309
2022-01-06 23:49:59,795 iteration 4638 : loss : 0.032178, loss_ce: 0.014820
2022-01-06 23:50:01,339 iteration 4639 : loss : 0.025758, loss_ce: 0.009322
2022-01-06 23:50:02,959 iteration 4640 : loss : 0.040349, loss_ce: 0.014819
2022-01-06 23:50:04,504 iteration 4641 : loss : 0.026159, loss_ce: 0.010429
 68%|███████████████████▊         | 273/400 [2:08:51<58:05, 27.44s/it]2022-01-06 23:50:06,062 iteration 4642 : loss : 0.025207, loss_ce: 0.005862
2022-01-06 23:50:07,565 iteration 4643 : loss : 0.021984, loss_ce: 0.008251
2022-01-06 23:50:09,117 iteration 4644 : loss : 0.029115, loss_ce: 0.009591
2022-01-06 23:50:10,689 iteration 4645 : loss : 0.023854, loss_ce: 0.010792
2022-01-06 23:50:12,239 iteration 4646 : loss : 0.037285, loss_ce: 0.010912
2022-01-06 23:50:13,720 iteration 4647 : loss : 0.020602, loss_ce: 0.006455
2022-01-06 23:50:15,248 iteration 4648 : loss : 0.037830, loss_ce: 0.015539
2022-01-06 23:50:16,824 iteration 4649 : loss : 0.020353, loss_ce: 0.008294
2022-01-06 23:50:18,448 iteration 4650 : loss : 0.041125, loss_ce: 0.011798
2022-01-06 23:50:19,964 iteration 4651 : loss : 0.019730, loss_ce: 0.008696
2022-01-06 23:50:21,607 iteration 4652 : loss : 0.020208, loss_ce: 0.008570
2022-01-06 23:50:23,161 iteration 4653 : loss : 0.022862, loss_ce: 0.009992
2022-01-06 23:50:24,708 iteration 4654 : loss : 0.021227, loss_ce: 0.007140
2022-01-06 23:50:26,340 iteration 4655 : loss : 0.036201, loss_ce: 0.014676
2022-01-06 23:50:27,987 iteration 4656 : loss : 0.032537, loss_ce: 0.013309
2022-01-06 23:50:29,591 iteration 4657 : loss : 0.031506, loss_ce: 0.016882
2022-01-06 23:50:31,197 iteration 4658 : loss : 0.047254, loss_ce: 0.015315
 68%|███████████████████▊         | 274/400 [2:09:17<57:09, 27.21s/it]2022-01-06 23:50:32,849 iteration 4659 : loss : 0.026937, loss_ce: 0.011924
2022-01-06 23:50:34,506 iteration 4660 : loss : 0.026332, loss_ce: 0.010582
2022-01-06 23:50:36,091 iteration 4661 : loss : 0.034183, loss_ce: 0.012983
2022-01-06 23:50:37,604 iteration 4662 : loss : 0.015649, loss_ce: 0.006418
2022-01-06 23:50:39,179 iteration 4663 : loss : 0.025391, loss_ce: 0.010777
2022-01-06 23:50:40,707 iteration 4664 : loss : 0.018490, loss_ce: 0.006476
2022-01-06 23:50:42,243 iteration 4665 : loss : 0.020195, loss_ce: 0.006827
2022-01-06 23:50:43,702 iteration 4666 : loss : 0.016449, loss_ce: 0.005599
2022-01-06 23:50:45,225 iteration 4667 : loss : 0.016341, loss_ce: 0.006500
2022-01-06 23:50:46,895 iteration 4668 : loss : 0.026608, loss_ce: 0.012469
2022-01-06 23:50:48,413 iteration 4669 : loss : 0.017648, loss_ce: 0.007291
2022-01-06 23:50:49,942 iteration 4670 : loss : 0.022931, loss_ce: 0.009617
2022-01-06 23:50:51,447 iteration 4671 : loss : 0.029834, loss_ce: 0.010601
2022-01-06 23:50:53,081 iteration 4672 : loss : 0.034709, loss_ce: 0.014042
2022-01-06 23:50:54,687 iteration 4673 : loss : 0.028551, loss_ce: 0.013532
2022-01-06 23:50:56,162 iteration 4674 : loss : 0.017429, loss_ce: 0.004976
2022-01-06 23:50:56,162 Training Data Eval:
2022-01-06 23:51:04,057   Average segmentation loss on training set: 0.0160
2022-01-06 23:51:04,057 Validation Data Eval:
2022-01-06 23:51:06,774   Average segmentation loss on validation set: 0.0658
2022-01-06 23:51:14,485 Found new lowest validation loss at iteration 4674! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-06 23:51:16,003 iteration 4675 : loss : 0.020084, loss_ce: 0.008066
 69%|██████████████████▌        | 275/400 [2:10:02<1:07:41, 32.49s/it]2022-01-06 23:51:17,589 iteration 4676 : loss : 0.027565, loss_ce: 0.010065
2022-01-06 23:51:19,012 iteration 4677 : loss : 0.023557, loss_ce: 0.008166
2022-01-06 23:51:20,475 iteration 4678 : loss : 0.032090, loss_ce: 0.008678
2022-01-06 23:51:21,988 iteration 4679 : loss : 0.043641, loss_ce: 0.011998
2022-01-06 23:51:23,427 iteration 4680 : loss : 0.020282, loss_ce: 0.008204
2022-01-06 23:51:24,856 iteration 4681 : loss : 0.026672, loss_ce: 0.007181
2022-01-06 23:51:26,272 iteration 4682 : loss : 0.020253, loss_ce: 0.007651
2022-01-06 23:51:27,635 iteration 4683 : loss : 0.022982, loss_ce: 0.005850
2022-01-06 23:51:29,192 iteration 4684 : loss : 0.020003, loss_ce: 0.009574
2022-01-06 23:51:30,754 iteration 4685 : loss : 0.035213, loss_ce: 0.011573
2022-01-06 23:51:32,279 iteration 4686 : loss : 0.021146, loss_ce: 0.006472
2022-01-06 23:51:33,773 iteration 4687 : loss : 0.018321, loss_ce: 0.008582
2022-01-06 23:51:35,226 iteration 4688 : loss : 0.014143, loss_ce: 0.006538
2022-01-06 23:51:36,689 iteration 4689 : loss : 0.014095, loss_ce: 0.006187
2022-01-06 23:51:38,172 iteration 4690 : loss : 0.021759, loss_ce: 0.008557
2022-01-06 23:51:39,724 iteration 4691 : loss : 0.028743, loss_ce: 0.016214
2022-01-06 23:51:41,221 iteration 4692 : loss : 0.020474, loss_ce: 0.006047
 69%|██████████████████▋        | 276/400 [2:10:27<1:02:38, 30.31s/it]2022-01-06 23:51:42,852 iteration 4693 : loss : 0.029564, loss_ce: 0.011016
2022-01-06 23:51:44,385 iteration 4694 : loss : 0.035056, loss_ce: 0.010494
2022-01-06 23:51:45,780 iteration 4695 : loss : 0.014179, loss_ce: 0.004872
2022-01-06 23:51:47,375 iteration 4696 : loss : 0.035935, loss_ce: 0.013511
2022-01-06 23:51:48,887 iteration 4697 : loss : 0.020683, loss_ce: 0.007781
2022-01-06 23:51:50,420 iteration 4698 : loss : 0.019900, loss_ce: 0.009677
2022-01-06 23:51:52,009 iteration 4699 : loss : 0.020809, loss_ce: 0.010194
2022-01-06 23:51:53,489 iteration 4700 : loss : 0.024235, loss_ce: 0.010387
2022-01-06 23:51:55,104 iteration 4701 : loss : 0.028756, loss_ce: 0.012196
2022-01-06 23:51:56,607 iteration 4702 : loss : 0.020975, loss_ce: 0.008869
2022-01-06 23:51:58,105 iteration 4703 : loss : 0.025690, loss_ce: 0.006608
2022-01-06 23:51:59,638 iteration 4704 : loss : 0.033255, loss_ce: 0.012739
2022-01-06 23:52:01,075 iteration 4705 : loss : 0.015373, loss_ce: 0.006630
2022-01-06 23:52:02,641 iteration 4706 : loss : 0.035901, loss_ce: 0.008333
2022-01-06 23:52:04,262 iteration 4707 : loss : 0.026401, loss_ce: 0.009066
2022-01-06 23:52:05,887 iteration 4708 : loss : 0.021154, loss_ce: 0.006693
2022-01-06 23:52:07,486 iteration 4709 : loss : 0.039391, loss_ce: 0.013474
 69%|████████████████████         | 277/400 [2:10:54<59:39, 29.10s/it]2022-01-06 23:52:09,136 iteration 4710 : loss : 0.063329, loss_ce: 0.012844
2022-01-06 23:52:10,830 iteration 4711 : loss : 0.031787, loss_ce: 0.012843
2022-01-06 23:52:12,382 iteration 4712 : loss : 0.023654, loss_ce: 0.010036
2022-01-06 23:52:13,929 iteration 4713 : loss : 0.020836, loss_ce: 0.007898
2022-01-06 23:52:15,495 iteration 4714 : loss : 0.040712, loss_ce: 0.012148
2022-01-06 23:52:17,093 iteration 4715 : loss : 0.034331, loss_ce: 0.012559
2022-01-06 23:52:18,648 iteration 4716 : loss : 0.030196, loss_ce: 0.013462
2022-01-06 23:52:20,182 iteration 4717 : loss : 0.026409, loss_ce: 0.010406
2022-01-06 23:52:21,670 iteration 4718 : loss : 0.021783, loss_ce: 0.007503
2022-01-06 23:52:23,144 iteration 4719 : loss : 0.023311, loss_ce: 0.011062
2022-01-06 23:52:24,633 iteration 4720 : loss : 0.019602, loss_ce: 0.007969
2022-01-06 23:52:26,136 iteration 4721 : loss : 0.024468, loss_ce: 0.010499
2022-01-06 23:52:27,705 iteration 4722 : loss : 0.052823, loss_ce: 0.019760
2022-01-06 23:52:29,227 iteration 4723 : loss : 0.016459, loss_ce: 0.006114
2022-01-06 23:52:30,692 iteration 4724 : loss : 0.018767, loss_ce: 0.007104
2022-01-06 23:52:32,161 iteration 4725 : loss : 0.021247, loss_ce: 0.008613
2022-01-06 23:52:33,674 iteration 4726 : loss : 0.022661, loss_ce: 0.009044
 70%|████████████████████▏        | 278/400 [2:11:20<57:23, 28.23s/it]2022-01-06 23:52:35,296 iteration 4727 : loss : 0.021971, loss_ce: 0.006812
2022-01-06 23:52:36,761 iteration 4728 : loss : 0.021356, loss_ce: 0.008543
2022-01-06 23:52:38,240 iteration 4729 : loss : 0.020463, loss_ce: 0.007602
2022-01-06 23:52:39,721 iteration 4730 : loss : 0.029624, loss_ce: 0.012785
2022-01-06 23:52:41,293 iteration 4731 : loss : 0.028877, loss_ce: 0.009605
2022-01-06 23:52:42,824 iteration 4732 : loss : 0.019759, loss_ce: 0.006689
2022-01-06 23:52:44,337 iteration 4733 : loss : 0.021922, loss_ce: 0.008365
2022-01-06 23:52:45,823 iteration 4734 : loss : 0.022146, loss_ce: 0.009094
2022-01-06 23:52:47,410 iteration 4735 : loss : 0.029194, loss_ce: 0.010798
2022-01-06 23:52:48,863 iteration 4736 : loss : 0.024118, loss_ce: 0.009904
2022-01-06 23:52:50,454 iteration 4737 : loss : 0.029889, loss_ce: 0.012438
2022-01-06 23:52:52,005 iteration 4738 : loss : 0.026690, loss_ce: 0.010175
2022-01-06 23:52:53,561 iteration 4739 : loss : 0.021983, loss_ce: 0.007636
2022-01-06 23:52:55,164 iteration 4740 : loss : 0.033895, loss_ce: 0.018454
2022-01-06 23:52:56,708 iteration 4741 : loss : 0.017867, loss_ce: 0.006453
2022-01-06 23:52:58,250 iteration 4742 : loss : 0.022175, loss_ce: 0.010711
2022-01-06 23:52:59,888 iteration 4743 : loss : 0.021057, loss_ce: 0.006859
 70%|████████████████████▏        | 279/400 [2:11:46<55:42, 27.62s/it]2022-01-06 23:53:01,556 iteration 4744 : loss : 0.019819, loss_ce: 0.007272
2022-01-06 23:53:03,103 iteration 4745 : loss : 0.038178, loss_ce: 0.010972
2022-01-06 23:53:04,614 iteration 4746 : loss : 0.019676, loss_ce: 0.009476
2022-01-06 23:53:06,139 iteration 4747 : loss : 0.027030, loss_ce: 0.008266
2022-01-06 23:53:07,695 iteration 4748 : loss : 0.026000, loss_ce: 0.009089
2022-01-06 23:53:09,246 iteration 4749 : loss : 0.033286, loss_ce: 0.013248
2022-01-06 23:53:10,814 iteration 4750 : loss : 0.026739, loss_ce: 0.010283
2022-01-06 23:53:12,309 iteration 4751 : loss : 0.019082, loss_ce: 0.007578
2022-01-06 23:53:13,851 iteration 4752 : loss : 0.016210, loss_ce: 0.006028
2022-01-06 23:53:15,416 iteration 4753 : loss : 0.026524, loss_ce: 0.008688
2022-01-06 23:53:16,956 iteration 4754 : loss : 0.019036, loss_ce: 0.006438
2022-01-06 23:53:18,578 iteration 4755 : loss : 0.027734, loss_ce: 0.008831
2022-01-06 23:53:20,069 iteration 4756 : loss : 0.018701, loss_ce: 0.009154
2022-01-06 23:53:21,585 iteration 4757 : loss : 0.023745, loss_ce: 0.009737
2022-01-06 23:53:23,073 iteration 4758 : loss : 0.020134, loss_ce: 0.006788
2022-01-06 23:53:24,613 iteration 4759 : loss : 0.015847, loss_ce: 0.005502
2022-01-06 23:53:24,613 Training Data Eval:
2022-01-06 23:53:32,462   Average segmentation loss on training set: 0.0188
2022-01-06 23:53:32,462 Validation Data Eval:
2022-01-06 23:53:35,153   Average segmentation loss on validation set: 0.0745
2022-01-06 23:53:36,659 iteration 4760 : loss : 0.019425, loss_ce: 0.008420
 70%|██████████████████▉        | 280/400 [2:12:23<1:00:44, 30.37s/it]2022-01-06 23:53:38,223 iteration 4761 : loss : 0.024797, loss_ce: 0.007446
2022-01-06 23:53:39,769 iteration 4762 : loss : 0.023262, loss_ce: 0.009350
2022-01-06 23:53:41,248 iteration 4763 : loss : 0.016677, loss_ce: 0.006959
2022-01-06 23:53:42,806 iteration 4764 : loss : 0.020117, loss_ce: 0.007160
2022-01-06 23:53:44,315 iteration 4765 : loss : 0.026342, loss_ce: 0.011483
2022-01-06 23:53:45,910 iteration 4766 : loss : 0.027139, loss_ce: 0.008607
2022-01-06 23:53:47,469 iteration 4767 : loss : 0.027619, loss_ce: 0.010492
2022-01-06 23:53:48,965 iteration 4768 : loss : 0.021154, loss_ce: 0.007062
2022-01-06 23:53:50,530 iteration 4769 : loss : 0.029717, loss_ce: 0.011222
2022-01-06 23:53:52,123 iteration 4770 : loss : 0.025422, loss_ce: 0.009691
2022-01-06 23:53:53,592 iteration 4771 : loss : 0.021238, loss_ce: 0.007366
2022-01-06 23:53:55,157 iteration 4772 : loss : 0.022941, loss_ce: 0.009564
2022-01-06 23:53:56,699 iteration 4773 : loss : 0.020887, loss_ce: 0.007025
2022-01-06 23:53:58,241 iteration 4774 : loss : 0.020072, loss_ce: 0.008887
2022-01-06 23:53:59,722 iteration 4775 : loss : 0.036708, loss_ce: 0.010763
2022-01-06 23:54:01,227 iteration 4776 : loss : 0.020050, loss_ce: 0.011538
2022-01-06 23:54:02,861 iteration 4777 : loss : 0.025397, loss_ce: 0.012476
 70%|████████████████████▎        | 281/400 [2:12:49<57:44, 29.12s/it]2022-01-06 23:54:04,355 iteration 4778 : loss : 0.013911, loss_ce: 0.004138
2022-01-06 23:54:05,854 iteration 4779 : loss : 0.021659, loss_ce: 0.007840
2022-01-06 23:54:07,315 iteration 4780 : loss : 0.015392, loss_ce: 0.005172
2022-01-06 23:54:08,803 iteration 4781 : loss : 0.030372, loss_ce: 0.009318
2022-01-06 23:54:10,373 iteration 4782 : loss : 0.029013, loss_ce: 0.015370
2022-01-06 23:54:11,889 iteration 4783 : loss : 0.028465, loss_ce: 0.012261
2022-01-06 23:54:13,345 iteration 4784 : loss : 0.014992, loss_ce: 0.006379
2022-01-06 23:54:14,916 iteration 4785 : loss : 0.025462, loss_ce: 0.008943
2022-01-06 23:54:16,455 iteration 4786 : loss : 0.023125, loss_ce: 0.008565
2022-01-06 23:54:17,992 iteration 4787 : loss : 0.037395, loss_ce: 0.012267
2022-01-06 23:54:19,538 iteration 4788 : loss : 0.025736, loss_ce: 0.009468
2022-01-06 23:54:20,995 iteration 4789 : loss : 0.017661, loss_ce: 0.008673
2022-01-06 23:54:22,596 iteration 4790 : loss : 0.033053, loss_ce: 0.012135
2022-01-06 23:54:24,095 iteration 4791 : loss : 0.022879, loss_ce: 0.011140
2022-01-06 23:54:25,648 iteration 4792 : loss : 0.041815, loss_ce: 0.011302
2022-01-06 23:54:27,117 iteration 4793 : loss : 0.015424, loss_ce: 0.005968
2022-01-06 23:54:28,566 iteration 4794 : loss : 0.026534, loss_ce: 0.007643
 70%|████████████████████▍        | 282/400 [2:13:15<55:14, 28.09s/it]2022-01-06 23:54:30,201 iteration 4795 : loss : 0.032173, loss_ce: 0.010551
2022-01-06 23:54:31,779 iteration 4796 : loss : 0.015984, loss_ce: 0.005245
2022-01-06 23:54:33,368 iteration 4797 : loss : 0.027402, loss_ce: 0.012318
2022-01-06 23:54:34,999 iteration 4798 : loss : 0.026750, loss_ce: 0.009404
2022-01-06 23:54:36,564 iteration 4799 : loss : 0.025480, loss_ce: 0.008550
2022-01-06 23:54:38,138 iteration 4800 : loss : 0.022233, loss_ce: 0.006893
2022-01-06 23:54:39,771 iteration 4801 : loss : 0.022629, loss_ce: 0.009270
2022-01-06 23:54:41,259 iteration 4802 : loss : 0.018394, loss_ce: 0.007643
2022-01-06 23:54:42,790 iteration 4803 : loss : 0.019685, loss_ce: 0.009001
2022-01-06 23:54:44,403 iteration 4804 : loss : 0.061746, loss_ce: 0.015982
2022-01-06 23:54:46,003 iteration 4805 : loss : 0.040096, loss_ce: 0.014773
2022-01-06 23:54:47,587 iteration 4806 : loss : 0.023382, loss_ce: 0.009070
2022-01-06 23:54:49,059 iteration 4807 : loss : 0.022065, loss_ce: 0.008797
2022-01-06 23:54:50,538 iteration 4808 : loss : 0.020494, loss_ce: 0.009030
2022-01-06 23:54:52,111 iteration 4809 : loss : 0.033631, loss_ce: 0.018126
2022-01-06 23:54:53,674 iteration 4810 : loss : 0.022768, loss_ce: 0.005970
2022-01-06 23:54:55,252 iteration 4811 : loss : 0.033012, loss_ce: 0.011884
 71%|████████████████████▌        | 283/400 [2:13:41<53:57, 27.67s/it]2022-01-06 23:54:56,809 iteration 4812 : loss : 0.019791, loss_ce: 0.007649
2022-01-06 23:54:58,380 iteration 4813 : loss : 0.017155, loss_ce: 0.003777
2022-01-06 23:54:59,972 iteration 4814 : loss : 0.043864, loss_ce: 0.010409
2022-01-06 23:55:01,455 iteration 4815 : loss : 0.017012, loss_ce: 0.007148
2022-01-06 23:55:03,095 iteration 4816 : loss : 0.024388, loss_ce: 0.009452
2022-01-06 23:55:04,627 iteration 4817 : loss : 0.021955, loss_ce: 0.008472
2022-01-06 23:55:06,142 iteration 4818 : loss : 0.019604, loss_ce: 0.007066
2022-01-06 23:55:07,598 iteration 4819 : loss : 0.021250, loss_ce: 0.008295
2022-01-06 23:55:09,099 iteration 4820 : loss : 0.024622, loss_ce: 0.010889
2022-01-06 23:55:10,717 iteration 4821 : loss : 0.032308, loss_ce: 0.013756
2022-01-06 23:55:12,313 iteration 4822 : loss : 0.021864, loss_ce: 0.007889
2022-01-06 23:55:13,845 iteration 4823 : loss : 0.021950, loss_ce: 0.009251
2022-01-06 23:55:15,436 iteration 4824 : loss : 0.023859, loss_ce: 0.010243
2022-01-06 23:55:16,986 iteration 4825 : loss : 0.025557, loss_ce: 0.010462
2022-01-06 23:55:18,527 iteration 4826 : loss : 0.019777, loss_ce: 0.006024
2022-01-06 23:55:20,079 iteration 4827 : loss : 0.016620, loss_ce: 0.007009
2022-01-06 23:55:21,603 iteration 4828 : loss : 0.019621, loss_ce: 0.008182
 71%|████████████████████▌        | 284/400 [2:14:08<52:44, 27.28s/it]2022-01-06 23:55:23,174 iteration 4829 : loss : 0.019531, loss_ce: 0.006395
2022-01-06 23:55:24,762 iteration 4830 : loss : 0.019417, loss_ce: 0.008983
2022-01-06 23:55:26,339 iteration 4831 : loss : 0.019771, loss_ce: 0.008468
2022-01-06 23:55:27,887 iteration 4832 : loss : 0.021790, loss_ce: 0.006788
2022-01-06 23:55:29,481 iteration 4833 : loss : 0.031905, loss_ce: 0.013900
2022-01-06 23:55:30,981 iteration 4834 : loss : 0.017467, loss_ce: 0.006156
2022-01-06 23:55:32,639 iteration 4835 : loss : 0.022046, loss_ce: 0.010494
2022-01-06 23:55:34,176 iteration 4836 : loss : 0.042549, loss_ce: 0.011882
2022-01-06 23:55:35,757 iteration 4837 : loss : 0.027669, loss_ce: 0.007883
2022-01-06 23:55:37,271 iteration 4838 : loss : 0.017986, loss_ce: 0.005508
2022-01-06 23:55:38,826 iteration 4839 : loss : 0.026914, loss_ce: 0.010129
2022-01-06 23:55:40,351 iteration 4840 : loss : 0.020771, loss_ce: 0.010294
2022-01-06 23:55:41,803 iteration 4841 : loss : 0.013671, loss_ce: 0.004367
2022-01-06 23:55:43,335 iteration 4842 : loss : 0.019511, loss_ce: 0.006732
2022-01-06 23:55:44,954 iteration 4843 : loss : 0.025595, loss_ce: 0.007817
2022-01-06 23:55:46,488 iteration 4844 : loss : 0.041424, loss_ce: 0.014372
2022-01-06 23:55:46,488 Training Data Eval:
2022-01-06 23:55:54,395   Average segmentation loss on training set: 0.0546
2022-01-06 23:55:54,396 Validation Data Eval:
2022-01-06 23:55:57,139   Average segmentation loss on validation set: 0.1606
2022-01-06 23:55:58,712 iteration 4845 : loss : 0.028665, loss_ce: 0.014326
 71%|████████████████████▋        | 285/400 [2:14:45<57:55, 30.23s/it]2022-01-06 23:56:00,255 iteration 4846 : loss : 0.021531, loss_ce: 0.007538
2022-01-06 23:56:01,750 iteration 4847 : loss : 0.018039, loss_ce: 0.006257
2022-01-06 23:56:03,323 iteration 4848 : loss : 0.029476, loss_ce: 0.013342
2022-01-06 23:56:04,867 iteration 4849 : loss : 0.021616, loss_ce: 0.005599
2022-01-06 23:56:06,358 iteration 4850 : loss : 0.022646, loss_ce: 0.006452
2022-01-06 23:56:07,985 iteration 4851 : loss : 0.021636, loss_ce: 0.009622
2022-01-06 23:56:09,492 iteration 4852 : loss : 0.017694, loss_ce: 0.006080
2022-01-06 23:56:11,010 iteration 4853 : loss : 0.019766, loss_ce: 0.008321
2022-01-06 23:56:12,528 iteration 4854 : loss : 0.020249, loss_ce: 0.007843
2022-01-06 23:56:14,021 iteration 4855 : loss : 0.026005, loss_ce: 0.006594
2022-01-06 23:56:15,626 iteration 4856 : loss : 0.035133, loss_ce: 0.014694
2022-01-06 23:56:17,103 iteration 4857 : loss : 0.014923, loss_ce: 0.005245
2022-01-06 23:56:18,690 iteration 4858 : loss : 0.022863, loss_ce: 0.009320
2022-01-06 23:56:20,144 iteration 4859 : loss : 0.013819, loss_ce: 0.005514
2022-01-06 23:56:21,704 iteration 4860 : loss : 0.022046, loss_ce: 0.009215
2022-01-06 23:56:23,204 iteration 4861 : loss : 0.022739, loss_ce: 0.007462
2022-01-06 23:56:24,765 iteration 4862 : loss : 0.026902, loss_ce: 0.013203
 72%|████████████████████▋        | 286/400 [2:15:11<55:02, 28.97s/it]2022-01-06 23:56:26,407 iteration 4863 : loss : 0.021341, loss_ce: 0.005495
2022-01-06 23:56:27,909 iteration 4864 : loss : 0.017788, loss_ce: 0.006508
2022-01-06 23:56:29,386 iteration 4865 : loss : 0.017020, loss_ce: 0.008459
2022-01-06 23:56:30,955 iteration 4866 : loss : 0.028324, loss_ce: 0.008030
2022-01-06 23:56:32,466 iteration 4867 : loss : 0.017495, loss_ce: 0.006156
2022-01-06 23:56:33,889 iteration 4868 : loss : 0.018362, loss_ce: 0.007525
2022-01-06 23:56:35,499 iteration 4869 : loss : 0.031032, loss_ce: 0.013749
2022-01-06 23:56:37,019 iteration 4870 : loss : 0.018525, loss_ce: 0.007585
2022-01-06 23:56:38,629 iteration 4871 : loss : 0.023026, loss_ce: 0.007906
2022-01-06 23:56:40,092 iteration 4872 : loss : 0.016038, loss_ce: 0.004701
2022-01-06 23:56:41,703 iteration 4873 : loss : 0.019891, loss_ce: 0.008344
2022-01-06 23:56:43,249 iteration 4874 : loss : 0.023232, loss_ce: 0.007184
2022-01-06 23:56:44,744 iteration 4875 : loss : 0.022730, loss_ce: 0.008629
2022-01-06 23:56:46,186 iteration 4876 : loss : 0.022342, loss_ce: 0.007481
2022-01-06 23:56:47,761 iteration 4877 : loss : 0.024993, loss_ce: 0.013208
2022-01-06 23:56:49,339 iteration 4878 : loss : 0.043035, loss_ce: 0.015353
2022-01-06 23:56:50,840 iteration 4879 : loss : 0.017785, loss_ce: 0.005839
 72%|████████████████████▊        | 287/400 [2:15:37<52:55, 28.10s/it]2022-01-06 23:56:52,377 iteration 4880 : loss : 0.020822, loss_ce: 0.008414
2022-01-06 23:56:53,842 iteration 4881 : loss : 0.019238, loss_ce: 0.007686
2022-01-06 23:56:55,280 iteration 4882 : loss : 0.014162, loss_ce: 0.006305
2022-01-06 23:56:56,830 iteration 4883 : loss : 0.017319, loss_ce: 0.006778
2022-01-06 23:56:58,389 iteration 4884 : loss : 0.019325, loss_ce: 0.005968
2022-01-06 23:56:59,813 iteration 4885 : loss : 0.014378, loss_ce: 0.005620
2022-01-06 23:57:01,322 iteration 4886 : loss : 0.024887, loss_ce: 0.006951
2022-01-06 23:57:02,776 iteration 4887 : loss : 0.014173, loss_ce: 0.004396
2022-01-06 23:57:04,306 iteration 4888 : loss : 0.044582, loss_ce: 0.011657
2022-01-06 23:57:05,816 iteration 4889 : loss : 0.023977, loss_ce: 0.010952
2022-01-06 23:57:07,281 iteration 4890 : loss : 0.019470, loss_ce: 0.010396
2022-01-06 23:57:08,684 iteration 4891 : loss : 0.013603, loss_ce: 0.004714
2022-01-06 23:57:10,217 iteration 4892 : loss : 0.019024, loss_ce: 0.007337
2022-01-06 23:57:11,715 iteration 4893 : loss : 0.021294, loss_ce: 0.009473
2022-01-06 23:57:13,199 iteration 4894 : loss : 0.017723, loss_ce: 0.008205
2022-01-06 23:57:14,682 iteration 4895 : loss : 0.022508, loss_ce: 0.005582
2022-01-06 23:57:16,145 iteration 4896 : loss : 0.022977, loss_ce: 0.006871
 72%|████████████████████▉        | 288/400 [2:16:02<50:53, 27.27s/it]2022-01-06 23:57:17,668 iteration 4897 : loss : 0.019341, loss_ce: 0.007227
2022-01-06 23:57:19,222 iteration 4898 : loss : 0.040422, loss_ce: 0.023214
2022-01-06 23:57:20,727 iteration 4899 : loss : 0.022508, loss_ce: 0.007157
2022-01-06 23:57:22,199 iteration 4900 : loss : 0.023237, loss_ce: 0.008639
2022-01-06 23:57:23,836 iteration 4901 : loss : 0.032564, loss_ce: 0.013100
2022-01-06 23:57:25,315 iteration 4902 : loss : 0.022008, loss_ce: 0.008706
2022-01-06 23:57:26,846 iteration 4903 : loss : 0.018008, loss_ce: 0.009304
2022-01-06 23:57:28,261 iteration 4904 : loss : 0.021754, loss_ce: 0.006689
2022-01-06 23:57:29,724 iteration 4905 : loss : 0.024163, loss_ce: 0.006784
2022-01-06 23:57:31,246 iteration 4906 : loss : 0.021924, loss_ce: 0.008271
2022-01-06 23:57:32,716 iteration 4907 : loss : 0.021073, loss_ce: 0.008521
2022-01-06 23:57:34,295 iteration 4908 : loss : 0.025429, loss_ce: 0.011348
2022-01-06 23:57:35,822 iteration 4909 : loss : 0.018782, loss_ce: 0.005853
2022-01-06 23:57:37,282 iteration 4910 : loss : 0.019469, loss_ce: 0.009140
2022-01-06 23:57:38,751 iteration 4911 : loss : 0.015857, loss_ce: 0.006029
2022-01-06 23:57:40,253 iteration 4912 : loss : 0.020191, loss_ce: 0.009177
2022-01-06 23:57:41,740 iteration 4913 : loss : 0.026596, loss_ce: 0.009878
 72%|████████████████████▉        | 289/400 [2:16:28<49:30, 26.76s/it]2022-01-06 23:57:43,321 iteration 4914 : loss : 0.027550, loss_ce: 0.011616
2022-01-06 23:57:44,780 iteration 4915 : loss : 0.026879, loss_ce: 0.010147
2022-01-06 23:57:46,259 iteration 4916 : loss : 0.019317, loss_ce: 0.005370
2022-01-06 23:57:47,756 iteration 4917 : loss : 0.021730, loss_ce: 0.007275
2022-01-06 23:57:49,277 iteration 4918 : loss : 0.025078, loss_ce: 0.009642
2022-01-06 23:57:50,914 iteration 4919 : loss : 0.039564, loss_ce: 0.019221
2022-01-06 23:57:52,392 iteration 4920 : loss : 0.015601, loss_ce: 0.006399
2022-01-06 23:57:53,847 iteration 4921 : loss : 0.017072, loss_ce: 0.008717
2022-01-06 23:57:55,300 iteration 4922 : loss : 0.020304, loss_ce: 0.005489
2022-01-06 23:57:56,844 iteration 4923 : loss : 0.025910, loss_ce: 0.010584
2022-01-06 23:57:58,415 iteration 4924 : loss : 0.026602, loss_ce: 0.014272
2022-01-06 23:57:59,940 iteration 4925 : loss : 0.017535, loss_ce: 0.006682
2022-01-06 23:58:01,488 iteration 4926 : loss : 0.022592, loss_ce: 0.010338
2022-01-06 23:58:02,972 iteration 4927 : loss : 0.024413, loss_ce: 0.007605
2022-01-06 23:58:04,521 iteration 4928 : loss : 0.030422, loss_ce: 0.011712
2022-01-06 23:58:06,095 iteration 4929 : loss : 0.027945, loss_ce: 0.012254
2022-01-06 23:58:06,095 Training Data Eval:
2022-01-06 23:58:13,846   Average segmentation loss on training set: 0.0170
2022-01-06 23:58:13,846 Validation Data Eval:
2022-01-06 23:58:16,538   Average segmentation loss on validation set: 0.0811
2022-01-06 23:58:18,070 iteration 4930 : loss : 0.019618, loss_ce: 0.006596
 72%|█████████████████████        | 290/400 [2:17:04<54:19, 29.63s/it]2022-01-06 23:58:19,666 iteration 4931 : loss : 0.019279, loss_ce: 0.008870
2022-01-06 23:58:21,206 iteration 4932 : loss : 0.020774, loss_ce: 0.010192
2022-01-06 23:58:22,676 iteration 4933 : loss : 0.020948, loss_ce: 0.010114
2022-01-06 23:58:24,257 iteration 4934 : loss : 0.033278, loss_ce: 0.010047
2022-01-06 23:58:25,782 iteration 4935 : loss : 0.020682, loss_ce: 0.006897
2022-01-06 23:58:27,381 iteration 4936 : loss : 0.025726, loss_ce: 0.008882
2022-01-06 23:58:28,938 iteration 4937 : loss : 0.028369, loss_ce: 0.014429
2022-01-06 23:58:30,535 iteration 4938 : loss : 0.042797, loss_ce: 0.014799
2022-01-06 23:58:32,136 iteration 4939 : loss : 0.031392, loss_ce: 0.011482
2022-01-06 23:58:33,690 iteration 4940 : loss : 0.026539, loss_ce: 0.010565
2022-01-06 23:58:35,279 iteration 4941 : loss : 0.022373, loss_ce: 0.010392
2022-01-06 23:58:36,869 iteration 4942 : loss : 0.028750, loss_ce: 0.008986
2022-01-06 23:58:38,535 iteration 4943 : loss : 0.037262, loss_ce: 0.007909
2022-01-06 23:58:40,110 iteration 4944 : loss : 0.027326, loss_ce: 0.008877
2022-01-06 23:58:41,726 iteration 4945 : loss : 0.034409, loss_ce: 0.014687
2022-01-06 23:58:43,355 iteration 4946 : loss : 0.034717, loss_ce: 0.010786
2022-01-06 23:58:44,915 iteration 4947 : loss : 0.025955, loss_ce: 0.008302
 73%|█████████████████████        | 291/400 [2:17:31<52:18, 28.80s/it]2022-01-06 23:58:46,559 iteration 4948 : loss : 0.029991, loss_ce: 0.010693
2022-01-06 23:58:48,109 iteration 4949 : loss : 0.028346, loss_ce: 0.012279
2022-01-06 23:58:49,718 iteration 4950 : loss : 0.043669, loss_ce: 0.020127
2022-01-06 23:58:51,320 iteration 4951 : loss : 0.022193, loss_ce: 0.007935
2022-01-06 23:58:52,914 iteration 4952 : loss : 0.020490, loss_ce: 0.011343
2022-01-06 23:58:54,530 iteration 4953 : loss : 0.021661, loss_ce: 0.008851
2022-01-06 23:58:56,078 iteration 4954 : loss : 0.015621, loss_ce: 0.005170
2022-01-06 23:58:57,592 iteration 4955 : loss : 0.027431, loss_ce: 0.008240
2022-01-06 23:58:59,148 iteration 4956 : loss : 0.033673, loss_ce: 0.009933
2022-01-06 23:59:00,634 iteration 4957 : loss : 0.019277, loss_ce: 0.007508
2022-01-06 23:59:02,202 iteration 4958 : loss : 0.037800, loss_ce: 0.017815
2022-01-06 23:59:03,780 iteration 4959 : loss : 0.023972, loss_ce: 0.009098
2022-01-06 23:59:05,276 iteration 4960 : loss : 0.017910, loss_ce: 0.006380
2022-01-06 23:59:06,846 iteration 4961 : loss : 0.019015, loss_ce: 0.007533
2022-01-06 23:59:08,345 iteration 4962 : loss : 0.035096, loss_ce: 0.007389
2022-01-06 23:59:09,885 iteration 4963 : loss : 0.022103, loss_ce: 0.008466
2022-01-06 23:59:11,489 iteration 4964 : loss : 0.023697, loss_ce: 0.008657
 73%|█████████████████████▏       | 292/400 [2:17:58<50:38, 28.13s/it]2022-01-06 23:59:13,154 iteration 4965 : loss : 0.019957, loss_ce: 0.008820
2022-01-06 23:59:14,752 iteration 4966 : loss : 0.031169, loss_ce: 0.012067
2022-01-06 23:59:16,313 iteration 4967 : loss : 0.028333, loss_ce: 0.014198
2022-01-06 23:59:17,879 iteration 4968 : loss : 0.020424, loss_ce: 0.009061
2022-01-06 23:59:19,522 iteration 4969 : loss : 0.058374, loss_ce: 0.018622
2022-01-06 23:59:21,017 iteration 4970 : loss : 0.022570, loss_ce: 0.010831
2022-01-06 23:59:22,536 iteration 4971 : loss : 0.025345, loss_ce: 0.013422
2022-01-06 23:59:24,106 iteration 4972 : loss : 0.020211, loss_ce: 0.008660
2022-01-06 23:59:25,636 iteration 4973 : loss : 0.020187, loss_ce: 0.007367
2022-01-06 23:59:27,281 iteration 4974 : loss : 0.038033, loss_ce: 0.012317
2022-01-06 23:59:28,801 iteration 4975 : loss : 0.019923, loss_ce: 0.005798
2022-01-06 23:59:30,275 iteration 4976 : loss : 0.019089, loss_ce: 0.007213
2022-01-06 23:59:31,839 iteration 4977 : loss : 0.026543, loss_ce: 0.006416
2022-01-06 23:59:33,352 iteration 4978 : loss : 0.024039, loss_ce: 0.005844
2022-01-06 23:59:34,859 iteration 4979 : loss : 0.024123, loss_ce: 0.009721
2022-01-06 23:59:36,390 iteration 4980 : loss : 0.023401, loss_ce: 0.006453
2022-01-06 23:59:37,901 iteration 4981 : loss : 0.022001, loss_ce: 0.008172
 73%|█████████████████████▏       | 293/400 [2:18:24<49:14, 27.62s/it]2022-01-06 23:59:39,461 iteration 4982 : loss : 0.019356, loss_ce: 0.008838
2022-01-06 23:59:40,965 iteration 4983 : loss : 0.023626, loss_ce: 0.010863
2022-01-06 23:59:42,572 iteration 4984 : loss : 0.023972, loss_ce: 0.008666
2022-01-06 23:59:44,014 iteration 4985 : loss : 0.017596, loss_ce: 0.005055
2022-01-06 23:59:45,602 iteration 4986 : loss : 0.023910, loss_ce: 0.010485
2022-01-06 23:59:47,077 iteration 4987 : loss : 0.023284, loss_ce: 0.007551
2022-01-06 23:59:48,610 iteration 4988 : loss : 0.021784, loss_ce: 0.006672
2022-01-06 23:59:50,184 iteration 4989 : loss : 0.025162, loss_ce: 0.009130
2022-01-06 23:59:51,782 iteration 4990 : loss : 0.026396, loss_ce: 0.011529
2022-01-06 23:59:53,321 iteration 4991 : loss : 0.022847, loss_ce: 0.007383
2022-01-06 23:59:54,772 iteration 4992 : loss : 0.019321, loss_ce: 0.009595
2022-01-06 23:59:56,287 iteration 4993 : loss : 0.022347, loss_ce: 0.007071
2022-01-06 23:59:57,764 iteration 4994 : loss : 0.022982, loss_ce: 0.006969
2022-01-06 23:59:59,258 iteration 4995 : loss : 0.022085, loss_ce: 0.010007
2022-01-07 00:00:00,839 iteration 4996 : loss : 0.033171, loss_ce: 0.009248
2022-01-07 00:00:02,292 iteration 4997 : loss : 0.017972, loss_ce: 0.005274
2022-01-07 00:00:03,766 iteration 4998 : loss : 0.028361, loss_ce: 0.010908
 74%|█████████████████████▎       | 294/400 [2:18:50<47:51, 27.09s/it]2022-01-07 00:00:05,308 iteration 4999 : loss : 0.017249, loss_ce: 0.006195
2022-01-07 00:00:06,805 iteration 5000 : loss : 0.025471, loss_ce: 0.010056
2022-01-07 00:00:08,303 iteration 5001 : loss : 0.019404, loss_ce: 0.008377
2022-01-07 00:00:09,800 iteration 5002 : loss : 0.015632, loss_ce: 0.005686
2022-01-07 00:00:11,333 iteration 5003 : loss : 0.035956, loss_ce: 0.010033
2022-01-07 00:00:12,833 iteration 5004 : loss : 0.032185, loss_ce: 0.011134
2022-01-07 00:00:14,347 iteration 5005 : loss : 0.022289, loss_ce: 0.010697
2022-01-07 00:00:15,910 iteration 5006 : loss : 0.024427, loss_ce: 0.010322
2022-01-07 00:00:17,451 iteration 5007 : loss : 0.026662, loss_ce: 0.012075
2022-01-07 00:00:19,003 iteration 5008 : loss : 0.027531, loss_ce: 0.013372
2022-01-07 00:00:20,556 iteration 5009 : loss : 0.021481, loss_ce: 0.005541
2022-01-07 00:00:22,098 iteration 5010 : loss : 0.023990, loss_ce: 0.008235
2022-01-07 00:00:23,599 iteration 5011 : loss : 0.017100, loss_ce: 0.007357
2022-01-07 00:00:25,139 iteration 5012 : loss : 0.030954, loss_ce: 0.011834
2022-01-07 00:00:26,602 iteration 5013 : loss : 0.022450, loss_ce: 0.008019
2022-01-07 00:00:28,100 iteration 5014 : loss : 0.025258, loss_ce: 0.009101
2022-01-07 00:00:28,100 Training Data Eval:
2022-01-07 00:00:35,885   Average segmentation loss on training set: 0.0129
2022-01-07 00:00:35,885 Validation Data Eval:
2022-01-07 00:00:38,585   Average segmentation loss on validation set: 0.0689
2022-01-07 00:00:40,106 iteration 5015 : loss : 0.017467, loss_ce: 0.005542
 74%|█████████████████████▍       | 295/400 [2:19:26<52:15, 29.86s/it]2022-01-07 00:00:41,688 iteration 5016 : loss : 0.024510, loss_ce: 0.012026
2022-01-07 00:00:43,182 iteration 5017 : loss : 0.020430, loss_ce: 0.006720
2022-01-07 00:00:44,696 iteration 5018 : loss : 0.023976, loss_ce: 0.010677
2022-01-07 00:00:46,229 iteration 5019 : loss : 0.017153, loss_ce: 0.005478
2022-01-07 00:00:47,765 iteration 5020 : loss : 0.025328, loss_ce: 0.008420
2022-01-07 00:00:49,313 iteration 5021 : loss : 0.025718, loss_ce: 0.014419
2022-01-07 00:00:50,774 iteration 5022 : loss : 0.017274, loss_ce: 0.005429
2022-01-07 00:00:52,279 iteration 5023 : loss : 0.026843, loss_ce: 0.006861
2022-01-07 00:00:53,877 iteration 5024 : loss : 0.036681, loss_ce: 0.007074
2022-01-07 00:00:55,447 iteration 5025 : loss : 0.031874, loss_ce: 0.005679
2022-01-07 00:00:56,955 iteration 5026 : loss : 0.019568, loss_ce: 0.007255
2022-01-07 00:00:58,431 iteration 5027 : loss : 0.017021, loss_ce: 0.007171
2022-01-07 00:00:59,927 iteration 5028 : loss : 0.021781, loss_ce: 0.008596
2022-01-07 00:01:01,534 iteration 5029 : loss : 0.033224, loss_ce: 0.007484
2022-01-07 00:01:03,057 iteration 5030 : loss : 0.019118, loss_ce: 0.010122
2022-01-07 00:01:04,645 iteration 5031 : loss : 0.020838, loss_ce: 0.009742
2022-01-07 00:01:06,174 iteration 5032 : loss : 0.027515, loss_ce: 0.014279
 74%|█████████████████████▍       | 296/400 [2:19:52<49:47, 28.73s/it]2022-01-07 00:01:07,867 iteration 5033 : loss : 0.028814, loss_ce: 0.010097
2022-01-07 00:01:09,370 iteration 5034 : loss : 0.022478, loss_ce: 0.006893
2022-01-07 00:01:10,866 iteration 5035 : loss : 0.016844, loss_ce: 0.006864
2022-01-07 00:01:12,366 iteration 5036 : loss : 0.027997, loss_ce: 0.009758
2022-01-07 00:01:13,889 iteration 5037 : loss : 0.027285, loss_ce: 0.009486
2022-01-07 00:01:15,377 iteration 5038 : loss : 0.028093, loss_ce: 0.006981
2022-01-07 00:01:16,969 iteration 5039 : loss : 0.018344, loss_ce: 0.009302
2022-01-07 00:01:18,483 iteration 5040 : loss : 0.032300, loss_ce: 0.011186
2022-01-07 00:01:20,059 iteration 5041 : loss : 0.020549, loss_ce: 0.006967
2022-01-07 00:01:21,540 iteration 5042 : loss : 0.024087, loss_ce: 0.009892
2022-01-07 00:01:23,057 iteration 5043 : loss : 0.018599, loss_ce: 0.008361
2022-01-07 00:01:24,587 iteration 5044 : loss : 0.037533, loss_ce: 0.019603
2022-01-07 00:01:26,149 iteration 5045 : loss : 0.027413, loss_ce: 0.013650
2022-01-07 00:01:27,663 iteration 5046 : loss : 0.026059, loss_ce: 0.007408
2022-01-07 00:01:29,152 iteration 5047 : loss : 0.025764, loss_ce: 0.007047
2022-01-07 00:01:30,635 iteration 5048 : loss : 0.018353, loss_ce: 0.006933
2022-01-07 00:01:32,127 iteration 5049 : loss : 0.017242, loss_ce: 0.007185
 74%|█████████████████████▌       | 297/400 [2:20:18<47:53, 27.89s/it]2022-01-07 00:01:33,696 iteration 5050 : loss : 0.028453, loss_ce: 0.009237
2022-01-07 00:01:35,197 iteration 5051 : loss : 0.021287, loss_ce: 0.008083
2022-01-07 00:01:36,693 iteration 5052 : loss : 0.017650, loss_ce: 0.007873
2022-01-07 00:01:38,183 iteration 5053 : loss : 0.030110, loss_ce: 0.011017
2022-01-07 00:01:39,696 iteration 5054 : loss : 0.016962, loss_ce: 0.005908
2022-01-07 00:01:41,184 iteration 5055 : loss : 0.021579, loss_ce: 0.007581
2022-01-07 00:01:42,672 iteration 5056 : loss : 0.022800, loss_ce: 0.005696
2022-01-07 00:01:44,229 iteration 5057 : loss : 0.028543, loss_ce: 0.017125
2022-01-07 00:01:45,815 iteration 5058 : loss : 0.022683, loss_ce: 0.009684
2022-01-07 00:01:47,256 iteration 5059 : loss : 0.015349, loss_ce: 0.003976
2022-01-07 00:01:48,807 iteration 5060 : loss : 0.028020, loss_ce: 0.008090
2022-01-07 00:01:50,371 iteration 5061 : loss : 0.018904, loss_ce: 0.007728
2022-01-07 00:01:51,929 iteration 5062 : loss : 0.033058, loss_ce: 0.010168
2022-01-07 00:01:53,535 iteration 5063 : loss : 0.023477, loss_ce: 0.008952
2022-01-07 00:01:55,043 iteration 5064 : loss : 0.018400, loss_ce: 0.007481
2022-01-07 00:01:56,555 iteration 5065 : loss : 0.023942, loss_ce: 0.009199
2022-01-07 00:01:58,107 iteration 5066 : loss : 0.019324, loss_ce: 0.011184
 74%|█████████████████████▌       | 298/400 [2:20:44<46:26, 27.32s/it]2022-01-07 00:01:59,803 iteration 5067 : loss : 0.019190, loss_ce: 0.007641
2022-01-07 00:02:01,393 iteration 5068 : loss : 0.020622, loss_ce: 0.008294
2022-01-07 00:02:02,968 iteration 5069 : loss : 0.026969, loss_ce: 0.010218
2022-01-07 00:02:04,503 iteration 5070 : loss : 0.023511, loss_ce: 0.009036
2022-01-07 00:02:06,112 iteration 5071 : loss : 0.026769, loss_ce: 0.010106
2022-01-07 00:02:07,547 iteration 5072 : loss : 0.016788, loss_ce: 0.005655
2022-01-07 00:02:09,079 iteration 5073 : loss : 0.019749, loss_ce: 0.010147
2022-01-07 00:02:10,645 iteration 5074 : loss : 0.023699, loss_ce: 0.008333
2022-01-07 00:02:12,177 iteration 5075 : loss : 0.020734, loss_ce: 0.009168
2022-01-07 00:02:13,757 iteration 5076 : loss : 0.031730, loss_ce: 0.014797
2022-01-07 00:02:15,266 iteration 5077 : loss : 0.015953, loss_ce: 0.005664
2022-01-07 00:02:16,805 iteration 5078 : loss : 0.015741, loss_ce: 0.004354
2022-01-07 00:02:18,310 iteration 5079 : loss : 0.016499, loss_ce: 0.006548
2022-01-07 00:02:19,842 iteration 5080 : loss : 0.018515, loss_ce: 0.006016
2022-01-07 00:02:21,393 iteration 5081 : loss : 0.021529, loss_ce: 0.011136
2022-01-07 00:02:22,954 iteration 5082 : loss : 0.035460, loss_ce: 0.017562
2022-01-07 00:02:24,488 iteration 5083 : loss : 0.021892, loss_ce: 0.007350
 75%|█████████████████████▋       | 299/400 [2:21:11<45:30, 27.04s/it]2022-01-07 00:02:26,044 iteration 5084 : loss : 0.017497, loss_ce: 0.005848
2022-01-07 00:02:27,574 iteration 5085 : loss : 0.035734, loss_ce: 0.014029
2022-01-07 00:02:29,070 iteration 5086 : loss : 0.019399, loss_ce: 0.007549
2022-01-07 00:02:30,640 iteration 5087 : loss : 0.033855, loss_ce: 0.013029
2022-01-07 00:02:32,183 iteration 5088 : loss : 0.018323, loss_ce: 0.006577
2022-01-07 00:02:33,669 iteration 5089 : loss : 0.016797, loss_ce: 0.005219
2022-01-07 00:02:35,162 iteration 5090 : loss : 0.017602, loss_ce: 0.006833
2022-01-07 00:02:36,771 iteration 5091 : loss : 0.034266, loss_ce: 0.013481
2022-01-07 00:02:38,295 iteration 5092 : loss : 0.014228, loss_ce: 0.006714
2022-01-07 00:02:39,791 iteration 5093 : loss : 0.018320, loss_ce: 0.008354
2022-01-07 00:02:41,275 iteration 5094 : loss : 0.014523, loss_ce: 0.005346
2022-01-07 00:02:42,858 iteration 5095 : loss : 0.033581, loss_ce: 0.010106
2022-01-07 00:02:44,363 iteration 5096 : loss : 0.016026, loss_ce: 0.005990
2022-01-07 00:02:45,885 iteration 5097 : loss : 0.015467, loss_ce: 0.007213
2022-01-07 00:02:47,455 iteration 5098 : loss : 0.024803, loss_ce: 0.009217
2022-01-07 00:02:48,986 iteration 5099 : loss : 0.017223, loss_ce: 0.005842
2022-01-07 00:02:48,987 Training Data Eval:
2022-01-07 00:02:56,899   Average segmentation loss on training set: 0.0133
2022-01-07 00:02:56,899 Validation Data Eval:
2022-01-07 00:02:59,609   Average segmentation loss on validation set: 0.0657
2022-01-07 00:03:06,076 Found new lowest validation loss at iteration 5099! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-07 00:03:07,551 iteration 5100 : loss : 0.013612, loss_ce: 0.006265
 75%|█████████████████████▊       | 300/400 [2:21:54<53:04, 31.85s/it]2022-01-07 00:03:09,065 iteration 5101 : loss : 0.017595, loss_ce: 0.007556
2022-01-07 00:03:10,469 iteration 5102 : loss : 0.012460, loss_ce: 0.004718
2022-01-07 00:03:11,896 iteration 5103 : loss : 0.017773, loss_ce: 0.007479
2022-01-07 00:03:13,402 iteration 5104 : loss : 0.018174, loss_ce: 0.007073
2022-01-07 00:03:14,789 iteration 5105 : loss : 0.017027, loss_ce: 0.006247
2022-01-07 00:03:16,312 iteration 5106 : loss : 0.018018, loss_ce: 0.006722
2022-01-07 00:03:17,777 iteration 5107 : loss : 0.014268, loss_ce: 0.005210
2022-01-07 00:03:19,342 iteration 5108 : loss : 0.025932, loss_ce: 0.009589
2022-01-07 00:03:20,883 iteration 5109 : loss : 0.023642, loss_ce: 0.007405
2022-01-07 00:03:22,409 iteration 5110 : loss : 0.020871, loss_ce: 0.009352
2022-01-07 00:03:23,968 iteration 5111 : loss : 0.016588, loss_ce: 0.005382
2022-01-07 00:03:25,458 iteration 5112 : loss : 0.021713, loss_ce: 0.007906
2022-01-07 00:03:27,034 iteration 5113 : loss : 0.046269, loss_ce: 0.010549
2022-01-07 00:03:28,545 iteration 5114 : loss : 0.023334, loss_ce: 0.010795
2022-01-07 00:03:30,036 iteration 5115 : loss : 0.019828, loss_ce: 0.005387
2022-01-07 00:03:31,539 iteration 5116 : loss : 0.017434, loss_ce: 0.006793
2022-01-07 00:03:33,115 iteration 5117 : loss : 0.022220, loss_ce: 0.010553
 75%|█████████████████████▊       | 301/400 [2:22:19<49:26, 29.96s/it]2022-01-07 00:03:34,702 iteration 5118 : loss : 0.019282, loss_ce: 0.007568
2022-01-07 00:03:36,228 iteration 5119 : loss : 0.018234, loss_ce: 0.007283
2022-01-07 00:03:37,719 iteration 5120 : loss : 0.031246, loss_ce: 0.010199
2022-01-07 00:03:39,241 iteration 5121 : loss : 0.027245, loss_ce: 0.013767
2022-01-07 00:03:40,721 iteration 5122 : loss : 0.021189, loss_ce: 0.008664
2022-01-07 00:03:42,231 iteration 5123 : loss : 0.022959, loss_ce: 0.006093
2022-01-07 00:03:43,775 iteration 5124 : loss : 0.017856, loss_ce: 0.006493
2022-01-07 00:03:45,212 iteration 5125 : loss : 0.018570, loss_ce: 0.009571
2022-01-07 00:03:46,690 iteration 5126 : loss : 0.023301, loss_ce: 0.008182
2022-01-07 00:03:48,250 iteration 5127 : loss : 0.026473, loss_ce: 0.009610
2022-01-07 00:03:49,747 iteration 5128 : loss : 0.015466, loss_ce: 0.006772
2022-01-07 00:03:51,292 iteration 5129 : loss : 0.019983, loss_ce: 0.006188
2022-01-07 00:03:52,831 iteration 5130 : loss : 0.019732, loss_ce: 0.006491
2022-01-07 00:03:54,288 iteration 5131 : loss : 0.011973, loss_ce: 0.004406
2022-01-07 00:03:55,823 iteration 5132 : loss : 0.016928, loss_ce: 0.007562
2022-01-07 00:03:57,333 iteration 5133 : loss : 0.020065, loss_ce: 0.005969
2022-01-07 00:03:58,842 iteration 5134 : loss : 0.019284, loss_ce: 0.008312
 76%|█████████████████████▉       | 302/400 [2:22:45<46:51, 28.69s/it]2022-01-07 00:04:00,574 iteration 5135 : loss : 0.037820, loss_ce: 0.013872
2022-01-07 00:04:02,114 iteration 5136 : loss : 0.020597, loss_ce: 0.007377
2022-01-07 00:04:03,698 iteration 5137 : loss : 0.016761, loss_ce: 0.005796
2022-01-07 00:04:05,207 iteration 5138 : loss : 0.020110, loss_ce: 0.008699
2022-01-07 00:04:06,760 iteration 5139 : loss : 0.015884, loss_ce: 0.006007
2022-01-07 00:04:08,394 iteration 5140 : loss : 0.019324, loss_ce: 0.008161
2022-01-07 00:04:09,918 iteration 5141 : loss : 0.018710, loss_ce: 0.006375
2022-01-07 00:04:11,485 iteration 5142 : loss : 0.023209, loss_ce: 0.007914
2022-01-07 00:04:12,974 iteration 5143 : loss : 0.015653, loss_ce: 0.006086
2022-01-07 00:04:14,556 iteration 5144 : loss : 0.021152, loss_ce: 0.011830
2022-01-07 00:04:16,039 iteration 5145 : loss : 0.015419, loss_ce: 0.005148
2022-01-07 00:04:17,659 iteration 5146 : loss : 0.025686, loss_ce: 0.012085
2022-01-07 00:04:19,273 iteration 5147 : loss : 0.020214, loss_ce: 0.006737
2022-01-07 00:04:20,797 iteration 5148 : loss : 0.016863, loss_ce: 0.007185
2022-01-07 00:04:22,313 iteration 5149 : loss : 0.043392, loss_ce: 0.009435
2022-01-07 00:04:23,827 iteration 5150 : loss : 0.015728, loss_ce: 0.005986
2022-01-07 00:04:25,369 iteration 5151 : loss : 0.027520, loss_ce: 0.005236
 76%|█████████████████████▉       | 303/400 [2:23:12<45:20, 28.04s/it]2022-01-07 00:04:26,913 iteration 5152 : loss : 0.015597, loss_ce: 0.003906
2022-01-07 00:04:28,402 iteration 5153 : loss : 0.013488, loss_ce: 0.005043
2022-01-07 00:04:29,992 iteration 5154 : loss : 0.023650, loss_ce: 0.009215
2022-01-07 00:04:31,465 iteration 5155 : loss : 0.015913, loss_ce: 0.005082
2022-01-07 00:04:32,974 iteration 5156 : loss : 0.022628, loss_ce: 0.008559
2022-01-07 00:04:34,433 iteration 5157 : loss : 0.018310, loss_ce: 0.005258
2022-01-07 00:04:35,945 iteration 5158 : loss : 0.021883, loss_ce: 0.007908
2022-01-07 00:04:37,512 iteration 5159 : loss : 0.022594, loss_ce: 0.007842
2022-01-07 00:04:38,981 iteration 5160 : loss : 0.017329, loss_ce: 0.006026
2022-01-07 00:04:40,495 iteration 5161 : loss : 0.015009, loss_ce: 0.005831
2022-01-07 00:04:41,980 iteration 5162 : loss : 0.017955, loss_ce: 0.008324
2022-01-07 00:04:43,520 iteration 5163 : loss : 0.017945, loss_ce: 0.007392
2022-01-07 00:04:45,067 iteration 5164 : loss : 0.016774, loss_ce: 0.007845
2022-01-07 00:04:46,537 iteration 5165 : loss : 0.020351, loss_ce: 0.006289
2022-01-07 00:04:48,064 iteration 5166 : loss : 0.014978, loss_ce: 0.006663
2022-01-07 00:04:49,604 iteration 5167 : loss : 0.021427, loss_ce: 0.007561
2022-01-07 00:04:51,181 iteration 5168 : loss : 0.019508, loss_ce: 0.007266
 76%|██████████████████████       | 304/400 [2:23:37<43:47, 27.37s/it]2022-01-07 00:04:52,852 iteration 5169 : loss : 0.034648, loss_ce: 0.015369
2022-01-07 00:04:54,329 iteration 5170 : loss : 0.017213, loss_ce: 0.005275
2022-01-07 00:04:55,920 iteration 5171 : loss : 0.020933, loss_ce: 0.008557
2022-01-07 00:04:57,388 iteration 5172 : loss : 0.016266, loss_ce: 0.004817
2022-01-07 00:04:58,819 iteration 5173 : loss : 0.013256, loss_ce: 0.005679
2022-01-07 00:05:00,427 iteration 5174 : loss : 0.021518, loss_ce: 0.009622
2022-01-07 00:05:01,927 iteration 5175 : loss : 0.018641, loss_ce: 0.008427
2022-01-07 00:05:03,571 iteration 5176 : loss : 0.028055, loss_ce: 0.012874
2022-01-07 00:05:05,052 iteration 5177 : loss : 0.018338, loss_ce: 0.007446
2022-01-07 00:05:06,570 iteration 5178 : loss : 0.015852, loss_ce: 0.005101
2022-01-07 00:05:08,142 iteration 5179 : loss : 0.030929, loss_ce: 0.008230
2022-01-07 00:05:09,657 iteration 5180 : loss : 0.019323, loss_ce: 0.007095
2022-01-07 00:05:11,161 iteration 5181 : loss : 0.016859, loss_ce: 0.003855
2022-01-07 00:05:12,680 iteration 5182 : loss : 0.024111, loss_ce: 0.009598
2022-01-07 00:05:14,177 iteration 5183 : loss : 0.022525, loss_ce: 0.009192
2022-01-07 00:05:15,654 iteration 5184 : loss : 0.024682, loss_ce: 0.007867
2022-01-07 00:05:15,654 Training Data Eval:
2022-01-07 00:05:23,469   Average segmentation loss on training set: 0.0140
2022-01-07 00:05:23,469 Validation Data Eval:
2022-01-07 00:05:26,182   Average segmentation loss on validation set: 0.0867
2022-01-07 00:05:27,691 iteration 5185 : loss : 0.018464, loss_ce: 0.006562
 76%|██████████████████████       | 305/400 [2:24:14<47:40, 30.11s/it]2022-01-07 00:05:29,275 iteration 5186 : loss : 0.019893, loss_ce: 0.008668
2022-01-07 00:05:30,732 iteration 5187 : loss : 0.020523, loss_ce: 0.008214
2022-01-07 00:05:32,210 iteration 5188 : loss : 0.016226, loss_ce: 0.006747
2022-01-07 00:05:33,700 iteration 5189 : loss : 0.016516, loss_ce: 0.008355
2022-01-07 00:05:35,170 iteration 5190 : loss : 0.020494, loss_ce: 0.009222
2022-01-07 00:05:36,738 iteration 5191 : loss : 0.020001, loss_ce: 0.006433
2022-01-07 00:05:38,280 iteration 5192 : loss : 0.021901, loss_ce: 0.008537
2022-01-07 00:05:39,745 iteration 5193 : loss : 0.013227, loss_ce: 0.004567
2022-01-07 00:05:41,279 iteration 5194 : loss : 0.022010, loss_ce: 0.009107
2022-01-07 00:05:42,762 iteration 5195 : loss : 0.016475, loss_ce: 0.005345
2022-01-07 00:05:44,189 iteration 5196 : loss : 0.013908, loss_ce: 0.003415
2022-01-07 00:05:45,654 iteration 5197 : loss : 0.013526, loss_ce: 0.004391
2022-01-07 00:05:47,121 iteration 5198 : loss : 0.016270, loss_ce: 0.007405
2022-01-07 00:05:48,683 iteration 5199 : loss : 0.014628, loss_ce: 0.005495
2022-01-07 00:05:50,181 iteration 5200 : loss : 0.023151, loss_ce: 0.006292
2022-01-07 00:05:51,692 iteration 5201 : loss : 0.019691, loss_ce: 0.006119
2022-01-07 00:05:53,267 iteration 5202 : loss : 0.020238, loss_ce: 0.009635
 76%|██████████████████████▏      | 306/400 [2:24:40<45:02, 28.75s/it]2022-01-07 00:05:54,794 iteration 5203 : loss : 0.014246, loss_ce: 0.005226
2022-01-07 00:05:56,289 iteration 5204 : loss : 0.022773, loss_ce: 0.011139
2022-01-07 00:05:57,914 iteration 5205 : loss : 0.022207, loss_ce: 0.008060
2022-01-07 00:05:59,404 iteration 5206 : loss : 0.016522, loss_ce: 0.005666
2022-01-07 00:06:00,895 iteration 5207 : loss : 0.022998, loss_ce: 0.009660
2022-01-07 00:06:02,463 iteration 5208 : loss : 0.025069, loss_ce: 0.009708
2022-01-07 00:06:03,924 iteration 5209 : loss : 0.015327, loss_ce: 0.006539
2022-01-07 00:06:05,476 iteration 5210 : loss : 0.022040, loss_ce: 0.009805
2022-01-07 00:06:06,986 iteration 5211 : loss : 0.020567, loss_ce: 0.008143
2022-01-07 00:06:08,478 iteration 5212 : loss : 0.020093, loss_ce: 0.008069
2022-01-07 00:06:09,889 iteration 5213 : loss : 0.014003, loss_ce: 0.005250
2022-01-07 00:06:11,484 iteration 5214 : loss : 0.030309, loss_ce: 0.015144
2022-01-07 00:06:13,011 iteration 5215 : loss : 0.023082, loss_ce: 0.008415
2022-01-07 00:06:14,552 iteration 5216 : loss : 0.017757, loss_ce: 0.006187
2022-01-07 00:06:16,125 iteration 5217 : loss : 0.018639, loss_ce: 0.006702
2022-01-07 00:06:17,598 iteration 5218 : loss : 0.030488, loss_ce: 0.005145
2022-01-07 00:06:19,198 iteration 5219 : loss : 0.028590, loss_ce: 0.010051
 77%|██████████████████████▎      | 307/400 [2:25:05<43:15, 27.91s/it]2022-01-07 00:06:20,754 iteration 5220 : loss : 0.016708, loss_ce: 0.005659
2022-01-07 00:06:22,264 iteration 5221 : loss : 0.021704, loss_ce: 0.007368
2022-01-07 00:06:23,730 iteration 5222 : loss : 0.018022, loss_ce: 0.006639
2022-01-07 00:06:25,304 iteration 5223 : loss : 0.022058, loss_ce: 0.008209
2022-01-07 00:06:26,817 iteration 5224 : loss : 0.016396, loss_ce: 0.005230
2022-01-07 00:06:28,393 iteration 5225 : loss : 0.021295, loss_ce: 0.008052
2022-01-07 00:06:29,902 iteration 5226 : loss : 0.025127, loss_ce: 0.009588
2022-01-07 00:06:31,493 iteration 5227 : loss : 0.020885, loss_ce: 0.010125
2022-01-07 00:06:32,970 iteration 5228 : loss : 0.021538, loss_ce: 0.006015
2022-01-07 00:06:34,586 iteration 5229 : loss : 0.019374, loss_ce: 0.008590
2022-01-07 00:06:36,001 iteration 5230 : loss : 0.013250, loss_ce: 0.005905
2022-01-07 00:06:37,541 iteration 5231 : loss : 0.028062, loss_ce: 0.011532
2022-01-07 00:06:39,101 iteration 5232 : loss : 0.019233, loss_ce: 0.006037
2022-01-07 00:06:40,650 iteration 5233 : loss : 0.019320, loss_ce: 0.007760
2022-01-07 00:06:42,128 iteration 5234 : loss : 0.016200, loss_ce: 0.006772
2022-01-07 00:06:43,646 iteration 5235 : loss : 0.017552, loss_ce: 0.006119
2022-01-07 00:06:45,195 iteration 5236 : loss : 0.020694, loss_ce: 0.007066
 77%|██████████████████████▎      | 308/400 [2:25:31<41:54, 27.33s/it]2022-01-07 00:06:46,660 iteration 5237 : loss : 0.014259, loss_ce: 0.006026
2022-01-07 00:06:48,205 iteration 5238 : loss : 0.017685, loss_ce: 0.007382
2022-01-07 00:06:49,735 iteration 5239 : loss : 0.058274, loss_ce: 0.017002
2022-01-07 00:06:51,284 iteration 5240 : loss : 0.176518, loss_ce: 0.006249
2022-01-07 00:06:52,758 iteration 5241 : loss : 0.016514, loss_ce: 0.006637
2022-01-07 00:06:54,277 iteration 5242 : loss : 0.019045, loss_ce: 0.008834
2022-01-07 00:06:55,885 iteration 5243 : loss : 0.023007, loss_ce: 0.007224
2022-01-07 00:06:57,457 iteration 5244 : loss : 0.016107, loss_ce: 0.006706
2022-01-07 00:06:59,006 iteration 5245 : loss : 0.019156, loss_ce: 0.006666
2022-01-07 00:07:00,561 iteration 5246 : loss : 0.020983, loss_ce: 0.006564
2022-01-07 00:07:02,022 iteration 5247 : loss : 0.013937, loss_ce: 0.005593
2022-01-07 00:07:03,585 iteration 5248 : loss : 0.014967, loss_ce: 0.005063
2022-01-07 00:07:05,184 iteration 5249 : loss : 0.028433, loss_ce: 0.010430
2022-01-07 00:07:06,731 iteration 5250 : loss : 0.026280, loss_ce: 0.012465
2022-01-07 00:07:08,181 iteration 5251 : loss : 0.015189, loss_ce: 0.005239
2022-01-07 00:07:09,706 iteration 5252 : loss : 0.014746, loss_ce: 0.005744
2022-01-07 00:07:11,258 iteration 5253 : loss : 0.042270, loss_ce: 0.008941
 77%|██████████████████████▍      | 309/400 [2:25:57<40:52, 26.95s/it]2022-01-07 00:07:12,829 iteration 5254 : loss : 0.011691, loss_ce: 0.004172
2022-01-07 00:07:14,344 iteration 5255 : loss : 0.019111, loss_ce: 0.010280
2022-01-07 00:07:15,806 iteration 5256 : loss : 0.014851, loss_ce: 0.006240
2022-01-07 00:07:17,333 iteration 5257 : loss : 0.019261, loss_ce: 0.007786
2022-01-07 00:07:18,890 iteration 5258 : loss : 0.019318, loss_ce: 0.005340
2022-01-07 00:07:20,366 iteration 5259 : loss : 0.022791, loss_ce: 0.005248
2022-01-07 00:07:21,906 iteration 5260 : loss : 0.020126, loss_ce: 0.007793
2022-01-07 00:07:23,459 iteration 5261 : loss : 0.030885, loss_ce: 0.014410
2022-01-07 00:07:24,974 iteration 5262 : loss : 0.012585, loss_ce: 0.004952
2022-01-07 00:07:26,488 iteration 5263 : loss : 0.017708, loss_ce: 0.007238
2022-01-07 00:07:27,949 iteration 5264 : loss : 0.015115, loss_ce: 0.006112
2022-01-07 00:07:29,485 iteration 5265 : loss : 0.020141, loss_ce: 0.008657
2022-01-07 00:07:31,064 iteration 5266 : loss : 0.022181, loss_ce: 0.007152
2022-01-07 00:07:32,565 iteration 5267 : loss : 0.033619, loss_ce: 0.009229
2022-01-07 00:07:34,088 iteration 5268 : loss : 0.023413, loss_ce: 0.006024
2022-01-07 00:07:35,697 iteration 5269 : loss : 0.026606, loss_ce: 0.008098
2022-01-07 00:07:35,697 Training Data Eval:
2022-01-07 00:07:43,462   Average segmentation loss on training set: 0.0162
2022-01-07 00:07:43,462 Validation Data Eval:
2022-01-07 00:07:46,190   Average segmentation loss on validation set: 0.1076
2022-01-07 00:07:47,724 iteration 5270 : loss : 0.015388, loss_ce: 0.006782
 78%|██████████████████████▍      | 310/400 [2:26:34<44:42, 29.80s/it]2022-01-07 00:07:49,277 iteration 5271 : loss : 0.016584, loss_ce: 0.006924
2022-01-07 00:07:50,828 iteration 5272 : loss : 0.023913, loss_ce: 0.012521
2022-01-07 00:07:52,321 iteration 5273 : loss : 0.017816, loss_ce: 0.004870
2022-01-07 00:07:53,828 iteration 5274 : loss : 0.019054, loss_ce: 0.007273
2022-01-07 00:07:55,287 iteration 5275 : loss : 0.015366, loss_ce: 0.006702
2022-01-07 00:07:56,828 iteration 5276 : loss : 0.024348, loss_ce: 0.011525
2022-01-07 00:07:58,351 iteration 5277 : loss : 0.018802, loss_ce: 0.007295
2022-01-07 00:07:59,788 iteration 5278 : loss : 0.015790, loss_ce: 0.006174
2022-01-07 00:08:01,304 iteration 5279 : loss : 0.031016, loss_ce: 0.012733
2022-01-07 00:08:02,943 iteration 5280 : loss : 0.025706, loss_ce: 0.010759
2022-01-07 00:08:04,432 iteration 5281 : loss : 0.015216, loss_ce: 0.005210
2022-01-07 00:08:05,880 iteration 5282 : loss : 0.017223, loss_ce: 0.005558
2022-01-07 00:08:07,398 iteration 5283 : loss : 0.025983, loss_ce: 0.007851
2022-01-07 00:08:08,942 iteration 5284 : loss : 0.034412, loss_ce: 0.011744
2022-01-07 00:08:10,458 iteration 5285 : loss : 0.018468, loss_ce: 0.008026
2022-01-07 00:08:11,915 iteration 5286 : loss : 0.013723, loss_ce: 0.005808
2022-01-07 00:08:13,441 iteration 5287 : loss : 0.017621, loss_ce: 0.003828
 78%|██████████████████████▌      | 311/400 [2:27:00<42:23, 28.58s/it]2022-01-07 00:08:15,069 iteration 5288 : loss : 0.036433, loss_ce: 0.017295
2022-01-07 00:08:16,640 iteration 5289 : loss : 0.043868, loss_ce: 0.018401
2022-01-07 00:08:18,164 iteration 5290 : loss : 0.043963, loss_ce: 0.007821
2022-01-07 00:08:19,676 iteration 5291 : loss : 0.039486, loss_ce: 0.014016
2022-01-07 00:08:21,249 iteration 5292 : loss : 0.031928, loss_ce: 0.014294
2022-01-07 00:08:22,783 iteration 5293 : loss : 0.021455, loss_ce: 0.009122
2022-01-07 00:08:24,346 iteration 5294 : loss : 0.018627, loss_ce: 0.006552
2022-01-07 00:08:25,953 iteration 5295 : loss : 0.023910, loss_ce: 0.008248
2022-01-07 00:08:27,516 iteration 5296 : loss : 0.025255, loss_ce: 0.007134
2022-01-07 00:08:29,096 iteration 5297 : loss : 0.022172, loss_ce: 0.008802
2022-01-07 00:08:30,691 iteration 5298 : loss : 0.023142, loss_ce: 0.008369
2022-01-07 00:08:32,288 iteration 5299 : loss : 0.016284, loss_ce: 0.005998
2022-01-07 00:08:33,905 iteration 5300 : loss : 0.022693, loss_ce: 0.008565
2022-01-07 00:08:35,415 iteration 5301 : loss : 0.023899, loss_ce: 0.008283
2022-01-07 00:08:36,926 iteration 5302 : loss : 0.015690, loss_ce: 0.007856
2022-01-07 00:08:38,444 iteration 5303 : loss : 0.023669, loss_ce: 0.008868
2022-01-07 00:08:39,935 iteration 5304 : loss : 0.021085, loss_ce: 0.008786
 78%|██████████████████████▌      | 312/400 [2:27:26<41:00, 27.95s/it]2022-01-07 00:08:41,567 iteration 5305 : loss : 0.025733, loss_ce: 0.008490
2022-01-07 00:08:43,222 iteration 5306 : loss : 0.027646, loss_ce: 0.012224
2022-01-07 00:08:44,891 iteration 5307 : loss : 0.021867, loss_ce: 0.008147
2022-01-07 00:08:46,499 iteration 5308 : loss : 0.050905, loss_ce: 0.016382
2022-01-07 00:08:48,101 iteration 5309 : loss : 0.027012, loss_ce: 0.010408
2022-01-07 00:08:49,616 iteration 5310 : loss : 0.016512, loss_ce: 0.007894
2022-01-07 00:08:51,111 iteration 5311 : loss : 0.022608, loss_ce: 0.006554
2022-01-07 00:08:52,706 iteration 5312 : loss : 0.022937, loss_ce: 0.010484
2022-01-07 00:08:54,354 iteration 5313 : loss : 0.021488, loss_ce: 0.007261
2022-01-07 00:08:55,936 iteration 5314 : loss : 0.021150, loss_ce: 0.007188
2022-01-07 00:08:57,568 iteration 5315 : loss : 0.027566, loss_ce: 0.007695
2022-01-07 00:08:59,191 iteration 5316 : loss : 0.021207, loss_ce: 0.007668
2022-01-07 00:09:00,763 iteration 5317 : loss : 0.022735, loss_ce: 0.009488
2022-01-07 00:09:02,296 iteration 5318 : loss : 0.022894, loss_ce: 0.010078
2022-01-07 00:09:03,913 iteration 5319 : loss : 0.028423, loss_ce: 0.010545
2022-01-07 00:09:05,518 iteration 5320 : loss : 0.026828, loss_ce: 0.012016
2022-01-07 00:09:07,145 iteration 5321 : loss : 0.027642, loss_ce: 0.010661
 78%|██████████████████████▋      | 313/400 [2:27:53<40:12, 27.73s/it]2022-01-07 00:09:08,689 iteration 5322 : loss : 0.014627, loss_ce: 0.005493
2022-01-07 00:09:10,257 iteration 5323 : loss : 0.024040, loss_ce: 0.011323
2022-01-07 00:09:11,747 iteration 5324 : loss : 0.018354, loss_ce: 0.005892
2022-01-07 00:09:13,252 iteration 5325 : loss : 0.021139, loss_ce: 0.005013
2022-01-07 00:09:14,814 iteration 5326 : loss : 0.020914, loss_ce: 0.008121
2022-01-07 00:09:16,312 iteration 5327 : loss : 0.019371, loss_ce: 0.006548
2022-01-07 00:09:17,832 iteration 5328 : loss : 0.024623, loss_ce: 0.012449
2022-01-07 00:09:19,413 iteration 5329 : loss : 0.027556, loss_ce: 0.008896
2022-01-07 00:09:20,969 iteration 5330 : loss : 0.025187, loss_ce: 0.009460
2022-01-07 00:09:22,496 iteration 5331 : loss : 0.014239, loss_ce: 0.004662
2022-01-07 00:09:24,043 iteration 5332 : loss : 0.020251, loss_ce: 0.006318
2022-01-07 00:09:25,630 iteration 5333 : loss : 0.024470, loss_ce: 0.012872
2022-01-07 00:09:27,201 iteration 5334 : loss : 0.054826, loss_ce: 0.023629
2022-01-07 00:09:28,803 iteration 5335 : loss : 0.018716, loss_ce: 0.005818
2022-01-07 00:09:30,376 iteration 5336 : loss : 0.018546, loss_ce: 0.006784
2022-01-07 00:09:31,905 iteration 5337 : loss : 0.015757, loss_ce: 0.006130
2022-01-07 00:09:33,436 iteration 5338 : loss : 0.015934, loss_ce: 0.006788
 78%|██████████████████████▊      | 314/400 [2:28:20<39:07, 27.30s/it]2022-01-07 00:09:35,191 iteration 5339 : loss : 0.023649, loss_ce: 0.008543
2022-01-07 00:09:36,792 iteration 5340 : loss : 0.026608, loss_ce: 0.013474
2022-01-07 00:09:38,368 iteration 5341 : loss : 0.020589, loss_ce: 0.004706
2022-01-07 00:09:39,938 iteration 5342 : loss : 0.015388, loss_ce: 0.007976
2022-01-07 00:09:41,472 iteration 5343 : loss : 0.014612, loss_ce: 0.004595
2022-01-07 00:09:43,046 iteration 5344 : loss : 0.019066, loss_ce: 0.006490
2022-01-07 00:09:44,596 iteration 5345 : loss : 0.019503, loss_ce: 0.007034
2022-01-07 00:09:46,189 iteration 5346 : loss : 0.018058, loss_ce: 0.006579
2022-01-07 00:09:47,780 iteration 5347 : loss : 0.018175, loss_ce: 0.005126
2022-01-07 00:09:49,372 iteration 5348 : loss : 0.019712, loss_ce: 0.005009
2022-01-07 00:09:50,924 iteration 5349 : loss : 0.020573, loss_ce: 0.007884
2022-01-07 00:09:52,408 iteration 5350 : loss : 0.013198, loss_ce: 0.004873
2022-01-07 00:09:53,904 iteration 5351 : loss : 0.024676, loss_ce: 0.010044
2022-01-07 00:09:55,486 iteration 5352 : loss : 0.028001, loss_ce: 0.010715
2022-01-07 00:09:57,066 iteration 5353 : loss : 0.012419, loss_ce: 0.004293
2022-01-07 00:09:58,573 iteration 5354 : loss : 0.015027, loss_ce: 0.006863
2022-01-07 00:09:58,574 Training Data Eval:
2022-01-07 00:10:06,445   Average segmentation loss on training set: 0.0127
2022-01-07 00:10:06,445 Validation Data Eval:
2022-01-07 00:10:09,162   Average segmentation loss on validation set: 0.0834
2022-01-07 00:10:10,623 iteration 5355 : loss : 0.022438, loss_ce: 0.007949
 79%|██████████████████████▊      | 315/400 [2:28:57<42:52, 30.27s/it]2022-01-07 00:10:12,222 iteration 5356 : loss : 0.019887, loss_ce: 0.008726
2022-01-07 00:10:13,685 iteration 5357 : loss : 0.017274, loss_ce: 0.004060
2022-01-07 00:10:15,322 iteration 5358 : loss : 0.030090, loss_ce: 0.015170
2022-01-07 00:10:16,776 iteration 5359 : loss : 0.014491, loss_ce: 0.005339
2022-01-07 00:10:18,318 iteration 5360 : loss : 0.018483, loss_ce: 0.007271
2022-01-07 00:10:19,842 iteration 5361 : loss : 0.019145, loss_ce: 0.006868
2022-01-07 00:10:21,391 iteration 5362 : loss : 0.025858, loss_ce: 0.009860
2022-01-07 00:10:22,911 iteration 5363 : loss : 0.025875, loss_ce: 0.010378
2022-01-07 00:10:24,437 iteration 5364 : loss : 0.022541, loss_ce: 0.004821
2022-01-07 00:10:25,950 iteration 5365 : loss : 0.017564, loss_ce: 0.007374
2022-01-07 00:10:27,489 iteration 5366 : loss : 0.018015, loss_ce: 0.006788
2022-01-07 00:10:29,052 iteration 5367 : loss : 0.017039, loss_ce: 0.007166
2022-01-07 00:10:30,543 iteration 5368 : loss : 0.015530, loss_ce: 0.007885
2022-01-07 00:10:32,071 iteration 5369 : loss : 0.014610, loss_ce: 0.005372
2022-01-07 00:10:33,581 iteration 5370 : loss : 0.013548, loss_ce: 0.005273
2022-01-07 00:10:35,141 iteration 5371 : loss : 0.020768, loss_ce: 0.009753
2022-01-07 00:10:36,680 iteration 5372 : loss : 0.020290, loss_ce: 0.006500
 79%|██████████████████████▉      | 316/400 [2:29:23<40:36, 29.00s/it]2022-01-07 00:10:38,310 iteration 5373 : loss : 0.017208, loss_ce: 0.007885
2022-01-07 00:10:39,852 iteration 5374 : loss : 0.019721, loss_ce: 0.008269
2022-01-07 00:10:41,450 iteration 5375 : loss : 0.012826, loss_ce: 0.003963
2022-01-07 00:10:43,007 iteration 5376 : loss : 0.019564, loss_ce: 0.006951
2022-01-07 00:10:44,553 iteration 5377 : loss : 0.013991, loss_ce: 0.004407
2022-01-07 00:10:46,135 iteration 5378 : loss : 0.017042, loss_ce: 0.006211
2022-01-07 00:10:47,650 iteration 5379 : loss : 0.010890, loss_ce: 0.004034
2022-01-07 00:10:49,230 iteration 5380 : loss : 0.017651, loss_ce: 0.007501
2022-01-07 00:10:50,801 iteration 5381 : loss : 0.016590, loss_ce: 0.007167
2022-01-07 00:10:52,392 iteration 5382 : loss : 0.016478, loss_ce: 0.004976
2022-01-07 00:10:54,044 iteration 5383 : loss : 0.025077, loss_ce: 0.013035
2022-01-07 00:10:55,764 iteration 5384 : loss : 0.028970, loss_ce: 0.013271
2022-01-07 00:10:57,315 iteration 5385 : loss : 0.017889, loss_ce: 0.006891
2022-01-07 00:10:58,808 iteration 5386 : loss : 0.016308, loss_ce: 0.005241
2022-01-07 00:11:00,350 iteration 5387 : loss : 0.020907, loss_ce: 0.009219
2022-01-07 00:11:01,959 iteration 5388 : loss : 0.019492, loss_ce: 0.006576
2022-01-07 00:11:03,632 iteration 5389 : loss : 0.026635, loss_ce: 0.010961
 79%|██████████████████████▉      | 317/400 [2:29:50<39:16, 28.39s/it]2022-01-07 00:11:05,228 iteration 5390 : loss : 0.021916, loss_ce: 0.006061
2022-01-07 00:11:06,822 iteration 5391 : loss : 0.016605, loss_ce: 0.005271
2022-01-07 00:11:08,428 iteration 5392 : loss : 0.019714, loss_ce: 0.009993
2022-01-07 00:11:09,971 iteration 5393 : loss : 0.042353, loss_ce: 0.011582
2022-01-07 00:11:11,479 iteration 5394 : loss : 0.017519, loss_ce: 0.008368
2022-01-07 00:11:13,043 iteration 5395 : loss : 0.021231, loss_ce: 0.006819
2022-01-07 00:11:14,542 iteration 5396 : loss : 0.020500, loss_ce: 0.010076
2022-01-07 00:11:16,141 iteration 5397 : loss : 0.017646, loss_ce: 0.005656
2022-01-07 00:11:17,681 iteration 5398 : loss : 0.017943, loss_ce: 0.009166
2022-01-07 00:11:19,163 iteration 5399 : loss : 0.018554, loss_ce: 0.005507
2022-01-07 00:11:20,791 iteration 5400 : loss : 0.039612, loss_ce: 0.010022
2022-01-07 00:11:22,350 iteration 5401 : loss : 0.023460, loss_ce: 0.009835
2022-01-07 00:11:23,884 iteration 5402 : loss : 0.021868, loss_ce: 0.008506
2022-01-07 00:11:25,385 iteration 5403 : loss : 0.028337, loss_ce: 0.012184
2022-01-07 00:11:26,882 iteration 5404 : loss : 0.017199, loss_ce: 0.004918
2022-01-07 00:11:28,384 iteration 5405 : loss : 0.015770, loss_ce: 0.006461
2022-01-07 00:11:29,948 iteration 5406 : loss : 0.017492, loss_ce: 0.005007
 80%|███████████████████████      | 318/400 [2:30:16<37:56, 27.76s/it]2022-01-07 00:11:31,554 iteration 5407 : loss : 0.020402, loss_ce: 0.007325
2022-01-07 00:11:33,063 iteration 5408 : loss : 0.021121, loss_ce: 0.007570
2022-01-07 00:11:34,650 iteration 5409 : loss : 0.027706, loss_ce: 0.012634
2022-01-07 00:11:36,140 iteration 5410 : loss : 0.017329, loss_ce: 0.008501
2022-01-07 00:11:37,667 iteration 5411 : loss : 0.025204, loss_ce: 0.012017
2022-01-07 00:11:39,197 iteration 5412 : loss : 0.015825, loss_ce: 0.006734
2022-01-07 00:11:40,698 iteration 5413 : loss : 0.020290, loss_ce: 0.010235
2022-01-07 00:11:42,231 iteration 5414 : loss : 0.017927, loss_ce: 0.007252
2022-01-07 00:11:43,841 iteration 5415 : loss : 0.030522, loss_ce: 0.012494
2022-01-07 00:11:45,322 iteration 5416 : loss : 0.012371, loss_ce: 0.004239
2022-01-07 00:11:46,897 iteration 5417 : loss : 0.017237, loss_ce: 0.005341
2022-01-07 00:11:48,439 iteration 5418 : loss : 0.024573, loss_ce: 0.009661
2022-01-07 00:11:50,023 iteration 5419 : loss : 0.027545, loss_ce: 0.006893
2022-01-07 00:11:51,622 iteration 5420 : loss : 0.021415, loss_ce: 0.008506
2022-01-07 00:11:53,184 iteration 5421 : loss : 0.023974, loss_ce: 0.010969
2022-01-07 00:11:54,623 iteration 5422 : loss : 0.014592, loss_ce: 0.006345
2022-01-07 00:11:56,184 iteration 5423 : loss : 0.030283, loss_ce: 0.007160
 80%|███████████████████████▏     | 319/400 [2:30:42<36:51, 27.31s/it]2022-01-07 00:11:57,827 iteration 5424 : loss : 0.020899, loss_ce: 0.007252
2022-01-07 00:11:59,316 iteration 5425 : loss : 0.017463, loss_ce: 0.005188
2022-01-07 00:12:00,836 iteration 5426 : loss : 0.022874, loss_ce: 0.006136
2022-01-07 00:12:02,284 iteration 5427 : loss : 0.019982, loss_ce: 0.007920
2022-01-07 00:12:03,791 iteration 5428 : loss : 0.020846, loss_ce: 0.008066
2022-01-07 00:12:05,327 iteration 5429 : loss : 0.016147, loss_ce: 0.005903
2022-01-07 00:12:06,854 iteration 5430 : loss : 0.024055, loss_ce: 0.010435
2022-01-07 00:12:08,317 iteration 5431 : loss : 0.016724, loss_ce: 0.005963
2022-01-07 00:12:09,892 iteration 5432 : loss : 0.018766, loss_ce: 0.008283
2022-01-07 00:12:11,466 iteration 5433 : loss : 0.019481, loss_ce: 0.007253
2022-01-07 00:12:12,943 iteration 5434 : loss : 0.018613, loss_ce: 0.008122
2022-01-07 00:12:14,546 iteration 5435 : loss : 0.018607, loss_ce: 0.007351
2022-01-07 00:12:16,110 iteration 5436 : loss : 0.017632, loss_ce: 0.006785
2022-01-07 00:12:17,701 iteration 5437 : loss : 0.043809, loss_ce: 0.007864
2022-01-07 00:12:19,296 iteration 5438 : loss : 0.021946, loss_ce: 0.008323
2022-01-07 00:12:20,821 iteration 5439 : loss : 0.024127, loss_ce: 0.010529
2022-01-07 00:12:20,821 Training Data Eval:
2022-01-07 00:12:28,809   Average segmentation loss on training set: 0.0116
2022-01-07 00:12:28,810 Validation Data Eval:
2022-01-07 00:12:31,588   Average segmentation loss on validation set: 0.0673
2022-01-07 00:12:33,110 iteration 5440 : loss : 0.017743, loss_ce: 0.005975
 80%|███████████████████████▏     | 320/400 [2:31:19<40:15, 30.19s/it]2022-01-07 00:12:34,705 iteration 5441 : loss : 0.016357, loss_ce: 0.004932
2022-01-07 00:12:36,227 iteration 5442 : loss : 0.017266, loss_ce: 0.006057
2022-01-07 00:12:37,782 iteration 5443 : loss : 0.015571, loss_ce: 0.005603
2022-01-07 00:12:39,275 iteration 5444 : loss : 0.015229, loss_ce: 0.004945
2022-01-07 00:12:40,841 iteration 5445 : loss : 0.031568, loss_ce: 0.009492
2022-01-07 00:12:42,361 iteration 5446 : loss : 0.017457, loss_ce: 0.008326
2022-01-07 00:12:44,008 iteration 5447 : loss : 0.023548, loss_ce: 0.008047
2022-01-07 00:12:45,526 iteration 5448 : loss : 0.014551, loss_ce: 0.005852
2022-01-07 00:12:47,092 iteration 5449 : loss : 0.033412, loss_ce: 0.019260
2022-01-07 00:12:48,647 iteration 5450 : loss : 0.020913, loss_ce: 0.008950
2022-01-07 00:12:50,130 iteration 5451 : loss : 0.016256, loss_ce: 0.005561
2022-01-07 00:12:51,684 iteration 5452 : loss : 0.020720, loss_ce: 0.008157
2022-01-07 00:12:53,185 iteration 5453 : loss : 0.014926, loss_ce: 0.005668
2022-01-07 00:12:54,773 iteration 5454 : loss : 0.022061, loss_ce: 0.012122
2022-01-07 00:12:56,332 iteration 5455 : loss : 0.029232, loss_ce: 0.008305
2022-01-07 00:12:57,857 iteration 5456 : loss : 0.018554, loss_ce: 0.004536
2022-01-07 00:12:59,339 iteration 5457 : loss : 0.021321, loss_ce: 0.007916
 80%|███████████████████████▎     | 321/400 [2:31:46<38:11, 29.01s/it]2022-01-07 00:13:00,898 iteration 5458 : loss : 0.019678, loss_ce: 0.006902
2022-01-07 00:13:02,365 iteration 5459 : loss : 0.016065, loss_ce: 0.007138
2022-01-07 00:13:03,892 iteration 5460 : loss : 0.014727, loss_ce: 0.005686
2022-01-07 00:13:05,415 iteration 5461 : loss : 0.024960, loss_ce: 0.008779
2022-01-07 00:13:06,930 iteration 5462 : loss : 0.017962, loss_ce: 0.006334
2022-01-07 00:13:08,465 iteration 5463 : loss : 0.022317, loss_ce: 0.005817
2022-01-07 00:13:09,995 iteration 5464 : loss : 0.017666, loss_ce: 0.007391
2022-01-07 00:13:11,579 iteration 5465 : loss : 0.021507, loss_ce: 0.007699
2022-01-07 00:13:13,147 iteration 5466 : loss : 0.018121, loss_ce: 0.007988
2022-01-07 00:13:14,681 iteration 5467 : loss : 0.035501, loss_ce: 0.013934
2022-01-07 00:13:16,223 iteration 5468 : loss : 0.015791, loss_ce: 0.006519
2022-01-07 00:13:17,677 iteration 5469 : loss : 0.019572, loss_ce: 0.007597
2022-01-07 00:13:19,176 iteration 5470 : loss : 0.011827, loss_ce: 0.004177
2022-01-07 00:13:20,674 iteration 5471 : loss : 0.022069, loss_ce: 0.007164
2022-01-07 00:13:22,207 iteration 5472 : loss : 0.030996, loss_ce: 0.009359
2022-01-07 00:13:23,669 iteration 5473 : loss : 0.015564, loss_ce: 0.005319
2022-01-07 00:13:25,150 iteration 5474 : loss : 0.016116, loss_ce: 0.005456
 80%|███████████████████████▎     | 322/400 [2:32:11<36:27, 28.04s/it]2022-01-07 00:13:26,751 iteration 5475 : loss : 0.022117, loss_ce: 0.006160
2022-01-07 00:13:28,342 iteration 5476 : loss : 0.021894, loss_ce: 0.009667
2022-01-07 00:13:29,879 iteration 5477 : loss : 0.037340, loss_ce: 0.005505
2022-01-07 00:13:31,335 iteration 5478 : loss : 0.020697, loss_ce: 0.006770
2022-01-07 00:13:32,864 iteration 5479 : loss : 0.023582, loss_ce: 0.012934
2022-01-07 00:13:34,389 iteration 5480 : loss : 0.026431, loss_ce: 0.009406
2022-01-07 00:13:35,891 iteration 5481 : loss : 0.016440, loss_ce: 0.005566
2022-01-07 00:13:37,484 iteration 5482 : loss : 0.028237, loss_ce: 0.006519
2022-01-07 00:13:38,923 iteration 5483 : loss : 0.013344, loss_ce: 0.005118
2022-01-07 00:13:40,459 iteration 5484 : loss : 0.021496, loss_ce: 0.008439
2022-01-07 00:13:41,971 iteration 5485 : loss : 0.016048, loss_ce: 0.006789
2022-01-07 00:13:43,481 iteration 5486 : loss : 0.019433, loss_ce: 0.009124
2022-01-07 00:13:45,000 iteration 5487 : loss : 0.023304, loss_ce: 0.008713
2022-01-07 00:13:46,474 iteration 5488 : loss : 0.014472, loss_ce: 0.006542
2022-01-07 00:13:47,945 iteration 5489 : loss : 0.014721, loss_ce: 0.007982
2022-01-07 00:13:49,517 iteration 5490 : loss : 0.023987, loss_ce: 0.009861
2022-01-07 00:13:51,002 iteration 5491 : loss : 0.015272, loss_ce: 0.004696
 81%|███████████████████████▍     | 323/400 [2:32:37<35:08, 27.39s/it]2022-01-07 00:13:52,491 iteration 5492 : loss : 0.015612, loss_ce: 0.005779
2022-01-07 00:13:54,011 iteration 5493 : loss : 0.025838, loss_ce: 0.007340
2022-01-07 00:13:55,473 iteration 5494 : loss : 0.030637, loss_ce: 0.011963
2022-01-07 00:13:56,982 iteration 5495 : loss : 0.014450, loss_ce: 0.004242
2022-01-07 00:13:58,510 iteration 5496 : loss : 0.017219, loss_ce: 0.004786
2022-01-07 00:14:00,032 iteration 5497 : loss : 0.015398, loss_ce: 0.006827
2022-01-07 00:14:01,582 iteration 5498 : loss : 0.026263, loss_ce: 0.011092
2022-01-07 00:14:03,116 iteration 5499 : loss : 0.030722, loss_ce: 0.023263
2022-01-07 00:14:04,565 iteration 5500 : loss : 0.015950, loss_ce: 0.007156
2022-01-07 00:14:06,068 iteration 5501 : loss : 0.021755, loss_ce: 0.009353
2022-01-07 00:14:07,519 iteration 5502 : loss : 0.012171, loss_ce: 0.005141
2022-01-07 00:14:09,003 iteration 5503 : loss : 0.022643, loss_ce: 0.006386
2022-01-07 00:14:10,478 iteration 5504 : loss : 0.015776, loss_ce: 0.006015
2022-01-07 00:14:11,981 iteration 5505 : loss : 0.015121, loss_ce: 0.006140
2022-01-07 00:14:13,511 iteration 5506 : loss : 0.013363, loss_ce: 0.004791
2022-01-07 00:14:15,012 iteration 5507 : loss : 0.021472, loss_ce: 0.009333
2022-01-07 00:14:16,515 iteration 5508 : loss : 0.020720, loss_ce: 0.006983
 81%|███████████████████████▍     | 324/400 [2:33:03<33:58, 26.83s/it]2022-01-07 00:14:18,078 iteration 5509 : loss : 0.027579, loss_ce: 0.007963
2022-01-07 00:14:19,663 iteration 5510 : loss : 0.027965, loss_ce: 0.010660
2022-01-07 00:14:21,150 iteration 5511 : loss : 0.015434, loss_ce: 0.006053
2022-01-07 00:14:22,691 iteration 5512 : loss : 0.021767, loss_ce: 0.008421
2022-01-07 00:14:24,168 iteration 5513 : loss : 0.013168, loss_ce: 0.005541
2022-01-07 00:14:25,608 iteration 5514 : loss : 0.012438, loss_ce: 0.004173
2022-01-07 00:14:27,175 iteration 5515 : loss : 0.030187, loss_ce: 0.007260
2022-01-07 00:14:28,663 iteration 5516 : loss : 0.024695, loss_ce: 0.010251
2022-01-07 00:14:30,196 iteration 5517 : loss : 0.017782, loss_ce: 0.007823
2022-01-07 00:14:31,734 iteration 5518 : loss : 0.018202, loss_ce: 0.005733
2022-01-07 00:14:33,283 iteration 5519 : loss : 0.022232, loss_ce: 0.008462
2022-01-07 00:14:34,820 iteration 5520 : loss : 0.015678, loss_ce: 0.006536
2022-01-07 00:14:36,366 iteration 5521 : loss : 0.023331, loss_ce: 0.008772
2022-01-07 00:14:37,889 iteration 5522 : loss : 0.023857, loss_ce: 0.008400
2022-01-07 00:14:39,374 iteration 5523 : loss : 0.012180, loss_ce: 0.004425
2022-01-07 00:14:40,868 iteration 5524 : loss : 0.035247, loss_ce: 0.015592
2022-01-07 00:14:40,868 Training Data Eval:
2022-01-07 00:14:48,709   Average segmentation loss on training set: 0.0161
2022-01-07 00:14:48,709 Validation Data Eval:
2022-01-07 00:14:51,419   Average segmentation loss on validation set: 0.0762
2022-01-07 00:14:52,940 iteration 5525 : loss : 0.022181, loss_ce: 0.011306
 81%|███████████████████████▌     | 325/400 [2:33:39<37:07, 29.71s/it]2022-01-07 00:14:54,558 iteration 5526 : loss : 0.035331, loss_ce: 0.014219
2022-01-07 00:14:56,099 iteration 5527 : loss : 0.018103, loss_ce: 0.007678
2022-01-07 00:14:57,638 iteration 5528 : loss : 0.018177, loss_ce: 0.007226
2022-01-07 00:14:59,201 iteration 5529 : loss : 0.024361, loss_ce: 0.009485
2022-01-07 00:15:00,681 iteration 5530 : loss : 0.016873, loss_ce: 0.008418
2022-01-07 00:15:02,170 iteration 5531 : loss : 0.018254, loss_ce: 0.008645
2022-01-07 00:15:03,652 iteration 5532 : loss : 0.014738, loss_ce: 0.006495
2022-01-07 00:15:05,220 iteration 5533 : loss : 0.019879, loss_ce: 0.005455
2022-01-07 00:15:06,734 iteration 5534 : loss : 0.017553, loss_ce: 0.005489
2022-01-07 00:15:08,312 iteration 5535 : loss : 0.025516, loss_ce: 0.010434
2022-01-07 00:15:09,917 iteration 5536 : loss : 0.014179, loss_ce: 0.006008
2022-01-07 00:15:11,460 iteration 5537 : loss : 0.036919, loss_ce: 0.012961
2022-01-07 00:15:12,980 iteration 5538 : loss : 0.022403, loss_ce: 0.008920
2022-01-07 00:15:14,577 iteration 5539 : loss : 0.020766, loss_ce: 0.005360
2022-01-07 00:15:16,059 iteration 5540 : loss : 0.022042, loss_ce: 0.010197
2022-01-07 00:15:17,579 iteration 5541 : loss : 0.018562, loss_ce: 0.007142
2022-01-07 00:15:19,063 iteration 5542 : loss : 0.015471, loss_ce: 0.005539
 82%|███████████████████████▋     | 326/400 [2:34:05<35:18, 28.63s/it]2022-01-07 00:15:20,660 iteration 5543 : loss : 0.019295, loss_ce: 0.007066
2022-01-07 00:15:22,166 iteration 5544 : loss : 0.022915, loss_ce: 0.009845
2022-01-07 00:15:23,645 iteration 5545 : loss : 0.018562, loss_ce: 0.008255
2022-01-07 00:15:25,109 iteration 5546 : loss : 0.025603, loss_ce: 0.007659
2022-01-07 00:15:26,626 iteration 5547 : loss : 0.015071, loss_ce: 0.005279
2022-01-07 00:15:28,179 iteration 5548 : loss : 0.025717, loss_ce: 0.008953
2022-01-07 00:15:29,626 iteration 5549 : loss : 0.019710, loss_ce: 0.008150
2022-01-07 00:15:31,105 iteration 5550 : loss : 0.014103, loss_ce: 0.005364
2022-01-07 00:15:32,634 iteration 5551 : loss : 0.020442, loss_ce: 0.008363
2022-01-07 00:15:34,107 iteration 5552 : loss : 0.014645, loss_ce: 0.005588
2022-01-07 00:15:35,663 iteration 5553 : loss : 0.026506, loss_ce: 0.010646
2022-01-07 00:15:37,232 iteration 5554 : loss : 0.017062, loss_ce: 0.005755
2022-01-07 00:15:38,790 iteration 5555 : loss : 0.036018, loss_ce: 0.009665
2022-01-07 00:15:40,257 iteration 5556 : loss : 0.016324, loss_ce: 0.006050
2022-01-07 00:15:41,825 iteration 5557 : loss : 0.032280, loss_ce: 0.008362
2022-01-07 00:15:43,342 iteration 5558 : loss : 0.022582, loss_ce: 0.007723
2022-01-07 00:15:44,772 iteration 5559 : loss : 0.017029, loss_ce: 0.007051
 82%|███████████████████████▋     | 327/400 [2:34:31<33:45, 27.75s/it]2022-01-07 00:15:46,492 iteration 5560 : loss : 0.034535, loss_ce: 0.013546
2022-01-07 00:15:47,982 iteration 5561 : loss : 0.018179, loss_ce: 0.006098
2022-01-07 00:15:49,482 iteration 5562 : loss : 0.029255, loss_ce: 0.008550
2022-01-07 00:15:50,989 iteration 5563 : loss : 0.014902, loss_ce: 0.005817
2022-01-07 00:15:52,584 iteration 5564 : loss : 0.022983, loss_ce: 0.007383
2022-01-07 00:15:54,097 iteration 5565 : loss : 0.017613, loss_ce: 0.007286
2022-01-07 00:15:55,648 iteration 5566 : loss : 0.019291, loss_ce: 0.008141
2022-01-07 00:15:57,244 iteration 5567 : loss : 0.023384, loss_ce: 0.011664
2022-01-07 00:15:58,749 iteration 5568 : loss : 0.026387, loss_ce: 0.009704
2022-01-07 00:16:00,304 iteration 5569 : loss : 0.018701, loss_ce: 0.008077
2022-01-07 00:16:01,838 iteration 5570 : loss : 0.015570, loss_ce: 0.005132
2022-01-07 00:16:03,417 iteration 5571 : loss : 0.019854, loss_ce: 0.009099
2022-01-07 00:16:04,929 iteration 5572 : loss : 0.022339, loss_ce: 0.009753
2022-01-07 00:16:06,428 iteration 5573 : loss : 0.016110, loss_ce: 0.004628
2022-01-07 00:16:08,029 iteration 5574 : loss : 0.030884, loss_ce: 0.011767
2022-01-07 00:16:09,444 iteration 5575 : loss : 0.013126, loss_ce: 0.005927
2022-01-07 00:16:10,844 iteration 5576 : loss : 0.014093, loss_ce: 0.004759
 82%|███████████████████████▊     | 328/400 [2:34:57<32:41, 27.25s/it]2022-01-07 00:16:12,396 iteration 5577 : loss : 0.018459, loss_ce: 0.007056
2022-01-07 00:16:13,876 iteration 5578 : loss : 0.020011, loss_ce: 0.004996
2022-01-07 00:16:15,366 iteration 5579 : loss : 0.014578, loss_ce: 0.004407
2022-01-07 00:16:16,916 iteration 5580 : loss : 0.016275, loss_ce: 0.005475
2022-01-07 00:16:18,420 iteration 5581 : loss : 0.017533, loss_ce: 0.006720
2022-01-07 00:16:19,979 iteration 5582 : loss : 0.018744, loss_ce: 0.006017
2022-01-07 00:16:21,497 iteration 5583 : loss : 0.018384, loss_ce: 0.004980
2022-01-07 00:16:23,124 iteration 5584 : loss : 0.021782, loss_ce: 0.009434
2022-01-07 00:16:24,636 iteration 5585 : loss : 0.017742, loss_ce: 0.007196
2022-01-07 00:16:26,165 iteration 5586 : loss : 0.021435, loss_ce: 0.008945
2022-01-07 00:16:27,701 iteration 5587 : loss : 0.018219, loss_ce: 0.008819
2022-01-07 00:16:29,308 iteration 5588 : loss : 0.026286, loss_ce: 0.009111
2022-01-07 00:16:30,887 iteration 5589 : loss : 0.022757, loss_ce: 0.009537
2022-01-07 00:16:32,421 iteration 5590 : loss : 0.019876, loss_ce: 0.007499
2022-01-07 00:16:33,934 iteration 5591 : loss : 0.017148, loss_ce: 0.007471
2022-01-07 00:16:35,433 iteration 5592 : loss : 0.017092, loss_ce: 0.005718
2022-01-07 00:16:36,931 iteration 5593 : loss : 0.016407, loss_ce: 0.009306
 82%|███████████████████████▊     | 329/400 [2:35:23<31:50, 26.90s/it]2022-01-07 00:16:38,526 iteration 5594 : loss : 0.019489, loss_ce: 0.007392
2022-01-07 00:16:40,028 iteration 5595 : loss : 0.021546, loss_ce: 0.009241
2022-01-07 00:16:41,534 iteration 5596 : loss : 0.013170, loss_ce: 0.004059
2022-01-07 00:16:43,081 iteration 5597 : loss : 0.015940, loss_ce: 0.005261
2022-01-07 00:16:44,691 iteration 5598 : loss : 0.016244, loss_ce: 0.006688
2022-01-07 00:16:46,280 iteration 5599 : loss : 0.014225, loss_ce: 0.004872
2022-01-07 00:16:47,851 iteration 5600 : loss : 0.016991, loss_ce: 0.007770
2022-01-07 00:16:49,414 iteration 5601 : loss : 0.015382, loss_ce: 0.004397
2022-01-07 00:16:50,905 iteration 5602 : loss : 0.018705, loss_ce: 0.005251
2022-01-07 00:16:52,479 iteration 5603 : loss : 0.024885, loss_ce: 0.010525
2022-01-07 00:16:53,974 iteration 5604 : loss : 0.014190, loss_ce: 0.006260
2022-01-07 00:16:55,478 iteration 5605 : loss : 0.017125, loss_ce: 0.005350
2022-01-07 00:16:57,063 iteration 5606 : loss : 0.024636, loss_ce: 0.008378
2022-01-07 00:16:58,585 iteration 5607 : loss : 0.021322, loss_ce: 0.008835
2022-01-07 00:17:00,212 iteration 5608 : loss : 0.020154, loss_ce: 0.009272
2022-01-07 00:17:01,680 iteration 5609 : loss : 0.019089, loss_ce: 0.008820
2022-01-07 00:17:01,680 Training Data Eval:
2022-01-07 00:17:09,489   Average segmentation loss on training set: 0.0132
2022-01-07 00:17:09,490 Validation Data Eval:
2022-01-07 00:17:12,200   Average segmentation loss on validation set: 0.0838
2022-01-07 00:17:13,767 iteration 5610 : loss : 0.016933, loss_ce: 0.006366
 82%|███████████████████████▉     | 330/400 [2:36:00<34:51, 29.88s/it]2022-01-07 00:17:15,398 iteration 5611 : loss : 0.018074, loss_ce: 0.008403
2022-01-07 00:17:16,942 iteration 5612 : loss : 0.016462, loss_ce: 0.005539
2022-01-07 00:17:18,479 iteration 5613 : loss : 0.036050, loss_ce: 0.017917
2022-01-07 00:17:20,021 iteration 5614 : loss : 0.020595, loss_ce: 0.006765
2022-01-07 00:17:21,584 iteration 5615 : loss : 0.020208, loss_ce: 0.007665
2022-01-07 00:17:23,155 iteration 5616 : loss : 0.030387, loss_ce: 0.013459
2022-01-07 00:17:24,696 iteration 5617 : loss : 0.014473, loss_ce: 0.005399
2022-01-07 00:17:26,180 iteration 5618 : loss : 0.016413, loss_ce: 0.007048
2022-01-07 00:17:27,748 iteration 5619 : loss : 0.041993, loss_ce: 0.014934
2022-01-07 00:17:29,322 iteration 5620 : loss : 0.017676, loss_ce: 0.005887
2022-01-07 00:17:30,820 iteration 5621 : loss : 0.018973, loss_ce: 0.008223
2022-01-07 00:17:32,417 iteration 5622 : loss : 0.021622, loss_ce: 0.009249
2022-01-07 00:17:33,899 iteration 5623 : loss : 0.021321, loss_ce: 0.006077
2022-01-07 00:17:35,454 iteration 5624 : loss : 0.021914, loss_ce: 0.008794
2022-01-07 00:17:37,016 iteration 5625 : loss : 0.026259, loss_ce: 0.010824
2022-01-07 00:17:38,542 iteration 5626 : loss : 0.021216, loss_ce: 0.009519
2022-01-07 00:17:40,055 iteration 5627 : loss : 0.024784, loss_ce: 0.007666
 83%|███████████████████████▉     | 331/400 [2:36:26<33:07, 28.80s/it]2022-01-07 00:17:41,530 iteration 5628 : loss : 0.015267, loss_ce: 0.005600
2022-01-07 00:17:43,073 iteration 5629 : loss : 0.019933, loss_ce: 0.009123
2022-01-07 00:17:44,588 iteration 5630 : loss : 0.038229, loss_ce: 0.011496
2022-01-07 00:17:46,195 iteration 5631 : loss : 0.022860, loss_ce: 0.011563
2022-01-07 00:17:47,642 iteration 5632 : loss : 0.019549, loss_ce: 0.008409
2022-01-07 00:17:49,210 iteration 5633 : loss : 0.028616, loss_ce: 0.010663
2022-01-07 00:17:50,699 iteration 5634 : loss : 0.026167, loss_ce: 0.008547
2022-01-07 00:17:52,262 iteration 5635 : loss : 0.018932, loss_ce: 0.004839
2022-01-07 00:17:53,742 iteration 5636 : loss : 0.014687, loss_ce: 0.006603
2022-01-07 00:17:55,291 iteration 5637 : loss : 0.021506, loss_ce: 0.009315
2022-01-07 00:17:56,715 iteration 5638 : loss : 0.016894, loss_ce: 0.004966
2022-01-07 00:17:58,189 iteration 5639 : loss : 0.027320, loss_ce: 0.007349
2022-01-07 00:17:59,695 iteration 5640 : loss : 0.015853, loss_ce: 0.007791
2022-01-07 00:18:01,240 iteration 5641 : loss : 0.019173, loss_ce: 0.009317
2022-01-07 00:18:02,745 iteration 5642 : loss : 0.021836, loss_ce: 0.004373
2022-01-07 00:18:04,306 iteration 5643 : loss : 0.020310, loss_ce: 0.006441
2022-01-07 00:18:05,842 iteration 5644 : loss : 0.023299, loss_ce: 0.008759
 83%|████████████████████████     | 332/400 [2:36:52<31:37, 27.90s/it]2022-01-07 00:18:07,515 iteration 5645 : loss : 0.022675, loss_ce: 0.009291
2022-01-07 00:18:08,991 iteration 5646 : loss : 0.022219, loss_ce: 0.007651
2022-01-07 00:18:10,499 iteration 5647 : loss : 0.019631, loss_ce: 0.008674
2022-01-07 00:18:12,048 iteration 5648 : loss : 0.029313, loss_ce: 0.009029
2022-01-07 00:18:13,603 iteration 5649 : loss : 0.016733, loss_ce: 0.007343
2022-01-07 00:18:15,127 iteration 5650 : loss : 0.017425, loss_ce: 0.007002
2022-01-07 00:18:16,647 iteration 5651 : loss : 0.022321, loss_ce: 0.011617
2022-01-07 00:18:18,114 iteration 5652 : loss : 0.013287, loss_ce: 0.004966
2022-01-07 00:18:19,629 iteration 5653 : loss : 0.022092, loss_ce: 0.007288
2022-01-07 00:18:21,100 iteration 5654 : loss : 0.016691, loss_ce: 0.004962
2022-01-07 00:18:22,639 iteration 5655 : loss : 0.029292, loss_ce: 0.011387
2022-01-07 00:18:24,157 iteration 5656 : loss : 0.014800, loss_ce: 0.004611
2022-01-07 00:18:25,683 iteration 5657 : loss : 0.017439, loss_ce: 0.006608
2022-01-07 00:18:27,198 iteration 5658 : loss : 0.028328, loss_ce: 0.012039
2022-01-07 00:18:28,691 iteration 5659 : loss : 0.015951, loss_ce: 0.004557
2022-01-07 00:18:30,162 iteration 5660 : loss : 0.013321, loss_ce: 0.004588
2022-01-07 00:18:31,766 iteration 5661 : loss : 0.021436, loss_ce: 0.006520
 83%|████████████████████████▏    | 333/400 [2:37:18<30:29, 27.31s/it]2022-01-07 00:18:33,338 iteration 5662 : loss : 0.023019, loss_ce: 0.006667
2022-01-07 00:18:34,841 iteration 5663 : loss : 0.018440, loss_ce: 0.007025
2022-01-07 00:18:36,356 iteration 5664 : loss : 0.026400, loss_ce: 0.008323
2022-01-07 00:18:37,796 iteration 5665 : loss : 0.017385, loss_ce: 0.006181
2022-01-07 00:18:39,344 iteration 5666 : loss : 0.016740, loss_ce: 0.005998
2022-01-07 00:18:40,909 iteration 5667 : loss : 0.016718, loss_ce: 0.006766
2022-01-07 00:18:42,415 iteration 5668 : loss : 0.027928, loss_ce: 0.012867
2022-01-07 00:18:43,948 iteration 5669 : loss : 0.018295, loss_ce: 0.009096
2022-01-07 00:18:45,466 iteration 5670 : loss : 0.015021, loss_ce: 0.005671
2022-01-07 00:18:46,986 iteration 5671 : loss : 0.018133, loss_ce: 0.006654
2022-01-07 00:18:48,485 iteration 5672 : loss : 0.023228, loss_ce: 0.006000
2022-01-07 00:18:49,957 iteration 5673 : loss : 0.019711, loss_ce: 0.006411
2022-01-07 00:18:51,527 iteration 5674 : loss : 0.020625, loss_ce: 0.010059
2022-01-07 00:18:53,007 iteration 5675 : loss : 0.015354, loss_ce: 0.006814
2022-01-07 00:18:54,584 iteration 5676 : loss : 0.017186, loss_ce: 0.006242
2022-01-07 00:18:56,165 iteration 5677 : loss : 0.014464, loss_ce: 0.006637
2022-01-07 00:18:57,730 iteration 5678 : loss : 0.020584, loss_ce: 0.006766
 84%|████████████████████████▏    | 334/400 [2:37:44<29:35, 26.90s/it]2022-01-07 00:18:59,332 iteration 5679 : loss : 0.025304, loss_ce: 0.011491
2022-01-07 00:19:00,786 iteration 5680 : loss : 0.016239, loss_ce: 0.004834
2022-01-07 00:19:02,375 iteration 5681 : loss : 0.024275, loss_ce: 0.009373
2022-01-07 00:19:03,964 iteration 5682 : loss : 0.020248, loss_ce: 0.008829
2022-01-07 00:19:05,473 iteration 5683 : loss : 0.015399, loss_ce: 0.005121
2022-01-07 00:19:06,962 iteration 5684 : loss : 0.013821, loss_ce: 0.007140
2022-01-07 00:19:08,503 iteration 5685 : loss : 0.026580, loss_ce: 0.012010
2022-01-07 00:19:10,013 iteration 5686 : loss : 0.015856, loss_ce: 0.005650
2022-01-07 00:19:11,534 iteration 5687 : loss : 0.012949, loss_ce: 0.005034
2022-01-07 00:19:13,137 iteration 5688 : loss : 0.017381, loss_ce: 0.006792
2022-01-07 00:19:14,664 iteration 5689 : loss : 0.018552, loss_ce: 0.004806
2022-01-07 00:19:16,205 iteration 5690 : loss : 0.018452, loss_ce: 0.008012
2022-01-07 00:19:17,746 iteration 5691 : loss : 0.020158, loss_ce: 0.008802
2022-01-07 00:19:19,283 iteration 5692 : loss : 0.031732, loss_ce: 0.014492
2022-01-07 00:19:20,800 iteration 5693 : loss : 0.034226, loss_ce: 0.008919
2022-01-07 00:19:22,306 iteration 5694 : loss : 0.025109, loss_ce: 0.010512
2022-01-07 00:19:22,306 Training Data Eval:
2022-01-07 00:19:30,163   Average segmentation loss on training set: 0.0105
2022-01-07 00:19:30,163 Validation Data Eval:
2022-01-07 00:19:32,935   Average segmentation loss on validation set: 0.0738
2022-01-07 00:19:34,441 iteration 5695 : loss : 0.014700, loss_ce: 0.004839
 84%|████████████████████████▎    | 335/400 [2:38:21<32:19, 29.84s/it]2022-01-07 00:19:36,014 iteration 5696 : loss : 0.013005, loss_ce: 0.006421
2022-01-07 00:19:37,516 iteration 5697 : loss : 0.013833, loss_ce: 0.006244
2022-01-07 00:19:39,128 iteration 5698 : loss : 0.019233, loss_ce: 0.006464
2022-01-07 00:19:40,612 iteration 5699 : loss : 0.015151, loss_ce: 0.006557
2022-01-07 00:19:42,147 iteration 5700 : loss : 0.018465, loss_ce: 0.007540
2022-01-07 00:19:43,642 iteration 5701 : loss : 0.012865, loss_ce: 0.005581
2022-01-07 00:19:45,278 iteration 5702 : loss : 0.021931, loss_ce: 0.007675
2022-01-07 00:19:46,845 iteration 5703 : loss : 0.020579, loss_ce: 0.008350
2022-01-07 00:19:48,405 iteration 5704 : loss : 0.018823, loss_ce: 0.006848
2022-01-07 00:19:49,970 iteration 5705 : loss : 0.018490, loss_ce: 0.006479
2022-01-07 00:19:51,431 iteration 5706 : loss : 0.014107, loss_ce: 0.005334
2022-01-07 00:19:52,960 iteration 5707 : loss : 0.015002, loss_ce: 0.005026
2022-01-07 00:19:54,447 iteration 5708 : loss : 0.015463, loss_ce: 0.005517
2022-01-07 00:19:55,980 iteration 5709 : loss : 0.015043, loss_ce: 0.005268
2022-01-07 00:19:57,497 iteration 5710 : loss : 0.024912, loss_ce: 0.007370
2022-01-07 00:19:58,961 iteration 5711 : loss : 0.017612, loss_ce: 0.005609
2022-01-07 00:20:00,501 iteration 5712 : loss : 0.019239, loss_ce: 0.008417
 84%|████████████████████████▎    | 336/400 [2:38:47<30:37, 28.71s/it]2022-01-07 00:20:02,106 iteration 5713 : loss : 0.019858, loss_ce: 0.005869
2022-01-07 00:20:03,657 iteration 5714 : loss : 0.024681, loss_ce: 0.008266
2022-01-07 00:20:05,234 iteration 5715 : loss : 0.014543, loss_ce: 0.005785
2022-01-07 00:20:06,737 iteration 5716 : loss : 0.016471, loss_ce: 0.008137
2022-01-07 00:20:08,339 iteration 5717 : loss : 0.016574, loss_ce: 0.006874
2022-01-07 00:20:09,811 iteration 5718 : loss : 0.012279, loss_ce: 0.004085
2022-01-07 00:20:11,281 iteration 5719 : loss : 0.015583, loss_ce: 0.007208
2022-01-07 00:20:12,844 iteration 5720 : loss : 0.023531, loss_ce: 0.010058
2022-01-07 00:20:14,412 iteration 5721 : loss : 0.024962, loss_ce: 0.012623
2022-01-07 00:20:15,914 iteration 5722 : loss : 0.021802, loss_ce: 0.009150
2022-01-07 00:20:17,444 iteration 5723 : loss : 0.019413, loss_ce: 0.006990
2022-01-07 00:20:18,997 iteration 5724 : loss : 0.017514, loss_ce: 0.005709
2022-01-07 00:20:20,504 iteration 5725 : loss : 0.039042, loss_ce: 0.013089
2022-01-07 00:20:21,974 iteration 5726 : loss : 0.030905, loss_ce: 0.006765
2022-01-07 00:20:23,503 iteration 5727 : loss : 0.019276, loss_ce: 0.006424
2022-01-07 00:20:24,980 iteration 5728 : loss : 0.031316, loss_ce: 0.009258
2022-01-07 00:20:26,547 iteration 5729 : loss : 0.021449, loss_ce: 0.009256
 84%|████████████████████████▍    | 337/400 [2:39:13<29:18, 27.91s/it]2022-01-07 00:20:28,069 iteration 5730 : loss : 0.013898, loss_ce: 0.006127
2022-01-07 00:20:29,614 iteration 5731 : loss : 0.018355, loss_ce: 0.008228
2022-01-07 00:20:31,075 iteration 5732 : loss : 0.018259, loss_ce: 0.006650
2022-01-07 00:20:32,639 iteration 5733 : loss : 0.036853, loss_ce: 0.009478
2022-01-07 00:20:34,189 iteration 5734 : loss : 0.021553, loss_ce: 0.008588
2022-01-07 00:20:35,675 iteration 5735 : loss : 0.018465, loss_ce: 0.005332
2022-01-07 00:20:37,164 iteration 5736 : loss : 0.020598, loss_ce: 0.008138
2022-01-07 00:20:38,739 iteration 5737 : loss : 0.025294, loss_ce: 0.009360
2022-01-07 00:20:40,220 iteration 5738 : loss : 0.023400, loss_ce: 0.006900
2022-01-07 00:20:41,704 iteration 5739 : loss : 0.015251, loss_ce: 0.005812
2022-01-07 00:20:43,214 iteration 5740 : loss : 0.016195, loss_ce: 0.006211
2022-01-07 00:20:44,711 iteration 5741 : loss : 0.023237, loss_ce: 0.011534
2022-01-07 00:20:46,219 iteration 5742 : loss : 0.016633, loss_ce: 0.003716
2022-01-07 00:20:47,788 iteration 5743 : loss : 0.016069, loss_ce: 0.007353
2022-01-07 00:20:49,346 iteration 5744 : loss : 0.024647, loss_ce: 0.012294
2022-01-07 00:20:50,900 iteration 5745 : loss : 0.016841, loss_ce: 0.007759
2022-01-07 00:20:52,358 iteration 5746 : loss : 0.021694, loss_ce: 0.006180
 84%|████████████████████████▌    | 338/400 [2:39:39<28:11, 27.28s/it]2022-01-07 00:20:53,923 iteration 5747 : loss : 0.017836, loss_ce: 0.006684
2022-01-07 00:20:55,451 iteration 5748 : loss : 0.022381, loss_ce: 0.008837
2022-01-07 00:20:56,972 iteration 5749 : loss : 0.023408, loss_ce: 0.009437
2022-01-07 00:20:58,486 iteration 5750 : loss : 0.017468, loss_ce: 0.006457
2022-01-07 00:20:59,954 iteration 5751 : loss : 0.017172, loss_ce: 0.005655
2022-01-07 00:21:01,457 iteration 5752 : loss : 0.017478, loss_ce: 0.005401
2022-01-07 00:21:03,016 iteration 5753 : loss : 0.016285, loss_ce: 0.006579
2022-01-07 00:21:04,571 iteration 5754 : loss : 0.016294, loss_ce: 0.005032
2022-01-07 00:21:06,076 iteration 5755 : loss : 0.015678, loss_ce: 0.004838
2022-01-07 00:21:07,560 iteration 5756 : loss : 0.015838, loss_ce: 0.005497
2022-01-07 00:21:08,982 iteration 5757 : loss : 0.013169, loss_ce: 0.005365
2022-01-07 00:21:10,555 iteration 5758 : loss : 0.035841, loss_ce: 0.015354
2022-01-07 00:21:12,122 iteration 5759 : loss : 0.014941, loss_ce: 0.006035
2022-01-07 00:21:13,718 iteration 5760 : loss : 0.018776, loss_ce: 0.009737
2022-01-07 00:21:15,312 iteration 5761 : loss : 0.027345, loss_ce: 0.011745
2022-01-07 00:21:16,772 iteration 5762 : loss : 0.012616, loss_ce: 0.005386
2022-01-07 00:21:18,319 iteration 5763 : loss : 0.019607, loss_ce: 0.009882
 85%|████████████████████████▌    | 339/400 [2:40:05<27:19, 26.88s/it]2022-01-07 00:21:19,886 iteration 5764 : loss : 0.017069, loss_ce: 0.006359
2022-01-07 00:21:21,446 iteration 5765 : loss : 0.046826, loss_ce: 0.013338
2022-01-07 00:21:22,985 iteration 5766 : loss : 0.023996, loss_ce: 0.012780
2022-01-07 00:21:24,507 iteration 5767 : loss : 0.017069, loss_ce: 0.009844
2022-01-07 00:21:26,129 iteration 5768 : loss : 0.016885, loss_ce: 0.006273
2022-01-07 00:21:27,705 iteration 5769 : loss : 0.023658, loss_ce: 0.008042
2022-01-07 00:21:29,197 iteration 5770 : loss : 0.014972, loss_ce: 0.005192
2022-01-07 00:21:30,736 iteration 5771 : loss : 0.019968, loss_ce: 0.007201
2022-01-07 00:21:32,244 iteration 5772 : loss : 0.017541, loss_ce: 0.006212
2022-01-07 00:21:33,692 iteration 5773 : loss : 0.014690, loss_ce: 0.004720
2022-01-07 00:21:35,134 iteration 5774 : loss : 0.015582, loss_ce: 0.004138
2022-01-07 00:21:36,679 iteration 5775 : loss : 0.016204, loss_ce: 0.006633
2022-01-07 00:21:38,284 iteration 5776 : loss : 0.021636, loss_ce: 0.010193
2022-01-07 00:21:39,728 iteration 5777 : loss : 0.015068, loss_ce: 0.006525
2022-01-07 00:21:41,236 iteration 5778 : loss : 0.019753, loss_ce: 0.006339
2022-01-07 00:21:42,760 iteration 5779 : loss : 0.014596, loss_ce: 0.004861
2022-01-07 00:21:42,760 Training Data Eval:
2022-01-07 00:21:50,677   Average segmentation loss on training set: 0.0100
2022-01-07 00:21:50,677 Validation Data Eval:
2022-01-07 00:21:53,393   Average segmentation loss on validation set: 0.0642
2022-01-07 00:21:59,864 Found new lowest validation loss at iteration 5779! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-07 00:22:01,339 iteration 5780 : loss : 0.020459, loss_ce: 0.006964
 85%|████████████████████████▋    | 340/400 [2:40:48<31:43, 31.73s/it]2022-01-07 00:22:02,809 iteration 5781 : loss : 0.022434, loss_ce: 0.007086
2022-01-07 00:22:04,303 iteration 5782 : loss : 0.016544, loss_ce: 0.006388
2022-01-07 00:22:05,741 iteration 5783 : loss : 0.019532, loss_ce: 0.007871
2022-01-07 00:22:07,202 iteration 5784 : loss : 0.015459, loss_ce: 0.007916
2022-01-07 00:22:08,602 iteration 5785 : loss : 0.020114, loss_ce: 0.009386
2022-01-07 00:22:10,012 iteration 5786 : loss : 0.014399, loss_ce: 0.005989
2022-01-07 00:22:11,415 iteration 5787 : loss : 0.014708, loss_ce: 0.005297
2022-01-07 00:22:12,978 iteration 5788 : loss : 0.023347, loss_ce: 0.008788
2022-01-07 00:22:14,443 iteration 5789 : loss : 0.015716, loss_ce: 0.005647
2022-01-07 00:22:15,890 iteration 5790 : loss : 0.014672, loss_ce: 0.005151
2022-01-07 00:22:17,328 iteration 5791 : loss : 0.014651, loss_ce: 0.005278
2022-01-07 00:22:18,861 iteration 5792 : loss : 0.019644, loss_ce: 0.007376
2022-01-07 00:22:20,466 iteration 5793 : loss : 0.022250, loss_ce: 0.008372
2022-01-07 00:22:21,993 iteration 5794 : loss : 0.014806, loss_ce: 0.005452
2022-01-07 00:22:23,508 iteration 5795 : loss : 0.016850, loss_ce: 0.005708
2022-01-07 00:22:24,978 iteration 5796 : loss : 0.013811, loss_ce: 0.003871
2022-01-07 00:22:26,512 iteration 5797 : loss : 0.023646, loss_ce: 0.009014
 85%|████████████████████████▋    | 341/400 [2:41:13<29:15, 29.76s/it]2022-01-07 00:22:28,114 iteration 5798 : loss : 0.022836, loss_ce: 0.009069
2022-01-07 00:22:29,637 iteration 5799 : loss : 0.017643, loss_ce: 0.007474
2022-01-07 00:22:31,146 iteration 5800 : loss : 0.014939, loss_ce: 0.006274
2022-01-07 00:22:32,654 iteration 5801 : loss : 0.021032, loss_ce: 0.007021
2022-01-07 00:22:34,111 iteration 5802 : loss : 0.013937, loss_ce: 0.006264
2022-01-07 00:22:35,595 iteration 5803 : loss : 0.020745, loss_ce: 0.011213
2022-01-07 00:22:37,111 iteration 5804 : loss : 0.014976, loss_ce: 0.005479
2022-01-07 00:22:38,707 iteration 5805 : loss : 0.016237, loss_ce: 0.005729
2022-01-07 00:22:40,203 iteration 5806 : loss : 0.020740, loss_ce: 0.006066
2022-01-07 00:22:41,781 iteration 5807 : loss : 0.022945, loss_ce: 0.007018
2022-01-07 00:22:43,306 iteration 5808 : loss : 0.021002, loss_ce: 0.009777
2022-01-07 00:22:44,758 iteration 5809 : loss : 0.011925, loss_ce: 0.002915
2022-01-07 00:22:46,230 iteration 5810 : loss : 0.018211, loss_ce: 0.007016
2022-01-07 00:22:47,679 iteration 5811 : loss : 0.012217, loss_ce: 0.003480
2022-01-07 00:22:49,194 iteration 5812 : loss : 0.019060, loss_ce: 0.006442
2022-01-07 00:22:50,657 iteration 5813 : loss : 0.018870, loss_ce: 0.007414
2022-01-07 00:22:52,221 iteration 5814 : loss : 0.017049, loss_ce: 0.007042
 86%|████████████████████████▊    | 342/400 [2:41:38<27:35, 28.55s/it]2022-01-07 00:22:53,724 iteration 5815 : loss : 0.013808, loss_ce: 0.004805
2022-01-07 00:22:55,217 iteration 5816 : loss : 0.012305, loss_ce: 0.005771
2022-01-07 00:22:56,697 iteration 5817 : loss : 0.015344, loss_ce: 0.006409
2022-01-07 00:22:58,125 iteration 5818 : loss : 0.015198, loss_ce: 0.006489
2022-01-07 00:22:59,673 iteration 5819 : loss : 0.021081, loss_ce: 0.007530
2022-01-07 00:23:01,240 iteration 5820 : loss : 0.030636, loss_ce: 0.014754
2022-01-07 00:23:02,679 iteration 5821 : loss : 0.012836, loss_ce: 0.004313
2022-01-07 00:23:04,159 iteration 5822 : loss : 0.010556, loss_ce: 0.004710
2022-01-07 00:23:05,761 iteration 5823 : loss : 0.016804, loss_ce: 0.007201
2022-01-07 00:23:07,270 iteration 5824 : loss : 0.016658, loss_ce: 0.004899
2022-01-07 00:23:08,843 iteration 5825 : loss : 0.018101, loss_ce: 0.006919
2022-01-07 00:23:10,417 iteration 5826 : loss : 0.019270, loss_ce: 0.004258
2022-01-07 00:23:11,936 iteration 5827 : loss : 0.019698, loss_ce: 0.006232
2022-01-07 00:23:13,549 iteration 5828 : loss : 0.023274, loss_ce: 0.008717
2022-01-07 00:23:15,052 iteration 5829 : loss : 0.015937, loss_ce: 0.006194
2022-01-07 00:23:16,524 iteration 5830 : loss : 0.027246, loss_ce: 0.010969
2022-01-07 00:23:18,018 iteration 5831 : loss : 0.023764, loss_ce: 0.008874
 86%|████████████████████████▊    | 343/400 [2:42:04<26:19, 27.72s/it]2022-01-07 00:23:19,581 iteration 5832 : loss : 0.014888, loss_ce: 0.004523
2022-01-07 00:23:21,124 iteration 5833 : loss : 0.016749, loss_ce: 0.006681
2022-01-07 00:23:22,567 iteration 5834 : loss : 0.013294, loss_ce: 0.004905
2022-01-07 00:23:24,019 iteration 5835 : loss : 0.020417, loss_ce: 0.006306
2022-01-07 00:23:25,567 iteration 5836 : loss : 0.026405, loss_ce: 0.014197
2022-01-07 00:23:27,075 iteration 5837 : loss : 0.019014, loss_ce: 0.003988
2022-01-07 00:23:28,509 iteration 5838 : loss : 0.011077, loss_ce: 0.003229
2022-01-07 00:23:30,016 iteration 5839 : loss : 0.015578, loss_ce: 0.005428
2022-01-07 00:23:31,556 iteration 5840 : loss : 0.021665, loss_ce: 0.007559
2022-01-07 00:23:33,037 iteration 5841 : loss : 0.017111, loss_ce: 0.008369
2022-01-07 00:23:34,522 iteration 5842 : loss : 0.011504, loss_ce: 0.004210
2022-01-07 00:23:35,991 iteration 5843 : loss : 0.017132, loss_ce: 0.006471
2022-01-07 00:23:37,616 iteration 5844 : loss : 0.029936, loss_ce: 0.008527
2022-01-07 00:23:39,117 iteration 5845 : loss : 0.020444, loss_ce: 0.007511
2022-01-07 00:23:40,631 iteration 5846 : loss : 0.021156, loss_ce: 0.009114
2022-01-07 00:23:42,154 iteration 5847 : loss : 0.017281, loss_ce: 0.006887
2022-01-07 00:23:43,662 iteration 5848 : loss : 0.015405, loss_ce: 0.005385
 86%|████████████████████████▉    | 344/400 [2:42:30<25:17, 27.10s/it]2022-01-07 00:23:45,215 iteration 5849 : loss : 0.018713, loss_ce: 0.007882
2022-01-07 00:23:46,662 iteration 5850 : loss : 0.012512, loss_ce: 0.005163
2022-01-07 00:23:48,169 iteration 5851 : loss : 0.017925, loss_ce: 0.007422
2022-01-07 00:23:49,649 iteration 5852 : loss : 0.013770, loss_ce: 0.005276
2022-01-07 00:23:51,081 iteration 5853 : loss : 0.016547, loss_ce: 0.005553
2022-01-07 00:23:52,559 iteration 5854 : loss : 0.021263, loss_ce: 0.007281
2022-01-07 00:23:54,126 iteration 5855 : loss : 0.020578, loss_ce: 0.008512
2022-01-07 00:23:55,626 iteration 5856 : loss : 0.025698, loss_ce: 0.011994
2022-01-07 00:23:57,137 iteration 5857 : loss : 0.017307, loss_ce: 0.007241
2022-01-07 00:23:58,571 iteration 5858 : loss : 0.014681, loss_ce: 0.005584
2022-01-07 00:24:00,117 iteration 5859 : loss : 0.016160, loss_ce: 0.006017
2022-01-07 00:24:01,677 iteration 5860 : loss : 0.020658, loss_ce: 0.006421
2022-01-07 00:24:03,178 iteration 5861 : loss : 0.018946, loss_ce: 0.005263
2022-01-07 00:24:04,731 iteration 5862 : loss : 0.017050, loss_ce: 0.007668
2022-01-07 00:24:06,162 iteration 5863 : loss : 0.012019, loss_ce: 0.003985
2022-01-07 00:24:07,694 iteration 5864 : loss : 0.021193, loss_ce: 0.006971
2022-01-07 00:24:07,694 Training Data Eval:
2022-01-07 00:24:15,587   Average segmentation loss on training set: 0.0096
2022-01-07 00:24:15,588 Validation Data Eval:
2022-01-07 00:24:18,364   Average segmentation loss on validation set: 0.0709
2022-01-07 00:24:19,872 iteration 5865 : loss : 0.014695, loss_ce: 0.004786
 86%|█████████████████████████    | 345/400 [2:43:06<27:20, 29.84s/it]2022-01-07 00:24:21,569 iteration 5866 : loss : 0.014389, loss_ce: 0.006167
2022-01-07 00:24:23,187 iteration 5867 : loss : 0.028757, loss_ce: 0.008521
2022-01-07 00:24:24,788 iteration 5868 : loss : 0.019686, loss_ce: 0.007042
2022-01-07 00:24:26,281 iteration 5869 : loss : 0.013426, loss_ce: 0.005593
2022-01-07 00:24:27,918 iteration 5870 : loss : 0.025364, loss_ce: 0.007314
2022-01-07 00:24:29,514 iteration 5871 : loss : 0.014536, loss_ce: 0.005564
2022-01-07 00:24:31,121 iteration 5872 : loss : 0.032801, loss_ce: 0.010704
2022-01-07 00:24:32,791 iteration 5873 : loss : 0.028043, loss_ce: 0.008695
2022-01-07 00:24:34,309 iteration 5874 : loss : 0.016029, loss_ce: 0.005536
2022-01-07 00:24:35,842 iteration 5875 : loss : 0.016244, loss_ce: 0.008407
2022-01-07 00:24:37,390 iteration 5876 : loss : 0.018488, loss_ce: 0.006970
2022-01-07 00:24:38,930 iteration 5877 : loss : 0.017899, loss_ce: 0.006648
2022-01-07 00:24:40,491 iteration 5878 : loss : 0.020022, loss_ce: 0.008486
2022-01-07 00:24:41,993 iteration 5879 : loss : 0.014461, loss_ce: 0.006737
2022-01-07 00:24:43,619 iteration 5880 : loss : 0.017282, loss_ce: 0.004557
2022-01-07 00:24:45,099 iteration 5881 : loss : 0.013063, loss_ce: 0.004345
2022-01-07 00:24:46,613 iteration 5882 : loss : 0.016870, loss_ce: 0.007495
 86%|█████████████████████████    | 346/400 [2:43:33<26:00, 28.90s/it]2022-01-07 00:24:48,172 iteration 5883 : loss : 0.020703, loss_ce: 0.006624
2022-01-07 00:24:49,677 iteration 5884 : loss : 0.013591, loss_ce: 0.005822
2022-01-07 00:24:51,321 iteration 5885 : loss : 0.042417, loss_ce: 0.016977
2022-01-07 00:24:52,798 iteration 5886 : loss : 0.015943, loss_ce: 0.004860
2022-01-07 00:24:54,307 iteration 5887 : loss : 0.012919, loss_ce: 0.005195
2022-01-07 00:24:55,909 iteration 5888 : loss : 0.016827, loss_ce: 0.004900
2022-01-07 00:24:57,508 iteration 5889 : loss : 0.018334, loss_ce: 0.007404
2022-01-07 00:24:59,057 iteration 5890 : loss : 0.011441, loss_ce: 0.003816
2022-01-07 00:25:00,645 iteration 5891 : loss : 0.024662, loss_ce: 0.008449
2022-01-07 00:25:02,263 iteration 5892 : loss : 0.017169, loss_ce: 0.006699
2022-01-07 00:25:03,758 iteration 5893 : loss : 0.013056, loss_ce: 0.004270
2022-01-07 00:25:05,363 iteration 5894 : loss : 0.018180, loss_ce: 0.009362
2022-01-07 00:25:06,915 iteration 5895 : loss : 0.019579, loss_ce: 0.006601
2022-01-07 00:25:08,442 iteration 5896 : loss : 0.014522, loss_ce: 0.006333
2022-01-07 00:25:10,038 iteration 5897 : loss : 0.026691, loss_ce: 0.011079
2022-01-07 00:25:11,611 iteration 5898 : loss : 0.012387, loss_ce: 0.004309
2022-01-07 00:25:13,112 iteration 5899 : loss : 0.017075, loss_ce: 0.006944
 87%|█████████████████████████▏   | 347/400 [2:43:59<24:53, 28.18s/it]2022-01-07 00:25:14,701 iteration 5900 : loss : 0.014376, loss_ce: 0.004889
2022-01-07 00:25:16,294 iteration 5901 : loss : 0.018064, loss_ce: 0.005999
2022-01-07 00:25:17,811 iteration 5902 : loss : 0.016853, loss_ce: 0.006395
2022-01-07 00:25:19,381 iteration 5903 : loss : 0.010806, loss_ce: 0.004729
2022-01-07 00:25:20,878 iteration 5904 : loss : 0.016718, loss_ce: 0.006486
2022-01-07 00:25:22,433 iteration 5905 : loss : 0.019977, loss_ce: 0.005122
2022-01-07 00:25:23,900 iteration 5906 : loss : 0.016079, loss_ce: 0.005895
2022-01-07 00:25:25,450 iteration 5907 : loss : 0.013366, loss_ce: 0.004648
2022-01-07 00:25:26,970 iteration 5908 : loss : 0.013852, loss_ce: 0.004493
2022-01-07 00:25:28,429 iteration 5909 : loss : 0.015142, loss_ce: 0.004391
2022-01-07 00:25:29,950 iteration 5910 : loss : 0.021493, loss_ce: 0.007632
2022-01-07 00:25:31,491 iteration 5911 : loss : 0.026953, loss_ce: 0.010300
2022-01-07 00:25:32,942 iteration 5912 : loss : 0.015342, loss_ce: 0.004973
2022-01-07 00:25:34,439 iteration 5913 : loss : 0.016331, loss_ce: 0.006747
2022-01-07 00:25:35,965 iteration 5914 : loss : 0.018366, loss_ce: 0.006518
2022-01-07 00:25:37,409 iteration 5915 : loss : 0.014627, loss_ce: 0.006082
2022-01-07 00:25:38,980 iteration 5916 : loss : 0.023859, loss_ce: 0.008546
 87%|█████████████████████████▏   | 348/400 [2:44:25<23:49, 27.49s/it]2022-01-07 00:25:40,538 iteration 5917 : loss : 0.020018, loss_ce: 0.008020
2022-01-07 00:25:42,008 iteration 5918 : loss : 0.015130, loss_ce: 0.005695
2022-01-07 00:25:43,570 iteration 5919 : loss : 0.022526, loss_ce: 0.010180
2022-01-07 00:25:45,129 iteration 5920 : loss : 0.017498, loss_ce: 0.009130
2022-01-07 00:25:46,764 iteration 5921 : loss : 0.031213, loss_ce: 0.009466
2022-01-07 00:25:48,260 iteration 5922 : loss : 0.016623, loss_ce: 0.004551
2022-01-07 00:25:49,790 iteration 5923 : loss : 0.015833, loss_ce: 0.004765
2022-01-07 00:25:51,410 iteration 5924 : loss : 0.027532, loss_ce: 0.006023
2022-01-07 00:25:52,947 iteration 5925 : loss : 0.015206, loss_ce: 0.005357
2022-01-07 00:25:54,472 iteration 5926 : loss : 0.015139, loss_ce: 0.005924
2022-01-07 00:25:56,021 iteration 5927 : loss : 0.023704, loss_ce: 0.009464
2022-01-07 00:25:57,489 iteration 5928 : loss : 0.018500, loss_ce: 0.005609
2022-01-07 00:25:58,956 iteration 5929 : loss : 0.023674, loss_ce: 0.013632
2022-01-07 00:26:00,465 iteration 5930 : loss : 0.018072, loss_ce: 0.008862
2022-01-07 00:26:01,998 iteration 5931 : loss : 0.020782, loss_ce: 0.005486
2022-01-07 00:26:03,561 iteration 5932 : loss : 0.016851, loss_ce: 0.005850
2022-01-07 00:26:05,065 iteration 5933 : loss : 0.029084, loss_ce: 0.012731
 87%|█████████████████████████▎   | 349/400 [2:44:51<23:00, 27.06s/it]2022-01-07 00:26:06,582 iteration 5934 : loss : 0.015313, loss_ce: 0.004978
2022-01-07 00:26:08,215 iteration 5935 : loss : 0.016180, loss_ce: 0.005223
2022-01-07 00:26:09,725 iteration 5936 : loss : 0.013442, loss_ce: 0.005662
2022-01-07 00:26:11,210 iteration 5937 : loss : 0.013147, loss_ce: 0.004687
2022-01-07 00:26:12,655 iteration 5938 : loss : 0.010071, loss_ce: 0.003945
2022-01-07 00:26:14,196 iteration 5939 : loss : 0.021664, loss_ce: 0.009613
2022-01-07 00:26:15,658 iteration 5940 : loss : 0.015411, loss_ce: 0.006345
2022-01-07 00:26:17,139 iteration 5941 : loss : 0.019211, loss_ce: 0.008195
2022-01-07 00:26:18,616 iteration 5942 : loss : 0.015220, loss_ce: 0.007335
2022-01-07 00:26:20,134 iteration 5943 : loss : 0.019452, loss_ce: 0.006581
2022-01-07 00:26:21,643 iteration 5944 : loss : 0.015595, loss_ce: 0.007929
2022-01-07 00:26:23,123 iteration 5945 : loss : 0.017838, loss_ce: 0.004571
2022-01-07 00:26:24,725 iteration 5946 : loss : 0.028649, loss_ce: 0.010184
2022-01-07 00:26:26,269 iteration 5947 : loss : 0.026212, loss_ce: 0.009612
2022-01-07 00:26:27,755 iteration 5948 : loss : 0.017201, loss_ce: 0.007090
2022-01-07 00:26:29,334 iteration 5949 : loss : 0.016625, loss_ce: 0.006247
2022-01-07 00:26:29,334 Training Data Eval:
2022-01-07 00:26:37,212   Average segmentation loss on training set: 0.0096
2022-01-07 00:26:37,212 Validation Data Eval:
2022-01-07 00:26:39,956   Average segmentation loss on validation set: 0.0642
2022-01-07 00:26:46,081 Found new lowest validation loss at iteration 5949! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-07 00:26:47,581 iteration 5950 : loss : 0.015283, loss_ce: 0.005242
 88%|█████████████████████████▍   | 350/400 [2:45:34<26:25, 31.70s/it]2022-01-07 00:26:49,034 iteration 5951 : loss : 0.014648, loss_ce: 0.005993
2022-01-07 00:26:50,471 iteration 5952 : loss : 0.014765, loss_ce: 0.005378
2022-01-07 00:26:51,916 iteration 5953 : loss : 0.019183, loss_ce: 0.007557
2022-01-07 00:26:53,323 iteration 5954 : loss : 0.018385, loss_ce: 0.006244
2022-01-07 00:26:54,756 iteration 5955 : loss : 0.016303, loss_ce: 0.005458
2022-01-07 00:26:56,222 iteration 5956 : loss : 0.022547, loss_ce: 0.012837
2022-01-07 00:26:57,677 iteration 5957 : loss : 0.013068, loss_ce: 0.004678
2022-01-07 00:26:59,076 iteration 5958 : loss : 0.014062, loss_ce: 0.005207
2022-01-07 00:27:00,511 iteration 5959 : loss : 0.017192, loss_ce: 0.006573
2022-01-07 00:27:01,957 iteration 5960 : loss : 0.016631, loss_ce: 0.005275
2022-01-07 00:27:03,404 iteration 5961 : loss : 0.024338, loss_ce: 0.006379
2022-01-07 00:27:04,882 iteration 5962 : loss : 0.040717, loss_ce: 0.007180
2022-01-07 00:27:06,408 iteration 5963 : loss : 0.019343, loss_ce: 0.008133
2022-01-07 00:27:07,875 iteration 5964 : loss : 0.011018, loss_ce: 0.004854
2022-01-07 00:27:09,323 iteration 5965 : loss : 0.014062, loss_ce: 0.006189
2022-01-07 00:27:10,870 iteration 5966 : loss : 0.016172, loss_ce: 0.004421
2022-01-07 00:27:12,392 iteration 5967 : loss : 0.024323, loss_ce: 0.012628
 88%|█████████████████████████▍   | 351/400 [2:45:59<24:12, 29.63s/it]2022-01-07 00:27:14,023 iteration 5968 : loss : 0.019106, loss_ce: 0.008256
2022-01-07 00:27:15,506 iteration 5969 : loss : 0.017143, loss_ce: 0.006863
2022-01-07 00:27:17,044 iteration 5970 : loss : 0.015186, loss_ce: 0.006097
2022-01-07 00:27:18,486 iteration 5971 : loss : 0.012385, loss_ce: 0.003565
2022-01-07 00:27:19,901 iteration 5972 : loss : 0.013864, loss_ce: 0.004556
2022-01-07 00:27:21,418 iteration 5973 : loss : 0.021601, loss_ce: 0.009784
2022-01-07 00:27:23,009 iteration 5974 : loss : 0.028193, loss_ce: 0.011952
2022-01-07 00:27:24,468 iteration 5975 : loss : 0.020241, loss_ce: 0.007266
2022-01-07 00:27:25,975 iteration 5976 : loss : 0.014526, loss_ce: 0.004344
2022-01-07 00:27:27,444 iteration 5977 : loss : 0.017063, loss_ce: 0.004593
2022-01-07 00:27:28,910 iteration 5978 : loss : 0.012774, loss_ce: 0.005977
2022-01-07 00:27:30,430 iteration 5979 : loss : 0.014658, loss_ce: 0.006565
2022-01-07 00:27:31,999 iteration 5980 : loss : 0.014578, loss_ce: 0.005600
2022-01-07 00:27:33,495 iteration 5981 : loss : 0.014931, loss_ce: 0.006685
2022-01-07 00:27:35,027 iteration 5982 : loss : 0.018795, loss_ce: 0.006991
2022-01-07 00:27:36,613 iteration 5983 : loss : 0.015861, loss_ce: 0.005103
2022-01-07 00:27:38,123 iteration 5984 : loss : 0.018889, loss_ce: 0.007197
 88%|█████████████████████████▌   | 352/400 [2:46:24<22:46, 28.46s/it]2022-01-07 00:27:39,753 iteration 5985 : loss : 0.014595, loss_ce: 0.004377
2022-01-07 00:27:41,209 iteration 5986 : loss : 0.013401, loss_ce: 0.004094
2022-01-07 00:27:42,757 iteration 5987 : loss : 0.014888, loss_ce: 0.006679
2022-01-07 00:27:44,328 iteration 5988 : loss : 0.027253, loss_ce: 0.011082
2022-01-07 00:27:45,870 iteration 5989 : loss : 0.013120, loss_ce: 0.004645
2022-01-07 00:27:47,372 iteration 5990 : loss : 0.017212, loss_ce: 0.006162
2022-01-07 00:27:48,906 iteration 5991 : loss : 0.020190, loss_ce: 0.005959
2022-01-07 00:27:50,436 iteration 5992 : loss : 0.020076, loss_ce: 0.007941
2022-01-07 00:27:51,950 iteration 5993 : loss : 0.014091, loss_ce: 0.005359
2022-01-07 00:27:53,506 iteration 5994 : loss : 0.035183, loss_ce: 0.019321
2022-01-07 00:27:55,006 iteration 5995 : loss : 0.017417, loss_ce: 0.007610
2022-01-07 00:27:56,481 iteration 5996 : loss : 0.013765, loss_ce: 0.004468
2022-01-07 00:27:58,020 iteration 5997 : loss : 0.018763, loss_ce: 0.007034
2022-01-07 00:27:59,603 iteration 5998 : loss : 0.017205, loss_ce: 0.006019
2022-01-07 00:28:01,210 iteration 5999 : loss : 0.019299, loss_ce: 0.006465
2022-01-07 00:28:02,735 iteration 6000 : loss : 0.015614, loss_ce: 0.006813
2022-01-07 00:28:04,179 iteration 6001 : loss : 0.014997, loss_ce: 0.006870
 88%|█████████████████████████▌   | 353/400 [2:46:50<21:43, 27.74s/it]2022-01-07 00:28:05,780 iteration 6002 : loss : 0.018303, loss_ce: 0.006905
2022-01-07 00:28:07,269 iteration 6003 : loss : 0.016833, loss_ce: 0.006021
2022-01-07 00:28:08,835 iteration 6004 : loss : 0.016026, loss_ce: 0.007236
2022-01-07 00:28:10,362 iteration 6005 : loss : 0.019586, loss_ce: 0.007311
2022-01-07 00:28:11,980 iteration 6006 : loss : 0.022367, loss_ce: 0.009984
2022-01-07 00:28:13,507 iteration 6007 : loss : 0.011957, loss_ce: 0.004567
2022-01-07 00:28:15,023 iteration 6008 : loss : 0.022114, loss_ce: 0.008258
2022-01-07 00:28:16,460 iteration 6009 : loss : 0.012591, loss_ce: 0.005861
2022-01-07 00:28:17,918 iteration 6010 : loss : 0.012730, loss_ce: 0.005336
2022-01-07 00:28:19,468 iteration 6011 : loss : 0.016228, loss_ce: 0.005217
2022-01-07 00:28:21,046 iteration 6012 : loss : 0.023474, loss_ce: 0.005083
2022-01-07 00:28:22,594 iteration 6013 : loss : 0.017821, loss_ce: 0.005718
2022-01-07 00:28:24,152 iteration 6014 : loss : 0.038289, loss_ce: 0.010550
2022-01-07 00:28:25,669 iteration 6015 : loss : 0.015072, loss_ce: 0.006429
2022-01-07 00:28:27,233 iteration 6016 : loss : 0.017980, loss_ce: 0.006256
2022-01-07 00:28:28,790 iteration 6017 : loss : 0.019912, loss_ce: 0.009066
2022-01-07 00:28:30,378 iteration 6018 : loss : 0.020118, loss_ce: 0.007994
 88%|█████████████████████████▋   | 354/400 [2:47:17<20:54, 27.28s/it]2022-01-07 00:28:32,011 iteration 6019 : loss : 0.017787, loss_ce: 0.008402
2022-01-07 00:28:33,564 iteration 6020 : loss : 0.012912, loss_ce: 0.006035
2022-01-07 00:28:35,141 iteration 6021 : loss : 0.028462, loss_ce: 0.007237
2022-01-07 00:28:36,654 iteration 6022 : loss : 0.017494, loss_ce: 0.007601
2022-01-07 00:28:38,251 iteration 6023 : loss : 0.019237, loss_ce: 0.008807
2022-01-07 00:28:39,798 iteration 6024 : loss : 0.019267, loss_ce: 0.006104
2022-01-07 00:28:41,318 iteration 6025 : loss : 0.016697, loss_ce: 0.006712
2022-01-07 00:28:42,888 iteration 6026 : loss : 0.015113, loss_ce: 0.005512
2022-01-07 00:28:44,439 iteration 6027 : loss : 0.019548, loss_ce: 0.007555
2022-01-07 00:28:46,014 iteration 6028 : loss : 0.015567, loss_ce: 0.006424
2022-01-07 00:28:47,532 iteration 6029 : loss : 0.023626, loss_ce: 0.007193
2022-01-07 00:28:48,997 iteration 6030 : loss : 0.014212, loss_ce: 0.006457
2022-01-07 00:28:50,546 iteration 6031 : loss : 0.017975, loss_ce: 0.008832
2022-01-07 00:28:52,030 iteration 6032 : loss : 0.015573, loss_ce: 0.004771
2022-01-07 00:28:53,605 iteration 6033 : loss : 0.018976, loss_ce: 0.006263
2022-01-07 00:28:55,135 iteration 6034 : loss : 0.014506, loss_ce: 0.005350
2022-01-07 00:28:55,135 Training Data Eval:
2022-01-07 00:29:02,999   Average segmentation loss on training set: 0.0100
2022-01-07 00:29:03,000 Validation Data Eval:
2022-01-07 00:29:05,712   Average segmentation loss on validation set: 0.0667
2022-01-07 00:29:07,133 iteration 6035 : loss : 0.012037, loss_ce: 0.004515
 89%|█████████████████████████▋   | 355/400 [2:47:53<22:35, 30.12s/it]2022-01-07 00:29:08,797 iteration 6036 : loss : 0.023507, loss_ce: 0.005599
2022-01-07 00:29:10,361 iteration 6037 : loss : 0.018336, loss_ce: 0.006662
2022-01-07 00:29:11,864 iteration 6038 : loss : 0.020500, loss_ce: 0.008431
2022-01-07 00:29:13,358 iteration 6039 : loss : 0.012573, loss_ce: 0.003992
2022-01-07 00:29:14,887 iteration 6040 : loss : 0.016175, loss_ce: 0.006150
2022-01-07 00:29:16,378 iteration 6041 : loss : 0.018645, loss_ce: 0.006281
2022-01-07 00:29:17,930 iteration 6042 : loss : 0.012128, loss_ce: 0.005203
2022-01-07 00:29:19,415 iteration 6043 : loss : 0.015971, loss_ce: 0.008192
2022-01-07 00:29:20,885 iteration 6044 : loss : 0.014058, loss_ce: 0.006179
2022-01-07 00:29:22,514 iteration 6045 : loss : 0.021126, loss_ce: 0.009140
2022-01-07 00:29:23,992 iteration 6046 : loss : 0.017634, loss_ce: 0.007688
2022-01-07 00:29:25,465 iteration 6047 : loss : 0.011808, loss_ce: 0.004925
2022-01-07 00:29:27,056 iteration 6048 : loss : 0.024069, loss_ce: 0.007561
2022-01-07 00:29:28,526 iteration 6049 : loss : 0.014227, loss_ce: 0.005046
2022-01-07 00:29:29,998 iteration 6050 : loss : 0.016693, loss_ce: 0.004577
2022-01-07 00:29:31,471 iteration 6051 : loss : 0.011602, loss_ce: 0.004714
2022-01-07 00:29:33,032 iteration 6052 : loss : 0.015186, loss_ce: 0.005683
 89%|█████████████████████████▊   | 356/400 [2:48:19<21:09, 28.85s/it]2022-01-07 00:29:34,593 iteration 6053 : loss : 0.015838, loss_ce: 0.004864
2022-01-07 00:29:36,088 iteration 6054 : loss : 0.014415, loss_ce: 0.004893
2022-01-07 00:29:37,623 iteration 6055 : loss : 0.016123, loss_ce: 0.007109
2022-01-07 00:29:39,126 iteration 6056 : loss : 0.016077, loss_ce: 0.006029
2022-01-07 00:29:40,567 iteration 6057 : loss : 0.017495, loss_ce: 0.006801
2022-01-07 00:29:42,037 iteration 6058 : loss : 0.011204, loss_ce: 0.005899
2022-01-07 00:29:43,609 iteration 6059 : loss : 0.018002, loss_ce: 0.005989
2022-01-07 00:29:45,136 iteration 6060 : loss : 0.020229, loss_ce: 0.005749
2022-01-07 00:29:46,606 iteration 6061 : loss : 0.017701, loss_ce: 0.008291
2022-01-07 00:29:48,236 iteration 6062 : loss : 0.024912, loss_ce: 0.009403
2022-01-07 00:29:49,707 iteration 6063 : loss : 0.014950, loss_ce: 0.006664
2022-01-07 00:29:51,175 iteration 6064 : loss : 0.012912, loss_ce: 0.004400
2022-01-07 00:29:52,711 iteration 6065 : loss : 0.018324, loss_ce: 0.006607
2022-01-07 00:29:54,166 iteration 6066 : loss : 0.023110, loss_ce: 0.010582
2022-01-07 00:29:55,700 iteration 6067 : loss : 0.021358, loss_ce: 0.008142
2022-01-07 00:29:57,216 iteration 6068 : loss : 0.014567, loss_ce: 0.002541
2022-01-07 00:29:58,797 iteration 6069 : loss : 0.016409, loss_ce: 0.006796
 89%|█████████████████████████▉   | 357/400 [2:48:45<20:00, 27.93s/it]2022-01-07 00:30:00,364 iteration 6070 : loss : 0.018095, loss_ce: 0.008527
2022-01-07 00:30:01,871 iteration 6071 : loss : 0.014205, loss_ce: 0.005825
2022-01-07 00:30:03,376 iteration 6072 : loss : 0.020544, loss_ce: 0.007577
2022-01-07 00:30:04,936 iteration 6073 : loss : 0.023393, loss_ce: 0.007032
2022-01-07 00:30:06,418 iteration 6074 : loss : 0.020775, loss_ce: 0.012529
2022-01-07 00:30:08,016 iteration 6075 : loss : 0.029728, loss_ce: 0.013499
2022-01-07 00:30:09,582 iteration 6076 : loss : 0.011765, loss_ce: 0.004665
2022-01-07 00:30:11,039 iteration 6077 : loss : 0.011692, loss_ce: 0.004209
2022-01-07 00:30:12,578 iteration 6078 : loss : 0.016091, loss_ce: 0.008191
2022-01-07 00:30:14,190 iteration 6079 : loss : 0.015630, loss_ce: 0.006771
2022-01-07 00:30:15,681 iteration 6080 : loss : 0.012894, loss_ce: 0.004727
2022-01-07 00:30:17,297 iteration 6081 : loss : 0.021817, loss_ce: 0.008829
2022-01-07 00:30:18,830 iteration 6082 : loss : 0.022670, loss_ce: 0.006212
2022-01-07 00:30:20,414 iteration 6083 : loss : 0.015064, loss_ce: 0.005009
2022-01-07 00:30:21,922 iteration 6084 : loss : 0.016610, loss_ce: 0.004703
2022-01-07 00:30:23,427 iteration 6085 : loss : 0.015558, loss_ce: 0.006484
2022-01-07 00:30:25,107 iteration 6086 : loss : 0.022195, loss_ce: 0.005887
 90%|█████████████████████████▉   | 358/400 [2:49:11<19:12, 27.44s/it]2022-01-07 00:30:26,657 iteration 6087 : loss : 0.013644, loss_ce: 0.005510
2022-01-07 00:30:28,221 iteration 6088 : loss : 0.018598, loss_ce: 0.008473
2022-01-07 00:30:29,760 iteration 6089 : loss : 0.019247, loss_ce: 0.008946
2022-01-07 00:30:31,301 iteration 6090 : loss : 0.018295, loss_ce: 0.005319
2022-01-07 00:30:32,853 iteration 6091 : loss : 0.012266, loss_ce: 0.004495
2022-01-07 00:30:34,419 iteration 6092 : loss : 0.015010, loss_ce: 0.006206
2022-01-07 00:30:35,903 iteration 6093 : loss : 0.013938, loss_ce: 0.004860
2022-01-07 00:30:37,402 iteration 6094 : loss : 0.014303, loss_ce: 0.004581
2022-01-07 00:30:38,976 iteration 6095 : loss : 0.017504, loss_ce: 0.006862
2022-01-07 00:30:40,544 iteration 6096 : loss : 0.020001, loss_ce: 0.008293
2022-01-07 00:30:42,023 iteration 6097 : loss : 0.014742, loss_ce: 0.006416
2022-01-07 00:30:43,577 iteration 6098 : loss : 0.020166, loss_ce: 0.007380
2022-01-07 00:30:45,150 iteration 6099 : loss : 0.017295, loss_ce: 0.004303
2022-01-07 00:30:46,753 iteration 6100 : loss : 0.032779, loss_ce: 0.010920
2022-01-07 00:30:48,340 iteration 6101 : loss : 0.014222, loss_ce: 0.005156
2022-01-07 00:30:49,853 iteration 6102 : loss : 0.014695, loss_ce: 0.004636
2022-01-07 00:30:51,309 iteration 6103 : loss : 0.010751, loss_ce: 0.003889
 90%|██████████████████████████   | 359/400 [2:49:38<18:29, 27.07s/it]2022-01-07 00:30:52,958 iteration 6104 : loss : 0.022068, loss_ce: 0.006873
2022-01-07 00:30:54,466 iteration 6105 : loss : 0.015140, loss_ce: 0.005453
2022-01-07 00:30:55,943 iteration 6106 : loss : 0.014117, loss_ce: 0.004668
2022-01-07 00:30:57,442 iteration 6107 : loss : 0.021195, loss_ce: 0.008530
2022-01-07 00:30:59,034 iteration 6108 : loss : 0.013837, loss_ce: 0.003851
2022-01-07 00:31:00,560 iteration 6109 : loss : 0.014890, loss_ce: 0.005698
2022-01-07 00:31:02,126 iteration 6110 : loss : 0.014524, loss_ce: 0.006387
2022-01-07 00:31:03,727 iteration 6111 : loss : 0.020864, loss_ce: 0.008749
2022-01-07 00:31:05,307 iteration 6112 : loss : 0.015484, loss_ce: 0.005518
2022-01-07 00:31:06,905 iteration 6113 : loss : 0.020003, loss_ce: 0.006324
2022-01-07 00:31:08,425 iteration 6114 : loss : 0.015234, loss_ce: 0.006409
2022-01-07 00:31:10,024 iteration 6115 : loss : 0.028932, loss_ce: 0.011658
2022-01-07 00:31:11,570 iteration 6116 : loss : 0.015091, loss_ce: 0.006332
2022-01-07 00:31:13,068 iteration 6117 : loss : 0.020474, loss_ce: 0.009152
2022-01-07 00:31:14,650 iteration 6118 : loss : 0.015096, loss_ce: 0.006249
2022-01-07 00:31:16,156 iteration 6119 : loss : 0.016983, loss_ce: 0.005284
2022-01-07 00:31:16,156 Training Data Eval:
2022-01-07 00:31:24,025   Average segmentation loss on training set: 0.0098
2022-01-07 00:31:24,026 Validation Data Eval:
2022-01-07 00:31:26,752   Average segmentation loss on validation set: 0.0671
2022-01-07 00:31:28,279 iteration 6120 : loss : 0.015695, loss_ce: 0.007380
 90%|██████████████████████████   | 360/400 [2:50:15<20:01, 30.04s/it]2022-01-07 00:31:29,889 iteration 6121 : loss : 0.017129, loss_ce: 0.007717
2022-01-07 00:31:31,442 iteration 6122 : loss : 0.019180, loss_ce: 0.006654
2022-01-07 00:31:33,028 iteration 6123 : loss : 0.018704, loss_ce: 0.007490
2022-01-07 00:31:34,556 iteration 6124 : loss : 0.017459, loss_ce: 0.006029
2022-01-07 00:31:36,163 iteration 6125 : loss : 0.025890, loss_ce: 0.007710
2022-01-07 00:31:37,668 iteration 6126 : loss : 0.026716, loss_ce: 0.006807
2022-01-07 00:31:39,158 iteration 6127 : loss : 0.014026, loss_ce: 0.005353
2022-01-07 00:31:40,724 iteration 6128 : loss : 0.016122, loss_ce: 0.005392
2022-01-07 00:31:42,228 iteration 6129 : loss : 0.015867, loss_ce: 0.006038
2022-01-07 00:31:43,820 iteration 6130 : loss : 0.017106, loss_ce: 0.006314
2022-01-07 00:31:45,368 iteration 6131 : loss : 0.024910, loss_ce: 0.012194
2022-01-07 00:31:46,929 iteration 6132 : loss : 0.023583, loss_ce: 0.004433
2022-01-07 00:31:48,432 iteration 6133 : loss : 0.013914, loss_ce: 0.007088
2022-01-07 00:31:49,906 iteration 6134 : loss : 0.011639, loss_ce: 0.005168
2022-01-07 00:31:51,390 iteration 6135 : loss : 0.011485, loss_ce: 0.004196
2022-01-07 00:31:52,953 iteration 6136 : loss : 0.016535, loss_ce: 0.005023
2022-01-07 00:31:54,473 iteration 6137 : loss : 0.019695, loss_ce: 0.008023
 90%|██████████████████████████▏  | 361/400 [2:50:41<18:46, 28.89s/it]2022-01-07 00:31:56,021 iteration 6138 : loss : 0.013048, loss_ce: 0.004670
2022-01-07 00:31:57,592 iteration 6139 : loss : 0.017006, loss_ce: 0.007604
2022-01-07 00:31:59,120 iteration 6140 : loss : 0.012400, loss_ce: 0.004657
2022-01-07 00:32:00,692 iteration 6141 : loss : 0.015188, loss_ce: 0.005498
2022-01-07 00:32:02,366 iteration 6142 : loss : 0.020540, loss_ce: 0.007855
2022-01-07 00:32:03,843 iteration 6143 : loss : 0.014332, loss_ce: 0.006939
2022-01-07 00:32:05,383 iteration 6144 : loss : 0.012759, loss_ce: 0.005053
2022-01-07 00:32:06,924 iteration 6145 : loss : 0.025255, loss_ce: 0.007807
2022-01-07 00:32:08,435 iteration 6146 : loss : 0.014578, loss_ce: 0.006391
2022-01-07 00:32:09,961 iteration 6147 : loss : 0.012914, loss_ce: 0.003233
2022-01-07 00:32:11,448 iteration 6148 : loss : 0.011206, loss_ce: 0.003728
2022-01-07 00:32:12,926 iteration 6149 : loss : 0.016505, loss_ce: 0.005420
2022-01-07 00:32:14,437 iteration 6150 : loss : 0.024368, loss_ce: 0.006972
2022-01-07 00:32:15,958 iteration 6151 : loss : 0.022193, loss_ce: 0.005093
2022-01-07 00:32:17,446 iteration 6152 : loss : 0.018698, loss_ce: 0.007913
2022-01-07 00:32:19,041 iteration 6153 : loss : 0.019887, loss_ce: 0.007843
2022-01-07 00:32:20,662 iteration 6154 : loss : 0.019626, loss_ce: 0.007919
 90%|██████████████████████████▏  | 362/400 [2:51:07<17:46, 28.08s/it]2022-01-07 00:32:22,260 iteration 6155 : loss : 0.016343, loss_ce: 0.006738
2022-01-07 00:32:23,788 iteration 6156 : loss : 0.011311, loss_ce: 0.004947
2022-01-07 00:32:25,373 iteration 6157 : loss : 0.015601, loss_ce: 0.004895
2022-01-07 00:32:26,878 iteration 6158 : loss : 0.013774, loss_ce: 0.004936
2022-01-07 00:32:28,336 iteration 6159 : loss : 0.010259, loss_ce: 0.003977
2022-01-07 00:32:30,052 iteration 6160 : loss : 0.028674, loss_ce: 0.013505
2022-01-07 00:32:31,515 iteration 6161 : loss : 0.019250, loss_ce: 0.006032
2022-01-07 00:32:33,064 iteration 6162 : loss : 0.011415, loss_ce: 0.004141
2022-01-07 00:32:34,578 iteration 6163 : loss : 0.018482, loss_ce: 0.006588
2022-01-07 00:32:36,069 iteration 6164 : loss : 0.013531, loss_ce: 0.004068
2022-01-07 00:32:37,607 iteration 6165 : loss : 0.013470, loss_ce: 0.004820
2022-01-07 00:32:39,165 iteration 6166 : loss : 0.019950, loss_ce: 0.009742
2022-01-07 00:32:40,615 iteration 6167 : loss : 0.013232, loss_ce: 0.004350
2022-01-07 00:32:42,237 iteration 6168 : loss : 0.023001, loss_ce: 0.008146
2022-01-07 00:32:43,751 iteration 6169 : loss : 0.015642, loss_ce: 0.006377
2022-01-07 00:32:45,305 iteration 6170 : loss : 0.013786, loss_ce: 0.005765
2022-01-07 00:32:46,824 iteration 6171 : loss : 0.018082, loss_ce: 0.004946
 91%|██████████████████████████▎  | 363/400 [2:51:33<16:57, 27.50s/it]2022-01-07 00:32:48,396 iteration 6172 : loss : 0.019486, loss_ce: 0.008443
2022-01-07 00:32:49,832 iteration 6173 : loss : 0.011710, loss_ce: 0.004201
2022-01-07 00:32:51,334 iteration 6174 : loss : 0.015783, loss_ce: 0.007931
2022-01-07 00:32:52,789 iteration 6175 : loss : 0.010565, loss_ce: 0.002901
2022-01-07 00:32:54,329 iteration 6176 : loss : 0.016416, loss_ce: 0.008450
2022-01-07 00:32:55,934 iteration 6177 : loss : 0.016437, loss_ce: 0.007864
2022-01-07 00:32:57,486 iteration 6178 : loss : 0.023732, loss_ce: 0.008265
2022-01-07 00:32:59,017 iteration 6179 : loss : 0.020047, loss_ce: 0.006496
2022-01-07 00:33:00,549 iteration 6180 : loss : 0.029330, loss_ce: 0.004709
2022-01-07 00:33:02,041 iteration 6181 : loss : 0.019263, loss_ce: 0.007669
2022-01-07 00:33:03,576 iteration 6182 : loss : 0.012127, loss_ce: 0.004654
2022-01-07 00:33:05,043 iteration 6183 : loss : 0.019375, loss_ce: 0.006852
2022-01-07 00:33:06,507 iteration 6184 : loss : 0.015764, loss_ce: 0.006155
2022-01-07 00:33:08,059 iteration 6185 : loss : 0.022944, loss_ce: 0.009626
2022-01-07 00:33:09,515 iteration 6186 : loss : 0.012596, loss_ce: 0.004948
2022-01-07 00:33:10,983 iteration 6187 : loss : 0.016295, loss_ce: 0.005173
2022-01-07 00:33:12,461 iteration 6188 : loss : 0.021262, loss_ce: 0.007323
 91%|██████████████████████████▍  | 364/400 [2:51:59<16:09, 26.94s/it]2022-01-07 00:33:14,101 iteration 6189 : loss : 0.023374, loss_ce: 0.009702
2022-01-07 00:33:15,624 iteration 6190 : loss : 0.018635, loss_ce: 0.010387
2022-01-07 00:33:17,197 iteration 6191 : loss : 0.016347, loss_ce: 0.005331
2022-01-07 00:33:18,747 iteration 6192 : loss : 0.016924, loss_ce: 0.006508
2022-01-07 00:33:20,231 iteration 6193 : loss : 0.011676, loss_ce: 0.003647
2022-01-07 00:33:21,782 iteration 6194 : loss : 0.013322, loss_ce: 0.005452
2022-01-07 00:33:23,358 iteration 6195 : loss : 0.019595, loss_ce: 0.006471
2022-01-07 00:33:24,837 iteration 6196 : loss : 0.021697, loss_ce: 0.006278
2022-01-07 00:33:26,446 iteration 6197 : loss : 0.018877, loss_ce: 0.006622
2022-01-07 00:33:28,038 iteration 6198 : loss : 0.018925, loss_ce: 0.007811
2022-01-07 00:33:29,497 iteration 6199 : loss : 0.012021, loss_ce: 0.004829
2022-01-07 00:33:31,026 iteration 6200 : loss : 0.024253, loss_ce: 0.010175
2022-01-07 00:33:32,541 iteration 6201 : loss : 0.024709, loss_ce: 0.010050
2022-01-07 00:33:34,025 iteration 6202 : loss : 0.012105, loss_ce: 0.003930
2022-01-07 00:33:35,502 iteration 6203 : loss : 0.034967, loss_ce: 0.009541
2022-01-07 00:33:37,037 iteration 6204 : loss : 0.026736, loss_ce: 0.013062
2022-01-07 00:33:37,037 Training Data Eval:
2022-01-07 00:33:44,814   Average segmentation loss on training set: 0.0093
2022-01-07 00:33:44,815 Validation Data Eval:
2022-01-07 00:33:47,510   Average segmentation loss on validation set: 0.0721
2022-01-07 00:33:49,125 iteration 6205 : loss : 0.027193, loss_ce: 0.013550
 91%|██████████████████████████▍  | 365/400 [2:52:35<17:24, 29.86s/it]2022-01-07 00:33:50,701 iteration 6206 : loss : 0.015477, loss_ce: 0.007549
2022-01-07 00:33:52,203 iteration 6207 : loss : 0.020550, loss_ce: 0.011436
2022-01-07 00:33:53,728 iteration 6208 : loss : 0.022326, loss_ce: 0.007954
2022-01-07 00:33:55,251 iteration 6209 : loss : 0.038027, loss_ce: 0.013320
2022-01-07 00:33:56,737 iteration 6210 : loss : 0.017876, loss_ce: 0.006641
2022-01-07 00:33:58,185 iteration 6211 : loss : 0.013230, loss_ce: 0.004759
2022-01-07 00:33:59,664 iteration 6212 : loss : 0.017989, loss_ce: 0.006502
2022-01-07 00:34:01,170 iteration 6213 : loss : 0.012590, loss_ce: 0.004799
2022-01-07 00:34:02,656 iteration 6214 : loss : 0.013704, loss_ce: 0.005811
2022-01-07 00:34:04,217 iteration 6215 : loss : 0.022620, loss_ce: 0.007845
2022-01-07 00:34:05,709 iteration 6216 : loss : 0.014130, loss_ce: 0.006000
2022-01-07 00:34:07,144 iteration 6217 : loss : 0.018048, loss_ce: 0.005764
2022-01-07 00:34:08,644 iteration 6218 : loss : 0.024762, loss_ce: 0.011232
2022-01-07 00:34:10,146 iteration 6219 : loss : 0.016023, loss_ce: 0.005698
2022-01-07 00:34:11,622 iteration 6220 : loss : 0.013824, loss_ce: 0.003548
2022-01-07 00:34:13,145 iteration 6221 : loss : 0.014850, loss_ce: 0.004387
2022-01-07 00:34:14,603 iteration 6222 : loss : 0.013219, loss_ce: 0.005991
 92%|██████████████████████████▌  | 366/400 [2:53:01<16:10, 28.54s/it]2022-01-07 00:34:16,190 iteration 6223 : loss : 0.017374, loss_ce: 0.007178
2022-01-07 00:34:17,680 iteration 6224 : loss : 0.016061, loss_ce: 0.006553
2022-01-07 00:34:19,189 iteration 6225 : loss : 0.021411, loss_ce: 0.006743
2022-01-07 00:34:20,673 iteration 6226 : loss : 0.017863, loss_ce: 0.005691
2022-01-07 00:34:22,130 iteration 6227 : loss : 0.012774, loss_ce: 0.005287
2022-01-07 00:34:23,667 iteration 6228 : loss : 0.016270, loss_ce: 0.006525
2022-01-07 00:34:25,218 iteration 6229 : loss : 0.012213, loss_ce: 0.006068
2022-01-07 00:34:26,781 iteration 6230 : loss : 0.017468, loss_ce: 0.005525
2022-01-07 00:34:28,349 iteration 6231 : loss : 0.017153, loss_ce: 0.006623
2022-01-07 00:34:29,890 iteration 6232 : loss : 0.015736, loss_ce: 0.006357
2022-01-07 00:34:31,408 iteration 6233 : loss : 0.014288, loss_ce: 0.006243
2022-01-07 00:34:32,909 iteration 6234 : loss : 0.020343, loss_ce: 0.004193
2022-01-07 00:34:34,396 iteration 6235 : loss : 0.016042, loss_ce: 0.008776
2022-01-07 00:34:35,913 iteration 6236 : loss : 0.016467, loss_ce: 0.008998
2022-01-07 00:34:37,456 iteration 6237 : loss : 0.019471, loss_ce: 0.009383
2022-01-07 00:34:39,057 iteration 6238 : loss : 0.021289, loss_ce: 0.006008
2022-01-07 00:34:40,600 iteration 6239 : loss : 0.012823, loss_ce: 0.003708
 92%|██████████████████████████▌  | 367/400 [2:53:27<15:16, 27.78s/it]2022-01-07 00:34:42,190 iteration 6240 : loss : 0.012079, loss_ce: 0.004121
2022-01-07 00:34:43,703 iteration 6241 : loss : 0.012443, loss_ce: 0.005930
2022-01-07 00:34:45,323 iteration 6242 : loss : 0.019016, loss_ce: 0.005750
2022-01-07 00:34:46,854 iteration 6243 : loss : 0.013394, loss_ce: 0.005303
2022-01-07 00:34:48,470 iteration 6244 : loss : 0.015063, loss_ce: 0.005365
2022-01-07 00:34:49,993 iteration 6245 : loss : 0.017130, loss_ce: 0.006127
2022-01-07 00:34:51,505 iteration 6246 : loss : 0.014732, loss_ce: 0.005909
2022-01-07 00:34:53,070 iteration 6247 : loss : 0.020496, loss_ce: 0.005428
2022-01-07 00:34:54,576 iteration 6248 : loss : 0.015515, loss_ce: 0.007264
2022-01-07 00:34:56,086 iteration 6249 : loss : 0.019275, loss_ce: 0.005742
2022-01-07 00:34:57,631 iteration 6250 : loss : 0.014475, loss_ce: 0.005725
2022-01-07 00:34:59,236 iteration 6251 : loss : 0.022451, loss_ce: 0.009985
2022-01-07 00:35:00,714 iteration 6252 : loss : 0.015412, loss_ce: 0.007690
2022-01-07 00:35:02,278 iteration 6253 : loss : 0.016621, loss_ce: 0.005415
2022-01-07 00:35:03,779 iteration 6254 : loss : 0.015212, loss_ce: 0.007272
2022-01-07 00:35:05,326 iteration 6255 : loss : 0.014978, loss_ce: 0.006298
2022-01-07 00:35:06,939 iteration 6256 : loss : 0.021883, loss_ce: 0.010303
 92%|██████████████████████████▋  | 368/400 [2:53:53<14:35, 27.35s/it]2022-01-07 00:35:08,570 iteration 6257 : loss : 0.018053, loss_ce: 0.006047
2022-01-07 00:35:10,147 iteration 6258 : loss : 0.020758, loss_ce: 0.007199
2022-01-07 00:35:11,733 iteration 6259 : loss : 0.016720, loss_ce: 0.006886
2022-01-07 00:35:13,241 iteration 6260 : loss : 0.017888, loss_ce: 0.007306
2022-01-07 00:35:14,753 iteration 6261 : loss : 0.016288, loss_ce: 0.005743
2022-01-07 00:35:16,304 iteration 6262 : loss : 0.020263, loss_ce: 0.007038
2022-01-07 00:35:17,839 iteration 6263 : loss : 0.026960, loss_ce: 0.005977
2022-01-07 00:35:19,383 iteration 6264 : loss : 0.014149, loss_ce: 0.005406
2022-01-07 00:35:20,914 iteration 6265 : loss : 0.017057, loss_ce: 0.006379
2022-01-07 00:35:22,415 iteration 6266 : loss : 0.019711, loss_ce: 0.009829
2022-01-07 00:35:23,907 iteration 6267 : loss : 0.014498, loss_ce: 0.006566
2022-01-07 00:35:25,429 iteration 6268 : loss : 0.015054, loss_ce: 0.006397
2022-01-07 00:35:26,839 iteration 6269 : loss : 0.011405, loss_ce: 0.003887
2022-01-07 00:35:28,341 iteration 6270 : loss : 0.013230, loss_ce: 0.005944
2022-01-07 00:35:29,840 iteration 6271 : loss : 0.018780, loss_ce: 0.006281
2022-01-07 00:35:31,394 iteration 6272 : loss : 0.025947, loss_ce: 0.009573
2022-01-07 00:35:32,937 iteration 6273 : loss : 0.018914, loss_ce: 0.007654
 92%|██████████████████████████▊  | 369/400 [2:54:19<13:55, 26.94s/it]2022-01-07 00:35:34,638 iteration 6274 : loss : 0.028661, loss_ce: 0.011478
2022-01-07 00:35:36,181 iteration 6275 : loss : 0.018024, loss_ce: 0.008706
2022-01-07 00:35:37,710 iteration 6276 : loss : 0.030650, loss_ce: 0.011244
2022-01-07 00:35:39,187 iteration 6277 : loss : 0.015097, loss_ce: 0.004106
2022-01-07 00:35:40,708 iteration 6278 : loss : 0.016693, loss_ce: 0.005573
2022-01-07 00:35:42,219 iteration 6279 : loss : 0.017229, loss_ce: 0.007740
2022-01-07 00:35:43,704 iteration 6280 : loss : 0.015738, loss_ce: 0.006103
2022-01-07 00:35:45,319 iteration 6281 : loss : 0.023219, loss_ce: 0.005810
2022-01-07 00:35:46,815 iteration 6282 : loss : 0.015590, loss_ce: 0.005992
2022-01-07 00:35:48,299 iteration 6283 : loss : 0.012816, loss_ce: 0.006065
2022-01-07 00:35:49,799 iteration 6284 : loss : 0.017955, loss_ce: 0.007472
2022-01-07 00:35:51,344 iteration 6285 : loss : 0.017558, loss_ce: 0.006018
2022-01-07 00:35:52,830 iteration 6286 : loss : 0.017650, loss_ce: 0.005101
2022-01-07 00:35:54,310 iteration 6287 : loss : 0.026375, loss_ce: 0.007994
2022-01-07 00:35:55,741 iteration 6288 : loss : 0.010734, loss_ce: 0.003710
2022-01-07 00:35:57,265 iteration 6289 : loss : 0.014507, loss_ce: 0.006275
2022-01-07 00:35:57,265 Training Data Eval:
2022-01-07 00:36:05,094   Average segmentation loss on training set: 0.0093
2022-01-07 00:36:05,095 Validation Data Eval:
2022-01-07 00:36:07,784   Average segmentation loss on validation set: 0.0600
2022-01-07 00:36:13,545 Found new lowest validation loss at iteration 6289! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_DROPOUT_best_val_loss_seed1234.pth
2022-01-07 00:36:15,064 iteration 6290 : loss : 0.020352, loss_ce: 0.009855
 92%|██████████████████████████▊  | 370/400 [2:55:01<15:44, 31.50s/it]2022-01-07 00:36:16,555 iteration 6291 : loss : 0.010962, loss_ce: 0.004311
2022-01-07 00:36:17,994 iteration 6292 : loss : 0.017211, loss_ce: 0.006220
2022-01-07 00:36:19,458 iteration 6293 : loss : 0.019723, loss_ce: 0.008847
2022-01-07 00:36:20,934 iteration 6294 : loss : 0.017344, loss_ce: 0.005016
2022-01-07 00:36:22,364 iteration 6295 : loss : 0.012906, loss_ce: 0.005398
2022-01-07 00:36:23,874 iteration 6296 : loss : 0.027659, loss_ce: 0.009899
2022-01-07 00:36:25,323 iteration 6297 : loss : 0.021845, loss_ce: 0.008010
2022-01-07 00:36:26,803 iteration 6298 : loss : 0.025467, loss_ce: 0.008664
2022-01-07 00:36:28,237 iteration 6299 : loss : 0.015237, loss_ce: 0.004973
2022-01-07 00:36:29,699 iteration 6300 : loss : 0.018124, loss_ce: 0.006046
2022-01-07 00:36:31,171 iteration 6301 : loss : 0.014982, loss_ce: 0.010182
2022-01-07 00:36:32,695 iteration 6302 : loss : 0.017583, loss_ce: 0.006711
2022-01-07 00:36:34,128 iteration 6303 : loss : 0.012118, loss_ce: 0.002814
2022-01-07 00:36:35,588 iteration 6304 : loss : 0.012968, loss_ce: 0.003810
2022-01-07 00:36:37,129 iteration 6305 : loss : 0.019149, loss_ce: 0.008252
2022-01-07 00:36:38,628 iteration 6306 : loss : 0.019953, loss_ce: 0.004370
2022-01-07 00:36:40,132 iteration 6307 : loss : 0.011513, loss_ce: 0.005502
 93%|██████████████████████████▉  | 371/400 [2:55:26<14:17, 29.57s/it]2022-01-07 00:36:41,657 iteration 6308 : loss : 0.016286, loss_ce: 0.003544
2022-01-07 00:36:43,178 iteration 6309 : loss : 0.017461, loss_ce: 0.006591
2022-01-07 00:36:44,679 iteration 6310 : loss : 0.016766, loss_ce: 0.006611
2022-01-07 00:36:46,200 iteration 6311 : loss : 0.015778, loss_ce: 0.006478
2022-01-07 00:36:47,739 iteration 6312 : loss : 0.018316, loss_ce: 0.005693
2022-01-07 00:36:49,311 iteration 6313 : loss : 0.021042, loss_ce: 0.009639
2022-01-07 00:36:50,809 iteration 6314 : loss : 0.014619, loss_ce: 0.004604
2022-01-07 00:36:52,281 iteration 6315 : loss : 0.016463, loss_ce: 0.006005
2022-01-07 00:36:53,779 iteration 6316 : loss : 0.013189, loss_ce: 0.005220
2022-01-07 00:36:55,277 iteration 6317 : loss : 0.012931, loss_ce: 0.003356
2022-01-07 00:36:56,838 iteration 6318 : loss : 0.017871, loss_ce: 0.006860
2022-01-07 00:36:58,368 iteration 6319 : loss : 0.013950, loss_ce: 0.004847
2022-01-07 00:36:59,853 iteration 6320 : loss : 0.016009, loss_ce: 0.005214
2022-01-07 00:37:01,398 iteration 6321 : loss : 0.016342, loss_ce: 0.007909
2022-01-07 00:37:02,946 iteration 6322 : loss : 0.022711, loss_ce: 0.012458
2022-01-07 00:37:04,568 iteration 6323 : loss : 0.019861, loss_ce: 0.006395
2022-01-07 00:37:06,137 iteration 6324 : loss : 0.022000, loss_ce: 0.010833
 93%|██████████████████████████▉  | 372/400 [2:55:52<13:18, 28.50s/it]2022-01-07 00:37:07,721 iteration 6325 : loss : 0.023657, loss_ce: 0.008245
2022-01-07 00:37:09,191 iteration 6326 : loss : 0.017438, loss_ce: 0.007537
2022-01-07 00:37:10,681 iteration 6327 : loss : 0.021322, loss_ce: 0.006377
2022-01-07 00:37:12,235 iteration 6328 : loss : 0.013249, loss_ce: 0.006553
2022-01-07 00:37:13,679 iteration 6329 : loss : 0.014400, loss_ce: 0.007709
2022-01-07 00:37:15,169 iteration 6330 : loss : 0.015986, loss_ce: 0.006113
2022-01-07 00:37:16,720 iteration 6331 : loss : 0.017094, loss_ce: 0.006663
2022-01-07 00:37:18,245 iteration 6332 : loss : 0.013231, loss_ce: 0.003732
2022-01-07 00:37:19,738 iteration 6333 : loss : 0.011299, loss_ce: 0.005227
2022-01-07 00:37:21,236 iteration 6334 : loss : 0.018848, loss_ce: 0.004945
2022-01-07 00:37:22,811 iteration 6335 : loss : 0.014513, loss_ce: 0.005743
2022-01-07 00:37:24,285 iteration 6336 : loss : 0.016971, loss_ce: 0.006868
2022-01-07 00:37:25,796 iteration 6337 : loss : 0.017474, loss_ce: 0.004988
2022-01-07 00:37:27,292 iteration 6338 : loss : 0.015612, loss_ce: 0.004965
2022-01-07 00:37:28,789 iteration 6339 : loss : 0.016827, loss_ce: 0.006831
2022-01-07 00:37:30,285 iteration 6340 : loss : 0.014368, loss_ce: 0.006201
2022-01-07 00:37:31,724 iteration 6341 : loss : 0.011821, loss_ce: 0.004208
 93%|███████████████████████████  | 373/400 [2:56:18<12:25, 27.63s/it]2022-01-07 00:37:33,321 iteration 6342 : loss : 0.016849, loss_ce: 0.004816
2022-01-07 00:37:34,799 iteration 6343 : loss : 0.015815, loss_ce: 0.005811
2022-01-07 00:37:36,278 iteration 6344 : loss : 0.022989, loss_ce: 0.007477
2022-01-07 00:37:37,755 iteration 6345 : loss : 0.024063, loss_ce: 0.004188
2022-01-07 00:37:39,257 iteration 6346 : loss : 0.019564, loss_ce: 0.005997
2022-01-07 00:37:40,760 iteration 6347 : loss : 0.015962, loss_ce: 0.005846
2022-01-07 00:37:42,160 iteration 6348 : loss : 0.010159, loss_ce: 0.004393
2022-01-07 00:37:43,653 iteration 6349 : loss : 0.020158, loss_ce: 0.009703
2022-01-07 00:37:45,253 iteration 6350 : loss : 0.024176, loss_ce: 0.009428
2022-01-07 00:37:46,800 iteration 6351 : loss : 0.013504, loss_ce: 0.006147
2022-01-07 00:37:48,300 iteration 6352 : loss : 0.017434, loss_ce: 0.005455
2022-01-07 00:37:49,812 iteration 6353 : loss : 0.017814, loss_ce: 0.006881
2022-01-07 00:37:51,309 iteration 6354 : loss : 0.015847, loss_ce: 0.007546
2022-01-07 00:37:52,779 iteration 6355 : loss : 0.020707, loss_ce: 0.007438
2022-01-07 00:37:54,328 iteration 6356 : loss : 0.018601, loss_ce: 0.004525
2022-01-07 00:37:55,799 iteration 6357 : loss : 0.017180, loss_ce: 0.005769
2022-01-07 00:37:57,397 iteration 6358 : loss : 0.023491, loss_ce: 0.010216
 94%|███████████████████████████  | 374/400 [2:56:44<11:43, 27.04s/it]2022-01-07 00:37:59,010 iteration 6359 : loss : 0.020563, loss_ce: 0.005870
2022-01-07 00:38:00,456 iteration 6360 : loss : 0.015541, loss_ce: 0.006683
2022-01-07 00:38:01,988 iteration 6361 : loss : 0.014460, loss_ce: 0.004962
2022-01-07 00:38:03,522 iteration 6362 : loss : 0.017837, loss_ce: 0.007558
2022-01-07 00:38:05,077 iteration 6363 : loss : 0.025773, loss_ce: 0.010420
2022-01-07 00:38:06,537 iteration 6364 : loss : 0.019837, loss_ce: 0.006935
2022-01-07 00:38:08,042 iteration 6365 : loss : 0.014208, loss_ce: 0.007740
2022-01-07 00:38:09,624 iteration 6366 : loss : 0.024732, loss_ce: 0.009945
2022-01-07 00:38:11,231 iteration 6367 : loss : 0.026252, loss_ce: 0.010586
2022-01-07 00:38:12,773 iteration 6368 : loss : 0.020747, loss_ce: 0.007768
2022-01-07 00:38:14,278 iteration 6369 : loss : 0.013157, loss_ce: 0.005711
2022-01-07 00:38:15,784 iteration 6370 : loss : 0.020130, loss_ce: 0.007023
2022-01-07 00:38:17,404 iteration 6371 : loss : 0.022353, loss_ce: 0.006970
2022-01-07 00:38:18,870 iteration 6372 : loss : 0.009989, loss_ce: 0.004196
2022-01-07 00:38:20,337 iteration 6373 : loss : 0.013139, loss_ce: 0.004819
2022-01-07 00:38:21,847 iteration 6374 : loss : 0.019486, loss_ce: 0.008400
2022-01-07 00:38:21,847 Training Data Eval:
2022-01-07 00:38:29,591   Average segmentation loss on training set: 0.0085
2022-01-07 00:38:29,591 Validation Data Eval:
2022-01-07 00:38:32,276   Average segmentation loss on validation set: 0.0628
2022-01-07 00:38:33,815 iteration 6375 : loss : 0.017741, loss_ce: 0.006078
 94%|███████████████████████████▏ | 375/400 [2:57:20<12:26, 29.85s/it]2022-01-07 00:38:35,428 iteration 6376 : loss : 0.019220, loss_ce: 0.005747
2022-01-07 00:38:36,963 iteration 6377 : loss : 0.023099, loss_ce: 0.007958
2022-01-07 00:38:38,404 iteration 6378 : loss : 0.013030, loss_ce: 0.005528
2022-01-07 00:38:39,928 iteration 6379 : loss : 0.017224, loss_ce: 0.005057
2022-01-07 00:38:41,421 iteration 6380 : loss : 0.016422, loss_ce: 0.006395
2022-01-07 00:38:42,907 iteration 6381 : loss : 0.012117, loss_ce: 0.003859
2022-01-07 00:38:44,341 iteration 6382 : loss : 0.013803, loss_ce: 0.004857
2022-01-07 00:38:45,894 iteration 6383 : loss : 0.019702, loss_ce: 0.007318
2022-01-07 00:38:47,377 iteration 6384 : loss : 0.015885, loss_ce: 0.006395
2022-01-07 00:38:48,917 iteration 6385 : loss : 0.019582, loss_ce: 0.006564
2022-01-07 00:38:50,486 iteration 6386 : loss : 0.016079, loss_ce: 0.004986
2022-01-07 00:38:52,086 iteration 6387 : loss : 0.020429, loss_ce: 0.009470
2022-01-07 00:38:53,631 iteration 6388 : loss : 0.015910, loss_ce: 0.006846
2022-01-07 00:38:55,039 iteration 6389 : loss : 0.013090, loss_ce: 0.004543
2022-01-07 00:38:56,550 iteration 6390 : loss : 0.016608, loss_ce: 0.007813
2022-01-07 00:38:58,016 iteration 6391 : loss : 0.013069, loss_ce: 0.006845
2022-01-07 00:38:59,578 iteration 6392 : loss : 0.021750, loss_ce: 0.007421
 94%|███████████████████████████▎ | 376/400 [2:57:46<11:27, 28.63s/it]2022-01-07 00:39:01,121 iteration 6393 : loss : 0.012349, loss_ce: 0.004587
2022-01-07 00:39:02,671 iteration 6394 : loss : 0.021977, loss_ce: 0.008283
2022-01-07 00:39:04,158 iteration 6395 : loss : 0.014489, loss_ce: 0.003958
2022-01-07 00:39:05,652 iteration 6396 : loss : 0.016148, loss_ce: 0.006326
2022-01-07 00:39:07,168 iteration 6397 : loss : 0.014086, loss_ce: 0.005446
2022-01-07 00:39:08,697 iteration 6398 : loss : 0.015970, loss_ce: 0.007271
2022-01-07 00:39:10,202 iteration 6399 : loss : 0.017112, loss_ce: 0.005547
2022-01-07 00:39:11,696 iteration 6400 : loss : 0.015996, loss_ce: 0.006510
2022-01-07 00:39:13,222 iteration 6401 : loss : 0.016879, loss_ce: 0.006871
2022-01-07 00:39:14,724 iteration 6402 : loss : 0.015467, loss_ce: 0.008196
2022-01-07 00:39:16,321 iteration 6403 : loss : 0.022766, loss_ce: 0.009907
2022-01-07 00:39:17,833 iteration 6404 : loss : 0.012605, loss_ce: 0.005229
2022-01-07 00:39:19,328 iteration 6405 : loss : 0.023572, loss_ce: 0.007152
2022-01-07 00:39:21,005 iteration 6406 : loss : 0.027810, loss_ce: 0.006703
2022-01-07 00:39:22,565 iteration 6407 : loss : 0.019297, loss_ce: 0.005487
2022-01-07 00:39:24,015 iteration 6408 : loss : 0.011714, loss_ce: 0.004072
2022-01-07 00:39:25,550 iteration 6409 : loss : 0.021757, loss_ce: 0.009356
 94%|███████████████████████████▎ | 377/400 [2:58:12<10:40, 27.83s/it]2022-01-07 00:39:27,121 iteration 6410 : loss : 0.012884, loss_ce: 0.004591
2022-01-07 00:39:28,678 iteration 6411 : loss : 0.018968, loss_ce: 0.007675
2022-01-07 00:39:30,230 iteration 6412 : loss : 0.016235, loss_ce: 0.006228
2022-01-07 00:39:31,739 iteration 6413 : loss : 0.015661, loss_ce: 0.006783
2022-01-07 00:39:33,240 iteration 6414 : loss : 0.013796, loss_ce: 0.005851
2022-01-07 00:39:34,723 iteration 6415 : loss : 0.013380, loss_ce: 0.004574
2022-01-07 00:39:36,247 iteration 6416 : loss : 0.013373, loss_ce: 0.005655
2022-01-07 00:39:37,714 iteration 6417 : loss : 0.014034, loss_ce: 0.005506
2022-01-07 00:39:39,212 iteration 6418 : loss : 0.018817, loss_ce: 0.007207
2022-01-07 00:39:40,818 iteration 6419 : loss : 0.018675, loss_ce: 0.007371
2022-01-07 00:39:42,401 iteration 6420 : loss : 0.024019, loss_ce: 0.006542
2022-01-07 00:39:43,954 iteration 6421 : loss : 0.015150, loss_ce: 0.005876
2022-01-07 00:39:45,486 iteration 6422 : loss : 0.015899, loss_ce: 0.004718
2022-01-07 00:39:47,043 iteration 6423 : loss : 0.015604, loss_ce: 0.006237
2022-01-07 00:39:48,663 iteration 6424 : loss : 0.023642, loss_ce: 0.012137
2022-01-07 00:39:50,187 iteration 6425 : loss : 0.015035, loss_ce: 0.004232
2022-01-07 00:39:51,805 iteration 6426 : loss : 0.026313, loss_ce: 0.009680
 94%|███████████████████████████▍ | 378/400 [2:58:38<10:01, 27.35s/it]2022-01-07 00:39:53,451 iteration 6427 : loss : 0.012459, loss_ce: 0.004085
2022-01-07 00:39:55,011 iteration 6428 : loss : 0.017385, loss_ce: 0.005247
2022-01-07 00:39:56,577 iteration 6429 : loss : 0.016287, loss_ce: 0.004905
2022-01-07 00:39:58,178 iteration 6430 : loss : 0.023640, loss_ce: 0.012249
2022-01-07 00:39:59,669 iteration 6431 : loss : 0.012486, loss_ce: 0.004701
2022-01-07 00:40:01,227 iteration 6432 : loss : 0.014421, loss_ce: 0.006070
2022-01-07 00:40:02,768 iteration 6433 : loss : 0.013731, loss_ce: 0.004629
2022-01-07 00:40:04,279 iteration 6434 : loss : 0.018821, loss_ce: 0.008669
2022-01-07 00:40:05,827 iteration 6435 : loss : 0.020727, loss_ce: 0.007921
2022-01-07 00:40:07,357 iteration 6436 : loss : 0.011864, loss_ce: 0.004893
2022-01-07 00:40:08,906 iteration 6437 : loss : 0.015403, loss_ce: 0.005043
2022-01-07 00:40:10,479 iteration 6438 : loss : 0.018943, loss_ce: 0.006443
2022-01-07 00:40:11,976 iteration 6439 : loss : 0.013175, loss_ce: 0.006240
2022-01-07 00:40:13,550 iteration 6440 : loss : 0.020814, loss_ce: 0.009829
2022-01-07 00:40:15,034 iteration 6441 : loss : 0.010203, loss_ce: 0.004250
2022-01-07 00:40:16,590 iteration 6442 : loss : 0.019310, loss_ce: 0.006084
2022-01-07 00:40:18,076 iteration 6443 : loss : 0.012314, loss_ce: 0.004385
 95%|███████████████████████████▍ | 379/400 [2:59:04<09:27, 27.03s/it]2022-01-07 00:40:19,706 iteration 6444 : loss : 0.019051, loss_ce: 0.011800
2022-01-07 00:40:21,269 iteration 6445 : loss : 0.015670, loss_ce: 0.006728
2022-01-07 00:40:22,851 iteration 6446 : loss : 0.016336, loss_ce: 0.006992
2022-01-07 00:40:24,446 iteration 6447 : loss : 0.032661, loss_ce: 0.007556
2022-01-07 00:40:26,049 iteration 6448 : loss : 0.013736, loss_ce: 0.005250
2022-01-07 00:40:27,597 iteration 6449 : loss : 0.011413, loss_ce: 0.002869
2022-01-07 00:40:29,125 iteration 6450 : loss : 0.015703, loss_ce: 0.006885
2022-01-07 00:40:30,701 iteration 6451 : loss : 0.018865, loss_ce: 0.006358
2022-01-07 00:40:32,342 iteration 6452 : loss : 0.019652, loss_ce: 0.007800
2022-01-07 00:40:33,895 iteration 6453 : loss : 0.016837, loss_ce: 0.006055
2022-01-07 00:40:35,406 iteration 6454 : loss : 0.016551, loss_ce: 0.005607
2022-01-07 00:40:37,015 iteration 6455 : loss : 0.015328, loss_ce: 0.007019
2022-01-07 00:40:38,580 iteration 6456 : loss : 0.017005, loss_ce: 0.006273
2022-01-07 00:40:40,172 iteration 6457 : loss : 0.017762, loss_ce: 0.006185
2022-01-07 00:40:41,694 iteration 6458 : loss : 0.014472, loss_ce: 0.005503
2022-01-07 00:40:43,244 iteration 6459 : loss : 0.010244, loss_ce: 0.002494
2022-01-07 00:40:43,244 Training Data Eval:
2022-01-07 00:40:51,307   Average segmentation loss on training set: 0.0088
2022-01-07 00:40:51,307 Validation Data Eval:
2022-01-07 00:40:54,070   Average segmentation loss on validation set: 0.0728
2022-01-07 00:40:55,678 iteration 6460 : loss : 0.017552, loss_ce: 0.008091
 95%|███████████████████████████▌ | 380/400 [2:59:42<10:04, 30.20s/it]2022-01-07 00:40:57,364 iteration 6461 : loss : 0.016977, loss_ce: 0.007002
2022-01-07 00:40:58,908 iteration 6462 : loss : 0.011406, loss_ce: 0.004717
2022-01-07 00:41:00,532 iteration 6463 : loss : 0.022459, loss_ce: 0.010725
2022-01-07 00:41:02,106 iteration 6464 : loss : 0.020853, loss_ce: 0.006364
2022-01-07 00:41:03,719 iteration 6465 : loss : 0.016805, loss_ce: 0.006042
2022-01-07 00:41:05,246 iteration 6466 : loss : 0.013592, loss_ce: 0.004463
2022-01-07 00:41:06,795 iteration 6467 : loss : 0.014337, loss_ce: 0.005505
2022-01-07 00:41:08,425 iteration 6468 : loss : 0.019170, loss_ce: 0.006733
2022-01-07 00:41:09,984 iteration 6469 : loss : 0.019660, loss_ce: 0.007638
2022-01-07 00:41:11,524 iteration 6470 : loss : 0.012471, loss_ce: 0.003582
2022-01-07 00:41:13,095 iteration 6471 : loss : 0.017351, loss_ce: 0.006837
2022-01-07 00:41:14,676 iteration 6472 : loss : 0.017981, loss_ce: 0.005880
2022-01-07 00:41:16,215 iteration 6473 : loss : 0.014865, loss_ce: 0.005303
2022-01-07 00:41:17,843 iteration 6474 : loss : 0.013217, loss_ce: 0.006518
2022-01-07 00:41:19,435 iteration 6475 : loss : 0.020584, loss_ce: 0.011861
2022-01-07 00:41:20,900 iteration 6476 : loss : 0.013466, loss_ce: 0.005175
2022-01-07 00:41:22,550 iteration 6477 : loss : 0.028761, loss_ce: 0.010137
 95%|███████████████████████████▌ | 381/400 [3:00:09<09:14, 29.20s/it]2022-01-07 00:41:24,235 iteration 6478 : loss : 0.023610, loss_ce: 0.007722
2022-01-07 00:41:25,789 iteration 6479 : loss : 0.016350, loss_ce: 0.005899
2022-01-07 00:41:27,347 iteration 6480 : loss : 0.017234, loss_ce: 0.007399
2022-01-07 00:41:28,947 iteration 6481 : loss : 0.014506, loss_ce: 0.007358
2022-01-07 00:41:30,449 iteration 6482 : loss : 0.011939, loss_ce: 0.004132
2022-01-07 00:41:32,049 iteration 6483 : loss : 0.016044, loss_ce: 0.003684
2022-01-07 00:41:33,578 iteration 6484 : loss : 0.013442, loss_ce: 0.005282
2022-01-07 00:41:35,140 iteration 6485 : loss : 0.016292, loss_ce: 0.007119
2022-01-07 00:41:36,756 iteration 6486 : loss : 0.015643, loss_ce: 0.005539
2022-01-07 00:41:38,430 iteration 6487 : loss : 0.024408, loss_ce: 0.010908
2022-01-07 00:41:40,037 iteration 6488 : loss : 0.021266, loss_ce: 0.008235
2022-01-07 00:41:41,575 iteration 6489 : loss : 0.012693, loss_ce: 0.006542
2022-01-07 00:41:43,164 iteration 6490 : loss : 0.019941, loss_ce: 0.005354
2022-01-07 00:41:44,712 iteration 6491 : loss : 0.019657, loss_ce: 0.007817
2022-01-07 00:41:46,360 iteration 6492 : loss : 0.021786, loss_ce: 0.009511
2022-01-07 00:41:47,956 iteration 6493 : loss : 0.016827, loss_ce: 0.005979
2022-01-07 00:41:49,431 iteration 6494 : loss : 0.008960, loss_ce: 0.003694
 96%|███████████████████████████▋ | 382/400 [3:00:36<08:33, 28.51s/it]2022-01-07 00:41:51,030 iteration 6495 : loss : 0.012952, loss_ce: 0.004897
2022-01-07 00:41:52,600 iteration 6496 : loss : 0.018405, loss_ce: 0.004563
2022-01-07 00:41:54,204 iteration 6497 : loss : 0.011808, loss_ce: 0.004235
2022-01-07 00:41:55,808 iteration 6498 : loss : 0.021687, loss_ce: 0.007386
2022-01-07 00:41:57,410 iteration 6499 : loss : 0.022048, loss_ce: 0.009635
2022-01-07 00:41:58,968 iteration 6500 : loss : 0.014379, loss_ce: 0.005850
2022-01-07 00:42:00,562 iteration 6501 : loss : 0.016067, loss_ce: 0.005954
2022-01-07 00:42:02,184 iteration 6502 : loss : 0.014159, loss_ce: 0.007749
2022-01-07 00:42:03,725 iteration 6503 : loss : 0.010818, loss_ce: 0.002854
2022-01-07 00:42:05,240 iteration 6504 : loss : 0.013284, loss_ce: 0.004974
2022-01-07 00:42:06,688 iteration 6505 : loss : 0.011445, loss_ce: 0.005442
2022-01-07 00:42:08,232 iteration 6506 : loss : 0.022355, loss_ce: 0.006346
2022-01-07 00:42:09,861 iteration 6507 : loss : 0.032882, loss_ce: 0.016922
2022-01-07 00:42:11,460 iteration 6508 : loss : 0.013795, loss_ce: 0.005068
2022-01-07 00:42:13,046 iteration 6509 : loss : 0.019143, loss_ce: 0.007324
2022-01-07 00:42:14,632 iteration 6510 : loss : 0.016391, loss_ce: 0.006992
2022-01-07 00:42:16,128 iteration 6511 : loss : 0.013553, loss_ce: 0.005249
 96%|███████████████████████████▊ | 383/400 [3:01:02<07:55, 27.97s/it]2022-01-07 00:42:17,724 iteration 6512 : loss : 0.015361, loss_ce: 0.004933
2022-01-07 00:42:19,298 iteration 6513 : loss : 0.018969, loss_ce: 0.006110
2022-01-07 00:42:20,963 iteration 6514 : loss : 0.014957, loss_ce: 0.004370
2022-01-07 00:42:22,590 iteration 6515 : loss : 0.019524, loss_ce: 0.006222
2022-01-07 00:42:24,190 iteration 6516 : loss : 0.016861, loss_ce: 0.007146
2022-01-07 00:42:25,752 iteration 6517 : loss : 0.025438, loss_ce: 0.004968
2022-01-07 00:42:27,328 iteration 6518 : loss : 0.013519, loss_ce: 0.004723
2022-01-07 00:42:28,894 iteration 6519 : loss : 0.017949, loss_ce: 0.005523
2022-01-07 00:42:30,506 iteration 6520 : loss : 0.018304, loss_ce: 0.006456
2022-01-07 00:42:32,045 iteration 6521 : loss : 0.023430, loss_ce: 0.009386
2022-01-07 00:42:33,627 iteration 6522 : loss : 0.012814, loss_ce: 0.005117
2022-01-07 00:42:35,131 iteration 6523 : loss : 0.010721, loss_ce: 0.004206
2022-01-07 00:42:36,743 iteration 6524 : loss : 0.022329, loss_ce: 0.010142
2022-01-07 00:42:38,322 iteration 6525 : loss : 0.019802, loss_ce: 0.007940
2022-01-07 00:42:39,895 iteration 6526 : loss : 0.015853, loss_ce: 0.006292
2022-01-07 00:42:41,396 iteration 6527 : loss : 0.012469, loss_ce: 0.005461
2022-01-07 00:42:42,925 iteration 6528 : loss : 0.016716, loss_ce: 0.005451
 96%|███████████████████████████▊ | 384/400 [3:01:29<07:21, 27.61s/it]2022-01-07 00:42:44,604 iteration 6529 : loss : 0.018993, loss_ce: 0.006916
2022-01-07 00:42:46,201 iteration 6530 : loss : 0.015398, loss_ce: 0.005353
2022-01-07 00:42:47,714 iteration 6531 : loss : 0.013205, loss_ce: 0.003616
2022-01-07 00:42:49,310 iteration 6532 : loss : 0.014997, loss_ce: 0.005698
2022-01-07 00:42:50,810 iteration 6533 : loss : 0.009416, loss_ce: 0.003850
2022-01-07 00:42:52,382 iteration 6534 : loss : 0.022124, loss_ce: 0.011305
2022-01-07 00:42:53,965 iteration 6535 : loss : 0.020820, loss_ce: 0.009849
2022-01-07 00:42:55,561 iteration 6536 : loss : 0.013182, loss_ce: 0.004500
2022-01-07 00:42:57,150 iteration 6537 : loss : 0.024266, loss_ce: 0.006373
2022-01-07 00:42:58,747 iteration 6538 : loss : 0.018481, loss_ce: 0.005902
2022-01-07 00:43:00,364 iteration 6539 : loss : 0.016917, loss_ce: 0.008168
2022-01-07 00:43:01,860 iteration 6540 : loss : 0.011480, loss_ce: 0.004732
2022-01-07 00:43:03,401 iteration 6541 : loss : 0.016622, loss_ce: 0.010045
2022-01-07 00:43:05,057 iteration 6542 : loss : 0.013338, loss_ce: 0.004445
2022-01-07 00:43:06,596 iteration 6543 : loss : 0.012149, loss_ce: 0.003567
2022-01-07 00:43:08,199 iteration 6544 : loss : 0.020465, loss_ce: 0.007867
2022-01-07 00:43:08,199 Training Data Eval:
2022-01-07 00:43:16,261   Average segmentation loss on training set: 0.0083
2022-01-07 00:43:16,261 Validation Data Eval:
2022-01-07 00:43:19,046   Average segmentation loss on validation set: 0.0732
2022-01-07 00:43:20,689 iteration 6545 : loss : 0.015162, loss_ce: 0.005979
 96%|███████████████████████████▉ | 385/400 [3:02:07<07:39, 30.66s/it]2022-01-07 00:43:22,453 iteration 6546 : loss : 0.017155, loss_ce: 0.006966
2022-01-07 00:43:24,023 iteration 6547 : loss : 0.017207, loss_ce: 0.007696
2022-01-07 00:43:25,622 iteration 6548 : loss : 0.013171, loss_ce: 0.005527
2022-01-07 00:43:27,162 iteration 6549 : loss : 0.014184, loss_ce: 0.005628
2022-01-07 00:43:28,770 iteration 6550 : loss : 0.016668, loss_ce: 0.005911
2022-01-07 00:43:30,396 iteration 6551 : loss : 0.023585, loss_ce: 0.010378
2022-01-07 00:43:32,066 iteration 6552 : loss : 0.028278, loss_ce: 0.010276
2022-01-07 00:43:33,761 iteration 6553 : loss : 0.030445, loss_ce: 0.009482
2022-01-07 00:43:35,363 iteration 6554 : loss : 0.017801, loss_ce: 0.007459
2022-01-07 00:43:36,945 iteration 6555 : loss : 0.015832, loss_ce: 0.006618
2022-01-07 00:43:38,519 iteration 6556 : loss : 0.018501, loss_ce: 0.004346
2022-01-07 00:43:39,995 iteration 6557 : loss : 0.009261, loss_ce: 0.003809
2022-01-07 00:43:41,557 iteration 6558 : loss : 0.017154, loss_ce: 0.005094
2022-01-07 00:43:43,050 iteration 6559 : loss : 0.011051, loss_ce: 0.004084
2022-01-07 00:43:44,620 iteration 6560 : loss : 0.014198, loss_ce: 0.006575
2022-01-07 00:43:46,227 iteration 6561 : loss : 0.017016, loss_ce: 0.004770
2022-01-07 00:43:47,757 iteration 6562 : loss : 0.013352, loss_ce: 0.004827
 96%|███████████████████████████▉ | 386/400 [3:02:34<06:54, 29.58s/it]2022-01-07 00:43:49,512 iteration 6563 : loss : 0.029654, loss_ce: 0.016385
2022-01-07 00:43:51,066 iteration 6564 : loss : 0.017633, loss_ce: 0.007313
2022-01-07 00:43:52,587 iteration 6565 : loss : 0.022175, loss_ce: 0.004736
2022-01-07 00:43:54,134 iteration 6566 : loss : 0.013053, loss_ce: 0.005058
2022-01-07 00:43:55,626 iteration 6567 : loss : 0.013103, loss_ce: 0.005579
2022-01-07 00:43:57,168 iteration 6568 : loss : 0.012969, loss_ce: 0.005612
2022-01-07 00:43:58,737 iteration 6569 : loss : 0.012780, loss_ce: 0.004974
2022-01-07 00:44:00,256 iteration 6570 : loss : 0.010313, loss_ce: 0.004561
2022-01-07 00:44:01,804 iteration 6571 : loss : 0.027169, loss_ce: 0.008912
2022-01-07 00:44:03,295 iteration 6572 : loss : 0.011306, loss_ce: 0.004847
2022-01-07 00:44:04,925 iteration 6573 : loss : 0.021187, loss_ce: 0.007557
2022-01-07 00:44:06,564 iteration 6574 : loss : 0.013675, loss_ce: 0.006385
2022-01-07 00:44:08,214 iteration 6575 : loss : 0.057681, loss_ce: 0.010415
2022-01-07 00:44:09,749 iteration 6576 : loss : 0.013369, loss_ce: 0.005984
2022-01-07 00:44:11,338 iteration 6577 : loss : 0.023055, loss_ce: 0.004381
2022-01-07 00:44:12,862 iteration 6578 : loss : 0.012803, loss_ce: 0.004953
2022-01-07 00:44:14,436 iteration 6579 : loss : 0.017794, loss_ce: 0.006922
 97%|████████████████████████████ | 387/400 [3:03:01<06:13, 28.71s/it]2022-01-07 00:44:16,077 iteration 6580 : loss : 0.023274, loss_ce: 0.007035
2022-01-07 00:44:17,625 iteration 6581 : loss : 0.019401, loss_ce: 0.006542
2022-01-07 00:44:19,260 iteration 6582 : loss : 0.016966, loss_ce: 0.006102
2022-01-07 00:44:20,838 iteration 6583 : loss : 0.015697, loss_ce: 0.006000
2022-01-07 00:44:22,428 iteration 6584 : loss : 0.015170, loss_ce: 0.006731
2022-01-07 00:44:24,055 iteration 6585 : loss : 0.028350, loss_ce: 0.009654
2022-01-07 00:44:25,657 iteration 6586 : loss : 0.025173, loss_ce: 0.008506
2022-01-07 00:44:27,219 iteration 6587 : loss : 0.020901, loss_ce: 0.009408
2022-01-07 00:44:28,803 iteration 6588 : loss : 0.022394, loss_ce: 0.008622
2022-01-07 00:44:30,316 iteration 6589 : loss : 0.014737, loss_ce: 0.005585
2022-01-07 00:44:31,948 iteration 6590 : loss : 0.014760, loss_ce: 0.007003
2022-01-07 00:44:33,577 iteration 6591 : loss : 0.022535, loss_ce: 0.008700
2022-01-07 00:44:35,121 iteration 6592 : loss : 0.011065, loss_ce: 0.004264
2022-01-07 00:44:36,764 iteration 6593 : loss : 0.022843, loss_ce: 0.004972
2022-01-07 00:44:38,332 iteration 6594 : loss : 0.014700, loss_ce: 0.007573
2022-01-07 00:44:39,884 iteration 6595 : loss : 0.016081, loss_ce: 0.005080
2022-01-07 00:44:41,511 iteration 6596 : loss : 0.020720, loss_ce: 0.008232
 97%|████████████████████████████▏| 388/400 [3:03:28<05:38, 28.22s/it]2022-01-07 00:44:43,139 iteration 6597 : loss : 0.016866, loss_ce: 0.006956
2022-01-07 00:44:44,663 iteration 6598 : loss : 0.018145, loss_ce: 0.004285
2022-01-07 00:44:46,254 iteration 6599 : loss : 0.014693, loss_ce: 0.006124
2022-01-07 00:44:47,719 iteration 6600 : loss : 0.010929, loss_ce: 0.004532
2022-01-07 00:44:49,276 iteration 6601 : loss : 0.014714, loss_ce: 0.004858
2022-01-07 00:44:50,806 iteration 6602 : loss : 0.014895, loss_ce: 0.005347
2022-01-07 00:44:52,372 iteration 6603 : loss : 0.019447, loss_ce: 0.007525
2022-01-07 00:44:53,861 iteration 6604 : loss : 0.009719, loss_ce: 0.004075
2022-01-07 00:44:55,431 iteration 6605 : loss : 0.012108, loss_ce: 0.004018
2022-01-07 00:44:57,028 iteration 6606 : loss : 0.015740, loss_ce: 0.007400
2022-01-07 00:44:58,624 iteration 6607 : loss : 0.021252, loss_ce: 0.009775
2022-01-07 00:45:00,219 iteration 6608 : loss : 0.012511, loss_ce: 0.005419
2022-01-07 00:45:01,745 iteration 6609 : loss : 0.015276, loss_ce: 0.005522
2022-01-07 00:45:03,287 iteration 6610 : loss : 0.020761, loss_ce: 0.010289
2022-01-07 00:45:04,832 iteration 6611 : loss : 0.011966, loss_ce: 0.002723
2022-01-07 00:45:06,440 iteration 6612 : loss : 0.029711, loss_ce: 0.014910
2022-01-07 00:45:07,902 iteration 6613 : loss : 0.011087, loss_ce: 0.004108
 97%|████████████████████████████▏| 389/400 [3:03:54<05:04, 27.67s/it]2022-01-07 00:45:09,530 iteration 6614 : loss : 0.014280, loss_ce: 0.005485
2022-01-07 00:45:11,113 iteration 6615 : loss : 0.016760, loss_ce: 0.008054
2022-01-07 00:45:12,671 iteration 6616 : loss : 0.015361, loss_ce: 0.004435
2022-01-07 00:45:14,368 iteration 6617 : loss : 0.018835, loss_ce: 0.005979
2022-01-07 00:45:15,961 iteration 6618 : loss : 0.013366, loss_ce: 0.004200
2022-01-07 00:45:17,624 iteration 6619 : loss : 0.017691, loss_ce: 0.007366
2022-01-07 00:45:19,245 iteration 6620 : loss : 0.023336, loss_ce: 0.010574
2022-01-07 00:45:20,805 iteration 6621 : loss : 0.022391, loss_ce: 0.007448
2022-01-07 00:45:22,364 iteration 6622 : loss : 0.013042, loss_ce: 0.006311
2022-01-07 00:45:23,929 iteration 6623 : loss : 0.025185, loss_ce: 0.009240
2022-01-07 00:45:25,538 iteration 6624 : loss : 0.022647, loss_ce: 0.009862
2022-01-07 00:45:27,171 iteration 6625 : loss : 0.013883, loss_ce: 0.005788
2022-01-07 00:45:28,738 iteration 6626 : loss : 0.015423, loss_ce: 0.003996
2022-01-07 00:45:30,231 iteration 6627 : loss : 0.015345, loss_ce: 0.005696
2022-01-07 00:45:31,792 iteration 6628 : loss : 0.017496, loss_ce: 0.007643
2022-01-07 00:45:33,351 iteration 6629 : loss : 0.013709, loss_ce: 0.006781
2022-01-07 00:45:33,351 Training Data Eval:
2022-01-07 00:45:41,416   Average segmentation loss on training set: 0.0085
2022-01-07 00:45:41,416 Validation Data Eval:
2022-01-07 00:45:44,194   Average segmentation loss on validation set: 0.0629
2022-01-07 00:45:45,800 iteration 6630 : loss : 0.015001, loss_ce: 0.004629
 98%|████████████████████████████▎| 390/400 [3:04:32<05:07, 30.74s/it]2022-01-07 00:45:47,552 iteration 6631 : loss : 0.014741, loss_ce: 0.005255
2022-01-07 00:45:49,135 iteration 6632 : loss : 0.015544, loss_ce: 0.006468
2022-01-07 00:45:50,716 iteration 6633 : loss : 0.021727, loss_ce: 0.005311
2022-01-07 00:45:52,171 iteration 6634 : loss : 0.012112, loss_ce: 0.003822
2022-01-07 00:45:53,692 iteration 6635 : loss : 0.017328, loss_ce: 0.004938
2022-01-07 00:45:55,311 iteration 6636 : loss : 0.023806, loss_ce: 0.008472
2022-01-07 00:45:56,907 iteration 6637 : loss : 0.012632, loss_ce: 0.005983
2022-01-07 00:45:58,524 iteration 6638 : loss : 0.018149, loss_ce: 0.006767
2022-01-07 00:46:00,171 iteration 6639 : loss : 0.027719, loss_ce: 0.008443
2022-01-07 00:46:01,734 iteration 6640 : loss : 0.014969, loss_ce: 0.004669
2022-01-07 00:46:03,270 iteration 6641 : loss : 0.014529, loss_ce: 0.007632
2022-01-07 00:46:04,915 iteration 6642 : loss : 0.015473, loss_ce: 0.007550
2022-01-07 00:46:06,510 iteration 6643 : loss : 0.014347, loss_ce: 0.006116
2022-01-07 00:46:08,086 iteration 6644 : loss : 0.014522, loss_ce: 0.004866
2022-01-07 00:46:09,604 iteration 6645 : loss : 0.010152, loss_ce: 0.004720
2022-01-07 00:46:11,155 iteration 6646 : loss : 0.013539, loss_ce: 0.005704
2022-01-07 00:46:12,633 iteration 6647 : loss : 0.012664, loss_ce: 0.004110
 98%|████████████████████████████▎| 391/400 [3:04:59<04:26, 29.56s/it]2022-01-07 00:46:14,220 iteration 6648 : loss : 0.013651, loss_ce: 0.004638
2022-01-07 00:46:15,850 iteration 6649 : loss : 0.017531, loss_ce: 0.007717
2022-01-07 00:46:17,362 iteration 6650 : loss : 0.019066, loss_ce: 0.007532
2022-01-07 00:46:18,891 iteration 6651 : loss : 0.010569, loss_ce: 0.003790
2022-01-07 00:46:20,533 iteration 6652 : loss : 0.022407, loss_ce: 0.007001
2022-01-07 00:46:22,163 iteration 6653 : loss : 0.017988, loss_ce: 0.008002
2022-01-07 00:46:23,703 iteration 6654 : loss : 0.012050, loss_ce: 0.004319
2022-01-07 00:46:25,298 iteration 6655 : loss : 0.018000, loss_ce: 0.008511
2022-01-07 00:46:26,903 iteration 6656 : loss : 0.015629, loss_ce: 0.006532
2022-01-07 00:46:28,569 iteration 6657 : loss : 0.023398, loss_ce: 0.009276
2022-01-07 00:46:30,165 iteration 6658 : loss : 0.020768, loss_ce: 0.008725
2022-01-07 00:46:31,787 iteration 6659 : loss : 0.018982, loss_ce: 0.006761
2022-01-07 00:46:33,326 iteration 6660 : loss : 0.023014, loss_ce: 0.007508
2022-01-07 00:46:34,821 iteration 6661 : loss : 0.011555, loss_ce: 0.004378
2022-01-07 00:46:36,409 iteration 6662 : loss : 0.011242, loss_ce: 0.003992
2022-01-07 00:46:38,034 iteration 6663 : loss : 0.019073, loss_ce: 0.007257
2022-01-07 00:46:39,600 iteration 6664 : loss : 0.014833, loss_ce: 0.004615
 98%|████████████████████████████▍| 392/400 [3:05:26<03:50, 28.79s/it]2022-01-07 00:46:41,253 iteration 6665 : loss : 0.017555, loss_ce: 0.006178
2022-01-07 00:46:42,844 iteration 6666 : loss : 0.013478, loss_ce: 0.005581
2022-01-07 00:46:44,385 iteration 6667 : loss : 0.016638, loss_ce: 0.006285
2022-01-07 00:46:45,859 iteration 6668 : loss : 0.009925, loss_ce: 0.004191
2022-01-07 00:46:47,442 iteration 6669 : loss : 0.014819, loss_ce: 0.004783
2022-01-07 00:46:48,951 iteration 6670 : loss : 0.013549, loss_ce: 0.004466
2022-01-07 00:46:50,591 iteration 6671 : loss : 0.013911, loss_ce: 0.005489
2022-01-07 00:46:52,161 iteration 6672 : loss : 0.023297, loss_ce: 0.011378
2022-01-07 00:46:53,809 iteration 6673 : loss : 0.019750, loss_ce: 0.008615
2022-01-07 00:46:55,443 iteration 6674 : loss : 0.021497, loss_ce: 0.008142
2022-01-07 00:46:56,988 iteration 6675 : loss : 0.021198, loss_ce: 0.006163
2022-01-07 00:46:58,489 iteration 6676 : loss : 0.010522, loss_ce: 0.004193
2022-01-07 00:47:00,076 iteration 6677 : loss : 0.011516, loss_ce: 0.002670
2022-01-07 00:47:01,737 iteration 6678 : loss : 0.027987, loss_ce: 0.014948
2022-01-07 00:47:03,338 iteration 6679 : loss : 0.017554, loss_ce: 0.005965
2022-01-07 00:47:04,900 iteration 6680 : loss : 0.019219, loss_ce: 0.007880
2022-01-07 00:47:06,428 iteration 6681 : loss : 0.017401, loss_ce: 0.006100
 98%|████████████████████████████▍| 393/400 [3:05:53<03:17, 28.20s/it]2022-01-07 00:47:08,058 iteration 6682 : loss : 0.012539, loss_ce: 0.003881
2022-01-07 00:47:09,680 iteration 6683 : loss : 0.017200, loss_ce: 0.005894
2022-01-07 00:47:11,345 iteration 6684 : loss : 0.040005, loss_ce: 0.005029
2022-01-07 00:47:13,039 iteration 6685 : loss : 0.012809, loss_ce: 0.004955
2022-01-07 00:47:14,692 iteration 6686 : loss : 0.019251, loss_ce: 0.005875
2022-01-07 00:47:16,308 iteration 6687 : loss : 0.016143, loss_ce: 0.006498
2022-01-07 00:47:17,901 iteration 6688 : loss : 0.013909, loss_ce: 0.004803
2022-01-07 00:47:19,449 iteration 6689 : loss : 0.020226, loss_ce: 0.008937
2022-01-07 00:47:21,084 iteration 6690 : loss : 0.019660, loss_ce: 0.007538
2022-01-07 00:47:22,604 iteration 6691 : loss : 0.011228, loss_ce: 0.003989
2022-01-07 00:47:24,133 iteration 6692 : loss : 0.010431, loss_ce: 0.004873
2022-01-07 00:47:25,793 iteration 6693 : loss : 0.015370, loss_ce: 0.005787
2022-01-07 00:47:27,506 iteration 6694 : loss : 0.010568, loss_ce: 0.004639
2022-01-07 00:47:29,404 iteration 6695 : loss : 0.012800, loss_ce: 0.003644
2022-01-07 00:47:31,269 iteration 6696 : loss : 0.018233, loss_ce: 0.010946
2022-01-07 00:47:33,136 iteration 6697 : loss : 0.013314, loss_ce: 0.004695
2022-01-07 00:47:34,876 iteration 6698 : loss : 0.016363, loss_ce: 0.005186
 98%|████████████████████████████▌| 394/400 [3:06:21<02:49, 28.27s/it]2022-01-07 00:47:36,699 iteration 6699 : loss : 0.022311, loss_ce: 0.005906
2022-01-07 00:47:38,451 iteration 6700 : loss : 0.016569, loss_ce: 0.007927
2022-01-07 00:47:40,145 iteration 6701 : loss : 0.030681, loss_ce: 0.013529
2022-01-07 00:47:41,706 iteration 6702 : loss : 0.015964, loss_ce: 0.005662
2022-01-07 00:47:43,374 iteration 6703 : loss : 0.018609, loss_ce: 0.005552
2022-01-07 00:47:44,958 iteration 6704 : loss : 0.014047, loss_ce: 0.007206
2022-01-07 00:47:46,476 iteration 6705 : loss : 0.012814, loss_ce: 0.004124
2022-01-07 00:47:48,022 iteration 6706 : loss : 0.014056, loss_ce: 0.004944
2022-01-07 00:47:49,586 iteration 6707 : loss : 0.013695, loss_ce: 0.004812
2022-01-07 00:47:51,077 iteration 6708 : loss : 0.011986, loss_ce: 0.003431
2022-01-07 00:47:52,598 iteration 6709 : loss : 0.014825, loss_ce: 0.004023
2022-01-07 00:47:54,180 iteration 6710 : loss : 0.015553, loss_ce: 0.005644
2022-01-07 00:47:55,746 iteration 6711 : loss : 0.021604, loss_ce: 0.009478
2022-01-07 00:47:57,399 iteration 6712 : loss : 0.016341, loss_ce: 0.006357
2022-01-07 00:47:59,164 iteration 6713 : loss : 0.013490, loss_ce: 0.004447
2022-01-07 00:48:01,096 iteration 6714 : loss : 0.016138, loss_ce: 0.007963
2022-01-07 00:48:01,096 Training Data Eval:
2022-01-07 00:48:12,947   Average segmentation loss on training set: 0.0085
2022-01-07 00:48:12,948 Validation Data Eval:
2022-01-07 00:48:17,462   Average segmentation loss on validation set: 0.0635
2022-01-07 00:48:19,847 iteration 6715 : loss : 0.010030, loss_ce: 0.002928
 99%|████████████████████████████▋| 395/400 [3:07:06<02:46, 33.28s/it]2022-01-07 00:48:22,170 iteration 6716 : loss : 0.010583, loss_ce: 0.003665
2022-01-07 00:48:24,451 iteration 6717 : loss : 0.019224, loss_ce: 0.005868
2022-01-07 00:48:26,695 iteration 6718 : loss : 0.023149, loss_ce: 0.012340
2022-01-07 00:48:28,850 iteration 6719 : loss : 0.023321, loss_ce: 0.009251
2022-01-07 00:48:30,918 iteration 6720 : loss : 0.029801, loss_ce: 0.005625
2022-01-07 00:48:32,931 iteration 6721 : loss : 0.034083, loss_ce: 0.018802
2022-01-07 00:48:34,857 iteration 6722 : loss : 0.017146, loss_ce: 0.007593
2022-01-07 00:48:36,816 iteration 6723 : loss : 0.019308, loss_ce: 0.007283
2022-01-07 00:48:38,683 iteration 6724 : loss : 0.015984, loss_ce: 0.006114
2022-01-07 00:48:40,600 iteration 6725 : loss : 0.015536, loss_ce: 0.005249
2022-01-07 00:48:42,415 iteration 6726 : loss : 0.010540, loss_ce: 0.003838
2022-01-07 00:48:44,360 iteration 6727 : loss : 0.035704, loss_ce: 0.013648
2022-01-07 00:48:46,124 iteration 6728 : loss : 0.015580, loss_ce: 0.006209
2022-01-07 00:48:47,965 iteration 6729 : loss : 0.018339, loss_ce: 0.005595
2022-01-07 00:48:49,806 iteration 6730 : loss : 0.015673, loss_ce: 0.006913
2022-01-07 00:48:51,557 iteration 6731 : loss : 0.018804, loss_ce: 0.005834
2022-01-07 00:48:53,264 iteration 6732 : loss : 0.012427, loss_ce: 0.006001
 99%|████████████████████████████▋| 396/400 [3:07:40<02:13, 33.32s/it]2022-01-07 00:48:55,111 iteration 6733 : loss : 0.010989, loss_ce: 0.004778
2022-01-07 00:48:56,913 iteration 6734 : loss : 0.015664, loss_ce: 0.003947
2022-01-07 00:48:58,763 iteration 6735 : loss : 0.030581, loss_ce: 0.008570
2022-01-07 00:49:00,605 iteration 6736 : loss : 0.011946, loss_ce: 0.005590
2022-01-07 00:49:02,428 iteration 6737 : loss : 0.012832, loss_ce: 0.006414
2022-01-07 00:49:04,256 iteration 6738 : loss : 0.012716, loss_ce: 0.004123
2022-01-07 00:49:06,110 iteration 6739 : loss : 0.014592, loss_ce: 0.006101
2022-01-07 00:49:07,940 iteration 6740 : loss : 0.020083, loss_ce: 0.006659
2022-01-07 00:49:09,749 iteration 6741 : loss : 0.017269, loss_ce: 0.007326
2022-01-07 00:49:11,547 iteration 6742 : loss : 0.015547, loss_ce: 0.007803
2022-01-07 00:49:13,281 iteration 6743 : loss : 0.012170, loss_ce: 0.002847
2022-01-07 00:49:15,070 iteration 6744 : loss : 0.017910, loss_ce: 0.007861
2022-01-07 00:49:16,844 iteration 6745 : loss : 0.015525, loss_ce: 0.006572
2022-01-07 00:49:18,598 iteration 6746 : loss : 0.019210, loss_ce: 0.007603
2022-01-07 00:49:20,350 iteration 6747 : loss : 0.012593, loss_ce: 0.004404
2022-01-07 00:49:22,188 iteration 6748 : loss : 0.011051, loss_ce: 0.004754
2022-01-07 00:49:23,967 iteration 6749 : loss : 0.013102, loss_ce: 0.003760
 99%|████████████████████████████▊| 397/400 [3:08:10<01:37, 32.53s/it]2022-01-07 00:49:25,760 iteration 6750 : loss : 0.011717, loss_ce: 0.005195
2022-01-07 00:49:27,519 iteration 6751 : loss : 0.012779, loss_ce: 0.005516
2022-01-07 00:49:29,131 iteration 6752 : loss : 0.014351, loss_ce: 0.004743
2022-01-07 00:49:30,784 iteration 6753 : loss : 0.016919, loss_ce: 0.007463
2022-01-07 00:49:32,430 iteration 6754 : loss : 0.013024, loss_ce: 0.004786
2022-01-07 00:49:34,008 iteration 6755 : loss : 0.019395, loss_ce: 0.006056
2022-01-07 00:49:35,628 iteration 6756 : loss : 0.016283, loss_ce: 0.007631
2022-01-07 00:49:37,333 iteration 6757 : loss : 0.015112, loss_ce: 0.005034
2022-01-07 00:49:38,917 iteration 6758 : loss : 0.014927, loss_ce: 0.007258
2022-01-07 00:49:40,516 iteration 6759 : loss : 0.011194, loss_ce: 0.004781
2022-01-07 00:49:42,155 iteration 6760 : loss : 0.025125, loss_ce: 0.004668
2022-01-07 00:49:43,721 iteration 6761 : loss : 0.014398, loss_ce: 0.005399
2022-01-07 00:49:45,269 iteration 6762 : loss : 0.015492, loss_ce: 0.005031
2022-01-07 00:49:46,890 iteration 6763 : loss : 0.016978, loss_ce: 0.007184
2022-01-07 00:49:48,425 iteration 6764 : loss : 0.016722, loss_ce: 0.007350
2022-01-07 00:49:50,048 iteration 6765 : loss : 0.018752, loss_ce: 0.006591
2022-01-07 00:49:51,586 iteration 6766 : loss : 0.011768, loss_ce: 0.005665
100%|████████████████████████████▊| 398/400 [3:08:38<01:02, 31.06s/it]2022-01-07 00:49:53,205 iteration 6767 : loss : 0.041260, loss_ce: 0.011009
2022-01-07 00:49:54,857 iteration 6768 : loss : 0.019355, loss_ce: 0.007662
2022-01-07 00:49:56,488 iteration 6769 : loss : 0.015036, loss_ce: 0.006276
2022-01-07 00:49:58,039 iteration 6770 : loss : 0.013984, loss_ce: 0.006605
2022-01-07 00:49:59,704 iteration 6771 : loss : 0.017439, loss_ce: 0.003572
2022-01-07 00:50:01,260 iteration 6772 : loss : 0.014192, loss_ce: 0.005446
2022-01-07 00:50:02,901 iteration 6773 : loss : 0.011718, loss_ce: 0.003640
2022-01-07 00:50:04,801 iteration 6774 : loss : 0.019731, loss_ce: 0.005233
2022-01-07 00:50:06,695 iteration 6775 : loss : 0.015673, loss_ce: 0.005830
2022-01-07 00:50:08,498 iteration 6776 : loss : 0.016010, loss_ce: 0.005110
2022-01-07 00:50:10,339 iteration 6777 : loss : 0.019754, loss_ce: 0.010804
2022-01-07 00:50:12,051 iteration 6778 : loss : 0.013791, loss_ce: 0.005304
2022-01-07 00:50:13,796 iteration 6779 : loss : 0.017248, loss_ce: 0.006368
2022-01-07 00:50:15,464 iteration 6780 : loss : 0.014545, loss_ce: 0.006288
2022-01-07 00:50:17,207 iteration 6781 : loss : 0.017977, loss_ce: 0.008583
2022-01-07 00:50:18,986 iteration 6782 : loss : 0.012352, loss_ce: 0.004893
2022-01-07 00:50:20,754 iteration 6783 : loss : 0.015228, loss_ce: 0.005070
100%|████████████████████████████▉| 399/400 [3:09:07<00:30, 30.49s/it]2022-01-07 00:50:22,693 iteration 6784 : loss : 0.015457, loss_ce: 0.005207
2022-01-07 00:50:24,555 iteration 6785 : loss : 0.020230, loss_ce: 0.006866
2022-01-07 00:50:26,385 iteration 6786 : loss : 0.010422, loss_ce: 0.004139
2022-01-07 00:50:28,168 iteration 6787 : loss : 0.011481, loss_ce: 0.003674
2022-01-07 00:50:29,955 iteration 6788 : loss : 0.014888, loss_ce: 0.006149
2022-01-07 00:50:31,708 iteration 6789 : loss : 0.019972, loss_ce: 0.005673
2022-01-07 00:50:33,443 iteration 6790 : loss : 0.011153, loss_ce: 0.004799
2022-01-07 00:50:35,243 iteration 6791 : loss : 0.013794, loss_ce: 0.005356
2022-01-07 00:50:37,088 iteration 6792 : loss : 0.012159, loss_ce: 0.005285
2022-01-07 00:50:39,112 iteration 6793 : loss : 0.025591, loss_ce: 0.008553
2022-01-07 00:50:41,211 iteration 6794 : loss : 0.023952, loss_ce: 0.008107
2022-01-07 00:50:43,478 iteration 6795 : loss : 0.024545, loss_ce: 0.008840
2022-01-07 00:50:46,102 iteration 6796 : loss : 0.024949, loss_ce: 0.014612
2022-01-07 00:50:48,518 iteration 6797 : loss : 0.012694, loss_ce: 0.004103
2022-01-07 00:50:51,172 iteration 6798 : loss : 0.015108, loss_ce: 0.004683
2022-01-07 00:50:53,849 iteration 6799 : loss : 0.022147, loss_ce: 0.011332
2022-01-07 00:50:53,849 Training Data Eval:
2022-01-07 00:51:08,326   Average segmentation loss on training set: 0.0080
2022-01-07 00:51:08,327 Validation Data Eval:
2022-01-07 00:51:13,326   Average segmentation loss on validation set: 0.0635
2022-01-07 00:51:15,896 iteration 6800 : loss : 0.010961, loss_ce: 0.002937
100%|█████████████████████████████| 400/400 [3:10:02<00:00, 37.89s/it]100%|█████████████████████████████| 400/400 [3:10:02<00:00, 28.51s/it]
