2022-01-11 22:12:55,845 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:12:55,846 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:12:55,846 ============================================================
2022-01-11 22:12:55,846 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:12:55,846 ============================================================
2022-01-11 22:12:55,846 Loading data...
2022-01-11 22:12:55,846 Reading NCI - RUNMC images...
2022-01-11 22:12:55,846 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-11 22:12:55,846 Already preprocessed this configuration. Loading now!
2022-01-11 22:12:55,864 Training Images: (256, 256, 286)
2022-01-11 22:12:55,865 Training Labels: (256, 256, 286)
2022-01-11 22:12:55,865 Validation Images: (256, 256, 98)
2022-01-11 22:12:55,865 Validation Labels: (256, 256, 98)
2022-01-11 22:12:55,865 ============================================================
2022-01-11 22:12:55,911 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-11 22:12:58,351 iteration 1 : loss : 0.985024, loss_ce: 1.214489
2022-01-11 22:12:59,749 iteration 2 : loss : 0.920041, loss_ce: 1.112155
2022-01-11 22:13:01,261 iteration 3 : loss : 0.866510, loss_ce: 1.016282
2022-01-11 22:13:02,678 iteration 4 : loss : 0.812036, loss_ce: 0.923884
2022-01-11 22:13:04,098 iteration 5 : loss : 0.759794, loss_ce: 0.848028
2022-01-11 22:13:05,537 iteration 6 : loss : 0.728790, loss_ce: 0.787976
2022-01-11 22:13:07,040 iteration 7 : loss : 0.682083, loss_ce: 0.724677
2022-01-11 22:13:08,485 iteration 8 : loss : 0.669165, loss_ce: 0.672693
2022-01-11 22:13:09,948 iteration 9 : loss : 0.607889, loss_ce: 0.635944
2022-01-11 22:13:11,439 iteration 10 : loss : 0.602996, loss_ce: 0.570971
2022-01-11 22:13:12,989 iteration 11 : loss : 0.570962, loss_ce: 0.541535
2022-01-11 22:13:14,425 iteration 12 : loss : 0.542239, loss_ce: 0.493084
2022-01-11 22:13:15,843 iteration 13 : loss : 0.534452, loss_ce: 0.466664
2022-01-11 22:13:17,232 iteration 14 : loss : 0.501385, loss_ce: 0.431299
2022-01-11 22:13:18,702 iteration 15 : loss : 0.474611, loss_ce: 0.395628
2022-01-11 22:13:20,152 iteration 16 : loss : 0.482276, loss_ce: 0.380199
2022-01-11 22:13:21,598 iteration 17 : loss : 0.422539, loss_ce: 0.339690
  0%|                               | 1/400 [00:25<2:51:11, 25.74s/it]2022-01-11 22:13:23,133 iteration 18 : loss : 0.450221, loss_ce: 0.307388
2022-01-11 22:13:24,516 iteration 19 : loss : 0.386112, loss_ce: 0.273263
2022-01-11 22:13:26,036 iteration 20 : loss : 0.378604, loss_ce: 0.255304
2022-01-11 22:13:27,459 iteration 21 : loss : 0.397424, loss_ce: 0.239848
2022-01-11 22:13:28,895 iteration 22 : loss : 0.358548, loss_ce: 0.235740
2022-01-11 22:13:30,417 iteration 23 : loss : 0.354197, loss_ce: 0.207094
2022-01-11 22:13:31,871 iteration 24 : loss : 0.336781, loss_ce: 0.202562
2022-01-11 22:13:33,371 iteration 25 : loss : 0.372800, loss_ce: 0.238350
2022-01-11 22:13:34,799 iteration 26 : loss : 0.313588, loss_ce: 0.178081
2022-01-11 22:13:36,172 iteration 27 : loss : 0.310473, loss_ce: 0.178622
2022-01-11 22:13:37,556 iteration 28 : loss : 0.298254, loss_ce: 0.162952
2022-01-11 22:13:39,046 iteration 29 : loss : 0.299347, loss_ce: 0.157501
2022-01-11 22:13:40,522 iteration 30 : loss : 0.305734, loss_ce: 0.155610
2022-01-11 22:13:41,911 iteration 31 : loss : 0.287170, loss_ce: 0.151749
2022-01-11 22:13:43,415 iteration 32 : loss : 0.299965, loss_ce: 0.168605
2022-01-11 22:13:44,904 iteration 33 : loss : 0.303081, loss_ce: 0.169754
2022-01-11 22:13:46,396 iteration 34 : loss : 0.291710, loss_ce: 0.169207
  0%|▏                              | 2/400 [00:50<2:47:02, 25.18s/it]2022-01-11 22:13:47,928 iteration 35 : loss : 0.290302, loss_ce: 0.144192
2022-01-11 22:13:49,443 iteration 36 : loss : 0.293071, loss_ce: 0.151255
2022-01-11 22:13:50,960 iteration 37 : loss : 0.278481, loss_ce: 0.118204
2022-01-11 22:13:52,393 iteration 38 : loss : 0.273382, loss_ce: 0.129112
2022-01-11 22:13:53,830 iteration 39 : loss : 0.243456, loss_ce: 0.113261
2022-01-11 22:13:55,349 iteration 40 : loss : 0.279573, loss_ce: 0.139274
2022-01-11 22:13:56,864 iteration 41 : loss : 0.312069, loss_ce: 0.153027
2022-01-11 22:13:58,357 iteration 42 : loss : 0.265786, loss_ce: 0.133639
2022-01-11 22:13:59,756 iteration 43 : loss : 0.268233, loss_ce: 0.126270
2022-01-11 22:14:01,292 iteration 44 : loss : 0.230916, loss_ce: 0.107805
2022-01-11 22:14:02,756 iteration 45 : loss : 0.266778, loss_ce: 0.124772
2022-01-11 22:14:04,232 iteration 46 : loss : 0.249653, loss_ce: 0.111593
2022-01-11 22:14:05,743 iteration 47 : loss : 0.222880, loss_ce: 0.097150
2022-01-11 22:14:07,281 iteration 48 : loss : 0.237289, loss_ce: 0.113043
2022-01-11 22:14:08,900 iteration 49 : loss : 0.299642, loss_ce: 0.145372
2022-01-11 22:14:10,434 iteration 50 : loss : 0.322005, loss_ce: 0.137213
2022-01-11 22:14:11,950 iteration 51 : loss : 0.307487, loss_ce: 0.145188
  1%|▏                              | 3/400 [01:16<2:47:44, 25.35s/it]2022-01-11 22:14:13,592 iteration 52 : loss : 0.306818, loss_ce: 0.149548
2022-01-11 22:14:15,169 iteration 53 : loss : 0.281617, loss_ce: 0.126023
2022-01-11 22:14:16,719 iteration 54 : loss : 0.252253, loss_ce: 0.104852
2022-01-11 22:14:18,286 iteration 55 : loss : 0.305265, loss_ce: 0.155784
2022-01-11 22:14:19,827 iteration 56 : loss : 0.260963, loss_ce: 0.116476
2022-01-11 22:14:21,395 iteration 57 : loss : 0.240029, loss_ce: 0.107832
2022-01-11 22:14:22,957 iteration 58 : loss : 0.310704, loss_ce: 0.138580
2022-01-11 22:14:24,490 iteration 59 : loss : 0.232030, loss_ce: 0.108197
2022-01-11 22:14:26,061 iteration 60 : loss : 0.298042, loss_ce: 0.131877
2022-01-11 22:14:27,611 iteration 61 : loss : 0.260040, loss_ce: 0.126235
2022-01-11 22:14:29,159 iteration 62 : loss : 0.347756, loss_ce: 0.142582
2022-01-11 22:14:30,648 iteration 63 : loss : 0.298009, loss_ce: 0.150148
2022-01-11 22:14:32,189 iteration 64 : loss : 0.327747, loss_ce: 0.145714
2022-01-11 22:14:33,687 iteration 65 : loss : 0.249729, loss_ce: 0.096235
2022-01-11 22:14:35,208 iteration 66 : loss : 0.219013, loss_ce: 0.092282
2022-01-11 22:14:36,811 iteration 67 : loss : 0.258469, loss_ce: 0.097827
2022-01-11 22:14:38,364 iteration 68 : loss : 0.232454, loss_ce: 0.101829
  1%|▎                              | 4/400 [01:42<2:50:04, 25.77s/it]2022-01-11 22:14:39,975 iteration 69 : loss : 0.214039, loss_ce: 0.094226
2022-01-11 22:14:41,597 iteration 70 : loss : 0.245317, loss_ce: 0.105859
2022-01-11 22:14:43,115 iteration 71 : loss : 0.225074, loss_ce: 0.095875
2022-01-11 22:14:44,689 iteration 72 : loss : 0.231213, loss_ce: 0.100006
2022-01-11 22:14:46,216 iteration 73 : loss : 0.247479, loss_ce: 0.123045
2022-01-11 22:14:47,699 iteration 74 : loss : 0.226785, loss_ce: 0.096836
2022-01-11 22:14:49,216 iteration 75 : loss : 0.217770, loss_ce: 0.094678
2022-01-11 22:14:50,729 iteration 76 : loss : 0.241888, loss_ce: 0.103907
2022-01-11 22:14:52,183 iteration 77 : loss : 0.232082, loss_ce: 0.107637
2022-01-11 22:14:53,731 iteration 78 : loss : 0.278361, loss_ce: 0.126781
2022-01-11 22:14:55,248 iteration 79 : loss : 0.287377, loss_ce: 0.107176
2022-01-11 22:14:56,751 iteration 80 : loss : 0.248787, loss_ce: 0.112632
2022-01-11 22:14:58,250 iteration 81 : loss : 0.251686, loss_ce: 0.109155
2022-01-11 22:14:59,767 iteration 82 : loss : 0.235670, loss_ce: 0.092029
2022-01-11 22:15:01,285 iteration 83 : loss : 0.248396, loss_ce: 0.091203
2022-01-11 22:15:02,890 iteration 84 : loss : 0.266150, loss_ce: 0.139277
2022-01-11 22:15:02,890 Training Data Eval:
2022-01-11 22:15:10,832   Average segmentation loss on training set: 1.0727
2022-01-11 22:15:10,833 Validation Data Eval:
2022-01-11 22:15:13,572   Average segmentation loss on validation set: 1.0445
2022-01-11 22:15:15,102 iteration 85 : loss : 0.261101, loss_ce: 0.106492
  1%|▍                              | 5/400 [02:19<3:15:40, 29.72s/it]2022-01-11 22:15:16,714 iteration 86 : loss : 0.276865, loss_ce: 0.100769
2022-01-11 22:15:18,391 iteration 87 : loss : 0.226771, loss_ce: 0.095538
2022-01-11 22:15:19,908 iteration 88 : loss : 0.242842, loss_ce: 0.104637
2022-01-11 22:15:21,491 iteration 89 : loss : 0.252962, loss_ce: 0.108283
2022-01-11 22:15:23,074 iteration 90 : loss : 0.236361, loss_ce: 0.099271
2022-01-11 22:15:24,704 iteration 91 : loss : 0.214824, loss_ce: 0.107204
2022-01-11 22:15:26,193 iteration 92 : loss : 0.231246, loss_ce: 0.102063
2022-01-11 22:15:27,735 iteration 93 : loss : 0.253551, loss_ce: 0.092653
2022-01-11 22:15:29,275 iteration 94 : loss : 0.233669, loss_ce: 0.092565
2022-01-11 22:15:30,939 iteration 95 : loss : 0.242237, loss_ce: 0.110485
2022-01-11 22:15:32,471 iteration 96 : loss : 0.231718, loss_ce: 0.103214
2022-01-11 22:15:34,003 iteration 97 : loss : 0.250440, loss_ce: 0.100798
2022-01-11 22:15:35,554 iteration 98 : loss : 0.246957, loss_ce: 0.105840
2022-01-11 22:15:37,163 iteration 99 : loss : 0.217023, loss_ce: 0.098881
2022-01-11 22:15:38,737 iteration 100 : loss : 0.229484, loss_ce: 0.095998
2022-01-11 22:15:40,284 iteration 101 : loss : 0.164398, loss_ce: 0.062818
2022-01-11 22:15:41,769 iteration 102 : loss : 0.215975, loss_ce: 0.085665
  2%|▍                              | 6/400 [02:45<3:08:22, 28.69s/it]2022-01-11 22:15:43,449 iteration 103 : loss : 0.200147, loss_ce: 0.086866
2022-01-11 22:15:45,079 iteration 104 : loss : 0.222671, loss_ce: 0.090797
2022-01-11 22:15:46,736 iteration 105 : loss : 0.265152, loss_ce: 0.109866
2022-01-11 22:15:48,254 iteration 106 : loss : 0.223970, loss_ce: 0.084805
2022-01-11 22:15:49,934 iteration 107 : loss : 0.217800, loss_ce: 0.097646
2022-01-11 22:15:51,431 iteration 108 : loss : 0.272533, loss_ce: 0.113038
2022-01-11 22:15:52,969 iteration 109 : loss : 0.223237, loss_ce: 0.104715
2022-01-11 22:15:54,457 iteration 110 : loss : 0.189045, loss_ce: 0.083023
2022-01-11 22:15:56,035 iteration 111 : loss : 0.261866, loss_ce: 0.134019
2022-01-11 22:15:57,520 iteration 112 : loss : 0.212019, loss_ce: 0.087771
2022-01-11 22:15:59,077 iteration 113 : loss : 0.267901, loss_ce: 0.142039
2022-01-11 22:16:00,593 iteration 114 : loss : 0.256476, loss_ce: 0.089945
2022-01-11 22:16:02,135 iteration 115 : loss : 0.217744, loss_ce: 0.093919
2022-01-11 22:16:03,726 iteration 116 : loss : 0.276093, loss_ce: 0.125223
2022-01-11 22:16:05,316 iteration 117 : loss : 0.223799, loss_ce: 0.096806
2022-01-11 22:16:06,878 iteration 118 : loss : 0.283355, loss_ce: 0.126724
2022-01-11 22:16:08,448 iteration 119 : loss : 0.209972, loss_ce: 0.076983
  2%|▌                              | 7/400 [03:12<3:03:36, 28.03s/it]2022-01-11 22:16:10,003 iteration 120 : loss : 0.347646, loss_ce: 0.170624
2022-01-11 22:16:11,500 iteration 121 : loss : 0.224408, loss_ce: 0.095806
2022-01-11 22:16:13,093 iteration 122 : loss : 0.262183, loss_ce: 0.101897
2022-01-11 22:16:14,668 iteration 123 : loss : 0.217176, loss_ce: 0.085074
2022-01-11 22:16:16,211 iteration 124 : loss : 0.222572, loss_ce: 0.089449
2022-01-11 22:16:17,720 iteration 125 : loss : 0.224472, loss_ce: 0.103988
2022-01-11 22:16:19,280 iteration 126 : loss : 0.226195, loss_ce: 0.089728
2022-01-11 22:16:20,787 iteration 127 : loss : 0.204829, loss_ce: 0.096593
2022-01-11 22:16:22,316 iteration 128 : loss : 0.213369, loss_ce: 0.090084
2022-01-11 22:16:23,910 iteration 129 : loss : 0.219177, loss_ce: 0.089839
2022-01-11 22:16:25,456 iteration 130 : loss : 0.204723, loss_ce: 0.084995
2022-01-11 22:16:27,090 iteration 131 : loss : 0.206865, loss_ce: 0.106312
2022-01-11 22:16:28,744 iteration 132 : loss : 0.192457, loss_ce: 0.057027
2022-01-11 22:16:30,269 iteration 133 : loss : 0.201297, loss_ce: 0.084533
2022-01-11 22:16:31,891 iteration 134 : loss : 0.218223, loss_ce: 0.097034
2022-01-11 22:16:33,515 iteration 135 : loss : 0.231265, loss_ce: 0.106602
2022-01-11 22:16:35,061 iteration 136 : loss : 0.156690, loss_ce: 0.077638
  2%|▌                              | 8/400 [03:39<3:00:10, 27.58s/it]2022-01-11 22:16:36,717 iteration 137 : loss : 0.233849, loss_ce: 0.084843
2022-01-11 22:16:38,247 iteration 138 : loss : 0.230395, loss_ce: 0.127442
2022-01-11 22:16:39,831 iteration 139 : loss : 0.266459, loss_ce: 0.113534
2022-01-11 22:16:41,390 iteration 140 : loss : 0.178964, loss_ce: 0.074314
2022-01-11 22:16:42,986 iteration 141 : loss : 0.230211, loss_ce: 0.109719
2022-01-11 22:16:44,591 iteration 142 : loss : 0.219116, loss_ce: 0.087835
2022-01-11 22:16:46,215 iteration 143 : loss : 0.226510, loss_ce: 0.103894
2022-01-11 22:16:47,811 iteration 144 : loss : 0.237690, loss_ce: 0.099334
2022-01-11 22:16:49,360 iteration 145 : loss : 0.199738, loss_ce: 0.094296
2022-01-11 22:16:50,951 iteration 146 : loss : 0.192066, loss_ce: 0.104154
2022-01-11 22:16:52,553 iteration 147 : loss : 0.219429, loss_ce: 0.095892
2022-01-11 22:16:54,032 iteration 148 : loss : 0.226205, loss_ce: 0.108667
2022-01-11 22:16:55,577 iteration 149 : loss : 0.273610, loss_ce: 0.124510
2022-01-11 22:16:57,101 iteration 150 : loss : 0.267723, loss_ce: 0.088742
2022-01-11 22:16:58,630 iteration 151 : loss : 0.215591, loss_ce: 0.091030
2022-01-11 22:17:00,166 iteration 152 : loss : 0.219832, loss_ce: 0.074289
2022-01-11 22:17:01,743 iteration 153 : loss : 0.263238, loss_ce: 0.108520
  2%|▋                              | 9/400 [04:05<2:57:53, 27.30s/it]2022-01-11 22:17:03,283 iteration 154 : loss : 0.319240, loss_ce: 0.145301
2022-01-11 22:17:04,851 iteration 155 : loss : 0.222744, loss_ce: 0.076220
2022-01-11 22:17:06,484 iteration 156 : loss : 0.200237, loss_ce: 0.076780
2022-01-11 22:17:08,056 iteration 157 : loss : 0.273764, loss_ce: 0.108543
2022-01-11 22:17:09,734 iteration 158 : loss : 0.211239, loss_ce: 0.096561
2022-01-11 22:17:11,186 iteration 159 : loss : 0.191353, loss_ce: 0.081936
2022-01-11 22:17:12,806 iteration 160 : loss : 0.178342, loss_ce: 0.063379
2022-01-11 22:17:14,419 iteration 161 : loss : 0.244430, loss_ce: 0.105089
2022-01-11 22:17:16,044 iteration 162 : loss : 0.224641, loss_ce: 0.103638
2022-01-11 22:17:17,607 iteration 163 : loss : 0.241038, loss_ce: 0.086628
2022-01-11 22:17:19,187 iteration 164 : loss : 0.229466, loss_ce: 0.087917
2022-01-11 22:17:20,730 iteration 165 : loss : 0.260199, loss_ce: 0.119903
2022-01-11 22:17:22,261 iteration 166 : loss : 0.244778, loss_ce: 0.099644
2022-01-11 22:17:23,806 iteration 167 : loss : 0.184536, loss_ce: 0.081150
2022-01-11 22:17:25,393 iteration 168 : loss : 0.252360, loss_ce: 0.130033
2022-01-11 22:17:26,950 iteration 169 : loss : 0.221276, loss_ce: 0.116199
2022-01-11 22:17:26,950 Training Data Eval:
2022-01-11 22:17:34,896   Average segmentation loss on training set: 0.5164
2022-01-11 22:17:34,896 Validation Data Eval:
2022-01-11 22:17:37,632   Average segmentation loss on validation set: 0.4605
2022-01-11 22:17:43,636 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 22:17:45,141 iteration 170 : loss : 0.194014, loss_ce: 0.092379
  2%|▊                             | 10/400 [04:49<3:29:43, 32.27s/it]2022-01-11 22:17:46,653 iteration 171 : loss : 0.215363, loss_ce: 0.093659
2022-01-11 22:17:48,141 iteration 172 : loss : 0.238232, loss_ce: 0.112780
2022-01-11 22:17:49,650 iteration 173 : loss : 0.187354, loss_ce: 0.092215
2022-01-11 22:17:51,068 iteration 174 : loss : 0.230403, loss_ce: 0.102695
2022-01-11 22:17:52,472 iteration 175 : loss : 0.275061, loss_ce: 0.123425
2022-01-11 22:17:53,957 iteration 176 : loss : 0.192794, loss_ce: 0.093033
2022-01-11 22:17:55,418 iteration 177 : loss : 0.215714, loss_ce: 0.088757
2022-01-11 22:17:56,879 iteration 178 : loss : 0.157681, loss_ce: 0.071342
2022-01-11 22:17:58,364 iteration 179 : loss : 0.219115, loss_ce: 0.094287
2022-01-11 22:17:59,892 iteration 180 : loss : 0.183374, loss_ce: 0.071403
2022-01-11 22:18:01,484 iteration 181 : loss : 0.173190, loss_ce: 0.069784
2022-01-11 22:18:03,021 iteration 182 : loss : 0.156612, loss_ce: 0.068136
2022-01-11 22:18:04,538 iteration 183 : loss : 0.191773, loss_ce: 0.069308
2022-01-11 22:18:06,015 iteration 184 : loss : 0.214088, loss_ce: 0.077653
2022-01-11 22:18:07,603 iteration 185 : loss : 0.226288, loss_ce: 0.082620
2022-01-11 22:18:09,193 iteration 186 : loss : 0.252207, loss_ce: 0.101348
2022-01-11 22:18:10,820 iteration 187 : loss : 0.213315, loss_ce: 0.082487
  3%|▊                             | 11/400 [05:14<3:16:07, 30.25s/it]2022-01-11 22:18:12,456 iteration 188 : loss : 0.222180, loss_ce: 0.094363
2022-01-11 22:18:14,041 iteration 189 : loss : 0.212780, loss_ce: 0.086684
2022-01-11 22:18:15,619 iteration 190 : loss : 0.235172, loss_ce: 0.088058
2022-01-11 22:18:17,163 iteration 191 : loss : 0.212777, loss_ce: 0.085221
2022-01-11 22:18:18,661 iteration 192 : loss : 0.248340, loss_ce: 0.119599
2022-01-11 22:18:20,276 iteration 193 : loss : 0.178770, loss_ce: 0.079359
2022-01-11 22:18:21,826 iteration 194 : loss : 0.257908, loss_ce: 0.076966
2022-01-11 22:18:23,352 iteration 195 : loss : 0.220692, loss_ce: 0.103852
2022-01-11 22:18:24,834 iteration 196 : loss : 0.211489, loss_ce: 0.076218
2022-01-11 22:18:26,404 iteration 197 : loss : 0.276297, loss_ce: 0.115406
2022-01-11 22:18:28,004 iteration 198 : loss : 0.179117, loss_ce: 0.076971
2022-01-11 22:18:29,530 iteration 199 : loss : 0.197389, loss_ce: 0.085462
2022-01-11 22:18:31,072 iteration 200 : loss : 0.204793, loss_ce: 0.087029
2022-01-11 22:18:32,641 iteration 201 : loss : 0.166172, loss_ce: 0.071038
2022-01-11 22:18:34,232 iteration 202 : loss : 0.218915, loss_ce: 0.101312
2022-01-11 22:18:35,753 iteration 203 : loss : 0.146254, loss_ce: 0.060718
2022-01-11 22:18:37,395 iteration 204 : loss : 0.233909, loss_ce: 0.085708
  3%|▉                             | 12/400 [05:41<3:08:23, 29.13s/it]2022-01-11 22:18:38,960 iteration 205 : loss : 0.192803, loss_ce: 0.072630
2022-01-11 22:18:40,519 iteration 206 : loss : 0.174384, loss_ce: 0.061484
2022-01-11 22:18:42,096 iteration 207 : loss : 0.193452, loss_ce: 0.077798
2022-01-11 22:18:43,632 iteration 208 : loss : 0.240217, loss_ce: 0.080360
2022-01-11 22:18:45,262 iteration 209 : loss : 0.292596, loss_ce: 0.149473
2022-01-11 22:18:46,738 iteration 210 : loss : 0.195780, loss_ce: 0.076717
2022-01-11 22:18:48,331 iteration 211 : loss : 0.228417, loss_ce: 0.114227
2022-01-11 22:18:49,909 iteration 212 : loss : 0.216661, loss_ce: 0.091829
2022-01-11 22:18:51,560 iteration 213 : loss : 0.167580, loss_ce: 0.084349
2022-01-11 22:18:53,129 iteration 214 : loss : 0.211955, loss_ce: 0.077921
2022-01-11 22:18:54,690 iteration 215 : loss : 0.231472, loss_ce: 0.095003
2022-01-11 22:18:56,398 iteration 216 : loss : 0.219238, loss_ce: 0.089098
2022-01-11 22:18:58,048 iteration 217 : loss : 0.204413, loss_ce: 0.075677
2022-01-11 22:18:59,628 iteration 218 : loss : 0.186126, loss_ce: 0.087499
2022-01-11 22:19:01,098 iteration 219 : loss : 0.192111, loss_ce: 0.083963
2022-01-11 22:19:02,684 iteration 220 : loss : 0.168481, loss_ce: 0.076635
2022-01-11 22:19:04,253 iteration 221 : loss : 0.230999, loss_ce: 0.088624
  3%|▉                             | 13/400 [06:08<3:03:27, 28.44s/it]2022-01-11 22:19:05,864 iteration 222 : loss : 0.204639, loss_ce: 0.094364
2022-01-11 22:19:07,472 iteration 223 : loss : 0.167080, loss_ce: 0.071794
2022-01-11 22:19:09,031 iteration 224 : loss : 0.284583, loss_ce: 0.149688
2022-01-11 22:19:10,683 iteration 225 : loss : 0.294871, loss_ce: 0.088348
2022-01-11 22:19:12,239 iteration 226 : loss : 0.187739, loss_ce: 0.071155
2022-01-11 22:19:13,839 iteration 227 : loss : 0.239549, loss_ce: 0.108929
2022-01-11 22:19:15,398 iteration 228 : loss : 0.194468, loss_ce: 0.087789
2022-01-11 22:19:16,919 iteration 229 : loss : 0.197316, loss_ce: 0.070884
2022-01-11 22:19:18,486 iteration 230 : loss : 0.170042, loss_ce: 0.070052
2022-01-11 22:19:20,094 iteration 231 : loss : 0.305709, loss_ce: 0.129489
2022-01-11 22:19:21,696 iteration 232 : loss : 0.147398, loss_ce: 0.061373
2022-01-11 22:19:23,223 iteration 233 : loss : 0.289587, loss_ce: 0.136590
2022-01-11 22:19:24,851 iteration 234 : loss : 0.174351, loss_ce: 0.071102
2022-01-11 22:19:26,397 iteration 235 : loss : 0.228719, loss_ce: 0.102447
2022-01-11 22:19:27,980 iteration 236 : loss : 0.216240, loss_ce: 0.086931
2022-01-11 22:19:29,521 iteration 237 : loss : 0.171848, loss_ce: 0.064078
2022-01-11 22:19:31,081 iteration 238 : loss : 0.222085, loss_ce: 0.108237
  4%|█                             | 14/400 [06:35<2:59:51, 27.96s/it]2022-01-11 22:19:32,626 iteration 239 : loss : 0.206559, loss_ce: 0.080528
2022-01-11 22:19:34,219 iteration 240 : loss : 0.179469, loss_ce: 0.088089
2022-01-11 22:19:35,822 iteration 241 : loss : 0.254968, loss_ce: 0.107427
2022-01-11 22:19:37,414 iteration 242 : loss : 0.225634, loss_ce: 0.130363
2022-01-11 22:19:39,107 iteration 243 : loss : 0.221696, loss_ce: 0.099125
2022-01-11 22:19:40,620 iteration 244 : loss : 0.218796, loss_ce: 0.061365
2022-01-11 22:19:42,206 iteration 245 : loss : 0.175238, loss_ce: 0.079551
2022-01-11 22:19:43,822 iteration 246 : loss : 0.192915, loss_ce: 0.080454
2022-01-11 22:19:45,389 iteration 247 : loss : 0.250186, loss_ce: 0.086606
2022-01-11 22:19:46,947 iteration 248 : loss : 0.191994, loss_ce: 0.068429
2022-01-11 22:19:48,537 iteration 249 : loss : 0.212726, loss_ce: 0.112425
2022-01-11 22:19:50,088 iteration 250 : loss : 0.230572, loss_ce: 0.097369
2022-01-11 22:19:51,590 iteration 251 : loss : 0.243308, loss_ce: 0.088900
2022-01-11 22:19:53,141 iteration 252 : loss : 0.134430, loss_ce: 0.055525
2022-01-11 22:19:54,720 iteration 253 : loss : 0.197206, loss_ce: 0.074208
2022-01-11 22:19:56,353 iteration 254 : loss : 0.299453, loss_ce: 0.100140
2022-01-11 22:19:56,353 Training Data Eval:
2022-01-11 22:20:04,309   Average segmentation loss on training set: 0.1621
2022-01-11 22:20:04,309 Validation Data Eval:
2022-01-11 22:20:07,049   Average segmentation loss on validation set: 0.1757
2022-01-11 22:20:12,993 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 22:20:14,423 iteration 255 : loss : 0.190292, loss_ce: 0.082295
  4%|█▏                            | 15/400 [07:18<3:29:09, 32.60s/it]2022-01-11 22:20:15,948 iteration 256 : loss : 0.192540, loss_ce: 0.076069
2022-01-11 22:20:17,466 iteration 257 : loss : 0.178341, loss_ce: 0.080511
2022-01-11 22:20:19,026 iteration 258 : loss : 0.201000, loss_ce: 0.072794
2022-01-11 22:20:20,419 iteration 259 : loss : 0.153434, loss_ce: 0.074029
2022-01-11 22:20:21,954 iteration 260 : loss : 0.198383, loss_ce: 0.090662
2022-01-11 22:20:23,444 iteration 261 : loss : 0.245245, loss_ce: 0.091686
2022-01-11 22:20:25,030 iteration 262 : loss : 0.232815, loss_ce: 0.101494
2022-01-11 22:20:26,513 iteration 263 : loss : 0.246410, loss_ce: 0.108177
2022-01-11 22:20:28,032 iteration 264 : loss : 0.243936, loss_ce: 0.125196
2022-01-11 22:20:29,557 iteration 265 : loss : 0.159838, loss_ce: 0.074860
2022-01-11 22:20:31,169 iteration 266 : loss : 0.223010, loss_ce: 0.105989
2022-01-11 22:20:32,746 iteration 267 : loss : 0.246330, loss_ce: 0.085285
2022-01-11 22:20:34,335 iteration 268 : loss : 0.235461, loss_ce: 0.102760
2022-01-11 22:20:36,104 iteration 269 : loss : 0.250436, loss_ce: 0.079119
2022-01-11 22:20:37,789 iteration 270 : loss : 0.159970, loss_ce: 0.061893
2022-01-11 22:20:39,248 iteration 271 : loss : 0.192228, loss_ce: 0.049665
2022-01-11 22:20:40,829 iteration 272 : loss : 0.230530, loss_ce: 0.101899
  4%|█▏                            | 16/400 [07:44<3:16:41, 30.73s/it]2022-01-11 22:20:42,401 iteration 273 : loss : 0.188378, loss_ce: 0.069465
2022-01-11 22:20:43,919 iteration 274 : loss : 0.143654, loss_ce: 0.061080
2022-01-11 22:20:45,451 iteration 275 : loss : 0.156967, loss_ce: 0.065846
2022-01-11 22:20:47,039 iteration 276 : loss : 0.155173, loss_ce: 0.075555
2022-01-11 22:20:48,645 iteration 277 : loss : 0.171636, loss_ce: 0.066537
2022-01-11 22:20:50,092 iteration 278 : loss : 0.223374, loss_ce: 0.082121
2022-01-11 22:20:51,609 iteration 279 : loss : 0.216844, loss_ce: 0.081493
2022-01-11 22:20:53,118 iteration 280 : loss : 0.206921, loss_ce: 0.091179
2022-01-11 22:20:54,688 iteration 281 : loss : 0.186746, loss_ce: 0.081753
2022-01-11 22:20:56,232 iteration 282 : loss : 0.256129, loss_ce: 0.104723
2022-01-11 22:20:57,761 iteration 283 : loss : 0.211034, loss_ce: 0.106397
2022-01-11 22:20:59,386 iteration 284 : loss : 0.223296, loss_ce: 0.108922
2022-01-11 22:21:00,889 iteration 285 : loss : 0.207819, loss_ce: 0.079758
2022-01-11 22:21:02,414 iteration 286 : loss : 0.177431, loss_ce: 0.082412
2022-01-11 22:21:04,040 iteration 287 : loss : 0.243085, loss_ce: 0.116454
2022-01-11 22:21:05,655 iteration 288 : loss : 0.223306, loss_ce: 0.092851
2022-01-11 22:21:07,197 iteration 289 : loss : 0.184407, loss_ce: 0.073961
  4%|█▎                            | 17/400 [08:11<3:07:47, 29.42s/it]2022-01-11 22:21:08,812 iteration 290 : loss : 0.174125, loss_ce: 0.071088
2022-01-11 22:21:10,337 iteration 291 : loss : 0.213465, loss_ce: 0.084619
2022-01-11 22:21:11,870 iteration 292 : loss : 0.174796, loss_ce: 0.072175
2022-01-11 22:21:13,412 iteration 293 : loss : 0.243469, loss_ce: 0.115028
2022-01-11 22:21:14,974 iteration 294 : loss : 0.184288, loss_ce: 0.077060
2022-01-11 22:21:16,633 iteration 295 : loss : 0.223852, loss_ce: 0.080793
2022-01-11 22:21:18,172 iteration 296 : loss : 0.196027, loss_ce: 0.071031
2022-01-11 22:21:19,734 iteration 297 : loss : 0.184078, loss_ce: 0.070476
2022-01-11 22:21:21,266 iteration 298 : loss : 0.166247, loss_ce: 0.063694
2022-01-11 22:21:22,821 iteration 299 : loss : 0.159002, loss_ce: 0.060066
2022-01-11 22:21:24,387 iteration 300 : loss : 0.197067, loss_ce: 0.069858
2022-01-11 22:21:25,963 iteration 301 : loss : 0.260163, loss_ce: 0.151494
2022-01-11 22:21:27,581 iteration 302 : loss : 0.146909, loss_ce: 0.063160
2022-01-11 22:21:29,177 iteration 303 : loss : 0.203541, loss_ce: 0.084795
2022-01-11 22:21:30,759 iteration 304 : loss : 0.242497, loss_ce: 0.117455
2022-01-11 22:21:32,377 iteration 305 : loss : 0.191849, loss_ce: 0.085191
2022-01-11 22:21:33,990 iteration 306 : loss : 0.158735, loss_ce: 0.083962
  4%|█▎                            | 18/400 [08:38<3:02:17, 28.63s/it]2022-01-11 22:21:35,610 iteration 307 : loss : 0.205664, loss_ce: 0.082088
2022-01-11 22:21:37,136 iteration 308 : loss : 0.204338, loss_ce: 0.102502
2022-01-11 22:21:38,720 iteration 309 : loss : 0.286911, loss_ce: 0.121797
2022-01-11 22:21:40,332 iteration 310 : loss : 0.215264, loss_ce: 0.087529
2022-01-11 22:21:41,856 iteration 311 : loss : 0.191440, loss_ce: 0.062417
2022-01-11 22:21:43,418 iteration 312 : loss : 0.174601, loss_ce: 0.083133
2022-01-11 22:21:44,994 iteration 313 : loss : 0.195624, loss_ce: 0.091365
2022-01-11 22:21:46,497 iteration 314 : loss : 0.186281, loss_ce: 0.084883
2022-01-11 22:21:48,047 iteration 315 : loss : 0.229717, loss_ce: 0.090589
2022-01-11 22:21:49,620 iteration 316 : loss : 0.208773, loss_ce: 0.076918
2022-01-11 22:21:51,188 iteration 317 : loss : 0.248180, loss_ce: 0.104051
2022-01-11 22:21:52,767 iteration 318 : loss : 0.168533, loss_ce: 0.075653
2022-01-11 22:21:54,275 iteration 319 : loss : 0.175166, loss_ce: 0.074818
2022-01-11 22:21:55,910 iteration 320 : loss : 0.180013, loss_ce: 0.068212
2022-01-11 22:21:57,508 iteration 321 : loss : 0.157658, loss_ce: 0.052134
2022-01-11 22:21:59,061 iteration 322 : loss : 0.318556, loss_ce: 0.171135
2022-01-11 22:22:00,682 iteration 323 : loss : 0.182701, loss_ce: 0.061938
  5%|█▍                            | 19/400 [09:04<2:58:06, 28.05s/it]2022-01-11 22:22:02,276 iteration 324 : loss : 0.211491, loss_ce: 0.097301
2022-01-11 22:22:03,849 iteration 325 : loss : 0.260949, loss_ce: 0.094455
2022-01-11 22:22:05,438 iteration 326 : loss : 0.184395, loss_ce: 0.088633
2022-01-11 22:22:07,104 iteration 327 : loss : 0.203643, loss_ce: 0.092593
2022-01-11 22:22:08,655 iteration 328 : loss : 0.176368, loss_ce: 0.078494
2022-01-11 22:22:10,247 iteration 329 : loss : 0.210347, loss_ce: 0.109260
2022-01-11 22:22:11,861 iteration 330 : loss : 0.210414, loss_ce: 0.092633
2022-01-11 22:22:13,377 iteration 331 : loss : 0.209738, loss_ce: 0.104990
2022-01-11 22:22:14,924 iteration 332 : loss : 0.180477, loss_ce: 0.063366
2022-01-11 22:22:16,507 iteration 333 : loss : 0.136251, loss_ce: 0.060340
2022-01-11 22:22:18,074 iteration 334 : loss : 0.196002, loss_ce: 0.085888
2022-01-11 22:22:19,616 iteration 335 : loss : 0.223528, loss_ce: 0.079989
2022-01-11 22:22:21,243 iteration 336 : loss : 0.163982, loss_ce: 0.062716
2022-01-11 22:22:22,808 iteration 337 : loss : 0.202912, loss_ce: 0.071448
2022-01-11 22:22:24,336 iteration 338 : loss : 0.220814, loss_ce: 0.121671
2022-01-11 22:22:25,862 iteration 339 : loss : 0.197444, loss_ce: 0.070190
2022-01-11 22:22:25,863 Training Data Eval:
2022-01-11 22:22:33,818   Average segmentation loss on training set: 0.2002
2022-01-11 22:22:33,819 Validation Data Eval:
2022-01-11 22:22:36,557   Average segmentation loss on validation set: 0.2391
2022-01-11 22:22:38,171 iteration 340 : loss : 0.202204, loss_ce: 0.084382
  5%|█▌                            | 20/400 [09:42<3:15:34, 30.88s/it]2022-01-11 22:22:39,822 iteration 341 : loss : 0.147795, loss_ce: 0.063438
2022-01-11 22:22:41,415 iteration 342 : loss : 0.166981, loss_ce: 0.056248
2022-01-11 22:22:42,983 iteration 343 : loss : 0.181502, loss_ce: 0.071577
2022-01-11 22:22:44,581 iteration 344 : loss : 0.205490, loss_ce: 0.091995
2022-01-11 22:22:46,138 iteration 345 : loss : 0.145661, loss_ce: 0.049624
2022-01-11 22:22:47,765 iteration 346 : loss : 0.194623, loss_ce: 0.079568
2022-01-11 22:22:49,284 iteration 347 : loss : 0.188220, loss_ce: 0.087239
2022-01-11 22:22:50,873 iteration 348 : loss : 0.185483, loss_ce: 0.076038
2022-01-11 22:22:52,563 iteration 349 : loss : 0.182516, loss_ce: 0.092273
2022-01-11 22:22:54,120 iteration 350 : loss : 0.150109, loss_ce: 0.059644
2022-01-11 22:22:55,735 iteration 351 : loss : 0.182144, loss_ce: 0.091002
2022-01-11 22:22:57,328 iteration 352 : loss : 0.162795, loss_ce: 0.066269
2022-01-11 22:22:58,847 iteration 353 : loss : 0.185475, loss_ce: 0.067546
2022-01-11 22:23:00,414 iteration 354 : loss : 0.204547, loss_ce: 0.079525
2022-01-11 22:23:02,031 iteration 355 : loss : 0.208453, loss_ce: 0.084688
2022-01-11 22:23:03,711 iteration 356 : loss : 0.171918, loss_ce: 0.083462
2022-01-11 22:23:05,217 iteration 357 : loss : 0.139812, loss_ce: 0.055828
  5%|█▌                            | 21/400 [10:09<3:07:47, 29.73s/it]2022-01-11 22:23:06,845 iteration 358 : loss : 0.190705, loss_ce: 0.086364
2022-01-11 22:23:08,470 iteration 359 : loss : 0.199058, loss_ce: 0.079126
2022-01-11 22:23:09,988 iteration 360 : loss : 0.128640, loss_ce: 0.048170
2022-01-11 22:23:11,587 iteration 361 : loss : 0.212105, loss_ce: 0.080932
2022-01-11 22:23:13,227 iteration 362 : loss : 0.172680, loss_ce: 0.055692
2022-01-11 22:23:14,869 iteration 363 : loss : 0.238248, loss_ce: 0.093013
2022-01-11 22:23:16,455 iteration 364 : loss : 0.155291, loss_ce: 0.063558
2022-01-11 22:23:17,935 iteration 365 : loss : 0.173443, loss_ce: 0.083133
2022-01-11 22:23:19,523 iteration 366 : loss : 0.189239, loss_ce: 0.087189
2022-01-11 22:23:21,016 iteration 367 : loss : 0.164937, loss_ce: 0.069823
2022-01-11 22:23:22,622 iteration 368 : loss : 0.187676, loss_ce: 0.072801
2022-01-11 22:23:24,257 iteration 369 : loss : 0.189143, loss_ce: 0.067134
2022-01-11 22:23:25,867 iteration 370 : loss : 0.164144, loss_ce: 0.079085
2022-01-11 22:23:27,445 iteration 371 : loss : 0.203679, loss_ce: 0.071129
2022-01-11 22:23:29,036 iteration 372 : loss : 0.226641, loss_ce: 0.117493
2022-01-11 22:23:30,547 iteration 373 : loss : 0.160701, loss_ce: 0.056216
2022-01-11 22:23:32,097 iteration 374 : loss : 0.171050, loss_ce: 0.065446
  6%|█▋                            | 22/400 [10:36<3:01:55, 28.88s/it]2022-01-11 22:23:33,747 iteration 375 : loss : 0.265019, loss_ce: 0.116135
2022-01-11 22:23:35,255 iteration 376 : loss : 0.112169, loss_ce: 0.039232
2022-01-11 22:23:36,812 iteration 377 : loss : 0.156603, loss_ce: 0.055346
2022-01-11 22:23:38,444 iteration 378 : loss : 0.125088, loss_ce: 0.049102
2022-01-11 22:23:39,934 iteration 379 : loss : 0.133334, loss_ce: 0.059266
2022-01-11 22:23:41,525 iteration 380 : loss : 0.143273, loss_ce: 0.052948
2022-01-11 22:23:43,089 iteration 381 : loss : 0.143264, loss_ce: 0.052273
2022-01-11 22:23:44,580 iteration 382 : loss : 0.149534, loss_ce: 0.068173
2022-01-11 22:23:46,137 iteration 383 : loss : 0.175173, loss_ce: 0.072374
2022-01-11 22:23:47,726 iteration 384 : loss : 0.130975, loss_ce: 0.056998
2022-01-11 22:23:49,284 iteration 385 : loss : 0.129512, loss_ce: 0.053946
2022-01-11 22:23:50,858 iteration 386 : loss : 0.171032, loss_ce: 0.058590
2022-01-11 22:23:52,446 iteration 387 : loss : 0.199226, loss_ce: 0.065365
2022-01-11 22:23:54,047 iteration 388 : loss : 0.201280, loss_ce: 0.084989
2022-01-11 22:23:55,639 iteration 389 : loss : 0.241947, loss_ce: 0.125305
2022-01-11 22:23:57,185 iteration 390 : loss : 0.122803, loss_ce: 0.052145
2022-01-11 22:23:58,736 iteration 391 : loss : 0.215544, loss_ce: 0.131787
  6%|█▋                            | 23/400 [11:02<2:57:12, 28.20s/it]2022-01-11 22:24:00,364 iteration 392 : loss : 0.148590, loss_ce: 0.075744
2022-01-11 22:24:01,891 iteration 393 : loss : 0.137397, loss_ce: 0.054966
2022-01-11 22:24:03,500 iteration 394 : loss : 0.142665, loss_ce: 0.056835
2022-01-11 22:24:05,089 iteration 395 : loss : 0.165679, loss_ce: 0.078482
2022-01-11 22:24:06,614 iteration 396 : loss : 0.155000, loss_ce: 0.064356
2022-01-11 22:24:08,185 iteration 397 : loss : 0.192719, loss_ce: 0.089504
2022-01-11 22:24:09,772 iteration 398 : loss : 0.124545, loss_ce: 0.052570
2022-01-11 22:24:11,243 iteration 399 : loss : 0.156665, loss_ce: 0.068581
2022-01-11 22:24:12,813 iteration 400 : loss : 0.148397, loss_ce: 0.057206
2022-01-11 22:24:14,434 iteration 401 : loss : 0.173182, loss_ce: 0.075098
2022-01-11 22:24:15,982 iteration 402 : loss : 0.104564, loss_ce: 0.049683
2022-01-11 22:24:17,524 iteration 403 : loss : 0.145150, loss_ce: 0.061041
2022-01-11 22:24:19,114 iteration 404 : loss : 0.176199, loss_ce: 0.073160
2022-01-11 22:24:20,745 iteration 405 : loss : 0.167509, loss_ce: 0.071321
2022-01-11 22:24:22,290 iteration 406 : loss : 0.136661, loss_ce: 0.055338
2022-01-11 22:24:23,790 iteration 407 : loss : 0.137894, loss_ce: 0.053837
2022-01-11 22:24:25,343 iteration 408 : loss : 0.183538, loss_ce: 0.072773
  6%|█▊                            | 24/400 [11:29<2:53:44, 27.73s/it]2022-01-11 22:24:26,976 iteration 409 : loss : 0.148263, loss_ce: 0.074346
2022-01-11 22:24:28,564 iteration 410 : loss : 0.161974, loss_ce: 0.061864
2022-01-11 22:24:30,124 iteration 411 : loss : 0.287500, loss_ce: 0.085757
2022-01-11 22:24:31,601 iteration 412 : loss : 0.126536, loss_ce: 0.051133
2022-01-11 22:24:33,159 iteration 413 : loss : 0.159420, loss_ce: 0.064208
2022-01-11 22:24:34,666 iteration 414 : loss : 0.205984, loss_ce: 0.104996
2022-01-11 22:24:36,127 iteration 415 : loss : 0.132180, loss_ce: 0.052259
2022-01-11 22:24:37,676 iteration 416 : loss : 0.165318, loss_ce: 0.079908
2022-01-11 22:24:39,200 iteration 417 : loss : 0.212649, loss_ce: 0.082515
2022-01-11 22:24:40,817 iteration 418 : loss : 0.141308, loss_ce: 0.062569
2022-01-11 22:24:42,349 iteration 419 : loss : 0.185462, loss_ce: 0.110054
2022-01-11 22:24:43,923 iteration 420 : loss : 0.195454, loss_ce: 0.080223
2022-01-11 22:24:45,449 iteration 421 : loss : 0.182454, loss_ce: 0.078671
2022-01-11 22:24:46,964 iteration 422 : loss : 0.190486, loss_ce: 0.080932
2022-01-11 22:24:48,489 iteration 423 : loss : 0.171376, loss_ce: 0.059095
2022-01-11 22:24:50,063 iteration 424 : loss : 0.163569, loss_ce: 0.068249
2022-01-11 22:24:50,063 Training Data Eval:
2022-01-11 22:24:58,027   Average segmentation loss on training set: 0.1424
2022-01-11 22:24:58,028 Validation Data Eval:
2022-01-11 22:25:00,771   Average segmentation loss on validation set: 0.2100
2022-01-11 22:25:02,312 iteration 425 : loss : 0.176991, loss_ce: 0.070310
  6%|█▉                            | 25/400 [12:06<3:10:36, 30.50s/it]2022-01-11 22:25:03,869 iteration 426 : loss : 0.155802, loss_ce: 0.050294
2022-01-11 22:25:05,464 iteration 427 : loss : 0.152870, loss_ce: 0.052015
2022-01-11 22:25:07,032 iteration 428 : loss : 0.173029, loss_ce: 0.065007
2022-01-11 22:25:08,552 iteration 429 : loss : 0.251059, loss_ce: 0.137288
2022-01-11 22:25:10,066 iteration 430 : loss : 0.149013, loss_ce: 0.055778
2022-01-11 22:25:11,601 iteration 431 : loss : 0.242011, loss_ce: 0.103041
2022-01-11 22:25:13,188 iteration 432 : loss : 0.118605, loss_ce: 0.047586
2022-01-11 22:25:14,808 iteration 433 : loss : 0.112154, loss_ce: 0.046979
2022-01-11 22:25:16,381 iteration 434 : loss : 0.118723, loss_ce: 0.051095
2022-01-11 22:25:17,883 iteration 435 : loss : 0.141731, loss_ce: 0.060722
2022-01-11 22:25:19,425 iteration 436 : loss : 0.123116, loss_ce: 0.056691
2022-01-11 22:25:20,970 iteration 437 : loss : 0.204708, loss_ce: 0.106275
2022-01-11 22:25:22,472 iteration 438 : loss : 0.147967, loss_ce: 0.055387
2022-01-11 22:25:24,034 iteration 439 : loss : 0.176965, loss_ce: 0.061540
2022-01-11 22:25:25,579 iteration 440 : loss : 0.181400, loss_ce: 0.076836
2022-01-11 22:25:27,176 iteration 441 : loss : 0.145065, loss_ce: 0.058610
2022-01-11 22:25:28,759 iteration 442 : loss : 0.135893, loss_ce: 0.066397
  6%|█▉                            | 26/400 [12:32<3:02:32, 29.28s/it]2022-01-11 22:25:30,334 iteration 443 : loss : 0.162900, loss_ce: 0.083340
2022-01-11 22:25:31,844 iteration 444 : loss : 0.128476, loss_ce: 0.053182
2022-01-11 22:25:33,394 iteration 445 : loss : 0.173136, loss_ce: 0.056933
2022-01-11 22:25:35,064 iteration 446 : loss : 0.166943, loss_ce: 0.068809
2022-01-11 22:25:36,676 iteration 447 : loss : 0.163665, loss_ce: 0.086960
2022-01-11 22:25:38,268 iteration 448 : loss : 0.258196, loss_ce: 0.077829
2022-01-11 22:25:39,827 iteration 449 : loss : 0.172691, loss_ce: 0.084768
2022-01-11 22:25:41,365 iteration 450 : loss : 0.195037, loss_ce: 0.085657
2022-01-11 22:25:42,991 iteration 451 : loss : 0.106726, loss_ce: 0.045484
2022-01-11 22:25:44,580 iteration 452 : loss : 0.144540, loss_ce: 0.059922
2022-01-11 22:25:46,160 iteration 453 : loss : 0.188817, loss_ce: 0.082758
2022-01-11 22:25:47,730 iteration 454 : loss : 0.233910, loss_ce: 0.087872
2022-01-11 22:25:49,340 iteration 455 : loss : 0.120875, loss_ce: 0.051024
2022-01-11 22:25:50,917 iteration 456 : loss : 0.145174, loss_ce: 0.057178
2022-01-11 22:25:52,484 iteration 457 : loss : 0.177963, loss_ce: 0.073018
2022-01-11 22:25:54,117 iteration 458 : loss : 0.178221, loss_ce: 0.085003
2022-01-11 22:25:55,615 iteration 459 : loss : 0.120889, loss_ce: 0.045527
  7%|██                            | 27/400 [12:59<2:57:30, 28.55s/it]2022-01-11 22:25:57,233 iteration 460 : loss : 0.169236, loss_ce: 0.086931
2022-01-11 22:25:58,805 iteration 461 : loss : 0.141796, loss_ce: 0.062936
2022-01-11 22:26:00,355 iteration 462 : loss : 0.128566, loss_ce: 0.056379
2022-01-11 22:26:01,964 iteration 463 : loss : 0.155814, loss_ce: 0.068257
2022-01-11 22:26:03,520 iteration 464 : loss : 0.116278, loss_ce: 0.046473
2022-01-11 22:26:05,031 iteration 465 : loss : 0.163119, loss_ce: 0.058599
2022-01-11 22:26:06,575 iteration 466 : loss : 0.119124, loss_ce: 0.042914
2022-01-11 22:26:08,116 iteration 467 : loss : 0.156004, loss_ce: 0.059559
2022-01-11 22:26:09,773 iteration 468 : loss : 0.154679, loss_ce: 0.067978
2022-01-11 22:26:11,313 iteration 469 : loss : 0.133120, loss_ce: 0.048147
2022-01-11 22:26:12,857 iteration 470 : loss : 0.148646, loss_ce: 0.056022
2022-01-11 22:26:14,400 iteration 471 : loss : 0.153482, loss_ce: 0.052519
2022-01-11 22:26:16,007 iteration 472 : loss : 0.136094, loss_ce: 0.058080
2022-01-11 22:26:17,693 iteration 473 : loss : 0.178133, loss_ce: 0.074536
2022-01-11 22:26:19,329 iteration 474 : loss : 0.177142, loss_ce: 0.075089
2022-01-11 22:26:20,859 iteration 475 : loss : 0.096191, loss_ce: 0.039692
2022-01-11 22:26:22,475 iteration 476 : loss : 0.154212, loss_ce: 0.081588
  7%|██                            | 28/400 [13:26<2:53:53, 28.05s/it]2022-01-11 22:26:24,065 iteration 477 : loss : 0.155773, loss_ce: 0.053741
2022-01-11 22:26:25,650 iteration 478 : loss : 0.101469, loss_ce: 0.036935
2022-01-11 22:26:27,192 iteration 479 : loss : 0.137509, loss_ce: 0.064725
2022-01-11 22:26:28,768 iteration 480 : loss : 0.128303, loss_ce: 0.061990
2022-01-11 22:26:30,258 iteration 481 : loss : 0.171049, loss_ce: 0.071893
2022-01-11 22:26:31,832 iteration 482 : loss : 0.146087, loss_ce: 0.039069
2022-01-11 22:26:33,423 iteration 483 : loss : 0.150197, loss_ce: 0.058740
2022-01-11 22:26:35,020 iteration 484 : loss : 0.208127, loss_ce: 0.101889
2022-01-11 22:26:36,559 iteration 485 : loss : 0.115869, loss_ce: 0.055022
2022-01-11 22:26:38,099 iteration 486 : loss : 0.148951, loss_ce: 0.047740
2022-01-11 22:26:39,705 iteration 487 : loss : 0.114025, loss_ce: 0.044565
2022-01-11 22:26:41,202 iteration 488 : loss : 0.121644, loss_ce: 0.048108
2022-01-11 22:26:42,770 iteration 489 : loss : 0.145382, loss_ce: 0.067668
2022-01-11 22:26:44,233 iteration 490 : loss : 0.131328, loss_ce: 0.052533
2022-01-11 22:26:45,802 iteration 491 : loss : 0.139341, loss_ce: 0.061417
2022-01-11 22:26:47,341 iteration 492 : loss : 0.096378, loss_ce: 0.048681
2022-01-11 22:26:48,931 iteration 493 : loss : 0.135680, loss_ce: 0.064613
  7%|██▏                           | 29/400 [13:53<2:50:28, 27.57s/it]2022-01-11 22:26:50,513 iteration 494 : loss : 0.132260, loss_ce: 0.060126
2022-01-11 22:26:52,105 iteration 495 : loss : 0.124279, loss_ce: 0.042829
2022-01-11 22:26:53,656 iteration 496 : loss : 0.188086, loss_ce: 0.087984
2022-01-11 22:26:55,233 iteration 497 : loss : 0.143507, loss_ce: 0.067277
2022-01-11 22:26:56,839 iteration 498 : loss : 0.162757, loss_ce: 0.073387
2022-01-11 22:26:58,335 iteration 499 : loss : 0.105038, loss_ce: 0.048003
2022-01-11 22:26:59,857 iteration 500 : loss : 0.131603, loss_ce: 0.053432
2022-01-11 22:27:01,758 iteration 501 : loss : 0.135165, loss_ce: 0.056935
2022-01-11 22:27:03,343 iteration 502 : loss : 0.149670, loss_ce: 0.065244
2022-01-11 22:27:04,862 iteration 503 : loss : 0.157960, loss_ce: 0.052997
2022-01-11 22:27:06,449 iteration 504 : loss : 0.102504, loss_ce: 0.036492
2022-01-11 22:27:07,961 iteration 505 : loss : 0.119808, loss_ce: 0.048512
2022-01-11 22:27:09,601 iteration 506 : loss : 0.114046, loss_ce: 0.047811
2022-01-11 22:27:11,170 iteration 507 : loss : 0.185078, loss_ce: 0.072736
2022-01-11 22:27:12,738 iteration 508 : loss : 0.166383, loss_ce: 0.073844
2022-01-11 22:27:14,334 iteration 509 : loss : 0.129131, loss_ce: 0.051031
2022-01-11 22:27:14,335 Training Data Eval:
2022-01-11 22:27:22,276   Average segmentation loss on training set: 0.1077
2022-01-11 22:27:22,276 Validation Data Eval:
2022-01-11 22:27:25,018   Average segmentation loss on validation set: 0.1710
2022-01-11 22:27:30,926 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 22:27:32,452 iteration 510 : loss : 0.114064, loss_ce: 0.053070
  8%|██▎                           | 30/400 [14:36<3:19:30, 32.35s/it]2022-01-11 22:27:33,973 iteration 511 : loss : 0.179819, loss_ce: 0.081922
2022-01-11 22:27:35,395 iteration 512 : loss : 0.182186, loss_ce: 0.075296
2022-01-11 22:27:36,827 iteration 513 : loss : 0.088902, loss_ce: 0.030145
2022-01-11 22:27:38,279 iteration 514 : loss : 0.110595, loss_ce: 0.049261
2022-01-11 22:27:39,721 iteration 515 : loss : 0.152847, loss_ce: 0.065960
2022-01-11 22:27:41,209 iteration 516 : loss : 0.115863, loss_ce: 0.045866
2022-01-11 22:27:42,727 iteration 517 : loss : 0.156948, loss_ce: 0.073496
2022-01-11 22:27:44,140 iteration 518 : loss : 0.160031, loss_ce: 0.058891
2022-01-11 22:27:45,606 iteration 519 : loss : 0.159242, loss_ce: 0.075095
2022-01-11 22:27:47,079 iteration 520 : loss : 0.113640, loss_ce: 0.049537
2022-01-11 22:27:48,643 iteration 521 : loss : 0.168536, loss_ce: 0.066878
2022-01-11 22:27:50,247 iteration 522 : loss : 0.096143, loss_ce: 0.036024
2022-01-11 22:27:51,884 iteration 523 : loss : 0.130775, loss_ce: 0.058539
2022-01-11 22:27:53,549 iteration 524 : loss : 0.103564, loss_ce: 0.040174
2022-01-11 22:27:55,123 iteration 525 : loss : 0.174346, loss_ce: 0.080064
2022-01-11 22:27:56,686 iteration 526 : loss : 0.130463, loss_ce: 0.067668
2022-01-11 22:27:58,247 iteration 527 : loss : 0.128661, loss_ce: 0.047751
  8%|██▎                           | 31/400 [15:02<3:06:52, 30.39s/it]2022-01-11 22:27:59,832 iteration 528 : loss : 0.113342, loss_ce: 0.050571
2022-01-11 22:28:01,406 iteration 529 : loss : 0.141948, loss_ce: 0.044365
2022-01-11 22:28:02,931 iteration 530 : loss : 0.143158, loss_ce: 0.053976
2022-01-11 22:28:04,449 iteration 531 : loss : 0.121528, loss_ce: 0.039039
2022-01-11 22:28:05,983 iteration 532 : loss : 0.142293, loss_ce: 0.072382
2022-01-11 22:28:07,636 iteration 533 : loss : 0.085363, loss_ce: 0.035403
2022-01-11 22:28:09,150 iteration 534 : loss : 0.106866, loss_ce: 0.050085
2022-01-11 22:28:10,752 iteration 535 : loss : 0.104159, loss_ce: 0.037883
2022-01-11 22:28:12,384 iteration 536 : loss : 0.135034, loss_ce: 0.057616
2022-01-11 22:28:14,024 iteration 537 : loss : 0.127873, loss_ce: 0.045843
2022-01-11 22:28:15,612 iteration 538 : loss : 0.121703, loss_ce: 0.051603
2022-01-11 22:28:17,165 iteration 539 : loss : 0.119782, loss_ce: 0.049256
2022-01-11 22:28:18,748 iteration 540 : loss : 0.129268, loss_ce: 0.046661
2022-01-11 22:28:20,296 iteration 541 : loss : 0.082677, loss_ce: 0.036023
2022-01-11 22:28:21,859 iteration 542 : loss : 0.148958, loss_ce: 0.069988
2022-01-11 22:28:23,374 iteration 543 : loss : 0.100833, loss_ce: 0.045547
2022-01-11 22:28:24,915 iteration 544 : loss : 0.118573, loss_ce: 0.046431
  8%|██▍                           | 32/400 [15:29<2:59:31, 29.27s/it]2022-01-11 22:28:26,536 iteration 545 : loss : 0.095492, loss_ce: 0.047936
2022-01-11 22:28:28,097 iteration 546 : loss : 0.113794, loss_ce: 0.042557
2022-01-11 22:28:29,659 iteration 547 : loss : 0.116732, loss_ce: 0.056346
2022-01-11 22:28:31,207 iteration 548 : loss : 0.164857, loss_ce: 0.062447
2022-01-11 22:28:32,794 iteration 549 : loss : 0.115194, loss_ce: 0.051551
2022-01-11 22:28:34,331 iteration 550 : loss : 0.108394, loss_ce: 0.048180
2022-01-11 22:28:35,892 iteration 551 : loss : 0.119037, loss_ce: 0.053238
2022-01-11 22:28:37,431 iteration 552 : loss : 0.108860, loss_ce: 0.046291
2022-01-11 22:28:39,023 iteration 553 : loss : 0.110843, loss_ce: 0.043676
2022-01-11 22:28:40,529 iteration 554 : loss : 0.122810, loss_ce: 0.058181
2022-01-11 22:28:42,084 iteration 555 : loss : 0.109076, loss_ce: 0.029703
2022-01-11 22:28:43,689 iteration 556 : loss : 0.134161, loss_ce: 0.061705
2022-01-11 22:28:45,179 iteration 557 : loss : 0.126088, loss_ce: 0.041830
2022-01-11 22:28:46,786 iteration 558 : loss : 0.152652, loss_ce: 0.049310
2022-01-11 22:28:48,363 iteration 559 : loss : 0.120229, loss_ce: 0.047761
2022-01-11 22:28:49,951 iteration 560 : loss : 0.122706, loss_ce: 0.033610
2022-01-11 22:28:51,558 iteration 561 : loss : 0.157900, loss_ce: 0.079230
  8%|██▍                           | 33/400 [15:55<2:54:13, 28.48s/it]2022-01-11 22:28:53,274 iteration 562 : loss : 0.147891, loss_ce: 0.070911
2022-01-11 22:28:54,748 iteration 563 : loss : 0.109377, loss_ce: 0.050872
2022-01-11 22:28:56,347 iteration 564 : loss : 0.120034, loss_ce: 0.058764
2022-01-11 22:28:57,930 iteration 565 : loss : 0.112293, loss_ce: 0.047982
2022-01-11 22:28:59,555 iteration 566 : loss : 0.107744, loss_ce: 0.050345
2022-01-11 22:29:01,099 iteration 567 : loss : 0.121514, loss_ce: 0.048719
2022-01-11 22:29:02,638 iteration 568 : loss : 0.180341, loss_ce: 0.067621
2022-01-11 22:29:04,199 iteration 569 : loss : 0.151631, loss_ce: 0.063384
2022-01-11 22:29:05,798 iteration 570 : loss : 0.080780, loss_ce: 0.039300
2022-01-11 22:29:07,353 iteration 571 : loss : 0.192965, loss_ce: 0.074944
2022-01-11 22:29:08,932 iteration 572 : loss : 0.100258, loss_ce: 0.037944
2022-01-11 22:29:10,464 iteration 573 : loss : 0.113890, loss_ce: 0.046618
2022-01-11 22:29:12,025 iteration 574 : loss : 0.142975, loss_ce: 0.055982
2022-01-11 22:29:13,638 iteration 575 : loss : 0.133856, loss_ce: 0.046617
2022-01-11 22:29:15,234 iteration 576 : loss : 0.124278, loss_ce: 0.053294
2022-01-11 22:29:16,800 iteration 577 : loss : 0.086600, loss_ce: 0.037959
2022-01-11 22:29:18,320 iteration 578 : loss : 0.113288, loss_ce: 0.047302
  8%|██▌                           | 34/400 [16:22<2:50:35, 27.97s/it]2022-01-11 22:29:20,078 iteration 579 : loss : 0.119467, loss_ce: 0.060231
2022-01-11 22:29:21,646 iteration 580 : loss : 0.151446, loss_ce: 0.060120
2022-01-11 22:29:23,259 iteration 581 : loss : 0.124571, loss_ce: 0.038982
2022-01-11 22:29:24,819 iteration 582 : loss : 0.126271, loss_ce: 0.053888
2022-01-11 22:29:26,406 iteration 583 : loss : 0.190833, loss_ce: 0.060456
2022-01-11 22:29:27,941 iteration 584 : loss : 0.084440, loss_ce: 0.037121
2022-01-11 22:29:29,560 iteration 585 : loss : 0.132185, loss_ce: 0.052657
2022-01-11 22:29:31,093 iteration 586 : loss : 0.154919, loss_ce: 0.063878
2022-01-11 22:29:32,610 iteration 587 : loss : 0.094169, loss_ce: 0.044862
2022-01-11 22:29:34,198 iteration 588 : loss : 0.119193, loss_ce: 0.043949
2022-01-11 22:29:35,748 iteration 589 : loss : 0.096484, loss_ce: 0.037121
2022-01-11 22:29:37,310 iteration 590 : loss : 0.128311, loss_ce: 0.045878
2022-01-11 22:29:38,877 iteration 591 : loss : 0.128621, loss_ce: 0.057934
2022-01-11 22:29:40,452 iteration 592 : loss : 0.106062, loss_ce: 0.043343
2022-01-11 22:29:42,053 iteration 593 : loss : 0.076391, loss_ce: 0.029084
2022-01-11 22:29:43,638 iteration 594 : loss : 0.110649, loss_ce: 0.045513
2022-01-11 22:29:43,638 Training Data Eval:
2022-01-11 22:29:51,601   Average segmentation loss on training set: 0.1641
2022-01-11 22:29:51,602 Validation Data Eval:
2022-01-11 22:29:54,349   Average segmentation loss on validation set: 0.2470
2022-01-11 22:29:55,986 iteration 595 : loss : 0.129572, loss_ce: 0.074744
  9%|██▋                           | 35/400 [17:00<3:07:49, 30.88s/it]2022-01-11 22:29:57,602 iteration 596 : loss : 0.089286, loss_ce: 0.037133
2022-01-11 22:29:59,143 iteration 597 : loss : 0.120913, loss_ce: 0.043112
2022-01-11 22:30:00,674 iteration 598 : loss : 0.094356, loss_ce: 0.037454
2022-01-11 22:30:02,225 iteration 599 : loss : 0.105786, loss_ce: 0.040828
2022-01-11 22:30:03,769 iteration 600 : loss : 0.137840, loss_ce: 0.062567
2022-01-11 22:30:05,347 iteration 601 : loss : 0.103164, loss_ce: 0.039164
2022-01-11 22:30:06,930 iteration 602 : loss : 0.094165, loss_ce: 0.036893
2022-01-11 22:30:08,434 iteration 603 : loss : 0.125443, loss_ce: 0.046596
2022-01-11 22:30:09,951 iteration 604 : loss : 0.132095, loss_ce: 0.051861
2022-01-11 22:30:11,450 iteration 605 : loss : 0.097376, loss_ce: 0.035811
2022-01-11 22:30:13,135 iteration 606 : loss : 0.103003, loss_ce: 0.043985
2022-01-11 22:30:14,694 iteration 607 : loss : 0.139914, loss_ce: 0.056302
2022-01-11 22:30:16,250 iteration 608 : loss : 0.108874, loss_ce: 0.052213
2022-01-11 22:30:17,739 iteration 609 : loss : 0.095927, loss_ce: 0.039814
2022-01-11 22:30:19,342 iteration 610 : loss : 0.105132, loss_ce: 0.039781
2022-01-11 22:30:21,008 iteration 611 : loss : 0.140352, loss_ce: 0.068923
2022-01-11 22:30:22,590 iteration 612 : loss : 0.085918, loss_ce: 0.028441
  9%|██▋                           | 36/400 [17:26<2:59:33, 29.60s/it]2022-01-11 22:30:24,215 iteration 613 : loss : 0.092025, loss_ce: 0.037724
2022-01-11 22:30:25,742 iteration 614 : loss : 0.104685, loss_ce: 0.040510
2022-01-11 22:30:27,269 iteration 615 : loss : 0.176538, loss_ce: 0.090406
2022-01-11 22:30:28,891 iteration 616 : loss : 0.106047, loss_ce: 0.048911
2022-01-11 22:30:30,417 iteration 617 : loss : 0.099955, loss_ce: 0.049796
2022-01-11 22:30:31,998 iteration 618 : loss : 0.149570, loss_ce: 0.043676
2022-01-11 22:30:33,566 iteration 619 : loss : 0.187824, loss_ce: 0.070959
2022-01-11 22:30:35,220 iteration 620 : loss : 0.176677, loss_ce: 0.049758
2022-01-11 22:30:36,789 iteration 621 : loss : 0.095544, loss_ce: 0.036229
2022-01-11 22:30:38,469 iteration 622 : loss : 0.117703, loss_ce: 0.046009
2022-01-11 22:30:40,116 iteration 623 : loss : 0.135288, loss_ce: 0.064370
2022-01-11 22:30:41,654 iteration 624 : loss : 0.108261, loss_ce: 0.034438
2022-01-11 22:30:43,167 iteration 625 : loss : 0.087038, loss_ce: 0.035606
2022-01-11 22:30:44,772 iteration 626 : loss : 0.085948, loss_ce: 0.032786
2022-01-11 22:30:46,372 iteration 627 : loss : 0.151650, loss_ce: 0.060857
2022-01-11 22:30:48,022 iteration 628 : loss : 0.118969, loss_ce: 0.048637
2022-01-11 22:30:49,578 iteration 629 : loss : 0.078333, loss_ce: 0.032703
  9%|██▊                           | 37/400 [17:53<2:54:18, 28.81s/it]2022-01-11 22:30:51,209 iteration 630 : loss : 0.085038, loss_ce: 0.036947
2022-01-11 22:30:52,743 iteration 631 : loss : 0.068454, loss_ce: 0.031279
2022-01-11 22:30:54,294 iteration 632 : loss : 0.137020, loss_ce: 0.056880
2022-01-11 22:30:55,928 iteration 633 : loss : 0.108920, loss_ce: 0.045666
2022-01-11 22:30:57,503 iteration 634 : loss : 0.172737, loss_ce: 0.065869
2022-01-11 22:30:59,170 iteration 635 : loss : 0.107312, loss_ce: 0.046982
2022-01-11 22:31:00,755 iteration 636 : loss : 0.142849, loss_ce: 0.063323
2022-01-11 22:31:02,261 iteration 637 : loss : 0.100391, loss_ce: 0.044061
2022-01-11 22:31:03,908 iteration 638 : loss : 0.142773, loss_ce: 0.048818
2022-01-11 22:31:05,437 iteration 639 : loss : 0.100268, loss_ce: 0.044457
2022-01-11 22:31:06,946 iteration 640 : loss : 0.136850, loss_ce: 0.043473
2022-01-11 22:31:08,467 iteration 641 : loss : 0.140623, loss_ce: 0.053562
2022-01-11 22:31:09,988 iteration 642 : loss : 0.114161, loss_ce: 0.032641
2022-01-11 22:31:11,543 iteration 643 : loss : 0.116301, loss_ce: 0.043915
2022-01-11 22:31:13,101 iteration 644 : loss : 0.111273, loss_ce: 0.053463
2022-01-11 22:31:14,623 iteration 645 : loss : 0.123169, loss_ce: 0.041037
2022-01-11 22:31:16,247 iteration 646 : loss : 0.085358, loss_ce: 0.037309
 10%|██▊                           | 38/400 [18:20<2:49:57, 28.17s/it]2022-01-11 22:31:17,838 iteration 647 : loss : 0.141394, loss_ce: 0.063538
2022-01-11 22:31:19,399 iteration 648 : loss : 0.091577, loss_ce: 0.039399
2022-01-11 22:31:20,994 iteration 649 : loss : 0.112124, loss_ce: 0.050092
2022-01-11 22:31:22,621 iteration 650 : loss : 0.095445, loss_ce: 0.037275
2022-01-11 22:31:24,177 iteration 651 : loss : 0.070594, loss_ce: 0.028897
2022-01-11 22:31:25,646 iteration 652 : loss : 0.078408, loss_ce: 0.029840
2022-01-11 22:31:27,262 iteration 653 : loss : 0.118868, loss_ce: 0.045193
2022-01-11 22:31:28,772 iteration 654 : loss : 0.094616, loss_ce: 0.037456
2022-01-11 22:31:30,343 iteration 655 : loss : 0.134830, loss_ce: 0.050117
2022-01-11 22:31:31,914 iteration 656 : loss : 0.109420, loss_ce: 0.041663
2022-01-11 22:31:33,545 iteration 657 : loss : 0.064526, loss_ce: 0.023250
2022-01-11 22:31:35,135 iteration 658 : loss : 0.108936, loss_ce: 0.034387
2022-01-11 22:31:36,728 iteration 659 : loss : 0.091955, loss_ce: 0.037193
2022-01-11 22:31:38,267 iteration 660 : loss : 0.101599, loss_ce: 0.047031
2022-01-11 22:31:39,850 iteration 661 : loss : 0.110541, loss_ce: 0.046062
2022-01-11 22:31:41,497 iteration 662 : loss : 0.148275, loss_ce: 0.035292
2022-01-11 22:31:43,062 iteration 663 : loss : 0.120672, loss_ce: 0.054125
 10%|██▉                           | 39/400 [18:47<2:47:01, 27.76s/it]2022-01-11 22:31:44,695 iteration 664 : loss : 0.102200, loss_ce: 0.046005
2022-01-11 22:31:46,191 iteration 665 : loss : 0.099815, loss_ce: 0.045069
2022-01-11 22:31:47,747 iteration 666 : loss : 0.073417, loss_ce: 0.026574
2022-01-11 22:31:49,352 iteration 667 : loss : 0.059896, loss_ce: 0.025378
2022-01-11 22:31:50,929 iteration 668 : loss : 0.124439, loss_ce: 0.044655
2022-01-11 22:31:52,474 iteration 669 : loss : 0.106117, loss_ce: 0.037596
2022-01-11 22:31:54,148 iteration 670 : loss : 0.110728, loss_ce: 0.040965
2022-01-11 22:31:55,692 iteration 671 : loss : 0.098488, loss_ce: 0.038016
2022-01-11 22:31:57,323 iteration 672 : loss : 0.099984, loss_ce: 0.040988
2022-01-11 22:31:58,919 iteration 673 : loss : 0.092361, loss_ce: 0.040226
2022-01-11 22:32:00,494 iteration 674 : loss : 0.084921, loss_ce: 0.042324
2022-01-11 22:32:02,096 iteration 675 : loss : 0.147260, loss_ce: 0.066030
2022-01-11 22:32:03,636 iteration 676 : loss : 0.117016, loss_ce: 0.044245
2022-01-11 22:32:05,199 iteration 677 : loss : 0.071020, loss_ce: 0.033501
2022-01-11 22:32:06,799 iteration 678 : loss : 0.183939, loss_ce: 0.066684
2022-01-11 22:32:08,372 iteration 679 : loss : 0.137068, loss_ce: 0.061688
2022-01-11 22:32:08,373 Training Data Eval:
2022-01-11 22:32:16,340   Average segmentation loss on training set: 0.1018
2022-01-11 22:32:16,340 Validation Data Eval:
2022-01-11 22:32:19,084   Average segmentation loss on validation set: 0.1493
2022-01-11 22:32:24,997 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 22:32:26,456 iteration 680 : loss : 0.072023, loss_ce: 0.026475
 10%|███                           | 40/400 [19:30<3:14:43, 32.45s/it]2022-01-11 22:32:27,985 iteration 681 : loss : 0.065301, loss_ce: 0.027101
2022-01-11 22:32:29,375 iteration 682 : loss : 0.140876, loss_ce: 0.040418
2022-01-11 22:32:30,836 iteration 683 : loss : 0.091872, loss_ce: 0.042877
2022-01-11 22:32:32,260 iteration 684 : loss : 0.095687, loss_ce: 0.035120
2022-01-11 22:32:33,784 iteration 685 : loss : 0.114581, loss_ce: 0.044164
2022-01-11 22:32:35,249 iteration 686 : loss : 0.157634, loss_ce: 0.058970
2022-01-11 22:32:36,720 iteration 687 : loss : 0.089274, loss_ce: 0.037181
2022-01-11 22:32:38,175 iteration 688 : loss : 0.117320, loss_ce: 0.043357
2022-01-11 22:32:39,679 iteration 689 : loss : 0.092628, loss_ce: 0.043870
2022-01-11 22:32:41,184 iteration 690 : loss : 0.090443, loss_ce: 0.031532
2022-01-11 22:32:42,750 iteration 691 : loss : 0.128818, loss_ce: 0.052979
2022-01-11 22:32:44,375 iteration 692 : loss : 0.082726, loss_ce: 0.030625
2022-01-11 22:32:45,889 iteration 693 : loss : 0.097309, loss_ce: 0.045678
2022-01-11 22:32:47,467 iteration 694 : loss : 0.120143, loss_ce: 0.050592
2022-01-11 22:32:49,007 iteration 695 : loss : 0.108415, loss_ce: 0.048844
2022-01-11 22:32:50,640 iteration 696 : loss : 0.071458, loss_ce: 0.029216
2022-01-11 22:32:52,210 iteration 697 : loss : 0.092321, loss_ce: 0.038386
 10%|███                           | 41/400 [19:56<3:02:09, 30.45s/it]2022-01-11 22:32:53,783 iteration 698 : loss : 0.113285, loss_ce: 0.034292
2022-01-11 22:32:55,395 iteration 699 : loss : 0.114635, loss_ce: 0.049342
2022-01-11 22:32:56,947 iteration 700 : loss : 0.101011, loss_ce: 0.049557
2022-01-11 22:32:58,491 iteration 701 : loss : 0.123682, loss_ce: 0.061582
2022-01-11 22:33:00,096 iteration 702 : loss : 0.108724, loss_ce: 0.039888
2022-01-11 22:33:01,748 iteration 703 : loss : 0.063880, loss_ce: 0.029252
2022-01-11 22:33:03,258 iteration 704 : loss : 0.080305, loss_ce: 0.031139
2022-01-11 22:33:04,824 iteration 705 : loss : 0.109671, loss_ce: 0.037237
2022-01-11 22:33:06,409 iteration 706 : loss : 0.086255, loss_ce: 0.031093
2022-01-11 22:33:07,914 iteration 707 : loss : 0.093277, loss_ce: 0.042508
2022-01-11 22:33:09,484 iteration 708 : loss : 0.067434, loss_ce: 0.025973
2022-01-11 22:33:11,040 iteration 709 : loss : 0.120056, loss_ce: 0.049808
2022-01-11 22:33:12,631 iteration 710 : loss : 0.105464, loss_ce: 0.043060
2022-01-11 22:33:14,219 iteration 711 : loss : 0.077658, loss_ce: 0.034236
2022-01-11 22:33:15,867 iteration 712 : loss : 0.086787, loss_ce: 0.034120
2022-01-11 22:33:17,429 iteration 713 : loss : 0.079356, loss_ce: 0.036074
2022-01-11 22:33:19,058 iteration 714 : loss : 0.104169, loss_ce: 0.037160
 10%|███▏                          | 42/400 [20:23<2:55:12, 29.37s/it]2022-01-11 22:33:20,703 iteration 715 : loss : 0.105012, loss_ce: 0.046183
2022-01-11 22:33:22,321 iteration 716 : loss : 0.079234, loss_ce: 0.037551
2022-01-11 22:33:23,891 iteration 717 : loss : 0.113426, loss_ce: 0.043364
2022-01-11 22:33:25,481 iteration 718 : loss : 0.100703, loss_ce: 0.050557
2022-01-11 22:33:27,121 iteration 719 : loss : 0.094983, loss_ce: 0.038037
2022-01-11 22:33:28,695 iteration 720 : loss : 0.085161, loss_ce: 0.030980
2022-01-11 22:33:30,260 iteration 721 : loss : 0.092665, loss_ce: 0.029350
2022-01-11 22:33:31,822 iteration 722 : loss : 0.099443, loss_ce: 0.032318
2022-01-11 22:33:33,445 iteration 723 : loss : 0.125575, loss_ce: 0.062756
2022-01-11 22:33:34,993 iteration 724 : loss : 0.142633, loss_ce: 0.068132
2022-01-11 22:33:36,623 iteration 725 : loss : 0.084704, loss_ce: 0.032512
2022-01-11 22:33:38,180 iteration 726 : loss : 0.111911, loss_ce: 0.045498
2022-01-11 22:33:39,688 iteration 727 : loss : 0.079707, loss_ce: 0.030762
2022-01-11 22:33:41,292 iteration 728 : loss : 0.088926, loss_ce: 0.035764
2022-01-11 22:33:42,910 iteration 729 : loss : 0.080877, loss_ce: 0.035674
2022-01-11 22:33:44,505 iteration 730 : loss : 0.086879, loss_ce: 0.035006
2022-01-11 22:33:45,994 iteration 731 : loss : 0.068046, loss_ce: 0.026443
 11%|███▏                          | 43/400 [20:50<2:50:23, 28.64s/it]2022-01-11 22:33:47,550 iteration 732 : loss : 0.088002, loss_ce: 0.041158
2022-01-11 22:33:49,044 iteration 733 : loss : 0.098748, loss_ce: 0.048283
2022-01-11 22:33:50,647 iteration 734 : loss : 0.086953, loss_ce: 0.038941
2022-01-11 22:33:52,209 iteration 735 : loss : 0.092046, loss_ce: 0.037525
2022-01-11 22:33:53,745 iteration 736 : loss : 0.076424, loss_ce: 0.034025
2022-01-11 22:33:55,428 iteration 737 : loss : 0.081212, loss_ce: 0.038394
2022-01-11 22:33:56,914 iteration 738 : loss : 0.112056, loss_ce: 0.067035
2022-01-11 22:33:58,513 iteration 739 : loss : 0.072534, loss_ce: 0.026661
2022-01-11 22:34:00,064 iteration 740 : loss : 0.071157, loss_ce: 0.028708
2022-01-11 22:34:01,595 iteration 741 : loss : 0.086168, loss_ce: 0.032271
2022-01-11 22:34:03,087 iteration 742 : loss : 0.101795, loss_ce: 0.052019
2022-01-11 22:34:04,651 iteration 743 : loss : 0.072108, loss_ce: 0.029822
2022-01-11 22:34:06,245 iteration 744 : loss : 0.117542, loss_ce: 0.046354
2022-01-11 22:34:07,772 iteration 745 : loss : 0.132172, loss_ce: 0.028107
2022-01-11 22:34:09,310 iteration 746 : loss : 0.099960, loss_ce: 0.038090
2022-01-11 22:34:10,843 iteration 747 : loss : 0.104819, loss_ce: 0.045798
2022-01-11 22:34:12,366 iteration 748 : loss : 0.082234, loss_ce: 0.036121
 11%|███▎                          | 44/400 [21:16<2:45:52, 27.96s/it]2022-01-11 22:34:13,913 iteration 749 : loss : 0.185339, loss_ce: 0.036075
2022-01-11 22:34:15,455 iteration 750 : loss : 0.080642, loss_ce: 0.028976
2022-01-11 22:34:17,034 iteration 751 : loss : 0.071291, loss_ce: 0.022303
2022-01-11 22:34:18,572 iteration 752 : loss : 0.133869, loss_ce: 0.069036
2022-01-11 22:34:20,086 iteration 753 : loss : 0.120823, loss_ce: 0.055374
2022-01-11 22:34:21,611 iteration 754 : loss : 0.115015, loss_ce: 0.042842
2022-01-11 22:34:23,090 iteration 755 : loss : 0.098969, loss_ce: 0.052888
2022-01-11 22:34:24,574 iteration 756 : loss : 0.069356, loss_ce: 0.024690
2022-01-11 22:34:26,118 iteration 757 : loss : 0.109502, loss_ce: 0.048976
2022-01-11 22:34:27,754 iteration 758 : loss : 0.093714, loss_ce: 0.042334
2022-01-11 22:34:29,441 iteration 759 : loss : 0.109787, loss_ce: 0.040131
2022-01-11 22:34:30,989 iteration 760 : loss : 0.102404, loss_ce: 0.056233
2022-01-11 22:34:32,576 iteration 761 : loss : 0.082445, loss_ce: 0.038970
2022-01-11 22:34:34,171 iteration 762 : loss : 0.117523, loss_ce: 0.049951
2022-01-11 22:34:35,780 iteration 763 : loss : 0.110740, loss_ce: 0.044398
2022-01-11 22:34:37,314 iteration 764 : loss : 0.102982, loss_ce: 0.048581
2022-01-11 22:34:37,315 Training Data Eval:
2022-01-11 22:34:45,279   Average segmentation loss on training set: 0.0864
2022-01-11 22:34:45,280 Validation Data Eval:
2022-01-11 22:34:48,021   Average segmentation loss on validation set: 0.1112
2022-01-11 22:34:54,617 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 22:34:56,240 iteration 765 : loss : 0.114018, loss_ce: 0.047192
 11%|███▍                          | 45/400 [22:00<3:13:40, 32.73s/it]2022-01-11 22:34:57,789 iteration 766 : loss : 0.117785, loss_ce: 0.049850
2022-01-11 22:34:59,287 iteration 767 : loss : 0.100915, loss_ce: 0.040020
2022-01-11 22:35:00,766 iteration 768 : loss : 0.088945, loss_ce: 0.032470
2022-01-11 22:35:02,265 iteration 769 : loss : 0.107417, loss_ce: 0.042530
2022-01-11 22:35:03,651 iteration 770 : loss : 0.068196, loss_ce: 0.027041
2022-01-11 22:35:05,120 iteration 771 : loss : 0.124149, loss_ce: 0.032754
2022-01-11 22:35:06,600 iteration 772 : loss : 0.079417, loss_ce: 0.030571
2022-01-11 22:35:08,095 iteration 773 : loss : 0.127376, loss_ce: 0.031392
2022-01-11 22:35:09,697 iteration 774 : loss : 0.091523, loss_ce: 0.038296
2022-01-11 22:35:11,222 iteration 775 : loss : 0.146802, loss_ce: 0.075410
2022-01-11 22:35:12,763 iteration 776 : loss : 0.096785, loss_ce: 0.040002
2022-01-11 22:35:14,524 iteration 777 : loss : 0.150851, loss_ce: 0.078276
2022-01-11 22:35:16,034 iteration 778 : loss : 0.105112, loss_ce: 0.033123
2022-01-11 22:35:17,669 iteration 779 : loss : 0.118171, loss_ce: 0.053794
2022-01-11 22:35:19,309 iteration 780 : loss : 0.110181, loss_ce: 0.039437
2022-01-11 22:35:20,844 iteration 781 : loss : 0.087337, loss_ce: 0.047443
2022-01-11 22:35:22,342 iteration 782 : loss : 0.075303, loss_ce: 0.038347
 12%|███▍                          | 46/400 [22:26<3:01:23, 30.74s/it]2022-01-11 22:35:23,939 iteration 783 : loss : 0.128915, loss_ce: 0.058607
2022-01-11 22:35:25,473 iteration 784 : loss : 0.092957, loss_ce: 0.035049
2022-01-11 22:35:27,052 iteration 785 : loss : 0.102232, loss_ce: 0.045926
2022-01-11 22:35:28,602 iteration 786 : loss : 0.130818, loss_ce: 0.049495
2022-01-11 22:35:30,161 iteration 787 : loss : 0.098995, loss_ce: 0.038591
2022-01-11 22:35:31,779 iteration 788 : loss : 0.105837, loss_ce: 0.044285
2022-01-11 22:35:33,270 iteration 789 : loss : 0.118620, loss_ce: 0.047049
2022-01-11 22:35:34,887 iteration 790 : loss : 0.095008, loss_ce: 0.037665
2022-01-11 22:35:36,447 iteration 791 : loss : 0.074731, loss_ce: 0.029963
2022-01-11 22:35:37,962 iteration 792 : loss : 0.077652, loss_ce: 0.035823
2022-01-11 22:35:39,594 iteration 793 : loss : 0.105348, loss_ce: 0.058540
2022-01-11 22:35:41,077 iteration 794 : loss : 0.066779, loss_ce: 0.027260
2022-01-11 22:35:42,590 iteration 795 : loss : 0.077255, loss_ce: 0.030916
2022-01-11 22:35:44,102 iteration 796 : loss : 0.070509, loss_ce: 0.035508
2022-01-11 22:35:45,707 iteration 797 : loss : 0.181872, loss_ce: 0.099070
2022-01-11 22:35:47,251 iteration 798 : loss : 0.098000, loss_ce: 0.034273
2022-01-11 22:35:48,751 iteration 799 : loss : 0.071443, loss_ce: 0.032941
 12%|███▌                          | 47/400 [22:52<2:53:13, 29.44s/it]2022-01-11 22:35:50,454 iteration 800 : loss : 0.108439, loss_ce: 0.038914
2022-01-11 22:35:52,044 iteration 801 : loss : 0.089298, loss_ce: 0.036029
2022-01-11 22:35:53,605 iteration 802 : loss : 0.120319, loss_ce: 0.049301
2022-01-11 22:35:55,194 iteration 803 : loss : 0.130299, loss_ce: 0.072172
2022-01-11 22:35:56,844 iteration 804 : loss : 0.205570, loss_ce: 0.093408
2022-01-11 22:35:58,555 iteration 805 : loss : 0.087729, loss_ce: 0.029771
2022-01-11 22:36:00,108 iteration 806 : loss : 0.080972, loss_ce: 0.033046
2022-01-11 22:36:01,621 iteration 807 : loss : 0.091797, loss_ce: 0.034899
2022-01-11 22:36:03,222 iteration 808 : loss : 0.103599, loss_ce: 0.055493
2022-01-11 22:36:04,799 iteration 809 : loss : 0.092123, loss_ce: 0.033200
2022-01-11 22:36:06,333 iteration 810 : loss : 0.121147, loss_ce: 0.043698
2022-01-11 22:36:07,842 iteration 811 : loss : 0.124435, loss_ce: 0.067320
2022-01-11 22:36:09,369 iteration 812 : loss : 0.074085, loss_ce: 0.033424
2022-01-11 22:36:10,932 iteration 813 : loss : 0.100299, loss_ce: 0.033400
2022-01-11 22:36:12,601 iteration 814 : loss : 0.103433, loss_ce: 0.036498
2022-01-11 22:36:14,237 iteration 815 : loss : 0.116662, loss_ce: 0.047058
2022-01-11 22:36:15,821 iteration 816 : loss : 0.094250, loss_ce: 0.034901
 12%|███▌                          | 48/400 [23:19<2:48:33, 28.73s/it]2022-01-11 22:36:17,410 iteration 817 : loss : 0.104556, loss_ce: 0.053328
2022-01-11 22:36:18,925 iteration 818 : loss : 0.069910, loss_ce: 0.029643
2022-01-11 22:36:20,484 iteration 819 : loss : 0.072738, loss_ce: 0.030529
2022-01-11 22:36:21,978 iteration 820 : loss : 0.058940, loss_ce: 0.025405
2022-01-11 22:36:23,593 iteration 821 : loss : 0.087305, loss_ce: 0.034119
2022-01-11 22:36:25,192 iteration 822 : loss : 0.112825, loss_ce: 0.050690
2022-01-11 22:36:26,704 iteration 823 : loss : 0.083636, loss_ce: 0.033511
2022-01-11 22:36:28,214 iteration 824 : loss : 0.121826, loss_ce: 0.055797
2022-01-11 22:36:29,771 iteration 825 : loss : 0.079781, loss_ce: 0.030913
2022-01-11 22:36:31,311 iteration 826 : loss : 0.112536, loss_ce: 0.033350
2022-01-11 22:36:32,869 iteration 827 : loss : 0.135701, loss_ce: 0.053217
2022-01-11 22:36:34,425 iteration 828 : loss : 0.075748, loss_ce: 0.029693
2022-01-11 22:36:36,001 iteration 829 : loss : 0.093406, loss_ce: 0.036348
2022-01-11 22:36:37,617 iteration 830 : loss : 0.077194, loss_ce: 0.028628
2022-01-11 22:36:39,184 iteration 831 : loss : 0.134201, loss_ce: 0.060621
2022-01-11 22:36:40,719 iteration 832 : loss : 0.083800, loss_ce: 0.034428
2022-01-11 22:36:42,330 iteration 833 : loss : 0.091727, loss_ce: 0.047289
 12%|███▋                          | 49/400 [23:46<2:44:10, 28.06s/it]2022-01-11 22:36:43,935 iteration 834 : loss : 0.089384, loss_ce: 0.036817
2022-01-11 22:36:45,415 iteration 835 : loss : 0.099835, loss_ce: 0.029827
2022-01-11 22:36:46,988 iteration 836 : loss : 0.083912, loss_ce: 0.041166
2022-01-11 22:36:48,493 iteration 837 : loss : 0.072708, loss_ce: 0.037472
2022-01-11 22:36:49,990 iteration 838 : loss : 0.085672, loss_ce: 0.037694
2022-01-11 22:36:51,564 iteration 839 : loss : 0.082648, loss_ce: 0.041323
2022-01-11 22:36:53,113 iteration 840 : loss : 0.086588, loss_ce: 0.032351
2022-01-11 22:36:54,620 iteration 841 : loss : 0.077404, loss_ce: 0.031765
2022-01-11 22:36:56,171 iteration 842 : loss : 0.089050, loss_ce: 0.033064
2022-01-11 22:36:57,814 iteration 843 : loss : 0.084010, loss_ce: 0.029558
2022-01-11 22:36:59,357 iteration 844 : loss : 0.082428, loss_ce: 0.028974
2022-01-11 22:37:00,941 iteration 845 : loss : 0.092845, loss_ce: 0.036754
2022-01-11 22:37:02,613 iteration 846 : loss : 0.079013, loss_ce: 0.033337
2022-01-11 22:37:04,190 iteration 847 : loss : 0.065364, loss_ce: 0.024943
2022-01-11 22:37:05,665 iteration 848 : loss : 0.083852, loss_ce: 0.036972
2022-01-11 22:37:07,239 iteration 849 : loss : 0.105406, loss_ce: 0.042734
2022-01-11 22:37:07,239 Training Data Eval:
2022-01-11 22:37:15,211   Average segmentation loss on training set: 0.0855
2022-01-11 22:37:15,211 Validation Data Eval:
2022-01-11 22:37:17,950   Average segmentation loss on validation set: 0.1787
2022-01-11 22:37:19,552 iteration 850 : loss : 0.108963, loss_ce: 0.046159
 12%|███▊                          | 50/400 [24:23<2:59:44, 30.81s/it]2022-01-11 22:37:21,180 iteration 851 : loss : 0.059315, loss_ce: 0.024248
2022-01-11 22:37:22,716 iteration 852 : loss : 0.062415, loss_ce: 0.027935
2022-01-11 22:37:24,357 iteration 853 : loss : 0.069900, loss_ce: 0.027935
2022-01-11 22:37:25,942 iteration 854 : loss : 0.050265, loss_ce: 0.015384
2022-01-11 22:37:27,526 iteration 855 : loss : 0.082217, loss_ce: 0.037062
2022-01-11 22:37:29,130 iteration 856 : loss : 0.076500, loss_ce: 0.027289
2022-01-11 22:37:30,659 iteration 857 : loss : 0.075511, loss_ce: 0.032213
2022-01-11 22:37:32,133 iteration 858 : loss : 0.072291, loss_ce: 0.035678
2022-01-11 22:37:33,681 iteration 859 : loss : 0.081378, loss_ce: 0.028821
2022-01-11 22:37:35,185 iteration 860 : loss : 0.077723, loss_ce: 0.024338
2022-01-11 22:37:36,804 iteration 861 : loss : 0.079473, loss_ce: 0.032039
2022-01-11 22:37:38,293 iteration 862 : loss : 0.076603, loss_ce: 0.024014
2022-01-11 22:37:39,922 iteration 863 : loss : 0.125480, loss_ce: 0.059692
2022-01-11 22:37:41,507 iteration 864 : loss : 0.069017, loss_ce: 0.030215
2022-01-11 22:37:42,993 iteration 865 : loss : 0.063895, loss_ce: 0.025670
2022-01-11 22:37:44,605 iteration 866 : loss : 0.086660, loss_ce: 0.040267
2022-01-11 22:37:46,103 iteration 867 : loss : 0.049158, loss_ce: 0.019269
 13%|███▊                          | 51/400 [24:50<2:51:47, 29.53s/it]2022-01-11 22:37:47,729 iteration 868 : loss : 0.129090, loss_ce: 0.045328
2022-01-11 22:37:49,272 iteration 869 : loss : 0.075042, loss_ce: 0.032162
2022-01-11 22:37:50,752 iteration 870 : loss : 0.057855, loss_ce: 0.022496
2022-01-11 22:37:52,260 iteration 871 : loss : 0.060771, loss_ce: 0.023190
2022-01-11 22:37:53,786 iteration 872 : loss : 0.065933, loss_ce: 0.032588
2022-01-11 22:37:55,415 iteration 873 : loss : 0.057175, loss_ce: 0.019861
2022-01-11 22:37:56,891 iteration 874 : loss : 0.057649, loss_ce: 0.025541
2022-01-11 22:37:58,582 iteration 875 : loss : 0.104837, loss_ce: 0.037021
2022-01-11 22:38:00,136 iteration 876 : loss : 0.052195, loss_ce: 0.024314
2022-01-11 22:38:01,592 iteration 877 : loss : 0.052309, loss_ce: 0.019913
2022-01-11 22:38:03,191 iteration 878 : loss : 0.080430, loss_ce: 0.036429
2022-01-11 22:38:04,718 iteration 879 : loss : 0.074053, loss_ce: 0.033404
2022-01-11 22:38:06,294 iteration 880 : loss : 0.056034, loss_ce: 0.021327
2022-01-11 22:38:07,825 iteration 881 : loss : 0.105519, loss_ce: 0.050073
2022-01-11 22:38:09,422 iteration 882 : loss : 0.080188, loss_ce: 0.032393
2022-01-11 22:38:10,970 iteration 883 : loss : 0.061493, loss_ce: 0.032243
2022-01-11 22:38:12,600 iteration 884 : loss : 0.103783, loss_ce: 0.045969
 13%|███▉                          | 52/400 [25:16<2:46:00, 28.62s/it]2022-01-11 22:38:14,209 iteration 885 : loss : 0.100273, loss_ce: 0.037964
2022-01-11 22:38:15,786 iteration 886 : loss : 0.075952, loss_ce: 0.028830
2022-01-11 22:38:17,382 iteration 887 : loss : 0.099665, loss_ce: 0.040893
2022-01-11 22:38:18,940 iteration 888 : loss : 0.054306, loss_ce: 0.020238
2022-01-11 22:38:20,545 iteration 889 : loss : 0.081139, loss_ce: 0.027782
2022-01-11 22:38:22,189 iteration 890 : loss : 0.081995, loss_ce: 0.030555
2022-01-11 22:38:23,742 iteration 891 : loss : 0.058299, loss_ce: 0.021781
2022-01-11 22:38:25,300 iteration 892 : loss : 0.070938, loss_ce: 0.036986
2022-01-11 22:38:26,841 iteration 893 : loss : 0.059905, loss_ce: 0.021397
2022-01-11 22:38:28,493 iteration 894 : loss : 0.086738, loss_ce: 0.035154
2022-01-11 22:38:30,017 iteration 895 : loss : 0.116295, loss_ce: 0.049078
2022-01-11 22:38:31,637 iteration 896 : loss : 0.131809, loss_ce: 0.051500
2022-01-11 22:38:33,260 iteration 897 : loss : 0.098575, loss_ce: 0.050757
2022-01-11 22:38:34,785 iteration 898 : loss : 0.068810, loss_ce: 0.031397
2022-01-11 22:38:36,330 iteration 899 : loss : 0.068363, loss_ce: 0.033383
2022-01-11 22:38:37,834 iteration 900 : loss : 0.059685, loss_ce: 0.025487
2022-01-11 22:38:39,366 iteration 901 : loss : 0.084566, loss_ce: 0.030400
 13%|███▉                          | 53/400 [25:43<2:42:18, 28.06s/it]2022-01-11 22:38:40,990 iteration 902 : loss : 0.069551, loss_ce: 0.028753
2022-01-11 22:38:42,633 iteration 903 : loss : 0.079031, loss_ce: 0.030246
2022-01-11 22:38:44,214 iteration 904 : loss : 0.084022, loss_ce: 0.029044
2022-01-11 22:38:45,791 iteration 905 : loss : 0.100354, loss_ce: 0.049529
2022-01-11 22:38:47,344 iteration 906 : loss : 0.060922, loss_ce: 0.027207
2022-01-11 22:38:48,958 iteration 907 : loss : 0.085946, loss_ce: 0.030119
2022-01-11 22:38:50,423 iteration 908 : loss : 0.065076, loss_ce: 0.029557
2022-01-11 22:38:51,998 iteration 909 : loss : 0.080431, loss_ce: 0.032347
2022-01-11 22:38:53,482 iteration 910 : loss : 0.112794, loss_ce: 0.050940
2022-01-11 22:38:55,031 iteration 911 : loss : 0.053666, loss_ce: 0.022611
2022-01-11 22:38:56,580 iteration 912 : loss : 0.077109, loss_ce: 0.029619
2022-01-11 22:38:58,113 iteration 913 : loss : 0.085975, loss_ce: 0.029723
2022-01-11 22:38:59,661 iteration 914 : loss : 0.111805, loss_ce: 0.050688
2022-01-11 22:39:01,179 iteration 915 : loss : 0.099033, loss_ce: 0.030496
2022-01-11 22:39:02,706 iteration 916 : loss : 0.056733, loss_ce: 0.019660
2022-01-11 22:39:04,281 iteration 917 : loss : 0.095705, loss_ce: 0.038898
2022-01-11 22:39:05,859 iteration 918 : loss : 0.073321, loss_ce: 0.030173
 14%|████                          | 54/400 [26:09<2:39:06, 27.59s/it]2022-01-11 22:39:07,473 iteration 919 : loss : 0.074283, loss_ce: 0.031499
2022-01-11 22:39:09,046 iteration 920 : loss : 0.081028, loss_ce: 0.032594
2022-01-11 22:39:10,643 iteration 921 : loss : 0.070807, loss_ce: 0.028159
2022-01-11 22:39:12,256 iteration 922 : loss : 0.084927, loss_ce: 0.038946
2022-01-11 22:39:13,800 iteration 923 : loss : 0.060524, loss_ce: 0.028089
2022-01-11 22:39:15,404 iteration 924 : loss : 0.116982, loss_ce: 0.040811
2022-01-11 22:39:16,922 iteration 925 : loss : 0.062383, loss_ce: 0.024780
2022-01-11 22:39:18,528 iteration 926 : loss : 0.065720, loss_ce: 0.029658
2022-01-11 22:39:20,015 iteration 927 : loss : 0.080659, loss_ce: 0.029710
2022-01-11 22:39:21,567 iteration 928 : loss : 0.064623, loss_ce: 0.028914
2022-01-11 22:39:23,171 iteration 929 : loss : 0.092840, loss_ce: 0.036852
2022-01-11 22:39:24,742 iteration 930 : loss : 0.092387, loss_ce: 0.037071
2022-01-11 22:39:26,305 iteration 931 : loss : 0.104894, loss_ce: 0.030267
2022-01-11 22:39:27,865 iteration 932 : loss : 0.059469, loss_ce: 0.021766
2022-01-11 22:39:29,388 iteration 933 : loss : 0.109037, loss_ce: 0.048345
2022-01-11 22:39:30,944 iteration 934 : loss : 0.061097, loss_ce: 0.026771
2022-01-11 22:39:30,944 Training Data Eval:
2022-01-11 22:39:38,920   Average segmentation loss on training set: 0.0820
2022-01-11 22:39:38,920 Validation Data Eval:
2022-01-11 22:39:41,665   Average segmentation loss on validation set: 0.1661
2022-01-11 22:39:43,337 iteration 935 : loss : 0.096812, loss_ce: 0.030080
 14%|████▏                         | 55/400 [26:47<2:55:42, 30.56s/it]2022-01-11 22:39:44,921 iteration 936 : loss : 0.074005, loss_ce: 0.024139
2022-01-11 22:39:46,503 iteration 937 : loss : 0.066609, loss_ce: 0.024032
2022-01-11 22:39:48,114 iteration 938 : loss : 0.086344, loss_ce: 0.031317
2022-01-11 22:39:49,632 iteration 939 : loss : 0.069752, loss_ce: 0.025552
2022-01-11 22:39:51,216 iteration 940 : loss : 0.085197, loss_ce: 0.037521
2022-01-11 22:39:52,885 iteration 941 : loss : 0.081601, loss_ce: 0.030219
2022-01-11 22:39:54,398 iteration 942 : loss : 0.063888, loss_ce: 0.024056
2022-01-11 22:39:55,945 iteration 943 : loss : 0.058882, loss_ce: 0.026744
2022-01-11 22:39:57,459 iteration 944 : loss : 0.068350, loss_ce: 0.028882
2022-01-11 22:39:58,983 iteration 945 : loss : 0.070945, loss_ce: 0.029072
2022-01-11 22:40:00,510 iteration 946 : loss : 0.051400, loss_ce: 0.024436
2022-01-11 22:40:02,020 iteration 947 : loss : 0.067788, loss_ce: 0.027245
2022-01-11 22:40:03,532 iteration 948 : loss : 0.083125, loss_ce: 0.029967
2022-01-11 22:40:05,078 iteration 949 : loss : 0.062675, loss_ce: 0.026185
2022-01-11 22:40:06,616 iteration 950 : loss : 0.086048, loss_ce: 0.042472
2022-01-11 22:40:08,202 iteration 951 : loss : 0.081955, loss_ce: 0.051779
2022-01-11 22:40:09,708 iteration 952 : loss : 0.057014, loss_ce: 0.023813
 14%|████▏                         | 56/400 [27:13<2:48:00, 29.30s/it]2022-01-11 22:40:11,316 iteration 953 : loss : 0.081584, loss_ce: 0.026791
2022-01-11 22:40:12,898 iteration 954 : loss : 0.065039, loss_ce: 0.025909
2022-01-11 22:40:14,376 iteration 955 : loss : 0.043867, loss_ce: 0.019233
2022-01-11 22:40:15,963 iteration 956 : loss : 0.077601, loss_ce: 0.033736
2022-01-11 22:40:17,534 iteration 957 : loss : 0.065248, loss_ce: 0.025238
2022-01-11 22:40:19,105 iteration 958 : loss : 0.095122, loss_ce: 0.033809
2022-01-11 22:40:20,736 iteration 959 : loss : 0.078305, loss_ce: 0.037616
2022-01-11 22:40:22,425 iteration 960 : loss : 0.077410, loss_ce: 0.033293
2022-01-11 22:40:23,943 iteration 961 : loss : 0.095625, loss_ce: 0.026463
2022-01-11 22:40:25,447 iteration 962 : loss : 0.070629, loss_ce: 0.020448
2022-01-11 22:40:27,068 iteration 963 : loss : 0.064684, loss_ce: 0.024665
2022-01-11 22:40:28,651 iteration 964 : loss : 0.064615, loss_ce: 0.031497
2022-01-11 22:40:30,186 iteration 965 : loss : 0.055072, loss_ce: 0.025772
2022-01-11 22:40:31,745 iteration 966 : loss : 0.071443, loss_ce: 0.030490
2022-01-11 22:40:33,321 iteration 967 : loss : 0.058826, loss_ce: 0.025840
2022-01-11 22:40:34,835 iteration 968 : loss : 0.119655, loss_ce: 0.043340
2022-01-11 22:40:36,399 iteration 969 : loss : 0.055049, loss_ce: 0.022981
 14%|████▎                         | 57/400 [27:40<2:43:02, 28.52s/it]2022-01-11 22:40:37,975 iteration 970 : loss : 0.068189, loss_ce: 0.034221
2022-01-11 22:40:39,554 iteration 971 : loss : 0.071927, loss_ce: 0.022359
2022-01-11 22:40:41,057 iteration 972 : loss : 0.050170, loss_ce: 0.024046
2022-01-11 22:40:42,631 iteration 973 : loss : 0.065580, loss_ce: 0.020459
2022-01-11 22:40:44,136 iteration 974 : loss : 0.075056, loss_ce: 0.031264
2022-01-11 22:40:45,735 iteration 975 : loss : 0.072118, loss_ce: 0.023776
2022-01-11 22:40:47,267 iteration 976 : loss : 0.056809, loss_ce: 0.023628
2022-01-11 22:40:48,819 iteration 977 : loss : 0.058896, loss_ce: 0.019134
2022-01-11 22:40:50,389 iteration 978 : loss : 0.040913, loss_ce: 0.015384
2022-01-11 22:40:51,917 iteration 979 : loss : 0.097693, loss_ce: 0.052898
2022-01-11 22:40:53,442 iteration 980 : loss : 0.053535, loss_ce: 0.020881
2022-01-11 22:40:55,076 iteration 981 : loss : 0.056939, loss_ce: 0.029933
2022-01-11 22:40:56,608 iteration 982 : loss : 0.065055, loss_ce: 0.033130
2022-01-11 22:40:58,213 iteration 983 : loss : 0.117518, loss_ce: 0.036042
2022-01-11 22:40:59,702 iteration 984 : loss : 0.057243, loss_ce: 0.025553
2022-01-11 22:41:01,253 iteration 985 : loss : 0.061697, loss_ce: 0.019597
2022-01-11 22:41:02,849 iteration 986 : loss : 0.066963, loss_ce: 0.026320
 14%|████▎                         | 58/400 [28:06<2:39:02, 27.90s/it]2022-01-11 22:41:04,495 iteration 987 : loss : 0.051190, loss_ce: 0.022197
2022-01-11 22:41:06,070 iteration 988 : loss : 0.060581, loss_ce: 0.027413
2022-01-11 22:41:07,649 iteration 989 : loss : 0.086591, loss_ce: 0.042018
2022-01-11 22:41:09,208 iteration 990 : loss : 0.068020, loss_ce: 0.030549
2022-01-11 22:41:10,766 iteration 991 : loss : 0.056427, loss_ce: 0.024465
2022-01-11 22:41:12,386 iteration 992 : loss : 0.072266, loss_ce: 0.035785
2022-01-11 22:41:13,960 iteration 993 : loss : 0.064226, loss_ce: 0.025097
2022-01-11 22:41:15,614 iteration 994 : loss : 0.082664, loss_ce: 0.042079
2022-01-11 22:41:17,177 iteration 995 : loss : 0.131845, loss_ce: 0.041674
2022-01-11 22:41:18,796 iteration 996 : loss : 0.059579, loss_ce: 0.026239
2022-01-11 22:41:20,412 iteration 997 : loss : 0.095478, loss_ce: 0.037604
2022-01-11 22:41:21,914 iteration 998 : loss : 0.101834, loss_ce: 0.040481
2022-01-11 22:41:23,512 iteration 999 : loss : 0.065269, loss_ce: 0.029231
2022-01-11 22:41:25,034 iteration 1000 : loss : 0.076407, loss_ce: 0.024547
2022-01-11 22:41:26,562 iteration 1001 : loss : 0.142375, loss_ce: 0.038889
2022-01-11 22:41:28,206 iteration 1002 : loss : 0.067951, loss_ce: 0.028099
2022-01-11 22:41:29,828 iteration 1003 : loss : 0.097604, loss_ce: 0.040020
 15%|████▍                         | 59/400 [28:33<2:36:59, 27.62s/it]2022-01-11 22:41:31,424 iteration 1004 : loss : 0.137253, loss_ce: 0.061384
2022-01-11 22:41:32,970 iteration 1005 : loss : 0.086637, loss_ce: 0.029910
2022-01-11 22:41:34,611 iteration 1006 : loss : 0.096242, loss_ce: 0.038133
2022-01-11 22:41:36,129 iteration 1007 : loss : 0.070782, loss_ce: 0.035792
2022-01-11 22:41:37,632 iteration 1008 : loss : 0.141593, loss_ce: 0.058553
2022-01-11 22:41:39,146 iteration 1009 : loss : 0.076587, loss_ce: 0.027648
2022-01-11 22:41:40,665 iteration 1010 : loss : 0.079751, loss_ce: 0.028879
2022-01-11 22:41:42,271 iteration 1011 : loss : 0.085445, loss_ce: 0.041514
2022-01-11 22:41:43,817 iteration 1012 : loss : 0.059485, loss_ce: 0.023812
2022-01-11 22:41:45,320 iteration 1013 : loss : 0.062680, loss_ce: 0.021351
2022-01-11 22:41:46,888 iteration 1014 : loss : 0.114047, loss_ce: 0.046087
2022-01-11 22:41:48,384 iteration 1015 : loss : 0.075972, loss_ce: 0.033047
2022-01-11 22:41:49,953 iteration 1016 : loss : 0.072872, loss_ce: 0.030365
2022-01-11 22:41:51,420 iteration 1017 : loss : 0.062134, loss_ce: 0.025776
2022-01-11 22:41:52,893 iteration 1018 : loss : 0.087759, loss_ce: 0.033250
2022-01-11 22:41:54,435 iteration 1019 : loss : 0.081117, loss_ce: 0.035121
2022-01-11 22:41:54,435 Training Data Eval:
2022-01-11 22:42:02,414   Average segmentation loss on training set: 0.0747
2022-01-11 22:42:02,415 Validation Data Eval:
2022-01-11 22:42:05,157   Average segmentation loss on validation set: 0.1191
2022-01-11 22:42:06,680 iteration 1020 : loss : 0.059162, loss_ce: 0.021876
 15%|████▌                         | 60/400 [29:10<2:52:13, 30.39s/it]2022-01-11 22:42:08,369 iteration 1021 : loss : 0.094614, loss_ce: 0.024467
2022-01-11 22:42:09,933 iteration 1022 : loss : 0.087734, loss_ce: 0.040601
2022-01-11 22:42:11,499 iteration 1023 : loss : 0.127440, loss_ce: 0.033971
2022-01-11 22:42:13,125 iteration 1024 : loss : 0.056751, loss_ce: 0.021089
2022-01-11 22:42:14,730 iteration 1025 : loss : 0.058524, loss_ce: 0.025374
2022-01-11 22:42:16,211 iteration 1026 : loss : 0.050217, loss_ce: 0.017725
2022-01-11 22:42:17,825 iteration 1027 : loss : 0.080027, loss_ce: 0.026189
2022-01-11 22:42:19,428 iteration 1028 : loss : 0.091128, loss_ce: 0.039181
2022-01-11 22:42:20,980 iteration 1029 : loss : 0.058201, loss_ce: 0.023351
2022-01-11 22:42:22,532 iteration 1030 : loss : 0.062372, loss_ce: 0.022221
2022-01-11 22:42:24,144 iteration 1031 : loss : 0.095461, loss_ce: 0.047261
2022-01-11 22:42:25,724 iteration 1032 : loss : 0.068290, loss_ce: 0.030282
2022-01-11 22:42:27,254 iteration 1033 : loss : 0.072148, loss_ce: 0.028321
2022-01-11 22:42:28,868 iteration 1034 : loss : 0.108257, loss_ce: 0.057109
2022-01-11 22:42:30,439 iteration 1035 : loss : 0.051848, loss_ce: 0.022583
2022-01-11 22:42:32,017 iteration 1036 : loss : 0.062709, loss_ce: 0.029911
2022-01-11 22:42:33,541 iteration 1037 : loss : 0.074219, loss_ce: 0.026689
 15%|████▌                         | 61/400 [29:37<2:45:42, 29.33s/it]2022-01-11 22:42:35,169 iteration 1038 : loss : 0.049288, loss_ce: 0.020512
2022-01-11 22:42:36,721 iteration 1039 : loss : 0.061617, loss_ce: 0.029913
2022-01-11 22:42:38,306 iteration 1040 : loss : 0.082381, loss_ce: 0.035128
2022-01-11 22:42:39,873 iteration 1041 : loss : 0.066930, loss_ce: 0.032243
2022-01-11 22:42:41,454 iteration 1042 : loss : 0.051593, loss_ce: 0.021628
2022-01-11 22:42:43,028 iteration 1043 : loss : 0.079102, loss_ce: 0.024363
2022-01-11 22:42:44,612 iteration 1044 : loss : 0.061191, loss_ce: 0.023991
2022-01-11 22:42:46,095 iteration 1045 : loss : 0.057131, loss_ce: 0.027857
2022-01-11 22:42:47,614 iteration 1046 : loss : 0.087169, loss_ce: 0.021957
2022-01-11 22:42:49,224 iteration 1047 : loss : 0.074868, loss_ce: 0.027544
2022-01-11 22:42:50,786 iteration 1048 : loss : 0.076934, loss_ce: 0.025528
2022-01-11 22:42:52,358 iteration 1049 : loss : 0.093937, loss_ce: 0.032200
2022-01-11 22:42:53,986 iteration 1050 : loss : 0.092541, loss_ce: 0.027353
2022-01-11 22:42:55,551 iteration 1051 : loss : 0.073400, loss_ce: 0.030841
2022-01-11 22:42:57,125 iteration 1052 : loss : 0.072609, loss_ce: 0.033958
2022-01-11 22:42:58,617 iteration 1053 : loss : 0.072748, loss_ce: 0.024689
2022-01-11 22:43:00,249 iteration 1054 : loss : 0.058816, loss_ce: 0.026252
 16%|████▋                         | 62/400 [30:04<2:40:48, 28.55s/it]2022-01-11 22:43:01,841 iteration 1055 : loss : 0.058432, loss_ce: 0.019103
2022-01-11 22:43:03,366 iteration 1056 : loss : 0.078657, loss_ce: 0.038635
2022-01-11 22:43:04,895 iteration 1057 : loss : 0.088422, loss_ce: 0.039919
2022-01-11 22:43:06,448 iteration 1058 : loss : 0.068399, loss_ce: 0.034125
2022-01-11 22:43:07,921 iteration 1059 : loss : 0.084727, loss_ce: 0.034275
2022-01-11 22:43:09,465 iteration 1060 : loss : 0.051346, loss_ce: 0.019965
2022-01-11 22:43:11,053 iteration 1061 : loss : 0.059754, loss_ce: 0.021555
2022-01-11 22:43:12,631 iteration 1062 : loss : 0.068581, loss_ce: 0.027657
2022-01-11 22:43:14,248 iteration 1063 : loss : 0.103197, loss_ce: 0.034288
2022-01-11 22:43:15,794 iteration 1064 : loss : 0.084637, loss_ce: 0.023512
2022-01-11 22:43:17,285 iteration 1065 : loss : 0.053404, loss_ce: 0.020761
2022-01-11 22:43:18,874 iteration 1066 : loss : 0.054354, loss_ce: 0.022962
2022-01-11 22:43:20,412 iteration 1067 : loss : 0.149140, loss_ce: 0.036982
2022-01-11 22:43:22,015 iteration 1068 : loss : 0.081386, loss_ce: 0.042289
2022-01-11 22:43:23,598 iteration 1069 : loss : 0.106252, loss_ce: 0.051800
2022-01-11 22:43:25,148 iteration 1070 : loss : 0.054596, loss_ce: 0.023461
2022-01-11 22:43:26,687 iteration 1071 : loss : 0.060120, loss_ce: 0.030942
 16%|████▋                         | 63/400 [30:30<2:36:46, 27.91s/it]2022-01-11 22:43:28,373 iteration 1072 : loss : 0.064209, loss_ce: 0.021200
2022-01-11 22:43:29,908 iteration 1073 : loss : 0.105914, loss_ce: 0.026386
2022-01-11 22:43:31,450 iteration 1074 : loss : 0.075813, loss_ce: 0.029171
2022-01-11 22:43:33,032 iteration 1075 : loss : 0.063571, loss_ce: 0.024955
2022-01-11 22:43:34,540 iteration 1076 : loss : 0.057414, loss_ce: 0.021145
2022-01-11 22:43:36,086 iteration 1077 : loss : 0.058339, loss_ce: 0.021875
2022-01-11 22:43:37,704 iteration 1078 : loss : 0.099375, loss_ce: 0.050144
2022-01-11 22:43:39,219 iteration 1079 : loss : 0.048764, loss_ce: 0.019582
2022-01-11 22:43:40,726 iteration 1080 : loss : 0.049288, loss_ce: 0.020600
2022-01-11 22:43:42,314 iteration 1081 : loss : 0.063354, loss_ce: 0.024056
2022-01-11 22:43:43,892 iteration 1082 : loss : 0.066236, loss_ce: 0.034052
2022-01-11 22:43:45,496 iteration 1083 : loss : 0.086468, loss_ce: 0.038789
2022-01-11 22:43:47,021 iteration 1084 : loss : 0.074593, loss_ce: 0.038820
2022-01-11 22:43:48,643 iteration 1085 : loss : 0.065526, loss_ce: 0.024976
2022-01-11 22:43:50,174 iteration 1086 : loss : 0.061291, loss_ce: 0.026668
2022-01-11 22:43:51,737 iteration 1087 : loss : 0.055656, loss_ce: 0.021061
2022-01-11 22:43:53,345 iteration 1088 : loss : 0.058369, loss_ce: 0.026826
 16%|████▊                         | 64/400 [30:57<2:34:12, 27.54s/it]2022-01-11 22:43:54,918 iteration 1089 : loss : 0.077446, loss_ce: 0.032096
2022-01-11 22:43:56,459 iteration 1090 : loss : 0.048555, loss_ce: 0.023000
2022-01-11 22:43:58,015 iteration 1091 : loss : 0.072568, loss_ce: 0.034971
2022-01-11 22:43:59,576 iteration 1092 : loss : 0.058603, loss_ce: 0.024883
2022-01-11 22:44:01,148 iteration 1093 : loss : 0.083998, loss_ce: 0.035174
2022-01-11 22:44:02,751 iteration 1094 : loss : 0.080438, loss_ce: 0.029654
2022-01-11 22:44:04,258 iteration 1095 : loss : 0.042739, loss_ce: 0.018512
2022-01-11 22:44:05,914 iteration 1096 : loss : 0.053906, loss_ce: 0.024093
2022-01-11 22:44:07,488 iteration 1097 : loss : 0.064308, loss_ce: 0.030576
2022-01-11 22:44:09,000 iteration 1098 : loss : 0.054678, loss_ce: 0.023418
2022-01-11 22:44:10,582 iteration 1099 : loss : 0.069733, loss_ce: 0.025449
2022-01-11 22:44:12,214 iteration 1100 : loss : 0.063937, loss_ce: 0.021532
2022-01-11 22:44:13,777 iteration 1101 : loss : 0.059920, loss_ce: 0.025503
2022-01-11 22:44:15,407 iteration 1102 : loss : 0.083628, loss_ce: 0.027180
2022-01-11 22:44:16,944 iteration 1103 : loss : 0.069925, loss_ce: 0.025728
2022-01-11 22:44:18,476 iteration 1104 : loss : 0.054778, loss_ce: 0.021432
2022-01-11 22:44:18,477 Training Data Eval:
2022-01-11 22:44:26,440   Average segmentation loss on training set: 0.0451
2022-01-11 22:44:26,441 Validation Data Eval:
2022-01-11 22:44:29,187   Average segmentation loss on validation set: 0.1088
2022-01-11 22:44:37,513 Found new lowest validation loss at iteration 1104! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 22:44:39,029 iteration 1105 : loss : 0.057199, loss_ce: 0.016346
 16%|████▉                         | 65/400 [31:43<3:04:08, 32.98s/it]2022-01-11 22:44:40,459 iteration 1106 : loss : 0.060435, loss_ce: 0.021296
2022-01-11 22:44:41,895 iteration 1107 : loss : 0.066911, loss_ce: 0.033469
2022-01-11 22:44:43,359 iteration 1108 : loss : 0.069566, loss_ce: 0.015926
2022-01-11 22:44:44,755 iteration 1109 : loss : 0.049419, loss_ce: 0.021839
2022-01-11 22:44:46,218 iteration 1110 : loss : 0.072579, loss_ce: 0.032738
2022-01-11 22:44:47,648 iteration 1111 : loss : 0.048001, loss_ce: 0.020432
2022-01-11 22:44:49,086 iteration 1112 : loss : 0.054872, loss_ce: 0.021440
2022-01-11 22:44:50,517 iteration 1113 : loss : 0.061119, loss_ce: 0.022414
2022-01-11 22:44:52,046 iteration 1114 : loss : 0.059020, loss_ce: 0.025976
2022-01-11 22:44:53,670 iteration 1115 : loss : 0.055496, loss_ce: 0.024523
2022-01-11 22:44:55,272 iteration 1116 : loss : 0.093076, loss_ce: 0.028554
2022-01-11 22:44:56,979 iteration 1117 : loss : 0.066699, loss_ce: 0.024540
2022-01-11 22:44:58,612 iteration 1118 : loss : 0.078759, loss_ce: 0.027927
2022-01-11 22:45:00,172 iteration 1119 : loss : 0.046903, loss_ce: 0.020299
2022-01-11 22:45:01,721 iteration 1120 : loss : 0.053922, loss_ce: 0.018154
2022-01-11 22:45:03,309 iteration 1121 : loss : 0.068879, loss_ce: 0.019601
2022-01-11 22:45:04,905 iteration 1122 : loss : 0.091958, loss_ce: 0.046836
 16%|████▉                         | 66/400 [32:09<2:51:43, 30.85s/it]2022-01-11 22:45:06,489 iteration 1123 : loss : 0.092766, loss_ce: 0.031982
2022-01-11 22:45:08,103 iteration 1124 : loss : 0.058968, loss_ce: 0.026896
2022-01-11 22:45:09,674 iteration 1125 : loss : 0.040622, loss_ce: 0.012331
2022-01-11 22:45:11,238 iteration 1126 : loss : 0.097532, loss_ce: 0.043226
2022-01-11 22:45:12,791 iteration 1127 : loss : 0.069670, loss_ce: 0.027285
2022-01-11 22:45:14,426 iteration 1128 : loss : 0.077274, loss_ce: 0.029450
2022-01-11 22:45:16,068 iteration 1129 : loss : 0.108577, loss_ce: 0.036626
2022-01-11 22:45:17,628 iteration 1130 : loss : 0.054408, loss_ce: 0.025287
2022-01-11 22:45:19,192 iteration 1131 : loss : 0.077039, loss_ce: 0.034076
2022-01-11 22:45:20,811 iteration 1132 : loss : 0.058862, loss_ce: 0.024035
2022-01-11 22:45:22,331 iteration 1133 : loss : 0.049864, loss_ce: 0.024616
2022-01-11 22:45:23,910 iteration 1134 : loss : 0.084354, loss_ce: 0.027932
2022-01-11 22:45:25,452 iteration 1135 : loss : 0.070064, loss_ce: 0.031210
2022-01-11 22:45:27,017 iteration 1136 : loss : 0.042555, loss_ce: 0.015724
2022-01-11 22:45:28,568 iteration 1137 : loss : 0.089968, loss_ce: 0.037751
2022-01-11 22:45:30,182 iteration 1138 : loss : 0.075784, loss_ce: 0.035848
2022-01-11 22:45:31,769 iteration 1139 : loss : 0.064797, loss_ce: 0.022324
 17%|█████                         | 67/400 [32:35<2:44:34, 29.65s/it]2022-01-11 22:45:33,388 iteration 1140 : loss : 0.081828, loss_ce: 0.033739
2022-01-11 22:45:34,929 iteration 1141 : loss : 0.068152, loss_ce: 0.027080
2022-01-11 22:45:36,561 iteration 1142 : loss : 0.062366, loss_ce: 0.029395
2022-01-11 22:45:38,136 iteration 1143 : loss : 0.065712, loss_ce: 0.024664
2022-01-11 22:45:39,739 iteration 1144 : loss : 0.069541, loss_ce: 0.035129
2022-01-11 22:45:41,263 iteration 1145 : loss : 0.046542, loss_ce: 0.018755
2022-01-11 22:45:42,849 iteration 1146 : loss : 0.097406, loss_ce: 0.045787
2022-01-11 22:45:44,338 iteration 1147 : loss : 0.067574, loss_ce: 0.027093
2022-01-11 22:45:45,960 iteration 1148 : loss : 0.055557, loss_ce: 0.022091
2022-01-11 22:45:47,481 iteration 1149 : loss : 0.057705, loss_ce: 0.027733
2022-01-11 22:45:49,054 iteration 1150 : loss : 0.063265, loss_ce: 0.027194
2022-01-11 22:45:50,651 iteration 1151 : loss : 0.078440, loss_ce: 0.033367
2022-01-11 22:45:52,339 iteration 1152 : loss : 0.065666, loss_ce: 0.025987
2022-01-11 22:45:53,829 iteration 1153 : loss : 0.047682, loss_ce: 0.018303
2022-01-11 22:45:55,440 iteration 1154 : loss : 0.094022, loss_ce: 0.033371
2022-01-11 22:45:56,977 iteration 1155 : loss : 0.069644, loss_ce: 0.023173
2022-01-11 22:45:58,597 iteration 1156 : loss : 0.079953, loss_ce: 0.038174
 17%|█████                         | 68/400 [33:02<2:39:23, 28.81s/it]2022-01-11 22:46:00,198 iteration 1157 : loss : 0.089555, loss_ce: 0.025655
2022-01-11 22:46:01,795 iteration 1158 : loss : 0.058806, loss_ce: 0.019445
2022-01-11 22:46:03,434 iteration 1159 : loss : 0.071853, loss_ce: 0.024581
2022-01-11 22:46:05,129 iteration 1160 : loss : 0.072330, loss_ce: 0.029810
2022-01-11 22:46:06,762 iteration 1161 : loss : 0.045326, loss_ce: 0.019220
2022-01-11 22:46:08,368 iteration 1162 : loss : 0.056531, loss_ce: 0.019966
2022-01-11 22:46:09,914 iteration 1163 : loss : 0.067896, loss_ce: 0.030560
2022-01-11 22:46:11,476 iteration 1164 : loss : 0.083777, loss_ce: 0.030543
2022-01-11 22:46:13,053 iteration 1165 : loss : 0.060039, loss_ce: 0.029970
2022-01-11 22:46:14,690 iteration 1166 : loss : 0.067365, loss_ce: 0.022575
2022-01-11 22:46:16,232 iteration 1167 : loss : 0.054941, loss_ce: 0.028122
2022-01-11 22:46:17,796 iteration 1168 : loss : 0.059940, loss_ce: 0.026976
2022-01-11 22:46:19,334 iteration 1169 : loss : 0.051582, loss_ce: 0.021863
2022-01-11 22:46:20,880 iteration 1170 : loss : 0.074113, loss_ce: 0.031947
2022-01-11 22:46:22,472 iteration 1171 : loss : 0.059079, loss_ce: 0.022637
2022-01-11 22:46:24,051 iteration 1172 : loss : 0.056666, loss_ce: 0.026475
2022-01-11 22:46:25,647 iteration 1173 : loss : 0.090495, loss_ce: 0.036888
 17%|█████▏                        | 69/400 [33:29<2:35:59, 28.28s/it]2022-01-11 22:46:27,230 iteration 1174 : loss : 0.064854, loss_ce: 0.022486
2022-01-11 22:46:28,797 iteration 1175 : loss : 0.053869, loss_ce: 0.020934
2022-01-11 22:46:30,354 iteration 1176 : loss : 0.045274, loss_ce: 0.017974
2022-01-11 22:46:31,884 iteration 1177 : loss : 0.048615, loss_ce: 0.019785
2022-01-11 22:46:33,476 iteration 1178 : loss : 0.049603, loss_ce: 0.015658
2022-01-11 22:46:35,124 iteration 1179 : loss : 0.069324, loss_ce: 0.029773
2022-01-11 22:46:36,656 iteration 1180 : loss : 0.066516, loss_ce: 0.028235
2022-01-11 22:46:38,212 iteration 1181 : loss : 0.049795, loss_ce: 0.018187
2022-01-11 22:46:39,783 iteration 1182 : loss : 0.059570, loss_ce: 0.021197
2022-01-11 22:46:41,301 iteration 1183 : loss : 0.055855, loss_ce: 0.025843
2022-01-11 22:46:42,824 iteration 1184 : loss : 0.041994, loss_ce: 0.020776
2022-01-11 22:46:44,420 iteration 1185 : loss : 0.059303, loss_ce: 0.021294
2022-01-11 22:46:45,954 iteration 1186 : loss : 0.063148, loss_ce: 0.033190
2022-01-11 22:46:47,577 iteration 1187 : loss : 0.053327, loss_ce: 0.020895
2022-01-11 22:46:49,111 iteration 1188 : loss : 0.063726, loss_ce: 0.025169
2022-01-11 22:46:50,752 iteration 1189 : loss : 0.053188, loss_ce: 0.021582
2022-01-11 22:46:50,752 Training Data Eval:
2022-01-11 22:46:58,746   Average segmentation loss on training set: 0.0492
2022-01-11 22:46:58,747 Validation Data Eval:
2022-01-11 22:47:01,501   Average segmentation loss on validation set: 0.0744
2022-01-11 22:47:07,311 Found new lowest validation loss at iteration 1189! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 22:47:08,789 iteration 1190 : loss : 0.069835, loss_ce: 0.031153
 18%|█████▎                        | 70/400 [34:12<3:00:04, 32.74s/it]2022-01-11 22:47:10,354 iteration 1191 : loss : 0.063867, loss_ce: 0.027803
2022-01-11 22:47:11,769 iteration 1192 : loss : 0.040016, loss_ce: 0.018821
2022-01-11 22:47:13,308 iteration 1193 : loss : 0.080080, loss_ce: 0.023524
2022-01-11 22:47:14,750 iteration 1194 : loss : 0.078898, loss_ce: 0.025504
2022-01-11 22:47:16,158 iteration 1195 : loss : 0.049970, loss_ce: 0.023690
2022-01-11 22:47:17,564 iteration 1196 : loss : 0.046639, loss_ce: 0.019828
2022-01-11 22:47:18,937 iteration 1197 : loss : 0.055235, loss_ce: 0.020378
2022-01-11 22:47:20,442 iteration 1198 : loss : 0.042362, loss_ce: 0.018935
2022-01-11 22:47:21,976 iteration 1199 : loss : 0.043733, loss_ce: 0.016232
2022-01-11 22:47:23,524 iteration 1200 : loss : 0.077388, loss_ce: 0.024882
2022-01-11 22:47:25,041 iteration 1201 : loss : 0.047114, loss_ce: 0.020490
2022-01-11 22:47:26,620 iteration 1202 : loss : 0.048496, loss_ce: 0.018471
2022-01-11 22:47:28,207 iteration 1203 : loss : 0.066674, loss_ce: 0.020198
2022-01-11 22:47:29,772 iteration 1204 : loss : 0.053379, loss_ce: 0.019127
2022-01-11 22:47:31,344 iteration 1205 : loss : 0.093260, loss_ce: 0.045107
2022-01-11 22:47:32,937 iteration 1206 : loss : 0.049849, loss_ce: 0.018970
2022-01-11 22:47:34,569 iteration 1207 : loss : 0.040862, loss_ce: 0.012256
 18%|█████▎                        | 71/400 [34:38<2:48:04, 30.65s/it]2022-01-11 22:47:36,207 iteration 1208 : loss : 0.053362, loss_ce: 0.024746
2022-01-11 22:47:37,779 iteration 1209 : loss : 0.052101, loss_ce: 0.019392
2022-01-11 22:47:39,355 iteration 1210 : loss : 0.054213, loss_ce: 0.022220
2022-01-11 22:47:40,879 iteration 1211 : loss : 0.038364, loss_ce: 0.014971
2022-01-11 22:47:42,389 iteration 1212 : loss : 0.097200, loss_ce: 0.030189
2022-01-11 22:47:44,074 iteration 1213 : loss : 0.069597, loss_ce: 0.029319
2022-01-11 22:47:45,646 iteration 1214 : loss : 0.041588, loss_ce: 0.017048
2022-01-11 22:47:47,227 iteration 1215 : loss : 0.098388, loss_ce: 0.049425
2022-01-11 22:47:48,906 iteration 1216 : loss : 0.060469, loss_ce: 0.020862
2022-01-11 22:47:50,484 iteration 1217 : loss : 0.038650, loss_ce: 0.013572
2022-01-11 22:47:52,012 iteration 1218 : loss : 0.056761, loss_ce: 0.023221
2022-01-11 22:47:53,517 iteration 1219 : loss : 0.047624, loss_ce: 0.016505
2022-01-11 22:47:55,082 iteration 1220 : loss : 0.050504, loss_ce: 0.021106
2022-01-11 22:47:56,616 iteration 1221 : loss : 0.070379, loss_ce: 0.026486
2022-01-11 22:47:58,243 iteration 1222 : loss : 0.059565, loss_ce: 0.020040
2022-01-11 22:47:59,794 iteration 1223 : loss : 0.061265, loss_ce: 0.032363
2022-01-11 22:48:01,406 iteration 1224 : loss : 0.101227, loss_ce: 0.043297
 18%|█████▍                        | 72/400 [35:05<2:41:18, 29.51s/it]2022-01-11 22:48:02,987 iteration 1225 : loss : 0.049203, loss_ce: 0.021339
2022-01-11 22:48:04,582 iteration 1226 : loss : 0.087126, loss_ce: 0.039042
2022-01-11 22:48:06,094 iteration 1227 : loss : 0.052463, loss_ce: 0.018439
2022-01-11 22:48:07,643 iteration 1228 : loss : 0.051753, loss_ce: 0.019937
2022-01-11 22:48:09,136 iteration 1229 : loss : 0.068698, loss_ce: 0.020282
2022-01-11 22:48:10,665 iteration 1230 : loss : 0.050778, loss_ce: 0.021473
2022-01-11 22:48:12,269 iteration 1231 : loss : 0.064736, loss_ce: 0.031315
2022-01-11 22:48:13,796 iteration 1232 : loss : 0.061924, loss_ce: 0.027198
2022-01-11 22:48:15,335 iteration 1233 : loss : 0.041987, loss_ce: 0.017764
2022-01-11 22:48:16,945 iteration 1234 : loss : 0.050325, loss_ce: 0.021619
2022-01-11 22:48:18,541 iteration 1235 : loss : 0.090837, loss_ce: 0.029270
2022-01-11 22:48:20,066 iteration 1236 : loss : 0.055153, loss_ce: 0.021741
2022-01-11 22:48:21,551 iteration 1237 : loss : 0.082313, loss_ce: 0.034504
2022-01-11 22:48:23,153 iteration 1238 : loss : 0.058380, loss_ce: 0.020895
2022-01-11 22:48:24,662 iteration 1239 : loss : 0.044475, loss_ce: 0.016322
2022-01-11 22:48:26,181 iteration 1240 : loss : 0.050298, loss_ce: 0.022132
2022-01-11 22:48:27,699 iteration 1241 : loss : 0.066941, loss_ce: 0.035550
 18%|█████▍                        | 73/400 [35:31<2:35:33, 28.54s/it]2022-01-11 22:48:29,310 iteration 1242 : loss : 0.095326, loss_ce: 0.057185
2022-01-11 22:48:30,830 iteration 1243 : loss : 0.077399, loss_ce: 0.025669
2022-01-11 22:48:32,355 iteration 1244 : loss : 0.063447, loss_ce: 0.026613
2022-01-11 22:48:33,931 iteration 1245 : loss : 0.071294, loss_ce: 0.027829
2022-01-11 22:48:35,487 iteration 1246 : loss : 0.046104, loss_ce: 0.017886
2022-01-11 22:48:37,004 iteration 1247 : loss : 0.062744, loss_ce: 0.027821
2022-01-11 22:48:38,576 iteration 1248 : loss : 0.081170, loss_ce: 0.025834
2022-01-11 22:48:40,097 iteration 1249 : loss : 0.041853, loss_ce: 0.014927
2022-01-11 22:48:41,741 iteration 1250 : loss : 0.074471, loss_ce: 0.027373
2022-01-11 22:48:43,403 iteration 1251 : loss : 0.063449, loss_ce: 0.030359
2022-01-11 22:48:44,973 iteration 1252 : loss : 0.060535, loss_ce: 0.025063
2022-01-11 22:48:46,466 iteration 1253 : loss : 0.049193, loss_ce: 0.018405
2022-01-11 22:48:48,025 iteration 1254 : loss : 0.053865, loss_ce: 0.029057
2022-01-11 22:48:49,593 iteration 1255 : loss : 0.049694, loss_ce: 0.019795
2022-01-11 22:48:51,142 iteration 1256 : loss : 0.060895, loss_ce: 0.025943
2022-01-11 22:48:52,695 iteration 1257 : loss : 0.070748, loss_ce: 0.022966
2022-01-11 22:48:54,263 iteration 1258 : loss : 0.038020, loss_ce: 0.016851
 18%|█████▌                        | 74/400 [35:58<2:31:50, 27.95s/it]2022-01-11 22:48:55,829 iteration 1259 : loss : 0.043377, loss_ce: 0.020271
2022-01-11 22:48:57,441 iteration 1260 : loss : 0.042513, loss_ce: 0.020285
2022-01-11 22:48:58,977 iteration 1261 : loss : 0.041608, loss_ce: 0.013457
2022-01-11 22:49:00,617 iteration 1262 : loss : 0.058592, loss_ce: 0.023994
2022-01-11 22:49:02,268 iteration 1263 : loss : 0.104211, loss_ce: 0.043510
2022-01-11 22:49:03,812 iteration 1264 : loss : 0.041798, loss_ce: 0.017988
2022-01-11 22:49:05,428 iteration 1265 : loss : 0.051191, loss_ce: 0.024754
2022-01-11 22:49:07,034 iteration 1266 : loss : 0.070456, loss_ce: 0.031997
2022-01-11 22:49:08,595 iteration 1267 : loss : 0.065988, loss_ce: 0.025918
2022-01-11 22:49:10,107 iteration 1268 : loss : 0.065310, loss_ce: 0.023968
2022-01-11 22:49:11,727 iteration 1269 : loss : 0.114777, loss_ce: 0.030645
2022-01-11 22:49:13,312 iteration 1270 : loss : 0.053472, loss_ce: 0.023428
2022-01-11 22:49:14,870 iteration 1271 : loss : 0.053923, loss_ce: 0.025557
2022-01-11 22:49:16,354 iteration 1272 : loss : 0.058718, loss_ce: 0.022341
2022-01-11 22:49:17,892 iteration 1273 : loss : 0.052093, loss_ce: 0.020922
2022-01-11 22:49:19,441 iteration 1274 : loss : 0.060247, loss_ce: 0.021426
2022-01-11 22:49:19,441 Training Data Eval:
2022-01-11 22:49:27,412   Average segmentation loss on training set: 0.0402
2022-01-11 22:49:27,412 Validation Data Eval:
2022-01-11 22:49:30,161   Average segmentation loss on validation set: 0.1246
2022-01-11 22:49:31,717 iteration 1275 : loss : 0.050772, loss_ce: 0.017870
 19%|█████▋                        | 75/400 [36:35<2:46:49, 30.80s/it]2022-01-11 22:49:33,234 iteration 1276 : loss : 0.046998, loss_ce: 0.020366
2022-01-11 22:49:34,890 iteration 1277 : loss : 0.055442, loss_ce: 0.020264
2022-01-11 22:49:36,413 iteration 1278 : loss : 0.032763, loss_ce: 0.011918
2022-01-11 22:49:38,048 iteration 1279 : loss : 0.057987, loss_ce: 0.022759
2022-01-11 22:49:39,627 iteration 1280 : loss : 0.063537, loss_ce: 0.027268
2022-01-11 22:49:41,201 iteration 1281 : loss : 0.072746, loss_ce: 0.035534
2022-01-11 22:49:42,710 iteration 1282 : loss : 0.034084, loss_ce: 0.014703
2022-01-11 22:49:44,315 iteration 1283 : loss : 0.085026, loss_ce: 0.025703
2022-01-11 22:49:45,832 iteration 1284 : loss : 0.053499, loss_ce: 0.020236
2022-01-11 22:49:47,451 iteration 1285 : loss : 0.057589, loss_ce: 0.021235
2022-01-11 22:49:49,083 iteration 1286 : loss : 0.133150, loss_ce: 0.039151
2022-01-11 22:49:50,612 iteration 1287 : loss : 0.037057, loss_ce: 0.019110
2022-01-11 22:49:52,135 iteration 1288 : loss : 0.051337, loss_ce: 0.024155
2022-01-11 22:49:53,694 iteration 1289 : loss : 0.056706, loss_ce: 0.021092
2022-01-11 22:49:55,284 iteration 1290 : loss : 0.058344, loss_ce: 0.022153
2022-01-11 22:49:56,883 iteration 1291 : loss : 0.070375, loss_ce: 0.033783
2022-01-11 22:49:58,495 iteration 1292 : loss : 0.058296, loss_ce: 0.023240
 19%|█████▋                        | 76/400 [37:02<2:39:48, 29.59s/it]2022-01-11 22:50:00,116 iteration 1293 : loss : 0.045109, loss_ce: 0.014905
2022-01-11 22:50:01,696 iteration 1294 : loss : 0.053044, loss_ce: 0.021223
2022-01-11 22:50:03,311 iteration 1295 : loss : 0.072312, loss_ce: 0.020784
2022-01-11 22:50:04,886 iteration 1296 : loss : 0.054371, loss_ce: 0.018548
2022-01-11 22:50:06,403 iteration 1297 : loss : 0.061466, loss_ce: 0.033351
2022-01-11 22:50:07,901 iteration 1298 : loss : 0.040701, loss_ce: 0.015226
2022-01-11 22:50:09,459 iteration 1299 : loss : 0.055712, loss_ce: 0.027664
2022-01-11 22:50:11,006 iteration 1300 : loss : 0.069850, loss_ce: 0.021397
2022-01-11 22:50:12,591 iteration 1301 : loss : 0.047760, loss_ce: 0.021191
2022-01-11 22:50:14,126 iteration 1302 : loss : 0.066140, loss_ce: 0.025601
2022-01-11 22:50:15,656 iteration 1303 : loss : 0.054078, loss_ce: 0.021587
2022-01-11 22:50:17,210 iteration 1304 : loss : 0.053387, loss_ce: 0.021006
2022-01-11 22:50:18,767 iteration 1305 : loss : 0.059683, loss_ce: 0.024549
2022-01-11 22:50:20,362 iteration 1306 : loss : 0.088816, loss_ce: 0.030849
2022-01-11 22:50:21,952 iteration 1307 : loss : 0.058173, loss_ce: 0.029089
2022-01-11 22:50:23,507 iteration 1308 : loss : 0.091598, loss_ce: 0.023884
2022-01-11 22:50:25,003 iteration 1309 : loss : 0.061311, loss_ce: 0.021886
 19%|█████▊                        | 77/400 [37:29<2:34:20, 28.67s/it]2022-01-11 22:50:26,669 iteration 1310 : loss : 0.049688, loss_ce: 0.018582
2022-01-11 22:50:28,208 iteration 1311 : loss : 0.052845, loss_ce: 0.024134
2022-01-11 22:50:29,721 iteration 1312 : loss : 0.038997, loss_ce: 0.015326
2022-01-11 22:50:31,257 iteration 1313 : loss : 0.055919, loss_ce: 0.014537
2022-01-11 22:50:32,820 iteration 1314 : loss : 0.046148, loss_ce: 0.021713
2022-01-11 22:50:34,403 iteration 1315 : loss : 0.072512, loss_ce: 0.026690
2022-01-11 22:50:36,056 iteration 1316 : loss : 0.080241, loss_ce: 0.034213
2022-01-11 22:50:37,600 iteration 1317 : loss : 0.082152, loss_ce: 0.043302
2022-01-11 22:50:39,224 iteration 1318 : loss : 0.142006, loss_ce: 0.028973
2022-01-11 22:50:40,811 iteration 1319 : loss : 0.078004, loss_ce: 0.036539
2022-01-11 22:50:42,352 iteration 1320 : loss : 0.051134, loss_ce: 0.022641
2022-01-11 22:50:43,926 iteration 1321 : loss : 0.084750, loss_ce: 0.027406
2022-01-11 22:50:45,435 iteration 1322 : loss : 0.078354, loss_ce: 0.035572
2022-01-11 22:50:46,974 iteration 1323 : loss : 0.046039, loss_ce: 0.016384
2022-01-11 22:50:48,544 iteration 1324 : loss : 0.047478, loss_ce: 0.021389
2022-01-11 22:50:50,145 iteration 1325 : loss : 0.080099, loss_ce: 0.028696
2022-01-11 22:50:51,614 iteration 1326 : loss : 0.064326, loss_ce: 0.024670
 20%|█████▊                        | 78/400 [37:55<2:30:32, 28.05s/it]2022-01-11 22:50:53,177 iteration 1327 : loss : 0.086806, loss_ce: 0.028414
2022-01-11 22:50:54,758 iteration 1328 : loss : 0.062202, loss_ce: 0.027874
2022-01-11 22:50:56,318 iteration 1329 : loss : 0.049953, loss_ce: 0.023418
2022-01-11 22:50:57,850 iteration 1330 : loss : 0.140975, loss_ce: 0.032710
2022-01-11 22:50:59,352 iteration 1331 : loss : 0.055984, loss_ce: 0.027431
2022-01-11 22:51:00,867 iteration 1332 : loss : 0.043088, loss_ce: 0.014280
2022-01-11 22:51:02,413 iteration 1333 : loss : 0.047683, loss_ce: 0.013615
2022-01-11 22:51:03,981 iteration 1334 : loss : 0.062113, loss_ce: 0.024209
2022-01-11 22:51:05,511 iteration 1335 : loss : 0.055014, loss_ce: 0.017135
2022-01-11 22:51:07,039 iteration 1336 : loss : 0.049734, loss_ce: 0.019608
2022-01-11 22:51:08,643 iteration 1337 : loss : 0.064374, loss_ce: 0.028387
2022-01-11 22:51:10,221 iteration 1338 : loss : 0.063089, loss_ce: 0.020683
2022-01-11 22:51:11,791 iteration 1339 : loss : 0.050194, loss_ce: 0.026207
2022-01-11 22:51:13,380 iteration 1340 : loss : 0.049740, loss_ce: 0.021941
2022-01-11 22:51:14,951 iteration 1341 : loss : 0.073775, loss_ce: 0.026914
2022-01-11 22:51:16,505 iteration 1342 : loss : 0.065610, loss_ce: 0.025623
2022-01-11 22:51:18,011 iteration 1343 : loss : 0.048632, loss_ce: 0.018040
 20%|█████▉                        | 79/400 [38:22<2:27:25, 27.55s/it]2022-01-11 22:51:19,606 iteration 1344 : loss : 0.059865, loss_ce: 0.025312
2022-01-11 22:51:21,231 iteration 1345 : loss : 0.069456, loss_ce: 0.025374
2022-01-11 22:51:22,766 iteration 1346 : loss : 0.068189, loss_ce: 0.030545
2022-01-11 22:51:24,393 iteration 1347 : loss : 0.032827, loss_ce: 0.014084
2022-01-11 22:51:26,052 iteration 1348 : loss : 0.078652, loss_ce: 0.032038
2022-01-11 22:51:27,598 iteration 1349 : loss : 0.065826, loss_ce: 0.027386
2022-01-11 22:51:29,136 iteration 1350 : loss : 0.059401, loss_ce: 0.028199
2022-01-11 22:51:30,683 iteration 1351 : loss : 0.048195, loss_ce: 0.024580
2022-01-11 22:51:32,291 iteration 1352 : loss : 0.047714, loss_ce: 0.022410
2022-01-11 22:51:33,860 iteration 1353 : loss : 0.057074, loss_ce: 0.020231
2022-01-11 22:51:35,448 iteration 1354 : loss : 0.068101, loss_ce: 0.025557
2022-01-11 22:51:37,104 iteration 1355 : loss : 0.192769, loss_ce: 0.051260
2022-01-11 22:51:38,683 iteration 1356 : loss : 0.051141, loss_ce: 0.022072
2022-01-11 22:51:40,260 iteration 1357 : loss : 0.041858, loss_ce: 0.015686
2022-01-11 22:51:41,860 iteration 1358 : loss : 0.073444, loss_ce: 0.034068
2022-01-11 22:51:43,420 iteration 1359 : loss : 0.084677, loss_ce: 0.044813
2022-01-11 22:51:43,420 Training Data Eval:
2022-01-11 22:51:51,399   Average segmentation loss on training set: 0.0443
2022-01-11 22:51:51,400 Validation Data Eval:
2022-01-11 22:51:54,144   Average segmentation loss on validation set: 0.0918
2022-01-11 22:51:55,761 iteration 1360 : loss : 0.045814, loss_ce: 0.012391
 20%|██████                        | 80/400 [38:59<2:43:16, 30.61s/it]2022-01-11 22:51:57,469 iteration 1361 : loss : 0.083645, loss_ce: 0.036568
2022-01-11 22:51:59,037 iteration 1362 : loss : 0.061526, loss_ce: 0.020113
2022-01-11 22:52:00,764 iteration 1363 : loss : 0.081374, loss_ce: 0.036525
2022-01-11 22:52:02,196 iteration 1364 : loss : 0.055225, loss_ce: 0.015359
2022-01-11 22:52:03,834 iteration 1365 : loss : 0.049385, loss_ce: 0.021027
2022-01-11 22:52:05,422 iteration 1366 : loss : 0.066829, loss_ce: 0.027380
2022-01-11 22:52:06,980 iteration 1367 : loss : 0.056138, loss_ce: 0.020294
2022-01-11 22:52:08,545 iteration 1368 : loss : 0.046660, loss_ce: 0.021772
2022-01-11 22:52:10,066 iteration 1369 : loss : 0.053022, loss_ce: 0.019488
2022-01-11 22:52:11,671 iteration 1370 : loss : 0.101495, loss_ce: 0.050621
2022-01-11 22:52:13,215 iteration 1371 : loss : 0.041892, loss_ce: 0.016706
2022-01-11 22:52:14,751 iteration 1372 : loss : 0.060908, loss_ce: 0.025322
2022-01-11 22:52:16,264 iteration 1373 : loss : 0.059583, loss_ce: 0.023077
2022-01-11 22:52:17,837 iteration 1374 : loss : 0.044183, loss_ce: 0.017688
2022-01-11 22:52:19,416 iteration 1375 : loss : 0.048664, loss_ce: 0.022779
2022-01-11 22:52:20,986 iteration 1376 : loss : 0.052961, loss_ce: 0.017277
2022-01-11 22:52:22,532 iteration 1377 : loss : 0.028581, loss_ce: 0.011032
 20%|██████                        | 81/400 [39:26<2:36:38, 29.46s/it]2022-01-11 22:52:24,060 iteration 1378 : loss : 0.059665, loss_ce: 0.023709
2022-01-11 22:52:25,650 iteration 1379 : loss : 0.050332, loss_ce: 0.019005
2022-01-11 22:52:27,214 iteration 1380 : loss : 0.046953, loss_ce: 0.020097
2022-01-11 22:52:28,837 iteration 1381 : loss : 0.052439, loss_ce: 0.019901
2022-01-11 22:52:30,320 iteration 1382 : loss : 0.048401, loss_ce: 0.019697
2022-01-11 22:52:31,937 iteration 1383 : loss : 0.049436, loss_ce: 0.018650
2022-01-11 22:52:33,498 iteration 1384 : loss : 0.053723, loss_ce: 0.020244
2022-01-11 22:52:34,995 iteration 1385 : loss : 0.068861, loss_ce: 0.027661
2022-01-11 22:52:36,525 iteration 1386 : loss : 0.042680, loss_ce: 0.016004
2022-01-11 22:52:38,059 iteration 1387 : loss : 0.054992, loss_ce: 0.022886
2022-01-11 22:52:39,622 iteration 1388 : loss : 0.059118, loss_ce: 0.018873
2022-01-11 22:52:41,177 iteration 1389 : loss : 0.066979, loss_ce: 0.027708
2022-01-11 22:52:42,777 iteration 1390 : loss : 0.045459, loss_ce: 0.019000
2022-01-11 22:52:44,384 iteration 1391 : loss : 0.051671, loss_ce: 0.018981
2022-01-11 22:52:45,976 iteration 1392 : loss : 0.047733, loss_ce: 0.024679
2022-01-11 22:52:47,517 iteration 1393 : loss : 0.058762, loss_ce: 0.019699
2022-01-11 22:52:49,114 iteration 1394 : loss : 0.040509, loss_ce: 0.017613
 20%|██████▏                       | 82/400 [39:53<2:31:33, 28.60s/it]2022-01-11 22:52:50,675 iteration 1395 : loss : 0.051004, loss_ce: 0.020409
2022-01-11 22:52:52,219 iteration 1396 : loss : 0.056333, loss_ce: 0.026842
2022-01-11 22:52:53,841 iteration 1397 : loss : 0.062353, loss_ce: 0.022325
2022-01-11 22:52:55,453 iteration 1398 : loss : 0.049976, loss_ce: 0.022564
2022-01-11 22:52:56,930 iteration 1399 : loss : 0.053555, loss_ce: 0.024092
2022-01-11 22:52:58,415 iteration 1400 : loss : 0.046286, loss_ce: 0.015653
2022-01-11 22:53:00,034 iteration 1401 : loss : 0.058040, loss_ce: 0.020783
2022-01-11 22:53:01,612 iteration 1402 : loss : 0.050676, loss_ce: 0.019464
2022-01-11 22:53:03,110 iteration 1403 : loss : 0.052707, loss_ce: 0.019279
2022-01-11 22:53:04,612 iteration 1404 : loss : 0.038395, loss_ce: 0.018512
2022-01-11 22:53:06,157 iteration 1405 : loss : 0.054936, loss_ce: 0.021012
2022-01-11 22:53:07,728 iteration 1406 : loss : 0.042213, loss_ce: 0.014509
2022-01-11 22:53:09,328 iteration 1407 : loss : 0.037718, loss_ce: 0.016239
2022-01-11 22:53:10,871 iteration 1408 : loss : 0.055708, loss_ce: 0.019402
2022-01-11 22:53:12,439 iteration 1409 : loss : 0.089843, loss_ce: 0.028453
2022-01-11 22:53:14,068 iteration 1410 : loss : 0.042058, loss_ce: 0.017631
2022-01-11 22:53:15,604 iteration 1411 : loss : 0.044861, loss_ce: 0.015446
 21%|██████▏                       | 83/400 [40:19<2:27:44, 27.96s/it]2022-01-11 22:53:17,167 iteration 1412 : loss : 0.058709, loss_ce: 0.022396
2022-01-11 22:53:18,779 iteration 1413 : loss : 0.080391, loss_ce: 0.027764
2022-01-11 22:53:20,310 iteration 1414 : loss : 0.036628, loss_ce: 0.012607
2022-01-11 22:53:21,902 iteration 1415 : loss : 0.060264, loss_ce: 0.016207
2022-01-11 22:53:23,438 iteration 1416 : loss : 0.051693, loss_ce: 0.019257
2022-01-11 22:53:24,970 iteration 1417 : loss : 0.055059, loss_ce: 0.019126
2022-01-11 22:53:26,503 iteration 1418 : loss : 0.046761, loss_ce: 0.018772
2022-01-11 22:53:28,015 iteration 1419 : loss : 0.047182, loss_ce: 0.018402
2022-01-11 22:53:29,611 iteration 1420 : loss : 0.053253, loss_ce: 0.019865
2022-01-11 22:53:31,247 iteration 1421 : loss : 0.079890, loss_ce: 0.036897
2022-01-11 22:53:32,827 iteration 1422 : loss : 0.038896, loss_ce: 0.012420
2022-01-11 22:53:34,320 iteration 1423 : loss : 0.049018, loss_ce: 0.018727
2022-01-11 22:53:35,907 iteration 1424 : loss : 0.051701, loss_ce: 0.023800
2022-01-11 22:53:37,491 iteration 1425 : loss : 0.031568, loss_ce: 0.013354
2022-01-11 22:53:39,016 iteration 1426 : loss : 0.046684, loss_ce: 0.020738
2022-01-11 22:53:40,564 iteration 1427 : loss : 0.068937, loss_ce: 0.031777
2022-01-11 22:53:42,030 iteration 1428 : loss : 0.059226, loss_ce: 0.024224
 21%|██████▎                       | 84/400 [40:46<2:24:51, 27.50s/it]2022-01-11 22:53:43,692 iteration 1429 : loss : 0.063937, loss_ce: 0.033612
2022-01-11 22:53:45,204 iteration 1430 : loss : 0.159676, loss_ce: 0.047723
2022-01-11 22:53:46,753 iteration 1431 : loss : 0.068684, loss_ce: 0.033530
2022-01-11 22:53:48,327 iteration 1432 : loss : 0.059634, loss_ce: 0.026505
2022-01-11 22:53:49,930 iteration 1433 : loss : 0.057211, loss_ce: 0.019444
2022-01-11 22:53:51,465 iteration 1434 : loss : 0.051794, loss_ce: 0.022036
2022-01-11 22:53:53,043 iteration 1435 : loss : 0.052653, loss_ce: 0.021247
2022-01-11 22:53:54,629 iteration 1436 : loss : 0.079113, loss_ce: 0.030382
2022-01-11 22:53:56,226 iteration 1437 : loss : 0.059107, loss_ce: 0.025013
2022-01-11 22:53:57,780 iteration 1438 : loss : 0.051794, loss_ce: 0.023687
2022-01-11 22:53:59,397 iteration 1439 : loss : 0.069135, loss_ce: 0.026414
2022-01-11 22:54:00,991 iteration 1440 : loss : 0.057243, loss_ce: 0.031021
2022-01-11 22:54:02,534 iteration 1441 : loss : 0.038499, loss_ce: 0.018228
2022-01-11 22:54:04,106 iteration 1442 : loss : 0.047680, loss_ce: 0.020676
2022-01-11 22:54:05,697 iteration 1443 : loss : 0.086941, loss_ce: 0.037551
2022-01-11 22:54:07,260 iteration 1444 : loss : 0.152413, loss_ce: 0.033402
2022-01-11 22:54:07,261 Training Data Eval:
2022-01-11 22:54:15,230   Average segmentation loss on training set: 0.0381
2022-01-11 22:54:15,230 Validation Data Eval:
2022-01-11 22:54:17,965   Average segmentation loss on validation set: 0.1002
2022-01-11 22:54:19,531 iteration 1445 : loss : 0.048062, loss_ce: 0.017186
 21%|██████▍                       | 85/400 [41:23<2:40:07, 30.50s/it]2022-01-11 22:54:21,217 iteration 1446 : loss : 0.059855, loss_ce: 0.019792
2022-01-11 22:54:22,776 iteration 1447 : loss : 0.058035, loss_ce: 0.020352
2022-01-11 22:54:24,285 iteration 1448 : loss : 0.073527, loss_ce: 0.042460
2022-01-11 22:54:25,874 iteration 1449 : loss : 0.057065, loss_ce: 0.019707
2022-01-11 22:54:27,534 iteration 1450 : loss : 0.047146, loss_ce: 0.024413
2022-01-11 22:54:29,197 iteration 1451 : loss : 0.060292, loss_ce: 0.022137
2022-01-11 22:54:30,723 iteration 1452 : loss : 0.047206, loss_ce: 0.016409
2022-01-11 22:54:32,267 iteration 1453 : loss : 0.061713, loss_ce: 0.028325
2022-01-11 22:54:33,846 iteration 1454 : loss : 0.087234, loss_ce: 0.034182
2022-01-11 22:54:35,367 iteration 1455 : loss : 0.042293, loss_ce: 0.020018
2022-01-11 22:54:37,004 iteration 1456 : loss : 0.124112, loss_ce: 0.043687
2022-01-11 22:54:38,567 iteration 1457 : loss : 0.057044, loss_ce: 0.026435
2022-01-11 22:54:40,121 iteration 1458 : loss : 0.043112, loss_ce: 0.015621
2022-01-11 22:54:41,634 iteration 1459 : loss : 0.043861, loss_ce: 0.012547
2022-01-11 22:54:43,254 iteration 1460 : loss : 0.085814, loss_ce: 0.038382
2022-01-11 22:54:44,818 iteration 1461 : loss : 0.067390, loss_ce: 0.031668
2022-01-11 22:54:46,415 iteration 1462 : loss : 0.049807, loss_ce: 0.022620
 22%|██████▍                       | 86/400 [41:50<2:33:57, 29.42s/it]2022-01-11 22:54:48,007 iteration 1463 : loss : 0.051576, loss_ce: 0.023633
2022-01-11 22:54:49,533 iteration 1464 : loss : 0.044437, loss_ce: 0.017231
2022-01-11 22:54:51,117 iteration 1465 : loss : 0.058473, loss_ce: 0.019974
2022-01-11 22:54:52,707 iteration 1466 : loss : 0.037980, loss_ce: 0.015961
2022-01-11 22:54:54,266 iteration 1467 : loss : 0.048835, loss_ce: 0.021143
2022-01-11 22:54:55,799 iteration 1468 : loss : 0.043789, loss_ce: 0.021585
2022-01-11 22:54:57,362 iteration 1469 : loss : 0.048846, loss_ce: 0.017558
2022-01-11 22:54:58,959 iteration 1470 : loss : 0.096595, loss_ce: 0.035987
2022-01-11 22:55:00,528 iteration 1471 : loss : 0.087105, loss_ce: 0.055624
2022-01-11 22:55:01,992 iteration 1472 : loss : 0.040617, loss_ce: 0.016128
2022-01-11 22:55:03,635 iteration 1473 : loss : 0.066185, loss_ce: 0.024443
2022-01-11 22:55:05,241 iteration 1474 : loss : 0.089481, loss_ce: 0.035170
2022-01-11 22:55:06,785 iteration 1475 : loss : 0.055295, loss_ce: 0.026459
2022-01-11 22:55:08,410 iteration 1476 : loss : 0.096319, loss_ce: 0.035300
2022-01-11 22:55:09,996 iteration 1477 : loss : 0.055420, loss_ce: 0.019619
2022-01-11 22:55:11,621 iteration 1478 : loss : 0.062975, loss_ce: 0.026565
2022-01-11 22:55:13,151 iteration 1479 : loss : 0.052209, loss_ce: 0.019318
 22%|██████▌                       | 87/400 [42:17<2:29:16, 28.61s/it]2022-01-11 22:55:14,755 iteration 1480 : loss : 0.053990, loss_ce: 0.018222
2022-01-11 22:55:16,279 iteration 1481 : loss : 0.048425, loss_ce: 0.019930
2022-01-11 22:55:17,828 iteration 1482 : loss : 0.037703, loss_ce: 0.015628
2022-01-11 22:55:19,458 iteration 1483 : loss : 0.046087, loss_ce: 0.016994
2022-01-11 22:55:21,127 iteration 1484 : loss : 0.061491, loss_ce: 0.019404
2022-01-11 22:55:22,741 iteration 1485 : loss : 0.054282, loss_ce: 0.023110
2022-01-11 22:55:24,275 iteration 1486 : loss : 0.050914, loss_ce: 0.022271
2022-01-11 22:55:25,832 iteration 1487 : loss : 0.053521, loss_ce: 0.025151
2022-01-11 22:55:27,424 iteration 1488 : loss : 0.053036, loss_ce: 0.021950
2022-01-11 22:55:29,007 iteration 1489 : loss : 0.029128, loss_ce: 0.011267
2022-01-11 22:55:30,618 iteration 1490 : loss : 0.070161, loss_ce: 0.027123
2022-01-11 22:55:32,254 iteration 1491 : loss : 0.062208, loss_ce: 0.025174
2022-01-11 22:55:33,783 iteration 1492 : loss : 0.047062, loss_ce: 0.019915
2022-01-11 22:55:35,385 iteration 1493 : loss : 0.064229, loss_ce: 0.022701
2022-01-11 22:55:36,911 iteration 1494 : loss : 0.044955, loss_ce: 0.018821
2022-01-11 22:55:38,453 iteration 1495 : loss : 0.046862, loss_ce: 0.020857
2022-01-11 22:55:40,019 iteration 1496 : loss : 0.086432, loss_ce: 0.023499
 22%|██████▌                       | 88/400 [42:44<2:26:03, 28.09s/it]2022-01-11 22:55:41,581 iteration 1497 : loss : 0.062734, loss_ce: 0.028586
2022-01-11 22:55:43,116 iteration 1498 : loss : 0.037732, loss_ce: 0.012595
2022-01-11 22:55:44,712 iteration 1499 : loss : 0.061427, loss_ce: 0.026319
2022-01-11 22:55:46,215 iteration 1500 : loss : 0.056373, loss_ce: 0.021364
2022-01-11 22:55:47,761 iteration 1501 : loss : 0.061072, loss_ce: 0.020449
2022-01-11 22:55:49,352 iteration 1502 : loss : 0.053252, loss_ce: 0.018808
2022-01-11 22:55:50,895 iteration 1503 : loss : 0.031214, loss_ce: 0.013822
2022-01-11 22:55:52,447 iteration 1504 : loss : 0.042708, loss_ce: 0.014829
2022-01-11 22:55:53,954 iteration 1505 : loss : 0.038905, loss_ce: 0.014111
2022-01-11 22:55:55,512 iteration 1506 : loss : 0.045500, loss_ce: 0.018286
2022-01-11 22:55:57,067 iteration 1507 : loss : 0.057570, loss_ce: 0.021162
2022-01-11 22:55:58,593 iteration 1508 : loss : 0.061971, loss_ce: 0.029183
2022-01-11 22:56:00,136 iteration 1509 : loss : 0.051722, loss_ce: 0.016710
2022-01-11 22:56:01,659 iteration 1510 : loss : 0.036525, loss_ce: 0.014130
2022-01-11 22:56:03,233 iteration 1511 : loss : 0.045262, loss_ce: 0.020273
2022-01-11 22:56:04,757 iteration 1512 : loss : 0.039912, loss_ce: 0.014849
2022-01-11 22:56:06,334 iteration 1513 : loss : 0.047493, loss_ce: 0.022923
 22%|██████▋                       | 89/400 [43:10<2:22:49, 27.56s/it]2022-01-11 22:56:07,901 iteration 1514 : loss : 0.057569, loss_ce: 0.020281
2022-01-11 22:56:09,450 iteration 1515 : loss : 0.070108, loss_ce: 0.028560
2022-01-11 22:56:11,016 iteration 1516 : loss : 0.045472, loss_ce: 0.016107
2022-01-11 22:56:12,563 iteration 1517 : loss : 0.043637, loss_ce: 0.019539
2022-01-11 22:56:14,154 iteration 1518 : loss : 0.052930, loss_ce: 0.017068
2022-01-11 22:56:15,692 iteration 1519 : loss : 0.040802, loss_ce: 0.016078
2022-01-11 22:56:17,227 iteration 1520 : loss : 0.048290, loss_ce: 0.017935
2022-01-11 22:56:18,758 iteration 1521 : loss : 0.058749, loss_ce: 0.022982
2022-01-11 22:56:20,406 iteration 1522 : loss : 0.041028, loss_ce: 0.014078
2022-01-11 22:56:21,924 iteration 1523 : loss : 0.047129, loss_ce: 0.023486
2022-01-11 22:56:23,503 iteration 1524 : loss : 0.082076, loss_ce: 0.034135
2022-01-11 22:56:25,043 iteration 1525 : loss : 0.032856, loss_ce: 0.014290
2022-01-11 22:56:26,660 iteration 1526 : loss : 0.048117, loss_ce: 0.022021
2022-01-11 22:56:28,273 iteration 1527 : loss : 0.079758, loss_ce: 0.043180
2022-01-11 22:56:29,872 iteration 1528 : loss : 0.046687, loss_ce: 0.017551
2022-01-11 22:56:31,405 iteration 1529 : loss : 0.036076, loss_ce: 0.014346
2022-01-11 22:56:31,405 Training Data Eval:
2022-01-11 22:56:39,377   Average segmentation loss on training set: 0.0321
2022-01-11 22:56:39,377 Validation Data Eval:
2022-01-11 22:56:42,120   Average segmentation loss on validation set: 0.0840
2022-01-11 22:56:43,678 iteration 1530 : loss : 0.057788, loss_ce: 0.024802
 22%|██████▊                       | 90/400 [43:47<2:37:33, 30.49s/it]2022-01-11 22:56:45,402 iteration 1531 : loss : 0.054250, loss_ce: 0.023452
2022-01-11 22:56:47,000 iteration 1532 : loss : 0.067628, loss_ce: 0.024601
2022-01-11 22:56:48,524 iteration 1533 : loss : 0.031504, loss_ce: 0.014344
2022-01-11 22:56:50,137 iteration 1534 : loss : 0.049796, loss_ce: 0.018795
2022-01-11 22:56:51,687 iteration 1535 : loss : 0.043073, loss_ce: 0.016631
2022-01-11 22:56:53,222 iteration 1536 : loss : 0.077502, loss_ce: 0.050845
2022-01-11 22:56:54,749 iteration 1537 : loss : 0.058159, loss_ce: 0.019128
2022-01-11 22:56:56,265 iteration 1538 : loss : 0.133110, loss_ce: 0.028206
2022-01-11 22:56:57,819 iteration 1539 : loss : 0.063122, loss_ce: 0.021923
2022-01-11 22:56:59,387 iteration 1540 : loss : 0.052561, loss_ce: 0.026012
2022-01-11 22:57:00,866 iteration 1541 : loss : 0.036951, loss_ce: 0.011505
2022-01-11 22:57:02,432 iteration 1542 : loss : 0.054384, loss_ce: 0.024435
2022-01-11 22:57:03,965 iteration 1543 : loss : 0.071253, loss_ce: 0.020189
2022-01-11 22:57:05,465 iteration 1544 : loss : 0.054388, loss_ce: 0.019516
2022-01-11 22:57:07,007 iteration 1545 : loss : 0.058007, loss_ce: 0.026282
2022-01-11 22:57:08,551 iteration 1546 : loss : 0.055594, loss_ce: 0.023664
2022-01-11 22:57:10,090 iteration 1547 : loss : 0.068989, loss_ce: 0.023236
 23%|██████▊                       | 91/400 [44:14<2:30:44, 29.27s/it]2022-01-11 22:57:11,687 iteration 1548 : loss : 0.046918, loss_ce: 0.017844
2022-01-11 22:57:13,188 iteration 1549 : loss : 0.043846, loss_ce: 0.018663
2022-01-11 22:57:14,837 iteration 1550 : loss : 0.053310, loss_ce: 0.023988
2022-01-11 22:57:16,385 iteration 1551 : loss : 0.052297, loss_ce: 0.021715
2022-01-11 22:57:17,934 iteration 1552 : loss : 0.052099, loss_ce: 0.020970
2022-01-11 22:57:19,424 iteration 1553 : loss : 0.036762, loss_ce: 0.019230
2022-01-11 22:57:20,950 iteration 1554 : loss : 0.077214, loss_ce: 0.031154
2022-01-11 22:57:22,574 iteration 1555 : loss : 0.048832, loss_ce: 0.018811
2022-01-11 22:57:24,247 iteration 1556 : loss : 0.044541, loss_ce: 0.017199
2022-01-11 22:57:25,853 iteration 1557 : loss : 0.093408, loss_ce: 0.023352
2022-01-11 22:57:27,420 iteration 1558 : loss : 0.063989, loss_ce: 0.018573
2022-01-11 22:57:29,077 iteration 1559 : loss : 0.046227, loss_ce: 0.019928
2022-01-11 22:57:30,605 iteration 1560 : loss : 0.062332, loss_ce: 0.018402
2022-01-11 22:57:32,156 iteration 1561 : loss : 0.059313, loss_ce: 0.023540
2022-01-11 22:57:33,723 iteration 1562 : loss : 0.057564, loss_ce: 0.027196
2022-01-11 22:57:35,269 iteration 1563 : loss : 0.066261, loss_ce: 0.037509
2022-01-11 22:57:36,827 iteration 1564 : loss : 0.051171, loss_ce: 0.017707
 23%|██████▉                       | 92/400 [44:40<2:26:21, 28.51s/it]2022-01-11 22:57:38,447 iteration 1565 : loss : 0.050547, loss_ce: 0.019679
2022-01-11 22:57:39,977 iteration 1566 : loss : 0.052781, loss_ce: 0.024815
2022-01-11 22:57:41,573 iteration 1567 : loss : 0.058362, loss_ce: 0.017076
2022-01-11 22:57:43,215 iteration 1568 : loss : 0.047133, loss_ce: 0.020889
2022-01-11 22:57:44,750 iteration 1569 : loss : 0.046202, loss_ce: 0.019498
2022-01-11 22:57:46,325 iteration 1570 : loss : 0.045780, loss_ce: 0.017841
2022-01-11 22:57:47,869 iteration 1571 : loss : 0.054470, loss_ce: 0.023852
2022-01-11 22:57:49,427 iteration 1572 : loss : 0.070676, loss_ce: 0.030281
2022-01-11 22:57:51,003 iteration 1573 : loss : 0.084712, loss_ce: 0.024790
2022-01-11 22:57:52,569 iteration 1574 : loss : 0.045065, loss_ce: 0.021264
2022-01-11 22:57:54,154 iteration 1575 : loss : 0.086297, loss_ce: 0.034894
2022-01-11 22:57:55,702 iteration 1576 : loss : 0.053575, loss_ce: 0.017222
2022-01-11 22:57:57,216 iteration 1577 : loss : 0.072396, loss_ce: 0.023473
2022-01-11 22:57:58,692 iteration 1578 : loss : 0.047170, loss_ce: 0.019017
2022-01-11 22:58:00,243 iteration 1579 : loss : 0.033877, loss_ce: 0.013432
2022-01-11 22:58:01,822 iteration 1580 : loss : 0.041706, loss_ce: 0.015121
2022-01-11 22:58:03,405 iteration 1581 : loss : 0.053892, loss_ce: 0.026334
 23%|██████▉                       | 93/400 [45:07<2:22:54, 27.93s/it]2022-01-11 22:58:05,031 iteration 1582 : loss : 0.049100, loss_ce: 0.022719
2022-01-11 22:58:06,588 iteration 1583 : loss : 0.051826, loss_ce: 0.014492
2022-01-11 22:58:08,093 iteration 1584 : loss : 0.041266, loss_ce: 0.013980
2022-01-11 22:58:09,639 iteration 1585 : loss : 0.033856, loss_ce: 0.015232
2022-01-11 22:58:11,131 iteration 1586 : loss : 0.042253, loss_ce: 0.020044
2022-01-11 22:58:12,668 iteration 1587 : loss : 0.064640, loss_ce: 0.025294
2022-01-11 22:58:14,209 iteration 1588 : loss : 0.038728, loss_ce: 0.011033
2022-01-11 22:58:15,772 iteration 1589 : loss : 0.042262, loss_ce: 0.017916
2022-01-11 22:58:17,395 iteration 1590 : loss : 0.050215, loss_ce: 0.015428
2022-01-11 22:58:18,934 iteration 1591 : loss : 0.041975, loss_ce: 0.020975
2022-01-11 22:58:20,411 iteration 1592 : loss : 0.031261, loss_ce: 0.013450
2022-01-11 22:58:21,963 iteration 1593 : loss : 0.084876, loss_ce: 0.026154
2022-01-11 22:58:23,507 iteration 1594 : loss : 0.051013, loss_ce: 0.017035
2022-01-11 22:58:25,021 iteration 1595 : loss : 0.046478, loss_ce: 0.024696
2022-01-11 22:58:26,499 iteration 1596 : loss : 0.054164, loss_ce: 0.020938
2022-01-11 22:58:28,068 iteration 1597 : loss : 0.067841, loss_ce: 0.018743
2022-01-11 22:58:29,648 iteration 1598 : loss : 0.056187, loss_ce: 0.024000
 24%|███████                       | 94/400 [45:33<2:19:51, 27.42s/it]2022-01-11 22:58:31,285 iteration 1599 : loss : 0.050289, loss_ce: 0.024279
2022-01-11 22:58:32,823 iteration 1600 : loss : 0.038796, loss_ce: 0.015669
2022-01-11 22:58:34,423 iteration 1601 : loss : 0.045027, loss_ce: 0.015987
2022-01-11 22:58:35,991 iteration 1602 : loss : 0.054576, loss_ce: 0.019028
2022-01-11 22:58:37,536 iteration 1603 : loss : 0.042510, loss_ce: 0.014514
2022-01-11 22:58:39,071 iteration 1604 : loss : 0.052033, loss_ce: 0.012456
2022-01-11 22:58:40,614 iteration 1605 : loss : 0.060598, loss_ce: 0.023976
2022-01-11 22:58:42,183 iteration 1606 : loss : 0.036147, loss_ce: 0.014051
2022-01-11 22:58:43,728 iteration 1607 : loss : 0.064367, loss_ce: 0.025463
2022-01-11 22:58:45,350 iteration 1608 : loss : 0.088094, loss_ce: 0.039296
2022-01-11 22:58:46,873 iteration 1609 : loss : 0.044752, loss_ce: 0.018847
2022-01-11 22:58:48,522 iteration 1610 : loss : 0.047930, loss_ce: 0.017502
2022-01-11 22:58:50,155 iteration 1611 : loss : 0.043581, loss_ce: 0.021172
2022-01-11 22:58:51,779 iteration 1612 : loss : 0.043756, loss_ce: 0.017628
2022-01-11 22:58:53,332 iteration 1613 : loss : 0.051706, loss_ce: 0.020089
2022-01-11 22:58:54,942 iteration 1614 : loss : 0.053894, loss_ce: 0.022438
2022-01-11 22:58:54,942 Training Data Eval:
2022-01-11 22:59:02,909   Average segmentation loss on training set: 0.0377
2022-01-11 22:59:02,910 Validation Data Eval:
2022-01-11 22:59:05,653   Average segmentation loss on validation set: 0.0747
2022-01-11 22:59:07,266 iteration 1615 : loss : 0.057008, loss_ce: 0.021748
 24%|███████▏                      | 95/400 [46:11<2:34:57, 30.48s/it]2022-01-11 22:59:08,877 iteration 1616 : loss : 0.047913, loss_ce: 0.018090
2022-01-11 22:59:10,430 iteration 1617 : loss : 0.059694, loss_ce: 0.028298
2022-01-11 22:59:11,942 iteration 1618 : loss : 0.040260, loss_ce: 0.013456
2022-01-11 22:59:13,502 iteration 1619 : loss : 0.052657, loss_ce: 0.028779
2022-01-11 22:59:15,164 iteration 1620 : loss : 0.041144, loss_ce: 0.013888
2022-01-11 22:59:16,825 iteration 1621 : loss : 0.055622, loss_ce: 0.023814
2022-01-11 22:59:18,379 iteration 1622 : loss : 0.042408, loss_ce: 0.018734
2022-01-11 22:59:19,900 iteration 1623 : loss : 0.039385, loss_ce: 0.013264
2022-01-11 22:59:21,472 iteration 1624 : loss : 0.040997, loss_ce: 0.016913
2022-01-11 22:59:22,971 iteration 1625 : loss : 0.037420, loss_ce: 0.014058
2022-01-11 22:59:24,509 iteration 1626 : loss : 0.037856, loss_ce: 0.016150
2022-01-11 22:59:26,176 iteration 1627 : loss : 0.066411, loss_ce: 0.027947
2022-01-11 22:59:27,809 iteration 1628 : loss : 0.049717, loss_ce: 0.019859
2022-01-11 22:59:29,340 iteration 1629 : loss : 0.033538, loss_ce: 0.011147
2022-01-11 22:59:30,856 iteration 1630 : loss : 0.029907, loss_ce: 0.011254
2022-01-11 22:59:32,368 iteration 1631 : loss : 0.051426, loss_ce: 0.017156
2022-01-11 22:59:33,883 iteration 1632 : loss : 0.039800, loss_ce: 0.016788
 24%|███████▏                      | 96/400 [46:38<2:28:33, 29.32s/it]2022-01-11 22:59:35,473 iteration 1633 : loss : 0.064846, loss_ce: 0.023306
2022-01-11 22:59:37,053 iteration 1634 : loss : 0.047499, loss_ce: 0.017961
2022-01-11 22:59:38,586 iteration 1635 : loss : 0.053611, loss_ce: 0.017307
2022-01-11 22:59:40,083 iteration 1636 : loss : 0.036503, loss_ce: 0.017599
2022-01-11 22:59:41,655 iteration 1637 : loss : 0.045092, loss_ce: 0.013607
2022-01-11 22:59:43,204 iteration 1638 : loss : 0.048751, loss_ce: 0.023735
2022-01-11 22:59:44,814 iteration 1639 : loss : 0.056535, loss_ce: 0.027343
2022-01-11 22:59:46,340 iteration 1640 : loss : 0.049365, loss_ce: 0.014876
2022-01-11 22:59:47,971 iteration 1641 : loss : 0.047451, loss_ce: 0.016838
2022-01-11 22:59:49,484 iteration 1642 : loss : 0.038498, loss_ce: 0.013034
2022-01-11 22:59:50,982 iteration 1643 : loss : 0.041497, loss_ce: 0.013576
2022-01-11 22:59:52,588 iteration 1644 : loss : 0.043866, loss_ce: 0.017250
2022-01-11 22:59:54,224 iteration 1645 : loss : 0.049971, loss_ce: 0.020767
2022-01-11 22:59:55,744 iteration 1646 : loss : 0.055563, loss_ce: 0.019760
2022-01-11 22:59:57,275 iteration 1647 : loss : 0.064835, loss_ce: 0.026465
2022-01-11 22:59:58,749 iteration 1648 : loss : 0.039955, loss_ce: 0.016494
2022-01-11 23:00:00,257 iteration 1649 : loss : 0.038178, loss_ce: 0.013886
 24%|███████▎                      | 97/400 [47:04<2:23:37, 28.44s/it]2022-01-11 23:00:01,956 iteration 1650 : loss : 0.080995, loss_ce: 0.048208
2022-01-11 23:00:03,495 iteration 1651 : loss : 0.059723, loss_ce: 0.017859
2022-01-11 23:00:05,067 iteration 1652 : loss : 0.062895, loss_ce: 0.022211
2022-01-11 23:00:06,606 iteration 1653 : loss : 0.037693, loss_ce: 0.012870
2022-01-11 23:00:08,148 iteration 1654 : loss : 0.048978, loss_ce: 0.016000
2022-01-11 23:00:09,664 iteration 1655 : loss : 0.051619, loss_ce: 0.018203
2022-01-11 23:00:11,272 iteration 1656 : loss : 0.059043, loss_ce: 0.019394
2022-01-11 23:00:12,819 iteration 1657 : loss : 0.059588, loss_ce: 0.024380
2022-01-11 23:00:14,427 iteration 1658 : loss : 0.059881, loss_ce: 0.036909
2022-01-11 23:00:16,056 iteration 1659 : loss : 0.063717, loss_ce: 0.026323
2022-01-11 23:00:17,607 iteration 1660 : loss : 0.043525, loss_ce: 0.018504
2022-01-11 23:00:19,109 iteration 1661 : loss : 0.047622, loss_ce: 0.017518
2022-01-11 23:00:20,699 iteration 1662 : loss : 0.047253, loss_ce: 0.017533
2022-01-11 23:00:22,298 iteration 1663 : loss : 0.044961, loss_ce: 0.022339
2022-01-11 23:00:23,848 iteration 1664 : loss : 0.061054, loss_ce: 0.020850
2022-01-11 23:00:25,394 iteration 1665 : loss : 0.050146, loss_ce: 0.022878
2022-01-11 23:00:26,953 iteration 1666 : loss : 0.044323, loss_ce: 0.013308
 24%|███████▎                      | 98/400 [47:31<2:20:29, 27.91s/it]2022-01-11 23:00:28,496 iteration 1667 : loss : 0.042189, loss_ce: 0.020190
2022-01-11 23:00:30,115 iteration 1668 : loss : 0.053880, loss_ce: 0.018868
2022-01-11 23:00:31,693 iteration 1669 : loss : 0.066320, loss_ce: 0.024675
2022-01-11 23:00:33,166 iteration 1670 : loss : 0.033099, loss_ce: 0.010298
2022-01-11 23:00:34,682 iteration 1671 : loss : 0.035489, loss_ce: 0.015364
2022-01-11 23:00:36,290 iteration 1672 : loss : 0.042226, loss_ce: 0.016120
2022-01-11 23:00:37,878 iteration 1673 : loss : 0.047043, loss_ce: 0.015373
2022-01-11 23:00:39,511 iteration 1674 : loss : 0.066257, loss_ce: 0.027092
2022-01-11 23:00:41,035 iteration 1675 : loss : 0.062285, loss_ce: 0.019775
2022-01-11 23:00:42,629 iteration 1676 : loss : 0.043554, loss_ce: 0.021539
2022-01-11 23:00:44,196 iteration 1677 : loss : 0.072229, loss_ce: 0.019823
2022-01-11 23:00:45,795 iteration 1678 : loss : 0.041207, loss_ce: 0.021036
2022-01-11 23:00:47,327 iteration 1679 : loss : 0.063103, loss_ce: 0.023493
2022-01-11 23:00:48,902 iteration 1680 : loss : 0.042534, loss_ce: 0.019977
2022-01-11 23:00:50,448 iteration 1681 : loss : 0.051801, loss_ce: 0.022363
2022-01-11 23:00:52,007 iteration 1682 : loss : 0.047088, loss_ce: 0.022644
2022-01-11 23:00:53,543 iteration 1683 : loss : 0.050769, loss_ce: 0.027643
 25%|███████▍                      | 99/400 [47:57<2:18:03, 27.52s/it]2022-01-11 23:00:55,165 iteration 1684 : loss : 0.053415, loss_ce: 0.026018
2022-01-11 23:00:56,681 iteration 1685 : loss : 0.038629, loss_ce: 0.018898
2022-01-11 23:00:58,178 iteration 1686 : loss : 0.033193, loss_ce: 0.012802
2022-01-11 23:00:59,701 iteration 1687 : loss : 0.040925, loss_ce: 0.016448
2022-01-11 23:01:01,258 iteration 1688 : loss : 0.058196, loss_ce: 0.027382
2022-01-11 23:01:02,768 iteration 1689 : loss : 0.046854, loss_ce: 0.015983
2022-01-11 23:01:04,353 iteration 1690 : loss : 0.067646, loss_ce: 0.027253
2022-01-11 23:01:05,887 iteration 1691 : loss : 0.035052, loss_ce: 0.014643
2022-01-11 23:01:07,470 iteration 1692 : loss : 0.056866, loss_ce: 0.020305
2022-01-11 23:01:09,082 iteration 1693 : loss : 0.044135, loss_ce: 0.019726
2022-01-11 23:01:10,669 iteration 1694 : loss : 0.039630, loss_ce: 0.015345
2022-01-11 23:01:12,151 iteration 1695 : loss : 0.033116, loss_ce: 0.011062
2022-01-11 23:01:13,692 iteration 1696 : loss : 0.076325, loss_ce: 0.031249
2022-01-11 23:01:15,275 iteration 1697 : loss : 0.047951, loss_ce: 0.018074
2022-01-11 23:01:16,818 iteration 1698 : loss : 0.032845, loss_ce: 0.010226
2022-01-11 23:01:18,279 iteration 1699 : loss : 0.044278, loss_ce: 0.021113
2022-01-11 23:01:18,279 Training Data Eval:
2022-01-11 23:01:26,245   Average segmentation loss on training set: 0.0292
2022-01-11 23:01:26,245 Validation Data Eval:
2022-01-11 23:01:28,992   Average segmentation loss on validation set: 0.0933
2022-01-11 23:01:30,570 iteration 1700 : loss : 0.047223, loss_ce: 0.014100
 25%|███████▎                     | 100/400 [48:34<2:31:50, 30.37s/it]2022-01-11 23:01:32,204 iteration 1701 : loss : 0.041633, loss_ce: 0.017177
2022-01-11 23:01:33,768 iteration 1702 : loss : 0.039861, loss_ce: 0.018008
2022-01-11 23:01:35,278 iteration 1703 : loss : 0.037248, loss_ce: 0.014840
2022-01-11 23:01:36,895 iteration 1704 : loss : 0.057821, loss_ce: 0.020605
2022-01-11 23:01:38,438 iteration 1705 : loss : 0.048317, loss_ce: 0.022941
2022-01-11 23:01:40,012 iteration 1706 : loss : 0.045265, loss_ce: 0.021636
2022-01-11 23:01:41,572 iteration 1707 : loss : 0.044192, loss_ce: 0.017017
2022-01-11 23:01:43,150 iteration 1708 : loss : 0.048441, loss_ce: 0.019160
2022-01-11 23:01:44,635 iteration 1709 : loss : 0.038738, loss_ce: 0.017715
2022-01-11 23:01:46,124 iteration 1710 : loss : 0.035931, loss_ce: 0.015374
2022-01-11 23:01:47,688 iteration 1711 : loss : 0.047017, loss_ce: 0.015975
2022-01-11 23:01:49,196 iteration 1712 : loss : 0.030505, loss_ce: 0.009834
2022-01-11 23:01:50,697 iteration 1713 : loss : 0.036006, loss_ce: 0.012796
2022-01-11 23:01:52,330 iteration 1714 : loss : 0.062842, loss_ce: 0.018113
2022-01-11 23:01:53,907 iteration 1715 : loss : 0.073482, loss_ce: 0.016130
2022-01-11 23:01:55,524 iteration 1716 : loss : 0.065017, loss_ce: 0.022478
2022-01-11 23:01:57,075 iteration 1717 : loss : 0.050292, loss_ce: 0.019920
 25%|███████▎                     | 101/400 [49:01<2:25:34, 29.21s/it]2022-01-11 23:01:58,636 iteration 1718 : loss : 0.046785, loss_ce: 0.016756
2022-01-11 23:02:00,207 iteration 1719 : loss : 0.046422, loss_ce: 0.022726
2022-01-11 23:02:01,681 iteration 1720 : loss : 0.041848, loss_ce: 0.014124
2022-01-11 23:02:03,222 iteration 1721 : loss : 0.041632, loss_ce: 0.018240
2022-01-11 23:02:04,811 iteration 1722 : loss : 0.044537, loss_ce: 0.014140
2022-01-11 23:02:06,352 iteration 1723 : loss : 0.063865, loss_ce: 0.030602
2022-01-11 23:02:07,937 iteration 1724 : loss : 0.044747, loss_ce: 0.014767
2022-01-11 23:02:09,587 iteration 1725 : loss : 0.073454, loss_ce: 0.025305
2022-01-11 23:02:11,205 iteration 1726 : loss : 0.068918, loss_ce: 0.019525
2022-01-11 23:02:12,757 iteration 1727 : loss : 0.032054, loss_ce: 0.015979
2022-01-11 23:02:14,287 iteration 1728 : loss : 0.038153, loss_ce: 0.013239
2022-01-11 23:02:15,831 iteration 1729 : loss : 0.033341, loss_ce: 0.013613
2022-01-11 23:02:17,398 iteration 1730 : loss : 0.037728, loss_ce: 0.014872
2022-01-11 23:02:19,009 iteration 1731 : loss : 0.040989, loss_ce: 0.014099
2022-01-11 23:02:20,557 iteration 1732 : loss : 0.030753, loss_ce: 0.014360
2022-01-11 23:02:22,171 iteration 1733 : loss : 0.061337, loss_ce: 0.025499
2022-01-11 23:02:23,668 iteration 1734 : loss : 0.040377, loss_ce: 0.013541
 26%|███████▍                     | 102/400 [49:27<2:21:10, 28.42s/it]2022-01-11 23:02:25,222 iteration 1735 : loss : 0.057983, loss_ce: 0.024895
2022-01-11 23:02:26,759 iteration 1736 : loss : 0.046665, loss_ce: 0.018116
2022-01-11 23:02:28,319 iteration 1737 : loss : 0.051674, loss_ce: 0.020448
2022-01-11 23:02:29,840 iteration 1738 : loss : 0.046971, loss_ce: 0.018642
2022-01-11 23:02:31,474 iteration 1739 : loss : 0.045459, loss_ce: 0.020050
2022-01-11 23:02:33,051 iteration 1740 : loss : 0.063480, loss_ce: 0.024770
2022-01-11 23:02:34,589 iteration 1741 : loss : 0.041380, loss_ce: 0.014796
2022-01-11 23:02:36,234 iteration 1742 : loss : 0.048693, loss_ce: 0.021578
2022-01-11 23:02:37,773 iteration 1743 : loss : 0.047189, loss_ce: 0.014416
2022-01-11 23:02:39,355 iteration 1744 : loss : 0.040251, loss_ce: 0.014912
2022-01-11 23:02:41,041 iteration 1745 : loss : 0.057340, loss_ce: 0.032868
2022-01-11 23:02:42,580 iteration 1746 : loss : 0.035655, loss_ce: 0.014716
2022-01-11 23:02:44,106 iteration 1747 : loss : 0.042322, loss_ce: 0.018213
2022-01-11 23:02:45,659 iteration 1748 : loss : 0.037770, loss_ce: 0.014703
2022-01-11 23:02:47,209 iteration 1749 : loss : 0.037240, loss_ce: 0.012634
2022-01-11 23:02:48,714 iteration 1750 : loss : 0.039851, loss_ce: 0.018250
2022-01-11 23:02:50,358 iteration 1751 : loss : 0.056648, loss_ce: 0.017134
 26%|███████▍                     | 103/400 [49:54<2:18:07, 27.90s/it]2022-01-11 23:02:51,912 iteration 1752 : loss : 0.037163, loss_ce: 0.013155
2022-01-11 23:02:53,420 iteration 1753 : loss : 0.049676, loss_ce: 0.023712
2022-01-11 23:02:54,951 iteration 1754 : loss : 0.038056, loss_ce: 0.015228
2022-01-11 23:02:56,497 iteration 1755 : loss : 0.037573, loss_ce: 0.014822
2022-01-11 23:02:58,062 iteration 1756 : loss : 0.047136, loss_ce: 0.020923
2022-01-11 23:02:59,672 iteration 1757 : loss : 0.050384, loss_ce: 0.024972
2022-01-11 23:03:01,252 iteration 1758 : loss : 0.061780, loss_ce: 0.022412
2022-01-11 23:03:02,737 iteration 1759 : loss : 0.076363, loss_ce: 0.026385
2022-01-11 23:03:04,279 iteration 1760 : loss : 0.042685, loss_ce: 0.018098
2022-01-11 23:03:05,800 iteration 1761 : loss : 0.040230, loss_ce: 0.015739
2022-01-11 23:03:07,345 iteration 1762 : loss : 0.045551, loss_ce: 0.017045
2022-01-11 23:03:08,821 iteration 1763 : loss : 0.043272, loss_ce: 0.018436
2022-01-11 23:03:10,453 iteration 1764 : loss : 0.031158, loss_ce: 0.008372
2022-01-11 23:03:11,998 iteration 1765 : loss : 0.046047, loss_ce: 0.015620
2022-01-11 23:03:13,601 iteration 1766 : loss : 0.046542, loss_ce: 0.017478
2022-01-11 23:03:15,167 iteration 1767 : loss : 0.057694, loss_ce: 0.019232
2022-01-11 23:03:16,710 iteration 1768 : loss : 0.057610, loss_ce: 0.021290
 26%|███████▌                     | 104/400 [50:20<2:15:22, 27.44s/it]2022-01-11 23:03:18,334 iteration 1769 : loss : 0.045099, loss_ce: 0.016896
2022-01-11 23:03:19,865 iteration 1770 : loss : 0.035404, loss_ce: 0.011059
2022-01-11 23:03:21,368 iteration 1771 : loss : 0.023947, loss_ce: 0.007774
2022-01-11 23:03:22,971 iteration 1772 : loss : 0.053662, loss_ce: 0.023614
2022-01-11 23:03:24,533 iteration 1773 : loss : 0.048454, loss_ce: 0.020663
2022-01-11 23:03:26,056 iteration 1774 : loss : 0.054803, loss_ce: 0.018549
2022-01-11 23:03:27,594 iteration 1775 : loss : 0.042493, loss_ce: 0.015316
2022-01-11 23:03:29,115 iteration 1776 : loss : 0.048244, loss_ce: 0.017591
2022-01-11 23:03:30,618 iteration 1777 : loss : 0.042098, loss_ce: 0.017563
2022-01-11 23:03:32,207 iteration 1778 : loss : 0.050474, loss_ce: 0.021166
2022-01-11 23:03:33,755 iteration 1779 : loss : 0.063327, loss_ce: 0.030717
2022-01-11 23:03:35,327 iteration 1780 : loss : 0.077429, loss_ce: 0.022584
2022-01-11 23:03:36,899 iteration 1781 : loss : 0.056444, loss_ce: 0.021799
2022-01-11 23:03:38,386 iteration 1782 : loss : 0.071710, loss_ce: 0.016797
2022-01-11 23:03:39,889 iteration 1783 : loss : 0.054908, loss_ce: 0.026421
2022-01-11 23:03:41,462 iteration 1784 : loss : 0.052369, loss_ce: 0.027042
2022-01-11 23:03:41,462 Training Data Eval:
2022-01-11 23:03:49,418   Average segmentation loss on training set: 0.0317
2022-01-11 23:03:49,419 Validation Data Eval:
2022-01-11 23:03:52,157   Average segmentation loss on validation set: 0.0848
2022-01-11 23:03:53,726 iteration 1785 : loss : 0.050684, loss_ce: 0.023488
 26%|███████▌                     | 105/400 [50:57<2:29:02, 30.31s/it]2022-01-11 23:03:55,363 iteration 1786 : loss : 0.043642, loss_ce: 0.014137
2022-01-11 23:03:56,873 iteration 1787 : loss : 0.038198, loss_ce: 0.012361
2022-01-11 23:03:58,447 iteration 1788 : loss : 0.051610, loss_ce: 0.020668
2022-01-11 23:04:00,028 iteration 1789 : loss : 0.046306, loss_ce: 0.022444
2022-01-11 23:04:01,722 iteration 1790 : loss : 0.037256, loss_ce: 0.012238
2022-01-11 23:04:03,297 iteration 1791 : loss : 0.045047, loss_ce: 0.018486
2022-01-11 23:04:04,876 iteration 1792 : loss : 0.039667, loss_ce: 0.015172
2022-01-11 23:04:06,465 iteration 1793 : loss : 0.081646, loss_ce: 0.041938
2022-01-11 23:04:08,018 iteration 1794 : loss : 0.041940, loss_ce: 0.022937
2022-01-11 23:04:09,486 iteration 1795 : loss : 0.034717, loss_ce: 0.013538
2022-01-11 23:04:11,116 iteration 1796 : loss : 0.044971, loss_ce: 0.020052
2022-01-11 23:04:12,625 iteration 1797 : loss : 0.042501, loss_ce: 0.017574
2022-01-11 23:04:14,185 iteration 1798 : loss : 0.043549, loss_ce: 0.015482
2022-01-11 23:04:15,804 iteration 1799 : loss : 0.043369, loss_ce: 0.017480
2022-01-11 23:04:17,303 iteration 1800 : loss : 0.039565, loss_ce: 0.012644
2022-01-11 23:04:18,868 iteration 1801 : loss : 0.064015, loss_ce: 0.022711
2022-01-11 23:04:20,481 iteration 1802 : loss : 0.054264, loss_ce: 0.023688
 26%|███████▋                     | 106/400 [51:24<2:23:17, 29.24s/it]2022-01-11 23:04:22,115 iteration 1803 : loss : 0.036165, loss_ce: 0.014205
2022-01-11 23:04:23,612 iteration 1804 : loss : 0.041336, loss_ce: 0.015196
2022-01-11 23:04:25,288 iteration 1805 : loss : 0.063252, loss_ce: 0.026891
2022-01-11 23:04:26,911 iteration 1806 : loss : 0.049499, loss_ce: 0.024144
2022-01-11 23:04:28,457 iteration 1807 : loss : 0.046271, loss_ce: 0.019837
2022-01-11 23:04:30,050 iteration 1808 : loss : 0.032027, loss_ce: 0.013536
2022-01-11 23:04:31,584 iteration 1809 : loss : 0.046404, loss_ce: 0.021697
2022-01-11 23:04:33,176 iteration 1810 : loss : 0.046120, loss_ce: 0.023365
2022-01-11 23:04:34,711 iteration 1811 : loss : 0.046172, loss_ce: 0.015407
2022-01-11 23:04:36,239 iteration 1812 : loss : 0.041540, loss_ce: 0.014283
2022-01-11 23:04:37,796 iteration 1813 : loss : 0.043136, loss_ce: 0.019359
2022-01-11 23:04:39,341 iteration 1814 : loss : 0.028834, loss_ce: 0.011597
2022-01-11 23:04:40,954 iteration 1815 : loss : 0.063046, loss_ce: 0.021534
2022-01-11 23:04:42,516 iteration 1816 : loss : 0.031302, loss_ce: 0.012635
2022-01-11 23:04:44,130 iteration 1817 : loss : 0.041269, loss_ce: 0.017569
2022-01-11 23:04:45,616 iteration 1818 : loss : 0.031728, loss_ce: 0.010123
2022-01-11 23:04:47,200 iteration 1819 : loss : 0.051897, loss_ce: 0.016422
 27%|███████▊                     | 107/400 [51:51<2:19:06, 28.49s/it]2022-01-11 23:04:48,694 iteration 1820 : loss : 0.034090, loss_ce: 0.015100
2022-01-11 23:04:50,350 iteration 1821 : loss : 0.050387, loss_ce: 0.016792
2022-01-11 23:04:51,937 iteration 1822 : loss : 0.050052, loss_ce: 0.025480
2022-01-11 23:04:53,556 iteration 1823 : loss : 0.075972, loss_ce: 0.027764
2022-01-11 23:04:55,142 iteration 1824 : loss : 0.056037, loss_ce: 0.017089
2022-01-11 23:04:56,650 iteration 1825 : loss : 0.032444, loss_ce: 0.013760
2022-01-11 23:04:58,269 iteration 1826 : loss : 0.065222, loss_ce: 0.021751
2022-01-11 23:04:59,772 iteration 1827 : loss : 0.031847, loss_ce: 0.010404
2022-01-11 23:05:01,334 iteration 1828 : loss : 0.042914, loss_ce: 0.015785
2022-01-11 23:05:02,863 iteration 1829 : loss : 0.043046, loss_ce: 0.019182
2022-01-11 23:05:04,512 iteration 1830 : loss : 0.048919, loss_ce: 0.018409
2022-01-11 23:05:06,083 iteration 1831 : loss : 0.036926, loss_ce: 0.014703
2022-01-11 23:05:07,612 iteration 1832 : loss : 0.046407, loss_ce: 0.017966
2022-01-11 23:05:09,109 iteration 1833 : loss : 0.042678, loss_ce: 0.018966
2022-01-11 23:05:10,651 iteration 1834 : loss : 0.060683, loss_ce: 0.022365
2022-01-11 23:05:12,219 iteration 1835 : loss : 0.033829, loss_ce: 0.012664
2022-01-11 23:05:13,754 iteration 1836 : loss : 0.051227, loss_ce: 0.017164
 27%|███████▊                     | 108/400 [52:17<2:15:48, 27.91s/it]2022-01-11 23:05:15,370 iteration 1837 : loss : 0.040656, loss_ce: 0.017052
2022-01-11 23:05:16,844 iteration 1838 : loss : 0.050834, loss_ce: 0.019307
2022-01-11 23:05:18,357 iteration 1839 : loss : 0.053988, loss_ce: 0.022630
2022-01-11 23:05:19,927 iteration 1840 : loss : 0.032493, loss_ce: 0.014809
2022-01-11 23:05:21,414 iteration 1841 : loss : 0.040879, loss_ce: 0.017377
2022-01-11 23:05:22,948 iteration 1842 : loss : 0.033711, loss_ce: 0.011744
2022-01-11 23:05:24,515 iteration 1843 : loss : 0.040700, loss_ce: 0.020025
2022-01-11 23:05:26,052 iteration 1844 : loss : 0.041327, loss_ce: 0.012191
2022-01-11 23:05:27,574 iteration 1845 : loss : 0.042253, loss_ce: 0.012093
2022-01-11 23:05:29,192 iteration 1846 : loss : 0.040625, loss_ce: 0.019102
2022-01-11 23:05:30,708 iteration 1847 : loss : 0.047644, loss_ce: 0.015607
2022-01-11 23:05:32,245 iteration 1848 : loss : 0.049751, loss_ce: 0.020806
2022-01-11 23:05:33,770 iteration 1849 : loss : 0.055525, loss_ce: 0.024868
2022-01-11 23:05:35,437 iteration 1850 : loss : 0.058792, loss_ce: 0.024134
2022-01-11 23:05:36,978 iteration 1851 : loss : 0.057268, loss_ce: 0.017000
2022-01-11 23:05:38,539 iteration 1852 : loss : 0.043716, loss_ce: 0.020572
2022-01-11 23:05:40,054 iteration 1853 : loss : 0.033559, loss_ce: 0.013593
 27%|███████▉                     | 109/400 [52:44<2:13:00, 27.43s/it]2022-01-11 23:05:41,627 iteration 1854 : loss : 0.036145, loss_ce: 0.014264
2022-01-11 23:05:43,233 iteration 1855 : loss : 0.036727, loss_ce: 0.012957
2022-01-11 23:05:44,841 iteration 1856 : loss : 0.058760, loss_ce: 0.020344
2022-01-11 23:05:46,444 iteration 1857 : loss : 0.035440, loss_ce: 0.015423
2022-01-11 23:05:48,012 iteration 1858 : loss : 0.040495, loss_ce: 0.015395
2022-01-11 23:05:49,586 iteration 1859 : loss : 0.039666, loss_ce: 0.015876
2022-01-11 23:05:51,093 iteration 1860 : loss : 0.053759, loss_ce: 0.016735
2022-01-11 23:05:52,682 iteration 1861 : loss : 0.038573, loss_ce: 0.013954
2022-01-11 23:05:54,178 iteration 1862 : loss : 0.037627, loss_ce: 0.019289
2022-01-11 23:05:55,730 iteration 1863 : loss : 0.066846, loss_ce: 0.036691
2022-01-11 23:05:57,308 iteration 1864 : loss : 0.051410, loss_ce: 0.012583
2022-01-11 23:05:58,791 iteration 1865 : loss : 0.033887, loss_ce: 0.014919
2022-01-11 23:06:00,414 iteration 1866 : loss : 0.041103, loss_ce: 0.018561
2022-01-11 23:06:01,968 iteration 1867 : loss : 0.027207, loss_ce: 0.012700
2022-01-11 23:06:03,498 iteration 1868 : loss : 0.041072, loss_ce: 0.017524
2022-01-11 23:06:05,025 iteration 1869 : loss : 0.049546, loss_ce: 0.015842
2022-01-11 23:06:05,026 Training Data Eval:
2022-01-11 23:06:13,004   Average segmentation loss on training set: 0.0287
2022-01-11 23:06:13,004 Validation Data Eval:
2022-01-11 23:06:15,756   Average segmentation loss on validation set: 0.0916
2022-01-11 23:06:17,357 iteration 1870 : loss : 0.032853, loss_ce: 0.012340
 28%|███████▉                     | 110/400 [53:21<2:26:53, 30.39s/it]2022-01-11 23:06:19,008 iteration 1871 : loss : 0.031272, loss_ce: 0.011989
2022-01-11 23:06:20,657 iteration 1872 : loss : 0.077876, loss_ce: 0.023458
2022-01-11 23:06:22,154 iteration 1873 : loss : 0.060252, loss_ce: 0.028460
2022-01-11 23:06:23,769 iteration 1874 : loss : 0.066190, loss_ce: 0.036221
2022-01-11 23:06:25,246 iteration 1875 : loss : 0.037770, loss_ce: 0.012095
2022-01-11 23:06:26,879 iteration 1876 : loss : 0.040636, loss_ce: 0.011512
2022-01-11 23:06:28,468 iteration 1877 : loss : 0.052149, loss_ce: 0.021106
2022-01-11 23:06:30,007 iteration 1878 : loss : 0.038464, loss_ce: 0.014919
2022-01-11 23:06:31,529 iteration 1879 : loss : 0.051454, loss_ce: 0.022192
2022-01-11 23:06:33,046 iteration 1880 : loss : 0.044835, loss_ce: 0.022218
2022-01-11 23:06:34,596 iteration 1881 : loss : 0.060672, loss_ce: 0.020042
2022-01-11 23:06:36,117 iteration 1882 : loss : 0.043459, loss_ce: 0.017050
2022-01-11 23:06:37,598 iteration 1883 : loss : 0.045335, loss_ce: 0.014196
2022-01-11 23:06:39,184 iteration 1884 : loss : 0.040542, loss_ce: 0.016181
2022-01-11 23:06:40,653 iteration 1885 : loss : 0.031344, loss_ce: 0.012355
2022-01-11 23:06:42,291 iteration 1886 : loss : 0.053775, loss_ce: 0.023390
2022-01-11 23:06:43,922 iteration 1887 : loss : 0.049263, loss_ce: 0.021183
 28%|████████                     | 111/400 [53:48<2:20:50, 29.24s/it]2022-01-11 23:06:45,507 iteration 1888 : loss : 0.049476, loss_ce: 0.021488
2022-01-11 23:06:47,112 iteration 1889 : loss : 0.048469, loss_ce: 0.017979
2022-01-11 23:06:48,692 iteration 1890 : loss : 0.046660, loss_ce: 0.016792
2022-01-11 23:06:50,282 iteration 1891 : loss : 0.093798, loss_ce: 0.040912
2022-01-11 23:06:51,855 iteration 1892 : loss : 0.038764, loss_ce: 0.017068
2022-01-11 23:06:53,436 iteration 1893 : loss : 0.050693, loss_ce: 0.027803
2022-01-11 23:06:54,985 iteration 1894 : loss : 0.049840, loss_ce: 0.023334
2022-01-11 23:06:56,520 iteration 1895 : loss : 0.049815, loss_ce: 0.014897
2022-01-11 23:06:58,180 iteration 1896 : loss : 0.053557, loss_ce: 0.027748
2022-01-11 23:06:59,720 iteration 1897 : loss : 0.055691, loss_ce: 0.017974
2022-01-11 23:07:01,333 iteration 1898 : loss : 0.035937, loss_ce: 0.012686
2022-01-11 23:07:02,895 iteration 1899 : loss : 0.052591, loss_ce: 0.025625
2022-01-11 23:07:04,383 iteration 1900 : loss : 0.042825, loss_ce: 0.020594
2022-01-11 23:07:05,929 iteration 1901 : loss : 0.036515, loss_ce: 0.014460
2022-01-11 23:07:07,504 iteration 1902 : loss : 0.058880, loss_ce: 0.027402
2022-01-11 23:07:09,108 iteration 1903 : loss : 0.074690, loss_ce: 0.022892
2022-01-11 23:07:10,751 iteration 1904 : loss : 0.045350, loss_ce: 0.017463
 28%|████████                     | 112/400 [54:14<2:16:52, 28.52s/it]2022-01-11 23:07:12,283 iteration 1905 : loss : 0.034118, loss_ce: 0.014076
2022-01-11 23:07:13,885 iteration 1906 : loss : 0.042863, loss_ce: 0.017852
2022-01-11 23:07:15,533 iteration 1907 : loss : 0.044170, loss_ce: 0.017583
2022-01-11 23:07:17,111 iteration 1908 : loss : 0.042744, loss_ce: 0.017941
2022-01-11 23:07:18,718 iteration 1909 : loss : 0.053309, loss_ce: 0.027803
2022-01-11 23:07:20,321 iteration 1910 : loss : 0.041409, loss_ce: 0.019569
2022-01-11 23:07:21,998 iteration 1911 : loss : 0.037343, loss_ce: 0.014698
2022-01-11 23:07:23,592 iteration 1912 : loss : 0.053990, loss_ce: 0.016849
2022-01-11 23:07:25,129 iteration 1913 : loss : 0.036613, loss_ce: 0.014150
2022-01-11 23:07:26,646 iteration 1914 : loss : 0.036615, loss_ce: 0.016460
2022-01-11 23:07:28,221 iteration 1915 : loss : 0.057751, loss_ce: 0.019676
2022-01-11 23:07:29,792 iteration 1916 : loss : 0.041148, loss_ce: 0.016549
2022-01-11 23:07:31,385 iteration 1917 : loss : 0.044895, loss_ce: 0.017927
2022-01-11 23:07:32,910 iteration 1918 : loss : 0.038212, loss_ce: 0.015844
2022-01-11 23:07:34,458 iteration 1919 : loss : 0.047278, loss_ce: 0.026668
2022-01-11 23:07:36,006 iteration 1920 : loss : 0.035883, loss_ce: 0.012412
2022-01-11 23:07:37,593 iteration 1921 : loss : 0.041683, loss_ce: 0.014594
 28%|████████▏                    | 113/400 [54:41<2:13:59, 28.01s/it]2022-01-11 23:07:39,142 iteration 1922 : loss : 0.042090, loss_ce: 0.014817
2022-01-11 23:07:40,630 iteration 1923 : loss : 0.028268, loss_ce: 0.012458
2022-01-11 23:07:42,210 iteration 1924 : loss : 0.067362, loss_ce: 0.037086
2022-01-11 23:07:43,732 iteration 1925 : loss : 0.054151, loss_ce: 0.024057
2022-01-11 23:07:45,266 iteration 1926 : loss : 0.035739, loss_ce: 0.015864
2022-01-11 23:07:46,856 iteration 1927 : loss : 0.047777, loss_ce: 0.019340
2022-01-11 23:07:48,445 iteration 1928 : loss : 0.036070, loss_ce: 0.015467
2022-01-11 23:07:49,967 iteration 1929 : loss : 0.041729, loss_ce: 0.018494
2022-01-11 23:07:51,525 iteration 1930 : loss : 0.055343, loss_ce: 0.018512
2022-01-11 23:07:53,062 iteration 1931 : loss : 0.050887, loss_ce: 0.015955
2022-01-11 23:07:54,620 iteration 1932 : loss : 0.036238, loss_ce: 0.010868
2022-01-11 23:07:56,187 iteration 1933 : loss : 0.056222, loss_ce: 0.020646
2022-01-11 23:07:57,631 iteration 1934 : loss : 0.032196, loss_ce: 0.016052
2022-01-11 23:07:59,162 iteration 1935 : loss : 0.042166, loss_ce: 0.019191
2022-01-11 23:08:00,762 iteration 1936 : loss : 0.032840, loss_ce: 0.012414
2022-01-11 23:08:02,354 iteration 1937 : loss : 0.031851, loss_ce: 0.012971
2022-01-11 23:08:03,943 iteration 1938 : loss : 0.039603, loss_ce: 0.012852
 28%|████████▎                    | 114/400 [55:08<2:11:10, 27.52s/it]2022-01-11 23:08:05,469 iteration 1939 : loss : 0.028161, loss_ce: 0.013930
2022-01-11 23:08:07,007 iteration 1940 : loss : 0.033450, loss_ce: 0.013409
2022-01-11 23:08:08,619 iteration 1941 : loss : 0.034053, loss_ce: 0.015420
2022-01-11 23:08:10,166 iteration 1942 : loss : 0.044108, loss_ce: 0.015488
2022-01-11 23:08:11,654 iteration 1943 : loss : 0.031641, loss_ce: 0.012301
2022-01-11 23:08:13,190 iteration 1944 : loss : 0.032977, loss_ce: 0.013382
2022-01-11 23:08:14,740 iteration 1945 : loss : 0.032881, loss_ce: 0.016163
2022-01-11 23:08:16,297 iteration 1946 : loss : 0.039883, loss_ce: 0.015119
2022-01-11 23:08:17,841 iteration 1947 : loss : 0.048575, loss_ce: 0.019625
2022-01-11 23:08:19,386 iteration 1948 : loss : 0.035485, loss_ce: 0.015763
2022-01-11 23:08:20,882 iteration 1949 : loss : 0.033110, loss_ce: 0.017069
2022-01-11 23:08:22,474 iteration 1950 : loss : 0.048514, loss_ce: 0.021759
2022-01-11 23:08:24,121 iteration 1951 : loss : 0.063670, loss_ce: 0.024288
2022-01-11 23:08:25,635 iteration 1952 : loss : 0.045018, loss_ce: 0.012584
2022-01-11 23:08:27,208 iteration 1953 : loss : 0.040092, loss_ce: 0.020014
2022-01-11 23:08:28,825 iteration 1954 : loss : 0.050414, loss_ce: 0.017835
2022-01-11 23:08:28,825 Training Data Eval:
2022-01-11 23:08:36,814   Average segmentation loss on training set: 0.0260
2022-01-11 23:08:36,814 Validation Data Eval:
2022-01-11 23:08:39,564   Average segmentation loss on validation set: 0.0884
2022-01-11 23:08:41,120 iteration 1955 : loss : 0.026609, loss_ce: 0.010092
 29%|████████▎                    | 115/400 [55:45<2:24:28, 30.42s/it]2022-01-11 23:08:42,791 iteration 1956 : loss : 0.043269, loss_ce: 0.016427
2022-01-11 23:08:44,312 iteration 1957 : loss : 0.034528, loss_ce: 0.014258
2022-01-11 23:08:45,864 iteration 1958 : loss : 0.037137, loss_ce: 0.018697
2022-01-11 23:08:47,508 iteration 1959 : loss : 0.059859, loss_ce: 0.016287
2022-01-11 23:08:49,058 iteration 1960 : loss : 0.049969, loss_ce: 0.016662
2022-01-11 23:08:50,625 iteration 1961 : loss : 0.043556, loss_ce: 0.016936
2022-01-11 23:08:52,183 iteration 1962 : loss : 0.037466, loss_ce: 0.014161
2022-01-11 23:08:53,760 iteration 1963 : loss : 0.042358, loss_ce: 0.016660
2022-01-11 23:08:55,298 iteration 1964 : loss : 0.036901, loss_ce: 0.010744
2022-01-11 23:08:56,820 iteration 1965 : loss : 0.039794, loss_ce: 0.018617
2022-01-11 23:08:58,334 iteration 1966 : loss : 0.046761, loss_ce: 0.020842
2022-01-11 23:08:59,848 iteration 1967 : loss : 0.043305, loss_ce: 0.012258
2022-01-11 23:09:01,347 iteration 1968 : loss : 0.038085, loss_ce: 0.020737
2022-01-11 23:09:02,928 iteration 1969 : loss : 0.035394, loss_ce: 0.012563
2022-01-11 23:09:04,604 iteration 1970 : loss : 0.045278, loss_ce: 0.016646
2022-01-11 23:09:06,074 iteration 1971 : loss : 0.039150, loss_ce: 0.012200
2022-01-11 23:09:07,740 iteration 1972 : loss : 0.042652, loss_ce: 0.014275
 29%|████████▍                    | 116/400 [56:11<2:18:34, 29.28s/it]2022-01-11 23:09:09,256 iteration 1973 : loss : 0.032475, loss_ce: 0.012491
2022-01-11 23:09:10,758 iteration 1974 : loss : 0.036915, loss_ce: 0.016034
2022-01-11 23:09:12,377 iteration 1975 : loss : 0.037985, loss_ce: 0.012970
2022-01-11 23:09:13,867 iteration 1976 : loss : 0.037069, loss_ce: 0.016061
2022-01-11 23:09:15,499 iteration 1977 : loss : 0.059756, loss_ce: 0.023668
2022-01-11 23:09:17,023 iteration 1978 : loss : 0.030905, loss_ce: 0.011840
2022-01-11 23:09:18,564 iteration 1979 : loss : 0.035468, loss_ce: 0.017017
2022-01-11 23:09:20,159 iteration 1980 : loss : 0.035449, loss_ce: 0.012000
2022-01-11 23:09:21,672 iteration 1981 : loss : 0.032045, loss_ce: 0.012736
2022-01-11 23:09:23,286 iteration 1982 : loss : 0.034318, loss_ce: 0.011743
2022-01-11 23:09:24,929 iteration 1983 : loss : 0.045385, loss_ce: 0.019130
2022-01-11 23:09:26,454 iteration 1984 : loss : 0.039147, loss_ce: 0.012663
2022-01-11 23:09:27,986 iteration 1985 : loss : 0.035283, loss_ce: 0.012074
2022-01-11 23:09:29,507 iteration 1986 : loss : 0.035938, loss_ce: 0.012357
2022-01-11 23:09:31,023 iteration 1987 : loss : 0.022144, loss_ce: 0.008301
2022-01-11 23:09:32,654 iteration 1988 : loss : 0.039603, loss_ce: 0.016171
2022-01-11 23:09:34,198 iteration 1989 : loss : 0.031592, loss_ce: 0.015284
 29%|████████▍                    | 117/400 [56:38<2:14:05, 28.43s/it]2022-01-11 23:09:35,769 iteration 1990 : loss : 0.045575, loss_ce: 0.019267
2022-01-11 23:09:37,244 iteration 1991 : loss : 0.032826, loss_ce: 0.014073
2022-01-11 23:09:38,838 iteration 1992 : loss : 0.043180, loss_ce: 0.018947
2022-01-11 23:09:40,389 iteration 1993 : loss : 0.054700, loss_ce: 0.014239
2022-01-11 23:09:41,974 iteration 1994 : loss : 0.033125, loss_ce: 0.012996
2022-01-11 23:09:43,520 iteration 1995 : loss : 0.034080, loss_ce: 0.013768
2022-01-11 23:09:45,081 iteration 1996 : loss : 0.037484, loss_ce: 0.015692
2022-01-11 23:09:46,646 iteration 1997 : loss : 0.042596, loss_ce: 0.020012
2022-01-11 23:09:48,242 iteration 1998 : loss : 0.053868, loss_ce: 0.028215
2022-01-11 23:09:49,821 iteration 1999 : loss : 0.037949, loss_ce: 0.014827
2022-01-11 23:09:51,486 iteration 2000 : loss : 0.069948, loss_ce: 0.018451
2022-01-11 23:09:53,038 iteration 2001 : loss : 0.035570, loss_ce: 0.014225
2022-01-11 23:09:54,525 iteration 2002 : loss : 0.029421, loss_ce: 0.012581
2022-01-11 23:09:55,968 iteration 2003 : loss : 0.024491, loss_ce: 0.012010
2022-01-11 23:09:57,491 iteration 2004 : loss : 0.040287, loss_ce: 0.012619
2022-01-11 23:09:59,092 iteration 2005 : loss : 0.032078, loss_ce: 0.011535
2022-01-11 23:10:00,676 iteration 2006 : loss : 0.042445, loss_ce: 0.013532
 30%|████████▌                    | 118/400 [57:04<2:10:51, 27.84s/it]2022-01-11 23:10:02,327 iteration 2007 : loss : 0.038597, loss_ce: 0.015092
2022-01-11 23:10:03,996 iteration 2008 : loss : 0.062859, loss_ce: 0.018969
2022-01-11 23:10:05,548 iteration 2009 : loss : 0.039806, loss_ce: 0.015760
2022-01-11 23:10:07,105 iteration 2010 : loss : 0.046553, loss_ce: 0.019413
2022-01-11 23:10:08,745 iteration 2011 : loss : 0.044075, loss_ce: 0.019557
2022-01-11 23:10:10,322 iteration 2012 : loss : 0.030426, loss_ce: 0.015014
2022-01-11 23:10:11,811 iteration 2013 : loss : 0.028369, loss_ce: 0.012039
2022-01-11 23:10:13,353 iteration 2014 : loss : 0.046652, loss_ce: 0.016580
2022-01-11 23:10:14,946 iteration 2015 : loss : 0.033135, loss_ce: 0.013039
2022-01-11 23:10:16,491 iteration 2016 : loss : 0.041663, loss_ce: 0.014829
2022-01-11 23:10:18,092 iteration 2017 : loss : 0.031770, loss_ce: 0.009905
2022-01-11 23:10:19,740 iteration 2018 : loss : 0.044826, loss_ce: 0.021030
2022-01-11 23:10:21,279 iteration 2019 : loss : 0.045314, loss_ce: 0.017972
2022-01-11 23:10:22,822 iteration 2020 : loss : 0.026422, loss_ce: 0.009561
2022-01-11 23:10:24,407 iteration 2021 : loss : 0.048271, loss_ce: 0.016654
2022-01-11 23:10:25,999 iteration 2022 : loss : 0.033577, loss_ce: 0.015304
2022-01-11 23:10:27,612 iteration 2023 : loss : 0.045121, loss_ce: 0.018049
 30%|████████▋                    | 119/400 [57:31<2:09:07, 27.57s/it]2022-01-11 23:10:29,171 iteration 2024 : loss : 0.031732, loss_ce: 0.009977
2022-01-11 23:10:30,746 iteration 2025 : loss : 0.030575, loss_ce: 0.014645
2022-01-11 23:10:32,367 iteration 2026 : loss : 0.030274, loss_ce: 0.011767
2022-01-11 23:10:33,910 iteration 2027 : loss : 0.033005, loss_ce: 0.010423
2022-01-11 23:10:35,417 iteration 2028 : loss : 0.028335, loss_ce: 0.014028
2022-01-11 23:10:36,961 iteration 2029 : loss : 0.030208, loss_ce: 0.011551
2022-01-11 23:10:38,485 iteration 2030 : loss : 0.038064, loss_ce: 0.014637
2022-01-11 23:10:40,055 iteration 2031 : loss : 0.032855, loss_ce: 0.012150
2022-01-11 23:10:41,690 iteration 2032 : loss : 0.041053, loss_ce: 0.017949
2022-01-11 23:10:43,353 iteration 2033 : loss : 0.051652, loss_ce: 0.025464
2022-01-11 23:10:44,879 iteration 2034 : loss : 0.031308, loss_ce: 0.012828
2022-01-11 23:10:46,432 iteration 2035 : loss : 0.035388, loss_ce: 0.013102
2022-01-11 23:10:48,046 iteration 2036 : loss : 0.039851, loss_ce: 0.019946
2022-01-11 23:10:49,603 iteration 2037 : loss : 0.077082, loss_ce: 0.017441
2022-01-11 23:10:51,196 iteration 2038 : loss : 0.069489, loss_ce: 0.022616
2022-01-11 23:10:52,833 iteration 2039 : loss : 0.052895, loss_ce: 0.021457
2022-01-11 23:10:52,834 Training Data Eval:
2022-01-11 23:11:00,810   Average segmentation loss on training set: 0.0255
2022-01-11 23:11:00,810 Validation Data Eval:
2022-01-11 23:11:03,554   Average segmentation loss on validation set: 0.0747
2022-01-11 23:11:05,190 iteration 2040 : loss : 0.040713, loss_ce: 0.013895
 30%|████████▋                    | 120/400 [58:09<2:22:41, 30.58s/it]2022-01-11 23:11:06,841 iteration 2041 : loss : 0.050670, loss_ce: 0.011483
2022-01-11 23:11:08,386 iteration 2042 : loss : 0.043318, loss_ce: 0.017064
2022-01-11 23:11:09,936 iteration 2043 : loss : 0.031072, loss_ce: 0.010264
2022-01-11 23:11:11,494 iteration 2044 : loss : 0.046566, loss_ce: 0.018730
2022-01-11 23:11:13,036 iteration 2045 : loss : 0.029690, loss_ce: 0.009472
2022-01-11 23:11:14,615 iteration 2046 : loss : 0.042171, loss_ce: 0.019470
2022-01-11 23:11:16,156 iteration 2047 : loss : 0.046995, loss_ce: 0.019764
2022-01-11 23:11:17,734 iteration 2048 : loss : 0.037525, loss_ce: 0.014690
2022-01-11 23:11:19,271 iteration 2049 : loss : 0.033409, loss_ce: 0.008812
2022-01-11 23:11:20,846 iteration 2050 : loss : 0.055822, loss_ce: 0.027520
2022-01-11 23:11:22,423 iteration 2051 : loss : 0.038886, loss_ce: 0.019549
2022-01-11 23:11:23,931 iteration 2052 : loss : 0.045014, loss_ce: 0.016988
2022-01-11 23:11:25,509 iteration 2053 : loss : 0.054465, loss_ce: 0.015314
2022-01-11 23:11:27,052 iteration 2054 : loss : 0.036173, loss_ce: 0.014479
2022-01-11 23:11:28,605 iteration 2055 : loss : 0.069783, loss_ce: 0.023159
2022-01-11 23:11:30,210 iteration 2056 : loss : 0.037057, loss_ce: 0.015795
2022-01-11 23:11:31,725 iteration 2057 : loss : 0.038576, loss_ce: 0.016243
 30%|████████▊                    | 121/400 [58:35<2:16:31, 29.36s/it]2022-01-11 23:11:33,326 iteration 2058 : loss : 0.046931, loss_ce: 0.024216
2022-01-11 23:11:34,942 iteration 2059 : loss : 0.049354, loss_ce: 0.017998
2022-01-11 23:11:36,427 iteration 2060 : loss : 0.037700, loss_ce: 0.013469
2022-01-11 23:11:38,016 iteration 2061 : loss : 0.046239, loss_ce: 0.017387
2022-01-11 23:11:39,557 iteration 2062 : loss : 0.039304, loss_ce: 0.014366
2022-01-11 23:11:41,112 iteration 2063 : loss : 0.059153, loss_ce: 0.015964
2022-01-11 23:11:42,623 iteration 2064 : loss : 0.027761, loss_ce: 0.009772
2022-01-11 23:11:44,160 iteration 2065 : loss : 0.042337, loss_ce: 0.012771
2022-01-11 23:11:45,713 iteration 2066 : loss : 0.037859, loss_ce: 0.014410
2022-01-11 23:11:47,290 iteration 2067 : loss : 0.034501, loss_ce: 0.011936
2022-01-11 23:11:48,902 iteration 2068 : loss : 0.036834, loss_ce: 0.016922
2022-01-11 23:11:50,436 iteration 2069 : loss : 0.043199, loss_ce: 0.011948
2022-01-11 23:11:52,040 iteration 2070 : loss : 0.037060, loss_ce: 0.015166
2022-01-11 23:11:53,653 iteration 2071 : loss : 0.045395, loss_ce: 0.017704
2022-01-11 23:11:55,221 iteration 2072 : loss : 0.042707, loss_ce: 0.015518
2022-01-11 23:11:56,817 iteration 2073 : loss : 0.046969, loss_ce: 0.012645
2022-01-11 23:11:58,402 iteration 2074 : loss : 0.046010, loss_ce: 0.024547
 30%|████████▊                    | 122/400 [59:02<2:12:19, 28.56s/it]2022-01-11 23:11:59,969 iteration 2075 : loss : 0.032817, loss_ce: 0.011094
2022-01-11 23:12:01,546 iteration 2076 : loss : 0.045921, loss_ce: 0.013220
2022-01-11 23:12:03,131 iteration 2077 : loss : 0.064312, loss_ce: 0.034828
2022-01-11 23:12:04,798 iteration 2078 : loss : 0.040237, loss_ce: 0.014382
2022-01-11 23:12:06,335 iteration 2079 : loss : 0.050739, loss_ce: 0.015742
2022-01-11 23:12:07,945 iteration 2080 : loss : 0.043703, loss_ce: 0.016890
2022-01-11 23:12:09,424 iteration 2081 : loss : 0.043615, loss_ce: 0.016063
2022-01-11 23:12:10,888 iteration 2082 : loss : 0.026899, loss_ce: 0.009988
2022-01-11 23:12:12,431 iteration 2083 : loss : 0.037251, loss_ce: 0.019307
2022-01-11 23:12:13,978 iteration 2084 : loss : 0.039399, loss_ce: 0.017259
2022-01-11 23:12:15,485 iteration 2085 : loss : 0.038707, loss_ce: 0.012766
2022-01-11 23:12:17,031 iteration 2086 : loss : 0.043295, loss_ce: 0.017203
2022-01-11 23:12:18,551 iteration 2087 : loss : 0.041507, loss_ce: 0.016017
2022-01-11 23:12:20,171 iteration 2088 : loss : 0.065223, loss_ce: 0.022144
2022-01-11 23:12:21,767 iteration 2089 : loss : 0.034030, loss_ce: 0.016540
2022-01-11 23:12:23,264 iteration 2090 : loss : 0.033446, loss_ce: 0.013180
2022-01-11 23:12:24,863 iteration 2091 : loss : 0.049766, loss_ce: 0.023279
 31%|████████▉                    | 123/400 [59:28<2:08:55, 27.93s/it]2022-01-11 23:12:26,386 iteration 2092 : loss : 0.023537, loss_ce: 0.010139
2022-01-11 23:12:27,922 iteration 2093 : loss : 0.047496, loss_ce: 0.017214
2022-01-11 23:12:29,488 iteration 2094 : loss : 0.045433, loss_ce: 0.013772
2022-01-11 23:12:30,999 iteration 2095 : loss : 0.022753, loss_ce: 0.009733
2022-01-11 23:12:32,626 iteration 2096 : loss : 0.036423, loss_ce: 0.016730
2022-01-11 23:12:34,194 iteration 2097 : loss : 0.056573, loss_ce: 0.018017
2022-01-11 23:12:35,751 iteration 2098 : loss : 0.055528, loss_ce: 0.022157
2022-01-11 23:12:37,316 iteration 2099 : loss : 0.026846, loss_ce: 0.008444
2022-01-11 23:12:38,902 iteration 2100 : loss : 0.045188, loss_ce: 0.024200
2022-01-11 23:12:40,483 iteration 2101 : loss : 0.040849, loss_ce: 0.014549
2022-01-11 23:12:42,086 iteration 2102 : loss : 0.044095, loss_ce: 0.023791
2022-01-11 23:12:43,569 iteration 2103 : loss : 0.041868, loss_ce: 0.019903
2022-01-11 23:12:45,068 iteration 2104 : loss : 0.049417, loss_ce: 0.015100
2022-01-11 23:12:46,697 iteration 2105 : loss : 0.044963, loss_ce: 0.015483
2022-01-11 23:12:48,227 iteration 2106 : loss : 0.024968, loss_ce: 0.010869
2022-01-11 23:12:49,861 iteration 2107 : loss : 0.025211, loss_ce: 0.010245
2022-01-11 23:12:51,444 iteration 2108 : loss : 0.051429, loss_ce: 0.022248
 31%|████████▉                    | 124/400 [59:55<2:06:36, 27.52s/it]2022-01-11 23:12:53,038 iteration 2109 : loss : 0.038141, loss_ce: 0.016119
2022-01-11 23:12:54,615 iteration 2110 : loss : 0.044984, loss_ce: 0.016409
2022-01-11 23:12:56,241 iteration 2111 : loss : 0.050237, loss_ce: 0.013738
2022-01-11 23:12:57,812 iteration 2112 : loss : 0.046589, loss_ce: 0.019907
2022-01-11 23:12:59,345 iteration 2113 : loss : 0.036852, loss_ce: 0.019073
2022-01-11 23:13:00,946 iteration 2114 : loss : 0.034679, loss_ce: 0.017603
2022-01-11 23:13:02,537 iteration 2115 : loss : 0.031759, loss_ce: 0.009571
2022-01-11 23:13:04,113 iteration 2116 : loss : 0.040061, loss_ce: 0.013573
2022-01-11 23:13:05,680 iteration 2117 : loss : 0.032205, loss_ce: 0.011978
2022-01-11 23:13:07,281 iteration 2118 : loss : 0.037445, loss_ce: 0.012789
2022-01-11 23:13:08,895 iteration 2119 : loss : 0.040622, loss_ce: 0.015537
2022-01-11 23:13:10,471 iteration 2120 : loss : 0.041576, loss_ce: 0.013085
2022-01-11 23:13:11,953 iteration 2121 : loss : 0.051446, loss_ce: 0.017842
2022-01-11 23:13:13,496 iteration 2122 : loss : 0.028310, loss_ce: 0.010149
2022-01-11 23:13:15,119 iteration 2123 : loss : 0.060672, loss_ce: 0.028965
2022-01-11 23:13:16,745 iteration 2124 : loss : 0.061196, loss_ce: 0.025402
2022-01-11 23:13:16,746 Training Data Eval:
2022-01-11 23:13:24,704   Average segmentation loss on training set: 0.0348
2022-01-11 23:13:24,705 Validation Data Eval:
2022-01-11 23:13:27,451   Average segmentation loss on validation set: 0.1174
2022-01-11 23:13:29,056 iteration 2125 : loss : 0.036319, loss_ce: 0.019434
 31%|████████▍                  | 125/400 [1:00:33<2:20:01, 30.55s/it]2022-01-11 23:13:30,722 iteration 2126 : loss : 0.036382, loss_ce: 0.014172
2022-01-11 23:13:32,323 iteration 2127 : loss : 0.044367, loss_ce: 0.021054
2022-01-11 23:13:33,909 iteration 2128 : loss : 0.042081, loss_ce: 0.015067
2022-01-11 23:13:35,449 iteration 2129 : loss : 0.052681, loss_ce: 0.018428
2022-01-11 23:13:37,048 iteration 2130 : loss : 0.087786, loss_ce: 0.027599
2022-01-11 23:13:38,601 iteration 2131 : loss : 0.050168, loss_ce: 0.015858
2022-01-11 23:13:40,168 iteration 2132 : loss : 0.028452, loss_ce: 0.011445
2022-01-11 23:13:41,801 iteration 2133 : loss : 0.065150, loss_ce: 0.025642
2022-01-11 23:13:43,302 iteration 2134 : loss : 0.035490, loss_ce: 0.016375
2022-01-11 23:13:44,868 iteration 2135 : loss : 0.047416, loss_ce: 0.016251
2022-01-11 23:13:46,421 iteration 2136 : loss : 0.035412, loss_ce: 0.015981
2022-01-11 23:13:47,990 iteration 2137 : loss : 0.065336, loss_ce: 0.027378
2022-01-11 23:13:49,514 iteration 2138 : loss : 0.033511, loss_ce: 0.014988
2022-01-11 23:13:51,111 iteration 2139 : loss : 0.046509, loss_ce: 0.026743
2022-01-11 23:13:52,658 iteration 2140 : loss : 0.032866, loss_ce: 0.014587
2022-01-11 23:13:54,276 iteration 2141 : loss : 0.057015, loss_ce: 0.018136
2022-01-11 23:13:55,797 iteration 2142 : loss : 0.036418, loss_ce: 0.010760
 32%|████████▌                  | 126/400 [1:00:59<2:14:17, 29.41s/it]2022-01-11 23:13:57,321 iteration 2143 : loss : 0.032289, loss_ce: 0.013276
2022-01-11 23:13:58,899 iteration 2144 : loss : 0.045211, loss_ce: 0.015104
2022-01-11 23:14:00,483 iteration 2145 : loss : 0.035048, loss_ce: 0.011065
2022-01-11 23:14:02,003 iteration 2146 : loss : 0.025582, loss_ce: 0.011292
2022-01-11 23:14:03,583 iteration 2147 : loss : 0.034874, loss_ce: 0.010468
2022-01-11 23:14:05,111 iteration 2148 : loss : 0.035007, loss_ce: 0.015033
2022-01-11 23:14:06,677 iteration 2149 : loss : 0.049757, loss_ce: 0.022408
2022-01-11 23:14:08,201 iteration 2150 : loss : 0.042000, loss_ce: 0.016599
2022-01-11 23:14:09,819 iteration 2151 : loss : 0.046653, loss_ce: 0.023349
2022-01-11 23:14:11,353 iteration 2152 : loss : 0.027822, loss_ce: 0.010342
2022-01-11 23:14:12,861 iteration 2153 : loss : 0.027184, loss_ce: 0.010848
2022-01-11 23:14:14,530 iteration 2154 : loss : 0.048102, loss_ce: 0.016826
2022-01-11 23:14:16,079 iteration 2155 : loss : 0.033482, loss_ce: 0.012951
2022-01-11 23:14:17,587 iteration 2156 : loss : 0.049918, loss_ce: 0.022165
2022-01-11 23:14:19,265 iteration 2157 : loss : 0.057848, loss_ce: 0.019937
2022-01-11 23:14:20,892 iteration 2158 : loss : 0.041906, loss_ce: 0.019174
2022-01-11 23:14:22,463 iteration 2159 : loss : 0.057904, loss_ce: 0.019461
 32%|████████▌                  | 127/400 [1:01:26<2:10:03, 28.58s/it]2022-01-11 23:14:24,017 iteration 2160 : loss : 0.036141, loss_ce: 0.013732
2022-01-11 23:14:25,531 iteration 2161 : loss : 0.032429, loss_ce: 0.011316
2022-01-11 23:14:27,070 iteration 2162 : loss : 0.043637, loss_ce: 0.016245
2022-01-11 23:14:28,589 iteration 2163 : loss : 0.031772, loss_ce: 0.014276
2022-01-11 23:14:30,171 iteration 2164 : loss : 0.028626, loss_ce: 0.013787
2022-01-11 23:14:31,732 iteration 2165 : loss : 0.054121, loss_ce: 0.019268
2022-01-11 23:14:33,239 iteration 2166 : loss : 0.034150, loss_ce: 0.015122
2022-01-11 23:14:34,729 iteration 2167 : loss : 0.032146, loss_ce: 0.012394
2022-01-11 23:14:36,259 iteration 2168 : loss : 0.032187, loss_ce: 0.013283
2022-01-11 23:14:37,844 iteration 2169 : loss : 0.043784, loss_ce: 0.016527
2022-01-11 23:14:39,296 iteration 2170 : loss : 0.028408, loss_ce: 0.010444
2022-01-11 23:14:40,833 iteration 2171 : loss : 0.048303, loss_ce: 0.017483
2022-01-11 23:14:42,344 iteration 2172 : loss : 0.046762, loss_ce: 0.015625
2022-01-11 23:14:43,857 iteration 2173 : loss : 0.038506, loss_ce: 0.009297
2022-01-11 23:14:45,411 iteration 2174 : loss : 0.036740, loss_ce: 0.014479
2022-01-11 23:14:47,034 iteration 2175 : loss : 0.030124, loss_ce: 0.011362
2022-01-11 23:14:48,486 iteration 2176 : loss : 0.030785, loss_ce: 0.014245
 32%|████████▋                  | 128/400 [1:01:52<2:06:06, 27.82s/it]2022-01-11 23:14:50,005 iteration 2177 : loss : 0.048155, loss_ce: 0.026053
2022-01-11 23:14:51,558 iteration 2178 : loss : 0.030210, loss_ce: 0.009267
2022-01-11 23:14:53,189 iteration 2179 : loss : 0.036999, loss_ce: 0.012347
2022-01-11 23:14:54,787 iteration 2180 : loss : 0.039591, loss_ce: 0.013438
2022-01-11 23:14:56,280 iteration 2181 : loss : 0.033693, loss_ce: 0.013421
2022-01-11 23:14:57,840 iteration 2182 : loss : 0.049942, loss_ce: 0.020129
2022-01-11 23:14:59,408 iteration 2183 : loss : 0.068710, loss_ce: 0.019487
2022-01-11 23:15:00,892 iteration 2184 : loss : 0.052811, loss_ce: 0.028948
2022-01-11 23:15:02,398 iteration 2185 : loss : 0.031840, loss_ce: 0.012110
2022-01-11 23:15:04,027 iteration 2186 : loss : 0.035099, loss_ce: 0.013064
2022-01-11 23:15:05,655 iteration 2187 : loss : 0.038196, loss_ce: 0.013417
2022-01-11 23:15:07,342 iteration 2188 : loss : 0.044367, loss_ce: 0.022266
2022-01-11 23:15:09,003 iteration 2189 : loss : 0.058389, loss_ce: 0.018996
2022-01-11 23:15:10,566 iteration 2190 : loss : 0.047654, loss_ce: 0.023846
2022-01-11 23:15:12,141 iteration 2191 : loss : 0.030296, loss_ce: 0.010665
2022-01-11 23:15:13,666 iteration 2192 : loss : 0.036754, loss_ce: 0.013531
2022-01-11 23:15:15,237 iteration 2193 : loss : 0.027689, loss_ce: 0.008825
 32%|████████▋                  | 129/400 [1:02:19<2:04:11, 27.50s/it]2022-01-11 23:15:16,866 iteration 2194 : loss : 0.037255, loss_ce: 0.015587
2022-01-11 23:15:18,434 iteration 2195 : loss : 0.034004, loss_ce: 0.018837
2022-01-11 23:15:20,025 iteration 2196 : loss : 0.046158, loss_ce: 0.019848
2022-01-11 23:15:21,652 iteration 2197 : loss : 0.028876, loss_ce: 0.011364
2022-01-11 23:15:23,204 iteration 2198 : loss : 0.047029, loss_ce: 0.017992
2022-01-11 23:15:24,774 iteration 2199 : loss : 0.036188, loss_ce: 0.016463
2022-01-11 23:15:26,409 iteration 2200 : loss : 0.055985, loss_ce: 0.016724
2022-01-11 23:15:27,902 iteration 2201 : loss : 0.029242, loss_ce: 0.011062
2022-01-11 23:15:29,556 iteration 2202 : loss : 0.040516, loss_ce: 0.018631
2022-01-11 23:15:31,137 iteration 2203 : loss : 0.039641, loss_ce: 0.012959
2022-01-11 23:15:32,642 iteration 2204 : loss : 0.036394, loss_ce: 0.009826
2022-01-11 23:15:34,222 iteration 2205 : loss : 0.044502, loss_ce: 0.022594
2022-01-11 23:15:35,862 iteration 2206 : loss : 0.056383, loss_ce: 0.021216
2022-01-11 23:15:37,432 iteration 2207 : loss : 0.034835, loss_ce: 0.011230
2022-01-11 23:15:39,041 iteration 2208 : loss : 0.038310, loss_ce: 0.011274
2022-01-11 23:15:40,594 iteration 2209 : loss : 0.043622, loss_ce: 0.017104
2022-01-11 23:15:40,595 Training Data Eval:
2022-01-11 23:15:48,541   Average segmentation loss on training set: 0.0276
2022-01-11 23:15:48,541 Validation Data Eval:
2022-01-11 23:15:51,295   Average segmentation loss on validation set: 0.0812
2022-01-11 23:15:52,891 iteration 2210 : loss : 0.037641, loss_ce: 0.017569
 32%|████████▊                  | 130/400 [1:02:57<2:17:27, 30.54s/it]2022-01-11 23:15:54,495 iteration 2211 : loss : 0.033760, loss_ce: 0.009332
2022-01-11 23:15:56,013 iteration 2212 : loss : 0.036741, loss_ce: 0.015755
2022-01-11 23:15:57,579 iteration 2213 : loss : 0.059058, loss_ce: 0.014430
2022-01-11 23:15:59,065 iteration 2214 : loss : 0.036786, loss_ce: 0.013346
2022-01-11 23:16:00,555 iteration 2215 : loss : 0.025288, loss_ce: 0.010002
2022-01-11 23:16:02,080 iteration 2216 : loss : 0.048835, loss_ce: 0.023905
2022-01-11 23:16:03,589 iteration 2217 : loss : 0.031383, loss_ce: 0.013733
2022-01-11 23:16:05,118 iteration 2218 : loss : 0.033035, loss_ce: 0.014372
2022-01-11 23:16:06,700 iteration 2219 : loss : 0.038547, loss_ce: 0.016451
2022-01-11 23:16:08,327 iteration 2220 : loss : 0.048754, loss_ce: 0.024320
2022-01-11 23:16:09,941 iteration 2221 : loss : 0.050769, loss_ce: 0.017139
2022-01-11 23:16:11,420 iteration 2222 : loss : 0.034056, loss_ce: 0.013271
2022-01-11 23:16:12,927 iteration 2223 : loss : 0.045699, loss_ce: 0.016219
2022-01-11 23:16:14,545 iteration 2224 : loss : 0.041841, loss_ce: 0.017073
2022-01-11 23:16:16,111 iteration 2225 : loss : 0.041796, loss_ce: 0.013880
2022-01-11 23:16:17,715 iteration 2226 : loss : 0.044020, loss_ce: 0.017828
2022-01-11 23:16:19,300 iteration 2227 : loss : 0.039406, loss_ce: 0.011080
 33%|████████▊                  | 131/400 [1:03:23<2:11:22, 29.30s/it]2022-01-11 23:16:20,956 iteration 2228 : loss : 0.041850, loss_ce: 0.022031
2022-01-11 23:16:22,512 iteration 2229 : loss : 0.039298, loss_ce: 0.015307
2022-01-11 23:16:24,195 iteration 2230 : loss : 0.049916, loss_ce: 0.020480
2022-01-11 23:16:25,744 iteration 2231 : loss : 0.037154, loss_ce: 0.020190
2022-01-11 23:16:27,279 iteration 2232 : loss : 0.038759, loss_ce: 0.019004
2022-01-11 23:16:29,008 iteration 2233 : loss : 0.091374, loss_ce: 0.029652
2022-01-11 23:16:30,586 iteration 2234 : loss : 0.036847, loss_ce: 0.010017
2022-01-11 23:16:32,136 iteration 2235 : loss : 0.042943, loss_ce: 0.019089
2022-01-11 23:16:33,677 iteration 2236 : loss : 0.034675, loss_ce: 0.013240
2022-01-11 23:16:35,254 iteration 2237 : loss : 0.041998, loss_ce: 0.020198
2022-01-11 23:16:36,778 iteration 2238 : loss : 0.035624, loss_ce: 0.010370
2022-01-11 23:16:38,315 iteration 2239 : loss : 0.026372, loss_ce: 0.006745
2022-01-11 23:16:39,796 iteration 2240 : loss : 0.040675, loss_ce: 0.011049
2022-01-11 23:16:41,350 iteration 2241 : loss : 0.031542, loss_ce: 0.011520
2022-01-11 23:16:42,968 iteration 2242 : loss : 0.034775, loss_ce: 0.011950
2022-01-11 23:16:44,543 iteration 2243 : loss : 0.049405, loss_ce: 0.022755
2022-01-11 23:16:46,077 iteration 2244 : loss : 0.024620, loss_ce: 0.008717
 33%|████████▉                  | 132/400 [1:03:50<2:07:30, 28.55s/it]2022-01-11 23:16:47,648 iteration 2245 : loss : 0.027763, loss_ce: 0.009533
2022-01-11 23:16:49,118 iteration 2246 : loss : 0.028741, loss_ce: 0.007939
2022-01-11 23:16:50,677 iteration 2247 : loss : 0.036417, loss_ce: 0.016743
2022-01-11 23:16:52,289 iteration 2248 : loss : 0.043380, loss_ce: 0.020526
2022-01-11 23:16:53,840 iteration 2249 : loss : 0.034874, loss_ce: 0.017475
2022-01-11 23:16:55,410 iteration 2250 : loss : 0.033933, loss_ce: 0.012472
2022-01-11 23:16:57,002 iteration 2251 : loss : 0.029904, loss_ce: 0.013176
2022-01-11 23:16:58,559 iteration 2252 : loss : 0.036367, loss_ce: 0.012538
2022-01-11 23:17:00,211 iteration 2253 : loss : 0.042893, loss_ce: 0.018224
2022-01-11 23:17:01,809 iteration 2254 : loss : 0.028039, loss_ce: 0.010098
2022-01-11 23:17:03,355 iteration 2255 : loss : 0.028059, loss_ce: 0.010089
2022-01-11 23:17:04,891 iteration 2256 : loss : 0.025666, loss_ce: 0.010747
2022-01-11 23:17:06,407 iteration 2257 : loss : 0.037222, loss_ce: 0.011984
2022-01-11 23:17:07,930 iteration 2258 : loss : 0.046365, loss_ce: 0.015332
2022-01-11 23:17:09,546 iteration 2259 : loss : 0.033967, loss_ce: 0.013567
2022-01-11 23:17:11,035 iteration 2260 : loss : 0.034152, loss_ce: 0.013461
2022-01-11 23:17:12,610 iteration 2261 : loss : 0.049798, loss_ce: 0.023634
 33%|████████▉                  | 133/400 [1:04:16<2:04:20, 27.94s/it]2022-01-11 23:17:14,164 iteration 2262 : loss : 0.027261, loss_ce: 0.014786
2022-01-11 23:17:15,792 iteration 2263 : loss : 0.045924, loss_ce: 0.016569
2022-01-11 23:17:17,427 iteration 2264 : loss : 0.037145, loss_ce: 0.011020
2022-01-11 23:17:19,000 iteration 2265 : loss : 0.035730, loss_ce: 0.013921
2022-01-11 23:17:20,522 iteration 2266 : loss : 0.028376, loss_ce: 0.011844
2022-01-11 23:17:22,048 iteration 2267 : loss : 0.023791, loss_ce: 0.008480
2022-01-11 23:17:23,601 iteration 2268 : loss : 0.036359, loss_ce: 0.015405
2022-01-11 23:17:25,129 iteration 2269 : loss : 0.034052, loss_ce: 0.018458
2022-01-11 23:17:26,694 iteration 2270 : loss : 0.038771, loss_ce: 0.012380
2022-01-11 23:17:28,240 iteration 2271 : loss : 0.038877, loss_ce: 0.016739
2022-01-11 23:17:29,783 iteration 2272 : loss : 0.037199, loss_ce: 0.018457
2022-01-11 23:17:31,377 iteration 2273 : loss : 0.030719, loss_ce: 0.010129
2022-01-11 23:17:32,960 iteration 2274 : loss : 0.035772, loss_ce: 0.011506
2022-01-11 23:17:34,560 iteration 2275 : loss : 0.046810, loss_ce: 0.016221
2022-01-11 23:17:36,050 iteration 2276 : loss : 0.033243, loss_ce: 0.012750
2022-01-11 23:17:37,623 iteration 2277 : loss : 0.041044, loss_ce: 0.014387
2022-01-11 23:17:39,141 iteration 2278 : loss : 0.034465, loss_ce: 0.011927
 34%|█████████                  | 134/400 [1:04:43<2:01:59, 27.52s/it]2022-01-11 23:17:40,685 iteration 2279 : loss : 0.046481, loss_ce: 0.016998
2022-01-11 23:17:42,232 iteration 2280 : loss : 0.035690, loss_ce: 0.016899
2022-01-11 23:17:43,786 iteration 2281 : loss : 0.054601, loss_ce: 0.012833
2022-01-11 23:17:45,382 iteration 2282 : loss : 0.032851, loss_ce: 0.011866
2022-01-11 23:17:47,006 iteration 2283 : loss : 0.033930, loss_ce: 0.015470
2022-01-11 23:17:48,534 iteration 2284 : loss : 0.033959, loss_ce: 0.017765
2022-01-11 23:17:50,075 iteration 2285 : loss : 0.030025, loss_ce: 0.010341
2022-01-11 23:17:51,611 iteration 2286 : loss : 0.026683, loss_ce: 0.010080
2022-01-11 23:17:53,156 iteration 2287 : loss : 0.049325, loss_ce: 0.019667
2022-01-11 23:17:54,749 iteration 2288 : loss : 0.034614, loss_ce: 0.013029
2022-01-11 23:17:56,260 iteration 2289 : loss : 0.027702, loss_ce: 0.006909
2022-01-11 23:17:57,870 iteration 2290 : loss : 0.033211, loss_ce: 0.017919
2022-01-11 23:17:59,412 iteration 2291 : loss : 0.028387, loss_ce: 0.010449
2022-01-11 23:18:00,951 iteration 2292 : loss : 0.033724, loss_ce: 0.014098
2022-01-11 23:18:02,432 iteration 2293 : loss : 0.033468, loss_ce: 0.016509
2022-01-11 23:18:04,026 iteration 2294 : loss : 0.054132, loss_ce: 0.021322
2022-01-11 23:18:04,027 Training Data Eval:
2022-01-11 23:18:12,009   Average segmentation loss on training set: 0.0240
2022-01-11 23:18:12,010 Validation Data Eval:
2022-01-11 23:18:14,756   Average segmentation loss on validation set: 0.1128
2022-01-11 23:18:16,397 iteration 2295 : loss : 0.034937, loss_ce: 0.011977
 34%|█████████                  | 135/400 [1:05:20<2:14:26, 30.44s/it]2022-01-11 23:18:17,932 iteration 2296 : loss : 0.022916, loss_ce: 0.009209
2022-01-11 23:18:19,484 iteration 2297 : loss : 0.033847, loss_ce: 0.013120
2022-01-11 23:18:21,037 iteration 2298 : loss : 0.024865, loss_ce: 0.009931
2022-01-11 23:18:22,658 iteration 2299 : loss : 0.065535, loss_ce: 0.030696
2022-01-11 23:18:24,174 iteration 2300 : loss : 0.034026, loss_ce: 0.013716
2022-01-11 23:18:25,794 iteration 2301 : loss : 0.027751, loss_ce: 0.008448
2022-01-11 23:18:27,392 iteration 2302 : loss : 0.057024, loss_ce: 0.022004
2022-01-11 23:18:28,958 iteration 2303 : loss : 0.036709, loss_ce: 0.013777
2022-01-11 23:18:30,578 iteration 2304 : loss : 0.033908, loss_ce: 0.011679
2022-01-11 23:18:32,153 iteration 2305 : loss : 0.034200, loss_ce: 0.013090
2022-01-11 23:18:33,807 iteration 2306 : loss : 0.031289, loss_ce: 0.015139
2022-01-11 23:18:35,356 iteration 2307 : loss : 0.070801, loss_ce: 0.027110
2022-01-11 23:18:36,874 iteration 2308 : loss : 0.025301, loss_ce: 0.008566
2022-01-11 23:18:38,408 iteration 2309 : loss : 0.034139, loss_ce: 0.014568
2022-01-11 23:18:40,036 iteration 2310 : loss : 0.039197, loss_ce: 0.015968
2022-01-11 23:18:41,509 iteration 2311 : loss : 0.039201, loss_ce: 0.013361
2022-01-11 23:18:43,069 iteration 2312 : loss : 0.041059, loss_ce: 0.017263
 34%|█████████▏                 | 136/400 [1:05:47<2:08:56, 29.31s/it]2022-01-11 23:18:44,695 iteration 2313 : loss : 0.041045, loss_ce: 0.017059
2022-01-11 23:18:46,151 iteration 2314 : loss : 0.023667, loss_ce: 0.007344
2022-01-11 23:18:47,659 iteration 2315 : loss : 0.022954, loss_ce: 0.007549
2022-01-11 23:18:49,282 iteration 2316 : loss : 0.045004, loss_ce: 0.020689
2022-01-11 23:18:50,774 iteration 2317 : loss : 0.027680, loss_ce: 0.008241
2022-01-11 23:18:52,313 iteration 2318 : loss : 0.031096, loss_ce: 0.014822
2022-01-11 23:18:53,858 iteration 2319 : loss : 0.037190, loss_ce: 0.014446
2022-01-11 23:18:55,479 iteration 2320 : loss : 0.049022, loss_ce: 0.020073
2022-01-11 23:18:57,032 iteration 2321 : loss : 0.041591, loss_ce: 0.017385
2022-01-11 23:18:58,591 iteration 2322 : loss : 0.032105, loss_ce: 0.014734
2022-01-11 23:19:00,058 iteration 2323 : loss : 0.032215, loss_ce: 0.013216
2022-01-11 23:19:01,597 iteration 2324 : loss : 0.029651, loss_ce: 0.014015
2022-01-11 23:19:03,041 iteration 2325 : loss : 0.026325, loss_ce: 0.011918
2022-01-11 23:19:04,570 iteration 2326 : loss : 0.058837, loss_ce: 0.012941
2022-01-11 23:19:06,082 iteration 2327 : loss : 0.034308, loss_ce: 0.012858
2022-01-11 23:19:07,630 iteration 2328 : loss : 0.037087, loss_ce: 0.015666
2022-01-11 23:19:09,143 iteration 2329 : loss : 0.029749, loss_ce: 0.011520
 34%|█████████▏                 | 137/400 [1:06:13<2:04:13, 28.34s/it]2022-01-11 23:19:10,754 iteration 2330 : loss : 0.054846, loss_ce: 0.026867
2022-01-11 23:19:12,261 iteration 2331 : loss : 0.049804, loss_ce: 0.017781
2022-01-11 23:19:13,886 iteration 2332 : loss : 0.037908, loss_ce: 0.013062
2022-01-11 23:19:15,429 iteration 2333 : loss : 0.037968, loss_ce: 0.017230
2022-01-11 23:19:17,092 iteration 2334 : loss : 0.028850, loss_ce: 0.010781
2022-01-11 23:19:18,745 iteration 2335 : loss : 0.053991, loss_ce: 0.031323
2022-01-11 23:19:20,355 iteration 2336 : loss : 0.040180, loss_ce: 0.017404
2022-01-11 23:19:21,908 iteration 2337 : loss : 0.054130, loss_ce: 0.018819
2022-01-11 23:19:23,497 iteration 2338 : loss : 0.057483, loss_ce: 0.021928
2022-01-11 23:19:25,070 iteration 2339 : loss : 0.052417, loss_ce: 0.015332
2022-01-11 23:19:26,568 iteration 2340 : loss : 0.036256, loss_ce: 0.013532
2022-01-11 23:19:28,129 iteration 2341 : loss : 0.037419, loss_ce: 0.014436
2022-01-11 23:19:29,696 iteration 2342 : loss : 0.032100, loss_ce: 0.011845
2022-01-11 23:19:31,367 iteration 2343 : loss : 0.033866, loss_ce: 0.013811
2022-01-11 23:19:32,877 iteration 2344 : loss : 0.079618, loss_ce: 0.018613
2022-01-11 23:19:34,409 iteration 2345 : loss : 0.049648, loss_ce: 0.024873
2022-01-11 23:19:35,952 iteration 2346 : loss : 0.059341, loss_ce: 0.024947
 34%|█████████▎                 | 138/400 [1:06:40<2:01:44, 27.88s/it]2022-01-11 23:19:37,619 iteration 2347 : loss : 0.059339, loss_ce: 0.026567
2022-01-11 23:19:39,182 iteration 2348 : loss : 0.040356, loss_ce: 0.015145
2022-01-11 23:19:40,717 iteration 2349 : loss : 0.035252, loss_ce: 0.013423
2022-01-11 23:19:42,249 iteration 2350 : loss : 0.054018, loss_ce: 0.018337
2022-01-11 23:19:43,877 iteration 2351 : loss : 0.041979, loss_ce: 0.019434
2022-01-11 23:19:45,402 iteration 2352 : loss : 0.026399, loss_ce: 0.013470
2022-01-11 23:19:46,928 iteration 2353 : loss : 0.025563, loss_ce: 0.011024
2022-01-11 23:19:48,444 iteration 2354 : loss : 0.032936, loss_ce: 0.015716
2022-01-11 23:19:49,967 iteration 2355 : loss : 0.039531, loss_ce: 0.013785
2022-01-11 23:19:51,559 iteration 2356 : loss : 0.036450, loss_ce: 0.014270
2022-01-11 23:19:53,111 iteration 2357 : loss : 0.060196, loss_ce: 0.019843
2022-01-11 23:19:54,673 iteration 2358 : loss : 0.041080, loss_ce: 0.016197
2022-01-11 23:19:56,214 iteration 2359 : loss : 0.042547, loss_ce: 0.014390
2022-01-11 23:19:57,736 iteration 2360 : loss : 0.047862, loss_ce: 0.015125
2022-01-11 23:19:59,283 iteration 2361 : loss : 0.028125, loss_ce: 0.009927
2022-01-11 23:20:00,956 iteration 2362 : loss : 0.056335, loss_ce: 0.025041
2022-01-11 23:20:02,514 iteration 2363 : loss : 0.033074, loss_ce: 0.015874
 35%|█████████▍                 | 139/400 [1:07:06<1:59:33, 27.49s/it]2022-01-11 23:20:04,174 iteration 2364 : loss : 0.041076, loss_ce: 0.014097
2022-01-11 23:20:05,796 iteration 2365 : loss : 0.039066, loss_ce: 0.013628
2022-01-11 23:20:07,387 iteration 2366 : loss : 0.040811, loss_ce: 0.017148
2022-01-11 23:20:08,895 iteration 2367 : loss : 0.040249, loss_ce: 0.010397
2022-01-11 23:20:10,401 iteration 2368 : loss : 0.030230, loss_ce: 0.012402
2022-01-11 23:20:11,963 iteration 2369 : loss : 0.038362, loss_ce: 0.012780
2022-01-11 23:20:13,503 iteration 2370 : loss : 0.037898, loss_ce: 0.014370
2022-01-11 23:20:15,093 iteration 2371 : loss : 0.030156, loss_ce: 0.012529
2022-01-11 23:20:16,618 iteration 2372 : loss : 0.031802, loss_ce: 0.010524
2022-01-11 23:20:18,167 iteration 2373 : loss : 0.045639, loss_ce: 0.016623
2022-01-11 23:20:19,671 iteration 2374 : loss : 0.033913, loss_ce: 0.012899
2022-01-11 23:20:21,194 iteration 2375 : loss : 0.030682, loss_ce: 0.010654
2022-01-11 23:20:22,769 iteration 2376 : loss : 0.044696, loss_ce: 0.021990
2022-01-11 23:20:24,338 iteration 2377 : loss : 0.032381, loss_ce: 0.014125
2022-01-11 23:20:25,915 iteration 2378 : loss : 0.023833, loss_ce: 0.009072
2022-01-11 23:20:27,509 iteration 2379 : loss : 0.042600, loss_ce: 0.017810
2022-01-11 23:20:27,509 Training Data Eval:
2022-01-11 23:20:35,488   Average segmentation loss on training set: 0.0232
2022-01-11 23:20:35,488 Validation Data Eval:
2022-01-11 23:20:38,238   Average segmentation loss on validation set: 0.0743
2022-01-11 23:20:44,022 Found new lowest validation loss at iteration 2379! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 23:20:45,449 iteration 2380 : loss : 0.029356, loss_ce: 0.013228
 35%|█████████▍                 | 140/400 [1:07:49<2:19:10, 32.12s/it]2022-01-11 23:20:46,898 iteration 2381 : loss : 0.040050, loss_ce: 0.015130
2022-01-11 23:20:48,383 iteration 2382 : loss : 0.034824, loss_ce: 0.011790
2022-01-11 23:20:49,801 iteration 2383 : loss : 0.024858, loss_ce: 0.008726
2022-01-11 23:20:51,314 iteration 2384 : loss : 0.043968, loss_ce: 0.018435
2022-01-11 23:20:52,838 iteration 2385 : loss : 0.050529, loss_ce: 0.018205
2022-01-11 23:20:54,292 iteration 2386 : loss : 0.036484, loss_ce: 0.011976
2022-01-11 23:20:55,725 iteration 2387 : loss : 0.038953, loss_ce: 0.012625
2022-01-11 23:20:57,197 iteration 2388 : loss : 0.030679, loss_ce: 0.013553
2022-01-11 23:20:58,610 iteration 2389 : loss : 0.019607, loss_ce: 0.006275
2022-01-11 23:21:00,164 iteration 2390 : loss : 0.045293, loss_ce: 0.023575
2022-01-11 23:21:01,724 iteration 2391 : loss : 0.040587, loss_ce: 0.024571
2022-01-11 23:21:03,328 iteration 2392 : loss : 0.050801, loss_ce: 0.022227
2022-01-11 23:21:04,932 iteration 2393 : loss : 0.059747, loss_ce: 0.019041
2022-01-11 23:21:06,420 iteration 2394 : loss : 0.033902, loss_ce: 0.011021
2022-01-11 23:21:08,039 iteration 2395 : loss : 0.038529, loss_ce: 0.016896
2022-01-11 23:21:09,569 iteration 2396 : loss : 0.037846, loss_ce: 0.019480
2022-01-11 23:21:11,165 iteration 2397 : loss : 0.040901, loss_ce: 0.015416
 35%|█████████▌                 | 141/400 [1:08:15<2:10:21, 30.20s/it]2022-01-11 23:21:12,760 iteration 2398 : loss : 0.040998, loss_ce: 0.014943
2022-01-11 23:21:14,300 iteration 2399 : loss : 0.032042, loss_ce: 0.009775
2022-01-11 23:21:15,918 iteration 2400 : loss : 0.048419, loss_ce: 0.023024
2022-01-11 23:21:17,483 iteration 2401 : loss : 0.031598, loss_ce: 0.011837
2022-01-11 23:21:19,081 iteration 2402 : loss : 0.032018, loss_ce: 0.016684
2022-01-11 23:21:20,608 iteration 2403 : loss : 0.038797, loss_ce: 0.017650
2022-01-11 23:21:22,126 iteration 2404 : loss : 0.042549, loss_ce: 0.020051
2022-01-11 23:21:23,618 iteration 2405 : loss : 0.031755, loss_ce: 0.011232
2022-01-11 23:21:25,303 iteration 2406 : loss : 0.074740, loss_ce: 0.028690
2022-01-11 23:21:26,809 iteration 2407 : loss : 0.038550, loss_ce: 0.020139
2022-01-11 23:21:28,319 iteration 2408 : loss : 0.023617, loss_ce: 0.010356
2022-01-11 23:21:29,926 iteration 2409 : loss : 0.035969, loss_ce: 0.012966
2022-01-11 23:21:31,550 iteration 2410 : loss : 0.036903, loss_ce: 0.015473
2022-01-11 23:21:33,140 iteration 2411 : loss : 0.046399, loss_ce: 0.013423
2022-01-11 23:21:34,667 iteration 2412 : loss : 0.019877, loss_ce: 0.007772
2022-01-11 23:21:36,298 iteration 2413 : loss : 0.036183, loss_ce: 0.015191
2022-01-11 23:21:37,878 iteration 2414 : loss : 0.036214, loss_ce: 0.014863
 36%|█████████▌                 | 142/400 [1:08:42<2:05:21, 29.15s/it]2022-01-11 23:21:39,496 iteration 2415 : loss : 0.039027, loss_ce: 0.010779
2022-01-11 23:21:41,043 iteration 2416 : loss : 0.028989, loss_ce: 0.010717
2022-01-11 23:21:42,600 iteration 2417 : loss : 0.031913, loss_ce: 0.011370
2022-01-11 23:21:44,256 iteration 2418 : loss : 0.034372, loss_ce: 0.013414
2022-01-11 23:21:45,845 iteration 2419 : loss : 0.037624, loss_ce: 0.019561
2022-01-11 23:21:47,507 iteration 2420 : loss : 0.040786, loss_ce: 0.018871
2022-01-11 23:21:49,140 iteration 2421 : loss : 0.037611, loss_ce: 0.013879
2022-01-11 23:21:50,746 iteration 2422 : loss : 0.037310, loss_ce: 0.013716
2022-01-11 23:21:52,302 iteration 2423 : loss : 0.060609, loss_ce: 0.026685
2022-01-11 23:21:53,863 iteration 2424 : loss : 0.026818, loss_ce: 0.011080
2022-01-11 23:21:55,522 iteration 2425 : loss : 0.049535, loss_ce: 0.022062
2022-01-11 23:21:57,078 iteration 2426 : loss : 0.044712, loss_ce: 0.013191
2022-01-11 23:21:58,556 iteration 2427 : loss : 0.028501, loss_ce: 0.013730
2022-01-11 23:22:00,082 iteration 2428 : loss : 0.040801, loss_ce: 0.012626
2022-01-11 23:22:01,621 iteration 2429 : loss : 0.043362, loss_ce: 0.015959
2022-01-11 23:22:03,158 iteration 2430 : loss : 0.024610, loss_ce: 0.009000
2022-01-11 23:22:04,732 iteration 2431 : loss : 0.035910, loss_ce: 0.019336
 36%|█████████▋                 | 143/400 [1:09:08<2:01:54, 28.46s/it]2022-01-11 23:22:06,304 iteration 2432 : loss : 0.033077, loss_ce: 0.011638
2022-01-11 23:22:07,859 iteration 2433 : loss : 0.034692, loss_ce: 0.014089
2022-01-11 23:22:09,438 iteration 2434 : loss : 0.034413, loss_ce: 0.011284
2022-01-11 23:22:11,097 iteration 2435 : loss : 0.051880, loss_ce: 0.032143
2022-01-11 23:22:12,590 iteration 2436 : loss : 0.026187, loss_ce: 0.011327
2022-01-11 23:22:14,208 iteration 2437 : loss : 0.031154, loss_ce: 0.010753
2022-01-11 23:22:15,837 iteration 2438 : loss : 0.045249, loss_ce: 0.016575
2022-01-11 23:22:17,454 iteration 2439 : loss : 0.054184, loss_ce: 0.017254
2022-01-11 23:22:19,080 iteration 2440 : loss : 0.055972, loss_ce: 0.020863
2022-01-11 23:22:20,647 iteration 2441 : loss : 0.030378, loss_ce: 0.010159
2022-01-11 23:22:22,198 iteration 2442 : loss : 0.032679, loss_ce: 0.013881
2022-01-11 23:22:23,705 iteration 2443 : loss : 0.030892, loss_ce: 0.011790
2022-01-11 23:22:25,273 iteration 2444 : loss : 0.034496, loss_ce: 0.014150
2022-01-11 23:22:26,858 iteration 2445 : loss : 0.041703, loss_ce: 0.017350
2022-01-11 23:22:28,507 iteration 2446 : loss : 0.033082, loss_ce: 0.017776
2022-01-11 23:22:30,014 iteration 2447 : loss : 0.028336, loss_ce: 0.009555
2022-01-11 23:22:31,631 iteration 2448 : loss : 0.033125, loss_ce: 0.012834
 36%|█████████▋                 | 144/400 [1:09:35<1:59:26, 27.99s/it]2022-01-11 23:22:33,180 iteration 2449 : loss : 0.040268, loss_ce: 0.016169
2022-01-11 23:22:34,695 iteration 2450 : loss : 0.026295, loss_ce: 0.012751
2022-01-11 23:22:36,246 iteration 2451 : loss : 0.030329, loss_ce: 0.009299
2022-01-11 23:22:37,772 iteration 2452 : loss : 0.031354, loss_ce: 0.010194
2022-01-11 23:22:39,346 iteration 2453 : loss : 0.054991, loss_ce: 0.019261
2022-01-11 23:22:40,902 iteration 2454 : loss : 0.033848, loss_ce: 0.010905
2022-01-11 23:22:42,424 iteration 2455 : loss : 0.036395, loss_ce: 0.015132
2022-01-11 23:22:43,928 iteration 2456 : loss : 0.028469, loss_ce: 0.012786
2022-01-11 23:22:45,470 iteration 2457 : loss : 0.035668, loss_ce: 0.014187
2022-01-11 23:22:46,988 iteration 2458 : loss : 0.029538, loss_ce: 0.011767
2022-01-11 23:22:48,460 iteration 2459 : loss : 0.025582, loss_ce: 0.010443
2022-01-11 23:22:49,994 iteration 2460 : loss : 0.044172, loss_ce: 0.012279
2022-01-11 23:22:51,618 iteration 2461 : loss : 0.042233, loss_ce: 0.015725
2022-01-11 23:22:53,212 iteration 2462 : loss : 0.036943, loss_ce: 0.010961
2022-01-11 23:22:54,754 iteration 2463 : loss : 0.031930, loss_ce: 0.015650
2022-01-11 23:22:56,338 iteration 2464 : loss : 0.027038, loss_ce: 0.009932
2022-01-11 23:22:56,338 Training Data Eval:
2022-01-11 23:23:04,349   Average segmentation loss on training set: 0.0218
2022-01-11 23:23:04,350 Validation Data Eval:
2022-01-11 23:23:07,105   Average segmentation loss on validation set: 0.0867
2022-01-11 23:23:08,660 iteration 2465 : loss : 0.032199, loss_ce: 0.012468
 36%|█████████▊                 | 145/400 [1:10:12<2:10:29, 30.70s/it]2022-01-11 23:23:10,324 iteration 2466 : loss : 0.034203, loss_ce: 0.013282
2022-01-11 23:23:11,963 iteration 2467 : loss : 0.036818, loss_ce: 0.016602
2022-01-11 23:23:13,512 iteration 2468 : loss : 0.058616, loss_ce: 0.035708
2022-01-11 23:23:15,025 iteration 2469 : loss : 0.021513, loss_ce: 0.009469
2022-01-11 23:23:16,585 iteration 2470 : loss : 0.034890, loss_ce: 0.011956
2022-01-11 23:23:18,229 iteration 2471 : loss : 0.048960, loss_ce: 0.018391
2022-01-11 23:23:19,766 iteration 2472 : loss : 0.035510, loss_ce: 0.013398
2022-01-11 23:23:21,372 iteration 2473 : loss : 0.034483, loss_ce: 0.014611
2022-01-11 23:23:22,910 iteration 2474 : loss : 0.033511, loss_ce: 0.014279
2022-01-11 23:23:24,472 iteration 2475 : loss : 0.026923, loss_ce: 0.010016
2022-01-11 23:23:25,990 iteration 2476 : loss : 0.034647, loss_ce: 0.017964
2022-01-11 23:23:27,624 iteration 2477 : loss : 0.029819, loss_ce: 0.011245
2022-01-11 23:23:29,212 iteration 2478 : loss : 0.028746, loss_ce: 0.010144
2022-01-11 23:23:30,700 iteration 2479 : loss : 0.030860, loss_ce: 0.012780
2022-01-11 23:23:32,261 iteration 2480 : loss : 0.037020, loss_ce: 0.014637
2022-01-11 23:23:33,880 iteration 2481 : loss : 0.033389, loss_ce: 0.012941
2022-01-11 23:23:35,423 iteration 2482 : loss : 0.023533, loss_ce: 0.010679
 36%|█████████▊                 | 146/400 [1:10:39<2:04:58, 29.52s/it]2022-01-11 23:23:37,096 iteration 2483 : loss : 0.040743, loss_ce: 0.019575
2022-01-11 23:23:38,685 iteration 2484 : loss : 0.034684, loss_ce: 0.008151
2022-01-11 23:23:40,211 iteration 2485 : loss : 0.025094, loss_ce: 0.011190
2022-01-11 23:23:41,771 iteration 2486 : loss : 0.035639, loss_ce: 0.015173
2022-01-11 23:23:43,310 iteration 2487 : loss : 0.029063, loss_ce: 0.009930
2022-01-11 23:23:44,915 iteration 2488 : loss : 0.030559, loss_ce: 0.012280
2022-01-11 23:23:46,470 iteration 2489 : loss : 0.029348, loss_ce: 0.012922
2022-01-11 23:23:48,071 iteration 2490 : loss : 0.035068, loss_ce: 0.015532
2022-01-11 23:23:49,711 iteration 2491 : loss : 0.032676, loss_ce: 0.015279
2022-01-11 23:23:51,182 iteration 2492 : loss : 0.025940, loss_ce: 0.010246
2022-01-11 23:23:52,758 iteration 2493 : loss : 0.031394, loss_ce: 0.014709
2022-01-11 23:23:54,307 iteration 2494 : loss : 0.040738, loss_ce: 0.011834
2022-01-11 23:23:55,846 iteration 2495 : loss : 0.029380, loss_ce: 0.013624
2022-01-11 23:23:57,560 iteration 2496 : loss : 0.040117, loss_ce: 0.011892
2022-01-11 23:23:59,140 iteration 2497 : loss : 0.037496, loss_ce: 0.013230
2022-01-11 23:24:00,744 iteration 2498 : loss : 0.039116, loss_ce: 0.017234
2022-01-11 23:24:02,246 iteration 2499 : loss : 0.023149, loss_ce: 0.006172
 37%|█████████▉                 | 147/400 [1:11:06<2:01:04, 28.71s/it]2022-01-11 23:24:03,830 iteration 2500 : loss : 0.028015, loss_ce: 0.010036
2022-01-11 23:24:05,377 iteration 2501 : loss : 0.025638, loss_ce: 0.007588
2022-01-11 23:24:07,006 iteration 2502 : loss : 0.054199, loss_ce: 0.028622
2022-01-11 23:24:08,515 iteration 2503 : loss : 0.034859, loss_ce: 0.012333
2022-01-11 23:24:10,042 iteration 2504 : loss : 0.027405, loss_ce: 0.009405
2022-01-11 23:24:11,638 iteration 2505 : loss : 0.044148, loss_ce: 0.017504
2022-01-11 23:24:13,198 iteration 2506 : loss : 0.038967, loss_ce: 0.012305
2022-01-11 23:24:14,719 iteration 2507 : loss : 0.030963, loss_ce: 0.012924
2022-01-11 23:24:16,252 iteration 2508 : loss : 0.031903, loss_ce: 0.010781
2022-01-11 23:24:17,799 iteration 2509 : loss : 0.050741, loss_ce: 0.018246
2022-01-11 23:24:19,324 iteration 2510 : loss : 0.025982, loss_ce: 0.011908
2022-01-11 23:24:20,894 iteration 2511 : loss : 0.037144, loss_ce: 0.019404
2022-01-11 23:24:22,471 iteration 2512 : loss : 0.041271, loss_ce: 0.014104
2022-01-11 23:24:24,086 iteration 2513 : loss : 0.034000, loss_ce: 0.010769
2022-01-11 23:24:25,649 iteration 2514 : loss : 0.045807, loss_ce: 0.014751
2022-01-11 23:24:27,198 iteration 2515 : loss : 0.024637, loss_ce: 0.014684
2022-01-11 23:24:28,737 iteration 2516 : loss : 0.023963, loss_ce: 0.010925
 37%|█████████▉                 | 148/400 [1:11:32<1:57:47, 28.05s/it]2022-01-11 23:24:30,315 iteration 2517 : loss : 0.027326, loss_ce: 0.010957
2022-01-11 23:24:31,874 iteration 2518 : loss : 0.044481, loss_ce: 0.013302
2022-01-11 23:24:33,457 iteration 2519 : loss : 0.053168, loss_ce: 0.019734
2022-01-11 23:24:35,015 iteration 2520 : loss : 0.051210, loss_ce: 0.019661
2022-01-11 23:24:36,548 iteration 2521 : loss : 0.031807, loss_ce: 0.010567
2022-01-11 23:24:38,122 iteration 2522 : loss : 0.029473, loss_ce: 0.014791
2022-01-11 23:24:39,727 iteration 2523 : loss : 0.043206, loss_ce: 0.011310
2022-01-11 23:24:41,263 iteration 2524 : loss : 0.028128, loss_ce: 0.012038
2022-01-11 23:24:42,834 iteration 2525 : loss : 0.040776, loss_ce: 0.016253
2022-01-11 23:24:44,426 iteration 2526 : loss : 0.034003, loss_ce: 0.011942
2022-01-11 23:24:45,986 iteration 2527 : loss : 0.028214, loss_ce: 0.010965
2022-01-11 23:24:47,554 iteration 2528 : loss : 0.036571, loss_ce: 0.012797
2022-01-11 23:24:49,122 iteration 2529 : loss : 0.039397, loss_ce: 0.016315
2022-01-11 23:24:50,692 iteration 2530 : loss : 0.035147, loss_ce: 0.016920
2022-01-11 23:24:52,321 iteration 2531 : loss : 0.033151, loss_ce: 0.012629
2022-01-11 23:24:53,884 iteration 2532 : loss : 0.033892, loss_ce: 0.010480
2022-01-11 23:24:55,449 iteration 2533 : loss : 0.035755, loss_ce: 0.009226
 37%|██████████                 | 149/400 [1:11:59<1:55:38, 27.64s/it]2022-01-11 23:24:56,975 iteration 2534 : loss : 0.030146, loss_ce: 0.013045
2022-01-11 23:24:58,605 iteration 2535 : loss : 0.045307, loss_ce: 0.013355
2022-01-11 23:25:00,127 iteration 2536 : loss : 0.030925, loss_ce: 0.015546
2022-01-11 23:25:01,655 iteration 2537 : loss : 0.027546, loss_ce: 0.009954
2022-01-11 23:25:03,125 iteration 2538 : loss : 0.024006, loss_ce: 0.010533
2022-01-11 23:25:04,712 iteration 2539 : loss : 0.030367, loss_ce: 0.011449
2022-01-11 23:25:06,222 iteration 2540 : loss : 0.024679, loss_ce: 0.007537
2022-01-11 23:25:07,757 iteration 2541 : loss : 0.032762, loss_ce: 0.012819
2022-01-11 23:25:09,258 iteration 2542 : loss : 0.026019, loss_ce: 0.008317
2022-01-11 23:25:10,767 iteration 2543 : loss : 0.024232, loss_ce: 0.007553
2022-01-11 23:25:12,324 iteration 2544 : loss : 0.053003, loss_ce: 0.018865
2022-01-11 23:25:13,971 iteration 2545 : loss : 0.025313, loss_ce: 0.010432
2022-01-11 23:25:15,641 iteration 2546 : loss : 0.046465, loss_ce: 0.020702
2022-01-11 23:25:17,131 iteration 2547 : loss : 0.022669, loss_ce: 0.010624
2022-01-11 23:25:18,719 iteration 2548 : loss : 0.034999, loss_ce: 0.015534
2022-01-11 23:25:20,297 iteration 2549 : loss : 0.062490, loss_ce: 0.016760
2022-01-11 23:25:20,297 Training Data Eval:
2022-01-11 23:25:28,283   Average segmentation loss on training set: 0.0309
2022-01-11 23:25:28,283 Validation Data Eval:
2022-01-11 23:25:31,028   Average segmentation loss on validation set: 0.0755
2022-01-11 23:25:32,564 iteration 2550 : loss : 0.046017, loss_ce: 0.015859
 38%|██████████▏                | 150/400 [1:12:36<2:07:02, 30.49s/it]2022-01-11 23:25:34,153 iteration 2551 : loss : 0.028407, loss_ce: 0.013550
2022-01-11 23:25:35,772 iteration 2552 : loss : 0.032201, loss_ce: 0.011891
2022-01-11 23:25:37,272 iteration 2553 : loss : 0.022573, loss_ce: 0.007555
2022-01-11 23:25:38,830 iteration 2554 : loss : 0.035856, loss_ce: 0.013681
2022-01-11 23:25:40,380 iteration 2555 : loss : 0.040504, loss_ce: 0.011960
2022-01-11 23:25:41,984 iteration 2556 : loss : 0.027297, loss_ce: 0.009919
2022-01-11 23:25:43,520 iteration 2557 : loss : 0.044303, loss_ce: 0.020579
2022-01-11 23:25:45,093 iteration 2558 : loss : 0.036538, loss_ce: 0.020690
2022-01-11 23:25:46,723 iteration 2559 : loss : 0.029954, loss_ce: 0.011418
2022-01-11 23:25:48,269 iteration 2560 : loss : 0.031085, loss_ce: 0.016555
2022-01-11 23:25:49,899 iteration 2561 : loss : 0.060501, loss_ce: 0.025358
2022-01-11 23:25:51,476 iteration 2562 : loss : 0.031190, loss_ce: 0.012235
2022-01-11 23:25:53,064 iteration 2563 : loss : 0.039427, loss_ce: 0.016906
2022-01-11 23:25:54,573 iteration 2564 : loss : 0.035110, loss_ce: 0.014101
2022-01-11 23:25:56,106 iteration 2565 : loss : 0.029345, loss_ce: 0.009773
2022-01-11 23:25:57,644 iteration 2566 : loss : 0.033722, loss_ce: 0.013345
2022-01-11 23:25:59,214 iteration 2567 : loss : 0.048147, loss_ce: 0.017705
 38%|██████████▏                | 151/400 [1:13:03<2:01:44, 29.33s/it]2022-01-11 23:26:00,827 iteration 2568 : loss : 0.029087, loss_ce: 0.012950
2022-01-11 23:26:02,346 iteration 2569 : loss : 0.028932, loss_ce: 0.014189
2022-01-11 23:26:03,889 iteration 2570 : loss : 0.046265, loss_ce: 0.021130
2022-01-11 23:26:05,433 iteration 2571 : loss : 0.023125, loss_ce: 0.008683
2022-01-11 23:26:06,936 iteration 2572 : loss : 0.052713, loss_ce: 0.015557
2022-01-11 23:26:08,445 iteration 2573 : loss : 0.027938, loss_ce: 0.011279
2022-01-11 23:26:09,987 iteration 2574 : loss : 0.033226, loss_ce: 0.011496
2022-01-11 23:26:11,649 iteration 2575 : loss : 0.033336, loss_ce: 0.014646
2022-01-11 23:26:13,214 iteration 2576 : loss : 0.025533, loss_ce: 0.009769
2022-01-11 23:26:14,773 iteration 2577 : loss : 0.047302, loss_ce: 0.019190
2022-01-11 23:26:16,358 iteration 2578 : loss : 0.048284, loss_ce: 0.015149
2022-01-11 23:26:17,856 iteration 2579 : loss : 0.028268, loss_ce: 0.008061
2022-01-11 23:26:19,480 iteration 2580 : loss : 0.047812, loss_ce: 0.012468
2022-01-11 23:26:21,069 iteration 2581 : loss : 0.025341, loss_ce: 0.010920
2022-01-11 23:26:22,691 iteration 2582 : loss : 0.027673, loss_ce: 0.010404
2022-01-11 23:26:24,243 iteration 2583 : loss : 0.023490, loss_ce: 0.008389
2022-01-11 23:26:25,717 iteration 2584 : loss : 0.023090, loss_ce: 0.009434
 38%|██████████▎                | 152/400 [1:13:29<1:57:44, 28.49s/it]2022-01-11 23:26:27,298 iteration 2585 : loss : 0.027947, loss_ce: 0.008459
2022-01-11 23:26:28,856 iteration 2586 : loss : 0.038535, loss_ce: 0.015880
2022-01-11 23:26:30,352 iteration 2587 : loss : 0.028105, loss_ce: 0.010683
2022-01-11 23:26:31,890 iteration 2588 : loss : 0.029959, loss_ce: 0.014584
2022-01-11 23:26:33,474 iteration 2589 : loss : 0.049313, loss_ce: 0.019418
2022-01-11 23:26:35,067 iteration 2590 : loss : 0.042798, loss_ce: 0.019473
2022-01-11 23:26:36,588 iteration 2591 : loss : 0.024262, loss_ce: 0.008241
2022-01-11 23:26:38,127 iteration 2592 : loss : 0.030069, loss_ce: 0.011064
2022-01-11 23:26:39,659 iteration 2593 : loss : 0.029535, loss_ce: 0.012152
2022-01-11 23:26:41,332 iteration 2594 : loss : 0.050398, loss_ce: 0.026034
2022-01-11 23:26:42,918 iteration 2595 : loss : 0.027550, loss_ce: 0.009434
2022-01-11 23:26:44,446 iteration 2596 : loss : 0.028998, loss_ce: 0.012929
2022-01-11 23:26:45,954 iteration 2597 : loss : 0.030397, loss_ce: 0.011650
2022-01-11 23:26:47,500 iteration 2598 : loss : 0.025485, loss_ce: 0.009970
2022-01-11 23:26:49,049 iteration 2599 : loss : 0.032637, loss_ce: 0.008347
2022-01-11 23:26:50,607 iteration 2600 : loss : 0.035424, loss_ce: 0.011621
2022-01-11 23:26:52,105 iteration 2601 : loss : 0.027419, loss_ce: 0.010891
 38%|██████████▎                | 153/400 [1:13:56<1:54:40, 27.86s/it]2022-01-11 23:26:53,730 iteration 2602 : loss : 0.028153, loss_ce: 0.012027
2022-01-11 23:26:55,220 iteration 2603 : loss : 0.023874, loss_ce: 0.009367
2022-01-11 23:26:56,758 iteration 2604 : loss : 0.029976, loss_ce: 0.012191
2022-01-11 23:26:58,423 iteration 2605 : loss : 0.025922, loss_ce: 0.008859
2022-01-11 23:26:59,973 iteration 2606 : loss : 0.029982, loss_ce: 0.012245
2022-01-11 23:27:01,537 iteration 2607 : loss : 0.031766, loss_ce: 0.013591
2022-01-11 23:27:03,046 iteration 2608 : loss : 0.028438, loss_ce: 0.013787
2022-01-11 23:27:04,569 iteration 2609 : loss : 0.028456, loss_ce: 0.008845
2022-01-11 23:27:06,219 iteration 2610 : loss : 0.033021, loss_ce: 0.013729
2022-01-11 23:27:07,736 iteration 2611 : loss : 0.029913, loss_ce: 0.011469
2022-01-11 23:27:09,204 iteration 2612 : loss : 0.025151, loss_ce: 0.012392
2022-01-11 23:27:10,723 iteration 2613 : loss : 0.029264, loss_ce: 0.009495
2022-01-11 23:27:12,328 iteration 2614 : loss : 0.055821, loss_ce: 0.017243
2022-01-11 23:27:13,840 iteration 2615 : loss : 0.030827, loss_ce: 0.011985
2022-01-11 23:27:15,367 iteration 2616 : loss : 0.027378, loss_ce: 0.010026
2022-01-11 23:27:16,989 iteration 2617 : loss : 0.031520, loss_ce: 0.009471
2022-01-11 23:27:18,545 iteration 2618 : loss : 0.033223, loss_ce: 0.016352
 38%|██████████▍                | 154/400 [1:14:22<1:52:28, 27.43s/it]2022-01-11 23:27:20,176 iteration 2619 : loss : 0.047412, loss_ce: 0.015855
2022-01-11 23:27:21,694 iteration 2620 : loss : 0.029681, loss_ce: 0.011247
2022-01-11 23:27:23,262 iteration 2621 : loss : 0.031932, loss_ce: 0.011057
2022-01-11 23:27:24,824 iteration 2622 : loss : 0.034935, loss_ce: 0.016790
2022-01-11 23:27:26,412 iteration 2623 : loss : 0.041515, loss_ce: 0.018432
2022-01-11 23:27:27,921 iteration 2624 : loss : 0.032104, loss_ce: 0.011444
2022-01-11 23:27:29,460 iteration 2625 : loss : 0.027383, loss_ce: 0.010390
2022-01-11 23:27:31,077 iteration 2626 : loss : 0.037376, loss_ce: 0.017439
2022-01-11 23:27:32,641 iteration 2627 : loss : 0.038052, loss_ce: 0.012228
2022-01-11 23:27:34,141 iteration 2628 : loss : 0.029939, loss_ce: 0.010340
2022-01-11 23:27:35,744 iteration 2629 : loss : 0.035829, loss_ce: 0.011068
2022-01-11 23:27:37,244 iteration 2630 : loss : 0.028841, loss_ce: 0.009297
2022-01-11 23:27:38,815 iteration 2631 : loss : 0.028697, loss_ce: 0.012483
2022-01-11 23:27:40,396 iteration 2632 : loss : 0.028386, loss_ce: 0.010971
2022-01-11 23:27:41,920 iteration 2633 : loss : 0.025484, loss_ce: 0.011686
2022-01-11 23:27:43,491 iteration 2634 : loss : 0.023895, loss_ce: 0.008686
2022-01-11 23:27:43,492 Training Data Eval:
2022-01-11 23:27:51,482   Average segmentation loss on training set: 0.0214
2022-01-11 23:27:51,483 Validation Data Eval:
2022-01-11 23:27:54,235   Average segmentation loss on validation set: 0.0753
2022-01-11 23:27:55,860 iteration 2635 : loss : 0.034544, loss_ce: 0.014179
 39%|██████████▍                | 155/400 [1:14:59<2:04:06, 30.39s/it]2022-01-11 23:27:57,421 iteration 2636 : loss : 0.024017, loss_ce: 0.007299
2022-01-11 23:27:59,104 iteration 2637 : loss : 0.044210, loss_ce: 0.012455
2022-01-11 23:28:00,660 iteration 2638 : loss : 0.026784, loss_ce: 0.010818
2022-01-11 23:28:02,128 iteration 2639 : loss : 0.025093, loss_ce: 0.009130
2022-01-11 23:28:03,638 iteration 2640 : loss : 0.036640, loss_ce: 0.018290
2022-01-11 23:28:05,272 iteration 2641 : loss : 0.040322, loss_ce: 0.016564
2022-01-11 23:28:06,859 iteration 2642 : loss : 0.038422, loss_ce: 0.012878
2022-01-11 23:28:08,453 iteration 2643 : loss : 0.043699, loss_ce: 0.014639
2022-01-11 23:28:09,987 iteration 2644 : loss : 0.031570, loss_ce: 0.008941
2022-01-11 23:28:11,494 iteration 2645 : loss : 0.028758, loss_ce: 0.014072
2022-01-11 23:28:13,021 iteration 2646 : loss : 0.033355, loss_ce: 0.010559
2022-01-11 23:28:14,622 iteration 2647 : loss : 0.037089, loss_ce: 0.021493
2022-01-11 23:28:16,095 iteration 2648 : loss : 0.019229, loss_ce: 0.008325
2022-01-11 23:28:17,669 iteration 2649 : loss : 0.035540, loss_ce: 0.015568
2022-01-11 23:28:19,245 iteration 2650 : loss : 0.029644, loss_ce: 0.013227
2022-01-11 23:28:20,777 iteration 2651 : loss : 0.034624, loss_ce: 0.012063
2022-01-11 23:28:22,342 iteration 2652 : loss : 0.029144, loss_ce: 0.013149
 39%|██████████▌                | 156/400 [1:15:26<1:58:50, 29.22s/it]2022-01-11 23:28:24,011 iteration 2653 : loss : 0.045743, loss_ce: 0.015234
2022-01-11 23:28:25,542 iteration 2654 : loss : 0.021901, loss_ce: 0.008287
2022-01-11 23:28:27,031 iteration 2655 : loss : 0.020935, loss_ce: 0.007982
2022-01-11 23:28:28,524 iteration 2656 : loss : 0.019198, loss_ce: 0.008830
2022-01-11 23:28:30,032 iteration 2657 : loss : 0.025676, loss_ce: 0.009793
2022-01-11 23:28:31,547 iteration 2658 : loss : 0.027109, loss_ce: 0.011031
2022-01-11 23:28:33,194 iteration 2659 : loss : 0.043237, loss_ce: 0.021333
2022-01-11 23:28:34,719 iteration 2660 : loss : 0.027119, loss_ce: 0.010203
2022-01-11 23:28:36,192 iteration 2661 : loss : 0.033263, loss_ce: 0.012548
2022-01-11 23:28:37,786 iteration 2662 : loss : 0.035353, loss_ce: 0.015840
2022-01-11 23:28:39,291 iteration 2663 : loss : 0.042688, loss_ce: 0.020109
2022-01-11 23:28:40,895 iteration 2664 : loss : 0.028020, loss_ce: 0.009796
2022-01-11 23:28:42,422 iteration 2665 : loss : 0.026261, loss_ce: 0.009739
2022-01-11 23:28:43,980 iteration 2666 : loss : 0.030796, loss_ce: 0.009754
2022-01-11 23:28:45,569 iteration 2667 : loss : 0.024012, loss_ce: 0.009731
2022-01-11 23:28:47,149 iteration 2668 : loss : 0.036387, loss_ce: 0.013414
2022-01-11 23:28:48,740 iteration 2669 : loss : 0.030088, loss_ce: 0.013554
 39%|██████████▌                | 157/400 [1:15:52<1:54:55, 28.38s/it]2022-01-11 23:28:50,349 iteration 2670 : loss : 0.033300, loss_ce: 0.008781
2022-01-11 23:28:51,905 iteration 2671 : loss : 0.028198, loss_ce: 0.008498
2022-01-11 23:28:53,473 iteration 2672 : loss : 0.024267, loss_ce: 0.009097
2022-01-11 23:28:55,068 iteration 2673 : loss : 0.060310, loss_ce: 0.023920
2022-01-11 23:28:56,585 iteration 2674 : loss : 0.028318, loss_ce: 0.013306
2022-01-11 23:28:58,190 iteration 2675 : loss : 0.033617, loss_ce: 0.013592
2022-01-11 23:28:59,749 iteration 2676 : loss : 0.031021, loss_ce: 0.008958
2022-01-11 23:29:01,247 iteration 2677 : loss : 0.032785, loss_ce: 0.015972
2022-01-11 23:29:02,742 iteration 2678 : loss : 0.023220, loss_ce: 0.007131
2022-01-11 23:29:04,317 iteration 2679 : loss : 0.025414, loss_ce: 0.012195
2022-01-11 23:29:05,923 iteration 2680 : loss : 0.038026, loss_ce: 0.017675
2022-01-11 23:29:07,469 iteration 2681 : loss : 0.035083, loss_ce: 0.016633
2022-01-11 23:29:08,971 iteration 2682 : loss : 0.026288, loss_ce: 0.010356
2022-01-11 23:29:10,537 iteration 2683 : loss : 0.028283, loss_ce: 0.013485
2022-01-11 23:29:12,101 iteration 2684 : loss : 0.029181, loss_ce: 0.007838
2022-01-11 23:29:13,646 iteration 2685 : loss : 0.030113, loss_ce: 0.014382
2022-01-11 23:29:15,189 iteration 2686 : loss : 0.023226, loss_ce: 0.008481
 40%|██████████▋                | 158/400 [1:16:19<1:52:07, 27.80s/it]2022-01-11 23:29:16,755 iteration 2687 : loss : 0.022565, loss_ce: 0.009461
2022-01-11 23:29:18,286 iteration 2688 : loss : 0.041764, loss_ce: 0.015205
2022-01-11 23:29:19,740 iteration 2689 : loss : 0.022990, loss_ce: 0.008506
2022-01-11 23:29:21,284 iteration 2690 : loss : 0.025361, loss_ce: 0.010246
2022-01-11 23:29:22,843 iteration 2691 : loss : 0.022342, loss_ce: 0.007157
2022-01-11 23:29:24,376 iteration 2692 : loss : 0.028618, loss_ce: 0.009198
2022-01-11 23:29:25,900 iteration 2693 : loss : 0.035851, loss_ce: 0.012957
2022-01-11 23:29:27,372 iteration 2694 : loss : 0.022226, loss_ce: 0.007905
2022-01-11 23:29:28,967 iteration 2695 : loss : 0.036648, loss_ce: 0.012905
2022-01-11 23:29:30,615 iteration 2696 : loss : 0.041684, loss_ce: 0.016690
2022-01-11 23:29:32,215 iteration 2697 : loss : 0.031499, loss_ce: 0.016886
2022-01-11 23:29:33,709 iteration 2698 : loss : 0.019550, loss_ce: 0.007549
2022-01-11 23:29:35,224 iteration 2699 : loss : 0.027745, loss_ce: 0.008974
2022-01-11 23:29:36,736 iteration 2700 : loss : 0.032992, loss_ce: 0.009818
2022-01-11 23:29:38,298 iteration 2701 : loss : 0.029002, loss_ce: 0.012147
2022-01-11 23:29:39,864 iteration 2702 : loss : 0.033737, loss_ce: 0.007913
2022-01-11 23:29:41,429 iteration 2703 : loss : 0.032446, loss_ce: 0.012583
 40%|██████████▋                | 159/400 [1:16:45<1:49:46, 27.33s/it]2022-01-11 23:29:42,955 iteration 2704 : loss : 0.023434, loss_ce: 0.007110
2022-01-11 23:29:44,462 iteration 2705 : loss : 0.029908, loss_ce: 0.010137
2022-01-11 23:29:46,039 iteration 2706 : loss : 0.038984, loss_ce: 0.018725
2022-01-11 23:29:47,581 iteration 2707 : loss : 0.026121, loss_ce: 0.009984
2022-01-11 23:29:49,102 iteration 2708 : loss : 0.029737, loss_ce: 0.010172
2022-01-11 23:29:50,708 iteration 2709 : loss : 0.033171, loss_ce: 0.010978
2022-01-11 23:29:52,296 iteration 2710 : loss : 0.034167, loss_ce: 0.014415
2022-01-11 23:29:53,821 iteration 2711 : loss : 0.022843, loss_ce: 0.007885
2022-01-11 23:29:55,349 iteration 2712 : loss : 0.047208, loss_ce: 0.019699
2022-01-11 23:29:56,911 iteration 2713 : loss : 0.032704, loss_ce: 0.012703
2022-01-11 23:29:58,465 iteration 2714 : loss : 0.027297, loss_ce: 0.011627
2022-01-11 23:30:00,006 iteration 2715 : loss : 0.023225, loss_ce: 0.012046
2022-01-11 23:30:01,547 iteration 2716 : loss : 0.037298, loss_ce: 0.010668
2022-01-11 23:30:03,046 iteration 2717 : loss : 0.026293, loss_ce: 0.010060
2022-01-11 23:30:04,615 iteration 2718 : loss : 0.032217, loss_ce: 0.011634
2022-01-11 23:30:06,131 iteration 2719 : loss : 0.024247, loss_ce: 0.010832
2022-01-11 23:30:06,131 Training Data Eval:
2022-01-11 23:30:14,097   Average segmentation loss on training set: 0.0207
2022-01-11 23:30:14,098 Validation Data Eval:
2022-01-11 23:30:16,851   Average segmentation loss on validation set: 0.0708
2022-01-11 23:30:22,722 Found new lowest validation loss at iteration 2719! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 23:30:24,175 iteration 2720 : loss : 0.024614, loss_ce: 0.010643
 40%|██████████▊                | 160/400 [1:17:28<2:07:48, 31.95s/it]2022-01-11 23:30:25,671 iteration 2721 : loss : 0.027975, loss_ce: 0.009685
2022-01-11 23:30:27,149 iteration 2722 : loss : 0.026882, loss_ce: 0.015070
2022-01-11 23:30:28,681 iteration 2723 : loss : 0.035922, loss_ce: 0.016665
2022-01-11 23:30:30,082 iteration 2724 : loss : 0.029734, loss_ce: 0.010084
2022-01-11 23:30:31,519 iteration 2725 : loss : 0.049488, loss_ce: 0.015109
2022-01-11 23:30:33,052 iteration 2726 : loss : 0.022812, loss_ce: 0.008507
2022-01-11 23:30:34,508 iteration 2727 : loss : 0.029046, loss_ce: 0.012334
2022-01-11 23:30:36,006 iteration 2728 : loss : 0.025099, loss_ce: 0.009880
2022-01-11 23:30:37,501 iteration 2729 : loss : 0.045815, loss_ce: 0.022761
2022-01-11 23:30:39,060 iteration 2730 : loss : 0.034980, loss_ce: 0.013547
2022-01-11 23:30:40,606 iteration 2731 : loss : 0.032567, loss_ce: 0.013259
2022-01-11 23:30:42,085 iteration 2732 : loss : 0.027769, loss_ce: 0.011145
2022-01-11 23:30:43,645 iteration 2733 : loss : 0.021596, loss_ce: 0.009415
2022-01-11 23:30:45,212 iteration 2734 : loss : 0.047251, loss_ce: 0.012814
2022-01-11 23:30:46,783 iteration 2735 : loss : 0.025936, loss_ce: 0.012520
2022-01-11 23:30:48,327 iteration 2736 : loss : 0.031307, loss_ce: 0.009637
2022-01-11 23:30:49,869 iteration 2737 : loss : 0.033930, loss_ce: 0.009187
 40%|██████████▊                | 161/400 [1:17:53<1:59:47, 30.08s/it]2022-01-11 23:30:51,507 iteration 2738 : loss : 0.037883, loss_ce: 0.019753
2022-01-11 23:30:53,021 iteration 2739 : loss : 0.025526, loss_ce: 0.007819
2022-01-11 23:30:54,657 iteration 2740 : loss : 0.034070, loss_ce: 0.013524
2022-01-11 23:30:56,209 iteration 2741 : loss : 0.033872, loss_ce: 0.013008
2022-01-11 23:30:57,776 iteration 2742 : loss : 0.045233, loss_ce: 0.019677
2022-01-11 23:30:59,280 iteration 2743 : loss : 0.040616, loss_ce: 0.016279
2022-01-11 23:31:00,833 iteration 2744 : loss : 0.024102, loss_ce: 0.008519
2022-01-11 23:31:02,342 iteration 2745 : loss : 0.026039, loss_ce: 0.008238
2022-01-11 23:31:03,984 iteration 2746 : loss : 0.076876, loss_ce: 0.018384
2022-01-11 23:31:05,508 iteration 2747 : loss : 0.025082, loss_ce: 0.009848
2022-01-11 23:31:07,101 iteration 2748 : loss : 0.039261, loss_ce: 0.011250
2022-01-11 23:31:08,725 iteration 2749 : loss : 0.038652, loss_ce: 0.014431
2022-01-11 23:31:10,276 iteration 2750 : loss : 0.035722, loss_ce: 0.013851
2022-01-11 23:31:11,831 iteration 2751 : loss : 0.052040, loss_ce: 0.014175
2022-01-11 23:31:13,473 iteration 2752 : loss : 0.029036, loss_ce: 0.011165
2022-01-11 23:31:15,129 iteration 2753 : loss : 0.037628, loss_ce: 0.016962
2022-01-11 23:31:16,712 iteration 2754 : loss : 0.024945, loss_ce: 0.012343
 40%|██████████▉                | 162/400 [1:18:20<1:55:26, 29.10s/it]2022-01-11 23:31:18,196 iteration 2755 : loss : 0.024027, loss_ce: 0.007484
2022-01-11 23:31:19,779 iteration 2756 : loss : 0.033842, loss_ce: 0.014453
2022-01-11 23:31:21,350 iteration 2757 : loss : 0.030833, loss_ce: 0.013889
2022-01-11 23:31:22,909 iteration 2758 : loss : 0.028198, loss_ce: 0.010864
2022-01-11 23:31:24,471 iteration 2759 : loss : 0.045871, loss_ce: 0.016678
2022-01-11 23:31:26,046 iteration 2760 : loss : 0.025228, loss_ce: 0.011854
2022-01-11 23:31:27,649 iteration 2761 : loss : 0.038077, loss_ce: 0.012892
2022-01-11 23:31:29,211 iteration 2762 : loss : 0.024218, loss_ce: 0.010265
2022-01-11 23:31:30,808 iteration 2763 : loss : 0.058539, loss_ce: 0.018353
2022-01-11 23:31:32,314 iteration 2764 : loss : 0.020826, loss_ce: 0.008449
2022-01-11 23:31:33,984 iteration 2765 : loss : 0.031844, loss_ce: 0.013051
2022-01-11 23:31:35,485 iteration 2766 : loss : 0.029942, loss_ce: 0.010391
2022-01-11 23:31:37,062 iteration 2767 : loss : 0.025588, loss_ce: 0.010752
2022-01-11 23:31:38,603 iteration 2768 : loss : 0.028043, loss_ce: 0.009107
2022-01-11 23:31:40,100 iteration 2769 : loss : 0.024025, loss_ce: 0.010481
2022-01-11 23:31:41,637 iteration 2770 : loss : 0.032152, loss_ce: 0.015490
2022-01-11 23:31:43,203 iteration 2771 : loss : 0.021130, loss_ce: 0.009320
 41%|███████████                | 163/400 [1:18:47<1:51:52, 28.32s/it]2022-01-11 23:31:44,745 iteration 2772 : loss : 0.032221, loss_ce: 0.010413
2022-01-11 23:31:46,320 iteration 2773 : loss : 0.031727, loss_ce: 0.012996
2022-01-11 23:31:47,886 iteration 2774 : loss : 0.045000, loss_ce: 0.009387
2022-01-11 23:31:49,450 iteration 2775 : loss : 0.034876, loss_ce: 0.013416
2022-01-11 23:31:50,997 iteration 2776 : loss : 0.025161, loss_ce: 0.009537
2022-01-11 23:31:52,614 iteration 2777 : loss : 0.020742, loss_ce: 0.007715
2022-01-11 23:31:54,135 iteration 2778 : loss : 0.031019, loss_ce: 0.009313
2022-01-11 23:31:55,666 iteration 2779 : loss : 0.023144, loss_ce: 0.008961
2022-01-11 23:31:57,219 iteration 2780 : loss : 0.025175, loss_ce: 0.010806
2022-01-11 23:31:58,876 iteration 2781 : loss : 0.028933, loss_ce: 0.011173
2022-01-11 23:32:00,503 iteration 2782 : loss : 0.031088, loss_ce: 0.013392
2022-01-11 23:32:02,055 iteration 2783 : loss : 0.031223, loss_ce: 0.009743
2022-01-11 23:32:03,516 iteration 2784 : loss : 0.027406, loss_ce: 0.007503
2022-01-11 23:32:05,031 iteration 2785 : loss : 0.021585, loss_ce: 0.009497
2022-01-11 23:32:06,535 iteration 2786 : loss : 0.023182, loss_ce: 0.009977
2022-01-11 23:32:08,061 iteration 2787 : loss : 0.026438, loss_ce: 0.009750
2022-01-11 23:32:09,742 iteration 2788 : loss : 0.041252, loss_ce: 0.016260
 41%|███████████                | 164/400 [1:19:13<1:49:17, 27.79s/it]2022-01-11 23:32:11,353 iteration 2789 : loss : 0.024965, loss_ce: 0.007374
2022-01-11 23:32:12,982 iteration 2790 : loss : 0.049046, loss_ce: 0.023327
2022-01-11 23:32:14,481 iteration 2791 : loss : 0.024156, loss_ce: 0.007224
2022-01-11 23:32:16,027 iteration 2792 : loss : 0.029368, loss_ce: 0.011335
2022-01-11 23:32:17,581 iteration 2793 : loss : 0.029338, loss_ce: 0.010710
2022-01-11 23:32:19,193 iteration 2794 : loss : 0.032069, loss_ce: 0.016134
2022-01-11 23:32:20,746 iteration 2795 : loss : 0.033229, loss_ce: 0.014954
2022-01-11 23:32:22,267 iteration 2796 : loss : 0.030035, loss_ce: 0.009655
2022-01-11 23:32:23,856 iteration 2797 : loss : 0.032355, loss_ce: 0.017648
2022-01-11 23:32:25,442 iteration 2798 : loss : 0.039847, loss_ce: 0.011917
2022-01-11 23:32:27,076 iteration 2799 : loss : 0.037579, loss_ce: 0.015248
2022-01-11 23:32:28,646 iteration 2800 : loss : 0.030024, loss_ce: 0.014121
2022-01-11 23:32:30,124 iteration 2801 : loss : 0.025393, loss_ce: 0.007125
2022-01-11 23:32:31,649 iteration 2802 : loss : 0.026058, loss_ce: 0.009493
2022-01-11 23:32:33,231 iteration 2803 : loss : 0.025553, loss_ce: 0.009066
2022-01-11 23:32:34,711 iteration 2804 : loss : 0.029338, loss_ce: 0.011640
2022-01-11 23:32:34,711 Training Data Eval:
2022-01-11 23:32:42,720   Average segmentation loss on training set: 0.0185
2022-01-11 23:32:42,721 Validation Data Eval:
2022-01-11 23:32:45,470   Average segmentation loss on validation set: 0.0820
2022-01-11 23:32:47,128 iteration 2805 : loss : 0.042037, loss_ce: 0.014657
 41%|███████████▏               | 165/400 [1:19:51<2:00:07, 30.67s/it]2022-01-11 23:32:48,790 iteration 2806 : loss : 0.028877, loss_ce: 0.011706
2022-01-11 23:32:50,425 iteration 2807 : loss : 0.033575, loss_ce: 0.010451
2022-01-11 23:32:52,042 iteration 2808 : loss : 0.043616, loss_ce: 0.013646
2022-01-11 23:32:53,626 iteration 2809 : loss : 0.028942, loss_ce: 0.010119
2022-01-11 23:32:55,127 iteration 2810 : loss : 0.038644, loss_ce: 0.013280
2022-01-11 23:32:56,733 iteration 2811 : loss : 0.024970, loss_ce: 0.012316
2022-01-11 23:32:58,236 iteration 2812 : loss : 0.019719, loss_ce: 0.008317
2022-01-11 23:32:59,864 iteration 2813 : loss : 0.037726, loss_ce: 0.019499
2022-01-11 23:33:01,435 iteration 2814 : loss : 0.029423, loss_ce: 0.010597
2022-01-11 23:33:02,990 iteration 2815 : loss : 0.031410, loss_ce: 0.013754
2022-01-11 23:33:04,548 iteration 2816 : loss : 0.026636, loss_ce: 0.009765
2022-01-11 23:33:06,184 iteration 2817 : loss : 0.050598, loss_ce: 0.009713
2022-01-11 23:33:07,801 iteration 2818 : loss : 0.034646, loss_ce: 0.016834
2022-01-11 23:33:09,359 iteration 2819 : loss : 0.026472, loss_ce: 0.011689
2022-01-11 23:33:11,019 iteration 2820 : loss : 0.049826, loss_ce: 0.018837
2022-01-11 23:33:12,645 iteration 2821 : loss : 0.042278, loss_ce: 0.014974
2022-01-11 23:33:14,241 iteration 2822 : loss : 0.029884, loss_ce: 0.009632
 42%|███████████▏               | 166/400 [1:20:18<1:55:26, 29.60s/it]2022-01-11 23:33:15,944 iteration 2823 : loss : 0.048042, loss_ce: 0.021526
2022-01-11 23:33:17,474 iteration 2824 : loss : 0.028554, loss_ce: 0.013482
2022-01-11 23:33:19,050 iteration 2825 : loss : 0.047102, loss_ce: 0.024598
2022-01-11 23:33:20,723 iteration 2826 : loss : 0.032899, loss_ce: 0.012560
2022-01-11 23:33:22,387 iteration 2827 : loss : 0.040630, loss_ce: 0.013208
2022-01-11 23:33:23,966 iteration 2828 : loss : 0.032341, loss_ce: 0.010451
2022-01-11 23:33:25,528 iteration 2829 : loss : 0.028379, loss_ce: 0.014173
2022-01-11 23:33:27,089 iteration 2830 : loss : 0.035284, loss_ce: 0.013399
2022-01-11 23:33:28,563 iteration 2831 : loss : 0.021713, loss_ce: 0.009130
2022-01-11 23:33:30,253 iteration 2832 : loss : 0.028614, loss_ce: 0.012221
2022-01-11 23:33:31,797 iteration 2833 : loss : 0.025683, loss_ce: 0.009083
2022-01-11 23:33:33,422 iteration 2834 : loss : 0.035530, loss_ce: 0.011704
2022-01-11 23:33:35,014 iteration 2835 : loss : 0.042380, loss_ce: 0.013571
2022-01-11 23:33:36,566 iteration 2836 : loss : 0.038196, loss_ce: 0.013664
2022-01-11 23:33:38,178 iteration 2837 : loss : 0.036199, loss_ce: 0.015622
2022-01-11 23:33:39,678 iteration 2838 : loss : 0.022185, loss_ce: 0.009391
2022-01-11 23:33:41,275 iteration 2839 : loss : 0.030215, loss_ce: 0.010806
 42%|███████████▎               | 167/400 [1:20:45<1:51:57, 28.83s/it]2022-01-11 23:33:42,931 iteration 2840 : loss : 0.023629, loss_ce: 0.008559
2022-01-11 23:33:44,480 iteration 2841 : loss : 0.020791, loss_ce: 0.006514
2022-01-11 23:33:46,025 iteration 2842 : loss : 0.036568, loss_ce: 0.012874
2022-01-11 23:33:47,630 iteration 2843 : loss : 0.037168, loss_ce: 0.010770
2022-01-11 23:33:49,205 iteration 2844 : loss : 0.032308, loss_ce: 0.015110
2022-01-11 23:33:50,757 iteration 2845 : loss : 0.065396, loss_ce: 0.033045
2022-01-11 23:33:52,325 iteration 2846 : loss : 0.022734, loss_ce: 0.008193
2022-01-11 23:33:53,773 iteration 2847 : loss : 0.017607, loss_ce: 0.009041
2022-01-11 23:33:55,359 iteration 2848 : loss : 0.028723, loss_ce: 0.010176
2022-01-11 23:33:56,976 iteration 2849 : loss : 0.042748, loss_ce: 0.015471
2022-01-11 23:33:58,578 iteration 2850 : loss : 0.038103, loss_ce: 0.013619
2022-01-11 23:34:00,185 iteration 2851 : loss : 0.040503, loss_ce: 0.015284
2022-01-11 23:34:01,762 iteration 2852 : loss : 0.037404, loss_ce: 0.010599
2022-01-11 23:34:03,394 iteration 2853 : loss : 0.025251, loss_ce: 0.009029
2022-01-11 23:34:04,982 iteration 2854 : loss : 0.028814, loss_ce: 0.009765
2022-01-11 23:34:06,512 iteration 2855 : loss : 0.028701, loss_ce: 0.014289
2022-01-11 23:34:08,115 iteration 2856 : loss : 0.039113, loss_ce: 0.019344
 42%|███████████▎               | 168/400 [1:21:12<1:49:10, 28.23s/it]2022-01-11 23:34:09,633 iteration 2857 : loss : 0.023875, loss_ce: 0.008011
2022-01-11 23:34:11,209 iteration 2858 : loss : 0.025375, loss_ce: 0.011635
2022-01-11 23:34:12,745 iteration 2859 : loss : 0.020310, loss_ce: 0.008499
2022-01-11 23:34:14,347 iteration 2860 : loss : 0.031393, loss_ce: 0.010041
2022-01-11 23:34:15,836 iteration 2861 : loss : 0.024593, loss_ce: 0.010866
2022-01-11 23:34:17,481 iteration 2862 : loss : 0.049906, loss_ce: 0.015594
2022-01-11 23:34:18,996 iteration 2863 : loss : 0.028043, loss_ce: 0.007947
2022-01-11 23:34:20,595 iteration 2864 : loss : 0.027367, loss_ce: 0.010654
2022-01-11 23:34:22,202 iteration 2865 : loss : 0.041336, loss_ce: 0.017302
2022-01-11 23:34:23,750 iteration 2866 : loss : 0.031754, loss_ce: 0.012725
2022-01-11 23:34:25,407 iteration 2867 : loss : 0.037817, loss_ce: 0.015833
2022-01-11 23:34:26,933 iteration 2868 : loss : 0.030422, loss_ce: 0.012708
2022-01-11 23:34:28,572 iteration 2869 : loss : 0.032568, loss_ce: 0.013554
2022-01-11 23:34:30,164 iteration 2870 : loss : 0.028515, loss_ce: 0.010359
2022-01-11 23:34:31,683 iteration 2871 : loss : 0.020891, loss_ce: 0.008460
2022-01-11 23:34:33,174 iteration 2872 : loss : 0.038691, loss_ce: 0.014352
2022-01-11 23:34:34,781 iteration 2873 : loss : 0.032786, loss_ce: 0.010132
 42%|███████████▍               | 169/400 [1:21:38<1:46:53, 27.77s/it]2022-01-11 23:34:36,416 iteration 2874 : loss : 0.033076, loss_ce: 0.011528
2022-01-11 23:34:37,999 iteration 2875 : loss : 0.024520, loss_ce: 0.012045
2022-01-11 23:34:39,582 iteration 2876 : loss : 0.038551, loss_ce: 0.015186
2022-01-11 23:34:41,206 iteration 2877 : loss : 0.040138, loss_ce: 0.010714
2022-01-11 23:34:42,762 iteration 2878 : loss : 0.029267, loss_ce: 0.013059
2022-01-11 23:34:44,361 iteration 2879 : loss : 0.031220, loss_ce: 0.011019
2022-01-11 23:34:45,982 iteration 2880 : loss : 0.037721, loss_ce: 0.010982
2022-01-11 23:34:47,549 iteration 2881 : loss : 0.024071, loss_ce: 0.009962
2022-01-11 23:34:49,164 iteration 2882 : loss : 0.024545, loss_ce: 0.012149
2022-01-11 23:34:50,705 iteration 2883 : loss : 0.031963, loss_ce: 0.014086
2022-01-11 23:34:52,262 iteration 2884 : loss : 0.033541, loss_ce: 0.014961
2022-01-11 23:34:53,837 iteration 2885 : loss : 0.026394, loss_ce: 0.010523
2022-01-11 23:34:55,323 iteration 2886 : loss : 0.043836, loss_ce: 0.014384
2022-01-11 23:34:56,963 iteration 2887 : loss : 0.031533, loss_ce: 0.016642
2022-01-11 23:34:58,542 iteration 2888 : loss : 0.058043, loss_ce: 0.013122
2022-01-11 23:35:00,211 iteration 2889 : loss : 0.038615, loss_ce: 0.016884
2022-01-11 23:35:00,211 Training Data Eval:
2022-01-11 23:35:08,202   Average segmentation loss on training set: 0.0194
2022-01-11 23:35:08,203 Validation Data Eval:
2022-01-11 23:35:10,955   Average segmentation loss on validation set: 0.0913
2022-01-11 23:35:12,584 iteration 2890 : loss : 0.030438, loss_ce: 0.011399
 42%|███████████▍               | 170/400 [1:22:16<1:57:57, 30.77s/it]2022-01-11 23:35:14,223 iteration 2891 : loss : 0.027670, loss_ce: 0.011816
2022-01-11 23:35:15,863 iteration 2892 : loss : 0.034414, loss_ce: 0.008342
2022-01-11 23:35:17,402 iteration 2893 : loss : 0.028119, loss_ce: 0.010076
2022-01-11 23:35:19,046 iteration 2894 : loss : 0.038086, loss_ce: 0.012033
2022-01-11 23:35:20,674 iteration 2895 : loss : 0.032694, loss_ce: 0.014814
2022-01-11 23:35:22,214 iteration 2896 : loss : 0.039952, loss_ce: 0.013340
2022-01-11 23:35:23,796 iteration 2897 : loss : 0.036142, loss_ce: 0.012846
2022-01-11 23:35:25,308 iteration 2898 : loss : 0.022329, loss_ce: 0.009405
2022-01-11 23:35:26,911 iteration 2899 : loss : 0.035304, loss_ce: 0.012740
2022-01-11 23:35:28,466 iteration 2900 : loss : 0.033365, loss_ce: 0.013719
2022-01-11 23:35:30,040 iteration 2901 : loss : 0.034435, loss_ce: 0.011154
2022-01-11 23:35:31,548 iteration 2902 : loss : 0.033499, loss_ce: 0.011271
2022-01-11 23:35:33,080 iteration 2903 : loss : 0.028879, loss_ce: 0.015028
2022-01-11 23:35:34,604 iteration 2904 : loss : 0.028737, loss_ce: 0.012848
2022-01-11 23:35:36,107 iteration 2905 : loss : 0.026696, loss_ce: 0.013003
2022-01-11 23:35:37,710 iteration 2906 : loss : 0.041340, loss_ce: 0.022102
2022-01-11 23:35:39,240 iteration 2907 : loss : 0.028223, loss_ce: 0.009105
 43%|███████████▌               | 171/400 [1:22:43<1:52:44, 29.54s/it]2022-01-11 23:35:40,881 iteration 2908 : loss : 0.052996, loss_ce: 0.023268
2022-01-11 23:35:42,499 iteration 2909 : loss : 0.040161, loss_ce: 0.018965
2022-01-11 23:35:44,078 iteration 2910 : loss : 0.025227, loss_ce: 0.010544
2022-01-11 23:35:45,687 iteration 2911 : loss : 0.033702, loss_ce: 0.011216
2022-01-11 23:35:47,226 iteration 2912 : loss : 0.031766, loss_ce: 0.010969
2022-01-11 23:35:48,858 iteration 2913 : loss : 0.030200, loss_ce: 0.013783
2022-01-11 23:35:50,411 iteration 2914 : loss : 0.034691, loss_ce: 0.013197
2022-01-11 23:35:51,960 iteration 2915 : loss : 0.027070, loss_ce: 0.011286
2022-01-11 23:35:53,541 iteration 2916 : loss : 0.024015, loss_ce: 0.009220
2022-01-11 23:35:55,041 iteration 2917 : loss : 0.025632, loss_ce: 0.012470
2022-01-11 23:35:56,577 iteration 2918 : loss : 0.046622, loss_ce: 0.015200
2022-01-11 23:35:58,135 iteration 2919 : loss : 0.025384, loss_ce: 0.009072
2022-01-11 23:35:59,733 iteration 2920 : loss : 0.038395, loss_ce: 0.015544
2022-01-11 23:36:01,298 iteration 2921 : loss : 0.026721, loss_ce: 0.011915
2022-01-11 23:36:02,796 iteration 2922 : loss : 0.033430, loss_ce: 0.010595
2022-01-11 23:36:04,335 iteration 2923 : loss : 0.032914, loss_ce: 0.010649
2022-01-11 23:36:05,897 iteration 2924 : loss : 0.067487, loss_ce: 0.024652
 43%|███████████▌               | 172/400 [1:23:10<1:48:57, 28.67s/it]2022-01-11 23:36:07,412 iteration 2925 : loss : 0.023939, loss_ce: 0.012347
2022-01-11 23:36:09,062 iteration 2926 : loss : 0.037595, loss_ce: 0.016757
2022-01-11 23:36:10,611 iteration 2927 : loss : 0.031345, loss_ce: 0.012106
2022-01-11 23:36:12,071 iteration 2928 : loss : 0.029593, loss_ce: 0.009465
2022-01-11 23:36:13,608 iteration 2929 : loss : 0.031223, loss_ce: 0.012773
2022-01-11 23:36:15,239 iteration 2930 : loss : 0.047008, loss_ce: 0.019567
2022-01-11 23:36:16,776 iteration 2931 : loss : 0.032302, loss_ce: 0.011613
2022-01-11 23:36:18,297 iteration 2932 : loss : 0.033216, loss_ce: 0.013355
2022-01-11 23:36:19,924 iteration 2933 : loss : 0.041579, loss_ce: 0.012808
2022-01-11 23:36:21,477 iteration 2934 : loss : 0.030678, loss_ce: 0.010979
2022-01-11 23:36:23,052 iteration 2935 : loss : 0.031384, loss_ce: 0.010787
2022-01-11 23:36:24,608 iteration 2936 : loss : 0.022020, loss_ce: 0.007015
2022-01-11 23:36:26,180 iteration 2937 : loss : 0.025551, loss_ce: 0.012082
2022-01-11 23:36:27,776 iteration 2938 : loss : 0.033309, loss_ce: 0.010441
2022-01-11 23:36:29,386 iteration 2939 : loss : 0.022127, loss_ce: 0.006762
2022-01-11 23:36:31,036 iteration 2940 : loss : 0.057236, loss_ce: 0.027362
2022-01-11 23:36:32,543 iteration 2941 : loss : 0.026348, loss_ce: 0.008641
 43%|███████████▋               | 173/400 [1:23:36<1:46:11, 28.07s/it]2022-01-11 23:36:34,100 iteration 2942 : loss : 0.023399, loss_ce: 0.007702
2022-01-11 23:36:35,687 iteration 2943 : loss : 0.033677, loss_ce: 0.009859
2022-01-11 23:36:37,217 iteration 2944 : loss : 0.023861, loss_ce: 0.012046
2022-01-11 23:36:38,794 iteration 2945 : loss : 0.021015, loss_ce: 0.006137
2022-01-11 23:36:40,256 iteration 2946 : loss : 0.032774, loss_ce: 0.012301
2022-01-11 23:36:41,795 iteration 2947 : loss : 0.028994, loss_ce: 0.011493
2022-01-11 23:36:43,322 iteration 2948 : loss : 0.023123, loss_ce: 0.006442
2022-01-11 23:36:44,937 iteration 2949 : loss : 0.036329, loss_ce: 0.012764
2022-01-11 23:36:46,503 iteration 2950 : loss : 0.026481, loss_ce: 0.011709
2022-01-11 23:36:48,090 iteration 2951 : loss : 0.025021, loss_ce: 0.011202
2022-01-11 23:36:49,676 iteration 2952 : loss : 0.022893, loss_ce: 0.007035
2022-01-11 23:36:51,300 iteration 2953 : loss : 0.039153, loss_ce: 0.020239
2022-01-11 23:36:52,946 iteration 2954 : loss : 0.053610, loss_ce: 0.015977
2022-01-11 23:36:54,477 iteration 2955 : loss : 0.026632, loss_ce: 0.011971
2022-01-11 23:36:56,022 iteration 2956 : loss : 0.028068, loss_ce: 0.013600
2022-01-11 23:36:57,535 iteration 2957 : loss : 0.023468, loss_ce: 0.009260
2022-01-11 23:36:59,102 iteration 2958 : loss : 0.038783, loss_ce: 0.017272
 44%|███████████▋               | 174/400 [1:24:03<1:44:00, 27.61s/it]2022-01-11 23:37:00,674 iteration 2959 : loss : 0.023642, loss_ce: 0.008503
2022-01-11 23:37:02,209 iteration 2960 : loss : 0.028510, loss_ce: 0.013802
2022-01-11 23:37:03,779 iteration 2961 : loss : 0.035311, loss_ce: 0.012652
2022-01-11 23:37:05,471 iteration 2962 : loss : 0.031449, loss_ce: 0.010529
2022-01-11 23:37:07,045 iteration 2963 : loss : 0.036546, loss_ce: 0.014224
2022-01-11 23:37:08,524 iteration 2964 : loss : 0.022527, loss_ce: 0.008456
2022-01-11 23:37:10,100 iteration 2965 : loss : 0.024011, loss_ce: 0.013283
2022-01-11 23:37:11,700 iteration 2966 : loss : 0.040133, loss_ce: 0.013832
2022-01-11 23:37:13,256 iteration 2967 : loss : 0.029988, loss_ce: 0.010756
2022-01-11 23:37:14,831 iteration 2968 : loss : 0.026198, loss_ce: 0.011174
2022-01-11 23:37:16,372 iteration 2969 : loss : 0.030534, loss_ce: 0.012720
2022-01-11 23:37:17,946 iteration 2970 : loss : 0.037075, loss_ce: 0.014935
2022-01-11 23:37:19,497 iteration 2971 : loss : 0.024207, loss_ce: 0.010503
2022-01-11 23:37:21,057 iteration 2972 : loss : 0.031132, loss_ce: 0.014121
2022-01-11 23:37:22,569 iteration 2973 : loss : 0.023716, loss_ce: 0.005976
2022-01-11 23:37:24,177 iteration 2974 : loss : 0.026678, loss_ce: 0.007637
2022-01-11 23:37:24,177 Training Data Eval:
2022-01-11 23:37:32,176   Average segmentation loss on training set: 0.0178
2022-01-11 23:37:32,177 Validation Data Eval:
2022-01-11 23:37:34,935   Average segmentation loss on validation set: 0.0674
2022-01-11 23:37:40,712 Found new lowest validation loss at iteration 2974! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 23:37:42,149 iteration 2975 : loss : 0.020440, loss_ce: 0.007533
 44%|███████████▊               | 175/400 [1:24:46<2:00:54, 32.24s/it]2022-01-11 23:37:43,558 iteration 2976 : loss : 0.026346, loss_ce: 0.013157
2022-01-11 23:37:45,049 iteration 2977 : loss : 0.032492, loss_ce: 0.015003
2022-01-11 23:37:46,521 iteration 2978 : loss : 0.026449, loss_ce: 0.008212
2022-01-11 23:37:48,031 iteration 2979 : loss : 0.033445, loss_ce: 0.011465
2022-01-11 23:37:49,530 iteration 2980 : loss : 0.027641, loss_ce: 0.007957
2022-01-11 23:37:51,096 iteration 2981 : loss : 0.031662, loss_ce: 0.013067
2022-01-11 23:37:52,562 iteration 2982 : loss : 0.023563, loss_ce: 0.008522
2022-01-11 23:37:54,037 iteration 2983 : loss : 0.025705, loss_ce: 0.009536
2022-01-11 23:37:55,522 iteration 2984 : loss : 0.032888, loss_ce: 0.010974
2022-01-11 23:37:57,066 iteration 2985 : loss : 0.038020, loss_ce: 0.012470
2022-01-11 23:37:58,576 iteration 2986 : loss : 0.025088, loss_ce: 0.010700
2022-01-11 23:38:00,156 iteration 2987 : loss : 0.034181, loss_ce: 0.011496
2022-01-11 23:38:01,782 iteration 2988 : loss : 0.026909, loss_ce: 0.010159
2022-01-11 23:38:03,395 iteration 2989 : loss : 0.028978, loss_ce: 0.011711
2022-01-11 23:38:04,909 iteration 2990 : loss : 0.017485, loss_ce: 0.007041
2022-01-11 23:38:06,474 iteration 2991 : loss : 0.025924, loss_ce: 0.010496
2022-01-11 23:38:08,113 iteration 2992 : loss : 0.031936, loss_ce: 0.014134
 44%|███████████▉               | 176/400 [1:25:12<1:53:20, 30.36s/it]2022-01-11 23:38:09,667 iteration 2993 : loss : 0.026577, loss_ce: 0.013363
2022-01-11 23:38:11,244 iteration 2994 : loss : 0.041570, loss_ce: 0.010394
2022-01-11 23:38:12,793 iteration 2995 : loss : 0.031586, loss_ce: 0.012793
2022-01-11 23:38:14,347 iteration 2996 : loss : 0.018560, loss_ce: 0.009354
2022-01-11 23:38:15,904 iteration 2997 : loss : 0.038582, loss_ce: 0.013682
2022-01-11 23:38:17,484 iteration 2998 : loss : 0.041652, loss_ce: 0.018040
2022-01-11 23:38:19,102 iteration 2999 : loss : 0.070714, loss_ce: 0.018508
2022-01-11 23:38:20,742 iteration 3000 : loss : 0.039362, loss_ce: 0.014514
2022-01-11 23:38:22,377 iteration 3001 : loss : 0.045459, loss_ce: 0.010748
2022-01-11 23:38:23,894 iteration 3002 : loss : 0.022075, loss_ce: 0.008861
2022-01-11 23:38:25,432 iteration 3003 : loss : 0.024421, loss_ce: 0.008787
2022-01-11 23:38:26,977 iteration 3004 : loss : 0.022903, loss_ce: 0.009495
2022-01-11 23:38:28,548 iteration 3005 : loss : 0.031110, loss_ce: 0.012687
2022-01-11 23:38:30,142 iteration 3006 : loss : 0.025888, loss_ce: 0.010049
2022-01-11 23:38:31,731 iteration 3007 : loss : 0.035238, loss_ce: 0.013864
2022-01-11 23:38:33,330 iteration 3008 : loss : 0.038773, loss_ce: 0.012800
2022-01-11 23:38:34,830 iteration 3009 : loss : 0.029404, loss_ce: 0.012381
 44%|███████████▉               | 177/400 [1:25:38<1:48:46, 29.27s/it]2022-01-11 23:38:36,535 iteration 3010 : loss : 0.048578, loss_ce: 0.021089
2022-01-11 23:38:38,126 iteration 3011 : loss : 0.032395, loss_ce: 0.013473
2022-01-11 23:38:39,653 iteration 3012 : loss : 0.021706, loss_ce: 0.009388
2022-01-11 23:38:41,154 iteration 3013 : loss : 0.037253, loss_ce: 0.011115
2022-01-11 23:38:42,725 iteration 3014 : loss : 0.043518, loss_ce: 0.016773
2022-01-11 23:38:44,291 iteration 3015 : loss : 0.044482, loss_ce: 0.010835
2022-01-11 23:38:45,843 iteration 3016 : loss : 0.042958, loss_ce: 0.016984
2022-01-11 23:38:47,383 iteration 3017 : loss : 0.029914, loss_ce: 0.016495
2022-01-11 23:38:48,915 iteration 3018 : loss : 0.019131, loss_ce: 0.008808
2022-01-11 23:38:50,413 iteration 3019 : loss : 0.029177, loss_ce: 0.011615
2022-01-11 23:38:52,001 iteration 3020 : loss : 0.034551, loss_ce: 0.011914
2022-01-11 23:38:53,584 iteration 3021 : loss : 0.019349, loss_ce: 0.006926
2022-01-11 23:38:55,138 iteration 3022 : loss : 0.021480, loss_ce: 0.008222
2022-01-11 23:38:56,719 iteration 3023 : loss : 0.029369, loss_ce: 0.010629
2022-01-11 23:38:58,376 iteration 3024 : loss : 0.044783, loss_ce: 0.028277
2022-01-11 23:39:00,003 iteration 3025 : loss : 0.035751, loss_ce: 0.015174
2022-01-11 23:39:01,529 iteration 3026 : loss : 0.023644, loss_ce: 0.009757
 44%|████████████               | 178/400 [1:26:05<1:45:25, 28.50s/it]2022-01-11 23:39:03,179 iteration 3027 : loss : 0.041225, loss_ce: 0.015481
2022-01-11 23:39:04,699 iteration 3028 : loss : 0.024832, loss_ce: 0.009743
2022-01-11 23:39:06,278 iteration 3029 : loss : 0.023568, loss_ce: 0.009108
2022-01-11 23:39:07,812 iteration 3030 : loss : 0.020193, loss_ce: 0.008793
2022-01-11 23:39:09,365 iteration 3031 : loss : 0.032215, loss_ce: 0.011974
2022-01-11 23:39:10,892 iteration 3032 : loss : 0.024319, loss_ce: 0.008512
2022-01-11 23:39:12,485 iteration 3033 : loss : 0.029632, loss_ce: 0.009457
2022-01-11 23:39:14,098 iteration 3034 : loss : 0.038303, loss_ce: 0.015992
2022-01-11 23:39:15,687 iteration 3035 : loss : 0.029001, loss_ce: 0.013113
2022-01-11 23:39:17,231 iteration 3036 : loss : 0.042550, loss_ce: 0.022275
2022-01-11 23:39:18,793 iteration 3037 : loss : 0.024868, loss_ce: 0.009216
2022-01-11 23:39:20,388 iteration 3038 : loss : 0.030192, loss_ce: 0.010839
2022-01-11 23:39:21,901 iteration 3039 : loss : 0.028945, loss_ce: 0.009208
2022-01-11 23:39:23,481 iteration 3040 : loss : 0.035557, loss_ce: 0.009699
2022-01-11 23:39:25,095 iteration 3041 : loss : 0.032063, loss_ce: 0.011943
2022-01-11 23:39:26,663 iteration 3042 : loss : 0.028080, loss_ce: 0.011775
2022-01-11 23:39:28,391 iteration 3043 : loss : 0.059076, loss_ce: 0.019307
 45%|████████████               | 179/400 [1:26:32<1:43:09, 28.01s/it]2022-01-11 23:39:29,995 iteration 3044 : loss : 0.034739, loss_ce: 0.014530
2022-01-11 23:39:31,505 iteration 3045 : loss : 0.029211, loss_ce: 0.011897
2022-01-11 23:39:33,052 iteration 3046 : loss : 0.022112, loss_ce: 0.007620
2022-01-11 23:39:34,716 iteration 3047 : loss : 0.039165, loss_ce: 0.019620
2022-01-11 23:39:36,236 iteration 3048 : loss : 0.031234, loss_ce: 0.010637
2022-01-11 23:39:37,796 iteration 3049 : loss : 0.028909, loss_ce: 0.012001
2022-01-11 23:39:39,382 iteration 3050 : loss : 0.030578, loss_ce: 0.013806
2022-01-11 23:39:40,951 iteration 3051 : loss : 0.033271, loss_ce: 0.010370
2022-01-11 23:39:42,499 iteration 3052 : loss : 0.023235, loss_ce: 0.008523
2022-01-11 23:39:44,140 iteration 3053 : loss : 0.019694, loss_ce: 0.007867
2022-01-11 23:39:45,660 iteration 3054 : loss : 0.026831, loss_ce: 0.007491
2022-01-11 23:39:47,244 iteration 3055 : loss : 0.031723, loss_ce: 0.011668
2022-01-11 23:39:48,787 iteration 3056 : loss : 0.023395, loss_ce: 0.009742
2022-01-11 23:39:50,300 iteration 3057 : loss : 0.028984, loss_ce: 0.011790
2022-01-11 23:39:51,881 iteration 3058 : loss : 0.024846, loss_ce: 0.009801
2022-01-11 23:39:53,474 iteration 3059 : loss : 0.022005, loss_ce: 0.006195
2022-01-11 23:39:53,474 Training Data Eval:
2022-01-11 23:40:01,458   Average segmentation loss on training set: 0.0181
2022-01-11 23:40:01,458 Validation Data Eval:
2022-01-11 23:40:04,216   Average segmentation loss on validation set: 0.0749
2022-01-11 23:40:05,760 iteration 3060 : loss : 0.025797, loss_ce: 0.013175
 45%|████████████▏              | 180/400 [1:27:09<1:52:59, 30.82s/it]2022-01-11 23:40:07,304 iteration 3061 : loss : 0.022240, loss_ce: 0.008829
2022-01-11 23:40:08,882 iteration 3062 : loss : 0.032079, loss_ce: 0.012022
2022-01-11 23:40:10,495 iteration 3063 : loss : 0.064135, loss_ce: 0.023165
2022-01-11 23:40:12,115 iteration 3064 : loss : 0.020854, loss_ce: 0.008717
2022-01-11 23:40:13,600 iteration 3065 : loss : 0.020710, loss_ce: 0.006994
2022-01-11 23:40:15,136 iteration 3066 : loss : 0.028961, loss_ce: 0.013842
2022-01-11 23:40:16,735 iteration 3067 : loss : 0.036156, loss_ce: 0.018099
2022-01-11 23:40:18,327 iteration 3068 : loss : 0.026648, loss_ce: 0.007576
2022-01-11 23:40:19,894 iteration 3069 : loss : 0.037688, loss_ce: 0.010341
2022-01-11 23:40:21,471 iteration 3070 : loss : 0.022545, loss_ce: 0.008291
2022-01-11 23:40:22,988 iteration 3071 : loss : 0.030991, loss_ce: 0.013096
2022-01-11 23:40:24,502 iteration 3072 : loss : 0.024655, loss_ce: 0.009032
2022-01-11 23:40:26,045 iteration 3073 : loss : 0.028118, loss_ce: 0.010763
2022-01-11 23:40:27,674 iteration 3074 : loss : 0.033774, loss_ce: 0.014734
2022-01-11 23:40:29,271 iteration 3075 : loss : 0.026406, loss_ce: 0.009132
2022-01-11 23:40:30,900 iteration 3076 : loss : 0.031287, loss_ce: 0.012626
2022-01-11 23:40:32,416 iteration 3077 : loss : 0.030947, loss_ce: 0.011023
 45%|████████████▏              | 181/400 [1:27:36<1:47:54, 29.57s/it]2022-01-11 23:40:33,959 iteration 3078 : loss : 0.026664, loss_ce: 0.011402
2022-01-11 23:40:35,633 iteration 3079 : loss : 0.021309, loss_ce: 0.007176
2022-01-11 23:40:37,126 iteration 3080 : loss : 0.039143, loss_ce: 0.012486
2022-01-11 23:40:38,718 iteration 3081 : loss : 0.025449, loss_ce: 0.009289
2022-01-11 23:40:40,273 iteration 3082 : loss : 0.038294, loss_ce: 0.008957
2022-01-11 23:40:41,880 iteration 3083 : loss : 0.030069, loss_ce: 0.011265
2022-01-11 23:40:43,336 iteration 3084 : loss : 0.020893, loss_ce: 0.009299
2022-01-11 23:40:44,917 iteration 3085 : loss : 0.022157, loss_ce: 0.009088
2022-01-11 23:40:46,578 iteration 3086 : loss : 0.028224, loss_ce: 0.014708
2022-01-11 23:40:48,114 iteration 3087 : loss : 0.049125, loss_ce: 0.022179
2022-01-11 23:40:49,695 iteration 3088 : loss : 0.025498, loss_ce: 0.007442
2022-01-11 23:40:51,341 iteration 3089 : loss : 0.047306, loss_ce: 0.017366
2022-01-11 23:40:52,820 iteration 3090 : loss : 0.024835, loss_ce: 0.010537
2022-01-11 23:40:54,389 iteration 3091 : loss : 0.034340, loss_ce: 0.015360
2022-01-11 23:40:56,089 iteration 3092 : loss : 0.036287, loss_ce: 0.015399
2022-01-11 23:40:57,622 iteration 3093 : loss : 0.026788, loss_ce: 0.011422
2022-01-11 23:40:59,200 iteration 3094 : loss : 0.027054, loss_ce: 0.011008
 46%|████████████▎              | 182/400 [1:28:03<1:44:23, 28.73s/it]2022-01-11 23:41:00,798 iteration 3095 : loss : 0.027058, loss_ce: 0.010291
2022-01-11 23:41:02,375 iteration 3096 : loss : 0.022243, loss_ce: 0.008974
2022-01-11 23:41:03,911 iteration 3097 : loss : 0.022524, loss_ce: 0.009400
2022-01-11 23:41:05,430 iteration 3098 : loss : 0.023472, loss_ce: 0.009644
2022-01-11 23:41:06,986 iteration 3099 : loss : 0.019855, loss_ce: 0.006418
2022-01-11 23:41:08,493 iteration 3100 : loss : 0.022913, loss_ce: 0.007010
2022-01-11 23:41:10,098 iteration 3101 : loss : 0.022843, loss_ce: 0.008134
2022-01-11 23:41:11,660 iteration 3102 : loss : 0.026034, loss_ce: 0.009781
2022-01-11 23:41:13,236 iteration 3103 : loss : 0.024811, loss_ce: 0.010166
2022-01-11 23:41:14,760 iteration 3104 : loss : 0.024880, loss_ce: 0.010452
2022-01-11 23:41:16,261 iteration 3105 : loss : 0.018706, loss_ce: 0.008063
2022-01-11 23:41:17,809 iteration 3106 : loss : 0.029823, loss_ce: 0.017303
2022-01-11 23:41:19,352 iteration 3107 : loss : 0.018809, loss_ce: 0.005200
2022-01-11 23:41:20,921 iteration 3108 : loss : 0.030438, loss_ce: 0.010490
2022-01-11 23:41:22,503 iteration 3109 : loss : 0.031452, loss_ce: 0.009450
2022-01-11 23:41:24,102 iteration 3110 : loss : 0.033872, loss_ce: 0.015035
2022-01-11 23:41:25,682 iteration 3111 : loss : 0.024912, loss_ce: 0.009770
 46%|████████████▎              | 183/400 [1:28:29<1:41:28, 28.06s/it]2022-01-11 23:41:27,294 iteration 3112 : loss : 0.030472, loss_ce: 0.011593
2022-01-11 23:41:28,808 iteration 3113 : loss : 0.020949, loss_ce: 0.009107
2022-01-11 23:41:30,409 iteration 3114 : loss : 0.025822, loss_ce: 0.010671
2022-01-11 23:41:31,973 iteration 3115 : loss : 0.022886, loss_ce: 0.010581
2022-01-11 23:41:33,517 iteration 3116 : loss : 0.020522, loss_ce: 0.009133
2022-01-11 23:41:35,121 iteration 3117 : loss : 0.022076, loss_ce: 0.008092
2022-01-11 23:41:36,653 iteration 3118 : loss : 0.033954, loss_ce: 0.012833
2022-01-11 23:41:38,311 iteration 3119 : loss : 0.035974, loss_ce: 0.011499
2022-01-11 23:41:39,810 iteration 3120 : loss : 0.020320, loss_ce: 0.008248
2022-01-11 23:41:41,437 iteration 3121 : loss : 0.021850, loss_ce: 0.010270
2022-01-11 23:41:42,985 iteration 3122 : loss : 0.032546, loss_ce: 0.008547
2022-01-11 23:41:44,598 iteration 3123 : loss : 0.034151, loss_ce: 0.015986
2022-01-11 23:41:46,085 iteration 3124 : loss : 0.023314, loss_ce: 0.008910
2022-01-11 23:41:47,643 iteration 3125 : loss : 0.032378, loss_ce: 0.010907
2022-01-11 23:41:49,242 iteration 3126 : loss : 0.024456, loss_ce: 0.012956
2022-01-11 23:41:50,855 iteration 3127 : loss : 0.024940, loss_ce: 0.008536
2022-01-11 23:41:52,471 iteration 3128 : loss : 0.025560, loss_ce: 0.008340
 46%|████████████▍              | 184/400 [1:28:56<1:39:38, 27.68s/it]2022-01-11 23:41:54,051 iteration 3129 : loss : 0.025478, loss_ce: 0.009530
2022-01-11 23:41:55,707 iteration 3130 : loss : 0.030164, loss_ce: 0.014601
2022-01-11 23:41:57,256 iteration 3131 : loss : 0.049556, loss_ce: 0.021111
2022-01-11 23:41:58,805 iteration 3132 : loss : 0.025143, loss_ce: 0.007500
2022-01-11 23:42:00,359 iteration 3133 : loss : 0.031470, loss_ce: 0.011932
2022-01-11 23:42:01,885 iteration 3134 : loss : 0.020559, loss_ce: 0.009110
2022-01-11 23:42:03,443 iteration 3135 : loss : 0.021340, loss_ce: 0.009605
2022-01-11 23:42:04,979 iteration 3136 : loss : 0.027026, loss_ce: 0.010884
2022-01-11 23:42:06,642 iteration 3137 : loss : 0.027098, loss_ce: 0.011456
2022-01-11 23:42:08,246 iteration 3138 : loss : 0.047712, loss_ce: 0.019086
2022-01-11 23:42:09,753 iteration 3139 : loss : 0.025049, loss_ce: 0.008725
2022-01-11 23:42:11,266 iteration 3140 : loss : 0.029456, loss_ce: 0.013544
2022-01-11 23:42:12,843 iteration 3141 : loss : 0.038645, loss_ce: 0.020646
2022-01-11 23:42:14,382 iteration 3142 : loss : 0.028580, loss_ce: 0.009756
2022-01-11 23:42:15,991 iteration 3143 : loss : 0.044666, loss_ce: 0.017263
2022-01-11 23:42:17,542 iteration 3144 : loss : 0.024532, loss_ce: 0.009064
2022-01-11 23:42:17,543 Training Data Eval:
2022-01-11 23:42:25,539   Average segmentation loss on training set: 0.0160
2022-01-11 23:42:25,539 Validation Data Eval:
2022-01-11 23:42:28,291   Average segmentation loss on validation set: 0.0670
2022-01-11 23:42:34,165 Found new lowest validation loss at iteration 3144! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 23:42:35,580 iteration 3145 : loss : 0.019077, loss_ce: 0.007627
 46%|████████████▍              | 185/400 [1:29:39<1:55:45, 32.31s/it]2022-01-11 23:42:37,214 iteration 3146 : loss : 0.047446, loss_ce: 0.014760
2022-01-11 23:42:38,718 iteration 3147 : loss : 0.032928, loss_ce: 0.015017
2022-01-11 23:42:40,125 iteration 3148 : loss : 0.018389, loss_ce: 0.006499
2022-01-11 23:42:41,628 iteration 3149 : loss : 0.026442, loss_ce: 0.007203
2022-01-11 23:42:43,134 iteration 3150 : loss : 0.037086, loss_ce: 0.013732
2022-01-11 23:42:44,638 iteration 3151 : loss : 0.031491, loss_ce: 0.014358
2022-01-11 23:42:46,139 iteration 3152 : loss : 0.030710, loss_ce: 0.011448
2022-01-11 23:42:47,635 iteration 3153 : loss : 0.029645, loss_ce: 0.015724
2022-01-11 23:42:49,198 iteration 3154 : loss : 0.037473, loss_ce: 0.016482
2022-01-11 23:42:50,748 iteration 3155 : loss : 0.023745, loss_ce: 0.008186
2022-01-11 23:42:52,256 iteration 3156 : loss : 0.028992, loss_ce: 0.009489
2022-01-11 23:42:53,856 iteration 3157 : loss : 0.023942, loss_ce: 0.012225
2022-01-11 23:42:55,391 iteration 3158 : loss : 0.026473, loss_ce: 0.007213
2022-01-11 23:42:56,999 iteration 3159 : loss : 0.028523, loss_ce: 0.014128
2022-01-11 23:42:58,608 iteration 3160 : loss : 0.026850, loss_ce: 0.012038
2022-01-11 23:43:00,265 iteration 3161 : loss : 0.024160, loss_ce: 0.008750
2022-01-11 23:43:01,851 iteration 3162 : loss : 0.029946, loss_ce: 0.009883
 46%|████████████▌              | 186/400 [1:30:05<1:48:46, 30.50s/it]2022-01-11 23:43:03,418 iteration 3163 : loss : 0.023557, loss_ce: 0.008373
2022-01-11 23:43:05,092 iteration 3164 : loss : 0.027003, loss_ce: 0.012107
2022-01-11 23:43:06,722 iteration 3165 : loss : 0.039889, loss_ce: 0.016303
2022-01-11 23:43:08,265 iteration 3166 : loss : 0.034164, loss_ce: 0.014151
2022-01-11 23:43:09,894 iteration 3167 : loss : 0.027615, loss_ce: 0.009575
2022-01-11 23:43:11,465 iteration 3168 : loss : 0.023722, loss_ce: 0.009483
2022-01-11 23:43:13,074 iteration 3169 : loss : 0.040265, loss_ce: 0.016301
2022-01-11 23:43:14,663 iteration 3170 : loss : 0.024010, loss_ce: 0.010439
2022-01-11 23:43:16,131 iteration 3171 : loss : 0.019997, loss_ce: 0.005904
2022-01-11 23:43:17,660 iteration 3172 : loss : 0.019257, loss_ce: 0.007645
2022-01-11 23:43:19,232 iteration 3173 : loss : 0.054229, loss_ce: 0.014991
2022-01-11 23:43:20,779 iteration 3174 : loss : 0.032016, loss_ce: 0.014158
2022-01-11 23:43:22,290 iteration 3175 : loss : 0.029388, loss_ce: 0.009506
2022-01-11 23:43:23,944 iteration 3176 : loss : 0.034205, loss_ce: 0.010206
2022-01-11 23:43:25,534 iteration 3177 : loss : 0.024837, loss_ce: 0.008833
2022-01-11 23:43:27,190 iteration 3178 : loss : 0.024404, loss_ce: 0.007802
2022-01-11 23:43:28,761 iteration 3179 : loss : 0.029141, loss_ce: 0.015314
 47%|████████████▌              | 187/400 [1:30:32<1:44:26, 29.42s/it]2022-01-11 23:43:30,387 iteration 3180 : loss : 0.031965, loss_ce: 0.014823
2022-01-11 23:43:31,935 iteration 3181 : loss : 0.025305, loss_ce: 0.011231
2022-01-11 23:43:33,456 iteration 3182 : loss : 0.025365, loss_ce: 0.012019
2022-01-11 23:43:35,082 iteration 3183 : loss : 0.037963, loss_ce: 0.015185
2022-01-11 23:43:36,659 iteration 3184 : loss : 0.032832, loss_ce: 0.013037
2022-01-11 23:43:38,246 iteration 3185 : loss : 0.027757, loss_ce: 0.009086
2022-01-11 23:43:39,855 iteration 3186 : loss : 0.023764, loss_ce: 0.008771
2022-01-11 23:43:41,462 iteration 3187 : loss : 0.024715, loss_ce: 0.010785
2022-01-11 23:43:43,066 iteration 3188 : loss : 0.024491, loss_ce: 0.009491
2022-01-11 23:43:44,618 iteration 3189 : loss : 0.028425, loss_ce: 0.009614
2022-01-11 23:43:46,283 iteration 3190 : loss : 0.032697, loss_ce: 0.012226
2022-01-11 23:43:47,903 iteration 3191 : loss : 0.027178, loss_ce: 0.009065
2022-01-11 23:43:49,413 iteration 3192 : loss : 0.025917, loss_ce: 0.005725
2022-01-11 23:43:50,977 iteration 3193 : loss : 0.022951, loss_ce: 0.009704
2022-01-11 23:43:52,662 iteration 3194 : loss : 0.052255, loss_ce: 0.025058
2022-01-11 23:43:54,236 iteration 3195 : loss : 0.026490, loss_ce: 0.011516
2022-01-11 23:43:55,815 iteration 3196 : loss : 0.025304, loss_ce: 0.011761
 47%|████████████▋              | 188/400 [1:30:59<1:41:26, 28.71s/it]2022-01-11 23:43:57,406 iteration 3197 : loss : 0.025431, loss_ce: 0.010615
2022-01-11 23:43:59,013 iteration 3198 : loss : 0.024807, loss_ce: 0.011065
2022-01-11 23:44:00,615 iteration 3199 : loss : 0.025990, loss_ce: 0.006299
2022-01-11 23:44:02,232 iteration 3200 : loss : 0.027858, loss_ce: 0.010055
2022-01-11 23:44:03,846 iteration 3201 : loss : 0.031862, loss_ce: 0.011973
2022-01-11 23:44:05,459 iteration 3202 : loss : 0.044557, loss_ce: 0.013609
2022-01-11 23:44:06,956 iteration 3203 : loss : 0.022206, loss_ce: 0.009940
2022-01-11 23:44:08,451 iteration 3204 : loss : 0.020435, loss_ce: 0.008519
2022-01-11 23:44:09,943 iteration 3205 : loss : 0.025048, loss_ce: 0.007933
2022-01-11 23:44:11,400 iteration 3206 : loss : 0.023925, loss_ce: 0.007700
2022-01-11 23:44:12,991 iteration 3207 : loss : 0.034322, loss_ce: 0.014153
2022-01-11 23:44:14,552 iteration 3208 : loss : 0.026911, loss_ce: 0.010113
2022-01-11 23:44:16,097 iteration 3209 : loss : 0.039452, loss_ce: 0.014430
2022-01-11 23:44:17,692 iteration 3210 : loss : 0.021752, loss_ce: 0.009727
2022-01-11 23:44:19,207 iteration 3211 : loss : 0.021684, loss_ce: 0.007282
2022-01-11 23:44:20,834 iteration 3212 : loss : 0.049430, loss_ce: 0.025520
2022-01-11 23:44:22,475 iteration 3213 : loss : 0.027378, loss_ce: 0.009543
 47%|████████████▊              | 189/400 [1:31:26<1:38:47, 28.09s/it]2022-01-11 23:44:24,029 iteration 3214 : loss : 0.018935, loss_ce: 0.008666
2022-01-11 23:44:25,596 iteration 3215 : loss : 0.020262, loss_ce: 0.008904
2022-01-11 23:44:27,164 iteration 3216 : loss : 0.035485, loss_ce: 0.013852
2022-01-11 23:44:28,759 iteration 3217 : loss : 0.028571, loss_ce: 0.011934
2022-01-11 23:44:30,364 iteration 3218 : loss : 0.038304, loss_ce: 0.012448
2022-01-11 23:44:31,923 iteration 3219 : loss : 0.036160, loss_ce: 0.016813
2022-01-11 23:44:33,445 iteration 3220 : loss : 0.023743, loss_ce: 0.011920
2022-01-11 23:44:35,094 iteration 3221 : loss : 0.056038, loss_ce: 0.025927
2022-01-11 23:44:36,631 iteration 3222 : loss : 0.024141, loss_ce: 0.006900
2022-01-11 23:44:38,227 iteration 3223 : loss : 0.028186, loss_ce: 0.010106
2022-01-11 23:44:39,864 iteration 3224 : loss : 0.039522, loss_ce: 0.013200
2022-01-11 23:44:41,416 iteration 3225 : loss : 0.035026, loss_ce: 0.016384
2022-01-11 23:44:42,980 iteration 3226 : loss : 0.025228, loss_ce: 0.010743
2022-01-11 23:44:44,566 iteration 3227 : loss : 0.026470, loss_ce: 0.010041
2022-01-11 23:44:46,106 iteration 3228 : loss : 0.031691, loss_ce: 0.009913
2022-01-11 23:44:47,622 iteration 3229 : loss : 0.029335, loss_ce: 0.008622
2022-01-11 23:44:47,623 Training Data Eval:
2022-01-11 23:44:55,639   Average segmentation loss on training set: 0.0213
2022-01-11 23:44:55,639 Validation Data Eval:
2022-01-11 23:44:58,395   Average segmentation loss on validation set: 0.0712
2022-01-11 23:44:59,906 iteration 3230 : loss : 0.041983, loss_ce: 0.014781
 48%|████████████▊              | 190/400 [1:32:04<1:48:08, 30.90s/it]2022-01-11 23:45:01,475 iteration 3231 : loss : 0.046818, loss_ce: 0.010781
2022-01-11 23:45:03,028 iteration 3232 : loss : 0.022712, loss_ce: 0.007247
2022-01-11 23:45:04,618 iteration 3233 : loss : 0.033059, loss_ce: 0.016820
2022-01-11 23:45:06,216 iteration 3234 : loss : 0.021432, loss_ce: 0.006912
2022-01-11 23:45:07,804 iteration 3235 : loss : 0.048032, loss_ce: 0.015537
2022-01-11 23:45:09,320 iteration 3236 : loss : 0.019939, loss_ce: 0.009868
2022-01-11 23:45:10,978 iteration 3237 : loss : 0.029818, loss_ce: 0.013154
2022-01-11 23:45:12,559 iteration 3238 : loss : 0.019993, loss_ce: 0.009079
2022-01-11 23:45:14,133 iteration 3239 : loss : 0.020271, loss_ce: 0.007312
2022-01-11 23:45:15,657 iteration 3240 : loss : 0.027485, loss_ce: 0.006546
2022-01-11 23:45:17,158 iteration 3241 : loss : 0.042640, loss_ce: 0.008497
2022-01-11 23:45:18,728 iteration 3242 : loss : 0.030973, loss_ce: 0.012799
2022-01-11 23:45:20,240 iteration 3243 : loss : 0.028862, loss_ce: 0.010373
2022-01-11 23:45:21,837 iteration 3244 : loss : 0.025188, loss_ce: 0.010804
2022-01-11 23:45:23,365 iteration 3245 : loss : 0.018345, loss_ce: 0.007947
2022-01-11 23:45:24,995 iteration 3246 : loss : 0.039059, loss_ce: 0.022050
2022-01-11 23:45:26,529 iteration 3247 : loss : 0.027916, loss_ce: 0.011612
 48%|████████████▉              | 191/400 [1:32:30<1:43:09, 29.61s/it]2022-01-11 23:45:28,087 iteration 3248 : loss : 0.020344, loss_ce: 0.007139
2022-01-11 23:45:29,610 iteration 3249 : loss : 0.021504, loss_ce: 0.008050
2022-01-11 23:45:31,154 iteration 3250 : loss : 0.032571, loss_ce: 0.010574
2022-01-11 23:45:32,708 iteration 3251 : loss : 0.029949, loss_ce: 0.013746
2022-01-11 23:45:34,274 iteration 3252 : loss : 0.034429, loss_ce: 0.011957
2022-01-11 23:45:35,826 iteration 3253 : loss : 0.021095, loss_ce: 0.009061
2022-01-11 23:45:37,367 iteration 3254 : loss : 0.031618, loss_ce: 0.011763
2022-01-11 23:45:38,848 iteration 3255 : loss : 0.028405, loss_ce: 0.011425
2022-01-11 23:45:40,389 iteration 3256 : loss : 0.021142, loss_ce: 0.007343
2022-01-11 23:45:41,910 iteration 3257 : loss : 0.019518, loss_ce: 0.008036
2022-01-11 23:45:43,470 iteration 3258 : loss : 0.025997, loss_ce: 0.011643
2022-01-11 23:45:44,972 iteration 3259 : loss : 0.025402, loss_ce: 0.009716
2022-01-11 23:45:46,539 iteration 3260 : loss : 0.033485, loss_ce: 0.016182
2022-01-11 23:45:48,073 iteration 3261 : loss : 0.032105, loss_ce: 0.008540
2022-01-11 23:45:49,669 iteration 3262 : loss : 0.033102, loss_ce: 0.012180
2022-01-11 23:45:51,326 iteration 3263 : loss : 0.050861, loss_ce: 0.011826
2022-01-11 23:45:52,885 iteration 3264 : loss : 0.037700, loss_ce: 0.015562
 48%|████████████▉              | 192/400 [1:32:57<1:39:16, 28.64s/it]2022-01-11 23:45:54,430 iteration 3265 : loss : 0.019772, loss_ce: 0.008196
2022-01-11 23:45:56,040 iteration 3266 : loss : 0.033597, loss_ce: 0.012898
2022-01-11 23:45:57,625 iteration 3267 : loss : 0.030484, loss_ce: 0.011579
2022-01-11 23:45:59,150 iteration 3268 : loss : 0.021430, loss_ce: 0.007597
2022-01-11 23:46:00,771 iteration 3269 : loss : 0.047020, loss_ce: 0.023742
2022-01-11 23:46:02,400 iteration 3270 : loss : 0.033014, loss_ce: 0.010976
2022-01-11 23:46:04,006 iteration 3271 : loss : 0.032213, loss_ce: 0.010566
2022-01-11 23:46:05,625 iteration 3272 : loss : 0.022444, loss_ce: 0.009218
2022-01-11 23:46:07,137 iteration 3273 : loss : 0.020170, loss_ce: 0.006261
2022-01-11 23:46:08,684 iteration 3274 : loss : 0.022553, loss_ce: 0.011674
2022-01-11 23:46:10,304 iteration 3275 : loss : 0.038085, loss_ce: 0.010940
2022-01-11 23:46:11,834 iteration 3276 : loss : 0.022454, loss_ce: 0.009097
2022-01-11 23:46:13,412 iteration 3277 : loss : 0.037332, loss_ce: 0.018987
2022-01-11 23:46:14,983 iteration 3278 : loss : 0.028309, loss_ce: 0.010796
2022-01-11 23:46:16,513 iteration 3279 : loss : 0.029605, loss_ce: 0.011722
2022-01-11 23:46:18,055 iteration 3280 : loss : 0.024329, loss_ce: 0.008163
2022-01-11 23:46:19,559 iteration 3281 : loss : 0.025108, loss_ce: 0.007589
 48%|█████████████              | 193/400 [1:33:23<1:36:45, 28.05s/it]2022-01-11 23:46:21,188 iteration 3282 : loss : 0.026368, loss_ce: 0.011373
2022-01-11 23:46:22,733 iteration 3283 : loss : 0.021502, loss_ce: 0.008049
2022-01-11 23:46:24,288 iteration 3284 : loss : 0.026590, loss_ce: 0.010986
2022-01-11 23:46:25,832 iteration 3285 : loss : 0.039640, loss_ce: 0.020789
2022-01-11 23:46:27,387 iteration 3286 : loss : 0.018215, loss_ce: 0.006555
2022-01-11 23:46:28,961 iteration 3287 : loss : 0.031479, loss_ce: 0.014267
2022-01-11 23:46:30,480 iteration 3288 : loss : 0.028299, loss_ce: 0.015264
2022-01-11 23:46:32,012 iteration 3289 : loss : 0.036521, loss_ce: 0.009703
2022-01-11 23:46:33,611 iteration 3290 : loss : 0.044422, loss_ce: 0.014789
2022-01-11 23:46:35,221 iteration 3291 : loss : 0.035132, loss_ce: 0.013273
2022-01-11 23:46:36,803 iteration 3292 : loss : 0.022775, loss_ce: 0.007119
2022-01-11 23:46:38,369 iteration 3293 : loss : 0.031112, loss_ce: 0.012236
2022-01-11 23:46:39,905 iteration 3294 : loss : 0.021722, loss_ce: 0.009690
2022-01-11 23:46:41,552 iteration 3295 : loss : 0.039032, loss_ce: 0.018045
2022-01-11 23:46:43,140 iteration 3296 : loss : 0.031729, loss_ce: 0.010046
2022-01-11 23:46:44,637 iteration 3297 : loss : 0.019734, loss_ce: 0.007682
2022-01-11 23:46:46,248 iteration 3298 : loss : 0.035585, loss_ce: 0.015607
 48%|█████████████              | 194/400 [1:33:50<1:34:53, 27.64s/it]2022-01-11 23:46:47,783 iteration 3299 : loss : 0.020782, loss_ce: 0.009496
2022-01-11 23:46:49,312 iteration 3300 : loss : 0.032973, loss_ce: 0.011388
2022-01-11 23:46:50,916 iteration 3301 : loss : 0.021860, loss_ce: 0.009533
2022-01-11 23:46:52,463 iteration 3302 : loss : 0.022191, loss_ce: 0.008683
2022-01-11 23:46:54,000 iteration 3303 : loss : 0.022394, loss_ce: 0.008369
2022-01-11 23:46:55,564 iteration 3304 : loss : 0.024558, loss_ce: 0.008810
2022-01-11 23:46:57,137 iteration 3305 : loss : 0.021620, loss_ce: 0.007384
2022-01-11 23:46:58,756 iteration 3306 : loss : 0.044883, loss_ce: 0.018718
2022-01-11 23:47:00,292 iteration 3307 : loss : 0.026200, loss_ce: 0.010084
2022-01-11 23:47:01,854 iteration 3308 : loss : 0.048759, loss_ce: 0.023919
2022-01-11 23:47:03,413 iteration 3309 : loss : 0.042610, loss_ce: 0.007801
2022-01-11 23:47:05,047 iteration 3310 : loss : 0.050170, loss_ce: 0.021110
2022-01-11 23:47:06,693 iteration 3311 : loss : 0.018273, loss_ce: 0.006152
2022-01-11 23:47:08,194 iteration 3312 : loss : 0.027163, loss_ce: 0.009906
2022-01-11 23:47:09,721 iteration 3313 : loss : 0.020093, loss_ce: 0.007359
2022-01-11 23:47:11,359 iteration 3314 : loss : 0.029484, loss_ce: 0.009928
2022-01-11 23:47:11,359 Training Data Eval:
2022-01-11 23:47:19,368   Average segmentation loss on training set: 0.0233
2022-01-11 23:47:19,368 Validation Data Eval:
2022-01-11 23:47:22,139   Average segmentation loss on validation set: 0.0706
2022-01-11 23:47:23,663 iteration 3315 : loss : 0.020447, loss_ce: 0.009808
 49%|█████████████▏             | 195/400 [1:34:27<1:44:27, 30.57s/it]2022-01-11 23:47:25,217 iteration 3316 : loss : 0.019775, loss_ce: 0.007567
2022-01-11 23:47:26,753 iteration 3317 : loss : 0.023941, loss_ce: 0.009705
2022-01-11 23:47:28,412 iteration 3318 : loss : 0.023672, loss_ce: 0.010162
2022-01-11 23:47:29,947 iteration 3319 : loss : 0.022595, loss_ce: 0.008295
2022-01-11 23:47:31,497 iteration 3320 : loss : 0.024479, loss_ce: 0.009220
2022-01-11 23:47:33,065 iteration 3321 : loss : 0.028857, loss_ce: 0.011095
2022-01-11 23:47:34,669 iteration 3322 : loss : 0.037262, loss_ce: 0.020939
2022-01-11 23:47:36,191 iteration 3323 : loss : 0.026177, loss_ce: 0.008597
2022-01-11 23:47:37,827 iteration 3324 : loss : 0.029794, loss_ce: 0.014518
2022-01-11 23:47:39,346 iteration 3325 : loss : 0.033951, loss_ce: 0.010430
2022-01-11 23:47:40,936 iteration 3326 : loss : 0.025820, loss_ce: 0.010089
2022-01-11 23:47:42,585 iteration 3327 : loss : 0.050253, loss_ce: 0.018390
2022-01-11 23:47:44,152 iteration 3328 : loss : 0.031967, loss_ce: 0.011230
2022-01-11 23:47:45,685 iteration 3329 : loss : 0.025187, loss_ce: 0.011032
2022-01-11 23:47:47,237 iteration 3330 : loss : 0.035182, loss_ce: 0.012860
2022-01-11 23:47:48,801 iteration 3331 : loss : 0.020922, loss_ce: 0.008070
2022-01-11 23:47:50,281 iteration 3332 : loss : 0.021990, loss_ce: 0.010749
 49%|█████████████▏             | 196/400 [1:34:54<1:39:55, 29.39s/it]2022-01-11 23:47:51,859 iteration 3333 : loss : 0.022223, loss_ce: 0.009350
2022-01-11 23:47:53,431 iteration 3334 : loss : 0.020999, loss_ce: 0.009318
2022-01-11 23:47:54,913 iteration 3335 : loss : 0.019558, loss_ce: 0.008931
2022-01-11 23:47:56,469 iteration 3336 : loss : 0.060104, loss_ce: 0.028086
2022-01-11 23:47:58,075 iteration 3337 : loss : 0.026034, loss_ce: 0.010909
2022-01-11 23:47:59,717 iteration 3338 : loss : 0.026323, loss_ce: 0.013668
2022-01-11 23:48:01,261 iteration 3339 : loss : 0.036600, loss_ce: 0.010329
2022-01-11 23:48:02,790 iteration 3340 : loss : 0.027581, loss_ce: 0.011083
2022-01-11 23:48:04,347 iteration 3341 : loss : 0.024229, loss_ce: 0.008742
2022-01-11 23:48:05,948 iteration 3342 : loss : 0.038131, loss_ce: 0.019013
2022-01-11 23:48:07,525 iteration 3343 : loss : 0.036353, loss_ce: 0.014093
2022-01-11 23:48:09,097 iteration 3344 : loss : 0.031705, loss_ce: 0.012416
2022-01-11 23:48:10,644 iteration 3345 : loss : 0.026523, loss_ce: 0.010923
2022-01-11 23:48:12,218 iteration 3346 : loss : 0.022338, loss_ce: 0.011108
2022-01-11 23:48:13,806 iteration 3347 : loss : 0.053232, loss_ce: 0.013643
2022-01-11 23:48:15,447 iteration 3348 : loss : 0.031770, loss_ce: 0.010000
2022-01-11 23:48:17,027 iteration 3349 : loss : 0.029470, loss_ce: 0.011080
 49%|█████████████▎             | 197/400 [1:35:21<1:36:44, 28.59s/it]2022-01-11 23:48:18,584 iteration 3350 : loss : 0.019750, loss_ce: 0.007506
2022-01-11 23:48:20,089 iteration 3351 : loss : 0.021762, loss_ce: 0.008521
2022-01-11 23:48:21,633 iteration 3352 : loss : 0.040484, loss_ce: 0.014176
2022-01-11 23:48:23,216 iteration 3353 : loss : 0.024134, loss_ce: 0.008795
2022-01-11 23:48:24,779 iteration 3354 : loss : 0.019467, loss_ce: 0.006541
2022-01-11 23:48:26,400 iteration 3355 : loss : 0.029705, loss_ce: 0.014013
2022-01-11 23:48:27,995 iteration 3356 : loss : 0.040030, loss_ce: 0.012775
2022-01-11 23:48:29,608 iteration 3357 : loss : 0.024943, loss_ce: 0.007851
2022-01-11 23:48:31,230 iteration 3358 : loss : 0.029290, loss_ce: 0.010050
2022-01-11 23:48:32,813 iteration 3359 : loss : 0.023058, loss_ce: 0.009242
2022-01-11 23:48:34,453 iteration 3360 : loss : 0.033979, loss_ce: 0.014866
2022-01-11 23:48:35,954 iteration 3361 : loss : 0.030153, loss_ce: 0.007913
2022-01-11 23:48:37,485 iteration 3362 : loss : 0.031617, loss_ce: 0.012719
2022-01-11 23:48:39,002 iteration 3363 : loss : 0.023052, loss_ce: 0.010248
2022-01-11 23:48:40,576 iteration 3364 : loss : 0.030720, loss_ce: 0.010618
2022-01-11 23:48:42,097 iteration 3365 : loss : 0.023681, loss_ce: 0.012557
2022-01-11 23:48:43,650 iteration 3366 : loss : 0.027553, loss_ce: 0.010476
 50%|█████████████▎             | 198/400 [1:35:47<1:34:16, 28.00s/it]2022-01-11 23:48:45,242 iteration 3367 : loss : 0.028659, loss_ce: 0.009154
2022-01-11 23:48:46,774 iteration 3368 : loss : 0.024382, loss_ce: 0.010754
2022-01-11 23:48:48,352 iteration 3369 : loss : 0.022575, loss_ce: 0.008051
2022-01-11 23:48:49,939 iteration 3370 : loss : 0.027234, loss_ce: 0.012380
2022-01-11 23:48:51,419 iteration 3371 : loss : 0.017195, loss_ce: 0.006693
2022-01-11 23:48:52,959 iteration 3372 : loss : 0.029160, loss_ce: 0.016128
2022-01-11 23:48:54,514 iteration 3373 : loss : 0.021276, loss_ce: 0.010226
2022-01-11 23:48:56,024 iteration 3374 : loss : 0.019923, loss_ce: 0.008432
2022-01-11 23:48:57,544 iteration 3375 : loss : 0.021486, loss_ce: 0.008789
2022-01-11 23:48:59,069 iteration 3376 : loss : 0.021911, loss_ce: 0.009156
2022-01-11 23:49:00,668 iteration 3377 : loss : 0.043775, loss_ce: 0.015488
2022-01-11 23:49:02,228 iteration 3378 : loss : 0.023972, loss_ce: 0.008734
2022-01-11 23:49:03,709 iteration 3379 : loss : 0.017117, loss_ce: 0.007703
2022-01-11 23:49:05,332 iteration 3380 : loss : 0.028434, loss_ce: 0.010936
2022-01-11 23:49:06,830 iteration 3381 : loss : 0.060199, loss_ce: 0.010760
2022-01-11 23:49:08,396 iteration 3382 : loss : 0.028721, loss_ce: 0.010701
2022-01-11 23:49:09,943 iteration 3383 : loss : 0.023660, loss_ce: 0.008677
 50%|█████████████▍             | 199/400 [1:36:14<1:32:05, 27.49s/it]2022-01-11 23:49:11,492 iteration 3384 : loss : 0.023081, loss_ce: 0.008114
2022-01-11 23:49:13,080 iteration 3385 : loss : 0.027819, loss_ce: 0.013145
2022-01-11 23:49:14,678 iteration 3386 : loss : 0.032057, loss_ce: 0.015427
2022-01-11 23:49:16,257 iteration 3387 : loss : 0.044292, loss_ce: 0.017607
2022-01-11 23:49:17,755 iteration 3388 : loss : 0.020860, loss_ce: 0.008296
2022-01-11 23:49:19,342 iteration 3389 : loss : 0.030576, loss_ce: 0.012767
2022-01-11 23:49:20,833 iteration 3390 : loss : 0.030167, loss_ce: 0.008421
2022-01-11 23:49:22,485 iteration 3391 : loss : 0.040105, loss_ce: 0.013645
2022-01-11 23:49:24,087 iteration 3392 : loss : 0.033917, loss_ce: 0.013581
2022-01-11 23:49:25,673 iteration 3393 : loss : 0.026234, loss_ce: 0.009365
2022-01-11 23:49:27,143 iteration 3394 : loss : 0.026831, loss_ce: 0.010179
2022-01-11 23:49:28,804 iteration 3395 : loss : 0.036466, loss_ce: 0.012598
2022-01-11 23:49:30,378 iteration 3396 : loss : 0.032903, loss_ce: 0.012889
2022-01-11 23:49:31,917 iteration 3397 : loss : 0.024839, loss_ce: 0.010510
2022-01-11 23:49:33,511 iteration 3398 : loss : 0.024684, loss_ce: 0.008221
2022-01-11 23:49:35,083 iteration 3399 : loss : 0.017731, loss_ce: 0.007939
2022-01-11 23:49:35,083 Training Data Eval:
2022-01-11 23:49:43,078   Average segmentation loss on training set: 0.0189
2022-01-11 23:49:43,079 Validation Data Eval:
2022-01-11 23:49:45,828   Average segmentation loss on validation set: 0.1105
2022-01-11 23:49:47,390 iteration 3400 : loss : 0.039529, loss_ce: 0.011212
 50%|█████████████▌             | 200/400 [1:36:51<1:41:35, 30.48s/it]2022-01-11 23:49:49,112 iteration 3401 : loss : 0.029010, loss_ce: 0.012656
2022-01-11 23:49:50,725 iteration 3402 : loss : 0.024986, loss_ce: 0.009207
2022-01-11 23:49:52,263 iteration 3403 : loss : 0.024713, loss_ce: 0.011895
2022-01-11 23:49:53,812 iteration 3404 : loss : 0.022106, loss_ce: 0.008399
2022-01-11 23:49:55,356 iteration 3405 : loss : 0.033108, loss_ce: 0.013272
2022-01-11 23:49:56,968 iteration 3406 : loss : 0.033856, loss_ce: 0.011740
2022-01-11 23:49:58,539 iteration 3407 : loss : 0.058685, loss_ce: 0.025007
2022-01-11 23:50:00,119 iteration 3408 : loss : 0.028159, loss_ce: 0.011386
2022-01-11 23:50:01,711 iteration 3409 : loss : 0.025761, loss_ce: 0.011586
2022-01-11 23:50:03,290 iteration 3410 : loss : 0.026728, loss_ce: 0.008932
2022-01-11 23:50:04,897 iteration 3411 : loss : 0.029913, loss_ce: 0.010558
2022-01-11 23:50:06,405 iteration 3412 : loss : 0.023628, loss_ce: 0.008108
2022-01-11 23:50:07,902 iteration 3413 : loss : 0.017973, loss_ce: 0.007633
2022-01-11 23:50:09,485 iteration 3414 : loss : 0.023941, loss_ce: 0.006561
2022-01-11 23:50:11,081 iteration 3415 : loss : 0.024371, loss_ce: 0.009304
2022-01-11 23:50:12,678 iteration 3416 : loss : 0.024852, loss_ce: 0.009878
2022-01-11 23:50:14,307 iteration 3417 : loss : 0.039645, loss_ce: 0.016702
 50%|█████████████▌             | 201/400 [1:37:18<1:37:32, 29.41s/it]2022-01-11 23:50:15,910 iteration 3418 : loss : 0.026146, loss_ce: 0.011530
2022-01-11 23:50:17,500 iteration 3419 : loss : 0.022969, loss_ce: 0.007993
2022-01-11 23:50:19,141 iteration 3420 : loss : 0.033443, loss_ce: 0.010622
2022-01-11 23:50:20,712 iteration 3421 : loss : 0.031069, loss_ce: 0.011105
2022-01-11 23:50:22,218 iteration 3422 : loss : 0.018831, loss_ce: 0.006303
2022-01-11 23:50:23,776 iteration 3423 : loss : 0.018528, loss_ce: 0.007663
2022-01-11 23:50:25,347 iteration 3424 : loss : 0.028661, loss_ce: 0.014845
2022-01-11 23:50:26,861 iteration 3425 : loss : 0.022273, loss_ce: 0.011347
2022-01-11 23:50:28,433 iteration 3426 : loss : 0.023991, loss_ce: 0.008163
2022-01-11 23:50:30,003 iteration 3427 : loss : 0.033804, loss_ce: 0.010888
2022-01-11 23:50:31,576 iteration 3428 : loss : 0.020198, loss_ce: 0.007706
2022-01-11 23:50:33,180 iteration 3429 : loss : 0.023053, loss_ce: 0.008013
2022-01-11 23:50:34,782 iteration 3430 : loss : 0.027259, loss_ce: 0.012176
2022-01-11 23:50:36,266 iteration 3431 : loss : 0.025627, loss_ce: 0.010850
2022-01-11 23:50:37,796 iteration 3432 : loss : 0.027630, loss_ce: 0.008644
2022-01-11 23:50:39,325 iteration 3433 : loss : 0.023423, loss_ce: 0.008859
2022-01-11 23:50:40,932 iteration 3434 : loss : 0.056423, loss_ce: 0.018959
 50%|█████████████▋             | 202/400 [1:37:45<1:34:17, 28.57s/it]2022-01-11 23:50:42,446 iteration 3435 : loss : 0.023009, loss_ce: 0.007133
2022-01-11 23:50:43,989 iteration 3436 : loss : 0.029007, loss_ce: 0.012122
2022-01-11 23:50:45,635 iteration 3437 : loss : 0.026442, loss_ce: 0.013010
2022-01-11 23:50:47,245 iteration 3438 : loss : 0.044600, loss_ce: 0.021136
2022-01-11 23:50:48,840 iteration 3439 : loss : 0.024507, loss_ce: 0.008963
2022-01-11 23:50:50,362 iteration 3440 : loss : 0.018249, loss_ce: 0.007229
2022-01-11 23:50:51,916 iteration 3441 : loss : 0.027955, loss_ce: 0.010704
2022-01-11 23:50:53,569 iteration 3442 : loss : 0.026848, loss_ce: 0.009063
2022-01-11 23:50:55,165 iteration 3443 : loss : 0.021742, loss_ce: 0.007281
2022-01-11 23:50:56,764 iteration 3444 : loss : 0.031929, loss_ce: 0.013084
2022-01-11 23:50:58,327 iteration 3445 : loss : 0.023971, loss_ce: 0.006292
2022-01-11 23:50:59,872 iteration 3446 : loss : 0.023642, loss_ce: 0.008711
2022-01-11 23:51:01,423 iteration 3447 : loss : 0.022627, loss_ce: 0.007789
2022-01-11 23:51:03,032 iteration 3448 : loss : 0.025144, loss_ce: 0.006659
2022-01-11 23:51:04,606 iteration 3449 : loss : 0.025845, loss_ce: 0.011139
2022-01-11 23:51:06,150 iteration 3450 : loss : 0.041036, loss_ce: 0.021332
2022-01-11 23:51:07,775 iteration 3451 : loss : 0.040452, loss_ce: 0.016130
 51%|█████████████▋             | 203/400 [1:38:11<1:32:06, 28.05s/it]2022-01-11 23:51:09,364 iteration 3452 : loss : 0.026605, loss_ce: 0.010535
2022-01-11 23:51:10,908 iteration 3453 : loss : 0.020924, loss_ce: 0.007923
2022-01-11 23:51:12,443 iteration 3454 : loss : 0.024148, loss_ce: 0.008029
2022-01-11 23:51:14,065 iteration 3455 : loss : 0.038377, loss_ce: 0.017155
2022-01-11 23:51:15,581 iteration 3456 : loss : 0.024442, loss_ce: 0.008395
2022-01-11 23:51:17,240 iteration 3457 : loss : 0.030715, loss_ce: 0.011791
2022-01-11 23:51:18,824 iteration 3458 : loss : 0.024318, loss_ce: 0.009245
2022-01-11 23:51:20,365 iteration 3459 : loss : 0.022222, loss_ce: 0.008888
2022-01-11 23:51:21,918 iteration 3460 : loss : 0.033814, loss_ce: 0.014544
2022-01-11 23:51:23,483 iteration 3461 : loss : 0.038042, loss_ce: 0.016900
2022-01-11 23:51:25,027 iteration 3462 : loss : 0.023865, loss_ce: 0.010158
2022-01-11 23:51:26,626 iteration 3463 : loss : 0.026946, loss_ce: 0.009213
2022-01-11 23:51:28,122 iteration 3464 : loss : 0.017464, loss_ce: 0.006589
2022-01-11 23:51:29,726 iteration 3465 : loss : 0.025418, loss_ce: 0.008930
2022-01-11 23:51:31,302 iteration 3466 : loss : 0.023443, loss_ce: 0.008077
2022-01-11 23:51:32,859 iteration 3467 : loss : 0.023059, loss_ce: 0.010862
2022-01-11 23:51:34,406 iteration 3468 : loss : 0.023726, loss_ce: 0.011499
 51%|█████████████▊             | 204/400 [1:38:38<1:30:14, 27.63s/it]2022-01-11 23:51:35,972 iteration 3469 : loss : 0.021784, loss_ce: 0.005583
2022-01-11 23:51:37,580 iteration 3470 : loss : 0.033650, loss_ce: 0.012497
2022-01-11 23:51:39,061 iteration 3471 : loss : 0.018974, loss_ce: 0.007211
2022-01-11 23:51:40,643 iteration 3472 : loss : 0.024144, loss_ce: 0.010675
2022-01-11 23:51:42,300 iteration 3473 : loss : 0.038120, loss_ce: 0.018843
2022-01-11 23:51:43,851 iteration 3474 : loss : 0.021648, loss_ce: 0.007772
2022-01-11 23:51:45,483 iteration 3475 : loss : 0.032238, loss_ce: 0.009545
2022-01-11 23:51:47,002 iteration 3476 : loss : 0.021641, loss_ce: 0.006454
2022-01-11 23:51:48,587 iteration 3477 : loss : 0.025269, loss_ce: 0.007569
2022-01-11 23:51:50,157 iteration 3478 : loss : 0.021499, loss_ce: 0.008571
2022-01-11 23:51:51,680 iteration 3479 : loss : 0.024694, loss_ce: 0.009957
2022-01-11 23:51:53,152 iteration 3480 : loss : 0.022779, loss_ce: 0.008847
2022-01-11 23:51:54,691 iteration 3481 : loss : 0.027117, loss_ce: 0.009529
2022-01-11 23:51:56,257 iteration 3482 : loss : 0.026545, loss_ce: 0.010204
2022-01-11 23:51:57,876 iteration 3483 : loss : 0.033218, loss_ce: 0.013355
2022-01-11 23:51:59,359 iteration 3484 : loss : 0.020284, loss_ce: 0.008100
2022-01-11 23:51:59,360 Training Data Eval:
2022-01-11 23:52:07,338   Average segmentation loss on training set: 0.0161
2022-01-11 23:52:07,338 Validation Data Eval:
2022-01-11 23:52:10,096   Average segmentation loss on validation set: 0.0700
2022-01-11 23:52:11,750 iteration 3485 : loss : 0.037439, loss_ce: 0.016504
 51%|█████████████▊             | 205/400 [1:39:15<1:39:16, 30.54s/it]2022-01-11 23:52:13,343 iteration 3486 : loss : 0.035652, loss_ce: 0.013088
2022-01-11 23:52:14,849 iteration 3487 : loss : 0.021482, loss_ce: 0.009231
2022-01-11 23:52:16,399 iteration 3488 : loss : 0.036413, loss_ce: 0.016243
2022-01-11 23:52:17,955 iteration 3489 : loss : 0.040409, loss_ce: 0.014483
2022-01-11 23:52:19,543 iteration 3490 : loss : 0.035590, loss_ce: 0.014232
2022-01-11 23:52:21,156 iteration 3491 : loss : 0.031339, loss_ce: 0.012220
2022-01-11 23:52:22,656 iteration 3492 : loss : 0.032068, loss_ce: 0.012104
2022-01-11 23:52:24,310 iteration 3493 : loss : 0.028550, loss_ce: 0.011063
2022-01-11 23:52:25,896 iteration 3494 : loss : 0.024165, loss_ce: 0.006265
2022-01-11 23:52:27,460 iteration 3495 : loss : 0.035969, loss_ce: 0.010434
2022-01-11 23:52:29,062 iteration 3496 : loss : 0.023427, loss_ce: 0.010121
2022-01-11 23:52:30,637 iteration 3497 : loss : 0.036658, loss_ce: 0.013960
2022-01-11 23:52:32,294 iteration 3498 : loss : 0.025988, loss_ce: 0.010617
2022-01-11 23:52:33,816 iteration 3499 : loss : 0.020616, loss_ce: 0.007976
2022-01-11 23:52:35,304 iteration 3500 : loss : 0.017270, loss_ce: 0.005526
2022-01-11 23:52:36,830 iteration 3501 : loss : 0.019860, loss_ce: 0.006437
2022-01-11 23:52:38,403 iteration 3502 : loss : 0.029530, loss_ce: 0.009888
 52%|█████████████▉             | 206/400 [1:39:42<1:34:58, 29.37s/it]2022-01-11 23:52:40,022 iteration 3503 : loss : 0.025784, loss_ce: 0.009548
2022-01-11 23:52:41,559 iteration 3504 : loss : 0.018446, loss_ce: 0.005856
2022-01-11 23:52:43,029 iteration 3505 : loss : 0.018598, loss_ce: 0.008814
2022-01-11 23:52:44,602 iteration 3506 : loss : 0.025921, loss_ce: 0.007494
2022-01-11 23:52:46,139 iteration 3507 : loss : 0.022984, loss_ce: 0.006913
2022-01-11 23:52:47,807 iteration 3508 : loss : 0.050905, loss_ce: 0.022319
2022-01-11 23:52:49,375 iteration 3509 : loss : 0.020294, loss_ce: 0.007682
2022-01-11 23:52:50,883 iteration 3510 : loss : 0.024244, loss_ce: 0.010472
2022-01-11 23:52:52,382 iteration 3511 : loss : 0.018246, loss_ce: 0.008493
2022-01-11 23:52:54,013 iteration 3512 : loss : 0.024505, loss_ce: 0.007233
2022-01-11 23:52:55,573 iteration 3513 : loss : 0.021428, loss_ce: 0.006925
2022-01-11 23:52:57,246 iteration 3514 : loss : 0.043340, loss_ce: 0.023183
2022-01-11 23:52:58,789 iteration 3515 : loss : 0.023335, loss_ce: 0.006990
2022-01-11 23:53:00,326 iteration 3516 : loss : 0.017784, loss_ce: 0.007338
2022-01-11 23:53:01,865 iteration 3517 : loss : 0.030479, loss_ce: 0.014611
2022-01-11 23:53:03,399 iteration 3518 : loss : 0.023618, loss_ce: 0.013145
2022-01-11 23:53:04,976 iteration 3519 : loss : 0.027986, loss_ce: 0.009654
 52%|█████████████▉             | 207/400 [1:40:09<1:31:47, 28.53s/it]2022-01-11 23:53:06,592 iteration 3520 : loss : 0.030967, loss_ce: 0.013342
2022-01-11 23:53:08,117 iteration 3521 : loss : 0.044070, loss_ce: 0.022551
2022-01-11 23:53:09,612 iteration 3522 : loss : 0.017735, loss_ce: 0.008042
2022-01-11 23:53:11,151 iteration 3523 : loss : 0.017385, loss_ce: 0.006395
2022-01-11 23:53:12,723 iteration 3524 : loss : 0.024215, loss_ce: 0.010745
2022-01-11 23:53:14,363 iteration 3525 : loss : 0.024054, loss_ce: 0.009067
2022-01-11 23:53:15,936 iteration 3526 : loss : 0.019919, loss_ce: 0.007433
2022-01-11 23:53:17,523 iteration 3527 : loss : 0.034476, loss_ce: 0.010596
2022-01-11 23:53:19,191 iteration 3528 : loss : 0.039723, loss_ce: 0.016588
2022-01-11 23:53:20,774 iteration 3529 : loss : 0.030356, loss_ce: 0.008250
2022-01-11 23:53:22,358 iteration 3530 : loss : 0.020654, loss_ce: 0.007513
2022-01-11 23:53:23,863 iteration 3531 : loss : 0.029094, loss_ce: 0.009537
2022-01-11 23:53:25,502 iteration 3532 : loss : 0.030936, loss_ce: 0.013667
2022-01-11 23:53:27,042 iteration 3533 : loss : 0.023276, loss_ce: 0.006790
2022-01-11 23:53:28,629 iteration 3534 : loss : 0.029455, loss_ce: 0.012048
2022-01-11 23:53:30,248 iteration 3535 : loss : 0.033348, loss_ce: 0.011305
2022-01-11 23:53:31,826 iteration 3536 : loss : 0.024122, loss_ce: 0.011120
 52%|██████████████             | 208/400 [1:40:35<1:29:41, 28.03s/it]2022-01-11 23:53:33,345 iteration 3537 : loss : 0.018470, loss_ce: 0.006760
2022-01-11 23:53:34,840 iteration 3538 : loss : 0.017202, loss_ce: 0.007103
2022-01-11 23:53:36,336 iteration 3539 : loss : 0.026380, loss_ce: 0.013684
2022-01-11 23:53:37,791 iteration 3540 : loss : 0.018310, loss_ce: 0.005156
2022-01-11 23:53:39,426 iteration 3541 : loss : 0.037776, loss_ce: 0.014467
2022-01-11 23:53:40,982 iteration 3542 : loss : 0.026277, loss_ce: 0.012384
2022-01-11 23:53:42,630 iteration 3543 : loss : 0.031626, loss_ce: 0.010641
2022-01-11 23:53:44,122 iteration 3544 : loss : 0.016250, loss_ce: 0.007208
2022-01-11 23:53:45,680 iteration 3545 : loss : 0.021739, loss_ce: 0.009794
2022-01-11 23:53:47,166 iteration 3546 : loss : 0.019444, loss_ce: 0.007201
2022-01-11 23:53:48,720 iteration 3547 : loss : 0.026883, loss_ce: 0.013371
2022-01-11 23:53:50,221 iteration 3548 : loss : 0.029001, loss_ce: 0.010087
2022-01-11 23:53:51,793 iteration 3549 : loss : 0.033925, loss_ce: 0.012446
2022-01-11 23:53:53,390 iteration 3550 : loss : 0.023301, loss_ce: 0.010199
2022-01-11 23:53:54,994 iteration 3551 : loss : 0.039293, loss_ce: 0.012667
2022-01-11 23:53:56,599 iteration 3552 : loss : 0.030497, loss_ce: 0.009792
2022-01-11 23:53:58,214 iteration 3553 : loss : 0.025529, loss_ce: 0.010715
 52%|██████████████             | 209/400 [1:41:02<1:27:39, 27.54s/it]2022-01-11 23:53:59,810 iteration 3554 : loss : 0.026768, loss_ce: 0.010673
2022-01-11 23:54:01,330 iteration 3555 : loss : 0.020407, loss_ce: 0.006160
2022-01-11 23:54:02,869 iteration 3556 : loss : 0.022468, loss_ce: 0.009772
2022-01-11 23:54:04,502 iteration 3557 : loss : 0.022366, loss_ce: 0.006175
2022-01-11 23:54:06,003 iteration 3558 : loss : 0.047247, loss_ce: 0.013548
2022-01-11 23:54:07,548 iteration 3559 : loss : 0.028138, loss_ce: 0.008997
2022-01-11 23:54:09,010 iteration 3560 : loss : 0.021105, loss_ce: 0.010379
2022-01-11 23:54:10,637 iteration 3561 : loss : 0.026700, loss_ce: 0.010228
2022-01-11 23:54:12,153 iteration 3562 : loss : 0.023085, loss_ce: 0.008102
2022-01-11 23:54:13,731 iteration 3563 : loss : 0.023388, loss_ce: 0.007589
2022-01-11 23:54:15,287 iteration 3564 : loss : 0.028226, loss_ce: 0.009228
2022-01-11 23:54:16,940 iteration 3565 : loss : 0.056678, loss_ce: 0.014290
2022-01-11 23:54:18,497 iteration 3566 : loss : 0.027654, loss_ce: 0.011958
2022-01-11 23:54:20,128 iteration 3567 : loss : 0.027224, loss_ce: 0.013897
2022-01-11 23:54:21,738 iteration 3568 : loss : 0.027398, loss_ce: 0.010768
2022-01-11 23:54:23,331 iteration 3569 : loss : 0.044855, loss_ce: 0.009364
2022-01-11 23:54:23,332 Training Data Eval:
2022-01-11 23:54:31,300   Average segmentation loss on training set: 0.0187
2022-01-11 23:54:31,300 Validation Data Eval:
2022-01-11 23:54:34,058   Average segmentation loss on validation set: 0.0752
2022-01-11 23:54:35,649 iteration 3570 : loss : 0.030116, loss_ce: 0.010958
 52%|██████████████▏            | 210/400 [1:41:39<1:36:36, 30.51s/it]2022-01-11 23:54:37,246 iteration 3571 : loss : 0.024760, loss_ce: 0.010818
2022-01-11 23:54:38,817 iteration 3572 : loss : 0.025875, loss_ce: 0.007253
2022-01-11 23:54:40,378 iteration 3573 : loss : 0.040193, loss_ce: 0.014275
2022-01-11 23:54:41,906 iteration 3574 : loss : 0.017876, loss_ce: 0.007441
2022-01-11 23:54:43,448 iteration 3575 : loss : 0.023195, loss_ce: 0.008565
2022-01-11 23:54:45,022 iteration 3576 : loss : 0.031103, loss_ce: 0.010947
2022-01-11 23:54:46,629 iteration 3577 : loss : 0.022444, loss_ce: 0.010916
2022-01-11 23:54:48,222 iteration 3578 : loss : 0.032619, loss_ce: 0.012696
2022-01-11 23:54:49,750 iteration 3579 : loss : 0.029789, loss_ce: 0.010711
2022-01-11 23:54:51,326 iteration 3580 : loss : 0.025544, loss_ce: 0.009123
2022-01-11 23:54:52,851 iteration 3581 : loss : 0.029860, loss_ce: 0.005878
2022-01-11 23:54:54,398 iteration 3582 : loss : 0.028406, loss_ce: 0.012009
2022-01-11 23:54:55,979 iteration 3583 : loss : 0.037211, loss_ce: 0.013631
2022-01-11 23:54:57,502 iteration 3584 : loss : 0.020235, loss_ce: 0.008389
2022-01-11 23:54:59,055 iteration 3585 : loss : 0.026396, loss_ce: 0.009693
2022-01-11 23:55:00,562 iteration 3586 : loss : 0.024465, loss_ce: 0.008842
2022-01-11 23:55:02,109 iteration 3587 : loss : 0.030366, loss_ce: 0.011072
 53%|██████████████▏            | 211/400 [1:42:06<1:32:16, 29.29s/it]2022-01-11 23:55:03,628 iteration 3588 : loss : 0.018951, loss_ce: 0.006705
2022-01-11 23:55:05,208 iteration 3589 : loss : 0.037674, loss_ce: 0.014321
2022-01-11 23:55:06,749 iteration 3590 : loss : 0.024276, loss_ce: 0.010904
2022-01-11 23:55:08,279 iteration 3591 : loss : 0.026298, loss_ce: 0.008554
2022-01-11 23:55:09,877 iteration 3592 : loss : 0.029001, loss_ce: 0.012148
2022-01-11 23:55:11,433 iteration 3593 : loss : 0.035051, loss_ce: 0.008928
2022-01-11 23:55:12,995 iteration 3594 : loss : 0.022380, loss_ce: 0.006658
2022-01-11 23:55:14,666 iteration 3595 : loss : 0.062255, loss_ce: 0.013902
2022-01-11 23:55:16,148 iteration 3596 : loss : 0.023924, loss_ce: 0.009531
2022-01-11 23:55:17,784 iteration 3597 : loss : 0.043816, loss_ce: 0.015253
2022-01-11 23:55:19,332 iteration 3598 : loss : 0.033290, loss_ce: 0.013719
2022-01-11 23:55:20,894 iteration 3599 : loss : 0.027374, loss_ce: 0.010125
2022-01-11 23:55:22,429 iteration 3600 : loss : 0.026045, loss_ce: 0.012810
2022-01-11 23:55:23,975 iteration 3601 : loss : 0.033756, loss_ce: 0.013840
2022-01-11 23:55:25,502 iteration 3602 : loss : 0.024494, loss_ce: 0.010536
2022-01-11 23:55:27,013 iteration 3603 : loss : 0.023261, loss_ce: 0.008711
2022-01-11 23:55:28,529 iteration 3604 : loss : 0.021988, loss_ce: 0.008812
 53%|██████████████▎            | 212/400 [1:42:32<1:29:05, 28.43s/it]2022-01-11 23:55:30,110 iteration 3605 : loss : 0.018396, loss_ce: 0.007072
2022-01-11 23:55:31,661 iteration 3606 : loss : 0.028641, loss_ce: 0.010567
2022-01-11 23:55:33,162 iteration 3607 : loss : 0.022752, loss_ce: 0.008507
2022-01-11 23:55:34,658 iteration 3608 : loss : 0.025421, loss_ce: 0.011061
2022-01-11 23:55:36,244 iteration 3609 : loss : 0.034758, loss_ce: 0.011552
2022-01-11 23:55:37,749 iteration 3610 : loss : 0.029560, loss_ce: 0.011537
2022-01-11 23:55:39,338 iteration 3611 : loss : 0.021622, loss_ce: 0.009277
2022-01-11 23:55:40,934 iteration 3612 : loss : 0.024741, loss_ce: 0.009680
2022-01-11 23:55:42,481 iteration 3613 : loss : 0.031903, loss_ce: 0.012057
2022-01-11 23:55:44,069 iteration 3614 : loss : 0.019256, loss_ce: 0.005593
2022-01-11 23:55:45,594 iteration 3615 : loss : 0.033299, loss_ce: 0.009960
2022-01-11 23:55:47,211 iteration 3616 : loss : 0.034504, loss_ce: 0.008416
2022-01-11 23:55:48,755 iteration 3617 : loss : 0.031726, loss_ce: 0.012345
2022-01-11 23:55:50,299 iteration 3618 : loss : 0.027739, loss_ce: 0.012811
2022-01-11 23:55:51,821 iteration 3619 : loss : 0.023678, loss_ce: 0.009440
2022-01-11 23:55:53,386 iteration 3620 : loss : 0.025865, loss_ce: 0.011112
2022-01-11 23:55:54,933 iteration 3621 : loss : 0.035928, loss_ce: 0.015144
 53%|██████████████▍            | 213/400 [1:42:59<1:26:42, 27.82s/it]2022-01-11 23:55:56,585 iteration 3622 : loss : 0.033375, loss_ce: 0.011878
2022-01-11 23:55:58,131 iteration 3623 : loss : 0.020829, loss_ce: 0.008684
2022-01-11 23:55:59,749 iteration 3624 : loss : 0.039807, loss_ce: 0.015094
2022-01-11 23:56:01,361 iteration 3625 : loss : 0.034649, loss_ce: 0.016360
2022-01-11 23:56:02,910 iteration 3626 : loss : 0.024965, loss_ce: 0.009709
2022-01-11 23:56:04,502 iteration 3627 : loss : 0.020557, loss_ce: 0.010160
2022-01-11 23:56:06,075 iteration 3628 : loss : 0.028161, loss_ce: 0.010162
2022-01-11 23:56:07,686 iteration 3629 : loss : 0.038844, loss_ce: 0.016744
2022-01-11 23:56:09,239 iteration 3630 : loss : 0.021643, loss_ce: 0.009131
2022-01-11 23:56:10,728 iteration 3631 : loss : 0.018475, loss_ce: 0.007623
2022-01-11 23:56:12,224 iteration 3632 : loss : 0.017817, loss_ce: 0.007816
2022-01-11 23:56:13,837 iteration 3633 : loss : 0.030167, loss_ce: 0.012182
2022-01-11 23:56:15,380 iteration 3634 : loss : 0.039130, loss_ce: 0.014292
2022-01-11 23:56:16,896 iteration 3635 : loss : 0.036341, loss_ce: 0.013425
2022-01-11 23:56:18,416 iteration 3636 : loss : 0.027584, loss_ce: 0.010578
2022-01-11 23:56:20,012 iteration 3637 : loss : 0.036460, loss_ce: 0.012976
2022-01-11 23:56:21,502 iteration 3638 : loss : 0.026962, loss_ce: 0.009923
 54%|██████████████▍            | 214/400 [1:43:25<1:25:05, 27.45s/it]2022-01-11 23:56:23,067 iteration 3639 : loss : 0.021800, loss_ce: 0.010948
2022-01-11 23:56:24,657 iteration 3640 : loss : 0.021251, loss_ce: 0.007635
2022-01-11 23:56:26,210 iteration 3641 : loss : 0.024735, loss_ce: 0.007795
2022-01-11 23:56:27,798 iteration 3642 : loss : 0.026724, loss_ce: 0.011486
2022-01-11 23:56:29,347 iteration 3643 : loss : 0.028447, loss_ce: 0.013103
2022-01-11 23:56:30,889 iteration 3644 : loss : 0.020259, loss_ce: 0.009379
2022-01-11 23:56:32,415 iteration 3645 : loss : 0.021830, loss_ce: 0.008526
2022-01-11 23:56:33,996 iteration 3646 : loss : 0.020269, loss_ce: 0.007491
2022-01-11 23:56:35,574 iteration 3647 : loss : 0.030040, loss_ce: 0.013819
2022-01-11 23:56:37,218 iteration 3648 : loss : 0.032085, loss_ce: 0.011619
2022-01-11 23:56:38,788 iteration 3649 : loss : 0.026644, loss_ce: 0.008322
2022-01-11 23:56:40,347 iteration 3650 : loss : 0.034421, loss_ce: 0.011355
2022-01-11 23:56:41,928 iteration 3651 : loss : 0.024784, loss_ce: 0.010365
2022-01-11 23:56:43,453 iteration 3652 : loss : 0.023062, loss_ce: 0.010604
2022-01-11 23:56:45,057 iteration 3653 : loss : 0.040468, loss_ce: 0.007758
2022-01-11 23:56:46,614 iteration 3654 : loss : 0.022237, loss_ce: 0.006598
2022-01-11 23:56:46,614 Training Data Eval:
2022-01-11 23:56:54,594   Average segmentation loss on training set: 0.0161
2022-01-11 23:56:54,594 Validation Data Eval:
2022-01-11 23:56:57,345   Average segmentation loss on validation set: 0.0779
2022-01-11 23:56:58,827 iteration 3655 : loss : 0.019357, loss_ce: 0.006582
 54%|██████████████▌            | 215/400 [1:44:02<1:33:45, 30.41s/it]2022-01-11 23:57:00,480 iteration 3656 : loss : 0.033268, loss_ce: 0.007107
2022-01-11 23:57:02,023 iteration 3657 : loss : 0.024975, loss_ce: 0.013723
2022-01-11 23:57:03,585 iteration 3658 : loss : 0.019690, loss_ce: 0.008499
2022-01-11 23:57:05,179 iteration 3659 : loss : 0.027021, loss_ce: 0.006670
2022-01-11 23:57:06,826 iteration 3660 : loss : 0.043494, loss_ce: 0.013304
2022-01-11 23:57:08,475 iteration 3661 : loss : 0.030905, loss_ce: 0.014652
2022-01-11 23:57:09,948 iteration 3662 : loss : 0.020771, loss_ce: 0.007893
2022-01-11 23:57:11,484 iteration 3663 : loss : 0.017103, loss_ce: 0.005506
2022-01-11 23:57:13,023 iteration 3664 : loss : 0.037704, loss_ce: 0.011580
2022-01-11 23:57:14,566 iteration 3665 : loss : 0.021226, loss_ce: 0.009116
2022-01-11 23:57:16,208 iteration 3666 : loss : 0.030164, loss_ce: 0.013357
2022-01-11 23:57:17,795 iteration 3667 : loss : 0.038724, loss_ce: 0.014856
2022-01-11 23:57:19,390 iteration 3668 : loss : 0.031813, loss_ce: 0.012229
2022-01-11 23:57:20,953 iteration 3669 : loss : 0.028270, loss_ce: 0.013421
2022-01-11 23:57:22,480 iteration 3670 : loss : 0.034465, loss_ce: 0.010468
2022-01-11 23:57:24,066 iteration 3671 : loss : 0.025895, loss_ce: 0.010352
2022-01-11 23:57:25,584 iteration 3672 : loss : 0.020586, loss_ce: 0.008697
 54%|██████████████▌            | 216/400 [1:44:29<1:29:53, 29.31s/it]2022-01-11 23:57:27,163 iteration 3673 : loss : 0.022308, loss_ce: 0.011069
2022-01-11 23:57:28,711 iteration 3674 : loss : 0.022026, loss_ce: 0.009101
2022-01-11 23:57:30,254 iteration 3675 : loss : 0.019267, loss_ce: 0.009709
2022-01-11 23:57:31,800 iteration 3676 : loss : 0.016213, loss_ce: 0.005820
2022-01-11 23:57:33,337 iteration 3677 : loss : 0.032252, loss_ce: 0.016088
2022-01-11 23:57:34,958 iteration 3678 : loss : 0.036398, loss_ce: 0.012579
2022-01-11 23:57:36,508 iteration 3679 : loss : 0.025702, loss_ce: 0.009974
2022-01-11 23:57:38,048 iteration 3680 : loss : 0.030991, loss_ce: 0.009817
2022-01-11 23:57:39,640 iteration 3681 : loss : 0.032221, loss_ce: 0.013290
2022-01-11 23:57:41,179 iteration 3682 : loss : 0.021656, loss_ce: 0.008816
2022-01-11 23:57:42,771 iteration 3683 : loss : 0.050653, loss_ce: 0.004952
2022-01-11 23:57:44,299 iteration 3684 : loss : 0.015763, loss_ce: 0.004594
2022-01-11 23:57:45,836 iteration 3685 : loss : 0.027374, loss_ce: 0.011673
2022-01-11 23:57:47,411 iteration 3686 : loss : 0.035874, loss_ce: 0.014729
2022-01-11 23:57:48,970 iteration 3687 : loss : 0.027076, loss_ce: 0.009094
2022-01-11 23:57:50,509 iteration 3688 : loss : 0.027772, loss_ce: 0.012849
2022-01-11 23:57:51,999 iteration 3689 : loss : 0.020748, loss_ce: 0.007382
 54%|██████████████▋            | 217/400 [1:44:56<1:26:45, 28.45s/it]2022-01-11 23:57:53,560 iteration 3690 : loss : 0.018138, loss_ce: 0.004470
2022-01-11 23:57:55,176 iteration 3691 : loss : 0.043808, loss_ce: 0.009564
2022-01-11 23:57:56,732 iteration 3692 : loss : 0.028092, loss_ce: 0.012528
2022-01-11 23:57:58,360 iteration 3693 : loss : 0.029703, loss_ce: 0.011342
2022-01-11 23:57:59,928 iteration 3694 : loss : 0.018866, loss_ce: 0.008263
2022-01-11 23:58:01,448 iteration 3695 : loss : 0.021241, loss_ce: 0.008581
2022-01-11 23:58:03,036 iteration 3696 : loss : 0.028431, loss_ce: 0.013713
2022-01-11 23:58:04,569 iteration 3697 : loss : 0.022086, loss_ce: 0.007059
2022-01-11 23:58:06,124 iteration 3698 : loss : 0.028344, loss_ce: 0.010923
2022-01-11 23:58:07,741 iteration 3699 : loss : 0.031647, loss_ce: 0.011661
2022-01-11 23:58:09,291 iteration 3700 : loss : 0.025768, loss_ce: 0.011603
2022-01-11 23:58:10,820 iteration 3701 : loss : 0.023248, loss_ce: 0.009739
2022-01-11 23:58:12,343 iteration 3702 : loss : 0.031375, loss_ce: 0.012024
2022-01-11 23:58:13,999 iteration 3703 : loss : 0.037892, loss_ce: 0.018090
2022-01-11 23:58:15,513 iteration 3704 : loss : 0.018537, loss_ce: 0.005824
2022-01-11 23:58:17,033 iteration 3705 : loss : 0.020419, loss_ce: 0.006778
2022-01-11 23:58:18,525 iteration 3706 : loss : 0.020767, loss_ce: 0.005423
 55%|██████████████▋            | 218/400 [1:45:22<1:24:31, 27.87s/it]2022-01-11 23:58:20,109 iteration 3707 : loss : 0.033979, loss_ce: 0.011929
2022-01-11 23:58:21,675 iteration 3708 : loss : 0.026127, loss_ce: 0.010185
2022-01-11 23:58:23,233 iteration 3709 : loss : 0.021318, loss_ce: 0.006792
2022-01-11 23:58:24,829 iteration 3710 : loss : 0.029126, loss_ce: 0.010461
2022-01-11 23:58:26,361 iteration 3711 : loss : 0.019985, loss_ce: 0.007184
2022-01-11 23:58:27,938 iteration 3712 : loss : 0.028699, loss_ce: 0.008922
2022-01-11 23:58:29,463 iteration 3713 : loss : 0.022409, loss_ce: 0.008056
2022-01-11 23:58:30,990 iteration 3714 : loss : 0.017375, loss_ce: 0.006213
2022-01-11 23:58:32,556 iteration 3715 : loss : 0.023356, loss_ce: 0.009161
2022-01-11 23:58:34,084 iteration 3716 : loss : 0.021951, loss_ce: 0.007937
2022-01-11 23:58:35,648 iteration 3717 : loss : 0.041985, loss_ce: 0.017697
2022-01-11 23:58:37,257 iteration 3718 : loss : 0.027375, loss_ce: 0.011912
2022-01-11 23:58:38,769 iteration 3719 : loss : 0.021751, loss_ce: 0.005593
2022-01-11 23:58:40,338 iteration 3720 : loss : 0.019471, loss_ce: 0.009365
2022-01-11 23:58:41,889 iteration 3721 : loss : 0.021487, loss_ce: 0.008623
2022-01-11 23:58:43,434 iteration 3722 : loss : 0.019330, loss_ce: 0.007014
2022-01-11 23:58:44,954 iteration 3723 : loss : 0.034503, loss_ce: 0.010828
 55%|██████████████▊            | 219/400 [1:45:49<1:22:46, 27.44s/it]2022-01-11 23:58:46,488 iteration 3724 : loss : 0.019936, loss_ce: 0.009919
2022-01-11 23:58:48,009 iteration 3725 : loss : 0.023777, loss_ce: 0.010581
2022-01-11 23:58:49,611 iteration 3726 : loss : 0.029585, loss_ce: 0.014921
2022-01-11 23:58:51,179 iteration 3727 : loss : 0.022850, loss_ce: 0.008652
2022-01-11 23:58:52,761 iteration 3728 : loss : 0.027755, loss_ce: 0.007292
2022-01-11 23:58:54,286 iteration 3729 : loss : 0.018026, loss_ce: 0.004778
2022-01-11 23:58:55,827 iteration 3730 : loss : 0.018963, loss_ce: 0.007849
2022-01-11 23:58:57,372 iteration 3731 : loss : 0.022641, loss_ce: 0.007536
2022-01-11 23:58:58,952 iteration 3732 : loss : 0.023158, loss_ce: 0.007754
2022-01-11 23:59:00,464 iteration 3733 : loss : 0.020575, loss_ce: 0.007770
2022-01-11 23:59:02,029 iteration 3734 : loss : 0.024593, loss_ce: 0.011231
2022-01-11 23:59:03,588 iteration 3735 : loss : 0.032261, loss_ce: 0.010724
2022-01-11 23:59:05,202 iteration 3736 : loss : 0.028831, loss_ce: 0.015055
2022-01-11 23:59:06,726 iteration 3737 : loss : 0.020244, loss_ce: 0.006416
2022-01-11 23:59:08,275 iteration 3738 : loss : 0.020474, loss_ce: 0.008385
2022-01-11 23:59:09,788 iteration 3739 : loss : 0.024302, loss_ce: 0.010795
2022-01-11 23:59:09,788 Training Data Eval:
2022-01-11 23:59:17,795   Average segmentation loss on training set: 0.0141
2022-01-11 23:59:17,795 Validation Data Eval:
2022-01-11 23:59:20,554   Average segmentation loss on validation set: 0.0660
2022-01-11 23:59:26,442 Found new lowest validation loss at iteration 3739! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-11 23:59:28,056 iteration 3740 : loss : 0.027772, loss_ce: 0.012743
 55%|██████████████▊            | 220/400 [1:46:32<1:36:24, 32.13s/it]2022-01-11 23:59:29,669 iteration 3741 : loss : 0.024413, loss_ce: 0.007661
2022-01-11 23:59:31,124 iteration 3742 : loss : 0.023227, loss_ce: 0.011694
2022-01-11 23:59:32,579 iteration 3743 : loss : 0.028803, loss_ce: 0.010934
2022-01-11 23:59:34,085 iteration 3744 : loss : 0.035796, loss_ce: 0.013897
2022-01-11 23:59:35,612 iteration 3745 : loss : 0.036116, loss_ce: 0.013375
2022-01-11 23:59:37,147 iteration 3746 : loss : 0.025479, loss_ce: 0.010572
2022-01-11 23:59:38,600 iteration 3747 : loss : 0.021739, loss_ce: 0.007030
2022-01-11 23:59:40,044 iteration 3748 : loss : 0.018540, loss_ce: 0.008658
2022-01-11 23:59:41,563 iteration 3749 : loss : 0.022409, loss_ce: 0.007800
2022-01-11 23:59:43,153 iteration 3750 : loss : 0.038999, loss_ce: 0.012286
2022-01-11 23:59:44,748 iteration 3751 : loss : 0.032534, loss_ce: 0.009323
2022-01-11 23:59:46,307 iteration 3752 : loss : 0.026085, loss_ce: 0.006604
2022-01-11 23:59:47,838 iteration 3753 : loss : 0.023277, loss_ce: 0.007737
2022-01-11 23:59:49,418 iteration 3754 : loss : 0.018558, loss_ce: 0.006753
2022-01-11 23:59:51,041 iteration 3755 : loss : 0.026306, loss_ce: 0.013616
2022-01-11 23:59:52,546 iteration 3756 : loss : 0.027172, loss_ce: 0.012987
2022-01-11 23:59:54,071 iteration 3757 : loss : 0.026989, loss_ce: 0.012053
 55%|██████████████▉            | 221/400 [1:46:58<1:30:23, 30.30s/it]2022-01-11 23:59:55,744 iteration 3758 : loss : 0.020961, loss_ce: 0.006886
2022-01-11 23:59:57,271 iteration 3759 : loss : 0.021763, loss_ce: 0.007871
2022-01-11 23:59:58,831 iteration 3760 : loss : 0.018662, loss_ce: 0.007586
2022-01-12 00:00:00,446 iteration 3761 : loss : 0.021530, loss_ce: 0.008962
2022-01-12 00:00:02,065 iteration 3762 : loss : 0.020765, loss_ce: 0.009930
2022-01-12 00:00:03,728 iteration 3763 : loss : 0.034266, loss_ce: 0.012190
2022-01-12 00:00:05,298 iteration 3764 : loss : 0.030324, loss_ce: 0.011207
2022-01-12 00:00:06,830 iteration 3765 : loss : 0.021370, loss_ce: 0.009400
2022-01-12 00:00:08,455 iteration 3766 : loss : 0.043004, loss_ce: 0.011802
2022-01-12 00:00:10,021 iteration 3767 : loss : 0.017762, loss_ce: 0.006194
2022-01-12 00:00:11,557 iteration 3768 : loss : 0.027328, loss_ce: 0.008793
2022-01-12 00:00:13,097 iteration 3769 : loss : 0.020516, loss_ce: 0.008870
2022-01-12 00:00:14,695 iteration 3770 : loss : 0.029538, loss_ce: 0.006274
2022-01-12 00:00:16,382 iteration 3771 : loss : 0.032370, loss_ce: 0.010881
2022-01-12 00:00:18,012 iteration 3772 : loss : 0.027378, loss_ce: 0.007479
2022-01-12 00:00:19,605 iteration 3773 : loss : 0.020013, loss_ce: 0.007711
2022-01-12 00:00:21,235 iteration 3774 : loss : 0.046754, loss_ce: 0.023444
 56%|██████████████▉            | 222/400 [1:47:25<1:27:05, 29.36s/it]2022-01-12 00:00:22,821 iteration 3775 : loss : 0.024802, loss_ce: 0.009791
2022-01-12 00:00:24,320 iteration 3776 : loss : 0.043905, loss_ce: 0.023458
2022-01-12 00:00:25,871 iteration 3777 : loss : 0.022301, loss_ce: 0.005985
2022-01-12 00:00:27,406 iteration 3778 : loss : 0.015593, loss_ce: 0.006101
2022-01-12 00:00:28,942 iteration 3779 : loss : 0.018092, loss_ce: 0.007962
2022-01-12 00:00:30,521 iteration 3780 : loss : 0.025609, loss_ce: 0.010016
2022-01-12 00:00:32,090 iteration 3781 : loss : 0.029126, loss_ce: 0.011840
2022-01-12 00:00:33,677 iteration 3782 : loss : 0.080430, loss_ce: 0.015699
2022-01-12 00:00:35,273 iteration 3783 : loss : 0.020235, loss_ce: 0.009811
2022-01-12 00:00:36,868 iteration 3784 : loss : 0.020951, loss_ce: 0.009060
2022-01-12 00:00:38,409 iteration 3785 : loss : 0.020649, loss_ce: 0.008563
2022-01-12 00:00:39,951 iteration 3786 : loss : 0.018853, loss_ce: 0.007186
2022-01-12 00:00:41,482 iteration 3787 : loss : 0.022253, loss_ce: 0.007368
2022-01-12 00:00:43,113 iteration 3788 : loss : 0.052539, loss_ce: 0.014556
2022-01-12 00:00:44,683 iteration 3789 : loss : 0.022175, loss_ce: 0.005528
2022-01-12 00:00:46,277 iteration 3790 : loss : 0.038241, loss_ce: 0.012985
2022-01-12 00:00:47,870 iteration 3791 : loss : 0.022897, loss_ce: 0.008733
 56%|███████████████            | 223/400 [1:47:52<1:24:12, 28.54s/it]2022-01-12 00:00:49,444 iteration 3792 : loss : 0.014665, loss_ce: 0.004962
2022-01-12 00:00:51,059 iteration 3793 : loss : 0.023899, loss_ce: 0.008082
2022-01-12 00:00:52,646 iteration 3794 : loss : 0.025101, loss_ce: 0.010281
2022-01-12 00:00:54,213 iteration 3795 : loss : 0.019453, loss_ce: 0.006034
2022-01-12 00:00:55,698 iteration 3796 : loss : 0.015642, loss_ce: 0.004914
2022-01-12 00:00:57,237 iteration 3797 : loss : 0.027252, loss_ce: 0.011093
2022-01-12 00:00:58,745 iteration 3798 : loss : 0.020256, loss_ce: 0.007425
2022-01-12 00:01:00,297 iteration 3799 : loss : 0.020197, loss_ce: 0.007873
2022-01-12 00:01:01,850 iteration 3800 : loss : 0.026667, loss_ce: 0.008373
2022-01-12 00:01:03,404 iteration 3801 : loss : 0.035076, loss_ce: 0.008363
2022-01-12 00:01:04,944 iteration 3802 : loss : 0.021415, loss_ce: 0.010139
2022-01-12 00:01:06,504 iteration 3803 : loss : 0.016363, loss_ce: 0.005009
2022-01-12 00:01:08,066 iteration 3804 : loss : 0.021184, loss_ce: 0.007828
2022-01-12 00:01:09,599 iteration 3805 : loss : 0.021786, loss_ce: 0.007921
2022-01-12 00:01:11,136 iteration 3806 : loss : 0.027817, loss_ce: 0.013935
2022-01-12 00:01:12,734 iteration 3807 : loss : 0.022756, loss_ce: 0.008249
2022-01-12 00:01:14,300 iteration 3808 : loss : 0.016953, loss_ce: 0.006352
 56%|███████████████            | 224/400 [1:48:18<1:21:51, 27.91s/it]2022-01-12 00:01:15,852 iteration 3809 : loss : 0.017703, loss_ce: 0.006312
2022-01-12 00:01:17,399 iteration 3810 : loss : 0.021032, loss_ce: 0.009525
2022-01-12 00:01:18,956 iteration 3811 : loss : 0.019385, loss_ce: 0.005053
2022-01-12 00:01:20,537 iteration 3812 : loss : 0.022976, loss_ce: 0.011252
2022-01-12 00:01:22,088 iteration 3813 : loss : 0.017221, loss_ce: 0.007759
2022-01-12 00:01:23,684 iteration 3814 : loss : 0.023003, loss_ce: 0.009533
2022-01-12 00:01:25,252 iteration 3815 : loss : 0.028828, loss_ce: 0.008512
2022-01-12 00:01:26,827 iteration 3816 : loss : 0.017955, loss_ce: 0.007096
2022-01-12 00:01:28,355 iteration 3817 : loss : 0.020723, loss_ce: 0.007999
2022-01-12 00:01:29,955 iteration 3818 : loss : 0.016075, loss_ce: 0.007361
2022-01-12 00:01:31,558 iteration 3819 : loss : 0.021832, loss_ce: 0.009380
2022-01-12 00:01:33,144 iteration 3820 : loss : 0.023777, loss_ce: 0.009135
2022-01-12 00:01:34,739 iteration 3821 : loss : 0.025143, loss_ce: 0.010260
2022-01-12 00:01:36,276 iteration 3822 : loss : 0.029222, loss_ce: 0.009358
2022-01-12 00:01:37,914 iteration 3823 : loss : 0.024430, loss_ce: 0.006645
2022-01-12 00:01:39,484 iteration 3824 : loss : 0.034705, loss_ce: 0.012952
2022-01-12 00:01:39,484 Training Data Eval:
2022-01-12 00:01:47,480   Average segmentation loss on training set: 0.0141
2022-01-12 00:01:47,480 Validation Data Eval:
2022-01-12 00:01:50,240   Average segmentation loss on validation set: 0.0769
2022-01-12 00:01:51,882 iteration 3825 : loss : 0.025395, loss_ce: 0.009307
 56%|███████████████▏           | 225/400 [1:48:56<1:29:51, 30.81s/it]2022-01-12 00:01:53,498 iteration 3826 : loss : 0.020344, loss_ce: 0.009345
2022-01-12 00:01:55,026 iteration 3827 : loss : 0.023415, loss_ce: 0.008077
2022-01-12 00:01:56,618 iteration 3828 : loss : 0.020903, loss_ce: 0.007137
2022-01-12 00:01:58,196 iteration 3829 : loss : 0.023084, loss_ce: 0.007818
2022-01-12 00:01:59,732 iteration 3830 : loss : 0.017888, loss_ce: 0.006730
2022-01-12 00:02:01,257 iteration 3831 : loss : 0.020232, loss_ce: 0.006734
2022-01-12 00:02:02,781 iteration 3832 : loss : 0.020020, loss_ce: 0.004833
2022-01-12 00:02:04,349 iteration 3833 : loss : 0.020691, loss_ce: 0.009413
2022-01-12 00:02:05,943 iteration 3834 : loss : 0.024687, loss_ce: 0.011115
2022-01-12 00:02:07,521 iteration 3835 : loss : 0.022614, loss_ce: 0.007420
2022-01-12 00:02:09,100 iteration 3836 : loss : 0.022334, loss_ce: 0.009437
2022-01-12 00:02:10,698 iteration 3837 : loss : 0.020310, loss_ce: 0.006837
2022-01-12 00:02:12,333 iteration 3838 : loss : 0.030786, loss_ce: 0.012360
2022-01-12 00:02:13,891 iteration 3839 : loss : 0.019369, loss_ce: 0.007771
2022-01-12 00:02:15,465 iteration 3840 : loss : 0.026336, loss_ce: 0.007119
2022-01-12 00:02:17,005 iteration 3841 : loss : 0.021014, loss_ce: 0.007908
2022-01-12 00:02:18,591 iteration 3842 : loss : 0.040972, loss_ce: 0.019609
 56%|███████████████▎           | 226/400 [1:49:22<1:25:46, 29.58s/it]2022-01-12 00:02:20,166 iteration 3843 : loss : 0.020616, loss_ce: 0.008365
2022-01-12 00:02:21,777 iteration 3844 : loss : 0.028588, loss_ce: 0.010260
2022-01-12 00:02:23,369 iteration 3845 : loss : 0.020331, loss_ce: 0.009324
2022-01-12 00:02:24,900 iteration 3846 : loss : 0.017074, loss_ce: 0.006405
2022-01-12 00:02:26,452 iteration 3847 : loss : 0.020595, loss_ce: 0.007639
2022-01-12 00:02:28,055 iteration 3848 : loss : 0.018425, loss_ce: 0.008873
2022-01-12 00:02:29,713 iteration 3849 : loss : 0.024682, loss_ce: 0.011857
2022-01-12 00:02:31,346 iteration 3850 : loss : 0.044954, loss_ce: 0.010797
2022-01-12 00:02:32,915 iteration 3851 : loss : 0.021536, loss_ce: 0.009547
2022-01-12 00:02:34,504 iteration 3852 : loss : 0.020352, loss_ce: 0.005566
2022-01-12 00:02:36,034 iteration 3853 : loss : 0.015569, loss_ce: 0.005132
2022-01-12 00:02:37,621 iteration 3854 : loss : 0.031773, loss_ce: 0.008713
2022-01-12 00:02:39,307 iteration 3855 : loss : 0.033453, loss_ce: 0.011013
2022-01-12 00:02:40,750 iteration 3856 : loss : 0.018121, loss_ce: 0.007962
2022-01-12 00:02:42,305 iteration 3857 : loss : 0.040614, loss_ce: 0.021091
2022-01-12 00:02:43,785 iteration 3858 : loss : 0.020190, loss_ce: 0.007806
2022-01-12 00:02:45,340 iteration 3859 : loss : 0.021083, loss_ce: 0.008644
 57%|███████████████▎           | 227/400 [1:49:49<1:22:50, 28.73s/it]2022-01-12 00:02:46,949 iteration 3860 : loss : 0.017001, loss_ce: 0.006368
2022-01-12 00:02:48,577 iteration 3861 : loss : 0.056867, loss_ce: 0.025917
2022-01-12 00:02:50,169 iteration 3862 : loss : 0.048355, loss_ce: 0.021593
2022-01-12 00:02:51,741 iteration 3863 : loss : 0.020556, loss_ce: 0.006906
2022-01-12 00:02:53,346 iteration 3864 : loss : 0.024822, loss_ce: 0.008733
2022-01-12 00:02:54,909 iteration 3865 : loss : 0.022434, loss_ce: 0.007124
2022-01-12 00:02:56,568 iteration 3866 : loss : 0.024122, loss_ce: 0.007828
2022-01-12 00:02:58,115 iteration 3867 : loss : 0.025688, loss_ce: 0.009162
2022-01-12 00:02:59,737 iteration 3868 : loss : 0.039966, loss_ce: 0.014428
2022-01-12 00:03:01,328 iteration 3869 : loss : 0.033374, loss_ce: 0.016619
2022-01-12 00:03:02,927 iteration 3870 : loss : 0.022567, loss_ce: 0.009113
2022-01-12 00:03:04,511 iteration 3871 : loss : 0.037534, loss_ce: 0.011207
2022-01-12 00:03:06,057 iteration 3872 : loss : 0.019508, loss_ce: 0.007290
2022-01-12 00:03:07,589 iteration 3873 : loss : 0.026233, loss_ce: 0.013052
2022-01-12 00:03:09,181 iteration 3874 : loss : 0.029538, loss_ce: 0.012429
2022-01-12 00:03:10,765 iteration 3875 : loss : 0.030473, loss_ce: 0.011483
2022-01-12 00:03:12,362 iteration 3876 : loss : 0.039039, loss_ce: 0.013063
 57%|███████████████▍           | 228/400 [1:50:16<1:20:53, 28.22s/it]2022-01-12 00:03:13,926 iteration 3877 : loss : 0.021108, loss_ce: 0.007991
2022-01-12 00:03:15,525 iteration 3878 : loss : 0.025755, loss_ce: 0.010590
2022-01-12 00:03:17,063 iteration 3879 : loss : 0.023087, loss_ce: 0.009015
2022-01-12 00:03:18,663 iteration 3880 : loss : 0.023373, loss_ce: 0.010022
2022-01-12 00:03:20,205 iteration 3881 : loss : 0.027596, loss_ce: 0.012167
2022-01-12 00:03:21,857 iteration 3882 : loss : 0.041454, loss_ce: 0.010520
2022-01-12 00:03:23,475 iteration 3883 : loss : 0.038976, loss_ce: 0.012949
2022-01-12 00:03:25,066 iteration 3884 : loss : 0.024446, loss_ce: 0.010895
2022-01-12 00:03:26,671 iteration 3885 : loss : 0.107059, loss_ce: 0.018760
2022-01-12 00:03:28,276 iteration 3886 : loss : 0.028653, loss_ce: 0.011151
2022-01-12 00:03:29,798 iteration 3887 : loss : 0.022274, loss_ce: 0.007578
2022-01-12 00:03:31,400 iteration 3888 : loss : 0.032838, loss_ce: 0.018897
2022-01-12 00:03:32,962 iteration 3889 : loss : 0.019595, loss_ce: 0.007309
2022-01-12 00:03:34,622 iteration 3890 : loss : 0.045484, loss_ce: 0.012916
2022-01-12 00:03:36,156 iteration 3891 : loss : 0.031136, loss_ce: 0.012589
2022-01-12 00:03:37,728 iteration 3892 : loss : 0.025408, loss_ce: 0.011646
2022-01-12 00:03:39,330 iteration 3893 : loss : 0.030971, loss_ce: 0.009994
 57%|███████████████▍           | 229/400 [1:50:43<1:19:21, 27.84s/it]2022-01-12 00:03:40,923 iteration 3894 : loss : 0.019885, loss_ce: 0.005797
2022-01-12 00:03:42,442 iteration 3895 : loss : 0.020617, loss_ce: 0.007339
2022-01-12 00:03:44,000 iteration 3896 : loss : 0.027069, loss_ce: 0.010895
2022-01-12 00:03:45,594 iteration 3897 : loss : 0.027802, loss_ce: 0.012554
2022-01-12 00:03:47,191 iteration 3898 : loss : 0.028851, loss_ce: 0.009788
2022-01-12 00:03:48,700 iteration 3899 : loss : 0.022477, loss_ce: 0.008179
2022-01-12 00:03:50,237 iteration 3900 : loss : 0.028361, loss_ce: 0.009622
2022-01-12 00:03:51,750 iteration 3901 : loss : 0.023039, loss_ce: 0.009351
2022-01-12 00:03:53,281 iteration 3902 : loss : 0.027463, loss_ce: 0.009624
2022-01-12 00:03:54,778 iteration 3903 : loss : 0.024534, loss_ce: 0.007692
2022-01-12 00:03:56,326 iteration 3904 : loss : 0.021341, loss_ce: 0.007255
2022-01-12 00:03:57,865 iteration 3905 : loss : 0.031987, loss_ce: 0.012703
2022-01-12 00:03:59,353 iteration 3906 : loss : 0.016594, loss_ce: 0.006973
2022-01-12 00:04:00,826 iteration 3907 : loss : 0.016675, loss_ce: 0.006233
2022-01-12 00:04:02,324 iteration 3908 : loss : 0.019811, loss_ce: 0.007761
2022-01-12 00:04:03,874 iteration 3909 : loss : 0.024544, loss_ce: 0.013230
2022-01-12 00:04:03,875 Training Data Eval:
2022-01-12 00:04:11,885   Average segmentation loss on training set: 0.0175
2022-01-12 00:04:11,886 Validation Data Eval:
2022-01-12 00:04:14,641   Average segmentation loss on validation set: 0.0852
2022-01-12 00:04:16,275 iteration 3910 : loss : 0.019887, loss_ce: 0.006557
 57%|███████████████▌           | 230/400 [1:51:20<1:26:37, 30.58s/it]2022-01-12 00:04:17,846 iteration 3911 : loss : 0.023851, loss_ce: 0.008670
2022-01-12 00:04:19,436 iteration 3912 : loss : 0.033036, loss_ce: 0.013243
2022-01-12 00:04:21,005 iteration 3913 : loss : 0.021610, loss_ce: 0.008273
2022-01-12 00:04:22,563 iteration 3914 : loss : 0.019789, loss_ce: 0.010399
2022-01-12 00:04:24,112 iteration 3915 : loss : 0.027254, loss_ce: 0.012342
2022-01-12 00:04:25,637 iteration 3916 : loss : 0.017991, loss_ce: 0.005901
2022-01-12 00:04:27,226 iteration 3917 : loss : 0.025791, loss_ce: 0.009321
2022-01-12 00:04:28,815 iteration 3918 : loss : 0.030190, loss_ce: 0.010813
2022-01-12 00:04:30,495 iteration 3919 : loss : 0.024482, loss_ce: 0.006835
2022-01-12 00:04:32,018 iteration 3920 : loss : 0.018757, loss_ce: 0.007047
2022-01-12 00:04:33,569 iteration 3921 : loss : 0.023609, loss_ce: 0.008870
2022-01-12 00:04:35,161 iteration 3922 : loss : 0.021644, loss_ce: 0.008082
2022-01-12 00:04:36,705 iteration 3923 : loss : 0.021088, loss_ce: 0.009729
2022-01-12 00:04:38,220 iteration 3924 : loss : 0.023879, loss_ce: 0.006312
2022-01-12 00:04:39,845 iteration 3925 : loss : 0.027433, loss_ce: 0.009894
2022-01-12 00:04:41,448 iteration 3926 : loss : 0.023566, loss_ce: 0.010666
2022-01-12 00:04:43,001 iteration 3927 : loss : 0.027925, loss_ce: 0.011401
 58%|███████████████▌           | 231/400 [1:51:47<1:22:52, 29.42s/it]2022-01-12 00:04:44,653 iteration 3928 : loss : 0.034866, loss_ce: 0.015639
2022-01-12 00:04:46,217 iteration 3929 : loss : 0.019431, loss_ce: 0.009313
2022-01-12 00:04:47,818 iteration 3930 : loss : 0.022115, loss_ce: 0.008658
2022-01-12 00:04:49,418 iteration 3931 : loss : 0.027748, loss_ce: 0.012955
2022-01-12 00:04:51,014 iteration 3932 : loss : 0.032653, loss_ce: 0.011632
2022-01-12 00:04:52,642 iteration 3933 : loss : 0.018181, loss_ce: 0.006948
2022-01-12 00:04:54,226 iteration 3934 : loss : 0.021310, loss_ce: 0.009000
2022-01-12 00:04:55,713 iteration 3935 : loss : 0.017445, loss_ce: 0.007006
2022-01-12 00:04:57,395 iteration 3936 : loss : 0.026348, loss_ce: 0.010939
2022-01-12 00:04:58,910 iteration 3937 : loss : 0.024870, loss_ce: 0.012407
2022-01-12 00:05:00,511 iteration 3938 : loss : 0.069631, loss_ce: 0.014836
2022-01-12 00:05:02,002 iteration 3939 : loss : 0.024669, loss_ce: 0.007754
2022-01-12 00:05:03,543 iteration 3940 : loss : 0.023047, loss_ce: 0.008555
2022-01-12 00:05:05,102 iteration 3941 : loss : 0.022325, loss_ce: 0.008494
2022-01-12 00:05:06,710 iteration 3942 : loss : 0.058538, loss_ce: 0.026679
2022-01-12 00:05:08,230 iteration 3943 : loss : 0.022247, loss_ce: 0.006125
2022-01-12 00:05:09,837 iteration 3944 : loss : 0.026980, loss_ce: 0.009706
 58%|███████████████▋           | 232/400 [1:52:13<1:20:12, 28.64s/it]2022-01-12 00:05:11,442 iteration 3945 : loss : 0.025315, loss_ce: 0.011598
2022-01-12 00:05:12,903 iteration 3946 : loss : 0.016411, loss_ce: 0.007947
2022-01-12 00:05:14,456 iteration 3947 : loss : 0.016354, loss_ce: 0.005031
2022-01-12 00:05:16,143 iteration 3948 : loss : 0.024929, loss_ce: 0.008693
2022-01-12 00:05:17,741 iteration 3949 : loss : 0.026573, loss_ce: 0.011484
2022-01-12 00:05:19,370 iteration 3950 : loss : 0.023017, loss_ce: 0.007314
2022-01-12 00:05:20,873 iteration 3951 : loss : 0.018875, loss_ce: 0.007555
2022-01-12 00:05:22,466 iteration 3952 : loss : 0.021085, loss_ce: 0.007888
2022-01-12 00:05:24,073 iteration 3953 : loss : 0.044099, loss_ce: 0.017945
2022-01-12 00:05:25,602 iteration 3954 : loss : 0.024677, loss_ce: 0.009017
2022-01-12 00:05:27,144 iteration 3955 : loss : 0.027241, loss_ce: 0.013946
2022-01-12 00:05:28,820 iteration 3956 : loss : 0.024516, loss_ce: 0.010658
2022-01-12 00:05:30,343 iteration 3957 : loss : 0.026916, loss_ce: 0.011677
2022-01-12 00:05:31,876 iteration 3958 : loss : 0.021712, loss_ce: 0.009114
2022-01-12 00:05:33,518 iteration 3959 : loss : 0.030005, loss_ce: 0.009969
2022-01-12 00:05:35,057 iteration 3960 : loss : 0.020240, loss_ce: 0.009849
2022-01-12 00:05:36,612 iteration 3961 : loss : 0.018178, loss_ce: 0.006962
 58%|███████████████▋           | 233/400 [1:52:40<1:18:09, 28.08s/it]2022-01-12 00:05:38,236 iteration 3962 : loss : 0.035876, loss_ce: 0.012013
2022-01-12 00:05:39,792 iteration 3963 : loss : 0.026021, loss_ce: 0.014862
2022-01-12 00:05:41,429 iteration 3964 : loss : 0.029646, loss_ce: 0.012538
2022-01-12 00:05:42,941 iteration 3965 : loss : 0.019854, loss_ce: 0.005741
2022-01-12 00:05:44,555 iteration 3966 : loss : 0.032382, loss_ce: 0.011135
2022-01-12 00:05:46,132 iteration 3967 : loss : 0.022032, loss_ce: 0.007730
2022-01-12 00:05:47,600 iteration 3968 : loss : 0.022057, loss_ce: 0.007736
2022-01-12 00:05:49,184 iteration 3969 : loss : 0.022666, loss_ce: 0.006590
2022-01-12 00:05:50,730 iteration 3970 : loss : 0.027682, loss_ce: 0.009469
2022-01-12 00:05:52,194 iteration 3971 : loss : 0.016590, loss_ce: 0.006560
2022-01-12 00:05:53,779 iteration 3972 : loss : 0.021924, loss_ce: 0.006424
2022-01-12 00:05:55,300 iteration 3973 : loss : 0.025773, loss_ce: 0.013802
2022-01-12 00:05:56,785 iteration 3974 : loss : 0.017077, loss_ce: 0.005221
2022-01-12 00:05:58,360 iteration 3975 : loss : 0.022039, loss_ce: 0.009471
2022-01-12 00:05:59,970 iteration 3976 : loss : 0.027903, loss_ce: 0.010867
2022-01-12 00:06:01,580 iteration 3977 : loss : 0.023605, loss_ce: 0.009886
2022-01-12 00:06:03,222 iteration 3978 : loss : 0.038529, loss_ce: 0.016525
 58%|███████████████▊           | 234/400 [1:53:07<1:16:28, 27.64s/it]2022-01-12 00:06:04,829 iteration 3979 : loss : 0.022982, loss_ce: 0.008326
2022-01-12 00:06:06,417 iteration 3980 : loss : 0.025741, loss_ce: 0.008586
2022-01-12 00:06:07,944 iteration 3981 : loss : 0.021481, loss_ce: 0.008656
2022-01-12 00:06:09,405 iteration 3982 : loss : 0.016195, loss_ce: 0.005775
2022-01-12 00:06:10,894 iteration 3983 : loss : 0.017746, loss_ce: 0.007876
2022-01-12 00:06:12,424 iteration 3984 : loss : 0.019029, loss_ce: 0.008088
2022-01-12 00:06:14,003 iteration 3985 : loss : 0.035804, loss_ce: 0.021196
2022-01-12 00:06:15,639 iteration 3986 : loss : 0.025616, loss_ce: 0.010990
2022-01-12 00:06:17,224 iteration 3987 : loss : 0.027998, loss_ce: 0.011709
2022-01-12 00:06:18,770 iteration 3988 : loss : 0.020498, loss_ce: 0.009594
2022-01-12 00:06:20,297 iteration 3989 : loss : 0.018211, loss_ce: 0.003911
2022-01-12 00:06:21,835 iteration 3990 : loss : 0.049518, loss_ce: 0.013306
2022-01-12 00:06:23,443 iteration 3991 : loss : 0.026228, loss_ce: 0.007040
2022-01-12 00:06:25,020 iteration 3992 : loss : 0.021782, loss_ce: 0.006939
2022-01-12 00:06:26,624 iteration 3993 : loss : 0.025279, loss_ce: 0.008217
2022-01-12 00:06:28,150 iteration 3994 : loss : 0.017557, loss_ce: 0.008055
2022-01-12 00:06:28,150 Training Data Eval:
2022-01-12 00:06:36,139   Average segmentation loss on training set: 0.0148
2022-01-12 00:06:36,140 Validation Data Eval:
2022-01-12 00:06:38,895   Average segmentation loss on validation set: 0.0950
2022-01-12 00:06:40,413 iteration 3995 : loss : 0.019188, loss_ce: 0.007291
 59%|███████████████▊           | 235/400 [1:53:44<1:23:53, 30.51s/it]2022-01-12 00:06:42,075 iteration 3996 : loss : 0.024702, loss_ce: 0.007225
2022-01-12 00:06:43,588 iteration 3997 : loss : 0.020646, loss_ce: 0.006635
2022-01-12 00:06:45,173 iteration 3998 : loss : 0.033928, loss_ce: 0.013494
2022-01-12 00:06:46,722 iteration 3999 : loss : 0.018757, loss_ce: 0.004458
2022-01-12 00:06:48,322 iteration 4000 : loss : 0.019583, loss_ce: 0.009495
2022-01-12 00:06:49,827 iteration 4001 : loss : 0.018514, loss_ce: 0.006950
2022-01-12 00:06:51,375 iteration 4002 : loss : 0.017553, loss_ce: 0.007094
2022-01-12 00:06:52,944 iteration 4003 : loss : 0.019773, loss_ce: 0.008498
2022-01-12 00:06:54,532 iteration 4004 : loss : 0.033197, loss_ce: 0.007630
2022-01-12 00:06:56,098 iteration 4005 : loss : 0.031954, loss_ce: 0.015510
2022-01-12 00:06:57,615 iteration 4006 : loss : 0.016591, loss_ce: 0.006809
2022-01-12 00:06:59,218 iteration 4007 : loss : 0.043661, loss_ce: 0.013177
2022-01-12 00:07:00,737 iteration 4008 : loss : 0.025876, loss_ce: 0.012885
2022-01-12 00:07:02,271 iteration 4009 : loss : 0.029349, loss_ce: 0.012682
2022-01-12 00:07:03,873 iteration 4010 : loss : 0.033236, loss_ce: 0.013582
2022-01-12 00:07:05,381 iteration 4011 : loss : 0.017580, loss_ce: 0.005123
2022-01-12 00:07:07,008 iteration 4012 : loss : 0.034343, loss_ce: 0.015133
 59%|███████████████▉           | 236/400 [1:54:11<1:20:10, 29.33s/it]2022-01-12 00:07:08,681 iteration 4013 : loss : 0.029652, loss_ce: 0.007867
2022-01-12 00:07:10,179 iteration 4014 : loss : 0.023162, loss_ce: 0.007965
2022-01-12 00:07:11,781 iteration 4015 : loss : 0.032272, loss_ce: 0.013996
2022-01-12 00:07:13,371 iteration 4016 : loss : 0.021684, loss_ce: 0.008060
2022-01-12 00:07:14,976 iteration 4017 : loss : 0.021444, loss_ce: 0.008288
2022-01-12 00:07:16,500 iteration 4018 : loss : 0.018964, loss_ce: 0.006442
2022-01-12 00:07:18,063 iteration 4019 : loss : 0.023792, loss_ce: 0.013020
2022-01-12 00:07:19,570 iteration 4020 : loss : 0.017506, loss_ce: 0.009488
2022-01-12 00:07:21,086 iteration 4021 : loss : 0.018260, loss_ce: 0.007932
2022-01-12 00:07:22,659 iteration 4022 : loss : 0.022364, loss_ce: 0.010509
2022-01-12 00:07:24,205 iteration 4023 : loss : 0.017241, loss_ce: 0.006110
2022-01-12 00:07:25,710 iteration 4024 : loss : 0.016900, loss_ce: 0.005802
2022-01-12 00:07:27,223 iteration 4025 : loss : 0.017185, loss_ce: 0.005739
2022-01-12 00:07:28,797 iteration 4026 : loss : 0.020002, loss_ce: 0.008155
2022-01-12 00:07:30,365 iteration 4027 : loss : 0.026415, loss_ce: 0.009232
2022-01-12 00:07:31,965 iteration 4028 : loss : 0.048397, loss_ce: 0.012874
2022-01-12 00:07:33,482 iteration 4029 : loss : 0.018757, loss_ce: 0.007393
 59%|███████████████▉           | 237/400 [1:54:37<1:17:21, 28.48s/it]2022-01-12 00:07:35,133 iteration 4030 : loss : 0.030809, loss_ce: 0.013458
2022-01-12 00:07:36,699 iteration 4031 : loss : 0.032415, loss_ce: 0.010660
2022-01-12 00:07:38,245 iteration 4032 : loss : 0.025336, loss_ce: 0.012228
2022-01-12 00:07:39,794 iteration 4033 : loss : 0.033234, loss_ce: 0.011162
2022-01-12 00:07:41,286 iteration 4034 : loss : 0.023367, loss_ce: 0.008015
2022-01-12 00:07:42,809 iteration 4035 : loss : 0.018510, loss_ce: 0.005751
2022-01-12 00:07:44,393 iteration 4036 : loss : 0.022021, loss_ce: 0.007750
2022-01-12 00:07:45,904 iteration 4037 : loss : 0.018978, loss_ce: 0.008669
2022-01-12 00:07:47,512 iteration 4038 : loss : 0.022662, loss_ce: 0.007565
2022-01-12 00:07:49,141 iteration 4039 : loss : 0.033312, loss_ce: 0.014111
2022-01-12 00:07:50,767 iteration 4040 : loss : 0.032092, loss_ce: 0.012974
2022-01-12 00:07:52,369 iteration 4041 : loss : 0.018447, loss_ce: 0.005911
2022-01-12 00:07:53,883 iteration 4042 : loss : 0.019962, loss_ce: 0.006747
2022-01-12 00:07:55,452 iteration 4043 : loss : 0.176544, loss_ce: 0.005482
2022-01-12 00:07:56,996 iteration 4044 : loss : 0.021393, loss_ce: 0.009197
2022-01-12 00:07:58,548 iteration 4045 : loss : 0.020438, loss_ce: 0.010185
2022-01-12 00:08:00,146 iteration 4046 : loss : 0.033682, loss_ce: 0.017004
 60%|████████████████           | 238/400 [1:55:04<1:15:24, 27.93s/it]2022-01-12 00:08:01,807 iteration 4047 : loss : 0.019595, loss_ce: 0.008947
2022-01-12 00:08:03,395 iteration 4048 : loss : 0.029833, loss_ce: 0.011166
2022-01-12 00:08:04,925 iteration 4049 : loss : 0.017554, loss_ce: 0.006696
2022-01-12 00:08:06,516 iteration 4050 : loss : 0.024182, loss_ce: 0.009493
2022-01-12 00:08:08,173 iteration 4051 : loss : 0.056733, loss_ce: 0.010775
2022-01-12 00:08:09,670 iteration 4052 : loss : 0.020413, loss_ce: 0.007517
2022-01-12 00:08:11,160 iteration 4053 : loss : 0.021908, loss_ce: 0.007407
2022-01-12 00:08:12,668 iteration 4054 : loss : 0.027010, loss_ce: 0.008163
2022-01-12 00:08:14,219 iteration 4055 : loss : 0.019481, loss_ce: 0.008652
2022-01-12 00:08:15,798 iteration 4056 : loss : 0.029148, loss_ce: 0.014639
2022-01-12 00:08:17,321 iteration 4057 : loss : 0.019584, loss_ce: 0.005414
2022-01-12 00:08:18,873 iteration 4058 : loss : 0.019365, loss_ce: 0.007835
2022-01-12 00:08:20,531 iteration 4059 : loss : 0.031780, loss_ce: 0.015251
2022-01-12 00:08:22,099 iteration 4060 : loss : 0.025580, loss_ce: 0.012373
2022-01-12 00:08:23,670 iteration 4061 : loss : 0.024545, loss_ce: 0.011450
2022-01-12 00:08:25,259 iteration 4062 : loss : 0.033790, loss_ce: 0.012894
2022-01-12 00:08:26,830 iteration 4063 : loss : 0.047978, loss_ce: 0.020118
 60%|████████████████▏          | 239/400 [1:55:30<1:13:56, 27.56s/it]2022-01-12 00:08:28,396 iteration 4064 : loss : 0.030458, loss_ce: 0.007096
2022-01-12 00:08:29,933 iteration 4065 : loss : 0.021716, loss_ce: 0.008633
2022-01-12 00:08:31,564 iteration 4066 : loss : 0.027280, loss_ce: 0.010535
2022-01-12 00:08:33,083 iteration 4067 : loss : 0.023930, loss_ce: 0.011289
2022-01-12 00:08:34,632 iteration 4068 : loss : 0.020954, loss_ce: 0.008204
2022-01-12 00:08:36,216 iteration 4069 : loss : 0.021559, loss_ce: 0.008922
2022-01-12 00:08:37,800 iteration 4070 : loss : 0.031985, loss_ce: 0.014505
2022-01-12 00:08:39,409 iteration 4071 : loss : 0.028392, loss_ce: 0.010490
2022-01-12 00:08:40,947 iteration 4072 : loss : 0.019585, loss_ce: 0.006615
2022-01-12 00:08:42,473 iteration 4073 : loss : 0.030007, loss_ce: 0.009180
2022-01-12 00:08:44,030 iteration 4074 : loss : 0.023986, loss_ce: 0.010764
2022-01-12 00:08:45,636 iteration 4075 : loss : 0.019599, loss_ce: 0.007107
2022-01-12 00:08:47,153 iteration 4076 : loss : 0.026925, loss_ce: 0.010879
2022-01-12 00:08:48,708 iteration 4077 : loss : 0.017422, loss_ce: 0.006157
2022-01-12 00:08:50,326 iteration 4078 : loss : 0.032274, loss_ce: 0.012277
2022-01-12 00:08:51,915 iteration 4079 : loss : 0.028447, loss_ce: 0.013844
2022-01-12 00:08:51,915 Training Data Eval:
2022-01-12 00:08:59,904   Average segmentation loss on training set: 0.0157
2022-01-12 00:08:59,905 Validation Data Eval:
2022-01-12 00:09:02,659   Average segmentation loss on validation set: 0.0694
2022-01-12 00:09:04,274 iteration 4080 : loss : 0.025274, loss_ce: 0.010911
 60%|████████████████▏          | 240/400 [1:56:08<1:21:24, 30.53s/it]2022-01-12 00:09:05,909 iteration 4081 : loss : 0.024073, loss_ce: 0.009847
2022-01-12 00:09:07,484 iteration 4082 : loss : 0.036040, loss_ce: 0.012239
2022-01-12 00:09:09,000 iteration 4083 : loss : 0.017318, loss_ce: 0.006950
2022-01-12 00:09:10,567 iteration 4084 : loss : 0.027892, loss_ce: 0.010152
2022-01-12 00:09:12,083 iteration 4085 : loss : 0.021057, loss_ce: 0.006239
2022-01-12 00:09:13,760 iteration 4086 : loss : 0.031675, loss_ce: 0.011960
2022-01-12 00:09:15,323 iteration 4087 : loss : 0.036742, loss_ce: 0.007009
2022-01-12 00:09:16,816 iteration 4088 : loss : 0.025723, loss_ce: 0.012336
2022-01-12 00:09:18,360 iteration 4089 : loss : 0.033613, loss_ce: 0.010047
2022-01-12 00:09:19,829 iteration 4090 : loss : 0.022106, loss_ce: 0.007258
2022-01-12 00:09:21,381 iteration 4091 : loss : 0.038240, loss_ce: 0.014907
2022-01-12 00:09:22,957 iteration 4092 : loss : 0.022041, loss_ce: 0.009288
2022-01-12 00:09:24,526 iteration 4093 : loss : 0.075886, loss_ce: 0.033379
2022-01-12 00:09:26,054 iteration 4094 : loss : 0.025245, loss_ce: 0.010309
2022-01-12 00:09:27,623 iteration 4095 : loss : 0.026348, loss_ce: 0.011779
2022-01-12 00:09:29,149 iteration 4096 : loss : 0.023323, loss_ce: 0.013251
2022-01-12 00:09:30,691 iteration 4097 : loss : 0.023913, loss_ce: 0.009985
 60%|████████████████▎          | 241/400 [1:56:34<1:17:37, 29.29s/it]2022-01-12 00:09:32,275 iteration 4098 : loss : 0.023161, loss_ce: 0.008753
2022-01-12 00:09:33,788 iteration 4099 : loss : 0.040343, loss_ce: 0.012218
2022-01-12 00:09:35,342 iteration 4100 : loss : 0.028295, loss_ce: 0.010423
2022-01-12 00:09:36,940 iteration 4101 : loss : 0.035210, loss_ce: 0.012331
2022-01-12 00:09:38,524 iteration 4102 : loss : 0.027581, loss_ce: 0.011972
2022-01-12 00:09:40,100 iteration 4103 : loss : 0.028727, loss_ce: 0.011885
2022-01-12 00:09:41,674 iteration 4104 : loss : 0.036984, loss_ce: 0.016749
2022-01-12 00:09:43,246 iteration 4105 : loss : 0.026299, loss_ce: 0.011143
2022-01-12 00:09:44,777 iteration 4106 : loss : 0.023701, loss_ce: 0.010569
2022-01-12 00:09:46,304 iteration 4107 : loss : 0.019293, loss_ce: 0.007273
2022-01-12 00:09:47,846 iteration 4108 : loss : 0.025074, loss_ce: 0.010190
2022-01-12 00:09:49,345 iteration 4109 : loss : 0.020985, loss_ce: 0.007491
2022-01-12 00:09:50,934 iteration 4110 : loss : 0.026406, loss_ce: 0.011436
2022-01-12 00:09:52,641 iteration 4111 : loss : 0.035985, loss_ce: 0.018333
2022-01-12 00:09:54,166 iteration 4112 : loss : 0.020436, loss_ce: 0.007029
2022-01-12 00:09:55,748 iteration 4113 : loss : 0.029445, loss_ce: 0.008952
2022-01-12 00:09:57,297 iteration 4114 : loss : 0.020769, loss_ce: 0.009846
 60%|████████████████▎          | 242/400 [1:57:01<1:15:00, 28.48s/it]2022-01-12 00:09:58,861 iteration 4115 : loss : 0.021627, loss_ce: 0.011148
2022-01-12 00:10:00,461 iteration 4116 : loss : 0.023077, loss_ce: 0.008016
2022-01-12 00:10:01,964 iteration 4117 : loss : 0.017495, loss_ce: 0.005912
2022-01-12 00:10:03,582 iteration 4118 : loss : 0.025897, loss_ce: 0.012534
2022-01-12 00:10:05,243 iteration 4119 : loss : 0.040257, loss_ce: 0.011182
2022-01-12 00:10:06,816 iteration 4120 : loss : 0.023300, loss_ce: 0.006513
2022-01-12 00:10:08,327 iteration 4121 : loss : 0.022863, loss_ce: 0.006888
2022-01-12 00:10:09,877 iteration 4122 : loss : 0.039393, loss_ce: 0.011728
2022-01-12 00:10:11,390 iteration 4123 : loss : 0.013377, loss_ce: 0.005713
2022-01-12 00:10:12,985 iteration 4124 : loss : 0.031632, loss_ce: 0.016811
2022-01-12 00:10:14,571 iteration 4125 : loss : 0.027350, loss_ce: 0.013314
2022-01-12 00:10:16,132 iteration 4126 : loss : 0.019000, loss_ce: 0.007513
2022-01-12 00:10:17,644 iteration 4127 : loss : 0.015733, loss_ce: 0.007790
2022-01-12 00:10:19,288 iteration 4128 : loss : 0.071824, loss_ce: 0.019797
2022-01-12 00:10:20,768 iteration 4129 : loss : 0.025112, loss_ce: 0.005968
2022-01-12 00:10:22,260 iteration 4130 : loss : 0.017289, loss_ce: 0.005298
2022-01-12 00:10:23,802 iteration 4131 : loss : 0.017198, loss_ce: 0.007816
 61%|████████████████▍          | 243/400 [1:57:27<1:12:58, 27.89s/it]2022-01-12 00:10:25,446 iteration 4132 : loss : 0.023484, loss_ce: 0.006817
2022-01-12 00:10:27,018 iteration 4133 : loss : 0.022937, loss_ce: 0.007057
2022-01-12 00:10:28,607 iteration 4134 : loss : 0.026597, loss_ce: 0.011793
2022-01-12 00:10:30,088 iteration 4135 : loss : 0.020246, loss_ce: 0.009449
2022-01-12 00:10:31,650 iteration 4136 : loss : 0.019925, loss_ce: 0.008843
2022-01-12 00:10:33,139 iteration 4137 : loss : 0.019406, loss_ce: 0.007883
2022-01-12 00:10:34,717 iteration 4138 : loss : 0.025646, loss_ce: 0.010919
2022-01-12 00:10:36,291 iteration 4139 : loss : 0.022165, loss_ce: 0.010404
2022-01-12 00:10:37,870 iteration 4140 : loss : 0.024710, loss_ce: 0.009866
2022-01-12 00:10:39,450 iteration 4141 : loss : 0.026896, loss_ce: 0.009646
2022-01-12 00:10:41,074 iteration 4142 : loss : 0.056919, loss_ce: 0.021801
2022-01-12 00:10:42,543 iteration 4143 : loss : 0.020675, loss_ce: 0.006579
2022-01-12 00:10:44,039 iteration 4144 : loss : 0.016878, loss_ce: 0.003340
2022-01-12 00:10:45,655 iteration 4145 : loss : 0.025288, loss_ce: 0.012423
2022-01-12 00:10:47,195 iteration 4146 : loss : 0.021855, loss_ce: 0.010315
2022-01-12 00:10:48,758 iteration 4147 : loss : 0.021170, loss_ce: 0.007914
2022-01-12 00:10:50,283 iteration 4148 : loss : 0.028118, loss_ce: 0.013285
 61%|████████████████▍          | 244/400 [1:57:54<1:11:25, 27.47s/it]2022-01-12 00:10:51,806 iteration 4149 : loss : 0.018442, loss_ce: 0.006209
2022-01-12 00:10:53,390 iteration 4150 : loss : 0.018848, loss_ce: 0.006017
2022-01-12 00:10:55,006 iteration 4151 : loss : 0.033534, loss_ce: 0.010570
2022-01-12 00:10:56,566 iteration 4152 : loss : 0.023719, loss_ce: 0.010741
2022-01-12 00:10:58,062 iteration 4153 : loss : 0.016463, loss_ce: 0.006636
2022-01-12 00:10:59,637 iteration 4154 : loss : 0.014949, loss_ce: 0.005399
2022-01-12 00:11:01,181 iteration 4155 : loss : 0.021299, loss_ce: 0.007080
2022-01-12 00:11:02,685 iteration 4156 : loss : 0.016337, loss_ce: 0.005859
2022-01-12 00:11:04,292 iteration 4157 : loss : 0.025879, loss_ce: 0.013092
2022-01-12 00:11:05,824 iteration 4158 : loss : 0.017914, loss_ce: 0.006925
2022-01-12 00:11:07,360 iteration 4159 : loss : 0.020072, loss_ce: 0.007890
2022-01-12 00:11:09,012 iteration 4160 : loss : 0.030319, loss_ce: 0.009853
2022-01-12 00:11:10,645 iteration 4161 : loss : 0.027604, loss_ce: 0.008652
2022-01-12 00:11:12,281 iteration 4162 : loss : 0.030550, loss_ce: 0.009769
2022-01-12 00:11:13,885 iteration 4163 : loss : 0.028455, loss_ce: 0.014774
2022-01-12 00:11:15,441 iteration 4164 : loss : 0.023521, loss_ce: 0.011085
2022-01-12 00:11:15,442 Training Data Eval:
2022-01-12 00:11:23,441   Average segmentation loss on training set: 0.0138
2022-01-12 00:11:23,441 Validation Data Eval:
2022-01-12 00:11:26,191   Average segmentation loss on validation set: 0.0996
2022-01-12 00:11:27,798 iteration 4165 : loss : 0.022846, loss_ce: 0.006977
 61%|████████████████▌          | 245/400 [1:58:31<1:18:44, 30.48s/it]2022-01-12 00:11:29,409 iteration 4166 : loss : 0.018050, loss_ce: 0.007211
2022-01-12 00:11:30,902 iteration 4167 : loss : 0.020069, loss_ce: 0.006681
2022-01-12 00:11:32,423 iteration 4168 : loss : 0.015414, loss_ce: 0.006174
2022-01-12 00:11:33,945 iteration 4169 : loss : 0.017616, loss_ce: 0.008770
2022-01-12 00:11:35,602 iteration 4170 : loss : 0.026185, loss_ce: 0.011825
2022-01-12 00:11:37,091 iteration 4171 : loss : 0.016503, loss_ce: 0.005216
2022-01-12 00:11:38,648 iteration 4172 : loss : 0.016184, loss_ce: 0.007281
2022-01-12 00:11:40,232 iteration 4173 : loss : 0.019756, loss_ce: 0.005791
2022-01-12 00:11:41,759 iteration 4174 : loss : 0.022142, loss_ce: 0.006209
2022-01-12 00:11:43,271 iteration 4175 : loss : 0.017386, loss_ce: 0.007772
2022-01-12 00:11:44,813 iteration 4176 : loss : 0.019271, loss_ce: 0.008490
2022-01-12 00:11:46,417 iteration 4177 : loss : 0.018010, loss_ce: 0.005090
2022-01-12 00:11:47,931 iteration 4178 : loss : 0.016727, loss_ce: 0.006467
2022-01-12 00:11:49,472 iteration 4179 : loss : 0.020564, loss_ce: 0.009310
2022-01-12 00:11:50,998 iteration 4180 : loss : 0.015701, loss_ce: 0.004880
2022-01-12 00:11:52,544 iteration 4181 : loss : 0.022237, loss_ce: 0.009409
2022-01-12 00:11:54,088 iteration 4182 : loss : 0.019803, loss_ce: 0.005679
 62%|████████████████▌          | 246/400 [1:58:58<1:15:00, 29.23s/it]2022-01-12 00:11:55,701 iteration 4183 : loss : 0.017677, loss_ce: 0.005700
2022-01-12 00:11:57,293 iteration 4184 : loss : 0.038426, loss_ce: 0.018556
2022-01-12 00:11:58,845 iteration 4185 : loss : 0.019897, loss_ce: 0.008277
2022-01-12 00:12:00,375 iteration 4186 : loss : 0.015605, loss_ce: 0.004333
2022-01-12 00:12:01,981 iteration 4187 : loss : 0.021764, loss_ce: 0.006882
2022-01-12 00:12:03,502 iteration 4188 : loss : 0.025206, loss_ce: 0.007299
2022-01-12 00:12:05,161 iteration 4189 : loss : 0.023804, loss_ce: 0.010374
2022-01-12 00:12:06,767 iteration 4190 : loss : 0.030126, loss_ce: 0.013588
2022-01-12 00:12:08,377 iteration 4191 : loss : 0.026856, loss_ce: 0.013067
2022-01-12 00:12:09,850 iteration 4192 : loss : 0.016809, loss_ce: 0.004653
2022-01-12 00:12:11,432 iteration 4193 : loss : 0.023791, loss_ce: 0.012330
2022-01-12 00:12:13,018 iteration 4194 : loss : 0.018768, loss_ce: 0.007001
2022-01-12 00:12:14,550 iteration 4195 : loss : 0.018378, loss_ce: 0.005391
2022-01-12 00:12:16,138 iteration 4196 : loss : 0.018217, loss_ce: 0.007321
2022-01-12 00:12:17,691 iteration 4197 : loss : 0.018555, loss_ce: 0.005098
2022-01-12 00:12:19,245 iteration 4198 : loss : 0.025496, loss_ce: 0.011093
2022-01-12 00:12:20,748 iteration 4199 : loss : 0.016296, loss_ce: 0.006274
 62%|████████████████▋          | 247/400 [1:59:24<1:12:33, 28.46s/it]2022-01-12 00:12:22,339 iteration 4200 : loss : 0.017278, loss_ce: 0.005932
2022-01-12 00:12:23,924 iteration 4201 : loss : 0.018637, loss_ce: 0.008249
2022-01-12 00:12:25,498 iteration 4202 : loss : 0.033016, loss_ce: 0.014603
2022-01-12 00:12:27,138 iteration 4203 : loss : 0.019354, loss_ce: 0.008410
2022-01-12 00:12:28,665 iteration 4204 : loss : 0.021414, loss_ce: 0.008753
2022-01-12 00:12:30,214 iteration 4205 : loss : 0.027734, loss_ce: 0.010577
2022-01-12 00:12:31,883 iteration 4206 : loss : 0.038783, loss_ce: 0.013517
2022-01-12 00:12:33,473 iteration 4207 : loss : 0.035885, loss_ce: 0.011536
2022-01-12 00:12:34,948 iteration 4208 : loss : 0.014637, loss_ce: 0.005513
2022-01-12 00:12:36,477 iteration 4209 : loss : 0.020055, loss_ce: 0.008951
2022-01-12 00:12:38,014 iteration 4210 : loss : 0.024140, loss_ce: 0.011447
2022-01-12 00:12:39,587 iteration 4211 : loss : 0.021561, loss_ce: 0.008536
2022-01-12 00:12:41,194 iteration 4212 : loss : 0.031108, loss_ce: 0.012405
2022-01-12 00:12:42,719 iteration 4213 : loss : 0.025229, loss_ce: 0.006033
2022-01-12 00:12:44,304 iteration 4214 : loss : 0.048372, loss_ce: 0.013077
2022-01-12 00:12:45,854 iteration 4215 : loss : 0.018514, loss_ce: 0.006305
2022-01-12 00:12:47,399 iteration 4216 : loss : 0.023871, loss_ce: 0.007850
 62%|████████████████▋          | 248/400 [1:59:51<1:10:42, 27.91s/it]2022-01-12 00:12:49,033 iteration 4217 : loss : 0.033718, loss_ce: 0.011012
2022-01-12 00:12:50,571 iteration 4218 : loss : 0.019014, loss_ce: 0.006542
2022-01-12 00:12:52,102 iteration 4219 : loss : 0.023262, loss_ce: 0.009420
2022-01-12 00:12:53,582 iteration 4220 : loss : 0.023428, loss_ce: 0.006227
2022-01-12 00:12:55,216 iteration 4221 : loss : 0.024105, loss_ce: 0.011073
2022-01-12 00:12:56,853 iteration 4222 : loss : 0.037520, loss_ce: 0.013175
2022-01-12 00:12:58,377 iteration 4223 : loss : 0.024172, loss_ce: 0.008237
2022-01-12 00:12:59,987 iteration 4224 : loss : 0.030937, loss_ce: 0.008510
2022-01-12 00:13:01,577 iteration 4225 : loss : 0.025920, loss_ce: 0.006735
2022-01-12 00:13:03,182 iteration 4226 : loss : 0.025433, loss_ce: 0.011290
2022-01-12 00:13:04,752 iteration 4227 : loss : 0.022915, loss_ce: 0.010489
2022-01-12 00:13:06,269 iteration 4228 : loss : 0.021860, loss_ce: 0.010424
2022-01-12 00:13:07,821 iteration 4229 : loss : 0.026453, loss_ce: 0.010935
2022-01-12 00:13:09,369 iteration 4230 : loss : 0.029256, loss_ce: 0.013621
2022-01-12 00:13:10,972 iteration 4231 : loss : 0.022796, loss_ce: 0.009580
2022-01-12 00:13:12,577 iteration 4232 : loss : 0.023390, loss_ce: 0.007970
2022-01-12 00:13:14,126 iteration 4233 : loss : 0.024180, loss_ce: 0.013072
 62%|████████████████▊          | 249/400 [2:00:18<1:09:21, 27.56s/it]2022-01-12 00:13:15,638 iteration 4234 : loss : 0.036072, loss_ce: 0.007607
2022-01-12 00:13:17,142 iteration 4235 : loss : 0.014010, loss_ce: 0.005451
2022-01-12 00:13:18,722 iteration 4236 : loss : 0.022127, loss_ce: 0.009518
2022-01-12 00:13:20,242 iteration 4237 : loss : 0.019532, loss_ce: 0.008130
2022-01-12 00:13:21,797 iteration 4238 : loss : 0.023986, loss_ce: 0.011496
2022-01-12 00:13:23,473 iteration 4239 : loss : 0.028600, loss_ce: 0.010489
2022-01-12 00:13:25,035 iteration 4240 : loss : 0.022593, loss_ce: 0.006882
2022-01-12 00:13:26,513 iteration 4241 : loss : 0.020264, loss_ce: 0.010022
2022-01-12 00:13:28,075 iteration 4242 : loss : 0.020986, loss_ce: 0.006272
2022-01-12 00:13:29,619 iteration 4243 : loss : 0.028240, loss_ce: 0.011561
2022-01-12 00:13:31,113 iteration 4244 : loss : 0.015810, loss_ce: 0.007564
2022-01-12 00:13:32,721 iteration 4245 : loss : 0.020582, loss_ce: 0.008708
2022-01-12 00:13:34,358 iteration 4246 : loss : 0.021743, loss_ce: 0.005424
2022-01-12 00:13:35,950 iteration 4247 : loss : 0.021303, loss_ce: 0.008832
2022-01-12 00:13:37,460 iteration 4248 : loss : 0.028381, loss_ce: 0.011420
2022-01-12 00:13:39,079 iteration 4249 : loss : 0.029308, loss_ce: 0.009133
2022-01-12 00:13:39,079 Training Data Eval:
2022-01-12 00:13:47,069   Average segmentation loss on training set: 0.0122
2022-01-12 00:13:47,069 Validation Data Eval:
2022-01-12 00:13:49,815   Average segmentation loss on validation set: 0.0770
2022-01-12 00:13:51,382 iteration 4250 : loss : 0.032252, loss_ce: 0.010810
 62%|████████████████▉          | 250/400 [2:00:55<1:16:09, 30.47s/it]2022-01-12 00:13:53,044 iteration 4251 : loss : 0.026720, loss_ce: 0.007113
2022-01-12 00:13:54,660 iteration 4252 : loss : 0.020007, loss_ce: 0.008584
2022-01-12 00:13:56,229 iteration 4253 : loss : 0.025192, loss_ce: 0.009097
2022-01-12 00:13:57,833 iteration 4254 : loss : 0.020118, loss_ce: 0.008269
2022-01-12 00:13:59,438 iteration 4255 : loss : 0.024832, loss_ce: 0.008527
2022-01-12 00:14:00,909 iteration 4256 : loss : 0.016922, loss_ce: 0.007938
2022-01-12 00:14:02,467 iteration 4257 : loss : 0.023266, loss_ce: 0.009090
2022-01-12 00:14:03,964 iteration 4258 : loss : 0.020607, loss_ce: 0.006866
2022-01-12 00:14:05,547 iteration 4259 : loss : 0.024143, loss_ce: 0.010697
2022-01-12 00:14:07,138 iteration 4260 : loss : 0.020843, loss_ce: 0.006675
2022-01-12 00:14:08,729 iteration 4261 : loss : 0.043694, loss_ce: 0.014340
2022-01-12 00:14:10,249 iteration 4262 : loss : 0.017806, loss_ce: 0.007488
2022-01-12 00:14:11,837 iteration 4263 : loss : 0.018985, loss_ce: 0.008252
2022-01-12 00:14:13,443 iteration 4264 : loss : 0.027752, loss_ce: 0.010947
2022-01-12 00:14:14,992 iteration 4265 : loss : 0.023771, loss_ce: 0.006872
2022-01-12 00:14:16,572 iteration 4266 : loss : 0.023422, loss_ce: 0.007068
2022-01-12 00:14:18,189 iteration 4267 : loss : 0.023123, loss_ce: 0.008984
 63%|████████████████▉          | 251/400 [2:01:22<1:12:55, 29.37s/it]2022-01-12 00:14:19,791 iteration 4268 : loss : 0.015827, loss_ce: 0.003876
2022-01-12 00:14:21,329 iteration 4269 : loss : 0.027010, loss_ce: 0.008577
2022-01-12 00:14:22,863 iteration 4270 : loss : 0.018798, loss_ce: 0.009914
2022-01-12 00:14:24,358 iteration 4271 : loss : 0.015101, loss_ce: 0.006049
2022-01-12 00:14:25,910 iteration 4272 : loss : 0.016941, loss_ce: 0.007790
2022-01-12 00:14:27,413 iteration 4273 : loss : 0.016110, loss_ce: 0.006465
2022-01-12 00:14:29,046 iteration 4274 : loss : 0.020632, loss_ce: 0.009293
2022-01-12 00:14:30,593 iteration 4275 : loss : 0.021164, loss_ce: 0.007490
2022-01-12 00:14:32,193 iteration 4276 : loss : 0.020667, loss_ce: 0.006323
2022-01-12 00:14:33,802 iteration 4277 : loss : 0.018721, loss_ce: 0.005870
2022-01-12 00:14:35,409 iteration 4278 : loss : 0.025110, loss_ce: 0.009270
2022-01-12 00:14:36,960 iteration 4279 : loss : 0.023255, loss_ce: 0.012222
2022-01-12 00:14:38,526 iteration 4280 : loss : 0.028238, loss_ce: 0.008677
2022-01-12 00:14:40,088 iteration 4281 : loss : 0.022850, loss_ce: 0.007302
2022-01-12 00:14:41,585 iteration 4282 : loss : 0.019138, loss_ce: 0.007234
2022-01-12 00:14:43,138 iteration 4283 : loss : 0.026994, loss_ce: 0.007487
2022-01-12 00:14:44,663 iteration 4284 : loss : 0.014932, loss_ce: 0.006003
 63%|█████████████████          | 252/400 [2:01:48<1:10:17, 28.50s/it]2022-01-12 00:14:46,208 iteration 4285 : loss : 0.018732, loss_ce: 0.005760
2022-01-12 00:14:47,803 iteration 4286 : loss : 0.020130, loss_ce: 0.007516
2022-01-12 00:14:49,316 iteration 4287 : loss : 0.019969, loss_ce: 0.009383
2022-01-12 00:14:50,843 iteration 4288 : loss : 0.018571, loss_ce: 0.008604
2022-01-12 00:14:52,407 iteration 4289 : loss : 0.018467, loss_ce: 0.007304
2022-01-12 00:14:53,960 iteration 4290 : loss : 0.033891, loss_ce: 0.009584
2022-01-12 00:14:55,534 iteration 4291 : loss : 0.023069, loss_ce: 0.007608
2022-01-12 00:14:57,105 iteration 4292 : loss : 0.016524, loss_ce: 0.007933
2022-01-12 00:14:58,674 iteration 4293 : loss : 0.023841, loss_ce: 0.007575
2022-01-12 00:15:00,212 iteration 4294 : loss : 0.018898, loss_ce: 0.004451
2022-01-12 00:15:01,859 iteration 4295 : loss : 0.023477, loss_ce: 0.007646
2022-01-12 00:15:03,405 iteration 4296 : loss : 0.016139, loss_ce: 0.005343
2022-01-12 00:15:04,978 iteration 4297 : loss : 0.018994, loss_ce: 0.006830
2022-01-12 00:15:06,520 iteration 4298 : loss : 0.022433, loss_ce: 0.011966
2022-01-12 00:15:08,111 iteration 4299 : loss : 0.017456, loss_ce: 0.007615
2022-01-12 00:15:09,731 iteration 4300 : loss : 0.020740, loss_ce: 0.008143
2022-01-12 00:15:11,352 iteration 4301 : loss : 0.016000, loss_ce: 0.006473
 63%|█████████████████          | 253/400 [2:02:15<1:08:29, 27.96s/it]2022-01-12 00:15:12,981 iteration 4302 : loss : 0.031263, loss_ce: 0.015276
2022-01-12 00:15:14,583 iteration 4303 : loss : 0.042291, loss_ce: 0.006491
2022-01-12 00:15:16,105 iteration 4304 : loss : 0.024587, loss_ce: 0.009567
2022-01-12 00:15:17,630 iteration 4305 : loss : 0.020577, loss_ce: 0.008340
2022-01-12 00:15:19,177 iteration 4306 : loss : 0.021089, loss_ce: 0.007120
2022-01-12 00:15:20,684 iteration 4307 : loss : 0.020302, loss_ce: 0.010931
2022-01-12 00:15:22,275 iteration 4308 : loss : 0.029708, loss_ce: 0.008735
2022-01-12 00:15:23,893 iteration 4309 : loss : 0.031683, loss_ce: 0.013394
2022-01-12 00:15:25,458 iteration 4310 : loss : 0.015349, loss_ce: 0.006132
2022-01-12 00:15:26,978 iteration 4311 : loss : 0.028672, loss_ce: 0.012057
2022-01-12 00:15:28,490 iteration 4312 : loss : 0.022713, loss_ce: 0.007176
2022-01-12 00:15:30,017 iteration 4313 : loss : 0.016350, loss_ce: 0.007878
2022-01-12 00:15:31,549 iteration 4314 : loss : 0.020858, loss_ce: 0.008028
2022-01-12 00:15:33,145 iteration 4315 : loss : 0.028080, loss_ce: 0.009654
2022-01-12 00:15:34,675 iteration 4316 : loss : 0.022004, loss_ce: 0.007345
2022-01-12 00:15:36,292 iteration 4317 : loss : 0.026986, loss_ce: 0.013011
2022-01-12 00:15:37,803 iteration 4318 : loss : 0.019142, loss_ce: 0.007603
 64%|█████████████████▏         | 254/400 [2:02:41<1:06:55, 27.50s/it]2022-01-12 00:15:39,447 iteration 4319 : loss : 0.022689, loss_ce: 0.007262
2022-01-12 00:15:41,051 iteration 4320 : loss : 0.023984, loss_ce: 0.009363
2022-01-12 00:15:42,522 iteration 4321 : loss : 0.016833, loss_ce: 0.005077
2022-01-12 00:15:44,082 iteration 4322 : loss : 0.016240, loss_ce: 0.005347
2022-01-12 00:15:45,585 iteration 4323 : loss : 0.017791, loss_ce: 0.008536
2022-01-12 00:15:47,245 iteration 4324 : loss : 0.021730, loss_ce: 0.009904
2022-01-12 00:15:48,812 iteration 4325 : loss : 0.019422, loss_ce: 0.007970
2022-01-12 00:15:50,356 iteration 4326 : loss : 0.016701, loss_ce: 0.007889
2022-01-12 00:15:51,952 iteration 4327 : loss : 0.017537, loss_ce: 0.004490
2022-01-12 00:15:53,473 iteration 4328 : loss : 0.026776, loss_ce: 0.009293
2022-01-12 00:15:55,095 iteration 4329 : loss : 0.041568, loss_ce: 0.011215
2022-01-12 00:15:56,692 iteration 4330 : loss : 0.018780, loss_ce: 0.008410
2022-01-12 00:15:58,318 iteration 4331 : loss : 0.055852, loss_ce: 0.016172
2022-01-12 00:15:59,924 iteration 4332 : loss : 0.032416, loss_ce: 0.010460
2022-01-12 00:16:01,545 iteration 4333 : loss : 0.027248, loss_ce: 0.011107
2022-01-12 00:16:03,115 iteration 4334 : loss : 0.022125, loss_ce: 0.010487
2022-01-12 00:16:03,115 Training Data Eval:
2022-01-12 00:16:11,096   Average segmentation loss on training set: 0.0138
2022-01-12 00:16:11,097 Validation Data Eval:
2022-01-12 00:16:13,847   Average segmentation loss on validation set: 0.0837
2022-01-12 00:16:15,469 iteration 4335 : loss : 0.031280, loss_ce: 0.008504
 64%|█████████████████▏         | 255/400 [2:03:19<1:13:50, 30.55s/it]2022-01-12 00:16:16,976 iteration 4336 : loss : 0.013537, loss_ce: 0.006947
2022-01-12 00:16:18,619 iteration 4337 : loss : 0.038402, loss_ce: 0.017182
2022-01-12 00:16:20,173 iteration 4338 : loss : 0.023126, loss_ce: 0.008785
2022-01-12 00:16:21,693 iteration 4339 : loss : 0.017384, loss_ce: 0.006385
2022-01-12 00:16:23,291 iteration 4340 : loss : 0.024827, loss_ce: 0.010818
2022-01-12 00:16:24,800 iteration 4341 : loss : 0.017881, loss_ce: 0.005561
2022-01-12 00:16:26,398 iteration 4342 : loss : 0.021582, loss_ce: 0.006091
2022-01-12 00:16:27,997 iteration 4343 : loss : 0.017908, loss_ce: 0.005343
2022-01-12 00:16:29,582 iteration 4344 : loss : 0.025370, loss_ce: 0.012238
2022-01-12 00:16:31,232 iteration 4345 : loss : 0.026141, loss_ce: 0.010927
2022-01-12 00:16:32,730 iteration 4346 : loss : 0.019504, loss_ce: 0.005585
2022-01-12 00:16:34,284 iteration 4347 : loss : 0.015382, loss_ce: 0.007171
2022-01-12 00:16:35,801 iteration 4348 : loss : 0.017020, loss_ce: 0.008220
2022-01-12 00:16:37,277 iteration 4349 : loss : 0.013806, loss_ce: 0.003681
2022-01-12 00:16:38,754 iteration 4350 : loss : 0.021189, loss_ce: 0.007228
2022-01-12 00:16:40,335 iteration 4351 : loss : 0.033004, loss_ce: 0.012668
2022-01-12 00:16:41,906 iteration 4352 : loss : 0.021893, loss_ce: 0.009008
 64%|█████████████████▎         | 256/400 [2:03:46<1:10:21, 29.32s/it]2022-01-12 00:16:43,456 iteration 4353 : loss : 0.017630, loss_ce: 0.006018
2022-01-12 00:16:45,082 iteration 4354 : loss : 0.024390, loss_ce: 0.012028
2022-01-12 00:16:46,683 iteration 4355 : loss : 0.014948, loss_ce: 0.006185
2022-01-12 00:16:48,330 iteration 4356 : loss : 0.028589, loss_ce: 0.006797
2022-01-12 00:16:49,923 iteration 4357 : loss : 0.017984, loss_ce: 0.008216
2022-01-12 00:16:51,530 iteration 4358 : loss : 0.029909, loss_ce: 0.011456
2022-01-12 00:16:53,106 iteration 4359 : loss : 0.024055, loss_ce: 0.008825
2022-01-12 00:16:54,660 iteration 4360 : loss : 0.027765, loss_ce: 0.014719
2022-01-12 00:16:56,277 iteration 4361 : loss : 0.025132, loss_ce: 0.008598
2022-01-12 00:16:57,835 iteration 4362 : loss : 0.016729, loss_ce: 0.007177
2022-01-12 00:16:59,396 iteration 4363 : loss : 0.016046, loss_ce: 0.006997
2022-01-12 00:17:00,997 iteration 4364 : loss : 0.018146, loss_ce: 0.007146
2022-01-12 00:17:02,647 iteration 4365 : loss : 0.031305, loss_ce: 0.013164
2022-01-12 00:17:04,147 iteration 4366 : loss : 0.015672, loss_ce: 0.004502
2022-01-12 00:17:05,616 iteration 4367 : loss : 0.018900, loss_ce: 0.008788
2022-01-12 00:17:07,117 iteration 4368 : loss : 0.021242, loss_ce: 0.007102
2022-01-12 00:17:08,686 iteration 4369 : loss : 0.019228, loss_ce: 0.006925
 64%|█████████████████▎         | 257/400 [2:04:12<1:08:03, 28.56s/it]2022-01-12 00:17:10,297 iteration 4370 : loss : 0.017222, loss_ce: 0.005726
2022-01-12 00:17:11,877 iteration 4371 : loss : 0.020921, loss_ce: 0.010175
2022-01-12 00:17:13,473 iteration 4372 : loss : 0.027680, loss_ce: 0.009430
2022-01-12 00:17:15,055 iteration 4373 : loss : 0.021162, loss_ce: 0.011066
2022-01-12 00:17:16,641 iteration 4374 : loss : 0.022657, loss_ce: 0.008151
2022-01-12 00:17:18,224 iteration 4375 : loss : 0.027343, loss_ce: 0.014273
2022-01-12 00:17:19,757 iteration 4376 : loss : 0.023775, loss_ce: 0.008805
2022-01-12 00:17:21,292 iteration 4377 : loss : 0.022469, loss_ce: 0.007262
2022-01-12 00:17:22,779 iteration 4378 : loss : 0.015844, loss_ce: 0.005853
2022-01-12 00:17:24,291 iteration 4379 : loss : 0.016923, loss_ce: 0.005807
2022-01-12 00:17:25,776 iteration 4380 : loss : 0.015651, loss_ce: 0.005131
2022-01-12 00:17:27,352 iteration 4381 : loss : 0.030900, loss_ce: 0.012409
2022-01-12 00:17:28,912 iteration 4382 : loss : 0.048588, loss_ce: 0.010462
2022-01-12 00:17:30,448 iteration 4383 : loss : 0.018220, loss_ce: 0.005455
2022-01-12 00:17:32,060 iteration 4384 : loss : 0.021343, loss_ce: 0.008989
2022-01-12 00:17:33,672 iteration 4385 : loss : 0.031420, loss_ce: 0.011210
2022-01-12 00:17:35,188 iteration 4386 : loss : 0.018498, loss_ce: 0.008263
 64%|█████████████████▍         | 258/400 [2:04:39<1:06:07, 27.94s/it]2022-01-12 00:17:36,715 iteration 4387 : loss : 0.025225, loss_ce: 0.006820
2022-01-12 00:17:38,277 iteration 4388 : loss : 0.032459, loss_ce: 0.013831
2022-01-12 00:17:39,853 iteration 4389 : loss : 0.052399, loss_ce: 0.016000
2022-01-12 00:17:41,365 iteration 4390 : loss : 0.016980, loss_ce: 0.005687
2022-01-12 00:17:42,939 iteration 4391 : loss : 0.020019, loss_ce: 0.006333
2022-01-12 00:17:44,487 iteration 4392 : loss : 0.021715, loss_ce: 0.006101
2022-01-12 00:17:46,102 iteration 4393 : loss : 0.021952, loss_ce: 0.009696
2022-01-12 00:17:47,684 iteration 4394 : loss : 0.029230, loss_ce: 0.012045
2022-01-12 00:17:49,322 iteration 4395 : loss : 0.032246, loss_ce: 0.010481
2022-01-12 00:17:50,834 iteration 4396 : loss : 0.027169, loss_ce: 0.007661
2022-01-12 00:17:52,498 iteration 4397 : loss : 0.023195, loss_ce: 0.011113
2022-01-12 00:17:54,009 iteration 4398 : loss : 0.019611, loss_ce: 0.008085
2022-01-12 00:17:55,613 iteration 4399 : loss : 0.023076, loss_ce: 0.009012
2022-01-12 00:17:57,181 iteration 4400 : loss : 0.031467, loss_ce: 0.008951
2022-01-12 00:17:58,862 iteration 4401 : loss : 0.028010, loss_ce: 0.010952
2022-01-12 00:18:00,420 iteration 4402 : loss : 0.047198, loss_ce: 0.016529
2022-01-12 00:18:01,999 iteration 4403 : loss : 0.028171, loss_ce: 0.011082
 65%|█████████████████▍         | 259/400 [2:05:06<1:04:51, 27.60s/it]2022-01-12 00:18:03,561 iteration 4404 : loss : 0.028733, loss_ce: 0.010841
2022-01-12 00:18:05,150 iteration 4405 : loss : 0.019896, loss_ce: 0.009022
2022-01-12 00:18:06,732 iteration 4406 : loss : 0.023342, loss_ce: 0.011426
2022-01-12 00:18:08,348 iteration 4407 : loss : 0.027286, loss_ce: 0.009254
2022-01-12 00:18:09,885 iteration 4408 : loss : 0.016547, loss_ce: 0.006965
2022-01-12 00:18:11,472 iteration 4409 : loss : 0.024391, loss_ce: 0.009305
2022-01-12 00:18:13,157 iteration 4410 : loss : 0.032585, loss_ce: 0.012384
2022-01-12 00:18:14,745 iteration 4411 : loss : 0.020872, loss_ce: 0.008469
2022-01-12 00:18:16,298 iteration 4412 : loss : 0.025749, loss_ce: 0.009277
2022-01-12 00:18:17,880 iteration 4413 : loss : 0.017534, loss_ce: 0.005515
2022-01-12 00:18:19,397 iteration 4414 : loss : 0.018815, loss_ce: 0.005691
2022-01-12 00:18:20,996 iteration 4415 : loss : 0.019144, loss_ce: 0.006539
2022-01-12 00:18:22,600 iteration 4416 : loss : 0.032294, loss_ce: 0.010293
2022-01-12 00:18:24,147 iteration 4417 : loss : 0.021950, loss_ce: 0.007012
2022-01-12 00:18:25,705 iteration 4418 : loss : 0.027805, loss_ce: 0.013914
2022-01-12 00:18:27,309 iteration 4419 : loss : 0.018638, loss_ce: 0.005714
2022-01-12 00:18:27,309 Training Data Eval:
2022-01-12 00:18:35,299   Average segmentation loss on training set: 0.0132
2022-01-12 00:18:35,299 Validation Data Eval:
2022-01-12 00:18:38,051   Average segmentation loss on validation set: 0.0793
2022-01-12 00:18:39,572 iteration 4420 : loss : 0.017692, loss_ce: 0.008897
 65%|█████████████████▌         | 260/400 [2:05:43<1:11:23, 30.59s/it]2022-01-12 00:18:41,156 iteration 4421 : loss : 0.019966, loss_ce: 0.006782
2022-01-12 00:18:42,736 iteration 4422 : loss : 0.025655, loss_ce: 0.010598
2022-01-12 00:18:44,358 iteration 4423 : loss : 0.022067, loss_ce: 0.008498
2022-01-12 00:18:45,870 iteration 4424 : loss : 0.019559, loss_ce: 0.006179
2022-01-12 00:18:47,369 iteration 4425 : loss : 0.019086, loss_ce: 0.006456
2022-01-12 00:18:48,986 iteration 4426 : loss : 0.015636, loss_ce: 0.006454
2022-01-12 00:18:50,486 iteration 4427 : loss : 0.017112, loss_ce: 0.007861
2022-01-12 00:18:51,949 iteration 4428 : loss : 0.014167, loss_ce: 0.006483
2022-01-12 00:18:53,441 iteration 4429 : loss : 0.021258, loss_ce: 0.006655
2022-01-12 00:18:54,933 iteration 4430 : loss : 0.015111, loss_ce: 0.005972
2022-01-12 00:18:56,511 iteration 4431 : loss : 0.030080, loss_ce: 0.012343
2022-01-12 00:18:58,103 iteration 4432 : loss : 0.024315, loss_ce: 0.009321
2022-01-12 00:18:59,626 iteration 4433 : loss : 0.025104, loss_ce: 0.005658
2022-01-12 00:19:01,117 iteration 4434 : loss : 0.012673, loss_ce: 0.005222
2022-01-12 00:19:02,669 iteration 4435 : loss : 0.020410, loss_ce: 0.004353
2022-01-12 00:19:04,234 iteration 4436 : loss : 0.018931, loss_ce: 0.007736
2022-01-12 00:19:05,855 iteration 4437 : loss : 0.025083, loss_ce: 0.010064
 65%|█████████████████▌         | 261/400 [2:06:09<1:07:52, 29.30s/it]2022-01-12 00:19:07,507 iteration 4438 : loss : 0.042486, loss_ce: 0.011910
2022-01-12 00:19:09,091 iteration 4439 : loss : 0.019198, loss_ce: 0.007052
2022-01-12 00:19:10,659 iteration 4440 : loss : 0.027254, loss_ce: 0.011340
2022-01-12 00:19:12,326 iteration 4441 : loss : 0.025248, loss_ce: 0.009337
2022-01-12 00:19:13,929 iteration 4442 : loss : 0.021799, loss_ce: 0.011935
2022-01-12 00:19:15,509 iteration 4443 : loss : 0.033707, loss_ce: 0.013924
2022-01-12 00:19:17,008 iteration 4444 : loss : 0.022222, loss_ce: 0.007604
2022-01-12 00:19:18,567 iteration 4445 : loss : 0.033717, loss_ce: 0.014957
2022-01-12 00:19:20,162 iteration 4446 : loss : 0.023450, loss_ce: 0.010982
2022-01-12 00:19:21,695 iteration 4447 : loss : 0.018855, loss_ce: 0.006773
2022-01-12 00:19:23,279 iteration 4448 : loss : 0.028961, loss_ce: 0.012144
2022-01-12 00:19:24,857 iteration 4449 : loss : 0.022936, loss_ce: 0.009783
2022-01-12 00:19:26,475 iteration 4450 : loss : 0.029732, loss_ce: 0.009260
2022-01-12 00:19:28,014 iteration 4451 : loss : 0.022127, loss_ce: 0.008010
2022-01-12 00:19:29,598 iteration 4452 : loss : 0.018391, loss_ce: 0.008110
2022-01-12 00:19:31,135 iteration 4453 : loss : 0.016574, loss_ce: 0.005488
2022-01-12 00:19:32,664 iteration 4454 : loss : 0.023500, loss_ce: 0.010222
 66%|█████████████████▋         | 262/400 [2:06:36<1:05:40, 28.55s/it]2022-01-12 00:19:34,278 iteration 4455 : loss : 0.020649, loss_ce: 0.005804
2022-01-12 00:19:35,760 iteration 4456 : loss : 0.015222, loss_ce: 0.005647
2022-01-12 00:19:37,349 iteration 4457 : loss : 0.023297, loss_ce: 0.010446
2022-01-12 00:19:39,008 iteration 4458 : loss : 0.032367, loss_ce: 0.015153
2022-01-12 00:19:40,716 iteration 4459 : loss : 0.025149, loss_ce: 0.010811
2022-01-12 00:19:42,226 iteration 4460 : loss : 0.022178, loss_ce: 0.008212
2022-01-12 00:19:43,801 iteration 4461 : loss : 0.024220, loss_ce: 0.007841
2022-01-12 00:19:45,408 iteration 4462 : loss : 0.023477, loss_ce: 0.010340
2022-01-12 00:19:46,945 iteration 4463 : loss : 0.030251, loss_ce: 0.011278
2022-01-12 00:19:48,474 iteration 4464 : loss : 0.018850, loss_ce: 0.006840
2022-01-12 00:19:49,971 iteration 4465 : loss : 0.018668, loss_ce: 0.004285
2022-01-12 00:19:51,488 iteration 4466 : loss : 0.024072, loss_ce: 0.007567
2022-01-12 00:19:53,055 iteration 4467 : loss : 0.041524, loss_ce: 0.019123
2022-01-12 00:19:54,642 iteration 4468 : loss : 0.027662, loss_ce: 0.012832
2022-01-12 00:19:56,264 iteration 4469 : loss : 0.026771, loss_ce: 0.014595
2022-01-12 00:19:57,868 iteration 4470 : loss : 0.018888, loss_ce: 0.007508
2022-01-12 00:19:59,504 iteration 4471 : loss : 0.051764, loss_ce: 0.026991
 66%|█████████████████▊         | 263/400 [2:07:03<1:04:00, 28.04s/it]2022-01-12 00:20:01,070 iteration 4472 : loss : 0.021809, loss_ce: 0.011624
2022-01-12 00:20:02,592 iteration 4473 : loss : 0.019198, loss_ce: 0.008314
2022-01-12 00:20:04,156 iteration 4474 : loss : 0.021659, loss_ce: 0.010231
2022-01-12 00:20:05,716 iteration 4475 : loss : 0.018921, loss_ce: 0.005661
2022-01-12 00:20:07,224 iteration 4476 : loss : 0.022888, loss_ce: 0.009202
2022-01-12 00:20:08,748 iteration 4477 : loss : 0.016599, loss_ce: 0.008025
2022-01-12 00:20:10,406 iteration 4478 : loss : 0.029180, loss_ce: 0.014824
2022-01-12 00:20:11,957 iteration 4479 : loss : 0.027504, loss_ce: 0.007715
2022-01-12 00:20:13,528 iteration 4480 : loss : 0.017390, loss_ce: 0.006114
2022-01-12 00:20:15,056 iteration 4481 : loss : 0.021274, loss_ce: 0.008937
2022-01-12 00:20:16,570 iteration 4482 : loss : 0.016348, loss_ce: 0.004798
2022-01-12 00:20:18,094 iteration 4483 : loss : 0.023121, loss_ce: 0.008074
2022-01-12 00:20:19,650 iteration 4484 : loss : 0.012813, loss_ce: 0.004362
2022-01-12 00:20:21,134 iteration 4485 : loss : 0.015389, loss_ce: 0.006219
2022-01-12 00:20:22,750 iteration 4486 : loss : 0.016215, loss_ce: 0.006392
2022-01-12 00:20:24,324 iteration 4487 : loss : 0.018933, loss_ce: 0.008930
2022-01-12 00:20:25,838 iteration 4488 : loss : 0.026190, loss_ce: 0.009433
 66%|█████████████████▊         | 264/400 [2:07:29<1:02:23, 27.53s/it]2022-01-12 00:20:27,469 iteration 4489 : loss : 0.025905, loss_ce: 0.012303
2022-01-12 00:20:29,024 iteration 4490 : loss : 0.021049, loss_ce: 0.008574
2022-01-12 00:20:30,671 iteration 4491 : loss : 0.048311, loss_ce: 0.016408
2022-01-12 00:20:32,187 iteration 4492 : loss : 0.019831, loss_ce: 0.004990
2022-01-12 00:20:33,820 iteration 4493 : loss : 0.024256, loss_ce: 0.014454
2022-01-12 00:20:35,345 iteration 4494 : loss : 0.017927, loss_ce: 0.005893
2022-01-12 00:20:36,906 iteration 4495 : loss : 0.030942, loss_ce: 0.008472
2022-01-12 00:20:38,472 iteration 4496 : loss : 0.024332, loss_ce: 0.009798
2022-01-12 00:20:39,932 iteration 4497 : loss : 0.020639, loss_ce: 0.009372
2022-01-12 00:20:41,485 iteration 4498 : loss : 0.018305, loss_ce: 0.005605
2022-01-12 00:20:43,017 iteration 4499 : loss : 0.024989, loss_ce: 0.010003
2022-01-12 00:20:44,580 iteration 4500 : loss : 0.026664, loss_ce: 0.008522
2022-01-12 00:20:46,101 iteration 4501 : loss : 0.017280, loss_ce: 0.007570
2022-01-12 00:20:47,643 iteration 4502 : loss : 0.019009, loss_ce: 0.006085
2022-01-12 00:20:49,285 iteration 4503 : loss : 0.018093, loss_ce: 0.006751
2022-01-12 00:20:50,742 iteration 4504 : loss : 0.020385, loss_ce: 0.008333
2022-01-12 00:20:50,743 Training Data Eval:
2022-01-12 00:20:58,737   Average segmentation loss on training set: 0.0128
2022-01-12 00:20:58,737 Validation Data Eval:
2022-01-12 00:21:01,496   Average segmentation loss on validation set: 0.0666
2022-01-12 00:21:03,073 iteration 4505 : loss : 0.022196, loss_ce: 0.010271
 66%|█████████████████▉         | 265/400 [2:08:07<1:08:29, 30.44s/it]2022-01-12 00:21:04,691 iteration 4506 : loss : 0.020631, loss_ce: 0.007787
2022-01-12 00:21:06,306 iteration 4507 : loss : 0.022874, loss_ce: 0.009238
2022-01-12 00:21:07,860 iteration 4508 : loss : 0.023749, loss_ce: 0.009285
2022-01-12 00:21:09,367 iteration 4509 : loss : 0.016730, loss_ce: 0.005731
2022-01-12 00:21:10,923 iteration 4510 : loss : 0.018912, loss_ce: 0.006515
2022-01-12 00:21:12,572 iteration 4511 : loss : 0.025158, loss_ce: 0.010564
2022-01-12 00:21:14,051 iteration 4512 : loss : 0.015761, loss_ce: 0.005400
2022-01-12 00:21:15,579 iteration 4513 : loss : 0.017677, loss_ce: 0.006033
2022-01-12 00:21:17,102 iteration 4514 : loss : 0.020567, loss_ce: 0.008142
2022-01-12 00:21:18,643 iteration 4515 : loss : 0.014829, loss_ce: 0.005682
2022-01-12 00:21:20,209 iteration 4516 : loss : 0.019777, loss_ce: 0.007729
2022-01-12 00:21:21,798 iteration 4517 : loss : 0.018201, loss_ce: 0.006694
2022-01-12 00:21:23,334 iteration 4518 : loss : 0.019375, loss_ce: 0.008214
2022-01-12 00:21:24,818 iteration 4519 : loss : 0.031171, loss_ce: 0.010940
2022-01-12 00:21:26,375 iteration 4520 : loss : 0.019753, loss_ce: 0.005645
2022-01-12 00:21:27,950 iteration 4521 : loss : 0.021698, loss_ce: 0.008431
2022-01-12 00:21:29,550 iteration 4522 : loss : 0.031827, loss_ce: 0.010391
 66%|█████████████████▉         | 266/400 [2:08:33<1:05:19, 29.25s/it]2022-01-12 00:21:31,199 iteration 4523 : loss : 0.027282, loss_ce: 0.012343
2022-01-12 00:21:32,762 iteration 4524 : loss : 0.020219, loss_ce: 0.007964
2022-01-12 00:21:34,344 iteration 4525 : loss : 0.026088, loss_ce: 0.012065
2022-01-12 00:21:35,903 iteration 4526 : loss : 0.019470, loss_ce: 0.005872
2022-01-12 00:21:37,397 iteration 4527 : loss : 0.022666, loss_ce: 0.008069
2022-01-12 00:21:38,921 iteration 4528 : loss : 0.021161, loss_ce: 0.008178
2022-01-12 00:21:40,491 iteration 4529 : loss : 0.018379, loss_ce: 0.007469
2022-01-12 00:21:42,017 iteration 4530 : loss : 0.023739, loss_ce: 0.005775
2022-01-12 00:21:43,582 iteration 4531 : loss : 0.017461, loss_ce: 0.006161
2022-01-12 00:21:45,164 iteration 4532 : loss : 0.024499, loss_ce: 0.008817
2022-01-12 00:21:46,684 iteration 4533 : loss : 0.019281, loss_ce: 0.008411
2022-01-12 00:21:48,213 iteration 4534 : loss : 0.029904, loss_ce: 0.007847
2022-01-12 00:21:49,792 iteration 4535 : loss : 0.016904, loss_ce: 0.006627
2022-01-12 00:21:51,367 iteration 4536 : loss : 0.018272, loss_ce: 0.006863
2022-01-12 00:21:52,916 iteration 4537 : loss : 0.028332, loss_ce: 0.011021
2022-01-12 00:21:54,459 iteration 4538 : loss : 0.022435, loss_ce: 0.008023
2022-01-12 00:21:56,007 iteration 4539 : loss : 0.017318, loss_ce: 0.006868
 67%|██████████████████         | 267/400 [2:09:00<1:02:59, 28.41s/it]2022-01-12 00:21:57,569 iteration 4540 : loss : 0.013359, loss_ce: 0.004783
2022-01-12 00:21:59,147 iteration 4541 : loss : 0.025496, loss_ce: 0.011463
2022-01-12 00:22:00,663 iteration 4542 : loss : 0.028300, loss_ce: 0.011202
2022-01-12 00:22:02,205 iteration 4543 : loss : 0.018249, loss_ce: 0.006241
2022-01-12 00:22:03,716 iteration 4544 : loss : 0.022119, loss_ce: 0.007253
2022-01-12 00:22:05,322 iteration 4545 : loss : 0.023836, loss_ce: 0.010978
2022-01-12 00:22:06,971 iteration 4546 : loss : 0.026927, loss_ce: 0.011494
2022-01-12 00:22:08,607 iteration 4547 : loss : 0.021693, loss_ce: 0.008871
2022-01-12 00:22:10,092 iteration 4548 : loss : 0.017154, loss_ce: 0.006095
2022-01-12 00:22:11,599 iteration 4549 : loss : 0.024032, loss_ce: 0.006097
2022-01-12 00:22:13,159 iteration 4550 : loss : 0.037316, loss_ce: 0.019948
2022-01-12 00:22:14,746 iteration 4551 : loss : 0.025991, loss_ce: 0.010904
2022-01-12 00:22:16,376 iteration 4552 : loss : 0.031562, loss_ce: 0.009031
2022-01-12 00:22:17,948 iteration 4553 : loss : 0.037871, loss_ce: 0.012688
2022-01-12 00:22:19,465 iteration 4554 : loss : 0.019508, loss_ce: 0.008660
2022-01-12 00:22:21,012 iteration 4555 : loss : 0.025244, loss_ce: 0.010856
2022-01-12 00:22:22,560 iteration 4556 : loss : 0.026794, loss_ce: 0.007123
 67%|██████████████████         | 268/400 [2:09:26<1:01:16, 27.85s/it]2022-01-12 00:22:24,121 iteration 4557 : loss : 0.016857, loss_ce: 0.006807
2022-01-12 00:22:25,702 iteration 4558 : loss : 0.022552, loss_ce: 0.007385
2022-01-12 00:22:27,312 iteration 4559 : loss : 0.031897, loss_ce: 0.010019
2022-01-12 00:22:28,885 iteration 4560 : loss : 0.025519, loss_ce: 0.013409
2022-01-12 00:22:30,389 iteration 4561 : loss : 0.016664, loss_ce: 0.005506
2022-01-12 00:22:31,951 iteration 4562 : loss : 0.023157, loss_ce: 0.009153
2022-01-12 00:22:33,493 iteration 4563 : loss : 0.025597, loss_ce: 0.008772
2022-01-12 00:22:35,022 iteration 4564 : loss : 0.017177, loss_ce: 0.007201
2022-01-12 00:22:36,569 iteration 4565 : loss : 0.027063, loss_ce: 0.013208
2022-01-12 00:22:38,131 iteration 4566 : loss : 0.023530, loss_ce: 0.009316
2022-01-12 00:22:39,682 iteration 4567 : loss : 0.027314, loss_ce: 0.010364
2022-01-12 00:22:41,172 iteration 4568 : loss : 0.017198, loss_ce: 0.005543
2022-01-12 00:22:42,673 iteration 4569 : loss : 0.029748, loss_ce: 0.012709
2022-01-12 00:22:44,190 iteration 4570 : loss : 0.020534, loss_ce: 0.007995
2022-01-12 00:22:45,792 iteration 4571 : loss : 0.023042, loss_ce: 0.010223
2022-01-12 00:22:47,384 iteration 4572 : loss : 0.027015, loss_ce: 0.009920
2022-01-12 00:22:48,990 iteration 4573 : loss : 0.028412, loss_ce: 0.011072
 67%|███████████████████▌         | 269/400 [2:09:53<59:52, 27.43s/it]2022-01-12 00:22:50,532 iteration 4574 : loss : 0.013579, loss_ce: 0.004304
2022-01-12 00:22:52,111 iteration 4575 : loss : 0.014881, loss_ce: 0.003944
2022-01-12 00:22:53,603 iteration 4576 : loss : 0.017121, loss_ce: 0.007387
2022-01-12 00:22:55,113 iteration 4577 : loss : 0.015084, loss_ce: 0.006819
2022-01-12 00:22:56,659 iteration 4578 : loss : 0.018601, loss_ce: 0.007608
2022-01-12 00:22:58,179 iteration 4579 : loss : 0.024984, loss_ce: 0.014932
2022-01-12 00:22:59,706 iteration 4580 : loss : 0.016733, loss_ce: 0.006073
2022-01-12 00:23:01,264 iteration 4581 : loss : 0.024147, loss_ce: 0.010545
2022-01-12 00:23:02,822 iteration 4582 : loss : 0.023476, loss_ce: 0.008778
2022-01-12 00:23:04,363 iteration 4583 : loss : 0.019233, loss_ce: 0.007152
2022-01-12 00:23:05,893 iteration 4584 : loss : 0.015732, loss_ce: 0.005841
2022-01-12 00:23:07,460 iteration 4585 : loss : 0.033847, loss_ce: 0.011913
2022-01-12 00:23:08,969 iteration 4586 : loss : 0.017694, loss_ce: 0.007541
2022-01-12 00:23:10,487 iteration 4587 : loss : 0.017516, loss_ce: 0.005611
2022-01-12 00:23:12,012 iteration 4588 : loss : 0.019024, loss_ce: 0.006186
2022-01-12 00:23:13,574 iteration 4589 : loss : 0.022468, loss_ce: 0.008102
2022-01-12 00:23:13,575 Training Data Eval:
2022-01-12 00:23:21,570   Average segmentation loss on training set: 0.0128
2022-01-12 00:23:21,571 Validation Data Eval:
2022-01-12 00:23:24,328   Average segmentation loss on validation set: 0.0735
2022-01-12 00:23:25,880 iteration 4590 : loss : 0.019286, loss_ce: 0.008920
 68%|██████████████████▏        | 270/400 [2:10:30<1:05:34, 30.27s/it]2022-01-12 00:23:27,480 iteration 4591 : loss : 0.028165, loss_ce: 0.007593
2022-01-12 00:23:29,095 iteration 4592 : loss : 0.033457, loss_ce: 0.019284
2022-01-12 00:23:30,677 iteration 4593 : loss : 0.022804, loss_ce: 0.010846
2022-01-12 00:23:32,303 iteration 4594 : loss : 0.042338, loss_ce: 0.013751
2022-01-12 00:23:33,845 iteration 4595 : loss : 0.016670, loss_ce: 0.005839
2022-01-12 00:23:35,376 iteration 4596 : loss : 0.027407, loss_ce: 0.008714
2022-01-12 00:23:36,947 iteration 4597 : loss : 0.016663, loss_ce: 0.007468
2022-01-12 00:23:38,565 iteration 4598 : loss : 0.025889, loss_ce: 0.008614
2022-01-12 00:23:40,153 iteration 4599 : loss : 0.019455, loss_ce: 0.009032
2022-01-12 00:23:41,684 iteration 4600 : loss : 0.024749, loss_ce: 0.012160
2022-01-12 00:23:43,256 iteration 4601 : loss : 0.023904, loss_ce: 0.010945
2022-01-12 00:23:44,902 iteration 4602 : loss : 0.025993, loss_ce: 0.006637
2022-01-12 00:23:46,465 iteration 4603 : loss : 0.025989, loss_ce: 0.008345
2022-01-12 00:23:48,049 iteration 4604 : loss : 0.020541, loss_ce: 0.009500
2022-01-12 00:23:49,656 iteration 4605 : loss : 0.034263, loss_ce: 0.005570
2022-01-12 00:23:51,246 iteration 4606 : loss : 0.019167, loss_ce: 0.007282
2022-01-12 00:23:52,850 iteration 4607 : loss : 0.018797, loss_ce: 0.010893
 68%|██████████████████▎        | 271/400 [2:10:56<1:02:56, 29.28s/it]2022-01-12 00:23:54,401 iteration 4608 : loss : 0.017204, loss_ce: 0.005935
2022-01-12 00:23:55,968 iteration 4609 : loss : 0.028686, loss_ce: 0.011076
2022-01-12 00:23:57,486 iteration 4610 : loss : 0.029880, loss_ce: 0.008718
2022-01-12 00:23:58,998 iteration 4611 : loss : 0.027362, loss_ce: 0.011776
2022-01-12 00:24:00,475 iteration 4612 : loss : 0.016450, loss_ce: 0.005231
2022-01-12 00:24:02,034 iteration 4613 : loss : 0.022046, loss_ce: 0.006917
2022-01-12 00:24:03,578 iteration 4614 : loss : 0.017118, loss_ce: 0.008407
2022-01-12 00:24:05,169 iteration 4615 : loss : 0.019101, loss_ce: 0.007079
2022-01-12 00:24:06,824 iteration 4616 : loss : 0.023776, loss_ce: 0.011773
2022-01-12 00:24:08,390 iteration 4617 : loss : 0.028349, loss_ce: 0.011786
2022-01-12 00:24:09,890 iteration 4618 : loss : 0.026118, loss_ce: 0.010495
2022-01-12 00:24:11,411 iteration 4619 : loss : 0.013444, loss_ce: 0.005760
2022-01-12 00:24:13,022 iteration 4620 : loss : 0.016772, loss_ce: 0.004870
2022-01-12 00:24:14,562 iteration 4621 : loss : 0.024244, loss_ce: 0.013284
2022-01-12 00:24:16,116 iteration 4622 : loss : 0.039618, loss_ce: 0.016849
2022-01-12 00:24:17,670 iteration 4623 : loss : 0.020083, loss_ce: 0.006237
2022-01-12 00:24:19,239 iteration 4624 : loss : 0.017093, loss_ce: 0.008037
 68%|██████████████████▎        | 272/400 [2:11:23<1:00:36, 28.41s/it]2022-01-12 00:24:20,861 iteration 4625 : loss : 0.015438, loss_ce: 0.005064
2022-01-12 00:24:22,463 iteration 4626 : loss : 0.020861, loss_ce: 0.009131
2022-01-12 00:24:24,105 iteration 4627 : loss : 0.021860, loss_ce: 0.007946
2022-01-12 00:24:25,637 iteration 4628 : loss : 0.017994, loss_ce: 0.006576
2022-01-12 00:24:27,144 iteration 4629 : loss : 0.017026, loss_ce: 0.006110
2022-01-12 00:24:28,701 iteration 4630 : loss : 0.019291, loss_ce: 0.008094
2022-01-12 00:24:30,256 iteration 4631 : loss : 0.026090, loss_ce: 0.008869
2022-01-12 00:24:31,813 iteration 4632 : loss : 0.017465, loss_ce: 0.006140
2022-01-12 00:24:33,362 iteration 4633 : loss : 0.015294, loss_ce: 0.005282
2022-01-12 00:24:34,869 iteration 4634 : loss : 0.014177, loss_ce: 0.006409
2022-01-12 00:24:36,351 iteration 4635 : loss : 0.012368, loss_ce: 0.005056
2022-01-12 00:24:37,986 iteration 4636 : loss : 0.022276, loss_ce: 0.008635
2022-01-12 00:24:39,522 iteration 4637 : loss : 0.023248, loss_ce: 0.005866
2022-01-12 00:24:41,192 iteration 4638 : loss : 0.029251, loss_ce: 0.012192
2022-01-12 00:24:42,787 iteration 4639 : loss : 0.024100, loss_ce: 0.008686
2022-01-12 00:24:44,454 iteration 4640 : loss : 0.033383, loss_ce: 0.013063
2022-01-12 00:24:46,051 iteration 4641 : loss : 0.020200, loss_ce: 0.007675
 68%|███████████████████▊         | 273/400 [2:11:50<59:07, 27.93s/it]2022-01-12 00:24:47,632 iteration 4642 : loss : 0.030806, loss_ce: 0.006108
2022-01-12 00:24:49,176 iteration 4643 : loss : 0.020019, loss_ce: 0.007474
2022-01-12 00:24:50,784 iteration 4644 : loss : 0.018505, loss_ce: 0.005998
2022-01-12 00:24:52,392 iteration 4645 : loss : 0.027509, loss_ce: 0.013327
2022-01-12 00:24:53,972 iteration 4646 : loss : 0.024537, loss_ce: 0.006792
2022-01-12 00:24:55,463 iteration 4647 : loss : 0.017704, loss_ce: 0.005920
2022-01-12 00:24:56,997 iteration 4648 : loss : 0.036246, loss_ce: 0.013544
2022-01-12 00:24:58,571 iteration 4649 : loss : 0.017992, loss_ce: 0.007643
2022-01-12 00:25:00,179 iteration 4650 : loss : 0.035598, loss_ce: 0.011055
2022-01-12 00:25:01,691 iteration 4651 : loss : 0.022691, loss_ce: 0.011743
2022-01-12 00:25:03,316 iteration 4652 : loss : 0.021803, loss_ce: 0.009039
2022-01-12 00:25:04,842 iteration 4653 : loss : 0.018538, loss_ce: 0.007456
2022-01-12 00:25:06,388 iteration 4654 : loss : 0.025436, loss_ce: 0.006515
2022-01-12 00:25:08,008 iteration 4655 : loss : 0.030034, loss_ce: 0.011890
2022-01-12 00:25:09,641 iteration 4656 : loss : 0.021219, loss_ce: 0.009408
2022-01-12 00:25:11,238 iteration 4657 : loss : 0.024481, loss_ce: 0.011898
2022-01-12 00:25:12,831 iteration 4658 : loss : 0.035028, loss_ce: 0.016462
 68%|███████████████████▊         | 274/400 [2:12:16<57:55, 27.58s/it]2022-01-12 00:25:14,444 iteration 4659 : loss : 0.022405, loss_ce: 0.009877
2022-01-12 00:25:16,048 iteration 4660 : loss : 0.036107, loss_ce: 0.014832
2022-01-12 00:25:17,645 iteration 4661 : loss : 0.039978, loss_ce: 0.015166
2022-01-12 00:25:19,166 iteration 4662 : loss : 0.014950, loss_ce: 0.005618
2022-01-12 00:25:20,761 iteration 4663 : loss : 0.020935, loss_ce: 0.008346
2022-01-12 00:25:22,312 iteration 4664 : loss : 0.013488, loss_ce: 0.005552
2022-01-12 00:25:23,867 iteration 4665 : loss : 0.025122, loss_ce: 0.009761
2022-01-12 00:25:25,336 iteration 4666 : loss : 0.014754, loss_ce: 0.005377
2022-01-12 00:25:26,864 iteration 4667 : loss : 0.015188, loss_ce: 0.006238
2022-01-12 00:25:28,538 iteration 4668 : loss : 0.029505, loss_ce: 0.014689
2022-01-12 00:25:30,047 iteration 4669 : loss : 0.014841, loss_ce: 0.007150
2022-01-12 00:25:31,582 iteration 4670 : loss : 0.020862, loss_ce: 0.008313
2022-01-12 00:25:33,099 iteration 4671 : loss : 0.020185, loss_ce: 0.005685
2022-01-12 00:25:34,738 iteration 4672 : loss : 0.035959, loss_ce: 0.013058
2022-01-12 00:25:36,355 iteration 4673 : loss : 0.033820, loss_ce: 0.016035
2022-01-12 00:25:37,856 iteration 4674 : loss : 0.013716, loss_ce: 0.003769
2022-01-12 00:25:37,856 Training Data Eval:
2022-01-12 00:25:45,852   Average segmentation loss on training set: 0.0121
2022-01-12 00:25:45,852 Validation Data Eval:
2022-01-12 00:25:48,609   Average segmentation loss on validation set: 0.0750
2022-01-12 00:25:50,235 iteration 4675 : loss : 0.018880, loss_ce: 0.007124
 69%|██████████████████▌        | 275/400 [2:12:54<1:03:36, 30.53s/it]2022-01-12 00:25:51,937 iteration 4676 : loss : 0.028685, loss_ce: 0.009244
2022-01-12 00:25:53,485 iteration 4677 : loss : 0.027885, loss_ce: 0.008586
2022-01-12 00:25:55,070 iteration 4678 : loss : 0.019268, loss_ce: 0.006015
2022-01-12 00:25:56,708 iteration 4679 : loss : 0.030184, loss_ce: 0.008272
2022-01-12 00:25:58,271 iteration 4680 : loss : 0.016360, loss_ce: 0.006488
2022-01-12 00:25:59,822 iteration 4681 : loss : 0.026560, loss_ce: 0.007029
2022-01-12 00:26:01,363 iteration 4682 : loss : 0.022067, loss_ce: 0.008772
2022-01-12 00:26:02,836 iteration 4683 : loss : 0.019795, loss_ce: 0.005240
2022-01-12 00:26:04,453 iteration 4684 : loss : 0.018639, loss_ce: 0.008557
2022-01-12 00:26:06,044 iteration 4685 : loss : 0.040403, loss_ce: 0.013722
2022-01-12 00:26:07,609 iteration 4686 : loss : 0.017764, loss_ce: 0.005092
2022-01-12 00:26:09,141 iteration 4687 : loss : 0.017436, loss_ce: 0.007681
2022-01-12 00:26:10,633 iteration 4688 : loss : 0.013240, loss_ce: 0.005712
2022-01-12 00:26:12,134 iteration 4689 : loss : 0.012529, loss_ce: 0.005470
2022-01-12 00:26:13,653 iteration 4690 : loss : 0.023809, loss_ce: 0.009461
2022-01-12 00:26:15,248 iteration 4691 : loss : 0.029304, loss_ce: 0.015367
2022-01-12 00:26:16,794 iteration 4692 : loss : 0.020485, loss_ce: 0.006305
 69%|██████████████████▋        | 276/400 [2:13:20<1:00:38, 29.34s/it]2022-01-12 00:26:18,439 iteration 4693 : loss : 0.039616, loss_ce: 0.017092
2022-01-12 00:26:20,017 iteration 4694 : loss : 0.020865, loss_ce: 0.008144
2022-01-12 00:26:21,446 iteration 4695 : loss : 0.013837, loss_ce: 0.004654
2022-01-12 00:26:23,078 iteration 4696 : loss : 0.027216, loss_ce: 0.010291
2022-01-12 00:26:24,617 iteration 4697 : loss : 0.021414, loss_ce: 0.007778
2022-01-12 00:26:26,182 iteration 4698 : loss : 0.024510, loss_ce: 0.011508
2022-01-12 00:26:27,802 iteration 4699 : loss : 0.016542, loss_ce: 0.007793
2022-01-12 00:26:29,323 iteration 4700 : loss : 0.021606, loss_ce: 0.009792
2022-01-12 00:26:30,976 iteration 4701 : loss : 0.021420, loss_ce: 0.008194
2022-01-12 00:26:32,516 iteration 4702 : loss : 0.024284, loss_ce: 0.009717
2022-01-12 00:26:34,048 iteration 4703 : loss : 0.022467, loss_ce: 0.005679
2022-01-12 00:26:35,608 iteration 4704 : loss : 0.022757, loss_ce: 0.007789
2022-01-12 00:26:37,046 iteration 4705 : loss : 0.013098, loss_ce: 0.005719
2022-01-12 00:26:38,601 iteration 4706 : loss : 0.021414, loss_ce: 0.005268
2022-01-12 00:26:40,220 iteration 4707 : loss : 0.018114, loss_ce: 0.006837
2022-01-12 00:26:41,843 iteration 4708 : loss : 0.022355, loss_ce: 0.007737
2022-01-12 00:26:43,435 iteration 4709 : loss : 0.025082, loss_ce: 0.008989
 69%|████████████████████         | 277/400 [2:13:47<58:29, 28.53s/it]2022-01-12 00:26:45,045 iteration 4710 : loss : 0.024116, loss_ce: 0.005066
2022-01-12 00:26:46,739 iteration 4711 : loss : 0.025524, loss_ce: 0.009994
2022-01-12 00:26:48,280 iteration 4712 : loss : 0.019614, loss_ce: 0.007443
2022-01-12 00:26:49,840 iteration 4713 : loss : 0.015208, loss_ce: 0.005707
2022-01-12 00:26:51,421 iteration 4714 : loss : 0.027513, loss_ce: 0.008846
2022-01-12 00:26:53,032 iteration 4715 : loss : 0.028871, loss_ce: 0.010707
2022-01-12 00:26:54,610 iteration 4716 : loss : 0.016043, loss_ce: 0.006228
2022-01-12 00:26:56,161 iteration 4717 : loss : 0.030954, loss_ce: 0.013074
2022-01-12 00:26:57,666 iteration 4718 : loss : 0.015037, loss_ce: 0.005067
2022-01-12 00:26:59,172 iteration 4719 : loss : 0.017517, loss_ce: 0.007929
2022-01-12 00:27:00,696 iteration 4720 : loss : 0.015212, loss_ce: 0.006192
2022-01-12 00:27:02,246 iteration 4721 : loss : 0.018396, loss_ce: 0.007897
2022-01-12 00:27:03,859 iteration 4722 : loss : 0.026764, loss_ce: 0.008431
2022-01-12 00:27:05,416 iteration 4723 : loss : 0.013342, loss_ce: 0.005130
2022-01-12 00:27:06,915 iteration 4724 : loss : 0.016879, loss_ce: 0.006332
2022-01-12 00:27:08,420 iteration 4725 : loss : 0.016727, loss_ce: 0.006895
2022-01-12 00:27:09,982 iteration 4726 : loss : 0.018819, loss_ce: 0.008151
 70%|████████████████████▏        | 278/400 [2:14:14<56:48, 27.94s/it]2022-01-12 00:27:11,638 iteration 4727 : loss : 0.021591, loss_ce: 0.006652
2022-01-12 00:27:13,146 iteration 4728 : loss : 0.014556, loss_ce: 0.005604
2022-01-12 00:27:14,666 iteration 4729 : loss : 0.015426, loss_ce: 0.005471
2022-01-12 00:27:16,198 iteration 4730 : loss : 0.022767, loss_ce: 0.007178
2022-01-12 00:27:17,813 iteration 4731 : loss : 0.021949, loss_ce: 0.007228
2022-01-12 00:27:19,384 iteration 4732 : loss : 0.016548, loss_ce: 0.005518
2022-01-12 00:27:20,944 iteration 4733 : loss : 0.017247, loss_ce: 0.007436
2022-01-12 00:27:22,489 iteration 4734 : loss : 0.015734, loss_ce: 0.006013
2022-01-12 00:27:24,134 iteration 4735 : loss : 0.021491, loss_ce: 0.006848
2022-01-12 00:27:25,631 iteration 4736 : loss : 0.022998, loss_ce: 0.008965
2022-01-12 00:27:27,250 iteration 4737 : loss : 0.021921, loss_ce: 0.009749
2022-01-12 00:27:28,801 iteration 4738 : loss : 0.024044, loss_ce: 0.008644
2022-01-12 00:27:30,366 iteration 4739 : loss : 0.018292, loss_ce: 0.007075
2022-01-12 00:27:31,965 iteration 4740 : loss : 0.026800, loss_ce: 0.011230
2022-01-12 00:27:33,508 iteration 4741 : loss : 0.014429, loss_ce: 0.004884
2022-01-12 00:27:35,066 iteration 4742 : loss : 0.020591, loss_ce: 0.009774
2022-01-12 00:27:36,703 iteration 4743 : loss : 0.019970, loss_ce: 0.006741
 70%|████████████████████▏        | 279/400 [2:14:40<55:36, 27.57s/it]2022-01-12 00:27:38,350 iteration 4744 : loss : 0.018292, loss_ce: 0.006527
2022-01-12 00:27:39,901 iteration 4745 : loss : 0.029672, loss_ce: 0.007690
2022-01-12 00:27:41,424 iteration 4746 : loss : 0.014966, loss_ce: 0.007994
2022-01-12 00:27:42,964 iteration 4747 : loss : 0.028446, loss_ce: 0.012610
2022-01-12 00:27:44,528 iteration 4748 : loss : 0.021487, loss_ce: 0.008067
2022-01-12 00:27:46,098 iteration 4749 : loss : 0.021209, loss_ce: 0.006711
2022-01-12 00:27:47,707 iteration 4750 : loss : 0.019291, loss_ce: 0.007140
2022-01-12 00:27:49,247 iteration 4751 : loss : 0.017484, loss_ce: 0.006979
2022-01-12 00:27:50,834 iteration 4752 : loss : 0.022493, loss_ce: 0.007052
2022-01-12 00:27:52,426 iteration 4753 : loss : 0.019396, loss_ce: 0.006376
2022-01-12 00:27:54,011 iteration 4754 : loss : 0.016243, loss_ce: 0.005287
2022-01-12 00:27:55,671 iteration 4755 : loss : 0.021461, loss_ce: 0.005731
2022-01-12 00:27:57,205 iteration 4756 : loss : 0.016488, loss_ce: 0.007906
2022-01-12 00:27:58,750 iteration 4757 : loss : 0.019709, loss_ce: 0.007632
2022-01-12 00:28:00,286 iteration 4758 : loss : 0.016939, loss_ce: 0.005555
2022-01-12 00:28:01,862 iteration 4759 : loss : 0.013134, loss_ce: 0.004570
2022-01-12 00:28:01,862 Training Data Eval:
2022-01-12 00:28:09,864   Average segmentation loss on training set: 0.0111
2022-01-12 00:28:09,865 Validation Data Eval:
2022-01-12 00:28:12,612   Average segmentation loss on validation set: 0.0707
2022-01-12 00:28:14,160 iteration 4760 : loss : 0.020520, loss_ce: 0.010060
 70%|██████████████████▉        | 280/400 [2:15:18<1:01:04, 30.54s/it]2022-01-12 00:28:15,764 iteration 4761 : loss : 0.015123, loss_ce: 0.005083
2022-01-12 00:28:17,369 iteration 4762 : loss : 0.019176, loss_ce: 0.006307
2022-01-12 00:28:18,895 iteration 4763 : loss : 0.018579, loss_ce: 0.008613
2022-01-12 00:28:20,504 iteration 4764 : loss : 0.019735, loss_ce: 0.006804
2022-01-12 00:28:22,055 iteration 4765 : loss : 0.027044, loss_ce: 0.012417
2022-01-12 00:28:23,698 iteration 4766 : loss : 0.020395, loss_ce: 0.007057
2022-01-12 00:28:25,296 iteration 4767 : loss : 0.014363, loss_ce: 0.005712
2022-01-12 00:28:26,823 iteration 4768 : loss : 0.017390, loss_ce: 0.005621
2022-01-12 00:28:28,438 iteration 4769 : loss : 0.021557, loss_ce: 0.007427
2022-01-12 00:28:30,057 iteration 4770 : loss : 0.013143, loss_ce: 0.004766
2022-01-12 00:28:31,568 iteration 4771 : loss : 0.016272, loss_ce: 0.005235
2022-01-12 00:28:33,187 iteration 4772 : loss : 0.020784, loss_ce: 0.008939
2022-01-12 00:28:34,787 iteration 4773 : loss : 0.025906, loss_ce: 0.011333
2022-01-12 00:28:36,382 iteration 4774 : loss : 0.015860, loss_ce: 0.006345
2022-01-12 00:28:37,911 iteration 4775 : loss : 0.021137, loss_ce: 0.006127
2022-01-12 00:28:39,466 iteration 4776 : loss : 0.019045, loss_ce: 0.011916
2022-01-12 00:28:41,169 iteration 4777 : loss : 0.028660, loss_ce: 0.016744
 70%|████████████████████▎        | 281/400 [2:15:45<58:27, 29.48s/it]2022-01-12 00:28:42,694 iteration 4778 : loss : 0.011036, loss_ce: 0.003151
2022-01-12 00:28:44,240 iteration 4779 : loss : 0.031527, loss_ce: 0.011549
2022-01-12 00:28:45,743 iteration 4780 : loss : 0.026302, loss_ce: 0.011406
2022-01-12 00:28:47,297 iteration 4781 : loss : 0.024244, loss_ce: 0.007230
2022-01-12 00:28:48,920 iteration 4782 : loss : 0.017260, loss_ce: 0.007798
2022-01-12 00:28:50,485 iteration 4783 : loss : 0.015597, loss_ce: 0.007755
2022-01-12 00:28:52,001 iteration 4784 : loss : 0.012006, loss_ce: 0.004907
2022-01-12 00:28:53,637 iteration 4785 : loss : 0.023684, loss_ce: 0.008709
2022-01-12 00:28:55,213 iteration 4786 : loss : 0.020955, loss_ce: 0.007646
2022-01-12 00:28:56,792 iteration 4787 : loss : 0.022594, loss_ce: 0.007569
2022-01-12 00:28:58,386 iteration 4788 : loss : 0.017266, loss_ce: 0.006072
2022-01-12 00:28:59,884 iteration 4789 : loss : 0.015583, loss_ce: 0.007361
2022-01-12 00:29:01,539 iteration 4790 : loss : 0.018651, loss_ce: 0.005825
2022-01-12 00:29:03,084 iteration 4791 : loss : 0.022899, loss_ce: 0.011503
2022-01-12 00:29:04,709 iteration 4792 : loss : 0.034421, loss_ce: 0.009143
2022-01-12 00:29:06,240 iteration 4793 : loss : 0.015369, loss_ce: 0.006122
2022-01-12 00:29:07,737 iteration 4794 : loss : 0.017775, loss_ce: 0.006001
 70%|████████████████████▍        | 282/400 [2:16:11<56:15, 28.60s/it]2022-01-12 00:29:09,369 iteration 4795 : loss : 0.026522, loss_ce: 0.010732
2022-01-12 00:29:10,940 iteration 4796 : loss : 0.017674, loss_ce: 0.005670
2022-01-12 00:29:12,537 iteration 4797 : loss : 0.038379, loss_ce: 0.014660
2022-01-12 00:29:14,169 iteration 4798 : loss : 0.027699, loss_ce: 0.008419
2022-01-12 00:29:15,736 iteration 4799 : loss : 0.011994, loss_ce: 0.003706
2022-01-12 00:29:17,316 iteration 4800 : loss : 0.023086, loss_ce: 0.007205
2022-01-12 00:29:18,963 iteration 4801 : loss : 0.019517, loss_ce: 0.007448
2022-01-12 00:29:20,460 iteration 4802 : loss : 0.014518, loss_ce: 0.006819
2022-01-12 00:29:22,003 iteration 4803 : loss : 0.019214, loss_ce: 0.008196
2022-01-12 00:29:23,640 iteration 4804 : loss : 0.034921, loss_ce: 0.010061
2022-01-12 00:29:25,258 iteration 4805 : loss : 0.026960, loss_ce: 0.009736
2022-01-12 00:29:26,877 iteration 4806 : loss : 0.030105, loss_ce: 0.011901
2022-01-12 00:29:28,364 iteration 4807 : loss : 0.018134, loss_ce: 0.007727
2022-01-12 00:29:29,866 iteration 4808 : loss : 0.015286, loss_ce: 0.006966
2022-01-12 00:29:31,449 iteration 4809 : loss : 0.020863, loss_ce: 0.010699
2022-01-12 00:29:33,021 iteration 4810 : loss : 0.018586, loss_ce: 0.004669
2022-01-12 00:29:34,591 iteration 4811 : loss : 0.014139, loss_ce: 0.005052
 71%|████████████████████▌        | 283/400 [2:16:38<54:45, 28.08s/it]2022-01-12 00:29:36,129 iteration 4812 : loss : 0.019478, loss_ce: 0.007257
2022-01-12 00:29:37,710 iteration 4813 : loss : 0.013571, loss_ce: 0.003256
2022-01-12 00:29:39,317 iteration 4814 : loss : 0.027668, loss_ce: 0.007937
2022-01-12 00:29:40,833 iteration 4815 : loss : 0.016451, loss_ce: 0.007310
2022-01-12 00:29:42,507 iteration 4816 : loss : 0.019519, loss_ce: 0.009467
2022-01-12 00:29:44,055 iteration 4817 : loss : 0.020008, loss_ce: 0.008060
2022-01-12 00:29:45,601 iteration 4818 : loss : 0.016782, loss_ce: 0.007056
2022-01-12 00:29:47,085 iteration 4819 : loss : 0.027136, loss_ce: 0.009386
2022-01-12 00:29:48,623 iteration 4820 : loss : 0.019926, loss_ce: 0.007579
2022-01-12 00:29:50,279 iteration 4821 : loss : 0.019347, loss_ce: 0.009281
2022-01-12 00:29:51,918 iteration 4822 : loss : 0.018910, loss_ce: 0.006049
2022-01-12 00:29:53,473 iteration 4823 : loss : 0.019665, loss_ce: 0.008537
2022-01-12 00:29:55,052 iteration 4824 : loss : 0.021507, loss_ce: 0.008518
2022-01-12 00:29:56,593 iteration 4825 : loss : 0.027518, loss_ce: 0.010492
2022-01-12 00:29:58,120 iteration 4826 : loss : 0.020278, loss_ce: 0.006304
2022-01-12 00:29:59,656 iteration 4827 : loss : 0.016131, loss_ce: 0.006611
2022-01-12 00:30:01,186 iteration 4828 : loss : 0.016440, loss_ce: 0.006512
 71%|████████████████████▌        | 284/400 [2:17:05<53:25, 27.64s/it]2022-01-12 00:30:02,743 iteration 4829 : loss : 0.015492, loss_ce: 0.005292
2022-01-12 00:30:04,333 iteration 4830 : loss : 0.018079, loss_ce: 0.008470
2022-01-12 00:30:05,892 iteration 4831 : loss : 0.016093, loss_ce: 0.006255
2022-01-12 00:30:07,444 iteration 4832 : loss : 0.039069, loss_ce: 0.012635
2022-01-12 00:30:09,044 iteration 4833 : loss : 0.019969, loss_ce: 0.006878
2022-01-12 00:30:10,537 iteration 4834 : loss : 0.013688, loss_ce: 0.004651
2022-01-12 00:30:12,202 iteration 4835 : loss : 0.020313, loss_ce: 0.011540
2022-01-12 00:30:13,773 iteration 4836 : loss : 0.029774, loss_ce: 0.008191
2022-01-12 00:30:15,381 iteration 4837 : loss : 0.022899, loss_ce: 0.006844
2022-01-12 00:30:16,914 iteration 4838 : loss : 0.013330, loss_ce: 0.004114
2022-01-12 00:30:18,504 iteration 4839 : loss : 0.027151, loss_ce: 0.009789
2022-01-12 00:30:20,064 iteration 4840 : loss : 0.018404, loss_ce: 0.008969
2022-01-12 00:30:21,567 iteration 4841 : loss : 0.012028, loss_ce: 0.003731
2022-01-12 00:30:23,130 iteration 4842 : loss : 0.017028, loss_ce: 0.006158
2022-01-12 00:30:24,768 iteration 4843 : loss : 0.019968, loss_ce: 0.006223
2022-01-12 00:30:26,318 iteration 4844 : loss : 0.021847, loss_ce: 0.007979
2022-01-12 00:30:26,318 Training Data Eval:
2022-01-12 00:30:34,314   Average segmentation loss on training set: 0.0117
2022-01-12 00:30:34,315 Validation Data Eval:
2022-01-12 00:30:37,074   Average segmentation loss on validation set: 0.0698
2022-01-12 00:30:38,682 iteration 4845 : loss : 0.024227, loss_ce: 0.010801
 71%|████████████████████▋        | 285/400 [2:17:42<58:38, 30.59s/it]2022-01-12 00:30:40,254 iteration 4846 : loss : 0.020416, loss_ce: 0.007530
2022-01-12 00:30:41,790 iteration 4847 : loss : 0.021665, loss_ce: 0.007571
2022-01-12 00:30:43,401 iteration 4848 : loss : 0.027283, loss_ce: 0.013328
2022-01-12 00:30:44,992 iteration 4849 : loss : 0.020858, loss_ce: 0.005480
2022-01-12 00:30:46,535 iteration 4850 : loss : 0.017569, loss_ce: 0.005360
2022-01-12 00:30:48,197 iteration 4851 : loss : 0.016024, loss_ce: 0.005841
2022-01-12 00:30:49,740 iteration 4852 : loss : 0.014080, loss_ce: 0.004543
2022-01-12 00:30:51,308 iteration 4853 : loss : 0.021712, loss_ce: 0.009722
2022-01-12 00:30:52,891 iteration 4854 : loss : 0.014814, loss_ce: 0.005956
2022-01-12 00:30:54,424 iteration 4855 : loss : 0.018678, loss_ce: 0.004949
2022-01-12 00:30:56,060 iteration 4856 : loss : 0.026652, loss_ce: 0.012028
2022-01-12 00:30:57,589 iteration 4857 : loss : 0.012102, loss_ce: 0.004510
2022-01-12 00:30:59,210 iteration 4858 : loss : 0.016958, loss_ce: 0.006340
2022-01-12 00:31:00,718 iteration 4859 : loss : 0.011727, loss_ce: 0.004724
2022-01-12 00:31:02,320 iteration 4860 : loss : 0.037345, loss_ce: 0.016764
2022-01-12 00:31:03,845 iteration 4861 : loss : 0.017794, loss_ce: 0.005921
2022-01-12 00:31:05,443 iteration 4862 : loss : 0.021353, loss_ce: 0.012069
 72%|████████████████████▋        | 286/400 [2:18:09<55:56, 29.44s/it]2022-01-12 00:31:07,114 iteration 4863 : loss : 0.019195, loss_ce: 0.005228
2022-01-12 00:31:08,662 iteration 4864 : loss : 0.020281, loss_ce: 0.007209
2022-01-12 00:31:10,195 iteration 4865 : loss : 0.015405, loss_ce: 0.007879
2022-01-12 00:31:11,820 iteration 4866 : loss : 0.020412, loss_ce: 0.006168
2022-01-12 00:31:13,382 iteration 4867 : loss : 0.017332, loss_ce: 0.006493
2022-01-12 00:31:14,850 iteration 4868 : loss : 0.016964, loss_ce: 0.007434
2022-01-12 00:31:16,505 iteration 4869 : loss : 0.024083, loss_ce: 0.010930
2022-01-12 00:31:18,059 iteration 4870 : loss : 0.012623, loss_ce: 0.004902
2022-01-12 00:31:19,700 iteration 4871 : loss : 0.020340, loss_ce: 0.008060
2022-01-12 00:31:21,200 iteration 4872 : loss : 0.012286, loss_ce: 0.003942
2022-01-12 00:31:22,845 iteration 4873 : loss : 0.020105, loss_ce: 0.007739
2022-01-12 00:31:24,417 iteration 4874 : loss : 0.027057, loss_ce: 0.008398
2022-01-12 00:31:25,956 iteration 4875 : loss : 0.018070, loss_ce: 0.007093
2022-01-12 00:31:27,433 iteration 4876 : loss : 0.019121, loss_ce: 0.006112
2022-01-12 00:31:29,034 iteration 4877 : loss : 0.023849, loss_ce: 0.013082
2022-01-12 00:31:30,656 iteration 4878 : loss : 0.032360, loss_ce: 0.010231
2022-01-12 00:31:32,218 iteration 4879 : loss : 0.013214, loss_ce: 0.004695
 72%|████████████████████▊        | 287/400 [2:18:36<53:56, 28.64s/it]2022-01-12 00:31:33,797 iteration 4880 : loss : 0.019491, loss_ce: 0.007380
2022-01-12 00:31:35,313 iteration 4881 : loss : 0.016917, loss_ce: 0.006991
2022-01-12 00:31:36,786 iteration 4882 : loss : 0.014024, loss_ce: 0.006204
2022-01-12 00:31:38,372 iteration 4883 : loss : 0.019160, loss_ce: 0.008907
2022-01-12 00:31:39,954 iteration 4884 : loss : 0.019020, loss_ce: 0.005312
2022-01-12 00:31:41,399 iteration 4885 : loss : 0.013575, loss_ce: 0.005409
2022-01-12 00:31:42,949 iteration 4886 : loss : 0.018234, loss_ce: 0.005594
2022-01-12 00:31:44,444 iteration 4887 : loss : 0.013695, loss_ce: 0.004062
2022-01-12 00:31:46,026 iteration 4888 : loss : 0.055512, loss_ce: 0.015864
2022-01-12 00:31:47,592 iteration 4889 : loss : 0.025351, loss_ce: 0.011646
2022-01-12 00:31:49,099 iteration 4890 : loss : 0.015734, loss_ce: 0.008199
2022-01-12 00:31:50,552 iteration 4891 : loss : 0.011724, loss_ce: 0.004341
2022-01-12 00:31:52,148 iteration 4892 : loss : 0.014009, loss_ce: 0.005536
2022-01-12 00:31:53,714 iteration 4893 : loss : 0.017292, loss_ce: 0.007565
2022-01-12 00:31:55,253 iteration 4894 : loss : 0.013544, loss_ce: 0.006139
2022-01-12 00:31:56,787 iteration 4895 : loss : 0.028821, loss_ce: 0.005562
2022-01-12 00:31:58,299 iteration 4896 : loss : 0.018147, loss_ce: 0.005053
 72%|████████████████████▉        | 288/400 [2:19:02<52:02, 27.88s/it]2022-01-12 00:31:59,862 iteration 4897 : loss : 0.018788, loss_ce: 0.007369
2022-01-12 00:32:01,474 iteration 4898 : loss : 0.035432, loss_ce: 0.019162
2022-01-12 00:32:03,032 iteration 4899 : loss : 0.013208, loss_ce: 0.003902
2022-01-12 00:32:04,555 iteration 4900 : loss : 0.023983, loss_ce: 0.009425
2022-01-12 00:32:06,076 iteration 4901 : loss : 0.022674, loss_ce: 0.005913
2022-01-12 00:32:07,611 iteration 4902 : loss : 0.014666, loss_ce: 0.004924
2022-01-12 00:32:09,197 iteration 4903 : loss : 0.017800, loss_ce: 0.008063
2022-01-12 00:32:10,653 iteration 4904 : loss : 0.018132, loss_ce: 0.005132
2022-01-12 00:32:12,176 iteration 4905 : loss : 0.016777, loss_ce: 0.005060
2022-01-12 00:32:13,774 iteration 4906 : loss : 0.018119, loss_ce: 0.006256
2022-01-12 00:32:15,289 iteration 4907 : loss : 0.018921, loss_ce: 0.007380
2022-01-12 00:32:16,922 iteration 4908 : loss : 0.021374, loss_ce: 0.009274
2022-01-12 00:32:18,498 iteration 4909 : loss : 0.022502, loss_ce: 0.006807
2022-01-12 00:32:20,005 iteration 4910 : loss : 0.018133, loss_ce: 0.008333
2022-01-12 00:32:21,550 iteration 4911 : loss : 0.013179, loss_ce: 0.004747
2022-01-12 00:32:23,103 iteration 4912 : loss : 0.018677, loss_ce: 0.009120
2022-01-12 00:32:24,657 iteration 4913 : loss : 0.016358, loss_ce: 0.006292
 72%|████████████████████▉        | 289/400 [2:19:28<50:43, 27.42s/it]2022-01-12 00:32:26,278 iteration 4914 : loss : 0.019147, loss_ce: 0.006402
2022-01-12 00:32:27,787 iteration 4915 : loss : 0.023075, loss_ce: 0.008614
2022-01-12 00:32:29,305 iteration 4916 : loss : 0.014003, loss_ce: 0.003921
2022-01-12 00:32:30,850 iteration 4917 : loss : 0.018712, loss_ce: 0.005498
2022-01-12 00:32:32,425 iteration 4918 : loss : 0.017628, loss_ce: 0.006610
2022-01-12 00:32:34,129 iteration 4919 : loss : 0.024070, loss_ce: 0.010970
2022-01-12 00:32:35,673 iteration 4920 : loss : 0.014860, loss_ce: 0.005938
2022-01-12 00:32:37,177 iteration 4921 : loss : 0.015798, loss_ce: 0.007732
2022-01-12 00:32:38,666 iteration 4922 : loss : 0.014373, loss_ce: 0.003811
2022-01-12 00:32:40,268 iteration 4923 : loss : 0.023278, loss_ce: 0.008775
2022-01-12 00:32:41,903 iteration 4924 : loss : 0.023008, loss_ce: 0.011982
2022-01-12 00:32:43,485 iteration 4925 : loss : 0.019306, loss_ce: 0.007156
2022-01-12 00:32:45,090 iteration 4926 : loss : 0.017622, loss_ce: 0.008720
2022-01-12 00:32:46,624 iteration 4927 : loss : 0.026675, loss_ce: 0.012342
2022-01-12 00:32:48,224 iteration 4928 : loss : 0.023175, loss_ce: 0.008201
2022-01-12 00:32:49,859 iteration 4929 : loss : 0.021940, loss_ce: 0.009708
2022-01-12 00:32:49,859 Training Data Eval:
2022-01-12 00:32:57,797   Average segmentation loss on training set: 0.0116
2022-01-12 00:32:57,797 Validation Data Eval:
2022-01-12 00:33:00,553   Average segmentation loss on validation set: 0.0727
2022-01-12 00:33:02,131 iteration 4930 : loss : 0.022428, loss_ce: 0.007599
 72%|█████████████████████        | 290/400 [2:20:06<55:47, 30.43s/it]2022-01-12 00:33:03,720 iteration 4931 : loss : 0.016951, loss_ce: 0.008074
2022-01-12 00:33:05,310 iteration 4932 : loss : 0.020417, loss_ce: 0.011005
2022-01-12 00:33:06,817 iteration 4933 : loss : 0.021056, loss_ce: 0.014170
2022-01-12 00:33:08,435 iteration 4934 : loss : 0.035547, loss_ce: 0.012780
2022-01-12 00:33:09,996 iteration 4935 : loss : 0.026095, loss_ce: 0.009224
2022-01-12 00:33:11,593 iteration 4936 : loss : 0.026866, loss_ce: 0.010801
2022-01-12 00:33:13,173 iteration 4937 : loss : 0.029987, loss_ce: 0.017549
2022-01-12 00:33:14,781 iteration 4938 : loss : 0.033841, loss_ce: 0.013343
2022-01-12 00:33:16,391 iteration 4939 : loss : 0.036217, loss_ce: 0.014210
2022-01-12 00:33:17,929 iteration 4940 : loss : 0.025719, loss_ce: 0.010556
2022-01-12 00:33:19,506 iteration 4941 : loss : 0.026978, loss_ce: 0.014497
2022-01-12 00:33:21,075 iteration 4942 : loss : 0.024918, loss_ce: 0.009775
2022-01-12 00:33:22,722 iteration 4943 : loss : 0.038475, loss_ce: 0.008893
2022-01-12 00:33:24,277 iteration 4944 : loss : 0.022531, loss_ce: 0.008055
2022-01-12 00:33:25,887 iteration 4945 : loss : 0.019316, loss_ce: 0.007151
2022-01-12 00:33:27,511 iteration 4946 : loss : 0.032391, loss_ce: 0.009120
2022-01-12 00:33:29,048 iteration 4947 : loss : 0.019473, loss_ce: 0.006554
 73%|█████████████████████        | 291/400 [2:20:33<53:22, 29.38s/it]2022-01-12 00:33:30,668 iteration 4948 : loss : 0.038442, loss_ce: 0.011782
2022-01-12 00:33:32,204 iteration 4949 : loss : 0.020640, loss_ce: 0.008525
2022-01-12 00:33:33,805 iteration 4950 : loss : 0.026139, loss_ce: 0.011041
2022-01-12 00:33:35,395 iteration 4951 : loss : 0.023194, loss_ce: 0.007699
2022-01-12 00:33:36,977 iteration 4952 : loss : 0.019278, loss_ce: 0.010651
2022-01-12 00:33:38,572 iteration 4953 : loss : 0.026715, loss_ce: 0.010775
2022-01-12 00:33:40,096 iteration 4954 : loss : 0.016499, loss_ce: 0.005346
2022-01-12 00:33:41,594 iteration 4955 : loss : 0.028772, loss_ce: 0.008715
2022-01-12 00:33:43,142 iteration 4956 : loss : 0.019708, loss_ce: 0.005721
2022-01-12 00:33:44,616 iteration 4957 : loss : 0.015698, loss_ce: 0.006506
2022-01-12 00:33:46,184 iteration 4958 : loss : 0.023967, loss_ce: 0.012283
2022-01-12 00:33:47,743 iteration 4959 : loss : 0.017632, loss_ce: 0.006464
2022-01-12 00:33:49,239 iteration 4960 : loss : 0.022668, loss_ce: 0.009002
2022-01-12 00:33:50,812 iteration 4961 : loss : 0.018165, loss_ce: 0.007246
2022-01-12 00:33:52,324 iteration 4962 : loss : 0.026595, loss_ce: 0.005995
2022-01-12 00:33:53,854 iteration 4963 : loss : 0.019202, loss_ce: 0.007965
2022-01-12 00:33:55,448 iteration 4964 : loss : 0.047444, loss_ce: 0.015736
 73%|█████████████████████▏       | 292/400 [2:20:59<51:16, 28.49s/it]2022-01-12 00:33:57,097 iteration 4965 : loss : 0.029148, loss_ce: 0.012426
2022-01-12 00:33:58,689 iteration 4966 : loss : 0.022160, loss_ce: 0.008946
2022-01-12 00:34:00,251 iteration 4967 : loss : 0.022359, loss_ce: 0.009325
2022-01-12 00:34:01,829 iteration 4968 : loss : 0.019656, loss_ce: 0.009416
2022-01-12 00:34:03,483 iteration 4969 : loss : 0.048050, loss_ce: 0.012645
2022-01-12 00:34:05,007 iteration 4970 : loss : 0.017795, loss_ce: 0.007225
2022-01-12 00:34:06,536 iteration 4971 : loss : 0.030733, loss_ce: 0.019608
2022-01-12 00:34:08,131 iteration 4972 : loss : 0.021143, loss_ce: 0.009525
2022-01-12 00:34:09,698 iteration 4973 : loss : 0.020583, loss_ce: 0.007795
2022-01-12 00:34:11,364 iteration 4974 : loss : 0.021186, loss_ce: 0.007497
2022-01-12 00:34:12,901 iteration 4975 : loss : 0.019205, loss_ce: 0.005759
2022-01-12 00:34:14,401 iteration 4976 : loss : 0.020569, loss_ce: 0.007598
2022-01-12 00:34:15,989 iteration 4977 : loss : 0.025860, loss_ce: 0.005827
2022-01-12 00:34:17,522 iteration 4978 : loss : 0.019715, loss_ce: 0.004886
2022-01-12 00:34:19,076 iteration 4979 : loss : 0.016835, loss_ce: 0.005566
2022-01-12 00:34:20,640 iteration 4980 : loss : 0.022607, loss_ce: 0.006011
2022-01-12 00:34:22,189 iteration 4981 : loss : 0.017532, loss_ce: 0.005559
 73%|█████████████████████▏       | 293/400 [2:21:26<49:52, 27.96s/it]2022-01-12 00:34:23,755 iteration 4982 : loss : 0.016751, loss_ce: 0.007349
2022-01-12 00:34:25,287 iteration 4983 : loss : 0.020413, loss_ce: 0.009769
2022-01-12 00:34:26,917 iteration 4984 : loss : 0.022110, loss_ce: 0.008258
2022-01-12 00:34:28,397 iteration 4985 : loss : 0.014215, loss_ce: 0.003840
2022-01-12 00:34:30,034 iteration 4986 : loss : 0.027379, loss_ce: 0.012216
2022-01-12 00:34:31,540 iteration 4987 : loss : 0.017329, loss_ce: 0.006028
2022-01-12 00:34:33,101 iteration 4988 : loss : 0.020854, loss_ce: 0.007250
2022-01-12 00:34:34,698 iteration 4989 : loss : 0.026663, loss_ce: 0.010027
2022-01-12 00:34:36,315 iteration 4990 : loss : 0.018174, loss_ce: 0.008018
2022-01-12 00:34:37,877 iteration 4991 : loss : 0.018697, loss_ce: 0.005365
2022-01-12 00:34:39,350 iteration 4992 : loss : 0.019747, loss_ce: 0.011754
2022-01-12 00:34:40,896 iteration 4993 : loss : 0.024721, loss_ce: 0.008492
2022-01-12 00:34:42,416 iteration 4994 : loss : 0.017187, loss_ce: 0.005837
2022-01-12 00:34:43,942 iteration 4995 : loss : 0.020391, loss_ce: 0.009769
2022-01-12 00:34:45,542 iteration 4996 : loss : 0.026235, loss_ce: 0.007688
2022-01-12 00:34:47,029 iteration 4997 : loss : 0.015741, loss_ce: 0.004756
2022-01-12 00:34:48,529 iteration 4998 : loss : 0.015530, loss_ce: 0.006352
 74%|█████████████████████▎       | 294/400 [2:21:52<48:32, 27.48s/it]2022-01-12 00:34:50,087 iteration 4999 : loss : 0.016148, loss_ce: 0.005798
2022-01-12 00:34:51,632 iteration 5000 : loss : 0.014934, loss_ce: 0.005813
2022-01-12 00:34:53,171 iteration 5001 : loss : 0.015786, loss_ce: 0.007173
2022-01-12 00:34:54,723 iteration 5002 : loss : 0.014036, loss_ce: 0.005303
2022-01-12 00:34:56,300 iteration 5003 : loss : 0.029495, loss_ce: 0.006706
2022-01-12 00:34:57,836 iteration 5004 : loss : 0.022977, loss_ce: 0.007944
2022-01-12 00:34:59,378 iteration 5005 : loss : 0.024736, loss_ce: 0.010886
2022-01-12 00:35:00,973 iteration 5006 : loss : 0.019028, loss_ce: 0.007556
2022-01-12 00:35:02,555 iteration 5007 : loss : 0.031932, loss_ce: 0.015590
2022-01-12 00:35:04,141 iteration 5008 : loss : 0.027454, loss_ce: 0.010234
2022-01-12 00:35:05,727 iteration 5009 : loss : 0.016992, loss_ce: 0.005033
2022-01-12 00:35:07,308 iteration 5010 : loss : 0.023721, loss_ce: 0.008073
2022-01-12 00:35:08,856 iteration 5011 : loss : 0.016545, loss_ce: 0.008260
2022-01-12 00:35:10,414 iteration 5012 : loss : 0.020508, loss_ce: 0.008194
2022-01-12 00:35:11,917 iteration 5013 : loss : 0.018812, loss_ce: 0.007443
2022-01-12 00:35:13,476 iteration 5014 : loss : 0.022359, loss_ce: 0.008015
2022-01-12 00:35:13,477 Training Data Eval:
2022-01-12 00:35:21,454   Average segmentation loss on training set: 0.0106
2022-01-12 00:35:21,455 Validation Data Eval:
2022-01-12 00:35:24,208   Average segmentation loss on validation set: 0.0720
2022-01-12 00:35:25,766 iteration 5015 : loss : 0.029068, loss_ce: 0.011793
 74%|█████████████████████▍       | 295/400 [2:22:29<53:12, 30.40s/it]2022-01-12 00:35:27,362 iteration 5016 : loss : 0.022379, loss_ce: 0.011896
2022-01-12 00:35:28,894 iteration 5017 : loss : 0.014827, loss_ce: 0.005196
2022-01-12 00:35:30,434 iteration 5018 : loss : 0.018504, loss_ce: 0.007401
2022-01-12 00:35:32,015 iteration 5019 : loss : 0.017492, loss_ce: 0.006121
2022-01-12 00:35:33,584 iteration 5020 : loss : 0.017214, loss_ce: 0.005018
2022-01-12 00:35:35,173 iteration 5021 : loss : 0.017523, loss_ce: 0.008516
2022-01-12 00:35:36,682 iteration 5022 : loss : 0.017290, loss_ce: 0.005422
2022-01-12 00:35:38,248 iteration 5023 : loss : 0.023776, loss_ce: 0.007519
2022-01-12 00:35:39,894 iteration 5024 : loss : 0.029058, loss_ce: 0.006784
2022-01-12 00:35:41,507 iteration 5025 : loss : 0.031162, loss_ce: 0.005585
2022-01-12 00:35:43,078 iteration 5026 : loss : 0.027677, loss_ce: 0.009904
2022-01-12 00:35:44,597 iteration 5027 : loss : 0.015828, loss_ce: 0.006667
2022-01-12 00:35:46,134 iteration 5028 : loss : 0.023946, loss_ce: 0.008831
2022-01-12 00:35:47,772 iteration 5029 : loss : 0.021648, loss_ce: 0.005177
2022-01-12 00:35:49,328 iteration 5030 : loss : 0.020406, loss_ce: 0.010670
2022-01-12 00:35:50,938 iteration 5031 : loss : 0.021742, loss_ce: 0.009936
2022-01-12 00:35:52,495 iteration 5032 : loss : 0.019236, loss_ce: 0.009751
 74%|█████████████████████▍       | 296/400 [2:22:56<50:47, 29.30s/it]2022-01-12 00:35:54,189 iteration 5033 : loss : 0.024174, loss_ce: 0.008439
2022-01-12 00:35:55,731 iteration 5034 : loss : 0.022548, loss_ce: 0.006345
2022-01-12 00:35:57,249 iteration 5035 : loss : 0.024128, loss_ce: 0.012227
2022-01-12 00:35:58,773 iteration 5036 : loss : 0.020855, loss_ce: 0.008682
2022-01-12 00:36:00,334 iteration 5037 : loss : 0.018779, loss_ce: 0.006889
2022-01-12 00:36:01,851 iteration 5038 : loss : 0.035340, loss_ce: 0.009215
2022-01-12 00:36:03,488 iteration 5039 : loss : 0.020298, loss_ce: 0.010870
2022-01-12 00:36:05,038 iteration 5040 : loss : 0.013309, loss_ce: 0.003797
2022-01-12 00:36:06,649 iteration 5041 : loss : 0.020887, loss_ce: 0.007243
2022-01-12 00:36:08,163 iteration 5042 : loss : 0.017076, loss_ce: 0.006949
2022-01-12 00:36:09,715 iteration 5043 : loss : 0.017756, loss_ce: 0.008422
2022-01-12 00:36:11,270 iteration 5044 : loss : 0.022350, loss_ce: 0.009768
2022-01-12 00:36:12,871 iteration 5045 : loss : 0.028205, loss_ce: 0.012623
2022-01-12 00:36:14,443 iteration 5046 : loss : 0.022671, loss_ce: 0.005597
2022-01-12 00:36:15,978 iteration 5047 : loss : 0.021529, loss_ce: 0.006294
2022-01-12 00:36:17,504 iteration 5048 : loss : 0.016235, loss_ce: 0.006454
2022-01-12 00:36:19,028 iteration 5049 : loss : 0.019035, loss_ce: 0.008811
 74%|█████████████████████▌       | 297/400 [2:23:23<48:52, 28.47s/it]2022-01-12 00:36:20,630 iteration 5050 : loss : 0.029587, loss_ce: 0.010707
2022-01-12 00:36:22,174 iteration 5051 : loss : 0.019840, loss_ce: 0.007804
2022-01-12 00:36:23,717 iteration 5052 : loss : 0.018575, loss_ce: 0.008006
2022-01-12 00:36:25,257 iteration 5053 : loss : 0.014689, loss_ce: 0.005524
2022-01-12 00:36:26,815 iteration 5054 : loss : 0.015287, loss_ce: 0.004695
2022-01-12 00:36:28,358 iteration 5055 : loss : 0.019215, loss_ce: 0.006277
2022-01-12 00:36:29,888 iteration 5056 : loss : 0.019328, loss_ce: 0.004612
2022-01-12 00:36:31,499 iteration 5057 : loss : 0.017477, loss_ce: 0.009221
2022-01-12 00:36:33,121 iteration 5058 : loss : 0.026269, loss_ce: 0.010681
2022-01-12 00:36:34,568 iteration 5059 : loss : 0.018375, loss_ce: 0.005311
2022-01-12 00:36:36,140 iteration 5060 : loss : 0.017697, loss_ce: 0.005156
2022-01-12 00:36:37,734 iteration 5061 : loss : 0.015647, loss_ce: 0.006311
2022-01-12 00:36:39,301 iteration 5062 : loss : 0.030514, loss_ce: 0.008729
2022-01-12 00:36:40,916 iteration 5063 : loss : 0.022296, loss_ce: 0.008949
2022-01-12 00:36:42,443 iteration 5064 : loss : 0.017263, loss_ce: 0.006363
2022-01-12 00:36:43,971 iteration 5065 : loss : 0.027999, loss_ce: 0.009955
2022-01-12 00:36:45,541 iteration 5066 : loss : 0.015215, loss_ce: 0.007660
 74%|█████████████████████▌       | 298/400 [2:23:49<47:24, 27.88s/it]2022-01-12 00:36:47,227 iteration 5067 : loss : 0.022154, loss_ce: 0.008083
2022-01-12 00:36:48,809 iteration 5068 : loss : 0.019366, loss_ce: 0.008387
2022-01-12 00:36:50,368 iteration 5069 : loss : 0.018874, loss_ce: 0.006832
2022-01-12 00:36:51,908 iteration 5070 : loss : 0.014368, loss_ce: 0.004406
2022-01-12 00:36:53,515 iteration 5071 : loss : 0.034460, loss_ce: 0.016127
2022-01-12 00:36:54,966 iteration 5072 : loss : 0.013877, loss_ce: 0.004905
2022-01-12 00:36:56,515 iteration 5073 : loss : 0.016996, loss_ce: 0.009152
2022-01-12 00:36:58,098 iteration 5074 : loss : 0.020069, loss_ce: 0.006162
2022-01-12 00:36:59,663 iteration 5075 : loss : 0.019487, loss_ce: 0.009252
2022-01-12 00:37:01,276 iteration 5076 : loss : 0.023088, loss_ce: 0.011770
2022-01-12 00:37:02,813 iteration 5077 : loss : 0.016809, loss_ce: 0.005906
2022-01-12 00:37:04,391 iteration 5078 : loss : 0.013961, loss_ce: 0.004081
2022-01-12 00:37:05,914 iteration 5079 : loss : 0.017570, loss_ce: 0.007614
2022-01-12 00:37:07,475 iteration 5080 : loss : 0.021748, loss_ce: 0.007917
2022-01-12 00:37:09,069 iteration 5081 : loss : 0.023167, loss_ce: 0.011664
2022-01-12 00:37:10,688 iteration 5082 : loss : 0.020241, loss_ce: 0.009493
2022-01-12 00:37:12,266 iteration 5083 : loss : 0.020960, loss_ce: 0.006709
 75%|█████████████████████▋       | 299/400 [2:24:16<46:21, 27.54s/it]2022-01-12 00:37:13,833 iteration 5084 : loss : 0.023188, loss_ce: 0.007570
2022-01-12 00:37:15,390 iteration 5085 : loss : 0.022330, loss_ce: 0.007191
2022-01-12 00:37:16,931 iteration 5086 : loss : 0.012864, loss_ce: 0.004859
2022-01-12 00:37:18,550 iteration 5087 : loss : 0.026194, loss_ce: 0.010469
2022-01-12 00:37:20,134 iteration 5088 : loss : 0.021185, loss_ce: 0.007307
2022-01-12 00:37:21,667 iteration 5089 : loss : 0.013861, loss_ce: 0.004665
2022-01-12 00:37:23,211 iteration 5090 : loss : 0.017140, loss_ce: 0.005433
2022-01-12 00:37:24,859 iteration 5091 : loss : 0.040750, loss_ce: 0.020407
2022-01-12 00:37:26,410 iteration 5092 : loss : 0.015113, loss_ce: 0.006323
2022-01-12 00:37:27,942 iteration 5093 : loss : 0.015725, loss_ce: 0.006892
2022-01-12 00:37:29,452 iteration 5094 : loss : 0.013871, loss_ce: 0.004848
2022-01-12 00:37:31,057 iteration 5095 : loss : 0.027805, loss_ce: 0.010243
2022-01-12 00:37:32,562 iteration 5096 : loss : 0.014222, loss_ce: 0.004796
2022-01-12 00:37:34,085 iteration 5097 : loss : 0.013220, loss_ce: 0.006464
2022-01-12 00:37:35,664 iteration 5098 : loss : 0.027958, loss_ce: 0.010583
2022-01-12 00:37:37,211 iteration 5099 : loss : 0.020497, loss_ce: 0.006619
2022-01-12 00:37:37,211 Training Data Eval:
2022-01-12 00:37:45,202   Average segmentation loss on training set: 0.0110
2022-01-12 00:37:45,202 Validation Data Eval:
2022-01-12 00:37:47,951   Average segmentation loss on validation set: 0.0642
2022-01-12 00:37:53,762 Found new lowest validation loss at iteration 5099! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-12 00:37:55,219 iteration 5100 : loss : 0.013322, loss_ce: 0.006165
 75%|█████████████████████▊       | 300/400 [2:24:59<53:36, 32.16s/it]2022-01-12 00:37:56,730 iteration 5101 : loss : 0.015279, loss_ce: 0.006828
2022-01-12 00:37:58,157 iteration 5102 : loss : 0.016263, loss_ce: 0.007094
2022-01-12 00:37:59,606 iteration 5103 : loss : 0.020704, loss_ce: 0.008838
2022-01-12 00:38:01,140 iteration 5104 : loss : 0.019736, loss_ce: 0.008126
2022-01-12 00:38:02,554 iteration 5105 : loss : 0.016853, loss_ce: 0.005674
2022-01-12 00:38:04,092 iteration 5106 : loss : 0.025995, loss_ce: 0.010035
2022-01-12 00:38:05,510 iteration 5107 : loss : 0.014759, loss_ce: 0.005471
2022-01-12 00:38:07,008 iteration 5108 : loss : 0.024022, loss_ce: 0.008542
2022-01-12 00:38:08,486 iteration 5109 : loss : 0.027613, loss_ce: 0.007559
2022-01-12 00:38:09,987 iteration 5110 : loss : 0.026777, loss_ce: 0.011573
2022-01-12 00:38:11,585 iteration 5111 : loss : 0.021835, loss_ce: 0.008097
2022-01-12 00:38:13,111 iteration 5112 : loss : 0.019727, loss_ce: 0.005692
2022-01-12 00:38:14,739 iteration 5113 : loss : 0.060064, loss_ce: 0.008304
2022-01-12 00:38:16,299 iteration 5114 : loss : 0.017389, loss_ce: 0.008472
2022-01-12 00:38:17,827 iteration 5115 : loss : 0.023749, loss_ce: 0.007923
2022-01-12 00:38:19,370 iteration 5116 : loss : 0.015106, loss_ce: 0.006285
2022-01-12 00:38:20,995 iteration 5117 : loss : 0.021743, loss_ce: 0.010793
 75%|█████████████████████▊       | 301/400 [2:25:25<49:54, 30.25s/it]2022-01-12 00:38:22,611 iteration 5118 : loss : 0.014174, loss_ce: 0.005229
2022-01-12 00:38:24,183 iteration 5119 : loss : 0.020855, loss_ce: 0.006919
2022-01-12 00:38:25,715 iteration 5120 : loss : 0.026560, loss_ce: 0.006438
2022-01-12 00:38:27,278 iteration 5121 : loss : 0.019165, loss_ce: 0.008392
2022-01-12 00:38:28,808 iteration 5122 : loss : 0.020933, loss_ce: 0.008562
2022-01-12 00:38:30,361 iteration 5123 : loss : 0.023694, loss_ce: 0.006394
2022-01-12 00:38:31,955 iteration 5124 : loss : 0.016282, loss_ce: 0.006286
2022-01-12 00:38:33,434 iteration 5125 : loss : 0.016994, loss_ce: 0.008144
2022-01-12 00:38:34,950 iteration 5126 : loss : 0.020743, loss_ce: 0.007554
2022-01-12 00:38:36,560 iteration 5127 : loss : 0.024970, loss_ce: 0.007723
2022-01-12 00:38:38,094 iteration 5128 : loss : 0.014947, loss_ce: 0.005791
2022-01-12 00:38:39,678 iteration 5129 : loss : 0.019405, loss_ce: 0.006123
2022-01-12 00:38:41,249 iteration 5130 : loss : 0.018930, loss_ce: 0.005973
2022-01-12 00:38:42,747 iteration 5131 : loss : 0.012755, loss_ce: 0.004567
2022-01-12 00:38:44,303 iteration 5132 : loss : 0.018703, loss_ce: 0.008493
2022-01-12 00:38:45,835 iteration 5133 : loss : 0.018679, loss_ce: 0.005435
2022-01-12 00:38:47,347 iteration 5134 : loss : 0.017976, loss_ce: 0.007287
 76%|█████████████████████▉       | 302/400 [2:25:51<47:29, 29.08s/it]2022-01-12 00:38:49,087 iteration 5135 : loss : 0.040979, loss_ce: 0.015954
2022-01-12 00:38:50,650 iteration 5136 : loss : 0.029430, loss_ce: 0.008210
2022-01-12 00:38:52,268 iteration 5137 : loss : 0.043018, loss_ce: 0.014567
2022-01-12 00:38:53,804 iteration 5138 : loss : 0.021063, loss_ce: 0.008230
2022-01-12 00:38:55,373 iteration 5139 : loss : 0.017054, loss_ce: 0.005739
2022-01-12 00:38:57,022 iteration 5140 : loss : 0.022648, loss_ce: 0.010411
2022-01-12 00:38:58,552 iteration 5141 : loss : 0.019270, loss_ce: 0.006234
2022-01-12 00:39:00,140 iteration 5142 : loss : 0.020239, loss_ce: 0.006978
2022-01-12 00:39:01,636 iteration 5143 : loss : 0.015622, loss_ce: 0.005890
2022-01-12 00:39:03,240 iteration 5144 : loss : 0.025184, loss_ce: 0.012107
2022-01-12 00:39:04,736 iteration 5145 : loss : 0.012856, loss_ce: 0.004259
2022-01-12 00:39:06,365 iteration 5146 : loss : 0.019880, loss_ce: 0.008650
2022-01-12 00:39:07,984 iteration 5147 : loss : 0.023724, loss_ce: 0.007739
2022-01-12 00:39:09,541 iteration 5148 : loss : 0.016814, loss_ce: 0.007140
2022-01-12 00:39:11,089 iteration 5149 : loss : 0.032808, loss_ce: 0.007782
2022-01-12 00:39:12,642 iteration 5150 : loss : 0.015276, loss_ce: 0.005748
2022-01-12 00:39:14,225 iteration 5151 : loss : 0.023638, loss_ce: 0.004676
 76%|█████████████████████▉       | 303/400 [2:26:18<45:56, 28.42s/it]2022-01-12 00:39:15,788 iteration 5152 : loss : 0.016696, loss_ce: 0.003459
2022-01-12 00:39:17,269 iteration 5153 : loss : 0.012137, loss_ce: 0.004526
2022-01-12 00:39:18,895 iteration 5154 : loss : 0.026635, loss_ce: 0.012789
2022-01-12 00:39:20,407 iteration 5155 : loss : 0.012661, loss_ce: 0.004442
2022-01-12 00:39:21,948 iteration 5156 : loss : 0.018886, loss_ce: 0.007786
2022-01-12 00:39:23,451 iteration 5157 : loss : 0.012984, loss_ce: 0.003974
2022-01-12 00:39:24,989 iteration 5158 : loss : 0.016811, loss_ce: 0.005620
2022-01-12 00:39:26,604 iteration 5159 : loss : 0.037457, loss_ce: 0.013845
2022-01-12 00:39:28,111 iteration 5160 : loss : 0.019150, loss_ce: 0.006760
2022-01-12 00:39:29,663 iteration 5161 : loss : 0.017251, loss_ce: 0.006669
2022-01-12 00:39:31,173 iteration 5162 : loss : 0.014109, loss_ce: 0.006006
2022-01-12 00:39:32,750 iteration 5163 : loss : 0.033063, loss_ce: 0.012814
2022-01-12 00:39:34,331 iteration 5164 : loss : 0.027787, loss_ce: 0.012478
2022-01-12 00:39:35,833 iteration 5165 : loss : 0.018398, loss_ce: 0.006176
2022-01-12 00:39:37,393 iteration 5166 : loss : 0.019095, loss_ce: 0.009465
2022-01-12 00:39:38,963 iteration 5167 : loss : 0.041682, loss_ce: 0.014879
2022-01-12 00:39:40,580 iteration 5168 : loss : 0.022817, loss_ce: 0.008826
 76%|██████████████████████       | 304/400 [2:26:44<44:28, 27.80s/it]2022-01-12 00:39:42,279 iteration 5169 : loss : 0.025660, loss_ce: 0.010195
2022-01-12 00:39:43,781 iteration 5170 : loss : 0.018860, loss_ce: 0.006486
2022-01-12 00:39:45,406 iteration 5171 : loss : 0.018932, loss_ce: 0.008665
2022-01-12 00:39:46,908 iteration 5172 : loss : 0.014470, loss_ce: 0.004172
2022-01-12 00:39:48,358 iteration 5173 : loss : 0.012620, loss_ce: 0.005386
2022-01-12 00:39:49,995 iteration 5174 : loss : 0.022285, loss_ce: 0.008433
2022-01-12 00:39:51,536 iteration 5175 : loss : 0.018041, loss_ce: 0.008251
2022-01-12 00:39:53,231 iteration 5176 : loss : 0.030472, loss_ce: 0.015466
2022-01-12 00:39:54,753 iteration 5177 : loss : 0.016718, loss_ce: 0.007086
2022-01-12 00:39:56,302 iteration 5178 : loss : 0.016275, loss_ce: 0.005782
2022-01-12 00:39:57,921 iteration 5179 : loss : 0.020274, loss_ce: 0.006616
2022-01-12 00:39:59,488 iteration 5180 : loss : 0.016975, loss_ce: 0.005993
2022-01-12 00:40:01,007 iteration 5181 : loss : 0.011867, loss_ce: 0.003013
2022-01-12 00:40:02,579 iteration 5182 : loss : 0.022697, loss_ce: 0.010033
2022-01-12 00:40:04,116 iteration 5183 : loss : 0.024208, loss_ce: 0.011011
2022-01-12 00:40:05,649 iteration 5184 : loss : 0.021282, loss_ce: 0.007812
2022-01-12 00:40:05,649 Training Data Eval:
2022-01-12 00:40:13,647   Average segmentation loss on training set: 0.0117
2022-01-12 00:40:13,647 Validation Data Eval:
2022-01-12 00:40:16,400   Average segmentation loss on validation set: 0.0701
2022-01-12 00:40:17,965 iteration 5185 : loss : 0.027094, loss_ce: 0.010291
 76%|██████████████████████       | 305/400 [2:27:22<48:34, 30.68s/it]2022-01-12 00:40:19,601 iteration 5186 : loss : 0.015765, loss_ce: 0.007265
2022-01-12 00:40:21,128 iteration 5187 : loss : 0.017841, loss_ce: 0.007379
2022-01-12 00:40:22,664 iteration 5188 : loss : 0.016480, loss_ce: 0.007479
2022-01-12 00:40:24,190 iteration 5189 : loss : 0.018894, loss_ce: 0.009585
2022-01-12 00:40:25,708 iteration 5190 : loss : 0.022610, loss_ce: 0.012254
2022-01-12 00:40:27,313 iteration 5191 : loss : 0.023874, loss_ce: 0.008146
2022-01-12 00:40:28,904 iteration 5192 : loss : 0.020868, loss_ce: 0.007206
2022-01-12 00:40:30,422 iteration 5193 : loss : 0.011046, loss_ce: 0.003660
2022-01-12 00:40:32,008 iteration 5194 : loss : 0.025455, loss_ce: 0.010351
2022-01-12 00:40:33,547 iteration 5195 : loss : 0.016836, loss_ce: 0.005617
2022-01-12 00:40:35,024 iteration 5196 : loss : 0.013596, loss_ce: 0.003263
2022-01-12 00:40:36,531 iteration 5197 : loss : 0.013145, loss_ce: 0.004228
2022-01-12 00:40:38,061 iteration 5198 : loss : 0.017473, loss_ce: 0.007651
2022-01-12 00:40:39,653 iteration 5199 : loss : 0.012421, loss_ce: 0.004590
2022-01-12 00:40:41,207 iteration 5200 : loss : 0.020757, loss_ce: 0.006130
2022-01-12 00:40:42,783 iteration 5201 : loss : 0.025184, loss_ce: 0.009618
2022-01-12 00:40:44,422 iteration 5202 : loss : 0.026938, loss_ce: 0.014808
 76%|██████████████████████▏      | 306/400 [2:27:48<46:04, 29.41s/it]2022-01-12 00:40:45,998 iteration 5203 : loss : 0.013452, loss_ce: 0.004919
2022-01-12 00:40:47,531 iteration 5204 : loss : 0.036927, loss_ce: 0.014901
2022-01-12 00:40:49,211 iteration 5205 : loss : 0.021399, loss_ce: 0.008776
2022-01-12 00:40:50,770 iteration 5206 : loss : 0.028579, loss_ce: 0.009551
2022-01-12 00:40:52,337 iteration 5207 : loss : 0.017971, loss_ce: 0.007822
2022-01-12 00:40:53,955 iteration 5208 : loss : 0.018301, loss_ce: 0.007141
2022-01-12 00:40:55,466 iteration 5209 : loss : 0.015340, loss_ce: 0.006733
2022-01-12 00:40:57,070 iteration 5210 : loss : 0.017422, loss_ce: 0.006686
2022-01-12 00:40:58,643 iteration 5211 : loss : 0.023292, loss_ce: 0.009237
2022-01-12 00:41:00,205 iteration 5212 : loss : 0.021032, loss_ce: 0.008668
2022-01-12 00:41:01,675 iteration 5213 : loss : 0.014756, loss_ce: 0.005666
2022-01-12 00:41:03,326 iteration 5214 : loss : 0.038199, loss_ce: 0.020365
2022-01-12 00:41:04,919 iteration 5215 : loss : 0.021651, loss_ce: 0.007341
2022-01-12 00:41:06,530 iteration 5216 : loss : 0.022936, loss_ce: 0.008259
2022-01-12 00:41:08,174 iteration 5217 : loss : 0.017185, loss_ce: 0.006135
2022-01-12 00:41:09,696 iteration 5218 : loss : 0.025267, loss_ce: 0.003927
2022-01-12 00:41:11,350 iteration 5219 : loss : 0.025256, loss_ce: 0.009591
 77%|██████████████████████▎      | 307/400 [2:28:15<44:25, 28.67s/it]2022-01-12 00:41:12,932 iteration 5220 : loss : 0.018237, loss_ce: 0.005776
2022-01-12 00:41:14,475 iteration 5221 : loss : 0.018024, loss_ce: 0.005819
2022-01-12 00:41:15,986 iteration 5222 : loss : 0.013978, loss_ce: 0.005107
2022-01-12 00:41:17,594 iteration 5223 : loss : 0.025504, loss_ce: 0.010556
2022-01-12 00:41:19,144 iteration 5224 : loss : 0.018621, loss_ce: 0.005887
2022-01-12 00:41:20,755 iteration 5225 : loss : 0.013640, loss_ce: 0.004299
2022-01-12 00:41:22,297 iteration 5226 : loss : 0.020041, loss_ce: 0.008592
2022-01-12 00:41:23,932 iteration 5227 : loss : 0.017409, loss_ce: 0.007863
2022-01-12 00:41:25,458 iteration 5228 : loss : 0.014007, loss_ce: 0.004380
2022-01-12 00:41:27,122 iteration 5229 : loss : 0.021938, loss_ce: 0.009853
2022-01-12 00:41:28,576 iteration 5230 : loss : 0.012397, loss_ce: 0.005440
2022-01-12 00:41:30,161 iteration 5231 : loss : 0.022803, loss_ce: 0.008980
2022-01-12 00:41:31,732 iteration 5232 : loss : 0.024749, loss_ce: 0.007364
2022-01-12 00:41:33,336 iteration 5233 : loss : 0.018325, loss_ce: 0.006806
2022-01-12 00:41:34,865 iteration 5234 : loss : 0.016222, loss_ce: 0.007266
2022-01-12 00:41:36,435 iteration 5235 : loss : 0.021811, loss_ce: 0.007204
2022-01-12 00:41:38,037 iteration 5236 : loss : 0.020773, loss_ce: 0.007053
 77%|██████████████████████▎      | 308/400 [2:28:42<43:02, 28.07s/it]2022-01-12 00:41:39,525 iteration 5237 : loss : 0.012468, loss_ce: 0.005055
2022-01-12 00:41:41,123 iteration 5238 : loss : 0.014908, loss_ce: 0.006475
2022-01-12 00:41:42,706 iteration 5239 : loss : 0.018008, loss_ce: 0.004838
2022-01-12 00:41:44,296 iteration 5240 : loss : 0.173636, loss_ce: 0.004364
2022-01-12 00:41:45,837 iteration 5241 : loss : 0.012995, loss_ce: 0.004951
2022-01-12 00:41:47,394 iteration 5242 : loss : 0.027636, loss_ce: 0.010902
2022-01-12 00:41:49,034 iteration 5243 : loss : 0.037797, loss_ce: 0.012344
2022-01-12 00:41:50,616 iteration 5244 : loss : 0.016510, loss_ce: 0.008018
2022-01-12 00:41:52,186 iteration 5245 : loss : 0.016351, loss_ce: 0.005590
2022-01-12 00:41:53,768 iteration 5246 : loss : 0.018176, loss_ce: 0.005698
2022-01-12 00:41:55,261 iteration 5247 : loss : 0.012632, loss_ce: 0.005040
2022-01-12 00:41:56,866 iteration 5248 : loss : 0.014934, loss_ce: 0.004718
2022-01-12 00:41:58,490 iteration 5249 : loss : 0.025537, loss_ce: 0.009911
2022-01-12 00:42:00,081 iteration 5250 : loss : 0.018755, loss_ce: 0.009304
2022-01-12 00:42:01,578 iteration 5251 : loss : 0.012883, loss_ce: 0.004281
2022-01-12 00:42:03,147 iteration 5252 : loss : 0.014428, loss_ce: 0.005738
2022-01-12 00:42:04,739 iteration 5253 : loss : 0.030078, loss_ce: 0.007934
 77%|██████████████████████▍      | 309/400 [2:29:08<41:57, 27.66s/it]2022-01-12 00:42:06,322 iteration 5254 : loss : 0.011881, loss_ce: 0.003988
2022-01-12 00:42:07,876 iteration 5255 : loss : 0.019392, loss_ce: 0.010703
2022-01-12 00:42:09,388 iteration 5256 : loss : 0.014801, loss_ce: 0.006066
2022-01-12 00:42:10,962 iteration 5257 : loss : 0.015830, loss_ce: 0.006282
2022-01-12 00:42:12,565 iteration 5258 : loss : 0.023265, loss_ce: 0.008286
2022-01-12 00:42:14,083 iteration 5259 : loss : 0.017156, loss_ce: 0.004463
2022-01-12 00:42:15,663 iteration 5260 : loss : 0.014674, loss_ce: 0.005733
2022-01-12 00:42:17,247 iteration 5261 : loss : 0.021612, loss_ce: 0.008908
2022-01-12 00:42:18,800 iteration 5262 : loss : 0.011871, loss_ce: 0.004653
2022-01-12 00:42:20,369 iteration 5263 : loss : 0.017404, loss_ce: 0.007414
2022-01-12 00:42:21,886 iteration 5264 : loss : 0.011917, loss_ce: 0.004886
2022-01-12 00:42:23,498 iteration 5265 : loss : 0.026087, loss_ce: 0.009480
2022-01-12 00:42:25,134 iteration 5266 : loss : 0.020008, loss_ce: 0.006410
2022-01-12 00:42:26,681 iteration 5267 : loss : 0.021169, loss_ce: 0.006595
2022-01-12 00:42:28,259 iteration 5268 : loss : 0.020552, loss_ce: 0.005875
2022-01-12 00:42:29,929 iteration 5269 : loss : 0.034256, loss_ce: 0.009806
2022-01-12 00:42:29,929 Training Data Eval:
2022-01-12 00:42:37,927   Average segmentation loss on training set: 0.0096
2022-01-12 00:42:37,927 Validation Data Eval:
2022-01-12 00:42:40,683   Average segmentation loss on validation set: 0.0771
2022-01-12 00:42:42,243 iteration 5270 : loss : 0.014578, loss_ce: 0.006448
 78%|██████████████████████▍      | 310/400 [2:29:46<45:55, 30.61s/it]2022-01-12 00:42:43,826 iteration 5271 : loss : 0.022705, loss_ce: 0.008852
2022-01-12 00:42:45,421 iteration 5272 : loss : 0.021102, loss_ce: 0.009668
2022-01-12 00:42:46,961 iteration 5273 : loss : 0.013425, loss_ce: 0.003819
2022-01-12 00:42:48,511 iteration 5274 : loss : 0.016672, loss_ce: 0.006741
2022-01-12 00:42:50,010 iteration 5275 : loss : 0.012791, loss_ce: 0.005701
2022-01-12 00:42:51,598 iteration 5276 : loss : 0.021974, loss_ce: 0.007987
2022-01-12 00:42:53,171 iteration 5277 : loss : 0.020883, loss_ce: 0.007823
2022-01-12 00:42:54,660 iteration 5278 : loss : 0.014062, loss_ce: 0.005784
2022-01-12 00:42:56,229 iteration 5279 : loss : 0.017242, loss_ce: 0.007518
2022-01-12 00:42:57,906 iteration 5280 : loss : 0.022328, loss_ce: 0.010130
2022-01-12 00:42:59,445 iteration 5281 : loss : 0.017340, loss_ce: 0.005423
2022-01-12 00:43:00,953 iteration 5282 : loss : 0.011963, loss_ce: 0.003584
2022-01-12 00:43:02,518 iteration 5283 : loss : 0.022551, loss_ce: 0.006827
2022-01-12 00:43:04,098 iteration 5284 : loss : 0.028185, loss_ce: 0.007532
2022-01-12 00:43:05,654 iteration 5285 : loss : 0.015282, loss_ce: 0.006683
2022-01-12 00:43:07,140 iteration 5286 : loss : 0.012754, loss_ce: 0.005093
2022-01-12 00:43:08,687 iteration 5287 : loss : 0.017057, loss_ce: 0.004391
 78%|██████████████████████▌      | 311/400 [2:30:12<43:33, 29.36s/it]2022-01-12 00:43:10,315 iteration 5288 : loss : 0.031099, loss_ce: 0.012072
2022-01-12 00:43:11,894 iteration 5289 : loss : 0.021538, loss_ce: 0.007844
2022-01-12 00:43:13,429 iteration 5290 : loss : 0.020408, loss_ce: 0.004347
2022-01-12 00:43:14,966 iteration 5291 : loss : 0.025975, loss_ce: 0.011132
2022-01-12 00:43:16,564 iteration 5292 : loss : 0.025737, loss_ce: 0.010721
2022-01-12 00:43:18,141 iteration 5293 : loss : 0.018219, loss_ce: 0.007620
2022-01-12 00:43:19,743 iteration 5294 : loss : 0.021306, loss_ce: 0.007961
2022-01-12 00:43:21,385 iteration 5295 : loss : 0.032571, loss_ce: 0.011917
2022-01-12 00:43:22,972 iteration 5296 : loss : 0.021145, loss_ce: 0.005643
2022-01-12 00:43:24,564 iteration 5297 : loss : 0.015146, loss_ce: 0.004914
2022-01-12 00:43:26,161 iteration 5298 : loss : 0.029530, loss_ce: 0.010820
2022-01-12 00:43:27,753 iteration 5299 : loss : 0.015933, loss_ce: 0.005668
2022-01-12 00:43:29,366 iteration 5300 : loss : 0.020460, loss_ce: 0.006924
2022-01-12 00:43:30,868 iteration 5301 : loss : 0.016097, loss_ce: 0.005728
2022-01-12 00:43:32,376 iteration 5302 : loss : 0.014488, loss_ce: 0.007241
2022-01-12 00:43:33,893 iteration 5303 : loss : 0.016650, loss_ce: 0.006581
2022-01-12 00:43:35,391 iteration 5304 : loss : 0.018284, loss_ce: 0.006770
 78%|██████████████████████▌      | 312/400 [2:30:39<41:53, 28.57s/it]2022-01-12 00:43:37,003 iteration 5305 : loss : 0.022027, loss_ce: 0.008528
2022-01-12 00:43:38,645 iteration 5306 : loss : 0.025491, loss_ce: 0.010721
2022-01-12 00:43:40,301 iteration 5307 : loss : 0.018316, loss_ce: 0.006366
2022-01-12 00:43:41,922 iteration 5308 : loss : 0.023799, loss_ce: 0.008095
2022-01-12 00:43:43,535 iteration 5309 : loss : 0.029027, loss_ce: 0.008367
2022-01-12 00:43:45,067 iteration 5310 : loss : 0.014273, loss_ce: 0.006530
2022-01-12 00:43:46,598 iteration 5311 : loss : 0.018881, loss_ce: 0.005535
2022-01-12 00:43:48,219 iteration 5312 : loss : 0.020369, loss_ce: 0.009242
2022-01-12 00:43:49,876 iteration 5313 : loss : 0.023561, loss_ce: 0.007713
2022-01-12 00:43:51,446 iteration 5314 : loss : 0.019906, loss_ce: 0.006453
2022-01-12 00:43:53,056 iteration 5315 : loss : 0.028689, loss_ce: 0.007091
2022-01-12 00:43:54,651 iteration 5316 : loss : 0.017586, loss_ce: 0.006751
2022-01-12 00:43:56,213 iteration 5317 : loss : 0.020850, loss_ce: 0.010374
2022-01-12 00:43:57,741 iteration 5318 : loss : 0.013814, loss_ce: 0.004989
2022-01-12 00:43:59,351 iteration 5319 : loss : 0.016014, loss_ce: 0.006357
2022-01-12 00:44:00,943 iteration 5320 : loss : 0.017134, loss_ce: 0.007302
2022-01-12 00:44:02,564 iteration 5321 : loss : 0.023593, loss_ce: 0.010106
 78%|██████████████████████▋      | 313/400 [2:31:06<40:50, 28.17s/it]2022-01-12 00:44:04,163 iteration 5322 : loss : 0.013874, loss_ce: 0.005836
2022-01-12 00:44:05,732 iteration 5323 : loss : 0.021805, loss_ce: 0.009444
2022-01-12 00:44:07,231 iteration 5324 : loss : 0.015244, loss_ce: 0.005166
2022-01-12 00:44:08,759 iteration 5325 : loss : 0.015473, loss_ce: 0.003730
2022-01-12 00:44:10,354 iteration 5326 : loss : 0.023320, loss_ce: 0.008725
2022-01-12 00:44:11,883 iteration 5327 : loss : 0.015176, loss_ce: 0.005358
2022-01-12 00:44:13,431 iteration 5328 : loss : 0.021425, loss_ce: 0.014520
2022-01-12 00:44:15,036 iteration 5329 : loss : 0.024208, loss_ce: 0.008621
2022-01-12 00:44:16,608 iteration 5330 : loss : 0.023996, loss_ce: 0.009800
2022-01-12 00:44:18,156 iteration 5331 : loss : 0.015424, loss_ce: 0.004983
2022-01-12 00:44:19,721 iteration 5332 : loss : 0.019945, loss_ce: 0.007914
2022-01-12 00:44:21,324 iteration 5333 : loss : 0.020227, loss_ce: 0.010861
2022-01-12 00:44:22,903 iteration 5334 : loss : 0.027076, loss_ce: 0.012324
2022-01-12 00:44:24,497 iteration 5335 : loss : 0.017104, loss_ce: 0.005304
2022-01-12 00:44:26,066 iteration 5336 : loss : 0.015000, loss_ce: 0.005676
2022-01-12 00:44:27,593 iteration 5337 : loss : 0.015083, loss_ce: 0.005845
2022-01-12 00:44:29,120 iteration 5338 : loss : 0.013684, loss_ce: 0.005442
 78%|██████████████████████▊      | 314/400 [2:31:33<39:38, 27.66s/it]2022-01-12 00:44:30,862 iteration 5339 : loss : 0.028346, loss_ce: 0.011879
2022-01-12 00:44:32,469 iteration 5340 : loss : 0.020508, loss_ce: 0.010830
2022-01-12 00:44:34,050 iteration 5341 : loss : 0.020484, loss_ce: 0.003496
2022-01-12 00:44:35,614 iteration 5342 : loss : 0.015493, loss_ce: 0.008095
2022-01-12 00:44:37,160 iteration 5343 : loss : 0.013341, loss_ce: 0.004487
2022-01-12 00:44:38,750 iteration 5344 : loss : 0.025279, loss_ce: 0.010703
2022-01-12 00:44:40,302 iteration 5345 : loss : 0.016174, loss_ce: 0.005589
2022-01-12 00:44:41,887 iteration 5346 : loss : 0.031491, loss_ce: 0.010717
2022-01-12 00:44:43,480 iteration 5347 : loss : 0.013926, loss_ce: 0.003886
2022-01-12 00:44:45,078 iteration 5348 : loss : 0.017374, loss_ce: 0.004111
2022-01-12 00:44:46,650 iteration 5349 : loss : 0.021047, loss_ce: 0.008772
2022-01-12 00:44:48,154 iteration 5350 : loss : 0.012853, loss_ce: 0.004933
2022-01-12 00:44:49,667 iteration 5351 : loss : 0.025097, loss_ce: 0.012225
2022-01-12 00:44:51,280 iteration 5352 : loss : 0.019089, loss_ce: 0.007674
2022-01-12 00:44:52,878 iteration 5353 : loss : 0.012944, loss_ce: 0.004428
2022-01-12 00:44:54,413 iteration 5354 : loss : 0.015244, loss_ce: 0.007584
2022-01-12 00:44:54,414 Training Data Eval:
2022-01-12 00:45:02,394   Average segmentation loss on training set: 0.0107
2022-01-12 00:45:02,394 Validation Data Eval:
2022-01-12 00:45:05,147   Average segmentation loss on validation set: 0.0742
2022-01-12 00:45:06,661 iteration 5355 : loss : 0.016871, loss_ce: 0.006348
 79%|██████████████████████▊      | 315/400 [2:32:10<43:23, 30.63s/it]2022-01-12 00:45:08,283 iteration 5356 : loss : 0.017437, loss_ce: 0.007866
2022-01-12 00:45:09,791 iteration 5357 : loss : 0.017527, loss_ce: 0.004034
2022-01-12 00:45:11,472 iteration 5358 : loss : 0.036300, loss_ce: 0.016528
2022-01-12 00:45:12,966 iteration 5359 : loss : 0.011602, loss_ce: 0.004317
2022-01-12 00:45:14,546 iteration 5360 : loss : 0.018812, loss_ce: 0.008737
2022-01-12 00:45:16,126 iteration 5361 : loss : 0.023358, loss_ce: 0.008594
2022-01-12 00:45:17,721 iteration 5362 : loss : 0.017886, loss_ce: 0.005527
2022-01-12 00:45:19,294 iteration 5363 : loss : 0.020171, loss_ce: 0.010173
2022-01-12 00:45:20,866 iteration 5364 : loss : 0.022376, loss_ce: 0.004689
2022-01-12 00:45:22,410 iteration 5365 : loss : 0.015796, loss_ce: 0.006152
2022-01-12 00:45:23,999 iteration 5366 : loss : 0.018386, loss_ce: 0.007529
2022-01-12 00:45:25,610 iteration 5367 : loss : 0.014577, loss_ce: 0.006338
2022-01-12 00:45:27,156 iteration 5368 : loss : 0.013584, loss_ce: 0.006287
2022-01-12 00:45:28,725 iteration 5369 : loss : 0.014657, loss_ce: 0.005609
2022-01-12 00:45:30,270 iteration 5370 : loss : 0.011682, loss_ce: 0.004577
2022-01-12 00:45:31,836 iteration 5371 : loss : 0.020028, loss_ce: 0.009065
2022-01-12 00:45:33,363 iteration 5372 : loss : 0.015670, loss_ce: 0.005466
 79%|██████████████████████▉      | 316/400 [2:32:37<41:13, 29.45s/it]2022-01-12 00:45:34,963 iteration 5373 : loss : 0.013607, loss_ce: 0.006639
2022-01-12 00:45:36,485 iteration 5374 : loss : 0.016399, loss_ce: 0.007393
2022-01-12 00:45:38,076 iteration 5375 : loss : 0.013076, loss_ce: 0.004469
2022-01-12 00:45:39,624 iteration 5376 : loss : 0.017616, loss_ce: 0.006502
2022-01-12 00:45:41,145 iteration 5377 : loss : 0.012605, loss_ce: 0.004063
2022-01-12 00:45:42,718 iteration 5378 : loss : 0.023770, loss_ce: 0.009069
2022-01-12 00:45:44,222 iteration 5379 : loss : 0.012090, loss_ce: 0.004262
2022-01-12 00:45:45,781 iteration 5380 : loss : 0.016975, loss_ce: 0.007193
2022-01-12 00:45:47,307 iteration 5381 : loss : 0.024192, loss_ce: 0.013401
2022-01-12 00:45:48,890 iteration 5382 : loss : 0.012315, loss_ce: 0.003572
2022-01-12 00:45:50,523 iteration 5383 : loss : 0.022217, loss_ce: 0.010154
2022-01-12 00:45:52,220 iteration 5384 : loss : 0.023455, loss_ce: 0.010216
2022-01-12 00:45:53,746 iteration 5385 : loss : 0.016226, loss_ce: 0.005729
2022-01-12 00:45:55,225 iteration 5386 : loss : 0.012357, loss_ce: 0.004165
2022-01-12 00:45:56,757 iteration 5387 : loss : 0.016041, loss_ce: 0.007966
2022-01-12 00:45:58,364 iteration 5388 : loss : 0.030254, loss_ce: 0.008599
2022-01-12 00:46:00,029 iteration 5389 : loss : 0.025403, loss_ce: 0.010063
 79%|██████████████████████▉      | 317/400 [2:33:04<39:35, 28.61s/it]2022-01-12 00:46:01,592 iteration 5390 : loss : 0.028833, loss_ce: 0.006444
2022-01-12 00:46:03,183 iteration 5391 : loss : 0.019312, loss_ce: 0.005765
2022-01-12 00:46:04,789 iteration 5392 : loss : 0.017891, loss_ce: 0.009704
2022-01-12 00:46:06,362 iteration 5393 : loss : 0.029079, loss_ce: 0.008282
2022-01-12 00:46:07,899 iteration 5394 : loss : 0.016152, loss_ce: 0.007501
2022-01-12 00:46:09,483 iteration 5395 : loss : 0.021202, loss_ce: 0.006184
2022-01-12 00:46:10,993 iteration 5396 : loss : 0.014690, loss_ce: 0.007644
2022-01-12 00:46:12,599 iteration 5397 : loss : 0.016761, loss_ce: 0.005402
2022-01-12 00:46:14,163 iteration 5398 : loss : 0.020383, loss_ce: 0.008545
2022-01-12 00:46:15,662 iteration 5399 : loss : 0.015107, loss_ce: 0.004650
2022-01-12 00:46:17,314 iteration 5400 : loss : 0.024077, loss_ce: 0.007835
2022-01-12 00:46:18,912 iteration 5401 : loss : 0.023645, loss_ce: 0.010216
2022-01-12 00:46:20,459 iteration 5402 : loss : 0.015476, loss_ce: 0.004917
2022-01-12 00:46:21,975 iteration 5403 : loss : 0.022817, loss_ce: 0.009674
2022-01-12 00:46:23,495 iteration 5404 : loss : 0.018290, loss_ce: 0.004942
2022-01-12 00:46:25,037 iteration 5405 : loss : 0.015032, loss_ce: 0.006404
2022-01-12 00:46:26,642 iteration 5406 : loss : 0.018238, loss_ce: 0.006134
 80%|███████████████████████      | 318/400 [2:33:30<38:16, 28.01s/it]2022-01-12 00:46:28,271 iteration 5407 : loss : 0.024692, loss_ce: 0.009451
2022-01-12 00:46:29,814 iteration 5408 : loss : 0.026608, loss_ce: 0.009808
2022-01-12 00:46:31,445 iteration 5409 : loss : 0.021629, loss_ce: 0.009287
2022-01-12 00:46:32,972 iteration 5410 : loss : 0.017542, loss_ce: 0.007716
2022-01-12 00:46:34,528 iteration 5411 : loss : 0.016606, loss_ce: 0.006815
2022-01-12 00:46:36,092 iteration 5412 : loss : 0.015869, loss_ce: 0.007434
2022-01-12 00:46:37,625 iteration 5413 : loss : 0.017557, loss_ce: 0.008281
2022-01-12 00:46:39,186 iteration 5414 : loss : 0.016609, loss_ce: 0.006789
2022-01-12 00:46:40,816 iteration 5415 : loss : 0.032266, loss_ce: 0.013132
2022-01-12 00:46:42,305 iteration 5416 : loss : 0.012094, loss_ce: 0.004176
2022-01-12 00:46:43,891 iteration 5417 : loss : 0.015252, loss_ce: 0.004934
2022-01-12 00:46:45,437 iteration 5418 : loss : 0.020204, loss_ce: 0.008070
2022-01-12 00:46:47,026 iteration 5419 : loss : 0.021874, loss_ce: 0.005327
2022-01-12 00:46:48,642 iteration 5420 : loss : 0.018360, loss_ce: 0.007463
2022-01-12 00:46:50,212 iteration 5421 : loss : 0.018524, loss_ce: 0.009199
2022-01-12 00:46:51,654 iteration 5422 : loss : 0.017580, loss_ce: 0.007377
2022-01-12 00:46:53,250 iteration 5423 : loss : 0.031389, loss_ce: 0.006395
 80%|███████████████████████▏     | 319/400 [2:33:57<37:14, 27.59s/it]2022-01-12 00:46:54,906 iteration 5424 : loss : 0.017540, loss_ce: 0.005762
2022-01-12 00:46:56,419 iteration 5425 : loss : 0.013374, loss_ce: 0.004059
2022-01-12 00:46:57,974 iteration 5426 : loss : 0.019419, loss_ce: 0.005853
2022-01-12 00:46:59,451 iteration 5427 : loss : 0.016130, loss_ce: 0.006532
2022-01-12 00:47:00,986 iteration 5428 : loss : 0.038801, loss_ce: 0.014351
2022-01-12 00:47:02,548 iteration 5429 : loss : 0.013552, loss_ce: 0.004649
2022-01-12 00:47:04,106 iteration 5430 : loss : 0.031195, loss_ce: 0.015354
2022-01-12 00:47:05,606 iteration 5431 : loss : 0.014883, loss_ce: 0.005322
2022-01-12 00:47:07,224 iteration 5432 : loss : 0.015220, loss_ce: 0.007473
2022-01-12 00:47:08,835 iteration 5433 : loss : 0.030814, loss_ce: 0.014508
2022-01-12 00:47:10,344 iteration 5434 : loss : 0.021983, loss_ce: 0.010401
2022-01-12 00:47:11,970 iteration 5435 : loss : 0.018428, loss_ce: 0.007418
2022-01-12 00:47:13,675 iteration 5436 : loss : 0.014396, loss_ce: 0.005674
2022-01-12 00:47:15,267 iteration 5437 : loss : 0.045088, loss_ce: 0.008525
2022-01-12 00:47:16,851 iteration 5438 : loss : 0.015775, loss_ce: 0.006833
2022-01-12 00:47:18,383 iteration 5439 : loss : 0.016328, loss_ce: 0.006903
2022-01-12 00:47:18,384 Training Data Eval:
2022-01-12 00:47:26,359   Average segmentation loss on training set: 0.0095
2022-01-12 00:47:26,359 Validation Data Eval:
2022-01-12 00:47:29,106   Average segmentation loss on validation set: 0.0753
2022-01-12 00:47:30,621 iteration 5440 : loss : 0.018594, loss_ce: 0.005854
 80%|███████████████████████▏     | 320/400 [2:34:34<40:42, 30.53s/it]2022-01-12 00:47:32,203 iteration 5441 : loss : 0.015663, loss_ce: 0.006103
2022-01-12 00:47:33,721 iteration 5442 : loss : 0.015843, loss_ce: 0.005283
2022-01-12 00:47:35,277 iteration 5443 : loss : 0.018418, loss_ce: 0.006393
2022-01-12 00:47:36,766 iteration 5444 : loss : 0.014143, loss_ce: 0.004984
2022-01-12 00:47:38,337 iteration 5445 : loss : 0.016915, loss_ce: 0.006053
2022-01-12 00:47:39,862 iteration 5446 : loss : 0.018368, loss_ce: 0.008487
2022-01-12 00:47:41,502 iteration 5447 : loss : 0.021015, loss_ce: 0.007491
2022-01-12 00:47:43,017 iteration 5448 : loss : 0.016927, loss_ce: 0.006164
2022-01-12 00:47:44,592 iteration 5449 : loss : 0.018161, loss_ce: 0.009506
2022-01-12 00:47:46,138 iteration 5450 : loss : 0.017136, loss_ce: 0.006782
2022-01-12 00:47:47,639 iteration 5451 : loss : 0.014883, loss_ce: 0.005059
2022-01-12 00:47:49,208 iteration 5452 : loss : 0.021379, loss_ce: 0.008483
2022-01-12 00:47:50,724 iteration 5453 : loss : 0.013327, loss_ce: 0.005067
2022-01-12 00:47:52,325 iteration 5454 : loss : 0.032672, loss_ce: 0.020438
2022-01-12 00:47:53,903 iteration 5455 : loss : 0.020103, loss_ce: 0.007815
2022-01-12 00:47:55,472 iteration 5456 : loss : 0.024876, loss_ce: 0.004280
2022-01-12 00:47:56,987 iteration 5457 : loss : 0.014169, loss_ce: 0.004550
 80%|███████████████████████▎     | 321/400 [2:35:01<38:33, 29.28s/it]2022-01-12 00:47:58,556 iteration 5458 : loss : 0.025637, loss_ce: 0.008809
2022-01-12 00:48:00,051 iteration 5459 : loss : 0.017043, loss_ce: 0.007732
2022-01-12 00:48:01,622 iteration 5460 : loss : 0.012954, loss_ce: 0.004970
2022-01-12 00:48:03,176 iteration 5461 : loss : 0.020544, loss_ce: 0.007811
2022-01-12 00:48:04,718 iteration 5462 : loss : 0.020637, loss_ce: 0.007325
2022-01-12 00:48:06,297 iteration 5463 : loss : 0.017981, loss_ce: 0.005829
2022-01-12 00:48:07,862 iteration 5464 : loss : 0.018197, loss_ce: 0.008286
2022-01-12 00:48:09,488 iteration 5465 : loss : 0.022664, loss_ce: 0.009054
2022-01-12 00:48:11,100 iteration 5466 : loss : 0.016545, loss_ce: 0.006587
2022-01-12 00:48:12,689 iteration 5467 : loss : 0.030651, loss_ce: 0.013193
2022-01-12 00:48:14,275 iteration 5468 : loss : 0.013939, loss_ce: 0.005408
2022-01-12 00:48:15,777 iteration 5469 : loss : 0.020327, loss_ce: 0.006860
2022-01-12 00:48:17,338 iteration 5470 : loss : 0.009522, loss_ce: 0.003524
2022-01-12 00:48:18,888 iteration 5471 : loss : 0.028881, loss_ce: 0.008309
2022-01-12 00:48:20,481 iteration 5472 : loss : 0.022808, loss_ce: 0.009152
2022-01-12 00:48:21,985 iteration 5473 : loss : 0.012927, loss_ce: 0.004830
2022-01-12 00:48:23,497 iteration 5474 : loss : 0.013926, loss_ce: 0.004438
 80%|███████████████████████▎     | 322/400 [2:35:27<36:58, 28.45s/it]2022-01-12 00:48:25,105 iteration 5475 : loss : 0.017096, loss_ce: 0.004829
2022-01-12 00:48:26,712 iteration 5476 : loss : 0.020836, loss_ce: 0.008730
2022-01-12 00:48:28,252 iteration 5477 : loss : 0.030311, loss_ce: 0.004149
2022-01-12 00:48:29,727 iteration 5478 : loss : 0.015328, loss_ce: 0.005101
2022-01-12 00:48:31,278 iteration 5479 : loss : 0.012609, loss_ce: 0.005471
2022-01-12 00:48:32,824 iteration 5480 : loss : 0.013730, loss_ce: 0.004370
2022-01-12 00:48:34,356 iteration 5481 : loss : 0.014007, loss_ce: 0.005007
2022-01-12 00:48:35,987 iteration 5482 : loss : 0.035530, loss_ce: 0.007771
2022-01-12 00:48:37,458 iteration 5483 : loss : 0.012597, loss_ce: 0.005241
2022-01-12 00:48:39,028 iteration 5484 : loss : 0.021406, loss_ce: 0.007536
2022-01-12 00:48:40,585 iteration 5485 : loss : 0.016949, loss_ce: 0.006583
2022-01-12 00:48:42,144 iteration 5486 : loss : 0.019021, loss_ce: 0.008565
2022-01-12 00:48:43,715 iteration 5487 : loss : 0.017646, loss_ce: 0.007593
2022-01-12 00:48:45,236 iteration 5488 : loss : 0.015331, loss_ce: 0.007239
2022-01-12 00:48:46,748 iteration 5489 : loss : 0.013822, loss_ce: 0.007081
2022-01-12 00:48:48,369 iteration 5490 : loss : 0.019699, loss_ce: 0.007945
2022-01-12 00:48:49,898 iteration 5491 : loss : 0.012553, loss_ce: 0.003612
 81%|███████████████████████▍     | 323/400 [2:35:54<35:43, 27.83s/it]2022-01-12 00:48:51,413 iteration 5492 : loss : 0.014141, loss_ce: 0.005160
2022-01-12 00:48:52,994 iteration 5493 : loss : 0.040900, loss_ce: 0.012051
2022-01-12 00:48:54,503 iteration 5494 : loss : 0.016163, loss_ce: 0.005357
2022-01-12 00:48:56,074 iteration 5495 : loss : 0.015098, loss_ce: 0.004686
2022-01-12 00:48:57,646 iteration 5496 : loss : 0.020331, loss_ce: 0.005061
2022-01-12 00:48:59,211 iteration 5497 : loss : 0.017329, loss_ce: 0.007490
2022-01-12 00:49:00,816 iteration 5498 : loss : 0.022995, loss_ce: 0.007664
2022-01-12 00:49:02,398 iteration 5499 : loss : 0.022186, loss_ce: 0.012279
2022-01-12 00:49:03,890 iteration 5500 : loss : 0.027259, loss_ce: 0.012896
2022-01-12 00:49:05,444 iteration 5501 : loss : 0.018666, loss_ce: 0.008457
2022-01-12 00:49:06,968 iteration 5502 : loss : 0.011170, loss_ce: 0.004762
2022-01-12 00:49:08,513 iteration 5503 : loss : 0.016625, loss_ce: 0.004347
2022-01-12 00:49:10,026 iteration 5504 : loss : 0.017056, loss_ce: 0.006150
2022-01-12 00:49:11,566 iteration 5505 : loss : 0.022252, loss_ce: 0.009930
2022-01-12 00:49:13,143 iteration 5506 : loss : 0.020158, loss_ce: 0.007243
2022-01-12 00:49:14,694 iteration 5507 : loss : 0.014632, loss_ce: 0.006521
2022-01-12 00:49:16,261 iteration 5508 : loss : 0.018573, loss_ce: 0.006118
 81%|███████████████████████▍     | 324/400 [2:36:20<34:41, 27.39s/it]2022-01-12 00:49:17,864 iteration 5509 : loss : 0.033480, loss_ce: 0.008308
2022-01-12 00:49:19,502 iteration 5510 : loss : 0.024075, loss_ce: 0.009020
2022-01-12 00:49:21,040 iteration 5511 : loss : 0.017989, loss_ce: 0.006956
2022-01-12 00:49:22,645 iteration 5512 : loss : 0.024852, loss_ce: 0.010624
2022-01-12 00:49:24,165 iteration 5513 : loss : 0.016164, loss_ce: 0.007457
2022-01-12 00:49:25,648 iteration 5514 : loss : 0.011219, loss_ce: 0.003929
2022-01-12 00:49:27,260 iteration 5515 : loss : 0.035140, loss_ce: 0.009242
2022-01-12 00:49:28,791 iteration 5516 : loss : 0.020031, loss_ce: 0.007706
2022-01-12 00:49:30,354 iteration 5517 : loss : 0.015973, loss_ce: 0.007123
2022-01-12 00:49:31,915 iteration 5518 : loss : 0.026233, loss_ce: 0.008806
2022-01-12 00:49:33,481 iteration 5519 : loss : 0.020765, loss_ce: 0.008055
2022-01-12 00:49:35,037 iteration 5520 : loss : 0.013449, loss_ce: 0.005325
2022-01-12 00:49:36,611 iteration 5521 : loss : 0.023586, loss_ce: 0.009244
2022-01-12 00:49:38,165 iteration 5522 : loss : 0.020632, loss_ce: 0.007859
2022-01-12 00:49:39,694 iteration 5523 : loss : 0.011380, loss_ce: 0.003920
2022-01-12 00:49:41,231 iteration 5524 : loss : 0.019241, loss_ce: 0.007707
2022-01-12 00:49:41,231 Training Data Eval:
2022-01-12 00:49:49,217   Average segmentation loss on training set: 0.0105
2022-01-12 00:49:49,217 Validation Data Eval:
2022-01-12 00:49:51,969   Average segmentation loss on validation set: 0.0606
2022-01-12 00:49:57,816 Found new lowest validation loss at iteration 5524! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed1234.pth
2022-01-12 00:49:59,281 iteration 5525 : loss : 0.019040, loss_ce: 0.009753
 81%|███████████████████████▌     | 325/400 [2:37:03<40:06, 32.08s/it]2022-01-12 00:50:00,830 iteration 5526 : loss : 0.030319, loss_ce: 0.009731
2022-01-12 00:50:02,304 iteration 5527 : loss : 0.020627, loss_ce: 0.009531
2022-01-12 00:50:03,758 iteration 5528 : loss : 0.023349, loss_ce: 0.009289
2022-01-12 00:50:05,249 iteration 5529 : loss : 0.024146, loss_ce: 0.010183
2022-01-12 00:50:06,654 iteration 5530 : loss : 0.018304, loss_ce: 0.009899
2022-01-12 00:50:08,085 iteration 5531 : loss : 0.017655, loss_ce: 0.007832
2022-01-12 00:50:09,499 iteration 5532 : loss : 0.012674, loss_ce: 0.005255
2022-01-12 00:50:11,010 iteration 5533 : loss : 0.019631, loss_ce: 0.005374
2022-01-12 00:50:12,508 iteration 5534 : loss : 0.015183, loss_ce: 0.005800
2022-01-12 00:50:14,121 iteration 5535 : loss : 0.021331, loss_ce: 0.009591
2022-01-12 00:50:15,780 iteration 5536 : loss : 0.013891, loss_ce: 0.005885
2022-01-12 00:50:17,386 iteration 5537 : loss : 0.021946, loss_ce: 0.007295
2022-01-12 00:50:18,966 iteration 5538 : loss : 0.017454, loss_ce: 0.006921
2022-01-12 00:50:20,628 iteration 5539 : loss : 0.018035, loss_ce: 0.003463
2022-01-12 00:50:22,165 iteration 5540 : loss : 0.020471, loss_ce: 0.008441
2022-01-12 00:50:23,744 iteration 5541 : loss : 0.018252, loss_ce: 0.006175
2022-01-12 00:50:25,286 iteration 5542 : loss : 0.013815, loss_ce: 0.005357
 82%|███████████████████████▋     | 326/400 [2:37:29<37:19, 30.26s/it]2022-01-12 00:50:26,918 iteration 5543 : loss : 0.018733, loss_ce: 0.006226
2022-01-12 00:50:28,476 iteration 5544 : loss : 0.013513, loss_ce: 0.005713
2022-01-12 00:50:30,020 iteration 5545 : loss : 0.014281, loss_ce: 0.005521
2022-01-12 00:50:31,546 iteration 5546 : loss : 0.014146, loss_ce: 0.004591
2022-01-12 00:50:33,108 iteration 5547 : loss : 0.013874, loss_ce: 0.004847
2022-01-12 00:50:34,721 iteration 5548 : loss : 0.020898, loss_ce: 0.007139
2022-01-12 00:50:36,234 iteration 5549 : loss : 0.014889, loss_ce: 0.005769
2022-01-12 00:50:37,776 iteration 5550 : loss : 0.015696, loss_ce: 0.006069
2022-01-12 00:50:39,344 iteration 5551 : loss : 0.020274, loss_ce: 0.008264
2022-01-12 00:50:40,853 iteration 5552 : loss : 0.012692, loss_ce: 0.004992
2022-01-12 00:50:42,434 iteration 5553 : loss : 0.018454, loss_ce: 0.006981
2022-01-12 00:50:44,046 iteration 5554 : loss : 0.015950, loss_ce: 0.005066
2022-01-12 00:50:45,661 iteration 5555 : loss : 0.036736, loss_ce: 0.009713
2022-01-12 00:50:47,157 iteration 5556 : loss : 0.014577, loss_ce: 0.005625
2022-01-12 00:50:48,781 iteration 5557 : loss : 0.040545, loss_ce: 0.011670
2022-01-12 00:50:50,364 iteration 5558 : loss : 0.016330, loss_ce: 0.005697
2022-01-12 00:50:51,849 iteration 5559 : loss : 0.011787, loss_ce: 0.005088
 82%|███████████████████████▋     | 327/400 [2:37:55<35:27, 29.15s/it]2022-01-12 00:50:53,610 iteration 5560 : loss : 0.030057, loss_ce: 0.011833
2022-01-12 00:50:55,155 iteration 5561 : loss : 0.015381, loss_ce: 0.005112
2022-01-12 00:50:56,715 iteration 5562 : loss : 0.023757, loss_ce: 0.007368
2022-01-12 00:50:58,274 iteration 5563 : loss : 0.013001, loss_ce: 0.005040
2022-01-12 00:50:59,922 iteration 5564 : loss : 0.022182, loss_ce: 0.006698
2022-01-12 00:51:01,495 iteration 5565 : loss : 0.014358, loss_ce: 0.005766
2022-01-12 00:51:03,107 iteration 5566 : loss : 0.014974, loss_ce: 0.006431
2022-01-12 00:51:04,766 iteration 5567 : loss : 0.017693, loss_ce: 0.008989
2022-01-12 00:51:06,331 iteration 5568 : loss : 0.030239, loss_ce: 0.010975
2022-01-12 00:51:07,946 iteration 5569 : loss : 0.015719, loss_ce: 0.006687
2022-01-12 00:51:09,526 iteration 5570 : loss : 0.012118, loss_ce: 0.004132
2022-01-12 00:51:11,126 iteration 5571 : loss : 0.020743, loss_ce: 0.009215
2022-01-12 00:51:12,684 iteration 5572 : loss : 0.017809, loss_ce: 0.007592
2022-01-12 00:51:14,241 iteration 5573 : loss : 0.014685, loss_ce: 0.004585
2022-01-12 00:51:15,912 iteration 5574 : loss : 0.018654, loss_ce: 0.005827
2022-01-12 00:51:17,385 iteration 5575 : loss : 0.010247, loss_ce: 0.004589
2022-01-12 00:51:18,838 iteration 5576 : loss : 0.011562, loss_ce: 0.003573
 82%|███████████████████████▊     | 328/400 [2:38:22<34:12, 28.50s/it]2022-01-12 00:51:20,436 iteration 5577 : loss : 0.015689, loss_ce: 0.005756
2022-01-12 00:51:21,975 iteration 5578 : loss : 0.021049, loss_ce: 0.006614
2022-01-12 00:51:23,538 iteration 5579 : loss : 0.013842, loss_ce: 0.004176
2022-01-12 00:51:25,146 iteration 5580 : loss : 0.012112, loss_ce: 0.004265
2022-01-12 00:51:26,698 iteration 5581 : loss : 0.012872, loss_ce: 0.004351
2022-01-12 00:51:28,298 iteration 5582 : loss : 0.018287, loss_ce: 0.007638
2022-01-12 00:51:29,852 iteration 5583 : loss : 0.019245, loss_ce: 0.005516
2022-01-12 00:51:31,499 iteration 5584 : loss : 0.025552, loss_ce: 0.011285
2022-01-12 00:51:33,047 iteration 5585 : loss : 0.017258, loss_ce: 0.005902
2022-01-12 00:51:34,615 iteration 5586 : loss : 0.015139, loss_ce: 0.006718
2022-01-12 00:51:36,183 iteration 5587 : loss : 0.016761, loss_ce: 0.007886
2022-01-12 00:51:37,825 iteration 5588 : loss : 0.019019, loss_ce: 0.006582
2022-01-12 00:51:39,445 iteration 5589 : loss : 0.025384, loss_ce: 0.009207
2022-01-12 00:51:41,022 iteration 5590 : loss : 0.021511, loss_ce: 0.008059
2022-01-12 00:51:42,605 iteration 5591 : loss : 0.015424, loss_ce: 0.006433
2022-01-12 00:51:44,158 iteration 5592 : loss : 0.019540, loss_ce: 0.006474
2022-01-12 00:51:45,718 iteration 5593 : loss : 0.016688, loss_ce: 0.008872
 82%|███████████████████████▊     | 329/400 [2:38:49<33:09, 28.01s/it]2022-01-12 00:51:47,351 iteration 5594 : loss : 0.014399, loss_ce: 0.005064
2022-01-12 00:51:48,900 iteration 5595 : loss : 0.016573, loss_ce: 0.006616
2022-01-12 00:51:50,444 iteration 5596 : loss : 0.010813, loss_ce: 0.003139
2022-01-12 00:51:52,036 iteration 5597 : loss : 0.015571, loss_ce: 0.005024
2022-01-12 00:51:53,670 iteration 5598 : loss : 0.031191, loss_ce: 0.015006
2022-01-12 00:51:55,275 iteration 5599 : loss : 0.014070, loss_ce: 0.004835
2022-01-12 00:51:56,876 iteration 5600 : loss : 0.013693, loss_ce: 0.005419
2022-01-12 00:51:58,476 iteration 5601 : loss : 0.014033, loss_ce: 0.004140
2022-01-12 00:52:00,010 iteration 5602 : loss : 0.022327, loss_ce: 0.005807
2022-01-12 00:52:01,635 iteration 5603 : loss : 0.021397, loss_ce: 0.009254
2022-01-12 00:52:03,179 iteration 5604 : loss : 0.017036, loss_ce: 0.007883
2022-01-12 00:52:04,732 iteration 5605 : loss : 0.012889, loss_ce: 0.004103
2022-01-12 00:52:06,363 iteration 5606 : loss : 0.028433, loss_ce: 0.008420
2022-01-12 00:52:07,937 iteration 5607 : loss : 0.017995, loss_ce: 0.006794
2022-01-12 00:52:09,616 iteration 5608 : loss : 0.018519, loss_ce: 0.009052
2022-01-12 00:52:11,154 iteration 5609 : loss : 0.016108, loss_ce: 0.007513
2022-01-12 00:52:11,154 Training Data Eval:
2022-01-12 00:52:19,151   Average segmentation loss on training set: 0.0099
2022-01-12 00:52:19,151 Validation Data Eval:
2022-01-12 00:52:21,900   Average segmentation loss on validation set: 0.0700
2022-01-12 00:52:23,516 iteration 5610 : loss : 0.018327, loss_ce: 0.006855
 82%|███████████████████████▉     | 330/400 [2:39:27<36:06, 30.95s/it]2022-01-12 00:52:25,182 iteration 5611 : loss : 0.018573, loss_ce: 0.009479
2022-01-12 00:52:26,778 iteration 5612 : loss : 0.017003, loss_ce: 0.005287
2022-01-12 00:52:28,352 iteration 5613 : loss : 0.023341, loss_ce: 0.011570
2022-01-12 00:52:29,930 iteration 5614 : loss : 0.021111, loss_ce: 0.007265
2022-01-12 00:52:31,543 iteration 5615 : loss : 0.019884, loss_ce: 0.007309
2022-01-12 00:52:33,153 iteration 5616 : loss : 0.017306, loss_ce: 0.006192
2022-01-12 00:52:34,738 iteration 5617 : loss : 0.012586, loss_ce: 0.004494
2022-01-12 00:52:36,268 iteration 5618 : loss : 0.014031, loss_ce: 0.005964
2022-01-12 00:52:37,879 iteration 5619 : loss : 0.020787, loss_ce: 0.007106
2022-01-12 00:52:39,494 iteration 5620 : loss : 0.016160, loss_ce: 0.005077
2022-01-12 00:52:41,042 iteration 5621 : loss : 0.021457, loss_ce: 0.008733
2022-01-12 00:52:42,704 iteration 5622 : loss : 0.016338, loss_ce: 0.006487
2022-01-12 00:52:44,245 iteration 5623 : loss : 0.021325, loss_ce: 0.006061
2022-01-12 00:52:45,852 iteration 5624 : loss : 0.016582, loss_ce: 0.007212
2022-01-12 00:52:47,460 iteration 5625 : loss : 0.021273, loss_ce: 0.009559
2022-01-12 00:52:49,043 iteration 5626 : loss : 0.018870, loss_ce: 0.007687
2022-01-12 00:52:50,613 iteration 5627 : loss : 0.026988, loss_ce: 0.007342
 83%|███████████████████████▉     | 331/400 [2:39:54<34:15, 29.79s/it]2022-01-12 00:52:52,135 iteration 5628 : loss : 0.013711, loss_ce: 0.004934
2022-01-12 00:52:53,738 iteration 5629 : loss : 0.023741, loss_ce: 0.012464
2022-01-12 00:52:55,316 iteration 5630 : loss : 0.015133, loss_ce: 0.003972
2022-01-12 00:52:56,983 iteration 5631 : loss : 0.018035, loss_ce: 0.008077
2022-01-12 00:52:58,479 iteration 5632 : loss : 0.016894, loss_ce: 0.007567
2022-01-12 00:53:00,100 iteration 5633 : loss : 0.021609, loss_ce: 0.009251
2022-01-12 00:53:01,805 iteration 5634 : loss : 0.018729, loss_ce: 0.006833
2022-01-12 00:53:03,387 iteration 5635 : loss : 0.041376, loss_ce: 0.011153
2022-01-12 00:53:04,902 iteration 5636 : loss : 0.012324, loss_ce: 0.005750
2022-01-12 00:53:06,489 iteration 5637 : loss : 0.019673, loss_ce: 0.008767
2022-01-12 00:53:07,959 iteration 5638 : loss : 0.013848, loss_ce: 0.004913
2022-01-12 00:53:09,476 iteration 5639 : loss : 0.015857, loss_ce: 0.004748
2022-01-12 00:53:11,033 iteration 5640 : loss : 0.019219, loss_ce: 0.010573
2022-01-12 00:53:12,640 iteration 5641 : loss : 0.014757, loss_ce: 0.007445
2022-01-12 00:53:14,195 iteration 5642 : loss : 0.013852, loss_ce: 0.002756
2022-01-12 00:53:15,816 iteration 5643 : loss : 0.015713, loss_ce: 0.004249
2022-01-12 00:53:17,403 iteration 5644 : loss : 0.015259, loss_ce: 0.005553
 83%|████████████████████████     | 332/400 [2:40:21<32:44, 28.90s/it]2022-01-12 00:53:19,096 iteration 5645 : loss : 0.030592, loss_ce: 0.011463
2022-01-12 00:53:20,629 iteration 5646 : loss : 0.016541, loss_ce: 0.005917
2022-01-12 00:53:22,195 iteration 5647 : loss : 0.016380, loss_ce: 0.007401
2022-01-12 00:53:23,805 iteration 5648 : loss : 0.017470, loss_ce: 0.006243
2022-01-12 00:53:25,409 iteration 5649 : loss : 0.025369, loss_ce: 0.013468
2022-01-12 00:53:26,982 iteration 5650 : loss : 0.016137, loss_ce: 0.006296
2022-01-12 00:53:28,547 iteration 5651 : loss : 0.019481, loss_ce: 0.009028
2022-01-12 00:53:30,046 iteration 5652 : loss : 0.012250, loss_ce: 0.004476
2022-01-12 00:53:31,604 iteration 5653 : loss : 0.015542, loss_ce: 0.006628
2022-01-12 00:53:33,127 iteration 5654 : loss : 0.012780, loss_ce: 0.003524
2022-01-12 00:53:34,720 iteration 5655 : loss : 0.018834, loss_ce: 0.007141
2022-01-12 00:53:36,288 iteration 5656 : loss : 0.014841, loss_ce: 0.005612
2022-01-12 00:53:37,871 iteration 5657 : loss : 0.014550, loss_ce: 0.005844
2022-01-12 00:53:39,442 iteration 5658 : loss : 0.018290, loss_ce: 0.009619
2022-01-12 00:53:40,990 iteration 5659 : loss : 0.014264, loss_ce: 0.003948
2022-01-12 00:53:42,499 iteration 5660 : loss : 0.011629, loss_ce: 0.004328
2022-01-12 00:53:44,152 iteration 5661 : loss : 0.019450, loss_ce: 0.006108
 83%|████████████████████████▏    | 333/400 [2:40:48<31:32, 28.25s/it]2022-01-12 00:53:45,748 iteration 5662 : loss : 0.020861, loss_ce: 0.005390
2022-01-12 00:53:47,313 iteration 5663 : loss : 0.015757, loss_ce: 0.006283
2022-01-12 00:53:48,885 iteration 5664 : loss : 0.023088, loss_ce: 0.007016
2022-01-12 00:53:50,376 iteration 5665 : loss : 0.018194, loss_ce: 0.006367
2022-01-12 00:53:51,967 iteration 5666 : loss : 0.015391, loss_ce: 0.005749
2022-01-12 00:53:53,571 iteration 5667 : loss : 0.015391, loss_ce: 0.005703
2022-01-12 00:53:55,141 iteration 5668 : loss : 0.016280, loss_ce: 0.006724
2022-01-12 00:53:56,753 iteration 5669 : loss : 0.024326, loss_ce: 0.011507
2022-01-12 00:53:58,336 iteration 5670 : loss : 0.012215, loss_ce: 0.004592
2022-01-12 00:53:59,908 iteration 5671 : loss : 0.014833, loss_ce: 0.005758
2022-01-12 00:54:01,472 iteration 5672 : loss : 0.020795, loss_ce: 0.005650
2022-01-12 00:54:03,012 iteration 5673 : loss : 0.016739, loss_ce: 0.004806
2022-01-12 00:54:04,643 iteration 5674 : loss : 0.014140, loss_ce: 0.006721
2022-01-12 00:54:06,183 iteration 5675 : loss : 0.017463, loss_ce: 0.008417
2022-01-12 00:54:07,805 iteration 5676 : loss : 0.018030, loss_ce: 0.006754
2022-01-12 00:54:09,419 iteration 5677 : loss : 0.017143, loss_ce: 0.008359
2022-01-12 00:54:11,012 iteration 5678 : loss : 0.032393, loss_ce: 0.011332
 84%|████████████████████████▏    | 334/400 [2:41:15<30:36, 27.83s/it]2022-01-12 00:54:12,628 iteration 5679 : loss : 0.018222, loss_ce: 0.007745
2022-01-12 00:54:14,097 iteration 5680 : loss : 0.012695, loss_ce: 0.003257
2022-01-12 00:54:15,713 iteration 5681 : loss : 0.019606, loss_ce: 0.005715
2022-01-12 00:54:17,337 iteration 5682 : loss : 0.018108, loss_ce: 0.007354
2022-01-12 00:54:18,882 iteration 5683 : loss : 0.011184, loss_ce: 0.003632
2022-01-12 00:54:20,414 iteration 5684 : loss : 0.011199, loss_ce: 0.005727
2022-01-12 00:54:22,009 iteration 5685 : loss : 0.018906, loss_ce: 0.008766
2022-01-12 00:54:23,565 iteration 5686 : loss : 0.015964, loss_ce: 0.005833
2022-01-12 00:54:25,123 iteration 5687 : loss : 0.013264, loss_ce: 0.005489
2022-01-12 00:54:26,778 iteration 5688 : loss : 0.016484, loss_ce: 0.005599
2022-01-12 00:54:28,358 iteration 5689 : loss : 0.034547, loss_ce: 0.007198
2022-01-12 00:54:29,941 iteration 5690 : loss : 0.014090, loss_ce: 0.006181
2022-01-12 00:54:31,538 iteration 5691 : loss : 0.022623, loss_ce: 0.010892
2022-01-12 00:54:33,124 iteration 5692 : loss : 0.014692, loss_ce: 0.006028
2022-01-12 00:54:34,702 iteration 5693 : loss : 0.020701, loss_ce: 0.005820
2022-01-12 00:54:36,264 iteration 5694 : loss : 0.018336, loss_ce: 0.008526
2022-01-12 00:54:36,264 Training Data Eval:
2022-01-12 00:54:44,267   Average segmentation loss on training set: 0.0094
2022-01-12 00:54:44,268 Validation Data Eval:
2022-01-12 00:54:47,021   Average segmentation loss on validation set: 0.0745
2022-01-12 00:54:48,556 iteration 5695 : loss : 0.014755, loss_ce: 0.004537
 84%|████████████████████████▎    | 335/400 [2:41:52<33:18, 30.74s/it]2022-01-12 00:54:50,114 iteration 5696 : loss : 0.013085, loss_ce: 0.006574
2022-01-12 00:54:51,637 iteration 5697 : loss : 0.014061, loss_ce: 0.006058
2022-01-12 00:54:53,249 iteration 5698 : loss : 0.022792, loss_ce: 0.008980
2022-01-12 00:54:54,752 iteration 5699 : loss : 0.012945, loss_ce: 0.005463
2022-01-12 00:54:56,295 iteration 5700 : loss : 0.014812, loss_ce: 0.005065
2022-01-12 00:54:57,807 iteration 5701 : loss : 0.011448, loss_ce: 0.005024
2022-01-12 00:54:59,467 iteration 5702 : loss : 0.015679, loss_ce: 0.005158
2022-01-12 00:55:01,068 iteration 5703 : loss : 0.021406, loss_ce: 0.009727
2022-01-12 00:55:02,655 iteration 5704 : loss : 0.016288, loss_ce: 0.006076
2022-01-12 00:55:04,244 iteration 5705 : loss : 0.019280, loss_ce: 0.006631
2022-01-12 00:55:05,734 iteration 5706 : loss : 0.012805, loss_ce: 0.004789
2022-01-12 00:55:07,277 iteration 5707 : loss : 0.024946, loss_ce: 0.012901
2022-01-12 00:55:08,799 iteration 5708 : loss : 0.015790, loss_ce: 0.005818
2022-01-12 00:55:10,368 iteration 5709 : loss : 0.013927, loss_ce: 0.004446
2022-01-12 00:55:11,929 iteration 5710 : loss : 0.016331, loss_ce: 0.004721
2022-01-12 00:55:13,435 iteration 5711 : loss : 0.013979, loss_ce: 0.004908
2022-01-12 00:55:15,020 iteration 5712 : loss : 0.021020, loss_ce: 0.009363
 84%|████████████████████████▎    | 336/400 [2:42:19<31:25, 29.46s/it]2022-01-12 00:55:16,662 iteration 5713 : loss : 0.020391, loss_ce: 0.006766
2022-01-12 00:55:18,270 iteration 5714 : loss : 0.018353, loss_ce: 0.006793
2022-01-12 00:55:19,894 iteration 5715 : loss : 0.016162, loss_ce: 0.006543
2022-01-12 00:55:21,440 iteration 5716 : loss : 0.015760, loss_ce: 0.005772
2022-01-12 00:55:23,085 iteration 5717 : loss : 0.023497, loss_ce: 0.010058
2022-01-12 00:55:24,600 iteration 5718 : loss : 0.015846, loss_ce: 0.005110
2022-01-12 00:55:26,109 iteration 5719 : loss : 0.013768, loss_ce: 0.005767
2022-01-12 00:55:27,716 iteration 5720 : loss : 0.015017, loss_ce: 0.005915
2022-01-12 00:55:29,294 iteration 5721 : loss : 0.016549, loss_ce: 0.008138
2022-01-12 00:55:30,843 iteration 5722 : loss : 0.021239, loss_ce: 0.008071
2022-01-12 00:55:32,418 iteration 5723 : loss : 0.019993, loss_ce: 0.006409
2022-01-12 00:55:34,026 iteration 5724 : loss : 0.018468, loss_ce: 0.006476
2022-01-12 00:55:35,598 iteration 5725 : loss : 0.016223, loss_ce: 0.006624
2022-01-12 00:55:37,119 iteration 5726 : loss : 0.017517, loss_ce: 0.004733
2022-01-12 00:55:38,702 iteration 5727 : loss : 0.018918, loss_ce: 0.005963
2022-01-12 00:55:40,240 iteration 5728 : loss : 0.017077, loss_ce: 0.005012
2022-01-12 00:55:41,873 iteration 5729 : loss : 0.018457, loss_ce: 0.008477
 84%|████████████████████████▍    | 337/400 [2:42:46<30:06, 28.68s/it]2022-01-12 00:55:43,433 iteration 5730 : loss : 0.013529, loss_ce: 0.006228
2022-01-12 00:55:45,033 iteration 5731 : loss : 0.020010, loss_ce: 0.009671
2022-01-12 00:55:46,556 iteration 5732 : loss : 0.012196, loss_ce: 0.004891
2022-01-12 00:55:48,161 iteration 5733 : loss : 0.017937, loss_ce: 0.004912
2022-01-12 00:55:49,757 iteration 5734 : loss : 0.021379, loss_ce: 0.009729
2022-01-12 00:55:51,296 iteration 5735 : loss : 0.012779, loss_ce: 0.003544
2022-01-12 00:55:52,836 iteration 5736 : loss : 0.017136, loss_ce: 0.007149
2022-01-12 00:55:54,467 iteration 5737 : loss : 0.029846, loss_ce: 0.008343
2022-01-12 00:55:56,000 iteration 5738 : loss : 0.013081, loss_ce: 0.004648
2022-01-12 00:55:57,529 iteration 5739 : loss : 0.014322, loss_ce: 0.005667
2022-01-12 00:55:59,088 iteration 5740 : loss : 0.023010, loss_ce: 0.012278
2022-01-12 00:56:00,655 iteration 5741 : loss : 0.015495, loss_ce: 0.007357
2022-01-12 00:56:02,230 iteration 5742 : loss : 0.014659, loss_ce: 0.003385
2022-01-12 00:56:03,807 iteration 5743 : loss : 0.013809, loss_ce: 0.005632
2022-01-12 00:56:05,428 iteration 5744 : loss : 0.018388, loss_ce: 0.008055
2022-01-12 00:56:07,047 iteration 5745 : loss : 0.015237, loss_ce: 0.006298
2022-01-12 00:56:08,573 iteration 5746 : loss : 0.016266, loss_ce: 0.004421
 84%|████████████████████████▌    | 338/400 [2:43:12<29:01, 28.09s/it]2022-01-12 00:56:10,175 iteration 5747 : loss : 0.014623, loss_ce: 0.005035
2022-01-12 00:56:11,758 iteration 5748 : loss : 0.016742, loss_ce: 0.006087
2022-01-12 00:56:13,328 iteration 5749 : loss : 0.018818, loss_ce: 0.006807
2022-01-12 00:56:14,895 iteration 5750 : loss : 0.016160, loss_ce: 0.005463
2022-01-12 00:56:16,410 iteration 5751 : loss : 0.019987, loss_ce: 0.006161
2022-01-12 00:56:17,947 iteration 5752 : loss : 0.016168, loss_ce: 0.004776
2022-01-12 00:56:19,536 iteration 5753 : loss : 0.015937, loss_ce: 0.006604
2022-01-12 00:56:21,142 iteration 5754 : loss : 0.021318, loss_ce: 0.006225
2022-01-12 00:56:22,705 iteration 5755 : loss : 0.014141, loss_ce: 0.005057
2022-01-12 00:56:24,239 iteration 5756 : loss : 0.014128, loss_ce: 0.005143
2022-01-12 00:56:25,697 iteration 5757 : loss : 0.014054, loss_ce: 0.005219
2022-01-12 00:56:27,310 iteration 5758 : loss : 0.020269, loss_ce: 0.008287
2022-01-12 00:56:28,925 iteration 5759 : loss : 0.015213, loss_ce: 0.006893
2022-01-12 00:56:30,558 iteration 5760 : loss : 0.029479, loss_ce: 0.014532
2022-01-12 00:56:32,192 iteration 5761 : loss : 0.030306, loss_ce: 0.013384
2022-01-12 00:56:33,700 iteration 5762 : loss : 0.014489, loss_ce: 0.006733
2022-01-12 00:56:35,310 iteration 5763 : loss : 0.019766, loss_ce: 0.010960
 85%|████████████████████████▌    | 339/400 [2:43:39<28:08, 27.68s/it]2022-01-12 00:56:36,916 iteration 5764 : loss : 0.018472, loss_ce: 0.006796
2022-01-12 00:56:38,533 iteration 5765 : loss : 0.033559, loss_ce: 0.007823
2022-01-12 00:56:40,111 iteration 5766 : loss : 0.018458, loss_ce: 0.009503
2022-01-12 00:56:41,671 iteration 5767 : loss : 0.013478, loss_ce: 0.007262
2022-01-12 00:56:43,328 iteration 5768 : loss : 0.023695, loss_ce: 0.008553
2022-01-12 00:56:44,927 iteration 5769 : loss : 0.039170, loss_ce: 0.013147
2022-01-12 00:56:46,456 iteration 5770 : loss : 0.013411, loss_ce: 0.005026
2022-01-12 00:56:48,036 iteration 5771 : loss : 0.021375, loss_ce: 0.008185
2022-01-12 00:56:49,603 iteration 5772 : loss : 0.014940, loss_ce: 0.005387
2022-01-12 00:56:51,094 iteration 5773 : loss : 0.013846, loss_ce: 0.004858
2022-01-12 00:56:52,569 iteration 5774 : loss : 0.014885, loss_ce: 0.004252
2022-01-12 00:56:54,143 iteration 5775 : loss : 0.014122, loss_ce: 0.005885
2022-01-12 00:56:55,794 iteration 5776 : loss : 0.017168, loss_ce: 0.008041
2022-01-12 00:56:57,285 iteration 5777 : loss : 0.015165, loss_ce: 0.006347
2022-01-12 00:56:58,826 iteration 5778 : loss : 0.019610, loss_ce: 0.005886
2022-01-12 00:57:00,372 iteration 5779 : loss : 0.012555, loss_ce: 0.004134
2022-01-12 00:57:00,373 Training Data Eval:
2022-01-12 00:57:08,373   Average segmentation loss on training set: 0.0095
2022-01-12 00:57:08,373 Validation Data Eval:
2022-01-12 00:57:11,124   Average segmentation loss on validation set: 0.0765
2022-01-12 00:57:12,682 iteration 5780 : loss : 0.022332, loss_ce: 0.006886
 85%|████████████████████████▋    | 340/400 [2:44:16<30:35, 30.59s/it]2022-01-12 00:57:14,255 iteration 5781 : loss : 0.022104, loss_ce: 0.006963
2022-01-12 00:57:15,882 iteration 5782 : loss : 0.026328, loss_ce: 0.009468
2022-01-12 00:57:17,446 iteration 5783 : loss : 0.016539, loss_ce: 0.005622
2022-01-12 00:57:19,033 iteration 5784 : loss : 0.017787, loss_ce: 0.010573
2022-01-12 00:57:20,555 iteration 5785 : loss : 0.011253, loss_ce: 0.004203
2022-01-12 00:57:22,090 iteration 5786 : loss : 0.013441, loss_ce: 0.005651
2022-01-12 00:57:23,619 iteration 5787 : loss : 0.013731, loss_ce: 0.005032
2022-01-12 00:57:25,301 iteration 5788 : loss : 0.020274, loss_ce: 0.008151
2022-01-12 00:57:26,872 iteration 5789 : loss : 0.013086, loss_ce: 0.004496
2022-01-12 00:57:28,427 iteration 5790 : loss : 0.014227, loss_ce: 0.005156
2022-01-12 00:57:29,957 iteration 5791 : loss : 0.014811, loss_ce: 0.005513
2022-01-12 00:57:31,548 iteration 5792 : loss : 0.023337, loss_ce: 0.008092
2022-01-12 00:57:33,211 iteration 5793 : loss : 0.018997, loss_ce: 0.007964
2022-01-12 00:57:34,773 iteration 5794 : loss : 0.015099, loss_ce: 0.005721
2022-01-12 00:57:36,326 iteration 5795 : loss : 0.013571, loss_ce: 0.004618
2022-01-12 00:57:37,849 iteration 5796 : loss : 0.011852, loss_ce: 0.003242
2022-01-12 00:57:39,430 iteration 5797 : loss : 0.020870, loss_ce: 0.008794
 85%|████████████████████████▋    | 341/400 [2:44:43<28:56, 29.44s/it]2022-01-12 00:57:41,070 iteration 5798 : loss : 0.021164, loss_ce: 0.007916
2022-01-12 00:57:42,631 iteration 5799 : loss : 0.015669, loss_ce: 0.005679
2022-01-12 00:57:44,174 iteration 5800 : loss : 0.013037, loss_ce: 0.005260
2022-01-12 00:57:45,733 iteration 5801 : loss : 0.021501, loss_ce: 0.007520
2022-01-12 00:57:47,242 iteration 5802 : loss : 0.011916, loss_ce: 0.005048
2022-01-12 00:57:48,779 iteration 5803 : loss : 0.015639, loss_ce: 0.007950
2022-01-12 00:57:50,336 iteration 5804 : loss : 0.015052, loss_ce: 0.005777
2022-01-12 00:57:51,972 iteration 5805 : loss : 0.020632, loss_ce: 0.007697
2022-01-12 00:57:53,499 iteration 5806 : loss : 0.020609, loss_ce: 0.006092
2022-01-12 00:57:55,116 iteration 5807 : loss : 0.022853, loss_ce: 0.010075
2022-01-12 00:57:56,685 iteration 5808 : loss : 0.018409, loss_ce: 0.008247
2022-01-12 00:57:58,191 iteration 5809 : loss : 0.011214, loss_ce: 0.002790
2022-01-12 00:57:59,724 iteration 5810 : loss : 0.016101, loss_ce: 0.005315
2022-01-12 00:58:01,222 iteration 5811 : loss : 0.009603, loss_ce: 0.002918
2022-01-12 00:58:02,780 iteration 5812 : loss : 0.019912, loss_ce: 0.006425
2022-01-12 00:58:04,282 iteration 5813 : loss : 0.014145, loss_ce: 0.006536
2022-01-12 00:58:05,863 iteration 5814 : loss : 0.019680, loss_ce: 0.008549
 86%|████████████████████████▊    | 342/400 [2:45:09<27:35, 28.54s/it]2022-01-12 00:58:07,404 iteration 5815 : loss : 0.010775, loss_ce: 0.003506
2022-01-12 00:58:08,941 iteration 5816 : loss : 0.015570, loss_ce: 0.006972
2022-01-12 00:58:10,466 iteration 5817 : loss : 0.011843, loss_ce: 0.004320
2022-01-12 00:58:11,937 iteration 5818 : loss : 0.013780, loss_ce: 0.005823
2022-01-12 00:58:13,543 iteration 5819 : loss : 0.030322, loss_ce: 0.011315
2022-01-12 00:58:15,173 iteration 5820 : loss : 0.019335, loss_ce: 0.009913
2022-01-12 00:58:16,674 iteration 5821 : loss : 0.011733, loss_ce: 0.003777
2022-01-12 00:58:18,198 iteration 5822 : loss : 0.010282, loss_ce: 0.004528
2022-01-12 00:58:19,852 iteration 5823 : loss : 0.020571, loss_ce: 0.008585
2022-01-12 00:58:21,404 iteration 5824 : loss : 0.015671, loss_ce: 0.005135
2022-01-12 00:58:22,994 iteration 5825 : loss : 0.018741, loss_ce: 0.007848
2022-01-12 00:58:24,585 iteration 5826 : loss : 0.016687, loss_ce: 0.004061
2022-01-12 00:58:26,142 iteration 5827 : loss : 0.017227, loss_ce: 0.005178
2022-01-12 00:58:27,809 iteration 5828 : loss : 0.019705, loss_ce: 0.006925
2022-01-12 00:58:29,360 iteration 5829 : loss : 0.016008, loss_ce: 0.006898
2022-01-12 00:58:30,890 iteration 5830 : loss : 0.020189, loss_ce: 0.008434
2022-01-12 00:58:32,425 iteration 5831 : loss : 0.018503, loss_ce: 0.006558
 86%|████████████████████████▊    | 343/400 [2:45:36<26:32, 27.94s/it]2022-01-12 00:58:34,033 iteration 5832 : loss : 0.014596, loss_ce: 0.004245
2022-01-12 00:58:35,625 iteration 5833 : loss : 0.016385, loss_ce: 0.007166
2022-01-12 00:58:37,123 iteration 5834 : loss : 0.013195, loss_ce: 0.004731
2022-01-12 00:58:38,639 iteration 5835 : loss : 0.017757, loss_ce: 0.004557
2022-01-12 00:58:40,257 iteration 5836 : loss : 0.022146, loss_ce: 0.011141
2022-01-12 00:58:41,825 iteration 5837 : loss : 0.023213, loss_ce: 0.005549
2022-01-12 00:58:43,299 iteration 5838 : loss : 0.009918, loss_ce: 0.002879
2022-01-12 00:58:44,862 iteration 5839 : loss : 0.012085, loss_ce: 0.003981
2022-01-12 00:58:46,471 iteration 5840 : loss : 0.016499, loss_ce: 0.006565
2022-01-12 00:58:47,998 iteration 5841 : loss : 0.015813, loss_ce: 0.006684
2022-01-12 00:58:49,524 iteration 5842 : loss : 0.008766, loss_ce: 0.003390
2022-01-12 00:58:51,062 iteration 5843 : loss : 0.017120, loss_ce: 0.006930
2022-01-12 00:58:52,748 iteration 5844 : loss : 0.024638, loss_ce: 0.007270
2022-01-12 00:58:54,303 iteration 5845 : loss : 0.018856, loss_ce: 0.007559
2022-01-12 00:58:55,886 iteration 5846 : loss : 0.025694, loss_ce: 0.010811
2022-01-12 00:58:57,466 iteration 5847 : loss : 0.015256, loss_ce: 0.005715
2022-01-12 00:58:59,030 iteration 5848 : loss : 0.017809, loss_ce: 0.006722
 86%|████████████████████████▉    | 344/400 [2:46:03<25:42, 27.54s/it]2022-01-12 00:59:00,622 iteration 5849 : loss : 0.015164, loss_ce: 0.005944
2022-01-12 00:59:02,112 iteration 5850 : loss : 0.012981, loss_ce: 0.005271
2022-01-12 00:59:03,668 iteration 5851 : loss : 0.017586, loss_ce: 0.008280
2022-01-12 00:59:05,202 iteration 5852 : loss : 0.013343, loss_ce: 0.005002
2022-01-12 00:59:06,684 iteration 5853 : loss : 0.015021, loss_ce: 0.004912
2022-01-12 00:59:08,208 iteration 5854 : loss : 0.018182, loss_ce: 0.006516
2022-01-12 00:59:09,836 iteration 5855 : loss : 0.017599, loss_ce: 0.007500
2022-01-12 00:59:11,394 iteration 5856 : loss : 0.017525, loss_ce: 0.008101
2022-01-12 00:59:12,960 iteration 5857 : loss : 0.015231, loss_ce: 0.005147
2022-01-12 00:59:14,449 iteration 5858 : loss : 0.013323, loss_ce: 0.004991
2022-01-12 00:59:16,040 iteration 5859 : loss : 0.019771, loss_ce: 0.007523
2022-01-12 00:59:17,619 iteration 5860 : loss : 0.022430, loss_ce: 0.009063
2022-01-12 00:59:19,148 iteration 5861 : loss : 0.012935, loss_ce: 0.003854
2022-01-12 00:59:20,740 iteration 5862 : loss : 0.012612, loss_ce: 0.005744
2022-01-12 00:59:22,215 iteration 5863 : loss : 0.010126, loss_ce: 0.003600
2022-01-12 00:59:23,771 iteration 5864 : loss : 0.036365, loss_ce: 0.017085
2022-01-12 00:59:23,771 Training Data Eval:
2022-01-12 00:59:31,748   Average segmentation loss on training set: 0.0086
2022-01-12 00:59:31,748 Validation Data Eval:
2022-01-12 00:59:34,499   Average segmentation loss on validation set: 0.0760
2022-01-12 00:59:36,011 iteration 5865 : loss : 0.019468, loss_ce: 0.005847
 86%|█████████████████████████    | 345/400 [2:46:40<27:50, 30.37s/it]2022-01-12 00:59:37,685 iteration 5866 : loss : 0.014337, loss_ce: 0.005770
2022-01-12 00:59:39,293 iteration 5867 : loss : 0.013517, loss_ce: 0.004208
2022-01-12 00:59:40,884 iteration 5868 : loss : 0.014481, loss_ce: 0.005231
2022-01-12 00:59:42,374 iteration 5869 : loss : 0.011670, loss_ce: 0.004757
2022-01-12 00:59:44,010 iteration 5870 : loss : 0.019085, loss_ce: 0.004921
2022-01-12 00:59:45,589 iteration 5871 : loss : 0.012560, loss_ce: 0.004291
2022-01-12 00:59:47,175 iteration 5872 : loss : 0.026633, loss_ce: 0.009497
2022-01-12 00:59:48,840 iteration 5873 : loss : 0.019556, loss_ce: 0.006930
2022-01-12 00:59:50,348 iteration 5874 : loss : 0.014600, loss_ce: 0.004790
2022-01-12 00:59:51,879 iteration 5875 : loss : 0.014018, loss_ce: 0.006727
2022-01-12 00:59:53,452 iteration 5876 : loss : 0.019877, loss_ce: 0.007894
2022-01-12 00:59:55,012 iteration 5877 : loss : 0.015171, loss_ce: 0.005827
2022-01-12 00:59:56,585 iteration 5878 : loss : 0.012636, loss_ce: 0.005308
2022-01-12 00:59:58,100 iteration 5879 : loss : 0.013841, loss_ce: 0.006038
2022-01-12 00:59:59,755 iteration 5880 : loss : 0.015794, loss_ce: 0.004233
2022-01-12 01:00:01,245 iteration 5881 : loss : 0.012804, loss_ce: 0.004889
2022-01-12 01:00:02,789 iteration 5882 : loss : 0.014509, loss_ce: 0.006464
 86%|█████████████████████████    | 346/400 [2:47:06<26:21, 29.30s/it]2022-01-12 01:00:04,359 iteration 5883 : loss : 0.011554, loss_ce: 0.004045
2022-01-12 01:00:05,881 iteration 5884 : loss : 0.016701, loss_ce: 0.006651
2022-01-12 01:00:07,544 iteration 5885 : loss : 0.026597, loss_ce: 0.009622
2022-01-12 01:00:09,037 iteration 5886 : loss : 0.013255, loss_ce: 0.003715
2022-01-12 01:00:10,573 iteration 5887 : loss : 0.012860, loss_ce: 0.005457
2022-01-12 01:00:12,209 iteration 5888 : loss : 0.015566, loss_ce: 0.005462
2022-01-12 01:00:13,832 iteration 5889 : loss : 0.015100, loss_ce: 0.006203
2022-01-12 01:00:15,398 iteration 5890 : loss : 0.012195, loss_ce: 0.004014
2022-01-12 01:00:17,006 iteration 5891 : loss : 0.017695, loss_ce: 0.006104
2022-01-12 01:00:18,652 iteration 5892 : loss : 0.017987, loss_ce: 0.006955
2022-01-12 01:00:20,174 iteration 5893 : loss : 0.012029, loss_ce: 0.003910
2022-01-12 01:00:21,805 iteration 5894 : loss : 0.017470, loss_ce: 0.006377
2022-01-12 01:00:23,383 iteration 5895 : loss : 0.022011, loss_ce: 0.007194
2022-01-12 01:00:24,918 iteration 5896 : loss : 0.014165, loss_ce: 0.006556
2022-01-12 01:00:26,532 iteration 5897 : loss : 0.022274, loss_ce: 0.007556
2022-01-12 01:00:28,124 iteration 5898 : loss : 0.015057, loss_ce: 0.005787
2022-01-12 01:00:29,647 iteration 5899 : loss : 0.012635, loss_ce: 0.004416
 87%|█████████████████████████▏   | 347/400 [2:47:33<25:13, 28.56s/it]2022-01-12 01:00:31,257 iteration 5900 : loss : 0.016420, loss_ce: 0.005897
2022-01-12 01:00:32,892 iteration 5901 : loss : 0.015156, loss_ce: 0.005063
2022-01-12 01:00:34,447 iteration 5902 : loss : 0.018381, loss_ce: 0.006829
2022-01-12 01:00:36,068 iteration 5903 : loss : 0.016667, loss_ce: 0.007980
2022-01-12 01:00:37,617 iteration 5904 : loss : 0.012761, loss_ce: 0.005004
2022-01-12 01:00:39,232 iteration 5905 : loss : 0.025857, loss_ce: 0.006574
2022-01-12 01:00:40,748 iteration 5906 : loss : 0.014893, loss_ce: 0.005400
2022-01-12 01:00:42,340 iteration 5907 : loss : 0.010985, loss_ce: 0.003948
2022-01-12 01:00:43,912 iteration 5908 : loss : 0.010964, loss_ce: 0.003667
2022-01-12 01:00:45,385 iteration 5909 : loss : 0.014619, loss_ce: 0.004159
2022-01-12 01:00:46,959 iteration 5910 : loss : 0.016918, loss_ce: 0.005156
2022-01-12 01:00:48,556 iteration 5911 : loss : 0.019554, loss_ce: 0.008457
2022-01-12 01:00:50,051 iteration 5912 : loss : 0.011776, loss_ce: 0.003719
2022-01-12 01:00:51,587 iteration 5913 : loss : 0.015407, loss_ce: 0.007439
2022-01-12 01:00:53,156 iteration 5914 : loss : 0.021598, loss_ce: 0.007110
2022-01-12 01:00:54,634 iteration 5915 : loss : 0.022415, loss_ce: 0.008237
2022-01-12 01:00:56,231 iteration 5916 : loss : 0.022589, loss_ce: 0.008370
 87%|█████████████████████████▏   | 348/400 [2:48:00<24:14, 27.97s/it]2022-01-12 01:00:57,806 iteration 5917 : loss : 0.015484, loss_ce: 0.007401
2022-01-12 01:00:59,310 iteration 5918 : loss : 0.013832, loss_ce: 0.005406
2022-01-12 01:01:00,907 iteration 5919 : loss : 0.018969, loss_ce: 0.008672
2022-01-12 01:01:02,508 iteration 5920 : loss : 0.017742, loss_ce: 0.008853
2022-01-12 01:01:04,196 iteration 5921 : loss : 0.026251, loss_ce: 0.008527
2022-01-12 01:01:05,741 iteration 5922 : loss : 0.013843, loss_ce: 0.003746
2022-01-12 01:01:07,308 iteration 5923 : loss : 0.016004, loss_ce: 0.005225
2022-01-12 01:01:08,964 iteration 5924 : loss : 0.020311, loss_ce: 0.003900
2022-01-12 01:01:10,545 iteration 5925 : loss : 0.017004, loss_ce: 0.006772
2022-01-12 01:01:12,112 iteration 5926 : loss : 0.014431, loss_ce: 0.005612
2022-01-12 01:01:13,689 iteration 5927 : loss : 0.012783, loss_ce: 0.004866
2022-01-12 01:01:15,185 iteration 5928 : loss : 0.017185, loss_ce: 0.004104
2022-01-12 01:01:16,701 iteration 5929 : loss : 0.011842, loss_ce: 0.005564
2022-01-12 01:01:18,214 iteration 5930 : loss : 0.017916, loss_ce: 0.008909
2022-01-12 01:01:19,770 iteration 5931 : loss : 0.016837, loss_ce: 0.004342
2022-01-12 01:01:21,372 iteration 5932 : loss : 0.013328, loss_ce: 0.004724
2022-01-12 01:01:22,923 iteration 5933 : loss : 0.017117, loss_ce: 0.006658
 87%|█████████████████████████▎   | 349/400 [2:48:27<23:26, 27.58s/it]2022-01-12 01:01:24,479 iteration 5934 : loss : 0.013460, loss_ce: 0.004281
2022-01-12 01:01:26,162 iteration 5935 : loss : 0.018102, loss_ce: 0.005986
2022-01-12 01:01:27,722 iteration 5936 : loss : 0.013665, loss_ce: 0.005500
2022-01-12 01:01:29,259 iteration 5937 : loss : 0.011201, loss_ce: 0.004081
2022-01-12 01:01:30,751 iteration 5938 : loss : 0.008931, loss_ce: 0.003290
2022-01-12 01:01:32,344 iteration 5939 : loss : 0.011669, loss_ce: 0.004618
2022-01-12 01:01:33,852 iteration 5940 : loss : 0.012352, loss_ce: 0.004958
2022-01-12 01:01:35,364 iteration 5941 : loss : 0.013813, loss_ce: 0.005558
2022-01-12 01:01:36,893 iteration 5942 : loss : 0.013346, loss_ce: 0.005517
2022-01-12 01:01:38,461 iteration 5943 : loss : 0.014525, loss_ce: 0.003832
2022-01-12 01:01:40,022 iteration 5944 : loss : 0.015293, loss_ce: 0.007107
2022-01-12 01:01:41,542 iteration 5945 : loss : 0.014482, loss_ce: 0.003077
2022-01-12 01:01:43,184 iteration 5946 : loss : 0.015874, loss_ce: 0.005502
2022-01-12 01:01:44,773 iteration 5947 : loss : 0.017785, loss_ce: 0.006847
2022-01-12 01:01:46,306 iteration 5948 : loss : 0.018019, loss_ce: 0.006906
2022-01-12 01:01:47,923 iteration 5949 : loss : 0.013090, loss_ce: 0.004667
2022-01-12 01:01:47,923 Training Data Eval:
2022-01-12 01:01:55,908   Average segmentation loss on training set: 0.0081
2022-01-12 01:01:55,908 Validation Data Eval:
2022-01-12 01:01:58,666   Average segmentation loss on validation set: 0.0717
2022-01-12 01:02:00,254 iteration 5950 : loss : 0.025785, loss_ce: 0.009466
 88%|█████████████████████████▍   | 350/400 [2:49:04<25:25, 30.51s/it]2022-01-12 01:02:01,813 iteration 5951 : loss : 0.015098, loss_ce: 0.005011
2022-01-12 01:02:03,371 iteration 5952 : loss : 0.018984, loss_ce: 0.007973
2022-01-12 01:02:04,934 iteration 5953 : loss : 0.017650, loss_ce: 0.007541
2022-01-12 01:02:06,463 iteration 5954 : loss : 0.013060, loss_ce: 0.004758
2022-01-12 01:02:08,025 iteration 5955 : loss : 0.013784, loss_ce: 0.004774
2022-01-12 01:02:09,612 iteration 5956 : loss : 0.020623, loss_ce: 0.010779
2022-01-12 01:02:11,177 iteration 5957 : loss : 0.011919, loss_ce: 0.004875
2022-01-12 01:02:12,705 iteration 5958 : loss : 0.011598, loss_ce: 0.004250
2022-01-12 01:02:14,255 iteration 5959 : loss : 0.013104, loss_ce: 0.005313
2022-01-12 01:02:15,773 iteration 5960 : loss : 0.015545, loss_ce: 0.004771
2022-01-12 01:02:17,283 iteration 5961 : loss : 0.018498, loss_ce: 0.004179
2022-01-12 01:02:18,822 iteration 5962 : loss : 0.015920, loss_ce: 0.003640
2022-01-12 01:02:20,405 iteration 5963 : loss : 0.016965, loss_ce: 0.007772
2022-01-12 01:02:21,943 iteration 5964 : loss : 0.010581, loss_ce: 0.004456
2022-01-12 01:02:23,439 iteration 5965 : loss : 0.011055, loss_ce: 0.005056
2022-01-12 01:02:25,038 iteration 5966 : loss : 0.016419, loss_ce: 0.004413
2022-01-12 01:02:26,594 iteration 5967 : loss : 0.013105, loss_ce: 0.005124
 88%|█████████████████████████▍   | 351/400 [2:49:30<23:53, 29.26s/it]2022-01-12 01:02:28,244 iteration 5968 : loss : 0.017698, loss_ce: 0.006910
2022-01-12 01:02:29,764 iteration 5969 : loss : 0.016344, loss_ce: 0.007567
2022-01-12 01:02:31,352 iteration 5970 : loss : 0.018866, loss_ce: 0.007742
2022-01-12 01:02:32,856 iteration 5971 : loss : 0.010657, loss_ce: 0.003228
2022-01-12 01:02:34,330 iteration 5972 : loss : 0.010970, loss_ce: 0.003600
2022-01-12 01:02:35,892 iteration 5973 : loss : 0.016274, loss_ce: 0.008021
2022-01-12 01:02:37,537 iteration 5974 : loss : 0.020794, loss_ce: 0.008394
2022-01-12 01:02:39,029 iteration 5975 : loss : 0.013478, loss_ce: 0.004677
2022-01-12 01:02:40,570 iteration 5976 : loss : 0.017107, loss_ce: 0.004933
2022-01-12 01:02:42,086 iteration 5977 : loss : 0.024260, loss_ce: 0.005232
2022-01-12 01:02:43,599 iteration 5978 : loss : 0.012343, loss_ce: 0.005537
2022-01-12 01:02:45,157 iteration 5979 : loss : 0.011217, loss_ce: 0.004985
2022-01-12 01:02:46,739 iteration 5980 : loss : 0.020483, loss_ce: 0.006699
2022-01-12 01:02:48,250 iteration 5981 : loss : 0.029262, loss_ce: 0.013484
2022-01-12 01:02:49,804 iteration 5982 : loss : 0.016445, loss_ce: 0.006474
2022-01-12 01:02:51,412 iteration 5983 : loss : 0.019667, loss_ce: 0.005957
2022-01-12 01:02:52,935 iteration 5984 : loss : 0.014078, loss_ce: 0.004816
 88%|█████████████████████████▌   | 352/400 [2:49:57<22:42, 28.38s/it]2022-01-12 01:02:54,522 iteration 5985 : loss : 0.013683, loss_ce: 0.004317
2022-01-12 01:02:55,994 iteration 5986 : loss : 0.012677, loss_ce: 0.003939
2022-01-12 01:02:57,579 iteration 5987 : loss : 0.015582, loss_ce: 0.006625
2022-01-12 01:02:59,188 iteration 5988 : loss : 0.022411, loss_ce: 0.008174
2022-01-12 01:03:00,773 iteration 5989 : loss : 0.025358, loss_ce: 0.009284
2022-01-12 01:03:02,307 iteration 5990 : loss : 0.011723, loss_ce: 0.004208
2022-01-12 01:03:03,888 iteration 5991 : loss : 0.018481, loss_ce: 0.005477
2022-01-12 01:03:05,448 iteration 5992 : loss : 0.016919, loss_ce: 0.006288
2022-01-12 01:03:06,996 iteration 5993 : loss : 0.011263, loss_ce: 0.005103
2022-01-12 01:03:08,588 iteration 5994 : loss : 0.026948, loss_ce: 0.015058
2022-01-12 01:03:10,122 iteration 5995 : loss : 0.015356, loss_ce: 0.005799
2022-01-12 01:03:11,634 iteration 5996 : loss : 0.011843, loss_ce: 0.003479
2022-01-12 01:03:13,217 iteration 5997 : loss : 0.022444, loss_ce: 0.008673
2022-01-12 01:03:14,835 iteration 5998 : loss : 0.015229, loss_ce: 0.005043
2022-01-12 01:03:16,481 iteration 5999 : loss : 0.021638, loss_ce: 0.007309
2022-01-12 01:03:18,050 iteration 6000 : loss : 0.013800, loss_ce: 0.005896
2022-01-12 01:03:19,543 iteration 6001 : loss : 0.013144, loss_ce: 0.006154
 88%|█████████████████████████▌   | 353/400 [2:50:23<21:48, 27.85s/it]2022-01-12 01:03:21,155 iteration 6002 : loss : 0.014456, loss_ce: 0.005485
2022-01-12 01:03:22,683 iteration 6003 : loss : 0.017348, loss_ce: 0.005528
2022-01-12 01:03:24,292 iteration 6004 : loss : 0.017184, loss_ce: 0.007715
2022-01-12 01:03:25,874 iteration 6005 : loss : 0.018724, loss_ce: 0.006526
2022-01-12 01:03:27,518 iteration 6006 : loss : 0.015499, loss_ce: 0.006297
2022-01-12 01:03:29,080 iteration 6007 : loss : 0.011307, loss_ce: 0.004584
2022-01-12 01:03:30,644 iteration 6008 : loss : 0.013903, loss_ce: 0.004772
2022-01-12 01:03:32,135 iteration 6009 : loss : 0.011283, loss_ce: 0.005110
2022-01-12 01:03:33,647 iteration 6010 : loss : 0.011222, loss_ce: 0.004555
2022-01-12 01:03:35,242 iteration 6011 : loss : 0.016492, loss_ce: 0.005555
2022-01-12 01:03:36,859 iteration 6012 : loss : 0.023980, loss_ce: 0.004848
2022-01-12 01:03:38,441 iteration 6013 : loss : 0.016644, loss_ce: 0.005790
2022-01-12 01:03:40,023 iteration 6014 : loss : 0.020686, loss_ce: 0.005335
2022-01-12 01:03:41,546 iteration 6015 : loss : 0.014893, loss_ce: 0.005745
2022-01-12 01:03:43,114 iteration 6016 : loss : 0.016261, loss_ce: 0.006101
2022-01-12 01:03:44,670 iteration 6017 : loss : 0.015374, loss_ce: 0.006224
2022-01-12 01:03:46,238 iteration 6018 : loss : 0.021317, loss_ce: 0.010007
 88%|█████████████████████████▋   | 354/400 [2:50:50<21:05, 27.50s/it]2022-01-12 01:03:47,832 iteration 6019 : loss : 0.019583, loss_ce: 0.010556
2022-01-12 01:03:49,375 iteration 6020 : loss : 0.012544, loss_ce: 0.005759
2022-01-12 01:03:50,945 iteration 6021 : loss : 0.017076, loss_ce: 0.005201
2022-01-12 01:03:52,461 iteration 6022 : loss : 0.012519, loss_ce: 0.005239
2022-01-12 01:03:54,060 iteration 6023 : loss : 0.014199, loss_ce: 0.005587
2022-01-12 01:03:55,605 iteration 6024 : loss : 0.017306, loss_ce: 0.005542
2022-01-12 01:03:57,119 iteration 6025 : loss : 0.015581, loss_ce: 0.005972
2022-01-12 01:03:58,680 iteration 6026 : loss : 0.020072, loss_ce: 0.007161
2022-01-12 01:04:00,242 iteration 6027 : loss : 0.028011, loss_ce: 0.009811
2022-01-12 01:04:01,836 iteration 6028 : loss : 0.014663, loss_ce: 0.006290
2022-01-12 01:04:03,371 iteration 6029 : loss : 0.019195, loss_ce: 0.005724
2022-01-12 01:04:04,865 iteration 6030 : loss : 0.012661, loss_ce: 0.006153
2022-01-12 01:04:06,450 iteration 6031 : loss : 0.013884, loss_ce: 0.006521
2022-01-12 01:04:07,972 iteration 6032 : loss : 0.017373, loss_ce: 0.005046
2022-01-12 01:04:09,576 iteration 6033 : loss : 0.017741, loss_ce: 0.006361
2022-01-12 01:04:11,131 iteration 6034 : loss : 0.015269, loss_ce: 0.005761
2022-01-12 01:04:11,131 Training Data Eval:
2022-01-12 01:04:19,121   Average segmentation loss on training set: 0.0082
2022-01-12 01:04:19,121 Validation Data Eval:
2022-01-12 01:04:21,874   Average segmentation loss on validation set: 0.0654
2022-01-12 01:04:23,342 iteration 6035 : loss : 0.010664, loss_ce: 0.004095
 89%|█████████████████████████▋   | 355/400 [2:51:27<22:47, 30.39s/it]2022-01-12 01:04:25,031 iteration 6036 : loss : 0.020815, loss_ce: 0.006335
2022-01-12 01:04:26,634 iteration 6037 : loss : 0.016496, loss_ce: 0.005039
2022-01-12 01:04:28,164 iteration 6038 : loss : 0.013512, loss_ce: 0.004945
2022-01-12 01:04:29,696 iteration 6039 : loss : 0.017685, loss_ce: 0.005559
2022-01-12 01:04:31,269 iteration 6040 : loss : 0.016278, loss_ce: 0.006069
2022-01-12 01:04:32,798 iteration 6041 : loss : 0.011475, loss_ce: 0.004090
2022-01-12 01:04:34,375 iteration 6042 : loss : 0.014736, loss_ce: 0.006887
2022-01-12 01:04:35,894 iteration 6043 : loss : 0.011876, loss_ce: 0.005596
2022-01-12 01:04:37,393 iteration 6044 : loss : 0.011211, loss_ce: 0.005054
2022-01-12 01:04:39,024 iteration 6045 : loss : 0.018412, loss_ce: 0.007386
2022-01-12 01:04:40,520 iteration 6046 : loss : 0.016363, loss_ce: 0.007059
2022-01-12 01:04:42,027 iteration 6047 : loss : 0.010212, loss_ce: 0.004290
2022-01-12 01:04:43,650 iteration 6048 : loss : 0.024753, loss_ce: 0.007011
2022-01-12 01:04:45,138 iteration 6049 : loss : 0.013331, loss_ce: 0.004625
2022-01-12 01:04:46,638 iteration 6050 : loss : 0.014130, loss_ce: 0.003478
2022-01-12 01:04:48,134 iteration 6051 : loss : 0.010555, loss_ce: 0.004280
2022-01-12 01:04:49,706 iteration 6052 : loss : 0.019108, loss_ce: 0.007685
 89%|█████████████████████████▊   | 356/400 [2:51:53<21:23, 29.18s/it]2022-01-12 01:04:51,280 iteration 6053 : loss : 0.015646, loss_ce: 0.005685
2022-01-12 01:04:52,806 iteration 6054 : loss : 0.013778, loss_ce: 0.004515
2022-01-12 01:04:54,393 iteration 6055 : loss : 0.013428, loss_ce: 0.005898
2022-01-12 01:04:55,939 iteration 6056 : loss : 0.012912, loss_ce: 0.004199
2022-01-12 01:04:57,418 iteration 6057 : loss : 0.016999, loss_ce: 0.006834
2022-01-12 01:04:58,929 iteration 6058 : loss : 0.012461, loss_ce: 0.007148
2022-01-12 01:05:00,541 iteration 6059 : loss : 0.019382, loss_ce: 0.006099
2022-01-12 01:05:02,101 iteration 6060 : loss : 0.019482, loss_ce: 0.006362
2022-01-12 01:05:03,608 iteration 6061 : loss : 0.014049, loss_ce: 0.006629
2022-01-12 01:05:05,284 iteration 6062 : loss : 0.019748, loss_ce: 0.007209
2022-01-12 01:05:06,796 iteration 6063 : loss : 0.012680, loss_ce: 0.005441
2022-01-12 01:05:08,326 iteration 6064 : loss : 0.011814, loss_ce: 0.003889
2022-01-12 01:05:09,913 iteration 6065 : loss : 0.023960, loss_ce: 0.009258
2022-01-12 01:05:11,408 iteration 6066 : loss : 0.016039, loss_ce: 0.006210
2022-01-12 01:05:12,994 iteration 6067 : loss : 0.025409, loss_ce: 0.008800
2022-01-12 01:05:14,547 iteration 6068 : loss : 0.014855, loss_ce: 0.002613
2022-01-12 01:05:16,180 iteration 6069 : loss : 0.016689, loss_ce: 0.007444
 89%|█████████████████████████▉   | 357/400 [2:52:20<20:19, 28.37s/it]2022-01-12 01:05:17,781 iteration 6070 : loss : 0.014681, loss_ce: 0.006532
2022-01-12 01:05:19,338 iteration 6071 : loss : 0.015671, loss_ce: 0.006738
2022-01-12 01:05:20,884 iteration 6072 : loss : 0.017135, loss_ce: 0.005858
2022-01-12 01:05:22,450 iteration 6073 : loss : 0.016722, loss_ce: 0.004727
2022-01-12 01:05:23,933 iteration 6074 : loss : 0.016023, loss_ce: 0.006549
2022-01-12 01:05:25,528 iteration 6075 : loss : 0.015357, loss_ce: 0.006680
2022-01-12 01:05:27,088 iteration 6076 : loss : 0.014275, loss_ce: 0.005882
2022-01-12 01:05:28,544 iteration 6077 : loss : 0.010367, loss_ce: 0.003788
2022-01-12 01:05:30,085 iteration 6078 : loss : 0.014208, loss_ce: 0.007482
2022-01-12 01:05:31,688 iteration 6079 : loss : 0.015153, loss_ce: 0.006760
2022-01-12 01:05:33,169 iteration 6080 : loss : 0.012721, loss_ce: 0.004755
2022-01-12 01:05:34,802 iteration 6081 : loss : 0.019342, loss_ce: 0.007355
2022-01-12 01:05:36,349 iteration 6082 : loss : 0.016603, loss_ce: 0.003761
2022-01-12 01:05:37,941 iteration 6083 : loss : 0.017511, loss_ce: 0.005655
2022-01-12 01:05:39,448 iteration 6084 : loss : 0.015548, loss_ce: 0.004436
2022-01-12 01:05:40,944 iteration 6085 : loss : 0.016041, loss_ce: 0.007100
2022-01-12 01:05:42,635 iteration 6086 : loss : 0.022090, loss_ce: 0.006622
 90%|█████████████████████████▉   | 358/400 [2:52:46<19:27, 27.79s/it]2022-01-12 01:05:44,168 iteration 6087 : loss : 0.012979, loss_ce: 0.004961
2022-01-12 01:05:45,740 iteration 6088 : loss : 0.017016, loss_ce: 0.007220
2022-01-12 01:05:47,278 iteration 6089 : loss : 0.028235, loss_ce: 0.013324
2022-01-12 01:05:48,829 iteration 6090 : loss : 0.014676, loss_ce: 0.003865
2022-01-12 01:05:50,382 iteration 6091 : loss : 0.012530, loss_ce: 0.005551
2022-01-12 01:05:51,937 iteration 6092 : loss : 0.013221, loss_ce: 0.005576
2022-01-12 01:05:53,416 iteration 6093 : loss : 0.011772, loss_ce: 0.004028
2022-01-12 01:05:54,917 iteration 6094 : loss : 0.014525, loss_ce: 0.005299
2022-01-12 01:05:56,502 iteration 6095 : loss : 0.015469, loss_ce: 0.006231
2022-01-12 01:05:58,088 iteration 6096 : loss : 0.013625, loss_ce: 0.004709
2022-01-12 01:05:59,578 iteration 6097 : loss : 0.012549, loss_ce: 0.005467
2022-01-12 01:06:01,138 iteration 6098 : loss : 0.021083, loss_ce: 0.008036
2022-01-12 01:06:02,721 iteration 6099 : loss : 0.023217, loss_ce: 0.006193
2022-01-12 01:06:04,346 iteration 6100 : loss : 0.017906, loss_ce: 0.006167
2022-01-12 01:06:05,929 iteration 6101 : loss : 0.013699, loss_ce: 0.005021
2022-01-12 01:06:07,437 iteration 6102 : loss : 0.014979, loss_ce: 0.004811
2022-01-12 01:06:08,890 iteration 6103 : loss : 0.008651, loss_ce: 0.003200
 90%|██████████████████████████   | 359/400 [2:53:13<18:40, 27.33s/it]2022-01-12 01:06:10,522 iteration 6104 : loss : 0.017839, loss_ce: 0.005381
2022-01-12 01:06:12,023 iteration 6105 : loss : 0.014466, loss_ce: 0.004881
2022-01-12 01:06:13,492 iteration 6106 : loss : 0.012141, loss_ce: 0.004182
2022-01-12 01:06:14,998 iteration 6107 : loss : 0.017006, loss_ce: 0.006099
2022-01-12 01:06:16,590 iteration 6108 : loss : 0.013944, loss_ce: 0.003981
2022-01-12 01:06:18,118 iteration 6109 : loss : 0.011844, loss_ce: 0.004670
2022-01-12 01:06:19,690 iteration 6110 : loss : 0.021156, loss_ce: 0.010069
2022-01-12 01:06:21,294 iteration 6111 : loss : 0.016391, loss_ce: 0.006516
2022-01-12 01:06:22,873 iteration 6112 : loss : 0.012531, loss_ce: 0.004345
2022-01-12 01:06:24,495 iteration 6113 : loss : 0.015906, loss_ce: 0.005225
2022-01-12 01:06:26,034 iteration 6114 : loss : 0.015348, loss_ce: 0.006597
2022-01-12 01:06:27,663 iteration 6115 : loss : 0.024558, loss_ce: 0.008675
2022-01-12 01:06:29,228 iteration 6116 : loss : 0.014530, loss_ce: 0.005994
2022-01-12 01:06:30,750 iteration 6117 : loss : 0.019434, loss_ce: 0.008845
2022-01-12 01:06:32,347 iteration 6118 : loss : 0.016058, loss_ce: 0.007346
2022-01-12 01:06:33,895 iteration 6119 : loss : 0.022174, loss_ce: 0.008510
2022-01-12 01:06:33,896 Training Data Eval:
2022-01-12 01:06:41,883   Average segmentation loss on training set: 0.0082
2022-01-12 01:06:41,883 Validation Data Eval:
2022-01-12 01:06:44,636   Average segmentation loss on validation set: 0.0763
2022-01-12 01:06:46,194 iteration 6120 : loss : 0.014721, loss_ce: 0.006500
 90%|██████████████████████████   | 360/400 [2:53:50<20:12, 30.32s/it]2022-01-12 01:06:47,814 iteration 6121 : loss : 0.014477, loss_ce: 0.006078
2022-01-12 01:06:49,411 iteration 6122 : loss : 0.017483, loss_ce: 0.006214
2022-01-12 01:06:51,050 iteration 6123 : loss : 0.017154, loss_ce: 0.007034
2022-01-12 01:06:52,619 iteration 6124 : loss : 0.012944, loss_ce: 0.003918
2022-01-12 01:06:54,258 iteration 6125 : loss : 0.035185, loss_ce: 0.011702
2022-01-12 01:06:55,803 iteration 6126 : loss : 0.016919, loss_ce: 0.003602
2022-01-12 01:06:57,329 iteration 6127 : loss : 0.012561, loss_ce: 0.004797
2022-01-12 01:06:58,958 iteration 6128 : loss : 0.015643, loss_ce: 0.005674
2022-01-12 01:07:00,524 iteration 6129 : loss : 0.017574, loss_ce: 0.005813
2022-01-12 01:07:02,154 iteration 6130 : loss : 0.022507, loss_ce: 0.011776
2022-01-12 01:07:03,716 iteration 6131 : loss : 0.022547, loss_ce: 0.013665
2022-01-12 01:07:05,283 iteration 6132 : loss : 0.016139, loss_ce: 0.003127
2022-01-12 01:07:06,811 iteration 6133 : loss : 0.012145, loss_ce: 0.005951
2022-01-12 01:07:08,302 iteration 6134 : loss : 0.009559, loss_ce: 0.004061
2022-01-12 01:07:09,794 iteration 6135 : loss : 0.012054, loss_ce: 0.005048
2022-01-12 01:07:11,374 iteration 6136 : loss : 0.014815, loss_ce: 0.004832
2022-01-12 01:07:12,908 iteration 6137 : loss : 0.014530, loss_ce: 0.005123
 90%|██████████████████████████▏  | 361/400 [2:54:17<19:00, 29.24s/it]2022-01-12 01:07:14,460 iteration 6138 : loss : 0.011017, loss_ce: 0.003854
2022-01-12 01:07:16,046 iteration 6139 : loss : 0.017811, loss_ce: 0.007423
2022-01-12 01:07:17,602 iteration 6140 : loss : 0.010958, loss_ce: 0.004035
2022-01-12 01:07:19,186 iteration 6141 : loss : 0.011381, loss_ce: 0.004117
2022-01-12 01:07:20,871 iteration 6142 : loss : 0.017683, loss_ce: 0.006046
2022-01-12 01:07:22,357 iteration 6143 : loss : 0.011420, loss_ce: 0.005221
2022-01-12 01:07:23,927 iteration 6144 : loss : 0.010509, loss_ce: 0.004291
2022-01-12 01:07:25,510 iteration 6145 : loss : 0.018867, loss_ce: 0.006752
2022-01-12 01:07:27,062 iteration 6146 : loss : 0.013057, loss_ce: 0.005450
2022-01-12 01:07:28,600 iteration 6147 : loss : 0.014507, loss_ce: 0.003922
2022-01-12 01:07:30,108 iteration 6148 : loss : 0.010087, loss_ce: 0.003545
2022-01-12 01:07:31,621 iteration 6149 : loss : 0.017263, loss_ce: 0.006422
2022-01-12 01:07:33,150 iteration 6150 : loss : 0.014477, loss_ce: 0.004756
2022-01-12 01:07:34,706 iteration 6151 : loss : 0.014126, loss_ce: 0.003286
2022-01-12 01:07:36,220 iteration 6152 : loss : 0.014384, loss_ce: 0.005884
2022-01-12 01:07:37,835 iteration 6153 : loss : 0.018922, loss_ce: 0.008498
2022-01-12 01:07:39,481 iteration 6154 : loss : 0.022956, loss_ce: 0.010790
 90%|██████████████████████████▏  | 362/400 [2:54:43<18:00, 28.44s/it]2022-01-12 01:07:41,049 iteration 6155 : loss : 0.013956, loss_ce: 0.006070
2022-01-12 01:07:42,587 iteration 6156 : loss : 0.010572, loss_ce: 0.004467
2022-01-12 01:07:44,187 iteration 6157 : loss : 0.014046, loss_ce: 0.004750
2022-01-12 01:07:45,716 iteration 6158 : loss : 0.014179, loss_ce: 0.004917
2022-01-12 01:07:47,191 iteration 6159 : loss : 0.009984, loss_ce: 0.003976
2022-01-12 01:07:48,908 iteration 6160 : loss : 0.031207, loss_ce: 0.015856
2022-01-12 01:07:50,391 iteration 6161 : loss : 0.016003, loss_ce: 0.004789
2022-01-12 01:07:51,958 iteration 6162 : loss : 0.011400, loss_ce: 0.004258
2022-01-12 01:07:53,518 iteration 6163 : loss : 0.011698, loss_ce: 0.004501
2022-01-12 01:07:55,037 iteration 6164 : loss : 0.014812, loss_ce: 0.004546
2022-01-12 01:07:56,609 iteration 6165 : loss : 0.011409, loss_ce: 0.003954
2022-01-12 01:07:58,180 iteration 6166 : loss : 0.020337, loss_ce: 0.009958
2022-01-12 01:07:59,648 iteration 6167 : loss : 0.010383, loss_ce: 0.003191
2022-01-12 01:08:01,300 iteration 6168 : loss : 0.030179, loss_ce: 0.009034
2022-01-12 01:08:02,850 iteration 6169 : loss : 0.013852, loss_ce: 0.005960
2022-01-12 01:08:04,439 iteration 6170 : loss : 0.014238, loss_ce: 0.006091
2022-01-12 01:08:05,994 iteration 6171 : loss : 0.015088, loss_ce: 0.004519
 91%|██████████████████████████▎  | 363/400 [2:55:10<17:10, 27.86s/it]2022-01-12 01:08:07,577 iteration 6172 : loss : 0.012600, loss_ce: 0.004709
2022-01-12 01:08:09,047 iteration 6173 : loss : 0.009823, loss_ce: 0.003505
2022-01-12 01:08:10,586 iteration 6174 : loss : 0.013151, loss_ce: 0.006257
2022-01-12 01:08:12,083 iteration 6175 : loss : 0.010488, loss_ce: 0.003025
2022-01-12 01:08:13,664 iteration 6176 : loss : 0.016733, loss_ce: 0.008371
2022-01-12 01:08:15,308 iteration 6177 : loss : 0.015863, loss_ce: 0.007583
2022-01-12 01:08:16,904 iteration 6178 : loss : 0.016123, loss_ce: 0.005121
2022-01-12 01:08:18,484 iteration 6179 : loss : 0.020640, loss_ce: 0.006851
2022-01-12 01:08:20,070 iteration 6180 : loss : 0.018074, loss_ce: 0.002911
2022-01-12 01:08:21,600 iteration 6181 : loss : 0.014597, loss_ce: 0.005495
2022-01-12 01:08:23,174 iteration 6182 : loss : 0.012943, loss_ce: 0.005149
2022-01-12 01:08:24,696 iteration 6183 : loss : 0.015127, loss_ce: 0.005554
2022-01-12 01:08:26,216 iteration 6184 : loss : 0.015098, loss_ce: 0.005380
2022-01-12 01:08:27,826 iteration 6185 : loss : 0.017801, loss_ce: 0.005378
2022-01-12 01:08:29,327 iteration 6186 : loss : 0.012747, loss_ce: 0.004867
2022-01-12 01:08:30,847 iteration 6187 : loss : 0.014680, loss_ce: 0.004649
2022-01-12 01:08:32,354 iteration 6188 : loss : 0.019409, loss_ce: 0.006282
 91%|██████████████████████████▍  | 364/400 [2:55:36<16:26, 27.41s/it]2022-01-12 01:08:34,027 iteration 6189 : loss : 0.021713, loss_ce: 0.007718
2022-01-12 01:08:35,602 iteration 6190 : loss : 0.012475, loss_ce: 0.004664
2022-01-12 01:08:37,241 iteration 6191 : loss : 0.011410, loss_ce: 0.003289
2022-01-12 01:08:38,845 iteration 6192 : loss : 0.016872, loss_ce: 0.005940
2022-01-12 01:08:40,366 iteration 6193 : loss : 0.011045, loss_ce: 0.003491
2022-01-12 01:08:41,970 iteration 6194 : loss : 0.015033, loss_ce: 0.005918
2022-01-12 01:08:43,610 iteration 6195 : loss : 0.016461, loss_ce: 0.005568
2022-01-12 01:08:45,130 iteration 6196 : loss : 0.017356, loss_ce: 0.006859
2022-01-12 01:08:46,767 iteration 6197 : loss : 0.024384, loss_ce: 0.007744
2022-01-12 01:08:48,408 iteration 6198 : loss : 0.020621, loss_ce: 0.008645
2022-01-12 01:08:49,944 iteration 6199 : loss : 0.011347, loss_ce: 0.004310
2022-01-12 01:08:51,545 iteration 6200 : loss : 0.025349, loss_ce: 0.010280
2022-01-12 01:08:53,113 iteration 6201 : loss : 0.012870, loss_ce: 0.005289
2022-01-12 01:08:54,657 iteration 6202 : loss : 0.012227, loss_ce: 0.003708
2022-01-12 01:08:56,199 iteration 6203 : loss : 0.032704, loss_ce: 0.009130
2022-01-12 01:08:57,796 iteration 6204 : loss : 0.017399, loss_ce: 0.007846
2022-01-12 01:08:57,797 Training Data Eval:
2022-01-12 01:09:05,786   Average segmentation loss on training set: 0.0078
2022-01-12 01:09:05,786 Validation Data Eval:
2022-01-12 01:09:08,537   Average segmentation loss on validation set: 0.0718
2022-01-12 01:09:10,203 iteration 6205 : loss : 0.016875, loss_ce: 0.007394
 91%|██████████████████████████▍  | 365/400 [2:56:14<17:48, 30.54s/it]2022-01-12 01:09:11,832 iteration 6206 : loss : 0.027643, loss_ce: 0.010404
2022-01-12 01:09:13,412 iteration 6207 : loss : 0.014383, loss_ce: 0.007551
2022-01-12 01:09:15,002 iteration 6208 : loss : 0.015056, loss_ce: 0.005820
2022-01-12 01:09:16,590 iteration 6209 : loss : 0.020406, loss_ce: 0.008293
2022-01-12 01:09:18,143 iteration 6210 : loss : 0.022269, loss_ce: 0.008888
2022-01-12 01:09:19,638 iteration 6211 : loss : 0.012726, loss_ce: 0.004440
2022-01-12 01:09:21,186 iteration 6212 : loss : 0.018759, loss_ce: 0.007161
2022-01-12 01:09:22,738 iteration 6213 : loss : 0.011433, loss_ce: 0.004381
2022-01-12 01:09:24,281 iteration 6214 : loss : 0.010670, loss_ce: 0.004254
2022-01-12 01:09:25,892 iteration 6215 : loss : 0.020135, loss_ce: 0.006768
2022-01-12 01:09:27,441 iteration 6216 : loss : 0.023000, loss_ce: 0.010297
2022-01-12 01:09:28,937 iteration 6217 : loss : 0.015331, loss_ce: 0.005364
2022-01-12 01:09:30,503 iteration 6218 : loss : 0.015913, loss_ce: 0.006339
2022-01-12 01:09:32,057 iteration 6219 : loss : 0.013298, loss_ce: 0.004617
2022-01-12 01:09:33,581 iteration 6220 : loss : 0.016398, loss_ce: 0.004974
2022-01-12 01:09:35,156 iteration 6221 : loss : 0.012033, loss_ce: 0.003688
2022-01-12 01:09:36,666 iteration 6222 : loss : 0.011330, loss_ce: 0.005248
 92%|██████████████████████████▌  | 366/400 [2:56:40<16:36, 29.32s/it]2022-01-12 01:09:38,282 iteration 6223 : loss : 0.017312, loss_ce: 0.007841
2022-01-12 01:09:39,818 iteration 6224 : loss : 0.015526, loss_ce: 0.005535
2022-01-12 01:09:41,378 iteration 6225 : loss : 0.028313, loss_ce: 0.009291
2022-01-12 01:09:42,895 iteration 6226 : loss : 0.013006, loss_ce: 0.004691
2022-01-12 01:09:44,392 iteration 6227 : loss : 0.013707, loss_ce: 0.006216
2022-01-12 01:09:45,961 iteration 6228 : loss : 0.016966, loss_ce: 0.006444
2022-01-12 01:09:47,530 iteration 6229 : loss : 0.010146, loss_ce: 0.004585
2022-01-12 01:09:49,112 iteration 6230 : loss : 0.022823, loss_ce: 0.007671
2022-01-12 01:09:50,718 iteration 6231 : loss : 0.038747, loss_ce: 0.020151
2022-01-12 01:09:52,291 iteration 6232 : loss : 0.012083, loss_ce: 0.004792
2022-01-12 01:09:53,845 iteration 6233 : loss : 0.013410, loss_ce: 0.005352
2022-01-12 01:09:55,388 iteration 6234 : loss : 0.022941, loss_ce: 0.004505
2022-01-12 01:09:56,913 iteration 6235 : loss : 0.013547, loss_ce: 0.007017
2022-01-12 01:09:58,477 iteration 6236 : loss : 0.016435, loss_ce: 0.009625
2022-01-12 01:10:00,043 iteration 6237 : loss : 0.017862, loss_ce: 0.008254
2022-01-12 01:10:01,653 iteration 6238 : loss : 0.019071, loss_ce: 0.005366
2022-01-12 01:10:03,215 iteration 6239 : loss : 0.015391, loss_ce: 0.004053
 92%|██████████████████████████▌  | 367/400 [2:57:07<15:40, 28.49s/it]2022-01-12 01:10:04,796 iteration 6240 : loss : 0.010223, loss_ce: 0.003448
2022-01-12 01:10:06,317 iteration 6241 : loss : 0.010560, loss_ce: 0.004920
2022-01-12 01:10:07,943 iteration 6242 : loss : 0.020270, loss_ce: 0.006177
2022-01-12 01:10:09,486 iteration 6243 : loss : 0.011107, loss_ce: 0.004601
2022-01-12 01:10:11,118 iteration 6244 : loss : 0.016898, loss_ce: 0.009275
2022-01-12 01:10:12,660 iteration 6245 : loss : 0.014891, loss_ce: 0.005103
2022-01-12 01:10:14,201 iteration 6246 : loss : 0.015750, loss_ce: 0.007838
2022-01-12 01:10:15,780 iteration 6247 : loss : 0.023860, loss_ce: 0.006526
2022-01-12 01:10:17,328 iteration 6248 : loss : 0.012671, loss_ce: 0.005318
2022-01-12 01:10:18,856 iteration 6249 : loss : 0.025949, loss_ce: 0.006828
2022-01-12 01:10:20,435 iteration 6250 : loss : 0.012251, loss_ce: 0.004250
2022-01-12 01:10:22,079 iteration 6251 : loss : 0.016825, loss_ce: 0.006704
2022-01-12 01:10:23,582 iteration 6252 : loss : 0.013996, loss_ce: 0.006158
2022-01-12 01:10:25,188 iteration 6253 : loss : 0.015443, loss_ce: 0.005426
2022-01-12 01:10:26,728 iteration 6254 : loss : 0.013209, loss_ce: 0.006320
2022-01-12 01:10:28,300 iteration 6255 : loss : 0.013585, loss_ce: 0.005473
2022-01-12 01:10:29,914 iteration 6256 : loss : 0.019210, loss_ce: 0.007906
 92%|██████████████████████████▋  | 368/400 [2:57:34<14:54, 27.95s/it]2022-01-12 01:10:31,544 iteration 6257 : loss : 0.013874, loss_ce: 0.004656
2022-01-12 01:10:33,139 iteration 6258 : loss : 0.022020, loss_ce: 0.007621
2022-01-12 01:10:34,754 iteration 6259 : loss : 0.020781, loss_ce: 0.008595
2022-01-12 01:10:36,290 iteration 6260 : loss : 0.026746, loss_ce: 0.012566
2022-01-12 01:10:37,833 iteration 6261 : loss : 0.017913, loss_ce: 0.006281
2022-01-12 01:10:39,420 iteration 6262 : loss : 0.034618, loss_ce: 0.008894
2022-01-12 01:10:40,989 iteration 6263 : loss : 0.021991, loss_ce: 0.004938
2022-01-12 01:10:42,556 iteration 6264 : loss : 0.011982, loss_ce: 0.004871
2022-01-12 01:10:44,130 iteration 6265 : loss : 0.014003, loss_ce: 0.005673
2022-01-12 01:10:45,676 iteration 6266 : loss : 0.017329, loss_ce: 0.009078
2022-01-12 01:10:47,208 iteration 6267 : loss : 0.014785, loss_ce: 0.006427
2022-01-12 01:10:48,764 iteration 6268 : loss : 0.013606, loss_ce: 0.005200
2022-01-12 01:10:50,197 iteration 6269 : loss : 0.011878, loss_ce: 0.003864
2022-01-12 01:10:51,746 iteration 6270 : loss : 0.011895, loss_ce: 0.005448
2022-01-12 01:10:53,287 iteration 6271 : loss : 0.014127, loss_ce: 0.003975
2022-01-12 01:10:54,880 iteration 6272 : loss : 0.021229, loss_ce: 0.007994
2022-01-12 01:10:56,475 iteration 6273 : loss : 0.014120, loss_ce: 0.005603
 92%|██████████████████████████▊  | 369/400 [2:58:00<14:13, 27.53s/it]2022-01-12 01:10:58,195 iteration 6274 : loss : 0.020577, loss_ce: 0.006131
2022-01-12 01:10:59,757 iteration 6275 : loss : 0.016255, loss_ce: 0.007869
2022-01-12 01:11:01,330 iteration 6276 : loss : 0.013599, loss_ce: 0.005366
2022-01-12 01:11:02,860 iteration 6277 : loss : 0.012371, loss_ce: 0.003761
2022-01-12 01:11:04,435 iteration 6278 : loss : 0.016090, loss_ce: 0.005895
2022-01-12 01:11:05,981 iteration 6279 : loss : 0.020216, loss_ce: 0.008449
2022-01-12 01:11:07,503 iteration 6280 : loss : 0.020443, loss_ce: 0.007268
2022-01-12 01:11:09,156 iteration 6281 : loss : 0.028753, loss_ce: 0.007292
2022-01-12 01:11:10,694 iteration 6282 : loss : 0.011425, loss_ce: 0.004139
2022-01-12 01:11:12,216 iteration 6283 : loss : 0.011815, loss_ce: 0.005309
2022-01-12 01:11:13,777 iteration 6284 : loss : 0.016264, loss_ce: 0.006732
2022-01-12 01:11:15,367 iteration 6285 : loss : 0.013214, loss_ce: 0.004610
2022-01-12 01:11:16,899 iteration 6286 : loss : 0.011939, loss_ce: 0.004142
2022-01-12 01:11:18,435 iteration 6287 : loss : 0.013348, loss_ce: 0.003701
2022-01-12 01:11:19,920 iteration 6288 : loss : 0.010595, loss_ce: 0.003771
2022-01-12 01:11:21,499 iteration 6289 : loss : 0.013512, loss_ce: 0.006087
2022-01-12 01:11:21,499 Training Data Eval:
2022-01-12 01:11:29,496   Average segmentation loss on training set: 0.0077
2022-01-12 01:11:29,497 Validation Data Eval:
2022-01-12 01:11:32,248   Average segmentation loss on validation set: 0.0652
2022-01-12 01:11:33,862 iteration 6290 : loss : 0.019371, loss_ce: 0.009153
 92%|██████████████████████████▊  | 370/400 [2:58:37<15:14, 30.49s/it]2022-01-12 01:11:35,463 iteration 6291 : loss : 0.010519, loss_ce: 0.004069
2022-01-12 01:11:37,032 iteration 6292 : loss : 0.016811, loss_ce: 0.005295
2022-01-12 01:11:38,634 iteration 6293 : loss : 0.019481, loss_ce: 0.009300
2022-01-12 01:11:40,242 iteration 6294 : loss : 0.021152, loss_ce: 0.008045
2022-01-12 01:11:41,804 iteration 6295 : loss : 0.015285, loss_ce: 0.007462
2022-01-12 01:11:43,436 iteration 6296 : loss : 0.019172, loss_ce: 0.006565
2022-01-12 01:11:45,000 iteration 6297 : loss : 0.018070, loss_ce: 0.006956
2022-01-12 01:11:46,600 iteration 6298 : loss : 0.024189, loss_ce: 0.009960
2022-01-12 01:11:48,158 iteration 6299 : loss : 0.013029, loss_ce: 0.004612
2022-01-12 01:11:49,697 iteration 6300 : loss : 0.015353, loss_ce: 0.005090
2022-01-12 01:11:51,227 iteration 6301 : loss : 0.012588, loss_ce: 0.007167
2022-01-12 01:11:52,814 iteration 6302 : loss : 0.012575, loss_ce: 0.004594
2022-01-12 01:11:54,298 iteration 6303 : loss : 0.010638, loss_ce: 0.002468
2022-01-12 01:11:55,790 iteration 6304 : loss : 0.012367, loss_ce: 0.003455
2022-01-12 01:11:57,368 iteration 6305 : loss : 0.018803, loss_ce: 0.008856
2022-01-12 01:11:58,904 iteration 6306 : loss : 0.023704, loss_ce: 0.005096
2022-01-12 01:12:00,459 iteration 6307 : loss : 0.011438, loss_ce: 0.005382
 93%|██████████████████████████▉  | 371/400 [2:59:04<14:10, 29.32s/it]2022-01-12 01:12:02,015 iteration 6308 : loss : 0.013340, loss_ce: 0.002933
2022-01-12 01:12:03,596 iteration 6309 : loss : 0.018910, loss_ce: 0.007672
2022-01-12 01:12:05,142 iteration 6310 : loss : 0.018072, loss_ce: 0.007417
2022-01-12 01:12:06,724 iteration 6311 : loss : 0.010607, loss_ce: 0.004199
2022-01-12 01:12:08,317 iteration 6312 : loss : 0.026331, loss_ce: 0.009718
2022-01-12 01:12:09,950 iteration 6313 : loss : 0.016110, loss_ce: 0.007649
2022-01-12 01:12:11,497 iteration 6314 : loss : 0.010340, loss_ce: 0.003536
2022-01-12 01:12:13,024 iteration 6315 : loss : 0.013357, loss_ce: 0.005143
2022-01-12 01:12:14,582 iteration 6316 : loss : 0.015229, loss_ce: 0.005209
2022-01-12 01:12:16,121 iteration 6317 : loss : 0.013534, loss_ce: 0.003827
2022-01-12 01:12:17,721 iteration 6318 : loss : 0.023026, loss_ce: 0.008184
2022-01-12 01:12:19,290 iteration 6319 : loss : 0.013678, loss_ce: 0.004923
2022-01-12 01:12:20,838 iteration 6320 : loss : 0.012267, loss_ce: 0.003954
2022-01-12 01:12:22,436 iteration 6321 : loss : 0.019838, loss_ce: 0.011117
2022-01-12 01:12:24,047 iteration 6322 : loss : 0.015252, loss_ce: 0.007597
2022-01-12 01:12:25,745 iteration 6323 : loss : 0.018922, loss_ce: 0.006506
2022-01-12 01:12:27,376 iteration 6324 : loss : 0.019070, loss_ce: 0.009115
 93%|██████████████████████████▉  | 372/400 [2:59:31<13:20, 28.60s/it]2022-01-12 01:12:28,990 iteration 6325 : loss : 0.017885, loss_ce: 0.005451
2022-01-12 01:12:30,521 iteration 6326 : loss : 0.015529, loss_ce: 0.006687
2022-01-12 01:12:32,092 iteration 6327 : loss : 0.023900, loss_ce: 0.007914
2022-01-12 01:12:33,709 iteration 6328 : loss : 0.011156, loss_ce: 0.005050
2022-01-12 01:12:35,200 iteration 6329 : loss : 0.012886, loss_ce: 0.006533
2022-01-12 01:12:36,748 iteration 6330 : loss : 0.016759, loss_ce: 0.006480
2022-01-12 01:12:38,373 iteration 6331 : loss : 0.015751, loss_ce: 0.005763
2022-01-12 01:12:39,974 iteration 6332 : loss : 0.015597, loss_ce: 0.004426
2022-01-12 01:12:41,539 iteration 6333 : loss : 0.010065, loss_ce: 0.004672
2022-01-12 01:12:43,098 iteration 6334 : loss : 0.014582, loss_ce: 0.003688
2022-01-12 01:12:44,732 iteration 6335 : loss : 0.017431, loss_ce: 0.007948
2022-01-12 01:12:46,262 iteration 6336 : loss : 0.011693, loss_ce: 0.004622
2022-01-12 01:12:47,853 iteration 6337 : loss : 0.016721, loss_ce: 0.004918
2022-01-12 01:12:49,418 iteration 6338 : loss : 0.016428, loss_ce: 0.004986
2022-01-12 01:12:50,966 iteration 6339 : loss : 0.013319, loss_ce: 0.004505
2022-01-12 01:12:52,517 iteration 6340 : loss : 0.016185, loss_ce: 0.006883
2022-01-12 01:12:54,014 iteration 6341 : loss : 0.010162, loss_ce: 0.003551
 93%|███████████████████████████  | 373/400 [2:59:58<12:36, 28.01s/it]2022-01-12 01:12:55,646 iteration 6342 : loss : 0.017116, loss_ce: 0.004915
2022-01-12 01:12:57,178 iteration 6343 : loss : 0.012143, loss_ce: 0.004417
2022-01-12 01:12:58,713 iteration 6344 : loss : 0.012615, loss_ce: 0.003405
2022-01-12 01:13:00,233 iteration 6345 : loss : 0.032467, loss_ce: 0.003525
2022-01-12 01:13:01,811 iteration 6346 : loss : 0.025848, loss_ce: 0.007110
2022-01-12 01:13:03,374 iteration 6347 : loss : 0.018016, loss_ce: 0.006429
2022-01-12 01:13:04,848 iteration 6348 : loss : 0.009689, loss_ce: 0.004080
2022-01-12 01:13:06,413 iteration 6349 : loss : 0.021185, loss_ce: 0.011013
2022-01-12 01:13:08,065 iteration 6350 : loss : 0.037584, loss_ce: 0.014339
2022-01-12 01:13:09,670 iteration 6351 : loss : 0.013084, loss_ce: 0.005843
2022-01-12 01:13:11,230 iteration 6352 : loss : 0.020327, loss_ce: 0.006140
2022-01-12 01:13:12,801 iteration 6353 : loss : 0.015946, loss_ce: 0.006505
2022-01-12 01:13:14,366 iteration 6354 : loss : 0.015860, loss_ce: 0.006961
2022-01-12 01:13:15,900 iteration 6355 : loss : 0.017243, loss_ce: 0.005837
2022-01-12 01:13:17,501 iteration 6356 : loss : 0.020867, loss_ce: 0.004760
2022-01-12 01:13:19,023 iteration 6357 : loss : 0.014519, loss_ce: 0.005092
2022-01-12 01:13:20,673 iteration 6358 : loss : 0.018460, loss_ce: 0.008224
 94%|███████████████████████████  | 374/400 [3:00:24<11:57, 27.61s/it]2022-01-12 01:13:22,318 iteration 6359 : loss : 0.024293, loss_ce: 0.006108
2022-01-12 01:13:23,831 iteration 6360 : loss : 0.014377, loss_ce: 0.005919
2022-01-12 01:13:25,423 iteration 6361 : loss : 0.015766, loss_ce: 0.006101
2022-01-12 01:13:27,017 iteration 6362 : loss : 0.016274, loss_ce: 0.006748
2022-01-12 01:13:28,632 iteration 6363 : loss : 0.020258, loss_ce: 0.007784
2022-01-12 01:13:30,159 iteration 6364 : loss : 0.012989, loss_ce: 0.003134
2022-01-12 01:13:31,728 iteration 6365 : loss : 0.028902, loss_ce: 0.013353
2022-01-12 01:13:33,376 iteration 6366 : loss : 0.019229, loss_ce: 0.009042
2022-01-12 01:13:35,034 iteration 6367 : loss : 0.014387, loss_ce: 0.005367
2022-01-12 01:13:36,623 iteration 6368 : loss : 0.018524, loss_ce: 0.007950
2022-01-12 01:13:38,188 iteration 6369 : loss : 0.013251, loss_ce: 0.006446
2022-01-12 01:13:39,751 iteration 6370 : loss : 0.014890, loss_ce: 0.005180
2022-01-12 01:13:41,393 iteration 6371 : loss : 0.024656, loss_ce: 0.009631
2022-01-12 01:13:42,905 iteration 6372 : loss : 0.009569, loss_ce: 0.003841
2022-01-12 01:13:44,428 iteration 6373 : loss : 0.011210, loss_ce: 0.004086
2022-01-12 01:13:46,002 iteration 6374 : loss : 0.016021, loss_ce: 0.006694
2022-01-12 01:13:46,002 Training Data Eval:
2022-01-12 01:13:53,984   Average segmentation loss on training set: 0.0078
2022-01-12 01:13:53,985 Validation Data Eval:
2022-01-12 01:13:56,735   Average segmentation loss on validation set: 0.0674
2022-01-12 01:13:58,318 iteration 6375 : loss : 0.016695, loss_ce: 0.005801
 94%|███████████████████████████▏ | 375/400 [3:01:02<12:45, 30.62s/it]2022-01-12 01:13:59,950 iteration 6376 : loss : 0.020633, loss_ce: 0.006422
2022-01-12 01:14:01,519 iteration 6377 : loss : 0.019145, loss_ce: 0.007842
2022-01-12 01:14:02,999 iteration 6378 : loss : 0.011918, loss_ce: 0.005395
2022-01-12 01:14:04,557 iteration 6379 : loss : 0.014744, loss_ce: 0.003594
2022-01-12 01:14:06,096 iteration 6380 : loss : 0.015803, loss_ce: 0.006185
2022-01-12 01:14:07,622 iteration 6381 : loss : 0.009744, loss_ce: 0.003072
2022-01-12 01:14:09,093 iteration 6382 : loss : 0.012383, loss_ce: 0.004338
2022-01-12 01:14:10,677 iteration 6383 : loss : 0.024168, loss_ce: 0.009789
2022-01-12 01:14:12,201 iteration 6384 : loss : 0.016780, loss_ce: 0.005903
2022-01-12 01:14:13,787 iteration 6385 : loss : 0.017808, loss_ce: 0.005923
2022-01-12 01:14:15,386 iteration 6386 : loss : 0.017884, loss_ce: 0.005847
2022-01-12 01:14:17,029 iteration 6387 : loss : 0.015577, loss_ce: 0.007171
2022-01-12 01:14:18,616 iteration 6388 : loss : 0.015683, loss_ce: 0.005921
2022-01-12 01:14:20,075 iteration 6389 : loss : 0.011472, loss_ce: 0.003925
2022-01-12 01:14:21,638 iteration 6390 : loss : 0.016011, loss_ce: 0.007680
2022-01-12 01:14:23,145 iteration 6391 : loss : 0.011761, loss_ce: 0.006547
2022-01-12 01:14:24,754 iteration 6392 : loss : 0.017962, loss_ce: 0.005099
 94%|███████████████████████████▎ | 376/400 [3:01:28<11:44, 29.36s/it]2022-01-12 01:14:26,332 iteration 6393 : loss : 0.012713, loss_ce: 0.004875
2022-01-12 01:14:27,934 iteration 6394 : loss : 0.019984, loss_ce: 0.008501
2022-01-12 01:14:29,475 iteration 6395 : loss : 0.016276, loss_ce: 0.006578
2022-01-12 01:14:31,028 iteration 6396 : loss : 0.021006, loss_ce: 0.005501
2022-01-12 01:14:32,603 iteration 6397 : loss : 0.014109, loss_ce: 0.005895
2022-01-12 01:14:34,167 iteration 6398 : loss : 0.011766, loss_ce: 0.003982
2022-01-12 01:14:35,711 iteration 6399 : loss : 0.014151, loss_ce: 0.004745
2022-01-12 01:14:37,249 iteration 6400 : loss : 0.020324, loss_ce: 0.008534
2022-01-12 01:14:38,842 iteration 6401 : loss : 0.014624, loss_ce: 0.005189
2022-01-12 01:14:40,373 iteration 6402 : loss : 0.014880, loss_ce: 0.007492
2022-01-12 01:14:41,998 iteration 6403 : loss : 0.019200, loss_ce: 0.007631
2022-01-12 01:14:43,542 iteration 6404 : loss : 0.010494, loss_ce: 0.004449
2022-01-12 01:14:45,070 iteration 6405 : loss : 0.021426, loss_ce: 0.006482
2022-01-12 01:14:46,775 iteration 6406 : loss : 0.027554, loss_ce: 0.006169
2022-01-12 01:14:48,357 iteration 6407 : loss : 0.017087, loss_ce: 0.004662
2022-01-12 01:14:49,805 iteration 6408 : loss : 0.010425, loss_ce: 0.003555
2022-01-12 01:14:51,344 iteration 6409 : loss : 0.014101, loss_ce: 0.006279
 94%|███████████████████████████▎ | 377/400 [3:01:55<10:56, 28.53s/it]2022-01-12 01:14:52,892 iteration 6410 : loss : 0.012300, loss_ce: 0.003717
2022-01-12 01:14:54,474 iteration 6411 : loss : 0.017461, loss_ce: 0.007054
2022-01-12 01:14:56,061 iteration 6412 : loss : 0.014817, loss_ce: 0.005305
2022-01-12 01:14:57,588 iteration 6413 : loss : 0.013910, loss_ce: 0.006162
2022-01-12 01:14:59,116 iteration 6414 : loss : 0.014553, loss_ce: 0.006868
2022-01-12 01:15:00,623 iteration 6415 : loss : 0.011854, loss_ce: 0.003841
2022-01-12 01:15:02,182 iteration 6416 : loss : 0.010624, loss_ce: 0.004184
2022-01-12 01:15:03,671 iteration 6417 : loss : 0.017984, loss_ce: 0.007077
2022-01-12 01:15:05,209 iteration 6418 : loss : 0.013479, loss_ce: 0.005284
2022-01-12 01:15:06,851 iteration 6419 : loss : 0.016790, loss_ce: 0.006738
2022-01-12 01:15:08,466 iteration 6420 : loss : 0.030957, loss_ce: 0.007532
2022-01-12 01:15:10,046 iteration 6421 : loss : 0.014929, loss_ce: 0.005492
2022-01-12 01:15:11,605 iteration 6422 : loss : 0.017097, loss_ce: 0.005379
2022-01-12 01:15:13,181 iteration 6423 : loss : 0.012571, loss_ce: 0.004933
2022-01-12 01:15:14,812 iteration 6424 : loss : 0.021300, loss_ce: 0.010141
2022-01-12 01:15:16,347 iteration 6425 : loss : 0.017559, loss_ce: 0.006501
2022-01-12 01:15:17,960 iteration 6426 : loss : 0.021465, loss_ce: 0.007478
 94%|███████████████████████████▍ | 378/400 [3:02:22<10:15, 27.95s/it]2022-01-12 01:15:19,578 iteration 6427 : loss : 0.012575, loss_ce: 0.003856
2022-01-12 01:15:21,132 iteration 6428 : loss : 0.021804, loss_ce: 0.006862
2022-01-12 01:15:22,701 iteration 6429 : loss : 0.014344, loss_ce: 0.004896
2022-01-12 01:15:24,302 iteration 6430 : loss : 0.016494, loss_ce: 0.007222
2022-01-12 01:15:25,809 iteration 6431 : loss : 0.017702, loss_ce: 0.005823
2022-01-12 01:15:27,378 iteration 6432 : loss : 0.013287, loss_ce: 0.006128
2022-01-12 01:15:28,932 iteration 6433 : loss : 0.011164, loss_ce: 0.003413
2022-01-12 01:15:30,467 iteration 6434 : loss : 0.013755, loss_ce: 0.005526
2022-01-12 01:15:32,006 iteration 6435 : loss : 0.019857, loss_ce: 0.008167
2022-01-12 01:15:33,532 iteration 6436 : loss : 0.013290, loss_ce: 0.005634
2022-01-12 01:15:35,081 iteration 6437 : loss : 0.013393, loss_ce: 0.004312
2022-01-12 01:15:36,670 iteration 6438 : loss : 0.013705, loss_ce: 0.004241
2022-01-12 01:15:38,201 iteration 6439 : loss : 0.010567, loss_ce: 0.004249
2022-01-12 01:15:39,801 iteration 6440 : loss : 0.025249, loss_ce: 0.012963
2022-01-12 01:15:41,294 iteration 6441 : loss : 0.010555, loss_ce: 0.004562
2022-01-12 01:15:42,872 iteration 6442 : loss : 0.013733, loss_ce: 0.004087
2022-01-12 01:15:44,364 iteration 6443 : loss : 0.010008, loss_ce: 0.003286
 95%|███████████████████████████▍ | 379/400 [3:02:48<09:37, 27.49s/it]2022-01-12 01:15:45,987 iteration 6444 : loss : 0.013850, loss_ce: 0.007527
2022-01-12 01:15:47,541 iteration 6445 : loss : 0.012052, loss_ce: 0.004807
2022-01-12 01:15:49,124 iteration 6446 : loss : 0.014689, loss_ce: 0.006161
2022-01-12 01:15:50,719 iteration 6447 : loss : 0.021552, loss_ce: 0.004016
2022-01-12 01:15:52,307 iteration 6448 : loss : 0.014179, loss_ce: 0.005864
2022-01-12 01:15:53,839 iteration 6449 : loss : 0.011685, loss_ce: 0.002830
2022-01-12 01:15:55,360 iteration 6450 : loss : 0.017834, loss_ce: 0.009101
2022-01-12 01:15:56,931 iteration 6451 : loss : 0.013708, loss_ce: 0.004622
2022-01-12 01:15:58,558 iteration 6452 : loss : 0.016883, loss_ce: 0.005689
2022-01-12 01:16:00,088 iteration 6453 : loss : 0.015104, loss_ce: 0.005704
2022-01-12 01:16:01,581 iteration 6454 : loss : 0.014988, loss_ce: 0.005784
2022-01-12 01:16:03,176 iteration 6455 : loss : 0.010678, loss_ce: 0.004679
2022-01-12 01:16:04,731 iteration 6456 : loss : 0.017712, loss_ce: 0.006129
2022-01-12 01:16:06,312 iteration 6457 : loss : 0.017139, loss_ce: 0.005615
2022-01-12 01:16:07,821 iteration 6458 : loss : 0.011933, loss_ce: 0.004476
2022-01-12 01:16:09,358 iteration 6459 : loss : 0.007890, loss_ce: 0.001998
2022-01-12 01:16:09,358 Training Data Eval:
2022-01-12 01:16:17,349   Average segmentation loss on training set: 0.0074
2022-01-12 01:16:17,349 Validation Data Eval:
2022-01-12 01:16:20,089   Average segmentation loss on validation set: 0.0750
2022-01-12 01:16:21,701 iteration 6460 : loss : 0.012898, loss_ce: 0.005954
 95%|███████████████████████████▌ | 380/400 [3:03:25<10:08, 30.45s/it]2022-01-12 01:16:23,355 iteration 6461 : loss : 0.018146, loss_ce: 0.007481
2022-01-12 01:16:24,897 iteration 6462 : loss : 0.010048, loss_ce: 0.004002
2022-01-12 01:16:26,510 iteration 6463 : loss : 0.019946, loss_ce: 0.008735
2022-01-12 01:16:28,067 iteration 6464 : loss : 0.018946, loss_ce: 0.005267
2022-01-12 01:16:29,653 iteration 6465 : loss : 0.013046, loss_ce: 0.005061
2022-01-12 01:16:31,160 iteration 6466 : loss : 0.013593, loss_ce: 0.004379
2022-01-12 01:16:32,707 iteration 6467 : loss : 0.011687, loss_ce: 0.004273
2022-01-12 01:16:34,323 iteration 6468 : loss : 0.014892, loss_ce: 0.005305
2022-01-12 01:16:35,863 iteration 6469 : loss : 0.014267, loss_ce: 0.006089
2022-01-12 01:16:37,395 iteration 6470 : loss : 0.014041, loss_ce: 0.004341
2022-01-12 01:16:38,952 iteration 6471 : loss : 0.011177, loss_ce: 0.003779
2022-01-12 01:16:40,520 iteration 6472 : loss : 0.018515, loss_ce: 0.005889
2022-01-12 01:16:42,042 iteration 6473 : loss : 0.012473, loss_ce: 0.004241
2022-01-12 01:16:43,653 iteration 6474 : loss : 0.017627, loss_ce: 0.008371
2022-01-12 01:16:45,214 iteration 6475 : loss : 0.015587, loss_ce: 0.008072
2022-01-12 01:16:46,666 iteration 6476 : loss : 0.011983, loss_ce: 0.004363
2022-01-12 01:16:48,305 iteration 6477 : loss : 0.015799, loss_ce: 0.006182
 95%|███████████████████████████▌ | 381/400 [3:03:52<09:16, 29.29s/it]2022-01-12 01:16:49,944 iteration 6478 : loss : 0.016220, loss_ce: 0.005235
2022-01-12 01:16:51,469 iteration 6479 : loss : 0.016445, loss_ce: 0.005911
2022-01-12 01:16:53,022 iteration 6480 : loss : 0.014465, loss_ce: 0.007355
2022-01-12 01:16:54,606 iteration 6481 : loss : 0.012721, loss_ce: 0.005686
2022-01-12 01:16:56,109 iteration 6482 : loss : 0.009681, loss_ce: 0.003460
2022-01-12 01:16:57,686 iteration 6483 : loss : 0.017646, loss_ce: 0.004400
2022-01-12 01:16:59,197 iteration 6484 : loss : 0.014471, loss_ce: 0.006606
2022-01-12 01:17:00,755 iteration 6485 : loss : 0.013697, loss_ce: 0.005804
2022-01-12 01:17:02,355 iteration 6486 : loss : 0.038016, loss_ce: 0.015776
2022-01-12 01:17:03,998 iteration 6487 : loss : 0.017525, loss_ce: 0.007129
2022-01-12 01:17:05,578 iteration 6488 : loss : 0.015381, loss_ce: 0.005659
2022-01-12 01:17:07,098 iteration 6489 : loss : 0.010540, loss_ce: 0.005162
2022-01-12 01:17:08,667 iteration 6490 : loss : 0.015923, loss_ce: 0.004335
2022-01-12 01:17:10,197 iteration 6491 : loss : 0.015548, loss_ce: 0.006221
2022-01-12 01:17:11,811 iteration 6492 : loss : 0.027928, loss_ce: 0.010414
2022-01-12 01:17:13,375 iteration 6493 : loss : 0.020191, loss_ce: 0.007038
2022-01-12 01:17:14,836 iteration 6494 : loss : 0.007905, loss_ce: 0.003219
 96%|███████████████████████████▋ | 382/400 [3:04:18<08:32, 28.46s/it]2022-01-12 01:17:16,404 iteration 6495 : loss : 0.013278, loss_ce: 0.005355
2022-01-12 01:17:17,956 iteration 6496 : loss : 0.018224, loss_ce: 0.004327
2022-01-12 01:17:19,542 iteration 6497 : loss : 0.012263, loss_ce: 0.003956
2022-01-12 01:17:21,136 iteration 6498 : loss : 0.023848, loss_ce: 0.007993
2022-01-12 01:17:22,728 iteration 6499 : loss : 0.014673, loss_ce: 0.006260
2022-01-12 01:17:24,270 iteration 6500 : loss : 0.013203, loss_ce: 0.005133
2022-01-12 01:17:25,842 iteration 6501 : loss : 0.026739, loss_ce: 0.009927
2022-01-12 01:17:27,440 iteration 6502 : loss : 0.014082, loss_ce: 0.007751
2022-01-12 01:17:28,956 iteration 6503 : loss : 0.009429, loss_ce: 0.002280
2022-01-12 01:17:30,455 iteration 6504 : loss : 0.012384, loss_ce: 0.004692
2022-01-12 01:17:31,901 iteration 6505 : loss : 0.010541, loss_ce: 0.004849
2022-01-12 01:17:33,440 iteration 6506 : loss : 0.018012, loss_ce: 0.005609
2022-01-12 01:17:35,058 iteration 6507 : loss : 0.021858, loss_ce: 0.011968
2022-01-12 01:17:36,640 iteration 6508 : loss : 0.017566, loss_ce: 0.006332
2022-01-12 01:17:38,209 iteration 6509 : loss : 0.016944, loss_ce: 0.006778
2022-01-12 01:17:39,777 iteration 6510 : loss : 0.014899, loss_ce: 0.005521
2022-01-12 01:17:41,257 iteration 6511 : loss : 0.010626, loss_ce: 0.004004
 96%|███████████████████████████▊ | 383/400 [3:04:45<07:53, 27.85s/it]2022-01-12 01:17:42,814 iteration 6512 : loss : 0.012934, loss_ce: 0.004440
2022-01-12 01:17:44,386 iteration 6513 : loss : 0.013952, loss_ce: 0.004974
2022-01-12 01:17:46,030 iteration 6514 : loss : 0.011935, loss_ce: 0.003603
2022-01-12 01:17:47,624 iteration 6515 : loss : 0.017560, loss_ce: 0.006714
2022-01-12 01:17:49,194 iteration 6516 : loss : 0.017072, loss_ce: 0.006663
2022-01-12 01:17:50,737 iteration 6517 : loss : 0.014523, loss_ce: 0.003234
2022-01-12 01:17:52,289 iteration 6518 : loss : 0.011761, loss_ce: 0.003957
2022-01-12 01:17:53,839 iteration 6519 : loss : 0.016563, loss_ce: 0.005364
2022-01-12 01:17:55,431 iteration 6520 : loss : 0.017403, loss_ce: 0.005880
2022-01-12 01:17:56,955 iteration 6521 : loss : 0.015708, loss_ce: 0.006577
2022-01-12 01:17:58,528 iteration 6522 : loss : 0.011909, loss_ce: 0.004745
2022-01-12 01:18:00,028 iteration 6523 : loss : 0.009865, loss_ce: 0.003919
2022-01-12 01:18:01,636 iteration 6524 : loss : 0.022836, loss_ce: 0.010266
2022-01-12 01:18:03,208 iteration 6525 : loss : 0.016322, loss_ce: 0.006096
2022-01-12 01:18:04,771 iteration 6526 : loss : 0.017282, loss_ce: 0.007369
2022-01-12 01:18:06,255 iteration 6527 : loss : 0.011966, loss_ce: 0.005113
2022-01-12 01:18:07,774 iteration 6528 : loss : 0.011982, loss_ce: 0.003770
 96%|███████████████████████████▊ | 384/400 [3:05:11<07:19, 27.45s/it]2022-01-12 01:18:09,409 iteration 6529 : loss : 0.015923, loss_ce: 0.005393
2022-01-12 01:18:10,989 iteration 6530 : loss : 0.017305, loss_ce: 0.005680
2022-01-12 01:18:12,490 iteration 6531 : loss : 0.014730, loss_ce: 0.004612
2022-01-12 01:18:14,075 iteration 6532 : loss : 0.011799, loss_ce: 0.004479
2022-01-12 01:18:15,560 iteration 6533 : loss : 0.008465, loss_ce: 0.003291
2022-01-12 01:18:17,118 iteration 6534 : loss : 0.016799, loss_ce: 0.007570
2022-01-12 01:18:18,692 iteration 6535 : loss : 0.013689, loss_ce: 0.005187
2022-01-12 01:18:20,279 iteration 6536 : loss : 0.012914, loss_ce: 0.004066
2022-01-12 01:18:21,857 iteration 6537 : loss : 0.018278, loss_ce: 0.005583
2022-01-12 01:18:23,433 iteration 6538 : loss : 0.011838, loss_ce: 0.003905
2022-01-12 01:18:25,028 iteration 6539 : loss : 0.014859, loss_ce: 0.006195
2022-01-12 01:18:26,507 iteration 6540 : loss : 0.010215, loss_ce: 0.004289
2022-01-12 01:18:28,046 iteration 6541 : loss : 0.019659, loss_ce: 0.012910
2022-01-12 01:18:29,695 iteration 6542 : loss : 0.010539, loss_ce: 0.003225
2022-01-12 01:18:31,215 iteration 6543 : loss : 0.010889, loss_ce: 0.003636
2022-01-12 01:18:32,804 iteration 6544 : loss : 0.024208, loss_ce: 0.009803
2022-01-12 01:18:32,805 Training Data Eval:
2022-01-12 01:18:40,767   Average segmentation loss on training set: 0.0071
2022-01-12 01:18:40,767 Validation Data Eval:
2022-01-12 01:18:43,524   Average segmentation loss on validation set: 0.0794
2022-01-12 01:18:45,177 iteration 6545 : loss : 0.014460, loss_ce: 0.005654
 96%|███████████████████████████▉ | 385/400 [3:05:49<07:36, 30.43s/it]2022-01-12 01:18:46,922 iteration 6546 : loss : 0.015642, loss_ce: 0.006373
2022-01-12 01:18:48,479 iteration 6547 : loss : 0.017169, loss_ce: 0.007209
2022-01-12 01:18:50,083 iteration 6548 : loss : 0.011712, loss_ce: 0.004697
2022-01-12 01:18:51,618 iteration 6549 : loss : 0.014218, loss_ce: 0.004913
2022-01-12 01:18:53,213 iteration 6550 : loss : 0.020275, loss_ce: 0.007124
2022-01-12 01:18:54,832 iteration 6551 : loss : 0.018189, loss_ce: 0.007323
2022-01-12 01:18:56,498 iteration 6552 : loss : 0.020359, loss_ce: 0.007530
2022-01-12 01:18:58,186 iteration 6553 : loss : 0.019343, loss_ce: 0.006801
2022-01-12 01:18:59,769 iteration 6554 : loss : 0.021170, loss_ce: 0.006949
2022-01-12 01:19:01,340 iteration 6555 : loss : 0.010460, loss_ce: 0.004207
2022-01-12 01:19:02,921 iteration 6556 : loss : 0.014488, loss_ce: 0.003751
2022-01-12 01:19:04,395 iteration 6557 : loss : 0.008599, loss_ce: 0.003514
2022-01-12 01:19:05,939 iteration 6558 : loss : 0.022127, loss_ce: 0.006881
2022-01-12 01:19:07,418 iteration 6559 : loss : 0.010015, loss_ce: 0.003694
2022-01-12 01:19:08,981 iteration 6560 : loss : 0.014309, loss_ce: 0.007280
2022-01-12 01:19:10,571 iteration 6561 : loss : 0.017876, loss_ce: 0.005214
2022-01-12 01:19:12,094 iteration 6562 : loss : 0.012974, loss_ce: 0.004724
 96%|███████████████████████████▉ | 386/400 [3:06:16<06:51, 29.38s/it]2022-01-12 01:19:13,822 iteration 6563 : loss : 0.026052, loss_ce: 0.012371
2022-01-12 01:19:15,359 iteration 6564 : loss : 0.017065, loss_ce: 0.006642
2022-01-12 01:19:16,862 iteration 6565 : loss : 0.034160, loss_ce: 0.007672
2022-01-12 01:19:18,368 iteration 6566 : loss : 0.013838, loss_ce: 0.005794
2022-01-12 01:19:19,843 iteration 6567 : loss : 0.012458, loss_ce: 0.005499
2022-01-12 01:19:21,376 iteration 6568 : loss : 0.013252, loss_ce: 0.005551
2022-01-12 01:19:22,936 iteration 6569 : loss : 0.011832, loss_ce: 0.004650
2022-01-12 01:19:24,437 iteration 6570 : loss : 0.009640, loss_ce: 0.004703
2022-01-12 01:19:25,977 iteration 6571 : loss : 0.012516, loss_ce: 0.003728
2022-01-12 01:19:27,457 iteration 6572 : loss : 0.010734, loss_ce: 0.004570
2022-01-12 01:19:29,078 iteration 6573 : loss : 0.016394, loss_ce: 0.005439
2022-01-12 01:19:30,698 iteration 6574 : loss : 0.011785, loss_ce: 0.004923
2022-01-12 01:19:32,311 iteration 6575 : loss : 0.026193, loss_ce: 0.008124
2022-01-12 01:19:33,835 iteration 6576 : loss : 0.012942, loss_ce: 0.005448
2022-01-12 01:19:35,421 iteration 6577 : loss : 0.032153, loss_ce: 0.009142
2022-01-12 01:19:36,936 iteration 6578 : loss : 0.019106, loss_ce: 0.007056
2022-01-12 01:19:38,498 iteration 6579 : loss : 0.013428, loss_ce: 0.004817
 97%|████████████████████████████ | 387/400 [3:06:42<06:10, 28.49s/it]2022-01-12 01:19:40,096 iteration 6580 : loss : 0.014692, loss_ce: 0.004329
2022-01-12 01:19:41,629 iteration 6581 : loss : 0.023686, loss_ce: 0.007869
2022-01-12 01:19:43,254 iteration 6582 : loss : 0.018199, loss_ce: 0.006393
2022-01-12 01:19:44,806 iteration 6583 : loss : 0.015892, loss_ce: 0.005927
2022-01-12 01:19:46,375 iteration 6584 : loss : 0.013573, loss_ce: 0.006140
2022-01-12 01:19:47,986 iteration 6585 : loss : 0.015506, loss_ce: 0.005953
2022-01-12 01:19:49,573 iteration 6586 : loss : 0.026871, loss_ce: 0.007738
2022-01-12 01:19:51,104 iteration 6587 : loss : 0.013132, loss_ce: 0.004819
2022-01-12 01:19:52,656 iteration 6588 : loss : 0.013417, loss_ce: 0.005610
2022-01-12 01:19:54,156 iteration 6589 : loss : 0.015202, loss_ce: 0.005169
2022-01-12 01:19:55,767 iteration 6590 : loss : 0.015150, loss_ce: 0.007129
2022-01-12 01:19:57,368 iteration 6591 : loss : 0.017040, loss_ce: 0.006631
2022-01-12 01:19:58,886 iteration 6592 : loss : 0.011695, loss_ce: 0.004531
2022-01-12 01:20:00,508 iteration 6593 : loss : 0.023750, loss_ce: 0.005034
2022-01-12 01:20:02,060 iteration 6594 : loss : 0.011639, loss_ce: 0.005563
2022-01-12 01:20:03,591 iteration 6595 : loss : 0.010837, loss_ce: 0.003762
2022-01-12 01:20:05,192 iteration 6596 : loss : 0.019632, loss_ce: 0.006623
 97%|████████████████████████████▏| 388/400 [3:07:09<05:35, 27.95s/it]2022-01-12 01:20:06,774 iteration 6597 : loss : 0.015324, loss_ce: 0.006319
2022-01-12 01:20:08,280 iteration 6598 : loss : 0.021478, loss_ce: 0.003713
2022-01-12 01:20:09,860 iteration 6599 : loss : 0.012162, loss_ce: 0.005122
2022-01-12 01:20:11,313 iteration 6600 : loss : 0.009251, loss_ce: 0.003764
2022-01-12 01:20:12,867 iteration 6601 : loss : 0.012668, loss_ce: 0.004023
2022-01-12 01:20:14,384 iteration 6602 : loss : 0.012415, loss_ce: 0.004532
2022-01-12 01:20:15,940 iteration 6603 : loss : 0.014731, loss_ce: 0.005082
2022-01-12 01:20:17,420 iteration 6604 : loss : 0.008271, loss_ce: 0.003487
2022-01-12 01:20:18,975 iteration 6605 : loss : 0.013601, loss_ce: 0.004403
2022-01-12 01:20:20,555 iteration 6606 : loss : 0.014239, loss_ce: 0.006792
2022-01-12 01:20:22,144 iteration 6607 : loss : 0.015987, loss_ce: 0.006242
2022-01-12 01:20:23,740 iteration 6608 : loss : 0.011126, loss_ce: 0.004555
2022-01-12 01:20:25,255 iteration 6609 : loss : 0.020008, loss_ce: 0.006197
2022-01-12 01:20:26,790 iteration 6610 : loss : 0.018116, loss_ce: 0.008539
2022-01-12 01:20:28,311 iteration 6611 : loss : 0.011092, loss_ce: 0.002564
2022-01-12 01:20:29,913 iteration 6612 : loss : 0.025904, loss_ce: 0.011727
2022-01-12 01:20:31,361 iteration 6613 : loss : 0.011931, loss_ce: 0.004328
 97%|████████████████████████████▏| 389/400 [3:07:35<05:01, 27.42s/it]2022-01-12 01:20:32,949 iteration 6614 : loss : 0.013635, loss_ce: 0.005441
2022-01-12 01:20:34,514 iteration 6615 : loss : 0.011694, loss_ce: 0.005135
2022-01-12 01:20:36,057 iteration 6616 : loss : 0.015550, loss_ce: 0.005332
2022-01-12 01:20:37,728 iteration 6617 : loss : 0.020038, loss_ce: 0.008157
2022-01-12 01:20:39,308 iteration 6618 : loss : 0.012221, loss_ce: 0.003835
2022-01-12 01:20:40,945 iteration 6619 : loss : 0.016924, loss_ce: 0.007147
2022-01-12 01:20:42,530 iteration 6620 : loss : 0.026546, loss_ce: 0.015281
2022-01-12 01:20:44,069 iteration 6621 : loss : 0.017509, loss_ce: 0.005666
2022-01-12 01:20:45,624 iteration 6622 : loss : 0.016720, loss_ce: 0.009024
2022-01-12 01:20:47,173 iteration 6623 : loss : 0.016000, loss_ce: 0.005378
2022-01-12 01:20:48,763 iteration 6624 : loss : 0.042578, loss_ce: 0.019053
2022-01-12 01:20:50,365 iteration 6625 : loss : 0.015713, loss_ce: 0.006798
2022-01-12 01:20:51,900 iteration 6626 : loss : 0.021292, loss_ce: 0.006185
2022-01-12 01:20:53,384 iteration 6627 : loss : 0.012684, loss_ce: 0.004233
2022-01-12 01:20:54,925 iteration 6628 : loss : 0.014450, loss_ce: 0.007598
2022-01-12 01:20:56,457 iteration 6629 : loss : 0.013492, loss_ce: 0.007306
2022-01-12 01:20:56,457 Training Data Eval:
2022-01-12 01:21:04,451   Average segmentation loss on training set: 0.0073
2022-01-12 01:21:04,451 Validation Data Eval:
2022-01-12 01:21:07,203   Average segmentation loss on validation set: 0.0736
2022-01-12 01:21:08,807 iteration 6630 : loss : 0.025222, loss_ce: 0.008140
 98%|████████████████████████████▎| 390/400 [3:08:12<05:04, 30.43s/it]2022-01-12 01:21:10,474 iteration 6631 : loss : 0.014683, loss_ce: 0.004870
2022-01-12 01:21:12,050 iteration 6632 : loss : 0.014847, loss_ce: 0.006268
2022-01-12 01:21:13,618 iteration 6633 : loss : 0.017610, loss_ce: 0.005909
2022-01-12 01:21:15,060 iteration 6634 : loss : 0.009778, loss_ce: 0.003143
2022-01-12 01:21:16,579 iteration 6635 : loss : 0.013295, loss_ce: 0.003163
2022-01-12 01:21:18,191 iteration 6636 : loss : 0.036226, loss_ce: 0.012192
2022-01-12 01:21:19,781 iteration 6637 : loss : 0.011463, loss_ce: 0.004950
2022-01-12 01:21:21,386 iteration 6638 : loss : 0.015239, loss_ce: 0.005669
2022-01-12 01:21:23,002 iteration 6639 : loss : 0.017996, loss_ce: 0.006089
2022-01-12 01:21:24,547 iteration 6640 : loss : 0.016548, loss_ce: 0.005132
2022-01-12 01:21:26,077 iteration 6641 : loss : 0.011604, loss_ce: 0.006064
2022-01-12 01:21:27,709 iteration 6642 : loss : 0.015269, loss_ce: 0.006703
2022-01-12 01:21:29,293 iteration 6643 : loss : 0.016017, loss_ce: 0.006771
2022-01-12 01:21:30,855 iteration 6644 : loss : 0.013435, loss_ce: 0.004833
2022-01-12 01:21:32,357 iteration 6645 : loss : 0.009179, loss_ce: 0.004279
2022-01-12 01:21:33,892 iteration 6646 : loss : 0.012702, loss_ce: 0.005275
2022-01-12 01:21:35,358 iteration 6647 : loss : 0.010651, loss_ce: 0.003417
 98%|████████████████████████████▎| 391/400 [3:08:39<04:23, 29.26s/it]2022-01-12 01:21:36,918 iteration 6648 : loss : 0.012378, loss_ce: 0.003969
2022-01-12 01:21:38,538 iteration 6649 : loss : 0.015785, loss_ce: 0.006657
2022-01-12 01:21:40,038 iteration 6650 : loss : 0.018670, loss_ce: 0.007180
2022-01-12 01:21:41,556 iteration 6651 : loss : 0.010958, loss_ce: 0.003632
2022-01-12 01:21:43,176 iteration 6652 : loss : 0.018381, loss_ce: 0.006163
2022-01-12 01:21:44,768 iteration 6653 : loss : 0.013360, loss_ce: 0.005234
2022-01-12 01:21:46,292 iteration 6654 : loss : 0.011423, loss_ce: 0.004204
2022-01-12 01:21:47,870 iteration 6655 : loss : 0.011152, loss_ce: 0.004406
2022-01-12 01:21:49,458 iteration 6656 : loss : 0.011068, loss_ce: 0.004386
2022-01-12 01:21:51,095 iteration 6657 : loss : 0.018227, loss_ce: 0.006048
2022-01-12 01:21:52,664 iteration 6658 : loss : 0.017955, loss_ce: 0.006825
2022-01-12 01:21:54,260 iteration 6659 : loss : 0.014260, loss_ce: 0.005640
2022-01-12 01:21:55,769 iteration 6660 : loss : 0.014839, loss_ce: 0.004957
2022-01-12 01:21:57,255 iteration 6661 : loss : 0.009742, loss_ce: 0.003503
2022-01-12 01:21:58,834 iteration 6662 : loss : 0.011535, loss_ce: 0.004208
2022-01-12 01:22:00,449 iteration 6663 : loss : 0.018839, loss_ce: 0.006941
2022-01-12 01:22:02,000 iteration 6664 : loss : 0.013948, loss_ce: 0.004776
 98%|████████████████████████████▍| 392/400 [3:09:06<03:47, 28.48s/it]2022-01-12 01:22:03,618 iteration 6665 : loss : 0.020133, loss_ce: 0.006687
2022-01-12 01:22:05,187 iteration 6666 : loss : 0.012913, loss_ce: 0.004761
2022-01-12 01:22:06,718 iteration 6667 : loss : 0.017193, loss_ce: 0.005782
2022-01-12 01:22:08,188 iteration 6668 : loss : 0.008505, loss_ce: 0.003492
2022-01-12 01:22:09,756 iteration 6669 : loss : 0.012459, loss_ce: 0.003977
2022-01-12 01:22:11,246 iteration 6670 : loss : 0.022699, loss_ce: 0.007313
2022-01-12 01:22:12,889 iteration 6671 : loss : 0.015774, loss_ce: 0.005940
2022-01-12 01:22:14,446 iteration 6672 : loss : 0.013768, loss_ce: 0.006428
2022-01-12 01:22:16,092 iteration 6673 : loss : 0.015410, loss_ce: 0.006901
2022-01-12 01:22:17,707 iteration 6674 : loss : 0.013364, loss_ce: 0.004812
2022-01-12 01:22:19,241 iteration 6675 : loss : 0.013381, loss_ce: 0.004359
2022-01-12 01:22:20,723 iteration 6676 : loss : 0.010161, loss_ce: 0.003813
2022-01-12 01:22:22,308 iteration 6677 : loss : 0.011002, loss_ce: 0.002626
2022-01-12 01:22:23,952 iteration 6678 : loss : 0.015317, loss_ce: 0.006700
2022-01-12 01:22:25,522 iteration 6679 : loss : 0.016969, loss_ce: 0.006535
2022-01-12 01:22:27,071 iteration 6680 : loss : 0.025131, loss_ce: 0.012143
2022-01-12 01:22:28,593 iteration 6681 : loss : 0.012573, loss_ce: 0.004390
 98%|████████████████████████████▍| 393/400 [3:09:32<03:15, 27.91s/it]2022-01-12 01:22:30,200 iteration 6682 : loss : 0.012581, loss_ce: 0.004121
2022-01-12 01:22:31,815 iteration 6683 : loss : 0.017885, loss_ce: 0.005712
2022-01-12 01:22:33,398 iteration 6684 : loss : 0.026895, loss_ce: 0.004843
2022-01-12 01:22:34,943 iteration 6685 : loss : 0.013696, loss_ce: 0.004825
2022-01-12 01:22:36,523 iteration 6686 : loss : 0.016268, loss_ce: 0.005114
2022-01-12 01:22:38,095 iteration 6687 : loss : 0.013642, loss_ce: 0.005485
2022-01-12 01:22:39,670 iteration 6688 : loss : 0.019334, loss_ce: 0.007905
2022-01-12 01:22:41,215 iteration 6689 : loss : 0.011424, loss_ce: 0.004655
2022-01-12 01:22:42,833 iteration 6690 : loss : 0.014934, loss_ce: 0.005732
2022-01-12 01:22:44,336 iteration 6691 : loss : 0.013213, loss_ce: 0.005603
2022-01-12 01:22:45,842 iteration 6692 : loss : 0.010303, loss_ce: 0.004884
2022-01-12 01:22:47,410 iteration 6693 : loss : 0.020037, loss_ce: 0.008767
2022-01-12 01:22:48,899 iteration 6694 : loss : 0.009503, loss_ce: 0.004023
2022-01-12 01:22:50,432 iteration 6695 : loss : 0.013112, loss_ce: 0.003603
2022-01-12 01:22:51,988 iteration 6696 : loss : 0.013673, loss_ce: 0.007360
2022-01-12 01:22:53,541 iteration 6697 : loss : 0.016860, loss_ce: 0.006526
2022-01-12 01:22:55,021 iteration 6698 : loss : 0.016893, loss_ce: 0.006036
 98%|████████████████████████████▌| 394/400 [3:09:59<02:44, 27.47s/it]2022-01-12 01:22:56,632 iteration 6699 : loss : 0.013248, loss_ce: 0.003598
2022-01-12 01:22:58,228 iteration 6700 : loss : 0.013679, loss_ce: 0.005763
2022-01-12 01:22:59,857 iteration 6701 : loss : 0.035894, loss_ce: 0.011114
2022-01-12 01:23:01,384 iteration 6702 : loss : 0.013635, loss_ce: 0.004784
2022-01-12 01:23:03,030 iteration 6703 : loss : 0.015978, loss_ce: 0.004882
2022-01-12 01:23:04,583 iteration 6704 : loss : 0.014748, loss_ce: 0.007955
2022-01-12 01:23:06,093 iteration 6705 : loss : 0.013244, loss_ce: 0.004242
2022-01-12 01:23:07,631 iteration 6706 : loss : 0.024786, loss_ce: 0.008314
2022-01-12 01:23:09,179 iteration 6707 : loss : 0.014601, loss_ce: 0.005255
2022-01-12 01:23:10,662 iteration 6708 : loss : 0.008861, loss_ce: 0.002787
2022-01-12 01:23:12,172 iteration 6709 : loss : 0.011246, loss_ce: 0.003044
2022-01-12 01:23:13,745 iteration 6710 : loss : 0.012993, loss_ce: 0.005024
2022-01-12 01:23:15,289 iteration 6711 : loss : 0.015404, loss_ce: 0.006635
2022-01-12 01:23:16,862 iteration 6712 : loss : 0.013605, loss_ce: 0.005412
2022-01-12 01:23:18,387 iteration 6713 : loss : 0.014542, loss_ce: 0.004545
2022-01-12 01:23:19,935 iteration 6714 : loss : 0.015129, loss_ce: 0.006598
2022-01-12 01:23:19,935 Training Data Eval:
2022-01-12 01:23:27,919   Average segmentation loss on training set: 0.0074
2022-01-12 01:23:27,919 Validation Data Eval:
2022-01-12 01:23:30,672   Average segmentation loss on validation set: 0.0754
2022-01-12 01:23:32,205 iteration 6715 : loss : 0.008598, loss_ce: 0.002603
 99%|████████████████████████████▋| 395/400 [3:10:36<02:31, 30.38s/it]2022-01-12 01:23:33,735 iteration 6716 : loss : 0.010490, loss_ce: 0.003518
2022-01-12 01:23:35,323 iteration 6717 : loss : 0.015548, loss_ce: 0.004589
2022-01-12 01:23:36,946 iteration 6718 : loss : 0.019282, loss_ce: 0.009009
2022-01-12 01:23:38,557 iteration 6719 : loss : 0.021921, loss_ce: 0.007907
2022-01-12 01:23:40,152 iteration 6720 : loss : 0.022578, loss_ce: 0.005540
2022-01-12 01:23:41,737 iteration 6721 : loss : 0.014483, loss_ce: 0.006149
2022-01-12 01:23:43,273 iteration 6722 : loss : 0.011534, loss_ce: 0.004785
2022-01-12 01:23:44,845 iteration 6723 : loss : 0.016475, loss_ce: 0.006510
2022-01-12 01:23:46,343 iteration 6724 : loss : 0.014249, loss_ce: 0.005767
2022-01-12 01:23:47,895 iteration 6725 : loss : 0.012352, loss_ce: 0.004289
2022-01-12 01:23:49,378 iteration 6726 : loss : 0.012156, loss_ce: 0.004761
2022-01-12 01:23:51,024 iteration 6727 : loss : 0.020862, loss_ce: 0.007423
2022-01-12 01:23:52,537 iteration 6728 : loss : 0.013503, loss_ce: 0.005390
2022-01-12 01:23:54,130 iteration 6729 : loss : 0.016536, loss_ce: 0.006341
2022-01-12 01:23:55,774 iteration 6730 : loss : 0.016229, loss_ce: 0.006324
2022-01-12 01:23:57,368 iteration 6731 : loss : 0.018018, loss_ce: 0.005989
2022-01-12 01:23:58,908 iteration 6732 : loss : 0.010168, loss_ce: 0.004407
 99%|████████████████████████████▋| 396/400 [3:11:03<01:57, 29.28s/it]2022-01-12 01:24:00,454 iteration 6733 : loss : 0.008444, loss_ce: 0.003413
2022-01-12 01:24:01,995 iteration 6734 : loss : 0.014834, loss_ce: 0.003820
2022-01-12 01:24:03,571 iteration 6735 : loss : 0.014685, loss_ce: 0.005502
2022-01-12 01:24:05,113 iteration 6736 : loss : 0.014859, loss_ce: 0.007198
2022-01-12 01:24:06,621 iteration 6737 : loss : 0.014248, loss_ce: 0.007752
2022-01-12 01:24:08,112 iteration 6738 : loss : 0.011316, loss_ce: 0.003624
2022-01-12 01:24:09,676 iteration 6739 : loss : 0.011469, loss_ce: 0.004864
2022-01-12 01:24:11,265 iteration 6740 : loss : 0.017715, loss_ce: 0.006811
2022-01-12 01:24:12,833 iteration 6741 : loss : 0.017223, loss_ce: 0.007774
2022-01-12 01:24:14,373 iteration 6742 : loss : 0.012103, loss_ce: 0.005903
2022-01-12 01:24:15,868 iteration 6743 : loss : 0.010774, loss_ce: 0.002543
2022-01-12 01:24:17,453 iteration 6744 : loss : 0.015151, loss_ce: 0.006803
2022-01-12 01:24:18,998 iteration 6745 : loss : 0.011705, loss_ce: 0.005244
2022-01-12 01:24:20,496 iteration 6746 : loss : 0.013301, loss_ce: 0.005248
2022-01-12 01:24:22,011 iteration 6747 : loss : 0.014969, loss_ce: 0.004305
2022-01-12 01:24:23,578 iteration 6748 : loss : 0.009535, loss_ce: 0.004047
2022-01-12 01:24:25,119 iteration 6749 : loss : 0.022117, loss_ce: 0.007900
 99%|████████████████████████████▊| 397/400 [3:11:29<01:25, 28.36s/it]2022-01-12 01:24:26,704 iteration 6750 : loss : 0.010247, loss_ce: 0.004648
2022-01-12 01:24:28,308 iteration 6751 : loss : 0.014324, loss_ce: 0.005349
2022-01-12 01:24:29,788 iteration 6752 : loss : 0.010392, loss_ce: 0.003085
2022-01-12 01:24:31,361 iteration 6753 : loss : 0.014810, loss_ce: 0.006089
2022-01-12 01:24:32,947 iteration 6754 : loss : 0.010107, loss_ce: 0.003613
2022-01-12 01:24:34,462 iteration 6755 : loss : 0.012081, loss_ce: 0.003784
2022-01-12 01:24:35,987 iteration 6756 : loss : 0.012573, loss_ce: 0.005464
2022-01-12 01:24:37,604 iteration 6757 : loss : 0.022278, loss_ce: 0.006173
2022-01-12 01:24:39,131 iteration 6758 : loss : 0.015957, loss_ce: 0.007469
2022-01-12 01:24:40,703 iteration 6759 : loss : 0.010466, loss_ce: 0.004663
2022-01-12 01:24:42,323 iteration 6760 : loss : 0.020242, loss_ce: 0.004277
2022-01-12 01:24:43,865 iteration 6761 : loss : 0.014899, loss_ce: 0.006152
2022-01-12 01:24:45,405 iteration 6762 : loss : 0.017869, loss_ce: 0.005211
2022-01-12 01:24:47,005 iteration 6763 : loss : 0.017440, loss_ce: 0.007854
2022-01-12 01:24:48,518 iteration 6764 : loss : 0.023058, loss_ce: 0.009711
2022-01-12 01:24:50,134 iteration 6765 : loss : 0.017910, loss_ce: 0.005966
2022-01-12 01:24:51,662 iteration 6766 : loss : 0.012179, loss_ce: 0.005604
100%|████████████████████████████▊| 398/400 [3:11:55<00:55, 27.81s/it]2022-01-12 01:24:53,263 iteration 6767 : loss : 0.016534, loss_ce: 0.005351
2022-01-12 01:24:54,898 iteration 6768 : loss : 0.014897, loss_ce: 0.005000
2022-01-12 01:24:56,474 iteration 6769 : loss : 0.013998, loss_ce: 0.005805
2022-01-12 01:24:58,017 iteration 6770 : loss : 0.014654, loss_ce: 0.007374
2022-01-12 01:24:59,687 iteration 6771 : loss : 0.020870, loss_ce: 0.005425
2022-01-12 01:25:01,216 iteration 6772 : loss : 0.012619, loss_ce: 0.004791
2022-01-12 01:25:02,648 iteration 6773 : loss : 0.010016, loss_ce: 0.003115
2022-01-12 01:25:04,238 iteration 6774 : loss : 0.018255, loss_ce: 0.005866
2022-01-12 01:25:05,822 iteration 6775 : loss : 0.014131, loss_ce: 0.004349
2022-01-12 01:25:07,378 iteration 6776 : loss : 0.019061, loss_ce: 0.006032
2022-01-12 01:25:09,004 iteration 6777 : loss : 0.016896, loss_ce: 0.009114
2022-01-12 01:25:10,538 iteration 6778 : loss : 0.012544, loss_ce: 0.004729
2022-01-12 01:25:12,176 iteration 6779 : loss : 0.014421, loss_ce: 0.005489
2022-01-12 01:25:13,757 iteration 6780 : loss : 0.015920, loss_ce: 0.007446
2022-01-12 01:25:15,366 iteration 6781 : loss : 0.015561, loss_ce: 0.006810
2022-01-12 01:25:16,976 iteration 6782 : loss : 0.011438, loss_ce: 0.004429
2022-01-12 01:25:18,502 iteration 6783 : loss : 0.014921, loss_ce: 0.005424
100%|████████████████████████████▉| 399/400 [3:12:22<00:27, 27.52s/it]2022-01-12 01:25:20,159 iteration 6784 : loss : 0.015725, loss_ce: 0.005242
2022-01-12 01:25:21,739 iteration 6785 : loss : 0.015520, loss_ce: 0.005318
2022-01-12 01:25:23,258 iteration 6786 : loss : 0.011270, loss_ce: 0.004535
2022-01-12 01:25:24,784 iteration 6787 : loss : 0.011644, loss_ce: 0.003732
2022-01-12 01:25:26,358 iteration 6788 : loss : 0.011663, loss_ce: 0.004667
2022-01-12 01:25:27,917 iteration 6789 : loss : 0.014187, loss_ce: 0.004245
2022-01-12 01:25:29,461 iteration 6790 : loss : 0.011215, loss_ce: 0.004586
2022-01-12 01:25:31,035 iteration 6791 : loss : 0.011784, loss_ce: 0.004487
2022-01-12 01:25:32,548 iteration 6792 : loss : 0.012456, loss_ce: 0.005637
2022-01-12 01:25:34,178 iteration 6793 : loss : 0.031398, loss_ce: 0.011086
2022-01-12 01:25:35,732 iteration 6794 : loss : 0.022311, loss_ce: 0.007716
2022-01-12 01:25:37,328 iteration 6795 : loss : 0.012034, loss_ce: 0.004163
2022-01-12 01:25:38,935 iteration 6796 : loss : 0.027209, loss_ce: 0.014612
2022-01-12 01:25:40,417 iteration 6797 : loss : 0.011790, loss_ce: 0.003682
2022-01-12 01:25:42,027 iteration 6798 : loss : 0.013506, loss_ce: 0.004066
2022-01-12 01:25:43,675 iteration 6799 : loss : 0.018472, loss_ce: 0.009049
2022-01-12 01:25:43,675 Training Data Eval:
2022-01-12 01:25:51,642   Average segmentation loss on training set: 0.0070
2022-01-12 01:25:51,642 Validation Data Eval:
2022-01-12 01:25:54,385   Average segmentation loss on validation set: 0.0704
2022-01-12 01:25:55,899 iteration 6800 : loss : 0.010313, loss_ce: 0.002692
100%|█████████████████████████████| 400/400 [3:13:00<00:00, 30.49s/it]100%|█████████████████████████████| 400/400 [3:13:00<00:00, 28.95s/it]
