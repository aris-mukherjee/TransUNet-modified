2022-01-08 09:20:29,288 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:20:29,289 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:20:29,289 ============================================================
2022-01-08 09:20:29,289 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-08 09:20:29,289 ============================================================
2022-01-08 09:20:29,289 Loading data...
2022-01-08 09:20:29,289 Reading NCI - RUNMC images...
2022-01-08 09:20:29,289 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-08 09:20:29,292 Already preprocessed this configuration. Loading now!
2022-01-08 09:20:29,316 Training Images: (256, 256, 286)
2022-01-08 09:20:29,316 Training Labels: (256, 256, 286)
2022-01-08 09:20:29,316 Validation Images: (256, 256, 98)
2022-01-08 09:20:29,316 Validation Labels: (256, 256, 98)
2022-01-08 09:20:29,317 ============================================================
2022-01-08 09:20:29,365 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-08 09:20:31,975 iteration 1 : loss : 1.077009, loss_ce: 1.371862
2022-01-08 09:20:33,213 iteration 2 : loss : 0.997308, loss_ce: 1.236072
2022-01-08 09:20:34,574 iteration 3 : loss : 0.930859, loss_ce: 1.122022
2022-01-08 09:20:35,834 iteration 4 : loss : 0.908347, loss_ce: 1.076706
2022-01-08 09:20:37,096 iteration 5 : loss : 0.856479, loss_ce: 1.001336
2022-01-08 09:20:38,377 iteration 6 : loss : 0.811111, loss_ce: 0.925673
2022-01-08 09:20:39,720 iteration 7 : loss : 0.764414, loss_ce: 0.857401
2022-01-08 09:20:41,000 iteration 8 : loss : 0.741903, loss_ce: 0.794856
2022-01-08 09:20:42,296 iteration 9 : loss : 0.682056, loss_ce: 0.751037
2022-01-08 09:20:43,621 iteration 10 : loss : 0.680520, loss_ce: 0.689384
2022-01-08 09:20:45,000 iteration 11 : loss : 0.636643, loss_ce: 0.651122
2022-01-08 09:20:46,268 iteration 12 : loss : 0.606484, loss_ce: 0.597808
2022-01-08 09:20:47,512 iteration 13 : loss : 0.589032, loss_ce: 0.558258
2022-01-08 09:20:48,730 iteration 14 : loss : 0.554818, loss_ce: 0.515585
2022-01-08 09:20:50,022 iteration 15 : loss : 0.523049, loss_ce: 0.474969
2022-01-08 09:20:51,298 iteration 16 : loss : 0.522714, loss_ce: 0.449095
2022-01-08 09:20:52,562 iteration 17 : loss : 0.471029, loss_ce: 0.411922
  0%|                               | 1/400 [00:23<2:34:42, 23.27s/it]2022-01-08 09:20:53,922 iteration 18 : loss : 0.483876, loss_ce: 0.365917
2022-01-08 09:20:55,118 iteration 19 : loss : 0.433833, loss_ce: 0.335497
2022-01-08 09:20:56,452 iteration 20 : loss : 0.413555, loss_ce: 0.305190
2022-01-08 09:20:57,695 iteration 21 : loss : 0.419159, loss_ce: 0.279176
2022-01-08 09:20:58,966 iteration 22 : loss : 0.382701, loss_ce: 0.270686
2022-01-08 09:21:00,315 iteration 23 : loss : 0.379708, loss_ce: 0.237271
2022-01-08 09:21:01,583 iteration 24 : loss : 0.361169, loss_ce: 0.229463
2022-01-08 09:21:02,908 iteration 25 : loss : 0.384702, loss_ce: 0.261545
2022-01-08 09:21:04,154 iteration 26 : loss : 0.341471, loss_ce: 0.204102
2022-01-08 09:21:05,350 iteration 27 : loss : 0.330266, loss_ce: 0.202447
2022-01-08 09:21:06,555 iteration 28 : loss : 0.324530, loss_ce: 0.184037
2022-01-08 09:21:07,865 iteration 29 : loss : 0.319333, loss_ce: 0.176018
2022-01-08 09:21:09,163 iteration 30 : loss : 0.315151, loss_ce: 0.170451
2022-01-08 09:21:10,369 iteration 31 : loss : 0.297392, loss_ce: 0.161671
2022-01-08 09:21:11,701 iteration 32 : loss : 0.323294, loss_ce: 0.188619
2022-01-08 09:21:13,011 iteration 33 : loss : 0.307789, loss_ce: 0.173132
2022-01-08 09:21:14,316 iteration 34 : loss : 0.313141, loss_ce: 0.182729
  0%|▏                              | 2/400 [00:45<2:28:21, 22.37s/it]2022-01-08 09:21:15,662 iteration 35 : loss : 0.284758, loss_ce: 0.143957
2022-01-08 09:21:16,986 iteration 36 : loss : 0.292217, loss_ce: 0.156802
2022-01-08 09:21:18,318 iteration 37 : loss : 0.288549, loss_ce: 0.134984
2022-01-08 09:21:19,565 iteration 38 : loss : 0.260344, loss_ce: 0.131290
2022-01-08 09:21:20,821 iteration 39 : loss : 0.250666, loss_ce: 0.124890
2022-01-08 09:21:22,151 iteration 40 : loss : 0.296683, loss_ce: 0.148944
2022-01-08 09:21:23,472 iteration 41 : loss : 0.317308, loss_ce: 0.157717
2022-01-08 09:21:24,770 iteration 42 : loss : 0.275182, loss_ce: 0.134627
2022-01-08 09:21:25,978 iteration 43 : loss : 0.266976, loss_ce: 0.124602
2022-01-08 09:21:27,324 iteration 44 : loss : 0.232896, loss_ce: 0.113028
2022-01-08 09:21:28,623 iteration 45 : loss : 0.231781, loss_ce: 0.110732
2022-01-08 09:21:29,927 iteration 46 : loss : 0.266684, loss_ce: 0.112108
2022-01-08 09:21:31,260 iteration 47 : loss : 0.229324, loss_ce: 0.098536
2022-01-08 09:21:32,568 iteration 48 : loss : 0.249529, loss_ce: 0.107739
2022-01-08 09:21:33,917 iteration 49 : loss : 0.303723, loss_ce: 0.143635
2022-01-08 09:21:35,179 iteration 50 : loss : 0.331691, loss_ce: 0.154461
2022-01-08 09:21:36,466 iteration 51 : loss : 0.273072, loss_ce: 0.129031
  1%|▏                              | 3/400 [01:07<2:27:19, 22.27s/it]2022-01-08 09:21:37,931 iteration 52 : loss : 0.291022, loss_ce: 0.148263
2022-01-08 09:21:39,349 iteration 53 : loss : 0.258900, loss_ce: 0.125308
2022-01-08 09:21:40,728 iteration 54 : loss : 0.286243, loss_ce: 0.123489
2022-01-08 09:21:42,113 iteration 55 : loss : 0.338025, loss_ce: 0.182058
2022-01-08 09:21:43,483 iteration 56 : loss : 0.281481, loss_ce: 0.130999
2022-01-08 09:21:44,873 iteration 57 : loss : 0.280433, loss_ce: 0.135539
2022-01-08 09:21:46,263 iteration 58 : loss : 0.291460, loss_ce: 0.126736
2022-01-08 09:21:47,615 iteration 59 : loss : 0.259155, loss_ce: 0.118628
2022-01-08 09:21:49,012 iteration 60 : loss : 0.290096, loss_ce: 0.122853
2022-01-08 09:21:50,385 iteration 61 : loss : 0.282225, loss_ce: 0.139859
2022-01-08 09:21:51,750 iteration 62 : loss : 0.343868, loss_ce: 0.142536
2022-01-08 09:21:53,040 iteration 63 : loss : 0.297086, loss_ce: 0.148998
2022-01-08 09:21:54,394 iteration 64 : loss : 0.368839, loss_ce: 0.170555
2022-01-08 09:21:55,702 iteration 65 : loss : 0.270778, loss_ce: 0.104479
2022-01-08 09:21:57,043 iteration 66 : loss : 0.255608, loss_ce: 0.108321
2022-01-08 09:21:58,453 iteration 67 : loss : 0.272098, loss_ce: 0.095848
2022-01-08 09:21:59,802 iteration 68 : loss : 0.282332, loss_ce: 0.128447
  1%|▎                              | 4/400 [01:30<2:29:43, 22.69s/it]2022-01-08 09:22:01,222 iteration 69 : loss : 0.244880, loss_ce: 0.109878
2022-01-08 09:22:02,659 iteration 70 : loss : 0.253542, loss_ce: 0.112019
2022-01-08 09:22:04,014 iteration 71 : loss : 0.247033, loss_ce: 0.105838
2022-01-08 09:22:05,398 iteration 72 : loss : 0.235972, loss_ce: 0.104946
2022-01-08 09:22:06,720 iteration 73 : loss : 0.260183, loss_ce: 0.129962
2022-01-08 09:22:08,020 iteration 74 : loss : 0.233011, loss_ce: 0.100751
2022-01-08 09:22:09,365 iteration 75 : loss : 0.252354, loss_ce: 0.110627
2022-01-08 09:22:10,702 iteration 76 : loss : 0.253345, loss_ce: 0.109598
2022-01-08 09:22:11,946 iteration 77 : loss : 0.246264, loss_ce: 0.114896
2022-01-08 09:22:13,297 iteration 78 : loss : 0.271721, loss_ce: 0.123942
2022-01-08 09:22:14,614 iteration 79 : loss : 0.278786, loss_ce: 0.109183
2022-01-08 09:22:15,924 iteration 80 : loss : 0.281058, loss_ce: 0.132742
2022-01-08 09:22:17,239 iteration 81 : loss : 0.254628, loss_ce: 0.109223
2022-01-08 09:22:18,588 iteration 82 : loss : 0.245019, loss_ce: 0.097302
2022-01-08 09:22:19,924 iteration 83 : loss : 0.270821, loss_ce: 0.103182
2022-01-08 09:22:21,338 iteration 84 : loss : 0.265135, loss_ce: 0.148673
2022-01-08 09:22:21,338 Training Data Eval:
2022-01-08 09:22:28,199   Average segmentation loss on training set: 0.2166
2022-01-08 09:22:28,200 Validation Data Eval:
2022-01-08 09:22:30,689   Average segmentation loss on validation set: 0.2188
2022-01-08 09:22:34,819 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:22:36,071 iteration 85 : loss : 0.279032, loss_ce: 0.122429
  1%|▍                              | 5/400 [02:06<3:01:35, 27.58s/it]2022-01-08 09:22:37,389 iteration 86 : loss : 0.291421, loss_ce: 0.112407
2022-01-08 09:22:38,788 iteration 87 : loss : 0.203904, loss_ce: 0.083138
2022-01-08 09:22:40,004 iteration 88 : loss : 0.245966, loss_ce: 0.106186
2022-01-08 09:22:41,302 iteration 89 : loss : 0.276450, loss_ce: 0.113801
2022-01-08 09:22:42,591 iteration 90 : loss : 0.245850, loss_ce: 0.100478
2022-01-08 09:22:43,944 iteration 91 : loss : 0.234362, loss_ce: 0.117802
2022-01-08 09:22:45,152 iteration 92 : loss : 0.233637, loss_ce: 0.108366
2022-01-08 09:22:46,412 iteration 93 : loss : 0.250057, loss_ce: 0.094475
2022-01-08 09:22:47,720 iteration 94 : loss : 0.253619, loss_ce: 0.100737
2022-01-08 09:22:49,196 iteration 95 : loss : 0.239679, loss_ce: 0.109503
2022-01-08 09:22:50,543 iteration 96 : loss : 0.230993, loss_ce: 0.099283
2022-01-08 09:22:51,897 iteration 97 : loss : 0.263437, loss_ce: 0.108867
2022-01-08 09:22:53,257 iteration 98 : loss : 0.267992, loss_ce: 0.119622
2022-01-08 09:22:54,698 iteration 99 : loss : 0.239222, loss_ce: 0.108650
2022-01-08 09:22:56,088 iteration 100 : loss : 0.230491, loss_ce: 0.098093
2022-01-08 09:22:57,458 iteration 101 : loss : 0.168977, loss_ce: 0.065937
2022-01-08 09:22:58,760 iteration 102 : loss : 0.235749, loss_ce: 0.098224
  2%|▍                              | 6/400 [02:29<2:50:13, 25.92s/it]2022-01-08 09:23:00,257 iteration 103 : loss : 0.214259, loss_ce: 0.092928
2022-01-08 09:23:01,705 iteration 104 : loss : 0.248156, loss_ce: 0.105323
2022-01-08 09:23:03,179 iteration 105 : loss : 0.286375, loss_ce: 0.118242
2022-01-08 09:23:04,497 iteration 106 : loss : 0.229354, loss_ce: 0.095166
2022-01-08 09:23:06,024 iteration 107 : loss : 0.247011, loss_ce: 0.110944
2022-01-08 09:23:07,317 iteration 108 : loss : 0.268230, loss_ce: 0.109048
2022-01-08 09:23:08,647 iteration 109 : loss : 0.209490, loss_ce: 0.089057
2022-01-08 09:23:09,931 iteration 110 : loss : 0.219914, loss_ce: 0.096852
2022-01-08 09:23:11,314 iteration 111 : loss : 0.253385, loss_ce: 0.117796
2022-01-08 09:23:12,618 iteration 112 : loss : 0.220365, loss_ce: 0.087605
2022-01-08 09:23:13,987 iteration 113 : loss : 0.266860, loss_ce: 0.137742
2022-01-08 09:23:15,303 iteration 114 : loss : 0.260065, loss_ce: 0.095320
2022-01-08 09:23:16,640 iteration 115 : loss : 0.229905, loss_ce: 0.098222
2022-01-08 09:23:18,035 iteration 116 : loss : 0.248114, loss_ce: 0.115693
2022-01-08 09:23:19,413 iteration 117 : loss : 0.228898, loss_ce: 0.104091
2022-01-08 09:23:20,775 iteration 118 : loss : 0.251431, loss_ce: 0.114837
2022-01-08 09:23:22,150 iteration 119 : loss : 0.204024, loss_ce: 0.081007
  2%|▌                              | 7/400 [02:52<2:44:22, 25.10s/it]2022-01-08 09:23:23,511 iteration 120 : loss : 0.320991, loss_ce: 0.157243
2022-01-08 09:23:24,810 iteration 121 : loss : 0.204362, loss_ce: 0.082464
2022-01-08 09:23:26,221 iteration 122 : loss : 0.207286, loss_ce: 0.073018
2022-01-08 09:23:27,592 iteration 123 : loss : 0.222502, loss_ce: 0.086246
2022-01-08 09:23:28,944 iteration 124 : loss : 0.241242, loss_ce: 0.091851
2022-01-08 09:23:30,257 iteration 125 : loss : 0.223675, loss_ce: 0.105304
2022-01-08 09:23:31,635 iteration 126 : loss : 0.230935, loss_ce: 0.083563
2022-01-08 09:23:32,947 iteration 127 : loss : 0.218362, loss_ce: 0.099691
2022-01-08 09:23:34,286 iteration 128 : loss : 0.204913, loss_ce: 0.087972
2022-01-08 09:23:35,685 iteration 129 : loss : 0.224129, loss_ce: 0.088511
2022-01-08 09:23:37,026 iteration 130 : loss : 0.239695, loss_ce: 0.093078
2022-01-08 09:23:38,462 iteration 131 : loss : 0.225640, loss_ce: 0.114613
2022-01-08 09:23:39,910 iteration 132 : loss : 0.194187, loss_ce: 0.061584
2022-01-08 09:23:41,236 iteration 133 : loss : 0.203433, loss_ce: 0.077654
2022-01-08 09:23:42,658 iteration 134 : loss : 0.232065, loss_ce: 0.093557
2022-01-08 09:23:44,091 iteration 135 : loss : 0.233824, loss_ce: 0.102156
2022-01-08 09:23:45,430 iteration 136 : loss : 0.199449, loss_ce: 0.091147
  2%|▌                              | 8/400 [03:16<2:40:09, 24.51s/it]2022-01-08 09:23:46,882 iteration 137 : loss : 0.232747, loss_ce: 0.087192
2022-01-08 09:23:48,213 iteration 138 : loss : 0.236131, loss_ce: 0.126347
2022-01-08 09:23:49,598 iteration 139 : loss : 0.262783, loss_ce: 0.109191
2022-01-08 09:23:50,946 iteration 140 : loss : 0.217569, loss_ce: 0.079937
2022-01-08 09:23:52,350 iteration 141 : loss : 0.210558, loss_ce: 0.103036
2022-01-08 09:23:53,765 iteration 142 : loss : 0.224159, loss_ce: 0.090381
2022-01-08 09:23:55,218 iteration 143 : loss : 0.204572, loss_ce: 0.095037
2022-01-08 09:23:56,643 iteration 144 : loss : 0.223890, loss_ce: 0.090885
2022-01-08 09:23:57,998 iteration 145 : loss : 0.247724, loss_ce: 0.099888
2022-01-08 09:23:59,391 iteration 146 : loss : 0.177663, loss_ce: 0.084040
2022-01-08 09:24:00,786 iteration 147 : loss : 0.205568, loss_ce: 0.083406
2022-01-08 09:24:02,058 iteration 148 : loss : 0.207548, loss_ce: 0.090586
2022-01-08 09:24:03,411 iteration 149 : loss : 0.250561, loss_ce: 0.108705
2022-01-08 09:24:04,736 iteration 150 : loss : 0.258256, loss_ce: 0.093158
2022-01-08 09:24:06,066 iteration 151 : loss : 0.207361, loss_ce: 0.094949
2022-01-08 09:24:07,400 iteration 152 : loss : 0.209204, loss_ce: 0.071459
2022-01-08 09:24:08,782 iteration 153 : loss : 0.231898, loss_ce: 0.097047
  2%|▋                              | 9/400 [03:39<2:37:23, 24.15s/it]2022-01-08 09:24:10,130 iteration 154 : loss : 0.262356, loss_ce: 0.112430
2022-01-08 09:24:11,500 iteration 155 : loss : 0.208850, loss_ce: 0.074591
2022-01-08 09:24:12,940 iteration 156 : loss : 0.154736, loss_ce: 0.061575
2022-01-08 09:24:14,318 iteration 157 : loss : 0.241579, loss_ce: 0.090088
2022-01-08 09:24:15,798 iteration 158 : loss : 0.227482, loss_ce: 0.099858
2022-01-08 09:24:17,054 iteration 159 : loss : 0.219268, loss_ce: 0.097863
2022-01-08 09:24:18,493 iteration 160 : loss : 0.164530, loss_ce: 0.056351
2022-01-08 09:24:19,909 iteration 161 : loss : 0.253198, loss_ce: 0.102831
2022-01-08 09:24:21,347 iteration 162 : loss : 0.226597, loss_ce: 0.109020
2022-01-08 09:24:22,707 iteration 163 : loss : 0.242208, loss_ce: 0.099178
2022-01-08 09:24:24,085 iteration 164 : loss : 0.197570, loss_ce: 0.076863
2022-01-08 09:24:25,435 iteration 165 : loss : 0.245160, loss_ce: 0.110139
2022-01-08 09:24:26,773 iteration 166 : loss : 0.220883, loss_ce: 0.088075
2022-01-08 09:24:28,133 iteration 167 : loss : 0.181150, loss_ce: 0.077435
2022-01-08 09:24:29,524 iteration 168 : loss : 0.220312, loss_ce: 0.112782
2022-01-08 09:24:30,894 iteration 169 : loss : 0.201594, loss_ce: 0.105337
2022-01-08 09:24:30,895 Training Data Eval:
2022-01-08 09:24:37,712   Average segmentation loss on training set: 3.8434
2022-01-08 09:24:37,712 Validation Data Eval:
2022-01-08 09:24:40,070   Average segmentation loss on validation set: 3.7157
2022-01-08 09:24:41,486 iteration 170 : loss : 0.198115, loss_ce: 0.092886
  2%|▊                             | 10/400 [04:12<2:54:08, 26.79s/it]2022-01-08 09:24:42,915 iteration 171 : loss : 0.222300, loss_ce: 0.094125
2022-01-08 09:24:44,329 iteration 172 : loss : 0.204221, loss_ce: 0.094135
2022-01-08 09:24:45,765 iteration 173 : loss : 0.191323, loss_ce: 0.089270
2022-01-08 09:24:47,085 iteration 174 : loss : 0.226847, loss_ce: 0.099753
2022-01-08 09:24:48,394 iteration 175 : loss : 0.274332, loss_ce: 0.118258
2022-01-08 09:24:49,784 iteration 176 : loss : 0.204909, loss_ce: 0.094746
2022-01-08 09:24:51,157 iteration 177 : loss : 0.224659, loss_ce: 0.092951
2022-01-08 09:24:52,526 iteration 178 : loss : 0.164286, loss_ce: 0.072341
2022-01-08 09:24:53,879 iteration 179 : loss : 0.210953, loss_ce: 0.092999
2022-01-08 09:24:55,222 iteration 180 : loss : 0.189922, loss_ce: 0.076252
2022-01-08 09:24:56,614 iteration 181 : loss : 0.174886, loss_ce: 0.077150
2022-01-08 09:24:57,940 iteration 182 : loss : 0.161148, loss_ce: 0.071912
2022-01-08 09:24:59,255 iteration 183 : loss : 0.176521, loss_ce: 0.066371
2022-01-08 09:25:00,525 iteration 184 : loss : 0.214419, loss_ce: 0.085099
2022-01-08 09:25:01,921 iteration 185 : loss : 0.283593, loss_ce: 0.108351
2022-01-08 09:25:03,308 iteration 186 : loss : 0.262280, loss_ce: 0.114130
2022-01-08 09:25:04,736 iteration 187 : loss : 0.229459, loss_ce: 0.094150
  3%|▊                             | 11/400 [04:35<2:46:39, 25.71s/it]2022-01-08 09:25:06,171 iteration 188 : loss : 0.244731, loss_ce: 0.101106
2022-01-08 09:25:07,553 iteration 189 : loss : 0.206973, loss_ce: 0.090494
2022-01-08 09:25:08,944 iteration 190 : loss : 0.214740, loss_ce: 0.082940
2022-01-08 09:25:10,281 iteration 191 : loss : 0.175445, loss_ce: 0.071287
2022-01-08 09:25:11,579 iteration 192 : loss : 0.215124, loss_ce: 0.104403
2022-01-08 09:25:13,004 iteration 193 : loss : 0.222488, loss_ce: 0.106507
2022-01-08 09:25:14,349 iteration 194 : loss : 0.283186, loss_ce: 0.093270
2022-01-08 09:25:15,686 iteration 195 : loss : 0.233556, loss_ce: 0.109807
2022-01-08 09:25:16,967 iteration 196 : loss : 0.217931, loss_ce: 0.083622
2022-01-08 09:25:18,336 iteration 197 : loss : 0.269301, loss_ce: 0.118896
2022-01-08 09:25:19,728 iteration 198 : loss : 0.183485, loss_ce: 0.086547
2022-01-08 09:25:21,053 iteration 199 : loss : 0.208057, loss_ce: 0.091516
2022-01-08 09:25:22,399 iteration 200 : loss : 0.197102, loss_ce: 0.084353
2022-01-08 09:25:23,783 iteration 201 : loss : 0.144311, loss_ce: 0.059317
2022-01-08 09:25:25,181 iteration 202 : loss : 0.196852, loss_ce: 0.087418
2022-01-08 09:25:26,504 iteration 203 : loss : 0.179772, loss_ce: 0.073248
2022-01-08 09:25:27,948 iteration 204 : loss : 0.255719, loss_ce: 0.101738
  3%|▉                             | 12/400 [04:58<2:41:19, 24.95s/it]2022-01-08 09:25:29,296 iteration 205 : loss : 0.166209, loss_ce: 0.070639
2022-01-08 09:25:30,649 iteration 206 : loss : 0.173668, loss_ce: 0.060213
2022-01-08 09:25:32,042 iteration 207 : loss : 0.153623, loss_ce: 0.069867
2022-01-08 09:25:33,392 iteration 208 : loss : 0.213238, loss_ce: 0.067074
2022-01-08 09:25:34,829 iteration 209 : loss : 0.256527, loss_ce: 0.119536
2022-01-08 09:25:36,102 iteration 210 : loss : 0.204861, loss_ce: 0.074499
2022-01-08 09:25:37,502 iteration 211 : loss : 0.213509, loss_ce: 0.091548
2022-01-08 09:25:38,877 iteration 212 : loss : 0.216416, loss_ce: 0.082964
2022-01-08 09:25:40,347 iteration 213 : loss : 0.166233, loss_ce: 0.087805
2022-01-08 09:25:41,712 iteration 214 : loss : 0.200674, loss_ce: 0.072221
2022-01-08 09:25:43,083 iteration 215 : loss : 0.205447, loss_ce: 0.087869
2022-01-08 09:25:44,601 iteration 216 : loss : 0.173217, loss_ce: 0.071027
2022-01-08 09:25:46,051 iteration 217 : loss : 0.157018, loss_ce: 0.062986
2022-01-08 09:25:47,430 iteration 218 : loss : 0.176972, loss_ce: 0.088812
2022-01-08 09:25:48,700 iteration 219 : loss : 0.183365, loss_ce: 0.081141
2022-01-08 09:25:50,097 iteration 220 : loss : 0.185079, loss_ce: 0.088811
2022-01-08 09:25:51,467 iteration 221 : loss : 0.219610, loss_ce: 0.086623
  3%|▉                             | 13/400 [05:22<2:38:07, 24.52s/it]2022-01-08 09:25:52,889 iteration 222 : loss : 0.198132, loss_ce: 0.097485
2022-01-08 09:25:54,298 iteration 223 : loss : 0.165734, loss_ce: 0.072898
2022-01-08 09:25:55,650 iteration 224 : loss : 0.251901, loss_ce: 0.132498
2022-01-08 09:25:57,099 iteration 225 : loss : 0.298493, loss_ce: 0.098380
2022-01-08 09:25:58,460 iteration 226 : loss : 0.154720, loss_ce: 0.060278
2022-01-08 09:25:59,874 iteration 227 : loss : 0.231144, loss_ce: 0.106605
2022-01-08 09:26:01,237 iteration 228 : loss : 0.192228, loss_ce: 0.076235
2022-01-08 09:26:02,561 iteration 229 : loss : 0.204835, loss_ce: 0.077084
2022-01-08 09:26:03,936 iteration 230 : loss : 0.180983, loss_ce: 0.078715
2022-01-08 09:26:05,345 iteration 231 : loss : 0.245853, loss_ce: 0.097042
2022-01-08 09:26:06,739 iteration 232 : loss : 0.167237, loss_ce: 0.071210
2022-01-08 09:26:08,062 iteration 233 : loss : 0.248073, loss_ce: 0.110098
2022-01-08 09:26:09,496 iteration 234 : loss : 0.224514, loss_ce: 0.090390
2022-01-08 09:26:10,842 iteration 235 : loss : 0.204942, loss_ce: 0.089361
2022-01-08 09:26:12,237 iteration 236 : loss : 0.187482, loss_ce: 0.081450
2022-01-08 09:26:13,591 iteration 237 : loss : 0.159128, loss_ce: 0.062251
2022-01-08 09:26:14,959 iteration 238 : loss : 0.182852, loss_ce: 0.083760
  4%|█                             | 14/400 [05:45<2:35:43, 24.21s/it]2022-01-08 09:26:16,313 iteration 239 : loss : 0.194303, loss_ce: 0.073098
2022-01-08 09:26:17,719 iteration 240 : loss : 0.176592, loss_ce: 0.086780
2022-01-08 09:26:19,129 iteration 241 : loss : 0.223675, loss_ce: 0.090646
2022-01-08 09:26:20,515 iteration 242 : loss : 0.201486, loss_ce: 0.108520
2022-01-08 09:26:22,041 iteration 243 : loss : 0.240149, loss_ce: 0.103013
2022-01-08 09:26:23,336 iteration 244 : loss : 0.215643, loss_ce: 0.063978
2022-01-08 09:26:24,718 iteration 245 : loss : 0.179792, loss_ce: 0.085988
2022-01-08 09:26:26,134 iteration 246 : loss : 0.191228, loss_ce: 0.079431
2022-01-08 09:26:27,489 iteration 247 : loss : 0.240259, loss_ce: 0.079206
2022-01-08 09:26:28,860 iteration 248 : loss : 0.167118, loss_ce: 0.053026
2022-01-08 09:26:30,195 iteration 249 : loss : 0.226513, loss_ce: 0.119879
2022-01-08 09:26:31,550 iteration 250 : loss : 0.201196, loss_ce: 0.084770
2022-01-08 09:26:32,853 iteration 251 : loss : 0.196941, loss_ce: 0.063965
2022-01-08 09:26:34,209 iteration 252 : loss : 0.159405, loss_ce: 0.064000
2022-01-08 09:26:35,595 iteration 253 : loss : 0.223287, loss_ce: 0.099236
2022-01-08 09:26:37,032 iteration 254 : loss : 0.276175, loss_ce: 0.090530
2022-01-08 09:26:37,032 Training Data Eval:
2022-01-08 09:26:43,869   Average segmentation loss on training set: 0.3743
2022-01-08 09:26:43,870 Validation Data Eval:
2022-01-08 09:26:46,227   Average segmentation loss on validation set: 0.3437
2022-01-08 09:26:47,566 iteration 255 : loss : 0.179673, loss_ce: 0.080123
  4%|█▏                            | 15/400 [06:18<2:51:35, 26.74s/it]2022-01-08 09:26:49,007 iteration 256 : loss : 0.171442, loss_ce: 0.078075
2022-01-08 09:26:50,439 iteration 257 : loss : 0.172321, loss_ce: 0.076245
2022-01-08 09:26:51,911 iteration 258 : loss : 0.180353, loss_ce: 0.070943
2022-01-08 09:26:53,203 iteration 259 : loss : 0.132824, loss_ce: 0.064034
2022-01-08 09:26:54,645 iteration 260 : loss : 0.161159, loss_ce: 0.071348
2022-01-08 09:26:56,044 iteration 261 : loss : 0.247429, loss_ce: 0.092747
2022-01-08 09:26:57,518 iteration 262 : loss : 0.196638, loss_ce: 0.078676
2022-01-08 09:26:58,891 iteration 263 : loss : 0.147827, loss_ce: 0.063511
2022-01-08 09:27:00,297 iteration 264 : loss : 0.230497, loss_ce: 0.123589
2022-01-08 09:27:01,626 iteration 265 : loss : 0.130929, loss_ce: 0.057567
2022-01-08 09:27:03,034 iteration 266 : loss : 0.165013, loss_ce: 0.083157
2022-01-08 09:27:04,407 iteration 267 : loss : 0.148301, loss_ce: 0.048291
2022-01-08 09:27:05,781 iteration 268 : loss : 0.186952, loss_ce: 0.081775
2022-01-08 09:27:07,344 iteration 269 : loss : 0.197536, loss_ce: 0.075697
2022-01-08 09:27:08,809 iteration 270 : loss : 0.177732, loss_ce: 0.082473
2022-01-08 09:27:10,061 iteration 271 : loss : 0.206178, loss_ce: 0.065669
2022-01-08 09:27:11,457 iteration 272 : loss : 0.204894, loss_ce: 0.100612
  4%|█▏                            | 16/400 [06:42<2:45:38, 25.88s/it]2022-01-08 09:27:12,823 iteration 273 : loss : 0.154411, loss_ce: 0.058983
2022-01-08 09:27:14,138 iteration 274 : loss : 0.150768, loss_ce: 0.065080
2022-01-08 09:27:15,466 iteration 275 : loss : 0.150800, loss_ce: 0.060218
2022-01-08 09:27:16,846 iteration 276 : loss : 0.151521, loss_ce: 0.076952
2022-01-08 09:27:18,239 iteration 277 : loss : 0.181688, loss_ce: 0.070419
2022-01-08 09:27:19,476 iteration 278 : loss : 0.193327, loss_ce: 0.069190
2022-01-08 09:27:20,793 iteration 279 : loss : 0.195700, loss_ce: 0.070514
2022-01-08 09:27:22,114 iteration 280 : loss : 0.194341, loss_ce: 0.082315
2022-01-08 09:27:23,497 iteration 281 : loss : 0.161584, loss_ce: 0.065264
2022-01-08 09:27:24,848 iteration 282 : loss : 0.234164, loss_ce: 0.095692
2022-01-08 09:27:26,184 iteration 283 : loss : 0.215727, loss_ce: 0.106899
2022-01-08 09:27:27,611 iteration 284 : loss : 0.279714, loss_ce: 0.129214
2022-01-08 09:27:28,909 iteration 285 : loss : 0.223684, loss_ce: 0.070098
2022-01-08 09:27:30,235 iteration 286 : loss : 0.176148, loss_ce: 0.073628
2022-01-08 09:27:31,682 iteration 287 : loss : 0.184487, loss_ce: 0.081820
2022-01-08 09:27:33,106 iteration 288 : loss : 0.215862, loss_ce: 0.091301
2022-01-08 09:27:34,454 iteration 289 : loss : 0.171853, loss_ce: 0.073069
  4%|█▎                            | 17/400 [07:05<2:39:40, 25.01s/it]2022-01-08 09:27:35,873 iteration 290 : loss : 0.177137, loss_ce: 0.073267
2022-01-08 09:27:37,201 iteration 291 : loss : 0.191368, loss_ce: 0.080182
2022-01-08 09:27:38,543 iteration 292 : loss : 0.150570, loss_ce: 0.064611
2022-01-08 09:27:39,896 iteration 293 : loss : 0.216999, loss_ce: 0.092895
2022-01-08 09:27:41,266 iteration 294 : loss : 0.176342, loss_ce: 0.075009
2022-01-08 09:27:42,724 iteration 295 : loss : 0.179643, loss_ce: 0.061413
2022-01-08 09:27:44,058 iteration 296 : loss : 0.173973, loss_ce: 0.063749
2022-01-08 09:27:45,436 iteration 297 : loss : 0.166575, loss_ce: 0.062725
2022-01-08 09:27:46,774 iteration 298 : loss : 0.142494, loss_ce: 0.050116
2022-01-08 09:27:48,136 iteration 299 : loss : 0.170620, loss_ce: 0.058955
2022-01-08 09:27:49,501 iteration 300 : loss : 0.183950, loss_ce: 0.064697
2022-01-08 09:27:50,876 iteration 301 : loss : 0.230364, loss_ce: 0.126077
2022-01-08 09:27:52,302 iteration 302 : loss : 0.136172, loss_ce: 0.058421
2022-01-08 09:27:53,704 iteration 303 : loss : 0.179079, loss_ce: 0.080384
2022-01-08 09:27:55,088 iteration 304 : loss : 0.237141, loss_ce: 0.109445
2022-01-08 09:27:56,498 iteration 305 : loss : 0.171589, loss_ce: 0.074429
2022-01-08 09:27:57,906 iteration 306 : loss : 0.132393, loss_ce: 0.070642
  4%|█▎                            | 18/400 [07:28<2:36:16, 24.55s/it]2022-01-08 09:27:59,338 iteration 307 : loss : 0.201199, loss_ce: 0.078248
2022-01-08 09:28:00,683 iteration 308 : loss : 0.155755, loss_ce: 0.080620
2022-01-08 09:28:02,078 iteration 309 : loss : 0.265314, loss_ce: 0.124778
2022-01-08 09:28:03,495 iteration 310 : loss : 0.180444, loss_ce: 0.083619
2022-01-08 09:28:04,824 iteration 311 : loss : 0.206395, loss_ce: 0.070953
2022-01-08 09:28:06,186 iteration 312 : loss : 0.145001, loss_ce: 0.069276
2022-01-08 09:28:07,612 iteration 313 : loss : 0.180680, loss_ce: 0.099101
2022-01-08 09:28:08,905 iteration 314 : loss : 0.147579, loss_ce: 0.071337
2022-01-08 09:28:10,250 iteration 315 : loss : 0.204356, loss_ce: 0.075654
2022-01-08 09:28:11,635 iteration 316 : loss : 0.177288, loss_ce: 0.065022
2022-01-08 09:28:13,012 iteration 317 : loss : 0.210939, loss_ce: 0.088874
2022-01-08 09:28:14,404 iteration 318 : loss : 0.132761, loss_ce: 0.059044
2022-01-08 09:28:15,724 iteration 319 : loss : 0.135410, loss_ce: 0.061835
2022-01-08 09:28:17,169 iteration 320 : loss : 0.145814, loss_ce: 0.053642
2022-01-08 09:28:18,574 iteration 321 : loss : 0.144271, loss_ce: 0.049252
2022-01-08 09:28:19,934 iteration 322 : loss : 0.211895, loss_ce: 0.099559
2022-01-08 09:28:21,358 iteration 323 : loss : 0.171167, loss_ce: 0.055557
  5%|█▍                            | 19/400 [07:52<2:33:46, 24.22s/it]2022-01-08 09:28:22,754 iteration 324 : loss : 0.177897, loss_ce: 0.064047
2022-01-08 09:28:24,129 iteration 325 : loss : 0.192183, loss_ce: 0.067791
2022-01-08 09:28:25,532 iteration 326 : loss : 0.152749, loss_ce: 0.070580
2022-01-08 09:28:26,997 iteration 327 : loss : 0.202222, loss_ce: 0.076861
2022-01-08 09:28:28,355 iteration 328 : loss : 0.198017, loss_ce: 0.090020
2022-01-08 09:28:29,759 iteration 329 : loss : 0.156035, loss_ce: 0.083090
2022-01-08 09:28:31,181 iteration 330 : loss : 0.167401, loss_ce: 0.071757
2022-01-08 09:28:32,517 iteration 331 : loss : 0.153065, loss_ce: 0.073391
2022-01-08 09:28:33,882 iteration 332 : loss : 0.152194, loss_ce: 0.054727
2022-01-08 09:28:35,269 iteration 333 : loss : 0.126289, loss_ce: 0.061483
2022-01-08 09:28:36,644 iteration 334 : loss : 0.175131, loss_ce: 0.079054
2022-01-08 09:28:37,985 iteration 335 : loss : 0.234924, loss_ce: 0.077910
2022-01-08 09:28:39,394 iteration 336 : loss : 0.144410, loss_ce: 0.055339
2022-01-08 09:28:40,765 iteration 337 : loss : 0.153029, loss_ce: 0.055958
2022-01-08 09:28:42,102 iteration 338 : loss : 0.173015, loss_ce: 0.096168
2022-01-08 09:28:43,435 iteration 339 : loss : 0.167203, loss_ce: 0.061652
2022-01-08 09:28:43,435 Training Data Eval:
2022-01-08 09:28:50,316   Average segmentation loss on training set: 0.1327
2022-01-08 09:28:50,316 Validation Data Eval:
2022-01-08 09:28:52,681   Average segmentation loss on validation set: 0.1747
2022-01-08 09:28:56,850 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8BLOCKS_best_val_loss_seed1234.pth
2022-01-08 09:28:58,194 iteration 340 : loss : 0.167695, loss_ce: 0.069735
  5%|█▌                            | 20/400 [08:28<2:57:21, 28.00s/it]2022-01-08 09:28:59,562 iteration 341 : loss : 0.110535, loss_ce: 0.048896
2022-01-08 09:29:00,870 iteration 342 : loss : 0.137381, loss_ce: 0.050168
2022-01-08 09:29:02,145 iteration 343 : loss : 0.157741, loss_ce: 0.057061
