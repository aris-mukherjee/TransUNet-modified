2022-01-11 22:11:51,279 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:11:51,280 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:11:51,280 ============================================================
2022-01-11 22:11:51,280 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-11 22:11:51,280 ============================================================
2022-01-11 22:11:51,280 Loading data...
2022-01-11 22:11:51,280 Reading NCI - RUNMC images...
2022-01-11 22:11:51,280 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-11 22:11:51,283 Already preprocessed this configuration. Loading now!
2022-01-11 22:11:51,315 Training Images: (256, 256, 286)
2022-01-11 22:11:51,316 Training Labels: (256, 256, 286)
2022-01-11 22:11:51,316 Validation Images: (256, 256, 98)
2022-01-11 22:11:51,316 Validation Labels: (256, 256, 98)
2022-01-11 22:11:51,316 ============================================================
2022-01-11 22:11:51,360 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-11 22:11:54,303 iteration 1 : loss : 0.765677, loss_ce: 0.862266
2022-01-11 22:11:55,744 iteration 2 : loss : 0.724199, loss_ce: 0.774192
2022-01-11 22:11:57,163 iteration 3 : loss : 0.683382, loss_ce: 0.685836
2022-01-11 22:11:58,645 iteration 4 : loss : 0.633337, loss_ce: 0.639803
2022-01-11 22:12:00,044 iteration 5 : loss : 0.604585, loss_ce: 0.566104
2022-01-11 22:12:01,551 iteration 6 : loss : 0.564697, loss_ce: 0.513692
2022-01-11 22:12:03,056 iteration 7 : loss : 0.542861, loss_ce: 0.464439
2022-01-11 22:12:04,558 iteration 8 : loss : 0.501286, loss_ce: 0.418666
2022-01-11 22:12:05,935 iteration 9 : loss : 0.483757, loss_ce: 0.408526
2022-01-11 22:12:07,440 iteration 10 : loss : 0.460410, loss_ce: 0.365510
2022-01-11 22:12:08,937 iteration 11 : loss : 0.435021, loss_ce: 0.330519
2022-01-11 22:12:10,473 iteration 12 : loss : 0.425032, loss_ce: 0.309282
2022-01-11 22:12:11,905 iteration 13 : loss : 0.399344, loss_ce: 0.272957
2022-01-11 22:12:13,446 iteration 14 : loss : 0.393788, loss_ce: 0.259011
2022-01-11 22:12:14,903 iteration 15 : loss : 0.378438, loss_ce: 0.231212
2022-01-11 22:12:16,346 iteration 16 : loss : 0.371291, loss_ce: 0.226823
2022-01-11 22:12:17,757 iteration 17 : loss : 0.362058, loss_ce: 0.196647
  0%|                               | 1/400 [00:26<2:56:19, 26.52s/it]2022-01-11 22:12:19,376 iteration 18 : loss : 0.348445, loss_ce: 0.199913
2022-01-11 22:12:20,882 iteration 19 : loss : 0.333557, loss_ce: 0.161031
2022-01-11 22:12:22,462 iteration 20 : loss : 0.330176, loss_ce: 0.176786
2022-01-11 22:12:23,906 iteration 21 : loss : 0.335561, loss_ce: 0.161635
2022-01-11 22:12:25,288 iteration 22 : loss : 0.312070, loss_ce: 0.148675
2022-01-11 22:12:26,820 iteration 23 : loss : 0.318614, loss_ce: 0.148751
2022-01-11 22:12:28,350 iteration 24 : loss : 0.292877, loss_ce: 0.144473
2022-01-11 22:12:29,901 iteration 25 : loss : 0.337344, loss_ce: 0.188302
2022-01-11 22:12:31,362 iteration 26 : loss : 0.300698, loss_ce: 0.149091
2022-01-11 22:12:32,766 iteration 27 : loss : 0.300164, loss_ce: 0.140105
2022-01-11 22:12:34,158 iteration 28 : loss : 0.287131, loss_ce: 0.131215
2022-01-11 22:12:35,657 iteration 29 : loss : 0.338535, loss_ce: 0.164000
2022-01-11 22:12:37,097 iteration 30 : loss : 0.273769, loss_ce: 0.128597
2022-01-11 22:12:38,497 iteration 31 : loss : 0.283285, loss_ce: 0.116611
2022-01-11 22:12:39,987 iteration 32 : loss : 0.286322, loss_ce: 0.150271
2022-01-11 22:12:41,574 iteration 33 : loss : 0.287991, loss_ce: 0.131998
2022-01-11 22:12:43,273 iteration 34 : loss : 0.230003, loss_ce: 0.097457
  0%|▏                              | 2/400 [00:51<2:51:41, 25.88s/it]2022-01-11 22:12:44,924 iteration 35 : loss : 0.248986, loss_ce: 0.114939
2022-01-11 22:12:46,535 iteration 36 : loss : 0.278506, loss_ce: 0.095005
2022-01-11 22:12:48,170 iteration 37 : loss : 0.253049, loss_ce: 0.094910
2022-01-11 22:12:49,775 iteration 38 : loss : 0.249979, loss_ce: 0.103948
2022-01-11 22:12:51,272 iteration 39 : loss : 0.284586, loss_ce: 0.120871
2022-01-11 22:12:52,839 iteration 40 : loss : 0.325816, loss_ce: 0.153998
2022-01-11 22:12:54,334 iteration 41 : loss : 0.203739, loss_ce: 0.092703
2022-01-11 22:12:55,953 iteration 42 : loss : 0.273103, loss_ce: 0.131073
2022-01-11 22:12:57,545 iteration 43 : loss : 0.236844, loss_ce: 0.107060
2022-01-11 22:12:59,051 iteration 44 : loss : 0.261194, loss_ce: 0.140787
2022-01-11 22:13:00,715 iteration 45 : loss : 0.284847, loss_ce: 0.121849
2022-01-11 22:13:02,331 iteration 46 : loss : 0.271339, loss_ce: 0.127672
2022-01-11 22:13:03,928 iteration 47 : loss : 0.264979, loss_ce: 0.100750
2022-01-11 22:13:05,528 iteration 48 : loss : 0.237520, loss_ce: 0.107554
2022-01-11 22:13:07,176 iteration 49 : loss : 0.296484, loss_ce: 0.138599
2022-01-11 22:13:08,818 iteration 50 : loss : 0.252318, loss_ce: 0.109035
2022-01-11 22:13:10,494 iteration 51 : loss : 0.224842, loss_ce: 0.102414
  1%|▏                              | 3/400 [01:19<2:55:18, 26.49s/it]2022-01-11 22:13:12,077 iteration 52 : loss : 0.275550, loss_ce: 0.117333
2022-01-11 22:13:13,591 iteration 53 : loss : 0.235795, loss_ce: 0.114953
2022-01-11 22:13:15,248 iteration 54 : loss : 0.248873, loss_ce: 0.108151
2022-01-11 22:13:16,872 iteration 55 : loss : 0.302044, loss_ce: 0.122242
2022-01-11 22:13:18,508 iteration 56 : loss : 0.253257, loss_ce: 0.124041
2022-01-11 22:13:20,063 iteration 57 : loss : 0.268915, loss_ce: 0.129932
2022-01-11 22:13:21,606 iteration 58 : loss : 0.256502, loss_ce: 0.125498
2022-01-11 22:13:23,137 iteration 59 : loss : 0.221503, loss_ce: 0.095540
2022-01-11 22:13:24,691 iteration 60 : loss : 0.219782, loss_ce: 0.102606
2022-01-11 22:13:26,270 iteration 61 : loss : 0.239676, loss_ce: 0.099685
2022-01-11 22:13:27,938 iteration 62 : loss : 0.277438, loss_ce: 0.107174
2022-01-11 22:13:29,490 iteration 63 : loss : 0.243266, loss_ce: 0.111003
2022-01-11 22:13:31,112 iteration 64 : loss : 0.298537, loss_ce: 0.123998
2022-01-11 22:13:32,717 iteration 65 : loss : 0.262969, loss_ce: 0.102556
2022-01-11 22:13:34,394 iteration 66 : loss : 0.274345, loss_ce: 0.118065
2022-01-11 22:13:35,967 iteration 67 : loss : 0.236960, loss_ce: 0.112150
2022-01-11 22:13:37,479 iteration 68 : loss : 0.241136, loss_ce: 0.118151
  1%|▎                              | 4/400 [01:46<2:56:07, 26.69s/it]2022-01-11 22:13:39,153 iteration 69 : loss : 0.281650, loss_ce: 0.128585
2022-01-11 22:13:40,714 iteration 70 : loss : 0.302633, loss_ce: 0.125809
2022-01-11 22:13:42,303 iteration 71 : loss : 0.266910, loss_ce: 0.136280
2022-01-11 22:13:43,922 iteration 72 : loss : 0.243803, loss_ce: 0.104407
2022-01-11 22:13:45,539 iteration 73 : loss : 0.228644, loss_ce: 0.086589
2022-01-11 22:13:47,158 iteration 74 : loss : 0.219906, loss_ce: 0.087052
2022-01-11 22:13:48,717 iteration 75 : loss : 0.234638, loss_ce: 0.110766
2022-01-11 22:13:50,217 iteration 76 : loss : 0.247496, loss_ce: 0.114069
2022-01-11 22:13:51,817 iteration 77 : loss : 0.259121, loss_ce: 0.110477
2022-01-11 22:13:53,403 iteration 78 : loss : 0.220733, loss_ce: 0.083461
2022-01-11 22:13:55,020 iteration 79 : loss : 0.254666, loss_ce: 0.096770
2022-01-11 22:13:56,517 iteration 80 : loss : 0.235692, loss_ce: 0.087738
2022-01-11 22:13:58,174 iteration 81 : loss : 0.269605, loss_ce: 0.125028
2022-01-11 22:13:59,752 iteration 82 : loss : 0.291628, loss_ce: 0.096049
2022-01-11 22:14:01,283 iteration 83 : loss : 0.290178, loss_ce: 0.086623
2022-01-11 22:14:02,977 iteration 84 : loss : 0.267778, loss_ce: 0.114366
2022-01-11 22:14:02,977 Training Data Eval:
2022-01-11 22:14:10,940   Average segmentation loss on training set: 0.4140
2022-01-11 22:14:10,941 Validation Data Eval:
2022-01-11 22:14:13,832   Average segmentation loss on validation set: 0.3910
2022-01-11 22:14:19,808 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:14:21,231 iteration 85 : loss : 0.254198, loss_ce: 0.118486
  1%|▍                              | 5/400 [02:29<3:36:13, 32.84s/it]2022-01-11 22:14:22,785 iteration 86 : loss : 0.213273, loss_ce: 0.102240
2022-01-11 22:14:24,235 iteration 87 : loss : 0.225830, loss_ce: 0.098027
2022-01-11 22:14:25,655 iteration 88 : loss : 0.211853, loss_ce: 0.106012
2022-01-11 22:14:27,234 iteration 89 : loss : 0.272300, loss_ce: 0.104600
2022-01-11 22:14:28,804 iteration 90 : loss : 0.227606, loss_ce: 0.097929
2022-01-11 22:14:30,450 iteration 91 : loss : 0.234439, loss_ce: 0.105497
2022-01-11 22:14:32,106 iteration 92 : loss : 0.210011, loss_ce: 0.075986
2022-01-11 22:14:33,632 iteration 93 : loss : 0.229085, loss_ce: 0.079786
2022-01-11 22:14:35,253 iteration 94 : loss : 0.220742, loss_ce: 0.099954
2022-01-11 22:14:36,836 iteration 95 : loss : 0.259969, loss_ce: 0.137390
2022-01-11 22:14:38,502 iteration 96 : loss : 0.231728, loss_ce: 0.099965
2022-01-11 22:14:40,096 iteration 97 : loss : 0.267981, loss_ce: 0.111939
2022-01-11 22:14:41,747 iteration 98 : loss : 0.235616, loss_ce: 0.097276
2022-01-11 22:14:43,324 iteration 99 : loss : 0.270121, loss_ce: 0.107086
2022-01-11 22:14:44,950 iteration 100 : loss : 0.251166, loss_ce: 0.108622
2022-01-11 22:14:46,519 iteration 101 : loss : 0.236776, loss_ce: 0.105245
2022-01-11 22:14:48,088 iteration 102 : loss : 0.248353, loss_ce: 0.121662
  2%|▍                              | 6/400 [02:56<3:22:17, 30.81s/it]2022-01-11 22:14:49,744 iteration 103 : loss : 0.248272, loss_ce: 0.096818
2022-01-11 22:14:51,406 iteration 104 : loss : 0.234770, loss_ce: 0.098985
2022-01-11 22:14:53,050 iteration 105 : loss : 0.202834, loss_ce: 0.081030
2022-01-11 22:14:54,784 iteration 106 : loss : 0.250846, loss_ce: 0.102346
2022-01-11 22:14:56,433 iteration 107 : loss : 0.228123, loss_ce: 0.080009
2022-01-11 22:14:58,019 iteration 108 : loss : 0.239221, loss_ce: 0.078586
2022-01-11 22:14:59,536 iteration 109 : loss : 0.301759, loss_ce: 0.125507
2022-01-11 22:15:01,094 iteration 110 : loss : 0.254383, loss_ce: 0.101607
2022-01-11 22:15:02,749 iteration 111 : loss : 0.203622, loss_ce: 0.085898
2022-01-11 22:15:04,332 iteration 112 : loss : 0.230061, loss_ce: 0.086150
2022-01-11 22:15:05,863 iteration 113 : loss : 0.207408, loss_ce: 0.075991
2022-01-11 22:15:07,489 iteration 114 : loss : 0.211973, loss_ce: 0.077943
2022-01-11 22:15:09,058 iteration 115 : loss : 0.192542, loss_ce: 0.095573
2022-01-11 22:15:10,683 iteration 116 : loss : 0.239452, loss_ce: 0.118343
2022-01-11 22:15:12,240 iteration 117 : loss : 0.208317, loss_ce: 0.091076
2022-01-11 22:15:13,818 iteration 118 : loss : 0.262244, loss_ce: 0.126075
2022-01-11 22:15:15,412 iteration 119 : loss : 0.190868, loss_ce: 0.071834
  2%|▌                              | 7/400 [03:24<3:14:19, 29.67s/it]2022-01-11 22:15:17,137 iteration 120 : loss : 0.188022, loss_ce: 0.087005
2022-01-11 22:15:18,763 iteration 121 : loss : 0.283526, loss_ce: 0.117234
2022-01-11 22:15:20,289 iteration 122 : loss : 0.225058, loss_ce: 0.101803
2022-01-11 22:15:21,932 iteration 123 : loss : 0.200284, loss_ce: 0.083840
2022-01-11 22:15:23,578 iteration 124 : loss : 0.234941, loss_ce: 0.092606
2022-01-11 22:15:25,158 iteration 125 : loss : 0.195174, loss_ce: 0.098720
2022-01-11 22:15:26,809 iteration 126 : loss : 0.264606, loss_ce: 0.103475
2022-01-11 22:15:28,454 iteration 127 : loss : 0.202945, loss_ce: 0.088888
2022-01-11 22:15:30,124 iteration 128 : loss : 0.165469, loss_ce: 0.079580
2022-01-11 22:15:31,690 iteration 129 : loss : 0.225192, loss_ce: 0.084598
2022-01-11 22:15:33,360 iteration 130 : loss : 0.229586, loss_ce: 0.108825
2022-01-11 22:15:34,958 iteration 131 : loss : 0.212337, loss_ce: 0.106071
2022-01-11 22:15:36,510 iteration 132 : loss : 0.239146, loss_ce: 0.094154
2022-01-11 22:15:38,156 iteration 133 : loss : 0.228584, loss_ce: 0.082576
2022-01-11 22:15:39,767 iteration 134 : loss : 0.258183, loss_ce: 0.105132
2022-01-11 22:15:41,387 iteration 135 : loss : 0.202183, loss_ce: 0.081419
2022-01-11 22:15:42,973 iteration 136 : loss : 0.240234, loss_ce: 0.104786
  2%|▌                              | 8/400 [03:51<3:09:26, 29.00s/it]2022-01-11 22:15:44,540 iteration 137 : loss : 0.163128, loss_ce: 0.054010
2022-01-11 22:15:46,091 iteration 138 : loss : 0.220929, loss_ce: 0.117070
2022-01-11 22:15:47,750 iteration 139 : loss : 0.210751, loss_ce: 0.081513
2022-01-11 22:15:49,461 iteration 140 : loss : 0.245000, loss_ce: 0.097927
2022-01-11 22:15:51,080 iteration 141 : loss : 0.240918, loss_ce: 0.099178
2022-01-11 22:15:52,616 iteration 142 : loss : 0.214271, loss_ce: 0.082233
2022-01-11 22:15:54,166 iteration 143 : loss : 0.198909, loss_ce: 0.084503
2022-01-11 22:15:55,816 iteration 144 : loss : 0.242326, loss_ce: 0.094215
2022-01-11 22:15:57,623 iteration 145 : loss : 0.203900, loss_ce: 0.097510
2022-01-11 22:15:59,198 iteration 146 : loss : 0.236618, loss_ce: 0.093700
2022-01-11 22:16:00,786 iteration 147 : loss : 0.254817, loss_ce: 0.110983
2022-01-11 22:16:02,389 iteration 148 : loss : 0.216024, loss_ce: 0.107759
2022-01-11 22:16:04,015 iteration 149 : loss : 0.207698, loss_ce: 0.086119
2022-01-11 22:16:05,602 iteration 150 : loss : 0.287665, loss_ce: 0.156767
2022-01-11 22:16:07,209 iteration 151 : loss : 0.184643, loss_ce: 0.087465
2022-01-11 22:16:08,880 iteration 152 : loss : 0.204320, loss_ce: 0.095275
2022-01-11 22:16:10,557 iteration 153 : loss : 0.179966, loss_ce: 0.086113
  2%|▋                              | 9/400 [04:19<3:06:04, 28.55s/it]2022-01-11 22:16:12,197 iteration 154 : loss : 0.171021, loss_ce: 0.070447
2022-01-11 22:16:13,822 iteration 155 : loss : 0.258258, loss_ce: 0.101609
2022-01-11 22:16:15,437 iteration 156 : loss : 0.199515, loss_ce: 0.078311
2022-01-11 22:16:17,060 iteration 157 : loss : 0.257753, loss_ce: 0.092001
2022-01-11 22:16:18,682 iteration 158 : loss : 0.224199, loss_ce: 0.095068
2022-01-11 22:16:20,296 iteration 159 : loss : 0.241203, loss_ce: 0.094603
2022-01-11 22:16:21,878 iteration 160 : loss : 0.301992, loss_ce: 0.106823
2022-01-11 22:16:23,513 iteration 161 : loss : 0.222689, loss_ce: 0.102719
2022-01-11 22:16:25,210 iteration 162 : loss : 0.194696, loss_ce: 0.090933
2022-01-11 22:16:26,717 iteration 163 : loss : 0.185007, loss_ce: 0.078056
2022-01-11 22:16:28,376 iteration 164 : loss : 0.166480, loss_ce: 0.070236
2022-01-11 22:16:29,961 iteration 165 : loss : 0.160790, loss_ce: 0.061691
2022-01-11 22:16:31,639 iteration 166 : loss : 0.232119, loss_ce: 0.092888
2022-01-11 22:16:33,295 iteration 167 : loss : 0.204479, loss_ce: 0.090238
2022-01-11 22:16:34,955 iteration 168 : loss : 0.250922, loss_ce: 0.120013
2022-01-11 22:16:36,555 iteration 169 : loss : 0.203484, loss_ce: 0.083982
2022-01-11 22:16:36,555 Training Data Eval:
2022-01-11 22:16:44,530   Average segmentation loss on training set: 0.2131
2022-01-11 22:16:44,530 Validation Data Eval:
2022-01-11 22:16:47,275   Average segmentation loss on validation set: 0.2386
2022-01-11 22:16:53,131 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:16:54,567 iteration 170 : loss : 0.212855, loss_ce: 0.092232
  2%|▊                             | 10/400 [05:03<3:36:37, 33.33s/it]2022-01-11 22:16:56,099 iteration 171 : loss : 0.252517, loss_ce: 0.126799
2022-01-11 22:16:57,462 iteration 172 : loss : 0.304423, loss_ce: 0.115753
2022-01-11 22:16:58,910 iteration 173 : loss : 0.185289, loss_ce: 0.077575
2022-01-11 22:17:00,570 iteration 174 : loss : 0.248006, loss_ce: 0.104729
2022-01-11 22:17:02,152 iteration 175 : loss : 0.213563, loss_ce: 0.099023
2022-01-11 22:17:03,860 iteration 176 : loss : 0.229650, loss_ce: 0.086490
2022-01-11 22:17:05,459 iteration 177 : loss : 0.251924, loss_ce: 0.093351
2022-01-11 22:17:07,129 iteration 178 : loss : 0.212904, loss_ce: 0.070356
2022-01-11 22:17:08,757 iteration 179 : loss : 0.174939, loss_ce: 0.070257
2022-01-11 22:17:10,383 iteration 180 : loss : 0.209232, loss_ce: 0.076258
2022-01-11 22:17:12,047 iteration 181 : loss : 0.187468, loss_ce: 0.071657
2022-01-11 22:17:13,684 iteration 182 : loss : 0.154851, loss_ce: 0.061213
2022-01-11 22:17:15,336 iteration 183 : loss : 0.168464, loss_ce: 0.065109
2022-01-11 22:17:16,956 iteration 184 : loss : 0.193400, loss_ce: 0.084745
2022-01-11 22:17:18,514 iteration 185 : loss : 0.232943, loss_ce: 0.111398
2022-01-11 22:17:20,076 iteration 186 : loss : 0.184513, loss_ce: 0.084226
2022-01-11 22:17:21,608 iteration 187 : loss : 0.240372, loss_ce: 0.125949
  3%|▊                             | 11/400 [05:30<3:23:35, 31.40s/it]2022-01-11 22:17:23,266 iteration 188 : loss : 0.301424, loss_ce: 0.126196
2022-01-11 22:17:24,908 iteration 189 : loss : 0.230565, loss_ce: 0.100231
2022-01-11 22:17:26,593 iteration 190 : loss : 0.195435, loss_ce: 0.070895
2022-01-11 22:17:28,213 iteration 191 : loss : 0.219322, loss_ce: 0.104610
2022-01-11 22:17:29,719 iteration 192 : loss : 0.168351, loss_ce: 0.077756
2022-01-11 22:17:31,273 iteration 193 : loss : 0.312096, loss_ce: 0.130926
2022-01-11 22:17:32,806 iteration 194 : loss : 0.257018, loss_ce: 0.113062
2022-01-11 22:17:34,502 iteration 195 : loss : 0.154439, loss_ce: 0.060116
2022-01-11 22:17:36,136 iteration 196 : loss : 0.190741, loss_ce: 0.063900
2022-01-11 22:17:37,754 iteration 197 : loss : 0.175012, loss_ce: 0.070151
2022-01-11 22:17:39,404 iteration 198 : loss : 0.254371, loss_ce: 0.122904
2022-01-11 22:17:41,037 iteration 199 : loss : 0.189349, loss_ce: 0.086740
2022-01-11 22:17:42,629 iteration 200 : loss : 0.214372, loss_ce: 0.085137
2022-01-11 22:17:44,214 iteration 201 : loss : 0.219939, loss_ce: 0.098276
2022-01-11 22:17:45,687 iteration 202 : loss : 0.209630, loss_ce: 0.076569
2022-01-11 22:17:47,342 iteration 203 : loss : 0.146716, loss_ce: 0.064536
2022-01-11 22:17:48,916 iteration 204 : loss : 0.191324, loss_ce: 0.095136
  3%|▉                             | 12/400 [05:57<3:15:02, 30.16s/it]2022-01-11 22:17:50,643 iteration 205 : loss : 0.169212, loss_ce: 0.069516
2022-01-11 22:17:52,255 iteration 206 : loss : 0.342993, loss_ce: 0.148590
2022-01-11 22:17:53,956 iteration 207 : loss : 0.246156, loss_ce: 0.092317
2022-01-11 22:17:55,613 iteration 208 : loss : 0.205763, loss_ce: 0.099675
2022-01-11 22:17:57,245 iteration 209 : loss : 0.199913, loss_ce: 0.084051
2022-01-11 22:17:58,840 iteration 210 : loss : 0.207193, loss_ce: 0.103381
2022-01-11 22:18:00,453 iteration 211 : loss : 0.180841, loss_ce: 0.088502
2022-01-11 22:18:02,124 iteration 212 : loss : 0.184273, loss_ce: 0.084777
2022-01-11 22:18:03,776 iteration 213 : loss : 0.210137, loss_ce: 0.096989
2022-01-11 22:18:05,359 iteration 214 : loss : 0.235501, loss_ce: 0.098139
2022-01-11 22:18:06,912 iteration 215 : loss : 0.208620, loss_ce: 0.082295
2022-01-11 22:18:08,576 iteration 216 : loss : 0.191054, loss_ce: 0.071368
2022-01-11 22:18:10,240 iteration 217 : loss : 0.230604, loss_ce: 0.091399
2022-01-11 22:18:11,833 iteration 218 : loss : 0.185403, loss_ce: 0.080301
2022-01-11 22:18:13,416 iteration 219 : loss : 0.205735, loss_ce: 0.093675
2022-01-11 22:18:15,048 iteration 220 : loss : 0.191014, loss_ce: 0.070252
2022-01-11 22:18:16,691 iteration 221 : loss : 0.177568, loss_ce: 0.067376
  3%|▉                             | 13/400 [06:25<3:09:50, 29.43s/it]2022-01-11 22:18:18,347 iteration 222 : loss : 0.165645, loss_ce: 0.065081
2022-01-11 22:18:19,943 iteration 223 : loss : 0.293085, loss_ce: 0.135987
2022-01-11 22:18:21,537 iteration 224 : loss : 0.149644, loss_ce: 0.053481
2022-01-11 22:18:23,266 iteration 225 : loss : 0.168157, loss_ce: 0.075349
2022-01-11 22:18:24,906 iteration 226 : loss : 0.172113, loss_ce: 0.066383
2022-01-11 22:18:26,516 iteration 227 : loss : 0.201521, loss_ce: 0.082920
2022-01-11 22:18:28,113 iteration 228 : loss : 0.187661, loss_ce: 0.063939
2022-01-11 22:18:29,746 iteration 229 : loss : 0.205200, loss_ce: 0.080481
2022-01-11 22:18:31,293 iteration 230 : loss : 0.135013, loss_ce: 0.059461
2022-01-11 22:18:32,891 iteration 231 : loss : 0.175640, loss_ce: 0.071778
2022-01-11 22:18:34,499 iteration 232 : loss : 0.172187, loss_ce: 0.074396
2022-01-11 22:18:36,033 iteration 233 : loss : 0.166396, loss_ce: 0.081202
2022-01-11 22:18:37,609 iteration 234 : loss : 0.244253, loss_ce: 0.106104
2022-01-11 22:18:39,169 iteration 235 : loss : 0.171343, loss_ce: 0.076182
2022-01-11 22:18:40,772 iteration 236 : loss : 0.327043, loss_ce: 0.141416
2022-01-11 22:18:42,309 iteration 237 : loss : 0.252683, loss_ce: 0.134155
2022-01-11 22:18:43,956 iteration 238 : loss : 0.205744, loss_ce: 0.075384
  4%|█                             | 14/400 [06:52<3:05:10, 28.78s/it]2022-01-11 22:18:45,590 iteration 239 : loss : 0.223865, loss_ce: 0.088039
2022-01-11 22:18:47,285 iteration 240 : loss : 0.170345, loss_ce: 0.062141
2022-01-11 22:18:48,938 iteration 241 : loss : 0.159023, loss_ce: 0.065489
2022-01-11 22:18:50,518 iteration 242 : loss : 0.181615, loss_ce: 0.070170
2022-01-11 22:18:52,106 iteration 243 : loss : 0.148481, loss_ce: 0.064290
2022-01-11 22:18:53,692 iteration 244 : loss : 0.159655, loss_ce: 0.067232
2022-01-11 22:18:55,241 iteration 245 : loss : 0.166903, loss_ce: 0.055931
2022-01-11 22:18:56,831 iteration 246 : loss : 0.233516, loss_ce: 0.086232
2022-01-11 22:18:58,459 iteration 247 : loss : 0.192779, loss_ce: 0.091885
2022-01-11 22:19:00,101 iteration 248 : loss : 0.149453, loss_ce: 0.069002
2022-01-11 22:19:01,686 iteration 249 : loss : 0.156444, loss_ce: 0.066204
2022-01-11 22:19:03,281 iteration 250 : loss : 0.194032, loss_ce: 0.071504
2022-01-11 22:19:04,816 iteration 251 : loss : 0.169611, loss_ce: 0.074206
2022-01-11 22:19:06,490 iteration 252 : loss : 0.200173, loss_ce: 0.095534
2022-01-11 22:19:08,102 iteration 253 : loss : 0.146846, loss_ce: 0.059577
2022-01-11 22:19:09,673 iteration 254 : loss : 0.141199, loss_ce: 0.069666
2022-01-11 22:19:09,674 Training Data Eval:
2022-01-11 22:19:17,652   Average segmentation loss on training set: 0.2473
2022-01-11 22:19:17,653 Validation Data Eval:
2022-01-11 22:19:20,401   Average segmentation loss on validation set: 0.2367
2022-01-11 22:19:26,322 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:19:27,733 iteration 255 : loss : 0.184998, loss_ce: 0.091776
  4%|█▏                            | 15/400 [07:36<3:33:40, 33.30s/it]2022-01-11 22:19:29,177 iteration 256 : loss : 0.128232, loss_ce: 0.059588
2022-01-11 22:19:30,593 iteration 257 : loss : 0.184781, loss_ce: 0.082875
2022-01-11 22:19:32,160 iteration 258 : loss : 0.120770, loss_ce: 0.053502
2022-01-11 22:19:33,778 iteration 259 : loss : 0.170915, loss_ce: 0.076288
2022-01-11 22:19:35,317 iteration 260 : loss : 0.139320, loss_ce: 0.072747
2022-01-11 22:19:36,941 iteration 261 : loss : 0.172181, loss_ce: 0.073733
2022-01-11 22:19:38,587 iteration 262 : loss : 0.294558, loss_ce: 0.144672
2022-01-11 22:19:40,147 iteration 263 : loss : 0.110997, loss_ce: 0.047198
2022-01-11 22:19:41,800 iteration 264 : loss : 0.207184, loss_ce: 0.067522
2022-01-11 22:19:43,508 iteration 265 : loss : 0.215235, loss_ce: 0.127353
2022-01-11 22:19:45,140 iteration 266 : loss : 0.166960, loss_ce: 0.070883
2022-01-11 22:19:46,848 iteration 267 : loss : 0.149104, loss_ce: 0.050449
2022-01-11 22:19:48,481 iteration 268 : loss : 0.183905, loss_ce: 0.082903
2022-01-11 22:19:50,099 iteration 269 : loss : 0.250514, loss_ce: 0.122005
2022-01-11 22:19:51,700 iteration 270 : loss : 0.223427, loss_ce: 0.083347
2022-01-11 22:19:53,281 iteration 271 : loss : 0.211596, loss_ce: 0.098220
2022-01-11 22:19:54,834 iteration 272 : loss : 0.170151, loss_ce: 0.069128
  4%|█▏                            | 16/400 [08:03<3:21:11, 31.44s/it]2022-01-11 22:19:56,538 iteration 273 : loss : 0.167799, loss_ce: 0.072258
2022-01-11 22:19:58,145 iteration 274 : loss : 0.255929, loss_ce: 0.117397
2022-01-11 22:19:59,726 iteration 275 : loss : 0.231256, loss_ce: 0.068356
2022-01-11 22:20:01,301 iteration 276 : loss : 0.115946, loss_ce: 0.051903
2022-01-11 22:20:02,879 iteration 277 : loss : 0.119721, loss_ce: 0.049109
2022-01-11 22:20:04,482 iteration 278 : loss : 0.194032, loss_ce: 0.085192
2022-01-11 22:20:06,067 iteration 279 : loss : 0.207253, loss_ce: 0.091171
2022-01-11 22:20:07,657 iteration 280 : loss : 0.182041, loss_ce: 0.091671
2022-01-11 22:20:09,317 iteration 281 : loss : 0.147752, loss_ce: 0.069772
2022-01-11 22:20:10,944 iteration 282 : loss : 0.102660, loss_ce: 0.041607
2022-01-11 22:20:12,509 iteration 283 : loss : 0.168251, loss_ce: 0.071970
2022-01-11 22:20:14,129 iteration 284 : loss : 0.166599, loss_ce: 0.091070
2022-01-11 22:20:15,779 iteration 285 : loss : 0.165199, loss_ce: 0.058675
2022-01-11 22:20:17,281 iteration 286 : loss : 0.151601, loss_ce: 0.061117
2022-01-11 22:20:18,880 iteration 287 : loss : 0.162498, loss_ce: 0.069533
2022-01-11 22:20:20,490 iteration 288 : loss : 0.144786, loss_ce: 0.061452
2022-01-11 22:20:22,083 iteration 289 : loss : 0.177007, loss_ce: 0.073598
  4%|█▎                            | 17/400 [08:30<3:12:37, 30.18s/it]2022-01-11 22:20:23,755 iteration 290 : loss : 0.182252, loss_ce: 0.080728
2022-01-11 22:20:25,311 iteration 291 : loss : 0.171689, loss_ce: 0.066902
2022-01-11 22:20:26,849 iteration 292 : loss : 0.202505, loss_ce: 0.092884
2022-01-11 22:20:28,492 iteration 293 : loss : 0.152143, loss_ce: 0.061892
2022-01-11 22:20:30,077 iteration 294 : loss : 0.123860, loss_ce: 0.060407
2022-01-11 22:20:31,734 iteration 295 : loss : 0.147163, loss_ce: 0.069496
2022-01-11 22:20:33,391 iteration 296 : loss : 0.225416, loss_ce: 0.105473
2022-01-11 22:20:35,014 iteration 297 : loss : 0.138173, loss_ce: 0.073404
2022-01-11 22:20:36,601 iteration 298 : loss : 0.152305, loss_ce: 0.077399
2022-01-11 22:20:38,134 iteration 299 : loss : 0.148356, loss_ce: 0.059760
2022-01-11 22:20:39,708 iteration 300 : loss : 0.126633, loss_ce: 0.055084
2022-01-11 22:20:41,216 iteration 301 : loss : 0.176938, loss_ce: 0.065986
2022-01-11 22:20:42,871 iteration 302 : loss : 0.140126, loss_ce: 0.054462
2022-01-11 22:20:44,523 iteration 303 : loss : 0.124113, loss_ce: 0.040942
2022-01-11 22:20:46,177 iteration 304 : loss : 0.171121, loss_ce: 0.069383
2022-01-11 22:20:47,755 iteration 305 : loss : 0.119515, loss_ce: 0.047222
2022-01-11 22:20:49,346 iteration 306 : loss : 0.143303, loss_ce: 0.055404
  4%|█▎                            | 18/400 [08:58<3:06:33, 29.30s/it]2022-01-11 22:20:50,916 iteration 307 : loss : 0.195527, loss_ce: 0.076853
2022-01-11 22:20:52,443 iteration 308 : loss : 0.139706, loss_ce: 0.061368
2022-01-11 22:20:53,979 iteration 309 : loss : 0.174632, loss_ce: 0.088274
2022-01-11 22:20:55,599 iteration 310 : loss : 0.181140, loss_ce: 0.072984
2022-01-11 22:20:57,246 iteration 311 : loss : 0.116225, loss_ce: 0.050088
2022-01-11 22:20:58,902 iteration 312 : loss : 0.125677, loss_ce: 0.052332
2022-01-11 22:21:00,477 iteration 313 : loss : 0.169124, loss_ce: 0.070311
2022-01-11 22:21:02,091 iteration 314 : loss : 0.147137, loss_ce: 0.068976
2022-01-11 22:21:03,768 iteration 315 : loss : 0.206372, loss_ce: 0.088862
2022-01-11 22:21:05,377 iteration 316 : loss : 0.180293, loss_ce: 0.073697
2022-01-11 22:21:06,928 iteration 317 : loss : 0.139273, loss_ce: 0.046571
2022-01-11 22:21:08,503 iteration 318 : loss : 0.151644, loss_ce: 0.064774
2022-01-11 22:21:10,170 iteration 319 : loss : 0.126289, loss_ce: 0.052351
2022-01-11 22:21:11,736 iteration 320 : loss : 0.160651, loss_ce: 0.059732
2022-01-11 22:21:13,269 iteration 321 : loss : 0.123184, loss_ce: 0.063590
2022-01-11 22:21:14,880 iteration 322 : loss : 0.139922, loss_ce: 0.062032
2022-01-11 22:21:16,542 iteration 323 : loss : 0.133260, loss_ce: 0.063181
  5%|█▍                            | 19/400 [09:25<3:02:03, 28.67s/it]2022-01-11 22:21:18,104 iteration 324 : loss : 0.165546, loss_ce: 0.069668
2022-01-11 22:21:19,717 iteration 325 : loss : 0.149047, loss_ce: 0.057408
2022-01-11 22:21:21,321 iteration 326 : loss : 0.128153, loss_ce: 0.047151
2022-01-11 22:21:23,004 iteration 327 : loss : 0.163583, loss_ce: 0.065508
2022-01-11 22:21:24,657 iteration 328 : loss : 0.142591, loss_ce: 0.064380
2022-01-11 22:21:26,281 iteration 329 : loss : 0.120907, loss_ce: 0.049648
2022-01-11 22:21:27,790 iteration 330 : loss : 0.114423, loss_ce: 0.052180
2022-01-11 22:21:29,463 iteration 331 : loss : 0.145390, loss_ce: 0.055403
2022-01-11 22:21:31,126 iteration 332 : loss : 0.108063, loss_ce: 0.048965
2022-01-11 22:21:32,668 iteration 333 : loss : 0.123298, loss_ce: 0.049767
2022-01-11 22:21:34,312 iteration 334 : loss : 0.168167, loss_ce: 0.083597
2022-01-11 22:21:35,889 iteration 335 : loss : 0.155000, loss_ce: 0.057638
2022-01-11 22:21:37,530 iteration 336 : loss : 0.101085, loss_ce: 0.039037
2022-01-11 22:21:39,090 iteration 337 : loss : 0.137594, loss_ce: 0.052103
2022-01-11 22:21:40,617 iteration 338 : loss : 0.145039, loss_ce: 0.069784
2022-01-11 22:21:42,213 iteration 339 : loss : 0.123615, loss_ce: 0.042952
2022-01-11 22:21:42,214 Training Data Eval:
2022-01-11 22:21:50,193   Average segmentation loss on training set: 0.1448
2022-01-11 22:21:50,194 Validation Data Eval:
2022-01-11 22:21:52,952   Average segmentation loss on validation set: 0.1933
2022-01-11 22:21:58,921 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:22:00,375 iteration 340 : loss : 0.151578, loss_ce: 0.052233
  5%|█▌                            | 20/400 [10:09<3:30:23, 33.22s/it]2022-01-11 22:22:01,941 iteration 341 : loss : 0.218215, loss_ce: 0.108686
2022-01-11 22:22:03,488 iteration 342 : loss : 0.134184, loss_ce: 0.060090
2022-01-11 22:22:05,096 iteration 343 : loss : 0.173481, loss_ce: 0.089062
2022-01-11 22:22:06,604 iteration 344 : loss : 0.187279, loss_ce: 0.069100
2022-01-11 22:22:08,227 iteration 345 : loss : 0.179923, loss_ce: 0.079754
2022-01-11 22:22:09,907 iteration 346 : loss : 0.140323, loss_ce: 0.051463
2022-01-11 22:22:11,491 iteration 347 : loss : 0.123417, loss_ce: 0.064560
2022-01-11 22:22:13,203 iteration 348 : loss : 0.115452, loss_ce: 0.051229
2022-01-11 22:22:14,795 iteration 349 : loss : 0.144968, loss_ce: 0.069890
2022-01-11 22:22:16,318 iteration 350 : loss : 0.117683, loss_ce: 0.042125
2022-01-11 22:22:17,999 iteration 351 : loss : 0.140702, loss_ce: 0.061059
2022-01-11 22:22:19,590 iteration 352 : loss : 0.151132, loss_ce: 0.058025
2022-01-11 22:22:21,302 iteration 353 : loss : 0.137606, loss_ce: 0.047259
2022-01-11 22:22:22,941 iteration 354 : loss : 0.184467, loss_ce: 0.077700
2022-01-11 22:22:24,631 iteration 355 : loss : 0.177819, loss_ce: 0.069323
2022-01-11 22:22:26,191 iteration 356 : loss : 0.138639, loss_ce: 0.055079
2022-01-11 22:22:27,763 iteration 357 : loss : 0.157240, loss_ce: 0.073358
  5%|█▌                            | 21/400 [10:36<3:18:47, 31.47s/it]2022-01-11 22:22:29,490 iteration 358 : loss : 0.155302, loss_ce: 0.062151
2022-01-11 22:22:31,170 iteration 359 : loss : 0.103701, loss_ce: 0.051094
2022-01-11 22:22:32,784 iteration 360 : loss : 0.239547, loss_ce: 0.076545
2022-01-11 22:22:34,436 iteration 361 : loss : 0.208542, loss_ce: 0.076622
2022-01-11 22:22:35,955 iteration 362 : loss : 0.135574, loss_ce: 0.056341
2022-01-11 22:22:37,634 iteration 363 : loss : 0.155639, loss_ce: 0.083598
2022-01-11 22:22:39,271 iteration 364 : loss : 0.200630, loss_ce: 0.108834
2022-01-11 22:22:40,859 iteration 365 : loss : 0.137291, loss_ce: 0.057439
2022-01-11 22:22:42,465 iteration 366 : loss : 0.127082, loss_ce: 0.053941
2022-01-11 22:22:44,120 iteration 367 : loss : 0.149739, loss_ce: 0.061806
2022-01-11 22:22:45,679 iteration 368 : loss : 0.143910, loss_ce: 0.044507
2022-01-11 22:22:47,304 iteration 369 : loss : 0.145421, loss_ce: 0.071966
2022-01-11 22:22:48,817 iteration 370 : loss : 0.180440, loss_ce: 0.055793
2022-01-11 22:22:50,497 iteration 371 : loss : 0.133252, loss_ce: 0.069765
2022-01-11 22:22:52,168 iteration 372 : loss : 0.164432, loss_ce: 0.067929
2022-01-11 22:22:53,806 iteration 373 : loss : 0.144177, loss_ce: 0.053548
2022-01-11 22:22:55,383 iteration 374 : loss : 0.112306, loss_ce: 0.049898
  6%|█▋                            | 22/400 [11:04<3:10:58, 30.31s/it]2022-01-11 22:22:57,081 iteration 375 : loss : 0.122292, loss_ce: 0.051557
2022-01-11 22:22:58,717 iteration 376 : loss : 0.134639, loss_ce: 0.053603
2022-01-11 22:23:00,357 iteration 377 : loss : 0.157043, loss_ce: 0.053645
2022-01-11 22:23:02,004 iteration 378 : loss : 0.159670, loss_ce: 0.070708
2022-01-11 22:23:03,555 iteration 379 : loss : 0.181629, loss_ce: 0.068106
2022-01-11 22:23:05,197 iteration 380 : loss : 0.123001, loss_ce: 0.055109
2022-01-11 22:23:06,707 iteration 381 : loss : 0.100584, loss_ce: 0.037084
2022-01-11 22:23:08,433 iteration 382 : loss : 0.140214, loss_ce: 0.063339
2022-01-11 22:23:10,096 iteration 383 : loss : 0.144414, loss_ce: 0.063985
2022-01-11 22:23:11,707 iteration 384 : loss : 0.115910, loss_ce: 0.049457
2022-01-11 22:23:13,270 iteration 385 : loss : 0.118646, loss_ce: 0.046861
2022-01-11 22:23:14,920 iteration 386 : loss : 0.156664, loss_ce: 0.074207
2022-01-11 22:23:16,552 iteration 387 : loss : 0.135092, loss_ce: 0.054676
2022-01-11 22:23:18,213 iteration 388 : loss : 0.103228, loss_ce: 0.051104
2022-01-11 22:23:19,790 iteration 389 : loss : 0.173479, loss_ce: 0.067776
2022-01-11 22:23:21,436 iteration 390 : loss : 0.133547, loss_ce: 0.068109
2022-01-11 22:23:23,015 iteration 391 : loss : 0.158332, loss_ce: 0.058138
  6%|█▋                            | 23/400 [11:31<3:05:24, 29.51s/it]2022-01-11 22:23:24,796 iteration 392 : loss : 0.098303, loss_ce: 0.042216
2022-01-11 22:23:26,367 iteration 393 : loss : 0.135834, loss_ce: 0.069855
2022-01-11 22:23:28,000 iteration 394 : loss : 0.157800, loss_ce: 0.063391
2022-01-11 22:23:29,584 iteration 395 : loss : 0.118193, loss_ce: 0.057379
2022-01-11 22:23:31,260 iteration 396 : loss : 0.119921, loss_ce: 0.049594
2022-01-11 22:23:32,903 iteration 397 : loss : 0.131720, loss_ce: 0.059072
2022-01-11 22:23:34,547 iteration 398 : loss : 0.150527, loss_ce: 0.070497
2022-01-11 22:23:36,147 iteration 399 : loss : 0.100191, loss_ce: 0.037849
2022-01-11 22:23:37,824 iteration 400 : loss : 0.106609, loss_ce: 0.051601
2022-01-11 22:23:39,358 iteration 401 : loss : 0.160434, loss_ce: 0.053931
2022-01-11 22:23:41,016 iteration 402 : loss : 0.181159, loss_ce: 0.077066
2022-01-11 22:23:42,736 iteration 403 : loss : 0.111033, loss_ce: 0.046131
2022-01-11 22:23:44,346 iteration 404 : loss : 0.098143, loss_ce: 0.030313
2022-01-11 22:23:46,013 iteration 405 : loss : 0.155082, loss_ce: 0.056981
2022-01-11 22:23:47,581 iteration 406 : loss : 0.153286, loss_ce: 0.060431
2022-01-11 22:23:49,148 iteration 407 : loss : 0.121157, loss_ce: 0.042294
2022-01-11 22:23:50,712 iteration 408 : loss : 0.109104, loss_ce: 0.038010
  6%|█▊                            | 24/400 [11:59<3:01:30, 28.96s/it]2022-01-11 22:23:52,522 iteration 409 : loss : 0.088311, loss_ce: 0.035202
2022-01-11 22:23:54,134 iteration 410 : loss : 0.175616, loss_ce: 0.071168
2022-01-11 22:23:55,784 iteration 411 : loss : 0.135062, loss_ce: 0.052592
2022-01-11 22:23:57,385 iteration 412 : loss : 0.129943, loss_ce: 0.053241
2022-01-11 22:23:58,984 iteration 413 : loss : 0.144968, loss_ce: 0.061124
2022-01-11 22:24:00,612 iteration 414 : loss : 0.118174, loss_ce: 0.056443
2022-01-11 22:24:02,282 iteration 415 : loss : 0.144562, loss_ce: 0.076612
2022-01-11 22:24:03,758 iteration 416 : loss : 0.131149, loss_ce: 0.048613
2022-01-11 22:24:05,347 iteration 417 : loss : 0.179632, loss_ce: 0.076011
2022-01-11 22:24:06,959 iteration 418 : loss : 0.102009, loss_ce: 0.037563
2022-01-11 22:24:08,532 iteration 419 : loss : 0.226214, loss_ce: 0.084238
2022-01-11 22:24:10,139 iteration 420 : loss : 0.116143, loss_ce: 0.051654
2022-01-11 22:24:11,659 iteration 421 : loss : 0.122587, loss_ce: 0.032433
2022-01-11 22:24:13,303 iteration 422 : loss : 0.126997, loss_ce: 0.045465
2022-01-11 22:24:14,875 iteration 423 : loss : 0.130263, loss_ce: 0.052127
2022-01-11 22:24:16,443 iteration 424 : loss : 0.134841, loss_ce: 0.066043
2022-01-11 22:24:16,443 Training Data Eval:
2022-01-11 22:24:24,419   Average segmentation loss on training set: 0.2378
2022-01-11 22:24:24,420 Validation Data Eval:
2022-01-11 22:24:27,162   Average segmentation loss on validation set: 0.3110
2022-01-11 22:24:28,819 iteration 425 : loss : 0.167469, loss_ce: 0.078369
  6%|█▉                            | 25/400 [12:37<3:18:10, 31.71s/it]2022-01-11 22:24:30,509 iteration 426 : loss : 0.125726, loss_ce: 0.052804
2022-01-11 22:24:32,074 iteration 427 : loss : 0.147321, loss_ce: 0.062632
2022-01-11 22:24:33,793 iteration 428 : loss : 0.107836, loss_ce: 0.048834
2022-01-11 22:24:35,383 iteration 429 : loss : 0.118919, loss_ce: 0.044410
2022-01-11 22:24:36,920 iteration 430 : loss : 0.115174, loss_ce: 0.054658
2022-01-11 22:24:38,576 iteration 431 : loss : 0.110542, loss_ce: 0.059923
2022-01-11 22:24:40,216 iteration 432 : loss : 0.126473, loss_ce: 0.054250
2022-01-11 22:24:41,740 iteration 433 : loss : 0.127488, loss_ce: 0.057768
2022-01-11 22:24:43,299 iteration 434 : loss : 0.093896, loss_ce: 0.033453
2022-01-11 22:24:44,917 iteration 435 : loss : 0.109591, loss_ce: 0.043134
2022-01-11 22:24:46,564 iteration 436 : loss : 0.159442, loss_ce: 0.053796
2022-01-11 22:24:48,135 iteration 437 : loss : 0.121557, loss_ce: 0.050644
2022-01-11 22:24:49,700 iteration 438 : loss : 0.129598, loss_ce: 0.049720
2022-01-11 22:24:51,278 iteration 439 : loss : 0.144754, loss_ce: 0.061151
2022-01-11 22:24:53,018 iteration 440 : loss : 0.154089, loss_ce: 0.063794
2022-01-11 22:24:54,610 iteration 441 : loss : 0.116347, loss_ce: 0.050417
2022-01-11 22:24:56,172 iteration 442 : loss : 0.094433, loss_ce: 0.041325
  6%|█▉                            | 26/400 [13:04<3:09:30, 30.40s/it]2022-01-11 22:24:57,867 iteration 443 : loss : 0.086022, loss_ce: 0.037829
2022-01-11 22:24:59,425 iteration 444 : loss : 0.082339, loss_ce: 0.028947
2022-01-11 22:25:00,965 iteration 445 : loss : 0.096914, loss_ce: 0.039877
2022-01-11 22:25:02,547 iteration 446 : loss : 0.113580, loss_ce: 0.041015
2022-01-11 22:25:04,196 iteration 447 : loss : 0.093845, loss_ce: 0.038818
2022-01-11 22:25:05,710 iteration 448 : loss : 0.133553, loss_ce: 0.046534
2022-01-11 22:25:07,273 iteration 449 : loss : 0.114806, loss_ce: 0.039700
2022-01-11 22:25:08,792 iteration 450 : loss : 0.131865, loss_ce: 0.067711
2022-01-11 22:25:10,369 iteration 451 : loss : 0.153951, loss_ce: 0.067717
2022-01-11 22:25:11,981 iteration 452 : loss : 0.118146, loss_ce: 0.040701
2022-01-11 22:25:13,537 iteration 453 : loss : 0.072894, loss_ce: 0.027973
2022-01-11 22:25:15,241 iteration 454 : loss : 0.117759, loss_ce: 0.052146
2022-01-11 22:25:16,877 iteration 455 : loss : 0.117717, loss_ce: 0.050224
2022-01-11 22:25:18,468 iteration 456 : loss : 0.095149, loss_ce: 0.038340
2022-01-11 22:25:20,050 iteration 457 : loss : 0.102211, loss_ce: 0.050723
2022-01-11 22:25:21,629 iteration 458 : loss : 0.092373, loss_ce: 0.050205
2022-01-11 22:25:23,233 iteration 459 : loss : 0.128493, loss_ce: 0.061937
  7%|██                            | 27/400 [13:31<3:02:45, 29.40s/it]2022-01-11 22:25:24,809 iteration 460 : loss : 0.113184, loss_ce: 0.058983
2022-01-11 22:25:26,379 iteration 461 : loss : 0.096069, loss_ce: 0.046776
2022-01-11 22:25:28,060 iteration 462 : loss : 0.152052, loss_ce: 0.060309
2022-01-11 22:25:29,678 iteration 463 : loss : 0.155235, loss_ce: 0.062669
2022-01-11 22:25:31,428 iteration 464 : loss : 0.174422, loss_ce: 0.069586
2022-01-11 22:25:33,084 iteration 465 : loss : 0.113917, loss_ce: 0.049518
2022-01-11 22:25:34,653 iteration 466 : loss : 0.091860, loss_ce: 0.040049
2022-01-11 22:25:36,249 iteration 467 : loss : 0.139851, loss_ce: 0.060381
2022-01-11 22:25:37,832 iteration 468 : loss : 0.081659, loss_ce: 0.031842
2022-01-11 22:25:39,482 iteration 469 : loss : 0.110407, loss_ce: 0.047418
2022-01-11 22:25:41,093 iteration 470 : loss : 0.097873, loss_ce: 0.037543
2022-01-11 22:25:42,692 iteration 471 : loss : 0.196907, loss_ce: 0.112120
2022-01-11 22:25:44,322 iteration 472 : loss : 0.069839, loss_ce: 0.030851
2022-01-11 22:25:45,942 iteration 473 : loss : 0.106225, loss_ce: 0.051658
2022-01-11 22:25:47,505 iteration 474 : loss : 0.188567, loss_ce: 0.075071
2022-01-11 22:25:49,093 iteration 475 : loss : 0.148061, loss_ce: 0.053102
2022-01-11 22:25:50,762 iteration 476 : loss : 0.088060, loss_ce: 0.040671
  7%|██                            | 28/400 [13:59<2:58:47, 28.84s/it]2022-01-11 22:25:52,408 iteration 477 : loss : 0.084556, loss_ce: 0.037061
2022-01-11 22:25:53,940 iteration 478 : loss : 0.089960, loss_ce: 0.044029
2022-01-11 22:25:55,572 iteration 479 : loss : 0.101468, loss_ce: 0.037052
2022-01-11 22:25:57,178 iteration 480 : loss : 0.129751, loss_ce: 0.044600
2022-01-11 22:25:58,728 iteration 481 : loss : 0.074555, loss_ce: 0.023476
2022-01-11 22:26:00,412 iteration 482 : loss : 0.096241, loss_ce: 0.040042
2022-01-11 22:26:01,941 iteration 483 : loss : 0.125462, loss_ce: 0.066265
2022-01-11 22:26:03,562 iteration 484 : loss : 0.096810, loss_ce: 0.036143
2022-01-11 22:26:05,173 iteration 485 : loss : 0.142376, loss_ce: 0.063145
2022-01-11 22:26:06,895 iteration 486 : loss : 0.118235, loss_ce: 0.050701
2022-01-11 22:26:08,499 iteration 487 : loss : 0.140976, loss_ce: 0.052551
2022-01-11 22:26:10,025 iteration 488 : loss : 0.111901, loss_ce: 0.057411
2022-01-11 22:26:11,658 iteration 489 : loss : 0.111841, loss_ce: 0.058113
2022-01-11 22:26:13,248 iteration 490 : loss : 0.139485, loss_ce: 0.074968
2022-01-11 22:26:14,914 iteration 491 : loss : 0.149298, loss_ce: 0.051787
2022-01-11 22:26:16,480 iteration 492 : loss : 0.121187, loss_ce: 0.074085
2022-01-11 22:26:18,029 iteration 493 : loss : 0.090176, loss_ce: 0.044941
  7%|██▏                           | 29/400 [14:26<2:55:24, 28.37s/it]2022-01-11 22:26:19,668 iteration 494 : loss : 0.164599, loss_ce: 0.076219
2022-01-11 22:26:21,195 iteration 495 : loss : 0.125398, loss_ce: 0.049480
2022-01-11 22:26:22,806 iteration 496 : loss : 0.107094, loss_ce: 0.042412
2022-01-11 22:26:24,504 iteration 497 : loss : 0.122222, loss_ce: 0.056357
2022-01-11 22:26:26,229 iteration 498 : loss : 0.144354, loss_ce: 0.087651
2022-01-11 22:26:27,863 iteration 499 : loss : 0.121662, loss_ce: 0.043261
2022-01-11 22:26:29,602 iteration 500 : loss : 0.187203, loss_ce: 0.063031
2022-01-11 22:26:31,207 iteration 501 : loss : 0.110262, loss_ce: 0.040743
2022-01-11 22:26:32,745 iteration 502 : loss : 0.099563, loss_ce: 0.045424
2022-01-11 22:26:34,464 iteration 503 : loss : 0.147093, loss_ce: 0.040821
2022-01-11 22:26:36,197 iteration 504 : loss : 0.109524, loss_ce: 0.043015
2022-01-11 22:26:37,774 iteration 505 : loss : 0.107355, loss_ce: 0.044866
2022-01-11 22:26:39,410 iteration 506 : loss : 0.117010, loss_ce: 0.048268
2022-01-11 22:26:41,075 iteration 507 : loss : 0.086367, loss_ce: 0.032555
2022-01-11 22:26:42,667 iteration 508 : loss : 0.085711, loss_ce: 0.036800
2022-01-11 22:26:44,297 iteration 509 : loss : 0.096237, loss_ce: 0.044928
2022-01-11 22:26:44,298 Training Data Eval:
2022-01-11 22:26:52,261   Average segmentation loss on training set: 0.0805
2022-01-11 22:26:52,262 Validation Data Eval:
2022-01-11 22:26:55,003   Average segmentation loss on validation set: 0.1062
2022-01-11 22:27:00,984 Found new lowest validation loss at iteration 509! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:27:02,364 iteration 510 : loss : 0.118116, loss_ce: 0.045451
  8%|██▎                           | 30/400 [15:11<3:24:27, 33.16s/it]2022-01-11 22:27:03,722 iteration 511 : loss : 0.081170, loss_ce: 0.035766
2022-01-11 22:27:05,358 iteration 512 : loss : 0.141014, loss_ce: 0.061212
2022-01-11 22:27:06,789 iteration 513 : loss : 0.136925, loss_ce: 0.070274
2022-01-11 22:27:08,387 iteration 514 : loss : 0.119376, loss_ce: 0.047787
2022-01-11 22:27:09,960 iteration 515 : loss : 0.077718, loss_ce: 0.035081
2022-01-11 22:27:11,472 iteration 516 : loss : 0.066098, loss_ce: 0.033626
2022-01-11 22:27:13,220 iteration 517 : loss : 0.090280, loss_ce: 0.034104
2022-01-11 22:27:14,860 iteration 518 : loss : 0.128499, loss_ce: 0.071222
2022-01-11 22:27:16,422 iteration 519 : loss : 0.091183, loss_ce: 0.038251
2022-01-11 22:27:17,974 iteration 520 : loss : 0.123854, loss_ce: 0.065303
2022-01-11 22:27:19,673 iteration 521 : loss : 0.204452, loss_ce: 0.054189
2022-01-11 22:27:21,243 iteration 522 : loss : 0.079693, loss_ce: 0.032690
2022-01-11 22:27:22,877 iteration 523 : loss : 0.076288, loss_ce: 0.032493
2022-01-11 22:27:24,358 iteration 524 : loss : 0.086259, loss_ce: 0.039858
2022-01-11 22:27:25,931 iteration 525 : loss : 0.101409, loss_ce: 0.040226
2022-01-11 22:27:27,550 iteration 526 : loss : 0.093408, loss_ce: 0.038033
2022-01-11 22:27:29,135 iteration 527 : loss : 0.108247, loss_ce: 0.044917
  8%|██▎                           | 31/400 [15:37<3:12:08, 31.24s/it]2022-01-11 22:27:30,824 iteration 528 : loss : 0.079351, loss_ce: 0.032367
2022-01-11 22:27:32,412 iteration 529 : loss : 0.096077, loss_ce: 0.044256
2022-01-11 22:27:33,972 iteration 530 : loss : 0.069659, loss_ce: 0.027601
2022-01-11 22:27:35,633 iteration 531 : loss : 0.171122, loss_ce: 0.068364
2022-01-11 22:27:37,147 iteration 532 : loss : 0.114669, loss_ce: 0.034802
2022-01-11 22:27:38,765 iteration 533 : loss : 0.073471, loss_ce: 0.033809
2022-01-11 22:27:40,377 iteration 534 : loss : 0.108787, loss_ce: 0.058157
2022-01-11 22:27:41,980 iteration 535 : loss : 0.095528, loss_ce: 0.038006
2022-01-11 22:27:43,509 iteration 536 : loss : 0.126114, loss_ce: 0.064226
2022-01-11 22:27:45,115 iteration 537 : loss : 0.115356, loss_ce: 0.047541
2022-01-11 22:27:46,687 iteration 538 : loss : 0.066064, loss_ce: 0.026844
2022-01-11 22:27:48,236 iteration 539 : loss : 0.121542, loss_ce: 0.045795
2022-01-11 22:27:49,781 iteration 540 : loss : 0.083819, loss_ce: 0.037899
2022-01-11 22:27:51,365 iteration 541 : loss : 0.096885, loss_ce: 0.049698
2022-01-11 22:27:52,881 iteration 542 : loss : 0.091692, loss_ce: 0.035492
2022-01-11 22:27:54,434 iteration 543 : loss : 0.144103, loss_ce: 0.051031
2022-01-11 22:27:56,014 iteration 544 : loss : 0.091617, loss_ce: 0.032711
  8%|██▍                           | 32/400 [16:04<3:03:35, 29.93s/it]2022-01-11 22:27:57,633 iteration 545 : loss : 0.144438, loss_ce: 0.063405
2022-01-11 22:27:59,236 iteration 546 : loss : 0.082172, loss_ce: 0.038964
2022-01-11 22:28:00,825 iteration 547 : loss : 0.117122, loss_ce: 0.053578
2022-01-11 22:28:02,351 iteration 548 : loss : 0.112547, loss_ce: 0.048636
2022-01-11 22:28:03,966 iteration 549 : loss : 0.107075, loss_ce: 0.048124
2022-01-11 22:28:05,666 iteration 550 : loss : 0.134028, loss_ce: 0.069324
2022-01-11 22:28:07,303 iteration 551 : loss : 0.125912, loss_ce: 0.054665
2022-01-11 22:28:08,953 iteration 552 : loss : 0.096085, loss_ce: 0.040915
2022-01-11 22:28:10,616 iteration 553 : loss : 0.115172, loss_ce: 0.056721
2022-01-11 22:28:12,145 iteration 554 : loss : 0.066243, loss_ce: 0.026608
2022-01-11 22:28:13,772 iteration 555 : loss : 0.107690, loss_ce: 0.052665
2022-01-11 22:28:15,475 iteration 556 : loss : 0.086516, loss_ce: 0.032014
2022-01-11 22:28:17,072 iteration 557 : loss : 0.129653, loss_ce: 0.065287
2022-01-11 22:28:18,633 iteration 558 : loss : 0.102683, loss_ce: 0.039322
2022-01-11 22:28:20,250 iteration 559 : loss : 0.112556, loss_ce: 0.051393
2022-01-11 22:28:21,888 iteration 560 : loss : 0.162578, loss_ce: 0.072082
2022-01-11 22:28:23,440 iteration 561 : loss : 0.128386, loss_ce: 0.037871
  8%|██▍                           | 33/400 [16:32<2:58:29, 29.18s/it]2022-01-11 22:28:25,064 iteration 562 : loss : 0.091268, loss_ce: 0.037183
2022-01-11 22:28:26,701 iteration 563 : loss : 0.102452, loss_ce: 0.046007
2022-01-11 22:28:28,358 iteration 564 : loss : 0.089994, loss_ce: 0.047849
2022-01-11 22:28:29,985 iteration 565 : loss : 0.130352, loss_ce: 0.066803
2022-01-11 22:28:31,514 iteration 566 : loss : 0.080607, loss_ce: 0.030171
2022-01-11 22:28:33,169 iteration 567 : loss : 0.103316, loss_ce: 0.037289
2022-01-11 22:28:34,735 iteration 568 : loss : 0.082020, loss_ce: 0.033047
2022-01-11 22:28:36,328 iteration 569 : loss : 0.076111, loss_ce: 0.026265
2022-01-11 22:28:38,012 iteration 570 : loss : 0.093501, loss_ce: 0.039573
2022-01-11 22:28:39,666 iteration 571 : loss : 0.081601, loss_ce: 0.033976
2022-01-11 22:28:41,311 iteration 572 : loss : 0.092212, loss_ce: 0.032570
2022-01-11 22:28:42,883 iteration 573 : loss : 0.102571, loss_ce: 0.037749
2022-01-11 22:28:44,461 iteration 574 : loss : 0.094888, loss_ce: 0.042686
2022-01-11 22:28:46,114 iteration 575 : loss : 0.124779, loss_ce: 0.052280
2022-01-11 22:28:47,720 iteration 576 : loss : 0.100485, loss_ce: 0.049917
2022-01-11 22:28:49,347 iteration 577 : loss : 0.176165, loss_ce: 0.055274
2022-01-11 22:28:50,986 iteration 578 : loss : 0.102840, loss_ce: 0.039204
  8%|██▌                           | 34/400 [16:59<2:55:00, 28.69s/it]2022-01-11 22:28:52,616 iteration 579 : loss : 0.073305, loss_ce: 0.023848
2022-01-11 22:28:54,204 iteration 580 : loss : 0.083245, loss_ce: 0.035030
2022-01-11 22:28:55,826 iteration 581 : loss : 0.071906, loss_ce: 0.033254
2022-01-11 22:28:57,451 iteration 582 : loss : 0.064933, loss_ce: 0.023428
2022-01-11 22:28:59,031 iteration 583 : loss : 0.131642, loss_ce: 0.063625
2022-01-11 22:29:00,516 iteration 584 : loss : 0.067029, loss_ce: 0.029283
2022-01-11 22:29:02,063 iteration 585 : loss : 0.077420, loss_ce: 0.029649
2022-01-11 22:29:03,693 iteration 586 : loss : 0.090845, loss_ce: 0.042112
2022-01-11 22:29:05,287 iteration 587 : loss : 0.104114, loss_ce: 0.036635
2022-01-11 22:29:06,917 iteration 588 : loss : 0.108280, loss_ce: 0.038766
2022-01-11 22:29:08,516 iteration 589 : loss : 0.077475, loss_ce: 0.032123
2022-01-11 22:29:10,078 iteration 590 : loss : 0.131100, loss_ce: 0.083549
2022-01-11 22:29:11,654 iteration 591 : loss : 0.084273, loss_ce: 0.034782
2022-01-11 22:29:13,220 iteration 592 : loss : 0.087363, loss_ce: 0.032240
2022-01-11 22:29:14,872 iteration 593 : loss : 0.090821, loss_ce: 0.035363
2022-01-11 22:29:16,523 iteration 594 : loss : 0.082276, loss_ce: 0.038796
2022-01-11 22:29:16,523 Training Data Eval:
2022-01-11 22:29:24,486   Average segmentation loss on training set: 0.0920
2022-01-11 22:29:24,486 Validation Data Eval:
2022-01-11 22:29:27,229   Average segmentation loss on validation set: 0.2014
2022-01-11 22:29:28,860 iteration 595 : loss : 0.072628, loss_ce: 0.026952
  9%|██▋                           | 35/400 [17:37<3:11:18, 31.45s/it]2022-01-11 22:29:30,444 iteration 596 : loss : 0.054962, loss_ce: 0.024979
2022-01-11 22:29:32,053 iteration 597 : loss : 0.119950, loss_ce: 0.054695
2022-01-11 22:29:33,686 iteration 598 : loss : 0.081005, loss_ce: 0.034248
2022-01-11 22:29:35,348 iteration 599 : loss : 0.066555, loss_ce: 0.026957
2022-01-11 22:29:36,859 iteration 600 : loss : 0.091153, loss_ce: 0.046150
2022-01-11 22:29:38,469 iteration 601 : loss : 0.075925, loss_ce: 0.029398
2022-01-11 22:29:40,116 iteration 602 : loss : 0.069413, loss_ce: 0.030983
2022-01-11 22:29:41,622 iteration 603 : loss : 0.101419, loss_ce: 0.036998
2022-01-11 22:29:43,194 iteration 604 : loss : 0.089451, loss_ce: 0.040817
2022-01-11 22:29:44,793 iteration 605 : loss : 0.078923, loss_ce: 0.034990
2022-01-11 22:29:46,354 iteration 606 : loss : 0.092598, loss_ce: 0.034389
2022-01-11 22:29:47,944 iteration 607 : loss : 0.088436, loss_ce: 0.035188
2022-01-11 22:29:49,661 iteration 608 : loss : 0.087302, loss_ce: 0.030173
2022-01-11 22:29:51,243 iteration 609 : loss : 0.072619, loss_ce: 0.029877
2022-01-11 22:29:52,894 iteration 610 : loss : 0.112540, loss_ce: 0.043541
2022-01-11 22:29:54,489 iteration 611 : loss : 0.110916, loss_ce: 0.041865
2022-01-11 22:29:56,074 iteration 612 : loss : 0.086827, loss_ce: 0.038484
  9%|██▋                           | 36/400 [18:04<3:03:03, 30.17s/it]2022-01-11 22:29:57,731 iteration 613 : loss : 0.126718, loss_ce: 0.065521
2022-01-11 22:29:59,443 iteration 614 : loss : 0.086257, loss_ce: 0.043102
2022-01-11 22:30:01,045 iteration 615 : loss : 0.077063, loss_ce: 0.029894
2022-01-11 22:30:02,590 iteration 616 : loss : 0.117160, loss_ce: 0.081580
2022-01-11 22:30:04,190 iteration 617 : loss : 0.123875, loss_ce: 0.058451
2022-01-11 22:30:05,741 iteration 618 : loss : 0.151635, loss_ce: 0.073487
2022-01-11 22:30:07,312 iteration 619 : loss : 0.107640, loss_ce: 0.037750
2022-01-11 22:30:09,000 iteration 620 : loss : 0.172308, loss_ce: 0.076921
2022-01-11 22:30:10,564 iteration 621 : loss : 0.098839, loss_ce: 0.038843
2022-01-11 22:30:12,171 iteration 622 : loss : 0.091622, loss_ce: 0.042546
2022-01-11 22:30:13,843 iteration 623 : loss : 0.103747, loss_ce: 0.038196
2022-01-11 22:30:15,421 iteration 624 : loss : 0.112978, loss_ce: 0.045598
2022-01-11 22:30:16,948 iteration 625 : loss : 0.146814, loss_ce: 0.053560
2022-01-11 22:30:18,492 iteration 626 : loss : 0.079891, loss_ce: 0.030020
2022-01-11 22:30:20,053 iteration 627 : loss : 0.103457, loss_ce: 0.034284
2022-01-11 22:30:21,629 iteration 628 : loss : 0.125914, loss_ce: 0.062156
2022-01-11 22:30:23,239 iteration 629 : loss : 0.095110, loss_ce: 0.032314
  9%|██▊                           | 37/400 [18:31<2:57:06, 29.27s/it]2022-01-11 22:30:24,858 iteration 630 : loss : 0.081282, loss_ce: 0.032069
2022-01-11 22:30:26,496 iteration 631 : loss : 0.104199, loss_ce: 0.055727
2022-01-11 22:30:28,098 iteration 632 : loss : 0.115831, loss_ce: 0.045616
2022-01-11 22:30:29,746 iteration 633 : loss : 0.074136, loss_ce: 0.030495
2022-01-11 22:30:31,413 iteration 634 : loss : 0.085416, loss_ce: 0.040192
2022-01-11 22:30:32,999 iteration 635 : loss : 0.145101, loss_ce: 0.055551
2022-01-11 22:30:34,689 iteration 636 : loss : 0.088203, loss_ce: 0.041718
2022-01-11 22:30:36,255 iteration 637 : loss : 0.101169, loss_ce: 0.036582
2022-01-11 22:30:37,836 iteration 638 : loss : 0.159453, loss_ce: 0.083366
2022-01-11 22:30:39,468 iteration 639 : loss : 0.081826, loss_ce: 0.034962
2022-01-11 22:30:41,041 iteration 640 : loss : 0.087461, loss_ce: 0.034181
2022-01-11 22:30:42,667 iteration 641 : loss : 0.076595, loss_ce: 0.028787
2022-01-11 22:30:44,284 iteration 642 : loss : 0.067342, loss_ce: 0.023871
2022-01-11 22:30:45,884 iteration 643 : loss : 0.080782, loss_ce: 0.030801
2022-01-11 22:30:47,473 iteration 644 : loss : 0.068784, loss_ce: 0.029716
2022-01-11 22:30:49,106 iteration 645 : loss : 0.101095, loss_ce: 0.039782
2022-01-11 22:30:50,700 iteration 646 : loss : 0.086213, loss_ce: 0.034564
 10%|██▊                           | 38/400 [18:59<2:53:19, 28.73s/it]2022-01-11 22:30:52,315 iteration 647 : loss : 0.081328, loss_ce: 0.024484
2022-01-11 22:30:53,862 iteration 648 : loss : 0.097853, loss_ce: 0.048434
2022-01-11 22:30:55,404 iteration 649 : loss : 0.071062, loss_ce: 0.029983
2022-01-11 22:30:57,046 iteration 650 : loss : 0.137273, loss_ce: 0.043089
2022-01-11 22:30:58,677 iteration 651 : loss : 0.070748, loss_ce: 0.024913
2022-01-11 22:31:00,230 iteration 652 : loss : 0.079521, loss_ce: 0.035932
2022-01-11 22:31:01,811 iteration 653 : loss : 0.066938, loss_ce: 0.031616
2022-01-11 22:31:03,409 iteration 654 : loss : 0.093608, loss_ce: 0.035657
2022-01-11 22:31:04,931 iteration 655 : loss : 0.095315, loss_ce: 0.039674
2022-01-11 22:31:06,574 iteration 656 : loss : 0.083030, loss_ce: 0.034768
2022-01-11 22:31:08,158 iteration 657 : loss : 0.091589, loss_ce: 0.038969
2022-01-11 22:31:09,754 iteration 658 : loss : 0.101621, loss_ce: 0.040612
2022-01-11 22:31:11,400 iteration 659 : loss : 0.104608, loss_ce: 0.040626
2022-01-11 22:31:13,013 iteration 660 : loss : 0.089589, loss_ce: 0.037453
2022-01-11 22:31:14,636 iteration 661 : loss : 0.068181, loss_ce: 0.032273
2022-01-11 22:31:16,271 iteration 662 : loss : 0.106127, loss_ce: 0.044549
2022-01-11 22:31:17,867 iteration 663 : loss : 0.131640, loss_ce: 0.065001
 10%|██▉                           | 39/400 [19:26<2:50:02, 28.26s/it]2022-01-11 22:31:19,471 iteration 664 : loss : 0.113802, loss_ce: 0.064385
2022-01-11 22:31:21,119 iteration 665 : loss : 0.087622, loss_ce: 0.030662
2022-01-11 22:31:22,742 iteration 666 : loss : 0.079234, loss_ce: 0.024092
2022-01-11 22:31:24,402 iteration 667 : loss : 0.102115, loss_ce: 0.045955
2022-01-11 22:31:25,992 iteration 668 : loss : 0.050782, loss_ce: 0.020580
2022-01-11 22:31:27,661 iteration 669 : loss : 0.118339, loss_ce: 0.054245
2022-01-11 22:31:29,305 iteration 670 : loss : 0.114380, loss_ce: 0.049090
2022-01-11 22:31:30,934 iteration 671 : loss : 0.094936, loss_ce: 0.039049
2022-01-11 22:31:32,447 iteration 672 : loss : 0.081052, loss_ce: 0.035706
2022-01-11 22:31:34,022 iteration 673 : loss : 0.110088, loss_ce: 0.045639
2022-01-11 22:31:35,781 iteration 674 : loss : 0.074801, loss_ce: 0.037250
2022-01-11 22:31:37,395 iteration 675 : loss : 0.097732, loss_ce: 0.036748
2022-01-11 22:31:38,987 iteration 676 : loss : 0.094400, loss_ce: 0.038660
2022-01-11 22:31:40,709 iteration 677 : loss : 0.066801, loss_ce: 0.030651
2022-01-11 22:31:42,342 iteration 678 : loss : 0.090601, loss_ce: 0.027264
2022-01-11 22:31:43,899 iteration 679 : loss : 0.081296, loss_ce: 0.034808
2022-01-11 22:31:43,899 Training Data Eval:
2022-01-11 22:31:51,877   Average segmentation loss on training set: 0.0638
2022-01-11 22:31:51,877 Validation Data Eval:
2022-01-11 22:31:54,629   Average segmentation loss on validation set: 0.0998
2022-01-11 22:32:00,720 Found new lowest validation loss at iteration 679! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:32:02,154 iteration 680 : loss : 0.080419, loss_ce: 0.038262
 10%|███                           | 40/400 [20:10<3:18:25, 33.07s/it]2022-01-11 22:32:03,799 iteration 681 : loss : 0.078629, loss_ce: 0.030052
2022-01-11 22:32:05,267 iteration 682 : loss : 0.076575, loss_ce: 0.029698
2022-01-11 22:32:06,707 iteration 683 : loss : 0.086535, loss_ce: 0.028000
2022-01-11 22:32:08,276 iteration 684 : loss : 0.070108, loss_ce: 0.029218
2022-01-11 22:32:09,936 iteration 685 : loss : 0.077117, loss_ce: 0.029746
2022-01-11 22:32:11,591 iteration 686 : loss : 0.076278, loss_ce: 0.031283
2022-01-11 22:32:13,249 iteration 687 : loss : 0.085158, loss_ce: 0.037855
2022-01-11 22:32:14,928 iteration 688 : loss : 0.090274, loss_ce: 0.035768
2022-01-11 22:32:16,581 iteration 689 : loss : 0.071566, loss_ce: 0.036820
2022-01-11 22:32:18,184 iteration 690 : loss : 0.111722, loss_ce: 0.047676
2022-01-11 22:32:19,830 iteration 691 : loss : 0.089483, loss_ce: 0.031416
2022-01-11 22:32:21,367 iteration 692 : loss : 0.058518, loss_ce: 0.025085
2022-01-11 22:32:22,942 iteration 693 : loss : 0.058931, loss_ce: 0.024397
2022-01-11 22:32:24,548 iteration 694 : loss : 0.092863, loss_ce: 0.029689
2022-01-11 22:32:26,153 iteration 695 : loss : 0.112977, loss_ce: 0.040002
2022-01-11 22:32:27,718 iteration 696 : loss : 0.177275, loss_ce: 0.045750
2022-01-11 22:32:29,220 iteration 697 : loss : 0.072713, loss_ce: 0.029029
 10%|███                           | 41/400 [20:37<3:07:05, 31.27s/it]2022-01-11 22:32:30,873 iteration 698 : loss : 0.088046, loss_ce: 0.042496
2022-01-11 22:32:32,500 iteration 699 : loss : 0.062146, loss_ce: 0.025689
2022-01-11 22:32:34,061 iteration 700 : loss : 0.106697, loss_ce: 0.043003
2022-01-11 22:32:35,626 iteration 701 : loss : 0.110266, loss_ce: 0.040377
2022-01-11 22:32:37,274 iteration 702 : loss : 0.082488, loss_ce: 0.035938
2022-01-11 22:32:38,876 iteration 703 : loss : 0.109983, loss_ce: 0.033656
2022-01-11 22:32:40,564 iteration 704 : loss : 0.099869, loss_ce: 0.035965
2022-01-11 22:32:42,217 iteration 705 : loss : 0.077681, loss_ce: 0.039786
2022-01-11 22:32:43,892 iteration 706 : loss : 0.117977, loss_ce: 0.038570
2022-01-11 22:32:45,485 iteration 707 : loss : 0.083376, loss_ce: 0.037826
2022-01-11 22:32:47,102 iteration 708 : loss : 0.075297, loss_ce: 0.028387
2022-01-11 22:32:48,727 iteration 709 : loss : 0.091067, loss_ce: 0.032998
2022-01-11 22:32:50,284 iteration 710 : loss : 0.069223, loss_ce: 0.032063
2022-01-11 22:32:51,944 iteration 711 : loss : 0.069889, loss_ce: 0.030225
2022-01-11 22:32:53,548 iteration 712 : loss : 0.089274, loss_ce: 0.027767
2022-01-11 22:32:55,077 iteration 713 : loss : 0.046511, loss_ce: 0.019000
2022-01-11 22:32:56,611 iteration 714 : loss : 0.083948, loss_ce: 0.028745
 10%|███▏                          | 42/400 [21:05<2:59:37, 30.10s/it]2022-01-11 22:32:58,337 iteration 715 : loss : 0.082080, loss_ce: 0.042051
2022-01-11 22:32:59,926 iteration 716 : loss : 0.070647, loss_ce: 0.028765
2022-01-11 22:33:01,503 iteration 717 : loss : 0.118940, loss_ce: 0.042226
2022-01-11 22:33:03,086 iteration 718 : loss : 0.134884, loss_ce: 0.061016
2022-01-11 22:33:04,777 iteration 719 : loss : 0.069401, loss_ce: 0.027360
2022-01-11 22:33:06,350 iteration 720 : loss : 0.120667, loss_ce: 0.049235
2022-01-11 22:33:07,989 iteration 721 : loss : 0.092212, loss_ce: 0.037345
2022-01-11 22:33:09,618 iteration 722 : loss : 0.083522, loss_ce: 0.034517
2022-01-11 22:33:11,173 iteration 723 : loss : 0.074479, loss_ce: 0.029303
2022-01-11 22:33:12,888 iteration 724 : loss : 0.096664, loss_ce: 0.044016
2022-01-11 22:33:14,421 iteration 725 : loss : 0.062211, loss_ce: 0.025011
2022-01-11 22:33:16,032 iteration 726 : loss : 0.116809, loss_ce: 0.036662
2022-01-11 22:33:17,681 iteration 727 : loss : 0.134426, loss_ce: 0.043308
2022-01-11 22:33:19,462 iteration 728 : loss : 0.076228, loss_ce: 0.035880
2022-01-11 22:33:20,959 iteration 729 : loss : 0.042373, loss_ce: 0.017818
2022-01-11 22:33:22,577 iteration 730 : loss : 0.064269, loss_ce: 0.030990
2022-01-11 22:33:24,187 iteration 731 : loss : 0.094331, loss_ce: 0.033123
 11%|███▏                          | 43/400 [21:32<2:54:35, 29.34s/it]2022-01-11 22:33:25,948 iteration 732 : loss : 0.092653, loss_ce: 0.042001
2022-01-11 22:33:27,580 iteration 733 : loss : 0.144574, loss_ce: 0.069671
2022-01-11 22:33:29,121 iteration 734 : loss : 0.082217, loss_ce: 0.032076
2022-01-11 22:33:30,726 iteration 735 : loss : 0.071861, loss_ce: 0.030975
2022-01-11 22:33:32,332 iteration 736 : loss : 0.070764, loss_ce: 0.025572
2022-01-11 22:33:33,990 iteration 737 : loss : 0.052027, loss_ce: 0.019479
2022-01-11 22:33:35,608 iteration 738 : loss : 0.048475, loss_ce: 0.021513
2022-01-11 22:33:37,200 iteration 739 : loss : 0.094746, loss_ce: 0.036481
2022-01-11 22:33:38,790 iteration 740 : loss : 0.093778, loss_ce: 0.043672
2022-01-11 22:33:40,407 iteration 741 : loss : 0.080287, loss_ce: 0.033686
2022-01-11 22:33:42,085 iteration 742 : loss : 0.098677, loss_ce: 0.040153
2022-01-11 22:33:43,660 iteration 743 : loss : 0.084346, loss_ce: 0.034822
2022-01-11 22:33:45,228 iteration 744 : loss : 0.075743, loss_ce: 0.030215
2022-01-11 22:33:46,781 iteration 745 : loss : 0.065099, loss_ce: 0.029070
2022-01-11 22:33:48,381 iteration 746 : loss : 0.110032, loss_ce: 0.051995
2022-01-11 22:33:50,002 iteration 747 : loss : 0.105517, loss_ce: 0.033441
2022-01-11 22:33:51,504 iteration 748 : loss : 0.062631, loss_ce: 0.026782
 11%|███▎                          | 44/400 [22:00<2:50:31, 28.74s/it]2022-01-11 22:33:53,157 iteration 749 : loss : 0.075656, loss_ce: 0.032443
2022-01-11 22:33:54,807 iteration 750 : loss : 0.061096, loss_ce: 0.025113
2022-01-11 22:33:56,332 iteration 751 : loss : 0.064982, loss_ce: 0.025912
2022-01-11 22:33:57,944 iteration 752 : loss : 0.074252, loss_ce: 0.031229
2022-01-11 22:33:59,631 iteration 753 : loss : 0.088576, loss_ce: 0.035828
2022-01-11 22:34:01,199 iteration 754 : loss : 0.089587, loss_ce: 0.035713
2022-01-11 22:34:02,801 iteration 755 : loss : 0.065597, loss_ce: 0.027337
2022-01-11 22:34:04,342 iteration 756 : loss : 0.072902, loss_ce: 0.031743
2022-01-11 22:34:05,899 iteration 757 : loss : 0.076884, loss_ce: 0.027405
2022-01-11 22:34:07,492 iteration 758 : loss : 0.070412, loss_ce: 0.027390
2022-01-11 22:34:09,007 iteration 759 : loss : 0.048608, loss_ce: 0.022242
2022-01-11 22:34:10,623 iteration 760 : loss : 0.106977, loss_ce: 0.038416
2022-01-11 22:34:12,277 iteration 761 : loss : 0.070447, loss_ce: 0.023026
2022-01-11 22:34:13,921 iteration 762 : loss : 0.099399, loss_ce: 0.047670
2022-01-11 22:34:15,506 iteration 763 : loss : 0.062415, loss_ce: 0.026152
2022-01-11 22:34:17,055 iteration 764 : loss : 0.073736, loss_ce: 0.031319
2022-01-11 22:34:17,055 Training Data Eval:
2022-01-11 22:34:25,033   Average segmentation loss on training set: 0.0631
2022-01-11 22:34:25,034 Validation Data Eval:
2022-01-11 22:34:27,777   Average segmentation loss on validation set: 0.1072
2022-01-11 22:34:29,393 iteration 765 : loss : 0.079906, loss_ce: 0.037069
 11%|███▍                          | 45/400 [22:38<3:06:16, 31.48s/it]2022-01-11 22:34:31,010 iteration 766 : loss : 0.084669, loss_ce: 0.037049
2022-01-11 22:34:32,668 iteration 767 : loss : 0.103970, loss_ce: 0.049002
2022-01-11 22:34:34,165 iteration 768 : loss : 0.078239, loss_ce: 0.029155
2022-01-11 22:34:35,758 iteration 769 : loss : 0.201940, loss_ce: 0.046993
2022-01-11 22:34:37,292 iteration 770 : loss : 0.055051, loss_ce: 0.023856
2022-01-11 22:34:38,947 iteration 771 : loss : 0.093426, loss_ce: 0.035488
2022-01-11 22:34:40,600 iteration 772 : loss : 0.071044, loss_ce: 0.034368
2022-01-11 22:34:42,206 iteration 773 : loss : 0.080976, loss_ce: 0.031014
2022-01-11 22:34:43,798 iteration 774 : loss : 0.072667, loss_ce: 0.036600
2022-01-11 22:34:45,380 iteration 775 : loss : 0.077624, loss_ce: 0.029833
2022-01-11 22:34:46,876 iteration 776 : loss : 0.073800, loss_ce: 0.032716
2022-01-11 22:34:48,524 iteration 777 : loss : 0.066804, loss_ce: 0.028737
2022-01-11 22:34:50,124 iteration 778 : loss : 0.048482, loss_ce: 0.019657
2022-01-11 22:34:51,725 iteration 779 : loss : 0.065080, loss_ce: 0.034219
2022-01-11 22:34:53,266 iteration 780 : loss : 0.075512, loss_ce: 0.029917
2022-01-11 22:34:54,915 iteration 781 : loss : 0.072154, loss_ce: 0.033722
2022-01-11 22:34:56,553 iteration 782 : loss : 0.100319, loss_ce: 0.048137
 12%|███▍                          | 46/400 [23:05<2:58:04, 30.18s/it]2022-01-11 22:34:58,239 iteration 783 : loss : 0.073044, loss_ce: 0.035832
2022-01-11 22:34:59,831 iteration 784 : loss : 0.106460, loss_ce: 0.042362
2022-01-11 22:35:01,462 iteration 785 : loss : 0.059079, loss_ce: 0.025757
2022-01-11 22:35:03,064 iteration 786 : loss : 0.065083, loss_ce: 0.023527
2022-01-11 22:35:04,671 iteration 787 : loss : 0.078379, loss_ce: 0.031754
2022-01-11 22:35:06,306 iteration 788 : loss : 0.062326, loss_ce: 0.019205
2022-01-11 22:35:08,041 iteration 789 : loss : 0.117230, loss_ce: 0.064166
2022-01-11 22:35:09,709 iteration 790 : loss : 0.053351, loss_ce: 0.019691
2022-01-11 22:35:11,285 iteration 791 : loss : 0.068430, loss_ce: 0.024279
2022-01-11 22:35:12,929 iteration 792 : loss : 0.088388, loss_ce: 0.033798
2022-01-11 22:35:14,433 iteration 793 : loss : 0.109392, loss_ce: 0.044219
2022-01-11 22:35:16,021 iteration 794 : loss : 0.047153, loss_ce: 0.020528
2022-01-11 22:35:17,619 iteration 795 : loss : 0.070620, loss_ce: 0.029833
2022-01-11 22:35:19,219 iteration 796 : loss : 0.063667, loss_ce: 0.025137
2022-01-11 22:35:20,766 iteration 797 : loss : 0.065677, loss_ce: 0.028737
2022-01-11 22:35:22,318 iteration 798 : loss : 0.126770, loss_ce: 0.039262
2022-01-11 22:35:23,946 iteration 799 : loss : 0.048822, loss_ce: 0.018854
 12%|███▌                          | 47/400 [23:32<2:52:39, 29.35s/it]2022-01-11 22:35:25,642 iteration 800 : loss : 0.085476, loss_ce: 0.025384
2022-01-11 22:35:27,194 iteration 801 : loss : 0.059648, loss_ce: 0.022517
2022-01-11 22:35:28,790 iteration 802 : loss : 0.079695, loss_ce: 0.038713
2022-01-11 22:35:30,459 iteration 803 : loss : 0.105373, loss_ce: 0.061549
2022-01-11 22:35:31,987 iteration 804 : loss : 0.062338, loss_ce: 0.028531
2022-01-11 22:35:33,517 iteration 805 : loss : 0.138377, loss_ce: 0.028841
2022-01-11 22:35:35,105 iteration 806 : loss : 0.084869, loss_ce: 0.033578
2022-01-11 22:35:36,800 iteration 807 : loss : 0.069725, loss_ce: 0.031905
2022-01-11 22:35:38,446 iteration 808 : loss : 0.070440, loss_ce: 0.027351
2022-01-11 22:35:40,030 iteration 809 : loss : 0.116535, loss_ce: 0.038305
2022-01-11 22:35:41,686 iteration 810 : loss : 0.098157, loss_ce: 0.032591
2022-01-11 22:35:43,251 iteration 811 : loss : 0.041752, loss_ce: 0.018186
2022-01-11 22:35:44,931 iteration 812 : loss : 0.124196, loss_ce: 0.045182
2022-01-11 22:35:46,521 iteration 813 : loss : 0.055950, loss_ce: 0.023563
2022-01-11 22:35:48,138 iteration 814 : loss : 0.066973, loss_ce: 0.031763
2022-01-11 22:35:49,787 iteration 815 : loss : 0.061921, loss_ce: 0.022027
2022-01-11 22:35:51,260 iteration 816 : loss : 0.070059, loss_ce: 0.024198
 12%|███▌                          | 48/400 [23:59<2:48:36, 28.74s/it]2022-01-11 22:35:52,860 iteration 817 : loss : 0.053532, loss_ce: 0.020708
2022-01-11 22:35:54,493 iteration 818 : loss : 0.101161, loss_ce: 0.041619
2022-01-11 22:35:56,084 iteration 819 : loss : 0.062703, loss_ce: 0.025402
2022-01-11 22:35:57,790 iteration 820 : loss : 0.080970, loss_ce: 0.028671
2022-01-11 22:35:59,296 iteration 821 : loss : 0.116135, loss_ce: 0.045346
2022-01-11 22:36:00,922 iteration 822 : loss : 0.056381, loss_ce: 0.024434
2022-01-11 22:36:02,512 iteration 823 : loss : 0.082377, loss_ce: 0.028226
2022-01-11 22:36:04,079 iteration 824 : loss : 0.070561, loss_ce: 0.032308
2022-01-11 22:36:05,662 iteration 825 : loss : 0.085881, loss_ce: 0.026580
2022-01-11 22:36:07,287 iteration 826 : loss : 0.052193, loss_ce: 0.022411
2022-01-11 22:36:08,876 iteration 827 : loss : 0.120776, loss_ce: 0.037946
2022-01-11 22:36:10,528 iteration 828 : loss : 0.069677, loss_ce: 0.028086
2022-01-11 22:36:12,117 iteration 829 : loss : 0.093378, loss_ce: 0.032666
2022-01-11 22:36:13,815 iteration 830 : loss : 0.081981, loss_ce: 0.026863
2022-01-11 22:36:15,425 iteration 831 : loss : 0.071039, loss_ce: 0.027976
2022-01-11 22:36:17,011 iteration 832 : loss : 0.050870, loss_ce: 0.021974
2022-01-11 22:36:18,570 iteration 833 : loss : 0.061800, loss_ce: 0.028863
 12%|███▋                          | 49/400 [24:27<2:45:36, 28.31s/it]2022-01-11 22:36:20,162 iteration 834 : loss : 0.090561, loss_ce: 0.029926
2022-01-11 22:36:21,787 iteration 835 : loss : 0.075087, loss_ce: 0.030630
2022-01-11 22:36:23,411 iteration 836 : loss : 0.056189, loss_ce: 0.025357
2022-01-11 22:36:24,989 iteration 837 : loss : 0.062885, loss_ce: 0.027341
2022-01-11 22:36:26,561 iteration 838 : loss : 0.084541, loss_ce: 0.035237
2022-01-11 22:36:28,239 iteration 839 : loss : 0.063051, loss_ce: 0.022144
2022-01-11 22:36:29,869 iteration 840 : loss : 0.085586, loss_ce: 0.032367
2022-01-11 22:36:31,483 iteration 841 : loss : 0.083245, loss_ce: 0.043068
2022-01-11 22:36:33,220 iteration 842 : loss : 0.094046, loss_ce: 0.035409
2022-01-11 22:36:34,787 iteration 843 : loss : 0.058510, loss_ce: 0.024518
2022-01-11 22:36:36,436 iteration 844 : loss : 0.097611, loss_ce: 0.041090
2022-01-11 22:36:38,008 iteration 845 : loss : 0.060858, loss_ce: 0.026476
2022-01-11 22:36:39,611 iteration 846 : loss : 0.084314, loss_ce: 0.034865
2022-01-11 22:36:41,254 iteration 847 : loss : 0.067007, loss_ce: 0.027390
2022-01-11 22:36:42,765 iteration 848 : loss : 0.048548, loss_ce: 0.018870
2022-01-11 22:36:44,383 iteration 849 : loss : 0.073342, loss_ce: 0.030949
2022-01-11 22:36:44,383 Training Data Eval:
2022-01-11 22:36:52,370   Average segmentation loss on training set: 0.0576
2022-01-11 22:36:52,371 Validation Data Eval:
2022-01-11 22:36:55,117   Average segmentation loss on validation set: 0.0830
2022-01-11 22:37:00,985 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:37:02,540 iteration 850 : loss : 0.067143, loss_ce: 0.022247
 12%|███▊                          | 50/400 [25:11<3:12:32, 33.01s/it]2022-01-11 22:37:04,069 iteration 851 : loss : 0.064939, loss_ce: 0.029667
2022-01-11 22:37:05,571 iteration 852 : loss : 0.068636, loss_ce: 0.021913
2022-01-11 22:37:07,087 iteration 853 : loss : 0.113220, loss_ce: 0.042438
2022-01-11 22:37:08,539 iteration 854 : loss : 0.069469, loss_ce: 0.026868
2022-01-11 22:37:10,037 iteration 855 : loss : 0.065067, loss_ce: 0.023506
2022-01-11 22:37:11,585 iteration 856 : loss : 0.065458, loss_ce: 0.023602
2022-01-11 22:37:13,269 iteration 857 : loss : 0.092033, loss_ce: 0.037549
2022-01-11 22:37:14,864 iteration 858 : loss : 0.103419, loss_ce: 0.034368
2022-01-11 22:37:16,572 iteration 859 : loss : 0.072801, loss_ce: 0.027429
2022-01-11 22:37:18,211 iteration 860 : loss : 0.088648, loss_ce: 0.037842
2022-01-11 22:37:19,812 iteration 861 : loss : 0.069791, loss_ce: 0.029240
2022-01-11 22:37:21,412 iteration 862 : loss : 0.062164, loss_ce: 0.015637
2022-01-11 22:37:22,934 iteration 863 : loss : 0.086166, loss_ce: 0.039961
2022-01-11 22:37:24,521 iteration 864 : loss : 0.056614, loss_ce: 0.023067
2022-01-11 22:37:26,050 iteration 865 : loss : 0.064635, loss_ce: 0.031461
2022-01-11 22:37:27,598 iteration 866 : loss : 0.058525, loss_ce: 0.021855
2022-01-11 22:37:29,157 iteration 867 : loss : 0.064639, loss_ce: 0.028601
 13%|███▊                          | 51/400 [25:37<3:00:50, 31.09s/it]2022-01-11 22:37:30,829 iteration 868 : loss : 0.051127, loss_ce: 0.019522
2022-01-11 22:37:32,393 iteration 869 : loss : 0.071955, loss_ce: 0.031416
2022-01-11 22:37:34,026 iteration 870 : loss : 0.095568, loss_ce: 0.026204
2022-01-11 22:37:35,643 iteration 871 : loss : 0.073169, loss_ce: 0.027099
2022-01-11 22:37:37,162 iteration 872 : loss : 0.044137, loss_ce: 0.020818
2022-01-11 22:37:38,695 iteration 873 : loss : 0.049972, loss_ce: 0.017845
2022-01-11 22:37:40,313 iteration 874 : loss : 0.075649, loss_ce: 0.036449
2022-01-11 22:37:41,923 iteration 875 : loss : 0.065792, loss_ce: 0.028853
2022-01-11 22:37:43,509 iteration 876 : loss : 0.055992, loss_ce: 0.024480
2022-01-11 22:37:45,039 iteration 877 : loss : 0.056420, loss_ce: 0.020006
2022-01-11 22:37:46,584 iteration 878 : loss : 0.056965, loss_ce: 0.022290
2022-01-11 22:37:48,171 iteration 879 : loss : 0.064234, loss_ce: 0.023295
2022-01-11 22:37:49,715 iteration 880 : loss : 0.048351, loss_ce: 0.020531
2022-01-11 22:37:51,358 iteration 881 : loss : 0.070125, loss_ce: 0.029890
2022-01-11 22:37:52,920 iteration 882 : loss : 0.066441, loss_ce: 0.028032
2022-01-11 22:37:54,500 iteration 883 : loss : 0.090156, loss_ce: 0.031606
2022-01-11 22:37:56,237 iteration 884 : loss : 0.054551, loss_ce: 0.021021
 13%|███▉                          | 52/400 [26:04<2:53:20, 29.89s/it]2022-01-11 22:37:57,963 iteration 885 : loss : 0.079711, loss_ce: 0.036529
2022-01-11 22:37:59,539 iteration 886 : loss : 0.086677, loss_ce: 0.034202
2022-01-11 22:38:01,130 iteration 887 : loss : 0.068703, loss_ce: 0.031006
2022-01-11 22:38:02,746 iteration 888 : loss : 0.058947, loss_ce: 0.022523
2022-01-11 22:38:04,318 iteration 889 : loss : 0.064656, loss_ce: 0.021772
2022-01-11 22:38:05,975 iteration 890 : loss : 0.053925, loss_ce: 0.022579
2022-01-11 22:38:07,607 iteration 891 : loss : 0.076729, loss_ce: 0.038448
2022-01-11 22:38:09,217 iteration 892 : loss : 0.119695, loss_ce: 0.037552
2022-01-11 22:38:10,897 iteration 893 : loss : 0.063056, loss_ce: 0.029112
2022-01-11 22:38:12,500 iteration 894 : loss : 0.070070, loss_ce: 0.029332
2022-01-11 22:38:14,120 iteration 895 : loss : 0.073903, loss_ce: 0.025225
2022-01-11 22:38:15,782 iteration 896 : loss : 0.082312, loss_ce: 0.024270
2022-01-11 22:38:17,303 iteration 897 : loss : 0.050180, loss_ce: 0.020507
2022-01-11 22:38:19,006 iteration 898 : loss : 0.071091, loss_ce: 0.024727
2022-01-11 22:38:20,616 iteration 899 : loss : 0.065200, loss_ce: 0.025014
2022-01-11 22:38:22,259 iteration 900 : loss : 0.087472, loss_ce: 0.038794
2022-01-11 22:38:23,843 iteration 901 : loss : 0.084061, loss_ce: 0.053746
 13%|███▉                          | 53/400 [26:32<2:48:53, 29.20s/it]2022-01-11 22:38:25,481 iteration 902 : loss : 0.054423, loss_ce: 0.022816
2022-01-11 22:38:27,109 iteration 903 : loss : 0.055767, loss_ce: 0.019691
2022-01-11 22:38:28,681 iteration 904 : loss : 0.060083, loss_ce: 0.026343
2022-01-11 22:38:30,235 iteration 905 : loss : 0.108944, loss_ce: 0.029780
2022-01-11 22:38:31,787 iteration 906 : loss : 0.052638, loss_ce: 0.019689
2022-01-11 22:38:33,429 iteration 907 : loss : 0.047900, loss_ce: 0.024302
2022-01-11 22:38:34,976 iteration 908 : loss : 0.045760, loss_ce: 0.023366
2022-01-11 22:38:36,605 iteration 909 : loss : 0.078066, loss_ce: 0.036530
2022-01-11 22:38:38,211 iteration 910 : loss : 0.076467, loss_ce: 0.025095
2022-01-11 22:38:39,838 iteration 911 : loss : 0.066211, loss_ce: 0.024216
2022-01-11 22:38:41,337 iteration 912 : loss : 0.067356, loss_ce: 0.025879
2022-01-11 22:38:42,989 iteration 913 : loss : 0.085694, loss_ce: 0.035539
2022-01-11 22:38:44,558 iteration 914 : loss : 0.055478, loss_ce: 0.025761
2022-01-11 22:38:46,195 iteration 915 : loss : 0.069610, loss_ce: 0.030959
2022-01-11 22:38:47,772 iteration 916 : loss : 0.068403, loss_ce: 0.026372
2022-01-11 22:38:49,377 iteration 917 : loss : 0.066230, loss_ce: 0.023130
2022-01-11 22:38:50,971 iteration 918 : loss : 0.081721, loss_ce: 0.029387
 14%|████                          | 54/400 [26:59<2:44:48, 28.58s/it]2022-01-11 22:38:52,679 iteration 919 : loss : 0.049798, loss_ce: 0.019325
2022-01-11 22:38:54,228 iteration 920 : loss : 0.093878, loss_ce: 0.062169
2022-01-11 22:38:55,929 iteration 921 : loss : 0.054669, loss_ce: 0.023764
2022-01-11 22:38:57,426 iteration 922 : loss : 0.057136, loss_ce: 0.023844
2022-01-11 22:38:58,958 iteration 923 : loss : 0.066137, loss_ce: 0.026830
2022-01-11 22:39:00,546 iteration 924 : loss : 0.064936, loss_ce: 0.022494
2022-01-11 22:39:02,086 iteration 925 : loss : 0.045401, loss_ce: 0.016684
2022-01-11 22:39:03,616 iteration 926 : loss : 0.058068, loss_ce: 0.020667
2022-01-11 22:39:05,212 iteration 927 : loss : 0.101089, loss_ce: 0.047175
2022-01-11 22:39:06,753 iteration 928 : loss : 0.055642, loss_ce: 0.020921
2022-01-11 22:39:08,498 iteration 929 : loss : 0.068727, loss_ce: 0.029812
2022-01-11 22:39:10,109 iteration 930 : loss : 0.068397, loss_ce: 0.041329
2022-01-11 22:39:11,724 iteration 931 : loss : 0.072938, loss_ce: 0.025825
2022-01-11 22:39:13,337 iteration 932 : loss : 0.080068, loss_ce: 0.026200
2022-01-11 22:39:15,089 iteration 933 : loss : 0.116269, loss_ce: 0.037624
2022-01-11 22:39:16,691 iteration 934 : loss : 0.057322, loss_ce: 0.021235
2022-01-11 22:39:16,692 Training Data Eval:
2022-01-11 22:39:24,673   Average segmentation loss on training set: 0.0442
2022-01-11 22:39:24,673 Validation Data Eval:
2022-01-11 22:39:27,421   Average segmentation loss on validation set: 0.0921
2022-01-11 22:39:28,974 iteration 935 : loss : 0.065487, loss_ce: 0.020723
 14%|████▏                         | 55/400 [27:37<3:00:34, 31.41s/it]2022-01-11 22:39:30,588 iteration 936 : loss : 0.062573, loss_ce: 0.017083
2022-01-11 22:39:32,145 iteration 937 : loss : 0.058171, loss_ce: 0.026428
2022-01-11 22:39:33,780 iteration 938 : loss : 0.070899, loss_ce: 0.038039
2022-01-11 22:39:35,369 iteration 939 : loss : 0.086081, loss_ce: 0.039438
2022-01-11 22:39:36,935 iteration 940 : loss : 0.065682, loss_ce: 0.030847
2022-01-11 22:39:38,553 iteration 941 : loss : 0.082936, loss_ce: 0.030140
2022-01-11 22:39:40,130 iteration 942 : loss : 0.062297, loss_ce: 0.023350
2022-01-11 22:39:41,634 iteration 943 : loss : 0.098326, loss_ce: 0.035904
2022-01-11 22:39:43,187 iteration 944 : loss : 0.048451, loss_ce: 0.020413
2022-01-11 22:39:44,751 iteration 945 : loss : 0.078716, loss_ce: 0.043945
2022-01-11 22:39:46,342 iteration 946 : loss : 0.056508, loss_ce: 0.022605
2022-01-11 22:39:48,053 iteration 947 : loss : 0.064433, loss_ce: 0.028431
2022-01-11 22:39:49,618 iteration 948 : loss : 0.067050, loss_ce: 0.026712
2022-01-11 22:39:51,235 iteration 949 : loss : 0.092739, loss_ce: 0.042019
2022-01-11 22:39:52,890 iteration 950 : loss : 0.097087, loss_ce: 0.030601
2022-01-11 22:39:54,570 iteration 951 : loss : 0.056465, loss_ce: 0.022729
2022-01-11 22:39:56,113 iteration 952 : loss : 0.067897, loss_ce: 0.020891
 14%|████▏                         | 56/400 [28:04<2:52:43, 30.13s/it]2022-01-11 22:39:57,767 iteration 953 : loss : 0.057519, loss_ce: 0.019751
2022-01-11 22:39:59,481 iteration 954 : loss : 0.038480, loss_ce: 0.014410
2022-01-11 22:40:01,062 iteration 955 : loss : 0.039568, loss_ce: 0.015491
2022-01-11 22:40:02,654 iteration 956 : loss : 0.050150, loss_ce: 0.020952
2022-01-11 22:40:04,135 iteration 957 : loss : 0.052101, loss_ce: 0.019435
2022-01-11 22:40:05,705 iteration 958 : loss : 0.052777, loss_ce: 0.017692
2022-01-11 22:40:07,238 iteration 959 : loss : 0.062580, loss_ce: 0.030958
2022-01-11 22:40:08,763 iteration 960 : loss : 0.053635, loss_ce: 0.021905
2022-01-11 22:40:10,353 iteration 961 : loss : 0.073812, loss_ce: 0.027549
2022-01-11 22:40:11,934 iteration 962 : loss : 0.047100, loss_ce: 0.015843
2022-01-11 22:40:13,566 iteration 963 : loss : 0.068753, loss_ce: 0.024653
2022-01-11 22:40:15,182 iteration 964 : loss : 0.055792, loss_ce: 0.023040
2022-01-11 22:40:16,773 iteration 965 : loss : 0.042010, loss_ce: 0.016487
2022-01-11 22:40:18,455 iteration 966 : loss : 0.081068, loss_ce: 0.034429
2022-01-11 22:40:20,047 iteration 967 : loss : 0.085339, loss_ce: 0.034537
2022-01-11 22:40:21,666 iteration 968 : loss : 0.060067, loss_ce: 0.022814
2022-01-11 22:40:23,320 iteration 969 : loss : 0.056384, loss_ce: 0.020941
 14%|████▎                         | 57/400 [28:31<2:47:12, 29.25s/it]2022-01-11 22:40:24,951 iteration 970 : loss : 0.054093, loss_ce: 0.022566
2022-01-11 22:40:26,641 iteration 971 : loss : 0.079184, loss_ce: 0.027567
2022-01-11 22:40:28,178 iteration 972 : loss : 0.049479, loss_ce: 0.017675
2022-01-11 22:40:29,705 iteration 973 : loss : 0.050555, loss_ce: 0.019166
2022-01-11 22:40:31,291 iteration 974 : loss : 0.058878, loss_ce: 0.022111
2022-01-11 22:40:32,848 iteration 975 : loss : 0.056530, loss_ce: 0.020715
2022-01-11 22:40:34,408 iteration 976 : loss : 0.087402, loss_ce: 0.032236
2022-01-11 22:40:36,066 iteration 977 : loss : 0.080685, loss_ce: 0.029926
2022-01-11 22:40:37,703 iteration 978 : loss : 0.081787, loss_ce: 0.038184
2022-01-11 22:40:39,348 iteration 979 : loss : 0.048726, loss_ce: 0.022933
2022-01-11 22:40:40,875 iteration 980 : loss : 0.068522, loss_ce: 0.022735
2022-01-11 22:40:42,541 iteration 981 : loss : 0.072023, loss_ce: 0.029087
2022-01-11 22:40:44,078 iteration 982 : loss : 0.062653, loss_ce: 0.031246
2022-01-11 22:40:45,697 iteration 983 : loss : 0.060718, loss_ce: 0.026306
2022-01-11 22:40:47,298 iteration 984 : loss : 0.057911, loss_ce: 0.025839
2022-01-11 22:40:48,892 iteration 985 : loss : 0.076417, loss_ce: 0.026744
2022-01-11 22:40:50,485 iteration 986 : loss : 0.072180, loss_ce: 0.023981
 14%|████▎                         | 58/400 [28:59<2:43:09, 28.62s/it]2022-01-11 22:40:52,128 iteration 987 : loss : 0.049475, loss_ce: 0.021999
2022-01-11 22:40:53,816 iteration 988 : loss : 0.072647, loss_ce: 0.030692
2022-01-11 22:40:55,375 iteration 989 : loss : 0.085646, loss_ce: 0.033489
2022-01-11 22:40:57,036 iteration 990 : loss : 0.058981, loss_ce: 0.030545
2022-01-11 22:40:58,700 iteration 991 : loss : 0.079898, loss_ce: 0.043626
2022-01-11 22:41:00,253 iteration 992 : loss : 0.081038, loss_ce: 0.034504
2022-01-11 22:41:01,990 iteration 993 : loss : 0.047480, loss_ce: 0.022639
2022-01-11 22:41:03,659 iteration 994 : loss : 0.063882, loss_ce: 0.020070
2022-01-11 22:41:05,298 iteration 995 : loss : 0.059775, loss_ce: 0.024978
2022-01-11 22:41:06,878 iteration 996 : loss : 0.042779, loss_ce: 0.017497
2022-01-11 22:41:08,507 iteration 997 : loss : 0.074678, loss_ce: 0.034842
2022-01-11 22:41:10,128 iteration 998 : loss : 0.062496, loss_ce: 0.020744
2022-01-11 22:41:11,832 iteration 999 : loss : 0.049919, loss_ce: 0.019694
2022-01-11 22:41:13,456 iteration 1000 : loss : 0.071605, loss_ce: 0.030550
2022-01-11 22:41:15,095 iteration 1001 : loss : 0.055061, loss_ce: 0.019446
2022-01-11 22:41:16,633 iteration 1002 : loss : 0.045063, loss_ce: 0.015795
2022-01-11 22:41:18,208 iteration 1003 : loss : 0.072518, loss_ce: 0.033004
 15%|████▍                         | 59/400 [29:26<2:41:09, 28.36s/it]2022-01-11 22:41:19,842 iteration 1004 : loss : 0.052662, loss_ce: 0.021380
2022-01-11 22:41:21,538 iteration 1005 : loss : 0.075824, loss_ce: 0.030429
2022-01-11 22:41:23,136 iteration 1006 : loss : 0.083038, loss_ce: 0.042942
2022-01-11 22:41:24,764 iteration 1007 : loss : 0.059687, loss_ce: 0.023668
2022-01-11 22:41:26,374 iteration 1008 : loss : 0.066371, loss_ce: 0.022358
2022-01-11 22:41:27,992 iteration 1009 : loss : 0.079414, loss_ce: 0.031076
2022-01-11 22:41:29,563 iteration 1010 : loss : 0.060369, loss_ce: 0.022924
2022-01-11 22:41:31,145 iteration 1011 : loss : 0.045934, loss_ce: 0.018295
2022-01-11 22:41:32,825 iteration 1012 : loss : 0.087884, loss_ce: 0.030950
2022-01-11 22:41:34,394 iteration 1013 : loss : 0.072520, loss_ce: 0.022459
2022-01-11 22:41:36,037 iteration 1014 : loss : 0.051191, loss_ce: 0.020029
2022-01-11 22:41:37,620 iteration 1015 : loss : 0.071558, loss_ce: 0.036874
2022-01-11 22:41:39,123 iteration 1016 : loss : 0.048725, loss_ce: 0.026096
2022-01-11 22:41:40,828 iteration 1017 : loss : 0.055656, loss_ce: 0.015857
2022-01-11 22:41:42,492 iteration 1018 : loss : 0.057324, loss_ce: 0.023805
2022-01-11 22:41:44,179 iteration 1019 : loss : 0.061252, loss_ce: 0.026205
2022-01-11 22:41:44,179 Training Data Eval:
2022-01-11 22:41:52,156   Average segmentation loss on training set: 0.0493
2022-01-11 22:41:52,156 Validation Data Eval:
2022-01-11 22:41:54,912   Average segmentation loss on validation set: 0.0731
2022-01-11 22:42:00,753 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:42:02,292 iteration 1020 : loss : 0.060374, loss_ce: 0.021768
 15%|████▌                         | 60/400 [30:10<3:07:25, 33.08s/it]2022-01-11 22:42:03,897 iteration 1021 : loss : 0.074317, loss_ce: 0.030871
2022-01-11 22:42:05,377 iteration 1022 : loss : 0.060212, loss_ce: 0.021844
2022-01-11 22:42:06,819 iteration 1023 : loss : 0.092932, loss_ce: 0.029106
2022-01-11 22:42:08,380 iteration 1024 : loss : 0.042549, loss_ce: 0.020073
2022-01-11 22:42:09,937 iteration 1025 : loss : 0.062668, loss_ce: 0.023974
2022-01-11 22:42:11,569 iteration 1026 : loss : 0.077392, loss_ce: 0.031884
2022-01-11 22:42:13,169 iteration 1027 : loss : 0.073766, loss_ce: 0.043423
2022-01-11 22:42:14,719 iteration 1028 : loss : 0.057985, loss_ce: 0.017939
2022-01-11 22:42:16,348 iteration 1029 : loss : 0.065859, loss_ce: 0.020114
2022-01-11 22:42:17,978 iteration 1030 : loss : 0.052996, loss_ce: 0.021836
2022-01-11 22:42:19,659 iteration 1031 : loss : 0.058384, loss_ce: 0.024312
2022-01-11 22:42:21,198 iteration 1032 : loss : 0.050082, loss_ce: 0.019242
2022-01-11 22:42:22,861 iteration 1033 : loss : 0.069381, loss_ce: 0.025559
2022-01-11 22:42:24,539 iteration 1034 : loss : 0.050966, loss_ce: 0.026361
2022-01-11 22:42:26,172 iteration 1035 : loss : 0.078540, loss_ce: 0.027960
2022-01-11 22:42:27,868 iteration 1036 : loss : 0.071948, loss_ce: 0.029730
2022-01-11 22:42:29,346 iteration 1037 : loss : 0.045799, loss_ce: 0.018610
 15%|████▌                         | 61/400 [30:38<2:56:40, 31.27s/it]2022-01-11 22:42:30,925 iteration 1038 : loss : 0.046608, loss_ce: 0.019748
2022-01-11 22:42:32,526 iteration 1039 : loss : 0.048656, loss_ce: 0.020259
2022-01-11 22:42:34,062 iteration 1040 : loss : 0.049668, loss_ce: 0.017246
2022-01-11 22:42:35,736 iteration 1041 : loss : 0.050593, loss_ce: 0.019319
2022-01-11 22:42:37,424 iteration 1042 : loss : 0.079278, loss_ce: 0.027383
2022-01-11 22:42:38,987 iteration 1043 : loss : 0.043345, loss_ce: 0.015798
2022-01-11 22:42:40,572 iteration 1044 : loss : 0.047609, loss_ce: 0.018149
2022-01-11 22:42:42,153 iteration 1045 : loss : 0.068659, loss_ce: 0.037352
2022-01-11 22:42:43,764 iteration 1046 : loss : 0.057203, loss_ce: 0.020189
2022-01-11 22:42:45,467 iteration 1047 : loss : 0.060752, loss_ce: 0.023026
2022-01-11 22:42:47,073 iteration 1048 : loss : 0.059806, loss_ce: 0.025661
2022-01-11 22:42:48,616 iteration 1049 : loss : 0.051907, loss_ce: 0.021220
2022-01-11 22:42:50,173 iteration 1050 : loss : 0.043793, loss_ce: 0.016808
2022-01-11 22:42:51,790 iteration 1051 : loss : 0.047929, loss_ce: 0.017680
2022-01-11 22:42:53,389 iteration 1052 : loss : 0.058129, loss_ce: 0.027477
2022-01-11 22:42:55,112 iteration 1053 : loss : 0.049376, loss_ce: 0.020547
2022-01-11 22:42:56,689 iteration 1054 : loss : 0.064465, loss_ce: 0.026281
 16%|████▋                         | 62/400 [31:05<2:49:29, 30.09s/it]2022-01-11 22:42:58,319 iteration 1055 : loss : 0.053664, loss_ce: 0.021907
2022-01-11 22:42:59,910 iteration 1056 : loss : 0.069349, loss_ce: 0.029541
2022-01-11 22:43:01,561 iteration 1057 : loss : 0.069692, loss_ce: 0.019979
2022-01-11 22:43:03,091 iteration 1058 : loss : 0.046836, loss_ce: 0.021503
2022-01-11 22:43:04,769 iteration 1059 : loss : 0.066454, loss_ce: 0.032502
2022-01-11 22:43:06,358 iteration 1060 : loss : 0.064360, loss_ce: 0.031248
2022-01-11 22:43:08,012 iteration 1061 : loss : 0.049567, loss_ce: 0.019240
2022-01-11 22:43:09,608 iteration 1062 : loss : 0.055858, loss_ce: 0.022144
2022-01-11 22:43:11,308 iteration 1063 : loss : 0.047114, loss_ce: 0.015875
2022-01-11 22:43:12,891 iteration 1064 : loss : 0.047947, loss_ce: 0.023616
2022-01-11 22:43:14,403 iteration 1065 : loss : 0.050975, loss_ce: 0.017895
2022-01-11 22:43:16,017 iteration 1066 : loss : 0.052173, loss_ce: 0.025474
2022-01-11 22:43:17,785 iteration 1067 : loss : 0.068110, loss_ce: 0.026370
2022-01-11 22:43:19,450 iteration 1068 : loss : 0.085523, loss_ce: 0.030588
2022-01-11 22:43:20,997 iteration 1069 : loss : 0.057789, loss_ce: 0.028041
2022-01-11 22:43:22,715 iteration 1070 : loss : 0.048688, loss_ce: 0.018580
2022-01-11 22:43:24,323 iteration 1071 : loss : 0.051774, loss_ce: 0.019915
 16%|████▋                         | 63/400 [31:33<2:44:51, 29.35s/it]2022-01-11 22:43:25,942 iteration 1072 : loss : 0.059337, loss_ce: 0.023484
2022-01-11 22:43:27,661 iteration 1073 : loss : 0.062712, loss_ce: 0.022664
2022-01-11 22:43:29,256 iteration 1074 : loss : 0.066482, loss_ce: 0.030768
2022-01-11 22:43:30,874 iteration 1075 : loss : 0.057625, loss_ce: 0.022770
2022-01-11 22:43:32,440 iteration 1076 : loss : 0.054682, loss_ce: 0.023675
2022-01-11 22:43:34,091 iteration 1077 : loss : 0.135997, loss_ce: 0.043650
2022-01-11 22:43:35,629 iteration 1078 : loss : 0.069308, loss_ce: 0.025915
2022-01-11 22:43:37,238 iteration 1079 : loss : 0.051626, loss_ce: 0.021173
2022-01-11 22:43:38,874 iteration 1080 : loss : 0.058863, loss_ce: 0.021895
2022-01-11 22:43:40,517 iteration 1081 : loss : 0.058342, loss_ce: 0.026628
2022-01-11 22:43:42,127 iteration 1082 : loss : 0.075184, loss_ce: 0.030857
2022-01-11 22:43:43,735 iteration 1083 : loss : 0.057715, loss_ce: 0.019137
2022-01-11 22:43:45,403 iteration 1084 : loss : 0.077038, loss_ce: 0.023446
2022-01-11 22:43:47,006 iteration 1085 : loss : 0.068745, loss_ce: 0.024889
2022-01-11 22:43:48,650 iteration 1086 : loss : 0.047802, loss_ce: 0.017693
2022-01-11 22:43:50,181 iteration 1087 : loss : 0.047656, loss_ce: 0.019162
2022-01-11 22:43:51,759 iteration 1088 : loss : 0.063808, loss_ce: 0.031214
 16%|████▊                         | 64/400 [32:00<2:41:09, 28.78s/it]2022-01-11 22:43:53,299 iteration 1089 : loss : 0.044514, loss_ce: 0.019782
2022-01-11 22:43:54,907 iteration 1090 : loss : 0.047369, loss_ce: 0.016466
2022-01-11 22:43:56,550 iteration 1091 : loss : 0.082187, loss_ce: 0.048055
2022-01-11 22:43:58,157 iteration 1092 : loss : 0.064313, loss_ce: 0.018915
2022-01-11 22:43:59,772 iteration 1093 : loss : 0.076147, loss_ce: 0.022825
2022-01-11 22:44:01,517 iteration 1094 : loss : 0.170744, loss_ce: 0.042812
2022-01-11 22:44:03,046 iteration 1095 : loss : 0.064009, loss_ce: 0.022008
2022-01-11 22:44:04,601 iteration 1096 : loss : 0.048144, loss_ce: 0.022628
2022-01-11 22:44:06,145 iteration 1097 : loss : 0.050269, loss_ce: 0.020427
2022-01-11 22:44:07,807 iteration 1098 : loss : 0.058053, loss_ce: 0.028891
2022-01-11 22:44:09,416 iteration 1099 : loss : 0.044025, loss_ce: 0.015798
2022-01-11 22:44:11,085 iteration 1100 : loss : 0.063391, loss_ce: 0.027395
2022-01-11 22:44:12,765 iteration 1101 : loss : 0.066175, loss_ce: 0.025753
2022-01-11 22:44:14,365 iteration 1102 : loss : 0.068090, loss_ce: 0.027525
2022-01-11 22:44:15,999 iteration 1103 : loss : 0.036354, loss_ce: 0.015002
2022-01-11 22:44:17,592 iteration 1104 : loss : 0.048190, loss_ce: 0.021942
2022-01-11 22:44:17,592 Training Data Eval:
2022-01-11 22:44:25,570   Average segmentation loss on training set: 0.0541
2022-01-11 22:44:25,571 Validation Data Eval:
2022-01-11 22:44:28,318   Average segmentation loss on validation set: 0.0699
2022-01-11 22:44:35,369 Found new lowest validation loss at iteration 1104! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:44:36,839 iteration 1105 : loss : 0.050718, loss_ce: 0.022109
 16%|████▉                         | 65/400 [32:45<3:07:59, 33.67s/it]2022-01-11 22:44:38,388 iteration 1106 : loss : 0.066971, loss_ce: 0.025909
2022-01-11 22:44:39,864 iteration 1107 : loss : 0.064334, loss_ce: 0.025071
2022-01-11 22:44:41,312 iteration 1108 : loss : 0.050607, loss_ce: 0.016605
2022-01-11 22:44:42,874 iteration 1109 : loss : 0.056302, loss_ce: 0.020277
2022-01-11 22:44:44,452 iteration 1110 : loss : 0.050179, loss_ce: 0.022293
2022-01-11 22:44:46,139 iteration 1111 : loss : 0.115954, loss_ce: 0.035887
2022-01-11 22:44:47,688 iteration 1112 : loss : 0.052563, loss_ce: 0.021857
2022-01-11 22:44:49,294 iteration 1113 : loss : 0.048253, loss_ce: 0.018516
2022-01-11 22:44:50,912 iteration 1114 : loss : 0.079047, loss_ce: 0.028372
2022-01-11 22:44:52,545 iteration 1115 : loss : 0.039773, loss_ce: 0.016189
2022-01-11 22:44:54,062 iteration 1116 : loss : 0.060397, loss_ce: 0.028027
2022-01-11 22:44:55,611 iteration 1117 : loss : 0.041255, loss_ce: 0.017190
2022-01-11 22:44:57,249 iteration 1118 : loss : 0.053747, loss_ce: 0.024539
2022-01-11 22:44:58,884 iteration 1119 : loss : 0.048441, loss_ce: 0.023976
2022-01-11 22:45:00,505 iteration 1120 : loss : 0.056666, loss_ce: 0.016585
2022-01-11 22:45:02,080 iteration 1121 : loss : 0.057458, loss_ce: 0.029020
2022-01-11 22:45:03,609 iteration 1122 : loss : 0.063420, loss_ce: 0.019283
 16%|████▉                         | 66/400 [33:12<2:55:54, 31.60s/it]2022-01-11 22:45:05,350 iteration 1123 : loss : 0.040537, loss_ce: 0.015810
2022-01-11 22:45:06,947 iteration 1124 : loss : 0.033194, loss_ce: 0.015917
2022-01-11 22:45:08,509 iteration 1125 : loss : 0.065145, loss_ce: 0.019287
2022-01-11 22:45:10,079 iteration 1126 : loss : 0.073929, loss_ce: 0.026083
2022-01-11 22:45:11,638 iteration 1127 : loss : 0.050041, loss_ce: 0.020334
2022-01-11 22:45:13,321 iteration 1128 : loss : 0.074886, loss_ce: 0.033869
2022-01-11 22:45:15,028 iteration 1129 : loss : 0.084341, loss_ce: 0.034745
2022-01-11 22:45:16,574 iteration 1130 : loss : 0.059122, loss_ce: 0.025037
2022-01-11 22:45:18,222 iteration 1131 : loss : 0.084408, loss_ce: 0.035850
2022-01-11 22:45:19,923 iteration 1132 : loss : 0.058803, loss_ce: 0.022929
2022-01-11 22:45:21,596 iteration 1133 : loss : 0.107534, loss_ce: 0.048729
2022-01-11 22:45:23,280 iteration 1134 : loss : 0.071514, loss_ce: 0.021579
2022-01-11 22:45:24,837 iteration 1135 : loss : 0.097381, loss_ce: 0.035119
2022-01-11 22:45:26,418 iteration 1136 : loss : 0.043327, loss_ce: 0.018871
2022-01-11 22:45:27,999 iteration 1137 : loss : 0.073520, loss_ce: 0.036489
2022-01-11 22:45:29,637 iteration 1138 : loss : 0.074523, loss_ce: 0.029309
2022-01-11 22:45:31,185 iteration 1139 : loss : 0.053647, loss_ce: 0.021858
 17%|█████                         | 67/400 [33:39<2:48:40, 30.39s/it]2022-01-11 22:45:32,827 iteration 1140 : loss : 0.055231, loss_ce: 0.027039
2022-01-11 22:45:34,455 iteration 1141 : loss : 0.078294, loss_ce: 0.035840
2022-01-11 22:45:36,210 iteration 1142 : loss : 0.072912, loss_ce: 0.032043
2022-01-11 22:45:37,836 iteration 1143 : loss : 0.054981, loss_ce: 0.023460
2022-01-11 22:45:39,474 iteration 1144 : loss : 0.095456, loss_ce: 0.034763
2022-01-11 22:45:41,104 iteration 1145 : loss : 0.070589, loss_ce: 0.024835
2022-01-11 22:45:42,684 iteration 1146 : loss : 0.047580, loss_ce: 0.020380
2022-01-11 22:45:44,280 iteration 1147 : loss : 0.054868, loss_ce: 0.020338
2022-01-11 22:45:45,863 iteration 1148 : loss : 0.066495, loss_ce: 0.030784
2022-01-11 22:45:47,452 iteration 1149 : loss : 0.046537, loss_ce: 0.025645
2022-01-11 22:45:49,016 iteration 1150 : loss : 0.048680, loss_ce: 0.017932
2022-01-11 22:45:50,586 iteration 1151 : loss : 0.041176, loss_ce: 0.018276
2022-01-11 22:45:52,146 iteration 1152 : loss : 0.067872, loss_ce: 0.028065
2022-01-11 22:45:53,835 iteration 1153 : loss : 0.081768, loss_ce: 0.028109
2022-01-11 22:45:55,486 iteration 1154 : loss : 0.067427, loss_ce: 0.023802
2022-01-11 22:45:57,145 iteration 1155 : loss : 0.050553, loss_ce: 0.016742
2022-01-11 22:45:58,869 iteration 1156 : loss : 0.077148, loss_ce: 0.024680
 17%|█████                         | 68/400 [34:07<2:43:39, 29.58s/it]2022-01-11 22:46:00,549 iteration 1157 : loss : 0.078400, loss_ce: 0.030140
2022-01-11 22:46:02,136 iteration 1158 : loss : 0.059564, loss_ce: 0.021371
2022-01-11 22:46:03,638 iteration 1159 : loss : 0.050899, loss_ce: 0.020627
2022-01-11 22:46:05,196 iteration 1160 : loss : 0.055125, loss_ce: 0.026935
2022-01-11 22:46:06,813 iteration 1161 : loss : 0.052111, loss_ce: 0.020529
2022-01-11 22:46:08,428 iteration 1162 : loss : 0.058037, loss_ce: 0.022685
2022-01-11 22:46:10,030 iteration 1163 : loss : 0.051902, loss_ce: 0.027020
2022-01-11 22:46:11,691 iteration 1164 : loss : 0.064583, loss_ce: 0.029230
2022-01-11 22:46:13,216 iteration 1165 : loss : 0.048241, loss_ce: 0.022680
2022-01-11 22:46:14,769 iteration 1166 : loss : 0.069328, loss_ce: 0.022817
2022-01-11 22:46:16,371 iteration 1167 : loss : 0.059577, loss_ce: 0.027420
2022-01-11 22:46:18,062 iteration 1168 : loss : 0.077767, loss_ce: 0.021325
2022-01-11 22:46:19,598 iteration 1169 : loss : 0.073357, loss_ce: 0.023640
2022-01-11 22:46:21,218 iteration 1170 : loss : 0.057428, loss_ce: 0.017797
2022-01-11 22:46:22,943 iteration 1171 : loss : 0.100736, loss_ce: 0.025802
2022-01-11 22:46:24,473 iteration 1172 : loss : 0.050631, loss_ce: 0.019281
2022-01-11 22:46:26,013 iteration 1173 : loss : 0.045011, loss_ce: 0.016436
 17%|█████▏                        | 69/400 [34:34<2:39:09, 28.85s/it]2022-01-11 22:46:27,696 iteration 1174 : loss : 0.055212, loss_ce: 0.018861
2022-01-11 22:46:29,380 iteration 1175 : loss : 0.066940, loss_ce: 0.031146
2022-01-11 22:46:30,944 iteration 1176 : loss : 0.054455, loss_ce: 0.025241
2022-01-11 22:46:32,519 iteration 1177 : loss : 0.079650, loss_ce: 0.037100
2022-01-11 22:46:34,053 iteration 1178 : loss : 0.049828, loss_ce: 0.018950
2022-01-11 22:46:35,709 iteration 1179 : loss : 0.056277, loss_ce: 0.024290
2022-01-11 22:46:37,339 iteration 1180 : loss : 0.059911, loss_ce: 0.026238
2022-01-11 22:46:38,881 iteration 1181 : loss : 0.052193, loss_ce: 0.017247
2022-01-11 22:46:40,452 iteration 1182 : loss : 0.054549, loss_ce: 0.021965
2022-01-11 22:46:42,036 iteration 1183 : loss : 0.039386, loss_ce: 0.017661
2022-01-11 22:46:43,659 iteration 1184 : loss : 0.081276, loss_ce: 0.034137
2022-01-11 22:46:45,243 iteration 1185 : loss : 0.050171, loss_ce: 0.026890
2022-01-11 22:46:46,862 iteration 1186 : loss : 0.046534, loss_ce: 0.015141
2022-01-11 22:46:48,400 iteration 1187 : loss : 0.037742, loss_ce: 0.017239
2022-01-11 22:46:50,030 iteration 1188 : loss : 0.062597, loss_ce: 0.023322
2022-01-11 22:46:51,698 iteration 1189 : loss : 0.088569, loss_ce: 0.022896
2022-01-11 22:46:51,699 Training Data Eval:
2022-01-11 22:46:59,669   Average segmentation loss on training set: 0.0448
2022-01-11 22:46:59,669 Validation Data Eval:
2022-01-11 22:47:02,415   Average segmentation loss on validation set: 0.0882
2022-01-11 22:47:03,975 iteration 1190 : loss : 0.074798, loss_ce: 0.031728
 18%|█████▎                        | 70/400 [35:12<2:53:41, 31.58s/it]2022-01-11 22:47:05,619 iteration 1191 : loss : 0.058741, loss_ce: 0.022829
2022-01-11 22:47:07,197 iteration 1192 : loss : 0.063845, loss_ce: 0.036013
2022-01-11 22:47:08,708 iteration 1193 : loss : 0.067517, loss_ce: 0.027333
2022-01-11 22:47:10,310 iteration 1194 : loss : 0.045635, loss_ce: 0.015607
2022-01-11 22:47:11,900 iteration 1195 : loss : 0.068547, loss_ce: 0.030456
2022-01-11 22:47:13,524 iteration 1196 : loss : 0.046678, loss_ce: 0.017318
2022-01-11 22:47:15,178 iteration 1197 : loss : 0.063852, loss_ce: 0.021724
2022-01-11 22:47:16,699 iteration 1198 : loss : 0.036733, loss_ce: 0.013278
2022-01-11 22:47:18,294 iteration 1199 : loss : 0.072507, loss_ce: 0.036629
2022-01-11 22:47:19,804 iteration 1200 : loss : 0.037891, loss_ce: 0.015398
2022-01-11 22:47:21,362 iteration 1201 : loss : 0.065081, loss_ce: 0.023893
2022-01-11 22:47:23,011 iteration 1202 : loss : 0.051187, loss_ce: 0.021160
2022-01-11 22:47:24,665 iteration 1203 : loss : 0.052490, loss_ce: 0.022915
2022-01-11 22:47:26,174 iteration 1204 : loss : 0.050761, loss_ce: 0.016369
2022-01-11 22:47:27,811 iteration 1205 : loss : 0.050724, loss_ce: 0.023068
2022-01-11 22:47:29,373 iteration 1206 : loss : 0.059374, loss_ce: 0.021550
2022-01-11 22:47:31,041 iteration 1207 : loss : 0.048332, loss_ce: 0.021394
 18%|█████▎                        | 71/400 [35:39<2:45:44, 30.23s/it]2022-01-11 22:47:32,656 iteration 1208 : loss : 0.050986, loss_ce: 0.022028
2022-01-11 22:47:34,268 iteration 1209 : loss : 0.045852, loss_ce: 0.018207
2022-01-11 22:47:35,791 iteration 1210 : loss : 0.042780, loss_ce: 0.016401
2022-01-11 22:47:37,452 iteration 1211 : loss : 0.057845, loss_ce: 0.029125
2022-01-11 22:47:39,030 iteration 1212 : loss : 0.042639, loss_ce: 0.017327
2022-01-11 22:47:40,511 iteration 1213 : loss : 0.037577, loss_ce: 0.013089
2022-01-11 22:47:42,192 iteration 1214 : loss : 0.062861, loss_ce: 0.024756
2022-01-11 22:47:43,778 iteration 1215 : loss : 0.038126, loss_ce: 0.015228
2022-01-11 22:47:45,361 iteration 1216 : loss : 0.043046, loss_ce: 0.015269
2022-01-11 22:47:47,086 iteration 1217 : loss : 0.080657, loss_ce: 0.033684
2022-01-11 22:47:48,619 iteration 1218 : loss : 0.036734, loss_ce: 0.015847
2022-01-11 22:47:50,378 iteration 1219 : loss : 0.059995, loss_ce: 0.030133
2022-01-11 22:47:51,974 iteration 1220 : loss : 0.049219, loss_ce: 0.019816
2022-01-11 22:47:53,657 iteration 1221 : loss : 0.046883, loss_ce: 0.016593
2022-01-11 22:47:55,224 iteration 1222 : loss : 0.049346, loss_ce: 0.017624
2022-01-11 22:47:56,794 iteration 1223 : loss : 0.047607, loss_ce: 0.017425
2022-01-11 22:47:58,364 iteration 1224 : loss : 0.078806, loss_ce: 0.025277
 18%|█████▍                        | 72/400 [36:07<2:40:29, 29.36s/it]2022-01-11 22:48:00,013 iteration 1225 : loss : 0.043155, loss_ce: 0.012528
2022-01-11 22:48:01,638 iteration 1226 : loss : 0.041670, loss_ce: 0.013841
2022-01-11 22:48:03,175 iteration 1227 : loss : 0.059652, loss_ce: 0.022520
2022-01-11 22:48:04,741 iteration 1228 : loss : 0.078672, loss_ce: 0.027738
2022-01-11 22:48:06,481 iteration 1229 : loss : 0.065234, loss_ce: 0.033658
2022-01-11 22:48:08,135 iteration 1230 : loss : 0.058164, loss_ce: 0.023612
2022-01-11 22:48:09,751 iteration 1231 : loss : 0.035003, loss_ce: 0.013896
2022-01-11 22:48:11,311 iteration 1232 : loss : 0.053482, loss_ce: 0.022126
2022-01-11 22:48:12,982 iteration 1233 : loss : 0.045354, loss_ce: 0.017470
2022-01-11 22:48:14,703 iteration 1234 : loss : 0.053505, loss_ce: 0.019781
2022-01-11 22:48:16,219 iteration 1235 : loss : 0.045556, loss_ce: 0.016873
2022-01-11 22:48:17,870 iteration 1236 : loss : 0.062515, loss_ce: 0.032064
2022-01-11 22:48:19,476 iteration 1237 : loss : 0.066667, loss_ce: 0.021998
2022-01-11 22:48:21,013 iteration 1238 : loss : 0.064177, loss_ce: 0.027062
2022-01-11 22:48:22,596 iteration 1239 : loss : 0.039687, loss_ce: 0.016352
2022-01-11 22:48:24,290 iteration 1240 : loss : 0.068102, loss_ce: 0.027615
2022-01-11 22:48:25,888 iteration 1241 : loss : 0.052532, loss_ce: 0.021525
 18%|█████▍                        | 73/400 [36:34<2:37:00, 28.81s/it]2022-01-11 22:48:27,595 iteration 1242 : loss : 0.046598, loss_ce: 0.024762
2022-01-11 22:48:29,211 iteration 1243 : loss : 0.043784, loss_ce: 0.017570
2022-01-11 22:48:30,934 iteration 1244 : loss : 0.056155, loss_ce: 0.020680
2022-01-11 22:48:32,620 iteration 1245 : loss : 0.048109, loss_ce: 0.018885
2022-01-11 22:48:34,215 iteration 1246 : loss : 0.051876, loss_ce: 0.018319
2022-01-11 22:48:36,005 iteration 1247 : loss : 0.089301, loss_ce: 0.031989
2022-01-11 22:48:37,587 iteration 1248 : loss : 0.038961, loss_ce: 0.014776
2022-01-11 22:48:39,182 iteration 1249 : loss : 0.072103, loss_ce: 0.041106
2022-01-11 22:48:40,788 iteration 1250 : loss : 0.082135, loss_ce: 0.018638
2022-01-11 22:48:42,466 iteration 1251 : loss : 0.052117, loss_ce: 0.022493
2022-01-11 22:48:44,019 iteration 1252 : loss : 0.072968, loss_ce: 0.021516
2022-01-11 22:48:45,610 iteration 1253 : loss : 0.055811, loss_ce: 0.022786
2022-01-11 22:48:47,221 iteration 1254 : loss : 0.040375, loss_ce: 0.013352
2022-01-11 22:48:48,947 iteration 1255 : loss : 0.075579, loss_ce: 0.031089
2022-01-11 22:48:50,549 iteration 1256 : loss : 0.055467, loss_ce: 0.020809
2022-01-11 22:48:52,054 iteration 1257 : loss : 0.049567, loss_ce: 0.019371
2022-01-11 22:48:53,548 iteration 1258 : loss : 0.049760, loss_ce: 0.022227
 18%|█████▌                        | 74/400 [37:02<2:34:38, 28.46s/it]2022-01-11 22:48:55,145 iteration 1259 : loss : 0.055392, loss_ce: 0.019136
2022-01-11 22:48:56,854 iteration 1260 : loss : 0.055247, loss_ce: 0.024572
2022-01-11 22:48:58,432 iteration 1261 : loss : 0.046266, loss_ce: 0.014615
2022-01-11 22:48:59,956 iteration 1262 : loss : 0.058554, loss_ce: 0.020396
2022-01-11 22:49:01,542 iteration 1263 : loss : 0.042700, loss_ce: 0.015662
2022-01-11 22:49:03,161 iteration 1264 : loss : 0.052850, loss_ce: 0.020381
2022-01-11 22:49:04,754 iteration 1265 : loss : 0.047990, loss_ce: 0.019669
2022-01-11 22:49:06,300 iteration 1266 : loss : 0.050746, loss_ce: 0.020881
2022-01-11 22:49:07,845 iteration 1267 : loss : 0.061493, loss_ce: 0.025485
2022-01-11 22:49:09,416 iteration 1268 : loss : 0.047105, loss_ce: 0.017079
2022-01-11 22:49:10,996 iteration 1269 : loss : 0.051194, loss_ce: 0.023593
2022-01-11 22:49:12,583 iteration 1270 : loss : 0.029541, loss_ce: 0.012461
2022-01-11 22:49:14,267 iteration 1271 : loss : 0.070202, loss_ce: 0.033454
2022-01-11 22:49:15,762 iteration 1272 : loss : 0.049245, loss_ce: 0.015387
2022-01-11 22:49:17,312 iteration 1273 : loss : 0.059264, loss_ce: 0.019249
2022-01-11 22:49:18,943 iteration 1274 : loss : 0.048395, loss_ce: 0.019625
2022-01-11 22:49:18,943 Training Data Eval:
2022-01-11 22:49:26,932   Average segmentation loss on training set: 0.0387
2022-01-11 22:49:26,933 Validation Data Eval:
2022-01-11 22:49:29,682   Average segmentation loss on validation set: 0.0751
2022-01-11 22:49:31,223 iteration 1275 : loss : 0.057432, loss_ce: 0.026092
 19%|█████▋                        | 75/400 [37:39<2:49:09, 31.23s/it]2022-01-11 22:49:32,832 iteration 1276 : loss : 0.036726, loss_ce: 0.015441
2022-01-11 22:49:34,545 iteration 1277 : loss : 0.068019, loss_ce: 0.024779
2022-01-11 22:49:36,145 iteration 1278 : loss : 0.116788, loss_ce: 0.044435
2022-01-11 22:49:37,722 iteration 1279 : loss : 0.049125, loss_ce: 0.017512
2022-01-11 22:49:39,346 iteration 1280 : loss : 0.049809, loss_ce: 0.022356
2022-01-11 22:49:40,890 iteration 1281 : loss : 0.050678, loss_ce: 0.021475
2022-01-11 22:49:42,475 iteration 1282 : loss : 0.048978, loss_ce: 0.019155
2022-01-11 22:49:43,994 iteration 1283 : loss : 0.054299, loss_ce: 0.016688
2022-01-11 22:49:45,570 iteration 1284 : loss : 0.038851, loss_ce: 0.014831
2022-01-11 22:49:47,097 iteration 1285 : loss : 0.053506, loss_ce: 0.016400
2022-01-11 22:49:48,751 iteration 1286 : loss : 0.058154, loss_ce: 0.019004
2022-01-11 22:49:50,388 iteration 1287 : loss : 0.045952, loss_ce: 0.020443
2022-01-11 22:49:51,932 iteration 1288 : loss : 0.046766, loss_ce: 0.018564
2022-01-11 22:49:53,513 iteration 1289 : loss : 0.092010, loss_ce: 0.038642
2022-01-11 22:49:55,073 iteration 1290 : loss : 0.086504, loss_ce: 0.029982
2022-01-11 22:49:56,613 iteration 1291 : loss : 0.051362, loss_ce: 0.018162
2022-01-11 22:49:58,158 iteration 1292 : loss : 0.096084, loss_ce: 0.058975
 19%|█████▋                        | 76/400 [38:06<2:41:39, 29.94s/it]2022-01-11 22:49:59,889 iteration 1293 : loss : 0.062489, loss_ce: 0.026548
2022-01-11 22:50:01,411 iteration 1294 : loss : 0.038159, loss_ce: 0.016679
2022-01-11 22:50:03,081 iteration 1295 : loss : 0.068227, loss_ce: 0.022014
2022-01-11 22:50:04,700 iteration 1296 : loss : 0.079913, loss_ce: 0.025530
2022-01-11 22:50:06,178 iteration 1297 : loss : 0.036654, loss_ce: 0.014425
2022-01-11 22:50:07,846 iteration 1298 : loss : 0.075340, loss_ce: 0.032095
2022-01-11 22:50:09,410 iteration 1299 : loss : 0.054983, loss_ce: 0.025230
2022-01-11 22:50:11,107 iteration 1300 : loss : 0.062217, loss_ce: 0.021335
2022-01-11 22:50:12,784 iteration 1301 : loss : 0.092777, loss_ce: 0.034112
2022-01-11 22:50:14,314 iteration 1302 : loss : 0.087463, loss_ce: 0.028898
2022-01-11 22:50:15,958 iteration 1303 : loss : 0.062676, loss_ce: 0.018760
2022-01-11 22:50:17,556 iteration 1304 : loss : 0.048981, loss_ce: 0.022729
2022-01-11 22:50:19,232 iteration 1305 : loss : 0.066280, loss_ce: 0.026350
2022-01-11 22:50:20,861 iteration 1306 : loss : 0.052822, loss_ce: 0.022000
2022-01-11 22:50:22,434 iteration 1307 : loss : 0.044470, loss_ce: 0.015655
2022-01-11 22:50:24,033 iteration 1308 : loss : 0.085555, loss_ce: 0.045067
2022-01-11 22:50:25,586 iteration 1309 : loss : 0.049609, loss_ce: 0.019093
 19%|█████▊                        | 77/400 [38:34<2:37:07, 29.19s/it]2022-01-11 22:50:27,206 iteration 1310 : loss : 0.055766, loss_ce: 0.021058
2022-01-11 22:50:28,922 iteration 1311 : loss : 0.066401, loss_ce: 0.022850
2022-01-11 22:50:30,525 iteration 1312 : loss : 0.052546, loss_ce: 0.017737
2022-01-11 22:50:32,108 iteration 1313 : loss : 0.050248, loss_ce: 0.021895
2022-01-11 22:50:33,755 iteration 1314 : loss : 0.058476, loss_ce: 0.028832
2022-01-11 22:50:35,433 iteration 1315 : loss : 0.047556, loss_ce: 0.026407
2022-01-11 22:50:36,997 iteration 1316 : loss : 0.045915, loss_ce: 0.020855
2022-01-11 22:50:38,534 iteration 1317 : loss : 0.046997, loss_ce: 0.020469
2022-01-11 22:50:40,070 iteration 1318 : loss : 0.060692, loss_ce: 0.025947
2022-01-11 22:50:41,686 iteration 1319 : loss : 0.061989, loss_ce: 0.024046
2022-01-11 22:50:43,212 iteration 1320 : loss : 0.051440, loss_ce: 0.021023
2022-01-11 22:50:44,799 iteration 1321 : loss : 0.055112, loss_ce: 0.024825
2022-01-11 22:50:46,365 iteration 1322 : loss : 0.048363, loss_ce: 0.020801
2022-01-11 22:50:48,012 iteration 1323 : loss : 0.052540, loss_ce: 0.016418
2022-01-11 22:50:49,694 iteration 1324 : loss : 0.049563, loss_ce: 0.019185
2022-01-11 22:50:51,272 iteration 1325 : loss : 0.075898, loss_ce: 0.034544
2022-01-11 22:50:52,873 iteration 1326 : loss : 0.047481, loss_ce: 0.019764
 20%|█████▊                        | 78/400 [39:01<2:33:34, 28.62s/it]2022-01-11 22:50:54,496 iteration 1327 : loss : 0.033402, loss_ce: 0.012686
2022-01-11 22:50:56,104 iteration 1328 : loss : 0.053834, loss_ce: 0.017309
2022-01-11 22:50:57,713 iteration 1329 : loss : 0.058402, loss_ce: 0.023879
2022-01-11 22:50:59,397 iteration 1330 : loss : 0.046279, loss_ce: 0.014393
2022-01-11 22:51:01,012 iteration 1331 : loss : 0.053628, loss_ce: 0.026281
2022-01-11 22:51:02,584 iteration 1332 : loss : 0.039416, loss_ce: 0.014355
2022-01-11 22:51:04,162 iteration 1333 : loss : 0.043347, loss_ce: 0.019147
2022-01-11 22:51:05,811 iteration 1334 : loss : 0.039581, loss_ce: 0.016854
2022-01-11 22:51:07,444 iteration 1335 : loss : 0.036419, loss_ce: 0.014576
2022-01-11 22:51:08,996 iteration 1336 : loss : 0.044515, loss_ce: 0.020464
2022-01-11 22:51:10,661 iteration 1337 : loss : 0.081201, loss_ce: 0.031723
2022-01-11 22:51:12,231 iteration 1338 : loss : 0.032394, loss_ce: 0.013901
2022-01-11 22:51:13,786 iteration 1339 : loss : 0.061745, loss_ce: 0.021928
2022-01-11 22:51:15,334 iteration 1340 : loss : 0.036403, loss_ce: 0.015935
2022-01-11 22:51:16,962 iteration 1341 : loss : 0.050756, loss_ce: 0.019102
2022-01-11 22:51:18,643 iteration 1342 : loss : 0.055615, loss_ce: 0.019527
2022-01-11 22:51:20,152 iteration 1343 : loss : 0.051274, loss_ce: 0.018094
 20%|█████▉                        | 79/400 [39:28<2:30:56, 28.21s/it]2022-01-11 22:51:21,802 iteration 1344 : loss : 0.038962, loss_ce: 0.013961
2022-01-11 22:51:23,344 iteration 1345 : loss : 0.040179, loss_ce: 0.011381
2022-01-11 22:51:24,965 iteration 1346 : loss : 0.058958, loss_ce: 0.023023
2022-01-11 22:51:26,542 iteration 1347 : loss : 0.052563, loss_ce: 0.022562
2022-01-11 22:51:28,078 iteration 1348 : loss : 0.076235, loss_ce: 0.033213
2022-01-11 22:51:29,659 iteration 1349 : loss : 0.057488, loss_ce: 0.020280
2022-01-11 22:51:31,221 iteration 1350 : loss : 0.062722, loss_ce: 0.019477
2022-01-11 22:51:32,876 iteration 1351 : loss : 0.055071, loss_ce: 0.021498
2022-01-11 22:51:34,405 iteration 1352 : loss : 0.047465, loss_ce: 0.022863
2022-01-11 22:51:35,983 iteration 1353 : loss : 0.051366, loss_ce: 0.024192
2022-01-11 22:51:37,559 iteration 1354 : loss : 0.043847, loss_ce: 0.015461
2022-01-11 22:51:39,066 iteration 1355 : loss : 0.055443, loss_ce: 0.017122
2022-01-11 22:51:40,713 iteration 1356 : loss : 0.067318, loss_ce: 0.025151
2022-01-11 22:51:42,224 iteration 1357 : loss : 0.041568, loss_ce: 0.019920
2022-01-11 22:51:43,776 iteration 1358 : loss : 0.060255, loss_ce: 0.029881
2022-01-11 22:51:45,447 iteration 1359 : loss : 0.058410, loss_ce: 0.031904
2022-01-11 22:51:45,447 Training Data Eval:
2022-01-11 22:51:53,383   Average segmentation loss on training set: 0.0375
2022-01-11 22:51:53,384 Validation Data Eval:
2022-01-11 22:51:56,121   Average segmentation loss on validation set: 0.0810
2022-01-11 22:51:57,717 iteration 1360 : loss : 0.039351, loss_ce: 0.014527
 20%|██████                        | 80/400 [40:06<2:45:25, 31.02s/it]2022-01-11 22:51:59,414 iteration 1361 : loss : 0.069959, loss_ce: 0.020562
2022-01-11 22:52:00,988 iteration 1362 : loss : 0.057300, loss_ce: 0.018535
2022-01-11 22:52:02,593 iteration 1363 : loss : 0.039114, loss_ce: 0.015369
2022-01-11 22:52:04,098 iteration 1364 : loss : 0.055764, loss_ce: 0.026939
2022-01-11 22:52:05,642 iteration 1365 : loss : 0.049380, loss_ce: 0.014288
2022-01-11 22:52:07,306 iteration 1366 : loss : 0.078870, loss_ce: 0.032772
2022-01-11 22:52:08,916 iteration 1367 : loss : 0.063252, loss_ce: 0.039006
2022-01-11 22:52:10,693 iteration 1368 : loss : 0.057086, loss_ce: 0.021365
2022-01-11 22:52:12,274 iteration 1369 : loss : 0.043917, loss_ce: 0.019013
2022-01-11 22:52:13,817 iteration 1370 : loss : 0.061851, loss_ce: 0.029360
2022-01-11 22:52:15,398 iteration 1371 : loss : 0.049356, loss_ce: 0.019165
2022-01-11 22:52:17,134 iteration 1372 : loss : 0.072157, loss_ce: 0.029383
2022-01-11 22:52:18,711 iteration 1373 : loss : 0.057817, loss_ce: 0.025266
2022-01-11 22:52:20,259 iteration 1374 : loss : 0.056295, loss_ce: 0.023605
2022-01-11 22:52:21,818 iteration 1375 : loss : 0.032444, loss_ce: 0.013098
2022-01-11 22:52:23,402 iteration 1376 : loss : 0.044450, loss_ce: 0.015676
2022-01-11 22:52:25,052 iteration 1377 : loss : 0.053556, loss_ce: 0.030629
 20%|██████                        | 81/400 [40:33<2:39:02, 29.92s/it]2022-01-11 22:52:26,789 iteration 1378 : loss : 0.060983, loss_ce: 0.036699
2022-01-11 22:52:28,441 iteration 1379 : loss : 0.067551, loss_ce: 0.027954
2022-01-11 22:52:30,038 iteration 1380 : loss : 0.079574, loss_ce: 0.027778
2022-01-11 22:52:31,684 iteration 1381 : loss : 0.062024, loss_ce: 0.030797
2022-01-11 22:52:33,306 iteration 1382 : loss : 0.066868, loss_ce: 0.025402
2022-01-11 22:52:34,844 iteration 1383 : loss : 0.067145, loss_ce: 0.021865
2022-01-11 22:52:36,477 iteration 1384 : loss : 0.066098, loss_ce: 0.031684
2022-01-11 22:52:38,092 iteration 1385 : loss : 0.037951, loss_ce: 0.017469
2022-01-11 22:52:39,675 iteration 1386 : loss : 0.062675, loss_ce: 0.023440
2022-01-11 22:52:41,389 iteration 1387 : loss : 0.049867, loss_ce: 0.018546
2022-01-11 22:52:42,973 iteration 1388 : loss : 0.045891, loss_ce: 0.017232
2022-01-11 22:52:44,549 iteration 1389 : loss : 0.053130, loss_ce: 0.020896
2022-01-11 22:52:46,175 iteration 1390 : loss : 0.049231, loss_ce: 0.020476
2022-01-11 22:52:47,783 iteration 1391 : loss : 0.045417, loss_ce: 0.013144
2022-01-11 22:52:49,394 iteration 1392 : loss : 0.065945, loss_ce: 0.025395
2022-01-11 22:52:50,964 iteration 1393 : loss : 0.039143, loss_ce: 0.014322
2022-01-11 22:52:52,545 iteration 1394 : loss : 0.043361, loss_ce: 0.019749
 20%|██████▏                       | 82/400 [41:01<2:34:42, 29.19s/it]2022-01-11 22:52:54,173 iteration 1395 : loss : 0.032972, loss_ce: 0.012676
2022-01-11 22:52:55,728 iteration 1396 : loss : 0.039439, loss_ce: 0.016748
2022-01-11 22:52:57,334 iteration 1397 : loss : 0.057102, loss_ce: 0.019065
2022-01-11 22:52:59,014 iteration 1398 : loss : 0.031185, loss_ce: 0.013401
2022-01-11 22:53:00,638 iteration 1399 : loss : 0.062088, loss_ce: 0.028935
2022-01-11 22:53:02,204 iteration 1400 : loss : 0.070044, loss_ce: 0.031133
2022-01-11 22:53:03,766 iteration 1401 : loss : 0.053314, loss_ce: 0.019221
2022-01-11 22:53:05,278 iteration 1402 : loss : 0.042277, loss_ce: 0.017361
2022-01-11 22:53:06,976 iteration 1403 : loss : 0.055744, loss_ce: 0.020003
2022-01-11 22:53:08,620 iteration 1404 : loss : 0.048634, loss_ce: 0.018179
2022-01-11 22:53:10,250 iteration 1405 : loss : 0.053104, loss_ce: 0.026312
2022-01-11 22:53:11,860 iteration 1406 : loss : 0.054645, loss_ce: 0.021457
2022-01-11 22:53:13,398 iteration 1407 : loss : 0.042139, loss_ce: 0.020119
2022-01-11 22:53:14,955 iteration 1408 : loss : 0.055135, loss_ce: 0.016339
2022-01-11 22:53:16,639 iteration 1409 : loss : 0.092830, loss_ce: 0.031584
2022-01-11 22:53:18,262 iteration 1410 : loss : 0.059271, loss_ce: 0.018574
2022-01-11 22:53:19,833 iteration 1411 : loss : 0.053511, loss_ce: 0.022181
 21%|██████▏                       | 83/400 [41:28<2:31:10, 28.62s/it]2022-01-11 22:53:21,372 iteration 1412 : loss : 0.049824, loss_ce: 0.015420
2022-01-11 22:53:23,007 iteration 1413 : loss : 0.037579, loss_ce: 0.013327
2022-01-11 22:53:24,594 iteration 1414 : loss : 0.037649, loss_ce: 0.018093
2022-01-11 22:53:26,110 iteration 1415 : loss : 0.042259, loss_ce: 0.015966
2022-01-11 22:53:27,700 iteration 1416 : loss : 0.038807, loss_ce: 0.010868
2022-01-11 22:53:29,332 iteration 1417 : loss : 0.048287, loss_ce: 0.018288
2022-01-11 22:53:30,845 iteration 1418 : loss : 0.036528, loss_ce: 0.012118
2022-01-11 22:53:32,559 iteration 1419 : loss : 0.062028, loss_ce: 0.020705
2022-01-11 22:53:34,208 iteration 1420 : loss : 0.055882, loss_ce: 0.018802
2022-01-11 22:53:35,755 iteration 1421 : loss : 0.039634, loss_ce: 0.016592
2022-01-11 22:53:37,336 iteration 1422 : loss : 0.059978, loss_ce: 0.021782
2022-01-11 22:53:38,884 iteration 1423 : loss : 0.064894, loss_ce: 0.025206
2022-01-11 22:53:40,511 iteration 1424 : loss : 0.045636, loss_ce: 0.016101
2022-01-11 22:53:42,173 iteration 1425 : loss : 0.063431, loss_ce: 0.025830
2022-01-11 22:53:43,633 iteration 1426 : loss : 0.041094, loss_ce: 0.016098
2022-01-11 22:53:45,193 iteration 1427 : loss : 0.048394, loss_ce: 0.021221
2022-01-11 22:53:46,781 iteration 1428 : loss : 0.049206, loss_ce: 0.019224
 21%|██████▎                       | 84/400 [41:55<2:28:04, 28.12s/it]2022-01-11 22:53:48,516 iteration 1429 : loss : 0.058498, loss_ce: 0.023844
2022-01-11 22:53:50,061 iteration 1430 : loss : 0.065643, loss_ce: 0.023949
2022-01-11 22:53:51,641 iteration 1431 : loss : 0.059879, loss_ce: 0.026090
2022-01-11 22:53:53,246 iteration 1432 : loss : 0.050648, loss_ce: 0.021756
2022-01-11 22:53:54,934 iteration 1433 : loss : 0.068214, loss_ce: 0.027907
2022-01-11 22:53:56,583 iteration 1434 : loss : 0.047846, loss_ce: 0.018311
2022-01-11 22:53:58,068 iteration 1435 : loss : 0.038209, loss_ce: 0.011605
2022-01-11 22:53:59,720 iteration 1436 : loss : 0.042677, loss_ce: 0.018659
2022-01-11 22:54:01,234 iteration 1437 : loss : 0.088024, loss_ce: 0.020859
2022-01-11 22:54:02,724 iteration 1438 : loss : 0.043429, loss_ce: 0.020712
2022-01-11 22:54:04,298 iteration 1439 : loss : 0.037139, loss_ce: 0.010779
2022-01-11 22:54:05,955 iteration 1440 : loss : 0.073930, loss_ce: 0.038230
2022-01-11 22:54:07,505 iteration 1441 : loss : 0.041871, loss_ce: 0.019272
2022-01-11 22:54:09,035 iteration 1442 : loss : 0.044424, loss_ce: 0.019115
2022-01-11 22:54:10,649 iteration 1443 : loss : 0.044307, loss_ce: 0.017227
2022-01-11 22:54:12,301 iteration 1444 : loss : 0.030250, loss_ce: 0.012355
2022-01-11 22:54:12,302 Training Data Eval:
2022-01-11 22:54:20,255   Average segmentation loss on training set: 0.0327
2022-01-11 22:54:20,255 Validation Data Eval:
2022-01-11 22:54:22,996   Average segmentation loss on validation set: 0.0744
2022-01-11 22:54:24,615 iteration 1445 : loss : 0.046467, loss_ce: 0.015720
 21%|██████▍                       | 85/400 [42:33<2:42:55, 31.03s/it]2022-01-11 22:54:26,296 iteration 1446 : loss : 0.048230, loss_ce: 0.015488
2022-01-11 22:54:27,928 iteration 1447 : loss : 0.054004, loss_ce: 0.021827
2022-01-11 22:54:29,504 iteration 1448 : loss : 0.035677, loss_ce: 0.016386
2022-01-11 22:54:31,102 iteration 1449 : loss : 0.053610, loss_ce: 0.017993
2022-01-11 22:54:32,592 iteration 1450 : loss : 0.051914, loss_ce: 0.020366
2022-01-11 22:54:34,116 iteration 1451 : loss : 0.039381, loss_ce: 0.014411
2022-01-11 22:54:35,706 iteration 1452 : loss : 0.079600, loss_ce: 0.019067
2022-01-11 22:54:37,347 iteration 1453 : loss : 0.063698, loss_ce: 0.020906
2022-01-11 22:54:38,890 iteration 1454 : loss : 0.043190, loss_ce: 0.017820
2022-01-11 22:54:40,439 iteration 1455 : loss : 0.048558, loss_ce: 0.018152
2022-01-11 22:54:42,028 iteration 1456 : loss : 0.060125, loss_ce: 0.020767
2022-01-11 22:54:43,619 iteration 1457 : loss : 0.044388, loss_ce: 0.017328
2022-01-11 22:54:45,191 iteration 1458 : loss : 0.037587, loss_ce: 0.013626
2022-01-11 22:54:46,670 iteration 1459 : loss : 0.036205, loss_ce: 0.014922
2022-01-11 22:54:48,312 iteration 1460 : loss : 0.106991, loss_ce: 0.056987
2022-01-11 22:54:49,887 iteration 1461 : loss : 0.045318, loss_ce: 0.015794
2022-01-11 22:54:51,461 iteration 1462 : loss : 0.045672, loss_ce: 0.017287
 22%|██████▍                       | 86/400 [43:00<2:35:50, 29.78s/it]2022-01-11 22:54:53,073 iteration 1463 : loss : 0.042493, loss_ce: 0.019385
2022-01-11 22:54:54,702 iteration 1464 : loss : 0.039684, loss_ce: 0.013589
2022-01-11 22:54:56,360 iteration 1465 : loss : 0.075394, loss_ce: 0.030132
2022-01-11 22:54:57,956 iteration 1466 : loss : 0.043639, loss_ce: 0.013048
2022-01-11 22:54:59,533 iteration 1467 : loss : 0.050971, loss_ce: 0.018810
2022-01-11 22:55:01,140 iteration 1468 : loss : 0.053311, loss_ce: 0.018110
2022-01-11 22:55:02,764 iteration 1469 : loss : 0.029387, loss_ce: 0.013443
2022-01-11 22:55:04,367 iteration 1470 : loss : 0.036210, loss_ce: 0.014122
2022-01-11 22:55:05,885 iteration 1471 : loss : 0.034954, loss_ce: 0.011307
2022-01-11 22:55:07,458 iteration 1472 : loss : 0.046678, loss_ce: 0.014840
2022-01-11 22:55:09,114 iteration 1473 : loss : 0.054476, loss_ce: 0.026229
2022-01-11 22:55:10,644 iteration 1474 : loss : 0.044619, loss_ce: 0.018933
2022-01-11 22:55:12,220 iteration 1475 : loss : 0.053013, loss_ce: 0.017796
2022-01-11 22:55:13,858 iteration 1476 : loss : 0.083063, loss_ce: 0.052439
2022-01-11 22:55:15,443 iteration 1477 : loss : 0.044945, loss_ce: 0.013973
2022-01-11 22:55:17,085 iteration 1478 : loss : 0.046450, loss_ce: 0.021115
2022-01-11 22:55:18,695 iteration 1479 : loss : 0.063395, loss_ce: 0.029889
 22%|██████▌                       | 87/400 [43:27<2:31:21, 29.01s/it]2022-01-11 22:55:20,322 iteration 1480 : loss : 0.043992, loss_ce: 0.023289
2022-01-11 22:55:21,986 iteration 1481 : loss : 0.035077, loss_ce: 0.014944
2022-01-11 22:55:23,548 iteration 1482 : loss : 0.034956, loss_ce: 0.014686
2022-01-11 22:55:25,066 iteration 1483 : loss : 0.030860, loss_ce: 0.012575
2022-01-11 22:55:26,596 iteration 1484 : loss : 0.036959, loss_ce: 0.015964
2022-01-11 22:55:28,138 iteration 1485 : loss : 0.040366, loss_ce: 0.014314
2022-01-11 22:55:29,699 iteration 1486 : loss : 0.042078, loss_ce: 0.015270
2022-01-11 22:55:31,352 iteration 1487 : loss : 0.045198, loss_ce: 0.016482
2022-01-11 22:55:32,919 iteration 1488 : loss : 0.040951, loss_ce: 0.014682
2022-01-11 22:55:34,530 iteration 1489 : loss : 0.035987, loss_ce: 0.013723
2022-01-11 22:55:36,115 iteration 1490 : loss : 0.034651, loss_ce: 0.014621
2022-01-11 22:55:37,640 iteration 1491 : loss : 0.041624, loss_ce: 0.014463
2022-01-11 22:55:39,254 iteration 1492 : loss : 0.039263, loss_ce: 0.015473
2022-01-11 22:55:40,952 iteration 1493 : loss : 0.062113, loss_ce: 0.025227
2022-01-11 22:55:42,575 iteration 1494 : loss : 0.040649, loss_ce: 0.013942
2022-01-11 22:55:44,168 iteration 1495 : loss : 0.044041, loss_ce: 0.014166
2022-01-11 22:55:45,759 iteration 1496 : loss : 0.041673, loss_ce: 0.016930
 22%|██████▌                       | 88/400 [43:54<2:27:49, 28.43s/it]2022-01-11 22:55:47,347 iteration 1497 : loss : 0.031690, loss_ce: 0.015331
2022-01-11 22:55:48,818 iteration 1498 : loss : 0.032181, loss_ce: 0.014411
2022-01-11 22:55:50,331 iteration 1499 : loss : 0.049387, loss_ce: 0.020466
2022-01-11 22:55:52,046 iteration 1500 : loss : 0.068222, loss_ce: 0.026328
2022-01-11 22:55:53,552 iteration 1501 : loss : 0.032513, loss_ce: 0.010580
2022-01-11 22:55:55,139 iteration 1502 : loss : 0.041264, loss_ce: 0.014129
2022-01-11 22:55:56,800 iteration 1503 : loss : 0.046469, loss_ce: 0.015707
2022-01-11 22:55:58,364 iteration 1504 : loss : 0.050468, loss_ce: 0.019840
2022-01-11 22:55:59,900 iteration 1505 : loss : 0.030946, loss_ce: 0.012416
2022-01-11 22:56:01,528 iteration 1506 : loss : 0.040655, loss_ce: 0.014225
2022-01-11 22:56:03,093 iteration 1507 : loss : 0.039992, loss_ce: 0.016467
2022-01-11 22:56:04,685 iteration 1508 : loss : 0.112140, loss_ce: 0.022337
2022-01-11 22:56:06,257 iteration 1509 : loss : 0.034845, loss_ce: 0.014854
2022-01-11 22:56:07,810 iteration 1510 : loss : 0.046082, loss_ce: 0.015076
2022-01-11 22:56:09,495 iteration 1511 : loss : 0.051529, loss_ce: 0.029103
2022-01-11 22:56:11,083 iteration 1512 : loss : 0.042954, loss_ce: 0.015010
2022-01-11 22:56:12,682 iteration 1513 : loss : 0.066842, loss_ce: 0.027918
 22%|██████▋                       | 89/400 [44:21<2:25:00, 27.98s/it]2022-01-11 22:56:14,334 iteration 1514 : loss : 0.057290, loss_ce: 0.022142
2022-01-11 22:56:15,925 iteration 1515 : loss : 0.044556, loss_ce: 0.022795
2022-01-11 22:56:17,542 iteration 1516 : loss : 0.046161, loss_ce: 0.026292
2022-01-11 22:56:19,089 iteration 1517 : loss : 0.041020, loss_ce: 0.017121
2022-01-11 22:56:20,595 iteration 1518 : loss : 0.041342, loss_ce: 0.016827
2022-01-11 22:56:22,164 iteration 1519 : loss : 0.047531, loss_ce: 0.017366
2022-01-11 22:56:23,905 iteration 1520 : loss : 0.063500, loss_ce: 0.029438
2022-01-11 22:56:25,466 iteration 1521 : loss : 0.039346, loss_ce: 0.013790
2022-01-11 22:56:27,148 iteration 1522 : loss : 0.040886, loss_ce: 0.015299
2022-01-11 22:56:28,763 iteration 1523 : loss : 0.043795, loss_ce: 0.020608
2022-01-11 22:56:30,307 iteration 1524 : loss : 0.034131, loss_ce: 0.012541
2022-01-11 22:56:31,900 iteration 1525 : loss : 0.041753, loss_ce: 0.020918
2022-01-11 22:56:33,484 iteration 1526 : loss : 0.057957, loss_ce: 0.025823
2022-01-11 22:56:35,183 iteration 1527 : loss : 0.039554, loss_ce: 0.017064
2022-01-11 22:56:36,828 iteration 1528 : loss : 0.049740, loss_ce: 0.018749
2022-01-11 22:56:38,421 iteration 1529 : loss : 0.049357, loss_ce: 0.016757
2022-01-11 22:56:38,422 Training Data Eval:
2022-01-11 22:56:46,415   Average segmentation loss on training set: 0.0325
2022-01-11 22:56:46,416 Validation Data Eval:
2022-01-11 22:56:49,165   Average segmentation loss on validation set: 0.1028
2022-01-11 22:56:50,713 iteration 1530 : loss : 0.045148, loss_ce: 0.020306
 22%|██████▊                       | 90/400 [44:59<2:40:08, 31.00s/it]2022-01-11 22:56:52,383 iteration 1531 : loss : 0.045236, loss_ce: 0.020630
2022-01-11 22:56:54,026 iteration 1532 : loss : 0.067422, loss_ce: 0.024218
2022-01-11 22:56:55,565 iteration 1533 : loss : 0.035153, loss_ce: 0.012335
2022-01-11 22:56:57,137 iteration 1534 : loss : 0.035728, loss_ce: 0.018198
2022-01-11 22:56:58,668 iteration 1535 : loss : 0.039939, loss_ce: 0.017601
2022-01-11 22:57:00,278 iteration 1536 : loss : 0.036464, loss_ce: 0.011621
2022-01-11 22:57:01,828 iteration 1537 : loss : 0.046665, loss_ce: 0.024750
2022-01-11 22:57:03,302 iteration 1538 : loss : 0.059575, loss_ce: 0.015243
2022-01-11 22:57:04,836 iteration 1539 : loss : 0.037083, loss_ce: 0.016403
2022-01-11 22:57:06,442 iteration 1540 : loss : 0.042248, loss_ce: 0.018598
2022-01-11 22:57:08,091 iteration 1541 : loss : 0.043923, loss_ce: 0.016985
2022-01-11 22:57:09,794 iteration 1542 : loss : 0.042463, loss_ce: 0.017330
2022-01-11 22:57:11,415 iteration 1543 : loss : 0.066482, loss_ce: 0.013005
2022-01-11 22:57:13,025 iteration 1544 : loss : 0.036538, loss_ce: 0.015697
2022-01-11 22:57:14,555 iteration 1545 : loss : 0.048401, loss_ce: 0.015894
2022-01-11 22:57:16,120 iteration 1546 : loss : 0.050469, loss_ce: 0.015702
2022-01-11 22:57:17,748 iteration 1547 : loss : 0.044078, loss_ce: 0.019845
 23%|██████▊                       | 91/400 [45:26<2:33:30, 29.81s/it]2022-01-11 22:57:19,374 iteration 1548 : loss : 0.046819, loss_ce: 0.014967
2022-01-11 22:57:20,898 iteration 1549 : loss : 0.046439, loss_ce: 0.015762
2022-01-11 22:57:22,526 iteration 1550 : loss : 0.042214, loss_ce: 0.012425
2022-01-11 22:57:24,150 iteration 1551 : loss : 0.053900, loss_ce: 0.019478
2022-01-11 22:57:25,714 iteration 1552 : loss : 0.034184, loss_ce: 0.011109
2022-01-11 22:57:27,355 iteration 1553 : loss : 0.085308, loss_ce: 0.035020
2022-01-11 22:57:28,852 iteration 1554 : loss : 0.056107, loss_ce: 0.025733
2022-01-11 22:57:30,400 iteration 1555 : loss : 0.047464, loss_ce: 0.014249
2022-01-11 22:57:32,088 iteration 1556 : loss : 0.058358, loss_ce: 0.025970
2022-01-11 22:57:33,656 iteration 1557 : loss : 0.054950, loss_ce: 0.019572
2022-01-11 22:57:35,174 iteration 1558 : loss : 0.039257, loss_ce: 0.015615
2022-01-11 22:57:36,847 iteration 1559 : loss : 0.065592, loss_ce: 0.027496
2022-01-11 22:57:38,407 iteration 1560 : loss : 0.036563, loss_ce: 0.018800
2022-01-11 22:57:39,992 iteration 1561 : loss : 0.060013, loss_ce: 0.030165
2022-01-11 22:57:41,587 iteration 1562 : loss : 0.045851, loss_ce: 0.024042
2022-01-11 22:57:43,217 iteration 1563 : loss : 0.053173, loss_ce: 0.020232
2022-01-11 22:57:44,791 iteration 1564 : loss : 0.047139, loss_ce: 0.017784
 23%|██████▉                       | 92/400 [45:53<2:28:45, 28.98s/it]2022-01-11 22:57:46,373 iteration 1565 : loss : 0.037970, loss_ce: 0.020343
2022-01-11 22:57:48,010 iteration 1566 : loss : 0.054629, loss_ce: 0.015647
2022-01-11 22:57:49,611 iteration 1567 : loss : 0.047564, loss_ce: 0.021650
2022-01-11 22:57:51,320 iteration 1568 : loss : 0.040608, loss_ce: 0.014212
2022-01-11 22:57:52,913 iteration 1569 : loss : 0.048628, loss_ce: 0.021872
2022-01-11 22:57:54,517 iteration 1570 : loss : 0.047083, loss_ce: 0.016124
2022-01-11 22:57:56,129 iteration 1571 : loss : 0.061247, loss_ce: 0.023991
2022-01-11 22:57:57,821 iteration 1572 : loss : 0.074509, loss_ce: 0.027511
2022-01-11 22:57:59,325 iteration 1573 : loss : 0.035979, loss_ce: 0.016822
2022-01-11 22:58:00,948 iteration 1574 : loss : 0.036805, loss_ce: 0.013472
2022-01-11 22:58:02,614 iteration 1575 : loss : 0.045349, loss_ce: 0.020567
2022-01-11 22:58:04,286 iteration 1576 : loss : 0.030692, loss_ce: 0.013092
2022-01-11 22:58:05,802 iteration 1577 : loss : 0.037826, loss_ce: 0.013154
2022-01-11 22:58:07,363 iteration 1578 : loss : 0.040483, loss_ce: 0.013920
2022-01-11 22:58:09,017 iteration 1579 : loss : 0.056620, loss_ce: 0.022208
2022-01-11 22:58:10,652 iteration 1580 : loss : 0.080089, loss_ce: 0.024539
2022-01-11 22:58:12,395 iteration 1581 : loss : 0.037080, loss_ce: 0.015020
 23%|██████▉                       | 93/400 [46:21<2:26:09, 28.57s/it]2022-01-11 22:58:14,063 iteration 1582 : loss : 0.038259, loss_ce: 0.014054
2022-01-11 22:58:15,688 iteration 1583 : loss : 0.058682, loss_ce: 0.023558
2022-01-11 22:58:17,207 iteration 1584 : loss : 0.049478, loss_ce: 0.016341
2022-01-11 22:58:18,811 iteration 1585 : loss : 0.078624, loss_ce: 0.017841
2022-01-11 22:58:20,486 iteration 1586 : loss : 0.040780, loss_ce: 0.012557
2022-01-11 22:58:22,080 iteration 1587 : loss : 0.036207, loss_ce: 0.011409
2022-01-11 22:58:23,747 iteration 1588 : loss : 0.046598, loss_ce: 0.024147
2022-01-11 22:58:25,353 iteration 1589 : loss : 0.039357, loss_ce: 0.011512
2022-01-11 22:58:26,973 iteration 1590 : loss : 0.058308, loss_ce: 0.025204
2022-01-11 22:58:28,520 iteration 1591 : loss : 0.064965, loss_ce: 0.033593
2022-01-11 22:58:30,073 iteration 1592 : loss : 0.059597, loss_ce: 0.026540
2022-01-11 22:58:31,749 iteration 1593 : loss : 0.059086, loss_ce: 0.017007
2022-01-11 22:58:33,361 iteration 1594 : loss : 0.042935, loss_ce: 0.021203
2022-01-11 22:58:34,985 iteration 1595 : loss : 0.055579, loss_ce: 0.024796
2022-01-11 22:58:36,586 iteration 1596 : loss : 0.047134, loss_ce: 0.021470
2022-01-11 22:58:38,138 iteration 1597 : loss : 0.045396, loss_ce: 0.015471
2022-01-11 22:58:39,769 iteration 1598 : loss : 0.040878, loss_ce: 0.017558
 24%|███████                       | 94/400 [46:48<2:23:51, 28.21s/it]2022-01-11 22:58:41,445 iteration 1599 : loss : 0.027363, loss_ce: 0.012058
2022-01-11 22:58:43,007 iteration 1600 : loss : 0.060027, loss_ce: 0.021178
2022-01-11 22:58:44,605 iteration 1601 : loss : 0.059383, loss_ce: 0.021992
2022-01-11 22:58:46,179 iteration 1602 : loss : 0.047059, loss_ce: 0.020130
2022-01-11 22:58:47,856 iteration 1603 : loss : 0.069365, loss_ce: 0.028038
2022-01-11 22:58:49,431 iteration 1604 : loss : 0.034742, loss_ce: 0.016690
2022-01-11 22:58:51,113 iteration 1605 : loss : 0.067255, loss_ce: 0.028549
2022-01-11 22:58:52,762 iteration 1606 : loss : 0.043992, loss_ce: 0.012811
2022-01-11 22:58:54,367 iteration 1607 : loss : 0.046088, loss_ce: 0.018951
2022-01-11 22:58:55,995 iteration 1608 : loss : 0.031210, loss_ce: 0.011353
2022-01-11 22:58:57,636 iteration 1609 : loss : 0.039433, loss_ce: 0.014241
2022-01-11 22:58:59,231 iteration 1610 : loss : 0.044558, loss_ce: 0.018684
2022-01-11 22:59:00,853 iteration 1611 : loss : 0.054106, loss_ce: 0.026676
2022-01-11 22:59:02,454 iteration 1612 : loss : 0.040415, loss_ce: 0.018570
2022-01-11 22:59:04,069 iteration 1613 : loss : 0.046273, loss_ce: 0.017583
2022-01-11 22:59:05,686 iteration 1614 : loss : 0.032499, loss_ce: 0.011900
2022-01-11 22:59:05,687 Training Data Eval:
2022-01-11 22:59:13,643   Average segmentation loss on training set: 0.0337
2022-01-11 22:59:13,644 Validation Data Eval:
2022-01-11 22:59:16,384   Average segmentation loss on validation set: 0.0661
2022-01-11 22:59:22,378 Found new lowest validation loss at iteration 1614! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 22:59:23,909 iteration 1615 : loss : 0.048956, loss_ce: 0.025330
 24%|███████▏                      | 95/400 [47:32<2:47:41, 32.99s/it]2022-01-11 22:59:25,402 iteration 1616 : loss : 0.039311, loss_ce: 0.012569
2022-01-11 22:59:26,814 iteration 1617 : loss : 0.030688, loss_ce: 0.014564
2022-01-11 22:59:28,268 iteration 1618 : loss : 0.023707, loss_ce: 0.009275
2022-01-11 22:59:29,825 iteration 1619 : loss : 0.035209, loss_ce: 0.017956
2022-01-11 22:59:31,347 iteration 1620 : loss : 0.036664, loss_ce: 0.016921
2022-01-11 22:59:32,978 iteration 1621 : loss : 0.040271, loss_ce: 0.015195
2022-01-11 22:59:34,542 iteration 1622 : loss : 0.033996, loss_ce: 0.013762
2022-01-11 22:59:36,068 iteration 1623 : loss : 0.031148, loss_ce: 0.015079
2022-01-11 22:59:37,674 iteration 1624 : loss : 0.049821, loss_ce: 0.019151
2022-01-11 22:59:39,177 iteration 1625 : loss : 0.030874, loss_ce: 0.015952
2022-01-11 22:59:40,727 iteration 1626 : loss : 0.037690, loss_ce: 0.015521
2022-01-11 22:59:42,272 iteration 1627 : loss : 0.046257, loss_ce: 0.015465
2022-01-11 22:59:43,897 iteration 1628 : loss : 0.065818, loss_ce: 0.019799
2022-01-11 22:59:45,505 iteration 1629 : loss : 0.041077, loss_ce: 0.017612
2022-01-11 22:59:47,023 iteration 1630 : loss : 0.039517, loss_ce: 0.014859
2022-01-11 22:59:48,606 iteration 1631 : loss : 0.053505, loss_ce: 0.024242
2022-01-11 22:59:50,084 iteration 1632 : loss : 0.040127, loss_ce: 0.014824
 24%|███████▏                      | 96/400 [47:58<2:36:47, 30.94s/it]2022-01-11 22:59:51,793 iteration 1633 : loss : 0.048033, loss_ce: 0.019213
2022-01-11 22:59:53,416 iteration 1634 : loss : 0.056916, loss_ce: 0.027514
2022-01-11 22:59:54,992 iteration 1635 : loss : 0.041946, loss_ce: 0.018532
2022-01-11 22:59:56,474 iteration 1636 : loss : 0.032493, loss_ce: 0.012233
2022-01-11 22:59:58,129 iteration 1637 : loss : 0.042168, loss_ce: 0.015886
2022-01-11 22:59:59,735 iteration 1638 : loss : 0.041522, loss_ce: 0.014698
2022-01-11 23:00:01,394 iteration 1639 : loss : 0.040816, loss_ce: 0.015471
2022-01-11 23:00:02,995 iteration 1640 : loss : 0.046757, loss_ce: 0.019618
2022-01-11 23:00:04,624 iteration 1641 : loss : 0.037853, loss_ce: 0.015180
2022-01-11 23:00:06,339 iteration 1642 : loss : 0.083459, loss_ce: 0.027955
2022-01-11 23:00:07,969 iteration 1643 : loss : 0.022474, loss_ce: 0.009082
2022-01-11 23:00:09,584 iteration 1644 : loss : 0.044766, loss_ce: 0.019374
2022-01-11 23:00:11,097 iteration 1645 : loss : 0.046382, loss_ce: 0.014641
2022-01-11 23:00:12,679 iteration 1646 : loss : 0.039850, loss_ce: 0.015971
2022-01-11 23:00:14,365 iteration 1647 : loss : 0.063819, loss_ce: 0.041797
2022-01-11 23:00:15,959 iteration 1648 : loss : 0.046156, loss_ce: 0.019417
2022-01-11 23:00:17,647 iteration 1649 : loss : 0.059349, loss_ce: 0.014932
 24%|███████▎                      | 97/400 [48:26<2:31:08, 29.93s/it]2022-01-11 23:00:19,285 iteration 1650 : loss : 0.030918, loss_ce: 0.010850
2022-01-11 23:00:20,953 iteration 1651 : loss : 0.105977, loss_ce: 0.052068
2022-01-11 23:00:22,626 iteration 1652 : loss : 0.072220, loss_ce: 0.019683
2022-01-11 23:00:24,280 iteration 1653 : loss : 0.063358, loss_ce: 0.030819
2022-01-11 23:00:25,868 iteration 1654 : loss : 0.039400, loss_ce: 0.012980
2022-01-11 23:00:27,439 iteration 1655 : loss : 0.057883, loss_ce: 0.017017
2022-01-11 23:00:29,007 iteration 1656 : loss : 0.030459, loss_ce: 0.014013
2022-01-11 23:00:30,691 iteration 1657 : loss : 0.063514, loss_ce: 0.028133
2022-01-11 23:00:32,236 iteration 1658 : loss : 0.036004, loss_ce: 0.014203
2022-01-11 23:00:33,823 iteration 1659 : loss : 0.041472, loss_ce: 0.019242
2022-01-11 23:00:35,564 iteration 1660 : loss : 0.055665, loss_ce: 0.024108
2022-01-11 23:00:37,183 iteration 1661 : loss : 0.049411, loss_ce: 0.018144
2022-01-11 23:00:38,767 iteration 1662 : loss : 0.046605, loss_ce: 0.017519
2022-01-11 23:00:40,433 iteration 1663 : loss : 0.049336, loss_ce: 0.020411
2022-01-11 23:00:42,093 iteration 1664 : loss : 0.053541, loss_ce: 0.018111
2022-01-11 23:00:43,659 iteration 1665 : loss : 0.026637, loss_ce: 0.012967
2022-01-11 23:00:45,215 iteration 1666 : loss : 0.030044, loss_ce: 0.011462
 24%|███████▎                      | 98/400 [48:53<2:27:04, 29.22s/it]2022-01-11 23:00:46,847 iteration 1667 : loss : 0.041081, loss_ce: 0.018079
2022-01-11 23:00:48,519 iteration 1668 : loss : 0.071208, loss_ce: 0.034074
2022-01-11 23:00:50,101 iteration 1669 : loss : 0.041781, loss_ce: 0.017156
2022-01-11 23:00:51,699 iteration 1670 : loss : 0.034717, loss_ce: 0.012311
2022-01-11 23:00:53,333 iteration 1671 : loss : 0.040402, loss_ce: 0.016055
2022-01-11 23:00:54,909 iteration 1672 : loss : 0.033554, loss_ce: 0.011256
2022-01-11 23:00:56,461 iteration 1673 : loss : 0.054100, loss_ce: 0.022496
2022-01-11 23:00:58,118 iteration 1674 : loss : 0.041395, loss_ce: 0.016445
2022-01-11 23:00:59,696 iteration 1675 : loss : 0.035662, loss_ce: 0.014295
2022-01-11 23:01:01,344 iteration 1676 : loss : 0.032584, loss_ce: 0.014190
2022-01-11 23:01:03,008 iteration 1677 : loss : 0.047296, loss_ce: 0.022676
2022-01-11 23:01:04,652 iteration 1678 : loss : 0.075656, loss_ce: 0.017907
2022-01-11 23:01:06,310 iteration 1679 : loss : 0.032088, loss_ce: 0.009700
2022-01-11 23:01:07,945 iteration 1680 : loss : 0.034055, loss_ce: 0.016015
2022-01-11 23:01:09,550 iteration 1681 : loss : 0.069985, loss_ce: 0.023388
2022-01-11 23:01:11,113 iteration 1682 : loss : 0.055047, loss_ce: 0.028270
2022-01-11 23:01:12,745 iteration 1683 : loss : 0.053927, loss_ce: 0.021647
 25%|███████▍                      | 99/400 [49:21<2:24:02, 28.71s/it]2022-01-11 23:01:14,383 iteration 1684 : loss : 0.034379, loss_ce: 0.013093
2022-01-11 23:01:15,937 iteration 1685 : loss : 0.040120, loss_ce: 0.015627
2022-01-11 23:01:17,599 iteration 1686 : loss : 0.047951, loss_ce: 0.019016
2022-01-11 23:01:19,150 iteration 1687 : loss : 0.034098, loss_ce: 0.010001
2022-01-11 23:01:20,766 iteration 1688 : loss : 0.038187, loss_ce: 0.014567
2022-01-11 23:01:22,403 iteration 1689 : loss : 0.041127, loss_ce: 0.018132
2022-01-11 23:01:23,936 iteration 1690 : loss : 0.043775, loss_ce: 0.014760
2022-01-11 23:01:25,453 iteration 1691 : loss : 0.050130, loss_ce: 0.016999
2022-01-11 23:01:27,068 iteration 1692 : loss : 0.030467, loss_ce: 0.013053
2022-01-11 23:01:28,744 iteration 1693 : loss : 0.043739, loss_ce: 0.016787
2022-01-11 23:01:30,387 iteration 1694 : loss : 0.037932, loss_ce: 0.011710
2022-01-11 23:01:31,968 iteration 1695 : loss : 0.034072, loss_ce: 0.009642
2022-01-11 23:01:33,580 iteration 1696 : loss : 0.055913, loss_ce: 0.027592
2022-01-11 23:01:35,203 iteration 1697 : loss : 0.044237, loss_ce: 0.022589
2022-01-11 23:01:36,829 iteration 1698 : loss : 0.033041, loss_ce: 0.012057
2022-01-11 23:01:38,414 iteration 1699 : loss : 0.037857, loss_ce: 0.015755
2022-01-11 23:01:38,414 Training Data Eval:
2022-01-11 23:01:46,395   Average segmentation loss on training set: 0.0284
2022-01-11 23:01:46,396 Validation Data Eval:
2022-01-11 23:01:49,140   Average segmentation loss on validation set: 0.1188
2022-01-11 23:01:50,760 iteration 1700 : loss : 0.031324, loss_ce: 0.011378
 25%|███████▎                     | 100/400 [49:59<2:37:31, 31.50s/it]2022-01-11 23:01:52,390 iteration 1701 : loss : 0.031986, loss_ce: 0.013020
2022-01-11 23:01:53,988 iteration 1702 : loss : 0.035887, loss_ce: 0.014274
2022-01-11 23:01:55,665 iteration 1703 : loss : 0.048486, loss_ce: 0.018687
2022-01-11 23:01:57,247 iteration 1704 : loss : 0.041565, loss_ce: 0.019261
2022-01-11 23:01:58,875 iteration 1705 : loss : 0.054017, loss_ce: 0.020143
2022-01-11 23:02:00,552 iteration 1706 : loss : 0.047144, loss_ce: 0.020416
2022-01-11 23:02:02,129 iteration 1707 : loss : 0.034332, loss_ce: 0.015671
2022-01-11 23:02:03,721 iteration 1708 : loss : 0.084699, loss_ce: 0.023473
2022-01-11 23:02:05,301 iteration 1709 : loss : 0.037492, loss_ce: 0.016173
2022-01-11 23:02:06,933 iteration 1710 : loss : 0.050141, loss_ce: 0.016195
2022-01-11 23:02:08,541 iteration 1711 : loss : 0.034722, loss_ce: 0.013126
2022-01-11 23:02:10,069 iteration 1712 : loss : 0.027162, loss_ce: 0.011577
2022-01-11 23:02:11,694 iteration 1713 : loss : 0.051570, loss_ce: 0.014426
2022-01-11 23:02:13,376 iteration 1714 : loss : 0.041024, loss_ce: 0.010632
2022-01-11 23:02:15,044 iteration 1715 : loss : 0.039512, loss_ce: 0.018678
2022-01-11 23:02:16,559 iteration 1716 : loss : 0.037801, loss_ce: 0.012196
2022-01-11 23:02:18,226 iteration 1717 : loss : 0.044005, loss_ce: 0.014157
 25%|███████▎                     | 101/400 [50:26<2:30:57, 30.29s/it]2022-01-11 23:02:19,880 iteration 1718 : loss : 0.040042, loss_ce: 0.020451
2022-01-11 23:02:21,461 iteration 1719 : loss : 0.063823, loss_ce: 0.012464
2022-01-11 23:02:23,122 iteration 1720 : loss : 0.045791, loss_ce: 0.016693
2022-01-11 23:02:24,684 iteration 1721 : loss : 0.037044, loss_ce: 0.013079
2022-01-11 23:02:26,320 iteration 1722 : loss : 0.039131, loss_ce: 0.012869
2022-01-11 23:02:27,865 iteration 1723 : loss : 0.039613, loss_ce: 0.014065
2022-01-11 23:02:29,493 iteration 1724 : loss : 0.037620, loss_ce: 0.013811
2022-01-11 23:02:31,095 iteration 1725 : loss : 0.058119, loss_ce: 0.019732
2022-01-11 23:02:32,707 iteration 1726 : loss : 0.034403, loss_ce: 0.014631
2022-01-11 23:02:34,386 iteration 1727 : loss : 0.051371, loss_ce: 0.024382
2022-01-11 23:02:35,903 iteration 1728 : loss : 0.026812, loss_ce: 0.008914
2022-01-11 23:02:37,523 iteration 1729 : loss : 0.041989, loss_ce: 0.015051
2022-01-11 23:02:39,189 iteration 1730 : loss : 0.030203, loss_ce: 0.013658
2022-01-11 23:02:40,781 iteration 1731 : loss : 0.051022, loss_ce: 0.027962
2022-01-11 23:02:42,356 iteration 1732 : loss : 0.037979, loss_ce: 0.015679
2022-01-11 23:02:43,898 iteration 1733 : loss : 0.036123, loss_ce: 0.015170
2022-01-11 23:02:45,473 iteration 1734 : loss : 0.038634, loss_ce: 0.017576
 26%|███████▍                     | 102/400 [50:54<2:25:54, 29.38s/it]2022-01-11 23:02:47,242 iteration 1735 : loss : 0.052784, loss_ce: 0.025151
2022-01-11 23:02:48,805 iteration 1736 : loss : 0.073407, loss_ce: 0.036317
2022-01-11 23:02:50,525 iteration 1737 : loss : 0.043662, loss_ce: 0.018155
2022-01-11 23:02:52,129 iteration 1738 : loss : 0.035261, loss_ce: 0.017618
2022-01-11 23:02:53,660 iteration 1739 : loss : 0.028180, loss_ce: 0.010234
2022-01-11 23:02:55,327 iteration 1740 : loss : 0.042139, loss_ce: 0.019023
2022-01-11 23:02:56,966 iteration 1741 : loss : 0.026248, loss_ce: 0.009284
2022-01-11 23:02:58,606 iteration 1742 : loss : 0.047221, loss_ce: 0.017695
2022-01-11 23:03:00,123 iteration 1743 : loss : 0.040540, loss_ce: 0.013138
2022-01-11 23:03:01,799 iteration 1744 : loss : 0.057471, loss_ce: 0.021082
2022-01-11 23:03:03,465 iteration 1745 : loss : 0.059334, loss_ce: 0.019166
2022-01-11 23:03:05,087 iteration 1746 : loss : 0.054637, loss_ce: 0.026177
2022-01-11 23:03:06,655 iteration 1747 : loss : 0.036662, loss_ce: 0.017618
2022-01-11 23:03:08,274 iteration 1748 : loss : 0.036420, loss_ce: 0.013604
2022-01-11 23:03:09,768 iteration 1749 : loss : 0.031368, loss_ce: 0.010128
2022-01-11 23:03:11,384 iteration 1750 : loss : 0.081273, loss_ce: 0.017449
2022-01-11 23:03:13,087 iteration 1751 : loss : 0.036255, loss_ce: 0.013216
 26%|███████▍                     | 103/400 [51:21<2:22:48, 28.85s/it]2022-01-11 23:03:14,816 iteration 1752 : loss : 0.084936, loss_ce: 0.022580
2022-01-11 23:03:16,309 iteration 1753 : loss : 0.043106, loss_ce: 0.013632
2022-01-11 23:03:17,979 iteration 1754 : loss : 0.049728, loss_ce: 0.018401
2022-01-11 23:03:19,546 iteration 1755 : loss : 0.033430, loss_ce: 0.014772
2022-01-11 23:03:21,156 iteration 1756 : loss : 0.085232, loss_ce: 0.037975
2022-01-11 23:03:22,707 iteration 1757 : loss : 0.031501, loss_ce: 0.012445
2022-01-11 23:03:24,289 iteration 1758 : loss : 0.039710, loss_ce: 0.018974
2022-01-11 23:03:25,854 iteration 1759 : loss : 0.037521, loss_ce: 0.011016
2022-01-11 23:03:27,417 iteration 1760 : loss : 0.042757, loss_ce: 0.014582
2022-01-11 23:03:29,065 iteration 1761 : loss : 0.053267, loss_ce: 0.016174
2022-01-11 23:03:30,564 iteration 1762 : loss : 0.030956, loss_ce: 0.012633
2022-01-11 23:03:32,117 iteration 1763 : loss : 0.034837, loss_ce: 0.013351
2022-01-11 23:03:33,603 iteration 1764 : loss : 0.028127, loss_ce: 0.008967
2022-01-11 23:03:35,208 iteration 1765 : loss : 0.034338, loss_ce: 0.018167
2022-01-11 23:03:36,870 iteration 1766 : loss : 0.052030, loss_ce: 0.026277
2022-01-11 23:03:38,434 iteration 1767 : loss : 0.034052, loss_ce: 0.014549
2022-01-11 23:03:40,047 iteration 1768 : loss : 0.045646, loss_ce: 0.018497
 26%|███████▌                     | 104/400 [51:48<2:19:31, 28.28s/it]2022-01-11 23:03:41,702 iteration 1769 : loss : 0.039233, loss_ce: 0.016845
2022-01-11 23:03:43,265 iteration 1770 : loss : 0.034833, loss_ce: 0.014526
2022-01-11 23:03:44,770 iteration 1771 : loss : 0.056541, loss_ce: 0.022225
2022-01-11 23:03:46,300 iteration 1772 : loss : 0.033268, loss_ce: 0.016002
2022-01-11 23:03:47,944 iteration 1773 : loss : 0.060776, loss_ce: 0.020637
2022-01-11 23:03:49,553 iteration 1774 : loss : 0.031352, loss_ce: 0.009664
2022-01-11 23:03:51,192 iteration 1775 : loss : 0.048232, loss_ce: 0.018896
2022-01-11 23:03:52,756 iteration 1776 : loss : 0.033120, loss_ce: 0.016158
2022-01-11 23:03:54,308 iteration 1777 : loss : 0.025939, loss_ce: 0.009986
2022-01-11 23:03:55,835 iteration 1778 : loss : 0.037687, loss_ce: 0.013797
2022-01-11 23:03:57,478 iteration 1779 : loss : 0.058562, loss_ce: 0.022981
2022-01-11 23:03:59,074 iteration 1780 : loss : 0.041024, loss_ce: 0.019080
2022-01-11 23:04:00,615 iteration 1781 : loss : 0.033570, loss_ce: 0.013204
2022-01-11 23:04:02,248 iteration 1782 : loss : 0.045343, loss_ce: 0.016996
2022-01-11 23:04:03,857 iteration 1783 : loss : 0.038472, loss_ce: 0.015542
2022-01-11 23:04:05,424 iteration 1784 : loss : 0.040120, loss_ce: 0.013869
2022-01-11 23:04:05,424 Training Data Eval:
2022-01-11 23:04:13,384   Average segmentation loss on training set: 0.0263
2022-01-11 23:04:13,385 Validation Data Eval:
2022-01-11 23:04:16,129   Average segmentation loss on validation set: 0.0862
2022-01-11 23:04:17,771 iteration 1785 : loss : 0.040103, loss_ce: 0.015986
 26%|███████▌                     | 105/400 [52:26<2:32:58, 31.11s/it]2022-01-11 23:04:19,426 iteration 1786 : loss : 0.032386, loss_ce: 0.016806
2022-01-11 23:04:20,953 iteration 1787 : loss : 0.042919, loss_ce: 0.017972
2022-01-11 23:04:22,600 iteration 1788 : loss : 0.029142, loss_ce: 0.015424
2022-01-11 23:04:24,233 iteration 1789 : loss : 0.033293, loss_ce: 0.011476
2022-01-11 23:04:25,851 iteration 1790 : loss : 0.039622, loss_ce: 0.021925
2022-01-11 23:04:27,512 iteration 1791 : loss : 0.043138, loss_ce: 0.018467
2022-01-11 23:04:29,065 iteration 1792 : loss : 0.033769, loss_ce: 0.012836
2022-01-11 23:04:30,630 iteration 1793 : loss : 0.049038, loss_ce: 0.019880
2022-01-11 23:04:32,205 iteration 1794 : loss : 0.025687, loss_ce: 0.009812
2022-01-11 23:04:33,836 iteration 1795 : loss : 0.038146, loss_ce: 0.013633
2022-01-11 23:04:35,435 iteration 1796 : loss : 0.042187, loss_ce: 0.018896
2022-01-11 23:04:37,116 iteration 1797 : loss : 0.038759, loss_ce: 0.012075
2022-01-11 23:04:38,796 iteration 1798 : loss : 0.031216, loss_ce: 0.011392
2022-01-11 23:04:40,383 iteration 1799 : loss : 0.039471, loss_ce: 0.015894
2022-01-11 23:04:42,011 iteration 1800 : loss : 0.029839, loss_ce: 0.010020
2022-01-11 23:04:43,552 iteration 1801 : loss : 0.032825, loss_ce: 0.011869
2022-01-11 23:04:45,122 iteration 1802 : loss : 0.035809, loss_ce: 0.013331
 26%|███████▋                     | 106/400 [52:53<2:26:55, 29.98s/it]2022-01-11 23:04:46,853 iteration 1803 : loss : 0.054782, loss_ce: 0.019166
2022-01-11 23:04:48,522 iteration 1804 : loss : 0.034464, loss_ce: 0.010329
2022-01-11 23:04:50,090 iteration 1805 : loss : 0.035250, loss_ce: 0.011477
2022-01-11 23:04:51,651 iteration 1806 : loss : 0.034723, loss_ce: 0.015206
2022-01-11 23:04:53,210 iteration 1807 : loss : 0.045936, loss_ce: 0.015772
2022-01-11 23:04:54,798 iteration 1808 : loss : 0.028570, loss_ce: 0.011480
2022-01-11 23:04:56,348 iteration 1809 : loss : 0.029145, loss_ce: 0.012330
2022-01-11 23:04:57,828 iteration 1810 : loss : 0.023573, loss_ce: 0.009545
2022-01-11 23:04:59,469 iteration 1811 : loss : 0.035061, loss_ce: 0.016947
2022-01-11 23:05:01,129 iteration 1812 : loss : 0.033013, loss_ce: 0.012813
2022-01-11 23:05:02,871 iteration 1813 : loss : 0.043940, loss_ce: 0.015249
2022-01-11 23:05:04,498 iteration 1814 : loss : 0.030712, loss_ce: 0.014994
2022-01-11 23:05:06,047 iteration 1815 : loss : 0.032156, loss_ce: 0.010268
2022-01-11 23:05:07,549 iteration 1816 : loss : 0.033673, loss_ce: 0.013443
2022-01-11 23:05:09,202 iteration 1817 : loss : 0.036926, loss_ce: 0.014346
2022-01-11 23:05:10,809 iteration 1818 : loss : 0.041458, loss_ce: 0.018573
2022-01-11 23:05:12,450 iteration 1819 : loss : 0.033728, loss_ce: 0.014100
 27%|███████▊                     | 107/400 [53:21<2:22:32, 29.19s/it]2022-01-11 23:05:14,172 iteration 1820 : loss : 0.030142, loss_ce: 0.014082
2022-01-11 23:05:15,745 iteration 1821 : loss : 0.023801, loss_ce: 0.010796
2022-01-11 23:05:17,337 iteration 1822 : loss : 0.029728, loss_ce: 0.010610
2022-01-11 23:05:18,850 iteration 1823 : loss : 0.026498, loss_ce: 0.008885
2022-01-11 23:05:20,319 iteration 1824 : loss : 0.026110, loss_ce: 0.010376
2022-01-11 23:05:21,946 iteration 1825 : loss : 0.050398, loss_ce: 0.019455
2022-01-11 23:05:23,443 iteration 1826 : loss : 0.035370, loss_ce: 0.016758
2022-01-11 23:05:25,133 iteration 1827 : loss : 0.140014, loss_ce: 0.045022
2022-01-11 23:05:26,706 iteration 1828 : loss : 0.037801, loss_ce: 0.016612
2022-01-11 23:05:28,286 iteration 1829 : loss : 0.055338, loss_ce: 0.019479
2022-01-11 23:05:29,857 iteration 1830 : loss : 0.032332, loss_ce: 0.014000
2022-01-11 23:05:31,522 iteration 1831 : loss : 0.080634, loss_ce: 0.046290
2022-01-11 23:05:33,100 iteration 1832 : loss : 0.038614, loss_ce: 0.011764
2022-01-11 23:05:34,676 iteration 1833 : loss : 0.031938, loss_ce: 0.015968
2022-01-11 23:05:36,375 iteration 1834 : loss : 0.038791, loss_ce: 0.013921
2022-01-11 23:05:37,940 iteration 1835 : loss : 0.042673, loss_ce: 0.018393
2022-01-11 23:05:39,536 iteration 1836 : loss : 0.049786, loss_ce: 0.019596
 27%|███████▊                     | 108/400 [53:48<2:18:59, 28.56s/it]2022-01-11 23:05:41,226 iteration 1837 : loss : 0.034270, loss_ce: 0.014603
2022-01-11 23:05:42,893 iteration 1838 : loss : 0.037525, loss_ce: 0.016769
2022-01-11 23:05:44,466 iteration 1839 : loss : 0.054818, loss_ce: 0.026400
2022-01-11 23:05:45,974 iteration 1840 : loss : 0.041298, loss_ce: 0.021420
2022-01-11 23:05:47,603 iteration 1841 : loss : 0.053410, loss_ce: 0.026623
2022-01-11 23:05:49,254 iteration 1842 : loss : 0.049761, loss_ce: 0.021018
2022-01-11 23:05:50,787 iteration 1843 : loss : 0.032377, loss_ce: 0.014664
2022-01-11 23:05:52,370 iteration 1844 : loss : 0.049446, loss_ce: 0.017038
2022-01-11 23:05:53,889 iteration 1845 : loss : 0.033744, loss_ce: 0.014128
2022-01-11 23:05:55,580 iteration 1846 : loss : 0.034446, loss_ce: 0.016430
2022-01-11 23:05:57,221 iteration 1847 : loss : 0.050048, loss_ce: 0.021258
2022-01-11 23:05:58,822 iteration 1848 : loss : 0.038503, loss_ce: 0.015881
2022-01-11 23:06:00,330 iteration 1849 : loss : 0.034195, loss_ce: 0.016053
2022-01-11 23:06:01,930 iteration 1850 : loss : 0.047691, loss_ce: 0.014353
2022-01-11 23:06:03,567 iteration 1851 : loss : 0.059534, loss_ce: 0.028818
2022-01-11 23:06:05,170 iteration 1852 : loss : 0.031050, loss_ce: 0.012838
2022-01-11 23:06:06,787 iteration 1853 : loss : 0.044183, loss_ce: 0.010272
 27%|███████▉                     | 109/400 [54:15<2:16:36, 28.17s/it]2022-01-11 23:06:08,409 iteration 1854 : loss : 0.037286, loss_ce: 0.013513
2022-01-11 23:06:09,998 iteration 1855 : loss : 0.037361, loss_ce: 0.012809
2022-01-11 23:06:11,592 iteration 1856 : loss : 0.040682, loss_ce: 0.009750
2022-01-11 23:06:13,200 iteration 1857 : loss : 0.044286, loss_ce: 0.016755
2022-01-11 23:06:14,772 iteration 1858 : loss : 0.043086, loss_ce: 0.024532
2022-01-11 23:06:16,312 iteration 1859 : loss : 0.034386, loss_ce: 0.017039
2022-01-11 23:06:18,026 iteration 1860 : loss : 0.059583, loss_ce: 0.024818
2022-01-11 23:06:19,649 iteration 1861 : loss : 0.053597, loss_ce: 0.019423
2022-01-11 23:06:21,246 iteration 1862 : loss : 0.031355, loss_ce: 0.015368
2022-01-11 23:06:22,894 iteration 1863 : loss : 0.047125, loss_ce: 0.018754
2022-01-11 23:06:24,512 iteration 1864 : loss : 0.036761, loss_ce: 0.014821
2022-01-11 23:06:26,211 iteration 1865 : loss : 0.076957, loss_ce: 0.039939
2022-01-11 23:06:27,862 iteration 1866 : loss : 0.038760, loss_ce: 0.019308
2022-01-11 23:06:29,488 iteration 1867 : loss : 0.102301, loss_ce: 0.036513
2022-01-11 23:06:31,049 iteration 1868 : loss : 0.037169, loss_ce: 0.018412
2022-01-11 23:06:32,643 iteration 1869 : loss : 0.058925, loss_ce: 0.016139
2022-01-11 23:06:32,643 Training Data Eval:
2022-01-11 23:06:40,615   Average segmentation loss on training set: 0.0292
2022-01-11 23:06:40,616 Validation Data Eval:
2022-01-11 23:06:43,360   Average segmentation loss on validation set: 0.0931
2022-01-11 23:06:44,891 iteration 1870 : loss : 0.040336, loss_ce: 0.011947
 28%|███████▉                     | 110/400 [54:53<2:30:32, 31.14s/it]2022-01-11 23:06:46,501 iteration 1871 : loss : 0.039750, loss_ce: 0.018660
2022-01-11 23:06:48,043 iteration 1872 : loss : 0.038257, loss_ce: 0.014507
2022-01-11 23:06:49,647 iteration 1873 : loss : 0.054662, loss_ce: 0.024711
2022-01-11 23:06:51,295 iteration 1874 : loss : 0.070872, loss_ce: 0.029287
2022-01-11 23:06:52,970 iteration 1875 : loss : 0.034019, loss_ce: 0.018421
2022-01-11 23:06:54,651 iteration 1876 : loss : 0.041407, loss_ce: 0.011724
2022-01-11 23:06:56,298 iteration 1877 : loss : 0.039750, loss_ce: 0.012193
2022-01-11 23:06:57,837 iteration 1878 : loss : 0.050592, loss_ce: 0.017718
2022-01-11 23:06:59,397 iteration 1879 : loss : 0.033501, loss_ce: 0.016130
2022-01-11 23:07:00,942 iteration 1880 : loss : 0.033489, loss_ce: 0.012014
2022-01-11 23:07:02,485 iteration 1881 : loss : 0.032936, loss_ce: 0.011564
2022-01-11 23:07:04,077 iteration 1882 : loss : 0.056533, loss_ce: 0.016218
2022-01-11 23:07:05,595 iteration 1883 : loss : 0.034186, loss_ce: 0.014196
2022-01-11 23:07:07,264 iteration 1884 : loss : 0.035189, loss_ce: 0.013355
2022-01-11 23:07:08,798 iteration 1885 : loss : 0.029193, loss_ce: 0.010936
2022-01-11 23:07:10,416 iteration 1886 : loss : 0.060215, loss_ce: 0.030474
2022-01-11 23:07:11,968 iteration 1887 : loss : 0.039878, loss_ce: 0.014496
 28%|████████                     | 111/400 [55:20<2:24:08, 29.93s/it]2022-01-11 23:07:13,641 iteration 1888 : loss : 0.035343, loss_ce: 0.011315
2022-01-11 23:07:15,326 iteration 1889 : loss : 0.054570, loss_ce: 0.025417
2022-01-11 23:07:16,842 iteration 1890 : loss : 0.033245, loss_ce: 0.011953
2022-01-11 23:07:18,456 iteration 1891 : loss : 0.036966, loss_ce: 0.018658
2022-01-11 23:07:20,043 iteration 1892 : loss : 0.038968, loss_ce: 0.012330
2022-01-11 23:07:21,684 iteration 1893 : loss : 0.052668, loss_ce: 0.018787
2022-01-11 23:07:23,276 iteration 1894 : loss : 0.054992, loss_ce: 0.022179
2022-01-11 23:07:24,905 iteration 1895 : loss : 0.042145, loss_ce: 0.013582
2022-01-11 23:07:26,516 iteration 1896 : loss : 0.031417, loss_ce: 0.014223
2022-01-11 23:07:28,116 iteration 1897 : loss : 0.059031, loss_ce: 0.022775
2022-01-11 23:07:29,811 iteration 1898 : loss : 0.039366, loss_ce: 0.014798
2022-01-11 23:07:31,328 iteration 1899 : loss : 0.029018, loss_ce: 0.011609
2022-01-11 23:07:32,992 iteration 1900 : loss : 0.054899, loss_ce: 0.023189
2022-01-11 23:07:34,688 iteration 1901 : loss : 0.039916, loss_ce: 0.013128
2022-01-11 23:07:36,278 iteration 1902 : loss : 0.038933, loss_ce: 0.012799
2022-01-11 23:07:37,909 iteration 1903 : loss : 0.046059, loss_ce: 0.019187
2022-01-11 23:07:39,568 iteration 1904 : loss : 0.042105, loss_ce: 0.013270
 28%|████████                     | 112/400 [55:48<2:20:18, 29.23s/it]2022-01-11 23:07:41,265 iteration 1905 : loss : 0.032751, loss_ce: 0.014782
2022-01-11 23:07:42,877 iteration 1906 : loss : 0.036482, loss_ce: 0.011228
2022-01-11 23:07:44,462 iteration 1907 : loss : 0.030552, loss_ce: 0.011617
2022-01-11 23:07:46,194 iteration 1908 : loss : 0.039859, loss_ce: 0.013675
2022-01-11 23:07:47,819 iteration 1909 : loss : 0.031207, loss_ce: 0.011946
2022-01-11 23:07:49,464 iteration 1910 : loss : 0.100891, loss_ce: 0.017708
2022-01-11 23:07:51,136 iteration 1911 : loss : 0.040643, loss_ce: 0.013065
2022-01-11 23:07:52,654 iteration 1912 : loss : 0.030700, loss_ce: 0.010668
2022-01-11 23:07:54,245 iteration 1913 : loss : 0.055138, loss_ce: 0.026348
2022-01-11 23:07:55,860 iteration 1914 : loss : 0.046519, loss_ce: 0.025430
2022-01-11 23:07:57,494 iteration 1915 : loss : 0.051449, loss_ce: 0.018447
2022-01-11 23:07:59,079 iteration 1916 : loss : 0.045369, loss_ce: 0.017466
2022-01-11 23:08:00,682 iteration 1917 : loss : 0.036322, loss_ce: 0.017031
2022-01-11 23:08:02,313 iteration 1918 : loss : 0.041714, loss_ce: 0.013438
2022-01-11 23:08:03,873 iteration 1919 : loss : 0.045936, loss_ce: 0.015192
2022-01-11 23:08:05,448 iteration 1920 : loss : 0.039107, loss_ce: 0.015484
2022-01-11 23:08:07,124 iteration 1921 : loss : 0.064094, loss_ce: 0.030898
 28%|████████▏                    | 113/400 [56:15<2:17:24, 28.73s/it]2022-01-11 23:08:08,727 iteration 1922 : loss : 0.050367, loss_ce: 0.024061
2022-01-11 23:08:10,314 iteration 1923 : loss : 0.035034, loss_ce: 0.015551
2022-01-11 23:08:11,980 iteration 1924 : loss : 0.035645, loss_ce: 0.016929
2022-01-11 23:08:13,588 iteration 1925 : loss : 0.058937, loss_ce: 0.024930
2022-01-11 23:08:15,171 iteration 1926 : loss : 0.027000, loss_ce: 0.011742
2022-01-11 23:08:16,748 iteration 1927 : loss : 0.035479, loss_ce: 0.013131
2022-01-11 23:08:18,439 iteration 1928 : loss : 0.050184, loss_ce: 0.022115
2022-01-11 23:08:20,005 iteration 1929 : loss : 0.043825, loss_ce: 0.017197
2022-01-11 23:08:21,558 iteration 1930 : loss : 0.032119, loss_ce: 0.015693
2022-01-11 23:08:23,181 iteration 1931 : loss : 0.071538, loss_ce: 0.017790
2022-01-11 23:08:24,846 iteration 1932 : loss : 0.049475, loss_ce: 0.017369
2022-01-11 23:08:26,509 iteration 1933 : loss : 0.061953, loss_ce: 0.031301
2022-01-11 23:08:28,007 iteration 1934 : loss : 0.031025, loss_ce: 0.013230
2022-01-11 23:08:29,623 iteration 1935 : loss : 0.060822, loss_ce: 0.022728
2022-01-11 23:08:31,294 iteration 1936 : loss : 0.040293, loss_ce: 0.016971
2022-01-11 23:08:32,837 iteration 1937 : loss : 0.037996, loss_ce: 0.008958
2022-01-11 23:08:34,357 iteration 1938 : loss : 0.034565, loss_ce: 0.011621
 28%|████████▎                    | 114/400 [56:43<2:14:47, 28.28s/it]2022-01-11 23:08:35,948 iteration 1939 : loss : 0.030085, loss_ce: 0.009073
2022-01-11 23:08:37,658 iteration 1940 : loss : 0.039711, loss_ce: 0.017376
2022-01-11 23:08:39,354 iteration 1941 : loss : 0.040135, loss_ce: 0.018428
2022-01-11 23:08:40,996 iteration 1942 : loss : 0.051473, loss_ce: 0.018349
2022-01-11 23:08:42,567 iteration 1943 : loss : 0.033892, loss_ce: 0.014289
2022-01-11 23:08:44,182 iteration 1944 : loss : 0.027998, loss_ce: 0.010659
2022-01-11 23:08:45,735 iteration 1945 : loss : 0.046558, loss_ce: 0.015925
2022-01-11 23:08:47,344 iteration 1946 : loss : 0.031469, loss_ce: 0.012199
2022-01-11 23:08:48,976 iteration 1947 : loss : 0.045413, loss_ce: 0.020877
2022-01-11 23:08:50,577 iteration 1948 : loss : 0.061388, loss_ce: 0.019185
2022-01-11 23:08:52,151 iteration 1949 : loss : 0.025444, loss_ce: 0.011423
2022-01-11 23:08:53,683 iteration 1950 : loss : 0.026011, loss_ce: 0.011024
2022-01-11 23:08:55,334 iteration 1951 : loss : 0.042279, loss_ce: 0.014055
2022-01-11 23:08:56,980 iteration 1952 : loss : 0.039631, loss_ce: 0.012870
2022-01-11 23:08:58,663 iteration 1953 : loss : 0.057661, loss_ce: 0.031715
2022-01-11 23:09:00,285 iteration 1954 : loss : 0.022626, loss_ce: 0.010345
2022-01-11 23:09:00,286 Training Data Eval:
2022-01-11 23:09:08,238   Average segmentation loss on training set: 0.0263
2022-01-11 23:09:08,239 Validation Data Eval:
2022-01-11 23:09:10,971   Average segmentation loss on validation set: 0.0892
2022-01-11 23:09:12,591 iteration 1955 : loss : 0.030684, loss_ce: 0.008180
 29%|████████▎                    | 115/400 [57:21<2:28:30, 31.27s/it]2022-01-11 23:09:14,190 iteration 1956 : loss : 0.038252, loss_ce: 0.010217
2022-01-11 23:09:15,889 iteration 1957 : loss : 0.047096, loss_ce: 0.019349
2022-01-11 23:09:17,587 iteration 1958 : loss : 0.037992, loss_ce: 0.013305
2022-01-11 23:09:19,214 iteration 1959 : loss : 0.041182, loss_ce: 0.015102
2022-01-11 23:09:20,741 iteration 1960 : loss : 0.040907, loss_ce: 0.020282
2022-01-11 23:09:22,362 iteration 1961 : loss : 0.033897, loss_ce: 0.015081
2022-01-11 23:09:24,016 iteration 1962 : loss : 0.032296, loss_ce: 0.015803
2022-01-11 23:09:25,550 iteration 1963 : loss : 0.036337, loss_ce: 0.013880
2022-01-11 23:09:27,098 iteration 1964 : loss : 0.027297, loss_ce: 0.010942
2022-01-11 23:09:28,653 iteration 1965 : loss : 0.043016, loss_ce: 0.021173
2022-01-11 23:09:30,343 iteration 1966 : loss : 0.054933, loss_ce: 0.024578
2022-01-11 23:09:32,029 iteration 1967 : loss : 0.068021, loss_ce: 0.022494
2022-01-11 23:09:33,690 iteration 1968 : loss : 0.041608, loss_ce: 0.022229
2022-01-11 23:09:35,323 iteration 1969 : loss : 0.036804, loss_ce: 0.015392
2022-01-11 23:09:36,921 iteration 1970 : loss : 0.046633, loss_ce: 0.015213
2022-01-11 23:09:38,573 iteration 1971 : loss : 0.049233, loss_ce: 0.022224
2022-01-11 23:09:40,144 iteration 1972 : loss : 0.050330, loss_ce: 0.016738
 29%|████████▍                    | 116/400 [57:48<2:22:42, 30.15s/it]2022-01-11 23:09:41,650 iteration 1973 : loss : 0.029348, loss_ce: 0.012154
2022-01-11 23:09:43,271 iteration 1974 : loss : 0.037038, loss_ce: 0.013739
2022-01-11 23:09:44,856 iteration 1975 : loss : 0.032541, loss_ce: 0.011527
2022-01-11 23:09:46,539 iteration 1976 : loss : 0.043490, loss_ce: 0.018637
2022-01-11 23:09:48,147 iteration 1977 : loss : 0.044274, loss_ce: 0.015025
2022-01-11 23:09:49,666 iteration 1978 : loss : 0.037211, loss_ce: 0.014949
2022-01-11 23:09:51,176 iteration 1979 : loss : 0.035430, loss_ce: 0.014603
2022-01-11 23:09:52,792 iteration 1980 : loss : 0.033750, loss_ce: 0.014333
2022-01-11 23:09:54,346 iteration 1981 : loss : 0.034832, loss_ce: 0.010849
2022-01-11 23:09:55,853 iteration 1982 : loss : 0.038314, loss_ce: 0.014304
2022-01-11 23:09:57,525 iteration 1983 : loss : 0.041397, loss_ce: 0.020413
2022-01-11 23:09:59,045 iteration 1984 : loss : 0.039321, loss_ce: 0.011490
2022-01-11 23:10:00,647 iteration 1985 : loss : 0.023351, loss_ce: 0.009430
2022-01-11 23:10:02,252 iteration 1986 : loss : 0.044315, loss_ce: 0.021951
2022-01-11 23:10:03,924 iteration 1987 : loss : 0.030311, loss_ce: 0.010718
2022-01-11 23:10:05,466 iteration 1988 : loss : 0.031935, loss_ce: 0.009254
2022-01-11 23:10:07,059 iteration 1989 : loss : 0.034685, loss_ce: 0.016901
 29%|████████▍                    | 117/400 [58:15<2:17:38, 29.18s/it]2022-01-11 23:10:08,603 iteration 1990 : loss : 0.026221, loss_ce: 0.011738
2022-01-11 23:10:10,139 iteration 1991 : loss : 0.023572, loss_ce: 0.008661
2022-01-11 23:10:11,765 iteration 1992 : loss : 0.038507, loss_ce: 0.014802
2022-01-11 23:10:13,509 iteration 1993 : loss : 0.043300, loss_ce: 0.021227
2022-01-11 23:10:15,052 iteration 1994 : loss : 0.028305, loss_ce: 0.011888
2022-01-11 23:10:16,638 iteration 1995 : loss : 0.052444, loss_ce: 0.015550
2022-01-11 23:10:18,252 iteration 1996 : loss : 0.050796, loss_ce: 0.015919
2022-01-11 23:10:19,841 iteration 1997 : loss : 0.024228, loss_ce: 0.008213
2022-01-11 23:10:21,454 iteration 1998 : loss : 0.038989, loss_ce: 0.015862
2022-01-11 23:10:23,029 iteration 1999 : loss : 0.044093, loss_ce: 0.018700
2022-01-11 23:10:24,718 iteration 2000 : loss : 0.035202, loss_ce: 0.008308
2022-01-11 23:10:26,293 iteration 2001 : loss : 0.040112, loss_ce: 0.018086
2022-01-11 23:10:27,966 iteration 2002 : loss : 0.048778, loss_ce: 0.030530
2022-01-11 23:10:29,516 iteration 2003 : loss : 0.035966, loss_ce: 0.016280
2022-01-11 23:10:31,140 iteration 2004 : loss : 0.040119, loss_ce: 0.013676
2022-01-11 23:10:32,790 iteration 2005 : loss : 0.042057, loss_ce: 0.015366
2022-01-11 23:10:34,431 iteration 2006 : loss : 0.056584, loss_ce: 0.018487
 30%|████████▌                    | 118/400 [58:43<2:14:36, 28.64s/it]2022-01-11 23:10:36,101 iteration 2007 : loss : 0.039742, loss_ce: 0.015940
2022-01-11 23:10:37,647 iteration 2008 : loss : 0.052310, loss_ce: 0.021337
2022-01-11 23:10:39,224 iteration 2009 : loss : 0.071526, loss_ce: 0.024225
2022-01-11 23:10:40,760 iteration 2010 : loss : 0.037636, loss_ce: 0.016343
2022-01-11 23:10:42,347 iteration 2011 : loss : 0.035841, loss_ce: 0.016839
2022-01-11 23:10:43,961 iteration 2012 : loss : 0.042031, loss_ce: 0.016601
2022-01-11 23:10:45,609 iteration 2013 : loss : 0.033613, loss_ce: 0.012113
2022-01-11 23:10:47,255 iteration 2014 : loss : 0.031282, loss_ce: 0.010414
2022-01-11 23:10:48,775 iteration 2015 : loss : 0.029102, loss_ce: 0.010487
2022-01-11 23:10:50,370 iteration 2016 : loss : 0.039244, loss_ce: 0.013522
2022-01-11 23:10:51,853 iteration 2017 : loss : 0.029367, loss_ce: 0.012453
2022-01-11 23:10:53,443 iteration 2018 : loss : 0.048821, loss_ce: 0.021848
2022-01-11 23:10:54,979 iteration 2019 : loss : 0.026319, loss_ce: 0.011729
2022-01-11 23:10:56,594 iteration 2020 : loss : 0.041770, loss_ce: 0.017256
2022-01-11 23:10:58,222 iteration 2021 : loss : 0.031609, loss_ce: 0.012350
2022-01-11 23:10:59,896 iteration 2022 : loss : 0.042946, loss_ce: 0.019680
2022-01-11 23:11:01,493 iteration 2023 : loss : 0.052585, loss_ce: 0.017485
 30%|████████▋                    | 119/400 [59:10<2:11:54, 28.16s/it]2022-01-11 23:11:03,169 iteration 2024 : loss : 0.050827, loss_ce: 0.021632
2022-01-11 23:11:04,720 iteration 2025 : loss : 0.025220, loss_ce: 0.008652
2022-01-11 23:11:06,289 iteration 2026 : loss : 0.035937, loss_ce: 0.013966
2022-01-11 23:11:07,914 iteration 2027 : loss : 0.043121, loss_ce: 0.016546
2022-01-11 23:11:09,526 iteration 2028 : loss : 0.060472, loss_ce: 0.021960
2022-01-11 23:11:11,044 iteration 2029 : loss : 0.035098, loss_ce: 0.018082
2022-01-11 23:11:12,649 iteration 2030 : loss : 0.026989, loss_ce: 0.009847
2022-01-11 23:11:14,278 iteration 2031 : loss : 0.035873, loss_ce: 0.012939
2022-01-11 23:11:15,918 iteration 2032 : loss : 0.034285, loss_ce: 0.012832
2022-01-11 23:11:17,499 iteration 2033 : loss : 0.043967, loss_ce: 0.012297
2022-01-11 23:11:19,007 iteration 2034 : loss : 0.028923, loss_ce: 0.010055
2022-01-11 23:11:20,521 iteration 2035 : loss : 0.026872, loss_ce: 0.012688
2022-01-11 23:11:22,157 iteration 2036 : loss : 0.033883, loss_ce: 0.014110
2022-01-11 23:11:23,747 iteration 2037 : loss : 0.036551, loss_ce: 0.013890
2022-01-11 23:11:25,300 iteration 2038 : loss : 0.030702, loss_ce: 0.011312
2022-01-11 23:11:26,928 iteration 2039 : loss : 0.033430, loss_ce: 0.014025
2022-01-11 23:11:26,928 Training Data Eval:
2022-01-11 23:11:34,884   Average segmentation loss on training set: 0.0336
2022-01-11 23:11:34,885 Validation Data Eval:
2022-01-11 23:11:37,642   Average segmentation loss on validation set: 0.1271
2022-01-11 23:11:39,200 iteration 2040 : loss : 0.033534, loss_ce: 0.012144
 30%|████████▋                    | 120/400 [59:47<2:24:47, 31.03s/it]2022-01-11 23:11:40,832 iteration 2041 : loss : 0.038684, loss_ce: 0.015503
2022-01-11 23:11:42,405 iteration 2042 : loss : 0.069916, loss_ce: 0.033080
2022-01-11 23:11:43,999 iteration 2043 : loss : 0.042382, loss_ce: 0.018323
2022-01-11 23:11:45,661 iteration 2044 : loss : 0.031407, loss_ce: 0.013371
2022-01-11 23:11:47,353 iteration 2045 : loss : 0.037659, loss_ce: 0.013982
2022-01-11 23:11:48,996 iteration 2046 : loss : 0.045011, loss_ce: 0.017279
2022-01-11 23:11:50,621 iteration 2047 : loss : 0.029067, loss_ce: 0.011047
2022-01-11 23:11:52,138 iteration 2048 : loss : 0.033381, loss_ce: 0.010570
2022-01-11 23:11:53,708 iteration 2049 : loss : 0.038227, loss_ce: 0.014108
2022-01-11 23:11:55,330 iteration 2050 : loss : 0.054898, loss_ce: 0.016683
2022-01-11 23:11:56,968 iteration 2051 : loss : 0.040407, loss_ce: 0.015544
2022-01-11 23:11:58,519 iteration 2052 : loss : 0.036736, loss_ce: 0.017527
2022-01-11 23:12:00,061 iteration 2053 : loss : 0.033757, loss_ce: 0.010803
2022-01-11 23:12:01,666 iteration 2054 : loss : 0.035849, loss_ce: 0.012491
2022-01-11 23:12:03,288 iteration 2055 : loss : 0.057688, loss_ce: 0.026632
2022-01-11 23:12:04,902 iteration 2056 : loss : 0.039414, loss_ce: 0.018006
2022-01-11 23:12:06,535 iteration 2057 : loss : 0.030040, loss_ce: 0.011783
 30%|████████▏                  | 121/400 [1:00:15<2:19:08, 29.92s/it]2022-01-11 23:12:08,272 iteration 2058 : loss : 0.031481, loss_ce: 0.014553
2022-01-11 23:12:09,920 iteration 2059 : loss : 0.031353, loss_ce: 0.010218
2022-01-11 23:12:11,580 iteration 2060 : loss : 0.033630, loss_ce: 0.010447
2022-01-11 23:12:13,153 iteration 2061 : loss : 0.027557, loss_ce: 0.013134
2022-01-11 23:12:14,686 iteration 2062 : loss : 0.031799, loss_ce: 0.011176
2022-01-11 23:12:16,229 iteration 2063 : loss : 0.022219, loss_ce: 0.009406
2022-01-11 23:12:17,836 iteration 2064 : loss : 0.029060, loss_ce: 0.013970
2022-01-11 23:12:19,450 iteration 2065 : loss : 0.038541, loss_ce: 0.013170
2022-01-11 23:12:21,108 iteration 2066 : loss : 0.067001, loss_ce: 0.023579
2022-01-11 23:12:22,704 iteration 2067 : loss : 0.032561, loss_ce: 0.011405
2022-01-11 23:12:24,365 iteration 2068 : loss : 0.040841, loss_ce: 0.019709
2022-01-11 23:12:26,043 iteration 2069 : loss : 0.057885, loss_ce: 0.015229
2022-01-11 23:12:27,618 iteration 2070 : loss : 0.028223, loss_ce: 0.010247
2022-01-11 23:12:29,210 iteration 2071 : loss : 0.032040, loss_ce: 0.013855
2022-01-11 23:12:30,749 iteration 2072 : loss : 0.034261, loss_ce: 0.013928
2022-01-11 23:12:32,273 iteration 2073 : loss : 0.034862, loss_ce: 0.010886
2022-01-11 23:12:33,880 iteration 2074 : loss : 0.062530, loss_ce: 0.022183
 30%|████████▏                  | 122/400 [1:00:42<2:15:03, 29.15s/it]2022-01-11 23:12:35,522 iteration 2075 : loss : 0.037295, loss_ce: 0.013959
2022-01-11 23:12:37,200 iteration 2076 : loss : 0.047733, loss_ce: 0.012578
2022-01-11 23:12:38,828 iteration 2077 : loss : 0.030441, loss_ce: 0.011520
2022-01-11 23:12:40,376 iteration 2078 : loss : 0.027077, loss_ce: 0.011324
2022-01-11 23:12:41,942 iteration 2079 : loss : 0.026733, loss_ce: 0.009230
2022-01-11 23:12:43,603 iteration 2080 : loss : 0.036882, loss_ce: 0.011989
2022-01-11 23:12:45,229 iteration 2081 : loss : 0.040292, loss_ce: 0.015886
2022-01-11 23:12:46,838 iteration 2082 : loss : 0.033453, loss_ce: 0.013908
2022-01-11 23:12:48,422 iteration 2083 : loss : 0.022798, loss_ce: 0.007918
2022-01-11 23:12:50,015 iteration 2084 : loss : 0.032396, loss_ce: 0.011599
2022-01-11 23:12:51,565 iteration 2085 : loss : 0.028304, loss_ce: 0.013300
2022-01-11 23:12:53,156 iteration 2086 : loss : 0.025605, loss_ce: 0.011713
2022-01-11 23:12:54,692 iteration 2087 : loss : 0.031648, loss_ce: 0.010734
2022-01-11 23:12:56,416 iteration 2088 : loss : 0.069067, loss_ce: 0.033451
2022-01-11 23:12:57,996 iteration 2089 : loss : 0.027895, loss_ce: 0.009972
2022-01-11 23:12:59,639 iteration 2090 : loss : 0.045660, loss_ce: 0.018100
2022-01-11 23:13:01,234 iteration 2091 : loss : 0.037883, loss_ce: 0.016138
 31%|████████▎                  | 123/400 [1:01:09<2:12:04, 28.61s/it]2022-01-11 23:13:02,859 iteration 2092 : loss : 0.028642, loss_ce: 0.011233
2022-01-11 23:13:04,551 iteration 2093 : loss : 0.027914, loss_ce: 0.012228
2022-01-11 23:13:06,199 iteration 2094 : loss : 0.029110, loss_ce: 0.013114
2022-01-11 23:13:07,833 iteration 2095 : loss : 0.041289, loss_ce: 0.017143
2022-01-11 23:13:09,499 iteration 2096 : loss : 0.045240, loss_ce: 0.015388
2022-01-11 23:13:11,053 iteration 2097 : loss : 0.027996, loss_ce: 0.013715
2022-01-11 23:13:12,728 iteration 2098 : loss : 0.030517, loss_ce: 0.011371
2022-01-11 23:13:14,382 iteration 2099 : loss : 0.074165, loss_ce: 0.025353
2022-01-11 23:13:16,108 iteration 2100 : loss : 0.051850, loss_ce: 0.026859
2022-01-11 23:13:17,724 iteration 2101 : loss : 0.037651, loss_ce: 0.012137
2022-01-11 23:13:19,309 iteration 2102 : loss : 0.029001, loss_ce: 0.012150
2022-01-11 23:13:21,042 iteration 2103 : loss : 0.051444, loss_ce: 0.021943
2022-01-11 23:13:22,699 iteration 2104 : loss : 0.027344, loss_ce: 0.009724
2022-01-11 23:13:24,288 iteration 2105 : loss : 0.051357, loss_ce: 0.017341
2022-01-11 23:13:25,990 iteration 2106 : loss : 0.041749, loss_ce: 0.011900
2022-01-11 23:13:27,610 iteration 2107 : loss : 0.034039, loss_ce: 0.012850
2022-01-11 23:13:29,246 iteration 2108 : loss : 0.025334, loss_ce: 0.009518
 31%|████████▎                  | 124/400 [1:01:37<2:10:46, 28.43s/it]2022-01-11 23:13:30,892 iteration 2109 : loss : 0.038756, loss_ce: 0.014680
2022-01-11 23:13:32,479 iteration 2110 : loss : 0.037831, loss_ce: 0.010637
2022-01-11 23:13:34,210 iteration 2111 : loss : 0.041146, loss_ce: 0.013928
2022-01-11 23:13:35,953 iteration 2112 : loss : 0.031456, loss_ce: 0.013036
2022-01-11 23:13:37,535 iteration 2113 : loss : 0.038041, loss_ce: 0.014963
2022-01-11 23:13:39,123 iteration 2114 : loss : 0.054225, loss_ce: 0.020851
2022-01-11 23:13:40,601 iteration 2115 : loss : 0.036085, loss_ce: 0.012654
2022-01-11 23:13:42,229 iteration 2116 : loss : 0.039173, loss_ce: 0.016424
2022-01-11 23:13:43,919 iteration 2117 : loss : 0.047226, loss_ce: 0.014152
2022-01-11 23:13:45,524 iteration 2118 : loss : 0.052577, loss_ce: 0.027544
2022-01-11 23:13:47,170 iteration 2119 : loss : 0.032783, loss_ce: 0.011047
2022-01-11 23:13:48,786 iteration 2120 : loss : 0.045865, loss_ce: 0.017680
2022-01-11 23:13:50,494 iteration 2121 : loss : 0.039721, loss_ce: 0.015362
2022-01-11 23:13:52,059 iteration 2122 : loss : 0.052383, loss_ce: 0.026255
2022-01-11 23:13:53,618 iteration 2123 : loss : 0.034567, loss_ce: 0.019028
2022-01-11 23:13:55,263 iteration 2124 : loss : 0.032996, loss_ce: 0.011980
2022-01-11 23:13:55,263 Training Data Eval:
2022-01-11 23:14:03,228   Average segmentation loss on training set: 0.0278
2022-01-11 23:14:03,229 Validation Data Eval:
2022-01-11 23:14:05,973   Average segmentation loss on validation set: 0.0962
2022-01-11 23:14:07,579 iteration 2125 : loss : 0.025219, loss_ce: 0.008828
 31%|████████▍                  | 125/400 [1:02:16<2:23:55, 31.40s/it]2022-01-11 23:14:09,216 iteration 2126 : loss : 0.025673, loss_ce: 0.009392
2022-01-11 23:14:10,905 iteration 2127 : loss : 0.034615, loss_ce: 0.012707
2022-01-11 23:14:12,445 iteration 2128 : loss : 0.039233, loss_ce: 0.017814
2022-01-11 23:14:14,061 iteration 2129 : loss : 0.037523, loss_ce: 0.015694
2022-01-11 23:14:15,603 iteration 2130 : loss : 0.045585, loss_ce: 0.017935
2022-01-11 23:14:17,298 iteration 2131 : loss : 0.066741, loss_ce: 0.028625
2022-01-11 23:14:18,984 iteration 2132 : loss : 0.040181, loss_ce: 0.014761
2022-01-11 23:14:20,640 iteration 2133 : loss : 0.036635, loss_ce: 0.015394
2022-01-11 23:14:22,298 iteration 2134 : loss : 0.085847, loss_ce: 0.030713
2022-01-11 23:14:23,937 iteration 2135 : loss : 0.027973, loss_ce: 0.009204
2022-01-11 23:14:25,556 iteration 2136 : loss : 0.037994, loss_ce: 0.013613
2022-01-11 23:14:27,169 iteration 2137 : loss : 0.043494, loss_ce: 0.012481
2022-01-11 23:14:28,741 iteration 2138 : loss : 0.081458, loss_ce: 0.022414
2022-01-11 23:14:30,257 iteration 2139 : loss : 0.029617, loss_ce: 0.013625
2022-01-11 23:14:31,827 iteration 2140 : loss : 0.031262, loss_ce: 0.011145
2022-01-11 23:14:33,442 iteration 2141 : loss : 0.035309, loss_ce: 0.014968
2022-01-11 23:14:35,043 iteration 2142 : loss : 0.060546, loss_ce: 0.032608
 32%|████████▌                  | 126/400 [1:02:43<2:18:00, 30.22s/it]2022-01-11 23:14:36,722 iteration 2143 : loss : 0.039969, loss_ce: 0.012257
2022-01-11 23:14:38,313 iteration 2144 : loss : 0.055636, loss_ce: 0.019824
2022-01-11 23:14:39,808 iteration 2145 : loss : 0.028483, loss_ce: 0.013494
2022-01-11 23:14:41,568 iteration 2146 : loss : 0.046220, loss_ce: 0.018200
2022-01-11 23:14:43,205 iteration 2147 : loss : 0.026407, loss_ce: 0.010897
2022-01-11 23:14:44,819 iteration 2148 : loss : 0.064456, loss_ce: 0.025429
2022-01-11 23:14:46,437 iteration 2149 : loss : 0.048232, loss_ce: 0.018228
2022-01-11 23:14:48,109 iteration 2150 : loss : 0.047721, loss_ce: 0.016288
2022-01-11 23:14:49,779 iteration 2151 : loss : 0.042207, loss_ce: 0.015404
2022-01-11 23:14:51,321 iteration 2152 : loss : 0.029292, loss_ce: 0.012676
2022-01-11 23:14:52,895 iteration 2153 : loss : 0.042815, loss_ce: 0.011357
2022-01-11 23:14:54,407 iteration 2154 : loss : 0.040076, loss_ce: 0.017711
2022-01-11 23:14:55,971 iteration 2155 : loss : 0.034072, loss_ce: 0.012998
2022-01-11 23:14:57,625 iteration 2156 : loss : 0.037170, loss_ce: 0.013964
2022-01-11 23:14:59,246 iteration 2157 : loss : 0.060627, loss_ce: 0.031650
2022-01-11 23:15:00,829 iteration 2158 : loss : 0.028938, loss_ce: 0.011756
2022-01-11 23:15:02,489 iteration 2159 : loss : 0.070674, loss_ce: 0.032113
 32%|████████▌                  | 127/400 [1:03:11<2:13:43, 29.39s/it]2022-01-11 23:15:04,061 iteration 2160 : loss : 0.026477, loss_ce: 0.008843
2022-01-11 23:15:05,686 iteration 2161 : loss : 0.034048, loss_ce: 0.013633
2022-01-11 23:15:07,300 iteration 2162 : loss : 0.033956, loss_ce: 0.014288
2022-01-11 23:15:08,793 iteration 2163 : loss : 0.049431, loss_ce: 0.014907
2022-01-11 23:15:10,410 iteration 2164 : loss : 0.031961, loss_ce: 0.014114
2022-01-11 23:15:12,009 iteration 2165 : loss : 0.024356, loss_ce: 0.007620
2022-01-11 23:15:13,682 iteration 2166 : loss : 0.051955, loss_ce: 0.025383
2022-01-11 23:15:15,283 iteration 2167 : loss : 0.028096, loss_ce: 0.013326
2022-01-11 23:15:16,897 iteration 2168 : loss : 0.035502, loss_ce: 0.014204
2022-01-11 23:15:18,521 iteration 2169 : loss : 0.030738, loss_ce: 0.013106
2022-01-11 23:15:20,092 iteration 2170 : loss : 0.038974, loss_ce: 0.012543
2022-01-11 23:15:21,688 iteration 2171 : loss : 0.021252, loss_ce: 0.006968
2022-01-11 23:15:23,169 iteration 2172 : loss : 0.029089, loss_ce: 0.013385
2022-01-11 23:15:24,766 iteration 2173 : loss : 0.037260, loss_ce: 0.013542
2022-01-11 23:15:26,408 iteration 2174 : loss : 0.036967, loss_ce: 0.016263
2022-01-11 23:15:27,956 iteration 2175 : loss : 0.038868, loss_ce: 0.013594
2022-01-11 23:15:29,524 iteration 2176 : loss : 0.029784, loss_ce: 0.014258
 32%|████████▋                  | 128/400 [1:03:38<2:10:00, 28.68s/it]2022-01-11 23:15:31,240 iteration 2177 : loss : 0.045438, loss_ce: 0.014583
2022-01-11 23:15:32,781 iteration 2178 : loss : 0.036059, loss_ce: 0.017891
2022-01-11 23:15:34,372 iteration 2179 : loss : 0.029308, loss_ce: 0.012141
2022-01-11 23:15:36,000 iteration 2180 : loss : 0.032704, loss_ce: 0.009824
2022-01-11 23:15:37,573 iteration 2181 : loss : 0.030986, loss_ce: 0.011324
2022-01-11 23:15:39,156 iteration 2182 : loss : 0.029002, loss_ce: 0.010789
2022-01-11 23:15:40,698 iteration 2183 : loss : 0.042214, loss_ce: 0.015508
2022-01-11 23:15:42,216 iteration 2184 : loss : 0.068821, loss_ce: 0.035339
2022-01-11 23:15:43,713 iteration 2185 : loss : 0.031985, loss_ce: 0.012389
2022-01-11 23:15:45,435 iteration 2186 : loss : 0.084968, loss_ce: 0.048545
2022-01-11 23:15:46,950 iteration 2187 : loss : 0.031068, loss_ce: 0.015701
2022-01-11 23:15:48,582 iteration 2188 : loss : 0.033068, loss_ce: 0.012648
2022-01-11 23:15:50,124 iteration 2189 : loss : 0.035191, loss_ce: 0.012729
2022-01-11 23:15:51,764 iteration 2190 : loss : 0.029965, loss_ce: 0.011648
2022-01-11 23:15:53,288 iteration 2191 : loss : 0.027932, loss_ce: 0.012380
2022-01-11 23:15:54,867 iteration 2192 : loss : 0.042230, loss_ce: 0.015290
2022-01-11 23:15:56,528 iteration 2193 : loss : 0.036959, loss_ce: 0.015951
 32%|████████▋                  | 129/400 [1:04:05<2:07:16, 28.18s/it]2022-01-11 23:15:58,136 iteration 2194 : loss : 0.039727, loss_ce: 0.016222
2022-01-11 23:15:59,694 iteration 2195 : loss : 0.048118, loss_ce: 0.017119
2022-01-11 23:16:01,289 iteration 2196 : loss : 0.034490, loss_ce: 0.014755
2022-01-11 23:16:02,921 iteration 2197 : loss : 0.049065, loss_ce: 0.016590
2022-01-11 23:16:04,486 iteration 2198 : loss : 0.024927, loss_ce: 0.011778
2022-01-11 23:16:06,262 iteration 2199 : loss : 0.042753, loss_ce: 0.018633
2022-01-11 23:16:07,863 iteration 2200 : loss : 0.026516, loss_ce: 0.010452
2022-01-11 23:16:09,479 iteration 2201 : loss : 0.044354, loss_ce: 0.013926
2022-01-11 23:16:11,075 iteration 2202 : loss : 0.037094, loss_ce: 0.010943
2022-01-11 23:16:12,724 iteration 2203 : loss : 0.030266, loss_ce: 0.009921
2022-01-11 23:16:14,388 iteration 2204 : loss : 0.034256, loss_ce: 0.012341
2022-01-11 23:16:15,879 iteration 2205 : loss : 0.026612, loss_ce: 0.011020
2022-01-11 23:16:17,394 iteration 2206 : loss : 0.024463, loss_ce: 0.011346
2022-01-11 23:16:18,999 iteration 2207 : loss : 0.042027, loss_ce: 0.018315
2022-01-11 23:16:20,578 iteration 2208 : loss : 0.029193, loss_ce: 0.010663
2022-01-11 23:16:22,242 iteration 2209 : loss : 0.042291, loss_ce: 0.012707
2022-01-11 23:16:22,242 Training Data Eval:
2022-01-11 23:16:30,204   Average segmentation loss on training set: 0.0227
2022-01-11 23:16:30,204 Validation Data Eval:
2022-01-11 23:16:32,940   Average segmentation loss on validation set: 0.0726
2022-01-11 23:16:34,552 iteration 2210 : loss : 0.030194, loss_ce: 0.011132
 32%|████████▊                  | 130/400 [1:04:43<2:20:05, 31.13s/it]2022-01-11 23:16:36,202 iteration 2211 : loss : 0.031165, loss_ce: 0.015802
2022-01-11 23:16:37,710 iteration 2212 : loss : 0.028931, loss_ce: 0.010137
2022-01-11 23:16:39,247 iteration 2213 : loss : 0.024040, loss_ce: 0.008943
2022-01-11 23:16:40,851 iteration 2214 : loss : 0.031856, loss_ce: 0.014411
2022-01-11 23:16:42,429 iteration 2215 : loss : 0.028949, loss_ce: 0.010748
2022-01-11 23:16:43,976 iteration 2216 : loss : 0.026704, loss_ce: 0.012265
2022-01-11 23:16:45,483 iteration 2217 : loss : 0.028801, loss_ce: 0.013107
2022-01-11 23:16:47,085 iteration 2218 : loss : 0.024696, loss_ce: 0.008784
2022-01-11 23:16:48,658 iteration 2219 : loss : 0.029717, loss_ce: 0.012328
2022-01-11 23:16:50,257 iteration 2220 : loss : 0.029214, loss_ce: 0.013505
2022-01-11 23:16:51,920 iteration 2221 : loss : 0.031785, loss_ce: 0.013407
2022-01-11 23:16:53,478 iteration 2222 : loss : 0.049269, loss_ce: 0.015152
2022-01-11 23:16:55,200 iteration 2223 : loss : 0.058064, loss_ce: 0.022791
2022-01-11 23:16:56,773 iteration 2224 : loss : 0.069005, loss_ce: 0.018552
2022-01-11 23:16:58,346 iteration 2225 : loss : 0.027665, loss_ce: 0.010879
2022-01-11 23:17:00,086 iteration 2226 : loss : 0.052900, loss_ce: 0.017271
2022-01-11 23:17:01,741 iteration 2227 : loss : 0.037179, loss_ce: 0.011164
 33%|████████▊                  | 131/400 [1:05:10<2:14:16, 29.95s/it]2022-01-11 23:17:03,444 iteration 2228 : loss : 0.064039, loss_ce: 0.023916
2022-01-11 23:17:05,074 iteration 2229 : loss : 0.039741, loss_ce: 0.017677
2022-01-11 23:17:06,651 iteration 2230 : loss : 0.039503, loss_ce: 0.013477
2022-01-11 23:17:08,193 iteration 2231 : loss : 0.030051, loss_ce: 0.013913
2022-01-11 23:17:09,733 iteration 2232 : loss : 0.030523, loss_ce: 0.014519
2022-01-11 23:17:11,271 iteration 2233 : loss : 0.048407, loss_ce: 0.013765
2022-01-11 23:17:12,899 iteration 2234 : loss : 0.041485, loss_ce: 0.016384
2022-01-11 23:17:14,422 iteration 2235 : loss : 0.036698, loss_ce: 0.016157
2022-01-11 23:17:15,979 iteration 2236 : loss : 0.040180, loss_ce: 0.014357
2022-01-11 23:17:17,567 iteration 2237 : loss : 0.031576, loss_ce: 0.012661
2022-01-11 23:17:19,136 iteration 2238 : loss : 0.043512, loss_ce: 0.013251
2022-01-11 23:17:20,744 iteration 2239 : loss : 0.037908, loss_ce: 0.012401
2022-01-11 23:17:22,259 iteration 2240 : loss : 0.029348, loss_ce: 0.010467
2022-01-11 23:17:23,881 iteration 2241 : loss : 0.038123, loss_ce: 0.015662
2022-01-11 23:17:25,413 iteration 2242 : loss : 0.031570, loss_ce: 0.013647
2022-01-11 23:17:26,941 iteration 2243 : loss : 0.033521, loss_ce: 0.011247
2022-01-11 23:17:28,511 iteration 2244 : loss : 0.022524, loss_ce: 0.008987
 33%|████████▉                  | 132/400 [1:05:37<2:09:30, 28.99s/it]2022-01-11 23:17:30,099 iteration 2245 : loss : 0.024998, loss_ce: 0.009552
2022-01-11 23:17:31,717 iteration 2246 : loss : 0.046381, loss_ce: 0.020684
2022-01-11 23:17:33,317 iteration 2247 : loss : 0.044574, loss_ce: 0.019194
2022-01-11 23:17:34,795 iteration 2248 : loss : 0.023641, loss_ce: 0.010525
2022-01-11 23:17:36,456 iteration 2249 : loss : 0.035499, loss_ce: 0.016271
2022-01-11 23:17:38,186 iteration 2250 : loss : 0.034506, loss_ce: 0.015263
2022-01-11 23:17:39,709 iteration 2251 : loss : 0.026879, loss_ce: 0.012181
2022-01-11 23:17:41,322 iteration 2252 : loss : 0.033154, loss_ce: 0.011887
2022-01-11 23:17:42,853 iteration 2253 : loss : 0.043151, loss_ce: 0.012617
2022-01-11 23:17:44,498 iteration 2254 : loss : 0.037793, loss_ce: 0.017114
2022-01-11 23:17:46,156 iteration 2255 : loss : 0.028768, loss_ce: 0.012971
2022-01-11 23:17:47,854 iteration 2256 : loss : 0.043469, loss_ce: 0.018366
2022-01-11 23:17:49,433 iteration 2257 : loss : 0.040633, loss_ce: 0.012598
2022-01-11 23:17:51,028 iteration 2258 : loss : 0.038484, loss_ce: 0.015100
2022-01-11 23:17:52,668 iteration 2259 : loss : 0.041281, loss_ce: 0.016881
2022-01-11 23:17:54,271 iteration 2260 : loss : 0.038375, loss_ce: 0.012756
2022-01-11 23:17:55,810 iteration 2261 : loss : 0.034684, loss_ce: 0.009168
 33%|████████▉                  | 133/400 [1:06:04<2:06:46, 28.49s/it]2022-01-11 23:17:57,589 iteration 2262 : loss : 0.039365, loss_ce: 0.018611
2022-01-11 23:17:59,224 iteration 2263 : loss : 0.031596, loss_ce: 0.012712
2022-01-11 23:18:00,857 iteration 2264 : loss : 0.039588, loss_ce: 0.010116
2022-01-11 23:18:02,484 iteration 2265 : loss : 0.029849, loss_ce: 0.013438
2022-01-11 23:18:04,074 iteration 2266 : loss : 0.027674, loss_ce: 0.010084
2022-01-11 23:18:05,668 iteration 2267 : loss : 0.030585, loss_ce: 0.009864
2022-01-11 23:18:07,177 iteration 2268 : loss : 0.023990, loss_ce: 0.010481
2022-01-11 23:18:08,790 iteration 2269 : loss : 0.032471, loss_ce: 0.013469
2022-01-11 23:18:10,327 iteration 2270 : loss : 0.032454, loss_ce: 0.012220
2022-01-11 23:18:11,894 iteration 2271 : loss : 0.041868, loss_ce: 0.024994
2022-01-11 23:18:13,424 iteration 2272 : loss : 0.027815, loss_ce: 0.007125
2022-01-11 23:18:15,064 iteration 2273 : loss : 0.038887, loss_ce: 0.010306
2022-01-11 23:18:16,591 iteration 2274 : loss : 0.037079, loss_ce: 0.010881
2022-01-11 23:18:18,169 iteration 2275 : loss : 0.031339, loss_ce: 0.013144
2022-01-11 23:18:19,758 iteration 2276 : loss : 0.039523, loss_ce: 0.013021
2022-01-11 23:18:21,325 iteration 2277 : loss : 0.029651, loss_ce: 0.012368
2022-01-11 23:18:22,908 iteration 2278 : loss : 0.025519, loss_ce: 0.011291
 34%|█████████                  | 134/400 [1:06:31<2:04:26, 28.07s/it]2022-01-11 23:18:24,647 iteration 2279 : loss : 0.039237, loss_ce: 0.017707
2022-01-11 23:18:26,275 iteration 2280 : loss : 0.041235, loss_ce: 0.021986
2022-01-11 23:18:27,789 iteration 2281 : loss : 0.027689, loss_ce: 0.009623
2022-01-11 23:18:29,511 iteration 2282 : loss : 0.035433, loss_ce: 0.011354
2022-01-11 23:18:31,068 iteration 2283 : loss : 0.030879, loss_ce: 0.012265
2022-01-11 23:18:32,765 iteration 2284 : loss : 0.050358, loss_ce: 0.018945
2022-01-11 23:18:34,317 iteration 2285 : loss : 0.025573, loss_ce: 0.009087
2022-01-11 23:18:35,886 iteration 2286 : loss : 0.027054, loss_ce: 0.009669
2022-01-11 23:18:37,518 iteration 2287 : loss : 0.028474, loss_ce: 0.008293
2022-01-11 23:18:39,027 iteration 2288 : loss : 0.029882, loss_ce: 0.011827
2022-01-11 23:18:40,643 iteration 2289 : loss : 0.032626, loss_ce: 0.011100
2022-01-11 23:18:42,239 iteration 2290 : loss : 0.026773, loss_ce: 0.009472
2022-01-11 23:18:43,821 iteration 2291 : loss : 0.026483, loss_ce: 0.009263
2022-01-11 23:18:45,479 iteration 2292 : loss : 0.036802, loss_ce: 0.012374
2022-01-11 23:18:46,996 iteration 2293 : loss : 0.038485, loss_ce: 0.016467
2022-01-11 23:18:48,557 iteration 2294 : loss : 0.028700, loss_ce: 0.012297
2022-01-11 23:18:48,558 Training Data Eval:
2022-01-11 23:18:56,529   Average segmentation loss on training set: 0.0208
2022-01-11 23:18:56,530 Validation Data Eval:
2022-01-11 23:18:59,277   Average segmentation loss on validation set: 0.0672
2022-01-11 23:19:00,864 iteration 2295 : loss : 0.022636, loss_ce: 0.007899
 34%|█████████                  | 135/400 [1:07:09<2:17:04, 31.04s/it]2022-01-11 23:19:02,502 iteration 2296 : loss : 0.028982, loss_ce: 0.010713
2022-01-11 23:19:04,104 iteration 2297 : loss : 0.029069, loss_ce: 0.008494
2022-01-11 23:19:05,822 iteration 2298 : loss : 0.063813, loss_ce: 0.021910
2022-01-11 23:19:07,401 iteration 2299 : loss : 0.022820, loss_ce: 0.006238
2022-01-11 23:19:09,085 iteration 2300 : loss : 0.035612, loss_ce: 0.012409
2022-01-11 23:19:10,669 iteration 2301 : loss : 0.032969, loss_ce: 0.014568
2022-01-11 23:19:12,241 iteration 2302 : loss : 0.039169, loss_ce: 0.019650
2022-01-11 23:19:13,783 iteration 2303 : loss : 0.035113, loss_ce: 0.011605
2022-01-11 23:19:15,272 iteration 2304 : loss : 0.035864, loss_ce: 0.014089
2022-01-11 23:19:16,897 iteration 2305 : loss : 0.040971, loss_ce: 0.013309
2022-01-11 23:19:18,433 iteration 2306 : loss : 0.030792, loss_ce: 0.010728
2022-01-11 23:19:19,952 iteration 2307 : loss : 0.027338, loss_ce: 0.010739
2022-01-11 23:19:21,506 iteration 2308 : loss : 0.034362, loss_ce: 0.015153
2022-01-11 23:19:23,123 iteration 2309 : loss : 0.034083, loss_ce: 0.014723
2022-01-11 23:19:24,737 iteration 2310 : loss : 0.035744, loss_ce: 0.015981
2022-01-11 23:19:26,426 iteration 2311 : loss : 0.031737, loss_ce: 0.011533
2022-01-11 23:19:28,049 iteration 2312 : loss : 0.027687, loss_ce: 0.012889
 34%|█████████▏                 | 136/400 [1:07:36<2:11:28, 29.88s/it]2022-01-11 23:19:29,679 iteration 2313 : loss : 0.031046, loss_ce: 0.016169
2022-01-11 23:19:31,219 iteration 2314 : loss : 0.036974, loss_ce: 0.014712
2022-01-11 23:19:32,828 iteration 2315 : loss : 0.028460, loss_ce: 0.010800
2022-01-11 23:19:34,461 iteration 2316 : loss : 0.028681, loss_ce: 0.014578
2022-01-11 23:19:35,972 iteration 2317 : loss : 0.037837, loss_ce: 0.015486
2022-01-11 23:19:37,640 iteration 2318 : loss : 0.026100, loss_ce: 0.011897
2022-01-11 23:19:39,326 iteration 2319 : loss : 0.052740, loss_ce: 0.018296
2022-01-11 23:19:40,867 iteration 2320 : loss : 0.032039, loss_ce: 0.015889
2022-01-11 23:19:42,464 iteration 2321 : loss : 0.035993, loss_ce: 0.013015
2022-01-11 23:19:44,068 iteration 2322 : loss : 0.025682, loss_ce: 0.010343
2022-01-11 23:19:45,736 iteration 2323 : loss : 0.042737, loss_ce: 0.014136
2022-01-11 23:19:47,351 iteration 2324 : loss : 0.025281, loss_ce: 0.008234
2022-01-11 23:19:48,944 iteration 2325 : loss : 0.041978, loss_ce: 0.018990
2022-01-11 23:19:50,596 iteration 2326 : loss : 0.031284, loss_ce: 0.015973
2022-01-11 23:19:52,173 iteration 2327 : loss : 0.026406, loss_ce: 0.008958
2022-01-11 23:19:53,875 iteration 2328 : loss : 0.033441, loss_ce: 0.012921
2022-01-11 23:19:55,458 iteration 2329 : loss : 0.044823, loss_ce: 0.018230
 34%|█████████▏                 | 137/400 [1:08:04<2:07:43, 29.14s/it]2022-01-11 23:19:57,077 iteration 2330 : loss : 0.025192, loss_ce: 0.009811
2022-01-11 23:19:58,617 iteration 2331 : loss : 0.033621, loss_ce: 0.009691
2022-01-11 23:20:00,299 iteration 2332 : loss : 0.037634, loss_ce: 0.019614
2022-01-11 23:20:01,823 iteration 2333 : loss : 0.026475, loss_ce: 0.014019
2022-01-11 23:20:03,403 iteration 2334 : loss : 0.040684, loss_ce: 0.009058
2022-01-11 23:20:05,113 iteration 2335 : loss : 0.043351, loss_ce: 0.021405
2022-01-11 23:20:06,681 iteration 2336 : loss : 0.031049, loss_ce: 0.014938
2022-01-11 23:20:08,273 iteration 2337 : loss : 0.025713, loss_ce: 0.009921
2022-01-11 23:20:09,899 iteration 2338 : loss : 0.048223, loss_ce: 0.024544
2022-01-11 23:20:11,494 iteration 2339 : loss : 0.067466, loss_ce: 0.031394
2022-01-11 23:20:13,133 iteration 2340 : loss : 0.039965, loss_ce: 0.016140
2022-01-11 23:20:14,724 iteration 2341 : loss : 0.033528, loss_ce: 0.013515
2022-01-11 23:20:16,356 iteration 2342 : loss : 0.030462, loss_ce: 0.011860
2022-01-11 23:20:17,951 iteration 2343 : loss : 0.033135, loss_ce: 0.014148
2022-01-11 23:20:19,444 iteration 2344 : loss : 0.030681, loss_ce: 0.013456
2022-01-11 23:20:20,997 iteration 2345 : loss : 0.028010, loss_ce: 0.011178
2022-01-11 23:20:22,515 iteration 2346 : loss : 0.028528, loss_ce: 0.010278
 34%|█████████▎                 | 138/400 [1:08:31<2:04:30, 28.51s/it]2022-01-11 23:20:24,193 iteration 2347 : loss : 0.048966, loss_ce: 0.020229
2022-01-11 23:20:25,764 iteration 2348 : loss : 0.040118, loss_ce: 0.012759
2022-01-11 23:20:27,373 iteration 2349 : loss : 0.042163, loss_ce: 0.021788
2022-01-11 23:20:28,870 iteration 2350 : loss : 0.027291, loss_ce: 0.013347
2022-01-11 23:20:30,481 iteration 2351 : loss : 0.039685, loss_ce: 0.011942
2022-01-11 23:20:32,177 iteration 2352 : loss : 0.033409, loss_ce: 0.013621
2022-01-11 23:20:33,691 iteration 2353 : loss : 0.028101, loss_ce: 0.012811
2022-01-11 23:20:35,289 iteration 2354 : loss : 0.038954, loss_ce: 0.013993
2022-01-11 23:20:36,812 iteration 2355 : loss : 0.035900, loss_ce: 0.015272
2022-01-11 23:20:38,499 iteration 2356 : loss : 0.044140, loss_ce: 0.017892
2022-01-11 23:20:40,162 iteration 2357 : loss : 0.035274, loss_ce: 0.015972
2022-01-11 23:20:41,860 iteration 2358 : loss : 0.060588, loss_ce: 0.023477
2022-01-11 23:20:43,513 iteration 2359 : loss : 0.077033, loss_ce: 0.016616
2022-01-11 23:20:45,011 iteration 2360 : loss : 0.058486, loss_ce: 0.012762
2022-01-11 23:20:46,529 iteration 2361 : loss : 0.027244, loss_ce: 0.009571
2022-01-11 23:20:48,118 iteration 2362 : loss : 0.060729, loss_ce: 0.024215
2022-01-11 23:20:49,710 iteration 2363 : loss : 0.050293, loss_ce: 0.014311
 35%|█████████▍                 | 139/400 [1:08:58<2:02:19, 28.12s/it]2022-01-11 23:20:51,336 iteration 2364 : loss : 0.064189, loss_ce: 0.029154
2022-01-11 23:20:52,854 iteration 2365 : loss : 0.049173, loss_ce: 0.019053
2022-01-11 23:20:54,456 iteration 2366 : loss : 0.043918, loss_ce: 0.019812
2022-01-11 23:20:56,009 iteration 2367 : loss : 0.052907, loss_ce: 0.014596
2022-01-11 23:20:57,694 iteration 2368 : loss : 0.039499, loss_ce: 0.012053
2022-01-11 23:20:59,271 iteration 2369 : loss : 0.044137, loss_ce: 0.014450
2022-01-11 23:21:00,812 iteration 2370 : loss : 0.027138, loss_ce: 0.009567
2022-01-11 23:21:02,426 iteration 2371 : loss : 0.039460, loss_ce: 0.014411
2022-01-11 23:21:03,939 iteration 2372 : loss : 0.028148, loss_ce: 0.014298
2022-01-11 23:21:05,543 iteration 2373 : loss : 0.033418, loss_ce: 0.012457
2022-01-11 23:21:07,190 iteration 2374 : loss : 0.033809, loss_ce: 0.013779
2022-01-11 23:21:08,729 iteration 2375 : loss : 0.032673, loss_ce: 0.010873
2022-01-11 23:21:10,386 iteration 2376 : loss : 0.047129, loss_ce: 0.022331
2022-01-11 23:21:11,966 iteration 2377 : loss : 0.027387, loss_ce: 0.006945
2022-01-11 23:21:13,668 iteration 2378 : loss : 0.046698, loss_ce: 0.023928
2022-01-11 23:21:15,308 iteration 2379 : loss : 0.061812, loss_ce: 0.017008
2022-01-11 23:21:15,308 Training Data Eval:
2022-01-11 23:21:23,272   Average segmentation loss on training set: 0.0308
2022-01-11 23:21:23,273 Validation Data Eval:
2022-01-11 23:21:26,015   Average segmentation loss on validation set: 0.1034
2022-01-11 23:21:27,741 iteration 2380 : loss : 0.032604, loss_ce: 0.011537
 35%|█████████▍                 | 140/400 [1:09:36<2:14:43, 31.09s/it]2022-01-11 23:21:29,332 iteration 2381 : loss : 0.031924, loss_ce: 0.011115
2022-01-11 23:21:30,990 iteration 2382 : loss : 0.046512, loss_ce: 0.021098
2022-01-11 23:21:32,642 iteration 2383 : loss : 0.044250, loss_ce: 0.017451
2022-01-11 23:21:34,242 iteration 2384 : loss : 0.052664, loss_ce: 0.015160
2022-01-11 23:21:35,857 iteration 2385 : loss : 0.038313, loss_ce: 0.020349
2022-01-11 23:21:37,557 iteration 2386 : loss : 0.037573, loss_ce: 0.012956
2022-01-11 23:21:39,122 iteration 2387 : loss : 0.037027, loss_ce: 0.012882
2022-01-11 23:21:40,744 iteration 2388 : loss : 0.027435, loss_ce: 0.011346
2022-01-11 23:21:42,283 iteration 2389 : loss : 0.037311, loss_ce: 0.014398
2022-01-11 23:21:43,882 iteration 2390 : loss : 0.029508, loss_ce: 0.012776
2022-01-11 23:21:45,431 iteration 2391 : loss : 0.031082, loss_ce: 0.012146
2022-01-11 23:21:46,961 iteration 2392 : loss : 0.030055, loss_ce: 0.012159
2022-01-11 23:21:48,488 iteration 2393 : loss : 0.030075, loss_ce: 0.014176
2022-01-11 23:21:50,086 iteration 2394 : loss : 0.059998, loss_ce: 0.014044
2022-01-11 23:21:51,838 iteration 2395 : loss : 0.028739, loss_ce: 0.011013
2022-01-11 23:21:53,528 iteration 2396 : loss : 0.029336, loss_ce: 0.011862
2022-01-11 23:21:55,141 iteration 2397 : loss : 0.047389, loss_ce: 0.015428
 35%|█████████▌                 | 141/400 [1:10:03<2:09:26, 29.99s/it]2022-01-11 23:21:56,905 iteration 2398 : loss : 0.065057, loss_ce: 0.042019
2022-01-11 23:21:58,429 iteration 2399 : loss : 0.029173, loss_ce: 0.011151
2022-01-11 23:22:00,007 iteration 2400 : loss : 0.036972, loss_ce: 0.011062
2022-01-11 23:22:01,612 iteration 2401 : loss : 0.030019, loss_ce: 0.011253
2022-01-11 23:22:03,177 iteration 2402 : loss : 0.030476, loss_ce: 0.013584
2022-01-11 23:22:04,761 iteration 2403 : loss : 0.023597, loss_ce: 0.008736
2022-01-11 23:22:06,311 iteration 2404 : loss : 0.038267, loss_ce: 0.019457
2022-01-11 23:22:07,904 iteration 2405 : loss : 0.032543, loss_ce: 0.013096
2022-01-11 23:22:09,573 iteration 2406 : loss : 0.045276, loss_ce: 0.020245
2022-01-11 23:22:11,105 iteration 2407 : loss : 0.036761, loss_ce: 0.013586
2022-01-11 23:22:12,686 iteration 2408 : loss : 0.031313, loss_ce: 0.014294
2022-01-11 23:22:14,279 iteration 2409 : loss : 0.039804, loss_ce: 0.014689
2022-01-11 23:22:15,927 iteration 2410 : loss : 0.057387, loss_ce: 0.017038
2022-01-11 23:22:17,459 iteration 2411 : loss : 0.031309, loss_ce: 0.009493
2022-01-11 23:22:19,121 iteration 2412 : loss : 0.033476, loss_ce: 0.012821
2022-01-11 23:22:20,821 iteration 2413 : loss : 0.036372, loss_ce: 0.019485
2022-01-11 23:22:22,481 iteration 2414 : loss : 0.046552, loss_ce: 0.019726
 36%|█████████▌                 | 142/400 [1:10:31<2:05:30, 29.19s/it]2022-01-11 23:22:24,145 iteration 2415 : loss : 0.033444, loss_ce: 0.010072
2022-01-11 23:22:25,746 iteration 2416 : loss : 0.037692, loss_ce: 0.019698
2022-01-11 23:22:27,364 iteration 2417 : loss : 0.026562, loss_ce: 0.008680
2022-01-11 23:22:28,998 iteration 2418 : loss : 0.034230, loss_ce: 0.011549
2022-01-11 23:22:30,598 iteration 2419 : loss : 0.027073, loss_ce: 0.013393
2022-01-11 23:22:32,136 iteration 2420 : loss : 0.024146, loss_ce: 0.009460
2022-01-11 23:22:33,851 iteration 2421 : loss : 0.031848, loss_ce: 0.008488
2022-01-11 23:22:35,491 iteration 2422 : loss : 0.036436, loss_ce: 0.011862
2022-01-11 23:22:37,118 iteration 2423 : loss : 0.034428, loss_ce: 0.016335
2022-01-11 23:22:38,650 iteration 2424 : loss : 0.036691, loss_ce: 0.013041
2022-01-11 23:22:40,322 iteration 2425 : loss : 0.039732, loss_ce: 0.019392
2022-01-11 23:22:41,931 iteration 2426 : loss : 0.027433, loss_ce: 0.010584
2022-01-11 23:22:43,658 iteration 2427 : loss : 0.068389, loss_ce: 0.022883
2022-01-11 23:22:45,350 iteration 2428 : loss : 0.052052, loss_ce: 0.021530
2022-01-11 23:22:47,067 iteration 2429 : loss : 0.028086, loss_ce: 0.012145
2022-01-11 23:22:48,691 iteration 2430 : loss : 0.028477, loss_ce: 0.012296
2022-01-11 23:22:50,261 iteration 2431 : loss : 0.026854, loss_ce: 0.012990
 36%|█████████▋                 | 143/400 [1:10:58<2:03:13, 28.77s/it]2022-01-11 23:22:51,949 iteration 2432 : loss : 0.025792, loss_ce: 0.011293
2022-01-11 23:22:53,566 iteration 2433 : loss : 0.049721, loss_ce: 0.019481
2022-01-11 23:22:55,180 iteration 2434 : loss : 0.025387, loss_ce: 0.007272
2022-01-11 23:22:56,926 iteration 2435 : loss : 0.034634, loss_ce: 0.020512
2022-01-11 23:22:58,436 iteration 2436 : loss : 0.039303, loss_ce: 0.014665
2022-01-11 23:22:59,959 iteration 2437 : loss : 0.022038, loss_ce: 0.009097
2022-01-11 23:23:01,704 iteration 2438 : loss : 0.038817, loss_ce: 0.017197
2022-01-11 23:23:03,334 iteration 2439 : loss : 0.037448, loss_ce: 0.016174
2022-01-11 23:23:04,916 iteration 2440 : loss : 0.028653, loss_ce: 0.009940
2022-01-11 23:23:06,493 iteration 2441 : loss : 0.041775, loss_ce: 0.012100
2022-01-11 23:23:08,044 iteration 2442 : loss : 0.040779, loss_ce: 0.012682
2022-01-11 23:23:09,571 iteration 2443 : loss : 0.025053, loss_ce: 0.011214
2022-01-11 23:23:11,192 iteration 2444 : loss : 0.024017, loss_ce: 0.009288
2022-01-11 23:23:12,688 iteration 2445 : loss : 0.047472, loss_ce: 0.011051
2022-01-11 23:23:14,385 iteration 2446 : loss : 0.026235, loss_ce: 0.009118
2022-01-11 23:23:16,071 iteration 2447 : loss : 0.035592, loss_ce: 0.015395
2022-01-11 23:23:17,681 iteration 2448 : loss : 0.039860, loss_ce: 0.014536
 36%|█████████▋                 | 144/400 [1:11:26<2:01:01, 28.36s/it]2022-01-11 23:23:19,310 iteration 2449 : loss : 0.032839, loss_ce: 0.012906
2022-01-11 23:23:20,981 iteration 2450 : loss : 0.040560, loss_ce: 0.016472
2022-01-11 23:23:22,656 iteration 2451 : loss : 0.033949, loss_ce: 0.012016
2022-01-11 23:23:24,183 iteration 2452 : loss : 0.019186, loss_ce: 0.008448
2022-01-11 23:23:25,777 iteration 2453 : loss : 0.035782, loss_ce: 0.009624
2022-01-11 23:23:27,415 iteration 2454 : loss : 0.029064, loss_ce: 0.013864
2022-01-11 23:23:29,056 iteration 2455 : loss : 0.039086, loss_ce: 0.017953
2022-01-11 23:23:30,809 iteration 2456 : loss : 0.049682, loss_ce: 0.022412
2022-01-11 23:23:32,411 iteration 2457 : loss : 0.033352, loss_ce: 0.012218
2022-01-11 23:23:34,097 iteration 2458 : loss : 0.029363, loss_ce: 0.009264
2022-01-11 23:23:35,662 iteration 2459 : loss : 0.046694, loss_ce: 0.013652
2022-01-11 23:23:37,249 iteration 2460 : loss : 0.032991, loss_ce: 0.010301
2022-01-11 23:23:38,864 iteration 2461 : loss : 0.046910, loss_ce: 0.016086
2022-01-11 23:23:40,442 iteration 2462 : loss : 0.030541, loss_ce: 0.014045
2022-01-11 23:23:42,076 iteration 2463 : loss : 0.031182, loss_ce: 0.010915
2022-01-11 23:23:43,772 iteration 2464 : loss : 0.033200, loss_ce: 0.014089
2022-01-11 23:23:43,773 Training Data Eval:
2022-01-11 23:23:51,743   Average segmentation loss on training set: 0.0237
2022-01-11 23:23:51,743 Validation Data Eval:
2022-01-11 23:23:54,488   Average segmentation loss on validation set: 0.1084
2022-01-11 23:23:56,049 iteration 2465 : loss : 0.032855, loss_ce: 0.011356
 36%|█████████▊                 | 145/400 [1:12:04<2:13:17, 31.36s/it]2022-01-11 23:23:57,663 iteration 2466 : loss : 0.028749, loss_ce: 0.010580
2022-01-11 23:23:59,305 iteration 2467 : loss : 0.041804, loss_ce: 0.025717
2022-01-11 23:24:00,851 iteration 2468 : loss : 0.034654, loss_ce: 0.015883
2022-01-11 23:24:02,363 iteration 2469 : loss : 0.018088, loss_ce: 0.008546
2022-01-11 23:24:04,002 iteration 2470 : loss : 0.038858, loss_ce: 0.013590
2022-01-11 23:24:05,664 iteration 2471 : loss : 0.032078, loss_ce: 0.012288
2022-01-11 23:24:07,366 iteration 2472 : loss : 0.033676, loss_ce: 0.013062
2022-01-11 23:24:08,898 iteration 2473 : loss : 0.027171, loss_ce: 0.010402
2022-01-11 23:24:10,537 iteration 2474 : loss : 0.028677, loss_ce: 0.009309
2022-01-11 23:24:12,061 iteration 2475 : loss : 0.022450, loss_ce: 0.009554
2022-01-11 23:24:13,753 iteration 2476 : loss : 0.043121, loss_ce: 0.011774
2022-01-11 23:24:15,348 iteration 2477 : loss : 0.026528, loss_ce: 0.010202
2022-01-11 23:24:16,935 iteration 2478 : loss : 0.026737, loss_ce: 0.007687
2022-01-11 23:24:18,657 iteration 2479 : loss : 0.024838, loss_ce: 0.007296
2022-01-11 23:24:20,342 iteration 2480 : loss : 0.044114, loss_ce: 0.016892
2022-01-11 23:24:21,919 iteration 2481 : loss : 0.025244, loss_ce: 0.009436
2022-01-11 23:24:23,522 iteration 2482 : loss : 0.043625, loss_ce: 0.016568
 36%|█████████▊                 | 146/400 [1:12:32<2:07:50, 30.20s/it]2022-01-11 23:24:25,176 iteration 2483 : loss : 0.029670, loss_ce: 0.011583
2022-01-11 23:24:26,854 iteration 2484 : loss : 0.036285, loss_ce: 0.012350
2022-01-11 23:24:28,565 iteration 2485 : loss : 0.029620, loss_ce: 0.011371
2022-01-11 23:24:30,130 iteration 2486 : loss : 0.023297, loss_ce: 0.010555
2022-01-11 23:24:31,702 iteration 2487 : loss : 0.026302, loss_ce: 0.009519
2022-01-11 23:24:33,350 iteration 2488 : loss : 0.031522, loss_ce: 0.009896
2022-01-11 23:24:35,010 iteration 2489 : loss : 0.037688, loss_ce: 0.012431
2022-01-11 23:24:36,574 iteration 2490 : loss : 0.023427, loss_ce: 0.009931
2022-01-11 23:24:38,179 iteration 2491 : loss : 0.028015, loss_ce: 0.012566
2022-01-11 23:24:39,733 iteration 2492 : loss : 0.030583, loss_ce: 0.011250
2022-01-11 23:24:41,303 iteration 2493 : loss : 0.023475, loss_ce: 0.008025
2022-01-11 23:24:42,839 iteration 2494 : loss : 0.038909, loss_ce: 0.017268
2022-01-11 23:24:44,412 iteration 2495 : loss : 0.032921, loss_ce: 0.009196
2022-01-11 23:24:46,060 iteration 2496 : loss : 0.032704, loss_ce: 0.009650
2022-01-11 23:24:47,655 iteration 2497 : loss : 0.032654, loss_ce: 0.013753
2022-01-11 23:24:49,159 iteration 2498 : loss : 0.027688, loss_ce: 0.012833
2022-01-11 23:24:50,834 iteration 2499 : loss : 0.027342, loss_ce: 0.013213
 37%|█████████▉                 | 147/400 [1:12:59<2:03:40, 29.33s/it]2022-01-11 23:24:52,514 iteration 2500 : loss : 0.035782, loss_ce: 0.013139
2022-01-11 23:24:54,055 iteration 2501 : loss : 0.027475, loss_ce: 0.009105
2022-01-11 23:24:55,636 iteration 2502 : loss : 0.033938, loss_ce: 0.011700
2022-01-11 23:24:57,245 iteration 2503 : loss : 0.032451, loss_ce: 0.016701
2022-01-11 23:24:58,889 iteration 2504 : loss : 0.026532, loss_ce: 0.009590
2022-01-11 23:25:00,419 iteration 2505 : loss : 0.022943, loss_ce: 0.010185
2022-01-11 23:25:01,980 iteration 2506 : loss : 0.038017, loss_ce: 0.009770
2022-01-11 23:25:03,631 iteration 2507 : loss : 0.021312, loss_ce: 0.007703
2022-01-11 23:25:05,210 iteration 2508 : loss : 0.024550, loss_ce: 0.012211
2022-01-11 23:25:06,872 iteration 2509 : loss : 0.036090, loss_ce: 0.013857
2022-01-11 23:25:08,450 iteration 2510 : loss : 0.024671, loss_ce: 0.007956
2022-01-11 23:25:09,978 iteration 2511 : loss : 0.026289, loss_ce: 0.012963
2022-01-11 23:25:11,622 iteration 2512 : loss : 0.026939, loss_ce: 0.008052
2022-01-11 23:25:13,177 iteration 2513 : loss : 0.024658, loss_ce: 0.010772
2022-01-11 23:25:14,805 iteration 2514 : loss : 0.036431, loss_ce: 0.010599
2022-01-11 23:25:16,388 iteration 2515 : loss : 0.028253, loss_ce: 0.013267
2022-01-11 23:25:18,170 iteration 2516 : loss : 0.038007, loss_ce: 0.015069
 37%|█████████▉                 | 148/400 [1:13:26<2:00:41, 28.73s/it]2022-01-11 23:25:19,841 iteration 2517 : loss : 0.034095, loss_ce: 0.012166
2022-01-11 23:25:21,483 iteration 2518 : loss : 0.035986, loss_ce: 0.010710
2022-01-11 23:25:23,026 iteration 2519 : loss : 0.020250, loss_ce: 0.006698
2022-01-11 23:25:24,676 iteration 2520 : loss : 0.027590, loss_ce: 0.012551
2022-01-11 23:25:26,297 iteration 2521 : loss : 0.027903, loss_ce: 0.014180
2022-01-11 23:25:27,935 iteration 2522 : loss : 0.031623, loss_ce: 0.012158
2022-01-11 23:25:29,538 iteration 2523 : loss : 0.039932, loss_ce: 0.011726
2022-01-11 23:25:31,203 iteration 2524 : loss : 0.025878, loss_ce: 0.011007
2022-01-11 23:25:32,772 iteration 2525 : loss : 0.024167, loss_ce: 0.007788
2022-01-11 23:25:34,302 iteration 2526 : loss : 0.024315, loss_ce: 0.008218
2022-01-11 23:25:35,921 iteration 2527 : loss : 0.043804, loss_ce: 0.023159
2022-01-11 23:25:37,502 iteration 2528 : loss : 0.032735, loss_ce: 0.015298
2022-01-11 23:25:39,141 iteration 2529 : loss : 0.032904, loss_ce: 0.011221
2022-01-11 23:25:40,668 iteration 2530 : loss : 0.019581, loss_ce: 0.006981
2022-01-11 23:25:42,293 iteration 2531 : loss : 0.034812, loss_ce: 0.010386
2022-01-11 23:25:43,988 iteration 2532 : loss : 0.034227, loss_ce: 0.015973
2022-01-11 23:25:45,519 iteration 2533 : loss : 0.035856, loss_ce: 0.008886
 37%|██████████                 | 149/400 [1:13:54<1:58:27, 28.32s/it]2022-01-11 23:25:47,166 iteration 2534 : loss : 0.025072, loss_ce: 0.010157
2022-01-11 23:25:48,652 iteration 2535 : loss : 0.030798, loss_ce: 0.012977
2022-01-11 23:25:50,208 iteration 2536 : loss : 0.019456, loss_ce: 0.008398
2022-01-11 23:25:51,902 iteration 2537 : loss : 0.040731, loss_ce: 0.023189
2022-01-11 23:25:53,359 iteration 2538 : loss : 0.028147, loss_ce: 0.009497
2022-01-11 23:25:54,867 iteration 2539 : loss : 0.035587, loss_ce: 0.016627
2022-01-11 23:25:56,462 iteration 2540 : loss : 0.028973, loss_ce: 0.013010
2022-01-11 23:25:58,142 iteration 2541 : loss : 0.039737, loss_ce: 0.015814
2022-01-11 23:25:59,733 iteration 2542 : loss : 0.037221, loss_ce: 0.014123
2022-01-11 23:26:01,431 iteration 2543 : loss : 0.035789, loss_ce: 0.011058
2022-01-11 23:26:03,129 iteration 2544 : loss : 0.038681, loss_ce: 0.014671
2022-01-11 23:26:04,679 iteration 2545 : loss : 0.029419, loss_ce: 0.012121
2022-01-11 23:26:06,494 iteration 2546 : loss : 0.038085, loss_ce: 0.015110
2022-01-11 23:26:08,172 iteration 2547 : loss : 0.037655, loss_ce: 0.013572
2022-01-11 23:26:09,856 iteration 2548 : loss : 0.043715, loss_ce: 0.019196
2022-01-11 23:26:11,454 iteration 2549 : loss : 0.028836, loss_ce: 0.009837
2022-01-11 23:26:11,455 Training Data Eval:
2022-01-11 23:26:19,414   Average segmentation loss on training set: 0.0213
2022-01-11 23:26:19,414 Validation Data Eval:
2022-01-11 23:26:22,147   Average segmentation loss on validation set: 0.0871
2022-01-11 23:26:23,740 iteration 2550 : loss : 0.033198, loss_ce: 0.007039
 38%|██████████▏                | 150/400 [1:14:32<2:10:22, 31.29s/it]2022-01-11 23:26:25,377 iteration 2551 : loss : 0.032373, loss_ce: 0.011590
2022-01-11 23:26:26,992 iteration 2552 : loss : 0.020055, loss_ce: 0.008418
2022-01-11 23:26:28,608 iteration 2553 : loss : 0.033357, loss_ce: 0.014304
2022-01-11 23:26:30,203 iteration 2554 : loss : 0.031143, loss_ce: 0.016228
2022-01-11 23:26:31,783 iteration 2555 : loss : 0.021793, loss_ce: 0.008621
2022-01-11 23:26:33,380 iteration 2556 : loss : 0.027700, loss_ce: 0.011357
2022-01-11 23:26:34,968 iteration 2557 : loss : 0.027100, loss_ce: 0.010384
2022-01-11 23:26:36,536 iteration 2558 : loss : 0.026833, loss_ce: 0.009398
2022-01-11 23:26:38,066 iteration 2559 : loss : 0.037770, loss_ce: 0.015959
2022-01-11 23:26:39,756 iteration 2560 : loss : 0.030798, loss_ce: 0.009655
2022-01-11 23:26:41,343 iteration 2561 : loss : 0.035466, loss_ce: 0.013139
2022-01-11 23:26:42,956 iteration 2562 : loss : 0.024538, loss_ce: 0.010112
2022-01-11 23:26:44,475 iteration 2563 : loss : 0.036952, loss_ce: 0.017052
2022-01-11 23:26:46,023 iteration 2564 : loss : 0.020163, loss_ce: 0.008921
2022-01-11 23:26:47,556 iteration 2565 : loss : 0.020059, loss_ce: 0.007849
2022-01-11 23:26:49,062 iteration 2566 : loss : 0.021842, loss_ce: 0.007885
2022-01-11 23:26:50,633 iteration 2567 : loss : 0.045821, loss_ce: 0.012325
 38%|██████████▏                | 151/400 [1:14:59<2:04:22, 29.97s/it]2022-01-11 23:26:52,231 iteration 2568 : loss : 0.021635, loss_ce: 0.009272
2022-01-11 23:26:53,804 iteration 2569 : loss : 0.032901, loss_ce: 0.012162
2022-01-11 23:26:55,493 iteration 2570 : loss : 0.047779, loss_ce: 0.020148
2022-01-11 23:26:57,076 iteration 2571 : loss : 0.026170, loss_ce: 0.007949
2022-01-11 23:26:58,593 iteration 2572 : loss : 0.030809, loss_ce: 0.009918
2022-01-11 23:27:00,166 iteration 2573 : loss : 0.029245, loss_ce: 0.010488
2022-01-11 23:27:01,712 iteration 2574 : loss : 0.029566, loss_ce: 0.014515
2022-01-11 23:27:03,225 iteration 2575 : loss : 0.025063, loss_ce: 0.007935
2022-01-11 23:27:04,741 iteration 2576 : loss : 0.024519, loss_ce: 0.006635
2022-01-11 23:27:06,358 iteration 2577 : loss : 0.043856, loss_ce: 0.023697
2022-01-11 23:27:07,919 iteration 2578 : loss : 0.022402, loss_ce: 0.007503
2022-01-11 23:27:09,530 iteration 2579 : loss : 0.032144, loss_ce: 0.012005
2022-01-11 23:27:11,062 iteration 2580 : loss : 0.029834, loss_ce: 0.009159
2022-01-11 23:27:12,683 iteration 2581 : loss : 0.027391, loss_ce: 0.012978
2022-01-11 23:27:14,359 iteration 2582 : loss : 0.023718, loss_ce: 0.011260
2022-01-11 23:27:15,917 iteration 2583 : loss : 0.024294, loss_ce: 0.010828
2022-01-11 23:27:17,485 iteration 2584 : loss : 0.050766, loss_ce: 0.016409
 38%|██████████▎                | 152/400 [1:15:26<2:00:00, 29.04s/it]2022-01-11 23:27:19,123 iteration 2585 : loss : 0.045388, loss_ce: 0.016815
2022-01-11 23:27:20,922 iteration 2586 : loss : 0.041370, loss_ce: 0.013018
2022-01-11 23:27:22,605 iteration 2587 : loss : 0.043250, loss_ce: 0.014565
2022-01-11 23:27:24,214 iteration 2588 : loss : 0.036850, loss_ce: 0.013018
2022-01-11 23:27:25,733 iteration 2589 : loss : 0.040522, loss_ce: 0.011249
2022-01-11 23:27:27,372 iteration 2590 : loss : 0.036137, loss_ce: 0.013841
2022-01-11 23:27:28,969 iteration 2591 : loss : 0.024698, loss_ce: 0.010462
2022-01-11 23:27:30,526 iteration 2592 : loss : 0.023180, loss_ce: 0.010907
2022-01-11 23:27:32,011 iteration 2593 : loss : 0.032560, loss_ce: 0.013882
2022-01-11 23:27:33,604 iteration 2594 : loss : 0.049271, loss_ce: 0.022501
2022-01-11 23:27:35,118 iteration 2595 : loss : 0.026223, loss_ce: 0.008237
2022-01-11 23:27:36,675 iteration 2596 : loss : 0.026989, loss_ce: 0.007937
2022-01-11 23:27:38,253 iteration 2597 : loss : 0.029065, loss_ce: 0.009427
2022-01-11 23:27:39,855 iteration 2598 : loss : 0.026260, loss_ce: 0.010014
2022-01-11 23:27:41,463 iteration 2599 : loss : 0.025526, loss_ce: 0.009639
2022-01-11 23:27:43,026 iteration 2600 : loss : 0.034208, loss_ce: 0.014811
2022-01-11 23:27:44,548 iteration 2601 : loss : 0.028631, loss_ce: 0.010631
 38%|██████████▎                | 153/400 [1:15:53<1:57:05, 28.44s/it]2022-01-11 23:27:46,154 iteration 2602 : loss : 0.035710, loss_ce: 0.012427
2022-01-11 23:27:47,773 iteration 2603 : loss : 0.041194, loss_ce: 0.009174
2022-01-11 23:27:49,382 iteration 2604 : loss : 0.027472, loss_ce: 0.011121
2022-01-11 23:27:50,965 iteration 2605 : loss : 0.028103, loss_ce: 0.008673
2022-01-11 23:27:52,565 iteration 2606 : loss : 0.026959, loss_ce: 0.009650
2022-01-11 23:27:54,182 iteration 2607 : loss : 0.027551, loss_ce: 0.012769
2022-01-11 23:27:55,723 iteration 2608 : loss : 0.030079, loss_ce: 0.010865
2022-01-11 23:27:57,358 iteration 2609 : loss : 0.027431, loss_ce: 0.011078
2022-01-11 23:27:58,989 iteration 2610 : loss : 0.029785, loss_ce: 0.009516
2022-01-11 23:28:00,645 iteration 2611 : loss : 0.029551, loss_ce: 0.011109
2022-01-11 23:28:02,278 iteration 2612 : loss : 0.043641, loss_ce: 0.017155
2022-01-11 23:28:03,942 iteration 2613 : loss : 0.027187, loss_ce: 0.010325
2022-01-11 23:28:05,601 iteration 2614 : loss : 0.023551, loss_ce: 0.008895
2022-01-11 23:28:07,215 iteration 2615 : loss : 0.040260, loss_ce: 0.018871
2022-01-11 23:28:08,880 iteration 2616 : loss : 0.032999, loss_ce: 0.014022
2022-01-11 23:28:10,490 iteration 2617 : loss : 0.036838, loss_ce: 0.014840
2022-01-11 23:28:12,152 iteration 2618 : loss : 0.045621, loss_ce: 0.013018
 38%|██████████▍                | 154/400 [1:16:20<1:55:35, 28.19s/it]2022-01-11 23:28:13,929 iteration 2619 : loss : 0.034318, loss_ce: 0.014290
2022-01-11 23:28:15,584 iteration 2620 : loss : 0.038510, loss_ce: 0.012815
2022-01-11 23:28:17,074 iteration 2621 : loss : 0.025340, loss_ce: 0.010760
2022-01-11 23:28:18,689 iteration 2622 : loss : 0.042759, loss_ce: 0.013984
2022-01-11 23:28:20,237 iteration 2623 : loss : 0.025988, loss_ce: 0.009626
2022-01-11 23:28:21,881 iteration 2624 : loss : 0.047446, loss_ce: 0.018578
2022-01-11 23:28:23,548 iteration 2625 : loss : 0.030513, loss_ce: 0.013684
2022-01-11 23:28:25,181 iteration 2626 : loss : 0.031151, loss_ce: 0.011585
2022-01-11 23:28:26,695 iteration 2627 : loss : 0.022821, loss_ce: 0.011551
2022-01-11 23:28:28,234 iteration 2628 : loss : 0.023239, loss_ce: 0.009174
2022-01-11 23:28:29,730 iteration 2629 : loss : 0.033425, loss_ce: 0.009818
2022-01-11 23:28:31,369 iteration 2630 : loss : 0.049791, loss_ce: 0.010935
2022-01-11 23:28:32,890 iteration 2631 : loss : 0.031209, loss_ce: 0.016773
2022-01-11 23:28:34,423 iteration 2632 : loss : 0.036134, loss_ce: 0.014094
2022-01-11 23:28:36,021 iteration 2633 : loss : 0.017610, loss_ce: 0.006238
2022-01-11 23:28:37,580 iteration 2634 : loss : 0.036189, loss_ce: 0.011378
2022-01-11 23:28:37,580 Training Data Eval:
2022-01-11 23:28:45,543   Average segmentation loss on training set: 0.0200
2022-01-11 23:28:45,543 Validation Data Eval:
2022-01-11 23:28:48,281   Average segmentation loss on validation set: 0.0877
2022-01-11 23:28:49,838 iteration 2635 : loss : 0.030231, loss_ce: 0.012943
 39%|██████████▍                | 155/400 [1:16:58<2:06:44, 31.04s/it]2022-01-11 23:28:51,404 iteration 2636 : loss : 0.026226, loss_ce: 0.008445
2022-01-11 23:28:52,979 iteration 2637 : loss : 0.023108, loss_ce: 0.011218
2022-01-11 23:28:54,663 iteration 2638 : loss : 0.058688, loss_ce: 0.025494
2022-01-11 23:28:56,243 iteration 2639 : loss : 0.024694, loss_ce: 0.010001
2022-01-11 23:28:57,858 iteration 2640 : loss : 0.041922, loss_ce: 0.015593
2022-01-11 23:28:59,434 iteration 2641 : loss : 0.025100, loss_ce: 0.009947
2022-01-11 23:29:01,048 iteration 2642 : loss : 0.026279, loss_ce: 0.009515
2022-01-11 23:29:02,629 iteration 2643 : loss : 0.033644, loss_ce: 0.015951
2022-01-11 23:29:04,245 iteration 2644 : loss : 0.033280, loss_ce: 0.017589
2022-01-11 23:29:05,779 iteration 2645 : loss : 0.026128, loss_ce: 0.011223
2022-01-11 23:29:07,327 iteration 2646 : loss : 0.024662, loss_ce: 0.008294
2022-01-11 23:29:08,979 iteration 2647 : loss : 0.061427, loss_ce: 0.024628
2022-01-11 23:29:10,537 iteration 2648 : loss : 0.036794, loss_ce: 0.012503
2022-01-11 23:29:12,137 iteration 2649 : loss : 0.038220, loss_ce: 0.015208
2022-01-11 23:29:13,868 iteration 2650 : loss : 0.032149, loss_ce: 0.011126
2022-01-11 23:29:15,432 iteration 2651 : loss : 0.026867, loss_ce: 0.011160
2022-01-11 23:29:17,117 iteration 2652 : loss : 0.034710, loss_ce: 0.014404
 39%|██████████▌                | 156/400 [1:17:25<2:01:38, 29.91s/it]2022-01-11 23:29:18,823 iteration 2653 : loss : 0.035044, loss_ce: 0.013000
2022-01-11 23:29:20,349 iteration 2654 : loss : 0.026093, loss_ce: 0.010780
2022-01-11 23:29:21,917 iteration 2655 : loss : 0.023152, loss_ce: 0.010204
2022-01-11 23:29:23,493 iteration 2656 : loss : 0.022196, loss_ce: 0.011392
2022-01-11 23:29:25,120 iteration 2657 : loss : 0.022970, loss_ce: 0.010282
2022-01-11 23:29:26,774 iteration 2658 : loss : 0.043053, loss_ce: 0.014641
2022-01-11 23:29:28,393 iteration 2659 : loss : 0.034180, loss_ce: 0.012155
2022-01-11 23:29:30,093 iteration 2660 : loss : 0.043714, loss_ce: 0.021976
2022-01-11 23:29:31,589 iteration 2661 : loss : 0.028640, loss_ce: 0.009259
2022-01-11 23:29:33,263 iteration 2662 : loss : 0.027197, loss_ce: 0.010883
2022-01-11 23:29:34,907 iteration 2663 : loss : 0.052593, loss_ce: 0.018205
2022-01-11 23:29:36,432 iteration 2664 : loss : 0.030497, loss_ce: 0.010747
2022-01-11 23:29:37,984 iteration 2665 : loss : 0.015723, loss_ce: 0.006329
2022-01-11 23:29:39,670 iteration 2666 : loss : 0.030931, loss_ce: 0.011264
2022-01-11 23:29:41,208 iteration 2667 : loss : 0.030551, loss_ce: 0.013233
2022-01-11 23:29:42,865 iteration 2668 : loss : 0.037206, loss_ce: 0.013646
2022-01-11 23:29:44,442 iteration 2669 : loss : 0.027902, loss_ce: 0.007939
 39%|██████████▌                | 157/400 [1:17:53<1:58:00, 29.14s/it]2022-01-11 23:29:45,994 iteration 2670 : loss : 0.023887, loss_ce: 0.009911
2022-01-11 23:29:47,511 iteration 2671 : loss : 0.036078, loss_ce: 0.013266
2022-01-11 23:29:49,069 iteration 2672 : loss : 0.028301, loss_ce: 0.009160
2022-01-11 23:29:50,614 iteration 2673 : loss : 0.039145, loss_ce: 0.011662
2022-01-11 23:29:52,227 iteration 2674 : loss : 0.047898, loss_ce: 0.019246
2022-01-11 23:29:53,877 iteration 2675 : loss : 0.041838, loss_ce: 0.015426
2022-01-11 23:29:55,447 iteration 2676 : loss : 0.052078, loss_ce: 0.018064
2022-01-11 23:29:57,028 iteration 2677 : loss : 0.040763, loss_ce: 0.015128
2022-01-11 23:29:58,551 iteration 2678 : loss : 0.051172, loss_ce: 0.027846
2022-01-11 23:30:00,143 iteration 2679 : loss : 0.047484, loss_ce: 0.020182
2022-01-11 23:30:01,806 iteration 2680 : loss : 0.040123, loss_ce: 0.018066
2022-01-11 23:30:03,417 iteration 2681 : loss : 0.039334, loss_ce: 0.011947
2022-01-11 23:30:05,060 iteration 2682 : loss : 0.045142, loss_ce: 0.023295
2022-01-11 23:30:06,616 iteration 2683 : loss : 0.035538, loss_ce: 0.016973
2022-01-11 23:30:08,226 iteration 2684 : loss : 0.027500, loss_ce: 0.012112
2022-01-11 23:30:09,875 iteration 2685 : loss : 0.035499, loss_ce: 0.016340
2022-01-11 23:30:11,512 iteration 2686 : loss : 0.089469, loss_ce: 0.030815
 40%|██████████▋                | 158/400 [1:18:20<1:55:01, 28.52s/it]2022-01-11 23:30:13,107 iteration 2687 : loss : 0.035174, loss_ce: 0.013930
2022-01-11 23:30:14,677 iteration 2688 : loss : 0.034397, loss_ce: 0.013648
2022-01-11 23:30:16,211 iteration 2689 : loss : 0.039312, loss_ce: 0.014363
2022-01-11 23:30:17,855 iteration 2690 : loss : 0.061842, loss_ce: 0.014304
2022-01-11 23:30:19,385 iteration 2691 : loss : 0.024175, loss_ce: 0.010267
2022-01-11 23:30:20,921 iteration 2692 : loss : 0.050015, loss_ce: 0.018909
2022-01-11 23:30:22,454 iteration 2693 : loss : 0.024842, loss_ce: 0.010226
2022-01-11 23:30:24,062 iteration 2694 : loss : 0.038977, loss_ce: 0.017336
2022-01-11 23:30:25,640 iteration 2695 : loss : 0.032147, loss_ce: 0.014842
2022-01-11 23:30:27,229 iteration 2696 : loss : 0.031936, loss_ce: 0.014722
2022-01-11 23:30:28,772 iteration 2697 : loss : 0.042055, loss_ce: 0.021390
2022-01-11 23:30:30,311 iteration 2698 : loss : 0.038896, loss_ce: 0.013871
2022-01-11 23:30:31,879 iteration 2699 : loss : 0.038305, loss_ce: 0.015977
2022-01-11 23:30:33,419 iteration 2700 : loss : 0.036501, loss_ce: 0.012919
2022-01-11 23:30:35,131 iteration 2701 : loss : 0.057848, loss_ce: 0.020652
2022-01-11 23:30:36,764 iteration 2702 : loss : 0.032623, loss_ce: 0.016462
2022-01-11 23:30:38,427 iteration 2703 : loss : 0.032988, loss_ce: 0.013140
 40%|██████████▋                | 159/400 [1:18:47<1:52:36, 28.04s/it]2022-01-11 23:30:40,156 iteration 2704 : loss : 0.073822, loss_ce: 0.022722
2022-01-11 23:30:41,701 iteration 2705 : loss : 0.027870, loss_ce: 0.012262
2022-01-11 23:30:43,343 iteration 2706 : loss : 0.078180, loss_ce: 0.035363
2022-01-11 23:30:44,911 iteration 2707 : loss : 0.036134, loss_ce: 0.017660
2022-01-11 23:30:46,547 iteration 2708 : loss : 0.029691, loss_ce: 0.011619
2022-01-11 23:30:48,135 iteration 2709 : loss : 0.029541, loss_ce: 0.011482
2022-01-11 23:30:49,648 iteration 2710 : loss : 0.025752, loss_ce: 0.011244
2022-01-11 23:30:51,266 iteration 2711 : loss : 0.025210, loss_ce: 0.008966
2022-01-11 23:30:52,898 iteration 2712 : loss : 0.031741, loss_ce: 0.009365
2022-01-11 23:30:54,429 iteration 2713 : loss : 0.030617, loss_ce: 0.014213
2022-01-11 23:30:56,093 iteration 2714 : loss : 0.031549, loss_ce: 0.012560
2022-01-11 23:30:57,730 iteration 2715 : loss : 0.050059, loss_ce: 0.012920
2022-01-11 23:30:59,373 iteration 2716 : loss : 0.030102, loss_ce: 0.010569
2022-01-11 23:31:00,968 iteration 2717 : loss : 0.038813, loss_ce: 0.017323
2022-01-11 23:31:02,623 iteration 2718 : loss : 0.044602, loss_ce: 0.014655
2022-01-11 23:31:04,196 iteration 2719 : loss : 0.030380, loss_ce: 0.013062
2022-01-11 23:31:04,196 Training Data Eval:
2022-01-11 23:31:12,183   Average segmentation loss on training set: 0.0232
2022-01-11 23:31:12,184 Validation Data Eval:
2022-01-11 23:31:14,927   Average segmentation loss on validation set: 0.0755
2022-01-11 23:31:16,464 iteration 2720 : loss : 0.030259, loss_ce: 0.009786
 40%|██████████▊                | 160/400 [1:19:25<2:04:08, 31.03s/it]2022-01-11 23:31:18,130 iteration 2721 : loss : 0.024264, loss_ce: 0.010094
2022-01-11 23:31:19,739 iteration 2722 : loss : 0.031289, loss_ce: 0.011901
2022-01-11 23:31:21,372 iteration 2723 : loss : 0.048200, loss_ce: 0.016557
2022-01-11 23:31:22,973 iteration 2724 : loss : 0.044287, loss_ce: 0.014572
2022-01-11 23:31:24,551 iteration 2725 : loss : 0.041228, loss_ce: 0.012956
2022-01-11 23:31:26,295 iteration 2726 : loss : 0.063824, loss_ce: 0.019080
2022-01-11 23:31:27,943 iteration 2727 : loss : 0.029548, loss_ce: 0.011631
2022-01-11 23:31:29,556 iteration 2728 : loss : 0.025901, loss_ce: 0.011477
2022-01-11 23:31:31,141 iteration 2729 : loss : 0.036139, loss_ce: 0.016441
2022-01-11 23:31:32,665 iteration 2730 : loss : 0.022991, loss_ce: 0.009062
2022-01-11 23:31:34,284 iteration 2731 : loss : 0.025924, loss_ce: 0.009499
2022-01-11 23:31:35,951 iteration 2732 : loss : 0.042252, loss_ce: 0.018689
2022-01-11 23:31:37,574 iteration 2733 : loss : 0.028898, loss_ce: 0.009231
2022-01-11 23:31:39,179 iteration 2734 : loss : 0.029094, loss_ce: 0.009469
2022-01-11 23:31:40,889 iteration 2735 : loss : 0.034551, loss_ce: 0.010126
2022-01-11 23:31:42,479 iteration 2736 : loss : 0.021779, loss_ce: 0.009756
2022-01-11 23:31:44,140 iteration 2737 : loss : 0.027601, loss_ce: 0.010155
 40%|██████████▊                | 161/400 [1:19:52<1:59:36, 30.03s/it]2022-01-11 23:31:45,717 iteration 2738 : loss : 0.018659, loss_ce: 0.007476
2022-01-11 23:31:47,383 iteration 2739 : loss : 0.034879, loss_ce: 0.014339
2022-01-11 23:31:49,079 iteration 2740 : loss : 0.042922, loss_ce: 0.015777
2022-01-11 23:31:50,652 iteration 2741 : loss : 0.019727, loss_ce: 0.006898
2022-01-11 23:31:52,172 iteration 2742 : loss : 0.030751, loss_ce: 0.009634
2022-01-11 23:31:53,798 iteration 2743 : loss : 0.025308, loss_ce: 0.009930
2022-01-11 23:31:55,559 iteration 2744 : loss : 0.062941, loss_ce: 0.022697
2022-01-11 23:31:57,153 iteration 2745 : loss : 0.029391, loss_ce: 0.010102
2022-01-11 23:31:58,729 iteration 2746 : loss : 0.021943, loss_ce: 0.007970
2022-01-11 23:32:00,443 iteration 2747 : loss : 0.053847, loss_ce: 0.024073
2022-01-11 23:32:02,013 iteration 2748 : loss : 0.031613, loss_ce: 0.012021
2022-01-11 23:32:03,624 iteration 2749 : loss : 0.038305, loss_ce: 0.017865
2022-01-11 23:32:05,213 iteration 2750 : loss : 0.029124, loss_ce: 0.008885
2022-01-11 23:32:06,786 iteration 2751 : loss : 0.032540, loss_ce: 0.008288
2022-01-11 23:32:08,332 iteration 2752 : loss : 0.032970, loss_ce: 0.014043
2022-01-11 23:32:09,914 iteration 2753 : loss : 0.028501, loss_ce: 0.011153
2022-01-11 23:32:11,518 iteration 2754 : loss : 0.049657, loss_ce: 0.015527
 40%|██████████▉                | 162/400 [1:20:20<1:55:57, 29.23s/it]2022-01-11 23:32:13,162 iteration 2755 : loss : 0.028285, loss_ce: 0.010294
2022-01-11 23:32:14,790 iteration 2756 : loss : 0.030061, loss_ce: 0.013740
2022-01-11 23:32:16,341 iteration 2757 : loss : 0.023493, loss_ce: 0.009616
2022-01-11 23:32:17,926 iteration 2758 : loss : 0.035496, loss_ce: 0.014422
2022-01-11 23:32:19,503 iteration 2759 : loss : 0.020568, loss_ce: 0.007120
2022-01-11 23:32:21,147 iteration 2760 : loss : 0.073925, loss_ce: 0.017835
2022-01-11 23:32:22,749 iteration 2761 : loss : 0.026772, loss_ce: 0.009677
2022-01-11 23:32:24,326 iteration 2762 : loss : 0.036140, loss_ce: 0.016700
2022-01-11 23:32:25,903 iteration 2763 : loss : 0.026906, loss_ce: 0.010867
2022-01-11 23:32:27,432 iteration 2764 : loss : 0.024078, loss_ce: 0.010244
2022-01-11 23:32:29,012 iteration 2765 : loss : 0.038121, loss_ce: 0.015681
2022-01-11 23:32:30,667 iteration 2766 : loss : 0.037688, loss_ce: 0.015968
2022-01-11 23:32:32,178 iteration 2767 : loss : 0.022092, loss_ce: 0.006858
2022-01-11 23:32:33,894 iteration 2768 : loss : 0.028812, loss_ce: 0.011889
2022-01-11 23:32:35,463 iteration 2769 : loss : 0.024693, loss_ce: 0.008509
2022-01-11 23:32:36,988 iteration 2770 : loss : 0.026787, loss_ce: 0.014022
2022-01-11 23:32:38,576 iteration 2771 : loss : 0.030519, loss_ce: 0.015680
 41%|███████████                | 163/400 [1:20:47<1:52:53, 28.58s/it]2022-01-11 23:32:40,286 iteration 2772 : loss : 0.040494, loss_ce: 0.013462
2022-01-11 23:32:41,810 iteration 2773 : loss : 0.027017, loss_ce: 0.010121
2022-01-11 23:32:43,381 iteration 2774 : loss : 0.027920, loss_ce: 0.012452
2022-01-11 23:32:44,983 iteration 2775 : loss : 0.030970, loss_ce: 0.013396
2022-01-11 23:32:46,588 iteration 2776 : loss : 0.034461, loss_ce: 0.015765
2022-01-11 23:32:48,229 iteration 2777 : loss : 0.031399, loss_ce: 0.014347
2022-01-11 23:32:49,746 iteration 2778 : loss : 0.021657, loss_ce: 0.009778
2022-01-11 23:32:51,370 iteration 2779 : loss : 0.029971, loss_ce: 0.011939
2022-01-11 23:32:53,048 iteration 2780 : loss : 0.024083, loss_ce: 0.009467
2022-01-11 23:32:54,560 iteration 2781 : loss : 0.022866, loss_ce: 0.009294
2022-01-11 23:32:56,186 iteration 2782 : loss : 0.033730, loss_ce: 0.013475
2022-01-11 23:32:57,867 iteration 2783 : loss : 0.026965, loss_ce: 0.009173
2022-01-11 23:32:59,580 iteration 2784 : loss : 0.041642, loss_ce: 0.021193
2022-01-11 23:33:01,101 iteration 2785 : loss : 0.021880, loss_ce: 0.007468
2022-01-11 23:33:02,725 iteration 2786 : loss : 0.035846, loss_ce: 0.013082
2022-01-11 23:33:04,408 iteration 2787 : loss : 0.029243, loss_ce: 0.007701
2022-01-11 23:33:06,022 iteration 2788 : loss : 0.027781, loss_ce: 0.008313
 41%|███████████                | 164/400 [1:21:14<1:51:04, 28.24s/it]2022-01-11 23:33:07,668 iteration 2789 : loss : 0.019621, loss_ce: 0.009225
2022-01-11 23:33:09,231 iteration 2790 : loss : 0.030537, loss_ce: 0.007378
2022-01-11 23:33:10,865 iteration 2791 : loss : 0.025240, loss_ce: 0.007567
2022-01-11 23:33:12,534 iteration 2792 : loss : 0.044336, loss_ce: 0.010632
2022-01-11 23:33:14,141 iteration 2793 : loss : 0.039919, loss_ce: 0.018550
2022-01-11 23:33:15,754 iteration 2794 : loss : 0.028254, loss_ce: 0.012129
2022-01-11 23:33:17,434 iteration 2795 : loss : 0.026128, loss_ce: 0.009857
2022-01-11 23:33:19,148 iteration 2796 : loss : 0.042669, loss_ce: 0.018828
2022-01-11 23:33:20,803 iteration 2797 : loss : 0.026571, loss_ce: 0.010282
2022-01-11 23:33:22,500 iteration 2798 : loss : 0.033245, loss_ce: 0.012036
2022-01-11 23:33:24,165 iteration 2799 : loss : 0.029398, loss_ce: 0.009481
2022-01-11 23:33:25,817 iteration 2800 : loss : 0.026773, loss_ce: 0.014701
2022-01-11 23:33:27,405 iteration 2801 : loss : 0.022391, loss_ce: 0.009944
2022-01-11 23:33:28,940 iteration 2802 : loss : 0.020045, loss_ce: 0.008246
2022-01-11 23:33:30,647 iteration 2803 : loss : 0.051950, loss_ce: 0.018824
2022-01-11 23:33:32,251 iteration 2804 : loss : 0.049042, loss_ce: 0.023150
2022-01-11 23:33:32,252 Training Data Eval:
2022-01-11 23:33:40,192   Average segmentation loss on training set: 0.0279
2022-01-11 23:33:40,192 Validation Data Eval:
2022-01-11 23:33:42,936   Average segmentation loss on validation set: 0.0630
2022-01-11 23:33:48,831 Found new lowest validation loss at iteration 2804! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-11 23:33:50,293 iteration 2805 : loss : 0.039254, loss_ce: 0.013457
 41%|███████████▏               | 165/400 [1:21:58<2:09:26, 33.05s/it]2022-01-11 23:33:51,833 iteration 2806 : loss : 0.032331, loss_ce: 0.011545
2022-01-11 23:33:53,470 iteration 2807 : loss : 0.043192, loss_ce: 0.016216
2022-01-11 23:33:54,836 iteration 2808 : loss : 0.027003, loss_ce: 0.012631
2022-01-11 23:33:56,340 iteration 2809 : loss : 0.045195, loss_ce: 0.019206
2022-01-11 23:33:57,970 iteration 2810 : loss : 0.037633, loss_ce: 0.014953
2022-01-11 23:33:59,528 iteration 2811 : loss : 0.024693, loss_ce: 0.009394
2022-01-11 23:34:01,138 iteration 2812 : loss : 0.026697, loss_ce: 0.010461
2022-01-11 23:34:02,788 iteration 2813 : loss : 0.033926, loss_ce: 0.010174
2022-01-11 23:34:04,389 iteration 2814 : loss : 0.027217, loss_ce: 0.010750
2022-01-11 23:34:05,943 iteration 2815 : loss : 0.023649, loss_ce: 0.010354
2022-01-11 23:34:07,487 iteration 2816 : loss : 0.029399, loss_ce: 0.013335
2022-01-11 23:34:09,036 iteration 2817 : loss : 0.031530, loss_ce: 0.009866
2022-01-11 23:34:10,724 iteration 2818 : loss : 0.044543, loss_ce: 0.012110
2022-01-11 23:34:12,370 iteration 2819 : loss : 0.035135, loss_ce: 0.017161
2022-01-11 23:34:14,099 iteration 2820 : loss : 0.025722, loss_ce: 0.010668
2022-01-11 23:34:15,691 iteration 2821 : loss : 0.029507, loss_ce: 0.013145
2022-01-11 23:34:17,259 iteration 2822 : loss : 0.027708, loss_ce: 0.009184
 42%|███████████▏               | 166/400 [1:22:25<2:01:45, 31.22s/it]2022-01-11 23:34:18,996 iteration 2823 : loss : 0.037469, loss_ce: 0.012959
2022-01-11 23:34:20,537 iteration 2824 : loss : 0.019963, loss_ce: 0.007019
2022-01-11 23:34:22,173 iteration 2825 : loss : 0.044198, loss_ce: 0.022760
2022-01-11 23:34:23,720 iteration 2826 : loss : 0.022939, loss_ce: 0.009485
2022-01-11 23:34:25,356 iteration 2827 : loss : 0.033287, loss_ce: 0.016762
2022-01-11 23:34:26,984 iteration 2828 : loss : 0.034958, loss_ce: 0.016605
2022-01-11 23:34:28,715 iteration 2829 : loss : 0.038905, loss_ce: 0.016523
2022-01-11 23:34:30,251 iteration 2830 : loss : 0.022005, loss_ce: 0.010050
2022-01-11 23:34:31,846 iteration 2831 : loss : 0.030342, loss_ce: 0.012285
2022-01-11 23:34:33,483 iteration 2832 : loss : 0.041852, loss_ce: 0.014156
2022-01-11 23:34:35,148 iteration 2833 : loss : 0.036526, loss_ce: 0.014186
2022-01-11 23:34:36,737 iteration 2834 : loss : 0.054372, loss_ce: 0.020839
2022-01-11 23:34:38,375 iteration 2835 : loss : 0.053887, loss_ce: 0.017279
2022-01-11 23:34:40,047 iteration 2836 : loss : 0.029396, loss_ce: 0.013460
2022-01-11 23:34:41,648 iteration 2837 : loss : 0.035758, loss_ce: 0.009301
2022-01-11 23:34:43,173 iteration 2838 : loss : 0.025060, loss_ce: 0.008175
2022-01-11 23:34:44,803 iteration 2839 : loss : 0.033424, loss_ce: 0.013467
 42%|███████████▎               | 167/400 [1:22:53<1:56:57, 30.12s/it]2022-01-11 23:34:46,411 iteration 2840 : loss : 0.044433, loss_ce: 0.023229
2022-01-11 23:34:48,034 iteration 2841 : loss : 0.027275, loss_ce: 0.010484
2022-01-11 23:34:49,655 iteration 2842 : loss : 0.060361, loss_ce: 0.019539
2022-01-11 23:34:51,169 iteration 2843 : loss : 0.025984, loss_ce: 0.010094
2022-01-11 23:34:52,668 iteration 2844 : loss : 0.017538, loss_ce: 0.006505
2022-01-11 23:34:54,240 iteration 2845 : loss : 0.029443, loss_ce: 0.011549
2022-01-11 23:34:55,944 iteration 2846 : loss : 0.034973, loss_ce: 0.009435
2022-01-11 23:34:57,589 iteration 2847 : loss : 0.052362, loss_ce: 0.018355
2022-01-11 23:34:59,196 iteration 2848 : loss : 0.037173, loss_ce: 0.010681
2022-01-11 23:35:00,832 iteration 2849 : loss : 0.035395, loss_ce: 0.014121
2022-01-11 23:35:02,423 iteration 2850 : loss : 0.041388, loss_ce: 0.018791
2022-01-11 23:35:04,123 iteration 2851 : loss : 0.038586, loss_ce: 0.017388
2022-01-11 23:35:05,673 iteration 2852 : loss : 0.032314, loss_ce: 0.010454
2022-01-11 23:35:07,287 iteration 2853 : loss : 0.064572, loss_ce: 0.019499
2022-01-11 23:35:08,820 iteration 2854 : loss : 0.031974, loss_ce: 0.010366
2022-01-11 23:35:10,384 iteration 2855 : loss : 0.025268, loss_ce: 0.010626
2022-01-11 23:35:11,885 iteration 2856 : loss : 0.020992, loss_ce: 0.010600
 42%|███████████▎               | 168/400 [1:23:20<1:52:57, 29.21s/it]2022-01-11 23:35:13,520 iteration 2857 : loss : 0.025924, loss_ce: 0.011338
2022-01-11 23:35:15,264 iteration 2858 : loss : 0.038345, loss_ce: 0.010802
2022-01-11 23:35:16,907 iteration 2859 : loss : 0.052896, loss_ce: 0.022780
2022-01-11 23:35:18,557 iteration 2860 : loss : 0.023351, loss_ce: 0.008319
2022-01-11 23:35:20,182 iteration 2861 : loss : 0.028906, loss_ce: 0.007977
2022-01-11 23:35:21,835 iteration 2862 : loss : 0.042961, loss_ce: 0.011246
2022-01-11 23:35:23,381 iteration 2863 : loss : 0.019297, loss_ce: 0.007197
2022-01-11 23:35:24,964 iteration 2864 : loss : 0.023656, loss_ce: 0.008682
2022-01-11 23:35:26,565 iteration 2865 : loss : 0.026198, loss_ce: 0.010014
2022-01-11 23:35:28,137 iteration 2866 : loss : 0.020370, loss_ce: 0.007250
2022-01-11 23:35:29,848 iteration 2867 : loss : 0.042389, loss_ce: 0.012516
2022-01-11 23:35:31,494 iteration 2868 : loss : 0.043939, loss_ce: 0.014734
2022-01-11 23:35:33,146 iteration 2869 : loss : 0.074074, loss_ce: 0.027603
2022-01-11 23:35:34,844 iteration 2870 : loss : 0.031779, loss_ce: 0.010194
2022-01-11 23:35:36,382 iteration 2871 : loss : 0.041294, loss_ce: 0.022430
2022-01-11 23:35:37,949 iteration 2872 : loss : 0.029940, loss_ce: 0.014540
2022-01-11 23:35:39,537 iteration 2873 : loss : 0.025342, loss_ce: 0.011051
 42%|███████████▍               | 169/400 [1:23:48<1:50:39, 28.74s/it]2022-01-11 23:35:41,162 iteration 2874 : loss : 0.022443, loss_ce: 0.008339
2022-01-11 23:35:42,838 iteration 2875 : loss : 0.036299, loss_ce: 0.016584
2022-01-11 23:35:44,393 iteration 2876 : loss : 0.041024, loss_ce: 0.019403
2022-01-11 23:35:45,921 iteration 2877 : loss : 0.041771, loss_ce: 0.014242
2022-01-11 23:35:47,499 iteration 2878 : loss : 0.027525, loss_ce: 0.008111
2022-01-11 23:35:49,005 iteration 2879 : loss : 0.026992, loss_ce: 0.011500
2022-01-11 23:35:50,626 iteration 2880 : loss : 0.033592, loss_ce: 0.015898
2022-01-11 23:35:52,134 iteration 2881 : loss : 0.023215, loss_ce: 0.009656
2022-01-11 23:35:53,689 iteration 2882 : loss : 0.044474, loss_ce: 0.013891
2022-01-11 23:35:55,313 iteration 2883 : loss : 0.039225, loss_ce: 0.011429
2022-01-11 23:35:56,968 iteration 2884 : loss : 0.033523, loss_ce: 0.013070
2022-01-11 23:35:58,574 iteration 2885 : loss : 0.025368, loss_ce: 0.009799
2022-01-11 23:36:00,123 iteration 2886 : loss : 0.025037, loss_ce: 0.008320
2022-01-11 23:36:01,682 iteration 2887 : loss : 0.029787, loss_ce: 0.010069
2022-01-11 23:36:03,315 iteration 2888 : loss : 0.023056, loss_ce: 0.009959
2022-01-11 23:36:04,916 iteration 2889 : loss : 0.025999, loss_ce: 0.010281
2022-01-11 23:36:04,916 Training Data Eval:
2022-01-11 23:36:12,880   Average segmentation loss on training set: 0.0179
2022-01-11 23:36:12,881 Validation Data Eval:
2022-01-11 23:36:15,626   Average segmentation loss on validation set: 0.0747
2022-01-11 23:36:17,199 iteration 2890 : loss : 0.028797, loss_ce: 0.012369
 42%|███████████▍               | 170/400 [1:24:25<2:00:26, 31.42s/it]2022-01-11 23:36:18,810 iteration 2891 : loss : 0.028150, loss_ce: 0.011684
2022-01-11 23:36:20,433 iteration 2892 : loss : 0.027016, loss_ce: 0.010000
2022-01-11 23:36:21,947 iteration 2893 : loss : 0.022511, loss_ce: 0.008837
2022-01-11 23:36:23,501 iteration 2894 : loss : 0.020954, loss_ce: 0.008634
2022-01-11 23:36:25,048 iteration 2895 : loss : 0.025171, loss_ce: 0.008889
2022-01-11 23:36:26,630 iteration 2896 : loss : 0.034734, loss_ce: 0.014746
2022-01-11 23:36:28,357 iteration 2897 : loss : 0.057491, loss_ce: 0.013597
2022-01-11 23:36:29,990 iteration 2898 : loss : 0.028664, loss_ce: 0.009771
2022-01-11 23:36:31,624 iteration 2899 : loss : 0.028085, loss_ce: 0.012432
2022-01-11 23:36:33,366 iteration 2900 : loss : 0.054146, loss_ce: 0.027730
2022-01-11 23:36:35,004 iteration 2901 : loss : 0.025831, loss_ce: 0.010298
2022-01-11 23:36:36,570 iteration 2902 : loss : 0.041494, loss_ce: 0.023420
2022-01-11 23:36:38,177 iteration 2903 : loss : 0.038324, loss_ce: 0.012109
2022-01-11 23:36:39,755 iteration 2904 : loss : 0.031531, loss_ce: 0.014708
2022-01-11 23:36:41,390 iteration 2905 : loss : 0.025264, loss_ce: 0.011446
2022-01-11 23:36:42,936 iteration 2906 : loss : 0.030101, loss_ce: 0.012910
2022-01-11 23:36:44,534 iteration 2907 : loss : 0.026104, loss_ce: 0.010313
 43%|███████████▌               | 171/400 [1:24:53<1:55:13, 30.19s/it]2022-01-11 23:36:46,198 iteration 2908 : loss : 0.037923, loss_ce: 0.015048
2022-01-11 23:36:47,845 iteration 2909 : loss : 0.037698, loss_ce: 0.012511
2022-01-11 23:36:49,494 iteration 2910 : loss : 0.025988, loss_ce: 0.010866
2022-01-11 23:36:51,156 iteration 2911 : loss : 0.032132, loss_ce: 0.011364
2022-01-11 23:36:52,712 iteration 2912 : loss : 0.023302, loss_ce: 0.009341
2022-01-11 23:36:54,332 iteration 2913 : loss : 0.026400, loss_ce: 0.008688
2022-01-11 23:36:55,979 iteration 2914 : loss : 0.035624, loss_ce: 0.017758
2022-01-11 23:36:57,588 iteration 2915 : loss : 0.037888, loss_ce: 0.020550
2022-01-11 23:36:59,188 iteration 2916 : loss : 0.028007, loss_ce: 0.011830
2022-01-11 23:37:00,799 iteration 2917 : loss : 0.025233, loss_ce: 0.009941
2022-01-11 23:37:02,322 iteration 2918 : loss : 0.023756, loss_ce: 0.013145
2022-01-11 23:37:03,932 iteration 2919 : loss : 0.025437, loss_ce: 0.008617
2022-01-11 23:37:05,510 iteration 2920 : loss : 0.021831, loss_ce: 0.008986
2022-01-11 23:37:07,098 iteration 2921 : loss : 0.024566, loss_ce: 0.012153
2022-01-11 23:37:08,671 iteration 2922 : loss : 0.027725, loss_ce: 0.008585
2022-01-11 23:37:10,353 iteration 2923 : loss : 0.032793, loss_ce: 0.010051
2022-01-11 23:37:12,001 iteration 2924 : loss : 0.022041, loss_ce: 0.009792
 43%|███████████▌               | 172/400 [1:25:20<1:51:37, 29.38s/it]2022-01-11 23:37:13,660 iteration 2925 : loss : 0.028119, loss_ce: 0.013492
2022-01-11 23:37:15,192 iteration 2926 : loss : 0.019568, loss_ce: 0.006861
2022-01-11 23:37:16,799 iteration 2927 : loss : 0.018015, loss_ce: 0.006268
2022-01-11 23:37:18,431 iteration 2928 : loss : 0.028015, loss_ce: 0.011403
2022-01-11 23:37:19,982 iteration 2929 : loss : 0.027620, loss_ce: 0.007775
2022-01-11 23:37:21,527 iteration 2930 : loss : 0.020903, loss_ce: 0.009324
2022-01-11 23:37:23,113 iteration 2931 : loss : 0.073694, loss_ce: 0.008022
2022-01-11 23:37:24,784 iteration 2932 : loss : 0.035286, loss_ce: 0.014241
2022-01-11 23:37:26,432 iteration 2933 : loss : 0.046197, loss_ce: 0.019641
2022-01-11 23:37:28,110 iteration 2934 : loss : 0.024472, loss_ce: 0.011652
2022-01-11 23:37:29,675 iteration 2935 : loss : 0.033696, loss_ce: 0.009967
2022-01-11 23:37:31,270 iteration 2936 : loss : 0.027296, loss_ce: 0.012341
2022-01-11 23:37:32,891 iteration 2937 : loss : 0.027066, loss_ce: 0.012472
2022-01-11 23:37:34,533 iteration 2938 : loss : 0.027342, loss_ce: 0.009307
2022-01-11 23:37:36,030 iteration 2939 : loss : 0.033665, loss_ce: 0.008329
2022-01-11 23:37:37,605 iteration 2940 : loss : 0.036068, loss_ce: 0.015958
2022-01-11 23:37:39,187 iteration 2941 : loss : 0.040556, loss_ce: 0.016828
 43%|███████████▋               | 173/400 [1:25:47<1:48:38, 28.72s/it]2022-01-11 23:37:40,930 iteration 2942 : loss : 0.041527, loss_ce: 0.022936
2022-01-11 23:37:42,577 iteration 2943 : loss : 0.029969, loss_ce: 0.014189
2022-01-11 23:37:44,189 iteration 2944 : loss : 0.030732, loss_ce: 0.010820
2022-01-11 23:37:45,786 iteration 2945 : loss : 0.040187, loss_ce: 0.018180
2022-01-11 23:37:47,445 iteration 2946 : loss : 0.032747, loss_ce: 0.012449
2022-01-11 23:37:49,006 iteration 2947 : loss : 0.024196, loss_ce: 0.008819
2022-01-11 23:37:50,635 iteration 2948 : loss : 0.023790, loss_ce: 0.009238
2022-01-11 23:37:52,206 iteration 2949 : loss : 0.027955, loss_ce: 0.010029
2022-01-11 23:37:53,714 iteration 2950 : loss : 0.028028, loss_ce: 0.010383
2022-01-11 23:37:55,298 iteration 2951 : loss : 0.027503, loss_ce: 0.008329
2022-01-11 23:37:56,791 iteration 2952 : loss : 0.066434, loss_ce: 0.014434
2022-01-11 23:37:58,467 iteration 2953 : loss : 0.025214, loss_ce: 0.009020
2022-01-11 23:38:00,065 iteration 2954 : loss : 0.029669, loss_ce: 0.012112
2022-01-11 23:38:01,665 iteration 2955 : loss : 0.058125, loss_ce: 0.015251
2022-01-11 23:38:03,183 iteration 2956 : loss : 0.032389, loss_ce: 0.018237
2022-01-11 23:38:04,710 iteration 2957 : loss : 0.041710, loss_ce: 0.015741
2022-01-11 23:38:06,300 iteration 2958 : loss : 0.045463, loss_ce: 0.022107
 44%|███████████▋               | 174/400 [1:26:14<1:46:21, 28.24s/it]2022-01-11 23:38:07,962 iteration 2959 : loss : 0.035656, loss_ce: 0.018301
2022-01-11 23:38:09,557 iteration 2960 : loss : 0.040942, loss_ce: 0.024368
2022-01-11 23:38:11,208 iteration 2961 : loss : 0.034675, loss_ce: 0.014208
2022-01-11 23:38:12,841 iteration 2962 : loss : 0.040033, loss_ce: 0.017240
2022-01-11 23:38:14,399 iteration 2963 : loss : 0.025635, loss_ce: 0.011635
2022-01-11 23:38:15,953 iteration 2964 : loss : 0.035840, loss_ce: 0.014001
2022-01-11 23:38:17,459 iteration 2965 : loss : 0.044115, loss_ce: 0.016379
2022-01-11 23:38:18,951 iteration 2966 : loss : 0.030815, loss_ce: 0.011351
2022-01-11 23:38:20,566 iteration 2967 : loss : 0.028488, loss_ce: 0.012675
2022-01-11 23:38:22,275 iteration 2968 : loss : 0.065625, loss_ce: 0.017032
2022-01-11 23:38:23,928 iteration 2969 : loss : 0.022717, loss_ce: 0.009647
2022-01-11 23:38:25,494 iteration 2970 : loss : 0.025644, loss_ce: 0.009083
2022-01-11 23:38:27,019 iteration 2971 : loss : 0.026699, loss_ce: 0.011036
2022-01-11 23:38:28,661 iteration 2972 : loss : 0.042923, loss_ce: 0.015435
2022-01-11 23:38:30,212 iteration 2973 : loss : 0.028909, loss_ce: 0.011676
2022-01-11 23:38:31,884 iteration 2974 : loss : 0.022644, loss_ce: 0.007528
2022-01-11 23:38:31,884 Training Data Eval:
2022-01-11 23:38:39,843   Average segmentation loss on training set: 0.0169
2022-01-11 23:38:39,843 Validation Data Eval:
2022-01-11 23:38:42,589   Average segmentation loss on validation set: 0.0734
2022-01-11 23:38:44,210 iteration 2975 : loss : 0.028109, loss_ce: 0.007602
 44%|███████████▊               | 175/400 [1:26:52<1:56:46, 31.14s/it]2022-01-11 23:38:45,942 iteration 2976 : loss : 0.042290, loss_ce: 0.021700
2022-01-11 23:38:47,458 iteration 2977 : loss : 0.025717, loss_ce: 0.010123
2022-01-11 23:38:48,964 iteration 2978 : loss : 0.019187, loss_ce: 0.006713
2022-01-11 23:38:50,586 iteration 2979 : loss : 0.025637, loss_ce: 0.008764
2022-01-11 23:38:52,319 iteration 2980 : loss : 0.032187, loss_ce: 0.015480
2022-01-11 23:38:53,943 iteration 2981 : loss : 0.028517, loss_ce: 0.009004
2022-01-11 23:38:55,629 iteration 2982 : loss : 0.027602, loss_ce: 0.012236
2022-01-11 23:38:57,222 iteration 2983 : loss : 0.033471, loss_ce: 0.014881
2022-01-11 23:38:58,882 iteration 2984 : loss : 0.020728, loss_ce: 0.008141
2022-01-11 23:39:00,512 iteration 2985 : loss : 0.034445, loss_ce: 0.010086
2022-01-11 23:39:02,175 iteration 2986 : loss : 0.029726, loss_ce: 0.011766
2022-01-11 23:39:03,729 iteration 2987 : loss : 0.027574, loss_ce: 0.013398
2022-01-11 23:39:05,292 iteration 2988 : loss : 0.023569, loss_ce: 0.007702
2022-01-11 23:39:06,913 iteration 2989 : loss : 0.024674, loss_ce: 0.007608
2022-01-11 23:39:08,541 iteration 2990 : loss : 0.034774, loss_ce: 0.012738
2022-01-11 23:39:10,228 iteration 2991 : loss : 0.027876, loss_ce: 0.009703
2022-01-11 23:39:11,978 iteration 2992 : loss : 0.027136, loss_ce: 0.010431
 44%|███████████▉               | 176/400 [1:27:20<1:52:28, 30.13s/it]2022-01-11 23:39:13,555 iteration 2993 : loss : 0.030290, loss_ce: 0.007257
2022-01-11 23:39:15,141 iteration 2994 : loss : 0.025586, loss_ce: 0.010181
2022-01-11 23:39:16,835 iteration 2995 : loss : 0.031348, loss_ce: 0.012772
2022-01-11 23:39:18,439 iteration 2996 : loss : 0.027223, loss_ce: 0.009133
2022-01-11 23:39:20,031 iteration 2997 : loss : 0.028123, loss_ce: 0.011598
2022-01-11 23:39:21,611 iteration 2998 : loss : 0.022481, loss_ce: 0.006296
2022-01-11 23:39:23,255 iteration 2999 : loss : 0.030130, loss_ce: 0.010257
2022-01-11 23:39:24,846 iteration 3000 : loss : 0.030359, loss_ce: 0.009516
2022-01-11 23:39:26,483 iteration 3001 : loss : 0.027616, loss_ce: 0.010930
2022-01-11 23:39:27,995 iteration 3002 : loss : 0.036256, loss_ce: 0.016063
2022-01-11 23:39:29,652 iteration 3003 : loss : 0.027460, loss_ce: 0.010130
2022-01-11 23:39:31,162 iteration 3004 : loss : 0.022213, loss_ce: 0.009216
2022-01-11 23:39:32,789 iteration 3005 : loss : 0.042752, loss_ce: 0.015270
2022-01-11 23:39:34,417 iteration 3006 : loss : 0.032494, loss_ce: 0.012212
2022-01-11 23:39:35,979 iteration 3007 : loss : 0.022756, loss_ce: 0.008185
2022-01-11 23:39:37,702 iteration 3008 : loss : 0.030067, loss_ce: 0.012120
2022-01-11 23:39:39,280 iteration 3009 : loss : 0.024564, loss_ce: 0.013285
 44%|███████████▉               | 177/400 [1:27:47<1:48:49, 29.28s/it]2022-01-11 23:39:40,910 iteration 3010 : loss : 0.026419, loss_ce: 0.009785
2022-01-11 23:39:42,513 iteration 3011 : loss : 0.050642, loss_ce: 0.021157
2022-01-11 23:39:44,077 iteration 3012 : loss : 0.027754, loss_ce: 0.009668
2022-01-11 23:39:45,635 iteration 3013 : loss : 0.019777, loss_ce: 0.008334
2022-01-11 23:39:47,307 iteration 3014 : loss : 0.029275, loss_ce: 0.009886
2022-01-11 23:39:48,914 iteration 3015 : loss : 0.026799, loss_ce: 0.011206
2022-01-11 23:39:50,492 iteration 3016 : loss : 0.024404, loss_ce: 0.008491
2022-01-11 23:39:52,068 iteration 3017 : loss : 0.034636, loss_ce: 0.009244
2022-01-11 23:39:53,606 iteration 3018 : loss : 0.019536, loss_ce: 0.005515
2022-01-11 23:39:55,185 iteration 3019 : loss : 0.031942, loss_ce: 0.011354
2022-01-11 23:39:56,641 iteration 3020 : loss : 0.023108, loss_ce: 0.011327
2022-01-11 23:39:58,126 iteration 3021 : loss : 0.024057, loss_ce: 0.008070
2022-01-11 23:39:59,681 iteration 3022 : loss : 0.028884, loss_ce: 0.008749
2022-01-11 23:40:01,252 iteration 3023 : loss : 0.024746, loss_ce: 0.012060
2022-01-11 23:40:02,843 iteration 3024 : loss : 0.041865, loss_ce: 0.017226
2022-01-11 23:40:04,327 iteration 3025 : loss : 0.024003, loss_ce: 0.009436
2022-01-11 23:40:05,945 iteration 3026 : loss : 0.026439, loss_ce: 0.011760
 44%|████████████               | 178/400 [1:28:14<1:45:26, 28.50s/it]2022-01-11 23:40:07,594 iteration 3027 : loss : 0.035472, loss_ce: 0.016520
2022-01-11 23:40:09,173 iteration 3028 : loss : 0.032855, loss_ce: 0.013951
2022-01-11 23:40:10,786 iteration 3029 : loss : 0.022852, loss_ce: 0.010516
2022-01-11 23:40:12,490 iteration 3030 : loss : 0.030754, loss_ce: 0.010966
2022-01-11 23:40:13,977 iteration 3031 : loss : 0.021386, loss_ce: 0.008489
2022-01-11 23:40:15,559 iteration 3032 : loss : 0.024050, loss_ce: 0.007856
2022-01-11 23:40:17,177 iteration 3033 : loss : 0.025565, loss_ce: 0.011936
2022-01-11 23:40:18,746 iteration 3034 : loss : 0.023034, loss_ce: 0.008356
2022-01-11 23:40:20,419 iteration 3035 : loss : 0.048322, loss_ce: 0.015964
2022-01-11 23:40:22,048 iteration 3036 : loss : 0.023721, loss_ce: 0.009725
2022-01-11 23:40:23,690 iteration 3037 : loss : 0.016421, loss_ce: 0.005684
2022-01-11 23:40:25,204 iteration 3038 : loss : 0.018073, loss_ce: 0.004493
2022-01-11 23:40:26,978 iteration 3039 : loss : 0.042722, loss_ce: 0.017390
2022-01-11 23:40:28,521 iteration 3040 : loss : 0.021754, loss_ce: 0.008942
2022-01-11 23:40:30,076 iteration 3041 : loss : 0.028228, loss_ce: 0.013005
2022-01-11 23:40:31,614 iteration 3042 : loss : 0.022730, loss_ce: 0.009558
2022-01-11 23:40:33,330 iteration 3043 : loss : 0.021669, loss_ce: 0.008738
 45%|████████████               | 179/400 [1:28:42<1:43:44, 28.16s/it]2022-01-11 23:40:35,010 iteration 3044 : loss : 0.042807, loss_ce: 0.015682
2022-01-11 23:40:36,592 iteration 3045 : loss : 0.019782, loss_ce: 0.007991
2022-01-11 23:40:38,166 iteration 3046 : loss : 0.029837, loss_ce: 0.010847
2022-01-11 23:40:39,882 iteration 3047 : loss : 0.031384, loss_ce: 0.013308
2022-01-11 23:40:41,548 iteration 3048 : loss : 0.026604, loss_ce: 0.013703
2022-01-11 23:40:43,104 iteration 3049 : loss : 0.027654, loss_ce: 0.011979
2022-01-11 23:40:44,806 iteration 3050 : loss : 0.026840, loss_ce: 0.012138
2022-01-11 23:40:46,443 iteration 3051 : loss : 0.037848, loss_ce: 0.009768
2022-01-11 23:40:48,131 iteration 3052 : loss : 0.035900, loss_ce: 0.014601
2022-01-11 23:40:49,754 iteration 3053 : loss : 0.020937, loss_ce: 0.008066
2022-01-11 23:40:51,372 iteration 3054 : loss : 0.036690, loss_ce: 0.013147
2022-01-11 23:40:53,007 iteration 3055 : loss : 0.026778, loss_ce: 0.010736
2022-01-11 23:40:54,604 iteration 3056 : loss : 0.024261, loss_ce: 0.008316
2022-01-11 23:40:56,152 iteration 3057 : loss : 0.018435, loss_ce: 0.006609
2022-01-11 23:40:57,828 iteration 3058 : loss : 0.040385, loss_ce: 0.012362
2022-01-11 23:40:59,521 iteration 3059 : loss : 0.040289, loss_ce: 0.015557
2022-01-11 23:40:59,521 Training Data Eval:
2022-01-11 23:41:07,490   Average segmentation loss on training set: 0.0166
2022-01-11 23:41:07,491 Validation Data Eval:
2022-01-11 23:41:10,245   Average segmentation loss on validation set: 0.0666
2022-01-11 23:41:11,825 iteration 3060 : loss : 0.057190, loss_ce: 0.035186
 45%|████████████▏              | 180/400 [1:29:20<1:54:38, 31.26s/it]2022-01-11 23:41:13,429 iteration 3061 : loss : 0.024316, loss_ce: 0.008936
2022-01-11 23:41:15,066 iteration 3062 : loss : 0.035009, loss_ce: 0.010488
2022-01-11 23:41:16,639 iteration 3063 : loss : 0.026591, loss_ce: 0.009394
2022-01-11 23:41:18,296 iteration 3064 : loss : 0.034892, loss_ce: 0.020081
2022-01-11 23:41:19,892 iteration 3065 : loss : 0.029263, loss_ce: 0.007763
2022-01-11 23:41:21,470 iteration 3066 : loss : 0.022598, loss_ce: 0.006014
2022-01-11 23:41:23,069 iteration 3067 : loss : 0.025616, loss_ce: 0.011542
2022-01-11 23:41:24,735 iteration 3068 : loss : 0.017613, loss_ce: 0.005996
2022-01-11 23:41:26,398 iteration 3069 : loss : 0.029189, loss_ce: 0.009151
2022-01-11 23:41:27,929 iteration 3070 : loss : 0.020208, loss_ce: 0.007581
2022-01-11 23:41:29,623 iteration 3071 : loss : 0.025783, loss_ce: 0.009612
2022-01-11 23:41:31,252 iteration 3072 : loss : 0.020279, loss_ce: 0.007249
2022-01-11 23:41:32,814 iteration 3073 : loss : 0.026777, loss_ce: 0.009952
2022-01-11 23:41:34,526 iteration 3074 : loss : 0.037663, loss_ce: 0.017977
2022-01-11 23:41:36,185 iteration 3075 : loss : 0.033906, loss_ce: 0.012579
2022-01-11 23:41:37,768 iteration 3076 : loss : 0.019184, loss_ce: 0.008998
2022-01-11 23:41:39,459 iteration 3077 : loss : 0.076171, loss_ce: 0.035013
 45%|████████████▏              | 181/400 [1:29:48<1:50:08, 30.17s/it]2022-01-11 23:41:40,975 iteration 3078 : loss : 0.017461, loss_ce: 0.007054
2022-01-11 23:41:42,636 iteration 3079 : loss : 0.047253, loss_ce: 0.016173
2022-01-11 23:41:44,240 iteration 3080 : loss : 0.028325, loss_ce: 0.010869
2022-01-11 23:41:45,814 iteration 3081 : loss : 0.025694, loss_ce: 0.008249
2022-01-11 23:41:47,430 iteration 3082 : loss : 0.022796, loss_ce: 0.008760
2022-01-11 23:41:49,039 iteration 3083 : loss : 0.029265, loss_ce: 0.013831
2022-01-11 23:41:50,619 iteration 3084 : loss : 0.024516, loss_ce: 0.009836
2022-01-11 23:41:52,231 iteration 3085 : loss : 0.055569, loss_ce: 0.009008
2022-01-11 23:41:53,781 iteration 3086 : loss : 0.024635, loss_ce: 0.009835
2022-01-11 23:41:55,330 iteration 3087 : loss : 0.028100, loss_ce: 0.008028
2022-01-11 23:41:56,854 iteration 3088 : loss : 0.024119, loss_ce: 0.008538
2022-01-11 23:41:58,383 iteration 3089 : loss : 0.025057, loss_ce: 0.009972
2022-01-11 23:41:59,986 iteration 3090 : loss : 0.031350, loss_ce: 0.010412
2022-01-11 23:42:01,538 iteration 3091 : loss : 0.037742, loss_ce: 0.013969
2022-01-11 23:42:03,040 iteration 3092 : loss : 0.032786, loss_ce: 0.014615
2022-01-11 23:42:04,618 iteration 3093 : loss : 0.023976, loss_ce: 0.007778
2022-01-11 23:42:06,164 iteration 3094 : loss : 0.035901, loss_ce: 0.017963
 46%|████████████▎              | 182/400 [1:30:14<1:45:50, 29.13s/it]2022-01-11 23:42:07,858 iteration 3095 : loss : 0.021608, loss_ce: 0.006652
2022-01-11 23:42:09,424 iteration 3096 : loss : 0.033647, loss_ce: 0.013684
2022-01-11 23:42:10,998 iteration 3097 : loss : 0.027116, loss_ce: 0.010956
2022-01-11 23:42:12,537 iteration 3098 : loss : 0.023390, loss_ce: 0.007764
2022-01-11 23:42:14,195 iteration 3099 : loss : 0.036813, loss_ce: 0.010517
2022-01-11 23:42:15,734 iteration 3100 : loss : 0.024546, loss_ce: 0.009021
2022-01-11 23:42:17,332 iteration 3101 : loss : 0.028357, loss_ce: 0.012450
2022-01-11 23:42:18,986 iteration 3102 : loss : 0.027485, loss_ce: 0.010408
2022-01-11 23:42:20,552 iteration 3103 : loss : 0.021946, loss_ce: 0.006754
2022-01-11 23:42:22,154 iteration 3104 : loss : 0.024798, loss_ce: 0.008588
2022-01-11 23:42:23,717 iteration 3105 : loss : 0.020136, loss_ce: 0.006833
2022-01-11 23:42:25,474 iteration 3106 : loss : 0.025905, loss_ce: 0.010942
2022-01-11 23:42:27,073 iteration 3107 : loss : 0.024385, loss_ce: 0.009418
2022-01-11 23:42:28,585 iteration 3108 : loss : 0.023830, loss_ce: 0.008742
2022-01-11 23:42:30,230 iteration 3109 : loss : 0.036072, loss_ce: 0.018786
2022-01-11 23:42:31,884 iteration 3110 : loss : 0.042545, loss_ce: 0.020829
2022-01-11 23:42:33,443 iteration 3111 : loss : 0.023925, loss_ce: 0.007584
 46%|████████████▎              | 183/400 [1:30:42<1:43:20, 28.58s/it]2022-01-11 23:42:35,026 iteration 3112 : loss : 0.020821, loss_ce: 0.008384
2022-01-11 23:42:36,612 iteration 3113 : loss : 0.025094, loss_ce: 0.010495
2022-01-11 23:42:38,327 iteration 3114 : loss : 0.065743, loss_ce: 0.017210
2022-01-11 23:42:39,987 iteration 3115 : loss : 0.032942, loss_ce: 0.014103
2022-01-11 23:42:41,603 iteration 3116 : loss : 0.025166, loss_ce: 0.010235
2022-01-11 23:42:43,187 iteration 3117 : loss : 0.030492, loss_ce: 0.012322
2022-01-11 23:42:44,863 iteration 3118 : loss : 0.039228, loss_ce: 0.015046
2022-01-11 23:42:46,505 iteration 3119 : loss : 0.029197, loss_ce: 0.010636
2022-01-11 23:42:48,110 iteration 3120 : loss : 0.020243, loss_ce: 0.006561
2022-01-11 23:42:49,731 iteration 3121 : loss : 0.029587, loss_ce: 0.008804
2022-01-11 23:42:51,405 iteration 3122 : loss : 0.027182, loss_ce: 0.007540
2022-01-11 23:42:53,030 iteration 3123 : loss : 0.021338, loss_ce: 0.008758
2022-01-11 23:42:54,666 iteration 3124 : loss : 0.030643, loss_ce: 0.012055
2022-01-11 23:42:56,292 iteration 3125 : loss : 0.027328, loss_ce: 0.012605
2022-01-11 23:42:57,819 iteration 3126 : loss : 0.023510, loss_ce: 0.009260
2022-01-11 23:42:59,438 iteration 3127 : loss : 0.023634, loss_ce: 0.009943
2022-01-11 23:43:00,962 iteration 3128 : loss : 0.019528, loss_ce: 0.006311
 46%|████████████▍              | 184/400 [1:31:09<1:41:43, 28.26s/it]2022-01-11 23:43:02,655 iteration 3129 : loss : 0.028612, loss_ce: 0.009044
2022-01-11 23:43:04,280 iteration 3130 : loss : 0.034641, loss_ce: 0.011727
2022-01-11 23:43:05,950 iteration 3131 : loss : 0.038425, loss_ce: 0.015239
2022-01-11 23:43:07,527 iteration 3132 : loss : 0.021803, loss_ce: 0.008982
2022-01-11 23:43:09,055 iteration 3133 : loss : 0.025050, loss_ce: 0.009777
2022-01-11 23:43:10,654 iteration 3134 : loss : 0.022910, loss_ce: 0.008180
2022-01-11 23:43:12,252 iteration 3135 : loss : 0.029925, loss_ce: 0.010122
2022-01-11 23:43:13,914 iteration 3136 : loss : 0.038039, loss_ce: 0.015006
2022-01-11 23:43:15,532 iteration 3137 : loss : 0.027710, loss_ce: 0.012393
2022-01-11 23:43:17,168 iteration 3138 : loss : 0.031950, loss_ce: 0.015838
2022-01-11 23:43:18,846 iteration 3139 : loss : 0.038275, loss_ce: 0.012683
2022-01-11 23:43:20,505 iteration 3140 : loss : 0.027580, loss_ce: 0.010765
2022-01-11 23:43:22,158 iteration 3141 : loss : 0.025700, loss_ce: 0.010488
2022-01-11 23:43:23,769 iteration 3142 : loss : 0.031912, loss_ce: 0.009960
2022-01-11 23:43:25,454 iteration 3143 : loss : 0.030509, loss_ce: 0.013541
2022-01-11 23:43:27,143 iteration 3144 : loss : 0.022090, loss_ce: 0.009849
2022-01-11 23:43:27,143 Training Data Eval:
2022-01-11 23:43:35,102   Average segmentation loss on training set: 0.0160
2022-01-11 23:43:35,102 Validation Data Eval:
2022-01-11 23:43:37,841   Average segmentation loss on validation set: 0.0686
2022-01-11 23:43:39,323 iteration 3145 : loss : 0.019856, loss_ce: 0.007139
 46%|████████████▍              | 185/400 [1:31:48<1:52:07, 31.29s/it]2022-01-11 23:43:40,862 iteration 3146 : loss : 0.019093, loss_ce: 0.006633
2022-01-11 23:43:42,428 iteration 3147 : loss : 0.024449, loss_ce: 0.010415
2022-01-11 23:43:44,006 iteration 3148 : loss : 0.034939, loss_ce: 0.009839
2022-01-11 23:43:45,570 iteration 3149 : loss : 0.016324, loss_ce: 0.005442
2022-01-11 23:43:47,151 iteration 3150 : loss : 0.036368, loss_ce: 0.022169
2022-01-11 23:43:48,692 iteration 3151 : loss : 0.031626, loss_ce: 0.007463
2022-01-11 23:43:50,264 iteration 3152 : loss : 0.021244, loss_ce: 0.008904
2022-01-11 23:43:51,867 iteration 3153 : loss : 0.029758, loss_ce: 0.011759
2022-01-11 23:43:53,417 iteration 3154 : loss : 0.029343, loss_ce: 0.013920
2022-01-11 23:43:54,971 iteration 3155 : loss : 0.020124, loss_ce: 0.006940
2022-01-11 23:43:56,638 iteration 3156 : loss : 0.031295, loss_ce: 0.012378
2022-01-11 23:43:58,275 iteration 3157 : loss : 0.025933, loss_ce: 0.010353
2022-01-11 23:43:59,841 iteration 3158 : loss : 0.028062, loss_ce: 0.011729
2022-01-11 23:44:01,436 iteration 3159 : loss : 0.019439, loss_ce: 0.007931
2022-01-11 23:44:03,027 iteration 3160 : loss : 0.018076, loss_ce: 0.006991
2022-01-11 23:44:04,563 iteration 3161 : loss : 0.023769, loss_ce: 0.008224
2022-01-11 23:44:06,230 iteration 3162 : loss : 0.032512, loss_ce: 0.013683
 46%|████████████▌              | 186/400 [1:32:14<1:46:55, 29.98s/it]2022-01-11 23:44:07,949 iteration 3163 : loss : 0.028393, loss_ce: 0.013331
2022-01-11 23:44:09,584 iteration 3164 : loss : 0.023520, loss_ce: 0.009405
2022-01-11 23:44:11,178 iteration 3165 : loss : 0.018313, loss_ce: 0.006171
2022-01-11 23:44:12,836 iteration 3166 : loss : 0.020852, loss_ce: 0.007382
2022-01-11 23:44:14,432 iteration 3167 : loss : 0.019482, loss_ce: 0.007223
2022-01-11 23:44:16,163 iteration 3168 : loss : 0.041270, loss_ce: 0.018447
2022-01-11 23:44:17,877 iteration 3169 : loss : 0.027638, loss_ce: 0.013063
2022-01-11 23:44:19,452 iteration 3170 : loss : 0.021016, loss_ce: 0.009054
2022-01-11 23:44:20,966 iteration 3171 : loss : 0.020686, loss_ce: 0.009458
2022-01-11 23:44:22,583 iteration 3172 : loss : 0.023007, loss_ce: 0.008466
2022-01-11 23:44:24,241 iteration 3173 : loss : 0.021625, loss_ce: 0.008510
2022-01-11 23:44:25,847 iteration 3174 : loss : 0.035157, loss_ce: 0.014372
2022-01-11 23:44:27,552 iteration 3175 : loss : 0.048071, loss_ce: 0.013639
2022-01-11 23:44:29,146 iteration 3176 : loss : 0.026510, loss_ce: 0.009215
2022-01-11 23:44:30,798 iteration 3177 : loss : 0.037855, loss_ce: 0.013021
2022-01-11 23:44:32,329 iteration 3178 : loss : 0.020416, loss_ce: 0.007079
2022-01-11 23:44:33,895 iteration 3179 : loss : 0.028045, loss_ce: 0.007804
 47%|████████████▌              | 187/400 [1:32:42<1:43:57, 29.28s/it]2022-01-11 23:44:35,589 iteration 3180 : loss : 0.035223, loss_ce: 0.014944
2022-01-11 23:44:37,141 iteration 3181 : loss : 0.020449, loss_ce: 0.007225
2022-01-11 23:44:38,756 iteration 3182 : loss : 0.022949, loss_ce: 0.008896
2022-01-11 23:44:40,319 iteration 3183 : loss : 0.016980, loss_ce: 0.004445
2022-01-11 23:44:41,935 iteration 3184 : loss : 0.031026, loss_ce: 0.012742
2022-01-11 23:44:43,572 iteration 3185 : loss : 0.027596, loss_ce: 0.010485
2022-01-11 23:44:45,088 iteration 3186 : loss : 0.018592, loss_ce: 0.008596
2022-01-11 23:44:46,699 iteration 3187 : loss : 0.016400, loss_ce: 0.006189
2022-01-11 23:44:48,276 iteration 3188 : loss : 0.019577, loss_ce: 0.008036
2022-01-11 23:44:49,895 iteration 3189 : loss : 0.025322, loss_ce: 0.011415
2022-01-11 23:44:51,494 iteration 3190 : loss : 0.026302, loss_ce: 0.010687
2022-01-11 23:44:53,221 iteration 3191 : loss : 0.047265, loss_ce: 0.012108
2022-01-11 23:44:54,769 iteration 3192 : loss : 0.035535, loss_ce: 0.011733
2022-01-11 23:44:56,424 iteration 3193 : loss : 0.027178, loss_ce: 0.011883
2022-01-11 23:44:58,082 iteration 3194 : loss : 0.045617, loss_ce: 0.012279
2022-01-11 23:44:59,686 iteration 3195 : loss : 0.025974, loss_ce: 0.012018
2022-01-11 23:45:01,280 iteration 3196 : loss : 0.030770, loss_ce: 0.012074
 47%|████████████▋              | 188/400 [1:33:09<1:41:27, 28.71s/it]2022-01-11 23:45:02,845 iteration 3197 : loss : 0.021157, loss_ce: 0.008558
2022-01-11 23:45:04,436 iteration 3198 : loss : 0.021870, loss_ce: 0.009467
2022-01-11 23:45:06,036 iteration 3199 : loss : 0.026156, loss_ce: 0.011573
2022-01-11 23:45:07,751 iteration 3200 : loss : 0.026283, loss_ce: 0.011336
2022-01-11 23:45:09,383 iteration 3201 : loss : 0.042132, loss_ce: 0.013546
2022-01-11 23:45:10,980 iteration 3202 : loss : 0.026891, loss_ce: 0.009089
2022-01-11 23:45:12,690 iteration 3203 : loss : 0.028284, loss_ce: 0.013206
2022-01-11 23:45:14,282 iteration 3204 : loss : 0.023233, loss_ce: 0.007472
2022-01-11 23:45:15,943 iteration 3205 : loss : 0.035154, loss_ce: 0.010524
2022-01-11 23:45:17,553 iteration 3206 : loss : 0.021603, loss_ce: 0.009198
2022-01-11 23:45:19,257 iteration 3207 : loss : 0.036345, loss_ce: 0.016113
2022-01-11 23:45:20,794 iteration 3208 : loss : 0.025711, loss_ce: 0.007939
2022-01-11 23:45:22,342 iteration 3209 : loss : 0.025660, loss_ce: 0.012942
2022-01-11 23:45:23,838 iteration 3210 : loss : 0.018014, loss_ce: 0.008430
2022-01-11 23:45:25,421 iteration 3211 : loss : 0.023953, loss_ce: 0.007250
2022-01-11 23:45:26,990 iteration 3212 : loss : 0.043337, loss_ce: 0.020900
2022-01-11 23:45:28,592 iteration 3213 : loss : 0.027914, loss_ce: 0.007241
 47%|████████████▊              | 189/400 [1:33:37<1:39:29, 28.29s/it]2022-01-11 23:45:30,260 iteration 3214 : loss : 0.025181, loss_ce: 0.010991
2022-01-11 23:45:31,952 iteration 3215 : loss : 0.053880, loss_ce: 0.019348
2022-01-11 23:45:33,610 iteration 3216 : loss : 0.024296, loss_ce: 0.009190
2022-01-11 23:45:35,277 iteration 3217 : loss : 0.037094, loss_ce: 0.008254
2022-01-11 23:45:36,899 iteration 3218 : loss : 0.041910, loss_ce: 0.017096
2022-01-11 23:45:38,466 iteration 3219 : loss : 0.022610, loss_ce: 0.008600
2022-01-11 23:45:40,107 iteration 3220 : loss : 0.030344, loss_ce: 0.012453
2022-01-11 23:45:41,674 iteration 3221 : loss : 0.025798, loss_ce: 0.010010
2022-01-11 23:45:43,276 iteration 3222 : loss : 0.023825, loss_ce: 0.013445
2022-01-11 23:45:44,901 iteration 3223 : loss : 0.033815, loss_ce: 0.009935
2022-01-11 23:45:46,562 iteration 3224 : loss : 0.023832, loss_ce: 0.007670
2022-01-11 23:45:48,135 iteration 3225 : loss : 0.045353, loss_ce: 0.014235
2022-01-11 23:45:49,835 iteration 3226 : loss : 0.040729, loss_ce: 0.013550
2022-01-11 23:45:51,456 iteration 3227 : loss : 0.028380, loss_ce: 0.010575
2022-01-11 23:45:52,990 iteration 3228 : loss : 0.021120, loss_ce: 0.008966
2022-01-11 23:45:54,681 iteration 3229 : loss : 0.033487, loss_ce: 0.013468
2022-01-11 23:45:54,682 Training Data Eval:
2022-01-11 23:46:02,649   Average segmentation loss on training set: 0.0204
2022-01-11 23:46:02,649 Validation Data Eval:
2022-01-11 23:46:05,393   Average segmentation loss on validation set: 0.0979
2022-01-11 23:46:07,089 iteration 3230 : loss : 0.034134, loss_ce: 0.011043
 48%|████████████▊              | 190/400 [1:34:15<1:49:43, 31.35s/it]2022-01-11 23:46:08,705 iteration 3231 : loss : 0.040940, loss_ce: 0.014971
2022-01-11 23:46:10,304 iteration 3232 : loss : 0.022613, loss_ce: 0.010091
2022-01-11 23:46:11,890 iteration 3233 : loss : 0.022273, loss_ce: 0.007551
2022-01-11 23:46:13,435 iteration 3234 : loss : 0.041989, loss_ce: 0.015675
2022-01-11 23:46:14,991 iteration 3235 : loss : 0.021692, loss_ce: 0.006035
2022-01-11 23:46:16,610 iteration 3236 : loss : 0.023159, loss_ce: 0.009630
2022-01-11 23:46:18,252 iteration 3237 : loss : 0.020397, loss_ce: 0.007749
2022-01-11 23:46:19,860 iteration 3238 : loss : 0.033206, loss_ce: 0.008383
2022-01-11 23:46:21,442 iteration 3239 : loss : 0.021676, loss_ce: 0.008425
2022-01-11 23:46:23,054 iteration 3240 : loss : 0.024209, loss_ce: 0.007808
2022-01-11 23:46:24,717 iteration 3241 : loss : 0.038053, loss_ce: 0.014944
2022-01-11 23:46:26,422 iteration 3242 : loss : 0.032917, loss_ce: 0.017005
2022-01-11 23:46:27,991 iteration 3243 : loss : 0.028302, loss_ce: 0.015146
2022-01-11 23:46:29,582 iteration 3244 : loss : 0.056094, loss_ce: 0.011803
2022-01-11 23:46:31,162 iteration 3245 : loss : 0.023648, loss_ce: 0.009924
2022-01-11 23:46:32,822 iteration 3246 : loss : 0.032154, loss_ce: 0.014104
2022-01-11 23:46:34,460 iteration 3247 : loss : 0.024677, loss_ce: 0.009590
 48%|████████████▉              | 191/400 [1:34:43<1:45:03, 30.16s/it]2022-01-11 23:46:36,282 iteration 3248 : loss : 0.036630, loss_ce: 0.016847
2022-01-11 23:46:37,807 iteration 3249 : loss : 0.026885, loss_ce: 0.009063
2022-01-11 23:46:39,435 iteration 3250 : loss : 0.029720, loss_ce: 0.011091
2022-01-11 23:46:41,055 iteration 3251 : loss : 0.039337, loss_ce: 0.008930
2022-01-11 23:46:42,594 iteration 3252 : loss : 0.031008, loss_ce: 0.013636
2022-01-11 23:46:44,221 iteration 3253 : loss : 0.031219, loss_ce: 0.013146
2022-01-11 23:46:45,828 iteration 3254 : loss : 0.031699, loss_ce: 0.010206
2022-01-11 23:46:47,486 iteration 3255 : loss : 0.026800, loss_ce: 0.010238
2022-01-11 23:46:49,018 iteration 3256 : loss : 0.025962, loss_ce: 0.015575
2022-01-11 23:46:50,606 iteration 3257 : loss : 0.030737, loss_ce: 0.010093
2022-01-11 23:46:52,290 iteration 3258 : loss : 0.036679, loss_ce: 0.014726
2022-01-11 23:46:53,883 iteration 3259 : loss : 0.030307, loss_ce: 0.010843
2022-01-11 23:46:55,441 iteration 3260 : loss : 0.023008, loss_ce: 0.009801
2022-01-11 23:46:57,105 iteration 3261 : loss : 0.035127, loss_ce: 0.013485
2022-01-11 23:46:58,709 iteration 3262 : loss : 0.040471, loss_ce: 0.021340
2022-01-11 23:47:00,297 iteration 3263 : loss : 0.045583, loss_ce: 0.015697
2022-01-11 23:47:01,866 iteration 3264 : loss : 0.026040, loss_ce: 0.008951
 48%|████████████▉              | 192/400 [1:35:10<1:41:41, 29.33s/it]2022-01-11 23:47:03,396 iteration 3265 : loss : 0.018478, loss_ce: 0.006588
2022-01-11 23:47:05,026 iteration 3266 : loss : 0.021496, loss_ce: 0.008826
2022-01-11 23:47:06,578 iteration 3267 : loss : 0.022713, loss_ce: 0.009667
2022-01-11 23:47:08,293 iteration 3268 : loss : 0.035669, loss_ce: 0.016450
2022-01-11 23:47:09,839 iteration 3269 : loss : 0.027547, loss_ce: 0.010750
2022-01-11 23:47:11,377 iteration 3270 : loss : 0.023250, loss_ce: 0.008768
2022-01-11 23:47:13,035 iteration 3271 : loss : 0.024975, loss_ce: 0.009196
2022-01-11 23:47:14,601 iteration 3272 : loss : 0.024104, loss_ce: 0.009008
2022-01-11 23:47:16,162 iteration 3273 : loss : 0.048854, loss_ce: 0.017811
2022-01-11 23:47:17,799 iteration 3274 : loss : 0.023976, loss_ce: 0.007897
2022-01-11 23:47:19,338 iteration 3275 : loss : 0.017240, loss_ce: 0.007207
2022-01-11 23:47:20,934 iteration 3276 : loss : 0.019681, loss_ce: 0.006587
2022-01-11 23:47:22,597 iteration 3277 : loss : 0.025694, loss_ce: 0.011406
2022-01-11 23:47:24,189 iteration 3278 : loss : 0.022322, loss_ce: 0.008598
2022-01-11 23:47:25,853 iteration 3279 : loss : 0.030286, loss_ce: 0.011765
2022-01-11 23:47:27,467 iteration 3280 : loss : 0.028712, loss_ce: 0.013022
2022-01-11 23:47:29,079 iteration 3281 : loss : 0.026534, loss_ce: 0.007418
 48%|█████████████              | 193/400 [1:35:37<1:39:00, 28.70s/it]2022-01-11 23:47:30,678 iteration 3282 : loss : 0.025911, loss_ce: 0.012196
2022-01-11 23:47:32,388 iteration 3283 : loss : 0.051927, loss_ce: 0.023217
2022-01-11 23:47:33,959 iteration 3284 : loss : 0.021336, loss_ce: 0.009733
2022-01-11 23:47:35,461 iteration 3285 : loss : 0.019843, loss_ce: 0.007218
2022-01-11 23:47:37,113 iteration 3286 : loss : 0.029783, loss_ce: 0.007223
2022-01-11 23:47:38,755 iteration 3287 : loss : 0.032702, loss_ce: 0.013983
2022-01-11 23:47:40,355 iteration 3288 : loss : 0.022334, loss_ce: 0.009511
2022-01-11 23:47:41,977 iteration 3289 : loss : 0.024318, loss_ce: 0.006266
2022-01-11 23:47:43,512 iteration 3290 : loss : 0.021198, loss_ce: 0.005658
2022-01-11 23:47:45,163 iteration 3291 : loss : 0.034060, loss_ce: 0.010901
2022-01-11 23:47:46,734 iteration 3292 : loss : 0.027069, loss_ce: 0.012251
2022-01-11 23:47:48,259 iteration 3293 : loss : 0.018618, loss_ce: 0.006686
2022-01-11 23:47:49,797 iteration 3294 : loss : 0.019372, loss_ce: 0.007620
2022-01-11 23:47:51,380 iteration 3295 : loss : 0.024718, loss_ce: 0.011550
2022-01-11 23:47:52,985 iteration 3296 : loss : 0.030407, loss_ce: 0.010804
2022-01-11 23:47:54,546 iteration 3297 : loss : 0.042195, loss_ce: 0.021421
2022-01-11 23:47:56,058 iteration 3298 : loss : 0.029147, loss_ce: 0.011935
 48%|█████████████              | 194/400 [1:36:04<1:36:45, 28.18s/it]2022-01-11 23:47:57,645 iteration 3299 : loss : 0.016798, loss_ce: 0.006873
2022-01-11 23:47:59,205 iteration 3300 : loss : 0.027238, loss_ce: 0.012681
2022-01-11 23:48:00,844 iteration 3301 : loss : 0.063719, loss_ce: 0.020418
2022-01-11 23:48:02,369 iteration 3302 : loss : 0.023707, loss_ce: 0.010018
2022-01-11 23:48:04,026 iteration 3303 : loss : 0.042421, loss_ce: 0.014122
2022-01-11 23:48:05,648 iteration 3304 : loss : 0.024755, loss_ce: 0.012421
2022-01-11 23:48:07,301 iteration 3305 : loss : 0.028297, loss_ce: 0.010125
2022-01-11 23:48:08,862 iteration 3306 : loss : 0.024756, loss_ce: 0.012017
2022-01-11 23:48:10,549 iteration 3307 : loss : 0.029807, loss_ce: 0.011294
2022-01-11 23:48:12,237 iteration 3308 : loss : 0.036238, loss_ce: 0.014127
2022-01-11 23:48:13,851 iteration 3309 : loss : 0.016659, loss_ce: 0.007031
2022-01-11 23:48:15,534 iteration 3310 : loss : 0.036587, loss_ce: 0.011289
2022-01-11 23:48:17,120 iteration 3311 : loss : 0.021888, loss_ce: 0.006657
2022-01-11 23:48:18,665 iteration 3312 : loss : 0.022344, loss_ce: 0.010128
2022-01-11 23:48:20,319 iteration 3313 : loss : 0.041400, loss_ce: 0.016353
2022-01-11 23:48:21,792 iteration 3314 : loss : 0.023645, loss_ce: 0.010239
2022-01-11 23:48:21,792 Training Data Eval:
2022-01-11 23:48:29,762   Average segmentation loss on training set: 0.0174
2022-01-11 23:48:29,763 Validation Data Eval:
2022-01-11 23:48:32,501   Average segmentation loss on validation set: 0.0964
2022-01-11 23:48:34,032 iteration 3315 : loss : 0.029173, loss_ce: 0.010839
 49%|█████████████▏             | 195/400 [1:36:42<1:46:19, 31.12s/it]2022-01-11 23:48:35,640 iteration 3316 : loss : 0.031580, loss_ce: 0.008821
2022-01-11 23:48:37,259 iteration 3317 : loss : 0.033983, loss_ce: 0.011778
2022-01-11 23:48:38,872 iteration 3318 : loss : 0.033039, loss_ce: 0.013985
2022-01-11 23:48:40,416 iteration 3319 : loss : 0.035220, loss_ce: 0.014616
2022-01-11 23:48:42,133 iteration 3320 : loss : 0.028371, loss_ce: 0.010280
2022-01-11 23:48:43,654 iteration 3321 : loss : 0.017346, loss_ce: 0.008589
2022-01-11 23:48:45,153 iteration 3322 : loss : 0.018126, loss_ce: 0.006535
2022-01-11 23:48:46,763 iteration 3323 : loss : 0.022585, loss_ce: 0.010567
2022-01-11 23:48:48,361 iteration 3324 : loss : 0.027954, loss_ce: 0.008657
2022-01-11 23:48:50,023 iteration 3325 : loss : 0.032983, loss_ce: 0.011247
2022-01-11 23:48:51,639 iteration 3326 : loss : 0.027047, loss_ce: 0.010135
2022-01-11 23:48:53,400 iteration 3327 : loss : 0.043866, loss_ce: 0.015071
2022-01-11 23:48:54,963 iteration 3328 : loss : 0.022651, loss_ce: 0.012056
2022-01-11 23:48:56,533 iteration 3329 : loss : 0.022527, loss_ce: 0.007763
2022-01-11 23:48:58,151 iteration 3330 : loss : 0.024880, loss_ce: 0.011877
2022-01-11 23:48:59,653 iteration 3331 : loss : 0.019097, loss_ce: 0.007494
2022-01-11 23:49:01,143 iteration 3332 : loss : 0.017448, loss_ce: 0.006308
 49%|█████████████▏             | 196/400 [1:37:09<1:41:43, 29.92s/it]2022-01-11 23:49:02,771 iteration 3333 : loss : 0.025636, loss_ce: 0.010646
2022-01-11 23:49:04,378 iteration 3334 : loss : 0.022600, loss_ce: 0.009694
2022-01-11 23:49:06,036 iteration 3335 : loss : 0.040587, loss_ce: 0.020367
2022-01-11 23:49:07,585 iteration 3336 : loss : 0.018120, loss_ce: 0.006725
2022-01-11 23:49:09,110 iteration 3337 : loss : 0.021505, loss_ce: 0.008015
2022-01-11 23:49:10,759 iteration 3338 : loss : 0.030917, loss_ce: 0.014001
2022-01-11 23:49:12,404 iteration 3339 : loss : 0.030386, loss_ce: 0.010010
2022-01-11 23:49:14,013 iteration 3340 : loss : 0.045032, loss_ce: 0.019758
2022-01-11 23:49:15,567 iteration 3341 : loss : 0.021143, loss_ce: 0.008964
2022-01-11 23:49:17,123 iteration 3342 : loss : 0.023570, loss_ce: 0.013352
2022-01-11 23:49:18,693 iteration 3343 : loss : 0.024595, loss_ce: 0.010354
2022-01-11 23:49:20,354 iteration 3344 : loss : 0.019623, loss_ce: 0.005895
2022-01-11 23:49:22,017 iteration 3345 : loss : 0.031616, loss_ce: 0.015707
2022-01-11 23:49:23,624 iteration 3346 : loss : 0.040268, loss_ce: 0.012110
2022-01-11 23:49:25,196 iteration 3347 : loss : 0.022035, loss_ce: 0.008220
2022-01-11 23:49:26,876 iteration 3348 : loss : 0.022446, loss_ce: 0.009217
2022-01-11 23:49:28,415 iteration 3349 : loss : 0.034930, loss_ce: 0.008157
 49%|█████████████▎             | 197/400 [1:37:37<1:38:32, 29.12s/it]2022-01-11 23:49:30,005 iteration 3350 : loss : 0.019781, loss_ce: 0.009061
2022-01-11 23:49:31,636 iteration 3351 : loss : 0.024947, loss_ce: 0.010001
2022-01-11 23:49:33,191 iteration 3352 : loss : 0.024644, loss_ce: 0.007555
2022-01-11 23:49:34,779 iteration 3353 : loss : 0.025383, loss_ce: 0.008284
2022-01-11 23:49:36,329 iteration 3354 : loss : 0.017726, loss_ce: 0.005690
2022-01-11 23:49:37,925 iteration 3355 : loss : 0.019149, loss_ce: 0.007331
2022-01-11 23:49:39,471 iteration 3356 : loss : 0.024768, loss_ce: 0.010250
2022-01-11 23:49:41,009 iteration 3357 : loss : 0.018032, loss_ce: 0.007076
2022-01-11 23:49:42,603 iteration 3358 : loss : 0.025643, loss_ce: 0.009225
2022-01-11 23:49:44,163 iteration 3359 : loss : 0.025087, loss_ce: 0.012959
2022-01-11 23:49:45,803 iteration 3360 : loss : 0.029230, loss_ce: 0.011889
2022-01-11 23:49:47,408 iteration 3361 : loss : 0.018533, loss_ce: 0.007054
2022-01-11 23:49:48,970 iteration 3362 : loss : 0.024354, loss_ce: 0.009008
2022-01-11 23:49:50,464 iteration 3363 : loss : 0.023280, loss_ce: 0.008133
2022-01-11 23:49:52,119 iteration 3364 : loss : 0.028711, loss_ce: 0.011358
2022-01-11 23:49:53,709 iteration 3365 : loss : 0.017581, loss_ce: 0.007247
2022-01-11 23:49:55,290 iteration 3366 : loss : 0.021184, loss_ce: 0.008768
 50%|█████████████▎             | 198/400 [1:38:03<1:35:46, 28.45s/it]2022-01-11 23:49:57,033 iteration 3367 : loss : 0.024997, loss_ce: 0.010694
2022-01-11 23:49:58,637 iteration 3368 : loss : 0.024667, loss_ce: 0.011667
2022-01-11 23:50:00,248 iteration 3369 : loss : 0.021201, loss_ce: 0.009317
2022-01-11 23:50:01,791 iteration 3370 : loss : 0.024304, loss_ce: 0.006283
2022-01-11 23:50:03,377 iteration 3371 : loss : 0.018248, loss_ce: 0.006764
2022-01-11 23:50:05,025 iteration 3372 : loss : 0.038066, loss_ce: 0.011965
2022-01-11 23:50:06,631 iteration 3373 : loss : 0.032136, loss_ce: 0.012347
2022-01-11 23:50:08,302 iteration 3374 : loss : 0.023390, loss_ce: 0.010155
2022-01-11 23:50:10,024 iteration 3375 : loss : 0.047381, loss_ce: 0.013706
2022-01-11 23:50:11,539 iteration 3376 : loss : 0.016431, loss_ce: 0.005550
2022-01-11 23:50:13,196 iteration 3377 : loss : 0.025678, loss_ce: 0.010235
2022-01-11 23:50:14,747 iteration 3378 : loss : 0.018660, loss_ce: 0.005331
2022-01-11 23:50:16,376 iteration 3379 : loss : 0.025490, loss_ce: 0.009620
2022-01-11 23:50:17,913 iteration 3380 : loss : 0.020794, loss_ce: 0.006782
2022-01-11 23:50:19,522 iteration 3381 : loss : 0.029991, loss_ce: 0.015713
2022-01-11 23:50:21,154 iteration 3382 : loss : 0.026665, loss_ce: 0.008796
2022-01-11 23:50:22,733 iteration 3383 : loss : 0.029133, loss_ce: 0.013636
 50%|█████████████▍             | 199/400 [1:38:31<1:34:17, 28.15s/it]2022-01-11 23:50:24,369 iteration 3384 : loss : 0.022049, loss_ce: 0.008455
2022-01-11 23:50:25,944 iteration 3385 : loss : 0.035137, loss_ce: 0.012078
2022-01-11 23:50:27,498 iteration 3386 : loss : 0.019448, loss_ce: 0.006858
2022-01-11 23:50:29,041 iteration 3387 : loss : 0.028502, loss_ce: 0.009792
2022-01-11 23:50:30,680 iteration 3388 : loss : 0.028341, loss_ce: 0.010892
2022-01-11 23:50:32,219 iteration 3389 : loss : 0.021502, loss_ce: 0.007266
2022-01-11 23:50:33,830 iteration 3390 : loss : 0.031203, loss_ce: 0.011797
2022-01-11 23:50:35,530 iteration 3391 : loss : 0.033702, loss_ce: 0.019944
2022-01-11 23:50:37,176 iteration 3392 : loss : 0.061812, loss_ce: 0.023478
2022-01-11 23:50:38,712 iteration 3393 : loss : 0.022601, loss_ce: 0.009217
2022-01-11 23:50:40,290 iteration 3394 : loss : 0.026476, loss_ce: 0.008467
2022-01-11 23:50:41,900 iteration 3395 : loss : 0.020621, loss_ce: 0.008281
2022-01-11 23:50:43,501 iteration 3396 : loss : 0.033381, loss_ce: 0.012954
2022-01-11 23:50:45,029 iteration 3397 : loss : 0.020487, loss_ce: 0.007981
2022-01-11 23:50:46,770 iteration 3398 : loss : 0.029174, loss_ce: 0.011674
2022-01-11 23:50:48,395 iteration 3399 : loss : 0.021001, loss_ce: 0.010532
2022-01-11 23:50:48,395 Training Data Eval:
2022-01-11 23:50:56,360   Average segmentation loss on training set: 0.0166
2022-01-11 23:50:56,360 Validation Data Eval:
2022-01-11 23:50:59,100   Average segmentation loss on validation set: 0.0650
2022-01-11 23:51:00,694 iteration 3400 : loss : 0.027279, loss_ce: 0.008157
 50%|█████████████▌             | 200/400 [1:39:09<1:43:37, 31.09s/it]2022-01-11 23:51:02,370 iteration 3401 : loss : 0.028126, loss_ce: 0.011603
2022-01-11 23:51:04,007 iteration 3402 : loss : 0.027519, loss_ce: 0.011350
2022-01-11 23:51:05,705 iteration 3403 : loss : 0.028924, loss_ce: 0.011335
2022-01-11 23:51:07,251 iteration 3404 : loss : 0.034305, loss_ce: 0.012111
2022-01-11 23:51:08,833 iteration 3405 : loss : 0.023434, loss_ce: 0.006050
2022-01-11 23:51:10,342 iteration 3406 : loss : 0.031000, loss_ce: 0.014766
2022-01-11 23:51:11,889 iteration 3407 : loss : 0.025024, loss_ce: 0.008468
2022-01-11 23:51:13,486 iteration 3408 : loss : 0.019008, loss_ce: 0.007521
2022-01-11 23:51:15,067 iteration 3409 : loss : 0.028367, loss_ce: 0.005349
2022-01-11 23:51:16,646 iteration 3410 : loss : 0.021544, loss_ce: 0.008466
2022-01-11 23:51:18,293 iteration 3411 : loss : 0.032971, loss_ce: 0.010250
2022-01-11 23:51:19,953 iteration 3412 : loss : 0.036227, loss_ce: 0.018724
2022-01-11 23:51:21,653 iteration 3413 : loss : 0.023303, loss_ce: 0.011043
2022-01-11 23:51:23,316 iteration 3414 : loss : 0.031302, loss_ce: 0.011799
2022-01-11 23:51:24,817 iteration 3415 : loss : 0.020421, loss_ce: 0.008131
2022-01-11 23:51:26,430 iteration 3416 : loss : 0.019768, loss_ce: 0.008044
2022-01-11 23:51:27,999 iteration 3417 : loss : 0.019684, loss_ce: 0.008226
 50%|█████████████▌             | 201/400 [1:39:36<1:39:21, 29.96s/it]2022-01-11 23:51:29,741 iteration 3418 : loss : 0.052920, loss_ce: 0.014912
2022-01-11 23:51:31,368 iteration 3419 : loss : 0.022290, loss_ce: 0.011028
2022-01-11 23:51:32,897 iteration 3420 : loss : 0.018822, loss_ce: 0.009911
2022-01-11 23:51:34,507 iteration 3421 : loss : 0.054815, loss_ce: 0.016253
2022-01-11 23:51:36,103 iteration 3422 : loss : 0.048288, loss_ce: 0.017736
2022-01-11 23:51:37,768 iteration 3423 : loss : 0.037183, loss_ce: 0.019590
2022-01-11 23:51:39,297 iteration 3424 : loss : 0.022438, loss_ce: 0.007299
2022-01-11 23:51:40,910 iteration 3425 : loss : 0.037536, loss_ce: 0.018398
2022-01-11 23:51:42,495 iteration 3426 : loss : 0.018867, loss_ce: 0.005271
2022-01-11 23:51:44,229 iteration 3427 : loss : 0.043309, loss_ce: 0.015645
2022-01-11 23:51:45,854 iteration 3428 : loss : 0.028213, loss_ce: 0.011450
2022-01-11 23:51:47,484 iteration 3429 : loss : 0.029479, loss_ce: 0.016584
2022-01-11 23:51:49,122 iteration 3430 : loss : 0.023269, loss_ce: 0.009410
2022-01-11 23:51:50,792 iteration 3431 : loss : 0.027064, loss_ce: 0.010127
2022-01-11 23:51:52,345 iteration 3432 : loss : 0.026361, loss_ce: 0.008180
2022-01-11 23:51:53,955 iteration 3433 : loss : 0.032425, loss_ce: 0.008748
2022-01-11 23:51:55,524 iteration 3434 : loss : 0.023800, loss_ce: 0.007141
 50%|█████████████▋             | 202/400 [1:40:04<1:36:26, 29.23s/it]2022-01-11 23:51:57,089 iteration 3435 : loss : 0.022747, loss_ce: 0.011043
2022-01-11 23:51:58,661 iteration 3436 : loss : 0.024193, loss_ce: 0.008881
2022-01-11 23:52:00,214 iteration 3437 : loss : 0.020207, loss_ce: 0.006887
2022-01-11 23:52:01,796 iteration 3438 : loss : 0.024568, loss_ce: 0.011840
2022-01-11 23:52:03,471 iteration 3439 : loss : 0.021787, loss_ce: 0.008912
2022-01-11 23:52:05,142 iteration 3440 : loss : 0.039174, loss_ce: 0.010244
2022-01-11 23:52:06,630 iteration 3441 : loss : 0.022388, loss_ce: 0.011670
2022-01-11 23:52:08,192 iteration 3442 : loss : 0.016297, loss_ce: 0.005136
2022-01-11 23:52:09,826 iteration 3443 : loss : 0.018770, loss_ce: 0.006587
2022-01-11 23:52:11,393 iteration 3444 : loss : 0.020909, loss_ce: 0.006898
2022-01-11 23:52:13,037 iteration 3445 : loss : 0.017551, loss_ce: 0.005942
2022-01-11 23:52:14,623 iteration 3446 : loss : 0.032374, loss_ce: 0.011511
2022-01-11 23:52:16,229 iteration 3447 : loss : 0.022697, loss_ce: 0.011423
2022-01-11 23:52:17,731 iteration 3448 : loss : 0.019508, loss_ce: 0.006493
2022-01-11 23:52:19,328 iteration 3449 : loss : 0.027470, loss_ce: 0.008032
2022-01-11 23:52:20,979 iteration 3450 : loss : 0.019812, loss_ce: 0.007021
2022-01-11 23:52:22,513 iteration 3451 : loss : 0.017417, loss_ce: 0.006813
 51%|█████████████▋             | 203/400 [1:40:31<1:33:45, 28.56s/it]2022-01-11 23:52:24,144 iteration 3452 : loss : 0.021063, loss_ce: 0.006558
2022-01-11 23:52:25,800 iteration 3453 : loss : 0.026165, loss_ce: 0.010657
2022-01-11 23:52:27,462 iteration 3454 : loss : 0.023269, loss_ce: 0.008376
2022-01-11 23:52:29,083 iteration 3455 : loss : 0.025110, loss_ce: 0.010410
2022-01-11 23:52:30,714 iteration 3456 : loss : 0.031771, loss_ce: 0.009065
2022-01-11 23:52:32,340 iteration 3457 : loss : 0.020781, loss_ce: 0.008011
2022-01-11 23:52:33,989 iteration 3458 : loss : 0.016271, loss_ce: 0.007598
2022-01-11 23:52:35,691 iteration 3459 : loss : 0.030809, loss_ce: 0.014852
2022-01-11 23:52:37,349 iteration 3460 : loss : 0.021955, loss_ce: 0.007142
2022-01-11 23:52:38,958 iteration 3461 : loss : 0.026506, loss_ce: 0.010600
2022-01-11 23:52:40,501 iteration 3462 : loss : 0.018124, loss_ce: 0.006194
2022-01-11 23:52:42,004 iteration 3463 : loss : 0.015797, loss_ce: 0.005054
2022-01-11 23:52:43,580 iteration 3464 : loss : 0.023653, loss_ce: 0.008737
2022-01-11 23:52:45,109 iteration 3465 : loss : 0.025878, loss_ce: 0.009674
2022-01-11 23:52:46,739 iteration 3466 : loss : 0.019224, loss_ce: 0.007998
2022-01-11 23:52:48,434 iteration 3467 : loss : 0.028670, loss_ce: 0.011178
2022-01-11 23:52:50,170 iteration 3468 : loss : 0.038347, loss_ce: 0.011695
 51%|█████████████▊             | 204/400 [1:40:58<1:32:24, 28.29s/it]2022-01-11 23:52:51,733 iteration 3469 : loss : 0.030469, loss_ce: 0.013617
2022-01-11 23:52:53,496 iteration 3470 : loss : 0.033236, loss_ce: 0.013464
2022-01-11 23:52:55,145 iteration 3471 : loss : 0.023792, loss_ce: 0.008066
2022-01-11 23:52:56,816 iteration 3472 : loss : 0.025477, loss_ce: 0.010869
2022-01-11 23:52:58,474 iteration 3473 : loss : 0.021051, loss_ce: 0.010344
2022-01-11 23:53:00,036 iteration 3474 : loss : 0.025433, loss_ce: 0.009948
2022-01-11 23:53:01,732 iteration 3475 : loss : 0.040653, loss_ce: 0.014797
2022-01-11 23:53:03,317 iteration 3476 : loss : 0.031976, loss_ce: 0.010808
2022-01-11 23:53:04,963 iteration 3477 : loss : 0.032018, loss_ce: 0.011584
2022-01-11 23:53:06,588 iteration 3478 : loss : 0.027298, loss_ce: 0.008878
2022-01-11 23:53:08,175 iteration 3479 : loss : 0.026469, loss_ce: 0.012465
2022-01-11 23:53:09,731 iteration 3480 : loss : 0.018961, loss_ce: 0.007370
2022-01-11 23:53:11,338 iteration 3481 : loss : 0.023710, loss_ce: 0.011779
2022-01-11 23:53:12,872 iteration 3482 : loss : 0.024093, loss_ce: 0.008270
2022-01-11 23:53:14,542 iteration 3483 : loss : 0.018957, loss_ce: 0.007040
2022-01-11 23:53:16,146 iteration 3484 : loss : 0.019235, loss_ce: 0.005790
2022-01-11 23:53:16,146 Training Data Eval:
2022-01-11 23:53:24,115   Average segmentation loss on training set: 0.0142
2022-01-11 23:53:24,116 Validation Data Eval:
2022-01-11 23:53:26,860   Average segmentation loss on validation set: 0.0706
2022-01-11 23:53:28,450 iteration 3485 : loss : 0.018496, loss_ce: 0.007101
 51%|█████████████▊             | 205/400 [1:41:37<1:41:40, 31.28s/it]2022-01-11 23:53:30,052 iteration 3486 : loss : 0.019612, loss_ce: 0.006309
2022-01-11 23:53:31,641 iteration 3487 : loss : 0.022639, loss_ce: 0.009154
2022-01-11 23:53:33,285 iteration 3488 : loss : 0.026363, loss_ce: 0.015058
2022-01-11 23:53:34,837 iteration 3489 : loss : 0.024718, loss_ce: 0.008857
2022-01-11 23:53:36,507 iteration 3490 : loss : 0.027565, loss_ce: 0.010853
2022-01-11 23:53:38,139 iteration 3491 : loss : 0.025014, loss_ce: 0.010723
2022-01-11 23:53:39,656 iteration 3492 : loss : 0.019235, loss_ce: 0.005532
2022-01-11 23:53:41,340 iteration 3493 : loss : 0.025254, loss_ce: 0.007088
2022-01-11 23:53:42,958 iteration 3494 : loss : 0.027191, loss_ce: 0.016697
2022-01-11 23:53:44,457 iteration 3495 : loss : 0.017005, loss_ce: 0.006992
2022-01-11 23:53:46,080 iteration 3496 : loss : 0.047467, loss_ce: 0.016391
2022-01-11 23:53:47,761 iteration 3497 : loss : 0.022156, loss_ce: 0.009952
2022-01-11 23:53:49,364 iteration 3498 : loss : 0.025822, loss_ce: 0.009985
2022-01-11 23:53:51,037 iteration 3499 : loss : 0.040655, loss_ce: 0.010447
2022-01-11 23:53:52,616 iteration 3500 : loss : 0.036101, loss_ce: 0.012188
2022-01-11 23:53:54,247 iteration 3501 : loss : 0.023648, loss_ce: 0.008625
2022-01-11 23:53:55,857 iteration 3502 : loss : 0.026894, loss_ce: 0.009442
 52%|█████████████▉             | 206/400 [1:42:04<1:37:23, 30.12s/it]2022-01-11 23:53:57,432 iteration 3503 : loss : 0.018173, loss_ce: 0.008993
2022-01-11 23:53:59,055 iteration 3504 : loss : 0.021585, loss_ce: 0.006185
2022-01-11 23:54:00,665 iteration 3505 : loss : 0.025610, loss_ce: 0.013281
2022-01-11 23:54:02,361 iteration 3506 : loss : 0.033636, loss_ce: 0.013563
2022-01-11 23:54:03,937 iteration 3507 : loss : 0.018025, loss_ce: 0.007312
2022-01-11 23:54:05,449 iteration 3508 : loss : 0.020478, loss_ce: 0.006478
2022-01-11 23:54:07,026 iteration 3509 : loss : 0.025578, loss_ce: 0.010183
2022-01-11 23:54:08,726 iteration 3510 : loss : 0.026273, loss_ce: 0.011103
2022-01-11 23:54:10,314 iteration 3511 : loss : 0.021082, loss_ce: 0.008866
2022-01-11 23:54:11,921 iteration 3512 : loss : 0.027183, loss_ce: 0.009838
2022-01-11 23:54:13,442 iteration 3513 : loss : 0.024338, loss_ce: 0.007703
2022-01-11 23:54:14,960 iteration 3514 : loss : 0.014260, loss_ce: 0.006064
2022-01-11 23:54:16,530 iteration 3515 : loss : 0.020007, loss_ce: 0.006241
2022-01-11 23:54:18,154 iteration 3516 : loss : 0.025604, loss_ce: 0.009071
2022-01-11 23:54:19,757 iteration 3517 : loss : 0.025729, loss_ce: 0.010515
2022-01-11 23:54:21,293 iteration 3518 : loss : 0.016029, loss_ce: 0.007424
2022-01-11 23:54:22,848 iteration 3519 : loss : 0.030232, loss_ce: 0.009613
 52%|█████████████▉             | 207/400 [1:42:31<1:33:52, 29.18s/it]2022-01-11 23:54:24,462 iteration 3520 : loss : 0.018539, loss_ce: 0.006162
2022-01-11 23:54:26,102 iteration 3521 : loss : 0.039413, loss_ce: 0.015340
2022-01-11 23:54:27,737 iteration 3522 : loss : 0.036888, loss_ce: 0.016928
2022-01-11 23:54:29,448 iteration 3523 : loss : 0.032189, loss_ce: 0.012861
2022-01-11 23:54:31,024 iteration 3524 : loss : 0.032128, loss_ce: 0.007871
2022-01-11 23:54:32,653 iteration 3525 : loss : 0.029987, loss_ce: 0.012225
2022-01-11 23:54:34,272 iteration 3526 : loss : 0.027632, loss_ce: 0.013760
2022-01-11 23:54:35,917 iteration 3527 : loss : 0.020243, loss_ce: 0.006100
2022-01-11 23:54:37,465 iteration 3528 : loss : 0.017458, loss_ce: 0.008880
2022-01-11 23:54:39,073 iteration 3529 : loss : 0.022769, loss_ce: 0.007706
2022-01-11 23:54:40,659 iteration 3530 : loss : 0.029741, loss_ce: 0.009607
2022-01-11 23:54:42,273 iteration 3531 : loss : 0.038920, loss_ce: 0.017428
2022-01-11 23:54:43,839 iteration 3532 : loss : 0.019416, loss_ce: 0.006846
2022-01-11 23:54:45,372 iteration 3533 : loss : 0.028736, loss_ce: 0.010702
2022-01-11 23:54:46,923 iteration 3534 : loss : 0.025763, loss_ce: 0.014014
2022-01-11 23:54:48,450 iteration 3535 : loss : 0.019997, loss_ce: 0.007490
2022-01-11 23:54:49,974 iteration 3536 : loss : 0.019140, loss_ce: 0.007221
 52%|██████████████             | 208/400 [1:42:58<1:31:24, 28.57s/it]2022-01-11 23:54:51,569 iteration 3537 : loss : 0.024353, loss_ce: 0.010039
2022-01-11 23:54:53,171 iteration 3538 : loss : 0.033418, loss_ce: 0.010153
2022-01-11 23:54:54,804 iteration 3539 : loss : 0.028047, loss_ce: 0.012751
2022-01-11 23:54:56,344 iteration 3540 : loss : 0.021820, loss_ce: 0.005234
2022-01-11 23:54:58,006 iteration 3541 : loss : 0.018367, loss_ce: 0.007426
2022-01-11 23:54:59,577 iteration 3542 : loss : 0.023019, loss_ce: 0.006900
2022-01-11 23:55:01,200 iteration 3543 : loss : 0.020858, loss_ce: 0.009366
2022-01-11 23:55:02,838 iteration 3544 : loss : 0.025730, loss_ce: 0.009336
2022-01-11 23:55:04,396 iteration 3545 : loss : 0.020784, loss_ce: 0.007289
2022-01-11 23:55:05,896 iteration 3546 : loss : 0.019074, loss_ce: 0.008452
2022-01-11 23:55:07,531 iteration 3547 : loss : 0.023223, loss_ce: 0.008002
2022-01-11 23:55:09,168 iteration 3548 : loss : 0.021540, loss_ce: 0.007052
2022-01-11 23:55:10,780 iteration 3549 : loss : 0.025093, loss_ce: 0.011354
2022-01-11 23:55:12,325 iteration 3550 : loss : 0.019625, loss_ce: 0.006646
2022-01-11 23:55:13,986 iteration 3551 : loss : 0.029387, loss_ce: 0.008592
2022-01-11 23:55:15,648 iteration 3552 : loss : 0.037537, loss_ce: 0.014226
2022-01-11 23:55:17,309 iteration 3553 : loss : 0.028814, loss_ce: 0.010522
 52%|██████████████             | 209/400 [1:43:25<1:29:44, 28.19s/it]2022-01-11 23:55:18,930 iteration 3554 : loss : 0.020572, loss_ce: 0.008011
2022-01-11 23:55:20,578 iteration 3555 : loss : 0.022928, loss_ce: 0.010129
2022-01-11 23:55:22,162 iteration 3556 : loss : 0.025156, loss_ce: 0.008689
2022-01-11 23:55:23,728 iteration 3557 : loss : 0.019106, loss_ce: 0.007993
2022-01-11 23:55:25,250 iteration 3558 : loss : 0.018220, loss_ce: 0.006144
2022-01-11 23:55:26,929 iteration 3559 : loss : 0.026003, loss_ce: 0.009355
2022-01-11 23:55:28,500 iteration 3560 : loss : 0.050358, loss_ce: 0.013652
2022-01-11 23:55:30,138 iteration 3561 : loss : 0.018737, loss_ce: 0.008584
2022-01-11 23:55:31,728 iteration 3562 : loss : 0.023319, loss_ce: 0.008287
2022-01-11 23:55:33,327 iteration 3563 : loss : 0.017239, loss_ce: 0.006052
2022-01-11 23:55:34,943 iteration 3564 : loss : 0.033609, loss_ce: 0.010222
2022-01-11 23:55:36,482 iteration 3565 : loss : 0.018424, loss_ce: 0.006936
2022-01-11 23:55:37,955 iteration 3566 : loss : 0.015282, loss_ce: 0.005774
2022-01-11 23:55:39,469 iteration 3567 : loss : 0.022420, loss_ce: 0.006568
2022-01-11 23:55:41,094 iteration 3568 : loss : 0.028513, loss_ce: 0.010885
2022-01-11 23:55:42,637 iteration 3569 : loss : 0.020424, loss_ce: 0.008375
2022-01-11 23:55:42,637 Training Data Eval:
2022-01-11 23:55:50,590   Average segmentation loss on training set: 0.0151
2022-01-11 23:55:50,590 Validation Data Eval:
2022-01-11 23:55:53,331   Average segmentation loss on validation set: 0.0741
2022-01-11 23:55:54,889 iteration 3570 : loss : 0.020261, loss_ce: 0.005860
 52%|██████████████▏            | 210/400 [1:44:03<1:38:12, 31.01s/it]2022-01-11 23:55:56,554 iteration 3571 : loss : 0.033163, loss_ce: 0.014107
2022-01-11 23:55:58,238 iteration 3572 : loss : 0.023303, loss_ce: 0.008015
2022-01-11 23:55:59,867 iteration 3573 : loss : 0.025222, loss_ce: 0.009923
2022-01-11 23:56:01,430 iteration 3574 : loss : 0.021713, loss_ce: 0.008763
2022-01-11 23:56:02,978 iteration 3575 : loss : 0.019465, loss_ce: 0.007232
2022-01-11 23:56:04,530 iteration 3576 : loss : 0.035013, loss_ce: 0.012282
2022-01-11 23:56:06,160 iteration 3577 : loss : 0.021735, loss_ce: 0.008800
2022-01-11 23:56:07,733 iteration 3578 : loss : 0.022545, loss_ce: 0.009045
2022-01-11 23:56:09,338 iteration 3579 : loss : 0.019339, loss_ce: 0.006772
2022-01-11 23:56:10,986 iteration 3580 : loss : 0.019000, loss_ce: 0.008786
2022-01-11 23:56:12,625 iteration 3581 : loss : 0.018605, loss_ce: 0.006567
2022-01-11 23:56:14,250 iteration 3582 : loss : 0.022020, loss_ce: 0.007164
2022-01-11 23:56:15,828 iteration 3583 : loss : 0.038109, loss_ce: 0.008127
2022-01-11 23:56:17,348 iteration 3584 : loss : 0.017535, loss_ce: 0.005973
2022-01-11 23:56:18,899 iteration 3585 : loss : 0.015434, loss_ce: 0.005852
2022-01-11 23:56:20,426 iteration 3586 : loss : 0.017367, loss_ce: 0.006270
2022-01-11 23:56:22,021 iteration 3587 : loss : 0.026337, loss_ce: 0.012375
 53%|██████████████▏            | 211/400 [1:44:30<1:34:00, 29.85s/it]2022-01-11 23:56:23,717 iteration 3588 : loss : 0.020232, loss_ce: 0.009025
2022-01-11 23:56:25,380 iteration 3589 : loss : 0.021306, loss_ce: 0.006804
2022-01-11 23:56:26,984 iteration 3590 : loss : 0.024709, loss_ce: 0.006743
2022-01-11 23:56:28,534 iteration 3591 : loss : 0.024639, loss_ce: 0.008315
2022-01-11 23:56:30,122 iteration 3592 : loss : 0.019220, loss_ce: 0.007379
2022-01-11 23:56:31,755 iteration 3593 : loss : 0.020049, loss_ce: 0.009953
2022-01-11 23:56:33,346 iteration 3594 : loss : 0.024991, loss_ce: 0.007938
2022-01-11 23:56:34,924 iteration 3595 : loss : 0.024399, loss_ce: 0.010765
2022-01-11 23:56:36,479 iteration 3596 : loss : 0.019122, loss_ce: 0.008247
2022-01-11 23:56:38,079 iteration 3597 : loss : 0.019840, loss_ce: 0.008549
2022-01-11 23:56:39,706 iteration 3598 : loss : 0.019133, loss_ce: 0.007735
2022-01-11 23:56:41,348 iteration 3599 : loss : 0.021513, loss_ce: 0.007450
2022-01-11 23:56:42,887 iteration 3600 : loss : 0.017950, loss_ce: 0.010598
2022-01-11 23:56:44,501 iteration 3601 : loss : 0.021390, loss_ce: 0.008253
2022-01-11 23:56:46,139 iteration 3602 : loss : 0.030950, loss_ce: 0.011133
2022-01-11 23:56:47,705 iteration 3603 : loss : 0.021185, loss_ce: 0.009992
2022-01-11 23:56:49,286 iteration 3604 : loss : 0.024912, loss_ce: 0.006469
 53%|██████████████▎            | 212/400 [1:44:57<1:31:05, 29.07s/it]2022-01-11 23:56:51,046 iteration 3605 : loss : 0.029328, loss_ce: 0.012743
2022-01-11 23:56:52,578 iteration 3606 : loss : 0.024102, loss_ce: 0.004821
2022-01-11 23:56:54,062 iteration 3607 : loss : 0.011963, loss_ce: 0.004164
2022-01-11 23:56:55,710 iteration 3608 : loss : 0.021559, loss_ce: 0.007390
2022-01-11 23:56:57,351 iteration 3609 : loss : 0.038208, loss_ce: 0.016568
2022-01-11 23:56:58,869 iteration 3610 : loss : 0.012901, loss_ce: 0.005160
2022-01-11 23:57:00,446 iteration 3611 : loss : 0.024989, loss_ce: 0.011256
2022-01-11 23:57:01,984 iteration 3612 : loss : 0.020519, loss_ce: 0.009280
2022-01-11 23:57:03,578 iteration 3613 : loss : 0.016174, loss_ce: 0.005868
2022-01-11 23:57:05,129 iteration 3614 : loss : 0.021402, loss_ce: 0.007839
2022-01-11 23:57:06,696 iteration 3615 : loss : 0.025650, loss_ce: 0.009130
2022-01-11 23:57:08,261 iteration 3616 : loss : 0.019039, loss_ce: 0.008756
2022-01-11 23:57:09,872 iteration 3617 : loss : 0.023371, loss_ce: 0.011755
2022-01-11 23:57:11,508 iteration 3618 : loss : 0.020015, loss_ce: 0.007176
2022-01-11 23:57:13,080 iteration 3619 : loss : 0.016528, loss_ce: 0.005978
2022-01-11 23:57:14,683 iteration 3620 : loss : 0.022142, loss_ce: 0.007150
2022-01-11 23:57:16,270 iteration 3621 : loss : 0.018513, loss_ce: 0.008349
 53%|██████████████▍            | 213/400 [1:45:24<1:28:39, 28.44s/it]2022-01-11 23:57:17,926 iteration 3622 : loss : 0.018810, loss_ce: 0.006061
2022-01-11 23:57:19,544 iteration 3623 : loss : 0.020456, loss_ce: 0.007637
2022-01-11 23:57:21,152 iteration 3624 : loss : 0.017800, loss_ce: 0.007004
2022-01-11 23:57:22,659 iteration 3625 : loss : 0.018392, loss_ce: 0.006872
2022-01-11 23:57:24,267 iteration 3626 : loss : 0.024017, loss_ce: 0.006885
2022-01-11 23:57:25,889 iteration 3627 : loss : 0.022467, loss_ce: 0.009017
2022-01-11 23:57:27,522 iteration 3628 : loss : 0.045288, loss_ce: 0.015159
2022-01-11 23:57:29,188 iteration 3629 : loss : 0.038652, loss_ce: 0.022914
2022-01-11 23:57:30,796 iteration 3630 : loss : 0.019406, loss_ce: 0.008552
2022-01-11 23:57:32,400 iteration 3631 : loss : 0.019877, loss_ce: 0.006938
2022-01-11 23:57:34,034 iteration 3632 : loss : 0.025107, loss_ce: 0.012986
2022-01-11 23:57:35,678 iteration 3633 : loss : 0.026232, loss_ce: 0.008905
2022-01-11 23:57:37,224 iteration 3634 : loss : 0.019406, loss_ce: 0.006222
2022-01-11 23:57:38,939 iteration 3635 : loss : 0.022289, loss_ce: 0.010837
2022-01-11 23:57:40,455 iteration 3636 : loss : 0.015137, loss_ce: 0.007139
2022-01-11 23:57:42,052 iteration 3637 : loss : 0.020995, loss_ce: 0.007962
2022-01-11 23:57:43,664 iteration 3638 : loss : 0.026813, loss_ce: 0.008973
 54%|██████████████▍            | 214/400 [1:45:52<1:27:12, 28.13s/it]2022-01-11 23:57:45,326 iteration 3639 : loss : 0.048402, loss_ce: 0.013213
2022-01-11 23:57:46,983 iteration 3640 : loss : 0.027486, loss_ce: 0.009275
2022-01-11 23:57:48,625 iteration 3641 : loss : 0.023537, loss_ce: 0.008176
2022-01-11 23:57:50,205 iteration 3642 : loss : 0.021840, loss_ce: 0.010366
2022-01-11 23:57:51,831 iteration 3643 : loss : 0.043960, loss_ce: 0.011490
2022-01-11 23:57:53,468 iteration 3644 : loss : 0.018553, loss_ce: 0.006941
2022-01-11 23:57:55,157 iteration 3645 : loss : 0.037330, loss_ce: 0.009101
2022-01-11 23:57:56,775 iteration 3646 : loss : 0.021800, loss_ce: 0.009842
2022-01-11 23:57:58,345 iteration 3647 : loss : 0.019893, loss_ce: 0.007650
2022-01-11 23:57:59,961 iteration 3648 : loss : 0.023018, loss_ce: 0.008655
2022-01-11 23:58:01,447 iteration 3649 : loss : 0.022034, loss_ce: 0.008533
2022-01-11 23:58:03,046 iteration 3650 : loss : 0.028344, loss_ce: 0.011321
2022-01-11 23:58:04,597 iteration 3651 : loss : 0.018221, loss_ce: 0.006451
2022-01-11 23:58:06,133 iteration 3652 : loss : 0.019043, loss_ce: 0.008235
2022-01-11 23:58:07,782 iteration 3653 : loss : 0.031620, loss_ce: 0.012344
2022-01-11 23:58:09,347 iteration 3654 : loss : 0.018814, loss_ce: 0.008150
2022-01-11 23:58:09,347 Training Data Eval:
2022-01-11 23:58:17,309   Average segmentation loss on training set: 0.0145
2022-01-11 23:58:17,309 Validation Data Eval:
2022-01-11 23:58:20,058   Average segmentation loss on validation set: 0.0870
2022-01-11 23:58:21,819 iteration 3655 : loss : 0.033442, loss_ce: 0.011073
 54%|██████████████▌            | 215/400 [1:46:30<1:35:59, 31.13s/it]2022-01-11 23:58:23,444 iteration 3656 : loss : 0.021228, loss_ce: 0.007023
2022-01-11 23:58:25,085 iteration 3657 : loss : 0.019813, loss_ce: 0.008284
2022-01-11 23:58:26,675 iteration 3658 : loss : 0.025049, loss_ce: 0.010153
2022-01-11 23:58:28,203 iteration 3659 : loss : 0.031447, loss_ce: 0.010142
2022-01-11 23:58:29,878 iteration 3660 : loss : 0.030304, loss_ce: 0.012735
2022-01-11 23:58:31,458 iteration 3661 : loss : 0.023705, loss_ce: 0.009437
2022-01-11 23:58:32,989 iteration 3662 : loss : 0.026452, loss_ce: 0.012656
2022-01-11 23:58:34,575 iteration 3663 : loss : 0.018388, loss_ce: 0.006931
2022-01-11 23:58:36,195 iteration 3664 : loss : 0.023299, loss_ce: 0.007226
2022-01-11 23:58:37,908 iteration 3665 : loss : 0.030000, loss_ce: 0.008688
2022-01-11 23:58:39,553 iteration 3666 : loss : 0.020059, loss_ce: 0.007838
2022-01-11 23:58:41,115 iteration 3667 : loss : 0.021478, loss_ce: 0.008157
2022-01-11 23:58:42,695 iteration 3668 : loss : 0.024135, loss_ce: 0.008636
2022-01-11 23:58:44,311 iteration 3669 : loss : 0.028169, loss_ce: 0.009731
2022-01-11 23:58:45,834 iteration 3670 : loss : 0.018540, loss_ce: 0.008742
2022-01-11 23:58:47,406 iteration 3671 : loss : 0.030857, loss_ce: 0.007832
2022-01-11 23:58:48,977 iteration 3672 : loss : 0.021310, loss_ce: 0.006050
 54%|██████████████▌            | 216/400 [1:46:57<1:31:49, 29.94s/it]2022-01-11 23:58:50,587 iteration 3673 : loss : 0.017364, loss_ce: 0.007683
2022-01-11 23:58:52,253 iteration 3674 : loss : 0.018352, loss_ce: 0.008597
2022-01-11 23:58:53,948 iteration 3675 : loss : 0.032965, loss_ce: 0.012832
2022-01-11 23:58:55,597 iteration 3676 : loss : 0.021269, loss_ce: 0.006203
2022-01-11 23:58:57,219 iteration 3677 : loss : 0.020452, loss_ce: 0.009002
2022-01-11 23:58:58,922 iteration 3678 : loss : 0.027011, loss_ce: 0.009680
2022-01-11 23:59:00,448 iteration 3679 : loss : 0.015320, loss_ce: 0.005219
2022-01-11 23:59:01,980 iteration 3680 : loss : 0.019288, loss_ce: 0.007999
2022-01-11 23:59:03,571 iteration 3681 : loss : 0.034493, loss_ce: 0.009803
2022-01-11 23:59:05,064 iteration 3682 : loss : 0.016015, loss_ce: 0.004913
2022-01-11 23:59:06,651 iteration 3683 : loss : 0.028304, loss_ce: 0.017988
2022-01-11 23:59:08,177 iteration 3684 : loss : 0.018183, loss_ce: 0.006121
2022-01-11 23:59:09,744 iteration 3685 : loss : 0.017805, loss_ce: 0.006506
2022-01-11 23:59:11,408 iteration 3686 : loss : 0.023213, loss_ce: 0.009210
2022-01-11 23:59:13,075 iteration 3687 : loss : 0.026892, loss_ce: 0.011790
2022-01-11 23:59:14,689 iteration 3688 : loss : 0.027990, loss_ce: 0.009732
2022-01-11 23:59:16,311 iteration 3689 : loss : 0.030617, loss_ce: 0.008571
 54%|██████████████▋            | 217/400 [1:47:24<1:28:56, 29.16s/it]2022-01-11 23:59:18,090 iteration 3690 : loss : 0.025526, loss_ce: 0.011597
2022-01-11 23:59:19,669 iteration 3691 : loss : 0.029367, loss_ce: 0.007943
2022-01-11 23:59:21,286 iteration 3692 : loss : 0.026706, loss_ce: 0.008149
2022-01-11 23:59:22,832 iteration 3693 : loss : 0.019107, loss_ce: 0.005050
2022-01-11 23:59:24,462 iteration 3694 : loss : 0.024725, loss_ce: 0.010075
2022-01-11 23:59:26,075 iteration 3695 : loss : 0.020266, loss_ce: 0.007294
2022-01-11 23:59:27,609 iteration 3696 : loss : 0.019080, loss_ce: 0.007923
2022-01-11 23:59:29,150 iteration 3697 : loss : 0.028622, loss_ce: 0.009084
2022-01-11 23:59:30,846 iteration 3698 : loss : 0.023186, loss_ce: 0.008000
2022-01-11 23:59:32,436 iteration 3699 : loss : 0.026880, loss_ce: 0.014945
2022-01-11 23:59:34,021 iteration 3700 : loss : 0.024818, loss_ce: 0.008302
2022-01-11 23:59:35,535 iteration 3701 : loss : 0.029887, loss_ce: 0.014441
2022-01-11 23:59:37,174 iteration 3702 : loss : 0.029863, loss_ce: 0.010951
2022-01-11 23:59:38,777 iteration 3703 : loss : 0.018558, loss_ce: 0.006419
2022-01-11 23:59:40,367 iteration 3704 : loss : 0.028884, loss_ce: 0.012107
2022-01-11 23:59:41,962 iteration 3705 : loss : 0.035573, loss_ce: 0.019600
2022-01-11 23:59:43,487 iteration 3706 : loss : 0.025956, loss_ce: 0.007960
 55%|██████████████▋            | 218/400 [1:47:52<1:26:38, 28.56s/it]2022-01-11 23:59:45,151 iteration 3707 : loss : 0.055253, loss_ce: 0.010627
2022-01-11 23:59:46,883 iteration 3708 : loss : 0.054111, loss_ce: 0.017527
2022-01-11 23:59:48,488 iteration 3709 : loss : 0.020105, loss_ce: 0.006547
2022-01-11 23:59:49,988 iteration 3710 : loss : 0.018313, loss_ce: 0.006950
2022-01-11 23:59:51,711 iteration 3711 : loss : 0.028996, loss_ce: 0.013256
2022-01-11 23:59:53,258 iteration 3712 : loss : 0.045617, loss_ce: 0.017610
2022-01-11 23:59:54,781 iteration 3713 : loss : 0.016228, loss_ce: 0.005892
2022-01-11 23:59:56,414 iteration 3714 : loss : 0.028770, loss_ce: 0.012798
2022-01-11 23:59:57,973 iteration 3715 : loss : 0.020126, loss_ce: 0.007892
2022-01-11 23:59:59,563 iteration 3716 : loss : 0.031001, loss_ce: 0.013329
2022-01-12 00:00:01,174 iteration 3717 : loss : 0.021268, loss_ce: 0.010564
2022-01-12 00:00:02,817 iteration 3718 : loss : 0.031191, loss_ce: 0.011584
2022-01-12 00:00:04,519 iteration 3719 : loss : 0.028563, loss_ce: 0.009205
2022-01-12 00:00:06,200 iteration 3720 : loss : 0.054888, loss_ce: 0.022239
2022-01-12 00:00:07,815 iteration 3721 : loss : 0.021620, loss_ce: 0.009161
2022-01-12 00:00:09,491 iteration 3722 : loss : 0.032656, loss_ce: 0.010494
2022-01-12 00:00:11,073 iteration 3723 : loss : 0.021753, loss_ce: 0.009909
 55%|██████████████▊            | 219/400 [1:48:19<1:25:16, 28.27s/it]2022-01-12 00:00:12,715 iteration 3724 : loss : 0.026482, loss_ce: 0.014715
2022-01-12 00:00:14,363 iteration 3725 : loss : 0.024356, loss_ce: 0.009312
2022-01-12 00:00:15,949 iteration 3726 : loss : 0.022903, loss_ce: 0.009891
2022-01-12 00:00:17,563 iteration 3727 : loss : 0.017802, loss_ce: 0.005874
2022-01-12 00:00:19,057 iteration 3728 : loss : 0.022354, loss_ce: 0.010595
2022-01-12 00:00:20,694 iteration 3729 : loss : 0.038212, loss_ce: 0.012021
2022-01-12 00:00:22,250 iteration 3730 : loss : 0.022717, loss_ce: 0.008302
2022-01-12 00:00:23,918 iteration 3731 : loss : 0.031319, loss_ce: 0.014732
2022-01-12 00:00:25,511 iteration 3732 : loss : 0.022286, loss_ce: 0.010704
2022-01-12 00:00:27,047 iteration 3733 : loss : 0.019424, loss_ce: 0.005379
2022-01-12 00:00:28,676 iteration 3734 : loss : 0.024402, loss_ce: 0.008629
2022-01-12 00:00:30,339 iteration 3735 : loss : 0.026483, loss_ce: 0.009016
2022-01-12 00:00:31,959 iteration 3736 : loss : 0.042682, loss_ce: 0.017621
2022-01-12 00:00:33,624 iteration 3737 : loss : 0.045774, loss_ce: 0.024935
2022-01-12 00:00:35,216 iteration 3738 : loss : 0.036075, loss_ce: 0.010579
2022-01-12 00:00:36,794 iteration 3739 : loss : 0.027075, loss_ce: 0.007076
2022-01-12 00:00:36,795 Training Data Eval:
2022-01-12 00:00:44,733   Average segmentation loss on training set: 0.0166
2022-01-12 00:00:44,734 Validation Data Eval:
2022-01-12 00:00:47,467   Average segmentation loss on validation set: 0.0677
2022-01-12 00:00:48,987 iteration 3740 : loss : 0.025885, loss_ce: 0.007985
 55%|██████████████▊            | 220/400 [1:48:57<1:33:29, 31.17s/it]2022-01-12 00:00:50,617 iteration 3741 : loss : 0.021347, loss_ce: 0.009347
2022-01-12 00:00:52,172 iteration 3742 : loss : 0.028640, loss_ce: 0.010622
2022-01-12 00:00:53,720 iteration 3743 : loss : 0.019455, loss_ce: 0.005770
2022-01-12 00:00:55,311 iteration 3744 : loss : 0.016824, loss_ce: 0.006564
2022-01-12 00:00:56,967 iteration 3745 : loss : 0.036247, loss_ce: 0.021877
2022-01-12 00:00:58,526 iteration 3746 : loss : 0.026681, loss_ce: 0.008086
2022-01-12 00:01:00,145 iteration 3747 : loss : 0.036506, loss_ce: 0.020363
2022-01-12 00:01:01,777 iteration 3748 : loss : 0.036801, loss_ce: 0.012835
2022-01-12 00:01:03,395 iteration 3749 : loss : 0.026400, loss_ce: 0.008329
2022-01-12 00:01:05,088 iteration 3750 : loss : 0.029279, loss_ce: 0.010833
2022-01-12 00:01:06,664 iteration 3751 : loss : 0.021196, loss_ce: 0.009470
2022-01-12 00:01:08,214 iteration 3752 : loss : 0.019277, loss_ce: 0.006969
2022-01-12 00:01:09,782 iteration 3753 : loss : 0.019777, loss_ce: 0.006699
2022-01-12 00:01:11,290 iteration 3754 : loss : 0.015885, loss_ce: 0.007591
2022-01-12 00:01:12,883 iteration 3755 : loss : 0.023077, loss_ce: 0.008352
2022-01-12 00:01:14,432 iteration 3756 : loss : 0.018359, loss_ce: 0.006550
2022-01-12 00:01:16,012 iteration 3757 : loss : 0.021926, loss_ce: 0.008337
 55%|██████████████▉            | 221/400 [1:49:24<1:29:16, 29.93s/it]2022-01-12 00:01:17,685 iteration 3758 : loss : 0.029552, loss_ce: 0.013619
2022-01-12 00:01:19,319 iteration 3759 : loss : 0.030823, loss_ce: 0.012736
2022-01-12 00:01:20,948 iteration 3760 : loss : 0.052917, loss_ce: 0.024933
2022-01-12 00:01:22,545 iteration 3761 : loss : 0.030520, loss_ce: 0.013853
2022-01-12 00:01:24,183 iteration 3762 : loss : 0.043955, loss_ce: 0.019709
2022-01-12 00:01:25,821 iteration 3763 : loss : 0.025980, loss_ce: 0.011638
2022-01-12 00:01:27,371 iteration 3764 : loss : 0.035189, loss_ce: 0.009084
2022-01-12 00:01:28,866 iteration 3765 : loss : 0.030603, loss_ce: 0.012613
2022-01-12 00:01:30,398 iteration 3766 : loss : 0.026086, loss_ce: 0.009467
2022-01-12 00:01:31,982 iteration 3767 : loss : 0.028068, loss_ce: 0.009539
2022-01-12 00:01:33,640 iteration 3768 : loss : 0.023416, loss_ce: 0.008914
2022-01-12 00:01:35,232 iteration 3769 : loss : 0.025590, loss_ce: 0.009408
2022-01-12 00:01:36,876 iteration 3770 : loss : 0.023982, loss_ce: 0.010313
2022-01-12 00:01:38,494 iteration 3771 : loss : 0.030240, loss_ce: 0.015445
2022-01-12 00:01:40,128 iteration 3772 : loss : 0.040742, loss_ce: 0.013419
2022-01-12 00:01:41,729 iteration 3773 : loss : 0.022100, loss_ce: 0.007045
2022-01-12 00:01:43,339 iteration 3774 : loss : 0.028153, loss_ce: 0.013346
 56%|██████████████▉            | 222/400 [1:49:52<1:26:27, 29.15s/it]2022-01-12 00:01:45,017 iteration 3775 : loss : 0.032486, loss_ce: 0.011354
2022-01-12 00:01:46,612 iteration 3776 : loss : 0.016107, loss_ce: 0.006410
2022-01-12 00:01:48,206 iteration 3777 : loss : 0.022141, loss_ce: 0.007340
2022-01-12 00:01:49,855 iteration 3778 : loss : 0.032313, loss_ce: 0.012229
2022-01-12 00:01:51,392 iteration 3779 : loss : 0.021534, loss_ce: 0.008093
2022-01-12 00:01:53,043 iteration 3780 : loss : 0.026451, loss_ce: 0.012757
2022-01-12 00:01:54,678 iteration 3781 : loss : 0.038079, loss_ce: 0.012606
2022-01-12 00:01:56,330 iteration 3782 : loss : 0.025839, loss_ce: 0.009778
2022-01-12 00:01:57,968 iteration 3783 : loss : 0.023069, loss_ce: 0.011742
2022-01-12 00:01:59,498 iteration 3784 : loss : 0.018436, loss_ce: 0.004954
2022-01-12 00:02:01,085 iteration 3785 : loss : 0.028804, loss_ce: 0.008193
2022-01-12 00:02:02,669 iteration 3786 : loss : 0.032198, loss_ce: 0.013481
2022-01-12 00:02:04,196 iteration 3787 : loss : 0.026722, loss_ce: 0.011794
2022-01-12 00:02:05,799 iteration 3788 : loss : 0.027024, loss_ce: 0.009200
2022-01-12 00:02:07,389 iteration 3789 : loss : 0.023871, loss_ce: 0.010388
2022-01-12 00:02:08,938 iteration 3790 : loss : 0.020463, loss_ce: 0.008390
2022-01-12 00:02:10,588 iteration 3791 : loss : 0.020114, loss_ce: 0.006983
 56%|███████████████            | 223/400 [1:50:19<1:24:17, 28.58s/it]2022-01-12 00:02:12,173 iteration 3792 : loss : 0.023107, loss_ce: 0.008219
2022-01-12 00:02:13,677 iteration 3793 : loss : 0.018750, loss_ce: 0.004481
2022-01-12 00:02:15,234 iteration 3794 : loss : 0.020412, loss_ce: 0.008214
2022-01-12 00:02:16,783 iteration 3795 : loss : 0.019627, loss_ce: 0.005917
2022-01-12 00:02:18,441 iteration 3796 : loss : 0.018529, loss_ce: 0.005570
2022-01-12 00:02:20,056 iteration 3797 : loss : 0.018395, loss_ce: 0.005876
2022-01-12 00:02:21,586 iteration 3798 : loss : 0.017819, loss_ce: 0.006812
2022-01-12 00:02:23,270 iteration 3799 : loss : 0.022363, loss_ce: 0.007355
2022-01-12 00:02:25,007 iteration 3800 : loss : 0.047873, loss_ce: 0.024650
2022-01-12 00:02:26,689 iteration 3801 : loss : 0.034403, loss_ce: 0.011842
2022-01-12 00:02:28,343 iteration 3802 : loss : 0.024478, loss_ce: 0.009729
2022-01-12 00:02:29,922 iteration 3803 : loss : 0.021120, loss_ce: 0.007732
2022-01-12 00:02:31,513 iteration 3804 : loss : 0.022600, loss_ce: 0.009310
2022-01-12 00:02:33,099 iteration 3805 : loss : 0.021361, loss_ce: 0.008660
2022-01-12 00:02:34,595 iteration 3806 : loss : 0.016985, loss_ce: 0.007392
2022-01-12 00:02:36,198 iteration 3807 : loss : 0.019225, loss_ce: 0.006571
2022-01-12 00:02:37,758 iteration 3808 : loss : 0.031408, loss_ce: 0.013713
 56%|███████████████            | 224/400 [1:50:46<1:22:35, 28.15s/it]2022-01-12 00:02:39,401 iteration 3809 : loss : 0.019288, loss_ce: 0.007979
2022-01-12 00:02:41,022 iteration 3810 : loss : 0.017243, loss_ce: 0.005997
2022-01-12 00:02:42,579 iteration 3811 : loss : 0.018923, loss_ce: 0.006735
2022-01-12 00:02:44,182 iteration 3812 : loss : 0.021454, loss_ce: 0.006805
2022-01-12 00:02:45,677 iteration 3813 : loss : 0.016120, loss_ce: 0.007033
2022-01-12 00:02:47,317 iteration 3814 : loss : 0.031705, loss_ce: 0.017613
2022-01-12 00:02:49,003 iteration 3815 : loss : 0.021669, loss_ce: 0.008054
2022-01-12 00:02:50,547 iteration 3816 : loss : 0.014948, loss_ce: 0.005292
2022-01-12 00:02:52,018 iteration 3817 : loss : 0.015248, loss_ce: 0.005509
2022-01-12 00:02:53,566 iteration 3818 : loss : 0.019522, loss_ce: 0.005956
2022-01-12 00:02:55,227 iteration 3819 : loss : 0.038960, loss_ce: 0.014538
2022-01-12 00:02:56,731 iteration 3820 : loss : 0.019571, loss_ce: 0.007766
2022-01-12 00:02:58,381 iteration 3821 : loss : 0.019087, loss_ce: 0.008822
2022-01-12 00:02:59,943 iteration 3822 : loss : 0.029446, loss_ce: 0.010337
2022-01-12 00:03:01,480 iteration 3823 : loss : 0.027435, loss_ce: 0.008701
2022-01-12 00:03:03,097 iteration 3824 : loss : 0.021776, loss_ce: 0.007840
2022-01-12 00:03:03,097 Training Data Eval:
2022-01-12 00:03:11,033   Average segmentation loss on training set: 0.0134
2022-01-12 00:03:11,034 Validation Data Eval:
2022-01-12 00:03:13,762   Average segmentation loss on validation set: 0.0647
2022-01-12 00:03:15,244 iteration 3825 : loss : 0.016913, loss_ce: 0.005872
 56%|███████████████▏           | 225/400 [1:51:23<1:30:17, 30.95s/it]2022-01-12 00:03:16,938 iteration 3826 : loss : 0.027457, loss_ce: 0.012748
2022-01-12 00:03:18,592 iteration 3827 : loss : 0.033648, loss_ce: 0.013571
2022-01-12 00:03:20,128 iteration 3828 : loss : 0.017075, loss_ce: 0.008927
2022-01-12 00:03:21,661 iteration 3829 : loss : 0.015006, loss_ce: 0.006078
2022-01-12 00:03:23,329 iteration 3830 : loss : 0.027495, loss_ce: 0.007385
2022-01-12 00:03:24,976 iteration 3831 : loss : 0.022499, loss_ce: 0.008877
2022-01-12 00:03:26,586 iteration 3832 : loss : 0.029258, loss_ce: 0.014336
2022-01-12 00:03:28,193 iteration 3833 : loss : 0.027796, loss_ce: 0.009536
2022-01-12 00:03:29,726 iteration 3834 : loss : 0.017195, loss_ce: 0.005079
2022-01-12 00:03:31,335 iteration 3835 : loss : 0.044738, loss_ce: 0.009543
2022-01-12 00:03:32,853 iteration 3836 : loss : 0.016634, loss_ce: 0.006653
2022-01-12 00:03:34,338 iteration 3837 : loss : 0.016667, loss_ce: 0.007078
2022-01-12 00:03:35,964 iteration 3838 : loss : 0.024413, loss_ce: 0.011838
2022-01-12 00:03:37,490 iteration 3839 : loss : 0.024011, loss_ce: 0.007697
2022-01-12 00:03:39,085 iteration 3840 : loss : 0.019726, loss_ce: 0.007180
2022-01-12 00:03:40,580 iteration 3841 : loss : 0.017045, loss_ce: 0.006518
2022-01-12 00:03:42,195 iteration 3842 : loss : 0.053862, loss_ce: 0.012377
 56%|███████████████▎           | 226/400 [1:51:50<1:26:16, 29.75s/it]2022-01-12 00:03:43,810 iteration 3843 : loss : 0.024698, loss_ce: 0.009016
2022-01-12 00:03:45,446 iteration 3844 : loss : 0.026142, loss_ce: 0.008917
2022-01-12 00:03:46,927 iteration 3845 : loss : 0.037269, loss_ce: 0.012116
2022-01-12 00:03:48,477 iteration 3846 : loss : 0.034356, loss_ce: 0.014384
2022-01-12 00:03:50,164 iteration 3847 : loss : 0.025711, loss_ce: 0.009056
2022-01-12 00:03:51,716 iteration 3848 : loss : 0.041003, loss_ce: 0.012542
2022-01-12 00:03:53,255 iteration 3849 : loss : 0.016970, loss_ce: 0.006591
2022-01-12 00:03:54,898 iteration 3850 : loss : 0.023264, loss_ce: 0.007577
2022-01-12 00:03:56,566 iteration 3851 : loss : 0.037162, loss_ce: 0.012684
2022-01-12 00:03:58,199 iteration 3852 : loss : 0.037699, loss_ce: 0.014330
2022-01-12 00:03:59,778 iteration 3853 : loss : 0.026683, loss_ce: 0.011013
2022-01-12 00:04:01,264 iteration 3854 : loss : 0.017909, loss_ce: 0.007082
2022-01-12 00:04:02,815 iteration 3855 : loss : 0.024378, loss_ce: 0.006921
2022-01-12 00:04:04,509 iteration 3856 : loss : 0.023823, loss_ce: 0.012107
2022-01-12 00:04:06,063 iteration 3857 : loss : 0.036617, loss_ce: 0.010406
2022-01-12 00:04:07,630 iteration 3858 : loss : 0.021366, loss_ce: 0.009289
2022-01-12 00:04:09,160 iteration 3859 : loss : 0.023974, loss_ce: 0.011013
 57%|███████████████▎           | 227/400 [1:52:17<1:23:22, 28.92s/it]2022-01-12 00:04:10,732 iteration 3860 : loss : 0.017211, loss_ce: 0.008094
2022-01-12 00:04:12,275 iteration 3861 : loss : 0.020862, loss_ce: 0.008663
2022-01-12 00:04:13,801 iteration 3862 : loss : 0.022280, loss_ce: 0.006494
2022-01-12 00:04:15,391 iteration 3863 : loss : 0.023083, loss_ce: 0.010776
2022-01-12 00:04:16,881 iteration 3864 : loss : 0.018455, loss_ce: 0.006617
2022-01-12 00:04:18,481 iteration 3865 : loss : 0.022484, loss_ce: 0.009592
2022-01-12 00:04:20,047 iteration 3866 : loss : 0.018039, loss_ce: 0.007881
2022-01-12 00:04:21,665 iteration 3867 : loss : 0.033575, loss_ce: 0.015501
2022-01-12 00:04:23,171 iteration 3868 : loss : 0.027985, loss_ce: 0.010015
2022-01-12 00:04:24,825 iteration 3869 : loss : 0.024940, loss_ce: 0.006895
2022-01-12 00:04:26,376 iteration 3870 : loss : 0.029439, loss_ce: 0.010921
2022-01-12 00:04:28,001 iteration 3871 : loss : 0.028148, loss_ce: 0.010509
2022-01-12 00:04:29,572 iteration 3872 : loss : 0.023482, loss_ce: 0.007414
2022-01-12 00:04:31,128 iteration 3873 : loss : 0.025832, loss_ce: 0.006750
2022-01-12 00:04:32,763 iteration 3874 : loss : 0.021756, loss_ce: 0.007401
2022-01-12 00:04:34,375 iteration 3875 : loss : 0.023910, loss_ce: 0.007527
2022-01-12 00:04:36,054 iteration 3876 : loss : 0.036951, loss_ce: 0.015585
 57%|███████████████▍           | 228/400 [1:52:44<1:21:09, 28.31s/it]2022-01-12 00:04:37,635 iteration 3877 : loss : 0.022773, loss_ce: 0.008836
2022-01-12 00:04:39,258 iteration 3878 : loss : 0.027836, loss_ce: 0.009941
2022-01-12 00:04:40,911 iteration 3879 : loss : 0.018126, loss_ce: 0.009946
2022-01-12 00:04:42,501 iteration 3880 : loss : 0.019517, loss_ce: 0.006799
2022-01-12 00:04:44,105 iteration 3881 : loss : 0.033414, loss_ce: 0.011221
2022-01-12 00:04:45,728 iteration 3882 : loss : 0.027749, loss_ce: 0.014272
2022-01-12 00:04:47,322 iteration 3883 : loss : 0.024339, loss_ce: 0.009448
2022-01-12 00:04:48,909 iteration 3884 : loss : 0.019034, loss_ce: 0.006445
2022-01-12 00:04:50,442 iteration 3885 : loss : 0.019127, loss_ce: 0.005046
2022-01-12 00:04:52,060 iteration 3886 : loss : 0.019442, loss_ce: 0.009115
2022-01-12 00:04:53,658 iteration 3887 : loss : 0.031843, loss_ce: 0.011033
2022-01-12 00:04:55,305 iteration 3888 : loss : 0.027795, loss_ce: 0.014506
2022-01-12 00:04:56,853 iteration 3889 : loss : 0.028166, loss_ce: 0.008191
2022-01-12 00:04:58,412 iteration 3890 : loss : 0.017625, loss_ce: 0.006653
2022-01-12 00:05:00,036 iteration 3891 : loss : 0.027005, loss_ce: 0.013297
2022-01-12 00:05:01,618 iteration 3892 : loss : 0.024576, loss_ce: 0.008247
2022-01-12 00:05:03,163 iteration 3893 : loss : 0.043749, loss_ce: 0.012999
 57%|███████████████▍           | 229/400 [1:53:11<1:19:39, 27.95s/it]2022-01-12 00:05:04,786 iteration 3894 : loss : 0.035102, loss_ce: 0.016490
2022-01-12 00:05:06,409 iteration 3895 : loss : 0.020355, loss_ce: 0.009777
2022-01-12 00:05:07,931 iteration 3896 : loss : 0.016613, loss_ce: 0.006586
2022-01-12 00:05:09,573 iteration 3897 : loss : 0.021076, loss_ce: 0.006363
2022-01-12 00:05:11,186 iteration 3898 : loss : 0.031245, loss_ce: 0.011883
2022-01-12 00:05:12,784 iteration 3899 : loss : 0.018054, loss_ce: 0.005327
2022-01-12 00:05:14,387 iteration 3900 : loss : 0.019489, loss_ce: 0.008511
2022-01-12 00:05:15,873 iteration 3901 : loss : 0.017796, loss_ce: 0.008396
2022-01-12 00:05:17,451 iteration 3902 : loss : 0.017488, loss_ce: 0.005944
2022-01-12 00:05:18,974 iteration 3903 : loss : 0.018791, loss_ce: 0.005622
2022-01-12 00:05:20,564 iteration 3904 : loss : 0.015935, loss_ce: 0.006519
2022-01-12 00:05:22,152 iteration 3905 : loss : 0.019506, loss_ce: 0.010222
2022-01-12 00:05:23,733 iteration 3906 : loss : 0.016474, loss_ce: 0.005466
2022-01-12 00:05:25,309 iteration 3907 : loss : 0.023822, loss_ce: 0.007311
2022-01-12 00:05:26,789 iteration 3908 : loss : 0.027476, loss_ce: 0.009843
2022-01-12 00:05:28,279 iteration 3909 : loss : 0.018341, loss_ce: 0.007572
2022-01-12 00:05:28,279 Training Data Eval:
2022-01-12 00:05:36,191   Average segmentation loss on training set: 0.0144
2022-01-12 00:05:36,192 Validation Data Eval:
2022-01-12 00:05:38,924   Average segmentation loss on validation set: 0.0778
2022-01-12 00:05:40,496 iteration 3910 : loss : 0.020171, loss_ce: 0.008525
 57%|███████████████▌           | 230/400 [1:53:49<1:27:09, 30.76s/it]2022-01-12 00:05:42,022 iteration 3911 : loss : 0.024031, loss_ce: 0.009521
2022-01-12 00:05:43,570 iteration 3912 : loss : 0.027240, loss_ce: 0.008358
2022-01-12 00:05:45,162 iteration 3913 : loss : 0.021160, loss_ce: 0.009229
2022-01-12 00:05:46,641 iteration 3914 : loss : 0.015429, loss_ce: 0.006830
2022-01-12 00:05:48,248 iteration 3915 : loss : 0.025417, loss_ce: 0.007107
2022-01-12 00:05:49,848 iteration 3916 : loss : 0.017332, loss_ce: 0.006328
2022-01-12 00:05:51,469 iteration 3917 : loss : 0.015616, loss_ce: 0.004953
2022-01-12 00:05:53,048 iteration 3918 : loss : 0.020246, loss_ce: 0.006584
2022-01-12 00:05:54,619 iteration 3919 : loss : 0.018592, loss_ce: 0.006745
2022-01-12 00:05:56,227 iteration 3920 : loss : 0.028306, loss_ce: 0.006546
2022-01-12 00:05:57,807 iteration 3921 : loss : 0.018171, loss_ce: 0.005382
2022-01-12 00:05:59,438 iteration 3922 : loss : 0.032481, loss_ce: 0.010269
2022-01-12 00:06:01,149 iteration 3923 : loss : 0.052182, loss_ce: 0.025793
2022-01-12 00:06:02,712 iteration 3924 : loss : 0.021704, loss_ce: 0.008259
2022-01-12 00:06:04,223 iteration 3925 : loss : 0.016913, loss_ce: 0.007652
2022-01-12 00:06:05,971 iteration 3926 : loss : 0.022219, loss_ce: 0.009595
2022-01-12 00:06:07,602 iteration 3927 : loss : 0.024756, loss_ce: 0.011842
 58%|███████████████▌           | 231/400 [1:54:16<1:23:33, 29.67s/it]2022-01-12 00:06:09,219 iteration 3928 : loss : 0.018565, loss_ce: 0.007202
2022-01-12 00:06:10,770 iteration 3929 : loss : 0.020422, loss_ce: 0.011687
2022-01-12 00:06:12,353 iteration 3930 : loss : 0.021440, loss_ce: 0.006305
2022-01-12 00:06:13,870 iteration 3931 : loss : 0.017339, loss_ce: 0.003996
2022-01-12 00:06:15,485 iteration 3932 : loss : 0.021993, loss_ce: 0.008394
2022-01-12 00:06:17,063 iteration 3933 : loss : 0.028088, loss_ce: 0.013099
2022-01-12 00:06:18,689 iteration 3934 : loss : 0.033383, loss_ce: 0.012176
2022-01-12 00:06:20,258 iteration 3935 : loss : 0.021645, loss_ce: 0.008072
2022-01-12 00:06:21,790 iteration 3936 : loss : 0.017425, loss_ce: 0.005559
2022-01-12 00:06:23,378 iteration 3937 : loss : 0.023828, loss_ce: 0.007377
2022-01-12 00:06:24,957 iteration 3938 : loss : 0.019368, loss_ce: 0.005029
2022-01-12 00:06:26,622 iteration 3939 : loss : 0.031672, loss_ce: 0.007630
2022-01-12 00:06:28,216 iteration 3940 : loss : 0.022349, loss_ce: 0.010883
2022-01-12 00:06:29,810 iteration 3941 : loss : 0.017893, loss_ce: 0.005471
2022-01-12 00:06:31,360 iteration 3942 : loss : 0.017607, loss_ce: 0.009272
2022-01-12 00:06:32,898 iteration 3943 : loss : 0.025162, loss_ce: 0.009534
2022-01-12 00:06:34,467 iteration 3944 : loss : 0.024937, loss_ce: 0.007591
 58%|███████████████▋           | 232/400 [1:54:43<1:20:43, 28.83s/it]2022-01-12 00:06:36,156 iteration 3945 : loss : 0.023514, loss_ce: 0.007563
2022-01-12 00:06:37,724 iteration 3946 : loss : 0.018141, loss_ce: 0.007289
2022-01-12 00:06:39,260 iteration 3947 : loss : 0.020409, loss_ce: 0.008933
2022-01-12 00:06:40,897 iteration 3948 : loss : 0.031419, loss_ce: 0.013063
2022-01-12 00:06:42,542 iteration 3949 : loss : 0.024797, loss_ce: 0.008206
2022-01-12 00:06:44,095 iteration 3950 : loss : 0.016882, loss_ce: 0.007958
2022-01-12 00:06:45,628 iteration 3951 : loss : 0.018595, loss_ce: 0.003781
2022-01-12 00:06:47,329 iteration 3952 : loss : 0.020449, loss_ce: 0.009567
2022-01-12 00:06:48,917 iteration 3953 : loss : 0.017938, loss_ce: 0.008276
2022-01-12 00:06:50,459 iteration 3954 : loss : 0.015442, loss_ce: 0.005841
2022-01-12 00:06:52,085 iteration 3955 : loss : 0.018789, loss_ce: 0.008850
2022-01-12 00:06:53,731 iteration 3956 : loss : 0.033749, loss_ce: 0.024091
2022-01-12 00:06:55,382 iteration 3957 : loss : 0.030812, loss_ce: 0.009168
2022-01-12 00:06:57,039 iteration 3958 : loss : 0.020229, loss_ce: 0.008914
2022-01-12 00:06:58,536 iteration 3959 : loss : 0.021311, loss_ce: 0.005075
2022-01-12 00:07:00,176 iteration 3960 : loss : 0.028271, loss_ce: 0.010586
2022-01-12 00:07:01,734 iteration 3961 : loss : 0.022454, loss_ce: 0.006230
 58%|███████████████▋           | 233/400 [1:55:10<1:18:55, 28.36s/it]2022-01-12 00:07:03,386 iteration 3962 : loss : 0.030392, loss_ce: 0.011342
2022-01-12 00:07:05,057 iteration 3963 : loss : 0.024063, loss_ce: 0.009814
2022-01-12 00:07:06,586 iteration 3964 : loss : 0.016423, loss_ce: 0.004942
2022-01-12 00:07:08,194 iteration 3965 : loss : 0.016615, loss_ce: 0.007717
2022-01-12 00:07:09,879 iteration 3966 : loss : 0.019377, loss_ce: 0.009349
2022-01-12 00:07:11,469 iteration 3967 : loss : 0.024271, loss_ce: 0.005630
2022-01-12 00:07:13,059 iteration 3968 : loss : 0.015809, loss_ce: 0.005539
2022-01-12 00:07:14,598 iteration 3969 : loss : 0.017561, loss_ce: 0.005514
2022-01-12 00:07:16,187 iteration 3970 : loss : 0.025486, loss_ce: 0.005945
2022-01-12 00:07:17,778 iteration 3971 : loss : 0.020021, loss_ce: 0.005731
2022-01-12 00:07:19,376 iteration 3972 : loss : 0.025203, loss_ce: 0.009221
2022-01-12 00:07:21,040 iteration 3973 : loss : 0.022260, loss_ce: 0.008729
2022-01-12 00:07:22,694 iteration 3974 : loss : 0.021162, loss_ce: 0.009829
2022-01-12 00:07:24,291 iteration 3975 : loss : 0.022165, loss_ce: 0.006456
2022-01-12 00:07:25,869 iteration 3976 : loss : 0.018697, loss_ce: 0.006397
2022-01-12 00:07:27,487 iteration 3977 : loss : 0.027461, loss_ce: 0.012028
2022-01-12 00:07:29,097 iteration 3978 : loss : 0.019728, loss_ce: 0.008091
 58%|███████████████▊           | 234/400 [1:55:37<1:17:37, 28.06s/it]2022-01-12 00:07:30,806 iteration 3979 : loss : 0.026098, loss_ce: 0.012911
2022-01-12 00:07:32,314 iteration 3980 : loss : 0.019998, loss_ce: 0.007537
2022-01-12 00:07:33,899 iteration 3981 : loss : 0.020735, loss_ce: 0.005772
2022-01-12 00:07:35,505 iteration 3982 : loss : 0.022058, loss_ce: 0.011259
2022-01-12 00:07:37,118 iteration 3983 : loss : 0.030891, loss_ce: 0.007950
2022-01-12 00:07:38,742 iteration 3984 : loss : 0.033631, loss_ce: 0.016176
2022-01-12 00:07:40,384 iteration 3985 : loss : 0.019405, loss_ce: 0.006088
2022-01-12 00:07:42,032 iteration 3986 : loss : 0.023499, loss_ce: 0.007445
2022-01-12 00:07:43,576 iteration 3987 : loss : 0.018665, loss_ce: 0.007828
2022-01-12 00:07:45,143 iteration 3988 : loss : 0.029830, loss_ce: 0.010212
2022-01-12 00:07:46,751 iteration 3989 : loss : 0.018138, loss_ce: 0.006924
2022-01-12 00:07:48,366 iteration 3990 : loss : 0.023028, loss_ce: 0.009047
2022-01-12 00:07:49,994 iteration 3991 : loss : 0.022021, loss_ce: 0.010302
2022-01-12 00:07:51,624 iteration 3992 : loss : 0.017681, loss_ce: 0.006741
2022-01-12 00:07:53,230 iteration 3993 : loss : 0.022919, loss_ce: 0.009338
2022-01-12 00:07:54,864 iteration 3994 : loss : 0.015472, loss_ce: 0.004827
2022-01-12 00:07:54,865 Training Data Eval:
2022-01-12 00:08:02,827   Average segmentation loss on training set: 0.0125
2022-01-12 00:08:02,827 Validation Data Eval:
2022-01-12 00:08:05,571   Average segmentation loss on validation set: 0.0766
2022-01-12 00:08:07,274 iteration 3995 : loss : 0.029259, loss_ce: 0.010618
 59%|███████████████▊           | 235/400 [1:56:15<1:25:30, 31.09s/it]2022-01-12 00:08:08,898 iteration 3996 : loss : 0.024207, loss_ce: 0.010461
2022-01-12 00:08:10,431 iteration 3997 : loss : 0.027639, loss_ce: 0.014667
2022-01-12 00:08:11,949 iteration 3998 : loss : 0.018577, loss_ce: 0.006055
2022-01-12 00:08:13,602 iteration 3999 : loss : 0.019590, loss_ce: 0.005158
2022-01-12 00:08:15,203 iteration 4000 : loss : 0.015408, loss_ce: 0.006227
2022-01-12 00:08:16,821 iteration 4001 : loss : 0.015092, loss_ce: 0.007784
2022-01-12 00:08:18,311 iteration 4002 : loss : 0.018524, loss_ce: 0.007213
2022-01-12 00:08:19,953 iteration 4003 : loss : 0.032553, loss_ce: 0.010219
2022-01-12 00:08:21,594 iteration 4004 : loss : 0.023757, loss_ce: 0.010071
2022-01-12 00:08:23,209 iteration 4005 : loss : 0.017971, loss_ce: 0.007197
2022-01-12 00:08:24,740 iteration 4006 : loss : 0.021950, loss_ce: 0.006491
2022-01-12 00:08:26,342 iteration 4007 : loss : 0.023641, loss_ce: 0.007847
2022-01-12 00:08:27,909 iteration 4008 : loss : 0.016666, loss_ce: 0.007434
2022-01-12 00:08:29,499 iteration 4009 : loss : 0.012646, loss_ce: 0.004264
2022-01-12 00:08:31,113 iteration 4010 : loss : 0.019323, loss_ce: 0.007261
2022-01-12 00:08:32,684 iteration 4011 : loss : 0.021545, loss_ce: 0.007827
2022-01-12 00:08:34,291 iteration 4012 : loss : 0.023121, loss_ce: 0.010306
 59%|███████████████▉           | 236/400 [1:56:42<1:21:38, 29.87s/it]2022-01-12 00:08:35,798 iteration 4013 : loss : 0.013316, loss_ce: 0.006239
2022-01-12 00:08:37,445 iteration 4014 : loss : 0.023606, loss_ce: 0.010407
2022-01-12 00:08:38,980 iteration 4015 : loss : 0.016980, loss_ce: 0.007832
2022-01-12 00:08:40,637 iteration 4016 : loss : 0.015243, loss_ce: 0.005403
2022-01-12 00:08:42,246 iteration 4017 : loss : 0.041667, loss_ce: 0.009775
2022-01-12 00:08:43,772 iteration 4018 : loss : 0.018797, loss_ce: 0.006947
2022-01-12 00:08:45,372 iteration 4019 : loss : 0.025910, loss_ce: 0.008192
2022-01-12 00:08:46,944 iteration 4020 : loss : 0.020903, loss_ce: 0.006169
2022-01-12 00:08:48,558 iteration 4021 : loss : 0.022144, loss_ce: 0.006223
2022-01-12 00:08:50,238 iteration 4022 : loss : 0.024538, loss_ce: 0.009022
2022-01-12 00:08:51,761 iteration 4023 : loss : 0.015216, loss_ce: 0.006894
2022-01-12 00:08:53,395 iteration 4024 : loss : 0.026679, loss_ce: 0.008240
2022-01-12 00:08:55,008 iteration 4025 : loss : 0.022552, loss_ce: 0.007965
2022-01-12 00:08:56,665 iteration 4026 : loss : 0.021802, loss_ce: 0.006016
2022-01-12 00:08:58,161 iteration 4027 : loss : 0.019203, loss_ce: 0.005963
2022-01-12 00:08:59,877 iteration 4028 : loss : 0.030254, loss_ce: 0.015331
2022-01-12 00:09:01,579 iteration 4029 : loss : 0.035812, loss_ce: 0.014589
 59%|███████████████▉           | 237/400 [1:57:10<1:19:03, 29.10s/it]2022-01-12 00:09:03,196 iteration 4030 : loss : 0.023367, loss_ce: 0.008996
2022-01-12 00:09:04,771 iteration 4031 : loss : 0.023195, loss_ce: 0.010684
2022-01-12 00:09:06,272 iteration 4032 : loss : 0.016549, loss_ce: 0.005267
2022-01-12 00:09:07,814 iteration 4033 : loss : 0.017267, loss_ce: 0.006768
2022-01-12 00:09:09,437 iteration 4034 : loss : 0.030872, loss_ce: 0.012100
2022-01-12 00:09:11,095 iteration 4035 : loss : 0.036372, loss_ce: 0.010894
2022-01-12 00:09:12,702 iteration 4036 : loss : 0.020735, loss_ce: 0.008826
2022-01-12 00:09:14,246 iteration 4037 : loss : 0.015179, loss_ce: 0.007769
2022-01-12 00:09:15,829 iteration 4038 : loss : 0.018655, loss_ce: 0.007895
2022-01-12 00:09:17,431 iteration 4039 : loss : 0.020408, loss_ce: 0.006433
2022-01-12 00:09:19,001 iteration 4040 : loss : 0.021726, loss_ce: 0.006939
2022-01-12 00:09:20,662 iteration 4041 : loss : 0.028687, loss_ce: 0.011779
2022-01-12 00:09:22,287 iteration 4042 : loss : 0.017146, loss_ce: 0.006952
2022-01-12 00:09:23,900 iteration 4043 : loss : 0.022559, loss_ce: 0.009512
2022-01-12 00:09:25,484 iteration 4044 : loss : 0.020904, loss_ce: 0.008138
2022-01-12 00:09:27,079 iteration 4045 : loss : 0.020191, loss_ce: 0.005699
2022-01-12 00:09:28,721 iteration 4046 : loss : 0.019374, loss_ce: 0.006644
 60%|████████████████           | 238/400 [1:57:37<1:16:58, 28.51s/it]2022-01-12 00:09:30,379 iteration 4047 : loss : 0.032947, loss_ce: 0.012372
2022-01-12 00:09:31,965 iteration 4048 : loss : 0.023993, loss_ce: 0.007500
2022-01-12 00:09:33,542 iteration 4049 : loss : 0.016205, loss_ce: 0.004707
2022-01-12 00:09:35,073 iteration 4050 : loss : 0.025601, loss_ce: 0.009943
2022-01-12 00:09:36,646 iteration 4051 : loss : 0.016194, loss_ce: 0.006213
2022-01-12 00:09:38,188 iteration 4052 : loss : 0.021682, loss_ce: 0.008423
2022-01-12 00:09:39,802 iteration 4053 : loss : 0.026762, loss_ce: 0.006512
2022-01-12 00:09:41,433 iteration 4054 : loss : 0.018053, loss_ce: 0.006769
2022-01-12 00:09:43,047 iteration 4055 : loss : 0.018299, loss_ce: 0.008145
2022-01-12 00:09:44,645 iteration 4056 : loss : 0.027579, loss_ce: 0.012825
2022-01-12 00:09:46,171 iteration 4057 : loss : 0.018813, loss_ce: 0.005924
2022-01-12 00:09:47,708 iteration 4058 : loss : 0.017594, loss_ce: 0.006723
2022-01-12 00:09:49,335 iteration 4059 : loss : 0.021987, loss_ce: 0.007979
2022-01-12 00:09:50,896 iteration 4060 : loss : 0.014932, loss_ce: 0.005966
2022-01-12 00:09:52,381 iteration 4061 : loss : 0.016626, loss_ce: 0.007621
2022-01-12 00:09:53,940 iteration 4062 : loss : 0.015643, loss_ce: 0.007232
2022-01-12 00:09:55,571 iteration 4063 : loss : 0.021713, loss_ce: 0.007657
 60%|████████████████▏          | 239/400 [1:58:04<1:15:09, 28.01s/it]2022-01-12 00:09:57,184 iteration 4064 : loss : 0.014881, loss_ce: 0.005206
2022-01-12 00:09:58,747 iteration 4065 : loss : 0.027853, loss_ce: 0.006226
2022-01-12 00:10:00,340 iteration 4066 : loss : 0.017103, loss_ce: 0.006005
2022-01-12 00:10:01,913 iteration 4067 : loss : 0.017816, loss_ce: 0.006570
2022-01-12 00:10:03,527 iteration 4068 : loss : 0.025733, loss_ce: 0.010420
2022-01-12 00:10:05,142 iteration 4069 : loss : 0.030900, loss_ce: 0.014150
2022-01-12 00:10:06,673 iteration 4070 : loss : 0.017380, loss_ce: 0.005054
2022-01-12 00:10:08,266 iteration 4071 : loss : 0.019517, loss_ce: 0.006696
2022-01-12 00:10:09,863 iteration 4072 : loss : 0.017872, loss_ce: 0.007114
2022-01-12 00:10:11,478 iteration 4073 : loss : 0.025806, loss_ce: 0.010366
2022-01-12 00:10:13,078 iteration 4074 : loss : 0.015905, loss_ce: 0.005895
2022-01-12 00:10:14,766 iteration 4075 : loss : 0.023982, loss_ce: 0.008479
2022-01-12 00:10:16,382 iteration 4076 : loss : 0.015489, loss_ce: 0.006743
2022-01-12 00:10:18,037 iteration 4077 : loss : 0.022191, loss_ce: 0.009455
2022-01-12 00:10:19,697 iteration 4078 : loss : 0.022305, loss_ce: 0.007412
2022-01-12 00:10:21,259 iteration 4079 : loss : 0.018587, loss_ce: 0.006731
2022-01-12 00:10:21,259 Training Data Eval:
2022-01-12 00:10:29,221   Average segmentation loss on training set: 0.0134
2022-01-12 00:10:29,222 Validation Data Eval:
2022-01-12 00:10:31,961   Average segmentation loss on validation set: 0.0735
2022-01-12 00:10:33,623 iteration 4080 : loss : 0.026484, loss_ce: 0.009547
 60%|████████████████▏          | 240/400 [1:58:42<1:22:44, 31.03s/it]2022-01-12 00:10:35,214 iteration 4081 : loss : 0.020623, loss_ce: 0.008079
2022-01-12 00:10:36,792 iteration 4082 : loss : 0.020971, loss_ce: 0.008455
2022-01-12 00:10:38,375 iteration 4083 : loss : 0.014878, loss_ce: 0.006442
2022-01-12 00:10:40,001 iteration 4084 : loss : 0.034764, loss_ce: 0.009722
2022-01-12 00:10:41,683 iteration 4085 : loss : 0.023724, loss_ce: 0.007415
2022-01-12 00:10:43,353 iteration 4086 : loss : 0.022208, loss_ce: 0.010616
2022-01-12 00:10:44,948 iteration 4087 : loss : 0.020184, loss_ce: 0.006010
2022-01-12 00:10:46,535 iteration 4088 : loss : 0.022061, loss_ce: 0.008945
2022-01-12 00:10:48,155 iteration 4089 : loss : 0.018636, loss_ce: 0.007100
2022-01-12 00:10:49,792 iteration 4090 : loss : 0.014519, loss_ce: 0.005610
2022-01-12 00:10:51,366 iteration 4091 : loss : 0.021641, loss_ce: 0.006846
2022-01-12 00:10:52,965 iteration 4092 : loss : 0.015764, loss_ce: 0.006181
2022-01-12 00:10:54,550 iteration 4093 : loss : 0.016861, loss_ce: 0.006894
2022-01-12 00:10:56,096 iteration 4094 : loss : 0.016687, loss_ce: 0.005825
2022-01-12 00:10:57,670 iteration 4095 : loss : 0.025666, loss_ce: 0.009319
2022-01-12 00:10:59,261 iteration 4096 : loss : 0.016311, loss_ce: 0.006849
2022-01-12 00:11:00,859 iteration 4097 : loss : 0.029512, loss_ce: 0.013302
 60%|████████████████▎          | 241/400 [1:59:09<1:19:11, 29.89s/it]2022-01-12 00:11:02,541 iteration 4098 : loss : 0.020350, loss_ce: 0.007295
2022-01-12 00:11:04,105 iteration 4099 : loss : 0.039797, loss_ce: 0.008936
2022-01-12 00:11:05,633 iteration 4100 : loss : 0.015713, loss_ce: 0.006717
2022-01-12 00:11:07,185 iteration 4101 : loss : 0.015103, loss_ce: 0.005973
2022-01-12 00:11:08,779 iteration 4102 : loss : 0.026672, loss_ce: 0.010105
2022-01-12 00:11:10,332 iteration 4103 : loss : 0.016495, loss_ce: 0.006402
2022-01-12 00:11:11,864 iteration 4104 : loss : 0.018538, loss_ce: 0.009277
2022-01-12 00:11:13,474 iteration 4105 : loss : 0.015293, loss_ce: 0.005803
2022-01-12 00:11:15,008 iteration 4106 : loss : 0.021321, loss_ce: 0.003386
2022-01-12 00:11:16,707 iteration 4107 : loss : 0.021406, loss_ce: 0.010820
2022-01-12 00:11:18,339 iteration 4108 : loss : 0.037384, loss_ce: 0.013438
2022-01-12 00:11:19,880 iteration 4109 : loss : 0.017616, loss_ce: 0.004485
2022-01-12 00:11:21,489 iteration 4110 : loss : 0.020639, loss_ce: 0.008289
2022-01-12 00:11:23,147 iteration 4111 : loss : 0.020215, loss_ce: 0.007317
2022-01-12 00:11:24,722 iteration 4112 : loss : 0.025096, loss_ce: 0.011017
2022-01-12 00:11:26,328 iteration 4113 : loss : 0.020450, loss_ce: 0.008006
2022-01-12 00:11:28,009 iteration 4114 : loss : 0.040045, loss_ce: 0.015857
 60%|████████████████▎          | 242/400 [1:59:36<1:16:32, 29.07s/it]2022-01-12 00:11:29,614 iteration 4115 : loss : 0.018025, loss_ce: 0.004836
2022-01-12 00:11:31,146 iteration 4116 : loss : 0.018071, loss_ce: 0.008376
2022-01-12 00:11:32,877 iteration 4117 : loss : 0.025604, loss_ce: 0.007456
2022-01-12 00:11:34,611 iteration 4118 : loss : 0.029304, loss_ce: 0.013628
2022-01-12 00:11:36,251 iteration 4119 : loss : 0.025045, loss_ce: 0.012207
2022-01-12 00:11:37,864 iteration 4120 : loss : 0.036678, loss_ce: 0.006529
2022-01-12 00:11:39,475 iteration 4121 : loss : 0.018643, loss_ce: 0.008147
2022-01-12 00:11:41,042 iteration 4122 : loss : 0.021250, loss_ce: 0.006674
2022-01-12 00:11:42,707 iteration 4123 : loss : 0.024604, loss_ce: 0.009629
2022-01-12 00:11:44,238 iteration 4124 : loss : 0.021353, loss_ce: 0.007724
2022-01-12 00:11:45,889 iteration 4125 : loss : 0.023334, loss_ce: 0.006892
2022-01-12 00:11:47,495 iteration 4126 : loss : 0.019881, loss_ce: 0.010155
2022-01-12 00:11:49,137 iteration 4127 : loss : 0.021407, loss_ce: 0.008310
2022-01-12 00:11:50,687 iteration 4128 : loss : 0.017942, loss_ce: 0.008533
2022-01-12 00:11:52,338 iteration 4129 : loss : 0.024865, loss_ce: 0.009320
2022-01-12 00:11:53,923 iteration 4130 : loss : 0.017603, loss_ce: 0.009095
2022-01-12 00:11:55,425 iteration 4131 : loss : 0.012968, loss_ce: 0.003558
 61%|████████████████▍          | 243/400 [2:00:04<1:14:45, 28.57s/it]2022-01-12 00:11:57,120 iteration 4132 : loss : 0.023872, loss_ce: 0.009506
2022-01-12 00:11:58,774 iteration 4133 : loss : 0.030512, loss_ce: 0.013529
2022-01-12 00:12:00,297 iteration 4134 : loss : 0.017358, loss_ce: 0.007010
2022-01-12 00:12:01,958 iteration 4135 : loss : 0.019486, loss_ce: 0.007724
2022-01-12 00:12:03,554 iteration 4136 : loss : 0.020139, loss_ce: 0.009588
2022-01-12 00:12:05,106 iteration 4137 : loss : 0.020340, loss_ce: 0.009338
2022-01-12 00:12:06,645 iteration 4138 : loss : 0.017516, loss_ce: 0.008099
2022-01-12 00:12:08,253 iteration 4139 : loss : 0.023191, loss_ce: 0.007572
2022-01-12 00:12:09,898 iteration 4140 : loss : 0.029931, loss_ce: 0.011186
2022-01-12 00:12:11,484 iteration 4141 : loss : 0.031650, loss_ce: 0.009619
2022-01-12 00:12:13,094 iteration 4142 : loss : 0.035812, loss_ce: 0.011214
2022-01-12 00:12:14,744 iteration 4143 : loss : 0.035727, loss_ce: 0.010053
2022-01-12 00:12:16,312 iteration 4144 : loss : 0.025587, loss_ce: 0.008777
2022-01-12 00:12:17,886 iteration 4145 : loss : 0.017114, loss_ce: 0.005511
2022-01-12 00:12:19,467 iteration 4146 : loss : 0.027489, loss_ce: 0.009565
2022-01-12 00:12:21,037 iteration 4147 : loss : 0.019810, loss_ce: 0.006777
2022-01-12 00:12:22,525 iteration 4148 : loss : 0.013820, loss_ce: 0.004711
 61%|████████████████▍          | 244/400 [2:00:31<1:13:08, 28.13s/it]2022-01-12 00:12:24,285 iteration 4149 : loss : 0.045821, loss_ce: 0.017499
2022-01-12 00:12:25,801 iteration 4150 : loss : 0.021255, loss_ce: 0.006174
2022-01-12 00:12:27,421 iteration 4151 : loss : 0.020873, loss_ce: 0.010791
2022-01-12 00:12:28,916 iteration 4152 : loss : 0.016792, loss_ce: 0.006724
2022-01-12 00:12:30,549 iteration 4153 : loss : 0.027729, loss_ce: 0.008322
2022-01-12 00:12:32,168 iteration 4154 : loss : 0.020849, loss_ce: 0.008727
2022-01-12 00:12:33,834 iteration 4155 : loss : 0.027567, loss_ce: 0.009064
2022-01-12 00:12:35,335 iteration 4156 : loss : 0.016173, loss_ce: 0.005840
2022-01-12 00:12:36,946 iteration 4157 : loss : 0.026659, loss_ce: 0.010648
2022-01-12 00:12:38,539 iteration 4158 : loss : 0.019505, loss_ce: 0.007522
2022-01-12 00:12:40,167 iteration 4159 : loss : 0.018538, loss_ce: 0.006870
2022-01-12 00:12:41,759 iteration 4160 : loss : 0.025323, loss_ce: 0.011885
2022-01-12 00:12:43,347 iteration 4161 : loss : 0.018166, loss_ce: 0.007860
2022-01-12 00:12:44,899 iteration 4162 : loss : 0.015504, loss_ce: 0.005761
2022-01-12 00:12:46,534 iteration 4163 : loss : 0.034599, loss_ce: 0.009781
2022-01-12 00:12:48,165 iteration 4164 : loss : 0.030044, loss_ce: 0.012393
2022-01-12 00:12:48,166 Training Data Eval:
2022-01-12 00:12:56,110   Average segmentation loss on training set: 0.0159
2022-01-12 00:12:56,110 Validation Data Eval:
2022-01-12 00:12:58,849   Average segmentation loss on validation set: 0.0731
2022-01-12 00:13:00,453 iteration 4165 : loss : 0.026316, loss_ce: 0.009580
 61%|████████████████▌          | 245/400 [2:01:09<1:20:15, 31.07s/it]2022-01-12 00:13:02,025 iteration 4166 : loss : 0.018072, loss_ce: 0.007347
2022-01-12 00:13:03,659 iteration 4167 : loss : 0.022906, loss_ce: 0.006755
2022-01-12 00:13:05,291 iteration 4168 : loss : 0.020289, loss_ce: 0.007753
2022-01-12 00:13:06,913 iteration 4169 : loss : 0.038559, loss_ce: 0.013343
2022-01-12 00:13:08,549 iteration 4170 : loss : 0.025541, loss_ce: 0.010633
2022-01-12 00:13:10,113 iteration 4171 : loss : 0.028266, loss_ce: 0.017653
2022-01-12 00:13:11,682 iteration 4172 : loss : 0.023775, loss_ce: 0.009273
2022-01-12 00:13:13,187 iteration 4173 : loss : 0.018991, loss_ce: 0.006420
2022-01-12 00:13:14,829 iteration 4174 : loss : 0.026176, loss_ce: 0.008857
2022-01-12 00:13:16,438 iteration 4175 : loss : 0.019237, loss_ce: 0.004843
2022-01-12 00:13:18,070 iteration 4176 : loss : 0.025667, loss_ce: 0.009082
2022-01-12 00:13:19,585 iteration 4177 : loss : 0.016839, loss_ce: 0.007899
2022-01-12 00:13:21,164 iteration 4178 : loss : 0.014854, loss_ce: 0.006229
2022-01-12 00:13:22,796 iteration 4179 : loss : 0.018654, loss_ce: 0.007468
2022-01-12 00:13:24,478 iteration 4180 : loss : 0.020031, loss_ce: 0.006432
2022-01-12 00:13:26,088 iteration 4181 : loss : 0.033687, loss_ce: 0.013881
2022-01-12 00:13:27,733 iteration 4182 : loss : 0.023129, loss_ce: 0.009474
 62%|████████████████▌          | 246/400 [2:01:36<1:16:49, 29.93s/it]2022-01-12 00:13:29,348 iteration 4183 : loss : 0.023306, loss_ce: 0.006787
2022-01-12 00:13:30,914 iteration 4184 : loss : 0.016875, loss_ce: 0.005588
2022-01-12 00:13:32,540 iteration 4185 : loss : 0.021716, loss_ce: 0.007241
2022-01-12 00:13:34,105 iteration 4186 : loss : 0.018092, loss_ce: 0.006454
2022-01-12 00:13:35,652 iteration 4187 : loss : 0.018186, loss_ce: 0.007382
2022-01-12 00:13:37,240 iteration 4188 : loss : 0.018508, loss_ce: 0.007293
2022-01-12 00:13:38,747 iteration 4189 : loss : 0.016986, loss_ce: 0.005863
2022-01-12 00:13:40,432 iteration 4190 : loss : 0.053770, loss_ce: 0.020513
2022-01-12 00:13:42,071 iteration 4191 : loss : 0.017580, loss_ce: 0.004253
2022-01-12 00:13:43,598 iteration 4192 : loss : 0.018929, loss_ce: 0.009367
2022-01-12 00:13:45,112 iteration 4193 : loss : 0.016776, loss_ce: 0.005212
2022-01-12 00:13:46,692 iteration 4194 : loss : 0.028257, loss_ce: 0.009362
2022-01-12 00:13:48,233 iteration 4195 : loss : 0.016235, loss_ce: 0.006591
2022-01-12 00:13:49,775 iteration 4196 : loss : 0.014719, loss_ce: 0.005537
2022-01-12 00:13:51,308 iteration 4197 : loss : 0.019414, loss_ce: 0.007245
2022-01-12 00:13:52,821 iteration 4198 : loss : 0.020779, loss_ce: 0.005330
2022-01-12 00:13:54,448 iteration 4199 : loss : 0.019718, loss_ce: 0.008926
 62%|████████████████▋          | 247/400 [2:02:03<1:13:51, 28.97s/it]2022-01-12 00:13:56,076 iteration 4200 : loss : 0.020943, loss_ce: 0.007078
2022-01-12 00:13:57,716 iteration 4201 : loss : 0.038362, loss_ce: 0.018618
2022-01-12 00:13:59,378 iteration 4202 : loss : 0.019063, loss_ce: 0.008018
2022-01-12 00:14:01,063 iteration 4203 : loss : 0.032128, loss_ce: 0.013194
2022-01-12 00:14:02,700 iteration 4204 : loss : 0.021262, loss_ce: 0.008230
2022-01-12 00:14:04,131 iteration 4205 : loss : 0.011251, loss_ce: 0.003863
2022-01-12 00:14:05,686 iteration 4206 : loss : 0.014648, loss_ce: 0.004902
2022-01-12 00:14:07,228 iteration 4207 : loss : 0.012717, loss_ce: 0.006110
2022-01-12 00:14:08,855 iteration 4208 : loss : 0.021332, loss_ce: 0.007208
2022-01-12 00:14:10,423 iteration 4209 : loss : 0.016119, loss_ce: 0.004718
2022-01-12 00:14:11,986 iteration 4210 : loss : 0.015082, loss_ce: 0.004997
2022-01-12 00:14:13,593 iteration 4211 : loss : 0.016629, loss_ce: 0.006408
2022-01-12 00:14:15,149 iteration 4212 : loss : 0.017911, loss_ce: 0.008939
2022-01-12 00:14:16,805 iteration 4213 : loss : 0.019233, loss_ce: 0.006610
2022-01-12 00:14:18,472 iteration 4214 : loss : 0.022911, loss_ce: 0.011015
2022-01-12 00:14:20,109 iteration 4215 : loss : 0.030811, loss_ce: 0.007262
2022-01-12 00:14:21,714 iteration 4216 : loss : 0.025013, loss_ce: 0.009001
 62%|████████████████▋          | 248/400 [2:02:30<1:12:05, 28.46s/it]2022-01-12 00:14:23,427 iteration 4217 : loss : 0.041274, loss_ce: 0.010472
2022-01-12 00:14:25,117 iteration 4218 : loss : 0.022767, loss_ce: 0.010866
2022-01-12 00:14:26,725 iteration 4219 : loss : 0.016125, loss_ce: 0.006548
2022-01-12 00:14:28,320 iteration 4220 : loss : 0.019092, loss_ce: 0.005991
2022-01-12 00:14:29,942 iteration 4221 : loss : 0.021419, loss_ce: 0.007322
2022-01-12 00:14:31,591 iteration 4222 : loss : 0.019413, loss_ce: 0.009075
2022-01-12 00:14:33,335 iteration 4223 : loss : 0.054910, loss_ce: 0.014020
2022-01-12 00:14:34,928 iteration 4224 : loss : 0.023634, loss_ce: 0.007293
2022-01-12 00:14:36,581 iteration 4225 : loss : 0.023923, loss_ce: 0.011929
2022-01-12 00:14:38,276 iteration 4226 : loss : 0.017657, loss_ce: 0.007013
2022-01-12 00:14:39,858 iteration 4227 : loss : 0.024251, loss_ce: 0.007550
2022-01-12 00:14:41,482 iteration 4228 : loss : 0.019143, loss_ce: 0.008944
2022-01-12 00:14:43,019 iteration 4229 : loss : 0.021994, loss_ce: 0.009397
2022-01-12 00:14:44,669 iteration 4230 : loss : 0.019132, loss_ce: 0.006468
2022-01-12 00:14:46,296 iteration 4231 : loss : 0.018845, loss_ce: 0.006527
2022-01-12 00:14:47,949 iteration 4232 : loss : 0.031792, loss_ce: 0.011138
2022-01-12 00:14:49,494 iteration 4233 : loss : 0.018078, loss_ce: 0.006554
 62%|████████████████▊          | 249/400 [2:02:58<1:11:06, 28.25s/it]2022-01-12 00:14:51,181 iteration 4234 : loss : 0.022305, loss_ce: 0.009606
2022-01-12 00:14:52,731 iteration 4235 : loss : 0.017054, loss_ce: 0.006151
2022-01-12 00:14:54,378 iteration 4236 : loss : 0.029946, loss_ce: 0.015241
2022-01-12 00:14:55,978 iteration 4237 : loss : 0.018556, loss_ce: 0.009631
2022-01-12 00:14:57,647 iteration 4238 : loss : 0.031678, loss_ce: 0.010074
2022-01-12 00:14:59,156 iteration 4239 : loss : 0.019019, loss_ce: 0.005598
2022-01-12 00:15:00,847 iteration 4240 : loss : 0.032501, loss_ce: 0.010625
2022-01-12 00:15:02,424 iteration 4241 : loss : 0.022917, loss_ce: 0.008199
2022-01-12 00:15:03,988 iteration 4242 : loss : 0.029073, loss_ce: 0.011428
2022-01-12 00:15:05,568 iteration 4243 : loss : 0.026083, loss_ce: 0.008038
2022-01-12 00:15:07,222 iteration 4244 : loss : 0.038743, loss_ce: 0.017503
2022-01-12 00:15:08,840 iteration 4245 : loss : 0.034925, loss_ce: 0.009639
2022-01-12 00:15:10,422 iteration 4246 : loss : 0.018201, loss_ce: 0.008532
2022-01-12 00:15:11,984 iteration 4247 : loss : 0.024023, loss_ce: 0.005834
2022-01-12 00:15:13,623 iteration 4248 : loss : 0.023209, loss_ce: 0.010111
2022-01-12 00:15:15,217 iteration 4249 : loss : 0.029635, loss_ce: 0.013124
2022-01-12 00:15:15,217 Training Data Eval:
2022-01-12 00:15:23,180   Average segmentation loss on training set: 0.0156
2022-01-12 00:15:23,180 Validation Data Eval:
2022-01-12 00:15:25,925   Average segmentation loss on validation set: 0.0789
2022-01-12 00:15:27,551 iteration 4250 : loss : 0.024957, loss_ce: 0.010010
 62%|████████████████▉          | 250/400 [2:03:36<1:17:59, 31.20s/it]2022-01-12 00:15:29,259 iteration 4251 : loss : 0.021935, loss_ce: 0.009113
2022-01-12 00:15:30,893 iteration 4252 : loss : 0.022617, loss_ce: 0.008833
2022-01-12 00:15:32,514 iteration 4253 : loss : 0.024351, loss_ce: 0.009539
2022-01-12 00:15:34,203 iteration 4254 : loss : 0.022140, loss_ce: 0.009355
2022-01-12 00:15:35,702 iteration 4255 : loss : 0.014056, loss_ce: 0.005332
2022-01-12 00:15:37,284 iteration 4256 : loss : 0.021960, loss_ce: 0.005023
2022-01-12 00:15:38,970 iteration 4257 : loss : 0.023491, loss_ce: 0.008078
2022-01-12 00:15:40,528 iteration 4258 : loss : 0.016422, loss_ce: 0.006152
2022-01-12 00:15:42,131 iteration 4259 : loss : 0.038326, loss_ce: 0.015885
2022-01-12 00:15:43,734 iteration 4260 : loss : 0.015502, loss_ce: 0.004962
2022-01-12 00:15:45,344 iteration 4261 : loss : 0.019176, loss_ce: 0.007941
2022-01-12 00:15:47,038 iteration 4262 : loss : 0.017709, loss_ce: 0.007612
2022-01-12 00:15:48,643 iteration 4263 : loss : 0.018528, loss_ce: 0.007432
2022-01-12 00:15:50,169 iteration 4264 : loss : 0.029955, loss_ce: 0.010211
2022-01-12 00:15:51,749 iteration 4265 : loss : 0.031004, loss_ce: 0.015817
2022-01-12 00:15:53,335 iteration 4266 : loss : 0.023969, loss_ce: 0.009295
2022-01-12 00:15:54,922 iteration 4267 : loss : 0.017630, loss_ce: 0.008228
 63%|████████████████▉          | 251/400 [2:04:03<1:14:36, 30.05s/it]2022-01-12 00:15:56,563 iteration 4268 : loss : 0.022061, loss_ce: 0.009625
2022-01-12 00:15:58,110 iteration 4269 : loss : 0.023610, loss_ce: 0.006930
2022-01-12 00:15:59,664 iteration 4270 : loss : 0.039184, loss_ce: 0.004175
2022-01-12 00:16:01,204 iteration 4271 : loss : 0.018253, loss_ce: 0.007975
2022-01-12 00:16:02,841 iteration 4272 : loss : 0.021382, loss_ce: 0.007346
2022-01-12 00:16:04,453 iteration 4273 : loss : 0.021501, loss_ce: 0.009300
2022-01-12 00:16:06,091 iteration 4274 : loss : 0.026523, loss_ce: 0.012944
2022-01-12 00:16:07,551 iteration 4275 : loss : 0.019673, loss_ce: 0.007545
2022-01-12 00:16:09,076 iteration 4276 : loss : 0.023180, loss_ce: 0.007563
2022-01-12 00:16:10,618 iteration 4277 : loss : 0.028527, loss_ce: 0.011535
2022-01-12 00:16:12,166 iteration 4278 : loss : 0.024757, loss_ce: 0.006739
2022-01-12 00:16:13,675 iteration 4279 : loss : 0.015847, loss_ce: 0.004662
2022-01-12 00:16:15,207 iteration 4280 : loss : 0.016005, loss_ce: 0.004446
2022-01-12 00:16:16,924 iteration 4281 : loss : 0.024884, loss_ce: 0.011770
2022-01-12 00:16:18,529 iteration 4282 : loss : 0.022060, loss_ce: 0.008948
2022-01-12 00:16:20,029 iteration 4283 : loss : 0.019065, loss_ce: 0.007879
2022-01-12 00:16:21,588 iteration 4284 : loss : 0.020505, loss_ce: 0.009778
 63%|█████████████████          | 252/400 [2:04:30<1:11:36, 29.03s/it]2022-01-12 00:16:23,265 iteration 4285 : loss : 0.021796, loss_ce: 0.009200
2022-01-12 00:16:24,896 iteration 4286 : loss : 0.019464, loss_ce: 0.006145
2022-01-12 00:16:26,556 iteration 4287 : loss : 0.042816, loss_ce: 0.016852
2022-01-12 00:16:28,145 iteration 4288 : loss : 0.020448, loss_ce: 0.006020
2022-01-12 00:16:29,759 iteration 4289 : loss : 0.018280, loss_ce: 0.006147
2022-01-12 00:16:31,407 iteration 4290 : loss : 0.049471, loss_ce: 0.019358
2022-01-12 00:16:32,922 iteration 4291 : loss : 0.021646, loss_ce: 0.007286
2022-01-12 00:16:34,585 iteration 4292 : loss : 0.029437, loss_ce: 0.011599
2022-01-12 00:16:36,196 iteration 4293 : loss : 0.034294, loss_ce: 0.015793
2022-01-12 00:16:37,792 iteration 4294 : loss : 0.029173, loss_ce: 0.013718
2022-01-12 00:16:39,445 iteration 4295 : loss : 0.023322, loss_ce: 0.010794
2022-01-12 00:16:40,970 iteration 4296 : loss : 0.015301, loss_ce: 0.006251
2022-01-12 00:16:42,570 iteration 4297 : loss : 0.021312, loss_ce: 0.006940
2022-01-12 00:16:44,130 iteration 4298 : loss : 0.020043, loss_ce: 0.006699
2022-01-12 00:16:45,802 iteration 4299 : loss : 0.040697, loss_ce: 0.018628
2022-01-12 00:16:47,458 iteration 4300 : loss : 0.025217, loss_ce: 0.007652
2022-01-12 00:16:49,039 iteration 4301 : loss : 0.017098, loss_ce: 0.006799
 63%|█████████████████          | 253/400 [2:04:57<1:09:57, 28.56s/it]2022-01-12 00:16:50,692 iteration 4302 : loss : 0.015415, loss_ce: 0.006223
2022-01-12 00:16:52,342 iteration 4303 : loss : 0.018520, loss_ce: 0.006202
2022-01-12 00:16:53,874 iteration 4304 : loss : 0.015337, loss_ce: 0.005510
2022-01-12 00:16:55,501 iteration 4305 : loss : 0.023696, loss_ce: 0.006806
2022-01-12 00:16:57,055 iteration 4306 : loss : 0.020896, loss_ce: 0.006525
2022-01-12 00:16:58,623 iteration 4307 : loss : 0.017443, loss_ce: 0.006709
2022-01-12 00:17:00,287 iteration 4308 : loss : 0.041489, loss_ce: 0.017518
2022-01-12 00:17:01,850 iteration 4309 : loss : 0.017225, loss_ce: 0.005253
2022-01-12 00:17:03,472 iteration 4310 : loss : 0.035054, loss_ce: 0.016942
2022-01-12 00:17:05,073 iteration 4311 : loss : 0.016719, loss_ce: 0.007633
2022-01-12 00:17:06,788 iteration 4312 : loss : 0.030655, loss_ce: 0.010377
2022-01-12 00:17:08,393 iteration 4313 : loss : 0.019845, loss_ce: 0.006671
2022-01-12 00:17:09,979 iteration 4314 : loss : 0.020273, loss_ce: 0.006176
2022-01-12 00:17:11,533 iteration 4315 : loss : 0.021480, loss_ce: 0.010618
2022-01-12 00:17:13,185 iteration 4316 : loss : 0.024073, loss_ce: 0.008159
2022-01-12 00:17:14,745 iteration 4317 : loss : 0.023690, loss_ce: 0.007147
2022-01-12 00:17:16,320 iteration 4318 : loss : 0.015955, loss_ce: 0.007316
 64%|█████████████████▏         | 254/400 [2:05:24<1:08:33, 28.17s/it]2022-01-12 00:17:18,006 iteration 4319 : loss : 0.025267, loss_ce: 0.008554
2022-01-12 00:17:19,658 iteration 4320 : loss : 0.024840, loss_ce: 0.009908
2022-01-12 00:17:21,317 iteration 4321 : loss : 0.019133, loss_ce: 0.007813
2022-01-12 00:17:22,939 iteration 4322 : loss : 0.029105, loss_ce: 0.009809
2022-01-12 00:17:24,498 iteration 4323 : loss : 0.012452, loss_ce: 0.003880
2022-01-12 00:17:26,117 iteration 4324 : loss : 0.032419, loss_ce: 0.013705
2022-01-12 00:17:27,844 iteration 4325 : loss : 0.035595, loss_ce: 0.009834
2022-01-12 00:17:29,450 iteration 4326 : loss : 0.036029, loss_ce: 0.023764
2022-01-12 00:17:31,009 iteration 4327 : loss : 0.016222, loss_ce: 0.006373
2022-01-12 00:17:32,584 iteration 4328 : loss : 0.029722, loss_ce: 0.012098
2022-01-12 00:17:34,218 iteration 4329 : loss : 0.022777, loss_ce: 0.008545
2022-01-12 00:17:35,773 iteration 4330 : loss : 0.014688, loss_ce: 0.005472
2022-01-12 00:17:37,511 iteration 4331 : loss : 0.049788, loss_ce: 0.015278
2022-01-12 00:17:39,166 iteration 4332 : loss : 0.025868, loss_ce: 0.011708
2022-01-12 00:17:40,685 iteration 4333 : loss : 0.016282, loss_ce: 0.005090
2022-01-12 00:17:42,228 iteration 4334 : loss : 0.016164, loss_ce: 0.006614
2022-01-12 00:17:42,228 Training Data Eval:
2022-01-12 00:17:50,166   Average segmentation loss on training set: 0.0133
2022-01-12 00:17:50,167 Validation Data Eval:
2022-01-12 00:17:52,899   Average segmentation loss on validation set: 0.0591
2022-01-12 00:17:58,824 Found new lowest validation loss at iteration 4334! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_4HEADS_best_val_loss_seed100.pth
2022-01-12 00:18:00,365 iteration 4335 : loss : 0.026708, loss_ce: 0.015971
 64%|█████████████████▏         | 255/400 [2:06:09<1:19:35, 32.94s/it]2022-01-12 00:18:01,870 iteration 4336 : loss : 0.021915, loss_ce: 0.008502
2022-01-12 00:18:03,372 iteration 4337 : loss : 0.025717, loss_ce: 0.010501
2022-01-12 00:18:04,953 iteration 4338 : loss : 0.022112, loss_ce: 0.007802
2022-01-12 00:18:06,465 iteration 4339 : loss : 0.024296, loss_ce: 0.006723
2022-01-12 00:18:08,088 iteration 4340 : loss : 0.024322, loss_ce: 0.010061
2022-01-12 00:18:09,732 iteration 4341 : loss : 0.027065, loss_ce: 0.011826
2022-01-12 00:18:11,318 iteration 4342 : loss : 0.038691, loss_ce: 0.011619
2022-01-12 00:18:12,925 iteration 4343 : loss : 0.017308, loss_ce: 0.005164
2022-01-12 00:18:14,494 iteration 4344 : loss : 0.018837, loss_ce: 0.006641
2022-01-12 00:18:16,092 iteration 4345 : loss : 0.024191, loss_ce: 0.008124
2022-01-12 00:18:17,661 iteration 4346 : loss : 0.015547, loss_ce: 0.006407
2022-01-12 00:18:19,299 iteration 4347 : loss : 0.027082, loss_ce: 0.008945
2022-01-12 00:18:20,901 iteration 4348 : loss : 0.020147, loss_ce: 0.011407
2022-01-12 00:18:22,495 iteration 4349 : loss : 0.026745, loss_ce: 0.008989
2022-01-12 00:18:24,059 iteration 4350 : loss : 0.019824, loss_ce: 0.007884
2022-01-12 00:18:25,582 iteration 4351 : loss : 0.015595, loss_ce: 0.006761
2022-01-12 00:18:27,147 iteration 4352 : loss : 0.014162, loss_ce: 0.005809
 64%|█████████████████▎         | 256/400 [2:06:35<1:14:37, 31.09s/it]2022-01-12 00:18:28,810 iteration 4353 : loss : 0.015992, loss_ce: 0.005525
2022-01-12 00:18:30,501 iteration 4354 : loss : 0.023328, loss_ce: 0.006887
2022-01-12 00:18:32,043 iteration 4355 : loss : 0.017438, loss_ce: 0.006319
2022-01-12 00:18:33,684 iteration 4356 : loss : 0.020331, loss_ce: 0.009954
2022-01-12 00:18:35,287 iteration 4357 : loss : 0.019307, loss_ce: 0.008241
2022-01-12 00:18:36,853 iteration 4358 : loss : 0.018679, loss_ce: 0.007907
2022-01-12 00:18:38,424 iteration 4359 : loss : 0.020684, loss_ce: 0.008084
2022-01-12 00:18:39,997 iteration 4360 : loss : 0.025524, loss_ce: 0.009589
2022-01-12 00:18:41,650 iteration 4361 : loss : 0.032731, loss_ce: 0.014623
2022-01-12 00:18:43,214 iteration 4362 : loss : 0.014998, loss_ce: 0.004574
2022-01-12 00:18:44,724 iteration 4363 : loss : 0.017512, loss_ce: 0.008110
2022-01-12 00:18:46,383 iteration 4364 : loss : 0.024827, loss_ce: 0.012168
2022-01-12 00:18:47,977 iteration 4365 : loss : 0.023575, loss_ce: 0.010584
2022-01-12 00:18:49,450 iteration 4366 : loss : 0.014837, loss_ce: 0.006872
2022-01-12 00:18:51,044 iteration 4367 : loss : 0.023683, loss_ce: 0.009125
2022-01-12 00:18:52,711 iteration 4368 : loss : 0.021109, loss_ce: 0.008425
2022-01-12 00:18:54,236 iteration 4369 : loss : 0.019970, loss_ce: 0.005271
 64%|█████████████████▎         | 257/400 [2:07:02<1:11:14, 29.89s/it]2022-01-12 00:18:55,894 iteration 4370 : loss : 0.041268, loss_ce: 0.008979
2022-01-12 00:18:57,493 iteration 4371 : loss : 0.019993, loss_ce: 0.009575
2022-01-12 00:18:59,010 iteration 4372 : loss : 0.017373, loss_ce: 0.008433
2022-01-12 00:19:00,637 iteration 4373 : loss : 0.017929, loss_ce: 0.005368
2022-01-12 00:19:02,211 iteration 4374 : loss : 0.018050, loss_ce: 0.008481
2022-01-12 00:19:03,722 iteration 4375 : loss : 0.018293, loss_ce: 0.008031
2022-01-12 00:19:05,285 iteration 4376 : loss : 0.017098, loss_ce: 0.005754
2022-01-12 00:19:06,984 iteration 4377 : loss : 0.047639, loss_ce: 0.017208
2022-01-12 00:19:08,614 iteration 4378 : loss : 0.024380, loss_ce: 0.010567
2022-01-12 00:19:10,330 iteration 4379 : loss : 0.035155, loss_ce: 0.015284
2022-01-12 00:19:11,964 iteration 4380 : loss : 0.015255, loss_ce: 0.007688
2022-01-12 00:19:13,617 iteration 4381 : loss : 0.027888, loss_ce: 0.010794
2022-01-12 00:19:15,194 iteration 4382 : loss : 0.021913, loss_ce: 0.007740
2022-01-12 00:19:16,778 iteration 4383 : loss : 0.027507, loss_ce: 0.006682
2022-01-12 00:19:18,392 iteration 4384 : loss : 0.021966, loss_ce: 0.008584
2022-01-12 00:19:19,889 iteration 4385 : loss : 0.015317, loss_ce: 0.006775
2022-01-12 00:19:21,489 iteration 4386 : loss : 0.018444, loss_ce: 0.007695
 64%|█████████████████▍         | 258/400 [2:07:30<1:08:52, 29.10s/it]2022-01-12 00:19:23,106 iteration 4387 : loss : 0.015937, loss_ce: 0.006353
2022-01-12 00:19:24,736 iteration 4388 : loss : 0.018957, loss_ce: 0.007939
2022-01-12 00:19:26,257 iteration 4389 : loss : 0.013292, loss_ce: 0.005841
2022-01-12 00:19:27,785 iteration 4390 : loss : 0.022963, loss_ce: 0.008838
2022-01-12 00:19:29,369 iteration 4391 : loss : 0.021283, loss_ce: 0.009679
2022-01-12 00:19:30,985 iteration 4392 : loss : 0.022465, loss_ce: 0.010289
2022-01-12 00:19:32,487 iteration 4393 : loss : 0.018539, loss_ce: 0.006200
2022-01-12 00:19:34,154 iteration 4394 : loss : 0.018557, loss_ce: 0.009094
2022-01-12 00:19:35,799 iteration 4395 : loss : 0.020940, loss_ce: 0.007451
2022-01-12 00:19:37,464 iteration 4396 : loss : 0.020635, loss_ce: 0.007519
2022-01-12 00:19:39,047 iteration 4397 : loss : 0.024755, loss_ce: 0.007758
2022-01-12 00:19:40,610 iteration 4398 : loss : 0.034673, loss_ce: 0.010639
2022-01-12 00:19:42,198 iteration 4399 : loss : 0.016451, loss_ce: 0.005947
2022-01-12 00:19:43,754 iteration 4400 : loss : 0.013557, loss_ce: 0.005673
2022-01-12 00:19:45,309 iteration 4401 : loss : 0.021318, loss_ce: 0.010027
2022-01-12 00:19:46,844 iteration 4402 : loss : 0.014281, loss_ce: 0.004642
2022-01-12 00:19:48,450 iteration 4403 : loss : 0.023048, loss_ce: 0.006427
 65%|█████████████████▍         | 259/400 [2:07:57<1:06:52, 28.46s/it]2022-01-12 00:19:50,026 iteration 4404 : loss : 0.013596, loss_ce: 0.006442
2022-01-12 00:19:51,666 iteration 4405 : loss : 0.014265, loss_ce: 0.003913
2022-01-12 00:19:53,289 iteration 4406 : loss : 0.022391, loss_ce: 0.008172
2022-01-12 00:19:54,821 iteration 4407 : loss : 0.016348, loss_ce: 0.005788
2022-01-12 00:19:56,326 iteration 4408 : loss : 0.019697, loss_ce: 0.005205
2022-01-12 00:19:57,917 iteration 4409 : loss : 0.014039, loss_ce: 0.006582
2022-01-12 00:19:59,508 iteration 4410 : loss : 0.020734, loss_ce: 0.007262
2022-01-12 00:20:01,167 iteration 4411 : loss : 0.019440, loss_ce: 0.009675
2022-01-12 00:20:02,694 iteration 4412 : loss : 0.017354, loss_ce: 0.007632
2022-01-12 00:20:04,346 iteration 4413 : loss : 0.024756, loss_ce: 0.007538
2022-01-12 00:20:05,836 iteration 4414 : loss : 0.020133, loss_ce: 0.007079
2022-01-12 00:20:07,439 iteration 4415 : loss : 0.019622, loss_ce: 0.008463
2022-01-12 00:20:08,990 iteration 4416 : loss : 0.018226, loss_ce: 0.004510
2022-01-12 00:20:10,568 iteration 4417 : loss : 0.021444, loss_ce: 0.007137
2022-01-12 00:20:12,170 iteration 4418 : loss : 0.023478, loss_ce: 0.010917
2022-01-12 00:20:13,788 iteration 4419 : loss : 0.026212, loss_ce: 0.010832
2022-01-12 00:20:13,788 Training Data Eval:
2022-01-12 00:20:21,735   Average segmentation loss on training set: 0.0109
2022-01-12 00:20:21,735 Validation Data Eval:
2022-01-12 00:20:24,470   Average segmentation loss on validation set: 0.0623
2022-01-12 00:20:25,994 iteration 4420 : loss : 0.012039, loss_ce: 0.004986
 65%|█████████████████▌         | 260/400 [2:08:34<1:12:45, 31.18s/it]2022-01-12 00:20:27,659 iteration 4421 : loss : 0.018138, loss_ce: 0.006826
2022-01-12 00:20:29,309 iteration 4422 : loss : 0.025379, loss_ce: 0.009652
2022-01-12 00:20:31,022 iteration 4423 : loss : 0.018979, loss_ce: 0.008005
2022-01-12 00:20:32,689 iteration 4424 : loss : 0.022554, loss_ce: 0.008122
2022-01-12 00:20:34,219 iteration 4425 : loss : 0.014077, loss_ce: 0.005103
2022-01-12 00:20:35,878 iteration 4426 : loss : 0.019629, loss_ce: 0.009049
2022-01-12 00:20:37,515 iteration 4427 : loss : 0.025235, loss_ce: 0.005939
2022-01-12 00:20:39,162 iteration 4428 : loss : 0.024279, loss_ce: 0.008193
2022-01-12 00:20:40,837 iteration 4429 : loss : 0.035429, loss_ce: 0.013087
2022-01-12 00:20:42,411 iteration 4430 : loss : 0.021274, loss_ce: 0.007777
2022-01-12 00:20:43,986 iteration 4431 : loss : 0.019705, loss_ce: 0.007778
2022-01-12 00:20:45,561 iteration 4432 : loss : 0.015257, loss_ce: 0.005726
2022-01-12 00:20:47,137 iteration 4433 : loss : 0.021642, loss_ce: 0.007625
2022-01-12 00:20:48,711 iteration 4434 : loss : 0.017232, loss_ce: 0.006905
2022-01-12 00:20:50,336 iteration 4435 : loss : 0.020736, loss_ce: 0.007128
2022-01-12 00:20:52,062 iteration 4436 : loss : 0.036054, loss_ce: 0.018152
2022-01-12 00:20:53,616 iteration 4437 : loss : 0.017303, loss_ce: 0.006498
 65%|█████████████████▌         | 261/400 [2:09:02<1:09:45, 30.11s/it]2022-01-12 00:20:55,317 iteration 4438 : loss : 0.020917, loss_ce: 0.007023
2022-01-12 00:20:56,866 iteration 4439 : loss : 0.025040, loss_ce: 0.008726
2022-01-12 00:20:58,505 iteration 4440 : loss : 0.024495, loss_ce: 0.009030
2022-01-12 00:21:00,137 iteration 4441 : loss : 0.019211, loss_ce: 0.008196
2022-01-12 00:21:01,680 iteration 4442 : loss : 0.013180, loss_ce: 0.005790
2022-01-12 00:21:03,278 iteration 4443 : loss : 0.022388, loss_ce: 0.009513
2022-01-12 00:21:04,826 iteration 4444 : loss : 0.023505, loss_ce: 0.007192
2022-01-12 00:21:06,437 iteration 4445 : loss : 0.019496, loss_ce: 0.007672
2022-01-12 00:21:08,068 iteration 4446 : loss : 0.021656, loss_ce: 0.008049
2022-01-12 00:21:09,689 iteration 4447 : loss : 0.021817, loss_ce: 0.008398
2022-01-12 00:21:11,268 iteration 4448 : loss : 0.038479, loss_ce: 0.008706
2022-01-12 00:21:12,873 iteration 4449 : loss : 0.018027, loss_ce: 0.008134
2022-01-12 00:21:14,530 iteration 4450 : loss : 0.016263, loss_ce: 0.005176
2022-01-12 00:21:16,053 iteration 4451 : loss : 0.015526, loss_ce: 0.005406
2022-01-12 00:21:17,655 iteration 4452 : loss : 0.023427, loss_ce: 0.010399
2022-01-12 00:21:19,348 iteration 4453 : loss : 0.024477, loss_ce: 0.011595
2022-01-12 00:21:20,982 iteration 4454 : loss : 0.023039, loss_ce: 0.011061
 66%|█████████████████▋         | 262/400 [2:09:29<1:07:22, 29.29s/it]2022-01-12 00:21:22,697 iteration 4455 : loss : 0.018193, loss_ce: 0.006772
2022-01-12 00:21:24,299 iteration 4456 : loss : 0.030011, loss_ce: 0.013857
2022-01-12 00:21:25,808 iteration 4457 : loss : 0.012647, loss_ce: 0.005958
2022-01-12 00:21:27,430 iteration 4458 : loss : 0.019011, loss_ce: 0.008380
2022-01-12 00:21:28,957 iteration 4459 : loss : 0.021441, loss_ce: 0.008473
2022-01-12 00:21:30,548 iteration 4460 : loss : 0.016452, loss_ce: 0.006567
2022-01-12 00:21:32,082 iteration 4461 : loss : 0.018323, loss_ce: 0.008513
2022-01-12 00:21:33,630 iteration 4462 : loss : 0.024105, loss_ce: 0.009665
2022-01-12 00:21:35,167 iteration 4463 : loss : 0.015673, loss_ce: 0.005799
2022-01-12 00:21:36,696 iteration 4464 : loss : 0.016855, loss_ce: 0.006265
2022-01-12 00:21:38,318 iteration 4465 : loss : 0.018737, loss_ce: 0.008060
2022-01-12 00:21:39,941 iteration 4466 : loss : 0.025882, loss_ce: 0.008964
2022-01-12 00:21:41,544 iteration 4467 : loss : 0.018703, loss_ce: 0.007204
2022-01-12 00:21:43,117 iteration 4468 : loss : 0.015947, loss_ce: 0.005582
2022-01-12 00:21:44,636 iteration 4469 : loss : 0.017480, loss_ce: 0.004932
2022-01-12 00:21:46,247 iteration 4470 : loss : 0.027293, loss_ce: 0.008932
2022-01-12 00:21:47,894 iteration 4471 : loss : 0.019560, loss_ce: 0.006589
 66%|█████████████████▊         | 263/400 [2:09:56<1:05:14, 28.58s/it]2022-01-12 00:21:49,487 iteration 4472 : loss : 0.017023, loss_ce: 0.007568
2022-01-12 00:21:51,059 iteration 4473 : loss : 0.017029, loss_ce: 0.006622
2022-01-12 00:21:52,672 iteration 4474 : loss : 0.014892, loss_ce: 0.005361
2022-01-12 00:21:54,218 iteration 4475 : loss : 0.015458, loss_ce: 0.006329
2022-01-12 00:21:55,709 iteration 4476 : loss : 0.012672, loss_ce: 0.004240
2022-01-12 00:21:57,296 iteration 4477 : loss : 0.018822, loss_ce: 0.007093
2022-01-12 00:21:58,899 iteration 4478 : loss : 0.019777, loss_ce: 0.006215
2022-01-12 00:22:00,498 iteration 4479 : loss : 0.041899, loss_ce: 0.006495
2022-01-12 00:22:02,025 iteration 4480 : loss : 0.014077, loss_ce: 0.006270
2022-01-12 00:22:03,554 iteration 4481 : loss : 0.012335, loss_ce: 0.005064
2022-01-12 00:22:05,206 iteration 4482 : loss : 0.022583, loss_ce: 0.008130
2022-01-12 00:22:06,902 iteration 4483 : loss : 0.026365, loss_ce: 0.009518
2022-01-12 00:22:08,571 iteration 4484 : loss : 0.023500, loss_ce: 0.007664
2022-01-12 00:22:10,131 iteration 4485 : loss : 0.014431, loss_ce: 0.004297
2022-01-12 00:22:11,844 iteration 4486 : loss : 0.019817, loss_ce: 0.008153
2022-01-12 00:22:13,398 iteration 4487 : loss : 0.018261, loss_ce: 0.008392
2022-01-12 00:22:14,977 iteration 4488 : loss : 0.019029, loss_ce: 0.008647
 66%|█████████████████▊         | 264/400 [2:10:23<1:03:45, 28.13s/it]2022-01-12 00:22:16,650 iteration 4489 : loss : 0.025539, loss_ce: 0.009627
2022-01-12 00:22:18,253 iteration 4490 : loss : 0.014279, loss_ce: 0.005075
2022-01-12 00:22:19,921 iteration 4491 : loss : 0.039735, loss_ce: 0.013994
2022-01-12 00:22:21,546 iteration 4492 : loss : 0.019155, loss_ce: 0.006863
2022-01-12 00:22:23,226 iteration 4493 : loss : 0.029668, loss_ce: 0.009147
2022-01-12 00:22:24,720 iteration 4494 : loss : 0.019218, loss_ce: 0.007171
2022-01-12 00:22:26,348 iteration 4495 : loss : 0.018105, loss_ce: 0.008390
2022-01-12 00:22:27,989 iteration 4496 : loss : 0.022338, loss_ce: 0.007470
2022-01-12 00:22:29,517 iteration 4497 : loss : 0.019240, loss_ce: 0.005564
2022-01-12 00:22:31,199 iteration 4498 : loss : 0.018842, loss_ce: 0.007600
2022-01-12 00:22:32,931 iteration 4499 : loss : 0.046989, loss_ce: 0.023716
2022-01-12 00:22:34,521 iteration 4500 : loss : 0.014593, loss_ce: 0.006091
2022-01-12 00:22:36,075 iteration 4501 : loss : 0.023929, loss_ce: 0.008402
2022-01-12 00:22:37,606 iteration 4502 : loss : 0.016922, loss_ce: 0.006375
2022-01-12 00:22:39,274 iteration 4503 : loss : 0.017426, loss_ce: 0.007086
2022-01-12 00:22:40,862 iteration 4504 : loss : 0.020065, loss_ce: 0.006154
2022-01-12 00:22:40,863 Training Data Eval:
2022-01-12 00:22:48,820   Average segmentation loss on training set: 0.0118
2022-01-12 00:22:48,820 Validation Data Eval:
2022-01-12 00:22:51,562   Average segmentation loss on validation set: 0.0820
2022-01-12 00:22:53,068 iteration 4505 : loss : 0.020785, loss_ce: 0.011492
 66%|█████████████████▉         | 265/400 [2:11:01<1:10:00, 31.12s/it]2022-01-12 00:22:54,742 iteration 4506 : loss : 0.027068, loss_ce: 0.008345
2022-01-12 00:22:56,290 iteration 4507 : loss : 0.012328, loss_ce: 0.003900
2022-01-12 00:22:57,931 iteration 4508 : loss : 0.024787, loss_ce: 0.007297
2022-01-12 00:22:59,550 iteration 4509 : loss : 0.024662, loss_ce: 0.007386
2022-01-12 00:23:01,150 iteration 4510 : loss : 0.015755, loss_ce: 0.005904
2022-01-12 00:23:02,707 iteration 4511 : loss : 0.019302, loss_ce: 0.007691
2022-01-12 00:23:04,290 iteration 4512 : loss : 0.011960, loss_ce: 0.004757
2022-01-12 00:23:05,901 iteration 4513 : loss : 0.017383, loss_ce: 0.008516
2022-01-12 00:23:07,429 iteration 4514 : loss : 0.014762, loss_ce: 0.005450
2022-01-12 00:23:08,996 iteration 4515 : loss : 0.022037, loss_ce: 0.007413
2022-01-12 00:23:10,715 iteration 4516 : loss : 0.027250, loss_ce: 0.011140
2022-01-12 00:23:12,295 iteration 4517 : loss : 0.018498, loss_ce: 0.006720
2022-01-12 00:23:13,921 iteration 4518 : loss : 0.021817, loss_ce: 0.007442
2022-01-12 00:23:15,515 iteration 4519 : loss : 0.019280, loss_ce: 0.008856
2022-01-12 00:23:17,068 iteration 4520 : loss : 0.016343, loss_ce: 0.005884
2022-01-12 00:23:18,679 iteration 4521 : loss : 0.021058, loss_ce: 0.007207
2022-01-12 00:23:20,205 iteration 4522 : loss : 0.028431, loss_ce: 0.013482
 66%|█████████████████▉         | 266/400 [2:11:28<1:06:49, 29.92s/it]2022-01-12 00:23:21,913 iteration 4523 : loss : 0.015381, loss_ce: 0.005702
2022-01-12 00:23:23,420 iteration 4524 : loss : 0.014378, loss_ce: 0.006508
2022-01-12 00:23:25,075 iteration 4525 : loss : 0.027998, loss_ce: 0.008067
2022-01-12 00:23:26,691 iteration 4526 : loss : 0.018496, loss_ce: 0.008210
2022-01-12 00:23:28,292 iteration 4527 : loss : 0.016676, loss_ce: 0.007196
2022-01-12 00:23:29,941 iteration 4528 : loss : 0.019606, loss_ce: 0.007801
2022-01-12 00:23:31,515 iteration 4529 : loss : 0.017588, loss_ce: 0.006940
2022-01-12 00:23:33,141 iteration 4530 : loss : 0.020655, loss_ce: 0.008514
2022-01-12 00:23:34,691 iteration 4531 : loss : 0.015382, loss_ce: 0.006502
2022-01-12 00:23:36,336 iteration 4532 : loss : 0.030970, loss_ce: 0.012825
2022-01-12 00:23:38,020 iteration 4533 : loss : 0.019977, loss_ce: 0.007295
2022-01-12 00:23:39,755 iteration 4534 : loss : 0.016700, loss_ce: 0.005285
2022-01-12 00:23:41,277 iteration 4535 : loss : 0.015319, loss_ce: 0.004818
2022-01-12 00:23:42,901 iteration 4536 : loss : 0.014289, loss_ce: 0.006440
2022-01-12 00:23:44,592 iteration 4537 : loss : 0.020083, loss_ce: 0.006388
2022-01-12 00:23:46,167 iteration 4538 : loss : 0.015940, loss_ce: 0.005689
2022-01-12 00:23:47,802 iteration 4539 : loss : 0.019366, loss_ce: 0.007919
 67%|██████████████████         | 267/400 [2:11:56<1:04:46, 29.22s/it]2022-01-12 00:23:49,492 iteration 4540 : loss : 0.017174, loss_ce: 0.004945
2022-01-12 00:23:51,016 iteration 4541 : loss : 0.014566, loss_ce: 0.005548
2022-01-12 00:23:52,623 iteration 4542 : loss : 0.018907, loss_ce: 0.003876
2022-01-12 00:23:54,116 iteration 4543 : loss : 0.013540, loss_ce: 0.005018
2022-01-12 00:23:55,751 iteration 4544 : loss : 0.022585, loss_ce: 0.010891
2022-01-12 00:23:57,436 iteration 4545 : loss : 0.033349, loss_ce: 0.015591
2022-01-12 00:23:59,055 iteration 4546 : loss : 0.018001, loss_ce: 0.006519
2022-01-12 00:24:00,684 iteration 4547 : loss : 0.016066, loss_ce: 0.006985
2022-01-12 00:24:02,288 iteration 4548 : loss : 0.025893, loss_ce: 0.011197
2022-01-12 00:24:03,966 iteration 4549 : loss : 0.021291, loss_ce: 0.009662
2022-01-12 00:24:05,635 iteration 4550 : loss : 0.028692, loss_ce: 0.007711
2022-01-12 00:24:07,301 iteration 4551 : loss : 0.022699, loss_ce: 0.005422
2022-01-12 00:24:08,811 iteration 4552 : loss : 0.012549, loss_ce: 0.005096
2022-01-12 00:24:10,337 iteration 4553 : loss : 0.019699, loss_ce: 0.007931
2022-01-12 00:24:11,898 iteration 4554 : loss : 0.016253, loss_ce: 0.006015
2022-01-12 00:24:13,556 iteration 4555 : loss : 0.026178, loss_ce: 0.008459
2022-01-12 00:24:15,151 iteration 4556 : loss : 0.019739, loss_ce: 0.009319
 67%|██████████████████         | 268/400 [2:12:23<1:03:03, 28.66s/it]2022-01-12 00:24:16,815 iteration 4557 : loss : 0.025620, loss_ce: 0.013587
2022-01-12 00:24:18,481 iteration 4558 : loss : 0.023352, loss_ce: 0.010313
2022-01-12 00:24:20,065 iteration 4559 : loss : 0.030440, loss_ce: 0.006925
2022-01-12 00:24:21,628 iteration 4560 : loss : 0.015064, loss_ce: 0.004396
2022-01-12 00:24:23,227 iteration 4561 : loss : 0.020819, loss_ce: 0.005011
2022-01-12 00:24:24,808 iteration 4562 : loss : 0.022085, loss_ce: 0.008564
2022-01-12 00:24:26,345 iteration 4563 : loss : 0.016664, loss_ce: 0.005336
2022-01-12 00:24:27,945 iteration 4564 : loss : 0.026623, loss_ce: 0.008731
2022-01-12 00:24:29,495 iteration 4565 : loss : 0.019386, loss_ce: 0.009012
2022-01-12 00:24:31,012 iteration 4566 : loss : 0.019841, loss_ce: 0.010554
2022-01-12 00:24:32,608 iteration 4567 : loss : 0.028233, loss_ce: 0.013566
2022-01-12 00:24:34,182 iteration 4568 : loss : 0.020373, loss_ce: 0.005863
2022-01-12 00:24:35,779 iteration 4569 : loss : 0.018714, loss_ce: 0.007346
2022-01-12 00:24:37,324 iteration 4570 : loss : 0.017237, loss_ce: 0.007089
2022-01-12 00:24:38,883 iteration 4571 : loss : 0.019755, loss_ce: 0.009072
2022-01-12 00:24:40,464 iteration 4572 : loss : 0.014963, loss_ce: 0.004926
2022-01-12 00:24:42,053 iteration 4573 : loss : 0.017238, loss_ce: 0.007421
 67%|██████████████████▏        | 269/400 [2:12:50<1:01:25, 28.14s/it]2022-01-12 00:24:43,676 iteration 4574 : loss : 0.020408, loss_ce: 0.006736
2022-01-12 00:24:45,228 iteration 4575 : loss : 0.017278, loss_ce: 0.007493
2022-01-12 00:24:46,849 iteration 4576 : loss : 0.014636, loss_ce: 0.007409
2022-01-12 00:24:48,500 iteration 4577 : loss : 0.026252, loss_ce: 0.009536
2022-01-12 00:24:50,107 iteration 4578 : loss : 0.016335, loss_ce: 0.006658
2022-01-12 00:24:51,684 iteration 4579 : loss : 0.018188, loss_ce: 0.007858
2022-01-12 00:24:53,264 iteration 4580 : loss : 0.016096, loss_ce: 0.006749
2022-01-12 00:24:54,966 iteration 4581 : loss : 0.021755, loss_ce: 0.007883
2022-01-12 00:24:56,564 iteration 4582 : loss : 0.026726, loss_ce: 0.011106
2022-01-12 00:24:58,173 iteration 4583 : loss : 0.021416, loss_ce: 0.007689
2022-01-12 00:24:59,724 iteration 4584 : loss : 0.025672, loss_ce: 0.009255
2022-01-12 00:25:01,342 iteration 4585 : loss : 0.023173, loss_ce: 0.007036
2022-01-12 00:25:02,898 iteration 4586 : loss : 0.014880, loss_ce: 0.006369
2022-01-12 00:25:04,594 iteration 4587 : loss : 0.023445, loss_ce: 0.008185
2022-01-12 00:25:06,147 iteration 4588 : loss : 0.014705, loss_ce: 0.003609
2022-01-12 00:25:07,733 iteration 4589 : loss : 0.018607, loss_ce: 0.008070
2022-01-12 00:25:07,733 Training Data Eval:
2022-01-12 00:25:15,702   Average segmentation loss on training set: 0.0121
2022-01-12 00:25:15,703 Validation Data Eval:
2022-01-12 00:25:18,450   Average segmentation loss on validation set: 0.0821
2022-01-12 00:25:20,167 iteration 4590 : loss : 0.022300, loss_ce: 0.008516
 68%|██████████████████▏        | 270/400 [2:13:28<1:07:26, 31.13s/it]2022-01-12 00:25:21,740 iteration 4591 : loss : 0.016625, loss_ce: 0.005036
2022-01-12 00:25:23,336 iteration 4592 : loss : 0.022348, loss_ce: 0.006314
2022-01-12 00:25:24,836 iteration 4593 : loss : 0.019911, loss_ce: 0.007045
2022-01-12 00:25:26,391 iteration 4594 : loss : 0.016607, loss_ce: 0.006571
2022-01-12 00:25:28,017 iteration 4595 : loss : 0.023305, loss_ce: 0.004706
2022-01-12 00:25:29,700 iteration 4596 : loss : 0.022664, loss_ce: 0.007567
2022-01-12 00:25:31,281 iteration 4597 : loss : 0.016216, loss_ce: 0.006999
2022-01-12 00:25:32,774 iteration 4598 : loss : 0.016392, loss_ce: 0.007343
2022-01-12 00:25:34,321 iteration 4599 : loss : 0.020264, loss_ce: 0.005406
2022-01-12 00:25:35,967 iteration 4600 : loss : 0.015631, loss_ce: 0.007644
2022-01-12 00:25:37,676 iteration 4601 : loss : 0.021370, loss_ce: 0.010648
2022-01-12 00:25:39,175 iteration 4602 : loss : 0.015281, loss_ce: 0.005758
2022-01-12 00:25:40,693 iteration 4603 : loss : 0.014016, loss_ce: 0.006302
2022-01-12 00:25:42,313 iteration 4604 : loss : 0.029849, loss_ce: 0.010021
2022-01-12 00:25:43,918 iteration 4605 : loss : 0.021735, loss_ce: 0.005206
2022-01-12 00:25:45,566 iteration 4606 : loss : 0.022865, loss_ce: 0.006616
2022-01-12 00:25:47,086 iteration 4607 : loss : 0.014741, loss_ce: 0.008185
 68%|██████████████████▎        | 271/400 [2:13:55<1:04:12, 29.87s/it]2022-01-12 00:25:48,747 iteration 4608 : loss : 0.020054, loss_ce: 0.006024
2022-01-12 00:25:50,415 iteration 4609 : loss : 0.025114, loss_ce: 0.012228
2022-01-12 00:25:52,054 iteration 4610 : loss : 0.023918, loss_ce: 0.006404
2022-01-12 00:25:53,641 iteration 4611 : loss : 0.017868, loss_ce: 0.007356
2022-01-12 00:25:55,144 iteration 4612 : loss : 0.014815, loss_ce: 0.005494
2022-01-12 00:25:56,768 iteration 4613 : loss : 0.016347, loss_ce: 0.006726
2022-01-12 00:25:58,396 iteration 4614 : loss : 0.021158, loss_ce: 0.008989
2022-01-12 00:26:00,032 iteration 4615 : loss : 0.025861, loss_ce: 0.010335
2022-01-12 00:26:01,700 iteration 4616 : loss : 0.020195, loss_ce: 0.008700
2022-01-12 00:26:03,232 iteration 4617 : loss : 0.018741, loss_ce: 0.007778
2022-01-12 00:26:04,890 iteration 4618 : loss : 0.032601, loss_ce: 0.012279
2022-01-12 00:26:06,542 iteration 4619 : loss : 0.022789, loss_ce: 0.007634
2022-01-12 00:26:08,188 iteration 4620 : loss : 0.017968, loss_ce: 0.005944
2022-01-12 00:26:09,852 iteration 4621 : loss : 0.037802, loss_ce: 0.014519
2022-01-12 00:26:11,451 iteration 4622 : loss : 0.016978, loss_ce: 0.005421
2022-01-12 00:26:13,021 iteration 4623 : loss : 0.018346, loss_ce: 0.006436
2022-01-12 00:26:14,607 iteration 4624 : loss : 0.018391, loss_ce: 0.007367
 68%|██████████████████▎        | 272/400 [2:14:23<1:02:12, 29.16s/it]2022-01-12 00:26:16,239 iteration 4625 : loss : 0.014473, loss_ce: 0.005976
2022-01-12 00:26:17,787 iteration 4626 : loss : 0.023316, loss_ce: 0.007493
2022-01-12 00:26:19,387 iteration 4627 : loss : 0.022602, loss_ce: 0.006426
2022-01-12 00:26:20,929 iteration 4628 : loss : 0.017294, loss_ce: 0.006470
2022-01-12 00:26:22,477 iteration 4629 : loss : 0.018034, loss_ce: 0.008192
2022-01-12 00:26:24,034 iteration 4630 : loss : 0.027088, loss_ce: 0.008997
2022-01-12 00:26:25,726 iteration 4631 : loss : 0.018886, loss_ce: 0.007140
2022-01-12 00:26:27,337 iteration 4632 : loss : 0.016558, loss_ce: 0.005271
2022-01-12 00:26:28,945 iteration 4633 : loss : 0.019995, loss_ce: 0.007355
2022-01-12 00:26:30,446 iteration 4634 : loss : 0.016034, loss_ce: 0.007177
2022-01-12 00:26:32,068 iteration 4635 : loss : 0.015778, loss_ce: 0.005372
2022-01-12 00:26:33,656 iteration 4636 : loss : 0.019186, loss_ce: 0.007025
2022-01-12 00:26:35,257 iteration 4637 : loss : 0.015879, loss_ce: 0.005906
2022-01-12 00:26:36,900 iteration 4638 : loss : 0.029601, loss_ce: 0.012323
2022-01-12 00:26:38,592 iteration 4639 : loss : 0.020634, loss_ce: 0.006462
2022-01-12 00:26:40,239 iteration 4640 : loss : 0.027449, loss_ce: 0.012869
2022-01-12 00:26:41,805 iteration 4641 : loss : 0.014704, loss_ce: 0.006406
 68%|██████████████████▍        | 273/400 [2:14:50<1:00:28, 28.57s/it]2022-01-12 00:26:43,488 iteration 4642 : loss : 0.024547, loss_ce: 0.007338
2022-01-12 00:26:45,120 iteration 4643 : loss : 0.026338, loss_ce: 0.011223
2022-01-12 00:26:46,742 iteration 4644 : loss : 0.023317, loss_ce: 0.008813
2022-01-12 00:26:48,504 iteration 4645 : loss : 0.022261, loss_ce: 0.010454
2022-01-12 00:26:50,076 iteration 4646 : loss : 0.014519, loss_ce: 0.006269
2022-01-12 00:26:51,639 iteration 4647 : loss : 0.018876, loss_ce: 0.007246
2022-01-12 00:26:53,140 iteration 4648 : loss : 0.015314, loss_ce: 0.006470
2022-01-12 00:26:54,747 iteration 4649 : loss : 0.021272, loss_ce: 0.007942
2022-01-12 00:26:56,333 iteration 4650 : loss : 0.020077, loss_ce: 0.007322
2022-01-12 00:26:57,940 iteration 4651 : loss : 0.018117, loss_ce: 0.007453
2022-01-12 00:26:59,551 iteration 4652 : loss : 0.017565, loss_ce: 0.007358
2022-01-12 00:27:01,139 iteration 4653 : loss : 0.018494, loss_ce: 0.007069
2022-01-12 00:27:02,737 iteration 4654 : loss : 0.015535, loss_ce: 0.006972
2022-01-12 00:27:04,359 iteration 4655 : loss : 0.017334, loss_ce: 0.005223
2022-01-12 00:27:06,050 iteration 4656 : loss : 0.032536, loss_ce: 0.007301
2022-01-12 00:27:07,639 iteration 4657 : loss : 0.035835, loss_ce: 0.005744
2022-01-12 00:27:09,190 iteration 4658 : loss : 0.021671, loss_ce: 0.005190
 68%|███████████████████▊         | 274/400 [2:15:17<59:15, 28.22s/it]2022-01-12 00:27:10,823 iteration 4659 : loss : 0.032733, loss_ce: 0.013882
2022-01-12 00:27:12,449 iteration 4660 : loss : 0.020423, loss_ce: 0.005269
2022-01-12 00:27:14,046 iteration 4661 : loss : 0.025367, loss_ce: 0.015199
2022-01-12 00:27:15,776 iteration 4662 : loss : 0.027005, loss_ce: 0.015386
2022-01-12 00:27:17,313 iteration 4663 : loss : 0.016338, loss_ce: 0.004632
2022-01-12 00:27:18,929 iteration 4664 : loss : 0.022603, loss_ce: 0.007064
2022-01-12 00:27:20,682 iteration 4665 : loss : 0.027152, loss_ce: 0.009723
2022-01-12 00:27:22,218 iteration 4666 : loss : 0.013625, loss_ce: 0.005320
2022-01-12 00:27:23,813 iteration 4667 : loss : 0.018459, loss_ce: 0.008058
2022-01-12 00:27:25,392 iteration 4668 : loss : 0.019225, loss_ce: 0.004936
2022-01-12 00:27:27,059 iteration 4669 : loss : 0.017598, loss_ce: 0.008101
2022-01-12 00:27:28,741 iteration 4670 : loss : 0.020538, loss_ce: 0.008836
2022-01-12 00:27:30,265 iteration 4671 : loss : 0.014721, loss_ce: 0.004518
2022-01-12 00:27:31,993 iteration 4672 : loss : 0.023097, loss_ce: 0.008693
2022-01-12 00:27:33,649 iteration 4673 : loss : 0.024694, loss_ce: 0.008656
2022-01-12 00:27:35,338 iteration 4674 : loss : 0.019090, loss_ce: 0.009617
2022-01-12 00:27:35,338 Training Data Eval:
2022-01-12 00:27:43,311   Average segmentation loss on training set: 0.0138
2022-01-12 00:27:43,312 Validation Data Eval:
2022-01-12 00:27:46,058   Average segmentation loss on validation set: 0.0708
2022-01-12 00:27:47,617 iteration 4675 : loss : 0.019781, loss_ce: 0.008479
 69%|██████████████████▌        | 275/400 [2:15:56<1:05:10, 31.28s/it]2022-01-12 00:27:49,338 iteration 4676 : loss : 0.020052, loss_ce: 0.007129
2022-01-12 00:27:50,916 iteration 4677 : loss : 0.018711, loss_ce: 0.008759
2022-01-12 00:27:52,507 iteration 4678 : loss : 0.026843, loss_ce: 0.008970
2022-01-12 00:27:54,104 iteration 4679 : loss : 0.019470, loss_ce: 0.010768
2022-01-12 00:27:55,647 iteration 4680 : loss : 0.015140, loss_ce: 0.006120
2022-01-12 00:27:57,249 iteration 4681 : loss : 0.023005, loss_ce: 0.007706
2022-01-12 00:27:58,788 iteration 4682 : loss : 0.018843, loss_ce: 0.005104
2022-01-12 00:28:00,372 iteration 4683 : loss : 0.027707, loss_ce: 0.012029
2022-01-12 00:28:02,026 iteration 4684 : loss : 0.019534, loss_ce: 0.008502
2022-01-12 00:28:03,642 iteration 4685 : loss : 0.016094, loss_ce: 0.007449
2022-01-12 00:28:05,155 iteration 4686 : loss : 0.016275, loss_ce: 0.005579
2022-01-12 00:28:06,805 iteration 4687 : loss : 0.018171, loss_ce: 0.007202
2022-01-12 00:28:08,487 iteration 4688 : loss : 0.018226, loss_ce: 0.006067
2022-01-12 00:28:10,173 iteration 4689 : loss : 0.024571, loss_ce: 0.010748
2022-01-12 00:28:11,763 iteration 4690 : loss : 0.017455, loss_ce: 0.005018
2022-01-12 00:28:13,320 iteration 4691 : loss : 0.019376, loss_ce: 0.007336
2022-01-12 00:28:14,910 iteration 4692 : loss : 0.016861, loss_ce: 0.006841
 69%|██████████████████▋        | 276/400 [2:16:23<1:02:10, 30.08s/it]2022-01-12 00:28:16,579 iteration 4693 : loss : 0.022372, loss_ce: 0.009337
2022-01-12 00:28:18,142 iteration 4694 : loss : 0.024748, loss_ce: 0.007652
2022-01-12 00:28:19,804 iteration 4695 : loss : 0.022748, loss_ce: 0.008508
2022-01-12 00:28:21,430 iteration 4696 : loss : 0.023075, loss_ce: 0.009270
2022-01-12 00:28:23,092 iteration 4697 : loss : 0.020363, loss_ce: 0.008136
2022-01-12 00:28:24,580 iteration 4698 : loss : 0.017303, loss_ce: 0.007581
2022-01-12 00:28:26,158 iteration 4699 : loss : 0.024117, loss_ce: 0.009461
2022-01-12 00:28:27,767 iteration 4700 : loss : 0.030577, loss_ce: 0.009245
2022-01-12 00:28:29,364 iteration 4701 : loss : 0.020495, loss_ce: 0.009859
2022-01-12 00:28:30,887 iteration 4702 : loss : 0.015443, loss_ce: 0.005744
2022-01-12 00:28:32,521 iteration 4703 : loss : 0.021152, loss_ce: 0.006891
2022-01-12 00:28:34,128 iteration 4704 : loss : 0.035120, loss_ce: 0.010127
2022-01-12 00:28:35,657 iteration 4705 : loss : 0.013374, loss_ce: 0.004925
2022-01-12 00:28:37,219 iteration 4706 : loss : 0.017017, loss_ce: 0.007417
2022-01-12 00:28:38,895 iteration 4707 : loss : 0.015906, loss_ce: 0.006798
2022-01-12 00:28:40,484 iteration 4708 : loss : 0.027654, loss_ce: 0.006352
2022-01-12 00:28:42,052 iteration 4709 : loss : 0.032266, loss_ce: 0.012037
 69%|████████████████████         | 277/400 [2:16:50<59:51, 29.20s/it]2022-01-12 00:28:43,780 iteration 4710 : loss : 0.018794, loss_ce: 0.005697
2022-01-12 00:28:45,338 iteration 4711 : loss : 0.020600, loss_ce: 0.010305
2022-01-12 00:28:46,950 iteration 4712 : loss : 0.017180, loss_ce: 0.007363
2022-01-12 00:28:48,503 iteration 4713 : loss : 0.014038, loss_ce: 0.005008
2022-01-12 00:28:50,068 iteration 4714 : loss : 0.012919, loss_ce: 0.005427
2022-01-12 00:28:51,757 iteration 4715 : loss : 0.024495, loss_ce: 0.007953
2022-01-12 00:28:53,354 iteration 4716 : loss : 0.016272, loss_ce: 0.006836
2022-01-12 00:28:54,947 iteration 4717 : loss : 0.024080, loss_ce: 0.007951
2022-01-12 00:28:56,560 iteration 4718 : loss : 0.018674, loss_ce: 0.006302
2022-01-12 00:28:58,109 iteration 4719 : loss : 0.012826, loss_ce: 0.004362
2022-01-12 00:28:59,766 iteration 4720 : loss : 0.019440, loss_ce: 0.007099
2022-01-12 00:29:01,350 iteration 4721 : loss : 0.016219, loss_ce: 0.005127
2022-01-12 00:29:02,938 iteration 4722 : loss : 0.015836, loss_ce: 0.006269
2022-01-12 00:29:04,562 iteration 4723 : loss : 0.039010, loss_ce: 0.010989
2022-01-12 00:29:06,098 iteration 4724 : loss : 0.018056, loss_ce: 0.010382
2022-01-12 00:29:07,659 iteration 4725 : loss : 0.016362, loss_ce: 0.006540
2022-01-12 00:29:09,209 iteration 4726 : loss : 0.015397, loss_ce: 0.005395
 70%|████████████████████▏        | 278/400 [2:17:17<58:07, 28.59s/it]2022-01-12 00:29:10,857 iteration 4727 : loss : 0.018602, loss_ce: 0.007072
2022-01-12 00:29:12,482 iteration 4728 : loss : 0.019821, loss_ce: 0.007580
2022-01-12 00:29:14,105 iteration 4729 : loss : 0.012476, loss_ce: 0.003939
2022-01-12 00:29:15,714 iteration 4730 : loss : 0.026992, loss_ce: 0.011836
2022-01-12 00:29:17,309 iteration 4731 : loss : 0.019677, loss_ce: 0.005398
2022-01-12 00:29:18,830 iteration 4732 : loss : 0.015642, loss_ce: 0.006811
2022-01-12 00:29:20,534 iteration 4733 : loss : 0.036129, loss_ce: 0.015216
2022-01-12 00:29:22,141 iteration 4734 : loss : 0.020716, loss_ce: 0.009703
2022-01-12 00:29:23,667 iteration 4735 : loss : 0.013232, loss_ce: 0.005281
2022-01-12 00:29:25,335 iteration 4736 : loss : 0.023350, loss_ce: 0.010059
2022-01-12 00:29:26,968 iteration 4737 : loss : 0.021585, loss_ce: 0.009281
2022-01-12 00:29:28,714 iteration 4738 : loss : 0.024860, loss_ce: 0.007114
2022-01-12 00:29:30,351 iteration 4739 : loss : 0.012130, loss_ce: 0.004530
2022-01-12 00:29:31,981 iteration 4740 : loss : 0.013399, loss_ce: 0.003601
2022-01-12 00:29:33,579 iteration 4741 : loss : 0.020040, loss_ce: 0.009422
2022-01-12 00:29:35,194 iteration 4742 : loss : 0.022401, loss_ce: 0.009013
2022-01-12 00:29:36,983 iteration 4743 : loss : 0.021641, loss_ce: 0.009673
 70%|████████████████████▏        | 279/400 [2:17:45<57:09, 28.34s/it]2022-01-12 00:29:38,698 iteration 4744 : loss : 0.016412, loss_ce: 0.006132
2022-01-12 00:29:40,320 iteration 4745 : loss : 0.018034, loss_ce: 0.007209
2022-01-12 00:29:41,922 iteration 4746 : loss : 0.019893, loss_ce: 0.006066
2022-01-12 00:29:43,459 iteration 4747 : loss : 0.013814, loss_ce: 0.006986
2022-01-12 00:29:45,163 iteration 4748 : loss : 0.018551, loss_ce: 0.006767
2022-01-12 00:29:46,697 iteration 4749 : loss : 0.018909, loss_ce: 0.008034
2022-01-12 00:29:48,312 iteration 4750 : loss : 0.024098, loss_ce: 0.007306
2022-01-12 00:29:49,952 iteration 4751 : loss : 0.017766, loss_ce: 0.007701
2022-01-12 00:29:51,537 iteration 4752 : loss : 0.017241, loss_ce: 0.004554
2022-01-12 00:29:53,107 iteration 4753 : loss : 0.012435, loss_ce: 0.004102
2022-01-12 00:29:54,723 iteration 4754 : loss : 0.016810, loss_ce: 0.007289
2022-01-12 00:29:56,429 iteration 4755 : loss : 0.032231, loss_ce: 0.007471
2022-01-12 00:29:58,076 iteration 4756 : loss : 0.017081, loss_ce: 0.004940
2022-01-12 00:29:59,691 iteration 4757 : loss : 0.024617, loss_ce: 0.012390
2022-01-12 00:30:01,371 iteration 4758 : loss : 0.020105, loss_ce: 0.007371
2022-01-12 00:30:02,969 iteration 4759 : loss : 0.024184, loss_ce: 0.014421
2022-01-12 00:30:02,969 Training Data Eval:
2022-01-12 00:30:10,919   Average segmentation loss on training set: 0.0107
2022-01-12 00:30:10,920 Validation Data Eval:
2022-01-12 00:30:13,664   Average segmentation loss on validation set: 0.0738
2022-01-12 00:30:15,289 iteration 4760 : loss : 0.027198, loss_ce: 0.008483
 70%|██████████████████▉        | 280/400 [2:18:23<1:02:39, 31.33s/it]2022-01-12 00:30:16,871 iteration 4761 : loss : 0.012899, loss_ce: 0.004480
2022-01-12 00:30:18,608 iteration 4762 : loss : 0.027039, loss_ce: 0.008396
2022-01-12 00:30:20,246 iteration 4763 : loss : 0.018448, loss_ce: 0.007891
2022-01-12 00:30:21,792 iteration 4764 : loss : 0.014806, loss_ce: 0.006261
2022-01-12 00:30:23,363 iteration 4765 : loss : 0.014039, loss_ce: 0.006053
2022-01-12 00:30:24,919 iteration 4766 : loss : 0.019128, loss_ce: 0.006430
2022-01-12 00:30:26,582 iteration 4767 : loss : 0.038452, loss_ce: 0.013938
2022-01-12 00:30:28,290 iteration 4768 : loss : 0.023741, loss_ce: 0.010094
2022-01-12 00:30:29,985 iteration 4769 : loss : 0.041908, loss_ce: 0.010407
2022-01-12 00:30:31,597 iteration 4770 : loss : 0.017652, loss_ce: 0.009413
2022-01-12 00:30:33,161 iteration 4771 : loss : 0.016517, loss_ce: 0.006472
2022-01-12 00:30:34,746 iteration 4772 : loss : 0.011797, loss_ce: 0.003635
2022-01-12 00:30:36,311 iteration 4773 : loss : 0.017467, loss_ce: 0.006332
2022-01-12 00:30:37,863 iteration 4774 : loss : 0.015909, loss_ce: 0.006011
2022-01-12 00:30:39,465 iteration 4775 : loss : 0.025629, loss_ce: 0.009724
2022-01-12 00:30:41,050 iteration 4776 : loss : 0.022196, loss_ce: 0.007995
2022-01-12 00:30:42,622 iteration 4777 : loss : 0.024169, loss_ce: 0.007340
 70%|████████████████████▎        | 281/400 [2:18:51<59:45, 30.13s/it]2022-01-12 00:30:44,331 iteration 4778 : loss : 0.023515, loss_ce: 0.008663
2022-01-12 00:30:45,889 iteration 4779 : loss : 0.014316, loss_ce: 0.004646
2022-01-12 00:30:47,442 iteration 4780 : loss : 0.040208, loss_ce: 0.008820
2022-01-12 00:30:49,003 iteration 4781 : loss : 0.013920, loss_ce: 0.006165
2022-01-12 00:30:50,666 iteration 4782 : loss : 0.028250, loss_ce: 0.009294
2022-01-12 00:30:52,194 iteration 4783 : loss : 0.018997, loss_ce: 0.005638
2022-01-12 00:30:53,862 iteration 4784 : loss : 0.019503, loss_ce: 0.006652
2022-01-12 00:30:55,543 iteration 4785 : loss : 0.031443, loss_ce: 0.013997
2022-01-12 00:30:57,128 iteration 4786 : loss : 0.021167, loss_ce: 0.007390
2022-01-12 00:30:58,766 iteration 4787 : loss : 0.021218, loss_ce: 0.005368
2022-01-12 00:31:00,377 iteration 4788 : loss : 0.019437, loss_ce: 0.008342
2022-01-12 00:31:02,001 iteration 4789 : loss : 0.025094, loss_ce: 0.009463
2022-01-12 00:31:03,607 iteration 4790 : loss : 0.026851, loss_ce: 0.011477
2022-01-12 00:31:05,164 iteration 4791 : loss : 0.021940, loss_ce: 0.009030
2022-01-12 00:31:06,739 iteration 4792 : loss : 0.025972, loss_ce: 0.012001
2022-01-12 00:31:08,389 iteration 4793 : loss : 0.018987, loss_ce: 0.009606
2022-01-12 00:31:09,981 iteration 4794 : loss : 0.028261, loss_ce: 0.005305
 70%|████████████████████▍        | 282/400 [2:19:18<57:37, 29.30s/it]2022-01-12 00:31:11,550 iteration 4795 : loss : 0.022690, loss_ce: 0.006352
2022-01-12 00:31:13,076 iteration 4796 : loss : 0.019822, loss_ce: 0.007702
2022-01-12 00:31:14,619 iteration 4797 : loss : 0.016396, loss_ce: 0.006093
2022-01-12 00:31:16,279 iteration 4798 : loss : 0.030294, loss_ce: 0.013686
2022-01-12 00:31:17,869 iteration 4799 : loss : 0.020944, loss_ce: 0.005309
2022-01-12 00:31:19,475 iteration 4800 : loss : 0.041083, loss_ce: 0.017259
2022-01-12 00:31:21,091 iteration 4801 : loss : 0.021088, loss_ce: 0.010016
2022-01-12 00:31:22,580 iteration 4802 : loss : 0.016186, loss_ce: 0.004900
2022-01-12 00:31:24,192 iteration 4803 : loss : 0.017485, loss_ce: 0.007280
2022-01-12 00:31:25,767 iteration 4804 : loss : 0.017473, loss_ce: 0.005176
2022-01-12 00:31:27,411 iteration 4805 : loss : 0.017905, loss_ce: 0.006213
2022-01-12 00:31:28,965 iteration 4806 : loss : 0.015629, loss_ce: 0.004539
2022-01-12 00:31:30,663 iteration 4807 : loss : 0.015361, loss_ce: 0.006434
2022-01-12 00:31:32,504 iteration 4808 : loss : 0.022813, loss_ce: 0.008911
2022-01-12 00:31:34,001 iteration 4809 : loss : 0.015391, loss_ce: 0.006523
2022-01-12 00:31:35,649 iteration 4810 : loss : 0.029925, loss_ce: 0.010931
2022-01-12 00:31:37,224 iteration 4811 : loss : 0.014471, loss_ce: 0.005451
 71%|████████████████████▌        | 283/400 [2:19:45<55:55, 28.68s/it]2022-01-12 00:31:38,834 iteration 4812 : loss : 0.026965, loss_ce: 0.010400
2022-01-12 00:31:40,440 iteration 4813 : loss : 0.020604, loss_ce: 0.009207
2022-01-12 00:31:41,981 iteration 4814 : loss : 0.018998, loss_ce: 0.009517
2022-01-12 00:31:43,577 iteration 4815 : loss : 0.016770, loss_ce: 0.005929
2022-01-12 00:31:45,132 iteration 4816 : loss : 0.016820, loss_ce: 0.005838
2022-01-12 00:31:46,587 iteration 4817 : loss : 0.016179, loss_ce: 0.005575
2022-01-12 00:31:48,229 iteration 4818 : loss : 0.027151, loss_ce: 0.011379
2022-01-12 00:31:49,833 iteration 4819 : loss : 0.017120, loss_ce: 0.005399
2022-01-12 00:31:51,493 iteration 4820 : loss : 0.016294, loss_ce: 0.007245
2022-01-12 00:31:53,072 iteration 4821 : loss : 0.015578, loss_ce: 0.006690
2022-01-12 00:31:54,696 iteration 4822 : loss : 0.019699, loss_ce: 0.006181
2022-01-12 00:31:56,262 iteration 4823 : loss : 0.021623, loss_ce: 0.007690
2022-01-12 00:31:57,931 iteration 4824 : loss : 0.014833, loss_ce: 0.005536
2022-01-12 00:31:59,520 iteration 4825 : loss : 0.010695, loss_ce: 0.003229
2022-01-12 00:32:01,093 iteration 4826 : loss : 0.050430, loss_ce: 0.025299
2022-01-12 00:32:02,814 iteration 4827 : loss : 0.019209, loss_ce: 0.006611
2022-01-12 00:32:04,485 iteration 4828 : loss : 0.023214, loss_ce: 0.009667
 71%|████████████████████▌        | 284/400 [2:20:13<54:37, 28.26s/it]2022-01-12 00:32:06,154 iteration 4829 : loss : 0.025139, loss_ce: 0.008564
2022-01-12 00:32:07,702 iteration 4830 : loss : 0.015570, loss_ce: 0.005075
2022-01-12 00:32:09,403 iteration 4831 : loss : 0.037392, loss_ce: 0.013628
2022-01-12 00:32:10,997 iteration 4832 : loss : 0.027176, loss_ce: 0.012631
2022-01-12 00:32:12,681 iteration 4833 : loss : 0.019598, loss_ce: 0.006257
2022-01-12 00:32:14,381 iteration 4834 : loss : 0.018248, loss_ce: 0.007175
2022-01-12 00:32:15,973 iteration 4835 : loss : 0.017888, loss_ce: 0.006770
2022-01-12 00:32:17,576 iteration 4836 : loss : 0.020554, loss_ce: 0.006777
2022-01-12 00:32:19,128 iteration 4837 : loss : 0.017782, loss_ce: 0.003981
2022-01-12 00:32:20,774 iteration 4838 : loss : 0.016894, loss_ce: 0.007903
2022-01-12 00:32:22,408 iteration 4839 : loss : 0.021711, loss_ce: 0.011794
2022-01-12 00:32:24,033 iteration 4840 : loss : 0.022533, loss_ce: 0.009054
2022-01-12 00:32:25,715 iteration 4841 : loss : 0.020821, loss_ce: 0.007994
2022-01-12 00:32:27,345 iteration 4842 : loss : 0.017939, loss_ce: 0.008287
2022-01-12 00:32:28,969 iteration 4843 : loss : 0.015786, loss_ce: 0.006197
2022-01-12 00:32:30,515 iteration 4844 : loss : 0.014055, loss_ce: 0.006308
2022-01-12 00:32:30,515 Training Data Eval:
2022-01-12 00:32:38,479   Average segmentation loss on training set: 0.0108
2022-01-12 00:32:38,480 Validation Data Eval:
2022-01-12 00:32:41,228   Average segmentation loss on validation set: 0.0781
2022-01-12 00:32:42,785 iteration 4845 : loss : 0.017364, loss_ce: 0.006499
 71%|████████████████████▋        | 285/400 [2:20:51<59:56, 31.27s/it]2022-01-12 00:32:44,443 iteration 4846 : loss : 0.016723, loss_ce: 0.006795
2022-01-12 00:32:46,020 iteration 4847 : loss : 0.020119, loss_ce: 0.005946
2022-01-12 00:32:47,735 iteration 4848 : loss : 0.025694, loss_ce: 0.011326
2022-01-12 00:32:49,327 iteration 4849 : loss : 0.018288, loss_ce: 0.007775
2022-01-12 00:32:50,967 iteration 4850 : loss : 0.036646, loss_ce: 0.007622
2022-01-12 00:32:52,595 iteration 4851 : loss : 0.022476, loss_ce: 0.009229
2022-01-12 00:32:54,177 iteration 4852 : loss : 0.016484, loss_ce: 0.006611
2022-01-12 00:32:55,752 iteration 4853 : loss : 0.015442, loss_ce: 0.005790
2022-01-12 00:32:57,268 iteration 4854 : loss : 0.013122, loss_ce: 0.004944
2022-01-12 00:32:58,856 iteration 4855 : loss : 0.029246, loss_ce: 0.014009
2022-01-12 00:33:00,474 iteration 4856 : loss : 0.018860, loss_ce: 0.007363
2022-01-12 00:33:02,112 iteration 4857 : loss : 0.022726, loss_ce: 0.013704
2022-01-12 00:33:03,710 iteration 4858 : loss : 0.018860, loss_ce: 0.006898
2022-01-12 00:33:05,238 iteration 4859 : loss : 0.010087, loss_ce: 0.003953
2022-01-12 00:33:06,798 iteration 4860 : loss : 0.014806, loss_ce: 0.003580
2022-01-12 00:33:08,312 iteration 4861 : loss : 0.016834, loss_ce: 0.004962
2022-01-12 00:33:10,002 iteration 4862 : loss : 0.019871, loss_ce: 0.006628
 72%|████████████████████▋        | 286/400 [2:21:18<57:06, 30.05s/it]2022-01-12 00:33:11,678 iteration 4863 : loss : 0.017487, loss_ce: 0.005414
2022-01-12 00:33:13,312 iteration 4864 : loss : 0.027703, loss_ce: 0.010348
2022-01-12 00:33:14,926 iteration 4865 : loss : 0.017450, loss_ce: 0.006828
2022-01-12 00:33:16,528 iteration 4866 : loss : 0.020714, loss_ce: 0.008746
2022-01-12 00:33:18,160 iteration 4867 : loss : 0.030352, loss_ce: 0.009503
2022-01-12 00:33:19,771 iteration 4868 : loss : 0.018042, loss_ce: 0.008084
2022-01-12 00:33:21,337 iteration 4869 : loss : 0.017662, loss_ce: 0.006232
2022-01-12 00:33:22,820 iteration 4870 : loss : 0.013596, loss_ce: 0.005339
2022-01-12 00:33:24,380 iteration 4871 : loss : 0.020347, loss_ce: 0.006335
2022-01-12 00:33:25,961 iteration 4872 : loss : 0.015266, loss_ce: 0.006446
2022-01-12 00:33:27,544 iteration 4873 : loss : 0.016425, loss_ce: 0.006152
2022-01-12 00:33:29,153 iteration 4874 : loss : 0.015039, loss_ce: 0.005501
2022-01-12 00:33:30,715 iteration 4875 : loss : 0.015886, loss_ce: 0.004293
2022-01-12 00:33:32,212 iteration 4876 : loss : 0.013450, loss_ce: 0.005723
2022-01-12 00:33:33,771 iteration 4877 : loss : 0.014311, loss_ce: 0.004917
2022-01-12 00:33:35,382 iteration 4878 : loss : 0.036639, loss_ce: 0.014028
2022-01-12 00:33:37,033 iteration 4879 : loss : 0.017203, loss_ce: 0.005914
 72%|████████████████████▊        | 287/400 [2:21:45<54:53, 29.15s/it]2022-01-12 00:33:38,702 iteration 4880 : loss : 0.013944, loss_ce: 0.004430
2022-01-12 00:33:40,233 iteration 4881 : loss : 0.013500, loss_ce: 0.004293
2022-01-12 00:33:41,764 iteration 4882 : loss : 0.012767, loss_ce: 0.003120
2022-01-12 00:33:43,384 iteration 4883 : loss : 0.021986, loss_ce: 0.008848
2022-01-12 00:33:44,927 iteration 4884 : loss : 0.022282, loss_ce: 0.008794
2022-01-12 00:33:46,522 iteration 4885 : loss : 0.024603, loss_ce: 0.008505
2022-01-12 00:33:48,165 iteration 4886 : loss : 0.014713, loss_ce: 0.004662
2022-01-12 00:33:49,727 iteration 4887 : loss : 0.018506, loss_ce: 0.006728
2022-01-12 00:33:51,347 iteration 4888 : loss : 0.017715, loss_ce: 0.004461
2022-01-12 00:33:52,919 iteration 4889 : loss : 0.015065, loss_ce: 0.006165
2022-01-12 00:33:54,521 iteration 4890 : loss : 0.016678, loss_ce: 0.006302
2022-01-12 00:33:56,127 iteration 4891 : loss : 0.020671, loss_ce: 0.009575
2022-01-12 00:33:57,731 iteration 4892 : loss : 0.019188, loss_ce: 0.009064
2022-01-12 00:33:59,507 iteration 4893 : loss : 0.022030, loss_ce: 0.008937
2022-01-12 00:34:01,111 iteration 4894 : loss : 0.015842, loss_ce: 0.008249
2022-01-12 00:34:02,704 iteration 4895 : loss : 0.018604, loss_ce: 0.008305
2022-01-12 00:34:04,280 iteration 4896 : loss : 0.019066, loss_ce: 0.007308
 72%|████████████████████▉        | 288/400 [2:22:12<53:20, 28.58s/it]2022-01-12 00:34:05,844 iteration 4897 : loss : 0.012533, loss_ce: 0.006196
2022-01-12 00:34:07,418 iteration 4898 : loss : 0.012466, loss_ce: 0.004590
2022-01-12 00:34:08,945 iteration 4899 : loss : 0.017481, loss_ce: 0.009240
2022-01-12 00:34:10,600 iteration 4900 : loss : 0.046083, loss_ce: 0.017011
2022-01-12 00:34:12,205 iteration 4901 : loss : 0.014866, loss_ce: 0.004321
2022-01-12 00:34:13,750 iteration 4902 : loss : 0.013780, loss_ce: 0.002904
2022-01-12 00:34:15,357 iteration 4903 : loss : 0.014439, loss_ce: 0.005996
2022-01-12 00:34:16,940 iteration 4904 : loss : 0.018906, loss_ce: 0.009436
2022-01-12 00:34:18,508 iteration 4905 : loss : 0.014555, loss_ce: 0.006008
2022-01-12 00:34:20,074 iteration 4906 : loss : 0.014924, loss_ce: 0.005002
2022-01-12 00:34:21,686 iteration 4907 : loss : 0.022103, loss_ce: 0.007569
2022-01-12 00:34:23,170 iteration 4908 : loss : 0.014702, loss_ce: 0.003996
2022-01-12 00:34:24,757 iteration 4909 : loss : 0.020403, loss_ce: 0.006763
2022-01-12 00:34:26,309 iteration 4910 : loss : 0.018126, loss_ce: 0.003832
2022-01-12 00:34:27,986 iteration 4911 : loss : 0.016461, loss_ce: 0.006261
2022-01-12 00:34:29,646 iteration 4912 : loss : 0.027897, loss_ce: 0.009429
2022-01-12 00:34:31,195 iteration 4913 : loss : 0.015768, loss_ce: 0.005453
 72%|████████████████████▉        | 289/400 [2:22:39<51:56, 28.08s/it]2022-01-12 00:34:32,745 iteration 4914 : loss : 0.013510, loss_ce: 0.004022
2022-01-12 00:34:34,367 iteration 4915 : loss : 0.019084, loss_ce: 0.007688
2022-01-12 00:34:35,981 iteration 4916 : loss : 0.020371, loss_ce: 0.006274
2022-01-12 00:34:37,570 iteration 4917 : loss : 0.024088, loss_ce: 0.008726
2022-01-12 00:34:39,114 iteration 4918 : loss : 0.015194, loss_ce: 0.006550
2022-01-12 00:34:40,654 iteration 4919 : loss : 0.014819, loss_ce: 0.005625
2022-01-12 00:34:42,261 iteration 4920 : loss : 0.014217, loss_ce: 0.006200
2022-01-12 00:34:43,824 iteration 4921 : loss : 0.013206, loss_ce: 0.002814
2022-01-12 00:34:45,399 iteration 4922 : loss : 0.019723, loss_ce: 0.003962
2022-01-12 00:34:47,004 iteration 4923 : loss : 0.019086, loss_ce: 0.007271
2022-01-12 00:34:48,632 iteration 4924 : loss : 0.019898, loss_ce: 0.007384
2022-01-12 00:34:50,369 iteration 4925 : loss : 0.020539, loss_ce: 0.008282
2022-01-12 00:34:51,945 iteration 4926 : loss : 0.017485, loss_ce: 0.006500
2022-01-12 00:34:53,605 iteration 4927 : loss : 0.015240, loss_ce: 0.005909
2022-01-12 00:34:55,299 iteration 4928 : loss : 0.023049, loss_ce: 0.011456
2022-01-12 00:34:56,960 iteration 4929 : loss : 0.032460, loss_ce: 0.013379
2022-01-12 00:34:56,961 Training Data Eval:
2022-01-12 00:35:04,925   Average segmentation loss on training set: 0.0104
2022-01-12 00:35:04,926 Validation Data Eval:
2022-01-12 00:35:07,669   Average segmentation loss on validation set: 0.0669
2022-01-12 00:35:09,273 iteration 4930 : loss : 0.013013, loss_ce: 0.005587
 72%|█████████████████████        | 290/400 [2:23:17<56:58, 31.08s/it]2022-01-12 00:35:10,954 iteration 4931 : loss : 0.019728, loss_ce: 0.011205
2022-01-12 00:35:12,509 iteration 4932 : loss : 0.014210, loss_ce: 0.005890
2022-01-12 00:35:14,061 iteration 4933 : loss : 0.019446, loss_ce: 0.007239
2022-01-12 00:35:15,664 iteration 4934 : loss : 0.019845, loss_ce: 0.009315
2022-01-12 00:35:17,296 iteration 4935 : loss : 0.022234, loss_ce: 0.006649
2022-01-12 00:35:18,895 iteration 4936 : loss : 0.012863, loss_ce: 0.005199
2022-01-12 00:35:20,511 iteration 4937 : loss : 0.018721, loss_ce: 0.006523
2022-01-12 00:35:22,106 iteration 4938 : loss : 0.019152, loss_ce: 0.006466
2022-01-12 00:35:23,769 iteration 4939 : loss : 0.018321, loss_ce: 0.007292
2022-01-12 00:35:25,327 iteration 4940 : loss : 0.033918, loss_ce: 0.009340
2022-01-12 00:35:26,918 iteration 4941 : loss : 0.014075, loss_ce: 0.005106
2022-01-12 00:35:28,557 iteration 4942 : loss : 0.019149, loss_ce: 0.007244
2022-01-12 00:35:30,109 iteration 4943 : loss : 0.010986, loss_ce: 0.003548
2022-01-12 00:35:31,715 iteration 4944 : loss : 0.024837, loss_ce: 0.010673
2022-01-12 00:35:33,412 iteration 4945 : loss : 0.022928, loss_ce: 0.006673
2022-01-12 00:35:35,085 iteration 4946 : loss : 0.021595, loss_ce: 0.010667
2022-01-12 00:35:36,805 iteration 4947 : loss : 0.022395, loss_ce: 0.007766
 73%|█████████████████████        | 291/400 [2:23:45<54:31, 30.01s/it]2022-01-12 00:35:38,513 iteration 4948 : loss : 0.021722, loss_ce: 0.008226
2022-01-12 00:35:40,087 iteration 4949 : loss : 0.012915, loss_ce: 0.004817
2022-01-12 00:35:41,634 iteration 4950 : loss : 0.017846, loss_ce: 0.005904
2022-01-12 00:35:43,200 iteration 4951 : loss : 0.028194, loss_ce: 0.009362
2022-01-12 00:35:44,700 iteration 4952 : loss : 0.020777, loss_ce: 0.007553
2022-01-12 00:35:46,338 iteration 4953 : loss : 0.021534, loss_ce: 0.010591
2022-01-12 00:35:47,861 iteration 4954 : loss : 0.018806, loss_ce: 0.007280
2022-01-12 00:35:49,425 iteration 4955 : loss : 0.015209, loss_ce: 0.005837
2022-01-12 00:35:51,010 iteration 4956 : loss : 0.016297, loss_ce: 0.005312
2022-01-12 00:35:52,598 iteration 4957 : loss : 0.013421, loss_ce: 0.005734
2022-01-12 00:35:54,201 iteration 4958 : loss : 0.029543, loss_ce: 0.017464
2022-01-12 00:35:55,710 iteration 4959 : loss : 0.014555, loss_ce: 0.005096
2022-01-12 00:35:57,293 iteration 4960 : loss : 0.017930, loss_ce: 0.006791
2022-01-12 00:35:58,850 iteration 4961 : loss : 0.025076, loss_ce: 0.012477
2022-01-12 00:36:00,496 iteration 4962 : loss : 0.033937, loss_ce: 0.013423
2022-01-12 00:36:02,038 iteration 4963 : loss : 0.014204, loss_ce: 0.005813
2022-01-12 00:36:03,717 iteration 4964 : loss : 0.021208, loss_ce: 0.009445
 73%|█████████████████████▏       | 292/400 [2:24:12<52:20, 29.08s/it]2022-01-12 00:36:05,419 iteration 4965 : loss : 0.020827, loss_ce: 0.007033
2022-01-12 00:36:07,156 iteration 4966 : loss : 0.024133, loss_ce: 0.006982
2022-01-12 00:36:08,688 iteration 4967 : loss : 0.011244, loss_ce: 0.004658
2022-01-12 00:36:10,285 iteration 4968 : loss : 0.017659, loss_ce: 0.007245
2022-01-12 00:36:11,853 iteration 4969 : loss : 0.044761, loss_ce: 0.009113
2022-01-12 00:36:13,403 iteration 4970 : loss : 0.011955, loss_ce: 0.004555
2022-01-12 00:36:15,049 iteration 4971 : loss : 0.016853, loss_ce: 0.006908
2022-01-12 00:36:16,704 iteration 4972 : loss : 0.027136, loss_ce: 0.008784
2022-01-12 00:36:18,281 iteration 4973 : loss : 0.014284, loss_ce: 0.005668
2022-01-12 00:36:19,853 iteration 4974 : loss : 0.014812, loss_ce: 0.005370
2022-01-12 00:36:21,416 iteration 4975 : loss : 0.028394, loss_ce: 0.008885
2022-01-12 00:36:22,986 iteration 4976 : loss : 0.021823, loss_ce: 0.008579
2022-01-12 00:36:24,676 iteration 4977 : loss : 0.022042, loss_ce: 0.010494
2022-01-12 00:36:26,263 iteration 4978 : loss : 0.038073, loss_ce: 0.010723
2022-01-12 00:36:27,875 iteration 4979 : loss : 0.016478, loss_ce: 0.008070
2022-01-12 00:36:29,366 iteration 4980 : loss : 0.012769, loss_ce: 0.004905
2022-01-12 00:36:30,951 iteration 4981 : loss : 0.024198, loss_ce: 0.009129
 73%|█████████████████████▏       | 293/400 [2:24:39<50:52, 28.53s/it]2022-01-12 00:36:32,569 iteration 4982 : loss : 0.020976, loss_ce: 0.009391
2022-01-12 00:36:34,127 iteration 4983 : loss : 0.014931, loss_ce: 0.005162
2022-01-12 00:36:35,782 iteration 4984 : loss : 0.021320, loss_ce: 0.008218
2022-01-12 00:36:37,409 iteration 4985 : loss : 0.016103, loss_ce: 0.005277
2022-01-12 00:36:39,107 iteration 4986 : loss : 0.038411, loss_ce: 0.017169
2022-01-12 00:36:40,678 iteration 4987 : loss : 0.016508, loss_ce: 0.007653
2022-01-12 00:36:42,266 iteration 4988 : loss : 0.016585, loss_ce: 0.007096
2022-01-12 00:36:43,841 iteration 4989 : loss : 0.015052, loss_ce: 0.005115
2022-01-12 00:36:45,544 iteration 4990 : loss : 0.021218, loss_ce: 0.006401
2022-01-12 00:36:47,151 iteration 4991 : loss : 0.020212, loss_ce: 0.005471
2022-01-12 00:36:48,756 iteration 4992 : loss : 0.016297, loss_ce: 0.007135
2022-01-12 00:36:50,417 iteration 4993 : loss : 0.022421, loss_ce: 0.011097
2022-01-12 00:36:52,108 iteration 4994 : loss : 0.021718, loss_ce: 0.005580
2022-01-12 00:36:53,702 iteration 4995 : loss : 0.018943, loss_ce: 0.006901
2022-01-12 00:36:55,307 iteration 4996 : loss : 0.023369, loss_ce: 0.009180
2022-01-12 00:36:56,993 iteration 4997 : loss : 0.022719, loss_ce: 0.009381
2022-01-12 00:36:58,539 iteration 4998 : loss : 0.016113, loss_ce: 0.005704
 74%|█████████████████████▎       | 294/400 [2:25:07<49:54, 28.25s/it]2022-01-12 00:37:00,238 iteration 4999 : loss : 0.015481, loss_ce: 0.006112
2022-01-12 00:37:01,771 iteration 5000 : loss : 0.012848, loss_ce: 0.004819
2022-01-12 00:37:03,329 iteration 5001 : loss : 0.023883, loss_ce: 0.007866
2022-01-12 00:37:04,954 iteration 5002 : loss : 0.029452, loss_ce: 0.011948
2022-01-12 00:37:06,516 iteration 5003 : loss : 0.024292, loss_ce: 0.007232
2022-01-12 00:37:08,204 iteration 5004 : loss : 0.027790, loss_ce: 0.009092
2022-01-12 00:37:09,757 iteration 5005 : loss : 0.015471, loss_ce: 0.005416
2022-01-12 00:37:11,392 iteration 5006 : loss : 0.016021, loss_ce: 0.004660
2022-01-12 00:37:12,967 iteration 5007 : loss : 0.016405, loss_ce: 0.005981
2022-01-12 00:37:14,487 iteration 5008 : loss : 0.017542, loss_ce: 0.005452
2022-01-12 00:37:16,065 iteration 5009 : loss : 0.021478, loss_ce: 0.006663
2022-01-12 00:37:17,762 iteration 5010 : loss : 0.033450, loss_ce: 0.010957
2022-01-12 00:37:19,371 iteration 5011 : loss : 0.017149, loss_ce: 0.009481
2022-01-12 00:37:21,052 iteration 5012 : loss : 0.031936, loss_ce: 0.016197
2022-01-12 00:37:22,827 iteration 5013 : loss : 0.017251, loss_ce: 0.006488
2022-01-12 00:37:24,336 iteration 5014 : loss : 0.016027, loss_ce: 0.005732
2022-01-12 00:37:24,336 Training Data Eval:
2022-01-12 00:37:32,302   Average segmentation loss on training set: 0.0107
2022-01-12 00:37:32,303 Validation Data Eval:
2022-01-12 00:37:35,052   Average segmentation loss on validation set: 0.0684
2022-01-12 00:37:36,622 iteration 5015 : loss : 0.018519, loss_ce: 0.003463
 74%|█████████████████████▍       | 295/400 [2:25:45<54:35, 31.20s/it]2022-01-12 00:37:38,209 iteration 5016 : loss : 0.020872, loss_ce: 0.008595
2022-01-12 00:37:39,779 iteration 5017 : loss : 0.020449, loss_ce: 0.006059
2022-01-12 00:37:41,541 iteration 5018 : loss : 0.019669, loss_ce: 0.007509
2022-01-12 00:37:43,106 iteration 5019 : loss : 0.016514, loss_ce: 0.004827
2022-01-12 00:37:44,732 iteration 5020 : loss : 0.014556, loss_ce: 0.005992
2022-01-12 00:37:46,430 iteration 5021 : loss : 0.023320, loss_ce: 0.008420
2022-01-12 00:37:48,087 iteration 5022 : loss : 0.030991, loss_ce: 0.015758
2022-01-12 00:37:49,730 iteration 5023 : loss : 0.016669, loss_ce: 0.006715
2022-01-12 00:37:51,308 iteration 5024 : loss : 0.012856, loss_ce: 0.004579
2022-01-12 00:37:52,859 iteration 5025 : loss : 0.014157, loss_ce: 0.005217
2022-01-12 00:37:54,456 iteration 5026 : loss : 0.026293, loss_ce: 0.011652
2022-01-12 00:37:56,056 iteration 5027 : loss : 0.015843, loss_ce: 0.005557
2022-01-12 00:37:57,667 iteration 5028 : loss : 0.022915, loss_ce: 0.008026
2022-01-12 00:37:59,223 iteration 5029 : loss : 0.015005, loss_ce: 0.006421
2022-01-12 00:38:00,840 iteration 5030 : loss : 0.022044, loss_ce: 0.007904
2022-01-12 00:38:02,516 iteration 5031 : loss : 0.025456, loss_ce: 0.012523
2022-01-12 00:38:04,220 iteration 5032 : loss : 0.019786, loss_ce: 0.006600
 74%|█████████████████████▍       | 296/400 [2:26:12<52:12, 30.12s/it]2022-01-12 00:38:05,879 iteration 5033 : loss : 0.021206, loss_ce: 0.012235
2022-01-12 00:38:07,483 iteration 5034 : loss : 0.022393, loss_ce: 0.005602
2022-01-12 00:38:09,066 iteration 5035 : loss : 0.016774, loss_ce: 0.006023
2022-01-12 00:38:10,597 iteration 5036 : loss : 0.014565, loss_ce: 0.005413
2022-01-12 00:38:12,179 iteration 5037 : loss : 0.014228, loss_ce: 0.005237
2022-01-12 00:38:13,915 iteration 5038 : loss : 0.025113, loss_ce: 0.011172
2022-01-12 00:38:15,506 iteration 5039 : loss : 0.017316, loss_ce: 0.006170
2022-01-12 00:38:17,142 iteration 5040 : loss : 0.018262, loss_ce: 0.007592
2022-01-12 00:38:18,732 iteration 5041 : loss : 0.015527, loss_ce: 0.005200
2022-01-12 00:38:20,325 iteration 5042 : loss : 0.013834, loss_ce: 0.005743
2022-01-12 00:38:21,883 iteration 5043 : loss : 0.016183, loss_ce: 0.005461
2022-01-12 00:38:23,481 iteration 5044 : loss : 0.013768, loss_ce: 0.004599
2022-01-12 00:38:24,974 iteration 5045 : loss : 0.013221, loss_ce: 0.004144
2022-01-12 00:38:26,574 iteration 5046 : loss : 0.013452, loss_ce: 0.004723
2022-01-12 00:38:28,138 iteration 5047 : loss : 0.016419, loss_ce: 0.006383
2022-01-12 00:38:29,670 iteration 5048 : loss : 0.011901, loss_ce: 0.004338
2022-01-12 00:38:31,399 iteration 5049 : loss : 0.028547, loss_ce: 0.011011
 74%|█████████████████████▌       | 297/400 [2:26:40<50:11, 29.24s/it]2022-01-12 00:38:33,025 iteration 5050 : loss : 0.013803, loss_ce: 0.003824
2022-01-12 00:38:34,562 iteration 5051 : loss : 0.014496, loss_ce: 0.007220
2022-01-12 00:38:36,198 iteration 5052 : loss : 0.015288, loss_ce: 0.007207
2022-01-12 00:38:37,732 iteration 5053 : loss : 0.015053, loss_ce: 0.005380
2022-01-12 00:38:39,344 iteration 5054 : loss : 0.013636, loss_ce: 0.005735
2022-01-12 00:38:40,908 iteration 5055 : loss : 0.020199, loss_ce: 0.008541
2022-01-12 00:38:42,481 iteration 5056 : loss : 0.016748, loss_ce: 0.005896
2022-01-12 00:38:44,057 iteration 5057 : loss : 0.015612, loss_ce: 0.007134
2022-01-12 00:38:45,613 iteration 5058 : loss : 0.018957, loss_ce: 0.006885
2022-01-12 00:38:47,215 iteration 5059 : loss : 0.016013, loss_ce: 0.007587
2022-01-12 00:38:48,866 iteration 5060 : loss : 0.019632, loss_ce: 0.004578
2022-01-12 00:38:50,464 iteration 5061 : loss : 0.014648, loss_ce: 0.006970
2022-01-12 00:38:52,130 iteration 5062 : loss : 0.017491, loss_ce: 0.006783
2022-01-12 00:38:53,683 iteration 5063 : loss : 0.012163, loss_ce: 0.004741
2022-01-12 00:38:55,301 iteration 5064 : loss : 0.017963, loss_ce: 0.004567
2022-01-12 00:38:56,991 iteration 5065 : loss : 0.041181, loss_ce: 0.010780
2022-01-12 00:38:58,461 iteration 5066 : loss : 0.012991, loss_ce: 0.005587
 74%|█████████████████████▌       | 298/400 [2:27:07<48:35, 28.58s/it]2022-01-12 00:39:00,213 iteration 5067 : loss : 0.024550, loss_ce: 0.008012
2022-01-12 00:39:01,803 iteration 5068 : loss : 0.013089, loss_ce: 0.004828
2022-01-12 00:39:03,356 iteration 5069 : loss : 0.019115, loss_ce: 0.006453
2022-01-12 00:39:04,924 iteration 5070 : loss : 0.014261, loss_ce: 0.007129
2022-01-12 00:39:06,572 iteration 5071 : loss : 0.016934, loss_ce: 0.007738
2022-01-12 00:39:08,197 iteration 5072 : loss : 0.015387, loss_ce: 0.005211
2022-01-12 00:39:09,859 iteration 5073 : loss : 0.033294, loss_ce: 0.008633
2022-01-12 00:39:11,507 iteration 5074 : loss : 0.016979, loss_ce: 0.006770
2022-01-12 00:39:13,015 iteration 5075 : loss : 0.014726, loss_ce: 0.006495
2022-01-12 00:39:14,541 iteration 5076 : loss : 0.019219, loss_ce: 0.007626
2022-01-12 00:39:16,200 iteration 5077 : loss : 0.018003, loss_ce: 0.006126
2022-01-12 00:39:17,811 iteration 5078 : loss : 0.019358, loss_ce: 0.006552
2022-01-12 00:39:19,396 iteration 5079 : loss : 0.011071, loss_ce: 0.004095
2022-01-12 00:39:21,026 iteration 5080 : loss : 0.018258, loss_ce: 0.008176
2022-01-12 00:39:22,652 iteration 5081 : loss : 0.018957, loss_ce: 0.008763
2022-01-12 00:39:24,220 iteration 5082 : loss : 0.017462, loss_ce: 0.004973
2022-01-12 00:39:25,784 iteration 5083 : loss : 0.023909, loss_ce: 0.008493
 75%|█████████████████████▋       | 299/400 [2:27:34<47:28, 28.21s/it]2022-01-12 00:39:27,434 iteration 5084 : loss : 0.032634, loss_ce: 0.019202
2022-01-12 00:39:29,138 iteration 5085 : loss : 0.016502, loss_ce: 0.006325
2022-01-12 00:39:30,656 iteration 5086 : loss : 0.012591, loss_ce: 0.003847
2022-01-12 00:39:32,268 iteration 5087 : loss : 0.014595, loss_ce: 0.003907
2022-01-12 00:39:33,794 iteration 5088 : loss : 0.011166, loss_ce: 0.005175
2022-01-12 00:39:35,369 iteration 5089 : loss : 0.020911, loss_ce: 0.007793
2022-01-12 00:39:36,984 iteration 5090 : loss : 0.021102, loss_ce: 0.008836
2022-01-12 00:39:38,620 iteration 5091 : loss : 0.020468, loss_ce: 0.007824
2022-01-12 00:39:40,204 iteration 5092 : loss : 0.020453, loss_ce: 0.005477
2022-01-12 00:39:41,806 iteration 5093 : loss : 0.018306, loss_ce: 0.005901
2022-01-12 00:39:43,428 iteration 5094 : loss : 0.015594, loss_ce: 0.005113
2022-01-12 00:39:45,045 iteration 5095 : loss : 0.012740, loss_ce: 0.005508
2022-01-12 00:39:46,747 iteration 5096 : loss : 0.017245, loss_ce: 0.004799
2022-01-12 00:39:48,406 iteration 5097 : loss : 0.024709, loss_ce: 0.009531
2022-01-12 00:39:49,996 iteration 5098 : loss : 0.016202, loss_ce: 0.006969
2022-01-12 00:39:51,635 iteration 5099 : loss : 0.015417, loss_ce: 0.005841
2022-01-12 00:39:51,635 Training Data Eval:
2022-01-12 00:39:59,619   Average segmentation loss on training set: 0.0096
2022-01-12 00:39:59,619 Validation Data Eval:
2022-01-12 00:40:02,367   Average segmentation loss on validation set: 0.0782
2022-01-12 00:40:03,925 iteration 5100 : loss : 0.021429, loss_ce: 0.006554
 75%|█████████████████████▊       | 300/400 [2:28:12<51:58, 31.19s/it]2022-01-12 00:40:05,592 iteration 5101 : loss : 0.021954, loss_ce: 0.004500
2022-01-12 00:40:07,273 iteration 5102 : loss : 0.020434, loss_ce: 0.009630
2022-01-12 00:40:08,828 iteration 5103 : loss : 0.016547, loss_ce: 0.006361
2022-01-12 00:40:10,428 iteration 5104 : loss : 0.024494, loss_ce: 0.006981
2022-01-12 00:40:11,987 iteration 5105 : loss : 0.014553, loss_ce: 0.004374
2022-01-12 00:40:13,575 iteration 5106 : loss : 0.023590, loss_ce: 0.008777
2022-01-12 00:40:15,200 iteration 5107 : loss : 0.022893, loss_ce: 0.009939
2022-01-12 00:40:16,825 iteration 5108 : loss : 0.019369, loss_ce: 0.008486
2022-01-12 00:40:18,417 iteration 5109 : loss : 0.011795, loss_ce: 0.004270
2022-01-12 00:40:19,906 iteration 5110 : loss : 0.011979, loss_ce: 0.004615
2022-01-12 00:40:21,515 iteration 5111 : loss : 0.019731, loss_ce: 0.006055
2022-01-12 00:40:23,049 iteration 5112 : loss : 0.012210, loss_ce: 0.005050
2022-01-12 00:40:24,724 iteration 5113 : loss : 0.015985, loss_ce: 0.006057
2022-01-12 00:40:26,275 iteration 5114 : loss : 0.011734, loss_ce: 0.004392
2022-01-12 00:40:27,846 iteration 5115 : loss : 0.011979, loss_ce: 0.005106
2022-01-12 00:40:29,451 iteration 5116 : loss : 0.018734, loss_ce: 0.006679
2022-01-12 00:40:31,060 iteration 5117 : loss : 0.016592, loss_ce: 0.005389
 75%|█████████████████████▊       | 301/400 [2:28:39<49:26, 29.97s/it]2022-01-12 00:40:32,724 iteration 5118 : loss : 0.014397, loss_ce: 0.005821
2022-01-12 00:40:34,329 iteration 5119 : loss : 0.022328, loss_ce: 0.007679
2022-01-12 00:40:36,025 iteration 5120 : loss : 0.012203, loss_ce: 0.003318
2022-01-12 00:40:37,588 iteration 5121 : loss : 0.014980, loss_ce: 0.005987
2022-01-12 00:40:39,148 iteration 5122 : loss : 0.017492, loss_ce: 0.005030
2022-01-12 00:40:40,755 iteration 5123 : loss : 0.017856, loss_ce: 0.009529
2022-01-12 00:40:42,304 iteration 5124 : loss : 0.014433, loss_ce: 0.003167
2022-01-12 00:40:43,908 iteration 5125 : loss : 0.020570, loss_ce: 0.008077
2022-01-12 00:40:45,471 iteration 5126 : loss : 0.014639, loss_ce: 0.005729
2022-01-12 00:40:47,086 iteration 5127 : loss : 0.041717, loss_ce: 0.015333
2022-01-12 00:40:48,809 iteration 5128 : loss : 0.017340, loss_ce: 0.005926
2022-01-12 00:40:50,478 iteration 5129 : loss : 0.020610, loss_ce: 0.007634
2022-01-12 00:40:52,007 iteration 5130 : loss : 0.014633, loss_ce: 0.005858
2022-01-12 00:40:53,628 iteration 5131 : loss : 0.016829, loss_ce: 0.005538
2022-01-12 00:40:55,240 iteration 5132 : loss : 0.021262, loss_ce: 0.010256
2022-01-12 00:40:56,787 iteration 5133 : loss : 0.018267, loss_ce: 0.007490
2022-01-12 00:40:58,315 iteration 5134 : loss : 0.012088, loss_ce: 0.004253
 76%|█████████████████████▉       | 302/400 [2:29:06<47:37, 29.16s/it]2022-01-12 00:40:59,935 iteration 5135 : loss : 0.014370, loss_ce: 0.005167
2022-01-12 00:41:01,615 iteration 5136 : loss : 0.020320, loss_ce: 0.007613
2022-01-12 00:41:03,303 iteration 5137 : loss : 0.027168, loss_ce: 0.010297
2022-01-12 00:41:05,019 iteration 5138 : loss : 0.030816, loss_ce: 0.014980
2022-01-12 00:41:06,552 iteration 5139 : loss : 0.011095, loss_ce: 0.003461
2022-01-12 00:41:08,205 iteration 5140 : loss : 0.022550, loss_ce: 0.010724
2022-01-12 00:41:09,733 iteration 5141 : loss : 0.017789, loss_ce: 0.008153
2022-01-12 00:41:11,433 iteration 5142 : loss : 0.021527, loss_ce: 0.006849
2022-01-12 00:41:12,970 iteration 5143 : loss : 0.012688, loss_ce: 0.005007
2022-01-12 00:41:14,495 iteration 5144 : loss : 0.013224, loss_ce: 0.005900
2022-01-12 00:41:16,119 iteration 5145 : loss : 0.022993, loss_ce: 0.008278
2022-01-12 00:41:17,867 iteration 5146 : loss : 0.030635, loss_ce: 0.008010
2022-01-12 00:41:19,483 iteration 5147 : loss : 0.015315, loss_ce: 0.008630
2022-01-12 00:41:21,111 iteration 5148 : loss : 0.030120, loss_ce: 0.013055
2022-01-12 00:41:22,686 iteration 5149 : loss : 0.016477, loss_ce: 0.005671
2022-01-12 00:41:24,259 iteration 5150 : loss : 0.014830, loss_ce: 0.006286
2022-01-12 00:41:25,797 iteration 5151 : loss : 0.013819, loss_ce: 0.004766
 76%|█████████████████████▉       | 303/400 [2:29:34<46:19, 28.65s/it]2022-01-12 00:41:27,478 iteration 5152 : loss : 0.024116, loss_ce: 0.007720
2022-01-12 00:41:29,004 iteration 5153 : loss : 0.012310, loss_ce: 0.006057
2022-01-12 00:41:30,643 iteration 5154 : loss : 0.021030, loss_ce: 0.007848
2022-01-12 00:41:32,175 iteration 5155 : loss : 0.009250, loss_ce: 0.003281
2022-01-12 00:41:33,745 iteration 5156 : loss : 0.017567, loss_ce: 0.006267
2022-01-12 00:41:35,426 iteration 5157 : loss : 0.014985, loss_ce: 0.005710
2022-01-12 00:41:37,049 iteration 5158 : loss : 0.019412, loss_ce: 0.008263
2022-01-12 00:41:38,704 iteration 5159 : loss : 0.027891, loss_ce: 0.007892
2022-01-12 00:41:40,375 iteration 5160 : loss : 0.016355, loss_ce: 0.006518
2022-01-12 00:41:42,038 iteration 5161 : loss : 0.017681, loss_ce: 0.007327
2022-01-12 00:41:43,601 iteration 5162 : loss : 0.011883, loss_ce: 0.003320
2022-01-12 00:41:45,185 iteration 5163 : loss : 0.018008, loss_ce: 0.006199
2022-01-12 00:41:46,706 iteration 5164 : loss : 0.018165, loss_ce: 0.009203
2022-01-12 00:41:48,316 iteration 5165 : loss : 0.014008, loss_ce: 0.004099
2022-01-12 00:41:49,928 iteration 5166 : loss : 0.014628, loss_ce: 0.006187
2022-01-12 00:41:51,477 iteration 5167 : loss : 0.013897, loss_ce: 0.005694
2022-01-12 00:41:53,133 iteration 5168 : loss : 0.019385, loss_ce: 0.008135
 76%|██████████████████████       | 304/400 [2:30:01<45:12, 28.26s/it]2022-01-12 00:41:54,726 iteration 5169 : loss : 0.016338, loss_ce: 0.006187
2022-01-12 00:41:56,323 iteration 5170 : loss : 0.015097, loss_ce: 0.007044
2022-01-12 00:41:57,897 iteration 5171 : loss : 0.017468, loss_ce: 0.004183
2022-01-12 00:41:59,502 iteration 5172 : loss : 0.014090, loss_ce: 0.004724
2022-01-12 00:42:01,061 iteration 5173 : loss : 0.016456, loss_ce: 0.007295
2022-01-12 00:42:02,688 iteration 5174 : loss : 0.023052, loss_ce: 0.008620
2022-01-12 00:42:04,375 iteration 5175 : loss : 0.028251, loss_ce: 0.009748
2022-01-12 00:42:05,979 iteration 5176 : loss : 0.022176, loss_ce: 0.008944
2022-01-12 00:42:07,721 iteration 5177 : loss : 0.027215, loss_ce: 0.012300
2022-01-12 00:42:09,303 iteration 5178 : loss : 0.026685, loss_ce: 0.005066
2022-01-12 00:42:10,966 iteration 5179 : loss : 0.017232, loss_ce: 0.007536
2022-01-12 00:42:12,580 iteration 5180 : loss : 0.017822, loss_ce: 0.007730
2022-01-12 00:42:14,232 iteration 5181 : loss : 0.018840, loss_ce: 0.007196
2022-01-12 00:42:15,893 iteration 5182 : loss : 0.026410, loss_ce: 0.008866
2022-01-12 00:42:17,557 iteration 5183 : loss : 0.022893, loss_ce: 0.006752
2022-01-12 00:42:19,260 iteration 5184 : loss : 0.022471, loss_ce: 0.013355
2022-01-12 00:42:19,260 Training Data Eval:
2022-01-12 00:42:27,212   Average segmentation loss on training set: 0.0099
2022-01-12 00:42:27,212 Validation Data Eval:
2022-01-12 00:42:29,953   Average segmentation loss on validation set: 0.0709
2022-01-12 00:42:31,559 iteration 5185 : loss : 0.018205, loss_ce: 0.008843
 76%|██████████████████████       | 305/400 [2:30:40<49:34, 31.31s/it]2022-01-12 00:42:33,213 iteration 5186 : loss : 0.015381, loss_ce: 0.006453
2022-01-12 00:42:34,877 iteration 5187 : loss : 0.014670, loss_ce: 0.005826
2022-01-12 00:42:36,396 iteration 5188 : loss : 0.018857, loss_ce: 0.005429
2022-01-12 00:42:38,001 iteration 5189 : loss : 0.013676, loss_ce: 0.005715
2022-01-12 00:42:39,521 iteration 5190 : loss : 0.017614, loss_ce: 0.007473
2022-01-12 00:42:41,067 iteration 5191 : loss : 0.015049, loss_ce: 0.005845
2022-01-12 00:42:42,712 iteration 5192 : loss : 0.031382, loss_ce: 0.010759
2022-01-12 00:42:44,343 iteration 5193 : loss : 0.024735, loss_ce: 0.011904
2022-01-12 00:42:45,938 iteration 5194 : loss : 0.021515, loss_ce: 0.008015
2022-01-12 00:42:47,557 iteration 5195 : loss : 0.019264, loss_ce: 0.005760
2022-01-12 00:42:49,309 iteration 5196 : loss : 0.043696, loss_ce: 0.010356
2022-01-12 00:42:50,804 iteration 5197 : loss : 0.014904, loss_ce: 0.004117
2022-01-12 00:42:52,464 iteration 5198 : loss : 0.029935, loss_ce: 0.010825
2022-01-12 00:42:54,110 iteration 5199 : loss : 0.028027, loss_ce: 0.011873
2022-01-12 00:42:55,742 iteration 5200 : loss : 0.019927, loss_ce: 0.006975
2022-01-12 00:42:57,310 iteration 5201 : loss : 0.013267, loss_ce: 0.004768
2022-01-12 00:42:58,921 iteration 5202 : loss : 0.023955, loss_ce: 0.009060
 76%|██████████████████████▏      | 306/400 [2:31:07<47:11, 30.13s/it]2022-01-12 00:43:00,451 iteration 5203 : loss : 0.014132, loss_ce: 0.004612
2022-01-12 00:43:02,081 iteration 5204 : loss : 0.022203, loss_ce: 0.008635
2022-01-12 00:43:03,715 iteration 5205 : loss : 0.022309, loss_ce: 0.010515
2022-01-12 00:43:05,243 iteration 5206 : loss : 0.015544, loss_ce: 0.006780
2022-01-12 00:43:06,832 iteration 5207 : loss : 0.019517, loss_ce: 0.008260
2022-01-12 00:43:08,505 iteration 5208 : loss : 0.019330, loss_ce: 0.007713
2022-01-12 00:43:10,119 iteration 5209 : loss : 0.017664, loss_ce: 0.006699
2022-01-12 00:43:11,696 iteration 5210 : loss : 0.016084, loss_ce: 0.005495
2022-01-12 00:43:13,250 iteration 5211 : loss : 0.015404, loss_ce: 0.004352
2022-01-12 00:43:14,854 iteration 5212 : loss : 0.017829, loss_ce: 0.005837
2022-01-12 00:43:16,493 iteration 5213 : loss : 0.046226, loss_ce: 0.013047
2022-01-12 00:43:18,083 iteration 5214 : loss : 0.022741, loss_ce: 0.014647
2022-01-12 00:43:19,661 iteration 5215 : loss : 0.026417, loss_ce: 0.009938
2022-01-12 00:43:21,232 iteration 5216 : loss : 0.015268, loss_ce: 0.005962
2022-01-12 00:43:22,766 iteration 5217 : loss : 0.012841, loss_ce: 0.004163
2022-01-12 00:43:24,338 iteration 5218 : loss : 0.015880, loss_ce: 0.005530
2022-01-12 00:43:25,962 iteration 5219 : loss : 0.023716, loss_ce: 0.010945
 77%|██████████████████████▎      | 307/400 [2:31:34<45:15, 29.20s/it]2022-01-12 00:43:27,680 iteration 5220 : loss : 0.022815, loss_ce: 0.008256
2022-01-12 00:43:29,251 iteration 5221 : loss : 0.013002, loss_ce: 0.004889
2022-01-12 00:43:30,750 iteration 5222 : loss : 0.013348, loss_ce: 0.003540
2022-01-12 00:43:32,326 iteration 5223 : loss : 0.017279, loss_ce: 0.007261
2022-01-12 00:43:34,009 iteration 5224 : loss : 0.024034, loss_ce: 0.009204
2022-01-12 00:43:35,535 iteration 5225 : loss : 0.013648, loss_ce: 0.003766
2022-01-12 00:43:37,265 iteration 5226 : loss : 0.036186, loss_ce: 0.013859
2022-01-12 00:43:38,754 iteration 5227 : loss : 0.011681, loss_ce: 0.004544
2022-01-12 00:43:40,250 iteration 5228 : loss : 0.012825, loss_ce: 0.005539
2022-01-12 00:43:41,800 iteration 5229 : loss : 0.019722, loss_ce: 0.008217
2022-01-12 00:43:43,415 iteration 5230 : loss : 0.022476, loss_ce: 0.012532
2022-01-12 00:43:45,036 iteration 5231 : loss : 0.028229, loss_ce: 0.008149
2022-01-12 00:43:46,581 iteration 5232 : loss : 0.019774, loss_ce: 0.007639
2022-01-12 00:43:48,210 iteration 5233 : loss : 0.021568, loss_ce: 0.009833
2022-01-12 00:43:49,712 iteration 5234 : loss : 0.012365, loss_ce: 0.004278
2022-01-12 00:43:51,400 iteration 5235 : loss : 0.027499, loss_ce: 0.009838
2022-01-12 00:43:52,975 iteration 5236 : loss : 0.021740, loss_ce: 0.008246
 77%|██████████████████████▎      | 308/400 [2:32:01<43:45, 28.54s/it]2022-01-12 00:43:54,648 iteration 5237 : loss : 0.021281, loss_ce: 0.007969
2022-01-12 00:43:56,257 iteration 5238 : loss : 0.017366, loss_ce: 0.007972
2022-01-12 00:43:57,862 iteration 5239 : loss : 0.014378, loss_ce: 0.004061
2022-01-12 00:43:59,572 iteration 5240 : loss : 0.016681, loss_ce: 0.006107
2022-01-12 00:44:01,206 iteration 5241 : loss : 0.020465, loss_ce: 0.008578
2022-01-12 00:44:02,844 iteration 5242 : loss : 0.021862, loss_ce: 0.010021
2022-01-12 00:44:04,473 iteration 5243 : loss : 0.016694, loss_ce: 0.005461
2022-01-12 00:44:06,097 iteration 5244 : loss : 0.018347, loss_ce: 0.006161
2022-01-12 00:44:07,697 iteration 5245 : loss : 0.024235, loss_ce: 0.010225
2022-01-12 00:44:09,181 iteration 5246 : loss : 0.010580, loss_ce: 0.004887
2022-01-12 00:44:10,769 iteration 5247 : loss : 0.016662, loss_ce: 0.006389
2022-01-12 00:44:12,419 iteration 5248 : loss : 0.015638, loss_ce: 0.005225
2022-01-12 00:44:13,992 iteration 5249 : loss : 0.026159, loss_ce: 0.009349
2022-01-12 00:44:15,581 iteration 5250 : loss : 0.018084, loss_ce: 0.008339
2022-01-12 00:44:17,248 iteration 5251 : loss : 0.014833, loss_ce: 0.005276
2022-01-12 00:44:18,830 iteration 5252 : loss : 0.014003, loss_ce: 0.004324
2022-01-12 00:44:20,475 iteration 5253 : loss : 0.012547, loss_ce: 0.004950
 77%|██████████████████████▍      | 309/400 [2:32:29<42:48, 28.23s/it]2022-01-12 00:44:22,028 iteration 5254 : loss : 0.013813, loss_ce: 0.004764
2022-01-12 00:44:23,704 iteration 5255 : loss : 0.015953, loss_ce: 0.004556
2022-01-12 00:44:25,317 iteration 5256 : loss : 0.014208, loss_ce: 0.005700
2022-01-12 00:44:27,021 iteration 5257 : loss : 0.024497, loss_ce: 0.006308
2022-01-12 00:44:28,580 iteration 5258 : loss : 0.023609, loss_ce: 0.010697
2022-01-12 00:44:30,228 iteration 5259 : loss : 0.018960, loss_ce: 0.005775
2022-01-12 00:44:31,809 iteration 5260 : loss : 0.021720, loss_ce: 0.008639
2022-01-12 00:44:33,392 iteration 5261 : loss : 0.013644, loss_ce: 0.005995
2022-01-12 00:44:34,925 iteration 5262 : loss : 0.010126, loss_ce: 0.004684
2022-01-12 00:44:36,542 iteration 5263 : loss : 0.019716, loss_ce: 0.007442
2022-01-12 00:44:38,109 iteration 5264 : loss : 0.012893, loss_ce: 0.004270
2022-01-12 00:44:39,617 iteration 5265 : loss : 0.012991, loss_ce: 0.004522
2022-01-12 00:44:41,217 iteration 5266 : loss : 0.021894, loss_ce: 0.009917
2022-01-12 00:44:42,743 iteration 5267 : loss : 0.060749, loss_ce: 0.034729
2022-01-12 00:44:44,413 iteration 5268 : loss : 0.019915, loss_ce: 0.008676
2022-01-12 00:44:46,095 iteration 5269 : loss : 0.016050, loss_ce: 0.006803
2022-01-12 00:44:46,096 Training Data Eval:
2022-01-12 00:44:54,064   Average segmentation loss on training set: 0.0103
2022-01-12 00:44:54,064 Validation Data Eval:
2022-01-12 00:44:56,802   Average segmentation loss on validation set: 0.0760
2022-01-12 00:44:58,439 iteration 5270 : loss : 0.026278, loss_ce: 0.011328
 78%|██████████████████████▍      | 310/400 [2:33:07<46:43, 31.15s/it]2022-01-12 00:45:00,136 iteration 5271 : loss : 0.016051, loss_ce: 0.006908
2022-01-12 00:45:01,745 iteration 5272 : loss : 0.024443, loss_ce: 0.009335
2022-01-12 00:45:03,335 iteration 5273 : loss : 0.013967, loss_ce: 0.006059
2022-01-12 00:45:04,878 iteration 5274 : loss : 0.014317, loss_ce: 0.004966
2022-01-12 00:45:06,512 iteration 5275 : loss : 0.018253, loss_ce: 0.005274
2022-01-12 00:45:08,129 iteration 5276 : loss : 0.014642, loss_ce: 0.004445
2022-01-12 00:45:09,657 iteration 5277 : loss : 0.017209, loss_ce: 0.004598
2022-01-12 00:45:11,392 iteration 5278 : loss : 0.023207, loss_ce: 0.010576
2022-01-12 00:45:12,980 iteration 5279 : loss : 0.014779, loss_ce: 0.004402
2022-01-12 00:45:14,646 iteration 5280 : loss : 0.026890, loss_ce: 0.009919
2022-01-12 00:45:16,240 iteration 5281 : loss : 0.027406, loss_ce: 0.017607
2022-01-12 00:45:17,868 iteration 5282 : loss : 0.017063, loss_ce: 0.006594
2022-01-12 00:45:19,468 iteration 5283 : loss : 0.014629, loss_ce: 0.004725
2022-01-12 00:45:21,094 iteration 5284 : loss : 0.019806, loss_ce: 0.009293
2022-01-12 00:45:22,728 iteration 5285 : loss : 0.018846, loss_ce: 0.008529
2022-01-12 00:45:24,383 iteration 5286 : loss : 0.012381, loss_ce: 0.004737
2022-01-12 00:45:26,105 iteration 5287 : loss : 0.028165, loss_ce: 0.014053
 78%|██████████████████████▌      | 311/400 [2:33:34<44:39, 30.11s/it]2022-01-12 00:45:27,693 iteration 5288 : loss : 0.015047, loss_ce: 0.005618
2022-01-12 00:45:29,315 iteration 5289 : loss : 0.014917, loss_ce: 0.005059
2022-01-12 00:45:30,969 iteration 5290 : loss : 0.026731, loss_ce: 0.009973
2022-01-12 00:45:32,630 iteration 5291 : loss : 0.020121, loss_ce: 0.007308
2022-01-12 00:45:34,284 iteration 5292 : loss : 0.014249, loss_ce: 0.005518
2022-01-12 00:45:35,859 iteration 5293 : loss : 0.014400, loss_ce: 0.004708
2022-01-12 00:45:37,536 iteration 5294 : loss : 0.021243, loss_ce: 0.008741
2022-01-12 00:45:39,108 iteration 5295 : loss : 0.011942, loss_ce: 0.004383
2022-01-12 00:45:40,710 iteration 5296 : loss : 0.016798, loss_ce: 0.005485
2022-01-12 00:45:42,410 iteration 5297 : loss : 0.021801, loss_ce: 0.007685
2022-01-12 00:45:43,974 iteration 5298 : loss : 0.014694, loss_ce: 0.007156
2022-01-12 00:45:45,639 iteration 5299 : loss : 0.018862, loss_ce: 0.008532
2022-01-12 00:45:47,158 iteration 5300 : loss : 0.019037, loss_ce: 0.006699
2022-01-12 00:45:48,769 iteration 5301 : loss : 0.047848, loss_ce: 0.017867
2022-01-12 00:45:50,329 iteration 5302 : loss : 0.013055, loss_ce: 0.005430
2022-01-12 00:45:51,817 iteration 5303 : loss : 0.012737, loss_ce: 0.004572
2022-01-12 00:45:53,464 iteration 5304 : loss : 0.030459, loss_ce: 0.012563
 78%|██████████████████████▌      | 312/400 [2:34:02<42:56, 29.28s/it]2022-01-12 00:45:55,107 iteration 5305 : loss : 0.021609, loss_ce: 0.007755
2022-01-12 00:45:56,755 iteration 5306 : loss : 0.024628, loss_ce: 0.010440
2022-01-12 00:45:58,347 iteration 5307 : loss : 0.016108, loss_ce: 0.006221
2022-01-12 00:45:59,997 iteration 5308 : loss : 0.014359, loss_ce: 0.006111
2022-01-12 00:46:01,581 iteration 5309 : loss : 0.020429, loss_ce: 0.006872
2022-01-12 00:46:03,176 iteration 5310 : loss : 0.017152, loss_ce: 0.007441
2022-01-12 00:46:04,793 iteration 5311 : loss : 0.016417, loss_ce: 0.006502
2022-01-12 00:46:06,393 iteration 5312 : loss : 0.026970, loss_ce: 0.009915
2022-01-12 00:46:08,038 iteration 5313 : loss : 0.029453, loss_ce: 0.007533
2022-01-12 00:46:09,586 iteration 5314 : loss : 0.014455, loss_ce: 0.006211
2022-01-12 00:46:11,238 iteration 5315 : loss : 0.016912, loss_ce: 0.006578
2022-01-12 00:46:12,706 iteration 5316 : loss : 0.012156, loss_ce: 0.005462
2022-01-12 00:46:14,322 iteration 5317 : loss : 0.019882, loss_ce: 0.008705
2022-01-12 00:46:15,966 iteration 5318 : loss : 0.017538, loss_ce: 0.005861
2022-01-12 00:46:17,620 iteration 5319 : loss : 0.029235, loss_ce: 0.009319
2022-01-12 00:46:19,274 iteration 5320 : loss : 0.023688, loss_ce: 0.008063
2022-01-12 00:46:20,815 iteration 5321 : loss : 0.019175, loss_ce: 0.003875
 78%|██████████████████████▋      | 313/400 [2:34:29<41:37, 28.70s/it]2022-01-12 00:46:22,507 iteration 5322 : loss : 0.015992, loss_ce: 0.006725
2022-01-12 00:46:24,186 iteration 5323 : loss : 0.015147, loss_ce: 0.005921
2022-01-12 00:46:25,795 iteration 5324 : loss : 0.013531, loss_ce: 0.005562
2022-01-12 00:46:27,413 iteration 5325 : loss : 0.016270, loss_ce: 0.006273
2022-01-12 00:46:29,120 iteration 5326 : loss : 0.017258, loss_ce: 0.004618
2022-01-12 00:46:30,751 iteration 5327 : loss : 0.025660, loss_ce: 0.010930
2022-01-12 00:46:32,349 iteration 5328 : loss : 0.015490, loss_ce: 0.006273
2022-01-12 00:46:33,945 iteration 5329 : loss : 0.026861, loss_ce: 0.012680
2022-01-12 00:46:35,543 iteration 5330 : loss : 0.020888, loss_ce: 0.007366
2022-01-12 00:46:37,088 iteration 5331 : loss : 0.013292, loss_ce: 0.005084
2022-01-12 00:46:38,887 iteration 5332 : loss : 0.030672, loss_ce: 0.012502
2022-01-12 00:46:40,499 iteration 5333 : loss : 0.017930, loss_ce: 0.006369
2022-01-12 00:46:41,991 iteration 5334 : loss : 0.012378, loss_ce: 0.003857
2022-01-12 00:46:43,593 iteration 5335 : loss : 0.016486, loss_ce: 0.005209
2022-01-12 00:46:45,169 iteration 5336 : loss : 0.016878, loss_ce: 0.006106
2022-01-12 00:46:46,761 iteration 5337 : loss : 0.016109, loss_ce: 0.006196
2022-01-12 00:46:48,378 iteration 5338 : loss : 0.019828, loss_ce: 0.007015
 78%|██████████████████████▊      | 314/400 [2:34:57<40:39, 28.37s/it]2022-01-12 00:46:50,033 iteration 5339 : loss : 0.017652, loss_ce: 0.006395
2022-01-12 00:46:51,609 iteration 5340 : loss : 0.012809, loss_ce: 0.005048
2022-01-12 00:46:53,278 iteration 5341 : loss : 0.020278, loss_ce: 0.004944
2022-01-12 00:46:54,923 iteration 5342 : loss : 0.017221, loss_ce: 0.005646
2022-01-12 00:46:56,550 iteration 5343 : loss : 0.020357, loss_ce: 0.007290
2022-01-12 00:46:58,111 iteration 5344 : loss : 0.016123, loss_ce: 0.007340
2022-01-12 00:46:59,677 iteration 5345 : loss : 0.015709, loss_ce: 0.005622
2022-01-12 00:47:01,236 iteration 5346 : loss : 0.015231, loss_ce: 0.005870
2022-01-12 00:47:02,849 iteration 5347 : loss : 0.011229, loss_ce: 0.004671
2022-01-12 00:47:04,392 iteration 5348 : loss : 0.015621, loss_ce: 0.005735
2022-01-12 00:47:05,954 iteration 5349 : loss : 0.013240, loss_ce: 0.004709
2022-01-12 00:47:07,539 iteration 5350 : loss : 0.012405, loss_ce: 0.003715
2022-01-12 00:47:09,072 iteration 5351 : loss : 0.011570, loss_ce: 0.003820
2022-01-12 00:47:10,734 iteration 5352 : loss : 0.027933, loss_ce: 0.011190
2022-01-12 00:47:12,348 iteration 5353 : loss : 0.015664, loss_ce: 0.006151
2022-01-12 00:47:13,885 iteration 5354 : loss : 0.013518, loss_ce: 0.005588
2022-01-12 00:47:13,886 Training Data Eval:
2022-01-12 00:47:21,840   Average segmentation loss on training set: 0.0089
2022-01-12 00:47:21,840 Validation Data Eval:
2022-01-12 00:47:24,579   Average segmentation loss on validation set: 0.0657
2022-01-12 00:47:26,176 iteration 5355 : loss : 0.016024, loss_ce: 0.006605
 79%|██████████████████████▊      | 315/400 [2:35:34<44:11, 31.19s/it]2022-01-12 00:47:27,850 iteration 5356 : loss : 0.025130, loss_ce: 0.010025
2022-01-12 00:47:29,421 iteration 5357 : loss : 0.009359, loss_ce: 0.003100
2022-01-12 00:47:30,970 iteration 5358 : loss : 0.015515, loss_ce: 0.006162
2022-01-12 00:47:32,516 iteration 5359 : loss : 0.012766, loss_ce: 0.003970
2022-01-12 00:47:34,170 iteration 5360 : loss : 0.020306, loss_ce: 0.008035
2022-01-12 00:47:35,747 iteration 5361 : loss : 0.014296, loss_ce: 0.006494
2022-01-12 00:47:37,361 iteration 5362 : loss : 0.015335, loss_ce: 0.005443
2022-01-12 00:47:39,128 iteration 5363 : loss : 0.017580, loss_ce: 0.007059
2022-01-12 00:47:40,734 iteration 5364 : loss : 0.027174, loss_ce: 0.008322
2022-01-12 00:47:42,355 iteration 5365 : loss : 0.022066, loss_ce: 0.007622
2022-01-12 00:47:43,931 iteration 5366 : loss : 0.017318, loss_ce: 0.006210
2022-01-12 00:47:45,420 iteration 5367 : loss : 0.013032, loss_ce: 0.004521
2022-01-12 00:47:47,143 iteration 5368 : loss : 0.021584, loss_ce: 0.007762
2022-01-12 00:47:48,710 iteration 5369 : loss : 0.010451, loss_ce: 0.003954
2022-01-12 00:47:50,269 iteration 5370 : loss : 0.014859, loss_ce: 0.006528
2022-01-12 00:47:51,834 iteration 5371 : loss : 0.013245, loss_ce: 0.003925
2022-01-12 00:47:53,334 iteration 5372 : loss : 0.010595, loss_ce: 0.004121
 79%|██████████████████████▉      | 316/400 [2:36:02<41:58, 29.98s/it]2022-01-12 00:47:54,952 iteration 5373 : loss : 0.019202, loss_ce: 0.007483
2022-01-12 00:47:56,511 iteration 5374 : loss : 0.015535, loss_ce: 0.003706
2022-01-12 00:47:58,109 iteration 5375 : loss : 0.014681, loss_ce: 0.005649
2022-01-12 00:47:59,728 iteration 5376 : loss : 0.017145, loss_ce: 0.005814
2022-01-12 00:48:01,348 iteration 5377 : loss : 0.019740, loss_ce: 0.004732
2022-01-12 00:48:02,969 iteration 5378 : loss : 0.019767, loss_ce: 0.007394
2022-01-12 00:48:04,546 iteration 5379 : loss : 0.016650, loss_ce: 0.006280
2022-01-12 00:48:06,061 iteration 5380 : loss : 0.012849, loss_ce: 0.005585
2022-01-12 00:48:07,725 iteration 5381 : loss : 0.017340, loss_ce: 0.007400
2022-01-12 00:48:09,309 iteration 5382 : loss : 0.017620, loss_ce: 0.006699
2022-01-12 00:48:10,886 iteration 5383 : loss : 0.017727, loss_ce: 0.008369
2022-01-12 00:48:12,453 iteration 5384 : loss : 0.011367, loss_ce: 0.005371
2022-01-12 00:48:14,057 iteration 5385 : loss : 0.027338, loss_ce: 0.011181
2022-01-12 00:48:15,623 iteration 5386 : loss : 0.014715, loss_ce: 0.005508
2022-01-12 00:48:17,327 iteration 5387 : loss : 0.015495, loss_ce: 0.005746
2022-01-12 00:48:18,963 iteration 5388 : loss : 0.011466, loss_ce: 0.003666
2022-01-12 00:48:20,638 iteration 5389 : loss : 0.020770, loss_ce: 0.009116
 79%|██████████████████████▉      | 317/400 [2:36:29<40:21, 29.18s/it]2022-01-12 00:48:22,239 iteration 5390 : loss : 0.013714, loss_ce: 0.004894
2022-01-12 00:48:23,926 iteration 5391 : loss : 0.024112, loss_ce: 0.008528
2022-01-12 00:48:25,510 iteration 5392 : loss : 0.017733, loss_ce: 0.005346
2022-01-12 00:48:27,064 iteration 5393 : loss : 0.015880, loss_ce: 0.005619
2022-01-12 00:48:28,647 iteration 5394 : loss : 0.013172, loss_ce: 0.004003
2022-01-12 00:48:30,227 iteration 5395 : loss : 0.019484, loss_ce: 0.007076
2022-01-12 00:48:31,777 iteration 5396 : loss : 0.014839, loss_ce: 0.005803
2022-01-12 00:48:33,422 iteration 5397 : loss : 0.018644, loss_ce: 0.007764
2022-01-12 00:48:35,175 iteration 5398 : loss : 0.020361, loss_ce: 0.006520
2022-01-12 00:48:36,823 iteration 5399 : loss : 0.018362, loss_ce: 0.005420
2022-01-12 00:48:38,395 iteration 5400 : loss : 0.020454, loss_ce: 0.009011
2022-01-12 00:48:40,011 iteration 5401 : loss : 0.014964, loss_ce: 0.007747
2022-01-12 00:48:41,578 iteration 5402 : loss : 0.020834, loss_ce: 0.011097
2022-01-12 00:48:43,176 iteration 5403 : loss : 0.018589, loss_ce: 0.007735
2022-01-12 00:48:44,802 iteration 5404 : loss : 0.012720, loss_ce: 0.005872
2022-01-12 00:48:46,536 iteration 5405 : loss : 0.023440, loss_ce: 0.006413
2022-01-12 00:48:48,088 iteration 5406 : loss : 0.011665, loss_ce: 0.005300
 80%|███████████████████████      | 318/400 [2:36:56<39:09, 28.66s/it]2022-01-12 00:48:49,699 iteration 5407 : loss : 0.019098, loss_ce: 0.009000
2022-01-12 00:48:51,265 iteration 5408 : loss : 0.015391, loss_ce: 0.006104
2022-01-12 00:48:52,770 iteration 5409 : loss : 0.011458, loss_ce: 0.003887
2022-01-12 00:48:54,373 iteration 5410 : loss : 0.020004, loss_ce: 0.007691
2022-01-12 00:48:55,918 iteration 5411 : loss : 0.021443, loss_ce: 0.010671
2022-01-12 00:48:57,535 iteration 5412 : loss : 0.014951, loss_ce: 0.005486
2022-01-12 00:48:59,104 iteration 5413 : loss : 0.017403, loss_ce: 0.007132
2022-01-12 00:49:00,703 iteration 5414 : loss : 0.012001, loss_ce: 0.004591
2022-01-12 00:49:02,424 iteration 5415 : loss : 0.024474, loss_ce: 0.008466
2022-01-12 00:49:03,974 iteration 5416 : loss : 0.014512, loss_ce: 0.004752
2022-01-12 00:49:05,569 iteration 5417 : loss : 0.014157, loss_ce: 0.004679
2022-01-12 00:49:07,246 iteration 5418 : loss : 0.015054, loss_ce: 0.006076
2022-01-12 00:49:08,771 iteration 5419 : loss : 0.017272, loss_ce: 0.007229
2022-01-12 00:49:10,302 iteration 5420 : loss : 0.013718, loss_ce: 0.005761
2022-01-12 00:49:11,942 iteration 5421 : loss : 0.017534, loss_ce: 0.007026
2022-01-12 00:49:13,611 iteration 5422 : loss : 0.019567, loss_ce: 0.004125
2022-01-12 00:49:15,266 iteration 5423 : loss : 0.016633, loss_ce: 0.006604
 80%|███████████████████████▏     | 319/400 [2:37:23<38:05, 28.22s/it]2022-01-12 00:49:16,897 iteration 5424 : loss : 0.017047, loss_ce: 0.005452
2022-01-12 00:49:18,460 iteration 5425 : loss : 0.019902, loss_ce: 0.004827
2022-01-12 00:49:20,113 iteration 5426 : loss : 0.020170, loss_ce: 0.009176
2022-01-12 00:49:21,626 iteration 5427 : loss : 0.013321, loss_ce: 0.004255
2022-01-12 00:49:23,174 iteration 5428 : loss : 0.010271, loss_ce: 0.003814
2022-01-12 00:49:24,742 iteration 5429 : loss : 0.018645, loss_ce: 0.007330
2022-01-12 00:49:26,452 iteration 5430 : loss : 0.022071, loss_ce: 0.008167
2022-01-12 00:49:28,026 iteration 5431 : loss : 0.013695, loss_ce: 0.005368
2022-01-12 00:49:29,583 iteration 5432 : loss : 0.013416, loss_ce: 0.005708
2022-01-12 00:49:31,241 iteration 5433 : loss : 0.018063, loss_ce: 0.007221
2022-01-12 00:49:32,819 iteration 5434 : loss : 0.012457, loss_ce: 0.004291
2022-01-12 00:49:34,533 iteration 5435 : loss : 0.019479, loss_ce: 0.009506
2022-01-12 00:49:36,126 iteration 5436 : loss : 0.012455, loss_ce: 0.004864
2022-01-12 00:49:37,584 iteration 5437 : loss : 0.011316, loss_ce: 0.004110
2022-01-12 00:49:39,274 iteration 5438 : loss : 0.033379, loss_ce: 0.012432
2022-01-12 00:49:40,826 iteration 5439 : loss : 0.008733, loss_ce: 0.003398
2022-01-12 00:49:40,827 Training Data Eval:
2022-01-12 00:49:48,787   Average segmentation loss on training set: 0.0083
2022-01-12 00:49:48,788 Validation Data Eval:
2022-01-12 00:49:51,533   Average segmentation loss on validation set: 0.0724
2022-01-12 00:49:53,166 iteration 5440 : loss : 0.026162, loss_ce: 0.007952
 80%|███████████████████████▏     | 320/400 [2:38:01<41:29, 31.12s/it]2022-01-12 00:49:54,684 iteration 5441 : loss : 0.010824, loss_ce: 0.003169
2022-01-12 00:49:56,283 iteration 5442 : loss : 0.015134, loss_ce: 0.005136
2022-01-12 00:49:57,912 iteration 5443 : loss : 0.010408, loss_ce: 0.003372
2022-01-12 00:49:59,535 iteration 5444 : loss : 0.013281, loss_ce: 0.004825
2022-01-12 00:50:01,241 iteration 5445 : loss : 0.018760, loss_ce: 0.008846
2022-01-12 00:50:02,805 iteration 5446 : loss : 0.016951, loss_ce: 0.005333
2022-01-12 00:50:04,380 iteration 5447 : loss : 0.014155, loss_ce: 0.006043
2022-01-12 00:50:06,001 iteration 5448 : loss : 0.019403, loss_ce: 0.007881
2022-01-12 00:50:07,580 iteration 5449 : loss : 0.017180, loss_ce: 0.005742
2022-01-12 00:50:09,234 iteration 5450 : loss : 0.024476, loss_ce: 0.013080
2022-01-12 00:50:10,820 iteration 5451 : loss : 0.014570, loss_ce: 0.005839
2022-01-12 00:50:12,476 iteration 5452 : loss : 0.025467, loss_ce: 0.011034
2022-01-12 00:50:14,091 iteration 5453 : loss : 0.016460, loss_ce: 0.006045
2022-01-12 00:50:15,577 iteration 5454 : loss : 0.009192, loss_ce: 0.004229
2022-01-12 00:50:17,286 iteration 5455 : loss : 0.027666, loss_ce: 0.008233
2022-01-12 00:50:18,949 iteration 5456 : loss : 0.013816, loss_ce: 0.005650
2022-01-12 00:50:20,545 iteration 5457 : loss : 0.019303, loss_ce: 0.005986
 80%|███████████████████████▎     | 321/400 [2:38:29<39:29, 30.00s/it]2022-01-12 00:50:22,157 iteration 5458 : loss : 0.014535, loss_ce: 0.005287
2022-01-12 00:50:23,710 iteration 5459 : loss : 0.014420, loss_ce: 0.005344
2022-01-12 00:50:25,334 iteration 5460 : loss : 0.023358, loss_ce: 0.008569
2022-01-12 00:50:26,978 iteration 5461 : loss : 0.017486, loss_ce: 0.007407
2022-01-12 00:50:28,581 iteration 5462 : loss : 0.015113, loss_ce: 0.003951
2022-01-12 00:50:30,155 iteration 5463 : loss : 0.014335, loss_ce: 0.005462
2022-01-12 00:50:31,733 iteration 5464 : loss : 0.018117, loss_ce: 0.006391
2022-01-12 00:50:33,269 iteration 5465 : loss : 0.014298, loss_ce: 0.004480
2022-01-12 00:50:34,947 iteration 5466 : loss : 0.017978, loss_ce: 0.006702
2022-01-12 00:50:36,522 iteration 5467 : loss : 0.013643, loss_ce: 0.006413
2022-01-12 00:50:38,139 iteration 5468 : loss : 0.025862, loss_ce: 0.011168
2022-01-12 00:50:39,671 iteration 5469 : loss : 0.010881, loss_ce: 0.003480
2022-01-12 00:50:41,315 iteration 5470 : loss : 0.016427, loss_ce: 0.006305
2022-01-12 00:50:42,917 iteration 5471 : loss : 0.019528, loss_ce: 0.005602
2022-01-12 00:50:44,573 iteration 5472 : loss : 0.013476, loss_ce: 0.004413
2022-01-12 00:50:46,211 iteration 5473 : loss : 0.017263, loss_ce: 0.005921
2022-01-12 00:50:47,851 iteration 5474 : loss : 0.016329, loss_ce: 0.007238
 80%|███████████████████████▎     | 322/400 [2:38:56<37:56, 29.19s/it]2022-01-12 00:50:49,481 iteration 5475 : loss : 0.013011, loss_ce: 0.003273
2022-01-12 00:50:51,057 iteration 5476 : loss : 0.023027, loss_ce: 0.008315
2022-01-12 00:50:52,616 iteration 5477 : loss : 0.014072, loss_ce: 0.005954
2022-01-12 00:50:54,210 iteration 5478 : loss : 0.014567, loss_ce: 0.007220
2022-01-12 00:50:55,913 iteration 5479 : loss : 0.015524, loss_ce: 0.007327
2022-01-12 00:50:57,585 iteration 5480 : loss : 0.028804, loss_ce: 0.011885
2022-01-12 00:50:59,188 iteration 5481 : loss : 0.016165, loss_ce: 0.005595
2022-01-12 00:51:00,770 iteration 5482 : loss : 0.010597, loss_ce: 0.003377
2022-01-12 00:51:02,385 iteration 5483 : loss : 0.016066, loss_ce: 0.006439
2022-01-12 00:51:03,958 iteration 5484 : loss : 0.012663, loss_ce: 0.005024
2022-01-12 00:51:05,450 iteration 5485 : loss : 0.011626, loss_ce: 0.004151
2022-01-12 00:51:07,140 iteration 5486 : loss : 0.020712, loss_ce: 0.009207
2022-01-12 00:51:08,917 iteration 5487 : loss : 0.033683, loss_ce: 0.012207
2022-01-12 00:51:10,524 iteration 5488 : loss : 0.020005, loss_ce: 0.007288
2022-01-12 00:51:12,115 iteration 5489 : loss : 0.013658, loss_ce: 0.005443
2022-01-12 00:51:13,669 iteration 5490 : loss : 0.023486, loss_ce: 0.007535
2022-01-12 00:51:15,242 iteration 5491 : loss : 0.013786, loss_ce: 0.005458
 81%|███████████████████████▍     | 323/400 [2:39:23<36:45, 28.65s/it]2022-01-12 00:51:16,878 iteration 5492 : loss : 0.014552, loss_ce: 0.004886
2022-01-12 00:51:18,446 iteration 5493 : loss : 0.013680, loss_ce: 0.005633
2022-01-12 00:51:19,975 iteration 5494 : loss : 0.020133, loss_ce: 0.005209
2022-01-12 00:51:21,637 iteration 5495 : loss : 0.022525, loss_ce: 0.003337
2022-01-12 00:51:23,176 iteration 5496 : loss : 0.011900, loss_ce: 0.003551
2022-01-12 00:51:24,760 iteration 5497 : loss : 0.010649, loss_ce: 0.003300
2022-01-12 00:51:26,384 iteration 5498 : loss : 0.019847, loss_ce: 0.010753
2022-01-12 00:51:27,978 iteration 5499 : loss : 0.021714, loss_ce: 0.008230
2022-01-12 00:51:29,606 iteration 5500 : loss : 0.016499, loss_ce: 0.006532
2022-01-12 00:51:31,298 iteration 5501 : loss : 0.019772, loss_ce: 0.008356
2022-01-12 00:51:32,952 iteration 5502 : loss : 0.016799, loss_ce: 0.006800
2022-01-12 00:51:34,530 iteration 5503 : loss : 0.017381, loss_ce: 0.005438
2022-01-12 00:51:36,149 iteration 5504 : loss : 0.018197, loss_ce: 0.008916
2022-01-12 00:51:37,759 iteration 5505 : loss : 0.015206, loss_ce: 0.006346
2022-01-12 00:51:39,303 iteration 5506 : loss : 0.014962, loss_ce: 0.005620
2022-01-12 00:51:40,929 iteration 5507 : loss : 0.020133, loss_ce: 0.006009
2022-01-12 00:51:42,534 iteration 5508 : loss : 0.013946, loss_ce: 0.005950
 81%|███████████████████████▍     | 324/400 [2:39:51<35:46, 28.24s/it]2022-01-12 00:51:44,168 iteration 5509 : loss : 0.015606, loss_ce: 0.005006
2022-01-12 00:51:45,910 iteration 5510 : loss : 0.018334, loss_ce: 0.005967
2022-01-12 00:51:47,523 iteration 5511 : loss : 0.019216, loss_ce: 0.009883
2022-01-12 00:51:49,188 iteration 5512 : loss : 0.018049, loss_ce: 0.003461
2022-01-12 00:51:50,753 iteration 5513 : loss : 0.019109, loss_ce: 0.003694
2022-01-12 00:51:52,401 iteration 5514 : loss : 0.015281, loss_ce: 0.005889
2022-01-12 00:51:54,096 iteration 5515 : loss : 0.025468, loss_ce: 0.007384
2022-01-12 00:51:55,602 iteration 5516 : loss : 0.015608, loss_ce: 0.007198
2022-01-12 00:51:57,245 iteration 5517 : loss : 0.021561, loss_ce: 0.006644
2022-01-12 00:51:58,818 iteration 5518 : loss : 0.035208, loss_ce: 0.013131
2022-01-12 00:52:00,396 iteration 5519 : loss : 0.016616, loss_ce: 0.004987
2022-01-12 00:52:02,029 iteration 5520 : loss : 0.019788, loss_ce: 0.007672
2022-01-12 00:52:03,772 iteration 5521 : loss : 0.024247, loss_ce: 0.009193
2022-01-12 00:52:05,303 iteration 5522 : loss : 0.013619, loss_ce: 0.006926
2022-01-12 00:52:06,915 iteration 5523 : loss : 0.015916, loss_ce: 0.006573
2022-01-12 00:52:08,548 iteration 5524 : loss : 0.017309, loss_ce: 0.007970
2022-01-12 00:52:08,549 Training Data Eval:
2022-01-12 00:52:16,513   Average segmentation loss on training set: 0.0092
2022-01-12 00:52:16,514 Validation Data Eval:
2022-01-12 00:52:19,252   Average segmentation loss on validation set: 0.0676
2022-01-12 00:52:20,904 iteration 5525 : loss : 0.019983, loss_ce: 0.008448
 81%|███████████████████████▌     | 325/400 [2:40:29<39:06, 31.28s/it]2022-01-12 00:52:22,640 iteration 5526 : loss : 0.017553, loss_ce: 0.006760
2022-01-12 00:52:24,206 iteration 5527 : loss : 0.015224, loss_ce: 0.005212
2022-01-12 00:52:25,840 iteration 5528 : loss : 0.017107, loss_ce: 0.007041
2022-01-12 00:52:27,424 iteration 5529 : loss : 0.014561, loss_ce: 0.004888
2022-01-12 00:52:28,960 iteration 5530 : loss : 0.020334, loss_ce: 0.004202
2022-01-12 00:52:30,574 iteration 5531 : loss : 0.011682, loss_ce: 0.004210
2022-01-12 00:52:32,084 iteration 5532 : loss : 0.012364, loss_ce: 0.004182
2022-01-12 00:52:33,713 iteration 5533 : loss : 0.014537, loss_ce: 0.006174
2022-01-12 00:52:35,341 iteration 5534 : loss : 0.019889, loss_ce: 0.006439
2022-01-12 00:52:36,955 iteration 5535 : loss : 0.014437, loss_ce: 0.005379
2022-01-12 00:52:38,558 iteration 5536 : loss : 0.019283, loss_ce: 0.005208
2022-01-12 00:52:40,157 iteration 5537 : loss : 0.017448, loss_ce: 0.005135
2022-01-12 00:52:41,740 iteration 5538 : loss : 0.015397, loss_ce: 0.008139
2022-01-12 00:52:43,351 iteration 5539 : loss : 0.015802, loss_ce: 0.006435
2022-01-12 00:52:44,973 iteration 5540 : loss : 0.013031, loss_ce: 0.004639
2022-01-12 00:52:46,495 iteration 5541 : loss : 0.014364, loss_ce: 0.005426
2022-01-12 00:52:48,068 iteration 5542 : loss : 0.013885, loss_ce: 0.005932
 82%|███████████████████████▋     | 326/400 [2:40:56<37:03, 30.04s/it]2022-01-12 00:52:49,758 iteration 5543 : loss : 0.019322, loss_ce: 0.008045
2022-01-12 00:52:51,488 iteration 5544 : loss : 0.018527, loss_ce: 0.006165
2022-01-12 00:52:53,158 iteration 5545 : loss : 0.017491, loss_ce: 0.006860
2022-01-12 00:52:54,675 iteration 5546 : loss : 0.010012, loss_ce: 0.004662
2022-01-12 00:52:56,300 iteration 5547 : loss : 0.016348, loss_ce: 0.006073
2022-01-12 00:52:57,857 iteration 5548 : loss : 0.015072, loss_ce: 0.005438
2022-01-12 00:52:59,439 iteration 5549 : loss : 0.013520, loss_ce: 0.005143
2022-01-12 00:53:01,052 iteration 5550 : loss : 0.023481, loss_ce: 0.010435
2022-01-12 00:53:02,688 iteration 5551 : loss : 0.013223, loss_ce: 0.003169
2022-01-12 00:53:04,392 iteration 5552 : loss : 0.027274, loss_ce: 0.007254
2022-01-12 00:53:06,017 iteration 5553 : loss : 0.012845, loss_ce: 0.004484
2022-01-12 00:53:07,716 iteration 5554 : loss : 0.028525, loss_ce: 0.009073
2022-01-12 00:53:09,400 iteration 5555 : loss : 0.018257, loss_ce: 0.007282
2022-01-12 00:53:11,050 iteration 5556 : loss : 0.015171, loss_ce: 0.007116
2022-01-12 00:53:12,711 iteration 5557 : loss : 0.017682, loss_ce: 0.006214
2022-01-12 00:53:14,297 iteration 5558 : loss : 0.015190, loss_ce: 0.005215
2022-01-12 00:53:15,826 iteration 5559 : loss : 0.022218, loss_ce: 0.006794
 82%|███████████████████████▋     | 327/400 [2:41:24<35:43, 29.36s/it]2022-01-12 00:53:17,461 iteration 5560 : loss : 0.023595, loss_ce: 0.010492
2022-01-12 00:53:19,152 iteration 5561 : loss : 0.018200, loss_ce: 0.004677
2022-01-12 00:53:20,781 iteration 5562 : loss : 0.017554, loss_ce: 0.005663
2022-01-12 00:53:22,449 iteration 5563 : loss : 0.018608, loss_ce: 0.007584
2022-01-12 00:53:23,964 iteration 5564 : loss : 0.017428, loss_ce: 0.005531
2022-01-12 00:53:25,604 iteration 5565 : loss : 0.018827, loss_ce: 0.005223
2022-01-12 00:53:27,168 iteration 5566 : loss : 0.012163, loss_ce: 0.004121
2022-01-12 00:53:28,830 iteration 5567 : loss : 0.013310, loss_ce: 0.006676
2022-01-12 00:53:30,500 iteration 5568 : loss : 0.018678, loss_ce: 0.007926
2022-01-12 00:53:32,163 iteration 5569 : loss : 0.023652, loss_ce: 0.010450
2022-01-12 00:53:33,766 iteration 5570 : loss : 0.014728, loss_ce: 0.006829
2022-01-12 00:53:35,364 iteration 5571 : loss : 0.021379, loss_ce: 0.005639
2022-01-12 00:53:36,857 iteration 5572 : loss : 0.012440, loss_ce: 0.004207
2022-01-12 00:53:38,489 iteration 5573 : loss : 0.015224, loss_ce: 0.005878
2022-01-12 00:53:40,050 iteration 5574 : loss : 0.014703, loss_ce: 0.005717
2022-01-12 00:53:41,578 iteration 5575 : loss : 0.010546, loss_ce: 0.003889
2022-01-12 00:53:43,072 iteration 5576 : loss : 0.018056, loss_ce: 0.005097
 82%|███████████████████████▊     | 328/400 [2:41:51<34:28, 28.72s/it]2022-01-12 00:53:44,768 iteration 5577 : loss : 0.025572, loss_ce: 0.007910
2022-01-12 00:53:46,371 iteration 5578 : loss : 0.011558, loss_ce: 0.003812
2022-01-12 00:53:48,080 iteration 5579 : loss : 0.016976, loss_ce: 0.005224
2022-01-12 00:53:49,744 iteration 5580 : loss : 0.022364, loss_ce: 0.007053
2022-01-12 00:53:51,387 iteration 5581 : loss : 0.018041, loss_ce: 0.007392
2022-01-12 00:53:52,977 iteration 5582 : loss : 0.018324, loss_ce: 0.008762
2022-01-12 00:53:54,525 iteration 5583 : loss : 0.010206, loss_ce: 0.002797
2022-01-12 00:53:56,104 iteration 5584 : loss : 0.012775, loss_ce: 0.004731
2022-01-12 00:53:57,618 iteration 5585 : loss : 0.021846, loss_ce: 0.010725
2022-01-12 00:53:59,160 iteration 5586 : loss : 0.013772, loss_ce: 0.005814
2022-01-12 00:54:00,838 iteration 5587 : loss : 0.029547, loss_ce: 0.013316
2022-01-12 00:54:02,433 iteration 5588 : loss : 0.020372, loss_ce: 0.007580
2022-01-12 00:54:04,091 iteration 5589 : loss : 0.023108, loss_ce: 0.007939
2022-01-12 00:54:05,618 iteration 5590 : loss : 0.012994, loss_ce: 0.005601
2022-01-12 00:54:07,251 iteration 5591 : loss : 0.020342, loss_ce: 0.007745
2022-01-12 00:54:08,888 iteration 5592 : loss : 0.016036, loss_ce: 0.006497
2022-01-12 00:54:10,381 iteration 5593 : loss : 0.012134, loss_ce: 0.006044
 82%|███████████████████████▊     | 329/400 [2:42:19<33:29, 28.30s/it]2022-01-12 00:54:12,038 iteration 5594 : loss : 0.011344, loss_ce: 0.004363
2022-01-12 00:54:13,582 iteration 5595 : loss : 0.010539, loss_ce: 0.004161
2022-01-12 00:54:15,259 iteration 5596 : loss : 0.025571, loss_ce: 0.008698
2022-01-12 00:54:16,883 iteration 5597 : loss : 0.017092, loss_ce: 0.007210
2022-01-12 00:54:18,611 iteration 5598 : loss : 0.020315, loss_ce: 0.009679
2022-01-12 00:54:20,315 iteration 5599 : loss : 0.014831, loss_ce: 0.005644
2022-01-12 00:54:21,854 iteration 5600 : loss : 0.013519, loss_ce: 0.005744
2022-01-12 00:54:23,473 iteration 5601 : loss : 0.013694, loss_ce: 0.004326
2022-01-12 00:54:25,079 iteration 5602 : loss : 0.021167, loss_ce: 0.007876
2022-01-12 00:54:26,688 iteration 5603 : loss : 0.019681, loss_ce: 0.009706
2022-01-12 00:54:28,296 iteration 5604 : loss : 0.023401, loss_ce: 0.007607
2022-01-12 00:54:29,853 iteration 5605 : loss : 0.013790, loss_ce: 0.006747
2022-01-12 00:54:31,494 iteration 5606 : loss : 0.017558, loss_ce: 0.005822
2022-01-12 00:54:33,031 iteration 5607 : loss : 0.012138, loss_ce: 0.005337
2022-01-12 00:54:34,652 iteration 5608 : loss : 0.015742, loss_ce: 0.008785
2022-01-12 00:54:36,214 iteration 5609 : loss : 0.012496, loss_ce: 0.004317
2022-01-12 00:54:36,214 Training Data Eval:
2022-01-12 00:54:44,176   Average segmentation loss on training set: 0.0086
2022-01-12 00:54:44,176 Validation Data Eval:
2022-01-12 00:54:46,931   Average segmentation loss on validation set: 0.0639
2022-01-12 00:54:48,559 iteration 5610 : loss : 0.019023, loss_ce: 0.006914
 82%|███████████████████████▉     | 330/400 [2:42:57<36:28, 31.27s/it]2022-01-12 00:54:50,144 iteration 5611 : loss : 0.009855, loss_ce: 0.003825
2022-01-12 00:54:51,796 iteration 5612 : loss : 0.021026, loss_ce: 0.006856
2022-01-12 00:54:53,366 iteration 5613 : loss : 0.013279, loss_ce: 0.002573
2022-01-12 00:54:54,930 iteration 5614 : loss : 0.020045, loss_ce: 0.007306
2022-01-12 00:54:56,526 iteration 5615 : loss : 0.015887, loss_ce: 0.007350
2022-01-12 00:54:58,119 iteration 5616 : loss : 0.012882, loss_ce: 0.003723
2022-01-12 00:54:59,887 iteration 5617 : loss : 0.021368, loss_ce: 0.006881
2022-01-12 00:55:01,544 iteration 5618 : loss : 0.014118, loss_ce: 0.004290
2022-01-12 00:55:03,157 iteration 5619 : loss : 0.013225, loss_ce: 0.006463
2022-01-12 00:55:04,694 iteration 5620 : loss : 0.016435, loss_ce: 0.006771
2022-01-12 00:55:06,221 iteration 5621 : loss : 0.014078, loss_ce: 0.005020
2022-01-12 00:55:07,717 iteration 5622 : loss : 0.011342, loss_ce: 0.004922
2022-01-12 00:55:09,361 iteration 5623 : loss : 0.013533, loss_ce: 0.005580
2022-01-12 00:55:10,936 iteration 5624 : loss : 0.017412, loss_ce: 0.006320
2022-01-12 00:55:12,525 iteration 5625 : loss : 0.015957, loss_ce: 0.006965
2022-01-12 00:55:14,135 iteration 5626 : loss : 0.015137, loss_ce: 0.005343
2022-01-12 00:55:15,722 iteration 5627 : loss : 0.012800, loss_ce: 0.005311
 83%|███████████████████████▉     | 331/400 [2:43:24<34:32, 30.03s/it]2022-01-12 00:55:17,351 iteration 5628 : loss : 0.010362, loss_ce: 0.004179
2022-01-12 00:55:18,970 iteration 5629 : loss : 0.015878, loss_ce: 0.006058
2022-01-12 00:55:20,607 iteration 5630 : loss : 0.012723, loss_ce: 0.005270
2022-01-12 00:55:22,236 iteration 5631 : loss : 0.031382, loss_ce: 0.018629
2022-01-12 00:55:23,934 iteration 5632 : loss : 0.017889, loss_ce: 0.006040
2022-01-12 00:55:25,443 iteration 5633 : loss : 0.018713, loss_ce: 0.005004
2022-01-12 00:55:27,119 iteration 5634 : loss : 0.041911, loss_ce: 0.010793
2022-01-12 00:55:28,730 iteration 5635 : loss : 0.014819, loss_ce: 0.005527
2022-01-12 00:55:30,247 iteration 5636 : loss : 0.015083, loss_ce: 0.004275
2022-01-12 00:55:31,943 iteration 5637 : loss : 0.014043, loss_ce: 0.005833
2022-01-12 00:55:33,535 iteration 5638 : loss : 0.011696, loss_ce: 0.005245
2022-01-12 00:55:35,156 iteration 5639 : loss : 0.023635, loss_ce: 0.007185
2022-01-12 00:55:36,769 iteration 5640 : loss : 0.018098, loss_ce: 0.006851
2022-01-12 00:55:38,309 iteration 5641 : loss : 0.012010, loss_ce: 0.004514
2022-01-12 00:55:40,034 iteration 5642 : loss : 0.018781, loss_ce: 0.007564
2022-01-12 00:55:41,539 iteration 5643 : loss : 0.017464, loss_ce: 0.008693
2022-01-12 00:55:43,119 iteration 5644 : loss : 0.017685, loss_ce: 0.008593
 83%|████████████████████████     | 332/400 [2:43:51<33:08, 29.24s/it]2022-01-12 00:55:44,696 iteration 5645 : loss : 0.012320, loss_ce: 0.003751
2022-01-12 00:55:46,270 iteration 5646 : loss : 0.012115, loss_ce: 0.003452
2022-01-12 00:55:47,792 iteration 5647 : loss : 0.010878, loss_ce: 0.003731
2022-01-12 00:55:49,388 iteration 5648 : loss : 0.015105, loss_ce: 0.007391
2022-01-12 00:55:50,933 iteration 5649 : loss : 0.012238, loss_ce: 0.003737
2022-01-12 00:55:52,601 iteration 5650 : loss : 0.031619, loss_ce: 0.012214
2022-01-12 00:55:54,196 iteration 5651 : loss : 0.018833, loss_ce: 0.008403
2022-01-12 00:55:55,797 iteration 5652 : loss : 0.013700, loss_ce: 0.004332
2022-01-12 00:55:57,388 iteration 5653 : loss : 0.020998, loss_ce: 0.007632
2022-01-12 00:55:58,974 iteration 5654 : loss : 0.015268, loss_ce: 0.005641
2022-01-12 00:56:00,464 iteration 5655 : loss : 0.012007, loss_ce: 0.005519
2022-01-12 00:56:02,157 iteration 5656 : loss : 0.026577, loss_ce: 0.008935
2022-01-12 00:56:03,809 iteration 5657 : loss : 0.022880, loss_ce: 0.008632
2022-01-12 00:56:05,460 iteration 5658 : loss : 0.022255, loss_ce: 0.010877
2022-01-12 00:56:07,025 iteration 5659 : loss : 0.014221, loss_ce: 0.005815
2022-01-12 00:56:08,655 iteration 5660 : loss : 0.025761, loss_ce: 0.007544
2022-01-12 00:56:10,254 iteration 5661 : loss : 0.015643, loss_ce: 0.007987
 83%|████████████████████████▏    | 333/400 [2:44:18<31:56, 28.61s/it]2022-01-12 00:56:11,886 iteration 5662 : loss : 0.026622, loss_ce: 0.011842
2022-01-12 00:56:13,439 iteration 5663 : loss : 0.012372, loss_ce: 0.003946
2022-01-12 00:56:15,042 iteration 5664 : loss : 0.015307, loss_ce: 0.006643
2022-01-12 00:56:16,594 iteration 5665 : loss : 0.010595, loss_ce: 0.004046
2022-01-12 00:56:18,203 iteration 5666 : loss : 0.014514, loss_ce: 0.003307
2022-01-12 00:56:19,848 iteration 5667 : loss : 0.014581, loss_ce: 0.005484
2022-01-12 00:56:21,462 iteration 5668 : loss : 0.018556, loss_ce: 0.006504
2022-01-12 00:56:22,992 iteration 5669 : loss : 0.014057, loss_ce: 0.005200
2022-01-12 00:56:24,564 iteration 5670 : loss : 0.018130, loss_ce: 0.006156
2022-01-12 00:56:26,105 iteration 5671 : loss : 0.013511, loss_ce: 0.004949
2022-01-12 00:56:27,807 iteration 5672 : loss : 0.026202, loss_ce: 0.013021
2022-01-12 00:56:29,387 iteration 5673 : loss : 0.016836, loss_ce: 0.006403
2022-01-12 00:56:30,986 iteration 5674 : loss : 0.011964, loss_ce: 0.003425
2022-01-12 00:56:32,546 iteration 5675 : loss : 0.013188, loss_ce: 0.006632
2022-01-12 00:56:34,212 iteration 5676 : loss : 0.026969, loss_ce: 0.011270
2022-01-12 00:56:35,795 iteration 5677 : loss : 0.018484, loss_ce: 0.005070
2022-01-12 00:56:37,498 iteration 5678 : loss : 0.021721, loss_ce: 0.007894
 84%|████████████████████████▏    | 334/400 [2:44:46<31:01, 28.20s/it]2022-01-12 00:56:39,184 iteration 5679 : loss : 0.015785, loss_ce: 0.005815
2022-01-12 00:56:40,738 iteration 5680 : loss : 0.017834, loss_ce: 0.008062
2022-01-12 00:56:42,352 iteration 5681 : loss : 0.024996, loss_ce: 0.011066
2022-01-12 00:56:43,935 iteration 5682 : loss : 0.018096, loss_ce: 0.006388
2022-01-12 00:56:45,568 iteration 5683 : loss : 0.019677, loss_ce: 0.008176
2022-01-12 00:56:47,244 iteration 5684 : loss : 0.019557, loss_ce: 0.007967
2022-01-12 00:56:48,897 iteration 5685 : loss : 0.016515, loss_ce: 0.004175
2022-01-12 00:56:50,486 iteration 5686 : loss : 0.018663, loss_ce: 0.006836
2022-01-12 00:56:52,050 iteration 5687 : loss : 0.014243, loss_ce: 0.003941
2022-01-12 00:56:53,648 iteration 5688 : loss : 0.017683, loss_ce: 0.005344
2022-01-12 00:56:55,233 iteration 5689 : loss : 0.014161, loss_ce: 0.005722
2022-01-12 00:56:56,863 iteration 5690 : loss : 0.018261, loss_ce: 0.009175
2022-01-12 00:56:58,440 iteration 5691 : loss : 0.011033, loss_ce: 0.002933
2022-01-12 00:56:59,990 iteration 5692 : loss : 0.012840, loss_ce: 0.004405
2022-01-12 00:57:01,624 iteration 5693 : loss : 0.019341, loss_ce: 0.008650
2022-01-12 00:57:03,280 iteration 5694 : loss : 0.020913, loss_ce: 0.007964
2022-01-12 00:57:03,280 Training Data Eval:
2022-01-12 00:57:11,244   Average segmentation loss on training set: 0.0086
2022-01-12 00:57:11,244 Validation Data Eval:
2022-01-12 00:57:13,990   Average segmentation loss on validation set: 0.0636
2022-01-12 00:57:15,554 iteration 5695 : loss : 0.016063, loss_ce: 0.005362
 84%|████████████████████████▎    | 335/400 [2:45:24<33:45, 31.16s/it]2022-01-12 00:57:17,168 iteration 5696 : loss : 0.013878, loss_ce: 0.004458
2022-01-12 00:57:18,827 iteration 5697 : loss : 0.016536, loss_ce: 0.006597
2022-01-12 00:57:20,436 iteration 5698 : loss : 0.015207, loss_ce: 0.006792
2022-01-12 00:57:22,123 iteration 5699 : loss : 0.018983, loss_ce: 0.006811
2022-01-12 00:57:23,728 iteration 5700 : loss : 0.014297, loss_ce: 0.005123
2022-01-12 00:57:25,306 iteration 5701 : loss : 0.014382, loss_ce: 0.007072
2022-01-12 00:57:26,908 iteration 5702 : loss : 0.011417, loss_ce: 0.003557
2022-01-12 00:57:28,486 iteration 5703 : loss : 0.011436, loss_ce: 0.004871
2022-01-12 00:57:30,111 iteration 5704 : loss : 0.014828, loss_ce: 0.006697
2022-01-12 00:57:31,741 iteration 5705 : loss : 0.014583, loss_ce: 0.005493
2022-01-12 00:57:33,292 iteration 5706 : loss : 0.015410, loss_ce: 0.003168
2022-01-12 00:57:35,017 iteration 5707 : loss : 0.021686, loss_ce: 0.008433
2022-01-12 00:57:36,600 iteration 5708 : loss : 0.014648, loss_ce: 0.005826
2022-01-12 00:57:38,218 iteration 5709 : loss : 0.019514, loss_ce: 0.009753
2022-01-12 00:57:39,912 iteration 5710 : loss : 0.015334, loss_ce: 0.006171
2022-01-12 00:57:41,492 iteration 5711 : loss : 0.014363, loss_ce: 0.007380
2022-01-12 00:57:43,073 iteration 5712 : loss : 0.017598, loss_ce: 0.007463
 84%|████████████████████████▎    | 336/400 [2:45:51<32:04, 30.07s/it]2022-01-12 00:57:44,659 iteration 5713 : loss : 0.017165, loss_ce: 0.004897
2022-01-12 00:57:46,224 iteration 5714 : loss : 0.017866, loss_ce: 0.007490
2022-01-12 00:57:47,783 iteration 5715 : loss : 0.011014, loss_ce: 0.004299
2022-01-12 00:57:49,403 iteration 5716 : loss : 0.013371, loss_ce: 0.005783
2022-01-12 00:57:51,063 iteration 5717 : loss : 0.020433, loss_ce: 0.008126
2022-01-12 00:57:52,582 iteration 5718 : loss : 0.020472, loss_ce: 0.005907
2022-01-12 00:57:54,224 iteration 5719 : loss : 0.012876, loss_ce: 0.005191
2022-01-12 00:57:55,780 iteration 5720 : loss : 0.013454, loss_ce: 0.005791
2022-01-12 00:57:57,261 iteration 5721 : loss : 0.010715, loss_ce: 0.002996
2022-01-12 00:57:58,836 iteration 5722 : loss : 0.013199, loss_ce: 0.004554
2022-01-12 00:58:00,448 iteration 5723 : loss : 0.015882, loss_ce: 0.006162
2022-01-12 00:58:01,944 iteration 5724 : loss : 0.009542, loss_ce: 0.003526
2022-01-12 00:58:03,566 iteration 5725 : loss : 0.012445, loss_ce: 0.005412
2022-01-12 00:58:05,096 iteration 5726 : loss : 0.012092, loss_ce: 0.005213
2022-01-12 00:58:06,668 iteration 5727 : loss : 0.018507, loss_ce: 0.006308
2022-01-12 00:58:08,227 iteration 5728 : loss : 0.012481, loss_ce: 0.005700
2022-01-12 00:58:09,861 iteration 5729 : loss : 0.016133, loss_ce: 0.006022
 84%|████████████████████████▍    | 337/400 [2:46:18<30:32, 29.08s/it]2022-01-12 00:58:11,523 iteration 5730 : loss : 0.025667, loss_ce: 0.006750
2022-01-12 00:58:13,165 iteration 5731 : loss : 0.014989, loss_ce: 0.007054
2022-01-12 00:58:14,751 iteration 5732 : loss : 0.014733, loss_ce: 0.004672
2022-01-12 00:58:16,306 iteration 5733 : loss : 0.013942, loss_ce: 0.003931
2022-01-12 00:58:17,904 iteration 5734 : loss : 0.021488, loss_ce: 0.005821
2022-01-12 00:58:19,510 iteration 5735 : loss : 0.014991, loss_ce: 0.007122
2022-01-12 00:58:21,085 iteration 5736 : loss : 0.014945, loss_ce: 0.008261
2022-01-12 00:58:22,659 iteration 5737 : loss : 0.035931, loss_ce: 0.023099
2022-01-12 00:58:24,230 iteration 5738 : loss : 0.009027, loss_ce: 0.003717
2022-01-12 00:58:25,756 iteration 5739 : loss : 0.014294, loss_ce: 0.006542
2022-01-12 00:58:27,370 iteration 5740 : loss : 0.025572, loss_ce: 0.012062
2022-01-12 00:58:28,980 iteration 5741 : loss : 0.016114, loss_ce: 0.007517
2022-01-12 00:58:30,602 iteration 5742 : loss : 0.022481, loss_ce: 0.009324
2022-01-12 00:58:32,158 iteration 5743 : loss : 0.012281, loss_ce: 0.004986
2022-01-12 00:58:33,806 iteration 5744 : loss : 0.014406, loss_ce: 0.005452
2022-01-12 00:58:35,416 iteration 5745 : loss : 0.013600, loss_ce: 0.004036
2022-01-12 00:58:36,930 iteration 5746 : loss : 0.015197, loss_ce: 0.003778
 84%|████████████████████████▌    | 338/400 [2:46:45<29:25, 28.48s/it]2022-01-12 00:58:38,598 iteration 5747 : loss : 0.014675, loss_ce: 0.004910
2022-01-12 00:58:40,231 iteration 5748 : loss : 0.016117, loss_ce: 0.005095
2022-01-12 00:58:41,884 iteration 5749 : loss : 0.016200, loss_ce: 0.007060
2022-01-12 00:58:43,583 iteration 5750 : loss : 0.019106, loss_ce: 0.005541
2022-01-12 00:58:45,108 iteration 5751 : loss : 0.012291, loss_ce: 0.005093
2022-01-12 00:58:46,739 iteration 5752 : loss : 0.015795, loss_ce: 0.004884
2022-01-12 00:58:48,320 iteration 5753 : loss : 0.023170, loss_ce: 0.008813
2022-01-12 00:58:49,837 iteration 5754 : loss : 0.011489, loss_ce: 0.004257
2022-01-12 00:58:51,433 iteration 5755 : loss : 0.014456, loss_ce: 0.006864
2022-01-12 00:58:53,030 iteration 5756 : loss : 0.011951, loss_ce: 0.003581
2022-01-12 00:58:54,612 iteration 5757 : loss : 0.013803, loss_ce: 0.006753
2022-01-12 00:58:56,212 iteration 5758 : loss : 0.017261, loss_ce: 0.004207
2022-01-12 00:58:57,829 iteration 5759 : loss : 0.014821, loss_ce: 0.007403
2022-01-12 00:58:59,511 iteration 5760 : loss : 0.018163, loss_ce: 0.006915
2022-01-12 00:59:01,028 iteration 5761 : loss : 0.010589, loss_ce: 0.003988
2022-01-12 00:59:02,815 iteration 5762 : loss : 0.021863, loss_ce: 0.008644
2022-01-12 00:59:04,410 iteration 5763 : loss : 0.015368, loss_ce: 0.005813
 85%|████████████████████████▌    | 339/400 [2:47:13<28:38, 28.18s/it]2022-01-12 00:59:06,066 iteration 5764 : loss : 0.018937, loss_ce: 0.006534
2022-01-12 00:59:07,706 iteration 5765 : loss : 0.016154, loss_ce: 0.005191
2022-01-12 00:59:09,313 iteration 5766 : loss : 0.011713, loss_ce: 0.004740
2022-01-12 00:59:10,934 iteration 5767 : loss : 0.015571, loss_ce: 0.006538
2022-01-12 00:59:12,657 iteration 5768 : loss : 0.026545, loss_ce: 0.008340
2022-01-12 00:59:14,244 iteration 5769 : loss : 0.012266, loss_ce: 0.004506
2022-01-12 00:59:15,838 iteration 5770 : loss : 0.013803, loss_ce: 0.005366
2022-01-12 00:59:17,429 iteration 5771 : loss : 0.019554, loss_ce: 0.009873
2022-01-12 00:59:19,041 iteration 5772 : loss : 0.021625, loss_ce: 0.008978
2022-01-12 00:59:20,733 iteration 5773 : loss : 0.013189, loss_ce: 0.005584
2022-01-12 00:59:22,417 iteration 5774 : loss : 0.017080, loss_ce: 0.007479
2022-01-12 00:59:24,122 iteration 5775 : loss : 0.016769, loss_ce: 0.006372
2022-01-12 00:59:25,683 iteration 5776 : loss : 0.013370, loss_ce: 0.004177
2022-01-12 00:59:27,248 iteration 5777 : loss : 0.013979, loss_ce: 0.005413
2022-01-12 00:59:28,877 iteration 5778 : loss : 0.017210, loss_ce: 0.005003
2022-01-12 00:59:30,467 iteration 5779 : loss : 0.016486, loss_ce: 0.006483
2022-01-12 00:59:30,467 Training Data Eval:
2022-01-12 00:59:38,422   Average segmentation loss on training set: 0.0083
2022-01-12 00:59:38,423 Validation Data Eval:
2022-01-12 00:59:41,157   Average segmentation loss on validation set: 0.0709
2022-01-12 00:59:42,793 iteration 5780 : loss : 0.013781, loss_ce: 0.005645
 85%|████████████████████████▋    | 340/400 [2:47:51<31:14, 31.24s/it]2022-01-12 00:59:44,568 iteration 5781 : loss : 0.030237, loss_ce: 0.010391
2022-01-12 00:59:46,063 iteration 5782 : loss : 0.010998, loss_ce: 0.005227
2022-01-12 00:59:47,609 iteration 5783 : loss : 0.017013, loss_ce: 0.006685
2022-01-12 00:59:49,155 iteration 5784 : loss : 0.015326, loss_ce: 0.005963
2022-01-12 00:59:50,665 iteration 5785 : loss : 0.012056, loss_ce: 0.005108
2022-01-12 00:59:52,273 iteration 5786 : loss : 0.020289, loss_ce: 0.004192
2022-01-12 00:59:53,846 iteration 5787 : loss : 0.017094, loss_ce: 0.006479
2022-01-12 00:59:55,327 iteration 5788 : loss : 0.012269, loss_ce: 0.004665
2022-01-12 00:59:56,845 iteration 5789 : loss : 0.009793, loss_ce: 0.003894
2022-01-12 00:59:58,441 iteration 5790 : loss : 0.011905, loss_ce: 0.004462
2022-01-12 01:00:00,012 iteration 5791 : loss : 0.012915, loss_ce: 0.006341
2022-01-12 01:00:01,556 iteration 5792 : loss : 0.011381, loss_ce: 0.004294
2022-01-12 01:00:03,192 iteration 5793 : loss : 0.015367, loss_ce: 0.005298
2022-01-12 01:00:04,860 iteration 5794 : loss : 0.016463, loss_ce: 0.006877
2022-01-12 01:00:06,399 iteration 5795 : loss : 0.009992, loss_ce: 0.004272
2022-01-12 01:00:08,013 iteration 5796 : loss : 0.010266, loss_ce: 0.004240
2022-01-12 01:00:09,617 iteration 5797 : loss : 0.012823, loss_ce: 0.004486
 85%|████████████████████████▋    | 341/400 [2:48:18<29:25, 29.92s/it]2022-01-12 01:00:11,353 iteration 5798 : loss : 0.016196, loss_ce: 0.004743
2022-01-12 01:00:12,938 iteration 5799 : loss : 0.014786, loss_ce: 0.004778
2022-01-12 01:00:14,512 iteration 5800 : loss : 0.013627, loss_ce: 0.003668
2022-01-12 01:00:16,004 iteration 5801 : loss : 0.009672, loss_ce: 0.003236
2022-01-12 01:00:17,624 iteration 5802 : loss : 0.012795, loss_ce: 0.006186
2022-01-12 01:00:19,129 iteration 5803 : loss : 0.011905, loss_ce: 0.004143
2022-01-12 01:00:20,841 iteration 5804 : loss : 0.043640, loss_ce: 0.017661
2022-01-12 01:00:22,398 iteration 5805 : loss : 0.014708, loss_ce: 0.006925
2022-01-12 01:00:23,930 iteration 5806 : loss : 0.011672, loss_ce: 0.004150
2022-01-12 01:00:25,540 iteration 5807 : loss : 0.026185, loss_ce: 0.009829
2022-01-12 01:00:27,098 iteration 5808 : loss : 0.019269, loss_ce: 0.008638
2022-01-12 01:00:28,790 iteration 5809 : loss : 0.020794, loss_ce: 0.006626
2022-01-12 01:00:30,303 iteration 5810 : loss : 0.017376, loss_ce: 0.004905
2022-01-12 01:00:31,879 iteration 5811 : loss : 0.014672, loss_ce: 0.007124
2022-01-12 01:00:33,497 iteration 5812 : loss : 0.018311, loss_ce: 0.008062
2022-01-12 01:00:35,058 iteration 5813 : loss : 0.021593, loss_ce: 0.006051
2022-01-12 01:00:36,648 iteration 5814 : loss : 0.017998, loss_ce: 0.007845
 86%|████████████████████████▊    | 342/400 [2:48:45<28:04, 29.05s/it]2022-01-12 01:00:38,396 iteration 5815 : loss : 0.030118, loss_ce: 0.013218
2022-01-12 01:00:39,922 iteration 5816 : loss : 0.012826, loss_ce: 0.003914
2022-01-12 01:00:41,537 iteration 5817 : loss : 0.012737, loss_ce: 0.003630
2022-01-12 01:00:43,108 iteration 5818 : loss : 0.016455, loss_ce: 0.005647
2022-01-12 01:00:44,797 iteration 5819 : loss : 0.017025, loss_ce: 0.006350
2022-01-12 01:00:46,379 iteration 5820 : loss : 0.011793, loss_ce: 0.005237
2022-01-12 01:00:47,953 iteration 5821 : loss : 0.016807, loss_ce: 0.005887
2022-01-12 01:00:49,615 iteration 5822 : loss : 0.015738, loss_ce: 0.005122
2022-01-12 01:00:51,223 iteration 5823 : loss : 0.010079, loss_ce: 0.003455
2022-01-12 01:00:52,817 iteration 5824 : loss : 0.017176, loss_ce: 0.006374
2022-01-12 01:00:54,410 iteration 5825 : loss : 0.016132, loss_ce: 0.007258
2022-01-12 01:00:55,968 iteration 5826 : loss : 0.021071, loss_ce: 0.005687
2022-01-12 01:00:57,501 iteration 5827 : loss : 0.015461, loss_ce: 0.004113
2022-01-12 01:00:58,989 iteration 5828 : loss : 0.009289, loss_ce: 0.004132
2022-01-12 01:01:00,569 iteration 5829 : loss : 0.014015, loss_ce: 0.005484
2022-01-12 01:01:02,168 iteration 5830 : loss : 0.018790, loss_ce: 0.008683
2022-01-12 01:01:03,817 iteration 5831 : loss : 0.015289, loss_ce: 0.006233
 86%|████████████████████████▊    | 343/400 [2:49:12<27:03, 28.49s/it]2022-01-12 01:01:05,540 iteration 5832 : loss : 0.015207, loss_ce: 0.006632
2022-01-12 01:01:07,106 iteration 5833 : loss : 0.012556, loss_ce: 0.004658
2022-01-12 01:01:08,804 iteration 5834 : loss : 0.018913, loss_ce: 0.007270
2022-01-12 01:01:10,572 iteration 5835 : loss : 0.028894, loss_ce: 0.010871
2022-01-12 01:01:12,180 iteration 5836 : loss : 0.016103, loss_ce: 0.006706
2022-01-12 01:01:13,871 iteration 5837 : loss : 0.018980, loss_ce: 0.007961
2022-01-12 01:01:15,552 iteration 5838 : loss : 0.020505, loss_ce: 0.005805
2022-01-12 01:01:17,066 iteration 5839 : loss : 0.013705, loss_ce: 0.004423
2022-01-12 01:01:18,880 iteration 5840 : loss : 0.022784, loss_ce: 0.013240
2022-01-12 01:01:20,465 iteration 5841 : loss : 0.010977, loss_ce: 0.004466
2022-01-12 01:01:22,143 iteration 5842 : loss : 0.016779, loss_ce: 0.006404
2022-01-12 01:01:23,830 iteration 5843 : loss : 0.020832, loss_ce: 0.007063
2022-01-12 01:01:25,461 iteration 5844 : loss : 0.017832, loss_ce: 0.004743
2022-01-12 01:01:27,127 iteration 5845 : loss : 0.016606, loss_ce: 0.005736
2022-01-12 01:01:28,726 iteration 5846 : loss : 0.014178, loss_ce: 0.006734
2022-01-12 01:01:30,306 iteration 5847 : loss : 0.009985, loss_ce: 0.003520
2022-01-12 01:01:31,816 iteration 5848 : loss : 0.010996, loss_ce: 0.003458
 86%|████████████████████████▉    | 344/400 [2:49:40<26:26, 28.34s/it]2022-01-12 01:01:33,397 iteration 5849 : loss : 0.013385, loss_ce: 0.004291
2022-01-12 01:01:35,072 iteration 5850 : loss : 0.021346, loss_ce: 0.005557
2022-01-12 01:01:36,623 iteration 5851 : loss : 0.011749, loss_ce: 0.005256
2022-01-12 01:01:38,186 iteration 5852 : loss : 0.014539, loss_ce: 0.004637
2022-01-12 01:01:39,713 iteration 5853 : loss : 0.016100, loss_ce: 0.004200
2022-01-12 01:01:41,298 iteration 5854 : loss : 0.018794, loss_ce: 0.007309
2022-01-12 01:01:42,908 iteration 5855 : loss : 0.014325, loss_ce: 0.003707
2022-01-12 01:01:44,428 iteration 5856 : loss : 0.014657, loss_ce: 0.005512
2022-01-12 01:01:46,044 iteration 5857 : loss : 0.015239, loss_ce: 0.007042
2022-01-12 01:01:47,614 iteration 5858 : loss : 0.026443, loss_ce: 0.008061
2022-01-12 01:01:49,229 iteration 5859 : loss : 0.013232, loss_ce: 0.006531
2022-01-12 01:01:50,864 iteration 5860 : loss : 0.013729, loss_ce: 0.005395
2022-01-12 01:01:52,495 iteration 5861 : loss : 0.012703, loss_ce: 0.005695
2022-01-12 01:01:54,075 iteration 5862 : loss : 0.012009, loss_ce: 0.004514
2022-01-12 01:01:55,655 iteration 5863 : loss : 0.013932, loss_ce: 0.005646
2022-01-12 01:01:57,232 iteration 5864 : loss : 0.013509, loss_ce: 0.004847
2022-01-12 01:01:57,233 Training Data Eval:
2022-01-12 01:02:05,190   Average segmentation loss on training set: 0.0079
2022-01-12 01:02:05,190 Validation Data Eval:
2022-01-12 01:02:07,936   Average segmentation loss on validation set: 0.0652
2022-01-12 01:02:09,498 iteration 5865 : loss : 0.011052, loss_ce: 0.004075
 86%|█████████████████████████    | 345/400 [2:50:18<28:32, 31.14s/it]2022-01-12 01:02:11,139 iteration 5866 : loss : 0.015520, loss_ce: 0.004914
2022-01-12 01:02:12,746 iteration 5867 : loss : 0.015547, loss_ce: 0.007130
2022-01-12 01:02:14,248 iteration 5868 : loss : 0.009987, loss_ce: 0.003842
2022-01-12 01:02:15,802 iteration 5869 : loss : 0.009613, loss_ce: 0.003370
2022-01-12 01:02:17,299 iteration 5870 : loss : 0.011638, loss_ce: 0.004321
2022-01-12 01:02:18,876 iteration 5871 : loss : 0.012671, loss_ce: 0.005437
2022-01-12 01:02:20,439 iteration 5872 : loss : 0.012968, loss_ce: 0.005544
2022-01-12 01:02:21,950 iteration 5873 : loss : 0.011593, loss_ce: 0.005156
2022-01-12 01:02:23,473 iteration 5874 : loss : 0.019614, loss_ce: 0.004108
2022-01-12 01:02:25,136 iteration 5875 : loss : 0.013354, loss_ce: 0.005496
2022-01-12 01:02:26,815 iteration 5876 : loss : 0.018358, loss_ce: 0.008056
2022-01-12 01:02:28,429 iteration 5877 : loss : 0.017457, loss_ce: 0.006512
2022-01-12 01:02:30,076 iteration 5878 : loss : 0.015212, loss_ce: 0.004905
2022-01-12 01:02:31,639 iteration 5879 : loss : 0.013928, loss_ce: 0.003936
2022-01-12 01:02:33,202 iteration 5880 : loss : 0.015598, loss_ce: 0.006074
2022-01-12 01:02:34,828 iteration 5881 : loss : 0.018280, loss_ce: 0.006017
2022-01-12 01:02:36,400 iteration 5882 : loss : 0.011974, loss_ce: 0.004151
 86%|█████████████████████████    | 346/400 [2:50:45<26:52, 29.87s/it]2022-01-12 01:02:38,154 iteration 5883 : loss : 0.014050, loss_ce: 0.005108
2022-01-12 01:02:39,704 iteration 5884 : loss : 0.012786, loss_ce: 0.004565
2022-01-12 01:02:41,370 iteration 5885 : loss : 0.022438, loss_ce: 0.005628
2022-01-12 01:02:42,916 iteration 5886 : loss : 0.009829, loss_ce: 0.003262
2022-01-12 01:02:44,472 iteration 5887 : loss : 0.026021, loss_ce: 0.008991
2022-01-12 01:02:45,960 iteration 5888 : loss : 0.010506, loss_ce: 0.004506
2022-01-12 01:02:47,523 iteration 5889 : loss : 0.013197, loss_ce: 0.005803
2022-01-12 01:02:49,165 iteration 5890 : loss : 0.018455, loss_ce: 0.007589
2022-01-12 01:02:50,799 iteration 5891 : loss : 0.024579, loss_ce: 0.006710
2022-01-12 01:02:52,476 iteration 5892 : loss : 0.013875, loss_ce: 0.006552
2022-01-12 01:02:53,983 iteration 5893 : loss : 0.015408, loss_ce: 0.004723
2022-01-12 01:02:55,525 iteration 5894 : loss : 0.012303, loss_ce: 0.003378
2022-01-12 01:02:57,158 iteration 5895 : loss : 0.018826, loss_ce: 0.008204
2022-01-12 01:02:58,650 iteration 5896 : loss : 0.011462, loss_ce: 0.003718
2022-01-12 01:03:00,249 iteration 5897 : loss : 0.014524, loss_ce: 0.006094
2022-01-12 01:03:01,898 iteration 5898 : loss : 0.021310, loss_ce: 0.008412
2022-01-12 01:03:03,474 iteration 5899 : loss : 0.016508, loss_ce: 0.005140
 87%|█████████████████████████▏   | 347/400 [2:51:12<25:38, 29.03s/it]2022-01-12 01:03:04,995 iteration 5900 : loss : 0.009126, loss_ce: 0.003664
2022-01-12 01:03:06,546 iteration 5901 : loss : 0.012691, loss_ce: 0.005834
2022-01-12 01:03:08,043 iteration 5902 : loss : 0.010754, loss_ce: 0.004075
2022-01-12 01:03:09,590 iteration 5903 : loss : 0.014810, loss_ce: 0.006500
2022-01-12 01:03:11,188 iteration 5904 : loss : 0.016215, loss_ce: 0.004930
2022-01-12 01:03:12,821 iteration 5905 : loss : 0.015673, loss_ce: 0.007379
2022-01-12 01:03:14,501 iteration 5906 : loss : 0.019960, loss_ce: 0.005228
2022-01-12 01:03:16,016 iteration 5907 : loss : 0.010048, loss_ce: 0.004318
2022-01-12 01:03:17,601 iteration 5908 : loss : 0.018557, loss_ce: 0.005852
2022-01-12 01:03:19,246 iteration 5909 : loss : 0.013787, loss_ce: 0.006000
2022-01-12 01:03:20,828 iteration 5910 : loss : 0.010113, loss_ce: 0.003308
2022-01-12 01:03:22,467 iteration 5911 : loss : 0.015200, loss_ce: 0.003927
2022-01-12 01:03:24,074 iteration 5912 : loss : 0.032860, loss_ce: 0.013106
2022-01-12 01:03:25,665 iteration 5913 : loss : 0.013717, loss_ce: 0.006249
2022-01-12 01:03:27,239 iteration 5914 : loss : 0.014864, loss_ce: 0.007642
2022-01-12 01:03:28,872 iteration 5915 : loss : 0.018010, loss_ce: 0.008962
2022-01-12 01:03:30,401 iteration 5916 : loss : 0.009340, loss_ce: 0.003212
 87%|█████████████████████████▏   | 348/400 [2:51:39<24:36, 28.40s/it]2022-01-12 01:03:31,980 iteration 5917 : loss : 0.014303, loss_ce: 0.005235
2022-01-12 01:03:33,647 iteration 5918 : loss : 0.016495, loss_ce: 0.008613
2022-01-12 01:03:35,321 iteration 5919 : loss : 0.012951, loss_ce: 0.004629
2022-01-12 01:03:37,004 iteration 5920 : loss : 0.017158, loss_ce: 0.005678
2022-01-12 01:03:38,593 iteration 5921 : loss : 0.019697, loss_ce: 0.006698
2022-01-12 01:03:40,237 iteration 5922 : loss : 0.014539, loss_ce: 0.007342
2022-01-12 01:03:41,843 iteration 5923 : loss : 0.013663, loss_ce: 0.005461
2022-01-12 01:03:43,343 iteration 5924 : loss : 0.010186, loss_ce: 0.003625
2022-01-12 01:03:44,921 iteration 5925 : loss : 0.014799, loss_ce: 0.005518
2022-01-12 01:03:46,579 iteration 5926 : loss : 0.024148, loss_ce: 0.006531
2022-01-12 01:03:48,138 iteration 5927 : loss : 0.009583, loss_ce: 0.004075
2022-01-12 01:03:49,809 iteration 5928 : loss : 0.012803, loss_ce: 0.005882
2022-01-12 01:03:51,430 iteration 5929 : loss : 0.015383, loss_ce: 0.007376
2022-01-12 01:03:53,116 iteration 5930 : loss : 0.017580, loss_ce: 0.006707
2022-01-12 01:03:54,656 iteration 5931 : loss : 0.012475, loss_ce: 0.005344
2022-01-12 01:03:56,265 iteration 5932 : loss : 0.016653, loss_ce: 0.006675
2022-01-12 01:03:57,904 iteration 5933 : loss : 0.019522, loss_ce: 0.006156
 87%|█████████████████████████▎   | 349/400 [2:52:06<23:54, 28.13s/it]2022-01-12 01:03:59,570 iteration 5934 : loss : 0.011426, loss_ce: 0.004429
2022-01-12 01:04:01,186 iteration 5935 : loss : 0.012605, loss_ce: 0.003539
2022-01-12 01:04:02,912 iteration 5936 : loss : 0.017609, loss_ce: 0.006131
2022-01-12 01:04:04,604 iteration 5937 : loss : 0.016238, loss_ce: 0.007117
2022-01-12 01:04:06,225 iteration 5938 : loss : 0.018346, loss_ce: 0.007338
2022-01-12 01:04:07,804 iteration 5939 : loss : 0.013777, loss_ce: 0.005453
2022-01-12 01:04:09,435 iteration 5940 : loss : 0.014176, loss_ce: 0.006110
2022-01-12 01:04:10,934 iteration 5941 : loss : 0.011710, loss_ce: 0.005162
2022-01-12 01:04:12,568 iteration 5942 : loss : 0.030861, loss_ce: 0.009063
2022-01-12 01:04:14,087 iteration 5943 : loss : 0.009276, loss_ce: 0.003368
2022-01-12 01:04:15,680 iteration 5944 : loss : 0.015775, loss_ce: 0.005683
2022-01-12 01:04:17,255 iteration 5945 : loss : 0.010076, loss_ce: 0.003165
2022-01-12 01:04:18,798 iteration 5946 : loss : 0.016931, loss_ce: 0.005774
2022-01-12 01:04:20,321 iteration 5947 : loss : 0.014250, loss_ce: 0.005007
2022-01-12 01:04:21,982 iteration 5948 : loss : 0.015614, loss_ce: 0.006368
2022-01-12 01:04:23,526 iteration 5949 : loss : 0.010670, loss_ce: 0.004203
2022-01-12 01:04:23,526 Training Data Eval:
2022-01-12 01:04:31,488   Average segmentation loss on training set: 0.0080
2022-01-12 01:04:31,489 Validation Data Eval:
2022-01-12 01:04:34,230   Average segmentation loss on validation set: 0.0674
2022-01-12 01:04:35,893 iteration 5950 : loss : 0.013802, loss_ce: 0.003685
 88%|█████████████████████████▍   | 350/400 [2:52:44<25:54, 31.09s/it]2022-01-12 01:04:37,500 iteration 5951 : loss : 0.010024, loss_ce: 0.002611
2022-01-12 01:04:39,105 iteration 5952 : loss : 0.011494, loss_ce: 0.005489
2022-01-12 01:04:40,715 iteration 5953 : loss : 0.014800, loss_ce: 0.006332
2022-01-12 01:04:42,411 iteration 5954 : loss : 0.021302, loss_ce: 0.006501
2022-01-12 01:04:43,983 iteration 5955 : loss : 0.012760, loss_ce: 0.004699
2022-01-12 01:04:45,667 iteration 5956 : loss : 0.016186, loss_ce: 0.005166
2022-01-12 01:04:47,246 iteration 5957 : loss : 0.021648, loss_ce: 0.005841
2022-01-12 01:04:48,793 iteration 5958 : loss : 0.010806, loss_ce: 0.004737
2022-01-12 01:04:50,383 iteration 5959 : loss : 0.012829, loss_ce: 0.004377
2022-01-12 01:04:51,924 iteration 5960 : loss : 0.009225, loss_ce: 0.002872
2022-01-12 01:04:53,529 iteration 5961 : loss : 0.015339, loss_ce: 0.005823
2022-01-12 01:04:55,186 iteration 5962 : loss : 0.015063, loss_ce: 0.006152
2022-01-12 01:04:56,689 iteration 5963 : loss : 0.010140, loss_ce: 0.004341
2022-01-12 01:04:58,459 iteration 5964 : loss : 0.036156, loss_ce: 0.013442
2022-01-12 01:05:00,025 iteration 5965 : loss : 0.013383, loss_ce: 0.005631
2022-01-12 01:05:01,666 iteration 5966 : loss : 0.017210, loss_ce: 0.007038
2022-01-12 01:05:03,226 iteration 5967 : loss : 0.018540, loss_ce: 0.006031
 88%|█████████████████████████▍   | 351/400 [2:53:11<24:28, 29.96s/it]2022-01-12 01:05:04,835 iteration 5968 : loss : 0.016063, loss_ce: 0.006980
2022-01-12 01:05:06,469 iteration 5969 : loss : 0.032634, loss_ce: 0.009846
2022-01-12 01:05:08,080 iteration 5970 : loss : 0.011409, loss_ce: 0.003889
2022-01-12 01:05:09,583 iteration 5971 : loss : 0.011508, loss_ce: 0.004551
2022-01-12 01:05:11,133 iteration 5972 : loss : 0.013175, loss_ce: 0.004769
2022-01-12 01:05:12,741 iteration 5973 : loss : 0.009346, loss_ce: 0.003815
2022-01-12 01:05:14,277 iteration 5974 : loss : 0.012763, loss_ce: 0.004986
2022-01-12 01:05:15,902 iteration 5975 : loss : 0.023279, loss_ce: 0.007510
2022-01-12 01:05:17,511 iteration 5976 : loss : 0.016255, loss_ce: 0.003989
2022-01-12 01:05:19,132 iteration 5977 : loss : 0.012732, loss_ce: 0.004231
2022-01-12 01:05:20,738 iteration 5978 : loss : 0.013827, loss_ce: 0.005685
2022-01-12 01:05:22,297 iteration 5979 : loss : 0.024763, loss_ce: 0.014829
2022-01-12 01:05:23,931 iteration 5980 : loss : 0.009497, loss_ce: 0.003280
2022-01-12 01:05:25,555 iteration 5981 : loss : 0.015530, loss_ce: 0.005639
2022-01-12 01:05:27,221 iteration 5982 : loss : 0.024834, loss_ce: 0.008645
2022-01-12 01:05:28,740 iteration 5983 : loss : 0.011372, loss_ce: 0.005076
2022-01-12 01:05:30,274 iteration 5984 : loss : 0.015132, loss_ce: 0.004419
 88%|█████████████████████████▌   | 352/400 [2:53:38<23:16, 29.09s/it]2022-01-12 01:05:32,041 iteration 5985 : loss : 0.027338, loss_ce: 0.011440
2022-01-12 01:05:33,607 iteration 5986 : loss : 0.020074, loss_ce: 0.007811
2022-01-12 01:05:35,272 iteration 5987 : loss : 0.035287, loss_ce: 0.005852
2022-01-12 01:05:36,965 iteration 5988 : loss : 0.014770, loss_ce: 0.007291
2022-01-12 01:05:38,550 iteration 5989 : loss : 0.015750, loss_ce: 0.006355
2022-01-12 01:05:40,186 iteration 5990 : loss : 0.016598, loss_ce: 0.008708
2022-01-12 01:05:41,687 iteration 5991 : loss : 0.011092, loss_ce: 0.003957
2022-01-12 01:05:43,315 iteration 5992 : loss : 0.017712, loss_ce: 0.005083
2022-01-12 01:05:44,862 iteration 5993 : loss : 0.008410, loss_ce: 0.002728
2022-01-12 01:05:46,433 iteration 5994 : loss : 0.016517, loss_ce: 0.005498
2022-01-12 01:05:48,046 iteration 5995 : loss : 0.017842, loss_ce: 0.006656
2022-01-12 01:05:49,721 iteration 5996 : loss : 0.015329, loss_ce: 0.004917
2022-01-12 01:05:51,411 iteration 5997 : loss : 0.015450, loss_ce: 0.006976
2022-01-12 01:05:52,962 iteration 5998 : loss : 0.012541, loss_ce: 0.004501
2022-01-12 01:05:54,593 iteration 5999 : loss : 0.021675, loss_ce: 0.005878
2022-01-12 01:05:56,224 iteration 6000 : loss : 0.019277, loss_ce: 0.008050
2022-01-12 01:05:57,906 iteration 6001 : loss : 0.026537, loss_ce: 0.008256
 88%|█████████████████████████▌   | 353/400 [2:54:06<22:26, 28.65s/it]2022-01-12 01:05:59,580 iteration 6002 : loss : 0.012650, loss_ce: 0.005203
2022-01-12 01:06:01,232 iteration 6003 : loss : 0.018979, loss_ce: 0.006452
2022-01-12 01:06:02,831 iteration 6004 : loss : 0.015469, loss_ce: 0.008104
2022-01-12 01:06:04,476 iteration 6005 : loss : 0.010664, loss_ce: 0.004607
2022-01-12 01:06:06,012 iteration 6006 : loss : 0.012793, loss_ce: 0.006361
2022-01-12 01:06:07,614 iteration 6007 : loss : 0.012750, loss_ce: 0.004077
2022-01-12 01:06:09,203 iteration 6008 : loss : 0.022094, loss_ce: 0.009084
2022-01-12 01:06:10,892 iteration 6009 : loss : 0.023359, loss_ce: 0.008471
2022-01-12 01:06:12,383 iteration 6010 : loss : 0.010440, loss_ce: 0.004420
2022-01-12 01:06:14,000 iteration 6011 : loss : 0.025199, loss_ce: 0.008495
2022-01-12 01:06:15,599 iteration 6012 : loss : 0.026055, loss_ce: 0.008224
2022-01-12 01:06:17,315 iteration 6013 : loss : 0.027896, loss_ce: 0.012096
2022-01-12 01:06:18,917 iteration 6014 : loss : 0.016804, loss_ce: 0.006119
2022-01-12 01:06:20,474 iteration 6015 : loss : 0.015536, loss_ce: 0.005007
2022-01-12 01:06:22,081 iteration 6016 : loss : 0.015178, loss_ce: 0.004262
2022-01-12 01:06:23,609 iteration 6017 : loss : 0.017374, loss_ce: 0.005029
2022-01-12 01:06:25,311 iteration 6018 : loss : 0.016318, loss_ce: 0.006009
 88%|█████████████████████████▋   | 354/400 [2:54:33<21:40, 28.28s/it]2022-01-12 01:06:26,995 iteration 6019 : loss : 0.016685, loss_ce: 0.006346
2022-01-12 01:06:28,591 iteration 6020 : loss : 0.015711, loss_ce: 0.006704
2022-01-12 01:06:30,181 iteration 6021 : loss : 0.013179, loss_ce: 0.004565
2022-01-12 01:06:31,798 iteration 6022 : loss : 0.010352, loss_ce: 0.004222
2022-01-12 01:06:33,298 iteration 6023 : loss : 0.011275, loss_ce: 0.004182
2022-01-12 01:06:34,960 iteration 6024 : loss : 0.014794, loss_ce: 0.005486
2022-01-12 01:06:36,615 iteration 6025 : loss : 0.021608, loss_ce: 0.009064
2022-01-12 01:06:38,250 iteration 6026 : loss : 0.023569, loss_ce: 0.007767
2022-01-12 01:06:39,824 iteration 6027 : loss : 0.013475, loss_ce: 0.005377
2022-01-12 01:06:41,374 iteration 6028 : loss : 0.013779, loss_ce: 0.005461
2022-01-12 01:06:43,038 iteration 6029 : loss : 0.017614, loss_ce: 0.009131
2022-01-12 01:06:44,589 iteration 6030 : loss : 0.013403, loss_ce: 0.004437
2022-01-12 01:06:46,296 iteration 6031 : loss : 0.018758, loss_ce: 0.005592
2022-01-12 01:06:47,946 iteration 6032 : loss : 0.017673, loss_ce: 0.005369
2022-01-12 01:06:49,591 iteration 6033 : loss : 0.015490, loss_ce: 0.006829
2022-01-12 01:06:51,274 iteration 6034 : loss : 0.030033, loss_ce: 0.009471
2022-01-12 01:06:51,274 Training Data Eval:
2022-01-12 01:06:59,230   Average segmentation loss on training set: 0.0078
2022-01-12 01:06:59,230 Validation Data Eval:
2022-01-12 01:07:01,976   Average segmentation loss on validation set: 0.0655
2022-01-12 01:07:03,501 iteration 6035 : loss : 0.019675, loss_ce: 0.005002
 89%|█████████████████████████▋   | 355/400 [2:55:12<23:26, 31.25s/it]2022-01-12 01:07:05,170 iteration 6036 : loss : 0.020143, loss_ce: 0.006001
2022-01-12 01:07:06,852 iteration 6037 : loss : 0.017118, loss_ce: 0.006175
2022-01-12 01:07:08,420 iteration 6038 : loss : 0.015959, loss_ce: 0.006990
2022-01-12 01:07:10,023 iteration 6039 : loss : 0.018035, loss_ce: 0.009807
2022-01-12 01:07:11,629 iteration 6040 : loss : 0.014640, loss_ce: 0.007074
2022-01-12 01:07:13,353 iteration 6041 : loss : 0.038103, loss_ce: 0.014290
2022-01-12 01:07:15,032 iteration 6042 : loss : 0.014487, loss_ce: 0.005532
2022-01-12 01:07:16,643 iteration 6043 : loss : 0.015169, loss_ce: 0.007580
2022-01-12 01:07:18,222 iteration 6044 : loss : 0.013714, loss_ce: 0.004369
2022-01-12 01:07:19,882 iteration 6045 : loss : 0.022065, loss_ce: 0.008415
2022-01-12 01:07:21,490 iteration 6046 : loss : 0.018506, loss_ce: 0.006391
2022-01-12 01:07:23,053 iteration 6047 : loss : 0.011436, loss_ce: 0.002905
2022-01-12 01:07:24,770 iteration 6048 : loss : 0.027438, loss_ce: 0.004531
2022-01-12 01:07:26,428 iteration 6049 : loss : 0.016977, loss_ce: 0.007429
2022-01-12 01:07:28,003 iteration 6050 : loss : 0.015457, loss_ce: 0.006515
2022-01-12 01:07:29,664 iteration 6051 : loss : 0.015744, loss_ce: 0.006075
2022-01-12 01:07:31,169 iteration 6052 : loss : 0.013375, loss_ce: 0.003323
 89%|█████████████████████████▊   | 356/400 [2:55:39<22:07, 30.17s/it]2022-01-12 01:07:32,777 iteration 6053 : loss : 0.012291, loss_ce: 0.005810
2022-01-12 01:07:34,474 iteration 6054 : loss : 0.022400, loss_ce: 0.011055
2022-01-12 01:07:36,048 iteration 6055 : loss : 0.012911, loss_ce: 0.004472
2022-01-12 01:07:37,609 iteration 6056 : loss : 0.011413, loss_ce: 0.004272
2022-01-12 01:07:39,171 iteration 6057 : loss : 0.012867, loss_ce: 0.004468
2022-01-12 01:07:40,829 iteration 6058 : loss : 0.014733, loss_ce: 0.006203
2022-01-12 01:07:42,356 iteration 6059 : loss : 0.010421, loss_ce: 0.004194
2022-01-12 01:07:44,021 iteration 6060 : loss : 0.011330, loss_ce: 0.004547
2022-01-12 01:07:45,551 iteration 6061 : loss : 0.012433, loss_ce: 0.005352
2022-01-12 01:07:47,246 iteration 6062 : loss : 0.014880, loss_ce: 0.006912
2022-01-12 01:07:48,940 iteration 6063 : loss : 0.028822, loss_ce: 0.010734
2022-01-12 01:07:50,543 iteration 6064 : loss : 0.025807, loss_ce: 0.007066
2022-01-12 01:07:52,238 iteration 6065 : loss : 0.020481, loss_ce: 0.010033
2022-01-12 01:07:53,828 iteration 6066 : loss : 0.013192, loss_ce: 0.005486
2022-01-12 01:07:55,376 iteration 6067 : loss : 0.024517, loss_ce: 0.004303
2022-01-12 01:07:57,085 iteration 6068 : loss : 0.024266, loss_ce: 0.007465
2022-01-12 01:07:58,658 iteration 6069 : loss : 0.015993, loss_ce: 0.002530
 89%|█████████████████████████▉   | 357/400 [2:56:07<21:02, 29.37s/it]2022-01-12 01:08:00,343 iteration 6070 : loss : 0.014741, loss_ce: 0.006689
2022-01-12 01:08:02,000 iteration 6071 : loss : 0.017479, loss_ce: 0.005117
2022-01-12 01:08:03,620 iteration 6072 : loss : 0.012159, loss_ce: 0.004265
2022-01-12 01:08:05,263 iteration 6073 : loss : 0.017268, loss_ce: 0.006636
2022-01-12 01:08:06,881 iteration 6074 : loss : 0.009591, loss_ce: 0.002685
2022-01-12 01:08:08,550 iteration 6075 : loss : 0.016499, loss_ce: 0.006666
2022-01-12 01:08:10,124 iteration 6076 : loss : 0.011257, loss_ce: 0.004714
2022-01-12 01:08:11,689 iteration 6077 : loss : 0.011747, loss_ce: 0.005482
2022-01-12 01:08:13,298 iteration 6078 : loss : 0.011795, loss_ce: 0.003647
2022-01-12 01:08:14,914 iteration 6079 : loss : 0.016191, loss_ce: 0.003734
2022-01-12 01:08:16,517 iteration 6080 : loss : 0.012927, loss_ce: 0.004290
2022-01-12 01:08:18,032 iteration 6081 : loss : 0.013224, loss_ce: 0.004216
2022-01-12 01:08:19,704 iteration 6082 : loss : 0.020473, loss_ce: 0.007910
2022-01-12 01:08:21,397 iteration 6083 : loss : 0.027399, loss_ce: 0.012254
2022-01-12 01:08:22,945 iteration 6084 : loss : 0.015429, loss_ce: 0.005437
2022-01-12 01:08:24,487 iteration 6085 : loss : 0.012771, loss_ce: 0.006516
2022-01-12 01:08:26,235 iteration 6086 : loss : 0.024802, loss_ce: 0.008920
 90%|█████████████████████████▉   | 358/400 [2:56:34<20:10, 28.83s/it]2022-01-12 01:08:27,881 iteration 6087 : loss : 0.021794, loss_ce: 0.007282
2022-01-12 01:08:29,498 iteration 6088 : loss : 0.015588, loss_ce: 0.004878
2022-01-12 01:08:31,155 iteration 6089 : loss : 0.015910, loss_ce: 0.007851
2022-01-12 01:08:32,724 iteration 6090 : loss : 0.011377, loss_ce: 0.003414
2022-01-12 01:08:34,275 iteration 6091 : loss : 0.007769, loss_ce: 0.002235
2022-01-12 01:08:35,953 iteration 6092 : loss : 0.018976, loss_ce: 0.006332
2022-01-12 01:08:37,676 iteration 6093 : loss : 0.013400, loss_ce: 0.005826
2022-01-12 01:08:39,351 iteration 6094 : loss : 0.018360, loss_ce: 0.008897
2022-01-12 01:08:40,873 iteration 6095 : loss : 0.017474, loss_ce: 0.004295
2022-01-12 01:08:42,551 iteration 6096 : loss : 0.016621, loss_ce: 0.004010
2022-01-12 01:08:44,093 iteration 6097 : loss : 0.010612, loss_ce: 0.005027
2022-01-12 01:08:45,713 iteration 6098 : loss : 0.012479, loss_ce: 0.005899
2022-01-12 01:08:47,311 iteration 6099 : loss : 0.013490, loss_ce: 0.005951
2022-01-12 01:08:48,938 iteration 6100 : loss : 0.015752, loss_ce: 0.004639
2022-01-12 01:08:50,620 iteration 6101 : loss : 0.015251, loss_ce: 0.006435
2022-01-12 01:08:52,165 iteration 6102 : loss : 0.012805, loss_ce: 0.005141
2022-01-12 01:08:53,791 iteration 6103 : loss : 0.016764, loss_ce: 0.005416
 90%|██████████████████████████   | 359/400 [2:57:02<19:26, 28.45s/it]2022-01-12 01:08:55,478 iteration 6104 : loss : 0.018966, loss_ce: 0.008927
2022-01-12 01:08:57,048 iteration 6105 : loss : 0.018623, loss_ce: 0.004513
2022-01-12 01:08:58,598 iteration 6106 : loss : 0.009563, loss_ce: 0.003818
2022-01-12 01:09:00,138 iteration 6107 : loss : 0.010233, loss_ce: 0.004102
2022-01-12 01:09:01,726 iteration 6108 : loss : 0.015862, loss_ce: 0.006221
2022-01-12 01:09:03,368 iteration 6109 : loss : 0.013357, loss_ce: 0.006030
2022-01-12 01:09:04,895 iteration 6110 : loss : 0.014677, loss_ce: 0.006940
2022-01-12 01:09:06,505 iteration 6111 : loss : 0.024160, loss_ce: 0.007774
2022-01-12 01:09:08,158 iteration 6112 : loss : 0.014981, loss_ce: 0.004353
2022-01-12 01:09:09,752 iteration 6113 : loss : 0.019647, loss_ce: 0.007193
2022-01-12 01:09:11,347 iteration 6114 : loss : 0.019792, loss_ce: 0.009022
2022-01-12 01:09:13,014 iteration 6115 : loss : 0.027826, loss_ce: 0.009032
2022-01-12 01:09:14,611 iteration 6116 : loss : 0.011813, loss_ce: 0.003651
2022-01-12 01:09:16,172 iteration 6117 : loss : 0.014357, loss_ce: 0.005191
2022-01-12 01:09:17,799 iteration 6118 : loss : 0.013287, loss_ce: 0.006856
2022-01-12 01:09:19,375 iteration 6119 : loss : 0.013226, loss_ce: 0.005972
2022-01-12 01:09:19,376 Training Data Eval:
2022-01-12 01:09:27,328   Average segmentation loss on training set: 0.0074
2022-01-12 01:09:27,329 Validation Data Eval:
2022-01-12 01:09:30,071   Average segmentation loss on validation set: 0.0651
2022-01-12 01:09:31,715 iteration 6120 : loss : 0.018530, loss_ce: 0.007442
 90%|██████████████████████████   | 360/400 [2:57:40<20:51, 31.29s/it]2022-01-12 01:09:33,399 iteration 6121 : loss : 0.012789, loss_ce: 0.004618
2022-01-12 01:09:35,003 iteration 6122 : loss : 0.013408, loss_ce: 0.006224
2022-01-12 01:09:36,590 iteration 6123 : loss : 0.011129, loss_ce: 0.003900
2022-01-12 01:09:38,282 iteration 6124 : loss : 0.020877, loss_ce: 0.007386
2022-01-12 01:09:39,904 iteration 6125 : loss : 0.016756, loss_ce: 0.004864
2022-01-12 01:09:41,625 iteration 6126 : loss : 0.014950, loss_ce: 0.005390
2022-01-12 01:09:43,347 iteration 6127 : loss : 0.017611, loss_ce: 0.008510
2022-01-12 01:09:44,943 iteration 6128 : loss : 0.011175, loss_ce: 0.004162
2022-01-12 01:09:46,493 iteration 6129 : loss : 0.012763, loss_ce: 0.004332
2022-01-12 01:09:48,110 iteration 6130 : loss : 0.022653, loss_ce: 0.008647
2022-01-12 01:09:49,659 iteration 6131 : loss : 0.012485, loss_ce: 0.003897
2022-01-12 01:09:51,316 iteration 6132 : loss : 0.018585, loss_ce: 0.005972
2022-01-12 01:09:52,969 iteration 6133 : loss : 0.013835, loss_ce: 0.005712
2022-01-12 01:09:54,547 iteration 6134 : loss : 0.015071, loss_ce: 0.004670
2022-01-12 01:09:56,081 iteration 6135 : loss : 0.015095, loss_ce: 0.007199
2022-01-12 01:09:57,632 iteration 6136 : loss : 0.008166, loss_ce: 0.002348
2022-01-12 01:09:59,189 iteration 6137 : loss : 0.013522, loss_ce: 0.004573
 90%|██████████████████████████▏  | 361/400 [2:58:07<19:35, 30.14s/it]2022-01-12 01:10:00,781 iteration 6138 : loss : 0.012809, loss_ce: 0.004669
2022-01-12 01:10:02,339 iteration 6139 : loss : 0.012987, loss_ce: 0.003323
2022-01-12 01:10:03,883 iteration 6140 : loss : 0.012653, loss_ce: 0.004095
2022-01-12 01:10:05,398 iteration 6141 : loss : 0.013127, loss_ce: 0.005559
2022-01-12 01:10:07,017 iteration 6142 : loss : 0.013738, loss_ce: 0.005507
2022-01-12 01:10:08,545 iteration 6143 : loss : 0.014204, loss_ce: 0.006632
2022-01-12 01:10:10,201 iteration 6144 : loss : 0.018362, loss_ce: 0.008673
2022-01-12 01:10:11,734 iteration 6145 : loss : 0.013575, loss_ce: 0.005505
2022-01-12 01:10:13,235 iteration 6146 : loss : 0.010872, loss_ce: 0.003502
2022-01-12 01:10:14,792 iteration 6147 : loss : 0.012422, loss_ce: 0.004597
2022-01-12 01:10:16,345 iteration 6148 : loss : 0.013969, loss_ce: 0.005939
2022-01-12 01:10:17,876 iteration 6149 : loss : 0.012719, loss_ce: 0.004182
2022-01-12 01:10:19,480 iteration 6150 : loss : 0.021437, loss_ce: 0.007095
2022-01-12 01:10:21,202 iteration 6151 : loss : 0.015491, loss_ce: 0.004246
2022-01-12 01:10:22,775 iteration 6152 : loss : 0.010981, loss_ce: 0.004224
2022-01-12 01:10:24,374 iteration 6153 : loss : 0.012129, loss_ce: 0.004517
2022-01-12 01:10:26,031 iteration 6154 : loss : 0.022471, loss_ce: 0.007745
 90%|██████████████████████████▏  | 362/400 [2:58:34<18:27, 29.15s/it]2022-01-12 01:10:27,720 iteration 6155 : loss : 0.018138, loss_ce: 0.007133
2022-01-12 01:10:29,255 iteration 6156 : loss : 0.010484, loss_ce: 0.004431
2022-01-12 01:10:30,831 iteration 6157 : loss : 0.010408, loss_ce: 0.003069
2022-01-12 01:10:32,441 iteration 6158 : loss : 0.010482, loss_ce: 0.003981
2022-01-12 01:10:33,977 iteration 6159 : loss : 0.015107, loss_ce: 0.004778
2022-01-12 01:10:35,548 iteration 6160 : loss : 0.012342, loss_ce: 0.005080
2022-01-12 01:10:37,141 iteration 6161 : loss : 0.013006, loss_ce: 0.003621
2022-01-12 01:10:38,772 iteration 6162 : loss : 0.021245, loss_ce: 0.003847
2022-01-12 01:10:40,373 iteration 6163 : loss : 0.012845, loss_ce: 0.005748
2022-01-12 01:10:42,017 iteration 6164 : loss : 0.016156, loss_ce: 0.007666
2022-01-12 01:10:43,601 iteration 6165 : loss : 0.017255, loss_ce: 0.007401
2022-01-12 01:10:45,178 iteration 6166 : loss : 0.012723, loss_ce: 0.005645
2022-01-12 01:10:46,729 iteration 6167 : loss : 0.016087, loss_ce: 0.006356
2022-01-12 01:10:48,370 iteration 6168 : loss : 0.017083, loss_ce: 0.006426
2022-01-12 01:10:49,987 iteration 6169 : loss : 0.014727, loss_ce: 0.004165
2022-01-12 01:10:51,556 iteration 6170 : loss : 0.015489, loss_ce: 0.005807
2022-01-12 01:10:53,100 iteration 6171 : loss : 0.011336, loss_ce: 0.004416
 91%|██████████████████████████▎  | 363/400 [2:59:01<17:35, 28.53s/it]2022-01-12 01:10:54,696 iteration 6172 : loss : 0.013222, loss_ce: 0.004893
2022-01-12 01:10:56,381 iteration 6173 : loss : 0.014332, loss_ce: 0.004681
2022-01-12 01:10:57,962 iteration 6174 : loss : 0.013621, loss_ce: 0.004905
2022-01-12 01:10:59,601 iteration 6175 : loss : 0.026248, loss_ce: 0.010460
2022-01-12 01:11:01,209 iteration 6176 : loss : 0.012708, loss_ce: 0.004466
2022-01-12 01:11:02,899 iteration 6177 : loss : 0.018346, loss_ce: 0.007341
2022-01-12 01:11:04,494 iteration 6178 : loss : 0.012947, loss_ce: 0.006124
2022-01-12 01:11:06,084 iteration 6179 : loss : 0.017499, loss_ce: 0.009414
2022-01-12 01:11:07,682 iteration 6180 : loss : 0.016228, loss_ce: 0.005088
2022-01-12 01:11:09,165 iteration 6181 : loss : 0.009169, loss_ce: 0.002977
2022-01-12 01:11:10,761 iteration 6182 : loss : 0.016906, loss_ce: 0.004220
2022-01-12 01:11:12,432 iteration 6183 : loss : 0.023824, loss_ce: 0.009795
2022-01-12 01:11:14,050 iteration 6184 : loss : 0.019312, loss_ce: 0.006201
2022-01-12 01:11:15,624 iteration 6185 : loss : 0.039198, loss_ce: 0.015871
2022-01-12 01:11:17,222 iteration 6186 : loss : 0.017466, loss_ce: 0.005696
2022-01-12 01:11:18,879 iteration 6187 : loss : 0.013484, loss_ce: 0.005050
2022-01-12 01:11:20,376 iteration 6188 : loss : 0.012999, loss_ce: 0.004043
 91%|██████████████████████████▍  | 364/400 [2:59:29<16:53, 28.15s/it]2022-01-12 01:11:21,928 iteration 6189 : loss : 0.012670, loss_ce: 0.003524
2022-01-12 01:11:23,417 iteration 6190 : loss : 0.012661, loss_ce: 0.004318
2022-01-12 01:11:25,149 iteration 6191 : loss : 0.028349, loss_ce: 0.009904
2022-01-12 01:11:26,792 iteration 6192 : loss : 0.011339, loss_ce: 0.003989
2022-01-12 01:11:28,259 iteration 6193 : loss : 0.008648, loss_ce: 0.003270
2022-01-12 01:11:29,837 iteration 6194 : loss : 0.013895, loss_ce: 0.005032
2022-01-12 01:11:31,463 iteration 6195 : loss : 0.013611, loss_ce: 0.005399
2022-01-12 01:11:33,109 iteration 6196 : loss : 0.011424, loss_ce: 0.004610
2022-01-12 01:11:34,719 iteration 6197 : loss : 0.016562, loss_ce: 0.009173
2022-01-12 01:11:36,307 iteration 6198 : loss : 0.011510, loss_ce: 0.004438
2022-01-12 01:11:37,894 iteration 6199 : loss : 0.029195, loss_ce: 0.006703
2022-01-12 01:11:39,391 iteration 6200 : loss : 0.008582, loss_ce: 0.003981
2022-01-12 01:11:40,983 iteration 6201 : loss : 0.019074, loss_ce: 0.006684
2022-01-12 01:11:42,519 iteration 6202 : loss : 0.012602, loss_ce: 0.005233
2022-01-12 01:11:44,062 iteration 6203 : loss : 0.014678, loss_ce: 0.003481
2022-01-12 01:11:45,674 iteration 6204 : loss : 0.013068, loss_ce: 0.005746
2022-01-12 01:11:45,675 Training Data Eval:
2022-01-12 01:11:53,613   Average segmentation loss on training set: 0.0074
2022-01-12 01:11:53,613 Validation Data Eval:
2022-01-12 01:11:56,354   Average segmentation loss on validation set: 0.0754
2022-01-12 01:11:57,953 iteration 6205 : loss : 0.025252, loss_ce: 0.007924
 91%|██████████████████████████▍  | 365/400 [3:00:06<18:04, 30.98s/it]2022-01-12 01:11:59,603 iteration 6206 : loss : 0.018069, loss_ce: 0.009205
2022-01-12 01:12:01,153 iteration 6207 : loss : 0.013066, loss_ce: 0.003750
2022-01-12 01:12:02,856 iteration 6208 : loss : 0.021084, loss_ce: 0.007146
2022-01-12 01:12:04,419 iteration 6209 : loss : 0.015398, loss_ce: 0.004290
2022-01-12 01:12:05,955 iteration 6210 : loss : 0.013503, loss_ce: 0.005087
2022-01-12 01:12:07,575 iteration 6211 : loss : 0.015116, loss_ce: 0.006575
2022-01-12 01:12:09,130 iteration 6212 : loss : 0.010882, loss_ce: 0.003514
2022-01-12 01:12:10,681 iteration 6213 : loss : 0.020818, loss_ce: 0.004770
2022-01-12 01:12:12,229 iteration 6214 : loss : 0.012364, loss_ce: 0.002631
2022-01-12 01:12:13,848 iteration 6215 : loss : 0.016589, loss_ce: 0.008083
2022-01-12 01:12:15,443 iteration 6216 : loss : 0.014365, loss_ce: 0.004520
2022-01-12 01:12:16,939 iteration 6217 : loss : 0.011255, loss_ce: 0.003532
2022-01-12 01:12:18,522 iteration 6218 : loss : 0.015792, loss_ce: 0.008999
2022-01-12 01:12:20,072 iteration 6219 : loss : 0.014527, loss_ce: 0.005051
2022-01-12 01:12:21,637 iteration 6220 : loss : 0.014683, loss_ce: 0.004399
2022-01-12 01:12:23,309 iteration 6221 : loss : 0.012913, loss_ce: 0.005803
2022-01-12 01:12:24,904 iteration 6222 : loss : 0.014544, loss_ce: 0.006559
 92%|██████████████████████████▌  | 366/400 [3:00:33<16:52, 29.77s/it]2022-01-12 01:12:26,557 iteration 6223 : loss : 0.025766, loss_ce: 0.006307
2022-01-12 01:12:28,118 iteration 6224 : loss : 0.012548, loss_ce: 0.003738
2022-01-12 01:12:29,692 iteration 6225 : loss : 0.013743, loss_ce: 0.005206
2022-01-12 01:12:31,353 iteration 6226 : loss : 0.016708, loss_ce: 0.006762
2022-01-12 01:12:32,935 iteration 6227 : loss : 0.014869, loss_ce: 0.003359
2022-01-12 01:12:34,537 iteration 6228 : loss : 0.015105, loss_ce: 0.006528
2022-01-12 01:12:36,056 iteration 6229 : loss : 0.013425, loss_ce: 0.005548
2022-01-12 01:12:37,598 iteration 6230 : loss : 0.015340, loss_ce: 0.006437
2022-01-12 01:12:39,191 iteration 6231 : loss : 0.016643, loss_ce: 0.007028
2022-01-12 01:12:40,794 iteration 6232 : loss : 0.015378, loss_ce: 0.006561
2022-01-12 01:12:42,363 iteration 6233 : loss : 0.012119, loss_ce: 0.005496
2022-01-12 01:12:43,970 iteration 6234 : loss : 0.015208, loss_ce: 0.005975
2022-01-12 01:12:45,646 iteration 6235 : loss : 0.022821, loss_ce: 0.005576
2022-01-12 01:12:47,146 iteration 6236 : loss : 0.010497, loss_ce: 0.004145
2022-01-12 01:12:48,714 iteration 6237 : loss : 0.012297, loss_ce: 0.005192
2022-01-12 01:12:50,229 iteration 6238 : loss : 0.008256, loss_ce: 0.002743
2022-01-12 01:12:51,767 iteration 6239 : loss : 0.009682, loss_ce: 0.004079
 92%|██████████████████████████▌  | 367/400 [3:01:00<15:53, 28.90s/it]2022-01-12 01:12:53,444 iteration 6240 : loss : 0.014255, loss_ce: 0.005288
2022-01-12 01:12:55,073 iteration 6241 : loss : 0.020434, loss_ce: 0.010263
2022-01-12 01:12:56,705 iteration 6242 : loss : 0.014197, loss_ce: 0.005447
2022-01-12 01:12:58,207 iteration 6243 : loss : 0.009757, loss_ce: 0.003627
2022-01-12 01:12:59,740 iteration 6244 : loss : 0.010059, loss_ce: 0.002932
2022-01-12 01:13:01,292 iteration 6245 : loss : 0.012153, loss_ce: 0.004657
2022-01-12 01:13:02,830 iteration 6246 : loss : 0.013208, loss_ce: 0.005089
2022-01-12 01:13:04,368 iteration 6247 : loss : 0.014055, loss_ce: 0.003474
2022-01-12 01:13:06,002 iteration 6248 : loss : 0.020526, loss_ce: 0.008859
2022-01-12 01:13:07,557 iteration 6249 : loss : 0.016306, loss_ce: 0.005318
2022-01-12 01:13:09,169 iteration 6250 : loss : 0.026824, loss_ce: 0.010369
2022-01-12 01:13:10,849 iteration 6251 : loss : 0.012352, loss_ce: 0.004619
2022-01-12 01:13:12,474 iteration 6252 : loss : 0.028790, loss_ce: 0.008530
2022-01-12 01:13:14,016 iteration 6253 : loss : 0.014201, loss_ce: 0.003882
2022-01-12 01:13:15,523 iteration 6254 : loss : 0.013026, loss_ce: 0.004730
2022-01-12 01:13:17,162 iteration 6255 : loss : 0.013062, loss_ce: 0.004999
2022-01-12 01:13:18,744 iteration 6256 : loss : 0.012253, loss_ce: 0.004261
 92%|██████████████████████████▋  | 368/400 [3:01:27<15:06, 28.32s/it]2022-01-12 01:13:20,397 iteration 6257 : loss : 0.011263, loss_ce: 0.003600
2022-01-12 01:13:21,922 iteration 6258 : loss : 0.009788, loss_ce: 0.003300
2022-01-12 01:13:23,561 iteration 6259 : loss : 0.014340, loss_ce: 0.005271
2022-01-12 01:13:25,274 iteration 6260 : loss : 0.025734, loss_ce: 0.007254
2022-01-12 01:13:26,877 iteration 6261 : loss : 0.016531, loss_ce: 0.008010
2022-01-12 01:13:28,512 iteration 6262 : loss : 0.015008, loss_ce: 0.006063
2022-01-12 01:13:30,057 iteration 6263 : loss : 0.012415, loss_ce: 0.004058
2022-01-12 01:13:31,579 iteration 6264 : loss : 0.012835, loss_ce: 0.005254
2022-01-12 01:13:33,164 iteration 6265 : loss : 0.013852, loss_ce: 0.005775
2022-01-12 01:13:34,719 iteration 6266 : loss : 0.011086, loss_ce: 0.003989
2022-01-12 01:13:36,480 iteration 6267 : loss : 0.018371, loss_ce: 0.007454
2022-01-12 01:13:38,050 iteration 6268 : loss : 0.011681, loss_ce: 0.003888
2022-01-12 01:13:39,642 iteration 6269 : loss : 0.014186, loss_ce: 0.006116
2022-01-12 01:13:41,201 iteration 6270 : loss : 0.013403, loss_ce: 0.005691
2022-01-12 01:13:42,865 iteration 6271 : loss : 0.014669, loss_ce: 0.005915
2022-01-12 01:13:44,404 iteration 6272 : loss : 0.022557, loss_ce: 0.005776
2022-01-12 01:13:46,080 iteration 6273 : loss : 0.020982, loss_ce: 0.008184
 92%|██████████████████████████▊  | 369/400 [3:01:54<14:28, 28.03s/it]2022-01-12 01:13:47,821 iteration 6274 : loss : 0.018057, loss_ce: 0.006360
2022-01-12 01:13:49,381 iteration 6275 : loss : 0.015142, loss_ce: 0.003916
2022-01-12 01:13:50,892 iteration 6276 : loss : 0.015774, loss_ce: 0.006513
2022-01-12 01:13:52,467 iteration 6277 : loss : 0.014748, loss_ce: 0.005834
2022-01-12 01:13:54,118 iteration 6278 : loss : 0.017856, loss_ce: 0.006780
2022-01-12 01:13:55,761 iteration 6279 : loss : 0.015361, loss_ce: 0.004979
2022-01-12 01:13:57,283 iteration 6280 : loss : 0.013027, loss_ce: 0.004524
2022-01-12 01:13:58,836 iteration 6281 : loss : 0.015293, loss_ce: 0.004326
2022-01-12 01:14:00,412 iteration 6282 : loss : 0.023232, loss_ce: 0.013547
2022-01-12 01:14:02,084 iteration 6283 : loss : 0.015694, loss_ce: 0.005922
2022-01-12 01:14:03,693 iteration 6284 : loss : 0.014191, loss_ce: 0.004597
2022-01-12 01:14:05,299 iteration 6285 : loss : 0.019505, loss_ce: 0.009461
2022-01-12 01:14:06,941 iteration 6286 : loss : 0.013941, loss_ce: 0.007086
2022-01-12 01:14:08,528 iteration 6287 : loss : 0.012244, loss_ce: 0.004169
2022-01-12 01:14:10,029 iteration 6288 : loss : 0.015357, loss_ce: 0.005145
2022-01-12 01:14:11,591 iteration 6289 : loss : 0.015954, loss_ce: 0.007687
2022-01-12 01:14:11,591 Training Data Eval:
2022-01-12 01:14:19,547   Average segmentation loss on training set: 0.0073
2022-01-12 01:14:19,547 Validation Data Eval:
2022-01-12 01:14:22,292   Average segmentation loss on validation set: 0.0683
2022-01-12 01:14:23,868 iteration 6290 : loss : 0.010721, loss_ce: 0.003865
 92%|██████████████████████████▊  | 370/400 [3:02:32<15:28, 30.95s/it]2022-01-12 01:14:25,460 iteration 6291 : loss : 0.014276, loss_ce: 0.004244
2022-01-12 01:14:26,973 iteration 6292 : loss : 0.011971, loss_ce: 0.003505
2022-01-12 01:14:28,502 iteration 6293 : loss : 0.013276, loss_ce: 0.003531
2022-01-12 01:14:30,174 iteration 6294 : loss : 0.032989, loss_ce: 0.010582
2022-01-12 01:14:31,756 iteration 6295 : loss : 0.017646, loss_ce: 0.007855
2022-01-12 01:14:33,255 iteration 6296 : loss : 0.009910, loss_ce: 0.004034
2022-01-12 01:14:34,901 iteration 6297 : loss : 0.020585, loss_ce: 0.009641
2022-01-12 01:14:36,476 iteration 6298 : loss : 0.013185, loss_ce: 0.006381
2022-01-12 01:14:38,048 iteration 6299 : loss : 0.011210, loss_ce: 0.004243
2022-01-12 01:14:39,674 iteration 6300 : loss : 0.015769, loss_ce: 0.004388
2022-01-12 01:14:41,273 iteration 6301 : loss : 0.013598, loss_ce: 0.007468
2022-01-12 01:14:42,935 iteration 6302 : loss : 0.018767, loss_ce: 0.006976
2022-01-12 01:14:44,484 iteration 6303 : loss : 0.012487, loss_ce: 0.005138
2022-01-12 01:14:46,089 iteration 6304 : loss : 0.016880, loss_ce: 0.006911
2022-01-12 01:14:47,635 iteration 6305 : loss : 0.011356, loss_ce: 0.003834
2022-01-12 01:14:49,216 iteration 6306 : loss : 0.027106, loss_ce: 0.014199
2022-01-12 01:14:50,769 iteration 6307 : loss : 0.014060, loss_ce: 0.005064
 93%|██████████████████████████▉  | 371/400 [3:02:59<14:22, 29.74s/it]2022-01-12 01:14:52,435 iteration 6308 : loss : 0.014855, loss_ce: 0.005359
2022-01-12 01:14:53,993 iteration 6309 : loss : 0.015399, loss_ce: 0.003856
2022-01-12 01:14:55,578 iteration 6310 : loss : 0.012162, loss_ce: 0.004832
2022-01-12 01:14:57,222 iteration 6311 : loss : 0.020814, loss_ce: 0.006779
2022-01-12 01:14:58,780 iteration 6312 : loss : 0.010852, loss_ce: 0.004864
2022-01-12 01:15:00,383 iteration 6313 : loss : 0.014790, loss_ce: 0.003844
2022-01-12 01:15:02,013 iteration 6314 : loss : 0.013455, loss_ce: 0.005091
2022-01-12 01:15:03,578 iteration 6315 : loss : 0.013180, loss_ce: 0.005705
2022-01-12 01:15:05,084 iteration 6316 : loss : 0.011987, loss_ce: 0.003597
2022-01-12 01:15:06,579 iteration 6317 : loss : 0.013066, loss_ce: 0.004419
2022-01-12 01:15:08,216 iteration 6318 : loss : 0.021163, loss_ce: 0.007910
2022-01-12 01:15:09,788 iteration 6319 : loss : 0.014724, loss_ce: 0.005823
2022-01-12 01:15:11,392 iteration 6320 : loss : 0.011176, loss_ce: 0.005071
2022-01-12 01:15:12,981 iteration 6321 : loss : 0.014803, loss_ce: 0.005291
2022-01-12 01:15:14,676 iteration 6322 : loss : 0.025498, loss_ce: 0.011237
2022-01-12 01:15:16,398 iteration 6323 : loss : 0.023633, loss_ce: 0.008261
2022-01-12 01:15:17,981 iteration 6324 : loss : 0.018037, loss_ce: 0.007147
 93%|██████████████████████████▉  | 372/400 [3:03:26<13:31, 28.98s/it]2022-01-12 01:15:19,563 iteration 6325 : loss : 0.010640, loss_ce: 0.005061
2022-01-12 01:15:21,080 iteration 6326 : loss : 0.009583, loss_ce: 0.003978
2022-01-12 01:15:22,737 iteration 6327 : loss : 0.012288, loss_ce: 0.004942
2022-01-12 01:15:24,322 iteration 6328 : loss : 0.010124, loss_ce: 0.002925
2022-01-12 01:15:25,864 iteration 6329 : loss : 0.014726, loss_ce: 0.003714
2022-01-12 01:15:27,458 iteration 6330 : loss : 0.012836, loss_ce: 0.003916
2022-01-12 01:15:29,246 iteration 6331 : loss : 0.025180, loss_ce: 0.008828
2022-01-12 01:15:30,799 iteration 6332 : loss : 0.015159, loss_ce: 0.006295
2022-01-12 01:15:32,414 iteration 6333 : loss : 0.009901, loss_ce: 0.002786
2022-01-12 01:15:34,000 iteration 6334 : loss : 0.014266, loss_ce: 0.005865
2022-01-12 01:15:35,693 iteration 6335 : loss : 0.015822, loss_ce: 0.006609
2022-01-12 01:15:37,209 iteration 6336 : loss : 0.007949, loss_ce: 0.002998
2022-01-12 01:15:38,824 iteration 6337 : loss : 0.015431, loss_ce: 0.005425
2022-01-12 01:15:40,527 iteration 6338 : loss : 0.019608, loss_ce: 0.003149
2022-01-12 01:15:42,093 iteration 6339 : loss : 0.015902, loss_ce: 0.007043
2022-01-12 01:15:43,659 iteration 6340 : loss : 0.010890, loss_ce: 0.005455
2022-01-12 01:15:45,265 iteration 6341 : loss : 0.012431, loss_ce: 0.004505
 93%|███████████████████████████  | 373/400 [3:03:53<12:48, 28.47s/it]2022-01-12 01:15:46,919 iteration 6342 : loss : 0.020319, loss_ce: 0.011191
2022-01-12 01:15:48,501 iteration 6343 : loss : 0.014573, loss_ce: 0.004493
2022-01-12 01:15:50,173 iteration 6344 : loss : 0.016876, loss_ce: 0.006423
2022-01-12 01:15:51,771 iteration 6345 : loss : 0.010599, loss_ce: 0.003279
2022-01-12 01:15:53,318 iteration 6346 : loss : 0.012957, loss_ce: 0.005671
2022-01-12 01:15:54,964 iteration 6347 : loss : 0.020684, loss_ce: 0.010596
2022-01-12 01:15:56,532 iteration 6348 : loss : 0.014727, loss_ce: 0.006389
2022-01-12 01:15:58,111 iteration 6349 : loss : 0.014731, loss_ce: 0.004433
2022-01-12 01:15:59,701 iteration 6350 : loss : 0.010142, loss_ce: 0.003604
2022-01-12 01:16:01,322 iteration 6351 : loss : 0.014023, loss_ce: 0.006073
2022-01-12 01:16:02,973 iteration 6352 : loss : 0.012866, loss_ce: 0.004988
2022-01-12 01:16:04,708 iteration 6353 : loss : 0.027549, loss_ce: 0.007781
2022-01-12 01:16:06,288 iteration 6354 : loss : 0.019250, loss_ce: 0.004048
2022-01-12 01:16:07,956 iteration 6355 : loss : 0.012325, loss_ce: 0.004061
2022-01-12 01:16:09,652 iteration 6356 : loss : 0.021824, loss_ce: 0.007597
2022-01-12 01:16:11,289 iteration 6357 : loss : 0.012284, loss_ce: 0.006097
2022-01-12 01:16:12,973 iteration 6358 : loss : 0.014033, loss_ce: 0.004094
 94%|███████████████████████████  | 374/400 [3:04:21<12:14, 28.25s/it]2022-01-12 01:16:14,657 iteration 6359 : loss : 0.011099, loss_ce: 0.003439
2022-01-12 01:16:16,135 iteration 6360 : loss : 0.008204, loss_ce: 0.002917
2022-01-12 01:16:17,809 iteration 6361 : loss : 0.012136, loss_ce: 0.004309
2022-01-12 01:16:19,420 iteration 6362 : loss : 0.011238, loss_ce: 0.004525
2022-01-12 01:16:21,123 iteration 6363 : loss : 0.023297, loss_ce: 0.006836
2022-01-12 01:16:22,786 iteration 6364 : loss : 0.016270, loss_ce: 0.005519
2022-01-12 01:16:24,381 iteration 6365 : loss : 0.013934, loss_ce: 0.006096
2022-01-12 01:16:25,963 iteration 6366 : loss : 0.017322, loss_ce: 0.005872
2022-01-12 01:16:27,636 iteration 6367 : loss : 0.015270, loss_ce: 0.005805
2022-01-12 01:16:29,268 iteration 6368 : loss : 0.018961, loss_ce: 0.004839
2022-01-12 01:16:30,862 iteration 6369 : loss : 0.014158, loss_ce: 0.002956
2022-01-12 01:16:32,513 iteration 6370 : loss : 0.012718, loss_ce: 0.005832
2022-01-12 01:16:34,058 iteration 6371 : loss : 0.010086, loss_ce: 0.003964
2022-01-12 01:16:35,672 iteration 6372 : loss : 0.014112, loss_ce: 0.005373
2022-01-12 01:16:37,214 iteration 6373 : loss : 0.011192, loss_ce: 0.005263
2022-01-12 01:16:38,707 iteration 6374 : loss : 0.009985, loss_ce: 0.003754
2022-01-12 01:16:38,707 Training Data Eval:
2022-01-12 01:16:46,666   Average segmentation loss on training set: 0.0070
2022-01-12 01:16:46,667 Validation Data Eval:
2022-01-12 01:16:49,405   Average segmentation loss on validation set: 0.0684
2022-01-12 01:16:51,032 iteration 6375 : loss : 0.010964, loss_ce: 0.005490
 94%|███████████████████████████▏ | 375/400 [3:04:59<12:59, 31.18s/it]2022-01-12 01:16:52,756 iteration 6376 : loss : 0.015328, loss_ce: 0.005332
2022-01-12 01:16:54,377 iteration 6377 : loss : 0.018614, loss_ce: 0.005822
2022-01-12 01:16:55,948 iteration 6378 : loss : 0.015194, loss_ce: 0.008101
2022-01-12 01:16:57,545 iteration 6379 : loss : 0.015068, loss_ce: 0.006377
2022-01-12 01:16:59,211 iteration 6380 : loss : 0.022731, loss_ce: 0.007198
2022-01-12 01:17:00,890 iteration 6381 : loss : 0.016276, loss_ce: 0.006621
2022-01-12 01:17:02,502 iteration 6382 : loss : 0.015224, loss_ce: 0.006986
2022-01-12 01:17:04,153 iteration 6383 : loss : 0.015467, loss_ce: 0.005245
2022-01-12 01:17:05,827 iteration 6384 : loss : 0.020022, loss_ce: 0.008493
2022-01-12 01:17:07,276 iteration 6385 : loss : 0.009077, loss_ce: 0.003348
2022-01-12 01:17:08,926 iteration 6386 : loss : 0.012417, loss_ce: 0.003993
2022-01-12 01:17:10,582 iteration 6387 : loss : 0.017135, loss_ce: 0.008016
2022-01-12 01:17:12,096 iteration 6388 : loss : 0.012057, loss_ce: 0.005294
2022-01-12 01:17:13,743 iteration 6389 : loss : 0.014389, loss_ce: 0.007444
2022-01-12 01:17:15,380 iteration 6390 : loss : 0.017987, loss_ce: 0.005139
2022-01-12 01:17:17,029 iteration 6391 : loss : 0.014389, loss_ce: 0.006089
2022-01-12 01:17:18,699 iteration 6392 : loss : 0.017346, loss_ce: 0.003631
 94%|███████████████████████████▎ | 376/400 [3:05:27<12:03, 30.13s/it]2022-01-12 01:17:20,309 iteration 6393 : loss : 0.010424, loss_ce: 0.005515
2022-01-12 01:17:21,897 iteration 6394 : loss : 0.009401, loss_ce: 0.003962
2022-01-12 01:17:23,439 iteration 6395 : loss : 0.008749, loss_ce: 0.002595
2022-01-12 01:17:25,068 iteration 6396 : loss : 0.015438, loss_ce: 0.005460
2022-01-12 01:17:26,730 iteration 6397 : loss : 0.014878, loss_ce: 0.004517
2022-01-12 01:17:28,395 iteration 6398 : loss : 0.013778, loss_ce: 0.005318
2022-01-12 01:17:29,915 iteration 6399 : loss : 0.014476, loss_ce: 0.004157
2022-01-12 01:17:31,489 iteration 6400 : loss : 0.009581, loss_ce: 0.003449
2022-01-12 01:17:33,002 iteration 6401 : loss : 0.008499, loss_ce: 0.003286
2022-01-12 01:17:34,633 iteration 6402 : loss : 0.014511, loss_ce: 0.006183
2022-01-12 01:17:36,185 iteration 6403 : loss : 0.012650, loss_ce: 0.005053
2022-01-12 01:17:37,718 iteration 6404 : loss : 0.009968, loss_ce: 0.004697
2022-01-12 01:17:39,477 iteration 6405 : loss : 0.020866, loss_ce: 0.007943
2022-01-12 01:17:41,008 iteration 6406 : loss : 0.009009, loss_ce: 0.003503
2022-01-12 01:17:42,633 iteration 6407 : loss : 0.017013, loss_ce: 0.006451
2022-01-12 01:17:44,275 iteration 6408 : loss : 0.017718, loss_ce: 0.005543
2022-01-12 01:17:45,813 iteration 6409 : loss : 0.008474, loss_ce: 0.002856
 94%|███████████████████████████▎ | 377/400 [3:05:54<11:12, 29.23s/it]2022-01-12 01:17:47,374 iteration 6410 : loss : 0.011830, loss_ce: 0.004374
2022-01-12 01:17:48,930 iteration 6411 : loss : 0.013346, loss_ce: 0.005234
2022-01-12 01:17:50,590 iteration 6412 : loss : 0.015953, loss_ce: 0.005196
2022-01-12 01:17:52,189 iteration 6413 : loss : 0.021260, loss_ce: 0.009531
2022-01-12 01:17:53,834 iteration 6414 : loss : 0.015234, loss_ce: 0.005819
2022-01-12 01:17:55,472 iteration 6415 : loss : 0.011456, loss_ce: 0.003597
2022-01-12 01:17:57,007 iteration 6416 : loss : 0.010553, loss_ce: 0.003799
2022-01-12 01:17:58,652 iteration 6417 : loss : 0.018630, loss_ce: 0.006535
2022-01-12 01:18:00,237 iteration 6418 : loss : 0.015456, loss_ce: 0.007220
2022-01-12 01:18:01,845 iteration 6419 : loss : 0.014487, loss_ce: 0.005690
2022-01-12 01:18:03,474 iteration 6420 : loss : 0.019537, loss_ce: 0.009534
2022-01-12 01:18:05,136 iteration 6421 : loss : 0.014012, loss_ce: 0.006613
2022-01-12 01:18:06,718 iteration 6422 : loss : 0.011927, loss_ce: 0.004632
2022-01-12 01:18:08,315 iteration 6423 : loss : 0.013858, loss_ce: 0.004532
2022-01-12 01:18:09,856 iteration 6424 : loss : 0.013623, loss_ce: 0.004222
2022-01-12 01:18:11,598 iteration 6425 : loss : 0.020546, loss_ce: 0.007699
2022-01-12 01:18:13,133 iteration 6426 : loss : 0.013264, loss_ce: 0.004386
 94%|███████████████████████████▍ | 378/400 [3:06:21<10:30, 28.65s/it]2022-01-12 01:18:14,803 iteration 6427 : loss : 0.012768, loss_ce: 0.004859
2022-01-12 01:18:16,455 iteration 6428 : loss : 0.029229, loss_ce: 0.007929
2022-01-12 01:18:18,037 iteration 6429 : loss : 0.011257, loss_ce: 0.004780
2022-01-12 01:18:19,703 iteration 6430 : loss : 0.013766, loss_ce: 0.006230
2022-01-12 01:18:21,282 iteration 6431 : loss : 0.015214, loss_ce: 0.004026
2022-01-12 01:18:22,872 iteration 6432 : loss : 0.017543, loss_ce: 0.006905
2022-01-12 01:18:24,462 iteration 6433 : loss : 0.014722, loss_ce: 0.004106
2022-01-12 01:18:26,058 iteration 6434 : loss : 0.016889, loss_ce: 0.006314
2022-01-12 01:18:27,551 iteration 6435 : loss : 0.009017, loss_ce: 0.002969
2022-01-12 01:18:29,090 iteration 6436 : loss : 0.010118, loss_ce: 0.005162
2022-01-12 01:18:30,614 iteration 6437 : loss : 0.010087, loss_ce: 0.003644
2022-01-12 01:18:32,114 iteration 6438 : loss : 0.008889, loss_ce: 0.003868
2022-01-12 01:18:33,770 iteration 6439 : loss : 0.018877, loss_ce: 0.006374
2022-01-12 01:18:35,378 iteration 6440 : loss : 0.013896, loss_ce: 0.004710
2022-01-12 01:18:36,984 iteration 6441 : loss : 0.009542, loss_ce: 0.003965
2022-01-12 01:18:38,574 iteration 6442 : loss : 0.008432, loss_ce: 0.003548
2022-01-12 01:18:40,133 iteration 6443 : loss : 0.011797, loss_ce: 0.004049
 95%|███████████████████████████▍ | 379/400 [3:06:48<09:51, 28.16s/it]2022-01-12 01:18:41,739 iteration 6444 : loss : 0.014422, loss_ce: 0.006887
2022-01-12 01:18:43,282 iteration 6445 : loss : 0.008412, loss_ce: 0.003096
2022-01-12 01:18:44,875 iteration 6446 : loss : 0.013392, loss_ce: 0.004895
2022-01-12 01:18:46,455 iteration 6447 : loss : 0.010902, loss_ce: 0.004460
2022-01-12 01:18:48,037 iteration 6448 : loss : 0.040568, loss_ce: 0.007080
2022-01-12 01:18:49,637 iteration 6449 : loss : 0.016961, loss_ce: 0.005619
2022-01-12 01:18:51,286 iteration 6450 : loss : 0.018827, loss_ce: 0.005401
2022-01-12 01:18:52,869 iteration 6451 : loss : 0.013084, loss_ce: 0.004742
2022-01-12 01:18:54,524 iteration 6452 : loss : 0.022721, loss_ce: 0.005656
2022-01-12 01:18:56,099 iteration 6453 : loss : 0.014399, loss_ce: 0.007247
2022-01-12 01:18:57,701 iteration 6454 : loss : 0.012827, loss_ce: 0.004625
2022-01-12 01:18:59,236 iteration 6455 : loss : 0.012060, loss_ce: 0.004751
2022-01-12 01:19:00,840 iteration 6456 : loss : 0.011761, loss_ce: 0.005573
2022-01-12 01:19:02,395 iteration 6457 : loss : 0.011358, loss_ce: 0.004500
2022-01-12 01:19:03,942 iteration 6458 : loss : 0.011667, loss_ce: 0.006094
2022-01-12 01:19:05,606 iteration 6459 : loss : 0.016390, loss_ce: 0.006049
2022-01-12 01:19:05,606 Training Data Eval:
2022-01-12 01:19:13,586   Average segmentation loss on training set: 0.0068
2022-01-12 01:19:13,586 Validation Data Eval:
2022-01-12 01:19:16,341   Average segmentation loss on validation set: 0.0691
2022-01-12 01:19:18,024 iteration 6460 : loss : 0.011370, loss_ce: 0.004170
 95%|███████████████████████████▌ | 380/400 [3:07:26<10:21, 31.08s/it]2022-01-12 01:19:19,630 iteration 6461 : loss : 0.010491, loss_ce: 0.003365
2022-01-12 01:19:21,224 iteration 6462 : loss : 0.012141, loss_ce: 0.005084
2022-01-12 01:19:22,763 iteration 6463 : loss : 0.010066, loss_ce: 0.004516
2022-01-12 01:19:24,328 iteration 6464 : loss : 0.011992, loss_ce: 0.004261
2022-01-12 01:19:25,911 iteration 6465 : loss : 0.008742, loss_ce: 0.003096
2022-01-12 01:19:27,491 iteration 6466 : loss : 0.028319, loss_ce: 0.012353
2022-01-12 01:19:29,084 iteration 6467 : loss : 0.016857, loss_ce: 0.006869
2022-01-12 01:19:30,616 iteration 6468 : loss : 0.012429, loss_ce: 0.004727
2022-01-12 01:19:32,263 iteration 6469 : loss : 0.021476, loss_ce: 0.005614
2022-01-12 01:19:33,873 iteration 6470 : loss : 0.012519, loss_ce: 0.005684
2022-01-12 01:19:35,490 iteration 6471 : loss : 0.017807, loss_ce: 0.008004
2022-01-12 01:19:37,015 iteration 6472 : loss : 0.011611, loss_ce: 0.004026
2022-01-12 01:19:38,647 iteration 6473 : loss : 0.013864, loss_ce: 0.005522
2022-01-12 01:19:40,262 iteration 6474 : loss : 0.011388, loss_ce: 0.004708
2022-01-12 01:19:41,923 iteration 6475 : loss : 0.013202, loss_ce: 0.004319
2022-01-12 01:19:43,561 iteration 6476 : loss : 0.018237, loss_ce: 0.007417
2022-01-12 01:19:45,169 iteration 6477 : loss : 0.015774, loss_ce: 0.006703
 95%|███████████████████████████▌ | 381/400 [3:07:53<09:28, 29.90s/it]2022-01-12 01:19:46,817 iteration 6478 : loss : 0.012815, loss_ce: 0.005504
2022-01-12 01:19:48,414 iteration 6479 : loss : 0.013586, loss_ce: 0.003969
2022-01-12 01:19:49,983 iteration 6480 : loss : 0.009811, loss_ce: 0.003463
2022-01-12 01:19:51,738 iteration 6481 : loss : 0.024317, loss_ce: 0.008309
2022-01-12 01:19:53,309 iteration 6482 : loss : 0.013883, loss_ce: 0.005577
2022-01-12 01:19:54,970 iteration 6483 : loss : 0.015673, loss_ce: 0.005935
2022-01-12 01:19:56,469 iteration 6484 : loss : 0.018577, loss_ce: 0.006224
2022-01-12 01:19:58,082 iteration 6485 : loss : 0.013900, loss_ce: 0.004572
2022-01-12 01:19:59,697 iteration 6486 : loss : 0.016729, loss_ce: 0.005710
2022-01-12 01:20:01,344 iteration 6487 : loss : 0.013696, loss_ce: 0.005054
2022-01-12 01:20:02,987 iteration 6488 : loss : 0.011343, loss_ce: 0.003810
2022-01-12 01:20:04,593 iteration 6489 : loss : 0.016064, loss_ce: 0.004551
2022-01-12 01:20:06,180 iteration 6490 : loss : 0.011863, loss_ce: 0.003934
2022-01-12 01:20:07,721 iteration 6491 : loss : 0.012019, loss_ce: 0.005402
2022-01-12 01:20:09,260 iteration 6492 : loss : 0.010251, loss_ce: 0.005895
2022-01-12 01:20:10,840 iteration 6493 : loss : 0.017125, loss_ce: 0.007051
2022-01-12 01:20:12,468 iteration 6494 : loss : 0.016339, loss_ce: 0.006472
 96%|███████████████████████████▋ | 382/400 [3:08:21<08:44, 29.12s/it]2022-01-12 01:20:14,030 iteration 6495 : loss : 0.012354, loss_ce: 0.005868
2022-01-12 01:20:15,631 iteration 6496 : loss : 0.017926, loss_ce: 0.006726
2022-01-12 01:20:17,265 iteration 6497 : loss : 0.009898, loss_ce: 0.005282
2022-01-12 01:20:18,818 iteration 6498 : loss : 0.010800, loss_ce: 0.002880
2022-01-12 01:20:20,393 iteration 6499 : loss : 0.015505, loss_ce: 0.003723
2022-01-12 01:20:22,100 iteration 6500 : loss : 0.027085, loss_ce: 0.007161
2022-01-12 01:20:23,791 iteration 6501 : loss : 0.022490, loss_ce: 0.011431
2022-01-12 01:20:25,417 iteration 6502 : loss : 0.011150, loss_ce: 0.003981
2022-01-12 01:20:27,023 iteration 6503 : loss : 0.011110, loss_ce: 0.004007
2022-01-12 01:20:28,664 iteration 6504 : loss : 0.016903, loss_ce: 0.007086
2022-01-12 01:20:30,181 iteration 6505 : loss : 0.008602, loss_ce: 0.003269
2022-01-12 01:20:31,800 iteration 6506 : loss : 0.014299, loss_ce: 0.003853
2022-01-12 01:20:33,367 iteration 6507 : loss : 0.012506, loss_ce: 0.005183
2022-01-12 01:20:34,868 iteration 6508 : loss : 0.011781, loss_ce: 0.004118
2022-01-12 01:20:36,451 iteration 6509 : loss : 0.012781, loss_ce: 0.004371
2022-01-12 01:20:37,918 iteration 6510 : loss : 0.008042, loss_ce: 0.003094
2022-01-12 01:20:39,531 iteration 6511 : loss : 0.013109, loss_ce: 0.004850
 96%|███████████████████████████▊ | 383/400 [3:08:48<08:04, 28.50s/it]2022-01-12 01:20:41,194 iteration 6512 : loss : 0.019380, loss_ce: 0.007107
2022-01-12 01:20:42,856 iteration 6513 : loss : 0.013159, loss_ce: 0.004905
2022-01-12 01:20:44,461 iteration 6514 : loss : 0.026750, loss_ce: 0.007884
2022-01-12 01:20:46,122 iteration 6515 : loss : 0.017631, loss_ce: 0.006426
2022-01-12 01:20:47,686 iteration 6516 : loss : 0.010001, loss_ce: 0.003948
2022-01-12 01:20:49,224 iteration 6517 : loss : 0.010740, loss_ce: 0.004078
2022-01-12 01:20:50,840 iteration 6518 : loss : 0.015975, loss_ce: 0.006349
2022-01-12 01:20:52,505 iteration 6519 : loss : 0.021101, loss_ce: 0.009466
2022-01-12 01:20:54,036 iteration 6520 : loss : 0.012222, loss_ce: 0.004968
2022-01-12 01:20:55,681 iteration 6521 : loss : 0.017888, loss_ce: 0.007286
2022-01-12 01:20:57,285 iteration 6522 : loss : 0.014248, loss_ce: 0.004484
2022-01-12 01:20:58,918 iteration 6523 : loss : 0.013808, loss_ce: 0.005895
2022-01-12 01:21:00,538 iteration 6524 : loss : 0.015090, loss_ce: 0.005610
2022-01-12 01:21:02,189 iteration 6525 : loss : 0.018703, loss_ce: 0.006916
2022-01-12 01:21:03,797 iteration 6526 : loss : 0.012807, loss_ce: 0.005368
2022-01-12 01:21:05,345 iteration 6527 : loss : 0.016146, loss_ce: 0.005464
2022-01-12 01:21:06,892 iteration 6528 : loss : 0.009433, loss_ce: 0.003695
 96%|███████████████████████████▊ | 384/400 [3:09:15<07:30, 28.16s/it]2022-01-12 01:21:08,719 iteration 6529 : loss : 0.019990, loss_ce: 0.008938
2022-01-12 01:21:10,306 iteration 6530 : loss : 0.018229, loss_ce: 0.008644
2022-01-12 01:21:12,034 iteration 6531 : loss : 0.016727, loss_ce: 0.005699
2022-01-12 01:21:13,655 iteration 6532 : loss : 0.015225, loss_ce: 0.006000
2022-01-12 01:21:15,317 iteration 6533 : loss : 0.026757, loss_ce: 0.005042
2022-01-12 01:21:17,091 iteration 6534 : loss : 0.017455, loss_ce: 0.004510
2022-01-12 01:21:18,618 iteration 6535 : loss : 0.013371, loss_ce: 0.002654
2022-01-12 01:21:20,228 iteration 6536 : loss : 0.017343, loss_ce: 0.007514
2022-01-12 01:21:21,840 iteration 6537 : loss : 0.013865, loss_ce: 0.005723
2022-01-12 01:21:23,471 iteration 6538 : loss : 0.022038, loss_ce: 0.011668
2022-01-12 01:21:25,178 iteration 6539 : loss : 0.016473, loss_ce: 0.005675
2022-01-12 01:21:26,777 iteration 6540 : loss : 0.012391, loss_ce: 0.005576
2022-01-12 01:21:28,345 iteration 6541 : loss : 0.011034, loss_ce: 0.004705
2022-01-12 01:21:30,010 iteration 6542 : loss : 0.015066, loss_ce: 0.005950
2022-01-12 01:21:31,638 iteration 6543 : loss : 0.014730, loss_ce: 0.005179
2022-01-12 01:21:33,161 iteration 6544 : loss : 0.008276, loss_ce: 0.002764
2022-01-12 01:21:33,161 Training Data Eval:
2022-01-12 01:21:41,123   Average segmentation loss on training set: 0.0066
2022-01-12 01:21:41,124 Validation Data Eval:
2022-01-12 01:21:43,858   Average segmentation loss on validation set: 0.0712
2022-01-12 01:21:45,326 iteration 6545 : loss : 0.007512, loss_ce: 0.001777
 96%|███████████████████████████▉ | 385/400 [3:09:54<07:48, 31.24s/it]2022-01-12 01:21:46,981 iteration 6546 : loss : 0.015589, loss_ce: 0.005369
2022-01-12 01:21:48,776 iteration 6547 : loss : 0.018061, loss_ce: 0.006593
2022-01-12 01:21:50,279 iteration 6548 : loss : 0.011802, loss_ce: 0.006953
2022-01-12 01:21:51,765 iteration 6549 : loss : 0.006759, loss_ce: 0.002293
2022-01-12 01:21:53,314 iteration 6550 : loss : 0.010147, loss_ce: 0.003693
2022-01-12 01:21:54,962 iteration 6551 : loss : 0.013449, loss_ce: 0.006425
2022-01-12 01:21:56,452 iteration 6552 : loss : 0.011084, loss_ce: 0.005134
2022-01-12 01:21:58,119 iteration 6553 : loss : 0.019682, loss_ce: 0.007995
2022-01-12 01:21:59,710 iteration 6554 : loss : 0.012814, loss_ce: 0.004068
2022-01-12 01:22:01,387 iteration 6555 : loss : 0.015038, loss_ce: 0.005295
2022-01-12 01:22:03,045 iteration 6556 : loss : 0.013180, loss_ce: 0.003818
2022-01-12 01:22:04,588 iteration 6557 : loss : 0.012399, loss_ce: 0.004195
2022-01-12 01:22:06,130 iteration 6558 : loss : 0.012669, loss_ce: 0.003619
2022-01-12 01:22:07,759 iteration 6559 : loss : 0.017420, loss_ce: 0.008023
2022-01-12 01:22:09,465 iteration 6560 : loss : 0.036808, loss_ce: 0.007550
2022-01-12 01:22:11,010 iteration 6561 : loss : 0.011767, loss_ce: 0.004690
2022-01-12 01:22:12,497 iteration 6562 : loss : 0.008683, loss_ce: 0.002845
 96%|███████████████████████████▉ | 386/400 [3:10:21<07:00, 30.02s/it]2022-01-12 01:22:14,127 iteration 6563 : loss : 0.009827, loss_ce: 0.004221
2022-01-12 01:22:15,668 iteration 6564 : loss : 0.020863, loss_ce: 0.005433
2022-01-12 01:22:17,184 iteration 6565 : loss : 0.007825, loss_ce: 0.002306
2022-01-12 01:22:18,732 iteration 6566 : loss : 0.009614, loss_ce: 0.003752
2022-01-12 01:22:20,358 iteration 6567 : loss : 0.015186, loss_ce: 0.006446
2022-01-12 01:22:21,977 iteration 6568 : loss : 0.015694, loss_ce: 0.006425
2022-01-12 01:22:23,649 iteration 6569 : loss : 0.021949, loss_ce: 0.008392
2022-01-12 01:22:25,252 iteration 6570 : loss : 0.013395, loss_ce: 0.003512
2022-01-12 01:22:26,989 iteration 6571 : loss : 0.022104, loss_ce: 0.010948
2022-01-12 01:22:28,523 iteration 6572 : loss : 0.008622, loss_ce: 0.003466
2022-01-12 01:22:30,195 iteration 6573 : loss : 0.022035, loss_ce: 0.007931
2022-01-12 01:22:31,776 iteration 6574 : loss : 0.008761, loss_ce: 0.002543
2022-01-12 01:22:33,452 iteration 6575 : loss : 0.026369, loss_ce: 0.009114
2022-01-12 01:22:35,055 iteration 6576 : loss : 0.017162, loss_ce: 0.007134
2022-01-12 01:22:36,621 iteration 6577 : loss : 0.012000, loss_ce: 0.003913
2022-01-12 01:22:38,262 iteration 6578 : loss : 0.010107, loss_ce: 0.003556
2022-01-12 01:22:39,827 iteration 6579 : loss : 0.012788, loss_ce: 0.005714
 97%|████████████████████████████ | 387/400 [3:10:48<06:19, 29.21s/it]2022-01-12 01:22:41,526 iteration 6580 : loss : 0.016011, loss_ce: 0.006946
2022-01-12 01:22:43,070 iteration 6581 : loss : 0.009912, loss_ce: 0.002941
2022-01-12 01:22:44,773 iteration 6582 : loss : 0.017121, loss_ce: 0.005264
2022-01-12 01:22:46,396 iteration 6583 : loss : 0.022680, loss_ce: 0.007636
2022-01-12 01:22:47,934 iteration 6584 : loss : 0.012980, loss_ce: 0.005091
2022-01-12 01:22:49,438 iteration 6585 : loss : 0.009538, loss_ce: 0.002999
2022-01-12 01:22:50,996 iteration 6586 : loss : 0.010772, loss_ce: 0.002273
2022-01-12 01:22:52,519 iteration 6587 : loss : 0.010809, loss_ce: 0.005103
2022-01-12 01:22:54,135 iteration 6588 : loss : 0.009186, loss_ce: 0.003941
2022-01-12 01:22:55,750 iteration 6589 : loss : 0.012783, loss_ce: 0.004966
2022-01-12 01:22:57,315 iteration 6590 : loss : 0.010623, loss_ce: 0.004397
2022-01-12 01:22:58,889 iteration 6591 : loss : 0.009101, loss_ce: 0.003215
2022-01-12 01:23:00,578 iteration 6592 : loss : 0.013802, loss_ce: 0.007487
2022-01-12 01:23:02,066 iteration 6593 : loss : 0.009802, loss_ce: 0.005215
2022-01-12 01:23:03,628 iteration 6594 : loss : 0.009073, loss_ce: 0.003637
2022-01-12 01:23:05,257 iteration 6595 : loss : 0.013578, loss_ce: 0.003674
2022-01-12 01:23:06,789 iteration 6596 : loss : 0.009845, loss_ce: 0.002755
 97%|████████████████████████████▏| 388/400 [3:11:15<05:42, 28.54s/it]2022-01-12 01:23:08,371 iteration 6597 : loss : 0.010065, loss_ce: 0.004808
2022-01-12 01:23:10,018 iteration 6598 : loss : 0.020906, loss_ce: 0.003206
2022-01-12 01:23:11,687 iteration 6599 : loss : 0.014493, loss_ce: 0.005204
2022-01-12 01:23:13,214 iteration 6600 : loss : 0.010497, loss_ce: 0.004708
2022-01-12 01:23:14,915 iteration 6601 : loss : 0.017911, loss_ce: 0.007651
2022-01-12 01:23:16,565 iteration 6602 : loss : 0.014139, loss_ce: 0.005879
2022-01-12 01:23:18,120 iteration 6603 : loss : 0.011417, loss_ce: 0.005458
2022-01-12 01:23:19,756 iteration 6604 : loss : 0.018590, loss_ce: 0.011009
2022-01-12 01:23:21,324 iteration 6605 : loss : 0.014110, loss_ce: 0.005937
2022-01-12 01:23:22,999 iteration 6606 : loss : 0.018018, loss_ce: 0.005971
2022-01-12 01:23:24,671 iteration 6607 : loss : 0.017259, loss_ce: 0.006663
2022-01-12 01:23:26,208 iteration 6608 : loss : 0.011102, loss_ce: 0.004474
2022-01-12 01:23:27,864 iteration 6609 : loss : 0.037742, loss_ce: 0.014744
2022-01-12 01:23:29,440 iteration 6610 : loss : 0.011216, loss_ce: 0.004467
2022-01-12 01:23:31,006 iteration 6611 : loss : 0.009683, loss_ce: 0.002525
2022-01-12 01:23:32,596 iteration 6612 : loss : 0.012278, loss_ce: 0.003486
2022-01-12 01:23:34,243 iteration 6613 : loss : 0.023022, loss_ce: 0.008383
 97%|████████████████████████████▏| 389/400 [3:11:42<05:10, 28.21s/it]2022-01-12 01:23:35,840 iteration 6614 : loss : 0.007987, loss_ce: 0.002908
2022-01-12 01:23:37,348 iteration 6615 : loss : 0.007703, loss_ce: 0.003007
2022-01-12 01:23:38,982 iteration 6616 : loss : 0.013668, loss_ce: 0.004081
2022-01-12 01:23:40,556 iteration 6617 : loss : 0.023248, loss_ce: 0.008387
2022-01-12 01:23:42,243 iteration 6618 : loss : 0.014116, loss_ce: 0.005281
2022-01-12 01:23:43,856 iteration 6619 : loss : 0.013852, loss_ce: 0.004922
2022-01-12 01:23:45,478 iteration 6620 : loss : 0.016261, loss_ce: 0.005358
2022-01-12 01:23:47,143 iteration 6621 : loss : 0.016344, loss_ce: 0.006423
2022-01-12 01:23:48,730 iteration 6622 : loss : 0.012510, loss_ce: 0.005590
2022-01-12 01:23:50,258 iteration 6623 : loss : 0.013055, loss_ce: 0.004184
2022-01-12 01:23:51,851 iteration 6624 : loss : 0.014921, loss_ce: 0.005894
2022-01-12 01:23:53,475 iteration 6625 : loss : 0.029289, loss_ce: 0.011326
2022-01-12 01:23:55,061 iteration 6626 : loss : 0.015020, loss_ce: 0.006037
2022-01-12 01:23:56,696 iteration 6627 : loss : 0.022456, loss_ce: 0.007685
2022-01-12 01:23:58,338 iteration 6628 : loss : 0.016792, loss_ce: 0.007340
2022-01-12 01:24:00,061 iteration 6629 : loss : 0.020827, loss_ce: 0.006275
2022-01-12 01:24:00,061 Training Data Eval:
2022-01-12 01:24:08,016   Average segmentation loss on training set: 0.0066
2022-01-12 01:24:08,016 Validation Data Eval:
2022-01-12 01:24:10,751   Average segmentation loss on validation set: 0.0766
2022-01-12 01:24:12,310 iteration 6630 : loss : 0.019888, loss_ce: 0.007525
 98%|████████████████████████████▎| 390/400 [3:12:20<05:11, 31.17s/it]2022-01-12 01:24:13,955 iteration 6631 : loss : 0.015935, loss_ce: 0.005312
2022-01-12 01:24:15,516 iteration 6632 : loss : 0.010640, loss_ce: 0.005366
2022-01-12 01:24:17,141 iteration 6633 : loss : 0.013637, loss_ce: 0.003676
2022-01-12 01:24:18,705 iteration 6634 : loss : 0.011313, loss_ce: 0.004301
2022-01-12 01:24:20,301 iteration 6635 : loss : 0.012739, loss_ce: 0.005002
2022-01-12 01:24:21,897 iteration 6636 : loss : 0.009144, loss_ce: 0.004276
2022-01-12 01:24:23,447 iteration 6637 : loss : 0.013635, loss_ce: 0.004562
2022-01-12 01:24:25,111 iteration 6638 : loss : 0.024886, loss_ce: 0.006376
2022-01-12 01:24:26,771 iteration 6639 : loss : 0.014374, loss_ce: 0.006118
2022-01-12 01:24:28,345 iteration 6640 : loss : 0.011416, loss_ce: 0.003795
2022-01-12 01:24:29,956 iteration 6641 : loss : 0.010688, loss_ce: 0.003251
2022-01-12 01:24:31,574 iteration 6642 : loss : 0.016144, loss_ce: 0.008195
2022-01-12 01:24:33,190 iteration 6643 : loss : 0.015353, loss_ce: 0.005449
2022-01-12 01:24:34,928 iteration 6644 : loss : 0.024927, loss_ce: 0.010192
2022-01-12 01:24:36,489 iteration 6645 : loss : 0.011586, loss_ce: 0.004793
2022-01-12 01:24:38,205 iteration 6646 : loss : 0.016666, loss_ce: 0.004707
2022-01-12 01:24:39,805 iteration 6647 : loss : 0.015256, loss_ce: 0.005773
 98%|████████████████████████████▎| 391/400 [3:12:48<04:30, 30.07s/it]2022-01-12 01:24:41,419 iteration 6648 : loss : 0.018562, loss_ce: 0.004435
2022-01-12 01:24:43,055 iteration 6649 : loss : 0.016991, loss_ce: 0.005136
2022-01-12 01:24:44,741 iteration 6650 : loss : 0.019329, loss_ce: 0.006639
2022-01-12 01:24:46,465 iteration 6651 : loss : 0.020673, loss_ce: 0.009171
2022-01-12 01:24:48,072 iteration 6652 : loss : 0.012132, loss_ce: 0.005545
2022-01-12 01:24:49,600 iteration 6653 : loss : 0.008691, loss_ce: 0.003707
2022-01-12 01:24:51,180 iteration 6654 : loss : 0.012173, loss_ce: 0.003674
2022-01-12 01:24:52,818 iteration 6655 : loss : 0.014469, loss_ce: 0.005517
2022-01-12 01:24:54,452 iteration 6656 : loss : 0.017013, loss_ce: 0.007224
2022-01-12 01:24:56,072 iteration 6657 : loss : 0.011642, loss_ce: 0.003114
2022-01-12 01:24:57,715 iteration 6658 : loss : 0.015478, loss_ce: 0.005004
2022-01-12 01:24:59,335 iteration 6659 : loss : 0.014520, loss_ce: 0.006831
2022-01-12 01:25:01,012 iteration 6660 : loss : 0.015631, loss_ce: 0.007159
2022-01-12 01:25:02,644 iteration 6661 : loss : 0.012534, loss_ce: 0.004958
2022-01-12 01:25:04,289 iteration 6662 : loss : 0.016449, loss_ce: 0.004226
2022-01-12 01:25:05,917 iteration 6663 : loss : 0.014154, loss_ce: 0.005030
2022-01-12 01:25:07,567 iteration 6664 : loss : 0.015951, loss_ce: 0.008618
 98%|████████████████████████████▍| 392/400 [3:13:16<03:55, 29.38s/it]2022-01-12 01:25:09,247 iteration 6665 : loss : 0.029439, loss_ce: 0.013020
2022-01-12 01:25:10,982 iteration 6666 : loss : 0.019278, loss_ce: 0.005671
2022-01-12 01:25:12,628 iteration 6667 : loss : 0.018782, loss_ce: 0.003400
2022-01-12 01:25:14,151 iteration 6668 : loss : 0.012414, loss_ce: 0.006770
2022-01-12 01:25:15,735 iteration 6669 : loss : 0.017580, loss_ce: 0.006647
2022-01-12 01:25:17,318 iteration 6670 : loss : 0.015683, loss_ce: 0.007940
2022-01-12 01:25:18,946 iteration 6671 : loss : 0.012054, loss_ce: 0.004260
2022-01-12 01:25:20,604 iteration 6672 : loss : 0.014870, loss_ce: 0.004702
2022-01-12 01:25:22,202 iteration 6673 : loss : 0.015184, loss_ce: 0.003513
2022-01-12 01:25:23,801 iteration 6674 : loss : 0.016138, loss_ce: 0.007900
2022-01-12 01:25:25,484 iteration 6675 : loss : 0.015500, loss_ce: 0.007439
2022-01-12 01:25:27,063 iteration 6676 : loss : 0.011430, loss_ce: 0.004656
2022-01-12 01:25:28,606 iteration 6677 : loss : 0.012461, loss_ce: 0.004179
2022-01-12 01:25:30,195 iteration 6678 : loss : 0.012695, loss_ce: 0.005884
2022-01-12 01:25:31,854 iteration 6679 : loss : 0.013538, loss_ce: 0.005727
2022-01-12 01:25:33,398 iteration 6680 : loss : 0.009591, loss_ce: 0.003237
2022-01-12 01:25:34,909 iteration 6681 : loss : 0.012413, loss_ce: 0.004708
 98%|████████████████████████████▍| 393/400 [3:13:43<03:21, 28.77s/it]2022-01-12 01:25:36,644 iteration 6682 : loss : 0.014065, loss_ce: 0.006504
2022-01-12 01:25:38,196 iteration 6683 : loss : 0.010030, loss_ce: 0.003814
2022-01-12 01:25:39,796 iteration 6684 : loss : 0.010698, loss_ce: 0.004519
2022-01-12 01:25:41,336 iteration 6685 : loss : 0.013203, loss_ce: 0.004536
2022-01-12 01:25:43,011 iteration 6686 : loss : 0.022731, loss_ce: 0.009184
2022-01-12 01:25:44,603 iteration 6687 : loss : 0.013506, loss_ce: 0.004610
2022-01-12 01:25:46,210 iteration 6688 : loss : 0.018546, loss_ce: 0.007014
2022-01-12 01:25:47,722 iteration 6689 : loss : 0.009680, loss_ce: 0.002874
2022-01-12 01:25:49,311 iteration 6690 : loss : 0.006829, loss_ce: 0.002056
2022-01-12 01:25:50,964 iteration 6691 : loss : 0.017847, loss_ce: 0.005819
2022-01-12 01:25:52,525 iteration 6692 : loss : 0.017879, loss_ce: 0.007359
2022-01-12 01:25:54,102 iteration 6693 : loss : 0.019399, loss_ce: 0.006368
2022-01-12 01:25:55,725 iteration 6694 : loss : 0.013857, loss_ce: 0.005244
2022-01-12 01:25:57,289 iteration 6695 : loss : 0.012604, loss_ce: 0.003964
2022-01-12 01:25:58,882 iteration 6696 : loss : 0.009965, loss_ce: 0.003810
2022-01-12 01:26:00,467 iteration 6697 : loss : 0.012281, loss_ce: 0.004003
2022-01-12 01:26:02,151 iteration 6698 : loss : 0.014623, loss_ce: 0.005310
 98%|████████████████████████████▌| 394/400 [3:14:10<02:49, 28.31s/it]2022-01-12 01:26:03,775 iteration 6699 : loss : 0.016012, loss_ce: 0.005647
2022-01-12 01:26:05,446 iteration 6700 : loss : 0.019427, loss_ce: 0.005544
2022-01-12 01:26:07,037 iteration 6701 : loss : 0.011927, loss_ce: 0.004398
2022-01-12 01:26:08,554 iteration 6702 : loss : 0.008646, loss_ce: 0.004104
2022-01-12 01:26:10,073 iteration 6703 : loss : 0.011504, loss_ce: 0.004106
2022-01-12 01:26:11,702 iteration 6704 : loss : 0.019680, loss_ce: 0.011460
2022-01-12 01:26:13,237 iteration 6705 : loss : 0.013629, loss_ce: 0.006575
2022-01-12 01:26:14,896 iteration 6706 : loss : 0.014160, loss_ce: 0.004580
2022-01-12 01:26:16,428 iteration 6707 : loss : 0.008380, loss_ce: 0.003115
2022-01-12 01:26:18,028 iteration 6708 : loss : 0.012348, loss_ce: 0.004281
2022-01-12 01:26:19,637 iteration 6709 : loss : 0.017094, loss_ce: 0.007012
2022-01-12 01:26:21,297 iteration 6710 : loss : 0.020095, loss_ce: 0.005057
2022-01-12 01:26:22,929 iteration 6711 : loss : 0.013221, loss_ce: 0.004327
2022-01-12 01:26:24,625 iteration 6712 : loss : 0.016273, loss_ce: 0.006774
2022-01-12 01:26:26,149 iteration 6713 : loss : 0.009589, loss_ce: 0.003757
2022-01-12 01:26:27,758 iteration 6714 : loss : 0.015032, loss_ce: 0.005428
2022-01-12 01:26:27,758 Training Data Eval:
2022-01-12 01:26:35,720   Average segmentation loss on training set: 0.0067
2022-01-12 01:26:35,720 Validation Data Eval:
2022-01-12 01:26:38,470   Average segmentation loss on validation set: 0.0676
2022-01-12 01:26:40,007 iteration 6715 : loss : 0.012437, loss_ce: 0.004559
 99%|████████████████████████████▋| 395/400 [3:14:48<02:35, 31.17s/it]2022-01-12 01:26:41,630 iteration 6716 : loss : 0.012915, loss_ce: 0.005649
2022-01-12 01:26:43,206 iteration 6717 : loss : 0.010967, loss_ce: 0.004406
2022-01-12 01:26:44,801 iteration 6718 : loss : 0.015672, loss_ce: 0.005679
2022-01-12 01:26:46,350 iteration 6719 : loss : 0.017507, loss_ce: 0.005736
2022-01-12 01:26:47,918 iteration 6720 : loss : 0.010104, loss_ce: 0.002789
2022-01-12 01:26:49,463 iteration 6721 : loss : 0.009263, loss_ce: 0.003037
2022-01-12 01:26:51,139 iteration 6722 : loss : 0.012847, loss_ce: 0.004928
2022-01-12 01:26:52,641 iteration 6723 : loss : 0.010077, loss_ce: 0.003133
2022-01-12 01:26:54,223 iteration 6724 : loss : 0.015554, loss_ce: 0.004661
2022-01-12 01:26:55,841 iteration 6725 : loss : 0.013211, loss_ce: 0.004086
2022-01-12 01:26:57,479 iteration 6726 : loss : 0.018433, loss_ce: 0.008930
2022-01-12 01:26:59,197 iteration 6727 : loss : 0.018287, loss_ce: 0.005873
2022-01-12 01:27:00,788 iteration 6728 : loss : 0.012556, loss_ce: 0.004601
2022-01-12 01:27:02,379 iteration 6729 : loss : 0.013262, loss_ce: 0.003625
2022-01-12 01:27:03,929 iteration 6730 : loss : 0.009045, loss_ce: 0.003494
2022-01-12 01:27:05,525 iteration 6731 : loss : 0.011494, loss_ce: 0.004632
2022-01-12 01:27:07,163 iteration 6732 : loss : 0.012375, loss_ce: 0.004809
 99%|████████████████████████████▋| 396/400 [3:15:15<01:59, 29.97s/it]2022-01-12 01:27:08,795 iteration 6733 : loss : 0.009516, loss_ce: 0.003218
2022-01-12 01:27:10,513 iteration 6734 : loss : 0.020624, loss_ce: 0.007429
2022-01-12 01:27:12,073 iteration 6735 : loss : 0.008298, loss_ce: 0.003176
2022-01-12 01:27:13,722 iteration 6736 : loss : 0.014264, loss_ce: 0.005257
2022-01-12 01:27:15,384 iteration 6737 : loss : 0.014514, loss_ce: 0.005818
2022-01-12 01:27:16,882 iteration 6738 : loss : 0.009415, loss_ce: 0.002693
2022-01-12 01:27:18,441 iteration 6739 : loss : 0.010445, loss_ce: 0.004020
2022-01-12 01:27:20,070 iteration 6740 : loss : 0.011233, loss_ce: 0.005135
2022-01-12 01:27:21,656 iteration 6741 : loss : 0.009221, loss_ce: 0.003599
2022-01-12 01:27:23,215 iteration 6742 : loss : 0.011698, loss_ce: 0.003957
2022-01-12 01:27:24,853 iteration 6743 : loss : 0.013247, loss_ce: 0.006299
2022-01-12 01:27:26,478 iteration 6744 : loss : 0.016309, loss_ce: 0.006650
2022-01-12 01:27:28,022 iteration 6745 : loss : 0.011945, loss_ce: 0.004332
2022-01-12 01:27:29,626 iteration 6746 : loss : 0.011247, loss_ce: 0.004331
2022-01-12 01:27:31,293 iteration 6747 : loss : 0.016148, loss_ce: 0.004203
2022-01-12 01:27:32,937 iteration 6748 : loss : 0.016679, loss_ce: 0.005389
2022-01-12 01:27:34,544 iteration 6749 : loss : 0.011353, loss_ce: 0.005141
 99%|████████████████████████████▊| 397/400 [3:15:43<01:27, 29.19s/it]2022-01-12 01:27:36,237 iteration 6750 : loss : 0.013422, loss_ce: 0.004215
2022-01-12 01:27:37,947 iteration 6751 : loss : 0.021637, loss_ce: 0.006095
2022-01-12 01:27:39,625 iteration 6752 : loss : 0.017520, loss_ce: 0.007299
2022-01-12 01:27:41,212 iteration 6753 : loss : 0.015518, loss_ce: 0.006123
2022-01-12 01:27:42,893 iteration 6754 : loss : 0.014898, loss_ce: 0.006533
2022-01-12 01:27:44,448 iteration 6755 : loss : 0.010701, loss_ce: 0.002966
2022-01-12 01:27:45,930 iteration 6756 : loss : 0.007717, loss_ce: 0.003421
2022-01-12 01:27:47,443 iteration 6757 : loss : 0.010919, loss_ce: 0.004321
2022-01-12 01:27:49,004 iteration 6758 : loss : 0.008627, loss_ce: 0.004089
2022-01-12 01:27:50,635 iteration 6759 : loss : 0.014888, loss_ce: 0.004077
2022-01-12 01:27:52,189 iteration 6760 : loss : 0.010654, loss_ce: 0.004132
2022-01-12 01:27:53,762 iteration 6761 : loss : 0.013583, loss_ce: 0.004587
2022-01-12 01:27:55,368 iteration 6762 : loss : 0.012333, loss_ce: 0.003777
2022-01-12 01:27:56,955 iteration 6763 : loss : 0.012254, loss_ce: 0.005839
2022-01-12 01:27:58,627 iteration 6764 : loss : 0.012807, loss_ce: 0.004744
2022-01-12 01:28:00,244 iteration 6765 : loss : 0.018552, loss_ce: 0.005152
2022-01-12 01:28:01,839 iteration 6766 : loss : 0.010742, loss_ce: 0.005048
100%|████████████████████████████▊| 398/400 [3:16:10<00:57, 28.62s/it]2022-01-12 01:28:03,550 iteration 6767 : loss : 0.021988, loss_ce: 0.009770
2022-01-12 01:28:05,126 iteration 6768 : loss : 0.010109, loss_ce: 0.002990
2022-01-12 01:28:06,695 iteration 6769 : loss : 0.010308, loss_ce: 0.003479
2022-01-12 01:28:08,297 iteration 6770 : loss : 0.016010, loss_ce: 0.004700
2022-01-12 01:28:09,863 iteration 6771 : loss : 0.012374, loss_ce: 0.005412
2022-01-12 01:28:11,468 iteration 6772 : loss : 0.011781, loss_ce: 0.005205
2022-01-12 01:28:12,981 iteration 6773 : loss : 0.009932, loss_ce: 0.003840
2022-01-12 01:28:14,554 iteration 6774 : loss : 0.012049, loss_ce: 0.003899
2022-01-12 01:28:16,104 iteration 6775 : loss : 0.011479, loss_ce: 0.002078
2022-01-12 01:28:17,677 iteration 6776 : loss : 0.011253, loss_ce: 0.004572
2022-01-12 01:28:19,274 iteration 6777 : loss : 0.015776, loss_ce: 0.006089
2022-01-12 01:28:20,799 iteration 6778 : loss : 0.013524, loss_ce: 0.004721
2022-01-12 01:28:22,359 iteration 6779 : loss : 0.009209, loss_ce: 0.004110
2022-01-12 01:28:23,991 iteration 6780 : loss : 0.013032, loss_ce: 0.004560
2022-01-12 01:28:25,597 iteration 6781 : loss : 0.011224, loss_ce: 0.004424
2022-01-12 01:28:27,154 iteration 6782 : loss : 0.013948, loss_ce: 0.005701
2022-01-12 01:28:28,810 iteration 6783 : loss : 0.018431, loss_ce: 0.005171
100%|████████████████████████████▉| 399/400 [3:16:37<00:28, 28.13s/it]2022-01-12 01:28:30,372 iteration 6784 : loss : 0.013929, loss_ce: 0.006633
2022-01-12 01:28:31,950 iteration 6785 : loss : 0.010895, loss_ce: 0.003512
2022-01-12 01:28:33,598 iteration 6786 : loss : 0.012292, loss_ce: 0.004603
2022-01-12 01:28:35,177 iteration 6787 : loss : 0.013161, loss_ce: 0.003990
2022-01-12 01:28:36,776 iteration 6788 : loss : 0.018593, loss_ce: 0.009598
2022-01-12 01:28:38,450 iteration 6789 : loss : 0.018415, loss_ce: 0.005161
2022-01-12 01:28:40,048 iteration 6790 : loss : 0.014855, loss_ce: 0.006437
2022-01-12 01:28:41,721 iteration 6791 : loss : 0.021478, loss_ce: 0.006238
2022-01-12 01:28:43,290 iteration 6792 : loss : 0.014036, loss_ce: 0.006093
2022-01-12 01:28:44,790 iteration 6793 : loss : 0.008938, loss_ce: 0.003175
2022-01-12 01:28:46,415 iteration 6794 : loss : 0.016249, loss_ce: 0.005764
2022-01-12 01:28:47,956 iteration 6795 : loss : 0.008798, loss_ce: 0.003748
2022-01-12 01:28:49,535 iteration 6796 : loss : 0.010076, loss_ce: 0.002425
2022-01-12 01:28:51,099 iteration 6797 : loss : 0.013643, loss_ce: 0.004833
2022-01-12 01:28:52,740 iteration 6798 : loss : 0.014946, loss_ce: 0.005110
2022-01-12 01:28:54,327 iteration 6799 : loss : 0.012248, loss_ce: 0.004113
2022-01-12 01:28:54,328 Training Data Eval:
2022-01-12 01:29:02,279   Average segmentation loss on training set: 0.0064
2022-01-12 01:29:02,280 Validation Data Eval:
2022-01-12 01:29:05,022   Average segmentation loss on validation set: 0.0705
2022-01-12 01:29:06,608 iteration 6800 : loss : 0.011424, loss_ce: 0.004834
100%|█████████████████████████████| 400/400 [3:17:15<00:00, 31.03s/it]100%|█████████████████████████████| 400/400 [3:17:15<00:00, 29.59s/it]
