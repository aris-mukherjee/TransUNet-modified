2022-01-10 09:41:20,912 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-10 09:41:20,913 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-10 09:41:20,913 ============================================================
2022-01-10 09:41:20,913 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-10 09:41:20,913 ============================================================
2022-01-10 09:41:20,913 Loading data...
2022-01-10 09:41:20,913 Reading NCI - RUNMC images...
2022-01-10 09:41:20,913 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-10 09:41:20,916 Already preprocessed this configuration. Loading now!
2022-01-10 09:41:20,944 Training Images: (256, 256, 286)
2022-01-10 09:41:20,944 Training Labels: (256, 256, 286)
2022-01-10 09:41:20,944 Validation Images: (256, 256, 98)
2022-01-10 09:41:20,944 Validation Labels: (256, 256, 98)
2022-01-10 09:41:20,944 ============================================================
2022-01-10 09:41:20,994 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-10 09:41:23,955 iteration 1 : loss : 0.767100, loss_ce: 0.861030
2022-01-10 09:41:25,380 iteration 2 : loss : 0.725719, loss_ce: 0.777104
2022-01-10 09:41:26,785 iteration 3 : loss : 0.691072, loss_ce: 0.698650
2022-01-10 09:41:28,245 iteration 4 : loss : 0.637819, loss_ce: 0.647292
2022-01-10 09:41:29,627 iteration 5 : loss : 0.603514, loss_ce: 0.566618
2022-01-10 09:41:31,131 iteration 6 : loss : 0.567682, loss_ce: 0.519332
2022-01-10 09:41:32,601 iteration 7 : loss : 0.544731, loss_ce: 0.471298
2022-01-10 09:41:34,083 iteration 8 : loss : 0.505893, loss_ce: 0.422376
2022-01-10 09:41:35,445 iteration 9 : loss : 0.474002, loss_ce: 0.400524
2022-01-10 09:41:36,926 iteration 10 : loss : 0.458315, loss_ce: 0.363393
2022-01-10 09:41:38,424 iteration 11 : loss : 0.436230, loss_ce: 0.327485
2022-01-10 09:41:39,949 iteration 12 : loss : 0.423763, loss_ce: 0.310846
2022-01-10 09:41:41,388 iteration 13 : loss : 0.397327, loss_ce: 0.266097
2022-01-10 09:41:42,942 iteration 14 : loss : 0.402097, loss_ce: 0.264638
2022-01-10 09:41:44,407 iteration 15 : loss : 0.364047, loss_ce: 0.225840
2022-01-10 09:41:45,845 iteration 16 : loss : 0.368065, loss_ce: 0.228350
2022-01-10 09:41:47,258 iteration 17 : loss : 0.360113, loss_ce: 0.190349
  0%|                               | 1/400 [00:26<2:55:15, 26.36s/it]2022-01-10 09:41:48,838 iteration 18 : loss : 0.333781, loss_ce: 0.188865
2022-01-10 09:41:50,322 iteration 19 : loss : 0.348982, loss_ce: 0.166344
2022-01-10 09:41:51,910 iteration 20 : loss : 0.322582, loss_ce: 0.175479
2022-01-10 09:41:53,449 iteration 21 : loss : 0.345574, loss_ce: 0.168754
2022-01-10 09:41:54,959 iteration 22 : loss : 0.325540, loss_ce: 0.161314
2022-01-10 09:41:56,597 iteration 23 : loss : 0.328152, loss_ce: 0.158121
2022-01-10 09:41:58,230 iteration 24 : loss : 0.293803, loss_ce: 0.144909
2022-01-10 09:41:59,892 iteration 25 : loss : 0.353505, loss_ce: 0.196699
2022-01-10 09:42:01,460 iteration 26 : loss : 0.311867, loss_ce: 0.153079
2022-01-10 09:42:02,981 iteration 27 : loss : 0.307105, loss_ce: 0.138425
2022-01-10 09:42:04,492 iteration 28 : loss : 0.275394, loss_ce: 0.119054
2022-01-10 09:42:06,100 iteration 29 : loss : 0.327922, loss_ce: 0.159418
2022-01-10 09:42:07,640 iteration 30 : loss : 0.277314, loss_ce: 0.129003
2022-01-10 09:42:09,159 iteration 31 : loss : 0.272320, loss_ce: 0.109264
2022-01-10 09:42:10,715 iteration 32 : loss : 0.294541, loss_ce: 0.152698
2022-01-10 09:42:12,323 iteration 33 : loss : 0.289743, loss_ce: 0.131623
2022-01-10 09:42:14,000 iteration 34 : loss : 0.236020, loss_ce: 0.097378
  0%|▏                              | 2/400 [00:53<2:56:12, 26.56s/it]2022-01-10 09:42:15,663 iteration 35 : loss : 0.247907, loss_ce: 0.115773
2022-01-10 09:42:17,254 iteration 36 : loss : 0.275870, loss_ce: 0.095938
2022-01-10 09:42:18,866 iteration 37 : loss : 0.252370, loss_ce: 0.096870
2022-01-10 09:42:20,444 iteration 38 : loss : 0.262031, loss_ce: 0.103790
2022-01-10 09:42:21,930 iteration 39 : loss : 0.290317, loss_ce: 0.123350
2022-01-10 09:42:23,481 iteration 40 : loss : 0.309371, loss_ce: 0.142987
2022-01-10 09:42:24,952 iteration 41 : loss : 0.211592, loss_ce: 0.089353
2022-01-10 09:42:26,544 iteration 42 : loss : 0.258656, loss_ce: 0.121816
2022-01-10 09:42:28,120 iteration 43 : loss : 0.220972, loss_ce: 0.095707
2022-01-10 09:42:29,606 iteration 44 : loss : 0.251189, loss_ce: 0.129817
2022-01-10 09:42:31,247 iteration 45 : loss : 0.257692, loss_ce: 0.104920
2022-01-10 09:42:32,834 iteration 46 : loss : 0.250226, loss_ce: 0.109041
2022-01-10 09:42:34,418 iteration 47 : loss : 0.234026, loss_ce: 0.092604
2022-01-10 09:42:35,989 iteration 48 : loss : 0.249478, loss_ce: 0.119173
2022-01-10 09:42:37,611 iteration 49 : loss : 0.310030, loss_ce: 0.136341
2022-01-10 09:42:39,238 iteration 50 : loss : 0.217375, loss_ce: 0.090954
2022-01-10 09:42:40,882 iteration 51 : loss : 0.215879, loss_ce: 0.087594
  1%|▏                              | 3/400 [01:19<2:56:43, 26.71s/it]2022-01-10 09:42:42,462 iteration 52 : loss : 0.295303, loss_ce: 0.118773
2022-01-10 09:42:43,955 iteration 53 : loss : 0.232829, loss_ce: 0.101109
2022-01-10 09:42:45,591 iteration 54 : loss : 0.269253, loss_ce: 0.105125
2022-01-10 09:42:47,200 iteration 55 : loss : 0.317369, loss_ce: 0.116881
2022-01-10 09:42:48,821 iteration 56 : loss : 0.259510, loss_ce: 0.119094
2022-01-10 09:42:50,366 iteration 57 : loss : 0.278498, loss_ce: 0.131702
2022-01-10 09:42:51,882 iteration 58 : loss : 0.275152, loss_ce: 0.144185
2022-01-10 09:42:53,396 iteration 59 : loss : 0.267708, loss_ce: 0.123020
2022-01-10 09:42:54,940 iteration 60 : loss : 0.235593, loss_ce: 0.115215
2022-01-10 09:42:56,497 iteration 61 : loss : 0.244591, loss_ce: 0.108999
2022-01-10 09:42:58,146 iteration 62 : loss : 0.292244, loss_ce: 0.109726
2022-01-10 09:42:59,680 iteration 63 : loss : 0.263016, loss_ce: 0.122674
2022-01-10 09:43:01,280 iteration 64 : loss : 0.288755, loss_ce: 0.119180
2022-01-10 09:43:02,867 iteration 65 : loss : 0.263819, loss_ce: 0.101423
2022-01-10 09:43:04,504 iteration 66 : loss : 0.275913, loss_ce: 0.116092
2022-01-10 09:43:06,067 iteration 67 : loss : 0.252349, loss_ce: 0.106824
2022-01-10 09:43:07,567 iteration 68 : loss : 0.248234, loss_ce: 0.110924
  1%|▎                              | 4/400 [01:46<2:56:12, 26.70s/it]2022-01-10 09:43:09,240 iteration 69 : loss : 0.281615, loss_ce: 0.119689
2022-01-10 09:43:10,793 iteration 70 : loss : 0.282206, loss_ce: 0.112716
2022-01-10 09:43:12,358 iteration 71 : loss : 0.298968, loss_ce: 0.160673
2022-01-10 09:43:13,940 iteration 72 : loss : 0.261090, loss_ce: 0.114916
2022-01-10 09:43:15,545 iteration 73 : loss : 0.228181, loss_ce: 0.086675
2022-01-10 09:43:17,135 iteration 74 : loss : 0.214920, loss_ce: 0.085919
2022-01-10 09:43:18,681 iteration 75 : loss : 0.265948, loss_ce: 0.128974
2022-01-10 09:43:20,172 iteration 76 : loss : 0.252524, loss_ce: 0.110895
2022-01-10 09:43:21,762 iteration 77 : loss : 0.295463, loss_ce: 0.139902
2022-01-10 09:43:23,339 iteration 78 : loss : 0.238518, loss_ce: 0.095971
2022-01-10 09:43:24,930 iteration 79 : loss : 0.257705, loss_ce: 0.095454
2022-01-10 09:43:26,413 iteration 80 : loss : 0.239507, loss_ce: 0.084968
2022-01-10 09:43:28,058 iteration 81 : loss : 0.288414, loss_ce: 0.127150
2022-01-10 09:43:29,613 iteration 82 : loss : 0.295870, loss_ce: 0.100874
2022-01-10 09:43:31,129 iteration 83 : loss : 0.296587, loss_ce: 0.093463
2022-01-10 09:43:32,785 iteration 84 : loss : 0.290806, loss_ce: 0.134193
2022-01-10 09:43:32,786 Training Data Eval:
2022-01-10 09:43:40,704   Average segmentation loss on training set: 0.2270
2022-01-10 09:43:40,705 Validation Data Eval:
2022-01-10 09:43:43,589   Average segmentation loss on validation set: 0.2408
2022-01-10 09:43:49,385 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 09:43:50,808 iteration 85 : loss : 0.283780, loss_ce: 0.134010
  1%|▍                              | 5/400 [02:29<3:35:03, 32.67s/it]2022-01-10 09:43:52,399 iteration 86 : loss : 0.226615, loss_ce: 0.104162
2022-01-10 09:43:53,955 iteration 87 : loss : 0.225387, loss_ce: 0.092417
2022-01-10 09:43:55,471 iteration 88 : loss : 0.221198, loss_ce: 0.110343
2022-01-10 09:43:57,051 iteration 89 : loss : 0.293888, loss_ce: 0.119902
2022-01-10 09:43:58,612 iteration 90 : loss : 0.236500, loss_ce: 0.105382
2022-01-10 09:44:00,238 iteration 91 : loss : 0.244488, loss_ce: 0.111573
2022-01-10 09:44:01,875 iteration 92 : loss : 0.220564, loss_ce: 0.077538
2022-01-10 09:44:03,389 iteration 93 : loss : 0.216633, loss_ce: 0.072595
2022-01-10 09:44:04,998 iteration 94 : loss : 0.221630, loss_ce: 0.090839
2022-01-10 09:44:06,568 iteration 95 : loss : 0.280396, loss_ce: 0.136449
2022-01-10 09:44:08,216 iteration 96 : loss : 0.253080, loss_ce: 0.104157
2022-01-10 09:44:09,794 iteration 97 : loss : 0.255809, loss_ce: 0.104178
2022-01-10 09:44:11,412 iteration 98 : loss : 0.294590, loss_ce: 0.127982
2022-01-10 09:44:12,981 iteration 99 : loss : 0.293089, loss_ce: 0.123039
2022-01-10 09:44:14,593 iteration 100 : loss : 0.229275, loss_ce: 0.101300
2022-01-10 09:44:16,148 iteration 101 : loss : 0.234388, loss_ce: 0.100078
2022-01-10 09:44:17,697 iteration 102 : loss : 0.250457, loss_ce: 0.120266
  2%|▍                              | 6/400 [02:56<3:21:35, 30.70s/it]2022-01-10 09:44:19,360 iteration 103 : loss : 0.242039, loss_ce: 0.087698
2022-01-10 09:44:20,988 iteration 104 : loss : 0.239118, loss_ce: 0.096365
2022-01-10 09:44:22,615 iteration 105 : loss : 0.192156, loss_ce: 0.079209
2022-01-10 09:44:24,319 iteration 106 : loss : 0.228164, loss_ce: 0.086208
2022-01-10 09:44:25,937 iteration 107 : loss : 0.244673, loss_ce: 0.080749
2022-01-10 09:44:27,507 iteration 108 : loss : 0.232441, loss_ce: 0.075432
2022-01-10 09:44:29,008 iteration 109 : loss : 0.269404, loss_ce: 0.103688
2022-01-10 09:44:30,550 iteration 110 : loss : 0.209536, loss_ce: 0.082841
2022-01-10 09:44:32,183 iteration 111 : loss : 0.212805, loss_ce: 0.090366
2022-01-10 09:44:33,751 iteration 112 : loss : 0.215369, loss_ce: 0.072940
2022-01-10 09:44:35,261 iteration 113 : loss : 0.194158, loss_ce: 0.073533
2022-01-10 09:44:36,874 iteration 114 : loss : 0.219100, loss_ce: 0.078225
2022-01-10 09:44:38,431 iteration 115 : loss : 0.197047, loss_ce: 0.090855
2022-01-10 09:44:40,041 iteration 116 : loss : 0.265778, loss_ce: 0.132713
2022-01-10 09:44:41,578 iteration 117 : loss : 0.202792, loss_ce: 0.086710
2022-01-10 09:44:43,131 iteration 118 : loss : 0.257071, loss_ce: 0.121137
2022-01-10 09:44:44,717 iteration 119 : loss : 0.196821, loss_ce: 0.071533
  2%|▌                              | 7/400 [03:23<3:13:12, 29.50s/it]2022-01-10 09:44:46,420 iteration 120 : loss : 0.212681, loss_ce: 0.092138
2022-01-10 09:44:48,035 iteration 121 : loss : 0.297858, loss_ce: 0.125921
2022-01-10 09:44:49,539 iteration 122 : loss : 0.240885, loss_ce: 0.109957
2022-01-10 09:44:51,153 iteration 123 : loss : 0.212502, loss_ce: 0.090146
2022-01-10 09:44:52,766 iteration 124 : loss : 0.260344, loss_ce: 0.099460
2022-01-10 09:44:54,328 iteration 125 : loss : 0.211897, loss_ce: 0.100103
2022-01-10 09:44:55,944 iteration 126 : loss : 0.277101, loss_ce: 0.111525
2022-01-10 09:44:57,570 iteration 127 : loss : 0.186566, loss_ce: 0.077230
2022-01-10 09:44:59,211 iteration 128 : loss : 0.185562, loss_ce: 0.086249
2022-01-10 09:45:00,760 iteration 129 : loss : 0.207942, loss_ce: 0.074939
2022-01-10 09:45:02,397 iteration 130 : loss : 0.213581, loss_ce: 0.100969
2022-01-10 09:45:03,969 iteration 131 : loss : 0.245897, loss_ce: 0.118056
2022-01-10 09:45:05,497 iteration 132 : loss : 0.235879, loss_ce: 0.089730
2022-01-10 09:45:07,133 iteration 133 : loss : 0.242466, loss_ce: 0.086540
2022-01-10 09:45:08,724 iteration 134 : loss : 0.265295, loss_ce: 0.108513
2022-01-10 09:45:10,320 iteration 135 : loss : 0.220468, loss_ce: 0.097291
2022-01-10 09:45:11,879 iteration 136 : loss : 0.227352, loss_ce: 0.101082
  2%|▌                              | 8/400 [03:50<3:07:50, 28.75s/it]2022-01-10 09:45:13,440 iteration 137 : loss : 0.149110, loss_ce: 0.050727
2022-01-10 09:45:14,971 iteration 138 : loss : 0.227408, loss_ce: 0.116411
2022-01-10 09:45:16,620 iteration 139 : loss : 0.208229, loss_ce: 0.071793
2022-01-10 09:45:18,310 iteration 140 : loss : 0.216554, loss_ce: 0.084247
2022-01-10 09:45:19,908 iteration 141 : loss : 0.236713, loss_ce: 0.088620
2022-01-10 09:45:21,436 iteration 142 : loss : 0.177525, loss_ce: 0.070878
2022-01-10 09:45:22,960 iteration 143 : loss : 0.213027, loss_ce: 0.084898
2022-01-10 09:45:24,588 iteration 144 : loss : 0.246886, loss_ce: 0.093176
2022-01-10 09:45:26,346 iteration 145 : loss : 0.219304, loss_ce: 0.101577
2022-01-10 09:45:27,902 iteration 146 : loss : 0.231813, loss_ce: 0.081054
2022-01-10 09:45:29,465 iteration 147 : loss : 0.239856, loss_ce: 0.103278
2022-01-10 09:45:31,048 iteration 148 : loss : 0.214366, loss_ce: 0.099462
2022-01-10 09:45:32,638 iteration 149 : loss : 0.211550, loss_ce: 0.086705
2022-01-10 09:45:34,209 iteration 150 : loss : 0.275504, loss_ce: 0.139811
2022-01-10 09:45:35,787 iteration 151 : loss : 0.162305, loss_ce: 0.075835
2022-01-10 09:45:37,430 iteration 152 : loss : 0.185540, loss_ce: 0.080782
2022-01-10 09:45:39,073 iteration 153 : loss : 0.177851, loss_ce: 0.075639
  2%|▋                              | 9/400 [04:18<3:04:11, 28.26s/it]2022-01-10 09:45:40,714 iteration 154 : loss : 0.170115, loss_ce: 0.068978
2022-01-10 09:45:42,317 iteration 155 : loss : 0.226920, loss_ce: 0.092456
2022-01-10 09:45:43,909 iteration 156 : loss : 0.196265, loss_ce: 0.083486
2022-01-10 09:45:45,512 iteration 157 : loss : 0.240185, loss_ce: 0.089586
2022-01-10 09:45:47,114 iteration 158 : loss : 0.254856, loss_ce: 0.116626
2022-01-10 09:45:48,699 iteration 159 : loss : 0.209690, loss_ce: 0.084509
2022-01-10 09:45:50,263 iteration 160 : loss : 0.250443, loss_ce: 0.084439
2022-01-10 09:45:51,869 iteration 161 : loss : 0.162347, loss_ce: 0.069656
2022-01-10 09:45:53,534 iteration 162 : loss : 0.184349, loss_ce: 0.079711
2022-01-10 09:45:55,026 iteration 163 : loss : 0.155794, loss_ce: 0.065397
2022-01-10 09:45:56,659 iteration 164 : loss : 0.176410, loss_ce: 0.072301
2022-01-10 09:45:58,219 iteration 165 : loss : 0.169685, loss_ce: 0.063188
2022-01-10 09:45:59,861 iteration 166 : loss : 0.234710, loss_ce: 0.098087
2022-01-10 09:46:01,496 iteration 167 : loss : 0.191367, loss_ce: 0.082776
2022-01-10 09:46:03,117 iteration 168 : loss : 0.186485, loss_ce: 0.080982
2022-01-10 09:46:04,694 iteration 169 : loss : 0.201045, loss_ce: 0.083983
2022-01-10 09:46:04,694 Training Data Eval:
2022-01-10 09:46:12,619   Average segmentation loss on training set: 0.3692
2022-01-10 09:46:12,619 Validation Data Eval:
2022-01-10 09:46:15,354   Average segmentation loss on validation set: 0.3279
2022-01-10 09:46:16,880 iteration 170 : loss : 0.194409, loss_ce: 0.082326
  2%|▊                             | 10/400 [04:55<3:22:52, 31.21s/it]2022-01-10 09:46:18,531 iteration 171 : loss : 0.221678, loss_ce: 0.104300
2022-01-10 09:46:20,003 iteration 172 : loss : 0.278691, loss_ce: 0.105669
2022-01-10 09:46:21,507 iteration 173 : loss : 0.169621, loss_ce: 0.073288
2022-01-10 09:46:23,150 iteration 174 : loss : 0.223164, loss_ce: 0.086727
2022-01-10 09:46:24,710 iteration 175 : loss : 0.236255, loss_ce: 0.102043
2022-01-10 09:46:26,393 iteration 176 : loss : 0.206334, loss_ce: 0.080683
2022-01-10 09:46:27,971 iteration 177 : loss : 0.254821, loss_ce: 0.096311
2022-01-10 09:46:29,608 iteration 178 : loss : 0.211577, loss_ce: 0.070499
2022-01-10 09:46:31,228 iteration 179 : loss : 0.163285, loss_ce: 0.066442
2022-01-10 09:46:32,853 iteration 180 : loss : 0.230321, loss_ce: 0.080446
2022-01-10 09:46:34,481 iteration 181 : loss : 0.156555, loss_ce: 0.059446
2022-01-10 09:46:36,100 iteration 182 : loss : 0.165523, loss_ce: 0.062962
2022-01-10 09:46:37,731 iteration 183 : loss : 0.185835, loss_ce: 0.067666
2022-01-10 09:46:39,320 iteration 184 : loss : 0.176958, loss_ce: 0.075610
2022-01-10 09:46:40,856 iteration 185 : loss : 0.203293, loss_ce: 0.099139
2022-01-10 09:46:42,395 iteration 186 : loss : 0.171199, loss_ce: 0.074767
2022-01-10 09:46:43,913 iteration 187 : loss : 0.252192, loss_ce: 0.127054
  3%|▊                             | 11/400 [05:22<3:14:03, 29.93s/it]2022-01-10 09:46:45,560 iteration 188 : loss : 0.279279, loss_ce: 0.126621
2022-01-10 09:46:47,180 iteration 189 : loss : 0.252192, loss_ce: 0.124670
2022-01-10 09:46:48,821 iteration 190 : loss : 0.182251, loss_ce: 0.063206
2022-01-10 09:46:50,408 iteration 191 : loss : 0.234128, loss_ce: 0.115407
2022-01-10 09:46:51,892 iteration 192 : loss : 0.161154, loss_ce: 0.069122
2022-01-10 09:46:53,410 iteration 193 : loss : 0.249778, loss_ce: 0.092965
2022-01-10 09:46:54,924 iteration 194 : loss : 0.206382, loss_ce: 0.087051
2022-01-10 09:46:56,587 iteration 195 : loss : 0.147073, loss_ce: 0.060304
2022-01-10 09:46:58,203 iteration 196 : loss : 0.176402, loss_ce: 0.061166
2022-01-10 09:46:59,796 iteration 197 : loss : 0.185515, loss_ce: 0.082296
2022-01-10 09:47:01,411 iteration 198 : loss : 0.214089, loss_ce: 0.107966
2022-01-10 09:47:03,021 iteration 199 : loss : 0.176194, loss_ce: 0.083604
2022-01-10 09:47:04,583 iteration 200 : loss : 0.227945, loss_ce: 0.086946
2022-01-10 09:47:06,157 iteration 201 : loss : 0.234565, loss_ce: 0.100231
2022-01-10 09:47:07,614 iteration 202 : loss : 0.206417, loss_ce: 0.075456
2022-01-10 09:47:09,251 iteration 203 : loss : 0.162031, loss_ce: 0.067191
2022-01-10 09:47:10,819 iteration 204 : loss : 0.199866, loss_ce: 0.097700
  3%|▉                             | 12/400 [05:49<3:07:38, 29.02s/it]2022-01-10 09:47:12,546 iteration 205 : loss : 0.194129, loss_ce: 0.079157
2022-01-10 09:47:14,144 iteration 206 : loss : 0.322565, loss_ce: 0.143023
2022-01-10 09:47:15,801 iteration 207 : loss : 0.238877, loss_ce: 0.090207
2022-01-10 09:47:17,435 iteration 208 : loss : 0.210751, loss_ce: 0.106722
2022-01-10 09:47:19,035 iteration 209 : loss : 0.214819, loss_ce: 0.088472
2022-01-10 09:47:20,607 iteration 210 : loss : 0.216975, loss_ce: 0.118112
2022-01-10 09:47:22,200 iteration 211 : loss : 0.161111, loss_ce: 0.079919
2022-01-10 09:47:23,839 iteration 212 : loss : 0.179852, loss_ce: 0.082146
2022-01-10 09:47:25,473 iteration 213 : loss : 0.210642, loss_ce: 0.105097
2022-01-10 09:47:27,034 iteration 214 : loss : 0.243496, loss_ce: 0.106024
2022-01-10 09:47:28,574 iteration 215 : loss : 0.199869, loss_ce: 0.076530
2022-01-10 09:47:30,205 iteration 216 : loss : 0.161083, loss_ce: 0.053642
2022-01-10 09:47:31,849 iteration 217 : loss : 0.196506, loss_ce: 0.078698
2022-01-10 09:47:33,414 iteration 218 : loss : 0.188377, loss_ce: 0.082710
2022-01-10 09:47:34,971 iteration 219 : loss : 0.196127, loss_ce: 0.090451
2022-01-10 09:47:36,574 iteration 220 : loss : 0.155688, loss_ce: 0.062731
2022-01-10 09:47:38,202 iteration 221 : loss : 0.178042, loss_ce: 0.076286
  3%|▉                             | 13/400 [06:17<3:03:55, 28.52s/it]2022-01-10 09:47:39,862 iteration 222 : loss : 0.149638, loss_ce: 0.068012
2022-01-10 09:47:41,429 iteration 223 : loss : 0.297532, loss_ce: 0.153580
2022-01-10 09:47:42,999 iteration 224 : loss : 0.147848, loss_ce: 0.059265
2022-01-10 09:47:44,713 iteration 225 : loss : 0.153349, loss_ce: 0.070188
2022-01-10 09:47:46,336 iteration 226 : loss : 0.193922, loss_ce: 0.082542
2022-01-10 09:47:47,916 iteration 227 : loss : 0.148127, loss_ce: 0.061665
2022-01-10 09:47:49,487 iteration 228 : loss : 0.178073, loss_ce: 0.062032
2022-01-10 09:47:51,092 iteration 229 : loss : 0.203703, loss_ce: 0.087524
2022-01-10 09:47:52,628 iteration 230 : loss : 0.130996, loss_ce: 0.056682
2022-01-10 09:47:54,214 iteration 231 : loss : 0.152926, loss_ce: 0.064839
2022-01-10 09:47:55,779 iteration 232 : loss : 0.168074, loss_ce: 0.075717
2022-01-10 09:47:57,285 iteration 233 : loss : 0.171339, loss_ce: 0.085602
2022-01-10 09:47:58,852 iteration 234 : loss : 0.222031, loss_ce: 0.096111
2022-01-10 09:48:00,385 iteration 235 : loss : 0.185785, loss_ce: 0.083599
2022-01-10 09:48:01,955 iteration 236 : loss : 0.333364, loss_ce: 0.152393
2022-01-10 09:48:03,480 iteration 237 : loss : 0.241809, loss_ce: 0.126876
2022-01-10 09:48:05,094 iteration 238 : loss : 0.191221, loss_ce: 0.064033
  4%|█                             | 14/400 [06:44<3:00:19, 28.03s/it]2022-01-10 09:48:06,730 iteration 239 : loss : 0.191240, loss_ce: 0.069969
2022-01-10 09:48:08,400 iteration 240 : loss : 0.177400, loss_ce: 0.056242
2022-01-10 09:48:10,023 iteration 241 : loss : 0.154522, loss_ce: 0.064855
2022-01-10 09:48:11,589 iteration 242 : loss : 0.113493, loss_ce: 0.045254
2022-01-10 09:48:13,153 iteration 243 : loss : 0.133735, loss_ce: 0.053899
2022-01-10 09:48:14,722 iteration 244 : loss : 0.178434, loss_ce: 0.068073
2022-01-10 09:48:16,249 iteration 245 : loss : 0.132468, loss_ce: 0.047019
2022-01-10 09:48:17,810 iteration 246 : loss : 0.203820, loss_ce: 0.079524
2022-01-10 09:48:19,413 iteration 247 : loss : 0.169895, loss_ce: 0.080924
2022-01-10 09:48:21,025 iteration 248 : loss : 0.146829, loss_ce: 0.069069
2022-01-10 09:48:22,585 iteration 249 : loss : 0.148569, loss_ce: 0.060252
2022-01-10 09:48:24,159 iteration 250 : loss : 0.156542, loss_ce: 0.057243
2022-01-10 09:48:25,680 iteration 251 : loss : 0.133030, loss_ce: 0.057064
2022-01-10 09:48:27,334 iteration 252 : loss : 0.196440, loss_ce: 0.101077
2022-01-10 09:48:28,916 iteration 253 : loss : 0.118985, loss_ce: 0.043935
2022-01-10 09:48:30,475 iteration 254 : loss : 0.134817, loss_ce: 0.067541
2022-01-10 09:48:30,475 Training Data Eval:
2022-01-10 09:48:38,399   Average segmentation loss on training set: 0.1272
2022-01-10 09:48:38,400 Validation Data Eval:
2022-01-10 09:48:41,130   Average segmentation loss on validation set: 0.1643
2022-01-10 09:48:46,892 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 09:48:48,341 iteration 255 : loss : 0.201403, loss_ce: 0.097548
  4%|█▏                            | 15/400 [07:27<3:29:16, 32.61s/it]2022-01-10 09:48:49,886 iteration 256 : loss : 0.126045, loss_ce: 0.054785
2022-01-10 09:48:51,423 iteration 257 : loss : 0.162114, loss_ce: 0.074788
2022-01-10 09:48:53,054 iteration 258 : loss : 0.145314, loss_ce: 0.070797
2022-01-10 09:48:54,646 iteration 259 : loss : 0.183224, loss_ce: 0.091118
2022-01-10 09:48:56,174 iteration 260 : loss : 0.115081, loss_ce: 0.063257
2022-01-10 09:48:57,778 iteration 261 : loss : 0.181497, loss_ce: 0.081328
2022-01-10 09:48:59,411 iteration 262 : loss : 0.233765, loss_ce: 0.111794
2022-01-10 09:49:00,948 iteration 263 : loss : 0.105144, loss_ce: 0.044919
2022-01-10 09:49:02,581 iteration 264 : loss : 0.259217, loss_ce: 0.099590
2022-01-10 09:49:04,270 iteration 265 : loss : 0.197659, loss_ce: 0.101206
2022-01-10 09:49:05,880 iteration 266 : loss : 0.137794, loss_ce: 0.055379
2022-01-10 09:49:07,559 iteration 267 : loss : 0.146721, loss_ce: 0.050832
2022-01-10 09:49:09,164 iteration 268 : loss : 0.176315, loss_ce: 0.077370
2022-01-10 09:49:10,762 iteration 269 : loss : 0.250829, loss_ce: 0.109754
2022-01-10 09:49:12,337 iteration 270 : loss : 0.185708, loss_ce: 0.071448
2022-01-10 09:49:13,893 iteration 271 : loss : 0.148633, loss_ce: 0.068432
2022-01-10 09:49:15,426 iteration 272 : loss : 0.138484, loss_ce: 0.058665
  4%|█▏                            | 16/400 [07:54<3:18:05, 30.95s/it]2022-01-10 09:49:17,124 iteration 273 : loss : 0.183865, loss_ce: 0.074852
2022-01-10 09:49:18,714 iteration 274 : loss : 0.265479, loss_ce: 0.112774
2022-01-10 09:49:20,283 iteration 275 : loss : 0.227122, loss_ce: 0.058328
2022-01-10 09:49:21,840 iteration 276 : loss : 0.130941, loss_ce: 0.047963
2022-01-10 09:49:23,395 iteration 277 : loss : 0.110670, loss_ce: 0.043282
2022-01-10 09:49:24,984 iteration 278 : loss : 0.220850, loss_ce: 0.105030
2022-01-10 09:49:26,556 iteration 279 : loss : 0.205870, loss_ce: 0.090220
2022-01-10 09:49:28,130 iteration 280 : loss : 0.161464, loss_ce: 0.081043
2022-01-10 09:49:29,762 iteration 281 : loss : 0.143178, loss_ce: 0.075832
2022-01-10 09:49:31,370 iteration 282 : loss : 0.123444, loss_ce: 0.050029
2022-01-10 09:49:32,927 iteration 283 : loss : 0.156775, loss_ce: 0.064986
2022-01-10 09:49:34,527 iteration 284 : loss : 0.159890, loss_ce: 0.088273
2022-01-10 09:49:36,147 iteration 285 : loss : 0.166404, loss_ce: 0.055155
2022-01-10 09:49:37,636 iteration 286 : loss : 0.143666, loss_ce: 0.055071
2022-01-10 09:49:39,210 iteration 287 : loss : 0.155891, loss_ce: 0.068487
2022-01-10 09:49:40,804 iteration 288 : loss : 0.143314, loss_ce: 0.052552
2022-01-10 09:49:42,365 iteration 289 : loss : 0.189154, loss_ce: 0.063590
  4%|█▎                            | 17/400 [08:21<3:09:51, 29.74s/it]2022-01-10 09:49:44,046 iteration 290 : loss : 0.183092, loss_ce: 0.078246
2022-01-10 09:49:45,587 iteration 291 : loss : 0.164076, loss_ce: 0.060987
2022-01-10 09:49:47,111 iteration 292 : loss : 0.178419, loss_ce: 0.086159
2022-01-10 09:49:48,741 iteration 293 : loss : 0.133179, loss_ce: 0.054049
2022-01-10 09:49:50,308 iteration 294 : loss : 0.136891, loss_ce: 0.061745
2022-01-10 09:49:51,944 iteration 295 : loss : 0.134608, loss_ce: 0.064528
2022-01-10 09:49:53,587 iteration 296 : loss : 0.180117, loss_ce: 0.080501
2022-01-10 09:49:55,199 iteration 297 : loss : 0.124680, loss_ce: 0.060888
2022-01-10 09:49:56,769 iteration 298 : loss : 0.169424, loss_ce: 0.076852
2022-01-10 09:49:58,286 iteration 299 : loss : 0.185111, loss_ce: 0.073874
2022-01-10 09:49:59,840 iteration 300 : loss : 0.166732, loss_ce: 0.065346
2022-01-10 09:50:01,332 iteration 301 : loss : 0.172345, loss_ce: 0.063097
2022-01-10 09:50:02,978 iteration 302 : loss : 0.148721, loss_ce: 0.050837
2022-01-10 09:50:04,609 iteration 303 : loss : 0.116883, loss_ce: 0.040781
2022-01-10 09:50:06,235 iteration 304 : loss : 0.158298, loss_ce: 0.069517
2022-01-10 09:50:07,786 iteration 305 : loss : 0.111327, loss_ce: 0.048841
2022-01-10 09:50:09,359 iteration 306 : loss : 0.174539, loss_ce: 0.072499
  4%|█▎                            | 18/400 [08:48<3:04:06, 28.92s/it]2022-01-10 09:50:10,934 iteration 307 : loss : 0.186736, loss_ce: 0.073980
2022-01-10 09:50:12,433 iteration 308 : loss : 0.137845, loss_ce: 0.063435
2022-01-10 09:50:13,956 iteration 309 : loss : 0.162709, loss_ce: 0.073814
2022-01-10 09:50:15,537 iteration 310 : loss : 0.180343, loss_ce: 0.073174
2022-01-10 09:50:17,152 iteration 311 : loss : 0.147497, loss_ce: 0.060217
2022-01-10 09:50:18,777 iteration 312 : loss : 0.118585, loss_ce: 0.049173
2022-01-10 09:50:20,329 iteration 313 : loss : 0.148659, loss_ce: 0.063348
2022-01-10 09:50:21,924 iteration 314 : loss : 0.135241, loss_ce: 0.061750
2022-01-10 09:50:23,562 iteration 315 : loss : 0.171084, loss_ce: 0.073836
2022-01-10 09:50:25,145 iteration 316 : loss : 0.173986, loss_ce: 0.068553
2022-01-10 09:50:26,671 iteration 317 : loss : 0.153702, loss_ce: 0.057830
2022-01-10 09:50:28,220 iteration 318 : loss : 0.134661, loss_ce: 0.055151
2022-01-10 09:50:29,859 iteration 319 : loss : 0.116908, loss_ce: 0.050258
2022-01-10 09:50:31,409 iteration 320 : loss : 0.191286, loss_ce: 0.075910
2022-01-10 09:50:32,932 iteration 321 : loss : 0.101894, loss_ce: 0.053441
2022-01-10 09:50:34,507 iteration 322 : loss : 0.142179, loss_ce: 0.057554
2022-01-10 09:50:36,135 iteration 323 : loss : 0.122464, loss_ce: 0.052794
  5%|█▍                            | 19/400 [09:15<2:59:33, 28.28s/it]2022-01-10 09:50:37,704 iteration 324 : loss : 0.165490, loss_ce: 0.069006
2022-01-10 09:50:39,276 iteration 325 : loss : 0.132074, loss_ce: 0.051601
2022-01-10 09:50:40,838 iteration 326 : loss : 0.127501, loss_ce: 0.047694
2022-01-10 09:50:42,495 iteration 327 : loss : 0.163876, loss_ce: 0.056553
2022-01-10 09:50:44,119 iteration 328 : loss : 0.131836, loss_ce: 0.054727
2022-01-10 09:50:45,712 iteration 329 : loss : 0.133582, loss_ce: 0.054807
2022-01-10 09:50:47,200 iteration 330 : loss : 0.111911, loss_ce: 0.046197
2022-01-10 09:50:48,829 iteration 331 : loss : 0.118259, loss_ce: 0.041246
2022-01-10 09:50:50,460 iteration 332 : loss : 0.099274, loss_ce: 0.039498
2022-01-10 09:50:51,992 iteration 333 : loss : 0.106376, loss_ce: 0.042231
2022-01-10 09:50:53,613 iteration 334 : loss : 0.139137, loss_ce: 0.070525
2022-01-10 09:50:55,163 iteration 335 : loss : 0.143314, loss_ce: 0.049038
2022-01-10 09:50:56,784 iteration 336 : loss : 0.126114, loss_ce: 0.048957
2022-01-10 09:50:58,335 iteration 337 : loss : 0.120076, loss_ce: 0.047758
2022-01-10 09:50:59,841 iteration 338 : loss : 0.139807, loss_ce: 0.062978
2022-01-10 09:51:01,415 iteration 339 : loss : 0.114159, loss_ce: 0.041505
2022-01-10 09:51:01,415 Training Data Eval:
2022-01-10 09:51:09,376   Average segmentation loss on training set: 0.1195
2022-01-10 09:51:09,377 Validation Data Eval:
2022-01-10 09:51:12,129   Average segmentation loss on validation set: 0.1665
2022-01-10 09:51:13,684 iteration 340 : loss : 0.150612, loss_ce: 0.056997
  5%|█▌                            | 20/400 [09:52<3:16:41, 31.06s/it]2022-01-10 09:51:15,374 iteration 341 : loss : 0.158601, loss_ce: 0.065152
2022-01-10 09:51:17,020 iteration 342 : loss : 0.128408, loss_ce: 0.059018
2022-01-10 09:51:18,731 iteration 343 : loss : 0.153493, loss_ce: 0.084785
2022-01-10 09:51:20,309 iteration 344 : loss : 0.181442, loss_ce: 0.069375
2022-01-10 09:51:21,911 iteration 345 : loss : 0.207240, loss_ce: 0.095109
2022-01-10 09:51:23,569 iteration 346 : loss : 0.127599, loss_ce: 0.053350
2022-01-10 09:51:25,120 iteration 347 : loss : 0.111121, loss_ce: 0.056439
2022-01-10 09:51:26,814 iteration 348 : loss : 0.178524, loss_ce: 0.085041
2022-01-10 09:51:28,399 iteration 349 : loss : 0.166331, loss_ce: 0.077912
2022-01-10 09:51:29,888 iteration 350 : loss : 0.128328, loss_ce: 0.052566
2022-01-10 09:51:31,535 iteration 351 : loss : 0.124200, loss_ce: 0.044540
2022-01-10 09:51:33,121 iteration 352 : loss : 0.130302, loss_ce: 0.051162
2022-01-10 09:51:34,811 iteration 353 : loss : 0.167865, loss_ce: 0.054258
2022-01-10 09:51:36,429 iteration 354 : loss : 0.136189, loss_ce: 0.060053
2022-01-10 09:51:38,090 iteration 355 : loss : 0.131711, loss_ce: 0.053219
2022-01-10 09:51:39,627 iteration 356 : loss : 0.140224, loss_ce: 0.058874
2022-01-10 09:51:41,187 iteration 357 : loss : 0.135525, loss_ce: 0.056479
  5%|█▌                            | 21/400 [10:20<3:09:27, 29.99s/it]2022-01-10 09:51:42,913 iteration 358 : loss : 0.134318, loss_ce: 0.055766
2022-01-10 09:51:44,565 iteration 359 : loss : 0.109932, loss_ce: 0.050989
2022-01-10 09:51:46,164 iteration 360 : loss : 0.200574, loss_ce: 0.061895
2022-01-10 09:51:47,788 iteration 361 : loss : 0.215720, loss_ce: 0.077971
2022-01-10 09:51:49,285 iteration 362 : loss : 0.135006, loss_ce: 0.052710
2022-01-10 09:51:50,949 iteration 363 : loss : 0.166230, loss_ce: 0.085812
2022-01-10 09:51:52,555 iteration 364 : loss : 0.207350, loss_ce: 0.115815
2022-01-10 09:51:54,130 iteration 365 : loss : 0.119950, loss_ce: 0.043145
2022-01-10 09:51:55,706 iteration 366 : loss : 0.126825, loss_ce: 0.050986
2022-01-10 09:51:57,333 iteration 367 : loss : 0.144975, loss_ce: 0.064296
2022-01-10 09:51:58,867 iteration 368 : loss : 0.115933, loss_ce: 0.038669
2022-01-10 09:52:00,486 iteration 369 : loss : 0.134984, loss_ce: 0.068637
2022-01-10 09:52:01,986 iteration 370 : loss : 0.146386, loss_ce: 0.041194
2022-01-10 09:52:03,637 iteration 371 : loss : 0.150051, loss_ce: 0.066310
2022-01-10 09:52:05,291 iteration 372 : loss : 0.141747, loss_ce: 0.048394
2022-01-10 09:52:06,907 iteration 373 : loss : 0.106037, loss_ce: 0.039012
2022-01-10 09:52:08,463 iteration 374 : loss : 0.111681, loss_ce: 0.050929
  6%|█▋                            | 22/400 [10:47<3:03:48, 29.18s/it]2022-01-10 09:52:10,147 iteration 375 : loss : 0.131826, loss_ce: 0.055407
2022-01-10 09:52:11,764 iteration 376 : loss : 0.118501, loss_ce: 0.053322
2022-01-10 09:52:13,395 iteration 377 : loss : 0.184439, loss_ce: 0.062265
2022-01-10 09:52:15,012 iteration 378 : loss : 0.122734, loss_ce: 0.052659
2022-01-10 09:52:16,546 iteration 379 : loss : 0.166674, loss_ce: 0.064525
2022-01-10 09:52:18,174 iteration 380 : loss : 0.128232, loss_ce: 0.057584
2022-01-10 09:52:19,661 iteration 381 : loss : 0.098313, loss_ce: 0.037526
2022-01-10 09:52:21,365 iteration 382 : loss : 0.107851, loss_ce: 0.047695
2022-01-10 09:52:23,008 iteration 383 : loss : 0.137717, loss_ce: 0.061102
2022-01-10 09:52:24,600 iteration 384 : loss : 0.093952, loss_ce: 0.039556
2022-01-10 09:52:26,145 iteration 385 : loss : 0.113125, loss_ce: 0.046621
2022-01-10 09:52:27,770 iteration 386 : loss : 0.137746, loss_ce: 0.062969
2022-01-10 09:52:29,389 iteration 387 : loss : 0.118897, loss_ce: 0.047464
2022-01-10 09:52:31,037 iteration 388 : loss : 0.109433, loss_ce: 0.050541
2022-01-10 09:52:32,596 iteration 389 : loss : 0.133435, loss_ce: 0.046026
2022-01-10 09:52:34,215 iteration 390 : loss : 0.121739, loss_ce: 0.066310
2022-01-10 09:52:35,777 iteration 391 : loss : 0.139380, loss_ce: 0.050356
  6%|█▋                            | 23/400 [11:14<2:59:48, 28.62s/it]2022-01-10 09:52:37,551 iteration 392 : loss : 0.088506, loss_ce: 0.041686
2022-01-10 09:52:39,105 iteration 393 : loss : 0.117849, loss_ce: 0.061601
2022-01-10 09:52:40,715 iteration 394 : loss : 0.167698, loss_ce: 0.069344
2022-01-10 09:52:42,279 iteration 395 : loss : 0.108079, loss_ce: 0.051575
2022-01-10 09:52:43,914 iteration 396 : loss : 0.121808, loss_ce: 0.054448
2022-01-10 09:52:45,544 iteration 397 : loss : 0.113013, loss_ce: 0.048762
2022-01-10 09:52:47,164 iteration 398 : loss : 0.119417, loss_ce: 0.053630
2022-01-10 09:52:48,747 iteration 399 : loss : 0.067662, loss_ce: 0.025226
2022-01-10 09:52:50,395 iteration 400 : loss : 0.096293, loss_ce: 0.043975
2022-01-10 09:52:51,911 iteration 401 : loss : 0.133312, loss_ce: 0.041738
2022-01-10 09:52:53,560 iteration 402 : loss : 0.163177, loss_ce: 0.066436
2022-01-10 09:52:55,243 iteration 403 : loss : 0.087602, loss_ce: 0.038164
2022-01-10 09:52:56,846 iteration 404 : loss : 0.098715, loss_ce: 0.031633
2022-01-10 09:52:58,486 iteration 405 : loss : 0.136065, loss_ce: 0.043026
2022-01-10 09:53:00,039 iteration 406 : loss : 0.122872, loss_ce: 0.051858
2022-01-10 09:53:01,593 iteration 407 : loss : 0.106515, loss_ce: 0.043173
2022-01-10 09:53:03,140 iteration 408 : loss : 0.116031, loss_ce: 0.040353
  6%|█▊                            | 24/400 [11:42<2:56:57, 28.24s/it]2022-01-10 09:53:04,914 iteration 409 : loss : 0.116209, loss_ce: 0.049029
2022-01-10 09:53:06,496 iteration 410 : loss : 0.120281, loss_ce: 0.044677
2022-01-10 09:53:08,115 iteration 411 : loss : 0.109872, loss_ce: 0.043941
2022-01-10 09:53:09,697 iteration 412 : loss : 0.089172, loss_ce: 0.034016
2022-01-10 09:53:11,270 iteration 413 : loss : 0.093253, loss_ce: 0.038801
2022-01-10 09:53:12,873 iteration 414 : loss : 0.115373, loss_ce: 0.052520
2022-01-10 09:53:14,516 iteration 415 : loss : 0.130704, loss_ce: 0.071078
2022-01-10 09:53:15,985 iteration 416 : loss : 0.119709, loss_ce: 0.046150
2022-01-10 09:53:17,563 iteration 417 : loss : 0.174942, loss_ce: 0.066947
2022-01-10 09:53:19,162 iteration 418 : loss : 0.118186, loss_ce: 0.040811
2022-01-10 09:53:20,729 iteration 419 : loss : 0.220759, loss_ce: 0.077024
2022-01-10 09:53:22,328 iteration 420 : loss : 0.115886, loss_ce: 0.050279
2022-01-10 09:53:23,820 iteration 421 : loss : 0.129919, loss_ce: 0.039217
2022-01-10 09:53:25,447 iteration 422 : loss : 0.117326, loss_ce: 0.043566
2022-01-10 09:53:27,013 iteration 423 : loss : 0.115012, loss_ce: 0.042283
2022-01-10 09:53:28,574 iteration 424 : loss : 0.115535, loss_ce: 0.056731
2022-01-10 09:53:28,574 Training Data Eval:
2022-01-10 09:53:36,536   Average segmentation loss on training set: 0.1576
2022-01-10 09:53:36,536 Validation Data Eval:
2022-01-10 09:53:39,278   Average segmentation loss on validation set: 0.2658
2022-01-10 09:53:40,915 iteration 425 : loss : 0.165812, loss_ce: 0.070448
  6%|█▉                            | 25/400 [12:19<3:14:22, 31.10s/it]2022-01-10 09:53:42,608 iteration 426 : loss : 0.115157, loss_ce: 0.043678
2022-01-10 09:53:44,148 iteration 427 : loss : 0.119876, loss_ce: 0.048950
2022-01-10 09:53:45,841 iteration 428 : loss : 0.112666, loss_ce: 0.047337
2022-01-10 09:53:47,408 iteration 429 : loss : 0.083483, loss_ce: 0.029100
2022-01-10 09:53:48,915 iteration 430 : loss : 0.103130, loss_ce: 0.046166
2022-01-10 09:53:50,535 iteration 431 : loss : 0.103418, loss_ce: 0.057824
2022-01-10 09:53:52,143 iteration 432 : loss : 0.121982, loss_ce: 0.046578
2022-01-10 09:53:53,664 iteration 433 : loss : 0.113734, loss_ce: 0.047365
2022-01-10 09:53:55,189 iteration 434 : loss : 0.100101, loss_ce: 0.035026
2022-01-10 09:53:56,790 iteration 435 : loss : 0.106920, loss_ce: 0.040773
2022-01-10 09:53:58,412 iteration 436 : loss : 0.135386, loss_ce: 0.041049
2022-01-10 09:53:59,960 iteration 437 : loss : 0.092421, loss_ce: 0.038288
2022-01-10 09:54:01,499 iteration 438 : loss : 0.110459, loss_ce: 0.044181
2022-01-10 09:54:03,059 iteration 439 : loss : 0.141556, loss_ce: 0.054616
2022-01-10 09:54:04,781 iteration 440 : loss : 0.172283, loss_ce: 0.073340
2022-01-10 09:54:06,360 iteration 441 : loss : 0.082193, loss_ce: 0.035290
2022-01-10 09:54:07,908 iteration 442 : loss : 0.081356, loss_ce: 0.029942
  6%|█▉                            | 26/400 [12:46<3:06:11, 29.87s/it]2022-01-10 09:54:09,603 iteration 443 : loss : 0.097528, loss_ce: 0.041064
2022-01-10 09:54:11,146 iteration 444 : loss : 0.081944, loss_ce: 0.031565
2022-01-10 09:54:12,674 iteration 445 : loss : 0.129164, loss_ce: 0.053160
2022-01-10 09:54:14,231 iteration 446 : loss : 0.095969, loss_ce: 0.033216
2022-01-10 09:54:15,859 iteration 447 : loss : 0.092051, loss_ce: 0.036111
2022-01-10 09:54:17,366 iteration 448 : loss : 0.135543, loss_ce: 0.046968
2022-01-10 09:54:18,906 iteration 449 : loss : 0.100469, loss_ce: 0.029637
2022-01-10 09:54:20,405 iteration 450 : loss : 0.123966, loss_ce: 0.064674
2022-01-10 09:54:21,977 iteration 451 : loss : 0.131821, loss_ce: 0.056554
2022-01-10 09:54:23,558 iteration 452 : loss : 0.100640, loss_ce: 0.037414
2022-01-10 09:54:25,111 iteration 453 : loss : 0.061149, loss_ce: 0.023842
2022-01-10 09:54:26,792 iteration 454 : loss : 0.115718, loss_ce: 0.048974
2022-01-10 09:54:28,419 iteration 455 : loss : 0.114129, loss_ce: 0.047551
2022-01-10 09:54:29,992 iteration 456 : loss : 0.089489, loss_ce: 0.032003
2022-01-10 09:54:31,555 iteration 457 : loss : 0.093583, loss_ce: 0.046858
2022-01-10 09:54:33,117 iteration 458 : loss : 0.091475, loss_ce: 0.049956
2022-01-10 09:54:34,703 iteration 459 : loss : 0.114810, loss_ce: 0.047068
  7%|██                            | 27/400 [13:13<2:59:56, 28.95s/it]2022-01-10 09:54:36,285 iteration 460 : loss : 0.086652, loss_ce: 0.041031
2022-01-10 09:54:37,838 iteration 461 : loss : 0.077013, loss_ce: 0.036003
2022-01-10 09:54:39,499 iteration 462 : loss : 0.150686, loss_ce: 0.062883
2022-01-10 09:54:41,095 iteration 463 : loss : 0.125336, loss_ce: 0.043636
2022-01-10 09:54:42,828 iteration 464 : loss : 0.173977, loss_ce: 0.064385
2022-01-10 09:54:44,466 iteration 465 : loss : 0.119064, loss_ce: 0.054388
2022-01-10 09:54:46,027 iteration 466 : loss : 0.084927, loss_ce: 0.035731
2022-01-10 09:54:47,602 iteration 467 : loss : 0.120794, loss_ce: 0.050433
2022-01-10 09:54:49,157 iteration 468 : loss : 0.088793, loss_ce: 0.034377
2022-01-10 09:54:50,777 iteration 469 : loss : 0.081441, loss_ce: 0.037158
2022-01-10 09:54:52,358 iteration 470 : loss : 0.102771, loss_ce: 0.043851
2022-01-10 09:54:53,946 iteration 471 : loss : 0.180499, loss_ce: 0.099955
2022-01-10 09:54:55,569 iteration 472 : loss : 0.067570, loss_ce: 0.029654
2022-01-10 09:54:57,161 iteration 473 : loss : 0.096416, loss_ce: 0.042418
2022-01-10 09:54:58,704 iteration 474 : loss : 0.155896, loss_ce: 0.055098
2022-01-10 09:55:00,285 iteration 475 : loss : 0.110096, loss_ce: 0.035286
2022-01-10 09:55:01,940 iteration 476 : loss : 0.097206, loss_ce: 0.037379
  7%|██                            | 28/400 [13:40<2:56:16, 28.43s/it]2022-01-10 09:55:03,574 iteration 477 : loss : 0.091549, loss_ce: 0.039249
2022-01-10 09:55:05,090 iteration 478 : loss : 0.097204, loss_ce: 0.050212
2022-01-10 09:55:06,710 iteration 479 : loss : 0.101439, loss_ce: 0.043232
2022-01-10 09:55:08,293 iteration 480 : loss : 0.115285, loss_ce: 0.039315
2022-01-10 09:55:09,825 iteration 481 : loss : 0.094751, loss_ce: 0.030067
2022-01-10 09:55:11,482 iteration 482 : loss : 0.093864, loss_ce: 0.038353
2022-01-10 09:55:13,003 iteration 483 : loss : 0.144314, loss_ce: 0.069467
2022-01-10 09:55:14,595 iteration 484 : loss : 0.116524, loss_ce: 0.044209
2022-01-10 09:55:16,186 iteration 485 : loss : 0.110651, loss_ce: 0.045244
2022-01-10 09:55:17,873 iteration 486 : loss : 0.087207, loss_ce: 0.036533
2022-01-10 09:55:19,459 iteration 487 : loss : 0.123603, loss_ce: 0.045123
2022-01-10 09:55:20,961 iteration 488 : loss : 0.105946, loss_ce: 0.048698
2022-01-10 09:55:22,574 iteration 489 : loss : 0.102051, loss_ce: 0.047710
2022-01-10 09:55:24,146 iteration 490 : loss : 0.113572, loss_ce: 0.055666
2022-01-10 09:55:25,793 iteration 491 : loss : 0.131260, loss_ce: 0.046603
2022-01-10 09:55:27,349 iteration 492 : loss : 0.093787, loss_ce: 0.052311
2022-01-10 09:55:28,883 iteration 493 : loss : 0.085701, loss_ce: 0.042623
  7%|██▏                           | 29/400 [14:07<2:53:03, 27.99s/it]2022-01-10 09:55:30,516 iteration 494 : loss : 0.117546, loss_ce: 0.045657
2022-01-10 09:55:32,039 iteration 495 : loss : 0.116523, loss_ce: 0.039568
2022-01-10 09:55:33,623 iteration 496 : loss : 0.097056, loss_ce: 0.034734
2022-01-10 09:55:35,283 iteration 497 : loss : 0.109133, loss_ce: 0.051460
2022-01-10 09:55:36,972 iteration 498 : loss : 0.148863, loss_ce: 0.084069
2022-01-10 09:55:38,590 iteration 499 : loss : 0.099838, loss_ce: 0.032191
2022-01-10 09:55:40,311 iteration 500 : loss : 0.152073, loss_ce: 0.054032
2022-01-10 09:55:41,907 iteration 501 : loss : 0.109656, loss_ce: 0.040320
2022-01-10 09:55:43,440 iteration 502 : loss : 0.094396, loss_ce: 0.041685
2022-01-10 09:55:45,128 iteration 503 : loss : 0.109437, loss_ce: 0.034386
2022-01-10 09:55:46,845 iteration 504 : loss : 0.122438, loss_ce: 0.051750
2022-01-10 09:55:48,411 iteration 505 : loss : 0.095907, loss_ce: 0.037092
2022-01-10 09:55:50,034 iteration 506 : loss : 0.101762, loss_ce: 0.040131
2022-01-10 09:55:51,689 iteration 507 : loss : 0.082716, loss_ce: 0.032214
2022-01-10 09:55:53,280 iteration 508 : loss : 0.098920, loss_ce: 0.038934
2022-01-10 09:55:54,899 iteration 509 : loss : 0.098750, loss_ce: 0.040855
2022-01-10 09:55:54,900 Training Data Eval:
2022-01-10 09:56:02,852   Average segmentation loss on training set: 0.1063
2022-01-10 09:56:02,852 Validation Data Eval:
2022-01-10 09:56:05,596   Average segmentation loss on validation set: 0.1674
2022-01-10 09:56:07,078 iteration 510 : loss : 0.110446, loss_ce: 0.041341
  8%|██▎                           | 30/400 [14:46<3:11:27, 31.05s/it]2022-01-10 09:56:08,575 iteration 511 : loss : 0.073788, loss_ce: 0.031231
2022-01-10 09:56:10,308 iteration 512 : loss : 0.114455, loss_ce: 0.048491
2022-01-10 09:56:11,833 iteration 513 : loss : 0.129128, loss_ce: 0.063411
2022-01-10 09:56:13,447 iteration 514 : loss : 0.102701, loss_ce: 0.037324
2022-01-10 09:56:15,002 iteration 515 : loss : 0.063231, loss_ce: 0.027242
2022-01-10 09:56:16,488 iteration 516 : loss : 0.053951, loss_ce: 0.026420
2022-01-10 09:56:18,216 iteration 517 : loss : 0.082163, loss_ce: 0.028737
2022-01-10 09:56:19,830 iteration 518 : loss : 0.124940, loss_ce: 0.072204
2022-01-10 09:56:21,377 iteration 519 : loss : 0.095151, loss_ce: 0.035267
2022-01-10 09:56:22,911 iteration 520 : loss : 0.110153, loss_ce: 0.054793
2022-01-10 09:56:24,583 iteration 521 : loss : 0.181027, loss_ce: 0.050771
2022-01-10 09:56:26,129 iteration 522 : loss : 0.064301, loss_ce: 0.025915
2022-01-10 09:56:27,747 iteration 523 : loss : 0.076626, loss_ce: 0.032313
2022-01-10 09:56:29,204 iteration 524 : loss : 0.076723, loss_ce: 0.034268
2022-01-10 09:56:30,769 iteration 525 : loss : 0.064478, loss_ce: 0.028422
2022-01-10 09:56:32,372 iteration 526 : loss : 0.092723, loss_ce: 0.035657
2022-01-10 09:56:33,938 iteration 527 : loss : 0.093138, loss_ce: 0.037373
  8%|██▎                           | 31/400 [15:13<3:03:14, 29.79s/it]2022-01-10 09:56:35,622 iteration 528 : loss : 0.059826, loss_ce: 0.023910
2022-01-10 09:56:37,206 iteration 529 : loss : 0.080359, loss_ce: 0.039677
2022-01-10 09:56:38,761 iteration 530 : loss : 0.058043, loss_ce: 0.022713
2022-01-10 09:56:40,392 iteration 531 : loss : 0.169195, loss_ce: 0.068995
2022-01-10 09:56:41,899 iteration 532 : loss : 0.102300, loss_ce: 0.031060
2022-01-10 09:56:43,506 iteration 533 : loss : 0.076291, loss_ce: 0.035235
2022-01-10 09:56:45,095 iteration 534 : loss : 0.105735, loss_ce: 0.055218
2022-01-10 09:56:46,679 iteration 535 : loss : 0.086665, loss_ce: 0.036937
2022-01-10 09:56:48,191 iteration 536 : loss : 0.116899, loss_ce: 0.066982
2022-01-10 09:56:49,760 iteration 537 : loss : 0.085245, loss_ce: 0.036968
2022-01-10 09:56:51,321 iteration 538 : loss : 0.060116, loss_ce: 0.024561
2022-01-10 09:56:52,862 iteration 539 : loss : 0.106938, loss_ce: 0.045673
2022-01-10 09:56:54,378 iteration 540 : loss : 0.101018, loss_ce: 0.047926
2022-01-10 09:56:55,943 iteration 541 : loss : 0.094823, loss_ce: 0.043554
2022-01-10 09:56:57,454 iteration 542 : loss : 0.080507, loss_ce: 0.030323
2022-01-10 09:56:58,982 iteration 543 : loss : 0.129525, loss_ce: 0.040387
2022-01-10 09:57:00,550 iteration 544 : loss : 0.088074, loss_ce: 0.029526
  8%|██▍                           | 32/400 [15:39<2:56:53, 28.84s/it]2022-01-10 09:57:02,180 iteration 545 : loss : 0.125891, loss_ce: 0.054989
2022-01-10 09:57:03,771 iteration 546 : loss : 0.083352, loss_ce: 0.032941
2022-01-10 09:57:05,351 iteration 547 : loss : 0.113193, loss_ce: 0.046535
2022-01-10 09:57:06,856 iteration 548 : loss : 0.088623, loss_ce: 0.037322
2022-01-10 09:57:08,438 iteration 549 : loss : 0.095673, loss_ce: 0.041076
2022-01-10 09:57:10,116 iteration 550 : loss : 0.126938, loss_ce: 0.063232
2022-01-10 09:57:11,725 iteration 551 : loss : 0.121654, loss_ce: 0.044070
2022-01-10 09:57:13,361 iteration 552 : loss : 0.096145, loss_ce: 0.038312
2022-01-10 09:57:15,005 iteration 553 : loss : 0.095580, loss_ce: 0.048949
2022-01-10 09:57:16,521 iteration 554 : loss : 0.071787, loss_ce: 0.028472
2022-01-10 09:57:18,123 iteration 555 : loss : 0.088966, loss_ce: 0.044825
2022-01-10 09:57:19,790 iteration 556 : loss : 0.088475, loss_ce: 0.033232
2022-01-10 09:57:21,366 iteration 557 : loss : 0.124355, loss_ce: 0.063099
2022-01-10 09:57:22,902 iteration 558 : loss : 0.100127, loss_ce: 0.038064
2022-01-10 09:57:24,499 iteration 559 : loss : 0.138690, loss_ce: 0.055223
2022-01-10 09:57:26,121 iteration 560 : loss : 0.131509, loss_ce: 0.053775
2022-01-10 09:57:27,657 iteration 561 : loss : 0.126391, loss_ce: 0.036720
  8%|██▍                           | 33/400 [16:06<2:53:13, 28.32s/it]2022-01-10 09:57:29,272 iteration 562 : loss : 0.121584, loss_ce: 0.051493
2022-01-10 09:57:30,871 iteration 563 : loss : 0.085962, loss_ce: 0.038183
2022-01-10 09:57:32,514 iteration 564 : loss : 0.111034, loss_ce: 0.056688
2022-01-10 09:57:34,122 iteration 565 : loss : 0.165381, loss_ce: 0.087862
2022-01-10 09:57:35,618 iteration 566 : loss : 0.089360, loss_ce: 0.034739
2022-01-10 09:57:37,244 iteration 567 : loss : 0.119353, loss_ce: 0.041539
2022-01-10 09:57:38,797 iteration 568 : loss : 0.096393, loss_ce: 0.039017
2022-01-10 09:57:40,370 iteration 569 : loss : 0.080504, loss_ce: 0.028560
2022-01-10 09:57:42,034 iteration 570 : loss : 0.105097, loss_ce: 0.042729
2022-01-10 09:57:43,667 iteration 571 : loss : 0.092709, loss_ce: 0.036095
2022-01-10 09:57:45,287 iteration 572 : loss : 0.090187, loss_ce: 0.031772
2022-01-10 09:57:46,876 iteration 573 : loss : 0.098230, loss_ce: 0.036715
2022-01-10 09:57:48,439 iteration 574 : loss : 0.105225, loss_ce: 0.046878
2022-01-10 09:57:50,067 iteration 575 : loss : 0.095629, loss_ce: 0.039869
2022-01-10 09:57:51,655 iteration 576 : loss : 0.069627, loss_ce: 0.035835
2022-01-10 09:57:53,266 iteration 577 : loss : 0.151597, loss_ce: 0.048907
2022-01-10 09:57:54,870 iteration 578 : loss : 0.105932, loss_ce: 0.036935
  8%|██▌                           | 34/400 [16:33<2:50:42, 27.98s/it]2022-01-10 09:57:56,499 iteration 579 : loss : 0.074906, loss_ce: 0.025793
2022-01-10 09:57:58,072 iteration 580 : loss : 0.068610, loss_ce: 0.030194
2022-01-10 09:57:59,669 iteration 581 : loss : 0.076137, loss_ce: 0.036495
2022-01-10 09:58:01,276 iteration 582 : loss : 0.079305, loss_ce: 0.030975
2022-01-10 09:58:02,831 iteration 583 : loss : 0.127610, loss_ce: 0.061305
2022-01-10 09:58:04,305 iteration 584 : loss : 0.067432, loss_ce: 0.029169
2022-01-10 09:58:05,826 iteration 585 : loss : 0.098882, loss_ce: 0.039243
2022-01-10 09:58:07,446 iteration 586 : loss : 0.121932, loss_ce: 0.070420
2022-01-10 09:58:09,016 iteration 587 : loss : 0.073085, loss_ce: 0.026725
2022-01-10 09:58:10,642 iteration 588 : loss : 0.073198, loss_ce: 0.024024
2022-01-10 09:58:12,232 iteration 589 : loss : 0.079562, loss_ce: 0.033329
2022-01-10 09:58:13,779 iteration 590 : loss : 0.129184, loss_ce: 0.078634
2022-01-10 09:58:15,350 iteration 591 : loss : 0.072955, loss_ce: 0.031387
2022-01-10 09:58:16,912 iteration 592 : loss : 0.091730, loss_ce: 0.035898
2022-01-10 09:58:18,539 iteration 593 : loss : 0.095742, loss_ce: 0.038204
2022-01-10 09:58:20,183 iteration 594 : loss : 0.104786, loss_ce: 0.053050
2022-01-10 09:58:20,183 Training Data Eval:
2022-01-10 09:58:28,137   Average segmentation loss on training set: 0.0731
2022-01-10 09:58:28,138 Validation Data Eval:
2022-01-10 09:58:30,879   Average segmentation loss on validation set: 0.1072
2022-01-10 09:58:36,610 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 09:58:38,129 iteration 595 : loss : 0.085385, loss_ce: 0.031302
  9%|██▋                           | 35/400 [17:17<3:18:08, 32.57s/it]2022-01-10 09:58:39,695 iteration 596 : loss : 0.078083, loss_ce: 0.033696
2022-01-10 09:58:41,275 iteration 597 : loss : 0.101199, loss_ce: 0.044460
2022-01-10 09:58:42,873 iteration 598 : loss : 0.086221, loss_ce: 0.034676
2022-01-10 09:58:44,514 iteration 599 : loss : 0.083952, loss_ce: 0.032715
2022-01-10 09:58:46,008 iteration 600 : loss : 0.067669, loss_ce: 0.035584
2022-01-10 09:58:47,580 iteration 601 : loss : 0.079694, loss_ce: 0.028248
2022-01-10 09:58:49,205 iteration 602 : loss : 0.068354, loss_ce: 0.029417
2022-01-10 09:58:50,690 iteration 603 : loss : 0.077902, loss_ce: 0.033736
2022-01-10 09:58:52,242 iteration 604 : loss : 0.089447, loss_ce: 0.036565
2022-01-10 09:58:53,818 iteration 605 : loss : 0.081567, loss_ce: 0.034772
2022-01-10 09:58:55,358 iteration 606 : loss : 0.110433, loss_ce: 0.043802
2022-01-10 09:58:56,915 iteration 607 : loss : 0.085383, loss_ce: 0.031958
2022-01-10 09:58:58,596 iteration 608 : loss : 0.063450, loss_ce: 0.023078
2022-01-10 09:59:00,156 iteration 609 : loss : 0.070217, loss_ce: 0.028152
2022-01-10 09:59:01,788 iteration 610 : loss : 0.094127, loss_ce: 0.037706
2022-01-10 09:59:03,358 iteration 611 : loss : 0.092116, loss_ce: 0.032809
2022-01-10 09:59:04,961 iteration 612 : loss : 0.076011, loss_ce: 0.033914
  9%|██▋                           | 36/400 [17:44<3:07:07, 30.84s/it]2022-01-10 09:59:06,613 iteration 613 : loss : 0.110382, loss_ce: 0.054142
2022-01-10 09:59:08,310 iteration 614 : loss : 0.109281, loss_ce: 0.045389
2022-01-10 09:59:09,893 iteration 615 : loss : 0.080189, loss_ce: 0.030093
2022-01-10 09:59:11,413 iteration 616 : loss : 0.110880, loss_ce: 0.073932
2022-01-10 09:59:12,991 iteration 617 : loss : 0.113764, loss_ce: 0.056279
2022-01-10 09:59:14,519 iteration 618 : loss : 0.097474, loss_ce: 0.042202
2022-01-10 09:59:16,075 iteration 619 : loss : 0.088525, loss_ce: 0.030325
2022-01-10 09:59:17,746 iteration 620 : loss : 0.141414, loss_ce: 0.065653
2022-01-10 09:59:19,292 iteration 621 : loss : 0.094650, loss_ce: 0.038453
2022-01-10 09:59:20,894 iteration 622 : loss : 0.077349, loss_ce: 0.035598
2022-01-10 09:59:22,546 iteration 623 : loss : 0.126189, loss_ce: 0.039272
2022-01-10 09:59:24,109 iteration 624 : loss : 0.104845, loss_ce: 0.041784
2022-01-10 09:59:25,628 iteration 625 : loss : 0.113356, loss_ce: 0.046405
2022-01-10 09:59:27,144 iteration 626 : loss : 0.062786, loss_ce: 0.025756
2022-01-10 09:59:28,694 iteration 627 : loss : 0.077349, loss_ce: 0.027065
2022-01-10 09:59:30,242 iteration 628 : loss : 0.117268, loss_ce: 0.061531
2022-01-10 09:59:31,847 iteration 629 : loss : 0.081913, loss_ce: 0.028799
  9%|██▊                           | 37/400 [18:10<2:59:26, 29.66s/it]2022-01-10 09:59:33,470 iteration 630 : loss : 0.060702, loss_ce: 0.023190
2022-01-10 09:59:35,089 iteration 631 : loss : 0.112247, loss_ce: 0.061863
2022-01-10 09:59:36,679 iteration 632 : loss : 0.102518, loss_ce: 0.040526
2022-01-10 09:59:38,292 iteration 633 : loss : 0.075218, loss_ce: 0.029541
2022-01-10 09:59:39,926 iteration 634 : loss : 0.087091, loss_ce: 0.040541
2022-01-10 09:59:41,498 iteration 635 : loss : 0.139226, loss_ce: 0.046419
2022-01-10 09:59:43,158 iteration 636 : loss : 0.081444, loss_ce: 0.036911
2022-01-10 09:59:44,691 iteration 637 : loss : 0.060855, loss_ce: 0.021379
2022-01-10 09:59:46,238 iteration 638 : loss : 0.125600, loss_ce: 0.058667
2022-01-10 09:59:47,866 iteration 639 : loss : 0.076417, loss_ce: 0.033801
2022-01-10 09:59:49,426 iteration 640 : loss : 0.080997, loss_ce: 0.031017
2022-01-10 09:59:51,022 iteration 641 : loss : 0.078798, loss_ce: 0.031215
2022-01-10 09:59:52,628 iteration 642 : loss : 0.065466, loss_ce: 0.024968
2022-01-10 09:59:54,211 iteration 643 : loss : 0.076142, loss_ce: 0.032237
2022-01-10 09:59:55,787 iteration 644 : loss : 0.063689, loss_ce: 0.026803
2022-01-10 09:59:57,378 iteration 645 : loss : 0.091006, loss_ce: 0.036708
2022-01-10 09:59:58,953 iteration 646 : loss : 0.096375, loss_ce: 0.044558
 10%|██▊                           | 38/400 [18:38<2:54:19, 28.89s/it]2022-01-10 10:00:00,581 iteration 647 : loss : 0.082034, loss_ce: 0.025231
2022-01-10 10:00:02,118 iteration 648 : loss : 0.085543, loss_ce: 0.046157
2022-01-10 10:00:03,655 iteration 649 : loss : 0.068402, loss_ce: 0.029208
2022-01-10 10:00:05,281 iteration 650 : loss : 0.161470, loss_ce: 0.058146
2022-01-10 10:00:06,892 iteration 651 : loss : 0.066346, loss_ce: 0.023544
2022-01-10 10:00:08,414 iteration 652 : loss : 0.080196, loss_ce: 0.034512
2022-01-10 10:00:09,964 iteration 653 : loss : 0.065646, loss_ce: 0.028649
2022-01-10 10:00:11,542 iteration 654 : loss : 0.079685, loss_ce: 0.033581
2022-01-10 10:00:13,047 iteration 655 : loss : 0.089877, loss_ce: 0.036420
2022-01-10 10:00:14,678 iteration 656 : loss : 0.116607, loss_ce: 0.048299
2022-01-10 10:00:16,251 iteration 657 : loss : 0.095252, loss_ce: 0.039543
2022-01-10 10:00:17,818 iteration 658 : loss : 0.104479, loss_ce: 0.038359
2022-01-10 10:00:19,442 iteration 659 : loss : 0.106260, loss_ce: 0.044227
2022-01-10 10:00:21,050 iteration 660 : loss : 0.086181, loss_ce: 0.036129
2022-01-10 10:00:22,641 iteration 661 : loss : 0.067375, loss_ce: 0.031089
2022-01-10 10:00:24,244 iteration 662 : loss : 0.093252, loss_ce: 0.038233
2022-01-10 10:00:25,824 iteration 663 : loss : 0.097237, loss_ce: 0.042293
 10%|██▉                           | 39/400 [19:04<2:50:11, 28.29s/it]2022-01-10 10:00:27,426 iteration 664 : loss : 0.104984, loss_ce: 0.053521
2022-01-10 10:00:29,049 iteration 665 : loss : 0.067698, loss_ce: 0.024286
2022-01-10 10:00:30,651 iteration 666 : loss : 0.092857, loss_ce: 0.029761
2022-01-10 10:00:32,301 iteration 667 : loss : 0.097101, loss_ce: 0.040216
2022-01-10 10:00:33,877 iteration 668 : loss : 0.058292, loss_ce: 0.022265
2022-01-10 10:00:35,526 iteration 669 : loss : 0.088840, loss_ce: 0.038847
2022-01-10 10:00:37,166 iteration 670 : loss : 0.108892, loss_ce: 0.047146
2022-01-10 10:00:38,775 iteration 671 : loss : 0.091677, loss_ce: 0.040744
2022-01-10 10:00:40,279 iteration 672 : loss : 0.074712, loss_ce: 0.032812
2022-01-10 10:00:41,839 iteration 673 : loss : 0.099289, loss_ce: 0.037859
2022-01-10 10:00:43,563 iteration 674 : loss : 0.082880, loss_ce: 0.042876
2022-01-10 10:00:45,156 iteration 675 : loss : 0.099797, loss_ce: 0.035954
2022-01-10 10:00:46,730 iteration 676 : loss : 0.077733, loss_ce: 0.029439
2022-01-10 10:00:48,435 iteration 677 : loss : 0.069505, loss_ce: 0.031283
2022-01-10 10:00:50,058 iteration 678 : loss : 0.109671, loss_ce: 0.029700
2022-01-10 10:00:51,597 iteration 679 : loss : 0.086516, loss_ce: 0.039205
2022-01-10 10:00:51,597 Training Data Eval:
2022-01-10 10:00:59,574   Average segmentation loss on training set: 0.0750
2022-01-10 10:00:59,575 Validation Data Eval:
2022-01-10 10:01:02,333   Average segmentation loss on validation set: 0.1205
2022-01-10 10:01:03,886 iteration 680 : loss : 0.079369, loss_ce: 0.036386
 10%|███                           | 40/400 [19:42<3:07:19, 31.22s/it]2022-01-10 10:01:05,680 iteration 681 : loss : 0.085788, loss_ce: 0.032497
2022-01-10 10:01:07,266 iteration 682 : loss : 0.085627, loss_ce: 0.033768
2022-01-10 10:01:08,813 iteration 683 : loss : 0.081040, loss_ce: 0.026846
2022-01-10 10:01:10,468 iteration 684 : loss : 0.075584, loss_ce: 0.032701
2022-01-10 10:01:12,128 iteration 685 : loss : 0.081781, loss_ce: 0.032995
2022-01-10 10:01:13,762 iteration 686 : loss : 0.089069, loss_ce: 0.041317
2022-01-10 10:01:15,384 iteration 687 : loss : 0.080896, loss_ce: 0.037816
2022-01-10 10:01:17,038 iteration 688 : loss : 0.088735, loss_ce: 0.038850
2022-01-10 10:01:18,663 iteration 689 : loss : 0.088486, loss_ce: 0.048998
2022-01-10 10:01:20,251 iteration 690 : loss : 0.094202, loss_ce: 0.037358
2022-01-10 10:01:21,884 iteration 691 : loss : 0.087129, loss_ce: 0.028373
2022-01-10 10:01:23,401 iteration 692 : loss : 0.065150, loss_ce: 0.026617
2022-01-10 10:01:24,963 iteration 693 : loss : 0.069548, loss_ce: 0.030354
2022-01-10 10:01:26,550 iteration 694 : loss : 0.147521, loss_ce: 0.051598
2022-01-10 10:01:28,127 iteration 695 : loss : 0.105102, loss_ce: 0.042381
2022-01-10 10:01:29,675 iteration 696 : loss : 0.155140, loss_ce: 0.037539
2022-01-10 10:01:31,166 iteration 697 : loss : 0.057551, loss_ce: 0.024221
 10%|███                           | 41/400 [20:10<2:59:43, 30.04s/it]2022-01-10 10:01:32,819 iteration 698 : loss : 0.076848, loss_ce: 0.037753
2022-01-10 10:01:34,426 iteration 699 : loss : 0.069804, loss_ce: 0.029544
2022-01-10 10:01:35,973 iteration 700 : loss : 0.092443, loss_ce: 0.037395
2022-01-10 10:01:37,527 iteration 701 : loss : 0.115640, loss_ce: 0.042957
2022-01-10 10:01:39,158 iteration 702 : loss : 0.065274, loss_ce: 0.026156
2022-01-10 10:01:40,761 iteration 703 : loss : 0.129708, loss_ce: 0.042424
2022-01-10 10:01:42,437 iteration 704 : loss : 0.111255, loss_ce: 0.038056
2022-01-10 10:01:44,080 iteration 705 : loss : 0.076732, loss_ce: 0.041382
2022-01-10 10:01:45,731 iteration 706 : loss : 0.141508, loss_ce: 0.043621
2022-01-10 10:01:47,304 iteration 707 : loss : 0.077400, loss_ce: 0.035659
2022-01-10 10:01:48,912 iteration 708 : loss : 0.089465, loss_ce: 0.030096
2022-01-10 10:01:50,526 iteration 709 : loss : 0.094043, loss_ce: 0.036598
2022-01-10 10:01:52,054 iteration 710 : loss : 0.071963, loss_ce: 0.031370
2022-01-10 10:01:53,684 iteration 711 : loss : 0.069608, loss_ce: 0.029134
2022-01-10 10:01:55,255 iteration 712 : loss : 0.088557, loss_ce: 0.030984
2022-01-10 10:01:56,772 iteration 713 : loss : 0.065226, loss_ce: 0.027247
2022-01-10 10:01:58,296 iteration 714 : loss : 0.073241, loss_ce: 0.026739
 10%|███▏                          | 42/400 [20:37<2:54:00, 29.16s/it]2022-01-10 10:01:59,997 iteration 715 : loss : 0.082559, loss_ce: 0.038302
2022-01-10 10:02:01,566 iteration 716 : loss : 0.073304, loss_ce: 0.032472
2022-01-10 10:02:03,125 iteration 717 : loss : 0.103150, loss_ce: 0.036320
2022-01-10 10:02:04,695 iteration 718 : loss : 0.113351, loss_ce: 0.045950
2022-01-10 10:02:06,356 iteration 719 : loss : 0.080083, loss_ce: 0.029833
2022-01-10 10:02:07,906 iteration 720 : loss : 0.103290, loss_ce: 0.032529
2022-01-10 10:02:09,530 iteration 721 : loss : 0.076250, loss_ce: 0.029494
2022-01-10 10:02:11,124 iteration 722 : loss : 0.082494, loss_ce: 0.037154
2022-01-10 10:02:12,669 iteration 723 : loss : 0.069457, loss_ce: 0.025185
2022-01-10 10:02:14,364 iteration 724 : loss : 0.083626, loss_ce: 0.038856
2022-01-10 10:02:15,889 iteration 725 : loss : 0.061649, loss_ce: 0.023151
2022-01-10 10:02:17,484 iteration 726 : loss : 0.136512, loss_ce: 0.042642
2022-01-10 10:02:19,115 iteration 727 : loss : 0.095461, loss_ce: 0.026666
2022-01-10 10:02:20,867 iteration 728 : loss : 0.077433, loss_ce: 0.034926
2022-01-10 10:02:22,348 iteration 729 : loss : 0.050239, loss_ce: 0.020635
2022-01-10 10:02:23,940 iteration 730 : loss : 0.083205, loss_ce: 0.035840
2022-01-10 10:02:25,532 iteration 731 : loss : 0.090590, loss_ce: 0.036601
 11%|███▏                          | 43/400 [21:04<2:50:04, 28.58s/it]2022-01-10 10:02:27,278 iteration 732 : loss : 0.083323, loss_ce: 0.038332
2022-01-10 10:02:28,901 iteration 733 : loss : 0.123005, loss_ce: 0.051582
2022-01-10 10:02:30,440 iteration 734 : loss : 0.085682, loss_ce: 0.029122
2022-01-10 10:02:32,017 iteration 735 : loss : 0.076997, loss_ce: 0.030209
2022-01-10 10:02:33,633 iteration 736 : loss : 0.094353, loss_ce: 0.033543
2022-01-10 10:02:35,294 iteration 737 : loss : 0.068833, loss_ce: 0.025140
2022-01-10 10:02:36,889 iteration 738 : loss : 0.044795, loss_ce: 0.018879
2022-01-10 10:02:38,457 iteration 739 : loss : 0.099987, loss_ce: 0.037578
2022-01-10 10:02:40,036 iteration 740 : loss : 0.091138, loss_ce: 0.043392
2022-01-10 10:02:41,633 iteration 741 : loss : 0.079594, loss_ce: 0.030728
2022-01-10 10:02:43,300 iteration 742 : loss : 0.159256, loss_ce: 0.057961
2022-01-10 10:02:44,863 iteration 743 : loss : 0.072686, loss_ce: 0.029009
2022-01-10 10:02:46,421 iteration 744 : loss : 0.087496, loss_ce: 0.030841
2022-01-10 10:02:47,969 iteration 745 : loss : 0.062599, loss_ce: 0.029416
2022-01-10 10:02:49,554 iteration 746 : loss : 0.166179, loss_ce: 0.099491
2022-01-10 10:02:51,189 iteration 747 : loss : 0.102962, loss_ce: 0.030942
2022-01-10 10:02:52,680 iteration 748 : loss : 0.058124, loss_ce: 0.024050
 11%|███▎                          | 44/400 [21:31<2:47:04, 28.16s/it]2022-01-10 10:02:54,355 iteration 749 : loss : 0.064409, loss_ce: 0.027961
2022-01-10 10:02:55,990 iteration 750 : loss : 0.064659, loss_ce: 0.026735
2022-01-10 10:02:57,513 iteration 751 : loss : 0.058534, loss_ce: 0.024193
2022-01-10 10:02:59,118 iteration 752 : loss : 0.092289, loss_ce: 0.033786
2022-01-10 10:03:00,815 iteration 753 : loss : 0.084205, loss_ce: 0.037534
2022-01-10 10:03:02,391 iteration 754 : loss : 0.086777, loss_ce: 0.037314
2022-01-10 10:03:03,981 iteration 755 : loss : 0.054306, loss_ce: 0.021495
2022-01-10 10:03:05,515 iteration 756 : loss : 0.072692, loss_ce: 0.031353
2022-01-10 10:03:07,064 iteration 757 : loss : 0.069499, loss_ce: 0.025025
2022-01-10 10:03:08,654 iteration 758 : loss : 0.063763, loss_ce: 0.024746
2022-01-10 10:03:10,155 iteration 759 : loss : 0.048700, loss_ce: 0.021163
2022-01-10 10:03:11,769 iteration 760 : loss : 0.116231, loss_ce: 0.038343
2022-01-10 10:03:13,399 iteration 761 : loss : 0.058746, loss_ce: 0.019253
2022-01-10 10:03:15,016 iteration 762 : loss : 0.093298, loss_ce: 0.045379
2022-01-10 10:03:16,591 iteration 763 : loss : 0.068295, loss_ce: 0.030332
2022-01-10 10:03:18,128 iteration 764 : loss : 0.066002, loss_ce: 0.028048
2022-01-10 10:03:18,128 Training Data Eval:
2022-01-10 10:03:26,102   Average segmentation loss on training set: 0.0514
2022-01-10 10:03:26,103 Validation Data Eval:
2022-01-10 10:03:28,844   Average segmentation loss on validation set: 0.0936
2022-01-10 10:03:34,686 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 10:03:36,220 iteration 765 : loss : 0.088508, loss_ce: 0.041179
 11%|███▍                          | 45/400 [22:15<3:13:53, 32.77s/it]2022-01-10 10:03:37,802 iteration 766 : loss : 0.074359, loss_ce: 0.025067
2022-01-10 10:03:39,618 iteration 767 : loss : 0.093221, loss_ce: 0.042688
2022-01-10 10:03:41,105 iteration 768 : loss : 0.079411, loss_ce: 0.029426
2022-01-10 10:03:42,695 iteration 769 : loss : 0.159516, loss_ce: 0.031503
2022-01-10 10:03:44,220 iteration 770 : loss : 0.051501, loss_ce: 0.021783
2022-01-10 10:03:45,869 iteration 771 : loss : 0.106516, loss_ce: 0.036872
2022-01-10 10:03:47,518 iteration 772 : loss : 0.059180, loss_ce: 0.025982
2022-01-10 10:03:49,122 iteration 773 : loss : 0.093186, loss_ce: 0.036375
2022-01-10 10:03:50,705 iteration 774 : loss : 0.067440, loss_ce: 0.031642
2022-01-10 10:03:52,270 iteration 775 : loss : 0.066636, loss_ce: 0.026447
2022-01-10 10:03:53,759 iteration 776 : loss : 0.069367, loss_ce: 0.029887
2022-01-10 10:03:55,394 iteration 777 : loss : 0.080822, loss_ce: 0.034860
2022-01-10 10:03:56,991 iteration 778 : loss : 0.049166, loss_ce: 0.019041
2022-01-10 10:03:58,580 iteration 779 : loss : 0.063376, loss_ce: 0.033738
2022-01-10 10:04:00,107 iteration 780 : loss : 0.067506, loss_ce: 0.024297
2022-01-10 10:04:01,745 iteration 781 : loss : 0.078067, loss_ce: 0.034179
2022-01-10 10:04:03,377 iteration 782 : loss : 0.083278, loss_ce: 0.037063
 12%|███▍                          | 46/400 [22:42<3:03:23, 31.08s/it]2022-01-10 10:04:05,074 iteration 783 : loss : 0.060487, loss_ce: 0.027029
2022-01-10 10:04:06,668 iteration 784 : loss : 0.077039, loss_ce: 0.030028
2022-01-10 10:04:08,299 iteration 785 : loss : 0.064295, loss_ce: 0.028081
2022-01-10 10:04:09,887 iteration 786 : loss : 0.070233, loss_ce: 0.023872
2022-01-10 10:04:11,489 iteration 787 : loss : 0.056441, loss_ce: 0.024534
2022-01-10 10:04:13,110 iteration 788 : loss : 0.090129, loss_ce: 0.038859
2022-01-10 10:04:14,837 iteration 789 : loss : 0.100338, loss_ce: 0.054161
2022-01-10 10:04:16,491 iteration 790 : loss : 0.067524, loss_ce: 0.026431
2022-01-10 10:04:18,062 iteration 791 : loss : 0.085853, loss_ce: 0.033381
2022-01-10 10:04:19,686 iteration 792 : loss : 0.078418, loss_ce: 0.033475
2022-01-10 10:04:21,180 iteration 793 : loss : 0.109150, loss_ce: 0.043596
2022-01-10 10:04:22,754 iteration 794 : loss : 0.050860, loss_ce: 0.020569
2022-01-10 10:04:24,349 iteration 795 : loss : 0.065106, loss_ce: 0.027267
2022-01-10 10:04:25,938 iteration 796 : loss : 0.077038, loss_ce: 0.032506
2022-01-10 10:04:27,481 iteration 797 : loss : 0.063516, loss_ce: 0.027855
2022-01-10 10:04:29,029 iteration 798 : loss : 0.142828, loss_ce: 0.044308
2022-01-10 10:04:30,659 iteration 799 : loss : 0.052138, loss_ce: 0.020925
 12%|███▌                          | 47/400 [23:09<2:56:10, 29.95s/it]2022-01-10 10:04:32,369 iteration 800 : loss : 0.072591, loss_ce: 0.023234
2022-01-10 10:04:33,920 iteration 801 : loss : 0.072435, loss_ce: 0.027176
2022-01-10 10:04:35,497 iteration 802 : loss : 0.097734, loss_ce: 0.043019
2022-01-10 10:04:37,164 iteration 803 : loss : 0.104170, loss_ce: 0.064186
2022-01-10 10:04:38,690 iteration 804 : loss : 0.068813, loss_ce: 0.029642
2022-01-10 10:04:40,211 iteration 805 : loss : 0.126342, loss_ce: 0.026262
2022-01-10 10:04:41,803 iteration 806 : loss : 0.089038, loss_ce: 0.034959
2022-01-10 10:04:43,485 iteration 807 : loss : 0.074434, loss_ce: 0.028493
2022-01-10 10:04:45,125 iteration 808 : loss : 0.084029, loss_ce: 0.030716
2022-01-10 10:04:46,704 iteration 809 : loss : 0.111148, loss_ce: 0.032959
2022-01-10 10:04:48,351 iteration 810 : loss : 0.077127, loss_ce: 0.026950
2022-01-10 10:04:49,907 iteration 811 : loss : 0.042191, loss_ce: 0.016862
2022-01-10 10:04:51,571 iteration 812 : loss : 0.108450, loss_ce: 0.038284
2022-01-10 10:04:53,151 iteration 813 : loss : 0.062602, loss_ce: 0.025916
2022-01-10 10:04:54,755 iteration 814 : loss : 0.078871, loss_ce: 0.035180
2022-01-10 10:04:56,395 iteration 815 : loss : 0.063803, loss_ce: 0.024859
2022-01-10 10:04:57,862 iteration 816 : loss : 0.062137, loss_ce: 0.021328
 12%|███▌                          | 48/400 [23:36<2:50:50, 29.12s/it]2022-01-10 10:04:59,488 iteration 817 : loss : 0.092326, loss_ce: 0.033163
2022-01-10 10:05:01,107 iteration 818 : loss : 0.079199, loss_ce: 0.030517
2022-01-10 10:05:02,692 iteration 819 : loss : 0.069613, loss_ce: 0.026055
2022-01-10 10:05:04,388 iteration 820 : loss : 0.073181, loss_ce: 0.028405
2022-01-10 10:05:05,887 iteration 821 : loss : 0.096535, loss_ce: 0.036321
2022-01-10 10:05:07,507 iteration 822 : loss : 0.069753, loss_ce: 0.026646
2022-01-10 10:05:09,089 iteration 823 : loss : 0.077532, loss_ce: 0.027123
2022-01-10 10:05:10,647 iteration 824 : loss : 0.087966, loss_ce: 0.040030
2022-01-10 10:05:12,230 iteration 825 : loss : 0.055007, loss_ce: 0.017386
2022-01-10 10:05:13,845 iteration 826 : loss : 0.081402, loss_ce: 0.036717
2022-01-10 10:05:15,412 iteration 827 : loss : 0.134250, loss_ce: 0.040481
2022-01-10 10:05:17,046 iteration 828 : loss : 0.092057, loss_ce: 0.038563
2022-01-10 10:05:18,628 iteration 829 : loss : 0.082916, loss_ce: 0.026447
2022-01-10 10:05:20,320 iteration 830 : loss : 0.086639, loss_ce: 0.029973
2022-01-10 10:05:21,927 iteration 831 : loss : 0.080741, loss_ce: 0.029731
2022-01-10 10:05:23,499 iteration 832 : loss : 0.052045, loss_ce: 0.020999
2022-01-10 10:05:25,041 iteration 833 : loss : 0.061685, loss_ce: 0.031581
 12%|███▋                          | 49/400 [24:04<2:46:57, 28.54s/it]2022-01-10 10:05:26,651 iteration 834 : loss : 0.088091, loss_ce: 0.030403
2022-01-10 10:05:28,270 iteration 835 : loss : 0.076001, loss_ce: 0.029355
2022-01-10 10:05:29,878 iteration 836 : loss : 0.062058, loss_ce: 0.026185
2022-01-10 10:05:31,447 iteration 837 : loss : 0.074268, loss_ce: 0.032832
2022-01-10 10:05:33,008 iteration 838 : loss : 0.078220, loss_ce: 0.033648
2022-01-10 10:05:34,671 iteration 839 : loss : 0.065013, loss_ce: 0.021837
2022-01-10 10:05:36,284 iteration 840 : loss : 0.086218, loss_ce: 0.032050
2022-01-10 10:05:37,885 iteration 841 : loss : 0.074306, loss_ce: 0.039779
2022-01-10 10:05:39,612 iteration 842 : loss : 0.093765, loss_ce: 0.041773
2022-01-10 10:05:41,183 iteration 843 : loss : 0.043745, loss_ce: 0.016299
2022-01-10 10:05:42,814 iteration 844 : loss : 0.102231, loss_ce: 0.040796
2022-01-10 10:05:44,371 iteration 845 : loss : 0.055415, loss_ce: 0.023647
2022-01-10 10:05:45,965 iteration 846 : loss : 0.100786, loss_ce: 0.035158
2022-01-10 10:05:47,603 iteration 847 : loss : 0.069845, loss_ce: 0.028772
2022-01-10 10:05:49,108 iteration 848 : loss : 0.054135, loss_ce: 0.020090
2022-01-10 10:05:50,729 iteration 849 : loss : 0.064745, loss_ce: 0.030277
2022-01-10 10:05:50,730 Training Data Eval:
2022-01-10 10:05:58,710   Average segmentation loss on training set: 0.1024
2022-01-10 10:05:58,710 Validation Data Eval:
2022-01-10 10:06:01,461   Average segmentation loss on validation set: 0.1125
2022-01-10 10:06:03,121 iteration 850 : loss : 0.071528, loss_ce: 0.023715
 12%|███▊                          | 50/400 [24:42<3:03:10, 31.40s/it]2022-01-10 10:06:04,801 iteration 851 : loss : 0.060218, loss_ce: 0.028084
2022-01-10 10:06:06,433 iteration 852 : loss : 0.064068, loss_ce: 0.019685
2022-01-10 10:06:08,072 iteration 853 : loss : 0.084450, loss_ce: 0.033732
2022-01-10 10:06:09,611 iteration 854 : loss : 0.067918, loss_ce: 0.026544
2022-01-10 10:06:11,104 iteration 855 : loss : 0.059980, loss_ce: 0.023023
2022-01-10 10:06:12,642 iteration 856 : loss : 0.059634, loss_ce: 0.020205
2022-01-10 10:06:14,319 iteration 857 : loss : 0.061122, loss_ce: 0.022446
2022-01-10 10:06:15,901 iteration 858 : loss : 0.092232, loss_ce: 0.034174
2022-01-10 10:06:17,581 iteration 859 : loss : 0.101668, loss_ce: 0.038558
2022-01-10 10:06:19,218 iteration 860 : loss : 0.093293, loss_ce: 0.033494
2022-01-10 10:06:20,801 iteration 861 : loss : 0.075160, loss_ce: 0.027147
2022-01-10 10:06:22,389 iteration 862 : loss : 0.057656, loss_ce: 0.014311
2022-01-10 10:06:23,897 iteration 863 : loss : 0.089763, loss_ce: 0.041821
2022-01-10 10:06:25,472 iteration 864 : loss : 0.058197, loss_ce: 0.026261
2022-01-10 10:06:26,990 iteration 865 : loss : 0.069653, loss_ce: 0.034833
2022-01-10 10:06:28,520 iteration 866 : loss : 0.047557, loss_ce: 0.019229
2022-01-10 10:06:30,064 iteration 867 : loss : 0.059644, loss_ce: 0.025121
 13%|███▊                          | 51/400 [25:09<2:54:52, 30.07s/it]2022-01-10 10:06:31,760 iteration 868 : loss : 0.068529, loss_ce: 0.028234
2022-01-10 10:06:33,310 iteration 869 : loss : 0.068613, loss_ce: 0.029296
2022-01-10 10:06:34,921 iteration 870 : loss : 0.105090, loss_ce: 0.030045
2022-01-10 10:06:36,531 iteration 871 : loss : 0.066380, loss_ce: 0.025540
2022-01-10 10:06:38,050 iteration 872 : loss : 0.050158, loss_ce: 0.022243
2022-01-10 10:06:39,582 iteration 873 : loss : 0.051656, loss_ce: 0.018766
2022-01-10 10:06:41,197 iteration 874 : loss : 0.068009, loss_ce: 0.036718
2022-01-10 10:06:42,804 iteration 875 : loss : 0.071909, loss_ce: 0.032062
2022-01-10 10:06:44,383 iteration 876 : loss : 0.068945, loss_ce: 0.031167
2022-01-10 10:06:45,906 iteration 877 : loss : 0.045371, loss_ce: 0.016672
2022-01-10 10:06:47,442 iteration 878 : loss : 0.060322, loss_ce: 0.024128
2022-01-10 10:06:49,033 iteration 879 : loss : 0.070252, loss_ce: 0.027629
2022-01-10 10:06:50,578 iteration 880 : loss : 0.050198, loss_ce: 0.019785
2022-01-10 10:06:52,208 iteration 881 : loss : 0.086779, loss_ce: 0.034778
2022-01-10 10:06:53,769 iteration 882 : loss : 0.072796, loss_ce: 0.030869
2022-01-10 10:06:55,330 iteration 883 : loss : 0.107582, loss_ce: 0.036517
2022-01-10 10:06:57,043 iteration 884 : loss : 0.072665, loss_ce: 0.026620
 13%|███▉                          | 52/400 [25:36<2:48:59, 29.14s/it]2022-01-10 10:06:58,765 iteration 885 : loss : 0.078926, loss_ce: 0.033227
2022-01-10 10:07:00,335 iteration 886 : loss : 0.064225, loss_ce: 0.024172
2022-01-10 10:07:01,906 iteration 887 : loss : 0.086279, loss_ce: 0.036932
2022-01-10 10:07:03,501 iteration 888 : loss : 0.049466, loss_ce: 0.019318
2022-01-10 10:07:05,052 iteration 889 : loss : 0.066711, loss_ce: 0.023167
2022-01-10 10:07:06,695 iteration 890 : loss : 0.052713, loss_ce: 0.022819
2022-01-10 10:07:08,314 iteration 891 : loss : 0.091415, loss_ce: 0.045438
2022-01-10 10:07:09,911 iteration 892 : loss : 0.085476, loss_ce: 0.027128
2022-01-10 10:07:11,588 iteration 893 : loss : 0.077255, loss_ce: 0.034662
2022-01-10 10:07:13,191 iteration 894 : loss : 0.062221, loss_ce: 0.030217
2022-01-10 10:07:14,777 iteration 895 : loss : 0.053698, loss_ce: 0.019116
2022-01-10 10:07:16,409 iteration 896 : loss : 0.069949, loss_ce: 0.019166
2022-01-10 10:07:17,923 iteration 897 : loss : 0.048971, loss_ce: 0.018904
2022-01-10 10:07:19,616 iteration 898 : loss : 0.095817, loss_ce: 0.036562
2022-01-10 10:07:21,203 iteration 899 : loss : 0.054264, loss_ce: 0.019219
2022-01-10 10:07:22,817 iteration 900 : loss : 0.088884, loss_ce: 0.035071
2022-01-10 10:07:24,392 iteration 901 : loss : 0.068833, loss_ce: 0.037870
 13%|███▉                          | 53/400 [26:03<2:45:25, 28.60s/it]2022-01-10 10:07:26,044 iteration 902 : loss : 0.047641, loss_ce: 0.018566
2022-01-10 10:07:27,649 iteration 903 : loss : 0.061658, loss_ce: 0.019997
2022-01-10 10:07:29,206 iteration 904 : loss : 0.063434, loss_ce: 0.029899
2022-01-10 10:07:30,733 iteration 905 : loss : 0.144298, loss_ce: 0.042527
2022-01-10 10:07:32,270 iteration 906 : loss : 0.062249, loss_ce: 0.022675
2022-01-10 10:07:33,892 iteration 907 : loss : 0.047245, loss_ce: 0.023425
2022-01-10 10:07:35,417 iteration 908 : loss : 0.070011, loss_ce: 0.039072
2022-01-10 10:07:37,029 iteration 909 : loss : 0.092076, loss_ce: 0.041183
2022-01-10 10:07:38,618 iteration 910 : loss : 0.103206, loss_ce: 0.038532
2022-01-10 10:07:40,219 iteration 911 : loss : 0.079983, loss_ce: 0.026636
2022-01-10 10:07:41,713 iteration 912 : loss : 0.076394, loss_ce: 0.027479
2022-01-10 10:07:43,346 iteration 913 : loss : 0.075000, loss_ce: 0.027446
2022-01-10 10:07:44,900 iteration 914 : loss : 0.050394, loss_ce: 0.022870
2022-01-10 10:07:46,518 iteration 915 : loss : 0.087512, loss_ce: 0.038290
2022-01-10 10:07:48,081 iteration 916 : loss : 0.072252, loss_ce: 0.030884
2022-01-10 10:07:49,659 iteration 917 : loss : 0.081358, loss_ce: 0.032545
2022-01-10 10:07:51,236 iteration 918 : loss : 0.081078, loss_ce: 0.028640
 14%|████                          | 54/400 [26:30<2:41:53, 28.07s/it]2022-01-10 10:07:52,937 iteration 919 : loss : 0.065690, loss_ce: 0.028413
2022-01-10 10:07:54,463 iteration 920 : loss : 0.105595, loss_ce: 0.068402
2022-01-10 10:07:56,139 iteration 921 : loss : 0.050638, loss_ce: 0.022818
2022-01-10 10:07:57,617 iteration 922 : loss : 0.057913, loss_ce: 0.031255
2022-01-10 10:07:59,140 iteration 923 : loss : 0.085092, loss_ce: 0.032478
2022-01-10 10:08:00,706 iteration 924 : loss : 0.054750, loss_ce: 0.018581
2022-01-10 10:08:02,231 iteration 925 : loss : 0.046895, loss_ce: 0.017999
2022-01-10 10:08:03,748 iteration 926 : loss : 0.066169, loss_ce: 0.023006
2022-01-10 10:08:05,330 iteration 927 : loss : 0.062151, loss_ce: 0.027474
2022-01-10 10:08:06,853 iteration 928 : loss : 0.054543, loss_ce: 0.020600
2022-01-10 10:08:08,590 iteration 929 : loss : 0.080655, loss_ce: 0.039466
2022-01-10 10:08:10,179 iteration 930 : loss : 0.067833, loss_ce: 0.041656
2022-01-10 10:08:11,776 iteration 931 : loss : 0.064375, loss_ce: 0.025752
2022-01-10 10:08:13,373 iteration 932 : loss : 0.082743, loss_ce: 0.025634
2022-01-10 10:08:15,124 iteration 933 : loss : 0.100265, loss_ce: 0.033515
2022-01-10 10:08:16,702 iteration 934 : loss : 0.073887, loss_ce: 0.029952
2022-01-10 10:08:16,702 Training Data Eval:
2022-01-10 10:08:24,668   Average segmentation loss on training set: 0.0433
2022-01-10 10:08:24,669 Validation Data Eval:
2022-01-10 10:08:27,419   Average segmentation loss on validation set: 0.1125
2022-01-10 10:08:28,972 iteration 935 : loss : 0.057071, loss_ce: 0.018759
 14%|████▏                         | 55/400 [27:08<2:58:04, 30.97s/it]2022-01-10 10:08:30,594 iteration 936 : loss : 0.063347, loss_ce: 0.018145
2022-01-10 10:08:32,150 iteration 937 : loss : 0.060324, loss_ce: 0.023164
2022-01-10 10:08:33,761 iteration 938 : loss : 0.070289, loss_ce: 0.034091
2022-01-10 10:08:35,336 iteration 939 : loss : 0.063789, loss_ce: 0.025360
2022-01-10 10:08:36,896 iteration 940 : loss : 0.074590, loss_ce: 0.031615
2022-01-10 10:08:38,494 iteration 941 : loss : 0.116750, loss_ce: 0.037871
2022-01-10 10:08:40,067 iteration 942 : loss : 0.055307, loss_ce: 0.017314
2022-01-10 10:08:41,565 iteration 943 : loss : 0.086319, loss_ce: 0.033321
2022-01-10 10:08:43,112 iteration 944 : loss : 0.056005, loss_ce: 0.024137
2022-01-10 10:08:44,659 iteration 945 : loss : 0.081205, loss_ce: 0.043398
2022-01-10 10:08:46,248 iteration 946 : loss : 0.052509, loss_ce: 0.017307
2022-01-10 10:08:47,948 iteration 947 : loss : 0.066751, loss_ce: 0.031776
2022-01-10 10:08:49,491 iteration 948 : loss : 0.064501, loss_ce: 0.024598
2022-01-10 10:08:51,111 iteration 949 : loss : 0.051415, loss_ce: 0.019530
2022-01-10 10:08:52,733 iteration 950 : loss : 0.098360, loss_ce: 0.028283
2022-01-10 10:08:54,391 iteration 951 : loss : 0.065282, loss_ce: 0.027754
2022-01-10 10:08:55,924 iteration 952 : loss : 0.074814, loss_ce: 0.022239
 14%|████▏                         | 56/400 [27:34<2:50:39, 29.77s/it]2022-01-10 10:08:57,573 iteration 953 : loss : 0.060544, loss_ce: 0.022261
2022-01-10 10:08:59,257 iteration 954 : loss : 0.077006, loss_ce: 0.029073
2022-01-10 10:09:00,830 iteration 955 : loss : 0.047664, loss_ce: 0.019130
2022-01-10 10:09:02,400 iteration 956 : loss : 0.044606, loss_ce: 0.018430
2022-01-10 10:09:03,857 iteration 957 : loss : 0.084980, loss_ce: 0.028074
2022-01-10 10:09:05,403 iteration 958 : loss : 0.079067, loss_ce: 0.030852
2022-01-10 10:09:06,924 iteration 959 : loss : 0.061773, loss_ce: 0.028387
2022-01-10 10:09:08,442 iteration 960 : loss : 0.055646, loss_ce: 0.021724
2022-01-10 10:09:10,029 iteration 961 : loss : 0.068073, loss_ce: 0.024729
2022-01-10 10:09:11,584 iteration 962 : loss : 0.042857, loss_ce: 0.016252
2022-01-10 10:09:13,205 iteration 963 : loss : 0.081012, loss_ce: 0.029695
2022-01-10 10:09:14,817 iteration 964 : loss : 0.083538, loss_ce: 0.033346
2022-01-10 10:09:16,383 iteration 965 : loss : 0.062815, loss_ce: 0.027056
2022-01-10 10:09:18,052 iteration 966 : loss : 0.113344, loss_ce: 0.057270
2022-01-10 10:09:19,641 iteration 967 : loss : 0.131131, loss_ce: 0.057964
2022-01-10 10:09:21,238 iteration 968 : loss : 0.074399, loss_ce: 0.031688
2022-01-10 10:09:22,871 iteration 969 : loss : 0.055903, loss_ce: 0.020995
 14%|████▎                         | 57/400 [28:01<2:45:19, 28.92s/it]2022-01-10 10:09:24,506 iteration 970 : loss : 0.053445, loss_ce: 0.022104
2022-01-10 10:09:26,170 iteration 971 : loss : 0.101455, loss_ce: 0.039534
2022-01-10 10:09:27,698 iteration 972 : loss : 0.053131, loss_ce: 0.020030
2022-01-10 10:09:29,224 iteration 973 : loss : 0.070838, loss_ce: 0.026683
2022-01-10 10:09:30,794 iteration 974 : loss : 0.060378, loss_ce: 0.022995
2022-01-10 10:09:32,324 iteration 975 : loss : 0.088088, loss_ce: 0.032454
2022-01-10 10:09:33,868 iteration 976 : loss : 0.157726, loss_ce: 0.056658
2022-01-10 10:09:35,516 iteration 977 : loss : 0.084224, loss_ce: 0.033481
2022-01-10 10:09:37,142 iteration 978 : loss : 0.067663, loss_ce: 0.031176
2022-01-10 10:09:38,768 iteration 979 : loss : 0.060096, loss_ce: 0.029547
2022-01-10 10:09:40,288 iteration 980 : loss : 0.064358, loss_ce: 0.026183
2022-01-10 10:09:41,924 iteration 981 : loss : 0.095080, loss_ce: 0.042424
2022-01-10 10:09:43,447 iteration 982 : loss : 0.070001, loss_ce: 0.037178
2022-01-10 10:09:45,048 iteration 983 : loss : 0.068684, loss_ce: 0.029519
2022-01-10 10:09:46,639 iteration 984 : loss : 0.061970, loss_ce: 0.030445
2022-01-10 10:09:48,222 iteration 985 : loss : 0.075363, loss_ce: 0.025321
2022-01-10 10:09:49,789 iteration 986 : loss : 0.088882, loss_ce: 0.030676
 14%|████▎                         | 58/400 [28:28<2:41:24, 28.32s/it]2022-01-10 10:09:51,439 iteration 987 : loss : 0.049558, loss_ce: 0.022258
2022-01-10 10:09:53,099 iteration 988 : loss : 0.078354, loss_ce: 0.035601
2022-01-10 10:09:54,644 iteration 989 : loss : 0.061493, loss_ce: 0.017886
2022-01-10 10:09:56,299 iteration 990 : loss : 0.064826, loss_ce: 0.033581
2022-01-10 10:09:57,935 iteration 991 : loss : 0.096576, loss_ce: 0.050787
2022-01-10 10:09:59,464 iteration 992 : loss : 0.072963, loss_ce: 0.029020
2022-01-10 10:10:01,185 iteration 993 : loss : 0.073214, loss_ce: 0.029066
2022-01-10 10:10:02,828 iteration 994 : loss : 0.096175, loss_ce: 0.032908
2022-01-10 10:10:04,430 iteration 995 : loss : 0.068018, loss_ce: 0.029862
2022-01-10 10:10:05,989 iteration 996 : loss : 0.041443, loss_ce: 0.016320
2022-01-10 10:10:07,611 iteration 997 : loss : 0.097528, loss_ce: 0.051824
2022-01-10 10:10:09,197 iteration 998 : loss : 0.084458, loss_ce: 0.034357
2022-01-10 10:10:10,880 iteration 999 : loss : 0.050825, loss_ce: 0.021416
2022-01-10 10:10:12,496 iteration 1000 : loss : 0.071198, loss_ce: 0.029184
2022-01-10 10:10:14,104 iteration 1001 : loss : 0.060310, loss_ce: 0.023045
2022-01-10 10:10:15,639 iteration 1002 : loss : 0.046382, loss_ce: 0.017368
2022-01-10 10:10:17,185 iteration 1003 : loss : 0.068061, loss_ce: 0.027057
 15%|████▍                         | 59/400 [28:56<2:39:23, 28.04s/it]2022-01-10 10:10:18,819 iteration 1004 : loss : 0.071586, loss_ce: 0.028773
2022-01-10 10:10:20,506 iteration 1005 : loss : 0.099696, loss_ce: 0.033295
2022-01-10 10:10:22,101 iteration 1006 : loss : 0.080362, loss_ce: 0.039518
2022-01-10 10:10:23,724 iteration 1007 : loss : 0.056675, loss_ce: 0.022401
2022-01-10 10:10:25,331 iteration 1008 : loss : 0.071171, loss_ce: 0.023955
2022-01-10 10:10:26,924 iteration 1009 : loss : 0.069423, loss_ce: 0.023480
2022-01-10 10:10:28,468 iteration 1010 : loss : 0.083698, loss_ce: 0.036466
2022-01-10 10:10:30,042 iteration 1011 : loss : 0.054882, loss_ce: 0.021540
2022-01-10 10:10:31,708 iteration 1012 : loss : 0.095425, loss_ce: 0.031481
2022-01-10 10:10:33,268 iteration 1013 : loss : 0.083382, loss_ce: 0.025592
2022-01-10 10:10:34,890 iteration 1014 : loss : 0.056673, loss_ce: 0.021369
2022-01-10 10:10:36,465 iteration 1015 : loss : 0.073108, loss_ce: 0.032152
2022-01-10 10:10:37,968 iteration 1016 : loss : 0.060106, loss_ce: 0.031350
2022-01-10 10:10:39,665 iteration 1017 : loss : 0.072112, loss_ce: 0.028551
2022-01-10 10:10:41,314 iteration 1018 : loss : 0.058088, loss_ce: 0.023934
2022-01-10 10:10:42,983 iteration 1019 : loss : 0.062889, loss_ce: 0.028844
2022-01-10 10:10:42,983 Training Data Eval:
2022-01-10 10:10:50,950   Average segmentation loss on training set: 0.0651
2022-01-10 10:10:50,951 Validation Data Eval:
2022-01-10 10:10:53,706   Average segmentation loss on validation set: 0.0855
2022-01-10 10:10:59,698 Found new lowest validation loss at iteration 1019! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 10:11:01,276 iteration 1020 : loss : 0.065961, loss_ce: 0.024711
 15%|████▌                         | 60/400 [29:40<3:06:12, 32.86s/it]2022-01-10 10:11:02,991 iteration 1021 : loss : 0.069388, loss_ce: 0.029446
2022-01-10 10:11:04,569 iteration 1022 : loss : 0.058527, loss_ce: 0.022158
2022-01-10 10:11:06,128 iteration 1023 : loss : 0.108580, loss_ce: 0.035465
2022-01-10 10:11:07,757 iteration 1024 : loss : 0.042344, loss_ce: 0.021638
2022-01-10 10:11:09,309 iteration 1025 : loss : 0.083120, loss_ce: 0.037764
2022-01-10 10:11:10,922 iteration 1026 : loss : 0.070055, loss_ce: 0.025956
2022-01-10 10:11:12,505 iteration 1027 : loss : 0.072769, loss_ce: 0.037100
2022-01-10 10:11:14,046 iteration 1028 : loss : 0.069829, loss_ce: 0.022637
2022-01-10 10:11:15,655 iteration 1029 : loss : 0.082307, loss_ce: 0.025108
2022-01-10 10:11:17,268 iteration 1030 : loss : 0.045404, loss_ce: 0.016757
2022-01-10 10:11:18,918 iteration 1031 : loss : 0.049621, loss_ce: 0.019917
2022-01-10 10:11:20,473 iteration 1032 : loss : 0.048714, loss_ce: 0.018907
2022-01-10 10:11:22,115 iteration 1033 : loss : 0.065402, loss_ce: 0.022436
2022-01-10 10:11:23,781 iteration 1034 : loss : 0.053314, loss_ce: 0.024558
2022-01-10 10:11:25,403 iteration 1035 : loss : 0.100366, loss_ce: 0.036160
2022-01-10 10:11:27,100 iteration 1036 : loss : 0.060689, loss_ce: 0.025285
2022-01-10 10:11:28,572 iteration 1037 : loss : 0.043261, loss_ce: 0.018100
 15%|████▌                         | 61/400 [30:07<2:56:13, 31.19s/it]2022-01-10 10:11:30,152 iteration 1038 : loss : 0.056648, loss_ce: 0.025319
2022-01-10 10:11:31,742 iteration 1039 : loss : 0.053824, loss_ce: 0.020080
2022-01-10 10:11:33,264 iteration 1040 : loss : 0.056431, loss_ce: 0.018423
2022-01-10 10:11:34,912 iteration 1041 : loss : 0.066064, loss_ce: 0.024115
2022-01-10 10:11:36,556 iteration 1042 : loss : 0.064580, loss_ce: 0.022746
2022-01-10 10:11:38,092 iteration 1043 : loss : 0.043290, loss_ce: 0.016086
2022-01-10 10:11:39,664 iteration 1044 : loss : 0.056789, loss_ce: 0.023796
2022-01-10 10:11:41,216 iteration 1045 : loss : 0.065222, loss_ce: 0.036589
2022-01-10 10:11:42,804 iteration 1046 : loss : 0.063223, loss_ce: 0.024450
2022-01-10 10:11:44,485 iteration 1047 : loss : 0.062245, loss_ce: 0.025592
2022-01-10 10:11:46,090 iteration 1048 : loss : 0.050312, loss_ce: 0.023851
2022-01-10 10:11:47,614 iteration 1049 : loss : 0.052608, loss_ce: 0.021066
2022-01-10 10:11:49,165 iteration 1050 : loss : 0.048542, loss_ce: 0.019641
2022-01-10 10:11:50,769 iteration 1051 : loss : 0.061104, loss_ce: 0.022058
2022-01-10 10:11:52,355 iteration 1052 : loss : 0.079572, loss_ce: 0.038677
2022-01-10 10:11:54,071 iteration 1053 : loss : 0.054178, loss_ce: 0.022953
2022-01-10 10:11:55,645 iteration 1054 : loss : 0.053575, loss_ce: 0.020845
 16%|████▋                         | 62/400 [30:34<2:48:44, 29.95s/it]2022-01-10 10:11:57,285 iteration 1055 : loss : 0.046228, loss_ce: 0.018116
2022-01-10 10:11:58,879 iteration 1056 : loss : 0.060009, loss_ce: 0.025865
2022-01-10 10:12:00,504 iteration 1057 : loss : 0.080310, loss_ce: 0.020789
2022-01-10 10:12:02,030 iteration 1058 : loss : 0.058123, loss_ce: 0.027722
2022-01-10 10:12:03,706 iteration 1059 : loss : 0.062188, loss_ce: 0.026513
2022-01-10 10:12:05,273 iteration 1060 : loss : 0.050960, loss_ce: 0.024844
2022-01-10 10:12:06,903 iteration 1061 : loss : 0.049044, loss_ce: 0.019372
2022-01-10 10:12:08,501 iteration 1062 : loss : 0.044648, loss_ce: 0.016395
2022-01-10 10:12:10,169 iteration 1063 : loss : 0.063584, loss_ce: 0.022580
2022-01-10 10:12:11,728 iteration 1064 : loss : 0.049384, loss_ce: 0.023971
2022-01-10 10:12:13,237 iteration 1065 : loss : 0.054223, loss_ce: 0.019153
2022-01-10 10:12:14,845 iteration 1066 : loss : 0.047682, loss_ce: 0.026286
2022-01-10 10:12:16,579 iteration 1067 : loss : 0.076043, loss_ce: 0.030162
2022-01-10 10:12:18,239 iteration 1068 : loss : 0.089357, loss_ce: 0.035107
2022-01-10 10:12:19,766 iteration 1069 : loss : 0.057926, loss_ce: 0.025373
2022-01-10 10:12:21,459 iteration 1070 : loss : 0.050485, loss_ce: 0.019463
2022-01-10 10:12:23,050 iteration 1071 : loss : 0.051766, loss_ce: 0.021430
 16%|████▋                         | 63/400 [31:02<2:43:56, 29.19s/it]2022-01-10 10:12:24,673 iteration 1072 : loss : 0.053929, loss_ce: 0.020201
2022-01-10 10:12:26,370 iteration 1073 : loss : 0.073422, loss_ce: 0.025926
2022-01-10 10:12:27,944 iteration 1074 : loss : 0.036181, loss_ce: 0.015669
2022-01-10 10:12:29,547 iteration 1075 : loss : 0.056683, loss_ce: 0.023644
2022-01-10 10:12:31,112 iteration 1076 : loss : 0.056577, loss_ce: 0.021427
2022-01-10 10:12:32,752 iteration 1077 : loss : 0.105242, loss_ce: 0.038540
2022-01-10 10:12:34,277 iteration 1078 : loss : 0.070359, loss_ce: 0.025050
2022-01-10 10:12:35,870 iteration 1079 : loss : 0.085093, loss_ce: 0.036346
2022-01-10 10:12:37,488 iteration 1080 : loss : 0.064906, loss_ce: 0.026038
2022-01-10 10:12:39,122 iteration 1081 : loss : 0.056832, loss_ce: 0.022816
2022-01-10 10:12:40,728 iteration 1082 : loss : 0.068700, loss_ce: 0.028605
2022-01-10 10:12:42,322 iteration 1083 : loss : 0.068089, loss_ce: 0.022659
2022-01-10 10:12:43,981 iteration 1084 : loss : 0.106606, loss_ce: 0.030388
2022-01-10 10:12:45,559 iteration 1085 : loss : 0.062418, loss_ce: 0.021347
2022-01-10 10:12:47,183 iteration 1086 : loss : 0.053887, loss_ce: 0.019104
2022-01-10 10:12:48,708 iteration 1087 : loss : 0.059231, loss_ce: 0.024486
2022-01-10 10:12:50,268 iteration 1088 : loss : 0.063699, loss_ce: 0.032334
 16%|████▊                         | 64/400 [31:29<2:40:09, 28.60s/it]2022-01-10 10:12:51,810 iteration 1089 : loss : 0.055885, loss_ce: 0.023991
2022-01-10 10:12:53,392 iteration 1090 : loss : 0.059920, loss_ce: 0.019283
2022-01-10 10:12:55,026 iteration 1091 : loss : 0.072586, loss_ce: 0.036678
2022-01-10 10:12:56,628 iteration 1092 : loss : 0.081118, loss_ce: 0.025185
2022-01-10 10:12:58,237 iteration 1093 : loss : 0.087670, loss_ce: 0.025760
2022-01-10 10:12:59,960 iteration 1094 : loss : 0.133386, loss_ce: 0.026933
2022-01-10 10:13:01,483 iteration 1095 : loss : 0.056976, loss_ce: 0.018256
2022-01-10 10:13:03,028 iteration 1096 : loss : 0.058381, loss_ce: 0.025423
2022-01-10 10:13:04,546 iteration 1097 : loss : 0.083878, loss_ce: 0.036562
2022-01-10 10:13:06,194 iteration 1098 : loss : 0.051381, loss_ce: 0.023157
2022-01-10 10:13:07,789 iteration 1099 : loss : 0.050078, loss_ce: 0.016976
2022-01-10 10:13:09,444 iteration 1100 : loss : 0.065073, loss_ce: 0.031802
2022-01-10 10:13:11,097 iteration 1101 : loss : 0.074830, loss_ce: 0.035708
2022-01-10 10:13:12,691 iteration 1102 : loss : 0.068147, loss_ce: 0.026537
2022-01-10 10:13:14,300 iteration 1103 : loss : 0.049439, loss_ce: 0.021405
2022-01-10 10:13:15,878 iteration 1104 : loss : 0.060427, loss_ce: 0.026574
2022-01-10 10:13:15,878 Training Data Eval:
2022-01-10 10:13:23,846   Average segmentation loss on training set: 0.0628
2022-01-10 10:13:23,847 Validation Data Eval:
2022-01-10 10:13:26,594   Average segmentation loss on validation set: 0.0865
2022-01-10 10:13:28,169 iteration 1105 : loss : 0.055390, loss_ce: 0.022350
 16%|████▉                         | 65/400 [32:07<2:55:15, 31.39s/it]2022-01-10 10:13:29,876 iteration 1106 : loss : 0.067009, loss_ce: 0.024063
2022-01-10 10:13:31,455 iteration 1107 : loss : 0.056035, loss_ce: 0.023128
2022-01-10 10:13:33,004 iteration 1108 : loss : 0.065009, loss_ce: 0.020410
2022-01-10 10:13:34,661 iteration 1109 : loss : 0.067054, loss_ce: 0.023358
2022-01-10 10:13:36,247 iteration 1110 : loss : 0.051212, loss_ce: 0.019771
2022-01-10 10:13:37,923 iteration 1111 : loss : 0.075092, loss_ce: 0.025025
2022-01-10 10:13:39,451 iteration 1112 : loss : 0.062589, loss_ce: 0.025361
2022-01-10 10:13:41,028 iteration 1113 : loss : 0.052645, loss_ce: 0.019014
2022-01-10 10:13:42,631 iteration 1114 : loss : 0.078990, loss_ce: 0.025637
2022-01-10 10:13:44,237 iteration 1115 : loss : 0.046561, loss_ce: 0.016946
2022-01-10 10:13:45,742 iteration 1116 : loss : 0.057568, loss_ce: 0.026765
2022-01-10 10:13:47,264 iteration 1117 : loss : 0.057350, loss_ce: 0.023405
2022-01-10 10:13:48,873 iteration 1118 : loss : 0.053186, loss_ce: 0.023323
2022-01-10 10:13:50,500 iteration 1119 : loss : 0.053923, loss_ce: 0.022561
2022-01-10 10:13:52,090 iteration 1120 : loss : 0.071139, loss_ce: 0.021704
2022-01-10 10:13:53,648 iteration 1121 : loss : 0.045549, loss_ce: 0.020584
2022-01-10 10:13:55,156 iteration 1122 : loss : 0.068617, loss_ce: 0.019810
 16%|████▉                         | 66/400 [32:34<2:47:22, 30.07s/it]2022-01-10 10:13:56,886 iteration 1123 : loss : 0.042206, loss_ce: 0.016159
2022-01-10 10:13:58,465 iteration 1124 : loss : 0.043247, loss_ce: 0.020543
2022-01-10 10:14:00,009 iteration 1125 : loss : 0.077045, loss_ce: 0.019994
2022-01-10 10:14:01,564 iteration 1126 : loss : 0.067303, loss_ce: 0.022243
2022-01-10 10:14:03,118 iteration 1127 : loss : 0.051524, loss_ce: 0.018326
2022-01-10 10:14:04,773 iteration 1128 : loss : 0.059855, loss_ce: 0.025880
2022-01-10 10:14:06,457 iteration 1129 : loss : 0.059844, loss_ce: 0.024073
2022-01-10 10:14:07,974 iteration 1130 : loss : 0.043966, loss_ce: 0.017655
2022-01-10 10:14:09,607 iteration 1131 : loss : 0.089971, loss_ce: 0.036771
2022-01-10 10:14:11,263 iteration 1132 : loss : 0.061433, loss_ce: 0.020933
2022-01-10 10:14:12,892 iteration 1133 : loss : 0.066570, loss_ce: 0.028568
2022-01-10 10:14:14,540 iteration 1134 : loss : 0.070959, loss_ce: 0.020379
2022-01-10 10:14:16,089 iteration 1135 : loss : 0.071615, loss_ce: 0.026993
2022-01-10 10:14:17,650 iteration 1136 : loss : 0.033223, loss_ce: 0.014058
2022-01-10 10:14:19,215 iteration 1137 : loss : 0.067713, loss_ce: 0.035483
2022-01-10 10:14:20,818 iteration 1138 : loss : 0.073118, loss_ce: 0.026989
2022-01-10 10:14:22,354 iteration 1139 : loss : 0.049744, loss_ce: 0.019574
 17%|█████                         | 67/400 [33:01<2:42:06, 29.21s/it]2022-01-10 10:14:23,994 iteration 1140 : loss : 0.059166, loss_ce: 0.024176
2022-01-10 10:14:25,602 iteration 1141 : loss : 0.086673, loss_ce: 0.040483
2022-01-10 10:14:27,323 iteration 1142 : loss : 0.065864, loss_ce: 0.027086
2022-01-10 10:14:28,933 iteration 1143 : loss : 0.048792, loss_ce: 0.019396
2022-01-10 10:14:30,543 iteration 1144 : loss : 0.053750, loss_ce: 0.018283
2022-01-10 10:14:32,140 iteration 1145 : loss : 0.060657, loss_ce: 0.021685
2022-01-10 10:14:33,700 iteration 1146 : loss : 0.041758, loss_ce: 0.016713
2022-01-10 10:14:35,266 iteration 1147 : loss : 0.044546, loss_ce: 0.016316
2022-01-10 10:14:36,815 iteration 1148 : loss : 0.057384, loss_ce: 0.025601
2022-01-10 10:14:38,391 iteration 1149 : loss : 0.041008, loss_ce: 0.019901
2022-01-10 10:14:39,945 iteration 1150 : loss : 0.052403, loss_ce: 0.020580
2022-01-10 10:14:41,500 iteration 1151 : loss : 0.048327, loss_ce: 0.021481
2022-01-10 10:14:43,042 iteration 1152 : loss : 0.060833, loss_ce: 0.025765
2022-01-10 10:14:44,708 iteration 1153 : loss : 0.081722, loss_ce: 0.026321
2022-01-10 10:14:46,329 iteration 1154 : loss : 0.059819, loss_ce: 0.020577
2022-01-10 10:14:47,971 iteration 1155 : loss : 0.045630, loss_ce: 0.013915
2022-01-10 10:14:49,670 iteration 1156 : loss : 0.068717, loss_ce: 0.021883
 17%|█████                         | 68/400 [33:28<2:38:27, 28.64s/it]2022-01-10 10:14:51,318 iteration 1157 : loss : 0.061805, loss_ce: 0.023992
2022-01-10 10:14:52,884 iteration 1158 : loss : 0.064073, loss_ce: 0.023652
2022-01-10 10:14:54,377 iteration 1159 : loss : 0.040351, loss_ce: 0.016249
2022-01-10 10:14:55,922 iteration 1160 : loss : 0.059710, loss_ce: 0.031594
2022-01-10 10:14:57,536 iteration 1161 : loss : 0.050691, loss_ce: 0.016458
2022-01-10 10:14:59,137 iteration 1162 : loss : 0.061286, loss_ce: 0.022571
2022-01-10 10:15:00,727 iteration 1163 : loss : 0.053152, loss_ce: 0.025756
2022-01-10 10:15:02,362 iteration 1164 : loss : 0.083035, loss_ce: 0.033992
2022-01-10 10:15:03,871 iteration 1165 : loss : 0.045843, loss_ce: 0.021460
2022-01-10 10:15:05,399 iteration 1166 : loss : 0.061431, loss_ce: 0.020501
2022-01-10 10:15:06,984 iteration 1167 : loss : 0.057750, loss_ce: 0.025070
2022-01-10 10:15:08,629 iteration 1168 : loss : 0.081024, loss_ce: 0.023540
2022-01-10 10:15:10,145 iteration 1169 : loss : 0.069349, loss_ce: 0.021426
2022-01-10 10:15:11,741 iteration 1170 : loss : 0.057510, loss_ce: 0.017833
2022-01-10 10:15:13,454 iteration 1171 : loss : 0.091808, loss_ce: 0.023769
2022-01-10 10:15:14,974 iteration 1172 : loss : 0.056705, loss_ce: 0.022704
2022-01-10 10:15:16,489 iteration 1173 : loss : 0.047533, loss_ce: 0.017804
 17%|█████▏                        | 69/400 [33:55<2:34:59, 28.10s/it]2022-01-10 10:15:18,164 iteration 1174 : loss : 0.054699, loss_ce: 0.017893
2022-01-10 10:15:19,822 iteration 1175 : loss : 0.079860, loss_ce: 0.036995
2022-01-10 10:15:21,384 iteration 1176 : loss : 0.062353, loss_ce: 0.025115
2022-01-10 10:15:22,947 iteration 1177 : loss : 0.074301, loss_ce: 0.037031
2022-01-10 10:15:24,459 iteration 1178 : loss : 0.069052, loss_ce: 0.025724
2022-01-10 10:15:26,087 iteration 1179 : loss : 0.049026, loss_ce: 0.019876
2022-01-10 10:15:27,686 iteration 1180 : loss : 0.051999, loss_ce: 0.021188
2022-01-10 10:15:29,201 iteration 1181 : loss : 0.071918, loss_ce: 0.024784
2022-01-10 10:15:30,753 iteration 1182 : loss : 0.055795, loss_ce: 0.024500
2022-01-10 10:15:32,319 iteration 1183 : loss : 0.054545, loss_ce: 0.027479
2022-01-10 10:15:33,906 iteration 1184 : loss : 0.084236, loss_ce: 0.038048
2022-01-10 10:15:35,483 iteration 1185 : loss : 0.054008, loss_ce: 0.029521
2022-01-10 10:15:37,076 iteration 1186 : loss : 0.044503, loss_ce: 0.015636
2022-01-10 10:15:38,597 iteration 1187 : loss : 0.046316, loss_ce: 0.020346
2022-01-10 10:15:40,212 iteration 1188 : loss : 0.067522, loss_ce: 0.026595
2022-01-10 10:15:41,850 iteration 1189 : loss : 0.090473, loss_ce: 0.026701
2022-01-10 10:15:41,851 Training Data Eval:
2022-01-10 10:15:49,814   Average segmentation loss on training set: 0.0497
2022-01-10 10:15:49,815 Validation Data Eval:
2022-01-10 10:15:52,558   Average segmentation loss on validation set: 0.1131
2022-01-10 10:15:54,093 iteration 1190 : loss : 0.055498, loss_ce: 0.018879
 18%|█████▎                        | 70/400 [34:33<2:50:11, 30.94s/it]2022-01-10 10:15:55,738 iteration 1191 : loss : 0.059309, loss_ce: 0.022216
2022-01-10 10:15:57,295 iteration 1192 : loss : 0.085440, loss_ce: 0.045239
2022-01-10 10:15:58,790 iteration 1193 : loss : 0.060850, loss_ce: 0.024740
2022-01-10 10:16:00,379 iteration 1194 : loss : 0.043371, loss_ce: 0.015029
2022-01-10 10:16:01,961 iteration 1195 : loss : 0.058290, loss_ce: 0.022029
2022-01-10 10:16:03,547 iteration 1196 : loss : 0.044543, loss_ce: 0.019400
2022-01-10 10:16:05,171 iteration 1197 : loss : 0.073942, loss_ce: 0.023865
2022-01-10 10:16:06,676 iteration 1198 : loss : 0.037107, loss_ce: 0.013954
2022-01-10 10:16:08,245 iteration 1199 : loss : 0.058797, loss_ce: 0.029446
2022-01-10 10:16:09,743 iteration 1200 : loss : 0.043646, loss_ce: 0.016154
2022-01-10 10:16:11,286 iteration 1201 : loss : 0.092583, loss_ce: 0.026302
2022-01-10 10:16:12,916 iteration 1202 : loss : 0.053217, loss_ce: 0.024565
2022-01-10 10:16:14,555 iteration 1203 : loss : 0.046766, loss_ce: 0.020319
2022-01-10 10:16:16,057 iteration 1204 : loss : 0.052022, loss_ce: 0.015364
2022-01-10 10:16:17,663 iteration 1205 : loss : 0.049325, loss_ce: 0.022066
2022-01-10 10:16:19,206 iteration 1206 : loss : 0.054119, loss_ce: 0.019538
2022-01-10 10:16:20,841 iteration 1207 : loss : 0.043646, loss_ce: 0.020990
 18%|█████▎                        | 71/400 [34:59<2:42:46, 29.69s/it]2022-01-10 10:16:22,457 iteration 1208 : loss : 0.064853, loss_ce: 0.028478
2022-01-10 10:16:24,045 iteration 1209 : loss : 0.044762, loss_ce: 0.016683
2022-01-10 10:16:25,547 iteration 1210 : loss : 0.050640, loss_ce: 0.018175
2022-01-10 10:16:27,176 iteration 1211 : loss : 0.052015, loss_ce: 0.023123
2022-01-10 10:16:28,737 iteration 1212 : loss : 0.035173, loss_ce: 0.014164
2022-01-10 10:16:30,207 iteration 1213 : loss : 0.037245, loss_ce: 0.013532
2022-01-10 10:16:31,865 iteration 1214 : loss : 0.069849, loss_ce: 0.030597
2022-01-10 10:16:33,421 iteration 1215 : loss : 0.040574, loss_ce: 0.016292
2022-01-10 10:16:34,991 iteration 1216 : loss : 0.049986, loss_ce: 0.017002
2022-01-10 10:16:36,688 iteration 1217 : loss : 0.070769, loss_ce: 0.027025
2022-01-10 10:16:38,202 iteration 1218 : loss : 0.043534, loss_ce: 0.018888
2022-01-10 10:16:39,923 iteration 1219 : loss : 0.073635, loss_ce: 0.034629
2022-01-10 10:16:41,499 iteration 1220 : loss : 0.039991, loss_ce: 0.017714
2022-01-10 10:16:43,146 iteration 1221 : loss : 0.052069, loss_ce: 0.019739
2022-01-10 10:16:44,705 iteration 1222 : loss : 0.061988, loss_ce: 0.022342
2022-01-10 10:16:46,253 iteration 1223 : loss : 0.057170, loss_ce: 0.020831
2022-01-10 10:16:47,810 iteration 1224 : loss : 0.114837, loss_ce: 0.056979
 18%|█████▍                        | 72/400 [35:26<2:37:49, 28.87s/it]2022-01-10 10:16:49,448 iteration 1225 : loss : 0.054278, loss_ce: 0.016916
2022-01-10 10:16:51,051 iteration 1226 : loss : 0.053917, loss_ce: 0.019122
2022-01-10 10:16:52,578 iteration 1227 : loss : 0.050060, loss_ce: 0.017186
2022-01-10 10:16:54,124 iteration 1228 : loss : 0.074859, loss_ce: 0.021469
2022-01-10 10:16:55,829 iteration 1229 : loss : 0.065552, loss_ce: 0.028231
2022-01-10 10:16:57,461 iteration 1230 : loss : 0.045802, loss_ce: 0.017024
2022-01-10 10:16:59,060 iteration 1231 : loss : 0.056964, loss_ce: 0.024250
2022-01-10 10:17:00,598 iteration 1232 : loss : 0.050066, loss_ce: 0.019160
2022-01-10 10:17:02,235 iteration 1233 : loss : 0.037644, loss_ce: 0.013170
2022-01-10 10:17:03,930 iteration 1234 : loss : 0.048896, loss_ce: 0.020973
2022-01-10 10:17:05,432 iteration 1235 : loss : 0.055955, loss_ce: 0.020170
2022-01-10 10:17:07,061 iteration 1236 : loss : 0.057708, loss_ce: 0.030232
2022-01-10 10:17:08,640 iteration 1237 : loss : 0.057158, loss_ce: 0.018644
2022-01-10 10:17:10,167 iteration 1238 : loss : 0.063652, loss_ce: 0.025749
2022-01-10 10:17:11,728 iteration 1239 : loss : 0.041659, loss_ce: 0.018591
2022-01-10 10:17:13,394 iteration 1240 : loss : 0.054571, loss_ce: 0.019150
2022-01-10 10:17:14,984 iteration 1241 : loss : 0.044749, loss_ce: 0.019092
 18%|█████▍                        | 73/400 [35:54<2:34:35, 28.37s/it]2022-01-10 10:17:16,679 iteration 1242 : loss : 0.043707, loss_ce: 0.023656
2022-01-10 10:17:18,270 iteration 1243 : loss : 0.041678, loss_ce: 0.016332
2022-01-10 10:17:19,950 iteration 1244 : loss : 0.057205, loss_ce: 0.022250
2022-01-10 10:17:21,611 iteration 1245 : loss : 0.044025, loss_ce: 0.016471
2022-01-10 10:17:23,189 iteration 1246 : loss : 0.069116, loss_ce: 0.026607
2022-01-10 10:17:24,940 iteration 1247 : loss : 0.098349, loss_ce: 0.041872
2022-01-10 10:17:26,489 iteration 1248 : loss : 0.035940, loss_ce: 0.014499
2022-01-10 10:17:28,058 iteration 1249 : loss : 0.056078, loss_ce: 0.027185
2022-01-10 10:17:29,642 iteration 1250 : loss : 0.072136, loss_ce: 0.017941
2022-01-10 10:17:31,303 iteration 1251 : loss : 0.059449, loss_ce: 0.025877
2022-01-10 10:17:32,827 iteration 1252 : loss : 0.058607, loss_ce: 0.017674
2022-01-10 10:17:34,388 iteration 1253 : loss : 0.062116, loss_ce: 0.023125
2022-01-10 10:17:35,960 iteration 1254 : loss : 0.046861, loss_ce: 0.015701
2022-01-10 10:17:37,646 iteration 1255 : loss : 0.059156, loss_ce: 0.026032
2022-01-10 10:17:39,235 iteration 1256 : loss : 0.055714, loss_ce: 0.019880
2022-01-10 10:17:40,717 iteration 1257 : loss : 0.038961, loss_ce: 0.013816
2022-01-10 10:17:42,207 iteration 1258 : loss : 0.045373, loss_ce: 0.021818
 18%|█████▌                        | 74/400 [36:21<2:32:14, 28.02s/it]2022-01-10 10:17:43,805 iteration 1259 : loss : 0.051620, loss_ce: 0.019077
2022-01-10 10:17:45,484 iteration 1260 : loss : 0.039007, loss_ce: 0.015623
2022-01-10 10:17:47,041 iteration 1261 : loss : 0.044808, loss_ce: 0.015023
2022-01-10 10:17:48,551 iteration 1262 : loss : 0.049474, loss_ce: 0.018827
2022-01-10 10:17:50,110 iteration 1263 : loss : 0.044696, loss_ce: 0.017385
2022-01-10 10:17:51,712 iteration 1264 : loss : 0.047556, loss_ce: 0.020900
2022-01-10 10:17:53,283 iteration 1265 : loss : 0.049264, loss_ce: 0.018909
2022-01-10 10:17:54,828 iteration 1266 : loss : 0.040617, loss_ce: 0.018143
2022-01-10 10:17:56,357 iteration 1267 : loss : 0.079553, loss_ce: 0.037695
2022-01-10 10:17:57,887 iteration 1268 : loss : 0.045574, loss_ce: 0.016788
2022-01-10 10:17:59,445 iteration 1269 : loss : 0.062013, loss_ce: 0.028845
2022-01-10 10:18:01,025 iteration 1270 : loss : 0.032560, loss_ce: 0.014471
2022-01-10 10:18:02,682 iteration 1271 : loss : 0.064947, loss_ce: 0.029537
2022-01-10 10:18:04,162 iteration 1272 : loss : 0.044599, loss_ce: 0.015023
2022-01-10 10:18:05,695 iteration 1273 : loss : 0.069070, loss_ce: 0.022575
2022-01-10 10:18:07,304 iteration 1274 : loss : 0.048373, loss_ce: 0.020297
2022-01-10 10:18:07,305 Training Data Eval:
2022-01-10 10:18:15,262   Average segmentation loss on training set: 0.0446
2022-01-10 10:18:15,263 Validation Data Eval:
2022-01-10 10:18:18,007   Average segmentation loss on validation set: 0.0824
2022-01-10 10:18:23,777 Found new lowest validation loss at iteration 1274! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 10:18:25,236 iteration 1275 : loss : 0.052267, loss_ce: 0.021719
 19%|█████▋                        | 75/400 [37:04<2:56:10, 32.53s/it]2022-01-10 10:18:26,838 iteration 1276 : loss : 0.043351, loss_ce: 0.018853
2022-01-10 10:18:28,510 iteration 1277 : loss : 0.082849, loss_ce: 0.034871
2022-01-10 10:18:30,076 iteration 1278 : loss : 0.073511, loss_ce: 0.034597
2022-01-10 10:18:31,644 iteration 1279 : loss : 0.052188, loss_ce: 0.018053
2022-01-10 10:18:33,257 iteration 1280 : loss : 0.042799, loss_ce: 0.019048
2022-01-10 10:18:34,788 iteration 1281 : loss : 0.065066, loss_ce: 0.025577
2022-01-10 10:18:36,363 iteration 1282 : loss : 0.040990, loss_ce: 0.015410
2022-01-10 10:18:37,852 iteration 1283 : loss : 0.070373, loss_ce: 0.019914
2022-01-10 10:18:39,416 iteration 1284 : loss : 0.034380, loss_ce: 0.012857
2022-01-10 10:18:40,919 iteration 1285 : loss : 0.040146, loss_ce: 0.012411
2022-01-10 10:18:42,550 iteration 1286 : loss : 0.060752, loss_ce: 0.020362
2022-01-10 10:18:44,170 iteration 1287 : loss : 0.040626, loss_ce: 0.016868
2022-01-10 10:18:45,700 iteration 1288 : loss : 0.049279, loss_ce: 0.018414
2022-01-10 10:18:47,277 iteration 1289 : loss : 0.095882, loss_ce: 0.037614
2022-01-10 10:18:48,805 iteration 1290 : loss : 0.100692, loss_ce: 0.028658
2022-01-10 10:18:50,338 iteration 1291 : loss : 0.055158, loss_ce: 0.020261
2022-01-10 10:18:51,858 iteration 1292 : loss : 0.064378, loss_ce: 0.037165
 19%|█████▋                        | 76/400 [37:30<2:46:03, 30.75s/it]2022-01-10 10:18:53,581 iteration 1293 : loss : 0.074949, loss_ce: 0.037373
2022-01-10 10:18:55,084 iteration 1294 : loss : 0.044501, loss_ce: 0.021637
2022-01-10 10:18:56,697 iteration 1295 : loss : 0.068523, loss_ce: 0.023495
2022-01-10 10:18:58,298 iteration 1296 : loss : 0.107038, loss_ce: 0.033460
2022-01-10 10:18:59,757 iteration 1297 : loss : 0.040318, loss_ce: 0.015033
2022-01-10 10:19:01,407 iteration 1298 : loss : 0.067505, loss_ce: 0.024070
2022-01-10 10:19:02,955 iteration 1299 : loss : 0.042128, loss_ce: 0.017984
2022-01-10 10:19:04,604 iteration 1300 : loss : 0.061081, loss_ce: 0.020609
2022-01-10 10:19:06,276 iteration 1301 : loss : 0.053857, loss_ce: 0.016388
2022-01-10 10:19:07,792 iteration 1302 : loss : 0.056822, loss_ce: 0.019331
2022-01-10 10:19:09,406 iteration 1303 : loss : 0.058054, loss_ce: 0.017022
2022-01-10 10:19:10,999 iteration 1304 : loss : 0.061739, loss_ce: 0.031257
2022-01-10 10:19:12,663 iteration 1305 : loss : 0.056039, loss_ce: 0.020028
2022-01-10 10:19:14,265 iteration 1306 : loss : 0.050498, loss_ce: 0.021512
2022-01-10 10:19:15,796 iteration 1307 : loss : 0.043481, loss_ce: 0.016054
2022-01-10 10:19:17,364 iteration 1308 : loss : 0.060710, loss_ce: 0.028934
2022-01-10 10:19:18,909 iteration 1309 : loss : 0.041827, loss_ce: 0.015615
 19%|█████▊                        | 77/400 [37:57<2:39:34, 29.64s/it]2022-01-10 10:19:20,515 iteration 1310 : loss : 0.053246, loss_ce: 0.020298
2022-01-10 10:19:22,209 iteration 1311 : loss : 0.085399, loss_ce: 0.030642
2022-01-10 10:19:23,791 iteration 1312 : loss : 0.052542, loss_ce: 0.022170
2022-01-10 10:19:25,362 iteration 1313 : loss : 0.060522, loss_ce: 0.028375
2022-01-10 10:19:27,002 iteration 1314 : loss : 0.062870, loss_ce: 0.031447
2022-01-10 10:19:28,656 iteration 1315 : loss : 0.046280, loss_ce: 0.024242
2022-01-10 10:19:30,215 iteration 1316 : loss : 0.049477, loss_ce: 0.022077
2022-01-10 10:19:31,728 iteration 1317 : loss : 0.051621, loss_ce: 0.017771
2022-01-10 10:19:33,247 iteration 1318 : loss : 0.050205, loss_ce: 0.020301
2022-01-10 10:19:34,869 iteration 1319 : loss : 0.060145, loss_ce: 0.019777
2022-01-10 10:19:36,395 iteration 1320 : loss : 0.046038, loss_ce: 0.017619
2022-01-10 10:19:37,987 iteration 1321 : loss : 0.086310, loss_ce: 0.037272
2022-01-10 10:19:39,537 iteration 1322 : loss : 0.042357, loss_ce: 0.015632
2022-01-10 10:19:41,156 iteration 1323 : loss : 0.046933, loss_ce: 0.015096
2022-01-10 10:19:42,814 iteration 1324 : loss : 0.062102, loss_ce: 0.026611
2022-01-10 10:19:44,383 iteration 1325 : loss : 0.064615, loss_ce: 0.026602
2022-01-10 10:19:45,975 iteration 1326 : loss : 0.064577, loss_ce: 0.029533
 20%|█████▊                        | 78/400 [38:25<2:34:55, 28.87s/it]2022-01-10 10:19:47,609 iteration 1327 : loss : 0.042682, loss_ce: 0.017087
2022-01-10 10:19:49,206 iteration 1328 : loss : 0.049216, loss_ce: 0.019611
2022-01-10 10:19:50,806 iteration 1329 : loss : 0.058596, loss_ce: 0.026001
2022-01-10 10:19:52,482 iteration 1330 : loss : 0.056000, loss_ce: 0.020429
2022-01-10 10:19:54,079 iteration 1331 : loss : 0.053156, loss_ce: 0.021293
2022-01-10 10:19:55,639 iteration 1332 : loss : 0.052386, loss_ce: 0.021161
2022-01-10 10:19:57,198 iteration 1333 : loss : 0.045167, loss_ce: 0.018388
2022-01-10 10:19:58,829 iteration 1334 : loss : 0.045148, loss_ce: 0.018760
2022-01-10 10:20:00,442 iteration 1335 : loss : 0.037101, loss_ce: 0.015216
2022-01-10 10:20:01,973 iteration 1336 : loss : 0.042384, loss_ce: 0.020054
2022-01-10 10:20:03,610 iteration 1337 : loss : 0.061741, loss_ce: 0.026731
2022-01-10 10:20:05,158 iteration 1338 : loss : 0.033966, loss_ce: 0.015367
2022-01-10 10:20:06,701 iteration 1339 : loss : 0.060545, loss_ce: 0.024674
2022-01-10 10:20:08,240 iteration 1340 : loss : 0.041301, loss_ce: 0.017071
2022-01-10 10:20:09,836 iteration 1341 : loss : 0.052210, loss_ce: 0.022631
2022-01-10 10:20:11,493 iteration 1342 : loss : 0.051587, loss_ce: 0.018707
2022-01-10 10:20:12,986 iteration 1343 : loss : 0.050320, loss_ce: 0.017076
 20%|█████▉                        | 79/400 [38:52<2:31:27, 28.31s/it]2022-01-10 10:20:14,643 iteration 1344 : loss : 0.045838, loss_ce: 0.015198
2022-01-10 10:20:16,190 iteration 1345 : loss : 0.035122, loss_ce: 0.010834
2022-01-10 10:20:17,816 iteration 1346 : loss : 0.043710, loss_ce: 0.015464
2022-01-10 10:20:19,395 iteration 1347 : loss : 0.062950, loss_ce: 0.026825
2022-01-10 10:20:20,932 iteration 1348 : loss : 0.051906, loss_ce: 0.023690
2022-01-10 10:20:22,507 iteration 1349 : loss : 0.048051, loss_ce: 0.019064
2022-01-10 10:20:24,066 iteration 1350 : loss : 0.059757, loss_ce: 0.018419
2022-01-10 10:20:25,732 iteration 1351 : loss : 0.084821, loss_ce: 0.038016
2022-01-10 10:20:27,294 iteration 1352 : loss : 0.056816, loss_ce: 0.027295
2022-01-10 10:20:28,870 iteration 1353 : loss : 0.049829, loss_ce: 0.022870
2022-01-10 10:20:30,459 iteration 1354 : loss : 0.062297, loss_ce: 0.021773
2022-01-10 10:20:31,961 iteration 1355 : loss : 0.054535, loss_ce: 0.015958
2022-01-10 10:20:33,619 iteration 1356 : loss : 0.082532, loss_ce: 0.029422
2022-01-10 10:20:35,139 iteration 1357 : loss : 0.043379, loss_ce: 0.020250
2022-01-10 10:20:36,682 iteration 1358 : loss : 0.060959, loss_ce: 0.027869
2022-01-10 10:20:38,340 iteration 1359 : loss : 0.055482, loss_ce: 0.029789
2022-01-10 10:20:38,340 Training Data Eval:
2022-01-10 10:20:46,303   Average segmentation loss on training set: 0.0432
2022-01-10 10:20:46,303 Validation Data Eval:
2022-01-10 10:20:49,053   Average segmentation loss on validation set: 0.1439
2022-01-10 10:20:50,648 iteration 1360 : loss : 0.041688, loss_ce: 0.015954
 20%|██████                        | 80/400 [39:29<2:45:56, 31.11s/it]2022-01-10 10:20:52,339 iteration 1361 : loss : 0.072385, loss_ce: 0.022757
2022-01-10 10:20:53,911 iteration 1362 : loss : 0.071493, loss_ce: 0.021615
2022-01-10 10:20:55,514 iteration 1363 : loss : 0.040718, loss_ce: 0.014450
2022-01-10 10:20:57,010 iteration 1364 : loss : 0.056923, loss_ce: 0.027780
2022-01-10 10:20:58,552 iteration 1365 : loss : 0.053828, loss_ce: 0.016193
2022-01-10 10:21:00,216 iteration 1366 : loss : 0.057694, loss_ce: 0.021002
2022-01-10 10:21:01,807 iteration 1367 : loss : 0.058976, loss_ce: 0.033481
2022-01-10 10:21:03,576 iteration 1368 : loss : 0.053281, loss_ce: 0.020291
2022-01-10 10:21:05,140 iteration 1369 : loss : 0.040145, loss_ce: 0.017840
2022-01-10 10:21:06,684 iteration 1370 : loss : 0.059530, loss_ce: 0.026093
2022-01-10 10:21:08,261 iteration 1371 : loss : 0.076454, loss_ce: 0.030300
2022-01-10 10:21:09,975 iteration 1372 : loss : 0.073554, loss_ce: 0.028425
2022-01-10 10:21:11,537 iteration 1373 : loss : 0.049697, loss_ce: 0.022379
2022-01-10 10:21:13,086 iteration 1374 : loss : 0.047058, loss_ce: 0.017131
2022-01-10 10:21:14,634 iteration 1375 : loss : 0.043822, loss_ce: 0.017705
2022-01-10 10:21:16,215 iteration 1376 : loss : 0.048461, loss_ce: 0.016652
2022-01-10 10:21:17,853 iteration 1377 : loss : 0.045428, loss_ce: 0.025236
 20%|██████                        | 81/400 [39:56<2:39:12, 29.94s/it]2022-01-10 10:21:19,587 iteration 1378 : loss : 0.055132, loss_ce: 0.027899
2022-01-10 10:21:21,233 iteration 1379 : loss : 0.070189, loss_ce: 0.030662
2022-01-10 10:21:22,825 iteration 1380 : loss : 0.075117, loss_ce: 0.024174
2022-01-10 10:21:24,463 iteration 1381 : loss : 0.094104, loss_ce: 0.047911
2022-01-10 10:21:26,072 iteration 1382 : loss : 0.062649, loss_ce: 0.023756
2022-01-10 10:21:27,596 iteration 1383 : loss : 0.067553, loss_ce: 0.018374
2022-01-10 10:21:29,227 iteration 1384 : loss : 0.052022, loss_ce: 0.022961
2022-01-10 10:21:30,817 iteration 1385 : loss : 0.048135, loss_ce: 0.020377
2022-01-10 10:21:32,381 iteration 1386 : loss : 0.056737, loss_ce: 0.021978
2022-01-10 10:21:34,091 iteration 1387 : loss : 0.059570, loss_ce: 0.021632
2022-01-10 10:21:35,671 iteration 1388 : loss : 0.053822, loss_ce: 0.019003
2022-01-10 10:21:37,241 iteration 1389 : loss : 0.054436, loss_ce: 0.021543
2022-01-10 10:21:38,861 iteration 1390 : loss : 0.072153, loss_ce: 0.030304
2022-01-10 10:21:40,454 iteration 1391 : loss : 0.068984, loss_ce: 0.019976
2022-01-10 10:21:42,070 iteration 1392 : loss : 0.057999, loss_ce: 0.019639
2022-01-10 10:21:43,634 iteration 1393 : loss : 0.053764, loss_ce: 0.016344
2022-01-10 10:21:45,205 iteration 1394 : loss : 0.049490, loss_ce: 0.021779
 20%|██████▏                       | 82/400 [40:24<2:34:35, 29.17s/it]2022-01-10 10:21:46,853 iteration 1395 : loss : 0.044386, loss_ce: 0.016797
2022-01-10 10:21:48,401 iteration 1396 : loss : 0.049759, loss_ce: 0.019097
2022-01-10 10:21:50,007 iteration 1397 : loss : 0.091365, loss_ce: 0.032075
2022-01-10 10:21:51,686 iteration 1398 : loss : 0.041163, loss_ce: 0.015967
2022-01-10 10:21:53,300 iteration 1399 : loss : 0.076148, loss_ce: 0.036370
2022-01-10 10:21:54,867 iteration 1400 : loss : 0.094301, loss_ce: 0.055358
2022-01-10 10:21:56,422 iteration 1401 : loss : 0.061047, loss_ce: 0.023820
2022-01-10 10:21:57,934 iteration 1402 : loss : 0.055913, loss_ce: 0.022324
2022-01-10 10:21:59,621 iteration 1403 : loss : 0.070036, loss_ce: 0.025522
2022-01-10 10:22:01,255 iteration 1404 : loss : 0.052432, loss_ce: 0.019197
2022-01-10 10:22:02,885 iteration 1405 : loss : 0.054549, loss_ce: 0.023037
2022-01-10 10:22:04,490 iteration 1406 : loss : 0.055285, loss_ce: 0.020543
2022-01-10 10:22:06,020 iteration 1407 : loss : 0.056676, loss_ce: 0.027365
2022-01-10 10:22:07,580 iteration 1408 : loss : 0.054767, loss_ce: 0.019099
2022-01-10 10:22:09,238 iteration 1409 : loss : 0.072754, loss_ce: 0.024982
2022-01-10 10:22:10,867 iteration 1410 : loss : 0.066513, loss_ce: 0.024508
2022-01-10 10:22:12,435 iteration 1411 : loss : 0.053999, loss_ce: 0.023737
 21%|██████▏                       | 83/400 [40:51<2:31:00, 28.58s/it]2022-01-10 10:22:13,984 iteration 1412 : loss : 0.066571, loss_ce: 0.019400
2022-01-10 10:22:15,618 iteration 1413 : loss : 0.042301, loss_ce: 0.016303
2022-01-10 10:22:17,197 iteration 1414 : loss : 0.042400, loss_ce: 0.020231
2022-01-10 10:22:18,721 iteration 1415 : loss : 0.085562, loss_ce: 0.028237
2022-01-10 10:22:20,305 iteration 1416 : loss : 0.041052, loss_ce: 0.012568
2022-01-10 10:22:21,938 iteration 1417 : loss : 0.050246, loss_ce: 0.019840
2022-01-10 10:22:23,448 iteration 1418 : loss : 0.054921, loss_ce: 0.016063
2022-01-10 10:22:25,153 iteration 1419 : loss : 0.064656, loss_ce: 0.022009
2022-01-10 10:22:26,801 iteration 1420 : loss : 0.058713, loss_ce: 0.020532
2022-01-10 10:22:28,348 iteration 1421 : loss : 0.050480, loss_ce: 0.021169
2022-01-10 10:22:29,929 iteration 1422 : loss : 0.040747, loss_ce: 0.015289
2022-01-10 10:22:31,478 iteration 1423 : loss : 0.094387, loss_ce: 0.048208
2022-01-10 10:22:33,097 iteration 1424 : loss : 0.078233, loss_ce: 0.026991
2022-01-10 10:22:34,764 iteration 1425 : loss : 0.065031, loss_ce: 0.027971
2022-01-10 10:22:36,218 iteration 1426 : loss : 0.049049, loss_ce: 0.020301
2022-01-10 10:22:37,777 iteration 1427 : loss : 0.044668, loss_ce: 0.019777
2022-01-10 10:22:39,364 iteration 1428 : loss : 0.048699, loss_ce: 0.018126
 21%|██████▎                       | 84/400 [41:18<2:27:55, 28.09s/it]2022-01-10 10:22:41,104 iteration 1429 : loss : 0.065324, loss_ce: 0.028103
2022-01-10 10:22:42,648 iteration 1430 : loss : 0.050424, loss_ce: 0.018966
2022-01-10 10:22:44,220 iteration 1431 : loss : 0.046442, loss_ce: 0.023055
2022-01-10 10:22:45,814 iteration 1432 : loss : 0.052225, loss_ce: 0.020861
2022-01-10 10:22:47,498 iteration 1433 : loss : 0.068884, loss_ce: 0.026818
2022-01-10 10:22:49,136 iteration 1434 : loss : 0.064504, loss_ce: 0.025762
2022-01-10 10:22:50,621 iteration 1435 : loss : 0.046021, loss_ce: 0.015234
2022-01-10 10:22:52,278 iteration 1436 : loss : 0.046894, loss_ce: 0.020774
2022-01-10 10:22:53,784 iteration 1437 : loss : 0.104993, loss_ce: 0.025663
2022-01-10 10:22:55,272 iteration 1438 : loss : 0.039786, loss_ce: 0.018271
2022-01-10 10:22:56,840 iteration 1439 : loss : 0.043219, loss_ce: 0.014015
2022-01-10 10:22:58,504 iteration 1440 : loss : 0.050337, loss_ce: 0.023592
2022-01-10 10:23:00,059 iteration 1441 : loss : 0.038371, loss_ce: 0.015279
2022-01-10 10:23:01,590 iteration 1442 : loss : 0.051553, loss_ce: 0.021067
2022-01-10 10:23:03,203 iteration 1443 : loss : 0.055125, loss_ce: 0.021907
2022-01-10 10:23:04,842 iteration 1444 : loss : 0.040984, loss_ce: 0.020548
2022-01-10 10:23:04,843 Training Data Eval:
2022-01-10 10:23:12,800   Average segmentation loss on training set: 0.0702
2022-01-10 10:23:12,801 Validation Data Eval:
2022-01-10 10:23:15,543   Average segmentation loss on validation set: 0.1156
2022-01-10 10:23:17,148 iteration 1445 : loss : 0.055714, loss_ce: 0.019035
 21%|██████▍                       | 85/400 [41:56<2:42:44, 31.00s/it]2022-01-10 10:23:18,847 iteration 1446 : loss : 0.055642, loss_ce: 0.019923
2022-01-10 10:23:20,489 iteration 1447 : loss : 0.064686, loss_ce: 0.027512
2022-01-10 10:23:22,053 iteration 1448 : loss : 0.046669, loss_ce: 0.023156
2022-01-10 10:23:23,647 iteration 1449 : loss : 0.060798, loss_ce: 0.020935
2022-01-10 10:23:25,138 iteration 1450 : loss : 0.064240, loss_ce: 0.030564
2022-01-10 10:23:26,667 iteration 1451 : loss : 0.038620, loss_ce: 0.012755
2022-01-10 10:23:28,250 iteration 1452 : loss : 0.075292, loss_ce: 0.017377
2022-01-10 10:23:29,891 iteration 1453 : loss : 0.086127, loss_ce: 0.030132
2022-01-10 10:23:31,434 iteration 1454 : loss : 0.056358, loss_ce: 0.024259
2022-01-10 10:23:32,979 iteration 1455 : loss : 0.051562, loss_ce: 0.021373
2022-01-10 10:23:34,573 iteration 1456 : loss : 0.065211, loss_ce: 0.022582
2022-01-10 10:23:36,163 iteration 1457 : loss : 0.064125, loss_ce: 0.026411
2022-01-10 10:23:37,734 iteration 1458 : loss : 0.044233, loss_ce: 0.015376
2022-01-10 10:23:39,213 iteration 1459 : loss : 0.037614, loss_ce: 0.017478
2022-01-10 10:23:40,841 iteration 1460 : loss : 0.066117, loss_ce: 0.036799
2022-01-10 10:23:42,421 iteration 1461 : loss : 0.060372, loss_ce: 0.020352
2022-01-10 10:23:43,987 iteration 1462 : loss : 0.057891, loss_ce: 0.024285
 22%|██████▍                       | 86/400 [42:23<2:35:41, 29.75s/it]2022-01-10 10:23:45,606 iteration 1463 : loss : 0.036766, loss_ce: 0.016882
2022-01-10 10:23:47,232 iteration 1464 : loss : 0.053953, loss_ce: 0.019240
2022-01-10 10:23:48,874 iteration 1465 : loss : 0.069657, loss_ce: 0.031940
2022-01-10 10:23:50,462 iteration 1466 : loss : 0.066486, loss_ce: 0.026034
2022-01-10 10:23:52,030 iteration 1467 : loss : 0.053459, loss_ce: 0.021562
2022-01-10 10:23:53,624 iteration 1468 : loss : 0.043358, loss_ce: 0.016374
2022-01-10 10:23:55,242 iteration 1469 : loss : 0.050260, loss_ce: 0.021956
2022-01-10 10:23:56,829 iteration 1470 : loss : 0.044108, loss_ce: 0.016373
2022-01-10 10:23:58,344 iteration 1471 : loss : 0.040563, loss_ce: 0.014573
2022-01-10 10:23:59,909 iteration 1472 : loss : 0.046892, loss_ce: 0.016391
2022-01-10 10:24:01,552 iteration 1473 : loss : 0.053564, loss_ce: 0.031718
2022-01-10 10:24:03,080 iteration 1474 : loss : 0.053210, loss_ce: 0.020237
2022-01-10 10:24:04,647 iteration 1475 : loss : 0.060047, loss_ce: 0.020641
2022-01-10 10:24:06,284 iteration 1476 : loss : 0.069556, loss_ce: 0.037952
2022-01-10 10:24:07,858 iteration 1477 : loss : 0.067692, loss_ce: 0.022923
2022-01-10 10:24:09,500 iteration 1478 : loss : 0.041304, loss_ce: 0.015431
2022-01-10 10:24:11,114 iteration 1479 : loss : 0.068514, loss_ce: 0.028698
 22%|██████▌                       | 87/400 [42:50<2:31:05, 28.96s/it]2022-01-10 10:24:12,751 iteration 1480 : loss : 0.044176, loss_ce: 0.025047
2022-01-10 10:24:14,417 iteration 1481 : loss : 0.043487, loss_ce: 0.018453
2022-01-10 10:24:15,978 iteration 1482 : loss : 0.042201, loss_ce: 0.017748
2022-01-10 10:24:17,492 iteration 1483 : loss : 0.047993, loss_ce: 0.017903
2022-01-10 10:24:19,013 iteration 1484 : loss : 0.047516, loss_ce: 0.020442
2022-01-10 10:24:20,553 iteration 1485 : loss : 0.058564, loss_ce: 0.022792
2022-01-10 10:24:22,106 iteration 1486 : loss : 0.060629, loss_ce: 0.020779
2022-01-10 10:24:23,753 iteration 1487 : loss : 0.102185, loss_ce: 0.055494
2022-01-10 10:24:25,318 iteration 1488 : loss : 0.049812, loss_ce: 0.019248
2022-01-10 10:24:26,926 iteration 1489 : loss : 0.061066, loss_ce: 0.024239
2022-01-10 10:24:28,507 iteration 1490 : loss : 0.056009, loss_ce: 0.023840
2022-01-10 10:24:30,029 iteration 1491 : loss : 0.104731, loss_ce: 0.043744
2022-01-10 10:24:31,642 iteration 1492 : loss : 0.117632, loss_ce: 0.060973
2022-01-10 10:24:33,343 iteration 1493 : loss : 0.097711, loss_ce: 0.046028
2022-01-10 10:24:34,967 iteration 1494 : loss : 0.107908, loss_ce: 0.047811
2022-01-10 10:24:36,555 iteration 1495 : loss : 0.092983, loss_ce: 0.037617
2022-01-10 10:24:38,142 iteration 1496 : loss : 0.091370, loss_ce: 0.041843
 22%|██████▌                       | 88/400 [43:17<2:27:35, 28.38s/it]2022-01-10 10:24:39,753 iteration 1497 : loss : 0.046562, loss_ce: 0.019985
2022-01-10 10:24:41,227 iteration 1498 : loss : 0.042637, loss_ce: 0.017510
2022-01-10 10:24:42,742 iteration 1499 : loss : 0.079062, loss_ce: 0.038187
2022-01-10 10:24:44,464 iteration 1500 : loss : 0.081461, loss_ce: 0.034336
2022-01-10 10:24:45,969 iteration 1501 : loss : 0.065450, loss_ce: 0.023731
2022-01-10 10:24:47,558 iteration 1502 : loss : 0.090274, loss_ce: 0.034320
2022-01-10 10:24:49,216 iteration 1503 : loss : 0.065390, loss_ce: 0.024591
2022-01-10 10:24:50,776 iteration 1504 : loss : 0.076014, loss_ce: 0.032813
2022-01-10 10:24:52,313 iteration 1505 : loss : 0.058227, loss_ce: 0.023325
2022-01-10 10:24:53,946 iteration 1506 : loss : 0.057611, loss_ce: 0.020146
2022-01-10 10:24:55,499 iteration 1507 : loss : 0.069042, loss_ce: 0.031475
2022-01-10 10:24:57,094 iteration 1508 : loss : 0.131615, loss_ce: 0.030936
2022-01-10 10:24:58,661 iteration 1509 : loss : 0.064229, loss_ce: 0.027696
2022-01-10 10:25:00,214 iteration 1510 : loss : 0.087538, loss_ce: 0.031313
2022-01-10 10:25:01,900 iteration 1511 : loss : 0.057604, loss_ce: 0.031025
2022-01-10 10:25:03,490 iteration 1512 : loss : 0.055053, loss_ce: 0.020720
2022-01-10 10:25:05,089 iteration 1513 : loss : 0.065372, loss_ce: 0.025030
 22%|██████▋                       | 89/400 [43:44<2:24:52, 27.95s/it]2022-01-10 10:25:06,757 iteration 1514 : loss : 0.058487, loss_ce: 0.020917
2022-01-10 10:25:08,350 iteration 1515 : loss : 0.057768, loss_ce: 0.025342
2022-01-10 10:25:09,973 iteration 1516 : loss : 0.055144, loss_ce: 0.031051
2022-01-10 10:25:11,526 iteration 1517 : loss : 0.063415, loss_ce: 0.030406
2022-01-10 10:25:13,037 iteration 1518 : loss : 0.055631, loss_ce: 0.023188
2022-01-10 10:25:14,603 iteration 1519 : loss : 0.051677, loss_ce: 0.018507
2022-01-10 10:25:16,348 iteration 1520 : loss : 0.077981, loss_ce: 0.034574
2022-01-10 10:25:17,889 iteration 1521 : loss : 0.049233, loss_ce: 0.018998
2022-01-10 10:25:19,537 iteration 1522 : loss : 0.043891, loss_ce: 0.016251
2022-01-10 10:25:21,131 iteration 1523 : loss : 0.069400, loss_ce: 0.026974
2022-01-10 10:25:22,656 iteration 1524 : loss : 0.035553, loss_ce: 0.012709
2022-01-10 10:25:24,220 iteration 1525 : loss : 0.057481, loss_ce: 0.028541
2022-01-10 10:25:25,780 iteration 1526 : loss : 0.080470, loss_ce: 0.027233
2022-01-10 10:25:27,444 iteration 1527 : loss : 0.042497, loss_ce: 0.016846
2022-01-10 10:25:29,064 iteration 1528 : loss : 0.075624, loss_ce: 0.031955
2022-01-10 10:25:30,639 iteration 1529 : loss : 0.072681, loss_ce: 0.022694
2022-01-10 10:25:30,639 Training Data Eval:
2022-01-10 10:25:38,612   Average segmentation loss on training set: 0.0610
2022-01-10 10:25:38,612 Validation Data Eval:
2022-01-10 10:25:41,365   Average segmentation loss on validation set: 0.1516
2022-01-10 10:25:42,908 iteration 1530 : loss : 0.053808, loss_ce: 0.023780
 22%|██████▊                       | 90/400 [44:21<2:39:43, 30.91s/it]2022-01-10 10:25:44,584 iteration 1531 : loss : 0.073480, loss_ce: 0.032724
2022-01-10 10:25:46,213 iteration 1532 : loss : 0.076858, loss_ce: 0.028383
2022-01-10 10:25:47,751 iteration 1533 : loss : 0.050252, loss_ce: 0.019330
2022-01-10 10:25:49,316 iteration 1534 : loss : 0.048501, loss_ce: 0.023602
2022-01-10 10:25:50,852 iteration 1535 : loss : 0.060217, loss_ce: 0.024029
2022-01-10 10:25:52,455 iteration 1536 : loss : 0.051682, loss_ce: 0.016355
2022-01-10 10:25:54,010 iteration 1537 : loss : 0.054867, loss_ce: 0.030760
2022-01-10 10:25:55,477 iteration 1538 : loss : 0.101923, loss_ce: 0.033665
2022-01-10 10:25:56,994 iteration 1539 : loss : 0.046151, loss_ce: 0.022023
2022-01-10 10:25:58,584 iteration 1540 : loss : 0.057829, loss_ce: 0.028069
2022-01-10 10:26:00,221 iteration 1541 : loss : 0.041835, loss_ce: 0.016218
2022-01-10 10:26:01,912 iteration 1542 : loss : 0.064994, loss_ce: 0.026999
2022-01-10 10:26:03,511 iteration 1543 : loss : 0.073102, loss_ce: 0.016828
2022-01-10 10:26:05,116 iteration 1544 : loss : 0.046881, loss_ce: 0.020332
2022-01-10 10:26:06,629 iteration 1545 : loss : 0.061538, loss_ce: 0.027633
2022-01-10 10:26:08,187 iteration 1546 : loss : 0.060844, loss_ce: 0.020612
2022-01-10 10:26:09,797 iteration 1547 : loss : 0.045259, loss_ce: 0.019741
 23%|██████▊                       | 91/400 [44:48<2:32:59, 29.71s/it]2022-01-10 10:26:11,422 iteration 1548 : loss : 0.094707, loss_ce: 0.027695
2022-01-10 10:26:12,933 iteration 1549 : loss : 0.063597, loss_ce: 0.019129
2022-01-10 10:26:14,552 iteration 1550 : loss : 0.045836, loss_ce: 0.013154
2022-01-10 10:26:16,155 iteration 1551 : loss : 0.072673, loss_ce: 0.024335
2022-01-10 10:26:17,717 iteration 1552 : loss : 0.066948, loss_ce: 0.018959
2022-01-10 10:26:19,336 iteration 1553 : loss : 0.086684, loss_ce: 0.036781
2022-01-10 10:26:20,824 iteration 1554 : loss : 0.071188, loss_ce: 0.035647
2022-01-10 10:26:22,364 iteration 1555 : loss : 0.058530, loss_ce: 0.019988
2022-01-10 10:26:24,045 iteration 1556 : loss : 0.076448, loss_ce: 0.032071
2022-01-10 10:26:25,615 iteration 1557 : loss : 0.055312, loss_ce: 0.018845
2022-01-10 10:26:27,125 iteration 1558 : loss : 0.043154, loss_ce: 0.016555
2022-01-10 10:26:28,788 iteration 1559 : loss : 0.086089, loss_ce: 0.031396
2022-01-10 10:26:30,337 iteration 1560 : loss : 0.043292, loss_ce: 0.021336
2022-01-10 10:26:31,919 iteration 1561 : loss : 0.053794, loss_ce: 0.024094
2022-01-10 10:26:33,497 iteration 1562 : loss : 0.061248, loss_ce: 0.031097
2022-01-10 10:26:35,105 iteration 1563 : loss : 0.073425, loss_ce: 0.031149
2022-01-10 10:26:36,662 iteration 1564 : loss : 0.063512, loss_ce: 0.020565
 23%|██████▉                       | 92/400 [45:15<2:28:07, 28.85s/it]2022-01-10 10:26:38,258 iteration 1565 : loss : 0.058092, loss_ce: 0.032374
2022-01-10 10:26:39,879 iteration 1566 : loss : 0.063965, loss_ce: 0.018965
2022-01-10 10:26:41,473 iteration 1567 : loss : 0.053858, loss_ce: 0.023912
2022-01-10 10:26:43,168 iteration 1568 : loss : 0.059440, loss_ce: 0.019483
2022-01-10 10:26:44,754 iteration 1569 : loss : 0.062950, loss_ce: 0.027281
2022-01-10 10:26:46,356 iteration 1570 : loss : 0.049341, loss_ce: 0.019284
2022-01-10 10:26:47,945 iteration 1571 : loss : 0.072848, loss_ce: 0.027147
2022-01-10 10:26:49,627 iteration 1572 : loss : 0.073355, loss_ce: 0.026300
2022-01-10 10:26:51,125 iteration 1573 : loss : 0.042729, loss_ce: 0.019700
2022-01-10 10:26:52,740 iteration 1574 : loss : 0.039582, loss_ce: 0.014626
2022-01-10 10:26:54,419 iteration 1575 : loss : 0.057346, loss_ce: 0.024907
2022-01-10 10:26:56,071 iteration 1576 : loss : 0.043353, loss_ce: 0.019296
2022-01-10 10:26:57,577 iteration 1577 : loss : 0.050634, loss_ce: 0.018905
2022-01-10 10:26:59,130 iteration 1578 : loss : 0.048424, loss_ce: 0.017257
2022-01-10 10:27:00,770 iteration 1579 : loss : 0.060289, loss_ce: 0.023561
2022-01-10 10:27:02,389 iteration 1580 : loss : 0.074439, loss_ce: 0.030778
2022-01-10 10:27:04,106 iteration 1581 : loss : 0.042730, loss_ce: 0.017069
 23%|██████▉                       | 93/400 [45:43<2:25:28, 28.43s/it]2022-01-10 10:27:05,785 iteration 1582 : loss : 0.062599, loss_ce: 0.023197
2022-01-10 10:27:07,398 iteration 1583 : loss : 0.057373, loss_ce: 0.021617
2022-01-10 10:27:08,899 iteration 1584 : loss : 0.048052, loss_ce: 0.017028
2022-01-10 10:27:10,490 iteration 1585 : loss : 0.146345, loss_ce: 0.031613
2022-01-10 10:27:12,145 iteration 1586 : loss : 0.057458, loss_ce: 0.017472
2022-01-10 10:27:13,736 iteration 1587 : loss : 0.041928, loss_ce: 0.013477
2022-01-10 10:27:15,377 iteration 1588 : loss : 0.060366, loss_ce: 0.027174
2022-01-10 10:27:16,963 iteration 1589 : loss : 0.061998, loss_ce: 0.018504
2022-01-10 10:27:18,578 iteration 1590 : loss : 0.070311, loss_ce: 0.034738
2022-01-10 10:27:20,117 iteration 1591 : loss : 0.079071, loss_ce: 0.040561
2022-01-10 10:27:21,661 iteration 1592 : loss : 0.081051, loss_ce: 0.035042
2022-01-10 10:27:23,318 iteration 1593 : loss : 0.060454, loss_ce: 0.018181
2022-01-10 10:27:24,922 iteration 1594 : loss : 0.057835, loss_ce: 0.031750
2022-01-10 10:27:26,523 iteration 1595 : loss : 0.089505, loss_ce: 0.042533
2022-01-10 10:27:28,113 iteration 1596 : loss : 0.042079, loss_ce: 0.017038
2022-01-10 10:27:29,663 iteration 1597 : loss : 0.058388, loss_ce: 0.019721
2022-01-10 10:27:31,284 iteration 1598 : loss : 0.045402, loss_ce: 0.020565
 24%|███████                       | 94/400 [46:10<2:23:04, 28.05s/it]2022-01-10 10:27:32,957 iteration 1599 : loss : 0.037000, loss_ce: 0.017293
2022-01-10 10:27:34,514 iteration 1600 : loss : 0.065251, loss_ce: 0.027501
2022-01-10 10:27:36,097 iteration 1601 : loss : 0.086559, loss_ce: 0.037876
2022-01-10 10:27:37,643 iteration 1602 : loss : 0.059139, loss_ce: 0.027125
2022-01-10 10:27:39,307 iteration 1603 : loss : 0.056694, loss_ce: 0.022018
2022-01-10 10:27:40,876 iteration 1604 : loss : 0.057647, loss_ce: 0.026566
2022-01-10 10:27:42,537 iteration 1605 : loss : 0.060189, loss_ce: 0.022685
2022-01-10 10:27:44,172 iteration 1606 : loss : 0.044970, loss_ce: 0.014733
2022-01-10 10:27:45,756 iteration 1607 : loss : 0.044217, loss_ce: 0.019268
2022-01-10 10:27:47,366 iteration 1608 : loss : 0.036976, loss_ce: 0.013471
2022-01-10 10:27:48,979 iteration 1609 : loss : 0.041143, loss_ce: 0.014983
2022-01-10 10:27:50,552 iteration 1610 : loss : 0.064589, loss_ce: 0.029354
2022-01-10 10:27:52,162 iteration 1611 : loss : 0.063812, loss_ce: 0.026267
2022-01-10 10:27:53,757 iteration 1612 : loss : 0.047364, loss_ce: 0.021961
2022-01-10 10:27:55,366 iteration 1613 : loss : 0.050438, loss_ce: 0.018915
2022-01-10 10:27:56,977 iteration 1614 : loss : 0.038151, loss_ce: 0.012546
2022-01-10 10:27:56,978 Training Data Eval:
2022-01-10 10:28:04,944   Average segmentation loss on training set: 0.0376
2022-01-10 10:28:04,945 Validation Data Eval:
2022-01-10 10:28:07,691   Average segmentation loss on validation set: 0.0881
2022-01-10 10:28:09,309 iteration 1615 : loss : 0.068349, loss_ce: 0.034915
 24%|███████▏                      | 95/400 [46:48<2:37:48, 31.05s/it]2022-01-10 10:28:10,921 iteration 1616 : loss : 0.039869, loss_ce: 0.012306
2022-01-10 10:28:12,443 iteration 1617 : loss : 0.033157, loss_ce: 0.014854
2022-01-10 10:28:14,021 iteration 1618 : loss : 0.026567, loss_ce: 0.009524
2022-01-10 10:28:15,643 iteration 1619 : loss : 0.037170, loss_ce: 0.018689
2022-01-10 10:28:17,161 iteration 1620 : loss : 0.048886, loss_ce: 0.019977
2022-01-10 10:28:18,779 iteration 1621 : loss : 0.042179, loss_ce: 0.016832
2022-01-10 10:28:20,330 iteration 1622 : loss : 0.043538, loss_ce: 0.016549
2022-01-10 10:28:21,843 iteration 1623 : loss : 0.036623, loss_ce: 0.016681
2022-01-10 10:28:23,433 iteration 1624 : loss : 0.065452, loss_ce: 0.027995
2022-01-10 10:28:24,923 iteration 1625 : loss : 0.037261, loss_ce: 0.020452
2022-01-10 10:28:26,466 iteration 1626 : loss : 0.042629, loss_ce: 0.017870
2022-01-10 10:28:27,986 iteration 1627 : loss : 0.064189, loss_ce: 0.018769
2022-01-10 10:28:29,588 iteration 1628 : loss : 0.049831, loss_ce: 0.014850
2022-01-10 10:28:31,187 iteration 1629 : loss : 0.049523, loss_ce: 0.022030
2022-01-10 10:28:32,692 iteration 1630 : loss : 0.044837, loss_ce: 0.017569
2022-01-10 10:28:34,268 iteration 1631 : loss : 0.055800, loss_ce: 0.021194
2022-01-10 10:28:35,734 iteration 1632 : loss : 0.032501, loss_ce: 0.012612
 24%|███████▏                      | 96/400 [47:14<2:30:16, 29.66s/it]2022-01-10 10:28:37,431 iteration 1633 : loss : 0.037227, loss_ce: 0.013667
2022-01-10 10:28:39,042 iteration 1634 : loss : 0.039645, loss_ce: 0.016020
2022-01-10 10:28:40,602 iteration 1635 : loss : 0.045549, loss_ce: 0.018813
2022-01-10 10:28:42,074 iteration 1636 : loss : 0.038088, loss_ce: 0.014302
2022-01-10 10:28:43,706 iteration 1637 : loss : 0.049020, loss_ce: 0.017834
2022-01-10 10:28:45,302 iteration 1638 : loss : 0.039449, loss_ce: 0.014958
2022-01-10 10:28:46,945 iteration 1639 : loss : 0.050101, loss_ce: 0.020280
2022-01-10 10:28:48,524 iteration 1640 : loss : 0.050100, loss_ce: 0.021652
2022-01-10 10:28:50,137 iteration 1641 : loss : 0.062542, loss_ce: 0.025258
2022-01-10 10:28:51,827 iteration 1642 : loss : 0.070912, loss_ce: 0.025471
2022-01-10 10:28:53,438 iteration 1643 : loss : 0.029061, loss_ce: 0.011097
2022-01-10 10:28:55,037 iteration 1644 : loss : 0.048657, loss_ce: 0.018715
2022-01-10 10:28:56,523 iteration 1645 : loss : 0.047410, loss_ce: 0.015573
2022-01-10 10:28:58,079 iteration 1646 : loss : 0.037605, loss_ce: 0.014655
2022-01-10 10:28:59,732 iteration 1647 : loss : 0.050540, loss_ce: 0.027122
2022-01-10 10:29:01,305 iteration 1648 : loss : 0.070765, loss_ce: 0.032876
2022-01-10 10:29:02,979 iteration 1649 : loss : 0.062067, loss_ce: 0.016667
 24%|███████▎                      | 97/400 [47:42<2:26:06, 28.93s/it]2022-01-10 10:29:04,628 iteration 1650 : loss : 0.050921, loss_ce: 0.014284
2022-01-10 10:29:06,292 iteration 1651 : loss : 0.060320, loss_ce: 0.028869
2022-01-10 10:29:07,951 iteration 1652 : loss : 0.079282, loss_ce: 0.022702
2022-01-10 10:29:09,589 iteration 1653 : loss : 0.064683, loss_ce: 0.035583
2022-01-10 10:29:11,167 iteration 1654 : loss : 0.044096, loss_ce: 0.014692
2022-01-10 10:29:12,732 iteration 1655 : loss : 0.073452, loss_ce: 0.022991
2022-01-10 10:29:14,295 iteration 1656 : loss : 0.030525, loss_ce: 0.013774
2022-01-10 10:29:15,959 iteration 1657 : loss : 0.057203, loss_ce: 0.028539
2022-01-10 10:29:17,484 iteration 1658 : loss : 0.032060, loss_ce: 0.012879
2022-01-10 10:29:19,072 iteration 1659 : loss : 0.060944, loss_ce: 0.023810
2022-01-10 10:29:20,788 iteration 1660 : loss : 0.072420, loss_ce: 0.030657
2022-01-10 10:29:22,385 iteration 1661 : loss : 0.047029, loss_ce: 0.018984
2022-01-10 10:29:23,946 iteration 1662 : loss : 0.054112, loss_ce: 0.021256
2022-01-10 10:29:25,587 iteration 1663 : loss : 0.049508, loss_ce: 0.020923
2022-01-10 10:29:27,230 iteration 1664 : loss : 0.048061, loss_ce: 0.017722
2022-01-10 10:29:28,780 iteration 1665 : loss : 0.029571, loss_ce: 0.012871
2022-01-10 10:29:30,322 iteration 1666 : loss : 0.042295, loss_ce: 0.017244
 24%|███████▎                      | 98/400 [48:09<2:23:14, 28.46s/it]2022-01-10 10:29:31,959 iteration 1667 : loss : 0.040491, loss_ce: 0.019705
2022-01-10 10:29:33,609 iteration 1668 : loss : 0.053599, loss_ce: 0.023468
2022-01-10 10:29:35,185 iteration 1669 : loss : 0.045548, loss_ce: 0.022326
2022-01-10 10:29:36,756 iteration 1670 : loss : 0.049411, loss_ce: 0.017413
2022-01-10 10:29:38,369 iteration 1671 : loss : 0.038863, loss_ce: 0.016784
2022-01-10 10:29:39,931 iteration 1672 : loss : 0.038977, loss_ce: 0.013321
2022-01-10 10:29:41,472 iteration 1673 : loss : 0.033374, loss_ce: 0.013195
2022-01-10 10:29:43,113 iteration 1674 : loss : 0.041083, loss_ce: 0.017556
2022-01-10 10:29:44,676 iteration 1675 : loss : 0.038245, loss_ce: 0.014779
2022-01-10 10:29:46,315 iteration 1676 : loss : 0.036320, loss_ce: 0.017611
2022-01-10 10:29:47,964 iteration 1677 : loss : 0.043617, loss_ce: 0.017540
2022-01-10 10:29:49,583 iteration 1678 : loss : 0.076988, loss_ce: 0.024648
2022-01-10 10:29:51,213 iteration 1679 : loss : 0.038984, loss_ce: 0.012987
2022-01-10 10:29:52,826 iteration 1680 : loss : 0.052848, loss_ce: 0.026971
2022-01-10 10:29:54,414 iteration 1681 : loss : 0.064566, loss_ce: 0.016142
2022-01-10 10:29:55,958 iteration 1682 : loss : 0.050304, loss_ce: 0.024931
2022-01-10 10:29:57,575 iteration 1683 : loss : 0.061098, loss_ce: 0.028295
 25%|███████▍                      | 99/400 [48:36<2:20:57, 28.10s/it]2022-01-10 10:29:59,232 iteration 1684 : loss : 0.036654, loss_ce: 0.013008
2022-01-10 10:30:00,779 iteration 1685 : loss : 0.047688, loss_ce: 0.018186
2022-01-10 10:30:02,417 iteration 1686 : loss : 0.047914, loss_ce: 0.019483
2022-01-10 10:30:03,953 iteration 1687 : loss : 0.040646, loss_ce: 0.014054
2022-01-10 10:30:05,560 iteration 1688 : loss : 0.040236, loss_ce: 0.014930
2022-01-10 10:30:07,171 iteration 1689 : loss : 0.042392, loss_ce: 0.019586
2022-01-10 10:30:08,686 iteration 1690 : loss : 0.039518, loss_ce: 0.013321
2022-01-10 10:30:10,203 iteration 1691 : loss : 0.052720, loss_ce: 0.019155
2022-01-10 10:30:11,794 iteration 1692 : loss : 0.033592, loss_ce: 0.014469
2022-01-10 10:30:13,436 iteration 1693 : loss : 0.049774, loss_ce: 0.023270
2022-01-10 10:30:15,076 iteration 1694 : loss : 0.053160, loss_ce: 0.020999
2022-01-10 10:30:16,640 iteration 1695 : loss : 0.031012, loss_ce: 0.009968
2022-01-10 10:30:18,232 iteration 1696 : loss : 0.043373, loss_ce: 0.020825
2022-01-10 10:30:19,846 iteration 1697 : loss : 0.042543, loss_ce: 0.019823
2022-01-10 10:30:21,459 iteration 1698 : loss : 0.027988, loss_ce: 0.010525
2022-01-10 10:30:23,024 iteration 1699 : loss : 0.039915, loss_ce: 0.018477
2022-01-10 10:30:23,024 Training Data Eval:
2022-01-10 10:30:30,994   Average segmentation loss on training set: 0.0312
2022-01-10 10:30:30,994 Validation Data Eval:
2022-01-10 10:30:33,741   Average segmentation loss on validation set: 0.0918
2022-01-10 10:30:35,331 iteration 1700 : loss : 0.041025, loss_ce: 0.016413
 25%|███████▎                     | 100/400 [49:14<2:34:58, 31.00s/it]2022-01-10 10:30:36,967 iteration 1701 : loss : 0.037506, loss_ce: 0.016111
2022-01-10 10:30:38,551 iteration 1702 : loss : 0.057616, loss_ce: 0.020774
2022-01-10 10:30:40,200 iteration 1703 : loss : 0.065290, loss_ce: 0.033420
2022-01-10 10:30:41,771 iteration 1704 : loss : 0.045307, loss_ce: 0.022388
2022-01-10 10:30:43,387 iteration 1705 : loss : 0.043794, loss_ce: 0.017174
2022-01-10 10:30:45,046 iteration 1706 : loss : 0.065889, loss_ce: 0.025215
2022-01-10 10:30:46,613 iteration 1707 : loss : 0.040878, loss_ce: 0.019779
2022-01-10 10:30:48,190 iteration 1708 : loss : 0.099919, loss_ce: 0.033897
2022-01-10 10:30:49,766 iteration 1709 : loss : 0.041642, loss_ce: 0.019182
2022-01-10 10:30:51,397 iteration 1710 : loss : 0.051086, loss_ce: 0.018925
2022-01-10 10:30:52,974 iteration 1711 : loss : 0.050420, loss_ce: 0.024879
2022-01-10 10:30:54,486 iteration 1712 : loss : 0.033585, loss_ce: 0.017105
2022-01-10 10:30:56,094 iteration 1713 : loss : 0.047602, loss_ce: 0.015128
2022-01-10 10:30:57,761 iteration 1714 : loss : 0.043842, loss_ce: 0.012103
2022-01-10 10:30:59,402 iteration 1715 : loss : 0.044180, loss_ce: 0.020089
2022-01-10 10:31:00,911 iteration 1716 : loss : 0.041281, loss_ce: 0.012220
2022-01-10 10:31:02,547 iteration 1717 : loss : 0.041941, loss_ce: 0.013848
 25%|███████▎                     | 101/400 [49:41<2:28:47, 29.86s/it]2022-01-10 10:31:04,207 iteration 1718 : loss : 0.049090, loss_ce: 0.024819
2022-01-10 10:31:05,777 iteration 1719 : loss : 0.069743, loss_ce: 0.014953
2022-01-10 10:31:07,397 iteration 1720 : loss : 0.045784, loss_ce: 0.016615
2022-01-10 10:31:08,950 iteration 1721 : loss : 0.041338, loss_ce: 0.012150
2022-01-10 10:31:10,571 iteration 1722 : loss : 0.058244, loss_ce: 0.018274
2022-01-10 10:31:12,103 iteration 1723 : loss : 0.076427, loss_ce: 0.022037
2022-01-10 10:31:13,706 iteration 1724 : loss : 0.029812, loss_ce: 0.011042
2022-01-10 10:31:15,306 iteration 1725 : loss : 0.055638, loss_ce: 0.017596
2022-01-10 10:31:16,902 iteration 1726 : loss : 0.037798, loss_ce: 0.014750
2022-01-10 10:31:18,566 iteration 1727 : loss : 0.069199, loss_ce: 0.032833
2022-01-10 10:31:20,067 iteration 1728 : loss : 0.034168, loss_ce: 0.011382
2022-01-10 10:31:21,678 iteration 1729 : loss : 0.040231, loss_ce: 0.014043
2022-01-10 10:31:23,325 iteration 1730 : loss : 0.036400, loss_ce: 0.016290
2022-01-10 10:31:24,896 iteration 1731 : loss : 0.055405, loss_ce: 0.030640
2022-01-10 10:31:26,454 iteration 1732 : loss : 0.041096, loss_ce: 0.016761
2022-01-10 10:31:27,994 iteration 1733 : loss : 0.034747, loss_ce: 0.014417
2022-01-10 10:31:29,557 iteration 1734 : loss : 0.042627, loss_ce: 0.017607
 26%|███████▍                     | 102/400 [50:08<2:24:03, 29.00s/it]2022-01-10 10:31:31,335 iteration 1735 : loss : 0.045095, loss_ce: 0.020160
2022-01-10 10:31:32,897 iteration 1736 : loss : 0.043072, loss_ce: 0.018888
2022-01-10 10:31:34,616 iteration 1737 : loss : 0.058188, loss_ce: 0.023461
2022-01-10 10:31:36,208 iteration 1738 : loss : 0.032895, loss_ce: 0.015324
2022-01-10 10:31:37,731 iteration 1739 : loss : 0.032351, loss_ce: 0.012380
2022-01-10 10:31:39,366 iteration 1740 : loss : 0.035609, loss_ce: 0.015891
2022-01-10 10:31:40,993 iteration 1741 : loss : 0.032921, loss_ce: 0.012327
2022-01-10 10:31:42,623 iteration 1742 : loss : 0.048699, loss_ce: 0.019474
2022-01-10 10:31:44,131 iteration 1743 : loss : 0.047232, loss_ce: 0.014771
2022-01-10 10:31:45,785 iteration 1744 : loss : 0.058564, loss_ce: 0.023972
2022-01-10 10:31:47,433 iteration 1745 : loss : 0.046361, loss_ce: 0.018654
2022-01-10 10:31:49,031 iteration 1746 : loss : 0.054091, loss_ce: 0.027183
2022-01-10 10:31:50,580 iteration 1747 : loss : 0.040584, loss_ce: 0.016540
2022-01-10 10:31:52,183 iteration 1748 : loss : 0.050366, loss_ce: 0.017426
2022-01-10 10:31:53,666 iteration 1749 : loss : 0.028267, loss_ce: 0.009412
2022-01-10 10:31:55,260 iteration 1750 : loss : 0.095268, loss_ce: 0.019553
2022-01-10 10:31:56,943 iteration 1751 : loss : 0.047214, loss_ce: 0.017845
 26%|███████▍                     | 103/400 [50:36<2:21:10, 28.52s/it]2022-01-10 10:31:58,663 iteration 1752 : loss : 0.102977, loss_ce: 0.027848
2022-01-10 10:32:00,156 iteration 1753 : loss : 0.044142, loss_ce: 0.014106
2022-01-10 10:32:01,808 iteration 1754 : loss : 0.042327, loss_ce: 0.016274
2022-01-10 10:32:03,350 iteration 1755 : loss : 0.035821, loss_ce: 0.015512
2022-01-10 10:32:04,945 iteration 1756 : loss : 0.049689, loss_ce: 0.017492
2022-01-10 10:32:06,483 iteration 1757 : loss : 0.034246, loss_ce: 0.013933
2022-01-10 10:32:08,047 iteration 1758 : loss : 0.038753, loss_ce: 0.020338
2022-01-10 10:32:09,585 iteration 1759 : loss : 0.034754, loss_ce: 0.011665
2022-01-10 10:32:11,133 iteration 1760 : loss : 0.039196, loss_ce: 0.014103
2022-01-10 10:32:12,764 iteration 1761 : loss : 0.053366, loss_ce: 0.015847
2022-01-10 10:32:14,261 iteration 1762 : loss : 0.035529, loss_ce: 0.014640
2022-01-10 10:32:15,788 iteration 1763 : loss : 0.040532, loss_ce: 0.016276
2022-01-10 10:32:17,263 iteration 1764 : loss : 0.030257, loss_ce: 0.009148
2022-01-10 10:32:18,863 iteration 1765 : loss : 0.038560, loss_ce: 0.021154
2022-01-10 10:32:20,511 iteration 1766 : loss : 0.055155, loss_ce: 0.029106
2022-01-10 10:32:22,067 iteration 1767 : loss : 0.036717, loss_ce: 0.016700
2022-01-10 10:32:23,666 iteration 1768 : loss : 0.044293, loss_ce: 0.017173
 26%|███████▌                     | 104/400 [51:02<2:18:02, 27.98s/it]2022-01-10 10:32:25,320 iteration 1769 : loss : 0.041868, loss_ce: 0.015804
2022-01-10 10:32:26,873 iteration 1770 : loss : 0.036380, loss_ce: 0.015118
2022-01-10 10:32:28,368 iteration 1771 : loss : 0.046481, loss_ce: 0.019330
2022-01-10 10:32:29,898 iteration 1772 : loss : 0.038775, loss_ce: 0.019335
2022-01-10 10:32:31,522 iteration 1773 : loss : 0.069792, loss_ce: 0.022129
2022-01-10 10:32:33,119 iteration 1774 : loss : 0.032820, loss_ce: 0.011617
2022-01-10 10:32:34,741 iteration 1775 : loss : 0.049898, loss_ce: 0.022090
2022-01-10 10:32:36,289 iteration 1776 : loss : 0.034066, loss_ce: 0.016757
2022-01-10 10:32:37,816 iteration 1777 : loss : 0.034289, loss_ce: 0.013588
2022-01-10 10:32:39,328 iteration 1778 : loss : 0.038800, loss_ce: 0.013518
2022-01-10 10:32:40,958 iteration 1779 : loss : 0.037609, loss_ce: 0.013887
2022-01-10 10:32:42,547 iteration 1780 : loss : 0.050818, loss_ce: 0.024343
2022-01-10 10:32:44,071 iteration 1781 : loss : 0.042522, loss_ce: 0.017348
2022-01-10 10:32:45,688 iteration 1782 : loss : 0.040534, loss_ce: 0.015819
2022-01-10 10:32:47,294 iteration 1783 : loss : 0.036723, loss_ce: 0.013744
2022-01-10 10:32:48,844 iteration 1784 : loss : 0.042333, loss_ce: 0.015677
2022-01-10 10:32:48,845 Training Data Eval:
2022-01-10 10:32:56,814   Average segmentation loss on training set: 0.0280
2022-01-10 10:32:56,815 Validation Data Eval:
2022-01-10 10:32:59,567   Average segmentation loss on validation set: 0.0723
2022-01-10 10:33:05,334 Found new lowest validation loss at iteration 1784! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 10:33:06,877 iteration 1785 : loss : 0.037912, loss_ce: 0.015111
 26%|███████▌                     | 105/400 [51:45<2:40:01, 32.55s/it]2022-01-10 10:33:08,510 iteration 1786 : loss : 0.036120, loss_ce: 0.016785
2022-01-10 10:33:10,013 iteration 1787 : loss : 0.052311, loss_ce: 0.020052
2022-01-10 10:33:11,642 iteration 1788 : loss : 0.036854, loss_ce: 0.020488
2022-01-10 10:33:13,249 iteration 1789 : loss : 0.055905, loss_ce: 0.021447
2022-01-10 10:33:14,844 iteration 1790 : loss : 0.037264, loss_ce: 0.016354
2022-01-10 10:33:16,495 iteration 1791 : loss : 0.050130, loss_ce: 0.021198
2022-01-10 10:33:18,033 iteration 1792 : loss : 0.041056, loss_ce: 0.016287
2022-01-10 10:33:19,583 iteration 1793 : loss : 0.061274, loss_ce: 0.025531
2022-01-10 10:33:21,159 iteration 1794 : loss : 0.027749, loss_ce: 0.010694
2022-01-10 10:33:22,781 iteration 1795 : loss : 0.043058, loss_ce: 0.015798
2022-01-10 10:33:24,351 iteration 1796 : loss : 0.051733, loss_ce: 0.026422
2022-01-10 10:33:26,014 iteration 1797 : loss : 0.054866, loss_ce: 0.017126
2022-01-10 10:33:27,668 iteration 1798 : loss : 0.040529, loss_ce: 0.015462
2022-01-10 10:33:29,229 iteration 1799 : loss : 0.043758, loss_ce: 0.017199
2022-01-10 10:33:30,827 iteration 1800 : loss : 0.035203, loss_ce: 0.013941
2022-01-10 10:33:32,354 iteration 1801 : loss : 0.045019, loss_ce: 0.015564
2022-01-10 10:33:33,916 iteration 1802 : loss : 0.055431, loss_ce: 0.018074
 26%|███████▋                     | 106/400 [52:12<2:31:23, 30.90s/it]2022-01-10 10:33:35,638 iteration 1803 : loss : 0.048681, loss_ce: 0.017198
2022-01-10 10:33:37,289 iteration 1804 : loss : 0.032381, loss_ce: 0.010876
2022-01-10 10:33:38,842 iteration 1805 : loss : 0.041921, loss_ce: 0.012260
2022-01-10 10:33:40,392 iteration 1806 : loss : 0.048920, loss_ce: 0.024203
2022-01-10 10:33:41,942 iteration 1807 : loss : 0.066424, loss_ce: 0.023883
2022-01-10 10:33:43,505 iteration 1808 : loss : 0.025018, loss_ce: 0.010996
2022-01-10 10:33:45,048 iteration 1809 : loss : 0.029704, loss_ce: 0.011803
2022-01-10 10:33:46,523 iteration 1810 : loss : 0.028573, loss_ce: 0.011708
2022-01-10 10:33:48,142 iteration 1811 : loss : 0.039637, loss_ce: 0.018012
2022-01-10 10:33:49,792 iteration 1812 : loss : 0.039445, loss_ce: 0.014432
2022-01-10 10:33:51,513 iteration 1813 : loss : 0.053622, loss_ce: 0.020035
2022-01-10 10:33:53,136 iteration 1814 : loss : 0.034570, loss_ce: 0.017220
2022-01-10 10:33:54,670 iteration 1815 : loss : 0.034470, loss_ce: 0.010436
2022-01-10 10:33:56,166 iteration 1816 : loss : 0.034165, loss_ce: 0.013410
2022-01-10 10:33:57,815 iteration 1817 : loss : 0.046859, loss_ce: 0.017544
2022-01-10 10:33:59,399 iteration 1818 : loss : 0.059874, loss_ce: 0.022732
2022-01-10 10:34:01,027 iteration 1819 : loss : 0.055668, loss_ce: 0.028542
 27%|███████▊                     | 107/400 [52:40<2:25:19, 29.76s/it]2022-01-10 10:34:02,759 iteration 1820 : loss : 0.049698, loss_ce: 0.024092
2022-01-10 10:34:04,325 iteration 1821 : loss : 0.029941, loss_ce: 0.013715
2022-01-10 10:34:05,915 iteration 1822 : loss : 0.041018, loss_ce: 0.013642
2022-01-10 10:34:07,414 iteration 1823 : loss : 0.034695, loss_ce: 0.010581
2022-01-10 10:34:08,876 iteration 1824 : loss : 0.034470, loss_ce: 0.014392
2022-01-10 10:34:10,494 iteration 1825 : loss : 0.076900, loss_ce: 0.028027
2022-01-10 10:34:11,979 iteration 1826 : loss : 0.035980, loss_ce: 0.017448
2022-01-10 10:34:13,655 iteration 1827 : loss : 0.141358, loss_ce: 0.058188
2022-01-10 10:34:15,217 iteration 1828 : loss : 0.037732, loss_ce: 0.015668
2022-01-10 10:34:16,777 iteration 1829 : loss : 0.098615, loss_ce: 0.040135
2022-01-10 10:34:18,335 iteration 1830 : loss : 0.038279, loss_ce: 0.017737
2022-01-10 10:34:19,984 iteration 1831 : loss : 0.057388, loss_ce: 0.033124
2022-01-10 10:34:21,548 iteration 1832 : loss : 0.038966, loss_ce: 0.013788
2022-01-10 10:34:23,125 iteration 1833 : loss : 0.034947, loss_ce: 0.017201
2022-01-10 10:34:24,801 iteration 1834 : loss : 0.044138, loss_ce: 0.017024
2022-01-10 10:34:26,358 iteration 1835 : loss : 0.050360, loss_ce: 0.030114
2022-01-10 10:34:27,939 iteration 1836 : loss : 0.053386, loss_ce: 0.021120
 27%|███████▊                     | 108/400 [53:07<2:20:40, 28.91s/it]2022-01-10 10:34:29,636 iteration 1837 : loss : 0.028031, loss_ce: 0.011883
2022-01-10 10:34:31,281 iteration 1838 : loss : 0.041663, loss_ce: 0.019079
2022-01-10 10:34:32,840 iteration 1839 : loss : 0.056094, loss_ce: 0.022997
2022-01-10 10:34:34,345 iteration 1840 : loss : 0.047808, loss_ce: 0.023344
2022-01-10 10:34:35,960 iteration 1841 : loss : 0.082880, loss_ce: 0.036123
2022-01-10 10:34:37,598 iteration 1842 : loss : 0.057280, loss_ce: 0.021032
2022-01-10 10:34:39,110 iteration 1843 : loss : 0.039292, loss_ce: 0.019282
2022-01-10 10:34:40,692 iteration 1844 : loss : 0.047364, loss_ce: 0.017860
2022-01-10 10:34:42,205 iteration 1845 : loss : 0.034760, loss_ce: 0.015012
2022-01-10 10:34:43,878 iteration 1846 : loss : 0.039228, loss_ce: 0.017851
2022-01-10 10:34:45,501 iteration 1847 : loss : 0.054521, loss_ce: 0.019960
2022-01-10 10:34:47,088 iteration 1848 : loss : 0.038617, loss_ce: 0.017260
2022-01-10 10:34:48,583 iteration 1849 : loss : 0.041595, loss_ce: 0.019448
2022-01-10 10:34:50,164 iteration 1850 : loss : 0.064753, loss_ce: 0.016244
2022-01-10 10:34:51,780 iteration 1851 : loss : 0.038403, loss_ce: 0.017031
2022-01-10 10:34:53,376 iteration 1852 : loss : 0.041025, loss_ce: 0.019260
2022-01-10 10:34:54,967 iteration 1853 : loss : 0.057099, loss_ce: 0.013066
 27%|███████▉                     | 109/400 [53:34<2:17:28, 28.34s/it]2022-01-10 10:34:56,591 iteration 1854 : loss : 0.038644, loss_ce: 0.014315
2022-01-10 10:34:58,159 iteration 1855 : loss : 0.042631, loss_ce: 0.013663
2022-01-10 10:34:59,748 iteration 1856 : loss : 0.040516, loss_ce: 0.009677
2022-01-10 10:35:01,332 iteration 1857 : loss : 0.062079, loss_ce: 0.025838
2022-01-10 10:35:02,899 iteration 1858 : loss : 0.051391, loss_ce: 0.025337
2022-01-10 10:35:04,425 iteration 1859 : loss : 0.039509, loss_ce: 0.018328
2022-01-10 10:35:06,128 iteration 1860 : loss : 0.068642, loss_ce: 0.037719
2022-01-10 10:35:07,742 iteration 1861 : loss : 0.052132, loss_ce: 0.021358
2022-01-10 10:35:09,320 iteration 1862 : loss : 0.039359, loss_ce: 0.016892
2022-01-10 10:35:10,954 iteration 1863 : loss : 0.052004, loss_ce: 0.019109
2022-01-10 10:35:12,557 iteration 1864 : loss : 0.043173, loss_ce: 0.017129
2022-01-10 10:35:14,237 iteration 1865 : loss : 0.043650, loss_ce: 0.018867
2022-01-10 10:35:15,874 iteration 1866 : loss : 0.045250, loss_ce: 0.024598
2022-01-10 10:35:17,479 iteration 1867 : loss : 0.118398, loss_ce: 0.038510
2022-01-10 10:35:19,034 iteration 1868 : loss : 0.046223, loss_ce: 0.023965
2022-01-10 10:35:20,614 iteration 1869 : loss : 0.048847, loss_ce: 0.015996
2022-01-10 10:35:20,614 Training Data Eval:
2022-01-10 10:35:28,573   Average segmentation loss on training set: 0.0687
2022-01-10 10:35:28,573 Validation Data Eval:
2022-01-10 10:35:31,325   Average segmentation loss on validation set: 0.2112
2022-01-10 10:35:32,834 iteration 1870 : loss : 0.038912, loss_ce: 0.011259
 28%|███████▉                     | 110/400 [54:11<2:30:47, 31.20s/it]2022-01-10 10:35:34,455 iteration 1871 : loss : 0.049034, loss_ce: 0.023801
2022-01-10 10:35:35,988 iteration 1872 : loss : 0.054901, loss_ce: 0.023169
2022-01-10 10:35:37,570 iteration 1873 : loss : 0.058237, loss_ce: 0.027408
2022-01-10 10:35:39,199 iteration 1874 : loss : 0.043175, loss_ce: 0.020880
2022-01-10 10:35:40,860 iteration 1875 : loss : 0.038549, loss_ce: 0.020611
2022-01-10 10:35:42,524 iteration 1876 : loss : 0.047512, loss_ce: 0.017430
2022-01-10 10:35:44,165 iteration 1877 : loss : 0.043126, loss_ce: 0.014747
2022-01-10 10:35:45,700 iteration 1878 : loss : 0.056831, loss_ce: 0.021279
2022-01-10 10:35:47,241 iteration 1879 : loss : 0.041467, loss_ce: 0.019073
2022-01-10 10:35:48,777 iteration 1880 : loss : 0.038560, loss_ce: 0.015715
2022-01-10 10:35:50,302 iteration 1881 : loss : 0.037670, loss_ce: 0.013497
2022-01-10 10:35:51,885 iteration 1882 : loss : 0.047155, loss_ce: 0.014094
2022-01-10 10:35:53,397 iteration 1883 : loss : 0.033183, loss_ce: 0.013522
2022-01-10 10:35:55,038 iteration 1884 : loss : 0.044571, loss_ce: 0.019327
2022-01-10 10:35:56,567 iteration 1885 : loss : 0.028721, loss_ce: 0.010590
2022-01-10 10:35:58,171 iteration 1886 : loss : 0.041389, loss_ce: 0.016597
2022-01-10 10:35:59,708 iteration 1887 : loss : 0.076844, loss_ce: 0.039366
 28%|████████                     | 111/400 [54:38<2:24:01, 29.90s/it]2022-01-10 10:36:01,381 iteration 1888 : loss : 0.054933, loss_ce: 0.019814
2022-01-10 10:36:03,033 iteration 1889 : loss : 0.045984, loss_ce: 0.019984
2022-01-10 10:36:04,542 iteration 1890 : loss : 0.025986, loss_ce: 0.008947
2022-01-10 10:36:06,125 iteration 1891 : loss : 0.043580, loss_ce: 0.024410
2022-01-10 10:36:07,697 iteration 1892 : loss : 0.037087, loss_ce: 0.012601
2022-01-10 10:36:09,322 iteration 1893 : loss : 0.074755, loss_ce: 0.028073
2022-01-10 10:36:10,897 iteration 1894 : loss : 0.058634, loss_ce: 0.023597
2022-01-10 10:36:12,499 iteration 1895 : loss : 0.042278, loss_ce: 0.014688
2022-01-10 10:36:14,091 iteration 1896 : loss : 0.041415, loss_ce: 0.019177
2022-01-10 10:36:15,677 iteration 1897 : loss : 0.078442, loss_ce: 0.028520
2022-01-10 10:36:17,349 iteration 1898 : loss : 0.046648, loss_ce: 0.022580
2022-01-10 10:36:18,860 iteration 1899 : loss : 0.029166, loss_ce: 0.012449
2022-01-10 10:36:20,520 iteration 1900 : loss : 0.061799, loss_ce: 0.031282
2022-01-10 10:36:22,207 iteration 1901 : loss : 0.045817, loss_ce: 0.015963
2022-01-10 10:36:23,785 iteration 1902 : loss : 0.044410, loss_ce: 0.014853
2022-01-10 10:36:25,419 iteration 1903 : loss : 0.045059, loss_ce: 0.020674
2022-01-10 10:36:27,055 iteration 1904 : loss : 0.057975, loss_ce: 0.020183
 28%|████████                     | 112/400 [55:06<2:19:51, 29.14s/it]2022-01-10 10:36:28,751 iteration 1905 : loss : 0.055759, loss_ce: 0.022925
2022-01-10 10:36:30,344 iteration 1906 : loss : 0.041616, loss_ce: 0.012534
2022-01-10 10:36:31,921 iteration 1907 : loss : 0.037582, loss_ce: 0.013600
2022-01-10 10:36:33,634 iteration 1908 : loss : 0.044599, loss_ce: 0.014665
2022-01-10 10:36:35,247 iteration 1909 : loss : 0.058168, loss_ce: 0.019490
2022-01-10 10:36:36,864 iteration 1910 : loss : 0.087515, loss_ce: 0.018922
2022-01-10 10:36:38,501 iteration 1911 : loss : 0.034584, loss_ce: 0.011560
2022-01-10 10:36:40,007 iteration 1912 : loss : 0.033881, loss_ce: 0.011816
2022-01-10 10:36:41,591 iteration 1913 : loss : 0.047539, loss_ce: 0.020312
2022-01-10 10:36:43,182 iteration 1914 : loss : 0.036430, loss_ce: 0.016026
2022-01-10 10:36:44,793 iteration 1915 : loss : 0.053364, loss_ce: 0.022901
2022-01-10 10:36:46,378 iteration 1916 : loss : 0.057525, loss_ce: 0.021472
2022-01-10 10:36:47,959 iteration 1917 : loss : 0.037336, loss_ce: 0.017648
2022-01-10 10:36:49,570 iteration 1918 : loss : 0.038006, loss_ce: 0.012425
2022-01-10 10:36:51,127 iteration 1919 : loss : 0.070104, loss_ce: 0.025143
2022-01-10 10:36:52,683 iteration 1920 : loss : 0.041127, loss_ce: 0.017496
2022-01-10 10:36:54,339 iteration 1921 : loss : 0.048300, loss_ce: 0.023446
 28%|████████▏                    | 113/400 [55:33<2:16:42, 28.58s/it]2022-01-10 10:36:55,952 iteration 1922 : loss : 0.034864, loss_ce: 0.014892
2022-01-10 10:36:57,538 iteration 1923 : loss : 0.034750, loss_ce: 0.017532
2022-01-10 10:36:59,179 iteration 1924 : loss : 0.036440, loss_ce: 0.016763
2022-01-10 10:37:00,775 iteration 1925 : loss : 0.064896, loss_ce: 0.025916
2022-01-10 10:37:02,333 iteration 1926 : loss : 0.026926, loss_ce: 0.011215
2022-01-10 10:37:03,891 iteration 1927 : loss : 0.042431, loss_ce: 0.016858
2022-01-10 10:37:05,563 iteration 1928 : loss : 0.040423, loss_ce: 0.019326
2022-01-10 10:37:07,102 iteration 1929 : loss : 0.039291, loss_ce: 0.013983
2022-01-10 10:37:08,639 iteration 1930 : loss : 0.031703, loss_ce: 0.013547
2022-01-10 10:37:10,247 iteration 1931 : loss : 0.061869, loss_ce: 0.014179
2022-01-10 10:37:11,891 iteration 1932 : loss : 0.047221, loss_ce: 0.016308
2022-01-10 10:37:13,543 iteration 1933 : loss : 0.060451, loss_ce: 0.030769
2022-01-10 10:37:15,034 iteration 1934 : loss : 0.028192, loss_ce: 0.012401
2022-01-10 10:37:16,633 iteration 1935 : loss : 0.062070, loss_ce: 0.020692
2022-01-10 10:37:18,292 iteration 1936 : loss : 0.038207, loss_ce: 0.018096
2022-01-10 10:37:19,813 iteration 1937 : loss : 0.043067, loss_ce: 0.012347
2022-01-10 10:37:21,322 iteration 1938 : loss : 0.054433, loss_ce: 0.020221
 28%|████████▎                    | 114/400 [56:00<2:13:56, 28.10s/it]2022-01-10 10:37:22,924 iteration 1939 : loss : 0.033573, loss_ce: 0.009839
2022-01-10 10:37:24,612 iteration 1940 : loss : 0.062715, loss_ce: 0.028695
2022-01-10 10:37:26,307 iteration 1941 : loss : 0.041379, loss_ce: 0.016472
2022-01-10 10:37:27,932 iteration 1942 : loss : 0.053751, loss_ce: 0.017830
2022-01-10 10:37:29,505 iteration 1943 : loss : 0.037493, loss_ce: 0.016011
2022-01-10 10:37:31,104 iteration 1944 : loss : 0.031198, loss_ce: 0.012517
2022-01-10 10:37:32,651 iteration 1945 : loss : 0.042486, loss_ce: 0.015567
2022-01-10 10:37:34,242 iteration 1946 : loss : 0.041534, loss_ce: 0.017873
2022-01-10 10:37:35,864 iteration 1947 : loss : 0.053276, loss_ce: 0.025116
2022-01-10 10:37:37,451 iteration 1948 : loss : 0.062727, loss_ce: 0.022512
2022-01-10 10:37:39,010 iteration 1949 : loss : 0.031457, loss_ce: 0.012190
2022-01-10 10:37:40,535 iteration 1950 : loss : 0.031664, loss_ce: 0.014364
2022-01-10 10:37:42,175 iteration 1951 : loss : 0.057950, loss_ce: 0.021583
2022-01-10 10:37:43,792 iteration 1952 : loss : 0.053760, loss_ce: 0.017440
2022-01-10 10:37:45,450 iteration 1953 : loss : 0.050010, loss_ce: 0.024722
2022-01-10 10:37:47,062 iteration 1954 : loss : 0.024483, loss_ce: 0.011327
2022-01-10 10:37:47,062 Training Data Eval:
2022-01-10 10:37:55,035   Average segmentation loss on training set: 0.0290
2022-01-10 10:37:55,036 Validation Data Eval:
2022-01-10 10:37:57,779   Average segmentation loss on validation set: 0.0837
2022-01-10 10:37:59,387 iteration 1955 : loss : 0.050175, loss_ce: 0.014609
 29%|████████▎                    | 115/400 [56:38<2:27:40, 31.09s/it]2022-01-10 10:38:01,001 iteration 1956 : loss : 0.048212, loss_ce: 0.011427
2022-01-10 10:38:02,679 iteration 1957 : loss : 0.052253, loss_ce: 0.023020
2022-01-10 10:38:04,357 iteration 1958 : loss : 0.054746, loss_ce: 0.021237
2022-01-10 10:38:05,959 iteration 1959 : loss : 0.045344, loss_ce: 0.014702
2022-01-10 10:38:07,477 iteration 1960 : loss : 0.036696, loss_ce: 0.014251
2022-01-10 10:38:09,088 iteration 1961 : loss : 0.039743, loss_ce: 0.018357
2022-01-10 10:38:10,709 iteration 1962 : loss : 0.044891, loss_ce: 0.018656
2022-01-10 10:38:12,241 iteration 1963 : loss : 0.034985, loss_ce: 0.012733
2022-01-10 10:38:13,779 iteration 1964 : loss : 0.031594, loss_ce: 0.012628
2022-01-10 10:38:15,312 iteration 1965 : loss : 0.041896, loss_ce: 0.017451
2022-01-10 10:38:16,964 iteration 1966 : loss : 0.049807, loss_ce: 0.018347
2022-01-10 10:38:18,629 iteration 1967 : loss : 0.047905, loss_ce: 0.017940
2022-01-10 10:38:20,274 iteration 1968 : loss : 0.047671, loss_ce: 0.023040
2022-01-10 10:38:21,897 iteration 1969 : loss : 0.055343, loss_ce: 0.022782
2022-01-10 10:38:23,468 iteration 1970 : loss : 0.044196, loss_ce: 0.013587
2022-01-10 10:38:25,103 iteration 1971 : loss : 0.046255, loss_ce: 0.021831
2022-01-10 10:38:26,664 iteration 1972 : loss : 0.065497, loss_ce: 0.023059
 29%|████████▍                    | 116/400 [57:05<2:21:43, 29.94s/it]2022-01-10 10:38:28,179 iteration 1973 : loss : 0.035792, loss_ce: 0.014116
2022-01-10 10:38:29,778 iteration 1974 : loss : 0.046284, loss_ce: 0.017284
2022-01-10 10:38:31,358 iteration 1975 : loss : 0.046540, loss_ce: 0.015706
2022-01-10 10:38:33,016 iteration 1976 : loss : 0.036534, loss_ce: 0.015775
2022-01-10 10:38:34,607 iteration 1977 : loss : 0.042943, loss_ce: 0.013192
2022-01-10 10:38:36,113 iteration 1978 : loss : 0.026238, loss_ce: 0.010988
2022-01-10 10:38:37,619 iteration 1979 : loss : 0.036950, loss_ce: 0.014931
2022-01-10 10:38:39,225 iteration 1980 : loss : 0.047209, loss_ce: 0.021430
2022-01-10 10:38:40,771 iteration 1981 : loss : 0.037771, loss_ce: 0.012824
2022-01-10 10:38:42,265 iteration 1982 : loss : 0.063543, loss_ce: 0.022416
2022-01-10 10:38:43,919 iteration 1983 : loss : 0.059553, loss_ce: 0.032888
2022-01-10 10:38:45,435 iteration 1984 : loss : 0.049511, loss_ce: 0.014812
2022-01-10 10:38:47,029 iteration 1985 : loss : 0.049922, loss_ce: 0.018474
2022-01-10 10:38:48,623 iteration 1986 : loss : 0.045593, loss_ce: 0.020585
2022-01-10 10:38:50,266 iteration 1987 : loss : 0.030042, loss_ce: 0.011726
2022-01-10 10:38:51,791 iteration 1988 : loss : 0.035141, loss_ce: 0.010122
2022-01-10 10:38:53,349 iteration 1989 : loss : 0.037899, loss_ce: 0.020100
 29%|████████▍                    | 117/400 [57:32<2:16:38, 28.97s/it]2022-01-10 10:38:54,897 iteration 1990 : loss : 0.024254, loss_ce: 0.010100
2022-01-10 10:38:56,427 iteration 1991 : loss : 0.023685, loss_ce: 0.008912
2022-01-10 10:38:58,031 iteration 1992 : loss : 0.049770, loss_ce: 0.018894
2022-01-10 10:38:59,745 iteration 1993 : loss : 0.052183, loss_ce: 0.025760
2022-01-10 10:39:01,285 iteration 1994 : loss : 0.026735, loss_ce: 0.010566
2022-01-10 10:39:02,870 iteration 1995 : loss : 0.062140, loss_ce: 0.018438
2022-01-10 10:39:04,461 iteration 1996 : loss : 0.047496, loss_ce: 0.014475
2022-01-10 10:39:06,039 iteration 1997 : loss : 0.035238, loss_ce: 0.012274
2022-01-10 10:39:07,631 iteration 1998 : loss : 0.044972, loss_ce: 0.017085
2022-01-10 10:39:09,195 iteration 1999 : loss : 0.034699, loss_ce: 0.014573
2022-01-10 10:39:10,851 iteration 2000 : loss : 0.042208, loss_ce: 0.009383
2022-01-10 10:39:12,417 iteration 2001 : loss : 0.052338, loss_ce: 0.021966
2022-01-10 10:39:14,075 iteration 2002 : loss : 0.044115, loss_ce: 0.023479
2022-01-10 10:39:15,616 iteration 2003 : loss : 0.046443, loss_ce: 0.021455
2022-01-10 10:39:17,218 iteration 2004 : loss : 0.043788, loss_ce: 0.015971
2022-01-10 10:39:18,843 iteration 2005 : loss : 0.052845, loss_ce: 0.019115
2022-01-10 10:39:20,465 iteration 2006 : loss : 0.056318, loss_ce: 0.020450
 30%|████████▌                    | 118/400 [57:59<2:13:32, 28.41s/it]2022-01-10 10:39:22,146 iteration 2007 : loss : 0.037263, loss_ce: 0.015442
2022-01-10 10:39:23,686 iteration 2008 : loss : 0.046939, loss_ce: 0.020368
2022-01-10 10:39:25,243 iteration 2009 : loss : 0.076577, loss_ce: 0.028112
2022-01-10 10:39:26,765 iteration 2010 : loss : 0.051071, loss_ce: 0.022666
2022-01-10 10:39:28,328 iteration 2011 : loss : 0.043115, loss_ce: 0.017836
2022-01-10 10:39:29,922 iteration 2012 : loss : 0.051612, loss_ce: 0.019863
2022-01-10 10:39:31,560 iteration 2013 : loss : 0.032505, loss_ce: 0.012383
2022-01-10 10:39:33,189 iteration 2014 : loss : 0.030879, loss_ce: 0.010752
2022-01-10 10:39:34,697 iteration 2015 : loss : 0.030927, loss_ce: 0.011732
2022-01-10 10:39:36,271 iteration 2016 : loss : 0.042254, loss_ce: 0.013104
2022-01-10 10:39:37,755 iteration 2017 : loss : 0.025767, loss_ce: 0.010524
2022-01-10 10:39:39,334 iteration 2018 : loss : 0.052583, loss_ce: 0.022116
2022-01-10 10:39:40,857 iteration 2019 : loss : 0.029158, loss_ce: 0.012285
2022-01-10 10:39:42,448 iteration 2020 : loss : 0.045779, loss_ce: 0.019315
2022-01-10 10:39:44,059 iteration 2021 : loss : 0.042662, loss_ce: 0.015881
2022-01-10 10:39:45,702 iteration 2022 : loss : 0.062683, loss_ce: 0.034500
2022-01-10 10:39:47,280 iteration 2023 : loss : 0.045992, loss_ce: 0.014749
 30%|████████▋                    | 119/400 [58:26<2:10:48, 27.93s/it]2022-01-10 10:39:48,961 iteration 2024 : loss : 0.037300, loss_ce: 0.017070
2022-01-10 10:39:50,494 iteration 2025 : loss : 0.028410, loss_ce: 0.010942
2022-01-10 10:39:52,039 iteration 2026 : loss : 0.039413, loss_ce: 0.015804
2022-01-10 10:39:53,661 iteration 2027 : loss : 0.041941, loss_ce: 0.013805
2022-01-10 10:39:55,270 iteration 2028 : loss : 0.048441, loss_ce: 0.020628
2022-01-10 10:39:56,782 iteration 2029 : loss : 0.037712, loss_ce: 0.018755
2022-01-10 10:39:58,381 iteration 2030 : loss : 0.030097, loss_ce: 0.011759
2022-01-10 10:39:59,991 iteration 2031 : loss : 0.037230, loss_ce: 0.013795
2022-01-10 10:40:01,606 iteration 2032 : loss : 0.041314, loss_ce: 0.016793
2022-01-10 10:40:03,175 iteration 2033 : loss : 0.048555, loss_ce: 0.015420
2022-01-10 10:40:04,681 iteration 2034 : loss : 0.031429, loss_ce: 0.010614
2022-01-10 10:40:06,186 iteration 2035 : loss : 0.028587, loss_ce: 0.013181
2022-01-10 10:40:07,800 iteration 2036 : loss : 0.030278, loss_ce: 0.011959
2022-01-10 10:40:09,382 iteration 2037 : loss : 0.042535, loss_ce: 0.014982
2022-01-10 10:40:10,915 iteration 2038 : loss : 0.029541, loss_ce: 0.010570
2022-01-10 10:40:12,510 iteration 2039 : loss : 0.036458, loss_ce: 0.015706
2022-01-10 10:40:12,511 Training Data Eval:
2022-01-10 10:40:20,470   Average segmentation loss on training set: 0.0321
2022-01-10 10:40:20,471 Validation Data Eval:
2022-01-10 10:40:23,222   Average segmentation loss on validation set: 0.1135
2022-01-10 10:40:24,772 iteration 2040 : loss : 0.032129, loss_ce: 0.010827
 30%|████████▋                    | 120/400 [59:03<2:23:43, 30.80s/it]2022-01-10 10:40:26,406 iteration 2041 : loss : 0.031320, loss_ce: 0.012830
2022-01-10 10:40:27,966 iteration 2042 : loss : 0.030851, loss_ce: 0.012594
2022-01-10 10:40:29,546 iteration 2043 : loss : 0.034664, loss_ce: 0.016413
2022-01-10 10:40:31,187 iteration 2044 : loss : 0.029895, loss_ce: 0.012371
2022-01-10 10:40:32,853 iteration 2045 : loss : 0.032529, loss_ce: 0.014831
2022-01-10 10:40:34,473 iteration 2046 : loss : 0.044387, loss_ce: 0.017554
2022-01-10 10:40:36,070 iteration 2047 : loss : 0.032784, loss_ce: 0.011161
2022-01-10 10:40:37,580 iteration 2048 : loss : 0.030748, loss_ce: 0.010705
2022-01-10 10:40:39,137 iteration 2049 : loss : 0.027718, loss_ce: 0.009928
2022-01-10 10:40:40,745 iteration 2050 : loss : 0.039997, loss_ce: 0.012576
2022-01-10 10:40:42,360 iteration 2051 : loss : 0.057536, loss_ce: 0.022994
2022-01-10 10:40:43,905 iteration 2052 : loss : 0.033238, loss_ce: 0.016680
2022-01-10 10:40:45,438 iteration 2053 : loss : 0.033821, loss_ce: 0.011822
2022-01-10 10:40:47,024 iteration 2054 : loss : 0.047524, loss_ce: 0.019071
2022-01-10 10:40:48,631 iteration 2055 : loss : 0.043472, loss_ce: 0.021272
2022-01-10 10:40:50,226 iteration 2056 : loss : 0.036654, loss_ce: 0.014901
2022-01-10 10:40:51,843 iteration 2057 : loss : 0.028000, loss_ce: 0.011294
 30%|████████▊                    | 121/400 [59:30<2:18:01, 29.68s/it]2022-01-10 10:40:53,574 iteration 2058 : loss : 0.030317, loss_ce: 0.015307
2022-01-10 10:40:55,191 iteration 2059 : loss : 0.042735, loss_ce: 0.014143
2022-01-10 10:40:56,863 iteration 2060 : loss : 0.039053, loss_ce: 0.013693
2022-01-10 10:40:58,417 iteration 2061 : loss : 0.032101, loss_ce: 0.014539
2022-01-10 10:40:59,941 iteration 2062 : loss : 0.026315, loss_ce: 0.009083
2022-01-10 10:41:01,476 iteration 2063 : loss : 0.023079, loss_ce: 0.009876
2022-01-10 10:41:03,076 iteration 2064 : loss : 0.030006, loss_ce: 0.014001
2022-01-10 10:41:04,681 iteration 2065 : loss : 0.036446, loss_ce: 0.012218
2022-01-10 10:41:06,329 iteration 2066 : loss : 0.040553, loss_ce: 0.014870
2022-01-10 10:41:07,894 iteration 2067 : loss : 0.036793, loss_ce: 0.013463
2022-01-10 10:41:09,546 iteration 2068 : loss : 0.036692, loss_ce: 0.019159
2022-01-10 10:41:11,202 iteration 2069 : loss : 0.040182, loss_ce: 0.013019
2022-01-10 10:41:12,764 iteration 2070 : loss : 0.026463, loss_ce: 0.009725
2022-01-10 10:41:14,329 iteration 2071 : loss : 0.035721, loss_ce: 0.016362
2022-01-10 10:41:15,852 iteration 2072 : loss : 0.038123, loss_ce: 0.015591
2022-01-10 10:41:17,361 iteration 2073 : loss : 0.045288, loss_ce: 0.015707
2022-01-10 10:41:18,946 iteration 2074 : loss : 0.040737, loss_ce: 0.013298
 30%|████████▊                    | 122/400 [59:58<2:13:56, 28.91s/it]2022-01-10 10:41:20,603 iteration 2075 : loss : 0.037404, loss_ce: 0.014659
2022-01-10 10:41:22,255 iteration 2076 : loss : 0.062251, loss_ce: 0.020059
2022-01-10 10:41:23,860 iteration 2077 : loss : 0.042638, loss_ce: 0.014983
2022-01-10 10:41:25,389 iteration 2078 : loss : 0.035305, loss_ce: 0.015777
2022-01-10 10:41:26,948 iteration 2079 : loss : 0.030486, loss_ce: 0.010246
2022-01-10 10:41:28,591 iteration 2080 : loss : 0.035859, loss_ce: 0.013050
2022-01-10 10:41:30,196 iteration 2081 : loss : 0.041768, loss_ce: 0.020730
2022-01-10 10:41:31,775 iteration 2082 : loss : 0.040898, loss_ce: 0.014847
2022-01-10 10:41:33,347 iteration 2083 : loss : 0.025978, loss_ce: 0.009957
2022-01-10 10:41:34,922 iteration 2084 : loss : 0.037715, loss_ce: 0.014567
2022-01-10 10:41:36,463 iteration 2085 : loss : 0.036651, loss_ce: 0.017934
2022-01-10 10:41:38,033 iteration 2086 : loss : 0.027458, loss_ce: 0.012742
2022-01-10 10:41:39,564 iteration 2087 : loss : 0.039550, loss_ce: 0.012098
2022-01-10 10:41:41,265 iteration 2088 : loss : 0.041551, loss_ce: 0.020811
2022-01-10 10:41:42,821 iteration 2089 : loss : 0.037462, loss_ce: 0.014210
2022-01-10 10:41:44,446 iteration 2090 : loss : 0.039417, loss_ce: 0.013843
2022-01-10 10:41:46,029 iteration 2091 : loss : 0.048665, loss_ce: 0.019419
 31%|████████▎                  | 123/400 [1:00:25<2:10:55, 28.36s/it]2022-01-10 10:41:47,656 iteration 2092 : loss : 0.030131, loss_ce: 0.011625
2022-01-10 10:41:49,334 iteration 2093 : loss : 0.034896, loss_ce: 0.015864
2022-01-10 10:41:50,953 iteration 2094 : loss : 0.031984, loss_ce: 0.015963
2022-01-10 10:41:52,569 iteration 2095 : loss : 0.042045, loss_ce: 0.016734
2022-01-10 10:41:54,214 iteration 2096 : loss : 0.045189, loss_ce: 0.016435
2022-01-10 10:41:55,761 iteration 2097 : loss : 0.033681, loss_ce: 0.017069
2022-01-10 10:41:57,406 iteration 2098 : loss : 0.037879, loss_ce: 0.014218
2022-01-10 10:41:59,043 iteration 2099 : loss : 0.046647, loss_ce: 0.017843
2022-01-10 10:42:00,732 iteration 2100 : loss : 0.055760, loss_ce: 0.023520
2022-01-10 10:42:02,335 iteration 2101 : loss : 0.043680, loss_ce: 0.013230
2022-01-10 10:42:03,897 iteration 2102 : loss : 0.028892, loss_ce: 0.011368
2022-01-10 10:42:05,599 iteration 2103 : loss : 0.031278, loss_ce: 0.011917
2022-01-10 10:42:07,237 iteration 2104 : loss : 0.028969, loss_ce: 0.010885
2022-01-10 10:42:08,800 iteration 2105 : loss : 0.033481, loss_ce: 0.010104
2022-01-10 10:42:10,487 iteration 2106 : loss : 0.057705, loss_ce: 0.017598
2022-01-10 10:42:12,097 iteration 2107 : loss : 0.040549, loss_ce: 0.016322
2022-01-10 10:42:13,717 iteration 2108 : loss : 0.030527, loss_ce: 0.011577
 31%|████████▎                  | 124/400 [1:00:52<2:09:31, 28.16s/it]2022-01-10 10:42:15,376 iteration 2109 : loss : 0.037097, loss_ce: 0.015514
2022-01-10 10:42:16,961 iteration 2110 : loss : 0.039137, loss_ce: 0.013343
2022-01-10 10:42:18,692 iteration 2111 : loss : 0.046074, loss_ce: 0.016172
2022-01-10 10:42:20,423 iteration 2112 : loss : 0.037785, loss_ce: 0.015693
2022-01-10 10:42:21,986 iteration 2113 : loss : 0.059634, loss_ce: 0.023518
2022-01-10 10:42:23,556 iteration 2114 : loss : 0.039972, loss_ce: 0.017009
2022-01-10 10:42:25,023 iteration 2115 : loss : 0.040161, loss_ce: 0.012037
2022-01-10 10:42:26,628 iteration 2116 : loss : 0.046087, loss_ce: 0.019626
2022-01-10 10:42:28,285 iteration 2117 : loss : 0.040814, loss_ce: 0.014310
2022-01-10 10:42:29,872 iteration 2118 : loss : 0.054591, loss_ce: 0.023511
2022-01-10 10:42:31,491 iteration 2119 : loss : 0.059948, loss_ce: 0.019943
2022-01-10 10:42:33,077 iteration 2120 : loss : 0.051220, loss_ce: 0.019392
2022-01-10 10:42:34,764 iteration 2121 : loss : 0.039407, loss_ce: 0.014809
2022-01-10 10:42:36,305 iteration 2122 : loss : 0.031670, loss_ce: 0.012386
2022-01-10 10:42:37,860 iteration 2123 : loss : 0.038772, loss_ce: 0.020974
2022-01-10 10:42:39,480 iteration 2124 : loss : 0.039683, loss_ce: 0.014316
2022-01-10 10:42:39,480 Training Data Eval:
2022-01-10 10:42:47,446   Average segmentation loss on training set: 0.0250
2022-01-10 10:42:47,447 Validation Data Eval:
2022-01-10 10:42:50,203   Average segmentation loss on validation set: 0.0800
2022-01-10 10:42:51,795 iteration 2125 : loss : 0.027779, loss_ce: 0.009231
 31%|████████▍                  | 125/400 [1:01:30<2:22:42, 31.14s/it]2022-01-10 10:42:53,431 iteration 2126 : loss : 0.029631, loss_ce: 0.010486
2022-01-10 10:42:55,086 iteration 2127 : loss : 0.036203, loss_ce: 0.013529
2022-01-10 10:42:56,622 iteration 2128 : loss : 0.025581, loss_ce: 0.012603
2022-01-10 10:42:58,220 iteration 2129 : loss : 0.024155, loss_ce: 0.010578
2022-01-10 10:42:59,742 iteration 2130 : loss : 0.051027, loss_ce: 0.016809
2022-01-10 10:43:01,414 iteration 2131 : loss : 0.057152, loss_ce: 0.026285
2022-01-10 10:43:03,088 iteration 2132 : loss : 0.038352, loss_ce: 0.015667
2022-01-10 10:43:04,735 iteration 2133 : loss : 0.041322, loss_ce: 0.017209
2022-01-10 10:43:06,377 iteration 2134 : loss : 0.064317, loss_ce: 0.019850
2022-01-10 10:43:07,983 iteration 2135 : loss : 0.040287, loss_ce: 0.015425
2022-01-10 10:43:09,581 iteration 2136 : loss : 0.033539, loss_ce: 0.013135
2022-01-10 10:43:11,182 iteration 2137 : loss : 0.048911, loss_ce: 0.015383
2022-01-10 10:43:12,737 iteration 2138 : loss : 0.041791, loss_ce: 0.013535
2022-01-10 10:43:14,237 iteration 2139 : loss : 0.029061, loss_ce: 0.012972
2022-01-10 10:43:15,792 iteration 2140 : loss : 0.041430, loss_ce: 0.014879
2022-01-10 10:43:17,384 iteration 2141 : loss : 0.030846, loss_ce: 0.013161
2022-01-10 10:43:18,970 iteration 2142 : loss : 0.056532, loss_ce: 0.031409
 32%|████████▌                  | 126/400 [1:01:58<2:16:45, 29.95s/it]2022-01-10 10:43:20,654 iteration 2143 : loss : 0.035697, loss_ce: 0.011158
2022-01-10 10:43:22,240 iteration 2144 : loss : 0.041673, loss_ce: 0.012716
2022-01-10 10:43:23,738 iteration 2145 : loss : 0.024695, loss_ce: 0.012789
2022-01-10 10:43:25,488 iteration 2146 : loss : 0.060046, loss_ce: 0.024794
2022-01-10 10:43:27,103 iteration 2147 : loss : 0.041623, loss_ce: 0.016962
2022-01-10 10:43:28,705 iteration 2148 : loss : 0.036480, loss_ce: 0.014138
2022-01-10 10:43:30,320 iteration 2149 : loss : 0.049287, loss_ce: 0.018573
2022-01-10 10:43:31,973 iteration 2150 : loss : 0.041366, loss_ce: 0.014627
2022-01-10 10:43:33,625 iteration 2151 : loss : 0.034530, loss_ce: 0.012355
2022-01-10 10:43:35,151 iteration 2152 : loss : 0.032673, loss_ce: 0.013998
2022-01-10 10:43:36,709 iteration 2153 : loss : 0.040216, loss_ce: 0.010542
2022-01-10 10:43:38,218 iteration 2154 : loss : 0.027106, loss_ce: 0.012604
2022-01-10 10:43:39,756 iteration 2155 : loss : 0.037577, loss_ce: 0.014060
2022-01-10 10:43:41,380 iteration 2156 : loss : 0.041549, loss_ce: 0.015428
2022-01-10 10:43:42,989 iteration 2157 : loss : 0.057445, loss_ce: 0.031280
2022-01-10 10:43:44,546 iteration 2158 : loss : 0.032366, loss_ce: 0.012376
2022-01-10 10:43:46,183 iteration 2159 : loss : 0.069430, loss_ce: 0.023352
 32%|████████▌                  | 127/400 [1:02:25<2:12:31, 29.13s/it]2022-01-10 10:43:47,757 iteration 2160 : loss : 0.026035, loss_ce: 0.008747
2022-01-10 10:43:49,368 iteration 2161 : loss : 0.047668, loss_ce: 0.023050
2022-01-10 10:43:50,958 iteration 2162 : loss : 0.037585, loss_ce: 0.015634
2022-01-10 10:43:52,442 iteration 2163 : loss : 0.047217, loss_ce: 0.017741
2022-01-10 10:43:54,044 iteration 2164 : loss : 0.034689, loss_ce: 0.015411
2022-01-10 10:43:55,630 iteration 2165 : loss : 0.023264, loss_ce: 0.007870
2022-01-10 10:43:57,273 iteration 2166 : loss : 0.038260, loss_ce: 0.020421
2022-01-10 10:43:58,858 iteration 2167 : loss : 0.042946, loss_ce: 0.023436
2022-01-10 10:44:00,445 iteration 2168 : loss : 0.040804, loss_ce: 0.017846
2022-01-10 10:44:02,058 iteration 2169 : loss : 0.029367, loss_ce: 0.011390
2022-01-10 10:44:03,612 iteration 2170 : loss : 0.037677, loss_ce: 0.013451
2022-01-10 10:44:05,185 iteration 2171 : loss : 0.022240, loss_ce: 0.007418
2022-01-10 10:44:06,665 iteration 2172 : loss : 0.031265, loss_ce: 0.013872
2022-01-10 10:44:08,242 iteration 2173 : loss : 0.037674, loss_ce: 0.013406
2022-01-10 10:44:09,864 iteration 2174 : loss : 0.042873, loss_ce: 0.017147
2022-01-10 10:44:11,400 iteration 2175 : loss : 0.049197, loss_ce: 0.014739
2022-01-10 10:44:12,947 iteration 2176 : loss : 0.032697, loss_ce: 0.015611
 32%|████████▋                  | 128/400 [1:02:52<2:08:48, 28.42s/it]2022-01-10 10:44:14,650 iteration 2177 : loss : 0.045547, loss_ce: 0.013807
2022-01-10 10:44:16,182 iteration 2178 : loss : 0.036527, loss_ce: 0.018597
2022-01-10 10:44:17,754 iteration 2179 : loss : 0.027595, loss_ce: 0.011054
2022-01-10 10:44:19,372 iteration 2180 : loss : 0.036616, loss_ce: 0.011093
2022-01-10 10:44:20,932 iteration 2181 : loss : 0.028459, loss_ce: 0.009417
2022-01-10 10:44:22,497 iteration 2182 : loss : 0.033385, loss_ce: 0.012629
2022-01-10 10:44:24,033 iteration 2183 : loss : 0.048556, loss_ce: 0.013947
2022-01-10 10:44:25,546 iteration 2184 : loss : 0.039941, loss_ce: 0.019030
2022-01-10 10:44:27,032 iteration 2185 : loss : 0.033384, loss_ce: 0.013088
2022-01-10 10:44:28,724 iteration 2186 : loss : 0.058963, loss_ce: 0.028470
2022-01-10 10:44:30,241 iteration 2187 : loss : 0.030556, loss_ce: 0.014248
2022-01-10 10:44:31,892 iteration 2188 : loss : 0.042815, loss_ce: 0.014624
2022-01-10 10:44:33,427 iteration 2189 : loss : 0.031150, loss_ce: 0.012884
2022-01-10 10:44:35,063 iteration 2190 : loss : 0.038699, loss_ce: 0.013872
2022-01-10 10:44:36,570 iteration 2191 : loss : 0.031291, loss_ce: 0.014684
2022-01-10 10:44:38,150 iteration 2192 : loss : 0.045889, loss_ce: 0.016331
2022-01-10 10:44:39,805 iteration 2193 : loss : 0.037817, loss_ce: 0.015551
 32%|████████▋                  | 129/400 [1:03:18<2:06:14, 27.95s/it]2022-01-10 10:44:41,415 iteration 2194 : loss : 0.039862, loss_ce: 0.015480
2022-01-10 10:44:42,960 iteration 2195 : loss : 0.029020, loss_ce: 0.010810
2022-01-10 10:44:44,552 iteration 2196 : loss : 0.034906, loss_ce: 0.016808
2022-01-10 10:44:46,167 iteration 2197 : loss : 0.041668, loss_ce: 0.015610
2022-01-10 10:44:47,718 iteration 2198 : loss : 0.030464, loss_ce: 0.013578
2022-01-10 10:44:49,458 iteration 2199 : loss : 0.047212, loss_ce: 0.018189
2022-01-10 10:44:51,041 iteration 2200 : loss : 0.028401, loss_ce: 0.010675
2022-01-10 10:44:52,634 iteration 2201 : loss : 0.044950, loss_ce: 0.012932
2022-01-10 10:44:54,206 iteration 2202 : loss : 0.057291, loss_ce: 0.015597
2022-01-10 10:44:55,821 iteration 2203 : loss : 0.027366, loss_ce: 0.009013
2022-01-10 10:44:57,459 iteration 2204 : loss : 0.030151, loss_ce: 0.011135
2022-01-10 10:44:58,936 iteration 2205 : loss : 0.031010, loss_ce: 0.012784
2022-01-10 10:45:00,437 iteration 2206 : loss : 0.025898, loss_ce: 0.012113
2022-01-10 10:45:02,022 iteration 2207 : loss : 0.050857, loss_ce: 0.023507
2022-01-10 10:45:03,582 iteration 2208 : loss : 0.039758, loss_ce: 0.019661
2022-01-10 10:45:05,232 iteration 2209 : loss : 0.039227, loss_ce: 0.013717
2022-01-10 10:45:05,232 Training Data Eval:
2022-01-10 10:45:13,193   Average segmentation loss on training set: 0.0252
2022-01-10 10:45:13,193 Validation Data Eval:
2022-01-10 10:45:15,938   Average segmentation loss on validation set: 0.0749
2022-01-10 10:45:17,527 iteration 2210 : loss : 0.034362, loss_ce: 0.012296
 32%|████████▊                  | 130/400 [1:03:56<2:18:58, 30.88s/it]2022-01-10 10:45:19,172 iteration 2211 : loss : 0.034925, loss_ce: 0.016049
2022-01-10 10:45:20,672 iteration 2212 : loss : 0.028356, loss_ce: 0.010100
2022-01-10 10:45:22,186 iteration 2213 : loss : 0.023460, loss_ce: 0.008727
2022-01-10 10:45:23,778 iteration 2214 : loss : 0.032208, loss_ce: 0.014201
2022-01-10 10:45:25,341 iteration 2215 : loss : 0.023979, loss_ce: 0.008900
2022-01-10 10:45:26,872 iteration 2216 : loss : 0.040919, loss_ce: 0.017762
2022-01-10 10:45:28,371 iteration 2217 : loss : 0.031515, loss_ce: 0.014899
2022-01-10 10:45:29,958 iteration 2218 : loss : 0.029270, loss_ce: 0.011616
2022-01-10 10:45:31,517 iteration 2219 : loss : 0.028632, loss_ce: 0.011219
2022-01-10 10:45:33,110 iteration 2220 : loss : 0.028528, loss_ce: 0.015003
2022-01-10 10:45:34,760 iteration 2221 : loss : 0.049490, loss_ce: 0.020430
2022-01-10 10:45:36,297 iteration 2222 : loss : 0.061446, loss_ce: 0.019216
2022-01-10 10:45:37,998 iteration 2223 : loss : 0.044605, loss_ce: 0.015855
2022-01-10 10:45:39,547 iteration 2224 : loss : 0.050356, loss_ce: 0.015463
2022-01-10 10:45:41,116 iteration 2225 : loss : 0.030423, loss_ce: 0.012214
2022-01-10 10:45:42,837 iteration 2226 : loss : 0.037692, loss_ce: 0.014554
2022-01-10 10:45:44,480 iteration 2227 : loss : 0.057890, loss_ce: 0.017728
 33%|████████▊                  | 131/400 [1:04:23<2:13:10, 29.70s/it]2022-01-10 10:45:46,181 iteration 2228 : loss : 0.059549, loss_ce: 0.025040
2022-01-10 10:45:47,795 iteration 2229 : loss : 0.047189, loss_ce: 0.019069
2022-01-10 10:45:49,354 iteration 2230 : loss : 0.039148, loss_ce: 0.017065
2022-01-10 10:45:50,884 iteration 2231 : loss : 0.032275, loss_ce: 0.016051
2022-01-10 10:45:52,407 iteration 2232 : loss : 0.026989, loss_ce: 0.010436
2022-01-10 10:45:53,943 iteration 2233 : loss : 0.038869, loss_ce: 0.010446
2022-01-10 10:45:55,543 iteration 2234 : loss : 0.050829, loss_ce: 0.019512
2022-01-10 10:45:57,052 iteration 2235 : loss : 0.042324, loss_ce: 0.022629
2022-01-10 10:45:58,588 iteration 2236 : loss : 0.038919, loss_ce: 0.016063
2022-01-10 10:46:00,154 iteration 2237 : loss : 0.037567, loss_ce: 0.016071
2022-01-10 10:46:01,695 iteration 2238 : loss : 0.040308, loss_ce: 0.014253
2022-01-10 10:46:03,297 iteration 2239 : loss : 0.031878, loss_ce: 0.011755
2022-01-10 10:46:04,798 iteration 2240 : loss : 0.036466, loss_ce: 0.013614
2022-01-10 10:46:06,407 iteration 2241 : loss : 0.039350, loss_ce: 0.015630
2022-01-10 10:46:07,926 iteration 2242 : loss : 0.032110, loss_ce: 0.013262
2022-01-10 10:46:09,443 iteration 2243 : loss : 0.032450, loss_ce: 0.010643
2022-01-10 10:46:10,999 iteration 2244 : loss : 0.021361, loss_ce: 0.008044
 33%|████████▉                  | 132/400 [1:04:50<2:08:23, 28.75s/it]2022-01-10 10:46:12,598 iteration 2245 : loss : 0.026312, loss_ce: 0.010577
2022-01-10 10:46:14,195 iteration 2246 : loss : 0.029906, loss_ce: 0.012973
2022-01-10 10:46:15,767 iteration 2247 : loss : 0.043991, loss_ce: 0.015799
2022-01-10 10:46:17,242 iteration 2248 : loss : 0.021512, loss_ce: 0.009075
2022-01-10 10:46:18,871 iteration 2249 : loss : 0.039444, loss_ce: 0.017144
2022-01-10 10:46:20,573 iteration 2250 : loss : 0.041813, loss_ce: 0.018567
2022-01-10 10:46:22,075 iteration 2251 : loss : 0.032139, loss_ce: 0.012425
2022-01-10 10:46:23,678 iteration 2252 : loss : 0.032284, loss_ce: 0.010369
2022-01-10 10:46:25,204 iteration 2253 : loss : 0.035817, loss_ce: 0.011345
2022-01-10 10:46:26,831 iteration 2254 : loss : 0.045398, loss_ce: 0.018182
2022-01-10 10:46:28,454 iteration 2255 : loss : 0.025427, loss_ce: 0.011338
2022-01-10 10:46:30,126 iteration 2256 : loss : 0.035664, loss_ce: 0.014508
2022-01-10 10:46:31,691 iteration 2257 : loss : 0.032620, loss_ce: 0.011101
2022-01-10 10:46:33,261 iteration 2258 : loss : 0.035764, loss_ce: 0.014257
2022-01-10 10:46:34,883 iteration 2259 : loss : 0.043703, loss_ce: 0.016031
2022-01-10 10:46:36,475 iteration 2260 : loss : 0.043674, loss_ce: 0.016330
2022-01-10 10:46:38,010 iteration 2261 : loss : 0.041929, loss_ce: 0.009258
 33%|████████▉                  | 133/400 [1:05:17<2:05:36, 28.23s/it]2022-01-10 10:46:39,777 iteration 2262 : loss : 0.034018, loss_ce: 0.014688
2022-01-10 10:46:41,379 iteration 2263 : loss : 0.029691, loss_ce: 0.012184
2022-01-10 10:46:42,988 iteration 2264 : loss : 0.044125, loss_ce: 0.013074
2022-01-10 10:46:44,600 iteration 2265 : loss : 0.036258, loss_ce: 0.013599
2022-01-10 10:46:46,180 iteration 2266 : loss : 0.031221, loss_ce: 0.011203
2022-01-10 10:46:47,763 iteration 2267 : loss : 0.032202, loss_ce: 0.010333
2022-01-10 10:46:49,262 iteration 2268 : loss : 0.027801, loss_ce: 0.011725
2022-01-10 10:46:50,859 iteration 2269 : loss : 0.040680, loss_ce: 0.018009
2022-01-10 10:46:52,393 iteration 2270 : loss : 0.030674, loss_ce: 0.011578
2022-01-10 10:46:53,935 iteration 2271 : loss : 0.051770, loss_ce: 0.031885
2022-01-10 10:46:55,461 iteration 2272 : loss : 0.027187, loss_ce: 0.007446
2022-01-10 10:46:57,086 iteration 2273 : loss : 0.042369, loss_ce: 0.012647
2022-01-10 10:46:58,602 iteration 2274 : loss : 0.042644, loss_ce: 0.013298
2022-01-10 10:47:00,163 iteration 2275 : loss : 0.041643, loss_ce: 0.018072
2022-01-10 10:47:01,749 iteration 2276 : loss : 0.054111, loss_ce: 0.016649
2022-01-10 10:47:03,301 iteration 2277 : loss : 0.031389, loss_ce: 0.013216
2022-01-10 10:47:04,874 iteration 2278 : loss : 0.026123, loss_ce: 0.011845
 34%|█████████                  | 134/400 [1:05:43<2:03:20, 27.82s/it]2022-01-10 10:47:06,612 iteration 2279 : loss : 0.041064, loss_ce: 0.018384
2022-01-10 10:47:08,226 iteration 2280 : loss : 0.029240, loss_ce: 0.013080
2022-01-10 10:47:09,723 iteration 2281 : loss : 0.031672, loss_ce: 0.013223
2022-01-10 10:47:11,407 iteration 2282 : loss : 0.044289, loss_ce: 0.016502
2022-01-10 10:47:12,946 iteration 2283 : loss : 0.035383, loss_ce: 0.012868
2022-01-10 10:47:14,631 iteration 2284 : loss : 0.050466, loss_ce: 0.018567
2022-01-10 10:47:16,169 iteration 2285 : loss : 0.032329, loss_ce: 0.011441
2022-01-10 10:47:17,723 iteration 2286 : loss : 0.024647, loss_ce: 0.009184
2022-01-10 10:47:19,345 iteration 2287 : loss : 0.037863, loss_ce: 0.010471
2022-01-10 10:47:20,844 iteration 2288 : loss : 0.033886, loss_ce: 0.013363
2022-01-10 10:47:22,431 iteration 2289 : loss : 0.040886, loss_ce: 0.017614
2022-01-10 10:47:24,019 iteration 2290 : loss : 0.031590, loss_ce: 0.011374
2022-01-10 10:47:25,585 iteration 2291 : loss : 0.027835, loss_ce: 0.010399
2022-01-10 10:47:27,224 iteration 2292 : loss : 0.035575, loss_ce: 0.011192
2022-01-10 10:47:28,731 iteration 2293 : loss : 0.054097, loss_ce: 0.024783
2022-01-10 10:47:30,290 iteration 2294 : loss : 0.036055, loss_ce: 0.015447
2022-01-10 10:47:30,290 Training Data Eval:
2022-01-10 10:47:38,247   Average segmentation loss on training set: 0.0219
2022-01-10 10:47:38,247 Validation Data Eval:
2022-01-10 10:47:40,996   Average segmentation loss on validation set: 0.0623
2022-01-10 10:47:46,749 Found new lowest validation loss at iteration 2294! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 10:47:48,251 iteration 2295 : loss : 0.033124, loss_ce: 0.011708
 34%|█████████                  | 135/400 [1:06:27<2:23:28, 32.48s/it]2022-01-10 10:47:49,859 iteration 2296 : loss : 0.031002, loss_ce: 0.012613
2022-01-10 10:47:51,433 iteration 2297 : loss : 0.030927, loss_ce: 0.008822
2022-01-10 10:47:53,123 iteration 2298 : loss : 0.067107, loss_ce: 0.026127
2022-01-10 10:47:54,689 iteration 2299 : loss : 0.029149, loss_ce: 0.007780
2022-01-10 10:47:56,352 iteration 2300 : loss : 0.042411, loss_ce: 0.016543
2022-01-10 10:47:57,920 iteration 2301 : loss : 0.046390, loss_ce: 0.023530
2022-01-10 10:47:59,474 iteration 2302 : loss : 0.033276, loss_ce: 0.016036
2022-01-10 10:48:01,007 iteration 2303 : loss : 0.038239, loss_ce: 0.011564
2022-01-10 10:48:02,496 iteration 2304 : loss : 0.047332, loss_ce: 0.017989
2022-01-10 10:48:04,106 iteration 2305 : loss : 0.038879, loss_ce: 0.012211
2022-01-10 10:48:05,631 iteration 2306 : loss : 0.040498, loss_ce: 0.015384
2022-01-10 10:48:07,142 iteration 2307 : loss : 0.029145, loss_ce: 0.011177
2022-01-10 10:48:08,690 iteration 2308 : loss : 0.032031, loss_ce: 0.014922
2022-01-10 10:48:10,295 iteration 2309 : loss : 0.031207, loss_ce: 0.012635
2022-01-10 10:48:11,899 iteration 2310 : loss : 0.047227, loss_ce: 0.019927
2022-01-10 10:48:13,564 iteration 2311 : loss : 0.040191, loss_ce: 0.014471
2022-01-10 10:48:15,155 iteration 2312 : loss : 0.032905, loss_ce: 0.013668
 34%|█████████▏                 | 136/400 [1:06:54<2:15:33, 30.81s/it]2022-01-10 10:48:16,785 iteration 2313 : loss : 0.031273, loss_ce: 0.014527
2022-01-10 10:48:18,310 iteration 2314 : loss : 0.039025, loss_ce: 0.016270
2022-01-10 10:48:19,910 iteration 2315 : loss : 0.021588, loss_ce: 0.007391
2022-01-10 10:48:21,519 iteration 2316 : loss : 0.039521, loss_ce: 0.021682
2022-01-10 10:48:23,016 iteration 2317 : loss : 0.035220, loss_ce: 0.013337
2022-01-10 10:48:24,651 iteration 2318 : loss : 0.024683, loss_ce: 0.010868
2022-01-10 10:48:26,311 iteration 2319 : loss : 0.039145, loss_ce: 0.012948
2022-01-10 10:48:27,843 iteration 2320 : loss : 0.034577, loss_ce: 0.017772
2022-01-10 10:48:29,416 iteration 2321 : loss : 0.035452, loss_ce: 0.010328
2022-01-10 10:48:31,003 iteration 2322 : loss : 0.028764, loss_ce: 0.012832
2022-01-10 10:48:32,652 iteration 2323 : loss : 0.046962, loss_ce: 0.014047
2022-01-10 10:48:34,250 iteration 2324 : loss : 0.033799, loss_ce: 0.011332
2022-01-10 10:48:35,831 iteration 2325 : loss : 0.035772, loss_ce: 0.017549
2022-01-10 10:48:37,450 iteration 2326 : loss : 0.036109, loss_ce: 0.019478
2022-01-10 10:48:39,005 iteration 2327 : loss : 0.025011, loss_ce: 0.008518
2022-01-10 10:48:40,674 iteration 2328 : loss : 0.036833, loss_ce: 0.012655
2022-01-10 10:48:42,249 iteration 2329 : loss : 0.043731, loss_ce: 0.015412
 34%|█████████▏                 | 137/400 [1:07:21<2:10:09, 29.69s/it]2022-01-10 10:48:43,873 iteration 2330 : loss : 0.024421, loss_ce: 0.009354
2022-01-10 10:48:45,394 iteration 2331 : loss : 0.039533, loss_ce: 0.010307
2022-01-10 10:48:47,054 iteration 2332 : loss : 0.042643, loss_ce: 0.020575
2022-01-10 10:48:48,568 iteration 2333 : loss : 0.024696, loss_ce: 0.013194
2022-01-10 10:48:50,136 iteration 2334 : loss : 0.039130, loss_ce: 0.009558
2022-01-10 10:48:51,828 iteration 2335 : loss : 0.045209, loss_ce: 0.020008
2022-01-10 10:48:53,366 iteration 2336 : loss : 0.028410, loss_ce: 0.012582
2022-01-10 10:48:54,946 iteration 2337 : loss : 0.029716, loss_ce: 0.010846
2022-01-10 10:48:56,563 iteration 2338 : loss : 0.025921, loss_ce: 0.009288
2022-01-10 10:48:58,136 iteration 2339 : loss : 0.072730, loss_ce: 0.029759
2022-01-10 10:48:59,748 iteration 2340 : loss : 0.035217, loss_ce: 0.014400
2022-01-10 10:49:01,335 iteration 2341 : loss : 0.035514, loss_ce: 0.012681
2022-01-10 10:49:02,951 iteration 2342 : loss : 0.023660, loss_ce: 0.007627
2022-01-10 10:49:04,533 iteration 2343 : loss : 0.026668, loss_ce: 0.009390
2022-01-10 10:49:06,026 iteration 2344 : loss : 0.026595, loss_ce: 0.010127
2022-01-10 10:49:07,561 iteration 2345 : loss : 0.024047, loss_ce: 0.009271
2022-01-10 10:49:09,057 iteration 2346 : loss : 0.029741, loss_ce: 0.010384
 34%|█████████▎                 | 138/400 [1:07:48<2:05:53, 28.83s/it]2022-01-10 10:49:10,744 iteration 2347 : loss : 0.046436, loss_ce: 0.019806
2022-01-10 10:49:12,297 iteration 2348 : loss : 0.028817, loss_ce: 0.009876
2022-01-10 10:49:13,899 iteration 2349 : loss : 0.037008, loss_ce: 0.019182
2022-01-10 10:49:15,394 iteration 2350 : loss : 0.027637, loss_ce: 0.013399
2022-01-10 10:49:16,992 iteration 2351 : loss : 0.035970, loss_ce: 0.013349
2022-01-10 10:49:18,668 iteration 2352 : loss : 0.038517, loss_ce: 0.014667
2022-01-10 10:49:20,163 iteration 2353 : loss : 0.025689, loss_ce: 0.011435
2022-01-10 10:49:21,756 iteration 2354 : loss : 0.036040, loss_ce: 0.011773
2022-01-10 10:49:23,278 iteration 2355 : loss : 0.032309, loss_ce: 0.016054
2022-01-10 10:49:24,954 iteration 2356 : loss : 0.055901, loss_ce: 0.022071
2022-01-10 10:49:26,593 iteration 2357 : loss : 0.048456, loss_ce: 0.021721
2022-01-10 10:49:28,263 iteration 2358 : loss : 0.044942, loss_ce: 0.016291
2022-01-10 10:49:29,903 iteration 2359 : loss : 0.050473, loss_ce: 0.013286
2022-01-10 10:49:31,386 iteration 2360 : loss : 0.058064, loss_ce: 0.013263
2022-01-10 10:49:32,897 iteration 2361 : loss : 0.026059, loss_ce: 0.009724
2022-01-10 10:49:34,475 iteration 2362 : loss : 0.033592, loss_ce: 0.014139
2022-01-10 10:49:36,048 iteration 2363 : loss : 0.024722, loss_ce: 0.008187
 35%|█████████▍                 | 139/400 [1:08:15<2:03:01, 28.28s/it]2022-01-10 10:49:37,684 iteration 2364 : loss : 0.031901, loss_ce: 0.012893
2022-01-10 10:49:39,199 iteration 2365 : loss : 0.030205, loss_ce: 0.010344
2022-01-10 10:49:40,795 iteration 2366 : loss : 0.032145, loss_ce: 0.015386
2022-01-10 10:49:42,336 iteration 2367 : loss : 0.040616, loss_ce: 0.012468
2022-01-10 10:49:44,002 iteration 2368 : loss : 0.028841, loss_ce: 0.007692
2022-01-10 10:49:45,568 iteration 2369 : loss : 0.049474, loss_ce: 0.017672
2022-01-10 10:49:47,100 iteration 2370 : loss : 0.022197, loss_ce: 0.007774
2022-01-10 10:49:48,679 iteration 2371 : loss : 0.034735, loss_ce: 0.013583
2022-01-10 10:49:50,187 iteration 2372 : loss : 0.023761, loss_ce: 0.012176
2022-01-10 10:49:51,763 iteration 2373 : loss : 0.027868, loss_ce: 0.009564
2022-01-10 10:49:53,387 iteration 2374 : loss : 0.034903, loss_ce: 0.014032
2022-01-10 10:49:54,913 iteration 2375 : loss : 0.030689, loss_ce: 0.009516
2022-01-10 10:49:56,542 iteration 2376 : loss : 0.042168, loss_ce: 0.020910
2022-01-10 10:49:58,107 iteration 2377 : loss : 0.034680, loss_ce: 0.009457
2022-01-10 10:49:59,788 iteration 2378 : loss : 0.042267, loss_ce: 0.022472
2022-01-10 10:50:01,394 iteration 2379 : loss : 0.039814, loss_ce: 0.012380
2022-01-10 10:50:01,394 Training Data Eval:
2022-01-10 10:50:09,348   Average segmentation loss on training set: 0.0237
2022-01-10 10:50:09,348 Validation Data Eval:
2022-01-10 10:50:12,092   Average segmentation loss on validation set: 0.0774
2022-01-10 10:50:13,789 iteration 2380 : loss : 0.027036, loss_ce: 0.009689
 35%|█████████▍                 | 140/400 [1:08:52<2:14:49, 31.11s/it]2022-01-10 10:50:15,376 iteration 2381 : loss : 0.041659, loss_ce: 0.015238
2022-01-10 10:50:17,016 iteration 2382 : loss : 0.026891, loss_ce: 0.012115
2022-01-10 10:50:18,657 iteration 2383 : loss : 0.041355, loss_ce: 0.014503
2022-01-10 10:50:20,239 iteration 2384 : loss : 0.040563, loss_ce: 0.012650
2022-01-10 10:50:21,830 iteration 2385 : loss : 0.035738, loss_ce: 0.017519
2022-01-10 10:50:23,508 iteration 2386 : loss : 0.035547, loss_ce: 0.011524
2022-01-10 10:50:25,062 iteration 2387 : loss : 0.026701, loss_ce: 0.009484
2022-01-10 10:50:26,670 iteration 2388 : loss : 0.021095, loss_ce: 0.009383
2022-01-10 10:50:28,208 iteration 2389 : loss : 0.030862, loss_ce: 0.011917
2022-01-10 10:50:29,798 iteration 2390 : loss : 0.023911, loss_ce: 0.009818
2022-01-10 10:50:31,336 iteration 2391 : loss : 0.027566, loss_ce: 0.009521
2022-01-10 10:50:32,851 iteration 2392 : loss : 0.026937, loss_ce: 0.011661
2022-01-10 10:50:34,369 iteration 2393 : loss : 0.030586, loss_ce: 0.013280
2022-01-10 10:50:35,960 iteration 2394 : loss : 0.065150, loss_ce: 0.014620
2022-01-10 10:50:37,684 iteration 2395 : loss : 0.035732, loss_ce: 0.011571
2022-01-10 10:50:39,355 iteration 2396 : loss : 0.038456, loss_ce: 0.016811
2022-01-10 10:50:40,949 iteration 2397 : loss : 0.048516, loss_ce: 0.014539
 35%|█████████▌                 | 141/400 [1:09:20<2:09:12, 29.93s/it]2022-01-10 10:50:42,749 iteration 2398 : loss : 0.038626, loss_ce: 0.018623
2022-01-10 10:50:44,265 iteration 2399 : loss : 0.028209, loss_ce: 0.011251
2022-01-10 10:50:45,827 iteration 2400 : loss : 0.030467, loss_ce: 0.007826
2022-01-10 10:50:47,417 iteration 2401 : loss : 0.039519, loss_ce: 0.015177
2022-01-10 10:50:48,972 iteration 2402 : loss : 0.040318, loss_ce: 0.017193
2022-01-10 10:50:50,547 iteration 2403 : loss : 0.022696, loss_ce: 0.008225
2022-01-10 10:50:52,096 iteration 2404 : loss : 0.025063, loss_ce: 0.012044
2022-01-10 10:50:53,670 iteration 2405 : loss : 0.026043, loss_ce: 0.009796
2022-01-10 10:50:55,322 iteration 2406 : loss : 0.039163, loss_ce: 0.018404
2022-01-10 10:50:56,852 iteration 2407 : loss : 0.029733, loss_ce: 0.010362
2022-01-10 10:50:58,428 iteration 2408 : loss : 0.031747, loss_ce: 0.012171
2022-01-10 10:51:00,015 iteration 2409 : loss : 0.042381, loss_ce: 0.014420
2022-01-10 10:51:01,636 iteration 2410 : loss : 0.039675, loss_ce: 0.010893
2022-01-10 10:51:03,160 iteration 2411 : loss : 0.026280, loss_ce: 0.007866
2022-01-10 10:51:04,802 iteration 2412 : loss : 0.043091, loss_ce: 0.014607
2022-01-10 10:51:06,481 iteration 2413 : loss : 0.035076, loss_ce: 0.016408
2022-01-10 10:51:08,123 iteration 2414 : loss : 0.037875, loss_ce: 0.015127
 36%|█████████▌                 | 142/400 [1:09:47<2:05:08, 29.10s/it]2022-01-10 10:51:09,782 iteration 2415 : loss : 0.027054, loss_ce: 0.009012
2022-01-10 10:51:11,364 iteration 2416 : loss : 0.032345, loss_ce: 0.016104
2022-01-10 10:51:12,969 iteration 2417 : loss : 0.024581, loss_ce: 0.008318
2022-01-10 10:51:14,591 iteration 2418 : loss : 0.038953, loss_ce: 0.012231
2022-01-10 10:51:16,185 iteration 2419 : loss : 0.033702, loss_ce: 0.016839
2022-01-10 10:51:17,712 iteration 2420 : loss : 0.029838, loss_ce: 0.011798
2022-01-10 10:51:19,419 iteration 2421 : loss : 0.039544, loss_ce: 0.011921
2022-01-10 10:51:21,046 iteration 2422 : loss : 0.042829, loss_ce: 0.015364
2022-01-10 10:51:22,655 iteration 2423 : loss : 0.031295, loss_ce: 0.014242
2022-01-10 10:51:24,182 iteration 2424 : loss : 0.027920, loss_ce: 0.009857
2022-01-10 10:51:25,852 iteration 2425 : loss : 0.038877, loss_ce: 0.019847
2022-01-10 10:51:27,435 iteration 2426 : loss : 0.025348, loss_ce: 0.009748
2022-01-10 10:51:29,133 iteration 2427 : loss : 0.066508, loss_ce: 0.018601
2022-01-10 10:51:30,808 iteration 2428 : loss : 0.047738, loss_ce: 0.018471
2022-01-10 10:51:32,498 iteration 2429 : loss : 0.044751, loss_ce: 0.018330
2022-01-10 10:51:34,109 iteration 2430 : loss : 0.035002, loss_ce: 0.014714
2022-01-10 10:51:35,656 iteration 2431 : loss : 0.028883, loss_ce: 0.013944
 36%|█████████▋                 | 143/400 [1:10:14<2:02:39, 28.63s/it]2022-01-10 10:51:37,348 iteration 2432 : loss : 0.037632, loss_ce: 0.015183
2022-01-10 10:51:38,955 iteration 2433 : loss : 0.047385, loss_ce: 0.021230
2022-01-10 10:51:40,551 iteration 2434 : loss : 0.028501, loss_ce: 0.007872
2022-01-10 10:51:42,276 iteration 2435 : loss : 0.039287, loss_ce: 0.019677
2022-01-10 10:51:43,773 iteration 2436 : loss : 0.031157, loss_ce: 0.012101
2022-01-10 10:51:45,293 iteration 2437 : loss : 0.023183, loss_ce: 0.009281
2022-01-10 10:51:47,022 iteration 2438 : loss : 0.035310, loss_ce: 0.014546
2022-01-10 10:51:48,639 iteration 2439 : loss : 0.028268, loss_ce: 0.011522
2022-01-10 10:51:50,208 iteration 2440 : loss : 0.027768, loss_ce: 0.009652
2022-01-10 10:51:51,765 iteration 2441 : loss : 0.037106, loss_ce: 0.012421
2022-01-10 10:51:53,308 iteration 2442 : loss : 0.035553, loss_ce: 0.012531
2022-01-10 10:51:54,819 iteration 2443 : loss : 0.026972, loss_ce: 0.010709
2022-01-10 10:51:56,440 iteration 2444 : loss : 0.021284, loss_ce: 0.007664
2022-01-10 10:51:57,926 iteration 2445 : loss : 0.048703, loss_ce: 0.010464
2022-01-10 10:51:59,610 iteration 2446 : loss : 0.031385, loss_ce: 0.012274
2022-01-10 10:52:01,278 iteration 2447 : loss : 0.049096, loss_ce: 0.024843
2022-01-10 10:52:02,881 iteration 2448 : loss : 0.031415, loss_ce: 0.010453
 36%|█████████▋                 | 144/400 [1:10:41<2:00:21, 28.21s/it]2022-01-10 10:52:04,504 iteration 2449 : loss : 0.044912, loss_ce: 0.018560
2022-01-10 10:52:06,168 iteration 2450 : loss : 0.040006, loss_ce: 0.013595
2022-01-10 10:52:07,825 iteration 2451 : loss : 0.044089, loss_ce: 0.019618
2022-01-10 10:52:09,338 iteration 2452 : loss : 0.019949, loss_ce: 0.009174
2022-01-10 10:52:10,905 iteration 2453 : loss : 0.049160, loss_ce: 0.014548
2022-01-10 10:52:12,528 iteration 2454 : loss : 0.031165, loss_ce: 0.014004
2022-01-10 10:52:14,148 iteration 2455 : loss : 0.043120, loss_ce: 0.021951
2022-01-10 10:52:15,883 iteration 2456 : loss : 0.062107, loss_ce: 0.024506
2022-01-10 10:52:17,478 iteration 2457 : loss : 0.039594, loss_ce: 0.014040
2022-01-10 10:52:19,157 iteration 2458 : loss : 0.028399, loss_ce: 0.009290
2022-01-10 10:52:20,703 iteration 2459 : loss : 0.048952, loss_ce: 0.014469
2022-01-10 10:52:22,287 iteration 2460 : loss : 0.032647, loss_ce: 0.010576
2022-01-10 10:52:23,895 iteration 2461 : loss : 0.046935, loss_ce: 0.017806
2022-01-10 10:52:25,465 iteration 2462 : loss : 0.028854, loss_ce: 0.013207
2022-01-10 10:52:27,076 iteration 2463 : loss : 0.030534, loss_ce: 0.011137
2022-01-10 10:52:28,748 iteration 2464 : loss : 0.067497, loss_ce: 0.029538
2022-01-10 10:52:28,748 Training Data Eval:
2022-01-10 10:52:36,713   Average segmentation loss on training set: 0.0264
2022-01-10 10:52:36,714 Validation Data Eval:
2022-01-10 10:52:39,467   Average segmentation loss on validation set: 0.0708
2022-01-10 10:52:41,018 iteration 2465 : loss : 0.041340, loss_ce: 0.015449
 36%|█████████▊                 | 145/400 [1:11:20<2:12:32, 31.19s/it]2022-01-10 10:52:42,634 iteration 2466 : loss : 0.029107, loss_ce: 0.013029
2022-01-10 10:52:44,268 iteration 2467 : loss : 0.040183, loss_ce: 0.023638
2022-01-10 10:52:45,813 iteration 2468 : loss : 0.033133, loss_ce: 0.014748
2022-01-10 10:52:47,319 iteration 2469 : loss : 0.022371, loss_ce: 0.011303
2022-01-10 10:52:48,949 iteration 2470 : loss : 0.060546, loss_ce: 0.024673
2022-01-10 10:52:50,587 iteration 2471 : loss : 0.038605, loss_ce: 0.016043
2022-01-10 10:52:52,278 iteration 2472 : loss : 0.038103, loss_ce: 0.015107
2022-01-10 10:52:53,792 iteration 2473 : loss : 0.037138, loss_ce: 0.015841
2022-01-10 10:52:55,422 iteration 2474 : loss : 0.044251, loss_ce: 0.016482
2022-01-10 10:52:56,944 iteration 2475 : loss : 0.027878, loss_ce: 0.013045
2022-01-10 10:52:58,629 iteration 2476 : loss : 0.051488, loss_ce: 0.013057
2022-01-10 10:53:00,215 iteration 2477 : loss : 0.048105, loss_ce: 0.023266
2022-01-10 10:53:01,796 iteration 2478 : loss : 0.040246, loss_ce: 0.010931
2022-01-10 10:53:03,514 iteration 2479 : loss : 0.034316, loss_ce: 0.010441
2022-01-10 10:53:05,197 iteration 2480 : loss : 0.045544, loss_ce: 0.020190
2022-01-10 10:53:06,768 iteration 2481 : loss : 0.030776, loss_ce: 0.010805
2022-01-10 10:53:08,375 iteration 2482 : loss : 0.054330, loss_ce: 0.024972
 36%|█████████▊                 | 146/400 [1:11:47<2:07:10, 30.04s/it]2022-01-10 10:53:10,046 iteration 2483 : loss : 0.028168, loss_ce: 0.010858
2022-01-10 10:53:11,712 iteration 2484 : loss : 0.047413, loss_ce: 0.014649
2022-01-10 10:53:13,417 iteration 2485 : loss : 0.043973, loss_ce: 0.015811
2022-01-10 10:53:14,971 iteration 2486 : loss : 0.043700, loss_ce: 0.018852
2022-01-10 10:53:16,544 iteration 2487 : loss : 0.028343, loss_ce: 0.009333
2022-01-10 10:53:18,198 iteration 2488 : loss : 0.034270, loss_ce: 0.010244
2022-01-10 10:53:19,870 iteration 2489 : loss : 0.034651, loss_ce: 0.014758
2022-01-10 10:53:21,424 iteration 2490 : loss : 0.030230, loss_ce: 0.012957
2022-01-10 10:53:23,031 iteration 2491 : loss : 0.041673, loss_ce: 0.016463
2022-01-10 10:53:24,579 iteration 2492 : loss : 0.034185, loss_ce: 0.014099
2022-01-10 10:53:26,141 iteration 2493 : loss : 0.032171, loss_ce: 0.010539
2022-01-10 10:53:27,685 iteration 2494 : loss : 0.032844, loss_ce: 0.013127
2022-01-10 10:53:29,259 iteration 2495 : loss : 0.035946, loss_ce: 0.011704
2022-01-10 10:53:30,892 iteration 2496 : loss : 0.042443, loss_ce: 0.010894
2022-01-10 10:53:32,469 iteration 2497 : loss : 0.037790, loss_ce: 0.015479
2022-01-10 10:53:33,968 iteration 2498 : loss : 0.034918, loss_ce: 0.016743
2022-01-10 10:53:35,621 iteration 2499 : loss : 0.032430, loss_ce: 0.014521
 37%|█████████▉                 | 147/400 [1:12:14<2:03:06, 29.20s/it]2022-01-10 10:53:37,296 iteration 2500 : loss : 0.035251, loss_ce: 0.012815
2022-01-10 10:53:38,829 iteration 2501 : loss : 0.027678, loss_ce: 0.009744
2022-01-10 10:53:40,395 iteration 2502 : loss : 0.032400, loss_ce: 0.010765
2022-01-10 10:53:41,990 iteration 2503 : loss : 0.031886, loss_ce: 0.015630
2022-01-10 10:53:43,608 iteration 2504 : loss : 0.047777, loss_ce: 0.014707
2022-01-10 10:53:45,131 iteration 2505 : loss : 0.026108, loss_ce: 0.011772
2022-01-10 10:53:46,674 iteration 2506 : loss : 0.038593, loss_ce: 0.011514
2022-01-10 10:53:48,297 iteration 2507 : loss : 0.023898, loss_ce: 0.009213
2022-01-10 10:53:49,862 iteration 2508 : loss : 0.027489, loss_ce: 0.013139
2022-01-10 10:53:51,506 iteration 2509 : loss : 0.050186, loss_ce: 0.021727
2022-01-10 10:53:53,070 iteration 2510 : loss : 0.036688, loss_ce: 0.013209
2022-01-10 10:53:54,593 iteration 2511 : loss : 0.025725, loss_ce: 0.012566
2022-01-10 10:53:56,233 iteration 2512 : loss : 0.026125, loss_ce: 0.008298
2022-01-10 10:53:57,805 iteration 2513 : loss : 0.030102, loss_ce: 0.011865
2022-01-10 10:53:59,432 iteration 2514 : loss : 0.034929, loss_ce: 0.010581
2022-01-10 10:54:00,988 iteration 2515 : loss : 0.028001, loss_ce: 0.013811
2022-01-10 10:54:02,752 iteration 2516 : loss : 0.041739, loss_ce: 0.015908
 37%|█████████▉                 | 148/400 [1:12:41<2:00:02, 28.58s/it]2022-01-10 10:54:04,461 iteration 2517 : loss : 0.040580, loss_ce: 0.013106
2022-01-10 10:54:06,093 iteration 2518 : loss : 0.033759, loss_ce: 0.010733
2022-01-10 10:54:07,609 iteration 2519 : loss : 0.019793, loss_ce: 0.006186
2022-01-10 10:54:09,235 iteration 2520 : loss : 0.036619, loss_ce: 0.016045
2022-01-10 10:54:10,834 iteration 2521 : loss : 0.033071, loss_ce: 0.015325
2022-01-10 10:54:12,424 iteration 2522 : loss : 0.036414, loss_ce: 0.013866
2022-01-10 10:54:13,997 iteration 2523 : loss : 0.029268, loss_ce: 0.008838
2022-01-10 10:54:15,638 iteration 2524 : loss : 0.047694, loss_ce: 0.018196
2022-01-10 10:54:17,182 iteration 2525 : loss : 0.032167, loss_ce: 0.008948
2022-01-10 10:54:18,693 iteration 2526 : loss : 0.032251, loss_ce: 0.011395
2022-01-10 10:54:20,287 iteration 2527 : loss : 0.033623, loss_ce: 0.017623
2022-01-10 10:54:21,865 iteration 2528 : loss : 0.031435, loss_ce: 0.014694
2022-01-10 10:54:23,483 iteration 2529 : loss : 0.036979, loss_ce: 0.015380
2022-01-10 10:54:24,993 iteration 2530 : loss : 0.026176, loss_ce: 0.009534
2022-01-10 10:54:26,591 iteration 2531 : loss : 0.023969, loss_ce: 0.007061
2022-01-10 10:54:28,247 iteration 2532 : loss : 0.038607, loss_ce: 0.018163
2022-01-10 10:54:29,764 iteration 2533 : loss : 0.038455, loss_ce: 0.010470
 37%|██████████                 | 149/400 [1:13:08<1:57:34, 28.11s/it]2022-01-10 10:54:31,426 iteration 2534 : loss : 0.025676, loss_ce: 0.009279
2022-01-10 10:54:32,904 iteration 2535 : loss : 0.032323, loss_ce: 0.013449
2022-01-10 10:54:34,424 iteration 2536 : loss : 0.019868, loss_ce: 0.008908
2022-01-10 10:54:36,091 iteration 2537 : loss : 0.038426, loss_ce: 0.019570
2022-01-10 10:54:37,543 iteration 2538 : loss : 0.027933, loss_ce: 0.009858
2022-01-10 10:54:39,045 iteration 2539 : loss : 0.021349, loss_ce: 0.008687
2022-01-10 10:54:40,603 iteration 2540 : loss : 0.041514, loss_ce: 0.017876
2022-01-10 10:54:42,247 iteration 2541 : loss : 0.035157, loss_ce: 0.013460
2022-01-10 10:54:43,823 iteration 2542 : loss : 0.041750, loss_ce: 0.013013
2022-01-10 10:54:45,492 iteration 2543 : loss : 0.034751, loss_ce: 0.010733
2022-01-10 10:54:47,135 iteration 2544 : loss : 0.053313, loss_ce: 0.020364
2022-01-10 10:54:48,663 iteration 2545 : loss : 0.023036, loss_ce: 0.009930
2022-01-10 10:54:50,441 iteration 2546 : loss : 0.043617, loss_ce: 0.015737
2022-01-10 10:54:52,090 iteration 2547 : loss : 0.045336, loss_ce: 0.015808
2022-01-10 10:54:53,755 iteration 2548 : loss : 0.030857, loss_ce: 0.013648
2022-01-10 10:54:55,334 iteration 2549 : loss : 0.042250, loss_ce: 0.018589
2022-01-10 10:54:55,335 Training Data Eval:
2022-01-10 10:55:03,281   Average segmentation loss on training set: 0.0215
2022-01-10 10:55:03,282 Validation Data Eval:
2022-01-10 10:55:06,016   Average segmentation loss on validation set: 0.0808
2022-01-10 10:55:07,595 iteration 2550 : loss : 0.029767, loss_ce: 0.006677
 38%|██████████▏                | 150/400 [1:13:46<2:09:16, 31.02s/it]2022-01-10 10:55:09,237 iteration 2551 : loss : 0.039849, loss_ce: 0.015148
2022-01-10 10:55:10,840 iteration 2552 : loss : 0.027667, loss_ce: 0.010558
2022-01-10 10:55:12,439 iteration 2553 : loss : 0.028874, loss_ce: 0.010577
2022-01-10 10:55:14,000 iteration 2554 : loss : 0.041894, loss_ce: 0.020266
2022-01-10 10:55:15,557 iteration 2555 : loss : 0.030130, loss_ce: 0.013699
2022-01-10 10:55:17,129 iteration 2556 : loss : 0.034625, loss_ce: 0.014439
2022-01-10 10:55:18,707 iteration 2557 : loss : 0.028508, loss_ce: 0.011475
2022-01-10 10:55:20,253 iteration 2558 : loss : 0.031343, loss_ce: 0.010984
2022-01-10 10:55:21,771 iteration 2559 : loss : 0.030020, loss_ce: 0.014939
2022-01-10 10:55:23,446 iteration 2560 : loss : 0.033797, loss_ce: 0.012451
2022-01-10 10:55:25,015 iteration 2561 : loss : 0.054480, loss_ce: 0.021757
2022-01-10 10:55:26,623 iteration 2562 : loss : 0.031342, loss_ce: 0.013486
2022-01-10 10:55:28,121 iteration 2563 : loss : 0.040096, loss_ce: 0.015632
2022-01-10 10:55:29,651 iteration 2564 : loss : 0.022263, loss_ce: 0.009527
2022-01-10 10:55:31,167 iteration 2565 : loss : 0.020413, loss_ce: 0.007653
2022-01-10 10:55:32,663 iteration 2566 : loss : 0.024220, loss_ce: 0.008730
2022-01-10 10:55:34,210 iteration 2567 : loss : 0.038174, loss_ce: 0.009232
 38%|██████████▏                | 151/400 [1:14:13<2:03:16, 29.70s/it]2022-01-10 10:55:35,827 iteration 2568 : loss : 0.026197, loss_ce: 0.012450
2022-01-10 10:55:37,387 iteration 2569 : loss : 0.027732, loss_ce: 0.011101
2022-01-10 10:55:39,035 iteration 2570 : loss : 0.029225, loss_ce: 0.009474
2022-01-10 10:55:40,589 iteration 2571 : loss : 0.027581, loss_ce: 0.008460
2022-01-10 10:55:42,091 iteration 2572 : loss : 0.032300, loss_ce: 0.010913
2022-01-10 10:55:43,633 iteration 2573 : loss : 0.030264, loss_ce: 0.011856
2022-01-10 10:55:45,163 iteration 2574 : loss : 0.030408, loss_ce: 0.012530
2022-01-10 10:55:46,661 iteration 2575 : loss : 0.027777, loss_ce: 0.008611
2022-01-10 10:55:48,160 iteration 2576 : loss : 0.025357, loss_ce: 0.007237
2022-01-10 10:55:49,763 iteration 2577 : loss : 0.045101, loss_ce: 0.028641
2022-01-10 10:55:51,297 iteration 2578 : loss : 0.030621, loss_ce: 0.011355
2022-01-10 10:55:52,871 iteration 2579 : loss : 0.027196, loss_ce: 0.010727
2022-01-10 10:55:54,384 iteration 2580 : loss : 0.036907, loss_ce: 0.011323
2022-01-10 10:55:55,969 iteration 2581 : loss : 0.033224, loss_ce: 0.013112
2022-01-10 10:55:57,603 iteration 2582 : loss : 0.025419, loss_ce: 0.012698
2022-01-10 10:55:59,146 iteration 2583 : loss : 0.027213, loss_ce: 0.012134
2022-01-10 10:56:00,696 iteration 2584 : loss : 0.024883, loss_ce: 0.008126
 38%|██████████▎                | 152/400 [1:14:39<1:58:47, 28.74s/it]2022-01-10 10:56:02,334 iteration 2585 : loss : 0.040865, loss_ce: 0.016142
2022-01-10 10:56:04,080 iteration 2586 : loss : 0.050394, loss_ce: 0.013130
2022-01-10 10:56:05,732 iteration 2587 : loss : 0.037012, loss_ce: 0.012868
2022-01-10 10:56:07,314 iteration 2588 : loss : 0.055231, loss_ce: 0.020629
2022-01-10 10:56:08,812 iteration 2589 : loss : 0.036862, loss_ce: 0.009913
2022-01-10 10:56:10,427 iteration 2590 : loss : 0.046738, loss_ce: 0.015904
2022-01-10 10:56:12,000 iteration 2591 : loss : 0.057678, loss_ce: 0.030504
2022-01-10 10:56:13,537 iteration 2592 : loss : 0.023664, loss_ce: 0.011907
2022-01-10 10:56:15,007 iteration 2593 : loss : 0.027647, loss_ce: 0.012896
2022-01-10 10:56:16,579 iteration 2594 : loss : 0.041005, loss_ce: 0.018769
2022-01-10 10:56:18,090 iteration 2595 : loss : 0.023364, loss_ce: 0.008095
2022-01-10 10:56:19,621 iteration 2596 : loss : 0.036152, loss_ce: 0.011206
2022-01-10 10:56:21,167 iteration 2597 : loss : 0.033933, loss_ce: 0.012168
2022-01-10 10:56:22,737 iteration 2598 : loss : 0.024417, loss_ce: 0.009748
2022-01-10 10:56:24,314 iteration 2599 : loss : 0.024127, loss_ce: 0.009876
2022-01-10 10:56:25,842 iteration 2600 : loss : 0.035792, loss_ce: 0.014421
2022-01-10 10:56:27,356 iteration 2601 : loss : 0.028855, loss_ce: 0.010434
 38%|██████████▎                | 153/400 [1:15:06<1:55:44, 28.12s/it]2022-01-10 10:56:28,959 iteration 2602 : loss : 0.036068, loss_ce: 0.011561
2022-01-10 10:56:30,544 iteration 2603 : loss : 0.049589, loss_ce: 0.010693
2022-01-10 10:56:32,125 iteration 2604 : loss : 0.023604, loss_ce: 0.008436
2022-01-10 10:56:33,685 iteration 2605 : loss : 0.030298, loss_ce: 0.009768
2022-01-10 10:56:35,250 iteration 2606 : loss : 0.030487, loss_ce: 0.011565
2022-01-10 10:56:36,847 iteration 2607 : loss : 0.027554, loss_ce: 0.011981
2022-01-10 10:56:38,369 iteration 2608 : loss : 0.031108, loss_ce: 0.012260
2022-01-10 10:56:39,992 iteration 2609 : loss : 0.026173, loss_ce: 0.009249
2022-01-10 10:56:41,592 iteration 2610 : loss : 0.031638, loss_ce: 0.011337
2022-01-10 10:56:43,225 iteration 2611 : loss : 0.037563, loss_ce: 0.016429
2022-01-10 10:56:44,848 iteration 2612 : loss : 0.035242, loss_ce: 0.011997
2022-01-10 10:56:46,495 iteration 2613 : loss : 0.033166, loss_ce: 0.012330
2022-01-10 10:56:48,124 iteration 2614 : loss : 0.034295, loss_ce: 0.014528
2022-01-10 10:56:49,726 iteration 2615 : loss : 0.030881, loss_ce: 0.013167
2022-01-10 10:56:51,359 iteration 2616 : loss : 0.036870, loss_ce: 0.014015
2022-01-10 10:56:52,962 iteration 2617 : loss : 0.028236, loss_ce: 0.010732
2022-01-10 10:56:54,598 iteration 2618 : loss : 0.028822, loss_ce: 0.009339
 38%|██████████▍                | 154/400 [1:15:33<1:54:12, 27.85s/it]2022-01-10 10:56:56,375 iteration 2619 : loss : 0.028622, loss_ce: 0.012574
2022-01-10 10:56:57,989 iteration 2620 : loss : 0.028463, loss_ce: 0.010724
2022-01-10 10:56:59,465 iteration 2621 : loss : 0.029170, loss_ce: 0.011959
2022-01-10 10:57:01,067 iteration 2622 : loss : 0.050204, loss_ce: 0.016202
2022-01-10 10:57:02,604 iteration 2623 : loss : 0.024523, loss_ce: 0.009095
2022-01-10 10:57:04,229 iteration 2624 : loss : 0.038454, loss_ce: 0.013795
2022-01-10 10:57:05,871 iteration 2625 : loss : 0.031748, loss_ce: 0.013891
2022-01-10 10:57:07,479 iteration 2626 : loss : 0.028794, loss_ce: 0.012166
2022-01-10 10:57:08,987 iteration 2627 : loss : 0.025230, loss_ce: 0.012494
2022-01-10 10:57:10,502 iteration 2628 : loss : 0.029912, loss_ce: 0.011168
2022-01-10 10:57:11,992 iteration 2629 : loss : 0.030927, loss_ce: 0.007640
2022-01-10 10:57:13,591 iteration 2630 : loss : 0.050069, loss_ce: 0.009519
2022-01-10 10:57:15,097 iteration 2631 : loss : 0.035267, loss_ce: 0.017150
2022-01-10 10:57:16,612 iteration 2632 : loss : 0.025158, loss_ce: 0.008314
2022-01-10 10:57:18,199 iteration 2633 : loss : 0.018833, loss_ce: 0.006419
2022-01-10 10:57:19,750 iteration 2634 : loss : 0.031450, loss_ce: 0.010677
2022-01-10 10:57:19,750 Training Data Eval:
2022-01-10 10:57:27,693   Average segmentation loss on training set: 0.0228
2022-01-10 10:57:27,693 Validation Data Eval:
2022-01-10 10:57:30,432   Average segmentation loss on validation set: 0.0814
2022-01-10 10:57:31,953 iteration 2635 : loss : 0.031668, loss_ce: 0.013671
 39%|██████████▍                | 155/400 [1:16:11<2:05:21, 30.70s/it]2022-01-10 10:57:33,517 iteration 2636 : loss : 0.032950, loss_ce: 0.010060
2022-01-10 10:57:35,068 iteration 2637 : loss : 0.022598, loss_ce: 0.010293
2022-01-10 10:57:36,721 iteration 2638 : loss : 0.036496, loss_ce: 0.014521
2022-01-10 10:57:38,276 iteration 2639 : loss : 0.027667, loss_ce: 0.010656
2022-01-10 10:57:39,858 iteration 2640 : loss : 0.032788, loss_ce: 0.012203
2022-01-10 10:57:41,429 iteration 2641 : loss : 0.023231, loss_ce: 0.009444
2022-01-10 10:57:43,035 iteration 2642 : loss : 0.024546, loss_ce: 0.008906
2022-01-10 10:57:44,590 iteration 2643 : loss : 0.031317, loss_ce: 0.013897
2022-01-10 10:57:46,185 iteration 2644 : loss : 0.033751, loss_ce: 0.017108
2022-01-10 10:57:47,697 iteration 2645 : loss : 0.024878, loss_ce: 0.010631
2022-01-10 10:57:49,218 iteration 2646 : loss : 0.026924, loss_ce: 0.008386
2022-01-10 10:57:50,852 iteration 2647 : loss : 0.037272, loss_ce: 0.015584
2022-01-10 10:57:52,389 iteration 2648 : loss : 0.023703, loss_ce: 0.008707
2022-01-10 10:57:53,960 iteration 2649 : loss : 0.037330, loss_ce: 0.012959
2022-01-10 10:57:55,652 iteration 2650 : loss : 0.036749, loss_ce: 0.013440
2022-01-10 10:57:57,206 iteration 2651 : loss : 0.037396, loss_ce: 0.014376
2022-01-10 10:57:58,851 iteration 2652 : loss : 0.032147, loss_ce: 0.013805
 39%|██████████▌                | 156/400 [1:16:37<2:00:12, 29.56s/it]2022-01-10 10:58:00,546 iteration 2653 : loss : 0.033789, loss_ce: 0.014014
2022-01-10 10:58:02,056 iteration 2654 : loss : 0.019964, loss_ce: 0.008149
2022-01-10 10:58:03,608 iteration 2655 : loss : 0.024910, loss_ce: 0.010475
2022-01-10 10:58:05,143 iteration 2656 : loss : 0.021659, loss_ce: 0.010404
2022-01-10 10:58:06,726 iteration 2657 : loss : 0.028268, loss_ce: 0.011743
2022-01-10 10:58:08,369 iteration 2658 : loss : 0.059543, loss_ce: 0.019133
2022-01-10 10:58:09,962 iteration 2659 : loss : 0.026699, loss_ce: 0.009531
2022-01-10 10:58:11,625 iteration 2660 : loss : 0.044462, loss_ce: 0.021758
2022-01-10 10:58:13,113 iteration 2661 : loss : 0.027982, loss_ce: 0.009854
2022-01-10 10:58:14,745 iteration 2662 : loss : 0.022904, loss_ce: 0.008739
2022-01-10 10:58:16,374 iteration 2663 : loss : 0.035799, loss_ce: 0.013604
2022-01-10 10:58:17,881 iteration 2664 : loss : 0.035114, loss_ce: 0.011962
2022-01-10 10:58:19,415 iteration 2665 : loss : 0.016430, loss_ce: 0.007076
2022-01-10 10:58:21,078 iteration 2666 : loss : 0.044440, loss_ce: 0.016320
2022-01-10 10:58:22,604 iteration 2667 : loss : 0.028341, loss_ce: 0.010617
2022-01-10 10:58:24,253 iteration 2668 : loss : 0.033758, loss_ce: 0.015053
2022-01-10 10:58:25,819 iteration 2669 : loss : 0.035758, loss_ce: 0.010478
 39%|██████████▌                | 157/400 [1:17:04<1:56:34, 28.78s/it]2022-01-10 10:58:27,376 iteration 2670 : loss : 0.020983, loss_ce: 0.008655
2022-01-10 10:58:28,876 iteration 2671 : loss : 0.031390, loss_ce: 0.010684
2022-01-10 10:58:30,404 iteration 2672 : loss : 0.036987, loss_ce: 0.013657
2022-01-10 10:58:31,941 iteration 2673 : loss : 0.028602, loss_ce: 0.008250
2022-01-10 10:58:33,528 iteration 2674 : loss : 0.044526, loss_ce: 0.013784
2022-01-10 10:58:35,149 iteration 2675 : loss : 0.021936, loss_ce: 0.007794
2022-01-10 10:58:36,710 iteration 2676 : loss : 0.025731, loss_ce: 0.008383
2022-01-10 10:58:38,269 iteration 2677 : loss : 0.030845, loss_ce: 0.012914
2022-01-10 10:58:39,777 iteration 2678 : loss : 0.025739, loss_ce: 0.011573
2022-01-10 10:58:41,356 iteration 2679 : loss : 0.033223, loss_ce: 0.013275
2022-01-10 10:58:42,995 iteration 2680 : loss : 0.035999, loss_ce: 0.015430
2022-01-10 10:58:44,577 iteration 2681 : loss : 0.032886, loss_ce: 0.010901
2022-01-10 10:58:46,195 iteration 2682 : loss : 0.023629, loss_ce: 0.009111
2022-01-10 10:58:47,730 iteration 2683 : loss : 0.024005, loss_ce: 0.010420
2022-01-10 10:58:49,326 iteration 2684 : loss : 0.027485, loss_ce: 0.011897
2022-01-10 10:58:50,949 iteration 2685 : loss : 0.028923, loss_ce: 0.013666
2022-01-10 10:58:52,550 iteration 2686 : loss : 0.050676, loss_ce: 0.011042
 40%|██████████▋                | 158/400 [1:17:31<1:53:36, 28.17s/it]2022-01-10 10:58:54,140 iteration 2687 : loss : 0.036573, loss_ce: 0.016225
2022-01-10 10:58:55,698 iteration 2688 : loss : 0.032217, loss_ce: 0.011897
2022-01-10 10:58:57,235 iteration 2689 : loss : 0.028942, loss_ce: 0.009674
2022-01-10 10:58:58,864 iteration 2690 : loss : 0.048322, loss_ce: 0.011021
2022-01-10 10:59:00,390 iteration 2691 : loss : 0.023658, loss_ce: 0.009577
2022-01-10 10:59:01,916 iteration 2692 : loss : 0.052181, loss_ce: 0.022863
2022-01-10 10:59:03,429 iteration 2693 : loss : 0.024359, loss_ce: 0.010374
2022-01-10 10:59:05,028 iteration 2694 : loss : 0.036604, loss_ce: 0.015991
2022-01-10 10:59:06,582 iteration 2695 : loss : 0.042636, loss_ce: 0.023215
2022-01-10 10:59:08,149 iteration 2696 : loss : 0.025967, loss_ce: 0.012452
2022-01-10 10:59:09,677 iteration 2697 : loss : 0.044261, loss_ce: 0.018703
2022-01-10 10:59:11,201 iteration 2698 : loss : 0.036277, loss_ce: 0.011124
2022-01-10 10:59:12,748 iteration 2699 : loss : 0.034529, loss_ce: 0.014602
2022-01-10 10:59:14,279 iteration 2700 : loss : 0.036036, loss_ce: 0.011759
2022-01-10 10:59:15,974 iteration 2701 : loss : 0.039542, loss_ce: 0.012215
2022-01-10 10:59:17,587 iteration 2702 : loss : 0.028537, loss_ce: 0.014569
2022-01-10 10:59:19,241 iteration 2703 : loss : 0.029748, loss_ce: 0.012311
 40%|██████████▋                | 159/400 [1:17:58<1:51:21, 27.73s/it]2022-01-10 10:59:20,937 iteration 2704 : loss : 0.051554, loss_ce: 0.016587
2022-01-10 10:59:22,475 iteration 2705 : loss : 0.025280, loss_ce: 0.011468
2022-01-10 10:59:24,097 iteration 2706 : loss : 0.045238, loss_ce: 0.018546
2022-01-10 10:59:25,659 iteration 2707 : loss : 0.032965, loss_ce: 0.016622
2022-01-10 10:59:27,264 iteration 2708 : loss : 0.037583, loss_ce: 0.014488
2022-01-10 10:59:28,827 iteration 2709 : loss : 0.026159, loss_ce: 0.010611
2022-01-10 10:59:30,332 iteration 2710 : loss : 0.023604, loss_ce: 0.009900
2022-01-10 10:59:31,922 iteration 2711 : loss : 0.026808, loss_ce: 0.010105
2022-01-10 10:59:33,527 iteration 2712 : loss : 0.028202, loss_ce: 0.008389
2022-01-10 10:59:35,058 iteration 2713 : loss : 0.032621, loss_ce: 0.015040
2022-01-10 10:59:36,706 iteration 2714 : loss : 0.029609, loss_ce: 0.011409
2022-01-10 10:59:38,334 iteration 2715 : loss : 0.050093, loss_ce: 0.019228
2022-01-10 10:59:39,961 iteration 2716 : loss : 0.027812, loss_ce: 0.008953
2022-01-10 10:59:41,526 iteration 2717 : loss : 0.032880, loss_ce: 0.013796
2022-01-10 10:59:43,131 iteration 2718 : loss : 0.059899, loss_ce: 0.016127
2022-01-10 10:59:44,674 iteration 2719 : loss : 0.027489, loss_ce: 0.011121
2022-01-10 10:59:44,674 Training Data Eval:
2022-01-10 10:59:52,626   Average segmentation loss on training set: 0.0223
2022-01-10 10:59:52,627 Validation Data Eval:
2022-01-10 10:59:55,366   Average segmentation loss on validation set: 0.0758
2022-01-10 10:59:56,886 iteration 2720 : loss : 0.032467, loss_ce: 0.010297
 40%|██████████▊                | 160/400 [1:18:35<2:02:47, 30.70s/it]2022-01-10 10:59:58,544 iteration 2721 : loss : 0.026945, loss_ce: 0.011504
2022-01-10 11:00:00,137 iteration 2722 : loss : 0.048206, loss_ce: 0.017668
2022-01-10 11:00:01,744 iteration 2723 : loss : 0.031475, loss_ce: 0.011613
2022-01-10 11:00:03,341 iteration 2724 : loss : 0.041687, loss_ce: 0.014095
2022-01-10 11:00:04,897 iteration 2725 : loss : 0.043021, loss_ce: 0.014419
2022-01-10 11:00:06,623 iteration 2726 : loss : 0.063822, loss_ce: 0.020211
2022-01-10 11:00:08,272 iteration 2727 : loss : 0.029846, loss_ce: 0.013078
2022-01-10 11:00:09,853 iteration 2728 : loss : 0.022379, loss_ce: 0.008519
2022-01-10 11:00:11,429 iteration 2729 : loss : 0.049594, loss_ce: 0.021134
2022-01-10 11:00:12,935 iteration 2730 : loss : 0.022136, loss_ce: 0.008347
2022-01-10 11:00:14,533 iteration 2731 : loss : 0.033567, loss_ce: 0.013823
2022-01-10 11:00:16,188 iteration 2732 : loss : 0.038688, loss_ce: 0.018993
2022-01-10 11:00:17,786 iteration 2733 : loss : 0.028396, loss_ce: 0.010657
2022-01-10 11:00:19,373 iteration 2734 : loss : 0.031981, loss_ce: 0.011554
2022-01-10 11:00:21,049 iteration 2735 : loss : 0.033842, loss_ce: 0.009177
2022-01-10 11:00:22,623 iteration 2736 : loss : 0.020725, loss_ce: 0.009020
2022-01-10 11:00:24,243 iteration 2737 : loss : 0.033219, loss_ce: 0.012025
 40%|██████████▊                | 161/400 [1:19:03<1:58:17, 29.70s/it]2022-01-10 11:00:25,824 iteration 2738 : loss : 0.019564, loss_ce: 0.007321
2022-01-10 11:00:27,479 iteration 2739 : loss : 0.027614, loss_ce: 0.011795
2022-01-10 11:00:29,147 iteration 2740 : loss : 0.048049, loss_ce: 0.019685
2022-01-10 11:00:30,697 iteration 2741 : loss : 0.029833, loss_ce: 0.011292
2022-01-10 11:00:32,204 iteration 2742 : loss : 0.033249, loss_ce: 0.011350
2022-01-10 11:00:33,799 iteration 2743 : loss : 0.026849, loss_ce: 0.008909
2022-01-10 11:00:35,533 iteration 2744 : loss : 0.045989, loss_ce: 0.016395
2022-01-10 11:00:37,106 iteration 2745 : loss : 0.046306, loss_ce: 0.017015
2022-01-10 11:00:38,668 iteration 2746 : loss : 0.024537, loss_ce: 0.008563
2022-01-10 11:00:40,407 iteration 2747 : loss : 0.054327, loss_ce: 0.024683
2022-01-10 11:00:41,953 iteration 2748 : loss : 0.024346, loss_ce: 0.009285
2022-01-10 11:00:43,553 iteration 2749 : loss : 0.027233, loss_ce: 0.012715
2022-01-10 11:00:45,110 iteration 2750 : loss : 0.041008, loss_ce: 0.014395
2022-01-10 11:00:46,677 iteration 2751 : loss : 0.042345, loss_ce: 0.010325
2022-01-10 11:00:48,212 iteration 2752 : loss : 0.027322, loss_ce: 0.011953
2022-01-10 11:00:49,777 iteration 2753 : loss : 0.035386, loss_ce: 0.016694
2022-01-10 11:00:51,371 iteration 2754 : loss : 0.040223, loss_ce: 0.012578
 40%|██████████▉                | 162/400 [1:19:30<1:54:44, 28.93s/it]2022-01-10 11:00:53,004 iteration 2755 : loss : 0.028141, loss_ce: 0.013680
2022-01-10 11:00:54,610 iteration 2756 : loss : 0.030508, loss_ce: 0.013416
2022-01-10 11:00:56,143 iteration 2757 : loss : 0.027924, loss_ce: 0.011971
2022-01-10 11:00:57,715 iteration 2758 : loss : 0.027934, loss_ce: 0.010745
2022-01-10 11:00:59,272 iteration 2759 : loss : 0.024143, loss_ce: 0.008904
2022-01-10 11:01:00,897 iteration 2760 : loss : 0.049885, loss_ce: 0.011943
2022-01-10 11:01:02,480 iteration 2761 : loss : 0.032277, loss_ce: 0.013645
2022-01-10 11:01:04,042 iteration 2762 : loss : 0.031484, loss_ce: 0.011007
2022-01-10 11:01:05,610 iteration 2763 : loss : 0.034688, loss_ce: 0.016649
2022-01-10 11:01:07,112 iteration 2764 : loss : 0.024938, loss_ce: 0.010445
2022-01-10 11:01:08,685 iteration 2765 : loss : 0.034857, loss_ce: 0.015664
2022-01-10 11:01:10,312 iteration 2766 : loss : 0.036182, loss_ce: 0.012620
2022-01-10 11:01:11,820 iteration 2767 : loss : 0.025656, loss_ce: 0.006445
2022-01-10 11:01:13,522 iteration 2768 : loss : 0.033349, loss_ce: 0.013777
2022-01-10 11:01:15,074 iteration 2769 : loss : 0.031452, loss_ce: 0.011768
2022-01-10 11:01:16,600 iteration 2770 : loss : 0.029591, loss_ce: 0.014789
2022-01-10 11:01:18,154 iteration 2771 : loss : 0.042981, loss_ce: 0.021243
 41%|███████████                | 163/400 [1:19:57<1:51:43, 28.29s/it]2022-01-10 11:01:19,868 iteration 2772 : loss : 0.033750, loss_ce: 0.012735
2022-01-10 11:01:21,379 iteration 2773 : loss : 0.036185, loss_ce: 0.011367
2022-01-10 11:01:22,925 iteration 2774 : loss : 0.029515, loss_ce: 0.013157
2022-01-10 11:01:24,504 iteration 2775 : loss : 0.030589, loss_ce: 0.013355
2022-01-10 11:01:26,083 iteration 2776 : loss : 0.036084, loss_ce: 0.016298
2022-01-10 11:01:27,706 iteration 2777 : loss : 0.031383, loss_ce: 0.013856
2022-01-10 11:01:29,204 iteration 2778 : loss : 0.021484, loss_ce: 0.009953
2022-01-10 11:01:30,826 iteration 2779 : loss : 0.035107, loss_ce: 0.016001
2022-01-10 11:01:32,486 iteration 2780 : loss : 0.026704, loss_ce: 0.010501
2022-01-10 11:01:33,977 iteration 2781 : loss : 0.027048, loss_ce: 0.010632
2022-01-10 11:01:35,589 iteration 2782 : loss : 0.031042, loss_ce: 0.012227
2022-01-10 11:01:37,231 iteration 2783 : loss : 0.030874, loss_ce: 0.010844
2022-01-10 11:01:38,921 iteration 2784 : loss : 0.028185, loss_ce: 0.013522
2022-01-10 11:01:40,438 iteration 2785 : loss : 0.024131, loss_ce: 0.008910
2022-01-10 11:01:42,023 iteration 2786 : loss : 0.025679, loss_ce: 0.008535
2022-01-10 11:01:43,694 iteration 2787 : loss : 0.056672, loss_ce: 0.023350
2022-01-10 11:01:45,288 iteration 2788 : loss : 0.027804, loss_ce: 0.008740
 41%|███████████                | 164/400 [1:20:24<1:49:53, 27.94s/it]2022-01-10 11:01:46,932 iteration 2789 : loss : 0.032690, loss_ce: 0.018982
2022-01-10 11:01:48,487 iteration 2790 : loss : 0.031610, loss_ce: 0.008331
2022-01-10 11:01:50,079 iteration 2791 : loss : 0.025180, loss_ce: 0.006913
2022-01-10 11:01:51,717 iteration 2792 : loss : 0.038289, loss_ce: 0.009357
2022-01-10 11:01:53,312 iteration 2793 : loss : 0.036342, loss_ce: 0.017266
2022-01-10 11:01:54,903 iteration 2794 : loss : 0.025964, loss_ce: 0.010498
2022-01-10 11:01:56,561 iteration 2795 : loss : 0.053762, loss_ce: 0.030560
2022-01-10 11:01:58,248 iteration 2796 : loss : 0.043067, loss_ce: 0.014809
2022-01-10 11:01:59,873 iteration 2797 : loss : 0.026481, loss_ce: 0.010843
2022-01-10 11:02:01,552 iteration 2798 : loss : 0.025874, loss_ce: 0.009044
2022-01-10 11:02:03,202 iteration 2799 : loss : 0.035783, loss_ce: 0.012908
2022-01-10 11:02:04,814 iteration 2800 : loss : 0.020829, loss_ce: 0.009386
2022-01-10 11:02:06,387 iteration 2801 : loss : 0.025331, loss_ce: 0.010742
2022-01-10 11:02:07,897 iteration 2802 : loss : 0.024336, loss_ce: 0.010149
2022-01-10 11:02:09,578 iteration 2803 : loss : 0.053221, loss_ce: 0.017077
2022-01-10 11:02:11,172 iteration 2804 : loss : 0.042744, loss_ce: 0.017727
2022-01-10 11:02:11,173 Training Data Eval:
2022-01-10 11:02:19,112   Average segmentation loss on training set: 0.0241
2022-01-10 11:02:19,112 Validation Data Eval:
2022-01-10 11:02:21,859   Average segmentation loss on validation set: 0.0693
2022-01-10 11:02:23,409 iteration 2805 : loss : 0.038521, loss_ce: 0.012497
 41%|███████████▏               | 165/400 [1:21:02<2:01:23, 30.99s/it]2022-01-10 11:02:25,075 iteration 2806 : loss : 0.043040, loss_ce: 0.014735
2022-01-10 11:02:26,812 iteration 2807 : loss : 0.031599, loss_ce: 0.012222
2022-01-10 11:02:28,300 iteration 2808 : loss : 0.026857, loss_ce: 0.014541
2022-01-10 11:02:29,874 iteration 2809 : loss : 0.037602, loss_ce: 0.015747
2022-01-10 11:02:31,499 iteration 2810 : loss : 0.040239, loss_ce: 0.013378
2022-01-10 11:02:33,044 iteration 2811 : loss : 0.030600, loss_ce: 0.011088
2022-01-10 11:02:34,632 iteration 2812 : loss : 0.031120, loss_ce: 0.011890
2022-01-10 11:02:36,243 iteration 2813 : loss : 0.026899, loss_ce: 0.008175
2022-01-10 11:02:37,837 iteration 2814 : loss : 0.036233, loss_ce: 0.016065
2022-01-10 11:02:39,376 iteration 2815 : loss : 0.022434, loss_ce: 0.009307
2022-01-10 11:02:40,914 iteration 2816 : loss : 0.028507, loss_ce: 0.012886
2022-01-10 11:02:42,444 iteration 2817 : loss : 0.027680, loss_ce: 0.009337
2022-01-10 11:02:44,081 iteration 2818 : loss : 0.038541, loss_ce: 0.011309
2022-01-10 11:02:45,707 iteration 2819 : loss : 0.032809, loss_ce: 0.015228
2022-01-10 11:02:47,411 iteration 2820 : loss : 0.037781, loss_ce: 0.015211
2022-01-10 11:02:48,997 iteration 2821 : loss : 0.030166, loss_ce: 0.014095
2022-01-10 11:02:50,540 iteration 2822 : loss : 0.029071, loss_ce: 0.009950
 42%|███████████▏               | 166/400 [1:21:29<1:56:20, 29.83s/it]2022-01-10 11:02:52,253 iteration 2823 : loss : 0.048152, loss_ce: 0.020640
2022-01-10 11:02:53,780 iteration 2824 : loss : 0.025093, loss_ce: 0.008996
2022-01-10 11:02:55,380 iteration 2825 : loss : 0.030781, loss_ce: 0.013261
2022-01-10 11:02:56,908 iteration 2826 : loss : 0.019730, loss_ce: 0.008121
2022-01-10 11:02:58,574 iteration 2827 : loss : 0.042477, loss_ce: 0.021155
2022-01-10 11:03:00,172 iteration 2828 : loss : 0.030245, loss_ce: 0.013889
2022-01-10 11:03:01,867 iteration 2829 : loss : 0.038043, loss_ce: 0.015649
2022-01-10 11:03:03,382 iteration 2830 : loss : 0.025063, loss_ce: 0.011500
2022-01-10 11:03:04,969 iteration 2831 : loss : 0.032071, loss_ce: 0.014755
2022-01-10 11:03:06,581 iteration 2832 : loss : 0.043016, loss_ce: 0.012484
2022-01-10 11:03:08,214 iteration 2833 : loss : 0.043308, loss_ce: 0.015504
2022-01-10 11:03:09,791 iteration 2834 : loss : 0.084152, loss_ce: 0.028507
2022-01-10 11:03:11,411 iteration 2835 : loss : 0.070753, loss_ce: 0.016149
2022-01-10 11:03:13,045 iteration 2836 : loss : 0.035485, loss_ce: 0.015571
2022-01-10 11:03:14,631 iteration 2837 : loss : 0.035139, loss_ce: 0.009801
2022-01-10 11:03:16,155 iteration 2838 : loss : 0.019966, loss_ce: 0.006719
2022-01-10 11:03:17,777 iteration 2839 : loss : 0.033136, loss_ce: 0.012071
 42%|███████████▎               | 167/400 [1:21:56<1:52:49, 29.06s/it]2022-01-10 11:03:19,394 iteration 2840 : loss : 0.034202, loss_ce: 0.017900
2022-01-10 11:03:20,986 iteration 2841 : loss : 0.029058, loss_ce: 0.012919
2022-01-10 11:03:22,569 iteration 2842 : loss : 0.051467, loss_ce: 0.018708
2022-01-10 11:03:24,068 iteration 2843 : loss : 0.022070, loss_ce: 0.009351
2022-01-10 11:03:25,545 iteration 2844 : loss : 0.019743, loss_ce: 0.007165
2022-01-10 11:03:27,085 iteration 2845 : loss : 0.037672, loss_ce: 0.013581
2022-01-10 11:03:28,776 iteration 2846 : loss : 0.043306, loss_ce: 0.012695
2022-01-10 11:03:30,394 iteration 2847 : loss : 0.041032, loss_ce: 0.013574
2022-01-10 11:03:31,988 iteration 2848 : loss : 0.040231, loss_ce: 0.011620
2022-01-10 11:03:33,604 iteration 2849 : loss : 0.036473, loss_ce: 0.014614
2022-01-10 11:03:35,186 iteration 2850 : loss : 0.025507, loss_ce: 0.011813
2022-01-10 11:03:36,851 iteration 2851 : loss : 0.029719, loss_ce: 0.012787
2022-01-10 11:03:38,398 iteration 2852 : loss : 0.026999, loss_ce: 0.008287
2022-01-10 11:03:39,995 iteration 2853 : loss : 0.040334, loss_ce: 0.012704
2022-01-10 11:03:41,521 iteration 2854 : loss : 0.027115, loss_ce: 0.008228
2022-01-10 11:03:43,063 iteration 2855 : loss : 0.028083, loss_ce: 0.012265
2022-01-10 11:03:44,543 iteration 2856 : loss : 0.022822, loss_ce: 0.011231
 42%|███████████▎               | 168/400 [1:22:23<1:49:42, 28.37s/it]2022-01-10 11:03:46,196 iteration 2857 : loss : 0.044232, loss_ce: 0.021405
2022-01-10 11:03:47,881 iteration 2858 : loss : 0.043765, loss_ce: 0.015514
2022-01-10 11:03:49,502 iteration 2859 : loss : 0.027663, loss_ce: 0.009541
2022-01-10 11:03:51,124 iteration 2860 : loss : 0.029776, loss_ce: 0.010814
2022-01-10 11:03:52,712 iteration 2861 : loss : 0.030912, loss_ce: 0.008574
2022-01-10 11:03:54,326 iteration 2862 : loss : 0.052260, loss_ce: 0.012466
2022-01-10 11:03:55,869 iteration 2863 : loss : 0.024012, loss_ce: 0.010019
2022-01-10 11:03:57,426 iteration 2864 : loss : 0.029016, loss_ce: 0.009259
2022-01-10 11:03:58,983 iteration 2865 : loss : 0.027772, loss_ce: 0.010783
2022-01-10 11:04:00,535 iteration 2866 : loss : 0.020657, loss_ce: 0.007230
2022-01-10 11:04:02,215 iteration 2867 : loss : 0.044233, loss_ce: 0.015624
2022-01-10 11:04:03,830 iteration 2868 : loss : 0.043135, loss_ce: 0.015993
2022-01-10 11:04:05,468 iteration 2869 : loss : 0.028704, loss_ce: 0.010832
2022-01-10 11:04:07,134 iteration 2870 : loss : 0.045576, loss_ce: 0.013827
2022-01-10 11:04:08,649 iteration 2871 : loss : 0.038696, loss_ce: 0.024067
2022-01-10 11:04:10,193 iteration 2872 : loss : 0.034888, loss_ce: 0.017382
2022-01-10 11:04:11,762 iteration 2873 : loss : 0.025631, loss_ce: 0.011280
 42%|███████████▍               | 169/400 [1:22:50<1:47:54, 28.03s/it]2022-01-10 11:04:13,395 iteration 2874 : loss : 0.028274, loss_ce: 0.010247
2022-01-10 11:04:15,054 iteration 2875 : loss : 0.032600, loss_ce: 0.013218
2022-01-10 11:04:16,593 iteration 2876 : loss : 0.043895, loss_ce: 0.015779
2022-01-10 11:04:18,100 iteration 2877 : loss : 0.031403, loss_ce: 0.010462
2022-01-10 11:04:19,656 iteration 2878 : loss : 0.030299, loss_ce: 0.009045
2022-01-10 11:04:21,147 iteration 2879 : loss : 0.033294, loss_ce: 0.015871
2022-01-10 11:04:22,757 iteration 2880 : loss : 0.032395, loss_ce: 0.017033
2022-01-10 11:04:24,250 iteration 2881 : loss : 0.024017, loss_ce: 0.011521
2022-01-10 11:04:25,777 iteration 2882 : loss : 0.055944, loss_ce: 0.014911
2022-01-10 11:04:27,379 iteration 2883 : loss : 0.037687, loss_ce: 0.011357
2022-01-10 11:04:28,995 iteration 2884 : loss : 0.034401, loss_ce: 0.014654
2022-01-10 11:04:30,584 iteration 2885 : loss : 0.031760, loss_ce: 0.011071
2022-01-10 11:04:32,120 iteration 2886 : loss : 0.026042, loss_ce: 0.008355
2022-01-10 11:04:33,653 iteration 2887 : loss : 0.031526, loss_ce: 0.010340
2022-01-10 11:04:35,264 iteration 2888 : loss : 0.024954, loss_ce: 0.009922
2022-01-10 11:04:36,840 iteration 2889 : loss : 0.028927, loss_ce: 0.012362
2022-01-10 11:04:36,840 Training Data Eval:
2022-01-10 11:04:44,781   Average segmentation loss on training set: 0.0218
2022-01-10 11:04:44,781 Validation Data Eval:
2022-01-10 11:04:47,524   Average segmentation loss on validation set: 0.0861
2022-01-10 11:04:49,073 iteration 2890 : loss : 0.043982, loss_ce: 0.019510
 42%|███████████▍               | 170/400 [1:23:28<1:58:06, 30.81s/it]2022-01-10 11:04:50,696 iteration 2891 : loss : 0.031565, loss_ce: 0.013978
2022-01-10 11:04:52,291 iteration 2892 : loss : 0.030360, loss_ce: 0.011763
2022-01-10 11:04:53,779 iteration 2893 : loss : 0.037998, loss_ce: 0.023146
2022-01-10 11:04:55,314 iteration 2894 : loss : 0.024948, loss_ce: 0.010098
2022-01-10 11:04:56,840 iteration 2895 : loss : 0.040480, loss_ce: 0.017159
2022-01-10 11:04:58,391 iteration 2896 : loss : 0.053587, loss_ce: 0.023695
2022-01-10 11:05:00,078 iteration 2897 : loss : 0.064823, loss_ce: 0.019612
2022-01-10 11:05:01,676 iteration 2898 : loss : 0.033751, loss_ce: 0.011593
2022-01-10 11:05:03,250 iteration 2899 : loss : 0.032382, loss_ce: 0.014553
2022-01-10 11:05:04,954 iteration 2900 : loss : 0.066715, loss_ce: 0.028328
2022-01-10 11:05:06,581 iteration 2901 : loss : 0.042145, loss_ce: 0.015147
2022-01-10 11:05:08,128 iteration 2902 : loss : 0.049057, loss_ce: 0.027617
2022-01-10 11:05:09,718 iteration 2903 : loss : 0.034937, loss_ce: 0.010731
2022-01-10 11:05:11,277 iteration 2904 : loss : 0.028823, loss_ce: 0.011991
2022-01-10 11:05:12,887 iteration 2905 : loss : 0.026272, loss_ce: 0.010708
2022-01-10 11:05:14,427 iteration 2906 : loss : 0.025238, loss_ce: 0.010205
2022-01-10 11:05:16,003 iteration 2907 : loss : 0.032798, loss_ce: 0.013145
 43%|███████████▌               | 171/400 [1:23:55<1:53:08, 29.64s/it]2022-01-10 11:05:17,653 iteration 2908 : loss : 0.041148, loss_ce: 0.018560
2022-01-10 11:05:19,281 iteration 2909 : loss : 0.035735, loss_ce: 0.013038
2022-01-10 11:05:20,907 iteration 2910 : loss : 0.025588, loss_ce: 0.011226
2022-01-10 11:05:22,552 iteration 2911 : loss : 0.030922, loss_ce: 0.011820
2022-01-10 11:05:24,091 iteration 2912 : loss : 0.025109, loss_ce: 0.009290
2022-01-10 11:05:25,688 iteration 2913 : loss : 0.038117, loss_ce: 0.011973
2022-01-10 11:05:27,298 iteration 2914 : loss : 0.043989, loss_ce: 0.016844
2022-01-10 11:05:28,883 iteration 2915 : loss : 0.039884, loss_ce: 0.023780
2022-01-10 11:05:30,454 iteration 2916 : loss : 0.032690, loss_ce: 0.017310
2022-01-10 11:05:32,043 iteration 2917 : loss : 0.030216, loss_ce: 0.011852
2022-01-10 11:05:33,540 iteration 2918 : loss : 0.022669, loss_ce: 0.010253
2022-01-10 11:05:35,133 iteration 2919 : loss : 0.056589, loss_ce: 0.018891
2022-01-10 11:05:36,692 iteration 2920 : loss : 0.023338, loss_ce: 0.009635
2022-01-10 11:05:38,257 iteration 2921 : loss : 0.024611, loss_ce: 0.011250
2022-01-10 11:05:39,814 iteration 2922 : loss : 0.036791, loss_ce: 0.010812
2022-01-10 11:05:41,473 iteration 2923 : loss : 0.041847, loss_ce: 0.012373
2022-01-10 11:05:43,099 iteration 2924 : loss : 0.025499, loss_ce: 0.011420
 43%|███████████▌               | 172/400 [1:24:22<1:49:45, 28.88s/it]2022-01-10 11:05:44,760 iteration 2925 : loss : 0.042187, loss_ce: 0.018878
2022-01-10 11:05:46,268 iteration 2926 : loss : 0.021635, loss_ce: 0.007381
2022-01-10 11:05:47,853 iteration 2927 : loss : 0.024609, loss_ce: 0.008716
2022-01-10 11:05:49,459 iteration 2928 : loss : 0.035541, loss_ce: 0.015390
2022-01-10 11:05:50,995 iteration 2929 : loss : 0.044178, loss_ce: 0.013642
2022-01-10 11:05:52,514 iteration 2930 : loss : 0.024105, loss_ce: 0.010656
2022-01-10 11:05:54,076 iteration 2931 : loss : 0.118283, loss_ce: 0.020037
2022-01-10 11:05:55,727 iteration 2932 : loss : 0.031008, loss_ce: 0.012535
2022-01-10 11:05:57,343 iteration 2933 : loss : 0.045151, loss_ce: 0.019361
2022-01-10 11:05:58,992 iteration 2934 : loss : 0.034678, loss_ce: 0.014884
2022-01-10 11:06:00,539 iteration 2935 : loss : 0.028989, loss_ce: 0.008198
2022-01-10 11:06:02,112 iteration 2936 : loss : 0.047038, loss_ce: 0.019302
2022-01-10 11:06:03,719 iteration 2937 : loss : 0.055790, loss_ce: 0.029984
2022-01-10 11:06:05,321 iteration 2938 : loss : 0.055767, loss_ce: 0.022275
2022-01-10 11:06:06,807 iteration 2939 : loss : 0.029966, loss_ce: 0.009470
2022-01-10 11:06:08,367 iteration 2940 : loss : 0.032276, loss_ce: 0.012202
2022-01-10 11:06:09,931 iteration 2941 : loss : 0.050513, loss_ce: 0.021375
 43%|███████████▋               | 173/400 [1:24:48<1:46:56, 28.26s/it]2022-01-10 11:06:11,663 iteration 2942 : loss : 0.036731, loss_ce: 0.017600
2022-01-10 11:06:13,295 iteration 2943 : loss : 0.035604, loss_ce: 0.017723
2022-01-10 11:06:14,899 iteration 2944 : loss : 0.076220, loss_ce: 0.025613
2022-01-10 11:06:16,480 iteration 2945 : loss : 0.035193, loss_ce: 0.016117
2022-01-10 11:06:18,132 iteration 2946 : loss : 0.039328, loss_ce: 0.017001
2022-01-10 11:06:19,677 iteration 2947 : loss : 0.038307, loss_ce: 0.016898
2022-01-10 11:06:21,282 iteration 2948 : loss : 0.035171, loss_ce: 0.014570
2022-01-10 11:06:22,824 iteration 2949 : loss : 0.033270, loss_ce: 0.012437
2022-01-10 11:06:24,312 iteration 2950 : loss : 0.025484, loss_ce: 0.009846
2022-01-10 11:06:25,881 iteration 2951 : loss : 0.030959, loss_ce: 0.009864
2022-01-10 11:06:27,355 iteration 2952 : loss : 0.074489, loss_ce: 0.014837
2022-01-10 11:06:28,978 iteration 2953 : loss : 0.044164, loss_ce: 0.015914
2022-01-10 11:06:30,557 iteration 2954 : loss : 0.033403, loss_ce: 0.016193
2022-01-10 11:06:32,145 iteration 2955 : loss : 0.057118, loss_ce: 0.019776
2022-01-10 11:06:33,659 iteration 2956 : loss : 0.051031, loss_ce: 0.031079
2022-01-10 11:06:35,173 iteration 2957 : loss : 0.031468, loss_ce: 0.011942
2022-01-10 11:06:36,743 iteration 2958 : loss : 0.044601, loss_ce: 0.023203
 44%|███████████▋               | 174/400 [1:25:15<1:44:49, 27.83s/it]2022-01-10 11:06:38,421 iteration 2959 : loss : 0.028008, loss_ce: 0.012771
2022-01-10 11:06:39,997 iteration 2960 : loss : 0.031864, loss_ce: 0.014116
2022-01-10 11:06:41,613 iteration 2961 : loss : 0.031565, loss_ce: 0.014610
2022-01-10 11:06:43,235 iteration 2962 : loss : 0.030361, loss_ce: 0.011425
2022-01-10 11:06:44,783 iteration 2963 : loss : 0.034843, loss_ce: 0.014926
2022-01-10 11:06:46,319 iteration 2964 : loss : 0.038932, loss_ce: 0.015601
2022-01-10 11:06:47,819 iteration 2965 : loss : 0.054563, loss_ce: 0.019873
2022-01-10 11:06:49,305 iteration 2966 : loss : 0.032129, loss_ce: 0.013430
2022-01-10 11:06:50,898 iteration 2967 : loss : 0.044186, loss_ce: 0.022395
2022-01-10 11:06:52,564 iteration 2968 : loss : 0.049805, loss_ce: 0.013697
2022-01-10 11:06:54,196 iteration 2969 : loss : 0.022935, loss_ce: 0.009263
2022-01-10 11:06:55,755 iteration 2970 : loss : 0.043355, loss_ce: 0.016996
2022-01-10 11:06:57,263 iteration 2971 : loss : 0.046396, loss_ce: 0.020411
2022-01-10 11:06:58,882 iteration 2972 : loss : 0.062847, loss_ce: 0.024441
2022-01-10 11:07:00,413 iteration 2973 : loss : 0.045023, loss_ce: 0.018744
2022-01-10 11:07:02,058 iteration 2974 : loss : 0.026473, loss_ce: 0.009464
2022-01-10 11:07:02,058 Training Data Eval:
2022-01-10 11:07:10,013   Average segmentation loss on training set: 0.0528
2022-01-10 11:07:10,014 Validation Data Eval:
2022-01-10 11:07:12,758   Average segmentation loss on validation set: 0.2060
2022-01-10 11:07:14,353 iteration 2975 : loss : 0.032677, loss_ce: 0.010110
 44%|███████████▊               | 175/400 [1:25:53<1:55:22, 30.77s/it]2022-01-10 11:07:16,077 iteration 2976 : loss : 0.057229, loss_ce: 0.032225
2022-01-10 11:07:17,580 iteration 2977 : loss : 0.027268, loss_ce: 0.010777
2022-01-10 11:07:19,064 iteration 2978 : loss : 0.027212, loss_ce: 0.010145
2022-01-10 11:07:20,673 iteration 2979 : loss : 0.028848, loss_ce: 0.009418
2022-01-10 11:07:22,381 iteration 2980 : loss : 0.053965, loss_ce: 0.020004
2022-01-10 11:07:23,978 iteration 2981 : loss : 0.048345, loss_ce: 0.016216
2022-01-10 11:07:25,652 iteration 2982 : loss : 0.029555, loss_ce: 0.013613
2022-01-10 11:07:27,221 iteration 2983 : loss : 0.035307, loss_ce: 0.014770
2022-01-10 11:07:28,859 iteration 2984 : loss : 0.025705, loss_ce: 0.011094
2022-01-10 11:07:30,470 iteration 2985 : loss : 0.046803, loss_ce: 0.017615
2022-01-10 11:07:32,110 iteration 2986 : loss : 0.030831, loss_ce: 0.011725
2022-01-10 11:07:33,634 iteration 2987 : loss : 0.039257, loss_ce: 0.018690
2022-01-10 11:07:35,192 iteration 2988 : loss : 0.025354, loss_ce: 0.009524
2022-01-10 11:07:36,794 iteration 2989 : loss : 0.029430, loss_ce: 0.010462
2022-01-10 11:07:38,405 iteration 2990 : loss : 0.055359, loss_ce: 0.022403
2022-01-10 11:07:40,080 iteration 2991 : loss : 0.034721, loss_ce: 0.013334
2022-01-10 11:07:41,821 iteration 2992 : loss : 0.045902, loss_ce: 0.020216
 44%|███████████▉               | 176/400 [1:26:20<1:51:09, 29.77s/it]2022-01-10 11:07:43,401 iteration 2993 : loss : 0.025607, loss_ce: 0.009069
2022-01-10 11:07:44,974 iteration 2994 : loss : 0.028885, loss_ce: 0.010200
2022-01-10 11:07:46,658 iteration 2995 : loss : 0.038911, loss_ce: 0.017706
2022-01-10 11:07:48,247 iteration 2996 : loss : 0.032004, loss_ce: 0.012934
2022-01-10 11:07:49,823 iteration 2997 : loss : 0.024689, loss_ce: 0.009273
2022-01-10 11:07:51,391 iteration 2998 : loss : 0.024155, loss_ce: 0.007591
2022-01-10 11:07:53,022 iteration 2999 : loss : 0.044632, loss_ce: 0.019191
2022-01-10 11:07:54,598 iteration 3000 : loss : 0.043401, loss_ce: 0.015478
2022-01-10 11:07:56,226 iteration 3001 : loss : 0.035285, loss_ce: 0.013907
2022-01-10 11:07:57,724 iteration 3002 : loss : 0.034431, loss_ce: 0.013930
2022-01-10 11:07:59,368 iteration 3003 : loss : 0.025560, loss_ce: 0.009806
2022-01-10 11:08:00,863 iteration 3004 : loss : 0.023729, loss_ce: 0.010074
2022-01-10 11:08:02,470 iteration 3005 : loss : 0.040787, loss_ce: 0.018161
2022-01-10 11:08:04,089 iteration 3006 : loss : 0.050988, loss_ce: 0.020927
2022-01-10 11:08:05,648 iteration 3007 : loss : 0.028211, loss_ce: 0.012628
2022-01-10 11:08:07,357 iteration 3008 : loss : 0.042625, loss_ce: 0.015652
2022-01-10 11:08:08,932 iteration 3009 : loss : 0.042165, loss_ce: 0.019879
 44%|███████████▉               | 177/400 [1:26:47<1:47:41, 28.97s/it]2022-01-10 11:08:10,561 iteration 3010 : loss : 0.035104, loss_ce: 0.015602
2022-01-10 11:08:12,133 iteration 3011 : loss : 0.048195, loss_ce: 0.016440
2022-01-10 11:08:13,677 iteration 3012 : loss : 0.028909, loss_ce: 0.010943
2022-01-10 11:08:15,212 iteration 3013 : loss : 0.022289, loss_ce: 0.010128
2022-01-10 11:08:16,867 iteration 3014 : loss : 0.033629, loss_ce: 0.012631
2022-01-10 11:08:18,466 iteration 3015 : loss : 0.026196, loss_ce: 0.010517
2022-01-10 11:08:20,043 iteration 3016 : loss : 0.024557, loss_ce: 0.009474
2022-01-10 11:08:21,611 iteration 3017 : loss : 0.032325, loss_ce: 0.009360
2022-01-10 11:08:23,141 iteration 3018 : loss : 0.023685, loss_ce: 0.006580
2022-01-10 11:08:24,701 iteration 3019 : loss : 0.045923, loss_ce: 0.020174
2022-01-10 11:08:26,154 iteration 3020 : loss : 0.028148, loss_ce: 0.015122
2022-01-10 11:08:27,635 iteration 3021 : loss : 0.023452, loss_ce: 0.007896
2022-01-10 11:08:29,184 iteration 3022 : loss : 0.030424, loss_ce: 0.009379
2022-01-10 11:08:30,743 iteration 3023 : loss : 0.029532, loss_ce: 0.015626
2022-01-10 11:08:32,323 iteration 3024 : loss : 0.035570, loss_ce: 0.012091
2022-01-10 11:08:33,798 iteration 3025 : loss : 0.023403, loss_ce: 0.009801
2022-01-10 11:08:35,408 iteration 3026 : loss : 0.048719, loss_ce: 0.021596
 44%|████████████               | 178/400 [1:27:14<1:44:26, 28.23s/it]2022-01-10 11:08:37,049 iteration 3027 : loss : 0.029895, loss_ce: 0.014390
2022-01-10 11:08:38,617 iteration 3028 : loss : 0.040258, loss_ce: 0.017295
2022-01-10 11:08:40,205 iteration 3029 : loss : 0.032018, loss_ce: 0.014550
2022-01-10 11:08:41,887 iteration 3030 : loss : 0.028666, loss_ce: 0.010987
2022-01-10 11:08:43,370 iteration 3031 : loss : 0.025118, loss_ce: 0.010349
2022-01-10 11:08:44,934 iteration 3032 : loss : 0.033875, loss_ce: 0.013723
2022-01-10 11:08:46,534 iteration 3033 : loss : 0.033018, loss_ce: 0.014242
2022-01-10 11:08:48,093 iteration 3034 : loss : 0.029013, loss_ce: 0.011357
2022-01-10 11:08:49,754 iteration 3035 : loss : 0.050510, loss_ce: 0.016690
2022-01-10 11:08:51,354 iteration 3036 : loss : 0.028839, loss_ce: 0.011091
2022-01-10 11:08:52,989 iteration 3037 : loss : 0.029364, loss_ce: 0.011652
2022-01-10 11:08:54,490 iteration 3038 : loss : 0.021093, loss_ce: 0.005267
2022-01-10 11:08:56,237 iteration 3039 : loss : 0.031877, loss_ce: 0.011500
2022-01-10 11:08:57,763 iteration 3040 : loss : 0.031687, loss_ce: 0.013946
2022-01-10 11:08:59,303 iteration 3041 : loss : 0.032894, loss_ce: 0.014316
2022-01-10 11:09:00,836 iteration 3042 : loss : 0.044030, loss_ce: 0.021133
2022-01-10 11:09:02,529 iteration 3043 : loss : 0.027010, loss_ce: 0.010480
 45%|████████████               | 179/400 [1:27:41<1:42:44, 27.89s/it]2022-01-10 11:09:04,210 iteration 3044 : loss : 0.040823, loss_ce: 0.015132
2022-01-10 11:09:05,775 iteration 3045 : loss : 0.024260, loss_ce: 0.009700
2022-01-10 11:09:07,321 iteration 3046 : loss : 0.037282, loss_ce: 0.015045
2022-01-10 11:09:09,026 iteration 3047 : loss : 0.028040, loss_ce: 0.012218
2022-01-10 11:09:10,679 iteration 3048 : loss : 0.033480, loss_ce: 0.020828
2022-01-10 11:09:12,231 iteration 3049 : loss : 0.026695, loss_ce: 0.010315
2022-01-10 11:09:13,918 iteration 3050 : loss : 0.031178, loss_ce: 0.015095
2022-01-10 11:09:15,548 iteration 3051 : loss : 0.034230, loss_ce: 0.009130
2022-01-10 11:09:17,226 iteration 3052 : loss : 0.046697, loss_ce: 0.025051
2022-01-10 11:09:18,844 iteration 3053 : loss : 0.025190, loss_ce: 0.010195
2022-01-10 11:09:20,455 iteration 3054 : loss : 0.041931, loss_ce: 0.013850
2022-01-10 11:09:22,073 iteration 3055 : loss : 0.032221, loss_ce: 0.014688
2022-01-10 11:09:23,658 iteration 3056 : loss : 0.029052, loss_ce: 0.010000
2022-01-10 11:09:25,193 iteration 3057 : loss : 0.017639, loss_ce: 0.007048
2022-01-10 11:09:26,858 iteration 3058 : loss : 0.040457, loss_ce: 0.012479
2022-01-10 11:09:28,528 iteration 3059 : loss : 0.037999, loss_ce: 0.015818
2022-01-10 11:09:28,528 Training Data Eval:
2022-01-10 11:09:36,489   Average segmentation loss on training set: 0.0209
2022-01-10 11:09:36,489 Validation Data Eval:
2022-01-10 11:09:39,243   Average segmentation loss on validation set: 0.0726
2022-01-10 11:09:40,816 iteration 3060 : loss : 0.028811, loss_ce: 0.012472
 45%|████████████▏              | 180/400 [1:28:19<1:53:42, 31.01s/it]2022-01-10 11:09:42,425 iteration 3061 : loss : 0.029857, loss_ce: 0.010931
2022-01-10 11:09:44,037 iteration 3062 : loss : 0.025223, loss_ce: 0.008134
2022-01-10 11:09:45,595 iteration 3063 : loss : 0.033243, loss_ce: 0.011191
2022-01-10 11:09:47,237 iteration 3064 : loss : 0.041423, loss_ce: 0.024281
2022-01-10 11:09:48,817 iteration 3065 : loss : 0.034382, loss_ce: 0.010330
2022-01-10 11:09:50,384 iteration 3066 : loss : 0.021519, loss_ce: 0.005298
2022-01-10 11:09:51,982 iteration 3067 : loss : 0.038682, loss_ce: 0.015887
2022-01-10 11:09:53,638 iteration 3068 : loss : 0.022733, loss_ce: 0.008463
2022-01-10 11:09:55,289 iteration 3069 : loss : 0.043409, loss_ce: 0.010566
2022-01-10 11:09:56,808 iteration 3070 : loss : 0.025050, loss_ce: 0.009197
2022-01-10 11:09:58,472 iteration 3071 : loss : 0.029859, loss_ce: 0.012540
2022-01-10 11:10:00,083 iteration 3072 : loss : 0.031196, loss_ce: 0.012564
2022-01-10 11:10:01,627 iteration 3073 : loss : 0.033041, loss_ce: 0.013234
2022-01-10 11:10:03,324 iteration 3074 : loss : 0.044520, loss_ce: 0.020031
2022-01-10 11:10:04,957 iteration 3075 : loss : 0.027701, loss_ce: 0.010657
2022-01-10 11:10:06,522 iteration 3076 : loss : 0.040410, loss_ce: 0.021318
2022-01-10 11:10:08,190 iteration 3077 : loss : 0.068630, loss_ce: 0.031200
 45%|████████████▏              | 181/400 [1:28:47<1:49:12, 29.92s/it]2022-01-10 11:10:09,717 iteration 3078 : loss : 0.017282, loss_ce: 0.007415
2022-01-10 11:10:11,349 iteration 3079 : loss : 0.030994, loss_ce: 0.009775
2022-01-10 11:10:12,934 iteration 3080 : loss : 0.028877, loss_ce: 0.011569
2022-01-10 11:10:14,496 iteration 3081 : loss : 0.033085, loss_ce: 0.010774
2022-01-10 11:10:16,083 iteration 3082 : loss : 0.019676, loss_ce: 0.008019
2022-01-10 11:10:17,671 iteration 3083 : loss : 0.031552, loss_ce: 0.013977
2022-01-10 11:10:19,242 iteration 3084 : loss : 0.034383, loss_ce: 0.015394
2022-01-10 11:10:20,827 iteration 3085 : loss : 0.085674, loss_ce: 0.013351
2022-01-10 11:10:22,366 iteration 3086 : loss : 0.020223, loss_ce: 0.007700
2022-01-10 11:10:23,893 iteration 3087 : loss : 0.027217, loss_ce: 0.008723
2022-01-10 11:10:25,409 iteration 3088 : loss : 0.023990, loss_ce: 0.007955
2022-01-10 11:10:26,931 iteration 3089 : loss : 0.026695, loss_ce: 0.010000
2022-01-10 11:10:28,522 iteration 3090 : loss : 0.030799, loss_ce: 0.009148
2022-01-10 11:10:30,057 iteration 3091 : loss : 0.038734, loss_ce: 0.013318
2022-01-10 11:10:31,548 iteration 3092 : loss : 0.034792, loss_ce: 0.015270
2022-01-10 11:10:33,111 iteration 3093 : loss : 0.020442, loss_ce: 0.007193
2022-01-10 11:10:34,647 iteration 3094 : loss : 0.043855, loss_ce: 0.021728
 46%|████████████▎              | 182/400 [1:29:13<1:44:55, 28.88s/it]2022-01-10 11:10:36,340 iteration 3095 : loss : 0.024721, loss_ce: 0.007926
2022-01-10 11:10:37,889 iteration 3096 : loss : 0.022746, loss_ce: 0.008552
2022-01-10 11:10:39,447 iteration 3097 : loss : 0.029510, loss_ce: 0.010657
2022-01-10 11:10:40,967 iteration 3098 : loss : 0.025279, loss_ce: 0.008627
2022-01-10 11:10:42,603 iteration 3099 : loss : 0.035904, loss_ce: 0.010193
2022-01-10 11:10:44,131 iteration 3100 : loss : 0.024360, loss_ce: 0.008394
2022-01-10 11:10:45,700 iteration 3101 : loss : 0.026664, loss_ce: 0.013252
2022-01-10 11:10:47,321 iteration 3102 : loss : 0.025465, loss_ce: 0.009094
2022-01-10 11:10:48,881 iteration 3103 : loss : 0.028430, loss_ce: 0.008470
2022-01-10 11:10:50,454 iteration 3104 : loss : 0.025925, loss_ce: 0.009217
2022-01-10 11:10:52,003 iteration 3105 : loss : 0.028088, loss_ce: 0.009278
2022-01-10 11:10:53,731 iteration 3106 : loss : 0.028889, loss_ce: 0.012879
2022-01-10 11:10:55,309 iteration 3107 : loss : 0.030948, loss_ce: 0.012439
2022-01-10 11:10:56,807 iteration 3108 : loss : 0.025996, loss_ce: 0.008821
2022-01-10 11:10:58,447 iteration 3109 : loss : 0.041193, loss_ce: 0.022882
2022-01-10 11:11:00,090 iteration 3110 : loss : 0.036930, loss_ce: 0.017923
2022-01-10 11:11:01,626 iteration 3111 : loss : 0.028964, loss_ce: 0.009208
 46%|████████████▎              | 183/400 [1:29:40<1:42:23, 28.31s/it]2022-01-10 11:11:03,215 iteration 3112 : loss : 0.018144, loss_ce: 0.007211
2022-01-10 11:11:04,790 iteration 3113 : loss : 0.043260, loss_ce: 0.016852
2022-01-10 11:11:06,499 iteration 3114 : loss : 0.045196, loss_ce: 0.014329
2022-01-10 11:11:08,146 iteration 3115 : loss : 0.027977, loss_ce: 0.012274
2022-01-10 11:11:09,753 iteration 3116 : loss : 0.045317, loss_ce: 0.023398
2022-01-10 11:11:11,324 iteration 3117 : loss : 0.029170, loss_ce: 0.014381
2022-01-10 11:11:12,968 iteration 3118 : loss : 0.030103, loss_ce: 0.012100
2022-01-10 11:11:14,590 iteration 3119 : loss : 0.029381, loss_ce: 0.011163
2022-01-10 11:11:16,186 iteration 3120 : loss : 0.023251, loss_ce: 0.007121
2022-01-10 11:11:17,800 iteration 3121 : loss : 0.031629, loss_ce: 0.009976
2022-01-10 11:11:19,457 iteration 3122 : loss : 0.030777, loss_ce: 0.008902
2022-01-10 11:11:21,048 iteration 3123 : loss : 0.027836, loss_ce: 0.012391
2022-01-10 11:11:22,657 iteration 3124 : loss : 0.037020, loss_ce: 0.015730
2022-01-10 11:11:24,265 iteration 3125 : loss : 0.030383, loss_ce: 0.014979
2022-01-10 11:11:25,785 iteration 3126 : loss : 0.028512, loss_ce: 0.010310
2022-01-10 11:11:27,381 iteration 3127 : loss : 0.025195, loss_ce: 0.011267
2022-01-10 11:11:28,885 iteration 3128 : loss : 0.018887, loss_ce: 0.006981
 46%|████████████▍              | 184/400 [1:30:07<1:40:46, 27.99s/it]2022-01-10 11:11:30,560 iteration 3129 : loss : 0.030047, loss_ce: 0.012016
2022-01-10 11:11:32,164 iteration 3130 : loss : 0.027958, loss_ce: 0.009604
2022-01-10 11:11:33,817 iteration 3131 : loss : 0.039777, loss_ce: 0.015343
2022-01-10 11:11:35,385 iteration 3132 : loss : 0.020889, loss_ce: 0.008795
2022-01-10 11:11:36,894 iteration 3133 : loss : 0.025914, loss_ce: 0.010030
2022-01-10 11:11:38,475 iteration 3134 : loss : 0.021527, loss_ce: 0.007393
2022-01-10 11:11:40,054 iteration 3135 : loss : 0.031893, loss_ce: 0.012576
2022-01-10 11:11:41,694 iteration 3136 : loss : 0.041193, loss_ce: 0.015171
2022-01-10 11:11:43,292 iteration 3137 : loss : 0.027491, loss_ce: 0.012450
2022-01-10 11:11:44,914 iteration 3138 : loss : 0.035831, loss_ce: 0.018232
2022-01-10 11:11:46,578 iteration 3139 : loss : 0.067618, loss_ce: 0.025963
2022-01-10 11:11:48,210 iteration 3140 : loss : 0.038889, loss_ce: 0.015222
2022-01-10 11:11:49,841 iteration 3141 : loss : 0.025973, loss_ce: 0.009752
2022-01-10 11:11:51,421 iteration 3142 : loss : 0.024844, loss_ce: 0.009073
2022-01-10 11:11:53,075 iteration 3143 : loss : 0.025671, loss_ce: 0.010363
2022-01-10 11:11:54,742 iteration 3144 : loss : 0.033807, loss_ce: 0.015757
2022-01-10 11:11:54,742 Training Data Eval:
2022-01-10 11:12:02,695   Average segmentation loss on training set: 0.0185
2022-01-10 11:12:02,695 Validation Data Eval:
2022-01-10 11:12:05,444   Average segmentation loss on validation set: 0.0786
2022-01-10 11:12:06,907 iteration 3145 : loss : 0.026949, loss_ce: 0.010115
 46%|████████████▍              | 185/400 [1:30:45<1:51:05, 31.00s/it]2022-01-10 11:12:08,450 iteration 3146 : loss : 0.022962, loss_ce: 0.008580
2022-01-10 11:12:10,001 iteration 3147 : loss : 0.025763, loss_ce: 0.010962
2022-01-10 11:12:11,562 iteration 3148 : loss : 0.025790, loss_ce: 0.009438
2022-01-10 11:12:13,109 iteration 3149 : loss : 0.029883, loss_ce: 0.012196
2022-01-10 11:12:14,667 iteration 3150 : loss : 0.022445, loss_ce: 0.010155
2022-01-10 11:12:16,206 iteration 3151 : loss : 0.036169, loss_ce: 0.009042
2022-01-10 11:12:17,762 iteration 3152 : loss : 0.022580, loss_ce: 0.008976
2022-01-10 11:12:19,357 iteration 3153 : loss : 0.029118, loss_ce: 0.010565
2022-01-10 11:12:20,900 iteration 3154 : loss : 0.024402, loss_ce: 0.011744
2022-01-10 11:12:22,454 iteration 3155 : loss : 0.023783, loss_ce: 0.008153
2022-01-10 11:12:24,097 iteration 3156 : loss : 0.055608, loss_ce: 0.020272
2022-01-10 11:12:25,715 iteration 3157 : loss : 0.031662, loss_ce: 0.012803
2022-01-10 11:12:27,281 iteration 3158 : loss : 0.041686, loss_ce: 0.020156
2022-01-10 11:12:28,847 iteration 3159 : loss : 0.020813, loss_ce: 0.007833
2022-01-10 11:12:30,421 iteration 3160 : loss : 0.027597, loss_ce: 0.010750
2022-01-10 11:12:31,941 iteration 3161 : loss : 0.026938, loss_ce: 0.009781
2022-01-10 11:12:33,590 iteration 3162 : loss : 0.035008, loss_ce: 0.014042
 46%|████████████▌              | 186/400 [1:31:12<1:45:57, 29.71s/it]2022-01-10 11:12:35,294 iteration 3163 : loss : 0.036264, loss_ce: 0.016041
2022-01-10 11:12:36,912 iteration 3164 : loss : 0.018324, loss_ce: 0.006616
2022-01-10 11:12:38,478 iteration 3165 : loss : 0.020945, loss_ce: 0.007964
2022-01-10 11:12:40,123 iteration 3166 : loss : 0.024050, loss_ce: 0.008738
2022-01-10 11:12:41,700 iteration 3167 : loss : 0.021829, loss_ce: 0.008927
2022-01-10 11:12:43,413 iteration 3168 : loss : 0.032648, loss_ce: 0.014676
2022-01-10 11:12:45,091 iteration 3169 : loss : 0.028521, loss_ce: 0.013695
2022-01-10 11:12:46,653 iteration 3170 : loss : 0.022236, loss_ce: 0.009486
2022-01-10 11:12:48,160 iteration 3171 : loss : 0.031289, loss_ce: 0.017784
2022-01-10 11:12:49,763 iteration 3172 : loss : 0.024701, loss_ce: 0.008741
2022-01-10 11:12:51,403 iteration 3173 : loss : 0.024403, loss_ce: 0.010019
2022-01-10 11:12:52,987 iteration 3174 : loss : 0.035002, loss_ce: 0.014182
2022-01-10 11:12:54,670 iteration 3175 : loss : 0.050344, loss_ce: 0.012979
2022-01-10 11:12:56,244 iteration 3176 : loss : 0.031562, loss_ce: 0.010928
2022-01-10 11:12:57,880 iteration 3177 : loss : 0.044842, loss_ce: 0.016262
2022-01-10 11:12:59,405 iteration 3178 : loss : 0.029705, loss_ce: 0.009592
2022-01-10 11:13:00,955 iteration 3179 : loss : 0.028965, loss_ce: 0.011109
 47%|████████████▌              | 187/400 [1:31:40<1:42:58, 29.01s/it]2022-01-10 11:13:02,648 iteration 3180 : loss : 0.051414, loss_ce: 0.024436
2022-01-10 11:13:04,198 iteration 3181 : loss : 0.025710, loss_ce: 0.010754
2022-01-10 11:13:05,796 iteration 3182 : loss : 0.032485, loss_ce: 0.010686
2022-01-10 11:13:07,346 iteration 3183 : loss : 0.027742, loss_ce: 0.007955
2022-01-10 11:13:08,940 iteration 3184 : loss : 0.027746, loss_ce: 0.010094
2022-01-10 11:13:10,572 iteration 3185 : loss : 0.030077, loss_ce: 0.012299
2022-01-10 11:13:12,073 iteration 3186 : loss : 0.023881, loss_ce: 0.011865
2022-01-10 11:13:13,668 iteration 3187 : loss : 0.019655, loss_ce: 0.007764
2022-01-10 11:13:15,236 iteration 3188 : loss : 0.027580, loss_ce: 0.012604
2022-01-10 11:13:16,837 iteration 3189 : loss : 0.028195, loss_ce: 0.012818
2022-01-10 11:13:18,414 iteration 3190 : loss : 0.032187, loss_ce: 0.012139
2022-01-10 11:13:20,119 iteration 3191 : loss : 0.051613, loss_ce: 0.015136
2022-01-10 11:13:21,666 iteration 3192 : loss : 0.035730, loss_ce: 0.011468
2022-01-10 11:13:23,298 iteration 3193 : loss : 0.028876, loss_ce: 0.013541
2022-01-10 11:13:24,925 iteration 3194 : loss : 0.073280, loss_ce: 0.022059
2022-01-10 11:13:26,523 iteration 3195 : loss : 0.027257, loss_ce: 0.012746
2022-01-10 11:13:28,110 iteration 3196 : loss : 0.037344, loss_ce: 0.014733
 47%|████████████▋              | 188/400 [1:32:07<1:40:31, 28.45s/it]2022-01-10 11:13:29,683 iteration 3197 : loss : 0.030306, loss_ce: 0.011717
2022-01-10 11:13:31,252 iteration 3198 : loss : 0.038911, loss_ce: 0.016817
2022-01-10 11:13:32,842 iteration 3199 : loss : 0.034103, loss_ce: 0.016943
2022-01-10 11:13:34,532 iteration 3200 : loss : 0.028918, loss_ce: 0.013080
2022-01-10 11:13:36,150 iteration 3201 : loss : 0.053770, loss_ce: 0.011982
2022-01-10 11:13:37,744 iteration 3202 : loss : 0.039429, loss_ce: 0.014674
2022-01-10 11:13:39,442 iteration 3203 : loss : 0.039410, loss_ce: 0.016475
2022-01-10 11:13:41,023 iteration 3204 : loss : 0.049275, loss_ce: 0.016217
2022-01-10 11:13:42,670 iteration 3205 : loss : 0.037752, loss_ce: 0.011580
2022-01-10 11:13:44,267 iteration 3206 : loss : 0.034473, loss_ce: 0.011765
2022-01-10 11:13:45,951 iteration 3207 : loss : 0.052247, loss_ce: 0.021725
2022-01-10 11:13:47,486 iteration 3208 : loss : 0.033779, loss_ce: 0.011878
2022-01-10 11:13:49,024 iteration 3209 : loss : 0.028410, loss_ce: 0.015978
2022-01-10 11:13:50,508 iteration 3210 : loss : 0.021322, loss_ce: 0.010068
2022-01-10 11:13:52,076 iteration 3211 : loss : 0.030972, loss_ce: 0.009193
2022-01-10 11:13:53,624 iteration 3212 : loss : 0.025468, loss_ce: 0.010054
2022-01-10 11:13:55,210 iteration 3213 : loss : 0.036348, loss_ce: 0.011465
 47%|████████████▊              | 189/400 [1:32:34<1:38:37, 28.05s/it]2022-01-10 11:13:56,864 iteration 3214 : loss : 0.037996, loss_ce: 0.014577
2022-01-10 11:13:58,543 iteration 3215 : loss : 0.034768, loss_ce: 0.012893
2022-01-10 11:14:00,187 iteration 3216 : loss : 0.036420, loss_ce: 0.012671
2022-01-10 11:14:01,833 iteration 3217 : loss : 0.093643, loss_ce: 0.019777
2022-01-10 11:14:03,439 iteration 3218 : loss : 0.049358, loss_ce: 0.018358
2022-01-10 11:14:05,003 iteration 3219 : loss : 0.026899, loss_ce: 0.010525
2022-01-10 11:14:06,629 iteration 3220 : loss : 0.033510, loss_ce: 0.012072
2022-01-10 11:14:08,177 iteration 3221 : loss : 0.026425, loss_ce: 0.011185
2022-01-10 11:14:09,770 iteration 3222 : loss : 0.051095, loss_ce: 0.031329
2022-01-10 11:14:11,392 iteration 3223 : loss : 0.026340, loss_ce: 0.007982
2022-01-10 11:14:13,033 iteration 3224 : loss : 0.024113, loss_ce: 0.008613
2022-01-10 11:14:14,600 iteration 3225 : loss : 0.040995, loss_ce: 0.014785
2022-01-10 11:14:16,277 iteration 3226 : loss : 0.029458, loss_ce: 0.009020
2022-01-10 11:14:17,889 iteration 3227 : loss : 0.026873, loss_ce: 0.010398
2022-01-10 11:14:19,421 iteration 3228 : loss : 0.023467, loss_ce: 0.010165
2022-01-10 11:14:21,082 iteration 3229 : loss : 0.036891, loss_ce: 0.017560
2022-01-10 11:14:21,082 Training Data Eval:
2022-01-10 11:14:29,044   Average segmentation loss on training set: 0.0217
2022-01-10 11:14:29,045 Validation Data Eval:
2022-01-10 11:14:31,791   Average segmentation loss on validation set: 0.0924
2022-01-10 11:14:33,466 iteration 3230 : loss : 0.041268, loss_ce: 0.015273
 48%|████████████▊              | 190/400 [1:33:12<1:48:52, 31.11s/it]2022-01-10 11:14:35,079 iteration 3231 : loss : 0.030584, loss_ce: 0.011992
2022-01-10 11:14:36,667 iteration 3232 : loss : 0.025102, loss_ce: 0.011728
2022-01-10 11:14:38,232 iteration 3233 : loss : 0.025068, loss_ce: 0.008495
2022-01-10 11:14:39,763 iteration 3234 : loss : 0.040444, loss_ce: 0.014701
2022-01-10 11:14:41,305 iteration 3235 : loss : 0.022385, loss_ce: 0.006196
2022-01-10 11:14:42,917 iteration 3236 : loss : 0.026338, loss_ce: 0.012128
2022-01-10 11:14:44,546 iteration 3237 : loss : 0.033332, loss_ce: 0.014721
2022-01-10 11:14:46,148 iteration 3238 : loss : 0.038015, loss_ce: 0.012773
2022-01-10 11:14:47,730 iteration 3239 : loss : 0.025279, loss_ce: 0.010434
2022-01-10 11:14:49,335 iteration 3240 : loss : 0.026143, loss_ce: 0.009920
2022-01-10 11:14:50,969 iteration 3241 : loss : 0.029761, loss_ce: 0.011570
2022-01-10 11:14:52,655 iteration 3242 : loss : 0.034407, loss_ce: 0.015878
2022-01-10 11:14:54,206 iteration 3243 : loss : 0.028731, loss_ce: 0.013886
2022-01-10 11:14:55,784 iteration 3244 : loss : 0.052670, loss_ce: 0.018342
2022-01-10 11:14:57,342 iteration 3245 : loss : 0.027603, loss_ce: 0.011975
2022-01-10 11:14:58,995 iteration 3246 : loss : 0.032679, loss_ce: 0.015310
2022-01-10 11:15:00,606 iteration 3247 : loss : 0.023035, loss_ce: 0.008277
 48%|████████████▉              | 191/400 [1:33:39<1:44:13, 29.92s/it]2022-01-10 11:15:02,426 iteration 3248 : loss : 0.031091, loss_ce: 0.015504
2022-01-10 11:15:03,937 iteration 3249 : loss : 0.021076, loss_ce: 0.006251
2022-01-10 11:15:05,546 iteration 3250 : loss : 0.031064, loss_ce: 0.011389
2022-01-10 11:15:07,138 iteration 3251 : loss : 0.027500, loss_ce: 0.006895
2022-01-10 11:15:08,672 iteration 3252 : loss : 0.025482, loss_ce: 0.009637
2022-01-10 11:15:10,282 iteration 3253 : loss : 0.033080, loss_ce: 0.016682
2022-01-10 11:15:11,876 iteration 3254 : loss : 0.028545, loss_ce: 0.009031
2022-01-10 11:15:13,516 iteration 3255 : loss : 0.025680, loss_ce: 0.009613
2022-01-10 11:15:15,081 iteration 3256 : loss : 0.022554, loss_ce: 0.012068
2022-01-10 11:15:16,646 iteration 3257 : loss : 0.033496, loss_ce: 0.010709
2022-01-10 11:15:18,306 iteration 3258 : loss : 0.026489, loss_ce: 0.010971
2022-01-10 11:15:19,890 iteration 3259 : loss : 0.028980, loss_ce: 0.010782
2022-01-10 11:15:21,443 iteration 3260 : loss : 0.023303, loss_ce: 0.009456
2022-01-10 11:15:23,119 iteration 3261 : loss : 0.037767, loss_ce: 0.012991
2022-01-10 11:15:24,690 iteration 3262 : loss : 0.033184, loss_ce: 0.015516
2022-01-10 11:15:26,264 iteration 3263 : loss : 0.033487, loss_ce: 0.012017
2022-01-10 11:15:27,821 iteration 3264 : loss : 0.028808, loss_ce: 0.010158
 48%|████████████▉              | 192/400 [1:34:06<1:40:54, 29.11s/it]2022-01-10 11:15:29,348 iteration 3265 : loss : 0.016633, loss_ce: 0.006252
2022-01-10 11:15:30,960 iteration 3266 : loss : 0.022854, loss_ce: 0.009559
2022-01-10 11:15:32,492 iteration 3267 : loss : 0.021972, loss_ce: 0.009194
2022-01-10 11:15:34,183 iteration 3268 : loss : 0.034882, loss_ce: 0.014059
2022-01-10 11:15:35,711 iteration 3269 : loss : 0.028065, loss_ce: 0.011447
2022-01-10 11:15:37,236 iteration 3270 : loss : 0.026681, loss_ce: 0.009427
2022-01-10 11:15:38,883 iteration 3271 : loss : 0.027637, loss_ce: 0.010808
2022-01-10 11:15:40,438 iteration 3272 : loss : 0.024082, loss_ce: 0.008715
2022-01-10 11:15:41,982 iteration 3273 : loss : 0.047418, loss_ce: 0.017662
2022-01-10 11:15:43,596 iteration 3274 : loss : 0.024081, loss_ce: 0.008261
2022-01-10 11:15:45,128 iteration 3275 : loss : 0.021187, loss_ce: 0.009893
2022-01-10 11:15:46,706 iteration 3276 : loss : 0.024361, loss_ce: 0.008066
2022-01-10 11:15:48,344 iteration 3277 : loss : 0.026508, loss_ce: 0.010503
2022-01-10 11:15:49,922 iteration 3278 : loss : 0.026388, loss_ce: 0.009821
2022-01-10 11:15:51,560 iteration 3279 : loss : 0.054016, loss_ce: 0.032310
2022-01-10 11:15:53,165 iteration 3280 : loss : 0.043077, loss_ce: 0.017455
2022-01-10 11:15:54,755 iteration 3281 : loss : 0.027504, loss_ce: 0.007814
 48%|█████████████              | 193/400 [1:34:33<1:38:10, 28.46s/it]2022-01-10 11:15:56,365 iteration 3282 : loss : 0.021998, loss_ce: 0.009728
2022-01-10 11:15:58,056 iteration 3283 : loss : 0.032945, loss_ce: 0.013745
2022-01-10 11:15:59,610 iteration 3284 : loss : 0.023877, loss_ce: 0.011171
2022-01-10 11:16:01,098 iteration 3285 : loss : 0.019722, loss_ce: 0.007066
2022-01-10 11:16:02,739 iteration 3286 : loss : 0.073559, loss_ce: 0.022866
2022-01-10 11:16:04,363 iteration 3287 : loss : 0.033394, loss_ce: 0.015546
2022-01-10 11:16:05,946 iteration 3288 : loss : 0.034911, loss_ce: 0.011440
2022-01-10 11:16:07,538 iteration 3289 : loss : 0.034613, loss_ce: 0.009363
2022-01-10 11:16:09,067 iteration 3290 : loss : 0.025437, loss_ce: 0.007948
2022-01-10 11:16:10,703 iteration 3291 : loss : 0.046465, loss_ce: 0.015353
2022-01-10 11:16:12,257 iteration 3292 : loss : 0.040132, loss_ce: 0.020238
2022-01-10 11:16:13,770 iteration 3293 : loss : 0.037175, loss_ce: 0.017544
2022-01-10 11:16:15,298 iteration 3294 : loss : 0.022598, loss_ce: 0.009900
2022-01-10 11:16:16,868 iteration 3295 : loss : 0.029246, loss_ce: 0.013361
2022-01-10 11:16:18,469 iteration 3296 : loss : 0.026689, loss_ce: 0.010226
2022-01-10 11:16:20,022 iteration 3297 : loss : 0.043214, loss_ce: 0.018829
2022-01-10 11:16:21,527 iteration 3298 : loss : 0.026223, loss_ce: 0.011386
 48%|█████████████              | 194/400 [1:35:00<1:35:57, 27.95s/it]2022-01-10 11:16:23,122 iteration 3299 : loss : 0.032161, loss_ce: 0.013315
2022-01-10 11:16:24,669 iteration 3300 : loss : 0.039166, loss_ce: 0.018382
2022-01-10 11:16:26,285 iteration 3301 : loss : 0.038700, loss_ce: 0.013944
2022-01-10 11:16:27,797 iteration 3302 : loss : 0.029576, loss_ce: 0.011103
2022-01-10 11:16:29,421 iteration 3303 : loss : 0.030989, loss_ce: 0.011127
2022-01-10 11:16:31,022 iteration 3304 : loss : 0.029679, loss_ce: 0.015885
2022-01-10 11:16:32,642 iteration 3305 : loss : 0.039641, loss_ce: 0.016424
2022-01-10 11:16:34,186 iteration 3306 : loss : 0.025331, loss_ce: 0.009905
2022-01-10 11:16:35,845 iteration 3307 : loss : 0.032429, loss_ce: 0.013757
2022-01-10 11:16:37,503 iteration 3308 : loss : 0.041398, loss_ce: 0.014642
2022-01-10 11:16:39,100 iteration 3309 : loss : 0.021193, loss_ce: 0.009022
2022-01-10 11:16:40,752 iteration 3310 : loss : 0.034202, loss_ce: 0.010725
2022-01-10 11:16:42,328 iteration 3311 : loss : 0.037690, loss_ce: 0.013459
2022-01-10 11:16:43,849 iteration 3312 : loss : 0.023411, loss_ce: 0.011855
2022-01-10 11:16:45,474 iteration 3313 : loss : 0.051495, loss_ce: 0.021857
2022-01-10 11:16:46,939 iteration 3314 : loss : 0.022731, loss_ce: 0.009716
2022-01-10 11:16:46,939 Training Data Eval:
2022-01-10 11:16:54,888   Average segmentation loss on training set: 0.0193
2022-01-10 11:16:54,888 Validation Data Eval:
2022-01-10 11:16:57,628   Average segmentation loss on validation set: 0.0759
2022-01-10 11:16:59,180 iteration 3315 : loss : 0.025903, loss_ce: 0.009622
 49%|█████████████▏             | 195/400 [1:35:38<1:45:26, 30.86s/it]2022-01-10 11:17:00,804 iteration 3316 : loss : 0.024249, loss_ce: 0.006143
2022-01-10 11:17:02,405 iteration 3317 : loss : 0.033744, loss_ce: 0.013214
2022-01-10 11:17:03,999 iteration 3318 : loss : 0.031378, loss_ce: 0.011852
2022-01-10 11:17:05,523 iteration 3319 : loss : 0.020834, loss_ce: 0.008154
2022-01-10 11:17:07,216 iteration 3320 : loss : 0.037919, loss_ce: 0.015391
2022-01-10 11:17:08,727 iteration 3321 : loss : 0.020440, loss_ce: 0.010531
2022-01-10 11:17:10,209 iteration 3322 : loss : 0.021476, loss_ce: 0.007373
2022-01-10 11:17:11,802 iteration 3323 : loss : 0.038780, loss_ce: 0.017373
2022-01-10 11:17:13,381 iteration 3324 : loss : 0.029830, loss_ce: 0.010141
2022-01-10 11:17:15,020 iteration 3325 : loss : 0.033403, loss_ce: 0.012269
2022-01-10 11:17:16,611 iteration 3326 : loss : 0.025125, loss_ce: 0.009820
2022-01-10 11:17:18,340 iteration 3327 : loss : 0.043079, loss_ce: 0.014015
2022-01-10 11:17:19,890 iteration 3328 : loss : 0.026965, loss_ce: 0.013805
2022-01-10 11:17:21,436 iteration 3329 : loss : 0.062583, loss_ce: 0.023811
2022-01-10 11:17:23,027 iteration 3330 : loss : 0.035345, loss_ce: 0.018677
2022-01-10 11:17:24,515 iteration 3331 : loss : 0.025379, loss_ce: 0.010251
2022-01-10 11:17:25,979 iteration 3332 : loss : 0.026169, loss_ce: 0.008671
 49%|█████████████▏             | 196/400 [1:36:05<1:40:47, 29.64s/it]2022-01-10 11:17:27,605 iteration 3333 : loss : 0.041172, loss_ce: 0.018776
2022-01-10 11:17:29,194 iteration 3334 : loss : 0.023965, loss_ce: 0.010139
2022-01-10 11:17:30,833 iteration 3335 : loss : 0.047381, loss_ce: 0.018975
2022-01-10 11:17:32,360 iteration 3336 : loss : 0.024277, loss_ce: 0.009265
2022-01-10 11:17:33,882 iteration 3337 : loss : 0.033872, loss_ce: 0.013613
2022-01-10 11:17:35,488 iteration 3338 : loss : 0.023769, loss_ce: 0.008952
2022-01-10 11:17:37,111 iteration 3339 : loss : 0.055353, loss_ce: 0.020256
2022-01-10 11:17:38,700 iteration 3340 : loss : 0.056532, loss_ce: 0.031560
2022-01-10 11:17:40,245 iteration 3341 : loss : 0.024519, loss_ce: 0.011093
2022-01-10 11:17:41,778 iteration 3342 : loss : 0.037451, loss_ce: 0.017827
2022-01-10 11:17:43,338 iteration 3343 : loss : 0.032162, loss_ce: 0.013116
2022-01-10 11:17:44,977 iteration 3344 : loss : 0.030542, loss_ce: 0.010136
2022-01-10 11:17:46,617 iteration 3345 : loss : 0.031735, loss_ce: 0.017131
2022-01-10 11:17:48,204 iteration 3346 : loss : 0.033050, loss_ce: 0.012058
2022-01-10 11:17:49,760 iteration 3347 : loss : 0.022772, loss_ce: 0.008125
2022-01-10 11:17:51,419 iteration 3348 : loss : 0.026990, loss_ce: 0.011204
2022-01-10 11:17:52,949 iteration 3349 : loss : 0.043252, loss_ce: 0.013550
 49%|█████████████▎             | 197/400 [1:36:32<1:37:34, 28.84s/it]2022-01-10 11:17:54,532 iteration 3350 : loss : 0.026239, loss_ce: 0.011589
2022-01-10 11:17:56,142 iteration 3351 : loss : 0.021050, loss_ce: 0.008046
2022-01-10 11:17:57,681 iteration 3352 : loss : 0.033486, loss_ce: 0.011391
2022-01-10 11:17:59,257 iteration 3353 : loss : 0.038689, loss_ce: 0.013617
2022-01-10 11:18:00,797 iteration 3354 : loss : 0.028381, loss_ce: 0.008010
2022-01-10 11:18:02,387 iteration 3355 : loss : 0.023651, loss_ce: 0.010095
2022-01-10 11:18:03,911 iteration 3356 : loss : 0.036508, loss_ce: 0.016664
2022-01-10 11:18:05,441 iteration 3357 : loss : 0.021408, loss_ce: 0.008494
2022-01-10 11:18:07,007 iteration 3358 : loss : 0.021709, loss_ce: 0.007215
2022-01-10 11:18:08,557 iteration 3359 : loss : 0.024864, loss_ce: 0.011942
2022-01-10 11:18:10,182 iteration 3360 : loss : 0.026175, loss_ce: 0.010842
2022-01-10 11:18:11,772 iteration 3361 : loss : 0.036220, loss_ce: 0.014548
2022-01-10 11:18:13,318 iteration 3362 : loss : 0.027827, loss_ce: 0.009374
2022-01-10 11:18:14,796 iteration 3363 : loss : 0.032390, loss_ce: 0.011237
2022-01-10 11:18:16,426 iteration 3364 : loss : 0.041219, loss_ce: 0.016597
2022-01-10 11:18:17,997 iteration 3365 : loss : 0.021228, loss_ce: 0.008641
2022-01-10 11:18:19,561 iteration 3366 : loss : 0.023786, loss_ce: 0.009646
 50%|█████████████▎             | 198/400 [1:36:58<1:34:50, 28.17s/it]2022-01-10 11:18:21,317 iteration 3367 : loss : 0.028985, loss_ce: 0.011326
2022-01-10 11:18:22,893 iteration 3368 : loss : 0.034183, loss_ce: 0.015375
2022-01-10 11:18:24,484 iteration 3369 : loss : 0.026136, loss_ce: 0.011087
2022-01-10 11:18:26,008 iteration 3370 : loss : 0.029982, loss_ce: 0.007997
2022-01-10 11:18:27,573 iteration 3371 : loss : 0.019630, loss_ce: 0.007526
2022-01-10 11:18:29,203 iteration 3372 : loss : 0.035040, loss_ce: 0.010365
2022-01-10 11:18:30,790 iteration 3373 : loss : 0.026109, loss_ce: 0.010466
2022-01-10 11:18:32,449 iteration 3374 : loss : 0.022239, loss_ce: 0.009380
2022-01-10 11:18:34,136 iteration 3375 : loss : 0.040998, loss_ce: 0.016157
2022-01-10 11:18:35,637 iteration 3376 : loss : 0.026043, loss_ce: 0.009158
2022-01-10 11:18:37,262 iteration 3377 : loss : 0.031305, loss_ce: 0.011106
2022-01-10 11:18:38,804 iteration 3378 : loss : 0.034936, loss_ce: 0.010096
2022-01-10 11:18:40,420 iteration 3379 : loss : 0.023161, loss_ce: 0.008647
2022-01-10 11:18:41,946 iteration 3380 : loss : 0.023598, loss_ce: 0.008239
2022-01-10 11:18:43,534 iteration 3381 : loss : 0.034257, loss_ce: 0.018962
2022-01-10 11:18:45,153 iteration 3382 : loss : 0.025248, loss_ce: 0.007849
2022-01-10 11:18:46,727 iteration 3383 : loss : 0.026634, loss_ce: 0.011819
 50%|█████████████▍             | 199/400 [1:37:25<1:33:21, 27.87s/it]2022-01-10 11:18:48,370 iteration 3384 : loss : 0.028619, loss_ce: 0.011636
2022-01-10 11:18:49,911 iteration 3385 : loss : 0.023844, loss_ce: 0.009785
2022-01-10 11:18:51,447 iteration 3386 : loss : 0.028714, loss_ce: 0.009975
2022-01-10 11:18:52,967 iteration 3387 : loss : 0.034709, loss_ce: 0.011173
2022-01-10 11:18:54,589 iteration 3388 : loss : 0.037119, loss_ce: 0.015619
2022-01-10 11:18:56,112 iteration 3389 : loss : 0.022415, loss_ce: 0.007762
2022-01-10 11:18:57,696 iteration 3390 : loss : 0.030981, loss_ce: 0.012095
2022-01-10 11:18:59,371 iteration 3391 : loss : 0.029645, loss_ce: 0.016029
2022-01-10 11:19:00,991 iteration 3392 : loss : 0.039032, loss_ce: 0.017453
2022-01-10 11:19:02,508 iteration 3393 : loss : 0.028199, loss_ce: 0.012801
2022-01-10 11:19:04,064 iteration 3394 : loss : 0.029393, loss_ce: 0.009816
2022-01-10 11:19:05,644 iteration 3395 : loss : 0.023839, loss_ce: 0.009575
2022-01-10 11:19:07,227 iteration 3396 : loss : 0.033877, loss_ce: 0.012220
2022-01-10 11:19:08,736 iteration 3397 : loss : 0.027397, loss_ce: 0.010419
2022-01-10 11:19:10,452 iteration 3398 : loss : 0.040155, loss_ce: 0.014388
2022-01-10 11:19:12,046 iteration 3399 : loss : 0.036116, loss_ce: 0.019325
2022-01-10 11:19:12,047 Training Data Eval:
2022-01-10 11:19:20,003   Average segmentation loss on training set: 0.0177
2022-01-10 11:19:20,004 Validation Data Eval:
2022-01-10 11:19:22,747   Average segmentation loss on validation set: 0.0813
2022-01-10 11:19:24,335 iteration 3400 : loss : 0.030149, loss_ce: 0.007761
 50%|█████████████▌             | 200/400 [1:38:03<1:42:37, 30.79s/it]2022-01-10 11:19:26,014 iteration 3401 : loss : 0.051481, loss_ce: 0.019078
2022-01-10 11:19:27,626 iteration 3402 : loss : 0.023392, loss_ce: 0.009007
2022-01-10 11:19:29,308 iteration 3403 : loss : 0.038948, loss_ce: 0.014390
2022-01-10 11:19:30,850 iteration 3404 : loss : 0.025267, loss_ce: 0.008149
2022-01-10 11:19:32,416 iteration 3405 : loss : 0.033124, loss_ce: 0.008390
2022-01-10 11:19:33,916 iteration 3406 : loss : 0.041392, loss_ce: 0.019622
2022-01-10 11:19:35,451 iteration 3407 : loss : 0.021185, loss_ce: 0.008151
2022-01-10 11:19:37,029 iteration 3408 : loss : 0.019863, loss_ce: 0.007190
2022-01-10 11:19:38,594 iteration 3409 : loss : 0.033614, loss_ce: 0.007201
2022-01-10 11:19:40,160 iteration 3410 : loss : 0.024738, loss_ce: 0.009905
2022-01-10 11:19:41,781 iteration 3411 : loss : 0.030481, loss_ce: 0.009687
2022-01-10 11:19:43,428 iteration 3412 : loss : 0.028425, loss_ce: 0.012849
2022-01-10 11:19:45,103 iteration 3413 : loss : 0.032711, loss_ce: 0.014633
2022-01-10 11:19:46,746 iteration 3414 : loss : 0.045420, loss_ce: 0.011027
2022-01-10 11:19:48,221 iteration 3415 : loss : 0.021638, loss_ce: 0.008517
2022-01-10 11:19:49,824 iteration 3416 : loss : 0.032765, loss_ce: 0.014150
2022-01-10 11:19:51,378 iteration 3417 : loss : 0.025330, loss_ce: 0.010191
 50%|█████████████▌             | 201/400 [1:38:30<1:38:24, 29.67s/it]2022-01-10 11:19:53,114 iteration 3418 : loss : 0.035863, loss_ce: 0.010286
2022-01-10 11:19:54,714 iteration 3419 : loss : 0.050192, loss_ce: 0.034565
2022-01-10 11:19:56,237 iteration 3420 : loss : 0.024652, loss_ce: 0.013675
2022-01-10 11:19:57,824 iteration 3421 : loss : 0.044796, loss_ce: 0.013211
2022-01-10 11:19:59,394 iteration 3422 : loss : 0.031899, loss_ce: 0.011194
2022-01-10 11:20:01,030 iteration 3423 : loss : 0.035244, loss_ce: 0.014440
2022-01-10 11:20:02,550 iteration 3424 : loss : 0.025382, loss_ce: 0.008982
2022-01-10 11:20:04,141 iteration 3425 : loss : 0.036823, loss_ce: 0.016983
2022-01-10 11:20:05,694 iteration 3426 : loss : 0.024876, loss_ce: 0.007129
2022-01-10 11:20:07,384 iteration 3427 : loss : 0.033066, loss_ce: 0.012038
2022-01-10 11:20:08,982 iteration 3428 : loss : 0.023496, loss_ce: 0.009129
2022-01-10 11:20:10,589 iteration 3429 : loss : 0.027912, loss_ce: 0.013078
2022-01-10 11:20:12,189 iteration 3430 : loss : 0.019838, loss_ce: 0.006711
2022-01-10 11:20:13,830 iteration 3431 : loss : 0.038232, loss_ce: 0.014265
2022-01-10 11:20:15,373 iteration 3432 : loss : 0.028464, loss_ce: 0.008560
2022-01-10 11:20:16,962 iteration 3433 : loss : 0.021805, loss_ce: 0.006621
2022-01-10 11:20:18,508 iteration 3434 : loss : 0.021692, loss_ce: 0.006024
 50%|█████████████▋             | 202/400 [1:38:57<1:35:23, 28.91s/it]2022-01-10 11:20:20,083 iteration 3435 : loss : 0.019068, loss_ce: 0.009105
2022-01-10 11:20:21,642 iteration 3436 : loss : 0.028181, loss_ce: 0.011782
2022-01-10 11:20:23,164 iteration 3437 : loss : 0.022724, loss_ce: 0.007510
2022-01-10 11:20:24,731 iteration 3438 : loss : 0.028123, loss_ce: 0.013842
2022-01-10 11:20:26,386 iteration 3439 : loss : 0.023778, loss_ce: 0.009060
2022-01-10 11:20:28,044 iteration 3440 : loss : 0.044209, loss_ce: 0.012876
2022-01-10 11:20:29,510 iteration 3441 : loss : 0.020072, loss_ce: 0.010139
2022-01-10 11:20:31,048 iteration 3442 : loss : 0.016136, loss_ce: 0.004948
2022-01-10 11:20:32,666 iteration 3443 : loss : 0.020212, loss_ce: 0.007672
2022-01-10 11:20:34,219 iteration 3444 : loss : 0.025370, loss_ce: 0.009064
2022-01-10 11:20:35,841 iteration 3445 : loss : 0.020571, loss_ce: 0.006922
2022-01-10 11:20:37,419 iteration 3446 : loss : 0.031619, loss_ce: 0.011010
2022-01-10 11:20:39,005 iteration 3447 : loss : 0.023721, loss_ce: 0.012582
2022-01-10 11:20:40,504 iteration 3448 : loss : 0.021112, loss_ce: 0.007989
2022-01-10 11:20:42,098 iteration 3449 : loss : 0.033045, loss_ce: 0.008569
2022-01-10 11:20:43,727 iteration 3450 : loss : 0.033280, loss_ce: 0.013505
2022-01-10 11:20:45,253 iteration 3451 : loss : 0.017207, loss_ce: 0.007162
 51%|█████████████▋             | 203/400 [1:39:24<1:32:46, 28.26s/it]2022-01-10 11:20:46,882 iteration 3452 : loss : 0.019381, loss_ce: 0.006588
2022-01-10 11:20:48,514 iteration 3453 : loss : 0.028696, loss_ce: 0.009053
2022-01-10 11:20:50,156 iteration 3454 : loss : 0.048667, loss_ce: 0.019401
2022-01-10 11:20:51,762 iteration 3455 : loss : 0.025983, loss_ce: 0.011214
2022-01-10 11:20:53,366 iteration 3456 : loss : 0.032850, loss_ce: 0.010437
2022-01-10 11:20:54,983 iteration 3457 : loss : 0.038603, loss_ce: 0.019393
2022-01-10 11:20:56,605 iteration 3458 : loss : 0.020286, loss_ce: 0.009840
2022-01-10 11:20:58,274 iteration 3459 : loss : 0.032495, loss_ce: 0.014598
2022-01-10 11:20:59,917 iteration 3460 : loss : 0.027153, loss_ce: 0.009943
2022-01-10 11:21:01,499 iteration 3461 : loss : 0.027359, loss_ce: 0.011063
2022-01-10 11:21:03,033 iteration 3462 : loss : 0.022919, loss_ce: 0.008661
2022-01-10 11:21:04,526 iteration 3463 : loss : 0.020875, loss_ce: 0.007276
2022-01-10 11:21:06,087 iteration 3464 : loss : 0.029495, loss_ce: 0.011147
2022-01-10 11:21:07,615 iteration 3465 : loss : 0.029192, loss_ce: 0.010809
2022-01-10 11:21:09,215 iteration 3466 : loss : 0.020929, loss_ce: 0.008537
2022-01-10 11:21:10,890 iteration 3467 : loss : 0.027405, loss_ce: 0.012261
2022-01-10 11:21:12,602 iteration 3468 : loss : 0.027496, loss_ce: 0.009921
 51%|█████████████▊             | 204/400 [1:39:51<1:31:25, 27.99s/it]2022-01-10 11:21:14,170 iteration 3469 : loss : 0.021326, loss_ce: 0.009514
2022-01-10 11:21:15,901 iteration 3470 : loss : 0.026332, loss_ce: 0.010537
2022-01-10 11:21:17,541 iteration 3471 : loss : 0.023955, loss_ce: 0.008916
2022-01-10 11:21:19,187 iteration 3472 : loss : 0.030209, loss_ce: 0.012166
2022-01-10 11:21:20,840 iteration 3473 : loss : 0.023209, loss_ce: 0.011584
2022-01-10 11:21:22,396 iteration 3474 : loss : 0.023432, loss_ce: 0.009149
2022-01-10 11:21:24,069 iteration 3475 : loss : 0.036208, loss_ce: 0.015188
2022-01-10 11:21:25,642 iteration 3476 : loss : 0.020589, loss_ce: 0.007078
2022-01-10 11:21:27,277 iteration 3477 : loss : 0.035811, loss_ce: 0.011348
2022-01-10 11:21:28,884 iteration 3478 : loss : 0.025255, loss_ce: 0.008188
2022-01-10 11:21:30,453 iteration 3479 : loss : 0.034374, loss_ce: 0.016128
2022-01-10 11:21:31,989 iteration 3480 : loss : 0.021674, loss_ce: 0.009205
2022-01-10 11:21:33,579 iteration 3481 : loss : 0.027945, loss_ce: 0.014780
2022-01-10 11:21:35,083 iteration 3482 : loss : 0.029552, loss_ce: 0.010831
2022-01-10 11:21:36,719 iteration 3483 : loss : 0.021124, loss_ce: 0.008147
2022-01-10 11:21:38,303 iteration 3484 : loss : 0.026593, loss_ce: 0.008188
2022-01-10 11:21:38,303 Training Data Eval:
2022-01-10 11:21:46,254   Average segmentation loss on training set: 0.0167
2022-01-10 11:21:46,255 Validation Data Eval:
2022-01-10 11:21:49,001   Average segmentation loss on validation set: 0.0658
2022-01-10 11:21:50,581 iteration 3485 : loss : 0.023087, loss_ce: 0.009226
 51%|█████████████▊             | 205/400 [1:40:29<1:40:41, 30.98s/it]2022-01-10 11:21:52,187 iteration 3486 : loss : 0.021936, loss_ce: 0.007655
2022-01-10 11:21:53,771 iteration 3487 : loss : 0.022901, loss_ce: 0.008758
2022-01-10 11:21:55,404 iteration 3488 : loss : 0.023609, loss_ce: 0.013939
2022-01-10 11:21:56,948 iteration 3489 : loss : 0.026726, loss_ce: 0.008479
2022-01-10 11:21:58,601 iteration 3490 : loss : 0.029889, loss_ce: 0.013288
2022-01-10 11:22:00,222 iteration 3491 : loss : 0.028197, loss_ce: 0.014089
2022-01-10 11:22:01,718 iteration 3492 : loss : 0.021115, loss_ce: 0.006451
2022-01-10 11:22:03,391 iteration 3493 : loss : 0.033790, loss_ce: 0.008282
2022-01-10 11:22:05,001 iteration 3494 : loss : 0.025707, loss_ce: 0.014127
2022-01-10 11:22:06,485 iteration 3495 : loss : 0.016247, loss_ce: 0.006804
2022-01-10 11:22:08,090 iteration 3496 : loss : 0.044069, loss_ce: 0.011595
2022-01-10 11:22:09,746 iteration 3497 : loss : 0.030439, loss_ce: 0.015030
2022-01-10 11:22:11,339 iteration 3498 : loss : 0.023678, loss_ce: 0.009868
2022-01-10 11:22:12,984 iteration 3499 : loss : 0.038490, loss_ce: 0.010624
2022-01-10 11:22:14,542 iteration 3500 : loss : 0.025562, loss_ce: 0.007513
2022-01-10 11:22:16,146 iteration 3501 : loss : 0.026657, loss_ce: 0.008988
2022-01-10 11:22:17,742 iteration 3502 : loss : 0.028444, loss_ce: 0.011166
 52%|█████████████▉             | 206/400 [1:40:56<1:36:28, 29.84s/it]2022-01-10 11:22:19,309 iteration 3503 : loss : 0.018145, loss_ce: 0.008124
2022-01-10 11:22:20,898 iteration 3504 : loss : 0.019664, loss_ce: 0.005874
2022-01-10 11:22:22,481 iteration 3505 : loss : 0.023290, loss_ce: 0.012155
2022-01-10 11:22:24,158 iteration 3506 : loss : 0.025740, loss_ce: 0.009062
2022-01-10 11:22:25,712 iteration 3507 : loss : 0.022452, loss_ce: 0.009878
2022-01-10 11:22:27,213 iteration 3508 : loss : 0.031080, loss_ce: 0.009129
2022-01-10 11:22:28,777 iteration 3509 : loss : 0.034735, loss_ce: 0.016231
2022-01-10 11:22:30,460 iteration 3510 : loss : 0.023370, loss_ce: 0.009813
2022-01-10 11:22:32,019 iteration 3511 : loss : 0.022241, loss_ce: 0.009423
2022-01-10 11:22:33,612 iteration 3512 : loss : 0.026264, loss_ce: 0.010209
2022-01-10 11:22:35,129 iteration 3513 : loss : 0.027242, loss_ce: 0.008709
2022-01-10 11:22:36,618 iteration 3514 : loss : 0.014902, loss_ce: 0.006321
2022-01-10 11:22:38,168 iteration 3515 : loss : 0.025243, loss_ce: 0.009355
2022-01-10 11:22:39,780 iteration 3516 : loss : 0.026910, loss_ce: 0.009710
2022-01-10 11:22:41,357 iteration 3517 : loss : 0.024879, loss_ce: 0.009622
2022-01-10 11:22:42,878 iteration 3518 : loss : 0.018267, loss_ce: 0.008907
2022-01-10 11:22:44,411 iteration 3519 : loss : 0.026270, loss_ce: 0.009851
 52%|█████████████▉             | 207/400 [1:41:23<1:32:55, 28.89s/it]2022-01-10 11:22:46,026 iteration 3520 : loss : 0.025568, loss_ce: 0.008656
2022-01-10 11:22:47,639 iteration 3521 : loss : 0.031587, loss_ce: 0.011885
2022-01-10 11:22:49,254 iteration 3522 : loss : 0.053535, loss_ce: 0.024908
2022-01-10 11:22:50,947 iteration 3523 : loss : 0.037797, loss_ce: 0.014049
2022-01-10 11:22:52,508 iteration 3524 : loss : 0.032973, loss_ce: 0.011566
2022-01-10 11:22:54,118 iteration 3525 : loss : 0.027333, loss_ce: 0.011907
2022-01-10 11:22:55,727 iteration 3526 : loss : 0.024026, loss_ce: 0.011277
2022-01-10 11:22:57,341 iteration 3527 : loss : 0.041293, loss_ce: 0.011645
2022-01-10 11:22:58,873 iteration 3528 : loss : 0.018652, loss_ce: 0.009094
2022-01-10 11:23:00,466 iteration 3529 : loss : 0.031741, loss_ce: 0.010225
2022-01-10 11:23:02,039 iteration 3530 : loss : 0.028271, loss_ce: 0.009363
2022-01-10 11:23:03,651 iteration 3531 : loss : 0.021266, loss_ce: 0.008070
2022-01-10 11:23:05,194 iteration 3532 : loss : 0.022355, loss_ce: 0.007996
2022-01-10 11:23:06,722 iteration 3533 : loss : 0.024464, loss_ce: 0.010552
2022-01-10 11:23:08,257 iteration 3534 : loss : 0.025369, loss_ce: 0.013699
2022-01-10 11:23:09,769 iteration 3535 : loss : 0.027744, loss_ce: 0.010301
2022-01-10 11:23:11,285 iteration 3536 : loss : 0.017275, loss_ce: 0.005638
 52%|██████████████             | 208/400 [1:41:50<1:30:30, 28.28s/it]2022-01-10 11:23:12,885 iteration 3537 : loss : 0.019929, loss_ce: 0.007753
2022-01-10 11:23:14,461 iteration 3538 : loss : 0.029554, loss_ce: 0.009611
2022-01-10 11:23:16,079 iteration 3539 : loss : 0.020285, loss_ce: 0.009136
2022-01-10 11:23:17,596 iteration 3540 : loss : 0.022286, loss_ce: 0.005367
2022-01-10 11:23:19,234 iteration 3541 : loss : 0.022038, loss_ce: 0.011750
2022-01-10 11:23:20,787 iteration 3542 : loss : 0.021221, loss_ce: 0.006520
2022-01-10 11:23:22,389 iteration 3543 : loss : 0.026133, loss_ce: 0.014009
2022-01-10 11:23:24,011 iteration 3544 : loss : 0.025237, loss_ce: 0.009179
2022-01-10 11:23:25,550 iteration 3545 : loss : 0.024498, loss_ce: 0.008756
2022-01-10 11:23:27,040 iteration 3546 : loss : 0.023791, loss_ce: 0.011313
2022-01-10 11:23:28,654 iteration 3547 : loss : 0.024841, loss_ce: 0.008926
2022-01-10 11:23:30,266 iteration 3548 : loss : 0.029720, loss_ce: 0.010290
2022-01-10 11:23:31,854 iteration 3549 : loss : 0.023851, loss_ce: 0.011580
2022-01-10 11:23:33,384 iteration 3550 : loss : 0.020775, loss_ce: 0.006714
2022-01-10 11:23:35,022 iteration 3551 : loss : 0.034544, loss_ce: 0.011171
2022-01-10 11:23:36,660 iteration 3552 : loss : 0.036797, loss_ce: 0.014594
2022-01-10 11:23:38,305 iteration 3553 : loss : 0.030074, loss_ce: 0.010165
 52%|██████████████             | 209/400 [1:42:17<1:28:48, 27.90s/it]2022-01-10 11:23:39,931 iteration 3554 : loss : 0.021835, loss_ce: 0.008389
2022-01-10 11:23:41,561 iteration 3555 : loss : 0.030650, loss_ce: 0.013759
2022-01-10 11:23:43,129 iteration 3556 : loss : 0.028366, loss_ce: 0.009562
2022-01-10 11:23:44,675 iteration 3557 : loss : 0.021458, loss_ce: 0.008652
2022-01-10 11:23:46,182 iteration 3558 : loss : 0.020829, loss_ce: 0.007260
2022-01-10 11:23:47,819 iteration 3559 : loss : 0.031335, loss_ce: 0.011791
2022-01-10 11:23:49,377 iteration 3560 : loss : 0.037585, loss_ce: 0.010054
2022-01-10 11:23:50,990 iteration 3561 : loss : 0.022667, loss_ce: 0.009831
2022-01-10 11:23:52,571 iteration 3562 : loss : 0.022097, loss_ce: 0.007378
2022-01-10 11:23:54,143 iteration 3563 : loss : 0.019139, loss_ce: 0.007150
2022-01-10 11:23:55,745 iteration 3564 : loss : 0.030455, loss_ce: 0.009332
2022-01-10 11:23:57,264 iteration 3565 : loss : 0.022824, loss_ce: 0.008764
2022-01-10 11:23:58,722 iteration 3566 : loss : 0.016420, loss_ce: 0.006391
2022-01-10 11:24:00,222 iteration 3567 : loss : 0.024012, loss_ce: 0.007248
2022-01-10 11:24:01,823 iteration 3568 : loss : 0.025415, loss_ce: 0.009526
2022-01-10 11:24:03,351 iteration 3569 : loss : 0.025215, loss_ce: 0.011111
2022-01-10 11:24:03,351 Training Data Eval:
2022-01-10 11:24:11,297   Average segmentation loss on training set: 0.0160
2022-01-10 11:24:11,297 Validation Data Eval:
2022-01-10 11:24:14,040   Average segmentation loss on validation set: 0.0743
2022-01-10 11:24:15,578 iteration 3570 : loss : 0.019152, loss_ce: 0.005903
 52%|██████████████▏            | 210/400 [1:42:54<1:37:15, 30.71s/it]2022-01-10 11:24:17,228 iteration 3571 : loss : 0.032995, loss_ce: 0.014344
2022-01-10 11:24:18,874 iteration 3572 : loss : 0.020804, loss_ce: 0.008221
2022-01-10 11:24:20,476 iteration 3573 : loss : 0.023917, loss_ce: 0.008098
2022-01-10 11:24:22,020 iteration 3574 : loss : 0.025676, loss_ce: 0.013655
2022-01-10 11:24:23,546 iteration 3575 : loss : 0.020796, loss_ce: 0.007965
2022-01-10 11:24:25,090 iteration 3576 : loss : 0.029186, loss_ce: 0.010716
2022-01-10 11:24:26,701 iteration 3577 : loss : 0.018056, loss_ce: 0.006667
2022-01-10 11:24:28,252 iteration 3578 : loss : 0.023437, loss_ce: 0.009126
2022-01-10 11:24:29,835 iteration 3579 : loss : 0.017246, loss_ce: 0.006310
2022-01-10 11:24:31,459 iteration 3580 : loss : 0.019387, loss_ce: 0.009230
2022-01-10 11:24:33,081 iteration 3581 : loss : 0.024872, loss_ce: 0.008904
2022-01-10 11:24:34,679 iteration 3582 : loss : 0.032258, loss_ce: 0.011864
2022-01-10 11:24:36,238 iteration 3583 : loss : 0.032098, loss_ce: 0.008369
2022-01-10 11:24:37,734 iteration 3584 : loss : 0.017413, loss_ce: 0.006091
2022-01-10 11:24:39,271 iteration 3585 : loss : 0.015674, loss_ce: 0.006186
2022-01-10 11:24:40,781 iteration 3586 : loss : 0.015152, loss_ce: 0.004776
2022-01-10 11:24:42,352 iteration 3587 : loss : 0.015814, loss_ce: 0.006967
 53%|██████████████▏            | 211/400 [1:43:21<1:33:01, 29.53s/it]2022-01-10 11:24:44,034 iteration 3588 : loss : 0.016815, loss_ce: 0.006522
2022-01-10 11:24:45,682 iteration 3589 : loss : 0.038055, loss_ce: 0.009030
2022-01-10 11:24:47,265 iteration 3590 : loss : 0.021155, loss_ce: 0.005689
2022-01-10 11:24:48,795 iteration 3591 : loss : 0.018082, loss_ce: 0.006222
2022-01-10 11:24:50,358 iteration 3592 : loss : 0.020882, loss_ce: 0.007913
2022-01-10 11:24:51,958 iteration 3593 : loss : 0.020715, loss_ce: 0.010025
2022-01-10 11:24:53,535 iteration 3594 : loss : 0.022407, loss_ce: 0.007667
2022-01-10 11:24:55,098 iteration 3595 : loss : 0.048147, loss_ce: 0.018257
2022-01-10 11:24:56,628 iteration 3596 : loss : 0.018491, loss_ce: 0.007840
2022-01-10 11:24:58,212 iteration 3597 : loss : 0.022013, loss_ce: 0.008392
2022-01-10 11:24:59,825 iteration 3598 : loss : 0.025783, loss_ce: 0.012167
2022-01-10 11:25:01,434 iteration 3599 : loss : 0.034749, loss_ce: 0.010096
2022-01-10 11:25:02,952 iteration 3600 : loss : 0.019812, loss_ce: 0.010844
2022-01-10 11:25:04,556 iteration 3601 : loss : 0.029856, loss_ce: 0.012442
2022-01-10 11:25:06,171 iteration 3602 : loss : 0.024580, loss_ce: 0.009095
2022-01-10 11:25:07,714 iteration 3603 : loss : 0.027622, loss_ce: 0.014234
2022-01-10 11:25:09,284 iteration 3604 : loss : 0.025557, loss_ce: 0.006855
 53%|██████████████▎            | 212/400 [1:43:48<1:30:05, 28.75s/it]2022-01-10 11:25:11,036 iteration 3605 : loss : 0.032777, loss_ce: 0.012601
2022-01-10 11:25:12,552 iteration 3606 : loss : 0.030216, loss_ce: 0.007426
2022-01-10 11:25:14,025 iteration 3607 : loss : 0.013372, loss_ce: 0.004884
2022-01-10 11:25:15,662 iteration 3608 : loss : 0.032692, loss_ce: 0.009888
2022-01-10 11:25:17,287 iteration 3609 : loss : 0.049635, loss_ce: 0.017697
2022-01-10 11:25:18,801 iteration 3610 : loss : 0.014080, loss_ce: 0.005593
2022-01-10 11:25:20,370 iteration 3611 : loss : 0.024570, loss_ce: 0.012169
2022-01-10 11:25:21,892 iteration 3612 : loss : 0.021438, loss_ce: 0.009782
2022-01-10 11:25:23,473 iteration 3613 : loss : 0.016422, loss_ce: 0.006179
2022-01-10 11:25:25,011 iteration 3614 : loss : 0.025067, loss_ce: 0.010936
2022-01-10 11:25:26,566 iteration 3615 : loss : 0.031399, loss_ce: 0.010017
2022-01-10 11:25:28,122 iteration 3616 : loss : 0.020613, loss_ce: 0.009895
2022-01-10 11:25:29,724 iteration 3617 : loss : 0.029283, loss_ce: 0.015832
2022-01-10 11:25:31,328 iteration 3618 : loss : 0.031992, loss_ce: 0.010948
2022-01-10 11:25:32,876 iteration 3619 : loss : 0.018758, loss_ce: 0.006842
2022-01-10 11:25:34,466 iteration 3620 : loss : 0.057977, loss_ce: 0.022505
2022-01-10 11:25:36,027 iteration 3621 : loss : 0.018336, loss_ce: 0.008744
 53%|██████████████▍            | 213/400 [1:44:15<1:27:43, 28.15s/it]2022-01-10 11:25:37,676 iteration 3622 : loss : 0.024961, loss_ce: 0.007290
2022-01-10 11:25:39,260 iteration 3623 : loss : 0.029529, loss_ce: 0.010856
2022-01-10 11:25:40,857 iteration 3624 : loss : 0.020012, loss_ce: 0.008157
2022-01-10 11:25:42,350 iteration 3625 : loss : 0.019632, loss_ce: 0.007690
2022-01-10 11:25:43,931 iteration 3626 : loss : 0.023763, loss_ce: 0.008292
2022-01-10 11:25:45,546 iteration 3627 : loss : 0.032520, loss_ce: 0.013845
2022-01-10 11:25:47,162 iteration 3628 : loss : 0.058957, loss_ce: 0.019002
2022-01-10 11:25:48,814 iteration 3629 : loss : 0.030793, loss_ce: 0.015559
2022-01-10 11:25:50,408 iteration 3630 : loss : 0.020982, loss_ce: 0.009324
2022-01-10 11:25:51,987 iteration 3631 : loss : 0.021587, loss_ce: 0.007494
2022-01-10 11:25:53,606 iteration 3632 : loss : 0.036278, loss_ce: 0.023905
2022-01-10 11:25:55,235 iteration 3633 : loss : 0.024571, loss_ce: 0.007770
2022-01-10 11:25:56,790 iteration 3634 : loss : 0.020053, loss_ce: 0.005714
2022-01-10 11:25:58,484 iteration 3635 : loss : 0.022642, loss_ce: 0.010124
2022-01-10 11:25:59,980 iteration 3636 : loss : 0.018953, loss_ce: 0.009272
2022-01-10 11:26:01,556 iteration 3637 : loss : 0.034061, loss_ce: 0.016045
2022-01-10 11:26:03,152 iteration 3638 : loss : 0.040649, loss_ce: 0.015725
 54%|██████████████▍            | 214/400 [1:44:42<1:26:18, 27.84s/it]2022-01-10 11:26:04,801 iteration 3639 : loss : 0.047964, loss_ce: 0.018452
2022-01-10 11:26:06,424 iteration 3640 : loss : 0.044164, loss_ce: 0.016502
2022-01-10 11:26:08,048 iteration 3641 : loss : 0.021325, loss_ce: 0.007550
2022-01-10 11:26:09,603 iteration 3642 : loss : 0.024156, loss_ce: 0.011626
2022-01-10 11:26:11,206 iteration 3643 : loss : 0.027188, loss_ce: 0.008657
2022-01-10 11:26:12,814 iteration 3644 : loss : 0.024063, loss_ce: 0.009025
2022-01-10 11:26:14,481 iteration 3645 : loss : 0.065567, loss_ce: 0.015313
2022-01-10 11:26:16,091 iteration 3646 : loss : 0.024943, loss_ce: 0.012154
2022-01-10 11:26:17,649 iteration 3647 : loss : 0.018993, loss_ce: 0.008081
2022-01-10 11:26:19,253 iteration 3648 : loss : 0.025706, loss_ce: 0.009926
2022-01-10 11:26:20,727 iteration 3649 : loss : 0.019798, loss_ce: 0.007599
2022-01-10 11:26:22,309 iteration 3650 : loss : 0.021975, loss_ce: 0.009942
2022-01-10 11:26:23,846 iteration 3651 : loss : 0.026396, loss_ce: 0.009610
2022-01-10 11:26:25,358 iteration 3652 : loss : 0.026165, loss_ce: 0.010518
2022-01-10 11:26:26,995 iteration 3653 : loss : 0.038238, loss_ce: 0.017069
2022-01-10 11:26:28,552 iteration 3654 : loss : 0.022034, loss_ce: 0.008795
2022-01-10 11:26:28,553 Training Data Eval:
2022-01-10 11:26:36,505   Average segmentation loss on training set: 0.0158
2022-01-10 11:26:36,505 Validation Data Eval:
2022-01-10 11:26:39,246   Average segmentation loss on validation set: 0.0710
2022-01-10 11:26:40,966 iteration 3655 : loss : 0.049048, loss_ce: 0.018202
 54%|██████████████▌            | 215/400 [1:45:20<1:35:03, 30.83s/it]2022-01-10 11:26:42,596 iteration 3656 : loss : 0.023846, loss_ce: 0.008032
2022-01-10 11:26:44,223 iteration 3657 : loss : 0.019354, loss_ce: 0.008419
2022-01-10 11:26:45,801 iteration 3658 : loss : 0.033909, loss_ce: 0.016530
2022-01-10 11:26:47,311 iteration 3659 : loss : 0.029032, loss_ce: 0.010383
2022-01-10 11:26:48,968 iteration 3660 : loss : 0.027008, loss_ce: 0.011384
2022-01-10 11:26:50,536 iteration 3661 : loss : 0.022716, loss_ce: 0.009567
2022-01-10 11:26:52,043 iteration 3662 : loss : 0.024531, loss_ce: 0.012522
2022-01-10 11:26:53,613 iteration 3663 : loss : 0.018278, loss_ce: 0.007077
2022-01-10 11:26:55,212 iteration 3664 : loss : 0.029136, loss_ce: 0.008988
2022-01-10 11:26:56,904 iteration 3665 : loss : 0.033299, loss_ce: 0.009605
2022-01-10 11:26:58,530 iteration 3666 : loss : 0.025176, loss_ce: 0.010506
2022-01-10 11:27:00,084 iteration 3667 : loss : 0.021340, loss_ce: 0.008050
2022-01-10 11:27:01,647 iteration 3668 : loss : 0.027115, loss_ce: 0.010382
2022-01-10 11:27:03,243 iteration 3669 : loss : 0.030289, loss_ce: 0.009664
2022-01-10 11:27:04,750 iteration 3670 : loss : 0.019559, loss_ce: 0.009105
2022-01-10 11:27:06,312 iteration 3671 : loss : 0.035401, loss_ce: 0.010460
2022-01-10 11:27:07,863 iteration 3672 : loss : 0.034909, loss_ce: 0.009277
 54%|██████████████▌            | 216/400 [1:45:46<1:30:56, 29.65s/it]2022-01-10 11:27:09,476 iteration 3673 : loss : 0.020706, loss_ce: 0.008838
2022-01-10 11:27:11,133 iteration 3674 : loss : 0.018584, loss_ce: 0.008526
2022-01-10 11:27:12,790 iteration 3675 : loss : 0.024084, loss_ce: 0.010585
2022-01-10 11:27:14,430 iteration 3676 : loss : 0.028642, loss_ce: 0.007851
2022-01-10 11:27:16,034 iteration 3677 : loss : 0.027960, loss_ce: 0.012512
2022-01-10 11:27:17,694 iteration 3678 : loss : 0.035025, loss_ce: 0.011120
2022-01-10 11:27:19,195 iteration 3679 : loss : 0.018370, loss_ce: 0.006820
2022-01-10 11:27:20,720 iteration 3680 : loss : 0.019401, loss_ce: 0.007508
2022-01-10 11:27:22,290 iteration 3681 : loss : 0.038985, loss_ce: 0.010926
2022-01-10 11:27:23,798 iteration 3682 : loss : 0.017938, loss_ce: 0.005658
2022-01-10 11:27:25,364 iteration 3683 : loss : 0.023604, loss_ce: 0.010958
2022-01-10 11:27:26,871 iteration 3684 : loss : 0.020429, loss_ce: 0.006723
2022-01-10 11:27:28,426 iteration 3685 : loss : 0.021914, loss_ce: 0.008320
2022-01-10 11:27:30,069 iteration 3686 : loss : 0.022077, loss_ce: 0.008533
2022-01-10 11:27:31,725 iteration 3687 : loss : 0.030561, loss_ce: 0.012283
2022-01-10 11:27:33,327 iteration 3688 : loss : 0.024527, loss_ce: 0.009349
2022-01-10 11:27:34,940 iteration 3689 : loss : 0.038914, loss_ce: 0.010733
 54%|██████████████▋            | 217/400 [1:46:13<1:28:04, 28.88s/it]2022-01-10 11:27:36,712 iteration 3690 : loss : 0.036207, loss_ce: 0.017288
2022-01-10 11:27:38,274 iteration 3691 : loss : 0.027897, loss_ce: 0.006501
2022-01-10 11:27:39,889 iteration 3692 : loss : 0.027338, loss_ce: 0.008385
2022-01-10 11:27:41,420 iteration 3693 : loss : 0.016774, loss_ce: 0.004577
2022-01-10 11:27:43,032 iteration 3694 : loss : 0.042564, loss_ce: 0.014950
2022-01-10 11:27:44,632 iteration 3695 : loss : 0.019900, loss_ce: 0.006833
2022-01-10 11:27:46,167 iteration 3696 : loss : 0.016262, loss_ce: 0.006418
2022-01-10 11:27:47,700 iteration 3697 : loss : 0.025629, loss_ce: 0.008509
2022-01-10 11:27:49,367 iteration 3698 : loss : 0.028380, loss_ce: 0.008793
2022-01-10 11:27:50,948 iteration 3699 : loss : 0.020636, loss_ce: 0.009632
2022-01-10 11:27:52,517 iteration 3700 : loss : 0.020006, loss_ce: 0.006609
2022-01-10 11:27:54,025 iteration 3701 : loss : 0.032700, loss_ce: 0.018567
2022-01-10 11:27:55,638 iteration 3702 : loss : 0.021482, loss_ce: 0.007913
2022-01-10 11:27:57,215 iteration 3703 : loss : 0.020936, loss_ce: 0.007553
2022-01-10 11:27:58,793 iteration 3704 : loss : 0.023231, loss_ce: 0.009233
2022-01-10 11:28:00,389 iteration 3705 : loss : 0.031562, loss_ce: 0.018419
2022-01-10 11:28:01,900 iteration 3706 : loss : 0.024608, loss_ce: 0.007886
 55%|██████████████▋            | 218/400 [1:46:40<1:25:51, 28.30s/it]2022-01-10 11:28:03,555 iteration 3707 : loss : 0.053197, loss_ce: 0.011717
2022-01-10 11:28:05,258 iteration 3708 : loss : 0.046187, loss_ce: 0.015542
2022-01-10 11:28:06,847 iteration 3709 : loss : 0.019841, loss_ce: 0.006323
2022-01-10 11:28:08,337 iteration 3710 : loss : 0.018370, loss_ce: 0.006980
2022-01-10 11:28:10,026 iteration 3711 : loss : 0.030245, loss_ce: 0.012757
2022-01-10 11:28:11,560 iteration 3712 : loss : 0.031983, loss_ce: 0.010883
2022-01-10 11:28:13,073 iteration 3713 : loss : 0.017584, loss_ce: 0.006766
2022-01-10 11:28:14,681 iteration 3714 : loss : 0.041189, loss_ce: 0.020023
2022-01-10 11:28:16,229 iteration 3715 : loss : 0.019972, loss_ce: 0.007775
2022-01-10 11:28:17,810 iteration 3716 : loss : 0.032544, loss_ce: 0.015087
2022-01-10 11:28:19,399 iteration 3717 : loss : 0.021644, loss_ce: 0.010578
2022-01-10 11:28:21,016 iteration 3718 : loss : 0.028425, loss_ce: 0.010321
2022-01-10 11:28:22,702 iteration 3719 : loss : 0.041014, loss_ce: 0.014401
2022-01-10 11:28:24,375 iteration 3720 : loss : 0.028736, loss_ce: 0.010174
2022-01-10 11:28:25,969 iteration 3721 : loss : 0.029656, loss_ce: 0.012399
2022-01-10 11:28:27,626 iteration 3722 : loss : 0.022830, loss_ce: 0.006927
2022-01-10 11:28:29,183 iteration 3723 : loss : 0.023759, loss_ce: 0.009432
 55%|██████████████▊            | 219/400 [1:47:08<1:24:27, 27.99s/it]2022-01-10 11:28:30,813 iteration 3724 : loss : 0.021512, loss_ce: 0.010375
2022-01-10 11:28:32,442 iteration 3725 : loss : 0.029516, loss_ce: 0.014739
2022-01-10 11:28:34,014 iteration 3726 : loss : 0.037655, loss_ce: 0.021834
2022-01-10 11:28:35,600 iteration 3727 : loss : 0.016610, loss_ce: 0.006103
2022-01-10 11:28:37,095 iteration 3728 : loss : 0.019461, loss_ce: 0.009321
2022-01-10 11:28:38,713 iteration 3729 : loss : 0.033255, loss_ce: 0.009575
2022-01-10 11:28:40,249 iteration 3730 : loss : 0.023275, loss_ce: 0.007477
2022-01-10 11:28:41,878 iteration 3731 : loss : 0.034404, loss_ce: 0.012218
2022-01-10 11:28:43,463 iteration 3732 : loss : 0.022622, loss_ce: 0.011100
2022-01-10 11:28:44,973 iteration 3733 : loss : 0.017947, loss_ce: 0.004929
2022-01-10 11:28:46,585 iteration 3734 : loss : 0.024115, loss_ce: 0.008713
2022-01-10 11:28:48,241 iteration 3735 : loss : 0.023120, loss_ce: 0.007829
2022-01-10 11:28:49,852 iteration 3736 : loss : 0.025952, loss_ce: 0.011362
2022-01-10 11:28:51,501 iteration 3737 : loss : 0.033362, loss_ce: 0.012671
2022-01-10 11:28:53,087 iteration 3738 : loss : 0.035892, loss_ce: 0.010809
2022-01-10 11:28:54,665 iteration 3739 : loss : 0.028234, loss_ce: 0.007553
2022-01-10 11:28:54,665 Training Data Eval:
2022-01-10 11:29:02,615   Average segmentation loss on training set: 0.0151
2022-01-10 11:29:02,615 Validation Data Eval:
2022-01-10 11:29:05,356   Average segmentation loss on validation set: 0.0628
2022-01-10 11:29:06,880 iteration 3740 : loss : 0.026804, loss_ce: 0.008488
 55%|██████████████▊            | 220/400 [1:47:45<1:32:43, 30.91s/it]2022-01-10 11:29:08,533 iteration 3741 : loss : 0.029103, loss_ce: 0.013200
2022-01-10 11:29:10,090 iteration 3742 : loss : 0.020878, loss_ce: 0.007283
2022-01-10 11:29:11,638 iteration 3743 : loss : 0.020003, loss_ce: 0.005702
2022-01-10 11:29:13,243 iteration 3744 : loss : 0.017327, loss_ce: 0.006663
2022-01-10 11:29:14,906 iteration 3745 : loss : 0.033953, loss_ce: 0.021304
2022-01-10 11:29:16,459 iteration 3746 : loss : 0.021640, loss_ce: 0.004849
2022-01-10 11:29:18,075 iteration 3747 : loss : 0.025442, loss_ce: 0.012146
2022-01-10 11:29:19,714 iteration 3748 : loss : 0.033690, loss_ce: 0.012775
2022-01-10 11:29:21,306 iteration 3749 : loss : 0.023508, loss_ce: 0.006726
2022-01-10 11:29:22,989 iteration 3750 : loss : 0.026429, loss_ce: 0.009202
2022-01-10 11:29:24,555 iteration 3751 : loss : 0.020649, loss_ce: 0.009412
2022-01-10 11:29:26,110 iteration 3752 : loss : 0.024461, loss_ce: 0.010261
2022-01-10 11:29:27,675 iteration 3753 : loss : 0.025458, loss_ce: 0.008130
2022-01-10 11:29:29,171 iteration 3754 : loss : 0.020468, loss_ce: 0.009949
2022-01-10 11:29:30,759 iteration 3755 : loss : 0.027422, loss_ce: 0.011408
2022-01-10 11:29:32,303 iteration 3756 : loss : 0.018809, loss_ce: 0.006444
2022-01-10 11:29:33,873 iteration 3757 : loss : 0.023591, loss_ce: 0.009039
 55%|██████████████▉            | 221/400 [1:48:12<1:28:42, 29.74s/it]2022-01-10 11:29:35,555 iteration 3758 : loss : 0.030072, loss_ce: 0.011899
2022-01-10 11:29:37,186 iteration 3759 : loss : 0.032341, loss_ce: 0.016811
2022-01-10 11:29:38,811 iteration 3760 : loss : 0.045587, loss_ce: 0.023792
2022-01-10 11:29:40,403 iteration 3761 : loss : 0.016677, loss_ce: 0.007731
2022-01-10 11:29:42,035 iteration 3762 : loss : 0.033554, loss_ce: 0.014115
2022-01-10 11:29:43,665 iteration 3763 : loss : 0.028872, loss_ce: 0.014705
2022-01-10 11:29:45,214 iteration 3764 : loss : 0.045288, loss_ce: 0.009669
2022-01-10 11:29:46,708 iteration 3765 : loss : 0.024314, loss_ce: 0.008123
2022-01-10 11:29:48,241 iteration 3766 : loss : 0.018956, loss_ce: 0.006126
2022-01-10 11:29:49,827 iteration 3767 : loss : 0.031536, loss_ce: 0.011125
2022-01-10 11:29:51,477 iteration 3768 : loss : 0.021321, loss_ce: 0.008159
2022-01-10 11:29:53,060 iteration 3769 : loss : 0.024681, loss_ce: 0.009098
2022-01-10 11:29:54,693 iteration 3770 : loss : 0.022567, loss_ce: 0.007302
2022-01-10 11:29:56,305 iteration 3771 : loss : 0.025165, loss_ce: 0.011142
2022-01-10 11:29:57,935 iteration 3772 : loss : 0.055380, loss_ce: 0.013512
2022-01-10 11:29:59,531 iteration 3773 : loss : 0.027080, loss_ce: 0.008713
2022-01-10 11:30:01,133 iteration 3774 : loss : 0.028470, loss_ce: 0.013744
 56%|██████████████▉            | 222/400 [1:48:40<1:26:00, 28.99s/it]2022-01-10 11:30:02,832 iteration 3775 : loss : 0.040498, loss_ce: 0.017767
2022-01-10 11:30:04,420 iteration 3776 : loss : 0.020665, loss_ce: 0.008220
2022-01-10 11:30:06,007 iteration 3777 : loss : 0.032818, loss_ce: 0.009395
2022-01-10 11:30:07,652 iteration 3778 : loss : 0.036698, loss_ce: 0.014740
2022-01-10 11:30:09,183 iteration 3779 : loss : 0.032395, loss_ce: 0.015983
2022-01-10 11:30:10,828 iteration 3780 : loss : 0.030898, loss_ce: 0.013892
2022-01-10 11:30:12,454 iteration 3781 : loss : 0.025137, loss_ce: 0.010264
2022-01-10 11:30:14,099 iteration 3782 : loss : 0.037951, loss_ce: 0.013965
2022-01-10 11:30:15,730 iteration 3783 : loss : 0.020571, loss_ce: 0.009441
2022-01-10 11:30:17,256 iteration 3784 : loss : 0.024683, loss_ce: 0.006925
2022-01-10 11:30:18,840 iteration 3785 : loss : 0.027502, loss_ce: 0.007055
2022-01-10 11:30:20,420 iteration 3786 : loss : 0.023197, loss_ce: 0.008992
2022-01-10 11:30:21,940 iteration 3787 : loss : 0.040465, loss_ce: 0.016677
2022-01-10 11:30:23,540 iteration 3788 : loss : 0.035023, loss_ce: 0.011367
2022-01-10 11:30:25,120 iteration 3789 : loss : 0.021908, loss_ce: 0.009248
2022-01-10 11:30:26,660 iteration 3790 : loss : 0.020556, loss_ce: 0.007392
2022-01-10 11:30:28,298 iteration 3791 : loss : 0.026935, loss_ce: 0.011597
 56%|███████████████            | 223/400 [1:49:07<1:23:54, 28.44s/it]2022-01-10 11:30:29,895 iteration 3792 : loss : 0.026611, loss_ce: 0.010465
2022-01-10 11:30:31,389 iteration 3793 : loss : 0.016417, loss_ce: 0.004005
2022-01-10 11:30:32,941 iteration 3794 : loss : 0.022328, loss_ce: 0.008870
2022-01-10 11:30:34,483 iteration 3795 : loss : 0.024670, loss_ce: 0.007653
2022-01-10 11:30:36,125 iteration 3796 : loss : 0.019694, loss_ce: 0.006444
2022-01-10 11:30:37,726 iteration 3797 : loss : 0.017058, loss_ce: 0.005149
2022-01-10 11:30:39,253 iteration 3798 : loss : 0.018699, loss_ce: 0.007152
2022-01-10 11:30:40,919 iteration 3799 : loss : 0.025610, loss_ce: 0.008176
2022-01-10 11:30:42,649 iteration 3800 : loss : 0.039081, loss_ce: 0.017814
2022-01-10 11:30:44,319 iteration 3801 : loss : 0.028227, loss_ce: 0.008476
2022-01-10 11:30:45,963 iteration 3802 : loss : 0.037284, loss_ce: 0.015991
2022-01-10 11:30:47,534 iteration 3803 : loss : 0.024437, loss_ce: 0.009234
2022-01-10 11:30:49,119 iteration 3804 : loss : 0.024196, loss_ce: 0.008859
2022-01-10 11:30:50,701 iteration 3805 : loss : 0.027105, loss_ce: 0.011180
2022-01-10 11:30:52,189 iteration 3806 : loss : 0.019314, loss_ce: 0.008358
2022-01-10 11:30:53,789 iteration 3807 : loss : 0.023434, loss_ce: 0.008135
2022-01-10 11:30:55,348 iteration 3808 : loss : 0.024954, loss_ce: 0.011168
 56%|███████████████            | 224/400 [1:49:34<1:22:12, 28.02s/it]2022-01-10 11:30:56,997 iteration 3809 : loss : 0.019328, loss_ce: 0.007340
2022-01-10 11:30:58,614 iteration 3810 : loss : 0.026179, loss_ce: 0.008392
2022-01-10 11:31:00,166 iteration 3811 : loss : 0.019839, loss_ce: 0.007266
2022-01-10 11:31:01,762 iteration 3812 : loss : 0.021022, loss_ce: 0.007698
2022-01-10 11:31:03,247 iteration 3813 : loss : 0.020482, loss_ce: 0.009049
2022-01-10 11:31:04,880 iteration 3814 : loss : 0.032464, loss_ce: 0.016292
2022-01-10 11:31:06,564 iteration 3815 : loss : 0.023529, loss_ce: 0.009046
2022-01-10 11:31:08,099 iteration 3816 : loss : 0.015728, loss_ce: 0.005454
2022-01-10 11:31:09,566 iteration 3817 : loss : 0.016958, loss_ce: 0.005758
2022-01-10 11:31:11,115 iteration 3818 : loss : 0.021856, loss_ce: 0.007007
2022-01-10 11:31:12,737 iteration 3819 : loss : 0.030763, loss_ce: 0.011059
2022-01-10 11:31:14,240 iteration 3820 : loss : 0.014921, loss_ce: 0.006744
2022-01-10 11:31:15,884 iteration 3821 : loss : 0.026745, loss_ce: 0.011873
2022-01-10 11:31:17,444 iteration 3822 : loss : 0.024863, loss_ce: 0.007763
2022-01-10 11:31:18,984 iteration 3823 : loss : 0.027337, loss_ce: 0.009367
2022-01-10 11:31:20,592 iteration 3824 : loss : 0.029284, loss_ce: 0.010260
2022-01-10 11:31:20,592 Training Data Eval:
2022-01-10 11:31:28,536   Average segmentation loss on training set: 0.0154
2022-01-10 11:31:28,537 Validation Data Eval:
2022-01-10 11:31:31,272   Average segmentation loss on validation set: 0.0745
2022-01-10 11:31:32,753 iteration 3825 : loss : 0.018708, loss_ce: 0.006467
 56%|███████████████▏           | 225/400 [1:50:11<1:29:56, 30.84s/it]2022-01-10 11:31:34,459 iteration 3826 : loss : 0.039728, loss_ce: 0.017978
2022-01-10 11:31:36,105 iteration 3827 : loss : 0.026987, loss_ce: 0.010563
2022-01-10 11:31:37,637 iteration 3828 : loss : 0.017467, loss_ce: 0.009033
2022-01-10 11:31:39,170 iteration 3829 : loss : 0.022616, loss_ce: 0.011214
2022-01-10 11:31:40,828 iteration 3830 : loss : 0.030412, loss_ce: 0.008827
2022-01-10 11:31:42,458 iteration 3831 : loss : 0.025331, loss_ce: 0.009720
2022-01-10 11:31:44,066 iteration 3832 : loss : 0.026026, loss_ce: 0.011878
2022-01-10 11:31:45,671 iteration 3833 : loss : 0.032010, loss_ce: 0.010498
2022-01-10 11:31:47,197 iteration 3834 : loss : 0.026074, loss_ce: 0.008795
2022-01-10 11:31:48,796 iteration 3835 : loss : 0.044146, loss_ce: 0.010617
2022-01-10 11:31:50,307 iteration 3836 : loss : 0.018678, loss_ce: 0.008822
2022-01-10 11:31:51,783 iteration 3837 : loss : 0.017985, loss_ce: 0.007615
2022-01-10 11:31:53,404 iteration 3838 : loss : 0.021927, loss_ce: 0.009486
2022-01-10 11:31:54,930 iteration 3839 : loss : 0.026738, loss_ce: 0.007433
2022-01-10 11:31:56,511 iteration 3840 : loss : 0.022542, loss_ce: 0.009702
2022-01-10 11:31:58,001 iteration 3841 : loss : 0.025578, loss_ce: 0.009762
2022-01-10 11:31:59,610 iteration 3842 : loss : 0.052424, loss_ce: 0.012766
 56%|███████████████▎           | 226/400 [1:50:38<1:25:58, 29.64s/it]2022-01-10 11:32:01,234 iteration 3843 : loss : 0.023506, loss_ce: 0.007347
2022-01-10 11:32:02,863 iteration 3844 : loss : 0.024740, loss_ce: 0.009591
2022-01-10 11:32:04,343 iteration 3845 : loss : 0.027414, loss_ce: 0.009612
2022-01-10 11:32:05,889 iteration 3846 : loss : 0.028190, loss_ce: 0.013029
2022-01-10 11:32:07,568 iteration 3847 : loss : 0.027715, loss_ce: 0.010675
2022-01-10 11:32:09,116 iteration 3848 : loss : 0.021341, loss_ce: 0.008322
2022-01-10 11:32:10,659 iteration 3849 : loss : 0.023843, loss_ce: 0.009603
2022-01-10 11:32:12,297 iteration 3850 : loss : 0.021033, loss_ce: 0.007329
2022-01-10 11:32:13,958 iteration 3851 : loss : 0.052832, loss_ce: 0.014535
2022-01-10 11:32:15,579 iteration 3852 : loss : 0.021470, loss_ce: 0.007707
2022-01-10 11:32:17,155 iteration 3853 : loss : 0.022444, loss_ce: 0.009801
2022-01-10 11:32:18,637 iteration 3854 : loss : 0.016794, loss_ce: 0.006622
2022-01-10 11:32:20,187 iteration 3855 : loss : 0.018647, loss_ce: 0.005075
2022-01-10 11:32:21,875 iteration 3856 : loss : 0.026826, loss_ce: 0.014060
2022-01-10 11:32:23,430 iteration 3857 : loss : 0.041827, loss_ce: 0.011852
2022-01-10 11:32:24,991 iteration 3858 : loss : 0.029831, loss_ce: 0.013823
2022-01-10 11:32:26,519 iteration 3859 : loss : 0.022004, loss_ce: 0.010131
 57%|███████████████▎           | 227/400 [1:51:05<1:23:06, 28.83s/it]2022-01-10 11:32:28,107 iteration 3860 : loss : 0.027399, loss_ce: 0.011691
2022-01-10 11:32:29,654 iteration 3861 : loss : 0.021523, loss_ce: 0.009876
2022-01-10 11:32:31,173 iteration 3862 : loss : 0.022056, loss_ce: 0.006347
2022-01-10 11:32:32,757 iteration 3863 : loss : 0.021939, loss_ce: 0.010346
2022-01-10 11:32:34,243 iteration 3864 : loss : 0.028057, loss_ce: 0.009711
2022-01-10 11:32:35,844 iteration 3865 : loss : 0.018348, loss_ce: 0.008341
2022-01-10 11:32:37,403 iteration 3866 : loss : 0.020125, loss_ce: 0.008469
2022-01-10 11:32:39,012 iteration 3867 : loss : 0.030281, loss_ce: 0.011632
2022-01-10 11:32:40,513 iteration 3868 : loss : 0.018919, loss_ce: 0.007580
2022-01-10 11:32:42,161 iteration 3869 : loss : 0.025646, loss_ce: 0.007432
2022-01-10 11:32:43,705 iteration 3870 : loss : 0.022139, loss_ce: 0.007920
2022-01-10 11:32:45,329 iteration 3871 : loss : 0.027573, loss_ce: 0.010760
2022-01-10 11:32:46,900 iteration 3872 : loss : 0.020692, loss_ce: 0.007986
2022-01-10 11:32:48,449 iteration 3873 : loss : 0.016374, loss_ce: 0.004975
2022-01-10 11:32:50,076 iteration 3874 : loss : 0.023689, loss_ce: 0.007897
2022-01-10 11:32:51,684 iteration 3875 : loss : 0.025507, loss_ce: 0.007647
2022-01-10 11:32:53,360 iteration 3876 : loss : 0.025854, loss_ce: 0.010217
 57%|███████████████▍           | 228/400 [1:51:32<1:20:55, 28.23s/it]2022-01-10 11:32:54,962 iteration 3877 : loss : 0.019564, loss_ce: 0.007546
2022-01-10 11:32:56,568 iteration 3878 : loss : 0.023072, loss_ce: 0.008744
2022-01-10 11:32:58,207 iteration 3879 : loss : 0.019772, loss_ce: 0.010283
2022-01-10 11:32:59,790 iteration 3880 : loss : 0.021774, loss_ce: 0.007635
2022-01-10 11:33:01,391 iteration 3881 : loss : 0.034513, loss_ce: 0.011619
2022-01-10 11:33:03,016 iteration 3882 : loss : 0.018509, loss_ce: 0.008336
2022-01-10 11:33:04,599 iteration 3883 : loss : 0.020503, loss_ce: 0.007051
2022-01-10 11:33:06,184 iteration 3884 : loss : 0.017747, loss_ce: 0.005779
2022-01-10 11:33:07,718 iteration 3885 : loss : 0.017667, loss_ce: 0.005138
2022-01-10 11:33:09,335 iteration 3886 : loss : 0.023520, loss_ce: 0.010702
2022-01-10 11:33:10,930 iteration 3887 : loss : 0.024662, loss_ce: 0.010190
2022-01-10 11:33:12,573 iteration 3888 : loss : 0.029211, loss_ce: 0.014171
2022-01-10 11:33:14,125 iteration 3889 : loss : 0.028473, loss_ce: 0.007942
2022-01-10 11:33:15,673 iteration 3890 : loss : 0.017189, loss_ce: 0.005801
2022-01-10 11:33:17,286 iteration 3891 : loss : 0.024079, loss_ce: 0.011866
2022-01-10 11:33:18,874 iteration 3892 : loss : 0.029951, loss_ce: 0.009991
2022-01-10 11:33:20,420 iteration 3893 : loss : 0.029756, loss_ce: 0.007991
 57%|███████████████▍           | 229/400 [1:51:59<1:19:27, 27.88s/it]2022-01-10 11:33:22,051 iteration 3894 : loss : 0.022724, loss_ce: 0.009149
2022-01-10 11:33:23,661 iteration 3895 : loss : 0.019177, loss_ce: 0.009001
2022-01-10 11:33:25,180 iteration 3896 : loss : 0.018955, loss_ce: 0.007878
2022-01-10 11:33:26,812 iteration 3897 : loss : 0.020838, loss_ce: 0.006678
2022-01-10 11:33:28,418 iteration 3898 : loss : 0.025749, loss_ce: 0.007187
2022-01-10 11:33:30,014 iteration 3899 : loss : 0.015844, loss_ce: 0.005262
2022-01-10 11:33:31,617 iteration 3900 : loss : 0.018009, loss_ce: 0.008032
2022-01-10 11:33:33,101 iteration 3901 : loss : 0.018655, loss_ce: 0.009005
2022-01-10 11:33:34,681 iteration 3902 : loss : 0.016052, loss_ce: 0.005239
2022-01-10 11:33:36,204 iteration 3903 : loss : 0.018091, loss_ce: 0.005588
2022-01-10 11:33:37,794 iteration 3904 : loss : 0.016773, loss_ce: 0.006571
2022-01-10 11:33:39,381 iteration 3905 : loss : 0.030275, loss_ce: 0.017708
2022-01-10 11:33:40,963 iteration 3906 : loss : 0.025969, loss_ce: 0.010238
2022-01-10 11:33:42,535 iteration 3907 : loss : 0.024341, loss_ce: 0.007240
2022-01-10 11:33:44,022 iteration 3908 : loss : 0.020977, loss_ce: 0.007173
2022-01-10 11:33:45,516 iteration 3909 : loss : 0.018273, loss_ce: 0.007348
2022-01-10 11:33:45,516 Training Data Eval:
2022-01-10 11:33:53,473   Average segmentation loss on training set: 0.0139
2022-01-10 11:33:53,473 Validation Data Eval:
2022-01-10 11:33:56,221   Average segmentation loss on validation set: 0.0699
2022-01-10 11:33:57,795 iteration 3910 : loss : 0.022528, loss_ce: 0.009692
 57%|███████████████▌           | 230/400 [1:52:36<1:27:03, 30.73s/it]2022-01-10 11:33:59,342 iteration 3911 : loss : 0.018787, loss_ce: 0.008086
2022-01-10 11:34:00,893 iteration 3912 : loss : 0.037780, loss_ce: 0.011459
2022-01-10 11:34:02,483 iteration 3913 : loss : 0.019614, loss_ce: 0.009112
2022-01-10 11:34:03,962 iteration 3914 : loss : 0.015211, loss_ce: 0.007122
2022-01-10 11:34:05,565 iteration 3915 : loss : 0.027235, loss_ce: 0.009048
2022-01-10 11:34:07,159 iteration 3916 : loss : 0.031349, loss_ce: 0.013397
2022-01-10 11:34:08,777 iteration 3917 : loss : 0.022414, loss_ce: 0.007414
2022-01-10 11:34:10,353 iteration 3918 : loss : 0.019473, loss_ce: 0.005892
2022-01-10 11:34:11,928 iteration 3919 : loss : 0.024762, loss_ce: 0.009159
2022-01-10 11:34:13,541 iteration 3920 : loss : 0.029093, loss_ce: 0.005780
2022-01-10 11:34:15,114 iteration 3921 : loss : 0.026312, loss_ce: 0.007046
2022-01-10 11:34:16,727 iteration 3922 : loss : 0.049049, loss_ce: 0.013438
2022-01-10 11:34:18,410 iteration 3923 : loss : 0.039167, loss_ce: 0.017031
2022-01-10 11:34:19,959 iteration 3924 : loss : 0.021536, loss_ce: 0.007841
2022-01-10 11:34:21,457 iteration 3925 : loss : 0.023553, loss_ce: 0.011253
2022-01-10 11:34:23,182 iteration 3926 : loss : 0.028214, loss_ce: 0.011097
2022-01-10 11:34:24,792 iteration 3927 : loss : 0.030036, loss_ce: 0.014999
 58%|███████████████▌           | 231/400 [1:53:03<1:23:23, 29.61s/it]2022-01-10 11:34:26,412 iteration 3928 : loss : 0.020302, loss_ce: 0.007597
2022-01-10 11:34:27,943 iteration 3929 : loss : 0.026733, loss_ce: 0.015640
2022-01-10 11:34:29,518 iteration 3930 : loss : 0.026286, loss_ce: 0.008872
2022-01-10 11:34:31,017 iteration 3931 : loss : 0.020715, loss_ce: 0.005034
2022-01-10 11:34:32,605 iteration 3932 : loss : 0.028069, loss_ce: 0.010781
2022-01-10 11:34:34,159 iteration 3933 : loss : 0.019011, loss_ce: 0.009033
2022-01-10 11:34:35,761 iteration 3934 : loss : 0.032473, loss_ce: 0.013153
2022-01-10 11:34:37,329 iteration 3935 : loss : 0.022686, loss_ce: 0.008595
2022-01-10 11:34:38,857 iteration 3936 : loss : 0.021328, loss_ce: 0.007573
2022-01-10 11:34:40,430 iteration 3937 : loss : 0.028383, loss_ce: 0.008156
2022-01-10 11:34:42,003 iteration 3938 : loss : 0.026526, loss_ce: 0.008345
2022-01-10 11:34:43,644 iteration 3939 : loss : 0.039523, loss_ce: 0.009405
2022-01-10 11:34:45,222 iteration 3940 : loss : 0.021794, loss_ce: 0.009811
2022-01-10 11:34:46,797 iteration 3941 : loss : 0.027921, loss_ce: 0.009920
2022-01-10 11:34:48,337 iteration 3942 : loss : 0.022547, loss_ce: 0.012321
2022-01-10 11:34:49,869 iteration 3943 : loss : 0.025727, loss_ce: 0.011493
2022-01-10 11:34:51,419 iteration 3944 : loss : 0.021214, loss_ce: 0.006845
 58%|███████████████▋           | 232/400 [1:53:30<1:20:24, 28.72s/it]2022-01-10 11:34:53,106 iteration 3945 : loss : 0.028356, loss_ce: 0.009996
2022-01-10 11:34:54,661 iteration 3946 : loss : 0.019708, loss_ce: 0.007898
2022-01-10 11:34:56,194 iteration 3947 : loss : 0.024543, loss_ce: 0.010794
2022-01-10 11:34:57,812 iteration 3948 : loss : 0.032335, loss_ce: 0.014401
2022-01-10 11:34:59,452 iteration 3949 : loss : 0.039166, loss_ce: 0.013912
2022-01-10 11:35:01,003 iteration 3950 : loss : 0.019815, loss_ce: 0.009567
2022-01-10 11:35:02,522 iteration 3951 : loss : 0.019792, loss_ce: 0.004596
2022-01-10 11:35:04,200 iteration 3952 : loss : 0.023051, loss_ce: 0.011461
2022-01-10 11:35:05,786 iteration 3953 : loss : 0.018316, loss_ce: 0.007708
2022-01-10 11:35:07,312 iteration 3954 : loss : 0.015895, loss_ce: 0.005876
2022-01-10 11:35:08,919 iteration 3955 : loss : 0.020905, loss_ce: 0.008692
2022-01-10 11:35:10,544 iteration 3956 : loss : 0.028193, loss_ce: 0.015386
2022-01-10 11:35:12,184 iteration 3957 : loss : 0.036177, loss_ce: 0.010923
2022-01-10 11:35:13,827 iteration 3958 : loss : 0.020914, loss_ce: 0.010023
2022-01-10 11:35:15,317 iteration 3959 : loss : 0.018780, loss_ce: 0.005297
2022-01-10 11:35:16,938 iteration 3960 : loss : 0.028041, loss_ce: 0.010002
2022-01-10 11:35:18,487 iteration 3961 : loss : 0.020996, loss_ce: 0.005922
 58%|███████████████▋           | 233/400 [1:53:57<1:18:32, 28.22s/it]2022-01-10 11:35:20,139 iteration 3962 : loss : 0.024980, loss_ce: 0.010054
2022-01-10 11:35:21,783 iteration 3963 : loss : 0.027051, loss_ce: 0.011583
2022-01-10 11:35:23,313 iteration 3964 : loss : 0.018864, loss_ce: 0.006066
2022-01-10 11:35:24,909 iteration 3965 : loss : 0.016845, loss_ce: 0.007634
2022-01-10 11:35:26,584 iteration 3966 : loss : 0.028441, loss_ce: 0.012279
2022-01-10 11:35:28,164 iteration 3967 : loss : 0.025307, loss_ce: 0.006159
2022-01-10 11:35:29,740 iteration 3968 : loss : 0.020022, loss_ce: 0.007368
2022-01-10 11:35:31,279 iteration 3969 : loss : 0.023586, loss_ce: 0.008080
2022-01-10 11:35:32,856 iteration 3970 : loss : 0.021896, loss_ce: 0.006349
2022-01-10 11:35:34,426 iteration 3971 : loss : 0.022941, loss_ce: 0.007127
2022-01-10 11:35:36,010 iteration 3972 : loss : 0.038857, loss_ce: 0.016023
2022-01-10 11:35:37,664 iteration 3973 : loss : 0.023866, loss_ce: 0.009635
2022-01-10 11:35:39,309 iteration 3974 : loss : 0.022384, loss_ce: 0.010644
2022-01-10 11:35:40,885 iteration 3975 : loss : 0.030098, loss_ce: 0.007827
2022-01-10 11:35:42,451 iteration 3976 : loss : 0.019467, loss_ce: 0.006927
2022-01-10 11:35:44,067 iteration 3977 : loss : 0.025234, loss_ce: 0.010889
2022-01-10 11:35:45,661 iteration 3978 : loss : 0.026006, loss_ce: 0.011628
 58%|███████████████▊           | 234/400 [1:54:24<1:17:12, 27.91s/it]2022-01-10 11:35:47,381 iteration 3979 : loss : 0.027317, loss_ce: 0.012458
2022-01-10 11:35:48,879 iteration 3980 : loss : 0.018698, loss_ce: 0.007552
2022-01-10 11:35:50,454 iteration 3981 : loss : 0.034850, loss_ce: 0.009741
2022-01-10 11:35:52,048 iteration 3982 : loss : 0.022057, loss_ce: 0.011035
2022-01-10 11:35:53,644 iteration 3983 : loss : 0.035043, loss_ce: 0.007748
2022-01-10 11:35:55,261 iteration 3984 : loss : 0.034115, loss_ce: 0.014624
2022-01-10 11:35:56,885 iteration 3985 : loss : 0.019517, loss_ce: 0.006327
2022-01-10 11:35:58,525 iteration 3986 : loss : 0.023575, loss_ce: 0.007822
2022-01-10 11:36:00,057 iteration 3987 : loss : 0.026180, loss_ce: 0.014144
2022-01-10 11:36:01,643 iteration 3988 : loss : 0.021783, loss_ce: 0.007239
2022-01-10 11:36:03,259 iteration 3989 : loss : 0.026722, loss_ce: 0.009539
2022-01-10 11:36:04,870 iteration 3990 : loss : 0.023300, loss_ce: 0.009368
2022-01-10 11:36:06,506 iteration 3991 : loss : 0.032960, loss_ce: 0.018068
2022-01-10 11:36:08,130 iteration 3992 : loss : 0.041881, loss_ce: 0.016282
2022-01-10 11:36:09,736 iteration 3993 : loss : 0.022736, loss_ce: 0.008907
2022-01-10 11:36:11,385 iteration 3994 : loss : 0.018598, loss_ce: 0.005731
2022-01-10 11:36:11,386 Training Data Eval:
2022-01-10 11:36:19,395   Average segmentation loss on training set: 0.0141
2022-01-10 11:36:19,395 Validation Data Eval:
2022-01-10 11:36:22,149   Average segmentation loss on validation set: 0.0710
2022-01-10 11:36:23,861 iteration 3995 : loss : 0.021283, loss_ce: 0.007539
 59%|███████████████▊           | 235/400 [1:55:02<1:25:13, 30.99s/it]2022-01-10 11:36:25,509 iteration 3996 : loss : 0.021394, loss_ce: 0.009924
2022-01-10 11:36:27,049 iteration 3997 : loss : 0.026286, loss_ce: 0.011907
2022-01-10 11:36:28,565 iteration 3998 : loss : 0.026126, loss_ce: 0.007706
2022-01-10 11:36:30,207 iteration 3999 : loss : 0.041082, loss_ce: 0.010131
2022-01-10 11:36:31,807 iteration 4000 : loss : 0.016235, loss_ce: 0.006611
2022-01-10 11:36:33,443 iteration 4001 : loss : 0.022349, loss_ce: 0.011508
2022-01-10 11:36:34,944 iteration 4002 : loss : 0.018801, loss_ce: 0.006682
2022-01-10 11:36:36,574 iteration 4003 : loss : 0.047139, loss_ce: 0.015631
2022-01-10 11:36:38,208 iteration 4004 : loss : 0.033190, loss_ce: 0.012748
2022-01-10 11:36:39,816 iteration 4005 : loss : 0.022477, loss_ce: 0.008831
2022-01-10 11:36:41,346 iteration 4006 : loss : 0.046114, loss_ce: 0.014223
2022-01-10 11:36:42,902 iteration 4007 : loss : 0.021952, loss_ce: 0.007801
2022-01-10 11:36:44,460 iteration 4008 : loss : 0.019980, loss_ce: 0.009176
2022-01-10 11:36:46,037 iteration 4009 : loss : 0.028438, loss_ce: 0.010323
2022-01-10 11:36:47,627 iteration 4010 : loss : 0.026172, loss_ce: 0.010511
2022-01-10 11:36:49,186 iteration 4011 : loss : 0.031317, loss_ce: 0.010860
2022-01-10 11:36:50,779 iteration 4012 : loss : 0.026762, loss_ce: 0.011283
 59%|███████████████▉           | 236/400 [1:55:29<1:21:22, 29.77s/it]2022-01-10 11:36:52,298 iteration 4013 : loss : 0.016808, loss_ce: 0.008048
2022-01-10 11:36:53,933 iteration 4014 : loss : 0.020263, loss_ce: 0.008203
2022-01-10 11:36:55,453 iteration 4015 : loss : 0.025394, loss_ce: 0.015293
2022-01-10 11:36:57,094 iteration 4016 : loss : 0.020089, loss_ce: 0.007699
2022-01-10 11:36:58,692 iteration 4017 : loss : 0.031967, loss_ce: 0.010208
2022-01-10 11:37:00,194 iteration 4018 : loss : 0.020544, loss_ce: 0.008106
2022-01-10 11:37:01,782 iteration 4019 : loss : 0.030238, loss_ce: 0.009211
2022-01-10 11:37:03,338 iteration 4020 : loss : 0.023103, loss_ce: 0.007503
2022-01-10 11:37:04,940 iteration 4021 : loss : 0.030028, loss_ce: 0.008657
2022-01-10 11:37:06,614 iteration 4022 : loss : 0.033807, loss_ce: 0.010784
2022-01-10 11:37:08,128 iteration 4023 : loss : 0.019101, loss_ce: 0.009094
2022-01-10 11:37:09,754 iteration 4024 : loss : 0.027978, loss_ce: 0.008700
2022-01-10 11:37:11,346 iteration 4025 : loss : 0.019512, loss_ce: 0.007039
2022-01-10 11:37:12,984 iteration 4026 : loss : 0.024621, loss_ce: 0.007263
2022-01-10 11:37:14,479 iteration 4027 : loss : 0.023985, loss_ce: 0.008387
2022-01-10 11:37:16,191 iteration 4028 : loss : 0.041996, loss_ce: 0.019178
2022-01-10 11:37:17,882 iteration 4029 : loss : 0.033803, loss_ce: 0.015587
 59%|███████████████▉           | 237/400 [1:55:56<1:18:42, 28.97s/it]2022-01-10 11:37:19,505 iteration 4030 : loss : 0.017400, loss_ce: 0.006673
2022-01-10 11:37:21,054 iteration 4031 : loss : 0.026355, loss_ce: 0.012094
2022-01-10 11:37:22,541 iteration 4032 : loss : 0.016836, loss_ce: 0.005121
2022-01-10 11:37:24,059 iteration 4033 : loss : 0.022288, loss_ce: 0.008498
2022-01-10 11:37:25,675 iteration 4034 : loss : 0.038131, loss_ce: 0.015474
2022-01-10 11:37:27,274 iteration 4035 : loss : 0.031854, loss_ce: 0.008754
2022-01-10 11:37:28,853 iteration 4036 : loss : 0.023593, loss_ce: 0.010515
2022-01-10 11:37:30,387 iteration 4037 : loss : 0.020014, loss_ce: 0.010693
2022-01-10 11:37:31,956 iteration 4038 : loss : 0.022150, loss_ce: 0.008405
2022-01-10 11:37:33,538 iteration 4039 : loss : 0.031070, loss_ce: 0.009851
2022-01-10 11:37:35,103 iteration 4040 : loss : 0.023867, loss_ce: 0.007692
2022-01-10 11:37:36,718 iteration 4041 : loss : 0.018052, loss_ce: 0.007365
2022-01-10 11:37:38,322 iteration 4042 : loss : 0.022907, loss_ce: 0.008012
2022-01-10 11:37:39,914 iteration 4043 : loss : 0.020333, loss_ce: 0.007925
2022-01-10 11:37:41,473 iteration 4044 : loss : 0.020053, loss_ce: 0.008248
2022-01-10 11:37:43,065 iteration 4045 : loss : 0.020676, loss_ce: 0.006343
2022-01-10 11:37:44,685 iteration 4046 : loss : 0.022581, loss_ce: 0.007983
 60%|████████████████           | 238/400 [1:56:23<1:16:27, 28.32s/it]2022-01-10 11:37:46,347 iteration 4047 : loss : 0.016328, loss_ce: 0.006107
2022-01-10 11:37:47,909 iteration 4048 : loss : 0.022673, loss_ce: 0.007010
2022-01-10 11:37:49,483 iteration 4049 : loss : 0.017266, loss_ce: 0.004950
2022-01-10 11:37:50,998 iteration 4050 : loss : 0.022602, loss_ce: 0.007883
2022-01-10 11:37:52,565 iteration 4051 : loss : 0.021670, loss_ce: 0.009359
2022-01-10 11:37:54,080 iteration 4052 : loss : 0.020097, loss_ce: 0.007473
2022-01-10 11:37:55,676 iteration 4053 : loss : 0.038100, loss_ce: 0.009796
2022-01-10 11:37:57,286 iteration 4054 : loss : 0.022321, loss_ce: 0.008507
2022-01-10 11:37:58,865 iteration 4055 : loss : 0.017372, loss_ce: 0.007742
2022-01-10 11:38:00,431 iteration 4056 : loss : 0.020237, loss_ce: 0.009675
2022-01-10 11:38:01,942 iteration 4057 : loss : 0.019050, loss_ce: 0.005806
2022-01-10 11:38:03,462 iteration 4058 : loss : 0.014026, loss_ce: 0.005847
2022-01-10 11:38:05,071 iteration 4059 : loss : 0.031224, loss_ce: 0.013466
2022-01-10 11:38:06,602 iteration 4060 : loss : 0.031208, loss_ce: 0.013171
2022-01-10 11:38:08,072 iteration 4061 : loss : 0.018423, loss_ce: 0.008816
2022-01-10 11:38:09,598 iteration 4062 : loss : 0.016860, loss_ce: 0.007477
2022-01-10 11:38:11,207 iteration 4063 : loss : 0.023147, loss_ce: 0.008461
 60%|████████████████▏          | 239/400 [1:56:50<1:14:32, 27.78s/it]2022-01-10 11:38:12,836 iteration 4064 : loss : 0.021641, loss_ce: 0.009727
2022-01-10 11:38:14,361 iteration 4065 : loss : 0.019495, loss_ce: 0.005952
2022-01-10 11:38:15,932 iteration 4066 : loss : 0.018937, loss_ce: 0.006203
2022-01-10 11:38:17,492 iteration 4067 : loss : 0.031945, loss_ce: 0.014822
2022-01-10 11:38:19,092 iteration 4068 : loss : 0.028582, loss_ce: 0.010683
2022-01-10 11:38:20,695 iteration 4069 : loss : 0.025566, loss_ce: 0.012549
2022-01-10 11:38:22,214 iteration 4070 : loss : 0.020535, loss_ce: 0.006387
2022-01-10 11:38:23,792 iteration 4071 : loss : 0.022091, loss_ce: 0.008280
2022-01-10 11:38:25,363 iteration 4072 : loss : 0.017064, loss_ce: 0.006508
2022-01-10 11:38:26,974 iteration 4073 : loss : 0.028019, loss_ce: 0.012198
2022-01-10 11:38:28,550 iteration 4074 : loss : 0.022633, loss_ce: 0.009416
2022-01-10 11:38:30,216 iteration 4075 : loss : 0.025354, loss_ce: 0.008085
2022-01-10 11:38:31,823 iteration 4076 : loss : 0.022225, loss_ce: 0.011112
2022-01-10 11:38:33,461 iteration 4077 : loss : 0.023785, loss_ce: 0.009971
2022-01-10 11:38:35,110 iteration 4078 : loss : 0.025904, loss_ce: 0.008671
2022-01-10 11:38:36,655 iteration 4079 : loss : 0.015988, loss_ce: 0.005988
2022-01-10 11:38:36,655 Training Data Eval:
2022-01-10 11:38:44,593   Average segmentation loss on training set: 0.0132
2022-01-10 11:38:44,593 Validation Data Eval:
2022-01-10 11:38:47,335   Average segmentation loss on validation set: 0.0871
2022-01-10 11:38:48,986 iteration 4080 : loss : 0.022955, loss_ce: 0.010324
 60%|████████████████▏          | 240/400 [1:57:28<1:22:05, 30.78s/it]2022-01-10 11:38:50,584 iteration 4081 : loss : 0.016576, loss_ce: 0.007173
2022-01-10 11:38:52,152 iteration 4082 : loss : 0.023032, loss_ce: 0.008001
2022-01-10 11:38:53,700 iteration 4083 : loss : 0.016935, loss_ce: 0.007850
2022-01-10 11:38:55,314 iteration 4084 : loss : 0.025370, loss_ce: 0.008559
2022-01-10 11:38:56,976 iteration 4085 : loss : 0.026315, loss_ce: 0.007831
2022-01-10 11:38:58,602 iteration 4086 : loss : 0.024739, loss_ce: 0.012945
2022-01-10 11:39:00,167 iteration 4087 : loss : 0.026331, loss_ce: 0.007831
2022-01-10 11:39:01,734 iteration 4088 : loss : 0.029590, loss_ce: 0.011413
2022-01-10 11:39:03,342 iteration 4089 : loss : 0.031491, loss_ce: 0.010577
2022-01-10 11:39:04,955 iteration 4090 : loss : 0.016648, loss_ce: 0.006591
2022-01-10 11:39:06,495 iteration 4091 : loss : 0.023105, loss_ce: 0.007484
2022-01-10 11:39:08,074 iteration 4092 : loss : 0.016439, loss_ce: 0.006752
2022-01-10 11:39:09,630 iteration 4093 : loss : 0.016312, loss_ce: 0.006968
2022-01-10 11:39:11,166 iteration 4094 : loss : 0.015249, loss_ce: 0.004896
2022-01-10 11:39:12,716 iteration 4095 : loss : 0.018178, loss_ce: 0.007112
2022-01-10 11:39:14,287 iteration 4096 : loss : 0.018594, loss_ce: 0.007877
2022-01-10 11:39:15,868 iteration 4097 : loss : 0.027518, loss_ce: 0.011730
 60%|████████████████▎          | 241/400 [1:57:54<1:18:27, 29.61s/it]2022-01-10 11:39:17,527 iteration 4098 : loss : 0.028713, loss_ce: 0.010128
2022-01-10 11:39:19,082 iteration 4099 : loss : 0.022679, loss_ce: 0.007089
2022-01-10 11:39:20,589 iteration 4100 : loss : 0.019931, loss_ce: 0.008659
2022-01-10 11:39:22,129 iteration 4101 : loss : 0.019228, loss_ce: 0.007443
2022-01-10 11:39:23,704 iteration 4102 : loss : 0.020167, loss_ce: 0.008571
2022-01-10 11:39:25,248 iteration 4103 : loss : 0.017729, loss_ce: 0.006975
2022-01-10 11:39:26,763 iteration 4104 : loss : 0.040028, loss_ce: 0.017822
2022-01-10 11:39:28,350 iteration 4105 : loss : 0.027209, loss_ce: 0.012043
2022-01-10 11:39:29,865 iteration 4106 : loss : 0.023903, loss_ce: 0.004384
2022-01-10 11:39:31,534 iteration 4107 : loss : 0.021770, loss_ce: 0.011099
2022-01-10 11:39:33,157 iteration 4108 : loss : 0.032485, loss_ce: 0.011828
2022-01-10 11:39:34,669 iteration 4109 : loss : 0.023752, loss_ce: 0.005822
2022-01-10 11:39:36,264 iteration 4110 : loss : 0.023916, loss_ce: 0.011089
2022-01-10 11:39:37,897 iteration 4111 : loss : 0.024567, loss_ce: 0.009260
2022-01-10 11:39:39,446 iteration 4112 : loss : 0.036225, loss_ce: 0.015158
2022-01-10 11:39:41,037 iteration 4113 : loss : 0.026727, loss_ce: 0.010978
2022-01-10 11:39:42,686 iteration 4114 : loss : 0.027818, loss_ce: 0.010744
 60%|████████████████▎          | 242/400 [1:58:21<1:15:46, 28.78s/it]2022-01-10 11:39:44,288 iteration 4115 : loss : 0.020262, loss_ce: 0.005174
2022-01-10 11:39:45,794 iteration 4116 : loss : 0.017743, loss_ce: 0.007659
2022-01-10 11:39:47,512 iteration 4117 : loss : 0.038355, loss_ce: 0.010636
2022-01-10 11:39:49,214 iteration 4118 : loss : 0.026196, loss_ce: 0.011727
2022-01-10 11:39:50,830 iteration 4119 : loss : 0.020702, loss_ce: 0.009418
2022-01-10 11:39:52,424 iteration 4120 : loss : 0.043964, loss_ce: 0.007394
2022-01-10 11:39:54,026 iteration 4121 : loss : 0.020618, loss_ce: 0.009907
2022-01-10 11:39:55,569 iteration 4122 : loss : 0.018706, loss_ce: 0.006222
2022-01-10 11:39:57,222 iteration 4123 : loss : 0.032903, loss_ce: 0.011748
2022-01-10 11:39:58,738 iteration 4124 : loss : 0.022302, loss_ce: 0.008338
2022-01-10 11:40:00,378 iteration 4125 : loss : 0.026235, loss_ce: 0.007702
2022-01-10 11:40:01,975 iteration 4126 : loss : 0.019987, loss_ce: 0.009610
2022-01-10 11:40:03,584 iteration 4127 : loss : 0.021685, loss_ce: 0.008206
2022-01-10 11:40:05,137 iteration 4128 : loss : 0.022613, loss_ce: 0.010620
2022-01-10 11:40:06,776 iteration 4129 : loss : 0.023124, loss_ce: 0.008924
2022-01-10 11:40:08,344 iteration 4130 : loss : 0.017314, loss_ce: 0.008813
2022-01-10 11:40:09,839 iteration 4131 : loss : 0.015527, loss_ce: 0.004349
 61%|████████████████▍          | 243/400 [1:58:48<1:14:01, 28.29s/it]2022-01-10 11:40:11,519 iteration 4132 : loss : 0.022440, loss_ce: 0.008484
2022-01-10 11:40:13,149 iteration 4133 : loss : 0.021534, loss_ce: 0.010587
2022-01-10 11:40:14,649 iteration 4134 : loss : 0.021651, loss_ce: 0.008622
2022-01-10 11:40:16,272 iteration 4135 : loss : 0.028783, loss_ce: 0.011136
2022-01-10 11:40:17,856 iteration 4136 : loss : 0.020118, loss_ce: 0.010103
2022-01-10 11:40:19,398 iteration 4137 : loss : 0.024574, loss_ce: 0.011348
2022-01-10 11:40:20,932 iteration 4138 : loss : 0.036019, loss_ce: 0.018422
2022-01-10 11:40:22,531 iteration 4139 : loss : 0.022477, loss_ce: 0.006778
2022-01-10 11:40:24,158 iteration 4140 : loss : 0.035532, loss_ce: 0.010837
2022-01-10 11:40:25,737 iteration 4141 : loss : 0.021038, loss_ce: 0.005355
2022-01-10 11:40:27,327 iteration 4142 : loss : 0.034593, loss_ce: 0.012417
2022-01-10 11:40:28,955 iteration 4143 : loss : 0.028620, loss_ce: 0.007405
2022-01-10 11:40:30,511 iteration 4144 : loss : 0.016747, loss_ce: 0.006272
2022-01-10 11:40:32,054 iteration 4145 : loss : 0.020508, loss_ce: 0.006317
2022-01-10 11:40:33,632 iteration 4146 : loss : 0.020124, loss_ce: 0.005539
2022-01-10 11:40:35,173 iteration 4147 : loss : 0.021622, loss_ce: 0.007971
2022-01-10 11:40:36,656 iteration 4148 : loss : 0.013935, loss_ce: 0.005117
 61%|████████████████▍          | 244/400 [1:59:15<1:12:24, 27.85s/it]2022-01-10 11:40:38,411 iteration 4149 : loss : 0.029671, loss_ce: 0.010066
2022-01-10 11:40:39,903 iteration 4150 : loss : 0.019806, loss_ce: 0.005175
2022-01-10 11:40:41,517 iteration 4151 : loss : 0.018634, loss_ce: 0.009111
2022-01-10 11:40:42,998 iteration 4152 : loss : 0.014919, loss_ce: 0.006002
2022-01-10 11:40:44,610 iteration 4153 : loss : 0.024266, loss_ce: 0.006611
2022-01-10 11:40:46,205 iteration 4154 : loss : 0.025101, loss_ce: 0.010269
2022-01-10 11:40:47,845 iteration 4155 : loss : 0.025643, loss_ce: 0.009514
2022-01-10 11:40:49,329 iteration 4156 : loss : 0.013584, loss_ce: 0.005643
2022-01-10 11:40:50,927 iteration 4157 : loss : 0.021048, loss_ce: 0.007399
2022-01-10 11:40:52,508 iteration 4158 : loss : 0.020555, loss_ce: 0.008099
2022-01-10 11:40:54,111 iteration 4159 : loss : 0.016341, loss_ce: 0.007105
2022-01-10 11:40:55,693 iteration 4160 : loss : 0.024073, loss_ce: 0.011428
2022-01-10 11:40:57,275 iteration 4161 : loss : 0.017644, loss_ce: 0.007745
2022-01-10 11:40:58,819 iteration 4162 : loss : 0.016106, loss_ce: 0.005201
2022-01-10 11:41:00,441 iteration 4163 : loss : 0.029145, loss_ce: 0.008510
2022-01-10 11:41:02,056 iteration 4164 : loss : 0.020092, loss_ce: 0.008658
2022-01-10 11:41:02,056 Training Data Eval:
2022-01-10 11:41:10,004   Average segmentation loss on training set: 0.0127
2022-01-10 11:41:10,004 Validation Data Eval:
2022-01-10 11:41:12,749   Average segmentation loss on validation set: 0.0630
2022-01-10 11:41:14,329 iteration 4165 : loss : 0.022180, loss_ce: 0.008467
 61%|████████████████▌          | 245/400 [1:59:53<1:19:33, 30.80s/it]2022-01-10 11:41:15,902 iteration 4166 : loss : 0.015544, loss_ce: 0.006242
2022-01-10 11:41:17,505 iteration 4167 : loss : 0.027181, loss_ce: 0.007581
2022-01-10 11:41:19,117 iteration 4168 : loss : 0.018748, loss_ce: 0.007699
2022-01-10 11:41:20,717 iteration 4169 : loss : 0.024146, loss_ce: 0.009227
2022-01-10 11:41:22,337 iteration 4170 : loss : 0.030309, loss_ce: 0.013925
2022-01-10 11:41:23,874 iteration 4171 : loss : 0.026269, loss_ce: 0.016788
2022-01-10 11:41:25,437 iteration 4172 : loss : 0.021089, loss_ce: 0.008736
2022-01-10 11:41:26,923 iteration 4173 : loss : 0.015557, loss_ce: 0.005098
2022-01-10 11:41:28,543 iteration 4174 : loss : 0.027841, loss_ce: 0.010194
2022-01-10 11:41:30,130 iteration 4175 : loss : 0.017545, loss_ce: 0.005393
2022-01-10 11:41:31,783 iteration 4176 : loss : 0.036416, loss_ce: 0.012926
2022-01-10 11:41:33,293 iteration 4177 : loss : 0.017986, loss_ce: 0.009048
2022-01-10 11:41:34,857 iteration 4178 : loss : 0.014403, loss_ce: 0.005799
2022-01-10 11:41:36,475 iteration 4179 : loss : 0.023291, loss_ce: 0.008799
2022-01-10 11:41:38,124 iteration 4180 : loss : 0.021880, loss_ce: 0.007821
2022-01-10 11:41:39,729 iteration 4181 : loss : 0.029294, loss_ce: 0.010603
2022-01-10 11:41:41,363 iteration 4182 : loss : 0.022186, loss_ce: 0.008676
 62%|████████████████▌          | 246/400 [2:00:20<1:16:08, 29.67s/it]2022-01-10 11:41:42,980 iteration 4183 : loss : 0.028968, loss_ce: 0.007598
2022-01-10 11:41:44,541 iteration 4184 : loss : 0.028755, loss_ce: 0.008379
2022-01-10 11:41:46,145 iteration 4185 : loss : 0.025741, loss_ce: 0.008634
2022-01-10 11:41:47,700 iteration 4186 : loss : 0.019837, loss_ce: 0.007322
2022-01-10 11:41:49,225 iteration 4187 : loss : 0.017434, loss_ce: 0.007420
2022-01-10 11:41:50,782 iteration 4188 : loss : 0.019146, loss_ce: 0.007368
2022-01-10 11:41:52,274 iteration 4189 : loss : 0.018942, loss_ce: 0.006489
2022-01-10 11:41:53,940 iteration 4190 : loss : 0.027699, loss_ce: 0.012626
2022-01-10 11:41:55,573 iteration 4191 : loss : 0.018380, loss_ce: 0.004832
2022-01-10 11:41:57,075 iteration 4192 : loss : 0.016807, loss_ce: 0.008521
2022-01-10 11:41:58,563 iteration 4193 : loss : 0.014832, loss_ce: 0.004615
2022-01-10 11:42:00,133 iteration 4194 : loss : 0.024517, loss_ce: 0.008587
2022-01-10 11:42:01,655 iteration 4195 : loss : 0.018412, loss_ce: 0.008540
2022-01-10 11:42:03,184 iteration 4196 : loss : 0.015168, loss_ce: 0.005797
2022-01-10 11:42:04,705 iteration 4197 : loss : 0.016636, loss_ce: 0.006782
2022-01-10 11:42:06,204 iteration 4198 : loss : 0.017475, loss_ce: 0.004778
2022-01-10 11:42:07,810 iteration 4199 : loss : 0.019600, loss_ce: 0.007857
 62%|████████████████▋          | 247/400 [2:00:46<1:13:10, 28.70s/it]2022-01-10 11:42:09,424 iteration 4200 : loss : 0.019370, loss_ce: 0.006763
2022-01-10 11:42:11,030 iteration 4201 : loss : 0.023102, loss_ce: 0.011479
2022-01-10 11:42:12,669 iteration 4202 : loss : 0.017897, loss_ce: 0.007375
2022-01-10 11:42:14,309 iteration 4203 : loss : 0.023448, loss_ce: 0.009356
2022-01-10 11:42:15,915 iteration 4204 : loss : 0.019209, loss_ce: 0.008190
2022-01-10 11:42:17,339 iteration 4205 : loss : 0.011223, loss_ce: 0.003879
2022-01-10 11:42:18,870 iteration 4206 : loss : 0.014125, loss_ce: 0.005181
2022-01-10 11:42:20,396 iteration 4207 : loss : 0.014773, loss_ce: 0.007714
2022-01-10 11:42:22,002 iteration 4208 : loss : 0.022369, loss_ce: 0.007959
2022-01-10 11:42:23,550 iteration 4209 : loss : 0.019274, loss_ce: 0.006022
2022-01-10 11:42:25,088 iteration 4210 : loss : 0.012995, loss_ce: 0.004582
2022-01-10 11:42:26,682 iteration 4211 : loss : 0.018493, loss_ce: 0.007389
2022-01-10 11:42:28,213 iteration 4212 : loss : 0.014797, loss_ce: 0.006532
2022-01-10 11:42:29,852 iteration 4213 : loss : 0.016054, loss_ce: 0.004861
2022-01-10 11:42:31,486 iteration 4214 : loss : 0.022486, loss_ce: 0.010277
2022-01-10 11:42:33,099 iteration 4215 : loss : 0.030233, loss_ce: 0.007294
2022-01-10 11:42:34,676 iteration 4216 : loss : 0.022892, loss_ce: 0.008636
 62%|████████████████▋          | 248/400 [2:01:13<1:11:18, 28.15s/it]2022-01-10 11:42:36,399 iteration 4217 : loss : 0.031137, loss_ce: 0.007963
2022-01-10 11:42:38,064 iteration 4218 : loss : 0.026305, loss_ce: 0.013574
2022-01-10 11:42:39,673 iteration 4219 : loss : 0.016548, loss_ce: 0.006560
2022-01-10 11:42:41,243 iteration 4220 : loss : 0.017964, loss_ce: 0.005840
2022-01-10 11:42:42,862 iteration 4221 : loss : 0.026209, loss_ce: 0.009441
2022-01-10 11:42:44,481 iteration 4222 : loss : 0.022016, loss_ce: 0.009835
2022-01-10 11:42:46,188 iteration 4223 : loss : 0.030521, loss_ce: 0.008912
2022-01-10 11:42:47,771 iteration 4224 : loss : 0.027096, loss_ce: 0.009003
2022-01-10 11:42:49,405 iteration 4225 : loss : 0.016704, loss_ce: 0.007642
2022-01-10 11:42:51,053 iteration 4226 : loss : 0.028356, loss_ce: 0.012013
2022-01-10 11:42:52,601 iteration 4227 : loss : 0.029755, loss_ce: 0.009359
2022-01-10 11:42:54,209 iteration 4228 : loss : 0.022017, loss_ce: 0.010766
2022-01-10 11:42:55,743 iteration 4229 : loss : 0.019249, loss_ce: 0.007795
2022-01-10 11:42:57,365 iteration 4230 : loss : 0.026499, loss_ce: 0.010152
2022-01-10 11:42:58,982 iteration 4231 : loss : 0.019658, loss_ce: 0.007110
2022-01-10 11:43:00,614 iteration 4232 : loss : 0.028960, loss_ce: 0.012334
2022-01-10 11:43:02,137 iteration 4233 : loss : 0.018577, loss_ce: 0.006684
 62%|████████████████▊          | 249/400 [2:01:41<1:10:19, 27.94s/it]2022-01-10 11:43:03,831 iteration 4234 : loss : 0.024528, loss_ce: 0.009919
2022-01-10 11:43:05,358 iteration 4235 : loss : 0.016353, loss_ce: 0.006199
2022-01-10 11:43:06,997 iteration 4236 : loss : 0.022270, loss_ce: 0.010858
2022-01-10 11:43:08,568 iteration 4237 : loss : 0.028118, loss_ce: 0.014703
2022-01-10 11:43:10,224 iteration 4238 : loss : 0.028636, loss_ce: 0.008916
2022-01-10 11:43:11,733 iteration 4239 : loss : 0.022358, loss_ce: 0.007183
2022-01-10 11:43:13,390 iteration 4240 : loss : 0.028398, loss_ce: 0.011034
2022-01-10 11:43:14,963 iteration 4241 : loss : 0.019406, loss_ce: 0.007322
2022-01-10 11:43:16,496 iteration 4242 : loss : 0.026794, loss_ce: 0.007769
2022-01-10 11:43:18,064 iteration 4243 : loss : 0.027324, loss_ce: 0.011218
2022-01-10 11:43:19,695 iteration 4244 : loss : 0.021513, loss_ce: 0.007432
2022-01-10 11:43:21,283 iteration 4245 : loss : 0.044360, loss_ce: 0.013924
2022-01-10 11:43:22,849 iteration 4246 : loss : 0.019929, loss_ce: 0.010183
2022-01-10 11:43:24,401 iteration 4247 : loss : 0.020835, loss_ce: 0.004415
2022-01-10 11:43:26,013 iteration 4248 : loss : 0.032768, loss_ce: 0.011659
2022-01-10 11:43:27,601 iteration 4249 : loss : 0.022918, loss_ce: 0.011121
2022-01-10 11:43:27,601 Training Data Eval:
2022-01-10 11:43:35,546   Average segmentation loss on training set: 0.0140
2022-01-10 11:43:35,547 Validation Data Eval:
2022-01-10 11:43:38,293   Average segmentation loss on validation set: 0.0905
2022-01-10 11:43:39,908 iteration 4250 : loss : 0.020021, loss_ce: 0.006798
 62%|████████████████▉          | 250/400 [2:02:18<1:17:13, 30.89s/it]2022-01-10 11:43:41,589 iteration 4251 : loss : 0.020547, loss_ce: 0.007723
2022-01-10 11:43:43,216 iteration 4252 : loss : 0.021511, loss_ce: 0.008950
2022-01-10 11:43:44,831 iteration 4253 : loss : 0.026778, loss_ce: 0.010335
2022-01-10 11:43:46,511 iteration 4254 : loss : 0.022517, loss_ce: 0.009867
2022-01-10 11:43:48,003 iteration 4255 : loss : 0.014628, loss_ce: 0.005490
2022-01-10 11:43:49,576 iteration 4256 : loss : 0.020213, loss_ce: 0.006153
2022-01-10 11:43:51,234 iteration 4257 : loss : 0.028460, loss_ce: 0.011912
2022-01-10 11:43:52,787 iteration 4258 : loss : 0.017279, loss_ce: 0.006341
2022-01-10 11:43:54,376 iteration 4259 : loss : 0.034989, loss_ce: 0.014053
2022-01-10 11:43:55,969 iteration 4260 : loss : 0.021982, loss_ce: 0.008060
2022-01-10 11:43:57,571 iteration 4261 : loss : 0.019266, loss_ce: 0.008010
2022-01-10 11:43:59,260 iteration 4262 : loss : 0.034433, loss_ce: 0.017522
2022-01-10 11:44:00,852 iteration 4263 : loss : 0.016056, loss_ce: 0.006393
2022-01-10 11:44:02,376 iteration 4264 : loss : 0.030624, loss_ce: 0.009982
2022-01-10 11:44:03,943 iteration 4265 : loss : 0.024389, loss_ce: 0.008298
2022-01-10 11:44:05,524 iteration 4266 : loss : 0.020068, loss_ce: 0.008014
2022-01-10 11:44:07,111 iteration 4267 : loss : 0.017907, loss_ce: 0.008524
 63%|████████████████▉          | 251/400 [2:02:46<1:13:57, 29.78s/it]2022-01-10 11:44:08,757 iteration 4268 : loss : 0.025592, loss_ce: 0.012134
2022-01-10 11:44:10,285 iteration 4269 : loss : 0.022884, loss_ce: 0.006845
2022-01-10 11:44:11,830 iteration 4270 : loss : 0.036835, loss_ce: 0.008613
2022-01-10 11:44:13,357 iteration 4271 : loss : 0.020435, loss_ce: 0.010474
2022-01-10 11:44:14,977 iteration 4272 : loss : 0.017501, loss_ce: 0.006118
2022-01-10 11:44:16,576 iteration 4273 : loss : 0.018509, loss_ce: 0.007841
2022-01-10 11:44:18,173 iteration 4274 : loss : 0.028811, loss_ce: 0.014697
2022-01-10 11:44:19,622 iteration 4275 : loss : 0.018063, loss_ce: 0.007627
2022-01-10 11:44:21,145 iteration 4276 : loss : 0.030866, loss_ce: 0.011095
2022-01-10 11:44:22,679 iteration 4277 : loss : 0.014581, loss_ce: 0.005933
2022-01-10 11:44:24,227 iteration 4278 : loss : 0.023114, loss_ce: 0.006396
2022-01-10 11:44:25,727 iteration 4279 : loss : 0.020921, loss_ce: 0.008056
2022-01-10 11:44:27,244 iteration 4280 : loss : 0.020800, loss_ce: 0.006097
2022-01-10 11:44:28,942 iteration 4281 : loss : 0.041987, loss_ce: 0.021026
2022-01-10 11:44:30,530 iteration 4282 : loss : 0.018379, loss_ce: 0.007762
2022-01-10 11:44:32,006 iteration 4283 : loss : 0.017660, loss_ce: 0.007848
2022-01-10 11:44:33,560 iteration 4284 : loss : 0.019515, loss_ce: 0.009135
 63%|█████████████████          | 252/400 [2:03:12<1:10:59, 28.78s/it]2022-01-10 11:44:35,240 iteration 4285 : loss : 0.026681, loss_ce: 0.011731
2022-01-10 11:44:36,857 iteration 4286 : loss : 0.018947, loss_ce: 0.005750
2022-01-10 11:44:38,494 iteration 4287 : loss : 0.029261, loss_ce: 0.011913
2022-01-10 11:44:40,072 iteration 4288 : loss : 0.023278, loss_ce: 0.007757
2022-01-10 11:44:41,677 iteration 4289 : loss : 0.022824, loss_ce: 0.007290
2022-01-10 11:44:43,311 iteration 4290 : loss : 0.031886, loss_ce: 0.012595
2022-01-10 11:44:44,805 iteration 4291 : loss : 0.020061, loss_ce: 0.006977
2022-01-10 11:44:46,429 iteration 4292 : loss : 0.020122, loss_ce: 0.007977
2022-01-10 11:44:48,033 iteration 4293 : loss : 0.030817, loss_ce: 0.015983
2022-01-10 11:44:49,616 iteration 4294 : loss : 0.025347, loss_ce: 0.012961
2022-01-10 11:44:51,243 iteration 4295 : loss : 0.026097, loss_ce: 0.012147
2022-01-10 11:44:52,795 iteration 4296 : loss : 0.015217, loss_ce: 0.006326
2022-01-10 11:44:54,406 iteration 4297 : loss : 0.020241, loss_ce: 0.006761
2022-01-10 11:44:55,942 iteration 4298 : loss : 0.019603, loss_ce: 0.006345
2022-01-10 11:44:57,599 iteration 4299 : loss : 0.026579, loss_ce: 0.011588
2022-01-10 11:44:59,227 iteration 4300 : loss : 0.024468, loss_ce: 0.007218
2022-01-10 11:45:00,795 iteration 4301 : loss : 0.018262, loss_ce: 0.007133
 63%|█████████████████          | 253/400 [2:03:39<1:09:22, 28.32s/it]2022-01-10 11:45:02,509 iteration 4302 : loss : 0.016965, loss_ce: 0.007308
2022-01-10 11:45:04,157 iteration 4303 : loss : 0.020284, loss_ce: 0.006901
2022-01-10 11:45:05,675 iteration 4304 : loss : 0.015478, loss_ce: 0.005988
2022-01-10 11:45:07,291 iteration 4305 : loss : 0.023102, loss_ce: 0.006339
2022-01-10 11:45:08,841 iteration 4306 : loss : 0.023198, loss_ce: 0.007929
2022-01-10 11:45:10,394 iteration 4307 : loss : 0.019444, loss_ce: 0.007783
2022-01-10 11:45:12,026 iteration 4308 : loss : 0.030209, loss_ce: 0.011207
2022-01-10 11:45:13,569 iteration 4309 : loss : 0.018782, loss_ce: 0.006342
2022-01-10 11:45:15,177 iteration 4310 : loss : 0.026238, loss_ce: 0.013001
2022-01-10 11:45:16,766 iteration 4311 : loss : 0.018718, loss_ce: 0.009076
2022-01-10 11:45:18,456 iteration 4312 : loss : 0.025896, loss_ce: 0.010195
2022-01-10 11:45:20,045 iteration 4313 : loss : 0.020604, loss_ce: 0.006997
2022-01-10 11:45:21,624 iteration 4314 : loss : 0.027101, loss_ce: 0.008215
2022-01-10 11:45:23,174 iteration 4315 : loss : 0.017051, loss_ce: 0.006232
2022-01-10 11:45:24,810 iteration 4316 : loss : 0.023147, loss_ce: 0.009707
2022-01-10 11:45:26,374 iteration 4317 : loss : 0.024131, loss_ce: 0.008710
2022-01-10 11:45:27,935 iteration 4318 : loss : 0.018720, loss_ce: 0.009320
 64%|█████████████████▏         | 254/400 [2:04:06<1:08:02, 27.97s/it]2022-01-10 11:45:29,638 iteration 4319 : loss : 0.023527, loss_ce: 0.007254
2022-01-10 11:45:31,279 iteration 4320 : loss : 0.020470, loss_ce: 0.008489
2022-01-10 11:45:32,936 iteration 4321 : loss : 0.021729, loss_ce: 0.010423
2022-01-10 11:45:34,543 iteration 4322 : loss : 0.030080, loss_ce: 0.008846
2022-01-10 11:45:36,094 iteration 4323 : loss : 0.015954, loss_ce: 0.004905
2022-01-10 11:45:37,706 iteration 4324 : loss : 0.024737, loss_ce: 0.008665
2022-01-10 11:45:39,411 iteration 4325 : loss : 0.031877, loss_ce: 0.006679
2022-01-10 11:45:41,009 iteration 4326 : loss : 0.024537, loss_ce: 0.014157
2022-01-10 11:45:42,561 iteration 4327 : loss : 0.017752, loss_ce: 0.006924
2022-01-10 11:45:44,123 iteration 4328 : loss : 0.025063, loss_ce: 0.009487
2022-01-10 11:45:45,744 iteration 4329 : loss : 0.027547, loss_ce: 0.011314
2022-01-10 11:45:47,283 iteration 4330 : loss : 0.015011, loss_ce: 0.005437
2022-01-10 11:45:49,009 iteration 4331 : loss : 0.028889, loss_ce: 0.008425
2022-01-10 11:45:50,656 iteration 4332 : loss : 0.027079, loss_ce: 0.012318
2022-01-10 11:45:52,173 iteration 4333 : loss : 0.025478, loss_ce: 0.008639
2022-01-10 11:45:53,708 iteration 4334 : loss : 0.017533, loss_ce: 0.006613
2022-01-10 11:45:53,708 Training Data Eval:
2022-01-10 11:46:01,693   Average segmentation loss on training set: 0.0130
2022-01-10 11:46:01,693 Validation Data Eval:
2022-01-10 11:46:04,443   Average segmentation loss on validation set: 0.0656
2022-01-10 11:46:06,096 iteration 4335 : loss : 0.019553, loss_ce: 0.010093
 64%|█████████████████▏         | 255/400 [2:04:45<1:14:58, 31.02s/it]2022-01-10 11:46:07,737 iteration 4336 : loss : 0.020202, loss_ce: 0.008091
2022-01-10 11:46:09,357 iteration 4337 : loss : 0.030376, loss_ce: 0.011308
2022-01-10 11:46:11,044 iteration 4338 : loss : 0.026687, loss_ce: 0.010634
2022-01-10 11:46:12,603 iteration 4339 : loss : 0.027411, loss_ce: 0.009165
2022-01-10 11:46:14,205 iteration 4340 : loss : 0.021873, loss_ce: 0.008728
2022-01-10 11:46:15,829 iteration 4341 : loss : 0.027212, loss_ce: 0.011490
2022-01-10 11:46:17,403 iteration 4342 : loss : 0.027538, loss_ce: 0.008343
2022-01-10 11:46:19,009 iteration 4343 : loss : 0.015638, loss_ce: 0.004503
2022-01-10 11:46:20,555 iteration 4344 : loss : 0.020030, loss_ce: 0.006937
2022-01-10 11:46:22,139 iteration 4345 : loss : 0.023929, loss_ce: 0.007534
2022-01-10 11:46:23,686 iteration 4346 : loss : 0.016577, loss_ce: 0.007551
2022-01-10 11:46:25,318 iteration 4347 : loss : 0.023737, loss_ce: 0.007305
2022-01-10 11:46:26,920 iteration 4348 : loss : 0.020313, loss_ce: 0.009894
2022-01-10 11:46:28,507 iteration 4349 : loss : 0.025980, loss_ce: 0.010921
2022-01-10 11:46:30,050 iteration 4350 : loss : 0.016791, loss_ce: 0.006745
2022-01-10 11:46:31,565 iteration 4351 : loss : 0.016605, loss_ce: 0.007744
2022-01-10 11:46:33,099 iteration 4352 : loss : 0.013758, loss_ce: 0.005383
 64%|█████████████████▎         | 256/400 [2:05:12<1:11:34, 29.82s/it]2022-01-10 11:46:34,742 iteration 4353 : loss : 0.018358, loss_ce: 0.007564
2022-01-10 11:46:36,387 iteration 4354 : loss : 0.021500, loss_ce: 0.006821
2022-01-10 11:46:37,918 iteration 4355 : loss : 0.018131, loss_ce: 0.005811
2022-01-10 11:46:39,538 iteration 4356 : loss : 0.024464, loss_ce: 0.010925
2022-01-10 11:46:41,122 iteration 4357 : loss : 0.022813, loss_ce: 0.008680
2022-01-10 11:46:42,671 iteration 4358 : loss : 0.016111, loss_ce: 0.007098
2022-01-10 11:46:44,236 iteration 4359 : loss : 0.013782, loss_ce: 0.004832
2022-01-10 11:46:45,802 iteration 4360 : loss : 0.020121, loss_ce: 0.006456
2022-01-10 11:46:47,432 iteration 4361 : loss : 0.021219, loss_ce: 0.008904
2022-01-10 11:46:48,974 iteration 4362 : loss : 0.017520, loss_ce: 0.005359
2022-01-10 11:46:50,465 iteration 4363 : loss : 0.017461, loss_ce: 0.007984
2022-01-10 11:46:52,093 iteration 4364 : loss : 0.023534, loss_ce: 0.008482
2022-01-10 11:46:53,655 iteration 4365 : loss : 0.022916, loss_ce: 0.009065
2022-01-10 11:46:55,113 iteration 4366 : loss : 0.014663, loss_ce: 0.006102
2022-01-10 11:46:56,664 iteration 4367 : loss : 0.029621, loss_ce: 0.010807
2022-01-10 11:46:58,313 iteration 4368 : loss : 0.023909, loss_ce: 0.008988
2022-01-10 11:46:59,827 iteration 4369 : loss : 0.024707, loss_ce: 0.005711
 64%|█████████████████▎         | 257/400 [2:05:38<1:08:51, 28.89s/it]2022-01-10 11:47:01,474 iteration 4370 : loss : 0.023667, loss_ce: 0.005839
2022-01-10 11:47:03,045 iteration 4371 : loss : 0.027849, loss_ce: 0.014001
2022-01-10 11:47:04,544 iteration 4372 : loss : 0.014559, loss_ce: 0.006255
2022-01-10 11:47:06,156 iteration 4373 : loss : 0.020127, loss_ce: 0.006463
2022-01-10 11:47:07,722 iteration 4374 : loss : 0.017083, loss_ce: 0.007653
2022-01-10 11:47:09,220 iteration 4375 : loss : 0.018452, loss_ce: 0.007201
2022-01-10 11:47:10,756 iteration 4376 : loss : 0.023065, loss_ce: 0.007413
2022-01-10 11:47:12,409 iteration 4377 : loss : 0.041934, loss_ce: 0.017340
2022-01-10 11:47:14,017 iteration 4378 : loss : 0.020347, loss_ce: 0.007204
2022-01-10 11:47:15,704 iteration 4379 : loss : 0.023480, loss_ce: 0.010827
2022-01-10 11:47:17,302 iteration 4380 : loss : 0.020666, loss_ce: 0.008322
2022-01-10 11:47:18,919 iteration 4381 : loss : 0.028093, loss_ce: 0.010618
2022-01-10 11:47:20,484 iteration 4382 : loss : 0.022992, loss_ce: 0.008455
2022-01-10 11:47:22,050 iteration 4383 : loss : 0.022741, loss_ce: 0.005595
2022-01-10 11:47:23,646 iteration 4384 : loss : 0.019716, loss_ce: 0.007390
2022-01-10 11:47:25,125 iteration 4385 : loss : 0.016848, loss_ce: 0.007332
2022-01-10 11:47:26,712 iteration 4386 : loss : 0.020592, loss_ce: 0.008745
 64%|█████████████████▍         | 258/400 [2:06:05<1:06:57, 28.29s/it]2022-01-10 11:47:28,337 iteration 4387 : loss : 0.015827, loss_ce: 0.005853
2022-01-10 11:47:29,942 iteration 4388 : loss : 0.020664, loss_ce: 0.009056
2022-01-10 11:47:31,449 iteration 4389 : loss : 0.014498, loss_ce: 0.006427
2022-01-10 11:47:32,951 iteration 4390 : loss : 0.017692, loss_ce: 0.006236
2022-01-10 11:47:34,516 iteration 4391 : loss : 0.026387, loss_ce: 0.011067
2022-01-10 11:47:36,115 iteration 4392 : loss : 0.025538, loss_ce: 0.010787
2022-01-10 11:47:37,614 iteration 4393 : loss : 0.016100, loss_ce: 0.005633
2022-01-10 11:47:39,256 iteration 4394 : loss : 0.017815, loss_ce: 0.008784
2022-01-10 11:47:40,865 iteration 4395 : loss : 0.018408, loss_ce: 0.006900
2022-01-10 11:47:42,502 iteration 4396 : loss : 0.018610, loss_ce: 0.007061
2022-01-10 11:47:44,062 iteration 4397 : loss : 0.024649, loss_ce: 0.008196
2022-01-10 11:47:45,602 iteration 4398 : loss : 0.041912, loss_ce: 0.010386
2022-01-10 11:47:47,171 iteration 4399 : loss : 0.019833, loss_ce: 0.006155
2022-01-10 11:47:48,698 iteration 4400 : loss : 0.015372, loss_ce: 0.006387
2022-01-10 11:47:50,242 iteration 4401 : loss : 0.015862, loss_ce: 0.007237
2022-01-10 11:47:51,747 iteration 4402 : loss : 0.015571, loss_ce: 0.004956
2022-01-10 11:47:53,324 iteration 4403 : loss : 0.025083, loss_ce: 0.007982
 65%|█████████████████▍         | 259/400 [2:06:32<1:05:17, 27.78s/it]2022-01-10 11:47:54,894 iteration 4404 : loss : 0.013184, loss_ce: 0.006233
2022-01-10 11:47:56,496 iteration 4405 : loss : 0.032893, loss_ce: 0.010015
2022-01-10 11:47:58,076 iteration 4406 : loss : 0.026198, loss_ce: 0.009843
2022-01-10 11:47:59,597 iteration 4407 : loss : 0.015157, loss_ce: 0.005455
2022-01-10 11:48:01,076 iteration 4408 : loss : 0.023737, loss_ce: 0.006422
2022-01-10 11:48:02,641 iteration 4409 : loss : 0.014877, loss_ce: 0.006146
2022-01-10 11:48:04,213 iteration 4410 : loss : 0.023372, loss_ce: 0.007144
2022-01-10 11:48:05,835 iteration 4411 : loss : 0.016190, loss_ce: 0.007182
2022-01-10 11:48:07,347 iteration 4412 : loss : 0.027789, loss_ce: 0.012887
2022-01-10 11:48:08,977 iteration 4413 : loss : 0.020293, loss_ce: 0.005877
2022-01-10 11:48:10,454 iteration 4414 : loss : 0.015317, loss_ce: 0.005956
2022-01-10 11:48:12,033 iteration 4415 : loss : 0.021856, loss_ce: 0.009548
2022-01-10 11:48:13,577 iteration 4416 : loss : 0.029038, loss_ce: 0.006311
2022-01-10 11:48:15,136 iteration 4417 : loss : 0.022212, loss_ce: 0.008080
2022-01-10 11:48:16,716 iteration 4418 : loss : 0.020491, loss_ce: 0.009222
2022-01-10 11:48:18,320 iteration 4419 : loss : 0.022440, loss_ce: 0.007627
2022-01-10 11:48:18,320 Training Data Eval:
2022-01-10 11:48:26,266   Average segmentation loss on training set: 0.0118
2022-01-10 11:48:26,266 Validation Data Eval:
2022-01-10 11:48:29,003   Average segmentation loss on validation set: 0.0706
2022-01-10 11:48:30,521 iteration 4420 : loss : 0.013442, loss_ce: 0.005692
 65%|█████████████████▌         | 260/400 [2:07:09<1:11:25, 30.61s/it]2022-01-10 11:48:32,165 iteration 4421 : loss : 0.017686, loss_ce: 0.005714
2022-01-10 11:48:33,799 iteration 4422 : loss : 0.028572, loss_ce: 0.010728
2022-01-10 11:48:35,485 iteration 4423 : loss : 0.021139, loss_ce: 0.008709
2022-01-10 11:48:37,119 iteration 4424 : loss : 0.024178, loss_ce: 0.008270
2022-01-10 11:48:38,627 iteration 4425 : loss : 0.016346, loss_ce: 0.006032
2022-01-10 11:48:40,248 iteration 4426 : loss : 0.020303, loss_ce: 0.009050
2022-01-10 11:48:41,866 iteration 4427 : loss : 0.041414, loss_ce: 0.009766
2022-01-10 11:48:43,490 iteration 4428 : loss : 0.019597, loss_ce: 0.008523
2022-01-10 11:48:45,138 iteration 4429 : loss : 0.027542, loss_ce: 0.010329
2022-01-10 11:48:46,707 iteration 4430 : loss : 0.020677, loss_ce: 0.007018
2022-01-10 11:48:48,246 iteration 4431 : loss : 0.016842, loss_ce: 0.006476
2022-01-10 11:48:49,805 iteration 4432 : loss : 0.016294, loss_ce: 0.007223
2022-01-10 11:48:51,354 iteration 4433 : loss : 0.019119, loss_ce: 0.007305
2022-01-10 11:48:52,906 iteration 4434 : loss : 0.017751, loss_ce: 0.006792
2022-01-10 11:48:54,516 iteration 4435 : loss : 0.027244, loss_ce: 0.009975
2022-01-10 11:48:56,214 iteration 4436 : loss : 0.038412, loss_ce: 0.018325
2022-01-10 11:48:57,738 iteration 4437 : loss : 0.019856, loss_ce: 0.007743
 65%|█████████████████▌         | 261/400 [2:07:36<1:08:32, 29.59s/it]2022-01-10 11:48:59,424 iteration 4438 : loss : 0.032286, loss_ce: 0.012828
2022-01-10 11:49:00,969 iteration 4439 : loss : 0.021598, loss_ce: 0.007286
2022-01-10 11:49:02,588 iteration 4440 : loss : 0.032929, loss_ce: 0.011401
2022-01-10 11:49:04,214 iteration 4441 : loss : 0.024393, loss_ce: 0.010568
2022-01-10 11:49:05,745 iteration 4442 : loss : 0.015735, loss_ce: 0.007116
2022-01-10 11:49:07,339 iteration 4443 : loss : 0.031347, loss_ce: 0.012000
2022-01-10 11:49:08,882 iteration 4444 : loss : 0.030225, loss_ce: 0.011181
2022-01-10 11:49:10,468 iteration 4445 : loss : 0.019369, loss_ce: 0.006850
2022-01-10 11:49:12,060 iteration 4446 : loss : 0.022210, loss_ce: 0.008624
2022-01-10 11:49:13,651 iteration 4447 : loss : 0.026071, loss_ce: 0.009822
2022-01-10 11:49:15,220 iteration 4448 : loss : 0.032421, loss_ce: 0.010355
2022-01-10 11:49:16,817 iteration 4449 : loss : 0.021960, loss_ce: 0.010729
2022-01-10 11:49:18,450 iteration 4450 : loss : 0.022432, loss_ce: 0.007803
2022-01-10 11:49:19,959 iteration 4451 : loss : 0.018888, loss_ce: 0.006055
2022-01-10 11:49:21,538 iteration 4452 : loss : 0.021476, loss_ce: 0.008415
2022-01-10 11:49:23,202 iteration 4453 : loss : 0.030004, loss_ce: 0.012510
2022-01-10 11:49:24,797 iteration 4454 : loss : 0.021149, loss_ce: 0.010555
 66%|█████████████████▋         | 262/400 [2:08:03<1:06:18, 28.83s/it]2022-01-10 11:49:26,498 iteration 4455 : loss : 0.020955, loss_ce: 0.008656
2022-01-10 11:49:28,073 iteration 4456 : loss : 0.025030, loss_ce: 0.011920
2022-01-10 11:49:29,558 iteration 4457 : loss : 0.013962, loss_ce: 0.006784
2022-01-10 11:49:31,157 iteration 4458 : loss : 0.022726, loss_ce: 0.010088
2022-01-10 11:49:32,663 iteration 4459 : loss : 0.015583, loss_ce: 0.006506
2022-01-10 11:49:34,243 iteration 4460 : loss : 0.017833, loss_ce: 0.006574
2022-01-10 11:49:35,756 iteration 4461 : loss : 0.019323, loss_ce: 0.008660
2022-01-10 11:49:37,293 iteration 4462 : loss : 0.016615, loss_ce: 0.006009
2022-01-10 11:49:38,813 iteration 4463 : loss : 0.016796, loss_ce: 0.005783
2022-01-10 11:49:40,328 iteration 4464 : loss : 0.019625, loss_ce: 0.007660
2022-01-10 11:49:41,940 iteration 4465 : loss : 0.022605, loss_ce: 0.009702
2022-01-10 11:49:43,538 iteration 4466 : loss : 0.022568, loss_ce: 0.007223
2022-01-10 11:49:45,116 iteration 4467 : loss : 0.016124, loss_ce: 0.005858
2022-01-10 11:49:46,674 iteration 4468 : loss : 0.015902, loss_ce: 0.005034
2022-01-10 11:49:48,186 iteration 4469 : loss : 0.020792, loss_ce: 0.005808
2022-01-10 11:49:49,784 iteration 4470 : loss : 0.024694, loss_ce: 0.008917
2022-01-10 11:49:51,396 iteration 4471 : loss : 0.017810, loss_ce: 0.006326
 66%|█████████████████▊         | 263/400 [2:08:30<1:04:18, 28.16s/it]2022-01-10 11:49:52,987 iteration 4472 : loss : 0.018259, loss_ce: 0.008110
2022-01-10 11:49:54,540 iteration 4473 : loss : 0.016035, loss_ce: 0.006247
2022-01-10 11:49:56,138 iteration 4474 : loss : 0.027884, loss_ce: 0.012646
2022-01-10 11:49:57,677 iteration 4475 : loss : 0.018790, loss_ce: 0.008659
2022-01-10 11:49:59,147 iteration 4476 : loss : 0.015484, loss_ce: 0.005382
2022-01-10 11:50:00,724 iteration 4477 : loss : 0.021086, loss_ce: 0.006695
2022-01-10 11:50:02,298 iteration 4478 : loss : 0.026820, loss_ce: 0.009985
2022-01-10 11:50:03,881 iteration 4479 : loss : 0.020191, loss_ce: 0.005008
2022-01-10 11:50:05,409 iteration 4480 : loss : 0.016420, loss_ce: 0.006305
2022-01-10 11:50:06,918 iteration 4481 : loss : 0.013687, loss_ce: 0.005754
2022-01-10 11:50:08,553 iteration 4482 : loss : 0.019055, loss_ce: 0.006244
2022-01-10 11:50:10,202 iteration 4483 : loss : 0.021325, loss_ce: 0.008426
2022-01-10 11:50:11,840 iteration 4484 : loss : 0.024325, loss_ce: 0.008370
2022-01-10 11:50:13,375 iteration 4485 : loss : 0.013864, loss_ce: 0.004117
2022-01-10 11:50:15,051 iteration 4486 : loss : 0.025455, loss_ce: 0.010791
2022-01-10 11:50:16,587 iteration 4487 : loss : 0.040446, loss_ce: 0.025043
2022-01-10 11:50:18,149 iteration 4488 : loss : 0.016385, loss_ce: 0.006947
 66%|█████████████████▊         | 264/400 [2:08:57<1:02:52, 27.74s/it]2022-01-10 11:50:19,815 iteration 4489 : loss : 0.025047, loss_ce: 0.010482
2022-01-10 11:50:21,407 iteration 4490 : loss : 0.013591, loss_ce: 0.005756
2022-01-10 11:50:23,056 iteration 4491 : loss : 0.036169, loss_ce: 0.013072
2022-01-10 11:50:24,645 iteration 4492 : loss : 0.017788, loss_ce: 0.006520
2022-01-10 11:50:26,275 iteration 4493 : loss : 0.027510, loss_ce: 0.009730
2022-01-10 11:50:27,765 iteration 4494 : loss : 0.017419, loss_ce: 0.006819
2022-01-10 11:50:29,365 iteration 4495 : loss : 0.019471, loss_ce: 0.008885
2022-01-10 11:50:30,966 iteration 4496 : loss : 0.026824, loss_ce: 0.009214
2022-01-10 11:50:32,486 iteration 4497 : loss : 0.020438, loss_ce: 0.006376
2022-01-10 11:50:34,140 iteration 4498 : loss : 0.031391, loss_ce: 0.016133
2022-01-10 11:50:35,858 iteration 4499 : loss : 0.023697, loss_ce: 0.010521
2022-01-10 11:50:37,426 iteration 4500 : loss : 0.020733, loss_ce: 0.009906
2022-01-10 11:50:38,945 iteration 4501 : loss : 0.020153, loss_ce: 0.007809
2022-01-10 11:50:40,469 iteration 4502 : loss : 0.013951, loss_ce: 0.005065
2022-01-10 11:50:42,087 iteration 4503 : loss : 0.020417, loss_ce: 0.008873
2022-01-10 11:50:43,648 iteration 4504 : loss : 0.018261, loss_ce: 0.005822
2022-01-10 11:50:43,648 Training Data Eval:
2022-01-10 11:50:51,587   Average segmentation loss on training set: 0.0116
2022-01-10 11:50:51,587 Validation Data Eval:
2022-01-10 11:50:54,330   Average segmentation loss on validation set: 0.0748
2022-01-10 11:50:55,811 iteration 4505 : loss : 0.012891, loss_ce: 0.005401
 66%|█████████████████▉         | 265/400 [2:09:34<1:09:06, 30.72s/it]2022-01-10 11:50:57,497 iteration 4506 : loss : 0.027849, loss_ce: 0.008320
2022-01-10 11:50:59,026 iteration 4507 : loss : 0.013904, loss_ce: 0.004252
2022-01-10 11:51:00,656 iteration 4508 : loss : 0.030513, loss_ce: 0.007332
2022-01-10 11:51:02,244 iteration 4509 : loss : 0.026062, loss_ce: 0.007726
2022-01-10 11:51:03,828 iteration 4510 : loss : 0.015371, loss_ce: 0.005550
2022-01-10 11:51:05,364 iteration 4511 : loss : 0.018267, loss_ce: 0.006318
2022-01-10 11:51:06,924 iteration 4512 : loss : 0.012228, loss_ce: 0.005014
2022-01-10 11:51:08,520 iteration 4513 : loss : 0.017975, loss_ce: 0.008589
2022-01-10 11:51:10,017 iteration 4514 : loss : 0.015946, loss_ce: 0.006027
2022-01-10 11:51:11,567 iteration 4515 : loss : 0.026722, loss_ce: 0.008695
2022-01-10 11:51:13,247 iteration 4516 : loss : 0.021108, loss_ce: 0.008943
2022-01-10 11:51:14,817 iteration 4517 : loss : 0.015941, loss_ce: 0.005348
2022-01-10 11:51:16,413 iteration 4518 : loss : 0.031754, loss_ce: 0.009880
2022-01-10 11:51:17,997 iteration 4519 : loss : 0.022948, loss_ce: 0.010340
2022-01-10 11:51:19,526 iteration 4520 : loss : 0.018443, loss_ce: 0.006496
2022-01-10 11:51:21,120 iteration 4521 : loss : 0.020689, loss_ce: 0.006343
2022-01-10 11:51:22,637 iteration 4522 : loss : 0.019351, loss_ce: 0.009393
 66%|█████████████████▉         | 266/400 [2:10:01<1:05:59, 29.55s/it]2022-01-10 11:51:24,326 iteration 4523 : loss : 0.015043, loss_ce: 0.005633
2022-01-10 11:51:25,829 iteration 4524 : loss : 0.014311, loss_ce: 0.005690
2022-01-10 11:51:27,463 iteration 4525 : loss : 0.029441, loss_ce: 0.008052
2022-01-10 11:51:29,057 iteration 4526 : loss : 0.017511, loss_ce: 0.007113
2022-01-10 11:51:30,639 iteration 4527 : loss : 0.016599, loss_ce: 0.007302
2022-01-10 11:51:32,263 iteration 4528 : loss : 0.016587, loss_ce: 0.006119
2022-01-10 11:51:33,830 iteration 4529 : loss : 0.020447, loss_ce: 0.008361
2022-01-10 11:51:35,427 iteration 4530 : loss : 0.019914, loss_ce: 0.007820
2022-01-10 11:51:36,952 iteration 4531 : loss : 0.016876, loss_ce: 0.007218
2022-01-10 11:51:38,570 iteration 4532 : loss : 0.020464, loss_ce: 0.009187
2022-01-10 11:51:40,212 iteration 4533 : loss : 0.022069, loss_ce: 0.008183
2022-01-10 11:51:41,917 iteration 4534 : loss : 0.020309, loss_ce: 0.006927
2022-01-10 11:51:43,416 iteration 4535 : loss : 0.016173, loss_ce: 0.005153
2022-01-10 11:51:45,009 iteration 4536 : loss : 0.015744, loss_ce: 0.007686
2022-01-10 11:51:46,679 iteration 4537 : loss : 0.035695, loss_ce: 0.013304
2022-01-10 11:51:48,235 iteration 4538 : loss : 0.016773, loss_ce: 0.005761
2022-01-10 11:51:49,834 iteration 4539 : loss : 0.020706, loss_ce: 0.007735
 67%|██████████████████         | 267/400 [2:10:28<1:03:56, 28.84s/it]2022-01-10 11:51:51,523 iteration 4540 : loss : 0.019392, loss_ce: 0.005616
2022-01-10 11:51:53,029 iteration 4541 : loss : 0.013649, loss_ce: 0.005079
2022-01-10 11:51:54,602 iteration 4542 : loss : 0.015651, loss_ce: 0.004243
2022-01-10 11:51:56,087 iteration 4543 : loss : 0.018374, loss_ce: 0.007337
2022-01-10 11:51:57,686 iteration 4544 : loss : 0.029529, loss_ce: 0.018607
2022-01-10 11:51:59,340 iteration 4545 : loss : 0.029042, loss_ce: 0.013373
2022-01-10 11:52:00,950 iteration 4546 : loss : 0.022097, loss_ce: 0.008789
2022-01-10 11:52:02,540 iteration 4547 : loss : 0.018699, loss_ce: 0.008903
2022-01-10 11:52:04,119 iteration 4548 : loss : 0.023445, loss_ce: 0.010872
2022-01-10 11:52:05,783 iteration 4549 : loss : 0.022852, loss_ce: 0.009175
2022-01-10 11:52:07,428 iteration 4550 : loss : 0.027601, loss_ce: 0.009972
2022-01-10 11:52:09,069 iteration 4551 : loss : 0.021821, loss_ce: 0.005850
2022-01-10 11:52:10,558 iteration 4552 : loss : 0.013191, loss_ce: 0.004991
2022-01-10 11:52:12,081 iteration 4553 : loss : 0.020139, loss_ce: 0.007578
2022-01-10 11:52:13,640 iteration 4554 : loss : 0.017501, loss_ce: 0.006564
2022-01-10 11:52:15,257 iteration 4555 : loss : 0.032901, loss_ce: 0.011917
2022-01-10 11:52:16,833 iteration 4556 : loss : 0.020013, loss_ce: 0.009459
 67%|██████████████████         | 268/400 [2:10:55<1:02:14, 28.29s/it]2022-01-10 11:52:18,486 iteration 4557 : loss : 0.023779, loss_ce: 0.012762
2022-01-10 11:52:20,145 iteration 4558 : loss : 0.024205, loss_ce: 0.011509
2022-01-10 11:52:21,703 iteration 4559 : loss : 0.042495, loss_ce: 0.011438
2022-01-10 11:52:23,252 iteration 4560 : loss : 0.015254, loss_ce: 0.004112
2022-01-10 11:52:24,822 iteration 4561 : loss : 0.022687, loss_ce: 0.006423
2022-01-10 11:52:26,378 iteration 4562 : loss : 0.019396, loss_ce: 0.006715
2022-01-10 11:52:27,909 iteration 4563 : loss : 0.016646, loss_ce: 0.005370
2022-01-10 11:52:29,483 iteration 4564 : loss : 0.034826, loss_ce: 0.011869
2022-01-10 11:52:31,009 iteration 4565 : loss : 0.018459, loss_ce: 0.007971
2022-01-10 11:52:32,521 iteration 4566 : loss : 0.024203, loss_ce: 0.010593
2022-01-10 11:52:34,098 iteration 4567 : loss : 0.022656, loss_ce: 0.009742
2022-01-10 11:52:35,651 iteration 4568 : loss : 0.015558, loss_ce: 0.004690
2022-01-10 11:52:37,231 iteration 4569 : loss : 0.022011, loss_ce: 0.009259
2022-01-10 11:52:38,745 iteration 4570 : loss : 0.019153, loss_ce: 0.008830
2022-01-10 11:52:40,282 iteration 4571 : loss : 0.014580, loss_ce: 0.007670
2022-01-10 11:52:41,834 iteration 4572 : loss : 0.015645, loss_ce: 0.005495
2022-01-10 11:52:43,401 iteration 4573 : loss : 0.016813, loss_ce: 0.007454
 67%|██████████████████▏        | 269/400 [2:11:22<1:00:38, 27.77s/it]2022-01-10 11:52:45,030 iteration 4574 : loss : 0.017619, loss_ce: 0.005838
2022-01-10 11:52:46,545 iteration 4575 : loss : 0.014951, loss_ce: 0.005880
2022-01-10 11:52:48,133 iteration 4576 : loss : 0.015333, loss_ce: 0.007834
2022-01-10 11:52:49,760 iteration 4577 : loss : 0.031829, loss_ce: 0.013762
2022-01-10 11:52:51,343 iteration 4578 : loss : 0.016160, loss_ce: 0.006422
2022-01-10 11:52:52,898 iteration 4579 : loss : 0.018781, loss_ce: 0.008335
2022-01-10 11:52:54,451 iteration 4580 : loss : 0.019091, loss_ce: 0.009637
2022-01-10 11:52:56,101 iteration 4581 : loss : 0.021568, loss_ce: 0.006392
2022-01-10 11:52:57,682 iteration 4582 : loss : 0.016633, loss_ce: 0.007429
2022-01-10 11:52:59,268 iteration 4583 : loss : 0.015469, loss_ce: 0.005623
2022-01-10 11:53:00,811 iteration 4584 : loss : 0.020462, loss_ce: 0.007984
2022-01-10 11:53:02,407 iteration 4585 : loss : 0.020277, loss_ce: 0.006590
2022-01-10 11:53:03,942 iteration 4586 : loss : 0.015875, loss_ce: 0.006481
2022-01-10 11:53:05,606 iteration 4587 : loss : 0.024808, loss_ce: 0.008189
2022-01-10 11:53:07,151 iteration 4588 : loss : 0.031016, loss_ce: 0.006085
2022-01-10 11:53:08,710 iteration 4589 : loss : 0.016736, loss_ce: 0.007222
2022-01-10 11:53:08,710 Training Data Eval:
2022-01-10 11:53:16,646   Average segmentation loss on training set: 0.0120
2022-01-10 11:53:16,647 Validation Data Eval:
2022-01-10 11:53:19,388   Average segmentation loss on validation set: 0.0688
2022-01-10 11:53:21,091 iteration 4590 : loss : 0.026913, loss_ce: 0.010051
 68%|██████████████████▏        | 270/400 [2:12:00<1:06:36, 30.74s/it]2022-01-10 11:53:22,655 iteration 4591 : loss : 0.013571, loss_ce: 0.004067
2022-01-10 11:53:24,219 iteration 4592 : loss : 0.020538, loss_ce: 0.007405
2022-01-10 11:53:25,705 iteration 4593 : loss : 0.015612, loss_ce: 0.004860
2022-01-10 11:53:27,252 iteration 4594 : loss : 0.015381, loss_ce: 0.006605
2022-01-10 11:53:28,845 iteration 4595 : loss : 0.021558, loss_ce: 0.005258
2022-01-10 11:53:30,486 iteration 4596 : loss : 0.066964, loss_ce: 0.030063
2022-01-10 11:53:32,056 iteration 4597 : loss : 0.015636, loss_ce: 0.006360
2022-01-10 11:53:33,539 iteration 4598 : loss : 0.015345, loss_ce: 0.007212
2022-01-10 11:53:35,063 iteration 4599 : loss : 0.018440, loss_ce: 0.004849
2022-01-10 11:53:36,699 iteration 4600 : loss : 0.020308, loss_ce: 0.009924
2022-01-10 11:53:38,377 iteration 4601 : loss : 0.032745, loss_ce: 0.017709
2022-01-10 11:53:39,861 iteration 4602 : loss : 0.015065, loss_ce: 0.005522
2022-01-10 11:53:41,366 iteration 4603 : loss : 0.014421, loss_ce: 0.006302
2022-01-10 11:53:42,962 iteration 4604 : loss : 0.023686, loss_ce: 0.007958
2022-01-10 11:53:44,546 iteration 4605 : loss : 0.049434, loss_ce: 0.018714
2022-01-10 11:53:46,178 iteration 4606 : loss : 0.025261, loss_ce: 0.007426
2022-01-10 11:53:47,675 iteration 4607 : loss : 0.015065, loss_ce: 0.008023
 68%|██████████████████▎        | 271/400 [2:12:26<1:03:25, 29.50s/it]2022-01-10 11:53:49,333 iteration 4608 : loss : 0.018814, loss_ce: 0.006413
2022-01-10 11:53:50,954 iteration 4609 : loss : 0.020057, loss_ce: 0.009701
2022-01-10 11:53:52,552 iteration 4610 : loss : 0.029707, loss_ce: 0.007872
2022-01-10 11:53:54,124 iteration 4611 : loss : 0.019338, loss_ce: 0.008332
2022-01-10 11:53:55,621 iteration 4612 : loss : 0.014293, loss_ce: 0.005158
2022-01-10 11:53:57,205 iteration 4613 : loss : 0.020222, loss_ce: 0.008220
2022-01-10 11:53:58,804 iteration 4614 : loss : 0.020574, loss_ce: 0.008178
2022-01-10 11:54:00,416 iteration 4615 : loss : 0.032751, loss_ce: 0.015018
2022-01-10 11:54:02,049 iteration 4616 : loss : 0.034491, loss_ce: 0.009742
2022-01-10 11:54:03,566 iteration 4617 : loss : 0.019886, loss_ce: 0.009213
2022-01-10 11:54:05,190 iteration 4618 : loss : 0.024510, loss_ce: 0.009239
2022-01-10 11:54:06,806 iteration 4619 : loss : 0.024364, loss_ce: 0.008135
2022-01-10 11:54:08,424 iteration 4620 : loss : 0.021322, loss_ce: 0.006891
2022-01-10 11:54:10,073 iteration 4621 : loss : 0.028318, loss_ce: 0.009697
2022-01-10 11:54:11,636 iteration 4622 : loss : 0.017071, loss_ce: 0.005779
2022-01-10 11:54:13,190 iteration 4623 : loss : 0.029614, loss_ce: 0.010151
2022-01-10 11:54:14,767 iteration 4624 : loss : 0.019880, loss_ce: 0.008794
 68%|██████████████████▎        | 272/400 [2:12:53<1:01:23, 28.78s/it]2022-01-10 11:54:16,392 iteration 4625 : loss : 0.015099, loss_ce: 0.006327
2022-01-10 11:54:17,941 iteration 4626 : loss : 0.014894, loss_ce: 0.004717
2022-01-10 11:54:19,509 iteration 4627 : loss : 0.015998, loss_ce: 0.004886
2022-01-10 11:54:21,030 iteration 4628 : loss : 0.019901, loss_ce: 0.007008
2022-01-10 11:54:22,543 iteration 4629 : loss : 0.018878, loss_ce: 0.008408
2022-01-10 11:54:24,098 iteration 4630 : loss : 0.021537, loss_ce: 0.006653
2022-01-10 11:54:25,727 iteration 4631 : loss : 0.016908, loss_ce: 0.006846
2022-01-10 11:54:27,300 iteration 4632 : loss : 0.015309, loss_ce: 0.006023
2022-01-10 11:54:28,872 iteration 4633 : loss : 0.020443, loss_ce: 0.008107
2022-01-10 11:54:30,358 iteration 4634 : loss : 0.014450, loss_ce: 0.006392
2022-01-10 11:54:31,961 iteration 4635 : loss : 0.015178, loss_ce: 0.004930
2022-01-10 11:54:33,531 iteration 4636 : loss : 0.022219, loss_ce: 0.007691
2022-01-10 11:54:35,092 iteration 4637 : loss : 0.014879, loss_ce: 0.005530
2022-01-10 11:54:36,714 iteration 4638 : loss : 0.020331, loss_ce: 0.009239
2022-01-10 11:54:38,383 iteration 4639 : loss : 0.024560, loss_ce: 0.008100
2022-01-10 11:54:39,995 iteration 4640 : loss : 0.017454, loss_ce: 0.007470
2022-01-10 11:54:41,547 iteration 4641 : loss : 0.012133, loss_ce: 0.005173
 68%|███████████████████▊         | 273/400 [2:13:20<59:38, 28.18s/it]2022-01-10 11:54:43,225 iteration 4642 : loss : 0.023419, loss_ce: 0.006463
2022-01-10 11:54:44,847 iteration 4643 : loss : 0.027233, loss_ce: 0.011977
2022-01-10 11:54:46,451 iteration 4644 : loss : 0.014658, loss_ce: 0.005979
2022-01-10 11:54:48,202 iteration 4645 : loss : 0.019205, loss_ce: 0.007714
2022-01-10 11:54:49,747 iteration 4646 : loss : 0.013750, loss_ce: 0.005651
2022-01-10 11:54:51,283 iteration 4647 : loss : 0.017262, loss_ce: 0.006630
2022-01-10 11:54:52,764 iteration 4648 : loss : 0.013403, loss_ce: 0.005300
2022-01-10 11:54:54,345 iteration 4649 : loss : 0.027176, loss_ce: 0.011329
2022-01-10 11:54:55,891 iteration 4650 : loss : 0.019736, loss_ce: 0.006953
2022-01-10 11:54:57,466 iteration 4651 : loss : 0.016611, loss_ce: 0.006950
2022-01-10 11:54:59,060 iteration 4652 : loss : 0.022549, loss_ce: 0.009511
2022-01-10 11:55:00,635 iteration 4653 : loss : 0.019652, loss_ce: 0.007251
2022-01-10 11:55:02,208 iteration 4654 : loss : 0.019591, loss_ce: 0.009447
2022-01-10 11:55:03,806 iteration 4655 : loss : 0.017968, loss_ce: 0.006068
2022-01-10 11:55:05,470 iteration 4656 : loss : 0.052582, loss_ce: 0.011430
2022-01-10 11:55:07,043 iteration 4657 : loss : 0.041033, loss_ce: 0.005139
2022-01-10 11:55:08,575 iteration 4658 : loss : 0.035255, loss_ce: 0.007612
 68%|███████████████████▊         | 274/400 [2:13:47<58:27, 27.84s/it]2022-01-10 11:55:10,209 iteration 4659 : loss : 0.015562, loss_ce: 0.005489
2022-01-10 11:55:11,815 iteration 4660 : loss : 0.016520, loss_ce: 0.003802
2022-01-10 11:55:13,394 iteration 4661 : loss : 0.018761, loss_ce: 0.009734
2022-01-10 11:55:15,080 iteration 4662 : loss : 0.028512, loss_ce: 0.013673
2022-01-10 11:55:16,607 iteration 4663 : loss : 0.019952, loss_ce: 0.005776
2022-01-10 11:55:18,191 iteration 4664 : loss : 0.023375, loss_ce: 0.006112
2022-01-10 11:55:19,916 iteration 4665 : loss : 0.046463, loss_ce: 0.021226
2022-01-10 11:55:21,436 iteration 4666 : loss : 0.019337, loss_ce: 0.007126
2022-01-10 11:55:22,993 iteration 4667 : loss : 0.020042, loss_ce: 0.007767
2022-01-10 11:55:24,549 iteration 4668 : loss : 0.032755, loss_ce: 0.008648
2022-01-10 11:55:26,197 iteration 4669 : loss : 0.018404, loss_ce: 0.007831
2022-01-10 11:55:27,861 iteration 4670 : loss : 0.017647, loss_ce: 0.007354
2022-01-10 11:55:29,376 iteration 4671 : loss : 0.020294, loss_ce: 0.005961
2022-01-10 11:55:31,087 iteration 4672 : loss : 0.040540, loss_ce: 0.016305
2022-01-10 11:55:32,699 iteration 4673 : loss : 0.022329, loss_ce: 0.007731
2022-01-10 11:55:34,351 iteration 4674 : loss : 0.020949, loss_ce: 0.011465
2022-01-10 11:55:34,352 Training Data Eval:
2022-01-10 11:55:42,285   Average segmentation loss on training set: 0.0138
2022-01-10 11:55:42,285 Validation Data Eval:
2022-01-10 11:55:45,022   Average segmentation loss on validation set: 0.0747
2022-01-10 11:55:46,556 iteration 4675 : loss : 0.019881, loss_ce: 0.007493
 69%|██████████████████▌        | 275/400 [2:14:25<1:04:19, 30.88s/it]2022-01-10 11:55:48,255 iteration 4676 : loss : 0.032150, loss_ce: 0.012051
2022-01-10 11:55:49,813 iteration 4677 : loss : 0.020533, loss_ce: 0.008058
2022-01-10 11:55:51,389 iteration 4678 : loss : 0.023173, loss_ce: 0.007794
2022-01-10 11:55:52,966 iteration 4679 : loss : 0.023888, loss_ce: 0.012340
2022-01-10 11:55:54,503 iteration 4680 : loss : 0.018024, loss_ce: 0.006898
2022-01-10 11:55:56,085 iteration 4681 : loss : 0.033466, loss_ce: 0.009906
2022-01-10 11:55:57,592 iteration 4682 : loss : 0.027248, loss_ce: 0.006573
2022-01-10 11:55:59,166 iteration 4683 : loss : 0.029852, loss_ce: 0.013026
2022-01-10 11:56:00,776 iteration 4684 : loss : 0.022230, loss_ce: 0.009637
2022-01-10 11:56:02,370 iteration 4685 : loss : 0.019939, loss_ce: 0.008405
2022-01-10 11:56:03,864 iteration 4686 : loss : 0.016879, loss_ce: 0.005384
2022-01-10 11:56:05,502 iteration 4687 : loss : 0.020845, loss_ce: 0.009679
2022-01-10 11:56:07,143 iteration 4688 : loss : 0.020533, loss_ce: 0.006708
2022-01-10 11:56:08,804 iteration 4689 : loss : 0.022515, loss_ce: 0.009612
2022-01-10 11:56:10,364 iteration 4690 : loss : 0.019104, loss_ce: 0.005491
2022-01-10 11:56:11,897 iteration 4691 : loss : 0.024987, loss_ce: 0.007479
2022-01-10 11:56:13,470 iteration 4692 : loss : 0.018615, loss_ce: 0.008515
 69%|██████████████████▋        | 276/400 [2:14:52<1:01:21, 29.69s/it]2022-01-10 11:56:15,124 iteration 4693 : loss : 0.018370, loss_ce: 0.007542
2022-01-10 11:56:16,663 iteration 4694 : loss : 0.018662, loss_ce: 0.005681
2022-01-10 11:56:18,289 iteration 4695 : loss : 0.032037, loss_ce: 0.013503
2022-01-10 11:56:19,894 iteration 4696 : loss : 0.021899, loss_ce: 0.009579
2022-01-10 11:56:21,519 iteration 4697 : loss : 0.024990, loss_ce: 0.010662
2022-01-10 11:56:23,001 iteration 4698 : loss : 0.020737, loss_ce: 0.007016
2022-01-10 11:56:24,570 iteration 4699 : loss : 0.020698, loss_ce: 0.007800
2022-01-10 11:56:26,166 iteration 4700 : loss : 0.027305, loss_ce: 0.007909
2022-01-10 11:56:27,756 iteration 4701 : loss : 0.021776, loss_ce: 0.010309
2022-01-10 11:56:29,246 iteration 4702 : loss : 0.016491, loss_ce: 0.006560
2022-01-10 11:56:30,848 iteration 4703 : loss : 0.028748, loss_ce: 0.012110
2022-01-10 11:56:32,438 iteration 4704 : loss : 0.030068, loss_ce: 0.008656
2022-01-10 11:56:33,955 iteration 4705 : loss : 0.015228, loss_ce: 0.005714
2022-01-10 11:56:35,481 iteration 4706 : loss : 0.019297, loss_ce: 0.009178
2022-01-10 11:56:37,118 iteration 4707 : loss : 0.024580, loss_ce: 0.011258
2022-01-10 11:56:38,685 iteration 4708 : loss : 0.028081, loss_ce: 0.006262
2022-01-10 11:56:40,238 iteration 4709 : loss : 0.018673, loss_ce: 0.007956
 69%|████████████████████         | 277/400 [2:15:19<59:03, 28.81s/it]2022-01-10 11:56:41,949 iteration 4710 : loss : 0.027583, loss_ce: 0.009629
2022-01-10 11:56:43,498 iteration 4711 : loss : 0.021247, loss_ce: 0.009773
2022-01-10 11:56:45,069 iteration 4712 : loss : 0.017504, loss_ce: 0.007651
2022-01-10 11:56:46,596 iteration 4713 : loss : 0.014425, loss_ce: 0.005321
2022-01-10 11:56:48,122 iteration 4714 : loss : 0.020646, loss_ce: 0.009320
2022-01-10 11:56:49,775 iteration 4715 : loss : 0.048798, loss_ce: 0.013539
2022-01-10 11:56:51,356 iteration 4716 : loss : 0.014857, loss_ce: 0.006249
2022-01-10 11:56:52,933 iteration 4717 : loss : 0.017376, loss_ce: 0.004901
2022-01-10 11:56:54,486 iteration 4718 : loss : 0.019078, loss_ce: 0.007085
2022-01-10 11:56:56,032 iteration 4719 : loss : 0.014827, loss_ce: 0.005223
2022-01-10 11:56:57,652 iteration 4720 : loss : 0.017305, loss_ce: 0.007587
2022-01-10 11:56:59,232 iteration 4721 : loss : 0.015787, loss_ce: 0.005727
2022-01-10 11:57:00,795 iteration 4722 : loss : 0.018685, loss_ce: 0.007304
2022-01-10 11:57:02,407 iteration 4723 : loss : 0.024290, loss_ce: 0.008272
2022-01-10 11:57:03,933 iteration 4724 : loss : 0.017482, loss_ce: 0.009009
2022-01-10 11:57:05,473 iteration 4725 : loss : 0.022873, loss_ce: 0.008032
2022-01-10 11:57:07,008 iteration 4726 : loss : 0.021988, loss_ce: 0.008006
 70%|████████████████████▏        | 278/400 [2:15:46<57:20, 28.20s/it]2022-01-10 11:57:08,648 iteration 4727 : loss : 0.027673, loss_ce: 0.008912
2022-01-10 11:57:10,244 iteration 4728 : loss : 0.025552, loss_ce: 0.011946
2022-01-10 11:57:11,857 iteration 4729 : loss : 0.012362, loss_ce: 0.003858
2022-01-10 11:57:13,436 iteration 4730 : loss : 0.020436, loss_ce: 0.008022
2022-01-10 11:57:15,020 iteration 4731 : loss : 0.025106, loss_ce: 0.007317
2022-01-10 11:57:16,522 iteration 4732 : loss : 0.016035, loss_ce: 0.006817
2022-01-10 11:57:18,202 iteration 4733 : loss : 0.021445, loss_ce: 0.009449
2022-01-10 11:57:19,773 iteration 4734 : loss : 0.019591, loss_ce: 0.007394
2022-01-10 11:57:21,289 iteration 4735 : loss : 0.014950, loss_ce: 0.005929
2022-01-10 11:57:22,904 iteration 4736 : loss : 0.018320, loss_ce: 0.007494
2022-01-10 11:57:24,509 iteration 4737 : loss : 0.026013, loss_ce: 0.015723
2022-01-10 11:57:26,207 iteration 4738 : loss : 0.034570, loss_ce: 0.008612
2022-01-10 11:57:27,829 iteration 4739 : loss : 0.013821, loss_ce: 0.005386
2022-01-10 11:57:29,424 iteration 4740 : loss : 0.019267, loss_ce: 0.005255
2022-01-10 11:57:31,009 iteration 4741 : loss : 0.022716, loss_ce: 0.011742
2022-01-10 11:57:32,612 iteration 4742 : loss : 0.029032, loss_ce: 0.012564
2022-01-10 11:57:34,359 iteration 4743 : loss : 0.026406, loss_ce: 0.011467
 70%|████████████████████▏        | 279/400 [2:16:13<56:21, 27.94s/it]2022-01-10 11:57:36,070 iteration 4744 : loss : 0.017189, loss_ce: 0.005464
2022-01-10 11:57:37,674 iteration 4745 : loss : 0.018073, loss_ce: 0.007436
2022-01-10 11:57:39,260 iteration 4746 : loss : 0.018743, loss_ce: 0.005574
2022-01-10 11:57:40,763 iteration 4747 : loss : 0.012565, loss_ce: 0.005872
2022-01-10 11:57:42,448 iteration 4748 : loss : 0.018666, loss_ce: 0.007630
2022-01-10 11:57:43,967 iteration 4749 : loss : 0.021237, loss_ce: 0.008767
2022-01-10 11:57:45,567 iteration 4750 : loss : 0.021771, loss_ce: 0.006120
2022-01-10 11:57:47,176 iteration 4751 : loss : 0.016367, loss_ce: 0.006366
2022-01-10 11:57:48,755 iteration 4752 : loss : 0.017611, loss_ce: 0.004899
2022-01-10 11:57:50,305 iteration 4753 : loss : 0.012726, loss_ce: 0.004147
2022-01-10 11:57:51,898 iteration 4754 : loss : 0.016919, loss_ce: 0.007330
2022-01-10 11:57:53,571 iteration 4755 : loss : 0.034202, loss_ce: 0.008941
2022-01-10 11:57:55,191 iteration 4756 : loss : 0.015386, loss_ce: 0.004465
2022-01-10 11:57:56,777 iteration 4757 : loss : 0.016747, loss_ce: 0.008421
2022-01-10 11:57:58,445 iteration 4758 : loss : 0.024617, loss_ce: 0.008736
2022-01-10 11:58:00,027 iteration 4759 : loss : 0.038880, loss_ce: 0.023084
2022-01-10 11:58:00,027 Training Data Eval:
2022-01-10 11:58:07,962   Average segmentation loss on training set: 0.0108
2022-01-10 11:58:07,963 Validation Data Eval:
2022-01-10 11:58:10,705   Average segmentation loss on validation set: 0.0724
2022-01-10 11:58:12,299 iteration 4760 : loss : 0.025198, loss_ce: 0.007952
 70%|██████████████████▉        | 280/400 [2:16:51<1:01:53, 30.94s/it]2022-01-10 11:58:13,877 iteration 4761 : loss : 0.012657, loss_ce: 0.004363
2022-01-10 11:58:15,548 iteration 4762 : loss : 0.028589, loss_ce: 0.009131
2022-01-10 11:58:17,157 iteration 4763 : loss : 0.018272, loss_ce: 0.007951
2022-01-10 11:58:18,676 iteration 4764 : loss : 0.020783, loss_ce: 0.009259
2022-01-10 11:58:20,228 iteration 4765 : loss : 0.017199, loss_ce: 0.007735
2022-01-10 11:58:21,767 iteration 4766 : loss : 0.016219, loss_ce: 0.005000
2022-01-10 11:58:23,392 iteration 4767 : loss : 0.042422, loss_ce: 0.020916
2022-01-10 11:58:25,074 iteration 4768 : loss : 0.022816, loss_ce: 0.009862
2022-01-10 11:58:26,734 iteration 4769 : loss : 0.027894, loss_ce: 0.006823
2022-01-10 11:58:28,325 iteration 4770 : loss : 0.016319, loss_ce: 0.008099
2022-01-10 11:58:29,872 iteration 4771 : loss : 0.017788, loss_ce: 0.007490
2022-01-10 11:58:31,436 iteration 4772 : loss : 0.012928, loss_ce: 0.004061
2022-01-10 11:58:32,978 iteration 4773 : loss : 0.016605, loss_ce: 0.006702
2022-01-10 11:58:34,519 iteration 4774 : loss : 0.015119, loss_ce: 0.006038
2022-01-10 11:58:36,110 iteration 4775 : loss : 0.026371, loss_ce: 0.009737
2022-01-10 11:58:37,680 iteration 4776 : loss : 0.018916, loss_ce: 0.007391
2022-01-10 11:58:39,250 iteration 4777 : loss : 0.017182, loss_ce: 0.005702
 70%|████████████████████▎        | 281/400 [2:17:18<58:59, 29.74s/it]2022-01-10 11:58:40,959 iteration 4778 : loss : 0.020061, loss_ce: 0.008116
2022-01-10 11:58:42,500 iteration 4779 : loss : 0.017040, loss_ce: 0.005778
2022-01-10 11:58:44,040 iteration 4780 : loss : 0.022362, loss_ce: 0.005623
2022-01-10 11:58:45,587 iteration 4781 : loss : 0.014118, loss_ce: 0.006399
2022-01-10 11:58:47,219 iteration 4782 : loss : 0.019859, loss_ce: 0.006039
2022-01-10 11:58:48,729 iteration 4783 : loss : 0.015051, loss_ce: 0.004495
2022-01-10 11:58:50,378 iteration 4784 : loss : 0.017561, loss_ce: 0.006537
2022-01-10 11:58:52,050 iteration 4785 : loss : 0.024306, loss_ce: 0.011186
2022-01-10 11:58:53,630 iteration 4786 : loss : 0.014987, loss_ce: 0.005575
2022-01-10 11:58:55,255 iteration 4787 : loss : 0.019301, loss_ce: 0.005349
2022-01-10 11:58:56,857 iteration 4788 : loss : 0.017923, loss_ce: 0.007986
2022-01-10 11:58:58,460 iteration 4789 : loss : 0.020438, loss_ce: 0.007029
2022-01-10 11:59:00,039 iteration 4790 : loss : 0.018058, loss_ce: 0.007637
2022-01-10 11:59:01,582 iteration 4791 : loss : 0.013981, loss_ce: 0.005200
2022-01-10 11:59:03,146 iteration 4792 : loss : 0.025091, loss_ce: 0.012192
2022-01-10 11:59:04,758 iteration 4793 : loss : 0.017350, loss_ce: 0.008346
2022-01-10 11:59:06,319 iteration 4794 : loss : 0.035707, loss_ce: 0.006402
 70%|████████████████████▍        | 282/400 [2:17:45<56:55, 28.94s/it]2022-01-10 11:59:07,884 iteration 4795 : loss : 0.018099, loss_ce: 0.005751
2022-01-10 11:59:09,401 iteration 4796 : loss : 0.018931, loss_ce: 0.007276
2022-01-10 11:59:10,919 iteration 4797 : loss : 0.017091, loss_ce: 0.006418
2022-01-10 11:59:12,558 iteration 4798 : loss : 0.034756, loss_ce: 0.014438
2022-01-10 11:59:14,145 iteration 4799 : loss : 0.048397, loss_ce: 0.012564
2022-01-10 11:59:15,737 iteration 4800 : loss : 0.037359, loss_ce: 0.017857
2022-01-10 11:59:17,323 iteration 4801 : loss : 0.020370, loss_ce: 0.012070
2022-01-10 11:59:18,807 iteration 4802 : loss : 0.014230, loss_ce: 0.004683
2022-01-10 11:59:20,387 iteration 4803 : loss : 0.021929, loss_ce: 0.010045
2022-01-10 11:59:21,934 iteration 4804 : loss : 0.021863, loss_ce: 0.006365
2022-01-10 11:59:23,545 iteration 4805 : loss : 0.021362, loss_ce: 0.007521
2022-01-10 11:59:25,079 iteration 4806 : loss : 0.017499, loss_ce: 0.004832
2022-01-10 11:59:26,738 iteration 4807 : loss : 0.025124, loss_ce: 0.008432
2022-01-10 11:59:28,539 iteration 4808 : loss : 0.027160, loss_ce: 0.010389
2022-01-10 11:59:30,030 iteration 4809 : loss : 0.014805, loss_ce: 0.006241
2022-01-10 11:59:31,663 iteration 4810 : loss : 0.024049, loss_ce: 0.009423
2022-01-10 11:59:33,218 iteration 4811 : loss : 0.018956, loss_ce: 0.007253
 71%|████████████████████▌        | 283/400 [2:18:12<55:14, 28.33s/it]2022-01-10 11:59:34,815 iteration 4812 : loss : 0.017102, loss_ce: 0.006223
2022-01-10 11:59:36,414 iteration 4813 : loss : 0.022670, loss_ce: 0.007913
2022-01-10 11:59:37,951 iteration 4814 : loss : 0.014892, loss_ce: 0.007207
2022-01-10 11:59:39,532 iteration 4815 : loss : 0.017531, loss_ce: 0.006290
2022-01-10 11:59:41,077 iteration 4816 : loss : 0.017362, loss_ce: 0.006296
2022-01-10 11:59:42,526 iteration 4817 : loss : 0.015610, loss_ce: 0.005443
2022-01-10 11:59:44,141 iteration 4818 : loss : 0.027496, loss_ce: 0.011850
2022-01-10 11:59:45,742 iteration 4819 : loss : 0.023921, loss_ce: 0.007940
2022-01-10 11:59:47,383 iteration 4820 : loss : 0.018768, loss_ce: 0.008434
2022-01-10 11:59:48,943 iteration 4821 : loss : 0.017625, loss_ce: 0.008636
2022-01-10 11:59:50,544 iteration 4822 : loss : 0.027508, loss_ce: 0.009055
2022-01-10 11:59:52,094 iteration 4823 : loss : 0.016993, loss_ce: 0.006778
2022-01-10 11:59:53,730 iteration 4824 : loss : 0.017857, loss_ce: 0.006519
2022-01-10 11:59:55,292 iteration 4825 : loss : 0.012519, loss_ce: 0.003688
2022-01-10 11:59:56,843 iteration 4826 : loss : 0.040169, loss_ce: 0.015042
2022-01-10 11:59:58,537 iteration 4827 : loss : 0.019316, loss_ce: 0.006352
2022-01-10 12:00:00,196 iteration 4828 : loss : 0.031382, loss_ce: 0.012517
 71%|████████████████████▌        | 284/400 [2:18:39<53:59, 27.93s/it]2022-01-10 12:00:01,876 iteration 4829 : loss : 0.021809, loss_ce: 0.007203
2022-01-10 12:00:03,400 iteration 4830 : loss : 0.018246, loss_ce: 0.006165
2022-01-10 12:00:05,087 iteration 4831 : loss : 0.040353, loss_ce: 0.014291
2022-01-10 12:00:06,667 iteration 4832 : loss : 0.020172, loss_ce: 0.008569
2022-01-10 12:00:08,322 iteration 4833 : loss : 0.025715, loss_ce: 0.006150
2022-01-10 12:00:10,014 iteration 4834 : loss : 0.020689, loss_ce: 0.006901
2022-01-10 12:00:11,579 iteration 4835 : loss : 0.031613, loss_ce: 0.011724
2022-01-10 12:00:13,163 iteration 4836 : loss : 0.026229, loss_ce: 0.008997
2022-01-10 12:00:14,697 iteration 4837 : loss : 0.017247, loss_ce: 0.004143
2022-01-10 12:00:16,309 iteration 4838 : loss : 0.023667, loss_ce: 0.011271
2022-01-10 12:00:17,922 iteration 4839 : loss : 0.024514, loss_ce: 0.013430
2022-01-10 12:00:19,524 iteration 4840 : loss : 0.024704, loss_ce: 0.008932
2022-01-10 12:00:21,192 iteration 4841 : loss : 0.019135, loss_ce: 0.006361
2022-01-10 12:00:22,806 iteration 4842 : loss : 0.022284, loss_ce: 0.009995
2022-01-10 12:00:24,397 iteration 4843 : loss : 0.021618, loss_ce: 0.007546
2022-01-10 12:00:25,914 iteration 4844 : loss : 0.015591, loss_ce: 0.007251
2022-01-10 12:00:25,914 Training Data Eval:
2022-01-10 12:00:33,862   Average segmentation loss on training set: 0.0112
2022-01-10 12:00:33,863 Validation Data Eval:
2022-01-10 12:00:36,602   Average segmentation loss on validation set: 0.0748
2022-01-10 12:00:38,142 iteration 4845 : loss : 0.020367, loss_ce: 0.006971
 71%|████████████████████▋        | 285/400 [2:19:17<59:17, 30.93s/it]2022-01-10 12:00:39,800 iteration 4846 : loss : 0.021411, loss_ce: 0.007792
2022-01-10 12:00:41,356 iteration 4847 : loss : 0.017852, loss_ce: 0.005279
2022-01-10 12:00:43,064 iteration 4848 : loss : 0.027290, loss_ce: 0.012099
2022-01-10 12:00:44,635 iteration 4849 : loss : 0.017301, loss_ce: 0.007701
2022-01-10 12:00:46,246 iteration 4850 : loss : 0.060268, loss_ce: 0.011870
2022-01-10 12:00:47,852 iteration 4851 : loss : 0.016458, loss_ce: 0.007184
2022-01-10 12:00:49,406 iteration 4852 : loss : 0.020010, loss_ce: 0.007608
2022-01-10 12:00:50,960 iteration 4853 : loss : 0.018909, loss_ce: 0.007442
2022-01-10 12:00:52,456 iteration 4854 : loss : 0.015884, loss_ce: 0.005918
2022-01-10 12:00:54,043 iteration 4855 : loss : 0.018705, loss_ce: 0.007317
2022-01-10 12:00:55,646 iteration 4856 : loss : 0.024004, loss_ce: 0.008581
2022-01-10 12:00:57,263 iteration 4857 : loss : 0.023504, loss_ce: 0.012031
2022-01-10 12:00:58,847 iteration 4858 : loss : 0.018294, loss_ce: 0.007368
2022-01-10 12:01:00,369 iteration 4859 : loss : 0.016218, loss_ce: 0.006919
2022-01-10 12:01:01,916 iteration 4860 : loss : 0.025955, loss_ce: 0.006897
2022-01-10 12:01:03,418 iteration 4861 : loss : 0.026722, loss_ce: 0.007891
2022-01-10 12:01:05,079 iteration 4862 : loss : 0.020870, loss_ce: 0.007042
 72%|████████████████████▋        | 286/400 [2:19:44<56:29, 29.73s/it]2022-01-10 12:01:06,755 iteration 4863 : loss : 0.019008, loss_ce: 0.006102
2022-01-10 12:01:08,367 iteration 4864 : loss : 0.024902, loss_ce: 0.010700
2022-01-10 12:01:09,960 iteration 4865 : loss : 0.037582, loss_ce: 0.018354
2022-01-10 12:01:11,552 iteration 4866 : loss : 0.035241, loss_ce: 0.016297
2022-01-10 12:01:13,174 iteration 4867 : loss : 0.040954, loss_ce: 0.013466
2022-01-10 12:01:14,775 iteration 4868 : loss : 0.024216, loss_ce: 0.011393
2022-01-10 12:01:16,323 iteration 4869 : loss : 0.019855, loss_ce: 0.007469
2022-01-10 12:01:17,802 iteration 4870 : loss : 0.016432, loss_ce: 0.006365
2022-01-10 12:01:19,344 iteration 4871 : loss : 0.023723, loss_ce: 0.007158
2022-01-10 12:01:20,922 iteration 4872 : loss : 0.021141, loss_ce: 0.009275
2022-01-10 12:01:22,495 iteration 4873 : loss : 0.021560, loss_ce: 0.008318
2022-01-10 12:01:24,066 iteration 4874 : loss : 0.018480, loss_ce: 0.006281
2022-01-10 12:01:25,615 iteration 4875 : loss : 0.018780, loss_ce: 0.006395
2022-01-10 12:01:27,101 iteration 4876 : loss : 0.015725, loss_ce: 0.006683
2022-01-10 12:01:28,653 iteration 4877 : loss : 0.015906, loss_ce: 0.005010
2022-01-10 12:01:30,238 iteration 4878 : loss : 0.028477, loss_ce: 0.010622
2022-01-10 12:01:31,861 iteration 4879 : loss : 0.019007, loss_ce: 0.006743
 72%|████████████████████▊        | 287/400 [2:20:10<54:19, 28.85s/it]2022-01-10 12:01:33,521 iteration 4880 : loss : 0.016059, loss_ce: 0.005354
2022-01-10 12:01:35,033 iteration 4881 : loss : 0.012437, loss_ce: 0.004113
2022-01-10 12:01:36,538 iteration 4882 : loss : 0.014307, loss_ce: 0.003432
2022-01-10 12:01:38,137 iteration 4883 : loss : 0.019434, loss_ce: 0.007585
2022-01-10 12:01:39,668 iteration 4884 : loss : 0.020338, loss_ce: 0.008523
2022-01-10 12:01:41,241 iteration 4885 : loss : 0.019692, loss_ce: 0.005760
2022-01-10 12:01:42,847 iteration 4886 : loss : 0.016884, loss_ce: 0.005678
2022-01-10 12:01:44,398 iteration 4887 : loss : 0.022123, loss_ce: 0.008716
2022-01-10 12:01:45,991 iteration 4888 : loss : 0.019640, loss_ce: 0.005260
2022-01-10 12:01:47,543 iteration 4889 : loss : 0.016186, loss_ce: 0.006977
2022-01-10 12:01:49,137 iteration 4890 : loss : 0.016712, loss_ce: 0.005853
2022-01-10 12:01:50,724 iteration 4891 : loss : 0.019201, loss_ce: 0.008678
2022-01-10 12:01:52,310 iteration 4892 : loss : 0.028142, loss_ce: 0.011300
2022-01-10 12:01:54,059 iteration 4893 : loss : 0.024371, loss_ce: 0.008750
2022-01-10 12:01:55,642 iteration 4894 : loss : 0.015921, loss_ce: 0.006501
2022-01-10 12:01:57,212 iteration 4895 : loss : 0.023506, loss_ce: 0.012306
2022-01-10 12:01:58,773 iteration 4896 : loss : 0.023110, loss_ce: 0.008348
 72%|████████████████████▉        | 288/400 [2:20:37<52:45, 28.27s/it]2022-01-10 12:02:00,333 iteration 4897 : loss : 0.015007, loss_ce: 0.007396
2022-01-10 12:02:01,892 iteration 4898 : loss : 0.013521, loss_ce: 0.004772
2022-01-10 12:02:03,417 iteration 4899 : loss : 0.023848, loss_ce: 0.015113
2022-01-10 12:02:05,048 iteration 4900 : loss : 0.026577, loss_ce: 0.011772
2022-01-10 12:02:06,635 iteration 4901 : loss : 0.026095, loss_ce: 0.008802
2022-01-10 12:02:08,171 iteration 4902 : loss : 0.013931, loss_ce: 0.003112
2022-01-10 12:02:09,753 iteration 4903 : loss : 0.015385, loss_ce: 0.006443
2022-01-10 12:02:11,318 iteration 4904 : loss : 0.017243, loss_ce: 0.007921
2022-01-10 12:02:12,874 iteration 4905 : loss : 0.013985, loss_ce: 0.006289
2022-01-10 12:02:14,425 iteration 4906 : loss : 0.016782, loss_ce: 0.006009
2022-01-10 12:02:16,003 iteration 4907 : loss : 0.017080, loss_ce: 0.006311
2022-01-10 12:02:17,478 iteration 4908 : loss : 0.020029, loss_ce: 0.005359
2022-01-10 12:02:19,041 iteration 4909 : loss : 0.025579, loss_ce: 0.011396
2022-01-10 12:02:20,588 iteration 4910 : loss : 0.017278, loss_ce: 0.004622
2022-01-10 12:02:22,253 iteration 4911 : loss : 0.018089, loss_ce: 0.007596
2022-01-10 12:02:23,903 iteration 4912 : loss : 0.024018, loss_ce: 0.007846
2022-01-10 12:02:25,454 iteration 4913 : loss : 0.014917, loss_ce: 0.004450
 72%|████████████████████▉        | 289/400 [2:21:04<51:24, 27.79s/it]2022-01-10 12:02:27,020 iteration 4914 : loss : 0.015537, loss_ce: 0.004661
2022-01-10 12:02:28,632 iteration 4915 : loss : 0.020671, loss_ce: 0.008387
2022-01-10 12:02:30,209 iteration 4916 : loss : 0.028775, loss_ce: 0.008263
2022-01-10 12:02:31,765 iteration 4917 : loss : 0.016533, loss_ce: 0.006053
2022-01-10 12:02:33,287 iteration 4918 : loss : 0.016163, loss_ce: 0.007454
2022-01-10 12:02:34,798 iteration 4919 : loss : 0.026866, loss_ce: 0.016654
2022-01-10 12:02:36,378 iteration 4920 : loss : 0.019553, loss_ce: 0.009473
2022-01-10 12:02:37,918 iteration 4921 : loss : 0.013662, loss_ce: 0.003120
2022-01-10 12:02:39,464 iteration 4922 : loss : 0.018452, loss_ce: 0.003556
2022-01-10 12:02:41,023 iteration 4923 : loss : 0.022058, loss_ce: 0.008065
2022-01-10 12:02:42,600 iteration 4924 : loss : 0.017741, loss_ce: 0.006633
2022-01-10 12:02:44,278 iteration 4925 : loss : 0.026612, loss_ce: 0.009803
2022-01-10 12:02:45,812 iteration 4926 : loss : 0.019144, loss_ce: 0.007177
2022-01-10 12:02:47,427 iteration 4927 : loss : 0.021729, loss_ce: 0.008432
2022-01-10 12:02:49,054 iteration 4928 : loss : 0.022396, loss_ce: 0.011369
2022-01-10 12:02:50,665 iteration 4929 : loss : 0.047854, loss_ce: 0.016596
2022-01-10 12:02:50,665 Training Data Eval:
2022-01-10 12:02:58,563   Average segmentation loss on training set: 0.0117
2022-01-10 12:02:58,563 Validation Data Eval:
2022-01-10 12:03:01,296   Average segmentation loss on validation set: 0.0663
2022-01-10 12:03:02,863 iteration 4930 : loss : 0.012969, loss_ce: 0.005367
 72%|█████████████████████        | 290/400 [2:21:41<56:14, 30.68s/it]2022-01-10 12:03:04,521 iteration 4931 : loss : 0.017726, loss_ce: 0.008171
2022-01-10 12:03:06,049 iteration 4932 : loss : 0.017818, loss_ce: 0.006598
2022-01-10 12:03:07,564 iteration 4933 : loss : 0.018352, loss_ce: 0.006385
2022-01-10 12:03:09,132 iteration 4934 : loss : 0.022088, loss_ce: 0.009909
2022-01-10 12:03:10,724 iteration 4935 : loss : 0.029709, loss_ce: 0.011226
2022-01-10 12:03:12,290 iteration 4936 : loss : 0.016098, loss_ce: 0.007121
2022-01-10 12:03:13,871 iteration 4937 : loss : 0.025290, loss_ce: 0.008480
2022-01-10 12:03:15,431 iteration 4938 : loss : 0.031108, loss_ce: 0.010254
2022-01-10 12:03:17,048 iteration 4939 : loss : 0.023741, loss_ce: 0.008853
2022-01-10 12:03:18,570 iteration 4940 : loss : 0.018694, loss_ce: 0.005929
2022-01-10 12:03:20,128 iteration 4941 : loss : 0.015845, loss_ce: 0.005396
2022-01-10 12:03:21,729 iteration 4942 : loss : 0.021395, loss_ce: 0.008071
2022-01-10 12:03:23,252 iteration 4943 : loss : 0.013573, loss_ce: 0.004244
2022-01-10 12:03:24,812 iteration 4944 : loss : 0.030183, loss_ce: 0.011256
2022-01-10 12:03:26,457 iteration 4945 : loss : 0.020404, loss_ce: 0.005550
2022-01-10 12:03:28,072 iteration 4946 : loss : 0.025265, loss_ce: 0.013647
2022-01-10 12:03:29,733 iteration 4947 : loss : 0.023319, loss_ce: 0.008693
 73%|█████████████████████        | 291/400 [2:22:08<53:39, 29.54s/it]2022-01-10 12:03:31,414 iteration 4948 : loss : 0.022125, loss_ce: 0.007908
2022-01-10 12:03:32,958 iteration 4949 : loss : 0.017009, loss_ce: 0.006681
2022-01-10 12:03:34,479 iteration 4950 : loss : 0.017053, loss_ce: 0.006018
2022-01-10 12:03:36,023 iteration 4951 : loss : 0.023002, loss_ce: 0.008473
2022-01-10 12:03:37,506 iteration 4952 : loss : 0.022636, loss_ce: 0.009106
2022-01-10 12:03:39,102 iteration 4953 : loss : 0.014036, loss_ce: 0.005232
2022-01-10 12:03:40,608 iteration 4954 : loss : 0.019377, loss_ce: 0.007178
2022-01-10 12:03:42,147 iteration 4955 : loss : 0.013123, loss_ce: 0.005119
2022-01-10 12:03:43,688 iteration 4956 : loss : 0.020695, loss_ce: 0.007570
2022-01-10 12:03:45,241 iteration 4957 : loss : 0.016533, loss_ce: 0.006969
2022-01-10 12:03:46,808 iteration 4958 : loss : 0.017555, loss_ce: 0.007965
2022-01-10 12:03:48,285 iteration 4959 : loss : 0.014137, loss_ce: 0.004585
2022-01-10 12:03:49,839 iteration 4960 : loss : 0.019251, loss_ce: 0.005838
2022-01-10 12:03:51,349 iteration 4961 : loss : 0.015605, loss_ce: 0.005934
2022-01-10 12:03:52,945 iteration 4962 : loss : 0.021370, loss_ce: 0.008783
2022-01-10 12:03:54,455 iteration 4963 : loss : 0.020832, loss_ce: 0.008016
2022-01-10 12:03:56,089 iteration 4964 : loss : 0.021870, loss_ce: 0.007663
 73%|█████████████████████▏       | 292/400 [2:22:35<51:26, 28.58s/it]2022-01-10 12:03:57,771 iteration 4965 : loss : 0.028314, loss_ce: 0.010124
2022-01-10 12:03:59,446 iteration 4966 : loss : 0.031318, loss_ce: 0.009903
2022-01-10 12:04:00,958 iteration 4967 : loss : 0.012954, loss_ce: 0.005225
2022-01-10 12:04:02,520 iteration 4968 : loss : 0.017500, loss_ce: 0.007222
2022-01-10 12:04:04,057 iteration 4969 : loss : 0.016383, loss_ce: 0.003649
2022-01-10 12:04:05,572 iteration 4970 : loss : 0.011714, loss_ce: 0.004087
2022-01-10 12:04:07,170 iteration 4971 : loss : 0.024929, loss_ce: 0.009720
2022-01-10 12:04:08,794 iteration 4972 : loss : 0.015684, loss_ce: 0.005331
2022-01-10 12:04:10,334 iteration 4973 : loss : 0.016517, loss_ce: 0.006366
2022-01-10 12:04:11,867 iteration 4974 : loss : 0.020122, loss_ce: 0.007418
2022-01-10 12:04:13,403 iteration 4975 : loss : 0.016024, loss_ce: 0.005174
2022-01-10 12:04:14,937 iteration 4976 : loss : 0.025457, loss_ce: 0.010201
2022-01-10 12:04:16,584 iteration 4977 : loss : 0.024556, loss_ce: 0.013401
2022-01-10 12:04:18,140 iteration 4978 : loss : 0.024620, loss_ce: 0.006723
2022-01-10 12:04:19,709 iteration 4979 : loss : 0.021695, loss_ce: 0.011142
2022-01-10 12:04:21,182 iteration 4980 : loss : 0.013238, loss_ce: 0.004638
2022-01-10 12:04:22,740 iteration 4981 : loss : 0.026661, loss_ce: 0.008149
 73%|█████████████████████▏       | 293/400 [2:23:01<49:55, 28.00s/it]2022-01-10 12:04:24,342 iteration 4982 : loss : 0.019621, loss_ce: 0.008669
2022-01-10 12:04:25,870 iteration 4983 : loss : 0.017760, loss_ce: 0.006708
2022-01-10 12:04:27,488 iteration 4984 : loss : 0.023111, loss_ce: 0.008799
2022-01-10 12:04:29,064 iteration 4985 : loss : 0.017553, loss_ce: 0.005620
2022-01-10 12:04:30,710 iteration 4986 : loss : 0.029414, loss_ce: 0.012512
2022-01-10 12:04:32,249 iteration 4987 : loss : 0.017852, loss_ce: 0.008088
2022-01-10 12:04:33,797 iteration 4988 : loss : 0.019646, loss_ce: 0.008533
2022-01-10 12:04:35,339 iteration 4989 : loss : 0.020136, loss_ce: 0.006887
2022-01-10 12:04:36,982 iteration 4990 : loss : 0.017577, loss_ce: 0.006563
2022-01-10 12:04:38,551 iteration 4991 : loss : 0.029300, loss_ce: 0.006678
2022-01-10 12:04:40,111 iteration 4992 : loss : 0.017590, loss_ce: 0.008210
2022-01-10 12:04:41,729 iteration 4993 : loss : 0.023438, loss_ce: 0.010900
2022-01-10 12:04:43,376 iteration 4994 : loss : 0.021546, loss_ce: 0.005526
2022-01-10 12:04:44,940 iteration 4995 : loss : 0.022239, loss_ce: 0.008915
2022-01-10 12:04:46,498 iteration 4996 : loss : 0.023196, loss_ce: 0.008382
2022-01-10 12:04:48,134 iteration 4997 : loss : 0.018545, loss_ce: 0.007517
2022-01-10 12:04:49,649 iteration 4998 : loss : 0.018876, loss_ce: 0.006680
 74%|█████████████████████▎       | 294/400 [2:23:28<48:53, 27.68s/it]2022-01-10 12:04:51,306 iteration 4999 : loss : 0.022524, loss_ce: 0.009350
2022-01-10 12:04:52,809 iteration 5000 : loss : 0.013569, loss_ce: 0.004770
2022-01-10 12:04:54,342 iteration 5001 : loss : 0.022161, loss_ce: 0.007585
2022-01-10 12:04:55,931 iteration 5002 : loss : 0.025997, loss_ce: 0.010659
2022-01-10 12:04:57,467 iteration 5003 : loss : 0.017360, loss_ce: 0.004980
2022-01-10 12:04:59,094 iteration 5004 : loss : 0.026868, loss_ce: 0.009449
2022-01-10 12:05:00,620 iteration 5005 : loss : 0.017045, loss_ce: 0.005670
2022-01-10 12:05:02,217 iteration 5006 : loss : 0.013621, loss_ce: 0.004182
2022-01-10 12:05:03,752 iteration 5007 : loss : 0.013045, loss_ce: 0.005013
2022-01-10 12:05:05,250 iteration 5008 : loss : 0.019534, loss_ce: 0.006681
2022-01-10 12:05:06,801 iteration 5009 : loss : 0.023384, loss_ce: 0.007084
2022-01-10 12:05:08,457 iteration 5010 : loss : 0.023942, loss_ce: 0.008861
2022-01-10 12:05:10,033 iteration 5011 : loss : 0.020578, loss_ce: 0.013108
2022-01-10 12:05:11,655 iteration 5012 : loss : 0.022121, loss_ce: 0.009737
2022-01-10 12:05:13,376 iteration 5013 : loss : 0.020887, loss_ce: 0.008476
2022-01-10 12:05:14,855 iteration 5014 : loss : 0.017083, loss_ce: 0.006990
2022-01-10 12:05:14,855 Training Data Eval:
2022-01-10 12:05:22,747   Average segmentation loss on training set: 0.0108
2022-01-10 12:05:22,747 Validation Data Eval:
2022-01-10 12:05:25,481   Average segmentation loss on validation set: 0.0656
2022-01-10 12:05:27,018 iteration 5015 : loss : 0.013406, loss_ce: 0.002660
 74%|█████████████████████▍       | 295/400 [2:24:06<53:30, 30.58s/it]2022-01-10 12:05:28,588 iteration 5016 : loss : 0.021819, loss_ce: 0.007893
2022-01-10 12:05:30,117 iteration 5017 : loss : 0.017677, loss_ce: 0.005139
2022-01-10 12:05:31,822 iteration 5018 : loss : 0.020330, loss_ce: 0.007043
2022-01-10 12:05:33,342 iteration 5019 : loss : 0.021739, loss_ce: 0.006224
2022-01-10 12:05:34,926 iteration 5020 : loss : 0.014217, loss_ce: 0.005401
2022-01-10 12:05:36,557 iteration 5021 : loss : 0.021368, loss_ce: 0.009228
2022-01-10 12:05:38,175 iteration 5022 : loss : 0.029201, loss_ce: 0.013447
2022-01-10 12:05:39,764 iteration 5023 : loss : 0.013962, loss_ce: 0.005691
2022-01-10 12:05:41,300 iteration 5024 : loss : 0.012425, loss_ce: 0.004346
2022-01-10 12:05:42,822 iteration 5025 : loss : 0.014484, loss_ce: 0.005406
2022-01-10 12:05:44,385 iteration 5026 : loss : 0.018668, loss_ce: 0.007702
2022-01-10 12:05:45,943 iteration 5027 : loss : 0.013925, loss_ce: 0.004848
2022-01-10 12:05:47,519 iteration 5028 : loss : 0.018949, loss_ce: 0.006378
2022-01-10 12:05:49,039 iteration 5029 : loss : 0.016129, loss_ce: 0.006412
2022-01-10 12:05:50,621 iteration 5030 : loss : 0.021489, loss_ce: 0.008468
2022-01-10 12:05:52,248 iteration 5031 : loss : 0.023359, loss_ce: 0.009350
2022-01-10 12:05:53,884 iteration 5032 : loss : 0.028714, loss_ce: 0.009476
 74%|█████████████████████▍       | 296/400 [2:24:32<51:04, 29.47s/it]2022-01-10 12:05:55,518 iteration 5033 : loss : 0.020917, loss_ce: 0.011278
2022-01-10 12:05:57,080 iteration 5034 : loss : 0.028025, loss_ce: 0.007136
2022-01-10 12:05:58,621 iteration 5035 : loss : 0.016151, loss_ce: 0.005584
2022-01-10 12:06:00,115 iteration 5036 : loss : 0.011844, loss_ce: 0.004299
2022-01-10 12:06:01,643 iteration 5037 : loss : 0.012736, loss_ce: 0.005123
2022-01-10 12:06:03,332 iteration 5038 : loss : 0.019852, loss_ce: 0.009050
2022-01-10 12:06:04,869 iteration 5039 : loss : 0.023542, loss_ce: 0.009289
2022-01-10 12:06:06,464 iteration 5040 : loss : 0.018515, loss_ce: 0.007943
2022-01-10 12:06:08,022 iteration 5041 : loss : 0.017362, loss_ce: 0.005875
2022-01-10 12:06:09,578 iteration 5042 : loss : 0.014767, loss_ce: 0.005980
2022-01-10 12:06:11,105 iteration 5043 : loss : 0.014979, loss_ce: 0.005045
2022-01-10 12:06:12,674 iteration 5044 : loss : 0.013440, loss_ce: 0.004514
2022-01-10 12:06:14,146 iteration 5045 : loss : 0.014857, loss_ce: 0.004653
2022-01-10 12:06:15,710 iteration 5046 : loss : 0.021600, loss_ce: 0.007721
2022-01-10 12:06:17,239 iteration 5047 : loss : 0.016714, loss_ce: 0.007297
2022-01-10 12:06:18,745 iteration 5048 : loss : 0.013657, loss_ce: 0.005224
2022-01-10 12:06:20,376 iteration 5049 : loss : 0.021174, loss_ce: 0.009075
 74%|█████████████████████▌       | 297/400 [2:24:59<49:02, 28.57s/it]2022-01-10 12:06:21,982 iteration 5050 : loss : 0.015946, loss_ce: 0.004599
2022-01-10 12:06:23,486 iteration 5051 : loss : 0.013317, loss_ce: 0.005492
2022-01-10 12:06:25,078 iteration 5052 : loss : 0.023433, loss_ce: 0.010724
2022-01-10 12:06:26,584 iteration 5053 : loss : 0.018075, loss_ce: 0.006379
2022-01-10 12:06:28,163 iteration 5054 : loss : 0.014329, loss_ce: 0.006212
2022-01-10 12:06:29,689 iteration 5055 : loss : 0.014725, loss_ce: 0.005140
2022-01-10 12:06:31,223 iteration 5056 : loss : 0.015949, loss_ce: 0.005334
2022-01-10 12:06:32,771 iteration 5057 : loss : 0.014293, loss_ce: 0.006532
2022-01-10 12:06:34,295 iteration 5058 : loss : 0.016964, loss_ce: 0.006262
2022-01-10 12:06:35,853 iteration 5059 : loss : 0.016233, loss_ce: 0.007526
2022-01-10 12:06:37,463 iteration 5060 : loss : 0.028894, loss_ce: 0.007690
2022-01-10 12:06:39,027 iteration 5061 : loss : 0.013966, loss_ce: 0.006815
2022-01-10 12:06:40,647 iteration 5062 : loss : 0.016893, loss_ce: 0.006188
2022-01-10 12:06:42,174 iteration 5063 : loss : 0.014715, loss_ce: 0.005377
2022-01-10 12:06:43,743 iteration 5064 : loss : 0.024570, loss_ce: 0.005763
2022-01-10 12:06:45,367 iteration 5065 : loss : 0.038197, loss_ce: 0.011049
2022-01-10 12:06:46,820 iteration 5066 : loss : 0.016261, loss_ce: 0.007470
 74%|█████████████████████▌       | 298/400 [2:25:25<47:29, 27.93s/it]2022-01-10 12:06:48,539 iteration 5067 : loss : 0.032863, loss_ce: 0.011413
2022-01-10 12:06:50,086 iteration 5068 : loss : 0.013677, loss_ce: 0.004318
2022-01-10 12:06:51,609 iteration 5069 : loss : 0.017284, loss_ce: 0.005694
2022-01-10 12:06:53,151 iteration 5070 : loss : 0.016676, loss_ce: 0.008155
2022-01-10 12:06:54,762 iteration 5071 : loss : 0.020674, loss_ce: 0.008485
2022-01-10 12:06:56,346 iteration 5072 : loss : 0.017545, loss_ce: 0.005697
2022-01-10 12:06:57,963 iteration 5073 : loss : 0.024054, loss_ce: 0.005910
2022-01-10 12:06:59,563 iteration 5074 : loss : 0.018171, loss_ce: 0.007412
2022-01-10 12:07:01,051 iteration 5075 : loss : 0.022351, loss_ce: 0.009431
2022-01-10 12:07:02,545 iteration 5076 : loss : 0.017805, loss_ce: 0.007898
2022-01-10 12:07:04,157 iteration 5077 : loss : 0.028807, loss_ce: 0.010531
2022-01-10 12:07:05,724 iteration 5078 : loss : 0.016676, loss_ce: 0.006022
2022-01-10 12:07:07,268 iteration 5079 : loss : 0.011266, loss_ce: 0.003957
2022-01-10 12:07:08,848 iteration 5080 : loss : 0.020281, loss_ce: 0.007290
2022-01-10 12:07:10,431 iteration 5081 : loss : 0.023802, loss_ce: 0.010357
2022-01-10 12:07:11,967 iteration 5082 : loss : 0.019769, loss_ce: 0.004959
2022-01-10 12:07:13,495 iteration 5083 : loss : 0.020017, loss_ce: 0.007563
 75%|█████████████████████▋       | 299/400 [2:25:52<46:23, 27.56s/it]2022-01-10 12:07:15,121 iteration 5084 : loss : 0.016735, loss_ce: 0.007432
2022-01-10 12:07:16,779 iteration 5085 : loss : 0.018095, loss_ce: 0.006901
2022-01-10 12:07:18,269 iteration 5086 : loss : 0.012123, loss_ce: 0.003674
2022-01-10 12:07:19,840 iteration 5087 : loss : 0.015788, loss_ce: 0.003511
2022-01-10 12:07:21,341 iteration 5088 : loss : 0.010809, loss_ce: 0.005072
2022-01-10 12:07:22,883 iteration 5089 : loss : 0.016160, loss_ce: 0.005891
2022-01-10 12:07:24,468 iteration 5090 : loss : 0.020676, loss_ce: 0.009090
2022-01-10 12:07:26,065 iteration 5091 : loss : 0.022077, loss_ce: 0.007743
2022-01-10 12:07:27,620 iteration 5092 : loss : 0.030023, loss_ce: 0.007699
2022-01-10 12:07:29,184 iteration 5093 : loss : 0.024348, loss_ce: 0.007881
2022-01-10 12:07:30,764 iteration 5094 : loss : 0.013359, loss_ce: 0.004158
2022-01-10 12:07:32,345 iteration 5095 : loss : 0.014968, loss_ce: 0.006719
2022-01-10 12:07:33,981 iteration 5096 : loss : 0.017808, loss_ce: 0.004765
2022-01-10 12:07:35,591 iteration 5097 : loss : 0.028926, loss_ce: 0.017603
2022-01-10 12:07:37,153 iteration 5098 : loss : 0.016825, loss_ce: 0.007037
2022-01-10 12:07:38,754 iteration 5099 : loss : 0.021699, loss_ce: 0.007381
2022-01-10 12:07:38,755 Training Data Eval:
2022-01-10 12:07:46,659   Average segmentation loss on training set: 0.0101
2022-01-10 12:07:46,659 Validation Data Eval:
2022-01-10 12:07:49,387   Average segmentation loss on validation set: 0.0642
2022-01-10 12:07:50,916 iteration 5100 : loss : 0.017598, loss_ce: 0.005491
 75%|█████████████████████▊       | 300/400 [2:26:29<50:51, 30.52s/it]2022-01-10 12:07:52,559 iteration 5101 : loss : 0.018381, loss_ce: 0.005148
2022-01-10 12:07:54,194 iteration 5102 : loss : 0.017525, loss_ce: 0.008058
2022-01-10 12:07:55,719 iteration 5103 : loss : 0.017297, loss_ce: 0.006666
2022-01-10 12:07:57,280 iteration 5104 : loss : 0.019220, loss_ce: 0.005837
2022-01-10 12:07:58,814 iteration 5105 : loss : 0.015994, loss_ce: 0.005460
2022-01-10 12:08:00,376 iteration 5106 : loss : 0.026373, loss_ce: 0.011447
2022-01-10 12:08:01,954 iteration 5107 : loss : 0.017831, loss_ce: 0.006807
2022-01-10 12:08:03,542 iteration 5108 : loss : 0.018837, loss_ce: 0.007778
2022-01-10 12:08:05,103 iteration 5109 : loss : 0.014041, loss_ce: 0.005751
2022-01-10 12:08:06,573 iteration 5110 : loss : 0.014400, loss_ce: 0.005223
2022-01-10 12:08:08,153 iteration 5111 : loss : 0.022735, loss_ce: 0.008360
2022-01-10 12:08:09,662 iteration 5112 : loss : 0.013023, loss_ce: 0.005305
2022-01-10 12:08:11,292 iteration 5113 : loss : 0.021725, loss_ce: 0.009147
2022-01-10 12:08:12,808 iteration 5114 : loss : 0.012576, loss_ce: 0.005140
2022-01-10 12:08:14,334 iteration 5115 : loss : 0.020563, loss_ce: 0.010167
2022-01-10 12:08:15,903 iteration 5116 : loss : 0.016387, loss_ce: 0.005886
2022-01-10 12:08:17,476 iteration 5117 : loss : 0.017154, loss_ce: 0.005985
 75%|█████████████████████▊       | 301/400 [2:26:56<48:23, 29.33s/it]2022-01-10 12:08:19,109 iteration 5118 : loss : 0.020382, loss_ce: 0.009956
2022-01-10 12:08:20,677 iteration 5119 : loss : 0.027226, loss_ce: 0.008896
2022-01-10 12:08:22,325 iteration 5120 : loss : 0.012651, loss_ce: 0.003458
2022-01-10 12:08:23,852 iteration 5121 : loss : 0.016479, loss_ce: 0.006312
2022-01-10 12:08:25,387 iteration 5122 : loss : 0.020346, loss_ce: 0.005647
2022-01-10 12:08:26,944 iteration 5123 : loss : 0.018011, loss_ce: 0.009767
2022-01-10 12:08:28,465 iteration 5124 : loss : 0.017673, loss_ce: 0.004528
2022-01-10 12:08:30,036 iteration 5125 : loss : 0.018905, loss_ce: 0.007028
2022-01-10 12:08:31,570 iteration 5126 : loss : 0.014549, loss_ce: 0.005854
2022-01-10 12:08:33,152 iteration 5127 : loss : 0.023697, loss_ce: 0.009504
2022-01-10 12:08:34,815 iteration 5128 : loss : 0.024139, loss_ce: 0.009394
2022-01-10 12:08:36,444 iteration 5129 : loss : 0.023239, loss_ce: 0.008309
2022-01-10 12:08:37,947 iteration 5130 : loss : 0.015314, loss_ce: 0.005911
2022-01-10 12:08:39,536 iteration 5131 : loss : 0.013621, loss_ce: 0.004024
2022-01-10 12:08:41,115 iteration 5132 : loss : 0.019842, loss_ce: 0.009495
2022-01-10 12:08:42,633 iteration 5133 : loss : 0.014026, loss_ce: 0.005800
2022-01-10 12:08:44,140 iteration 5134 : loss : 0.013393, loss_ce: 0.004493
 76%|█████████████████████▉       | 302/400 [2:27:23<46:36, 28.53s/it]2022-01-10 12:08:45,747 iteration 5135 : loss : 0.015924, loss_ce: 0.006144
2022-01-10 12:08:47,391 iteration 5136 : loss : 0.019651, loss_ce: 0.007579
2022-01-10 12:08:49,028 iteration 5137 : loss : 0.025201, loss_ce: 0.007993
2022-01-10 12:08:50,685 iteration 5138 : loss : 0.031098, loss_ce: 0.015516
2022-01-10 12:08:52,186 iteration 5139 : loss : 0.011172, loss_ce: 0.003606
2022-01-10 12:08:53,804 iteration 5140 : loss : 0.019791, loss_ce: 0.008204
2022-01-10 12:08:55,310 iteration 5141 : loss : 0.014987, loss_ce: 0.007463
2022-01-10 12:08:56,963 iteration 5142 : loss : 0.018367, loss_ce: 0.006213
2022-01-10 12:08:58,471 iteration 5143 : loss : 0.012954, loss_ce: 0.005339
2022-01-10 12:08:59,970 iteration 5144 : loss : 0.012713, loss_ce: 0.005641
2022-01-10 12:09:01,553 iteration 5145 : loss : 0.020137, loss_ce: 0.006939
2022-01-10 12:09:03,245 iteration 5146 : loss : 0.030124, loss_ce: 0.008125
2022-01-10 12:09:04,821 iteration 5147 : loss : 0.013621, loss_ce: 0.006200
2022-01-10 12:09:06,414 iteration 5148 : loss : 0.020210, loss_ce: 0.007083
2022-01-10 12:09:07,950 iteration 5149 : loss : 0.017547, loss_ce: 0.005522
2022-01-10 12:09:09,486 iteration 5150 : loss : 0.015153, loss_ce: 0.006050
2022-01-10 12:09:10,987 iteration 5151 : loss : 0.014900, loss_ce: 0.004688
 76%|█████████████████████▉       | 303/400 [2:27:50<45:18, 28.02s/it]2022-01-10 12:09:12,643 iteration 5152 : loss : 0.024995, loss_ce: 0.008330
2022-01-10 12:09:14,143 iteration 5153 : loss : 0.015630, loss_ce: 0.008652
2022-01-10 12:09:15,752 iteration 5154 : loss : 0.024049, loss_ce: 0.008838
2022-01-10 12:09:17,254 iteration 5155 : loss : 0.010663, loss_ce: 0.003818
2022-01-10 12:09:18,795 iteration 5156 : loss : 0.019939, loss_ce: 0.009566
2022-01-10 12:09:20,414 iteration 5157 : loss : 0.013350, loss_ce: 0.005124
2022-01-10 12:09:21,998 iteration 5158 : loss : 0.016413, loss_ce: 0.006035
2022-01-10 12:09:23,612 iteration 5159 : loss : 0.024061, loss_ce: 0.007802
2022-01-10 12:09:25,236 iteration 5160 : loss : 0.015208, loss_ce: 0.006058
2022-01-10 12:09:26,854 iteration 5161 : loss : 0.013986, loss_ce: 0.005446
2022-01-10 12:09:28,385 iteration 5162 : loss : 0.015180, loss_ce: 0.004541
2022-01-10 12:09:29,935 iteration 5163 : loss : 0.017024, loss_ce: 0.005305
2022-01-10 12:09:31,438 iteration 5164 : loss : 0.016234, loss_ce: 0.008460
2022-01-10 12:09:33,004 iteration 5165 : loss : 0.013171, loss_ce: 0.003873
2022-01-10 12:09:34,579 iteration 5166 : loss : 0.016710, loss_ce: 0.007100
2022-01-10 12:09:36,096 iteration 5167 : loss : 0.018946, loss_ce: 0.007760
2022-01-10 12:09:37,710 iteration 5168 : loss : 0.022123, loss_ce: 0.009067
 76%|██████████████████████       | 304/400 [2:28:16<44:12, 27.63s/it]2022-01-10 12:09:39,289 iteration 5169 : loss : 0.015890, loss_ce: 0.005774
2022-01-10 12:09:40,860 iteration 5170 : loss : 0.014997, loss_ce: 0.007162
2022-01-10 12:09:42,396 iteration 5171 : loss : 0.017851, loss_ce: 0.004373
2022-01-10 12:09:43,970 iteration 5172 : loss : 0.015914, loss_ce: 0.005399
2022-01-10 12:09:45,502 iteration 5173 : loss : 0.018948, loss_ce: 0.010912
2022-01-10 12:09:47,090 iteration 5174 : loss : 0.019830, loss_ce: 0.007818
2022-01-10 12:09:48,733 iteration 5175 : loss : 0.023359, loss_ce: 0.008902
2022-01-10 12:09:50,294 iteration 5176 : loss : 0.019294, loss_ce: 0.007186
2022-01-10 12:09:51,977 iteration 5177 : loss : 0.029865, loss_ce: 0.012978
2022-01-10 12:09:53,526 iteration 5178 : loss : 0.023204, loss_ce: 0.003968
2022-01-10 12:09:55,145 iteration 5179 : loss : 0.014550, loss_ce: 0.006202
2022-01-10 12:09:56,716 iteration 5180 : loss : 0.018384, loss_ce: 0.007028
2022-01-10 12:09:58,325 iteration 5181 : loss : 0.022949, loss_ce: 0.009738
2022-01-10 12:09:59,939 iteration 5182 : loss : 0.020651, loss_ce: 0.007804
2022-01-10 12:10:01,558 iteration 5183 : loss : 0.020826, loss_ce: 0.006214
2022-01-10 12:10:03,216 iteration 5184 : loss : 0.017938, loss_ce: 0.009242
2022-01-10 12:10:03,217 Training Data Eval:
2022-01-10 12:10:11,115   Average segmentation loss on training set: 0.0110
2022-01-10 12:10:11,115 Validation Data Eval:
2022-01-10 12:10:13,844   Average segmentation loss on validation set: 0.0618
2022-01-10 12:10:19,570 Found new lowest validation loss at iteration 5184! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 12:10:21,075 iteration 5185 : loss : 0.015826, loss_ce: 0.007223
 76%|██████████████████████       | 305/400 [2:29:00<51:13, 32.36s/it]2022-01-10 12:10:22,674 iteration 5186 : loss : 0.015712, loss_ce: 0.007579
2022-01-10 12:10:24,290 iteration 5187 : loss : 0.017668, loss_ce: 0.007079
2022-01-10 12:10:25,787 iteration 5188 : loss : 0.018910, loss_ce: 0.005615
2022-01-10 12:10:27,353 iteration 5189 : loss : 0.015554, loss_ce: 0.006782
2022-01-10 12:10:28,880 iteration 5190 : loss : 0.018587, loss_ce: 0.008457
2022-01-10 12:10:30,394 iteration 5191 : loss : 0.018690, loss_ce: 0.007104
2022-01-10 12:10:32,000 iteration 5192 : loss : 0.015218, loss_ce: 0.006333
2022-01-10 12:10:33,580 iteration 5193 : loss : 0.021298, loss_ce: 0.009884
2022-01-10 12:10:35,143 iteration 5194 : loss : 0.024655, loss_ce: 0.009198
2022-01-10 12:10:36,725 iteration 5195 : loss : 0.028619, loss_ce: 0.008616
2022-01-10 12:10:38,412 iteration 5196 : loss : 0.021696, loss_ce: 0.006502
2022-01-10 12:10:39,891 iteration 5197 : loss : 0.015961, loss_ce: 0.004551
2022-01-10 12:10:41,504 iteration 5198 : loss : 0.025213, loss_ce: 0.009480
2022-01-10 12:10:43,117 iteration 5199 : loss : 0.012502, loss_ce: 0.004404
2022-01-10 12:10:44,715 iteration 5200 : loss : 0.017565, loss_ce: 0.006851
2022-01-10 12:10:46,256 iteration 5201 : loss : 0.024867, loss_ce: 0.007904
2022-01-10 12:10:47,820 iteration 5202 : loss : 0.019582, loss_ce: 0.007552
 76%|██████████████████████▏      | 306/400 [2:29:26<48:03, 30.67s/it]2022-01-10 12:10:49,352 iteration 5203 : loss : 0.014557, loss_ce: 0.004126
2022-01-10 12:10:50,948 iteration 5204 : loss : 0.029238, loss_ce: 0.012143
2022-01-10 12:10:52,527 iteration 5205 : loss : 0.020869, loss_ce: 0.009404
2022-01-10 12:10:54,033 iteration 5206 : loss : 0.013251, loss_ce: 0.005611
2022-01-10 12:10:55,581 iteration 5207 : loss : 0.016907, loss_ce: 0.007591
2022-01-10 12:10:57,213 iteration 5208 : loss : 0.018543, loss_ce: 0.006853
2022-01-10 12:10:58,797 iteration 5209 : loss : 0.019600, loss_ce: 0.007269
2022-01-10 12:11:00,344 iteration 5210 : loss : 0.014764, loss_ce: 0.004514
2022-01-10 12:11:01,868 iteration 5211 : loss : 0.016695, loss_ce: 0.004205
2022-01-10 12:11:03,437 iteration 5212 : loss : 0.032428, loss_ce: 0.011185
2022-01-10 12:11:05,032 iteration 5213 : loss : 0.031647, loss_ce: 0.008240
2022-01-10 12:11:06,586 iteration 5214 : loss : 0.024378, loss_ce: 0.016421
2022-01-10 12:11:08,127 iteration 5215 : loss : 0.021341, loss_ce: 0.008196
2022-01-10 12:11:09,676 iteration 5216 : loss : 0.016385, loss_ce: 0.006988
2022-01-10 12:11:11,191 iteration 5217 : loss : 0.014863, loss_ce: 0.005134
2022-01-10 12:11:12,732 iteration 5218 : loss : 0.018981, loss_ce: 0.005909
2022-01-10 12:11:14,315 iteration 5219 : loss : 0.020240, loss_ce: 0.008467
 77%|██████████████████████▎      | 307/400 [2:29:53<45:35, 29.42s/it]2022-01-10 12:11:16,012 iteration 5220 : loss : 0.020018, loss_ce: 0.007358
2022-01-10 12:11:17,553 iteration 5221 : loss : 0.012315, loss_ce: 0.004879
2022-01-10 12:11:19,036 iteration 5222 : loss : 0.014197, loss_ce: 0.003688
2022-01-10 12:11:20,580 iteration 5223 : loss : 0.017811, loss_ce: 0.006906
2022-01-10 12:11:22,212 iteration 5224 : loss : 0.033141, loss_ce: 0.011053
2022-01-10 12:11:23,711 iteration 5225 : loss : 0.016764, loss_ce: 0.004319
2022-01-10 12:11:25,401 iteration 5226 : loss : 0.026724, loss_ce: 0.010622
2022-01-10 12:11:26,877 iteration 5227 : loss : 0.013537, loss_ce: 0.004777
2022-01-10 12:11:28,361 iteration 5228 : loss : 0.015622, loss_ce: 0.007076
2022-01-10 12:11:29,877 iteration 5229 : loss : 0.018858, loss_ce: 0.007899
2022-01-10 12:11:31,455 iteration 5230 : loss : 0.022970, loss_ce: 0.012956
2022-01-10 12:11:33,044 iteration 5231 : loss : 0.026974, loss_ce: 0.007582
2022-01-10 12:11:34,564 iteration 5232 : loss : 0.015132, loss_ce: 0.006756
2022-01-10 12:11:36,160 iteration 5233 : loss : 0.024908, loss_ce: 0.012412
2022-01-10 12:11:37,635 iteration 5234 : loss : 0.011753, loss_ce: 0.004002
2022-01-10 12:11:39,278 iteration 5235 : loss : 0.025271, loss_ce: 0.009481
2022-01-10 12:11:40,826 iteration 5236 : loss : 0.024786, loss_ce: 0.009172
 77%|██████████████████████▎      | 308/400 [2:30:19<43:46, 28.54s/it]2022-01-10 12:11:42,469 iteration 5237 : loss : 0.017325, loss_ce: 0.004949
2022-01-10 12:11:44,038 iteration 5238 : loss : 0.017826, loss_ce: 0.008727
2022-01-10 12:11:45,606 iteration 5239 : loss : 0.017740, loss_ce: 0.005000
2022-01-10 12:11:47,262 iteration 5240 : loss : 0.018125, loss_ce: 0.005603
2022-01-10 12:11:48,865 iteration 5241 : loss : 0.022038, loss_ce: 0.009242
2022-01-10 12:11:50,464 iteration 5242 : loss : 0.022476, loss_ce: 0.009191
2022-01-10 12:11:52,043 iteration 5243 : loss : 0.018759, loss_ce: 0.006752
2022-01-10 12:11:53,636 iteration 5244 : loss : 0.019229, loss_ce: 0.007016
2022-01-10 12:11:55,208 iteration 5245 : loss : 0.018601, loss_ce: 0.006225
2022-01-10 12:11:56,673 iteration 5246 : loss : 0.011022, loss_ce: 0.004979
2022-01-10 12:11:58,234 iteration 5247 : loss : 0.016574, loss_ce: 0.005711
2022-01-10 12:11:59,831 iteration 5248 : loss : 0.015165, loss_ce: 0.005125
2022-01-10 12:12:01,376 iteration 5249 : loss : 0.022722, loss_ce: 0.008348
2022-01-10 12:12:02,941 iteration 5250 : loss : 0.017588, loss_ce: 0.007582
2022-01-10 12:12:04,560 iteration 5251 : loss : 0.016942, loss_ce: 0.006220
2022-01-10 12:12:06,116 iteration 5252 : loss : 0.016641, loss_ce: 0.005602
2022-01-10 12:12:07,708 iteration 5253 : loss : 0.012924, loss_ce: 0.005329
 77%|██████████████████████▍      | 309/400 [2:30:46<42:32, 28.05s/it]2022-01-10 12:12:09,263 iteration 5254 : loss : 0.012512, loss_ce: 0.004543
2022-01-10 12:12:10,877 iteration 5255 : loss : 0.013444, loss_ce: 0.004163
2022-01-10 12:12:12,459 iteration 5256 : loss : 0.015048, loss_ce: 0.006601
2022-01-10 12:12:14,104 iteration 5257 : loss : 0.023867, loss_ce: 0.005888
2022-01-10 12:12:15,625 iteration 5258 : loss : 0.020667, loss_ce: 0.007337
2022-01-10 12:12:17,218 iteration 5259 : loss : 0.017038, loss_ce: 0.004443
2022-01-10 12:12:18,757 iteration 5260 : loss : 0.016267, loss_ce: 0.006264
2022-01-10 12:12:20,315 iteration 5261 : loss : 0.014556, loss_ce: 0.006617
2022-01-10 12:12:21,814 iteration 5262 : loss : 0.009787, loss_ce: 0.004547
2022-01-10 12:12:23,382 iteration 5263 : loss : 0.029503, loss_ce: 0.010204
2022-01-10 12:12:24,920 iteration 5264 : loss : 0.012415, loss_ce: 0.004326
2022-01-10 12:12:26,410 iteration 5265 : loss : 0.012779, loss_ce: 0.004422
2022-01-10 12:12:27,977 iteration 5266 : loss : 0.016712, loss_ce: 0.007126
2022-01-10 12:12:29,483 iteration 5267 : loss : 0.019401, loss_ce: 0.007893
2022-01-10 12:12:31,106 iteration 5268 : loss : 0.018649, loss_ce: 0.006900
2022-01-10 12:12:32,746 iteration 5269 : loss : 0.020949, loss_ce: 0.009632
2022-01-10 12:12:32,747 Training Data Eval:
2022-01-10 12:12:40,663   Average segmentation loss on training set: 0.0098
2022-01-10 12:12:40,663 Validation Data Eval:
2022-01-10 12:12:43,390   Average segmentation loss on validation set: 0.0705
2022-01-10 12:12:44,998 iteration 5270 : loss : 0.036717, loss_ce: 0.015506
 78%|██████████████████████▍      | 310/400 [2:31:24<46:13, 30.82s/it]2022-01-10 12:12:46,653 iteration 5271 : loss : 0.018277, loss_ce: 0.009095
2022-01-10 12:12:48,222 iteration 5272 : loss : 0.026635, loss_ce: 0.012239
2022-01-10 12:12:49,780 iteration 5273 : loss : 0.013493, loss_ce: 0.005689
2022-01-10 12:12:51,296 iteration 5274 : loss : 0.015122, loss_ce: 0.005000
2022-01-10 12:12:52,883 iteration 5275 : loss : 0.047863, loss_ce: 0.013880
2022-01-10 12:12:54,464 iteration 5276 : loss : 0.020849, loss_ce: 0.007008
2022-01-10 12:12:55,961 iteration 5277 : loss : 0.024659, loss_ce: 0.007709
2022-01-10 12:12:57,644 iteration 5278 : loss : 0.024633, loss_ce: 0.010905
2022-01-10 12:12:59,197 iteration 5279 : loss : 0.017420, loss_ce: 0.005761
2022-01-10 12:13:00,829 iteration 5280 : loss : 0.021508, loss_ce: 0.009205
2022-01-10 12:13:02,400 iteration 5281 : loss : 0.018972, loss_ce: 0.010575
2022-01-10 12:13:03,996 iteration 5282 : loss : 0.021807, loss_ce: 0.009386
2022-01-10 12:13:05,560 iteration 5283 : loss : 0.018403, loss_ce: 0.006313
2022-01-10 12:13:07,152 iteration 5284 : loss : 0.014279, loss_ce: 0.006194
2022-01-10 12:13:08,738 iteration 5285 : loss : 0.027228, loss_ce: 0.012009
2022-01-10 12:13:10,355 iteration 5286 : loss : 0.014615, loss_ce: 0.005183
2022-01-10 12:13:12,025 iteration 5287 : loss : 0.023256, loss_ce: 0.010334
 78%|██████████████████████▌      | 311/400 [2:31:51<44:01, 29.68s/it]2022-01-10 12:13:13,605 iteration 5288 : loss : 0.017066, loss_ce: 0.006566
2022-01-10 12:13:15,179 iteration 5289 : loss : 0.015900, loss_ce: 0.005574
2022-01-10 12:13:16,795 iteration 5290 : loss : 0.019532, loss_ce: 0.008457
2022-01-10 12:13:18,414 iteration 5291 : loss : 0.016402, loss_ce: 0.007310
2022-01-10 12:13:20,018 iteration 5292 : loss : 0.013256, loss_ce: 0.004992
2022-01-10 12:13:21,567 iteration 5293 : loss : 0.020765, loss_ce: 0.006440
2022-01-10 12:13:23,205 iteration 5294 : loss : 0.021095, loss_ce: 0.010172
2022-01-10 12:13:24,736 iteration 5295 : loss : 0.013410, loss_ce: 0.004864
2022-01-10 12:13:26,307 iteration 5296 : loss : 0.016659, loss_ce: 0.004953
2022-01-10 12:13:27,953 iteration 5297 : loss : 0.023488, loss_ce: 0.007785
2022-01-10 12:13:29,473 iteration 5298 : loss : 0.014375, loss_ce: 0.006738
2022-01-10 12:13:31,102 iteration 5299 : loss : 0.022123, loss_ce: 0.010952
2022-01-10 12:13:32,597 iteration 5300 : loss : 0.014487, loss_ce: 0.005905
2022-01-10 12:13:34,168 iteration 5301 : loss : 0.036582, loss_ce: 0.012267
2022-01-10 12:13:35,703 iteration 5302 : loss : 0.014793, loss_ce: 0.006995
2022-01-10 12:13:37,163 iteration 5303 : loss : 0.013485, loss_ce: 0.004971
2022-01-10 12:13:38,772 iteration 5304 : loss : 0.022282, loss_ce: 0.009253
 78%|██████████████████████▌      | 312/400 [2:32:17<42:14, 28.80s/it]2022-01-10 12:13:40,400 iteration 5305 : loss : 0.020612, loss_ce: 0.006884
2022-01-10 12:13:41,992 iteration 5306 : loss : 0.024213, loss_ce: 0.009226
2022-01-10 12:13:43,558 iteration 5307 : loss : 0.017380, loss_ce: 0.006690
2022-01-10 12:13:45,158 iteration 5308 : loss : 0.018505, loss_ce: 0.007693
2022-01-10 12:13:46,719 iteration 5309 : loss : 0.014715, loss_ce: 0.005233
2022-01-10 12:13:48,280 iteration 5310 : loss : 0.014213, loss_ce: 0.006148
2022-01-10 12:13:49,857 iteration 5311 : loss : 0.014331, loss_ce: 0.005069
2022-01-10 12:13:51,422 iteration 5312 : loss : 0.015928, loss_ce: 0.005142
2022-01-10 12:13:53,026 iteration 5313 : loss : 0.018723, loss_ce: 0.005411
2022-01-10 12:13:54,552 iteration 5314 : loss : 0.013749, loss_ce: 0.006020
2022-01-10 12:13:56,150 iteration 5315 : loss : 0.020603, loss_ce: 0.009501
2022-01-10 12:13:57,606 iteration 5316 : loss : 0.011199, loss_ce: 0.005228
2022-01-10 12:13:59,181 iteration 5317 : loss : 0.015916, loss_ce: 0.005793
2022-01-10 12:14:00,778 iteration 5318 : loss : 0.017635, loss_ce: 0.005846
2022-01-10 12:14:02,388 iteration 5319 : loss : 0.029284, loss_ce: 0.011527
2022-01-10 12:14:03,996 iteration 5320 : loss : 0.028101, loss_ce: 0.013857
2022-01-10 12:14:05,504 iteration 5321 : loss : 0.023510, loss_ce: 0.005357
 78%|██████████████████████▋      | 313/400 [2:32:44<40:51, 28.18s/it]2022-01-10 12:14:07,173 iteration 5322 : loss : 0.020850, loss_ce: 0.009346
2022-01-10 12:14:08,812 iteration 5323 : loss : 0.021332, loss_ce: 0.006919
2022-01-10 12:14:10,392 iteration 5324 : loss : 0.015187, loss_ce: 0.006935
2022-01-10 12:14:11,974 iteration 5325 : loss : 0.020453, loss_ce: 0.008437
2022-01-10 12:14:13,626 iteration 5326 : loss : 0.024575, loss_ce: 0.006154
2022-01-10 12:14:15,221 iteration 5327 : loss : 0.017226, loss_ce: 0.006473
2022-01-10 12:14:16,782 iteration 5328 : loss : 0.019226, loss_ce: 0.007785
2022-01-10 12:14:18,353 iteration 5329 : loss : 0.019766, loss_ce: 0.007600
2022-01-10 12:14:19,920 iteration 5330 : loss : 0.025897, loss_ce: 0.009881
2022-01-10 12:14:21,427 iteration 5331 : loss : 0.010888, loss_ce: 0.004418
2022-01-10 12:14:23,166 iteration 5332 : loss : 0.027297, loss_ce: 0.011578
2022-01-10 12:14:24,730 iteration 5333 : loss : 0.019449, loss_ce: 0.007305
2022-01-10 12:14:26,200 iteration 5334 : loss : 0.015956, loss_ce: 0.005052
2022-01-10 12:14:27,768 iteration 5335 : loss : 0.017487, loss_ce: 0.004977
2022-01-10 12:14:29,317 iteration 5336 : loss : 0.013771, loss_ce: 0.004924
2022-01-10 12:14:30,880 iteration 5337 : loss : 0.016961, loss_ce: 0.007125
2022-01-10 12:14:32,459 iteration 5338 : loss : 0.025527, loss_ce: 0.011983
 78%|██████████████████████▊      | 314/400 [2:33:11<39:51, 27.81s/it]2022-01-10 12:14:34,073 iteration 5339 : loss : 0.019985, loss_ce: 0.006748
2022-01-10 12:14:35,610 iteration 5340 : loss : 0.015236, loss_ce: 0.005954
2022-01-10 12:14:37,246 iteration 5341 : loss : 0.022554, loss_ce: 0.006610
2022-01-10 12:14:38,850 iteration 5342 : loss : 0.031685, loss_ce: 0.011444
2022-01-10 12:14:40,437 iteration 5343 : loss : 0.017084, loss_ce: 0.006502
2022-01-10 12:14:41,962 iteration 5344 : loss : 0.015272, loss_ce: 0.006674
2022-01-10 12:14:43,502 iteration 5345 : loss : 0.014112, loss_ce: 0.005534
2022-01-10 12:14:45,032 iteration 5346 : loss : 0.017868, loss_ce: 0.006517
2022-01-10 12:14:46,608 iteration 5347 : loss : 0.014486, loss_ce: 0.006128
2022-01-10 12:14:48,122 iteration 5348 : loss : 0.017245, loss_ce: 0.006012
2022-01-10 12:14:49,650 iteration 5349 : loss : 0.014094, loss_ce: 0.004890
2022-01-10 12:14:51,198 iteration 5350 : loss : 0.014007, loss_ce: 0.004350
2022-01-10 12:14:52,702 iteration 5351 : loss : 0.015014, loss_ce: 0.005516
2022-01-10 12:14:54,318 iteration 5352 : loss : 0.017599, loss_ce: 0.006626
2022-01-10 12:14:55,893 iteration 5353 : loss : 0.015447, loss_ce: 0.005950
2022-01-10 12:14:57,406 iteration 5354 : loss : 0.014339, loss_ce: 0.005884
2022-01-10 12:14:57,407 Training Data Eval:
2022-01-10 12:15:05,305   Average segmentation loss on training set: 0.0099
2022-01-10 12:15:05,305 Validation Data Eval:
2022-01-10 12:15:08,030   Average segmentation loss on validation set: 0.0609
2022-01-10 12:15:13,811 Found new lowest validation loss at iteration 5354! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed100.pth
2022-01-10 12:15:15,312 iteration 5355 : loss : 0.025553, loss_ce: 0.013939
 79%|██████████████████████▊      | 315/400 [2:33:54<45:47, 32.33s/it]2022-01-10 12:15:16,960 iteration 5356 : loss : 0.020157, loss_ce: 0.008519
2022-01-10 12:15:18,485 iteration 5357 : loss : 0.009656, loss_ce: 0.003373
2022-01-10 12:15:20,002 iteration 5358 : loss : 0.014384, loss_ce: 0.006308
2022-01-10 12:15:21,517 iteration 5359 : loss : 0.013151, loss_ce: 0.004168
2022-01-10 12:15:23,117 iteration 5360 : loss : 0.017155, loss_ce: 0.006591
2022-01-10 12:15:24,658 iteration 5361 : loss : 0.015255, loss_ce: 0.006930
2022-01-10 12:15:26,236 iteration 5362 : loss : 0.014643, loss_ce: 0.005043
2022-01-10 12:15:27,941 iteration 5363 : loss : 0.023846, loss_ce: 0.010158
2022-01-10 12:15:29,501 iteration 5364 : loss : 0.027976, loss_ce: 0.009855
2022-01-10 12:15:31,086 iteration 5365 : loss : 0.020254, loss_ce: 0.006738
2022-01-10 12:15:32,628 iteration 5366 : loss : 0.018861, loss_ce: 0.006922
2022-01-10 12:15:34,097 iteration 5367 : loss : 0.011673, loss_ce: 0.004052
2022-01-10 12:15:35,756 iteration 5368 : loss : 0.036240, loss_ce: 0.013417
2022-01-10 12:15:37,291 iteration 5369 : loss : 0.016497, loss_ce: 0.009280
2022-01-10 12:15:38,824 iteration 5370 : loss : 0.013549, loss_ce: 0.006153
2022-01-10 12:15:40,368 iteration 5371 : loss : 0.015648, loss_ce: 0.004562
2022-01-10 12:15:41,843 iteration 5372 : loss : 0.011700, loss_ce: 0.004635
 79%|██████████████████████▉      | 316/400 [2:34:20<42:49, 30.59s/it]2022-01-10 12:15:43,439 iteration 5373 : loss : 0.019197, loss_ce: 0.007657
2022-01-10 12:15:44,965 iteration 5374 : loss : 0.028165, loss_ce: 0.006387
2022-01-10 12:15:46,530 iteration 5375 : loss : 0.015348, loss_ce: 0.005834
2022-01-10 12:15:48,097 iteration 5376 : loss : 0.022213, loss_ce: 0.007282
2022-01-10 12:15:49,676 iteration 5377 : loss : 0.018724, loss_ce: 0.004519
2022-01-10 12:15:51,251 iteration 5378 : loss : 0.025775, loss_ce: 0.008833
2022-01-10 12:15:52,796 iteration 5379 : loss : 0.015709, loss_ce: 0.006185
2022-01-10 12:15:54,286 iteration 5380 : loss : 0.014393, loss_ce: 0.006386
2022-01-10 12:15:55,909 iteration 5381 : loss : 0.016224, loss_ce: 0.006826
2022-01-10 12:15:57,470 iteration 5382 : loss : 0.018615, loss_ce: 0.006690
2022-01-10 12:15:59,018 iteration 5383 : loss : 0.023677, loss_ce: 0.011647
2022-01-10 12:16:00,554 iteration 5384 : loss : 0.011706, loss_ce: 0.005478
2022-01-10 12:16:02,131 iteration 5385 : loss : 0.019600, loss_ce: 0.007547
2022-01-10 12:16:03,654 iteration 5386 : loss : 0.018204, loss_ce: 0.006989
2022-01-10 12:16:05,305 iteration 5387 : loss : 0.016987, loss_ce: 0.005940
2022-01-10 12:16:06,900 iteration 5388 : loss : 0.015718, loss_ce: 0.004928
2022-01-10 12:16:08,537 iteration 5389 : loss : 0.023577, loss_ce: 0.010064
 79%|██████████████████████▉      | 317/400 [2:34:47<40:41, 29.42s/it]2022-01-10 12:16:10,126 iteration 5390 : loss : 0.012705, loss_ce: 0.004294
2022-01-10 12:16:11,767 iteration 5391 : loss : 0.027566, loss_ce: 0.009193
2022-01-10 12:16:13,316 iteration 5392 : loss : 0.021340, loss_ce: 0.006880
2022-01-10 12:16:14,835 iteration 5393 : loss : 0.014668, loss_ce: 0.005257
2022-01-10 12:16:16,385 iteration 5394 : loss : 0.015009, loss_ce: 0.004912
2022-01-10 12:16:17,940 iteration 5395 : loss : 0.021622, loss_ce: 0.007082
2022-01-10 12:16:19,468 iteration 5396 : loss : 0.015255, loss_ce: 0.006050
2022-01-10 12:16:21,064 iteration 5397 : loss : 0.016561, loss_ce: 0.007085
2022-01-10 12:16:22,763 iteration 5398 : loss : 0.032138, loss_ce: 0.011347
2022-01-10 12:16:24,369 iteration 5399 : loss : 0.019291, loss_ce: 0.006307
2022-01-10 12:16:25,906 iteration 5400 : loss : 0.017653, loss_ce: 0.007805
2022-01-10 12:16:27,495 iteration 5401 : loss : 0.015760, loss_ce: 0.008314
2022-01-10 12:16:29,035 iteration 5402 : loss : 0.020623, loss_ce: 0.010318
2022-01-10 12:16:30,597 iteration 5403 : loss : 0.015838, loss_ce: 0.006668
2022-01-10 12:16:32,187 iteration 5404 : loss : 0.012578, loss_ce: 0.005493
2022-01-10 12:16:33,857 iteration 5405 : loss : 0.021565, loss_ce: 0.006914
2022-01-10 12:16:35,375 iteration 5406 : loss : 0.012190, loss_ce: 0.005568
 80%|███████████████████████      | 318/400 [2:35:14<39:08, 28.64s/it]2022-01-10 12:16:36,970 iteration 5407 : loss : 0.015678, loss_ce: 0.006712
2022-01-10 12:16:38,512 iteration 5408 : loss : 0.014266, loss_ce: 0.005777
2022-01-10 12:16:39,996 iteration 5409 : loss : 0.013572, loss_ce: 0.004537
2022-01-10 12:16:41,566 iteration 5410 : loss : 0.017095, loss_ce: 0.005899
2022-01-10 12:16:43,077 iteration 5411 : loss : 0.011557, loss_ce: 0.004512
2022-01-10 12:16:44,652 iteration 5412 : loss : 0.018807, loss_ce: 0.007322
2022-01-10 12:16:46,187 iteration 5413 : loss : 0.012263, loss_ce: 0.005332
2022-01-10 12:16:47,755 iteration 5414 : loss : 0.011005, loss_ce: 0.004225
2022-01-10 12:16:49,412 iteration 5415 : loss : 0.024384, loss_ce: 0.010111
2022-01-10 12:16:50,936 iteration 5416 : loss : 0.015679, loss_ce: 0.005744
2022-01-10 12:16:52,489 iteration 5417 : loss : 0.012677, loss_ce: 0.003991
2022-01-10 12:16:54,127 iteration 5418 : loss : 0.015126, loss_ce: 0.006115
2022-01-10 12:16:55,629 iteration 5419 : loss : 0.015167, loss_ce: 0.006386
2022-01-10 12:16:57,128 iteration 5420 : loss : 0.013770, loss_ce: 0.005766
2022-01-10 12:16:58,728 iteration 5421 : loss : 0.019830, loss_ce: 0.006715
2022-01-10 12:17:00,352 iteration 5422 : loss : 0.023288, loss_ce: 0.004950
2022-01-10 12:17:01,969 iteration 5423 : loss : 0.019026, loss_ce: 0.008207
 80%|███████████████████████▏     | 319/400 [2:35:41<37:50, 28.03s/it]2022-01-10 12:17:03,591 iteration 5424 : loss : 0.019548, loss_ce: 0.006439
2022-01-10 12:17:05,123 iteration 5425 : loss : 0.023411, loss_ce: 0.005009
2022-01-10 12:17:06,741 iteration 5426 : loss : 0.026183, loss_ce: 0.012728
2022-01-10 12:17:08,232 iteration 5427 : loss : 0.016458, loss_ce: 0.005011
2022-01-10 12:17:09,758 iteration 5428 : loss : 0.012132, loss_ce: 0.004308
2022-01-10 12:17:11,292 iteration 5429 : loss : 0.017376, loss_ce: 0.006533
2022-01-10 12:17:12,958 iteration 5430 : loss : 0.013420, loss_ce: 0.004025
2022-01-10 12:17:14,505 iteration 5431 : loss : 0.015965, loss_ce: 0.007063
2022-01-10 12:17:16,041 iteration 5432 : loss : 0.015355, loss_ce: 0.007106
2022-01-10 12:17:17,656 iteration 5433 : loss : 0.016990, loss_ce: 0.008291
2022-01-10 12:17:19,197 iteration 5434 : loss : 0.013822, loss_ce: 0.005148
2022-01-10 12:17:20,857 iteration 5435 : loss : 0.023037, loss_ce: 0.010426
2022-01-10 12:17:22,414 iteration 5436 : loss : 0.012091, loss_ce: 0.004784
2022-01-10 12:17:23,860 iteration 5437 : loss : 0.012154, loss_ce: 0.004597
2022-01-10 12:17:25,509 iteration 5438 : loss : 0.019958, loss_ce: 0.007237
2022-01-10 12:17:27,036 iteration 5439 : loss : 0.011946, loss_ce: 0.004766
2022-01-10 12:17:27,036 Training Data Eval:
2022-01-10 12:17:34,939   Average segmentation loss on training set: 0.0091
2022-01-10 12:17:34,940 Validation Data Eval:
2022-01-10 12:17:37,669   Average segmentation loss on validation set: 0.0694
2022-01-10 12:17:39,268 iteration 5440 : loss : 0.023075, loss_ce: 0.006094
 80%|███████████████████████▏     | 320/400 [2:36:18<41:04, 30.81s/it]2022-01-10 12:17:40,781 iteration 5441 : loss : 0.010895, loss_ce: 0.003478
2022-01-10 12:17:42,336 iteration 5442 : loss : 0.021710, loss_ce: 0.008337
2022-01-10 12:17:43,922 iteration 5443 : loss : 0.011927, loss_ce: 0.003676
2022-01-10 12:17:45,501 iteration 5444 : loss : 0.014507, loss_ce: 0.005058
2022-01-10 12:17:47,150 iteration 5445 : loss : 0.031248, loss_ce: 0.014652
2022-01-10 12:17:48,678 iteration 5446 : loss : 0.016515, loss_ce: 0.005404
2022-01-10 12:17:50,224 iteration 5447 : loss : 0.016107, loss_ce: 0.006756
2022-01-10 12:17:51,802 iteration 5448 : loss : 0.018005, loss_ce: 0.006919
2022-01-10 12:17:53,352 iteration 5449 : loss : 0.016210, loss_ce: 0.005365
2022-01-10 12:17:54,969 iteration 5450 : loss : 0.016666, loss_ce: 0.008020
2022-01-10 12:17:56,526 iteration 5451 : loss : 0.027298, loss_ce: 0.011240
2022-01-10 12:17:58,143 iteration 5452 : loss : 0.015756, loss_ce: 0.005971
2022-01-10 12:17:59,722 iteration 5453 : loss : 0.012293, loss_ce: 0.004219
2022-01-10 12:18:01,185 iteration 5454 : loss : 0.010100, loss_ce: 0.004752
2022-01-10 12:18:02,848 iteration 5455 : loss : 0.023748, loss_ce: 0.008956
2022-01-10 12:18:04,473 iteration 5456 : loss : 0.017585, loss_ce: 0.007235
2022-01-10 12:18:06,036 iteration 5457 : loss : 0.015463, loss_ce: 0.005288
 80%|███████████████████████▎     | 321/400 [2:36:45<38:58, 29.60s/it]2022-01-10 12:18:07,630 iteration 5458 : loss : 0.015477, loss_ce: 0.006187
2022-01-10 12:18:09,155 iteration 5459 : loss : 0.014153, loss_ce: 0.005575
2022-01-10 12:18:10,744 iteration 5460 : loss : 0.018530, loss_ce: 0.007004
2022-01-10 12:18:12,337 iteration 5461 : loss : 0.019590, loss_ce: 0.008968
2022-01-10 12:18:13,905 iteration 5462 : loss : 0.015706, loss_ce: 0.003849
2022-01-10 12:18:15,445 iteration 5463 : loss : 0.015378, loss_ce: 0.005912
2022-01-10 12:18:16,992 iteration 5464 : loss : 0.016824, loss_ce: 0.006661
2022-01-10 12:18:18,501 iteration 5465 : loss : 0.014680, loss_ce: 0.005004
2022-01-10 12:18:20,137 iteration 5466 : loss : 0.019404, loss_ce: 0.008948
2022-01-10 12:18:21,688 iteration 5467 : loss : 0.017612, loss_ce: 0.009126
2022-01-10 12:18:23,262 iteration 5468 : loss : 0.022981, loss_ce: 0.011940
2022-01-10 12:18:24,771 iteration 5469 : loss : 0.010557, loss_ce: 0.003616
2022-01-10 12:18:26,373 iteration 5470 : loss : 0.027162, loss_ce: 0.013958
2022-01-10 12:18:27,943 iteration 5471 : loss : 0.017561, loss_ce: 0.006592
2022-01-10 12:18:29,558 iteration 5472 : loss : 0.013339, loss_ce: 0.004666
2022-01-10 12:18:31,158 iteration 5473 : loss : 0.022494, loss_ce: 0.007826
2022-01-10 12:18:32,765 iteration 5474 : loss : 0.019631, loss_ce: 0.007432
 80%|███████████████████████▎     | 322/400 [2:37:11<37:21, 28.74s/it]2022-01-10 12:18:34,375 iteration 5475 : loss : 0.015230, loss_ce: 0.004221
2022-01-10 12:18:35,914 iteration 5476 : loss : 0.019690, loss_ce: 0.005800
2022-01-10 12:18:37,444 iteration 5477 : loss : 0.012167, loss_ce: 0.004766
2022-01-10 12:18:39,001 iteration 5478 : loss : 0.019982, loss_ce: 0.009077
2022-01-10 12:18:40,654 iteration 5479 : loss : 0.014697, loss_ce: 0.006541
2022-01-10 12:18:42,284 iteration 5480 : loss : 0.018720, loss_ce: 0.007868
2022-01-10 12:18:43,849 iteration 5481 : loss : 0.016090, loss_ce: 0.005230
2022-01-10 12:18:45,407 iteration 5482 : loss : 0.015774, loss_ce: 0.005877
2022-01-10 12:18:46,981 iteration 5483 : loss : 0.015953, loss_ce: 0.006399
2022-01-10 12:18:48,516 iteration 5484 : loss : 0.017063, loss_ce: 0.007073
2022-01-10 12:18:49,984 iteration 5485 : loss : 0.012545, loss_ce: 0.004511
2022-01-10 12:18:51,622 iteration 5486 : loss : 0.020689, loss_ce: 0.009194
2022-01-10 12:18:53,325 iteration 5487 : loss : 0.021242, loss_ce: 0.008814
2022-01-10 12:18:54,890 iteration 5488 : loss : 0.018275, loss_ce: 0.007087
2022-01-10 12:18:56,449 iteration 5489 : loss : 0.014830, loss_ce: 0.005976
2022-01-10 12:18:57,979 iteration 5490 : loss : 0.025496, loss_ce: 0.007907
2022-01-10 12:18:59,525 iteration 5491 : loss : 0.013660, loss_ce: 0.005609
 81%|███████████████████████▍     | 323/400 [2:37:38<36:06, 28.14s/it]2022-01-10 12:19:01,143 iteration 5492 : loss : 0.020018, loss_ce: 0.008015
2022-01-10 12:19:02,681 iteration 5493 : loss : 0.015285, loss_ce: 0.006691
2022-01-10 12:19:04,191 iteration 5494 : loss : 0.015009, loss_ce: 0.003780
2022-01-10 12:19:05,810 iteration 5495 : loss : 0.025713, loss_ce: 0.005183
2022-01-10 12:19:07,332 iteration 5496 : loss : 0.011011, loss_ce: 0.003695
2022-01-10 12:19:08,892 iteration 5497 : loss : 0.011971, loss_ce: 0.003782
2022-01-10 12:19:10,477 iteration 5498 : loss : 0.018825, loss_ce: 0.010948
2022-01-10 12:19:12,036 iteration 5499 : loss : 0.017966, loss_ce: 0.006871
2022-01-10 12:19:13,621 iteration 5500 : loss : 0.017285, loss_ce: 0.006767
2022-01-10 12:19:15,270 iteration 5501 : loss : 0.020798, loss_ce: 0.008674
2022-01-10 12:19:16,876 iteration 5502 : loss : 0.016661, loss_ce: 0.007342
2022-01-10 12:19:18,412 iteration 5503 : loss : 0.018342, loss_ce: 0.005923
2022-01-10 12:19:19,999 iteration 5504 : loss : 0.016085, loss_ce: 0.008083
2022-01-10 12:19:21,571 iteration 5505 : loss : 0.019307, loss_ce: 0.008274
2022-01-10 12:19:23,093 iteration 5506 : loss : 0.014758, loss_ce: 0.005387
2022-01-10 12:19:24,677 iteration 5507 : loss : 0.019755, loss_ce: 0.005743
2022-01-10 12:19:26,247 iteration 5508 : loss : 0.017152, loss_ce: 0.008124
 81%|███████████████████████▍     | 324/400 [2:38:05<35:06, 27.72s/it]2022-01-10 12:19:27,853 iteration 5509 : loss : 0.012256, loss_ce: 0.003929
2022-01-10 12:19:29,537 iteration 5510 : loss : 0.024049, loss_ce: 0.007043
2022-01-10 12:19:31,110 iteration 5511 : loss : 0.020529, loss_ce: 0.011776
2022-01-10 12:19:32,735 iteration 5512 : loss : 0.015734, loss_ce: 0.002698
2022-01-10 12:19:34,264 iteration 5513 : loss : 0.015463, loss_ce: 0.003719
2022-01-10 12:19:35,872 iteration 5514 : loss : 0.014787, loss_ce: 0.006113
2022-01-10 12:19:37,515 iteration 5515 : loss : 0.020280, loss_ce: 0.006041
2022-01-10 12:19:39,001 iteration 5516 : loss : 0.010935, loss_ce: 0.004255
2022-01-10 12:19:40,594 iteration 5517 : loss : 0.030310, loss_ce: 0.009465
2022-01-10 12:19:42,141 iteration 5518 : loss : 0.020932, loss_ce: 0.009484
2022-01-10 12:19:43,675 iteration 5519 : loss : 0.033105, loss_ce: 0.010853
2022-01-10 12:19:45,264 iteration 5520 : loss : 0.024771, loss_ce: 0.010752
2022-01-10 12:19:46,948 iteration 5521 : loss : 0.018775, loss_ce: 0.006767
2022-01-10 12:19:48,454 iteration 5522 : loss : 0.013922, loss_ce: 0.006412
2022-01-10 12:19:50,027 iteration 5523 : loss : 0.015023, loss_ce: 0.006278
2022-01-10 12:19:51,617 iteration 5524 : loss : 0.021876, loss_ce: 0.010860
2022-01-10 12:19:51,618 Training Data Eval:
2022-01-10 12:19:59,531   Average segmentation loss on training set: 0.0091
2022-01-10 12:19:59,532 Validation Data Eval:
2022-01-10 12:20:02,260   Average segmentation loss on validation set: 0.0715
2022-01-10 12:20:03,871 iteration 5525 : loss : 0.026955, loss_ce: 0.012399
 81%|███████████████████████▌     | 325/400 [2:38:42<38:21, 30.69s/it]2022-01-10 12:20:05,587 iteration 5526 : loss : 0.017961, loss_ce: 0.007387
2022-01-10 12:20:07,116 iteration 5527 : loss : 0.016983, loss_ce: 0.006086
2022-01-10 12:20:08,706 iteration 5528 : loss : 0.015598, loss_ce: 0.005917
2022-01-10 12:20:10,244 iteration 5529 : loss : 0.013407, loss_ce: 0.004660
2022-01-10 12:20:11,750 iteration 5530 : loss : 0.014653, loss_ce: 0.003801
2022-01-10 12:20:13,318 iteration 5531 : loss : 0.012422, loss_ce: 0.004268
2022-01-10 12:20:14,796 iteration 5532 : loss : 0.013484, loss_ce: 0.004521
2022-01-10 12:20:16,387 iteration 5533 : loss : 0.018753, loss_ce: 0.007604
2022-01-10 12:20:17,965 iteration 5534 : loss : 0.018399, loss_ce: 0.007111
2022-01-10 12:20:19,550 iteration 5535 : loss : 0.017283, loss_ce: 0.007726
2022-01-10 12:20:21,105 iteration 5536 : loss : 0.018353, loss_ce: 0.005622
2022-01-10 12:20:22,662 iteration 5537 : loss : 0.012386, loss_ce: 0.003955
2022-01-10 12:20:24,210 iteration 5538 : loss : 0.013038, loss_ce: 0.006701
2022-01-10 12:20:25,782 iteration 5539 : loss : 0.013196, loss_ce: 0.004961
2022-01-10 12:20:27,364 iteration 5540 : loss : 0.012978, loss_ce: 0.004362
2022-01-10 12:20:28,858 iteration 5541 : loss : 0.019870, loss_ce: 0.008617
2022-01-10 12:20:30,390 iteration 5542 : loss : 0.016229, loss_ce: 0.006674
 82%|███████████████████████▋     | 326/400 [2:39:09<36:18, 29.44s/it]2022-01-10 12:20:32,065 iteration 5543 : loss : 0.032337, loss_ce: 0.013947
2022-01-10 12:20:33,731 iteration 5544 : loss : 0.021014, loss_ce: 0.006322
2022-01-10 12:20:35,356 iteration 5545 : loss : 0.016911, loss_ce: 0.005953
2022-01-10 12:20:36,851 iteration 5546 : loss : 0.010261, loss_ce: 0.004674
2022-01-10 12:20:38,441 iteration 5547 : loss : 0.015117, loss_ce: 0.006866
2022-01-10 12:20:39,967 iteration 5548 : loss : 0.017295, loss_ce: 0.005901
2022-01-10 12:20:41,515 iteration 5549 : loss : 0.016502, loss_ce: 0.007253
2022-01-10 12:20:43,100 iteration 5550 : loss : 0.019445, loss_ce: 0.008060
2022-01-10 12:20:44,683 iteration 5551 : loss : 0.013059, loss_ce: 0.003260
2022-01-10 12:20:46,341 iteration 5552 : loss : 0.026231, loss_ce: 0.007582
2022-01-10 12:20:47,933 iteration 5553 : loss : 0.018463, loss_ce: 0.007037
2022-01-10 12:20:49,586 iteration 5554 : loss : 0.018406, loss_ce: 0.004605
2022-01-10 12:20:51,219 iteration 5555 : loss : 0.019464, loss_ce: 0.008302
2022-01-10 12:20:52,832 iteration 5556 : loss : 0.013247, loss_ce: 0.005680
2022-01-10 12:20:54,439 iteration 5557 : loss : 0.019616, loss_ce: 0.005927
2022-01-10 12:20:55,991 iteration 5558 : loss : 0.022886, loss_ce: 0.007582
2022-01-10 12:20:57,491 iteration 5559 : loss : 0.014636, loss_ce: 0.005131
 82%|███████████████████████▋     | 327/400 [2:39:36<34:57, 28.74s/it]2022-01-10 12:20:59,109 iteration 5560 : loss : 0.019912, loss_ce: 0.007142
2022-01-10 12:21:00,755 iteration 5561 : loss : 0.020764, loss_ce: 0.005920
2022-01-10 12:21:02,340 iteration 5562 : loss : 0.021049, loss_ce: 0.006367
2022-01-10 12:21:03,966 iteration 5563 : loss : 0.020990, loss_ce: 0.006643
2022-01-10 12:21:05,457 iteration 5564 : loss : 0.018640, loss_ce: 0.006039
2022-01-10 12:21:07,042 iteration 5565 : loss : 0.025602, loss_ce: 0.007213
2022-01-10 12:21:08,568 iteration 5566 : loss : 0.012908, loss_ce: 0.004215
2022-01-10 12:21:10,182 iteration 5567 : loss : 0.014337, loss_ce: 0.007774
2022-01-10 12:21:11,799 iteration 5568 : loss : 0.020456, loss_ce: 0.010315
2022-01-10 12:21:13,419 iteration 5569 : loss : 0.020251, loss_ce: 0.007918
2022-01-10 12:21:14,989 iteration 5570 : loss : 0.016707, loss_ce: 0.007997
2022-01-10 12:21:16,550 iteration 5571 : loss : 0.017290, loss_ce: 0.005004
2022-01-10 12:21:18,020 iteration 5572 : loss : 0.013061, loss_ce: 0.004254
2022-01-10 12:21:19,616 iteration 5573 : loss : 0.019956, loss_ce: 0.007606
2022-01-10 12:21:21,140 iteration 5574 : loss : 0.014833, loss_ce: 0.005827
2022-01-10 12:21:22,640 iteration 5575 : loss : 0.012321, loss_ce: 0.004479
2022-01-10 12:21:24,111 iteration 5576 : loss : 0.015101, loss_ce: 0.004311
 82%|███████████████████████▊     | 328/400 [2:40:03<33:43, 28.10s/it]2022-01-10 12:21:25,782 iteration 5577 : loss : 0.024870, loss_ce: 0.008321
2022-01-10 12:21:27,352 iteration 5578 : loss : 0.012989, loss_ce: 0.004342
2022-01-10 12:21:29,011 iteration 5579 : loss : 0.017077, loss_ce: 0.004980
2022-01-10 12:21:30,635 iteration 5580 : loss : 0.015951, loss_ce: 0.004955
2022-01-10 12:21:32,227 iteration 5581 : loss : 0.020046, loss_ce: 0.009479
2022-01-10 12:21:33,783 iteration 5582 : loss : 0.016833, loss_ce: 0.006278
2022-01-10 12:21:35,300 iteration 5583 : loss : 0.010813, loss_ce: 0.003286
2022-01-10 12:21:36,842 iteration 5584 : loss : 0.014562, loss_ce: 0.005727
2022-01-10 12:21:38,338 iteration 5585 : loss : 0.014216, loss_ce: 0.004871
2022-01-10 12:21:39,852 iteration 5586 : loss : 0.011853, loss_ce: 0.004861
2022-01-10 12:21:41,486 iteration 5587 : loss : 0.019132, loss_ce: 0.008812
2022-01-10 12:21:43,044 iteration 5588 : loss : 0.021152, loss_ce: 0.008435
2022-01-10 12:21:44,669 iteration 5589 : loss : 0.025295, loss_ce: 0.008463
2022-01-10 12:21:46,173 iteration 5590 : loss : 0.013947, loss_ce: 0.005776
2022-01-10 12:21:47,760 iteration 5591 : loss : 0.022215, loss_ce: 0.007619
2022-01-10 12:21:49,342 iteration 5592 : loss : 0.020889, loss_ce: 0.008601
2022-01-10 12:21:50,824 iteration 5593 : loss : 0.011507, loss_ce: 0.005731
 82%|███████████████████████▊     | 329/400 [2:40:29<32:45, 27.68s/it]2022-01-10 12:21:52,439 iteration 5594 : loss : 0.010621, loss_ce: 0.003947
2022-01-10 12:21:53,954 iteration 5595 : loss : 0.011340, loss_ce: 0.004651
2022-01-10 12:21:55,585 iteration 5596 : loss : 0.024365, loss_ce: 0.006102
2022-01-10 12:21:57,170 iteration 5597 : loss : 0.026425, loss_ce: 0.011154
2022-01-10 12:21:58,850 iteration 5598 : loss : 0.028339, loss_ce: 0.013701
2022-01-10 12:22:00,497 iteration 5599 : loss : 0.015529, loss_ce: 0.005269
2022-01-10 12:22:02,010 iteration 5600 : loss : 0.012878, loss_ce: 0.004859
2022-01-10 12:22:03,594 iteration 5601 : loss : 0.017225, loss_ce: 0.005386
2022-01-10 12:22:05,166 iteration 5602 : loss : 0.015237, loss_ce: 0.005337
2022-01-10 12:22:06,736 iteration 5603 : loss : 0.021272, loss_ce: 0.010281
2022-01-10 12:22:08,308 iteration 5604 : loss : 0.030773, loss_ce: 0.010119
2022-01-10 12:22:09,834 iteration 5605 : loss : 0.021702, loss_ce: 0.010239
2022-01-10 12:22:11,432 iteration 5606 : loss : 0.024496, loss_ce: 0.007815
2022-01-10 12:22:12,947 iteration 5607 : loss : 0.014583, loss_ce: 0.005982
2022-01-10 12:22:14,537 iteration 5608 : loss : 0.017170, loss_ce: 0.007531
2022-01-10 12:22:16,060 iteration 5609 : loss : 0.014075, loss_ce: 0.005067
2022-01-10 12:22:16,061 Training Data Eval:
2022-01-10 12:22:23,961   Average segmentation loss on training set: 0.0090
2022-01-10 12:22:23,961 Validation Data Eval:
2022-01-10 12:22:26,699   Average segmentation loss on validation set: 0.0679
2022-01-10 12:22:28,290 iteration 5610 : loss : 0.024395, loss_ce: 0.012268
 82%|███████████████████████▉     | 330/400 [2:41:07<35:43, 30.62s/it]2022-01-10 12:22:29,857 iteration 5611 : loss : 0.010231, loss_ce: 0.003930
2022-01-10 12:22:31,472 iteration 5612 : loss : 0.019511, loss_ce: 0.005919
2022-01-10 12:22:33,005 iteration 5613 : loss : 0.019628, loss_ce: 0.003786
2022-01-10 12:22:34,545 iteration 5614 : loss : 0.016708, loss_ce: 0.006144
2022-01-10 12:22:36,102 iteration 5615 : loss : 0.016583, loss_ce: 0.007328
2022-01-10 12:22:37,660 iteration 5616 : loss : 0.013915, loss_ce: 0.004234
2022-01-10 12:22:39,367 iteration 5617 : loss : 0.028821, loss_ce: 0.009924
2022-01-10 12:22:40,984 iteration 5618 : loss : 0.021749, loss_ce: 0.008127
2022-01-10 12:22:42,558 iteration 5619 : loss : 0.013316, loss_ce: 0.005990
2022-01-10 12:22:44,073 iteration 5620 : loss : 0.014614, loss_ce: 0.005911
2022-01-10 12:22:45,576 iteration 5621 : loss : 0.020185, loss_ce: 0.007899
2022-01-10 12:22:47,046 iteration 5622 : loss : 0.012320, loss_ce: 0.005411
2022-01-10 12:22:48,658 iteration 5623 : loss : 0.016587, loss_ce: 0.005978
2022-01-10 12:22:50,205 iteration 5624 : loss : 0.013344, loss_ce: 0.004781
2022-01-10 12:22:51,751 iteration 5625 : loss : 0.018859, loss_ce: 0.009265
2022-01-10 12:22:53,337 iteration 5626 : loss : 0.017120, loss_ce: 0.005985
2022-01-10 12:22:54,890 iteration 5627 : loss : 0.013999, loss_ce: 0.005778
 83%|███████████████████████▉     | 331/400 [2:41:33<33:49, 29.41s/it]2022-01-10 12:22:56,509 iteration 5628 : loss : 0.012344, loss_ce: 0.004832
2022-01-10 12:22:58,083 iteration 5629 : loss : 0.018657, loss_ce: 0.007173
2022-01-10 12:22:59,670 iteration 5630 : loss : 0.012450, loss_ce: 0.004513
2022-01-10 12:23:01,263 iteration 5631 : loss : 0.015852, loss_ce: 0.007845
2022-01-10 12:23:02,905 iteration 5632 : loss : 0.015968, loss_ce: 0.005858
2022-01-10 12:23:04,391 iteration 5633 : loss : 0.016550, loss_ce: 0.003994
2022-01-10 12:23:06,014 iteration 5634 : loss : 0.028691, loss_ce: 0.006382
2022-01-10 12:23:07,575 iteration 5635 : loss : 0.012783, loss_ce: 0.005146
2022-01-10 12:23:09,071 iteration 5636 : loss : 0.022210, loss_ce: 0.007068
2022-01-10 12:23:10,711 iteration 5637 : loss : 0.017437, loss_ce: 0.007049
2022-01-10 12:23:12,273 iteration 5638 : loss : 0.016260, loss_ce: 0.007645
2022-01-10 12:23:13,852 iteration 5639 : loss : 0.015705, loss_ce: 0.004895
2022-01-10 12:23:15,420 iteration 5640 : loss : 0.015603, loss_ce: 0.005436
2022-01-10 12:23:16,937 iteration 5641 : loss : 0.011190, loss_ce: 0.004035
2022-01-10 12:23:18,605 iteration 5642 : loss : 0.017207, loss_ce: 0.008374
2022-01-10 12:23:20,090 iteration 5643 : loss : 0.013351, loss_ce: 0.006077
2022-01-10 12:23:21,636 iteration 5644 : loss : 0.015332, loss_ce: 0.006682
 83%|████████████████████████     | 332/400 [2:42:00<32:25, 28.62s/it]2022-01-10 12:23:23,210 iteration 5645 : loss : 0.011229, loss_ce: 0.003508
2022-01-10 12:23:24,751 iteration 5646 : loss : 0.026251, loss_ce: 0.009159
2022-01-10 12:23:26,249 iteration 5647 : loss : 0.009884, loss_ce: 0.003332
2022-01-10 12:23:27,809 iteration 5648 : loss : 0.014152, loss_ce: 0.006409
2022-01-10 12:23:29,327 iteration 5649 : loss : 0.013548, loss_ce: 0.004397
2022-01-10 12:23:30,943 iteration 5650 : loss : 0.026848, loss_ce: 0.011988
2022-01-10 12:23:32,495 iteration 5651 : loss : 0.014931, loss_ce: 0.006690
2022-01-10 12:23:34,069 iteration 5652 : loss : 0.012997, loss_ce: 0.004098
2022-01-10 12:23:35,628 iteration 5653 : loss : 0.016246, loss_ce: 0.004642
2022-01-10 12:23:37,187 iteration 5654 : loss : 0.013869, loss_ce: 0.005232
2022-01-10 12:23:38,663 iteration 5655 : loss : 0.013311, loss_ce: 0.006378
2022-01-10 12:23:40,309 iteration 5656 : loss : 0.021745, loss_ce: 0.007603
2022-01-10 12:23:41,928 iteration 5657 : loss : 0.027158, loss_ce: 0.009778
2022-01-10 12:23:43,544 iteration 5658 : loss : 0.019724, loss_ce: 0.008427
2022-01-10 12:23:45,074 iteration 5659 : loss : 0.014653, loss_ce: 0.006015
2022-01-10 12:23:46,666 iteration 5660 : loss : 0.024479, loss_ce: 0.007967
2022-01-10 12:23:48,229 iteration 5661 : loss : 0.018075, loss_ce: 0.009100
 83%|████████████████████████▏    | 333/400 [2:42:27<31:16, 28.01s/it]2022-01-10 12:23:49,844 iteration 5662 : loss : 0.019817, loss_ce: 0.008278
2022-01-10 12:23:51,372 iteration 5663 : loss : 0.015832, loss_ce: 0.004909
2022-01-10 12:23:52,941 iteration 5664 : loss : 0.017133, loss_ce: 0.007663
2022-01-10 12:23:54,456 iteration 5665 : loss : 0.012765, loss_ce: 0.004961
2022-01-10 12:23:56,033 iteration 5666 : loss : 0.013433, loss_ce: 0.003505
2022-01-10 12:23:57,641 iteration 5667 : loss : 0.012580, loss_ce: 0.004551
2022-01-10 12:23:59,217 iteration 5668 : loss : 0.017015, loss_ce: 0.006550
2022-01-10 12:24:00,726 iteration 5669 : loss : 0.012858, loss_ce: 0.004678
2022-01-10 12:24:02,269 iteration 5670 : loss : 0.014180, loss_ce: 0.004992
2022-01-10 12:24:03,786 iteration 5671 : loss : 0.011720, loss_ce: 0.004461
2022-01-10 12:24:05,443 iteration 5672 : loss : 0.038692, loss_ce: 0.019513
2022-01-10 12:24:06,985 iteration 5673 : loss : 0.017723, loss_ce: 0.006298
2022-01-10 12:24:08,541 iteration 5674 : loss : 0.013967, loss_ce: 0.004327
2022-01-10 12:24:10,068 iteration 5675 : loss : 0.012930, loss_ce: 0.005991
2022-01-10 12:24:11,687 iteration 5676 : loss : 0.019683, loss_ce: 0.007615
2022-01-10 12:24:13,261 iteration 5677 : loss : 0.016726, loss_ce: 0.004474
2022-01-10 12:24:14,910 iteration 5678 : loss : 0.028746, loss_ce: 0.009579
 84%|████████████████████████▏    | 334/400 [2:42:53<30:22, 27.61s/it]2022-01-10 12:24:16,569 iteration 5679 : loss : 0.015212, loss_ce: 0.005338
2022-01-10 12:24:18,092 iteration 5680 : loss : 0.019043, loss_ce: 0.006695
2022-01-10 12:24:19,674 iteration 5681 : loss : 0.017696, loss_ce: 0.006927
2022-01-10 12:24:21,225 iteration 5682 : loss : 0.013957, loss_ce: 0.005420
2022-01-10 12:24:22,821 iteration 5683 : loss : 0.023624, loss_ce: 0.009187
2022-01-10 12:24:24,454 iteration 5684 : loss : 0.023777, loss_ce: 0.007587
2022-01-10 12:24:26,061 iteration 5685 : loss : 0.017603, loss_ce: 0.005695
2022-01-10 12:24:27,628 iteration 5686 : loss : 0.021175, loss_ce: 0.008769
2022-01-10 12:24:29,160 iteration 5687 : loss : 0.014655, loss_ce: 0.004251
2022-01-10 12:24:30,718 iteration 5688 : loss : 0.016063, loss_ce: 0.005443
2022-01-10 12:24:32,264 iteration 5689 : loss : 0.015413, loss_ce: 0.006321
2022-01-10 12:24:33,856 iteration 5690 : loss : 0.022308, loss_ce: 0.011124
2022-01-10 12:24:35,397 iteration 5691 : loss : 0.013383, loss_ce: 0.003881
2022-01-10 12:24:36,922 iteration 5692 : loss : 0.012330, loss_ce: 0.004356
2022-01-10 12:24:38,512 iteration 5693 : loss : 0.024831, loss_ce: 0.012743
2022-01-10 12:24:40,116 iteration 5694 : loss : 0.024505, loss_ce: 0.011553
2022-01-10 12:24:40,116 Training Data Eval:
2022-01-10 12:24:48,022   Average segmentation loss on training set: 0.0092
2022-01-10 12:24:48,023 Validation Data Eval:
2022-01-10 12:24:50,756   Average segmentation loss on validation set: 0.0632
2022-01-10 12:24:52,287 iteration 5695 : loss : 0.021182, loss_ce: 0.006921
 84%|████████████████████████▎    | 335/400 [2:43:31<33:05, 30.54s/it]2022-01-10 12:24:53,883 iteration 5696 : loss : 0.017903, loss_ce: 0.005130
2022-01-10 12:24:55,507 iteration 5697 : loss : 0.015803, loss_ce: 0.006098
2022-01-10 12:24:57,081 iteration 5698 : loss : 0.015638, loss_ce: 0.005701
2022-01-10 12:24:58,717 iteration 5699 : loss : 0.019676, loss_ce: 0.007438
2022-01-10 12:25:00,285 iteration 5700 : loss : 0.015375, loss_ce: 0.005466
2022-01-10 12:25:01,831 iteration 5701 : loss : 0.014651, loss_ce: 0.007105
2022-01-10 12:25:03,405 iteration 5702 : loss : 0.020112, loss_ce: 0.008415
2022-01-10 12:25:04,962 iteration 5703 : loss : 0.040670, loss_ce: 0.019533
2022-01-10 12:25:06,561 iteration 5704 : loss : 0.017343, loss_ce: 0.007851
2022-01-10 12:25:08,146 iteration 5705 : loss : 0.017691, loss_ce: 0.006914
2022-01-10 12:25:09,677 iteration 5706 : loss : 0.015667, loss_ce: 0.003427
2022-01-10 12:25:11,360 iteration 5707 : loss : 0.019323, loss_ce: 0.006308
2022-01-10 12:25:12,912 iteration 5708 : loss : 0.014978, loss_ce: 0.006977
2022-01-10 12:25:14,497 iteration 5709 : loss : 0.020025, loss_ce: 0.009563
2022-01-10 12:25:16,147 iteration 5710 : loss : 0.020710, loss_ce: 0.009547
2022-01-10 12:25:17,690 iteration 5711 : loss : 0.016805, loss_ce: 0.008671
2022-01-10 12:25:19,243 iteration 5712 : loss : 0.015151, loss_ce: 0.006505
 84%|████████████████████████▎    | 336/400 [2:43:58<31:25, 29.47s/it]2022-01-10 12:25:20,825 iteration 5713 : loss : 0.023903, loss_ce: 0.006182
2022-01-10 12:25:22,357 iteration 5714 : loss : 0.013831, loss_ce: 0.005007
2022-01-10 12:25:23,890 iteration 5715 : loss : 0.011539, loss_ce: 0.004687
2022-01-10 12:25:25,495 iteration 5716 : loss : 0.013015, loss_ce: 0.005935
2022-01-10 12:25:27,101 iteration 5717 : loss : 0.016826, loss_ce: 0.006303
2022-01-10 12:25:28,602 iteration 5718 : loss : 0.018739, loss_ce: 0.005422
2022-01-10 12:25:30,194 iteration 5719 : loss : 0.014842, loss_ce: 0.006354
2022-01-10 12:25:31,728 iteration 5720 : loss : 0.018737, loss_ce: 0.008224
2022-01-10 12:25:33,194 iteration 5721 : loss : 0.011984, loss_ce: 0.003557
2022-01-10 12:25:34,739 iteration 5722 : loss : 0.013672, loss_ce: 0.004870
2022-01-10 12:25:36,286 iteration 5723 : loss : 0.017306, loss_ce: 0.007130
2022-01-10 12:25:37,755 iteration 5724 : loss : 0.011059, loss_ce: 0.004110
2022-01-10 12:25:39,342 iteration 5725 : loss : 0.015015, loss_ce: 0.006996
2022-01-10 12:25:40,851 iteration 5726 : loss : 0.013110, loss_ce: 0.005356
2022-01-10 12:25:42,398 iteration 5727 : loss : 0.015646, loss_ce: 0.005018
2022-01-10 12:25:43,935 iteration 5728 : loss : 0.014798, loss_ce: 0.007061
2022-01-10 12:25:45,522 iteration 5729 : loss : 0.012478, loss_ce: 0.004281
 84%|████████████████████████▍    | 337/400 [2:44:24<29:56, 28.51s/it]2022-01-10 12:25:47,158 iteration 5730 : loss : 0.024030, loss_ce: 0.006661
2022-01-10 12:25:48,766 iteration 5731 : loss : 0.012547, loss_ce: 0.005336
2022-01-10 12:25:50,318 iteration 5732 : loss : 0.016090, loss_ce: 0.004730
2022-01-10 12:25:51,844 iteration 5733 : loss : 0.011818, loss_ce: 0.003574
2022-01-10 12:25:53,410 iteration 5734 : loss : 0.029788, loss_ce: 0.007907
2022-01-10 12:25:54,982 iteration 5735 : loss : 0.015357, loss_ce: 0.006741
2022-01-10 12:25:56,535 iteration 5736 : loss : 0.015391, loss_ce: 0.008159
2022-01-10 12:25:58,077 iteration 5737 : loss : 0.017813, loss_ce: 0.007337
2022-01-10 12:25:59,615 iteration 5738 : loss : 0.010484, loss_ce: 0.004517
2022-01-10 12:26:01,120 iteration 5739 : loss : 0.016062, loss_ce: 0.006511
2022-01-10 12:26:02,707 iteration 5740 : loss : 0.018547, loss_ce: 0.006494
2022-01-10 12:26:04,280 iteration 5741 : loss : 0.017926, loss_ce: 0.008625
2022-01-10 12:26:05,853 iteration 5742 : loss : 0.020646, loss_ce: 0.009248
2022-01-10 12:26:07,384 iteration 5743 : loss : 0.012629, loss_ce: 0.005292
2022-01-10 12:26:08,993 iteration 5744 : loss : 0.014409, loss_ce: 0.005456
2022-01-10 12:26:10,557 iteration 5745 : loss : 0.013566, loss_ce: 0.004096
2022-01-10 12:26:12,044 iteration 5746 : loss : 0.010730, loss_ce: 0.002736
 84%|████████████████████████▌    | 338/400 [2:44:51<28:50, 27.91s/it]2022-01-10 12:26:13,697 iteration 5747 : loss : 0.012808, loss_ce: 0.003980
2022-01-10 12:26:15,293 iteration 5748 : loss : 0.020930, loss_ce: 0.007347
2022-01-10 12:26:16,902 iteration 5749 : loss : 0.017846, loss_ce: 0.007766
2022-01-10 12:26:18,547 iteration 5750 : loss : 0.028386, loss_ce: 0.008301
2022-01-10 12:26:20,053 iteration 5751 : loss : 0.012636, loss_ce: 0.005209
2022-01-10 12:26:21,638 iteration 5752 : loss : 0.018522, loss_ce: 0.005466
2022-01-10 12:26:23,184 iteration 5753 : loss : 0.019537, loss_ce: 0.007753
2022-01-10 12:26:24,680 iteration 5754 : loss : 0.011844, loss_ce: 0.004372
2022-01-10 12:26:26,239 iteration 5755 : loss : 0.015159, loss_ce: 0.006790
2022-01-10 12:26:27,803 iteration 5756 : loss : 0.013350, loss_ce: 0.004282
2022-01-10 12:26:29,346 iteration 5757 : loss : 0.012622, loss_ce: 0.006194
2022-01-10 12:26:30,908 iteration 5758 : loss : 0.020242, loss_ce: 0.005603
2022-01-10 12:26:32,481 iteration 5759 : loss : 0.014243, loss_ce: 0.006945
2022-01-10 12:26:34,120 iteration 5760 : loss : 0.021829, loss_ce: 0.007861
2022-01-10 12:26:35,608 iteration 5761 : loss : 0.009796, loss_ce: 0.003692
2022-01-10 12:26:37,337 iteration 5762 : loss : 0.021708, loss_ce: 0.009654
2022-01-10 12:26:38,897 iteration 5763 : loss : 0.013853, loss_ce: 0.005920
 85%|████████████████████████▌    | 339/400 [2:45:17<28:03, 27.60s/it]2022-01-10 12:26:40,541 iteration 5764 : loss : 0.017964, loss_ce: 0.006671
2022-01-10 12:26:42,140 iteration 5765 : loss : 0.017207, loss_ce: 0.005578
2022-01-10 12:26:43,722 iteration 5766 : loss : 0.015647, loss_ce: 0.005733
2022-01-10 12:26:45,309 iteration 5767 : loss : 0.015800, loss_ce: 0.006537
2022-01-10 12:26:46,984 iteration 5768 : loss : 0.027040, loss_ce: 0.009545
2022-01-10 12:26:48,543 iteration 5769 : loss : 0.010903, loss_ce: 0.004188
2022-01-10 12:26:50,104 iteration 5770 : loss : 0.016149, loss_ce: 0.006133
2022-01-10 12:26:51,667 iteration 5771 : loss : 0.031661, loss_ce: 0.014923
2022-01-10 12:26:53,242 iteration 5772 : loss : 0.016095, loss_ce: 0.006378
2022-01-10 12:26:54,885 iteration 5773 : loss : 0.014856, loss_ce: 0.006524
2022-01-10 12:26:56,521 iteration 5774 : loss : 0.017704, loss_ce: 0.006840
2022-01-10 12:26:58,175 iteration 5775 : loss : 0.022328, loss_ce: 0.008707
2022-01-10 12:26:59,706 iteration 5776 : loss : 0.013500, loss_ce: 0.004486
2022-01-10 12:27:01,239 iteration 5777 : loss : 0.017212, loss_ce: 0.006307
2022-01-10 12:27:02,839 iteration 5778 : loss : 0.016235, loss_ce: 0.004949
2022-01-10 12:27:04,402 iteration 5779 : loss : 0.020985, loss_ce: 0.010526
2022-01-10 12:27:04,402 Training Data Eval:
2022-01-10 12:27:12,312   Average segmentation loss on training set: 0.0087
2022-01-10 12:27:12,312 Validation Data Eval:
2022-01-10 12:27:15,038   Average segmentation loss on validation set: 0.0779
2022-01-10 12:27:16,639 iteration 5780 : loss : 0.013908, loss_ce: 0.005070
 85%|████████████████████████▋    | 340/400 [2:45:55<30:38, 30.64s/it]2022-01-10 12:27:18,383 iteration 5781 : loss : 0.024582, loss_ce: 0.008066
2022-01-10 12:27:19,860 iteration 5782 : loss : 0.010499, loss_ce: 0.004957
2022-01-10 12:27:21,373 iteration 5783 : loss : 0.017839, loss_ce: 0.007400
2022-01-10 12:27:22,894 iteration 5784 : loss : 0.015169, loss_ce: 0.005147
2022-01-10 12:27:24,383 iteration 5785 : loss : 0.012585, loss_ce: 0.005476
2022-01-10 12:27:25,950 iteration 5786 : loss : 0.017273, loss_ce: 0.004120
2022-01-10 12:27:27,492 iteration 5787 : loss : 0.015154, loss_ce: 0.005781
2022-01-10 12:27:28,955 iteration 5788 : loss : 0.011800, loss_ce: 0.004609
2022-01-10 12:27:30,440 iteration 5789 : loss : 0.009793, loss_ce: 0.003866
2022-01-10 12:27:32,016 iteration 5790 : loss : 0.009638, loss_ce: 0.003593
2022-01-10 12:27:33,561 iteration 5791 : loss : 0.016298, loss_ce: 0.006572
2022-01-10 12:27:35,069 iteration 5792 : loss : 0.011585, loss_ce: 0.004663
2022-01-10 12:27:36,666 iteration 5793 : loss : 0.021735, loss_ce: 0.006339
2022-01-10 12:27:38,295 iteration 5794 : loss : 0.015829, loss_ce: 0.007055
2022-01-10 12:27:39,811 iteration 5795 : loss : 0.016214, loss_ce: 0.007230
2022-01-10 12:27:41,380 iteration 5796 : loss : 0.019474, loss_ce: 0.009823
2022-01-10 12:27:42,944 iteration 5797 : loss : 0.015716, loss_ce: 0.005373
 85%|████████████████████████▋    | 341/400 [2:46:22<28:51, 29.34s/it]2022-01-10 12:27:44,650 iteration 5798 : loss : 0.034361, loss_ce: 0.008758
2022-01-10 12:27:46,205 iteration 5799 : loss : 0.016958, loss_ce: 0.005666
2022-01-10 12:27:47,751 iteration 5800 : loss : 0.015695, loss_ce: 0.004186
2022-01-10 12:27:49,224 iteration 5801 : loss : 0.010890, loss_ce: 0.003883
2022-01-10 12:27:50,804 iteration 5802 : loss : 0.014002, loss_ce: 0.006960
2022-01-10 12:27:52,295 iteration 5803 : loss : 0.017706, loss_ce: 0.005972
2022-01-10 12:27:53,949 iteration 5804 : loss : 0.020060, loss_ce: 0.007780
2022-01-10 12:27:55,476 iteration 5805 : loss : 0.023156, loss_ce: 0.013222
2022-01-10 12:27:56,984 iteration 5806 : loss : 0.015106, loss_ce: 0.005467
2022-01-10 12:27:58,555 iteration 5807 : loss : 0.012006, loss_ce: 0.004590
2022-01-10 12:28:00,090 iteration 5808 : loss : 0.018284, loss_ce: 0.007305
2022-01-10 12:28:01,738 iteration 5809 : loss : 0.023768, loss_ce: 0.007122
2022-01-10 12:28:03,232 iteration 5810 : loss : 0.014414, loss_ce: 0.003793
2022-01-10 12:28:04,774 iteration 5811 : loss : 0.012809, loss_ce: 0.005115
2022-01-10 12:28:06,358 iteration 5812 : loss : 0.017396, loss_ce: 0.007172
2022-01-10 12:28:07,878 iteration 5813 : loss : 0.019758, loss_ce: 0.006854
2022-01-10 12:28:09,435 iteration 5814 : loss : 0.015027, loss_ce: 0.006260
 86%|████████████████████████▊    | 342/400 [2:46:48<27:32, 28.48s/it]2022-01-10 12:28:11,153 iteration 5815 : loss : 0.026800, loss_ce: 0.011628
2022-01-10 12:28:12,653 iteration 5816 : loss : 0.012795, loss_ce: 0.004043
2022-01-10 12:28:14,224 iteration 5817 : loss : 0.017102, loss_ce: 0.004993
2022-01-10 12:28:15,768 iteration 5818 : loss : 0.018074, loss_ce: 0.005218
2022-01-10 12:28:17,417 iteration 5819 : loss : 0.017298, loss_ce: 0.008311
2022-01-10 12:28:18,974 iteration 5820 : loss : 0.011995, loss_ce: 0.005079
2022-01-10 12:28:20,520 iteration 5821 : loss : 0.034099, loss_ce: 0.011155
2022-01-10 12:28:22,142 iteration 5822 : loss : 0.034625, loss_ce: 0.009076
2022-01-10 12:28:23,722 iteration 5823 : loss : 0.015847, loss_ce: 0.005189
2022-01-10 12:28:25,284 iteration 5824 : loss : 0.015439, loss_ce: 0.005419
2022-01-10 12:28:26,839 iteration 5825 : loss : 0.019507, loss_ce: 0.008079
2022-01-10 12:28:28,365 iteration 5826 : loss : 0.025640, loss_ce: 0.009335
2022-01-10 12:28:29,875 iteration 5827 : loss : 0.020713, loss_ce: 0.005472
2022-01-10 12:28:31,349 iteration 5828 : loss : 0.010005, loss_ce: 0.004443
2022-01-10 12:28:32,905 iteration 5829 : loss : 0.014796, loss_ce: 0.005528
2022-01-10 12:28:34,478 iteration 5830 : loss : 0.018916, loss_ce: 0.008237
2022-01-10 12:28:36,090 iteration 5831 : loss : 0.020040, loss_ce: 0.007919
 86%|████████████████████████▊    | 343/400 [2:47:15<26:32, 27.94s/it]2022-01-10 12:28:37,795 iteration 5832 : loss : 0.014947, loss_ce: 0.005899
2022-01-10 12:28:39,336 iteration 5833 : loss : 0.009976, loss_ce: 0.003345
2022-01-10 12:28:40,978 iteration 5834 : loss : 0.020062, loss_ce: 0.007851
2022-01-10 12:28:42,691 iteration 5835 : loss : 0.034025, loss_ce: 0.014536
2022-01-10 12:28:44,242 iteration 5836 : loss : 0.016192, loss_ce: 0.007636
2022-01-10 12:28:45,883 iteration 5837 : loss : 0.020498, loss_ce: 0.009859
2022-01-10 12:28:47,521 iteration 5838 : loss : 0.017761, loss_ce: 0.004877
2022-01-10 12:28:49,016 iteration 5839 : loss : 0.014961, loss_ce: 0.005196
2022-01-10 12:28:50,768 iteration 5840 : loss : 0.016084, loss_ce: 0.007590
2022-01-10 12:28:52,330 iteration 5841 : loss : 0.012069, loss_ce: 0.005353
2022-01-10 12:28:53,965 iteration 5842 : loss : 0.018720, loss_ce: 0.007450
2022-01-10 12:28:55,604 iteration 5843 : loss : 0.016051, loss_ce: 0.005112
2022-01-10 12:28:57,201 iteration 5844 : loss : 0.017450, loss_ce: 0.004658
2022-01-10 12:28:58,817 iteration 5845 : loss : 0.020185, loss_ce: 0.006307
2022-01-10 12:29:00,392 iteration 5846 : loss : 0.015516, loss_ce: 0.006575
2022-01-10 12:29:01,949 iteration 5847 : loss : 0.010803, loss_ce: 0.003620
2022-01-10 12:29:03,438 iteration 5848 : loss : 0.014406, loss_ce: 0.004432
 86%|████████████████████████▉    | 344/400 [2:47:42<25:54, 27.76s/it]2022-01-10 12:29:05,004 iteration 5849 : loss : 0.013824, loss_ce: 0.005119
2022-01-10 12:29:06,628 iteration 5850 : loss : 0.027310, loss_ce: 0.006809
2022-01-10 12:29:08,148 iteration 5851 : loss : 0.011253, loss_ce: 0.004713
2022-01-10 12:29:09,678 iteration 5852 : loss : 0.012638, loss_ce: 0.004265
2022-01-10 12:29:11,177 iteration 5853 : loss : 0.015644, loss_ce: 0.004066
2022-01-10 12:29:12,733 iteration 5854 : loss : 0.023424, loss_ce: 0.008769
2022-01-10 12:29:14,275 iteration 5855 : loss : 0.014341, loss_ce: 0.003760
2022-01-10 12:29:15,775 iteration 5856 : loss : 0.014464, loss_ce: 0.006021
2022-01-10 12:29:17,363 iteration 5857 : loss : 0.016410, loss_ce: 0.007351
2022-01-10 12:29:18,900 iteration 5858 : loss : 0.019890, loss_ce: 0.005565
2022-01-10 12:29:20,484 iteration 5859 : loss : 0.016097, loss_ce: 0.008265
2022-01-10 12:29:22,071 iteration 5860 : loss : 0.011298, loss_ce: 0.004087
2022-01-10 12:29:23,657 iteration 5861 : loss : 0.015221, loss_ce: 0.006976
2022-01-10 12:29:25,207 iteration 5862 : loss : 0.014145, loss_ce: 0.005098
2022-01-10 12:29:26,753 iteration 5863 : loss : 0.014765, loss_ce: 0.005449
2022-01-10 12:29:28,299 iteration 5864 : loss : 0.016463, loss_ce: 0.006417
2022-01-10 12:29:28,299 Training Data Eval:
2022-01-10 12:29:36,220   Average segmentation loss on training set: 0.0085
2022-01-10 12:29:36,220 Validation Data Eval:
2022-01-10 12:29:38,960   Average segmentation loss on validation set: 0.0702
2022-01-10 12:29:40,487 iteration 5865 : loss : 0.013356, loss_ce: 0.004685
 86%|█████████████████████████    | 345/400 [2:48:19<28:00, 30.55s/it]2022-01-10 12:29:42,119 iteration 5866 : loss : 0.025214, loss_ce: 0.008579
2022-01-10 12:29:43,691 iteration 5867 : loss : 0.014497, loss_ce: 0.006459
2022-01-10 12:29:45,182 iteration 5868 : loss : 0.010554, loss_ce: 0.004020
2022-01-10 12:29:46,710 iteration 5869 : loss : 0.010127, loss_ce: 0.003531
2022-01-10 12:29:48,187 iteration 5870 : loss : 0.011838, loss_ce: 0.004397
2022-01-10 12:29:49,728 iteration 5871 : loss : 0.013532, loss_ce: 0.005293
2022-01-10 12:29:51,269 iteration 5872 : loss : 0.019264, loss_ce: 0.008914
2022-01-10 12:29:52,758 iteration 5873 : loss : 0.012705, loss_ce: 0.005521
2022-01-10 12:29:54,253 iteration 5874 : loss : 0.018732, loss_ce: 0.003957
2022-01-10 12:29:55,873 iteration 5875 : loss : 0.015118, loss_ce: 0.006886
2022-01-10 12:29:57,508 iteration 5876 : loss : 0.028144, loss_ce: 0.015067
2022-01-10 12:29:59,094 iteration 5877 : loss : 0.027127, loss_ce: 0.009549
2022-01-10 12:30:00,701 iteration 5878 : loss : 0.021379, loss_ce: 0.004265
2022-01-10 12:30:02,233 iteration 5879 : loss : 0.015983, loss_ce: 0.004451
2022-01-10 12:30:03,777 iteration 5880 : loss : 0.019580, loss_ce: 0.006955
2022-01-10 12:30:05,374 iteration 5881 : loss : 0.026759, loss_ce: 0.008109
2022-01-10 12:30:06,910 iteration 5882 : loss : 0.013303, loss_ce: 0.004672
 86%|█████████████████████████    | 346/400 [2:48:45<26:22, 29.31s/it]2022-01-10 12:30:08,632 iteration 5883 : loss : 0.016844, loss_ce: 0.005986
2022-01-10 12:30:10,154 iteration 5884 : loss : 0.014563, loss_ce: 0.005384
2022-01-10 12:30:11,772 iteration 5885 : loss : 0.032550, loss_ce: 0.007356
2022-01-10 12:30:13,300 iteration 5886 : loss : 0.009814, loss_ce: 0.003363
2022-01-10 12:30:14,831 iteration 5887 : loss : 0.018644, loss_ce: 0.005668
2022-01-10 12:30:16,297 iteration 5888 : loss : 0.011657, loss_ce: 0.005176
2022-01-10 12:30:17,831 iteration 5889 : loss : 0.013570, loss_ce: 0.006113
2022-01-10 12:30:19,426 iteration 5890 : loss : 0.014293, loss_ce: 0.006305
2022-01-10 12:30:21,030 iteration 5891 : loss : 0.021243, loss_ce: 0.007769
2022-01-10 12:30:22,658 iteration 5892 : loss : 0.015135, loss_ce: 0.006830
2022-01-10 12:30:24,149 iteration 5893 : loss : 0.010368, loss_ce: 0.002912
2022-01-10 12:30:25,664 iteration 5894 : loss : 0.016125, loss_ce: 0.004702
2022-01-10 12:30:27,252 iteration 5895 : loss : 0.021237, loss_ce: 0.007899
2022-01-10 12:30:28,728 iteration 5896 : loss : 0.013626, loss_ce: 0.004727
2022-01-10 12:30:30,299 iteration 5897 : loss : 0.014227, loss_ce: 0.006490
2022-01-10 12:30:31,913 iteration 5898 : loss : 0.019264, loss_ce: 0.007170
2022-01-10 12:30:33,457 iteration 5899 : loss : 0.013392, loss_ce: 0.004443
 87%|█████████████████████████▏   | 347/400 [2:49:12<25:09, 28.48s/it]2022-01-10 12:30:34,981 iteration 5900 : loss : 0.012312, loss_ce: 0.004979
2022-01-10 12:30:36,498 iteration 5901 : loss : 0.011166, loss_ce: 0.004634
2022-01-10 12:30:37,976 iteration 5902 : loss : 0.011331, loss_ce: 0.004249
2022-01-10 12:30:39,499 iteration 5903 : loss : 0.015397, loss_ce: 0.006783
2022-01-10 12:30:41,067 iteration 5904 : loss : 0.031567, loss_ce: 0.009279
2022-01-10 12:30:42,664 iteration 5905 : loss : 0.019149, loss_ce: 0.009924
2022-01-10 12:30:44,280 iteration 5906 : loss : 0.014695, loss_ce: 0.003841
2022-01-10 12:30:45,772 iteration 5907 : loss : 0.010525, loss_ce: 0.004667
2022-01-10 12:30:47,331 iteration 5908 : loss : 0.016702, loss_ce: 0.005566
2022-01-10 12:30:48,930 iteration 5909 : loss : 0.017392, loss_ce: 0.007419
2022-01-10 12:30:50,483 iteration 5910 : loss : 0.013356, loss_ce: 0.004630
2022-01-10 12:30:52,078 iteration 5911 : loss : 0.018616, loss_ce: 0.005036
2022-01-10 12:30:53,651 iteration 5912 : loss : 0.018946, loss_ce: 0.004123
2022-01-10 12:30:55,205 iteration 5913 : loss : 0.015250, loss_ce: 0.007333
2022-01-10 12:30:56,745 iteration 5914 : loss : 0.011831, loss_ce: 0.005289
2022-01-10 12:30:58,331 iteration 5915 : loss : 0.015998, loss_ce: 0.007860
2022-01-10 12:30:59,838 iteration 5916 : loss : 0.010232, loss_ce: 0.003650
 87%|█████████████████████████▏   | 348/400 [2:49:38<24:08, 27.85s/it]2022-01-10 12:31:01,402 iteration 5917 : loss : 0.014431, loss_ce: 0.005848
2022-01-10 12:31:03,035 iteration 5918 : loss : 0.024148, loss_ce: 0.014129
2022-01-10 12:31:04,668 iteration 5919 : loss : 0.018167, loss_ce: 0.006263
2022-01-10 12:31:06,304 iteration 5920 : loss : 0.015926, loss_ce: 0.004913
2022-01-10 12:31:07,858 iteration 5921 : loss : 0.017894, loss_ce: 0.006164
2022-01-10 12:31:09,462 iteration 5922 : loss : 0.015330, loss_ce: 0.007707
2022-01-10 12:31:11,040 iteration 5923 : loss : 0.016355, loss_ce: 0.006214
2022-01-10 12:31:12,524 iteration 5924 : loss : 0.011545, loss_ce: 0.004207
2022-01-10 12:31:14,071 iteration 5925 : loss : 0.013350, loss_ce: 0.004439
2022-01-10 12:31:15,696 iteration 5926 : loss : 0.018235, loss_ce: 0.004906
2022-01-10 12:31:17,222 iteration 5927 : loss : 0.010285, loss_ce: 0.004260
2022-01-10 12:31:18,839 iteration 5928 : loss : 0.014324, loss_ce: 0.006203
2022-01-10 12:31:20,430 iteration 5929 : loss : 0.018457, loss_ce: 0.009550
2022-01-10 12:31:22,066 iteration 5930 : loss : 0.021438, loss_ce: 0.007808
2022-01-10 12:31:23,584 iteration 5931 : loss : 0.011176, loss_ce: 0.004892
2022-01-10 12:31:25,162 iteration 5932 : loss : 0.012595, loss_ce: 0.004248
2022-01-10 12:31:26,766 iteration 5933 : loss : 0.020981, loss_ce: 0.005820
 87%|█████████████████████████▎   | 349/400 [2:50:05<23:26, 27.57s/it]2022-01-10 12:31:28,408 iteration 5934 : loss : 0.009353, loss_ce: 0.003215
2022-01-10 12:31:29,988 iteration 5935 : loss : 0.017970, loss_ce: 0.006303
2022-01-10 12:31:31,662 iteration 5936 : loss : 0.020197, loss_ce: 0.006403
2022-01-10 12:31:33,310 iteration 5937 : loss : 0.020273, loss_ce: 0.009418
2022-01-10 12:31:34,899 iteration 5938 : loss : 0.021196, loss_ce: 0.006649
2022-01-10 12:31:36,443 iteration 5939 : loss : 0.013882, loss_ce: 0.005936
2022-01-10 12:31:38,033 iteration 5940 : loss : 0.021458, loss_ce: 0.010104
2022-01-10 12:31:39,508 iteration 5941 : loss : 0.011839, loss_ce: 0.005291
2022-01-10 12:31:41,104 iteration 5942 : loss : 0.024346, loss_ce: 0.007747
2022-01-10 12:31:42,602 iteration 5943 : loss : 0.009669, loss_ce: 0.003544
2022-01-10 12:31:44,161 iteration 5944 : loss : 0.017378, loss_ce: 0.005429
2022-01-10 12:31:45,701 iteration 5945 : loss : 0.012571, loss_ce: 0.004147
2022-01-10 12:31:47,210 iteration 5946 : loss : 0.014910, loss_ce: 0.005576
2022-01-10 12:31:48,710 iteration 5947 : loss : 0.013386, loss_ce: 0.005666
2022-01-10 12:31:50,328 iteration 5948 : loss : 0.016484, loss_ce: 0.006292
2022-01-10 12:31:51,845 iteration 5949 : loss : 0.012486, loss_ce: 0.005174
2022-01-10 12:31:51,845 Training Data Eval:
2022-01-10 12:31:59,767   Average segmentation loss on training set: 0.0081
2022-01-10 12:31:59,767 Validation Data Eval:
2022-01-10 12:32:02,498   Average segmentation loss on validation set: 0.0697
2022-01-10 12:32:04,126 iteration 5950 : loss : 0.016258, loss_ce: 0.005152
 88%|█████████████████████████▍   | 350/400 [2:50:43<25:25, 30.51s/it]2022-01-10 12:32:05,722 iteration 5951 : loss : 0.019075, loss_ce: 0.006675
2022-01-10 12:32:07,292 iteration 5952 : loss : 0.012960, loss_ce: 0.006777
2022-01-10 12:32:08,873 iteration 5953 : loss : 0.015601, loss_ce: 0.007156
2022-01-10 12:32:10,510 iteration 5954 : loss : 0.023279, loss_ce: 0.006515
2022-01-10 12:32:12,057 iteration 5955 : loss : 0.014445, loss_ce: 0.005068
2022-01-10 12:32:13,689 iteration 5956 : loss : 0.016927, loss_ce: 0.005797
2022-01-10 12:32:15,235 iteration 5957 : loss : 0.013963, loss_ce: 0.004127
2022-01-10 12:32:16,757 iteration 5958 : loss : 0.011336, loss_ce: 0.004981
2022-01-10 12:32:18,309 iteration 5959 : loss : 0.015249, loss_ce: 0.005073
2022-01-10 12:32:19,825 iteration 5960 : loss : 0.009478, loss_ce: 0.002918
2022-01-10 12:32:21,404 iteration 5961 : loss : 0.016567, loss_ce: 0.006698
2022-01-10 12:32:23,014 iteration 5962 : loss : 0.016725, loss_ce: 0.006732
2022-01-10 12:32:24,505 iteration 5963 : loss : 0.011483, loss_ce: 0.004742
2022-01-10 12:32:26,231 iteration 5964 : loss : 0.040597, loss_ce: 0.018257
2022-01-10 12:32:27,767 iteration 5965 : loss : 0.019437, loss_ce: 0.008929
2022-01-10 12:32:29,374 iteration 5966 : loss : 0.016212, loss_ce: 0.006388
2022-01-10 12:32:30,906 iteration 5967 : loss : 0.019043, loss_ce: 0.006686
 88%|█████████████████████████▍   | 351/400 [2:51:09<24:00, 29.39s/it]2022-01-10 12:32:32,505 iteration 5968 : loss : 0.014260, loss_ce: 0.005783
2022-01-10 12:32:34,106 iteration 5969 : loss : 0.017954, loss_ce: 0.004825
2022-01-10 12:32:35,680 iteration 5970 : loss : 0.016098, loss_ce: 0.005803
2022-01-10 12:32:37,160 iteration 5971 : loss : 0.011105, loss_ce: 0.004494
2022-01-10 12:32:38,680 iteration 5972 : loss : 0.017079, loss_ce: 0.007082
2022-01-10 12:32:40,249 iteration 5973 : loss : 0.010293, loss_ce: 0.004369
2022-01-10 12:32:41,757 iteration 5974 : loss : 0.018622, loss_ce: 0.005991
2022-01-10 12:32:43,347 iteration 5975 : loss : 0.026307, loss_ce: 0.010578
2022-01-10 12:32:44,922 iteration 5976 : loss : 0.020327, loss_ce: 0.005220
2022-01-10 12:32:46,509 iteration 5977 : loss : 0.019961, loss_ce: 0.009330
2022-01-10 12:32:48,080 iteration 5978 : loss : 0.012785, loss_ce: 0.005086
2022-01-10 12:32:49,602 iteration 5979 : loss : 0.011900, loss_ce: 0.004696
2022-01-10 12:32:51,203 iteration 5980 : loss : 0.012848, loss_ce: 0.005456
2022-01-10 12:32:52,794 iteration 5981 : loss : 0.014431, loss_ce: 0.005700
2022-01-10 12:32:54,415 iteration 5982 : loss : 0.020945, loss_ce: 0.008887
2022-01-10 12:32:55,912 iteration 5983 : loss : 0.011914, loss_ce: 0.005622
2022-01-10 12:32:57,424 iteration 5984 : loss : 0.014591, loss_ce: 0.004622
 88%|█████████████████████████▌   | 352/400 [2:51:36<22:49, 28.53s/it]2022-01-10 12:32:59,148 iteration 5985 : loss : 0.023008, loss_ce: 0.010819
2022-01-10 12:33:00,682 iteration 5986 : loss : 0.012167, loss_ce: 0.005264
2022-01-10 12:33:02,299 iteration 5987 : loss : 0.032738, loss_ce: 0.004925
2022-01-10 12:33:03,941 iteration 5988 : loss : 0.013907, loss_ce: 0.006825
2022-01-10 12:33:05,496 iteration 5989 : loss : 0.015613, loss_ce: 0.005697
2022-01-10 12:33:07,091 iteration 5990 : loss : 0.015180, loss_ce: 0.008261
2022-01-10 12:33:08,571 iteration 5991 : loss : 0.011177, loss_ce: 0.003990
2022-01-10 12:33:10,166 iteration 5992 : loss : 0.016718, loss_ce: 0.005503
2022-01-10 12:33:11,675 iteration 5993 : loss : 0.009827, loss_ce: 0.003239
2022-01-10 12:33:13,220 iteration 5994 : loss : 0.015344, loss_ce: 0.005151
2022-01-10 12:33:14,803 iteration 5995 : loss : 0.020521, loss_ce: 0.008318
2022-01-10 12:33:16,439 iteration 5996 : loss : 0.015821, loss_ce: 0.006153
2022-01-10 12:33:18,067 iteration 5997 : loss : 0.016236, loss_ce: 0.007422
2022-01-10 12:33:19,588 iteration 5998 : loss : 0.013351, loss_ce: 0.004947
2022-01-10 12:33:21,173 iteration 5999 : loss : 0.029209, loss_ce: 0.009559
2022-01-10 12:33:22,758 iteration 6000 : loss : 0.019013, loss_ce: 0.009136
2022-01-10 12:33:24,386 iteration 6001 : loss : 0.023579, loss_ce: 0.007905
 88%|█████████████████████████▌   | 353/400 [2:52:03<21:58, 28.06s/it]2022-01-10 12:33:26,018 iteration 6002 : loss : 0.013452, loss_ce: 0.005490
2022-01-10 12:33:27,633 iteration 6003 : loss : 0.019484, loss_ce: 0.006348
2022-01-10 12:33:29,202 iteration 6004 : loss : 0.011120, loss_ce: 0.004714
2022-01-10 12:33:30,803 iteration 6005 : loss : 0.012843, loss_ce: 0.005781
2022-01-10 12:33:32,311 iteration 6006 : loss : 0.017060, loss_ce: 0.008728
2022-01-10 12:33:33,871 iteration 6007 : loss : 0.018788, loss_ce: 0.007079
2022-01-10 12:33:35,430 iteration 6008 : loss : 0.016100, loss_ce: 0.006375
2022-01-10 12:33:37,074 iteration 6009 : loss : 0.020319, loss_ce: 0.007203
2022-01-10 12:33:38,548 iteration 6010 : loss : 0.011981, loss_ce: 0.004815
2022-01-10 12:33:40,130 iteration 6011 : loss : 0.020387, loss_ce: 0.007253
2022-01-10 12:33:41,690 iteration 6012 : loss : 0.025813, loss_ce: 0.009575
2022-01-10 12:33:43,354 iteration 6013 : loss : 0.021283, loss_ce: 0.008311
2022-01-10 12:33:44,913 iteration 6014 : loss : 0.017815, loss_ce: 0.006763
2022-01-10 12:33:46,441 iteration 6015 : loss : 0.017470, loss_ce: 0.007176
2022-01-10 12:33:48,010 iteration 6016 : loss : 0.009925, loss_ce: 0.002715
2022-01-10 12:33:49,517 iteration 6017 : loss : 0.017038, loss_ce: 0.004360
2022-01-10 12:33:51,165 iteration 6018 : loss : 0.015960, loss_ce: 0.005648
 88%|█████████████████████████▋   | 354/400 [2:52:30<21:13, 27.68s/it]2022-01-10 12:33:52,828 iteration 6019 : loss : 0.019072, loss_ce: 0.008731
2022-01-10 12:33:54,390 iteration 6020 : loss : 0.024187, loss_ce: 0.010361
2022-01-10 12:33:55,946 iteration 6021 : loss : 0.024707, loss_ce: 0.008941
2022-01-10 12:33:57,524 iteration 6022 : loss : 0.012176, loss_ce: 0.005130
2022-01-10 12:33:59,007 iteration 6023 : loss : 0.011189, loss_ce: 0.003819
2022-01-10 12:34:00,623 iteration 6024 : loss : 0.029770, loss_ce: 0.011029
2022-01-10 12:34:02,238 iteration 6025 : loss : 0.030420, loss_ce: 0.010841
2022-01-10 12:34:03,844 iteration 6026 : loss : 0.019770, loss_ce: 0.006464
2022-01-10 12:34:05,380 iteration 6027 : loss : 0.017270, loss_ce: 0.006331
2022-01-10 12:34:06,902 iteration 6028 : loss : 0.016981, loss_ce: 0.006247
2022-01-10 12:34:08,509 iteration 6029 : loss : 0.017949, loss_ce: 0.008494
2022-01-10 12:34:10,026 iteration 6030 : loss : 0.015861, loss_ce: 0.006310
2022-01-10 12:34:11,683 iteration 6031 : loss : 0.021948, loss_ce: 0.006636
2022-01-10 12:34:13,292 iteration 6032 : loss : 0.016726, loss_ce: 0.005703
2022-01-10 12:34:14,893 iteration 6033 : loss : 0.018841, loss_ce: 0.008963
2022-01-10 12:34:16,539 iteration 6034 : loss : 0.017629, loss_ce: 0.006364
2022-01-10 12:34:16,539 Training Data Eval:
2022-01-10 12:34:24,453   Average segmentation loss on training set: 0.0082
2022-01-10 12:34:24,454 Validation Data Eval:
2022-01-10 12:34:27,189   Average segmentation loss on validation set: 0.0672
2022-01-10 12:34:28,690 iteration 6035 : loss : 0.015689, loss_ce: 0.003583
 89%|█████████████████████████▋   | 355/400 [2:53:07<22:58, 30.63s/it]2022-01-10 12:34:30,344 iteration 6036 : loss : 0.016185, loss_ce: 0.005837
2022-01-10 12:34:31,974 iteration 6037 : loss : 0.022948, loss_ce: 0.007894
2022-01-10 12:34:33,509 iteration 6038 : loss : 0.018353, loss_ce: 0.007430
2022-01-10 12:34:35,080 iteration 6039 : loss : 0.017186, loss_ce: 0.008178
2022-01-10 12:34:36,637 iteration 6040 : loss : 0.012682, loss_ce: 0.005326
2022-01-10 12:34:38,316 iteration 6041 : loss : 0.031233, loss_ce: 0.011261
2022-01-10 12:34:39,943 iteration 6042 : loss : 0.016602, loss_ce: 0.005846
2022-01-10 12:34:41,512 iteration 6043 : loss : 0.014452, loss_ce: 0.006585
2022-01-10 12:34:43,064 iteration 6044 : loss : 0.016634, loss_ce: 0.005011
2022-01-10 12:34:44,680 iteration 6045 : loss : 0.014586, loss_ce: 0.004977
2022-01-10 12:34:46,250 iteration 6046 : loss : 0.017362, loss_ce: 0.005321
2022-01-10 12:34:47,784 iteration 6047 : loss : 0.015554, loss_ce: 0.004078
2022-01-10 12:34:49,452 iteration 6048 : loss : 0.020778, loss_ce: 0.003484
2022-01-10 12:34:51,069 iteration 6049 : loss : 0.015948, loss_ce: 0.006841
2022-01-10 12:34:52,608 iteration 6050 : loss : 0.013377, loss_ce: 0.005424
2022-01-10 12:34:54,233 iteration 6051 : loss : 0.015398, loss_ce: 0.007383
2022-01-10 12:34:55,716 iteration 6052 : loss : 0.013130, loss_ce: 0.003305
 89%|█████████████████████████▊   | 356/400 [2:53:34<21:40, 29.55s/it]2022-01-10 12:34:57,306 iteration 6053 : loss : 0.027438, loss_ce: 0.012662
2022-01-10 12:34:58,959 iteration 6054 : loss : 0.023823, loss_ce: 0.009571
2022-01-10 12:35:00,509 iteration 6055 : loss : 0.014907, loss_ce: 0.004791
2022-01-10 12:35:02,043 iteration 6056 : loss : 0.010945, loss_ce: 0.004235
2022-01-10 12:35:03,568 iteration 6057 : loss : 0.014225, loss_ce: 0.004922
2022-01-10 12:35:05,182 iteration 6058 : loss : 0.018011, loss_ce: 0.007322
2022-01-10 12:35:06,688 iteration 6059 : loss : 0.012752, loss_ce: 0.005407
2022-01-10 12:35:08,306 iteration 6060 : loss : 0.014067, loss_ce: 0.005609
2022-01-10 12:35:09,815 iteration 6061 : loss : 0.017301, loss_ce: 0.007547
2022-01-10 12:35:11,463 iteration 6062 : loss : 0.015978, loss_ce: 0.007787
2022-01-10 12:35:13,105 iteration 6063 : loss : 0.018393, loss_ce: 0.005719
2022-01-10 12:35:14,679 iteration 6064 : loss : 0.016740, loss_ce: 0.004829
2022-01-10 12:35:16,326 iteration 6065 : loss : 0.018720, loss_ce: 0.009262
2022-01-10 12:35:17,881 iteration 6066 : loss : 0.015692, loss_ce: 0.006056
2022-01-10 12:35:19,404 iteration 6067 : loss : 0.022716, loss_ce: 0.004097
2022-01-10 12:35:21,060 iteration 6068 : loss : 0.020866, loss_ce: 0.007646
2022-01-10 12:35:22,606 iteration 6069 : loss : 0.039802, loss_ce: 0.004376
 89%|█████████████████████████▉   | 357/400 [2:54:01<20:36, 28.75s/it]2022-01-10 12:35:24,278 iteration 6070 : loss : 0.017029, loss_ce: 0.008418
2022-01-10 12:35:25,890 iteration 6071 : loss : 0.015662, loss_ce: 0.004461
2022-01-10 12:35:27,466 iteration 6072 : loss : 0.014809, loss_ce: 0.004855
2022-01-10 12:35:29,069 iteration 6073 : loss : 0.019880, loss_ce: 0.007124
2022-01-10 12:35:30,653 iteration 6074 : loss : 0.018562, loss_ce: 0.006047
2022-01-10 12:35:32,291 iteration 6075 : loss : 0.019181, loss_ce: 0.008080
2022-01-10 12:35:33,837 iteration 6076 : loss : 0.017823, loss_ce: 0.008248
2022-01-10 12:35:35,373 iteration 6077 : loss : 0.019755, loss_ce: 0.010073
2022-01-10 12:35:36,925 iteration 6078 : loss : 0.017635, loss_ce: 0.005486
2022-01-10 12:35:38,500 iteration 6079 : loss : 0.029599, loss_ce: 0.004676
2022-01-10 12:35:40,068 iteration 6080 : loss : 0.015096, loss_ce: 0.004951
2022-01-10 12:35:41,557 iteration 6081 : loss : 0.021164, loss_ce: 0.006380
2022-01-10 12:35:43,176 iteration 6082 : loss : 0.019743, loss_ce: 0.008554
2022-01-10 12:35:44,819 iteration 6083 : loss : 0.029422, loss_ce: 0.010899
2022-01-10 12:35:46,340 iteration 6084 : loss : 0.023940, loss_ce: 0.009424
2022-01-10 12:35:47,857 iteration 6085 : loss : 0.015083, loss_ce: 0.007176
2022-01-10 12:35:49,541 iteration 6086 : loss : 0.035716, loss_ce: 0.012410
 90%|█████████████████████████▉   | 358/400 [2:54:28<19:44, 28.20s/it]2022-01-10 12:35:51,170 iteration 6087 : loss : 0.021950, loss_ce: 0.007618
2022-01-10 12:35:52,738 iteration 6088 : loss : 0.018413, loss_ce: 0.005511
2022-01-10 12:35:54,355 iteration 6089 : loss : 0.016182, loss_ce: 0.007629
2022-01-10 12:35:55,899 iteration 6090 : loss : 0.018028, loss_ce: 0.005050
2022-01-10 12:35:57,417 iteration 6091 : loss : 0.009212, loss_ce: 0.002734
2022-01-10 12:35:59,052 iteration 6092 : loss : 0.026240, loss_ce: 0.010488
2022-01-10 12:36:00,722 iteration 6093 : loss : 0.022793, loss_ce: 0.011089
2022-01-10 12:36:02,357 iteration 6094 : loss : 0.021157, loss_ce: 0.010748
2022-01-10 12:36:03,861 iteration 6095 : loss : 0.017841, loss_ce: 0.004580
2022-01-10 12:36:05,503 iteration 6096 : loss : 0.033098, loss_ce: 0.009006
2022-01-10 12:36:07,019 iteration 6097 : loss : 0.011937, loss_ce: 0.005580
2022-01-10 12:36:08,606 iteration 6098 : loss : 0.014446, loss_ce: 0.007426
2022-01-10 12:36:10,169 iteration 6099 : loss : 0.017360, loss_ce: 0.007674
2022-01-10 12:36:11,762 iteration 6100 : loss : 0.018273, loss_ce: 0.005793
2022-01-10 12:36:13,404 iteration 6101 : loss : 0.020287, loss_ce: 0.008335
2022-01-10 12:36:14,923 iteration 6102 : loss : 0.011894, loss_ce: 0.004787
2022-01-10 12:36:16,513 iteration 6103 : loss : 0.017706, loss_ce: 0.005369
 90%|██████████████████████████   | 359/400 [2:54:55<19:01, 27.84s/it]2022-01-10 12:36:18,181 iteration 6104 : loss : 0.013581, loss_ce: 0.005384
2022-01-10 12:36:19,722 iteration 6105 : loss : 0.021476, loss_ce: 0.005852
2022-01-10 12:36:21,234 iteration 6106 : loss : 0.009729, loss_ce: 0.004058
2022-01-10 12:36:22,743 iteration 6107 : loss : 0.013434, loss_ce: 0.005277
2022-01-10 12:36:24,298 iteration 6108 : loss : 0.015221, loss_ce: 0.005187
2022-01-10 12:36:25,901 iteration 6109 : loss : 0.017117, loss_ce: 0.008089
2022-01-10 12:36:27,407 iteration 6110 : loss : 0.013297, loss_ce: 0.006507
2022-01-10 12:36:28,979 iteration 6111 : loss : 0.018537, loss_ce: 0.006718
2022-01-10 12:36:30,591 iteration 6112 : loss : 0.015835, loss_ce: 0.004648
2022-01-10 12:36:32,149 iteration 6113 : loss : 0.014680, loss_ce: 0.005206
2022-01-10 12:36:33,723 iteration 6114 : loss : 0.014578, loss_ce: 0.005602
2022-01-10 12:36:35,345 iteration 6115 : loss : 0.016563, loss_ce: 0.004990
2022-01-10 12:36:36,907 iteration 6116 : loss : 0.020357, loss_ce: 0.006888
2022-01-10 12:36:38,442 iteration 6117 : loss : 0.012519, loss_ce: 0.004333
2022-01-10 12:36:40,028 iteration 6118 : loss : 0.014937, loss_ce: 0.008090
2022-01-10 12:36:41,581 iteration 6119 : loss : 0.013164, loss_ce: 0.005716
2022-01-10 12:36:41,581 Training Data Eval:
2022-01-10 12:36:49,493   Average segmentation loss on training set: 0.0080
2022-01-10 12:36:49,493 Validation Data Eval:
2022-01-10 12:36:52,227   Average segmentation loss on validation set: 0.0700
2022-01-10 12:36:53,834 iteration 6120 : loss : 0.018144, loss_ce: 0.006738
 90%|██████████████████████████   | 360/400 [2:55:32<20:27, 30.68s/it]2022-01-10 12:36:55,494 iteration 6121 : loss : 0.018155, loss_ce: 0.005817
2022-01-10 12:36:57,067 iteration 6122 : loss : 0.017154, loss_ce: 0.008068
2022-01-10 12:36:58,617 iteration 6123 : loss : 0.012720, loss_ce: 0.004471
2022-01-10 12:37:00,259 iteration 6124 : loss : 0.017456, loss_ce: 0.006141
2022-01-10 12:37:01,834 iteration 6125 : loss : 0.012747, loss_ce: 0.004041
2022-01-10 12:37:03,507 iteration 6126 : loss : 0.022314, loss_ce: 0.009203
2022-01-10 12:37:05,180 iteration 6127 : loss : 0.018282, loss_ce: 0.009630
2022-01-10 12:37:06,742 iteration 6128 : loss : 0.012291, loss_ce: 0.004477
2022-01-10 12:37:08,264 iteration 6129 : loss : 0.013820, loss_ce: 0.004437
2022-01-10 12:37:09,847 iteration 6130 : loss : 0.023050, loss_ce: 0.009596
2022-01-10 12:37:11,376 iteration 6131 : loss : 0.016875, loss_ce: 0.005298
2022-01-10 12:37:12,997 iteration 6132 : loss : 0.015862, loss_ce: 0.004500
2022-01-10 12:37:14,605 iteration 6133 : loss : 0.014992, loss_ce: 0.006428
2022-01-10 12:37:16,151 iteration 6134 : loss : 0.014764, loss_ce: 0.005026
2022-01-10 12:37:17,666 iteration 6135 : loss : 0.012575, loss_ce: 0.006007
2022-01-10 12:37:19,190 iteration 6136 : loss : 0.009545, loss_ce: 0.002805
2022-01-10 12:37:20,717 iteration 6137 : loss : 0.016678, loss_ce: 0.005427
 90%|██████████████████████████▏  | 361/400 [2:55:59<19:12, 29.54s/it]2022-01-10 12:37:22,298 iteration 6138 : loss : 0.029838, loss_ce: 0.008765
2022-01-10 12:37:23,831 iteration 6139 : loss : 0.012936, loss_ce: 0.002991
2022-01-10 12:37:25,355 iteration 6140 : loss : 0.015822, loss_ce: 0.005670
2022-01-10 12:37:26,853 iteration 6141 : loss : 0.014016, loss_ce: 0.006467
2022-01-10 12:37:28,446 iteration 6142 : loss : 0.014423, loss_ce: 0.006321
2022-01-10 12:37:29,962 iteration 6143 : loss : 0.013568, loss_ce: 0.006033
2022-01-10 12:37:31,606 iteration 6144 : loss : 0.016793, loss_ce: 0.007460
2022-01-10 12:37:33,125 iteration 6145 : loss : 0.016217, loss_ce: 0.007290
2022-01-10 12:37:34,611 iteration 6146 : loss : 0.013913, loss_ce: 0.004400
2022-01-10 12:37:36,155 iteration 6147 : loss : 0.013775, loss_ce: 0.005277
2022-01-10 12:37:37,692 iteration 6148 : loss : 0.013667, loss_ce: 0.005523
2022-01-10 12:37:39,215 iteration 6149 : loss : 0.017893, loss_ce: 0.006262
2022-01-10 12:37:40,802 iteration 6150 : loss : 0.022157, loss_ce: 0.007051
2022-01-10 12:37:42,503 iteration 6151 : loss : 0.017275, loss_ce: 0.004959
2022-01-10 12:37:44,059 iteration 6152 : loss : 0.011982, loss_ce: 0.004853
2022-01-10 12:37:45,644 iteration 6153 : loss : 0.014611, loss_ce: 0.005245
2022-01-10 12:37:47,284 iteration 6154 : loss : 0.023125, loss_ce: 0.009758
 90%|██████████████████████████▏  | 362/400 [2:56:26<18:08, 28.65s/it]2022-01-10 12:37:48,979 iteration 6155 : loss : 0.021658, loss_ce: 0.008074
2022-01-10 12:37:50,496 iteration 6156 : loss : 0.010261, loss_ce: 0.004234
2022-01-10 12:37:52,045 iteration 6157 : loss : 0.015122, loss_ce: 0.004445
2022-01-10 12:37:53,630 iteration 6158 : loss : 0.011341, loss_ce: 0.004310
2022-01-10 12:37:55,148 iteration 6159 : loss : 0.012598, loss_ce: 0.004065
2022-01-10 12:37:56,702 iteration 6160 : loss : 0.012897, loss_ce: 0.004845
2022-01-10 12:37:58,274 iteration 6161 : loss : 0.012626, loss_ce: 0.003325
2022-01-10 12:37:59,877 iteration 6162 : loss : 0.017501, loss_ce: 0.004075
2022-01-10 12:38:01,456 iteration 6163 : loss : 0.018870, loss_ce: 0.008173
2022-01-10 12:38:03,074 iteration 6164 : loss : 0.015203, loss_ce: 0.006985
2022-01-10 12:38:04,636 iteration 6165 : loss : 0.019877, loss_ce: 0.009220
2022-01-10 12:38:06,188 iteration 6166 : loss : 0.015494, loss_ce: 0.007036
2022-01-10 12:38:07,720 iteration 6167 : loss : 0.025654, loss_ce: 0.011690
2022-01-10 12:38:09,328 iteration 6168 : loss : 0.022146, loss_ce: 0.007872
2022-01-10 12:38:10,919 iteration 6169 : loss : 0.014486, loss_ce: 0.004353
2022-01-10 12:38:12,461 iteration 6170 : loss : 0.016580, loss_ce: 0.006525
2022-01-10 12:38:13,985 iteration 6171 : loss : 0.012502, loss_ce: 0.004829
 91%|██████████████████████████▎  | 363/400 [2:56:53<17:18, 28.06s/it]2022-01-10 12:38:15,571 iteration 6172 : loss : 0.012894, loss_ce: 0.004589
2022-01-10 12:38:17,214 iteration 6173 : loss : 0.026574, loss_ce: 0.012182
2022-01-10 12:38:18,774 iteration 6174 : loss : 0.014679, loss_ce: 0.005136
2022-01-10 12:38:20,380 iteration 6175 : loss : 0.025811, loss_ce: 0.010691
2022-01-10 12:38:21,959 iteration 6176 : loss : 0.013905, loss_ce: 0.005171
2022-01-10 12:38:23,608 iteration 6177 : loss : 0.024922, loss_ce: 0.011476
2022-01-10 12:38:25,177 iteration 6178 : loss : 0.015615, loss_ce: 0.006363
2022-01-10 12:38:26,740 iteration 6179 : loss : 0.011452, loss_ce: 0.005273
2022-01-10 12:38:28,311 iteration 6180 : loss : 0.015448, loss_ce: 0.005932
2022-01-10 12:38:29,783 iteration 6181 : loss : 0.010196, loss_ce: 0.003388
2022-01-10 12:38:31,353 iteration 6182 : loss : 0.019070, loss_ce: 0.005558
2022-01-10 12:38:32,991 iteration 6183 : loss : 0.021844, loss_ce: 0.009635
2022-01-10 12:38:34,579 iteration 6184 : loss : 0.028112, loss_ce: 0.008485
2022-01-10 12:38:36,131 iteration 6185 : loss : 0.014362, loss_ce: 0.005444
2022-01-10 12:38:37,706 iteration 6186 : loss : 0.013410, loss_ce: 0.004518
2022-01-10 12:38:39,329 iteration 6187 : loss : 0.013165, loss_ce: 0.004540
2022-01-10 12:38:40,809 iteration 6188 : loss : 0.013365, loss_ce: 0.004422
 91%|██████████████████████████▍  | 364/400 [2:57:19<16:36, 27.69s/it]2022-01-10 12:38:42,361 iteration 6189 : loss : 0.010772, loss_ce: 0.003216
2022-01-10 12:38:43,833 iteration 6190 : loss : 0.013014, loss_ce: 0.004986
2022-01-10 12:38:45,522 iteration 6191 : loss : 0.024035, loss_ce: 0.008683
2022-01-10 12:38:47,133 iteration 6192 : loss : 0.013480, loss_ce: 0.004918
2022-01-10 12:38:48,588 iteration 6193 : loss : 0.009316, loss_ce: 0.003477
2022-01-10 12:38:50,141 iteration 6194 : loss : 0.017297, loss_ce: 0.006345
2022-01-10 12:38:51,739 iteration 6195 : loss : 0.010806, loss_ce: 0.004259
2022-01-10 12:38:53,341 iteration 6196 : loss : 0.011650, loss_ce: 0.004735
2022-01-10 12:38:54,922 iteration 6197 : loss : 0.017229, loss_ce: 0.009477
2022-01-10 12:38:56,486 iteration 6198 : loss : 0.013601, loss_ce: 0.004649
2022-01-10 12:38:58,050 iteration 6199 : loss : 0.015925, loss_ce: 0.004135
2022-01-10 12:38:59,532 iteration 6200 : loss : 0.009128, loss_ce: 0.004225
2022-01-10 12:39:01,098 iteration 6201 : loss : 0.020510, loss_ce: 0.007471
2022-01-10 12:39:02,613 iteration 6202 : loss : 0.014867, loss_ce: 0.006258
2022-01-10 12:39:04,138 iteration 6203 : loss : 0.012925, loss_ce: 0.003088
2022-01-10 12:39:05,723 iteration 6204 : loss : 0.011883, loss_ce: 0.005153
2022-01-10 12:39:05,723 Training Data Eval:
2022-01-10 12:39:13,637   Average segmentation loss on training set: 0.0079
2022-01-10 12:39:13,637 Validation Data Eval:
2022-01-10 12:39:16,371   Average segmentation loss on validation set: 0.0736
2022-01-10 12:39:17,944 iteration 6205 : loss : 0.025688, loss_ce: 0.007378
 91%|██████████████████████████▍  | 365/400 [2:57:57<17:48, 30.53s/it]2022-01-10 12:39:19,583 iteration 6206 : loss : 0.014450, loss_ce: 0.006704
2022-01-10 12:39:21,117 iteration 6207 : loss : 0.018238, loss_ce: 0.005798
2022-01-10 12:39:22,751 iteration 6208 : loss : 0.022424, loss_ce: 0.007773
2022-01-10 12:39:24,290 iteration 6209 : loss : 0.015147, loss_ce: 0.004182
2022-01-10 12:39:25,805 iteration 6210 : loss : 0.016059, loss_ce: 0.006360
2022-01-10 12:39:27,401 iteration 6211 : loss : 0.016852, loss_ce: 0.007152
2022-01-10 12:39:28,930 iteration 6212 : loss : 0.011401, loss_ce: 0.003696
2022-01-10 12:39:30,461 iteration 6213 : loss : 0.019644, loss_ce: 0.004430
2022-01-10 12:39:31,994 iteration 6214 : loss : 0.020064, loss_ce: 0.003619
2022-01-10 12:39:33,583 iteration 6215 : loss : 0.015287, loss_ce: 0.007107
2022-01-10 12:39:35,152 iteration 6216 : loss : 0.022274, loss_ce: 0.006315
2022-01-10 12:39:36,635 iteration 6217 : loss : 0.012550, loss_ce: 0.004123
2022-01-10 12:39:38,196 iteration 6218 : loss : 0.011749, loss_ce: 0.005845
2022-01-10 12:39:39,730 iteration 6219 : loss : 0.012744, loss_ce: 0.004673
2022-01-10 12:39:41,281 iteration 6220 : loss : 0.016639, loss_ce: 0.004758
2022-01-10 12:39:42,924 iteration 6221 : loss : 0.013115, loss_ce: 0.005674
2022-01-10 12:39:44,501 iteration 6222 : loss : 0.016726, loss_ce: 0.008158
 92%|██████████████████████████▌  | 366/400 [2:58:23<16:37, 29.34s/it]2022-01-10 12:39:46,151 iteration 6223 : loss : 0.020299, loss_ce: 0.004889
2022-01-10 12:39:47,694 iteration 6224 : loss : 0.013507, loss_ce: 0.004009
2022-01-10 12:39:49,245 iteration 6225 : loss : 0.019259, loss_ce: 0.007285
2022-01-10 12:39:50,880 iteration 6226 : loss : 0.018924, loss_ce: 0.007363
2022-01-10 12:39:52,443 iteration 6227 : loss : 0.014498, loss_ce: 0.003821
2022-01-10 12:39:54,026 iteration 6228 : loss : 0.016735, loss_ce: 0.006763
2022-01-10 12:39:55,528 iteration 6229 : loss : 0.011380, loss_ce: 0.004720
2022-01-10 12:39:57,047 iteration 6230 : loss : 0.015370, loss_ce: 0.006308
2022-01-10 12:39:58,616 iteration 6231 : loss : 0.014411, loss_ce: 0.006349
2022-01-10 12:40:00,192 iteration 6232 : loss : 0.020778, loss_ce: 0.007537
2022-01-10 12:40:01,739 iteration 6233 : loss : 0.016771, loss_ce: 0.007121
2022-01-10 12:40:03,322 iteration 6234 : loss : 0.014717, loss_ce: 0.005648
2022-01-10 12:40:04,967 iteration 6235 : loss : 0.049591, loss_ce: 0.012962
2022-01-10 12:40:06,451 iteration 6236 : loss : 0.010053, loss_ce: 0.003975
2022-01-10 12:40:08,000 iteration 6237 : loss : 0.015428, loss_ce: 0.005829
2022-01-10 12:40:09,501 iteration 6238 : loss : 0.008142, loss_ce: 0.002989
2022-01-10 12:40:11,023 iteration 6239 : loss : 0.011066, loss_ce: 0.004823
 92%|██████████████████████████▌  | 367/400 [2:58:50<15:40, 28.49s/it]2022-01-10 12:40:12,686 iteration 6240 : loss : 0.016940, loss_ce: 0.006654
2022-01-10 12:40:14,294 iteration 6241 : loss : 0.024109, loss_ce: 0.011169
2022-01-10 12:40:15,902 iteration 6242 : loss : 0.016222, loss_ce: 0.007538
2022-01-10 12:40:17,395 iteration 6243 : loss : 0.011722, loss_ce: 0.004302
2022-01-10 12:40:18,913 iteration 6244 : loss : 0.011946, loss_ce: 0.003332
2022-01-10 12:40:20,451 iteration 6245 : loss : 0.018243, loss_ce: 0.007865
2022-01-10 12:40:21,975 iteration 6246 : loss : 0.013656, loss_ce: 0.005281
2022-01-10 12:40:23,494 iteration 6247 : loss : 0.015473, loss_ce: 0.003655
2022-01-10 12:40:25,101 iteration 6248 : loss : 0.016816, loss_ce: 0.007135
2022-01-10 12:40:26,637 iteration 6249 : loss : 0.013377, loss_ce: 0.004847
2022-01-10 12:40:28,223 iteration 6250 : loss : 0.021801, loss_ce: 0.010243
2022-01-10 12:40:29,875 iteration 6251 : loss : 0.015669, loss_ce: 0.005683
2022-01-10 12:40:31,477 iteration 6252 : loss : 0.019262, loss_ce: 0.005089
2022-01-10 12:40:33,009 iteration 6253 : loss : 0.015675, loss_ce: 0.005526
2022-01-10 12:40:34,501 iteration 6254 : loss : 0.018565, loss_ce: 0.007330
2022-01-10 12:40:36,115 iteration 6255 : loss : 0.015834, loss_ce: 0.006633
2022-01-10 12:40:37,676 iteration 6256 : loss : 0.012420, loss_ce: 0.004427
 92%|██████████████████████████▋  | 368/400 [2:59:16<14:54, 27.94s/it]2022-01-10 12:40:39,323 iteration 6257 : loss : 0.014146, loss_ce: 0.004274
2022-01-10 12:40:40,834 iteration 6258 : loss : 0.015771, loss_ce: 0.005561
2022-01-10 12:40:42,448 iteration 6259 : loss : 0.016601, loss_ce: 0.006788
2022-01-10 12:40:44,135 iteration 6260 : loss : 0.030081, loss_ce: 0.006171
2022-01-10 12:40:45,711 iteration 6261 : loss : 0.017961, loss_ce: 0.007330
2022-01-10 12:40:47,326 iteration 6262 : loss : 0.015577, loss_ce: 0.005955
2022-01-10 12:40:48,860 iteration 6263 : loss : 0.013921, loss_ce: 0.004780
2022-01-10 12:40:50,370 iteration 6264 : loss : 0.013219, loss_ce: 0.005635
2022-01-10 12:40:51,934 iteration 6265 : loss : 0.013482, loss_ce: 0.005225
2022-01-10 12:40:53,467 iteration 6266 : loss : 0.010801, loss_ce: 0.003931
2022-01-10 12:40:55,195 iteration 6267 : loss : 0.034989, loss_ce: 0.016242
2022-01-10 12:40:56,753 iteration 6268 : loss : 0.012373, loss_ce: 0.004286
2022-01-10 12:40:58,325 iteration 6269 : loss : 0.012162, loss_ce: 0.004552
2022-01-10 12:40:59,867 iteration 6270 : loss : 0.011625, loss_ce: 0.005023
2022-01-10 12:41:01,503 iteration 6271 : loss : 0.017462, loss_ce: 0.007390
2022-01-10 12:41:03,022 iteration 6272 : loss : 0.015962, loss_ce: 0.004870
2022-01-10 12:41:04,671 iteration 6273 : loss : 0.020865, loss_ce: 0.007873
 92%|██████████████████████████▊  | 369/400 [2:59:43<14:17, 27.66s/it]2022-01-10 12:41:06,396 iteration 6274 : loss : 0.017281, loss_ce: 0.006191
2022-01-10 12:41:07,935 iteration 6275 : loss : 0.014543, loss_ce: 0.004051
2022-01-10 12:41:09,433 iteration 6276 : loss : 0.013592, loss_ce: 0.005529
2022-01-10 12:41:10,996 iteration 6277 : loss : 0.011487, loss_ce: 0.004241
2022-01-10 12:41:12,623 iteration 6278 : loss : 0.020189, loss_ce: 0.007675
2022-01-10 12:41:14,242 iteration 6279 : loss : 0.016202, loss_ce: 0.006131
2022-01-10 12:41:15,753 iteration 6280 : loss : 0.013446, loss_ce: 0.005269
2022-01-10 12:41:17,295 iteration 6281 : loss : 0.017214, loss_ce: 0.005031
2022-01-10 12:41:18,849 iteration 6282 : loss : 0.015703, loss_ce: 0.007633
2022-01-10 12:41:20,491 iteration 6283 : loss : 0.016141, loss_ce: 0.007093
2022-01-10 12:41:22,083 iteration 6284 : loss : 0.011246, loss_ce: 0.003405
2022-01-10 12:41:23,670 iteration 6285 : loss : 0.022969, loss_ce: 0.013636
2022-01-10 12:41:25,290 iteration 6286 : loss : 0.014908, loss_ce: 0.008323
2022-01-10 12:41:26,856 iteration 6287 : loss : 0.015373, loss_ce: 0.005822
2022-01-10 12:41:28,342 iteration 6288 : loss : 0.010710, loss_ce: 0.003835
2022-01-10 12:41:29,884 iteration 6289 : loss : 0.014655, loss_ce: 0.006762
2022-01-10 12:41:29,885 Training Data Eval:
2022-01-10 12:41:37,805   Average segmentation loss on training set: 0.0075
2022-01-10 12:41:37,805 Validation Data Eval:
2022-01-10 12:41:40,540   Average segmentation loss on validation set: 0.0659
2022-01-10 12:41:42,124 iteration 6290 : loss : 0.011265, loss_ce: 0.004309
 92%|██████████████████████████▊  | 370/400 [3:00:21<15:17, 30.59s/it]2022-01-10 12:41:43,714 iteration 6291 : loss : 0.018150, loss_ce: 0.005371
2022-01-10 12:41:45,184 iteration 6292 : loss : 0.012088, loss_ce: 0.003879
2022-01-10 12:41:46,698 iteration 6293 : loss : 0.017025, loss_ce: 0.005184
2022-01-10 12:41:48,337 iteration 6294 : loss : 0.024643, loss_ce: 0.006700
2022-01-10 12:41:49,896 iteration 6295 : loss : 0.015058, loss_ce: 0.006119
2022-01-10 12:41:51,384 iteration 6296 : loss : 0.010747, loss_ce: 0.004403
2022-01-10 12:41:53,001 iteration 6297 : loss : 0.015812, loss_ce: 0.006139
2022-01-10 12:41:54,559 iteration 6298 : loss : 0.014772, loss_ce: 0.007267
2022-01-10 12:41:56,114 iteration 6299 : loss : 0.015018, loss_ce: 0.005980
2022-01-10 12:41:57,716 iteration 6300 : loss : 0.013458, loss_ce: 0.003688
2022-01-10 12:41:59,296 iteration 6301 : loss : 0.013286, loss_ce: 0.006521
2022-01-10 12:42:00,927 iteration 6302 : loss : 0.013515, loss_ce: 0.005334
2022-01-10 12:42:02,460 iteration 6303 : loss : 0.012136, loss_ce: 0.004312
2022-01-10 12:42:04,043 iteration 6304 : loss : 0.019383, loss_ce: 0.007627
2022-01-10 12:42:05,575 iteration 6305 : loss : 0.011804, loss_ce: 0.003707
2022-01-10 12:42:07,140 iteration 6306 : loss : 0.017581, loss_ce: 0.009470
2022-01-10 12:42:08,677 iteration 6307 : loss : 0.011706, loss_ce: 0.003714
 93%|██████████████████████████▉  | 371/400 [3:00:47<14:12, 29.39s/it]2022-01-10 12:42:10,337 iteration 6308 : loss : 0.015142, loss_ce: 0.005978
2022-01-10 12:42:11,876 iteration 6309 : loss : 0.016821, loss_ce: 0.004602
2022-01-10 12:42:13,437 iteration 6310 : loss : 0.013897, loss_ce: 0.006476
2022-01-10 12:42:15,056 iteration 6311 : loss : 0.020423, loss_ce: 0.006448
2022-01-10 12:42:16,597 iteration 6312 : loss : 0.013563, loss_ce: 0.006090
2022-01-10 12:42:18,177 iteration 6313 : loss : 0.015901, loss_ce: 0.003951
2022-01-10 12:42:19,777 iteration 6314 : loss : 0.014671, loss_ce: 0.005450
2022-01-10 12:42:21,321 iteration 6315 : loss : 0.025151, loss_ce: 0.011568
2022-01-10 12:42:22,815 iteration 6316 : loss : 0.012129, loss_ce: 0.003674
2022-01-10 12:42:24,296 iteration 6317 : loss : 0.019919, loss_ce: 0.007512
2022-01-10 12:42:25,897 iteration 6318 : loss : 0.018641, loss_ce: 0.008478
2022-01-10 12:42:27,433 iteration 6319 : loss : 0.014327, loss_ce: 0.005583
2022-01-10 12:42:28,991 iteration 6320 : loss : 0.012808, loss_ce: 0.006413
2022-01-10 12:42:30,544 iteration 6321 : loss : 0.012996, loss_ce: 0.004805
2022-01-10 12:42:32,185 iteration 6322 : loss : 0.026592, loss_ce: 0.010010
2022-01-10 12:42:33,840 iteration 6323 : loss : 0.054047, loss_ce: 0.013050
2022-01-10 12:42:35,383 iteration 6324 : loss : 0.014548, loss_ce: 0.005815
 93%|██████████████████████████▉  | 372/400 [3:01:14<13:20, 28.58s/it]2022-01-10 12:42:36,958 iteration 6325 : loss : 0.012146, loss_ce: 0.006184
2022-01-10 12:42:38,451 iteration 6326 : loss : 0.009206, loss_ce: 0.003725
2022-01-10 12:42:40,059 iteration 6327 : loss : 0.012969, loss_ce: 0.005523
2022-01-10 12:42:41,607 iteration 6328 : loss : 0.008874, loss_ce: 0.002721
2022-01-10 12:42:43,115 iteration 6329 : loss : 0.016380, loss_ce: 0.004200
2022-01-10 12:42:44,675 iteration 6330 : loss : 0.009807, loss_ce: 0.003056
2022-01-10 12:42:46,417 iteration 6331 : loss : 0.023863, loss_ce: 0.007670
2022-01-10 12:42:47,947 iteration 6332 : loss : 0.021612, loss_ce: 0.010141
2022-01-10 12:42:49,534 iteration 6333 : loss : 0.012694, loss_ce: 0.003933
2022-01-10 12:42:51,096 iteration 6334 : loss : 0.012536, loss_ce: 0.005317
2022-01-10 12:42:52,754 iteration 6335 : loss : 0.019212, loss_ce: 0.007220
2022-01-10 12:42:54,251 iteration 6336 : loss : 0.009346, loss_ce: 0.003586
2022-01-10 12:42:55,833 iteration 6337 : loss : 0.015543, loss_ce: 0.006076
2022-01-10 12:42:57,486 iteration 6338 : loss : 0.014742, loss_ce: 0.002783
2022-01-10 12:42:59,025 iteration 6339 : loss : 0.014195, loss_ce: 0.006291
2022-01-10 12:43:00,559 iteration 6340 : loss : 0.012687, loss_ce: 0.006750
2022-01-10 12:43:02,135 iteration 6341 : loss : 0.015386, loss_ce: 0.005885
 93%|███████████████████████████  | 373/400 [3:01:41<12:36, 28.03s/it]2022-01-10 12:43:03,764 iteration 6342 : loss : 0.022933, loss_ce: 0.008169
2022-01-10 12:43:05,310 iteration 6343 : loss : 0.014336, loss_ce: 0.004268
2022-01-10 12:43:06,948 iteration 6344 : loss : 0.018232, loss_ce: 0.007396
2022-01-10 12:43:08,510 iteration 6345 : loss : 0.011819, loss_ce: 0.003776
2022-01-10 12:43:10,036 iteration 6346 : loss : 0.015264, loss_ce: 0.005918
2022-01-10 12:43:11,632 iteration 6347 : loss : 0.016724, loss_ce: 0.008259
2022-01-10 12:43:13,161 iteration 6348 : loss : 0.011647, loss_ce: 0.004896
2022-01-10 12:43:14,714 iteration 6349 : loss : 0.014983, loss_ce: 0.005237
2022-01-10 12:43:16,271 iteration 6350 : loss : 0.013639, loss_ce: 0.005079
2022-01-10 12:43:17,855 iteration 6351 : loss : 0.017220, loss_ce: 0.008159
2022-01-10 12:43:19,467 iteration 6352 : loss : 0.015113, loss_ce: 0.005081
2022-01-10 12:43:21,141 iteration 6353 : loss : 0.028691, loss_ce: 0.009507
2022-01-10 12:43:22,679 iteration 6354 : loss : 0.021003, loss_ce: 0.004476
2022-01-10 12:43:24,318 iteration 6355 : loss : 0.014743, loss_ce: 0.005096
2022-01-10 12:43:25,973 iteration 6356 : loss : 0.016575, loss_ce: 0.005514
2022-01-10 12:43:27,569 iteration 6357 : loss : 0.015321, loss_ce: 0.007693
2022-01-10 12:43:29,199 iteration 6358 : loss : 0.013962, loss_ce: 0.004482
 94%|███████████████████████████  | 374/400 [3:02:08<12:01, 27.74s/it]2022-01-10 12:43:30,829 iteration 6359 : loss : 0.012161, loss_ce: 0.003757
2022-01-10 12:43:32,286 iteration 6360 : loss : 0.008194, loss_ce: 0.002953
2022-01-10 12:43:33,920 iteration 6361 : loss : 0.013073, loss_ce: 0.004824
2022-01-10 12:43:35,497 iteration 6362 : loss : 0.013125, loss_ce: 0.005173
2022-01-10 12:43:37,159 iteration 6363 : loss : 0.031147, loss_ce: 0.010596
2022-01-10 12:43:38,769 iteration 6364 : loss : 0.009930, loss_ce: 0.002878
2022-01-10 12:43:40,336 iteration 6365 : loss : 0.015229, loss_ce: 0.007298
2022-01-10 12:43:41,896 iteration 6366 : loss : 0.013157, loss_ce: 0.004612
2022-01-10 12:43:43,517 iteration 6367 : loss : 0.030481, loss_ce: 0.011593
2022-01-10 12:43:45,115 iteration 6368 : loss : 0.029037, loss_ce: 0.007248
2022-01-10 12:43:46,679 iteration 6369 : loss : 0.017169, loss_ce: 0.003753
2022-01-10 12:43:48,276 iteration 6370 : loss : 0.018887, loss_ce: 0.008421
2022-01-10 12:43:49,793 iteration 6371 : loss : 0.010365, loss_ce: 0.004127
2022-01-10 12:43:51,376 iteration 6372 : loss : 0.016181, loss_ce: 0.006893
2022-01-10 12:43:52,894 iteration 6373 : loss : 0.010743, loss_ce: 0.004838
2022-01-10 12:43:54,363 iteration 6374 : loss : 0.010319, loss_ce: 0.004035
2022-01-10 12:43:54,363 Training Data Eval:
2022-01-10 12:44:02,268   Average segmentation loss on training set: 0.0076
2022-01-10 12:44:02,268 Validation Data Eval:
2022-01-10 12:44:04,999   Average segmentation loss on validation set: 0.0654
2022-01-10 12:44:06,591 iteration 6375 : loss : 0.011487, loss_ce: 0.005882
 94%|███████████████████████████▏ | 375/400 [3:02:45<12:45, 30.64s/it]2022-01-10 12:44:08,284 iteration 6376 : loss : 0.017894, loss_ce: 0.005911
2022-01-10 12:44:09,870 iteration 6377 : loss : 0.016298, loss_ce: 0.004316
2022-01-10 12:44:11,409 iteration 6378 : loss : 0.013758, loss_ce: 0.006555
2022-01-10 12:44:12,976 iteration 6379 : loss : 0.013892, loss_ce: 0.005836
2022-01-10 12:44:14,594 iteration 6380 : loss : 0.018318, loss_ce: 0.006665
2022-01-10 12:44:16,223 iteration 6381 : loss : 0.016499, loss_ce: 0.006948
2022-01-10 12:44:17,793 iteration 6382 : loss : 0.013771, loss_ce: 0.005950
2022-01-10 12:44:19,400 iteration 6383 : loss : 0.012660, loss_ce: 0.004341
2022-01-10 12:44:21,024 iteration 6384 : loss : 0.021358, loss_ce: 0.008020
2022-01-10 12:44:22,464 iteration 6385 : loss : 0.008730, loss_ce: 0.003409
2022-01-10 12:44:24,070 iteration 6386 : loss : 0.014814, loss_ce: 0.004508
2022-01-10 12:44:25,671 iteration 6387 : loss : 0.018375, loss_ce: 0.008934
2022-01-10 12:44:27,170 iteration 6388 : loss : 0.012057, loss_ce: 0.005463
2022-01-10 12:44:28,780 iteration 6389 : loss : 0.014724, loss_ce: 0.006885
2022-01-10 12:44:30,370 iteration 6390 : loss : 0.017136, loss_ce: 0.006378
2022-01-10 12:44:31,981 iteration 6391 : loss : 0.019157, loss_ce: 0.009629
2022-01-10 12:44:33,603 iteration 6392 : loss : 0.019332, loss_ce: 0.004166
 94%|███████████████████████████▎ | 376/400 [3:03:12<11:49, 29.55s/it]2022-01-10 12:44:35,188 iteration 6393 : loss : 0.012897, loss_ce: 0.005952
2022-01-10 12:44:36,735 iteration 6394 : loss : 0.010313, loss_ce: 0.004343
2022-01-10 12:44:38,260 iteration 6395 : loss : 0.009521, loss_ce: 0.002829
2022-01-10 12:44:39,851 iteration 6396 : loss : 0.012888, loss_ce: 0.005031
2022-01-10 12:44:41,461 iteration 6397 : loss : 0.023394, loss_ce: 0.006901
2022-01-10 12:44:43,080 iteration 6398 : loss : 0.012791, loss_ce: 0.004171
2022-01-10 12:44:44,581 iteration 6399 : loss : 0.015899, loss_ce: 0.005121
2022-01-10 12:44:46,120 iteration 6400 : loss : 0.016215, loss_ce: 0.007453
2022-01-10 12:44:47,609 iteration 6401 : loss : 0.015636, loss_ce: 0.005445
2022-01-10 12:44:49,207 iteration 6402 : loss : 0.013529, loss_ce: 0.005818
2022-01-10 12:44:50,730 iteration 6403 : loss : 0.016063, loss_ce: 0.005934
2022-01-10 12:44:52,228 iteration 6404 : loss : 0.012122, loss_ce: 0.006180
2022-01-10 12:44:53,920 iteration 6405 : loss : 0.016597, loss_ce: 0.006380
2022-01-10 12:44:55,425 iteration 6406 : loss : 0.009084, loss_ce: 0.003498
2022-01-10 12:44:57,009 iteration 6407 : loss : 0.016765, loss_ce: 0.006379
2022-01-10 12:44:58,606 iteration 6408 : loss : 0.016733, loss_ce: 0.006161
2022-01-10 12:45:00,117 iteration 6409 : loss : 0.009610, loss_ce: 0.003338
 94%|███████████████████████████▎ | 377/400 [3:03:39<10:58, 28.64s/it]2022-01-10 12:45:01,678 iteration 6410 : loss : 0.010642, loss_ce: 0.003922
2022-01-10 12:45:03,214 iteration 6411 : loss : 0.011670, loss_ce: 0.004591
2022-01-10 12:45:04,836 iteration 6412 : loss : 0.016982, loss_ce: 0.005644
2022-01-10 12:45:06,404 iteration 6413 : loss : 0.014210, loss_ce: 0.005905
2022-01-10 12:45:08,012 iteration 6414 : loss : 0.016520, loss_ce: 0.006437
2022-01-10 12:45:09,618 iteration 6415 : loss : 0.021341, loss_ce: 0.005905
2022-01-10 12:45:11,128 iteration 6416 : loss : 0.012491, loss_ce: 0.004381
2022-01-10 12:45:12,728 iteration 6417 : loss : 0.016865, loss_ce: 0.005234
2022-01-10 12:45:14,289 iteration 6418 : loss : 0.014084, loss_ce: 0.005559
2022-01-10 12:45:15,849 iteration 6419 : loss : 0.016585, loss_ce: 0.008392
2022-01-10 12:45:17,436 iteration 6420 : loss : 0.012208, loss_ce: 0.004787
2022-01-10 12:45:19,044 iteration 6421 : loss : 0.013712, loss_ce: 0.005765
2022-01-10 12:45:20,595 iteration 6422 : loss : 0.012527, loss_ce: 0.004920
2022-01-10 12:45:22,167 iteration 6423 : loss : 0.012504, loss_ce: 0.005054
2022-01-10 12:45:23,678 iteration 6424 : loss : 0.012991, loss_ce: 0.004206
2022-01-10 12:45:25,367 iteration 6425 : loss : 0.017215, loss_ce: 0.005261
2022-01-10 12:45:26,874 iteration 6426 : loss : 0.015052, loss_ce: 0.004885
 94%|███████████████████████████▍ | 378/400 [3:04:05<10:17, 28.08s/it]2022-01-10 12:45:28,519 iteration 6427 : loss : 0.015379, loss_ce: 0.005939
2022-01-10 12:45:30,127 iteration 6428 : loss : 0.023728, loss_ce: 0.006976
2022-01-10 12:45:31,677 iteration 6429 : loss : 0.011563, loss_ce: 0.004829
2022-01-10 12:45:33,301 iteration 6430 : loss : 0.016270, loss_ce: 0.008175
2022-01-10 12:45:34,859 iteration 6431 : loss : 0.016724, loss_ce: 0.005209
2022-01-10 12:45:36,420 iteration 6432 : loss : 0.024325, loss_ce: 0.009146
2022-01-10 12:45:37,983 iteration 6433 : loss : 0.013737, loss_ce: 0.003774
2022-01-10 12:45:39,552 iteration 6434 : loss : 0.019589, loss_ce: 0.007531
2022-01-10 12:45:41,021 iteration 6435 : loss : 0.009596, loss_ce: 0.003181
2022-01-10 12:45:42,528 iteration 6436 : loss : 0.011758, loss_ce: 0.005910
2022-01-10 12:45:44,019 iteration 6437 : loss : 0.010854, loss_ce: 0.004086
2022-01-10 12:45:45,498 iteration 6438 : loss : 0.010555, loss_ce: 0.005056
2022-01-10 12:45:47,111 iteration 6439 : loss : 0.017114, loss_ce: 0.005283
2022-01-10 12:45:48,681 iteration 6440 : loss : 0.011570, loss_ce: 0.004323
2022-01-10 12:45:50,256 iteration 6441 : loss : 0.010584, loss_ce: 0.004634
2022-01-10 12:45:51,813 iteration 6442 : loss : 0.010867, loss_ce: 0.005004
2022-01-10 12:45:53,344 iteration 6443 : loss : 0.011372, loss_ce: 0.004008
 95%|███████████████████████████▍ | 379/400 [3:04:32<09:39, 27.59s/it]2022-01-10 12:45:54,934 iteration 6444 : loss : 0.018958, loss_ce: 0.012001
2022-01-10 12:45:56,453 iteration 6445 : loss : 0.008563, loss_ce: 0.003144
2022-01-10 12:45:58,008 iteration 6446 : loss : 0.017035, loss_ce: 0.005548
2022-01-10 12:45:59,566 iteration 6447 : loss : 0.011269, loss_ce: 0.004909
2022-01-10 12:46:01,117 iteration 6448 : loss : 0.016692, loss_ce: 0.003438
2022-01-10 12:46:02,694 iteration 6449 : loss : 0.015508, loss_ce: 0.005045
2022-01-10 12:46:04,311 iteration 6450 : loss : 0.019476, loss_ce: 0.005695
2022-01-10 12:46:05,855 iteration 6451 : loss : 0.013527, loss_ce: 0.004884
2022-01-10 12:46:07,459 iteration 6452 : loss : 0.027120, loss_ce: 0.007183
2022-01-10 12:46:08,998 iteration 6453 : loss : 0.011988, loss_ce: 0.005285
2022-01-10 12:46:10,560 iteration 6454 : loss : 0.017050, loss_ce: 0.005431
2022-01-10 12:46:12,069 iteration 6455 : loss : 0.019079, loss_ce: 0.007622
2022-01-10 12:46:13,643 iteration 6456 : loss : 0.015605, loss_ce: 0.007493
2022-01-10 12:46:15,182 iteration 6457 : loss : 0.010364, loss_ce: 0.004154
2022-01-10 12:46:16,694 iteration 6458 : loss : 0.012845, loss_ce: 0.006495
2022-01-10 12:46:18,309 iteration 6459 : loss : 0.014645, loss_ce: 0.005656
2022-01-10 12:46:18,309 Training Data Eval:
2022-01-10 12:46:26,225   Average segmentation loss on training set: 0.0075
2022-01-10 12:46:26,225 Validation Data Eval:
2022-01-10 12:46:28,968   Average segmentation loss on validation set: 0.0676
2022-01-10 12:46:30,605 iteration 6460 : loss : 0.013041, loss_ce: 0.004574
 95%|███████████████████████████▌ | 380/400 [3:05:09<10:09, 30.49s/it]2022-01-10 12:46:32,196 iteration 6461 : loss : 0.010842, loss_ce: 0.003696
2022-01-10 12:46:33,765 iteration 6462 : loss : 0.012810, loss_ce: 0.004925
2022-01-10 12:46:35,274 iteration 6463 : loss : 0.010930, loss_ce: 0.004803
2022-01-10 12:46:36,810 iteration 6464 : loss : 0.016363, loss_ce: 0.007190
2022-01-10 12:46:38,364 iteration 6465 : loss : 0.011468, loss_ce: 0.003994
2022-01-10 12:46:39,916 iteration 6466 : loss : 0.023181, loss_ce: 0.010038
2022-01-10 12:46:41,474 iteration 6467 : loss : 0.015067, loss_ce: 0.006018
2022-01-10 12:46:42,981 iteration 6468 : loss : 0.012869, loss_ce: 0.004913
2022-01-10 12:46:44,585 iteration 6469 : loss : 0.021864, loss_ce: 0.005317
2022-01-10 12:46:46,172 iteration 6470 : loss : 0.012719, loss_ce: 0.005510
2022-01-10 12:46:47,748 iteration 6471 : loss : 0.018100, loss_ce: 0.008161
2022-01-10 12:46:49,259 iteration 6472 : loss : 0.017234, loss_ce: 0.005918
2022-01-10 12:46:50,852 iteration 6473 : loss : 0.016314, loss_ce: 0.007111
2022-01-10 12:46:52,440 iteration 6474 : loss : 0.011866, loss_ce: 0.005073
2022-01-10 12:46:54,049 iteration 6475 : loss : 0.017830, loss_ce: 0.006317
2022-01-10 12:46:55,639 iteration 6476 : loss : 0.014517, loss_ce: 0.004787
2022-01-10 12:46:57,215 iteration 6477 : loss : 0.014417, loss_ce: 0.006011
 95%|███████████████████████████▌ | 381/400 [3:05:36<09:17, 29.33s/it]2022-01-10 12:46:58,839 iteration 6478 : loss : 0.013141, loss_ce: 0.005264
2022-01-10 12:47:00,404 iteration 6479 : loss : 0.016936, loss_ce: 0.005910
2022-01-10 12:47:01,936 iteration 6480 : loss : 0.009945, loss_ce: 0.003629
2022-01-10 12:47:03,633 iteration 6481 : loss : 0.022827, loss_ce: 0.007787
2022-01-10 12:47:05,174 iteration 6482 : loss : 0.015804, loss_ce: 0.006704
2022-01-10 12:47:06,792 iteration 6483 : loss : 0.021107, loss_ce: 0.007920
2022-01-10 12:47:08,268 iteration 6484 : loss : 0.013724, loss_ce: 0.004826
2022-01-10 12:47:09,832 iteration 6485 : loss : 0.011389, loss_ce: 0.003572
2022-01-10 12:47:11,412 iteration 6486 : loss : 0.013966, loss_ce: 0.004954
2022-01-10 12:47:13,010 iteration 6487 : loss : 0.015853, loss_ce: 0.006243
2022-01-10 12:47:14,618 iteration 6488 : loss : 0.017278, loss_ce: 0.005753
2022-01-10 12:47:16,188 iteration 6489 : loss : 0.015557, loss_ce: 0.005101
2022-01-10 12:47:17,741 iteration 6490 : loss : 0.014238, loss_ce: 0.004870
2022-01-10 12:47:19,259 iteration 6491 : loss : 0.013001, loss_ce: 0.006064
2022-01-10 12:47:20,774 iteration 6492 : loss : 0.010409, loss_ce: 0.005553
2022-01-10 12:47:22,317 iteration 6493 : loss : 0.016146, loss_ce: 0.005941
2022-01-10 12:47:23,897 iteration 6494 : loss : 0.023695, loss_ce: 0.008105
 96%|███████████████████████████▋ | 382/400 [3:06:02<08:33, 28.53s/it]2022-01-10 12:47:25,455 iteration 6495 : loss : 0.012621, loss_ce: 0.005845
2022-01-10 12:47:27,009 iteration 6496 : loss : 0.014160, loss_ce: 0.004486
2022-01-10 12:47:28,605 iteration 6497 : loss : 0.012050, loss_ce: 0.006505
2022-01-10 12:47:30,131 iteration 6498 : loss : 0.013855, loss_ce: 0.003410
2022-01-10 12:47:31,680 iteration 6499 : loss : 0.018554, loss_ce: 0.003938
2022-01-10 12:47:33,340 iteration 6500 : loss : 0.021959, loss_ce: 0.005863
2022-01-10 12:47:34,976 iteration 6501 : loss : 0.028715, loss_ce: 0.017238
2022-01-10 12:47:36,559 iteration 6502 : loss : 0.010977, loss_ce: 0.003408
2022-01-10 12:47:38,139 iteration 6503 : loss : 0.012938, loss_ce: 0.004901
2022-01-10 12:47:39,728 iteration 6504 : loss : 0.018772, loss_ce: 0.009560
2022-01-10 12:47:41,228 iteration 6505 : loss : 0.008850, loss_ce: 0.003409
2022-01-10 12:47:42,809 iteration 6506 : loss : 0.021002, loss_ce: 0.004750
2022-01-10 12:47:44,342 iteration 6507 : loss : 0.011657, loss_ce: 0.004652
2022-01-10 12:47:45,820 iteration 6508 : loss : 0.012112, loss_ce: 0.004697
2022-01-10 12:47:47,365 iteration 6509 : loss : 0.012013, loss_ce: 0.004491
2022-01-10 12:47:48,821 iteration 6510 : loss : 0.008553, loss_ce: 0.003307
2022-01-10 12:47:50,400 iteration 6511 : loss : 0.012844, loss_ce: 0.005373
 96%|███████████████████████████▊ | 383/400 [3:06:29<07:54, 27.92s/it]2022-01-10 12:47:52,049 iteration 6512 : loss : 0.018117, loss_ce: 0.007183
2022-01-10 12:47:53,672 iteration 6513 : loss : 0.013759, loss_ce: 0.004848
2022-01-10 12:47:55,250 iteration 6514 : loss : 0.025218, loss_ce: 0.007585
2022-01-10 12:47:56,874 iteration 6515 : loss : 0.017474, loss_ce: 0.006378
2022-01-10 12:47:58,409 iteration 6516 : loss : 0.011836, loss_ce: 0.004727
2022-01-10 12:47:59,924 iteration 6517 : loss : 0.011742, loss_ce: 0.004710
2022-01-10 12:48:01,500 iteration 6518 : loss : 0.017371, loss_ce: 0.006224
2022-01-10 12:48:03,127 iteration 6519 : loss : 0.025012, loss_ce: 0.009855
2022-01-10 12:48:04,630 iteration 6520 : loss : 0.016190, loss_ce: 0.006327
2022-01-10 12:48:06,227 iteration 6521 : loss : 0.013890, loss_ce: 0.006035
2022-01-10 12:48:07,797 iteration 6522 : loss : 0.013122, loss_ce: 0.004382
2022-01-10 12:48:09,392 iteration 6523 : loss : 0.011436, loss_ce: 0.004888
2022-01-10 12:48:10,981 iteration 6524 : loss : 0.015323, loss_ce: 0.005695
2022-01-10 12:48:12,593 iteration 6525 : loss : 0.014956, loss_ce: 0.005194
2022-01-10 12:48:14,159 iteration 6526 : loss : 0.013327, loss_ce: 0.004877
2022-01-10 12:48:15,681 iteration 6527 : loss : 0.028975, loss_ce: 0.010284
2022-01-10 12:48:17,204 iteration 6528 : loss : 0.011698, loss_ce: 0.004654
 96%|███████████████████████████▊ | 384/400 [3:06:56<07:21, 27.59s/it]2022-01-10 12:48:18,984 iteration 6529 : loss : 0.014612, loss_ce: 0.006246
2022-01-10 12:48:20,545 iteration 6530 : loss : 0.027045, loss_ce: 0.014620
2022-01-10 12:48:22,205 iteration 6531 : loss : 0.021342, loss_ce: 0.009060
2022-01-10 12:48:23,789 iteration 6532 : loss : 0.017715, loss_ce: 0.007548
2022-01-10 12:48:25,408 iteration 6533 : loss : 0.023269, loss_ce: 0.004616
2022-01-10 12:48:27,122 iteration 6534 : loss : 0.025474, loss_ce: 0.005343
2022-01-10 12:48:28,630 iteration 6535 : loss : 0.013342, loss_ce: 0.002431
2022-01-10 12:48:30,200 iteration 6536 : loss : 0.028387, loss_ce: 0.010848
2022-01-10 12:48:31,772 iteration 6537 : loss : 0.014261, loss_ce: 0.005146
2022-01-10 12:48:33,364 iteration 6538 : loss : 0.021740, loss_ce: 0.010295
2022-01-10 12:48:35,029 iteration 6539 : loss : 0.025756, loss_ce: 0.010024
2022-01-10 12:48:36,598 iteration 6540 : loss : 0.012352, loss_ce: 0.005477
2022-01-10 12:48:38,144 iteration 6541 : loss : 0.016155, loss_ce: 0.006681
2022-01-10 12:48:39,765 iteration 6542 : loss : 0.018890, loss_ce: 0.008578
2022-01-10 12:48:41,364 iteration 6543 : loss : 0.019689, loss_ce: 0.006759
2022-01-10 12:48:42,858 iteration 6544 : loss : 0.008952, loss_ce: 0.003054
2022-01-10 12:48:42,858 Training Data Eval:
2022-01-10 12:48:50,775   Average segmentation loss on training set: 0.0074
2022-01-10 12:48:50,775 Validation Data Eval:
2022-01-10 12:48:53,502   Average segmentation loss on validation set: 0.0698
2022-01-10 12:48:54,950 iteration 6545 : loss : 0.008717, loss_ce: 0.002030
 96%|███████████████████████████▉ | 385/400 [3:07:34<07:39, 30.64s/it]2022-01-10 12:48:56,595 iteration 6546 : loss : 0.017579, loss_ce: 0.005327
2022-01-10 12:48:58,340 iteration 6547 : loss : 0.019592, loss_ce: 0.007618
2022-01-10 12:48:59,824 iteration 6548 : loss : 0.011961, loss_ce: 0.006341
2022-01-10 12:49:01,295 iteration 6549 : loss : 0.007865, loss_ce: 0.002744
2022-01-10 12:49:02,826 iteration 6550 : loss : 0.012572, loss_ce: 0.004849
2022-01-10 12:49:04,429 iteration 6551 : loss : 0.024630, loss_ce: 0.011770
2022-01-10 12:49:05,900 iteration 6552 : loss : 0.011470, loss_ce: 0.005282
2022-01-10 12:49:07,532 iteration 6553 : loss : 0.030968, loss_ce: 0.011106
2022-01-10 12:49:09,094 iteration 6554 : loss : 0.016510, loss_ce: 0.005435
2022-01-10 12:49:10,726 iteration 6555 : loss : 0.021868, loss_ce: 0.008041
2022-01-10 12:49:12,332 iteration 6556 : loss : 0.021807, loss_ce: 0.007737
2022-01-10 12:49:13,848 iteration 6557 : loss : 0.014415, loss_ce: 0.004397
2022-01-10 12:49:15,364 iteration 6558 : loss : 0.015058, loss_ce: 0.004319
2022-01-10 12:49:16,953 iteration 6559 : loss : 0.013388, loss_ce: 0.005633
2022-01-10 12:49:18,603 iteration 6560 : loss : 0.025219, loss_ce: 0.005635
2022-01-10 12:49:20,126 iteration 6561 : loss : 0.011829, loss_ce: 0.005142
2022-01-10 12:49:21,601 iteration 6562 : loss : 0.008661, loss_ce: 0.002786
 96%|███████████████████████████▉ | 386/400 [3:08:00<06:52, 29.44s/it]2022-01-10 12:49:23,208 iteration 6563 : loss : 0.011594, loss_ce: 0.005487
2022-01-10 12:49:24,730 iteration 6564 : loss : 0.011516, loss_ce: 0.002858
2022-01-10 12:49:26,222 iteration 6565 : loss : 0.008495, loss_ce: 0.002456
2022-01-10 12:49:27,748 iteration 6566 : loss : 0.010292, loss_ce: 0.003990
2022-01-10 12:49:29,336 iteration 6567 : loss : 0.024848, loss_ce: 0.010144
2022-01-10 12:49:30,920 iteration 6568 : loss : 0.015544, loss_ce: 0.006028
2022-01-10 12:49:32,542 iteration 6569 : loss : 0.020987, loss_ce: 0.006409
2022-01-10 12:49:34,104 iteration 6570 : loss : 0.020552, loss_ce: 0.005244
2022-01-10 12:49:35,794 iteration 6571 : loss : 0.020838, loss_ce: 0.010137
2022-01-10 12:49:37,301 iteration 6572 : loss : 0.009619, loss_ce: 0.003880
2022-01-10 12:49:38,924 iteration 6573 : loss : 0.023586, loss_ce: 0.007537
2022-01-10 12:49:40,468 iteration 6574 : loss : 0.009209, loss_ce: 0.002594
2022-01-10 12:49:42,103 iteration 6575 : loss : 0.020042, loss_ce: 0.007021
2022-01-10 12:49:43,675 iteration 6576 : loss : 0.020864, loss_ce: 0.008931
2022-01-10 12:49:45,207 iteration 6577 : loss : 0.015473, loss_ce: 0.005670
2022-01-10 12:49:46,799 iteration 6578 : loss : 0.010025, loss_ce: 0.003844
2022-01-10 12:49:48,335 iteration 6579 : loss : 0.013116, loss_ce: 0.005571
 97%|████████████████████████████ | 387/400 [3:08:27<06:12, 28.62s/it]2022-01-10 12:49:50,008 iteration 6580 : loss : 0.018057, loss_ce: 0.007748
2022-01-10 12:49:51,523 iteration 6581 : loss : 0.011692, loss_ce: 0.003525
2022-01-10 12:49:53,170 iteration 6582 : loss : 0.014170, loss_ce: 0.004949
2022-01-10 12:49:54,745 iteration 6583 : loss : 0.020425, loss_ce: 0.007397
2022-01-10 12:49:56,264 iteration 6584 : loss : 0.017317, loss_ce: 0.006899
2022-01-10 12:49:57,745 iteration 6585 : loss : 0.010387, loss_ce: 0.003322
2022-01-10 12:49:59,277 iteration 6586 : loss : 0.010073, loss_ce: 0.002105
2022-01-10 12:50:00,766 iteration 6587 : loss : 0.011846, loss_ce: 0.005404
2022-01-10 12:50:02,336 iteration 6588 : loss : 0.009432, loss_ce: 0.004132
2022-01-10 12:50:03,913 iteration 6589 : loss : 0.011768, loss_ce: 0.004873
2022-01-10 12:50:05,446 iteration 6590 : loss : 0.011695, loss_ce: 0.005128
2022-01-10 12:50:06,991 iteration 6591 : loss : 0.009720, loss_ce: 0.003792
2022-01-10 12:50:08,627 iteration 6592 : loss : 0.015128, loss_ce: 0.007890
2022-01-10 12:50:10,103 iteration 6593 : loss : 0.009594, loss_ce: 0.005089
2022-01-10 12:50:11,633 iteration 6594 : loss : 0.010488, loss_ce: 0.003616
2022-01-10 12:50:13,228 iteration 6595 : loss : 0.018000, loss_ce: 0.005759
2022-01-10 12:50:14,735 iteration 6596 : loss : 0.021964, loss_ce: 0.006539
 97%|████████████████████████████▏| 388/400 [3:08:53<05:35, 27.96s/it]2022-01-10 12:50:16,311 iteration 6597 : loss : 0.011185, loss_ce: 0.004951
2022-01-10 12:50:17,917 iteration 6598 : loss : 0.017162, loss_ce: 0.002639
2022-01-10 12:50:19,539 iteration 6599 : loss : 0.017619, loss_ce: 0.006916
2022-01-10 12:50:21,049 iteration 6600 : loss : 0.009925, loss_ce: 0.004061
2022-01-10 12:50:22,708 iteration 6601 : loss : 0.014705, loss_ce: 0.006152
2022-01-10 12:50:24,313 iteration 6602 : loss : 0.017099, loss_ce: 0.007040
2022-01-10 12:50:25,843 iteration 6603 : loss : 0.011443, loss_ce: 0.005154
2022-01-10 12:50:27,445 iteration 6604 : loss : 0.015566, loss_ce: 0.007614
2022-01-10 12:50:28,987 iteration 6605 : loss : 0.020239, loss_ce: 0.009862
2022-01-10 12:50:30,618 iteration 6606 : loss : 0.015868, loss_ce: 0.006071
2022-01-10 12:50:32,249 iteration 6607 : loss : 0.014860, loss_ce: 0.005671
2022-01-10 12:50:33,756 iteration 6608 : loss : 0.017981, loss_ce: 0.007329
2022-01-10 12:50:35,369 iteration 6609 : loss : 0.016410, loss_ce: 0.007183
2022-01-10 12:50:36,917 iteration 6610 : loss : 0.011378, loss_ce: 0.004533
2022-01-10 12:50:38,455 iteration 6611 : loss : 0.009207, loss_ce: 0.002337
2022-01-10 12:50:40,010 iteration 6612 : loss : 0.012526, loss_ce: 0.003482
2022-01-10 12:50:41,619 iteration 6613 : loss : 0.018466, loss_ce: 0.006103
 97%|████████████████████████████▏| 389/400 [3:09:20<05:04, 27.64s/it]2022-01-10 12:50:43,207 iteration 6614 : loss : 0.007281, loss_ce: 0.002602
2022-01-10 12:50:44,694 iteration 6615 : loss : 0.008581, loss_ce: 0.003422
2022-01-10 12:50:46,288 iteration 6616 : loss : 0.021509, loss_ce: 0.007254
2022-01-10 12:50:47,830 iteration 6617 : loss : 0.025891, loss_ce: 0.009969
2022-01-10 12:50:49,471 iteration 6618 : loss : 0.016412, loss_ce: 0.006495
2022-01-10 12:50:51,042 iteration 6619 : loss : 0.013322, loss_ce: 0.004577
2022-01-10 12:50:52,615 iteration 6620 : loss : 0.013894, loss_ce: 0.004587
2022-01-10 12:50:54,239 iteration 6621 : loss : 0.016958, loss_ce: 0.005517
2022-01-10 12:50:55,783 iteration 6622 : loss : 0.013651, loss_ce: 0.006223
2022-01-10 12:50:57,290 iteration 6623 : loss : 0.011587, loss_ce: 0.003975
2022-01-10 12:50:58,842 iteration 6624 : loss : 0.013866, loss_ce: 0.006435
2022-01-10 12:51:00,425 iteration 6625 : loss : 0.015147, loss_ce: 0.005785
2022-01-10 12:51:01,984 iteration 6626 : loss : 0.016866, loss_ce: 0.007766
2022-01-10 12:51:03,587 iteration 6627 : loss : 0.017774, loss_ce: 0.007135
2022-01-10 12:51:05,181 iteration 6628 : loss : 0.016717, loss_ce: 0.007415
2022-01-10 12:51:06,862 iteration 6629 : loss : 0.017239, loss_ce: 0.005299
2022-01-10 12:51:06,862 Training Data Eval:
2022-01-10 12:51:14,774   Average segmentation loss on training set: 0.0072
2022-01-10 12:51:14,774 Validation Data Eval:
2022-01-10 12:51:17,504   Average segmentation loss on validation set: 0.0748
2022-01-10 12:51:19,036 iteration 6630 : loss : 0.014249, loss_ce: 0.005871
 98%|████████████████████████████▎| 390/400 [3:09:58<05:05, 30.57s/it]2022-01-10 12:51:20,644 iteration 6631 : loss : 0.017035, loss_ce: 0.005569
2022-01-10 12:51:22,171 iteration 6632 : loss : 0.010469, loss_ce: 0.005169
2022-01-10 12:51:23,771 iteration 6633 : loss : 0.019167, loss_ce: 0.005098
2022-01-10 12:51:25,312 iteration 6634 : loss : 0.014184, loss_ce: 0.005348
2022-01-10 12:51:26,874 iteration 6635 : loss : 0.013004, loss_ce: 0.005501
2022-01-10 12:51:28,441 iteration 6636 : loss : 0.008859, loss_ce: 0.004288
2022-01-10 12:51:29,961 iteration 6637 : loss : 0.012984, loss_ce: 0.004623
2022-01-10 12:51:31,577 iteration 6638 : loss : 0.030373, loss_ce: 0.006348
2022-01-10 12:51:33,195 iteration 6639 : loss : 0.013833, loss_ce: 0.005671
2022-01-10 12:51:34,745 iteration 6640 : loss : 0.011999, loss_ce: 0.003658
2022-01-10 12:51:36,324 iteration 6641 : loss : 0.013504, loss_ce: 0.004058
2022-01-10 12:51:37,898 iteration 6642 : loss : 0.014774, loss_ce: 0.007001
2022-01-10 12:51:39,476 iteration 6643 : loss : 0.014953, loss_ce: 0.005557
2022-01-10 12:51:41,156 iteration 6644 : loss : 0.026193, loss_ce: 0.011585
2022-01-10 12:51:42,695 iteration 6645 : loss : 0.010785, loss_ce: 0.004699
2022-01-10 12:51:44,363 iteration 6646 : loss : 0.022245, loss_ce: 0.006803
2022-01-10 12:51:45,919 iteration 6647 : loss : 0.030364, loss_ce: 0.010947
 98%|████████████████████████████▎| 391/400 [3:10:24<04:25, 29.47s/it]2022-01-10 12:51:47,521 iteration 6648 : loss : 0.017559, loss_ce: 0.004341
2022-01-10 12:51:49,123 iteration 6649 : loss : 0.013429, loss_ce: 0.003795
2022-01-10 12:51:50,769 iteration 6650 : loss : 0.022880, loss_ce: 0.007321
2022-01-10 12:51:52,442 iteration 6651 : loss : 0.017256, loss_ce: 0.007117
2022-01-10 12:51:54,003 iteration 6652 : loss : 0.014663, loss_ce: 0.006639
2022-01-10 12:51:55,502 iteration 6653 : loss : 0.009267, loss_ce: 0.003829
2022-01-10 12:51:57,051 iteration 6654 : loss : 0.011950, loss_ce: 0.003785
2022-01-10 12:51:58,654 iteration 6655 : loss : 0.016069, loss_ce: 0.006424
2022-01-10 12:52:00,249 iteration 6656 : loss : 0.018098, loss_ce: 0.007445
2022-01-10 12:52:01,833 iteration 6657 : loss : 0.010763, loss_ce: 0.003123
2022-01-10 12:52:03,444 iteration 6658 : loss : 0.012972, loss_ce: 0.004517
2022-01-10 12:52:05,020 iteration 6659 : loss : 0.015391, loss_ce: 0.006754
2022-01-10 12:52:06,646 iteration 6660 : loss : 0.028426, loss_ce: 0.011848
2022-01-10 12:52:08,240 iteration 6661 : loss : 0.027544, loss_ce: 0.012235
2022-01-10 12:52:09,844 iteration 6662 : loss : 0.018001, loss_ce: 0.004341
2022-01-10 12:52:11,426 iteration 6663 : loss : 0.014042, loss_ce: 0.004833
2022-01-10 12:52:13,038 iteration 6664 : loss : 0.023640, loss_ce: 0.012105
 98%|████████████████████████████▍| 392/400 [3:10:52<03:50, 28.76s/it]2022-01-10 12:52:14,697 iteration 6665 : loss : 0.014478, loss_ce: 0.004685
2022-01-10 12:52:16,381 iteration 6666 : loss : 0.027793, loss_ce: 0.007086
2022-01-10 12:52:17,985 iteration 6667 : loss : 0.017010, loss_ce: 0.002980
2022-01-10 12:52:19,478 iteration 6668 : loss : 0.010720, loss_ce: 0.005478
2022-01-10 12:52:21,033 iteration 6669 : loss : 0.014065, loss_ce: 0.005499
2022-01-10 12:52:22,590 iteration 6670 : loss : 0.013829, loss_ce: 0.005655
2022-01-10 12:52:24,182 iteration 6671 : loss : 0.015111, loss_ce: 0.005323
2022-01-10 12:52:25,792 iteration 6672 : loss : 0.014947, loss_ce: 0.004407
2022-01-10 12:52:27,358 iteration 6673 : loss : 0.025331, loss_ce: 0.006069
2022-01-10 12:52:28,923 iteration 6674 : loss : 0.014183, loss_ce: 0.006582
2022-01-10 12:52:30,570 iteration 6675 : loss : 0.017693, loss_ce: 0.008546
2022-01-10 12:52:32,112 iteration 6676 : loss : 0.011749, loss_ce: 0.004726
2022-01-10 12:52:33,635 iteration 6677 : loss : 0.011959, loss_ce: 0.004195
2022-01-10 12:52:35,196 iteration 6678 : loss : 0.011248, loss_ce: 0.005362
2022-01-10 12:52:36,812 iteration 6679 : loss : 0.017026, loss_ce: 0.007988
2022-01-10 12:52:38,327 iteration 6680 : loss : 0.008832, loss_ce: 0.003198
2022-01-10 12:52:39,822 iteration 6681 : loss : 0.009594, loss_ce: 0.003203
 98%|████████████████████████████▍| 393/400 [3:11:18<03:17, 28.17s/it]2022-01-10 12:52:41,537 iteration 6682 : loss : 0.014519, loss_ce: 0.007219
2022-01-10 12:52:43,061 iteration 6683 : loss : 0.011831, loss_ce: 0.004505
2022-01-10 12:52:44,627 iteration 6684 : loss : 0.010481, loss_ce: 0.004751
2022-01-10 12:52:46,145 iteration 6685 : loss : 0.015600, loss_ce: 0.005287
2022-01-10 12:52:47,781 iteration 6686 : loss : 0.015629, loss_ce: 0.006462
2022-01-10 12:52:49,337 iteration 6687 : loss : 0.013470, loss_ce: 0.005021
2022-01-10 12:52:50,908 iteration 6688 : loss : 0.012405, loss_ce: 0.004605
2022-01-10 12:52:52,401 iteration 6689 : loss : 0.007637, loss_ce: 0.002024
2022-01-10 12:52:53,961 iteration 6690 : loss : 0.007645, loss_ce: 0.002304
2022-01-10 12:52:55,562 iteration 6691 : loss : 0.016660, loss_ce: 0.005731
2022-01-10 12:52:57,092 iteration 6692 : loss : 0.015146, loss_ce: 0.005494
2022-01-10 12:52:58,643 iteration 6693 : loss : 0.016904, loss_ce: 0.005432
2022-01-10 12:53:00,227 iteration 6694 : loss : 0.013648, loss_ce: 0.004657
2022-01-10 12:53:01,761 iteration 6695 : loss : 0.014374, loss_ce: 0.004457
2022-01-10 12:53:03,326 iteration 6696 : loss : 0.009679, loss_ce: 0.003864
2022-01-10 12:53:04,883 iteration 6697 : loss : 0.012740, loss_ce: 0.004636
2022-01-10 12:53:06,521 iteration 6698 : loss : 0.015314, loss_ce: 0.004894
 98%|████████████████████████████▌| 394/400 [3:11:45<02:46, 27.73s/it]2022-01-10 12:53:08,120 iteration 6699 : loss : 0.014664, loss_ce: 0.004543
2022-01-10 12:53:09,746 iteration 6700 : loss : 0.018914, loss_ce: 0.005365
2022-01-10 12:53:11,302 iteration 6701 : loss : 0.010602, loss_ce: 0.003685
2022-01-10 12:53:12,797 iteration 6702 : loss : 0.009354, loss_ce: 0.004491
2022-01-10 12:53:14,289 iteration 6703 : loss : 0.010588, loss_ce: 0.003715
2022-01-10 12:53:15,887 iteration 6704 : loss : 0.016228, loss_ce: 0.007637
2022-01-10 12:53:17,395 iteration 6705 : loss : 0.013760, loss_ce: 0.006371
2022-01-10 12:53:19,023 iteration 6706 : loss : 0.014435, loss_ce: 0.004663
2022-01-10 12:53:20,527 iteration 6707 : loss : 0.010125, loss_ce: 0.004206
2022-01-10 12:53:22,093 iteration 6708 : loss : 0.017147, loss_ce: 0.006295
2022-01-10 12:53:23,669 iteration 6709 : loss : 0.023662, loss_ce: 0.012097
2022-01-10 12:53:25,270 iteration 6710 : loss : 0.015033, loss_ce: 0.003298
2022-01-10 12:53:26,870 iteration 6711 : loss : 0.011971, loss_ce: 0.003825
2022-01-10 12:53:28,521 iteration 6712 : loss : 0.015619, loss_ce: 0.006597
2022-01-10 12:53:30,032 iteration 6713 : loss : 0.013021, loss_ce: 0.005734
2022-01-10 12:53:31,605 iteration 6714 : loss : 0.014142, loss_ce: 0.005111
2022-01-10 12:53:31,605 Training Data Eval:
2022-01-10 12:53:39,519   Average segmentation loss on training set: 0.0072
2022-01-10 12:53:39,519 Validation Data Eval:
2022-01-10 12:53:42,258   Average segmentation loss on validation set: 0.0639
2022-01-10 12:53:43,772 iteration 6715 : loss : 0.012122, loss_ce: 0.004593
 99%|████████████████████████████▋| 395/400 [3:12:22<02:32, 30.58s/it]2022-01-10 12:53:45,380 iteration 6716 : loss : 0.014834, loss_ce: 0.006957
2022-01-10 12:53:46,917 iteration 6717 : loss : 0.010494, loss_ce: 0.004293
2022-01-10 12:53:48,475 iteration 6718 : loss : 0.013130, loss_ce: 0.005035
2022-01-10 12:53:49,999 iteration 6719 : loss : 0.016682, loss_ce: 0.006138
2022-01-10 12:53:51,533 iteration 6720 : loss : 0.010552, loss_ce: 0.003330
2022-01-10 12:53:53,052 iteration 6721 : loss : 0.010484, loss_ce: 0.003417
2022-01-10 12:53:54,690 iteration 6722 : loss : 0.013496, loss_ce: 0.004940
2022-01-10 12:53:56,163 iteration 6723 : loss : 0.011615, loss_ce: 0.003642
2022-01-10 12:53:57,718 iteration 6724 : loss : 0.013820, loss_ce: 0.004596
2022-01-10 12:53:59,305 iteration 6725 : loss : 0.010694, loss_ce: 0.003459
2022-01-10 12:54:00,910 iteration 6726 : loss : 0.016403, loss_ce: 0.007294
2022-01-10 12:54:02,578 iteration 6727 : loss : 0.024392, loss_ce: 0.010250
2022-01-10 12:54:04,140 iteration 6728 : loss : 0.018826, loss_ce: 0.006291
2022-01-10 12:54:05,698 iteration 6729 : loss : 0.016279, loss_ce: 0.004842
2022-01-10 12:54:07,219 iteration 6730 : loss : 0.009080, loss_ce: 0.003331
2022-01-10 12:54:08,773 iteration 6731 : loss : 0.013658, loss_ce: 0.005130
2022-01-10 12:54:10,374 iteration 6732 : loss : 0.018883, loss_ce: 0.007928
 99%|████████████████████████████▋| 396/400 [3:12:49<01:57, 29.39s/it]2022-01-10 12:54:11,984 iteration 6733 : loss : 0.009994, loss_ce: 0.003601
2022-01-10 12:54:13,648 iteration 6734 : loss : 0.027120, loss_ce: 0.011343
2022-01-10 12:54:15,170 iteration 6735 : loss : 0.009071, loss_ce: 0.003668
2022-01-10 12:54:16,781 iteration 6736 : loss : 0.013407, loss_ce: 0.005419
2022-01-10 12:54:18,390 iteration 6737 : loss : 0.020918, loss_ce: 0.007471
2022-01-10 12:54:19,868 iteration 6738 : loss : 0.010076, loss_ce: 0.002836
2022-01-10 12:54:21,396 iteration 6739 : loss : 0.010385, loss_ce: 0.004130
2022-01-10 12:54:22,990 iteration 6740 : loss : 0.011933, loss_ce: 0.005864
2022-01-10 12:54:24,538 iteration 6741 : loss : 0.009687, loss_ce: 0.003438
2022-01-10 12:54:26,064 iteration 6742 : loss : 0.011608, loss_ce: 0.003729
2022-01-10 12:54:27,654 iteration 6743 : loss : 0.019310, loss_ce: 0.009648
2022-01-10 12:54:29,247 iteration 6744 : loss : 0.017805, loss_ce: 0.007172
2022-01-10 12:54:30,769 iteration 6745 : loss : 0.011561, loss_ce: 0.004609
2022-01-10 12:54:32,342 iteration 6746 : loss : 0.015041, loss_ce: 0.005646
2022-01-10 12:54:33,966 iteration 6747 : loss : 0.025320, loss_ce: 0.005571
2022-01-10 12:54:35,576 iteration 6748 : loss : 0.014042, loss_ce: 0.004574
2022-01-10 12:54:37,155 iteration 6749 : loss : 0.013222, loss_ce: 0.005439
 99%|████████████████████████████▊| 397/400 [3:13:16<01:25, 28.61s/it]2022-01-10 12:54:38,818 iteration 6750 : loss : 0.010141, loss_ce: 0.003728
2022-01-10 12:54:40,490 iteration 6751 : loss : 0.024761, loss_ce: 0.007616
2022-01-10 12:54:42,127 iteration 6752 : loss : 0.018170, loss_ce: 0.006767
2022-01-10 12:54:43,687 iteration 6753 : loss : 0.025937, loss_ce: 0.011532
2022-01-10 12:54:45,329 iteration 6754 : loss : 0.017342, loss_ce: 0.007612
2022-01-10 12:54:46,859 iteration 6755 : loss : 0.014520, loss_ce: 0.004375
2022-01-10 12:54:48,318 iteration 6756 : loss : 0.007935, loss_ce: 0.003671
2022-01-10 12:54:49,815 iteration 6757 : loss : 0.010174, loss_ce: 0.004236
2022-01-10 12:54:51,352 iteration 6758 : loss : 0.010205, loss_ce: 0.005379
2022-01-10 12:54:52,950 iteration 6759 : loss : 0.018042, loss_ce: 0.004777
2022-01-10 12:54:54,485 iteration 6760 : loss : 0.011419, loss_ce: 0.004389
2022-01-10 12:54:56,022 iteration 6761 : loss : 0.014518, loss_ce: 0.006191
2022-01-10 12:54:57,593 iteration 6762 : loss : 0.013962, loss_ce: 0.004330
2022-01-10 12:54:59,151 iteration 6763 : loss : 0.013739, loss_ce: 0.006064
2022-01-10 12:55:00,778 iteration 6764 : loss : 0.015532, loss_ce: 0.005311
2022-01-10 12:55:02,367 iteration 6765 : loss : 0.018738, loss_ce: 0.005240
2022-01-10 12:55:03,917 iteration 6766 : loss : 0.009821, loss_ce: 0.004265
100%|████████████████████████████▊| 398/400 [3:13:42<00:56, 28.05s/it]2022-01-10 12:55:05,600 iteration 6767 : loss : 0.020295, loss_ce: 0.008641
2022-01-10 12:55:07,139 iteration 6768 : loss : 0.012062, loss_ce: 0.003894
2022-01-10 12:55:08,679 iteration 6769 : loss : 0.010008, loss_ce: 0.003288
2022-01-10 12:55:10,253 iteration 6770 : loss : 0.019436, loss_ce: 0.005576
2022-01-10 12:55:11,790 iteration 6771 : loss : 0.013111, loss_ce: 0.006073
2022-01-10 12:55:13,364 iteration 6772 : loss : 0.017607, loss_ce: 0.007158
2022-01-10 12:55:14,859 iteration 6773 : loss : 0.008983, loss_ce: 0.003475
2022-01-10 12:55:16,408 iteration 6774 : loss : 0.018126, loss_ce: 0.005584
2022-01-10 12:55:17,924 iteration 6775 : loss : 0.012303, loss_ce: 0.002679
2022-01-10 12:55:19,474 iteration 6776 : loss : 0.012859, loss_ce: 0.004894
2022-01-10 12:55:21,040 iteration 6777 : loss : 0.013737, loss_ce: 0.005579
2022-01-10 12:55:22,548 iteration 6778 : loss : 0.012096, loss_ce: 0.004468
2022-01-10 12:55:24,087 iteration 6779 : loss : 0.011448, loss_ce: 0.005766
2022-01-10 12:55:25,683 iteration 6780 : loss : 0.011626, loss_ce: 0.004833
2022-01-10 12:55:27,255 iteration 6781 : loss : 0.013543, loss_ce: 0.005473
2022-01-10 12:55:28,788 iteration 6782 : loss : 0.014027, loss_ce: 0.005600
2022-01-10 12:55:30,399 iteration 6783 : loss : 0.017751, loss_ce: 0.005043
100%|████████████████████████████▉| 399/400 [3:14:09<00:27, 27.58s/it]2022-01-10 12:55:31,952 iteration 6784 : loss : 0.012745, loss_ce: 0.006318
2022-01-10 12:55:33,492 iteration 6785 : loss : 0.012623, loss_ce: 0.003724
2022-01-10 12:55:35,084 iteration 6786 : loss : 0.016109, loss_ce: 0.006475
2022-01-10 12:55:36,635 iteration 6787 : loss : 0.025046, loss_ce: 0.008044
2022-01-10 12:55:38,193 iteration 6788 : loss : 0.011376, loss_ce: 0.005074
2022-01-10 12:55:39,822 iteration 6789 : loss : 0.017958, loss_ce: 0.005295
2022-01-10 12:55:41,386 iteration 6790 : loss : 0.015209, loss_ce: 0.006799
2022-01-10 12:55:43,015 iteration 6791 : loss : 0.021454, loss_ce: 0.006821
2022-01-10 12:55:44,561 iteration 6792 : loss : 0.012579, loss_ce: 0.005212
2022-01-10 12:55:46,043 iteration 6793 : loss : 0.011964, loss_ce: 0.005195
2022-01-10 12:55:47,634 iteration 6794 : loss : 0.015825, loss_ce: 0.006163
2022-01-10 12:55:49,145 iteration 6795 : loss : 0.016074, loss_ce: 0.007480
2022-01-10 12:55:50,693 iteration 6796 : loss : 0.012726, loss_ce: 0.003675
2022-01-10 12:55:52,225 iteration 6797 : loss : 0.012291, loss_ce: 0.004484
2022-01-10 12:55:53,827 iteration 6798 : loss : 0.020210, loss_ce: 0.007487
2022-01-10 12:55:55,379 iteration 6799 : loss : 0.013067, loss_ce: 0.004499
2022-01-10 12:55:55,379 Training Data Eval:
2022-01-10 12:56:03,299   Average segmentation loss on training set: 0.0069
2022-01-10 12:56:03,299 Validation Data Eval:
2022-01-10 12:56:06,039   Average segmentation loss on validation set: 0.0685
2022-01-10 12:56:07,590 iteration 6800 : loss : 0.014705, loss_ce: 0.005993
100%|█████████████████████████████| 400/400 [3:14:46<00:00, 30.47s/it]100%|█████████████████████████████| 400/400 [3:14:46<00:00, 29.22s/it]
