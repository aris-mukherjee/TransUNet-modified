2022-01-17 11:22:54,221 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-17 11:22:54,223 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-17 11:22:54,223 ============================================================
2022-01-17 11:22:54,223 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-17 11:22:54,223 ============================================================
2022-01-17 11:22:54,223 Loading data...
2022-01-17 11:22:54,223 Reading NCI - RUNMC images...
2022-01-17 11:22:54,223 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-17 11:22:54,226 Already preprocessed this configuration. Loading now!
2022-01-17 11:22:54,259 Training Images: (256, 256, 286)
2022-01-17 11:22:54,259 Training Labels: (256, 256, 286)
2022-01-17 11:22:54,259 Validation Images: (256, 256, 98)
2022-01-17 11:22:54,259 Validation Labels: (256, 256, 98)
2022-01-17 11:22:54,259 ============================================================
2022-01-17 11:22:54,313 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-17 11:22:57,508 iteration 1 : loss : 1.051954, loss_ce: 1.337828
2022-01-17 11:22:58,368 iteration 2 : loss : 0.997065, loss_ce: 1.229412
2022-01-17 11:22:59,181 iteration 3 : loss : 0.918821, loss_ce: 1.083386
2022-01-17 11:23:00,045 iteration 4 : loss : 0.839618, loss_ce: 0.980326
2022-01-17 11:23:00,834 iteration 5 : loss : 0.826141, loss_ce: 0.932670
2022-01-17 11:23:01,729 iteration 6 : loss : 0.767997, loss_ce: 0.841590
2022-01-17 11:23:02,608 iteration 7 : loss : 0.740071, loss_ce: 0.790496
2022-01-17 11:23:03,482 iteration 8 : loss : 0.702982, loss_ce: 0.735752
2022-01-17 11:23:04,245 iteration 9 : loss : 0.661060, loss_ce: 0.695692
2022-01-17 11:23:05,130 iteration 10 : loss : 0.636958, loss_ce: 0.649105
2022-01-17 11:23:06,017 iteration 11 : loss : 0.618899, loss_ce: 0.609843
2022-01-17 11:23:06,935 iteration 12 : loss : 0.585226, loss_ce: 0.564540
2022-01-17 11:23:07,769 iteration 13 : loss : 0.562476, loss_ce: 0.519135
2022-01-17 11:23:08,690 iteration 14 : loss : 0.544662, loss_ce: 0.492893
2022-01-17 11:23:09,547 iteration 15 : loss : 0.515950, loss_ce: 0.451454
2022-01-17 11:23:10,383 iteration 16 : loss : 0.496697, loss_ce: 0.433654
2022-01-17 11:23:11,196 iteration 17 : loss : 0.498146, loss_ce: 0.394444
  0%|                               | 1/400 [00:16<1:52:44, 16.95s/it]2022-01-17 11:23:12,149 iteration 18 : loss : 0.451779, loss_ce: 0.366951
2022-01-17 11:23:13,026 iteration 19 : loss : 0.462927, loss_ce: 0.335990
2022-01-17 11:23:13,987 iteration 20 : loss : 0.427123, loss_ce: 0.328861
2022-01-17 11:23:14,816 iteration 21 : loss : 0.439056, loss_ce: 0.315709
2022-01-17 11:23:15,599 iteration 22 : loss : 0.414427, loss_ce: 0.281587
2022-01-17 11:23:16,499 iteration 23 : loss : 0.400730, loss_ce: 0.267320
2022-01-17 11:23:17,389 iteration 24 : loss : 0.374755, loss_ce: 0.252597
2022-01-17 11:23:18,314 iteration 25 : loss : 0.383611, loss_ce: 0.272870
2022-01-17 11:23:19,162 iteration 26 : loss : 0.342404, loss_ce: 0.232279
2022-01-17 11:23:19,962 iteration 27 : loss : 0.362008, loss_ce: 0.235428
2022-01-17 11:23:20,751 iteration 28 : loss : 0.349974, loss_ce: 0.223289
2022-01-17 11:23:21,619 iteration 29 : loss : 0.381858, loss_ce: 0.243938
2022-01-17 11:23:22,435 iteration 30 : loss : 0.342847, loss_ce: 0.209359
2022-01-17 11:23:23,234 iteration 31 : loss : 0.303252, loss_ce: 0.168335
2022-01-17 11:23:24,067 iteration 32 : loss : 0.314075, loss_ce: 0.205096
2022-01-17 11:23:24,944 iteration 33 : loss : 0.323289, loss_ce: 0.189706
2022-01-17 11:23:25,885 iteration 34 : loss : 0.259540, loss_ce: 0.145995
  0%|▏                              | 2/400 [00:31<1:43:30, 15.60s/it]2022-01-17 11:23:26,804 iteration 35 : loss : 0.276671, loss_ce: 0.162661
2022-01-17 11:23:27,673 iteration 36 : loss : 0.307303, loss_ce: 0.142904
2022-01-17 11:23:28,550 iteration 37 : loss : 0.271881, loss_ce: 0.135873
2022-01-17 11:23:29,406 iteration 38 : loss : 0.289854, loss_ce: 0.140060
2022-01-17 11:23:30,180 iteration 39 : loss : 0.317751, loss_ce: 0.157458
2022-01-17 11:23:31,002 iteration 40 : loss : 0.325457, loss_ce: 0.165934
2022-01-17 11:23:31,750 iteration 41 : loss : 0.275106, loss_ce: 0.134071
2022-01-17 11:23:32,618 iteration 42 : loss : 0.293227, loss_ce: 0.147866
2022-01-17 11:23:33,472 iteration 43 : loss : 0.234864, loss_ce: 0.109742
2022-01-17 11:23:34,240 iteration 44 : loss : 0.305076, loss_ce: 0.164524
2022-01-17 11:23:35,150 iteration 45 : loss : 0.301269, loss_ce: 0.141442
2022-01-17 11:23:36,007 iteration 46 : loss : 0.267118, loss_ce: 0.129667
2022-01-17 11:23:36,853 iteration 47 : loss : 0.276023, loss_ce: 0.128429
2022-01-17 11:23:37,696 iteration 48 : loss : 0.251864, loss_ce: 0.133236
2022-01-17 11:23:38,588 iteration 49 : loss : 0.314285, loss_ce: 0.161072
2022-01-17 11:23:39,483 iteration 50 : loss : 0.247771, loss_ce: 0.115701
2022-01-17 11:23:40,398 iteration 51 : loss : 0.226010, loss_ce: 0.104219
  1%|▏                              | 3/400 [00:46<1:39:56, 15.10s/it]2022-01-17 11:23:41,232 iteration 52 : loss : 0.254759, loss_ce: 0.110502
2022-01-17 11:23:42,008 iteration 53 : loss : 0.234476, loss_ce: 0.112105
2022-01-17 11:23:42,912 iteration 54 : loss : 0.264862, loss_ce: 0.112282
2022-01-17 11:23:43,787 iteration 55 : loss : 0.337949, loss_ce: 0.131933
2022-01-17 11:23:44,674 iteration 56 : loss : 0.294770, loss_ce: 0.142909
2022-01-17 11:23:45,493 iteration 57 : loss : 0.294841, loss_ce: 0.145933
2022-01-17 11:23:46,294 iteration 58 : loss : 0.268920, loss_ce: 0.132206
2022-01-17 11:23:47,095 iteration 59 : loss : 0.240122, loss_ce: 0.113756
2022-01-17 11:23:47,922 iteration 60 : loss : 0.222119, loss_ce: 0.112908
2022-01-17 11:23:48,757 iteration 61 : loss : 0.285738, loss_ce: 0.146845
2022-01-17 11:23:49,670 iteration 62 : loss : 0.309473, loss_ce: 0.146107
2022-01-17 11:23:50,480 iteration 63 : loss : 0.272602, loss_ce: 0.135879
2022-01-17 11:23:51,347 iteration 64 : loss : 0.261778, loss_ce: 0.126478
2022-01-17 11:23:52,209 iteration 65 : loss : 0.274457, loss_ce: 0.119073
2022-01-17 11:23:53,114 iteration 66 : loss : 0.274445, loss_ce: 0.130515
2022-01-17 11:23:53,948 iteration 67 : loss : 0.239522, loss_ce: 0.115665
2022-01-17 11:23:54,735 iteration 68 : loss : 0.231227, loss_ce: 0.107772
  1%|▎                              | 4/400 [01:00<1:37:40, 14.80s/it]2022-01-17 11:23:55,654 iteration 69 : loss : 0.280791, loss_ce: 0.127155
2022-01-17 11:23:56,481 iteration 70 : loss : 0.254101, loss_ce: 0.099817
2022-01-17 11:23:57,322 iteration 71 : loss : 0.264815, loss_ce: 0.141909
2022-01-17 11:23:58,181 iteration 72 : loss : 0.247825, loss_ce: 0.111472
2022-01-17 11:23:59,066 iteration 73 : loss : 0.242712, loss_ce: 0.095212
2022-01-17 11:23:59,926 iteration 74 : loss : 0.200787, loss_ce: 0.087279
2022-01-17 11:24:00,748 iteration 75 : loss : 0.224238, loss_ce: 0.114291
2022-01-17 11:24:01,518 iteration 76 : loss : 0.238821, loss_ce: 0.111971
2022-01-17 11:24:02,380 iteration 77 : loss : 0.289877, loss_ce: 0.136290
2022-01-17 11:24:03,235 iteration 78 : loss : 0.238555, loss_ce: 0.092254
2022-01-17 11:24:04,099 iteration 79 : loss : 0.274108, loss_ce: 0.119462
2022-01-17 11:24:04,858 iteration 80 : loss : 0.224471, loss_ce: 0.092522
2022-01-17 11:24:05,774 iteration 81 : loss : 0.249188, loss_ce: 0.113254
2022-01-17 11:24:06,600 iteration 82 : loss : 0.283716, loss_ce: 0.096362
2022-01-17 11:24:07,393 iteration 83 : loss : 0.295972, loss_ce: 0.094796
2022-01-17 11:24:08,321 iteration 84 : loss : 0.319253, loss_ce: 0.140987
2022-01-17 11:24:08,322 Training Data Eval:
2022-01-17 11:24:12,247   Average segmentation loss on training set: 0.2490
2022-01-17 11:24:12,247 Validation Data Eval:
2022-01-17 11:24:13,820   Average segmentation loss on validation set: 0.2736
2022-01-17 11:24:17,622 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_SE_NET_best_val_loss_seed100.pth
2022-01-17 11:24:18,443 iteration 85 : loss : 0.298786, loss_ce: 0.145926
  1%|▍                              | 5/400 [01:24<1:58:36, 18.02s/it]2022-01-17 11:24:19,367 iteration 86 : loss : 0.241926, loss_ce: 0.111620
2022-01-17 11:24:20,198 iteration 87 : loss : 0.228824, loss_ce: 0.093419
2022-01-17 11:24:21,000 iteration 88 : loss : 0.182863, loss_ce: 0.095867
2022-01-17 11:24:21,853 iteration 89 : loss : 0.287025, loss_ce: 0.126678
2022-01-17 11:24:22,693 iteration 90 : loss : 0.206335, loss_ce: 0.095475
2022-01-17 11:24:23,586 iteration 91 : loss : 0.212121, loss_ce: 0.103940
2022-01-17 11:24:24,493 iteration 92 : loss : 0.222662, loss_ce: 0.089309
2022-01-17 11:24:25,282 iteration 93 : loss : 0.205145, loss_ce: 0.077944
2022-01-17 11:24:26,159 iteration 94 : loss : 0.188367, loss_ce: 0.087048
2022-01-17 11:24:27,000 iteration 95 : loss : 0.294045, loss_ce: 0.147769
2022-01-17 11:24:27,910 iteration 96 : loss : 0.183689, loss_ce: 0.077217
2022-01-17 11:24:28,768 iteration 97 : loss : 0.271272, loss_ce: 0.111948
2022-01-17 11:24:29,659 iteration 98 : loss : 0.297682, loss_ce: 0.129401
2022-01-17 11:24:30,504 iteration 99 : loss : 0.266603, loss_ce: 0.114746
2022-01-17 11:24:31,382 iteration 100 : loss : 0.205710, loss_ce: 0.096015
2022-01-17 11:24:32,218 iteration 101 : loss : 0.230355, loss_ce: 0.105744
2022-01-17 11:24:33,046 iteration 102 : loss : 0.218509, loss_ce: 0.103251
  2%|▍                              | 6/400 [01:38<1:50:40, 16.85s/it]2022-01-17 11:24:33,953 iteration 103 : loss : 0.251328, loss_ce: 0.096982
2022-01-17 11:24:34,849 iteration 104 : loss : 0.216796, loss_ce: 0.091816
2022-01-17 11:24:35,745 iteration 105 : loss : 0.166078, loss_ce: 0.069021
2022-01-17 11:24:36,716 iteration 106 : loss : 0.214760, loss_ce: 0.095338
2022-01-17 11:24:37,607 iteration 107 : loss : 0.271737, loss_ce: 0.106715
2022-01-17 11:24:38,454 iteration 108 : loss : 0.243492, loss_ce: 0.091286
2022-01-17 11:24:39,240 iteration 109 : loss : 0.277609, loss_ce: 0.123091
2022-01-17 11:24:40,067 iteration 110 : loss : 0.187954, loss_ce: 0.088215
2022-01-17 11:24:40,997 iteration 111 : loss : 0.162187, loss_ce: 0.080931
2022-01-17 11:24:41,880 iteration 112 : loss : 0.211813, loss_ce: 0.079505
2022-01-17 11:24:42,733 iteration 113 : loss : 0.205503, loss_ce: 0.083969
2022-01-17 11:24:43,680 iteration 114 : loss : 0.212113, loss_ce: 0.079272
2022-01-17 11:24:44,580 iteration 115 : loss : 0.221823, loss_ce: 0.105386
2022-01-17 11:24:45,525 iteration 116 : loss : 0.232451, loss_ce: 0.123200
2022-01-17 11:24:46,410 iteration 117 : loss : 0.192326, loss_ce: 0.086624
2022-01-17 11:24:47,302 iteration 118 : loss : 0.272861, loss_ce: 0.138685
2022-01-17 11:24:48,211 iteration 119 : loss : 0.188484, loss_ce: 0.079961
  2%|▌                              | 7/400 [01:53<1:46:46, 16.30s/it]2022-01-17 11:24:49,207 iteration 120 : loss : 0.194399, loss_ce: 0.087889
2022-01-17 11:24:50,130 iteration 121 : loss : 0.279118, loss_ce: 0.115628
2022-01-17 11:24:50,956 iteration 122 : loss : 0.231228, loss_ce: 0.112904
2022-01-17 11:24:51,918 iteration 123 : loss : 0.205091, loss_ce: 0.091295
2022-01-17 11:24:52,873 iteration 124 : loss : 0.253154, loss_ce: 0.099892
2022-01-17 11:24:53,768 iteration 125 : loss : 0.201657, loss_ce: 0.101943
2022-01-17 11:24:54,715 iteration 126 : loss : 0.261625, loss_ce: 0.099639
2022-01-17 11:24:55,666 iteration 127 : loss : 0.174987, loss_ce: 0.075153
2022-01-17 11:24:56,631 iteration 128 : loss : 0.152694, loss_ce: 0.073700
2022-01-17 11:24:57,506 iteration 129 : loss : 0.199418, loss_ce: 0.073339
2022-01-17 11:24:58,476 iteration 130 : loss : 0.223482, loss_ce: 0.104058
2022-01-17 11:25:00,476 iteration 131 : loss : 0.215977, loss_ce: 0.104574
2022-01-17 11:25:01,302 iteration 132 : loss : 0.192711, loss_ce: 0.075187
2022-01-17 11:25:02,223 iteration 133 : loss : 0.193293, loss_ce: 0.072024
2022-01-17 11:25:03,097 iteration 134 : loss : 0.232136, loss_ce: 0.103742
2022-01-17 11:25:03,978 iteration 135 : loss : 0.194711, loss_ce: 0.086033
2022-01-17 11:25:04,829 iteration 136 : loss : 0.250610, loss_ce: 0.113790
  2%|▌                              | 8/400 [02:10<1:47:09, 16.40s/it]2022-01-17 11:25:05,670 iteration 137 : loss : 0.140802, loss_ce: 0.052340
2022-01-17 11:25:06,496 iteration 138 : loss : 0.185401, loss_ce: 0.096865
2022-01-17 11:25:07,438 iteration 139 : loss : 0.192343, loss_ce: 0.076082
2022-01-17 11:25:08,431 iteration 140 : loss : 0.224412, loss_ce: 0.088158
2022-01-17 11:25:09,333 iteration 141 : loss : 0.233131, loss_ce: 0.100129
2022-01-17 11:25:10,174 iteration 142 : loss : 0.171085, loss_ce: 0.071071
2022-01-17 11:25:11,023 iteration 143 : loss : 0.200475, loss_ce: 0.084633
2022-01-17 11:25:11,971 iteration 144 : loss : 0.224733, loss_ce: 0.086887
2022-01-17 11:25:13,039 iteration 145 : loss : 0.175237, loss_ce: 0.077390
2022-01-17 11:25:13,915 iteration 146 : loss : 0.195009, loss_ce: 0.080293
2022-01-17 11:25:14,801 iteration 147 : loss : 0.229705, loss_ce: 0.088265
2022-01-17 11:25:15,703 iteration 148 : loss : 0.185207, loss_ce: 0.092814
2022-01-17 11:25:16,617 iteration 149 : loss : 0.183791, loss_ce: 0.074727
2022-01-17 11:25:17,513 iteration 150 : loss : 0.259957, loss_ce: 0.138118
2022-01-17 11:25:18,410 iteration 151 : loss : 0.183032, loss_ce: 0.080804
2022-01-17 11:25:19,370 iteration 152 : loss : 0.191121, loss_ce: 0.082056
2022-01-17 11:25:20,334 iteration 153 : loss : 0.175486, loss_ce: 0.078537
  2%|▋                              | 9/400 [02:26<1:45:03, 16.12s/it]2022-01-17 11:25:21,272 iteration 154 : loss : 0.173249, loss_ce: 0.073882
2022-01-17 11:25:22,196 iteration 155 : loss : 0.241402, loss_ce: 0.113475
2022-01-17 11:25:23,110 iteration 156 : loss : 0.161960, loss_ce: 0.069720
2022-01-17 11:25:24,035 iteration 157 : loss : 0.210275, loss_ce: 0.077449
2022-01-17 11:25:24,949 iteration 158 : loss : 0.221502, loss_ce: 0.107673
2022-01-17 11:25:25,847 iteration 159 : loss : 0.212019, loss_ce: 0.091454
2022-01-17 11:25:26,735 iteration 160 : loss : 0.269634, loss_ce: 0.097937
2022-01-17 11:25:27,659 iteration 161 : loss : 0.160382, loss_ce: 0.067335
2022-01-17 11:25:28,641 iteration 162 : loss : 0.193293, loss_ce: 0.085547
2022-01-17 11:25:29,449 iteration 163 : loss : 0.124341, loss_ce: 0.053762
2022-01-17 11:25:30,401 iteration 164 : loss : 0.152951, loss_ce: 0.062380
2022-01-17 11:25:31,286 iteration 165 : loss : 0.157314, loss_ce: 0.058856
2022-01-17 11:25:32,254 iteration 166 : loss : 0.203651, loss_ce: 0.079475
2022-01-17 11:25:33,203 iteration 167 : loss : 0.137182, loss_ce: 0.061225
2022-01-17 11:25:34,151 iteration 168 : loss : 0.185115, loss_ce: 0.087331
2022-01-17 11:25:35,050 iteration 169 : loss : 0.187991, loss_ce: 0.085222
2022-01-17 11:25:35,050 Training Data Eval:
2022-01-17 11:25:39,290   Average segmentation loss on training set: 0.1431
2022-01-17 11:25:39,290 Validation Data Eval:
2022-01-17 11:25:40,962   Average segmentation loss on validation set: 0.1854
2022-01-17 11:25:44,649 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_SE_NET_best_val_loss_seed100.pth
2022-01-17 11:25:45,496 iteration 170 : loss : 0.177121, loss_ce: 0.078325
  2%|▊                             | 10/400 [02:51<2:02:55, 18.91s/it]2022-01-17 11:25:46,423 iteration 171 : loss : 0.174797, loss_ce: 0.081427
2022-01-17 11:25:47,186 iteration 172 : loss : 0.246457, loss_ce: 0.097213
2022-01-17 11:25:47,985 iteration 173 : loss : 0.136904, loss_ce: 0.060147
2022-01-17 11:25:48,914 iteration 174 : loss : 0.198997, loss_ce: 0.088390
2022-01-17 11:25:49,755 iteration 175 : loss : 0.220491, loss_ce: 0.102311
2022-01-17 11:25:50,720 iteration 176 : loss : 0.176343, loss_ce: 0.065095
2022-01-17 11:25:52,646 iteration 177 : loss : 0.238385, loss_ce: 0.085136
2022-01-17 11:25:53,589 iteration 178 : loss : 0.185543, loss_ce: 0.063934
2022-01-17 11:25:54,502 iteration 179 : loss : 0.153226, loss_ce: 0.061113
2022-01-17 11:25:55,415 iteration 180 : loss : 0.181176, loss_ce: 0.063014
2022-01-17 11:25:56,338 iteration 181 : loss : 0.136667, loss_ce: 0.052565
2022-01-17 11:25:57,245 iteration 182 : loss : 0.153369, loss_ce: 0.060462
2022-01-17 11:25:58,176 iteration 183 : loss : 0.146168, loss_ce: 0.061409
2022-01-17 11:25:59,061 iteration 184 : loss : 0.160746, loss_ce: 0.070023
2022-01-17 11:25:59,903 iteration 185 : loss : 0.179123, loss_ce: 0.082918
