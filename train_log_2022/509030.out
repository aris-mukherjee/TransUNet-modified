2022-01-20 15:40:11,468 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-20 15:40:11,469 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-20 15:40:11,469 ============================================================
2022-01-20 15:40:11,469 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-20 15:40:11,469 ============================================================
2022-01-20 15:40:11,469 Loading data...
2022-01-20 15:40:11,469 Reading NCI - RUNMC images...
2022-01-20 15:40:11,469 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-20 15:40:11,472 Already preprocessed this configuration. Loading now!
2022-01-20 15:40:11,494 Training Images: (256, 256, 286)
2022-01-20 15:40:11,494 Training Labels: (256, 256, 286)
2022-01-20 15:40:11,494 Validation Images: (256, 256, 98)
2022-01-20 15:40:11,494 Validation Labels: (256, 256, 98)
2022-01-20 15:40:11,494 ============================================================
2022-01-20 15:40:11,517 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-20 15:40:15,104 iteration 1 : loss : 0.765507, loss_ce: 0.862203
2022-01-20 15:40:16,333 iteration 2 : loss : 0.724417, loss_ce: 0.775930
2022-01-20 15:40:17,531 iteration 3 : loss : 0.686810, loss_ce: 0.694277
2022-01-20 15:40:18,807 iteration 4 : loss : 0.633328, loss_ce: 0.645075
2022-01-20 15:40:19,984 iteration 5 : loss : 0.606758, loss_ce: 0.568409
2022-01-20 15:40:21,287 iteration 6 : loss : 0.562914, loss_ce: 0.510330
2022-01-20 15:40:22,562 iteration 7 : loss : 0.544551, loss_ce: 0.467368
2022-01-20 15:40:23,837 iteration 8 : loss : 0.506057, loss_ce: 0.417697
2022-01-20 15:40:24,979 iteration 9 : loss : 0.470426, loss_ce: 0.398128
2022-01-20 15:40:26,268 iteration 10 : loss : 0.456269, loss_ce: 0.361855
2022-01-20 15:40:27,555 iteration 11 : loss : 0.437914, loss_ce: 0.325216
2022-01-20 15:40:28,890 iteration 12 : loss : 0.419916, loss_ce: 0.305483
2022-01-20 15:40:30,115 iteration 13 : loss : 0.399958, loss_ce: 0.268624
2022-01-20 15:40:31,444 iteration 14 : loss : 0.397992, loss_ce: 0.257848
2022-01-20 15:40:32,697 iteration 15 : loss : 0.370580, loss_ce: 0.228627
2022-01-20 15:40:33,923 iteration 16 : loss : 0.376727, loss_ce: 0.231729
2022-01-20 15:40:35,118 iteration 17 : loss : 0.370463, loss_ce: 0.200598
  0%|                               | 1/400 [00:23<2:37:33, 23.69s/it]2022-01-20 15:40:36,487 iteration 18 : loss : 0.363516, loss_ce: 0.214544
2022-01-20 15:40:37,770 iteration 19 : loss : 0.344116, loss_ce: 0.170243
2022-01-20 15:40:39,148 iteration 20 : loss : 0.333605, loss_ce: 0.175646
2022-01-20 15:40:40,376 iteration 21 : loss : 0.364505, loss_ce: 0.171584
2022-01-20 15:40:41,540 iteration 22 : loss : 0.320336, loss_ce: 0.154379
2022-01-20 15:40:42,846 iteration 23 : loss : 0.322433, loss_ce: 0.157462
2022-01-20 15:40:44,156 iteration 24 : loss : 0.290029, loss_ce: 0.152319
2022-01-20 15:40:45,491 iteration 25 : loss : 0.323366, loss_ce: 0.188919
2022-01-20 15:40:46,739 iteration 26 : loss : 0.316382, loss_ce: 0.166951
2022-01-20 15:40:47,929 iteration 27 : loss : 0.312681, loss_ce: 0.163428
2022-01-20 15:40:49,100 iteration 28 : loss : 0.244769, loss_ce: 0.119235
2022-01-20 15:40:50,389 iteration 29 : loss : 0.319065, loss_ce: 0.156730
2022-01-20 15:40:51,626 iteration 30 : loss : 0.271064, loss_ce: 0.129016
2022-01-20 15:40:52,855 iteration 31 : loss : 0.242056, loss_ce: 0.099365
2022-01-20 15:40:54,132 iteration 32 : loss : 0.281177, loss_ce: 0.149297
2022-01-20 15:40:55,469 iteration 33 : loss : 0.286876, loss_ce: 0.136888
2022-01-20 15:40:56,872 iteration 34 : loss : 0.220150, loss_ce: 0.098801
  0%|▏                              | 2/400 [00:45<2:29:29, 22.54s/it]2022-01-20 15:40:58,257 iteration 35 : loss : 0.230790, loss_ce: 0.111047
2022-01-20 15:40:59,562 iteration 36 : loss : 0.275086, loss_ce: 0.101493
2022-01-20 15:41:00,887 iteration 37 : loss : 0.241728, loss_ce: 0.100962
2022-01-20 15:41:02,179 iteration 38 : loss : 0.265456, loss_ce: 0.104689
2022-01-20 15:41:03,371 iteration 39 : loss : 0.277932, loss_ce: 0.124247
2022-01-20 15:41:04,655 iteration 40 : loss : 0.313580, loss_ce: 0.144833
2022-01-20 15:41:05,853 iteration 41 : loss : 0.199000, loss_ce: 0.090237
2022-01-20 15:41:07,185 iteration 42 : loss : 0.308322, loss_ce: 0.159262
2022-01-20 15:41:08,481 iteration 43 : loss : 0.240811, loss_ce: 0.117018
2022-01-20 15:41:09,684 iteration 44 : loss : 0.261865, loss_ce: 0.145994
2022-01-20 15:41:11,065 iteration 45 : loss : 0.269702, loss_ce: 0.116816
2022-01-20 15:41:12,366 iteration 46 : loss : 0.241829, loss_ce: 0.109469
2022-01-20 15:41:13,675 iteration 47 : loss : 0.253295, loss_ce: 0.095519
2022-01-20 15:41:14,983 iteration 48 : loss : 0.234202, loss_ce: 0.104033
2022-01-20 15:41:16,338 iteration 49 : loss : 0.308718, loss_ce: 0.137566
2022-01-20 15:41:17,691 iteration 50 : loss : 0.243508, loss_ce: 0.098391
2022-01-20 15:41:19,054 iteration 51 : loss : 0.218882, loss_ce: 0.091181
  1%|▏                              | 3/400 [01:07<2:28:02, 22.38s/it]2022-01-20 15:41:20,335 iteration 52 : loss : 0.277050, loss_ce: 0.107536
2022-01-20 15:41:21,540 iteration 53 : loss : 0.227244, loss_ce: 0.103833
2022-01-20 15:41:22,906 iteration 54 : loss : 0.199240, loss_ce: 0.083206
2022-01-20 15:41:24,234 iteration 55 : loss : 0.242880, loss_ce: 0.089672
2022-01-20 15:41:25,573 iteration 56 : loss : 0.408361, loss_ce: 0.216578
2022-01-20 15:41:26,833 iteration 57 : loss : 0.328530, loss_ce: 0.159536
2022-01-20 15:41:28,078 iteration 58 : loss : 0.369330, loss_ce: 0.178699
2022-01-20 15:41:29,314 iteration 59 : loss : 0.314120, loss_ce: 0.134817
2022-01-20 15:41:30,580 iteration 60 : loss : 0.316472, loss_ce: 0.146929
2022-01-20 15:41:31,856 iteration 61 : loss : 0.241320, loss_ce: 0.105851
2022-01-20 15:41:33,224 iteration 62 : loss : 0.285579, loss_ce: 0.109334
2022-01-20 15:41:34,472 iteration 63 : loss : 0.269925, loss_ce: 0.127762
2022-01-20 15:41:35,794 iteration 64 : loss : 0.287370, loss_ce: 0.120564
2022-01-20 15:41:37,111 iteration 65 : loss : 0.258293, loss_ce: 0.103559
2022-01-20 15:41:38,470 iteration 66 : loss : 0.275327, loss_ce: 0.118346
2022-01-20 15:41:39,745 iteration 67 : loss : 0.252439, loss_ce: 0.116257
2022-01-20 15:41:40,960 iteration 68 : loss : 0.240174, loss_ce: 0.104369
  1%|▎                              | 4/400 [01:29<2:26:26, 22.19s/it]2022-01-20 15:41:42,361 iteration 69 : loss : 0.277055, loss_ce: 0.121304
2022-01-20 15:41:43,634 iteration 70 : loss : 0.315492, loss_ce: 0.121653
2022-01-20 15:41:44,931 iteration 71 : loss : 0.288202, loss_ce: 0.152307
2022-01-20 15:41:46,251 iteration 72 : loss : 0.246589, loss_ce: 0.105332
2022-01-20 15:41:47,596 iteration 73 : loss : 0.227442, loss_ce: 0.084063
2022-01-20 15:41:48,926 iteration 74 : loss : 0.218025, loss_ce: 0.089844
2022-01-20 15:41:50,202 iteration 75 : loss : 0.223726, loss_ce: 0.108371
2022-01-20 15:41:51,411 iteration 76 : loss : 0.246972, loss_ce: 0.117719
2022-01-20 15:41:52,737 iteration 77 : loss : 0.273727, loss_ce: 0.123645
2022-01-20 15:41:54,074 iteration 78 : loss : 0.234824, loss_ce: 0.095431
2022-01-20 15:41:55,400 iteration 79 : loss : 0.264941, loss_ce: 0.102849
2022-01-20 15:41:56,598 iteration 80 : loss : 0.225630, loss_ce: 0.084570
2022-01-20 15:41:57,977 iteration 81 : loss : 0.311884, loss_ce: 0.141538
2022-01-20 15:41:59,242 iteration 82 : loss : 0.296307, loss_ce: 0.099069
2022-01-20 15:42:00,475 iteration 83 : loss : 0.306345, loss_ce: 0.097416
2022-01-20 15:42:01,882 iteration 84 : loss : 0.304022, loss_ce: 0.141547
2022-01-20 15:42:01,883 Training Data Eval:
2022-01-20 15:42:08,419   Average segmentation loss on training set: 0.3338
2022-01-20 15:42:08,419 Validation Data Eval:
2022-01-20 15:42:10,866   Average segmentation loss on validation set: 0.3874
2022-01-20 15:42:16,006 Found new lowest validation loss at iteration 84! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_NO_ATTN_best_val_loss_seed100.pth
2022-01-20 15:42:17,210 iteration 85 : loss : 0.294167, loss_ce: 0.144578
  1%|▍                              | 5/400 [02:05<2:59:29, 27.26s/it]2022-01-20 15:42:18,557 iteration 86 : loss : 0.230577, loss_ce: 0.110595
2022-01-20 15:42:19,776 iteration 87 : loss : 0.263210, loss_ce: 0.111167
2022-01-20 15:42:20,951 iteration 88 : loss : 0.236213, loss_ce: 0.119571
2022-01-20 15:42:22,210 iteration 89 : loss : 0.278693, loss_ce: 0.105235
2022-01-20 15:42:23,444 iteration 90 : loss : 0.223034, loss_ce: 0.095740
2022-01-20 15:42:24,742 iteration 91 : loss : 0.238844, loss_ce: 0.108505
2022-01-20 15:42:26,055 iteration 92 : loss : 0.236392, loss_ce: 0.087721
2022-01-20 15:42:27,264 iteration 93 : loss : 0.224250, loss_ce: 0.078846
2022-01-20 15:42:28,598 iteration 94 : loss : 0.218048, loss_ce: 0.095484
2022-01-20 15:42:29,892 iteration 95 : loss : 0.256504, loss_ce: 0.118400
2022-01-20 15:42:31,267 iteration 96 : loss : 0.237548, loss_ce: 0.099202
2022-01-20 15:42:32,564 iteration 97 : loss : 0.233868, loss_ce: 0.089236
2022-01-20 15:42:33,894 iteration 98 : loss : 0.275629, loss_ce: 0.110309
2022-01-20 15:42:35,183 iteration 99 : loss : 0.312715, loss_ce: 0.122188
2022-01-20 15:42:36,522 iteration 100 : loss : 0.240204, loss_ce: 0.101649
2022-01-20 15:42:37,806 iteration 101 : loss : 0.219205, loss_ce: 0.094294
2022-01-20 15:42:39,078 iteration 102 : loss : 0.239986, loss_ce: 0.113382
  2%|▍                              | 6/400 [02:27<2:46:57, 25.42s/it]2022-01-20 15:42:40,471 iteration 103 : loss : 0.257642, loss_ce: 0.098724
2022-01-20 15:42:41,830 iteration 104 : loss : 0.250413, loss_ce: 0.106527
2022-01-20 15:42:43,225 iteration 105 : loss : 0.167347, loss_ce: 0.065572
2022-01-20 15:42:44,644 iteration 106 : loss : 0.233265, loss_ce: 0.095142
2022-01-20 15:42:45,975 iteration 107 : loss : 0.251270, loss_ce: 0.090739
2022-01-20 15:42:47,268 iteration 108 : loss : 0.231688, loss_ce: 0.078529
2022-01-20 15:42:48,498 iteration 109 : loss : 0.270012, loss_ce: 0.105393
2022-01-20 15:42:49,787 iteration 110 : loss : 0.215533, loss_ce: 0.090016
2022-01-20 15:42:51,159 iteration 111 : loss : 0.194251, loss_ce: 0.091093
2022-01-20 15:42:52,448 iteration 112 : loss : 0.238325, loss_ce: 0.095934
2022-01-20 15:42:53,675 iteration 113 : loss : 0.227218, loss_ce: 0.089408
2022-01-20 15:42:55,024 iteration 114 : loss : 0.223671, loss_ce: 0.086040
2022-01-20 15:42:56,303 iteration 115 : loss : 0.181954, loss_ce: 0.094343
2022-01-20 15:42:57,642 iteration 116 : loss : 0.246019, loss_ce: 0.122494
2022-01-20 15:42:58,906 iteration 117 : loss : 0.231864, loss_ce: 0.097239
2022-01-20 15:43:00,199 iteration 118 : loss : 0.262272, loss_ce: 0.117182
2022-01-20 15:43:01,513 iteration 119 : loss : 0.180627, loss_ce: 0.064750
  2%|▌                              | 7/400 [02:50<2:40:08, 24.45s/it]2022-01-20 15:43:02,940 iteration 120 : loss : 0.194068, loss_ce: 0.081876
2022-01-20 15:43:04,273 iteration 121 : loss : 0.268924, loss_ce: 0.107307
2022-01-20 15:43:05,491 iteration 122 : loss : 0.226971, loss_ce: 0.101509
2022-01-20 15:43:06,849 iteration 123 : loss : 0.186249, loss_ce: 0.081483
2022-01-20 15:43:08,196 iteration 124 : loss : 0.222241, loss_ce: 0.083880
2022-01-20 15:43:09,492 iteration 125 : loss : 0.228651, loss_ce: 0.099465
2022-01-20 15:43:10,844 iteration 126 : loss : 0.264217, loss_ce: 0.101778
2022-01-20 15:43:12,212 iteration 127 : loss : 0.178828, loss_ce: 0.069140
2022-01-20 15:43:13,580 iteration 128 : loss : 0.157578, loss_ce: 0.069582
2022-01-20 15:43:14,846 iteration 129 : loss : 0.224178, loss_ce: 0.091605
2022-01-20 15:43:16,223 iteration 130 : loss : 0.239488, loss_ce: 0.116861
2022-01-20 15:43:17,531 iteration 131 : loss : 0.241936, loss_ce: 0.119784
2022-01-20 15:43:18,791 iteration 132 : loss : 0.226625, loss_ce: 0.087739
2022-01-20 15:43:20,173 iteration 133 : loss : 0.219405, loss_ce: 0.070371
2022-01-20 15:43:21,487 iteration 134 : loss : 0.265784, loss_ce: 0.103748
2022-01-20 15:43:22,811 iteration 135 : loss : 0.224694, loss_ce: 0.094880
2022-01-20 15:43:24,089 iteration 136 : loss : 0.216436, loss_ce: 0.098337
  2%|▌                              | 8/400 [03:12<2:35:49, 23.85s/it]2022-01-20 15:43:25,370 iteration 137 : loss : 0.158589, loss_ce: 0.053778
2022-01-20 15:43:26,625 iteration 138 : loss : 0.200671, loss_ce: 0.103207
2022-01-20 15:43:27,999 iteration 139 : loss : 0.193251, loss_ce: 0.071699
2022-01-20 15:43:29,420 iteration 140 : loss : 0.245634, loss_ce: 0.095530
2022-01-20 15:43:30,730 iteration 141 : loss : 0.202834, loss_ce: 0.079025
2022-01-20 15:43:31,973 iteration 142 : loss : 0.160074, loss_ce: 0.060380
2022-01-20 15:43:33,215 iteration 143 : loss : 0.219141, loss_ce: 0.089238
2022-01-20 15:43:34,563 iteration 144 : loss : 0.204730, loss_ce: 0.074150
2022-01-20 15:43:36,062 iteration 145 : loss : 0.161836, loss_ce: 0.069248
2022-01-20 15:43:37,325 iteration 146 : loss : 0.177640, loss_ce: 0.065429
2022-01-20 15:43:38,610 iteration 147 : loss : 0.206690, loss_ce: 0.085399
2022-01-20 15:43:39,928 iteration 148 : loss : 0.172902, loss_ce: 0.077653
2022-01-20 15:43:41,260 iteration 149 : loss : 0.191781, loss_ce: 0.073553
2022-01-20 15:43:42,558 iteration 150 : loss : 0.241312, loss_ce: 0.129851
2022-01-20 15:43:43,870 iteration 151 : loss : 0.161966, loss_ce: 0.071217
2022-01-20 15:43:45,243 iteration 152 : loss : 0.147296, loss_ce: 0.063247
2022-01-20 15:43:46,612 iteration 153 : loss : 0.159835, loss_ce: 0.068304
  2%|▋                              | 9/400 [03:35<2:32:43, 23.44s/it]2022-01-20 15:43:47,968 iteration 154 : loss : 0.163945, loss_ce: 0.061273
2022-01-20 15:43:49,290 iteration 155 : loss : 0.191274, loss_ce: 0.071446
2022-01-20 15:43:50,601 iteration 156 : loss : 0.147385, loss_ce: 0.056366
2022-01-20 15:43:51,931 iteration 157 : loss : 0.234593, loss_ce: 0.085156
2022-01-20 15:43:53,252 iteration 158 : loss : 0.165419, loss_ce: 0.075574
2022-01-20 15:43:54,574 iteration 159 : loss : 0.217004, loss_ce: 0.084166
2022-01-20 15:43:56,701 iteration 160 : loss : 0.261736, loss_ce: 0.096554
2022-01-20 15:43:57,993 iteration 161 : loss : 0.144670, loss_ce: 0.061454
2022-01-20 15:43:59,360 iteration 162 : loss : 0.156776, loss_ce: 0.064938
2022-01-20 15:44:00,553 iteration 163 : loss : 0.128712, loss_ce: 0.053600
2022-01-20 15:44:01,929 iteration 164 : loss : 0.154575, loss_ce: 0.056777
2022-01-20 15:44:03,216 iteration 165 : loss : 0.148653, loss_ce: 0.053314
2022-01-20 15:44:04,606 iteration 166 : loss : 0.192927, loss_ce: 0.071174
2022-01-20 15:44:05,977 iteration 167 : loss : 0.156616, loss_ce: 0.071326
2022-01-20 15:44:07,339 iteration 168 : loss : 0.153879, loss_ce: 0.064321
2022-01-20 15:44:08,644 iteration 169 : loss : 0.161118, loss_ce: 0.067882
2022-01-20 15:44:08,645 Training Data Eval:
