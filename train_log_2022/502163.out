2022-01-09 23:03:24,868 Logging directory: /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/TransUNet/log_dir/trRUNMC_cv1_r3/i2i2l/
2022-01-09 23:03:24,870 Tensorboard directory: /scratch_net/biwidl217_second/arismu/Tensorboard/trRUNMC_cv1_r3/i2i2l/
2022-01-09 23:03:24,870 ============================================================
2022-01-09 23:03:24,870 EXPERIMENT NAME: trRUNMC_cv1_r3/i2i2l/
2022-01-09 23:03:24,870 ============================================================
2022-01-09 23:03:24,870 Loading data...
2022-01-09 23:03:24,870 Reading NCI - RUNMC images...
2022-01-09 23:03:24,870 Data root directory: /itet-stor/arismu/bmicdatasets-originals/Originals/Challenge_Datasets/NCI_Prostate/
2022-01-09 23:03:24,874 Already preprocessed this configuration. Loading now!
2022-01-09 23:03:24,904 Training Images: (256, 256, 286)
2022-01-09 23:03:24,904 Training Labels: (256, 256, 286)
2022-01-09 23:03:24,904 Validation Images: (256, 256, 98)
2022-01-09 23:03:24,904 Validation Labels: (256, 256, 98)
2022-01-09 23:03:24,904 ============================================================
2022-01-09 23:03:24,956 17 iterations per epoch. 6800 max iterations 
  0%|                                         | 0/400 [00:00<?, ?it/s]2022-01-09 23:03:27,778 iteration 1 : loss : 0.984149, loss_ce: 1.214196
2022-01-09 23:03:29,161 iteration 2 : loss : 0.919466, loss_ce: 1.112493
2022-01-09 23:03:30,649 iteration 3 : loss : 0.862562, loss_ce: 1.013615
2022-01-09 23:03:32,050 iteration 4 : loss : 0.822324, loss_ce: 0.942452
2022-01-09 23:03:33,451 iteration 5 : loss : 0.764115, loss_ce: 0.856173
2022-01-09 23:03:34,869 iteration 6 : loss : 0.734495, loss_ce: 0.795345
2022-01-09 23:03:36,343 iteration 7 : loss : 0.683800, loss_ce: 0.732069
2022-01-09 23:03:37,774 iteration 8 : loss : 0.670485, loss_ce: 0.675900
2022-01-09 23:03:39,227 iteration 9 : loss : 0.605873, loss_ce: 0.635770
2022-01-09 23:03:40,699 iteration 10 : loss : 0.604665, loss_ce: 0.572151
2022-01-09 23:03:42,230 iteration 11 : loss : 0.567373, loss_ce: 0.542502
2022-01-09 23:03:43,647 iteration 12 : loss : 0.534124, loss_ce: 0.486495
2022-01-09 23:03:45,044 iteration 13 : loss : 0.525853, loss_ce: 0.456836
2022-01-09 23:03:46,424 iteration 14 : loss : 0.494461, loss_ce: 0.422202
2022-01-09 23:03:47,898 iteration 15 : loss : 0.456879, loss_ce: 0.377825
2022-01-09 23:03:49,398 iteration 16 : loss : 0.470442, loss_ce: 0.365099
2022-01-09 23:03:50,907 iteration 17 : loss : 0.413168, loss_ce: 0.327692
  0%|                               | 1/400 [00:26<2:53:11, 26.04s/it]2022-01-09 23:03:52,532 iteration 18 : loss : 0.432356, loss_ce: 0.293320
2022-01-09 23:03:53,984 iteration 19 : loss : 0.370388, loss_ce: 0.259198
2022-01-09 23:03:55,566 iteration 20 : loss : 0.362091, loss_ce: 0.242008
2022-01-09 23:03:57,077 iteration 21 : loss : 0.387063, loss_ce: 0.228010
2022-01-09 23:03:58,634 iteration 22 : loss : 0.337753, loss_ce: 0.220024
2022-01-09 23:04:00,259 iteration 23 : loss : 0.332372, loss_ce: 0.194968
2022-01-09 23:04:01,819 iteration 24 : loss : 0.314838, loss_ce: 0.185749
2022-01-09 23:04:03,437 iteration 25 : loss : 0.399799, loss_ce: 0.245824
2022-01-09 23:04:04,986 iteration 26 : loss : 0.315543, loss_ce: 0.175291
2022-01-09 23:04:06,482 iteration 27 : loss : 0.297314, loss_ce: 0.171177
2022-01-09 23:04:07,990 iteration 28 : loss : 0.299408, loss_ce: 0.161029
2022-01-09 23:04:09,596 iteration 29 : loss : 0.294719, loss_ce: 0.152869
2022-01-09 23:04:11,189 iteration 30 : loss : 0.292088, loss_ce: 0.153998
2022-01-09 23:04:12,700 iteration 31 : loss : 0.264493, loss_ce: 0.139475
2022-01-09 23:04:14,319 iteration 32 : loss : 0.286665, loss_ce: 0.158940
2022-01-09 23:04:15,930 iteration 33 : loss : 0.277162, loss_ce: 0.151554
2022-01-09 23:04:17,535 iteration 34 : loss : 0.286216, loss_ce: 0.163161
  0%|▏                              | 2/400 [00:52<2:54:57, 26.38s/it]2022-01-09 23:04:19,211 iteration 35 : loss : 0.259816, loss_ce: 0.127598
2022-01-09 23:04:20,834 iteration 36 : loss : 0.271793, loss_ce: 0.142229
2022-01-09 23:04:22,465 iteration 37 : loss : 0.277536, loss_ce: 0.122481
2022-01-09 23:04:24,010 iteration 38 : loss : 0.250008, loss_ce: 0.122908
2022-01-09 23:04:25,565 iteration 39 : loss : 0.225510, loss_ce: 0.108001
2022-01-09 23:04:27,195 iteration 40 : loss : 0.264745, loss_ce: 0.131736
2022-01-09 23:04:28,812 iteration 41 : loss : 0.334821, loss_ce: 0.166001
2022-01-09 23:04:30,407 iteration 42 : loss : 0.242447, loss_ce: 0.122090
2022-01-09 23:04:31,913 iteration 43 : loss : 0.249548, loss_ce: 0.115029
2022-01-09 23:04:33,555 iteration 44 : loss : 0.241678, loss_ce: 0.113585
2022-01-09 23:04:35,129 iteration 45 : loss : 0.217338, loss_ce: 0.094789
2022-01-09 23:04:36,717 iteration 46 : loss : 0.252053, loss_ce: 0.104866
2022-01-09 23:04:38,339 iteration 47 : loss : 0.222116, loss_ce: 0.093280
2022-01-09 23:04:39,936 iteration 48 : loss : 0.226478, loss_ce: 0.098800
2022-01-09 23:04:41,572 iteration 49 : loss : 0.296271, loss_ce: 0.141673
2022-01-09 23:04:43,115 iteration 50 : loss : 0.324747, loss_ce: 0.150787
2022-01-09 23:04:44,636 iteration 51 : loss : 0.297531, loss_ce: 0.146549
  1%|▏                              | 3/400 [01:19<2:56:39, 26.70s/it]2022-01-09 23:04:46,317 iteration 52 : loss : 0.277763, loss_ce: 0.139017
2022-01-09 23:04:47,927 iteration 53 : loss : 0.267505, loss_ce: 0.123628
2022-01-09 23:04:49,510 iteration 54 : loss : 0.243386, loss_ce: 0.103291
2022-01-09 23:04:51,105 iteration 55 : loss : 0.282111, loss_ce: 0.138246
2022-01-09 23:04:52,681 iteration 56 : loss : 0.250250, loss_ce: 0.104192
2022-01-09 23:04:54,279 iteration 57 : loss : 0.233230, loss_ce: 0.096411
2022-01-09 23:04:55,878 iteration 58 : loss : 0.265369, loss_ce: 0.102896
2022-01-09 23:04:57,442 iteration 59 : loss : 0.225098, loss_ce: 0.097262
2022-01-09 23:04:59,044 iteration 60 : loss : 0.282143, loss_ce: 0.123406
2022-01-09 23:05:00,631 iteration 61 : loss : 0.260960, loss_ce: 0.124886
2022-01-09 23:05:02,207 iteration 62 : loss : 0.351618, loss_ce: 0.146779
2022-01-09 23:05:03,711 iteration 63 : loss : 0.288409, loss_ce: 0.145078
2022-01-09 23:05:05,271 iteration 64 : loss : 0.316927, loss_ce: 0.146676
2022-01-09 23:05:06,786 iteration 65 : loss : 0.252765, loss_ce: 0.096648
2022-01-09 23:05:08,341 iteration 66 : loss : 0.251397, loss_ce: 0.111613
2022-01-09 23:05:09,968 iteration 67 : loss : 0.277321, loss_ce: 0.095052
2022-01-09 23:05:11,539 iteration 68 : loss : 0.255590, loss_ce: 0.110595
  1%|▎                              | 4/400 [01:46<2:56:43, 26.78s/it]2022-01-09 23:05:13,181 iteration 69 : loss : 0.245053, loss_ce: 0.101376
2022-01-09 23:05:14,835 iteration 70 : loss : 0.233256, loss_ce: 0.088012
2022-01-09 23:05:16,377 iteration 71 : loss : 0.239036, loss_ce: 0.096779
2022-01-09 23:05:17,966 iteration 72 : loss : 0.231812, loss_ce: 0.096044
2022-01-09 23:05:19,497 iteration 73 : loss : 0.242340, loss_ce: 0.115054
2022-01-09 23:05:20,999 iteration 74 : loss : 0.234159, loss_ce: 0.098257
2022-01-09 23:05:22,536 iteration 75 : loss : 0.224232, loss_ce: 0.098363
2022-01-09 23:05:24,064 iteration 76 : loss : 0.257679, loss_ce: 0.112969
2022-01-09 23:05:25,517 iteration 77 : loss : 0.230156, loss_ce: 0.105769
2022-01-09 23:05:27,074 iteration 78 : loss : 0.276793, loss_ce: 0.124372
2022-01-09 23:05:28,601 iteration 79 : loss : 0.300401, loss_ce: 0.116474
2022-01-09 23:05:30,113 iteration 80 : loss : 0.256738, loss_ce: 0.116550
2022-01-09 23:05:31,621 iteration 81 : loss : 0.242107, loss_ce: 0.104474
2022-01-09 23:05:33,152 iteration 82 : loss : 0.225150, loss_ce: 0.084983
2022-01-09 23:05:34,679 iteration 83 : loss : 0.254555, loss_ce: 0.092217
2022-01-09 23:05:36,297 iteration 84 : loss : 0.268968, loss_ce: 0.135684
2022-01-09 23:05:36,297 Training Data Eval:
2022-01-09 23:05:44,344   Average segmentation loss on training set: 4.1036
2022-01-09 23:05:44,344 Validation Data Eval:
2022-01-09 23:05:47,258   Average segmentation loss on validation set: 4.0865
2022-01-09 23:05:48,788 iteration 85 : loss : 0.273741, loss_ce: 0.115325
  1%|▍                              | 5/400 [02:23<3:21:06, 30.55s/it]2022-01-09 23:05:50,401 iteration 86 : loss : 0.285227, loss_ce: 0.100403
2022-01-09 23:05:52,087 iteration 87 : loss : 0.247445, loss_ce: 0.105403
2022-01-09 23:05:53,603 iteration 88 : loss : 0.235185, loss_ce: 0.098801
2022-01-09 23:05:55,192 iteration 89 : loss : 0.254186, loss_ce: 0.104251
2022-01-09 23:05:56,772 iteration 90 : loss : 0.234050, loss_ce: 0.095363
2022-01-09 23:05:58,416 iteration 91 : loss : 0.242164, loss_ce: 0.118468
2022-01-09 23:05:59,920 iteration 92 : loss : 0.240072, loss_ce: 0.106549
2022-01-09 23:06:01,466 iteration 93 : loss : 0.258110, loss_ce: 0.093432
2022-01-09 23:06:03,022 iteration 94 : loss : 0.243842, loss_ce: 0.093187
2022-01-09 23:06:04,706 iteration 95 : loss : 0.254868, loss_ce: 0.116380
2022-01-09 23:06:06,246 iteration 96 : loss : 0.225299, loss_ce: 0.097059
2022-01-09 23:06:07,790 iteration 97 : loss : 0.268151, loss_ce: 0.109572
2022-01-09 23:06:09,351 iteration 98 : loss : 0.246222, loss_ce: 0.110288
2022-01-09 23:06:10,992 iteration 99 : loss : 0.217737, loss_ce: 0.097959
2022-01-09 23:06:12,580 iteration 100 : loss : 0.222331, loss_ce: 0.096404
2022-01-09 23:06:14,142 iteration 101 : loss : 0.192143, loss_ce: 0.075167
2022-01-09 23:06:15,642 iteration 102 : loss : 0.240088, loss_ce: 0.100563
  2%|▍                              | 6/400 [02:50<3:12:23, 29.30s/it]2022-01-09 23:06:17,349 iteration 103 : loss : 0.228414, loss_ce: 0.099505
2022-01-09 23:06:18,992 iteration 104 : loss : 0.257209, loss_ce: 0.104364
2022-01-09 23:06:20,680 iteration 105 : loss : 0.242756, loss_ce: 0.087245
2022-01-09 23:06:22,207 iteration 106 : loss : 0.240795, loss_ce: 0.090683
2022-01-09 23:06:23,915 iteration 107 : loss : 0.350913, loss_ce: 0.180101
2022-01-09 23:06:25,422 iteration 108 : loss : 0.269375, loss_ce: 0.106012
2022-01-09 23:06:26,962 iteration 109 : loss : 0.209335, loss_ce: 0.089309
2022-01-09 23:06:28,451 iteration 110 : loss : 0.222780, loss_ce: 0.097217
2022-01-09 23:06:30,033 iteration 111 : loss : 0.266359, loss_ce: 0.123652
2022-01-09 23:06:31,530 iteration 112 : loss : 0.227701, loss_ce: 0.092804
2022-01-09 23:06:33,092 iteration 113 : loss : 0.274093, loss_ce: 0.143517
2022-01-09 23:06:34,621 iteration 114 : loss : 0.244906, loss_ce: 0.084765
2022-01-09 23:06:36,170 iteration 115 : loss : 0.238167, loss_ce: 0.097393
2022-01-09 23:06:37,776 iteration 116 : loss : 0.274120, loss_ce: 0.117838
2022-01-09 23:06:39,378 iteration 117 : loss : 0.235425, loss_ce: 0.097662
2022-01-09 23:06:40,947 iteration 118 : loss : 0.309180, loss_ce: 0.140066
2022-01-09 23:06:42,529 iteration 119 : loss : 0.207373, loss_ce: 0.072614
  2%|▌                              | 7/400 [03:17<3:06:44, 28.51s/it]2022-01-09 23:06:44,104 iteration 120 : loss : 0.343599, loss_ce: 0.173337
2022-01-09 23:06:45,611 iteration 121 : loss : 0.234069, loss_ce: 0.095581
2022-01-09 23:06:47,230 iteration 122 : loss : 0.239467, loss_ce: 0.090995
2022-01-09 23:06:48,811 iteration 123 : loss : 0.235027, loss_ce: 0.092234
2022-01-09 23:06:50,366 iteration 124 : loss : 0.267442, loss_ce: 0.107801
2022-01-09 23:06:51,886 iteration 125 : loss : 0.237087, loss_ce: 0.111680
2022-01-09 23:06:53,465 iteration 126 : loss : 0.233184, loss_ce: 0.089102
2022-01-09 23:06:54,979 iteration 127 : loss : 0.218398, loss_ce: 0.091486
2022-01-09 23:06:56,513 iteration 128 : loss : 0.223439, loss_ce: 0.084592
2022-01-09 23:06:58,124 iteration 129 : loss : 0.233406, loss_ce: 0.081738
2022-01-09 23:06:59,675 iteration 130 : loss : 0.226177, loss_ce: 0.084327
2022-01-09 23:07:01,316 iteration 131 : loss : 0.253079, loss_ce: 0.119267
2022-01-09 23:07:02,979 iteration 132 : loss : 0.205865, loss_ce: 0.059625
2022-01-09 23:07:04,516 iteration 133 : loss : 0.216026, loss_ce: 0.080393
2022-01-09 23:07:06,148 iteration 134 : loss : 0.223101, loss_ce: 0.090272
2022-01-09 23:07:07,791 iteration 135 : loss : 0.237303, loss_ce: 0.100783
2022-01-09 23:07:09,346 iteration 136 : loss : 0.189399, loss_ce: 0.080384
  2%|▌                              | 8/400 [03:44<3:02:44, 27.97s/it]2022-01-09 23:07:11,032 iteration 137 : loss : 0.235769, loss_ce: 0.082777
2022-01-09 23:07:12,566 iteration 138 : loss : 0.248692, loss_ce: 0.127740
2022-01-09 23:07:14,158 iteration 139 : loss : 0.272469, loss_ce: 0.116148
2022-01-09 23:07:15,725 iteration 140 : loss : 0.210986, loss_ce: 0.078249
2022-01-09 23:07:17,333 iteration 141 : loss : 0.215670, loss_ce: 0.089701
2022-01-09 23:07:18,954 iteration 142 : loss : 0.239365, loss_ce: 0.092648
2022-01-09 23:07:20,599 iteration 143 : loss : 0.262339, loss_ce: 0.115954
2022-01-09 23:07:22,213 iteration 144 : loss : 0.263341, loss_ce: 0.101180
2022-01-09 23:07:23,773 iteration 145 : loss : 0.276471, loss_ce: 0.114921
2022-01-09 23:07:25,378 iteration 146 : loss : 0.207698, loss_ce: 0.092649
2022-01-09 23:07:26,992 iteration 147 : loss : 0.233328, loss_ce: 0.093151
2022-01-09 23:07:28,482 iteration 148 : loss : 0.234466, loss_ce: 0.099430
2022-01-09 23:07:30,036 iteration 149 : loss : 0.264627, loss_ce: 0.120722
2022-01-09 23:07:31,565 iteration 150 : loss : 0.281150, loss_ce: 0.105415
2022-01-09 23:07:33,100 iteration 151 : loss : 0.233920, loss_ce: 0.104416
2022-01-09 23:07:34,646 iteration 152 : loss : 0.234906, loss_ce: 0.078917
2022-01-09 23:07:36,234 iteration 153 : loss : 0.254964, loss_ce: 0.112325
  2%|▋                              | 9/400 [04:11<3:00:05, 27.64s/it]2022-01-09 23:07:37,805 iteration 154 : loss : 0.257753, loss_ce: 0.107262
2022-01-09 23:07:39,384 iteration 155 : loss : 0.221138, loss_ce: 0.087336
2022-01-09 23:07:41,033 iteration 156 : loss : 0.217158, loss_ce: 0.081727
2022-01-09 23:07:42,619 iteration 157 : loss : 0.294165, loss_ce: 0.111915
2022-01-09 23:07:44,313 iteration 158 : loss : 0.229910, loss_ce: 0.099064
2022-01-09 23:07:45,771 iteration 159 : loss : 0.207595, loss_ce: 0.083449
2022-01-09 23:07:47,416 iteration 160 : loss : 0.198360, loss_ce: 0.062478
2022-01-09 23:07:49,049 iteration 161 : loss : 0.268129, loss_ce: 0.102903
2022-01-09 23:07:50,686 iteration 162 : loss : 0.234084, loss_ce: 0.101760
2022-01-09 23:07:52,256 iteration 163 : loss : 0.228107, loss_ce: 0.083155
2022-01-09 23:07:53,847 iteration 164 : loss : 0.232385, loss_ce: 0.087217
2022-01-09 23:07:55,410 iteration 165 : loss : 0.265785, loss_ce: 0.119287
2022-01-09 23:07:56,954 iteration 166 : loss : 0.249350, loss_ce: 0.097467
2022-01-09 23:07:58,517 iteration 167 : loss : 0.224940, loss_ce: 0.084417
2022-01-09 23:08:00,119 iteration 168 : loss : 0.259031, loss_ce: 0.123688
2022-01-09 23:08:01,693 iteration 169 : loss : 0.204471, loss_ce: 0.099272
2022-01-09 23:08:01,693 Training Data Eval:
2022-01-09 23:08:09,731   Average segmentation loss on training set: 0.3766
2022-01-09 23:08:09,732 Validation Data Eval:
2022-01-09 23:08:12,494   Average segmentation loss on validation set: 0.3415
2022-01-09 23:08:18,684 Found new lowest validation loss at iteration 169! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:08:20,195 iteration 170 : loss : 0.237933, loss_ce: 0.111333
  2%|▊                             | 10/400 [04:55<3:32:22, 32.67s/it]2022-01-09 23:08:21,705 iteration 171 : loss : 0.256799, loss_ce: 0.112419
2022-01-09 23:08:23,288 iteration 172 : loss : 0.234983, loss_ce: 0.105765
2022-01-09 23:08:24,902 iteration 173 : loss : 0.237001, loss_ce: 0.104094
2022-01-09 23:08:26,429 iteration 174 : loss : 0.225683, loss_ce: 0.101059
2022-01-09 23:08:27,948 iteration 175 : loss : 0.268217, loss_ce: 0.107418
2022-01-09 23:08:29,541 iteration 176 : loss : 0.206013, loss_ce: 0.095404
2022-01-09 23:08:31,105 iteration 177 : loss : 0.187439, loss_ce: 0.066432
2022-01-09 23:08:32,672 iteration 178 : loss : 0.172504, loss_ce: 0.073928
2022-01-09 23:08:34,237 iteration 179 : loss : 0.239325, loss_ce: 0.092906
2022-01-09 23:08:35,783 iteration 180 : loss : 0.197019, loss_ce: 0.070935
2022-01-09 23:08:37,379 iteration 181 : loss : 0.202182, loss_ce: 0.079963
2022-01-09 23:08:38,916 iteration 182 : loss : 0.185809, loss_ce: 0.072535
2022-01-09 23:08:40,432 iteration 183 : loss : 0.207023, loss_ce: 0.073840
2022-01-09 23:08:41,911 iteration 184 : loss : 0.239789, loss_ce: 0.092627
2022-01-09 23:08:43,505 iteration 185 : loss : 0.287988, loss_ce: 0.108491
2022-01-09 23:08:45,100 iteration 186 : loss : 0.264016, loss_ce: 0.117054
2022-01-09 23:08:46,730 iteration 187 : loss : 0.249957, loss_ce: 0.100332
  3%|▊                             | 11/400 [05:21<3:19:38, 30.79s/it]2022-01-09 23:08:48,388 iteration 188 : loss : 0.287949, loss_ce: 0.119610
2022-01-09 23:08:49,976 iteration 189 : loss : 0.192800, loss_ce: 0.077236
2022-01-09 23:08:51,566 iteration 190 : loss : 0.186427, loss_ce: 0.064738
2022-01-09 23:08:53,114 iteration 191 : loss : 0.183646, loss_ce: 0.070068
2022-01-09 23:08:54,616 iteration 192 : loss : 0.225307, loss_ce: 0.102608
2022-01-09 23:08:56,240 iteration 193 : loss : 0.202824, loss_ce: 0.085129
2022-01-09 23:08:57,793 iteration 194 : loss : 0.259236, loss_ce: 0.074990
2022-01-09 23:08:59,323 iteration 195 : loss : 0.244472, loss_ce: 0.102886
2022-01-09 23:09:00,809 iteration 196 : loss : 0.249209, loss_ce: 0.084113
2022-01-09 23:09:02,381 iteration 197 : loss : 0.261602, loss_ce: 0.112745
2022-01-09 23:09:03,982 iteration 198 : loss : 0.206787, loss_ce: 0.090681
2022-01-09 23:09:05,513 iteration 199 : loss : 0.217992, loss_ce: 0.094739
2022-01-09 23:09:07,062 iteration 200 : loss : 0.217818, loss_ce: 0.091430
2022-01-09 23:09:08,645 iteration 201 : loss : 0.165663, loss_ce: 0.062005
2022-01-09 23:09:10,251 iteration 202 : loss : 0.193508, loss_ce: 0.081571
2022-01-09 23:09:11,778 iteration 203 : loss : 0.183531, loss_ce: 0.068818
2022-01-09 23:09:13,434 iteration 204 : loss : 0.219183, loss_ce: 0.084722
  3%|▉                             | 12/400 [05:48<3:11:05, 29.55s/it]2022-01-09 23:09:15,030 iteration 205 : loss : 0.183235, loss_ce: 0.069954
2022-01-09 23:09:16,593 iteration 206 : loss : 0.183477, loss_ce: 0.062299
2022-01-09 23:09:18,179 iteration 207 : loss : 0.178421, loss_ce: 0.073897
2022-01-09 23:09:19,725 iteration 208 : loss : 0.218988, loss_ce: 0.068785
2022-01-09 23:09:21,366 iteration 209 : loss : 0.288056, loss_ce: 0.145806
2022-01-09 23:09:22,846 iteration 210 : loss : 0.190573, loss_ce: 0.074525
2022-01-09 23:09:24,452 iteration 211 : loss : 0.238066, loss_ce: 0.110431
2022-01-09 23:09:26,038 iteration 212 : loss : 0.245270, loss_ce: 0.092243
2022-01-09 23:09:27,746 iteration 213 : loss : 0.215335, loss_ce: 0.098141
2022-01-09 23:09:29,320 iteration 214 : loss : 0.268840, loss_ce: 0.094761
2022-01-09 23:09:30,894 iteration 215 : loss : 0.257795, loss_ce: 0.104687
2022-01-09 23:09:32,622 iteration 216 : loss : 0.302338, loss_ce: 0.120289
2022-01-09 23:09:34,287 iteration 217 : loss : 0.254532, loss_ce: 0.100773
2022-01-09 23:09:35,874 iteration 218 : loss : 0.246331, loss_ce: 0.105768
2022-01-09 23:09:37,349 iteration 219 : loss : 0.254560, loss_ce: 0.108332
2022-01-09 23:09:38,941 iteration 220 : loss : 0.211136, loss_ce: 0.098132
2022-01-09 23:09:40,514 iteration 221 : loss : 0.269911, loss_ce: 0.110538
  3%|▉                             | 13/400 [06:15<3:05:46, 28.80s/it]2022-01-09 23:09:42,163 iteration 222 : loss : 0.260543, loss_ce: 0.125620
2022-01-09 23:09:43,783 iteration 223 : loss : 0.202263, loss_ce: 0.091373
2022-01-09 23:09:45,355 iteration 224 : loss : 0.350473, loss_ce: 0.188220
2022-01-09 23:09:47,024 iteration 225 : loss : 0.311072, loss_ce: 0.106755
2022-01-09 23:09:48,593 iteration 226 : loss : 0.216770, loss_ce: 0.087115
2022-01-09 23:09:50,209 iteration 227 : loss : 0.274158, loss_ce: 0.132896
2022-01-09 23:09:51,781 iteration 228 : loss : 0.256823, loss_ce: 0.114034
2022-01-09 23:09:53,308 iteration 229 : loss : 0.204185, loss_ce: 0.066725
2022-01-09 23:09:54,889 iteration 230 : loss : 0.188641, loss_ce: 0.071163
2022-01-09 23:09:56,509 iteration 231 : loss : 0.275885, loss_ce: 0.105451
2022-01-09 23:09:58,123 iteration 232 : loss : 0.174292, loss_ce: 0.068741
2022-01-09 23:09:59,656 iteration 233 : loss : 0.252690, loss_ce: 0.113885
2022-01-09 23:10:01,296 iteration 234 : loss : 0.234468, loss_ce: 0.099324
2022-01-09 23:10:02,852 iteration 235 : loss : 0.210344, loss_ce: 0.094135
2022-01-09 23:10:04,453 iteration 236 : loss : 0.223693, loss_ce: 0.092415
2022-01-09 23:10:06,009 iteration 237 : loss : 0.192610, loss_ce: 0.080334
2022-01-09 23:10:07,578 iteration 238 : loss : 0.218720, loss_ce: 0.108646
  4%|█                             | 14/400 [06:42<3:01:55, 28.28s/it]2022-01-09 23:10:09,156 iteration 239 : loss : 0.230162, loss_ce: 0.087252
2022-01-09 23:10:10,759 iteration 240 : loss : 0.212965, loss_ce: 0.103380
2022-01-09 23:10:12,371 iteration 241 : loss : 0.266570, loss_ce: 0.110887
2022-01-09 23:10:13,965 iteration 242 : loss : 0.246384, loss_ce: 0.126359
2022-01-09 23:10:15,689 iteration 243 : loss : 0.257877, loss_ce: 0.103096
2022-01-09 23:10:17,216 iteration 244 : loss : 0.220455, loss_ce: 0.058448
2022-01-09 23:10:18,806 iteration 245 : loss : 0.203088, loss_ce: 0.093440
2022-01-09 23:10:20,434 iteration 246 : loss : 0.240123, loss_ce: 0.098821
2022-01-09 23:10:22,001 iteration 247 : loss : 0.261185, loss_ce: 0.092121
2022-01-09 23:10:23,563 iteration 248 : loss : 0.236381, loss_ce: 0.080464
2022-01-09 23:10:25,108 iteration 249 : loss : 0.252648, loss_ce: 0.139349
2022-01-09 23:10:26,669 iteration 250 : loss : 0.236170, loss_ce: 0.095483
2022-01-09 23:10:28,177 iteration 251 : loss : 0.257917, loss_ce: 0.093121
2022-01-09 23:10:29,735 iteration 252 : loss : 0.164537, loss_ce: 0.067137
2022-01-09 23:10:31,329 iteration 253 : loss : 0.229383, loss_ce: 0.086949
2022-01-09 23:10:32,983 iteration 254 : loss : 0.315521, loss_ce: 0.118616
2022-01-09 23:10:32,983 Training Data Eval:
2022-01-09 23:10:41,024   Average segmentation loss on training set: 0.2800
2022-01-09 23:10:41,024 Validation Data Eval:
2022-01-09 23:10:43,790   Average segmentation loss on validation set: 0.2607
2022-01-09 23:10:50,166 Found new lowest validation loss at iteration 254! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:10:51,605 iteration 255 : loss : 0.241793, loss_ce: 0.101262
  4%|█▏                            | 15/400 [07:26<3:31:55, 33.03s/it]2022-01-09 23:10:53,136 iteration 256 : loss : 0.249802, loss_ce: 0.101582
2022-01-09 23:10:54,756 iteration 257 : loss : 0.194642, loss_ce: 0.081784
2022-01-09 23:10:56,428 iteration 258 : loss : 0.208921, loss_ce: 0.078049
2022-01-09 23:10:57,927 iteration 259 : loss : 0.183092, loss_ce: 0.086542
2022-01-09 23:10:59,564 iteration 260 : loss : 0.205364, loss_ce: 0.092798
2022-01-09 23:11:01,159 iteration 261 : loss : 0.266207, loss_ce: 0.098938
2022-01-09 23:11:02,839 iteration 262 : loss : 0.225310, loss_ce: 0.091451
2022-01-09 23:11:04,422 iteration 263 : loss : 0.206930, loss_ce: 0.082500
2022-01-09 23:11:06,035 iteration 264 : loss : 0.274574, loss_ce: 0.137446
2022-01-09 23:11:07,576 iteration 265 : loss : 0.168306, loss_ce: 0.068394
2022-01-09 23:11:09,185 iteration 266 : loss : 0.201774, loss_ce: 0.094596
2022-01-09 23:11:10,757 iteration 267 : loss : 0.245310, loss_ce: 0.079539
2022-01-09 23:11:12,343 iteration 268 : loss : 0.253815, loss_ce: 0.112147
2022-01-09 23:11:14,122 iteration 269 : loss : 0.251562, loss_ce: 0.091300
2022-01-09 23:11:15,808 iteration 270 : loss : 0.188558, loss_ce: 0.071715
2022-01-09 23:11:17,270 iteration 271 : loss : 0.196954, loss_ce: 0.056663
2022-01-09 23:11:18,851 iteration 272 : loss : 0.202754, loss_ce: 0.094956
  4%|█▏                            | 16/400 [07:53<3:20:14, 31.29s/it]2022-01-09 23:11:20,443 iteration 273 : loss : 0.184222, loss_ce: 0.065740
2022-01-09 23:11:21,957 iteration 274 : loss : 0.162889, loss_ce: 0.066524
2022-01-09 23:11:23,481 iteration 275 : loss : 0.171475, loss_ce: 0.068237
2022-01-09 23:11:25,062 iteration 276 : loss : 0.186599, loss_ce: 0.091877
2022-01-09 23:11:26,664 iteration 277 : loss : 0.172890, loss_ce: 0.067281
2022-01-09 23:11:28,115 iteration 278 : loss : 0.234283, loss_ce: 0.087869
2022-01-09 23:11:29,632 iteration 279 : loss : 0.225158, loss_ce: 0.088551
2022-01-09 23:11:31,142 iteration 280 : loss : 0.206227, loss_ce: 0.089076
2022-01-09 23:11:32,716 iteration 281 : loss : 0.220395, loss_ce: 0.092144
2022-01-09 23:11:34,265 iteration 282 : loss : 0.248083, loss_ce: 0.103530
2022-01-09 23:11:35,794 iteration 283 : loss : 0.201838, loss_ce: 0.096605
2022-01-09 23:11:37,422 iteration 284 : loss : 0.247505, loss_ce: 0.119094
2022-01-09 23:11:38,932 iteration 285 : loss : 0.196395, loss_ce: 0.065976
2022-01-09 23:11:40,460 iteration 286 : loss : 0.178807, loss_ce: 0.079654
2022-01-09 23:11:42,109 iteration 287 : loss : 0.197442, loss_ce: 0.089665
2022-01-09 23:11:43,735 iteration 288 : loss : 0.225747, loss_ce: 0.091523
2022-01-09 23:11:45,284 iteration 289 : loss : 0.200439, loss_ce: 0.077885
  4%|█▎                            | 17/400 [08:20<3:10:22, 29.82s/it]2022-01-09 23:11:46,924 iteration 290 : loss : 0.169142, loss_ce: 0.063393
2022-01-09 23:11:48,458 iteration 291 : loss : 0.200918, loss_ce: 0.081475
2022-01-09 23:11:50,002 iteration 292 : loss : 0.182928, loss_ce: 0.071713
2022-01-09 23:11:51,549 iteration 293 : loss : 0.227875, loss_ce: 0.102140
2022-01-09 23:11:53,128 iteration 294 : loss : 0.209683, loss_ce: 0.085720
2022-01-09 23:11:54,796 iteration 295 : loss : 0.243055, loss_ce: 0.084795
2022-01-09 23:11:56,342 iteration 296 : loss : 0.209071, loss_ce: 0.072530
2022-01-09 23:11:57,917 iteration 297 : loss : 0.206787, loss_ce: 0.081270
2022-01-09 23:11:59,458 iteration 298 : loss : 0.191905, loss_ce: 0.067437
2022-01-09 23:12:01,026 iteration 299 : loss : 0.204084, loss_ce: 0.068202
2022-01-09 23:12:02,598 iteration 300 : loss : 0.214344, loss_ce: 0.076824
2022-01-09 23:12:04,177 iteration 301 : loss : 0.290773, loss_ce: 0.151479
2022-01-09 23:12:05,812 iteration 302 : loss : 0.202099, loss_ce: 0.082408
2022-01-09 23:12:07,425 iteration 303 : loss : 0.211954, loss_ce: 0.089635
2022-01-09 23:12:09,019 iteration 304 : loss : 0.275219, loss_ce: 0.141735
2022-01-09 23:12:10,649 iteration 305 : loss : 0.194750, loss_ce: 0.090259
2022-01-09 23:12:12,271 iteration 306 : loss : 0.163868, loss_ce: 0.081063
  4%|█▎                            | 18/400 [08:47<3:04:28, 28.98s/it]2022-01-09 23:12:13,927 iteration 307 : loss : 0.218301, loss_ce: 0.080932
2022-01-09 23:12:15,468 iteration 308 : loss : 0.233444, loss_ce: 0.112691
2022-01-09 23:12:17,065 iteration 309 : loss : 0.294046, loss_ce: 0.119349
2022-01-09 23:12:18,682 iteration 310 : loss : 0.209423, loss_ce: 0.094809
2022-01-09 23:12:20,217 iteration 311 : loss : 0.217757, loss_ce: 0.074040
2022-01-09 23:12:21,785 iteration 312 : loss : 0.182381, loss_ce: 0.085026
2022-01-09 23:12:23,364 iteration 313 : loss : 0.233697, loss_ce: 0.122528
2022-01-09 23:12:24,869 iteration 314 : loss : 0.183572, loss_ce: 0.081516
2022-01-09 23:12:26,423 iteration 315 : loss : 0.227292, loss_ce: 0.086805
2022-01-09 23:12:28,010 iteration 316 : loss : 0.228053, loss_ce: 0.085006
2022-01-09 23:12:29,585 iteration 317 : loss : 0.230283, loss_ce: 0.092698
2022-01-09 23:12:31,176 iteration 318 : loss : 0.176205, loss_ce: 0.079069
2022-01-09 23:12:32,691 iteration 319 : loss : 0.183409, loss_ce: 0.081928
2022-01-09 23:12:34,341 iteration 320 : loss : 0.199942, loss_ce: 0.077701
2022-01-09 23:12:35,953 iteration 321 : loss : 0.211929, loss_ce: 0.077087
2022-01-09 23:12:37,524 iteration 322 : loss : 0.257712, loss_ce: 0.127886
2022-01-09 23:12:39,152 iteration 323 : loss : 0.217552, loss_ce: 0.073394
  5%|█▍                            | 19/400 [09:14<2:59:59, 28.35s/it]2022-01-09 23:12:40,776 iteration 324 : loss : 0.190491, loss_ce: 0.071060
2022-01-09 23:12:42,352 iteration 325 : loss : 0.209428, loss_ce: 0.071984
2022-01-09 23:12:43,956 iteration 326 : loss : 0.174353, loss_ce: 0.072648
2022-01-09 23:12:45,631 iteration 327 : loss : 0.232885, loss_ce: 0.094867
2022-01-09 23:12:47,197 iteration 328 : loss : 0.189446, loss_ce: 0.077078
2022-01-09 23:12:48,805 iteration 329 : loss : 0.185213, loss_ce: 0.087123
2022-01-09 23:12:50,427 iteration 330 : loss : 0.191807, loss_ce: 0.079820
2022-01-09 23:12:51,959 iteration 331 : loss : 0.205267, loss_ce: 0.095405
2022-01-09 23:12:53,521 iteration 332 : loss : 0.173540, loss_ce: 0.061270
2022-01-09 23:12:55,119 iteration 333 : loss : 0.157645, loss_ce: 0.072751
2022-01-09 23:12:56,701 iteration 334 : loss : 0.203485, loss_ce: 0.088741
2022-01-09 23:12:58,252 iteration 335 : loss : 0.239652, loss_ce: 0.089496
2022-01-09 23:12:59,876 iteration 336 : loss : 0.178210, loss_ce: 0.068520
2022-01-09 23:13:01,447 iteration 337 : loss : 0.239295, loss_ce: 0.087489
2022-01-09 23:13:02,980 iteration 338 : loss : 0.207604, loss_ce: 0.116325
2022-01-09 23:13:04,521 iteration 339 : loss : 0.187514, loss_ce: 0.067435
2022-01-09 23:13:04,521 Training Data Eval:
2022-01-09 23:13:12,581   Average segmentation loss on training set: 0.1690
2022-01-09 23:13:12,581 Validation Data Eval:
2022-01-09 23:13:15,355   Average segmentation loss on validation set: 0.1950
2022-01-09 23:13:21,560 Found new lowest validation loss at iteration 339! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:13:23,084 iteration 340 : loss : 0.183399, loss_ce: 0.077841
  5%|█▌                            | 20/400 [09:58<3:29:08, 33.02s/it]2022-01-09 23:13:24,643 iteration 341 : loss : 0.196407, loss_ce: 0.080797
2022-01-09 23:13:26,242 iteration 342 : loss : 0.130641, loss_ce: 0.045640
2022-01-09 23:13:27,818 iteration 343 : loss : 0.180526, loss_ce: 0.067865
2022-01-09 23:13:29,431 iteration 344 : loss : 0.224124, loss_ce: 0.101115
2022-01-09 23:13:31,003 iteration 345 : loss : 0.143519, loss_ce: 0.048004
2022-01-09 23:13:32,643 iteration 346 : loss : 0.199602, loss_ce: 0.081966
2022-01-09 23:13:34,167 iteration 347 : loss : 0.231058, loss_ce: 0.111876
2022-01-09 23:13:35,768 iteration 348 : loss : 0.251253, loss_ce: 0.101807
2022-01-09 23:13:37,472 iteration 349 : loss : 0.200502, loss_ce: 0.109936
2022-01-09 23:13:39,045 iteration 350 : loss : 0.172462, loss_ce: 0.072831
2022-01-09 23:13:40,674 iteration 351 : loss : 0.160627, loss_ce: 0.076922
2022-01-09 23:13:42,287 iteration 352 : loss : 0.217079, loss_ce: 0.085900
2022-01-09 23:13:43,815 iteration 353 : loss : 0.222701, loss_ce: 0.085111
2022-01-09 23:13:45,398 iteration 354 : loss : 0.260009, loss_ce: 0.097790
2022-01-09 23:13:47,031 iteration 355 : loss : 0.243511, loss_ce: 0.096755
2022-01-09 23:13:48,736 iteration 356 : loss : 0.227464, loss_ce: 0.098238
2022-01-09 23:13:50,251 iteration 357 : loss : 0.162732, loss_ce: 0.061598
  5%|█▌                            | 21/400 [10:25<3:17:29, 31.27s/it]2022-01-09 23:13:51,920 iteration 358 : loss : 0.211272, loss_ce: 0.097851
2022-01-09 23:13:53,570 iteration 359 : loss : 0.193269, loss_ce: 0.083266
2022-01-09 23:13:55,098 iteration 360 : loss : 0.186587, loss_ce: 0.068705
2022-01-09 23:13:56,713 iteration 361 : loss : 0.251739, loss_ce: 0.095465
2022-01-09 23:13:58,377 iteration 362 : loss : 0.166980, loss_ce: 0.048956
2022-01-09 23:14:00,035 iteration 363 : loss : 0.178670, loss_ce: 0.069006
2022-01-09 23:14:01,642 iteration 364 : loss : 0.179320, loss_ce: 0.072330
2022-01-09 23:14:03,130 iteration 365 : loss : 0.196880, loss_ce: 0.092025
2022-01-09 23:14:04,742 iteration 366 : loss : 0.209111, loss_ce: 0.092600
2022-01-09 23:14:06,255 iteration 367 : loss : 0.187408, loss_ce: 0.074793
2022-01-09 23:14:07,876 iteration 368 : loss : 0.184559, loss_ce: 0.064711
2022-01-09 23:14:09,531 iteration 369 : loss : 0.187486, loss_ce: 0.059917
2022-01-09 23:14:11,169 iteration 370 : loss : 0.118976, loss_ce: 0.063061
2022-01-09 23:14:12,769 iteration 371 : loss : 0.219145, loss_ce: 0.073468
2022-01-09 23:14:14,374 iteration 372 : loss : 0.263634, loss_ce: 0.140063
2022-01-09 23:14:15,908 iteration 373 : loss : 0.207318, loss_ce: 0.071637
2022-01-09 23:14:17,469 iteration 374 : loss : 0.183149, loss_ce: 0.075711
  6%|█▋                            | 22/400 [10:52<3:09:20, 30.05s/it]2022-01-09 23:14:19,173 iteration 375 : loss : 0.255707, loss_ce: 0.108795
2022-01-09 23:14:20,708 iteration 376 : loss : 0.134735, loss_ce: 0.049544
2022-01-09 23:14:22,276 iteration 377 : loss : 0.157681, loss_ce: 0.056753
2022-01-09 23:14:23,921 iteration 378 : loss : 0.193190, loss_ce: 0.076056
2022-01-09 23:14:25,419 iteration 379 : loss : 0.163667, loss_ce: 0.072348
2022-01-09 23:14:27,028 iteration 380 : loss : 0.189072, loss_ce: 0.070744
2022-01-09 23:14:28,597 iteration 381 : loss : 0.162257, loss_ce: 0.059920
2022-01-09 23:14:30,093 iteration 382 : loss : 0.175572, loss_ce: 0.079125
2022-01-09 23:14:31,667 iteration 383 : loss : 0.191089, loss_ce: 0.077366
2022-01-09 23:14:33,263 iteration 384 : loss : 0.153025, loss_ce: 0.066586
2022-01-09 23:14:34,838 iteration 385 : loss : 0.151506, loss_ce: 0.063773
2022-01-09 23:14:36,423 iteration 386 : loss : 0.233744, loss_ce: 0.085045
2022-01-09 23:14:38,025 iteration 387 : loss : 0.247977, loss_ce: 0.081656
2022-01-09 23:14:39,637 iteration 388 : loss : 0.196616, loss_ce: 0.076588
2022-01-09 23:14:41,246 iteration 389 : loss : 0.255604, loss_ce: 0.129199
2022-01-09 23:14:42,808 iteration 390 : loss : 0.142313, loss_ce: 0.061374
2022-01-09 23:14:44,368 iteration 391 : loss : 0.223527, loss_ce: 0.122307
  6%|█▋                            | 23/400 [11:19<3:02:51, 29.10s/it]2022-01-09 23:14:46,030 iteration 392 : loss : 0.160426, loss_ce: 0.081410
2022-01-09 23:14:47,569 iteration 393 : loss : 0.146378, loss_ce: 0.054517
2022-01-09 23:14:49,201 iteration 394 : loss : 0.151750, loss_ce: 0.054450
2022-01-09 23:14:50,804 iteration 395 : loss : 0.193410, loss_ce: 0.082131
2022-01-09 23:14:52,358 iteration 396 : loss : 0.158034, loss_ce: 0.054580
2022-01-09 23:14:53,937 iteration 397 : loss : 0.195078, loss_ce: 0.092393
2022-01-09 23:14:55,537 iteration 398 : loss : 0.150671, loss_ce: 0.057341
2022-01-09 23:14:57,015 iteration 399 : loss : 0.172118, loss_ce: 0.075557
2022-01-09 23:14:58,601 iteration 400 : loss : 0.203121, loss_ce: 0.075462
2022-01-09 23:15:00,239 iteration 401 : loss : 0.203066, loss_ce: 0.081252
2022-01-09 23:15:01,800 iteration 402 : loss : 0.150325, loss_ce: 0.066179
2022-01-09 23:15:03,354 iteration 403 : loss : 0.150165, loss_ce: 0.054875
2022-01-09 23:15:04,964 iteration 404 : loss : 0.191083, loss_ce: 0.081956
2022-01-09 23:15:06,609 iteration 405 : loss : 0.208301, loss_ce: 0.087101
2022-01-09 23:15:08,164 iteration 406 : loss : 0.183113, loss_ce: 0.073050
2022-01-09 23:15:09,683 iteration 407 : loss : 0.192922, loss_ce: 0.074959
2022-01-09 23:15:11,245 iteration 408 : loss : 0.192537, loss_ce: 0.073324
  6%|█▊                            | 24/400 [11:46<2:58:12, 28.44s/it]2022-01-09 23:15:12,916 iteration 409 : loss : 0.179534, loss_ce: 0.093295
2022-01-09 23:15:14,520 iteration 410 : loss : 0.209523, loss_ce: 0.090451
2022-01-09 23:15:16,091 iteration 411 : loss : 0.313149, loss_ce: 0.102914
2022-01-09 23:15:17,577 iteration 412 : loss : 0.188653, loss_ce: 0.068537
2022-01-09 23:15:19,150 iteration 413 : loss : 0.200157, loss_ce: 0.073602
2022-01-09 23:15:20,668 iteration 414 : loss : 0.233636, loss_ce: 0.112962
2022-01-09 23:15:22,139 iteration 415 : loss : 0.152833, loss_ce: 0.058351
2022-01-09 23:15:23,695 iteration 416 : loss : 0.194025, loss_ce: 0.095074
2022-01-09 23:15:25,224 iteration 417 : loss : 0.221500, loss_ce: 0.092407
2022-01-09 23:15:26,853 iteration 418 : loss : 0.184250, loss_ce: 0.085572
2022-01-09 23:15:28,397 iteration 419 : loss : 0.201977, loss_ce: 0.108861
2022-01-09 23:15:29,980 iteration 420 : loss : 0.172563, loss_ce: 0.069717
2022-01-09 23:15:31,502 iteration 421 : loss : 0.191904, loss_ce: 0.078753
2022-01-09 23:15:33,022 iteration 422 : loss : 0.190018, loss_ce: 0.074111
2022-01-09 23:15:34,550 iteration 423 : loss : 0.191209, loss_ce: 0.060910
2022-01-09 23:15:36,133 iteration 424 : loss : 0.197131, loss_ce: 0.076210
2022-01-09 23:15:36,133 Training Data Eval:
2022-01-09 23:15:44,198   Average segmentation loss on training set: 0.1458
2022-01-09 23:15:44,199 Validation Data Eval:
2022-01-09 23:15:46,975   Average segmentation loss on validation set: 0.1930
2022-01-09 23:15:53,189 Found new lowest validation loss at iteration 424! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:15:54,650 iteration 425 : loss : 0.206180, loss_ce: 0.074301
  6%|█▉                            | 25/400 [12:29<3:25:47, 32.93s/it]2022-01-09 23:15:56,113 iteration 426 : loss : 0.167196, loss_ce: 0.054483
2022-01-09 23:15:57,717 iteration 427 : loss : 0.122295, loss_ce: 0.043697
2022-01-09 23:15:59,293 iteration 428 : loss : 0.168864, loss_ce: 0.063190
2022-01-09 23:16:00,813 iteration 429 : loss : 0.235607, loss_ce: 0.122161
2022-01-09 23:16:02,329 iteration 430 : loss : 0.180341, loss_ce: 0.065863
2022-01-09 23:16:03,871 iteration 431 : loss : 0.274461, loss_ce: 0.116046
2022-01-09 23:16:05,470 iteration 432 : loss : 0.124874, loss_ce: 0.052597
2022-01-09 23:16:07,108 iteration 433 : loss : 0.178974, loss_ce: 0.077961
2022-01-09 23:16:08,689 iteration 434 : loss : 0.134984, loss_ce: 0.058411
2022-01-09 23:16:10,204 iteration 435 : loss : 0.175242, loss_ce: 0.076908
2022-01-09 23:16:11,754 iteration 436 : loss : 0.131951, loss_ce: 0.062907
2022-01-09 23:16:13,312 iteration 437 : loss : 0.246387, loss_ce: 0.132374
2022-01-09 23:16:14,819 iteration 438 : loss : 0.182119, loss_ce: 0.065330
2022-01-09 23:16:16,393 iteration 439 : loss : 0.158697, loss_ce: 0.051251
2022-01-09 23:16:17,942 iteration 440 : loss : 0.229894, loss_ce: 0.084292
2022-01-09 23:16:19,554 iteration 441 : loss : 0.166002, loss_ce: 0.061690
2022-01-09 23:16:21,147 iteration 442 : loss : 0.206677, loss_ce: 0.090218
  6%|█▉                            | 26/400 [12:56<3:13:13, 31.00s/it]2022-01-09 23:16:22,756 iteration 443 : loss : 0.207846, loss_ce: 0.102377
2022-01-09 23:16:24,276 iteration 444 : loss : 0.177060, loss_ce: 0.066643
2022-01-09 23:16:25,832 iteration 445 : loss : 0.195555, loss_ce: 0.064800
2022-01-09 23:16:27,521 iteration 446 : loss : 0.173309, loss_ce: 0.066045
2022-01-09 23:16:29,155 iteration 447 : loss : 0.186226, loss_ce: 0.098936
2022-01-09 23:16:30,773 iteration 448 : loss : 0.280638, loss_ce: 0.090973
2022-01-09 23:16:32,352 iteration 449 : loss : 0.173800, loss_ce: 0.079425
2022-01-09 23:16:33,909 iteration 450 : loss : 0.167658, loss_ce: 0.064018
2022-01-09 23:16:35,560 iteration 451 : loss : 0.142188, loss_ce: 0.058941
2022-01-09 23:16:37,172 iteration 452 : loss : 0.137726, loss_ce: 0.058893
2022-01-09 23:16:38,775 iteration 453 : loss : 0.226666, loss_ce: 0.095532
2022-01-09 23:16:40,380 iteration 454 : loss : 0.212835, loss_ce: 0.073518
2022-01-09 23:16:42,015 iteration 455 : loss : 0.141723, loss_ce: 0.056623
2022-01-09 23:16:43,610 iteration 456 : loss : 0.185668, loss_ce: 0.070641
2022-01-09 23:16:45,196 iteration 457 : loss : 0.200029, loss_ce: 0.084550
2022-01-09 23:16:46,849 iteration 458 : loss : 0.226177, loss_ce: 0.113423
2022-01-09 23:16:48,367 iteration 459 : loss : 0.149504, loss_ce: 0.058527
  7%|██                            | 27/400 [13:23<3:05:38, 29.86s/it]2022-01-09 23:16:50,062 iteration 460 : loss : 0.193936, loss_ce: 0.095324
2022-01-09 23:16:51,657 iteration 461 : loss : 0.173345, loss_ce: 0.082774
2022-01-09 23:16:53,220 iteration 462 : loss : 0.162622, loss_ce: 0.065607
2022-01-09 23:16:54,856 iteration 463 : loss : 0.156025, loss_ce: 0.066101
2022-01-09 23:16:56,441 iteration 464 : loss : 0.131958, loss_ce: 0.052612
2022-01-09 23:16:57,969 iteration 465 : loss : 0.166325, loss_ce: 0.055514
2022-01-09 23:16:59,532 iteration 466 : loss : 0.173980, loss_ce: 0.058057
2022-01-09 23:17:01,081 iteration 467 : loss : 0.171448, loss_ce: 0.063990
2022-01-09 23:17:02,756 iteration 468 : loss : 0.169768, loss_ce: 0.079837
2022-01-09 23:17:04,309 iteration 469 : loss : 0.168138, loss_ce: 0.050739
2022-01-09 23:17:05,881 iteration 470 : loss : 0.168346, loss_ce: 0.069856
2022-01-09 23:17:07,444 iteration 471 : loss : 0.164712, loss_ce: 0.055011
2022-01-09 23:17:09,066 iteration 472 : loss : 0.197991, loss_ce: 0.076317
2022-01-09 23:17:10,783 iteration 473 : loss : 0.192480, loss_ce: 0.074145
2022-01-09 23:17:12,450 iteration 474 : loss : 0.209576, loss_ce: 0.092741
2022-01-09 23:17:14,004 iteration 475 : loss : 0.121063, loss_ce: 0.048332
2022-01-09 23:17:15,636 iteration 476 : loss : 0.184613, loss_ce: 0.104559
  7%|██                            | 28/400 [13:50<3:00:18, 29.08s/it]2022-01-09 23:17:17,255 iteration 477 : loss : 0.192311, loss_ce: 0.066448
2022-01-09 23:17:18,874 iteration 478 : loss : 0.159297, loss_ce: 0.056080
2022-01-09 23:17:20,443 iteration 479 : loss : 0.164130, loss_ce: 0.073600
2022-01-09 23:17:22,037 iteration 480 : loss : 0.142858, loss_ce: 0.065032
2022-01-09 23:17:23,533 iteration 481 : loss : 0.198221, loss_ce: 0.083739
2022-01-09 23:17:25,122 iteration 482 : loss : 0.164400, loss_ce: 0.045564
2022-01-09 23:17:26,720 iteration 483 : loss : 0.197719, loss_ce: 0.066313
2022-01-09 23:17:28,336 iteration 484 : loss : 0.219838, loss_ce: 0.099087
2022-01-09 23:17:29,895 iteration 485 : loss : 0.116985, loss_ce: 0.048560
2022-01-09 23:17:31,442 iteration 486 : loss : 0.193802, loss_ce: 0.063505
2022-01-09 23:17:33,071 iteration 487 : loss : 0.137357, loss_ce: 0.054726
2022-01-09 23:17:34,591 iteration 488 : loss : 0.128325, loss_ce: 0.051831
2022-01-09 23:17:36,165 iteration 489 : loss : 0.163409, loss_ce: 0.070325
2022-01-09 23:17:37,636 iteration 490 : loss : 0.150952, loss_ce: 0.060185
2022-01-09 23:17:39,219 iteration 491 : loss : 0.171970, loss_ce: 0.074154
2022-01-09 23:17:40,777 iteration 492 : loss : 0.130849, loss_ce: 0.063439
2022-01-09 23:17:42,377 iteration 493 : loss : 0.175196, loss_ce: 0.083570
  7%|██▏                           | 29/400 [14:17<2:55:28, 28.38s/it]2022-01-09 23:17:43,990 iteration 494 : loss : 0.165006, loss_ce: 0.069302
2022-01-09 23:17:45,600 iteration 495 : loss : 0.140143, loss_ce: 0.047787
2022-01-09 23:17:47,167 iteration 496 : loss : 0.180572, loss_ce: 0.080542
2022-01-09 23:17:48,766 iteration 497 : loss : 0.166840, loss_ce: 0.072110
2022-01-09 23:17:50,401 iteration 498 : loss : 0.152657, loss_ce: 0.072347
2022-01-09 23:17:51,911 iteration 499 : loss : 0.118022, loss_ce: 0.049976
2022-01-09 23:17:53,433 iteration 500 : loss : 0.149174, loss_ce: 0.059766
2022-01-09 23:17:54,956 iteration 501 : loss : 0.161612, loss_ce: 0.069107
2022-01-09 23:17:56,586 iteration 502 : loss : 0.155366, loss_ce: 0.062467
2022-01-09 23:17:58,124 iteration 503 : loss : 0.176039, loss_ce: 0.063171
2022-01-09 23:17:59,724 iteration 504 : loss : 0.135979, loss_ce: 0.047507
2022-01-09 23:18:01,254 iteration 505 : loss : 0.115829, loss_ce: 0.041690
2022-01-09 23:18:02,915 iteration 506 : loss : 0.128730, loss_ce: 0.056501
2022-01-09 23:18:04,500 iteration 507 : loss : 0.194970, loss_ce: 0.071683
2022-01-09 23:18:06,079 iteration 508 : loss : 0.199968, loss_ce: 0.086137
2022-01-09 23:18:07,700 iteration 509 : loss : 0.135392, loss_ce: 0.051335
2022-01-09 23:18:07,701 Training Data Eval:
2022-01-09 23:18:15,742   Average segmentation loss on training set: 0.1438
2022-01-09 23:18:15,743 Validation Data Eval:
2022-01-09 23:18:18,514   Average segmentation loss on validation set: 0.2166
2022-01-09 23:18:20,126 iteration 510 : loss : 0.130479, loss_ce: 0.057913
  8%|██▎                           | 30/400 [14:55<3:12:19, 31.19s/it]2022-01-09 23:18:21,759 iteration 511 : loss : 0.192300, loss_ce: 0.089184
2022-01-09 23:18:23,289 iteration 512 : loss : 0.198220, loss_ce: 0.088486
2022-01-09 23:18:24,835 iteration 513 : loss : 0.107779, loss_ce: 0.034099
2022-01-09 23:18:26,400 iteration 514 : loss : 0.125436, loss_ce: 0.056116
2022-01-09 23:18:27,952 iteration 515 : loss : 0.179036, loss_ce: 0.075107
2022-01-09 23:18:29,549 iteration 516 : loss : 0.095837, loss_ce: 0.036172
2022-01-09 23:18:31,177 iteration 517 : loss : 0.146782, loss_ce: 0.068624
2022-01-09 23:18:32,702 iteration 518 : loss : 0.140902, loss_ce: 0.052973
2022-01-09 23:18:34,280 iteration 519 : loss : 0.170068, loss_ce: 0.076934
2022-01-09 23:18:35,803 iteration 520 : loss : 0.132428, loss_ce: 0.052387
2022-01-09 23:18:37,372 iteration 521 : loss : 0.149553, loss_ce: 0.054120
2022-01-09 23:18:38,982 iteration 522 : loss : 0.144764, loss_ce: 0.049658
2022-01-09 23:18:40,772 iteration 523 : loss : 0.155735, loss_ce: 0.066276
2022-01-09 23:18:42,437 iteration 524 : loss : 0.135597, loss_ce: 0.048025
2022-01-09 23:18:44,010 iteration 525 : loss : 0.159881, loss_ce: 0.072704
2022-01-09 23:18:45,575 iteration 526 : loss : 0.154816, loss_ce: 0.064197
2022-01-09 23:18:47,146 iteration 527 : loss : 0.138715, loss_ce: 0.048227
  8%|██▎                           | 31/400 [15:22<3:04:07, 29.94s/it]2022-01-09 23:18:48,736 iteration 528 : loss : 0.126830, loss_ce: 0.053413
2022-01-09 23:18:50,321 iteration 529 : loss : 0.150504, loss_ce: 0.045232
2022-01-09 23:18:51,855 iteration 530 : loss : 0.158489, loss_ce: 0.057877
2022-01-09 23:18:53,374 iteration 531 : loss : 0.128433, loss_ce: 0.041969
2022-01-09 23:18:54,910 iteration 532 : loss : 0.135799, loss_ce: 0.066623
2022-01-09 23:18:56,576 iteration 533 : loss : 0.111550, loss_ce: 0.047833
2022-01-09 23:18:58,099 iteration 534 : loss : 0.117155, loss_ce: 0.051512
2022-01-09 23:18:59,714 iteration 535 : loss : 0.108958, loss_ce: 0.045506
2022-01-09 23:19:01,361 iteration 536 : loss : 0.170624, loss_ce: 0.074497
2022-01-09 23:19:03,025 iteration 537 : loss : 0.161506, loss_ce: 0.052748
2022-01-09 23:19:04,624 iteration 538 : loss : 0.139342, loss_ce: 0.061030
2022-01-09 23:19:06,184 iteration 539 : loss : 0.135363, loss_ce: 0.061078
2022-01-09 23:19:07,779 iteration 540 : loss : 0.155624, loss_ce: 0.052691
2022-01-09 23:19:09,334 iteration 541 : loss : 0.125018, loss_ce: 0.051773
2022-01-09 23:19:10,908 iteration 542 : loss : 0.165953, loss_ce: 0.071375
2022-01-09 23:19:12,432 iteration 543 : loss : 0.112018, loss_ce: 0.044685
2022-01-09 23:19:13,987 iteration 544 : loss : 0.133920, loss_ce: 0.053761
  8%|██▍                           | 32/400 [15:49<2:57:55, 29.01s/it]2022-01-09 23:19:15,624 iteration 545 : loss : 0.121402, loss_ce: 0.058355
2022-01-09 23:19:17,201 iteration 546 : loss : 0.136146, loss_ce: 0.050549
2022-01-09 23:19:18,773 iteration 547 : loss : 0.149168, loss_ce: 0.064020
2022-01-09 23:19:20,333 iteration 548 : loss : 0.183279, loss_ce: 0.060270
2022-01-09 23:19:21,928 iteration 549 : loss : 0.148239, loss_ce: 0.065937
2022-01-09 23:19:23,470 iteration 550 : loss : 0.141047, loss_ce: 0.063443
2022-01-09 23:19:25,032 iteration 551 : loss : 0.138435, loss_ce: 0.061408
2022-01-09 23:19:26,575 iteration 552 : loss : 0.139243, loss_ce: 0.054610
2022-01-09 23:19:28,177 iteration 553 : loss : 0.187632, loss_ce: 0.065106
2022-01-09 23:19:29,689 iteration 554 : loss : 0.134733, loss_ce: 0.069591
2022-01-09 23:19:31,248 iteration 555 : loss : 0.129968, loss_ce: 0.038397
2022-01-09 23:19:32,860 iteration 556 : loss : 0.160272, loss_ce: 0.065601
2022-01-09 23:19:34,354 iteration 557 : loss : 0.151182, loss_ce: 0.055780
2022-01-09 23:19:35,958 iteration 558 : loss : 0.170231, loss_ce: 0.057135
2022-01-09 23:19:37,523 iteration 559 : loss : 0.166914, loss_ce: 0.070097
2022-01-09 23:19:39,109 iteration 560 : loss : 0.148334, loss_ce: 0.045120
2022-01-09 23:19:40,718 iteration 561 : loss : 0.215359, loss_ce: 0.115223
  8%|██▍                           | 33/400 [16:15<2:53:16, 28.33s/it]2022-01-09 23:19:42,443 iteration 562 : loss : 0.184289, loss_ce: 0.085177
2022-01-09 23:19:43,929 iteration 563 : loss : 0.135747, loss_ce: 0.060121
2022-01-09 23:19:45,526 iteration 564 : loss : 0.148102, loss_ce: 0.076908
2022-01-09 23:19:47,122 iteration 565 : loss : 0.162336, loss_ce: 0.078137
2022-01-09 23:19:48,749 iteration 566 : loss : 0.139397, loss_ce: 0.065310
2022-01-09 23:19:50,299 iteration 567 : loss : 0.147439, loss_ce: 0.058434
2022-01-09 23:19:51,841 iteration 568 : loss : 0.224195, loss_ce: 0.082679
2022-01-09 23:19:53,409 iteration 569 : loss : 0.178967, loss_ce: 0.077108
2022-01-09 23:19:55,014 iteration 570 : loss : 0.107753, loss_ce: 0.055962
2022-01-09 23:19:56,566 iteration 571 : loss : 0.217386, loss_ce: 0.082367
2022-01-09 23:19:58,155 iteration 572 : loss : 0.124076, loss_ce: 0.043894
2022-01-09 23:19:59,695 iteration 573 : loss : 0.111734, loss_ce: 0.040675
2022-01-09 23:20:01,267 iteration 574 : loss : 0.181807, loss_ce: 0.063433
2022-01-09 23:20:02,900 iteration 575 : loss : 0.161215, loss_ce: 0.053770
2022-01-09 23:20:04,514 iteration 576 : loss : 0.158044, loss_ce: 0.065853
2022-01-09 23:20:06,090 iteration 577 : loss : 0.121532, loss_ce: 0.046620
2022-01-09 23:20:07,621 iteration 578 : loss : 0.198026, loss_ce: 0.086881
  8%|██▌                           | 34/400 [16:42<2:50:10, 27.90s/it]2022-01-09 23:20:09,400 iteration 579 : loss : 0.169209, loss_ce: 0.069049
2022-01-09 23:20:10,995 iteration 580 : loss : 0.160615, loss_ce: 0.066196
2022-01-09 23:20:12,617 iteration 581 : loss : 0.144614, loss_ce: 0.043031
2022-01-09 23:20:14,184 iteration 582 : loss : 0.126248, loss_ce: 0.054944
2022-01-09 23:20:15,764 iteration 583 : loss : 0.214882, loss_ce: 0.069205
2022-01-09 23:20:17,307 iteration 584 : loss : 0.087577, loss_ce: 0.038012
2022-01-09 23:20:18,929 iteration 585 : loss : 0.110795, loss_ce: 0.043003
2022-01-09 23:20:20,464 iteration 586 : loss : 0.173732, loss_ce: 0.065590
2022-01-09 23:20:21,994 iteration 587 : loss : 0.118071, loss_ce: 0.053348
2022-01-09 23:20:23,591 iteration 588 : loss : 0.129874, loss_ce: 0.043008
2022-01-09 23:20:25,152 iteration 589 : loss : 0.123711, loss_ce: 0.042865
2022-01-09 23:20:26,726 iteration 590 : loss : 0.127212, loss_ce: 0.044209
2022-01-09 23:20:28,307 iteration 591 : loss : 0.133945, loss_ce: 0.059441
2022-01-09 23:20:29,885 iteration 592 : loss : 0.127148, loss_ce: 0.048735
2022-01-09 23:20:31,489 iteration 593 : loss : 0.083669, loss_ce: 0.031173
2022-01-09 23:20:33,077 iteration 594 : loss : 0.112685, loss_ce: 0.047128
2022-01-09 23:20:33,078 Training Data Eval:
2022-01-09 23:20:41,132   Average segmentation loss on training set: 0.1115
2022-01-09 23:20:41,132 Validation Data Eval:
2022-01-09 23:20:43,905   Average segmentation loss on validation set: 0.1920
2022-01-09 23:20:50,093 Found new lowest validation loss at iteration 594! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:20:51,617 iteration 595 : loss : 0.162943, loss_ce: 0.088445
  9%|██▋                           | 35/400 [17:26<3:19:05, 32.73s/it]2022-01-09 23:20:53,104 iteration 596 : loss : 0.111988, loss_ce: 0.046127
2022-01-09 23:20:54,624 iteration 597 : loss : 0.158692, loss_ce: 0.057458
2022-01-09 23:20:56,158 iteration 598 : loss : 0.086496, loss_ce: 0.038812
2022-01-09 23:20:57,712 iteration 599 : loss : 0.129328, loss_ce: 0.050216
2022-01-09 23:20:59,267 iteration 600 : loss : 0.139206, loss_ce: 0.062790
2022-01-09 23:21:00,852 iteration 601 : loss : 0.127113, loss_ce: 0.048516
2022-01-09 23:21:02,452 iteration 602 : loss : 0.108812, loss_ce: 0.046177
2022-01-09 23:21:03,959 iteration 603 : loss : 0.155916, loss_ce: 0.062875
2022-01-09 23:21:05,482 iteration 604 : loss : 0.111512, loss_ce: 0.044792
2022-01-09 23:21:06,991 iteration 605 : loss : 0.134310, loss_ce: 0.056740
2022-01-09 23:21:08,690 iteration 606 : loss : 0.113436, loss_ce: 0.050595
2022-01-09 23:21:10,256 iteration 607 : loss : 0.143701, loss_ce: 0.057512
2022-01-09 23:21:11,823 iteration 608 : loss : 0.106850, loss_ce: 0.043653
2022-01-09 23:21:13,315 iteration 609 : loss : 0.110438, loss_ce: 0.045402
2022-01-09 23:21:14,929 iteration 610 : loss : 0.105406, loss_ce: 0.046122
2022-01-09 23:21:16,601 iteration 611 : loss : 0.170154, loss_ce: 0.094282
2022-01-09 23:21:18,197 iteration 612 : loss : 0.122500, loss_ce: 0.047353
  9%|██▋                           | 36/400 [17:53<3:07:22, 30.89s/it]2022-01-09 23:21:19,846 iteration 613 : loss : 0.125353, loss_ce: 0.056939
2022-01-09 23:21:21,384 iteration 614 : loss : 0.140933, loss_ce: 0.051875
2022-01-09 23:21:22,915 iteration 615 : loss : 0.191338, loss_ce: 0.092364
2022-01-09 23:21:24,559 iteration 616 : loss : 0.114663, loss_ce: 0.055427
2022-01-09 23:21:26,097 iteration 617 : loss : 0.135149, loss_ce: 0.069610
2022-01-09 23:21:27,680 iteration 618 : loss : 0.166035, loss_ce: 0.047361
2022-01-09 23:21:29,260 iteration 619 : loss : 0.212301, loss_ce: 0.088690
2022-01-09 23:21:30,933 iteration 620 : loss : 0.181820, loss_ce: 0.055381
2022-01-09 23:21:32,516 iteration 621 : loss : 0.100183, loss_ce: 0.034954
2022-01-09 23:21:34,214 iteration 622 : loss : 0.161741, loss_ce: 0.067246
2022-01-09 23:21:35,890 iteration 623 : loss : 0.184723, loss_ce: 0.089852
2022-01-09 23:21:37,459 iteration 624 : loss : 0.133864, loss_ce: 0.043887
2022-01-09 23:21:38,991 iteration 625 : loss : 0.143220, loss_ce: 0.065652
2022-01-09 23:21:40,614 iteration 626 : loss : 0.106269, loss_ce: 0.043787
2022-01-09 23:21:42,245 iteration 627 : loss : 0.139312, loss_ce: 0.054966
2022-01-09 23:21:43,923 iteration 628 : loss : 0.158815, loss_ce: 0.053492
2022-01-09 23:21:45,491 iteration 629 : loss : 0.098629, loss_ce: 0.041597
  9%|██▊                           | 37/400 [18:20<3:00:19, 29.81s/it]2022-01-09 23:21:47,148 iteration 630 : loss : 0.133247, loss_ce: 0.058584
2022-01-09 23:21:48,705 iteration 631 : loss : 0.109784, loss_ce: 0.050701
2022-01-09 23:21:50,264 iteration 632 : loss : 0.156334, loss_ce: 0.065312
2022-01-09 23:21:51,916 iteration 633 : loss : 0.139499, loss_ce: 0.058696
2022-01-09 23:21:53,506 iteration 634 : loss : 0.201978, loss_ce: 0.085622
2022-01-09 23:21:55,188 iteration 635 : loss : 0.143036, loss_ce: 0.062144
2022-01-09 23:21:56,793 iteration 636 : loss : 0.177787, loss_ce: 0.076459
2022-01-09 23:21:58,308 iteration 637 : loss : 0.167004, loss_ce: 0.080588
2022-01-09 23:21:59,966 iteration 638 : loss : 0.164133, loss_ce: 0.049761
2022-01-09 23:22:01,509 iteration 639 : loss : 0.153411, loss_ce: 0.075757
2022-01-09 23:22:03,025 iteration 640 : loss : 0.199502, loss_ce: 0.057656
2022-01-09 23:22:04,543 iteration 641 : loss : 0.144983, loss_ce: 0.062430
2022-01-09 23:22:06,069 iteration 642 : loss : 0.146331, loss_ce: 0.040120
2022-01-09 23:22:07,626 iteration 643 : loss : 0.154761, loss_ce: 0.064189
2022-01-09 23:22:09,192 iteration 644 : loss : 0.157372, loss_ce: 0.069149
2022-01-09 23:22:10,722 iteration 645 : loss : 0.133611, loss_ce: 0.042704
2022-01-09 23:22:12,353 iteration 646 : loss : 0.161472, loss_ce: 0.078399
 10%|██▊                           | 38/400 [18:47<2:54:29, 28.92s/it]2022-01-09 23:22:13,947 iteration 647 : loss : 0.158039, loss_ce: 0.071898
2022-01-09 23:22:15,521 iteration 648 : loss : 0.119688, loss_ce: 0.053368
2022-01-09 23:22:17,123 iteration 649 : loss : 0.125217, loss_ce: 0.054362
2022-01-09 23:22:18,770 iteration 650 : loss : 0.143006, loss_ce: 0.056713
2022-01-09 23:22:20,345 iteration 651 : loss : 0.093580, loss_ce: 0.039112
2022-01-09 23:22:21,820 iteration 652 : loss : 0.098403, loss_ce: 0.036804
2022-01-09 23:22:23,463 iteration 653 : loss : 0.145354, loss_ce: 0.063542
2022-01-09 23:22:24,990 iteration 654 : loss : 0.111589, loss_ce: 0.044281
2022-01-09 23:22:26,565 iteration 655 : loss : 0.207266, loss_ce: 0.068296
2022-01-09 23:22:28,151 iteration 656 : loss : 0.123791, loss_ce: 0.048389
2022-01-09 23:22:29,801 iteration 657 : loss : 0.111044, loss_ce: 0.040567
2022-01-09 23:22:31,403 iteration 658 : loss : 0.149228, loss_ce: 0.050259
2022-01-09 23:22:33,014 iteration 659 : loss : 0.133018, loss_ce: 0.053103
2022-01-09 23:22:34,566 iteration 660 : loss : 0.095886, loss_ce: 0.041802
2022-01-09 23:22:36,167 iteration 661 : loss : 0.133036, loss_ce: 0.052143
2022-01-09 23:22:37,840 iteration 662 : loss : 0.189148, loss_ce: 0.049174
2022-01-09 23:22:39,427 iteration 663 : loss : 0.120154, loss_ce: 0.054043
 10%|██▉                           | 39/400 [19:14<2:50:40, 28.37s/it]2022-01-09 23:22:41,070 iteration 664 : loss : 0.120830, loss_ce: 0.047717
2022-01-09 23:22:42,575 iteration 665 : loss : 0.127632, loss_ce: 0.056370
2022-01-09 23:22:44,138 iteration 666 : loss : 0.075857, loss_ce: 0.029615
2022-01-09 23:22:45,744 iteration 667 : loss : 0.097242, loss_ce: 0.041364
2022-01-09 23:22:47,326 iteration 668 : loss : 0.162622, loss_ce: 0.063448
2022-01-09 23:22:48,878 iteration 669 : loss : 0.137687, loss_ce: 0.054046
2022-01-09 23:22:50,578 iteration 670 : loss : 0.115861, loss_ce: 0.040935
2022-01-09 23:22:52,138 iteration 671 : loss : 0.147838, loss_ce: 0.053780
2022-01-09 23:22:53,786 iteration 672 : loss : 0.103429, loss_ce: 0.035332
2022-01-09 23:22:55,406 iteration 673 : loss : 0.120210, loss_ce: 0.054477
2022-01-09 23:22:57,011 iteration 674 : loss : 0.112252, loss_ce: 0.051770
2022-01-09 23:22:58,642 iteration 675 : loss : 0.153931, loss_ce: 0.067308
2022-01-09 23:23:00,213 iteration 676 : loss : 0.139999, loss_ce: 0.055620
2022-01-09 23:23:01,801 iteration 677 : loss : 0.107433, loss_ce: 0.053569
2022-01-09 23:23:03,412 iteration 678 : loss : 0.136546, loss_ce: 0.048964
2022-01-09 23:23:05,001 iteration 679 : loss : 0.110684, loss_ce: 0.050669
2022-01-09 23:23:05,001 Training Data Eval:
2022-01-09 23:23:13,077   Average segmentation loss on training set: 0.1808
2022-01-09 23:23:13,078 Validation Data Eval:
2022-01-09 23:23:15,856   Average segmentation loss on validation set: 0.3194
2022-01-09 23:23:17,434 iteration 680 : loss : 0.128644, loss_ce: 0.048649
 10%|███                           | 40/400 [19:52<3:07:35, 31.26s/it]2022-01-09 23:23:19,091 iteration 681 : loss : 0.093911, loss_ce: 0.035408
2022-01-09 23:23:20,595 iteration 682 : loss : 0.157592, loss_ce: 0.048655
2022-01-09 23:23:22,171 iteration 683 : loss : 0.117620, loss_ce: 0.056458
2022-01-09 23:23:23,705 iteration 684 : loss : 0.116753, loss_ce: 0.043486
2022-01-09 23:23:25,341 iteration 685 : loss : 0.117285, loss_ce: 0.051282
2022-01-09 23:23:26,918 iteration 686 : loss : 0.171248, loss_ce: 0.064450
2022-01-09 23:23:28,496 iteration 687 : loss : 0.113156, loss_ce: 0.048731
2022-01-09 23:23:30,050 iteration 688 : loss : 0.116279, loss_ce: 0.042273
2022-01-09 23:23:31,647 iteration 689 : loss : 0.113631, loss_ce: 0.053878
2022-01-09 23:23:33,213 iteration 690 : loss : 0.103753, loss_ce: 0.039391
2022-01-09 23:23:34,796 iteration 691 : loss : 0.117644, loss_ce: 0.046182
2022-01-09 23:23:36,427 iteration 692 : loss : 0.123340, loss_ce: 0.045596
2022-01-09 23:23:37,945 iteration 693 : loss : 0.101294, loss_ce: 0.043558
2022-01-09 23:23:39,533 iteration 694 : loss : 0.127195, loss_ce: 0.053677
2022-01-09 23:23:41,073 iteration 695 : loss : 0.145676, loss_ce: 0.077307
2022-01-09 23:23:42,711 iteration 696 : loss : 0.110923, loss_ce: 0.053936
2022-01-09 23:23:44,291 iteration 697 : loss : 0.110012, loss_ce: 0.045303
 10%|███                           | 41/400 [20:19<2:59:09, 29.94s/it]2022-01-09 23:23:45,895 iteration 698 : loss : 0.172297, loss_ce: 0.058148
2022-01-09 23:23:47,513 iteration 699 : loss : 0.114689, loss_ce: 0.047756
2022-01-09 23:23:49,070 iteration 700 : loss : 0.107798, loss_ce: 0.048761
2022-01-09 23:23:50,625 iteration 701 : loss : 0.140343, loss_ce: 0.066308
2022-01-09 23:23:52,238 iteration 702 : loss : 0.143931, loss_ce: 0.054371
2022-01-09 23:23:53,900 iteration 703 : loss : 0.083793, loss_ce: 0.036798
2022-01-09 23:23:55,418 iteration 704 : loss : 0.089268, loss_ce: 0.034998
2022-01-09 23:23:56,992 iteration 705 : loss : 0.101302, loss_ce: 0.037044
2022-01-09 23:23:58,582 iteration 706 : loss : 0.098640, loss_ce: 0.033808
2022-01-09 23:24:00,094 iteration 707 : loss : 0.101437, loss_ce: 0.041633
2022-01-09 23:24:01,665 iteration 708 : loss : 0.081400, loss_ce: 0.030908
2022-01-09 23:24:03,234 iteration 709 : loss : 0.120117, loss_ce: 0.045941
2022-01-09 23:24:04,841 iteration 710 : loss : 0.104852, loss_ce: 0.038777
2022-01-09 23:24:06,440 iteration 711 : loss : 0.134500, loss_ce: 0.063669
2022-01-09 23:24:08,103 iteration 712 : loss : 0.108589, loss_ce: 0.041517
2022-01-09 23:24:09,669 iteration 713 : loss : 0.104790, loss_ce: 0.048460
2022-01-09 23:24:11,313 iteration 714 : loss : 0.152623, loss_ce: 0.055958
 10%|███▏                          | 42/400 [20:46<2:53:26, 29.07s/it]2022-01-09 23:24:12,994 iteration 715 : loss : 0.127077, loss_ce: 0.051668
2022-01-09 23:24:14,622 iteration 716 : loss : 0.082379, loss_ce: 0.035959
2022-01-09 23:24:16,205 iteration 717 : loss : 0.163163, loss_ce: 0.062812
2022-01-09 23:24:17,801 iteration 718 : loss : 0.114492, loss_ce: 0.048362
2022-01-09 23:24:19,446 iteration 719 : loss : 0.121192, loss_ce: 0.047695
2022-01-09 23:24:21,025 iteration 720 : loss : 0.132290, loss_ce: 0.049970
2022-01-09 23:24:22,602 iteration 721 : loss : 0.112556, loss_ce: 0.039658
2022-01-09 23:24:24,170 iteration 722 : loss : 0.108947, loss_ce: 0.036768
2022-01-09 23:24:25,791 iteration 723 : loss : 0.123318, loss_ce: 0.057181
2022-01-09 23:24:27,348 iteration 724 : loss : 0.164152, loss_ce: 0.085208
2022-01-09 23:24:28,984 iteration 725 : loss : 0.106280, loss_ce: 0.039949
2022-01-09 23:24:30,557 iteration 726 : loss : 0.125633, loss_ce: 0.049979
2022-01-09 23:24:32,067 iteration 727 : loss : 0.086877, loss_ce: 0.032699
2022-01-09 23:24:33,678 iteration 728 : loss : 0.113611, loss_ce: 0.049977
2022-01-09 23:24:35,301 iteration 729 : loss : 0.086053, loss_ce: 0.037756
2022-01-09 23:24:36,896 iteration 730 : loss : 0.109465, loss_ce: 0.043468
2022-01-09 23:24:38,389 iteration 731 : loss : 0.093574, loss_ce: 0.035187
 11%|███▏                          | 43/400 [21:13<2:49:24, 28.47s/it]2022-01-09 23:24:39,964 iteration 732 : loss : 0.124961, loss_ce: 0.056067
2022-01-09 23:24:41,463 iteration 733 : loss : 0.144538, loss_ce: 0.074991
2022-01-09 23:24:43,078 iteration 734 : loss : 0.129872, loss_ce: 0.052484
2022-01-09 23:24:44,636 iteration 735 : loss : 0.123368, loss_ce: 0.048564
2022-01-09 23:24:46,176 iteration 736 : loss : 0.127849, loss_ce: 0.055325
2022-01-09 23:24:47,863 iteration 737 : loss : 0.111202, loss_ce: 0.049749
2022-01-09 23:24:49,358 iteration 738 : loss : 0.110115, loss_ce: 0.052176
2022-01-09 23:24:50,963 iteration 739 : loss : 0.122445, loss_ce: 0.045186
2022-01-09 23:24:52,522 iteration 740 : loss : 0.120942, loss_ce: 0.052420
2022-01-09 23:24:54,063 iteration 741 : loss : 0.121897, loss_ce: 0.048362
2022-01-09 23:24:55,562 iteration 742 : loss : 0.130777, loss_ce: 0.059142
2022-01-09 23:24:57,141 iteration 743 : loss : 0.098655, loss_ce: 0.040262
2022-01-09 23:24:58,746 iteration 744 : loss : 0.132268, loss_ce: 0.049655
2022-01-09 23:25:00,284 iteration 745 : loss : 0.154125, loss_ce: 0.035956
2022-01-09 23:25:01,833 iteration 746 : loss : 0.127931, loss_ce: 0.045132
2022-01-09 23:25:03,373 iteration 747 : loss : 0.129077, loss_ce: 0.054232
2022-01-09 23:25:04,900 iteration 748 : loss : 0.082631, loss_ce: 0.035472
 11%|███▎                          | 44/400 [21:40<2:45:26, 27.88s/it]2022-01-09 23:25:06,474 iteration 749 : loss : 0.230738, loss_ce: 0.046193
2022-01-09 23:25:08,026 iteration 750 : loss : 0.063840, loss_ce: 0.025539
2022-01-09 23:25:09,613 iteration 751 : loss : 0.107781, loss_ce: 0.039238
2022-01-09 23:25:11,165 iteration 752 : loss : 0.174727, loss_ce: 0.090087
2022-01-09 23:25:12,684 iteration 753 : loss : 0.135890, loss_ce: 0.060742
2022-01-09 23:25:14,213 iteration 754 : loss : 0.131147, loss_ce: 0.047680
2022-01-09 23:25:15,693 iteration 755 : loss : 0.138503, loss_ce: 0.065146
2022-01-09 23:25:17,182 iteration 756 : loss : 0.078018, loss_ce: 0.027563
2022-01-09 23:25:18,736 iteration 757 : loss : 0.163695, loss_ce: 0.077024
2022-01-09 23:25:20,387 iteration 758 : loss : 0.127177, loss_ce: 0.059314
2022-01-09 23:25:22,088 iteration 759 : loss : 0.185281, loss_ce: 0.076180
2022-01-09 23:25:23,641 iteration 760 : loss : 0.115813, loss_ce: 0.062049
2022-01-09 23:25:25,238 iteration 761 : loss : 0.116759, loss_ce: 0.052692
2022-01-09 23:25:26,853 iteration 762 : loss : 0.125521, loss_ce: 0.052042
2022-01-09 23:25:28,473 iteration 763 : loss : 0.117016, loss_ce: 0.050761
2022-01-09 23:25:30,022 iteration 764 : loss : 0.122005, loss_ce: 0.054601
2022-01-09 23:25:30,022 Training Data Eval:
2022-01-09 23:25:38,101   Average segmentation loss on training set: 0.1441
2022-01-09 23:25:38,102 Validation Data Eval:
2022-01-09 23:25:40,879   Average segmentation loss on validation set: 0.1481
2022-01-09 23:25:47,086 Found new lowest validation loss at iteration 764! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:25:48,726 iteration 765 : loss : 0.131334, loss_ce: 0.050112
 11%|███▍                          | 45/400 [22:23<3:13:14, 32.66s/it]2022-01-09 23:25:50,265 iteration 766 : loss : 0.147240, loss_ce: 0.067834
2022-01-09 23:25:51,852 iteration 767 : loss : 0.143965, loss_ce: 0.061426
2022-01-09 23:25:53,453 iteration 768 : loss : 0.139272, loss_ce: 0.046053
2022-01-09 23:25:55,069 iteration 769 : loss : 0.134385, loss_ce: 0.053883
2022-01-09 23:25:56,572 iteration 770 : loss : 0.101263, loss_ce: 0.038468
2022-01-09 23:25:58,155 iteration 771 : loss : 0.164427, loss_ce: 0.046603
2022-01-09 23:25:59,748 iteration 772 : loss : 0.106336, loss_ce: 0.040112
2022-01-09 23:26:01,354 iteration 773 : loss : 0.126198, loss_ce: 0.036447
2022-01-09 23:26:03,044 iteration 774 : loss : 0.134807, loss_ce: 0.062433
2022-01-09 23:26:04,602 iteration 775 : loss : 0.128703, loss_ce: 0.059722
2022-01-09 23:26:06,139 iteration 776 : loss : 0.114012, loss_ce: 0.040588
2022-01-09 23:26:07,889 iteration 777 : loss : 0.137047, loss_ce: 0.058919
2022-01-09 23:26:09,399 iteration 778 : loss : 0.140143, loss_ce: 0.043811
2022-01-09 23:26:11,033 iteration 779 : loss : 0.101878, loss_ce: 0.045493
2022-01-09 23:26:12,671 iteration 780 : loss : 0.122779, loss_ce: 0.046393
2022-01-09 23:26:14,211 iteration 781 : loss : 0.108522, loss_ce: 0.057481
2022-01-09 23:26:15,709 iteration 782 : loss : 0.092995, loss_ce: 0.048261
 12%|███▍                          | 46/400 [22:50<3:02:38, 30.96s/it]2022-01-09 23:26:17,313 iteration 783 : loss : 0.140636, loss_ce: 0.065478
2022-01-09 23:26:18,854 iteration 784 : loss : 0.121504, loss_ce: 0.046012
2022-01-09 23:26:20,437 iteration 785 : loss : 0.140951, loss_ce: 0.076966
2022-01-09 23:26:21,991 iteration 786 : loss : 0.143940, loss_ce: 0.058234
2022-01-09 23:26:23,556 iteration 787 : loss : 0.124364, loss_ce: 0.061253
2022-01-09 23:26:25,190 iteration 788 : loss : 0.170822, loss_ce: 0.068446
2022-01-09 23:26:26,684 iteration 789 : loss : 0.113153, loss_ce: 0.042815
2022-01-09 23:26:28,304 iteration 790 : loss : 0.120421, loss_ce: 0.041637
2022-01-09 23:26:29,877 iteration 791 : loss : 0.082561, loss_ce: 0.029878
2022-01-09 23:26:31,393 iteration 792 : loss : 0.089889, loss_ce: 0.036860
2022-01-09 23:26:33,029 iteration 793 : loss : 0.115003, loss_ce: 0.054798
2022-01-09 23:26:34,517 iteration 794 : loss : 0.075813, loss_ce: 0.029872
2022-01-09 23:26:36,027 iteration 795 : loss : 0.108912, loss_ce: 0.041652
2022-01-09 23:26:37,541 iteration 796 : loss : 0.097033, loss_ce: 0.043662
2022-01-09 23:26:39,151 iteration 797 : loss : 0.181190, loss_ce: 0.088327
2022-01-09 23:26:40,701 iteration 798 : loss : 0.096279, loss_ce: 0.034624
2022-01-09 23:26:42,201 iteration 799 : loss : 0.078079, loss_ce: 0.037136
 12%|███▌                          | 47/400 [23:17<2:54:15, 29.62s/it]2022-01-09 23:26:43,912 iteration 800 : loss : 0.091721, loss_ce: 0.032257
2022-01-09 23:26:45,504 iteration 801 : loss : 0.088103, loss_ce: 0.038029
2022-01-09 23:26:47,080 iteration 802 : loss : 0.106899, loss_ce: 0.038234
2022-01-09 23:26:48,676 iteration 803 : loss : 0.099284, loss_ce: 0.052940
2022-01-09 23:26:50,338 iteration 804 : loss : 0.192331, loss_ce: 0.093555
2022-01-09 23:26:52,064 iteration 805 : loss : 0.100104, loss_ce: 0.037532
2022-01-09 23:26:53,624 iteration 806 : loss : 0.103820, loss_ce: 0.046745
2022-01-09 23:26:55,147 iteration 807 : loss : 0.081792, loss_ce: 0.031506
2022-01-09 23:26:56,757 iteration 808 : loss : 0.144713, loss_ce: 0.071099
2022-01-09 23:26:58,343 iteration 809 : loss : 0.101400, loss_ce: 0.038877
2022-01-09 23:26:59,885 iteration 810 : loss : 0.143811, loss_ce: 0.055243
2022-01-09 23:27:01,398 iteration 811 : loss : 0.121369, loss_ce: 0.068989
2022-01-09 23:27:02,942 iteration 812 : loss : 0.099535, loss_ce: 0.044548
2022-01-09 23:27:04,511 iteration 813 : loss : 0.112592, loss_ce: 0.041389
2022-01-09 23:27:06,194 iteration 814 : loss : 0.133910, loss_ce: 0.042923
2022-01-09 23:27:07,833 iteration 815 : loss : 0.112388, loss_ce: 0.046962
2022-01-09 23:27:09,426 iteration 816 : loss : 0.109121, loss_ce: 0.039356
 12%|███▌                          | 48/400 [23:44<2:49:34, 28.90s/it]2022-01-09 23:27:11,050 iteration 817 : loss : 0.128988, loss_ce: 0.059900
2022-01-09 23:27:12,570 iteration 818 : loss : 0.085113, loss_ce: 0.035387
2022-01-09 23:27:14,140 iteration 819 : loss : 0.074744, loss_ce: 0.027982
2022-01-09 23:27:15,640 iteration 820 : loss : 0.093806, loss_ce: 0.035218
2022-01-09 23:27:17,268 iteration 821 : loss : 0.080872, loss_ce: 0.029654
2022-01-09 23:27:18,874 iteration 822 : loss : 0.130845, loss_ce: 0.051801
2022-01-09 23:27:20,394 iteration 823 : loss : 0.088259, loss_ce: 0.034212
2022-01-09 23:27:21,911 iteration 824 : loss : 0.090386, loss_ce: 0.043129
2022-01-09 23:27:23,471 iteration 825 : loss : 0.086141, loss_ce: 0.034934
2022-01-09 23:27:25,023 iteration 826 : loss : 0.126854, loss_ce: 0.036610
2022-01-09 23:27:26,583 iteration 827 : loss : 0.132573, loss_ce: 0.045748
2022-01-09 23:27:28,142 iteration 828 : loss : 0.086325, loss_ce: 0.032065
2022-01-09 23:27:29,730 iteration 829 : loss : 0.126341, loss_ce: 0.045970
2022-01-09 23:27:31,358 iteration 830 : loss : 0.088173, loss_ce: 0.032489
2022-01-09 23:27:32,928 iteration 831 : loss : 0.135240, loss_ce: 0.063147
2022-01-09 23:27:34,476 iteration 832 : loss : 0.078058, loss_ce: 0.028081
2022-01-09 23:27:36,097 iteration 833 : loss : 0.122256, loss_ce: 0.059852
 12%|███▋                          | 49/400 [24:11<2:45:10, 28.24s/it]2022-01-09 23:27:37,729 iteration 834 : loss : 0.088004, loss_ce: 0.036962
2022-01-09 23:27:39,212 iteration 835 : loss : 0.097302, loss_ce: 0.028862
2022-01-09 23:27:40,790 iteration 836 : loss : 0.081648, loss_ce: 0.041578
2022-01-09 23:27:42,306 iteration 837 : loss : 0.079209, loss_ce: 0.043943
2022-01-09 23:27:43,800 iteration 838 : loss : 0.096414, loss_ce: 0.043048
2022-01-09 23:27:45,384 iteration 839 : loss : 0.105703, loss_ce: 0.047445
2022-01-09 23:27:46,940 iteration 840 : loss : 0.115222, loss_ce: 0.040320
2022-01-09 23:27:48,459 iteration 841 : loss : 0.128837, loss_ce: 0.056613
2022-01-09 23:27:50,007 iteration 842 : loss : 0.118193, loss_ce: 0.046799
2022-01-09 23:27:51,659 iteration 843 : loss : 0.100932, loss_ce: 0.038504
2022-01-09 23:27:53,212 iteration 844 : loss : 0.102642, loss_ce: 0.036918
2022-01-09 23:27:54,805 iteration 845 : loss : 0.121576, loss_ce: 0.038964
2022-01-09 23:27:56,492 iteration 846 : loss : 0.117226, loss_ce: 0.057110
2022-01-09 23:27:58,078 iteration 847 : loss : 0.090195, loss_ce: 0.035844
2022-01-09 23:27:59,560 iteration 848 : loss : 0.087557, loss_ce: 0.037400
2022-01-09 23:28:01,144 iteration 849 : loss : 0.109767, loss_ce: 0.043455
2022-01-09 23:28:01,144 Training Data Eval:
2022-01-09 23:28:09,223   Average segmentation loss on training set: 0.0851
2022-01-09 23:28:09,223 Validation Data Eval:
2022-01-09 23:28:11,996   Average segmentation loss on validation set: 0.1004
2022-01-09 23:28:17,493 Found new lowest validation loss at iteration 849! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:28:19,003 iteration 850 : loss : 0.117477, loss_ce: 0.045151
 12%|███▊                          | 50/400 [24:54<3:10:20, 32.63s/it]2022-01-09 23:28:20,547 iteration 851 : loss : 0.094067, loss_ce: 0.041817
2022-01-09 23:28:22,093 iteration 852 : loss : 0.089814, loss_ce: 0.035816
2022-01-09 23:28:23,736 iteration 853 : loss : 0.093369, loss_ce: 0.036343
2022-01-09 23:28:25,334 iteration 854 : loss : 0.080652, loss_ce: 0.025968
2022-01-09 23:28:26,923 iteration 855 : loss : 0.109138, loss_ce: 0.046557
2022-01-09 23:28:28,532 iteration 856 : loss : 0.095728, loss_ce: 0.035917
2022-01-09 23:28:30,076 iteration 857 : loss : 0.086993, loss_ce: 0.038375
2022-01-09 23:28:31,558 iteration 858 : loss : 0.079662, loss_ce: 0.039707
2022-01-09 23:28:33,110 iteration 859 : loss : 0.087246, loss_ce: 0.032474
2022-01-09 23:28:34,619 iteration 860 : loss : 0.110298, loss_ce: 0.035163
2022-01-09 23:28:36,248 iteration 861 : loss : 0.110632, loss_ce: 0.047103
2022-01-09 23:28:37,744 iteration 862 : loss : 0.097875, loss_ce: 0.031522
2022-01-09 23:28:39,383 iteration 863 : loss : 0.122784, loss_ce: 0.062996
2022-01-09 23:28:40,984 iteration 864 : loss : 0.098971, loss_ce: 0.043389
2022-01-09 23:28:42,475 iteration 865 : loss : 0.097394, loss_ce: 0.038186
2022-01-09 23:28:44,101 iteration 866 : loss : 0.122962, loss_ce: 0.060337
2022-01-09 23:28:45,606 iteration 867 : loss : 0.056307, loss_ce: 0.019665
 13%|███▊                          | 51/400 [25:20<2:59:18, 30.83s/it]2022-01-09 23:28:47,264 iteration 868 : loss : 0.144341, loss_ce: 0.046297
2022-01-09 23:28:48,833 iteration 869 : loss : 0.088345, loss_ce: 0.036312
2022-01-09 23:28:50,312 iteration 870 : loss : 0.068711, loss_ce: 0.025200
2022-01-09 23:28:51,828 iteration 871 : loss : 0.071063, loss_ce: 0.026997
2022-01-09 23:28:53,364 iteration 872 : loss : 0.074743, loss_ce: 0.037700
2022-01-09 23:28:55,001 iteration 873 : loss : 0.083589, loss_ce: 0.029219
2022-01-09 23:28:56,487 iteration 874 : loss : 0.064746, loss_ce: 0.028579
2022-01-09 23:28:58,194 iteration 875 : loss : 0.124198, loss_ce: 0.048366
2022-01-09 23:28:59,762 iteration 876 : loss : 0.055830, loss_ce: 0.026064
2022-01-09 23:29:01,234 iteration 877 : loss : 0.083894, loss_ce: 0.034147
2022-01-09 23:29:02,849 iteration 878 : loss : 0.087710, loss_ce: 0.039442
2022-01-09 23:29:04,392 iteration 879 : loss : 0.093717, loss_ce: 0.040946
2022-01-09 23:29:05,983 iteration 880 : loss : 0.078179, loss_ce: 0.027982
2022-01-09 23:29:07,520 iteration 881 : loss : 0.114450, loss_ce: 0.049526
2022-01-09 23:29:09,128 iteration 882 : loss : 0.097131, loss_ce: 0.037975
2022-01-09 23:29:10,693 iteration 883 : loss : 0.088158, loss_ce: 0.039997
2022-01-09 23:29:12,350 iteration 884 : loss : 0.123139, loss_ce: 0.058379
 13%|███▉                          | 52/400 [25:47<2:51:41, 29.60s/it]2022-01-09 23:29:14,008 iteration 885 : loss : 0.132491, loss_ce: 0.054671
2022-01-09 23:29:15,610 iteration 886 : loss : 0.097571, loss_ce: 0.037060
2022-01-09 23:29:17,220 iteration 887 : loss : 0.136373, loss_ce: 0.058856
2022-01-09 23:29:18,799 iteration 888 : loss : 0.068474, loss_ce: 0.027103
2022-01-09 23:29:20,436 iteration 889 : loss : 0.103157, loss_ce: 0.035394
2022-01-09 23:29:22,120 iteration 890 : loss : 0.119287, loss_ce: 0.041982
2022-01-09 23:29:23,704 iteration 891 : loss : 0.072425, loss_ce: 0.027815
2022-01-09 23:29:25,281 iteration 892 : loss : 0.083182, loss_ce: 0.038648
2022-01-09 23:29:26,838 iteration 893 : loss : 0.103859, loss_ce: 0.035748
2022-01-09 23:29:28,515 iteration 894 : loss : 0.104400, loss_ce: 0.040851
2022-01-09 23:29:30,057 iteration 895 : loss : 0.116940, loss_ce: 0.038316
2022-01-09 23:29:31,691 iteration 896 : loss : 0.111123, loss_ce: 0.043359
2022-01-09 23:29:33,350 iteration 897 : loss : 0.118224, loss_ce: 0.059602
2022-01-09 23:29:34,908 iteration 898 : loss : 0.097792, loss_ce: 0.050914
2022-01-09 23:29:36,466 iteration 899 : loss : 0.084099, loss_ce: 0.037257
2022-01-09 23:29:37,985 iteration 900 : loss : 0.080988, loss_ce: 0.037791
2022-01-09 23:29:39,526 iteration 901 : loss : 0.136331, loss_ce: 0.051731
 13%|███▉                          | 53/400 [26:14<2:46:58, 28.87s/it]2022-01-09 23:29:41,185 iteration 902 : loss : 0.080947, loss_ce: 0.039712
2022-01-09 23:29:42,850 iteration 903 : loss : 0.077265, loss_ce: 0.027801
2022-01-09 23:29:44,460 iteration 904 : loss : 0.111368, loss_ce: 0.038400
2022-01-09 23:29:46,053 iteration 905 : loss : 0.127466, loss_ce: 0.064649
2022-01-09 23:29:47,622 iteration 906 : loss : 0.064835, loss_ce: 0.028878
2022-01-09 23:29:49,257 iteration 907 : loss : 0.103453, loss_ce: 0.034230
2022-01-09 23:29:50,736 iteration 908 : loss : 0.095981, loss_ce: 0.042014
2022-01-09 23:29:52,332 iteration 909 : loss : 0.117333, loss_ce: 0.056038
2022-01-09 23:29:53,838 iteration 910 : loss : 0.140828, loss_ce: 0.050046
2022-01-09 23:29:55,403 iteration 911 : loss : 0.075257, loss_ce: 0.032415
2022-01-09 23:29:56,961 iteration 912 : loss : 0.075675, loss_ce: 0.028919
2022-01-09 23:29:58,508 iteration 913 : loss : 0.119580, loss_ce: 0.041429
2022-01-09 23:30:00,066 iteration 914 : loss : 0.118644, loss_ce: 0.050467
2022-01-09 23:30:01,601 iteration 915 : loss : 0.197604, loss_ce: 0.055932
2022-01-09 23:30:03,138 iteration 916 : loss : 0.070144, loss_ce: 0.026216
2022-01-09 23:30:04,736 iteration 917 : loss : 0.109359, loss_ce: 0.044210
2022-01-09 23:30:06,347 iteration 918 : loss : 0.098997, loss_ce: 0.036456
 14%|████                          | 54/400 [26:41<2:42:56, 28.26s/it]2022-01-09 23:30:08,022 iteration 919 : loss : 0.069300, loss_ce: 0.031204
2022-01-09 23:30:09,617 iteration 920 : loss : 0.087238, loss_ce: 0.035937
2022-01-09 23:30:11,238 iteration 921 : loss : 0.091743, loss_ce: 0.041527
2022-01-09 23:30:12,874 iteration 922 : loss : 0.097870, loss_ce: 0.049931
2022-01-09 23:30:14,426 iteration 923 : loss : 0.077013, loss_ce: 0.035789
2022-01-09 23:30:16,049 iteration 924 : loss : 0.149563, loss_ce: 0.050915
2022-01-09 23:30:17,586 iteration 925 : loss : 0.078093, loss_ce: 0.030281
2022-01-09 23:30:19,209 iteration 926 : loss : 0.091861, loss_ce: 0.041725
2022-01-09 23:30:20,714 iteration 927 : loss : 0.115325, loss_ce: 0.045751
2022-01-09 23:30:22,275 iteration 928 : loss : 0.088808, loss_ce: 0.041777
2022-01-09 23:30:23,899 iteration 929 : loss : 0.097820, loss_ce: 0.037566
2022-01-09 23:30:25,490 iteration 930 : loss : 0.089715, loss_ce: 0.036391
2022-01-09 23:30:27,083 iteration 931 : loss : 0.153619, loss_ce: 0.035630
2022-01-09 23:30:28,660 iteration 932 : loss : 0.097326, loss_ce: 0.037649
2022-01-09 23:30:30,206 iteration 933 : loss : 0.101180, loss_ce: 0.040648
2022-01-09 23:30:31,779 iteration 934 : loss : 0.085186, loss_ce: 0.038926
2022-01-09 23:30:31,779 Training Data Eval:
2022-01-09 23:30:39,857   Average segmentation loss on training set: 0.0967
2022-01-09 23:30:39,857 Validation Data Eval:
2022-01-09 23:30:42,635   Average segmentation loss on validation set: 0.1816
2022-01-09 23:30:44,299 iteration 935 : loss : 0.152397, loss_ce: 0.050175
 14%|████▏                         | 55/400 [27:19<2:59:11, 31.17s/it]2022-01-09 23:30:45,909 iteration 936 : loss : 0.082227, loss_ce: 0.026621
2022-01-09 23:30:47,503 iteration 937 : loss : 0.091604, loss_ce: 0.028248
2022-01-09 23:30:49,123 iteration 938 : loss : 0.087649, loss_ce: 0.032649
2022-01-09 23:30:50,655 iteration 939 : loss : 0.076922, loss_ce: 0.026042
2022-01-09 23:30:52,246 iteration 940 : loss : 0.106709, loss_ce: 0.045944
2022-01-09 23:30:53,925 iteration 941 : loss : 0.145437, loss_ce: 0.057857
2022-01-09 23:30:55,459 iteration 942 : loss : 0.075689, loss_ce: 0.029525
2022-01-09 23:30:57,014 iteration 943 : loss : 0.090510, loss_ce: 0.048166
2022-01-09 23:30:58,543 iteration 944 : loss : 0.080401, loss_ce: 0.030220
2022-01-09 23:31:00,087 iteration 945 : loss : 0.096100, loss_ce: 0.040276
2022-01-09 23:31:01,626 iteration 946 : loss : 0.077683, loss_ce: 0.032580
2022-01-09 23:31:03,155 iteration 947 : loss : 0.096863, loss_ce: 0.035302
2022-01-09 23:31:04,680 iteration 948 : loss : 0.107523, loss_ce: 0.041171
2022-01-09 23:31:06,236 iteration 949 : loss : 0.091205, loss_ce: 0.033833
2022-01-09 23:31:07,782 iteration 950 : loss : 0.112541, loss_ce: 0.051553
2022-01-09 23:31:09,385 iteration 951 : loss : 0.092200, loss_ce: 0.057853
2022-01-09 23:31:10,903 iteration 952 : loss : 0.101802, loss_ce: 0.048974
 14%|████▏                         | 56/400 [27:45<2:50:49, 29.80s/it]2022-01-09 23:31:12,548 iteration 953 : loss : 0.092317, loss_ce: 0.034552
2022-01-09 23:31:14,166 iteration 954 : loss : 0.085641, loss_ce: 0.034746
2022-01-09 23:31:15,665 iteration 955 : loss : 0.063573, loss_ce: 0.026614
2022-01-09 23:31:17,267 iteration 956 : loss : 0.086228, loss_ce: 0.032411
2022-01-09 23:31:18,853 iteration 957 : loss : 0.076480, loss_ce: 0.026116
2022-01-09 23:31:20,454 iteration 958 : loss : 0.108075, loss_ce: 0.031923
2022-01-09 23:31:22,124 iteration 959 : loss : 0.153080, loss_ce: 0.069082
2022-01-09 23:31:23,851 iteration 960 : loss : 0.088641, loss_ce: 0.036432
2022-01-09 23:31:25,391 iteration 961 : loss : 0.140761, loss_ce: 0.044341
2022-01-09 23:31:26,902 iteration 962 : loss : 0.105241, loss_ce: 0.032426
2022-01-09 23:31:28,547 iteration 963 : loss : 0.108015, loss_ce: 0.042630
2022-01-09 23:31:30,159 iteration 964 : loss : 0.095238, loss_ce: 0.049930
2022-01-09 23:31:31,710 iteration 965 : loss : 0.072044, loss_ce: 0.032288
2022-01-09 23:31:33,285 iteration 966 : loss : 0.075900, loss_ce: 0.032326
2022-01-09 23:31:34,877 iteration 967 : loss : 0.069015, loss_ce: 0.027013
2022-01-09 23:31:36,404 iteration 968 : loss : 0.132110, loss_ce: 0.050729
2022-01-09 23:31:37,997 iteration 969 : loss : 0.072623, loss_ce: 0.029743
 14%|████▎                         | 57/400 [28:13<2:45:42, 28.99s/it]2022-01-09 23:31:39,608 iteration 970 : loss : 0.091693, loss_ce: 0.047155
2022-01-09 23:31:41,207 iteration 971 : loss : 0.121348, loss_ce: 0.041466
2022-01-09 23:31:42,718 iteration 972 : loss : 0.086814, loss_ce: 0.039764
2022-01-09 23:31:44,308 iteration 973 : loss : 0.081253, loss_ce: 0.027421
2022-01-09 23:31:45,826 iteration 974 : loss : 0.084109, loss_ce: 0.038073
2022-01-09 23:31:47,447 iteration 975 : loss : 0.111313, loss_ce: 0.040441
2022-01-09 23:31:49,038 iteration 976 : loss : 0.076478, loss_ce: 0.033373
2022-01-09 23:31:50,600 iteration 977 : loss : 0.074858, loss_ce: 0.027486
2022-01-09 23:31:52,193 iteration 978 : loss : 0.058714, loss_ce: 0.021777
2022-01-09 23:31:53,740 iteration 979 : loss : 0.137133, loss_ce: 0.070211
2022-01-09 23:31:55,288 iteration 980 : loss : 0.082142, loss_ce: 0.032101
2022-01-09 23:31:56,956 iteration 981 : loss : 0.077708, loss_ce: 0.035292
2022-01-09 23:31:58,503 iteration 982 : loss : 0.102663, loss_ce: 0.052420
2022-01-09 23:32:00,124 iteration 983 : loss : 0.142705, loss_ce: 0.048074
2022-01-09 23:32:01,623 iteration 984 : loss : 0.084332, loss_ce: 0.039952
2022-01-09 23:32:03,195 iteration 985 : loss : 0.078157, loss_ce: 0.027326
2022-01-09 23:32:04,819 iteration 986 : loss : 0.133014, loss_ce: 0.057994
 14%|████▎                         | 58/400 [28:39<2:41:32, 28.34s/it]2022-01-09 23:32:06,518 iteration 987 : loss : 0.070788, loss_ce: 0.032741
2022-01-09 23:32:08,120 iteration 988 : loss : 0.090263, loss_ce: 0.042557
2022-01-09 23:32:09,707 iteration 989 : loss : 0.084974, loss_ce: 0.032345
2022-01-09 23:32:11,280 iteration 990 : loss : 0.095679, loss_ce: 0.043887
2022-01-09 23:32:12,846 iteration 991 : loss : 0.082476, loss_ce: 0.035941
2022-01-09 23:32:14,499 iteration 992 : loss : 0.072816, loss_ce: 0.037059
2022-01-09 23:32:16,104 iteration 993 : loss : 0.078144, loss_ce: 0.027998
2022-01-09 23:32:17,789 iteration 994 : loss : 0.124815, loss_ce: 0.062858
2022-01-09 23:32:19,381 iteration 995 : loss : 0.118347, loss_ce: 0.040543
2022-01-09 23:32:21,021 iteration 996 : loss : 0.049569, loss_ce: 0.023540
2022-01-09 23:32:22,658 iteration 997 : loss : 0.101369, loss_ce: 0.041544
2022-01-09 23:32:24,188 iteration 998 : loss : 0.117066, loss_ce: 0.041989
2022-01-09 23:32:25,815 iteration 999 : loss : 0.091183, loss_ce: 0.039344
2022-01-09 23:32:27,360 iteration 1000 : loss : 0.131054, loss_ce: 0.049766
2022-01-09 23:32:28,910 iteration 1001 : loss : 0.136961, loss_ce: 0.037915
2022-01-09 23:32:30,581 iteration 1002 : loss : 0.090225, loss_ce: 0.037122
2022-01-09 23:32:32,239 iteration 1003 : loss : 0.103111, loss_ce: 0.042220
 15%|████▍                         | 59/400 [29:07<2:39:29, 28.06s/it]2022-01-09 23:32:33,885 iteration 1004 : loss : 0.141916, loss_ce: 0.063651
2022-01-09 23:32:35,453 iteration 1005 : loss : 0.086077, loss_ce: 0.033823
2022-01-09 23:32:37,114 iteration 1006 : loss : 0.121681, loss_ce: 0.054346
2022-01-09 23:32:38,654 iteration 1007 : loss : 0.121233, loss_ce: 0.060886
2022-01-09 23:32:40,180 iteration 1008 : loss : 0.211017, loss_ce: 0.107663
2022-01-09 23:32:41,712 iteration 1009 : loss : 0.111354, loss_ce: 0.049062
2022-01-09 23:32:43,246 iteration 1010 : loss : 0.129119, loss_ce: 0.049127
2022-01-09 23:32:44,871 iteration 1011 : loss : 0.099491, loss_ce: 0.042513
2022-01-09 23:32:46,432 iteration 1012 : loss : 0.064447, loss_ce: 0.024982
2022-01-09 23:32:47,949 iteration 1013 : loss : 0.078824, loss_ce: 0.025514
2022-01-09 23:32:49,531 iteration 1014 : loss : 0.150873, loss_ce: 0.074020
2022-01-09 23:32:51,038 iteration 1015 : loss : 0.083727, loss_ce: 0.035895
2022-01-09 23:32:52,623 iteration 1016 : loss : 0.104875, loss_ce: 0.044095
2022-01-09 23:32:54,106 iteration 1017 : loss : 0.090063, loss_ce: 0.035807
2022-01-09 23:32:55,586 iteration 1018 : loss : 0.124663, loss_ce: 0.052246
2022-01-09 23:32:57,141 iteration 1019 : loss : 0.107695, loss_ce: 0.046010
2022-01-09 23:32:57,142 Training Data Eval:
2022-01-09 23:33:05,219   Average segmentation loss on training set: 0.1856
2022-01-09 23:33:05,219 Validation Data Eval:
2022-01-09 23:33:07,994   Average segmentation loss on validation set: 0.2269
2022-01-09 23:33:09,511 iteration 1020 : loss : 0.082497, loss_ce: 0.030213
 15%|████▌                         | 60/400 [29:44<2:54:40, 30.83s/it]2022-01-09 23:33:11,216 iteration 1021 : loss : 0.122384, loss_ce: 0.033239
2022-01-09 23:33:12,779 iteration 1022 : loss : 0.093657, loss_ce: 0.043954
2022-01-09 23:33:14,354 iteration 1023 : loss : 0.148196, loss_ce: 0.043449
2022-01-09 23:33:15,985 iteration 1024 : loss : 0.099157, loss_ce: 0.042263
2022-01-09 23:33:17,605 iteration 1025 : loss : 0.064179, loss_ce: 0.029351
2022-01-09 23:33:19,089 iteration 1026 : loss : 0.096676, loss_ce: 0.043683
2022-01-09 23:33:20,705 iteration 1027 : loss : 0.125180, loss_ce: 0.043034
2022-01-09 23:33:22,315 iteration 1028 : loss : 0.150778, loss_ce: 0.066118
2022-01-09 23:33:23,876 iteration 1029 : loss : 0.072179, loss_ce: 0.029796
2022-01-09 23:33:25,447 iteration 1030 : loss : 0.100786, loss_ce: 0.039098
2022-01-09 23:33:27,077 iteration 1031 : loss : 0.144110, loss_ce: 0.083298
2022-01-09 23:33:28,675 iteration 1032 : loss : 0.133369, loss_ce: 0.065890
2022-01-09 23:33:30,222 iteration 1033 : loss : 0.123330, loss_ce: 0.042100
2022-01-09 23:33:31,863 iteration 1034 : loss : 0.126291, loss_ce: 0.047893
2022-01-09 23:33:33,454 iteration 1035 : loss : 0.096249, loss_ce: 0.040134
2022-01-09 23:33:35,050 iteration 1036 : loss : 0.089553, loss_ce: 0.041444
2022-01-09 23:33:36,591 iteration 1037 : loss : 0.115494, loss_ce: 0.046902
 15%|████▌                         | 61/400 [30:11<2:47:47, 29.70s/it]2022-01-09 23:33:38,237 iteration 1038 : loss : 0.066261, loss_ce: 0.027827
2022-01-09 23:33:39,812 iteration 1039 : loss : 0.097754, loss_ce: 0.044909
2022-01-09 23:33:41,426 iteration 1040 : loss : 0.123129, loss_ce: 0.059603
2022-01-09 23:33:43,022 iteration 1041 : loss : 0.084708, loss_ce: 0.037927
2022-01-09 23:33:44,626 iteration 1042 : loss : 0.079965, loss_ce: 0.036457
2022-01-09 23:33:46,211 iteration 1043 : loss : 0.148139, loss_ce: 0.054853
2022-01-09 23:33:47,819 iteration 1044 : loss : 0.078667, loss_ce: 0.031135
2022-01-09 23:33:49,313 iteration 1045 : loss : 0.085308, loss_ce: 0.040002
2022-01-09 23:33:50,836 iteration 1046 : loss : 0.149042, loss_ce: 0.041115
2022-01-09 23:33:52,455 iteration 1047 : loss : 0.128431, loss_ce: 0.051956
2022-01-09 23:33:54,032 iteration 1048 : loss : 0.109560, loss_ce: 0.038658
2022-01-09 23:33:55,613 iteration 1049 : loss : 0.121076, loss_ce: 0.052396
2022-01-09 23:33:57,241 iteration 1050 : loss : 0.104151, loss_ce: 0.032013
2022-01-09 23:33:58,811 iteration 1051 : loss : 0.107671, loss_ce: 0.042226
2022-01-09 23:34:00,385 iteration 1052 : loss : 0.206521, loss_ce: 0.091601
2022-01-09 23:34:01,888 iteration 1053 : loss : 0.144801, loss_ce: 0.051632
2022-01-09 23:34:03,533 iteration 1054 : loss : 0.099373, loss_ce: 0.050576
 16%|████▋                         | 62/400 [30:38<2:42:39, 28.88s/it]2022-01-09 23:34:05,173 iteration 1055 : loss : 0.068125, loss_ce: 0.022135
2022-01-09 23:34:06,720 iteration 1056 : loss : 0.156252, loss_ce: 0.079864
2022-01-09 23:34:08,271 iteration 1057 : loss : 0.143281, loss_ce: 0.075122
2022-01-09 23:34:09,837 iteration 1058 : loss : 0.081807, loss_ce: 0.037466
2022-01-09 23:34:11,328 iteration 1059 : loss : 0.100289, loss_ce: 0.038941
2022-01-09 23:34:12,888 iteration 1060 : loss : 0.083479, loss_ce: 0.030806
2022-01-09 23:34:14,485 iteration 1061 : loss : 0.115097, loss_ce: 0.055727
2022-01-09 23:34:16,072 iteration 1062 : loss : 0.107619, loss_ce: 0.044534
2022-01-09 23:34:17,711 iteration 1063 : loss : 0.136962, loss_ce: 0.044866
2022-01-09 23:34:19,278 iteration 1064 : loss : 0.131923, loss_ce: 0.046291
2022-01-09 23:34:20,789 iteration 1065 : loss : 0.085147, loss_ce: 0.035273
2022-01-09 23:34:22,399 iteration 1066 : loss : 0.098291, loss_ce: 0.043559
2022-01-09 23:34:23,955 iteration 1067 : loss : 0.182670, loss_ce: 0.050957
2022-01-09 23:34:25,576 iteration 1068 : loss : 0.093361, loss_ce: 0.041611
2022-01-09 23:34:27,174 iteration 1069 : loss : 0.113407, loss_ce: 0.049902
2022-01-09 23:34:28,744 iteration 1070 : loss : 0.069183, loss_ce: 0.030111
2022-01-09 23:34:30,303 iteration 1071 : loss : 0.079043, loss_ce: 0.037209
 16%|████▋                         | 63/400 [31:05<2:38:37, 28.24s/it]2022-01-09 23:34:32,029 iteration 1072 : loss : 0.098782, loss_ce: 0.036618
2022-01-09 23:34:33,585 iteration 1073 : loss : 0.216227, loss_ce: 0.065801
2022-01-09 23:34:35,146 iteration 1074 : loss : 0.099254, loss_ce: 0.044638
2022-01-09 23:34:36,749 iteration 1075 : loss : 0.101097, loss_ce: 0.040388
2022-01-09 23:34:38,266 iteration 1076 : loss : 0.071085, loss_ce: 0.027197
2022-01-09 23:34:39,825 iteration 1077 : loss : 0.101825, loss_ce: 0.036698
2022-01-09 23:34:41,465 iteration 1078 : loss : 0.139439, loss_ce: 0.067911
2022-01-09 23:34:43,000 iteration 1079 : loss : 0.073923, loss_ce: 0.027733
2022-01-09 23:34:44,522 iteration 1080 : loss : 0.066444, loss_ce: 0.029090
2022-01-09 23:34:46,117 iteration 1081 : loss : 0.086709, loss_ce: 0.030436
2022-01-09 23:34:47,712 iteration 1082 : loss : 0.081094, loss_ce: 0.042042
2022-01-09 23:34:49,336 iteration 1083 : loss : 0.100495, loss_ce: 0.048017
2022-01-09 23:34:50,889 iteration 1084 : loss : 0.110382, loss_ce: 0.065832
2022-01-09 23:34:52,538 iteration 1085 : loss : 0.099860, loss_ce: 0.047360
2022-01-09 23:34:54,089 iteration 1086 : loss : 0.085884, loss_ce: 0.037008
2022-01-09 23:34:55,660 iteration 1087 : loss : 0.082263, loss_ce: 0.036489
2022-01-09 23:34:57,293 iteration 1088 : loss : 0.086091, loss_ce: 0.038975
 16%|████▊                         | 64/400 [31:32<2:36:03, 27.87s/it]2022-01-09 23:34:58,898 iteration 1089 : loss : 0.133482, loss_ce: 0.057095
2022-01-09 23:35:00,456 iteration 1090 : loss : 0.076643, loss_ce: 0.038408
2022-01-09 23:35:02,034 iteration 1091 : loss : 0.100147, loss_ce: 0.045113
2022-01-09 23:35:03,609 iteration 1092 : loss : 0.070478, loss_ce: 0.028007
2022-01-09 23:35:05,204 iteration 1093 : loss : 0.138135, loss_ce: 0.061761
2022-01-09 23:35:06,843 iteration 1094 : loss : 0.089611, loss_ce: 0.039739
2022-01-09 23:35:08,371 iteration 1095 : loss : 0.070788, loss_ce: 0.034170
2022-01-09 23:35:10,043 iteration 1096 : loss : 0.090495, loss_ce: 0.037136
2022-01-09 23:35:11,639 iteration 1097 : loss : 0.095921, loss_ce: 0.041436
2022-01-09 23:35:13,173 iteration 1098 : loss : 0.067771, loss_ce: 0.032317
2022-01-09 23:35:14,771 iteration 1099 : loss : 0.101263, loss_ce: 0.032071
2022-01-09 23:35:16,425 iteration 1100 : loss : 0.109347, loss_ce: 0.044105
2022-01-09 23:35:17,998 iteration 1101 : loss : 0.097834, loss_ce: 0.044755
2022-01-09 23:35:19,649 iteration 1102 : loss : 0.128799, loss_ce: 0.041514
2022-01-09 23:35:21,211 iteration 1103 : loss : 0.124839, loss_ce: 0.051576
2022-01-09 23:35:22,767 iteration 1104 : loss : 0.114446, loss_ce: 0.046175
2022-01-09 23:35:22,767 Training Data Eval:
2022-01-09 23:35:30,850   Average segmentation loss on training set: 0.0718
2022-01-09 23:35:30,851 Validation Data Eval:
2022-01-09 23:35:33,633   Average segmentation loss on validation set: 0.1395
2022-01-09 23:35:35,257 iteration 1105 : loss : 0.083456, loss_ce: 0.026194
 16%|████▉                         | 65/400 [32:10<2:52:29, 30.90s/it]2022-01-09 23:35:36,823 iteration 1106 : loss : 0.100965, loss_ce: 0.041442
2022-01-09 23:35:38,375 iteration 1107 : loss : 0.103629, loss_ce: 0.049822
2022-01-09 23:35:39,956 iteration 1108 : loss : 0.166747, loss_ce: 0.056270
2022-01-09 23:35:41,469 iteration 1109 : loss : 0.083453, loss_ce: 0.037267
2022-01-09 23:35:43,048 iteration 1110 : loss : 0.104578, loss_ce: 0.042439
2022-01-09 23:35:44,596 iteration 1111 : loss : 0.065713, loss_ce: 0.027753
2022-01-09 23:35:46,152 iteration 1112 : loss : 0.093996, loss_ce: 0.038824
2022-01-09 23:35:47,691 iteration 1113 : loss : 0.088056, loss_ce: 0.030811
2022-01-09 23:35:49,321 iteration 1114 : loss : 0.106193, loss_ce: 0.054444
2022-01-09 23:35:51,021 iteration 1115 : loss : 0.084322, loss_ce: 0.039341
2022-01-09 23:35:52,643 iteration 1116 : loss : 0.133331, loss_ce: 0.046370
2022-01-09 23:35:54,335 iteration 1117 : loss : 0.099524, loss_ce: 0.038774
2022-01-09 23:35:55,958 iteration 1118 : loss : 0.130713, loss_ce: 0.043950
2022-01-09 23:35:57,507 iteration 1119 : loss : 0.071661, loss_ce: 0.030367
2022-01-09 23:35:59,047 iteration 1120 : loss : 0.095258, loss_ce: 0.032971
2022-01-09 23:36:00,637 iteration 1121 : loss : 0.097027, loss_ce: 0.029140
2022-01-09 23:36:02,217 iteration 1122 : loss : 0.112334, loss_ce: 0.049436
 16%|████▉                         | 66/400 [32:37<2:45:24, 29.72s/it]2022-01-09 23:36:03,808 iteration 1123 : loss : 0.153300, loss_ce: 0.062682
2022-01-09 23:36:05,417 iteration 1124 : loss : 0.090640, loss_ce: 0.041860
2022-01-09 23:36:06,986 iteration 1125 : loss : 0.074084, loss_ce: 0.022133
2022-01-09 23:36:08,542 iteration 1126 : loss : 0.097153, loss_ce: 0.036169
2022-01-09 23:36:10,090 iteration 1127 : loss : 0.098645, loss_ce: 0.039927
2022-01-09 23:36:11,721 iteration 1128 : loss : 0.125093, loss_ce: 0.052808
2022-01-09 23:36:13,352 iteration 1129 : loss : 0.094709, loss_ce: 0.030514
2022-01-09 23:36:14,907 iteration 1130 : loss : 0.071408, loss_ce: 0.034049
2022-01-09 23:36:16,463 iteration 1131 : loss : 0.106183, loss_ce: 0.042040
2022-01-09 23:36:18,073 iteration 1132 : loss : 0.063875, loss_ce: 0.026814
2022-01-09 23:36:19,587 iteration 1133 : loss : 0.072082, loss_ce: 0.033440
2022-01-09 23:36:21,157 iteration 1134 : loss : 0.098540, loss_ce: 0.035907
2022-01-09 23:36:22,696 iteration 1135 : loss : 0.082852, loss_ce: 0.036458
2022-01-09 23:36:24,259 iteration 1136 : loss : 0.051757, loss_ce: 0.019575
2022-01-09 23:36:25,803 iteration 1137 : loss : 0.117897, loss_ce: 0.054030
2022-01-09 23:36:27,415 iteration 1138 : loss : 0.080206, loss_ce: 0.032431
2022-01-09 23:36:29,005 iteration 1139 : loss : 0.096382, loss_ce: 0.034001
 17%|█████                         | 67/400 [33:04<2:40:02, 28.84s/it]2022-01-09 23:36:30,639 iteration 1140 : loss : 0.093797, loss_ce: 0.038348
2022-01-09 23:36:32,176 iteration 1141 : loss : 0.087264, loss_ce: 0.034829
2022-01-09 23:36:33,806 iteration 1142 : loss : 0.093470, loss_ce: 0.043744
2022-01-09 23:36:35,376 iteration 1143 : loss : 0.076863, loss_ce: 0.028539
2022-01-09 23:36:36,986 iteration 1144 : loss : 0.102787, loss_ce: 0.056027
2022-01-09 23:36:38,513 iteration 1145 : loss : 0.065686, loss_ce: 0.024329
2022-01-09 23:36:40,101 iteration 1146 : loss : 0.096288, loss_ce: 0.043212
2022-01-09 23:36:41,591 iteration 1147 : loss : 0.072117, loss_ce: 0.027899
2022-01-09 23:36:43,212 iteration 1148 : loss : 0.095493, loss_ce: 0.034741
2022-01-09 23:36:44,733 iteration 1149 : loss : 0.065158, loss_ce: 0.031686
2022-01-09 23:36:46,308 iteration 1150 : loss : 0.083057, loss_ce: 0.034793
2022-01-09 23:36:47,913 iteration 1151 : loss : 0.070164, loss_ce: 0.028452
2022-01-09 23:36:49,606 iteration 1152 : loss : 0.072071, loss_ce: 0.030094
2022-01-09 23:36:51,096 iteration 1153 : loss : 0.079278, loss_ce: 0.029460
2022-01-09 23:36:52,705 iteration 1154 : loss : 0.091826, loss_ce: 0.031305
2022-01-09 23:36:54,240 iteration 1155 : loss : 0.086121, loss_ce: 0.029733
2022-01-09 23:36:55,862 iteration 1156 : loss : 0.111773, loss_ce: 0.051926
 17%|█████                         | 68/400 [33:30<2:36:16, 28.24s/it]2022-01-09 23:36:57,476 iteration 1157 : loss : 0.152860, loss_ce: 0.053076
2022-01-09 23:36:59,066 iteration 1158 : loss : 0.073561, loss_ce: 0.026058
2022-01-09 23:37:00,715 iteration 1159 : loss : 0.057646, loss_ce: 0.019648
2022-01-09 23:37:02,406 iteration 1160 : loss : 0.103754, loss_ce: 0.041078
2022-01-09 23:37:04,041 iteration 1161 : loss : 0.056188, loss_ce: 0.023116
2022-01-09 23:37:05,647 iteration 1162 : loss : 0.081791, loss_ce: 0.029374
2022-01-09 23:37:07,202 iteration 1163 : loss : 0.073261, loss_ce: 0.033160
2022-01-09 23:37:08,767 iteration 1164 : loss : 0.091820, loss_ce: 0.035407
2022-01-09 23:37:10,336 iteration 1165 : loss : 0.074378, loss_ce: 0.037101
2022-01-09 23:37:11,983 iteration 1166 : loss : 0.050979, loss_ce: 0.020495
2022-01-09 23:37:13,524 iteration 1167 : loss : 0.078875, loss_ce: 0.035528
2022-01-09 23:37:15,080 iteration 1168 : loss : 0.072917, loss_ce: 0.030403
2022-01-09 23:37:16,623 iteration 1169 : loss : 0.070711, loss_ce: 0.030007
2022-01-09 23:37:18,168 iteration 1170 : loss : 0.099092, loss_ce: 0.042541
2022-01-09 23:37:19,759 iteration 1171 : loss : 0.083389, loss_ce: 0.029443
2022-01-09 23:37:21,338 iteration 1172 : loss : 0.116488, loss_ce: 0.056573
2022-01-09 23:37:22,943 iteration 1173 : loss : 0.082465, loss_ce: 0.027649
 17%|█████▏                        | 69/400 [33:58<2:33:52, 27.89s/it]2022-01-09 23:37:24,551 iteration 1174 : loss : 0.077608, loss_ce: 0.023799
2022-01-09 23:37:26,131 iteration 1175 : loss : 0.064571, loss_ce: 0.026472
2022-01-09 23:37:27,701 iteration 1176 : loss : 0.061122, loss_ce: 0.025291
2022-01-09 23:37:29,236 iteration 1177 : loss : 0.073263, loss_ce: 0.026512
2022-01-09 23:37:30,842 iteration 1178 : loss : 0.088607, loss_ce: 0.024796
2022-01-09 23:37:32,496 iteration 1179 : loss : 0.074021, loss_ce: 0.030036
2022-01-09 23:37:34,043 iteration 1180 : loss : 0.083282, loss_ce: 0.033733
2022-01-09 23:37:35,608 iteration 1181 : loss : 0.074037, loss_ce: 0.030197
2022-01-09 23:37:37,191 iteration 1182 : loss : 0.077135, loss_ce: 0.027663
2022-01-09 23:37:38,717 iteration 1183 : loss : 0.073995, loss_ce: 0.028495
2022-01-09 23:37:40,253 iteration 1184 : loss : 0.066189, loss_ce: 0.029921
2022-01-09 23:37:41,865 iteration 1185 : loss : 0.080747, loss_ce: 0.033808
2022-01-09 23:37:43,406 iteration 1186 : loss : 0.071407, loss_ce: 0.033437
2022-01-09 23:37:45,039 iteration 1187 : loss : 0.073529, loss_ce: 0.028101
2022-01-09 23:37:46,572 iteration 1188 : loss : 0.092289, loss_ce: 0.034652
2022-01-09 23:37:48,220 iteration 1189 : loss : 0.077777, loss_ce: 0.038596
2022-01-09 23:37:48,220 Training Data Eval:
2022-01-09 23:37:56,300   Average segmentation loss on training set: 0.0586
2022-01-09 23:37:56,301 Validation Data Eval:
2022-01-09 23:37:59,084   Average segmentation loss on validation set: 0.0910
2022-01-09 23:38:05,314 Found new lowest validation loss at iteration 1189! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-09 23:38:06,788 iteration 1190 : loss : 0.091129, loss_ce: 0.042486
 18%|█████▎                        | 70/400 [34:41<2:59:44, 32.68s/it]2022-01-09 23:38:08,364 iteration 1191 : loss : 0.072858, loss_ce: 0.033266
2022-01-09 23:38:09,887 iteration 1192 : loss : 0.060113, loss_ce: 0.029329
2022-01-09 23:38:11,534 iteration 1193 : loss : 0.096205, loss_ce: 0.031903
2022-01-09 23:38:13,093 iteration 1194 : loss : 0.099664, loss_ce: 0.030744
2022-01-09 23:38:14,616 iteration 1195 : loss : 0.072868, loss_ce: 0.033094
2022-01-09 23:38:16,137 iteration 1196 : loss : 0.074707, loss_ce: 0.032592
2022-01-09 23:38:17,625 iteration 1197 : loss : 0.071906, loss_ce: 0.026388
2022-01-09 23:38:19,233 iteration 1198 : loss : 0.057226, loss_ce: 0.027393
2022-01-09 23:38:20,808 iteration 1199 : loss : 0.056573, loss_ce: 0.022748
2022-01-09 23:38:22,359 iteration 1200 : loss : 0.075143, loss_ce: 0.025900
2022-01-09 23:38:23,876 iteration 1201 : loss : 0.066613, loss_ce: 0.030901
2022-01-09 23:38:25,467 iteration 1202 : loss : 0.091395, loss_ce: 0.041596
2022-01-09 23:38:27,049 iteration 1203 : loss : 0.096132, loss_ce: 0.030482
2022-01-09 23:38:28,620 iteration 1204 : loss : 0.081221, loss_ce: 0.026189
2022-01-09 23:38:30,198 iteration 1205 : loss : 0.116454, loss_ce: 0.052841
2022-01-09 23:38:31,785 iteration 1206 : loss : 0.077861, loss_ce: 0.031722
2022-01-09 23:38:33,408 iteration 1207 : loss : 0.061065, loss_ce: 0.021325
 18%|█████▎                        | 71/400 [35:08<2:49:14, 30.86s/it]2022-01-09 23:38:35,076 iteration 1208 : loss : 0.072839, loss_ce: 0.034696
2022-01-09 23:38:36,657 iteration 1209 : loss : 0.059688, loss_ce: 0.025675
2022-01-09 23:38:38,237 iteration 1210 : loss : 0.069999, loss_ce: 0.027312
2022-01-09 23:38:39,763 iteration 1211 : loss : 0.061051, loss_ce: 0.022938
2022-01-09 23:38:41,270 iteration 1212 : loss : 0.114505, loss_ce: 0.035187
2022-01-09 23:38:42,957 iteration 1213 : loss : 0.082690, loss_ce: 0.035280
2022-01-09 23:38:44,529 iteration 1214 : loss : 0.102397, loss_ce: 0.043986
2022-01-09 23:38:46,106 iteration 1215 : loss : 0.128224, loss_ce: 0.066525
2022-01-09 23:38:47,795 iteration 1216 : loss : 0.072168, loss_ce: 0.028067
2022-01-09 23:38:49,380 iteration 1217 : loss : 0.054823, loss_ce: 0.019909
2022-01-09 23:38:50,912 iteration 1218 : loss : 0.086776, loss_ce: 0.033566
2022-01-09 23:38:52,412 iteration 1219 : loss : 0.078275, loss_ce: 0.028242
2022-01-09 23:38:53,971 iteration 1220 : loss : 0.068021, loss_ce: 0.026180
2022-01-09 23:38:55,503 iteration 1221 : loss : 0.097790, loss_ce: 0.050062
2022-01-09 23:38:57,132 iteration 1222 : loss : 0.086357, loss_ce: 0.030529
2022-01-09 23:38:58,680 iteration 1223 : loss : 0.080162, loss_ce: 0.034379
2022-01-09 23:39:00,288 iteration 1224 : loss : 0.097904, loss_ce: 0.039424
 18%|█████▍                        | 72/400 [35:35<2:42:10, 29.67s/it]2022-01-09 23:39:01,892 iteration 1225 : loss : 0.075009, loss_ce: 0.031293
2022-01-09 23:39:03,499 iteration 1226 : loss : 0.086543, loss_ce: 0.039875
2022-01-09 23:39:05,022 iteration 1227 : loss : 0.070230, loss_ce: 0.022864
2022-01-09 23:39:06,579 iteration 1228 : loss : 0.088250, loss_ce: 0.033905
2022-01-09 23:39:08,081 iteration 1229 : loss : 0.069976, loss_ce: 0.022675
2022-01-09 23:39:09,611 iteration 1230 : loss : 0.068999, loss_ce: 0.028559
2022-01-09 23:39:11,214 iteration 1231 : loss : 0.063931, loss_ce: 0.029234
2022-01-09 23:39:12,743 iteration 1232 : loss : 0.083034, loss_ce: 0.035959
2022-01-09 23:39:14,281 iteration 1233 : loss : 0.049297, loss_ce: 0.019742
2022-01-09 23:39:15,892 iteration 1234 : loss : 0.088256, loss_ce: 0.037520
2022-01-09 23:39:17,491 iteration 1235 : loss : 0.124180, loss_ce: 0.038721
2022-01-09 23:39:19,023 iteration 1236 : loss : 0.056855, loss_ce: 0.022264
2022-01-09 23:39:20,505 iteration 1237 : loss : 0.101240, loss_ce: 0.050523
2022-01-09 23:39:22,116 iteration 1238 : loss : 0.069497, loss_ce: 0.023223
2022-01-09 23:39:23,632 iteration 1239 : loss : 0.051240, loss_ce: 0.018485
2022-01-09 23:39:25,156 iteration 1240 : loss : 0.057212, loss_ce: 0.024701
2022-01-09 23:39:26,681 iteration 1241 : loss : 0.092382, loss_ce: 0.046558
 18%|█████▍                        | 73/400 [36:01<2:36:19, 28.68s/it]2022-01-09 23:39:28,325 iteration 1242 : loss : 0.083717, loss_ce: 0.047500
2022-01-09 23:39:29,855 iteration 1243 : loss : 0.087494, loss_ce: 0.031107
2022-01-09 23:39:31,392 iteration 1244 : loss : 0.073107, loss_ce: 0.033076
2022-01-09 23:39:32,974 iteration 1245 : loss : 0.079708, loss_ce: 0.028991
2022-01-09 23:39:34,547 iteration 1246 : loss : 0.073183, loss_ce: 0.023622
2022-01-09 23:39:36,081 iteration 1247 : loss : 0.064387, loss_ce: 0.029213
2022-01-09 23:39:37,668 iteration 1248 : loss : 0.067243, loss_ce: 0.022070
2022-01-09 23:39:39,197 iteration 1249 : loss : 0.045175, loss_ce: 0.015968
2022-01-09 23:39:40,860 iteration 1250 : loss : 0.071977, loss_ce: 0.030310
2022-01-09 23:39:42,550 iteration 1251 : loss : 0.078029, loss_ce: 0.041862
2022-01-09 23:39:44,147 iteration 1252 : loss : 0.068906, loss_ce: 0.028223
2022-01-09 23:39:45,642 iteration 1253 : loss : 0.062327, loss_ce: 0.021810
2022-01-09 23:39:47,212 iteration 1254 : loss : 0.057090, loss_ce: 0.028360
2022-01-09 23:39:48,792 iteration 1255 : loss : 0.067727, loss_ce: 0.025611
2022-01-09 23:39:50,347 iteration 1256 : loss : 0.070022, loss_ce: 0.029671
2022-01-09 23:39:51,915 iteration 1257 : loss : 0.092713, loss_ce: 0.031479
2022-01-09 23:39:53,498 iteration 1258 : loss : 0.064706, loss_ce: 0.028757
 18%|█████▌                        | 74/400 [36:28<2:32:48, 28.12s/it]2022-01-09 23:39:55,085 iteration 1259 : loss : 0.048280, loss_ce: 0.021308
2022-01-09 23:39:56,703 iteration 1260 : loss : 0.049672, loss_ce: 0.022135
2022-01-09 23:39:58,254 iteration 1261 : loss : 0.058665, loss_ce: 0.020346
2022-01-09 23:39:59,903 iteration 1262 : loss : 0.065338, loss_ce: 0.025910
2022-01-09 23:40:01,576 iteration 1263 : loss : 0.145562, loss_ce: 0.067206
2022-01-09 23:40:03,137 iteration 1264 : loss : 0.056235, loss_ce: 0.026492
2022-01-09 23:40:04,763 iteration 1265 : loss : 0.072742, loss_ce: 0.037930
2022-01-09 23:40:06,384 iteration 1266 : loss : 0.060904, loss_ce: 0.025157
2022-01-09 23:40:07,982 iteration 1267 : loss : 0.070161, loss_ce: 0.027182
2022-01-09 23:40:09,523 iteration 1268 : loss : 0.096207, loss_ce: 0.036611
2022-01-09 23:40:11,164 iteration 1269 : loss : 0.144096, loss_ce: 0.041712
2022-01-09 23:40:12,773 iteration 1270 : loss : 0.066794, loss_ce: 0.028313
2022-01-09 23:40:14,339 iteration 1271 : loss : 0.074335, loss_ce: 0.034892
2022-01-09 23:40:15,831 iteration 1272 : loss : 0.074702, loss_ce: 0.027276
2022-01-09 23:40:17,381 iteration 1273 : loss : 0.075490, loss_ce: 0.027691
2022-01-09 23:40:18,947 iteration 1274 : loss : 0.079411, loss_ce: 0.025297
2022-01-09 23:40:18,947 Training Data Eval:
2022-01-09 23:40:27,012   Average segmentation loss on training set: 0.0579
2022-01-09 23:40:27,013 Validation Data Eval:
2022-01-09 23:40:29,789   Average segmentation loss on validation set: 0.1542
2022-01-09 23:40:31,345 iteration 1275 : loss : 0.056227, loss_ce: 0.022111
 19%|█████▋                        | 75/400 [37:06<2:48:08, 31.04s/it]2022-01-09 23:40:32,885 iteration 1276 : loss : 0.063808, loss_ce: 0.025496
2022-01-09 23:40:34,555 iteration 1277 : loss : 0.056362, loss_ce: 0.020869
2022-01-09 23:40:36,086 iteration 1278 : loss : 0.048942, loss_ce: 0.016695
2022-01-09 23:40:37,752 iteration 1279 : loss : 0.059458, loss_ce: 0.022948
2022-01-09 23:40:39,348 iteration 1280 : loss : 0.085245, loss_ce: 0.040109
2022-01-09 23:40:40,935 iteration 1281 : loss : 0.073672, loss_ce: 0.035687
2022-01-09 23:40:42,452 iteration 1282 : loss : 0.044842, loss_ce: 0.019335
2022-01-09 23:40:44,077 iteration 1283 : loss : 0.104860, loss_ce: 0.033260
2022-01-09 23:40:45,607 iteration 1284 : loss : 0.060746, loss_ce: 0.023907
2022-01-09 23:40:47,243 iteration 1285 : loss : 0.067263, loss_ce: 0.022617
2022-01-09 23:40:48,903 iteration 1286 : loss : 0.166101, loss_ce: 0.053562
2022-01-09 23:40:50,454 iteration 1287 : loss : 0.060842, loss_ce: 0.033859
2022-01-09 23:40:51,985 iteration 1288 : loss : 0.071927, loss_ce: 0.037968
2022-01-09 23:40:53,553 iteration 1289 : loss : 0.108833, loss_ce: 0.047585
2022-01-09 23:40:55,157 iteration 1290 : loss : 0.080554, loss_ce: 0.029120
2022-01-09 23:40:56,773 iteration 1291 : loss : 0.095502, loss_ce: 0.046668
2022-01-09 23:40:58,404 iteration 1292 : loss : 0.079956, loss_ce: 0.032105
 19%|█████▋                        | 76/400 [37:33<2:41:10, 29.85s/it]2022-01-09 23:41:00,067 iteration 1293 : loss : 0.064186, loss_ce: 0.024250
2022-01-09 23:41:01,672 iteration 1294 : loss : 0.087197, loss_ce: 0.034935
2022-01-09 23:41:03,322 iteration 1295 : loss : 0.083898, loss_ce: 0.022968
2022-01-09 23:41:04,921 iteration 1296 : loss : 0.076379, loss_ce: 0.027155
2022-01-09 23:41:06,458 iteration 1297 : loss : 0.090854, loss_ce: 0.050418
2022-01-09 23:41:07,967 iteration 1298 : loss : 0.053266, loss_ce: 0.022096
2022-01-09 23:41:09,551 iteration 1299 : loss : 0.061522, loss_ce: 0.029426
2022-01-09 23:41:11,116 iteration 1300 : loss : 0.083185, loss_ce: 0.028612
2022-01-09 23:41:12,717 iteration 1301 : loss : 0.058010, loss_ce: 0.023233
2022-01-09 23:41:14,267 iteration 1302 : loss : 0.082769, loss_ce: 0.035548
2022-01-09 23:41:15,821 iteration 1303 : loss : 0.067816, loss_ce: 0.029791
2022-01-09 23:41:17,404 iteration 1304 : loss : 0.067465, loss_ce: 0.025843
2022-01-09 23:41:18,973 iteration 1305 : loss : 0.077746, loss_ce: 0.031483
2022-01-09 23:41:20,585 iteration 1306 : loss : 0.093533, loss_ce: 0.034685
2022-01-09 23:41:22,197 iteration 1307 : loss : 0.057593, loss_ce: 0.026155
2022-01-09 23:41:23,782 iteration 1308 : loss : 0.104115, loss_ce: 0.029070
2022-01-09 23:41:25,297 iteration 1309 : loss : 0.065863, loss_ce: 0.024551
 19%|█████▊                        | 77/400 [38:00<2:35:54, 28.96s/it]2022-01-09 23:41:26,999 iteration 1310 : loss : 0.080972, loss_ce: 0.032427
2022-01-09 23:41:28,549 iteration 1311 : loss : 0.066531, loss_ce: 0.027166
2022-01-09 23:41:30,090 iteration 1312 : loss : 0.040038, loss_ce: 0.017850
2022-01-09 23:41:31,634 iteration 1313 : loss : 0.075548, loss_ce: 0.023044
2022-01-09 23:41:33,210 iteration 1314 : loss : 0.077756, loss_ce: 0.035508
2022-01-09 23:41:34,810 iteration 1315 : loss : 0.108743, loss_ce: 0.045685
2022-01-09 23:41:36,494 iteration 1316 : loss : 0.075106, loss_ce: 0.029816
2022-01-09 23:41:38,059 iteration 1317 : loss : 0.080133, loss_ce: 0.038798
2022-01-09 23:41:39,712 iteration 1318 : loss : 0.149274, loss_ce: 0.027152
2022-01-09 23:41:41,324 iteration 1319 : loss : 0.072130, loss_ce: 0.033973
2022-01-09 23:41:42,882 iteration 1320 : loss : 0.055413, loss_ce: 0.020123
2022-01-09 23:41:44,476 iteration 1321 : loss : 0.070126, loss_ce: 0.021159
2022-01-09 23:41:45,997 iteration 1322 : loss : 0.067781, loss_ce: 0.031911
2022-01-09 23:41:47,558 iteration 1323 : loss : 0.062977, loss_ce: 0.022776
2022-01-09 23:41:49,157 iteration 1324 : loss : 0.064539, loss_ce: 0.029510
2022-01-09 23:41:50,791 iteration 1325 : loss : 0.066221, loss_ce: 0.022636
2022-01-09 23:41:52,278 iteration 1326 : loss : 0.061497, loss_ce: 0.022800
 20%|█████▊                        | 78/400 [38:27<2:32:13, 28.37s/it]2022-01-09 23:41:53,874 iteration 1327 : loss : 0.097195, loss_ce: 0.034060
2022-01-09 23:41:55,474 iteration 1328 : loss : 0.084474, loss_ce: 0.039995
2022-01-09 23:41:57,045 iteration 1329 : loss : 0.055970, loss_ce: 0.025026
2022-01-09 23:41:58,588 iteration 1330 : loss : 0.094938, loss_ce: 0.030117
2022-01-09 23:42:00,099 iteration 1331 : loss : 0.071789, loss_ce: 0.034609
2022-01-09 23:42:01,622 iteration 1332 : loss : 0.064582, loss_ce: 0.022990
2022-01-09 23:42:03,180 iteration 1333 : loss : 0.073619, loss_ce: 0.022852
2022-01-09 23:42:04,754 iteration 1334 : loss : 0.071651, loss_ce: 0.025555
2022-01-09 23:42:06,297 iteration 1335 : loss : 0.107822, loss_ce: 0.034052
2022-01-09 23:42:07,843 iteration 1336 : loss : 0.053582, loss_ce: 0.021612
2022-01-09 23:42:09,469 iteration 1337 : loss : 0.105083, loss_ce: 0.048099
2022-01-09 23:42:11,074 iteration 1338 : loss : 0.070116, loss_ce: 0.022774
2022-01-09 23:42:12,669 iteration 1339 : loss : 0.073447, loss_ce: 0.033152
2022-01-09 23:42:14,263 iteration 1340 : loss : 0.069354, loss_ce: 0.034339
2022-01-09 23:42:15,846 iteration 1341 : loss : 0.098373, loss_ce: 0.045693
2022-01-09 23:42:17,409 iteration 1342 : loss : 0.107233, loss_ce: 0.051398
2022-01-09 23:42:18,928 iteration 1343 : loss : 0.056064, loss_ce: 0.021044
 20%|█████▉                        | 79/400 [38:54<2:29:00, 27.85s/it]2022-01-09 23:42:20,559 iteration 1344 : loss : 0.086806, loss_ce: 0.039067
2022-01-09 23:42:22,202 iteration 1345 : loss : 0.107503, loss_ce: 0.042296
2022-01-09 23:42:23,758 iteration 1346 : loss : 0.110836, loss_ce: 0.048254
2022-01-09 23:42:25,403 iteration 1347 : loss : 0.051912, loss_ce: 0.023317
2022-01-09 23:42:27,080 iteration 1348 : loss : 0.097615, loss_ce: 0.042570
2022-01-09 23:42:28,662 iteration 1349 : loss : 0.064121, loss_ce: 0.028739
2022-01-09 23:42:30,238 iteration 1350 : loss : 0.076094, loss_ce: 0.034560
2022-01-09 23:42:31,809 iteration 1351 : loss : 0.067289, loss_ce: 0.033036
2022-01-09 23:42:33,438 iteration 1352 : loss : 0.066184, loss_ce: 0.030278
2022-01-09 23:42:35,040 iteration 1353 : loss : 0.093414, loss_ce: 0.036103
2022-01-09 23:42:36,659 iteration 1354 : loss : 0.090374, loss_ce: 0.032966
2022-01-09 23:42:38,337 iteration 1355 : loss : 0.176256, loss_ce: 0.046924
2022-01-09 23:42:39,936 iteration 1356 : loss : 0.061039, loss_ce: 0.024724
2022-01-09 23:42:41,547 iteration 1357 : loss : 0.070834, loss_ce: 0.024727
2022-01-09 23:42:43,176 iteration 1358 : loss : 0.082076, loss_ce: 0.033539
2022-01-09 23:42:44,752 iteration 1359 : loss : 0.079668, loss_ce: 0.033167
2022-01-09 23:42:44,752 Training Data Eval:
2022-01-09 23:42:52,814   Average segmentation loss on training set: 0.0491
2022-01-09 23:42:52,815 Validation Data Eval:
2022-01-09 23:42:55,586   Average segmentation loss on validation set: 0.0946
2022-01-09 23:42:57,216 iteration 1360 : loss : 0.079393, loss_ce: 0.021020
 20%|██████                        | 80/400 [39:32<2:45:14, 30.98s/it]2022-01-09 23:42:58,942 iteration 1361 : loss : 0.133176, loss_ce: 0.058401
2022-01-09 23:43:00,528 iteration 1362 : loss : 0.079061, loss_ce: 0.027286
2022-01-09 23:43:02,263 iteration 1363 : loss : 0.127528, loss_ce: 0.065167
2022-01-09 23:43:03,720 iteration 1364 : loss : 0.082513, loss_ce: 0.022133
2022-01-09 23:43:05,361 iteration 1365 : loss : 0.078116, loss_ce: 0.034744
2022-01-09 23:43:06,960 iteration 1366 : loss : 0.078069, loss_ce: 0.035103
2022-01-09 23:43:08,528 iteration 1367 : loss : 0.067846, loss_ce: 0.027030
2022-01-09 23:43:10,114 iteration 1368 : loss : 0.062362, loss_ce: 0.030210
2022-01-09 23:43:11,667 iteration 1369 : loss : 0.067034, loss_ce: 0.025050
2022-01-09 23:43:13,285 iteration 1370 : loss : 0.079778, loss_ce: 0.037827
2022-01-09 23:43:14,836 iteration 1371 : loss : 0.064436, loss_ce: 0.023593
2022-01-09 23:43:16,378 iteration 1372 : loss : 0.074796, loss_ce: 0.030542
2022-01-09 23:43:17,911 iteration 1373 : loss : 0.063475, loss_ce: 0.023766
2022-01-09 23:43:19,498 iteration 1374 : loss : 0.078689, loss_ce: 0.029730
2022-01-09 23:43:21,094 iteration 1375 : loss : 0.078385, loss_ce: 0.035165
2022-01-09 23:43:22,678 iteration 1376 : loss : 0.070496, loss_ce: 0.022570
2022-01-09 23:43:24,241 iteration 1377 : loss : 0.046383, loss_ce: 0.018918
 20%|██████                        | 81/400 [39:59<2:38:25, 29.80s/it]2022-01-09 23:43:25,806 iteration 1378 : loss : 0.091928, loss_ce: 0.035388
2022-01-09 23:43:27,424 iteration 1379 : loss : 0.060361, loss_ce: 0.024354
2022-01-09 23:43:29,012 iteration 1380 : loss : 0.076680, loss_ce: 0.036746
2022-01-09 23:43:30,672 iteration 1381 : loss : 0.075641, loss_ce: 0.034366
2022-01-09 23:43:32,175 iteration 1382 : loss : 0.062484, loss_ce: 0.024995
2022-01-09 23:43:33,820 iteration 1383 : loss : 0.049424, loss_ce: 0.019760
2022-01-09 23:43:35,406 iteration 1384 : loss : 0.079220, loss_ce: 0.030780
2022-01-09 23:43:36,922 iteration 1385 : loss : 0.091774, loss_ce: 0.039620
2022-01-09 23:43:38,469 iteration 1386 : loss : 0.087668, loss_ce: 0.038049
2022-01-09 23:43:40,028 iteration 1387 : loss : 0.085312, loss_ce: 0.029541
2022-01-09 23:43:41,604 iteration 1388 : loss : 0.089073, loss_ce: 0.029413
2022-01-09 23:43:43,177 iteration 1389 : loss : 0.083836, loss_ce: 0.034734
2022-01-09 23:43:44,795 iteration 1390 : loss : 0.055351, loss_ce: 0.024351
2022-01-09 23:43:46,439 iteration 1391 : loss : 0.055673, loss_ce: 0.022192
2022-01-09 23:43:48,058 iteration 1392 : loss : 0.060751, loss_ce: 0.030489
2022-01-09 23:43:49,610 iteration 1393 : loss : 0.058090, loss_ce: 0.022899
2022-01-09 23:43:51,245 iteration 1394 : loss : 0.061803, loss_ce: 0.030307
 20%|██████▏                       | 82/400 [40:26<2:33:28, 28.96s/it]2022-01-09 23:43:52,863 iteration 1395 : loss : 0.087422, loss_ce: 0.036154
2022-01-09 23:43:54,419 iteration 1396 : loss : 0.050180, loss_ce: 0.023676
2022-01-09 23:43:56,050 iteration 1397 : loss : 0.063271, loss_ce: 0.026118
2022-01-09 23:43:57,693 iteration 1398 : loss : 0.077241, loss_ce: 0.034742
2022-01-09 23:43:59,195 iteration 1399 : loss : 0.063902, loss_ce: 0.028063
2022-01-09 23:44:00,693 iteration 1400 : loss : 0.086687, loss_ce: 0.030257
2022-01-09 23:44:02,330 iteration 1401 : loss : 0.069940, loss_ce: 0.026177
2022-01-09 23:44:03,920 iteration 1402 : loss : 0.102384, loss_ce: 0.042173
2022-01-09 23:44:05,446 iteration 1403 : loss : 0.060113, loss_ce: 0.023733
2022-01-09 23:44:06,968 iteration 1404 : loss : 0.036404, loss_ce: 0.017694
2022-01-09 23:44:08,539 iteration 1405 : loss : 0.059740, loss_ce: 0.023697
2022-01-09 23:44:10,121 iteration 1406 : loss : 0.068239, loss_ce: 0.027353
2022-01-09 23:44:11,742 iteration 1407 : loss : 0.045458, loss_ce: 0.016843
2022-01-09 23:44:13,315 iteration 1408 : loss : 0.062103, loss_ce: 0.022903
2022-01-09 23:44:14,899 iteration 1409 : loss : 0.081565, loss_ce: 0.027600
2022-01-09 23:44:16,546 iteration 1410 : loss : 0.059082, loss_ce: 0.023953
2022-01-09 23:44:18,099 iteration 1411 : loss : 0.057028, loss_ce: 0.020277
 21%|██████▏                       | 83/400 [40:53<2:29:39, 28.33s/it]2022-01-09 23:44:19,701 iteration 1412 : loss : 0.068322, loss_ce: 0.026575
2022-01-09 23:44:21,341 iteration 1413 : loss : 0.084564, loss_ce: 0.029865
2022-01-09 23:44:22,901 iteration 1414 : loss : 0.077943, loss_ce: 0.028268
2022-01-09 23:44:24,521 iteration 1415 : loss : 0.082459, loss_ce: 0.024242
2022-01-09 23:44:26,084 iteration 1416 : loss : 0.057479, loss_ce: 0.021591
2022-01-09 23:44:27,629 iteration 1417 : loss : 0.076177, loss_ce: 0.030030
2022-01-09 23:44:29,173 iteration 1418 : loss : 0.089058, loss_ce: 0.037840
2022-01-09 23:44:30,691 iteration 1419 : loss : 0.058510, loss_ce: 0.022351
2022-01-09 23:44:32,308 iteration 1420 : loss : 0.071215, loss_ce: 0.025186
2022-01-09 23:44:33,965 iteration 1421 : loss : 0.105443, loss_ce: 0.044732
2022-01-09 23:44:35,567 iteration 1422 : loss : 0.074288, loss_ce: 0.023605
2022-01-09 23:44:37,079 iteration 1423 : loss : 0.065690, loss_ce: 0.028303
2022-01-09 23:44:38,681 iteration 1424 : loss : 0.056054, loss_ce: 0.027447
2022-01-09 23:44:40,277 iteration 1425 : loss : 0.051828, loss_ce: 0.022158
2022-01-09 23:44:41,808 iteration 1426 : loss : 0.069705, loss_ce: 0.030902
2022-01-09 23:44:43,380 iteration 1427 : loss : 0.084419, loss_ce: 0.032431
2022-01-09 23:44:44,861 iteration 1428 : loss : 0.089965, loss_ce: 0.032356
 21%|██████▎                       | 84/400 [41:19<2:26:43, 27.86s/it]2022-01-09 23:44:46,564 iteration 1429 : loss : 0.087402, loss_ce: 0.048488
2022-01-09 23:44:48,099 iteration 1430 : loss : 0.208157, loss_ce: 0.068210
2022-01-09 23:44:49,674 iteration 1431 : loss : 0.106355, loss_ce: 0.056114
2022-01-09 23:44:51,264 iteration 1432 : loss : 0.067556, loss_ce: 0.032147
2022-01-09 23:44:52,893 iteration 1433 : loss : 0.088187, loss_ce: 0.031718
2022-01-09 23:44:54,448 iteration 1434 : loss : 0.065641, loss_ce: 0.030698
2022-01-09 23:44:56,049 iteration 1435 : loss : 0.065191, loss_ce: 0.028949
2022-01-09 23:44:57,663 iteration 1436 : loss : 0.103032, loss_ce: 0.042596
2022-01-09 23:44:59,284 iteration 1437 : loss : 0.082817, loss_ce: 0.032781
2022-01-09 23:45:00,870 iteration 1438 : loss : 0.063461, loss_ce: 0.029209
2022-01-09 23:45:02,501 iteration 1439 : loss : 0.072642, loss_ce: 0.032528
2022-01-09 23:45:04,121 iteration 1440 : loss : 0.070386, loss_ce: 0.032796
2022-01-09 23:45:05,686 iteration 1441 : loss : 0.056033, loss_ce: 0.022131
2022-01-09 23:45:07,272 iteration 1442 : loss : 0.055980, loss_ce: 0.024683
2022-01-09 23:45:08,881 iteration 1443 : loss : 0.080552, loss_ce: 0.029590
2022-01-09 23:45:10,467 iteration 1444 : loss : 0.139155, loss_ce: 0.036761
2022-01-09 23:45:10,467 Training Data Eval:
2022-01-09 23:45:18,549   Average segmentation loss on training set: 0.0499
2022-01-09 23:45:18,549 Validation Data Eval:
2022-01-09 23:45:21,320   Average segmentation loss on validation set: 0.1322
2022-01-09 23:45:22,899 iteration 1445 : loss : 0.082612, loss_ce: 0.025834
 21%|██████▍                       | 85/400 [41:57<2:42:16, 30.91s/it]2022-01-09 23:45:24,624 iteration 1446 : loss : 0.081215, loss_ce: 0.029543
2022-01-09 23:45:26,189 iteration 1447 : loss : 0.093768, loss_ce: 0.030436
2022-01-09 23:45:27,707 iteration 1448 : loss : 0.093566, loss_ce: 0.048122
2022-01-09 23:45:29,312 iteration 1449 : loss : 0.062104, loss_ce: 0.021133
2022-01-09 23:45:30,988 iteration 1450 : loss : 0.096335, loss_ce: 0.051968
2022-01-09 23:45:32,678 iteration 1451 : loss : 0.090040, loss_ce: 0.034754
2022-01-09 23:45:34,227 iteration 1452 : loss : 0.076525, loss_ce: 0.027020
2022-01-09 23:45:35,784 iteration 1453 : loss : 0.091249, loss_ce: 0.050630
2022-01-09 23:45:37,386 iteration 1454 : loss : 0.137694, loss_ce: 0.051837
2022-01-09 23:45:38,934 iteration 1455 : loss : 0.083037, loss_ce: 0.031605
2022-01-09 23:45:40,592 iteration 1456 : loss : 0.137884, loss_ce: 0.046957
2022-01-09 23:45:42,172 iteration 1457 : loss : 0.094931, loss_ce: 0.042862
2022-01-09 23:45:43,737 iteration 1458 : loss : 0.068849, loss_ce: 0.022994
2022-01-09 23:45:45,273 iteration 1459 : loss : 0.079693, loss_ce: 0.022121
2022-01-09 23:45:46,913 iteration 1460 : loss : 0.112919, loss_ce: 0.049385
2022-01-09 23:45:48,497 iteration 1461 : loss : 0.103208, loss_ce: 0.045326
2022-01-09 23:45:50,125 iteration 1462 : loss : 0.069287, loss_ce: 0.030884
 22%|██████▍                       | 86/400 [42:25<2:35:59, 29.81s/it]2022-01-09 23:45:51,750 iteration 1463 : loss : 0.079216, loss_ce: 0.036503
2022-01-09 23:45:53,303 iteration 1464 : loss : 0.079009, loss_ce: 0.031574
2022-01-09 23:45:54,909 iteration 1465 : loss : 0.099159, loss_ce: 0.028601
2022-01-09 23:45:56,520 iteration 1466 : loss : 0.070969, loss_ce: 0.030514
2022-01-09 23:45:58,108 iteration 1467 : loss : 0.081282, loss_ce: 0.036113
2022-01-09 23:45:59,664 iteration 1468 : loss : 0.051794, loss_ce: 0.023956
2022-01-09 23:46:01,239 iteration 1469 : loss : 0.081870, loss_ce: 0.037749
2022-01-09 23:46:02,850 iteration 1470 : loss : 0.100077, loss_ce: 0.034901
2022-01-09 23:46:04,451 iteration 1471 : loss : 0.093640, loss_ce: 0.059988
2022-01-09 23:46:05,931 iteration 1472 : loss : 0.060027, loss_ce: 0.023485
2022-01-09 23:46:07,591 iteration 1473 : loss : 0.069823, loss_ce: 0.029242
2022-01-09 23:46:09,215 iteration 1474 : loss : 0.075362, loss_ce: 0.028327
2022-01-09 23:46:10,790 iteration 1475 : loss : 0.063712, loss_ce: 0.025772
2022-01-09 23:46:12,453 iteration 1476 : loss : 0.149827, loss_ce: 0.045810
2022-01-09 23:46:14,078 iteration 1477 : loss : 0.090552, loss_ce: 0.030952
2022-01-09 23:46:15,740 iteration 1478 : loss : 0.074951, loss_ce: 0.028661
2022-01-09 23:46:17,280 iteration 1479 : loss : 0.075644, loss_ce: 0.026294
 22%|██████▌                       | 87/400 [42:52<2:31:20, 29.01s/it]2022-01-09 23:46:18,923 iteration 1480 : loss : 0.075906, loss_ce: 0.028786
2022-01-09 23:46:20,484 iteration 1481 : loss : 0.063468, loss_ce: 0.026383
2022-01-09 23:46:22,057 iteration 1482 : loss : 0.070122, loss_ce: 0.030695
2022-01-09 23:46:23,711 iteration 1483 : loss : 0.056999, loss_ce: 0.020530
2022-01-09 23:46:25,420 iteration 1484 : loss : 0.075122, loss_ce: 0.026644
2022-01-09 23:46:27,072 iteration 1485 : loss : 0.054481, loss_ce: 0.023737
2022-01-09 23:46:28,637 iteration 1486 : loss : 0.063866, loss_ce: 0.025314
2022-01-09 23:46:30,222 iteration 1487 : loss : 0.077298, loss_ce: 0.035716
2022-01-09 23:46:31,822 iteration 1488 : loss : 0.067982, loss_ce: 0.034009
2022-01-09 23:46:33,425 iteration 1489 : loss : 0.061629, loss_ce: 0.024487
2022-01-09 23:46:35,046 iteration 1490 : loss : 0.081777, loss_ce: 0.035090
2022-01-09 23:46:36,703 iteration 1491 : loss : 0.068850, loss_ce: 0.028728
2022-01-09 23:46:38,252 iteration 1492 : loss : 0.059848, loss_ce: 0.024253
2022-01-09 23:46:39,875 iteration 1493 : loss : 0.091318, loss_ce: 0.032650
2022-01-09 23:46:41,430 iteration 1494 : loss : 0.080961, loss_ce: 0.033514
2022-01-09 23:46:42,992 iteration 1495 : loss : 0.064017, loss_ce: 0.027617
2022-01-09 23:46:44,576 iteration 1496 : loss : 0.099106, loss_ce: 0.030319
 22%|██████▌                       | 88/400 [43:19<2:28:10, 28.50s/it]2022-01-09 23:46:46,168 iteration 1497 : loss : 0.105269, loss_ce: 0.056887
2022-01-09 23:46:47,719 iteration 1498 : loss : 0.072366, loss_ce: 0.022678
2022-01-09 23:46:49,333 iteration 1499 : loss : 0.075279, loss_ce: 0.033898
2022-01-09 23:46:50,847 iteration 1500 : loss : 0.089235, loss_ce: 0.032232
2022-01-09 23:46:52,409 iteration 1501 : loss : 0.056593, loss_ce: 0.023124
2022-01-09 23:46:54,018 iteration 1502 : loss : 0.075487, loss_ce: 0.029275
2022-01-09 23:46:55,579 iteration 1503 : loss : 0.058580, loss_ce: 0.028420
2022-01-09 23:46:57,140 iteration 1504 : loss : 0.072372, loss_ce: 0.027859
2022-01-09 23:46:58,657 iteration 1505 : loss : 0.048748, loss_ce: 0.018729
2022-01-09 23:47:00,222 iteration 1506 : loss : 0.064931, loss_ce: 0.025858
2022-01-09 23:47:01,798 iteration 1507 : loss : 0.088618, loss_ce: 0.030203
2022-01-09 23:47:03,347 iteration 1508 : loss : 0.073666, loss_ce: 0.036106
2022-01-09 23:47:04,913 iteration 1509 : loss : 0.077648, loss_ce: 0.030988
2022-01-09 23:47:06,446 iteration 1510 : loss : 0.050782, loss_ce: 0.021427
2022-01-09 23:47:08,039 iteration 1511 : loss : 0.084058, loss_ce: 0.042102
2022-01-09 23:47:09,578 iteration 1512 : loss : 0.058615, loss_ce: 0.023694
2022-01-09 23:47:11,168 iteration 1513 : loss : 0.073211, loss_ce: 0.035213
 22%|██████▋                       | 89/400 [43:46<2:24:44, 27.92s/it]2022-01-09 23:47:12,777 iteration 1514 : loss : 0.066075, loss_ce: 0.024309
2022-01-09 23:47:14,352 iteration 1515 : loss : 0.080164, loss_ce: 0.033838
2022-01-09 23:47:15,951 iteration 1516 : loss : 0.072856, loss_ce: 0.025631
2022-01-09 23:47:17,521 iteration 1517 : loss : 0.049340, loss_ce: 0.023598
2022-01-09 23:47:19,128 iteration 1518 : loss : 0.050000, loss_ce: 0.017279
2022-01-09 23:47:20,681 iteration 1519 : loss : 0.055919, loss_ce: 0.022217
2022-01-09 23:47:22,229 iteration 1520 : loss : 0.056437, loss_ce: 0.020154
2022-01-09 23:47:23,781 iteration 1521 : loss : 0.074492, loss_ce: 0.026505
2022-01-09 23:47:25,455 iteration 1522 : loss : 0.058611, loss_ce: 0.022410
2022-01-09 23:47:26,981 iteration 1523 : loss : 0.072352, loss_ce: 0.032714
2022-01-09 23:47:28,579 iteration 1524 : loss : 0.085112, loss_ce: 0.033961
2022-01-09 23:47:30,129 iteration 1525 : loss : 0.052819, loss_ce: 0.022697
2022-01-09 23:47:31,765 iteration 1526 : loss : 0.057331, loss_ce: 0.026254
2022-01-09 23:47:33,404 iteration 1527 : loss : 0.124591, loss_ce: 0.068525
2022-01-09 23:47:35,024 iteration 1528 : loss : 0.065588, loss_ce: 0.026206
2022-01-09 23:47:36,570 iteration 1529 : loss : 0.052043, loss_ce: 0.020454
2022-01-09 23:47:36,570 Training Data Eval:
2022-01-09 23:47:44,656   Average segmentation loss on training set: 0.0590
2022-01-09 23:47:44,656 Validation Data Eval:
2022-01-09 23:47:47,434   Average segmentation loss on validation set: 0.1067
2022-01-09 23:47:48,995 iteration 1530 : loss : 0.069835, loss_ce: 0.032781
 22%|██████▊                       | 90/400 [44:24<2:39:38, 30.90s/it]2022-01-09 23:47:50,752 iteration 1531 : loss : 0.092305, loss_ce: 0.036400
2022-01-09 23:47:52,377 iteration 1532 : loss : 0.072810, loss_ce: 0.029166
2022-01-09 23:47:53,918 iteration 1533 : loss : 0.066777, loss_ce: 0.029620
2022-01-09 23:47:55,545 iteration 1534 : loss : 0.146502, loss_ce: 0.057902
2022-01-09 23:47:57,107 iteration 1535 : loss : 0.055347, loss_ce: 0.022503
2022-01-09 23:47:58,651 iteration 1536 : loss : 0.080364, loss_ce: 0.042281
2022-01-09 23:48:00,192 iteration 1537 : loss : 0.091246, loss_ce: 0.032686
2022-01-09 23:48:01,714 iteration 1538 : loss : 0.200598, loss_ce: 0.045183
2022-01-09 23:48:03,283 iteration 1539 : loss : 0.054878, loss_ce: 0.020624
2022-01-09 23:48:04,858 iteration 1540 : loss : 0.073745, loss_ce: 0.036601
2022-01-09 23:48:06,347 iteration 1541 : loss : 0.066153, loss_ce: 0.023287
2022-01-09 23:48:07,925 iteration 1542 : loss : 0.069990, loss_ce: 0.032477
2022-01-09 23:48:09,476 iteration 1543 : loss : 0.072404, loss_ce: 0.024408
2022-01-09 23:48:10,986 iteration 1544 : loss : 0.053828, loss_ce: 0.023370
2022-01-09 23:48:12,547 iteration 1545 : loss : 0.081151, loss_ce: 0.034479
2022-01-09 23:48:14,102 iteration 1546 : loss : 0.063634, loss_ce: 0.029285
2022-01-09 23:48:15,652 iteration 1547 : loss : 0.071240, loss_ce: 0.028867
 23%|██████▊                       | 91/400 [44:50<2:32:33, 29.62s/it]2022-01-09 23:48:17,277 iteration 1548 : loss : 0.052725, loss_ce: 0.021082
2022-01-09 23:48:18,791 iteration 1549 : loss : 0.051743, loss_ce: 0.023515
2022-01-09 23:48:20,453 iteration 1550 : loss : 0.098382, loss_ce: 0.041859
2022-01-09 23:48:22,024 iteration 1551 : loss : 0.071543, loss_ce: 0.030166
2022-01-09 23:48:23,601 iteration 1552 : loss : 0.063372, loss_ce: 0.029033
2022-01-09 23:48:25,093 iteration 1553 : loss : 0.046569, loss_ce: 0.025299
2022-01-09 23:48:26,622 iteration 1554 : loss : 0.080783, loss_ce: 0.031852
2022-01-09 23:48:28,250 iteration 1555 : loss : 0.074049, loss_ce: 0.028820
2022-01-09 23:48:29,944 iteration 1556 : loss : 0.087756, loss_ce: 0.034946
2022-01-09 23:48:31,583 iteration 1557 : loss : 0.075960, loss_ce: 0.022624
2022-01-09 23:48:33,168 iteration 1558 : loss : 0.070522, loss_ce: 0.022372
2022-01-09 23:48:34,843 iteration 1559 : loss : 0.071635, loss_ce: 0.030189
2022-01-09 23:48:36,401 iteration 1560 : loss : 0.067121, loss_ce: 0.020958
2022-01-09 23:48:37,960 iteration 1561 : loss : 0.069048, loss_ce: 0.026292
2022-01-09 23:48:39,546 iteration 1562 : loss : 0.054301, loss_ce: 0.023351
2022-01-09 23:48:41,111 iteration 1563 : loss : 0.066479, loss_ce: 0.035793
2022-01-09 23:48:42,694 iteration 1564 : loss : 0.062630, loss_ce: 0.020667
 23%|██████▉                       | 92/400 [45:17<2:28:05, 28.85s/it]2022-01-09 23:48:44,354 iteration 1565 : loss : 0.044171, loss_ce: 0.017772
2022-01-09 23:48:45,911 iteration 1566 : loss : 0.077797, loss_ce: 0.034973
2022-01-09 23:48:47,524 iteration 1567 : loss : 0.084294, loss_ce: 0.022178
2022-01-09 23:48:49,201 iteration 1568 : loss : 0.063069, loss_ce: 0.031105
2022-01-09 23:48:50,756 iteration 1569 : loss : 0.052317, loss_ce: 0.022003
2022-01-09 23:48:52,343 iteration 1570 : loss : 0.067225, loss_ce: 0.027535
2022-01-09 23:48:53,907 iteration 1571 : loss : 0.058633, loss_ce: 0.026042
2022-01-09 23:48:55,478 iteration 1572 : loss : 0.065049, loss_ce: 0.029148
2022-01-09 23:48:57,063 iteration 1573 : loss : 0.105154, loss_ce: 0.030870
2022-01-09 23:48:58,648 iteration 1574 : loss : 0.058111, loss_ce: 0.025964
2022-01-09 23:49:00,249 iteration 1575 : loss : 0.088334, loss_ce: 0.039935
2022-01-09 23:49:01,811 iteration 1576 : loss : 0.063492, loss_ce: 0.021279
2022-01-09 23:49:03,349 iteration 1577 : loss : 0.058372, loss_ce: 0.020133
2022-01-09 23:49:04,830 iteration 1578 : loss : 0.051778, loss_ce: 0.022688
2022-01-09 23:49:06,391 iteration 1579 : loss : 0.056676, loss_ce: 0.024536
2022-01-09 23:49:07,982 iteration 1580 : loss : 0.059390, loss_ce: 0.019318
2022-01-09 23:49:09,580 iteration 1581 : loss : 0.047022, loss_ce: 0.018910
 23%|██████▉                       | 93/400 [45:44<2:24:36, 28.26s/it]2022-01-09 23:49:11,251 iteration 1582 : loss : 0.050647, loss_ce: 0.023759
2022-01-09 23:49:12,831 iteration 1583 : loss : 0.069892, loss_ce: 0.020812
2022-01-09 23:49:14,363 iteration 1584 : loss : 0.062421, loss_ce: 0.022440
2022-01-09 23:49:15,920 iteration 1585 : loss : 0.046147, loss_ce: 0.020603
2022-01-09 23:49:17,415 iteration 1586 : loss : 0.058907, loss_ce: 0.028281
2022-01-09 23:49:18,957 iteration 1587 : loss : 0.080741, loss_ce: 0.030501
2022-01-09 23:49:20,510 iteration 1588 : loss : 0.055153, loss_ce: 0.017783
2022-01-09 23:49:22,081 iteration 1589 : loss : 0.053814, loss_ce: 0.022538
2022-01-09 23:49:23,731 iteration 1590 : loss : 0.086504, loss_ce: 0.026429
2022-01-09 23:49:25,281 iteration 1591 : loss : 0.051632, loss_ce: 0.026767
2022-01-09 23:49:26,771 iteration 1592 : loss : 0.038748, loss_ce: 0.017069
2022-01-09 23:49:28,348 iteration 1593 : loss : 0.084708, loss_ce: 0.029432
2022-01-09 23:49:29,907 iteration 1594 : loss : 0.070679, loss_ce: 0.024694
2022-01-09 23:49:31,426 iteration 1595 : loss : 0.043755, loss_ce: 0.022954
2022-01-09 23:49:32,915 iteration 1596 : loss : 0.071280, loss_ce: 0.023434
2022-01-09 23:49:34,497 iteration 1597 : loss : 0.112642, loss_ce: 0.027993
2022-01-09 23:49:36,097 iteration 1598 : loss : 0.065616, loss_ce: 0.024787
 24%|███████                       | 94/400 [46:11<2:21:27, 27.74s/it]2022-01-09 23:49:37,769 iteration 1599 : loss : 0.055897, loss_ce: 0.026827
2022-01-09 23:49:39,322 iteration 1600 : loss : 0.051018, loss_ce: 0.022559
2022-01-09 23:49:40,928 iteration 1601 : loss : 0.066212, loss_ce: 0.024387
2022-01-09 23:49:42,515 iteration 1602 : loss : 0.068975, loss_ce: 0.023501
2022-01-09 23:49:44,077 iteration 1603 : loss : 0.040634, loss_ce: 0.015054
2022-01-09 23:49:45,625 iteration 1604 : loss : 0.068802, loss_ce: 0.019146
2022-01-09 23:49:47,180 iteration 1605 : loss : 0.062786, loss_ce: 0.024253
2022-01-09 23:49:48,760 iteration 1606 : loss : 0.048872, loss_ce: 0.019435
2022-01-09 23:49:50,327 iteration 1607 : loss : 0.063072, loss_ce: 0.024579
2022-01-09 23:49:51,976 iteration 1608 : loss : 0.126242, loss_ce: 0.059267
2022-01-09 23:49:53,531 iteration 1609 : loss : 0.049021, loss_ce: 0.018542
2022-01-09 23:49:55,191 iteration 1610 : loss : 0.078981, loss_ce: 0.034520
2022-01-09 23:49:56,845 iteration 1611 : loss : 0.071142, loss_ce: 0.033433
2022-01-09 23:49:58,516 iteration 1612 : loss : 0.048878, loss_ce: 0.019183
2022-01-09 23:50:00,100 iteration 1613 : loss : 0.060498, loss_ce: 0.021857
2022-01-09 23:50:01,734 iteration 1614 : loss : 0.068190, loss_ce: 0.030554
2022-01-09 23:50:01,734 Training Data Eval:
2022-01-09 23:50:09,794   Average segmentation loss on training set: 0.0390
2022-01-09 23:50:09,795 Validation Data Eval:
2022-01-09 23:50:12,572   Average segmentation loss on validation set: 0.0946
2022-01-09 23:50:14,192 iteration 1615 : loss : 0.077894, loss_ce: 0.026817
 24%|███████▏                      | 95/400 [46:49<2:36:47, 30.85s/it]2022-01-09 23:50:15,834 iteration 1616 : loss : 0.059917, loss_ce: 0.024930
2022-01-09 23:50:17,394 iteration 1617 : loss : 0.054328, loss_ce: 0.025240
2022-01-09 23:50:18,915 iteration 1618 : loss : 0.057954, loss_ce: 0.019983
2022-01-09 23:50:20,482 iteration 1619 : loss : 0.069279, loss_ce: 0.037638
2022-01-09 23:50:22,169 iteration 1620 : loss : 0.065084, loss_ce: 0.021266
2022-01-09 23:50:23,860 iteration 1621 : loss : 0.049419, loss_ce: 0.020318
2022-01-09 23:50:25,437 iteration 1622 : loss : 0.073723, loss_ce: 0.033236
2022-01-09 23:50:26,969 iteration 1623 : loss : 0.066483, loss_ce: 0.027974
2022-01-09 23:50:28,565 iteration 1624 : loss : 0.088262, loss_ce: 0.036031
2022-01-09 23:50:30,079 iteration 1625 : loss : 0.038152, loss_ce: 0.013477
2022-01-09 23:50:31,631 iteration 1626 : loss : 0.043038, loss_ce: 0.018444
2022-01-09 23:50:33,318 iteration 1627 : loss : 0.089138, loss_ce: 0.033589
2022-01-09 23:50:34,985 iteration 1628 : loss : 0.059726, loss_ce: 0.027566
2022-01-09 23:50:36,535 iteration 1629 : loss : 0.051266, loss_ce: 0.021144
2022-01-09 23:50:38,065 iteration 1630 : loss : 0.042866, loss_ce: 0.018007
2022-01-09 23:50:39,597 iteration 1631 : loss : 0.082042, loss_ce: 0.026441
2022-01-09 23:50:41,141 iteration 1632 : loss : 0.054626, loss_ce: 0.021677
 24%|███████▏                      | 96/400 [47:16<2:30:21, 29.67s/it]2022-01-09 23:50:42,773 iteration 1633 : loss : 0.068608, loss_ce: 0.022104
2022-01-09 23:50:44,371 iteration 1634 : loss : 0.054732, loss_ce: 0.019826
2022-01-09 23:50:45,921 iteration 1635 : loss : 0.077203, loss_ce: 0.022986
2022-01-09 23:50:47,424 iteration 1636 : loss : 0.055643, loss_ce: 0.026250
2022-01-09 23:50:49,014 iteration 1637 : loss : 0.061359, loss_ce: 0.020217
2022-01-09 23:50:50,580 iteration 1638 : loss : 0.080392, loss_ce: 0.033555
2022-01-09 23:50:52,208 iteration 1639 : loss : 0.069002, loss_ce: 0.029965
2022-01-09 23:50:53,750 iteration 1640 : loss : 0.073877, loss_ce: 0.025760
2022-01-09 23:50:55,398 iteration 1641 : loss : 0.068867, loss_ce: 0.026817
2022-01-09 23:50:56,928 iteration 1642 : loss : 0.049460, loss_ce: 0.018198
2022-01-09 23:50:58,438 iteration 1643 : loss : 0.058151, loss_ce: 0.019073
2022-01-09 23:51:00,066 iteration 1644 : loss : 0.066137, loss_ce: 0.027720
2022-01-09 23:51:01,728 iteration 1645 : loss : 0.064165, loss_ce: 0.027159
2022-01-09 23:51:03,268 iteration 1646 : loss : 0.079380, loss_ce: 0.030686
2022-01-09 23:51:04,824 iteration 1647 : loss : 0.092577, loss_ce: 0.034112
2022-01-09 23:51:06,317 iteration 1648 : loss : 0.049358, loss_ce: 0.020913
2022-01-09 23:51:07,828 iteration 1649 : loss : 0.049337, loss_ce: 0.019011
 24%|███████▎                      | 97/400 [47:42<2:25:20, 28.78s/it]2022-01-09 23:51:09,571 iteration 1650 : loss : 0.084552, loss_ce: 0.043774
2022-01-09 23:51:11,130 iteration 1651 : loss : 0.106221, loss_ce: 0.034308
2022-01-09 23:51:12,719 iteration 1652 : loss : 0.090543, loss_ce: 0.034156
2022-01-09 23:51:14,265 iteration 1653 : loss : 0.077312, loss_ce: 0.026485
2022-01-09 23:51:15,825 iteration 1654 : loss : 0.074526, loss_ce: 0.024431
2022-01-09 23:51:17,351 iteration 1655 : loss : 0.071686, loss_ce: 0.026506
2022-01-09 23:51:18,967 iteration 1656 : loss : 0.069981, loss_ce: 0.029792
2022-01-09 23:51:20,527 iteration 1657 : loss : 0.065108, loss_ce: 0.029524
2022-01-09 23:51:22,151 iteration 1658 : loss : 0.089479, loss_ce: 0.055008
2022-01-09 23:51:23,804 iteration 1659 : loss : 0.052076, loss_ce: 0.024573
2022-01-09 23:51:25,387 iteration 1660 : loss : 0.042915, loss_ce: 0.017857
2022-01-09 23:51:26,908 iteration 1661 : loss : 0.062616, loss_ce: 0.023425
2022-01-09 23:51:28,518 iteration 1662 : loss : 0.069793, loss_ce: 0.026538
2022-01-09 23:51:30,151 iteration 1663 : loss : 0.056348, loss_ce: 0.027570
2022-01-09 23:51:31,732 iteration 1664 : loss : 0.075571, loss_ce: 0.027705
2022-01-09 23:51:33,292 iteration 1665 : loss : 0.054854, loss_ce: 0.023396
2022-01-09 23:51:34,880 iteration 1666 : loss : 0.069168, loss_ce: 0.019660
 24%|███████▎                      | 98/400 [48:09<2:22:13, 28.26s/it]2022-01-09 23:51:36,462 iteration 1667 : loss : 0.078176, loss_ce: 0.038973
2022-01-09 23:51:38,108 iteration 1668 : loss : 0.069896, loss_ce: 0.023099
2022-01-09 23:51:39,720 iteration 1669 : loss : 0.061410, loss_ce: 0.022205
2022-01-09 23:51:41,224 iteration 1670 : loss : 0.039429, loss_ce: 0.012550
2022-01-09 23:51:42,746 iteration 1671 : loss : 0.057546, loss_ce: 0.025496
2022-01-09 23:51:44,380 iteration 1672 : loss : 0.073682, loss_ce: 0.027443
2022-01-09 23:51:45,994 iteration 1673 : loss : 0.055383, loss_ce: 0.018370
2022-01-09 23:51:47,663 iteration 1674 : loss : 0.086544, loss_ce: 0.036497
2022-01-09 23:51:49,219 iteration 1675 : loss : 0.070560, loss_ce: 0.022519
2022-01-09 23:51:50,849 iteration 1676 : loss : 0.062879, loss_ce: 0.030962
2022-01-09 23:51:52,445 iteration 1677 : loss : 0.112863, loss_ce: 0.029695
2022-01-09 23:51:54,062 iteration 1678 : loss : 0.072725, loss_ce: 0.030986
2022-01-09 23:51:55,624 iteration 1679 : loss : 0.090428, loss_ce: 0.049326
2022-01-09 23:51:57,229 iteration 1680 : loss : 0.056448, loss_ce: 0.029116
2022-01-09 23:51:58,794 iteration 1681 : loss : 0.071709, loss_ce: 0.029503
2022-01-09 23:52:00,371 iteration 1682 : loss : 0.051418, loss_ce: 0.022167
2022-01-09 23:52:01,915 iteration 1683 : loss : 0.060626, loss_ce: 0.026401
 25%|███████▍                      | 99/400 [48:37<2:19:56, 27.90s/it]2022-01-09 23:52:03,580 iteration 1684 : loss : 0.066359, loss_ce: 0.027719
2022-01-09 23:52:05,121 iteration 1685 : loss : 0.050228, loss_ce: 0.025003
2022-01-09 23:52:06,628 iteration 1686 : loss : 0.046775, loss_ce: 0.018269
2022-01-09 23:52:08,173 iteration 1687 : loss : 0.063851, loss_ce: 0.024537
2022-01-09 23:52:09,754 iteration 1688 : loss : 0.060501, loss_ce: 0.028270
2022-01-09 23:52:11,288 iteration 1689 : loss : 0.069205, loss_ce: 0.022815
2022-01-09 23:52:12,886 iteration 1690 : loss : 0.100075, loss_ce: 0.044773
2022-01-09 23:52:14,445 iteration 1691 : loss : 0.042544, loss_ce: 0.016593
2022-01-09 23:52:16,061 iteration 1692 : loss : 0.082181, loss_ce: 0.032897
2022-01-09 23:52:17,701 iteration 1693 : loss : 0.054476, loss_ce: 0.026670
2022-01-09 23:52:19,307 iteration 1694 : loss : 0.065546, loss_ce: 0.025503
2022-01-09 23:52:20,809 iteration 1695 : loss : 0.056921, loss_ce: 0.020331
2022-01-09 23:52:22,368 iteration 1696 : loss : 0.079743, loss_ce: 0.028682
2022-01-09 23:52:23,983 iteration 1697 : loss : 0.079515, loss_ce: 0.025581
2022-01-09 23:52:25,541 iteration 1698 : loss : 0.045784, loss_ce: 0.014680
2022-01-09 23:52:27,013 iteration 1699 : loss : 0.061691, loss_ce: 0.031673
2022-01-09 23:52:27,013 Training Data Eval:
2022-01-09 23:52:35,090   Average segmentation loss on training set: 0.0575
2022-01-09 23:52:35,091 Validation Data Eval:
2022-01-09 23:52:37,867   Average segmentation loss on validation set: 0.1919
2022-01-09 23:52:39,454 iteration 1700 : loss : 0.058469, loss_ce: 0.018765
 25%|███████▎                     | 100/400 [49:14<2:33:55, 30.79s/it]2022-01-09 23:52:41,118 iteration 1701 : loss : 0.054345, loss_ce: 0.023686
2022-01-09 23:52:42,699 iteration 1702 : loss : 0.053667, loss_ce: 0.023293
2022-01-09 23:52:44,222 iteration 1703 : loss : 0.057617, loss_ce: 0.023756
2022-01-09 23:52:45,866 iteration 1704 : loss : 0.053711, loss_ce: 0.022047
2022-01-09 23:52:47,440 iteration 1705 : loss : 0.052451, loss_ce: 0.024536
2022-01-09 23:52:49,029 iteration 1706 : loss : 0.050648, loss_ce: 0.022284
2022-01-09 23:52:50,599 iteration 1707 : loss : 0.060923, loss_ce: 0.023199
2022-01-09 23:52:52,205 iteration 1708 : loss : 0.044500, loss_ce: 0.014448
2022-01-09 23:52:53,713 iteration 1709 : loss : 0.045291, loss_ce: 0.019259
2022-01-09 23:52:55,205 iteration 1710 : loss : 0.040077, loss_ce: 0.018522
2022-01-09 23:52:56,790 iteration 1711 : loss : 0.057664, loss_ce: 0.022545
2022-01-09 23:52:58,313 iteration 1712 : loss : 0.049931, loss_ce: 0.018770
2022-01-09 23:52:59,827 iteration 1713 : loss : 0.060807, loss_ce: 0.022678
2022-01-09 23:53:01,476 iteration 1714 : loss : 0.064788, loss_ce: 0.023272
2022-01-09 23:53:03,084 iteration 1715 : loss : 0.079575, loss_ce: 0.023390
2022-01-09 23:53:04,733 iteration 1716 : loss : 0.069895, loss_ce: 0.023281
2022-01-09 23:53:06,306 iteration 1717 : loss : 0.049938, loss_ce: 0.018949
 25%|███████▎                     | 101/400 [49:41<2:27:32, 29.61s/it]2022-01-09 23:53:07,906 iteration 1718 : loss : 0.055823, loss_ce: 0.019348
2022-01-09 23:53:09,498 iteration 1719 : loss : 0.073212, loss_ce: 0.029683
2022-01-09 23:53:10,980 iteration 1720 : loss : 0.042019, loss_ce: 0.013873
2022-01-09 23:53:12,542 iteration 1721 : loss : 0.058418, loss_ce: 0.025066
2022-01-09 23:53:14,163 iteration 1722 : loss : 0.060962, loss_ce: 0.022575
2022-01-09 23:53:15,734 iteration 1723 : loss : 0.085240, loss_ce: 0.032687
2022-01-09 23:53:17,346 iteration 1724 : loss : 0.058226, loss_ce: 0.020805
2022-01-09 23:53:19,039 iteration 1725 : loss : 0.080747, loss_ce: 0.028620
2022-01-09 23:53:20,690 iteration 1726 : loss : 0.080428, loss_ce: 0.021256
2022-01-09 23:53:22,277 iteration 1727 : loss : 0.056186, loss_ce: 0.032423
2022-01-09 23:53:23,840 iteration 1728 : loss : 0.054230, loss_ce: 0.018348
2022-01-09 23:53:25,409 iteration 1729 : loss : 0.050652, loss_ce: 0.024486
2022-01-09 23:53:27,003 iteration 1730 : loss : 0.044495, loss_ce: 0.018862
2022-01-09 23:53:28,645 iteration 1731 : loss : 0.062911, loss_ce: 0.022130
2022-01-09 23:53:30,228 iteration 1732 : loss : 0.048501, loss_ce: 0.024193
2022-01-09 23:53:31,879 iteration 1733 : loss : 0.077345, loss_ce: 0.034346
2022-01-09 23:53:33,416 iteration 1734 : loss : 0.043145, loss_ce: 0.016018
 26%|███████▍                     | 102/400 [50:08<2:23:19, 28.86s/it]2022-01-09 23:53:35,014 iteration 1735 : loss : 0.079674, loss_ce: 0.026152
2022-01-09 23:53:36,579 iteration 1736 : loss : 0.043111, loss_ce: 0.018498
2022-01-09 23:53:38,154 iteration 1737 : loss : 0.052213, loss_ce: 0.019926
2022-01-09 23:53:39,700 iteration 1738 : loss : 0.049673, loss_ce: 0.018119
2022-01-09 23:53:41,365 iteration 1739 : loss : 0.061378, loss_ce: 0.028404
2022-01-09 23:53:42,980 iteration 1740 : loss : 0.069681, loss_ce: 0.024930
2022-01-09 23:53:44,550 iteration 1741 : loss : 0.050489, loss_ce: 0.018842
2022-01-09 23:53:46,230 iteration 1742 : loss : 0.081753, loss_ce: 0.039032
2022-01-09 23:53:47,796 iteration 1743 : loss : 0.060379, loss_ce: 0.021965
2022-01-09 23:53:49,398 iteration 1744 : loss : 0.048543, loss_ce: 0.018653
2022-01-09 23:53:51,118 iteration 1745 : loss : 0.097953, loss_ce: 0.058835
2022-01-09 23:53:52,688 iteration 1746 : loss : 0.040021, loss_ce: 0.017561
2022-01-09 23:53:54,246 iteration 1747 : loss : 0.053611, loss_ce: 0.023359
2022-01-09 23:53:55,829 iteration 1748 : loss : 0.042555, loss_ce: 0.017241
2022-01-09 23:53:57,408 iteration 1749 : loss : 0.070797, loss_ce: 0.021247
2022-01-09 23:53:58,933 iteration 1750 : loss : 0.047214, loss_ce: 0.022062
2022-01-09 23:54:00,600 iteration 1751 : loss : 0.097381, loss_ce: 0.038883
 26%|███████▍                     | 103/400 [50:35<2:20:20, 28.35s/it]2022-01-09 23:54:02,205 iteration 1752 : loss : 0.047062, loss_ce: 0.019052
2022-01-09 23:54:03,745 iteration 1753 : loss : 0.055130, loss_ce: 0.025020
2022-01-09 23:54:05,299 iteration 1754 : loss : 0.039527, loss_ce: 0.017327
2022-01-09 23:54:06,868 iteration 1755 : loss : 0.066370, loss_ce: 0.028701
2022-01-09 23:54:08,458 iteration 1756 : loss : 0.071038, loss_ce: 0.034793
2022-01-09 23:54:10,102 iteration 1757 : loss : 0.068973, loss_ce: 0.034382
2022-01-09 23:54:11,718 iteration 1758 : loss : 0.092120, loss_ce: 0.035474
2022-01-09 23:54:13,225 iteration 1759 : loss : 0.101852, loss_ce: 0.038089
2022-01-09 23:54:14,789 iteration 1760 : loss : 0.064529, loss_ce: 0.027743
2022-01-09 23:54:16,325 iteration 1761 : loss : 0.041059, loss_ce: 0.017484
2022-01-09 23:54:17,895 iteration 1762 : loss : 0.052468, loss_ce: 0.020840
2022-01-09 23:54:19,391 iteration 1763 : loss : 0.051482, loss_ce: 0.020885
2022-01-09 23:54:21,052 iteration 1764 : loss : 0.051569, loss_ce: 0.014234
2022-01-09 23:54:22,634 iteration 1765 : loss : 0.056458, loss_ce: 0.018373
2022-01-09 23:54:24,267 iteration 1766 : loss : 0.044295, loss_ce: 0.016581
2022-01-09 23:54:25,865 iteration 1767 : loss : 0.058665, loss_ce: 0.020961
2022-01-09 23:54:27,435 iteration 1768 : loss : 0.072416, loss_ce: 0.026572
 26%|███████▌                     | 104/400 [51:02<2:17:38, 27.90s/it]2022-01-09 23:54:29,104 iteration 1769 : loss : 0.059248, loss_ce: 0.022502
2022-01-09 23:54:30,668 iteration 1770 : loss : 0.046661, loss_ce: 0.015866
2022-01-09 23:54:32,202 iteration 1771 : loss : 0.038735, loss_ce: 0.012490
2022-01-09 23:54:33,837 iteration 1772 : loss : 0.073050, loss_ce: 0.040319
2022-01-09 23:54:35,436 iteration 1773 : loss : 0.061698, loss_ce: 0.025481
2022-01-09 23:54:36,982 iteration 1774 : loss : 0.054583, loss_ce: 0.017869
2022-01-09 23:54:38,537 iteration 1775 : loss : 0.054726, loss_ce: 0.022688
2022-01-09 23:54:40,081 iteration 1776 : loss : 0.058040, loss_ce: 0.021494
2022-01-09 23:54:41,604 iteration 1777 : loss : 0.047051, loss_ce: 0.018054
2022-01-09 23:54:43,219 iteration 1778 : loss : 0.056730, loss_ce: 0.025620
2022-01-09 23:54:44,798 iteration 1779 : loss : 0.048875, loss_ce: 0.021341
2022-01-09 23:54:46,398 iteration 1780 : loss : 0.100083, loss_ce: 0.035354
2022-01-09 23:54:47,998 iteration 1781 : loss : 0.053535, loss_ce: 0.018986
2022-01-09 23:54:49,506 iteration 1782 : loss : 0.072074, loss_ce: 0.022104
2022-01-09 23:54:51,043 iteration 1783 : loss : 0.057142, loss_ce: 0.024318
2022-01-09 23:54:52,642 iteration 1784 : loss : 0.082041, loss_ce: 0.041320
2022-01-09 23:54:52,642 Training Data Eval:
2022-01-09 23:55:00,711   Average segmentation loss on training set: 0.0488
2022-01-09 23:55:00,711 Validation Data Eval:
2022-01-09 23:55:03,483   Average segmentation loss on validation set: 0.1437
2022-01-09 23:55:05,052 iteration 1785 : loss : 0.050525, loss_ce: 0.021780
 26%|███████▌                     | 105/400 [51:40<2:31:30, 30.81s/it]2022-01-09 23:55:06,720 iteration 1786 : loss : 0.057375, loss_ce: 0.017140
2022-01-09 23:55:08,244 iteration 1787 : loss : 0.050683, loss_ce: 0.012067
2022-01-09 23:55:09,829 iteration 1788 : loss : 0.066778, loss_ce: 0.021786
2022-01-09 23:55:11,420 iteration 1789 : loss : 0.060705, loss_ce: 0.030863
2022-01-09 23:55:13,134 iteration 1790 : loss : 0.054632, loss_ce: 0.020683
2022-01-09 23:55:14,732 iteration 1791 : loss : 0.045999, loss_ce: 0.020986
2022-01-09 23:55:16,333 iteration 1792 : loss : 0.042704, loss_ce: 0.015963
2022-01-09 23:55:17,943 iteration 1793 : loss : 0.090834, loss_ce: 0.048774
2022-01-09 23:55:19,513 iteration 1794 : loss : 0.054427, loss_ce: 0.027567
2022-01-09 23:55:20,994 iteration 1795 : loss : 0.044151, loss_ce: 0.017699
2022-01-09 23:55:22,643 iteration 1796 : loss : 0.056357, loss_ce: 0.024801
2022-01-09 23:55:24,175 iteration 1797 : loss : 0.048255, loss_ce: 0.020771
2022-01-09 23:55:25,759 iteration 1798 : loss : 0.079115, loss_ce: 0.028623
2022-01-09 23:55:27,398 iteration 1799 : loss : 0.054073, loss_ce: 0.022670
2022-01-09 23:55:28,921 iteration 1800 : loss : 0.053435, loss_ce: 0.019170
2022-01-09 23:55:30,497 iteration 1801 : loss : 0.116424, loss_ce: 0.041656
2022-01-09 23:55:32,121 iteration 1802 : loss : 0.060136, loss_ce: 0.024006
 26%|███████▋                     | 106/400 [52:07<2:25:28, 29.69s/it]2022-01-09 23:55:33,797 iteration 1803 : loss : 0.062105, loss_ce: 0.025567
2022-01-09 23:55:35,314 iteration 1804 : loss : 0.063705, loss_ce: 0.022376
2022-01-09 23:55:37,009 iteration 1805 : loss : 0.087449, loss_ce: 0.030587
2022-01-09 23:55:38,670 iteration 1806 : loss : 0.065703, loss_ce: 0.031044
2022-01-09 23:55:40,243 iteration 1807 : loss : 0.052397, loss_ce: 0.022980
2022-01-09 23:55:41,863 iteration 1808 : loss : 0.042536, loss_ce: 0.018367
2022-01-09 23:55:43,419 iteration 1809 : loss : 0.049409, loss_ce: 0.021657
2022-01-09 23:55:45,026 iteration 1810 : loss : 0.064486, loss_ce: 0.031237
2022-01-09 23:55:46,590 iteration 1811 : loss : 0.057578, loss_ce: 0.021808
2022-01-09 23:55:48,144 iteration 1812 : loss : 0.049455, loss_ce: 0.017788
2022-01-09 23:55:49,723 iteration 1813 : loss : 0.066014, loss_ce: 0.028710
2022-01-09 23:55:51,280 iteration 1814 : loss : 0.075305, loss_ce: 0.028148
2022-01-09 23:55:52,980 iteration 1815 : loss : 0.082262, loss_ce: 0.024547
2022-01-09 23:55:54,573 iteration 1816 : loss : 0.061261, loss_ce: 0.025255
2022-01-09 23:55:56,223 iteration 1817 : loss : 0.074710, loss_ce: 0.029902
2022-01-09 23:55:57,749 iteration 1818 : loss : 0.043040, loss_ce: 0.016972
2022-01-09 23:55:59,377 iteration 1819 : loss : 0.094894, loss_ce: 0.031534
 27%|███████▊                     | 107/400 [52:34<2:21:25, 28.96s/it]2022-01-09 23:56:00,911 iteration 1820 : loss : 0.045365, loss_ce: 0.020226
2022-01-09 23:56:02,601 iteration 1821 : loss : 0.057862, loss_ce: 0.021320
2022-01-09 23:56:04,223 iteration 1822 : loss : 0.064916, loss_ce: 0.030721
2022-01-09 23:56:05,876 iteration 1823 : loss : 0.067799, loss_ce: 0.026064
2022-01-09 23:56:07,486 iteration 1824 : loss : 0.065403, loss_ce: 0.018482
2022-01-09 23:56:09,025 iteration 1825 : loss : 0.052235, loss_ce: 0.019975
2022-01-09 23:56:10,667 iteration 1826 : loss : 0.080396, loss_ce: 0.024953
2022-01-09 23:56:12,203 iteration 1827 : loss : 0.047504, loss_ce: 0.018233
2022-01-09 23:56:13,792 iteration 1828 : loss : 0.074857, loss_ce: 0.032926
2022-01-09 23:56:15,340 iteration 1829 : loss : 0.060851, loss_ce: 0.027728
2022-01-09 23:56:17,003 iteration 1830 : loss : 0.068572, loss_ce: 0.028102
2022-01-09 23:56:18,597 iteration 1831 : loss : 0.060052, loss_ce: 0.023782
2022-01-09 23:56:20,151 iteration 1832 : loss : 0.048113, loss_ce: 0.019287
2022-01-09 23:56:21,674 iteration 1833 : loss : 0.054733, loss_ce: 0.025451
2022-01-09 23:56:23,245 iteration 1834 : loss : 0.058352, loss_ce: 0.023732
2022-01-09 23:56:24,826 iteration 1835 : loss : 0.040681, loss_ce: 0.015916
2022-01-09 23:56:26,368 iteration 1836 : loss : 0.073894, loss_ce: 0.026883
 27%|███████▊                     | 108/400 [53:01<2:18:04, 28.37s/it]2022-01-09 23:56:28,043 iteration 1837 : loss : 0.069407, loss_ce: 0.027805
2022-01-09 23:56:29,548 iteration 1838 : loss : 0.052540, loss_ce: 0.023835
2022-01-09 23:56:31,087 iteration 1839 : loss : 0.059813, loss_ce: 0.025795
2022-01-09 23:56:32,682 iteration 1840 : loss : 0.042523, loss_ce: 0.022096
2022-01-09 23:56:34,199 iteration 1841 : loss : 0.050159, loss_ce: 0.022121
2022-01-09 23:56:35,760 iteration 1842 : loss : 0.041130, loss_ce: 0.013589
2022-01-09 23:56:37,357 iteration 1843 : loss : 0.045755, loss_ce: 0.022104
2022-01-09 23:56:38,918 iteration 1844 : loss : 0.054046, loss_ce: 0.018962
2022-01-09 23:56:40,468 iteration 1845 : loss : 0.042326, loss_ce: 0.014964
2022-01-09 23:56:42,115 iteration 1846 : loss : 0.050221, loss_ce: 0.021815
2022-01-09 23:56:43,657 iteration 1847 : loss : 0.055079, loss_ce: 0.021472
2022-01-09 23:56:45,222 iteration 1848 : loss : 0.061804, loss_ce: 0.024944
2022-01-09 23:56:46,766 iteration 1849 : loss : 0.064917, loss_ce: 0.029137
2022-01-09 23:56:48,466 iteration 1850 : loss : 0.085876, loss_ce: 0.032338
2022-01-09 23:56:50,043 iteration 1851 : loss : 0.110989, loss_ce: 0.029275
2022-01-09 23:56:51,637 iteration 1852 : loss : 0.056044, loss_ce: 0.022088
2022-01-09 23:56:53,186 iteration 1853 : loss : 0.050242, loss_ce: 0.020454
 27%|███████▉                     | 109/400 [53:28<2:15:20, 27.91s/it]2022-01-09 23:56:54,798 iteration 1854 : loss : 0.047334, loss_ce: 0.018604
2022-01-09 23:56:56,423 iteration 1855 : loss : 0.057246, loss_ce: 0.021933
2022-01-09 23:56:58,056 iteration 1856 : loss : 0.072827, loss_ce: 0.024286
2022-01-09 23:56:59,700 iteration 1857 : loss : 0.048862, loss_ce: 0.016628
2022-01-09 23:57:01,298 iteration 1858 : loss : 0.057213, loss_ce: 0.026490
2022-01-09 23:57:02,896 iteration 1859 : loss : 0.064639, loss_ce: 0.024807
2022-01-09 23:57:04,437 iteration 1860 : loss : 0.050909, loss_ce: 0.018006
2022-01-09 23:57:06,072 iteration 1861 : loss : 0.046991, loss_ce: 0.018667
2022-01-09 23:57:07,607 iteration 1862 : loss : 0.041828, loss_ce: 0.020163
2022-01-09 23:57:09,184 iteration 1863 : loss : 0.059842, loss_ce: 0.028334
2022-01-09 23:57:10,794 iteration 1864 : loss : 0.086690, loss_ce: 0.025081
2022-01-09 23:57:12,281 iteration 1865 : loss : 0.043614, loss_ce: 0.019534
2022-01-09 23:57:13,927 iteration 1866 : loss : 0.031966, loss_ce: 0.014913
2022-01-09 23:57:15,508 iteration 1867 : loss : 0.037773, loss_ce: 0.016426
2022-01-09 23:57:17,053 iteration 1868 : loss : 0.059731, loss_ce: 0.024686
2022-01-09 23:57:18,593 iteration 1869 : loss : 0.090329, loss_ce: 0.031849
2022-01-09 23:57:18,593 Training Data Eval:
2022-01-09 23:57:26,675   Average segmentation loss on training set: 0.0372
2022-01-09 23:57:26,675 Validation Data Eval:
2022-01-09 23:57:29,459   Average segmentation loss on validation set: 0.1039
2022-01-09 23:57:31,072 iteration 1870 : loss : 0.053281, loss_ce: 0.020189
 28%|███████▉                     | 110/400 [54:06<2:29:21, 30.90s/it]2022-01-09 23:57:32,759 iteration 1871 : loss : 0.041472, loss_ce: 0.015450
2022-01-09 23:57:34,442 iteration 1872 : loss : 0.099759, loss_ce: 0.030008
2022-01-09 23:57:35,973 iteration 1873 : loss : 0.058807, loss_ce: 0.022587
2022-01-09 23:57:37,619 iteration 1874 : loss : 0.092597, loss_ce: 0.045604
2022-01-09 23:57:39,116 iteration 1875 : loss : 0.053210, loss_ce: 0.020490
2022-01-09 23:57:40,768 iteration 1876 : loss : 0.043439, loss_ce: 0.013074
2022-01-09 23:57:42,390 iteration 1877 : loss : 0.082148, loss_ce: 0.034571
2022-01-09 23:57:43,944 iteration 1878 : loss : 0.040729, loss_ce: 0.018309
2022-01-09 23:57:45,489 iteration 1879 : loss : 0.043080, loss_ce: 0.015947
2022-01-09 23:57:47,028 iteration 1880 : loss : 0.067049, loss_ce: 0.031463
2022-01-09 23:57:48,604 iteration 1881 : loss : 0.057724, loss_ce: 0.020096
2022-01-09 23:57:50,134 iteration 1882 : loss : 0.047983, loss_ce: 0.018591
2022-01-09 23:57:51,627 iteration 1883 : loss : 0.063016, loss_ce: 0.022060
2022-01-09 23:57:53,232 iteration 1884 : loss : 0.049853, loss_ce: 0.018490
2022-01-09 23:57:54,718 iteration 1885 : loss : 0.048716, loss_ce: 0.020815
2022-01-09 23:57:56,369 iteration 1886 : loss : 0.045942, loss_ce: 0.019329
2022-01-09 23:57:58,013 iteration 1887 : loss : 0.071623, loss_ce: 0.033461
 28%|████████                     | 111/400 [54:33<2:23:06, 29.71s/it]2022-01-09 23:57:59,629 iteration 1888 : loss : 0.060231, loss_ce: 0.027417
2022-01-09 23:58:01,263 iteration 1889 : loss : 0.071928, loss_ce: 0.029660
2022-01-09 23:58:02,878 iteration 1890 : loss : 0.065153, loss_ce: 0.019660
2022-01-09 23:58:04,503 iteration 1891 : loss : 0.064503, loss_ce: 0.024692
2022-01-09 23:58:06,097 iteration 1892 : loss : 0.041023, loss_ce: 0.017681
2022-01-09 23:58:07,695 iteration 1893 : loss : 0.065054, loss_ce: 0.036315
2022-01-09 23:58:09,269 iteration 1894 : loss : 0.060774, loss_ce: 0.025705
2022-01-09 23:58:10,835 iteration 1895 : loss : 0.083958, loss_ce: 0.028115
2022-01-09 23:58:12,536 iteration 1896 : loss : 0.081030, loss_ce: 0.042627
2022-01-09 23:58:14,102 iteration 1897 : loss : 0.066427, loss_ce: 0.021714
2022-01-09 23:58:15,740 iteration 1898 : loss : 0.042253, loss_ce: 0.015603
2022-01-09 23:58:17,317 iteration 1899 : loss : 0.061867, loss_ce: 0.029903
2022-01-09 23:58:18,824 iteration 1900 : loss : 0.043177, loss_ce: 0.017116
2022-01-09 23:58:20,398 iteration 1901 : loss : 0.048948, loss_ce: 0.018345
2022-01-09 23:58:22,002 iteration 1902 : loss : 0.060228, loss_ce: 0.027383
2022-01-09 23:58:23,632 iteration 1903 : loss : 0.067791, loss_ce: 0.021609
2022-01-09 23:58:25,312 iteration 1904 : loss : 0.061388, loss_ce: 0.021790
 28%|████████                     | 112/400 [55:00<2:19:07, 28.99s/it]2022-01-09 23:58:26,896 iteration 1905 : loss : 0.047490, loss_ce: 0.021515
2022-01-09 23:58:28,532 iteration 1906 : loss : 0.041709, loss_ce: 0.015244
2022-01-09 23:58:30,221 iteration 1907 : loss : 0.072675, loss_ce: 0.030699
2022-01-09 23:58:31,842 iteration 1908 : loss : 0.046937, loss_ce: 0.021591
2022-01-09 23:58:33,490 iteration 1909 : loss : 0.071615, loss_ce: 0.037713
2022-01-09 23:58:35,124 iteration 1910 : loss : 0.040583, loss_ce: 0.017522
2022-01-09 23:58:36,828 iteration 1911 : loss : 0.042547, loss_ce: 0.016329
2022-01-09 23:58:38,460 iteration 1912 : loss : 0.060458, loss_ce: 0.018221
2022-01-09 23:58:40,039 iteration 1913 : loss : 0.049425, loss_ce: 0.019372
2022-01-09 23:58:41,584 iteration 1914 : loss : 0.041754, loss_ce: 0.019093
2022-01-09 23:58:43,173 iteration 1915 : loss : 0.059050, loss_ce: 0.022092
2022-01-09 23:58:44,754 iteration 1916 : loss : 0.057020, loss_ce: 0.022138
2022-01-09 23:58:46,364 iteration 1917 : loss : 0.072156, loss_ce: 0.028651
2022-01-09 23:58:47,907 iteration 1918 : loss : 0.051644, loss_ce: 0.023625
2022-01-09 23:58:49,471 iteration 1919 : loss : 0.049019, loss_ce: 0.023824
2022-01-09 23:58:51,042 iteration 1920 : loss : 0.039624, loss_ce: 0.015488
2022-01-09 23:58:52,658 iteration 1921 : loss : 0.060152, loss_ce: 0.025079
 28%|████████▏                    | 113/400 [55:27<2:16:17, 28.49s/it]2022-01-09 23:58:54,247 iteration 1922 : loss : 0.055524, loss_ce: 0.021220
2022-01-09 23:58:55,753 iteration 1923 : loss : 0.030191, loss_ce: 0.012922
2022-01-09 23:58:57,350 iteration 1924 : loss : 0.068789, loss_ce: 0.038057
2022-01-09 23:58:58,877 iteration 1925 : loss : 0.064020, loss_ce: 0.026700
2022-01-09 23:59:00,420 iteration 1926 : loss : 0.039438, loss_ce: 0.017036
2022-01-09 23:59:02,032 iteration 1927 : loss : 0.063909, loss_ce: 0.028139
2022-01-09 23:59:03,659 iteration 1928 : loss : 0.037290, loss_ce: 0.016512
2022-01-09 23:59:05,209 iteration 1929 : loss : 0.053353, loss_ce: 0.024395
2022-01-09 23:59:06,785 iteration 1930 : loss : 0.057481, loss_ce: 0.018834
2022-01-09 23:59:08,334 iteration 1931 : loss : 0.053193, loss_ce: 0.015296
2022-01-09 23:59:09,910 iteration 1932 : loss : 0.055807, loss_ce: 0.016567
2022-01-09 23:59:11,489 iteration 1933 : loss : 0.051467, loss_ce: 0.019615
2022-01-09 23:59:12,951 iteration 1934 : loss : 0.039434, loss_ce: 0.017725
2022-01-09 23:59:14,494 iteration 1935 : loss : 0.049066, loss_ce: 0.022464
2022-01-09 23:59:16,112 iteration 1936 : loss : 0.036454, loss_ce: 0.014007
2022-01-09 23:59:17,717 iteration 1937 : loss : 0.053722, loss_ce: 0.024410
2022-01-09 23:59:19,330 iteration 1938 : loss : 0.064252, loss_ce: 0.020161
 28%|████████▎                    | 114/400 [55:54<2:13:13, 27.95s/it]2022-01-09 23:59:20,903 iteration 1939 : loss : 0.037605, loss_ce: 0.019299
2022-01-09 23:59:22,456 iteration 1940 : loss : 0.032378, loss_ce: 0.012432
2022-01-09 23:59:24,081 iteration 1941 : loss : 0.039728, loss_ce: 0.017650
2022-01-09 23:59:25,650 iteration 1942 : loss : 0.067223, loss_ce: 0.018691
2022-01-09 23:59:27,150 iteration 1943 : loss : 0.039691, loss_ce: 0.013849
2022-01-09 23:59:28,694 iteration 1944 : loss : 0.047929, loss_ce: 0.017386
2022-01-09 23:59:30,251 iteration 1945 : loss : 0.041532, loss_ce: 0.021661
2022-01-09 23:59:31,810 iteration 1946 : loss : 0.054667, loss_ce: 0.019446
2022-01-09 23:59:33,358 iteration 1947 : loss : 0.067268, loss_ce: 0.023597
2022-01-09 23:59:34,902 iteration 1948 : loss : 0.042376, loss_ce: 0.018043
2022-01-09 23:59:36,398 iteration 1949 : loss : 0.042306, loss_ce: 0.021847
2022-01-09 23:59:38,004 iteration 1950 : loss : 0.042595, loss_ce: 0.016406
2022-01-09 23:59:39,677 iteration 1951 : loss : 0.047218, loss_ce: 0.016825
2022-01-09 23:59:41,199 iteration 1952 : loss : 0.056889, loss_ce: 0.016285
2022-01-09 23:59:42,788 iteration 1953 : loss : 0.062222, loss_ce: 0.037466
2022-01-09 23:59:44,436 iteration 1954 : loss : 0.049031, loss_ce: 0.017121
2022-01-09 23:59:44,436 Training Data Eval:
2022-01-09 23:59:52,504   Average segmentation loss on training set: 0.0345
2022-01-09 23:59:52,504 Validation Data Eval:
2022-01-09 23:59:55,277   Average segmentation loss on validation set: 0.1031
2022-01-09 23:59:56,828 iteration 1955 : loss : 0.035185, loss_ce: 0.013286
 29%|████████▎                    | 115/400 [56:31<2:26:22, 30.81s/it]2022-01-09 23:59:58,507 iteration 1956 : loss : 0.063318, loss_ce: 0.027345
2022-01-10 00:00:00,028 iteration 1957 : loss : 0.042919, loss_ce: 0.018262
2022-01-10 00:00:01,578 iteration 1958 : loss : 0.041805, loss_ce: 0.020278
2022-01-10 00:00:03,230 iteration 1959 : loss : 0.060431, loss_ce: 0.017982
2022-01-10 00:00:04,792 iteration 1960 : loss : 0.052872, loss_ce: 0.016327
2022-01-10 00:00:06,369 iteration 1961 : loss : 0.057840, loss_ce: 0.022779
2022-01-10 00:00:07,932 iteration 1962 : loss : 0.032128, loss_ce: 0.012635
2022-01-10 00:00:09,525 iteration 1963 : loss : 0.060108, loss_ce: 0.025007
2022-01-10 00:00:11,084 iteration 1964 : loss : 0.049221, loss_ce: 0.013712
2022-01-10 00:00:12,621 iteration 1965 : loss : 0.044554, loss_ce: 0.021163
2022-01-10 00:00:14,150 iteration 1966 : loss : 0.046981, loss_ce: 0.020878
2022-01-10 00:00:15,670 iteration 1967 : loss : 0.065057, loss_ce: 0.020147
2022-01-10 00:00:17,174 iteration 1968 : loss : 0.043503, loss_ce: 0.024199
2022-01-10 00:00:18,772 iteration 1969 : loss : 0.050562, loss_ce: 0.021118
2022-01-10 00:00:20,479 iteration 1970 : loss : 0.042023, loss_ce: 0.014910
2022-01-10 00:00:21,964 iteration 1971 : loss : 0.050165, loss_ce: 0.015745
2022-01-10 00:00:23,647 iteration 1972 : loss : 0.058390, loss_ce: 0.023760
 29%|████████▍                    | 116/400 [56:58<2:20:10, 29.62s/it]2022-01-10 00:00:25,198 iteration 1973 : loss : 0.037812, loss_ce: 0.015483
2022-01-10 00:00:26,703 iteration 1974 : loss : 0.043924, loss_ce: 0.017969
2022-01-10 00:00:28,346 iteration 1975 : loss : 0.065898, loss_ce: 0.027092
2022-01-10 00:00:29,843 iteration 1976 : loss : 0.045594, loss_ce: 0.017569
2022-01-10 00:00:31,491 iteration 1977 : loss : 0.078165, loss_ce: 0.036614
2022-01-10 00:00:33,028 iteration 1978 : loss : 0.052615, loss_ce: 0.021013
2022-01-10 00:00:34,579 iteration 1979 : loss : 0.046472, loss_ce: 0.022576
2022-01-10 00:00:36,189 iteration 1980 : loss : 0.054829, loss_ce: 0.018639
2022-01-10 00:00:37,716 iteration 1981 : loss : 0.052096, loss_ce: 0.021412
2022-01-10 00:00:39,345 iteration 1982 : loss : 0.036030, loss_ce: 0.012468
2022-01-10 00:00:41,007 iteration 1983 : loss : 0.050449, loss_ce: 0.022272
2022-01-10 00:00:42,546 iteration 1984 : loss : 0.048042, loss_ce: 0.013371
2022-01-10 00:00:44,094 iteration 1985 : loss : 0.036687, loss_ce: 0.013014
2022-01-10 00:00:45,627 iteration 1986 : loss : 0.041434, loss_ce: 0.013722
2022-01-10 00:00:47,153 iteration 1987 : loss : 0.028290, loss_ce: 0.011386
2022-01-10 00:00:48,806 iteration 1988 : loss : 0.039486, loss_ce: 0.016032
2022-01-10 00:00:50,376 iteration 1989 : loss : 0.046202, loss_ce: 0.020823
 29%|████████▍                    | 117/400 [57:25<2:15:35, 28.75s/it]2022-01-10 00:00:51,996 iteration 1990 : loss : 0.047067, loss_ce: 0.018519
2022-01-10 00:00:53,482 iteration 1991 : loss : 0.050341, loss_ce: 0.022990
2022-01-10 00:00:55,084 iteration 1992 : loss : 0.051364, loss_ce: 0.020949
2022-01-10 00:00:56,640 iteration 1993 : loss : 0.065407, loss_ce: 0.020320
2022-01-10 00:00:58,242 iteration 1994 : loss : 0.043600, loss_ce: 0.017068
2022-01-10 00:00:59,813 iteration 1995 : loss : 0.051695, loss_ce: 0.020808
2022-01-10 00:01:01,398 iteration 1996 : loss : 0.072830, loss_ce: 0.035250
2022-01-10 00:01:02,978 iteration 1997 : loss : 0.047468, loss_ce: 0.021541
2022-01-10 00:01:04,584 iteration 1998 : loss : 0.051755, loss_ce: 0.024261
2022-01-10 00:01:06,184 iteration 1999 : loss : 0.053070, loss_ce: 0.023290
2022-01-10 00:01:07,883 iteration 2000 : loss : 0.063651, loss_ce: 0.018261
2022-01-10 00:01:09,465 iteration 2001 : loss : 0.048302, loss_ce: 0.018994
2022-01-10 00:01:10,969 iteration 2002 : loss : 0.043705, loss_ce: 0.017259
2022-01-10 00:01:12,421 iteration 2003 : loss : 0.033730, loss_ce: 0.017318
2022-01-10 00:01:13,952 iteration 2004 : loss : 0.063558, loss_ce: 0.021206
2022-01-10 00:01:15,562 iteration 2005 : loss : 0.038402, loss_ce: 0.014977
2022-01-10 00:01:17,161 iteration 2006 : loss : 0.046633, loss_ce: 0.016534
 30%|████████▌                    | 118/400 [57:52<2:12:20, 28.16s/it]2022-01-10 00:01:18,849 iteration 2007 : loss : 0.045967, loss_ce: 0.018112
2022-01-10 00:01:20,546 iteration 2008 : loss : 0.059873, loss_ce: 0.018727
2022-01-10 00:01:22,128 iteration 2009 : loss : 0.045644, loss_ce: 0.018839
2022-01-10 00:01:23,698 iteration 2010 : loss : 0.050472, loss_ce: 0.021853
2022-01-10 00:01:25,358 iteration 2011 : loss : 0.048026, loss_ce: 0.018542
2022-01-10 00:01:26,945 iteration 2012 : loss : 0.037505, loss_ce: 0.018011
2022-01-10 00:01:28,448 iteration 2013 : loss : 0.030400, loss_ce: 0.012804
2022-01-10 00:01:29,995 iteration 2014 : loss : 0.042491, loss_ce: 0.013616
2022-01-10 00:01:31,603 iteration 2015 : loss : 0.035895, loss_ce: 0.014835
2022-01-10 00:01:33,164 iteration 2016 : loss : 0.075795, loss_ce: 0.028758
2022-01-10 00:01:34,783 iteration 2017 : loss : 0.035597, loss_ce: 0.012350
2022-01-10 00:01:36,466 iteration 2018 : loss : 0.039950, loss_ce: 0.016874
2022-01-10 00:01:38,031 iteration 2019 : loss : 0.064966, loss_ce: 0.025517
2022-01-10 00:01:39,578 iteration 2020 : loss : 0.031792, loss_ce: 0.011650
2022-01-10 00:01:41,179 iteration 2021 : loss : 0.051675, loss_ce: 0.018252
2022-01-10 00:01:42,784 iteration 2022 : loss : 0.038205, loss_ce: 0.017553
2022-01-10 00:01:44,426 iteration 2023 : loss : 0.062379, loss_ce: 0.025456
 30%|████████▋                    | 119/400 [58:19<2:10:37, 27.89s/it]2022-01-10 00:01:46,015 iteration 2024 : loss : 0.057979, loss_ce: 0.022882
2022-01-10 00:01:47,610 iteration 2025 : loss : 0.041842, loss_ce: 0.018460
2022-01-10 00:01:49,239 iteration 2026 : loss : 0.044258, loss_ce: 0.018177
2022-01-10 00:01:50,800 iteration 2027 : loss : 0.034352, loss_ce: 0.009834
2022-01-10 00:01:52,328 iteration 2028 : loss : 0.033583, loss_ce: 0.017444
2022-01-10 00:01:53,880 iteration 2029 : loss : 0.041869, loss_ce: 0.017733
2022-01-10 00:01:55,418 iteration 2030 : loss : 0.056520, loss_ce: 0.023222
2022-01-10 00:01:56,992 iteration 2031 : loss : 0.045004, loss_ce: 0.015892
2022-01-10 00:01:58,653 iteration 2032 : loss : 0.065997, loss_ce: 0.031709
2022-01-10 00:02:00,343 iteration 2033 : loss : 0.048482, loss_ce: 0.021570
2022-01-10 00:02:01,886 iteration 2034 : loss : 0.035934, loss_ce: 0.014750
2022-01-10 00:02:03,468 iteration 2035 : loss : 0.057135, loss_ce: 0.019564
2022-01-10 00:02:05,087 iteration 2036 : loss : 0.050101, loss_ce: 0.028911
2022-01-10 00:02:06,660 iteration 2037 : loss : 0.090756, loss_ce: 0.027796
2022-01-10 00:02:08,278 iteration 2038 : loss : 0.054805, loss_ce: 0.021223
2022-01-10 00:02:09,945 iteration 2039 : loss : 0.048796, loss_ce: 0.018645
2022-01-10 00:02:09,945 Training Data Eval:
2022-01-10 00:02:18,002   Average segmentation loss on training set: 0.0360
2022-01-10 00:02:18,003 Validation Data Eval:
2022-01-10 00:02:20,773   Average segmentation loss on validation set: 0.0788
2022-01-10 00:02:26,914 Found new lowest validation loss at iteration 2039! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-10 00:02:28,458 iteration 2040 : loss : 0.081766, loss_ce: 0.038023
 30%|████████▋                    | 120/400 [59:03<2:32:46, 32.74s/it]2022-01-10 00:02:30,017 iteration 2041 : loss : 0.057648, loss_ce: 0.015073
2022-01-10 00:02:31,569 iteration 2042 : loss : 0.056819, loss_ce: 0.024716
2022-01-10 00:02:33,120 iteration 2043 : loss : 0.035485, loss_ce: 0.011226
2022-01-10 00:02:34,690 iteration 2044 : loss : 0.045490, loss_ce: 0.017435
2022-01-10 00:02:36,236 iteration 2045 : loss : 0.032060, loss_ce: 0.009703
2022-01-10 00:02:37,822 iteration 2046 : loss : 0.056141, loss_ce: 0.027716
2022-01-10 00:02:39,375 iteration 2047 : loss : 0.058090, loss_ce: 0.022825
2022-01-10 00:02:40,958 iteration 2048 : loss : 0.037177, loss_ce: 0.013379
2022-01-10 00:02:42,509 iteration 2049 : loss : 0.050529, loss_ce: 0.013620
2022-01-10 00:02:44,102 iteration 2050 : loss : 0.046556, loss_ce: 0.019789
2022-01-10 00:02:45,695 iteration 2051 : loss : 0.053189, loss_ce: 0.025904
2022-01-10 00:02:47,215 iteration 2052 : loss : 0.045410, loss_ce: 0.018776
2022-01-10 00:02:48,796 iteration 2053 : loss : 0.065577, loss_ce: 0.021564
2022-01-10 00:02:50,347 iteration 2054 : loss : 0.035791, loss_ce: 0.013786
2022-01-10 00:02:51,907 iteration 2055 : loss : 0.074331, loss_ce: 0.024295
2022-01-10 00:02:53,523 iteration 2056 : loss : 0.056791, loss_ce: 0.023053
2022-01-10 00:02:55,053 iteration 2057 : loss : 0.044831, loss_ce: 0.018092
 30%|████████▊                    | 121/400 [59:30<2:23:38, 30.89s/it]2022-01-10 00:02:56,682 iteration 2058 : loss : 0.042847, loss_ce: 0.020319
2022-01-10 00:02:58,312 iteration 2059 : loss : 0.049882, loss_ce: 0.017571
2022-01-10 00:02:59,805 iteration 2060 : loss : 0.038973, loss_ce: 0.015070
2022-01-10 00:03:01,410 iteration 2061 : loss : 0.066453, loss_ce: 0.028324
2022-01-10 00:03:02,966 iteration 2062 : loss : 0.053356, loss_ce: 0.022329
2022-01-10 00:03:04,531 iteration 2063 : loss : 0.095743, loss_ce: 0.022599
2022-01-10 00:03:06,053 iteration 2064 : loss : 0.031996, loss_ce: 0.011904
2022-01-10 00:03:07,603 iteration 2065 : loss : 0.050643, loss_ce: 0.014357
2022-01-10 00:03:09,176 iteration 2066 : loss : 0.048905, loss_ce: 0.022034
2022-01-10 00:03:10,778 iteration 2067 : loss : 0.040236, loss_ce: 0.014363
2022-01-10 00:03:12,417 iteration 2068 : loss : 0.068886, loss_ce: 0.034898
2022-01-10 00:03:13,969 iteration 2069 : loss : 0.052069, loss_ce: 0.017803
2022-01-10 00:03:15,587 iteration 2070 : loss : 0.040125, loss_ce: 0.016580
2022-01-10 00:03:17,225 iteration 2071 : loss : 0.065718, loss_ce: 0.024920
2022-01-10 00:03:18,815 iteration 2072 : loss : 0.040785, loss_ce: 0.016383
2022-01-10 00:03:20,446 iteration 2073 : loss : 0.077464, loss_ce: 0.024086
2022-01-10 00:03:22,059 iteration 2074 : loss : 0.041701, loss_ce: 0.020937
 30%|████████▊                    | 122/400 [59:57<2:17:44, 29.73s/it]2022-01-10 00:03:23,666 iteration 2075 : loss : 0.039635, loss_ce: 0.014159
2022-01-10 00:03:25,252 iteration 2076 : loss : 0.055518, loss_ce: 0.016166
2022-01-10 00:03:26,856 iteration 2077 : loss : 0.045706, loss_ce: 0.020124
2022-01-10 00:03:28,557 iteration 2078 : loss : 0.054097, loss_ce: 0.016816
2022-01-10 00:03:30,115 iteration 2079 : loss : 0.051388, loss_ce: 0.017660
2022-01-10 00:03:31,750 iteration 2080 : loss : 0.052458, loss_ce: 0.020347
2022-01-10 00:03:33,243 iteration 2081 : loss : 0.051758, loss_ce: 0.018846
2022-01-10 00:03:34,717 iteration 2082 : loss : 0.029615, loss_ce: 0.011820
2022-01-10 00:03:36,278 iteration 2083 : loss : 0.056273, loss_ce: 0.030362
2022-01-10 00:03:37,839 iteration 2084 : loss : 0.088690, loss_ce: 0.048055
2022-01-10 00:03:39,368 iteration 2085 : loss : 0.038013, loss_ce: 0.011903
2022-01-10 00:03:40,933 iteration 2086 : loss : 0.070282, loss_ce: 0.030587
2022-01-10 00:03:42,469 iteration 2087 : loss : 0.046976, loss_ce: 0.018020
2022-01-10 00:03:44,109 iteration 2088 : loss : 0.037012, loss_ce: 0.016681
2022-01-10 00:03:45,728 iteration 2089 : loss : 0.043109, loss_ce: 0.018811
2022-01-10 00:03:47,238 iteration 2090 : loss : 0.044174, loss_ce: 0.017977
2022-01-10 00:03:48,850 iteration 2091 : loss : 0.041769, loss_ce: 0.018138
 31%|████████▎                  | 123/400 [1:00:23<2:13:10, 28.84s/it]2022-01-10 00:03:50,411 iteration 2092 : loss : 0.037761, loss_ce: 0.017173
2022-01-10 00:03:51,962 iteration 2093 : loss : 0.063061, loss_ce: 0.026235
2022-01-10 00:03:53,536 iteration 2094 : loss : 0.052272, loss_ce: 0.015497
2022-01-10 00:03:55,052 iteration 2095 : loss : 0.027016, loss_ce: 0.012165
2022-01-10 00:03:56,704 iteration 2096 : loss : 0.044366, loss_ce: 0.021990
2022-01-10 00:03:58,296 iteration 2097 : loss : 0.071999, loss_ce: 0.021311
2022-01-10 00:03:59,875 iteration 2098 : loss : 0.067261, loss_ce: 0.027242
2022-01-10 00:04:01,465 iteration 2099 : loss : 0.050538, loss_ce: 0.016580
2022-01-10 00:04:03,079 iteration 2100 : loss : 0.083230, loss_ce: 0.048405
2022-01-10 00:04:04,674 iteration 2101 : loss : 0.046487, loss_ce: 0.018156
2022-01-10 00:04:06,298 iteration 2102 : loss : 0.062658, loss_ce: 0.033823
2022-01-10 00:04:07,796 iteration 2103 : loss : 0.046415, loss_ce: 0.021775
2022-01-10 00:04:09,312 iteration 2104 : loss : 0.078837, loss_ce: 0.024198
2022-01-10 00:04:10,979 iteration 2105 : loss : 0.060107, loss_ce: 0.019813
2022-01-10 00:04:12,520 iteration 2106 : loss : 0.034515, loss_ce: 0.016251
2022-01-10 00:04:14,175 iteration 2107 : loss : 0.041821, loss_ce: 0.015211
2022-01-10 00:04:15,774 iteration 2108 : loss : 0.072481, loss_ce: 0.027980
 31%|████████▎                  | 124/400 [1:00:50<2:10:02, 28.27s/it]2022-01-10 00:04:17,407 iteration 2109 : loss : 0.058865, loss_ce: 0.022333
2022-01-10 00:04:19,026 iteration 2110 : loss : 0.046981, loss_ce: 0.016119
2022-01-10 00:04:20,692 iteration 2111 : loss : 0.071337, loss_ce: 0.019956
2022-01-10 00:04:22,289 iteration 2112 : loss : 0.067966, loss_ce: 0.029300
2022-01-10 00:04:23,839 iteration 2113 : loss : 0.067519, loss_ce: 0.030622
2022-01-10 00:04:25,452 iteration 2114 : loss : 0.044972, loss_ce: 0.021398
2022-01-10 00:04:27,061 iteration 2115 : loss : 0.045809, loss_ce: 0.014221
2022-01-10 00:04:28,662 iteration 2116 : loss : 0.042662, loss_ce: 0.016197
2022-01-10 00:04:30,254 iteration 2117 : loss : 0.038995, loss_ce: 0.014515
2022-01-10 00:04:31,874 iteration 2118 : loss : 0.054192, loss_ce: 0.018851
2022-01-10 00:04:33,500 iteration 2119 : loss : 0.064485, loss_ce: 0.020180
2022-01-10 00:04:35,086 iteration 2120 : loss : 0.061608, loss_ce: 0.019332
2022-01-10 00:04:36,572 iteration 2121 : loss : 0.036677, loss_ce: 0.015223
2022-01-10 00:04:38,128 iteration 2122 : loss : 0.046301, loss_ce: 0.015462
2022-01-10 00:04:39,761 iteration 2123 : loss : 0.072967, loss_ce: 0.036043
2022-01-10 00:04:41,419 iteration 2124 : loss : 0.051953, loss_ce: 0.024735
2022-01-10 00:04:41,420 Training Data Eval:
2022-01-10 00:04:49,488   Average segmentation loss on training set: 0.0353
2022-01-10 00:04:49,488 Validation Data Eval:
2022-01-10 00:04:52,261   Average segmentation loss on validation set: 0.0886
2022-01-10 00:04:53,857 iteration 2125 : loss : 0.050169, loss_ce: 0.026013
 31%|████████▍                  | 125/400 [1:01:28<2:23:04, 31.22s/it]2022-01-10 00:04:55,542 iteration 2126 : loss : 0.040944, loss_ce: 0.015868
2022-01-10 00:04:57,150 iteration 2127 : loss : 0.060102, loss_ce: 0.026874
2022-01-10 00:04:58,752 iteration 2128 : loss : 0.039346, loss_ce: 0.013277
2022-01-10 00:05:00,301 iteration 2129 : loss : 0.042017, loss_ce: 0.017780
2022-01-10 00:05:01,919 iteration 2130 : loss : 0.079441, loss_ce: 0.027020
2022-01-10 00:05:03,479 iteration 2131 : loss : 0.046637, loss_ce: 0.014841
2022-01-10 00:05:05,056 iteration 2132 : loss : 0.038124, loss_ce: 0.013462
2022-01-10 00:05:06,703 iteration 2133 : loss : 0.058837, loss_ce: 0.019788
2022-01-10 00:05:08,213 iteration 2134 : loss : 0.041924, loss_ce: 0.021472
2022-01-10 00:05:09,790 iteration 2135 : loss : 0.052817, loss_ce: 0.019699
2022-01-10 00:05:11,355 iteration 2136 : loss : 0.045289, loss_ce: 0.018701
2022-01-10 00:05:12,934 iteration 2137 : loss : 0.048809, loss_ce: 0.021154
2022-01-10 00:05:14,472 iteration 2138 : loss : 0.045675, loss_ce: 0.017591
2022-01-10 00:05:16,087 iteration 2139 : loss : 0.047667, loss_ce: 0.021564
2022-01-10 00:05:17,664 iteration 2140 : loss : 0.054615, loss_ce: 0.025424
2022-01-10 00:05:19,307 iteration 2141 : loss : 0.057987, loss_ce: 0.020675
2022-01-10 00:05:20,848 iteration 2142 : loss : 0.040506, loss_ce: 0.013646
 32%|████████▌                  | 126/400 [1:01:55<2:16:45, 29.95s/it]2022-01-10 00:05:22,413 iteration 2143 : loss : 0.043520, loss_ce: 0.016225
2022-01-10 00:05:24,008 iteration 2144 : loss : 0.043532, loss_ce: 0.012392
2022-01-10 00:05:25,614 iteration 2145 : loss : 0.049959, loss_ce: 0.014713
2022-01-10 00:05:27,138 iteration 2146 : loss : 0.030766, loss_ce: 0.014498
2022-01-10 00:05:28,729 iteration 2147 : loss : 0.052624, loss_ce: 0.017394
2022-01-10 00:05:30,283 iteration 2148 : loss : 0.047955, loss_ce: 0.020613
2022-01-10 00:05:31,865 iteration 2149 : loss : 0.044539, loss_ce: 0.016528
2022-01-10 00:05:33,416 iteration 2150 : loss : 0.037511, loss_ce: 0.014651
2022-01-10 00:05:35,054 iteration 2151 : loss : 0.063435, loss_ce: 0.028887
2022-01-10 00:05:36,590 iteration 2152 : loss : 0.034453, loss_ce: 0.013799
2022-01-10 00:05:38,115 iteration 2153 : loss : 0.039672, loss_ce: 0.016157
2022-01-10 00:05:39,820 iteration 2154 : loss : 0.065183, loss_ce: 0.023240
2022-01-10 00:05:41,402 iteration 2155 : loss : 0.037335, loss_ce: 0.015593
2022-01-10 00:05:42,933 iteration 2156 : loss : 0.061845, loss_ce: 0.029559
2022-01-10 00:05:44,651 iteration 2157 : loss : 0.090236, loss_ce: 0.030452
2022-01-10 00:05:46,323 iteration 2158 : loss : 0.046996, loss_ce: 0.022205
2022-01-10 00:05:47,928 iteration 2159 : loss : 0.080331, loss_ce: 0.025760
 32%|████████▌                  | 127/400 [1:02:23<2:12:20, 29.09s/it]2022-01-10 00:05:49,527 iteration 2160 : loss : 0.047959, loss_ce: 0.018727
2022-01-10 00:05:51,049 iteration 2161 : loss : 0.048361, loss_ce: 0.017934
2022-01-10 00:05:52,606 iteration 2162 : loss : 0.047136, loss_ce: 0.018832
2022-01-10 00:05:54,149 iteration 2163 : loss : 0.040192, loss_ce: 0.016170
2022-01-10 00:05:55,760 iteration 2164 : loss : 0.039851, loss_ce: 0.020716
2022-01-10 00:05:57,354 iteration 2165 : loss : 0.072097, loss_ce: 0.027517
2022-01-10 00:05:58,885 iteration 2166 : loss : 0.045457, loss_ce: 0.020745
2022-01-10 00:06:00,387 iteration 2167 : loss : 0.043701, loss_ce: 0.015596
2022-01-10 00:06:01,935 iteration 2168 : loss : 0.034211, loss_ce: 0.014141
2022-01-10 00:06:03,533 iteration 2169 : loss : 0.060642, loss_ce: 0.021718
2022-01-10 00:06:05,001 iteration 2170 : loss : 0.033569, loss_ce: 0.011705
2022-01-10 00:06:06,551 iteration 2171 : loss : 0.064726, loss_ce: 0.025350
2022-01-10 00:06:08,090 iteration 2172 : loss : 0.065349, loss_ce: 0.022023
2022-01-10 00:06:09,622 iteration 2173 : loss : 0.044506, loss_ce: 0.012059
2022-01-10 00:06:11,194 iteration 2174 : loss : 0.062008, loss_ce: 0.023667
2022-01-10 00:06:12,848 iteration 2175 : loss : 0.037328, loss_ce: 0.016612
2022-01-10 00:06:14,319 iteration 2176 : loss : 0.037738, loss_ce: 0.017588
 32%|████████▋                  | 128/400 [1:02:49<2:08:11, 28.28s/it]2022-01-10 00:06:15,875 iteration 2177 : loss : 0.067311, loss_ce: 0.037910
2022-01-10 00:06:17,443 iteration 2178 : loss : 0.033862, loss_ce: 0.011888
2022-01-10 00:06:19,087 iteration 2179 : loss : 0.053176, loss_ce: 0.016990
2022-01-10 00:06:20,703 iteration 2180 : loss : 0.045982, loss_ce: 0.017965
2022-01-10 00:06:22,213 iteration 2181 : loss : 0.038346, loss_ce: 0.014141
2022-01-10 00:06:23,787 iteration 2182 : loss : 0.055098, loss_ce: 0.021161
2022-01-10 00:06:25,374 iteration 2183 : loss : 0.071062, loss_ce: 0.024404
2022-01-10 00:06:26,891 iteration 2184 : loss : 0.043696, loss_ce: 0.020892
2022-01-10 00:06:28,412 iteration 2185 : loss : 0.043138, loss_ce: 0.016958
2022-01-10 00:06:30,057 iteration 2186 : loss : 0.051003, loss_ce: 0.021985
2022-01-10 00:06:31,718 iteration 2187 : loss : 0.053836, loss_ce: 0.017527
2022-01-10 00:06:33,443 iteration 2188 : loss : 0.068258, loss_ce: 0.040378
2022-01-10 00:06:35,154 iteration 2189 : loss : 0.056903, loss_ce: 0.017859
2022-01-10 00:06:36,744 iteration 2190 : loss : 0.047818, loss_ce: 0.021800
2022-01-10 00:06:38,347 iteration 2191 : loss : 0.031824, loss_ce: 0.012582
2022-01-10 00:06:39,894 iteration 2192 : loss : 0.034822, loss_ce: 0.013377
2022-01-10 00:06:41,490 iteration 2193 : loss : 0.047948, loss_ce: 0.020122
 32%|████████▋                  | 129/400 [1:03:16<2:06:13, 27.95s/it]2022-01-10 00:06:43,154 iteration 2194 : loss : 0.037391, loss_ce: 0.014433
2022-01-10 00:06:44,755 iteration 2195 : loss : 0.053874, loss_ce: 0.030230
2022-01-10 00:06:46,388 iteration 2196 : loss : 0.051402, loss_ce: 0.021717
2022-01-10 00:06:48,054 iteration 2197 : loss : 0.060702, loss_ce: 0.023973
2022-01-10 00:06:49,640 iteration 2198 : loss : 0.066465, loss_ce: 0.021440
2022-01-10 00:06:51,230 iteration 2199 : loss : 0.035134, loss_ce: 0.015325
2022-01-10 00:06:52,891 iteration 2200 : loss : 0.117636, loss_ce: 0.022366
2022-01-10 00:06:54,418 iteration 2201 : loss : 0.035223, loss_ce: 0.013351
2022-01-10 00:06:56,101 iteration 2202 : loss : 0.050953, loss_ce: 0.021519
2022-01-10 00:06:57,710 iteration 2203 : loss : 0.054041, loss_ce: 0.019287
2022-01-10 00:06:59,227 iteration 2204 : loss : 0.040169, loss_ce: 0.013961
2022-01-10 00:07:00,820 iteration 2205 : loss : 0.051648, loss_ce: 0.027923
2022-01-10 00:07:02,486 iteration 2206 : loss : 0.066992, loss_ce: 0.026767
2022-01-10 00:07:04,098 iteration 2207 : loss : 0.047741, loss_ce: 0.015528
2022-01-10 00:07:05,740 iteration 2208 : loss : 0.086484, loss_ce: 0.026867
2022-01-10 00:07:07,324 iteration 2209 : loss : 0.075462, loss_ce: 0.025433
2022-01-10 00:07:07,325 Training Data Eval:
2022-01-10 00:07:15,380   Average segmentation loss on training set: 0.0352
2022-01-10 00:07:15,380 Validation Data Eval:
2022-01-10 00:07:18,159   Average segmentation loss on validation set: 0.0836
2022-01-10 00:07:19,762 iteration 2210 : loss : 0.052722, loss_ce: 0.027411
 32%|████████▊                  | 130/400 [1:03:54<2:19:42, 31.04s/it]2022-01-10 00:07:21,401 iteration 2211 : loss : 0.041881, loss_ce: 0.011784
2022-01-10 00:07:22,931 iteration 2212 : loss : 0.051665, loss_ce: 0.021968
2022-01-10 00:07:24,513 iteration 2213 : loss : 0.066121, loss_ce: 0.015913
2022-01-10 00:07:26,018 iteration 2214 : loss : 0.047752, loss_ce: 0.015628
2022-01-10 00:07:27,516 iteration 2215 : loss : 0.030920, loss_ce: 0.012301
2022-01-10 00:07:29,058 iteration 2216 : loss : 0.034865, loss_ce: 0.015731
2022-01-10 00:07:30,579 iteration 2217 : loss : 0.044598, loss_ce: 0.022406
2022-01-10 00:07:32,128 iteration 2218 : loss : 0.038158, loss_ce: 0.014648
2022-01-10 00:07:33,729 iteration 2219 : loss : 0.054313, loss_ce: 0.025046
2022-01-10 00:07:35,384 iteration 2220 : loss : 0.066611, loss_ce: 0.034472
2022-01-10 00:07:37,030 iteration 2221 : loss : 0.058308, loss_ce: 0.018209
2022-01-10 00:07:38,530 iteration 2222 : loss : 0.040887, loss_ce: 0.017929
2022-01-10 00:07:40,055 iteration 2223 : loss : 0.045595, loss_ce: 0.015662
2022-01-10 00:07:41,691 iteration 2224 : loss : 0.054109, loss_ce: 0.021646
2022-01-10 00:07:43,283 iteration 2225 : loss : 0.058946, loss_ce: 0.020205
2022-01-10 00:07:44,925 iteration 2226 : loss : 0.041082, loss_ce: 0.018795
2022-01-10 00:07:46,547 iteration 2227 : loss : 0.070621, loss_ce: 0.025832
 33%|████████▊                  | 131/400 [1:04:21<2:13:27, 29.77s/it]2022-01-10 00:07:48,256 iteration 2228 : loss : 0.054446, loss_ce: 0.021582
2022-01-10 00:07:49,851 iteration 2229 : loss : 0.044114, loss_ce: 0.017730
2022-01-10 00:07:51,574 iteration 2230 : loss : 0.057261, loss_ce: 0.022262
2022-01-10 00:07:53,149 iteration 2231 : loss : 0.077174, loss_ce: 0.041380
2022-01-10 00:07:54,715 iteration 2232 : loss : 0.053519, loss_ce: 0.026961
2022-01-10 00:07:56,477 iteration 2233 : loss : 0.101606, loss_ce: 0.027779
2022-01-10 00:07:58,091 iteration 2234 : loss : 0.073016, loss_ce: 0.018933
2022-01-10 00:07:59,677 iteration 2235 : loss : 0.045184, loss_ce: 0.019398
2022-01-10 00:08:01,244 iteration 2236 : loss : 0.041538, loss_ce: 0.018641
2022-01-10 00:08:02,848 iteration 2237 : loss : 0.039225, loss_ce: 0.016800
2022-01-10 00:08:04,380 iteration 2238 : loss : 0.044247, loss_ce: 0.014061
2022-01-10 00:08:05,939 iteration 2239 : loss : 0.043428, loss_ce: 0.012721
2022-01-10 00:08:07,440 iteration 2240 : loss : 0.045599, loss_ce: 0.012985
2022-01-10 00:08:09,013 iteration 2241 : loss : 0.059932, loss_ce: 0.023951
2022-01-10 00:08:10,652 iteration 2242 : loss : 0.061800, loss_ce: 0.023996
2022-01-10 00:08:12,266 iteration 2243 : loss : 0.057264, loss_ce: 0.024640
2022-01-10 00:08:13,828 iteration 2244 : loss : 0.027831, loss_ce: 0.009508
 33%|████████▉                  | 132/400 [1:04:48<2:09:37, 29.02s/it]2022-01-10 00:08:15,453 iteration 2245 : loss : 0.036719, loss_ce: 0.012343
2022-01-10 00:08:16,941 iteration 2246 : loss : 0.034180, loss_ce: 0.010687
2022-01-10 00:08:18,526 iteration 2247 : loss : 0.044756, loss_ce: 0.021772
2022-01-10 00:08:20,171 iteration 2248 : loss : 0.063736, loss_ce: 0.027649
2022-01-10 00:08:21,752 iteration 2249 : loss : 0.036314, loss_ce: 0.016800
2022-01-10 00:08:23,335 iteration 2250 : loss : 0.039450, loss_ce: 0.015036
2022-01-10 00:08:24,945 iteration 2251 : loss : 0.033843, loss_ce: 0.015116
2022-01-10 00:08:26,529 iteration 2252 : loss : 0.053976, loss_ce: 0.020233
2022-01-10 00:08:28,206 iteration 2253 : loss : 0.054322, loss_ce: 0.021104
2022-01-10 00:08:29,835 iteration 2254 : loss : 0.036747, loss_ce: 0.014847
2022-01-10 00:08:31,415 iteration 2255 : loss : 0.031053, loss_ce: 0.012656
2022-01-10 00:08:32,981 iteration 2256 : loss : 0.031785, loss_ce: 0.012717
2022-01-10 00:08:34,512 iteration 2257 : loss : 0.050468, loss_ce: 0.017119
2022-01-10 00:08:36,039 iteration 2258 : loss : 0.057329, loss_ce: 0.022462
2022-01-10 00:08:37,676 iteration 2259 : loss : 0.051883, loss_ce: 0.022925
2022-01-10 00:08:39,191 iteration 2260 : loss : 0.037160, loss_ce: 0.015785
2022-01-10 00:08:40,786 iteration 2261 : loss : 0.068719, loss_ce: 0.030349
 33%|████████▉                  | 133/400 [1:05:15<2:06:23, 28.40s/it]2022-01-10 00:08:42,374 iteration 2262 : loss : 0.040785, loss_ce: 0.021245
2022-01-10 00:08:44,017 iteration 2263 : loss : 0.061384, loss_ce: 0.024297
2022-01-10 00:08:45,686 iteration 2264 : loss : 0.046077, loss_ce: 0.014899
2022-01-10 00:08:47,294 iteration 2265 : loss : 0.042748, loss_ce: 0.017540
2022-01-10 00:08:48,847 iteration 2266 : loss : 0.042546, loss_ce: 0.018481
2022-01-10 00:08:50,401 iteration 2267 : loss : 0.033477, loss_ce: 0.012094
2022-01-10 00:08:51,980 iteration 2268 : loss : 0.040802, loss_ce: 0.016619
2022-01-10 00:08:53,517 iteration 2269 : loss : 0.033606, loss_ce: 0.017114
2022-01-10 00:08:55,088 iteration 2270 : loss : 0.049154, loss_ce: 0.017855
2022-01-10 00:08:56,635 iteration 2271 : loss : 0.046533, loss_ce: 0.018667
2022-01-10 00:08:58,199 iteration 2272 : loss : 0.054570, loss_ce: 0.024412
2022-01-10 00:08:59,797 iteration 2273 : loss : 0.046399, loss_ce: 0.016352
2022-01-10 00:09:01,397 iteration 2274 : loss : 0.044622, loss_ce: 0.014715
2022-01-10 00:09:03,034 iteration 2275 : loss : 0.053710, loss_ce: 0.018476
2022-01-10 00:09:04,560 iteration 2276 : loss : 0.035412, loss_ce: 0.014933
2022-01-10 00:09:06,144 iteration 2277 : loss : 0.050091, loss_ce: 0.017180
2022-01-10 00:09:07,675 iteration 2278 : loss : 0.035126, loss_ce: 0.012475
 34%|█████████                  | 134/400 [1:05:42<2:03:54, 27.95s/it]2022-01-10 00:09:09,250 iteration 2279 : loss : 0.081110, loss_ce: 0.032088
2022-01-10 00:09:10,832 iteration 2280 : loss : 0.056471, loss_ce: 0.031421
2022-01-10 00:09:12,408 iteration 2281 : loss : 0.056410, loss_ce: 0.014507
2022-01-10 00:09:14,015 iteration 2282 : loss : 0.036030, loss_ce: 0.013197
2022-01-10 00:09:15,673 iteration 2283 : loss : 0.053055, loss_ce: 0.025398
2022-01-10 00:09:17,229 iteration 2284 : loss : 0.038272, loss_ce: 0.017909
2022-01-10 00:09:18,791 iteration 2285 : loss : 0.032028, loss_ce: 0.012013
2022-01-10 00:09:20,351 iteration 2286 : loss : 0.040593, loss_ce: 0.014899
2022-01-10 00:09:21,914 iteration 2287 : loss : 0.038565, loss_ce: 0.016340
2022-01-10 00:09:23,526 iteration 2288 : loss : 0.040348, loss_ce: 0.015607
2022-01-10 00:09:25,059 iteration 2289 : loss : 0.038217, loss_ce: 0.011430
2022-01-10 00:09:26,690 iteration 2290 : loss : 0.047355, loss_ce: 0.025748
2022-01-10 00:09:28,262 iteration 2291 : loss : 0.031299, loss_ce: 0.011638
2022-01-10 00:09:29,816 iteration 2292 : loss : 0.051835, loss_ce: 0.024966
2022-01-10 00:09:31,313 iteration 2293 : loss : 0.052529, loss_ce: 0.031268
2022-01-10 00:09:32,918 iteration 2294 : loss : 0.064256, loss_ce: 0.021365
2022-01-10 00:09:32,918 Training Data Eval:
2022-01-10 00:09:40,993   Average segmentation loss on training set: 0.0289
2022-01-10 00:09:40,993 Validation Data Eval:
2022-01-10 00:09:43,767   Average segmentation loss on validation set: 0.0970
2022-01-10 00:09:45,415 iteration 2295 : loss : 0.082016, loss_ce: 0.035933
 34%|█████████                  | 135/400 [1:06:20<2:16:24, 30.89s/it]2022-01-10 00:09:46,972 iteration 2296 : loss : 0.030202, loss_ce: 0.012081
2022-01-10 00:09:48,531 iteration 2297 : loss : 0.033737, loss_ce: 0.013307
2022-01-10 00:09:50,090 iteration 2298 : loss : 0.032746, loss_ce: 0.014417
2022-01-10 00:09:51,733 iteration 2299 : loss : 0.047865, loss_ce: 0.021885
2022-01-10 00:09:53,276 iteration 2300 : loss : 0.040403, loss_ce: 0.018360
2022-01-10 00:09:54,924 iteration 2301 : loss : 0.039311, loss_ce: 0.011950
2022-01-10 00:09:56,542 iteration 2302 : loss : 0.090022, loss_ce: 0.036421
2022-01-10 00:09:58,138 iteration 2303 : loss : 0.040716, loss_ce: 0.014253
2022-01-10 00:09:59,793 iteration 2304 : loss : 0.046794, loss_ce: 0.016842
2022-01-10 00:10:01,411 iteration 2305 : loss : 0.040146, loss_ce: 0.015588
2022-01-10 00:10:03,091 iteration 2306 : loss : 0.041572, loss_ce: 0.021371
2022-01-10 00:10:04,659 iteration 2307 : loss : 0.053711, loss_ce: 0.018015
2022-01-10 00:10:06,197 iteration 2308 : loss : 0.039143, loss_ce: 0.013311
2022-01-10 00:10:07,748 iteration 2309 : loss : 0.047822, loss_ce: 0.022460
2022-01-10 00:10:09,389 iteration 2310 : loss : 0.045393, loss_ce: 0.020129
2022-01-10 00:10:10,891 iteration 2311 : loss : 0.057994, loss_ce: 0.017605
2022-01-10 00:10:12,474 iteration 2312 : loss : 0.061812, loss_ce: 0.021017
 34%|█████████▏                 | 136/400 [1:06:47<2:10:49, 29.73s/it]2022-01-10 00:10:14,142 iteration 2313 : loss : 0.035503, loss_ce: 0.013860
2022-01-10 00:10:15,621 iteration 2314 : loss : 0.032580, loss_ce: 0.010631
2022-01-10 00:10:17,151 iteration 2315 : loss : 0.034554, loss_ce: 0.013490
2022-01-10 00:10:18,805 iteration 2316 : loss : 0.070578, loss_ce: 0.035389
2022-01-10 00:10:20,316 iteration 2317 : loss : 0.035636, loss_ce: 0.011133
2022-01-10 00:10:21,866 iteration 2318 : loss : 0.042043, loss_ce: 0.021052
2022-01-10 00:10:23,439 iteration 2319 : loss : 0.039191, loss_ce: 0.016798
2022-01-10 00:10:25,088 iteration 2320 : loss : 0.049101, loss_ce: 0.020497
2022-01-10 00:10:26,662 iteration 2321 : loss : 0.045521, loss_ce: 0.019056
2022-01-10 00:10:28,251 iteration 2322 : loss : 0.038776, loss_ce: 0.016681
2022-01-10 00:10:29,741 iteration 2323 : loss : 0.035103, loss_ce: 0.015034
2022-01-10 00:10:31,298 iteration 2324 : loss : 0.034839, loss_ce: 0.016230
2022-01-10 00:10:32,756 iteration 2325 : loss : 0.034441, loss_ce: 0.015934
2022-01-10 00:10:34,308 iteration 2326 : loss : 0.052451, loss_ce: 0.014486
2022-01-10 00:10:35,841 iteration 2327 : loss : 0.040799, loss_ce: 0.015687
2022-01-10 00:10:37,408 iteration 2328 : loss : 0.041118, loss_ce: 0.018821
2022-01-10 00:10:38,926 iteration 2329 : loss : 0.034279, loss_ce: 0.013849
 34%|█████████▏                 | 137/400 [1:07:14<2:06:02, 28.75s/it]2022-01-10 00:10:40,569 iteration 2330 : loss : 0.051872, loss_ce: 0.022390
2022-01-10 00:10:42,097 iteration 2331 : loss : 0.032255, loss_ce: 0.012347
2022-01-10 00:10:43,746 iteration 2332 : loss : 0.031838, loss_ce: 0.011694
2022-01-10 00:10:45,320 iteration 2333 : loss : 0.061219, loss_ce: 0.027997
2022-01-10 00:10:47,019 iteration 2334 : loss : 0.034316, loss_ce: 0.014640
2022-01-10 00:10:48,704 iteration 2335 : loss : 0.059904, loss_ce: 0.030164
2022-01-10 00:10:50,355 iteration 2336 : loss : 0.043350, loss_ce: 0.019758
2022-01-10 00:10:51,939 iteration 2337 : loss : 0.053587, loss_ce: 0.018335
2022-01-10 00:10:53,554 iteration 2338 : loss : 0.051731, loss_ce: 0.019115
2022-01-10 00:10:55,157 iteration 2339 : loss : 0.065532, loss_ce: 0.021956
2022-01-10 00:10:56,678 iteration 2340 : loss : 0.048162, loss_ce: 0.017958
2022-01-10 00:10:58,270 iteration 2341 : loss : 0.035978, loss_ce: 0.014866
2022-01-10 00:10:59,872 iteration 2342 : loss : 0.032182, loss_ce: 0.012791
2022-01-10 00:11:01,574 iteration 2343 : loss : 0.048366, loss_ce: 0.018799
2022-01-10 00:11:03,113 iteration 2344 : loss : 0.059736, loss_ce: 0.016891
2022-01-10 00:11:04,663 iteration 2345 : loss : 0.056782, loss_ce: 0.028356
2022-01-10 00:11:06,231 iteration 2346 : loss : 0.054650, loss_ce: 0.023897
 34%|█████████▎                 | 138/400 [1:07:41<2:03:39, 28.32s/it]2022-01-10 00:11:07,955 iteration 2347 : loss : 0.065774, loss_ce: 0.028447
2022-01-10 00:11:09,553 iteration 2348 : loss : 0.035766, loss_ce: 0.013339
2022-01-10 00:11:11,104 iteration 2349 : loss : 0.040720, loss_ce: 0.014215
2022-01-10 00:11:12,668 iteration 2350 : loss : 0.067988, loss_ce: 0.020627
2022-01-10 00:11:14,321 iteration 2351 : loss : 0.041922, loss_ce: 0.019208
2022-01-10 00:11:15,855 iteration 2352 : loss : 0.032254, loss_ce: 0.015510
2022-01-10 00:11:17,399 iteration 2353 : loss : 0.031161, loss_ce: 0.013396
2022-01-10 00:11:18,927 iteration 2354 : loss : 0.026903, loss_ce: 0.012334
2022-01-10 00:11:20,467 iteration 2355 : loss : 0.052603, loss_ce: 0.019814
2022-01-10 00:11:22,089 iteration 2356 : loss : 0.054883, loss_ce: 0.020994
2022-01-10 00:11:23,676 iteration 2357 : loss : 0.072513, loss_ce: 0.022525
2022-01-10 00:11:25,265 iteration 2358 : loss : 0.040928, loss_ce: 0.016092
2022-01-10 00:11:26,817 iteration 2359 : loss : 0.046923, loss_ce: 0.015312
2022-01-10 00:11:28,344 iteration 2360 : loss : 0.050530, loss_ce: 0.012773
2022-01-10 00:11:29,913 iteration 2361 : loss : 0.044525, loss_ce: 0.015652
2022-01-10 00:11:31,605 iteration 2362 : loss : 0.081528, loss_ce: 0.035606
2022-01-10 00:11:33,191 iteration 2363 : loss : 0.037155, loss_ce: 0.017807
 35%|█████████▍                 | 139/400 [1:08:08<2:01:24, 27.91s/it]2022-01-10 00:11:34,903 iteration 2364 : loss : 0.038837, loss_ce: 0.013719
2022-01-10 00:11:36,575 iteration 2365 : loss : 0.061258, loss_ce: 0.022606
2022-01-10 00:11:38,203 iteration 2366 : loss : 0.047836, loss_ce: 0.019739
2022-01-10 00:11:39,731 iteration 2367 : loss : 0.049003, loss_ce: 0.011543
2022-01-10 00:11:41,261 iteration 2368 : loss : 0.046887, loss_ce: 0.021416
2022-01-10 00:11:42,847 iteration 2369 : loss : 0.050201, loss_ce: 0.014953
2022-01-10 00:11:44,414 iteration 2370 : loss : 0.049862, loss_ce: 0.020868
2022-01-10 00:11:46,030 iteration 2371 : loss : 0.053275, loss_ce: 0.021569
2022-01-10 00:11:47,565 iteration 2372 : loss : 0.039773, loss_ce: 0.014512
2022-01-10 00:11:49,124 iteration 2373 : loss : 0.035873, loss_ce: 0.013938
2022-01-10 00:11:50,646 iteration 2374 : loss : 0.047216, loss_ce: 0.020686
2022-01-10 00:11:52,186 iteration 2375 : loss : 0.034477, loss_ce: 0.011950
2022-01-10 00:11:53,778 iteration 2376 : loss : 0.048067, loss_ce: 0.025798
2022-01-10 00:11:55,365 iteration 2377 : loss : 0.045611, loss_ce: 0.023394
2022-01-10 00:11:56,953 iteration 2378 : loss : 0.036563, loss_ce: 0.014459
2022-01-10 00:11:58,566 iteration 2379 : loss : 0.064087, loss_ce: 0.031237
2022-01-10 00:11:58,566 Training Data Eval:
2022-01-10 00:12:06,636   Average segmentation loss on training set: 0.0296
2022-01-10 00:12:06,637 Validation Data Eval:
2022-01-10 00:12:09,408   Average segmentation loss on validation set: 0.0829
2022-01-10 00:12:10,943 iteration 2380 : loss : 0.037432, loss_ce: 0.016950
 35%|█████████▍                 | 140/400 [1:08:46<2:13:43, 30.86s/it]2022-01-10 00:12:12,522 iteration 2381 : loss : 0.053663, loss_ce: 0.018892
2022-01-10 00:12:14,128 iteration 2382 : loss : 0.059240, loss_ce: 0.023943
2022-01-10 00:12:15,661 iteration 2383 : loss : 0.035627, loss_ce: 0.013459
2022-01-10 00:12:17,283 iteration 2384 : loss : 0.053732, loss_ce: 0.022959
2022-01-10 00:12:18,919 iteration 2385 : loss : 0.054565, loss_ce: 0.021782
2022-01-10 00:12:20,487 iteration 2386 : loss : 0.040896, loss_ce: 0.015280
2022-01-10 00:12:22,029 iteration 2387 : loss : 0.041358, loss_ce: 0.013961
2022-01-10 00:12:23,605 iteration 2388 : loss : 0.052599, loss_ce: 0.024038
2022-01-10 00:12:25,119 iteration 2389 : loss : 0.019917, loss_ce: 0.006794
2022-01-10 00:12:26,704 iteration 2390 : loss : 0.039237, loss_ce: 0.020865
2022-01-10 00:12:28,262 iteration 2391 : loss : 0.048169, loss_ce: 0.029408
2022-01-10 00:12:29,863 iteration 2392 : loss : 0.051883, loss_ce: 0.021000
2022-01-10 00:12:31,467 iteration 2393 : loss : 0.051554, loss_ce: 0.015315
2022-01-10 00:12:32,954 iteration 2394 : loss : 0.038406, loss_ce: 0.012674
2022-01-10 00:12:34,580 iteration 2395 : loss : 0.046117, loss_ce: 0.019285
2022-01-10 00:12:36,113 iteration 2396 : loss : 0.030380, loss_ce: 0.013910
2022-01-10 00:12:37,714 iteration 2397 : loss : 0.050489, loss_ce: 0.020402
 35%|█████████▌                 | 141/400 [1:09:12<2:07:56, 29.64s/it]2022-01-10 00:12:39,338 iteration 2398 : loss : 0.049722, loss_ce: 0.019167
2022-01-10 00:12:40,879 iteration 2399 : loss : 0.044107, loss_ce: 0.015304
2022-01-10 00:12:42,511 iteration 2400 : loss : 0.045110, loss_ce: 0.020811
2022-01-10 00:12:44,087 iteration 2401 : loss : 0.044730, loss_ce: 0.020218
2022-01-10 00:12:45,714 iteration 2402 : loss : 0.032802, loss_ce: 0.017333
2022-01-10 00:12:47,258 iteration 2403 : loss : 0.037667, loss_ce: 0.015866
2022-01-10 00:12:48,778 iteration 2404 : loss : 0.030353, loss_ce: 0.015195
2022-01-10 00:12:50,270 iteration 2405 : loss : 0.041777, loss_ce: 0.015084
2022-01-10 00:12:51,969 iteration 2406 : loss : 0.051079, loss_ce: 0.014741
2022-01-10 00:12:53,483 iteration 2407 : loss : 0.050213, loss_ce: 0.028207
2022-01-10 00:12:54,999 iteration 2408 : loss : 0.028212, loss_ce: 0.012625
2022-01-10 00:12:56,617 iteration 2409 : loss : 0.036157, loss_ce: 0.013661
2022-01-10 00:12:58,247 iteration 2410 : loss : 0.061176, loss_ce: 0.030045
2022-01-10 00:12:59,856 iteration 2411 : loss : 0.052883, loss_ce: 0.015103
2022-01-10 00:13:01,392 iteration 2412 : loss : 0.025676, loss_ce: 0.009466
2022-01-10 00:13:03,040 iteration 2413 : loss : 0.059970, loss_ce: 0.026016
2022-01-10 00:13:04,626 iteration 2414 : loss : 0.044315, loss_ce: 0.019679
 36%|█████████▌                 | 142/400 [1:09:39<2:03:54, 28.82s/it]2022-01-10 00:13:06,274 iteration 2415 : loss : 0.052315, loss_ce: 0.018146
2022-01-10 00:13:07,832 iteration 2416 : loss : 0.040786, loss_ce: 0.015627
2022-01-10 00:13:09,407 iteration 2417 : loss : 0.062938, loss_ce: 0.020488
2022-01-10 00:13:11,069 iteration 2418 : loss : 0.033103, loss_ce: 0.012345
2022-01-10 00:13:12,664 iteration 2419 : loss : 0.037578, loss_ce: 0.018646
2022-01-10 00:13:14,346 iteration 2420 : loss : 0.041117, loss_ce: 0.018211
2022-01-10 00:13:16,004 iteration 2421 : loss : 0.046231, loss_ce: 0.017743
2022-01-10 00:13:17,641 iteration 2422 : loss : 0.040640, loss_ce: 0.015734
2022-01-10 00:13:19,221 iteration 2423 : loss : 0.068416, loss_ce: 0.024855
2022-01-10 00:13:20,787 iteration 2424 : loss : 0.030434, loss_ce: 0.013280
2022-01-10 00:13:22,470 iteration 2425 : loss : 0.041029, loss_ce: 0.013945
2022-01-10 00:13:24,033 iteration 2426 : loss : 0.051842, loss_ce: 0.015598
2022-01-10 00:13:25,519 iteration 2427 : loss : 0.039456, loss_ce: 0.017526
2022-01-10 00:13:27,053 iteration 2428 : loss : 0.046285, loss_ce: 0.012665
2022-01-10 00:13:28,594 iteration 2429 : loss : 0.050443, loss_ce: 0.020451
2022-01-10 00:13:30,133 iteration 2430 : loss : 0.026814, loss_ce: 0.010125
2022-01-10 00:13:31,702 iteration 2431 : loss : 0.048561, loss_ce: 0.025794
 36%|█████████▋                 | 143/400 [1:10:06<2:01:11, 28.29s/it]2022-01-10 00:13:33,291 iteration 2432 : loss : 0.038376, loss_ce: 0.011829
2022-01-10 00:13:34,850 iteration 2433 : loss : 0.045829, loss_ce: 0.020951
2022-01-10 00:13:36,425 iteration 2434 : loss : 0.036605, loss_ce: 0.012038
2022-01-10 00:13:38,082 iteration 2435 : loss : 0.063288, loss_ce: 0.036189
2022-01-10 00:13:39,576 iteration 2436 : loss : 0.033286, loss_ce: 0.014179
2022-01-10 00:13:41,191 iteration 2437 : loss : 0.033079, loss_ce: 0.010388
2022-01-10 00:13:42,818 iteration 2438 : loss : 0.042182, loss_ce: 0.013200
2022-01-10 00:13:44,435 iteration 2439 : loss : 0.029315, loss_ce: 0.011099
2022-01-10 00:13:46,063 iteration 2440 : loss : 0.042392, loss_ce: 0.017705
2022-01-10 00:13:47,629 iteration 2441 : loss : 0.045363, loss_ce: 0.015304
2022-01-10 00:13:49,186 iteration 2442 : loss : 0.055343, loss_ce: 0.022137
2022-01-10 00:13:50,691 iteration 2443 : loss : 0.043373, loss_ce: 0.016512
2022-01-10 00:13:52,261 iteration 2444 : loss : 0.048120, loss_ce: 0.021544
2022-01-10 00:13:53,845 iteration 2445 : loss : 0.040783, loss_ce: 0.016733
2022-01-10 00:13:55,507 iteration 2446 : loss : 0.045351, loss_ce: 0.019121
2022-01-10 00:13:57,024 iteration 2447 : loss : 0.035599, loss_ce: 0.011484
2022-01-10 00:13:58,642 iteration 2448 : loss : 0.038832, loss_ce: 0.015482
 36%|█████████▋                 | 144/400 [1:10:33<1:58:59, 27.89s/it]2022-01-10 00:14:00,207 iteration 2449 : loss : 0.061698, loss_ce: 0.026827
2022-01-10 00:14:01,720 iteration 2450 : loss : 0.031915, loss_ce: 0.017941
2022-01-10 00:14:03,271 iteration 2451 : loss : 0.038345, loss_ce: 0.013832
2022-01-10 00:14:04,803 iteration 2452 : loss : 0.048044, loss_ce: 0.021095
2022-01-10 00:14:06,382 iteration 2453 : loss : 0.050374, loss_ce: 0.016420
2022-01-10 00:14:07,936 iteration 2454 : loss : 0.041086, loss_ce: 0.014453
2022-01-10 00:14:09,462 iteration 2455 : loss : 0.043709, loss_ce: 0.021920
2022-01-10 00:14:10,973 iteration 2456 : loss : 0.042406, loss_ce: 0.020253
2022-01-10 00:14:12,522 iteration 2457 : loss : 0.032062, loss_ce: 0.012610
2022-01-10 00:14:14,043 iteration 2458 : loss : 0.053410, loss_ce: 0.021910
2022-01-10 00:14:15,519 iteration 2459 : loss : 0.025925, loss_ce: 0.010624
2022-01-10 00:14:17,055 iteration 2460 : loss : 0.051785, loss_ce: 0.012950
2022-01-10 00:14:18,686 iteration 2461 : loss : 0.041218, loss_ce: 0.014252
2022-01-10 00:14:20,284 iteration 2462 : loss : 0.063410, loss_ce: 0.023524
2022-01-10 00:14:21,835 iteration 2463 : loss : 0.035443, loss_ce: 0.015618
2022-01-10 00:14:23,428 iteration 2464 : loss : 0.026949, loss_ce: 0.009989
2022-01-10 00:14:23,428 Training Data Eval:
2022-01-10 00:14:31,506   Average segmentation loss on training set: 0.0267
2022-01-10 00:14:31,506 Validation Data Eval:
2022-01-10 00:14:34,282   Average segmentation loss on validation set: 0.0873
2022-01-10 00:14:35,835 iteration 2465 : loss : 0.032332, loss_ce: 0.012029
 36%|█████████▊                 | 145/400 [1:11:10<2:10:23, 30.68s/it]2022-01-10 00:14:37,522 iteration 2466 : loss : 0.066459, loss_ce: 0.022064
2022-01-10 00:14:39,173 iteration 2467 : loss : 0.040050, loss_ce: 0.017363
2022-01-10 00:14:40,724 iteration 2468 : loss : 0.037873, loss_ce: 0.016770
2022-01-10 00:14:42,244 iteration 2469 : loss : 0.027443, loss_ce: 0.011282
2022-01-10 00:14:43,802 iteration 2470 : loss : 0.041032, loss_ce: 0.012987
2022-01-10 00:14:45,457 iteration 2471 : loss : 0.057425, loss_ce: 0.015889
2022-01-10 00:14:46,995 iteration 2472 : loss : 0.072244, loss_ce: 0.032211
2022-01-10 00:14:48,618 iteration 2473 : loss : 0.040231, loss_ce: 0.016506
2022-01-10 00:14:50,156 iteration 2474 : loss : 0.036295, loss_ce: 0.012890
2022-01-10 00:14:51,728 iteration 2475 : loss : 0.042694, loss_ce: 0.014960
2022-01-10 00:14:53,245 iteration 2476 : loss : 0.034946, loss_ce: 0.017759
2022-01-10 00:14:54,897 iteration 2477 : loss : 0.047689, loss_ce: 0.018438
2022-01-10 00:14:56,501 iteration 2478 : loss : 0.036137, loss_ce: 0.014109
2022-01-10 00:14:57,993 iteration 2479 : loss : 0.050555, loss_ce: 0.013703
2022-01-10 00:14:59,568 iteration 2480 : loss : 0.040712, loss_ce: 0.018115
2022-01-10 00:15:01,195 iteration 2481 : loss : 0.035539, loss_ce: 0.013753
2022-01-10 00:15:02,743 iteration 2482 : loss : 0.030309, loss_ce: 0.013242
 36%|█████████▊                 | 146/400 [1:11:37<2:05:05, 29.55s/it]2022-01-10 00:15:04,454 iteration 2483 : loss : 0.051949, loss_ce: 0.025165
2022-01-10 00:15:06,057 iteration 2484 : loss : 0.054577, loss_ce: 0.014376
2022-01-10 00:15:07,593 iteration 2485 : loss : 0.038347, loss_ce: 0.016947
2022-01-10 00:15:09,155 iteration 2486 : loss : 0.032773, loss_ce: 0.013031
2022-01-10 00:15:10,709 iteration 2487 : loss : 0.049922, loss_ce: 0.017616
2022-01-10 00:15:12,334 iteration 2488 : loss : 0.046384, loss_ce: 0.019651
2022-01-10 00:15:13,892 iteration 2489 : loss : 0.048361, loss_ce: 0.023179
2022-01-10 00:15:15,514 iteration 2490 : loss : 0.054371, loss_ce: 0.024636
2022-01-10 00:15:17,165 iteration 2491 : loss : 0.045318, loss_ce: 0.021316
2022-01-10 00:15:18,650 iteration 2492 : loss : 0.031244, loss_ce: 0.012033
2022-01-10 00:15:20,271 iteration 2493 : loss : 0.037987, loss_ce: 0.016760
2022-01-10 00:15:21,827 iteration 2494 : loss : 0.057716, loss_ce: 0.018491
2022-01-10 00:15:23,377 iteration 2495 : loss : 0.037641, loss_ce: 0.020001
2022-01-10 00:15:25,128 iteration 2496 : loss : 0.053308, loss_ce: 0.016327
2022-01-10 00:15:26,750 iteration 2497 : loss : 0.058727, loss_ce: 0.025174
2022-01-10 00:15:28,378 iteration 2498 : loss : 0.075784, loss_ce: 0.037098
2022-01-10 00:15:29,896 iteration 2499 : loss : 0.027274, loss_ce: 0.007406
 37%|█████████▉                 | 147/400 [1:12:04<2:01:34, 28.83s/it]2022-01-10 00:15:31,506 iteration 2500 : loss : 0.041690, loss_ce: 0.013519
2022-01-10 00:15:33,069 iteration 2501 : loss : 0.041580, loss_ce: 0.012972
2022-01-10 00:15:34,711 iteration 2502 : loss : 0.063951, loss_ce: 0.023410
2022-01-10 00:15:36,229 iteration 2503 : loss : 0.042559, loss_ce: 0.015813
2022-01-10 00:15:37,766 iteration 2504 : loss : 0.029995, loss_ce: 0.009896
2022-01-10 00:15:39,377 iteration 2505 : loss : 0.056102, loss_ce: 0.023877
2022-01-10 00:15:40,946 iteration 2506 : loss : 0.053513, loss_ce: 0.017137
2022-01-10 00:15:42,472 iteration 2507 : loss : 0.052794, loss_ce: 0.028148
2022-01-10 00:15:44,018 iteration 2508 : loss : 0.052943, loss_ce: 0.021028
2022-01-10 00:15:45,572 iteration 2509 : loss : 0.034097, loss_ce: 0.011322
2022-01-10 00:15:47,111 iteration 2510 : loss : 0.033189, loss_ce: 0.014068
2022-01-10 00:15:48,691 iteration 2511 : loss : 0.042083, loss_ce: 0.018090
2022-01-10 00:15:50,284 iteration 2512 : loss : 0.042723, loss_ce: 0.015534
2022-01-10 00:15:51,911 iteration 2513 : loss : 0.054076, loss_ce: 0.018450
2022-01-10 00:15:53,491 iteration 2514 : loss : 0.047588, loss_ce: 0.016011
2022-01-10 00:15:55,051 iteration 2515 : loss : 0.030898, loss_ce: 0.016673
2022-01-10 00:15:56,603 iteration 2516 : loss : 0.041873, loss_ce: 0.022383
 37%|█████████▉                 | 148/400 [1:12:31<1:58:24, 28.19s/it]2022-01-10 00:15:58,203 iteration 2517 : loss : 0.036816, loss_ce: 0.014834
2022-01-10 00:15:59,779 iteration 2518 : loss : 0.055591, loss_ce: 0.018390
2022-01-10 00:16:01,376 iteration 2519 : loss : 0.036754, loss_ce: 0.013513
2022-01-10 00:16:02,953 iteration 2520 : loss : 0.051719, loss_ce: 0.021453
2022-01-10 00:16:04,508 iteration 2521 : loss : 0.042053, loss_ce: 0.015505
2022-01-10 00:16:06,088 iteration 2522 : loss : 0.035571, loss_ce: 0.017646
2022-01-10 00:16:07,713 iteration 2523 : loss : 0.060283, loss_ce: 0.014812
2022-01-10 00:16:09,268 iteration 2524 : loss : 0.038606, loss_ce: 0.016333
2022-01-10 00:16:10,851 iteration 2525 : loss : 0.031318, loss_ce: 0.013559
2022-01-10 00:16:12,449 iteration 2526 : loss : 0.052396, loss_ce: 0.017327
2022-01-10 00:16:14,027 iteration 2527 : loss : 0.038003, loss_ce: 0.013334
2022-01-10 00:16:15,625 iteration 2528 : loss : 0.047875, loss_ce: 0.017452
2022-01-10 00:16:17,206 iteration 2529 : loss : 0.063228, loss_ce: 0.033054
2022-01-10 00:16:18,792 iteration 2530 : loss : 0.036971, loss_ce: 0.016853
2022-01-10 00:16:20,444 iteration 2531 : loss : 0.050288, loss_ce: 0.019694
2022-01-10 00:16:22,015 iteration 2532 : loss : 0.043141, loss_ce: 0.015191
2022-01-10 00:16:23,604 iteration 2533 : loss : 0.047040, loss_ce: 0.012977
 37%|██████████                 | 149/400 [1:12:58<1:56:26, 27.83s/it]2022-01-10 00:16:25,166 iteration 2534 : loss : 0.037085, loss_ce: 0.016957
2022-01-10 00:16:26,812 iteration 2535 : loss : 0.048284, loss_ce: 0.017622
2022-01-10 00:16:28,353 iteration 2536 : loss : 0.043439, loss_ce: 0.020941
2022-01-10 00:16:29,898 iteration 2537 : loss : 0.031766, loss_ce: 0.012323
2022-01-10 00:16:31,373 iteration 2538 : loss : 0.030168, loss_ce: 0.013687
2022-01-10 00:16:32,964 iteration 2539 : loss : 0.041551, loss_ce: 0.019809
2022-01-10 00:16:34,494 iteration 2540 : loss : 0.028435, loss_ce: 0.008810
2022-01-10 00:16:36,043 iteration 2541 : loss : 0.038972, loss_ce: 0.018608
2022-01-10 00:16:37,553 iteration 2542 : loss : 0.031226, loss_ce: 0.011138
2022-01-10 00:16:39,066 iteration 2543 : loss : 0.034654, loss_ce: 0.011777
2022-01-10 00:16:40,639 iteration 2544 : loss : 0.038073, loss_ce: 0.013495
2022-01-10 00:16:42,323 iteration 2545 : loss : 0.032285, loss_ce: 0.012570
2022-01-10 00:16:44,021 iteration 2546 : loss : 0.042438, loss_ce: 0.017364
2022-01-10 00:16:45,541 iteration 2547 : loss : 0.027694, loss_ce: 0.013304
2022-01-10 00:16:47,143 iteration 2548 : loss : 0.039340, loss_ce: 0.017923
2022-01-10 00:16:48,729 iteration 2549 : loss : 0.045762, loss_ce: 0.012916
2022-01-10 00:16:48,729 Training Data Eval:
2022-01-10 00:16:56,787   Average segmentation loss on training set: 0.0276
2022-01-10 00:16:56,787 Validation Data Eval:
2022-01-10 00:16:59,557   Average segmentation loss on validation set: 0.0847
2022-01-10 00:17:01,092 iteration 2550 : loss : 0.044223, loss_ce: 0.015677
 38%|██████████▏                | 150/400 [1:13:36<2:08:03, 30.73s/it]2022-01-10 00:17:02,705 iteration 2551 : loss : 0.034870, loss_ce: 0.017592
2022-01-10 00:17:04,335 iteration 2552 : loss : 0.030933, loss_ce: 0.012267
2022-01-10 00:17:05,841 iteration 2553 : loss : 0.028761, loss_ce: 0.009893
2022-01-10 00:17:07,413 iteration 2554 : loss : 0.039732, loss_ce: 0.013208
2022-01-10 00:17:08,968 iteration 2555 : loss : 0.031973, loss_ce: 0.008482
2022-01-10 00:17:10,587 iteration 2556 : loss : 0.032171, loss_ce: 0.014312
2022-01-10 00:17:12,145 iteration 2557 : loss : 0.054365, loss_ce: 0.027760
2022-01-10 00:17:13,726 iteration 2558 : loss : 0.045686, loss_ce: 0.024709
2022-01-10 00:17:15,381 iteration 2559 : loss : 0.035096, loss_ce: 0.013270
2022-01-10 00:17:16,952 iteration 2560 : loss : 0.034949, loss_ce: 0.017397
2022-01-10 00:17:18,610 iteration 2561 : loss : 0.039155, loss_ce: 0.013510
2022-01-10 00:17:20,202 iteration 2562 : loss : 0.041062, loss_ce: 0.017747
2022-01-10 00:17:21,808 iteration 2563 : loss : 0.045595, loss_ce: 0.021976
2022-01-10 00:17:23,340 iteration 2564 : loss : 0.040518, loss_ce: 0.016953
2022-01-10 00:17:24,876 iteration 2565 : loss : 0.034874, loss_ce: 0.013015
2022-01-10 00:17:26,429 iteration 2566 : loss : 0.043170, loss_ce: 0.018174
2022-01-10 00:17:28,012 iteration 2567 : loss : 0.049163, loss_ce: 0.014451
 38%|██████████▏                | 151/400 [1:14:03<2:02:47, 29.59s/it]2022-01-10 00:17:29,665 iteration 2568 : loss : 0.044056, loss_ce: 0.019679
2022-01-10 00:17:31,219 iteration 2569 : loss : 0.046117, loss_ce: 0.021522
2022-01-10 00:17:32,774 iteration 2570 : loss : 0.052300, loss_ce: 0.020544
2022-01-10 00:17:34,334 iteration 2571 : loss : 0.022845, loss_ce: 0.009324
2022-01-10 00:17:35,844 iteration 2572 : loss : 0.033326, loss_ce: 0.011526
2022-01-10 00:17:37,363 iteration 2573 : loss : 0.039943, loss_ce: 0.015858
2022-01-10 00:17:38,912 iteration 2574 : loss : 0.042471, loss_ce: 0.015315
2022-01-10 00:17:40,590 iteration 2575 : loss : 0.056538, loss_ce: 0.024333
2022-01-10 00:17:42,182 iteration 2576 : loss : 0.032153, loss_ce: 0.013848
2022-01-10 00:17:43,762 iteration 2577 : loss : 0.049397, loss_ce: 0.018489
2022-01-10 00:17:45,350 iteration 2578 : loss : 0.048009, loss_ce: 0.017194
2022-01-10 00:17:46,866 iteration 2579 : loss : 0.032307, loss_ce: 0.008634
2022-01-10 00:17:48,493 iteration 2580 : loss : 0.051398, loss_ce: 0.012285
2022-01-10 00:17:50,110 iteration 2581 : loss : 0.052733, loss_ce: 0.023267
2022-01-10 00:17:51,757 iteration 2582 : loss : 0.036417, loss_ce: 0.013753
2022-01-10 00:17:53,323 iteration 2583 : loss : 0.033850, loss_ce: 0.013099
2022-01-10 00:17:54,809 iteration 2584 : loss : 0.033980, loss_ce: 0.013559
 38%|██████████▎                | 152/400 [1:14:29<1:58:50, 28.75s/it]2022-01-10 00:17:56,440 iteration 2585 : loss : 0.031628, loss_ce: 0.010783
2022-01-10 00:17:58,020 iteration 2586 : loss : 0.047072, loss_ce: 0.020702
2022-01-10 00:17:59,532 iteration 2587 : loss : 0.030947, loss_ce: 0.010978
2022-01-10 00:18:01,078 iteration 2588 : loss : 0.033322, loss_ce: 0.016157
2022-01-10 00:18:02,687 iteration 2589 : loss : 0.041193, loss_ce: 0.013523
2022-01-10 00:18:04,309 iteration 2590 : loss : 0.032356, loss_ce: 0.014276
2022-01-10 00:18:05,849 iteration 2591 : loss : 0.031255, loss_ce: 0.010076
2022-01-10 00:18:07,390 iteration 2592 : loss : 0.040486, loss_ce: 0.015347
2022-01-10 00:18:08,939 iteration 2593 : loss : 0.032985, loss_ce: 0.014978
2022-01-10 00:18:10,637 iteration 2594 : loss : 0.050543, loss_ce: 0.025152
2022-01-10 00:18:12,252 iteration 2595 : loss : 0.045336, loss_ce: 0.016621
2022-01-10 00:18:13,792 iteration 2596 : loss : 0.029719, loss_ce: 0.013818
2022-01-10 00:18:15,313 iteration 2597 : loss : 0.033792, loss_ce: 0.013650
2022-01-10 00:18:16,859 iteration 2598 : loss : 0.028416, loss_ce: 0.011224
2022-01-10 00:18:18,416 iteration 2599 : loss : 0.032521, loss_ce: 0.008821
2022-01-10 00:18:19,975 iteration 2600 : loss : 0.031336, loss_ce: 0.011352
2022-01-10 00:18:21,490 iteration 2601 : loss : 0.030292, loss_ce: 0.012431
 38%|██████████▎                | 153/400 [1:14:56<1:55:48, 28.13s/it]2022-01-10 00:18:23,145 iteration 2602 : loss : 0.039730, loss_ce: 0.015405
2022-01-10 00:18:24,646 iteration 2603 : loss : 0.023894, loss_ce: 0.009524
2022-01-10 00:18:26,198 iteration 2604 : loss : 0.035916, loss_ce: 0.014063
2022-01-10 00:18:27,889 iteration 2605 : loss : 0.041369, loss_ce: 0.014430
2022-01-10 00:18:29,456 iteration 2606 : loss : 0.044344, loss_ce: 0.018724
2022-01-10 00:18:31,035 iteration 2607 : loss : 0.043483, loss_ce: 0.015267
2022-01-10 00:18:32,554 iteration 2608 : loss : 0.032971, loss_ce: 0.014854
2022-01-10 00:18:34,084 iteration 2609 : loss : 0.035142, loss_ce: 0.010414
2022-01-10 00:18:35,767 iteration 2610 : loss : 0.048150, loss_ce: 0.019642
2022-01-10 00:18:37,311 iteration 2611 : loss : 0.033658, loss_ce: 0.011406
2022-01-10 00:18:38,787 iteration 2612 : loss : 0.026256, loss_ce: 0.013142
2022-01-10 00:18:40,326 iteration 2613 : loss : 0.041606, loss_ce: 0.013684
2022-01-10 00:18:41,950 iteration 2614 : loss : 0.063858, loss_ce: 0.018363
2022-01-10 00:18:43,487 iteration 2615 : loss : 0.035216, loss_ce: 0.014923
2022-01-10 00:18:45,027 iteration 2616 : loss : 0.031572, loss_ce: 0.012411
2022-01-10 00:18:46,673 iteration 2617 : loss : 0.058369, loss_ce: 0.018448
2022-01-10 00:18:48,262 iteration 2618 : loss : 0.072328, loss_ce: 0.039304
 38%|██████████▍                | 154/400 [1:15:23<1:53:39, 27.72s/it]2022-01-10 00:18:49,940 iteration 2619 : loss : 0.059614, loss_ce: 0.017731
2022-01-10 00:18:51,486 iteration 2620 : loss : 0.036960, loss_ce: 0.014807
2022-01-10 00:18:53,088 iteration 2621 : loss : 0.035339, loss_ce: 0.012010
2022-01-10 00:18:54,689 iteration 2622 : loss : 0.041412, loss_ce: 0.019100
2022-01-10 00:18:56,302 iteration 2623 : loss : 0.061458, loss_ce: 0.032761
2022-01-10 00:18:57,834 iteration 2624 : loss : 0.028623, loss_ce: 0.009632
2022-01-10 00:18:59,385 iteration 2625 : loss : 0.026494, loss_ce: 0.010928
2022-01-10 00:19:01,020 iteration 2626 : loss : 0.054680, loss_ce: 0.023310
2022-01-10 00:19:02,622 iteration 2627 : loss : 0.047919, loss_ce: 0.013541
2022-01-10 00:19:04,152 iteration 2628 : loss : 0.033416, loss_ce: 0.012270
2022-01-10 00:19:05,786 iteration 2629 : loss : 0.049508, loss_ce: 0.017770
2022-01-10 00:19:07,305 iteration 2630 : loss : 0.037535, loss_ce: 0.011874
2022-01-10 00:19:08,892 iteration 2631 : loss : 0.033994, loss_ce: 0.015070
2022-01-10 00:19:10,491 iteration 2632 : loss : 0.039274, loss_ce: 0.016626
2022-01-10 00:19:12,031 iteration 2633 : loss : 0.032977, loss_ce: 0.015004
2022-01-10 00:19:13,621 iteration 2634 : loss : 0.032139, loss_ce: 0.011146
2022-01-10 00:19:13,621 Training Data Eval:
2022-01-10 00:19:21,699   Average segmentation loss on training set: 0.0250
2022-01-10 00:19:21,699 Validation Data Eval:
2022-01-10 00:19:24,477   Average segmentation loss on validation set: 0.0843
2022-01-10 00:19:26,115 iteration 2635 : loss : 0.049807, loss_ce: 0.021105
 39%|██████████▍                | 155/400 [1:16:01<2:05:36, 30.76s/it]2022-01-10 00:19:27,709 iteration 2636 : loss : 0.026165, loss_ce: 0.008344
2022-01-10 00:19:29,404 iteration 2637 : loss : 0.045935, loss_ce: 0.014588
2022-01-10 00:19:30,988 iteration 2638 : loss : 0.028325, loss_ce: 0.011226
2022-01-10 00:19:32,453 iteration 2639 : loss : 0.025025, loss_ce: 0.009796
2022-01-10 00:19:33,978 iteration 2640 : loss : 0.033807, loss_ce: 0.013136
2022-01-10 00:19:35,632 iteration 2641 : loss : 0.051597, loss_ce: 0.019767
2022-01-10 00:19:37,255 iteration 2642 : loss : 0.047273, loss_ce: 0.014668
2022-01-10 00:19:38,868 iteration 2643 : loss : 0.040587, loss_ce: 0.014668
2022-01-10 00:19:40,409 iteration 2644 : loss : 0.045769, loss_ce: 0.013793
2022-01-10 00:19:41,948 iteration 2645 : loss : 0.039526, loss_ce: 0.021693
2022-01-10 00:19:43,497 iteration 2646 : loss : 0.054452, loss_ce: 0.020767
2022-01-10 00:19:45,115 iteration 2647 : loss : 0.065070, loss_ce: 0.030467
2022-01-10 00:19:46,606 iteration 2648 : loss : 0.023686, loss_ce: 0.010883
2022-01-10 00:19:48,214 iteration 2649 : loss : 0.043614, loss_ce: 0.018885
2022-01-10 00:19:49,819 iteration 2650 : loss : 0.038484, loss_ce: 0.016190
2022-01-10 00:19:51,367 iteration 2651 : loss : 0.046555, loss_ce: 0.016628
2022-01-10 00:19:52,942 iteration 2652 : loss : 0.035548, loss_ce: 0.016434
 39%|██████████▌                | 156/400 [1:16:28<2:00:18, 29.58s/it]2022-01-10 00:19:54,668 iteration 2653 : loss : 0.048008, loss_ce: 0.015145
2022-01-10 00:19:56,241 iteration 2654 : loss : 0.029561, loss_ce: 0.012751
2022-01-10 00:19:57,758 iteration 2655 : loss : 0.025999, loss_ce: 0.010154
2022-01-10 00:19:59,256 iteration 2656 : loss : 0.024674, loss_ce: 0.011013
2022-01-10 00:20:00,772 iteration 2657 : loss : 0.030524, loss_ce: 0.011419
2022-01-10 00:20:02,306 iteration 2658 : loss : 0.030040, loss_ce: 0.011985
2022-01-10 00:20:03,990 iteration 2659 : loss : 0.050087, loss_ce: 0.022427
2022-01-10 00:20:05,557 iteration 2660 : loss : 0.031277, loss_ce: 0.012376
2022-01-10 00:20:07,045 iteration 2661 : loss : 0.035278, loss_ce: 0.013899
2022-01-10 00:20:08,659 iteration 2662 : loss : 0.048405, loss_ce: 0.021644
2022-01-10 00:20:10,182 iteration 2663 : loss : 0.047313, loss_ce: 0.020325
2022-01-10 00:20:11,806 iteration 2664 : loss : 0.028877, loss_ce: 0.010896
2022-01-10 00:20:13,357 iteration 2665 : loss : 0.030574, loss_ce: 0.011325
2022-01-10 00:20:14,934 iteration 2666 : loss : 0.030800, loss_ce: 0.010345
2022-01-10 00:20:16,538 iteration 2667 : loss : 0.038810, loss_ce: 0.014897
2022-01-10 00:20:18,132 iteration 2668 : loss : 0.042974, loss_ce: 0.018570
2022-01-10 00:20:19,748 iteration 2669 : loss : 0.044970, loss_ce: 0.016367
 39%|██████████▌                | 157/400 [1:16:54<1:56:26, 28.75s/it]2022-01-10 00:20:21,405 iteration 2670 : loss : 0.054980, loss_ce: 0.016803
2022-01-10 00:20:22,995 iteration 2671 : loss : 0.053116, loss_ce: 0.015020
2022-01-10 00:20:24,600 iteration 2672 : loss : 0.026918, loss_ce: 0.010740
2022-01-10 00:20:26,227 iteration 2673 : loss : 0.073291, loss_ce: 0.027920
2022-01-10 00:20:27,766 iteration 2674 : loss : 0.035948, loss_ce: 0.018442
2022-01-10 00:20:29,392 iteration 2675 : loss : 0.040477, loss_ce: 0.014482
2022-01-10 00:20:30,984 iteration 2676 : loss : 0.034380, loss_ce: 0.009137
2022-01-10 00:20:32,500 iteration 2677 : loss : 0.026932, loss_ce: 0.012284
2022-01-10 00:20:34,004 iteration 2678 : loss : 0.031537, loss_ce: 0.010142
2022-01-10 00:20:35,597 iteration 2679 : loss : 0.068286, loss_ce: 0.030062
2022-01-10 00:20:37,231 iteration 2680 : loss : 0.042467, loss_ce: 0.018323
2022-01-10 00:20:38,796 iteration 2681 : loss : 0.028109, loss_ce: 0.013460
2022-01-10 00:20:40,311 iteration 2682 : loss : 0.032561, loss_ce: 0.013637
2022-01-10 00:20:41,892 iteration 2683 : loss : 0.032514, loss_ce: 0.016119
2022-01-10 00:20:43,460 iteration 2684 : loss : 0.073546, loss_ce: 0.022266
2022-01-10 00:20:45,032 iteration 2685 : loss : 0.041983, loss_ce: 0.020207
2022-01-10 00:20:46,592 iteration 2686 : loss : 0.028377, loss_ce: 0.011080
 40%|██████████▋                | 158/400 [1:17:21<1:53:38, 28.18s/it]2022-01-10 00:20:48,189 iteration 2687 : loss : 0.027813, loss_ce: 0.013918
2022-01-10 00:20:49,740 iteration 2688 : loss : 0.034715, loss_ce: 0.014299
2022-01-10 00:20:51,203 iteration 2689 : loss : 0.031239, loss_ce: 0.012560
2022-01-10 00:20:52,753 iteration 2690 : loss : 0.037523, loss_ce: 0.015938
2022-01-10 00:20:54,332 iteration 2691 : loss : 0.027291, loss_ce: 0.009201
2022-01-10 00:20:55,889 iteration 2692 : loss : 0.040364, loss_ce: 0.014837
2022-01-10 00:20:57,418 iteration 2693 : loss : 0.030607, loss_ce: 0.012146
2022-01-10 00:20:58,896 iteration 2694 : loss : 0.034073, loss_ce: 0.012068
2022-01-10 00:21:00,511 iteration 2695 : loss : 0.046191, loss_ce: 0.016114
2022-01-10 00:21:02,181 iteration 2696 : loss : 0.054944, loss_ce: 0.020962
2022-01-10 00:21:03,806 iteration 2697 : loss : 0.042844, loss_ce: 0.021999
2022-01-10 00:21:05,321 iteration 2698 : loss : 0.028647, loss_ce: 0.012127
2022-01-10 00:21:06,853 iteration 2699 : loss : 0.035227, loss_ce: 0.011954
2022-01-10 00:21:08,384 iteration 2700 : loss : 0.042037, loss_ce: 0.012467
2022-01-10 00:21:09,956 iteration 2701 : loss : 0.050924, loss_ce: 0.022472
2022-01-10 00:21:11,559 iteration 2702 : loss : 0.063617, loss_ce: 0.014419
2022-01-10 00:21:13,167 iteration 2703 : loss : 0.045282, loss_ce: 0.017573
 40%|██████████▋                | 159/400 [1:17:48<1:51:14, 27.70s/it]2022-01-10 00:21:14,745 iteration 2704 : loss : 0.024448, loss_ce: 0.008095
2022-01-10 00:21:16,264 iteration 2705 : loss : 0.031994, loss_ce: 0.012168
2022-01-10 00:21:17,866 iteration 2706 : loss : 0.045370, loss_ce: 0.024718
2022-01-10 00:21:19,430 iteration 2707 : loss : 0.026515, loss_ce: 0.009427
2022-01-10 00:21:20,980 iteration 2708 : loss : 0.054261, loss_ce: 0.018438
2022-01-10 00:21:22,615 iteration 2709 : loss : 0.036828, loss_ce: 0.012113
2022-01-10 00:21:24,231 iteration 2710 : loss : 0.041487, loss_ce: 0.018135
2022-01-10 00:21:25,778 iteration 2711 : loss : 0.026072, loss_ce: 0.009270
2022-01-10 00:21:27,337 iteration 2712 : loss : 0.050953, loss_ce: 0.019719
2022-01-10 00:21:28,917 iteration 2713 : loss : 0.030339, loss_ce: 0.012597
2022-01-10 00:21:30,491 iteration 2714 : loss : 0.046488, loss_ce: 0.018211
2022-01-10 00:21:32,050 iteration 2715 : loss : 0.036228, loss_ce: 0.019435
2022-01-10 00:21:33,603 iteration 2716 : loss : 0.049877, loss_ce: 0.014280
2022-01-10 00:21:35,114 iteration 2717 : loss : 0.038142, loss_ce: 0.015164
2022-01-10 00:21:36,708 iteration 2718 : loss : 0.034683, loss_ce: 0.013107
2022-01-10 00:21:38,246 iteration 2719 : loss : 0.029065, loss_ce: 0.011632
2022-01-10 00:21:38,246 Training Data Eval:
2022-01-10 00:21:46,320   Average segmentation loss on training set: 0.0273
2022-01-10 00:21:46,320 Validation Data Eval:
2022-01-10 00:21:49,097   Average segmentation loss on validation set: 0.0715
2022-01-10 00:21:55,263 Found new lowest validation loss at iteration 2719! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-10 00:21:56,723 iteration 2720 : loss : 0.032262, loss_ce: 0.014260
 40%|██████████▊                | 160/400 [1:18:31<2:09:48, 32.45s/it]2022-01-10 00:21:58,223 iteration 2721 : loss : 0.034349, loss_ce: 0.011768
2022-01-10 00:21:59,807 iteration 2722 : loss : 0.032068, loss_ce: 0.016261
2022-01-10 00:22:01,453 iteration 2723 : loss : 0.041040, loss_ce: 0.018490
2022-01-10 00:22:02,968 iteration 2724 : loss : 0.038503, loss_ce: 0.014984
2022-01-10 00:22:04,512 iteration 2725 : loss : 0.052654, loss_ce: 0.016212
2022-01-10 00:22:06,147 iteration 2726 : loss : 0.037465, loss_ce: 0.013266
2022-01-10 00:22:07,711 iteration 2727 : loss : 0.031813, loss_ce: 0.010957
2022-01-10 00:22:09,319 iteration 2728 : loss : 0.037073, loss_ce: 0.013309
2022-01-10 00:22:10,918 iteration 2729 : loss : 0.056350, loss_ce: 0.024509
2022-01-10 00:22:12,556 iteration 2730 : loss : 0.056446, loss_ce: 0.023137
2022-01-10 00:22:14,116 iteration 2731 : loss : 0.036434, loss_ce: 0.012779
2022-01-10 00:22:15,597 iteration 2732 : loss : 0.026066, loss_ce: 0.010896
2022-01-10 00:22:17,157 iteration 2733 : loss : 0.038470, loss_ce: 0.020245
2022-01-10 00:22:18,734 iteration 2734 : loss : 0.050476, loss_ce: 0.014428
2022-01-10 00:22:20,313 iteration 2735 : loss : 0.031971, loss_ce: 0.014875
2022-01-10 00:22:21,864 iteration 2736 : loss : 0.037315, loss_ce: 0.012968
2022-01-10 00:22:23,413 iteration 2737 : loss : 0.052055, loss_ce: 0.015291
 40%|██████████▊                | 161/400 [1:18:58<2:02:23, 30.72s/it]2022-01-10 00:22:25,081 iteration 2738 : loss : 0.044848, loss_ce: 0.020975
2022-01-10 00:22:26,598 iteration 2739 : loss : 0.037070, loss_ce: 0.009984
2022-01-10 00:22:28,250 iteration 2740 : loss : 0.058168, loss_ce: 0.025251
2022-01-10 00:22:29,814 iteration 2741 : loss : 0.034159, loss_ce: 0.016522
2022-01-10 00:22:31,391 iteration 2742 : loss : 0.036282, loss_ce: 0.015796
2022-01-10 00:22:32,903 iteration 2743 : loss : 0.045271, loss_ce: 0.018817
2022-01-10 00:22:34,463 iteration 2744 : loss : 0.026620, loss_ce: 0.009168
2022-01-10 00:22:35,991 iteration 2745 : loss : 0.043083, loss_ce: 0.013760
2022-01-10 00:22:37,655 iteration 2746 : loss : 0.106636, loss_ce: 0.027165
2022-01-10 00:22:39,194 iteration 2747 : loss : 0.032927, loss_ce: 0.011884
2022-01-10 00:22:40,804 iteration 2748 : loss : 0.035530, loss_ce: 0.010330
2022-01-10 00:22:42,445 iteration 2749 : loss : 0.045422, loss_ce: 0.017074
2022-01-10 00:22:44,002 iteration 2750 : loss : 0.038641, loss_ce: 0.016409
2022-01-10 00:22:45,568 iteration 2751 : loss : 0.051842, loss_ce: 0.015343
2022-01-10 00:22:47,236 iteration 2752 : loss : 0.058248, loss_ce: 0.024235
2022-01-10 00:22:48,910 iteration 2753 : loss : 0.039846, loss_ce: 0.017365
2022-01-10 00:22:50,498 iteration 2754 : loss : 0.033185, loss_ce: 0.016342
 40%|██████████▉                | 162/400 [1:19:25<1:57:32, 29.63s/it]2022-01-10 00:22:51,999 iteration 2755 : loss : 0.027035, loss_ce: 0.009037
2022-01-10 00:22:53,581 iteration 2756 : loss : 0.033907, loss_ce: 0.014251
2022-01-10 00:22:55,152 iteration 2757 : loss : 0.025985, loss_ce: 0.011023
2022-01-10 00:22:56,718 iteration 2758 : loss : 0.032784, loss_ce: 0.011859
2022-01-10 00:22:58,288 iteration 2759 : loss : 0.039578, loss_ce: 0.014729
2022-01-10 00:22:59,860 iteration 2760 : loss : 0.047600, loss_ce: 0.022227
2022-01-10 00:23:01,467 iteration 2761 : loss : 0.033834, loss_ce: 0.012735
2022-01-10 00:23:03,033 iteration 2762 : loss : 0.045816, loss_ce: 0.020524
2022-01-10 00:23:04,631 iteration 2763 : loss : 0.047105, loss_ce: 0.014690
2022-01-10 00:23:06,140 iteration 2764 : loss : 0.021575, loss_ce: 0.008278
2022-01-10 00:23:07,807 iteration 2765 : loss : 0.031329, loss_ce: 0.011041
2022-01-10 00:23:09,309 iteration 2766 : loss : 0.035199, loss_ce: 0.011589
2022-01-10 00:23:10,889 iteration 2767 : loss : 0.044920, loss_ce: 0.021307
2022-01-10 00:23:12,435 iteration 2768 : loss : 0.036647, loss_ce: 0.011984
2022-01-10 00:23:13,938 iteration 2769 : loss : 0.025221, loss_ce: 0.010992
2022-01-10 00:23:15,472 iteration 2770 : loss : 0.032165, loss_ce: 0.013182
2022-01-10 00:23:17,036 iteration 2771 : loss : 0.028690, loss_ce: 0.012746
 41%|███████████                | 163/400 [1:19:52<1:53:23, 28.71s/it]2022-01-10 00:23:18,597 iteration 2772 : loss : 0.028780, loss_ce: 0.010389
2022-01-10 00:23:20,174 iteration 2773 : loss : 0.029711, loss_ce: 0.010767
2022-01-10 00:23:21,750 iteration 2774 : loss : 0.045621, loss_ce: 0.011822
2022-01-10 00:23:23,333 iteration 2775 : loss : 0.043409, loss_ce: 0.018888
2022-01-10 00:23:24,886 iteration 2776 : loss : 0.038824, loss_ce: 0.015030
2022-01-10 00:23:26,511 iteration 2777 : loss : 0.025054, loss_ce: 0.009425
2022-01-10 00:23:28,042 iteration 2778 : loss : 0.027892, loss_ce: 0.009116
2022-01-10 00:23:29,575 iteration 2779 : loss : 0.027208, loss_ce: 0.010511
2022-01-10 00:23:31,131 iteration 2780 : loss : 0.029055, loss_ce: 0.015177
2022-01-10 00:23:32,814 iteration 2781 : loss : 0.039752, loss_ce: 0.015884
2022-01-10 00:23:34,454 iteration 2782 : loss : 0.029163, loss_ce: 0.012771
2022-01-10 00:23:36,015 iteration 2783 : loss : 0.030058, loss_ce: 0.009160
2022-01-10 00:23:37,480 iteration 2784 : loss : 0.031448, loss_ce: 0.008782
2022-01-10 00:23:38,998 iteration 2785 : loss : 0.033656, loss_ce: 0.016155
2022-01-10 00:23:40,505 iteration 2786 : loss : 0.034347, loss_ce: 0.013072
2022-01-10 00:23:42,032 iteration 2787 : loss : 0.027279, loss_ce: 0.010877
2022-01-10 00:23:43,728 iteration 2788 : loss : 0.040449, loss_ce: 0.017287
 41%|███████████                | 164/400 [1:20:18<1:50:31, 28.10s/it]2022-01-10 00:23:45,379 iteration 2789 : loss : 0.023660, loss_ce: 0.007846
2022-01-10 00:23:47,031 iteration 2790 : loss : 0.048185, loss_ce: 0.018496
2022-01-10 00:23:48,543 iteration 2791 : loss : 0.029263, loss_ce: 0.008222
2022-01-10 00:23:50,093 iteration 2792 : loss : 0.034806, loss_ce: 0.014119
2022-01-10 00:23:51,657 iteration 2793 : loss : 0.044984, loss_ce: 0.014347
2022-01-10 00:23:53,280 iteration 2794 : loss : 0.030584, loss_ce: 0.014397
2022-01-10 00:23:54,842 iteration 2795 : loss : 0.034053, loss_ce: 0.012984
2022-01-10 00:23:56,380 iteration 2796 : loss : 0.033671, loss_ce: 0.011202
2022-01-10 00:23:57,991 iteration 2797 : loss : 0.036815, loss_ce: 0.015552
2022-01-10 00:23:59,581 iteration 2798 : loss : 0.034302, loss_ce: 0.010295
2022-01-10 00:24:01,239 iteration 2799 : loss : 0.037816, loss_ce: 0.014667
2022-01-10 00:24:02,828 iteration 2800 : loss : 0.031176, loss_ce: 0.014731
2022-01-10 00:24:04,314 iteration 2801 : loss : 0.043208, loss_ce: 0.011918
2022-01-10 00:24:05,838 iteration 2802 : loss : 0.031738, loss_ce: 0.011110
2022-01-10 00:24:07,422 iteration 2803 : loss : 0.038057, loss_ce: 0.015560
2022-01-10 00:24:08,906 iteration 2804 : loss : 0.030615, loss_ce: 0.012197
2022-01-10 00:24:08,906 Training Data Eval:
2022-01-10 00:24:16,981   Average segmentation loss on training set: 0.0225
2022-01-10 00:24:16,982 Validation Data Eval:
2022-01-10 00:24:19,751   Average segmentation loss on validation set: 0.0827
2022-01-10 00:24:21,411 iteration 2805 : loss : 0.045202, loss_ce: 0.018835
 41%|███████████▏               | 165/400 [1:20:56<2:01:19, 30.98s/it]2022-01-10 00:24:23,110 iteration 2806 : loss : 0.032942, loss_ce: 0.013238
2022-01-10 00:24:24,760 iteration 2807 : loss : 0.047075, loss_ce: 0.014322
2022-01-10 00:24:26,379 iteration 2808 : loss : 0.053673, loss_ce: 0.018568
2022-01-10 00:24:27,972 iteration 2809 : loss : 0.032421, loss_ce: 0.012866
2022-01-10 00:24:29,475 iteration 2810 : loss : 0.036479, loss_ce: 0.013847
2022-01-10 00:24:31,086 iteration 2811 : loss : 0.045698, loss_ce: 0.022136
2022-01-10 00:24:32,590 iteration 2812 : loss : 0.026095, loss_ce: 0.010755
2022-01-10 00:24:34,223 iteration 2813 : loss : 0.041563, loss_ce: 0.017414
2022-01-10 00:24:35,808 iteration 2814 : loss : 0.041132, loss_ce: 0.014570
2022-01-10 00:24:37,377 iteration 2815 : loss : 0.039644, loss_ce: 0.017503
2022-01-10 00:24:38,946 iteration 2816 : loss : 0.034909, loss_ce: 0.013461
2022-01-10 00:24:40,595 iteration 2817 : loss : 0.060331, loss_ce: 0.013441
2022-01-10 00:24:42,224 iteration 2818 : loss : 0.044523, loss_ce: 0.020931
2022-01-10 00:24:43,780 iteration 2819 : loss : 0.032839, loss_ce: 0.014454
2022-01-10 00:24:45,450 iteration 2820 : loss : 0.055884, loss_ce: 0.017247
2022-01-10 00:24:47,101 iteration 2821 : loss : 0.046355, loss_ce: 0.017196
2022-01-10 00:24:48,713 iteration 2822 : loss : 0.029540, loss_ce: 0.010463
 42%|███████████▏               | 166/400 [1:21:23<1:56:30, 29.87s/it]2022-01-10 00:24:50,452 iteration 2823 : loss : 0.044150, loss_ce: 0.019813
2022-01-10 00:24:52,000 iteration 2824 : loss : 0.029705, loss_ce: 0.014666
2022-01-10 00:24:53,585 iteration 2825 : loss : 0.050922, loss_ce: 0.025142
2022-01-10 00:24:55,261 iteration 2826 : loss : 0.042379, loss_ce: 0.017825
2022-01-10 00:24:56,959 iteration 2827 : loss : 0.068198, loss_ce: 0.022741
2022-01-10 00:24:58,564 iteration 2828 : loss : 0.052612, loss_ce: 0.019299
2022-01-10 00:25:00,136 iteration 2829 : loss : 0.035154, loss_ce: 0.016201
2022-01-10 00:25:01,696 iteration 2830 : loss : 0.045325, loss_ce: 0.018075
2022-01-10 00:25:03,178 iteration 2831 : loss : 0.026602, loss_ce: 0.011082
2022-01-10 00:25:04,871 iteration 2832 : loss : 0.058791, loss_ce: 0.028620
2022-01-10 00:25:06,430 iteration 2833 : loss : 0.031082, loss_ce: 0.011476
2022-01-10 00:25:08,063 iteration 2834 : loss : 0.049305, loss_ce: 0.016739
2022-01-10 00:25:09,671 iteration 2835 : loss : 0.043073, loss_ce: 0.013115
2022-01-10 00:25:11,244 iteration 2836 : loss : 0.042478, loss_ce: 0.015017
2022-01-10 00:25:12,876 iteration 2837 : loss : 0.050230, loss_ce: 0.020920
2022-01-10 00:25:14,390 iteration 2838 : loss : 0.027931, loss_ce: 0.013035
2022-01-10 00:25:15,998 iteration 2839 : loss : 0.036343, loss_ce: 0.014465
 42%|███████████▎               | 167/400 [1:21:51<1:52:59, 29.10s/it]2022-01-10 00:25:17,690 iteration 2840 : loss : 0.043157, loss_ce: 0.016460
2022-01-10 00:25:19,256 iteration 2841 : loss : 0.046464, loss_ce: 0.014410
2022-01-10 00:25:20,820 iteration 2842 : loss : 0.032165, loss_ce: 0.012809
2022-01-10 00:25:22,440 iteration 2843 : loss : 0.040856, loss_ce: 0.012024
2022-01-10 00:25:24,030 iteration 2844 : loss : 0.044685, loss_ce: 0.019691
2022-01-10 00:25:25,601 iteration 2845 : loss : 0.032845, loss_ce: 0.011998
2022-01-10 00:25:27,184 iteration 2846 : loss : 0.025968, loss_ce: 0.009866
2022-01-10 00:25:28,639 iteration 2847 : loss : 0.023952, loss_ce: 0.013520
2022-01-10 00:25:30,221 iteration 2848 : loss : 0.044949, loss_ce: 0.016414
2022-01-10 00:25:31,847 iteration 2849 : loss : 0.037196, loss_ce: 0.015648
2022-01-10 00:25:33,459 iteration 2850 : loss : 0.046729, loss_ce: 0.019149
2022-01-10 00:25:35,086 iteration 2851 : loss : 0.042907, loss_ce: 0.018180
2022-01-10 00:25:36,683 iteration 2852 : loss : 0.049185, loss_ce: 0.015111
2022-01-10 00:25:38,321 iteration 2853 : loss : 0.022537, loss_ce: 0.008883
2022-01-10 00:25:39,929 iteration 2854 : loss : 0.030432, loss_ce: 0.010767
2022-01-10 00:25:41,470 iteration 2855 : loss : 0.034056, loss_ce: 0.016199
2022-01-10 00:25:43,083 iteration 2856 : loss : 0.047478, loss_ce: 0.024102
 42%|███████████▎               | 168/400 [1:22:18<1:50:10, 28.49s/it]2022-01-10 00:25:44,627 iteration 2857 : loss : 0.025938, loss_ce: 0.008927
2022-01-10 00:25:46,209 iteration 2858 : loss : 0.033521, loss_ce: 0.015816
2022-01-10 00:25:47,753 iteration 2859 : loss : 0.025037, loss_ce: 0.010163
2022-01-10 00:25:49,365 iteration 2860 : loss : 0.041582, loss_ce: 0.013456
2022-01-10 00:25:50,859 iteration 2861 : loss : 0.027986, loss_ce: 0.012898
2022-01-10 00:25:52,519 iteration 2862 : loss : 0.062628, loss_ce: 0.019191
2022-01-10 00:25:54,048 iteration 2863 : loss : 0.032560, loss_ce: 0.009597
2022-01-10 00:25:55,651 iteration 2864 : loss : 0.033272, loss_ce: 0.013673
2022-01-10 00:25:57,264 iteration 2865 : loss : 0.046251, loss_ce: 0.018556
2022-01-10 00:25:58,821 iteration 2866 : loss : 0.039110, loss_ce: 0.015545
2022-01-10 00:26:00,497 iteration 2867 : loss : 0.045276, loss_ce: 0.018153
2022-01-10 00:26:02,036 iteration 2868 : loss : 0.031615, loss_ce: 0.011803
2022-01-10 00:26:03,693 iteration 2869 : loss : 0.039277, loss_ce: 0.017351
2022-01-10 00:26:05,302 iteration 2870 : loss : 0.024962, loss_ce: 0.008931
2022-01-10 00:26:06,846 iteration 2871 : loss : 0.027785, loss_ce: 0.010454
2022-01-10 00:26:08,349 iteration 2872 : loss : 0.033146, loss_ce: 0.014611
2022-01-10 00:26:09,969 iteration 2873 : loss : 0.027932, loss_ce: 0.009407
 42%|███████████▍               | 169/400 [1:22:45<1:47:51, 28.01s/it]2022-01-10 00:26:11,639 iteration 2874 : loss : 0.031467, loss_ce: 0.011399
2022-01-10 00:26:13,231 iteration 2875 : loss : 0.027942, loss_ce: 0.011916
2022-01-10 00:26:14,813 iteration 2876 : loss : 0.040356, loss_ce: 0.015400
2022-01-10 00:26:16,443 iteration 2877 : loss : 0.052675, loss_ce: 0.016570
2022-01-10 00:26:18,024 iteration 2878 : loss : 0.025477, loss_ce: 0.012724
2022-01-10 00:26:19,650 iteration 2879 : loss : 0.031163, loss_ce: 0.010981
2022-01-10 00:26:21,286 iteration 2880 : loss : 0.032058, loss_ce: 0.011446
2022-01-10 00:26:22,875 iteration 2881 : loss : 0.028802, loss_ce: 0.011994
2022-01-10 00:26:24,515 iteration 2882 : loss : 0.027926, loss_ce: 0.013229
2022-01-10 00:26:26,068 iteration 2883 : loss : 0.042480, loss_ce: 0.019509
2022-01-10 00:26:27,629 iteration 2884 : loss : 0.040361, loss_ce: 0.016514
2022-01-10 00:26:29,230 iteration 2885 : loss : 0.044490, loss_ce: 0.019872
2022-01-10 00:26:30,727 iteration 2886 : loss : 0.045786, loss_ce: 0.015790
2022-01-10 00:26:32,374 iteration 2887 : loss : 0.044188, loss_ce: 0.022480
2022-01-10 00:26:33,968 iteration 2888 : loss : 0.069060, loss_ce: 0.016595
2022-01-10 00:26:35,653 iteration 2889 : loss : 0.044989, loss_ce: 0.017541
2022-01-10 00:26:35,654 Training Data Eval:
2022-01-10 00:26:43,722   Average segmentation loss on training set: 0.0242
2022-01-10 00:26:43,722 Validation Data Eval:
2022-01-10 00:26:46,488   Average segmentation loss on validation set: 0.0808
2022-01-10 00:26:48,112 iteration 2890 : loss : 0.027135, loss_ce: 0.009456
 42%|███████████▍               | 170/400 [1:23:23<1:59:01, 31.05s/it]2022-01-10 00:26:49,776 iteration 2891 : loss : 0.040631, loss_ce: 0.017219
2022-01-10 00:26:51,433 iteration 2892 : loss : 0.032056, loss_ce: 0.008275
2022-01-10 00:26:52,972 iteration 2893 : loss : 0.025797, loss_ce: 0.008353
2022-01-10 00:26:54,625 iteration 2894 : loss : 0.037707, loss_ce: 0.010748
2022-01-10 00:26:56,263 iteration 2895 : loss : 0.046597, loss_ce: 0.022080
2022-01-10 00:26:57,812 iteration 2896 : loss : 0.030426, loss_ce: 0.008952
2022-01-10 00:26:59,393 iteration 2897 : loss : 0.042978, loss_ce: 0.018178
2022-01-10 00:27:00,915 iteration 2898 : loss : 0.029576, loss_ce: 0.013547
2022-01-10 00:27:02,521 iteration 2899 : loss : 0.040464, loss_ce: 0.013019
2022-01-10 00:27:04,093 iteration 2900 : loss : 0.032581, loss_ce: 0.011806
2022-01-10 00:27:05,684 iteration 2901 : loss : 0.037985, loss_ce: 0.011659
2022-01-10 00:27:07,194 iteration 2902 : loss : 0.057304, loss_ce: 0.021965
2022-01-10 00:27:08,730 iteration 2903 : loss : 0.029603, loss_ce: 0.015347
2022-01-10 00:27:10,260 iteration 2904 : loss : 0.038524, loss_ce: 0.016339
2022-01-10 00:27:11,768 iteration 2905 : loss : 0.036313, loss_ce: 0.019001
2022-01-10 00:27:13,378 iteration 2906 : loss : 0.035538, loss_ce: 0.018121
2022-01-10 00:27:14,921 iteration 2907 : loss : 0.037582, loss_ce: 0.014187
 43%|███████████▌               | 171/400 [1:23:50<1:53:39, 29.78s/it]2022-01-10 00:27:16,590 iteration 2908 : loss : 0.067458, loss_ce: 0.034416
2022-01-10 00:27:18,220 iteration 2909 : loss : 0.032697, loss_ce: 0.015088
2022-01-10 00:27:19,806 iteration 2910 : loss : 0.041946, loss_ce: 0.014826
2022-01-10 00:27:21,427 iteration 2911 : loss : 0.037014, loss_ce: 0.012766
2022-01-10 00:27:22,975 iteration 2912 : loss : 0.039720, loss_ce: 0.016542
2022-01-10 00:27:24,615 iteration 2913 : loss : 0.033259, loss_ce: 0.014786
2022-01-10 00:27:26,168 iteration 2914 : loss : 0.047727, loss_ce: 0.020044
2022-01-10 00:27:27,728 iteration 2915 : loss : 0.026887, loss_ce: 0.011185
2022-01-10 00:27:29,318 iteration 2916 : loss : 0.030469, loss_ce: 0.011965
2022-01-10 00:27:30,825 iteration 2917 : loss : 0.027291, loss_ce: 0.012225
2022-01-10 00:27:32,362 iteration 2918 : loss : 0.040743, loss_ce: 0.012238
2022-01-10 00:27:33,938 iteration 2919 : loss : 0.032809, loss_ce: 0.010207
2022-01-10 00:27:35,537 iteration 2920 : loss : 0.030699, loss_ce: 0.011964
2022-01-10 00:27:37,103 iteration 2921 : loss : 0.031996, loss_ce: 0.013572
2022-01-10 00:27:38,609 iteration 2922 : loss : 0.033607, loss_ce: 0.012331
2022-01-10 00:27:40,152 iteration 2923 : loss : 0.038347, loss_ce: 0.013134
2022-01-10 00:27:41,727 iteration 2924 : loss : 0.082671, loss_ce: 0.045536
 43%|███████████▌               | 172/400 [1:24:16<1:49:45, 28.89s/it]2022-01-10 00:27:43,260 iteration 2925 : loss : 0.026752, loss_ce: 0.013388
2022-01-10 00:27:44,910 iteration 2926 : loss : 0.048689, loss_ce: 0.022907
2022-01-10 00:27:46,479 iteration 2927 : loss : 0.029794, loss_ce: 0.012722
2022-01-10 00:27:47,938 iteration 2928 : loss : 0.031123, loss_ce: 0.010802
2022-01-10 00:27:49,481 iteration 2929 : loss : 0.027066, loss_ce: 0.011737
2022-01-10 00:27:51,129 iteration 2930 : loss : 0.048756, loss_ce: 0.022588
2022-01-10 00:27:52,678 iteration 2931 : loss : 0.043240, loss_ce: 0.019086
2022-01-10 00:27:54,206 iteration 2932 : loss : 0.044877, loss_ce: 0.018708
2022-01-10 00:27:55,851 iteration 2933 : loss : 0.051888, loss_ce: 0.017116
2022-01-10 00:27:57,420 iteration 2934 : loss : 0.040547, loss_ce: 0.016485
2022-01-10 00:27:58,999 iteration 2935 : loss : 0.040694, loss_ce: 0.015683
2022-01-10 00:28:00,564 iteration 2936 : loss : 0.026351, loss_ce: 0.008168
2022-01-10 00:28:02,163 iteration 2937 : loss : 0.031987, loss_ce: 0.015844
2022-01-10 00:28:03,780 iteration 2938 : loss : 0.043567, loss_ce: 0.012499
2022-01-10 00:28:05,401 iteration 2939 : loss : 0.034500, loss_ce: 0.012427
2022-01-10 00:28:07,078 iteration 2940 : loss : 0.073492, loss_ce: 0.038117
2022-01-10 00:28:08,605 iteration 2941 : loss : 0.028613, loss_ce: 0.008854
 43%|███████████▋               | 173/400 [1:24:43<1:47:00, 28.29s/it]2022-01-10 00:28:10,187 iteration 2942 : loss : 0.025488, loss_ce: 0.007177
2022-01-10 00:28:11,789 iteration 2943 : loss : 0.037591, loss_ce: 0.011648
2022-01-10 00:28:13,327 iteration 2944 : loss : 0.027791, loss_ce: 0.014167
2022-01-10 00:28:14,909 iteration 2945 : loss : 0.029318, loss_ce: 0.008212
2022-01-10 00:28:16,385 iteration 2946 : loss : 0.028501, loss_ce: 0.012627
2022-01-10 00:28:17,929 iteration 2947 : loss : 0.028474, loss_ce: 0.011626
2022-01-10 00:28:19,461 iteration 2948 : loss : 0.032530, loss_ce: 0.009406
2022-01-10 00:28:21,094 iteration 2949 : loss : 0.050826, loss_ce: 0.018084
2022-01-10 00:28:22,687 iteration 2950 : loss : 0.030871, loss_ce: 0.012485
2022-01-10 00:28:24,300 iteration 2951 : loss : 0.037096, loss_ce: 0.016476
2022-01-10 00:28:25,908 iteration 2952 : loss : 0.025681, loss_ce: 0.008839
2022-01-10 00:28:27,546 iteration 2953 : loss : 0.042787, loss_ce: 0.019786
2022-01-10 00:28:29,221 iteration 2954 : loss : 0.050510, loss_ce: 0.014128
2022-01-10 00:28:30,766 iteration 2955 : loss : 0.036530, loss_ce: 0.019029
2022-01-10 00:28:32,318 iteration 2956 : loss : 0.025214, loss_ce: 0.010686
2022-01-10 00:28:33,840 iteration 2957 : loss : 0.037655, loss_ce: 0.014300
2022-01-10 00:28:35,420 iteration 2958 : loss : 0.047419, loss_ce: 0.024055
 44%|███████████▋               | 174/400 [1:25:10<1:44:52, 27.84s/it]2022-01-10 00:28:37,020 iteration 2959 : loss : 0.022715, loss_ce: 0.008107
2022-01-10 00:28:38,575 iteration 2960 : loss : 0.052051, loss_ce: 0.023157
2022-01-10 00:28:40,161 iteration 2961 : loss : 0.034766, loss_ce: 0.013408
2022-01-10 00:28:41,883 iteration 2962 : loss : 0.025441, loss_ce: 0.008837
2022-01-10 00:28:43,486 iteration 2963 : loss : 0.044616, loss_ce: 0.016048
2022-01-10 00:28:44,975 iteration 2964 : loss : 0.035618, loss_ce: 0.014705
2022-01-10 00:28:46,558 iteration 2965 : loss : 0.028297, loss_ce: 0.013614
2022-01-10 00:28:48,174 iteration 2966 : loss : 0.056427, loss_ce: 0.020556
2022-01-10 00:28:49,742 iteration 2967 : loss : 0.049887, loss_ce: 0.021492
2022-01-10 00:28:51,325 iteration 2968 : loss : 0.030330, loss_ce: 0.012404
2022-01-10 00:28:52,880 iteration 2969 : loss : 0.040747, loss_ce: 0.019279
2022-01-10 00:28:54,465 iteration 2970 : loss : 0.034793, loss_ce: 0.013459
2022-01-10 00:28:56,023 iteration 2971 : loss : 0.029538, loss_ce: 0.011739
2022-01-10 00:28:57,589 iteration 2972 : loss : 0.039569, loss_ce: 0.018138
2022-01-10 00:28:59,109 iteration 2973 : loss : 0.033869, loss_ce: 0.008355
2022-01-10 00:29:00,738 iteration 2974 : loss : 0.044175, loss_ce: 0.012959
2022-01-10 00:29:00,738 Training Data Eval:
2022-01-10 00:29:08,809   Average segmentation loss on training set: 0.0213
2022-01-10 00:29:08,809 Validation Data Eval:
2022-01-10 00:29:11,587   Average segmentation loss on validation set: 0.0742
2022-01-10 00:29:13,131 iteration 2975 : loss : 0.023910, loss_ce: 0.009020
 44%|███████████▊               | 175/400 [1:25:48<1:55:30, 30.80s/it]2022-01-10 00:29:14,674 iteration 2976 : loss : 0.034792, loss_ce: 0.015377
2022-01-10 00:29:16,277 iteration 2977 : loss : 0.032664, loss_ce: 0.014975
2022-01-10 00:29:17,862 iteration 2978 : loss : 0.035589, loss_ce: 0.011132
2022-01-10 00:29:19,486 iteration 2979 : loss : 0.041810, loss_ce: 0.013945
2022-01-10 00:29:21,097 iteration 2980 : loss : 0.030056, loss_ce: 0.009441
2022-01-10 00:29:22,774 iteration 2981 : loss : 0.051808, loss_ce: 0.022352
2022-01-10 00:29:24,355 iteration 2982 : loss : 0.022519, loss_ce: 0.007788
2022-01-10 00:29:25,944 iteration 2983 : loss : 0.041653, loss_ce: 0.017283
2022-01-10 00:29:27,531 iteration 2984 : loss : 0.035960, loss_ce: 0.011643
2022-01-10 00:29:29,141 iteration 2985 : loss : 0.036208, loss_ce: 0.013310
2022-01-10 00:29:30,659 iteration 2986 : loss : 0.030952, loss_ce: 0.013995
2022-01-10 00:29:32,241 iteration 2987 : loss : 0.040653, loss_ce: 0.012904
2022-01-10 00:29:33,865 iteration 2988 : loss : 0.036328, loss_ce: 0.015689
2022-01-10 00:29:35,480 iteration 2989 : loss : 0.037120, loss_ce: 0.014315
2022-01-10 00:29:36,999 iteration 2990 : loss : 0.028707, loss_ce: 0.012190
2022-01-10 00:29:38,575 iteration 2991 : loss : 0.026092, loss_ce: 0.010314
2022-01-10 00:29:40,225 iteration 2992 : loss : 0.032651, loss_ce: 0.012558
 44%|███████████▉               | 176/400 [1:26:15<1:50:50, 29.69s/it]2022-01-10 00:29:41,802 iteration 2993 : loss : 0.041471, loss_ce: 0.019629
2022-01-10 00:29:43,390 iteration 2994 : loss : 0.050549, loss_ce: 0.011737
2022-01-10 00:29:44,942 iteration 2995 : loss : 0.039820, loss_ce: 0.012919
2022-01-10 00:29:46,510 iteration 2996 : loss : 0.028510, loss_ce: 0.013373
2022-01-10 00:29:48,076 iteration 2997 : loss : 0.033291, loss_ce: 0.010790
2022-01-10 00:29:49,658 iteration 2998 : loss : 0.030919, loss_ce: 0.012623
2022-01-10 00:29:51,287 iteration 2999 : loss : 0.077669, loss_ce: 0.014252
2022-01-10 00:29:52,941 iteration 3000 : loss : 0.045955, loss_ce: 0.021008
2022-01-10 00:29:54,590 iteration 3001 : loss : 0.079554, loss_ce: 0.016927
2022-01-10 00:29:56,116 iteration 3002 : loss : 0.036469, loss_ce: 0.015172
2022-01-10 00:29:57,662 iteration 3003 : loss : 0.034335, loss_ce: 0.013633
2022-01-10 00:29:59,218 iteration 3004 : loss : 0.024878, loss_ce: 0.010630
2022-01-10 00:30:00,797 iteration 3005 : loss : 0.032219, loss_ce: 0.013966
2022-01-10 00:30:02,409 iteration 3006 : loss : 0.040858, loss_ce: 0.017684
2022-01-10 00:30:04,015 iteration 3007 : loss : 0.040786, loss_ce: 0.016944
2022-01-10 00:30:05,625 iteration 3008 : loss : 0.041177, loss_ce: 0.016780
2022-01-10 00:30:07,133 iteration 3009 : loss : 0.035974, loss_ce: 0.013003
 44%|███████████▉               | 177/400 [1:26:42<1:47:14, 28.86s/it]2022-01-10 00:30:08,878 iteration 3010 : loss : 0.048387, loss_ce: 0.019261
2022-01-10 00:30:10,495 iteration 3011 : loss : 0.042085, loss_ce: 0.023497
2022-01-10 00:30:12,025 iteration 3012 : loss : 0.025115, loss_ce: 0.010779
2022-01-10 00:30:13,537 iteration 3013 : loss : 0.045359, loss_ce: 0.014801
2022-01-10 00:30:15,117 iteration 3014 : loss : 0.032397, loss_ce: 0.013456
2022-01-10 00:30:16,684 iteration 3015 : loss : 0.041304, loss_ce: 0.010677
2022-01-10 00:30:18,247 iteration 3016 : loss : 0.053761, loss_ce: 0.021527
2022-01-10 00:30:19,791 iteration 3017 : loss : 0.029351, loss_ce: 0.015357
2022-01-10 00:30:21,336 iteration 3018 : loss : 0.021506, loss_ce: 0.009639
2022-01-10 00:30:22,838 iteration 3019 : loss : 0.037737, loss_ce: 0.013085
2022-01-10 00:30:24,433 iteration 3020 : loss : 0.042988, loss_ce: 0.015817
2022-01-10 00:30:26,014 iteration 3021 : loss : 0.027319, loss_ce: 0.010211
2022-01-10 00:30:27,567 iteration 3022 : loss : 0.024374, loss_ce: 0.009291
2022-01-10 00:30:29,146 iteration 3023 : loss : 0.038591, loss_ce: 0.013617
2022-01-10 00:30:30,802 iteration 3024 : loss : 0.037449, loss_ce: 0.014847
2022-01-10 00:30:32,436 iteration 3025 : loss : 0.042292, loss_ce: 0.017288
2022-01-10 00:30:33,966 iteration 3026 : loss : 0.023437, loss_ce: 0.009991
 44%|████████████               | 178/400 [1:27:09<1:44:31, 28.25s/it]2022-01-10 00:30:35,653 iteration 3027 : loss : 0.058904, loss_ce: 0.021916
2022-01-10 00:30:37,186 iteration 3028 : loss : 0.027942, loss_ce: 0.011858
2022-01-10 00:30:38,779 iteration 3029 : loss : 0.031614, loss_ce: 0.012684
2022-01-10 00:30:40,324 iteration 3030 : loss : 0.028920, loss_ce: 0.012693
2022-01-10 00:30:41,900 iteration 3031 : loss : 0.054704, loss_ce: 0.018249
2022-01-10 00:30:43,433 iteration 3032 : loss : 0.037479, loss_ce: 0.013465
2022-01-10 00:30:45,038 iteration 3033 : loss : 0.047537, loss_ce: 0.015191
2022-01-10 00:30:46,660 iteration 3034 : loss : 0.028646, loss_ce: 0.009920
2022-01-10 00:30:48,254 iteration 3035 : loss : 0.030129, loss_ce: 0.013066
2022-01-10 00:30:49,810 iteration 3036 : loss : 0.043630, loss_ce: 0.023210
2022-01-10 00:30:51,382 iteration 3037 : loss : 0.026312, loss_ce: 0.010427
2022-01-10 00:30:52,988 iteration 3038 : loss : 0.029795, loss_ce: 0.010330
2022-01-10 00:30:54,501 iteration 3039 : loss : 0.034386, loss_ce: 0.011353
2022-01-10 00:30:56,081 iteration 3040 : loss : 0.050382, loss_ce: 0.013430
2022-01-10 00:30:57,701 iteration 3041 : loss : 0.032924, loss_ce: 0.012881
2022-01-10 00:30:59,274 iteration 3042 : loss : 0.061552, loss_ce: 0.033235
2022-01-10 00:31:01,017 iteration 3043 : loss : 0.032688, loss_ce: 0.011793
 45%|████████████               | 179/400 [1:27:36<1:42:43, 27.89s/it]2022-01-10 00:31:02,663 iteration 3044 : loss : 0.031494, loss_ce: 0.012818
2022-01-10 00:31:04,186 iteration 3045 : loss : 0.030360, loss_ce: 0.011508
2022-01-10 00:31:05,735 iteration 3046 : loss : 0.026033, loss_ce: 0.008465
2022-01-10 00:31:07,416 iteration 3047 : loss : 0.042916, loss_ce: 0.020436
2022-01-10 00:31:08,953 iteration 3048 : loss : 0.036027, loss_ce: 0.010702
2022-01-10 00:31:10,530 iteration 3049 : loss : 0.053126, loss_ce: 0.020646
2022-01-10 00:31:12,134 iteration 3050 : loss : 0.031810, loss_ce: 0.017294
2022-01-10 00:31:13,717 iteration 3051 : loss : 0.034766, loss_ce: 0.010540
2022-01-10 00:31:15,284 iteration 3052 : loss : 0.033548, loss_ce: 0.011842
2022-01-10 00:31:16,942 iteration 3053 : loss : 0.031252, loss_ce: 0.012965
2022-01-10 00:31:18,477 iteration 3054 : loss : 0.033180, loss_ce: 0.010052
2022-01-10 00:31:20,074 iteration 3055 : loss : 0.035804, loss_ce: 0.012361
2022-01-10 00:31:21,623 iteration 3056 : loss : 0.039365, loss_ce: 0.015518
2022-01-10 00:31:23,138 iteration 3057 : loss : 0.031668, loss_ce: 0.014197
2022-01-10 00:31:24,728 iteration 3058 : loss : 0.023079, loss_ce: 0.009139
2022-01-10 00:31:26,336 iteration 3059 : loss : 0.029663, loss_ce: 0.009357
2022-01-10 00:31:26,337 Training Data Eval:
2022-01-10 00:31:34,402   Average segmentation loss on training set: 0.0219
2022-01-10 00:31:34,403 Validation Data Eval:
2022-01-10 00:31:37,185   Average segmentation loss on validation set: 0.0816
2022-01-10 00:31:38,720 iteration 3060 : loss : 0.026438, loss_ce: 0.012680
 45%|████████████▏              | 180/400 [1:28:13<1:53:03, 30.83s/it]2022-01-10 00:31:40,275 iteration 3061 : loss : 0.027188, loss_ce: 0.010709
2022-01-10 00:31:41,844 iteration 3062 : loss : 0.036414, loss_ce: 0.013282
2022-01-10 00:31:43,450 iteration 3063 : loss : 0.031594, loss_ce: 0.011343
2022-01-10 00:31:45,060 iteration 3064 : loss : 0.032689, loss_ce: 0.015275
2022-01-10 00:31:46,540 iteration 3065 : loss : 0.023786, loss_ce: 0.008558
2022-01-10 00:31:48,075 iteration 3066 : loss : 0.028178, loss_ce: 0.012761
2022-01-10 00:31:49,670 iteration 3067 : loss : 0.046001, loss_ce: 0.022450
2022-01-10 00:31:51,257 iteration 3068 : loss : 0.032930, loss_ce: 0.010435
2022-01-10 00:31:52,831 iteration 3069 : loss : 0.050763, loss_ce: 0.016792
2022-01-10 00:31:54,412 iteration 3070 : loss : 0.024247, loss_ce: 0.011175
2022-01-10 00:31:55,934 iteration 3071 : loss : 0.025692, loss_ce: 0.010646
2022-01-10 00:31:57,451 iteration 3072 : loss : 0.027768, loss_ce: 0.009927
2022-01-10 00:31:59,005 iteration 3073 : loss : 0.026389, loss_ce: 0.009398
2022-01-10 00:32:00,646 iteration 3074 : loss : 0.046491, loss_ce: 0.018122
2022-01-10 00:32:02,251 iteration 3075 : loss : 0.029342, loss_ce: 0.010061
2022-01-10 00:32:03,894 iteration 3076 : loss : 0.033345, loss_ce: 0.011734
2022-01-10 00:32:05,427 iteration 3077 : loss : 0.040944, loss_ce: 0.013890
 45%|████████████▏              | 181/400 [1:28:40<1:48:00, 29.59s/it]2022-01-10 00:32:07,002 iteration 3078 : loss : 0.028033, loss_ce: 0.010732
2022-01-10 00:32:08,680 iteration 3079 : loss : 0.034075, loss_ce: 0.013574
2022-01-10 00:32:10,181 iteration 3080 : loss : 0.032418, loss_ce: 0.009267
2022-01-10 00:32:11,779 iteration 3081 : loss : 0.031504, loss_ce: 0.010682
2022-01-10 00:32:13,335 iteration 3082 : loss : 0.032589, loss_ce: 0.009035
2022-01-10 00:32:14,956 iteration 3083 : loss : 0.032471, loss_ce: 0.012720
2022-01-10 00:32:16,416 iteration 3084 : loss : 0.022445, loss_ce: 0.009743
2022-01-10 00:32:18,006 iteration 3085 : loss : 0.033980, loss_ce: 0.016348
2022-01-10 00:32:19,675 iteration 3086 : loss : 0.032121, loss_ce: 0.015662
2022-01-10 00:32:21,227 iteration 3087 : loss : 0.035606, loss_ce: 0.013474
2022-01-10 00:32:22,811 iteration 3088 : loss : 0.021141, loss_ce: 0.006706
2022-01-10 00:32:24,472 iteration 3089 : loss : 0.053792, loss_ce: 0.016056
2022-01-10 00:32:25,971 iteration 3090 : loss : 0.030614, loss_ce: 0.013802
2022-01-10 00:32:27,544 iteration 3091 : loss : 0.028697, loss_ce: 0.013615
2022-01-10 00:32:29,256 iteration 3092 : loss : 0.038430, loss_ce: 0.014690
2022-01-10 00:32:30,802 iteration 3093 : loss : 0.027186, loss_ce: 0.011401
2022-01-10 00:32:32,385 iteration 3094 : loss : 0.027842, loss_ce: 0.012111
 46%|████████████▎              | 182/400 [1:29:07<1:44:39, 28.81s/it]2022-01-10 00:32:34,019 iteration 3095 : loss : 0.030016, loss_ce: 0.012857
2022-01-10 00:32:35,609 iteration 3096 : loss : 0.033683, loss_ce: 0.014221
2022-01-10 00:32:37,164 iteration 3097 : loss : 0.023918, loss_ce: 0.009220
2022-01-10 00:32:38,694 iteration 3098 : loss : 0.022233, loss_ce: 0.009373
2022-01-10 00:32:40,265 iteration 3099 : loss : 0.022992, loss_ce: 0.007639
2022-01-10 00:32:41,785 iteration 3100 : loss : 0.028925, loss_ce: 0.008999
2022-01-10 00:32:43,406 iteration 3101 : loss : 0.023886, loss_ce: 0.008634
2022-01-10 00:32:44,980 iteration 3102 : loss : 0.035640, loss_ce: 0.014833
2022-01-10 00:32:46,569 iteration 3103 : loss : 0.034300, loss_ce: 0.014872
2022-01-10 00:32:48,100 iteration 3104 : loss : 0.034842, loss_ce: 0.018438
2022-01-10 00:32:49,609 iteration 3105 : loss : 0.022614, loss_ce: 0.010185
2022-01-10 00:32:51,164 iteration 3106 : loss : 0.027702, loss_ce: 0.013077
2022-01-10 00:32:52,714 iteration 3107 : loss : 0.025451, loss_ce: 0.007409
2022-01-10 00:32:54,304 iteration 3108 : loss : 0.033033, loss_ce: 0.010726
2022-01-10 00:32:55,909 iteration 3109 : loss : 0.032545, loss_ce: 0.012834
2022-01-10 00:32:57,517 iteration 3110 : loss : 0.051017, loss_ce: 0.023097
2022-01-10 00:32:59,106 iteration 3111 : loss : 0.027878, loss_ce: 0.011057
 46%|████████████▎              | 183/400 [1:29:34<1:41:54, 28.18s/it]2022-01-10 00:33:00,755 iteration 3112 : loss : 0.033014, loss_ce: 0.011836
2022-01-10 00:33:02,292 iteration 3113 : loss : 0.025973, loss_ce: 0.010737
2022-01-10 00:33:03,907 iteration 3114 : loss : 0.036457, loss_ce: 0.013743
2022-01-10 00:33:05,489 iteration 3115 : loss : 0.029201, loss_ce: 0.014051
2022-01-10 00:33:07,053 iteration 3116 : loss : 0.028692, loss_ce: 0.012913
2022-01-10 00:33:08,690 iteration 3117 : loss : 0.026381, loss_ce: 0.011302
2022-01-10 00:33:10,242 iteration 3118 : loss : 0.044552, loss_ce: 0.020297
2022-01-10 00:33:11,913 iteration 3119 : loss : 0.045660, loss_ce: 0.014348
2022-01-10 00:33:13,439 iteration 3120 : loss : 0.028270, loss_ce: 0.010828
2022-01-10 00:33:15,073 iteration 3121 : loss : 0.031504, loss_ce: 0.014485
2022-01-10 00:33:16,628 iteration 3122 : loss : 0.037613, loss_ce: 0.012417
2022-01-10 00:33:18,254 iteration 3123 : loss : 0.030347, loss_ce: 0.012873
2022-01-10 00:33:19,754 iteration 3124 : loss : 0.025181, loss_ce: 0.010104
2022-01-10 00:33:21,315 iteration 3125 : loss : 0.033548, loss_ce: 0.010089
2022-01-10 00:33:22,943 iteration 3126 : loss : 0.029293, loss_ce: 0.013984
2022-01-10 00:33:24,585 iteration 3127 : loss : 0.034267, loss_ce: 0.011117
2022-01-10 00:33:26,216 iteration 3128 : loss : 0.034441, loss_ce: 0.013177
 46%|████████████▍              | 184/400 [1:30:01<1:40:17, 27.86s/it]2022-01-10 00:33:27,817 iteration 3129 : loss : 0.028411, loss_ce: 0.010755
2022-01-10 00:33:29,484 iteration 3130 : loss : 0.036484, loss_ce: 0.014191
2022-01-10 00:33:31,045 iteration 3131 : loss : 0.030738, loss_ce: 0.010035
2022-01-10 00:33:32,617 iteration 3132 : loss : 0.033196, loss_ce: 0.010024
2022-01-10 00:33:34,192 iteration 3133 : loss : 0.049043, loss_ce: 0.018364
2022-01-10 00:33:35,737 iteration 3134 : loss : 0.024027, loss_ce: 0.011223
2022-01-10 00:33:37,305 iteration 3135 : loss : 0.032366, loss_ce: 0.014951
2022-01-10 00:33:38,857 iteration 3136 : loss : 0.029173, loss_ce: 0.012188
2022-01-10 00:33:40,541 iteration 3137 : loss : 0.032110, loss_ce: 0.014189
2022-01-10 00:33:42,182 iteration 3138 : loss : 0.048533, loss_ce: 0.022376
2022-01-10 00:33:43,699 iteration 3139 : loss : 0.030592, loss_ce: 0.011044
2022-01-10 00:33:45,219 iteration 3140 : loss : 0.031980, loss_ce: 0.012812
2022-01-10 00:33:46,808 iteration 3141 : loss : 0.046222, loss_ce: 0.020017
2022-01-10 00:33:48,359 iteration 3142 : loss : 0.052075, loss_ce: 0.019354
2022-01-10 00:33:49,974 iteration 3143 : loss : 0.051658, loss_ce: 0.017664
2022-01-10 00:33:51,538 iteration 3144 : loss : 0.027936, loss_ce: 0.010464
2022-01-10 00:33:51,538 Training Data Eval:
2022-01-10 00:33:59,604   Average segmentation loss on training set: 0.0225
2022-01-10 00:33:59,604 Validation Data Eval:
2022-01-10 00:34:02,375   Average segmentation loss on validation set: 0.1179
2022-01-10 00:34:03,907 iteration 3145 : loss : 0.021264, loss_ce: 0.008669
 46%|████████████▍              | 185/400 [1:30:39<1:50:23, 30.81s/it]2022-01-10 00:34:05,675 iteration 3146 : loss : 0.057546, loss_ce: 0.019068
2022-01-10 00:34:07,293 iteration 3147 : loss : 0.042805, loss_ce: 0.018775
2022-01-10 00:34:08,810 iteration 3148 : loss : 0.021662, loss_ce: 0.007706
2022-01-10 00:34:10,423 iteration 3149 : loss : 0.029699, loss_ce: 0.009011
2022-01-10 00:34:12,034 iteration 3150 : loss : 0.034467, loss_ce: 0.010388
2022-01-10 00:34:13,641 iteration 3151 : loss : 0.056283, loss_ce: 0.028068
2022-01-10 00:34:15,244 iteration 3152 : loss : 0.034955, loss_ce: 0.013101
2022-01-10 00:34:16,831 iteration 3153 : loss : 0.034651, loss_ce: 0.017019
2022-01-10 00:34:18,480 iteration 3154 : loss : 0.037392, loss_ce: 0.014555
2022-01-10 00:34:20,094 iteration 3155 : loss : 0.034075, loss_ce: 0.011917
2022-01-10 00:34:21,612 iteration 3156 : loss : 0.032312, loss_ce: 0.012157
2022-01-10 00:34:23,202 iteration 3157 : loss : 0.030157, loss_ce: 0.012062
2022-01-10 00:34:24,736 iteration 3158 : loss : 0.046202, loss_ce: 0.011832
2022-01-10 00:34:26,342 iteration 3159 : loss : 0.046194, loss_ce: 0.024131
2022-01-10 00:34:27,950 iteration 3160 : loss : 0.029394, loss_ce: 0.013261
2022-01-10 00:34:29,601 iteration 3161 : loss : 0.032065, loss_ce: 0.011983
2022-01-10 00:34:31,191 iteration 3162 : loss : 0.048486, loss_ce: 0.018589
 46%|████████████▌              | 186/400 [1:31:06<1:46:06, 29.75s/it]2022-01-10 00:34:32,778 iteration 3163 : loss : 0.030785, loss_ce: 0.011018
2022-01-10 00:34:34,456 iteration 3164 : loss : 0.050575, loss_ce: 0.023100
2022-01-10 00:34:36,081 iteration 3165 : loss : 0.033461, loss_ce: 0.013829
2022-01-10 00:34:37,624 iteration 3166 : loss : 0.034293, loss_ce: 0.015067
2022-01-10 00:34:39,249 iteration 3167 : loss : 0.037461, loss_ce: 0.013557
2022-01-10 00:34:40,824 iteration 3168 : loss : 0.033267, loss_ce: 0.014449
2022-01-10 00:34:42,432 iteration 3169 : loss : 0.034874, loss_ce: 0.013229
2022-01-10 00:34:44,011 iteration 3170 : loss : 0.031656, loss_ce: 0.013980
2022-01-10 00:34:45,476 iteration 3171 : loss : 0.028414, loss_ce: 0.008828
2022-01-10 00:34:47,014 iteration 3172 : loss : 0.026011, loss_ce: 0.010208
2022-01-10 00:34:48,589 iteration 3173 : loss : 0.044020, loss_ce: 0.014628
2022-01-10 00:34:50,135 iteration 3174 : loss : 0.035666, loss_ce: 0.016227
2022-01-10 00:34:51,649 iteration 3175 : loss : 0.064776, loss_ce: 0.024435
2022-01-10 00:34:53,300 iteration 3176 : loss : 0.052334, loss_ce: 0.014478
2022-01-10 00:34:54,877 iteration 3177 : loss : 0.040133, loss_ce: 0.017899
2022-01-10 00:34:56,542 iteration 3178 : loss : 0.036863, loss_ce: 0.010646
2022-01-10 00:34:58,128 iteration 3179 : loss : 0.044721, loss_ce: 0.021831
 47%|████████████▌              | 187/400 [1:31:33<1:42:37, 28.91s/it]2022-01-10 00:34:59,774 iteration 3180 : loss : 0.074505, loss_ce: 0.039652
2022-01-10 00:35:01,323 iteration 3181 : loss : 0.031763, loss_ce: 0.013670
2022-01-10 00:35:02,846 iteration 3182 : loss : 0.048953, loss_ce: 0.028032
2022-01-10 00:35:04,476 iteration 3183 : loss : 0.043752, loss_ce: 0.015179
2022-01-10 00:35:06,055 iteration 3184 : loss : 0.033139, loss_ce: 0.013027
2022-01-10 00:35:07,644 iteration 3185 : loss : 0.034706, loss_ce: 0.011244
2022-01-10 00:35:09,260 iteration 3186 : loss : 0.038661, loss_ce: 0.015113
2022-01-10 00:35:10,886 iteration 3187 : loss : 0.029577, loss_ce: 0.012497
2022-01-10 00:35:12,517 iteration 3188 : loss : 0.047448, loss_ce: 0.019492
2022-01-10 00:35:14,089 iteration 3189 : loss : 0.034426, loss_ce: 0.011805
2022-01-10 00:35:15,763 iteration 3190 : loss : 0.034414, loss_ce: 0.013257
2022-01-10 00:35:17,389 iteration 3191 : loss : 0.034958, loss_ce: 0.014467
2022-01-10 00:35:18,909 iteration 3192 : loss : 0.043590, loss_ce: 0.009611
2022-01-10 00:35:20,486 iteration 3193 : loss : 0.029473, loss_ce: 0.012295
2022-01-10 00:35:22,185 iteration 3194 : loss : 0.056673, loss_ce: 0.023695
2022-01-10 00:35:23,777 iteration 3195 : loss : 0.032321, loss_ce: 0.014204
2022-01-10 00:35:25,359 iteration 3196 : loss : 0.038315, loss_ce: 0.018866
 47%|████████████▋              | 188/400 [1:32:00<1:40:21, 28.40s/it]2022-01-10 00:35:26,978 iteration 3197 : loss : 0.027341, loss_ce: 0.012154
2022-01-10 00:35:28,596 iteration 3198 : loss : 0.045216, loss_ce: 0.027438
2022-01-10 00:35:30,204 iteration 3199 : loss : 0.053302, loss_ce: 0.013958
2022-01-10 00:35:31,832 iteration 3200 : loss : 0.039920, loss_ce: 0.013805
2022-01-10 00:35:33,457 iteration 3201 : loss : 0.032773, loss_ce: 0.012064
2022-01-10 00:35:35,085 iteration 3202 : loss : 0.036742, loss_ce: 0.010846
2022-01-10 00:35:36,595 iteration 3203 : loss : 0.026679, loss_ce: 0.012044
2022-01-10 00:35:38,091 iteration 3204 : loss : 0.024804, loss_ce: 0.010456
2022-01-10 00:35:39,588 iteration 3205 : loss : 0.035068, loss_ce: 0.012129
2022-01-10 00:35:41,050 iteration 3206 : loss : 0.024895, loss_ce: 0.008712
2022-01-10 00:35:42,654 iteration 3207 : loss : 0.038379, loss_ce: 0.014659
2022-01-10 00:35:44,224 iteration 3208 : loss : 0.034770, loss_ce: 0.013986
2022-01-10 00:35:45,773 iteration 3209 : loss : 0.041126, loss_ce: 0.016746
2022-01-10 00:35:47,374 iteration 3210 : loss : 0.029552, loss_ce: 0.013768
2022-01-10 00:35:48,894 iteration 3211 : loss : 0.030997, loss_ce: 0.010237
2022-01-10 00:35:50,539 iteration 3212 : loss : 0.051543, loss_ce: 0.026532
2022-01-10 00:35:52,200 iteration 3213 : loss : 0.032880, loss_ce: 0.009946
 47%|████████████▊              | 189/400 [1:32:27<1:38:14, 27.93s/it]2022-01-10 00:35:53,779 iteration 3214 : loss : 0.022170, loss_ce: 0.010290
2022-01-10 00:35:55,354 iteration 3215 : loss : 0.027620, loss_ce: 0.012121
2022-01-10 00:35:56,928 iteration 3216 : loss : 0.047371, loss_ce: 0.018331
2022-01-10 00:35:58,535 iteration 3217 : loss : 0.040392, loss_ce: 0.018147
2022-01-10 00:36:00,117 iteration 3218 : loss : 0.047893, loss_ce: 0.016678
2022-01-10 00:36:01,688 iteration 3219 : loss : 0.037926, loss_ce: 0.014891
2022-01-10 00:36:03,214 iteration 3220 : loss : 0.029179, loss_ce: 0.013791
2022-01-10 00:36:04,878 iteration 3221 : loss : 0.047329, loss_ce: 0.022397
2022-01-10 00:36:06,427 iteration 3222 : loss : 0.030085, loss_ce: 0.008398
2022-01-10 00:36:08,031 iteration 3223 : loss : 0.030911, loss_ce: 0.010306
2022-01-10 00:36:09,694 iteration 3224 : loss : 0.048676, loss_ce: 0.015146
2022-01-10 00:36:11,273 iteration 3225 : loss : 0.040752, loss_ce: 0.018315
2022-01-10 00:36:12,849 iteration 3226 : loss : 0.036881, loss_ce: 0.014593
2022-01-10 00:36:14,439 iteration 3227 : loss : 0.045481, loss_ce: 0.019203
2022-01-10 00:36:15,987 iteration 3228 : loss : 0.032680, loss_ce: 0.010336
2022-01-10 00:36:17,507 iteration 3229 : loss : 0.031290, loss_ce: 0.009202
2022-01-10 00:36:17,507 Training Data Eval:
2022-01-10 00:36:25,572   Average segmentation loss on training set: 0.0246
2022-01-10 00:36:25,572 Validation Data Eval:
2022-01-10 00:36:28,341   Average segmentation loss on validation set: 0.0872
2022-01-10 00:36:29,844 iteration 3230 : loss : 0.039993, loss_ce: 0.014663
 48%|████████████▊              | 190/400 [1:33:04<1:47:57, 30.85s/it]2022-01-10 00:36:31,430 iteration 3231 : loss : 0.048565, loss_ce: 0.011668
2022-01-10 00:36:32,980 iteration 3232 : loss : 0.028415, loss_ce: 0.009449
2022-01-10 00:36:34,557 iteration 3233 : loss : 0.036372, loss_ce: 0.017228
2022-01-10 00:36:36,158 iteration 3234 : loss : 0.023471, loss_ce: 0.007969
2022-01-10 00:36:37,754 iteration 3235 : loss : 0.054306, loss_ce: 0.021480
2022-01-10 00:36:39,269 iteration 3236 : loss : 0.033541, loss_ce: 0.018724
2022-01-10 00:36:40,937 iteration 3237 : loss : 0.032061, loss_ce: 0.013152
2022-01-10 00:36:42,522 iteration 3238 : loss : 0.034009, loss_ce: 0.015524
2022-01-10 00:36:44,106 iteration 3239 : loss : 0.027934, loss_ce: 0.010174
2022-01-10 00:36:45,631 iteration 3240 : loss : 0.053076, loss_ce: 0.013707
2022-01-10 00:36:47,134 iteration 3241 : loss : 0.033149, loss_ce: 0.009208
2022-01-10 00:36:48,712 iteration 3242 : loss : 0.035979, loss_ce: 0.014209
2022-01-10 00:36:50,225 iteration 3243 : loss : 0.039419, loss_ce: 0.016459
2022-01-10 00:36:51,823 iteration 3244 : loss : 0.037599, loss_ce: 0.015514
2022-01-10 00:36:53,356 iteration 3245 : loss : 0.021250, loss_ce: 0.009758
2022-01-10 00:36:54,992 iteration 3246 : loss : 0.042732, loss_ce: 0.020796
2022-01-10 00:36:56,537 iteration 3247 : loss : 0.034805, loss_ce: 0.013823
 48%|████████████▉              | 191/400 [1:33:31<1:43:06, 29.60s/it]2022-01-10 00:36:58,122 iteration 3248 : loss : 0.024183, loss_ce: 0.009113
2022-01-10 00:36:59,649 iteration 3249 : loss : 0.027051, loss_ce: 0.009928
2022-01-10 00:37:01,204 iteration 3250 : loss : 0.039633, loss_ce: 0.014237
2022-01-10 00:37:02,761 iteration 3251 : loss : 0.034320, loss_ce: 0.014249
2022-01-10 00:37:04,341 iteration 3252 : loss : 0.032438, loss_ce: 0.013400
2022-01-10 00:37:05,897 iteration 3253 : loss : 0.023636, loss_ce: 0.010486
2022-01-10 00:37:07,445 iteration 3254 : loss : 0.030683, loss_ce: 0.012004
2022-01-10 00:37:08,933 iteration 3255 : loss : 0.030270, loss_ce: 0.013052
2022-01-10 00:37:10,483 iteration 3256 : loss : 0.028217, loss_ce: 0.011015
2022-01-10 00:37:12,009 iteration 3257 : loss : 0.022841, loss_ce: 0.009821
2022-01-10 00:37:13,588 iteration 3258 : loss : 0.033183, loss_ce: 0.013483
2022-01-10 00:37:15,102 iteration 3259 : loss : 0.035138, loss_ce: 0.016000
2022-01-10 00:37:16,681 iteration 3260 : loss : 0.034783, loss_ce: 0.014590
2022-01-10 00:37:18,222 iteration 3261 : loss : 0.050570, loss_ce: 0.016361
2022-01-10 00:37:19,829 iteration 3262 : loss : 0.039110, loss_ce: 0.016256
2022-01-10 00:37:21,505 iteration 3263 : loss : 0.028019, loss_ce: 0.008569
2022-01-10 00:37:23,088 iteration 3264 : loss : 0.042810, loss_ce: 0.021239
 48%|████████████▉              | 192/400 [1:33:58<1:39:27, 28.69s/it]2022-01-10 00:37:24,665 iteration 3265 : loss : 0.025040, loss_ce: 0.010309
2022-01-10 00:37:26,285 iteration 3266 : loss : 0.036956, loss_ce: 0.016351
2022-01-10 00:37:27,877 iteration 3267 : loss : 0.028627, loss_ce: 0.012386
2022-01-10 00:37:29,406 iteration 3268 : loss : 0.029945, loss_ce: 0.011432
2022-01-10 00:37:31,031 iteration 3269 : loss : 0.041190, loss_ce: 0.018832
2022-01-10 00:37:32,686 iteration 3270 : loss : 0.042955, loss_ce: 0.013497
2022-01-10 00:37:34,309 iteration 3271 : loss : 0.034282, loss_ce: 0.010299
2022-01-10 00:37:35,937 iteration 3272 : loss : 0.025272, loss_ce: 0.011115
2022-01-10 00:37:37,454 iteration 3273 : loss : 0.026330, loss_ce: 0.008093
2022-01-10 00:37:39,018 iteration 3274 : loss : 0.028072, loss_ce: 0.015197
2022-01-10 00:37:40,649 iteration 3275 : loss : 0.038918, loss_ce: 0.010480
2022-01-10 00:37:42,195 iteration 3276 : loss : 0.033302, loss_ce: 0.015072
2022-01-10 00:37:43,796 iteration 3277 : loss : 0.029913, loss_ce: 0.015014
2022-01-10 00:37:45,377 iteration 3278 : loss : 0.039598, loss_ce: 0.014039
2022-01-10 00:37:46,915 iteration 3279 : loss : 0.029307, loss_ce: 0.011326
2022-01-10 00:37:48,472 iteration 3280 : loss : 0.024214, loss_ce: 0.009003
2022-01-10 00:37:49,988 iteration 3281 : loss : 0.023204, loss_ce: 0.007300
 48%|█████████████              | 193/400 [1:34:25<1:37:06, 28.15s/it]2022-01-10 00:37:51,646 iteration 3282 : loss : 0.036948, loss_ce: 0.014199
2022-01-10 00:37:53,217 iteration 3283 : loss : 0.025804, loss_ce: 0.008923
2022-01-10 00:37:54,790 iteration 3284 : loss : 0.029156, loss_ce: 0.012900
2022-01-10 00:37:56,347 iteration 3285 : loss : 0.056733, loss_ce: 0.029399
2022-01-10 00:37:57,903 iteration 3286 : loss : 0.019133, loss_ce: 0.007079
2022-01-10 00:37:59,486 iteration 3287 : loss : 0.039643, loss_ce: 0.017068
2022-01-10 00:38:01,005 iteration 3288 : loss : 0.032110, loss_ce: 0.018029
2022-01-10 00:38:02,540 iteration 3289 : loss : 0.033673, loss_ce: 0.009491
2022-01-10 00:38:04,160 iteration 3290 : loss : 0.035438, loss_ce: 0.010311
2022-01-10 00:38:05,790 iteration 3291 : loss : 0.051344, loss_ce: 0.019118
2022-01-10 00:38:07,379 iteration 3292 : loss : 0.031363, loss_ce: 0.009825
2022-01-10 00:38:08,946 iteration 3293 : loss : 0.031459, loss_ce: 0.013608
2022-01-10 00:38:10,486 iteration 3294 : loss : 0.028504, loss_ce: 0.014029
2022-01-10 00:38:12,141 iteration 3295 : loss : 0.047449, loss_ce: 0.018405
2022-01-10 00:38:13,742 iteration 3296 : loss : 0.038255, loss_ce: 0.010145
2022-01-10 00:38:15,249 iteration 3297 : loss : 0.027947, loss_ce: 0.011886
2022-01-10 00:38:16,861 iteration 3298 : loss : 0.039190, loss_ce: 0.015743
 48%|█████████████              | 194/400 [1:34:51<1:35:19, 27.77s/it]2022-01-10 00:38:18,421 iteration 3299 : loss : 0.024316, loss_ce: 0.010572
2022-01-10 00:38:19,957 iteration 3300 : loss : 0.041692, loss_ce: 0.015858
2022-01-10 00:38:21,581 iteration 3301 : loss : 0.044144, loss_ce: 0.023336
2022-01-10 00:38:23,149 iteration 3302 : loss : 0.029972, loss_ce: 0.011800
2022-01-10 00:38:24,699 iteration 3303 : loss : 0.026699, loss_ce: 0.011013
2022-01-10 00:38:26,273 iteration 3304 : loss : 0.032408, loss_ce: 0.012821
2022-01-10 00:38:27,868 iteration 3305 : loss : 0.029549, loss_ce: 0.010257
2022-01-10 00:38:29,487 iteration 3306 : loss : 0.043817, loss_ce: 0.016687
2022-01-10 00:38:31,030 iteration 3307 : loss : 0.035750, loss_ce: 0.013048
2022-01-10 00:38:32,609 iteration 3308 : loss : 0.035243, loss_ce: 0.014413
2022-01-10 00:38:34,175 iteration 3309 : loss : 0.034538, loss_ce: 0.008889
2022-01-10 00:38:35,817 iteration 3310 : loss : 0.071583, loss_ce: 0.025804
2022-01-10 00:38:37,482 iteration 3311 : loss : 0.030797, loss_ce: 0.010663
2022-01-10 00:38:39,003 iteration 3312 : loss : 0.029639, loss_ce: 0.009810
2022-01-10 00:38:40,538 iteration 3313 : loss : 0.028371, loss_ce: 0.010234
2022-01-10 00:38:42,175 iteration 3314 : loss : 0.042232, loss_ce: 0.012240
2022-01-10 00:38:42,175 Training Data Eval:
2022-01-10 00:38:50,236   Average segmentation loss on training set: 0.0214
2022-01-10 00:38:50,236 Validation Data Eval:
2022-01-10 00:38:53,018   Average segmentation loss on validation set: 0.0668
2022-01-10 00:38:59,451 Found new lowest validation loss at iteration 3314! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-10 00:39:00,881 iteration 3315 : loss : 0.020590, loss_ce: 0.009842
 49%|█████████████▏             | 195/400 [1:35:35<1:51:31, 32.64s/it]2022-01-10 00:39:02,327 iteration 3316 : loss : 0.019279, loss_ce: 0.007472
2022-01-10 00:39:03,842 iteration 3317 : loss : 0.030244, loss_ce: 0.011403
2022-01-10 00:39:05,509 iteration 3318 : loss : 0.037517, loss_ce: 0.014979
2022-01-10 00:39:07,051 iteration 3319 : loss : 0.031086, loss_ce: 0.011953
2022-01-10 00:39:08,606 iteration 3320 : loss : 0.026804, loss_ce: 0.010275
2022-01-10 00:39:10,190 iteration 3321 : loss : 0.023415, loss_ce: 0.006917
2022-01-10 00:39:11,768 iteration 3322 : loss : 0.051488, loss_ce: 0.031287
2022-01-10 00:39:13,291 iteration 3323 : loss : 0.027619, loss_ce: 0.010462
2022-01-10 00:39:14,939 iteration 3324 : loss : 0.044380, loss_ce: 0.021473
2022-01-10 00:39:16,467 iteration 3325 : loss : 0.033082, loss_ce: 0.010599
2022-01-10 00:39:18,065 iteration 3326 : loss : 0.030633, loss_ce: 0.011749
2022-01-10 00:39:19,734 iteration 3327 : loss : 0.082994, loss_ce: 0.028446
2022-01-10 00:39:21,308 iteration 3328 : loss : 0.033394, loss_ce: 0.012201
2022-01-10 00:39:22,849 iteration 3329 : loss : 0.024395, loss_ce: 0.010508
2022-01-10 00:39:24,408 iteration 3330 : loss : 0.039038, loss_ce: 0.012546
2022-01-10 00:39:25,993 iteration 3331 : loss : 0.030222, loss_ce: 0.011378
2022-01-10 00:39:27,485 iteration 3332 : loss : 0.023427, loss_ce: 0.010347
 49%|█████████████▏             | 196/400 [1:36:02<1:44:50, 30.83s/it]2022-01-10 00:39:29,096 iteration 3333 : loss : 0.028778, loss_ce: 0.011067
2022-01-10 00:39:30,685 iteration 3334 : loss : 0.020656, loss_ce: 0.009120
2022-01-10 00:39:32,181 iteration 3335 : loss : 0.021167, loss_ce: 0.009331
2022-01-10 00:39:33,751 iteration 3336 : loss : 0.045193, loss_ce: 0.016498
2022-01-10 00:39:35,370 iteration 3337 : loss : 0.035583, loss_ce: 0.014414
2022-01-10 00:39:37,036 iteration 3338 : loss : 0.029114, loss_ce: 0.014434
2022-01-10 00:39:38,602 iteration 3339 : loss : 0.034524, loss_ce: 0.009829
2022-01-10 00:39:40,149 iteration 3340 : loss : 0.020760, loss_ce: 0.007469
2022-01-10 00:39:41,711 iteration 3341 : loss : 0.023730, loss_ce: 0.009289
2022-01-10 00:39:43,328 iteration 3342 : loss : 0.038502, loss_ce: 0.019041
2022-01-10 00:39:44,924 iteration 3343 : loss : 0.030425, loss_ce: 0.011267
2022-01-10 00:39:46,512 iteration 3344 : loss : 0.038800, loss_ce: 0.013476
2022-01-10 00:39:48,061 iteration 3345 : loss : 0.029504, loss_ce: 0.010290
2022-01-10 00:39:49,639 iteration 3346 : loss : 0.024200, loss_ce: 0.011204
2022-01-10 00:39:51,244 iteration 3347 : loss : 0.050812, loss_ce: 0.013183
2022-01-10 00:39:52,904 iteration 3348 : loss : 0.030536, loss_ce: 0.010218
2022-01-10 00:39:54,498 iteration 3349 : loss : 0.037011, loss_ce: 0.013067
 49%|█████████████▎             | 197/400 [1:36:29<1:40:26, 29.69s/it]2022-01-10 00:39:56,092 iteration 3350 : loss : 0.027057, loss_ce: 0.011192
2022-01-10 00:39:57,618 iteration 3351 : loss : 0.024781, loss_ce: 0.009052
2022-01-10 00:39:59,170 iteration 3352 : loss : 0.027618, loss_ce: 0.010298
2022-01-10 00:40:00,761 iteration 3353 : loss : 0.029524, loss_ce: 0.010596
2022-01-10 00:40:02,351 iteration 3354 : loss : 0.019568, loss_ce: 0.006466
2022-01-10 00:40:03,997 iteration 3355 : loss : 0.032073, loss_ce: 0.016364
2022-01-10 00:40:05,617 iteration 3356 : loss : 0.028262, loss_ce: 0.009300
2022-01-10 00:40:07,247 iteration 3357 : loss : 0.036414, loss_ce: 0.010907
2022-01-10 00:40:08,895 iteration 3358 : loss : 0.030703, loss_ce: 0.011692
2022-01-10 00:40:10,501 iteration 3359 : loss : 0.025760, loss_ce: 0.010412
2022-01-10 00:40:12,168 iteration 3360 : loss : 0.032548, loss_ce: 0.013593
2022-01-10 00:40:13,692 iteration 3361 : loss : 0.027772, loss_ce: 0.006646
2022-01-10 00:40:15,230 iteration 3362 : loss : 0.029635, loss_ce: 0.011901
2022-01-10 00:40:16,760 iteration 3363 : loss : 0.024160, loss_ce: 0.011227
2022-01-10 00:40:18,354 iteration 3364 : loss : 0.032956, loss_ce: 0.012109
2022-01-10 00:40:19,887 iteration 3365 : loss : 0.023821, loss_ce: 0.012395
2022-01-10 00:40:21,452 iteration 3366 : loss : 0.025037, loss_ce: 0.009718
 50%|█████████████▎             | 198/400 [1:36:56<1:37:10, 28.87s/it]2022-01-10 00:40:23,080 iteration 3367 : loss : 0.034118, loss_ce: 0.013355
2022-01-10 00:40:24,645 iteration 3368 : loss : 0.031796, loss_ce: 0.017157
2022-01-10 00:40:26,239 iteration 3369 : loss : 0.021825, loss_ce: 0.008132
2022-01-10 00:40:27,837 iteration 3370 : loss : 0.027045, loss_ce: 0.011383
2022-01-10 00:40:29,327 iteration 3371 : loss : 0.018405, loss_ce: 0.007749
2022-01-10 00:40:30,885 iteration 3372 : loss : 0.041834, loss_ce: 0.017677
2022-01-10 00:40:32,447 iteration 3373 : loss : 0.028144, loss_ce: 0.013840
2022-01-10 00:40:33,966 iteration 3374 : loss : 0.021630, loss_ce: 0.009424
2022-01-10 00:40:35,491 iteration 3375 : loss : 0.024799, loss_ce: 0.009914
2022-01-10 00:40:37,030 iteration 3376 : loss : 0.034285, loss_ce: 0.013164
2022-01-10 00:40:38,641 iteration 3377 : loss : 0.041638, loss_ce: 0.012999
2022-01-10 00:40:40,204 iteration 3378 : loss : 0.025663, loss_ce: 0.009566
2022-01-10 00:40:41,691 iteration 3379 : loss : 0.020809, loss_ce: 0.009998
2022-01-10 00:40:43,318 iteration 3380 : loss : 0.043532, loss_ce: 0.019319
2022-01-10 00:40:44,825 iteration 3381 : loss : 0.046182, loss_ce: 0.007701
2022-01-10 00:40:46,401 iteration 3382 : loss : 0.031866, loss_ce: 0.011276
2022-01-10 00:40:47,961 iteration 3383 : loss : 0.025125, loss_ce: 0.009581
 50%|█████████████▍             | 199/400 [1:37:23<1:34:19, 28.16s/it]2022-01-10 00:40:49,537 iteration 3384 : loss : 0.025379, loss_ce: 0.009365
2022-01-10 00:40:51,152 iteration 3385 : loss : 0.026722, loss_ce: 0.011302
2022-01-10 00:40:52,778 iteration 3386 : loss : 0.037301, loss_ce: 0.015769
2022-01-10 00:40:54,387 iteration 3387 : loss : 0.036514, loss_ce: 0.015203
2022-01-10 00:40:55,900 iteration 3388 : loss : 0.031871, loss_ce: 0.013242
2022-01-10 00:40:57,495 iteration 3389 : loss : 0.039421, loss_ce: 0.014573
2022-01-10 00:40:59,002 iteration 3390 : loss : 0.028614, loss_ce: 0.008824
2022-01-10 00:41:00,673 iteration 3391 : loss : 0.055170, loss_ce: 0.020235
2022-01-10 00:41:02,282 iteration 3392 : loss : 0.044099, loss_ce: 0.017934
2022-01-10 00:41:03,886 iteration 3393 : loss : 0.028417, loss_ce: 0.010304
2022-01-10 00:41:05,373 iteration 3394 : loss : 0.026265, loss_ce: 0.009558
2022-01-10 00:41:07,052 iteration 3395 : loss : 0.044930, loss_ce: 0.014979
2022-01-10 00:41:08,643 iteration 3396 : loss : 0.034074, loss_ce: 0.012504
2022-01-10 00:41:10,197 iteration 3397 : loss : 0.025781, loss_ce: 0.011238
2022-01-10 00:41:11,809 iteration 3398 : loss : 0.042590, loss_ce: 0.013674
2022-01-10 00:41:13,400 iteration 3399 : loss : 0.025024, loss_ce: 0.011139
2022-01-10 00:41:13,400 Training Data Eval:
2022-01-10 00:41:21,460   Average segmentation loss on training set: 0.0203
2022-01-10 00:41:21,460 Validation Data Eval:
2022-01-10 00:41:24,233   Average segmentation loss on validation set: 0.1064
2022-01-10 00:41:25,805 iteration 3400 : loss : 0.026450, loss_ce: 0.009080
 50%|█████████████▌             | 200/400 [1:38:00<1:43:32, 31.06s/it]2022-01-10 00:41:27,561 iteration 3401 : loss : 0.038467, loss_ce: 0.017285
2022-01-10 00:41:29,191 iteration 3402 : loss : 0.034904, loss_ce: 0.012902
2022-01-10 00:41:30,738 iteration 3403 : loss : 0.027800, loss_ce: 0.013181
2022-01-10 00:41:32,288 iteration 3404 : loss : 0.024550, loss_ce: 0.009583
2022-01-10 00:41:33,842 iteration 3405 : loss : 0.028861, loss_ce: 0.010979
2022-01-10 00:41:35,467 iteration 3406 : loss : 0.044398, loss_ce: 0.019116
2022-01-10 00:41:37,050 iteration 3407 : loss : 0.043283, loss_ce: 0.017947
2022-01-10 00:41:38,647 iteration 3408 : loss : 0.028501, loss_ce: 0.009557
2022-01-10 00:41:40,255 iteration 3409 : loss : 0.047583, loss_ce: 0.022923
2022-01-10 00:41:41,853 iteration 3410 : loss : 0.036567, loss_ce: 0.013840
2022-01-10 00:41:43,484 iteration 3411 : loss : 0.035799, loss_ce: 0.015990
2022-01-10 00:41:45,008 iteration 3412 : loss : 0.030780, loss_ce: 0.013157
2022-01-10 00:41:46,514 iteration 3413 : loss : 0.019352, loss_ce: 0.008582
2022-01-10 00:41:48,112 iteration 3414 : loss : 0.035409, loss_ce: 0.009098
2022-01-10 00:41:49,733 iteration 3415 : loss : 0.023080, loss_ce: 0.008976
2022-01-10 00:41:51,354 iteration 3416 : loss : 0.022103, loss_ce: 0.006853
2022-01-10 00:41:53,018 iteration 3417 : loss : 0.046226, loss_ce: 0.017707
 50%|█████████████▌             | 201/400 [1:38:28<1:39:11, 29.91s/it]2022-01-10 00:41:54,663 iteration 3418 : loss : 0.021909, loss_ce: 0.010844
2022-01-10 00:41:56,276 iteration 3419 : loss : 0.036971, loss_ce: 0.012668
2022-01-10 00:41:57,949 iteration 3420 : loss : 0.039010, loss_ce: 0.012639
2022-01-10 00:41:59,551 iteration 3421 : loss : 0.035132, loss_ce: 0.013049
2022-01-10 00:42:01,075 iteration 3422 : loss : 0.022016, loss_ce: 0.007298
2022-01-10 00:42:02,655 iteration 3423 : loss : 0.022779, loss_ce: 0.009718
2022-01-10 00:42:04,235 iteration 3424 : loss : 0.037418, loss_ce: 0.015048
2022-01-10 00:42:05,759 iteration 3425 : loss : 0.022106, loss_ce: 0.009670
2022-01-10 00:42:07,342 iteration 3426 : loss : 0.028159, loss_ce: 0.009508
2022-01-10 00:42:08,938 iteration 3427 : loss : 0.040835, loss_ce: 0.012399
2022-01-10 00:42:10,538 iteration 3428 : loss : 0.032317, loss_ce: 0.012258
2022-01-10 00:42:12,165 iteration 3429 : loss : 0.025453, loss_ce: 0.009798
2022-01-10 00:42:13,796 iteration 3430 : loss : 0.027308, loss_ce: 0.010736
2022-01-10 00:42:15,301 iteration 3431 : loss : 0.025287, loss_ce: 0.010292
2022-01-10 00:42:16,838 iteration 3432 : loss : 0.026483, loss_ce: 0.009534
2022-01-10 00:42:18,374 iteration 3433 : loss : 0.029548, loss_ce: 0.011220
2022-01-10 00:42:19,991 iteration 3434 : loss : 0.048578, loss_ce: 0.016565
 50%|█████████████▋             | 202/400 [1:38:55<1:35:47, 29.03s/it]2022-01-10 00:42:21,543 iteration 3435 : loss : 0.025092, loss_ce: 0.008188
2022-01-10 00:42:23,088 iteration 3436 : loss : 0.024160, loss_ce: 0.010979
2022-01-10 00:42:24,762 iteration 3437 : loss : 0.033979, loss_ce: 0.016886
2022-01-10 00:42:26,408 iteration 3438 : loss : 0.043602, loss_ce: 0.020485
2022-01-10 00:42:28,037 iteration 3439 : loss : 0.032465, loss_ce: 0.011719
2022-01-10 00:42:29,587 iteration 3440 : loss : 0.020521, loss_ce: 0.007436
2022-01-10 00:42:31,156 iteration 3441 : loss : 0.024660, loss_ce: 0.009576
2022-01-10 00:42:32,840 iteration 3442 : loss : 0.048435, loss_ce: 0.015479
2022-01-10 00:42:34,465 iteration 3443 : loss : 0.028072, loss_ce: 0.009045
2022-01-10 00:42:36,090 iteration 3444 : loss : 0.035987, loss_ce: 0.013871
2022-01-10 00:42:37,676 iteration 3445 : loss : 0.063918, loss_ce: 0.022959
2022-01-10 00:42:39,247 iteration 3446 : loss : 0.027007, loss_ce: 0.010522
2022-01-10 00:42:40,811 iteration 3447 : loss : 0.036804, loss_ce: 0.013975
2022-01-10 00:42:42,444 iteration 3448 : loss : 0.033512, loss_ce: 0.009424
2022-01-10 00:42:44,051 iteration 3449 : loss : 0.040967, loss_ce: 0.016322
2022-01-10 00:42:45,633 iteration 3450 : loss : 0.032103, loss_ce: 0.014120
2022-01-10 00:42:47,294 iteration 3451 : loss : 0.039911, loss_ce: 0.017582
 51%|█████████████▋             | 203/400 [1:39:22<1:33:36, 28.51s/it]2022-01-10 00:42:48,931 iteration 3452 : loss : 0.045724, loss_ce: 0.016243
2022-01-10 00:42:50,490 iteration 3453 : loss : 0.029868, loss_ce: 0.011920
2022-01-10 00:42:52,032 iteration 3454 : loss : 0.032742, loss_ce: 0.010375
2022-01-10 00:42:53,682 iteration 3455 : loss : 0.041579, loss_ce: 0.017166
2022-01-10 00:42:55,219 iteration 3456 : loss : 0.020689, loss_ce: 0.006912
2022-01-10 00:42:56,893 iteration 3457 : loss : 0.033063, loss_ce: 0.012409
2022-01-10 00:42:58,511 iteration 3458 : loss : 0.035108, loss_ce: 0.012153
2022-01-10 00:43:00,088 iteration 3459 : loss : 0.036074, loss_ce: 0.014707
2022-01-10 00:43:01,659 iteration 3460 : loss : 0.026875, loss_ce: 0.012119
2022-01-10 00:43:03,234 iteration 3461 : loss : 0.021290, loss_ce: 0.007533
2022-01-10 00:43:04,792 iteration 3462 : loss : 0.025318, loss_ce: 0.011291
2022-01-10 00:43:06,405 iteration 3463 : loss : 0.027784, loss_ce: 0.011896
2022-01-10 00:43:07,929 iteration 3464 : loss : 0.025925, loss_ce: 0.009532
2022-01-10 00:43:09,552 iteration 3465 : loss : 0.032152, loss_ce: 0.010578
2022-01-10 00:43:11,146 iteration 3466 : loss : 0.027016, loss_ce: 0.009783
2022-01-10 00:43:12,725 iteration 3467 : loss : 0.031758, loss_ce: 0.019116
2022-01-10 00:43:14,293 iteration 3468 : loss : 0.040472, loss_ce: 0.016815
 51%|█████████████▊             | 204/400 [1:39:49<1:31:39, 28.06s/it]2022-01-10 00:43:15,893 iteration 3469 : loss : 0.027754, loss_ce: 0.008140
2022-01-10 00:43:17,521 iteration 3470 : loss : 0.036281, loss_ce: 0.013643
2022-01-10 00:43:19,029 iteration 3471 : loss : 0.022164, loss_ce: 0.008200
2022-01-10 00:43:20,624 iteration 3472 : loss : 0.033489, loss_ce: 0.014499
2022-01-10 00:43:22,315 iteration 3473 : loss : 0.046213, loss_ce: 0.021383
2022-01-10 00:43:23,892 iteration 3474 : loss : 0.037046, loss_ce: 0.010930
2022-01-10 00:43:25,538 iteration 3475 : loss : 0.057225, loss_ce: 0.016252
2022-01-10 00:43:27,083 iteration 3476 : loss : 0.024665, loss_ce: 0.008856
2022-01-10 00:43:28,686 iteration 3477 : loss : 0.039297, loss_ce: 0.012437
2022-01-10 00:43:30,261 iteration 3478 : loss : 0.031729, loss_ce: 0.011765
2022-01-10 00:43:31,803 iteration 3479 : loss : 0.028737, loss_ce: 0.011082
2022-01-10 00:43:33,298 iteration 3480 : loss : 0.026299, loss_ce: 0.010858
2022-01-10 00:43:34,862 iteration 3481 : loss : 0.036097, loss_ce: 0.015597
2022-01-10 00:43:36,433 iteration 3482 : loss : 0.024133, loss_ce: 0.009643
2022-01-10 00:43:38,067 iteration 3483 : loss : 0.046919, loss_ce: 0.021784
2022-01-10 00:43:39,561 iteration 3484 : loss : 0.025563, loss_ce: 0.010582
2022-01-10 00:43:39,561 Training Data Eval:
2022-01-10 00:43:47,619   Average segmentation loss on training set: 0.0206
2022-01-10 00:43:47,619 Validation Data Eval:
2022-01-10 00:43:50,396   Average segmentation loss on validation set: 0.0738
2022-01-10 00:43:52,057 iteration 3485 : loss : 0.040425, loss_ce: 0.018760
 51%|█████████████▊             | 205/400 [1:40:27<1:40:39, 30.97s/it]2022-01-10 00:43:53,687 iteration 3486 : loss : 0.030864, loss_ce: 0.014139
2022-01-10 00:43:55,205 iteration 3487 : loss : 0.025295, loss_ce: 0.010834
2022-01-10 00:43:56,755 iteration 3488 : loss : 0.044394, loss_ce: 0.015763
2022-01-10 00:43:58,315 iteration 3489 : loss : 0.040505, loss_ce: 0.015285
2022-01-10 00:43:59,911 iteration 3490 : loss : 0.033204, loss_ce: 0.012461
2022-01-10 00:44:01,545 iteration 3491 : loss : 0.040342, loss_ce: 0.018290
2022-01-10 00:44:03,060 iteration 3492 : loss : 0.032139, loss_ce: 0.012699
2022-01-10 00:44:04,735 iteration 3493 : loss : 0.034211, loss_ce: 0.013896
2022-01-10 00:44:06,338 iteration 3494 : loss : 0.024350, loss_ce: 0.007070
2022-01-10 00:44:07,905 iteration 3495 : loss : 0.035142, loss_ce: 0.011913
2022-01-10 00:44:09,511 iteration 3496 : loss : 0.038486, loss_ce: 0.021014
2022-01-10 00:44:11,114 iteration 3497 : loss : 0.043462, loss_ce: 0.018879
2022-01-10 00:44:12,800 iteration 3498 : loss : 0.040312, loss_ce: 0.014422
2022-01-10 00:44:14,348 iteration 3499 : loss : 0.028293, loss_ce: 0.010531
2022-01-10 00:44:15,841 iteration 3500 : loss : 0.020423, loss_ce: 0.006687
2022-01-10 00:44:17,372 iteration 3501 : loss : 0.028687, loss_ce: 0.010680
2022-01-10 00:44:18,961 iteration 3502 : loss : 0.038317, loss_ce: 0.015876
 52%|█████████████▉             | 206/400 [1:40:54<1:36:11, 29.75s/it]2022-01-10 00:44:20,609 iteration 3503 : loss : 0.026839, loss_ce: 0.009331
2022-01-10 00:44:22,161 iteration 3504 : loss : 0.025894, loss_ce: 0.009117
2022-01-10 00:44:23,637 iteration 3505 : loss : 0.022245, loss_ce: 0.010311
2022-01-10 00:44:25,229 iteration 3506 : loss : 0.037522, loss_ce: 0.011549
2022-01-10 00:44:26,786 iteration 3507 : loss : 0.034416, loss_ce: 0.010872
2022-01-10 00:44:28,472 iteration 3508 : loss : 0.045774, loss_ce: 0.019965
2022-01-10 00:44:30,068 iteration 3509 : loss : 0.024776, loss_ce: 0.009000
2022-01-10 00:44:31,595 iteration 3510 : loss : 0.030000, loss_ce: 0.012037
2022-01-10 00:44:33,110 iteration 3511 : loss : 0.023953, loss_ce: 0.011365
2022-01-10 00:44:34,753 iteration 3512 : loss : 0.035144, loss_ce: 0.011358
2022-01-10 00:44:36,333 iteration 3513 : loss : 0.025685, loss_ce: 0.008538
2022-01-10 00:44:38,029 iteration 3514 : loss : 0.051279, loss_ce: 0.026859
2022-01-10 00:44:39,587 iteration 3515 : loss : 0.031534, loss_ce: 0.009761
2022-01-10 00:44:41,134 iteration 3516 : loss : 0.019746, loss_ce: 0.008173
2022-01-10 00:44:42,688 iteration 3517 : loss : 0.033907, loss_ce: 0.012471
2022-01-10 00:44:44,247 iteration 3518 : loss : 0.031920, loss_ce: 0.016634
2022-01-10 00:44:45,839 iteration 3519 : loss : 0.039170, loss_ce: 0.013431
 52%|█████████████▉             | 207/400 [1:41:20<1:32:55, 28.89s/it]2022-01-10 00:44:47,485 iteration 3520 : loss : 0.030869, loss_ce: 0.011229
2022-01-10 00:44:49,031 iteration 3521 : loss : 0.041067, loss_ce: 0.018408
2022-01-10 00:44:50,543 iteration 3522 : loss : 0.028766, loss_ce: 0.013763
2022-01-10 00:44:52,102 iteration 3523 : loss : 0.023592, loss_ce: 0.008673
2022-01-10 00:44:53,705 iteration 3524 : loss : 0.027291, loss_ce: 0.011674
2022-01-10 00:44:55,374 iteration 3525 : loss : 0.026869, loss_ce: 0.010849
2022-01-10 00:44:56,969 iteration 3526 : loss : 0.038396, loss_ce: 0.015370
2022-01-10 00:44:58,587 iteration 3527 : loss : 0.050976, loss_ce: 0.016238
2022-01-10 00:45:00,290 iteration 3528 : loss : 0.045065, loss_ce: 0.020097
2022-01-10 00:45:01,902 iteration 3529 : loss : 0.035969, loss_ce: 0.009876
2022-01-10 00:45:03,509 iteration 3530 : loss : 0.025855, loss_ce: 0.010344
2022-01-10 00:45:05,041 iteration 3531 : loss : 0.032129, loss_ce: 0.009864
2022-01-10 00:45:06,693 iteration 3532 : loss : 0.037532, loss_ce: 0.016442
2022-01-10 00:45:08,255 iteration 3533 : loss : 0.025040, loss_ce: 0.007309
2022-01-10 00:45:09,863 iteration 3534 : loss : 0.042045, loss_ce: 0.016487
2022-01-10 00:45:11,519 iteration 3535 : loss : 0.031370, loss_ce: 0.014488
2022-01-10 00:45:13,137 iteration 3536 : loss : 0.025769, loss_ce: 0.010115
 52%|██████████████             | 208/400 [1:41:48<1:30:54, 28.41s/it]2022-01-10 00:45:14,697 iteration 3537 : loss : 0.025052, loss_ce: 0.009954
2022-01-10 00:45:16,207 iteration 3538 : loss : 0.019570, loss_ce: 0.008963
2022-01-10 00:45:17,730 iteration 3539 : loss : 0.019005, loss_ce: 0.008795
2022-01-10 00:45:19,196 iteration 3540 : loss : 0.025812, loss_ce: 0.007649
2022-01-10 00:45:20,843 iteration 3541 : loss : 0.037267, loss_ce: 0.015682
2022-01-10 00:45:22,416 iteration 3542 : loss : 0.032154, loss_ce: 0.013276
2022-01-10 00:45:24,104 iteration 3543 : loss : 0.046950, loss_ce: 0.015720
2022-01-10 00:45:25,610 iteration 3544 : loss : 0.017577, loss_ce: 0.007166
2022-01-10 00:45:27,175 iteration 3545 : loss : 0.025202, loss_ce: 0.011566
2022-01-10 00:45:28,681 iteration 3546 : loss : 0.022466, loss_ce: 0.008526
2022-01-10 00:45:30,255 iteration 3547 : loss : 0.028056, loss_ce: 0.011753
2022-01-10 00:45:31,775 iteration 3548 : loss : 0.029421, loss_ce: 0.009736
2022-01-10 00:45:33,354 iteration 3549 : loss : 0.025623, loss_ce: 0.009757
2022-01-10 00:45:34,966 iteration 3550 : loss : 0.024718, loss_ce: 0.011452
2022-01-10 00:45:36,599 iteration 3551 : loss : 0.056559, loss_ce: 0.016869
2022-01-10 00:45:38,234 iteration 3552 : loss : 0.046692, loss_ce: 0.013392
2022-01-10 00:45:39,875 iteration 3553 : loss : 0.040319, loss_ce: 0.017301
 52%|██████████████             | 209/400 [1:42:14<1:28:50, 27.91s/it]2022-01-10 00:45:41,523 iteration 3554 : loss : 0.035438, loss_ce: 0.016256
2022-01-10 00:45:43,076 iteration 3555 : loss : 0.023264, loss_ce: 0.007387
2022-01-10 00:45:44,629 iteration 3556 : loss : 0.026238, loss_ce: 0.012358
2022-01-10 00:45:46,288 iteration 3557 : loss : 0.036013, loss_ce: 0.010072
2022-01-10 00:45:47,815 iteration 3558 : loss : 0.040817, loss_ce: 0.011001
2022-01-10 00:45:49,372 iteration 3559 : loss : 0.027759, loss_ce: 0.008496
2022-01-10 00:45:50,841 iteration 3560 : loss : 0.024427, loss_ce: 0.011479
2022-01-10 00:45:52,489 iteration 3561 : loss : 0.036815, loss_ce: 0.015869
2022-01-10 00:45:54,028 iteration 3562 : loss : 0.028940, loss_ce: 0.010779
2022-01-10 00:45:55,633 iteration 3563 : loss : 0.026711, loss_ce: 0.008911
2022-01-10 00:45:57,214 iteration 3564 : loss : 0.044849, loss_ce: 0.015272
2022-01-10 00:45:58,900 iteration 3565 : loss : 0.052030, loss_ce: 0.016695
2022-01-10 00:46:00,490 iteration 3566 : loss : 0.034087, loss_ce: 0.011858
2022-01-10 00:46:02,151 iteration 3567 : loss : 0.043195, loss_ce: 0.019914
2022-01-10 00:46:03,791 iteration 3568 : loss : 0.032258, loss_ce: 0.014856
2022-01-10 00:46:05,425 iteration 3569 : loss : 0.050336, loss_ce: 0.011547
2022-01-10 00:46:05,425 Training Data Eval:
2022-01-10 00:46:13,475   Average segmentation loss on training set: 0.0209
2022-01-10 00:46:13,476 Validation Data Eval:
2022-01-10 00:46:16,251   Average segmentation loss on validation set: 0.1076
2022-01-10 00:46:17,844 iteration 3570 : loss : 0.040681, loss_ce: 0.015199
 52%|██████████████▏            | 210/400 [1:42:52<1:37:55, 30.93s/it]2022-01-10 00:46:19,471 iteration 3571 : loss : 0.023595, loss_ce: 0.010137
2022-01-10 00:46:21,053 iteration 3572 : loss : 0.033354, loss_ce: 0.009717
2022-01-10 00:46:22,629 iteration 3573 : loss : 0.045563, loss_ce: 0.020326
2022-01-10 00:46:24,175 iteration 3574 : loss : 0.022251, loss_ce: 0.009328
2022-01-10 00:46:25,728 iteration 3575 : loss : 0.030346, loss_ce: 0.012994
2022-01-10 00:46:27,316 iteration 3576 : loss : 0.038414, loss_ce: 0.014558
2022-01-10 00:46:28,943 iteration 3577 : loss : 0.025377, loss_ce: 0.011156
2022-01-10 00:46:30,569 iteration 3578 : loss : 0.031963, loss_ce: 0.012643
2022-01-10 00:46:32,122 iteration 3579 : loss : 0.028173, loss_ce: 0.010299
2022-01-10 00:46:33,728 iteration 3580 : loss : 0.032739, loss_ce: 0.010994
2022-01-10 00:46:35,269 iteration 3581 : loss : 0.034286, loss_ce: 0.007248
2022-01-10 00:46:36,832 iteration 3582 : loss : 0.025145, loss_ce: 0.010116
2022-01-10 00:46:38,441 iteration 3583 : loss : 0.044779, loss_ce: 0.018348
2022-01-10 00:46:39,984 iteration 3584 : loss : 0.027991, loss_ce: 0.011183
2022-01-10 00:46:41,547 iteration 3585 : loss : 0.023456, loss_ce: 0.009242
2022-01-10 00:46:43,063 iteration 3586 : loss : 0.027067, loss_ce: 0.011198
2022-01-10 00:46:44,624 iteration 3587 : loss : 0.031745, loss_ce: 0.015206
 53%|██████████████▏            | 211/400 [1:43:19<1:33:30, 29.68s/it]2022-01-10 00:46:46,173 iteration 3588 : loss : 0.024371, loss_ce: 0.009000
2022-01-10 00:46:47,783 iteration 3589 : loss : 0.039771, loss_ce: 0.015315
2022-01-10 00:46:49,354 iteration 3590 : loss : 0.037725, loss_ce: 0.015804
2022-01-10 00:46:50,912 iteration 3591 : loss : 0.031355, loss_ce: 0.011151
2022-01-10 00:46:52,540 iteration 3592 : loss : 0.024112, loss_ce: 0.009780
2022-01-10 00:46:54,132 iteration 3593 : loss : 0.034299, loss_ce: 0.009855
2022-01-10 00:46:55,721 iteration 3594 : loss : 0.027298, loss_ce: 0.008441
2022-01-10 00:46:57,426 iteration 3595 : loss : 0.041460, loss_ce: 0.008491
2022-01-10 00:46:58,935 iteration 3596 : loss : 0.027405, loss_ce: 0.011697
2022-01-10 00:47:00,589 iteration 3597 : loss : 0.044078, loss_ce: 0.020414
2022-01-10 00:47:02,165 iteration 3598 : loss : 0.032770, loss_ce: 0.016070
2022-01-10 00:47:03,754 iteration 3599 : loss : 0.044291, loss_ce: 0.019656
2022-01-10 00:47:05,316 iteration 3600 : loss : 0.023664, loss_ce: 0.011379
2022-01-10 00:47:06,891 iteration 3601 : loss : 0.033180, loss_ce: 0.014653
2022-01-10 00:47:08,442 iteration 3602 : loss : 0.026401, loss_ce: 0.011879
2022-01-10 00:47:09,972 iteration 3603 : loss : 0.018937, loss_ce: 0.007529
2022-01-10 00:47:11,494 iteration 3604 : loss : 0.025357, loss_ce: 0.010404
 53%|██████████████▎            | 212/400 [1:43:46<1:30:22, 28.84s/it]2022-01-10 00:47:13,111 iteration 3605 : loss : 0.021297, loss_ce: 0.008413
2022-01-10 00:47:14,683 iteration 3606 : loss : 0.031857, loss_ce: 0.012599
2022-01-10 00:47:16,193 iteration 3607 : loss : 0.025880, loss_ce: 0.009833
2022-01-10 00:47:17,705 iteration 3608 : loss : 0.028738, loss_ce: 0.013866
2022-01-10 00:47:19,299 iteration 3609 : loss : 0.034580, loss_ce: 0.012174
2022-01-10 00:47:20,834 iteration 3610 : loss : 0.028770, loss_ce: 0.012859
2022-01-10 00:47:22,438 iteration 3611 : loss : 0.041303, loss_ce: 0.017700
2022-01-10 00:47:24,063 iteration 3612 : loss : 0.031396, loss_ce: 0.012037
2022-01-10 00:47:25,643 iteration 3613 : loss : 0.025694, loss_ce: 0.010306
2022-01-10 00:47:27,261 iteration 3614 : loss : 0.023480, loss_ce: 0.008161
2022-01-10 00:47:28,806 iteration 3615 : loss : 0.041714, loss_ce: 0.013146
2022-01-10 00:47:30,449 iteration 3616 : loss : 0.040823, loss_ce: 0.014160
2022-01-10 00:47:32,003 iteration 3617 : loss : 0.033897, loss_ce: 0.013047
2022-01-10 00:47:33,568 iteration 3618 : loss : 0.034949, loss_ce: 0.017533
2022-01-10 00:47:35,130 iteration 3619 : loss : 0.022914, loss_ce: 0.009763
2022-01-10 00:47:36,731 iteration 3620 : loss : 0.028675, loss_ce: 0.011763
2022-01-10 00:47:38,292 iteration 3621 : loss : 0.031753, loss_ce: 0.014531
 53%|██████████████▍            | 213/400 [1:44:13<1:27:58, 28.23s/it]2022-01-10 00:47:39,984 iteration 3622 : loss : 0.033815, loss_ce: 0.013961
2022-01-10 00:47:41,561 iteration 3623 : loss : 0.032457, loss_ce: 0.014717
2022-01-10 00:47:43,210 iteration 3624 : loss : 0.025862, loss_ce: 0.008934
2022-01-10 00:47:44,852 iteration 3625 : loss : 0.048537, loss_ce: 0.021071
2022-01-10 00:47:46,425 iteration 3626 : loss : 0.026657, loss_ce: 0.010243
2022-01-10 00:47:48,037 iteration 3627 : loss : 0.028258, loss_ce: 0.014115
2022-01-10 00:47:49,631 iteration 3628 : loss : 0.025434, loss_ce: 0.007988
2022-01-10 00:47:51,286 iteration 3629 : loss : 0.027060, loss_ce: 0.011444
2022-01-10 00:47:52,865 iteration 3630 : loss : 0.029351, loss_ce: 0.012379
2022-01-10 00:47:54,377 iteration 3631 : loss : 0.030233, loss_ce: 0.013091
2022-01-10 00:47:55,891 iteration 3632 : loss : 0.017937, loss_ce: 0.008086
2022-01-10 00:47:57,519 iteration 3633 : loss : 0.042488, loss_ce: 0.015516
2022-01-10 00:47:59,087 iteration 3634 : loss : 0.025870, loss_ce: 0.009813
2022-01-10 00:48:00,615 iteration 3635 : loss : 0.021484, loss_ce: 0.007306
2022-01-10 00:48:02,144 iteration 3636 : loss : 0.028312, loss_ce: 0.012075
2022-01-10 00:48:03,772 iteration 3637 : loss : 0.040106, loss_ce: 0.012452
2022-01-10 00:48:05,290 iteration 3638 : loss : 0.020511, loss_ce: 0.007021
 54%|██████████████▍            | 214/400 [1:44:40<1:26:21, 27.86s/it]2022-01-10 00:48:06,898 iteration 3639 : loss : 0.046532, loss_ce: 0.025415
2022-01-10 00:48:08,511 iteration 3640 : loss : 0.047804, loss_ce: 0.020347
2022-01-10 00:48:10,091 iteration 3641 : loss : 0.031622, loss_ce: 0.010991
2022-01-10 00:48:11,708 iteration 3642 : loss : 0.034988, loss_ce: 0.014112
2022-01-10 00:48:13,279 iteration 3643 : loss : 0.028862, loss_ce: 0.012727
2022-01-10 00:48:14,838 iteration 3644 : loss : 0.025451, loss_ce: 0.013210
2022-01-10 00:48:16,373 iteration 3645 : loss : 0.024780, loss_ce: 0.010274
2022-01-10 00:48:17,959 iteration 3646 : loss : 0.027759, loss_ce: 0.010912
2022-01-10 00:48:19,577 iteration 3647 : loss : 0.041021, loss_ce: 0.016340
2022-01-10 00:48:21,260 iteration 3648 : loss : 0.039576, loss_ce: 0.016481
2022-01-10 00:48:22,855 iteration 3649 : loss : 0.065741, loss_ce: 0.020260
2022-01-10 00:48:24,435 iteration 3650 : loss : 0.031305, loss_ce: 0.009594
2022-01-10 00:48:26,034 iteration 3651 : loss : 0.021459, loss_ce: 0.008123
2022-01-10 00:48:27,581 iteration 3652 : loss : 0.026134, loss_ce: 0.011210
2022-01-10 00:48:29,217 iteration 3653 : loss : 0.038118, loss_ce: 0.008461
2022-01-10 00:48:30,801 iteration 3654 : loss : 0.035615, loss_ce: 0.011077
2022-01-10 00:48:30,801 Training Data Eval:
2022-01-10 00:48:38,863   Average segmentation loss on training set: 0.0203
2022-01-10 00:48:38,863 Validation Data Eval:
2022-01-10 00:48:41,633   Average segmentation loss on validation set: 0.0992
2022-01-10 00:48:43,111 iteration 3655 : loss : 0.035266, loss_ce: 0.014141
 54%|██████████████▌            | 215/400 [1:45:18<1:35:06, 30.85s/it]2022-01-10 00:48:44,794 iteration 3656 : loss : 0.065848, loss_ce: 0.010877
2022-01-10 00:48:46,338 iteration 3657 : loss : 0.032849, loss_ce: 0.015907
2022-01-10 00:48:47,913 iteration 3658 : loss : 0.030256, loss_ce: 0.012810
2022-01-10 00:48:49,527 iteration 3659 : loss : 0.039515, loss_ce: 0.010869
2022-01-10 00:48:51,198 iteration 3660 : loss : 0.049392, loss_ce: 0.016489
2022-01-10 00:48:52,883 iteration 3661 : loss : 0.029510, loss_ce: 0.012755
2022-01-10 00:48:54,385 iteration 3662 : loss : 0.024317, loss_ce: 0.008621
2022-01-10 00:48:55,926 iteration 3663 : loss : 0.019697, loss_ce: 0.006430
2022-01-10 00:48:57,481 iteration 3664 : loss : 0.029065, loss_ce: 0.009051
2022-01-10 00:48:59,045 iteration 3665 : loss : 0.022533, loss_ce: 0.010565
2022-01-10 00:49:00,726 iteration 3666 : loss : 0.042552, loss_ce: 0.019794
2022-01-10 00:49:02,346 iteration 3667 : loss : 0.057797, loss_ce: 0.023108
2022-01-10 00:49:03,974 iteration 3668 : loss : 0.033749, loss_ce: 0.014777
2022-01-10 00:49:05,567 iteration 3669 : loss : 0.033513, loss_ce: 0.015760
2022-01-10 00:49:07,124 iteration 3670 : loss : 0.048307, loss_ce: 0.016076
2022-01-10 00:49:08,744 iteration 3671 : loss : 0.032630, loss_ce: 0.013208
2022-01-10 00:49:10,274 iteration 3672 : loss : 0.027589, loss_ce: 0.011450
 54%|██████████████▌            | 216/400 [1:45:45<1:31:12, 29.74s/it]2022-01-10 00:49:11,891 iteration 3673 : loss : 0.027278, loss_ce: 0.013479
2022-01-10 00:49:13,473 iteration 3674 : loss : 0.029185, loss_ce: 0.012285
2022-01-10 00:49:15,032 iteration 3675 : loss : 0.033253, loss_ce: 0.017378
2022-01-10 00:49:16,590 iteration 3676 : loss : 0.022519, loss_ce: 0.007521
2022-01-10 00:49:18,150 iteration 3677 : loss : 0.037530, loss_ce: 0.018838
2022-01-10 00:49:19,798 iteration 3678 : loss : 0.035153, loss_ce: 0.011598
2022-01-10 00:49:21,361 iteration 3679 : loss : 0.027947, loss_ce: 0.010952
2022-01-10 00:49:22,910 iteration 3680 : loss : 0.052108, loss_ce: 0.020776
2022-01-10 00:49:24,534 iteration 3681 : loss : 0.022266, loss_ce: 0.007575
2022-01-10 00:49:26,092 iteration 3682 : loss : 0.030263, loss_ce: 0.012517
2022-01-10 00:49:27,712 iteration 3683 : loss : 0.049514, loss_ce: 0.005589
2022-01-10 00:49:29,260 iteration 3684 : loss : 0.015841, loss_ce: 0.005105
2022-01-10 00:49:30,816 iteration 3685 : loss : 0.030170, loss_ce: 0.012426
2022-01-10 00:49:32,411 iteration 3686 : loss : 0.033087, loss_ce: 0.012771
2022-01-10 00:49:33,992 iteration 3687 : loss : 0.023551, loss_ce: 0.007390
2022-01-10 00:49:35,557 iteration 3688 : loss : 0.036537, loss_ce: 0.018147
2022-01-10 00:49:37,067 iteration 3689 : loss : 0.025266, loss_ce: 0.008669
 54%|██████████████▋            | 217/400 [1:46:12<1:28:01, 28.86s/it]2022-01-10 00:49:38,660 iteration 3690 : loss : 0.020311, loss_ce: 0.005319
2022-01-10 00:49:40,305 iteration 3691 : loss : 0.068139, loss_ce: 0.016279
2022-01-10 00:49:41,887 iteration 3692 : loss : 0.029484, loss_ce: 0.014988
2022-01-10 00:49:43,534 iteration 3693 : loss : 0.041130, loss_ce: 0.016741
2022-01-10 00:49:45,118 iteration 3694 : loss : 0.021922, loss_ce: 0.008937
2022-01-10 00:49:46,649 iteration 3695 : loss : 0.023219, loss_ce: 0.009645
2022-01-10 00:49:48,265 iteration 3696 : loss : 0.030003, loss_ce: 0.016176
2022-01-10 00:49:49,827 iteration 3697 : loss : 0.027936, loss_ce: 0.008249
2022-01-10 00:49:51,403 iteration 3698 : loss : 0.027638, loss_ce: 0.010133
2022-01-10 00:49:53,045 iteration 3699 : loss : 0.041185, loss_ce: 0.017645
2022-01-10 00:49:54,626 iteration 3700 : loss : 0.028455, loss_ce: 0.012434
2022-01-10 00:49:56,181 iteration 3701 : loss : 0.027587, loss_ce: 0.012131
2022-01-10 00:49:57,723 iteration 3702 : loss : 0.037767, loss_ce: 0.015261
2022-01-10 00:49:59,402 iteration 3703 : loss : 0.038694, loss_ce: 0.016639
2022-01-10 00:50:00,944 iteration 3704 : loss : 0.024056, loss_ce: 0.007630
2022-01-10 00:50:02,478 iteration 3705 : loss : 0.021726, loss_ce: 0.007671
2022-01-10 00:50:04,000 iteration 3706 : loss : 0.027074, loss_ce: 0.007761
 55%|██████████████▋            | 218/400 [1:46:39<1:25:46, 28.28s/it]2022-01-10 00:50:05,624 iteration 3707 : loss : 0.037158, loss_ce: 0.012058
2022-01-10 00:50:07,204 iteration 3708 : loss : 0.026147, loss_ce: 0.011259
2022-01-10 00:50:08,782 iteration 3709 : loss : 0.025207, loss_ce: 0.008110
2022-01-10 00:50:10,402 iteration 3710 : loss : 0.039737, loss_ce: 0.014477
2022-01-10 00:50:11,956 iteration 3711 : loss : 0.021015, loss_ce: 0.007638
2022-01-10 00:50:13,555 iteration 3712 : loss : 0.031998, loss_ce: 0.010806
2022-01-10 00:50:15,086 iteration 3713 : loss : 0.029030, loss_ce: 0.010888
2022-01-10 00:50:16,628 iteration 3714 : loss : 0.020861, loss_ce: 0.007147
2022-01-10 00:50:18,212 iteration 3715 : loss : 0.030700, loss_ce: 0.012695
2022-01-10 00:50:19,749 iteration 3716 : loss : 0.052178, loss_ce: 0.017877
2022-01-10 00:50:21,337 iteration 3717 : loss : 0.038570, loss_ce: 0.017197
2022-01-10 00:50:22,964 iteration 3718 : loss : 0.029391, loss_ce: 0.014073
2022-01-10 00:50:24,487 iteration 3719 : loss : 0.021533, loss_ce: 0.005686
2022-01-10 00:50:26,078 iteration 3720 : loss : 0.022735, loss_ce: 0.011335
2022-01-10 00:50:27,659 iteration 3721 : loss : 0.024373, loss_ce: 0.010267
2022-01-10 00:50:29,220 iteration 3722 : loss : 0.032538, loss_ce: 0.014253
2022-01-10 00:50:30,760 iteration 3723 : loss : 0.041067, loss_ce: 0.013280
 55%|██████████████▊            | 219/400 [1:47:05<1:23:56, 27.82s/it]2022-01-10 00:50:32,323 iteration 3724 : loss : 0.025834, loss_ce: 0.013207
2022-01-10 00:50:33,854 iteration 3725 : loss : 0.032090, loss_ce: 0.015571
2022-01-10 00:50:35,484 iteration 3726 : loss : 0.033810, loss_ce: 0.015065
2022-01-10 00:50:37,068 iteration 3727 : loss : 0.027244, loss_ce: 0.011113
2022-01-10 00:50:38,662 iteration 3728 : loss : 0.043853, loss_ce: 0.012799
2022-01-10 00:50:40,194 iteration 3729 : loss : 0.024913, loss_ce: 0.007460
2022-01-10 00:50:41,754 iteration 3730 : loss : 0.023542, loss_ce: 0.009410
2022-01-10 00:50:43,306 iteration 3731 : loss : 0.023810, loss_ce: 0.008297
2022-01-10 00:50:44,908 iteration 3732 : loss : 0.032061, loss_ce: 0.011033
2022-01-10 00:50:46,437 iteration 3733 : loss : 0.031928, loss_ce: 0.013704
2022-01-10 00:50:48,024 iteration 3734 : loss : 0.029646, loss_ce: 0.011179
2022-01-10 00:50:49,604 iteration 3735 : loss : 0.025267, loss_ce: 0.007113
2022-01-10 00:50:51,231 iteration 3736 : loss : 0.028696, loss_ce: 0.012784
2022-01-10 00:50:52,768 iteration 3737 : loss : 0.024535, loss_ce: 0.008859
2022-01-10 00:50:54,336 iteration 3738 : loss : 0.026579, loss_ce: 0.010215
2022-01-10 00:50:55,862 iteration 3739 : loss : 0.030720, loss_ce: 0.013952
2022-01-10 00:50:55,862 Training Data Eval:
2022-01-10 00:51:03,934   Average segmentation loss on training set: 0.0171
2022-01-10 00:51:03,934 Validation Data Eval:
2022-01-10 00:51:06,717   Average segmentation loss on validation set: 0.0679
2022-01-10 00:51:08,435 iteration 3740 : loss : 0.043700, loss_ce: 0.024365
 55%|██████████████▊            | 220/400 [1:47:43<1:32:19, 30.78s/it]2022-01-10 00:51:10,184 iteration 3741 : loss : 0.060938, loss_ce: 0.018285
2022-01-10 00:51:11,750 iteration 3742 : loss : 0.027035, loss_ce: 0.013004
2022-01-10 00:51:13,320 iteration 3743 : loss : 0.030994, loss_ce: 0.012355
2022-01-10 00:51:14,939 iteration 3744 : loss : 0.027159, loss_ce: 0.010085
2022-01-10 00:51:16,580 iteration 3745 : loss : 0.031141, loss_ce: 0.009836
2022-01-10 00:51:18,224 iteration 3746 : loss : 0.027770, loss_ce: 0.011110
2022-01-10 00:51:19,793 iteration 3747 : loss : 0.026182, loss_ce: 0.009423
2022-01-10 00:51:21,348 iteration 3748 : loss : 0.022262, loss_ce: 0.011015
2022-01-10 00:51:22,971 iteration 3749 : loss : 0.027548, loss_ce: 0.009573
2022-01-10 00:51:24,622 iteration 3750 : loss : 0.036389, loss_ce: 0.014380
2022-01-10 00:51:26,226 iteration 3751 : loss : 0.036762, loss_ce: 0.013089
2022-01-10 00:51:27,787 iteration 3752 : loss : 0.035173, loss_ce: 0.012043
2022-01-10 00:51:29,318 iteration 3753 : loss : 0.028980, loss_ce: 0.008351
2022-01-10 00:51:30,898 iteration 3754 : loss : 0.023198, loss_ce: 0.008176
2022-01-10 00:51:32,522 iteration 3755 : loss : 0.028624, loss_ce: 0.013612
2022-01-10 00:51:34,029 iteration 3756 : loss : 0.026842, loss_ce: 0.011541
2022-01-10 00:51:35,553 iteration 3757 : loss : 0.031768, loss_ce: 0.015118
 55%|██████████████▉            | 221/400 [1:48:10<1:28:33, 29.68s/it]2022-01-10 00:51:37,262 iteration 3758 : loss : 0.024654, loss_ce: 0.008015
2022-01-10 00:51:38,797 iteration 3759 : loss : 0.027168, loss_ce: 0.008662
2022-01-10 00:51:40,354 iteration 3760 : loss : 0.024366, loss_ce: 0.009569
2022-01-10 00:51:41,970 iteration 3761 : loss : 0.026444, loss_ce: 0.010863
2022-01-10 00:51:43,606 iteration 3762 : loss : 0.023069, loss_ce: 0.011330
2022-01-10 00:51:45,278 iteration 3763 : loss : 0.036133, loss_ce: 0.013688
2022-01-10 00:51:46,849 iteration 3764 : loss : 0.032849, loss_ce: 0.011816
2022-01-10 00:51:48,377 iteration 3765 : loss : 0.023089, loss_ce: 0.009734
2022-01-10 00:51:50,004 iteration 3766 : loss : 0.038237, loss_ce: 0.010860
2022-01-10 00:51:51,577 iteration 3767 : loss : 0.017785, loss_ce: 0.006048
2022-01-10 00:51:53,114 iteration 3768 : loss : 0.035014, loss_ce: 0.010277
2022-01-10 00:51:54,657 iteration 3769 : loss : 0.023855, loss_ce: 0.010923
2022-01-10 00:51:56,260 iteration 3770 : loss : 0.030356, loss_ce: 0.007380
2022-01-10 00:51:57,956 iteration 3771 : loss : 0.035383, loss_ce: 0.014215
2022-01-10 00:51:59,595 iteration 3772 : loss : 0.034108, loss_ce: 0.008959
2022-01-10 00:52:01,198 iteration 3773 : loss : 0.027151, loss_ce: 0.011613
2022-01-10 00:52:02,847 iteration 3774 : loss : 0.055531, loss_ce: 0.027443
 56%|██████████████▉            | 222/400 [1:48:37<1:25:55, 28.96s/it]2022-01-10 00:52:04,463 iteration 3775 : loss : 0.026332, loss_ce: 0.011034
2022-01-10 00:52:05,965 iteration 3776 : loss : 0.023859, loss_ce: 0.010637
2022-01-10 00:52:07,522 iteration 3777 : loss : 0.022922, loss_ce: 0.006674
2022-01-10 00:52:09,059 iteration 3778 : loss : 0.022445, loss_ce: 0.009145
2022-01-10 00:52:10,597 iteration 3779 : loss : 0.019103, loss_ce: 0.009017
2022-01-10 00:52:12,185 iteration 3780 : loss : 0.046363, loss_ce: 0.017862
2022-01-10 00:52:13,768 iteration 3781 : loss : 0.036556, loss_ce: 0.016066
2022-01-10 00:52:15,365 iteration 3782 : loss : 0.082471, loss_ce: 0.016459
2022-01-10 00:52:16,961 iteration 3783 : loss : 0.018302, loss_ce: 0.008487
2022-01-10 00:52:18,566 iteration 3784 : loss : 0.025047, loss_ce: 0.011394
2022-01-10 00:52:20,113 iteration 3785 : loss : 0.027038, loss_ce: 0.010497
2022-01-10 00:52:21,657 iteration 3786 : loss : 0.019089, loss_ce: 0.007066
2022-01-10 00:52:23,190 iteration 3787 : loss : 0.028654, loss_ce: 0.010801
2022-01-10 00:52:24,826 iteration 3788 : loss : 0.032913, loss_ce: 0.010460
2022-01-10 00:52:26,398 iteration 3789 : loss : 0.025760, loss_ce: 0.007876
2022-01-10 00:52:28,006 iteration 3790 : loss : 0.035058, loss_ce: 0.014002
2022-01-10 00:52:29,608 iteration 3791 : loss : 0.027763, loss_ce: 0.011630
 56%|███████████████            | 223/400 [1:49:04<1:23:29, 28.30s/it]2022-01-10 00:52:31,207 iteration 3792 : loss : 0.019535, loss_ce: 0.007132
2022-01-10 00:52:32,829 iteration 3793 : loss : 0.032590, loss_ce: 0.011748
2022-01-10 00:52:34,431 iteration 3794 : loss : 0.035864, loss_ce: 0.015680
2022-01-10 00:52:36,010 iteration 3795 : loss : 0.021308, loss_ce: 0.007274
2022-01-10 00:52:37,495 iteration 3796 : loss : 0.021397, loss_ce: 0.007566
2022-01-10 00:52:39,045 iteration 3797 : loss : 0.023442, loss_ce: 0.008662
2022-01-10 00:52:40,553 iteration 3798 : loss : 0.022066, loss_ce: 0.007868
2022-01-10 00:52:42,105 iteration 3799 : loss : 0.025883, loss_ce: 0.011132
2022-01-10 00:52:43,660 iteration 3800 : loss : 0.021764, loss_ce: 0.006677
2022-01-10 00:52:45,217 iteration 3801 : loss : 0.047246, loss_ce: 0.011476
2022-01-10 00:52:46,757 iteration 3802 : loss : 0.035771, loss_ce: 0.018704
2022-01-10 00:52:48,321 iteration 3803 : loss : 0.020789, loss_ce: 0.006713
2022-01-10 00:52:49,887 iteration 3804 : loss : 0.030047, loss_ce: 0.010578
2022-01-10 00:52:51,426 iteration 3805 : loss : 0.027394, loss_ce: 0.011080
2022-01-10 00:52:52,965 iteration 3806 : loss : 0.031042, loss_ce: 0.015441
2022-01-10 00:52:54,567 iteration 3807 : loss : 0.026589, loss_ce: 0.010491
2022-01-10 00:52:56,145 iteration 3808 : loss : 0.023011, loss_ce: 0.009239
 56%|███████████████            | 224/400 [1:49:31<1:21:28, 27.77s/it]2022-01-10 00:52:57,713 iteration 3809 : loss : 0.018550, loss_ce: 0.006513
2022-01-10 00:52:59,261 iteration 3810 : loss : 0.025743, loss_ce: 0.009954
2022-01-10 00:53:00,825 iteration 3811 : loss : 0.037092, loss_ce: 0.012167
2022-01-10 00:53:02,407 iteration 3812 : loss : 0.026320, loss_ce: 0.013488
2022-01-10 00:53:03,964 iteration 3813 : loss : 0.023822, loss_ce: 0.011308
2022-01-10 00:53:05,561 iteration 3814 : loss : 0.030829, loss_ce: 0.011664
2022-01-10 00:53:07,133 iteration 3815 : loss : 0.041143, loss_ce: 0.012463
2022-01-10 00:53:08,720 iteration 3816 : loss : 0.027636, loss_ce: 0.011423
2022-01-10 00:53:10,249 iteration 3817 : loss : 0.026056, loss_ce: 0.010130
2022-01-10 00:53:11,852 iteration 3818 : loss : 0.021431, loss_ce: 0.009600
2022-01-10 00:53:13,473 iteration 3819 : loss : 0.024577, loss_ce: 0.012492
2022-01-10 00:53:15,072 iteration 3820 : loss : 0.030755, loss_ce: 0.011222
2022-01-10 00:53:16,679 iteration 3821 : loss : 0.029595, loss_ce: 0.011895
2022-01-10 00:53:18,220 iteration 3822 : loss : 0.033360, loss_ce: 0.012974
2022-01-10 00:53:19,875 iteration 3823 : loss : 0.024122, loss_ce: 0.008179
2022-01-10 00:53:21,461 iteration 3824 : loss : 0.025658, loss_ce: 0.009204
2022-01-10 00:53:21,461 Training Data Eval:
2022-01-10 00:53:29,525   Average segmentation loss on training set: 0.0223
2022-01-10 00:53:29,526 Validation Data Eval:
2022-01-10 00:53:32,308   Average segmentation loss on validation set: 0.0842
2022-01-10 00:53:33,949 iteration 3825 : loss : 0.031535, loss_ce: 0.011635
 56%|███████████████▏           | 225/400 [1:50:09<1:29:46, 30.78s/it]2022-01-10 00:53:35,598 iteration 3826 : loss : 0.025668, loss_ce: 0.011843
2022-01-10 00:53:37,127 iteration 3827 : loss : 0.024063, loss_ce: 0.009243
2022-01-10 00:53:38,715 iteration 3828 : loss : 0.023048, loss_ce: 0.007930
2022-01-10 00:53:40,294 iteration 3829 : loss : 0.027828, loss_ce: 0.008582
2022-01-10 00:53:41,836 iteration 3830 : loss : 0.020978, loss_ce: 0.008996
2022-01-10 00:53:43,361 iteration 3831 : loss : 0.027091, loss_ce: 0.009233
2022-01-10 00:53:44,884 iteration 3832 : loss : 0.028697, loss_ce: 0.007719
2022-01-10 00:53:46,455 iteration 3833 : loss : 0.027730, loss_ce: 0.013149
2022-01-10 00:53:48,061 iteration 3834 : loss : 0.034309, loss_ce: 0.019168
2022-01-10 00:53:49,650 iteration 3835 : loss : 0.032479, loss_ce: 0.012097
2022-01-10 00:53:51,228 iteration 3836 : loss : 0.029234, loss_ce: 0.013266
2022-01-10 00:53:52,827 iteration 3837 : loss : 0.029643, loss_ce: 0.011056
2022-01-10 00:53:54,491 iteration 3838 : loss : 0.031378, loss_ce: 0.011261
2022-01-10 00:53:56,064 iteration 3839 : loss : 0.025694, loss_ce: 0.011346
2022-01-10 00:53:57,651 iteration 3840 : loss : 0.038192, loss_ce: 0.010742
2022-01-10 00:53:59,195 iteration 3841 : loss : 0.022386, loss_ce: 0.008597
2022-01-10 00:54:00,791 iteration 3842 : loss : 0.036698, loss_ce: 0.018418
 56%|███████████████▎           | 226/400 [1:50:35<1:25:50, 29.60s/it]2022-01-10 00:54:02,390 iteration 3843 : loss : 0.022911, loss_ce: 0.009401
2022-01-10 00:54:04,004 iteration 3844 : loss : 0.033044, loss_ce: 0.012277
2022-01-10 00:54:05,610 iteration 3845 : loss : 0.027984, loss_ce: 0.014055
2022-01-10 00:54:07,151 iteration 3846 : loss : 0.019623, loss_ce: 0.007260
2022-01-10 00:54:08,710 iteration 3847 : loss : 0.025841, loss_ce: 0.010117
2022-01-10 00:54:10,333 iteration 3848 : loss : 0.023409, loss_ce: 0.011309
2022-01-10 00:54:12,008 iteration 3849 : loss : 0.027032, loss_ce: 0.011796
2022-01-10 00:54:13,665 iteration 3850 : loss : 0.081083, loss_ce: 0.024552
2022-01-10 00:54:15,251 iteration 3851 : loss : 0.022176, loss_ce: 0.010256
2022-01-10 00:54:16,850 iteration 3852 : loss : 0.025477, loss_ce: 0.006783
2022-01-10 00:54:18,383 iteration 3853 : loss : 0.024202, loss_ce: 0.006865
2022-01-10 00:54:19,986 iteration 3854 : loss : 0.025640, loss_ce: 0.007708
2022-01-10 00:54:21,700 iteration 3855 : loss : 0.039146, loss_ce: 0.017841
2022-01-10 00:54:23,159 iteration 3856 : loss : 0.024419, loss_ce: 0.011049
2022-01-10 00:54:24,715 iteration 3857 : loss : 0.033362, loss_ce: 0.015110
2022-01-10 00:54:26,200 iteration 3858 : loss : 0.021580, loss_ce: 0.008498
2022-01-10 00:54:27,760 iteration 3859 : loss : 0.024230, loss_ce: 0.010385
 57%|███████████████▎           | 227/400 [1:51:02<1:23:04, 28.81s/it]2022-01-10 00:54:29,391 iteration 3860 : loss : 0.021279, loss_ce: 0.008372
2022-01-10 00:54:31,036 iteration 3861 : loss : 0.044905, loss_ce: 0.019075
2022-01-10 00:54:32,638 iteration 3862 : loss : 0.045223, loss_ce: 0.017645
2022-01-10 00:54:34,233 iteration 3863 : loss : 0.026433, loss_ce: 0.009873
2022-01-10 00:54:35,845 iteration 3864 : loss : 0.035477, loss_ce: 0.013214
2022-01-10 00:54:37,416 iteration 3865 : loss : 0.027105, loss_ce: 0.009415
2022-01-10 00:54:39,100 iteration 3866 : loss : 0.023770, loss_ce: 0.008123
2022-01-10 00:54:40,667 iteration 3867 : loss : 0.028709, loss_ce: 0.009361
2022-01-10 00:54:42,296 iteration 3868 : loss : 0.035535, loss_ce: 0.011645
2022-01-10 00:54:43,904 iteration 3869 : loss : 0.021232, loss_ce: 0.010042
2022-01-10 00:54:45,532 iteration 3870 : loss : 0.025720, loss_ce: 0.010105
2022-01-10 00:54:47,139 iteration 3871 : loss : 0.028259, loss_ce: 0.007317
2022-01-10 00:54:48,683 iteration 3872 : loss : 0.023461, loss_ce: 0.009692
2022-01-10 00:54:50,221 iteration 3873 : loss : 0.029637, loss_ce: 0.015713
2022-01-10 00:54:51,819 iteration 3874 : loss : 0.030843, loss_ce: 0.012191
2022-01-10 00:54:53,414 iteration 3875 : loss : 0.030938, loss_ce: 0.012716
2022-01-10 00:54:55,012 iteration 3876 : loss : 0.031759, loss_ce: 0.009918
 57%|███████████████▍           | 228/400 [1:51:30<1:21:15, 28.34s/it]2022-01-10 00:54:56,619 iteration 3877 : loss : 0.024077, loss_ce: 0.009078
2022-01-10 00:54:58,231 iteration 3878 : loss : 0.031075, loss_ce: 0.012057
2022-01-10 00:54:59,780 iteration 3879 : loss : 0.018166, loss_ce: 0.006510
2022-01-10 00:55:01,390 iteration 3880 : loss : 0.025061, loss_ce: 0.010659
2022-01-10 00:55:02,946 iteration 3881 : loss : 0.034940, loss_ce: 0.019226
2022-01-10 00:55:04,606 iteration 3882 : loss : 0.057823, loss_ce: 0.012484
2022-01-10 00:55:06,236 iteration 3883 : loss : 0.040761, loss_ce: 0.013238
2022-01-10 00:55:07,838 iteration 3884 : loss : 0.027889, loss_ce: 0.011263
2022-01-10 00:55:09,460 iteration 3885 : loss : 0.074846, loss_ce: 0.013808
2022-01-10 00:55:11,086 iteration 3886 : loss : 0.036656, loss_ce: 0.013894
2022-01-10 00:55:12,618 iteration 3887 : loss : 0.027176, loss_ce: 0.009799
2022-01-10 00:55:14,233 iteration 3888 : loss : 0.036527, loss_ce: 0.018364
2022-01-10 00:55:15,822 iteration 3889 : loss : 0.027907, loss_ce: 0.010231
2022-01-10 00:55:17,511 iteration 3890 : loss : 0.053745, loss_ce: 0.017803
2022-01-10 00:55:19,069 iteration 3891 : loss : 0.045410, loss_ce: 0.019992
2022-01-10 00:55:20,653 iteration 3892 : loss : 0.032118, loss_ce: 0.011656
2022-01-10 00:55:22,264 iteration 3893 : loss : 0.039282, loss_ce: 0.013210
 57%|███████████████▍           | 229/400 [1:51:57<1:19:50, 28.02s/it]2022-01-10 00:55:23,882 iteration 3894 : loss : 0.021613, loss_ce: 0.006445
2022-01-10 00:55:25,421 iteration 3895 : loss : 0.030375, loss_ce: 0.012100
2022-01-10 00:55:27,002 iteration 3896 : loss : 0.039260, loss_ce: 0.022164
2022-01-10 00:55:28,619 iteration 3897 : loss : 0.033443, loss_ce: 0.013468
2022-01-10 00:55:30,229 iteration 3898 : loss : 0.030885, loss_ce: 0.010357
2022-01-10 00:55:31,743 iteration 3899 : loss : 0.038762, loss_ce: 0.013064
2022-01-10 00:55:33,288 iteration 3900 : loss : 0.037811, loss_ce: 0.013097
2022-01-10 00:55:34,813 iteration 3901 : loss : 0.027247, loss_ce: 0.012137
2022-01-10 00:55:36,346 iteration 3902 : loss : 0.028277, loss_ce: 0.010520
2022-01-10 00:55:37,848 iteration 3903 : loss : 0.026162, loss_ce: 0.009075
2022-01-10 00:55:39,400 iteration 3904 : loss : 0.054220, loss_ce: 0.015883
2022-01-10 00:55:40,957 iteration 3905 : loss : 0.039935, loss_ce: 0.012569
2022-01-10 00:55:42,450 iteration 3906 : loss : 0.019519, loss_ce: 0.008153
2022-01-10 00:55:43,925 iteration 3907 : loss : 0.017532, loss_ce: 0.006346
2022-01-10 00:55:45,424 iteration 3908 : loss : 0.030190, loss_ce: 0.011460
2022-01-10 00:55:46,984 iteration 3909 : loss : 0.038474, loss_ce: 0.022870
2022-01-10 00:55:46,984 Training Data Eval:
2022-01-10 00:55:55,023   Average segmentation loss on training set: 0.0235
2022-01-10 00:55:55,024 Validation Data Eval:
2022-01-10 00:55:57,794   Average segmentation loss on validation set: 0.0878
2022-01-10 00:55:59,426 iteration 3910 : loss : 0.044529, loss_ce: 0.019521
 57%|███████████████▌           | 230/400 [1:52:34<1:27:09, 30.76s/it]2022-01-10 00:56:01,009 iteration 3911 : loss : 0.021649, loss_ce: 0.009382
2022-01-10 00:56:02,606 iteration 3912 : loss : 0.035019, loss_ce: 0.014106
2022-01-10 00:56:04,173 iteration 3913 : loss : 0.021818, loss_ce: 0.008800
2022-01-10 00:56:05,732 iteration 3914 : loss : 0.024243, loss_ce: 0.011255
2022-01-10 00:56:07,290 iteration 3915 : loss : 0.036088, loss_ce: 0.016807
2022-01-10 00:56:08,818 iteration 3916 : loss : 0.025511, loss_ce: 0.009801
2022-01-10 00:56:10,420 iteration 3917 : loss : 0.032917, loss_ce: 0.014226
2022-01-10 00:56:12,020 iteration 3918 : loss : 0.047699, loss_ce: 0.014861
2022-01-10 00:56:13,715 iteration 3919 : loss : 0.050305, loss_ce: 0.015873
2022-01-10 00:56:15,249 iteration 3920 : loss : 0.028500, loss_ce: 0.011013
2022-01-10 00:56:16,814 iteration 3921 : loss : 0.025403, loss_ce: 0.010829
2022-01-10 00:56:18,425 iteration 3922 : loss : 0.025824, loss_ce: 0.009577
2022-01-10 00:56:19,975 iteration 3923 : loss : 0.021397, loss_ce: 0.010045
2022-01-10 00:56:21,496 iteration 3924 : loss : 0.032248, loss_ce: 0.009817
2022-01-10 00:56:23,139 iteration 3925 : loss : 0.039546, loss_ce: 0.016811
2022-01-10 00:56:24,760 iteration 3926 : loss : 0.035349, loss_ce: 0.013969
2022-01-10 00:56:26,327 iteration 3927 : loss : 0.040395, loss_ce: 0.016089
 58%|███████████████▌           | 231/400 [1:53:01<1:23:22, 29.60s/it]2022-01-10 00:56:28,004 iteration 3928 : loss : 0.062084, loss_ce: 0.033506
2022-01-10 00:56:29,579 iteration 3929 : loss : 0.041738, loss_ce: 0.022119
2022-01-10 00:56:31,205 iteration 3930 : loss : 0.035197, loss_ce: 0.015435
2022-01-10 00:56:32,819 iteration 3931 : loss : 0.028976, loss_ce: 0.011265
2022-01-10 00:56:34,434 iteration 3932 : loss : 0.044552, loss_ce: 0.014598
2022-01-10 00:56:36,087 iteration 3933 : loss : 0.034490, loss_ce: 0.013674
2022-01-10 00:56:37,695 iteration 3934 : loss : 0.027958, loss_ce: 0.011808
2022-01-10 00:56:39,202 iteration 3935 : loss : 0.036725, loss_ce: 0.016399
2022-01-10 00:56:40,904 iteration 3936 : loss : 0.054970, loss_ce: 0.024699
2022-01-10 00:56:42,442 iteration 3937 : loss : 0.042003, loss_ce: 0.018272
2022-01-10 00:56:44,066 iteration 3938 : loss : 0.111040, loss_ce: 0.026251
2022-01-10 00:56:45,571 iteration 3939 : loss : 0.040214, loss_ce: 0.014841
2022-01-10 00:56:47,117 iteration 3940 : loss : 0.036843, loss_ce: 0.014962
2022-01-10 00:56:48,687 iteration 3941 : loss : 0.036614, loss_ce: 0.015560
2022-01-10 00:56:50,312 iteration 3942 : loss : 0.059631, loss_ce: 0.023753
2022-01-10 00:56:51,852 iteration 3943 : loss : 0.040496, loss_ce: 0.010811
2022-01-10 00:56:53,485 iteration 3944 : loss : 0.060717, loss_ce: 0.029494
 58%|███████████████▋           | 232/400 [1:53:28<1:20:49, 28.87s/it]2022-01-10 00:56:55,135 iteration 3945 : loss : 0.039874, loss_ce: 0.017743
2022-01-10 00:56:56,621 iteration 3946 : loss : 0.028452, loss_ce: 0.014353
2022-01-10 00:56:58,179 iteration 3947 : loss : 0.035457, loss_ce: 0.009961
2022-01-10 00:56:59,879 iteration 3948 : loss : 0.050011, loss_ce: 0.017105
2022-01-10 00:57:01,496 iteration 3949 : loss : 0.044288, loss_ce: 0.015246
2022-01-10 00:57:03,146 iteration 3950 : loss : 0.032053, loss_ce: 0.010415
2022-01-10 00:57:04,662 iteration 3951 : loss : 0.034763, loss_ce: 0.016214
2022-01-10 00:57:06,279 iteration 3952 : loss : 0.033373, loss_ce: 0.011748
2022-01-10 00:57:07,914 iteration 3953 : loss : 0.070039, loss_ce: 0.029491
2022-01-10 00:57:09,481 iteration 3954 : loss : 0.039253, loss_ce: 0.014650
2022-01-10 00:57:11,040 iteration 3955 : loss : 0.022798, loss_ce: 0.007700
2022-01-10 00:57:12,734 iteration 3956 : loss : 0.040123, loss_ce: 0.018767
2022-01-10 00:57:14,276 iteration 3957 : loss : 0.050706, loss_ce: 0.032694
2022-01-10 00:57:15,827 iteration 3958 : loss : 0.031931, loss_ce: 0.012465
2022-01-10 00:57:17,483 iteration 3959 : loss : 0.040891, loss_ce: 0.016036
2022-01-10 00:57:19,041 iteration 3960 : loss : 0.029147, loss_ce: 0.013299
2022-01-10 00:57:20,615 iteration 3961 : loss : 0.032125, loss_ce: 0.014830
 58%|███████████████▋           | 233/400 [1:53:55<1:18:53, 28.35s/it]2022-01-10 00:57:22,277 iteration 3962 : loss : 0.025780, loss_ce: 0.009074
2022-01-10 00:57:23,856 iteration 3963 : loss : 0.046080, loss_ce: 0.021930
2022-01-10 00:57:25,519 iteration 3964 : loss : 0.029835, loss_ce: 0.012716
2022-01-10 00:57:27,052 iteration 3965 : loss : 0.030334, loss_ce: 0.010200
2022-01-10 00:57:28,672 iteration 3966 : loss : 0.064388, loss_ce: 0.023032
2022-01-10 00:57:30,255 iteration 3967 : loss : 0.032759, loss_ce: 0.012814
2022-01-10 00:57:31,737 iteration 3968 : loss : 0.026817, loss_ce: 0.010238
2022-01-10 00:57:33,322 iteration 3969 : loss : 0.042438, loss_ce: 0.013127
2022-01-10 00:57:34,882 iteration 3970 : loss : 0.027211, loss_ce: 0.009740
2022-01-10 00:57:36,355 iteration 3971 : loss : 0.021327, loss_ce: 0.008576
2022-01-10 00:57:37,952 iteration 3972 : loss : 0.034213, loss_ce: 0.008427
2022-01-10 00:57:39,475 iteration 3973 : loss : 0.037291, loss_ce: 0.019026
2022-01-10 00:57:40,959 iteration 3974 : loss : 0.021765, loss_ce: 0.006900
2022-01-10 00:57:42,541 iteration 3975 : loss : 0.028697, loss_ce: 0.010457
2022-01-10 00:57:44,179 iteration 3976 : loss : 0.043893, loss_ce: 0.016897
2022-01-10 00:57:45,813 iteration 3977 : loss : 0.024483, loss_ce: 0.010427
2022-01-10 00:57:47,469 iteration 3978 : loss : 0.048393, loss_ce: 0.018918
 58%|███████████████▊           | 234/400 [1:54:22<1:17:11, 27.90s/it]2022-01-10 00:57:49,104 iteration 3979 : loss : 0.023801, loss_ce: 0.008243
2022-01-10 00:57:50,719 iteration 3980 : loss : 0.033795, loss_ce: 0.011315
2022-01-10 00:57:52,276 iteration 3981 : loss : 0.028200, loss_ce: 0.012160
2022-01-10 00:57:53,751 iteration 3982 : loss : 0.018882, loss_ce: 0.006783
2022-01-10 00:57:55,244 iteration 3983 : loss : 0.020748, loss_ce: 0.009369
2022-01-10 00:57:56,780 iteration 3984 : loss : 0.018545, loss_ce: 0.007940
2022-01-10 00:57:58,370 iteration 3985 : loss : 0.034080, loss_ce: 0.019893
2022-01-10 00:58:00,028 iteration 3986 : loss : 0.029947, loss_ce: 0.013685
2022-01-10 00:58:01,634 iteration 3987 : loss : 0.031631, loss_ce: 0.012541
2022-01-10 00:58:03,209 iteration 3988 : loss : 0.033021, loss_ce: 0.014499
2022-01-10 00:58:04,758 iteration 3989 : loss : 0.026812, loss_ce: 0.006766
2022-01-10 00:58:06,305 iteration 3990 : loss : 0.042684, loss_ce: 0.013653
2022-01-10 00:58:07,935 iteration 3991 : loss : 0.041279, loss_ce: 0.009405
2022-01-10 00:58:09,526 iteration 3992 : loss : 0.027687, loss_ce: 0.008824
2022-01-10 00:58:11,146 iteration 3993 : loss : 0.048830, loss_ce: 0.018540
2022-01-10 00:58:12,699 iteration 3994 : loss : 0.027787, loss_ce: 0.013001
2022-01-10 00:58:12,699 Training Data Eval:
2022-01-10 00:58:20,727   Average segmentation loss on training set: 0.0202
2022-01-10 00:58:20,727 Validation Data Eval:
2022-01-10 00:58:23,499   Average segmentation loss on validation set: 0.0785
2022-01-10 00:58:25,015 iteration 3995 : loss : 0.026383, loss_ce: 0.011173
 59%|███████████████▊           | 235/400 [1:55:00<1:24:41, 30.79s/it]2022-01-10 00:58:26,701 iteration 3996 : loss : 0.028074, loss_ce: 0.008497
2022-01-10 00:58:28,219 iteration 3997 : loss : 0.031561, loss_ce: 0.010231
2022-01-10 00:58:29,811 iteration 3998 : loss : 0.037957, loss_ce: 0.016393
2022-01-10 00:58:31,368 iteration 3999 : loss : 0.025919, loss_ce: 0.006470
2022-01-10 00:58:32,992 iteration 4000 : loss : 0.028489, loss_ce: 0.015232
2022-01-10 00:58:34,500 iteration 4001 : loss : 0.022366, loss_ce: 0.008510
2022-01-10 00:58:36,074 iteration 4002 : loss : 0.024446, loss_ce: 0.009855
2022-01-10 00:58:37,657 iteration 4003 : loss : 0.031329, loss_ce: 0.014439
2022-01-10 00:58:39,264 iteration 4004 : loss : 0.033375, loss_ce: 0.012044
2022-01-10 00:58:40,841 iteration 4005 : loss : 0.030304, loss_ce: 0.013716
2022-01-10 00:58:42,365 iteration 4006 : loss : 0.024276, loss_ce: 0.009838
2022-01-10 00:58:43,984 iteration 4007 : loss : 0.045911, loss_ce: 0.011953
2022-01-10 00:58:45,521 iteration 4008 : loss : 0.026235, loss_ce: 0.009803
2022-01-10 00:58:47,087 iteration 4009 : loss : 0.032416, loss_ce: 0.016444
2022-01-10 00:58:48,720 iteration 4010 : loss : 0.041910, loss_ce: 0.019236
2022-01-10 00:58:50,247 iteration 4011 : loss : 0.034124, loss_ce: 0.010966
2022-01-10 00:58:51,893 iteration 4012 : loss : 0.026252, loss_ce: 0.011046
 59%|███████████████▉           | 236/400 [1:55:26<1:20:57, 29.62s/it]2022-01-10 00:58:53,604 iteration 4013 : loss : 0.044193, loss_ce: 0.012588
2022-01-10 00:58:55,129 iteration 4014 : loss : 0.028300, loss_ce: 0.010133
2022-01-10 00:58:56,752 iteration 4015 : loss : 0.042533, loss_ce: 0.016478
2022-01-10 00:58:58,362 iteration 4016 : loss : 0.026927, loss_ce: 0.011651
2022-01-10 00:58:59,993 iteration 4017 : loss : 0.032492, loss_ce: 0.013338
2022-01-10 00:59:01,531 iteration 4018 : loss : 0.026250, loss_ce: 0.009136
2022-01-10 00:59:03,102 iteration 4019 : loss : 0.024220, loss_ce: 0.011840
2022-01-10 00:59:04,628 iteration 4020 : loss : 0.026864, loss_ce: 0.016128
2022-01-10 00:59:06,171 iteration 4021 : loss : 0.028674, loss_ce: 0.012470
2022-01-10 00:59:07,771 iteration 4022 : loss : 0.031829, loss_ce: 0.015626
2022-01-10 00:59:09,343 iteration 4023 : loss : 0.019965, loss_ce: 0.006925
2022-01-10 00:59:10,857 iteration 4024 : loss : 0.023830, loss_ce: 0.008853
2022-01-10 00:59:12,389 iteration 4025 : loss : 0.021936, loss_ce: 0.007485
2022-01-10 00:59:13,991 iteration 4026 : loss : 0.029081, loss_ce: 0.012679
2022-01-10 00:59:15,590 iteration 4027 : loss : 0.030568, loss_ce: 0.011212
2022-01-10 00:59:17,219 iteration 4028 : loss : 0.032626, loss_ce: 0.011452
2022-01-10 00:59:18,751 iteration 4029 : loss : 0.026239, loss_ce: 0.010284
 59%|███████████████▉           | 237/400 [1:55:53<1:18:12, 28.79s/it]2022-01-10 00:59:20,445 iteration 4030 : loss : 0.027370, loss_ce: 0.011549
2022-01-10 00:59:22,042 iteration 4031 : loss : 0.047737, loss_ce: 0.014990
2022-01-10 00:59:23,609 iteration 4032 : loss : 0.027970, loss_ce: 0.012260
2022-01-10 00:59:25,182 iteration 4033 : loss : 0.055610, loss_ce: 0.018741
2022-01-10 00:59:26,693 iteration 4034 : loss : 0.052492, loss_ce: 0.015946
2022-01-10 00:59:28,242 iteration 4035 : loss : 0.018963, loss_ce: 0.005939
2022-01-10 00:59:29,844 iteration 4036 : loss : 0.030882, loss_ce: 0.010892
2022-01-10 00:59:31,367 iteration 4037 : loss : 0.019941, loss_ce: 0.009367
2022-01-10 00:59:32,998 iteration 4038 : loss : 0.034700, loss_ce: 0.011829
2022-01-10 00:59:34,658 iteration 4039 : loss : 0.027322, loss_ce: 0.009988
2022-01-10 00:59:36,315 iteration 4040 : loss : 0.034860, loss_ce: 0.016088
2022-01-10 00:59:37,951 iteration 4041 : loss : 0.028143, loss_ce: 0.009588
2022-01-10 00:59:39,495 iteration 4042 : loss : 0.033340, loss_ce: 0.011276
2022-01-10 00:59:41,085 iteration 4043 : loss : 0.181435, loss_ce: 0.012152
2022-01-10 00:59:42,625 iteration 4044 : loss : 0.026538, loss_ce: 0.011116
2022-01-10 00:59:44,188 iteration 4045 : loss : 0.027108, loss_ce: 0.013518
2022-01-10 00:59:45,814 iteration 4046 : loss : 0.027847, loss_ce: 0.012587
 60%|████████████████           | 238/400 [1:56:20<1:16:19, 28.27s/it]2022-01-10 00:59:47,533 iteration 4047 : loss : 0.023325, loss_ce: 0.011304
2022-01-10 00:59:49,154 iteration 4048 : loss : 0.030905, loss_ce: 0.011504
2022-01-10 00:59:50,699 iteration 4049 : loss : 0.025474, loss_ce: 0.009577
2022-01-10 00:59:52,305 iteration 4050 : loss : 0.029953, loss_ce: 0.012288
2022-01-10 00:59:53,987 iteration 4051 : loss : 0.080443, loss_ce: 0.013428
2022-01-10 00:59:55,504 iteration 4052 : loss : 0.026107, loss_ce: 0.009325
2022-01-10 00:59:56,994 iteration 4053 : loss : 0.025777, loss_ce: 0.009228
2022-01-10 00:59:58,519 iteration 4054 : loss : 0.037652, loss_ce: 0.010546
2022-01-10 01:00:00,082 iteration 4055 : loss : 0.025608, loss_ce: 0.012871
2022-01-10 01:00:01,670 iteration 4056 : loss : 0.033299, loss_ce: 0.015835
2022-01-10 01:00:03,196 iteration 4057 : loss : 0.031893, loss_ce: 0.008229
2022-01-10 01:00:04,761 iteration 4058 : loss : 0.021140, loss_ce: 0.008150
2022-01-10 01:00:06,428 iteration 4059 : loss : 0.049428, loss_ce: 0.022701
2022-01-10 01:00:08,012 iteration 4060 : loss : 0.028916, loss_ce: 0.015110
2022-01-10 01:00:09,587 iteration 4061 : loss : 0.052434, loss_ce: 0.027275
2022-01-10 01:00:11,193 iteration 4062 : loss : 0.035341, loss_ce: 0.014172
2022-01-10 01:00:12,782 iteration 4063 : loss : 0.049242, loss_ce: 0.014649
 60%|████████████████▏          | 239/400 [1:56:47<1:14:48, 27.88s/it]2022-01-10 01:00:14,392 iteration 4064 : loss : 0.033808, loss_ce: 0.009273
2022-01-10 01:00:15,937 iteration 4065 : loss : 0.016956, loss_ce: 0.006093
2022-01-10 01:00:17,571 iteration 4066 : loss : 0.037336, loss_ce: 0.013203
2022-01-10 01:00:19,111 iteration 4067 : loss : 0.023365, loss_ce: 0.010202
2022-01-10 01:00:20,682 iteration 4068 : loss : 0.031579, loss_ce: 0.013619
2022-01-10 01:00:22,281 iteration 4069 : loss : 0.027558, loss_ce: 0.012272
2022-01-10 01:00:23,878 iteration 4070 : loss : 0.023517, loss_ce: 0.008717
2022-01-10 01:00:25,515 iteration 4071 : loss : 0.027280, loss_ce: 0.009741
2022-01-10 01:00:27,085 iteration 4072 : loss : 0.027173, loss_ce: 0.009635
2022-01-10 01:00:28,625 iteration 4073 : loss : 0.028080, loss_ce: 0.009642
2022-01-10 01:00:30,195 iteration 4074 : loss : 0.032226, loss_ce: 0.014443
2022-01-10 01:00:31,809 iteration 4075 : loss : 0.023717, loss_ce: 0.008464
2022-01-10 01:00:33,340 iteration 4076 : loss : 0.025467, loss_ce: 0.010699
2022-01-10 01:00:34,906 iteration 4077 : loss : 0.031546, loss_ce: 0.017677
2022-01-10 01:00:36,533 iteration 4078 : loss : 0.046006, loss_ce: 0.016406
2022-01-10 01:00:38,139 iteration 4079 : loss : 0.021588, loss_ce: 0.009650
2022-01-10 01:00:38,139 Training Data Eval:
2022-01-10 01:00:46,172   Average segmentation loss on training set: 0.0182
2022-01-10 01:00:46,172 Validation Data Eval:
2022-01-10 01:00:48,937   Average segmentation loss on validation set: 0.0729
2022-01-10 01:00:50,555 iteration 4080 : loss : 0.028513, loss_ce: 0.013929
 60%|████████████████▏          | 240/400 [1:57:25<1:22:16, 30.85s/it]2022-01-10 01:00:52,216 iteration 4081 : loss : 0.044744, loss_ce: 0.015502
2022-01-10 01:00:53,798 iteration 4082 : loss : 0.022523, loss_ce: 0.008370
2022-01-10 01:00:55,324 iteration 4083 : loss : 0.023549, loss_ce: 0.010026
2022-01-10 01:00:56,894 iteration 4084 : loss : 0.026896, loss_ce: 0.008830
2022-01-10 01:00:58,420 iteration 4085 : loss : 0.025580, loss_ce: 0.008098
2022-01-10 01:01:00,107 iteration 4086 : loss : 0.042449, loss_ce: 0.015449
2022-01-10 01:01:01,705 iteration 4087 : loss : 0.032324, loss_ce: 0.009521
2022-01-10 01:01:03,217 iteration 4088 : loss : 0.043178, loss_ce: 0.022610
2022-01-10 01:01:04,788 iteration 4089 : loss : 0.027039, loss_ce: 0.010541
2022-01-10 01:01:06,272 iteration 4090 : loss : 0.020818, loss_ce: 0.006668
2022-01-10 01:01:07,834 iteration 4091 : loss : 0.059770, loss_ce: 0.022122
2022-01-10 01:01:09,415 iteration 4092 : loss : 0.023724, loss_ce: 0.009697
2022-01-10 01:01:10,999 iteration 4093 : loss : 0.033965, loss_ce: 0.012360
2022-01-10 01:01:12,545 iteration 4094 : loss : 0.023217, loss_ce: 0.008469
2022-01-10 01:01:14,131 iteration 4095 : loss : 0.034575, loss_ce: 0.014643
2022-01-10 01:01:15,670 iteration 4096 : loss : 0.023872, loss_ce: 0.013555
2022-01-10 01:01:17,223 iteration 4097 : loss : 0.039994, loss_ce: 0.019019
 60%|████████████████▎          | 241/400 [1:57:52<1:18:25, 29.60s/it]2022-01-10 01:01:18,832 iteration 4098 : loss : 0.026448, loss_ce: 0.011266
2022-01-10 01:01:20,368 iteration 4099 : loss : 0.038586, loss_ce: 0.010928
2022-01-10 01:01:21,944 iteration 4100 : loss : 0.021834, loss_ce: 0.007154
2022-01-10 01:01:23,561 iteration 4101 : loss : 0.030279, loss_ce: 0.010230
2022-01-10 01:01:25,174 iteration 4102 : loss : 0.041141, loss_ce: 0.017193
2022-01-10 01:01:26,770 iteration 4103 : loss : 0.055650, loss_ce: 0.025263
2022-01-10 01:01:28,348 iteration 4104 : loss : 0.049056, loss_ce: 0.026126
2022-01-10 01:01:29,937 iteration 4105 : loss : 0.030820, loss_ce: 0.013808
2022-01-10 01:01:31,495 iteration 4106 : loss : 0.027818, loss_ce: 0.011661
2022-01-10 01:01:33,048 iteration 4107 : loss : 0.024769, loss_ce: 0.010665
2022-01-10 01:01:34,621 iteration 4108 : loss : 0.024378, loss_ce: 0.010888
2022-01-10 01:01:36,137 iteration 4109 : loss : 0.025421, loss_ce: 0.009442
2022-01-10 01:01:37,742 iteration 4110 : loss : 0.031701, loss_ce: 0.012818
2022-01-10 01:01:39,460 iteration 4111 : loss : 0.039954, loss_ce: 0.022148
2022-01-10 01:01:41,005 iteration 4112 : loss : 0.028662, loss_ce: 0.011105
2022-01-10 01:01:42,617 iteration 4113 : loss : 0.040303, loss_ce: 0.011967
2022-01-10 01:01:44,186 iteration 4114 : loss : 0.032703, loss_ce: 0.015491
 60%|████████████████▎          | 242/400 [1:58:19<1:15:50, 28.80s/it]2022-01-10 01:01:45,771 iteration 4115 : loss : 0.030169, loss_ce: 0.015177
2022-01-10 01:01:47,398 iteration 4116 : loss : 0.029297, loss_ce: 0.010369
2022-01-10 01:01:48,915 iteration 4117 : loss : 0.019669, loss_ce: 0.007370
2022-01-10 01:01:50,547 iteration 4118 : loss : 0.030068, loss_ce: 0.011464
2022-01-10 01:01:52,234 iteration 4119 : loss : 0.042012, loss_ce: 0.013429
2022-01-10 01:01:53,835 iteration 4120 : loss : 0.028699, loss_ce: 0.007698
2022-01-10 01:01:55,373 iteration 4121 : loss : 0.024620, loss_ce: 0.008677
2022-01-10 01:01:56,934 iteration 4122 : loss : 0.032133, loss_ce: 0.008920
2022-01-10 01:01:58,453 iteration 4123 : loss : 0.015399, loss_ce: 0.006486
2022-01-10 01:02:00,062 iteration 4124 : loss : 0.026018, loss_ce: 0.012096
2022-01-10 01:02:01,657 iteration 4125 : loss : 0.034533, loss_ce: 0.017071
2022-01-10 01:02:03,248 iteration 4126 : loss : 0.027451, loss_ce: 0.010935
2022-01-10 01:02:04,780 iteration 4127 : loss : 0.019441, loss_ce: 0.009442
2022-01-10 01:02:06,455 iteration 4128 : loss : 0.051327, loss_ce: 0.018904
2022-01-10 01:02:07,974 iteration 4129 : loss : 0.029082, loss_ce: 0.006347
2022-01-10 01:02:09,479 iteration 4130 : loss : 0.018493, loss_ce: 0.005577
2022-01-10 01:02:11,030 iteration 4131 : loss : 0.023160, loss_ce: 0.010521
 61%|████████████████▍          | 243/400 [1:58:46<1:13:49, 28.22s/it]2022-01-10 01:02:12,704 iteration 4132 : loss : 0.039128, loss_ce: 0.012338
2022-01-10 01:02:14,295 iteration 4133 : loss : 0.035413, loss_ce: 0.011608
2022-01-10 01:02:15,904 iteration 4134 : loss : 0.030255, loss_ce: 0.013156
2022-01-10 01:02:17,406 iteration 4135 : loss : 0.029341, loss_ce: 0.012813
2022-01-10 01:02:18,971 iteration 4136 : loss : 0.023935, loss_ce: 0.010522
2022-01-10 01:02:20,474 iteration 4137 : loss : 0.026045, loss_ce: 0.009496
2022-01-10 01:02:22,069 iteration 4138 : loss : 0.021451, loss_ce: 0.009201
2022-01-10 01:02:23,667 iteration 4139 : loss : 0.021067, loss_ce: 0.009440
2022-01-10 01:02:25,256 iteration 4140 : loss : 0.020405, loss_ce: 0.008501
2022-01-10 01:02:26,848 iteration 4141 : loss : 0.026549, loss_ce: 0.009474
2022-01-10 01:02:28,494 iteration 4142 : loss : 0.039884, loss_ce: 0.015031
2022-01-10 01:02:29,979 iteration 4143 : loss : 0.033914, loss_ce: 0.010618
2022-01-10 01:02:31,485 iteration 4144 : loss : 0.018812, loss_ce: 0.004412
2022-01-10 01:02:33,105 iteration 4145 : loss : 0.029003, loss_ce: 0.011941
2022-01-10 01:02:34,648 iteration 4146 : loss : 0.019115, loss_ce: 0.008810
2022-01-10 01:02:36,232 iteration 4147 : loss : 0.022841, loss_ce: 0.008211
2022-01-10 01:02:37,776 iteration 4148 : loss : 0.038512, loss_ce: 0.018734
 61%|████████████████▍          | 244/400 [1:59:12<1:12:13, 27.78s/it]2022-01-10 01:02:39,327 iteration 4149 : loss : 0.022731, loss_ce: 0.008063
2022-01-10 01:02:40,910 iteration 4150 : loss : 0.019988, loss_ce: 0.006048
2022-01-10 01:02:42,530 iteration 4151 : loss : 0.032551, loss_ce: 0.012745
2022-01-10 01:02:44,110 iteration 4152 : loss : 0.023497, loss_ce: 0.009596
2022-01-10 01:02:45,614 iteration 4153 : loss : 0.020231, loss_ce: 0.008643
2022-01-10 01:02:47,196 iteration 4154 : loss : 0.022163, loss_ce: 0.008168
2022-01-10 01:02:48,754 iteration 4155 : loss : 0.023257, loss_ce: 0.007444
2022-01-10 01:02:50,266 iteration 4156 : loss : 0.016776, loss_ce: 0.005833
2022-01-10 01:02:51,891 iteration 4157 : loss : 0.036360, loss_ce: 0.017994
2022-01-10 01:02:53,430 iteration 4158 : loss : 0.021485, loss_ce: 0.007679
2022-01-10 01:02:54,979 iteration 4159 : loss : 0.021506, loss_ce: 0.008829
2022-01-10 01:02:56,659 iteration 4160 : loss : 0.033978, loss_ce: 0.012117
2022-01-10 01:02:58,310 iteration 4161 : loss : 0.030006, loss_ce: 0.009513
2022-01-10 01:02:59,964 iteration 4162 : loss : 0.034284, loss_ce: 0.011703
2022-01-10 01:03:01,597 iteration 4163 : loss : 0.033006, loss_ce: 0.014968
2022-01-10 01:03:03,186 iteration 4164 : loss : 0.025326, loss_ce: 0.009711
2022-01-10 01:03:03,186 Training Data Eval:
2022-01-10 01:03:11,234   Average segmentation loss on training set: 0.0186
2022-01-10 01:03:11,234 Validation Data Eval:
2022-01-10 01:03:14,003   Average segmentation loss on validation set: 0.0893
2022-01-10 01:03:15,602 iteration 4165 : loss : 0.034024, loss_ce: 0.010290
 61%|████████████████▌          | 245/400 [1:59:50<1:19:32, 30.79s/it]2022-01-10 01:03:17,222 iteration 4166 : loss : 0.021945, loss_ce: 0.008337
2022-01-10 01:03:18,721 iteration 4167 : loss : 0.023471, loss_ce: 0.006950
2022-01-10 01:03:20,244 iteration 4168 : loss : 0.018174, loss_ce: 0.007204
2022-01-10 01:03:21,770 iteration 4169 : loss : 0.019583, loss_ce: 0.009652
2022-01-10 01:03:23,443 iteration 4170 : loss : 0.040172, loss_ce: 0.018438
2022-01-10 01:03:24,947 iteration 4171 : loss : 0.020055, loss_ce: 0.006440
2022-01-10 01:03:26,507 iteration 4172 : loss : 0.022407, loss_ce: 0.010136
2022-01-10 01:03:28,111 iteration 4173 : loss : 0.034847, loss_ce: 0.012377
2022-01-10 01:03:29,659 iteration 4174 : loss : 0.030552, loss_ce: 0.009196
2022-01-10 01:03:31,182 iteration 4175 : loss : 0.018857, loss_ce: 0.007916
2022-01-10 01:03:32,733 iteration 4176 : loss : 0.024191, loss_ce: 0.010255
2022-01-10 01:03:34,344 iteration 4177 : loss : 0.026370, loss_ce: 0.008726
2022-01-10 01:03:35,870 iteration 4178 : loss : 0.020816, loss_ce: 0.008008
2022-01-10 01:03:37,419 iteration 4179 : loss : 0.025242, loss_ce: 0.011407
2022-01-10 01:03:38,959 iteration 4180 : loss : 0.019795, loss_ce: 0.006360
2022-01-10 01:03:40,516 iteration 4181 : loss : 0.028218, loss_ce: 0.011706
2022-01-10 01:03:42,080 iteration 4182 : loss : 0.022156, loss_ce: 0.006146
 62%|████████████████▌          | 246/400 [2:00:17<1:15:42, 29.50s/it]2022-01-10 01:03:43,718 iteration 4183 : loss : 0.028040, loss_ce: 0.011310
2022-01-10 01:03:45,325 iteration 4184 : loss : 0.027809, loss_ce: 0.013002
2022-01-10 01:03:46,895 iteration 4185 : loss : 0.019029, loss_ce: 0.008584
2022-01-10 01:03:48,449 iteration 4186 : loss : 0.025175, loss_ce: 0.007000
2022-01-10 01:03:50,065 iteration 4187 : loss : 0.033952, loss_ce: 0.010994
2022-01-10 01:03:51,598 iteration 4188 : loss : 0.026968, loss_ce: 0.008956
2022-01-10 01:03:53,271 iteration 4189 : loss : 0.024364, loss_ce: 0.010524
2022-01-10 01:03:54,898 iteration 4190 : loss : 0.031155, loss_ce: 0.014269
2022-01-10 01:03:56,538 iteration 4191 : loss : 0.029632, loss_ce: 0.011973
2022-01-10 01:03:58,018 iteration 4192 : loss : 0.015897, loss_ce: 0.004940
2022-01-10 01:03:59,607 iteration 4193 : loss : 0.036230, loss_ce: 0.017978
2022-01-10 01:04:01,222 iteration 4194 : loss : 0.021302, loss_ce: 0.007473
2022-01-10 01:04:02,780 iteration 4195 : loss : 0.024269, loss_ce: 0.007161
2022-01-10 01:04:04,383 iteration 4196 : loss : 0.019930, loss_ce: 0.008884
2022-01-10 01:04:05,942 iteration 4197 : loss : 0.027767, loss_ce: 0.008186
2022-01-10 01:04:07,505 iteration 4198 : loss : 0.021109, loss_ce: 0.008148
2022-01-10 01:04:09,018 iteration 4199 : loss : 0.019924, loss_ce: 0.007678
 62%|████████████████▋          | 247/400 [2:00:44<1:13:15, 28.73s/it]2022-01-10 01:04:10,643 iteration 4200 : loss : 0.022425, loss_ce: 0.008186
2022-01-10 01:04:12,245 iteration 4201 : loss : 0.029054, loss_ce: 0.012486
2022-01-10 01:04:13,842 iteration 4202 : loss : 0.022270, loss_ce: 0.009821
2022-01-10 01:04:15,511 iteration 4203 : loss : 0.027675, loss_ce: 0.012646
2022-01-10 01:04:17,068 iteration 4204 : loss : 0.020963, loss_ce: 0.008007
2022-01-10 01:04:18,633 iteration 4205 : loss : 0.021596, loss_ce: 0.008797
2022-01-10 01:04:20,323 iteration 4206 : loss : 0.036062, loss_ce: 0.012545
2022-01-10 01:04:21,938 iteration 4207 : loss : 0.024581, loss_ce: 0.009730
2022-01-10 01:04:23,426 iteration 4208 : loss : 0.016619, loss_ce: 0.006308
2022-01-10 01:04:24,967 iteration 4209 : loss : 0.018698, loss_ce: 0.008133
2022-01-10 01:04:26,508 iteration 4210 : loss : 0.020312, loss_ce: 0.008428
2022-01-10 01:04:28,094 iteration 4211 : loss : 0.024200, loss_ce: 0.010856
2022-01-10 01:04:29,713 iteration 4212 : loss : 0.033436, loss_ce: 0.012108
2022-01-10 01:04:31,252 iteration 4213 : loss : 0.024104, loss_ce: 0.007134
2022-01-10 01:04:32,863 iteration 4214 : loss : 0.030773, loss_ce: 0.011722
2022-01-10 01:04:34,427 iteration 4215 : loss : 0.022791, loss_ce: 0.007962
2022-01-10 01:04:35,986 iteration 4216 : loss : 0.022175, loss_ce: 0.008116
 62%|████████████████▋          | 248/400 [2:01:11<1:11:26, 28.20s/it]2022-01-10 01:04:37,660 iteration 4217 : loss : 0.033191, loss_ce: 0.012984
2022-01-10 01:04:39,218 iteration 4218 : loss : 0.021236, loss_ce: 0.007286
2022-01-10 01:04:40,758 iteration 4219 : loss : 0.020195, loss_ce: 0.007851
2022-01-10 01:04:42,241 iteration 4220 : loss : 0.026525, loss_ce: 0.005913
2022-01-10 01:04:43,899 iteration 4221 : loss : 0.029552, loss_ce: 0.015187
2022-01-10 01:04:45,562 iteration 4222 : loss : 0.030108, loss_ce: 0.009878
2022-01-10 01:04:47,113 iteration 4223 : loss : 0.026809, loss_ce: 0.010386
2022-01-10 01:04:48,744 iteration 4224 : loss : 0.025258, loss_ce: 0.006178
2022-01-10 01:04:50,342 iteration 4225 : loss : 0.036713, loss_ce: 0.011570
2022-01-10 01:04:51,964 iteration 4226 : loss : 0.025846, loss_ce: 0.012491
2022-01-10 01:04:53,563 iteration 4227 : loss : 0.019281, loss_ce: 0.008016
2022-01-10 01:04:55,093 iteration 4228 : loss : 0.022704, loss_ce: 0.010742
2022-01-10 01:04:56,660 iteration 4229 : loss : 0.024152, loss_ce: 0.008965
2022-01-10 01:04:58,216 iteration 4230 : loss : 0.031238, loss_ce: 0.015796
2022-01-10 01:04:59,832 iteration 4231 : loss : 0.024212, loss_ce: 0.010118
2022-01-10 01:05:01,445 iteration 4232 : loss : 0.025967, loss_ce: 0.008795
2022-01-10 01:05:03,014 iteration 4233 : loss : 0.023001, loss_ce: 0.012142
 62%|████████████████▊          | 249/400 [2:01:38<1:10:05, 27.85s/it]2022-01-10 01:05:04,565 iteration 4234 : loss : 0.029446, loss_ce: 0.006271
2022-01-10 01:05:06,074 iteration 4235 : loss : 0.015028, loss_ce: 0.005773
2022-01-10 01:05:07,672 iteration 4236 : loss : 0.023619, loss_ce: 0.009851
2022-01-10 01:05:09,201 iteration 4237 : loss : 0.025543, loss_ce: 0.010790
2022-01-10 01:05:10,762 iteration 4238 : loss : 0.040809, loss_ce: 0.018900
2022-01-10 01:05:12,465 iteration 4239 : loss : 0.033347, loss_ce: 0.015418
2022-01-10 01:05:14,054 iteration 4240 : loss : 0.025787, loss_ce: 0.008392
2022-01-10 01:05:15,546 iteration 4241 : loss : 0.022775, loss_ce: 0.009798
2022-01-10 01:05:17,124 iteration 4242 : loss : 0.029273, loss_ce: 0.008207
2022-01-10 01:05:18,670 iteration 4243 : loss : 0.026030, loss_ce: 0.009661
2022-01-10 01:05:20,169 iteration 4244 : loss : 0.019712, loss_ce: 0.009133
2022-01-10 01:05:21,784 iteration 4245 : loss : 0.030063, loss_ce: 0.014986
2022-01-10 01:05:23,431 iteration 4246 : loss : 0.042763, loss_ce: 0.010502
2022-01-10 01:05:25,040 iteration 4247 : loss : 0.021700, loss_ce: 0.008956
2022-01-10 01:05:26,561 iteration 4248 : loss : 0.028925, loss_ce: 0.011475
2022-01-10 01:05:28,194 iteration 4249 : loss : 0.026930, loss_ce: 0.008394
2022-01-10 01:05:28,194 Training Data Eval:
2022-01-10 01:05:36,234   Average segmentation loss on training set: 0.0160
2022-01-10 01:05:36,235 Validation Data Eval:
2022-01-10 01:05:39,004   Average segmentation loss on validation set: 0.0794
2022-01-10 01:05:40,563 iteration 4250 : loss : 0.027919, loss_ce: 0.009792
 62%|████████████████▉          | 250/400 [2:02:15<1:16:53, 30.76s/it]2022-01-10 01:05:42,247 iteration 4251 : loss : 0.026004, loss_ce: 0.007579
2022-01-10 01:05:43,862 iteration 4252 : loss : 0.031027, loss_ce: 0.016035
2022-01-10 01:05:45,437 iteration 4253 : loss : 0.036187, loss_ce: 0.015050
2022-01-10 01:05:47,051 iteration 4254 : loss : 0.024816, loss_ce: 0.009836
2022-01-10 01:05:48,674 iteration 4255 : loss : 0.030298, loss_ce: 0.009678
2022-01-10 01:05:50,162 iteration 4256 : loss : 0.020054, loss_ce: 0.008952
2022-01-10 01:05:51,732 iteration 4257 : loss : 0.025102, loss_ce: 0.009769
2022-01-10 01:05:53,235 iteration 4258 : loss : 0.032483, loss_ce: 0.010281
2022-01-10 01:05:54,826 iteration 4259 : loss : 0.028624, loss_ce: 0.012547
2022-01-10 01:05:56,420 iteration 4260 : loss : 0.034805, loss_ce: 0.010583
2022-01-10 01:05:58,018 iteration 4261 : loss : 0.022059, loss_ce: 0.008542
2022-01-10 01:05:59,554 iteration 4262 : loss : 0.019843, loss_ce: 0.008673
2022-01-10 01:06:01,146 iteration 4263 : loss : 0.028787, loss_ce: 0.011529
2022-01-10 01:06:02,771 iteration 4264 : loss : 0.040958, loss_ce: 0.017856
2022-01-10 01:06:04,340 iteration 4265 : loss : 0.040007, loss_ce: 0.010134
2022-01-10 01:06:05,942 iteration 4266 : loss : 0.032132, loss_ce: 0.011433
2022-01-10 01:06:07,566 iteration 4267 : loss : 0.024351, loss_ce: 0.009578
 63%|████████████████▉          | 251/400 [2:02:42<1:13:35, 29.63s/it]2022-01-10 01:06:09,200 iteration 4268 : loss : 0.019511, loss_ce: 0.004734
2022-01-10 01:06:10,763 iteration 4269 : loss : 0.032549, loss_ce: 0.009584
2022-01-10 01:06:12,300 iteration 4270 : loss : 0.020026, loss_ce: 0.010687
2022-01-10 01:06:13,799 iteration 4271 : loss : 0.023102, loss_ce: 0.009927
2022-01-10 01:06:15,363 iteration 4272 : loss : 0.023229, loss_ce: 0.011206
2022-01-10 01:06:16,870 iteration 4273 : loss : 0.016232, loss_ce: 0.006950
2022-01-10 01:06:18,522 iteration 4274 : loss : 0.028118, loss_ce: 0.012379
2022-01-10 01:06:20,070 iteration 4275 : loss : 0.020720, loss_ce: 0.007252
2022-01-10 01:06:21,689 iteration 4276 : loss : 0.039573, loss_ce: 0.011304
2022-01-10 01:06:23,309 iteration 4277 : loss : 0.030838, loss_ce: 0.010922
2022-01-10 01:06:24,926 iteration 4278 : loss : 0.030301, loss_ce: 0.008777
2022-01-10 01:06:26,482 iteration 4279 : loss : 0.020752, loss_ce: 0.010470
2022-01-10 01:06:28,067 iteration 4280 : loss : 0.026631, loss_ce: 0.008611
2022-01-10 01:06:29,648 iteration 4281 : loss : 0.029365, loss_ce: 0.008377
2022-01-10 01:06:31,158 iteration 4282 : loss : 0.018816, loss_ce: 0.008174
2022-01-10 01:06:32,716 iteration 4283 : loss : 0.026986, loss_ce: 0.007603
2022-01-10 01:06:34,253 iteration 4284 : loss : 0.019788, loss_ce: 0.007936
 63%|█████████████████          | 252/400 [2:03:09<1:10:54, 28.75s/it]2022-01-10 01:06:35,816 iteration 4285 : loss : 0.023072, loss_ce: 0.006949
2022-01-10 01:06:37,430 iteration 4286 : loss : 0.027789, loss_ce: 0.011531
2022-01-10 01:06:38,961 iteration 4287 : loss : 0.022212, loss_ce: 0.010726
2022-01-10 01:06:40,491 iteration 4288 : loss : 0.022355, loss_ce: 0.010188
2022-01-10 01:06:42,068 iteration 4289 : loss : 0.025889, loss_ce: 0.009616
2022-01-10 01:06:43,627 iteration 4290 : loss : 0.037122, loss_ce: 0.009255
2022-01-10 01:06:45,221 iteration 4291 : loss : 0.032829, loss_ce: 0.012143
2022-01-10 01:06:46,814 iteration 4292 : loss : 0.025886, loss_ce: 0.011189
2022-01-10 01:06:48,391 iteration 4293 : loss : 0.024328, loss_ce: 0.007521
2022-01-10 01:06:49,934 iteration 4294 : loss : 0.037046, loss_ce: 0.011269
2022-01-10 01:06:51,602 iteration 4295 : loss : 0.021419, loss_ce: 0.007406
2022-01-10 01:06:53,178 iteration 4296 : loss : 0.025205, loss_ce: 0.009094
2022-01-10 01:06:54,771 iteration 4297 : loss : 0.034228, loss_ce: 0.013960
2022-01-10 01:06:56,313 iteration 4298 : loss : 0.020588, loss_ce: 0.009888
2022-01-10 01:06:57,930 iteration 4299 : loss : 0.033213, loss_ce: 0.019171
2022-01-10 01:06:59,579 iteration 4300 : loss : 0.028558, loss_ce: 0.011236
2022-01-10 01:07:01,218 iteration 4301 : loss : 0.024439, loss_ce: 0.011249
 63%|█████████████████          | 253/400 [2:03:36<1:09:07, 28.22s/it]2022-01-10 01:07:02,894 iteration 4302 : loss : 0.027916, loss_ce: 0.014201
2022-01-10 01:07:04,533 iteration 4303 : loss : 0.042763, loss_ce: 0.007555
2022-01-10 01:07:06,075 iteration 4304 : loss : 0.027725, loss_ce: 0.011303
2022-01-10 01:07:07,625 iteration 4305 : loss : 0.029895, loss_ce: 0.012027
2022-01-10 01:07:09,178 iteration 4306 : loss : 0.018381, loss_ce: 0.005984
2022-01-10 01:07:10,694 iteration 4307 : loss : 0.020010, loss_ce: 0.009303
2022-01-10 01:07:12,294 iteration 4308 : loss : 0.022362, loss_ce: 0.008408
2022-01-10 01:07:13,930 iteration 4309 : loss : 0.026834, loss_ce: 0.009730
2022-01-10 01:07:15,511 iteration 4310 : loss : 0.018978, loss_ce: 0.008738
2022-01-10 01:07:17,049 iteration 4311 : loss : 0.031947, loss_ce: 0.013399
2022-01-10 01:07:18,572 iteration 4312 : loss : 0.023245, loss_ce: 0.008197
2022-01-10 01:07:20,106 iteration 4313 : loss : 0.019021, loss_ce: 0.009477
2022-01-10 01:07:21,645 iteration 4314 : loss : 0.021086, loss_ce: 0.007377
2022-01-10 01:07:23,245 iteration 4315 : loss : 0.021049, loss_ce: 0.007538
2022-01-10 01:07:24,781 iteration 4316 : loss : 0.027892, loss_ce: 0.008644
2022-01-10 01:07:26,402 iteration 4317 : loss : 0.029806, loss_ce: 0.011722
2022-01-10 01:07:27,921 iteration 4318 : loss : 0.018698, loss_ce: 0.007033
 64%|█████████████████▏         | 254/400 [2:04:03<1:07:32, 27.76s/it]2022-01-10 01:07:29,593 iteration 4319 : loss : 0.028451, loss_ce: 0.008942
2022-01-10 01:07:31,204 iteration 4320 : loss : 0.037943, loss_ce: 0.015248
2022-01-10 01:07:32,681 iteration 4321 : loss : 0.020412, loss_ce: 0.006016
2022-01-10 01:07:34,250 iteration 4322 : loss : 0.025241, loss_ce: 0.008352
2022-01-10 01:07:35,766 iteration 4323 : loss : 0.021228, loss_ce: 0.010511
2022-01-10 01:07:37,439 iteration 4324 : loss : 0.038224, loss_ce: 0.021709
2022-01-10 01:07:39,016 iteration 4325 : loss : 0.022363, loss_ce: 0.009646
2022-01-10 01:07:40,564 iteration 4326 : loss : 0.019132, loss_ce: 0.008581
2022-01-10 01:07:42,166 iteration 4327 : loss : 0.032379, loss_ce: 0.006842
2022-01-10 01:07:43,702 iteration 4328 : loss : 0.036027, loss_ce: 0.013724
2022-01-10 01:07:45,334 iteration 4329 : loss : 0.035335, loss_ce: 0.009354
2022-01-10 01:07:46,945 iteration 4330 : loss : 0.030357, loss_ce: 0.013331
2022-01-10 01:07:48,606 iteration 4331 : loss : 0.043179, loss_ce: 0.009715
2022-01-10 01:07:50,225 iteration 4332 : loss : 0.034479, loss_ce: 0.013415
2022-01-10 01:07:51,872 iteration 4333 : loss : 0.038322, loss_ce: 0.016279
2022-01-10 01:07:53,458 iteration 4334 : loss : 0.023394, loss_ce: 0.011013
2022-01-10 01:07:53,458 Training Data Eval:
2022-01-10 01:08:01,499   Average segmentation loss on training set: 0.0182
2022-01-10 01:08:01,499 Validation Data Eval:
2022-01-10 01:08:04,271   Average segmentation loss on validation set: 0.0657
2022-01-10 01:08:10,468 Found new lowest validation loss at iteration 4334! Save model to /scratch_net/biwidl217_second/arismu/Master_Thesis_Codes/project_TransUNet/model/2022/TU_8HEADS_best_val_loss_seed1234.pth
2022-01-10 01:08:11,983 iteration 4335 : loss : 0.041130, loss_ce: 0.011390
 64%|█████████████████▏         | 255/400 [2:04:47<1:18:54, 32.65s/it]2022-01-10 01:08:13,371 iteration 4336 : loss : 0.016921, loss_ce: 0.008626
2022-01-10 01:08:15,008 iteration 4337 : loss : 0.034866, loss_ce: 0.016706
2022-01-10 01:08:16,577 iteration 4338 : loss : 0.027660, loss_ce: 0.009949
2022-01-10 01:08:18,103 iteration 4339 : loss : 0.020406, loss_ce: 0.007910
2022-01-10 01:08:19,707 iteration 4340 : loss : 0.029296, loss_ce: 0.013103
2022-01-10 01:08:21,224 iteration 4341 : loss : 0.020925, loss_ce: 0.006902
2022-01-10 01:08:22,837 iteration 4342 : loss : 0.034366, loss_ce: 0.010261
2022-01-10 01:08:24,447 iteration 4343 : loss : 0.023992, loss_ce: 0.007718
2022-01-10 01:08:26,050 iteration 4344 : loss : 0.024213, loss_ce: 0.012400
2022-01-10 01:08:27,714 iteration 4345 : loss : 0.044447, loss_ce: 0.022251
2022-01-10 01:08:29,227 iteration 4346 : loss : 0.023266, loss_ce: 0.006392
2022-01-10 01:08:30,803 iteration 4347 : loss : 0.028612, loss_ce: 0.014321
2022-01-10 01:08:32,328 iteration 4348 : loss : 0.019645, loss_ce: 0.009780
2022-01-10 01:08:33,809 iteration 4349 : loss : 0.020322, loss_ce: 0.006468
2022-01-10 01:08:35,290 iteration 4350 : loss : 0.030313, loss_ce: 0.010370
2022-01-10 01:08:36,878 iteration 4351 : loss : 0.027407, loss_ce: 0.008225
2022-01-10 01:08:38,448 iteration 4352 : loss : 0.022326, loss_ce: 0.009202
 64%|█████████████████▎         | 256/400 [2:05:13<1:13:54, 30.80s/it]2022-01-10 01:08:40,030 iteration 4353 : loss : 0.022279, loss_ce: 0.007761
2022-01-10 01:08:41,678 iteration 4354 : loss : 0.036579, loss_ce: 0.018693
2022-01-10 01:08:43,298 iteration 4355 : loss : 0.020111, loss_ce: 0.008658
2022-01-10 01:08:44,961 iteration 4356 : loss : 0.031165, loss_ce: 0.008113
2022-01-10 01:08:46,581 iteration 4357 : loss : 0.024413, loss_ce: 0.011239
2022-01-10 01:08:48,225 iteration 4358 : loss : 0.038854, loss_ce: 0.015082
2022-01-10 01:08:49,826 iteration 4359 : loss : 0.027438, loss_ce: 0.009488
2022-01-10 01:08:51,391 iteration 4360 : loss : 0.036570, loss_ce: 0.019575
2022-01-10 01:08:53,021 iteration 4361 : loss : 0.026188, loss_ce: 0.010276
2022-01-10 01:08:54,597 iteration 4362 : loss : 0.023041, loss_ce: 0.010983
2022-01-10 01:08:56,171 iteration 4363 : loss : 0.028936, loss_ce: 0.012170
2022-01-10 01:08:57,773 iteration 4364 : loss : 0.022581, loss_ce: 0.009600
2022-01-10 01:08:59,439 iteration 4365 : loss : 0.032265, loss_ce: 0.018102
2022-01-10 01:09:00,966 iteration 4366 : loss : 0.018566, loss_ce: 0.005441
2022-01-10 01:09:02,439 iteration 4367 : loss : 0.023226, loss_ce: 0.009835
2022-01-10 01:09:03,942 iteration 4368 : loss : 0.022406, loss_ce: 0.007522
2022-01-10 01:09:05,526 iteration 4369 : loss : 0.027928, loss_ce: 0.010540
 64%|█████████████████▎         | 257/400 [2:05:40<1:10:44, 29.68s/it]2022-01-10 01:09:07,158 iteration 4370 : loss : 0.026029, loss_ce: 0.008286
2022-01-10 01:09:08,752 iteration 4371 : loss : 0.025549, loss_ce: 0.012565
2022-01-10 01:09:10,357 iteration 4372 : loss : 0.020160, loss_ce: 0.008109
2022-01-10 01:09:11,956 iteration 4373 : loss : 0.026815, loss_ce: 0.013808
2022-01-10 01:09:13,567 iteration 4374 : loss : 0.020187, loss_ce: 0.006777
2022-01-10 01:09:15,177 iteration 4375 : loss : 0.021135, loss_ce: 0.009884
2022-01-10 01:09:16,729 iteration 4376 : loss : 0.026864, loss_ce: 0.010536
2022-01-10 01:09:18,280 iteration 4377 : loss : 0.022803, loss_ce: 0.007044
2022-01-10 01:09:19,788 iteration 4378 : loss : 0.019602, loss_ce: 0.007447
2022-01-10 01:09:21,306 iteration 4379 : loss : 0.017790, loss_ce: 0.006475
2022-01-10 01:09:22,798 iteration 4380 : loss : 0.018208, loss_ce: 0.006288
2022-01-10 01:09:24,396 iteration 4381 : loss : 0.030482, loss_ce: 0.012629
2022-01-10 01:09:25,967 iteration 4382 : loss : 0.042359, loss_ce: 0.009309
2022-01-10 01:09:27,512 iteration 4383 : loss : 0.031023, loss_ce: 0.009286
2022-01-10 01:09:29,144 iteration 4384 : loss : 0.024220, loss_ce: 0.010154
2022-01-10 01:09:30,771 iteration 4385 : loss : 0.033094, loss_ce: 0.015554
2022-01-10 01:09:32,308 iteration 4386 : loss : 0.026499, loss_ce: 0.012537
 64%|█████████████████▍         | 258/400 [2:06:07<1:08:11, 28.81s/it]2022-01-10 01:09:33,879 iteration 4387 : loss : 0.021370, loss_ce: 0.006704
2022-01-10 01:09:35,454 iteration 4388 : loss : 0.032248, loss_ce: 0.014575
2022-01-10 01:09:37,033 iteration 4389 : loss : 0.032109, loss_ce: 0.010092
2022-01-10 01:09:38,548 iteration 4390 : loss : 0.028737, loss_ce: 0.009376
2022-01-10 01:09:40,138 iteration 4391 : loss : 0.028993, loss_ce: 0.009164
2022-01-10 01:09:41,704 iteration 4392 : loss : 0.026409, loss_ce: 0.007602
2022-01-10 01:09:43,346 iteration 4393 : loss : 0.024472, loss_ce: 0.011503
2022-01-10 01:09:44,957 iteration 4394 : loss : 0.024907, loss_ce: 0.009849
2022-01-10 01:09:46,635 iteration 4395 : loss : 0.032858, loss_ce: 0.013029
2022-01-10 01:09:48,181 iteration 4396 : loss : 0.045468, loss_ce: 0.016509
2022-01-10 01:09:49,861 iteration 4397 : loss : 0.026609, loss_ce: 0.012972
2022-01-10 01:09:51,388 iteration 4398 : loss : 0.019663, loss_ce: 0.008431
2022-01-10 01:09:53,009 iteration 4399 : loss : 0.025570, loss_ce: 0.009185
2022-01-10 01:09:54,601 iteration 4400 : loss : 0.045308, loss_ce: 0.014609
2022-01-10 01:09:56,316 iteration 4401 : loss : 0.032361, loss_ce: 0.012510
2022-01-10 01:09:57,913 iteration 4402 : loss : 0.023503, loss_ce: 0.007674
2022-01-10 01:09:59,511 iteration 4403 : loss : 0.022191, loss_ce: 0.007324
 65%|█████████████████▍         | 259/400 [2:06:34<1:06:34, 28.33s/it]2022-01-10 01:10:01,116 iteration 4404 : loss : 0.021111, loss_ce: 0.009426
2022-01-10 01:10:02,730 iteration 4405 : loss : 0.022577, loss_ce: 0.011135
2022-01-10 01:10:04,335 iteration 4406 : loss : 0.025221, loss_ce: 0.010929
2022-01-10 01:10:05,979 iteration 4407 : loss : 0.029124, loss_ce: 0.010449
2022-01-10 01:10:07,538 iteration 4408 : loss : 0.022631, loss_ce: 0.009489
2022-01-10 01:10:09,142 iteration 4409 : loss : 0.042719, loss_ce: 0.014240
2022-01-10 01:10:10,866 iteration 4410 : loss : 0.039272, loss_ce: 0.012388
2022-01-10 01:10:12,499 iteration 4411 : loss : 0.028903, loss_ce: 0.013369
2022-01-10 01:10:14,080 iteration 4412 : loss : 0.024071, loss_ce: 0.009055
2022-01-10 01:10:15,684 iteration 4413 : loss : 0.024510, loss_ce: 0.008768
2022-01-10 01:10:17,207 iteration 4414 : loss : 0.022766, loss_ce: 0.007387
2022-01-10 01:10:18,817 iteration 4415 : loss : 0.019684, loss_ce: 0.007313
2022-01-10 01:10:20,455 iteration 4416 : loss : 0.036361, loss_ce: 0.013756
2022-01-10 01:10:22,034 iteration 4417 : loss : 0.025697, loss_ce: 0.007594
2022-01-10 01:10:23,630 iteration 4418 : loss : 0.021829, loss_ce: 0.009605
2022-01-10 01:10:25,261 iteration 4419 : loss : 0.019665, loss_ce: 0.005920
2022-01-10 01:10:25,261 Training Data Eval:
2022-01-10 01:10:33,303   Average segmentation loss on training set: 0.0151
2022-01-10 01:10:33,303 Validation Data Eval:
2022-01-10 01:10:36,075   Average segmentation loss on validation set: 0.0908
2022-01-10 01:10:37,592 iteration 4420 : loss : 0.019756, loss_ce: 0.008581
 65%|█████████████████▌         | 260/400 [2:07:12<1:12:55, 31.26s/it]2022-01-10 01:10:39,195 iteration 4421 : loss : 0.020341, loss_ce: 0.006965
2022-01-10 01:10:40,788 iteration 4422 : loss : 0.022684, loss_ce: 0.009501
2022-01-10 01:10:42,424 iteration 4423 : loss : 0.042588, loss_ce: 0.017018
2022-01-10 01:10:43,946 iteration 4424 : loss : 0.018796, loss_ce: 0.006343
2022-01-10 01:10:45,459 iteration 4425 : loss : 0.020788, loss_ce: 0.007238
2022-01-10 01:10:47,091 iteration 4426 : loss : 0.019803, loss_ce: 0.008065
2022-01-10 01:10:48,612 iteration 4427 : loss : 0.016525, loss_ce: 0.006811
2022-01-10 01:10:50,089 iteration 4428 : loss : 0.016011, loss_ce: 0.007527
2022-01-10 01:10:51,586 iteration 4429 : loss : 0.026766, loss_ce: 0.008523
2022-01-10 01:10:53,089 iteration 4430 : loss : 0.019455, loss_ce: 0.009460
2022-01-10 01:10:54,672 iteration 4431 : loss : 0.023501, loss_ce: 0.010391
2022-01-10 01:10:56,286 iteration 4432 : loss : 0.031031, loss_ce: 0.013010
2022-01-10 01:10:57,837 iteration 4433 : loss : 0.030101, loss_ce: 0.007567
2022-01-10 01:10:59,348 iteration 4434 : loss : 0.015242, loss_ce: 0.006886
2022-01-10 01:11:00,917 iteration 4435 : loss : 0.031833, loss_ce: 0.007057
2022-01-10 01:11:02,480 iteration 4436 : loss : 0.022807, loss_ce: 0.009474
2022-01-10 01:11:04,108 iteration 4437 : loss : 0.033027, loss_ce: 0.011481
 65%|█████████████████▌         | 261/400 [2:07:39<1:09:06, 29.83s/it]2022-01-10 01:11:05,801 iteration 4438 : loss : 0.038495, loss_ce: 0.011259
2022-01-10 01:11:07,404 iteration 4439 : loss : 0.025246, loss_ce: 0.010702
2022-01-10 01:11:08,989 iteration 4440 : loss : 0.023983, loss_ce: 0.008715
2022-01-10 01:11:10,672 iteration 4441 : loss : 0.027367, loss_ce: 0.009982
2022-01-10 01:11:12,316 iteration 4442 : loss : 0.020062, loss_ce: 0.009525
2022-01-10 01:11:13,920 iteration 4443 : loss : 0.025569, loss_ce: 0.009902
2022-01-10 01:11:15,430 iteration 4444 : loss : 0.021665, loss_ce: 0.007289
2022-01-10 01:11:16,989 iteration 4445 : loss : 0.031777, loss_ce: 0.012698
2022-01-10 01:11:18,594 iteration 4446 : loss : 0.019933, loss_ce: 0.007894
2022-01-10 01:11:20,146 iteration 4447 : loss : 0.019382, loss_ce: 0.006883
2022-01-10 01:11:21,745 iteration 4448 : loss : 0.042685, loss_ce: 0.019199
2022-01-10 01:11:23,328 iteration 4449 : loss : 0.026429, loss_ce: 0.011256
2022-01-10 01:11:24,963 iteration 4450 : loss : 0.033055, loss_ce: 0.010065
2022-01-10 01:11:26,510 iteration 4451 : loss : 0.027453, loss_ce: 0.009801
2022-01-10 01:11:28,101 iteration 4452 : loss : 0.020726, loss_ce: 0.008999
2022-01-10 01:11:29,649 iteration 4453 : loss : 0.019045, loss_ce: 0.006778
2022-01-10 01:11:31,196 iteration 4454 : loss : 0.031588, loss_ce: 0.014218
 66%|█████████████████▋         | 262/400 [2:08:06<1:06:43, 29.01s/it]2022-01-10 01:11:32,842 iteration 4455 : loss : 0.018030, loss_ce: 0.004792
2022-01-10 01:11:34,333 iteration 4456 : loss : 0.017605, loss_ce: 0.006828
2022-01-10 01:11:35,925 iteration 4457 : loss : 0.037657, loss_ce: 0.013415
2022-01-10 01:11:37,599 iteration 4458 : loss : 0.041658, loss_ce: 0.017393
2022-01-10 01:11:39,340 iteration 4459 : loss : 0.028261, loss_ce: 0.013692
2022-01-10 01:11:40,884 iteration 4460 : loss : 0.025790, loss_ce: 0.008337
2022-01-10 01:11:42,468 iteration 4461 : loss : 0.022193, loss_ce: 0.006778
2022-01-10 01:11:44,087 iteration 4462 : loss : 0.025945, loss_ce: 0.010262
2022-01-10 01:11:45,655 iteration 4463 : loss : 0.029466, loss_ce: 0.011641
2022-01-10 01:11:47,206 iteration 4464 : loss : 0.020529, loss_ce: 0.007200
2022-01-10 01:11:48,705 iteration 4465 : loss : 0.020387, loss_ce: 0.004564
2022-01-10 01:11:50,226 iteration 4466 : loss : 0.021300, loss_ce: 0.006869
2022-01-10 01:11:51,799 iteration 4467 : loss : 0.046375, loss_ce: 0.019910
2022-01-10 01:11:53,406 iteration 4468 : loss : 0.022411, loss_ce: 0.009128
2022-01-10 01:11:55,058 iteration 4469 : loss : 0.026559, loss_ce: 0.012960
2022-01-10 01:11:56,684 iteration 4470 : loss : 0.022769, loss_ce: 0.009725
2022-01-10 01:11:58,339 iteration 4471 : loss : 0.033479, loss_ce: 0.012608
 66%|█████████████████▊         | 263/400 [2:08:33<1:04:57, 28.45s/it]2022-01-10 01:11:59,945 iteration 4472 : loss : 0.021792, loss_ce: 0.009205
2022-01-10 01:12:01,482 iteration 4473 : loss : 0.026551, loss_ce: 0.010282
2022-01-10 01:12:03,064 iteration 4474 : loss : 0.023872, loss_ce: 0.012285
2022-01-10 01:12:04,632 iteration 4475 : loss : 0.034254, loss_ce: 0.011743
2022-01-10 01:12:06,172 iteration 4476 : loss : 0.022253, loss_ce: 0.006568
2022-01-10 01:12:07,723 iteration 4477 : loss : 0.023426, loss_ce: 0.010570
2022-01-10 01:12:09,406 iteration 4478 : loss : 0.032293, loss_ce: 0.014915
2022-01-10 01:12:10,983 iteration 4479 : loss : 0.029994, loss_ce: 0.007800
2022-01-10 01:12:12,577 iteration 4480 : loss : 0.024759, loss_ce: 0.009530
2022-01-10 01:12:14,108 iteration 4481 : loss : 0.022814, loss_ce: 0.008775
2022-01-10 01:12:15,628 iteration 4482 : loss : 0.023317, loss_ce: 0.007090
2022-01-10 01:12:17,168 iteration 4483 : loss : 0.026850, loss_ce: 0.009629
2022-01-10 01:12:18,744 iteration 4484 : loss : 0.017036, loss_ce: 0.005437
2022-01-10 01:12:20,247 iteration 4485 : loss : 0.021002, loss_ce: 0.008086
2022-01-10 01:12:21,876 iteration 4486 : loss : 0.018799, loss_ce: 0.007059
2022-01-10 01:12:23,479 iteration 4487 : loss : 0.027363, loss_ce: 0.012385
2022-01-10 01:12:25,007 iteration 4488 : loss : 0.017993, loss_ce: 0.006999
 66%|█████████████████▊         | 264/400 [2:09:00<1:03:15, 27.91s/it]2022-01-10 01:12:26,661 iteration 4489 : loss : 0.024572, loss_ce: 0.010663
2022-01-10 01:12:28,230 iteration 4490 : loss : 0.030189, loss_ce: 0.013483
2022-01-10 01:12:29,903 iteration 4491 : loss : 0.021879, loss_ce: 0.007885
2022-01-10 01:12:31,455 iteration 4492 : loss : 0.027054, loss_ce: 0.007277
2022-01-10 01:12:33,119 iteration 4493 : loss : 0.040322, loss_ce: 0.027152
2022-01-10 01:12:34,661 iteration 4494 : loss : 0.026369, loss_ce: 0.009059
2022-01-10 01:12:36,250 iteration 4495 : loss : 0.028979, loss_ce: 0.007292
2022-01-10 01:12:37,842 iteration 4496 : loss : 0.024528, loss_ce: 0.007766
2022-01-10 01:12:39,315 iteration 4497 : loss : 0.023896, loss_ce: 0.011474
2022-01-10 01:12:40,892 iteration 4498 : loss : 0.023574, loss_ce: 0.008130
2022-01-10 01:12:42,432 iteration 4499 : loss : 0.024198, loss_ce: 0.009605
2022-01-10 01:12:44,002 iteration 4500 : loss : 0.023265, loss_ce: 0.008868
2022-01-10 01:12:45,549 iteration 4501 : loss : 0.023152, loss_ce: 0.011113
2022-01-10 01:12:47,104 iteration 4502 : loss : 0.022946, loss_ce: 0.007674
2022-01-10 01:12:48,758 iteration 4503 : loss : 0.022201, loss_ce: 0.007687
2022-01-10 01:12:50,248 iteration 4504 : loss : 0.016894, loss_ce: 0.006977
2022-01-10 01:12:50,248 Training Data Eval:
2022-01-10 01:12:58,286   Average segmentation loss on training set: 0.0150
2022-01-10 01:12:58,286 Validation Data Eval:
2022-01-10 01:13:01,062   Average segmentation loss on validation set: 0.0837
2022-01-10 01:13:02,637 iteration 4505 : loss : 0.018929, loss_ce: 0.006991
 66%|█████████████████▉         | 265/400 [2:09:37<1:09:22, 30.83s/it]2022-01-10 01:13:04,298 iteration 4506 : loss : 0.025461, loss_ce: 0.011328
2022-01-10 01:13:05,943 iteration 4507 : loss : 0.029265, loss_ce: 0.011309
2022-01-10 01:13:07,527 iteration 4508 : loss : 0.026889, loss_ce: 0.008055
2022-01-10 01:13:09,037 iteration 4509 : loss : 0.019335, loss_ce: 0.006598
2022-01-10 01:13:10,610 iteration 4510 : loss : 0.018606, loss_ce: 0.006635
2022-01-10 01:13:12,276 iteration 4511 : loss : 0.028820, loss_ce: 0.013073
2022-01-10 01:13:13,765 iteration 4512 : loss : 0.022086, loss_ce: 0.007706
2022-01-10 01:13:15,314 iteration 4513 : loss : 0.022937, loss_ce: 0.008843
2022-01-10 01:13:16,855 iteration 4514 : loss : 0.022319, loss_ce: 0.009559
2022-01-10 01:13:18,415 iteration 4515 : loss : 0.015899, loss_ce: 0.006458
2022-01-10 01:13:19,995 iteration 4516 : loss : 0.020567, loss_ce: 0.008670
2022-01-10 01:13:21,600 iteration 4517 : loss : 0.024970, loss_ce: 0.010333
2022-01-10 01:13:23,150 iteration 4518 : loss : 0.036439, loss_ce: 0.014451
2022-01-10 01:13:24,652 iteration 4519 : loss : 0.027194, loss_ce: 0.009534
2022-01-10 01:13:26,226 iteration 4520 : loss : 0.024836, loss_ce: 0.007169
2022-01-10 01:13:27,815 iteration 4521 : loss : 0.022830, loss_ce: 0.010309
2022-01-10 01:13:29,443 iteration 4522 : loss : 0.030569, loss_ce: 0.010785
 66%|█████████████████▉         | 266/400 [2:10:04<1:06:09, 29.62s/it]2022-01-10 01:13:31,147 iteration 4523 : loss : 0.029780, loss_ce: 0.013423
2022-01-10 01:13:32,738 iteration 4524 : loss : 0.029030, loss_ce: 0.012122
2022-01-10 01:13:34,343 iteration 4525 : loss : 0.028542, loss_ce: 0.014373
2022-01-10 01:13:35,911 iteration 4526 : loss : 0.019493, loss_ce: 0.006576
2022-01-10 01:13:37,410 iteration 4527 : loss : 0.023478, loss_ce: 0.008115
2022-01-10 01:13:38,945 iteration 4528 : loss : 0.034085, loss_ce: 0.010666
2022-01-10 01:13:40,535 iteration 4529 : loss : 0.021906, loss_ce: 0.010367
2022-01-10 01:13:42,071 iteration 4530 : loss : 0.027861, loss_ce: 0.007343
2022-01-10 01:13:43,646 iteration 4531 : loss : 0.021530, loss_ce: 0.007700
2022-01-10 01:13:45,244 iteration 4532 : loss : 0.015248, loss_ce: 0.005161
2022-01-10 01:13:46,790 iteration 4533 : loss : 0.039102, loss_ce: 0.019970
2022-01-10 01:13:48,344 iteration 4534 : loss : 0.037145, loss_ce: 0.009688
2022-01-10 01:13:49,948 iteration 4535 : loss : 0.025658, loss_ce: 0.008394
2022-01-10 01:13:51,540 iteration 4536 : loss : 0.022270, loss_ce: 0.008306
2022-01-10 01:13:53,119 iteration 4537 : loss : 0.031528, loss_ce: 0.012912
2022-01-10 01:13:54,689 iteration 4538 : loss : 0.023618, loss_ce: 0.008218
2022-01-10 01:13:56,257 iteration 4539 : loss : 0.025239, loss_ce: 0.010195
 67%|██████████████████         | 267/400 [2:10:31<1:03:47, 28.78s/it]2022-01-10 01:13:57,861 iteration 4540 : loss : 0.015814, loss_ce: 0.005788
2022-01-10 01:13:59,464 iteration 4541 : loss : 0.028298, loss_ce: 0.012053
2022-01-10 01:14:00,991 iteration 4542 : loss : 0.023927, loss_ce: 0.008463
2022-01-10 01:14:02,538 iteration 4543 : loss : 0.030106, loss_ce: 0.010764
2022-01-10 01:14:04,066 iteration 4544 : loss : 0.024922, loss_ce: 0.009462
2022-01-10 01:14:05,702 iteration 4545 : loss : 0.027874, loss_ce: 0.013248
2022-01-10 01:14:07,386 iteration 4546 : loss : 0.039794, loss_ce: 0.017485
2022-01-10 01:14:09,052 iteration 4547 : loss : 0.024316, loss_ce: 0.009964
2022-01-10 01:14:10,568 iteration 4548 : loss : 0.021277, loss_ce: 0.007618
2022-01-10 01:14:12,092 iteration 4549 : loss : 0.022309, loss_ce: 0.006429
2022-01-10 01:14:13,672 iteration 4550 : loss : 0.025827, loss_ce: 0.011050
2022-01-10 01:14:15,269 iteration 4551 : loss : 0.022204, loss_ce: 0.009436
2022-01-10 01:14:16,928 iteration 4552 : loss : 0.040628, loss_ce: 0.012298
2022-01-10 01:14:18,527 iteration 4553 : loss : 0.023414, loss_ce: 0.008163
2022-01-10 01:14:20,059 iteration 4554 : loss : 0.028163, loss_ce: 0.011916
2022-01-10 01:14:21,623 iteration 4555 : loss : 0.024286, loss_ce: 0.010790
2022-01-10 01:14:23,183 iteration 4556 : loss : 0.030953, loss_ce: 0.008900
 67%|██████████████████         | 268/400 [2:10:58<1:02:05, 28.22s/it]2022-01-10 01:14:24,780 iteration 4557 : loss : 0.015465, loss_ce: 0.006521
2022-01-10 01:14:26,386 iteration 4558 : loss : 0.028141, loss_ce: 0.010854
2022-01-10 01:14:28,031 iteration 4559 : loss : 0.024471, loss_ce: 0.008785
2022-01-10 01:14:29,643 iteration 4560 : loss : 0.037831, loss_ce: 0.019118
2022-01-10 01:14:31,181 iteration 4561 : loss : 0.017031, loss_ce: 0.005929
2022-01-10 01:14:32,768 iteration 4562 : loss : 0.025083, loss_ce: 0.010673
2022-01-10 01:14:34,326 iteration 4563 : loss : 0.020240, loss_ce: 0.007129
2022-01-10 01:14:35,870 iteration 4564 : loss : 0.022549, loss_ce: 0.009421
2022-01-10 01:14:37,433 iteration 4565 : loss : 0.023949, loss_ce: 0.010359
2022-01-10 01:14:39,017 iteration 4566 : loss : 0.024896, loss_ce: 0.010585
2022-01-10 01:14:40,584 iteration 4567 : loss : 0.029455, loss_ce: 0.014698
2022-01-10 01:14:42,079 iteration 4568 : loss : 0.019262, loss_ce: 0.006299
2022-01-10 01:14:43,597 iteration 4569 : loss : 0.021898, loss_ce: 0.009916
2022-01-10 01:14:45,124 iteration 4570 : loss : 0.019643, loss_ce: 0.007724
2022-01-10 01:14:46,731 iteration 4571 : loss : 0.031037, loss_ce: 0.015277
2022-01-10 01:14:48,333 iteration 4572 : loss : 0.032846, loss_ce: 0.012541
2022-01-10 01:14:49,954 iteration 4573 : loss : 0.032232, loss_ce: 0.010843
 67%|██████████████████▏        | 269/400 [2:11:25<1:00:40, 27.79s/it]2022-01-10 01:14:51,524 iteration 4574 : loss : 0.014858, loss_ce: 0.004884
2022-01-10 01:14:53,116 iteration 4575 : loss : 0.033128, loss_ce: 0.008468
2022-01-10 01:14:54,618 iteration 4576 : loss : 0.021201, loss_ce: 0.009422
2022-01-10 01:14:56,134 iteration 4577 : loss : 0.020194, loss_ce: 0.009471
2022-01-10 01:14:57,699 iteration 4578 : loss : 0.020471, loss_ce: 0.007928
2022-01-10 01:14:59,222 iteration 4579 : loss : 0.028375, loss_ce: 0.012599
2022-01-10 01:15:00,765 iteration 4580 : loss : 0.019471, loss_ce: 0.007055
2022-01-10 01:15:02,332 iteration 4581 : loss : 0.031022, loss_ce: 0.012725
2022-01-10 01:15:03,895 iteration 4582 : loss : 0.029724, loss_ce: 0.011538
2022-01-10 01:15:05,450 iteration 4583 : loss : 0.029802, loss_ce: 0.009620
2022-01-10 01:15:06,998 iteration 4584 : loss : 0.017977, loss_ce: 0.006380
2022-01-10 01:15:08,585 iteration 4585 : loss : 0.027490, loss_ce: 0.009342
2022-01-10 01:15:10,101 iteration 4586 : loss : 0.018756, loss_ce: 0.007888
2022-01-10 01:15:11,636 iteration 4587 : loss : 0.024471, loss_ce: 0.008262
2022-01-10 01:15:13,170 iteration 4588 : loss : 0.028647, loss_ce: 0.011364
2022-01-10 01:15:14,755 iteration 4589 : loss : 0.023386, loss_ce: 0.007686
2022-01-10 01:15:14,756 Training Data Eval:
2022-01-10 01:15:22,793   Average segmentation loss on training set: 0.0172
2022-01-10 01:15:22,793 Validation Data Eval:
2022-01-10 01:15:25,568   Average segmentation loss on validation set: 0.0789
2022-01-10 01:15:27,118 iteration 4590 : loss : 0.026420, loss_ce: 0.012396
 68%|██████████████████▏        | 270/400 [2:12:02<1:06:18, 30.60s/it]2022-01-10 01:15:28,746 iteration 4591 : loss : 0.034832, loss_ce: 0.009306
2022-01-10 01:15:30,364 iteration 4592 : loss : 0.024754, loss_ce: 0.007887
2022-01-10 01:15:31,960 iteration 4593 : loss : 0.029197, loss_ce: 0.014433
2022-01-10 01:15:33,597 iteration 4594 : loss : 0.055477, loss_ce: 0.014796
2022-01-10 01:15:35,161 iteration 4595 : loss : 0.022990, loss_ce: 0.008474
2022-01-10 01:15:36,703 iteration 4596 : loss : 0.022470, loss_ce: 0.007229
2022-01-10 01:15:38,288 iteration 4597 : loss : 0.022737, loss_ce: 0.010310
2022-01-10 01:15:39,918 iteration 4598 : loss : 0.030491, loss_ce: 0.010693
2022-01-10 01:15:41,526 iteration 4599 : loss : 0.022508, loss_ce: 0.011094
2022-01-10 01:15:43,083 iteration 4600 : loss : 0.021253, loss_ce: 0.007329
2022-01-10 01:15:44,681 iteration 4601 : loss : 0.026036, loss_ce: 0.011881
2022-01-10 01:15:46,353 iteration 4602 : loss : 0.035431, loss_ce: 0.010091
2022-01-10 01:15:47,944 iteration 4603 : loss : 0.023821, loss_ce: 0.008692
2022-01-10 01:15:49,557 iteration 4604 : loss : 0.020658, loss_ce: 0.009119
2022-01-10 01:15:51,187 iteration 4605 : loss : 0.050991, loss_ce: 0.008322
2022-01-10 01:15:52,810 iteration 4606 : loss : 0.022427, loss_ce: 0.008956
2022-01-10 01:15:54,435 iteration 4607 : loss : 0.021209, loss_ce: 0.011362
 68%|██████████████████▎        | 271/400 [2:12:29<1:03:40, 29.62s/it]2022-01-10 01:15:56,026 iteration 4608 : loss : 0.017599, loss_ce: 0.007407
2022-01-10 01:15:57,610 iteration 4609 : loss : 0.033578, loss_ce: 0.015399
2022-01-10 01:15:59,133 iteration 4610 : loss : 0.019979, loss_ce: 0.006980
2022-01-10 01:16:00,652 iteration 4611 : loss : 0.024722, loss_ce: 0.009476
2022-01-10 01:16:02,151 iteration 4612 : loss : 0.018341, loss_ce: 0.005559
2022-01-10 01:16:03,723 iteration 4613 : loss : 0.028428, loss_ce: 0.010108
2022-01-10 01:16:05,302 iteration 4614 : loss : 0.026643, loss_ce: 0.010844
2022-01-10 01:16:06,920 iteration 4615 : loss : 0.020939, loss_ce: 0.008698
2022-01-10 01:16:08,606 iteration 4616 : loss : 0.025313, loss_ce: 0.010729
2022-01-10 01:16:10,200 iteration 4617 : loss : 0.024245, loss_ce: 0.009286
2022-01-10 01:16:11,725 iteration 4618 : loss : 0.020983, loss_ce: 0.006817
2022-01-10 01:16:13,261 iteration 4619 : loss : 0.015095, loss_ce: 0.006746
2022-01-10 01:16:14,886 iteration 4620 : loss : 0.037458, loss_ce: 0.014821
2022-01-10 01:16:16,452 iteration 4621 : loss : 0.027071, loss_ce: 0.014056
2022-01-10 01:16:18,030 iteration 4622 : loss : 0.029547, loss_ce: 0.010357
2022-01-10 01:16:19,598 iteration 4623 : loss : 0.021908, loss_ce: 0.006403
2022-01-10 01:16:21,182 iteration 4624 : loss : 0.019957, loss_ce: 0.009187
 68%|██████████████████▎        | 272/400 [2:12:56<1:01:20, 28.75s/it]2022-01-10 01:16:22,851 iteration 4625 : loss : 0.021878, loss_ce: 0.008911
2022-01-10 01:16:24,488 iteration 4626 : loss : 0.022712, loss_ce: 0.009521
2022-01-10 01:16:26,167 iteration 4627 : loss : 0.028055, loss_ce: 0.011428
2022-01-10 01:16:27,720 iteration 4628 : loss : 0.026127, loss_ce: 0.008842
2022-01-10 01:16:29,229 iteration 4629 : loss : 0.019641, loss_ce: 0.006130
2022-01-10 01:16:30,792 iteration 4630 : loss : 0.021133, loss_ce: 0.008233
2022-01-10 01:16:32,373 iteration 4631 : loss : 0.028910, loss_ce: 0.011681
2022-01-10 01:16:33,950 iteration 4632 : loss : 0.056543, loss_ce: 0.022748
2022-01-10 01:16:35,505 iteration 4633 : loss : 0.017501, loss_ce: 0.005989
2022-01-10 01:16:37,039 iteration 4634 : loss : 0.021137, loss_ce: 0.011993
2022-01-10 01:16:38,532 iteration 4635 : loss : 0.016103, loss_ce: 0.006175
2022-01-10 01:16:40,181 iteration 4636 : loss : 0.022806, loss_ce: 0.009616
2022-01-10 01:16:41,729 iteration 4637 : loss : 0.025399, loss_ce: 0.007597
2022-01-10 01:16:43,414 iteration 4638 : loss : 0.030211, loss_ce: 0.013328
2022-01-10 01:16:45,035 iteration 4639 : loss : 0.034404, loss_ce: 0.010709
2022-01-10 01:16:46,734 iteration 4640 : loss : 0.026602, loss_ce: 0.011094
2022-01-10 01:16:48,357 iteration 4641 : loss : 0.027756, loss_ce: 0.011839
 68%|███████████████████▊         | 273/400 [2:13:23<59:51, 28.28s/it]2022-01-10 01:16:49,988 iteration 4642 : loss : 0.022102, loss_ce: 0.004952
2022-01-10 01:16:51,564 iteration 4643 : loss : 0.017679, loss_ce: 0.006324
2022-01-10 01:16:53,199 iteration 4644 : loss : 0.029833, loss_ce: 0.010394
2022-01-10 01:16:54,842 iteration 4645 : loss : 0.028315, loss_ce: 0.013600
2022-01-10 01:16:56,452 iteration 4646 : loss : 0.037697, loss_ce: 0.007921
2022-01-10 01:16:57,959 iteration 4647 : loss : 0.022717, loss_ce: 0.006985
2022-01-10 01:16:59,507 iteration 4648 : loss : 0.025037, loss_ce: 0.009160
2022-01-10 01:17:01,094 iteration 4649 : loss : 0.023329, loss_ce: 0.009796
2022-01-10 01:17:02,723 iteration 4650 : loss : 0.032729, loss_ce: 0.012583
2022-01-10 01:17:04,249 iteration 4651 : loss : 0.017038, loss_ce: 0.007208
2022-01-10 01:17:05,904 iteration 4652 : loss : 0.023406, loss_ce: 0.009485
2022-01-10 01:17:07,459 iteration 4653 : loss : 0.023603, loss_ce: 0.010479
2022-01-10 01:17:09,019 iteration 4654 : loss : 0.024991, loss_ce: 0.007489
2022-01-10 01:17:10,660 iteration 4655 : loss : 0.035106, loss_ce: 0.014738
2022-01-10 01:17:12,318 iteration 4656 : loss : 0.026647, loss_ce: 0.011752
2022-01-10 01:17:13,939 iteration 4657 : loss : 0.029879, loss_ce: 0.013807
2022-01-10 01:17:15,557 iteration 4658 : loss : 0.033583, loss_ce: 0.013272
 68%|███████████████████▊         | 274/400 [2:13:50<58:42, 27.95s/it]2022-01-10 01:17:17,220 iteration 4659 : loss : 0.028097, loss_ce: 0.013231
2022-01-10 01:17:18,860 iteration 4660 : loss : 0.024828, loss_ce: 0.009952
2022-01-10 01:17:20,471 iteration 4661 : loss : 0.028322, loss_ce: 0.010477
2022-01-10 01:17:22,008 iteration 4662 : loss : 0.016983, loss_ce: 0.006423
2022-01-10 01:17:23,615 iteration 4663 : loss : 0.027277, loss_ce: 0.010591
2022-01-10 01:17:25,172 iteration 4664 : loss : 0.022620, loss_ce: 0.008669
2022-01-10 01:17:26,740 iteration 4665 : loss : 0.021072, loss_ce: 0.007908
2022-01-10 01:17:28,216 iteration 4666 : loss : 0.020860, loss_ce: 0.006536
2022-01-10 01:17:29,753 iteration 4667 : loss : 0.016070, loss_ce: 0.006393
2022-01-10 01:17:31,449 iteration 4668 : loss : 0.027887, loss_ce: 0.014618
2022-01-10 01:17:32,985 iteration 4669 : loss : 0.016538, loss_ce: 0.007243
2022-01-10 01:17:34,526 iteration 4670 : loss : 0.029352, loss_ce: 0.011721
2022-01-10 01:17:36,050 iteration 4671 : loss : 0.030648, loss_ce: 0.013193
2022-01-10 01:17:37,719 iteration 4672 : loss : 0.036076, loss_ce: 0.012880
2022-01-10 01:17:39,355 iteration 4673 : loss : 0.025382, loss_ce: 0.011470
2022-01-10 01:17:40,867 iteration 4674 : loss : 0.019372, loss_ce: 0.005704
2022-01-10 01:17:40,867 Training Data Eval:
2022-01-10 01:17:48,904   Average segmentation loss on training set: 0.0142
2022-01-10 01:17:48,904 Validation Data Eval:
2022-01-10 01:17:51,675   Average segmentation loss on validation set: 0.0781
2022-01-10 01:17:53,300 iteration 4675 : loss : 0.020317, loss_ce: 0.008497
 69%|██████████████████▌        | 275/400 [2:14:28<1:04:21, 30.89s/it]2022-01-10 01:17:55,031 iteration 4676 : loss : 0.025530, loss_ce: 0.008241
2022-01-10 01:17:56,593 iteration 4677 : loss : 0.038827, loss_ce: 0.013401
2022-01-10 01:17:58,191 iteration 4678 : loss : 0.029356, loss_ce: 0.010397
2022-01-10 01:17:59,840 iteration 4679 : loss : 0.044043, loss_ce: 0.011392
2022-01-10 01:18:01,415 iteration 4680 : loss : 0.019158, loss_ce: 0.007550
2022-01-10 01:18:02,977 iteration 4681 : loss : 0.031568, loss_ce: 0.009599
2022-01-10 01:18:04,525 iteration 4682 : loss : 0.016905, loss_ce: 0.006322
2022-01-10 01:18:06,003 iteration 4683 : loss : 0.024184, loss_ce: 0.006514
2022-01-10 01:18:07,629 iteration 4684 : loss : 0.019489, loss_ce: 0.009285
2022-01-10 01:18:09,234 iteration 4685 : loss : 0.040933, loss_ce: 0.016163
2022-01-10 01:18:10,806 iteration 4686 : loss : 0.026490, loss_ce: 0.008524
2022-01-10 01:18:12,360 iteration 4687 : loss : 0.018400, loss_ce: 0.008318
2022-01-10 01:18:13,862 iteration 4688 : loss : 0.018166, loss_ce: 0.007676
2022-01-10 01:18:15,368 iteration 4689 : loss : 0.015417, loss_ce: 0.006849
2022-01-10 01:18:16,895 iteration 4690 : loss : 0.022459, loss_ce: 0.009465
2022-01-10 01:18:18,510 iteration 4691 : loss : 0.034433, loss_ce: 0.019826
2022-01-10 01:18:20,064 iteration 4692 : loss : 0.023621, loss_ce: 0.007422
 69%|██████████████████▋        | 276/400 [2:14:55<1:01:17, 29.65s/it]2022-01-10 01:18:21,763 iteration 4693 : loss : 0.026741, loss_ce: 0.009174
2022-01-10 01:18:23,364 iteration 4694 : loss : 0.030671, loss_ce: 0.012249
2022-01-10 01:18:24,801 iteration 4695 : loss : 0.016163, loss_ce: 0.005525
2022-01-10 01:18:26,443 iteration 4696 : loss : 0.024353, loss_ce: 0.009571
2022-01-10 01:18:27,985 iteration 4697 : loss : 0.022954, loss_ce: 0.008336
2022-01-10 01:18:29,561 iteration 4698 : loss : 0.019979, loss_ce: 0.008769
2022-01-10 01:18:31,198 iteration 4699 : loss : 0.023640, loss_ce: 0.010950
2022-01-10 01:18:32,726 iteration 4700 : loss : 0.021222, loss_ce: 0.009276
2022-01-10 01:18:34,396 iteration 4701 : loss : 0.026102, loss_ce: 0.011138
2022-01-10 01:18:35,946 iteration 4702 : loss : 0.023350, loss_ce: 0.009070
2022-01-10 01:18:37,479 iteration 4703 : loss : 0.022783, loss_ce: 0.005450
2022-01-10 01:18:39,048 iteration 4704 : loss : 0.019916, loss_ce: 0.007441
2022-01-10 01:18:40,496 iteration 4705 : loss : 0.015422, loss_ce: 0.006731
2022-01-10 01:18:42,063 iteration 4706 : loss : 0.019093, loss_ce: 0.004827
2022-01-10 01:18:43,685 iteration 4707 : loss : 0.022800, loss_ce: 0.009148
2022-01-10 01:18:45,320 iteration 4708 : loss : 0.023570, loss_ce: 0.007632
2022-01-10 01:18:46,932 iteration 4709 : loss : 0.031802, loss_ce: 0.010321
 69%|████████████████████         | 277/400 [2:15:22<59:04, 28.82s/it]2022-01-10 01:18:48,594 iteration 4710 : loss : 0.031293, loss_ce: 0.006788
2022-01-10 01:18:50,314 iteration 4711 : loss : 0.024012, loss_ce: 0.008789
2022-01-10 01:18:51,878 iteration 4712 : loss : 0.019632, loss_ce: 0.008419
2022-01-10 01:18:53,446 iteration 4713 : loss : 0.047047, loss_ce: 0.024108
2022-01-10 01:18:55,034 iteration 4714 : loss : 0.022966, loss_ce: 0.008067
2022-01-10 01:18:56,675 iteration 4715 : loss : 0.030213, loss_ce: 0.010854
2022-01-10 01:18:58,276 iteration 4716 : loss : 0.029107, loss_ce: 0.012706
2022-01-10 01:18:59,846 iteration 4717 : loss : 0.019645, loss_ce: 0.006841
2022-01-10 01:19:01,363 iteration 4718 : loss : 0.023763, loss_ce: 0.008849
2022-01-10 01:19:02,875 iteration 4719 : loss : 0.019882, loss_ce: 0.009403
2022-01-10 01:19:04,408 iteration 4720 : loss : 0.018896, loss_ce: 0.007837
2022-01-10 01:19:05,961 iteration 4721 : loss : 0.025221, loss_ce: 0.010792
2022-01-10 01:19:07,583 iteration 4722 : loss : 0.030505, loss_ce: 0.008900
2022-01-10 01:19:09,155 iteration 4723 : loss : 0.017217, loss_ce: 0.006568
2022-01-10 01:19:10,661 iteration 4724 : loss : 0.019435, loss_ce: 0.007929
2022-01-10 01:19:12,168 iteration 4725 : loss : 0.019210, loss_ce: 0.008151
2022-01-10 01:19:13,737 iteration 4726 : loss : 0.023019, loss_ce: 0.008766
 70%|████████████████████▏        | 278/400 [2:15:48<57:22, 28.22s/it]2022-01-10 01:19:15,423 iteration 4727 : loss : 0.026231, loss_ce: 0.009535
2022-01-10 01:19:16,943 iteration 4728 : loss : 0.018224, loss_ce: 0.007263
2022-01-10 01:19:18,473 iteration 4729 : loss : 0.018432, loss_ce: 0.006730
2022-01-10 01:19:20,005 iteration 4730 : loss : 0.017832, loss_ce: 0.007827
2022-01-10 01:19:21,629 iteration 4731 : loss : 0.026033, loss_ce: 0.008156
2022-01-10 01:19:23,215 iteration 4732 : loss : 0.026688, loss_ce: 0.009046
2022-01-10 01:19:24,793 iteration 4733 : loss : 0.019719, loss_ce: 0.007549
2022-01-10 01:19:26,348 iteration 4734 : loss : 0.024004, loss_ce: 0.008780
2022-01-10 01:19:28,007 iteration 4735 : loss : 0.030513, loss_ce: 0.010680
2022-01-10 01:19:29,517 iteration 4736 : loss : 0.020509, loss_ce: 0.008364
2022-01-10 01:19:31,164 iteration 4737 : loss : 0.038177, loss_ce: 0.013460
2022-01-10 01:19:32,737 iteration 4738 : loss : 0.045201, loss_ce: 0.022685
2022-01-10 01:19:34,308 iteration 4739 : loss : 0.017596, loss_ce: 0.006331
2022-01-10 01:19:35,924 iteration 4740 : loss : 0.029875, loss_ce: 0.012356
2022-01-10 01:19:37,476 iteration 4741 : loss : 0.017704, loss_ce: 0.005929
2022-01-10 01:19:39,036 iteration 4742 : loss : 0.025500, loss_ce: 0.014379
2022-01-10 01:19:40,688 iteration 4743 : loss : 0.029729, loss_ce: 0.010036
 70%|████████████████████▏        | 279/400 [2:16:15<56:08, 27.84s/it]2022-01-10 01:19:42,374 iteration 4744 : loss : 0.018375, loss_ce: 0.006315
2022-01-10 01:19:43,953 iteration 4745 : loss : 0.044303, loss_ce: 0.013134
2022-01-10 01:19:45,489 iteration 4746 : loss : 0.016519, loss_ce: 0.008719
2022-01-10 01:19:47,037 iteration 4747 : loss : 0.022305, loss_ce: 0.007596
2022-01-10 01:19:48,622 iteration 4748 : loss : 0.024296, loss_ce: 0.009415
2022-01-10 01:19:50,210 iteration 4749 : loss : 0.031507, loss_ce: 0.012333
2022-01-10 01:19:51,835 iteration 4750 : loss : 0.021170, loss_ce: 0.008055
2022-01-10 01:19:53,388 iteration 4751 : loss : 0.023239, loss_ce: 0.008686
2022-01-10 01:19:54,992 iteration 4752 : loss : 0.018927, loss_ce: 0.006253
2022-01-10 01:19:56,617 iteration 4753 : loss : 0.019471, loss_ce: 0.006613
2022-01-10 01:19:58,217 iteration 4754 : loss : 0.017268, loss_ce: 0.005865
2022-01-10 01:19:59,900 iteration 4755 : loss : 0.020640, loss_ce: 0.006011
2022-01-10 01:20:01,453 iteration 4756 : loss : 0.023638, loss_ce: 0.011869
2022-01-10 01:20:03,017 iteration 4757 : loss : 0.024707, loss_ce: 0.008745
2022-01-10 01:20:04,558 iteration 4758 : loss : 0.022313, loss_ce: 0.006790
2022-01-10 01:20:06,132 iteration 4759 : loss : 0.014227, loss_ce: 0.004912
2022-01-10 01:20:06,132 Training Data Eval:
2022-01-10 01:20:14,177   Average segmentation loss on training set: 0.0132
2022-01-10 01:20:14,177 Validation Data Eval:
2022-01-10 01:20:16,942   Average segmentation loss on validation set: 0.0766
2022-01-10 01:20:18,487 iteration 4760 : loss : 0.019381, loss_ce: 0.007907
 70%|██████████████████▉        | 280/400 [2:16:53<1:01:39, 30.83s/it]2022-01-10 01:20:20,104 iteration 4761 : loss : 0.015402, loss_ce: 0.004270
2022-01-10 01:20:21,736 iteration 4762 : loss : 0.026914, loss_ce: 0.007996
2022-01-10 01:20:23,258 iteration 4763 : loss : 0.015787, loss_ce: 0.006155
2022-01-10 01:20:24,861 iteration 4764 : loss : 0.025125, loss_ce: 0.009054
2022-01-10 01:20:26,414 iteration 4765 : loss : 0.023373, loss_ce: 0.010559
2022-01-10 01:20:28,051 iteration 4766 : loss : 0.026930, loss_ce: 0.009059
2022-01-10 01:20:29,650 iteration 4767 : loss : 0.021407, loss_ce: 0.009128
2022-01-10 01:20:31,183 iteration 4768 : loss : 0.021883, loss_ce: 0.007406
2022-01-10 01:20:32,794 iteration 4769 : loss : 0.036241, loss_ce: 0.013781
2022-01-10 01:20:34,412 iteration 4770 : loss : 0.017274, loss_ce: 0.006398
2022-01-10 01:20:35,927 iteration 4771 : loss : 0.016904, loss_ce: 0.005583
2022-01-10 01:20:37,547 iteration 4772 : loss : 0.023303, loss_ce: 0.010378
2022-01-10 01:20:39,154 iteration 4773 : loss : 0.029334, loss_ce: 0.012290
2022-01-10 01:20:40,751 iteration 4774 : loss : 0.019986, loss_ce: 0.008978
2022-01-10 01:20:42,293 iteration 4775 : loss : 0.033519, loss_ce: 0.010814
2022-01-10 01:20:43,857 iteration 4776 : loss : 0.017426, loss_ce: 0.009455
2022-01-10 01:20:45,568 iteration 4777 : loss : 0.031205, loss_ce: 0.014952
 70%|████████████████████▎        | 281/400 [2:17:20<58:54, 29.70s/it]2022-01-10 01:20:47,134 iteration 4778 : loss : 0.012637, loss_ce: 0.003778
2022-01-10 01:20:48,687 iteration 4779 : loss : 0.033362, loss_ce: 0.011133
2022-01-10 01:20:50,189 iteration 4780 : loss : 0.013760, loss_ce: 0.004757
2022-01-10 01:20:51,733 iteration 4781 : loss : 0.028591, loss_ce: 0.008663
2022-01-10 01:20:53,357 iteration 4782 : loss : 0.033126, loss_ce: 0.012937
2022-01-10 01:20:54,931 iteration 4783 : loss : 0.024488, loss_ce: 0.012811
2022-01-10 01:20:56,447 iteration 4784 : loss : 0.016280, loss_ce: 0.006812
2022-01-10 01:20:58,090 iteration 4785 : loss : 0.025277, loss_ce: 0.009596
2022-01-10 01:20:59,689 iteration 4786 : loss : 0.024060, loss_ce: 0.007425
2022-01-10 01:21:01,292 iteration 4787 : loss : 0.024325, loss_ce: 0.007711
2022-01-10 01:21:02,900 iteration 4788 : loss : 0.020999, loss_ce: 0.006723
2022-01-10 01:21:04,417 iteration 4789 : loss : 0.021898, loss_ce: 0.011687
2022-01-10 01:21:06,080 iteration 4790 : loss : 0.032280, loss_ce: 0.012332
2022-01-10 01:21:07,638 iteration 4791 : loss : 0.022413, loss_ce: 0.010115
2022-01-10 01:21:09,262 iteration 4792 : loss : 0.028258, loss_ce: 0.009305
2022-01-10 01:21:10,799 iteration 4793 : loss : 0.020426, loss_ce: 0.007714
2022-01-10 01:21:12,289 iteration 4794 : loss : 0.034411, loss_ce: 0.012556
 70%|████████████████████▍        | 282/400 [2:17:47<56:39, 28.81s/it]2022-01-10 01:21:13,940 iteration 4795 : loss : 0.020991, loss_ce: 0.006894
2022-01-10 01:21:15,532 iteration 4796 : loss : 0.015616, loss_ce: 0.005083
2022-01-10 01:21:17,135 iteration 4797 : loss : 0.031075, loss_ce: 0.012214
2022-01-10 01:21:18,782 iteration 4798 : loss : 0.035121, loss_ce: 0.012236
2022-01-10 01:21:20,360 iteration 4799 : loss : 0.017364, loss_ce: 0.005656
2022-01-10 01:21:21,945 iteration 4800 : loss : 0.023554, loss_ce: 0.006744
2022-01-10 01:21:23,606 iteration 4801 : loss : 0.025431, loss_ce: 0.009871
2022-01-10 01:21:25,114 iteration 4802 : loss : 0.015832, loss_ce: 0.006844
2022-01-10 01:21:26,667 iteration 4803 : loss : 0.027605, loss_ce: 0.012536
2022-01-10 01:21:28,311 iteration 4804 : loss : 0.062103, loss_ce: 0.017671
2022-01-10 01:21:29,945 iteration 4805 : loss : 0.038523, loss_ce: 0.014984
2022-01-10 01:21:31,590 iteration 4806 : loss : 0.020077, loss_ce: 0.008291
2022-01-10 01:21:33,103 iteration 4807 : loss : 0.026132, loss_ce: 0.011012
2022-01-10 01:21:34,606 iteration 4808 : loss : 0.018519, loss_ce: 0.007564
2022-01-10 01:21:36,191 iteration 4809 : loss : 0.027484, loss_ce: 0.013251
2022-01-10 01:21:37,760 iteration 4810 : loss : 0.025429, loss_ce: 0.008030
2022-01-10 01:21:39,351 iteration 4811 : loss : 0.022262, loss_ce: 0.007701
 71%|████████████████████▌        | 283/400 [2:18:14<55:08, 28.28s/it]2022-01-10 01:21:40,926 iteration 4812 : loss : 0.020965, loss_ce: 0.008899
2022-01-10 01:21:42,519 iteration 4813 : loss : 0.035739, loss_ce: 0.011161
2022-01-10 01:21:44,139 iteration 4814 : loss : 0.030234, loss_ce: 0.009032
2022-01-10 01:21:45,655 iteration 4815 : loss : 0.019670, loss_ce: 0.008824
2022-01-10 01:21:47,335 iteration 4816 : loss : 0.023753, loss_ce: 0.009693
2022-01-10 01:21:48,900 iteration 4817 : loss : 0.025566, loss_ce: 0.009340
2022-01-10 01:21:50,457 iteration 4818 : loss : 0.019324, loss_ce: 0.006893
2022-01-10 01:21:51,947 iteration 4819 : loss : 0.023742, loss_ce: 0.008999
2022-01-10 01:21:53,491 iteration 4820 : loss : 0.023336, loss_ce: 0.009738
2022-01-10 01:21:55,151 iteration 4821 : loss : 0.028193, loss_ce: 0.013572
2022-01-10 01:21:56,802 iteration 4822 : loss : 0.026920, loss_ce: 0.009255
2022-01-10 01:21:58,376 iteration 4823 : loss : 0.031946, loss_ce: 0.014335
2022-01-10 01:21:59,979 iteration 4824 : loss : 0.028283, loss_ce: 0.010725
2022-01-10 01:22:01,529 iteration 4825 : loss : 0.022412, loss_ce: 0.008408
2022-01-10 01:22:03,076 iteration 4826 : loss : 0.024689, loss_ce: 0.008692
2022-01-10 01:22:04,631 iteration 4827 : loss : 0.018515, loss_ce: 0.007868
2022-01-10 01:22:06,170 iteration 4828 : loss : 0.032869, loss_ce: 0.013073
 71%|████████████████████▌        | 284/400 [2:18:41<53:50, 27.85s/it]2022-01-10 01:22:07,747 iteration 4829 : loss : 0.023081, loss_ce: 0.008287
2022-01-10 01:22:09,356 iteration 4830 : loss : 0.021096, loss_ce: 0.009659
2022-01-10 01:22:10,944 iteration 4831 : loss : 0.022836, loss_ce: 0.009633
2022-01-10 01:22:12,509 iteration 4832 : loss : 0.023282, loss_ce: 0.007010
2022-01-10 01:22:14,123 iteration 4833 : loss : 0.037721, loss_ce: 0.013866
2022-01-10 01:22:15,640 iteration 4834 : loss : 0.015266, loss_ce: 0.005180
2022-01-10 01:22:17,321 iteration 4835 : loss : 0.026113, loss_ce: 0.011890
2022-01-10 01:22:18,906 iteration 4836 : loss : 0.024664, loss_ce: 0.008352
2022-01-10 01:22:20,530 iteration 4837 : loss : 0.022238, loss_ce: 0.006463
2022-01-10 01:22:22,083 iteration 4838 : loss : 0.016328, loss_ce: 0.005112
2022-01-10 01:22:23,686 iteration 4839 : loss : 0.023503, loss_ce: 0.008627
2022-01-10 01:22:25,264 iteration 4840 : loss : 0.023975, loss_ce: 0.011661
2022-01-10 01:22:26,771 iteration 4841 : loss : 0.014008, loss_ce: 0.004297
2022-01-10 01:22:28,353 iteration 4842 : loss : 0.021536, loss_ce: 0.008895
2022-01-10 01:22:30,018 iteration 4843 : loss : 0.020721, loss_ce: 0.007002
2022-01-10 01:22:31,596 iteration 4844 : loss : 0.028108, loss_ce: 0.009347
2022-01-10 01:22:31,596 Training Data Eval:
2022-01-10 01:22:39,637   Average segmentation loss on training set: 0.0132
2022-01-10 01:22:39,637 Validation Data Eval:
2022-01-10 01:22:42,413   Average segmentation loss on validation set: 0.0864
2022-01-10 01:22:44,026 iteration 4845 : loss : 0.025433, loss_ce: 0.011225
 71%|████████████████████▋        | 285/400 [2:19:19<59:07, 30.85s/it]2022-01-10 01:22:45,627 iteration 4846 : loss : 0.019250, loss_ce: 0.007379
2022-01-10 01:22:47,167 iteration 4847 : loss : 0.020149, loss_ce: 0.008074
2022-01-10 01:22:48,788 iteration 4848 : loss : 0.026425, loss_ce: 0.013021
2022-01-10 01:22:50,391 iteration 4849 : loss : 0.017679, loss_ce: 0.004779
2022-01-10 01:22:51,935 iteration 4850 : loss : 0.023774, loss_ce: 0.007503
2022-01-10 01:22:53,618 iteration 4851 : loss : 0.033156, loss_ce: 0.017101
2022-01-10 01:22:55,182 iteration 4852 : loss : 0.023597, loss_ce: 0.008218
2022-01-10 01:22:56,767 iteration 4853 : loss : 0.019841, loss_ce: 0.008679
2022-01-10 01:22:58,352 iteration 4854 : loss : 0.019768, loss_ce: 0.007885
2022-01-10 01:22:59,899 iteration 4855 : loss : 0.027280, loss_ce: 0.007866
2022-01-10 01:23:01,568 iteration 4856 : loss : 0.029629, loss_ce: 0.011546
2022-01-10 01:23:03,114 iteration 4857 : loss : 0.014290, loss_ce: 0.005176
2022-01-10 01:23:04,768 iteration 4858 : loss : 0.019779, loss_ce: 0.007751
2022-01-10 01:23:06,285 iteration 4859 : loss : 0.014044, loss_ce: 0.005719
2022-01-10 01:23:07,902 iteration 4860 : loss : 0.021087, loss_ce: 0.009850
2022-01-10 01:23:09,450 iteration 4861 : loss : 0.024512, loss_ce: 0.007947
2022-01-10 01:23:11,067 iteration 4862 : loss : 0.020318, loss_ce: 0.011438
 72%|████████████████████▋        | 286/400 [2:19:46<56:26, 29.70s/it]2022-01-10 01:23:12,776 iteration 4863 : loss : 0.029861, loss_ce: 0.009193
2022-01-10 01:23:14,350 iteration 4864 : loss : 0.021381, loss_ce: 0.007667
2022-01-10 01:23:15,902 iteration 4865 : loss : 0.018358, loss_ce: 0.008793
2022-01-10 01:23:17,543 iteration 4866 : loss : 0.021115, loss_ce: 0.007239
2022-01-10 01:23:19,132 iteration 4867 : loss : 0.019962, loss_ce: 0.006798
2022-01-10 01:23:20,613 iteration 4868 : loss : 0.017628, loss_ce: 0.007221
2022-01-10 01:23:22,283 iteration 4869 : loss : 0.030992, loss_ce: 0.014816
2022-01-10 01:23:23,858 iteration 4870 : loss : 0.013960, loss_ce: 0.005418
2022-01-10 01:23:25,535 iteration 4871 : loss : 0.032755, loss_ce: 0.010892
2022-01-10 01:23:27,045 iteration 4872 : loss : 0.015860, loss_ce: 0.005831
2022-01-10 01:23:28,718 iteration 4873 : loss : 0.021035, loss_ce: 0.009617
2022-01-10 01:23:30,320 iteration 4874 : loss : 0.021097, loss_ce: 0.006732
2022-01-10 01:23:31,880 iteration 4875 : loss : 0.022062, loss_ce: 0.008923
2022-01-10 01:23:33,372 iteration 4876 : loss : 0.023393, loss_ce: 0.007643
2022-01-10 01:23:34,996 iteration 4877 : loss : 0.038777, loss_ce: 0.020946
2022-01-10 01:23:36,645 iteration 4878 : loss : 0.034004, loss_ce: 0.009323
2022-01-10 01:23:38,227 iteration 4879 : loss : 0.016842, loss_ce: 0.005719
 72%|████████████████████▊        | 287/400 [2:20:13<54:30, 28.94s/it]2022-01-10 01:23:39,837 iteration 4880 : loss : 0.020911, loss_ce: 0.008458
2022-01-10 01:23:41,358 iteration 4881 : loss : 0.019192, loss_ce: 0.007612
2022-01-10 01:23:42,847 iteration 4882 : loss : 0.015015, loss_ce: 0.006474
2022-01-10 01:23:44,442 iteration 4883 : loss : 0.020525, loss_ce: 0.008098
2022-01-10 01:23:46,057 iteration 4884 : loss : 0.021208, loss_ce: 0.006326
2022-01-10 01:23:47,529 iteration 4885 : loss : 0.016064, loss_ce: 0.006013
2022-01-10 01:23:49,095 iteration 4886 : loss : 0.024689, loss_ce: 0.006690
2022-01-10 01:23:50,605 iteration 4887 : loss : 0.015868, loss_ce: 0.004688
2022-01-10 01:23:52,197 iteration 4888 : loss : 0.038904, loss_ce: 0.010775
2022-01-10 01:23:53,774 iteration 4889 : loss : 0.022520, loss_ce: 0.010024
2022-01-10 01:23:55,302 iteration 4890 : loss : 0.018245, loss_ce: 0.009456
2022-01-10 01:23:56,761 iteration 4891 : loss : 0.013221, loss_ce: 0.004817
2022-01-10 01:23:58,366 iteration 4892 : loss : 0.018996, loss_ce: 0.008513
2022-01-10 01:23:59,938 iteration 4893 : loss : 0.021196, loss_ce: 0.009455
2022-01-10 01:24:01,495 iteration 4894 : loss : 0.018705, loss_ce: 0.008659
2022-01-10 01:24:03,039 iteration 4895 : loss : 0.018676, loss_ce: 0.003720
2022-01-10 01:24:04,565 iteration 4896 : loss : 0.029222, loss_ce: 0.008978
 72%|████████████████████▉        | 288/400 [2:20:39<52:34, 28.16s/it]2022-01-10 01:24:06,160 iteration 4897 : loss : 0.018327, loss_ce: 0.006835
2022-01-10 01:24:07,789 iteration 4898 : loss : 0.022893, loss_ce: 0.009945
2022-01-10 01:24:09,363 iteration 4899 : loss : 0.017356, loss_ce: 0.005351
2022-01-10 01:24:10,899 iteration 4900 : loss : 0.020912, loss_ce: 0.007568
2022-01-10 01:24:12,447 iteration 4901 : loss : 0.032165, loss_ce: 0.010555
2022-01-10 01:24:13,995 iteration 4902 : loss : 0.015466, loss_ce: 0.005195
2022-01-10 01:24:15,583 iteration 4903 : loss : 0.020394, loss_ce: 0.009032
2022-01-10 01:24:17,054 iteration 4904 : loss : 0.016155, loss_ce: 0.005269
2022-01-10 01:24:18,579 iteration 4905 : loss : 0.021041, loss_ce: 0.007019
2022-01-10 01:24:20,174 iteration 4906 : loss : 0.019711, loss_ce: 0.007102
2022-01-10 01:24:21,699 iteration 4907 : loss : 0.020183, loss_ce: 0.008041
2022-01-10 01:24:23,338 iteration 4908 : loss : 0.022359, loss_ce: 0.009777
2022-01-10 01:24:24,938 iteration 4909 : loss : 0.019510, loss_ce: 0.005959
2022-01-10 01:24:26,464 iteration 4910 : loss : 0.017625, loss_ce: 0.008537
2022-01-10 01:24:28,005 iteration 4911 : loss : 0.013307, loss_ce: 0.004887
2022-01-10 01:24:29,575 iteration 4912 : loss : 0.020376, loss_ce: 0.009299
2022-01-10 01:24:31,133 iteration 4913 : loss : 0.024178, loss_ce: 0.009219
 72%|████████████████████▉        | 289/400 [2:21:06<51:12, 27.68s/it]2022-01-10 01:24:32,788 iteration 4914 : loss : 0.025380, loss_ce: 0.009404
2022-01-10 01:24:34,321 iteration 4915 : loss : 0.023185, loss_ce: 0.009343
2022-01-10 01:24:35,860 iteration 4916 : loss : 0.018674, loss_ce: 0.004987
2022-01-10 01:24:37,428 iteration 4917 : loss : 0.017920, loss_ce: 0.005243
2022-01-10 01:24:39,021 iteration 4918 : loss : 0.030323, loss_ce: 0.015927
2022-01-10 01:24:40,742 iteration 4919 : loss : 0.060307, loss_ce: 0.038045
2022-01-10 01:24:42,302 iteration 4920 : loss : 0.015810, loss_ce: 0.006181
2022-01-10 01:24:43,828 iteration 4921 : loss : 0.017953, loss_ce: 0.008713
2022-01-10 01:24:45,332 iteration 4922 : loss : 0.018525, loss_ce: 0.004937
2022-01-10 01:24:46,945 iteration 4923 : loss : 0.029420, loss_ce: 0.010965
2022-01-10 01:24:48,588 iteration 4924 : loss : 0.021045, loss_ce: 0.011187
2022-01-10 01:24:50,184 iteration 4925 : loss : 0.020885, loss_ce: 0.007959
2022-01-10 01:24:51,810 iteration 4926 : loss : 0.021609, loss_ce: 0.010953
2022-01-10 01:24:53,369 iteration 4927 : loss : 0.026932, loss_ce: 0.008855
2022-01-10 01:24:55,002 iteration 4928 : loss : 0.055585, loss_ce: 0.020735
2022-01-10 01:24:56,659 iteration 4929 : loss : 0.023977, loss_ce: 0.010018
2022-01-10 01:24:56,659 Training Data Eval:
2022-01-10 01:25:04,706   Average segmentation loss on training set: 0.0128
2022-01-10 01:25:04,706 Validation Data Eval:
2022-01-10 01:25:07,484   Average segmentation loss on validation set: 0.0739
2022-01-10 01:25:09,068 iteration 4930 : loss : 0.020078, loss_ce: 0.006608
 72%|█████████████████████        | 290/400 [2:21:44<56:23, 30.76s/it]2022-01-10 01:25:10,683 iteration 4931 : loss : 0.018129, loss_ce: 0.008019
2022-01-10 01:25:12,282 iteration 4932 : loss : 0.021938, loss_ce: 0.011668
2022-01-10 01:25:13,798 iteration 4933 : loss : 0.018698, loss_ce: 0.010751
2022-01-10 01:25:15,429 iteration 4934 : loss : 0.022176, loss_ce: 0.007218
2022-01-10 01:25:17,002 iteration 4935 : loss : 0.020335, loss_ce: 0.006795
2022-01-10 01:25:18,596 iteration 4936 : loss : 0.017747, loss_ce: 0.006303
2022-01-10 01:25:20,182 iteration 4937 : loss : 0.020594, loss_ce: 0.009262
2022-01-10 01:25:21,808 iteration 4938 : loss : 0.036844, loss_ce: 0.012954
2022-01-10 01:25:23,433 iteration 4939 : loss : 0.033343, loss_ce: 0.011364
2022-01-10 01:25:24,992 iteration 4940 : loss : 0.025665, loss_ce: 0.010064
2022-01-10 01:25:26,583 iteration 4941 : loss : 0.024111, loss_ce: 0.011574
2022-01-10 01:25:28,175 iteration 4942 : loss : 0.029938, loss_ce: 0.009725
2022-01-10 01:25:29,850 iteration 4943 : loss : 0.053790, loss_ce: 0.010443
2022-01-10 01:25:31,433 iteration 4944 : loss : 0.029186, loss_ce: 0.012709
2022-01-10 01:25:33,056 iteration 4945 : loss : 0.023684, loss_ce: 0.009567
2022-01-10 01:25:34,698 iteration 4946 : loss : 0.024175, loss_ce: 0.006831
2022-01-10 01:25:36,261 iteration 4947 : loss : 0.033301, loss_ce: 0.010367
 73%|█████████████████████        | 291/400 [2:22:11<53:56, 29.69s/it]2022-01-10 01:25:37,921 iteration 4948 : loss : 0.025453, loss_ce: 0.009331
2022-01-10 01:25:39,480 iteration 4949 : loss : 0.044474, loss_ce: 0.020786
2022-01-10 01:25:41,099 iteration 4950 : loss : 0.034098, loss_ce: 0.012669
2022-01-10 01:25:42,708 iteration 4951 : loss : 0.026264, loss_ce: 0.008681
2022-01-10 01:25:44,306 iteration 4952 : loss : 0.033385, loss_ce: 0.019260
2022-01-10 01:25:45,926 iteration 4953 : loss : 0.030311, loss_ce: 0.010823
2022-01-10 01:25:47,479 iteration 4954 : loss : 0.027223, loss_ce: 0.011073
2022-01-10 01:25:48,995 iteration 4955 : loss : 0.038022, loss_ce: 0.009256
2022-01-10 01:25:50,560 iteration 4956 : loss : 0.020621, loss_ce: 0.006657
2022-01-10 01:25:52,048 iteration 4957 : loss : 0.022333, loss_ce: 0.008509
2022-01-10 01:25:53,632 iteration 4958 : loss : 0.037654, loss_ce: 0.019954
2022-01-10 01:25:55,216 iteration 4959 : loss : 0.041220, loss_ce: 0.019695
2022-01-10 01:25:56,721 iteration 4960 : loss : 0.019881, loss_ce: 0.007458
2022-01-10 01:25:58,305 iteration 4961 : loss : 0.022944, loss_ce: 0.008888
2022-01-10 01:25:59,824 iteration 4962 : loss : 0.032458, loss_ce: 0.006807
2022-01-10 01:26:01,373 iteration 4963 : loss : 0.021067, loss_ce: 0.008515
2022-01-10 01:26:02,995 iteration 4964 : loss : 0.024310, loss_ce: 0.008099
 73%|█████████████████████▏       | 292/400 [2:22:38<51:50, 28.80s/it]2022-01-10 01:26:04,693 iteration 4965 : loss : 0.034370, loss_ce: 0.014253
2022-01-10 01:26:06,329 iteration 4966 : loss : 0.039980, loss_ce: 0.015410
2022-01-10 01:26:07,917 iteration 4967 : loss : 0.029091, loss_ce: 0.013136
2022-01-10 01:26:09,522 iteration 4968 : loss : 0.019688, loss_ce: 0.008786
2022-01-10 01:26:11,213 iteration 4969 : loss : 0.043737, loss_ce: 0.011055
2022-01-10 01:26:12,748 iteration 4970 : loss : 0.018848, loss_ce: 0.007767
2022-01-10 01:26:14,302 iteration 4971 : loss : 0.033767, loss_ce: 0.018656
2022-01-10 01:26:15,916 iteration 4972 : loss : 0.021906, loss_ce: 0.010217
2022-01-10 01:26:17,501 iteration 4973 : loss : 0.021559, loss_ce: 0.008278
2022-01-10 01:26:19,203 iteration 4974 : loss : 0.026789, loss_ce: 0.011259
2022-01-10 01:26:20,769 iteration 4975 : loss : 0.024557, loss_ce: 0.007948
2022-01-10 01:26:22,285 iteration 4976 : loss : 0.018876, loss_ce: 0.007245
2022-01-10 01:26:23,889 iteration 4977 : loss : 0.024739, loss_ce: 0.005715
2022-01-10 01:26:25,454 iteration 4978 : loss : 0.024970, loss_ce: 0.006694
2022-01-10 01:26:27,028 iteration 4979 : loss : 0.023910, loss_ce: 0.008820
2022-01-10 01:26:28,614 iteration 4980 : loss : 0.024137, loss_ce: 0.007315
2022-01-10 01:26:30,184 iteration 4981 : loss : 0.023522, loss_ce: 0.008386
 73%|█████████████████████▏       | 293/400 [2:23:05<50:30, 28.32s/it]2022-01-10 01:26:31,793 iteration 4982 : loss : 0.017293, loss_ce: 0.007943
2022-01-10 01:26:33,346 iteration 4983 : loss : 0.024512, loss_ce: 0.010464
2022-01-10 01:26:35,017 iteration 4984 : loss : 0.023550, loss_ce: 0.008309
2022-01-10 01:26:36,525 iteration 4985 : loss : 0.019375, loss_ce: 0.005655
2022-01-10 01:26:38,183 iteration 4986 : loss : 0.030043, loss_ce: 0.013126
2022-01-10 01:26:39,708 iteration 4987 : loss : 0.017037, loss_ce: 0.005639
2022-01-10 01:26:41,299 iteration 4988 : loss : 0.026031, loss_ce: 0.009421
2022-01-10 01:26:42,931 iteration 4989 : loss : 0.028467, loss_ce: 0.010244
2022-01-10 01:26:44,589 iteration 4990 : loss : 0.023210, loss_ce: 0.009907
2022-01-10 01:26:46,182 iteration 4991 : loss : 0.026269, loss_ce: 0.008576
2022-01-10 01:26:47,674 iteration 4992 : loss : 0.019020, loss_ce: 0.009569
2022-01-10 01:26:49,233 iteration 4993 : loss : 0.023624, loss_ce: 0.007868
2022-01-10 01:26:50,756 iteration 4994 : loss : 0.021300, loss_ce: 0.007026
2022-01-10 01:26:52,294 iteration 4995 : loss : 0.019599, loss_ce: 0.008586
2022-01-10 01:26:53,924 iteration 4996 : loss : 0.026243, loss_ce: 0.008486
2022-01-10 01:26:55,430 iteration 4997 : loss : 0.017036, loss_ce: 0.005035
2022-01-10 01:26:56,951 iteration 4998 : loss : 0.017877, loss_ce: 0.007021
 74%|█████████████████████▎       | 294/400 [2:23:32<49:12, 27.85s/it]2022-01-10 01:26:58,548 iteration 4999 : loss : 0.015192, loss_ce: 0.005674
2022-01-10 01:27:00,103 iteration 5000 : loss : 0.015300, loss_ce: 0.005867
2022-01-10 01:27:01,663 iteration 5001 : loss : 0.020909, loss_ce: 0.009214
2022-01-10 01:27:03,215 iteration 5002 : loss : 0.015098, loss_ce: 0.005120
2022-01-10 01:27:04,811 iteration 5003 : loss : 0.055539, loss_ce: 0.009862
2022-01-10 01:27:06,374 iteration 5004 : loss : 0.020885, loss_ce: 0.008740
2022-01-10 01:27:07,945 iteration 5005 : loss : 0.019503, loss_ce: 0.009260
2022-01-10 01:27:09,572 iteration 5006 : loss : 0.032157, loss_ce: 0.011969
2022-01-10 01:27:11,182 iteration 5007 : loss : 0.038628, loss_ce: 0.018140
2022-01-10 01:27:12,801 iteration 5008 : loss : 0.027310, loss_ce: 0.012044
2022-01-10 01:27:14,419 iteration 5009 : loss : 0.026234, loss_ce: 0.007244
2022-01-10 01:27:16,023 iteration 5010 : loss : 0.031743, loss_ce: 0.010681
2022-01-10 01:27:17,582 iteration 5011 : loss : 0.018000, loss_ce: 0.008313
2022-01-10 01:27:19,173 iteration 5012 : loss : 0.026419, loss_ce: 0.012300
2022-01-10 01:27:20,693 iteration 5013 : loss : 0.020687, loss_ce: 0.008263
2022-01-10 01:27:22,261 iteration 5014 : loss : 0.027123, loss_ce: 0.011079
2022-01-10 01:27:22,261 Training Data Eval:
2022-01-10 01:27:30,307   Average segmentation loss on training set: 0.0133
2022-01-10 01:27:30,307 Validation Data Eval:
2022-01-10 01:27:33,084   Average segmentation loss on validation set: 0.0711
2022-01-10 01:27:34,652 iteration 5015 : loss : 0.023935, loss_ce: 0.007912
 74%|█████████████████████▍       | 295/400 [2:24:09<53:54, 30.81s/it]2022-01-10 01:27:36,279 iteration 5016 : loss : 0.024436, loss_ce: 0.011447
2022-01-10 01:27:37,830 iteration 5017 : loss : 0.017150, loss_ce: 0.006209
2022-01-10 01:27:39,394 iteration 5018 : loss : 0.022885, loss_ce: 0.010065
2022-01-10 01:27:40,980 iteration 5019 : loss : 0.019212, loss_ce: 0.006319
2022-01-10 01:27:42,575 iteration 5020 : loss : 0.030921, loss_ce: 0.008827
2022-01-10 01:27:44,192 iteration 5021 : loss : 0.030586, loss_ce: 0.015102
2022-01-10 01:27:45,715 iteration 5022 : loss : 0.021343, loss_ce: 0.007524
2022-01-10 01:27:47,287 iteration 5023 : loss : 0.034483, loss_ce: 0.010486
2022-01-10 01:27:48,954 iteration 5024 : loss : 0.052627, loss_ce: 0.009787
2022-01-10 01:27:50,596 iteration 5025 : loss : 0.028621, loss_ce: 0.005938
2022-01-10 01:27:52,184 iteration 5026 : loss : 0.025063, loss_ce: 0.009083
2022-01-10 01:27:53,726 iteration 5027 : loss : 0.020669, loss_ce: 0.009511
2022-01-10 01:27:55,279 iteration 5028 : loss : 0.021644, loss_ce: 0.008426
2022-01-10 01:27:56,936 iteration 5029 : loss : 0.054815, loss_ce: 0.015537
2022-01-10 01:27:58,512 iteration 5030 : loss : 0.022518, loss_ce: 0.011147
2022-01-10 01:28:00,152 iteration 5031 : loss : 0.032035, loss_ce: 0.014589
2022-01-10 01:28:01,739 iteration 5032 : loss : 0.022138, loss_ce: 0.009742
 74%|█████████████████████▍       | 296/400 [2:24:36<51:27, 29.69s/it]2022-01-10 01:28:03,485 iteration 5033 : loss : 0.030120, loss_ce: 0.010424
2022-01-10 01:28:05,047 iteration 5034 : loss : 0.025226, loss_ce: 0.006853
2022-01-10 01:28:06,595 iteration 5035 : loss : 0.024302, loss_ce: 0.010638
2022-01-10 01:28:08,149 iteration 5036 : loss : 0.020394, loss_ce: 0.007433
2022-01-10 01:28:09,733 iteration 5037 : loss : 0.025276, loss_ce: 0.007716
2022-01-10 01:28:11,281 iteration 5038 : loss : 0.030126, loss_ce: 0.007354
2022-01-10 01:28:12,939 iteration 5039 : loss : 0.025579, loss_ce: 0.012991
2022-01-10 01:28:14,526 iteration 5040 : loss : 0.023166, loss_ce: 0.007070
2022-01-10 01:28:16,171 iteration 5041 : loss : 0.022528, loss_ce: 0.007543
2022-01-10 01:28:17,714 iteration 5042 : loss : 0.015961, loss_ce: 0.006363
2022-01-10 01:28:19,292 iteration 5043 : loss : 0.020709, loss_ce: 0.009399
2022-01-10 01:28:20,875 iteration 5044 : loss : 0.024102, loss_ce: 0.011404
2022-01-10 01:28:22,504 iteration 5045 : loss : 0.031867, loss_ce: 0.012495
2022-01-10 01:28:24,099 iteration 5046 : loss : 0.024800, loss_ce: 0.006430
2022-01-10 01:28:25,662 iteration 5047 : loss : 0.031325, loss_ce: 0.008615
2022-01-10 01:28:27,215 iteration 5048 : loss : 0.022211, loss_ce: 0.008589
2022-01-10 01:28:28,769 iteration 5049 : loss : 0.016347, loss_ce: 0.006603
 74%|█████████████████████▌       | 297/400 [2:25:03<49:35, 28.89s/it]2022-01-10 01:28:30,411 iteration 5050 : loss : 0.021297, loss_ce: 0.006040
2022-01-10 01:28:31,974 iteration 5051 : loss : 0.025882, loss_ce: 0.008975
2022-01-10 01:28:33,544 iteration 5052 : loss : 0.020004, loss_ce: 0.009493
2022-01-10 01:28:35,098 iteration 5053 : loss : 0.019166, loss_ce: 0.007187
2022-01-10 01:28:36,672 iteration 5054 : loss : 0.016656, loss_ce: 0.005137
2022-01-10 01:28:38,229 iteration 5055 : loss : 0.018265, loss_ce: 0.005601
2022-01-10 01:28:39,775 iteration 5056 : loss : 0.029707, loss_ce: 0.006843
2022-01-10 01:28:41,400 iteration 5057 : loss : 0.023968, loss_ce: 0.013447
2022-01-10 01:28:43,040 iteration 5058 : loss : 0.028637, loss_ce: 0.011616
2022-01-10 01:28:44,505 iteration 5059 : loss : 0.016682, loss_ce: 0.004059
2022-01-10 01:28:46,081 iteration 5060 : loss : 0.049725, loss_ce: 0.013623
2022-01-10 01:28:47,685 iteration 5061 : loss : 0.031153, loss_ce: 0.011716
2022-01-10 01:28:49,278 iteration 5062 : loss : 0.032077, loss_ce: 0.010293
2022-01-10 01:28:50,917 iteration 5063 : loss : 0.024261, loss_ce: 0.009731
2022-01-10 01:28:52,458 iteration 5064 : loss : 0.022407, loss_ce: 0.007404
2022-01-10 01:28:54,003 iteration 5065 : loss : 0.026026, loss_ce: 0.009564
2022-01-10 01:28:55,583 iteration 5066 : loss : 0.015742, loss_ce: 0.008026
 74%|█████████████████████▌       | 298/400 [2:25:30<48:03, 28.27s/it]2022-01-10 01:28:57,302 iteration 5067 : loss : 0.030169, loss_ce: 0.010934
2022-01-10 01:28:58,904 iteration 5068 : loss : 0.026456, loss_ce: 0.010583
2022-01-10 01:29:00,500 iteration 5069 : loss : 0.021369, loss_ce: 0.006996
2022-01-10 01:29:02,063 iteration 5070 : loss : 0.018567, loss_ce: 0.005324
2022-01-10 01:29:03,706 iteration 5071 : loss : 0.028703, loss_ce: 0.012875
2022-01-10 01:29:05,170 iteration 5072 : loss : 0.020792, loss_ce: 0.006472
2022-01-10 01:29:06,738 iteration 5073 : loss : 0.019903, loss_ce: 0.010632
2022-01-10 01:29:08,350 iteration 5074 : loss : 0.025152, loss_ce: 0.008072
2022-01-10 01:29:09,931 iteration 5075 : loss : 0.026073, loss_ce: 0.010502
2022-01-10 01:29:11,559 iteration 5076 : loss : 0.020975, loss_ce: 0.010340
2022-01-10 01:29:13,110 iteration 5077 : loss : 0.018209, loss_ce: 0.006213
2022-01-10 01:29:14,700 iteration 5078 : loss : 0.016696, loss_ce: 0.004507
2022-01-10 01:29:16,253 iteration 5079 : loss : 0.026539, loss_ce: 0.012134
2022-01-10 01:29:17,838 iteration 5080 : loss : 0.023245, loss_ce: 0.008658
2022-01-10 01:29:19,449 iteration 5081 : loss : 0.030607, loss_ce: 0.019718
2022-01-10 01:29:21,084 iteration 5082 : loss : 0.023622, loss_ce: 0.011252
2022-01-10 01:29:22,687 iteration 5083 : loss : 0.026677, loss_ce: 0.008013
 75%|█████████████████████▋       | 299/400 [2:25:57<47:00, 27.92s/it]2022-01-10 01:29:24,302 iteration 5084 : loss : 0.017348, loss_ce: 0.006135
2022-01-10 01:29:25,880 iteration 5085 : loss : 0.031667, loss_ce: 0.010354
2022-01-10 01:29:27,429 iteration 5086 : loss : 0.016126, loss_ce: 0.005973
2022-01-10 01:29:29,055 iteration 5087 : loss : 0.021514, loss_ce: 0.007760
2022-01-10 01:29:30,652 iteration 5088 : loss : 0.020745, loss_ce: 0.007226
2022-01-10 01:29:32,184 iteration 5089 : loss : 0.017927, loss_ce: 0.005675
2022-01-10 01:29:33,732 iteration 5090 : loss : 0.025165, loss_ce: 0.009289
2022-01-10 01:29:35,403 iteration 5091 : loss : 0.036379, loss_ce: 0.015651
2022-01-10 01:29:36,982 iteration 5092 : loss : 0.014665, loss_ce: 0.006594
2022-01-10 01:29:38,526 iteration 5093 : loss : 0.020797, loss_ce: 0.009646
2022-01-10 01:29:40,046 iteration 5094 : loss : 0.016413, loss_ce: 0.005883
2022-01-10 01:29:41,663 iteration 5095 : loss : 0.031948, loss_ce: 0.012834
2022-01-10 01:29:43,193 iteration 5096 : loss : 0.015617, loss_ce: 0.005616
2022-01-10 01:29:44,736 iteration 5097 : loss : 0.017650, loss_ce: 0.007842
2022-01-10 01:29:46,333 iteration 5098 : loss : 0.026162, loss_ce: 0.008666
2022-01-10 01:29:47,900 iteration 5099 : loss : 0.019128, loss_ce: 0.006375
2022-01-10 01:29:47,900 Training Data Eval:
2022-01-10 01:29:55,944   Average segmentation loss on training set: 0.0132
2022-01-10 01:29:55,945 Validation Data Eval:
2022-01-10 01:29:58,716   Average segmentation loss on validation set: 0.0791
2022-01-10 01:30:00,289 iteration 5100 : loss : 0.014540, loss_ce: 0.006726
 75%|█████████████████████▊       | 300/400 [2:26:35<51:22, 30.83s/it]2022-01-10 01:30:01,942 iteration 5101 : loss : 0.019388, loss_ce: 0.008258
2022-01-10 01:30:03,489 iteration 5102 : loss : 0.013648, loss_ce: 0.005121
2022-01-10 01:30:05,052 iteration 5103 : loss : 0.024463, loss_ce: 0.010858
2022-01-10 01:30:06,694 iteration 5104 : loss : 0.020998, loss_ce: 0.009396
2022-01-10 01:30:08,217 iteration 5105 : loss : 0.019336, loss_ce: 0.006964
2022-01-10 01:30:09,861 iteration 5106 : loss : 0.027688, loss_ce: 0.010812
2022-01-10 01:30:11,384 iteration 5107 : loss : 0.017533, loss_ce: 0.006870
2022-01-10 01:30:12,983 iteration 5108 : loss : 0.029128, loss_ce: 0.011293
2022-01-10 01:30:14,564 iteration 5109 : loss : 0.022343, loss_ce: 0.007225
2022-01-10 01:30:16,134 iteration 5110 : loss : 0.022919, loss_ce: 0.010747
2022-01-10 01:30:17,739 iteration 5111 : loss : 0.018638, loss_ce: 0.005881
2022-01-10 01:30:19,265 iteration 5112 : loss : 0.026682, loss_ce: 0.008322
2022-01-10 01:30:20,897 iteration 5113 : loss : 0.049135, loss_ce: 0.011250
2022-01-10 01:30:22,458 iteration 5114 : loss : 0.027635, loss_ce: 0.013152
2022-01-10 01:30:23,995 iteration 5115 : loss : 0.025282, loss_ce: 0.007886
2022-01-10 01:30:25,542 iteration 5116 : loss : 0.015686, loss_ce: 0.006043
2022-01-10 01:30:27,175 iteration 5117 : loss : 0.023616, loss_ce: 0.011191
 75%|█████████████████████▊       | 301/400 [2:27:02<48:54, 29.64s/it]2022-01-10 01:30:28,821 iteration 5118 : loss : 0.016432, loss_ce: 0.006366
2022-01-10 01:30:30,410 iteration 5119 : loss : 0.018871, loss_ce: 0.007068
2022-01-10 01:30:31,953 iteration 5120 : loss : 0.024488, loss_ce: 0.007323
2022-01-10 01:30:33,528 iteration 5121 : loss : 0.025043, loss_ce: 0.013268
2022-01-10 01:30:35,063 iteration 5122 : loss : 0.022805, loss_ce: 0.008885
2022-01-10 01:30:36,629 iteration 5123 : loss : 0.044457, loss_ce: 0.012884
2022-01-10 01:30:38,227 iteration 5124 : loss : 0.020916, loss_ce: 0.008586
2022-01-10 01:30:39,710 iteration 5125 : loss : 0.019954, loss_ce: 0.009569
2022-01-10 01:30:41,231 iteration 5126 : loss : 0.027856, loss_ce: 0.009753
2022-01-10 01:30:42,844 iteration 5127 : loss : 0.026553, loss_ce: 0.008451
2022-01-10 01:30:44,392 iteration 5128 : loss : 0.015439, loss_ce: 0.006491
2022-01-10 01:30:45,984 iteration 5129 : loss : 0.020227, loss_ce: 0.006398
2022-01-10 01:30:47,559 iteration 5130 : loss : 0.018579, loss_ce: 0.005959
2022-01-10 01:30:49,050 iteration 5131 : loss : 0.012791, loss_ce: 0.004675
2022-01-10 01:30:50,610 iteration 5132 : loss : 0.019527, loss_ce: 0.008202
2022-01-10 01:30:52,142 iteration 5133 : loss : 0.024967, loss_ce: 0.007958
2022-01-10 01:30:53,660 iteration 5134 : loss : 0.018194, loss_ce: 0.007475
 76%|█████████████████████▉       | 302/400 [2:27:28<46:52, 28.70s/it]2022-01-10 01:30:55,426 iteration 5135 : loss : 0.027580, loss_ce: 0.010774
2022-01-10 01:30:56,995 iteration 5136 : loss : 0.028467, loss_ce: 0.009184
2022-01-10 01:30:58,618 iteration 5137 : loss : 0.028249, loss_ce: 0.011649
2022-01-10 01:31:00,155 iteration 5138 : loss : 0.017980, loss_ce: 0.007832
2022-01-10 01:31:01,732 iteration 5139 : loss : 0.015628, loss_ce: 0.005622
2022-01-10 01:31:03,391 iteration 5140 : loss : 0.018864, loss_ce: 0.007455
2022-01-10 01:31:04,926 iteration 5141 : loss : 0.023351, loss_ce: 0.008567
2022-01-10 01:31:06,510 iteration 5142 : loss : 0.028295, loss_ce: 0.009678
2022-01-10 01:31:08,010 iteration 5143 : loss : 0.016107, loss_ce: 0.006154
2022-01-10 01:31:09,609 iteration 5144 : loss : 0.034645, loss_ce: 0.018601
2022-01-10 01:31:11,104 iteration 5145 : loss : 0.014386, loss_ce: 0.004697
2022-01-10 01:31:12,745 iteration 5146 : loss : 0.026780, loss_ce: 0.011583
2022-01-10 01:31:14,385 iteration 5147 : loss : 0.017982, loss_ce: 0.006304
2022-01-10 01:31:15,939 iteration 5148 : loss : 0.017623, loss_ce: 0.007098
2022-01-10 01:31:17,492 iteration 5149 : loss : 0.032992, loss_ce: 0.006895
2022-01-10 01:31:19,047 iteration 5150 : loss : 0.014028, loss_ce: 0.005601
2022-01-10 01:31:20,627 iteration 5151 : loss : 0.032739, loss_ce: 0.007011
 76%|█████████████████████▉       | 303/400 [2:27:55<45:33, 28.18s/it]2022-01-10 01:31:22,209 iteration 5152 : loss : 0.022322, loss_ce: 0.005162
2022-01-10 01:31:23,692 iteration 5153 : loss : 0.013299, loss_ce: 0.005140
2022-01-10 01:31:25,322 iteration 5154 : loss : 0.025158, loss_ce: 0.009774
2022-01-10 01:31:26,833 iteration 5155 : loss : 0.013704, loss_ce: 0.004760
2022-01-10 01:31:28,385 iteration 5156 : loss : 0.020000, loss_ce: 0.008008
2022-01-10 01:31:29,891 iteration 5157 : loss : 0.013430, loss_ce: 0.004286
2022-01-10 01:31:31,438 iteration 5158 : loss : 0.022169, loss_ce: 0.008282
2022-01-10 01:31:33,063 iteration 5159 : loss : 0.025081, loss_ce: 0.008564
2022-01-10 01:31:34,574 iteration 5160 : loss : 0.015262, loss_ce: 0.005342
2022-01-10 01:31:36,129 iteration 5161 : loss : 0.023574, loss_ce: 0.011454
2022-01-10 01:31:37,642 iteration 5162 : loss : 0.019815, loss_ce: 0.008619
2022-01-10 01:31:39,223 iteration 5163 : loss : 0.019864, loss_ce: 0.007811
2022-01-10 01:31:40,810 iteration 5164 : loss : 0.019708, loss_ce: 0.009740
2022-01-10 01:31:42,316 iteration 5165 : loss : 0.017396, loss_ce: 0.005298
2022-01-10 01:31:43,879 iteration 5166 : loss : 0.015154, loss_ce: 0.006446
2022-01-10 01:31:45,462 iteration 5167 : loss : 0.021172, loss_ce: 0.007366
2022-01-10 01:31:47,083 iteration 5168 : loss : 0.017773, loss_ce: 0.006932
 76%|██████████████████████       | 304/400 [2:28:22<44:15, 27.66s/it]2022-01-10 01:31:48,819 iteration 5169 : loss : 0.027648, loss_ce: 0.011841
2022-01-10 01:31:50,342 iteration 5170 : loss : 0.019492, loss_ce: 0.005591
2022-01-10 01:31:51,972 iteration 5171 : loss : 0.019024, loss_ce: 0.007844
2022-01-10 01:31:53,470 iteration 5172 : loss : 0.018549, loss_ce: 0.005434
2022-01-10 01:31:54,915 iteration 5173 : loss : 0.013291, loss_ce: 0.005700
2022-01-10 01:31:56,548 iteration 5174 : loss : 0.027226, loss_ce: 0.009480
2022-01-10 01:31:58,082 iteration 5175 : loss : 0.020095, loss_ce: 0.009528
2022-01-10 01:31:59,776 iteration 5176 : loss : 0.024917, loss_ce: 0.010588
2022-01-10 01:32:01,308 iteration 5177 : loss : 0.019855, loss_ce: 0.008410
2022-01-10 01:32:02,865 iteration 5178 : loss : 0.019889, loss_ce: 0.007820
2022-01-10 01:32:04,482 iteration 5179 : loss : 0.020756, loss_ce: 0.005850
2022-01-10 01:32:06,051 iteration 5180 : loss : 0.019921, loss_ce: 0.007384
2022-01-10 01:32:07,575 iteration 5181 : loss : 0.018547, loss_ce: 0.004555
2022-01-10 01:32:09,140 iteration 5182 : loss : 0.018316, loss_ce: 0.006172
2022-01-10 01:32:10,684 iteration 5183 : loss : 0.031002, loss_ce: 0.012710
2022-01-10 01:32:12,215 iteration 5184 : loss : 0.025975, loss_ce: 0.010142
2022-01-10 01:32:12,215 Training Data Eval:
2022-01-10 01:32:20,256   Average segmentation loss on training set: 0.0138
2022-01-10 01:32:20,256 Validation Data Eval:
2022-01-10 01:32:23,027   Average segmentation loss on validation set: 0.0681
2022-01-10 01:32:24,581 iteration 5185 : loss : 0.021606, loss_ce: 0.007756
 76%|██████████████████████       | 305/400 [2:28:59<48:28, 30.61s/it]2022-01-10 01:32:26,225 iteration 5186 : loss : 0.017972, loss_ce: 0.008014
2022-01-10 01:32:27,745 iteration 5187 : loss : 0.022104, loss_ce: 0.008025
2022-01-10 01:32:29,274 iteration 5188 : loss : 0.015576, loss_ce: 0.006153
2022-01-10 01:32:30,810 iteration 5189 : loss : 0.016154, loss_ce: 0.006779
2022-01-10 01:32:32,327 iteration 5190 : loss : 0.020127, loss_ce: 0.008772
2022-01-10 01:32:33,942 iteration 5191 : loss : 0.022645, loss_ce: 0.007608
2022-01-10 01:32:35,540 iteration 5192 : loss : 0.022872, loss_ce: 0.007041
2022-01-10 01:32:37,059 iteration 5193 : loss : 0.013243, loss_ce: 0.004535
2022-01-10 01:32:38,655 iteration 5194 : loss : 0.024092, loss_ce: 0.009782
2022-01-10 01:32:40,192 iteration 5195 : loss : 0.016679, loss_ce: 0.005484
2022-01-10 01:32:41,663 iteration 5196 : loss : 0.016097, loss_ce: 0.003829
2022-01-10 01:32:43,170 iteration 5197 : loss : 0.015332, loss_ce: 0.004896
2022-01-10 01:32:44,693 iteration 5198 : loss : 0.015771, loss_ce: 0.007360
2022-01-10 01:32:46,282 iteration 5199 : loss : 0.014531, loss_ce: 0.005209
2022-01-10 01:32:47,840 iteration 5200 : loss : 0.022754, loss_ce: 0.006625
2022-01-10 01:32:49,419 iteration 5201 : loss : 0.018899, loss_ce: 0.006279
2022-01-10 01:32:51,061 iteration 5202 : loss : 0.029206, loss_ce: 0.014016
 76%|██████████████████████▏      | 306/400 [2:29:26<46:00, 29.37s/it]2022-01-10 01:32:52,654 iteration 5203 : loss : 0.014053, loss_ce: 0.004998
2022-01-10 01:32:54,196 iteration 5204 : loss : 0.024811, loss_ce: 0.011210
2022-01-10 01:32:55,883 iteration 5205 : loss : 0.024574, loss_ce: 0.009424
2022-01-10 01:32:57,447 iteration 5206 : loss : 0.020441, loss_ce: 0.007715
2022-01-10 01:32:59,022 iteration 5207 : loss : 0.019005, loss_ce: 0.008152
2022-01-10 01:33:00,659 iteration 5208 : loss : 0.020864, loss_ce: 0.010544
2022-01-10 01:33:02,182 iteration 5209 : loss : 0.015396, loss_ce: 0.006393
2022-01-10 01:33:03,793 iteration 5210 : loss : 0.023688, loss_ce: 0.009454
2022-01-10 01:33:05,382 iteration 5211 : loss : 0.017745, loss_ce: 0.007435
2022-01-10 01:33:06,958 iteration 5212 : loss : 0.019097, loss_ce: 0.006828
2022-01-10 01:33:08,424 iteration 5213 : loss : 0.014803, loss_ce: 0.005606
2022-01-10 01:33:10,079 iteration 5214 : loss : 0.034479, loss_ce: 0.014386
2022-01-10 01:33:11,680 iteration 5215 : loss : 0.017484, loss_ce: 0.005687
2022-01-10 01:33:13,302 iteration 5216 : loss : 0.017974, loss_ce: 0.005929
2022-01-10 01:33:14,955 iteration 5217 : loss : 0.027514, loss_ce: 0.011135
2022-01-10 01:33:16,489 iteration 5218 : loss : 0.021107, loss_ce: 0.003146
2022-01-10 01:33:18,152 iteration 5219 : loss : 0.022441, loss_ce: 0.006193
 77%|██████████████████████▎      | 307/400 [2:29:53<44:28, 28.69s/it]2022-01-10 01:33:19,769 iteration 5220 : loss : 0.017380, loss_ce: 0.005545
2022-01-10 01:33:21,332 iteration 5221 : loss : 0.019110, loss_ce: 0.006382
2022-01-10 01:33:22,849 iteration 5222 : loss : 0.020353, loss_ce: 0.007325
2022-01-10 01:33:24,461 iteration 5223 : loss : 0.023053, loss_ce: 0.009226
2022-01-10 01:33:26,018 iteration 5224 : loss : 0.018327, loss_ce: 0.005481
2022-01-10 01:33:27,640 iteration 5225 : loss : 0.018292, loss_ce: 0.005298
2022-01-10 01:33:29,196 iteration 5226 : loss : 0.024553, loss_ce: 0.009005
2022-01-10 01:33:30,852 iteration 5227 : loss : 0.028364, loss_ce: 0.012603
2022-01-10 01:33:32,395 iteration 5228 : loss : 0.016661, loss_ce: 0.005059
2022-01-10 01:33:34,084 iteration 5229 : loss : 0.025712, loss_ce: 0.010384
2022-01-10 01:33:35,553 iteration 5230 : loss : 0.013452, loss_ce: 0.005839
2022-01-10 01:33:37,146 iteration 5231 : loss : 0.021228, loss_ce: 0.009393
2022-01-10 01:33:38,724 iteration 5232 : loss : 0.022245, loss_ce: 0.007407
2022-01-10 01:33:40,332 iteration 5233 : loss : 0.019057, loss_ce: 0.008504
2022-01-10 01:33:41,873 iteration 5234 : loss : 0.022522, loss_ce: 0.009183
2022-01-10 01:33:43,457 iteration 5235 : loss : 0.022940, loss_ce: 0.008263
2022-01-10 01:33:45,075 iteration 5236 : loss : 0.025416, loss_ce: 0.009018
 77%|██████████████████████▎      | 308/400 [2:30:20<43:10, 28.16s/it]2022-01-10 01:33:46,592 iteration 5237 : loss : 0.013846, loss_ce: 0.005721
2022-01-10 01:33:48,198 iteration 5238 : loss : 0.020575, loss_ce: 0.008647
2022-01-10 01:33:49,789 iteration 5239 : loss : 0.029230, loss_ce: 0.008497
2022-01-10 01:33:51,402 iteration 5240 : loss : 0.177386, loss_ce: 0.006573
2022-01-10 01:33:52,945 iteration 5241 : loss : 0.015426, loss_ce: 0.006232
2022-01-10 01:33:54,516 iteration 5242 : loss : 0.020703, loss_ce: 0.009500
2022-01-10 01:33:56,173 iteration 5243 : loss : 0.024996, loss_ce: 0.008099
2022-01-10 01:33:57,782 iteration 5244 : loss : 0.017674, loss_ce: 0.008010
2022-01-10 01:33:59,369 iteration 5245 : loss : 0.017757, loss_ce: 0.005946
2022-01-10 01:34:00,963 iteration 5246 : loss : 0.021575, loss_ce: 0.006911
2022-01-10 01:34:02,457 iteration 5247 : loss : 0.015077, loss_ce: 0.006119
2022-01-10 01:34:04,065 iteration 5248 : loss : 0.015600, loss_ce: 0.005107
2022-01-10 01:34:05,720 iteration 5249 : loss : 0.039195, loss_ce: 0.014203
2022-01-10 01:34:07,324 iteration 5250 : loss : 0.024194, loss_ce: 0.013521
2022-01-10 01:34:08,822 iteration 5251 : loss : 0.015964, loss_ce: 0.005520
2022-01-10 01:34:10,400 iteration 5252 : loss : 0.016507, loss_ce: 0.006751
2022-01-10 01:34:12,002 iteration 5253 : loss : 0.041305, loss_ce: 0.009284
 77%|██████████████████████▍      | 309/400 [2:30:47<42:08, 27.79s/it]2022-01-10 01:34:13,631 iteration 5254 : loss : 0.014604, loss_ce: 0.005358
2022-01-10 01:34:15,200 iteration 5255 : loss : 0.022443, loss_ce: 0.011855
2022-01-10 01:34:16,709 iteration 5256 : loss : 0.016749, loss_ce: 0.007001
2022-01-10 01:34:18,286 iteration 5257 : loss : 0.017223, loss_ce: 0.006827
2022-01-10 01:34:19,895 iteration 5258 : loss : 0.017644, loss_ce: 0.004549
2022-01-10 01:34:21,417 iteration 5259 : loss : 0.019146, loss_ce: 0.004597
2022-01-10 01:34:23,001 iteration 5260 : loss : 0.021885, loss_ce: 0.008880
2022-01-10 01:34:24,597 iteration 5261 : loss : 0.026167, loss_ce: 0.012185
2022-01-10 01:34:26,176 iteration 5262 : loss : 0.014536, loss_ce: 0.006295
2022-01-10 01:34:27,760 iteration 5263 : loss : 0.026892, loss_ce: 0.014164
2022-01-10 01:34:29,282 iteration 5264 : loss : 0.015212, loss_ce: 0.006642
2022-01-10 01:34:30,893 iteration 5265 : loss : 0.020723, loss_ce: 0.007520
2022-01-10 01:34:32,551 iteration 5266 : loss : 0.020831, loss_ce: 0.006579
2022-01-10 01:34:34,127 iteration 5267 : loss : 0.026189, loss_ce: 0.006993
2022-01-10 01:34:35,717 iteration 5268 : loss : 0.021473, loss_ce: 0.006062
2022-01-10 01:34:37,401 iteration 5269 : loss : 0.030222, loss_ce: 0.010452
2022-01-10 01:34:37,401 Training Data Eval:
2022-01-10 01:34:45,441   Average segmentation loss on training set: 0.0114
2022-01-10 01:34:45,441 Validation Data Eval:
2022-01-10 01:34:48,213   Average segmentation loss on validation set: 0.0804
2022-01-10 01:34:49,781 iteration 5270 : loss : 0.019540, loss_ce: 0.009246
 78%|██████████████████████▍      | 310/400 [2:31:24<46:10, 30.78s/it]2022-01-10 01:34:51,380 iteration 5271 : loss : 0.023762, loss_ce: 0.009158
2022-01-10 01:34:52,974 iteration 5272 : loss : 0.020455, loss_ce: 0.009317
2022-01-10 01:34:54,515 iteration 5273 : loss : 0.017814, loss_ce: 0.004488
2022-01-10 01:34:56,070 iteration 5274 : loss : 0.021773, loss_ce: 0.007810
2022-01-10 01:34:57,567 iteration 5275 : loss : 0.014266, loss_ce: 0.006530
2022-01-10 01:34:59,162 iteration 5276 : loss : 0.022879, loss_ce: 0.009491
2022-01-10 01:35:00,738 iteration 5277 : loss : 0.017421, loss_ce: 0.006119
2022-01-10 01:35:02,226 iteration 5278 : loss : 0.015356, loss_ce: 0.005952
2022-01-10 01:35:03,796 iteration 5279 : loss : 0.017779, loss_ce: 0.007934
2022-01-10 01:35:05,495 iteration 5280 : loss : 0.020547, loss_ce: 0.007829
2022-01-10 01:35:07,049 iteration 5281 : loss : 0.017272, loss_ce: 0.005854
2022-01-10 01:35:08,555 iteration 5282 : loss : 0.013971, loss_ce: 0.004222
2022-01-10 01:35:10,131 iteration 5283 : loss : 0.026747, loss_ce: 0.007765
2022-01-10 01:35:11,722 iteration 5284 : loss : 0.033062, loss_ce: 0.011720
2022-01-10 01:35:13,284 iteration 5285 : loss : 0.018542, loss_ce: 0.007935
2022-01-10 01:35:14,773 iteration 5286 : loss : 0.014961, loss_ce: 0.006436
2022-01-10 01:35:16,323 iteration 5287 : loss : 0.018920, loss_ce: 0.003835
 78%|██████████████████████▌      | 311/400 [2:31:51<43:46, 29.51s/it]2022-01-10 01:35:17,983 iteration 5288 : loss : 0.020110, loss_ce: 0.007500
2022-01-10 01:35:19,586 iteration 5289 : loss : 0.033787, loss_ce: 0.014134
2022-01-10 01:35:21,147 iteration 5290 : loss : 0.026773, loss_ce: 0.007737
2022-01-10 01:35:22,692 iteration 5291 : loss : 0.024722, loss_ce: 0.008818
2022-01-10 01:35:24,298 iteration 5292 : loss : 0.024066, loss_ce: 0.009471
2022-01-10 01:35:25,887 iteration 5293 : loss : 0.018609, loss_ce: 0.008830
2022-01-10 01:35:27,506 iteration 5294 : loss : 0.023588, loss_ce: 0.008157
2022-01-10 01:35:29,172 iteration 5295 : loss : 0.041658, loss_ce: 0.015140
2022-01-10 01:35:30,789 iteration 5296 : loss : 0.033439, loss_ce: 0.008210
2022-01-10 01:35:32,407 iteration 5297 : loss : 0.016658, loss_ce: 0.005578
2022-01-10 01:35:34,031 iteration 5298 : loss : 0.026325, loss_ce: 0.011391
2022-01-10 01:35:35,645 iteration 5299 : loss : 0.019528, loss_ce: 0.006840
2022-01-10 01:35:37,278 iteration 5300 : loss : 0.020461, loss_ce: 0.007148
2022-01-10 01:35:38,793 iteration 5301 : loss : 0.019650, loss_ce: 0.007128
2022-01-10 01:35:40,309 iteration 5302 : loss : 0.018684, loss_ce: 0.009814
2022-01-10 01:35:41,830 iteration 5303 : loss : 0.020447, loss_ce: 0.007382
2022-01-10 01:35:43,333 iteration 5304 : loss : 0.020306, loss_ce: 0.007782
 78%|██████████████████████▌      | 312/400 [2:32:18<42:11, 28.76s/it]2022-01-10 01:35:44,985 iteration 5305 : loss : 0.033527, loss_ce: 0.010671
2022-01-10 01:35:46,658 iteration 5306 : loss : 0.022593, loss_ce: 0.009877
2022-01-10 01:35:48,348 iteration 5307 : loss : 0.021654, loss_ce: 0.007732
2022-01-10 01:35:49,999 iteration 5308 : loss : 0.026272, loss_ce: 0.010281
2022-01-10 01:35:51,639 iteration 5309 : loss : 0.027445, loss_ce: 0.010457
2022-01-10 01:35:53,188 iteration 5310 : loss : 0.021730, loss_ce: 0.010404
2022-01-10 01:35:54,723 iteration 5311 : loss : 0.026231, loss_ce: 0.006745
2022-01-10 01:35:56,357 iteration 5312 : loss : 0.028397, loss_ce: 0.013215
2022-01-10 01:35:58,039 iteration 5313 : loss : 0.024412, loss_ce: 0.006675
2022-01-10 01:35:59,630 iteration 5314 : loss : 0.018536, loss_ce: 0.006216
2022-01-10 01:36:01,272 iteration 5315 : loss : 0.031891, loss_ce: 0.008570
2022-01-10 01:36:02,906 iteration 5316 : loss : 0.019847, loss_ce: 0.008553
2022-01-10 01:36:04,482 iteration 5317 : loss : 0.018876, loss_ce: 0.007739
2022-01-10 01:36:06,026 iteration 5318 : loss : 0.018667, loss_ce: 0.007435
2022-01-10 01:36:07,657 iteration 5319 : loss : 0.025882, loss_ce: 0.011848
2022-01-10 01:36:09,279 iteration 5320 : loss : 0.023873, loss_ce: 0.010147
2022-01-10 01:36:10,924 iteration 5321 : loss : 0.025620, loss_ce: 0.009675
 78%|██████████████████████▋      | 313/400 [2:32:46<41:11, 28.41s/it]2022-01-10 01:36:12,489 iteration 5322 : loss : 0.016720, loss_ce: 0.007194
2022-01-10 01:36:14,083 iteration 5323 : loss : 0.026433, loss_ce: 0.012168
2022-01-10 01:36:15,599 iteration 5324 : loss : 0.019434, loss_ce: 0.006656
2022-01-10 01:36:17,141 iteration 5325 : loss : 0.018035, loss_ce: 0.004184
2022-01-10 01:36:18,748 iteration 5326 : loss : 0.020818, loss_ce: 0.007481
2022-01-10 01:36:20,287 iteration 5327 : loss : 0.018138, loss_ce: 0.006468
2022-01-10 01:36:21,844 iteration 5328 : loss : 0.020479, loss_ce: 0.011247
2022-01-10 01:36:23,464 iteration 5329 : loss : 0.025928, loss_ce: 0.008291
2022-01-10 01:36:25,066 iteration 5330 : loss : 0.024767, loss_ce: 0.009050
2022-01-10 01:36:26,637 iteration 5331 : loss : 0.020012, loss_ce: 0.006818
2022-01-10 01:36:28,223 iteration 5332 : loss : 0.027811, loss_ce: 0.007888
2022-01-10 01:36:29,840 iteration 5333 : loss : 0.018887, loss_ce: 0.009427
2022-01-10 01:36:31,439 iteration 5334 : loss : 0.043408, loss_ce: 0.016962
2022-01-10 01:36:33,060 iteration 5335 : loss : 0.033122, loss_ce: 0.011497
2022-01-10 01:36:34,653 iteration 5336 : loss : 0.017791, loss_ce: 0.006799
2022-01-10 01:36:36,198 iteration 5337 : loss : 0.015501, loss_ce: 0.005553
2022-01-10 01:36:37,741 iteration 5338 : loss : 0.019609, loss_ce: 0.009923
 78%|██████████████████████▊      | 314/400 [2:33:12<40:01, 27.93s/it]2022-01-10 01:36:39,530 iteration 5339 : loss : 0.029716, loss_ce: 0.012732
2022-01-10 01:36:41,165 iteration 5340 : loss : 0.027500, loss_ce: 0.013941
2022-01-10 01:36:42,771 iteration 5341 : loss : 0.020315, loss_ce: 0.004144
2022-01-10 01:36:44,364 iteration 5342 : loss : 0.016803, loss_ce: 0.008707
2022-01-10 01:36:45,917 iteration 5343 : loss : 0.018277, loss_ce: 0.006029
2022-01-10 01:36:47,515 iteration 5344 : loss : 0.027528, loss_ce: 0.009497
2022-01-10 01:36:49,086 iteration 5345 : loss : 0.015749, loss_ce: 0.005463
2022-01-10 01:36:50,704 iteration 5346 : loss : 0.026215, loss_ce: 0.008851
2022-01-10 01:36:52,328 iteration 5347 : loss : 0.017063, loss_ce: 0.004801
2022-01-10 01:36:53,961 iteration 5348 : loss : 0.018729, loss_ce: 0.004528
2022-01-10 01:36:55,556 iteration 5349 : loss : 0.019501, loss_ce: 0.007314
2022-01-10 01:36:57,067 iteration 5350 : loss : 0.017375, loss_ce: 0.007513
2022-01-10 01:36:58,589 iteration 5351 : loss : 0.029880, loss_ce: 0.011511
2022-01-10 01:37:00,208 iteration 5352 : loss : 0.029537, loss_ce: 0.013175
2022-01-10 01:37:01,836 iteration 5353 : loss : 0.020780, loss_ce: 0.007997
2022-01-10 01:37:03,386 iteration 5354 : loss : 0.016672, loss_ce: 0.007611
2022-01-10 01:37:03,386 Training Data Eval:
2022-01-10 01:37:11,431   Average segmentation loss on training set: 0.0119
2022-01-10 01:37:11,432 Validation Data Eval:
2022-01-10 01:37:14,208   Average segmentation loss on validation set: 0.0711
2022-01-10 01:37:15,724 iteration 5355 : loss : 0.020684, loss_ce: 0.007810
 79%|██████████████████████▊      | 315/400 [2:33:50<43:50, 30.95s/it]2022-01-10 01:37:17,367 iteration 5356 : loss : 0.021727, loss_ce: 0.009568
2022-01-10 01:37:18,879 iteration 5357 : loss : 0.027927, loss_ce: 0.006385
2022-01-10 01:37:20,586 iteration 5358 : loss : 0.031428, loss_ce: 0.014487
2022-01-10 01:37:22,095 iteration 5359 : loss : 0.016525, loss_ce: 0.006009
2022-01-10 01:37:23,690 iteration 5360 : loss : 0.046236, loss_ce: 0.024245
2022-01-10 01:37:25,287 iteration 5361 : loss : 0.023489, loss_ce: 0.009031
2022-01-10 01:37:26,900 iteration 5362 : loss : 0.023512, loss_ce: 0.007599
2022-01-10 01:37:28,481 iteration 5363 : loss : 0.020471, loss_ce: 0.008540
2022-01-10 01:37:30,063 iteration 5364 : loss : 0.026405, loss_ce: 0.005834
2022-01-10 01:37:31,626 iteration 5365 : loss : 0.016947, loss_ce: 0.006556
2022-01-10 01:37:33,226 iteration 5366 : loss : 0.019756, loss_ce: 0.007892
2022-01-10 01:37:34,853 iteration 5367 : loss : 0.019919, loss_ce: 0.008836
2022-01-10 01:37:36,415 iteration 5368 : loss : 0.015927, loss_ce: 0.007379
2022-01-10 01:37:38,011 iteration 5369 : loss : 0.025602, loss_ce: 0.010328
2022-01-10 01:37:39,576 iteration 5370 : loss : 0.014327, loss_ce: 0.005636
2022-01-10 01:37:41,167 iteration 5371 : loss : 0.019390, loss_ce: 0.009374
2022-01-10 01:37:42,719 iteration 5372 : loss : 0.018799, loss_ce: 0.006169
 79%|██████████████████████▉      | 316/400 [2:34:17<41:40, 29.76s/it]2022-01-10 01:37:44,361 iteration 5373 : loss : 0.016352, loss_ce: 0.007513
2022-01-10 01:37:45,904 iteration 5374 : loss : 0.022711, loss_ce: 0.009904
2022-01-10 01:37:47,505 iteration 5375 : loss : 0.019994, loss_ce: 0.005531
2022-01-10 01:37:49,062 iteration 5376 : loss : 0.018885, loss_ce: 0.006383
2022-01-10 01:37:50,604 iteration 5377 : loss : 0.013992, loss_ce: 0.004687
2022-01-10 01:37:52,185 iteration 5378 : loss : 0.017456, loss_ce: 0.006380
2022-01-10 01:37:53,696 iteration 5379 : loss : 0.015574, loss_ce: 0.005534
2022-01-10 01:37:55,273 iteration 5380 : loss : 0.017161, loss_ce: 0.006857
2022-01-10 01:37:56,821 iteration 5381 : loss : 0.018968, loss_ce: 0.007708
2022-01-10 01:37:58,416 iteration 5382 : loss : 0.017147, loss_ce: 0.004843
2022-01-10 01:38:00,078 iteration 5383 : loss : 0.026208, loss_ce: 0.013027
2022-01-10 01:38:01,813 iteration 5384 : loss : 0.028006, loss_ce: 0.011447
2022-01-10 01:38:03,375 iteration 5385 : loss : 0.016042, loss_ce: 0.006147
2022-01-10 01:38:04,873 iteration 5386 : loss : 0.014759, loss_ce: 0.004737
2022-01-10 01:38:06,420 iteration 5387 : loss : 0.017172, loss_ce: 0.007555
2022-01-10 01:38:08,039 iteration 5388 : loss : 0.028503, loss_ce: 0.007619
2022-01-10 01:38:09,727 iteration 5389 : loss : 0.036712, loss_ce: 0.012662
 79%|██████████████████████▉      | 317/400 [2:34:44<40:01, 28.94s/it]2022-01-10 01:38:11,339 iteration 5390 : loss : 0.023006, loss_ce: 0.007687
2022-01-10 01:38:12,962 iteration 5391 : loss : 0.027414, loss_ce: 0.009901
2022-01-10 01:38:14,591 iteration 5392 : loss : 0.022264, loss_ce: 0.012904
2022-01-10 01:38:16,175 iteration 5393 : loss : 0.021988, loss_ce: 0.009131
2022-01-10 01:38:17,727 iteration 5394 : loss : 0.018811, loss_ce: 0.009084
2022-01-10 01:38:19,331 iteration 5395 : loss : 0.017616, loss_ce: 0.005906
2022-01-10 01:38:20,861 iteration 5396 : loss : 0.020138, loss_ce: 0.011133
2022-01-10 01:38:22,496 iteration 5397 : loss : 0.027129, loss_ce: 0.009914
2022-01-10 01:38:24,088 iteration 5398 : loss : 0.016164, loss_ce: 0.006345
2022-01-10 01:38:25,602 iteration 5399 : loss : 0.024815, loss_ce: 0.006835
2022-01-10 01:38:27,265 iteration 5400 : loss : 0.043996, loss_ce: 0.011471
2022-01-10 01:38:28,876 iteration 5401 : loss : 0.024172, loss_ce: 0.011094
2022-01-10 01:38:30,464 iteration 5402 : loss : 0.027263, loss_ce: 0.011009
2022-01-10 01:38:32,016 iteration 5403 : loss : 0.033420, loss_ce: 0.014422
2022-01-10 01:38:33,550 iteration 5404 : loss : 0.017449, loss_ce: 0.004626
2022-01-10 01:38:35,100 iteration 5405 : loss : 0.016562, loss_ce: 0.007016
2022-01-10 01:38:36,723 iteration 5406 : loss : 0.025265, loss_ce: 0.008563
 80%|███████████████████████      | 318/400 [2:35:11<38:44, 28.35s/it]2022-01-10 01:38:38,391 iteration 5407 : loss : 0.024651, loss_ce: 0.009555
2022-01-10 01:38:39,968 iteration 5408 : loss : 0.038499, loss_ce: 0.011014
2022-01-10 01:38:41,626 iteration 5409 : loss : 0.022169, loss_ce: 0.010396
2022-01-10 01:38:43,172 iteration 5410 : loss : 0.022323, loss_ce: 0.009193
2022-01-10 01:38:44,747 iteration 5411 : loss : 0.017878, loss_ce: 0.007263
2022-01-10 01:38:46,337 iteration 5412 : loss : 0.024007, loss_ce: 0.009777
2022-01-10 01:38:47,892 iteration 5413 : loss : 0.023094, loss_ce: 0.010974
2022-01-10 01:38:49,483 iteration 5414 : loss : 0.020911, loss_ce: 0.008047
2022-01-10 01:38:51,145 iteration 5415 : loss : 0.030403, loss_ce: 0.011622
2022-01-10 01:38:52,665 iteration 5416 : loss : 0.019514, loss_ce: 0.006545
2022-01-10 01:38:54,276 iteration 5417 : loss : 0.020409, loss_ce: 0.006518
2022-01-10 01:38:55,834 iteration 5418 : loss : 0.025914, loss_ce: 0.011248
2022-01-10 01:38:57,442 iteration 5419 : loss : 0.015963, loss_ce: 0.004495
2022-01-10 01:38:59,085 iteration 5420 : loss : 0.027395, loss_ce: 0.009399
2022-01-10 01:39:00,688 iteration 5421 : loss : 0.019733, loss_ce: 0.008655
2022-01-10 01:39:02,155 iteration 5422 : loss : 0.014234, loss_ce: 0.006262
2022-01-10 01:39:03,762 iteration 5423 : loss : 0.047049, loss_ce: 0.011762
 80%|███████████████████████▏     | 319/400 [2:35:38<37:44, 27.96s/it]2022-01-10 01:39:05,460 iteration 5424 : loss : 0.019876, loss_ce: 0.007016
2022-01-10 01:39:07,004 iteration 5425 : loss : 0.015006, loss_ce: 0.004731
2022-01-10 01:39:08,575 iteration 5426 : loss : 0.017536, loss_ce: 0.004599
2022-01-10 01:39:10,067 iteration 5427 : loss : 0.018537, loss_ce: 0.007195
2022-01-10 01:39:11,611 iteration 5428 : loss : 0.026373, loss_ce: 0.011845
2022-01-10 01:39:13,203 iteration 5429 : loss : 0.014603, loss_ce: 0.005429
2022-01-10 01:39:14,785 iteration 5430 : loss : 0.018489, loss_ce: 0.008137
2022-01-10 01:39:16,299 iteration 5431 : loss : 0.017905, loss_ce: 0.006081
2022-01-10 01:39:17,947 iteration 5432 : loss : 0.027930, loss_ce: 0.014145
2022-01-10 01:39:19,576 iteration 5433 : loss : 0.027591, loss_ce: 0.011233
2022-01-10 01:39:21,100 iteration 5434 : loss : 0.018023, loss_ce: 0.007772
2022-01-10 01:39:22,745 iteration 5435 : loss : 0.021655, loss_ce: 0.008537
2022-01-10 01:39:24,345 iteration 5436 : loss : 0.018049, loss_ce: 0.006973
2022-01-10 01:39:25,968 iteration 5437 : loss : 0.041965, loss_ce: 0.008430
2022-01-10 01:39:27,591 iteration 5438 : loss : 0.017213, loss_ce: 0.007608
2022-01-10 01:39:29,151 iteration 5439 : loss : 0.021003, loss_ce: 0.008746
2022-01-10 01:39:29,152 Training Data Eval:
2022-01-10 01:39:37,193   Average segmentation loss on training set: 0.0116
2022-01-10 01:39:37,193 Validation Data Eval:
2022-01-10 01:39:39,963   Average segmentation loss on validation set: 0.0693
2022-01-10 01:39:41,483 iteration 5440 : loss : 0.026934, loss_ce: 0.008969
 80%|███████████████████████▏     | 320/400 [2:36:16<41:11, 30.89s/it]2022-01-10 01:39:43,093 iteration 5441 : loss : 0.017477, loss_ce: 0.006384
2022-01-10 01:39:44,615 iteration 5442 : loss : 0.017562, loss_ce: 0.006367
2022-01-10 01:39:46,177 iteration 5443 : loss : 0.018529, loss_ce: 0.006381
2022-01-10 01:39:47,670 iteration 5444 : loss : 0.017403, loss_ce: 0.005196
2022-01-10 01:39:49,252 iteration 5445 : loss : 0.018739, loss_ce: 0.006776
2022-01-10 01:39:50,781 iteration 5446 : loss : 0.022640, loss_ce: 0.011240
2022-01-10 01:39:52,438 iteration 5447 : loss : 0.025630, loss_ce: 0.009614
2022-01-10 01:39:53,963 iteration 5448 : loss : 0.028280, loss_ce: 0.011981
2022-01-10 01:39:55,548 iteration 5449 : loss : 0.026288, loss_ce: 0.016084
2022-01-10 01:39:57,121 iteration 5450 : loss : 0.032486, loss_ce: 0.013583
2022-01-10 01:39:58,634 iteration 5451 : loss : 0.019936, loss_ce: 0.006369
2022-01-10 01:40:00,217 iteration 5452 : loss : 0.035544, loss_ce: 0.014840
2022-01-10 01:40:01,745 iteration 5453 : loss : 0.016914, loss_ce: 0.006744
2022-01-10 01:40:03,370 iteration 5454 : loss : 0.025487, loss_ce: 0.012180
2022-01-10 01:40:04,969 iteration 5455 : loss : 0.030931, loss_ce: 0.009288
2022-01-10 01:40:06,550 iteration 5456 : loss : 0.017969, loss_ce: 0.003800
2022-01-10 01:40:08,079 iteration 5457 : loss : 0.017188, loss_ce: 0.005448
 80%|███████████████████████▎     | 321/400 [2:36:43<38:58, 29.60s/it]2022-01-10 01:40:09,691 iteration 5458 : loss : 0.020954, loss_ce: 0.006847
2022-01-10 01:40:11,203 iteration 5459 : loss : 0.019778, loss_ce: 0.008713
2022-01-10 01:40:12,774 iteration 5460 : loss : 0.015432, loss_ce: 0.005763
2022-01-10 01:40:14,336 iteration 5461 : loss : 0.021173, loss_ce: 0.007499
2022-01-10 01:40:15,892 iteration 5462 : loss : 0.017848, loss_ce: 0.006330
2022-01-10 01:40:17,477 iteration 5463 : loss : 0.024185, loss_ce: 0.007547
2022-01-10 01:40:19,058 iteration 5464 : loss : 0.024379, loss_ce: 0.012340
2022-01-10 01:40:20,701 iteration 5465 : loss : 0.021865, loss_ce: 0.008382
2022-01-10 01:40:22,331 iteration 5466 : loss : 0.017207, loss_ce: 0.006456
2022-01-10 01:40:23,929 iteration 5467 : loss : 0.025948, loss_ce: 0.011229
2022-01-10 01:40:25,529 iteration 5468 : loss : 0.024906, loss_ce: 0.011870
2022-01-10 01:40:27,038 iteration 5469 : loss : 0.022094, loss_ce: 0.008299
2022-01-10 01:40:28,597 iteration 5470 : loss : 0.012127, loss_ce: 0.004292
2022-01-10 01:40:30,155 iteration 5471 : loss : 0.019366, loss_ce: 0.006293
2022-01-10 01:40:31,751 iteration 5472 : loss : 0.027797, loss_ce: 0.009414
2022-01-10 01:40:33,266 iteration 5473 : loss : 0.013798, loss_ce: 0.005024
2022-01-10 01:40:34,791 iteration 5474 : loss : 0.016571, loss_ce: 0.005621
 80%|███████████████████████▎     | 322/400 [2:37:09<37:21, 28.73s/it]2022-01-10 01:40:36,438 iteration 5475 : loss : 0.025760, loss_ce: 0.008404
2022-01-10 01:40:38,066 iteration 5476 : loss : 0.026149, loss_ce: 0.012272
2022-01-10 01:40:39,636 iteration 5477 : loss : 0.035758, loss_ce: 0.004646
2022-01-10 01:40:41,131 iteration 5478 : loss : 0.021040, loss_ce: 0.006953
2022-01-10 01:40:42,692 iteration 5479 : loss : 0.015002, loss_ce: 0.006450
2022-01-10 01:40:44,250 iteration 5480 : loss : 0.021824, loss_ce: 0.008485
2022-01-10 01:40:45,786 iteration 5481 : loss : 0.021860, loss_ce: 0.007287
2022-01-10 01:40:47,445 iteration 5482 : loss : 0.034898, loss_ce: 0.008931
2022-01-10 01:40:48,939 iteration 5483 : loss : 0.013749, loss_ce: 0.005356
2022-01-10 01:40:50,524 iteration 5484 : loss : 0.027555, loss_ce: 0.012431
2022-01-10 01:40:52,090 iteration 5485 : loss : 0.019415, loss_ce: 0.009288
2022-01-10 01:40:53,657 iteration 5486 : loss : 0.020650, loss_ce: 0.008914
2022-01-10 01:40:55,246 iteration 5487 : loss : 0.020059, loss_ce: 0.008679
2022-01-10 01:40:56,781 iteration 5488 : loss : 0.021082, loss_ce: 0.010393
2022-01-10 01:40:58,310 iteration 5489 : loss : 0.014316, loss_ce: 0.007385
2022-01-10 01:40:59,949 iteration 5490 : loss : 0.028171, loss_ce: 0.010752
2022-01-10 01:41:01,492 iteration 5491 : loss : 0.018066, loss_ce: 0.006015
 81%|███████████████████████▍     | 323/400 [2:37:36<36:05, 28.12s/it]2022-01-10 01:41:03,039 iteration 5492 : loss : 0.016623, loss_ce: 0.006303
2022-01-10 01:41:04,633 iteration 5493 : loss : 0.018245, loss_ce: 0.006243
2022-01-10 01:41:06,163 iteration 5494 : loss : 0.017630, loss_ce: 0.005468
2022-01-10 01:41:07,738 iteration 5495 : loss : 0.014893, loss_ce: 0.004540
2022-01-10 01:41:09,322 iteration 5496 : loss : 0.017723, loss_ce: 0.005374
2022-01-10 01:41:10,910 iteration 5497 : loss : 0.023914, loss_ce: 0.010008
2022-01-10 01:41:12,529 iteration 5498 : loss : 0.033445, loss_ce: 0.010827
2022-01-10 01:41:14,132 iteration 5499 : loss : 0.022446, loss_ce: 0.012713
2022-01-10 01:41:15,641 iteration 5500 : loss : 0.025462, loss_ce: 0.010607
2022-01-10 01:41:17,218 iteration 5501 : loss : 0.018188, loss_ce: 0.007515
2022-01-10 01:41:18,747 iteration 5502 : loss : 0.012299, loss_ce: 0.005196
2022-01-10 01:41:20,311 iteration 5503 : loss : 0.015600, loss_ce: 0.004263
2022-01-10 01:41:21,844 iteration 5504 : loss : 0.016644, loss_ce: 0.006028
2022-01-10 01:41:23,408 iteration 5505 : loss : 0.026768, loss_ce: 0.011867
2022-01-10 01:41:25,008 iteration 5506 : loss : 0.014838, loss_ce: 0.005430
2022-01-10 01:41:26,569 iteration 5507 : loss : 0.023410, loss_ce: 0.010470
2022-01-10 01:41:28,147 iteration 5508 : loss : 0.024768, loss_ce: 0.008331
 81%|███████████████████████▍     | 324/400 [2:38:03<35:04, 27.69s/it]2022-01-10 01:41:29,781 iteration 5509 : loss : 0.018921, loss_ce: 0.006159
2022-01-10 01:41:31,439 iteration 5510 : loss : 0.018096, loss_ce: 0.006606
2022-01-10 01:41:32,996 iteration 5511 : loss : 0.015043, loss_ce: 0.005673
2022-01-10 01:41:34,619 iteration 5512 : loss : 0.022230, loss_ce: 0.009339
2022-01-10 01:41:36,168 iteration 5513 : loss : 0.016677, loss_ce: 0.008058
2022-01-10 01:41:37,674 iteration 5514 : loss : 0.012613, loss_ce: 0.004343
2022-01-10 01:41:39,303 iteration 5515 : loss : 0.029070, loss_ce: 0.005803
2022-01-10 01:41:40,851 iteration 5516 : loss : 0.025269, loss_ce: 0.010384
2022-01-10 01:41:42,432 iteration 5517 : loss : 0.022475, loss_ce: 0.009521
2022-01-10 01:41:44,010 iteration 5518 : loss : 0.019585, loss_ce: 0.006626
2022-01-10 01:41:45,603 iteration 5519 : loss : 0.020459, loss_ce: 0.007628
2022-01-10 01:41:47,176 iteration 5520 : loss : 0.014741, loss_ce: 0.006003
2022-01-10 01:41:48,765 iteration 5521 : loss : 0.025766, loss_ce: 0.009354
2022-01-10 01:41:50,336 iteration 5522 : loss : 0.024750, loss_ce: 0.008361
2022-01-10 01:41:51,911 iteration 5523 : loss : 0.012735, loss_ce: 0.004394
2022-01-10 01:41:53,456 iteration 5524 : loss : 0.024417, loss_ce: 0.010070
2022-01-10 01:41:53,456 Training Data Eval:
2022-01-10 01:42:01,488   Average segmentation loss on training set: 0.0116
2022-01-10 01:42:01,488 Validation Data Eval:
2022-01-10 01:42:04,257   Average segmentation loss on validation set: 0.0679
2022-01-10 01:42:05,821 iteration 5525 : loss : 0.023237, loss_ce: 0.012013
 81%|███████████████████████▌     | 325/400 [2:38:40<38:21, 30.68s/it]2022-01-10 01:42:07,495 iteration 5526 : loss : 0.022193, loss_ce: 0.008110
2022-01-10 01:42:09,073 iteration 5527 : loss : 0.020082, loss_ce: 0.009239
2022-01-10 01:42:10,640 iteration 5528 : loss : 0.033850, loss_ce: 0.011829
2022-01-10 01:42:12,244 iteration 5529 : loss : 0.018777, loss_ce: 0.007462
2022-01-10 01:42:13,760 iteration 5530 : loss : 0.015920, loss_ce: 0.007783
2022-01-10 01:42:15,302 iteration 5531 : loss : 0.019874, loss_ce: 0.008664
2022-01-10 01:42:16,827 iteration 5532 : loss : 0.012329, loss_ce: 0.005439
2022-01-10 01:42:18,452 iteration 5533 : loss : 0.024151, loss_ce: 0.006499
2022-01-10 01:42:20,020 iteration 5534 : loss : 0.020576, loss_ce: 0.006733
2022-01-10 01:42:21,648 iteration 5535 : loss : 0.031708, loss_ce: 0.012183
2022-01-10 01:42:23,315 iteration 5536 : loss : 0.028281, loss_ce: 0.012190
2022-01-10 01:42:24,921 iteration 5537 : loss : 0.025282, loss_ce: 0.007914
2022-01-10 01:42:26,510 iteration 5538 : loss : 0.018539, loss_ce: 0.007407
2022-01-10 01:42:28,181 iteration 5539 : loss : 0.026558, loss_ce: 0.005664
2022-01-10 01:42:29,726 iteration 5540 : loss : 0.016212, loss_ce: 0.007719
2022-01-10 01:42:31,307 iteration 5541 : loss : 0.019029, loss_ce: 0.005574
2022-01-10 01:42:32,861 iteration 5542 : loss : 0.016209, loss_ce: 0.006605
 82%|███████████████████████▋     | 326/400 [2:39:07<36:29, 29.59s/it]2022-01-10 01:42:34,520 iteration 5543 : loss : 0.027010, loss_ce: 0.009202
2022-01-10 01:42:36,104 iteration 5544 : loss : 0.018877, loss_ce: 0.008661
2022-01-10 01:42:37,656 iteration 5545 : loss : 0.020206, loss_ce: 0.008090
2022-01-10 01:42:39,185 iteration 5546 : loss : 0.021938, loss_ce: 0.007553
2022-01-10 01:42:40,756 iteration 5547 : loss : 0.017574, loss_ce: 0.006706
2022-01-10 01:42:42,381 iteration 5548 : loss : 0.031619, loss_ce: 0.011950
2022-01-10 01:42:43,897 iteration 5549 : loss : 0.016448, loss_ce: 0.006528
2022-01-10 01:42:45,447 iteration 5550 : loss : 0.014950, loss_ce: 0.005837
2022-01-10 01:42:47,020 iteration 5551 : loss : 0.016490, loss_ce: 0.007792
2022-01-10 01:42:48,536 iteration 5552 : loss : 0.015494, loss_ce: 0.005888
2022-01-10 01:42:50,124 iteration 5553 : loss : 0.026065, loss_ce: 0.009786
2022-01-10 01:42:51,747 iteration 5554 : loss : 0.015354, loss_ce: 0.004988
2022-01-10 01:42:53,363 iteration 5555 : loss : 0.029217, loss_ce: 0.009864
2022-01-10 01:42:54,868 iteration 5556 : loss : 0.016126, loss_ce: 0.006559
2022-01-10 01:42:56,501 iteration 5557 : loss : 0.035257, loss_ce: 0.010727
2022-01-10 01:42:58,084 iteration 5558 : loss : 0.024547, loss_ce: 0.009970
2022-01-10 01:42:59,569 iteration 5559 : loss : 0.014208, loss_ce: 0.006041
 82%|███████████████████████▋     | 327/400 [2:39:34<34:56, 28.72s/it]2022-01-10 01:43:01,364 iteration 5560 : loss : 0.032082, loss_ce: 0.010265
2022-01-10 01:43:02,929 iteration 5561 : loss : 0.019724, loss_ce: 0.007079
2022-01-10 01:43:04,493 iteration 5562 : loss : 0.031403, loss_ce: 0.009880
2022-01-10 01:43:06,058 iteration 5563 : loss : 0.014818, loss_ce: 0.005987
2022-01-10 01:43:07,720 iteration 5564 : loss : 0.020714, loss_ce: 0.006855
2022-01-10 01:43:09,300 iteration 5565 : loss : 0.023662, loss_ce: 0.009658
2022-01-10 01:43:10,917 iteration 5566 : loss : 0.020419, loss_ce: 0.008783
2022-01-10 01:43:12,590 iteration 5567 : loss : 0.018273, loss_ce: 0.008594
2022-01-10 01:43:14,169 iteration 5568 : loss : 0.021211, loss_ce: 0.007891
2022-01-10 01:43:15,793 iteration 5569 : loss : 0.016768, loss_ce: 0.007333
2022-01-10 01:43:17,389 iteration 5570 : loss : 0.013399, loss_ce: 0.004518
2022-01-10 01:43:19,013 iteration 5571 : loss : 0.018031, loss_ce: 0.007514
2022-01-10 01:43:20,593 iteration 5572 : loss : 0.014626, loss_ce: 0.005776
2022-01-10 01:43:22,160 iteration 5573 : loss : 0.020113, loss_ce: 0.005793
2022-01-10 01:43:23,835 iteration 5574 : loss : 0.020032, loss_ce: 0.006722
2022-01-10 01:43:25,308 iteration 5575 : loss : 0.012566, loss_ce: 0.005380
2022-01-10 01:43:26,763 iteration 5576 : loss : 0.015102, loss_ce: 0.004967
 82%|███████████████████████▊     | 328/400 [2:40:01<33:54, 28.26s/it]2022-01-10 01:43:28,417 iteration 5577 : loss : 0.019261, loss_ce: 0.007129
2022-01-10 01:43:29,956 iteration 5578 : loss : 0.016766, loss_ce: 0.004322
2022-01-10 01:43:31,519 iteration 5579 : loss : 0.017764, loss_ce: 0.006037
2022-01-10 01:43:33,136 iteration 5580 : loss : 0.013030, loss_ce: 0.004879
2022-01-10 01:43:34,691 iteration 5581 : loss : 0.018731, loss_ce: 0.006971
2022-01-10 01:43:36,301 iteration 5582 : loss : 0.020845, loss_ce: 0.006455
2022-01-10 01:43:37,856 iteration 5583 : loss : 0.022039, loss_ce: 0.005447
2022-01-10 01:43:39,521 iteration 5584 : loss : 0.025771, loss_ce: 0.010743
2022-01-10 01:43:41,078 iteration 5585 : loss : 0.022117, loss_ce: 0.008806
2022-01-10 01:43:42,649 iteration 5586 : loss : 0.027731, loss_ce: 0.013411
2022-01-10 01:43:44,224 iteration 5587 : loss : 0.024576, loss_ce: 0.011204
2022-01-10 01:43:45,874 iteration 5588 : loss : 0.022111, loss_ce: 0.007474
2022-01-10 01:43:47,502 iteration 5589 : loss : 0.023721, loss_ce: 0.009205
2022-01-10 01:43:49,088 iteration 5590 : loss : 0.021256, loss_ce: 0.009038
2022-01-10 01:43:50,668 iteration 5591 : loss : 0.019302, loss_ce: 0.009244
2022-01-10 01:43:52,232 iteration 5592 : loss : 0.018784, loss_ce: 0.006811
2022-01-10 01:43:53,793 iteration 5593 : loss : 0.016284, loss_ce: 0.009576
 82%|███████████████████████▊     | 329/400 [2:40:28<33:00, 27.89s/it]2022-01-10 01:43:55,455 iteration 5594 : loss : 0.015942, loss_ce: 0.006176
2022-01-10 01:43:57,007 iteration 5595 : loss : 0.016022, loss_ce: 0.007159
2022-01-10 01:43:58,559 iteration 5596 : loss : 0.015116, loss_ce: 0.004694
2022-01-10 01:44:00,152 iteration 5597 : loss : 0.015270, loss_ce: 0.004824
2022-01-10 01:44:01,799 iteration 5598 : loss : 0.037183, loss_ce: 0.016977
2022-01-10 01:44:03,419 iteration 5599 : loss : 0.017253, loss_ce: 0.005821
2022-01-10 01:44:05,034 iteration 5600 : loss : 0.017462, loss_ce: 0.007215
2022-01-10 01:44:06,649 iteration 5601 : loss : 0.017023, loss_ce: 0.005186
2022-01-10 01:44:08,190 iteration 5602 : loss : 0.021260, loss_ce: 0.005717
2022-01-10 01:44:09,816 iteration 5603 : loss : 0.018890, loss_ce: 0.008856
2022-01-10 01:44:11,362 iteration 5604 : loss : 0.018425, loss_ce: 0.008713
2022-01-10 01:44:12,912 iteration 5605 : loss : 0.019880, loss_ce: 0.006464
2022-01-10 01:44:14,553 iteration 5606 : loss : 0.017747, loss_ce: 0.006172
2022-01-10 01:44:16,131 iteration 5607 : loss : 0.025353, loss_ce: 0.012977
2022-01-10 01:44:17,826 iteration 5608 : loss : 0.033267, loss_ce: 0.018345
2022-01-10 01:44:19,370 iteration 5609 : loss : 0.015769, loss_ce: 0.007042
2022-01-10 01:44:19,370 Training Data Eval:
2022-01-10 01:44:27,408   Average segmentation loss on training set: 0.0114
2022-01-10 01:44:27,409 Validation Data Eval:
2022-01-10 01:44:30,177   Average segmentation loss on validation set: 0.0741
2022-01-10 01:44:31,778 iteration 5610 : loss : 0.021283, loss_ce: 0.009161
 82%|███████████████████████▉     | 330/400 [2:41:06<36:04, 30.92s/it]2022-01-10 01:44:33,454 iteration 5611 : loss : 0.026046, loss_ce: 0.012736
2022-01-10 01:44:35,040 iteration 5612 : loss : 0.018482, loss_ce: 0.005898
2022-01-10 01:44:36,609 iteration 5613 : loss : 0.026788, loss_ce: 0.010956
2022-01-10 01:44:38,178 iteration 5614 : loss : 0.018060, loss_ce: 0.005824
2022-01-10 01:44:39,781 iteration 5615 : loss : 0.019682, loss_ce: 0.006844
2022-01-10 01:44:41,401 iteration 5616 : loss : 0.022402, loss_ce: 0.009406
2022-01-10 01:44:42,983 iteration 5617 : loss : 0.013609, loss_ce: 0.004941
2022-01-10 01:44:44,510 iteration 5618 : loss : 0.017561, loss_ce: 0.007671
2022-01-10 01:44:46,126 iteration 5619 : loss : 0.052021, loss_ce: 0.016356
2022-01-10 01:44:47,754 iteration 5620 : loss : 0.016219, loss_ce: 0.004892
2022-01-10 01:44:49,300 iteration 5621 : loss : 0.030072, loss_ce: 0.011765
2022-01-10 01:44:50,959 iteration 5622 : loss : 0.016562, loss_ce: 0.006637
2022-01-10 01:44:52,500 iteration 5623 : loss : 0.024109, loss_ce: 0.007013
2022-01-10 01:44:54,101 iteration 5624 : loss : 0.020492, loss_ce: 0.008998
2022-01-10 01:44:55,713 iteration 5625 : loss : 0.024592, loss_ce: 0.009617
2022-01-10 01:44:57,291 iteration 5626 : loss : 0.017956, loss_ce: 0.007362
2022-01-10 01:44:58,861 iteration 5627 : loss : 0.036169, loss_ce: 0.010423
 83%|███████████████████████▉     | 331/400 [2:41:33<34:14, 29.77s/it]2022-01-10 01:45:00,394 iteration 5628 : loss : 0.016450, loss_ce: 0.006082
2022-01-10 01:45:01,994 iteration 5629 : loss : 0.023018, loss_ce: 0.010739
2022-01-10 01:45:03,568 iteration 5630 : loss : 0.014528, loss_ce: 0.004672
2022-01-10 01:45:05,244 iteration 5631 : loss : 0.027557, loss_ce: 0.014252
2022-01-10 01:45:06,747 iteration 5632 : loss : 0.021513, loss_ce: 0.009858
2022-01-10 01:45:08,370 iteration 5633 : loss : 0.027680, loss_ce: 0.010354
2022-01-10 01:45:09,904 iteration 5634 : loss : 0.015185, loss_ce: 0.005471
2022-01-10 01:45:11,506 iteration 5635 : loss : 0.023684, loss_ce: 0.007368
2022-01-10 01:45:13,026 iteration 5636 : loss : 0.015399, loss_ce: 0.006930
2022-01-10 01:45:14,631 iteration 5637 : loss : 0.020548, loss_ce: 0.009748
2022-01-10 01:45:16,102 iteration 5638 : loss : 0.015656, loss_ce: 0.004878
2022-01-10 01:45:17,625 iteration 5639 : loss : 0.024298, loss_ce: 0.006663
2022-01-10 01:45:19,180 iteration 5640 : loss : 0.018043, loss_ce: 0.008892
2022-01-10 01:45:20,791 iteration 5641 : loss : 0.018147, loss_ce: 0.008674
2022-01-10 01:45:22,346 iteration 5642 : loss : 0.015532, loss_ce: 0.002820
2022-01-10 01:45:23,964 iteration 5643 : loss : 0.020082, loss_ce: 0.005562
2022-01-10 01:45:25,564 iteration 5644 : loss : 0.018662, loss_ce: 0.006990
 83%|████████████████████████     | 332/400 [2:42:00<32:41, 28.85s/it]2022-01-10 01:45:27,300 iteration 5645 : loss : 0.028833, loss_ce: 0.013651
2022-01-10 01:45:28,847 iteration 5646 : loss : 0.026572, loss_ce: 0.009352
2022-01-10 01:45:30,423 iteration 5647 : loss : 0.021024, loss_ce: 0.011706
2022-01-10 01:45:32,043 iteration 5648 : loss : 0.021469, loss_ce: 0.006979
2022-01-10 01:45:33,661 iteration 5649 : loss : 0.021707, loss_ce: 0.010084
2022-01-10 01:45:35,252 iteration 5650 : loss : 0.021111, loss_ce: 0.008021
2022-01-10 01:45:36,824 iteration 5651 : loss : 0.027537, loss_ce: 0.014331
2022-01-10 01:45:38,327 iteration 5652 : loss : 0.014307, loss_ce: 0.005619
2022-01-10 01:45:39,885 iteration 5653 : loss : 0.017697, loss_ce: 0.006006
2022-01-10 01:45:41,400 iteration 5654 : loss : 0.026208, loss_ce: 0.009897
2022-01-10 01:45:42,993 iteration 5655 : loss : 0.034699, loss_ce: 0.014568
2022-01-10 01:45:44,562 iteration 5656 : loss : 0.017065, loss_ce: 0.005810
2022-01-10 01:45:46,136 iteration 5657 : loss : 0.014780, loss_ce: 0.005911
2022-01-10 01:45:47,711 iteration 5658 : loss : 0.024263, loss_ce: 0.012980
2022-01-10 01:45:49,263 iteration 5659 : loss : 0.016274, loss_ce: 0.004666
2022-01-10 01:45:50,780 iteration 5660 : loss : 0.012284, loss_ce: 0.004545
2022-01-10 01:45:52,442 iteration 5661 : loss : 0.018161, loss_ce: 0.005634
 83%|████████████████████████▏    | 333/400 [2:42:27<31:33, 28.26s/it]2022-01-10 01:45:54,082 iteration 5662 : loss : 0.024003, loss_ce: 0.006911
2022-01-10 01:45:55,656 iteration 5663 : loss : 0.019856, loss_ce: 0.008103
2022-01-10 01:45:57,252 iteration 5664 : loss : 0.024058, loss_ce: 0.006812
2022-01-10 01:45:58,745 iteration 5665 : loss : 0.015356, loss_ce: 0.005491
2022-01-10 01:46:00,343 iteration 5666 : loss : 0.017538, loss_ce: 0.006056
2022-01-10 01:46:01,964 iteration 5667 : loss : 0.018231, loss_ce: 0.007338
2022-01-10 01:46:03,538 iteration 5668 : loss : 0.022080, loss_ce: 0.011215
2022-01-10 01:46:05,154 iteration 5669 : loss : 0.022272, loss_ce: 0.011146
2022-01-10 01:46:06,748 iteration 5670 : loss : 0.024985, loss_ce: 0.011428
2022-01-10 01:46:08,337 iteration 5671 : loss : 0.016244, loss_ce: 0.005860
2022-01-10 01:46:09,911 iteration 5672 : loss : 0.018214, loss_ce: 0.005308
2022-01-10 01:46:11,452 iteration 5673 : loss : 0.023097, loss_ce: 0.007563
2022-01-10 01:46:13,094 iteration 5674 : loss : 0.017590, loss_ce: 0.008753
2022-01-10 01:46:14,643 iteration 5675 : loss : 0.019487, loss_ce: 0.010132
2022-01-10 01:46:16,278 iteration 5676 : loss : 0.020550, loss_ce: 0.007302
2022-01-10 01:46:17,899 iteration 5677 : loss : 0.014071, loss_ce: 0.006674
2022-01-10 01:46:19,495 iteration 5678 : loss : 0.021206, loss_ce: 0.007612
 84%|████████████████████████▏    | 334/400 [2:42:54<30:40, 27.89s/it]2022-01-10 01:46:21,138 iteration 5679 : loss : 0.024071, loss_ce: 0.009978
2022-01-10 01:46:22,616 iteration 5680 : loss : 0.015952, loss_ce: 0.004241
2022-01-10 01:46:24,238 iteration 5681 : loss : 0.028383, loss_ce: 0.010806
2022-01-10 01:46:25,862 iteration 5682 : loss : 0.020163, loss_ce: 0.008448
2022-01-10 01:46:27,411 iteration 5683 : loss : 0.014848, loss_ce: 0.004934
2022-01-10 01:46:28,946 iteration 5684 : loss : 0.013325, loss_ce: 0.006611
2022-01-10 01:46:30,543 iteration 5685 : loss : 0.020631, loss_ce: 0.009383
2022-01-10 01:46:32,100 iteration 5686 : loss : 0.017890, loss_ce: 0.006025
2022-01-10 01:46:33,672 iteration 5687 : loss : 0.014153, loss_ce: 0.005399
2022-01-10 01:46:35,337 iteration 5688 : loss : 0.021754, loss_ce: 0.008280
2022-01-10 01:46:36,921 iteration 5689 : loss : 0.016845, loss_ce: 0.004255
2022-01-10 01:46:38,510 iteration 5690 : loss : 0.021612, loss_ce: 0.009399
2022-01-10 01:46:40,120 iteration 5691 : loss : 0.020889, loss_ce: 0.008628
2022-01-10 01:46:41,724 iteration 5692 : loss : 0.022788, loss_ce: 0.010124
2022-01-10 01:46:43,307 iteration 5693 : loss : 0.023383, loss_ce: 0.006477
2022-01-10 01:46:44,867 iteration 5694 : loss : 0.024844, loss_ce: 0.010799
2022-01-10 01:46:44,867 Training Data Eval:
2022-01-10 01:46:52,899   Average segmentation loss on training set: 0.0105
2022-01-10 01:46:52,899 Validation Data Eval:
2022-01-10 01:46:55,670   Average segmentation loss on validation set: 0.0789
2022-01-10 01:46:57,194 iteration 5695 : loss : 0.012967, loss_ce: 0.004621
 84%|████████████████████████▎    | 335/400 [2:43:32<33:24, 30.84s/it]2022-01-10 01:46:58,776 iteration 5696 : loss : 0.012988, loss_ce: 0.006477
2022-01-10 01:47:00,295 iteration 5697 : loss : 0.014922, loss_ce: 0.006468
2022-01-10 01:47:01,914 iteration 5698 : loss : 0.017549, loss_ce: 0.005430
2022-01-10 01:47:03,416 iteration 5699 : loss : 0.014453, loss_ce: 0.006004
2022-01-10 01:47:04,963 iteration 5700 : loss : 0.021028, loss_ce: 0.008697
2022-01-10 01:47:06,478 iteration 5701 : loss : 0.015027, loss_ce: 0.006498
2022-01-10 01:47:08,146 iteration 5702 : loss : 0.020151, loss_ce: 0.007764
2022-01-10 01:47:09,756 iteration 5703 : loss : 0.019766, loss_ce: 0.008289
2022-01-10 01:47:11,350 iteration 5704 : loss : 0.018588, loss_ce: 0.007417
2022-01-10 01:47:12,949 iteration 5705 : loss : 0.021034, loss_ce: 0.006011
2022-01-10 01:47:14,438 iteration 5706 : loss : 0.013994, loss_ce: 0.005406
2022-01-10 01:47:15,991 iteration 5707 : loss : 0.020861, loss_ce: 0.006671
2022-01-10 01:47:17,514 iteration 5708 : loss : 0.020051, loss_ce: 0.006963
2022-01-10 01:47:19,091 iteration 5709 : loss : 0.015425, loss_ce: 0.005053
2022-01-10 01:47:20,654 iteration 5710 : loss : 0.021175, loss_ce: 0.005953
2022-01-10 01:47:22,159 iteration 5711 : loss : 0.016415, loss_ce: 0.005705
2022-01-10 01:47:23,743 iteration 5712 : loss : 0.018133, loss_ce: 0.007790
 84%|████████████████████████▎    | 336/400 [2:43:58<31:31, 29.55s/it]2022-01-10 01:47:25,415 iteration 5713 : loss : 0.023726, loss_ce: 0.007328
2022-01-10 01:47:27,034 iteration 5714 : loss : 0.029207, loss_ce: 0.008270
2022-01-10 01:47:28,665 iteration 5715 : loss : 0.016903, loss_ce: 0.006669
2022-01-10 01:47:30,222 iteration 5716 : loss : 0.021141, loss_ce: 0.008157
2022-01-10 01:47:31,881 iteration 5717 : loss : 0.020807, loss_ce: 0.009534
2022-01-10 01:47:33,407 iteration 5718 : loss : 0.014832, loss_ce: 0.004742
2022-01-10 01:47:34,930 iteration 5719 : loss : 0.015565, loss_ce: 0.006156
2022-01-10 01:47:36,549 iteration 5720 : loss : 0.017683, loss_ce: 0.007121
2022-01-10 01:47:38,141 iteration 5721 : loss : 0.047219, loss_ce: 0.029389
2022-01-10 01:47:39,712 iteration 5722 : loss : 0.018589, loss_ce: 0.008928
2022-01-10 01:47:41,298 iteration 5723 : loss : 0.017497, loss_ce: 0.005888
2022-01-10 01:47:42,918 iteration 5724 : loss : 0.021515, loss_ce: 0.007389
2022-01-10 01:47:44,502 iteration 5725 : loss : 0.018059, loss_ce: 0.006914
2022-01-10 01:47:46,033 iteration 5726 : loss : 0.024069, loss_ce: 0.005806
2022-01-10 01:47:47,625 iteration 5727 : loss : 0.020981, loss_ce: 0.006734
2022-01-10 01:47:49,179 iteration 5728 : loss : 0.022457, loss_ce: 0.007676
2022-01-10 01:47:50,821 iteration 5729 : loss : 0.022867, loss_ce: 0.009568
 84%|████████████████████████▍    | 337/400 [2:44:25<30:15, 28.81s/it]2022-01-10 01:47:52,402 iteration 5730 : loss : 0.016780, loss_ce: 0.007569
2022-01-10 01:47:54,025 iteration 5731 : loss : 0.027834, loss_ce: 0.012380
2022-01-10 01:47:55,555 iteration 5732 : loss : 0.016651, loss_ce: 0.005982
2022-01-10 01:47:57,187 iteration 5733 : loss : 0.039360, loss_ce: 0.008894
2022-01-10 01:47:58,808 iteration 5734 : loss : 0.028139, loss_ce: 0.011218
2022-01-10 01:48:00,354 iteration 5735 : loss : 0.019861, loss_ce: 0.006795
2022-01-10 01:48:01,897 iteration 5736 : loss : 0.030547, loss_ce: 0.012939
2022-01-10 01:48:03,539 iteration 5737 : loss : 0.028568, loss_ce: 0.010835
2022-01-10 01:48:05,082 iteration 5738 : loss : 0.015123, loss_ce: 0.005458
2022-01-10 01:48:06,619 iteration 5739 : loss : 0.026448, loss_ce: 0.010122
2022-01-10 01:48:08,186 iteration 5740 : loss : 0.029623, loss_ce: 0.012549
2022-01-10 01:48:09,761 iteration 5741 : loss : 0.019624, loss_ce: 0.008885
2022-01-10 01:48:11,349 iteration 5742 : loss : 0.020606, loss_ce: 0.004530
2022-01-10 01:48:12,954 iteration 5743 : loss : 0.015904, loss_ce: 0.006539
2022-01-10 01:48:14,586 iteration 5744 : loss : 0.024889, loss_ce: 0.010893
2022-01-10 01:48:16,221 iteration 5745 : loss : 0.030209, loss_ce: 0.011879
2022-01-10 01:48:17,762 iteration 5746 : loss : 0.017189, loss_ce: 0.004069
 84%|████████████████████████▌    | 338/400 [2:44:52<29:11, 28.25s/it]2022-01-10 01:48:19,394 iteration 5747 : loss : 0.021283, loss_ce: 0.007669
2022-01-10 01:48:20,992 iteration 5748 : loss : 0.018722, loss_ce: 0.007387
2022-01-10 01:48:22,573 iteration 5749 : loss : 0.022437, loss_ce: 0.009541
2022-01-10 01:48:24,147 iteration 5750 : loss : 0.021345, loss_ce: 0.007782
2022-01-10 01:48:25,664 iteration 5751 : loss : 0.023399, loss_ce: 0.008371
2022-01-10 01:48:27,219 iteration 5752 : loss : 0.019624, loss_ce: 0.005881
2022-01-10 01:48:28,835 iteration 5753 : loss : 0.015280, loss_ce: 0.005726
2022-01-10 01:48:30,462 iteration 5754 : loss : 0.028001, loss_ce: 0.008456
2022-01-10 01:48:32,041 iteration 5755 : loss : 0.023053, loss_ce: 0.008977
2022-01-10 01:48:33,586 iteration 5756 : loss : 0.019364, loss_ce: 0.006753
2022-01-10 01:48:35,053 iteration 5757 : loss : 0.016277, loss_ce: 0.006792
2022-01-10 01:48:36,675 iteration 5758 : loss : 0.023767, loss_ce: 0.009931
2022-01-10 01:48:38,301 iteration 5759 : loss : 0.024159, loss_ce: 0.011598
2022-01-10 01:48:39,963 iteration 5760 : loss : 0.018798, loss_ce: 0.009581
2022-01-10 01:48:41,639 iteration 5761 : loss : 0.026843, loss_ce: 0.009793
2022-01-10 01:48:43,169 iteration 5762 : loss : 0.014492, loss_ce: 0.006406
2022-01-10 01:48:44,786 iteration 5763 : loss : 0.018703, loss_ce: 0.008134
 85%|████████████████████████▌    | 339/400 [2:45:19<28:20, 27.88s/it]2022-01-10 01:48:46,424 iteration 5764 : loss : 0.020019, loss_ce: 0.006883
2022-01-10 01:48:48,064 iteration 5765 : loss : 0.029620, loss_ce: 0.006939
2022-01-10 01:48:49,670 iteration 5766 : loss : 0.022543, loss_ce: 0.011975
2022-01-10 01:48:51,250 iteration 5767 : loss : 0.014786, loss_ce: 0.007729
2022-01-10 01:48:52,935 iteration 5768 : loss : 0.022422, loss_ce: 0.008427
2022-01-10 01:48:54,567 iteration 5769 : loss : 0.020096, loss_ce: 0.005899
2022-01-10 01:48:56,113 iteration 5770 : loss : 0.015411, loss_ce: 0.005367
2022-01-10 01:48:57,712 iteration 5771 : loss : 0.022366, loss_ce: 0.008542
2022-01-10 01:48:59,282 iteration 5772 : loss : 0.017863, loss_ce: 0.007240
2022-01-10 01:49:00,788 iteration 5773 : loss : 0.014917, loss_ce: 0.004792
2022-01-10 01:49:02,282 iteration 5774 : loss : 0.016112, loss_ce: 0.004248
2022-01-10 01:49:03,874 iteration 5775 : loss : 0.016109, loss_ce: 0.006249
2022-01-10 01:49:05,522 iteration 5776 : loss : 0.036591, loss_ce: 0.017985
2022-01-10 01:49:07,017 iteration 5777 : loss : 0.018615, loss_ce: 0.007572
2022-01-10 01:49:08,565 iteration 5778 : loss : 0.025575, loss_ce: 0.009226
2022-01-10 01:49:10,119 iteration 5779 : loss : 0.012546, loss_ce: 0.004289
2022-01-10 01:49:10,120 Training Data Eval:
2022-01-10 01:49:18,155   Average segmentation loss on training set: 0.0105
2022-01-10 01:49:18,155 Validation Data Eval:
2022-01-10 01:49:20,925   Average segmentation loss on validation set: 0.0801
2022-01-10 01:49:22,484 iteration 5780 : loss : 0.024557, loss_ce: 0.008174
 85%|████████████████████████▋    | 340/400 [2:45:57<30:49, 30.83s/it]2022-01-10 01:49:24,085 iteration 5781 : loss : 0.022677, loss_ce: 0.006161
2022-01-10 01:49:25,718 iteration 5782 : loss : 0.028979, loss_ce: 0.013722
2022-01-10 01:49:27,289 iteration 5783 : loss : 0.018028, loss_ce: 0.006368
2022-01-10 01:49:28,885 iteration 5784 : loss : 0.017051, loss_ce: 0.009378
2022-01-10 01:49:30,419 iteration 5785 : loss : 0.012856, loss_ce: 0.004515
2022-01-10 01:49:31,964 iteration 5786 : loss : 0.015349, loss_ce: 0.006636
2022-01-10 01:49:33,503 iteration 5787 : loss : 0.013445, loss_ce: 0.005261
2022-01-10 01:49:35,201 iteration 5788 : loss : 0.025621, loss_ce: 0.009398
2022-01-10 01:49:36,799 iteration 5789 : loss : 0.019814, loss_ce: 0.007541
2022-01-10 01:49:38,380 iteration 5790 : loss : 0.015923, loss_ce: 0.005549
2022-01-10 01:49:39,915 iteration 5791 : loss : 0.017438, loss_ce: 0.006287
2022-01-10 01:49:41,519 iteration 5792 : loss : 0.031698, loss_ce: 0.014793
2022-01-10 01:49:43,203 iteration 5793 : loss : 0.023087, loss_ce: 0.009188
2022-01-10 01:49:44,795 iteration 5794 : loss : 0.027671, loss_ce: 0.011028
2022-01-10 01:49:46,369 iteration 5795 : loss : 0.014862, loss_ce: 0.005092
2022-01-10 01:49:47,898 iteration 5796 : loss : 0.012874, loss_ce: 0.003430
2022-01-10 01:49:49,493 iteration 5797 : loss : 0.027297, loss_ce: 0.011174
 85%|████████████████████████▋    | 341/400 [2:46:24<29:11, 29.68s/it]2022-01-10 01:49:51,161 iteration 5798 : loss : 0.019616, loss_ce: 0.007510
2022-01-10 01:49:52,750 iteration 5799 : loss : 0.015997, loss_ce: 0.006323
2022-01-10 01:49:54,318 iteration 5800 : loss : 0.014822, loss_ce: 0.006064
2022-01-10 01:49:55,886 iteration 5801 : loss : 0.020080, loss_ce: 0.007007
2022-01-10 01:49:57,400 iteration 5802 : loss : 0.019691, loss_ce: 0.010238
2022-01-10 01:49:58,941 iteration 5803 : loss : 0.019736, loss_ce: 0.010115
2022-01-10 01:50:00,513 iteration 5804 : loss : 0.021130, loss_ce: 0.008994
2022-01-10 01:50:02,176 iteration 5805 : loss : 0.015981, loss_ce: 0.005117
2022-01-10 01:50:03,739 iteration 5806 : loss : 0.019721, loss_ce: 0.006256
2022-01-10 01:50:05,396 iteration 5807 : loss : 0.029462, loss_ce: 0.009698
2022-01-10 01:50:06,998 iteration 5808 : loss : 0.030553, loss_ce: 0.014765
2022-01-10 01:50:08,519 iteration 5809 : loss : 0.020848, loss_ce: 0.006509
2022-01-10 01:50:10,062 iteration 5810 : loss : 0.019694, loss_ce: 0.006861
2022-01-10 01:50:11,570 iteration 5811 : loss : 0.015753, loss_ce: 0.005159
2022-01-10 01:50:13,144 iteration 5812 : loss : 0.022545, loss_ce: 0.007656
2022-01-10 01:50:14,656 iteration 5813 : loss : 0.016795, loss_ce: 0.007440
2022-01-10 01:50:16,244 iteration 5814 : loss : 0.019378, loss_ce: 0.008998
 86%|████████████████████████▊    | 342/400 [2:46:51<27:50, 28.81s/it]2022-01-10 01:50:17,814 iteration 5815 : loss : 0.015774, loss_ce: 0.004753
2022-01-10 01:50:19,368 iteration 5816 : loss : 0.020850, loss_ce: 0.009196
2022-01-10 01:50:20,907 iteration 5817 : loss : 0.015107, loss_ce: 0.005498
2022-01-10 01:50:22,386 iteration 5818 : loss : 0.016453, loss_ce: 0.007135
2022-01-10 01:50:24,007 iteration 5819 : loss : 0.020706, loss_ce: 0.006745
2022-01-10 01:50:25,655 iteration 5820 : loss : 0.030996, loss_ce: 0.013590
2022-01-10 01:50:27,160 iteration 5821 : loss : 0.014537, loss_ce: 0.004874
2022-01-10 01:50:28,692 iteration 5822 : loss : 0.012040, loss_ce: 0.005414
2022-01-10 01:50:30,362 iteration 5823 : loss : 0.022937, loss_ce: 0.010346
2022-01-10 01:50:31,935 iteration 5824 : loss : 0.022233, loss_ce: 0.007935
2022-01-10 01:50:33,551 iteration 5825 : loss : 0.025853, loss_ce: 0.008997
2022-01-10 01:50:35,165 iteration 5826 : loss : 0.018196, loss_ce: 0.004640
2022-01-10 01:50:36,742 iteration 5827 : loss : 0.028694, loss_ce: 0.008583
2022-01-10 01:50:38,417 iteration 5828 : loss : 0.033143, loss_ce: 0.013300
2022-01-10 01:50:39,990 iteration 5829 : loss : 0.016381, loss_ce: 0.006746
2022-01-10 01:50:41,533 iteration 5830 : loss : 0.024661, loss_ce: 0.009097
2022-01-10 01:50:43,090 iteration 5831 : loss : 0.023316, loss_ce: 0.010546
 86%|████████████████████████▊    | 343/400 [2:47:18<26:48, 28.21s/it]2022-01-10 01:50:44,721 iteration 5832 : loss : 0.016521, loss_ce: 0.004625
2022-01-10 01:50:46,332 iteration 5833 : loss : 0.020134, loss_ce: 0.008924
2022-01-10 01:50:47,848 iteration 5834 : loss : 0.015179, loss_ce: 0.005325
2022-01-10 01:50:49,372 iteration 5835 : loss : 0.020965, loss_ce: 0.006423
2022-01-10 01:50:50,990 iteration 5836 : loss : 0.023560, loss_ce: 0.013035
2022-01-10 01:50:52,566 iteration 5837 : loss : 0.017957, loss_ce: 0.003889
2022-01-10 01:50:54,052 iteration 5838 : loss : 0.011753, loss_ce: 0.003636
2022-01-10 01:50:55,616 iteration 5839 : loss : 0.016025, loss_ce: 0.005792
2022-01-10 01:50:57,229 iteration 5840 : loss : 0.029990, loss_ce: 0.011736
2022-01-10 01:50:58,778 iteration 5841 : loss : 0.023331, loss_ce: 0.011166
2022-01-10 01:51:00,320 iteration 5842 : loss : 0.010738, loss_ce: 0.004006
2022-01-10 01:51:01,862 iteration 5843 : loss : 0.019540, loss_ce: 0.007373
2022-01-10 01:51:03,563 iteration 5844 : loss : 0.027699, loss_ce: 0.008047
2022-01-10 01:51:05,133 iteration 5845 : loss : 0.025418, loss_ce: 0.009484
2022-01-10 01:51:06,719 iteration 5846 : loss : 0.040661, loss_ce: 0.016635
2022-01-10 01:51:08,311 iteration 5847 : loss : 0.018689, loss_ce: 0.007427
2022-01-10 01:51:09,884 iteration 5848 : loss : 0.017878, loss_ce: 0.006202
 86%|████████████████████████▉    | 344/400 [2:47:44<25:56, 27.79s/it]2022-01-10 01:51:11,518 iteration 5849 : loss : 0.020101, loss_ce: 0.007863
2022-01-10 01:51:13,033 iteration 5850 : loss : 0.014042, loss_ce: 0.005676
2022-01-10 01:51:14,603 iteration 5851 : loss : 0.017816, loss_ce: 0.007879
2022-01-10 01:51:16,148 iteration 5852 : loss : 0.017361, loss_ce: 0.006639
2022-01-10 01:51:17,639 iteration 5853 : loss : 0.013468, loss_ce: 0.004499
2022-01-10 01:51:19,177 iteration 5854 : loss : 0.023263, loss_ce: 0.008272
2022-01-10 01:51:20,815 iteration 5855 : loss : 0.022431, loss_ce: 0.009386
2022-01-10 01:51:22,391 iteration 5856 : loss : 0.014813, loss_ce: 0.006346
2022-01-10 01:51:23,978 iteration 5857 : loss : 0.014795, loss_ce: 0.005240
2022-01-10 01:51:25,479 iteration 5858 : loss : 0.014648, loss_ce: 0.005611
2022-01-10 01:51:27,097 iteration 5859 : loss : 0.016680, loss_ce: 0.006858
2022-01-10 01:51:28,712 iteration 5860 : loss : 0.017949, loss_ce: 0.004827
2022-01-10 01:51:30,259 iteration 5861 : loss : 0.014238, loss_ce: 0.004531
2022-01-10 01:51:31,853 iteration 5862 : loss : 0.019243, loss_ce: 0.009604
2022-01-10 01:51:33,335 iteration 5863 : loss : 0.011510, loss_ce: 0.003758
2022-01-10 01:51:34,909 iteration 5864 : loss : 0.023649, loss_ce: 0.007592
2022-01-10 01:51:34,909 Training Data Eval:
2022-01-10 01:51:42,949   Average segmentation loss on training set: 0.0101
2022-01-10 01:51:42,949 Validation Data Eval:
2022-01-10 01:51:45,719   Average segmentation loss on validation set: 0.0794
2022-01-10 01:51:47,229 iteration 5865 : loss : 0.018320, loss_ce: 0.005676
 86%|█████████████████████████    | 345/400 [2:48:22<28:06, 30.66s/it]2022-01-10 01:51:48,927 iteration 5866 : loss : 0.015161, loss_ce: 0.006161
2022-01-10 01:51:50,551 iteration 5867 : loss : 0.019851, loss_ce: 0.005435
2022-01-10 01:51:52,159 iteration 5868 : loss : 0.026078, loss_ce: 0.009604
2022-01-10 01:51:53,656 iteration 5869 : loss : 0.018038, loss_ce: 0.007012
2022-01-10 01:51:55,306 iteration 5870 : loss : 0.033966, loss_ce: 0.010452
2022-01-10 01:51:56,907 iteration 5871 : loss : 0.013408, loss_ce: 0.004821
2022-01-10 01:51:58,517 iteration 5872 : loss : 0.020516, loss_ce: 0.006392
2022-01-10 01:52:00,210 iteration 5873 : loss : 0.037500, loss_ce: 0.014646
2022-01-10 01:52:01,748 iteration 5874 : loss : 0.017238, loss_ce: 0.005564
2022-01-10 01:52:03,306 iteration 5875 : loss : 0.016593, loss_ce: 0.007982
2022-01-10 01:52:04,882 iteration 5876 : loss : 0.016752, loss_ce: 0.006025
2022-01-10 01:52:06,457 iteration 5877 : loss : 0.021062, loss_ce: 0.007881
2022-01-10 01:52:08,057 iteration 5878 : loss : 0.017269, loss_ce: 0.007200
2022-01-10 01:52:09,591 iteration 5879 : loss : 0.016977, loss_ce: 0.007801
2022-01-10 01:52:11,259 iteration 5880 : loss : 0.027521, loss_ce: 0.007503
2022-01-10 01:52:12,774 iteration 5881 : loss : 0.015187, loss_ce: 0.006409
2022-01-10 01:52:14,329 iteration 5882 : loss : 0.018093, loss_ce: 0.008249
 86%|█████████████████████████    | 346/400 [2:48:49<26:37, 29.59s/it]2022-01-10 01:52:15,934 iteration 5883 : loss : 0.014690, loss_ce: 0.005216
2022-01-10 01:52:17,478 iteration 5884 : loss : 0.018073, loss_ce: 0.008962
2022-01-10 01:52:19,171 iteration 5885 : loss : 0.042225, loss_ce: 0.019017
2022-01-10 01:52:20,689 iteration 5886 : loss : 0.014687, loss_ce: 0.004412
2022-01-10 01:52:22,232 iteration 5887 : loss : 0.017327, loss_ce: 0.006704
2022-01-10 01:52:23,889 iteration 5888 : loss : 0.022656, loss_ce: 0.007437
2022-01-10 01:52:25,542 iteration 5889 : loss : 0.023217, loss_ce: 0.010655
2022-01-10 01:52:27,135 iteration 5890 : loss : 0.015317, loss_ce: 0.005229
2022-01-10 01:52:28,769 iteration 5891 : loss : 0.025997, loss_ce: 0.009508
2022-01-10 01:52:30,444 iteration 5892 : loss : 0.022442, loss_ce: 0.009096
2022-01-10 01:52:31,988 iteration 5893 : loss : 0.013533, loss_ce: 0.004580
2022-01-10 01:52:33,646 iteration 5894 : loss : 0.026777, loss_ce: 0.011639
2022-01-10 01:52:35,257 iteration 5895 : loss : 0.028577, loss_ce: 0.009486
2022-01-10 01:52:36,824 iteration 5896 : loss : 0.017221, loss_ce: 0.008269
2022-01-10 01:52:38,471 iteration 5897 : loss : 0.045348, loss_ce: 0.021966
2022-01-10 01:52:40,094 iteration 5898 : loss : 0.013745, loss_ce: 0.004546
2022-01-10 01:52:41,650 iteration 5899 : loss : 0.015646, loss_ce: 0.005831
 87%|█████████████████████████▏   | 347/400 [2:49:16<25:32, 28.91s/it]2022-01-10 01:52:43,307 iteration 5900 : loss : 0.016314, loss_ce: 0.005657
2022-01-10 01:52:44,962 iteration 5901 : loss : 0.031499, loss_ce: 0.010644
2022-01-10 01:52:46,541 iteration 5902 : loss : 0.018627, loss_ce: 0.007311
2022-01-10 01:52:48,189 iteration 5903 : loss : 0.019236, loss_ce: 0.008528
2022-01-10 01:52:49,756 iteration 5904 : loss : 0.019870, loss_ce: 0.007677
2022-01-10 01:52:51,382 iteration 5905 : loss : 0.023293, loss_ce: 0.007591
2022-01-10 01:52:52,910 iteration 5906 : loss : 0.018048, loss_ce: 0.007083
2022-01-10 01:52:54,519 iteration 5907 : loss : 0.012253, loss_ce: 0.004183
2022-01-10 01:52:56,107 iteration 5908 : loss : 0.015311, loss_ce: 0.004997
2022-01-10 01:52:57,598 iteration 5909 : loss : 0.014527, loss_ce: 0.004195
2022-01-10 01:52:59,181 iteration 5910 : loss : 0.022204, loss_ce: 0.007111
2022-01-10 01:53:00,804 iteration 5911 : loss : 0.033593, loss_ce: 0.011688
2022-01-10 01:53:02,325 iteration 5912 : loss : 0.018434, loss_ce: 0.006380
2022-01-10 01:53:03,878 iteration 5913 : loss : 0.018438, loss_ce: 0.009839
2022-01-10 01:53:05,466 iteration 5914 : loss : 0.021731, loss_ce: 0.008163
2022-01-10 01:53:06,961 iteration 5915 : loss : 0.030110, loss_ce: 0.013306
2022-01-10 01:53:08,582 iteration 5916 : loss : 0.030301, loss_ce: 0.011246
 87%|█████████████████████████▏   | 348/400 [2:49:43<24:32, 28.31s/it]2022-01-10 01:53:10,186 iteration 5917 : loss : 0.017022, loss_ce: 0.006074
2022-01-10 01:53:11,702 iteration 5918 : loss : 0.017150, loss_ce: 0.006933
2022-01-10 01:53:13,313 iteration 5919 : loss : 0.026060, loss_ce: 0.012471
2022-01-10 01:53:14,941 iteration 5920 : loss : 0.021382, loss_ce: 0.011687
2022-01-10 01:53:16,661 iteration 5921 : loss : 0.037908, loss_ce: 0.009973
2022-01-10 01:53:18,226 iteration 5922 : loss : 0.017727, loss_ce: 0.005535
2022-01-10 01:53:19,819 iteration 5923 : loss : 0.016491, loss_ce: 0.005198
2022-01-10 01:53:21,503 iteration 5924 : loss : 0.020061, loss_ce: 0.005214
2022-01-10 01:53:23,097 iteration 5925 : loss : 0.019593, loss_ce: 0.007601
2022-01-10 01:53:24,676 iteration 5926 : loss : 0.032971, loss_ce: 0.012175
2022-01-10 01:53:26,276 iteration 5927 : loss : 0.016409, loss_ce: 0.006742
2022-01-10 01:53:27,793 iteration 5928 : loss : 0.016689, loss_ce: 0.004503
2022-01-10 01:53:29,312 iteration 5929 : loss : 0.015520, loss_ce: 0.007624
2022-01-10 01:53:30,833 iteration 5930 : loss : 0.018861, loss_ce: 0.008615
2022-01-10 01:53:32,417 iteration 5931 : loss : 0.033304, loss_ce: 0.009626
2022-01-10 01:53:34,041 iteration 5932 : loss : 0.017148, loss_ce: 0.005738
2022-01-10 01:53:35,604 iteration 5933 : loss : 0.016825, loss_ce: 0.006173
 87%|█████████████████████████▎   | 349/400 [2:50:10<23:44, 27.92s/it]2022-01-10 01:53:37,189 iteration 5934 : loss : 0.016655, loss_ce: 0.005331
2022-01-10 01:53:38,892 iteration 5935 : loss : 0.019916, loss_ce: 0.006510
2022-01-10 01:53:40,474 iteration 5936 : loss : 0.017909, loss_ce: 0.007925
2022-01-10 01:53:42,023 iteration 5937 : loss : 0.016762, loss_ce: 0.005531
2022-01-10 01:53:43,525 iteration 5938 : loss : 0.010740, loss_ce: 0.004335
2022-01-10 01:53:45,133 iteration 5939 : loss : 0.017188, loss_ce: 0.007817
2022-01-10 01:53:46,654 iteration 5940 : loss : 0.017204, loss_ce: 0.006656
2022-01-10 01:53:48,191 iteration 5941 : loss : 0.017048, loss_ce: 0.007225
2022-01-10 01:53:49,736 iteration 5942 : loss : 0.015377, loss_ce: 0.006444
2022-01-10 01:53:51,322 iteration 5943 : loss : 0.016002, loss_ce: 0.004358
2022-01-10 01:53:52,904 iteration 5944 : loss : 0.019743, loss_ce: 0.010560
2022-01-10 01:53:54,451 iteration 5945 : loss : 0.016305, loss_ce: 0.003557
2022-01-10 01:53:56,116 iteration 5946 : loss : 0.023320, loss_ce: 0.009845
2022-01-10 01:53:57,725 iteration 5947 : loss : 0.023848, loss_ce: 0.009613
2022-01-10 01:53:59,277 iteration 5948 : loss : 0.017845, loss_ce: 0.007012
2022-01-10 01:54:00,921 iteration 5949 : loss : 0.019695, loss_ce: 0.007837
2022-01-10 01:54:00,922 Training Data Eval:
2022-01-10 01:54:08,950   Average segmentation loss on training set: 0.0098
2022-01-10 01:54:08,951 Validation Data Eval:
2022-01-10 01:54:11,724   Average segmentation loss on validation set: 0.0802
2022-01-10 01:54:13,311 iteration 5950 : loss : 0.016999, loss_ce: 0.005807
 88%|█████████████████████████▍   | 350/400 [2:50:48<25:43, 30.86s/it]2022-01-10 01:54:14,899 iteration 5951 : loss : 0.017850, loss_ce: 0.005566
2022-01-10 01:54:16,474 iteration 5952 : loss : 0.023625, loss_ce: 0.009639
2022-01-10 01:54:18,052 iteration 5953 : loss : 0.018046, loss_ce: 0.006900
2022-01-10 01:54:19,593 iteration 5954 : loss : 0.014083, loss_ce: 0.005299
2022-01-10 01:54:21,162 iteration 5955 : loss : 0.022693, loss_ce: 0.007865
2022-01-10 01:54:22,762 iteration 5956 : loss : 0.018184, loss_ce: 0.010290
2022-01-10 01:54:24,349 iteration 5957 : loss : 0.014261, loss_ce: 0.005850
2022-01-10 01:54:25,881 iteration 5958 : loss : 0.016385, loss_ce: 0.006866
2022-01-10 01:54:27,444 iteration 5959 : loss : 0.017913, loss_ce: 0.007172
2022-01-10 01:54:28,971 iteration 5960 : loss : 0.017403, loss_ce: 0.005257
2022-01-10 01:54:30,491 iteration 5961 : loss : 0.015355, loss_ce: 0.003718
2022-01-10 01:54:32,049 iteration 5962 : loss : 0.034166, loss_ce: 0.007976
2022-01-10 01:54:33,652 iteration 5963 : loss : 0.018225, loss_ce: 0.007460
2022-01-10 01:54:35,199 iteration 5964 : loss : 0.013388, loss_ce: 0.005927
2022-01-10 01:54:36,713 iteration 5965 : loss : 0.014369, loss_ce: 0.006270
2022-01-10 01:54:38,332 iteration 5966 : loss : 0.016651, loss_ce: 0.004736
2022-01-10 01:54:39,957 iteration 5967 : loss : 0.015262, loss_ce: 0.006022
 88%|█████████████████████████▍   | 351/400 [2:51:15<24:10, 29.60s/it]2022-01-10 01:54:41,658 iteration 5968 : loss : 0.023621, loss_ce: 0.008483
2022-01-10 01:54:43,209 iteration 5969 : loss : 0.019417, loss_ce: 0.008363
2022-01-10 01:54:44,819 iteration 5970 : loss : 0.018629, loss_ce: 0.007384
2022-01-10 01:54:46,327 iteration 5971 : loss : 0.013883, loss_ce: 0.004306
2022-01-10 01:54:47,807 iteration 5972 : loss : 0.014143, loss_ce: 0.004534
2022-01-10 01:54:49,380 iteration 5973 : loss : 0.025327, loss_ce: 0.013113
2022-01-10 01:54:51,047 iteration 5974 : loss : 0.041069, loss_ce: 0.016089
2022-01-10 01:54:52,568 iteration 5975 : loss : 0.022387, loss_ce: 0.008723
2022-01-10 01:54:54,111 iteration 5976 : loss : 0.014784, loss_ce: 0.004341
2022-01-10 01:54:55,632 iteration 5977 : loss : 0.017389, loss_ce: 0.004203
2022-01-10 01:54:57,146 iteration 5978 : loss : 0.022843, loss_ce: 0.011609
2022-01-10 01:54:58,707 iteration 5979 : loss : 0.012729, loss_ce: 0.005732
2022-01-10 01:55:00,316 iteration 5980 : loss : 0.012575, loss_ce: 0.003830
2022-01-10 01:55:01,854 iteration 5981 : loss : 0.032507, loss_ce: 0.013474
2022-01-10 01:55:03,430 iteration 5982 : loss : 0.020444, loss_ce: 0.008049
2022-01-10 01:55:05,059 iteration 5983 : loss : 0.019927, loss_ce: 0.006881
2022-01-10 01:55:06,604 iteration 5984 : loss : 0.018274, loss_ce: 0.006460
 88%|█████████████████████████▌   | 352/400 [2:51:41<22:58, 28.71s/it]2022-01-10 01:55:08,237 iteration 5985 : loss : 0.014701, loss_ce: 0.004333
2022-01-10 01:55:09,724 iteration 5986 : loss : 0.025233, loss_ce: 0.007374
2022-01-10 01:55:11,321 iteration 5987 : loss : 0.022429, loss_ce: 0.010490
2022-01-10 01:55:12,945 iteration 5988 : loss : 0.021989, loss_ce: 0.008527
2022-01-10 01:55:14,531 iteration 5989 : loss : 0.014761, loss_ce: 0.005788
2022-01-10 01:55:16,083 iteration 5990 : loss : 0.016763, loss_ce: 0.005857
2022-01-10 01:55:17,677 iteration 5991 : loss : 0.023296, loss_ce: 0.008259
2022-01-10 01:55:19,262 iteration 5992 : loss : 0.026769, loss_ce: 0.009719
2022-01-10 01:55:20,826 iteration 5993 : loss : 0.013613, loss_ce: 0.005939
2022-01-10 01:55:22,444 iteration 5994 : loss : 0.028929, loss_ce: 0.015789
2022-01-10 01:55:24,003 iteration 5995 : loss : 0.016063, loss_ce: 0.005805
2022-01-10 01:55:25,529 iteration 5996 : loss : 0.013570, loss_ce: 0.004001
2022-01-10 01:55:27,117 iteration 5997 : loss : 0.021590, loss_ce: 0.007752
2022-01-10 01:55:28,765 iteration 5998 : loss : 0.016419, loss_ce: 0.005359
2022-01-10 01:55:30,440 iteration 5999 : loss : 0.021158, loss_ce: 0.006802
2022-01-10 01:55:32,033 iteration 6000 : loss : 0.020163, loss_ce: 0.008846
2022-01-10 01:55:33,530 iteration 6001 : loss : 0.014302, loss_ce: 0.006706
 88%|█████████████████████████▌   | 353/400 [2:52:08<22:04, 28.18s/it]2022-01-10 01:55:35,171 iteration 6002 : loss : 0.023910, loss_ce: 0.008474
2022-01-10 01:55:36,711 iteration 6003 : loss : 0.014978, loss_ce: 0.005225
2022-01-10 01:55:38,330 iteration 6004 : loss : 0.020216, loss_ce: 0.009749
2022-01-10 01:55:39,935 iteration 6005 : loss : 0.020156, loss_ce: 0.006610
2022-01-10 01:55:41,612 iteration 6006 : loss : 0.017135, loss_ce: 0.007093
2022-01-10 01:55:43,211 iteration 6007 : loss : 0.012924, loss_ce: 0.004988
2022-01-10 01:55:44,797 iteration 6008 : loss : 0.015687, loss_ce: 0.005571
2022-01-10 01:55:46,296 iteration 6009 : loss : 0.012679, loss_ce: 0.005894
2022-01-10 01:55:47,811 iteration 6010 : loss : 0.014998, loss_ce: 0.006587
2022-01-10 01:55:49,417 iteration 6011 : loss : 0.019398, loss_ce: 0.006475
2022-01-10 01:55:51,054 iteration 6012 : loss : 0.017684, loss_ce: 0.004317
2022-01-10 01:55:52,672 iteration 6013 : loss : 0.019226, loss_ce: 0.006593
2022-01-10 01:55:54,279 iteration 6014 : loss : 0.028924, loss_ce: 0.007036
2022-01-10 01:55:55,822 iteration 6015 : loss : 0.017143, loss_ce: 0.006776
2022-01-10 01:55:57,396 iteration 6016 : loss : 0.022080, loss_ce: 0.008447
2022-01-10 01:55:58,967 iteration 6017 : loss : 0.016056, loss_ce: 0.007111
2022-01-10 01:56:00,560 iteration 6018 : loss : 0.026702, loss_ce: 0.009189
 88%|█████████████████████████▋   | 354/400 [2:52:35<21:20, 27.83s/it]2022-01-10 01:56:02,204 iteration 6019 : loss : 0.018659, loss_ce: 0.009812
2022-01-10 01:56:03,762 iteration 6020 : loss : 0.013833, loss_ce: 0.006524
2022-01-10 01:56:05,352 iteration 6021 : loss : 0.021312, loss_ce: 0.005353
2022-01-10 01:56:06,871 iteration 6022 : loss : 0.015483, loss_ce: 0.006308
2022-01-10 01:56:08,479 iteration 6023 : loss : 0.014861, loss_ce: 0.005745
2022-01-10 01:56:10,029 iteration 6024 : loss : 0.017706, loss_ce: 0.006033
2022-01-10 01:56:11,570 iteration 6025 : loss : 0.018259, loss_ce: 0.007871
2022-01-10 01:56:13,162 iteration 6026 : loss : 0.020745, loss_ce: 0.007561
2022-01-10 01:56:14,748 iteration 6027 : loss : 0.019538, loss_ce: 0.006445
2022-01-10 01:56:16,354 iteration 6028 : loss : 0.017198, loss_ce: 0.007695
2022-01-10 01:56:17,911 iteration 6029 : loss : 0.015795, loss_ce: 0.004079
2022-01-10 01:56:19,415 iteration 6030 : loss : 0.014531, loss_ce: 0.006919
2022-01-10 01:56:21,008 iteration 6031 : loss : 0.015112, loss_ce: 0.007190
2022-01-10 01:56:22,534 iteration 6032 : loss : 0.018354, loss_ce: 0.005679
2022-01-10 01:56:24,154 iteration 6033 : loss : 0.020350, loss_ce: 0.008023
2022-01-10 01:56:25,732 iteration 6034 : loss : 0.020078, loss_ce: 0.007268
2022-01-10 01:56:25,733 Training Data Eval:
2022-01-10 01:56:33,782   Average segmentation loss on training set: 0.0101
2022-01-10 01:56:33,783 Validation Data Eval:
2022-01-10 01:56:36,550   Average segmentation loss on validation set: 0.0687
2022-01-10 01:56:38,011 iteration 6035 : loss : 0.012877, loss_ce: 0.004957
 89%|█████████████████████████▋   | 355/400 [2:53:13<23:02, 30.72s/it]2022-01-10 01:56:39,735 iteration 6036 : loss : 0.022296, loss_ce: 0.006089
2022-01-10 01:56:41,353 iteration 6037 : loss : 0.025064, loss_ce: 0.008133
2022-01-10 01:56:42,912 iteration 6038 : loss : 0.017429, loss_ce: 0.006236
2022-01-10 01:56:44,452 iteration 6039 : loss : 0.015345, loss_ce: 0.005355
2022-01-10 01:56:46,029 iteration 6040 : loss : 0.016836, loss_ce: 0.006460
2022-01-10 01:56:47,565 iteration 6041 : loss : 0.013923, loss_ce: 0.004908
2022-01-10 01:56:49,151 iteration 6042 : loss : 0.017600, loss_ce: 0.009250
2022-01-10 01:56:50,674 iteration 6043 : loss : 0.013947, loss_ce: 0.006297
2022-01-10 01:56:52,178 iteration 6044 : loss : 0.012635, loss_ce: 0.005362
2022-01-10 01:56:53,816 iteration 6045 : loss : 0.031522, loss_ce: 0.013385
2022-01-10 01:56:55,327 iteration 6046 : loss : 0.016463, loss_ce: 0.007239
2022-01-10 01:56:56,838 iteration 6047 : loss : 0.012216, loss_ce: 0.005232
2022-01-10 01:56:58,472 iteration 6048 : loss : 0.019270, loss_ce: 0.006007
2022-01-10 01:56:59,974 iteration 6049 : loss : 0.015607, loss_ce: 0.005601
2022-01-10 01:57:01,474 iteration 6050 : loss : 0.014951, loss_ce: 0.004236
2022-01-10 01:57:02,974 iteration 6051 : loss : 0.013300, loss_ce: 0.005464
2022-01-10 01:57:04,566 iteration 6052 : loss : 0.018726, loss_ce: 0.007396
 89%|█████████████████████████▊   | 356/400 [2:53:39<21:36, 29.47s/it]2022-01-10 01:57:06,166 iteration 6053 : loss : 0.015235, loss_ce: 0.004823
2022-01-10 01:57:07,700 iteration 6054 : loss : 0.018052, loss_ce: 0.005940
2022-01-10 01:57:09,290 iteration 6055 : loss : 0.015143, loss_ce: 0.006403
2022-01-10 01:57:10,841 iteration 6056 : loss : 0.023110, loss_ce: 0.006788
2022-01-10 01:57:12,328 iteration 6057 : loss : 0.016477, loss_ce: 0.006295
2022-01-10 01:57:13,839 iteration 6058 : loss : 0.012050, loss_ce: 0.006243
2022-01-10 01:57:15,463 iteration 6059 : loss : 0.020351, loss_ce: 0.006927
2022-01-10 01:57:17,041 iteration 6060 : loss : 0.017609, loss_ce: 0.005857
2022-01-10 01:57:18,562 iteration 6061 : loss : 0.018347, loss_ce: 0.008930
2022-01-10 01:57:20,250 iteration 6062 : loss : 0.028064, loss_ce: 0.011205
2022-01-10 01:57:21,774 iteration 6063 : loss : 0.015377, loss_ce: 0.006797
2022-01-10 01:57:23,309 iteration 6064 : loss : 0.013471, loss_ce: 0.004249
2022-01-10 01:57:24,901 iteration 6065 : loss : 0.029212, loss_ce: 0.011498
2022-01-10 01:57:26,404 iteration 6066 : loss : 0.017403, loss_ce: 0.006596
2022-01-10 01:57:27,995 iteration 6067 : loss : 0.024775, loss_ce: 0.009093
2022-01-10 01:57:29,564 iteration 6068 : loss : 0.035479, loss_ce: 0.005712
2022-01-10 01:57:31,210 iteration 6069 : loss : 0.040460, loss_ce: 0.018442
 89%|█████████████████████████▉   | 357/400 [2:54:06<20:30, 28.62s/it]2022-01-10 01:57:32,849 iteration 6070 : loss : 0.015408, loss_ce: 0.006692
2022-01-10 01:57:34,423 iteration 6071 : loss : 0.015171, loss_ce: 0.006110
2022-01-10 01:57:35,980 iteration 6072 : loss : 0.025775, loss_ce: 0.008189
2022-01-10 01:57:37,553 iteration 6073 : loss : 0.019245, loss_ce: 0.005533
2022-01-10 01:57:39,039 iteration 6074 : loss : 0.015695, loss_ce: 0.006761
2022-01-10 01:57:40,640 iteration 6075 : loss : 0.020719, loss_ce: 0.008964
2022-01-10 01:57:42,214 iteration 6076 : loss : 0.015157, loss_ce: 0.006558
2022-01-10 01:57:43,677 iteration 6077 : loss : 0.013234, loss_ce: 0.004680
2022-01-10 01:57:45,228 iteration 6078 : loss : 0.016535, loss_ce: 0.008530
2022-01-10 01:57:46,861 iteration 6079 : loss : 0.017455, loss_ce: 0.008177
2022-01-10 01:57:48,365 iteration 6080 : loss : 0.013044, loss_ce: 0.004556
2022-01-10 01:57:50,009 iteration 6081 : loss : 0.033048, loss_ce: 0.013871
2022-01-10 01:57:51,571 iteration 6082 : loss : 0.033419, loss_ce: 0.007120
2022-01-10 01:57:53,181 iteration 6083 : loss : 0.018918, loss_ce: 0.005485
2022-01-10 01:57:54,710 iteration 6084 : loss : 0.018776, loss_ce: 0.004767
2022-01-10 01:57:56,235 iteration 6085 : loss : 0.021958, loss_ce: 0.007300
2022-01-10 01:57:57,957 iteration 6086 : loss : 0.027354, loss_ce: 0.007594
 90%|█████████████████████████▉   | 358/400 [2:54:33<19:38, 28.06s/it]2022-01-10 01:57:59,528 iteration 6087 : loss : 0.018513, loss_ce: 0.007618
2022-01-10 01:58:01,104 iteration 6088 : loss : 0.017187, loss_ce: 0.007739
2022-01-10 01:58:02,653 iteration 6089 : loss : 0.021819, loss_ce: 0.011643
2022-01-10 01:58:04,209 iteration 6090 : loss : 0.024415, loss_ce: 0.007894
2022-01-10 01:58:05,771 iteration 6091 : loss : 0.012564, loss_ce: 0.004900
2022-01-10 01:58:07,344 iteration 6092 : loss : 0.013710, loss_ce: 0.005380
2022-01-10 01:58:08,834 iteration 6093 : loss : 0.017480, loss_ce: 0.005993
2022-01-10 01:58:10,348 iteration 6094 : loss : 0.020799, loss_ce: 0.007572
2022-01-10 01:58:11,936 iteration 6095 : loss : 0.020078, loss_ce: 0.007799
2022-01-10 01:58:13,525 iteration 6096 : loss : 0.020442, loss_ce: 0.006574
2022-01-10 01:58:15,020 iteration 6097 : loss : 0.014839, loss_ce: 0.006951
2022-01-10 01:58:16,595 iteration 6098 : loss : 0.021810, loss_ce: 0.008747
2022-01-10 01:58:18,203 iteration 6099 : loss : 0.019653, loss_ce: 0.005500
2022-01-10 01:58:19,847 iteration 6100 : loss : 0.024655, loss_ce: 0.008115
2022-01-10 01:58:21,463 iteration 6101 : loss : 0.016824, loss_ce: 0.006824
2022-01-10 01:58:22,992 iteration 6102 : loss : 0.023898, loss_ce: 0.007875
2022-01-10 01:58:24,452 iteration 6103 : loss : 0.010677, loss_ce: 0.004071
 90%|██████████████████████████   | 359/400 [2:54:59<18:51, 27.59s/it]2022-01-10 01:58:26,118 iteration 6104 : loss : 0.027933, loss_ce: 0.008667
2022-01-10 01:58:27,633 iteration 6105 : loss : 0.021335, loss_ce: 0.008174
2022-01-10 01:58:29,115 iteration 6106 : loss : 0.016306, loss_ce: 0.005656
2022-01-10 01:58:30,626 iteration 6107 : loss : 0.023714, loss_ce: 0.007863
2022-01-10 01:58:32,233 iteration 6108 : loss : 0.019225, loss_ce: 0.005945
2022-01-10 01:58:33,776 iteration 6109 : loss : 0.014914, loss_ce: 0.005209
2022-01-10 01:58:35,356 iteration 6110 : loss : 0.016779, loss_ce: 0.007903
2022-01-10 01:58:36,974 iteration 6111 : loss : 0.029922, loss_ce: 0.012486
2022-01-10 01:58:38,573 iteration 6112 : loss : 0.018650, loss_ce: 0.006016
2022-01-10 01:58:40,204 iteration 6113 : loss : 0.018612, loss_ce: 0.005979
2022-01-10 01:58:41,755 iteration 6114 : loss : 0.026144, loss_ce: 0.012211
2022-01-10 01:58:43,401 iteration 6115 : loss : 0.036427, loss_ce: 0.014889
2022-01-10 01:58:44,998 iteration 6116 : loss : 0.017253, loss_ce: 0.008025
2022-01-10 01:58:46,535 iteration 6117 : loss : 0.019426, loss_ce: 0.007082
2022-01-10 01:58:48,141 iteration 6118 : loss : 0.015141, loss_ce: 0.006466
2022-01-10 01:58:49,695 iteration 6119 : loss : 0.020120, loss_ce: 0.006636
2022-01-10 01:58:49,695 Training Data Eval:
2022-01-10 01:58:57,730   Average segmentation loss on training set: 0.0100
2022-01-10 01:58:57,730 Validation Data Eval:
2022-01-10 01:59:00,501   Average segmentation loss on validation set: 0.0750
2022-01-10 01:59:02,052 iteration 6120 : loss : 0.018168, loss_ce: 0.007997
 90%|██████████████████████████   | 360/400 [2:55:37<20:23, 30.59s/it]2022-01-10 01:59:03,695 iteration 6121 : loss : 0.020725, loss_ce: 0.009095
2022-01-10 01:59:05,310 iteration 6122 : loss : 0.021725, loss_ce: 0.007695
2022-01-10 01:59:06,958 iteration 6123 : loss : 0.017598, loss_ce: 0.007117
2022-01-10 01:59:08,540 iteration 6124 : loss : 0.021794, loss_ce: 0.007821
2022-01-10 01:59:10,207 iteration 6125 : loss : 0.021240, loss_ce: 0.008428
2022-01-10 01:59:11,770 iteration 6126 : loss : 0.029447, loss_ce: 0.007146
2022-01-10 01:59:13,319 iteration 6127 : loss : 0.017997, loss_ce: 0.006798
2022-01-10 01:59:14,960 iteration 6128 : loss : 0.033549, loss_ce: 0.009145
2022-01-10 01:59:16,536 iteration 6129 : loss : 0.017067, loss_ce: 0.006906
2022-01-10 01:59:18,189 iteration 6130 : loss : 0.018085, loss_ce: 0.006876
2022-01-10 01:59:19,781 iteration 6131 : loss : 0.035432, loss_ce: 0.019228
2022-01-10 01:59:21,367 iteration 6132 : loss : 0.021700, loss_ce: 0.004426
2022-01-10 01:59:22,901 iteration 6133 : loss : 0.015752, loss_ce: 0.008033
2022-01-10 01:59:24,398 iteration 6134 : loss : 0.011173, loss_ce: 0.004856
2022-01-10 01:59:25,902 iteration 6135 : loss : 0.012469, loss_ce: 0.004773
2022-01-10 01:59:27,488 iteration 6136 : loss : 0.015897, loss_ce: 0.005391
2022-01-10 01:59:29,037 iteration 6137 : loss : 0.017213, loss_ce: 0.006137
 90%|██████████████████████████▏  | 361/400 [2:56:04<19:10, 29.51s/it]2022-01-10 01:59:30,623 iteration 6138 : loss : 0.014240, loss_ce: 0.004885
2022-01-10 01:59:32,225 iteration 6139 : loss : 0.016786, loss_ce: 0.006968
2022-01-10 01:59:33,788 iteration 6140 : loss : 0.013124, loss_ce: 0.004832
2022-01-10 01:59:35,389 iteration 6141 : loss : 0.019667, loss_ce: 0.006837
2022-01-10 01:59:37,108 iteration 6142 : loss : 0.021004, loss_ce: 0.007808
2022-01-10 01:59:38,622 iteration 6143 : loss : 0.014611, loss_ce: 0.006989
2022-01-10 01:59:40,207 iteration 6144 : loss : 0.012896, loss_ce: 0.005215
2022-01-10 01:59:41,792 iteration 6145 : loss : 0.019670, loss_ce: 0.006941
2022-01-10 01:59:43,357 iteration 6146 : loss : 0.015608, loss_ce: 0.006868
2022-01-10 01:59:44,922 iteration 6147 : loss : 0.016993, loss_ce: 0.004564
2022-01-10 01:59:46,444 iteration 6148 : loss : 0.011654, loss_ce: 0.003962
2022-01-10 01:59:47,964 iteration 6149 : loss : 0.033775, loss_ce: 0.014219
2022-01-10 01:59:49,505 iteration 6150 : loss : 0.020946, loss_ce: 0.006316
2022-01-10 01:59:51,062 iteration 6151 : loss : 0.018755, loss_ce: 0.004219
2022-01-10 01:59:52,584 iteration 6152 : loss : 0.017151, loss_ce: 0.006952
2022-01-10 01:59:54,211 iteration 6153 : loss : 0.028323, loss_ce: 0.011597
2022-01-10 01:59:55,872 iteration 6154 : loss : 0.025826, loss_ce: 0.010139
 90%|██████████████████████████▏  | 362/400 [2:56:30<18:10, 28.71s/it]2022-01-10 01:59:57,472 iteration 6155 : loss : 0.018028, loss_ce: 0.007460
2022-01-10 01:59:59,031 iteration 6156 : loss : 0.014035, loss_ce: 0.005776
2022-01-10 02:00:00,647 iteration 6157 : loss : 0.015735, loss_ce: 0.005238
2022-01-10 02:00:02,193 iteration 6158 : loss : 0.017159, loss_ce: 0.005832
2022-01-10 02:00:03,676 iteration 6159 : loss : 0.011468, loss_ce: 0.004285
2022-01-10 02:00:05,424 iteration 6160 : loss : 0.024734, loss_ce: 0.010683
2022-01-10 02:00:06,923 iteration 6161 : loss : 0.016835, loss_ce: 0.005312
2022-01-10 02:00:08,509 iteration 6162 : loss : 0.013300, loss_ce: 0.005016
2022-01-10 02:00:10,070 iteration 6163 : loss : 0.016334, loss_ce: 0.006466
2022-01-10 02:00:11,602 iteration 6164 : loss : 0.015977, loss_ce: 0.004323
2022-01-10 02:00:13,183 iteration 6165 : loss : 0.013981, loss_ce: 0.004981
2022-01-10 02:00:14,789 iteration 6166 : loss : 0.021089, loss_ce: 0.009053
2022-01-10 02:00:16,281 iteration 6167 : loss : 0.011761, loss_ce: 0.003822
2022-01-10 02:00:17,960 iteration 6168 : loss : 0.025352, loss_ce: 0.007749
2022-01-10 02:00:19,518 iteration 6169 : loss : 0.013487, loss_ce: 0.005431
2022-01-10 02:00:21,120 iteration 6170 : loss : 0.015543, loss_ce: 0.006730
2022-01-10 02:00:22,686 iteration 6171 : loss : 0.018262, loss_ce: 0.005337
 91%|██████████████████████████▎  | 363/400 [2:56:57<17:21, 28.14s/it]2022-01-10 02:00:24,304 iteration 6172 : loss : 0.015473, loss_ce: 0.005322
2022-01-10 02:00:25,783 iteration 6173 : loss : 0.013717, loss_ce: 0.004737
2022-01-10 02:00:27,326 iteration 6174 : loss : 0.018409, loss_ce: 0.008620
2022-01-10 02:00:28,830 iteration 6175 : loss : 0.011087, loss_ce: 0.003139
2022-01-10 02:00:30,415 iteration 6176 : loss : 0.016658, loss_ce: 0.008830
2022-01-10 02:00:32,077 iteration 6177 : loss : 0.023372, loss_ce: 0.010961
2022-01-10 02:00:33,701 iteration 6178 : loss : 0.017545, loss_ce: 0.006070
2022-01-10 02:00:35,306 iteration 6179 : loss : 0.019025, loss_ce: 0.006433
2022-01-10 02:00:36,907 iteration 6180 : loss : 0.026274, loss_ce: 0.004075
2022-01-10 02:00:38,453 iteration 6181 : loss : 0.020769, loss_ce: 0.009538
2022-01-10 02:00:40,043 iteration 6182 : loss : 0.013592, loss_ce: 0.005015
2022-01-10 02:00:41,571 iteration 6183 : loss : 0.030948, loss_ce: 0.010995
2022-01-10 02:00:43,098 iteration 6184 : loss : 0.014084, loss_ce: 0.005129
2022-01-10 02:00:44,710 iteration 6185 : loss : 0.025108, loss_ce: 0.007714
2022-01-10 02:00:46,219 iteration 6186 : loss : 0.014242, loss_ce: 0.005624
2022-01-10 02:00:47,745 iteration 6187 : loss : 0.035238, loss_ce: 0.013815
2022-01-10 02:00:49,261 iteration 6188 : loss : 0.020602, loss_ce: 0.006551
 91%|██████████████████████████▍  | 364/400 [2:57:24<16:36, 27.67s/it]2022-01-10 02:00:50,966 iteration 6189 : loss : 0.030504, loss_ce: 0.010476
2022-01-10 02:00:52,548 iteration 6190 : loss : 0.017817, loss_ce: 0.008050
2022-01-10 02:00:54,190 iteration 6191 : loss : 0.019099, loss_ce: 0.006175
2022-01-10 02:00:55,795 iteration 6192 : loss : 0.014885, loss_ce: 0.005645
2022-01-10 02:00:57,329 iteration 6193 : loss : 0.014604, loss_ce: 0.004820
2022-01-10 02:00:58,941 iteration 6194 : loss : 0.018942, loss_ce: 0.006385
2022-01-10 02:01:00,582 iteration 6195 : loss : 0.022142, loss_ce: 0.007809
2022-01-10 02:01:02,116 iteration 6196 : loss : 0.015454, loss_ce: 0.004747
2022-01-10 02:01:03,779 iteration 6197 : loss : 0.028228, loss_ce: 0.009318
2022-01-10 02:01:05,442 iteration 6198 : loss : 0.016824, loss_ce: 0.006457
2022-01-10 02:01:06,984 iteration 6199 : loss : 0.012745, loss_ce: 0.004786
2022-01-10 02:01:08,585 iteration 6200 : loss : 0.021017, loss_ce: 0.009401
2022-01-10 02:01:10,162 iteration 6201 : loss : 0.028237, loss_ce: 0.013119
2022-01-10 02:01:11,720 iteration 6202 : loss : 0.013268, loss_ce: 0.004323
2022-01-10 02:01:13,271 iteration 6203 : loss : 0.020022, loss_ce: 0.006371
2022-01-10 02:01:14,886 iteration 6204 : loss : 0.018311, loss_ce: 0.008553
2022-01-10 02:01:14,886 Training Data Eval:
2022-01-10 02:01:22,926   Average segmentation loss on training set: 0.0100
2022-01-10 02:01:22,926 Validation Data Eval:
2022-01-10 02:01:25,697   Average segmentation loss on validation set: 0.0710
2022-01-10 02:01:27,362 iteration 6205 : loss : 0.024662, loss_ce: 0.010784
 91%|██████████████████████████▍  | 365/400 [2:58:02<17:57, 30.80s/it]2022-01-10 02:01:29,014 iteration 6206 : loss : 0.018054, loss_ce: 0.008815
2022-01-10 02:01:30,596 iteration 6207 : loss : 0.020480, loss_ce: 0.011762
2022-01-10 02:01:32,192 iteration 6208 : loss : 0.016991, loss_ce: 0.006655
2022-01-10 02:01:33,788 iteration 6209 : loss : 0.024100, loss_ce: 0.008695
2022-01-10 02:01:35,341 iteration 6210 : loss : 0.018964, loss_ce: 0.006782
2022-01-10 02:01:36,852 iteration 6211 : loss : 0.015047, loss_ce: 0.005463
2022-01-10 02:01:38,397 iteration 6212 : loss : 0.018734, loss_ce: 0.006631
2022-01-10 02:01:39,965 iteration 6213 : loss : 0.013366, loss_ce: 0.005127
2022-01-10 02:01:41,518 iteration 6214 : loss : 0.012552, loss_ce: 0.004948
2022-01-10 02:01:43,142 iteration 6215 : loss : 0.021195, loss_ce: 0.007591
2022-01-10 02:01:44,698 iteration 6216 : loss : 0.014249, loss_ce: 0.006096
2022-01-10 02:01:46,198 iteration 6217 : loss : 0.018459, loss_ce: 0.006443
2022-01-10 02:01:47,771 iteration 6218 : loss : 0.021028, loss_ce: 0.009359
2022-01-10 02:01:49,328 iteration 6219 : loss : 0.014561, loss_ce: 0.005156
2022-01-10 02:01:50,863 iteration 6220 : loss : 0.012408, loss_ce: 0.002856
2022-01-10 02:01:52,451 iteration 6221 : loss : 0.018886, loss_ce: 0.005177
2022-01-10 02:01:53,974 iteration 6222 : loss : 0.012791, loss_ce: 0.006311
 92%|██████████████████████████▌  | 366/400 [2:58:29<16:44, 29.54s/it]2022-01-10 02:01:55,628 iteration 6223 : loss : 0.019016, loss_ce: 0.008644
2022-01-10 02:01:57,191 iteration 6224 : loss : 0.017391, loss_ce: 0.007328
2022-01-10 02:01:58,766 iteration 6225 : loss : 0.024319, loss_ce: 0.007864
2022-01-10 02:02:00,298 iteration 6226 : loss : 0.013654, loss_ce: 0.004507
2022-01-10 02:02:01,811 iteration 6227 : loss : 0.015005, loss_ce: 0.005741
2022-01-10 02:02:03,384 iteration 6228 : loss : 0.015676, loss_ce: 0.005310
2022-01-10 02:02:04,963 iteration 6229 : loss : 0.011622, loss_ce: 0.005328
2022-01-10 02:02:06,562 iteration 6230 : loss : 0.025764, loss_ce: 0.008026
2022-01-10 02:02:08,177 iteration 6231 : loss : 0.016889, loss_ce: 0.006608
2022-01-10 02:02:09,766 iteration 6232 : loss : 0.016249, loss_ce: 0.006578
2022-01-10 02:02:11,334 iteration 6233 : loss : 0.015342, loss_ce: 0.006558
2022-01-10 02:02:12,882 iteration 6234 : loss : 0.019258, loss_ce: 0.003828
2022-01-10 02:02:14,406 iteration 6235 : loss : 0.014451, loss_ce: 0.007447
2022-01-10 02:02:15,978 iteration 6236 : loss : 0.014657, loss_ce: 0.007130
2022-01-10 02:02:17,568 iteration 6237 : loss : 0.024496, loss_ce: 0.012389
2022-01-10 02:02:19,208 iteration 6238 : loss : 0.019594, loss_ce: 0.005363
2022-01-10 02:02:20,793 iteration 6239 : loss : 0.014662, loss_ce: 0.004322
 92%|██████████████████████████▌  | 367/400 [2:58:55<15:48, 28.73s/it]2022-01-10 02:02:22,410 iteration 6240 : loss : 0.013401, loss_ce: 0.003779
2022-01-10 02:02:23,935 iteration 6241 : loss : 0.012465, loss_ce: 0.005957
2022-01-10 02:02:25,575 iteration 6242 : loss : 0.020539, loss_ce: 0.006500
2022-01-10 02:02:27,134 iteration 6243 : loss : 0.011558, loss_ce: 0.004523
2022-01-10 02:02:28,786 iteration 6244 : loss : 0.015138, loss_ce: 0.005759
2022-01-10 02:02:30,350 iteration 6245 : loss : 0.018584, loss_ce: 0.006171
2022-01-10 02:02:31,900 iteration 6246 : loss : 0.018106, loss_ce: 0.008292
2022-01-10 02:02:33,488 iteration 6247 : loss : 0.016490, loss_ce: 0.004198
2022-01-10 02:02:35,045 iteration 6248 : loss : 0.013346, loss_ce: 0.004895
2022-01-10 02:02:36,599 iteration 6249 : loss : 0.015242, loss_ce: 0.004769
2022-01-10 02:02:38,196 iteration 6250 : loss : 0.015344, loss_ce: 0.005649
2022-01-10 02:02:39,856 iteration 6251 : loss : 0.019133, loss_ce: 0.006955
2022-01-10 02:02:41,388 iteration 6252 : loss : 0.014645, loss_ce: 0.006645
2022-01-10 02:02:43,003 iteration 6253 : loss : 0.018629, loss_ce: 0.006422
2022-01-10 02:02:44,550 iteration 6254 : loss : 0.016985, loss_ce: 0.007649
2022-01-10 02:02:46,135 iteration 6255 : loss : 0.016113, loss_ce: 0.006638
2022-01-10 02:02:47,776 iteration 6256 : loss : 0.017597, loss_ce: 0.007398
 92%|██████████████████████████▋  | 368/400 [2:59:22<15:02, 28.20s/it]2022-01-10 02:02:49,448 iteration 6257 : loss : 0.028808, loss_ce: 0.007823
2022-01-10 02:02:51,069 iteration 6258 : loss : 0.015335, loss_ce: 0.005812
2022-01-10 02:02:52,710 iteration 6259 : loss : 0.019729, loss_ce: 0.008250
2022-01-10 02:02:54,269 iteration 6260 : loss : 0.017111, loss_ce: 0.007137
2022-01-10 02:02:55,824 iteration 6261 : loss : 0.024390, loss_ce: 0.009394
2022-01-10 02:02:57,417 iteration 6262 : loss : 0.018785, loss_ce: 0.005190
2022-01-10 02:02:59,007 iteration 6263 : loss : 0.025232, loss_ce: 0.005925
2022-01-10 02:03:00,600 iteration 6264 : loss : 0.015960, loss_ce: 0.005461
2022-01-10 02:03:02,192 iteration 6265 : loss : 0.020977, loss_ce: 0.008984
2022-01-10 02:03:03,744 iteration 6266 : loss : 0.019802, loss_ce: 0.009712
2022-01-10 02:03:05,283 iteration 6267 : loss : 0.015296, loss_ce: 0.006810
2022-01-10 02:03:06,853 iteration 6268 : loss : 0.018140, loss_ce: 0.007554
2022-01-10 02:03:08,299 iteration 6269 : loss : 0.011690, loss_ce: 0.004098
2022-01-10 02:03:09,853 iteration 6270 : loss : 0.020454, loss_ce: 0.009334
2022-01-10 02:03:11,408 iteration 6271 : loss : 0.015945, loss_ce: 0.004393
2022-01-10 02:03:13,021 iteration 6272 : loss : 0.021011, loss_ce: 0.008347
2022-01-10 02:03:14,623 iteration 6273 : loss : 0.018756, loss_ce: 0.007507
 92%|██████████████████████████▊  | 369/400 [2:59:49<14:21, 27.80s/it]2022-01-10 02:03:16,380 iteration 6274 : loss : 0.025772, loss_ce: 0.009665
2022-01-10 02:03:17,980 iteration 6275 : loss : 0.015073, loss_ce: 0.007828
2022-01-10 02:03:19,568 iteration 6276 : loss : 0.016132, loss_ce: 0.006122
2022-01-10 02:03:21,108 iteration 6277 : loss : 0.011974, loss_ce: 0.003506
2022-01-10 02:03:22,694 iteration 6278 : loss : 0.023254, loss_ce: 0.006924
2022-01-10 02:03:24,256 iteration 6279 : loss : 0.021722, loss_ce: 0.010217
2022-01-10 02:03:25,798 iteration 6280 : loss : 0.019455, loss_ce: 0.007136
2022-01-10 02:03:27,484 iteration 6281 : loss : 0.029128, loss_ce: 0.008381
2022-01-10 02:03:29,046 iteration 6282 : loss : 0.014451, loss_ce: 0.006044
2022-01-10 02:03:30,583 iteration 6283 : loss : 0.012424, loss_ce: 0.005290
2022-01-10 02:03:32,158 iteration 6284 : loss : 0.017618, loss_ce: 0.007364
2022-01-10 02:03:33,773 iteration 6285 : loss : 0.016997, loss_ce: 0.006700
2022-01-10 02:03:35,321 iteration 6286 : loss : 0.017810, loss_ce: 0.006097
2022-01-10 02:03:36,867 iteration 6287 : loss : 0.018394, loss_ce: 0.004951
2022-01-10 02:03:38,355 iteration 6288 : loss : 0.011787, loss_ce: 0.004133
2022-01-10 02:03:39,940 iteration 6289 : loss : 0.015366, loss_ce: 0.006270
2022-01-10 02:03:39,940 Training Data Eval:
2022-01-10 02:03:47,981   Average segmentation loss on training set: 0.0092
2022-01-10 02:03:47,981 Validation Data Eval:
2022-01-10 02:03:50,757   Average segmentation loss on validation set: 0.0691
2022-01-10 02:03:52,364 iteration 6290 : loss : 0.019601, loss_ce: 0.007870
 92%|██████████████████████████▊  | 370/400 [3:00:27<15:23, 30.78s/it]2022-01-10 02:03:53,992 iteration 6291 : loss : 0.011204, loss_ce: 0.004514
2022-01-10 02:03:55,566 iteration 6292 : loss : 0.019637, loss_ce: 0.006153
2022-01-10 02:03:57,169 iteration 6293 : loss : 0.019200, loss_ce: 0.008486
2022-01-10 02:03:58,782 iteration 6294 : loss : 0.021658, loss_ce: 0.006094
2022-01-10 02:04:00,350 iteration 6295 : loss : 0.015967, loss_ce: 0.006786
2022-01-10 02:04:01,998 iteration 6296 : loss : 0.016771, loss_ce: 0.005900
2022-01-10 02:04:03,583 iteration 6297 : loss : 0.021371, loss_ce: 0.007301
2022-01-10 02:04:05,203 iteration 6298 : loss : 0.023133, loss_ce: 0.008382
2022-01-10 02:04:06,769 iteration 6299 : loss : 0.016556, loss_ce: 0.005647
2022-01-10 02:04:08,312 iteration 6300 : loss : 0.018161, loss_ce: 0.005513
2022-01-10 02:04:09,852 iteration 6301 : loss : 0.017649, loss_ce: 0.011107
2022-01-10 02:04:11,439 iteration 6302 : loss : 0.015813, loss_ce: 0.005681
2022-01-10 02:04:12,928 iteration 6303 : loss : 0.012410, loss_ce: 0.002964
2022-01-10 02:04:14,426 iteration 6304 : loss : 0.016652, loss_ce: 0.004354
2022-01-10 02:04:16,012 iteration 6305 : loss : 0.021024, loss_ce: 0.008147
2022-01-10 02:04:17,561 iteration 6306 : loss : 0.023441, loss_ce: 0.005902
2022-01-10 02:04:19,119 iteration 6307 : loss : 0.017471, loss_ce: 0.008092
 93%|██████████████████████████▉  | 371/400 [3:00:54<14:17, 29.57s/it]2022-01-10 02:04:20,700 iteration 6308 : loss : 0.014443, loss_ce: 0.003160
2022-01-10 02:04:22,280 iteration 6309 : loss : 0.022728, loss_ce: 0.008974
2022-01-10 02:04:23,839 iteration 6310 : loss : 0.019375, loss_ce: 0.009173
2022-01-10 02:04:25,421 iteration 6311 : loss : 0.018319, loss_ce: 0.008810
2022-01-10 02:04:27,016 iteration 6312 : loss : 0.028387, loss_ce: 0.009031
2022-01-10 02:04:28,656 iteration 6313 : loss : 0.024030, loss_ce: 0.011689
2022-01-10 02:04:30,222 iteration 6314 : loss : 0.021604, loss_ce: 0.009067
2022-01-10 02:04:31,758 iteration 6315 : loss : 0.016886, loss_ce: 0.005761
2022-01-10 02:04:33,323 iteration 6316 : loss : 0.015341, loss_ce: 0.005183
2022-01-10 02:04:34,878 iteration 6317 : loss : 0.018227, loss_ce: 0.004699
2022-01-10 02:04:36,507 iteration 6318 : loss : 0.022887, loss_ce: 0.008678
2022-01-10 02:04:38,099 iteration 6319 : loss : 0.023067, loss_ce: 0.008854
2022-01-10 02:04:39,658 iteration 6320 : loss : 0.015032, loss_ce: 0.004873
2022-01-10 02:04:41,274 iteration 6321 : loss : 0.019216, loss_ce: 0.009769
2022-01-10 02:04:42,903 iteration 6322 : loss : 0.029767, loss_ce: 0.015312
2022-01-10 02:04:44,614 iteration 6323 : loss : 0.023268, loss_ce: 0.008453
2022-01-10 02:04:46,267 iteration 6324 : loss : 0.020252, loss_ce: 0.010033
 93%|██████████████████████████▉  | 372/400 [3:01:21<13:27, 28.85s/it]2022-01-10 02:04:47,936 iteration 6325 : loss : 0.028814, loss_ce: 0.010109
2022-01-10 02:04:49,484 iteration 6326 : loss : 0.018707, loss_ce: 0.008400
2022-01-10 02:04:51,052 iteration 6327 : loss : 0.018752, loss_ce: 0.006533
2022-01-10 02:04:52,676 iteration 6328 : loss : 0.016941, loss_ce: 0.009366
2022-01-10 02:04:54,185 iteration 6329 : loss : 0.015089, loss_ce: 0.007408
2022-01-10 02:04:55,746 iteration 6330 : loss : 0.016429, loss_ce: 0.006447
2022-01-10 02:04:57,372 iteration 6331 : loss : 0.018113, loss_ce: 0.006734
2022-01-10 02:04:58,977 iteration 6332 : loss : 0.012608, loss_ce: 0.003647
2022-01-10 02:05:00,553 iteration 6333 : loss : 0.013600, loss_ce: 0.006741
2022-01-10 02:05:02,129 iteration 6334 : loss : 0.024007, loss_ce: 0.006225
2022-01-10 02:05:03,788 iteration 6335 : loss : 0.021848, loss_ce: 0.008432
2022-01-10 02:05:05,334 iteration 6336 : loss : 0.013919, loss_ce: 0.005670
2022-01-10 02:05:06,928 iteration 6337 : loss : 0.022845, loss_ce: 0.006606
2022-01-10 02:05:08,507 iteration 6338 : loss : 0.020246, loss_ce: 0.007153
2022-01-10 02:05:10,079 iteration 6339 : loss : 0.015554, loss_ce: 0.005439
2022-01-10 02:05:11,650 iteration 6340 : loss : 0.022567, loss_ce: 0.009437
2022-01-10 02:05:13,151 iteration 6341 : loss : 0.013364, loss_ce: 0.004657
 93%|███████████████████████████  | 373/400 [3:01:48<12:42, 28.26s/it]2022-01-10 02:05:14,825 iteration 6342 : loss : 0.022349, loss_ce: 0.006500
2022-01-10 02:05:16,383 iteration 6343 : loss : 0.015405, loss_ce: 0.005679
2022-01-10 02:05:17,936 iteration 6344 : loss : 0.017395, loss_ce: 0.005375
2022-01-10 02:05:19,477 iteration 6345 : loss : 0.022340, loss_ce: 0.003012
2022-01-10 02:05:21,060 iteration 6346 : loss : 0.017342, loss_ce: 0.006119
2022-01-10 02:05:22,639 iteration 6347 : loss : 0.016960, loss_ce: 0.006814
2022-01-10 02:05:24,120 iteration 6348 : loss : 0.011004, loss_ce: 0.004644
2022-01-10 02:05:25,698 iteration 6349 : loss : 0.023458, loss_ce: 0.009530
2022-01-10 02:05:27,381 iteration 6350 : loss : 0.026400, loss_ce: 0.009498
2022-01-10 02:05:29,010 iteration 6351 : loss : 0.020891, loss_ce: 0.009760
2022-01-10 02:05:30,585 iteration 6352 : loss : 0.019720, loss_ce: 0.005706
2022-01-10 02:05:32,167 iteration 6353 : loss : 0.022841, loss_ce: 0.008713
2022-01-10 02:05:33,742 iteration 6354 : loss : 0.028490, loss_ce: 0.013310
2022-01-10 02:05:35,294 iteration 6355 : loss : 0.023836, loss_ce: 0.008309
2022-01-10 02:05:36,927 iteration 6356 : loss : 0.028701, loss_ce: 0.008158
2022-01-10 02:05:38,475 iteration 6357 : loss : 0.017766, loss_ce: 0.006178
2022-01-10 02:05:40,156 iteration 6358 : loss : 0.021404, loss_ce: 0.009213
 94%|███████████████████████████  | 374/400 [3:02:15<12:04, 27.88s/it]2022-01-10 02:05:41,844 iteration 6359 : loss : 0.028895, loss_ce: 0.007525
2022-01-10 02:05:43,370 iteration 6360 : loss : 0.017312, loss_ce: 0.007013
2022-01-10 02:05:44,971 iteration 6361 : loss : 0.030852, loss_ce: 0.011323
2022-01-10 02:05:46,580 iteration 6362 : loss : 0.020952, loss_ce: 0.008540
2022-01-10 02:05:48,212 iteration 6363 : loss : 0.025017, loss_ce: 0.009072
2022-01-10 02:05:49,754 iteration 6364 : loss : 0.014119, loss_ce: 0.003643
2022-01-10 02:05:51,339 iteration 6365 : loss : 0.017969, loss_ce: 0.009441
2022-01-10 02:05:53,005 iteration 6366 : loss : 0.028553, loss_ce: 0.012518
2022-01-10 02:05:54,694 iteration 6367 : loss : 0.019921, loss_ce: 0.008101
2022-01-10 02:05:56,322 iteration 6368 : loss : 0.021888, loss_ce: 0.008328
2022-01-10 02:05:57,905 iteration 6369 : loss : 0.013832, loss_ce: 0.005910
2022-01-10 02:05:59,491 iteration 6370 : loss : 0.021897, loss_ce: 0.008593
2022-01-10 02:06:01,166 iteration 6371 : loss : 0.036936, loss_ce: 0.010456
2022-01-10 02:06:02,711 iteration 6372 : loss : 0.012040, loss_ce: 0.005071
2022-01-10 02:06:04,244 iteration 6373 : loss : 0.012681, loss_ce: 0.004712
2022-01-10 02:06:05,822 iteration 6374 : loss : 0.027899, loss_ce: 0.011842
2022-01-10 02:06:05,822 Training Data Eval:
2022-01-10 02:06:13,861   Average segmentation loss on training set: 0.0096
2022-01-10 02:06:13,861 Validation Data Eval:
2022-01-10 02:06:16,634   Average segmentation loss on validation set: 0.0704
2022-01-10 02:06:18,217 iteration 6375 : loss : 0.037203, loss_ce: 0.010875
 94%|███████████████████████████▏ | 375/400 [3:02:53<12:53, 30.93s/it]2022-01-10 02:06:19,884 iteration 6376 : loss : 0.021361, loss_ce: 0.006837
2022-01-10 02:06:21,474 iteration 6377 : loss : 0.017739, loss_ce: 0.006657
2022-01-10 02:06:22,961 iteration 6378 : loss : 0.013963, loss_ce: 0.005894
2022-01-10 02:06:24,527 iteration 6379 : loss : 0.018735, loss_ce: 0.005246
2022-01-10 02:06:26,070 iteration 6380 : loss : 0.014375, loss_ce: 0.005458
2022-01-10 02:06:27,604 iteration 6381 : loss : 0.018286, loss_ce: 0.005440
2022-01-10 02:06:29,084 iteration 6382 : loss : 0.016975, loss_ce: 0.006118
2022-01-10 02:06:30,694 iteration 6383 : loss : 0.019006, loss_ce: 0.006834
2022-01-10 02:06:32,233 iteration 6384 : loss : 0.021059, loss_ce: 0.008716
2022-01-10 02:06:33,834 iteration 6385 : loss : 0.020437, loss_ce: 0.006332
2022-01-10 02:06:35,454 iteration 6386 : loss : 0.017728, loss_ce: 0.005685
2022-01-10 02:06:37,121 iteration 6387 : loss : 0.027026, loss_ce: 0.015299
2022-01-10 02:06:38,732 iteration 6388 : loss : 0.019325, loss_ce: 0.007921
2022-01-10 02:06:40,200 iteration 6389 : loss : 0.013956, loss_ce: 0.004640
2022-01-10 02:06:41,768 iteration 6390 : loss : 0.018206, loss_ce: 0.008328
2022-01-10 02:06:43,295 iteration 6391 : loss : 0.016603, loss_ce: 0.009610
2022-01-10 02:06:44,926 iteration 6392 : loss : 0.024506, loss_ce: 0.007814
 94%|███████████████████████████▎ | 376/400 [3:03:20<11:52, 29.67s/it]2022-01-10 02:06:46,551 iteration 6393 : loss : 0.016923, loss_ce: 0.006871
2022-01-10 02:06:48,172 iteration 6394 : loss : 0.021211, loss_ce: 0.008303
2022-01-10 02:06:49,730 iteration 6395 : loss : 0.017593, loss_ce: 0.005455
2022-01-10 02:06:51,299 iteration 6396 : loss : 0.015833, loss_ce: 0.005480
2022-01-10 02:06:52,892 iteration 6397 : loss : 0.024661, loss_ce: 0.011311
2022-01-10 02:06:54,496 iteration 6398 : loss : 0.017824, loss_ce: 0.006020
2022-01-10 02:06:56,066 iteration 6399 : loss : 0.023357, loss_ce: 0.007640
2022-01-10 02:06:57,620 iteration 6400 : loss : 0.012783, loss_ce: 0.005099
2022-01-10 02:06:59,219 iteration 6401 : loss : 0.014655, loss_ce: 0.005703
2022-01-10 02:07:00,780 iteration 6402 : loss : 0.020708, loss_ce: 0.010672
2022-01-10 02:07:02,429 iteration 6403 : loss : 0.027530, loss_ce: 0.012357
2022-01-10 02:07:03,990 iteration 6404 : loss : 0.014848, loss_ce: 0.005970
2022-01-10 02:07:05,530 iteration 6405 : loss : 0.019902, loss_ce: 0.005936
2022-01-10 02:07:07,250 iteration 6406 : loss : 0.048291, loss_ce: 0.012501
2022-01-10 02:07:08,857 iteration 6407 : loss : 0.019848, loss_ce: 0.005170
2022-01-10 02:07:10,331 iteration 6408 : loss : 0.011664, loss_ce: 0.004007
2022-01-10 02:07:11,890 iteration 6409 : loss : 0.016717, loss_ce: 0.008214
 94%|███████████████████████████▎ | 377/400 [3:03:46<11:03, 28.86s/it]2022-01-10 02:07:13,485 iteration 6410 : loss : 0.013806, loss_ce: 0.004732
2022-01-10 02:07:15,082 iteration 6411 : loss : 0.021307, loss_ce: 0.008783
2022-01-10 02:07:16,685 iteration 6412 : loss : 0.017694, loss_ce: 0.006925
2022-01-10 02:07:18,246 iteration 6413 : loss : 0.017370, loss_ce: 0.007399
2022-01-10 02:07:19,788 iteration 6414 : loss : 0.017851, loss_ce: 0.007479
2022-01-10 02:07:21,309 iteration 6415 : loss : 0.011078, loss_ce: 0.003723
2022-01-10 02:07:22,883 iteration 6416 : loss : 0.012937, loss_ce: 0.004974
2022-01-10 02:07:24,391 iteration 6417 : loss : 0.014475, loss_ce: 0.005320
2022-01-10 02:07:25,932 iteration 6418 : loss : 0.022702, loss_ce: 0.008525
2022-01-10 02:07:27,589 iteration 6419 : loss : 0.019591, loss_ce: 0.008033
2022-01-10 02:07:29,228 iteration 6420 : loss : 0.020953, loss_ce: 0.005104
2022-01-10 02:07:30,838 iteration 6421 : loss : 0.022333, loss_ce: 0.008594
2022-01-10 02:07:32,416 iteration 6422 : loss : 0.018579, loss_ce: 0.005329
2022-01-10 02:07:34,020 iteration 6423 : loss : 0.016200, loss_ce: 0.006437
2022-01-10 02:07:35,690 iteration 6424 : loss : 0.022498, loss_ce: 0.012068
2022-01-10 02:07:37,255 iteration 6425 : loss : 0.017562, loss_ce: 0.005766
2022-01-10 02:07:38,902 iteration 6426 : loss : 0.025312, loss_ce: 0.009230
 94%|███████████████████████████▍ | 378/400 [3:04:13<10:22, 28.30s/it]2022-01-10 02:07:40,566 iteration 6427 : loss : 0.015063, loss_ce: 0.004834
2022-01-10 02:07:42,153 iteration 6428 : loss : 0.017800, loss_ce: 0.005089
2022-01-10 02:07:43,741 iteration 6429 : loss : 0.017272, loss_ce: 0.005795
2022-01-10 02:07:45,366 iteration 6430 : loss : 0.019685, loss_ce: 0.008180
2022-01-10 02:07:46,882 iteration 6431 : loss : 0.020609, loss_ce: 0.006850
2022-01-10 02:07:48,472 iteration 6432 : loss : 0.013311, loss_ce: 0.005264
2022-01-10 02:07:50,043 iteration 6433 : loss : 0.013375, loss_ce: 0.004226
2022-01-10 02:07:51,583 iteration 6434 : loss : 0.016593, loss_ce: 0.006361
2022-01-10 02:07:53,148 iteration 6435 : loss : 0.016686, loss_ce: 0.006180
2022-01-10 02:07:54,704 iteration 6436 : loss : 0.014067, loss_ce: 0.006171
2022-01-10 02:07:56,277 iteration 6437 : loss : 0.016355, loss_ce: 0.005070
2022-01-10 02:07:57,888 iteration 6438 : loss : 0.026868, loss_ce: 0.007497
2022-01-10 02:07:59,417 iteration 6439 : loss : 0.014104, loss_ce: 0.006319
2022-01-10 02:08:01,032 iteration 6440 : loss : 0.024158, loss_ce: 0.010919
2022-01-10 02:08:02,547 iteration 6441 : loss : 0.012969, loss_ce: 0.005622
2022-01-10 02:08:04,131 iteration 6442 : loss : 0.018419, loss_ce: 0.005767
2022-01-10 02:08:05,629 iteration 6443 : loss : 0.014220, loss_ce: 0.005002
 95%|███████████████████████████▍ | 379/400 [3:04:40<09:44, 27.83s/it]2022-01-10 02:08:07,274 iteration 6444 : loss : 0.013484, loss_ce: 0.006193
2022-01-10 02:08:08,843 iteration 6445 : loss : 0.014516, loss_ce: 0.006645
2022-01-10 02:08:10,432 iteration 6446 : loss : 0.020543, loss_ce: 0.008359
2022-01-10 02:08:12,035 iteration 6447 : loss : 0.021494, loss_ce: 0.003389
2022-01-10 02:08:13,646 iteration 6448 : loss : 0.018595, loss_ce: 0.007483
2022-01-10 02:08:15,196 iteration 6449 : loss : 0.017122, loss_ce: 0.003665
2022-01-10 02:08:16,723 iteration 6450 : loss : 0.016402, loss_ce: 0.007095
2022-01-10 02:08:18,302 iteration 6451 : loss : 0.014832, loss_ce: 0.005370
2022-01-10 02:08:19,947 iteration 6452 : loss : 0.027273, loss_ce: 0.010956
2022-01-10 02:08:21,500 iteration 6453 : loss : 0.015936, loss_ce: 0.006329
2022-01-10 02:08:23,014 iteration 6454 : loss : 0.016054, loss_ce: 0.005901
2022-01-10 02:08:24,629 iteration 6455 : loss : 0.018270, loss_ce: 0.007964
2022-01-10 02:08:26,195 iteration 6456 : loss : 0.016782, loss_ce: 0.005787
2022-01-10 02:08:27,787 iteration 6457 : loss : 0.018438, loss_ce: 0.006186
2022-01-10 02:08:29,327 iteration 6458 : loss : 0.014588, loss_ce: 0.005542
2022-01-10 02:08:30,882 iteration 6459 : loss : 0.010625, loss_ce: 0.002649
2022-01-10 02:08:30,882 Training Data Eval:
2022-01-10 02:08:38,923   Average segmentation loss on training set: 0.0092
2022-01-10 02:08:38,923 Validation Data Eval:
2022-01-10 02:08:41,679   Average segmentation loss on validation set: 0.0744
2022-01-10 02:08:43,288 iteration 6460 : loss : 0.019416, loss_ce: 0.009099
 95%|███████████████████████████▌ | 380/400 [3:05:18<10:15, 30.78s/it]2022-01-10 02:08:44,972 iteration 6461 : loss : 0.019020, loss_ce: 0.007061
2022-01-10 02:08:46,517 iteration 6462 : loss : 0.016694, loss_ce: 0.008356
2022-01-10 02:08:48,143 iteration 6463 : loss : 0.021926, loss_ce: 0.009217
2022-01-10 02:08:49,721 iteration 6464 : loss : 0.015505, loss_ce: 0.004344
2022-01-10 02:08:51,336 iteration 6465 : loss : 0.019305, loss_ce: 0.006808
2022-01-10 02:08:52,866 iteration 6466 : loss : 0.012776, loss_ce: 0.004162
2022-01-10 02:08:54,419 iteration 6467 : loss : 0.017075, loss_ce: 0.006309
2022-01-10 02:08:56,055 iteration 6468 : loss : 0.014598, loss_ce: 0.005026
2022-01-10 02:08:57,613 iteration 6469 : loss : 0.017509, loss_ce: 0.007486
2022-01-10 02:08:59,155 iteration 6470 : loss : 0.012580, loss_ce: 0.003510
2022-01-10 02:09:00,727 iteration 6471 : loss : 0.014946, loss_ce: 0.005425
2022-01-10 02:09:02,312 iteration 6472 : loss : 0.026643, loss_ce: 0.008621
2022-01-10 02:09:03,855 iteration 6473 : loss : 0.020915, loss_ce: 0.007161
2022-01-10 02:09:05,490 iteration 6474 : loss : 0.012739, loss_ce: 0.005437
2022-01-10 02:09:07,089 iteration 6475 : loss : 0.018829, loss_ce: 0.009895
2022-01-10 02:09:08,555 iteration 6476 : loss : 0.013169, loss_ce: 0.004867
2022-01-10 02:09:10,218 iteration 6477 : loss : 0.018817, loss_ce: 0.007240
 95%|███████████████████████████▌ | 381/400 [3:05:45<09:22, 29.62s/it]2022-01-10 02:09:11,908 iteration 6478 : loss : 0.021260, loss_ce: 0.006892
2022-01-10 02:09:13,466 iteration 6479 : loss : 0.018879, loss_ce: 0.006473
2022-01-10 02:09:15,027 iteration 6480 : loss : 0.018875, loss_ce: 0.008877
2022-01-10 02:09:16,627 iteration 6481 : loss : 0.015064, loss_ce: 0.006581
2022-01-10 02:09:18,132 iteration 6482 : loss : 0.013367, loss_ce: 0.005430
2022-01-10 02:09:19,733 iteration 6483 : loss : 0.022914, loss_ce: 0.005495
2022-01-10 02:09:21,265 iteration 6484 : loss : 0.013043, loss_ce: 0.005042
2022-01-10 02:09:22,824 iteration 6485 : loss : 0.022964, loss_ce: 0.009829
2022-01-10 02:09:24,440 iteration 6486 : loss : 0.017993, loss_ce: 0.006436
2022-01-10 02:09:26,115 iteration 6487 : loss : 0.026369, loss_ce: 0.010554
2022-01-10 02:09:27,726 iteration 6488 : loss : 0.015495, loss_ce: 0.006080
2022-01-10 02:09:29,264 iteration 6489 : loss : 0.013784, loss_ce: 0.007379
2022-01-10 02:09:30,856 iteration 6490 : loss : 0.019549, loss_ce: 0.005398
2022-01-10 02:09:32,407 iteration 6491 : loss : 0.014936, loss_ce: 0.005753
2022-01-10 02:09:34,057 iteration 6492 : loss : 0.021299, loss_ce: 0.009364
2022-01-10 02:09:35,655 iteration 6493 : loss : 0.026498, loss_ce: 0.009698
2022-01-10 02:09:37,132 iteration 6494 : loss : 0.009673, loss_ce: 0.003810
 96%|███████████████████████████▋ | 382/400 [3:06:12<08:38, 28.81s/it]2022-01-10 02:09:38,730 iteration 6495 : loss : 0.016679, loss_ce: 0.006670
2022-01-10 02:09:40,304 iteration 6496 : loss : 0.027828, loss_ce: 0.006739
2022-01-10 02:09:41,917 iteration 6497 : loss : 0.016033, loss_ce: 0.006877
2022-01-10 02:09:43,526 iteration 6498 : loss : 0.033632, loss_ce: 0.012285
2022-01-10 02:09:45,134 iteration 6499 : loss : 0.015751, loss_ce: 0.006475
2022-01-10 02:09:46,695 iteration 6500 : loss : 0.012998, loss_ce: 0.004829
2022-01-10 02:09:48,291 iteration 6501 : loss : 0.021729, loss_ce: 0.010080
2022-01-10 02:09:49,918 iteration 6502 : loss : 0.018529, loss_ce: 0.010536
2022-01-10 02:09:51,462 iteration 6503 : loss : 0.010655, loss_ce: 0.002885
2022-01-10 02:09:52,979 iteration 6504 : loss : 0.014849, loss_ce: 0.005403
2022-01-10 02:09:54,430 iteration 6505 : loss : 0.011348, loss_ce: 0.005411
2022-01-10 02:09:55,979 iteration 6506 : loss : 0.019444, loss_ce: 0.006760
2022-01-10 02:09:57,617 iteration 6507 : loss : 0.024308, loss_ce: 0.012167
2022-01-10 02:09:59,221 iteration 6508 : loss : 0.020620, loss_ce: 0.007927
2022-01-10 02:10:00,808 iteration 6509 : loss : 0.019195, loss_ce: 0.007734
2022-01-10 02:10:02,399 iteration 6510 : loss : 0.012475, loss_ce: 0.003967
2022-01-10 02:10:03,898 iteration 6511 : loss : 0.013435, loss_ce: 0.004837
 96%|███████████████████████████▊ | 383/400 [3:06:39<07:59, 28.20s/it]2022-01-10 02:10:05,497 iteration 6512 : loss : 0.017898, loss_ce: 0.005574
2022-01-10 02:10:07,076 iteration 6513 : loss : 0.019925, loss_ce: 0.008293
2022-01-10 02:10:08,749 iteration 6514 : loss : 0.013804, loss_ce: 0.003867
2022-01-10 02:10:10,384 iteration 6515 : loss : 0.025584, loss_ce: 0.008417
2022-01-10 02:10:11,989 iteration 6516 : loss : 0.018391, loss_ce: 0.007881
2022-01-10 02:10:13,554 iteration 6517 : loss : 0.024993, loss_ce: 0.004971
2022-01-10 02:10:15,133 iteration 6518 : loss : 0.014658, loss_ce: 0.005329
2022-01-10 02:10:16,701 iteration 6519 : loss : 0.015036, loss_ce: 0.004906
2022-01-10 02:10:18,322 iteration 6520 : loss : 0.015717, loss_ce: 0.005287
2022-01-10 02:10:19,865 iteration 6521 : loss : 0.023003, loss_ce: 0.009887
2022-01-10 02:10:21,456 iteration 6522 : loss : 0.013787, loss_ce: 0.005961
2022-01-10 02:10:22,961 iteration 6523 : loss : 0.011862, loss_ce: 0.004731
2022-01-10 02:10:24,581 iteration 6524 : loss : 0.019267, loss_ce: 0.008405
2022-01-10 02:10:26,168 iteration 6525 : loss : 0.020410, loss_ce: 0.008189
2022-01-10 02:10:27,747 iteration 6526 : loss : 0.015499, loss_ce: 0.005868
2022-01-10 02:10:29,252 iteration 6527 : loss : 0.013206, loss_ce: 0.005887
2022-01-10 02:10:30,786 iteration 6528 : loss : 0.014888, loss_ce: 0.004552
 96%|███████████████████████████▊ | 384/400 [3:07:05<07:24, 27.81s/it]2022-01-10 02:10:32,464 iteration 6529 : loss : 0.023020, loss_ce: 0.006711
2022-01-10 02:10:34,064 iteration 6530 : loss : 0.016863, loss_ce: 0.006579
2022-01-10 02:10:35,579 iteration 6531 : loss : 0.015383, loss_ce: 0.004518
2022-01-10 02:10:37,177 iteration 6532 : loss : 0.013551, loss_ce: 0.004998
2022-01-10 02:10:38,680 iteration 6533 : loss : 0.010796, loss_ce: 0.004076
2022-01-10 02:10:40,254 iteration 6534 : loss : 0.018872, loss_ce: 0.008774
2022-01-10 02:10:41,840 iteration 6535 : loss : 0.016481, loss_ce: 0.006261
2022-01-10 02:10:43,438 iteration 6536 : loss : 0.014115, loss_ce: 0.004396
2022-01-10 02:10:45,029 iteration 6537 : loss : 0.025901, loss_ce: 0.007500
2022-01-10 02:10:46,627 iteration 6538 : loss : 0.016707, loss_ce: 0.005430
2022-01-10 02:10:48,246 iteration 6539 : loss : 0.019156, loss_ce: 0.008541
2022-01-10 02:10:49,741 iteration 6540 : loss : 0.011884, loss_ce: 0.004991
2022-01-10 02:10:51,280 iteration 6541 : loss : 0.016328, loss_ce: 0.009185
2022-01-10 02:10:52,940 iteration 6542 : loss : 0.013078, loss_ce: 0.003986
2022-01-10 02:10:54,485 iteration 6543 : loss : 0.014008, loss_ce: 0.005333
2022-01-10 02:10:56,094 iteration 6544 : loss : 0.022900, loss_ce: 0.009071
2022-01-10 02:10:56,094 Training Data Eval:
2022-01-10 02:11:04,139   Average segmentation loss on training set: 0.0087
2022-01-10 02:11:04,140 Validation Data Eval:
2022-01-10 02:11:06,919   Average segmentation loss on validation set: 0.0799
2022-01-10 02:11:08,564 iteration 6545 : loss : 0.015759, loss_ce: 0.006188
 96%|███████████████████████████▉ | 385/400 [3:07:43<07:41, 30.79s/it]2022-01-10 02:11:10,335 iteration 6546 : loss : 0.015968, loss_ce: 0.006769
2022-01-10 02:11:11,909 iteration 6547 : loss : 0.016598, loss_ce: 0.007321
2022-01-10 02:11:13,515 iteration 6548 : loss : 0.012806, loss_ce: 0.004679
2022-01-10 02:11:15,060 iteration 6549 : loss : 0.017510, loss_ce: 0.006500
2022-01-10 02:11:16,675 iteration 6550 : loss : 0.026689, loss_ce: 0.008805
2022-01-10 02:11:18,315 iteration 6551 : loss : 0.024513, loss_ce: 0.009580
2022-01-10 02:11:19,996 iteration 6552 : loss : 0.023116, loss_ce: 0.008644
2022-01-10 02:11:21,701 iteration 6553 : loss : 0.022280, loss_ce: 0.006828
2022-01-10 02:11:23,304 iteration 6554 : loss : 0.024542, loss_ce: 0.008052
2022-01-10 02:11:24,889 iteration 6555 : loss : 0.014953, loss_ce: 0.007001
2022-01-10 02:11:26,467 iteration 6556 : loss : 0.016496, loss_ce: 0.003743
2022-01-10 02:11:27,947 iteration 6557 : loss : 0.010309, loss_ce: 0.004251
2022-01-10 02:11:29,513 iteration 6558 : loss : 0.023269, loss_ce: 0.007893
2022-01-10 02:11:31,011 iteration 6559 : loss : 0.012533, loss_ce: 0.004480
2022-01-10 02:11:32,581 iteration 6560 : loss : 0.014247, loss_ce: 0.006466
2022-01-10 02:11:34,192 iteration 6561 : loss : 0.014817, loss_ce: 0.004209
2022-01-10 02:11:35,727 iteration 6562 : loss : 0.014476, loss_ce: 0.005432
 96%|███████████████████████████▉ | 386/400 [3:08:10<06:55, 29.71s/it]2022-01-10 02:11:37,485 iteration 6563 : loss : 0.025064, loss_ce: 0.011340
2022-01-10 02:11:39,043 iteration 6564 : loss : 0.018221, loss_ce: 0.007153
2022-01-10 02:11:40,567 iteration 6565 : loss : 0.028141, loss_ce: 0.006250
2022-01-10 02:11:42,080 iteration 6566 : loss : 0.013006, loss_ce: 0.005284
2022-01-10 02:11:43,573 iteration 6567 : loss : 0.013651, loss_ce: 0.005287
2022-01-10 02:11:45,120 iteration 6568 : loss : 0.021696, loss_ce: 0.010166
2022-01-10 02:11:46,692 iteration 6569 : loss : 0.011413, loss_ce: 0.004364
2022-01-10 02:11:48,215 iteration 6570 : loss : 0.011368, loss_ce: 0.005123
2022-01-10 02:11:49,764 iteration 6571 : loss : 0.022596, loss_ce: 0.006664
2022-01-10 02:11:51,256 iteration 6572 : loss : 0.012122, loss_ce: 0.005082
2022-01-10 02:11:52,890 iteration 6573 : loss : 0.026335, loss_ce: 0.009643
2022-01-10 02:11:54,533 iteration 6574 : loss : 0.012747, loss_ce: 0.005205
2022-01-10 02:11:56,207 iteration 6575 : loss : 0.041280, loss_ce: 0.009932
2022-01-10 02:11:57,747 iteration 6576 : loss : 0.016725, loss_ce: 0.006964
2022-01-10 02:11:59,338 iteration 6577 : loss : 0.024004, loss_ce: 0.005540
2022-01-10 02:12:00,865 iteration 6578 : loss : 0.012949, loss_ce: 0.005037
2022-01-10 02:12:02,443 iteration 6579 : loss : 0.016734, loss_ce: 0.006435
 97%|████████████████████████████ | 387/400 [3:08:37<06:14, 28.81s/it]2022-01-10 02:12:04,086 iteration 6580 : loss : 0.018399, loss_ce: 0.006459
2022-01-10 02:12:05,635 iteration 6581 : loss : 0.016984, loss_ce: 0.005666
2022-01-10 02:12:07,274 iteration 6582 : loss : 0.018893, loss_ce: 0.006089
2022-01-10 02:12:08,854 iteration 6583 : loss : 0.018309, loss_ce: 0.006621
2022-01-10 02:12:10,449 iteration 6584 : loss : 0.015342, loss_ce: 0.006184
2022-01-10 02:12:12,079 iteration 6585 : loss : 0.027947, loss_ce: 0.009220
2022-01-10 02:12:13,683 iteration 6586 : loss : 0.032516, loss_ce: 0.010426
2022-01-10 02:12:15,246 iteration 6587 : loss : 0.018993, loss_ce: 0.007392
2022-01-10 02:12:16,833 iteration 6588 : loss : 0.019218, loss_ce: 0.007561
2022-01-10 02:12:18,349 iteration 6589 : loss : 0.025897, loss_ce: 0.008348
2022-01-10 02:12:19,984 iteration 6590 : loss : 0.025782, loss_ce: 0.011907
2022-01-10 02:12:21,618 iteration 6591 : loss : 0.018759, loss_ce: 0.007340
2022-01-10 02:12:23,166 iteration 6592 : loss : 0.013699, loss_ce: 0.005439
2022-01-10 02:12:24,813 iteration 6593 : loss : 0.023710, loss_ce: 0.005340
2022-01-10 02:12:26,382 iteration 6594 : loss : 0.014330, loss_ce: 0.006925
2022-01-10 02:12:27,937 iteration 6595 : loss : 0.012406, loss_ce: 0.004322
2022-01-10 02:12:29,567 iteration 6596 : loss : 0.031037, loss_ce: 0.012078
 97%|████████████████████████████▏| 388/400 [3:09:04<05:39, 28.30s/it]2022-01-10 02:12:31,191 iteration 6597 : loss : 0.018413, loss_ce: 0.007626
2022-01-10 02:12:32,716 iteration 6598 : loss : 0.019207, loss_ce: 0.004009
2022-01-10 02:12:34,311 iteration 6599 : loss : 0.016359, loss_ce: 0.005910
2022-01-10 02:12:35,776 iteration 6600 : loss : 0.011270, loss_ce: 0.004582
2022-01-10 02:12:37,337 iteration 6601 : loss : 0.013874, loss_ce: 0.004931
2022-01-10 02:12:38,868 iteration 6602 : loss : 0.014996, loss_ce: 0.005043
2022-01-10 02:12:40,436 iteration 6603 : loss : 0.018393, loss_ce: 0.005907
2022-01-10 02:12:41,926 iteration 6604 : loss : 0.010190, loss_ce: 0.004246
2022-01-10 02:12:43,498 iteration 6605 : loss : 0.017782, loss_ce: 0.006635
2022-01-10 02:12:45,099 iteration 6606 : loss : 0.017566, loss_ce: 0.008766
2022-01-10 02:12:46,700 iteration 6607 : loss : 0.017642, loss_ce: 0.007617
2022-01-10 02:12:48,296 iteration 6608 : loss : 0.011729, loss_ce: 0.004736
2022-01-10 02:12:49,826 iteration 6609 : loss : 0.013509, loss_ce: 0.004899
2022-01-10 02:12:51,369 iteration 6610 : loss : 0.022139, loss_ce: 0.009895
2022-01-10 02:12:52,915 iteration 6611 : loss : 0.012844, loss_ce: 0.002977
2022-01-10 02:12:54,528 iteration 6612 : loss : 0.025951, loss_ce: 0.009093
2022-01-10 02:12:55,992 iteration 6613 : loss : 0.015468, loss_ce: 0.006670
 97%|████████████████████████████▏| 389/400 [3:09:31<05:05, 27.74s/it]2022-01-10 02:12:57,620 iteration 6614 : loss : 0.018110, loss_ce: 0.007669
2022-01-10 02:12:59,206 iteration 6615 : loss : 0.013297, loss_ce: 0.005831
2022-01-10 02:13:00,765 iteration 6616 : loss : 0.024076, loss_ce: 0.007130
2022-01-10 02:13:02,465 iteration 6617 : loss : 0.019101, loss_ce: 0.006529
2022-01-10 02:13:04,059 iteration 6618 : loss : 0.018717, loss_ce: 0.006385
2022-01-10 02:13:05,723 iteration 6619 : loss : 0.025056, loss_ce: 0.010789
2022-01-10 02:13:07,345 iteration 6620 : loss : 0.019738, loss_ce: 0.008905
2022-01-10 02:13:08,905 iteration 6621 : loss : 0.023076, loss_ce: 0.007189
2022-01-10 02:13:10,465 iteration 6622 : loss : 0.016411, loss_ce: 0.008086
2022-01-10 02:13:12,032 iteration 6623 : loss : 0.026637, loss_ce: 0.009323
2022-01-10 02:13:13,643 iteration 6624 : loss : 0.025391, loss_ce: 0.009674
2022-01-10 02:13:15,275 iteration 6625 : loss : 0.013259, loss_ce: 0.005265
2022-01-10 02:13:16,841 iteration 6626 : loss : 0.020141, loss_ce: 0.005080
2022-01-10 02:13:18,334 iteration 6627 : loss : 0.028242, loss_ce: 0.008750
2022-01-10 02:13:19,894 iteration 6628 : loss : 0.016787, loss_ce: 0.008267
2022-01-10 02:13:21,454 iteration 6629 : loss : 0.014080, loss_ce: 0.006528
2022-01-10 02:13:21,454 Training Data Eval:
2022-01-10 02:13:29,500   Average segmentation loss on training set: 0.0093
2022-01-10 02:13:29,500 Validation Data Eval:
2022-01-10 02:13:32,276   Average segmentation loss on validation set: 0.0743
2022-01-10 02:13:33,884 iteration 6630 : loss : 0.023600, loss_ce: 0.007666
 98%|████████████████████████████▎| 390/400 [3:10:08<05:07, 30.79s/it]2022-01-10 02:13:35,589 iteration 6631 : loss : 0.017684, loss_ce: 0.006754
2022-01-10 02:13:37,175 iteration 6632 : loss : 0.017272, loss_ce: 0.007378
2022-01-10 02:13:38,759 iteration 6633 : loss : 0.024790, loss_ce: 0.006661
2022-01-10 02:13:40,213 iteration 6634 : loss : 0.012888, loss_ce: 0.004090
2022-01-10 02:13:41,739 iteration 6635 : loss : 0.017653, loss_ce: 0.004657
2022-01-10 02:13:43,366 iteration 6636 : loss : 0.025635, loss_ce: 0.008977
2022-01-10 02:13:44,968 iteration 6637 : loss : 0.013386, loss_ce: 0.006316
2022-01-10 02:13:46,587 iteration 6638 : loss : 0.018360, loss_ce: 0.006939
2022-01-10 02:13:48,238 iteration 6639 : loss : 0.021089, loss_ce: 0.007245
2022-01-10 02:13:49,806 iteration 6640 : loss : 0.021470, loss_ce: 0.008391
2022-01-10 02:13:51,345 iteration 6641 : loss : 0.014389, loss_ce: 0.007367
2022-01-10 02:13:52,990 iteration 6642 : loss : 0.015548, loss_ce: 0.006516
2022-01-10 02:13:54,586 iteration 6643 : loss : 0.018516, loss_ce: 0.009536
2022-01-10 02:13:56,168 iteration 6644 : loss : 0.016881, loss_ce: 0.005525
2022-01-10 02:13:57,686 iteration 6645 : loss : 0.016235, loss_ce: 0.005601
2022-01-10 02:13:59,237 iteration 6646 : loss : 0.013277, loss_ce: 0.005777
2022-01-10 02:14:00,717 iteration 6647 : loss : 0.012705, loss_ce: 0.004149
 98%|████████████████████████████▎| 391/400 [3:10:35<04:26, 29.60s/it]2022-01-10 02:14:02,299 iteration 6648 : loss : 0.020481, loss_ce: 0.006366
2022-01-10 02:14:03,931 iteration 6649 : loss : 0.017510, loss_ce: 0.007213
2022-01-10 02:14:05,442 iteration 6650 : loss : 0.017550, loss_ce: 0.006207
2022-01-10 02:14:06,972 iteration 6651 : loss : 0.011688, loss_ce: 0.004178
2022-01-10 02:14:08,614 iteration 6652 : loss : 0.029527, loss_ce: 0.010580
2022-01-10 02:14:10,245 iteration 6653 : loss : 0.017589, loss_ce: 0.007653
2022-01-10 02:14:11,783 iteration 6654 : loss : 0.011843, loss_ce: 0.004072
2022-01-10 02:14:13,377 iteration 6655 : loss : 0.013354, loss_ce: 0.005135
2022-01-10 02:14:14,982 iteration 6656 : loss : 0.013130, loss_ce: 0.005238
2022-01-10 02:14:16,650 iteration 6657 : loss : 0.020528, loss_ce: 0.006886
2022-01-10 02:14:18,246 iteration 6658 : loss : 0.026454, loss_ce: 0.010463
2022-01-10 02:14:19,866 iteration 6659 : loss : 0.021428, loss_ce: 0.009323
2022-01-10 02:14:21,405 iteration 6660 : loss : 0.015292, loss_ce: 0.004960
2022-01-10 02:14:22,902 iteration 6661 : loss : 0.011556, loss_ce: 0.004315
2022-01-10 02:14:24,490 iteration 6662 : loss : 0.018202, loss_ce: 0.008052
2022-01-10 02:14:26,119 iteration 6663 : loss : 0.018593, loss_ce: 0.006712
2022-01-10 02:14:27,685 iteration 6664 : loss : 0.016032, loss_ce: 0.005584
 98%|████████████████████████████▍| 392/400 [3:11:02<03:50, 28.81s/it]2022-01-10 02:14:29,333 iteration 6665 : loss : 0.014920, loss_ce: 0.005303
2022-01-10 02:14:30,929 iteration 6666 : loss : 0.022936, loss_ce: 0.008750
2022-01-10 02:14:32,470 iteration 6667 : loss : 0.015791, loss_ce: 0.005729
2022-01-10 02:14:33,944 iteration 6668 : loss : 0.010285, loss_ce: 0.004214
2022-01-10 02:14:35,529 iteration 6669 : loss : 0.016341, loss_ce: 0.005894
2022-01-10 02:14:37,039 iteration 6670 : loss : 0.018911, loss_ce: 0.006638
2022-01-10 02:14:38,683 iteration 6671 : loss : 0.016139, loss_ce: 0.007266
2022-01-10 02:14:40,252 iteration 6672 : loss : 0.016583, loss_ce: 0.008891
2022-01-10 02:14:41,902 iteration 6673 : loss : 0.019281, loss_ce: 0.009112
2022-01-10 02:14:43,537 iteration 6674 : loss : 0.019087, loss_ce: 0.007417
2022-01-10 02:14:45,082 iteration 6675 : loss : 0.030826, loss_ce: 0.010049
2022-01-10 02:14:46,583 iteration 6676 : loss : 0.013951, loss_ce: 0.004735
2022-01-10 02:14:48,172 iteration 6677 : loss : 0.013204, loss_ce: 0.003223
2022-01-10 02:14:49,834 iteration 6678 : loss : 0.020042, loss_ce: 0.010598
2022-01-10 02:14:51,436 iteration 6679 : loss : 0.013719, loss_ce: 0.004389
2022-01-10 02:14:52,996 iteration 6680 : loss : 0.024578, loss_ce: 0.010357
2022-01-10 02:14:54,522 iteration 6681 : loss : 0.014918, loss_ce: 0.004998
 98%|████████████████████████████▍| 393/400 [3:11:29<03:17, 28.22s/it]2022-01-10 02:14:56,154 iteration 6682 : loss : 0.017003, loss_ce: 0.005060
2022-01-10 02:14:57,770 iteration 6683 : loss : 0.018607, loss_ce: 0.006535
2022-01-10 02:14:59,367 iteration 6684 : loss : 0.032513, loss_ce: 0.005264
2022-01-10 02:15:00,928 iteration 6685 : loss : 0.015747, loss_ce: 0.005884
2022-01-10 02:15:02,520 iteration 6686 : loss : 0.015991, loss_ce: 0.005239
2022-01-10 02:15:04,111 iteration 6687 : loss : 0.021374, loss_ce: 0.008296
2022-01-10 02:15:05,699 iteration 6688 : loss : 0.023552, loss_ce: 0.009482
2022-01-10 02:15:07,246 iteration 6689 : loss : 0.014741, loss_ce: 0.006353
2022-01-10 02:15:08,881 iteration 6690 : loss : 0.017507, loss_ce: 0.006442
2022-01-10 02:15:10,398 iteration 6691 : loss : 0.012686, loss_ce: 0.004685
2022-01-10 02:15:11,917 iteration 6692 : loss : 0.013117, loss_ce: 0.006799
2022-01-10 02:15:13,504 iteration 6693 : loss : 0.013663, loss_ce: 0.005434
2022-01-10 02:15:15,008 iteration 6694 : loss : 0.010906, loss_ce: 0.004549
2022-01-10 02:15:16,551 iteration 6695 : loss : 0.016014, loss_ce: 0.005054
2022-01-10 02:15:18,112 iteration 6696 : loss : 0.014995, loss_ce: 0.007727
2022-01-10 02:15:19,686 iteration 6697 : loss : 0.016510, loss_ce: 0.005815
2022-01-10 02:15:21,178 iteration 6698 : loss : 0.018552, loss_ce: 0.006691
 98%|████████████████████████████▌| 394/400 [3:11:56<02:46, 27.75s/it]2022-01-10 02:15:22,823 iteration 6699 : loss : 0.015620, loss_ce: 0.004962
2022-01-10 02:15:24,450 iteration 6700 : loss : 0.021626, loss_ce: 0.010132
2022-01-10 02:15:26,103 iteration 6701 : loss : 0.034658, loss_ce: 0.009737
2022-01-10 02:15:27,644 iteration 6702 : loss : 0.017481, loss_ce: 0.006071
2022-01-10 02:15:29,314 iteration 6703 : loss : 0.021243, loss_ce: 0.006504
2022-01-10 02:15:30,899 iteration 6704 : loss : 0.017198, loss_ce: 0.008576
2022-01-10 02:15:32,416 iteration 6705 : loss : 0.014056, loss_ce: 0.004322
2022-01-10 02:15:33,962 iteration 6706 : loss : 0.015564, loss_ce: 0.005680
2022-01-10 02:15:35,527 iteration 6707 : loss : 0.014638, loss_ce: 0.004842
2022-01-10 02:15:37,015 iteration 6708 : loss : 0.010461, loss_ce: 0.003457
2022-01-10 02:15:38,538 iteration 6709 : loss : 0.014198, loss_ce: 0.003676
2022-01-10 02:15:40,116 iteration 6710 : loss : 0.014441, loss_ce: 0.005826
2022-01-10 02:15:41,679 iteration 6711 : loss : 0.017988, loss_ce: 0.008276
2022-01-10 02:15:43,286 iteration 6712 : loss : 0.015028, loss_ce: 0.005681
2022-01-10 02:15:44,827 iteration 6713 : loss : 0.013543, loss_ce: 0.004330
2022-01-10 02:15:46,384 iteration 6714 : loss : 0.014296, loss_ce: 0.005686
2022-01-10 02:15:46,384 Training Data Eval:
2022-01-10 02:15:54,418   Average segmentation loss on training set: 0.0090
2022-01-10 02:15:54,419 Validation Data Eval:
2022-01-10 02:15:57,193   Average segmentation loss on validation set: 0.0758
2022-01-10 02:15:58,723 iteration 6715 : loss : 0.010697, loss_ce: 0.003094
 99%|████████████████████████████▋| 395/400 [3:12:33<02:33, 30.69s/it]2022-01-10 02:16:00,270 iteration 6716 : loss : 0.012509, loss_ce: 0.004133
2022-01-10 02:16:01,870 iteration 6717 : loss : 0.018511, loss_ce: 0.005257
2022-01-10 02:16:03,505 iteration 6718 : loss : 0.023620, loss_ce: 0.011800
2022-01-10 02:16:05,125 iteration 6719 : loss : 0.024313, loss_ce: 0.008725
2022-01-10 02:16:06,726 iteration 6720 : loss : 0.032184, loss_ce: 0.005866
2022-01-10 02:16:08,336 iteration 6721 : loss : 0.016797, loss_ce: 0.007404
2022-01-10 02:16:09,895 iteration 6722 : loss : 0.018063, loss_ce: 0.009971
2022-01-10 02:16:11,476 iteration 6723 : loss : 0.018814, loss_ce: 0.007471
2022-01-10 02:16:12,998 iteration 6724 : loss : 0.015928, loss_ce: 0.006803
2022-01-10 02:16:14,561 iteration 6725 : loss : 0.011791, loss_ce: 0.003898
2022-01-10 02:16:16,052 iteration 6726 : loss : 0.010824, loss_ce: 0.003544
2022-01-10 02:16:17,717 iteration 6727 : loss : 0.018977, loss_ce: 0.006989
2022-01-10 02:16:19,249 iteration 6728 : loss : 0.021334, loss_ce: 0.006898
2022-01-10 02:16:20,866 iteration 6729 : loss : 0.025071, loss_ce: 0.008537
2022-01-10 02:16:22,534 iteration 6730 : loss : 0.016834, loss_ce: 0.007291
2022-01-10 02:16:24,153 iteration 6731 : loss : 0.019623, loss_ce: 0.006279
2022-01-10 02:16:25,722 iteration 6732 : loss : 0.013247, loss_ce: 0.006353
 99%|████████████████████████████▋| 396/400 [3:13:00<01:58, 29.58s/it]2022-01-10 02:16:27,297 iteration 6733 : loss : 0.009902, loss_ce: 0.004110
2022-01-10 02:16:28,855 iteration 6734 : loss : 0.015584, loss_ce: 0.004091
2022-01-10 02:16:30,458 iteration 6735 : loss : 0.023655, loss_ce: 0.007302
2022-01-10 02:16:32,016 iteration 6736 : loss : 0.013891, loss_ce: 0.006114
2022-01-10 02:16:33,533 iteration 6737 : loss : 0.013823, loss_ce: 0.007173
2022-01-10 02:16:35,037 iteration 6738 : loss : 0.013387, loss_ce: 0.004408
2022-01-10 02:16:36,608 iteration 6739 : loss : 0.016603, loss_ce: 0.007065
2022-01-10 02:16:38,212 iteration 6740 : loss : 0.019956, loss_ce: 0.007246
2022-01-10 02:16:39,798 iteration 6741 : loss : 0.018635, loss_ce: 0.007725
2022-01-10 02:16:41,345 iteration 6742 : loss : 0.015346, loss_ce: 0.007094
2022-01-10 02:16:42,858 iteration 6743 : loss : 0.012609, loss_ce: 0.003020
2022-01-10 02:16:44,469 iteration 6744 : loss : 0.019888, loss_ce: 0.008031
2022-01-10 02:16:46,035 iteration 6745 : loss : 0.022373, loss_ce: 0.009109
2022-01-10 02:16:47,541 iteration 6746 : loss : 0.010763, loss_ce: 0.002934
2022-01-10 02:16:49,067 iteration 6747 : loss : 0.022678, loss_ce: 0.006934
2022-01-10 02:16:50,658 iteration 6748 : loss : 0.013119, loss_ce: 0.005762
2022-01-10 02:16:52,212 iteration 6749 : loss : 0.013865, loss_ce: 0.003929
 99%|████████████████████████████▊| 397/400 [3:13:27<01:25, 28.65s/it]2022-01-10 02:16:53,833 iteration 6750 : loss : 0.011216, loss_ce: 0.004923
2022-01-10 02:16:55,466 iteration 6751 : loss : 0.013079, loss_ce: 0.005006
2022-01-10 02:16:56,970 iteration 6752 : loss : 0.012377, loss_ce: 0.004078
2022-01-10 02:16:58,539 iteration 6753 : loss : 0.020013, loss_ce: 0.008196
2022-01-10 02:17:00,137 iteration 6754 : loss : 0.012758, loss_ce: 0.004578
2022-01-10 02:17:01,671 iteration 6755 : loss : 0.015608, loss_ce: 0.004922
2022-01-10 02:17:03,206 iteration 6756 : loss : 0.016971, loss_ce: 0.007820
2022-01-10 02:17:04,837 iteration 6757 : loss : 0.015765, loss_ce: 0.005729
2022-01-10 02:17:06,373 iteration 6758 : loss : 0.014446, loss_ce: 0.006783
2022-01-10 02:17:07,951 iteration 6759 : loss : 0.015179, loss_ce: 0.006998
2022-01-10 02:17:09,586 iteration 6760 : loss : 0.024071, loss_ce: 0.004590
2022-01-10 02:17:11,154 iteration 6761 : loss : 0.018110, loss_ce: 0.006771
2022-01-10 02:17:12,700 iteration 6762 : loss : 0.016882, loss_ce: 0.005364
2022-01-10 02:17:14,322 iteration 6763 : loss : 0.026414, loss_ce: 0.010531
2022-01-10 02:17:15,859 iteration 6764 : loss : 0.020412, loss_ce: 0.008232
2022-01-10 02:17:17,481 iteration 6765 : loss : 0.017839, loss_ce: 0.006965
2022-01-10 02:17:19,021 iteration 6766 : loss : 0.012087, loss_ce: 0.005693
100%|████████████████████████████▊| 398/400 [3:13:54<00:56, 28.10s/it]2022-01-10 02:17:20,632 iteration 6767 : loss : 0.049562, loss_ce: 0.017838
2022-01-10 02:17:22,284 iteration 6768 : loss : 0.015150, loss_ce: 0.005564
2022-01-10 02:17:23,874 iteration 6769 : loss : 0.014781, loss_ce: 0.006151
2022-01-10 02:17:25,425 iteration 6770 : loss : 0.014741, loss_ce: 0.007425
2022-01-10 02:17:27,094 iteration 6771 : loss : 0.019623, loss_ce: 0.004236
2022-01-10 02:17:28,636 iteration 6772 : loss : 0.012425, loss_ce: 0.004538
2022-01-10 02:17:30,083 iteration 6773 : loss : 0.012107, loss_ce: 0.003694
2022-01-10 02:17:31,675 iteration 6774 : loss : 0.026863, loss_ce: 0.008103
2022-01-10 02:17:33,273 iteration 6775 : loss : 0.018254, loss_ce: 0.005767
2022-01-10 02:17:34,830 iteration 6776 : loss : 0.015215, loss_ce: 0.004816
2022-01-10 02:17:36,473 iteration 6777 : loss : 0.020760, loss_ce: 0.011885
2022-01-10 02:17:38,032 iteration 6778 : loss : 0.010122, loss_ce: 0.003324
2022-01-10 02:17:39,684 iteration 6779 : loss : 0.016791, loss_ce: 0.006994
2022-01-10 02:17:41,272 iteration 6780 : loss : 0.016137, loss_ce: 0.007799
2022-01-10 02:17:42,892 iteration 6781 : loss : 0.018861, loss_ce: 0.008277
2022-01-10 02:17:44,513 iteration 6782 : loss : 0.016660, loss_ce: 0.006568
2022-01-10 02:17:46,061 iteration 6783 : loss : 0.016384, loss_ce: 0.005671
100%|████████████████████████████▉| 399/400 [3:14:21<00:27, 27.78s/it]2022-01-10 02:17:47,759 iteration 6784 : loss : 0.020782, loss_ce: 0.008716
2022-01-10 02:17:49,361 iteration 6785 : loss : 0.016033, loss_ce: 0.005072
2022-01-10 02:17:50,888 iteration 6786 : loss : 0.012015, loss_ce: 0.004670
2022-01-10 02:17:52,423 iteration 6787 : loss : 0.012586, loss_ce: 0.004257
2022-01-10 02:17:54,010 iteration 6788 : loss : 0.015046, loss_ce: 0.006389
2022-01-10 02:17:55,584 iteration 6789 : loss : 0.023896, loss_ce: 0.007530
2022-01-10 02:17:57,133 iteration 6790 : loss : 0.011895, loss_ce: 0.005084
2022-01-10 02:17:58,723 iteration 6791 : loss : 0.016586, loss_ce: 0.006389
2022-01-10 02:18:00,243 iteration 6792 : loss : 0.013721, loss_ce: 0.006115
2022-01-10 02:18:01,882 iteration 6793 : loss : 0.045219, loss_ce: 0.018331
2022-01-10 02:18:03,457 iteration 6794 : loss : 0.021198, loss_ce: 0.006951
2022-01-10 02:18:05,067 iteration 6795 : loss : 0.017581, loss_ce: 0.006158
2022-01-10 02:18:06,692 iteration 6796 : loss : 0.018794, loss_ce: 0.009563
2022-01-10 02:18:08,184 iteration 6797 : loss : 0.013298, loss_ce: 0.004355
2022-01-10 02:18:09,788 iteration 6798 : loss : 0.018741, loss_ce: 0.005913
2022-01-10 02:18:11,448 iteration 6799 : loss : 0.019021, loss_ce: 0.009638
2022-01-10 02:18:11,489 Training Data Eval:
2022-01-10 02:18:19,529   Average segmentation loss on training set: 0.0086
2022-01-10 02:18:19,530 Validation Data Eval:
2022-01-10 02:18:22,298   Average segmentation loss on validation set: 0.0720
2022-01-10 02:18:23,816 iteration 6800 : loss : 0.012751, loss_ce: 0.003202
100%|█████████████████████████████| 400/400 [3:14:58<00:00, 30.78s/it]100%|█████████████████████████████| 400/400 [3:14:58<00:00, 29.25s/it]
